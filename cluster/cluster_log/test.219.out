Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=219, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12264-12319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793121
Iteration 2/25 | Loss: 0.00137612
Iteration 3/25 | Loss: 0.00126614
Iteration 4/25 | Loss: 0.00126611
Iteration 5/25 | Loss: 0.00124303
Iteration 6/25 | Loss: 0.00123959
Iteration 7/25 | Loss: 0.00123635
Iteration 8/25 | Loss: 0.00123464
Iteration 9/25 | Loss: 0.00123614
Iteration 10/25 | Loss: 0.00123256
Iteration 11/25 | Loss: 0.00123116
Iteration 12/25 | Loss: 0.00123082
Iteration 13/25 | Loss: 0.00123077
Iteration 14/25 | Loss: 0.00123077
Iteration 15/25 | Loss: 0.00123077
Iteration 16/25 | Loss: 0.00123077
Iteration 17/25 | Loss: 0.00123077
Iteration 18/25 | Loss: 0.00123077
Iteration 19/25 | Loss: 0.00123077
Iteration 20/25 | Loss: 0.00123076
Iteration 21/25 | Loss: 0.00123076
Iteration 22/25 | Loss: 0.00123076
Iteration 23/25 | Loss: 0.00123076
Iteration 24/25 | Loss: 0.00123076
Iteration 25/25 | Loss: 0.00123076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37441576
Iteration 2/25 | Loss: 0.00119665
Iteration 3/25 | Loss: 0.00119656
Iteration 4/25 | Loss: 0.00119656
Iteration 5/25 | Loss: 0.00119656
Iteration 6/25 | Loss: 0.00119656
Iteration 7/25 | Loss: 0.00119656
Iteration 8/25 | Loss: 0.00119656
Iteration 9/25 | Loss: 0.00119656
Iteration 10/25 | Loss: 0.00119656
Iteration 11/25 | Loss: 0.00119656
Iteration 12/25 | Loss: 0.00119656
Iteration 13/25 | Loss: 0.00119656
Iteration 14/25 | Loss: 0.00119656
Iteration 15/25 | Loss: 0.00119656
Iteration 16/25 | Loss: 0.00119656
Iteration 17/25 | Loss: 0.00119656
Iteration 18/25 | Loss: 0.00119656
Iteration 19/25 | Loss: 0.00119656
Iteration 20/25 | Loss: 0.00119656
Iteration 21/25 | Loss: 0.00119656
Iteration 22/25 | Loss: 0.00119656
Iteration 23/25 | Loss: 0.00119656
Iteration 24/25 | Loss: 0.00119656
Iteration 25/25 | Loss: 0.00119656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119656
Iteration 2/1000 | Loss: 0.00003458
Iteration 3/1000 | Loss: 0.00002325
Iteration 4/1000 | Loss: 0.00001927
Iteration 5/1000 | Loss: 0.00001760
Iteration 6/1000 | Loss: 0.00001645
Iteration 7/1000 | Loss: 0.00001558
Iteration 8/1000 | Loss: 0.00001511
Iteration 9/1000 | Loss: 0.00001492
Iteration 10/1000 | Loss: 0.00001469
Iteration 11/1000 | Loss: 0.00001448
Iteration 12/1000 | Loss: 0.00001441
Iteration 13/1000 | Loss: 0.00001417
Iteration 14/1000 | Loss: 0.00001412
Iteration 15/1000 | Loss: 0.00001406
Iteration 16/1000 | Loss: 0.00001404
Iteration 17/1000 | Loss: 0.00001397
Iteration 18/1000 | Loss: 0.00001395
Iteration 19/1000 | Loss: 0.00001394
Iteration 20/1000 | Loss: 0.00001394
Iteration 21/1000 | Loss: 0.00001394
Iteration 22/1000 | Loss: 0.00001393
Iteration 23/1000 | Loss: 0.00001393
Iteration 24/1000 | Loss: 0.00001392
Iteration 25/1000 | Loss: 0.00001392
Iteration 26/1000 | Loss: 0.00001391
Iteration 27/1000 | Loss: 0.00001391
Iteration 28/1000 | Loss: 0.00001391
Iteration 29/1000 | Loss: 0.00001391
Iteration 30/1000 | Loss: 0.00001391
Iteration 31/1000 | Loss: 0.00001391
Iteration 32/1000 | Loss: 0.00001390
Iteration 33/1000 | Loss: 0.00001390
Iteration 34/1000 | Loss: 0.00001390
Iteration 35/1000 | Loss: 0.00001390
Iteration 36/1000 | Loss: 0.00001390
Iteration 37/1000 | Loss: 0.00001390
Iteration 38/1000 | Loss: 0.00001389
Iteration 39/1000 | Loss: 0.00001389
Iteration 40/1000 | Loss: 0.00001388
Iteration 41/1000 | Loss: 0.00001388
Iteration 42/1000 | Loss: 0.00001387
Iteration 43/1000 | Loss: 0.00001387
Iteration 44/1000 | Loss: 0.00001386
Iteration 45/1000 | Loss: 0.00001386
Iteration 46/1000 | Loss: 0.00001386
Iteration 47/1000 | Loss: 0.00001386
Iteration 48/1000 | Loss: 0.00001385
Iteration 49/1000 | Loss: 0.00001385
Iteration 50/1000 | Loss: 0.00001384
Iteration 51/1000 | Loss: 0.00001384
Iteration 52/1000 | Loss: 0.00001384
Iteration 53/1000 | Loss: 0.00001384
Iteration 54/1000 | Loss: 0.00001383
Iteration 55/1000 | Loss: 0.00001383
Iteration 56/1000 | Loss: 0.00001383
Iteration 57/1000 | Loss: 0.00001383
Iteration 58/1000 | Loss: 0.00001383
Iteration 59/1000 | Loss: 0.00001382
Iteration 60/1000 | Loss: 0.00001382
Iteration 61/1000 | Loss: 0.00001382
Iteration 62/1000 | Loss: 0.00001382
Iteration 63/1000 | Loss: 0.00001381
Iteration 64/1000 | Loss: 0.00001381
Iteration 65/1000 | Loss: 0.00001380
Iteration 66/1000 | Loss: 0.00001380
Iteration 67/1000 | Loss: 0.00001380
Iteration 68/1000 | Loss: 0.00001380
Iteration 69/1000 | Loss: 0.00001380
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001379
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001379
Iteration 78/1000 | Loss: 0.00001378
Iteration 79/1000 | Loss: 0.00001378
Iteration 80/1000 | Loss: 0.00001378
Iteration 81/1000 | Loss: 0.00001378
Iteration 82/1000 | Loss: 0.00001377
Iteration 83/1000 | Loss: 0.00001377
Iteration 84/1000 | Loss: 0.00001377
Iteration 85/1000 | Loss: 0.00001376
Iteration 86/1000 | Loss: 0.00001376
Iteration 87/1000 | Loss: 0.00001376
Iteration 88/1000 | Loss: 0.00001375
Iteration 89/1000 | Loss: 0.00001375
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001374
Iteration 92/1000 | Loss: 0.00001374
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001373
Iteration 95/1000 | Loss: 0.00001373
Iteration 96/1000 | Loss: 0.00001373
Iteration 97/1000 | Loss: 0.00001373
Iteration 98/1000 | Loss: 0.00001373
Iteration 99/1000 | Loss: 0.00001372
Iteration 100/1000 | Loss: 0.00001372
Iteration 101/1000 | Loss: 0.00001372
Iteration 102/1000 | Loss: 0.00001372
Iteration 103/1000 | Loss: 0.00001372
Iteration 104/1000 | Loss: 0.00001371
Iteration 105/1000 | Loss: 0.00001371
Iteration 106/1000 | Loss: 0.00001371
Iteration 107/1000 | Loss: 0.00001370
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001370
Iteration 111/1000 | Loss: 0.00001370
Iteration 112/1000 | Loss: 0.00001369
Iteration 113/1000 | Loss: 0.00001369
Iteration 114/1000 | Loss: 0.00001369
Iteration 115/1000 | Loss: 0.00001368
Iteration 116/1000 | Loss: 0.00001368
Iteration 117/1000 | Loss: 0.00001368
Iteration 118/1000 | Loss: 0.00001368
Iteration 119/1000 | Loss: 0.00001367
Iteration 120/1000 | Loss: 0.00001367
Iteration 121/1000 | Loss: 0.00001367
Iteration 122/1000 | Loss: 0.00001367
Iteration 123/1000 | Loss: 0.00001367
Iteration 124/1000 | Loss: 0.00001367
Iteration 125/1000 | Loss: 0.00001367
Iteration 126/1000 | Loss: 0.00001367
Iteration 127/1000 | Loss: 0.00001367
Iteration 128/1000 | Loss: 0.00001367
Iteration 129/1000 | Loss: 0.00001367
Iteration 130/1000 | Loss: 0.00001366
Iteration 131/1000 | Loss: 0.00001366
Iteration 132/1000 | Loss: 0.00001366
Iteration 133/1000 | Loss: 0.00001366
Iteration 134/1000 | Loss: 0.00001366
Iteration 135/1000 | Loss: 0.00001366
Iteration 136/1000 | Loss: 0.00001366
Iteration 137/1000 | Loss: 0.00001366
Iteration 138/1000 | Loss: 0.00001366
Iteration 139/1000 | Loss: 0.00001366
Iteration 140/1000 | Loss: 0.00001366
Iteration 141/1000 | Loss: 0.00001366
Iteration 142/1000 | Loss: 0.00001366
Iteration 143/1000 | Loss: 0.00001366
Iteration 144/1000 | Loss: 0.00001366
Iteration 145/1000 | Loss: 0.00001366
Iteration 146/1000 | Loss: 0.00001366
Iteration 147/1000 | Loss: 0.00001366
Iteration 148/1000 | Loss: 0.00001366
Iteration 149/1000 | Loss: 0.00001365
Iteration 150/1000 | Loss: 0.00001365
Iteration 151/1000 | Loss: 0.00001365
Iteration 152/1000 | Loss: 0.00001365
Iteration 153/1000 | Loss: 0.00001365
Iteration 154/1000 | Loss: 0.00001365
Iteration 155/1000 | Loss: 0.00001365
Iteration 156/1000 | Loss: 0.00001365
Iteration 157/1000 | Loss: 0.00001365
Iteration 158/1000 | Loss: 0.00001365
Iteration 159/1000 | Loss: 0.00001365
Iteration 160/1000 | Loss: 0.00001365
Iteration 161/1000 | Loss: 0.00001365
Iteration 162/1000 | Loss: 0.00001365
Iteration 163/1000 | Loss: 0.00001365
Iteration 164/1000 | Loss: 0.00001365
Iteration 165/1000 | Loss: 0.00001365
Iteration 166/1000 | Loss: 0.00001365
Iteration 167/1000 | Loss: 0.00001365
Iteration 168/1000 | Loss: 0.00001365
Iteration 169/1000 | Loss: 0.00001365
Iteration 170/1000 | Loss: 0.00001365
Iteration 171/1000 | Loss: 0.00001364
Iteration 172/1000 | Loss: 0.00001364
Iteration 173/1000 | Loss: 0.00001364
Iteration 174/1000 | Loss: 0.00001364
Iteration 175/1000 | Loss: 0.00001364
Iteration 176/1000 | Loss: 0.00001364
Iteration 177/1000 | Loss: 0.00001364
Iteration 178/1000 | Loss: 0.00001364
Iteration 179/1000 | Loss: 0.00001364
Iteration 180/1000 | Loss: 0.00001364
Iteration 181/1000 | Loss: 0.00001364
Iteration 182/1000 | Loss: 0.00001364
Iteration 183/1000 | Loss: 0.00001364
Iteration 184/1000 | Loss: 0.00001364
Iteration 185/1000 | Loss: 0.00001364
Iteration 186/1000 | Loss: 0.00001364
Iteration 187/1000 | Loss: 0.00001364
Iteration 188/1000 | Loss: 0.00001364
Iteration 189/1000 | Loss: 0.00001364
Iteration 190/1000 | Loss: 0.00001364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.3644036698678974e-05, 1.3644036698678974e-05, 1.3644036698678974e-05, 1.3644036698678974e-05, 1.3644036698678974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3644036698678974e-05

Optimization complete. Final v2v error: 3.0831034183502197 mm

Highest mean error: 4.496069431304932 mm for frame 222

Lowest mean error: 2.5666699409484863 mm for frame 120

Saving results

Total time: 63.02012395858765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771090
Iteration 2/25 | Loss: 0.00162070
Iteration 3/25 | Loss: 0.00143128
Iteration 4/25 | Loss: 0.00130732
Iteration 5/25 | Loss: 0.00130934
Iteration 6/25 | Loss: 0.00128213
Iteration 7/25 | Loss: 0.00127130
Iteration 8/25 | Loss: 0.00125733
Iteration 9/25 | Loss: 0.00126234
Iteration 10/25 | Loss: 0.00125683
Iteration 11/25 | Loss: 0.00124950
Iteration 12/25 | Loss: 0.00124767
Iteration 13/25 | Loss: 0.00124732
Iteration 14/25 | Loss: 0.00124720
Iteration 15/25 | Loss: 0.00124719
Iteration 16/25 | Loss: 0.00124719
Iteration 17/25 | Loss: 0.00124719
Iteration 18/25 | Loss: 0.00124719
Iteration 19/25 | Loss: 0.00124719
Iteration 20/25 | Loss: 0.00124719
Iteration 21/25 | Loss: 0.00124719
Iteration 22/25 | Loss: 0.00124719
Iteration 23/25 | Loss: 0.00124719
Iteration 24/25 | Loss: 0.00124719
Iteration 25/25 | Loss: 0.00124718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.78226900
Iteration 2/25 | Loss: 0.00134743
Iteration 3/25 | Loss: 0.00116285
Iteration 4/25 | Loss: 0.00116285
Iteration 5/25 | Loss: 0.00116285
Iteration 6/25 | Loss: 0.00116285
Iteration 7/25 | Loss: 0.00116285
Iteration 8/25 | Loss: 0.00116285
Iteration 9/25 | Loss: 0.00116285
Iteration 10/25 | Loss: 0.00116285
Iteration 11/25 | Loss: 0.00116285
Iteration 12/25 | Loss: 0.00116284
Iteration 13/25 | Loss: 0.00116284
Iteration 14/25 | Loss: 0.00116284
Iteration 15/25 | Loss: 0.00116284
Iteration 16/25 | Loss: 0.00116284
Iteration 17/25 | Loss: 0.00116284
Iteration 18/25 | Loss: 0.00116284
Iteration 19/25 | Loss: 0.00116284
Iteration 20/25 | Loss: 0.00116284
Iteration 21/25 | Loss: 0.00116284
Iteration 22/25 | Loss: 0.00116284
Iteration 23/25 | Loss: 0.00116284
Iteration 24/25 | Loss: 0.00116284
Iteration 25/25 | Loss: 0.00116284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116284
Iteration 2/1000 | Loss: 0.00017250
Iteration 3/1000 | Loss: 0.00017996
Iteration 4/1000 | Loss: 0.00010709
Iteration 5/1000 | Loss: 0.00010189
Iteration 6/1000 | Loss: 0.00010426
Iteration 7/1000 | Loss: 0.00001964
Iteration 8/1000 | Loss: 0.00002000
Iteration 9/1000 | Loss: 0.00001811
Iteration 10/1000 | Loss: 0.00001762
Iteration 11/1000 | Loss: 0.00001722
Iteration 12/1000 | Loss: 0.00001691
Iteration 13/1000 | Loss: 0.00001653
Iteration 14/1000 | Loss: 0.00072552
Iteration 15/1000 | Loss: 0.00020390
Iteration 16/1000 | Loss: 0.00006183
Iteration 17/1000 | Loss: 0.00002593
Iteration 18/1000 | Loss: 0.00001884
Iteration 19/1000 | Loss: 0.00004863
Iteration 20/1000 | Loss: 0.00001466
Iteration 21/1000 | Loss: 0.00002622
Iteration 22/1000 | Loss: 0.00001317
Iteration 23/1000 | Loss: 0.00001273
Iteration 24/1000 | Loss: 0.00006129
Iteration 25/1000 | Loss: 0.00001474
Iteration 26/1000 | Loss: 0.00001284
Iteration 27/1000 | Loss: 0.00001225
Iteration 28/1000 | Loss: 0.00001212
Iteration 29/1000 | Loss: 0.00009951
Iteration 30/1000 | Loss: 0.00001197
Iteration 31/1000 | Loss: 0.00001188
Iteration 32/1000 | Loss: 0.00001187
Iteration 33/1000 | Loss: 0.00001183
Iteration 34/1000 | Loss: 0.00001182
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001181
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001180
Iteration 39/1000 | Loss: 0.00001180
Iteration 40/1000 | Loss: 0.00001179
Iteration 41/1000 | Loss: 0.00001178
Iteration 42/1000 | Loss: 0.00001178
Iteration 43/1000 | Loss: 0.00001178
Iteration 44/1000 | Loss: 0.00001176
Iteration 45/1000 | Loss: 0.00001176
Iteration 46/1000 | Loss: 0.00001176
Iteration 47/1000 | Loss: 0.00001175
Iteration 48/1000 | Loss: 0.00001175
Iteration 49/1000 | Loss: 0.00001175
Iteration 50/1000 | Loss: 0.00001173
Iteration 51/1000 | Loss: 0.00001173
Iteration 52/1000 | Loss: 0.00001173
Iteration 53/1000 | Loss: 0.00001173
Iteration 54/1000 | Loss: 0.00001173
Iteration 55/1000 | Loss: 0.00001173
Iteration 56/1000 | Loss: 0.00001173
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001171
Iteration 59/1000 | Loss: 0.00001171
Iteration 60/1000 | Loss: 0.00001171
Iteration 61/1000 | Loss: 0.00001171
Iteration 62/1000 | Loss: 0.00001171
Iteration 63/1000 | Loss: 0.00001171
Iteration 64/1000 | Loss: 0.00001171
Iteration 65/1000 | Loss: 0.00001171
Iteration 66/1000 | Loss: 0.00001170
Iteration 67/1000 | Loss: 0.00001170
Iteration 68/1000 | Loss: 0.00004735
Iteration 69/1000 | Loss: 0.00006555
Iteration 70/1000 | Loss: 0.00001785
Iteration 71/1000 | Loss: 0.00001682
Iteration 72/1000 | Loss: 0.00001180
Iteration 73/1000 | Loss: 0.00001168
Iteration 74/1000 | Loss: 0.00001168
Iteration 75/1000 | Loss: 0.00001167
Iteration 76/1000 | Loss: 0.00001167
Iteration 77/1000 | Loss: 0.00001167
Iteration 78/1000 | Loss: 0.00001167
Iteration 79/1000 | Loss: 0.00001167
Iteration 80/1000 | Loss: 0.00001167
Iteration 81/1000 | Loss: 0.00001166
Iteration 82/1000 | Loss: 0.00001166
Iteration 83/1000 | Loss: 0.00001166
Iteration 84/1000 | Loss: 0.00001178
Iteration 85/1000 | Loss: 0.00001164
Iteration 86/1000 | Loss: 0.00001164
Iteration 87/1000 | Loss: 0.00001163
Iteration 88/1000 | Loss: 0.00001163
Iteration 89/1000 | Loss: 0.00001163
Iteration 90/1000 | Loss: 0.00001163
Iteration 91/1000 | Loss: 0.00001163
Iteration 92/1000 | Loss: 0.00001163
Iteration 93/1000 | Loss: 0.00001162
Iteration 94/1000 | Loss: 0.00001161
Iteration 95/1000 | Loss: 0.00001161
Iteration 96/1000 | Loss: 0.00001161
Iteration 97/1000 | Loss: 0.00001161
Iteration 98/1000 | Loss: 0.00001160
Iteration 99/1000 | Loss: 0.00001160
Iteration 100/1000 | Loss: 0.00001159
Iteration 101/1000 | Loss: 0.00001159
Iteration 102/1000 | Loss: 0.00001158
Iteration 103/1000 | Loss: 0.00001157
Iteration 104/1000 | Loss: 0.00001157
Iteration 105/1000 | Loss: 0.00001157
Iteration 106/1000 | Loss: 0.00001157
Iteration 107/1000 | Loss: 0.00001156
Iteration 108/1000 | Loss: 0.00001156
Iteration 109/1000 | Loss: 0.00001156
Iteration 110/1000 | Loss: 0.00001156
Iteration 111/1000 | Loss: 0.00001156
Iteration 112/1000 | Loss: 0.00001156
Iteration 113/1000 | Loss: 0.00001156
Iteration 114/1000 | Loss: 0.00001156
Iteration 115/1000 | Loss: 0.00001156
Iteration 116/1000 | Loss: 0.00001156
Iteration 117/1000 | Loss: 0.00001156
Iteration 118/1000 | Loss: 0.00001156
Iteration 119/1000 | Loss: 0.00001156
Iteration 120/1000 | Loss: 0.00001155
Iteration 121/1000 | Loss: 0.00001155
Iteration 122/1000 | Loss: 0.00001155
Iteration 123/1000 | Loss: 0.00001155
Iteration 124/1000 | Loss: 0.00001155
Iteration 125/1000 | Loss: 0.00001155
Iteration 126/1000 | Loss: 0.00001154
Iteration 127/1000 | Loss: 0.00001154
Iteration 128/1000 | Loss: 0.00001153
Iteration 129/1000 | Loss: 0.00001153
Iteration 130/1000 | Loss: 0.00001153
Iteration 131/1000 | Loss: 0.00001153
Iteration 132/1000 | Loss: 0.00001153
Iteration 133/1000 | Loss: 0.00001153
Iteration 134/1000 | Loss: 0.00001153
Iteration 135/1000 | Loss: 0.00001153
Iteration 136/1000 | Loss: 0.00001153
Iteration 137/1000 | Loss: 0.00001153
Iteration 138/1000 | Loss: 0.00001153
Iteration 139/1000 | Loss: 0.00001153
Iteration 140/1000 | Loss: 0.00001153
Iteration 141/1000 | Loss: 0.00001153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.1527855349413585e-05, 1.1527855349413585e-05, 1.1527855349413585e-05, 1.1527855349413585e-05, 1.1527855349413585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1527855349413585e-05

Optimization complete. Final v2v error: 2.901503801345825 mm

Highest mean error: 3.8245606422424316 mm for frame 47

Lowest mean error: 2.6041526794433594 mm for frame 38

Saving results

Total time: 95.07265377044678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001911
Iteration 2/25 | Loss: 0.00230984
Iteration 3/25 | Loss: 0.00144460
Iteration 4/25 | Loss: 0.00133738
Iteration 5/25 | Loss: 0.00131177
Iteration 6/25 | Loss: 0.00131294
Iteration 7/25 | Loss: 0.00131198
Iteration 8/25 | Loss: 0.00130436
Iteration 9/25 | Loss: 0.00130448
Iteration 10/25 | Loss: 0.00129696
Iteration 11/25 | Loss: 0.00128692
Iteration 12/25 | Loss: 0.00128425
Iteration 13/25 | Loss: 0.00128293
Iteration 14/25 | Loss: 0.00128595
Iteration 15/25 | Loss: 0.00128553
Iteration 16/25 | Loss: 0.00128522
Iteration 17/25 | Loss: 0.00128507
Iteration 18/25 | Loss: 0.00128439
Iteration 19/25 | Loss: 0.00128400
Iteration 20/25 | Loss: 0.00128267
Iteration 21/25 | Loss: 0.00128096
Iteration 22/25 | Loss: 0.00128066
Iteration 23/25 | Loss: 0.00128065
Iteration 24/25 | Loss: 0.00128065
Iteration 25/25 | Loss: 0.00128065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46692228
Iteration 2/25 | Loss: 0.00140701
Iteration 3/25 | Loss: 0.00140701
Iteration 4/25 | Loss: 0.00140700
Iteration 5/25 | Loss: 0.00140700
Iteration 6/25 | Loss: 0.00140700
Iteration 7/25 | Loss: 0.00140700
Iteration 8/25 | Loss: 0.00140700
Iteration 9/25 | Loss: 0.00140700
Iteration 10/25 | Loss: 0.00140700
Iteration 11/25 | Loss: 0.00140700
Iteration 12/25 | Loss: 0.00140700
Iteration 13/25 | Loss: 0.00140700
Iteration 14/25 | Loss: 0.00140700
Iteration 15/25 | Loss: 0.00140700
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001407002448104322, 0.001407002448104322, 0.001407002448104322, 0.001407002448104322, 0.001407002448104322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001407002448104322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140700
Iteration 2/1000 | Loss: 0.00003888
Iteration 3/1000 | Loss: 0.00002838
Iteration 4/1000 | Loss: 0.00002524
Iteration 5/1000 | Loss: 0.00002264
Iteration 6/1000 | Loss: 0.00002137
Iteration 7/1000 | Loss: 0.00002059
Iteration 8/1000 | Loss: 0.00001989
Iteration 9/1000 | Loss: 0.00022749
Iteration 10/1000 | Loss: 0.00012633
Iteration 11/1000 | Loss: 0.00011069
Iteration 12/1000 | Loss: 0.00013048
Iteration 13/1000 | Loss: 0.00010946
Iteration 14/1000 | Loss: 0.00002855
Iteration 15/1000 | Loss: 0.00002285
Iteration 16/1000 | Loss: 0.00002126
Iteration 17/1000 | Loss: 0.00002035
Iteration 18/1000 | Loss: 0.00001990
Iteration 19/1000 | Loss: 0.00001961
Iteration 20/1000 | Loss: 0.00001908
Iteration 21/1000 | Loss: 0.00001867
Iteration 22/1000 | Loss: 0.00001817
Iteration 23/1000 | Loss: 0.00001754
Iteration 24/1000 | Loss: 0.00001722
Iteration 25/1000 | Loss: 0.00001718
Iteration 26/1000 | Loss: 0.00001718
Iteration 27/1000 | Loss: 0.00001718
Iteration 28/1000 | Loss: 0.00001717
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00001715
Iteration 31/1000 | Loss: 0.00001715
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001713
Iteration 35/1000 | Loss: 0.00001713
Iteration 36/1000 | Loss: 0.00001713
Iteration 37/1000 | Loss: 0.00001713
Iteration 38/1000 | Loss: 0.00001713
Iteration 39/1000 | Loss: 0.00001713
Iteration 40/1000 | Loss: 0.00001713
Iteration 41/1000 | Loss: 0.00001713
Iteration 42/1000 | Loss: 0.00001713
Iteration 43/1000 | Loss: 0.00001712
Iteration 44/1000 | Loss: 0.00001712
Iteration 45/1000 | Loss: 0.00001712
Iteration 46/1000 | Loss: 0.00001709
Iteration 47/1000 | Loss: 0.00001709
Iteration 48/1000 | Loss: 0.00001709
Iteration 49/1000 | Loss: 0.00001709
Iteration 50/1000 | Loss: 0.00001709
Iteration 51/1000 | Loss: 0.00001709
Iteration 52/1000 | Loss: 0.00001709
Iteration 53/1000 | Loss: 0.00001709
Iteration 54/1000 | Loss: 0.00001708
Iteration 55/1000 | Loss: 0.00001708
Iteration 56/1000 | Loss: 0.00001708
Iteration 57/1000 | Loss: 0.00001708
Iteration 58/1000 | Loss: 0.00001708
Iteration 59/1000 | Loss: 0.00001708
Iteration 60/1000 | Loss: 0.00001707
Iteration 61/1000 | Loss: 0.00001707
Iteration 62/1000 | Loss: 0.00001707
Iteration 63/1000 | Loss: 0.00001707
Iteration 64/1000 | Loss: 0.00001707
Iteration 65/1000 | Loss: 0.00001707
Iteration 66/1000 | Loss: 0.00001707
Iteration 67/1000 | Loss: 0.00001706
Iteration 68/1000 | Loss: 0.00001706
Iteration 69/1000 | Loss: 0.00001706
Iteration 70/1000 | Loss: 0.00001706
Iteration 71/1000 | Loss: 0.00001706
Iteration 72/1000 | Loss: 0.00001706
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001704
Iteration 75/1000 | Loss: 0.00001704
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001703
Iteration 78/1000 | Loss: 0.00001703
Iteration 79/1000 | Loss: 0.00001703
Iteration 80/1000 | Loss: 0.00001703
Iteration 81/1000 | Loss: 0.00001703
Iteration 82/1000 | Loss: 0.00001703
Iteration 83/1000 | Loss: 0.00001703
Iteration 84/1000 | Loss: 0.00001703
Iteration 85/1000 | Loss: 0.00001703
Iteration 86/1000 | Loss: 0.00001703
Iteration 87/1000 | Loss: 0.00001703
Iteration 88/1000 | Loss: 0.00001703
Iteration 89/1000 | Loss: 0.00001703
Iteration 90/1000 | Loss: 0.00001703
Iteration 91/1000 | Loss: 0.00001703
Iteration 92/1000 | Loss: 0.00001702
Iteration 93/1000 | Loss: 0.00001702
Iteration 94/1000 | Loss: 0.00001702
Iteration 95/1000 | Loss: 0.00001702
Iteration 96/1000 | Loss: 0.00001702
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001701
Iteration 101/1000 | Loss: 0.00001701
Iteration 102/1000 | Loss: 0.00001700
Iteration 103/1000 | Loss: 0.00001700
Iteration 104/1000 | Loss: 0.00001700
Iteration 105/1000 | Loss: 0.00001700
Iteration 106/1000 | Loss: 0.00001700
Iteration 107/1000 | Loss: 0.00001700
Iteration 108/1000 | Loss: 0.00001700
Iteration 109/1000 | Loss: 0.00001700
Iteration 110/1000 | Loss: 0.00001700
Iteration 111/1000 | Loss: 0.00001700
Iteration 112/1000 | Loss: 0.00001700
Iteration 113/1000 | Loss: 0.00001700
Iteration 114/1000 | Loss: 0.00001700
Iteration 115/1000 | Loss: 0.00001700
Iteration 116/1000 | Loss: 0.00001700
Iteration 117/1000 | Loss: 0.00001700
Iteration 118/1000 | Loss: 0.00001700
Iteration 119/1000 | Loss: 0.00001700
Iteration 120/1000 | Loss: 0.00001700
Iteration 121/1000 | Loss: 0.00001699
Iteration 122/1000 | Loss: 0.00001699
Iteration 123/1000 | Loss: 0.00001699
Iteration 124/1000 | Loss: 0.00001699
Iteration 125/1000 | Loss: 0.00001699
Iteration 126/1000 | Loss: 0.00001699
Iteration 127/1000 | Loss: 0.00001699
Iteration 128/1000 | Loss: 0.00001699
Iteration 129/1000 | Loss: 0.00001699
Iteration 130/1000 | Loss: 0.00001699
Iteration 131/1000 | Loss: 0.00001699
Iteration 132/1000 | Loss: 0.00001699
Iteration 133/1000 | Loss: 0.00001699
Iteration 134/1000 | Loss: 0.00001699
Iteration 135/1000 | Loss: 0.00001699
Iteration 136/1000 | Loss: 0.00001699
Iteration 137/1000 | Loss: 0.00001699
Iteration 138/1000 | Loss: 0.00001699
Iteration 139/1000 | Loss: 0.00001699
Iteration 140/1000 | Loss: 0.00001699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.6993466488202102e-05, 1.6993466488202102e-05, 1.6993466488202102e-05, 1.6993466488202102e-05, 1.6993466488202102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6993466488202102e-05

Optimization complete. Final v2v error: 3.5594918727874756 mm

Highest mean error: 4.04340124130249 mm for frame 124

Lowest mean error: 3.092761278152466 mm for frame 200

Saving results

Total time: 86.02169132232666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896196
Iteration 2/25 | Loss: 0.00178683
Iteration 3/25 | Loss: 0.00148576
Iteration 4/25 | Loss: 0.00148512
Iteration 5/25 | Loss: 0.00143942
Iteration 6/25 | Loss: 0.00142848
Iteration 7/25 | Loss: 0.00142159
Iteration 8/25 | Loss: 0.00142583
Iteration 9/25 | Loss: 0.00142850
Iteration 10/25 | Loss: 0.00143607
Iteration 11/25 | Loss: 0.00142873
Iteration 12/25 | Loss: 0.00141411
Iteration 13/25 | Loss: 0.00140631
Iteration 14/25 | Loss: 0.00141024
Iteration 15/25 | Loss: 0.00138796
Iteration 16/25 | Loss: 0.00137973
Iteration 17/25 | Loss: 0.00137987
Iteration 18/25 | Loss: 0.00139059
Iteration 19/25 | Loss: 0.00138801
Iteration 20/25 | Loss: 0.00137587
Iteration 21/25 | Loss: 0.00138685
Iteration 22/25 | Loss: 0.00138669
Iteration 23/25 | Loss: 0.00138081
Iteration 24/25 | Loss: 0.00138323
Iteration 25/25 | Loss: 0.00138320

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.30067372
Iteration 2/25 | Loss: 0.00138486
Iteration 3/25 | Loss: 0.00138476
Iteration 4/25 | Loss: 0.00138476
Iteration 5/25 | Loss: 0.00138476
Iteration 6/25 | Loss: 0.00138476
Iteration 7/25 | Loss: 0.00138476
Iteration 8/25 | Loss: 0.00138476
Iteration 9/25 | Loss: 0.00138476
Iteration 10/25 | Loss: 0.00138476
Iteration 11/25 | Loss: 0.00138476
Iteration 12/25 | Loss: 0.00138476
Iteration 13/25 | Loss: 0.00138475
Iteration 14/25 | Loss: 0.00138475
Iteration 15/25 | Loss: 0.00138475
Iteration 16/25 | Loss: 0.00138475
Iteration 17/25 | Loss: 0.00138475
Iteration 18/25 | Loss: 0.00138475
Iteration 19/25 | Loss: 0.00138475
Iteration 20/25 | Loss: 0.00138475
Iteration 21/25 | Loss: 0.00138475
Iteration 22/25 | Loss: 0.00138475
Iteration 23/25 | Loss: 0.00138475
Iteration 24/25 | Loss: 0.00138475
Iteration 25/25 | Loss: 0.00138475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138475
Iteration 2/1000 | Loss: 0.00419323
Iteration 3/1000 | Loss: 0.00192839
Iteration 4/1000 | Loss: 0.00028251
Iteration 5/1000 | Loss: 0.00035482
Iteration 6/1000 | Loss: 0.00119448
Iteration 7/1000 | Loss: 0.00093500
Iteration 8/1000 | Loss: 0.00088385
Iteration 9/1000 | Loss: 0.00054082
Iteration 10/1000 | Loss: 0.00041396
Iteration 11/1000 | Loss: 0.00038676
Iteration 12/1000 | Loss: 0.00041321
Iteration 13/1000 | Loss: 0.00034994
Iteration 14/1000 | Loss: 0.00037185
Iteration 15/1000 | Loss: 0.00046417
Iteration 16/1000 | Loss: 0.00042249
Iteration 17/1000 | Loss: 0.00040098
Iteration 18/1000 | Loss: 0.00016328
Iteration 19/1000 | Loss: 0.00031510
Iteration 20/1000 | Loss: 0.00027466
Iteration 21/1000 | Loss: 0.00016085
Iteration 22/1000 | Loss: 0.00023936
Iteration 23/1000 | Loss: 0.00013603
Iteration 24/1000 | Loss: 0.00020307
Iteration 25/1000 | Loss: 0.00032832
Iteration 26/1000 | Loss: 0.00024466
Iteration 27/1000 | Loss: 0.00035982
Iteration 28/1000 | Loss: 0.00039331
Iteration 29/1000 | Loss: 0.00019252
Iteration 30/1000 | Loss: 0.00010088
Iteration 31/1000 | Loss: 0.00019049
Iteration 32/1000 | Loss: 0.00029952
Iteration 33/1000 | Loss: 0.00022225
Iteration 34/1000 | Loss: 0.00021366
Iteration 35/1000 | Loss: 0.00021258
Iteration 36/1000 | Loss: 0.00021963
Iteration 37/1000 | Loss: 0.00036107
Iteration 38/1000 | Loss: 0.00039160
Iteration 39/1000 | Loss: 0.00038919
Iteration 40/1000 | Loss: 0.00038422
Iteration 41/1000 | Loss: 0.00040348
Iteration 42/1000 | Loss: 0.00035447
Iteration 43/1000 | Loss: 0.00042314
Iteration 44/1000 | Loss: 0.00033119
Iteration 45/1000 | Loss: 0.00022536
Iteration 46/1000 | Loss: 0.00022470
Iteration 47/1000 | Loss: 0.00021101
Iteration 48/1000 | Loss: 0.00021880
Iteration 49/1000 | Loss: 0.00010914
Iteration 50/1000 | Loss: 0.00020929
Iteration 51/1000 | Loss: 0.00012566
Iteration 52/1000 | Loss: 0.00011529
Iteration 53/1000 | Loss: 0.00012078
Iteration 54/1000 | Loss: 0.00016469
Iteration 55/1000 | Loss: 0.00011610
Iteration 56/1000 | Loss: 0.00017021
Iteration 57/1000 | Loss: 0.00019417
Iteration 58/1000 | Loss: 0.00015840
Iteration 59/1000 | Loss: 0.00023079
Iteration 60/1000 | Loss: 0.00028651
Iteration 61/1000 | Loss: 0.00014830
Iteration 62/1000 | Loss: 0.00029475
Iteration 63/1000 | Loss: 0.00031407
Iteration 64/1000 | Loss: 0.00031964
Iteration 65/1000 | Loss: 0.00029714
Iteration 66/1000 | Loss: 0.00029674
Iteration 67/1000 | Loss: 0.00011705
Iteration 68/1000 | Loss: 0.00027961
Iteration 69/1000 | Loss: 0.00030790
Iteration 70/1000 | Loss: 0.00028958
Iteration 71/1000 | Loss: 0.00024643
Iteration 72/1000 | Loss: 0.00026887
Iteration 73/1000 | Loss: 0.00029214
Iteration 74/1000 | Loss: 0.00029330
Iteration 75/1000 | Loss: 0.00027544
Iteration 76/1000 | Loss: 0.00025236
Iteration 77/1000 | Loss: 0.00009565
Iteration 78/1000 | Loss: 0.00019907
Iteration 79/1000 | Loss: 0.00027001
Iteration 80/1000 | Loss: 0.00025952
Iteration 81/1000 | Loss: 0.00020610
Iteration 82/1000 | Loss: 0.00018437
Iteration 83/1000 | Loss: 0.00028512
Iteration 84/1000 | Loss: 0.00021268
Iteration 85/1000 | Loss: 0.00028553
Iteration 86/1000 | Loss: 0.00018823
Iteration 87/1000 | Loss: 0.00033351
Iteration 88/1000 | Loss: 0.00017949
Iteration 89/1000 | Loss: 0.00020378
Iteration 90/1000 | Loss: 0.00019147
Iteration 91/1000 | Loss: 0.00016710
Iteration 92/1000 | Loss: 0.00028919
Iteration 93/1000 | Loss: 0.00019561
Iteration 94/1000 | Loss: 0.00024999
Iteration 95/1000 | Loss: 0.00017383
Iteration 96/1000 | Loss: 0.00030918
Iteration 97/1000 | Loss: 0.00025926
Iteration 98/1000 | Loss: 0.00018670
Iteration 99/1000 | Loss: 0.00051507
Iteration 100/1000 | Loss: 0.00031334
Iteration 101/1000 | Loss: 0.00011391
Iteration 102/1000 | Loss: 0.00020041
Iteration 103/1000 | Loss: 0.00026661
Iteration 104/1000 | Loss: 0.00036553
Iteration 105/1000 | Loss: 0.00035444
Iteration 106/1000 | Loss: 0.00018931
Iteration 107/1000 | Loss: 0.00035322
Iteration 108/1000 | Loss: 0.00040419
Iteration 109/1000 | Loss: 0.00029355
Iteration 110/1000 | Loss: 0.00027429
Iteration 111/1000 | Loss: 0.00023382
Iteration 112/1000 | Loss: 0.00023356
Iteration 113/1000 | Loss: 0.00028087
Iteration 114/1000 | Loss: 0.00032630
Iteration 115/1000 | Loss: 0.00036307
Iteration 116/1000 | Loss: 0.00044309
Iteration 117/1000 | Loss: 0.00027733
Iteration 118/1000 | Loss: 0.00008507
Iteration 119/1000 | Loss: 0.00023241
Iteration 120/1000 | Loss: 0.00024072
Iteration 121/1000 | Loss: 0.00026243
Iteration 122/1000 | Loss: 0.00029725
Iteration 123/1000 | Loss: 0.00035736
Iteration 124/1000 | Loss: 0.00026308
Iteration 125/1000 | Loss: 0.00030137
Iteration 126/1000 | Loss: 0.00019805
Iteration 127/1000 | Loss: 0.00030989
Iteration 128/1000 | Loss: 0.00038431
Iteration 129/1000 | Loss: 0.00026085
Iteration 130/1000 | Loss: 0.00039498
Iteration 131/1000 | Loss: 0.00021591
Iteration 132/1000 | Loss: 0.00024442
Iteration 133/1000 | Loss: 0.00034487
Iteration 134/1000 | Loss: 0.00036435
Iteration 135/1000 | Loss: 0.00021260
Iteration 136/1000 | Loss: 0.00021876
Iteration 137/1000 | Loss: 0.00020080
Iteration 138/1000 | Loss: 0.00024052
Iteration 139/1000 | Loss: 0.00018394
Iteration 140/1000 | Loss: 0.00020368
Iteration 141/1000 | Loss: 0.00018274
Iteration 142/1000 | Loss: 0.00012324
Iteration 143/1000 | Loss: 0.00039136
Iteration 144/1000 | Loss: 0.00017472
Iteration 145/1000 | Loss: 0.00034375
Iteration 146/1000 | Loss: 0.00013685
Iteration 147/1000 | Loss: 0.00014290
Iteration 148/1000 | Loss: 0.00017656
Iteration 149/1000 | Loss: 0.00012854
Iteration 150/1000 | Loss: 0.00017767
Iteration 151/1000 | Loss: 0.00019283
Iteration 152/1000 | Loss: 0.00041488
Iteration 153/1000 | Loss: 0.00030835
Iteration 154/1000 | Loss: 0.00023896
Iteration 155/1000 | Loss: 0.00023511
Iteration 156/1000 | Loss: 0.00028293
Iteration 157/1000 | Loss: 0.00022922
Iteration 158/1000 | Loss: 0.00030767
Iteration 159/1000 | Loss: 0.00010946
Iteration 160/1000 | Loss: 0.00020839
Iteration 161/1000 | Loss: 0.00004624
Iteration 162/1000 | Loss: 0.00007254
Iteration 163/1000 | Loss: 0.00004003
Iteration 164/1000 | Loss: 0.00008816
Iteration 165/1000 | Loss: 0.00019739
Iteration 166/1000 | Loss: 0.00008796
Iteration 167/1000 | Loss: 0.00008261
Iteration 168/1000 | Loss: 0.00018563
Iteration 169/1000 | Loss: 0.00008273
Iteration 170/1000 | Loss: 0.00022883
Iteration 171/1000 | Loss: 0.00004256
Iteration 172/1000 | Loss: 0.00045675
Iteration 173/1000 | Loss: 0.00004887
Iteration 174/1000 | Loss: 0.00003782
Iteration 175/1000 | Loss: 0.00003436
Iteration 176/1000 | Loss: 0.00003310
Iteration 177/1000 | Loss: 0.00003220
Iteration 178/1000 | Loss: 0.00003118
Iteration 179/1000 | Loss: 0.00003050
Iteration 180/1000 | Loss: 0.00003000
Iteration 181/1000 | Loss: 0.00005846
Iteration 182/1000 | Loss: 0.00002865
Iteration 183/1000 | Loss: 0.00002763
Iteration 184/1000 | Loss: 0.00002688
Iteration 185/1000 | Loss: 0.00002624
Iteration 186/1000 | Loss: 0.00002578
Iteration 187/1000 | Loss: 0.00002561
Iteration 188/1000 | Loss: 0.00002539
Iteration 189/1000 | Loss: 0.00002539
Iteration 190/1000 | Loss: 0.00002519
Iteration 191/1000 | Loss: 0.00002514
Iteration 192/1000 | Loss: 0.00002507
Iteration 193/1000 | Loss: 0.00002507
Iteration 194/1000 | Loss: 0.00002496
Iteration 195/1000 | Loss: 0.00002493
Iteration 196/1000 | Loss: 0.00002493
Iteration 197/1000 | Loss: 0.00002492
Iteration 198/1000 | Loss: 0.00002491
Iteration 199/1000 | Loss: 0.00002490
Iteration 200/1000 | Loss: 0.00002490
Iteration 201/1000 | Loss: 0.00002489
Iteration 202/1000 | Loss: 0.00002489
Iteration 203/1000 | Loss: 0.00002488
Iteration 204/1000 | Loss: 0.00002487
Iteration 205/1000 | Loss: 0.00002487
Iteration 206/1000 | Loss: 0.00002487
Iteration 207/1000 | Loss: 0.00002487
Iteration 208/1000 | Loss: 0.00002486
Iteration 209/1000 | Loss: 0.00002485
Iteration 210/1000 | Loss: 0.00002484
Iteration 211/1000 | Loss: 0.00002484
Iteration 212/1000 | Loss: 0.00002483
Iteration 213/1000 | Loss: 0.00002482
Iteration 214/1000 | Loss: 0.00002482
Iteration 215/1000 | Loss: 0.00002482
Iteration 216/1000 | Loss: 0.00002481
Iteration 217/1000 | Loss: 0.00002480
Iteration 218/1000 | Loss: 0.00002479
Iteration 219/1000 | Loss: 0.00002478
Iteration 220/1000 | Loss: 0.00002477
Iteration 221/1000 | Loss: 0.00002477
Iteration 222/1000 | Loss: 0.00002477
Iteration 223/1000 | Loss: 0.00002476
Iteration 224/1000 | Loss: 0.00002475
Iteration 225/1000 | Loss: 0.00002475
Iteration 226/1000 | Loss: 0.00002474
Iteration 227/1000 | Loss: 0.00002474
Iteration 228/1000 | Loss: 0.00002473
Iteration 229/1000 | Loss: 0.00002473
Iteration 230/1000 | Loss: 0.00002473
Iteration 231/1000 | Loss: 0.00002472
Iteration 232/1000 | Loss: 0.00002472
Iteration 233/1000 | Loss: 0.00002472
Iteration 234/1000 | Loss: 0.00002471
Iteration 235/1000 | Loss: 0.00002471
Iteration 236/1000 | Loss: 0.00002471
Iteration 237/1000 | Loss: 0.00002470
Iteration 238/1000 | Loss: 0.00002470
Iteration 239/1000 | Loss: 0.00002470
Iteration 240/1000 | Loss: 0.00002470
Iteration 241/1000 | Loss: 0.00002470
Iteration 242/1000 | Loss: 0.00002469
Iteration 243/1000 | Loss: 0.00002469
Iteration 244/1000 | Loss: 0.00002468
Iteration 245/1000 | Loss: 0.00002468
Iteration 246/1000 | Loss: 0.00002468
Iteration 247/1000 | Loss: 0.00002468
Iteration 248/1000 | Loss: 0.00002468
Iteration 249/1000 | Loss: 0.00002467
Iteration 250/1000 | Loss: 0.00002467
Iteration 251/1000 | Loss: 0.00002467
Iteration 252/1000 | Loss: 0.00002466
Iteration 253/1000 | Loss: 0.00002466
Iteration 254/1000 | Loss: 0.00002465
Iteration 255/1000 | Loss: 0.00002465
Iteration 256/1000 | Loss: 0.00002465
Iteration 257/1000 | Loss: 0.00002465
Iteration 258/1000 | Loss: 0.00002465
Iteration 259/1000 | Loss: 0.00002465
Iteration 260/1000 | Loss: 0.00002464
Iteration 261/1000 | Loss: 0.00002464
Iteration 262/1000 | Loss: 0.00002464
Iteration 263/1000 | Loss: 0.00002463
Iteration 264/1000 | Loss: 0.00002463
Iteration 265/1000 | Loss: 0.00002463
Iteration 266/1000 | Loss: 0.00002463
Iteration 267/1000 | Loss: 0.00002463
Iteration 268/1000 | Loss: 0.00002463
Iteration 269/1000 | Loss: 0.00002462
Iteration 270/1000 | Loss: 0.00002462
Iteration 271/1000 | Loss: 0.00002462
Iteration 272/1000 | Loss: 0.00002462
Iteration 273/1000 | Loss: 0.00002462
Iteration 274/1000 | Loss: 0.00002462
Iteration 275/1000 | Loss: 0.00002462
Iteration 276/1000 | Loss: 0.00002462
Iteration 277/1000 | Loss: 0.00002462
Iteration 278/1000 | Loss: 0.00002462
Iteration 279/1000 | Loss: 0.00002462
Iteration 280/1000 | Loss: 0.00002462
Iteration 281/1000 | Loss: 0.00002462
Iteration 282/1000 | Loss: 0.00002461
Iteration 283/1000 | Loss: 0.00002461
Iteration 284/1000 | Loss: 0.00002461
Iteration 285/1000 | Loss: 0.00002461
Iteration 286/1000 | Loss: 0.00002461
Iteration 287/1000 | Loss: 0.00002461
Iteration 288/1000 | Loss: 0.00002461
Iteration 289/1000 | Loss: 0.00002461
Iteration 290/1000 | Loss: 0.00002461
Iteration 291/1000 | Loss: 0.00002461
Iteration 292/1000 | Loss: 0.00002461
Iteration 293/1000 | Loss: 0.00002461
Iteration 294/1000 | Loss: 0.00002461
Iteration 295/1000 | Loss: 0.00002461
Iteration 296/1000 | Loss: 0.00002461
Iteration 297/1000 | Loss: 0.00002461
Iteration 298/1000 | Loss: 0.00002461
Iteration 299/1000 | Loss: 0.00002461
Iteration 300/1000 | Loss: 0.00002461
Iteration 301/1000 | Loss: 0.00002461
Iteration 302/1000 | Loss: 0.00002461
Iteration 303/1000 | Loss: 0.00002461
Iteration 304/1000 | Loss: 0.00002461
Iteration 305/1000 | Loss: 0.00002461
Iteration 306/1000 | Loss: 0.00002461
Iteration 307/1000 | Loss: 0.00002461
Iteration 308/1000 | Loss: 0.00002461
Iteration 309/1000 | Loss: 0.00002461
Iteration 310/1000 | Loss: 0.00002461
Iteration 311/1000 | Loss: 0.00002461
Iteration 312/1000 | Loss: 0.00002461
Iteration 313/1000 | Loss: 0.00002461
Iteration 314/1000 | Loss: 0.00002461
Iteration 315/1000 | Loss: 0.00002461
Iteration 316/1000 | Loss: 0.00002461
Iteration 317/1000 | Loss: 0.00002461
Iteration 318/1000 | Loss: 0.00002461
Iteration 319/1000 | Loss: 0.00002461
Iteration 320/1000 | Loss: 0.00002461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 320. Stopping optimization.
Last 5 losses: [2.4605211365269497e-05, 2.4605211365269497e-05, 2.4605211365269497e-05, 2.4605211365269497e-05, 2.4605211365269497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4605211365269497e-05

Optimization complete. Final v2v error: 4.115489959716797 mm

Highest mean error: 5.952475547790527 mm for frame 97

Lowest mean error: 3.1448607444763184 mm for frame 144

Saving results

Total time: 324.1972966194153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459995
Iteration 2/25 | Loss: 0.00139431
Iteration 3/25 | Loss: 0.00127797
Iteration 4/25 | Loss: 0.00126630
Iteration 5/25 | Loss: 0.00126200
Iteration 6/25 | Loss: 0.00126161
Iteration 7/25 | Loss: 0.00126161
Iteration 8/25 | Loss: 0.00126161
Iteration 9/25 | Loss: 0.00126161
Iteration 10/25 | Loss: 0.00126161
Iteration 11/25 | Loss: 0.00126161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012616064632311463, 0.0012616064632311463, 0.0012616064632311463, 0.0012616064632311463, 0.0012616064632311463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012616064632311463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34863079
Iteration 2/25 | Loss: 0.00099234
Iteration 3/25 | Loss: 0.00099234
Iteration 4/25 | Loss: 0.00099234
Iteration 5/25 | Loss: 0.00099234
Iteration 6/25 | Loss: 0.00099234
Iteration 7/25 | Loss: 0.00099234
Iteration 8/25 | Loss: 0.00099233
Iteration 9/25 | Loss: 0.00099233
Iteration 10/25 | Loss: 0.00099233
Iteration 11/25 | Loss: 0.00099233
Iteration 12/25 | Loss: 0.00099233
Iteration 13/25 | Loss: 0.00099233
Iteration 14/25 | Loss: 0.00099233
Iteration 15/25 | Loss: 0.00099233
Iteration 16/25 | Loss: 0.00099233
Iteration 17/25 | Loss: 0.00099233
Iteration 18/25 | Loss: 0.00099233
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009923343313857913, 0.0009923343313857913, 0.0009923343313857913, 0.0009923343313857913, 0.0009923343313857913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009923343313857913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099233
Iteration 2/1000 | Loss: 0.00002367
Iteration 3/1000 | Loss: 0.00001747
Iteration 4/1000 | Loss: 0.00001622
Iteration 5/1000 | Loss: 0.00001559
Iteration 6/1000 | Loss: 0.00001523
Iteration 7/1000 | Loss: 0.00001504
Iteration 8/1000 | Loss: 0.00001480
Iteration 9/1000 | Loss: 0.00001451
Iteration 10/1000 | Loss: 0.00001426
Iteration 11/1000 | Loss: 0.00001423
Iteration 12/1000 | Loss: 0.00001422
Iteration 13/1000 | Loss: 0.00001421
Iteration 14/1000 | Loss: 0.00001417
Iteration 15/1000 | Loss: 0.00001415
Iteration 16/1000 | Loss: 0.00001408
Iteration 17/1000 | Loss: 0.00001405
Iteration 18/1000 | Loss: 0.00001404
Iteration 19/1000 | Loss: 0.00001404
Iteration 20/1000 | Loss: 0.00001404
Iteration 21/1000 | Loss: 0.00001402
Iteration 22/1000 | Loss: 0.00001402
Iteration 23/1000 | Loss: 0.00001401
Iteration 24/1000 | Loss: 0.00001400
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001398
Iteration 27/1000 | Loss: 0.00001397
Iteration 28/1000 | Loss: 0.00001397
Iteration 29/1000 | Loss: 0.00001395
Iteration 30/1000 | Loss: 0.00001395
Iteration 31/1000 | Loss: 0.00001395
Iteration 32/1000 | Loss: 0.00001395
Iteration 33/1000 | Loss: 0.00001395
Iteration 34/1000 | Loss: 0.00001395
Iteration 35/1000 | Loss: 0.00001394
Iteration 36/1000 | Loss: 0.00001394
Iteration 37/1000 | Loss: 0.00001394
Iteration 38/1000 | Loss: 0.00001394
Iteration 39/1000 | Loss: 0.00001394
Iteration 40/1000 | Loss: 0.00001394
Iteration 41/1000 | Loss: 0.00001394
Iteration 42/1000 | Loss: 0.00001394
Iteration 43/1000 | Loss: 0.00001393
Iteration 44/1000 | Loss: 0.00001393
Iteration 45/1000 | Loss: 0.00001392
Iteration 46/1000 | Loss: 0.00001392
Iteration 47/1000 | Loss: 0.00001391
Iteration 48/1000 | Loss: 0.00001391
Iteration 49/1000 | Loss: 0.00001390
Iteration 50/1000 | Loss: 0.00001390
Iteration 51/1000 | Loss: 0.00001390
Iteration 52/1000 | Loss: 0.00001389
Iteration 53/1000 | Loss: 0.00001389
Iteration 54/1000 | Loss: 0.00001388
Iteration 55/1000 | Loss: 0.00001388
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001387
Iteration 58/1000 | Loss: 0.00001387
Iteration 59/1000 | Loss: 0.00001386
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001385
Iteration 63/1000 | Loss: 0.00001384
Iteration 64/1000 | Loss: 0.00001383
Iteration 65/1000 | Loss: 0.00001383
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001380
Iteration 74/1000 | Loss: 0.00001380
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001379
Iteration 78/1000 | Loss: 0.00001379
Iteration 79/1000 | Loss: 0.00001379
Iteration 80/1000 | Loss: 0.00001378
Iteration 81/1000 | Loss: 0.00001378
Iteration 82/1000 | Loss: 0.00001378
Iteration 83/1000 | Loss: 0.00001378
Iteration 84/1000 | Loss: 0.00001378
Iteration 85/1000 | Loss: 0.00001378
Iteration 86/1000 | Loss: 0.00001378
Iteration 87/1000 | Loss: 0.00001378
Iteration 88/1000 | Loss: 0.00001377
Iteration 89/1000 | Loss: 0.00001377
Iteration 90/1000 | Loss: 0.00001377
Iteration 91/1000 | Loss: 0.00001377
Iteration 92/1000 | Loss: 0.00001377
Iteration 93/1000 | Loss: 0.00001377
Iteration 94/1000 | Loss: 0.00001376
Iteration 95/1000 | Loss: 0.00001376
Iteration 96/1000 | Loss: 0.00001376
Iteration 97/1000 | Loss: 0.00001376
Iteration 98/1000 | Loss: 0.00001376
Iteration 99/1000 | Loss: 0.00001376
Iteration 100/1000 | Loss: 0.00001376
Iteration 101/1000 | Loss: 0.00001375
Iteration 102/1000 | Loss: 0.00001375
Iteration 103/1000 | Loss: 0.00001375
Iteration 104/1000 | Loss: 0.00001375
Iteration 105/1000 | Loss: 0.00001374
Iteration 106/1000 | Loss: 0.00001374
Iteration 107/1000 | Loss: 0.00001374
Iteration 108/1000 | Loss: 0.00001374
Iteration 109/1000 | Loss: 0.00001374
Iteration 110/1000 | Loss: 0.00001374
Iteration 111/1000 | Loss: 0.00001374
Iteration 112/1000 | Loss: 0.00001374
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001373
Iteration 116/1000 | Loss: 0.00001373
Iteration 117/1000 | Loss: 0.00001373
Iteration 118/1000 | Loss: 0.00001373
Iteration 119/1000 | Loss: 0.00001373
Iteration 120/1000 | Loss: 0.00001373
Iteration 121/1000 | Loss: 0.00001373
Iteration 122/1000 | Loss: 0.00001373
Iteration 123/1000 | Loss: 0.00001373
Iteration 124/1000 | Loss: 0.00001373
Iteration 125/1000 | Loss: 0.00001373
Iteration 126/1000 | Loss: 0.00001373
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.3727838449995033e-05, 1.3727838449995033e-05, 1.3727838449995033e-05, 1.3727838449995033e-05, 1.3727838449995033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3727838449995033e-05

Optimization complete. Final v2v error: 3.118720293045044 mm

Highest mean error: 3.3967957496643066 mm for frame 175

Lowest mean error: 2.922903537750244 mm for frame 0

Saving results

Total time: 37.435855865478516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00505277
Iteration 2/25 | Loss: 0.00148064
Iteration 3/25 | Loss: 0.00135957
Iteration 4/25 | Loss: 0.00134084
Iteration 5/25 | Loss: 0.00133657
Iteration 6/25 | Loss: 0.00133657
Iteration 7/25 | Loss: 0.00133657
Iteration 8/25 | Loss: 0.00133657
Iteration 9/25 | Loss: 0.00133657
Iteration 10/25 | Loss: 0.00133657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013365669874474406, 0.0013365669874474406, 0.0013365669874474406, 0.0013365669874474406, 0.0013365669874474406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013365669874474406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35531020
Iteration 2/25 | Loss: 0.00124872
Iteration 3/25 | Loss: 0.00124872
Iteration 4/25 | Loss: 0.00124872
Iteration 5/25 | Loss: 0.00124872
Iteration 6/25 | Loss: 0.00124872
Iteration 7/25 | Loss: 0.00124872
Iteration 8/25 | Loss: 0.00124871
Iteration 9/25 | Loss: 0.00124871
Iteration 10/25 | Loss: 0.00124871
Iteration 11/25 | Loss: 0.00124871
Iteration 12/25 | Loss: 0.00124871
Iteration 13/25 | Loss: 0.00124871
Iteration 14/25 | Loss: 0.00124871
Iteration 15/25 | Loss: 0.00124871
Iteration 16/25 | Loss: 0.00124871
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001248714281246066, 0.001248714281246066, 0.001248714281246066, 0.001248714281246066, 0.001248714281246066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001248714281246066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124871
Iteration 2/1000 | Loss: 0.00004551
Iteration 3/1000 | Loss: 0.00003469
Iteration 4/1000 | Loss: 0.00003219
Iteration 5/1000 | Loss: 0.00003089
Iteration 6/1000 | Loss: 0.00003009
Iteration 7/1000 | Loss: 0.00002942
Iteration 8/1000 | Loss: 0.00002907
Iteration 9/1000 | Loss: 0.00002871
Iteration 10/1000 | Loss: 0.00002846
Iteration 11/1000 | Loss: 0.00002826
Iteration 12/1000 | Loss: 0.00002809
Iteration 13/1000 | Loss: 0.00002808
Iteration 14/1000 | Loss: 0.00002795
Iteration 15/1000 | Loss: 0.00002794
Iteration 16/1000 | Loss: 0.00002793
Iteration 17/1000 | Loss: 0.00002791
Iteration 18/1000 | Loss: 0.00002781
Iteration 19/1000 | Loss: 0.00002779
Iteration 20/1000 | Loss: 0.00002779
Iteration 21/1000 | Loss: 0.00002778
Iteration 22/1000 | Loss: 0.00002778
Iteration 23/1000 | Loss: 0.00002778
Iteration 24/1000 | Loss: 0.00002778
Iteration 25/1000 | Loss: 0.00002778
Iteration 26/1000 | Loss: 0.00002778
Iteration 27/1000 | Loss: 0.00002778
Iteration 28/1000 | Loss: 0.00002777
Iteration 29/1000 | Loss: 0.00002777
Iteration 30/1000 | Loss: 0.00002777
Iteration 31/1000 | Loss: 0.00002777
Iteration 32/1000 | Loss: 0.00002777
Iteration 33/1000 | Loss: 0.00002776
Iteration 34/1000 | Loss: 0.00002776
Iteration 35/1000 | Loss: 0.00002776
Iteration 36/1000 | Loss: 0.00002775
Iteration 37/1000 | Loss: 0.00002775
Iteration 38/1000 | Loss: 0.00002775
Iteration 39/1000 | Loss: 0.00002775
Iteration 40/1000 | Loss: 0.00002774
Iteration 41/1000 | Loss: 0.00002774
Iteration 42/1000 | Loss: 0.00002774
Iteration 43/1000 | Loss: 0.00002774
Iteration 44/1000 | Loss: 0.00002774
Iteration 45/1000 | Loss: 0.00002774
Iteration 46/1000 | Loss: 0.00002773
Iteration 47/1000 | Loss: 0.00002773
Iteration 48/1000 | Loss: 0.00002773
Iteration 49/1000 | Loss: 0.00002773
Iteration 50/1000 | Loss: 0.00002773
Iteration 51/1000 | Loss: 0.00002773
Iteration 52/1000 | Loss: 0.00002773
Iteration 53/1000 | Loss: 0.00002773
Iteration 54/1000 | Loss: 0.00002773
Iteration 55/1000 | Loss: 0.00002773
Iteration 56/1000 | Loss: 0.00002773
Iteration 57/1000 | Loss: 0.00002773
Iteration 58/1000 | Loss: 0.00002773
Iteration 59/1000 | Loss: 0.00002773
Iteration 60/1000 | Loss: 0.00002773
Iteration 61/1000 | Loss: 0.00002773
Iteration 62/1000 | Loss: 0.00002773
Iteration 63/1000 | Loss: 0.00002773
Iteration 64/1000 | Loss: 0.00002773
Iteration 65/1000 | Loss: 0.00002773
Iteration 66/1000 | Loss: 0.00002773
Iteration 67/1000 | Loss: 0.00002773
Iteration 68/1000 | Loss: 0.00002773
Iteration 69/1000 | Loss: 0.00002773
Iteration 70/1000 | Loss: 0.00002773
Iteration 71/1000 | Loss: 0.00002773
Iteration 72/1000 | Loss: 0.00002773
Iteration 73/1000 | Loss: 0.00002773
Iteration 74/1000 | Loss: 0.00002773
Iteration 75/1000 | Loss: 0.00002773
Iteration 76/1000 | Loss: 0.00002773
Iteration 77/1000 | Loss: 0.00002773
Iteration 78/1000 | Loss: 0.00002773
Iteration 79/1000 | Loss: 0.00002773
Iteration 80/1000 | Loss: 0.00002773
Iteration 81/1000 | Loss: 0.00002773
Iteration 82/1000 | Loss: 0.00002773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [2.7727679480449297e-05, 2.7727679480449297e-05, 2.7727679480449297e-05, 2.7727679480449297e-05, 2.7727679480449297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7727679480449297e-05

Optimization complete. Final v2v error: 4.304834842681885 mm

Highest mean error: 4.920036315917969 mm for frame 172

Lowest mean error: 3.756294012069702 mm for frame 0

Saving results

Total time: 34.921061515808105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038406
Iteration 2/25 | Loss: 0.00212164
Iteration 3/25 | Loss: 0.00170990
Iteration 4/25 | Loss: 0.00166035
Iteration 5/25 | Loss: 0.00174484
Iteration 6/25 | Loss: 0.00161507
Iteration 7/25 | Loss: 0.00145888
Iteration 8/25 | Loss: 0.00132595
Iteration 9/25 | Loss: 0.00133759
Iteration 10/25 | Loss: 0.00131557
Iteration 11/25 | Loss: 0.00129001
Iteration 12/25 | Loss: 0.00127520
Iteration 13/25 | Loss: 0.00127273
Iteration 14/25 | Loss: 0.00127353
Iteration 15/25 | Loss: 0.00127185
Iteration 16/25 | Loss: 0.00127305
Iteration 17/25 | Loss: 0.00126720
Iteration 18/25 | Loss: 0.00126255
Iteration 19/25 | Loss: 0.00127598
Iteration 20/25 | Loss: 0.00126965
Iteration 21/25 | Loss: 0.00127343
Iteration 22/25 | Loss: 0.00126865
Iteration 23/25 | Loss: 0.00125868
Iteration 24/25 | Loss: 0.00124234
Iteration 25/25 | Loss: 0.00124811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48337507
Iteration 2/25 | Loss: 0.00184572
Iteration 3/25 | Loss: 0.00182275
Iteration 4/25 | Loss: 0.00182275
Iteration 5/25 | Loss: 0.00182275
Iteration 6/25 | Loss: 0.00182275
Iteration 7/25 | Loss: 0.00182275
Iteration 8/25 | Loss: 0.00182275
Iteration 9/25 | Loss: 0.00182275
Iteration 10/25 | Loss: 0.00182275
Iteration 11/25 | Loss: 0.00182275
Iteration 12/25 | Loss: 0.00182275
Iteration 13/25 | Loss: 0.00182275
Iteration 14/25 | Loss: 0.00182275
Iteration 15/25 | Loss: 0.00182275
Iteration 16/25 | Loss: 0.00182275
Iteration 17/25 | Loss: 0.00182275
Iteration 18/25 | Loss: 0.00182275
Iteration 19/25 | Loss: 0.00182275
Iteration 20/25 | Loss: 0.00182275
Iteration 21/25 | Loss: 0.00182275
Iteration 22/25 | Loss: 0.00182275
Iteration 23/25 | Loss: 0.00182275
Iteration 24/25 | Loss: 0.00182275
Iteration 25/25 | Loss: 0.00182275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182275
Iteration 2/1000 | Loss: 0.00010662
Iteration 3/1000 | Loss: 0.00051002
Iteration 4/1000 | Loss: 0.00080121
Iteration 5/1000 | Loss: 0.00018089
Iteration 6/1000 | Loss: 0.00008220
Iteration 7/1000 | Loss: 0.00023346
Iteration 8/1000 | Loss: 0.00025273
Iteration 9/1000 | Loss: 0.00016967
Iteration 10/1000 | Loss: 0.00010727
Iteration 11/1000 | Loss: 0.00010649
Iteration 12/1000 | Loss: 0.00002951
Iteration 13/1000 | Loss: 0.00002584
Iteration 14/1000 | Loss: 0.00019479
Iteration 15/1000 | Loss: 0.00003142
Iteration 16/1000 | Loss: 0.00004821
Iteration 17/1000 | Loss: 0.00002554
Iteration 18/1000 | Loss: 0.00020705
Iteration 19/1000 | Loss: 0.00015883
Iteration 20/1000 | Loss: 0.00002995
Iteration 21/1000 | Loss: 0.00034141
Iteration 22/1000 | Loss: 0.00019200
Iteration 23/1000 | Loss: 0.00004306
Iteration 24/1000 | Loss: 0.00022834
Iteration 25/1000 | Loss: 0.00006070
Iteration 26/1000 | Loss: 0.00013231
Iteration 27/1000 | Loss: 0.00012172
Iteration 28/1000 | Loss: 0.00012148
Iteration 29/1000 | Loss: 0.00017310
Iteration 30/1000 | Loss: 0.00011177
Iteration 31/1000 | Loss: 0.00015043
Iteration 32/1000 | Loss: 0.00003101
Iteration 33/1000 | Loss: 0.00014096
Iteration 34/1000 | Loss: 0.00005181
Iteration 35/1000 | Loss: 0.00002043
Iteration 36/1000 | Loss: 0.00018754
Iteration 37/1000 | Loss: 0.00026033
Iteration 38/1000 | Loss: 0.00017671
Iteration 39/1000 | Loss: 0.00023339
Iteration 40/1000 | Loss: 0.00016721
Iteration 41/1000 | Loss: 0.00022124
Iteration 42/1000 | Loss: 0.00013498
Iteration 43/1000 | Loss: 0.00021757
Iteration 44/1000 | Loss: 0.00003257
Iteration 45/1000 | Loss: 0.00001791
Iteration 46/1000 | Loss: 0.00004046
Iteration 47/1000 | Loss: 0.00001772
Iteration 48/1000 | Loss: 0.00001591
Iteration 49/1000 | Loss: 0.00001506
Iteration 50/1000 | Loss: 0.00001475
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001700
Iteration 53/1000 | Loss: 0.00001377
Iteration 54/1000 | Loss: 0.00001376
Iteration 55/1000 | Loss: 0.00001376
Iteration 56/1000 | Loss: 0.00001365
Iteration 57/1000 | Loss: 0.00001357
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001346
Iteration 61/1000 | Loss: 0.00001345
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001342
Iteration 65/1000 | Loss: 0.00001342
Iteration 66/1000 | Loss: 0.00001342
Iteration 67/1000 | Loss: 0.00001342
Iteration 68/1000 | Loss: 0.00001342
Iteration 69/1000 | Loss: 0.00001342
Iteration 70/1000 | Loss: 0.00001341
Iteration 71/1000 | Loss: 0.00001341
Iteration 72/1000 | Loss: 0.00001341
Iteration 73/1000 | Loss: 0.00001338
Iteration 74/1000 | Loss: 0.00001338
Iteration 75/1000 | Loss: 0.00001337
Iteration 76/1000 | Loss: 0.00001337
Iteration 77/1000 | Loss: 0.00001337
Iteration 78/1000 | Loss: 0.00001337
Iteration 79/1000 | Loss: 0.00001337
Iteration 80/1000 | Loss: 0.00001336
Iteration 81/1000 | Loss: 0.00001336
Iteration 82/1000 | Loss: 0.00001336
Iteration 83/1000 | Loss: 0.00001336
Iteration 84/1000 | Loss: 0.00001335
Iteration 85/1000 | Loss: 0.00001335
Iteration 86/1000 | Loss: 0.00001334
Iteration 87/1000 | Loss: 0.00001334
Iteration 88/1000 | Loss: 0.00001333
Iteration 89/1000 | Loss: 0.00001333
Iteration 90/1000 | Loss: 0.00001333
Iteration 91/1000 | Loss: 0.00001332
Iteration 92/1000 | Loss: 0.00001332
Iteration 93/1000 | Loss: 0.00001332
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001330
Iteration 98/1000 | Loss: 0.00001329
Iteration 99/1000 | Loss: 0.00001329
Iteration 100/1000 | Loss: 0.00001329
Iteration 101/1000 | Loss: 0.00001329
Iteration 102/1000 | Loss: 0.00001329
Iteration 103/1000 | Loss: 0.00001328
Iteration 104/1000 | Loss: 0.00001328
Iteration 105/1000 | Loss: 0.00001328
Iteration 106/1000 | Loss: 0.00001328
Iteration 107/1000 | Loss: 0.00001328
Iteration 108/1000 | Loss: 0.00001328
Iteration 109/1000 | Loss: 0.00001328
Iteration 110/1000 | Loss: 0.00001328
Iteration 111/1000 | Loss: 0.00001327
Iteration 112/1000 | Loss: 0.00001327
Iteration 113/1000 | Loss: 0.00001327
Iteration 114/1000 | Loss: 0.00001327
Iteration 115/1000 | Loss: 0.00001327
Iteration 116/1000 | Loss: 0.00001327
Iteration 117/1000 | Loss: 0.00001327
Iteration 118/1000 | Loss: 0.00001326
Iteration 119/1000 | Loss: 0.00001326
Iteration 120/1000 | Loss: 0.00001326
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001325
Iteration 125/1000 | Loss: 0.00001325
Iteration 126/1000 | Loss: 0.00001325
Iteration 127/1000 | Loss: 0.00001324
Iteration 128/1000 | Loss: 0.00001324
Iteration 129/1000 | Loss: 0.00001324
Iteration 130/1000 | Loss: 0.00001324
Iteration 131/1000 | Loss: 0.00001324
Iteration 132/1000 | Loss: 0.00001324
Iteration 133/1000 | Loss: 0.00001323
Iteration 134/1000 | Loss: 0.00001323
Iteration 135/1000 | Loss: 0.00001323
Iteration 136/1000 | Loss: 0.00001323
Iteration 137/1000 | Loss: 0.00001323
Iteration 138/1000 | Loss: 0.00001323
Iteration 139/1000 | Loss: 0.00001323
Iteration 140/1000 | Loss: 0.00001323
Iteration 141/1000 | Loss: 0.00001322
Iteration 142/1000 | Loss: 0.00001322
Iteration 143/1000 | Loss: 0.00001322
Iteration 144/1000 | Loss: 0.00001322
Iteration 145/1000 | Loss: 0.00001322
Iteration 146/1000 | Loss: 0.00001322
Iteration 147/1000 | Loss: 0.00001322
Iteration 148/1000 | Loss: 0.00001322
Iteration 149/1000 | Loss: 0.00001322
Iteration 150/1000 | Loss: 0.00001322
Iteration 151/1000 | Loss: 0.00001322
Iteration 152/1000 | Loss: 0.00001321
Iteration 153/1000 | Loss: 0.00001321
Iteration 154/1000 | Loss: 0.00029294
Iteration 155/1000 | Loss: 0.00029294
Iteration 156/1000 | Loss: 0.00029366
Iteration 157/1000 | Loss: 0.00024020
Iteration 158/1000 | Loss: 0.00002396
Iteration 159/1000 | Loss: 0.00001881
Iteration 160/1000 | Loss: 0.00002575
Iteration 161/1000 | Loss: 0.00001761
Iteration 162/1000 | Loss: 0.00001560
Iteration 163/1000 | Loss: 0.00001403
Iteration 164/1000 | Loss: 0.00001350
Iteration 165/1000 | Loss: 0.00001316
Iteration 166/1000 | Loss: 0.00001312
Iteration 167/1000 | Loss: 0.00028043
Iteration 168/1000 | Loss: 0.00002162
Iteration 169/1000 | Loss: 0.00023542
Iteration 170/1000 | Loss: 0.00015435
Iteration 171/1000 | Loss: 0.00002072
Iteration 172/1000 | Loss: 0.00001562
Iteration 173/1000 | Loss: 0.00001430
Iteration 174/1000 | Loss: 0.00001366
Iteration 175/1000 | Loss: 0.00001535
Iteration 176/1000 | Loss: 0.00001372
Iteration 177/1000 | Loss: 0.00001363
Iteration 178/1000 | Loss: 0.00001359
Iteration 179/1000 | Loss: 0.00001358
Iteration 180/1000 | Loss: 0.00001352
Iteration 181/1000 | Loss: 0.00001335
Iteration 182/1000 | Loss: 0.00001303
Iteration 183/1000 | Loss: 0.00001266
Iteration 184/1000 | Loss: 0.00001260
Iteration 185/1000 | Loss: 0.00001230
Iteration 186/1000 | Loss: 0.00001205
Iteration 187/1000 | Loss: 0.00001201
Iteration 188/1000 | Loss: 0.00001201
Iteration 189/1000 | Loss: 0.00001200
Iteration 190/1000 | Loss: 0.00001194
Iteration 191/1000 | Loss: 0.00001193
Iteration 192/1000 | Loss: 0.00001193
Iteration 193/1000 | Loss: 0.00001191
Iteration 194/1000 | Loss: 0.00001186
Iteration 195/1000 | Loss: 0.00001186
Iteration 196/1000 | Loss: 0.00001186
Iteration 197/1000 | Loss: 0.00001186
Iteration 198/1000 | Loss: 0.00001186
Iteration 199/1000 | Loss: 0.00001186
Iteration 200/1000 | Loss: 0.00001186
Iteration 201/1000 | Loss: 0.00001186
Iteration 202/1000 | Loss: 0.00001186
Iteration 203/1000 | Loss: 0.00001185
Iteration 204/1000 | Loss: 0.00001185
Iteration 205/1000 | Loss: 0.00001185
Iteration 206/1000 | Loss: 0.00001184
Iteration 207/1000 | Loss: 0.00001183
Iteration 208/1000 | Loss: 0.00001182
Iteration 209/1000 | Loss: 0.00001182
Iteration 210/1000 | Loss: 0.00001182
Iteration 211/1000 | Loss: 0.00001182
Iteration 212/1000 | Loss: 0.00001182
Iteration 213/1000 | Loss: 0.00001182
Iteration 214/1000 | Loss: 0.00001181
Iteration 215/1000 | Loss: 0.00001181
Iteration 216/1000 | Loss: 0.00001181
Iteration 217/1000 | Loss: 0.00001181
Iteration 218/1000 | Loss: 0.00001181
Iteration 219/1000 | Loss: 0.00001181
Iteration 220/1000 | Loss: 0.00001180
Iteration 221/1000 | Loss: 0.00001180
Iteration 222/1000 | Loss: 0.00001179
Iteration 223/1000 | Loss: 0.00001179
Iteration 224/1000 | Loss: 0.00001179
Iteration 225/1000 | Loss: 0.00001179
Iteration 226/1000 | Loss: 0.00001179
Iteration 227/1000 | Loss: 0.00001178
Iteration 228/1000 | Loss: 0.00001178
Iteration 229/1000 | Loss: 0.00001178
Iteration 230/1000 | Loss: 0.00001178
Iteration 231/1000 | Loss: 0.00001178
Iteration 232/1000 | Loss: 0.00001178
Iteration 233/1000 | Loss: 0.00001178
Iteration 234/1000 | Loss: 0.00001178
Iteration 235/1000 | Loss: 0.00001177
Iteration 236/1000 | Loss: 0.00001177
Iteration 237/1000 | Loss: 0.00001177
Iteration 238/1000 | Loss: 0.00001177
Iteration 239/1000 | Loss: 0.00001177
Iteration 240/1000 | Loss: 0.00001177
Iteration 241/1000 | Loss: 0.00001177
Iteration 242/1000 | Loss: 0.00001177
Iteration 243/1000 | Loss: 0.00001177
Iteration 244/1000 | Loss: 0.00001176
Iteration 245/1000 | Loss: 0.00001176
Iteration 246/1000 | Loss: 0.00001176
Iteration 247/1000 | Loss: 0.00001176
Iteration 248/1000 | Loss: 0.00001176
Iteration 249/1000 | Loss: 0.00001176
Iteration 250/1000 | Loss: 0.00001176
Iteration 251/1000 | Loss: 0.00001175
Iteration 252/1000 | Loss: 0.00001175
Iteration 253/1000 | Loss: 0.00001175
Iteration 254/1000 | Loss: 0.00001175
Iteration 255/1000 | Loss: 0.00001175
Iteration 256/1000 | Loss: 0.00001174
Iteration 257/1000 | Loss: 0.00001174
Iteration 258/1000 | Loss: 0.00001174
Iteration 259/1000 | Loss: 0.00001174
Iteration 260/1000 | Loss: 0.00001174
Iteration 261/1000 | Loss: 0.00001173
Iteration 262/1000 | Loss: 0.00001173
Iteration 263/1000 | Loss: 0.00001173
Iteration 264/1000 | Loss: 0.00001173
Iteration 265/1000 | Loss: 0.00001173
Iteration 266/1000 | Loss: 0.00001173
Iteration 267/1000 | Loss: 0.00001172
Iteration 268/1000 | Loss: 0.00001172
Iteration 269/1000 | Loss: 0.00001172
Iteration 270/1000 | Loss: 0.00001171
Iteration 271/1000 | Loss: 0.00001171
Iteration 272/1000 | Loss: 0.00001171
Iteration 273/1000 | Loss: 0.00001171
Iteration 274/1000 | Loss: 0.00001171
Iteration 275/1000 | Loss: 0.00001171
Iteration 276/1000 | Loss: 0.00001170
Iteration 277/1000 | Loss: 0.00001170
Iteration 278/1000 | Loss: 0.00001170
Iteration 279/1000 | Loss: 0.00001170
Iteration 280/1000 | Loss: 0.00001169
Iteration 281/1000 | Loss: 0.00001169
Iteration 282/1000 | Loss: 0.00001169
Iteration 283/1000 | Loss: 0.00001169
Iteration 284/1000 | Loss: 0.00001168
Iteration 285/1000 | Loss: 0.00001168
Iteration 286/1000 | Loss: 0.00001168
Iteration 287/1000 | Loss: 0.00001168
Iteration 288/1000 | Loss: 0.00001168
Iteration 289/1000 | Loss: 0.00001168
Iteration 290/1000 | Loss: 0.00001167
Iteration 291/1000 | Loss: 0.00001167
Iteration 292/1000 | Loss: 0.00001167
Iteration 293/1000 | Loss: 0.00001167
Iteration 294/1000 | Loss: 0.00001167
Iteration 295/1000 | Loss: 0.00001167
Iteration 296/1000 | Loss: 0.00001167
Iteration 297/1000 | Loss: 0.00001167
Iteration 298/1000 | Loss: 0.00001166
Iteration 299/1000 | Loss: 0.00001166
Iteration 300/1000 | Loss: 0.00001166
Iteration 301/1000 | Loss: 0.00001166
Iteration 302/1000 | Loss: 0.00001166
Iteration 303/1000 | Loss: 0.00001166
Iteration 304/1000 | Loss: 0.00001166
Iteration 305/1000 | Loss: 0.00001166
Iteration 306/1000 | Loss: 0.00001166
Iteration 307/1000 | Loss: 0.00001166
Iteration 308/1000 | Loss: 0.00001166
Iteration 309/1000 | Loss: 0.00001166
Iteration 310/1000 | Loss: 0.00001166
Iteration 311/1000 | Loss: 0.00001166
Iteration 312/1000 | Loss: 0.00001166
Iteration 313/1000 | Loss: 0.00001166
Iteration 314/1000 | Loss: 0.00001166
Iteration 315/1000 | Loss: 0.00001166
Iteration 316/1000 | Loss: 0.00001165
Iteration 317/1000 | Loss: 0.00001165
Iteration 318/1000 | Loss: 0.00001165
Iteration 319/1000 | Loss: 0.00001165
Iteration 320/1000 | Loss: 0.00001165
Iteration 321/1000 | Loss: 0.00001165
Iteration 322/1000 | Loss: 0.00001165
Iteration 323/1000 | Loss: 0.00001165
Iteration 324/1000 | Loss: 0.00001165
Iteration 325/1000 | Loss: 0.00001165
Iteration 326/1000 | Loss: 0.00001165
Iteration 327/1000 | Loss: 0.00001165
Iteration 328/1000 | Loss: 0.00001165
Iteration 329/1000 | Loss: 0.00001165
Iteration 330/1000 | Loss: 0.00001165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 330. Stopping optimization.
Last 5 losses: [1.1651348359009717e-05, 1.1651348359009717e-05, 1.1651348359009717e-05, 1.1651348359009717e-05, 1.1651348359009717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1651348359009717e-05

Optimization complete. Final v2v error: 2.889747142791748 mm

Highest mean error: 3.7795815467834473 mm for frame 58

Lowest mean error: 2.5006489753723145 mm for frame 44

Saving results

Total time: 179.94720649719238
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00555795
Iteration 2/25 | Loss: 0.00161239
Iteration 3/25 | Loss: 0.00136179
Iteration 4/25 | Loss: 0.00134205
Iteration 5/25 | Loss: 0.00133849
Iteration 6/25 | Loss: 0.00133795
Iteration 7/25 | Loss: 0.00133795
Iteration 8/25 | Loss: 0.00133795
Iteration 9/25 | Loss: 0.00133795
Iteration 10/25 | Loss: 0.00133795
Iteration 11/25 | Loss: 0.00133795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013379501178860664, 0.0013379501178860664, 0.0013379501178860664, 0.0013379501178860664, 0.0013379501178860664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013379501178860664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.42604303
Iteration 2/25 | Loss: 0.00093618
Iteration 3/25 | Loss: 0.00093608
Iteration 4/25 | Loss: 0.00093608
Iteration 5/25 | Loss: 0.00093608
Iteration 6/25 | Loss: 0.00093608
Iteration 7/25 | Loss: 0.00093608
Iteration 8/25 | Loss: 0.00093608
Iteration 9/25 | Loss: 0.00093608
Iteration 10/25 | Loss: 0.00093608
Iteration 11/25 | Loss: 0.00093608
Iteration 12/25 | Loss: 0.00093608
Iteration 13/25 | Loss: 0.00093608
Iteration 14/25 | Loss: 0.00093608
Iteration 15/25 | Loss: 0.00093608
Iteration 16/25 | Loss: 0.00093608
Iteration 17/25 | Loss: 0.00093608
Iteration 18/25 | Loss: 0.00093608
Iteration 19/25 | Loss: 0.00093608
Iteration 20/25 | Loss: 0.00093608
Iteration 21/25 | Loss: 0.00093608
Iteration 22/25 | Loss: 0.00093608
Iteration 23/25 | Loss: 0.00093608
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009360807598568499, 0.0009360807598568499, 0.0009360807598568499, 0.0009360807598568499, 0.0009360807598568499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009360807598568499

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093608
Iteration 2/1000 | Loss: 0.00004036
Iteration 3/1000 | Loss: 0.00002627
Iteration 4/1000 | Loss: 0.00002343
Iteration 5/1000 | Loss: 0.00002228
Iteration 6/1000 | Loss: 0.00002151
Iteration 7/1000 | Loss: 0.00002100
Iteration 8/1000 | Loss: 0.00002062
Iteration 9/1000 | Loss: 0.00002029
Iteration 10/1000 | Loss: 0.00002009
Iteration 11/1000 | Loss: 0.00001989
Iteration 12/1000 | Loss: 0.00001976
Iteration 13/1000 | Loss: 0.00001971
Iteration 14/1000 | Loss: 0.00001970
Iteration 15/1000 | Loss: 0.00001970
Iteration 16/1000 | Loss: 0.00001961
Iteration 17/1000 | Loss: 0.00001958
Iteration 18/1000 | Loss: 0.00001958
Iteration 19/1000 | Loss: 0.00001957
Iteration 20/1000 | Loss: 0.00001955
Iteration 21/1000 | Loss: 0.00001952
Iteration 22/1000 | Loss: 0.00001952
Iteration 23/1000 | Loss: 0.00001948
Iteration 24/1000 | Loss: 0.00001948
Iteration 25/1000 | Loss: 0.00001947
Iteration 26/1000 | Loss: 0.00001946
Iteration 27/1000 | Loss: 0.00001946
Iteration 28/1000 | Loss: 0.00001945
Iteration 29/1000 | Loss: 0.00001945
Iteration 30/1000 | Loss: 0.00001944
Iteration 31/1000 | Loss: 0.00001943
Iteration 32/1000 | Loss: 0.00001943
Iteration 33/1000 | Loss: 0.00001942
Iteration 34/1000 | Loss: 0.00001941
Iteration 35/1000 | Loss: 0.00001941
Iteration 36/1000 | Loss: 0.00001941
Iteration 37/1000 | Loss: 0.00001940
Iteration 38/1000 | Loss: 0.00001940
Iteration 39/1000 | Loss: 0.00001940
Iteration 40/1000 | Loss: 0.00001939
Iteration 41/1000 | Loss: 0.00001939
Iteration 42/1000 | Loss: 0.00001938
Iteration 43/1000 | Loss: 0.00001938
Iteration 44/1000 | Loss: 0.00001938
Iteration 45/1000 | Loss: 0.00001937
Iteration 46/1000 | Loss: 0.00001937
Iteration 47/1000 | Loss: 0.00001936
Iteration 48/1000 | Loss: 0.00001936
Iteration 49/1000 | Loss: 0.00001935
Iteration 50/1000 | Loss: 0.00001935
Iteration 51/1000 | Loss: 0.00001935
Iteration 52/1000 | Loss: 0.00001934
Iteration 53/1000 | Loss: 0.00001934
Iteration 54/1000 | Loss: 0.00001934
Iteration 55/1000 | Loss: 0.00001933
Iteration 56/1000 | Loss: 0.00001933
Iteration 57/1000 | Loss: 0.00001933
Iteration 58/1000 | Loss: 0.00001932
Iteration 59/1000 | Loss: 0.00001932
Iteration 60/1000 | Loss: 0.00001931
Iteration 61/1000 | Loss: 0.00001931
Iteration 62/1000 | Loss: 0.00001931
Iteration 63/1000 | Loss: 0.00001930
Iteration 64/1000 | Loss: 0.00001930
Iteration 65/1000 | Loss: 0.00001930
Iteration 66/1000 | Loss: 0.00001929
Iteration 67/1000 | Loss: 0.00001929
Iteration 68/1000 | Loss: 0.00001929
Iteration 69/1000 | Loss: 0.00001929
Iteration 70/1000 | Loss: 0.00001929
Iteration 71/1000 | Loss: 0.00001928
Iteration 72/1000 | Loss: 0.00001928
Iteration 73/1000 | Loss: 0.00001928
Iteration 74/1000 | Loss: 0.00001928
Iteration 75/1000 | Loss: 0.00001928
Iteration 76/1000 | Loss: 0.00001927
Iteration 77/1000 | Loss: 0.00001927
Iteration 78/1000 | Loss: 0.00001927
Iteration 79/1000 | Loss: 0.00001926
Iteration 80/1000 | Loss: 0.00001926
Iteration 81/1000 | Loss: 0.00001926
Iteration 82/1000 | Loss: 0.00001925
Iteration 83/1000 | Loss: 0.00001925
Iteration 84/1000 | Loss: 0.00001925
Iteration 85/1000 | Loss: 0.00001925
Iteration 86/1000 | Loss: 0.00001925
Iteration 87/1000 | Loss: 0.00001925
Iteration 88/1000 | Loss: 0.00001925
Iteration 89/1000 | Loss: 0.00001925
Iteration 90/1000 | Loss: 0.00001925
Iteration 91/1000 | Loss: 0.00001925
Iteration 92/1000 | Loss: 0.00001924
Iteration 93/1000 | Loss: 0.00001924
Iteration 94/1000 | Loss: 0.00001924
Iteration 95/1000 | Loss: 0.00001923
Iteration 96/1000 | Loss: 0.00001923
Iteration 97/1000 | Loss: 0.00001923
Iteration 98/1000 | Loss: 0.00001923
Iteration 99/1000 | Loss: 0.00001923
Iteration 100/1000 | Loss: 0.00001922
Iteration 101/1000 | Loss: 0.00001922
Iteration 102/1000 | Loss: 0.00001922
Iteration 103/1000 | Loss: 0.00001922
Iteration 104/1000 | Loss: 0.00001922
Iteration 105/1000 | Loss: 0.00001922
Iteration 106/1000 | Loss: 0.00001922
Iteration 107/1000 | Loss: 0.00001921
Iteration 108/1000 | Loss: 0.00001921
Iteration 109/1000 | Loss: 0.00001921
Iteration 110/1000 | Loss: 0.00001921
Iteration 111/1000 | Loss: 0.00001921
Iteration 112/1000 | Loss: 0.00001921
Iteration 113/1000 | Loss: 0.00001921
Iteration 114/1000 | Loss: 0.00001921
Iteration 115/1000 | Loss: 0.00001921
Iteration 116/1000 | Loss: 0.00001921
Iteration 117/1000 | Loss: 0.00001921
Iteration 118/1000 | Loss: 0.00001921
Iteration 119/1000 | Loss: 0.00001920
Iteration 120/1000 | Loss: 0.00001920
Iteration 121/1000 | Loss: 0.00001920
Iteration 122/1000 | Loss: 0.00001920
Iteration 123/1000 | Loss: 0.00001920
Iteration 124/1000 | Loss: 0.00001920
Iteration 125/1000 | Loss: 0.00001920
Iteration 126/1000 | Loss: 0.00001920
Iteration 127/1000 | Loss: 0.00001920
Iteration 128/1000 | Loss: 0.00001920
Iteration 129/1000 | Loss: 0.00001920
Iteration 130/1000 | Loss: 0.00001919
Iteration 131/1000 | Loss: 0.00001919
Iteration 132/1000 | Loss: 0.00001919
Iteration 133/1000 | Loss: 0.00001919
Iteration 134/1000 | Loss: 0.00001919
Iteration 135/1000 | Loss: 0.00001919
Iteration 136/1000 | Loss: 0.00001919
Iteration 137/1000 | Loss: 0.00001919
Iteration 138/1000 | Loss: 0.00001918
Iteration 139/1000 | Loss: 0.00001918
Iteration 140/1000 | Loss: 0.00001918
Iteration 141/1000 | Loss: 0.00001918
Iteration 142/1000 | Loss: 0.00001918
Iteration 143/1000 | Loss: 0.00001918
Iteration 144/1000 | Loss: 0.00001918
Iteration 145/1000 | Loss: 0.00001918
Iteration 146/1000 | Loss: 0.00001918
Iteration 147/1000 | Loss: 0.00001918
Iteration 148/1000 | Loss: 0.00001918
Iteration 149/1000 | Loss: 0.00001918
Iteration 150/1000 | Loss: 0.00001918
Iteration 151/1000 | Loss: 0.00001918
Iteration 152/1000 | Loss: 0.00001918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.918018278956879e-05, 1.918018278956879e-05, 1.918018278956879e-05, 1.918018278956879e-05, 1.918018278956879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.918018278956879e-05

Optimization complete. Final v2v error: 3.664677619934082 mm

Highest mean error: 5.370268821716309 mm for frame 187

Lowest mean error: 3.205300807952881 mm for frame 147

Saving results

Total time: 43.86197590827942
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823720
Iteration 2/25 | Loss: 0.00142993
Iteration 3/25 | Loss: 0.00128331
Iteration 4/25 | Loss: 0.00126446
Iteration 5/25 | Loss: 0.00126262
Iteration 6/25 | Loss: 0.00126458
Iteration 7/25 | Loss: 0.00126213
Iteration 8/25 | Loss: 0.00126309
Iteration 9/25 | Loss: 0.00126159
Iteration 10/25 | Loss: 0.00126109
Iteration 11/25 | Loss: 0.00125920
Iteration 12/25 | Loss: 0.00125996
Iteration 13/25 | Loss: 0.00125856
Iteration 14/25 | Loss: 0.00125719
Iteration 15/25 | Loss: 0.00125678
Iteration 16/25 | Loss: 0.00125696
Iteration 17/25 | Loss: 0.00125705
Iteration 18/25 | Loss: 0.00125762
Iteration 19/25 | Loss: 0.00125625
Iteration 20/25 | Loss: 0.00125554
Iteration 21/25 | Loss: 0.00125383
Iteration 22/25 | Loss: 0.00125456
Iteration 23/25 | Loss: 0.00125416
Iteration 24/25 | Loss: 0.00125330
Iteration 25/25 | Loss: 0.00125243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.63825464
Iteration 2/25 | Loss: 0.00097849
Iteration 3/25 | Loss: 0.00097847
Iteration 4/25 | Loss: 0.00097847
Iteration 5/25 | Loss: 0.00097846
Iteration 6/25 | Loss: 0.00097846
Iteration 7/25 | Loss: 0.00097846
Iteration 8/25 | Loss: 0.00097846
Iteration 9/25 | Loss: 0.00097846
Iteration 10/25 | Loss: 0.00097846
Iteration 11/25 | Loss: 0.00097846
Iteration 12/25 | Loss: 0.00097846
Iteration 13/25 | Loss: 0.00097846
Iteration 14/25 | Loss: 0.00097846
Iteration 15/25 | Loss: 0.00097846
Iteration 16/25 | Loss: 0.00097846
Iteration 17/25 | Loss: 0.00097846
Iteration 18/25 | Loss: 0.00097846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009784626308828592, 0.0009784626308828592, 0.0009784626308828592, 0.0009784626308828592, 0.0009784626308828592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009784626308828592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097846
Iteration 2/1000 | Loss: 0.00003799
Iteration 3/1000 | Loss: 0.00002599
Iteration 4/1000 | Loss: 0.00002280
Iteration 5/1000 | Loss: 0.00002150
Iteration 6/1000 | Loss: 0.00002033
Iteration 7/1000 | Loss: 0.00001956
Iteration 8/1000 | Loss: 0.00001897
Iteration 9/1000 | Loss: 0.00001859
Iteration 10/1000 | Loss: 0.00001821
Iteration 11/1000 | Loss: 0.00001788
Iteration 12/1000 | Loss: 0.00001777
Iteration 13/1000 | Loss: 0.00032518
Iteration 14/1000 | Loss: 0.00001933
Iteration 15/1000 | Loss: 0.00001782
Iteration 16/1000 | Loss: 0.00001720
Iteration 17/1000 | Loss: 0.00001670
Iteration 18/1000 | Loss: 0.00001627
Iteration 19/1000 | Loss: 0.00001612
Iteration 20/1000 | Loss: 0.00001607
Iteration 21/1000 | Loss: 0.00001607
Iteration 22/1000 | Loss: 0.00001606
Iteration 23/1000 | Loss: 0.00001606
Iteration 24/1000 | Loss: 0.00001605
Iteration 25/1000 | Loss: 0.00001605
Iteration 26/1000 | Loss: 0.00001604
Iteration 27/1000 | Loss: 0.00001604
Iteration 28/1000 | Loss: 0.00001603
Iteration 29/1000 | Loss: 0.00001603
Iteration 30/1000 | Loss: 0.00001603
Iteration 31/1000 | Loss: 0.00001602
Iteration 32/1000 | Loss: 0.00001600
Iteration 33/1000 | Loss: 0.00001598
Iteration 34/1000 | Loss: 0.00001598
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001595
Iteration 37/1000 | Loss: 0.00001595
Iteration 38/1000 | Loss: 0.00001595
Iteration 39/1000 | Loss: 0.00001594
Iteration 40/1000 | Loss: 0.00001594
Iteration 41/1000 | Loss: 0.00001594
Iteration 42/1000 | Loss: 0.00001593
Iteration 43/1000 | Loss: 0.00001593
Iteration 44/1000 | Loss: 0.00001593
Iteration 45/1000 | Loss: 0.00001587
Iteration 46/1000 | Loss: 0.00001580
Iteration 47/1000 | Loss: 0.00001580
Iteration 48/1000 | Loss: 0.00001579
Iteration 49/1000 | Loss: 0.00001579
Iteration 50/1000 | Loss: 0.00001578
Iteration 51/1000 | Loss: 0.00001577
Iteration 52/1000 | Loss: 0.00001577
Iteration 53/1000 | Loss: 0.00001574
Iteration 54/1000 | Loss: 0.00001572
Iteration 55/1000 | Loss: 0.00001571
Iteration 56/1000 | Loss: 0.00001569
Iteration 57/1000 | Loss: 0.00001568
Iteration 58/1000 | Loss: 0.00001568
Iteration 59/1000 | Loss: 0.00001567
Iteration 60/1000 | Loss: 0.00001567
Iteration 61/1000 | Loss: 0.00001566
Iteration 62/1000 | Loss: 0.00001566
Iteration 63/1000 | Loss: 0.00001565
Iteration 64/1000 | Loss: 0.00001564
Iteration 65/1000 | Loss: 0.00001564
Iteration 66/1000 | Loss: 0.00001563
Iteration 67/1000 | Loss: 0.00001563
Iteration 68/1000 | Loss: 0.00001562
Iteration 69/1000 | Loss: 0.00001560
Iteration 70/1000 | Loss: 0.00001560
Iteration 71/1000 | Loss: 0.00001560
Iteration 72/1000 | Loss: 0.00001560
Iteration 73/1000 | Loss: 0.00001560
Iteration 74/1000 | Loss: 0.00001560
Iteration 75/1000 | Loss: 0.00001560
Iteration 76/1000 | Loss: 0.00001559
Iteration 77/1000 | Loss: 0.00001559
Iteration 78/1000 | Loss: 0.00001559
Iteration 79/1000 | Loss: 0.00001559
Iteration 80/1000 | Loss: 0.00001558
Iteration 81/1000 | Loss: 0.00001558
Iteration 82/1000 | Loss: 0.00001558
Iteration 83/1000 | Loss: 0.00001558
Iteration 84/1000 | Loss: 0.00001557
Iteration 85/1000 | Loss: 0.00001557
Iteration 86/1000 | Loss: 0.00001557
Iteration 87/1000 | Loss: 0.00001557
Iteration 88/1000 | Loss: 0.00001557
Iteration 89/1000 | Loss: 0.00001557
Iteration 90/1000 | Loss: 0.00001556
Iteration 91/1000 | Loss: 0.00001556
Iteration 92/1000 | Loss: 0.00001556
Iteration 93/1000 | Loss: 0.00001555
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001555
Iteration 96/1000 | Loss: 0.00001554
Iteration 97/1000 | Loss: 0.00001554
Iteration 98/1000 | Loss: 0.00001554
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001553
Iteration 102/1000 | Loss: 0.00001553
Iteration 103/1000 | Loss: 0.00001553
Iteration 104/1000 | Loss: 0.00001553
Iteration 105/1000 | Loss: 0.00001552
Iteration 106/1000 | Loss: 0.00001552
Iteration 107/1000 | Loss: 0.00001552
Iteration 108/1000 | Loss: 0.00001552
Iteration 109/1000 | Loss: 0.00001552
Iteration 110/1000 | Loss: 0.00001552
Iteration 111/1000 | Loss: 0.00001551
Iteration 112/1000 | Loss: 0.00001551
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001550
Iteration 115/1000 | Loss: 0.00001550
Iteration 116/1000 | Loss: 0.00001550
Iteration 117/1000 | Loss: 0.00001550
Iteration 118/1000 | Loss: 0.00001550
Iteration 119/1000 | Loss: 0.00001550
Iteration 120/1000 | Loss: 0.00001549
Iteration 121/1000 | Loss: 0.00001549
Iteration 122/1000 | Loss: 0.00001549
Iteration 123/1000 | Loss: 0.00001549
Iteration 124/1000 | Loss: 0.00001549
Iteration 125/1000 | Loss: 0.00001549
Iteration 126/1000 | Loss: 0.00001549
Iteration 127/1000 | Loss: 0.00001549
Iteration 128/1000 | Loss: 0.00001549
Iteration 129/1000 | Loss: 0.00001549
Iteration 130/1000 | Loss: 0.00001549
Iteration 131/1000 | Loss: 0.00001549
Iteration 132/1000 | Loss: 0.00001549
Iteration 133/1000 | Loss: 0.00001549
Iteration 134/1000 | Loss: 0.00001548
Iteration 135/1000 | Loss: 0.00001548
Iteration 136/1000 | Loss: 0.00001548
Iteration 137/1000 | Loss: 0.00001548
Iteration 138/1000 | Loss: 0.00001548
Iteration 139/1000 | Loss: 0.00001548
Iteration 140/1000 | Loss: 0.00001548
Iteration 141/1000 | Loss: 0.00001548
Iteration 142/1000 | Loss: 0.00001548
Iteration 143/1000 | Loss: 0.00001548
Iteration 144/1000 | Loss: 0.00001547
Iteration 145/1000 | Loss: 0.00001547
Iteration 146/1000 | Loss: 0.00001547
Iteration 147/1000 | Loss: 0.00001547
Iteration 148/1000 | Loss: 0.00001547
Iteration 149/1000 | Loss: 0.00001547
Iteration 150/1000 | Loss: 0.00001547
Iteration 151/1000 | Loss: 0.00001547
Iteration 152/1000 | Loss: 0.00001547
Iteration 153/1000 | Loss: 0.00001547
Iteration 154/1000 | Loss: 0.00001547
Iteration 155/1000 | Loss: 0.00001547
Iteration 156/1000 | Loss: 0.00001547
Iteration 157/1000 | Loss: 0.00001547
Iteration 158/1000 | Loss: 0.00001547
Iteration 159/1000 | Loss: 0.00001547
Iteration 160/1000 | Loss: 0.00001547
Iteration 161/1000 | Loss: 0.00001547
Iteration 162/1000 | Loss: 0.00001547
Iteration 163/1000 | Loss: 0.00001547
Iteration 164/1000 | Loss: 0.00001547
Iteration 165/1000 | Loss: 0.00001547
Iteration 166/1000 | Loss: 0.00001547
Iteration 167/1000 | Loss: 0.00001547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.5466921468032524e-05, 1.5466921468032524e-05, 1.5466921468032524e-05, 1.5466921468032524e-05, 1.5466921468032524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5466921468032524e-05

Optimization complete. Final v2v error: 3.3158295154571533 mm

Highest mean error: 4.417703628540039 mm for frame 182

Lowest mean error: 2.8817293643951416 mm for frame 128

Saving results

Total time: 98.74972891807556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849359
Iteration 2/25 | Loss: 0.00250317
Iteration 3/25 | Loss: 0.00182035
Iteration 4/25 | Loss: 0.00170191
Iteration 5/25 | Loss: 0.00154994
Iteration 6/25 | Loss: 0.00148603
Iteration 7/25 | Loss: 0.00144191
Iteration 8/25 | Loss: 0.00142120
Iteration 9/25 | Loss: 0.00140667
Iteration 10/25 | Loss: 0.00139600
Iteration 11/25 | Loss: 0.00138834
Iteration 12/25 | Loss: 0.00138266
Iteration 13/25 | Loss: 0.00138157
Iteration 14/25 | Loss: 0.00138118
Iteration 15/25 | Loss: 0.00138107
Iteration 16/25 | Loss: 0.00138106
Iteration 17/25 | Loss: 0.00138106
Iteration 18/25 | Loss: 0.00138105
Iteration 19/25 | Loss: 0.00138105
Iteration 20/25 | Loss: 0.00138105
Iteration 21/25 | Loss: 0.00138105
Iteration 22/25 | Loss: 0.00138105
Iteration 23/25 | Loss: 0.00138105
Iteration 24/25 | Loss: 0.00138105
Iteration 25/25 | Loss: 0.00138105

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93936974
Iteration 2/25 | Loss: 0.00100039
Iteration 3/25 | Loss: 0.00100039
Iteration 4/25 | Loss: 0.00100038
Iteration 5/25 | Loss: 0.00100038
Iteration 6/25 | Loss: 0.00100038
Iteration 7/25 | Loss: 0.00100038
Iteration 8/25 | Loss: 0.00100038
Iteration 9/25 | Loss: 0.00100038
Iteration 10/25 | Loss: 0.00100038
Iteration 11/25 | Loss: 0.00100038
Iteration 12/25 | Loss: 0.00100038
Iteration 13/25 | Loss: 0.00100038
Iteration 14/25 | Loss: 0.00100038
Iteration 15/25 | Loss: 0.00100038
Iteration 16/25 | Loss: 0.00100038
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010003828210756183, 0.0010003828210756183, 0.0010003828210756183, 0.0010003828210756183, 0.0010003828210756183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010003828210756183

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100038
Iteration 2/1000 | Loss: 0.00005146
Iteration 3/1000 | Loss: 0.00003083
Iteration 4/1000 | Loss: 0.00002783
Iteration 5/1000 | Loss: 0.00002664
Iteration 6/1000 | Loss: 0.00002565
Iteration 7/1000 | Loss: 0.00002515
Iteration 8/1000 | Loss: 0.00002472
Iteration 9/1000 | Loss: 0.00002443
Iteration 10/1000 | Loss: 0.00002412
Iteration 11/1000 | Loss: 0.00002390
Iteration 12/1000 | Loss: 0.00002375
Iteration 13/1000 | Loss: 0.00002373
Iteration 14/1000 | Loss: 0.00002367
Iteration 15/1000 | Loss: 0.00002362
Iteration 16/1000 | Loss: 0.00002348
Iteration 17/1000 | Loss: 0.00002340
Iteration 18/1000 | Loss: 0.00002338
Iteration 19/1000 | Loss: 0.00002336
Iteration 20/1000 | Loss: 0.00002336
Iteration 21/1000 | Loss: 0.00002333
Iteration 22/1000 | Loss: 0.00002323
Iteration 23/1000 | Loss: 0.00002323
Iteration 24/1000 | Loss: 0.00002322
Iteration 25/1000 | Loss: 0.00002322
Iteration 26/1000 | Loss: 0.00002322
Iteration 27/1000 | Loss: 0.00002321
Iteration 28/1000 | Loss: 0.00002321
Iteration 29/1000 | Loss: 0.00002320
Iteration 30/1000 | Loss: 0.00002320
Iteration 31/1000 | Loss: 0.00002320
Iteration 32/1000 | Loss: 0.00002320
Iteration 33/1000 | Loss: 0.00002320
Iteration 34/1000 | Loss: 0.00002319
Iteration 35/1000 | Loss: 0.00002319
Iteration 36/1000 | Loss: 0.00002319
Iteration 37/1000 | Loss: 0.00002318
Iteration 38/1000 | Loss: 0.00002318
Iteration 39/1000 | Loss: 0.00002318
Iteration 40/1000 | Loss: 0.00002317
Iteration 41/1000 | Loss: 0.00002317
Iteration 42/1000 | Loss: 0.00002316
Iteration 43/1000 | Loss: 0.00002314
Iteration 44/1000 | Loss: 0.00002314
Iteration 45/1000 | Loss: 0.00002314
Iteration 46/1000 | Loss: 0.00002313
Iteration 47/1000 | Loss: 0.00002313
Iteration 48/1000 | Loss: 0.00002311
Iteration 49/1000 | Loss: 0.00002311
Iteration 50/1000 | Loss: 0.00002310
Iteration 51/1000 | Loss: 0.00002310
Iteration 52/1000 | Loss: 0.00002310
Iteration 53/1000 | Loss: 0.00002310
Iteration 54/1000 | Loss: 0.00002310
Iteration 55/1000 | Loss: 0.00002310
Iteration 56/1000 | Loss: 0.00002310
Iteration 57/1000 | Loss: 0.00002310
Iteration 58/1000 | Loss: 0.00002310
Iteration 59/1000 | Loss: 0.00002310
Iteration 60/1000 | Loss: 0.00002310
Iteration 61/1000 | Loss: 0.00002310
Iteration 62/1000 | Loss: 0.00002310
Iteration 63/1000 | Loss: 0.00002310
Iteration 64/1000 | Loss: 0.00002310
Iteration 65/1000 | Loss: 0.00002310
Iteration 66/1000 | Loss: 0.00002310
Iteration 67/1000 | Loss: 0.00002310
Iteration 68/1000 | Loss: 0.00002310
Iteration 69/1000 | Loss: 0.00002310
Iteration 70/1000 | Loss: 0.00002310
Iteration 71/1000 | Loss: 0.00002310
Iteration 72/1000 | Loss: 0.00002310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [2.3104774300009012e-05, 2.3104774300009012e-05, 2.3104774300009012e-05, 2.3104774300009012e-05, 2.3104774300009012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3104774300009012e-05

Optimization complete. Final v2v error: 3.991759777069092 mm

Highest mean error: 4.935492992401123 mm for frame 141

Lowest mean error: 3.4709465503692627 mm for frame 63

Saving results

Total time: 59.780038833618164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991614
Iteration 2/25 | Loss: 0.00253858
Iteration 3/25 | Loss: 0.00169147
Iteration 4/25 | Loss: 0.00151248
Iteration 5/25 | Loss: 0.00152628
Iteration 6/25 | Loss: 0.00149873
Iteration 7/25 | Loss: 0.00143602
Iteration 8/25 | Loss: 0.00141383
Iteration 9/25 | Loss: 0.00134629
Iteration 10/25 | Loss: 0.00132786
Iteration 11/25 | Loss: 0.00129778
Iteration 12/25 | Loss: 0.00129787
Iteration 13/25 | Loss: 0.00129418
Iteration 14/25 | Loss: 0.00129916
Iteration 15/25 | Loss: 0.00129499
Iteration 16/25 | Loss: 0.00129851
Iteration 17/25 | Loss: 0.00129094
Iteration 18/25 | Loss: 0.00128692
Iteration 19/25 | Loss: 0.00128626
Iteration 20/25 | Loss: 0.00128628
Iteration 21/25 | Loss: 0.00128992
Iteration 22/25 | Loss: 0.00128205
Iteration 23/25 | Loss: 0.00128071
Iteration 24/25 | Loss: 0.00128095
Iteration 25/25 | Loss: 0.00128025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37873423
Iteration 2/25 | Loss: 0.00139064
Iteration 3/25 | Loss: 0.00115360
Iteration 4/25 | Loss: 0.00115360
Iteration 5/25 | Loss: 0.00115360
Iteration 6/25 | Loss: 0.00115360
Iteration 7/25 | Loss: 0.00115360
Iteration 8/25 | Loss: 0.00115360
Iteration 9/25 | Loss: 0.00115360
Iteration 10/25 | Loss: 0.00115360
Iteration 11/25 | Loss: 0.00115360
Iteration 12/25 | Loss: 0.00115360
Iteration 13/25 | Loss: 0.00115360
Iteration 14/25 | Loss: 0.00115360
Iteration 15/25 | Loss: 0.00115360
Iteration 16/25 | Loss: 0.00115360
Iteration 17/25 | Loss: 0.00115360
Iteration 18/25 | Loss: 0.00115360
Iteration 19/25 | Loss: 0.00115360
Iteration 20/25 | Loss: 0.00115360
Iteration 21/25 | Loss: 0.00115360
Iteration 22/25 | Loss: 0.00115360
Iteration 23/25 | Loss: 0.00115360
Iteration 24/25 | Loss: 0.00115360
Iteration 25/25 | Loss: 0.00115360

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115360
Iteration 2/1000 | Loss: 0.00065323
Iteration 3/1000 | Loss: 0.00003536
Iteration 4/1000 | Loss: 0.00020413
Iteration 5/1000 | Loss: 0.00003203
Iteration 6/1000 | Loss: 0.00002386
Iteration 7/1000 | Loss: 0.00001943
Iteration 8/1000 | Loss: 0.00023631
Iteration 9/1000 | Loss: 0.00002210
Iteration 10/1000 | Loss: 0.00001789
Iteration 11/1000 | Loss: 0.00001671
Iteration 12/1000 | Loss: 0.00001613
Iteration 13/1000 | Loss: 0.00001572
Iteration 14/1000 | Loss: 0.00001540
Iteration 15/1000 | Loss: 0.00001521
Iteration 16/1000 | Loss: 0.00001503
Iteration 17/1000 | Loss: 0.00001485
Iteration 18/1000 | Loss: 0.00001484
Iteration 19/1000 | Loss: 0.00001483
Iteration 20/1000 | Loss: 0.00001482
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001474
Iteration 23/1000 | Loss: 0.00001473
Iteration 24/1000 | Loss: 0.00001472
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001471
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001470
Iteration 29/1000 | Loss: 0.00001470
Iteration 30/1000 | Loss: 0.00001469
Iteration 31/1000 | Loss: 0.00001462
Iteration 32/1000 | Loss: 0.00001461
Iteration 33/1000 | Loss: 0.00001460
Iteration 34/1000 | Loss: 0.00001460
Iteration 35/1000 | Loss: 0.00001459
Iteration 36/1000 | Loss: 0.00001459
Iteration 37/1000 | Loss: 0.00001459
Iteration 38/1000 | Loss: 0.00001459
Iteration 39/1000 | Loss: 0.00001459
Iteration 40/1000 | Loss: 0.00001459
Iteration 41/1000 | Loss: 0.00001458
Iteration 42/1000 | Loss: 0.00001458
Iteration 43/1000 | Loss: 0.00001458
Iteration 44/1000 | Loss: 0.00001458
Iteration 45/1000 | Loss: 0.00001458
Iteration 46/1000 | Loss: 0.00001458
Iteration 47/1000 | Loss: 0.00001458
Iteration 48/1000 | Loss: 0.00001458
Iteration 49/1000 | Loss: 0.00001458
Iteration 50/1000 | Loss: 0.00001458
Iteration 51/1000 | Loss: 0.00001458
Iteration 52/1000 | Loss: 0.00001458
Iteration 53/1000 | Loss: 0.00001458
Iteration 54/1000 | Loss: 0.00001458
Iteration 55/1000 | Loss: 0.00001458
Iteration 56/1000 | Loss: 0.00001458
Iteration 57/1000 | Loss: 0.00001458
Iteration 58/1000 | Loss: 0.00001458
Iteration 59/1000 | Loss: 0.00001458
Iteration 60/1000 | Loss: 0.00001458
Iteration 61/1000 | Loss: 0.00001458
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001456
Iteration 64/1000 | Loss: 0.00001456
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001456
Iteration 67/1000 | Loss: 0.00001455
Iteration 68/1000 | Loss: 0.00001455
Iteration 69/1000 | Loss: 0.00001455
Iteration 70/1000 | Loss: 0.00001455
Iteration 71/1000 | Loss: 0.00001455
Iteration 72/1000 | Loss: 0.00001454
Iteration 73/1000 | Loss: 0.00001454
Iteration 74/1000 | Loss: 0.00001453
Iteration 75/1000 | Loss: 0.00001453
Iteration 76/1000 | Loss: 0.00001453
Iteration 77/1000 | Loss: 0.00001453
Iteration 78/1000 | Loss: 0.00015951
Iteration 79/1000 | Loss: 0.00002795
Iteration 80/1000 | Loss: 0.00001935
Iteration 81/1000 | Loss: 0.00001548
Iteration 82/1000 | Loss: 0.00001603
Iteration 83/1000 | Loss: 0.00001447
Iteration 84/1000 | Loss: 0.00001447
Iteration 85/1000 | Loss: 0.00001447
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00001446
Iteration 91/1000 | Loss: 0.00001446
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001446
Iteration 95/1000 | Loss: 0.00001446
Iteration 96/1000 | Loss: 0.00001446
Iteration 97/1000 | Loss: 0.00001446
Iteration 98/1000 | Loss: 0.00001446
Iteration 99/1000 | Loss: 0.00001445
Iteration 100/1000 | Loss: 0.00001445
Iteration 101/1000 | Loss: 0.00001445
Iteration 102/1000 | Loss: 0.00001445
Iteration 103/1000 | Loss: 0.00001444
Iteration 104/1000 | Loss: 0.00001444
Iteration 105/1000 | Loss: 0.00001444
Iteration 106/1000 | Loss: 0.00001443
Iteration 107/1000 | Loss: 0.00001443
Iteration 108/1000 | Loss: 0.00001443
Iteration 109/1000 | Loss: 0.00001443
Iteration 110/1000 | Loss: 0.00001443
Iteration 111/1000 | Loss: 0.00001443
Iteration 112/1000 | Loss: 0.00001443
Iteration 113/1000 | Loss: 0.00001442
Iteration 114/1000 | Loss: 0.00001442
Iteration 115/1000 | Loss: 0.00001442
Iteration 116/1000 | Loss: 0.00001442
Iteration 117/1000 | Loss: 0.00001442
Iteration 118/1000 | Loss: 0.00001442
Iteration 119/1000 | Loss: 0.00001442
Iteration 120/1000 | Loss: 0.00001442
Iteration 121/1000 | Loss: 0.00001441
Iteration 122/1000 | Loss: 0.00001441
Iteration 123/1000 | Loss: 0.00001441
Iteration 124/1000 | Loss: 0.00001441
Iteration 125/1000 | Loss: 0.00001441
Iteration 126/1000 | Loss: 0.00001441
Iteration 127/1000 | Loss: 0.00001441
Iteration 128/1000 | Loss: 0.00001441
Iteration 129/1000 | Loss: 0.00001441
Iteration 130/1000 | Loss: 0.00001441
Iteration 131/1000 | Loss: 0.00001441
Iteration 132/1000 | Loss: 0.00001441
Iteration 133/1000 | Loss: 0.00001441
Iteration 134/1000 | Loss: 0.00001441
Iteration 135/1000 | Loss: 0.00001441
Iteration 136/1000 | Loss: 0.00001440
Iteration 137/1000 | Loss: 0.00001440
Iteration 138/1000 | Loss: 0.00001440
Iteration 139/1000 | Loss: 0.00001440
Iteration 140/1000 | Loss: 0.00001440
Iteration 141/1000 | Loss: 0.00001440
Iteration 142/1000 | Loss: 0.00001440
Iteration 143/1000 | Loss: 0.00001440
Iteration 144/1000 | Loss: 0.00001440
Iteration 145/1000 | Loss: 0.00001440
Iteration 146/1000 | Loss: 0.00001440
Iteration 147/1000 | Loss: 0.00001440
Iteration 148/1000 | Loss: 0.00001440
Iteration 149/1000 | Loss: 0.00001439
Iteration 150/1000 | Loss: 0.00001439
Iteration 151/1000 | Loss: 0.00001439
Iteration 152/1000 | Loss: 0.00001439
Iteration 153/1000 | Loss: 0.00001439
Iteration 154/1000 | Loss: 0.00001439
Iteration 155/1000 | Loss: 0.00001439
Iteration 156/1000 | Loss: 0.00001439
Iteration 157/1000 | Loss: 0.00001439
Iteration 158/1000 | Loss: 0.00001438
Iteration 159/1000 | Loss: 0.00001438
Iteration 160/1000 | Loss: 0.00001438
Iteration 161/1000 | Loss: 0.00001438
Iteration 162/1000 | Loss: 0.00001438
Iteration 163/1000 | Loss: 0.00001438
Iteration 164/1000 | Loss: 0.00001437
Iteration 165/1000 | Loss: 0.00001437
Iteration 166/1000 | Loss: 0.00001437
Iteration 167/1000 | Loss: 0.00001437
Iteration 168/1000 | Loss: 0.00001436
Iteration 169/1000 | Loss: 0.00001436
Iteration 170/1000 | Loss: 0.00001436
Iteration 171/1000 | Loss: 0.00001436
Iteration 172/1000 | Loss: 0.00001436
Iteration 173/1000 | Loss: 0.00001436
Iteration 174/1000 | Loss: 0.00001436
Iteration 175/1000 | Loss: 0.00001436
Iteration 176/1000 | Loss: 0.00001436
Iteration 177/1000 | Loss: 0.00001435
Iteration 178/1000 | Loss: 0.00001435
Iteration 179/1000 | Loss: 0.00001435
Iteration 180/1000 | Loss: 0.00001435
Iteration 181/1000 | Loss: 0.00001435
Iteration 182/1000 | Loss: 0.00001435
Iteration 183/1000 | Loss: 0.00001435
Iteration 184/1000 | Loss: 0.00001435
Iteration 185/1000 | Loss: 0.00001435
Iteration 186/1000 | Loss: 0.00001435
Iteration 187/1000 | Loss: 0.00001435
Iteration 188/1000 | Loss: 0.00001435
Iteration 189/1000 | Loss: 0.00001435
Iteration 190/1000 | Loss: 0.00001434
Iteration 191/1000 | Loss: 0.00001434
Iteration 192/1000 | Loss: 0.00001434
Iteration 193/1000 | Loss: 0.00001434
Iteration 194/1000 | Loss: 0.00001434
Iteration 195/1000 | Loss: 0.00001434
Iteration 196/1000 | Loss: 0.00001434
Iteration 197/1000 | Loss: 0.00001434
Iteration 198/1000 | Loss: 0.00001434
Iteration 199/1000 | Loss: 0.00001434
Iteration 200/1000 | Loss: 0.00001434
Iteration 201/1000 | Loss: 0.00001434
Iteration 202/1000 | Loss: 0.00001434
Iteration 203/1000 | Loss: 0.00001434
Iteration 204/1000 | Loss: 0.00001434
Iteration 205/1000 | Loss: 0.00001434
Iteration 206/1000 | Loss: 0.00001434
Iteration 207/1000 | Loss: 0.00001434
Iteration 208/1000 | Loss: 0.00001434
Iteration 209/1000 | Loss: 0.00001434
Iteration 210/1000 | Loss: 0.00001434
Iteration 211/1000 | Loss: 0.00001434
Iteration 212/1000 | Loss: 0.00001434
Iteration 213/1000 | Loss: 0.00001434
Iteration 214/1000 | Loss: 0.00001434
Iteration 215/1000 | Loss: 0.00001434
Iteration 216/1000 | Loss: 0.00001434
Iteration 217/1000 | Loss: 0.00001434
Iteration 218/1000 | Loss: 0.00001434
Iteration 219/1000 | Loss: 0.00001434
Iteration 220/1000 | Loss: 0.00001434
Iteration 221/1000 | Loss: 0.00001434
Iteration 222/1000 | Loss: 0.00001434
Iteration 223/1000 | Loss: 0.00001434
Iteration 224/1000 | Loss: 0.00001434
Iteration 225/1000 | Loss: 0.00001434
Iteration 226/1000 | Loss: 0.00001434
Iteration 227/1000 | Loss: 0.00001434
Iteration 228/1000 | Loss: 0.00001434
Iteration 229/1000 | Loss: 0.00001434
Iteration 230/1000 | Loss: 0.00001434
Iteration 231/1000 | Loss: 0.00001434
Iteration 232/1000 | Loss: 0.00001434
Iteration 233/1000 | Loss: 0.00001434
Iteration 234/1000 | Loss: 0.00001434
Iteration 235/1000 | Loss: 0.00001434
Iteration 236/1000 | Loss: 0.00001434
Iteration 237/1000 | Loss: 0.00001434
Iteration 238/1000 | Loss: 0.00001434
Iteration 239/1000 | Loss: 0.00001434
Iteration 240/1000 | Loss: 0.00001434
Iteration 241/1000 | Loss: 0.00001434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.4344500414154027e-05, 1.4344500414154027e-05, 1.4344500414154027e-05, 1.4344500414154027e-05, 1.4344500414154027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4344500414154027e-05

Optimization complete. Final v2v error: 3.170759439468384 mm

Highest mean error: 4.129705429077148 mm for frame 69

Lowest mean error: 2.7407398223876953 mm for frame 118

Saving results

Total time: 89.01291704177856
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917606
Iteration 2/25 | Loss: 0.00186577
Iteration 3/25 | Loss: 0.00152708
Iteration 4/25 | Loss: 0.00148967
Iteration 5/25 | Loss: 0.00148298
Iteration 6/25 | Loss: 0.00148161
Iteration 7/25 | Loss: 0.00148161
Iteration 8/25 | Loss: 0.00148161
Iteration 9/25 | Loss: 0.00148161
Iteration 10/25 | Loss: 0.00148161
Iteration 11/25 | Loss: 0.00148161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014816136099398136, 0.0014816136099398136, 0.0014816136099398136, 0.0014816136099398136, 0.0014816136099398136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014816136099398136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74561536
Iteration 2/25 | Loss: 0.00082377
Iteration 3/25 | Loss: 0.00082377
Iteration 4/25 | Loss: 0.00082377
Iteration 5/25 | Loss: 0.00082377
Iteration 6/25 | Loss: 0.00082377
Iteration 7/25 | Loss: 0.00082377
Iteration 8/25 | Loss: 0.00082376
Iteration 9/25 | Loss: 0.00082376
Iteration 10/25 | Loss: 0.00082376
Iteration 11/25 | Loss: 0.00082376
Iteration 12/25 | Loss: 0.00082376
Iteration 13/25 | Loss: 0.00082376
Iteration 14/25 | Loss: 0.00082376
Iteration 15/25 | Loss: 0.00082376
Iteration 16/25 | Loss: 0.00082376
Iteration 17/25 | Loss: 0.00082376
Iteration 18/25 | Loss: 0.00082376
Iteration 19/25 | Loss: 0.00082376
Iteration 20/25 | Loss: 0.00082376
Iteration 21/25 | Loss: 0.00082376
Iteration 22/25 | Loss: 0.00082376
Iteration 23/25 | Loss: 0.00082376
Iteration 24/25 | Loss: 0.00082376
Iteration 25/25 | Loss: 0.00082376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082376
Iteration 2/1000 | Loss: 0.00006311
Iteration 3/1000 | Loss: 0.00004253
Iteration 4/1000 | Loss: 0.00003839
Iteration 5/1000 | Loss: 0.00003714
Iteration 6/1000 | Loss: 0.00003620
Iteration 7/1000 | Loss: 0.00003566
Iteration 8/1000 | Loss: 0.00003526
Iteration 9/1000 | Loss: 0.00003505
Iteration 10/1000 | Loss: 0.00003482
Iteration 11/1000 | Loss: 0.00003468
Iteration 12/1000 | Loss: 0.00003449
Iteration 13/1000 | Loss: 0.00003433
Iteration 14/1000 | Loss: 0.00003423
Iteration 15/1000 | Loss: 0.00003417
Iteration 16/1000 | Loss: 0.00003408
Iteration 17/1000 | Loss: 0.00003407
Iteration 18/1000 | Loss: 0.00003406
Iteration 19/1000 | Loss: 0.00003406
Iteration 20/1000 | Loss: 0.00003406
Iteration 21/1000 | Loss: 0.00003406
Iteration 22/1000 | Loss: 0.00003405
Iteration 23/1000 | Loss: 0.00003405
Iteration 24/1000 | Loss: 0.00003405
Iteration 25/1000 | Loss: 0.00003405
Iteration 26/1000 | Loss: 0.00003405
Iteration 27/1000 | Loss: 0.00003405
Iteration 28/1000 | Loss: 0.00003404
Iteration 29/1000 | Loss: 0.00003403
Iteration 30/1000 | Loss: 0.00003403
Iteration 31/1000 | Loss: 0.00003403
Iteration 32/1000 | Loss: 0.00003402
Iteration 33/1000 | Loss: 0.00003391
Iteration 34/1000 | Loss: 0.00003389
Iteration 35/1000 | Loss: 0.00003389
Iteration 36/1000 | Loss: 0.00003387
Iteration 37/1000 | Loss: 0.00003387
Iteration 38/1000 | Loss: 0.00003387
Iteration 39/1000 | Loss: 0.00003387
Iteration 40/1000 | Loss: 0.00003386
Iteration 41/1000 | Loss: 0.00003386
Iteration 42/1000 | Loss: 0.00003386
Iteration 43/1000 | Loss: 0.00003386
Iteration 44/1000 | Loss: 0.00003384
Iteration 45/1000 | Loss: 0.00003384
Iteration 46/1000 | Loss: 0.00003383
Iteration 47/1000 | Loss: 0.00003383
Iteration 48/1000 | Loss: 0.00003383
Iteration 49/1000 | Loss: 0.00003383
Iteration 50/1000 | Loss: 0.00003383
Iteration 51/1000 | Loss: 0.00003383
Iteration 52/1000 | Loss: 0.00003382
Iteration 53/1000 | Loss: 0.00003382
Iteration 54/1000 | Loss: 0.00003382
Iteration 55/1000 | Loss: 0.00003382
Iteration 56/1000 | Loss: 0.00003382
Iteration 57/1000 | Loss: 0.00003382
Iteration 58/1000 | Loss: 0.00003382
Iteration 59/1000 | Loss: 0.00003382
Iteration 60/1000 | Loss: 0.00003382
Iteration 61/1000 | Loss: 0.00003381
Iteration 62/1000 | Loss: 0.00003381
Iteration 63/1000 | Loss: 0.00003381
Iteration 64/1000 | Loss: 0.00003381
Iteration 65/1000 | Loss: 0.00003380
Iteration 66/1000 | Loss: 0.00003379
Iteration 67/1000 | Loss: 0.00003379
Iteration 68/1000 | Loss: 0.00003379
Iteration 69/1000 | Loss: 0.00003379
Iteration 70/1000 | Loss: 0.00003379
Iteration 71/1000 | Loss: 0.00003379
Iteration 72/1000 | Loss: 0.00003379
Iteration 73/1000 | Loss: 0.00003379
Iteration 74/1000 | Loss: 0.00003379
Iteration 75/1000 | Loss: 0.00003379
Iteration 76/1000 | Loss: 0.00003378
Iteration 77/1000 | Loss: 0.00003378
Iteration 78/1000 | Loss: 0.00003377
Iteration 79/1000 | Loss: 0.00003377
Iteration 80/1000 | Loss: 0.00003377
Iteration 81/1000 | Loss: 0.00003377
Iteration 82/1000 | Loss: 0.00003377
Iteration 83/1000 | Loss: 0.00003377
Iteration 84/1000 | Loss: 0.00003377
Iteration 85/1000 | Loss: 0.00003376
Iteration 86/1000 | Loss: 0.00003376
Iteration 87/1000 | Loss: 0.00003376
Iteration 88/1000 | Loss: 0.00003375
Iteration 89/1000 | Loss: 0.00003375
Iteration 90/1000 | Loss: 0.00003375
Iteration 91/1000 | Loss: 0.00003375
Iteration 92/1000 | Loss: 0.00003375
Iteration 93/1000 | Loss: 0.00003374
Iteration 94/1000 | Loss: 0.00003374
Iteration 95/1000 | Loss: 0.00003374
Iteration 96/1000 | Loss: 0.00003374
Iteration 97/1000 | Loss: 0.00003374
Iteration 98/1000 | Loss: 0.00003374
Iteration 99/1000 | Loss: 0.00003374
Iteration 100/1000 | Loss: 0.00003373
Iteration 101/1000 | Loss: 0.00003373
Iteration 102/1000 | Loss: 0.00003373
Iteration 103/1000 | Loss: 0.00003372
Iteration 104/1000 | Loss: 0.00003372
Iteration 105/1000 | Loss: 0.00003372
Iteration 106/1000 | Loss: 0.00003372
Iteration 107/1000 | Loss: 0.00003372
Iteration 108/1000 | Loss: 0.00003371
Iteration 109/1000 | Loss: 0.00003371
Iteration 110/1000 | Loss: 0.00003371
Iteration 111/1000 | Loss: 0.00003371
Iteration 112/1000 | Loss: 0.00003371
Iteration 113/1000 | Loss: 0.00003370
Iteration 114/1000 | Loss: 0.00003370
Iteration 115/1000 | Loss: 0.00003370
Iteration 116/1000 | Loss: 0.00003370
Iteration 117/1000 | Loss: 0.00003369
Iteration 118/1000 | Loss: 0.00003369
Iteration 119/1000 | Loss: 0.00003369
Iteration 120/1000 | Loss: 0.00003369
Iteration 121/1000 | Loss: 0.00003369
Iteration 122/1000 | Loss: 0.00003369
Iteration 123/1000 | Loss: 0.00003369
Iteration 124/1000 | Loss: 0.00003369
Iteration 125/1000 | Loss: 0.00003369
Iteration 126/1000 | Loss: 0.00003369
Iteration 127/1000 | Loss: 0.00003369
Iteration 128/1000 | Loss: 0.00003369
Iteration 129/1000 | Loss: 0.00003369
Iteration 130/1000 | Loss: 0.00003368
Iteration 131/1000 | Loss: 0.00003368
Iteration 132/1000 | Loss: 0.00003368
Iteration 133/1000 | Loss: 0.00003368
Iteration 134/1000 | Loss: 0.00003368
Iteration 135/1000 | Loss: 0.00003368
Iteration 136/1000 | Loss: 0.00003368
Iteration 137/1000 | Loss: 0.00003368
Iteration 138/1000 | Loss: 0.00003368
Iteration 139/1000 | Loss: 0.00003368
Iteration 140/1000 | Loss: 0.00003368
Iteration 141/1000 | Loss: 0.00003368
Iteration 142/1000 | Loss: 0.00003368
Iteration 143/1000 | Loss: 0.00003367
Iteration 144/1000 | Loss: 0.00003367
Iteration 145/1000 | Loss: 0.00003367
Iteration 146/1000 | Loss: 0.00003367
Iteration 147/1000 | Loss: 0.00003367
Iteration 148/1000 | Loss: 0.00003367
Iteration 149/1000 | Loss: 0.00003367
Iteration 150/1000 | Loss: 0.00003367
Iteration 151/1000 | Loss: 0.00003367
Iteration 152/1000 | Loss: 0.00003367
Iteration 153/1000 | Loss: 0.00003367
Iteration 154/1000 | Loss: 0.00003367
Iteration 155/1000 | Loss: 0.00003367
Iteration 156/1000 | Loss: 0.00003367
Iteration 157/1000 | Loss: 0.00003366
Iteration 158/1000 | Loss: 0.00003366
Iteration 159/1000 | Loss: 0.00003366
Iteration 160/1000 | Loss: 0.00003366
Iteration 161/1000 | Loss: 0.00003366
Iteration 162/1000 | Loss: 0.00003366
Iteration 163/1000 | Loss: 0.00003366
Iteration 164/1000 | Loss: 0.00003366
Iteration 165/1000 | Loss: 0.00003366
Iteration 166/1000 | Loss: 0.00003366
Iteration 167/1000 | Loss: 0.00003366
Iteration 168/1000 | Loss: 0.00003366
Iteration 169/1000 | Loss: 0.00003366
Iteration 170/1000 | Loss: 0.00003365
Iteration 171/1000 | Loss: 0.00003365
Iteration 172/1000 | Loss: 0.00003365
Iteration 173/1000 | Loss: 0.00003365
Iteration 174/1000 | Loss: 0.00003365
Iteration 175/1000 | Loss: 0.00003365
Iteration 176/1000 | Loss: 0.00003365
Iteration 177/1000 | Loss: 0.00003365
Iteration 178/1000 | Loss: 0.00003365
Iteration 179/1000 | Loss: 0.00003365
Iteration 180/1000 | Loss: 0.00003365
Iteration 181/1000 | Loss: 0.00003365
Iteration 182/1000 | Loss: 0.00003365
Iteration 183/1000 | Loss: 0.00003365
Iteration 184/1000 | Loss: 0.00003365
Iteration 185/1000 | Loss: 0.00003365
Iteration 186/1000 | Loss: 0.00003364
Iteration 187/1000 | Loss: 0.00003364
Iteration 188/1000 | Loss: 0.00003364
Iteration 189/1000 | Loss: 0.00003364
Iteration 190/1000 | Loss: 0.00003364
Iteration 191/1000 | Loss: 0.00003364
Iteration 192/1000 | Loss: 0.00003364
Iteration 193/1000 | Loss: 0.00003364
Iteration 194/1000 | Loss: 0.00003364
Iteration 195/1000 | Loss: 0.00003364
Iteration 196/1000 | Loss: 0.00003364
Iteration 197/1000 | Loss: 0.00003364
Iteration 198/1000 | Loss: 0.00003364
Iteration 199/1000 | Loss: 0.00003364
Iteration 200/1000 | Loss: 0.00003364
Iteration 201/1000 | Loss: 0.00003364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [3.364121221238747e-05, 3.364121221238747e-05, 3.364121221238747e-05, 3.364121221238747e-05, 3.364121221238747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.364121221238747e-05

Optimization complete. Final v2v error: 4.783657550811768 mm

Highest mean error: 5.031964302062988 mm for frame 32

Lowest mean error: 4.398157119750977 mm for frame 0

Saving results

Total time: 43.910643577575684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400296
Iteration 2/25 | Loss: 0.00128129
Iteration 3/25 | Loss: 0.00120727
Iteration 4/25 | Loss: 0.00119786
Iteration 5/25 | Loss: 0.00119505
Iteration 6/25 | Loss: 0.00119485
Iteration 7/25 | Loss: 0.00119485
Iteration 8/25 | Loss: 0.00119485
Iteration 9/25 | Loss: 0.00119485
Iteration 10/25 | Loss: 0.00119485
Iteration 11/25 | Loss: 0.00119485
Iteration 12/25 | Loss: 0.00119485
Iteration 13/25 | Loss: 0.00119485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011948460014536977, 0.0011948460014536977, 0.0011948460014536977, 0.0011948460014536977, 0.0011948460014536977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011948460014536977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.96548581
Iteration 2/25 | Loss: 0.00096858
Iteration 3/25 | Loss: 0.00096857
Iteration 4/25 | Loss: 0.00096857
Iteration 5/25 | Loss: 0.00096857
Iteration 6/25 | Loss: 0.00096857
Iteration 7/25 | Loss: 0.00096857
Iteration 8/25 | Loss: 0.00096857
Iteration 9/25 | Loss: 0.00096857
Iteration 10/25 | Loss: 0.00096857
Iteration 11/25 | Loss: 0.00096857
Iteration 12/25 | Loss: 0.00096857
Iteration 13/25 | Loss: 0.00096857
Iteration 14/25 | Loss: 0.00096857
Iteration 15/25 | Loss: 0.00096856
Iteration 16/25 | Loss: 0.00096856
Iteration 17/25 | Loss: 0.00096856
Iteration 18/25 | Loss: 0.00096856
Iteration 19/25 | Loss: 0.00096856
Iteration 20/25 | Loss: 0.00096856
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009685648838058114, 0.0009685648838058114, 0.0009685648838058114, 0.0009685648838058114, 0.0009685648838058114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009685648838058114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096856
Iteration 2/1000 | Loss: 0.00001932
Iteration 3/1000 | Loss: 0.00001522
Iteration 4/1000 | Loss: 0.00001412
Iteration 5/1000 | Loss: 0.00001326
Iteration 6/1000 | Loss: 0.00001269
Iteration 7/1000 | Loss: 0.00001257
Iteration 8/1000 | Loss: 0.00001229
Iteration 9/1000 | Loss: 0.00001220
Iteration 10/1000 | Loss: 0.00001215
Iteration 11/1000 | Loss: 0.00001184
Iteration 12/1000 | Loss: 0.00001175
Iteration 13/1000 | Loss: 0.00001155
Iteration 14/1000 | Loss: 0.00001143
Iteration 15/1000 | Loss: 0.00001137
Iteration 16/1000 | Loss: 0.00001133
Iteration 17/1000 | Loss: 0.00001131
Iteration 18/1000 | Loss: 0.00001130
Iteration 19/1000 | Loss: 0.00001129
Iteration 20/1000 | Loss: 0.00001128
Iteration 21/1000 | Loss: 0.00001119
Iteration 22/1000 | Loss: 0.00001117
Iteration 23/1000 | Loss: 0.00001116
Iteration 24/1000 | Loss: 0.00001116
Iteration 25/1000 | Loss: 0.00001116
Iteration 26/1000 | Loss: 0.00001115
Iteration 27/1000 | Loss: 0.00001115
Iteration 28/1000 | Loss: 0.00001115
Iteration 29/1000 | Loss: 0.00001114
Iteration 30/1000 | Loss: 0.00001112
Iteration 31/1000 | Loss: 0.00001111
Iteration 32/1000 | Loss: 0.00001111
Iteration 33/1000 | Loss: 0.00001111
Iteration 34/1000 | Loss: 0.00001110
Iteration 35/1000 | Loss: 0.00001108
Iteration 36/1000 | Loss: 0.00001107
Iteration 37/1000 | Loss: 0.00001106
Iteration 38/1000 | Loss: 0.00001105
Iteration 39/1000 | Loss: 0.00001105
Iteration 40/1000 | Loss: 0.00001103
Iteration 41/1000 | Loss: 0.00001099
Iteration 42/1000 | Loss: 0.00001099
Iteration 43/1000 | Loss: 0.00001098
Iteration 44/1000 | Loss: 0.00001098
Iteration 45/1000 | Loss: 0.00001097
Iteration 46/1000 | Loss: 0.00001097
Iteration 47/1000 | Loss: 0.00001095
Iteration 48/1000 | Loss: 0.00001093
Iteration 49/1000 | Loss: 0.00001092
Iteration 50/1000 | Loss: 0.00001092
Iteration 51/1000 | Loss: 0.00001092
Iteration 52/1000 | Loss: 0.00001091
Iteration 53/1000 | Loss: 0.00001091
Iteration 54/1000 | Loss: 0.00001091
Iteration 55/1000 | Loss: 0.00001089
Iteration 56/1000 | Loss: 0.00001088
Iteration 57/1000 | Loss: 0.00001088
Iteration 58/1000 | Loss: 0.00001087
Iteration 59/1000 | Loss: 0.00001087
Iteration 60/1000 | Loss: 0.00001087
Iteration 61/1000 | Loss: 0.00001087
Iteration 62/1000 | Loss: 0.00001087
Iteration 63/1000 | Loss: 0.00001086
Iteration 64/1000 | Loss: 0.00001086
Iteration 65/1000 | Loss: 0.00001086
Iteration 66/1000 | Loss: 0.00001085
Iteration 67/1000 | Loss: 0.00001085
Iteration 68/1000 | Loss: 0.00001085
Iteration 69/1000 | Loss: 0.00001085
Iteration 70/1000 | Loss: 0.00001084
Iteration 71/1000 | Loss: 0.00001084
Iteration 72/1000 | Loss: 0.00001084
Iteration 73/1000 | Loss: 0.00001084
Iteration 74/1000 | Loss: 0.00001084
Iteration 75/1000 | Loss: 0.00001084
Iteration 76/1000 | Loss: 0.00001083
Iteration 77/1000 | Loss: 0.00001083
Iteration 78/1000 | Loss: 0.00001083
Iteration 79/1000 | Loss: 0.00001082
Iteration 80/1000 | Loss: 0.00001082
Iteration 81/1000 | Loss: 0.00001081
Iteration 82/1000 | Loss: 0.00001081
Iteration 83/1000 | Loss: 0.00001080
Iteration 84/1000 | Loss: 0.00001080
Iteration 85/1000 | Loss: 0.00001080
Iteration 86/1000 | Loss: 0.00001079
Iteration 87/1000 | Loss: 0.00001079
Iteration 88/1000 | Loss: 0.00001079
Iteration 89/1000 | Loss: 0.00001079
Iteration 90/1000 | Loss: 0.00001079
Iteration 91/1000 | Loss: 0.00001078
Iteration 92/1000 | Loss: 0.00001078
Iteration 93/1000 | Loss: 0.00001078
Iteration 94/1000 | Loss: 0.00001077
Iteration 95/1000 | Loss: 0.00001077
Iteration 96/1000 | Loss: 0.00001077
Iteration 97/1000 | Loss: 0.00001076
Iteration 98/1000 | Loss: 0.00001076
Iteration 99/1000 | Loss: 0.00001076
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001076
Iteration 102/1000 | Loss: 0.00001076
Iteration 103/1000 | Loss: 0.00001076
Iteration 104/1000 | Loss: 0.00001075
Iteration 105/1000 | Loss: 0.00001074
Iteration 106/1000 | Loss: 0.00001074
Iteration 107/1000 | Loss: 0.00001074
Iteration 108/1000 | Loss: 0.00001074
Iteration 109/1000 | Loss: 0.00001074
Iteration 110/1000 | Loss: 0.00001074
Iteration 111/1000 | Loss: 0.00001074
Iteration 112/1000 | Loss: 0.00001074
Iteration 113/1000 | Loss: 0.00001074
Iteration 114/1000 | Loss: 0.00001073
Iteration 115/1000 | Loss: 0.00001073
Iteration 116/1000 | Loss: 0.00001073
Iteration 117/1000 | Loss: 0.00001073
Iteration 118/1000 | Loss: 0.00001072
Iteration 119/1000 | Loss: 0.00001072
Iteration 120/1000 | Loss: 0.00001072
Iteration 121/1000 | Loss: 0.00001072
Iteration 122/1000 | Loss: 0.00001072
Iteration 123/1000 | Loss: 0.00001072
Iteration 124/1000 | Loss: 0.00001072
Iteration 125/1000 | Loss: 0.00001071
Iteration 126/1000 | Loss: 0.00001071
Iteration 127/1000 | Loss: 0.00001071
Iteration 128/1000 | Loss: 0.00001071
Iteration 129/1000 | Loss: 0.00001071
Iteration 130/1000 | Loss: 0.00001071
Iteration 131/1000 | Loss: 0.00001071
Iteration 132/1000 | Loss: 0.00001071
Iteration 133/1000 | Loss: 0.00001071
Iteration 134/1000 | Loss: 0.00001071
Iteration 135/1000 | Loss: 0.00001071
Iteration 136/1000 | Loss: 0.00001071
Iteration 137/1000 | Loss: 0.00001071
Iteration 138/1000 | Loss: 0.00001071
Iteration 139/1000 | Loss: 0.00001071
Iteration 140/1000 | Loss: 0.00001070
Iteration 141/1000 | Loss: 0.00001070
Iteration 142/1000 | Loss: 0.00001070
Iteration 143/1000 | Loss: 0.00001070
Iteration 144/1000 | Loss: 0.00001070
Iteration 145/1000 | Loss: 0.00001069
Iteration 146/1000 | Loss: 0.00001069
Iteration 147/1000 | Loss: 0.00001069
Iteration 148/1000 | Loss: 0.00001069
Iteration 149/1000 | Loss: 0.00001069
Iteration 150/1000 | Loss: 0.00001069
Iteration 151/1000 | Loss: 0.00001069
Iteration 152/1000 | Loss: 0.00001069
Iteration 153/1000 | Loss: 0.00001069
Iteration 154/1000 | Loss: 0.00001068
Iteration 155/1000 | Loss: 0.00001068
Iteration 156/1000 | Loss: 0.00001068
Iteration 157/1000 | Loss: 0.00001068
Iteration 158/1000 | Loss: 0.00001068
Iteration 159/1000 | Loss: 0.00001068
Iteration 160/1000 | Loss: 0.00001068
Iteration 161/1000 | Loss: 0.00001068
Iteration 162/1000 | Loss: 0.00001068
Iteration 163/1000 | Loss: 0.00001067
Iteration 164/1000 | Loss: 0.00001067
Iteration 165/1000 | Loss: 0.00001067
Iteration 166/1000 | Loss: 0.00001067
Iteration 167/1000 | Loss: 0.00001067
Iteration 168/1000 | Loss: 0.00001067
Iteration 169/1000 | Loss: 0.00001067
Iteration 170/1000 | Loss: 0.00001066
Iteration 171/1000 | Loss: 0.00001066
Iteration 172/1000 | Loss: 0.00001066
Iteration 173/1000 | Loss: 0.00001066
Iteration 174/1000 | Loss: 0.00001066
Iteration 175/1000 | Loss: 0.00001065
Iteration 176/1000 | Loss: 0.00001065
Iteration 177/1000 | Loss: 0.00001065
Iteration 178/1000 | Loss: 0.00001065
Iteration 179/1000 | Loss: 0.00001064
Iteration 180/1000 | Loss: 0.00001064
Iteration 181/1000 | Loss: 0.00001064
Iteration 182/1000 | Loss: 0.00001064
Iteration 183/1000 | Loss: 0.00001064
Iteration 184/1000 | Loss: 0.00001064
Iteration 185/1000 | Loss: 0.00001064
Iteration 186/1000 | Loss: 0.00001064
Iteration 187/1000 | Loss: 0.00001064
Iteration 188/1000 | Loss: 0.00001064
Iteration 189/1000 | Loss: 0.00001064
Iteration 190/1000 | Loss: 0.00001064
Iteration 191/1000 | Loss: 0.00001064
Iteration 192/1000 | Loss: 0.00001064
Iteration 193/1000 | Loss: 0.00001064
Iteration 194/1000 | Loss: 0.00001064
Iteration 195/1000 | Loss: 0.00001064
Iteration 196/1000 | Loss: 0.00001064
Iteration 197/1000 | Loss: 0.00001064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.0638883395586163e-05, 1.0638883395586163e-05, 1.0638883395586163e-05, 1.0638883395586163e-05, 1.0638883395586163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0638883395586163e-05

Optimization complete. Final v2v error: 2.811584711074829 mm

Highest mean error: 3.126626491546631 mm for frame 82

Lowest mean error: 2.6877875328063965 mm for frame 144

Saving results

Total time: 43.423583984375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925389
Iteration 2/25 | Loss: 0.00214947
Iteration 3/25 | Loss: 0.00171121
Iteration 4/25 | Loss: 0.00158966
Iteration 5/25 | Loss: 0.00159609
Iteration 6/25 | Loss: 0.00161349
Iteration 7/25 | Loss: 0.00156796
Iteration 8/25 | Loss: 0.00153783
Iteration 9/25 | Loss: 0.00153284
Iteration 10/25 | Loss: 0.00155822
Iteration 11/25 | Loss: 0.00156850
Iteration 12/25 | Loss: 0.00155528
Iteration 13/25 | Loss: 0.00151486
Iteration 14/25 | Loss: 0.00150288
Iteration 15/25 | Loss: 0.00149978
Iteration 16/25 | Loss: 0.00149952
Iteration 17/25 | Loss: 0.00149949
Iteration 18/25 | Loss: 0.00149948
Iteration 19/25 | Loss: 0.00149948
Iteration 20/25 | Loss: 0.00149948
Iteration 21/25 | Loss: 0.00149948
Iteration 22/25 | Loss: 0.00149948
Iteration 23/25 | Loss: 0.00149948
Iteration 24/25 | Loss: 0.00149948
Iteration 25/25 | Loss: 0.00149948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27471113
Iteration 2/25 | Loss: 0.00137586
Iteration 3/25 | Loss: 0.00137586
Iteration 4/25 | Loss: 0.00137586
Iteration 5/25 | Loss: 0.00137586
Iteration 6/25 | Loss: 0.00137586
Iteration 7/25 | Loss: 0.00137586
Iteration 8/25 | Loss: 0.00137586
Iteration 9/25 | Loss: 0.00137586
Iteration 10/25 | Loss: 0.00137586
Iteration 11/25 | Loss: 0.00137586
Iteration 12/25 | Loss: 0.00137586
Iteration 13/25 | Loss: 0.00137586
Iteration 14/25 | Loss: 0.00137586
Iteration 15/25 | Loss: 0.00137586
Iteration 16/25 | Loss: 0.00137586
Iteration 17/25 | Loss: 0.00137586
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013758583227172494, 0.0013758583227172494, 0.0013758583227172494, 0.0013758583227172494, 0.0013758583227172494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013758583227172494

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137586
Iteration 2/1000 | Loss: 0.02090850
Iteration 3/1000 | Loss: 0.01299417
Iteration 4/1000 | Loss: 0.01313662
Iteration 5/1000 | Loss: 0.00797642
Iteration 6/1000 | Loss: 0.02236237
Iteration 7/1000 | Loss: 0.01231448
Iteration 8/1000 | Loss: 0.02454940
Iteration 9/1000 | Loss: 0.02047905
Iteration 10/1000 | Loss: 0.00841830
Iteration 11/1000 | Loss: 0.00879661
Iteration 12/1000 | Loss: 0.00444512
Iteration 13/1000 | Loss: 0.00110423
Iteration 14/1000 | Loss: 0.00100041
Iteration 15/1000 | Loss: 0.00309869
Iteration 16/1000 | Loss: 0.00409740
Iteration 17/1000 | Loss: 0.00185078
Iteration 18/1000 | Loss: 0.00188093
Iteration 19/1000 | Loss: 0.00168065
Iteration 20/1000 | Loss: 0.00229388
Iteration 21/1000 | Loss: 0.00079629
Iteration 22/1000 | Loss: 0.00041308
Iteration 23/1000 | Loss: 0.00125875
Iteration 24/1000 | Loss: 0.00099663
Iteration 25/1000 | Loss: 0.00198132
Iteration 26/1000 | Loss: 0.00491852
Iteration 27/1000 | Loss: 0.00089008
Iteration 28/1000 | Loss: 0.00193569
Iteration 29/1000 | Loss: 0.00119910
Iteration 30/1000 | Loss: 0.00103290
Iteration 31/1000 | Loss: 0.00154247
Iteration 32/1000 | Loss: 0.00064746
Iteration 33/1000 | Loss: 0.00291482
Iteration 34/1000 | Loss: 0.00460259
Iteration 35/1000 | Loss: 0.00301291
Iteration 36/1000 | Loss: 0.00056942
Iteration 37/1000 | Loss: 0.00217075
Iteration 38/1000 | Loss: 0.00288608
Iteration 39/1000 | Loss: 0.00150849
Iteration 40/1000 | Loss: 0.00152263
Iteration 41/1000 | Loss: 0.00157912
Iteration 42/1000 | Loss: 0.00105006
Iteration 43/1000 | Loss: 0.00043166
Iteration 44/1000 | Loss: 0.00325791
Iteration 45/1000 | Loss: 0.00231744
Iteration 46/1000 | Loss: 0.00145346
Iteration 47/1000 | Loss: 0.00074979
Iteration 48/1000 | Loss: 0.00088073
Iteration 49/1000 | Loss: 0.00247029
Iteration 50/1000 | Loss: 0.00441112
Iteration 51/1000 | Loss: 0.00250072
Iteration 52/1000 | Loss: 0.00053635
Iteration 53/1000 | Loss: 0.00052058
Iteration 54/1000 | Loss: 0.00048967
Iteration 55/1000 | Loss: 0.00052838
Iteration 56/1000 | Loss: 0.00217241
Iteration 57/1000 | Loss: 0.00120579
Iteration 58/1000 | Loss: 0.00067794
Iteration 59/1000 | Loss: 0.00230348
Iteration 60/1000 | Loss: 0.00230130
Iteration 61/1000 | Loss: 0.00189035
Iteration 62/1000 | Loss: 0.00142010
Iteration 63/1000 | Loss: 0.00140831
Iteration 64/1000 | Loss: 0.00119232
Iteration 65/1000 | Loss: 0.00092555
Iteration 66/1000 | Loss: 0.00187404
Iteration 67/1000 | Loss: 0.00085572
Iteration 68/1000 | Loss: 0.00122793
Iteration 69/1000 | Loss: 0.00135165
Iteration 70/1000 | Loss: 0.00133181
Iteration 71/1000 | Loss: 0.00101201
Iteration 72/1000 | Loss: 0.00059035
Iteration 73/1000 | Loss: 0.00080812
Iteration 74/1000 | Loss: 0.00159100
Iteration 75/1000 | Loss: 0.00117129
Iteration 76/1000 | Loss: 0.00171952
Iteration 77/1000 | Loss: 0.00151014
Iteration 78/1000 | Loss: 0.00157027
Iteration 79/1000 | Loss: 0.00139540
Iteration 80/1000 | Loss: 0.00081685
Iteration 81/1000 | Loss: 0.00086032
Iteration 82/1000 | Loss: 0.00062032
Iteration 83/1000 | Loss: 0.00041217
Iteration 84/1000 | Loss: 0.00049778
Iteration 85/1000 | Loss: 0.00072438
Iteration 86/1000 | Loss: 0.00122110
Iteration 87/1000 | Loss: 0.00166782
Iteration 88/1000 | Loss: 0.00061156
Iteration 89/1000 | Loss: 0.00052292
Iteration 90/1000 | Loss: 0.00176496
Iteration 91/1000 | Loss: 0.00089463
Iteration 92/1000 | Loss: 0.00027763
Iteration 93/1000 | Loss: 0.00041057
Iteration 94/1000 | Loss: 0.00029390
Iteration 95/1000 | Loss: 0.00150295
Iteration 96/1000 | Loss: 0.00086093
Iteration 97/1000 | Loss: 0.00026594
Iteration 98/1000 | Loss: 0.00036631
Iteration 99/1000 | Loss: 0.00022845
Iteration 100/1000 | Loss: 0.00023784
Iteration 101/1000 | Loss: 0.00144587
Iteration 102/1000 | Loss: 0.00039950
Iteration 103/1000 | Loss: 0.00041006
Iteration 104/1000 | Loss: 0.00026775
Iteration 105/1000 | Loss: 0.00047771
Iteration 106/1000 | Loss: 0.00049405
Iteration 107/1000 | Loss: 0.00043550
Iteration 108/1000 | Loss: 0.00115263
Iteration 109/1000 | Loss: 0.00044044
Iteration 110/1000 | Loss: 0.00021658
Iteration 111/1000 | Loss: 0.00017779
Iteration 112/1000 | Loss: 0.00027359
Iteration 113/1000 | Loss: 0.00018181
Iteration 114/1000 | Loss: 0.00023982
Iteration 115/1000 | Loss: 0.00016015
Iteration 116/1000 | Loss: 0.00025260
Iteration 117/1000 | Loss: 0.00078170
Iteration 118/1000 | Loss: 0.00020460
Iteration 119/1000 | Loss: 0.00017636
Iteration 120/1000 | Loss: 0.00014587
Iteration 121/1000 | Loss: 0.00017075
Iteration 122/1000 | Loss: 0.00028661
Iteration 123/1000 | Loss: 0.00015357
Iteration 124/1000 | Loss: 0.00016402
Iteration 125/1000 | Loss: 0.00012579
Iteration 126/1000 | Loss: 0.00011214
Iteration 127/1000 | Loss: 0.00013483
Iteration 128/1000 | Loss: 0.00062822
Iteration 129/1000 | Loss: 0.00088676
Iteration 130/1000 | Loss: 0.00016123
Iteration 131/1000 | Loss: 0.00015639
Iteration 132/1000 | Loss: 0.00015011
Iteration 133/1000 | Loss: 0.00013435
Iteration 134/1000 | Loss: 0.00012442
Iteration 135/1000 | Loss: 0.00012349
Iteration 136/1000 | Loss: 0.00012756
Iteration 137/1000 | Loss: 0.00010062
Iteration 138/1000 | Loss: 0.00046123
Iteration 139/1000 | Loss: 0.00062362
Iteration 140/1000 | Loss: 0.00057340
Iteration 141/1000 | Loss: 0.00013287
Iteration 142/1000 | Loss: 0.00011781
Iteration 143/1000 | Loss: 0.00008353
Iteration 144/1000 | Loss: 0.00009164
Iteration 145/1000 | Loss: 0.00008244
Iteration 146/1000 | Loss: 0.00009868
Iteration 147/1000 | Loss: 0.00041939
Iteration 148/1000 | Loss: 0.00012023
Iteration 149/1000 | Loss: 0.00010330
Iteration 150/1000 | Loss: 0.00013342
Iteration 151/1000 | Loss: 0.00008786
Iteration 152/1000 | Loss: 0.00007522
Iteration 153/1000 | Loss: 0.00008635
Iteration 154/1000 | Loss: 0.00038855
Iteration 155/1000 | Loss: 0.00011230
Iteration 156/1000 | Loss: 0.00008043
Iteration 157/1000 | Loss: 0.00007034
Iteration 158/1000 | Loss: 0.00006325
Iteration 159/1000 | Loss: 0.00005865
Iteration 160/1000 | Loss: 0.00005616
Iteration 161/1000 | Loss: 0.00005456
Iteration 162/1000 | Loss: 0.00005319
Iteration 163/1000 | Loss: 0.00005215
Iteration 164/1000 | Loss: 0.00005136
Iteration 165/1000 | Loss: 0.00005074
Iteration 166/1000 | Loss: 0.00005031
Iteration 167/1000 | Loss: 0.00004969
Iteration 168/1000 | Loss: 0.00020931
Iteration 169/1000 | Loss: 0.00005038
Iteration 170/1000 | Loss: 0.00004839
Iteration 171/1000 | Loss: 0.00004763
Iteration 172/1000 | Loss: 0.00016432
Iteration 173/1000 | Loss: 0.00004910
Iteration 174/1000 | Loss: 0.00004596
Iteration 175/1000 | Loss: 0.00004476
Iteration 176/1000 | Loss: 0.00004361
Iteration 177/1000 | Loss: 0.00004303
Iteration 178/1000 | Loss: 0.00004245
Iteration 179/1000 | Loss: 0.00004217
Iteration 180/1000 | Loss: 0.00004207
Iteration 181/1000 | Loss: 0.00015831
Iteration 182/1000 | Loss: 0.00004458
Iteration 183/1000 | Loss: 0.00004149
Iteration 184/1000 | Loss: 0.00004058
Iteration 185/1000 | Loss: 0.00003967
Iteration 186/1000 | Loss: 0.00003912
Iteration 187/1000 | Loss: 0.00003880
Iteration 188/1000 | Loss: 0.00003862
Iteration 189/1000 | Loss: 0.00003854
Iteration 190/1000 | Loss: 0.00003841
Iteration 191/1000 | Loss: 0.00003841
Iteration 192/1000 | Loss: 0.00003840
Iteration 193/1000 | Loss: 0.00003839
Iteration 194/1000 | Loss: 0.00003838
Iteration 195/1000 | Loss: 0.00003838
Iteration 196/1000 | Loss: 0.00003837
Iteration 197/1000 | Loss: 0.00003837
Iteration 198/1000 | Loss: 0.00003836
Iteration 199/1000 | Loss: 0.00003836
Iteration 200/1000 | Loss: 0.00003836
Iteration 201/1000 | Loss: 0.00003835
Iteration 202/1000 | Loss: 0.00003835
Iteration 203/1000 | Loss: 0.00003835
Iteration 204/1000 | Loss: 0.00003835
Iteration 205/1000 | Loss: 0.00003834
Iteration 206/1000 | Loss: 0.00003834
Iteration 207/1000 | Loss: 0.00003834
Iteration 208/1000 | Loss: 0.00003834
Iteration 209/1000 | Loss: 0.00003833
Iteration 210/1000 | Loss: 0.00003833
Iteration 211/1000 | Loss: 0.00003833
Iteration 212/1000 | Loss: 0.00003832
Iteration 213/1000 | Loss: 0.00003832
Iteration 214/1000 | Loss: 0.00003832
Iteration 215/1000 | Loss: 0.00003832
Iteration 216/1000 | Loss: 0.00003832
Iteration 217/1000 | Loss: 0.00003832
Iteration 218/1000 | Loss: 0.00003832
Iteration 219/1000 | Loss: 0.00003831
Iteration 220/1000 | Loss: 0.00003831
Iteration 221/1000 | Loss: 0.00003831
Iteration 222/1000 | Loss: 0.00003831
Iteration 223/1000 | Loss: 0.00003831
Iteration 224/1000 | Loss: 0.00003831
Iteration 225/1000 | Loss: 0.00022256
Iteration 226/1000 | Loss: 0.00004199
Iteration 227/1000 | Loss: 0.00003844
Iteration 228/1000 | Loss: 0.00003720
Iteration 229/1000 | Loss: 0.00003632
Iteration 230/1000 | Loss: 0.00003583
Iteration 231/1000 | Loss: 0.00003543
Iteration 232/1000 | Loss: 0.00003526
Iteration 233/1000 | Loss: 0.00003523
Iteration 234/1000 | Loss: 0.00003521
Iteration 235/1000 | Loss: 0.00003521
Iteration 236/1000 | Loss: 0.00003519
Iteration 237/1000 | Loss: 0.00003519
Iteration 238/1000 | Loss: 0.00003517
Iteration 239/1000 | Loss: 0.00003517
Iteration 240/1000 | Loss: 0.00003516
Iteration 241/1000 | Loss: 0.00003507
Iteration 242/1000 | Loss: 0.00003504
Iteration 243/1000 | Loss: 0.00003504
Iteration 244/1000 | Loss: 0.00003504
Iteration 245/1000 | Loss: 0.00003504
Iteration 246/1000 | Loss: 0.00003504
Iteration 247/1000 | Loss: 0.00003504
Iteration 248/1000 | Loss: 0.00003503
Iteration 249/1000 | Loss: 0.00003503
Iteration 250/1000 | Loss: 0.00003503
Iteration 251/1000 | Loss: 0.00003501
Iteration 252/1000 | Loss: 0.00003500
Iteration 253/1000 | Loss: 0.00003500
Iteration 254/1000 | Loss: 0.00003499
Iteration 255/1000 | Loss: 0.00003497
Iteration 256/1000 | Loss: 0.00003497
Iteration 257/1000 | Loss: 0.00003497
Iteration 258/1000 | Loss: 0.00003497
Iteration 259/1000 | Loss: 0.00003497
Iteration 260/1000 | Loss: 0.00003497
Iteration 261/1000 | Loss: 0.00003497
Iteration 262/1000 | Loss: 0.00003497
Iteration 263/1000 | Loss: 0.00003497
Iteration 264/1000 | Loss: 0.00003496
Iteration 265/1000 | Loss: 0.00003496
Iteration 266/1000 | Loss: 0.00003496
Iteration 267/1000 | Loss: 0.00003496
Iteration 268/1000 | Loss: 0.00003495
Iteration 269/1000 | Loss: 0.00003495
Iteration 270/1000 | Loss: 0.00003495
Iteration 271/1000 | Loss: 0.00003495
Iteration 272/1000 | Loss: 0.00003495
Iteration 273/1000 | Loss: 0.00003495
Iteration 274/1000 | Loss: 0.00003494
Iteration 275/1000 | Loss: 0.00003494
Iteration 276/1000 | Loss: 0.00003494
Iteration 277/1000 | Loss: 0.00003494
Iteration 278/1000 | Loss: 0.00003494
Iteration 279/1000 | Loss: 0.00003494
Iteration 280/1000 | Loss: 0.00003493
Iteration 281/1000 | Loss: 0.00003493
Iteration 282/1000 | Loss: 0.00003493
Iteration 283/1000 | Loss: 0.00003493
Iteration 284/1000 | Loss: 0.00003493
Iteration 285/1000 | Loss: 0.00003493
Iteration 286/1000 | Loss: 0.00003493
Iteration 287/1000 | Loss: 0.00003493
Iteration 288/1000 | Loss: 0.00003493
Iteration 289/1000 | Loss: 0.00003493
Iteration 290/1000 | Loss: 0.00003493
Iteration 291/1000 | Loss: 0.00003493
Iteration 292/1000 | Loss: 0.00003493
Iteration 293/1000 | Loss: 0.00003493
Iteration 294/1000 | Loss: 0.00003493
Iteration 295/1000 | Loss: 0.00003493
Iteration 296/1000 | Loss: 0.00003493
Iteration 297/1000 | Loss: 0.00003493
Iteration 298/1000 | Loss: 0.00003492
Iteration 299/1000 | Loss: 0.00003492
Iteration 300/1000 | Loss: 0.00003492
Iteration 301/1000 | Loss: 0.00003492
Iteration 302/1000 | Loss: 0.00003492
Iteration 303/1000 | Loss: 0.00003492
Iteration 304/1000 | Loss: 0.00003492
Iteration 305/1000 | Loss: 0.00003492
Iteration 306/1000 | Loss: 0.00003492
Iteration 307/1000 | Loss: 0.00003492
Iteration 308/1000 | Loss: 0.00003492
Iteration 309/1000 | Loss: 0.00003492
Iteration 310/1000 | Loss: 0.00003492
Iteration 311/1000 | Loss: 0.00003492
Iteration 312/1000 | Loss: 0.00003492
Iteration 313/1000 | Loss: 0.00003492
Iteration 314/1000 | Loss: 0.00003492
Iteration 315/1000 | Loss: 0.00003492
Iteration 316/1000 | Loss: 0.00003492
Iteration 317/1000 | Loss: 0.00003492
Iteration 318/1000 | Loss: 0.00003492
Iteration 319/1000 | Loss: 0.00003492
Iteration 320/1000 | Loss: 0.00003491
Iteration 321/1000 | Loss: 0.00003491
Iteration 322/1000 | Loss: 0.00003491
Iteration 323/1000 | Loss: 0.00003491
Iteration 324/1000 | Loss: 0.00003491
Iteration 325/1000 | Loss: 0.00003491
Iteration 326/1000 | Loss: 0.00003491
Iteration 327/1000 | Loss: 0.00003491
Iteration 328/1000 | Loss: 0.00003491
Iteration 329/1000 | Loss: 0.00003491
Iteration 330/1000 | Loss: 0.00003491
Iteration 331/1000 | Loss: 0.00003491
Iteration 332/1000 | Loss: 0.00003491
Iteration 333/1000 | Loss: 0.00003491
Iteration 334/1000 | Loss: 0.00003491
Iteration 335/1000 | Loss: 0.00003491
Iteration 336/1000 | Loss: 0.00003491
Iteration 337/1000 | Loss: 0.00003491
Iteration 338/1000 | Loss: 0.00003491
Iteration 339/1000 | Loss: 0.00003491
Iteration 340/1000 | Loss: 0.00003491
Iteration 341/1000 | Loss: 0.00003491
Iteration 342/1000 | Loss: 0.00003491
Iteration 343/1000 | Loss: 0.00003491
Iteration 344/1000 | Loss: 0.00003491
Iteration 345/1000 | Loss: 0.00003491
Iteration 346/1000 | Loss: 0.00003491
Iteration 347/1000 | Loss: 0.00003491
Iteration 348/1000 | Loss: 0.00003491
Iteration 349/1000 | Loss: 0.00003491
Iteration 350/1000 | Loss: 0.00003491
Iteration 351/1000 | Loss: 0.00003491
Iteration 352/1000 | Loss: 0.00003491
Iteration 353/1000 | Loss: 0.00003491
Iteration 354/1000 | Loss: 0.00003491
Iteration 355/1000 | Loss: 0.00003491
Iteration 356/1000 | Loss: 0.00003491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 356. Stopping optimization.
Last 5 losses: [3.490583549137227e-05, 3.490583549137227e-05, 3.490583549137227e-05, 3.490583549137227e-05, 3.490583549137227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.490583549137227e-05

Optimization complete. Final v2v error: 4.412356853485107 mm

Highest mean error: 6.597376823425293 mm for frame 83

Lowest mean error: 3.7482643127441406 mm for frame 129

Saving results

Total time: 310.7205903530121
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797983
Iteration 2/25 | Loss: 0.00137557
Iteration 3/25 | Loss: 0.00125125
Iteration 4/25 | Loss: 0.00122655
Iteration 5/25 | Loss: 0.00122030
Iteration 6/25 | Loss: 0.00121881
Iteration 7/25 | Loss: 0.00121868
Iteration 8/25 | Loss: 0.00121868
Iteration 9/25 | Loss: 0.00121868
Iteration 10/25 | Loss: 0.00121868
Iteration 11/25 | Loss: 0.00121868
Iteration 12/25 | Loss: 0.00121868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012186809908598661, 0.0012186809908598661, 0.0012186809908598661, 0.0012186809908598661, 0.0012186809908598661]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012186809908598661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31763780
Iteration 2/25 | Loss: 0.00088821
Iteration 3/25 | Loss: 0.00088821
Iteration 4/25 | Loss: 0.00088820
Iteration 5/25 | Loss: 0.00088820
Iteration 6/25 | Loss: 0.00088820
Iteration 7/25 | Loss: 0.00088820
Iteration 8/25 | Loss: 0.00088820
Iteration 9/25 | Loss: 0.00088820
Iteration 10/25 | Loss: 0.00088820
Iteration 11/25 | Loss: 0.00088820
Iteration 12/25 | Loss: 0.00088820
Iteration 13/25 | Loss: 0.00088820
Iteration 14/25 | Loss: 0.00088820
Iteration 15/25 | Loss: 0.00088820
Iteration 16/25 | Loss: 0.00088820
Iteration 17/25 | Loss: 0.00088820
Iteration 18/25 | Loss: 0.00088820
Iteration 19/25 | Loss: 0.00088820
Iteration 20/25 | Loss: 0.00088820
Iteration 21/25 | Loss: 0.00088820
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008882035035640001, 0.0008882035035640001, 0.0008882035035640001, 0.0008882035035640001, 0.0008882035035640001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008882035035640001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088820
Iteration 2/1000 | Loss: 0.00004441
Iteration 3/1000 | Loss: 0.00003002
Iteration 4/1000 | Loss: 0.00002535
Iteration 5/1000 | Loss: 0.00002416
Iteration 6/1000 | Loss: 0.00002308
Iteration 7/1000 | Loss: 0.00002232
Iteration 8/1000 | Loss: 0.00002181
Iteration 9/1000 | Loss: 0.00002134
Iteration 10/1000 | Loss: 0.00002107
Iteration 11/1000 | Loss: 0.00002077
Iteration 12/1000 | Loss: 0.00002051
Iteration 13/1000 | Loss: 0.00002042
Iteration 14/1000 | Loss: 0.00002023
Iteration 15/1000 | Loss: 0.00002010
Iteration 16/1000 | Loss: 0.00002001
Iteration 17/1000 | Loss: 0.00001995
Iteration 18/1000 | Loss: 0.00001991
Iteration 19/1000 | Loss: 0.00001991
Iteration 20/1000 | Loss: 0.00001988
Iteration 21/1000 | Loss: 0.00001986
Iteration 22/1000 | Loss: 0.00001986
Iteration 23/1000 | Loss: 0.00001985
Iteration 24/1000 | Loss: 0.00001985
Iteration 25/1000 | Loss: 0.00001985
Iteration 26/1000 | Loss: 0.00001985
Iteration 27/1000 | Loss: 0.00001985
Iteration 28/1000 | Loss: 0.00001984
Iteration 29/1000 | Loss: 0.00001983
Iteration 30/1000 | Loss: 0.00001982
Iteration 31/1000 | Loss: 0.00001982
Iteration 32/1000 | Loss: 0.00001981
Iteration 33/1000 | Loss: 0.00001981
Iteration 34/1000 | Loss: 0.00001981
Iteration 35/1000 | Loss: 0.00001981
Iteration 36/1000 | Loss: 0.00001981
Iteration 37/1000 | Loss: 0.00001981
Iteration 38/1000 | Loss: 0.00001981
Iteration 39/1000 | Loss: 0.00001980
Iteration 40/1000 | Loss: 0.00001980
Iteration 41/1000 | Loss: 0.00001980
Iteration 42/1000 | Loss: 0.00001980
Iteration 43/1000 | Loss: 0.00001980
Iteration 44/1000 | Loss: 0.00001979
Iteration 45/1000 | Loss: 0.00001978
Iteration 46/1000 | Loss: 0.00001978
Iteration 47/1000 | Loss: 0.00001978
Iteration 48/1000 | Loss: 0.00001977
Iteration 49/1000 | Loss: 0.00001977
Iteration 50/1000 | Loss: 0.00001977
Iteration 51/1000 | Loss: 0.00001976
Iteration 52/1000 | Loss: 0.00001976
Iteration 53/1000 | Loss: 0.00001976
Iteration 54/1000 | Loss: 0.00001976
Iteration 55/1000 | Loss: 0.00001975
Iteration 56/1000 | Loss: 0.00001975
Iteration 57/1000 | Loss: 0.00001975
Iteration 58/1000 | Loss: 0.00001975
Iteration 59/1000 | Loss: 0.00001975
Iteration 60/1000 | Loss: 0.00001975
Iteration 61/1000 | Loss: 0.00001975
Iteration 62/1000 | Loss: 0.00001974
Iteration 63/1000 | Loss: 0.00001974
Iteration 64/1000 | Loss: 0.00001974
Iteration 65/1000 | Loss: 0.00001974
Iteration 66/1000 | Loss: 0.00001974
Iteration 67/1000 | Loss: 0.00001974
Iteration 68/1000 | Loss: 0.00001974
Iteration 69/1000 | Loss: 0.00001974
Iteration 70/1000 | Loss: 0.00001974
Iteration 71/1000 | Loss: 0.00001974
Iteration 72/1000 | Loss: 0.00001974
Iteration 73/1000 | Loss: 0.00001973
Iteration 74/1000 | Loss: 0.00001973
Iteration 75/1000 | Loss: 0.00001973
Iteration 76/1000 | Loss: 0.00001973
Iteration 77/1000 | Loss: 0.00001972
Iteration 78/1000 | Loss: 0.00001972
Iteration 79/1000 | Loss: 0.00001972
Iteration 80/1000 | Loss: 0.00001972
Iteration 81/1000 | Loss: 0.00001971
Iteration 82/1000 | Loss: 0.00001971
Iteration 83/1000 | Loss: 0.00001971
Iteration 84/1000 | Loss: 0.00001971
Iteration 85/1000 | Loss: 0.00001970
Iteration 86/1000 | Loss: 0.00001970
Iteration 87/1000 | Loss: 0.00001970
Iteration 88/1000 | Loss: 0.00001970
Iteration 89/1000 | Loss: 0.00001970
Iteration 90/1000 | Loss: 0.00001969
Iteration 91/1000 | Loss: 0.00001969
Iteration 92/1000 | Loss: 0.00001969
Iteration 93/1000 | Loss: 0.00001969
Iteration 94/1000 | Loss: 0.00001969
Iteration 95/1000 | Loss: 0.00001969
Iteration 96/1000 | Loss: 0.00001969
Iteration 97/1000 | Loss: 0.00001969
Iteration 98/1000 | Loss: 0.00001969
Iteration 99/1000 | Loss: 0.00001969
Iteration 100/1000 | Loss: 0.00001969
Iteration 101/1000 | Loss: 0.00001969
Iteration 102/1000 | Loss: 0.00001969
Iteration 103/1000 | Loss: 0.00001969
Iteration 104/1000 | Loss: 0.00001969
Iteration 105/1000 | Loss: 0.00001969
Iteration 106/1000 | Loss: 0.00001969
Iteration 107/1000 | Loss: 0.00001968
Iteration 108/1000 | Loss: 0.00001968
Iteration 109/1000 | Loss: 0.00001968
Iteration 110/1000 | Loss: 0.00001967
Iteration 111/1000 | Loss: 0.00001967
Iteration 112/1000 | Loss: 0.00001967
Iteration 113/1000 | Loss: 0.00001967
Iteration 114/1000 | Loss: 0.00001966
Iteration 115/1000 | Loss: 0.00001966
Iteration 116/1000 | Loss: 0.00001966
Iteration 117/1000 | Loss: 0.00001966
Iteration 118/1000 | Loss: 0.00001966
Iteration 119/1000 | Loss: 0.00001966
Iteration 120/1000 | Loss: 0.00001966
Iteration 121/1000 | Loss: 0.00001965
Iteration 122/1000 | Loss: 0.00001965
Iteration 123/1000 | Loss: 0.00001965
Iteration 124/1000 | Loss: 0.00001965
Iteration 125/1000 | Loss: 0.00001965
Iteration 126/1000 | Loss: 0.00001964
Iteration 127/1000 | Loss: 0.00001964
Iteration 128/1000 | Loss: 0.00001964
Iteration 129/1000 | Loss: 0.00001964
Iteration 130/1000 | Loss: 0.00001964
Iteration 131/1000 | Loss: 0.00001964
Iteration 132/1000 | Loss: 0.00001963
Iteration 133/1000 | Loss: 0.00001963
Iteration 134/1000 | Loss: 0.00001963
Iteration 135/1000 | Loss: 0.00001963
Iteration 136/1000 | Loss: 0.00001963
Iteration 137/1000 | Loss: 0.00001963
Iteration 138/1000 | Loss: 0.00001963
Iteration 139/1000 | Loss: 0.00001963
Iteration 140/1000 | Loss: 0.00001963
Iteration 141/1000 | Loss: 0.00001963
Iteration 142/1000 | Loss: 0.00001963
Iteration 143/1000 | Loss: 0.00001963
Iteration 144/1000 | Loss: 0.00001962
Iteration 145/1000 | Loss: 0.00001962
Iteration 146/1000 | Loss: 0.00001962
Iteration 147/1000 | Loss: 0.00001962
Iteration 148/1000 | Loss: 0.00001962
Iteration 149/1000 | Loss: 0.00001962
Iteration 150/1000 | Loss: 0.00001962
Iteration 151/1000 | Loss: 0.00001962
Iteration 152/1000 | Loss: 0.00001962
Iteration 153/1000 | Loss: 0.00001961
Iteration 154/1000 | Loss: 0.00001961
Iteration 155/1000 | Loss: 0.00001961
Iteration 156/1000 | Loss: 0.00001961
Iteration 157/1000 | Loss: 0.00001961
Iteration 158/1000 | Loss: 0.00001961
Iteration 159/1000 | Loss: 0.00001961
Iteration 160/1000 | Loss: 0.00001961
Iteration 161/1000 | Loss: 0.00001960
Iteration 162/1000 | Loss: 0.00001960
Iteration 163/1000 | Loss: 0.00001960
Iteration 164/1000 | Loss: 0.00001960
Iteration 165/1000 | Loss: 0.00001960
Iteration 166/1000 | Loss: 0.00001959
Iteration 167/1000 | Loss: 0.00001959
Iteration 168/1000 | Loss: 0.00001959
Iteration 169/1000 | Loss: 0.00001959
Iteration 170/1000 | Loss: 0.00001959
Iteration 171/1000 | Loss: 0.00001959
Iteration 172/1000 | Loss: 0.00001958
Iteration 173/1000 | Loss: 0.00001958
Iteration 174/1000 | Loss: 0.00001958
Iteration 175/1000 | Loss: 0.00001958
Iteration 176/1000 | Loss: 0.00001958
Iteration 177/1000 | Loss: 0.00001957
Iteration 178/1000 | Loss: 0.00001957
Iteration 179/1000 | Loss: 0.00001957
Iteration 180/1000 | Loss: 0.00001957
Iteration 181/1000 | Loss: 0.00001956
Iteration 182/1000 | Loss: 0.00001956
Iteration 183/1000 | Loss: 0.00001956
Iteration 184/1000 | Loss: 0.00001956
Iteration 185/1000 | Loss: 0.00001955
Iteration 186/1000 | Loss: 0.00001955
Iteration 187/1000 | Loss: 0.00001955
Iteration 188/1000 | Loss: 0.00001955
Iteration 189/1000 | Loss: 0.00001955
Iteration 190/1000 | Loss: 0.00001955
Iteration 191/1000 | Loss: 0.00001954
Iteration 192/1000 | Loss: 0.00001954
Iteration 193/1000 | Loss: 0.00001954
Iteration 194/1000 | Loss: 0.00001954
Iteration 195/1000 | Loss: 0.00001954
Iteration 196/1000 | Loss: 0.00001954
Iteration 197/1000 | Loss: 0.00001954
Iteration 198/1000 | Loss: 0.00001953
Iteration 199/1000 | Loss: 0.00001953
Iteration 200/1000 | Loss: 0.00001953
Iteration 201/1000 | Loss: 0.00001953
Iteration 202/1000 | Loss: 0.00001953
Iteration 203/1000 | Loss: 0.00001953
Iteration 204/1000 | Loss: 0.00001953
Iteration 205/1000 | Loss: 0.00001953
Iteration 206/1000 | Loss: 0.00001953
Iteration 207/1000 | Loss: 0.00001953
Iteration 208/1000 | Loss: 0.00001953
Iteration 209/1000 | Loss: 0.00001953
Iteration 210/1000 | Loss: 0.00001953
Iteration 211/1000 | Loss: 0.00001953
Iteration 212/1000 | Loss: 0.00001953
Iteration 213/1000 | Loss: 0.00001953
Iteration 214/1000 | Loss: 0.00001953
Iteration 215/1000 | Loss: 0.00001953
Iteration 216/1000 | Loss: 0.00001953
Iteration 217/1000 | Loss: 0.00001953
Iteration 218/1000 | Loss: 0.00001953
Iteration 219/1000 | Loss: 0.00001953
Iteration 220/1000 | Loss: 0.00001953
Iteration 221/1000 | Loss: 0.00001953
Iteration 222/1000 | Loss: 0.00001953
Iteration 223/1000 | Loss: 0.00001953
Iteration 224/1000 | Loss: 0.00001953
Iteration 225/1000 | Loss: 0.00001953
Iteration 226/1000 | Loss: 0.00001953
Iteration 227/1000 | Loss: 0.00001953
Iteration 228/1000 | Loss: 0.00001953
Iteration 229/1000 | Loss: 0.00001953
Iteration 230/1000 | Loss: 0.00001953
Iteration 231/1000 | Loss: 0.00001953
Iteration 232/1000 | Loss: 0.00001953
Iteration 233/1000 | Loss: 0.00001953
Iteration 234/1000 | Loss: 0.00001953
Iteration 235/1000 | Loss: 0.00001953
Iteration 236/1000 | Loss: 0.00001953
Iteration 237/1000 | Loss: 0.00001953
Iteration 238/1000 | Loss: 0.00001953
Iteration 239/1000 | Loss: 0.00001953
Iteration 240/1000 | Loss: 0.00001953
Iteration 241/1000 | Loss: 0.00001953
Iteration 242/1000 | Loss: 0.00001953
Iteration 243/1000 | Loss: 0.00001953
Iteration 244/1000 | Loss: 0.00001953
Iteration 245/1000 | Loss: 0.00001953
Iteration 246/1000 | Loss: 0.00001953
Iteration 247/1000 | Loss: 0.00001953
Iteration 248/1000 | Loss: 0.00001953
Iteration 249/1000 | Loss: 0.00001953
Iteration 250/1000 | Loss: 0.00001953
Iteration 251/1000 | Loss: 0.00001953
Iteration 252/1000 | Loss: 0.00001953
Iteration 253/1000 | Loss: 0.00001953
Iteration 254/1000 | Loss: 0.00001953
Iteration 255/1000 | Loss: 0.00001953
Iteration 256/1000 | Loss: 0.00001953
Iteration 257/1000 | Loss: 0.00001953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.952998536580708e-05, 1.952998536580708e-05, 1.952998536580708e-05, 1.952998536580708e-05, 1.952998536580708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.952998536580708e-05

Optimization complete. Final v2v error: 3.788031816482544 mm

Highest mean error: 4.143349647521973 mm for frame 24

Lowest mean error: 3.5204951763153076 mm for frame 139

Saving results

Total time: 45.473912477493286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00720025
Iteration 2/25 | Loss: 0.00133588
Iteration 3/25 | Loss: 0.00124773
Iteration 4/25 | Loss: 0.00123979
Iteration 5/25 | Loss: 0.00123868
Iteration 6/25 | Loss: 0.00123868
Iteration 7/25 | Loss: 0.00123868
Iteration 8/25 | Loss: 0.00123868
Iteration 9/25 | Loss: 0.00123868
Iteration 10/25 | Loss: 0.00123868
Iteration 11/25 | Loss: 0.00123868
Iteration 12/25 | Loss: 0.00123868
Iteration 13/25 | Loss: 0.00123868
Iteration 14/25 | Loss: 0.00123868
Iteration 15/25 | Loss: 0.00123868
Iteration 16/25 | Loss: 0.00123868
Iteration 17/25 | Loss: 0.00123868
Iteration 18/25 | Loss: 0.00123868
Iteration 19/25 | Loss: 0.00123868
Iteration 20/25 | Loss: 0.00123868
Iteration 21/25 | Loss: 0.00123868
Iteration 22/25 | Loss: 0.00123868
Iteration 23/25 | Loss: 0.00123868
Iteration 24/25 | Loss: 0.00123868
Iteration 25/25 | Loss: 0.00123868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012386813759803772, 0.0012386813759803772, 0.0012386813759803772, 0.0012386813759803772, 0.0012386813759803772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012386813759803772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 16.77899551
Iteration 2/25 | Loss: 0.00102478
Iteration 3/25 | Loss: 0.00102472
Iteration 4/25 | Loss: 0.00102472
Iteration 5/25 | Loss: 0.00102472
Iteration 6/25 | Loss: 0.00102472
Iteration 7/25 | Loss: 0.00102472
Iteration 8/25 | Loss: 0.00102472
Iteration 9/25 | Loss: 0.00102472
Iteration 10/25 | Loss: 0.00102472
Iteration 11/25 | Loss: 0.00102472
Iteration 12/25 | Loss: 0.00102472
Iteration 13/25 | Loss: 0.00102472
Iteration 14/25 | Loss: 0.00102472
Iteration 15/25 | Loss: 0.00102472
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010247164173051715, 0.0010247164173051715, 0.0010247164173051715, 0.0010247164173051715, 0.0010247164173051715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010247164173051715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102472
Iteration 2/1000 | Loss: 0.00001951
Iteration 3/1000 | Loss: 0.00001517
Iteration 4/1000 | Loss: 0.00001391
Iteration 5/1000 | Loss: 0.00001313
Iteration 6/1000 | Loss: 0.00001283
Iteration 7/1000 | Loss: 0.00001273
Iteration 8/1000 | Loss: 0.00001239
Iteration 9/1000 | Loss: 0.00001217
Iteration 10/1000 | Loss: 0.00001193
Iteration 11/1000 | Loss: 0.00001173
Iteration 12/1000 | Loss: 0.00001173
Iteration 13/1000 | Loss: 0.00001158
Iteration 14/1000 | Loss: 0.00001153
Iteration 15/1000 | Loss: 0.00001149
Iteration 16/1000 | Loss: 0.00001148
Iteration 17/1000 | Loss: 0.00001148
Iteration 18/1000 | Loss: 0.00001148
Iteration 19/1000 | Loss: 0.00001146
Iteration 20/1000 | Loss: 0.00001144
Iteration 21/1000 | Loss: 0.00001143
Iteration 22/1000 | Loss: 0.00001142
Iteration 23/1000 | Loss: 0.00001141
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001136
Iteration 26/1000 | Loss: 0.00001127
Iteration 27/1000 | Loss: 0.00001127
Iteration 28/1000 | Loss: 0.00001122
Iteration 29/1000 | Loss: 0.00001120
Iteration 30/1000 | Loss: 0.00001120
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001120
Iteration 33/1000 | Loss: 0.00001119
Iteration 34/1000 | Loss: 0.00001119
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001116
Iteration 37/1000 | Loss: 0.00001115
Iteration 38/1000 | Loss: 0.00001114
Iteration 39/1000 | Loss: 0.00001114
Iteration 40/1000 | Loss: 0.00001114
Iteration 41/1000 | Loss: 0.00001114
Iteration 42/1000 | Loss: 0.00001113
Iteration 43/1000 | Loss: 0.00001113
Iteration 44/1000 | Loss: 0.00001112
Iteration 45/1000 | Loss: 0.00001112
Iteration 46/1000 | Loss: 0.00001112
Iteration 47/1000 | Loss: 0.00001111
Iteration 48/1000 | Loss: 0.00001111
Iteration 49/1000 | Loss: 0.00001110
Iteration 50/1000 | Loss: 0.00001110
Iteration 51/1000 | Loss: 0.00001110
Iteration 52/1000 | Loss: 0.00001109
Iteration 53/1000 | Loss: 0.00001109
Iteration 54/1000 | Loss: 0.00001109
Iteration 55/1000 | Loss: 0.00001108
Iteration 56/1000 | Loss: 0.00001108
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001107
Iteration 59/1000 | Loss: 0.00001106
Iteration 60/1000 | Loss: 0.00001106
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001105
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001103
Iteration 66/1000 | Loss: 0.00001103
Iteration 67/1000 | Loss: 0.00001103
Iteration 68/1000 | Loss: 0.00001103
Iteration 69/1000 | Loss: 0.00001103
Iteration 70/1000 | Loss: 0.00001102
Iteration 71/1000 | Loss: 0.00001102
Iteration 72/1000 | Loss: 0.00001102
Iteration 73/1000 | Loss: 0.00001102
Iteration 74/1000 | Loss: 0.00001102
Iteration 75/1000 | Loss: 0.00001102
Iteration 76/1000 | Loss: 0.00001101
Iteration 77/1000 | Loss: 0.00001101
Iteration 78/1000 | Loss: 0.00001101
Iteration 79/1000 | Loss: 0.00001101
Iteration 80/1000 | Loss: 0.00001100
Iteration 81/1000 | Loss: 0.00001100
Iteration 82/1000 | Loss: 0.00001100
Iteration 83/1000 | Loss: 0.00001099
Iteration 84/1000 | Loss: 0.00001099
Iteration 85/1000 | Loss: 0.00001098
Iteration 86/1000 | Loss: 0.00001098
Iteration 87/1000 | Loss: 0.00001098
Iteration 88/1000 | Loss: 0.00001097
Iteration 89/1000 | Loss: 0.00001097
Iteration 90/1000 | Loss: 0.00001097
Iteration 91/1000 | Loss: 0.00001097
Iteration 92/1000 | Loss: 0.00001096
Iteration 93/1000 | Loss: 0.00001096
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001096
Iteration 97/1000 | Loss: 0.00001095
Iteration 98/1000 | Loss: 0.00001095
Iteration 99/1000 | Loss: 0.00001095
Iteration 100/1000 | Loss: 0.00001094
Iteration 101/1000 | Loss: 0.00001094
Iteration 102/1000 | Loss: 0.00001094
Iteration 103/1000 | Loss: 0.00001093
Iteration 104/1000 | Loss: 0.00001093
Iteration 105/1000 | Loss: 0.00001092
Iteration 106/1000 | Loss: 0.00001092
Iteration 107/1000 | Loss: 0.00001092
Iteration 108/1000 | Loss: 0.00001092
Iteration 109/1000 | Loss: 0.00001092
Iteration 110/1000 | Loss: 0.00001092
Iteration 111/1000 | Loss: 0.00001092
Iteration 112/1000 | Loss: 0.00001092
Iteration 113/1000 | Loss: 0.00001092
Iteration 114/1000 | Loss: 0.00001092
Iteration 115/1000 | Loss: 0.00001092
Iteration 116/1000 | Loss: 0.00001091
Iteration 117/1000 | Loss: 0.00001091
Iteration 118/1000 | Loss: 0.00001090
Iteration 119/1000 | Loss: 0.00001090
Iteration 120/1000 | Loss: 0.00001090
Iteration 121/1000 | Loss: 0.00001089
Iteration 122/1000 | Loss: 0.00001089
Iteration 123/1000 | Loss: 0.00001089
Iteration 124/1000 | Loss: 0.00001089
Iteration 125/1000 | Loss: 0.00001089
Iteration 126/1000 | Loss: 0.00001089
Iteration 127/1000 | Loss: 0.00001089
Iteration 128/1000 | Loss: 0.00001089
Iteration 129/1000 | Loss: 0.00001089
Iteration 130/1000 | Loss: 0.00001089
Iteration 131/1000 | Loss: 0.00001089
Iteration 132/1000 | Loss: 0.00001088
Iteration 133/1000 | Loss: 0.00001088
Iteration 134/1000 | Loss: 0.00001088
Iteration 135/1000 | Loss: 0.00001088
Iteration 136/1000 | Loss: 0.00001087
Iteration 137/1000 | Loss: 0.00001087
Iteration 138/1000 | Loss: 0.00001087
Iteration 139/1000 | Loss: 0.00001086
Iteration 140/1000 | Loss: 0.00001086
Iteration 141/1000 | Loss: 0.00001086
Iteration 142/1000 | Loss: 0.00001086
Iteration 143/1000 | Loss: 0.00001086
Iteration 144/1000 | Loss: 0.00001086
Iteration 145/1000 | Loss: 0.00001085
Iteration 146/1000 | Loss: 0.00001085
Iteration 147/1000 | Loss: 0.00001085
Iteration 148/1000 | Loss: 0.00001085
Iteration 149/1000 | Loss: 0.00001085
Iteration 150/1000 | Loss: 0.00001085
Iteration 151/1000 | Loss: 0.00001085
Iteration 152/1000 | Loss: 0.00001085
Iteration 153/1000 | Loss: 0.00001085
Iteration 154/1000 | Loss: 0.00001085
Iteration 155/1000 | Loss: 0.00001085
Iteration 156/1000 | Loss: 0.00001085
Iteration 157/1000 | Loss: 0.00001084
Iteration 158/1000 | Loss: 0.00001084
Iteration 159/1000 | Loss: 0.00001084
Iteration 160/1000 | Loss: 0.00001084
Iteration 161/1000 | Loss: 0.00001084
Iteration 162/1000 | Loss: 0.00001084
Iteration 163/1000 | Loss: 0.00001084
Iteration 164/1000 | Loss: 0.00001084
Iteration 165/1000 | Loss: 0.00001083
Iteration 166/1000 | Loss: 0.00001083
Iteration 167/1000 | Loss: 0.00001083
Iteration 168/1000 | Loss: 0.00001083
Iteration 169/1000 | Loss: 0.00001083
Iteration 170/1000 | Loss: 0.00001083
Iteration 171/1000 | Loss: 0.00001083
Iteration 172/1000 | Loss: 0.00001083
Iteration 173/1000 | Loss: 0.00001083
Iteration 174/1000 | Loss: 0.00001083
Iteration 175/1000 | Loss: 0.00001083
Iteration 176/1000 | Loss: 0.00001083
Iteration 177/1000 | Loss: 0.00001083
Iteration 178/1000 | Loss: 0.00001083
Iteration 179/1000 | Loss: 0.00001083
Iteration 180/1000 | Loss: 0.00001083
Iteration 181/1000 | Loss: 0.00001083
Iteration 182/1000 | Loss: 0.00001083
Iteration 183/1000 | Loss: 0.00001083
Iteration 184/1000 | Loss: 0.00001083
Iteration 185/1000 | Loss: 0.00001083
Iteration 186/1000 | Loss: 0.00001083
Iteration 187/1000 | Loss: 0.00001083
Iteration 188/1000 | Loss: 0.00001083
Iteration 189/1000 | Loss: 0.00001083
Iteration 190/1000 | Loss: 0.00001083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.0832488442247268e-05, 1.0832488442247268e-05, 1.0832488442247268e-05, 1.0832488442247268e-05, 1.0832488442247268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0832488442247268e-05

Optimization complete. Final v2v error: 2.801828384399414 mm

Highest mean error: 3.3093011379241943 mm for frame 149

Lowest mean error: 2.5835585594177246 mm for frame 58

Saving results

Total time: 44.549153566360474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046407
Iteration 2/25 | Loss: 0.00401447
Iteration 3/25 | Loss: 0.00327518
Iteration 4/25 | Loss: 0.00317062
Iteration 5/25 | Loss: 0.00359144
Iteration 6/25 | Loss: 0.00302182
Iteration 7/25 | Loss: 0.00267938
Iteration 8/25 | Loss: 0.00259657
Iteration 9/25 | Loss: 0.00237922
Iteration 10/25 | Loss: 0.00226291
Iteration 11/25 | Loss: 0.00225883
Iteration 12/25 | Loss: 0.00208320
Iteration 13/25 | Loss: 0.00203410
Iteration 14/25 | Loss: 0.00196738
Iteration 15/25 | Loss: 0.00187473
Iteration 16/25 | Loss: 0.00175488
Iteration 17/25 | Loss: 0.00176819
Iteration 18/25 | Loss: 0.00169643
Iteration 19/25 | Loss: 0.00169099
Iteration 20/25 | Loss: 0.00169405
Iteration 21/25 | Loss: 0.00172141
Iteration 22/25 | Loss: 0.00168039
Iteration 23/25 | Loss: 0.00167420
Iteration 24/25 | Loss: 0.00170917
Iteration 25/25 | Loss: 0.00167359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06820476
Iteration 2/25 | Loss: 0.00279021
Iteration 3/25 | Loss: 0.00243215
Iteration 4/25 | Loss: 0.00243215
Iteration 5/25 | Loss: 0.00243215
Iteration 6/25 | Loss: 0.00243215
Iteration 7/25 | Loss: 0.00243215
Iteration 8/25 | Loss: 0.00243215
Iteration 9/25 | Loss: 0.00243215
Iteration 10/25 | Loss: 0.00243215
Iteration 11/25 | Loss: 0.00243215
Iteration 12/25 | Loss: 0.00243215
Iteration 13/25 | Loss: 0.00243215
Iteration 14/25 | Loss: 0.00243215
Iteration 15/25 | Loss: 0.00243215
Iteration 16/25 | Loss: 0.00243215
Iteration 17/25 | Loss: 0.00243215
Iteration 18/25 | Loss: 0.00243215
Iteration 19/25 | Loss: 0.00243215
Iteration 20/25 | Loss: 0.00243215
Iteration 21/25 | Loss: 0.00243215
Iteration 22/25 | Loss: 0.00243215
Iteration 23/25 | Loss: 0.00243215
Iteration 24/25 | Loss: 0.00243215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0024321486707776785, 0.0024321486707776785, 0.0024321486707776785, 0.0024321486707776785, 0.0024321486707776785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024321486707776785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243215
Iteration 2/1000 | Loss: 0.00167254
Iteration 3/1000 | Loss: 0.00021934
Iteration 4/1000 | Loss: 0.00017625
Iteration 5/1000 | Loss: 0.00015305
Iteration 6/1000 | Loss: 0.00014214
Iteration 7/1000 | Loss: 0.00081820
Iteration 8/1000 | Loss: 0.00303256
Iteration 9/1000 | Loss: 0.00719736
Iteration 10/1000 | Loss: 0.00346412
Iteration 11/1000 | Loss: 0.00189222
Iteration 12/1000 | Loss: 0.00473429
Iteration 13/1000 | Loss: 0.00226134
Iteration 14/1000 | Loss: 0.00108566
Iteration 15/1000 | Loss: 0.00192382
Iteration 16/1000 | Loss: 0.00177043
Iteration 17/1000 | Loss: 0.00285885
Iteration 18/1000 | Loss: 0.00158681
Iteration 19/1000 | Loss: 0.00240037
Iteration 20/1000 | Loss: 0.00068826
Iteration 21/1000 | Loss: 0.00079137
Iteration 22/1000 | Loss: 0.00036979
Iteration 23/1000 | Loss: 0.00033630
Iteration 24/1000 | Loss: 0.00037147
Iteration 25/1000 | Loss: 0.00031912
Iteration 26/1000 | Loss: 0.00116187
Iteration 27/1000 | Loss: 0.00051446
Iteration 28/1000 | Loss: 0.00104254
Iteration 29/1000 | Loss: 0.00051417
Iteration 30/1000 | Loss: 0.00048101
Iteration 31/1000 | Loss: 0.00038052
Iteration 32/1000 | Loss: 0.00031642
Iteration 33/1000 | Loss: 0.00012217
Iteration 34/1000 | Loss: 0.00036975
Iteration 35/1000 | Loss: 0.00011941
Iteration 36/1000 | Loss: 0.00150898
Iteration 37/1000 | Loss: 0.00014402
Iteration 38/1000 | Loss: 0.00207565
Iteration 39/1000 | Loss: 0.00029280
Iteration 40/1000 | Loss: 0.00138448
Iteration 41/1000 | Loss: 0.00057254
Iteration 42/1000 | Loss: 0.00013017
Iteration 43/1000 | Loss: 0.00066491
Iteration 44/1000 | Loss: 0.00033645
Iteration 45/1000 | Loss: 0.00090368
Iteration 46/1000 | Loss: 0.00061695
Iteration 47/1000 | Loss: 0.00027033
Iteration 48/1000 | Loss: 0.00066161
Iteration 49/1000 | Loss: 0.00009505
Iteration 50/1000 | Loss: 0.00053071
Iteration 51/1000 | Loss: 0.00007603
Iteration 52/1000 | Loss: 0.00019096
Iteration 53/1000 | Loss: 0.00007159
Iteration 54/1000 | Loss: 0.00005862
Iteration 55/1000 | Loss: 0.00005373
Iteration 56/1000 | Loss: 0.00021500
Iteration 57/1000 | Loss: 0.00005120
Iteration 58/1000 | Loss: 0.00004593
Iteration 59/1000 | Loss: 0.00004339
Iteration 60/1000 | Loss: 0.00004195
Iteration 61/1000 | Loss: 0.00022038
Iteration 62/1000 | Loss: 0.00004896
Iteration 63/1000 | Loss: 0.00004481
Iteration 64/1000 | Loss: 0.00004274
Iteration 65/1000 | Loss: 0.00004131
Iteration 66/1000 | Loss: 0.00004041
Iteration 67/1000 | Loss: 0.00003939
Iteration 68/1000 | Loss: 0.00003840
Iteration 69/1000 | Loss: 0.00003757
Iteration 70/1000 | Loss: 0.00004990
Iteration 71/1000 | Loss: 0.00015087
Iteration 72/1000 | Loss: 0.00005842
Iteration 73/1000 | Loss: 0.00005761
Iteration 74/1000 | Loss: 0.00014961
Iteration 75/1000 | Loss: 0.00010979
Iteration 76/1000 | Loss: 0.00004801
Iteration 77/1000 | Loss: 0.00006136
Iteration 78/1000 | Loss: 0.00004215
Iteration 79/1000 | Loss: 0.00006079
Iteration 80/1000 | Loss: 0.00009357
Iteration 81/1000 | Loss: 0.00007299
Iteration 82/1000 | Loss: 0.00004028
Iteration 83/1000 | Loss: 0.00003840
Iteration 84/1000 | Loss: 0.00009033
Iteration 85/1000 | Loss: 0.00007754
Iteration 86/1000 | Loss: 0.00007209
Iteration 87/1000 | Loss: 0.00004592
Iteration 88/1000 | Loss: 0.00004158
Iteration 89/1000 | Loss: 0.00005677
Iteration 90/1000 | Loss: 0.00004866
Iteration 91/1000 | Loss: 0.00004516
Iteration 92/1000 | Loss: 0.00004365
Iteration 93/1000 | Loss: 0.00003607
Iteration 94/1000 | Loss: 0.00006941
Iteration 95/1000 | Loss: 0.00004176
Iteration 96/1000 | Loss: 0.00005461
Iteration 97/1000 | Loss: 0.00003772
Iteration 98/1000 | Loss: 0.00003555
Iteration 99/1000 | Loss: 0.00003490
Iteration 100/1000 | Loss: 0.00003426
Iteration 101/1000 | Loss: 0.00003400
Iteration 102/1000 | Loss: 0.00003386
Iteration 103/1000 | Loss: 0.00003381
Iteration 104/1000 | Loss: 0.00003365
Iteration 105/1000 | Loss: 0.00003356
Iteration 106/1000 | Loss: 0.00003352
Iteration 107/1000 | Loss: 0.00003350
Iteration 108/1000 | Loss: 0.00003349
Iteration 109/1000 | Loss: 0.00003349
Iteration 110/1000 | Loss: 0.00003348
Iteration 111/1000 | Loss: 0.00003347
Iteration 112/1000 | Loss: 0.00003346
Iteration 113/1000 | Loss: 0.00003346
Iteration 114/1000 | Loss: 0.00003346
Iteration 115/1000 | Loss: 0.00003345
Iteration 116/1000 | Loss: 0.00003344
Iteration 117/1000 | Loss: 0.00003343
Iteration 118/1000 | Loss: 0.00003343
Iteration 119/1000 | Loss: 0.00003342
Iteration 120/1000 | Loss: 0.00003341
Iteration 121/1000 | Loss: 0.00003340
Iteration 122/1000 | Loss: 0.00003339
Iteration 123/1000 | Loss: 0.00003339
Iteration 124/1000 | Loss: 0.00003339
Iteration 125/1000 | Loss: 0.00003339
Iteration 126/1000 | Loss: 0.00003339
Iteration 127/1000 | Loss: 0.00003338
Iteration 128/1000 | Loss: 0.00003338
Iteration 129/1000 | Loss: 0.00003338
Iteration 130/1000 | Loss: 0.00003338
Iteration 131/1000 | Loss: 0.00003338
Iteration 132/1000 | Loss: 0.00003338
Iteration 133/1000 | Loss: 0.00003337
Iteration 134/1000 | Loss: 0.00003337
Iteration 135/1000 | Loss: 0.00003337
Iteration 136/1000 | Loss: 0.00003337
Iteration 137/1000 | Loss: 0.00003336
Iteration 138/1000 | Loss: 0.00003336
Iteration 139/1000 | Loss: 0.00003336
Iteration 140/1000 | Loss: 0.00003336
Iteration 141/1000 | Loss: 0.00003336
Iteration 142/1000 | Loss: 0.00003336
Iteration 143/1000 | Loss: 0.00003336
Iteration 144/1000 | Loss: 0.00003336
Iteration 145/1000 | Loss: 0.00003336
Iteration 146/1000 | Loss: 0.00003335
Iteration 147/1000 | Loss: 0.00003335
Iteration 148/1000 | Loss: 0.00003335
Iteration 149/1000 | Loss: 0.00003335
Iteration 150/1000 | Loss: 0.00003335
Iteration 151/1000 | Loss: 0.00003334
Iteration 152/1000 | Loss: 0.00003334
Iteration 153/1000 | Loss: 0.00003333
Iteration 154/1000 | Loss: 0.00003333
Iteration 155/1000 | Loss: 0.00003333
Iteration 156/1000 | Loss: 0.00003333
Iteration 157/1000 | Loss: 0.00003333
Iteration 158/1000 | Loss: 0.00003333
Iteration 159/1000 | Loss: 0.00003333
Iteration 160/1000 | Loss: 0.00003333
Iteration 161/1000 | Loss: 0.00003333
Iteration 162/1000 | Loss: 0.00003332
Iteration 163/1000 | Loss: 0.00003332
Iteration 164/1000 | Loss: 0.00003332
Iteration 165/1000 | Loss: 0.00003331
Iteration 166/1000 | Loss: 0.00003331
Iteration 167/1000 | Loss: 0.00003331
Iteration 168/1000 | Loss: 0.00003330
Iteration 169/1000 | Loss: 0.00003330
Iteration 170/1000 | Loss: 0.00003330
Iteration 171/1000 | Loss: 0.00003330
Iteration 172/1000 | Loss: 0.00003330
Iteration 173/1000 | Loss: 0.00003330
Iteration 174/1000 | Loss: 0.00003330
Iteration 175/1000 | Loss: 0.00003329
Iteration 176/1000 | Loss: 0.00003329
Iteration 177/1000 | Loss: 0.00003329
Iteration 178/1000 | Loss: 0.00003329
Iteration 179/1000 | Loss: 0.00003329
Iteration 180/1000 | Loss: 0.00003328
Iteration 181/1000 | Loss: 0.00003328
Iteration 182/1000 | Loss: 0.00003328
Iteration 183/1000 | Loss: 0.00003327
Iteration 184/1000 | Loss: 0.00003327
Iteration 185/1000 | Loss: 0.00003327
Iteration 186/1000 | Loss: 0.00003327
Iteration 187/1000 | Loss: 0.00003327
Iteration 188/1000 | Loss: 0.00003326
Iteration 189/1000 | Loss: 0.00003326
Iteration 190/1000 | Loss: 0.00003326
Iteration 191/1000 | Loss: 0.00003326
Iteration 192/1000 | Loss: 0.00003326
Iteration 193/1000 | Loss: 0.00003326
Iteration 194/1000 | Loss: 0.00003326
Iteration 195/1000 | Loss: 0.00003326
Iteration 196/1000 | Loss: 0.00003326
Iteration 197/1000 | Loss: 0.00003326
Iteration 198/1000 | Loss: 0.00003326
Iteration 199/1000 | Loss: 0.00003326
Iteration 200/1000 | Loss: 0.00003326
Iteration 201/1000 | Loss: 0.00003326
Iteration 202/1000 | Loss: 0.00003326
Iteration 203/1000 | Loss: 0.00003326
Iteration 204/1000 | Loss: 0.00003326
Iteration 205/1000 | Loss: 0.00003326
Iteration 206/1000 | Loss: 0.00003326
Iteration 207/1000 | Loss: 0.00003325
Iteration 208/1000 | Loss: 0.00003325
Iteration 209/1000 | Loss: 0.00003325
Iteration 210/1000 | Loss: 0.00003325
Iteration 211/1000 | Loss: 0.00003325
Iteration 212/1000 | Loss: 0.00003325
Iteration 213/1000 | Loss: 0.00003325
Iteration 214/1000 | Loss: 0.00003325
Iteration 215/1000 | Loss: 0.00003325
Iteration 216/1000 | Loss: 0.00003325
Iteration 217/1000 | Loss: 0.00003325
Iteration 218/1000 | Loss: 0.00003325
Iteration 219/1000 | Loss: 0.00003325
Iteration 220/1000 | Loss: 0.00003325
Iteration 221/1000 | Loss: 0.00003325
Iteration 222/1000 | Loss: 0.00003325
Iteration 223/1000 | Loss: 0.00003325
Iteration 224/1000 | Loss: 0.00003325
Iteration 225/1000 | Loss: 0.00003325
Iteration 226/1000 | Loss: 0.00003325
Iteration 227/1000 | Loss: 0.00003325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [3.32542258547619e-05, 3.32542258547619e-05, 3.32542258547619e-05, 3.32542258547619e-05, 3.32542258547619e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.32542258547619e-05

Optimization complete. Final v2v error: 4.451279163360596 mm

Highest mean error: 6.494019508361816 mm for frame 63

Lowest mean error: 4.091298580169678 mm for frame 163

Saving results

Total time: 209.147479057312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003595
Iteration 2/25 | Loss: 0.00218119
Iteration 3/25 | Loss: 0.00178689
Iteration 4/25 | Loss: 0.00163720
Iteration 5/25 | Loss: 0.00154320
Iteration 6/25 | Loss: 0.00157967
Iteration 7/25 | Loss: 0.00152151
Iteration 8/25 | Loss: 0.00141551
Iteration 9/25 | Loss: 0.00139314
Iteration 10/25 | Loss: 0.00134741
Iteration 11/25 | Loss: 0.00133412
Iteration 12/25 | Loss: 0.00131235
Iteration 13/25 | Loss: 0.00133046
Iteration 14/25 | Loss: 0.00131906
Iteration 15/25 | Loss: 0.00129737
Iteration 16/25 | Loss: 0.00127067
Iteration 17/25 | Loss: 0.00126896
Iteration 18/25 | Loss: 0.00125953
Iteration 19/25 | Loss: 0.00126201
Iteration 20/25 | Loss: 0.00126027
Iteration 21/25 | Loss: 0.00125990
Iteration 22/25 | Loss: 0.00126222
Iteration 23/25 | Loss: 0.00126265
Iteration 24/25 | Loss: 0.00126135
Iteration 25/25 | Loss: 0.00125835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35196495
Iteration 2/25 | Loss: 0.00125456
Iteration 3/25 | Loss: 0.00120724
Iteration 4/25 | Loss: 0.00120724
Iteration 5/25 | Loss: 0.00120724
Iteration 6/25 | Loss: 0.00120724
Iteration 7/25 | Loss: 0.00120724
Iteration 8/25 | Loss: 0.00120724
Iteration 9/25 | Loss: 0.00120724
Iteration 10/25 | Loss: 0.00120724
Iteration 11/25 | Loss: 0.00120724
Iteration 12/25 | Loss: 0.00120723
Iteration 13/25 | Loss: 0.00120723
Iteration 14/25 | Loss: 0.00120723
Iteration 15/25 | Loss: 0.00120723
Iteration 16/25 | Loss: 0.00120723
Iteration 17/25 | Loss: 0.00120723
Iteration 18/25 | Loss: 0.00120723
Iteration 19/25 | Loss: 0.00120723
Iteration 20/25 | Loss: 0.00120723
Iteration 21/25 | Loss: 0.00120723
Iteration 22/25 | Loss: 0.00120723
Iteration 23/25 | Loss: 0.00120723
Iteration 24/25 | Loss: 0.00120723
Iteration 25/25 | Loss: 0.00120723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120723
Iteration 2/1000 | Loss: 0.00010419
Iteration 3/1000 | Loss: 0.00035083
Iteration 4/1000 | Loss: 0.00036736
Iteration 5/1000 | Loss: 0.00005717
Iteration 6/1000 | Loss: 0.00062024
Iteration 7/1000 | Loss: 0.00058976
Iteration 8/1000 | Loss: 0.00003461
Iteration 9/1000 | Loss: 0.00013548
Iteration 10/1000 | Loss: 0.00007154
Iteration 11/1000 | Loss: 0.00061043
Iteration 12/1000 | Loss: 0.00036438
Iteration 13/1000 | Loss: 0.00016914
Iteration 14/1000 | Loss: 0.00005516
Iteration 15/1000 | Loss: 0.00004881
Iteration 16/1000 | Loss: 0.00033810
Iteration 17/1000 | Loss: 0.00007087
Iteration 18/1000 | Loss: 0.00016552
Iteration 19/1000 | Loss: 0.00028543
Iteration 20/1000 | Loss: 0.00021044
Iteration 21/1000 | Loss: 0.00051798
Iteration 22/1000 | Loss: 0.00011699
Iteration 23/1000 | Loss: 0.00025425
Iteration 24/1000 | Loss: 0.00003728
Iteration 25/1000 | Loss: 0.00020841
Iteration 26/1000 | Loss: 0.00022485
Iteration 27/1000 | Loss: 0.00026188
Iteration 28/1000 | Loss: 0.00022047
Iteration 29/1000 | Loss: 0.00027960
Iteration 30/1000 | Loss: 0.00021446
Iteration 31/1000 | Loss: 0.00002720
Iteration 32/1000 | Loss: 0.00002359
Iteration 33/1000 | Loss: 0.00002098
Iteration 34/1000 | Loss: 0.00023799
Iteration 35/1000 | Loss: 0.00029027
Iteration 36/1000 | Loss: 0.00009459
Iteration 37/1000 | Loss: 0.00025130
Iteration 38/1000 | Loss: 0.00009548
Iteration 39/1000 | Loss: 0.00003217
Iteration 40/1000 | Loss: 0.00007190
Iteration 41/1000 | Loss: 0.00001785
Iteration 42/1000 | Loss: 0.00001676
Iteration 43/1000 | Loss: 0.00006727
Iteration 44/1000 | Loss: 0.00001596
Iteration 45/1000 | Loss: 0.00001550
Iteration 46/1000 | Loss: 0.00011057
Iteration 47/1000 | Loss: 0.00024377
Iteration 48/1000 | Loss: 0.00001549
Iteration 49/1000 | Loss: 0.00001470
Iteration 50/1000 | Loss: 0.00001431
Iteration 51/1000 | Loss: 0.00033116
Iteration 52/1000 | Loss: 0.00121266
Iteration 53/1000 | Loss: 0.00012594
Iteration 54/1000 | Loss: 0.00001526
Iteration 55/1000 | Loss: 0.00012637
Iteration 56/1000 | Loss: 0.00001410
Iteration 57/1000 | Loss: 0.00001368
Iteration 58/1000 | Loss: 0.00001367
Iteration 59/1000 | Loss: 0.00012680
Iteration 60/1000 | Loss: 0.00001364
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001341
Iteration 63/1000 | Loss: 0.00001341
Iteration 64/1000 | Loss: 0.00001340
Iteration 65/1000 | Loss: 0.00001340
Iteration 66/1000 | Loss: 0.00001340
Iteration 67/1000 | Loss: 0.00001339
Iteration 68/1000 | Loss: 0.00001339
Iteration 69/1000 | Loss: 0.00001338
Iteration 70/1000 | Loss: 0.00001337
Iteration 71/1000 | Loss: 0.00001336
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001334
Iteration 80/1000 | Loss: 0.00001334
Iteration 81/1000 | Loss: 0.00001334
Iteration 82/1000 | Loss: 0.00001334
Iteration 83/1000 | Loss: 0.00001334
Iteration 84/1000 | Loss: 0.00001334
Iteration 85/1000 | Loss: 0.00001333
Iteration 86/1000 | Loss: 0.00001333
Iteration 87/1000 | Loss: 0.00001333
Iteration 88/1000 | Loss: 0.00001333
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001331
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001331
Iteration 100/1000 | Loss: 0.00001331
Iteration 101/1000 | Loss: 0.00001331
Iteration 102/1000 | Loss: 0.00001331
Iteration 103/1000 | Loss: 0.00001331
Iteration 104/1000 | Loss: 0.00001330
Iteration 105/1000 | Loss: 0.00001327
Iteration 106/1000 | Loss: 0.00001324
Iteration 107/1000 | Loss: 0.00001324
Iteration 108/1000 | Loss: 0.00026855
Iteration 109/1000 | Loss: 0.00010759
Iteration 110/1000 | Loss: 0.00002014
Iteration 111/1000 | Loss: 0.00001355
Iteration 112/1000 | Loss: 0.00001328
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001321
Iteration 116/1000 | Loss: 0.00001321
Iteration 117/1000 | Loss: 0.00001321
Iteration 118/1000 | Loss: 0.00001321
Iteration 119/1000 | Loss: 0.00001321
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001320
Iteration 122/1000 | Loss: 0.00001320
Iteration 123/1000 | Loss: 0.00001320
Iteration 124/1000 | Loss: 0.00001320
Iteration 125/1000 | Loss: 0.00001320
Iteration 126/1000 | Loss: 0.00001320
Iteration 127/1000 | Loss: 0.00001320
Iteration 128/1000 | Loss: 0.00001319
Iteration 129/1000 | Loss: 0.00001319
Iteration 130/1000 | Loss: 0.00001319
Iteration 131/1000 | Loss: 0.00001319
Iteration 132/1000 | Loss: 0.00001318
Iteration 133/1000 | Loss: 0.00001318
Iteration 134/1000 | Loss: 0.00001317
Iteration 135/1000 | Loss: 0.00001316
Iteration 136/1000 | Loss: 0.00001316
Iteration 137/1000 | Loss: 0.00001316
Iteration 138/1000 | Loss: 0.00001316
Iteration 139/1000 | Loss: 0.00001316
Iteration 140/1000 | Loss: 0.00001316
Iteration 141/1000 | Loss: 0.00001316
Iteration 142/1000 | Loss: 0.00001316
Iteration 143/1000 | Loss: 0.00001316
Iteration 144/1000 | Loss: 0.00001316
Iteration 145/1000 | Loss: 0.00001316
Iteration 146/1000 | Loss: 0.00001316
Iteration 147/1000 | Loss: 0.00001315
Iteration 148/1000 | Loss: 0.00001315
Iteration 149/1000 | Loss: 0.00001315
Iteration 150/1000 | Loss: 0.00001315
Iteration 151/1000 | Loss: 0.00001315
Iteration 152/1000 | Loss: 0.00001315
Iteration 153/1000 | Loss: 0.00001314
Iteration 154/1000 | Loss: 0.00001314
Iteration 155/1000 | Loss: 0.00001314
Iteration 156/1000 | Loss: 0.00001314
Iteration 157/1000 | Loss: 0.00001314
Iteration 158/1000 | Loss: 0.00001314
Iteration 159/1000 | Loss: 0.00001314
Iteration 160/1000 | Loss: 0.00001314
Iteration 161/1000 | Loss: 0.00001314
Iteration 162/1000 | Loss: 0.00001314
Iteration 163/1000 | Loss: 0.00001314
Iteration 164/1000 | Loss: 0.00001314
Iteration 165/1000 | Loss: 0.00001314
Iteration 166/1000 | Loss: 0.00001314
Iteration 167/1000 | Loss: 0.00001314
Iteration 168/1000 | Loss: 0.00001314
Iteration 169/1000 | Loss: 0.00001314
Iteration 170/1000 | Loss: 0.00001314
Iteration 171/1000 | Loss: 0.00001313
Iteration 172/1000 | Loss: 0.00001313
Iteration 173/1000 | Loss: 0.00001313
Iteration 174/1000 | Loss: 0.00001313
Iteration 175/1000 | Loss: 0.00001313
Iteration 176/1000 | Loss: 0.00001313
Iteration 177/1000 | Loss: 0.00001313
Iteration 178/1000 | Loss: 0.00001313
Iteration 179/1000 | Loss: 0.00001312
Iteration 180/1000 | Loss: 0.00001312
Iteration 181/1000 | Loss: 0.00001312
Iteration 182/1000 | Loss: 0.00001312
Iteration 183/1000 | Loss: 0.00001312
Iteration 184/1000 | Loss: 0.00001312
Iteration 185/1000 | Loss: 0.00001312
Iteration 186/1000 | Loss: 0.00001312
Iteration 187/1000 | Loss: 0.00001311
Iteration 188/1000 | Loss: 0.00001311
Iteration 189/1000 | Loss: 0.00001311
Iteration 190/1000 | Loss: 0.00001311
Iteration 191/1000 | Loss: 0.00001311
Iteration 192/1000 | Loss: 0.00001311
Iteration 193/1000 | Loss: 0.00001311
Iteration 194/1000 | Loss: 0.00001311
Iteration 195/1000 | Loss: 0.00001311
Iteration 196/1000 | Loss: 0.00001310
Iteration 197/1000 | Loss: 0.00001310
Iteration 198/1000 | Loss: 0.00001310
Iteration 199/1000 | Loss: 0.00001310
Iteration 200/1000 | Loss: 0.00001310
Iteration 201/1000 | Loss: 0.00001310
Iteration 202/1000 | Loss: 0.00001310
Iteration 203/1000 | Loss: 0.00001310
Iteration 204/1000 | Loss: 0.00001309
Iteration 205/1000 | Loss: 0.00001309
Iteration 206/1000 | Loss: 0.00001309
Iteration 207/1000 | Loss: 0.00001309
Iteration 208/1000 | Loss: 0.00001309
Iteration 209/1000 | Loss: 0.00001309
Iteration 210/1000 | Loss: 0.00001309
Iteration 211/1000 | Loss: 0.00001309
Iteration 212/1000 | Loss: 0.00001309
Iteration 213/1000 | Loss: 0.00001309
Iteration 214/1000 | Loss: 0.00001309
Iteration 215/1000 | Loss: 0.00001309
Iteration 216/1000 | Loss: 0.00001309
Iteration 217/1000 | Loss: 0.00001309
Iteration 218/1000 | Loss: 0.00001309
Iteration 219/1000 | Loss: 0.00001309
Iteration 220/1000 | Loss: 0.00001309
Iteration 221/1000 | Loss: 0.00001309
Iteration 222/1000 | Loss: 0.00001309
Iteration 223/1000 | Loss: 0.00001309
Iteration 224/1000 | Loss: 0.00001308
Iteration 225/1000 | Loss: 0.00001308
Iteration 226/1000 | Loss: 0.00001308
Iteration 227/1000 | Loss: 0.00001308
Iteration 228/1000 | Loss: 0.00001308
Iteration 229/1000 | Loss: 0.00001308
Iteration 230/1000 | Loss: 0.00001308
Iteration 231/1000 | Loss: 0.00001308
Iteration 232/1000 | Loss: 0.00001308
Iteration 233/1000 | Loss: 0.00001308
Iteration 234/1000 | Loss: 0.00001308
Iteration 235/1000 | Loss: 0.00001308
Iteration 236/1000 | Loss: 0.00001308
Iteration 237/1000 | Loss: 0.00001308
Iteration 238/1000 | Loss: 0.00001308
Iteration 239/1000 | Loss: 0.00001308
Iteration 240/1000 | Loss: 0.00001308
Iteration 241/1000 | Loss: 0.00001308
Iteration 242/1000 | Loss: 0.00001308
Iteration 243/1000 | Loss: 0.00001308
Iteration 244/1000 | Loss: 0.00001308
Iteration 245/1000 | Loss: 0.00001308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.3077664334559813e-05, 1.3077664334559813e-05, 1.3077664334559813e-05, 1.3077664334559813e-05, 1.3077664334559813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3077664334559813e-05

Optimization complete. Final v2v error: 3.026972532272339 mm

Highest mean error: 5.500035285949707 mm for frame 41

Lowest mean error: 2.719045400619507 mm for frame 27

Saving results

Total time: 145.55706596374512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00751234
Iteration 2/25 | Loss: 0.00157847
Iteration 3/25 | Loss: 0.00129242
Iteration 4/25 | Loss: 0.00127877
Iteration 5/25 | Loss: 0.00127848
Iteration 6/25 | Loss: 0.00127848
Iteration 7/25 | Loss: 0.00127848
Iteration 8/25 | Loss: 0.00127848
Iteration 9/25 | Loss: 0.00127848
Iteration 10/25 | Loss: 0.00127848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001278484589420259, 0.001278484589420259, 0.001278484589420259, 0.001278484589420259, 0.001278484589420259]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001278484589420259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31015360
Iteration 2/25 | Loss: 0.00072461
Iteration 3/25 | Loss: 0.00072459
Iteration 4/25 | Loss: 0.00072459
Iteration 5/25 | Loss: 0.00072459
Iteration 6/25 | Loss: 0.00072459
Iteration 7/25 | Loss: 0.00072459
Iteration 8/25 | Loss: 0.00072459
Iteration 9/25 | Loss: 0.00072459
Iteration 10/25 | Loss: 0.00072459
Iteration 11/25 | Loss: 0.00072459
Iteration 12/25 | Loss: 0.00072459
Iteration 13/25 | Loss: 0.00072459
Iteration 14/25 | Loss: 0.00072459
Iteration 15/25 | Loss: 0.00072459
Iteration 16/25 | Loss: 0.00072459
Iteration 17/25 | Loss: 0.00072459
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007245861925184727, 0.0007245861925184727, 0.0007245861925184727, 0.0007245861925184727, 0.0007245861925184727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007245861925184727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072459
Iteration 2/1000 | Loss: 0.00002845
Iteration 3/1000 | Loss: 0.00001981
Iteration 4/1000 | Loss: 0.00001799
Iteration 5/1000 | Loss: 0.00001671
Iteration 6/1000 | Loss: 0.00001603
Iteration 7/1000 | Loss: 0.00001560
Iteration 8/1000 | Loss: 0.00001517
Iteration 9/1000 | Loss: 0.00001468
Iteration 10/1000 | Loss: 0.00001456
Iteration 11/1000 | Loss: 0.00001447
Iteration 12/1000 | Loss: 0.00001428
Iteration 13/1000 | Loss: 0.00001410
Iteration 14/1000 | Loss: 0.00001408
Iteration 15/1000 | Loss: 0.00001406
Iteration 16/1000 | Loss: 0.00001405
Iteration 17/1000 | Loss: 0.00001403
Iteration 18/1000 | Loss: 0.00001401
Iteration 19/1000 | Loss: 0.00001396
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001395
Iteration 22/1000 | Loss: 0.00001394
Iteration 23/1000 | Loss: 0.00001393
Iteration 24/1000 | Loss: 0.00001387
Iteration 25/1000 | Loss: 0.00001380
Iteration 26/1000 | Loss: 0.00001379
Iteration 27/1000 | Loss: 0.00001371
Iteration 28/1000 | Loss: 0.00001367
Iteration 29/1000 | Loss: 0.00001366
Iteration 30/1000 | Loss: 0.00001365
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001350
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001350
Iteration 37/1000 | Loss: 0.00001350
Iteration 38/1000 | Loss: 0.00001349
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001349
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001345
Iteration 49/1000 | Loss: 0.00001343
Iteration 50/1000 | Loss: 0.00001343
Iteration 51/1000 | Loss: 0.00001343
Iteration 52/1000 | Loss: 0.00001343
Iteration 53/1000 | Loss: 0.00001343
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001343
Iteration 57/1000 | Loss: 0.00001343
Iteration 58/1000 | Loss: 0.00001342
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001342
Iteration 65/1000 | Loss: 0.00001342
Iteration 66/1000 | Loss: 0.00001342
Iteration 67/1000 | Loss: 0.00001341
Iteration 68/1000 | Loss: 0.00001338
Iteration 69/1000 | Loss: 0.00001338
Iteration 70/1000 | Loss: 0.00001337
Iteration 71/1000 | Loss: 0.00001337
Iteration 72/1000 | Loss: 0.00001336
Iteration 73/1000 | Loss: 0.00001336
Iteration 74/1000 | Loss: 0.00001336
Iteration 75/1000 | Loss: 0.00001335
Iteration 76/1000 | Loss: 0.00001335
Iteration 77/1000 | Loss: 0.00001335
Iteration 78/1000 | Loss: 0.00001335
Iteration 79/1000 | Loss: 0.00001335
Iteration 80/1000 | Loss: 0.00001334
Iteration 81/1000 | Loss: 0.00001334
Iteration 82/1000 | Loss: 0.00001334
Iteration 83/1000 | Loss: 0.00001334
Iteration 84/1000 | Loss: 0.00001334
Iteration 85/1000 | Loss: 0.00001333
Iteration 86/1000 | Loss: 0.00001333
Iteration 87/1000 | Loss: 0.00001333
Iteration 88/1000 | Loss: 0.00001333
Iteration 89/1000 | Loss: 0.00001333
Iteration 90/1000 | Loss: 0.00001332
Iteration 91/1000 | Loss: 0.00001332
Iteration 92/1000 | Loss: 0.00001332
Iteration 93/1000 | Loss: 0.00001332
Iteration 94/1000 | Loss: 0.00001332
Iteration 95/1000 | Loss: 0.00001332
Iteration 96/1000 | Loss: 0.00001332
Iteration 97/1000 | Loss: 0.00001332
Iteration 98/1000 | Loss: 0.00001332
Iteration 99/1000 | Loss: 0.00001331
Iteration 100/1000 | Loss: 0.00001331
Iteration 101/1000 | Loss: 0.00001331
Iteration 102/1000 | Loss: 0.00001331
Iteration 103/1000 | Loss: 0.00001331
Iteration 104/1000 | Loss: 0.00001330
Iteration 105/1000 | Loss: 0.00001330
Iteration 106/1000 | Loss: 0.00001330
Iteration 107/1000 | Loss: 0.00001330
Iteration 108/1000 | Loss: 0.00001330
Iteration 109/1000 | Loss: 0.00001330
Iteration 110/1000 | Loss: 0.00001330
Iteration 111/1000 | Loss: 0.00001330
Iteration 112/1000 | Loss: 0.00001330
Iteration 113/1000 | Loss: 0.00001330
Iteration 114/1000 | Loss: 0.00001330
Iteration 115/1000 | Loss: 0.00001330
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001330
Iteration 118/1000 | Loss: 0.00001330
Iteration 119/1000 | Loss: 0.00001330
Iteration 120/1000 | Loss: 0.00001330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.3295159078552388e-05, 1.3295159078552388e-05, 1.3295159078552388e-05, 1.3295159078552388e-05, 1.3295159078552388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3295159078552388e-05

Optimization complete. Final v2v error: 3.0796778202056885 mm

Highest mean error: 3.282992362976074 mm for frame 135

Lowest mean error: 2.932347059249878 mm for frame 175

Saving results

Total time: 40.70105290412903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00610307
Iteration 2/25 | Loss: 0.00143181
Iteration 3/25 | Loss: 0.00129263
Iteration 4/25 | Loss: 0.00127645
Iteration 5/25 | Loss: 0.00127460
Iteration 6/25 | Loss: 0.00127460
Iteration 7/25 | Loss: 0.00127460
Iteration 8/25 | Loss: 0.00127460
Iteration 9/25 | Loss: 0.00127460
Iteration 10/25 | Loss: 0.00127460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001274602022022009, 0.001274602022022009, 0.001274602022022009, 0.001274602022022009, 0.001274602022022009]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001274602022022009

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.54359317
Iteration 2/25 | Loss: 0.00101052
Iteration 3/25 | Loss: 0.00101052
Iteration 4/25 | Loss: 0.00101052
Iteration 5/25 | Loss: 0.00101052
Iteration 6/25 | Loss: 0.00101052
Iteration 7/25 | Loss: 0.00101052
Iteration 8/25 | Loss: 0.00101052
Iteration 9/25 | Loss: 0.00101052
Iteration 10/25 | Loss: 0.00101052
Iteration 11/25 | Loss: 0.00101052
Iteration 12/25 | Loss: 0.00101052
Iteration 13/25 | Loss: 0.00101052
Iteration 14/25 | Loss: 0.00101052
Iteration 15/25 | Loss: 0.00101052
Iteration 16/25 | Loss: 0.00101052
Iteration 17/25 | Loss: 0.00101052
Iteration 18/25 | Loss: 0.00101052
Iteration 19/25 | Loss: 0.00101052
Iteration 20/25 | Loss: 0.00101052
Iteration 21/25 | Loss: 0.00101052
Iteration 22/25 | Loss: 0.00101052
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010105168912559748, 0.0010105168912559748, 0.0010105168912559748, 0.0010105168912559748, 0.0010105168912559748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010105168912559748

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101052
Iteration 2/1000 | Loss: 0.00004281
Iteration 3/1000 | Loss: 0.00002890
Iteration 4/1000 | Loss: 0.00002592
Iteration 5/1000 | Loss: 0.00002482
Iteration 6/1000 | Loss: 0.00002395
Iteration 7/1000 | Loss: 0.00002331
Iteration 8/1000 | Loss: 0.00002296
Iteration 9/1000 | Loss: 0.00002271
Iteration 10/1000 | Loss: 0.00002238
Iteration 11/1000 | Loss: 0.00002228
Iteration 12/1000 | Loss: 0.00002227
Iteration 13/1000 | Loss: 0.00002206
Iteration 14/1000 | Loss: 0.00002189
Iteration 15/1000 | Loss: 0.00002188
Iteration 16/1000 | Loss: 0.00002180
Iteration 17/1000 | Loss: 0.00002179
Iteration 18/1000 | Loss: 0.00002179
Iteration 19/1000 | Loss: 0.00002179
Iteration 20/1000 | Loss: 0.00002179
Iteration 21/1000 | Loss: 0.00002179
Iteration 22/1000 | Loss: 0.00002179
Iteration 23/1000 | Loss: 0.00002178
Iteration 24/1000 | Loss: 0.00002178
Iteration 25/1000 | Loss: 0.00002177
Iteration 26/1000 | Loss: 0.00002171
Iteration 27/1000 | Loss: 0.00002169
Iteration 28/1000 | Loss: 0.00002168
Iteration 29/1000 | Loss: 0.00002167
Iteration 30/1000 | Loss: 0.00002163
Iteration 31/1000 | Loss: 0.00002162
Iteration 32/1000 | Loss: 0.00002162
Iteration 33/1000 | Loss: 0.00002161
Iteration 34/1000 | Loss: 0.00002160
Iteration 35/1000 | Loss: 0.00002160
Iteration 36/1000 | Loss: 0.00002159
Iteration 37/1000 | Loss: 0.00002158
Iteration 38/1000 | Loss: 0.00002155
Iteration 39/1000 | Loss: 0.00002154
Iteration 40/1000 | Loss: 0.00002154
Iteration 41/1000 | Loss: 0.00002153
Iteration 42/1000 | Loss: 0.00002153
Iteration 43/1000 | Loss: 0.00002151
Iteration 44/1000 | Loss: 0.00002146
Iteration 45/1000 | Loss: 0.00002146
Iteration 46/1000 | Loss: 0.00002145
Iteration 47/1000 | Loss: 0.00002145
Iteration 48/1000 | Loss: 0.00002145
Iteration 49/1000 | Loss: 0.00002145
Iteration 50/1000 | Loss: 0.00002145
Iteration 51/1000 | Loss: 0.00002140
Iteration 52/1000 | Loss: 0.00002139
Iteration 53/1000 | Loss: 0.00002139
Iteration 54/1000 | Loss: 0.00002138
Iteration 55/1000 | Loss: 0.00002136
Iteration 56/1000 | Loss: 0.00002135
Iteration 57/1000 | Loss: 0.00002135
Iteration 58/1000 | Loss: 0.00002134
Iteration 59/1000 | Loss: 0.00002134
Iteration 60/1000 | Loss: 0.00002134
Iteration 61/1000 | Loss: 0.00002133
Iteration 62/1000 | Loss: 0.00002133
Iteration 63/1000 | Loss: 0.00002133
Iteration 64/1000 | Loss: 0.00002129
Iteration 65/1000 | Loss: 0.00002129
Iteration 66/1000 | Loss: 0.00002129
Iteration 67/1000 | Loss: 0.00002129
Iteration 68/1000 | Loss: 0.00002129
Iteration 69/1000 | Loss: 0.00002129
Iteration 70/1000 | Loss: 0.00002129
Iteration 71/1000 | Loss: 0.00002129
Iteration 72/1000 | Loss: 0.00002128
Iteration 73/1000 | Loss: 0.00002128
Iteration 74/1000 | Loss: 0.00002128
Iteration 75/1000 | Loss: 0.00002127
Iteration 76/1000 | Loss: 0.00002127
Iteration 77/1000 | Loss: 0.00002127
Iteration 78/1000 | Loss: 0.00002126
Iteration 79/1000 | Loss: 0.00002126
Iteration 80/1000 | Loss: 0.00002126
Iteration 81/1000 | Loss: 0.00002126
Iteration 82/1000 | Loss: 0.00002125
Iteration 83/1000 | Loss: 0.00002125
Iteration 84/1000 | Loss: 0.00002125
Iteration 85/1000 | Loss: 0.00002124
Iteration 86/1000 | Loss: 0.00002124
Iteration 87/1000 | Loss: 0.00002124
Iteration 88/1000 | Loss: 0.00002120
Iteration 89/1000 | Loss: 0.00002119
Iteration 90/1000 | Loss: 0.00002119
Iteration 91/1000 | Loss: 0.00002118
Iteration 92/1000 | Loss: 0.00002118
Iteration 93/1000 | Loss: 0.00002118
Iteration 94/1000 | Loss: 0.00002118
Iteration 95/1000 | Loss: 0.00002117
Iteration 96/1000 | Loss: 0.00002117
Iteration 97/1000 | Loss: 0.00002117
Iteration 98/1000 | Loss: 0.00002117
Iteration 99/1000 | Loss: 0.00002117
Iteration 100/1000 | Loss: 0.00002117
Iteration 101/1000 | Loss: 0.00002117
Iteration 102/1000 | Loss: 0.00002116
Iteration 103/1000 | Loss: 0.00002116
Iteration 104/1000 | Loss: 0.00002116
Iteration 105/1000 | Loss: 0.00002116
Iteration 106/1000 | Loss: 0.00002115
Iteration 107/1000 | Loss: 0.00002115
Iteration 108/1000 | Loss: 0.00002115
Iteration 109/1000 | Loss: 0.00002115
Iteration 110/1000 | Loss: 0.00002115
Iteration 111/1000 | Loss: 0.00002115
Iteration 112/1000 | Loss: 0.00002115
Iteration 113/1000 | Loss: 0.00002114
Iteration 114/1000 | Loss: 0.00002114
Iteration 115/1000 | Loss: 0.00002114
Iteration 116/1000 | Loss: 0.00002114
Iteration 117/1000 | Loss: 0.00002114
Iteration 118/1000 | Loss: 0.00002114
Iteration 119/1000 | Loss: 0.00002113
Iteration 120/1000 | Loss: 0.00002113
Iteration 121/1000 | Loss: 0.00002113
Iteration 122/1000 | Loss: 0.00002113
Iteration 123/1000 | Loss: 0.00002113
Iteration 124/1000 | Loss: 0.00002113
Iteration 125/1000 | Loss: 0.00002112
Iteration 126/1000 | Loss: 0.00002112
Iteration 127/1000 | Loss: 0.00002111
Iteration 128/1000 | Loss: 0.00002111
Iteration 129/1000 | Loss: 0.00002111
Iteration 130/1000 | Loss: 0.00002111
Iteration 131/1000 | Loss: 0.00002111
Iteration 132/1000 | Loss: 0.00002111
Iteration 133/1000 | Loss: 0.00002110
Iteration 134/1000 | Loss: 0.00002110
Iteration 135/1000 | Loss: 0.00002110
Iteration 136/1000 | Loss: 0.00002110
Iteration 137/1000 | Loss: 0.00002110
Iteration 138/1000 | Loss: 0.00002110
Iteration 139/1000 | Loss: 0.00002110
Iteration 140/1000 | Loss: 0.00002110
Iteration 141/1000 | Loss: 0.00002110
Iteration 142/1000 | Loss: 0.00002109
Iteration 143/1000 | Loss: 0.00002109
Iteration 144/1000 | Loss: 0.00002109
Iteration 145/1000 | Loss: 0.00002109
Iteration 146/1000 | Loss: 0.00002108
Iteration 147/1000 | Loss: 0.00002108
Iteration 148/1000 | Loss: 0.00002108
Iteration 149/1000 | Loss: 0.00002108
Iteration 150/1000 | Loss: 0.00002108
Iteration 151/1000 | Loss: 0.00002108
Iteration 152/1000 | Loss: 0.00002108
Iteration 153/1000 | Loss: 0.00002108
Iteration 154/1000 | Loss: 0.00002108
Iteration 155/1000 | Loss: 0.00002108
Iteration 156/1000 | Loss: 0.00002108
Iteration 157/1000 | Loss: 0.00002108
Iteration 158/1000 | Loss: 0.00002108
Iteration 159/1000 | Loss: 0.00002108
Iteration 160/1000 | Loss: 0.00002108
Iteration 161/1000 | Loss: 0.00002108
Iteration 162/1000 | Loss: 0.00002108
Iteration 163/1000 | Loss: 0.00002108
Iteration 164/1000 | Loss: 0.00002108
Iteration 165/1000 | Loss: 0.00002108
Iteration 166/1000 | Loss: 0.00002108
Iteration 167/1000 | Loss: 0.00002108
Iteration 168/1000 | Loss: 0.00002108
Iteration 169/1000 | Loss: 0.00002107
Iteration 170/1000 | Loss: 0.00002107
Iteration 171/1000 | Loss: 0.00002107
Iteration 172/1000 | Loss: 0.00002107
Iteration 173/1000 | Loss: 0.00002107
Iteration 174/1000 | Loss: 0.00002107
Iteration 175/1000 | Loss: 0.00002107
Iteration 176/1000 | Loss: 0.00002106
Iteration 177/1000 | Loss: 0.00002106
Iteration 178/1000 | Loss: 0.00002106
Iteration 179/1000 | Loss: 0.00002106
Iteration 180/1000 | Loss: 0.00002106
Iteration 181/1000 | Loss: 0.00002106
Iteration 182/1000 | Loss: 0.00002106
Iteration 183/1000 | Loss: 0.00002106
Iteration 184/1000 | Loss: 0.00002106
Iteration 185/1000 | Loss: 0.00002106
Iteration 186/1000 | Loss: 0.00002106
Iteration 187/1000 | Loss: 0.00002106
Iteration 188/1000 | Loss: 0.00002106
Iteration 189/1000 | Loss: 0.00002106
Iteration 190/1000 | Loss: 0.00002106
Iteration 191/1000 | Loss: 0.00002106
Iteration 192/1000 | Loss: 0.00002106
Iteration 193/1000 | Loss: 0.00002106
Iteration 194/1000 | Loss: 0.00002105
Iteration 195/1000 | Loss: 0.00002105
Iteration 196/1000 | Loss: 0.00002105
Iteration 197/1000 | Loss: 0.00002105
Iteration 198/1000 | Loss: 0.00002105
Iteration 199/1000 | Loss: 0.00002105
Iteration 200/1000 | Loss: 0.00002105
Iteration 201/1000 | Loss: 0.00002105
Iteration 202/1000 | Loss: 0.00002105
Iteration 203/1000 | Loss: 0.00002105
Iteration 204/1000 | Loss: 0.00002105
Iteration 205/1000 | Loss: 0.00002105
Iteration 206/1000 | Loss: 0.00002105
Iteration 207/1000 | Loss: 0.00002105
Iteration 208/1000 | Loss: 0.00002105
Iteration 209/1000 | Loss: 0.00002105
Iteration 210/1000 | Loss: 0.00002105
Iteration 211/1000 | Loss: 0.00002105
Iteration 212/1000 | Loss: 0.00002105
Iteration 213/1000 | Loss: 0.00002105
Iteration 214/1000 | Loss: 0.00002105
Iteration 215/1000 | Loss: 0.00002105
Iteration 216/1000 | Loss: 0.00002105
Iteration 217/1000 | Loss: 0.00002105
Iteration 218/1000 | Loss: 0.00002105
Iteration 219/1000 | Loss: 0.00002105
Iteration 220/1000 | Loss: 0.00002105
Iteration 221/1000 | Loss: 0.00002105
Iteration 222/1000 | Loss: 0.00002105
Iteration 223/1000 | Loss: 0.00002105
Iteration 224/1000 | Loss: 0.00002105
Iteration 225/1000 | Loss: 0.00002105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.104691884596832e-05, 2.104691884596832e-05, 2.104691884596832e-05, 2.104691884596832e-05, 2.104691884596832e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.104691884596832e-05

Optimization complete. Final v2v error: 3.885963201522827 mm

Highest mean error: 4.136691093444824 mm for frame 65

Lowest mean error: 3.7488958835601807 mm for frame 118

Saving results

Total time: 43.69108819961548
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837771
Iteration 2/25 | Loss: 0.00143542
Iteration 3/25 | Loss: 0.00123760
Iteration 4/25 | Loss: 0.00121682
Iteration 5/25 | Loss: 0.00121130
Iteration 6/25 | Loss: 0.00121059
Iteration 7/25 | Loss: 0.00121059
Iteration 8/25 | Loss: 0.00121059
Iteration 9/25 | Loss: 0.00121059
Iteration 10/25 | Loss: 0.00121059
Iteration 11/25 | Loss: 0.00121059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012105852365493774, 0.0012105852365493774, 0.0012105852365493774, 0.0012105852365493774, 0.0012105852365493774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012105852365493774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.67899418
Iteration 2/25 | Loss: 0.00106566
Iteration 3/25 | Loss: 0.00106566
Iteration 4/25 | Loss: 0.00106566
Iteration 5/25 | Loss: 0.00106566
Iteration 6/25 | Loss: 0.00106566
Iteration 7/25 | Loss: 0.00106566
Iteration 8/25 | Loss: 0.00106566
Iteration 9/25 | Loss: 0.00106566
Iteration 10/25 | Loss: 0.00106566
Iteration 11/25 | Loss: 0.00106566
Iteration 12/25 | Loss: 0.00106566
Iteration 13/25 | Loss: 0.00106566
Iteration 14/25 | Loss: 0.00106566
Iteration 15/25 | Loss: 0.00106566
Iteration 16/25 | Loss: 0.00106566
Iteration 17/25 | Loss: 0.00106566
Iteration 18/25 | Loss: 0.00106566
Iteration 19/25 | Loss: 0.00106566
Iteration 20/25 | Loss: 0.00106566
Iteration 21/25 | Loss: 0.00106566
Iteration 22/25 | Loss: 0.00106566
Iteration 23/25 | Loss: 0.00106566
Iteration 24/25 | Loss: 0.00106566
Iteration 25/25 | Loss: 0.00106566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106566
Iteration 2/1000 | Loss: 0.00002264
Iteration 3/1000 | Loss: 0.00001711
Iteration 4/1000 | Loss: 0.00001530
Iteration 5/1000 | Loss: 0.00001446
Iteration 6/1000 | Loss: 0.00001377
Iteration 7/1000 | Loss: 0.00001342
Iteration 8/1000 | Loss: 0.00001303
Iteration 9/1000 | Loss: 0.00001276
Iteration 10/1000 | Loss: 0.00001253
Iteration 11/1000 | Loss: 0.00001238
Iteration 12/1000 | Loss: 0.00001237
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001227
Iteration 15/1000 | Loss: 0.00001224
Iteration 16/1000 | Loss: 0.00001223
Iteration 17/1000 | Loss: 0.00001222
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001219
Iteration 20/1000 | Loss: 0.00001219
Iteration 21/1000 | Loss: 0.00001218
Iteration 22/1000 | Loss: 0.00001217
Iteration 23/1000 | Loss: 0.00001216
Iteration 24/1000 | Loss: 0.00001215
Iteration 25/1000 | Loss: 0.00001215
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001212
Iteration 28/1000 | Loss: 0.00001211
Iteration 29/1000 | Loss: 0.00001210
Iteration 30/1000 | Loss: 0.00001208
Iteration 31/1000 | Loss: 0.00001206
Iteration 32/1000 | Loss: 0.00001205
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001193
Iteration 37/1000 | Loss: 0.00001193
Iteration 38/1000 | Loss: 0.00001191
Iteration 39/1000 | Loss: 0.00001190
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001189
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001188
Iteration 46/1000 | Loss: 0.00001188
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001186
Iteration 50/1000 | Loss: 0.00001186
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001186
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001186
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001185
Iteration 57/1000 | Loss: 0.00001185
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001185
Iteration 60/1000 | Loss: 0.00001185
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001183
Iteration 64/1000 | Loss: 0.00001182
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001180
Iteration 67/1000 | Loss: 0.00001180
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001177
Iteration 73/1000 | Loss: 0.00001177
Iteration 74/1000 | Loss: 0.00001177
Iteration 75/1000 | Loss: 0.00001176
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001175
Iteration 78/1000 | Loss: 0.00001175
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001174
Iteration 81/1000 | Loss: 0.00001174
Iteration 82/1000 | Loss: 0.00001174
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001173
Iteration 85/1000 | Loss: 0.00001173
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001171
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.1710058970493264e-05, 1.1710058970493264e-05, 1.1710058970493264e-05, 1.1710058970493264e-05, 1.1710058970493264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1710058970493264e-05

Optimization complete. Final v2v error: 2.952686071395874 mm

Highest mean error: 3.270394802093506 mm for frame 190

Lowest mean error: 2.6722588539123535 mm for frame 0

Saving results

Total time: 39.27304744720459
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463658
Iteration 2/25 | Loss: 0.00133507
Iteration 3/25 | Loss: 0.00124734
Iteration 4/25 | Loss: 0.00123727
Iteration 5/25 | Loss: 0.00123517
Iteration 6/25 | Loss: 0.00123515
Iteration 7/25 | Loss: 0.00123515
Iteration 8/25 | Loss: 0.00123515
Iteration 9/25 | Loss: 0.00123515
Iteration 10/25 | Loss: 0.00123515
Iteration 11/25 | Loss: 0.00123515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012351543409749866, 0.0012351543409749866, 0.0012351543409749866, 0.0012351543409749866, 0.0012351543409749866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012351543409749866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34925342
Iteration 2/25 | Loss: 0.00114486
Iteration 3/25 | Loss: 0.00114485
Iteration 4/25 | Loss: 0.00114485
Iteration 5/25 | Loss: 0.00114485
Iteration 6/25 | Loss: 0.00114485
Iteration 7/25 | Loss: 0.00114485
Iteration 8/25 | Loss: 0.00114485
Iteration 9/25 | Loss: 0.00114485
Iteration 10/25 | Loss: 0.00114485
Iteration 11/25 | Loss: 0.00114485
Iteration 12/25 | Loss: 0.00114485
Iteration 13/25 | Loss: 0.00114485
Iteration 14/25 | Loss: 0.00114485
Iteration 15/25 | Loss: 0.00114485
Iteration 16/25 | Loss: 0.00114485
Iteration 17/25 | Loss: 0.00114485
Iteration 18/25 | Loss: 0.00114485
Iteration 19/25 | Loss: 0.00114485
Iteration 20/25 | Loss: 0.00114485
Iteration 21/25 | Loss: 0.00114485
Iteration 22/25 | Loss: 0.00114485
Iteration 23/25 | Loss: 0.00114485
Iteration 24/25 | Loss: 0.00114485
Iteration 25/25 | Loss: 0.00114485
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011448458535596728, 0.0011448458535596728, 0.0011448458535596728, 0.0011448458535596728, 0.0011448458535596728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011448458535596728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114485
Iteration 2/1000 | Loss: 0.00002473
Iteration 3/1000 | Loss: 0.00001619
Iteration 4/1000 | Loss: 0.00001435
Iteration 5/1000 | Loss: 0.00001309
Iteration 6/1000 | Loss: 0.00001240
Iteration 7/1000 | Loss: 0.00001196
Iteration 8/1000 | Loss: 0.00001167
Iteration 9/1000 | Loss: 0.00001147
Iteration 10/1000 | Loss: 0.00001143
Iteration 11/1000 | Loss: 0.00001142
Iteration 12/1000 | Loss: 0.00001139
Iteration 13/1000 | Loss: 0.00001135
Iteration 14/1000 | Loss: 0.00001126
Iteration 15/1000 | Loss: 0.00001126
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001126
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001122
Iteration 20/1000 | Loss: 0.00001121
Iteration 21/1000 | Loss: 0.00001117
Iteration 22/1000 | Loss: 0.00001114
Iteration 23/1000 | Loss: 0.00001109
Iteration 24/1000 | Loss: 0.00001106
Iteration 25/1000 | Loss: 0.00001105
Iteration 26/1000 | Loss: 0.00001104
Iteration 27/1000 | Loss: 0.00001104
Iteration 28/1000 | Loss: 0.00001104
Iteration 29/1000 | Loss: 0.00001103
Iteration 30/1000 | Loss: 0.00001103
Iteration 31/1000 | Loss: 0.00001102
Iteration 32/1000 | Loss: 0.00001101
Iteration 33/1000 | Loss: 0.00001100
Iteration 34/1000 | Loss: 0.00001099
Iteration 35/1000 | Loss: 0.00001098
Iteration 36/1000 | Loss: 0.00001098
Iteration 37/1000 | Loss: 0.00001097
Iteration 38/1000 | Loss: 0.00001097
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001097
Iteration 41/1000 | Loss: 0.00001096
Iteration 42/1000 | Loss: 0.00001095
Iteration 43/1000 | Loss: 0.00001091
Iteration 44/1000 | Loss: 0.00001091
Iteration 45/1000 | Loss: 0.00001090
Iteration 46/1000 | Loss: 0.00001090
Iteration 47/1000 | Loss: 0.00001089
Iteration 48/1000 | Loss: 0.00001088
Iteration 49/1000 | Loss: 0.00001087
Iteration 50/1000 | Loss: 0.00001087
Iteration 51/1000 | Loss: 0.00001087
Iteration 52/1000 | Loss: 0.00001086
Iteration 53/1000 | Loss: 0.00001086
Iteration 54/1000 | Loss: 0.00001086
Iteration 55/1000 | Loss: 0.00001086
Iteration 56/1000 | Loss: 0.00001086
Iteration 57/1000 | Loss: 0.00001085
Iteration 58/1000 | Loss: 0.00001085
Iteration 59/1000 | Loss: 0.00001084
Iteration 60/1000 | Loss: 0.00001084
Iteration 61/1000 | Loss: 0.00001084
Iteration 62/1000 | Loss: 0.00001084
Iteration 63/1000 | Loss: 0.00001084
Iteration 64/1000 | Loss: 0.00001083
Iteration 65/1000 | Loss: 0.00001083
Iteration 66/1000 | Loss: 0.00001083
Iteration 67/1000 | Loss: 0.00001083
Iteration 68/1000 | Loss: 0.00001083
Iteration 69/1000 | Loss: 0.00001083
Iteration 70/1000 | Loss: 0.00001083
Iteration 71/1000 | Loss: 0.00001083
Iteration 72/1000 | Loss: 0.00001082
Iteration 73/1000 | Loss: 0.00001082
Iteration 74/1000 | Loss: 0.00001082
Iteration 75/1000 | Loss: 0.00001082
Iteration 76/1000 | Loss: 0.00001082
Iteration 77/1000 | Loss: 0.00001081
Iteration 78/1000 | Loss: 0.00001081
Iteration 79/1000 | Loss: 0.00001081
Iteration 80/1000 | Loss: 0.00001080
Iteration 81/1000 | Loss: 0.00001079
Iteration 82/1000 | Loss: 0.00001079
Iteration 83/1000 | Loss: 0.00001078
Iteration 84/1000 | Loss: 0.00001078
Iteration 85/1000 | Loss: 0.00001078
Iteration 86/1000 | Loss: 0.00001077
Iteration 87/1000 | Loss: 0.00001077
Iteration 88/1000 | Loss: 0.00001077
Iteration 89/1000 | Loss: 0.00001076
Iteration 90/1000 | Loss: 0.00001076
Iteration 91/1000 | Loss: 0.00001075
Iteration 92/1000 | Loss: 0.00001075
Iteration 93/1000 | Loss: 0.00001074
Iteration 94/1000 | Loss: 0.00001074
Iteration 95/1000 | Loss: 0.00001074
Iteration 96/1000 | Loss: 0.00001074
Iteration 97/1000 | Loss: 0.00001074
Iteration 98/1000 | Loss: 0.00001074
Iteration 99/1000 | Loss: 0.00001073
Iteration 100/1000 | Loss: 0.00001072
Iteration 101/1000 | Loss: 0.00001072
Iteration 102/1000 | Loss: 0.00001072
Iteration 103/1000 | Loss: 0.00001071
Iteration 104/1000 | Loss: 0.00001071
Iteration 105/1000 | Loss: 0.00001071
Iteration 106/1000 | Loss: 0.00001071
Iteration 107/1000 | Loss: 0.00001071
Iteration 108/1000 | Loss: 0.00001071
Iteration 109/1000 | Loss: 0.00001070
Iteration 110/1000 | Loss: 0.00001070
Iteration 111/1000 | Loss: 0.00001069
Iteration 112/1000 | Loss: 0.00001069
Iteration 113/1000 | Loss: 0.00001069
Iteration 114/1000 | Loss: 0.00001068
Iteration 115/1000 | Loss: 0.00001068
Iteration 116/1000 | Loss: 0.00001067
Iteration 117/1000 | Loss: 0.00001067
Iteration 118/1000 | Loss: 0.00001067
Iteration 119/1000 | Loss: 0.00001067
Iteration 120/1000 | Loss: 0.00001066
Iteration 121/1000 | Loss: 0.00001066
Iteration 122/1000 | Loss: 0.00001065
Iteration 123/1000 | Loss: 0.00001065
Iteration 124/1000 | Loss: 0.00001064
Iteration 125/1000 | Loss: 0.00001064
Iteration 126/1000 | Loss: 0.00001064
Iteration 127/1000 | Loss: 0.00001064
Iteration 128/1000 | Loss: 0.00001063
Iteration 129/1000 | Loss: 0.00001063
Iteration 130/1000 | Loss: 0.00001063
Iteration 131/1000 | Loss: 0.00001062
Iteration 132/1000 | Loss: 0.00001062
Iteration 133/1000 | Loss: 0.00001061
Iteration 134/1000 | Loss: 0.00001061
Iteration 135/1000 | Loss: 0.00001060
Iteration 136/1000 | Loss: 0.00001060
Iteration 137/1000 | Loss: 0.00001060
Iteration 138/1000 | Loss: 0.00001060
Iteration 139/1000 | Loss: 0.00001060
Iteration 140/1000 | Loss: 0.00001059
Iteration 141/1000 | Loss: 0.00001059
Iteration 142/1000 | Loss: 0.00001059
Iteration 143/1000 | Loss: 0.00001059
Iteration 144/1000 | Loss: 0.00001059
Iteration 145/1000 | Loss: 0.00001059
Iteration 146/1000 | Loss: 0.00001059
Iteration 147/1000 | Loss: 0.00001059
Iteration 148/1000 | Loss: 0.00001059
Iteration 149/1000 | Loss: 0.00001059
Iteration 150/1000 | Loss: 0.00001058
Iteration 151/1000 | Loss: 0.00001056
Iteration 152/1000 | Loss: 0.00001056
Iteration 153/1000 | Loss: 0.00001056
Iteration 154/1000 | Loss: 0.00001056
Iteration 155/1000 | Loss: 0.00001056
Iteration 156/1000 | Loss: 0.00001056
Iteration 157/1000 | Loss: 0.00001056
Iteration 158/1000 | Loss: 0.00001056
Iteration 159/1000 | Loss: 0.00001056
Iteration 160/1000 | Loss: 0.00001056
Iteration 161/1000 | Loss: 0.00001055
Iteration 162/1000 | Loss: 0.00001055
Iteration 163/1000 | Loss: 0.00001055
Iteration 164/1000 | Loss: 0.00001055
Iteration 165/1000 | Loss: 0.00001054
Iteration 166/1000 | Loss: 0.00001054
Iteration 167/1000 | Loss: 0.00001054
Iteration 168/1000 | Loss: 0.00001053
Iteration 169/1000 | Loss: 0.00001053
Iteration 170/1000 | Loss: 0.00001053
Iteration 171/1000 | Loss: 0.00001053
Iteration 172/1000 | Loss: 0.00001053
Iteration 173/1000 | Loss: 0.00001052
Iteration 174/1000 | Loss: 0.00001052
Iteration 175/1000 | Loss: 0.00001052
Iteration 176/1000 | Loss: 0.00001052
Iteration 177/1000 | Loss: 0.00001052
Iteration 178/1000 | Loss: 0.00001052
Iteration 179/1000 | Loss: 0.00001052
Iteration 180/1000 | Loss: 0.00001052
Iteration 181/1000 | Loss: 0.00001052
Iteration 182/1000 | Loss: 0.00001051
Iteration 183/1000 | Loss: 0.00001051
Iteration 184/1000 | Loss: 0.00001051
Iteration 185/1000 | Loss: 0.00001051
Iteration 186/1000 | Loss: 0.00001051
Iteration 187/1000 | Loss: 0.00001051
Iteration 188/1000 | Loss: 0.00001051
Iteration 189/1000 | Loss: 0.00001051
Iteration 190/1000 | Loss: 0.00001051
Iteration 191/1000 | Loss: 0.00001051
Iteration 192/1000 | Loss: 0.00001051
Iteration 193/1000 | Loss: 0.00001051
Iteration 194/1000 | Loss: 0.00001051
Iteration 195/1000 | Loss: 0.00001051
Iteration 196/1000 | Loss: 0.00001051
Iteration 197/1000 | Loss: 0.00001051
Iteration 198/1000 | Loss: 0.00001051
Iteration 199/1000 | Loss: 0.00001051
Iteration 200/1000 | Loss: 0.00001051
Iteration 201/1000 | Loss: 0.00001050
Iteration 202/1000 | Loss: 0.00001050
Iteration 203/1000 | Loss: 0.00001050
Iteration 204/1000 | Loss: 0.00001050
Iteration 205/1000 | Loss: 0.00001050
Iteration 206/1000 | Loss: 0.00001050
Iteration 207/1000 | Loss: 0.00001050
Iteration 208/1000 | Loss: 0.00001049
Iteration 209/1000 | Loss: 0.00001049
Iteration 210/1000 | Loss: 0.00001049
Iteration 211/1000 | Loss: 0.00001048
Iteration 212/1000 | Loss: 0.00001048
Iteration 213/1000 | Loss: 0.00001048
Iteration 214/1000 | Loss: 0.00001048
Iteration 215/1000 | Loss: 0.00001048
Iteration 216/1000 | Loss: 0.00001048
Iteration 217/1000 | Loss: 0.00001048
Iteration 218/1000 | Loss: 0.00001048
Iteration 219/1000 | Loss: 0.00001048
Iteration 220/1000 | Loss: 0.00001047
Iteration 221/1000 | Loss: 0.00001047
Iteration 222/1000 | Loss: 0.00001047
Iteration 223/1000 | Loss: 0.00001047
Iteration 224/1000 | Loss: 0.00001047
Iteration 225/1000 | Loss: 0.00001047
Iteration 226/1000 | Loss: 0.00001047
Iteration 227/1000 | Loss: 0.00001047
Iteration 228/1000 | Loss: 0.00001047
Iteration 229/1000 | Loss: 0.00001047
Iteration 230/1000 | Loss: 0.00001047
Iteration 231/1000 | Loss: 0.00001046
Iteration 232/1000 | Loss: 0.00001046
Iteration 233/1000 | Loss: 0.00001046
Iteration 234/1000 | Loss: 0.00001046
Iteration 235/1000 | Loss: 0.00001046
Iteration 236/1000 | Loss: 0.00001046
Iteration 237/1000 | Loss: 0.00001046
Iteration 238/1000 | Loss: 0.00001046
Iteration 239/1000 | Loss: 0.00001046
Iteration 240/1000 | Loss: 0.00001046
Iteration 241/1000 | Loss: 0.00001046
Iteration 242/1000 | Loss: 0.00001045
Iteration 243/1000 | Loss: 0.00001044
Iteration 244/1000 | Loss: 0.00001044
Iteration 245/1000 | Loss: 0.00001044
Iteration 246/1000 | Loss: 0.00001043
Iteration 247/1000 | Loss: 0.00001043
Iteration 248/1000 | Loss: 0.00001043
Iteration 249/1000 | Loss: 0.00001043
Iteration 250/1000 | Loss: 0.00001042
Iteration 251/1000 | Loss: 0.00001042
Iteration 252/1000 | Loss: 0.00001041
Iteration 253/1000 | Loss: 0.00001041
Iteration 254/1000 | Loss: 0.00001041
Iteration 255/1000 | Loss: 0.00001041
Iteration 256/1000 | Loss: 0.00001041
Iteration 257/1000 | Loss: 0.00001041
Iteration 258/1000 | Loss: 0.00001040
Iteration 259/1000 | Loss: 0.00001040
Iteration 260/1000 | Loss: 0.00001040
Iteration 261/1000 | Loss: 0.00001040
Iteration 262/1000 | Loss: 0.00001040
Iteration 263/1000 | Loss: 0.00001040
Iteration 264/1000 | Loss: 0.00001040
Iteration 265/1000 | Loss: 0.00001040
Iteration 266/1000 | Loss: 0.00001040
Iteration 267/1000 | Loss: 0.00001040
Iteration 268/1000 | Loss: 0.00001040
Iteration 269/1000 | Loss: 0.00001039
Iteration 270/1000 | Loss: 0.00001039
Iteration 271/1000 | Loss: 0.00001039
Iteration 272/1000 | Loss: 0.00001039
Iteration 273/1000 | Loss: 0.00001039
Iteration 274/1000 | Loss: 0.00001039
Iteration 275/1000 | Loss: 0.00001039
Iteration 276/1000 | Loss: 0.00001039
Iteration 277/1000 | Loss: 0.00001039
Iteration 278/1000 | Loss: 0.00001039
Iteration 279/1000 | Loss: 0.00001039
Iteration 280/1000 | Loss: 0.00001039
Iteration 281/1000 | Loss: 0.00001038
Iteration 282/1000 | Loss: 0.00001038
Iteration 283/1000 | Loss: 0.00001038
Iteration 284/1000 | Loss: 0.00001038
Iteration 285/1000 | Loss: 0.00001038
Iteration 286/1000 | Loss: 0.00001038
Iteration 287/1000 | Loss: 0.00001038
Iteration 288/1000 | Loss: 0.00001038
Iteration 289/1000 | Loss: 0.00001038
Iteration 290/1000 | Loss: 0.00001038
Iteration 291/1000 | Loss: 0.00001038
Iteration 292/1000 | Loss: 0.00001038
Iteration 293/1000 | Loss: 0.00001038
Iteration 294/1000 | Loss: 0.00001038
Iteration 295/1000 | Loss: 0.00001038
Iteration 296/1000 | Loss: 0.00001038
Iteration 297/1000 | Loss: 0.00001038
Iteration 298/1000 | Loss: 0.00001038
Iteration 299/1000 | Loss: 0.00001038
Iteration 300/1000 | Loss: 0.00001037
Iteration 301/1000 | Loss: 0.00001037
Iteration 302/1000 | Loss: 0.00001037
Iteration 303/1000 | Loss: 0.00001037
Iteration 304/1000 | Loss: 0.00001037
Iteration 305/1000 | Loss: 0.00001037
Iteration 306/1000 | Loss: 0.00001036
Iteration 307/1000 | Loss: 0.00001036
Iteration 308/1000 | Loss: 0.00001036
Iteration 309/1000 | Loss: 0.00001036
Iteration 310/1000 | Loss: 0.00001036
Iteration 311/1000 | Loss: 0.00001036
Iteration 312/1000 | Loss: 0.00001036
Iteration 313/1000 | Loss: 0.00001036
Iteration 314/1000 | Loss: 0.00001036
Iteration 315/1000 | Loss: 0.00001036
Iteration 316/1000 | Loss: 0.00001036
Iteration 317/1000 | Loss: 0.00001036
Iteration 318/1000 | Loss: 0.00001036
Iteration 319/1000 | Loss: 0.00001036
Iteration 320/1000 | Loss: 0.00001036
Iteration 321/1000 | Loss: 0.00001036
Iteration 322/1000 | Loss: 0.00001036
Iteration 323/1000 | Loss: 0.00001036
Iteration 324/1000 | Loss: 0.00001036
Iteration 325/1000 | Loss: 0.00001036
Iteration 326/1000 | Loss: 0.00001036
Iteration 327/1000 | Loss: 0.00001036
Iteration 328/1000 | Loss: 0.00001036
Iteration 329/1000 | Loss: 0.00001036
Iteration 330/1000 | Loss: 0.00001036
Iteration 331/1000 | Loss: 0.00001036
Iteration 332/1000 | Loss: 0.00001036
Iteration 333/1000 | Loss: 0.00001036
Iteration 334/1000 | Loss: 0.00001036
Iteration 335/1000 | Loss: 0.00001036
Iteration 336/1000 | Loss: 0.00001036
Iteration 337/1000 | Loss: 0.00001036
Iteration 338/1000 | Loss: 0.00001036
Iteration 339/1000 | Loss: 0.00001036
Iteration 340/1000 | Loss: 0.00001036
Iteration 341/1000 | Loss: 0.00001036
Iteration 342/1000 | Loss: 0.00001036
Iteration 343/1000 | Loss: 0.00001036
Iteration 344/1000 | Loss: 0.00001036
Iteration 345/1000 | Loss: 0.00001036
Iteration 346/1000 | Loss: 0.00001036
Iteration 347/1000 | Loss: 0.00001036
Iteration 348/1000 | Loss: 0.00001036
Iteration 349/1000 | Loss: 0.00001036
Iteration 350/1000 | Loss: 0.00001036
Iteration 351/1000 | Loss: 0.00001036
Iteration 352/1000 | Loss: 0.00001036
Iteration 353/1000 | Loss: 0.00001036
Iteration 354/1000 | Loss: 0.00001036
Iteration 355/1000 | Loss: 0.00001036
Iteration 356/1000 | Loss: 0.00001036
Iteration 357/1000 | Loss: 0.00001036
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 357. Stopping optimization.
Last 5 losses: [1.0358382496633567e-05, 1.0358382496633567e-05, 1.0358382496633567e-05, 1.0358382496633567e-05, 1.0358382496633567e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0358382496633567e-05

Optimization complete. Final v2v error: 2.6960442066192627 mm

Highest mean error: 3.040968656539917 mm for frame 61

Lowest mean error: 2.5578343868255615 mm for frame 28

Saving results

Total time: 48.14911484718323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072481
Iteration 2/25 | Loss: 0.00267998
Iteration 3/25 | Loss: 0.00223361
Iteration 4/25 | Loss: 0.00237262
Iteration 5/25 | Loss: 0.00149921
Iteration 6/25 | Loss: 0.00146281
Iteration 7/25 | Loss: 0.00143829
Iteration 8/25 | Loss: 0.00143516
Iteration 9/25 | Loss: 0.00142539
Iteration 10/25 | Loss: 0.00142224
Iteration 11/25 | Loss: 0.00142193
Iteration 12/25 | Loss: 0.00142191
Iteration 13/25 | Loss: 0.00142190
Iteration 14/25 | Loss: 0.00142190
Iteration 15/25 | Loss: 0.00142190
Iteration 16/25 | Loss: 0.00142190
Iteration 17/25 | Loss: 0.00142189
Iteration 18/25 | Loss: 0.00142188
Iteration 19/25 | Loss: 0.00142188
Iteration 20/25 | Loss: 0.00142188
Iteration 21/25 | Loss: 0.00142187
Iteration 22/25 | Loss: 0.00142187
Iteration 23/25 | Loss: 0.00142187
Iteration 24/25 | Loss: 0.00142187
Iteration 25/25 | Loss: 0.00142187

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61516345
Iteration 2/25 | Loss: 0.00137295
Iteration 3/25 | Loss: 0.00136532
Iteration 4/25 | Loss: 0.00136532
Iteration 5/25 | Loss: 0.00136532
Iteration 6/25 | Loss: 0.00136531
Iteration 7/25 | Loss: 0.00136531
Iteration 8/25 | Loss: 0.00136531
Iteration 9/25 | Loss: 0.00136531
Iteration 10/25 | Loss: 0.00136531
Iteration 11/25 | Loss: 0.00136531
Iteration 12/25 | Loss: 0.00136531
Iteration 13/25 | Loss: 0.00136531
Iteration 14/25 | Loss: 0.00136531
Iteration 15/25 | Loss: 0.00136531
Iteration 16/25 | Loss: 0.00136531
Iteration 17/25 | Loss: 0.00136531
Iteration 18/25 | Loss: 0.00136531
Iteration 19/25 | Loss: 0.00136531
Iteration 20/25 | Loss: 0.00136531
Iteration 21/25 | Loss: 0.00136531
Iteration 22/25 | Loss: 0.00136531
Iteration 23/25 | Loss: 0.00136531
Iteration 24/25 | Loss: 0.00136531
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001365312491543591, 0.001365312491543591, 0.001365312491543591, 0.001365312491543591, 0.001365312491543591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001365312491543591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136531
Iteration 2/1000 | Loss: 0.00006525
Iteration 3/1000 | Loss: 0.00003848
Iteration 4/1000 | Loss: 0.00003885
Iteration 5/1000 | Loss: 0.00010785
Iteration 6/1000 | Loss: 0.00003477
Iteration 7/1000 | Loss: 0.00003089
Iteration 8/1000 | Loss: 0.00003045
Iteration 9/1000 | Loss: 0.00002999
Iteration 10/1000 | Loss: 0.00002971
Iteration 11/1000 | Loss: 0.00002951
Iteration 12/1000 | Loss: 0.00002930
Iteration 13/1000 | Loss: 0.00002909
Iteration 14/1000 | Loss: 0.00002896
Iteration 15/1000 | Loss: 0.00002895
Iteration 16/1000 | Loss: 0.00002893
Iteration 17/1000 | Loss: 0.00002891
Iteration 18/1000 | Loss: 0.00002882
Iteration 19/1000 | Loss: 0.00002879
Iteration 20/1000 | Loss: 0.00002877
Iteration 21/1000 | Loss: 0.00002876
Iteration 22/1000 | Loss: 0.00002871
Iteration 23/1000 | Loss: 0.00002871
Iteration 24/1000 | Loss: 0.00002871
Iteration 25/1000 | Loss: 0.00002870
Iteration 26/1000 | Loss: 0.00002870
Iteration 27/1000 | Loss: 0.00002870
Iteration 28/1000 | Loss: 0.00002869
Iteration 29/1000 | Loss: 0.00002868
Iteration 30/1000 | Loss: 0.00002868
Iteration 31/1000 | Loss: 0.00002868
Iteration 32/1000 | Loss: 0.00002866
Iteration 33/1000 | Loss: 0.00002866
Iteration 34/1000 | Loss: 0.00002865
Iteration 35/1000 | Loss: 0.00002865
Iteration 36/1000 | Loss: 0.00002865
Iteration 37/1000 | Loss: 0.00002865
Iteration 38/1000 | Loss: 0.00002865
Iteration 39/1000 | Loss: 0.00002865
Iteration 40/1000 | Loss: 0.00002864
Iteration 41/1000 | Loss: 0.00002864
Iteration 42/1000 | Loss: 0.00002864
Iteration 43/1000 | Loss: 0.00002863
Iteration 44/1000 | Loss: 0.00002863
Iteration 45/1000 | Loss: 0.00002863
Iteration 46/1000 | Loss: 0.00002862
Iteration 47/1000 | Loss: 0.00002862
Iteration 48/1000 | Loss: 0.00002861
Iteration 49/1000 | Loss: 0.00002860
Iteration 50/1000 | Loss: 0.00002860
Iteration 51/1000 | Loss: 0.00002860
Iteration 52/1000 | Loss: 0.00002860
Iteration 53/1000 | Loss: 0.00002860
Iteration 54/1000 | Loss: 0.00002860
Iteration 55/1000 | Loss: 0.00002860
Iteration 56/1000 | Loss: 0.00002860
Iteration 57/1000 | Loss: 0.00002860
Iteration 58/1000 | Loss: 0.00002860
Iteration 59/1000 | Loss: 0.00002860
Iteration 60/1000 | Loss: 0.00002859
Iteration 61/1000 | Loss: 0.00002859
Iteration 62/1000 | Loss: 0.00002859
Iteration 63/1000 | Loss: 0.00002859
Iteration 64/1000 | Loss: 0.00002859
Iteration 65/1000 | Loss: 0.00002859
Iteration 66/1000 | Loss: 0.00002859
Iteration 67/1000 | Loss: 0.00002858
Iteration 68/1000 | Loss: 0.00002858
Iteration 69/1000 | Loss: 0.00002858
Iteration 70/1000 | Loss: 0.00002857
Iteration 71/1000 | Loss: 0.00002857
Iteration 72/1000 | Loss: 0.00002856
Iteration 73/1000 | Loss: 0.00002856
Iteration 74/1000 | Loss: 0.00002856
Iteration 75/1000 | Loss: 0.00002856
Iteration 76/1000 | Loss: 0.00002856
Iteration 77/1000 | Loss: 0.00002856
Iteration 78/1000 | Loss: 0.00002856
Iteration 79/1000 | Loss: 0.00002856
Iteration 80/1000 | Loss: 0.00002855
Iteration 81/1000 | Loss: 0.00002855
Iteration 82/1000 | Loss: 0.00002855
Iteration 83/1000 | Loss: 0.00002855
Iteration 84/1000 | Loss: 0.00002855
Iteration 85/1000 | Loss: 0.00002855
Iteration 86/1000 | Loss: 0.00002855
Iteration 87/1000 | Loss: 0.00002855
Iteration 88/1000 | Loss: 0.00002855
Iteration 89/1000 | Loss: 0.00002854
Iteration 90/1000 | Loss: 0.00002854
Iteration 91/1000 | Loss: 0.00002854
Iteration 92/1000 | Loss: 0.00002854
Iteration 93/1000 | Loss: 0.00002854
Iteration 94/1000 | Loss: 0.00002854
Iteration 95/1000 | Loss: 0.00002854
Iteration 96/1000 | Loss: 0.00002854
Iteration 97/1000 | Loss: 0.00002854
Iteration 98/1000 | Loss: 0.00002854
Iteration 99/1000 | Loss: 0.00002853
Iteration 100/1000 | Loss: 0.00002853
Iteration 101/1000 | Loss: 0.00002853
Iteration 102/1000 | Loss: 0.00002853
Iteration 103/1000 | Loss: 0.00002853
Iteration 104/1000 | Loss: 0.00002853
Iteration 105/1000 | Loss: 0.00002853
Iteration 106/1000 | Loss: 0.00002853
Iteration 107/1000 | Loss: 0.00002853
Iteration 108/1000 | Loss: 0.00002853
Iteration 109/1000 | Loss: 0.00002853
Iteration 110/1000 | Loss: 0.00002853
Iteration 111/1000 | Loss: 0.00002853
Iteration 112/1000 | Loss: 0.00002853
Iteration 113/1000 | Loss: 0.00002853
Iteration 114/1000 | Loss: 0.00002853
Iteration 115/1000 | Loss: 0.00002853
Iteration 116/1000 | Loss: 0.00002852
Iteration 117/1000 | Loss: 0.00002852
Iteration 118/1000 | Loss: 0.00002852
Iteration 119/1000 | Loss: 0.00002852
Iteration 120/1000 | Loss: 0.00002852
Iteration 121/1000 | Loss: 0.00002852
Iteration 122/1000 | Loss: 0.00002852
Iteration 123/1000 | Loss: 0.00002852
Iteration 124/1000 | Loss: 0.00002852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.8523536457214504e-05, 2.8523536457214504e-05, 2.8523536457214504e-05, 2.8523536457214504e-05, 2.8523536457214504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8523536457214504e-05

Optimization complete. Final v2v error: 3.8924715518951416 mm

Highest mean error: 9.910337448120117 mm for frame 51

Lowest mean error: 3.098604202270508 mm for frame 147

Saving results

Total time: 49.42813801765442
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967492
Iteration 2/25 | Loss: 0.00259654
Iteration 3/25 | Loss: 0.00195340
Iteration 4/25 | Loss: 0.00177833
Iteration 5/25 | Loss: 0.00188237
Iteration 6/25 | Loss: 0.00180822
Iteration 7/25 | Loss: 0.00173663
Iteration 8/25 | Loss: 0.00161868
Iteration 9/25 | Loss: 0.00150820
Iteration 10/25 | Loss: 0.00142801
Iteration 11/25 | Loss: 0.00135169
Iteration 12/25 | Loss: 0.00133212
Iteration 13/25 | Loss: 0.00130720
Iteration 14/25 | Loss: 0.00130126
Iteration 15/25 | Loss: 0.00129215
Iteration 16/25 | Loss: 0.00128876
Iteration 17/25 | Loss: 0.00129554
Iteration 18/25 | Loss: 0.00129480
Iteration 19/25 | Loss: 0.00129608
Iteration 20/25 | Loss: 0.00128470
Iteration 21/25 | Loss: 0.00128417
Iteration 22/25 | Loss: 0.00128135
Iteration 23/25 | Loss: 0.00127736
Iteration 24/25 | Loss: 0.00127490
Iteration 25/25 | Loss: 0.00127358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37193549
Iteration 2/25 | Loss: 0.00136604
Iteration 3/25 | Loss: 0.00100683
Iteration 4/25 | Loss: 0.00100683
Iteration 5/25 | Loss: 0.00100683
Iteration 6/25 | Loss: 0.00100682
Iteration 7/25 | Loss: 0.00100682
Iteration 8/25 | Loss: 0.00100682
Iteration 9/25 | Loss: 0.00100682
Iteration 10/25 | Loss: 0.00100682
Iteration 11/25 | Loss: 0.00100682
Iteration 12/25 | Loss: 0.00100682
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010068229166790843, 0.0010068229166790843, 0.0010068229166790843, 0.0010068229166790843, 0.0010068229166790843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010068229166790843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100682
Iteration 2/1000 | Loss: 0.00021017
Iteration 3/1000 | Loss: 0.00097238
Iteration 4/1000 | Loss: 0.00004920
Iteration 5/1000 | Loss: 0.00004200
Iteration 6/1000 | Loss: 0.00003892
Iteration 7/1000 | Loss: 0.00003723
Iteration 8/1000 | Loss: 0.00003599
Iteration 9/1000 | Loss: 0.00013732
Iteration 10/1000 | Loss: 0.00003589
Iteration 11/1000 | Loss: 0.00003475
Iteration 12/1000 | Loss: 0.00020004
Iteration 13/1000 | Loss: 0.00025154
Iteration 14/1000 | Loss: 0.00003468
Iteration 15/1000 | Loss: 0.00003376
Iteration 16/1000 | Loss: 0.00003340
Iteration 17/1000 | Loss: 0.00003311
Iteration 18/1000 | Loss: 0.00003280
Iteration 19/1000 | Loss: 0.00003255
Iteration 20/1000 | Loss: 0.00003226
Iteration 21/1000 | Loss: 0.00022398
Iteration 22/1000 | Loss: 0.00069277
Iteration 23/1000 | Loss: 0.00021483
Iteration 24/1000 | Loss: 0.00003302
Iteration 25/1000 | Loss: 0.00052768
Iteration 26/1000 | Loss: 0.00019197
Iteration 27/1000 | Loss: 0.00003972
Iteration 28/1000 | Loss: 0.00105972
Iteration 29/1000 | Loss: 0.00084833
Iteration 30/1000 | Loss: 0.00003385
Iteration 31/1000 | Loss: 0.00053609
Iteration 32/1000 | Loss: 0.00016029
Iteration 33/1000 | Loss: 0.00022919
Iteration 34/1000 | Loss: 0.00003575
Iteration 35/1000 | Loss: 0.00061969
Iteration 36/1000 | Loss: 0.00058752
Iteration 37/1000 | Loss: 0.00004612
Iteration 38/1000 | Loss: 0.00011895
Iteration 39/1000 | Loss: 0.00006668
Iteration 40/1000 | Loss: 0.00003383
Iteration 41/1000 | Loss: 0.00030594
Iteration 42/1000 | Loss: 0.00040748
Iteration 43/1000 | Loss: 0.00032517
Iteration 44/1000 | Loss: 0.00012609
Iteration 45/1000 | Loss: 0.00173608
Iteration 46/1000 | Loss: 0.00020594
Iteration 47/1000 | Loss: 0.00004662
Iteration 48/1000 | Loss: 0.00020416
Iteration 49/1000 | Loss: 0.00004276
Iteration 50/1000 | Loss: 0.00052586
Iteration 51/1000 | Loss: 0.00063482
Iteration 52/1000 | Loss: 0.00036233
Iteration 53/1000 | Loss: 0.00005370
Iteration 54/1000 | Loss: 0.00003755
Iteration 55/1000 | Loss: 0.00003423
Iteration 56/1000 | Loss: 0.00016635
Iteration 57/1000 | Loss: 0.00013436
Iteration 58/1000 | Loss: 0.00003902
Iteration 59/1000 | Loss: 0.00002968
Iteration 60/1000 | Loss: 0.00003231
Iteration 61/1000 | Loss: 0.00002740
Iteration 62/1000 | Loss: 0.00002661
Iteration 63/1000 | Loss: 0.00002610
Iteration 64/1000 | Loss: 0.00002568
Iteration 65/1000 | Loss: 0.00002551
Iteration 66/1000 | Loss: 0.00002549
Iteration 67/1000 | Loss: 0.00002534
Iteration 68/1000 | Loss: 0.00002530
Iteration 69/1000 | Loss: 0.00002529
Iteration 70/1000 | Loss: 0.00002526
Iteration 71/1000 | Loss: 0.00002523
Iteration 72/1000 | Loss: 0.00002522
Iteration 73/1000 | Loss: 0.00002521
Iteration 74/1000 | Loss: 0.00002521
Iteration 75/1000 | Loss: 0.00002521
Iteration 76/1000 | Loss: 0.00002521
Iteration 77/1000 | Loss: 0.00002521
Iteration 78/1000 | Loss: 0.00002521
Iteration 79/1000 | Loss: 0.00002521
Iteration 80/1000 | Loss: 0.00002521
Iteration 81/1000 | Loss: 0.00002521
Iteration 82/1000 | Loss: 0.00002520
Iteration 83/1000 | Loss: 0.00002520
Iteration 84/1000 | Loss: 0.00002520
Iteration 85/1000 | Loss: 0.00002520
Iteration 86/1000 | Loss: 0.00002520
Iteration 87/1000 | Loss: 0.00002520
Iteration 88/1000 | Loss: 0.00002520
Iteration 89/1000 | Loss: 0.00002520
Iteration 90/1000 | Loss: 0.00002519
Iteration 91/1000 | Loss: 0.00002519
Iteration 92/1000 | Loss: 0.00002518
Iteration 93/1000 | Loss: 0.00002518
Iteration 94/1000 | Loss: 0.00002518
Iteration 95/1000 | Loss: 0.00002517
Iteration 96/1000 | Loss: 0.00002517
Iteration 97/1000 | Loss: 0.00002517
Iteration 98/1000 | Loss: 0.00002517
Iteration 99/1000 | Loss: 0.00002517
Iteration 100/1000 | Loss: 0.00002516
Iteration 101/1000 | Loss: 0.00002516
Iteration 102/1000 | Loss: 0.00002516
Iteration 103/1000 | Loss: 0.00002516
Iteration 104/1000 | Loss: 0.00002516
Iteration 105/1000 | Loss: 0.00002515
Iteration 106/1000 | Loss: 0.00002515
Iteration 107/1000 | Loss: 0.00002515
Iteration 108/1000 | Loss: 0.00002514
Iteration 109/1000 | Loss: 0.00002514
Iteration 110/1000 | Loss: 0.00002514
Iteration 111/1000 | Loss: 0.00002514
Iteration 112/1000 | Loss: 0.00002513
Iteration 113/1000 | Loss: 0.00002513
Iteration 114/1000 | Loss: 0.00002513
Iteration 115/1000 | Loss: 0.00002513
Iteration 116/1000 | Loss: 0.00002513
Iteration 117/1000 | Loss: 0.00002512
Iteration 118/1000 | Loss: 0.00002512
Iteration 119/1000 | Loss: 0.00002512
Iteration 120/1000 | Loss: 0.00002512
Iteration 121/1000 | Loss: 0.00002512
Iteration 122/1000 | Loss: 0.00002512
Iteration 123/1000 | Loss: 0.00002512
Iteration 124/1000 | Loss: 0.00002512
Iteration 125/1000 | Loss: 0.00002512
Iteration 126/1000 | Loss: 0.00002511
Iteration 127/1000 | Loss: 0.00002510
Iteration 128/1000 | Loss: 0.00002510
Iteration 129/1000 | Loss: 0.00002510
Iteration 130/1000 | Loss: 0.00002509
Iteration 131/1000 | Loss: 0.00002509
Iteration 132/1000 | Loss: 0.00002509
Iteration 133/1000 | Loss: 0.00002509
Iteration 134/1000 | Loss: 0.00002508
Iteration 135/1000 | Loss: 0.00002508
Iteration 136/1000 | Loss: 0.00002508
Iteration 137/1000 | Loss: 0.00002508
Iteration 138/1000 | Loss: 0.00002508
Iteration 139/1000 | Loss: 0.00002507
Iteration 140/1000 | Loss: 0.00002507
Iteration 141/1000 | Loss: 0.00002507
Iteration 142/1000 | Loss: 0.00002507
Iteration 143/1000 | Loss: 0.00002507
Iteration 144/1000 | Loss: 0.00002507
Iteration 145/1000 | Loss: 0.00002507
Iteration 146/1000 | Loss: 0.00002507
Iteration 147/1000 | Loss: 0.00002507
Iteration 148/1000 | Loss: 0.00002507
Iteration 149/1000 | Loss: 0.00002507
Iteration 150/1000 | Loss: 0.00002507
Iteration 151/1000 | Loss: 0.00002507
Iteration 152/1000 | Loss: 0.00002507
Iteration 153/1000 | Loss: 0.00002507
Iteration 154/1000 | Loss: 0.00002507
Iteration 155/1000 | Loss: 0.00002507
Iteration 156/1000 | Loss: 0.00002507
Iteration 157/1000 | Loss: 0.00002507
Iteration 158/1000 | Loss: 0.00002507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.5069051844184287e-05, 2.5069051844184287e-05, 2.5069051844184287e-05, 2.5069051844184287e-05, 2.5069051844184287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5069051844184287e-05

Optimization complete. Final v2v error: 4.292966365814209 mm

Highest mean error: 4.90172815322876 mm for frame 104

Lowest mean error: 3.593010187149048 mm for frame 97

Saving results

Total time: 145.07856488227844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765549
Iteration 2/25 | Loss: 0.00142906
Iteration 3/25 | Loss: 0.00130035
Iteration 4/25 | Loss: 0.00122808
Iteration 5/25 | Loss: 0.00121103
Iteration 6/25 | Loss: 0.00120917
Iteration 7/25 | Loss: 0.00120884
Iteration 8/25 | Loss: 0.00120868
Iteration 9/25 | Loss: 0.00120865
Iteration 10/25 | Loss: 0.00120865
Iteration 11/25 | Loss: 0.00120865
Iteration 12/25 | Loss: 0.00120865
Iteration 13/25 | Loss: 0.00120865
Iteration 14/25 | Loss: 0.00120865
Iteration 15/25 | Loss: 0.00120865
Iteration 16/25 | Loss: 0.00120865
Iteration 17/25 | Loss: 0.00120865
Iteration 18/25 | Loss: 0.00120865
Iteration 19/25 | Loss: 0.00120865
Iteration 20/25 | Loss: 0.00120865
Iteration 21/25 | Loss: 0.00120865
Iteration 22/25 | Loss: 0.00120865
Iteration 23/25 | Loss: 0.00120865
Iteration 24/25 | Loss: 0.00120865
Iteration 25/25 | Loss: 0.00120865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89342475
Iteration 2/25 | Loss: 0.00109357
Iteration 3/25 | Loss: 0.00109357
Iteration 4/25 | Loss: 0.00109357
Iteration 5/25 | Loss: 0.00109357
Iteration 6/25 | Loss: 0.00109357
Iteration 7/25 | Loss: 0.00109357
Iteration 8/25 | Loss: 0.00109357
Iteration 9/25 | Loss: 0.00109357
Iteration 10/25 | Loss: 0.00109357
Iteration 11/25 | Loss: 0.00109357
Iteration 12/25 | Loss: 0.00109357
Iteration 13/25 | Loss: 0.00109357
Iteration 14/25 | Loss: 0.00109357
Iteration 15/25 | Loss: 0.00109357
Iteration 16/25 | Loss: 0.00109357
Iteration 17/25 | Loss: 0.00109357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010935678146779537, 0.0010935678146779537, 0.0010935678146779537, 0.0010935678146779537, 0.0010935678146779537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010935678146779537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109357
Iteration 2/1000 | Loss: 0.00002012
Iteration 3/1000 | Loss: 0.00001585
Iteration 4/1000 | Loss: 0.00001441
Iteration 5/1000 | Loss: 0.00001360
Iteration 6/1000 | Loss: 0.00001298
Iteration 7/1000 | Loss: 0.00001258
Iteration 8/1000 | Loss: 0.00001238
Iteration 9/1000 | Loss: 0.00001213
Iteration 10/1000 | Loss: 0.00001188
Iteration 11/1000 | Loss: 0.00001185
Iteration 12/1000 | Loss: 0.00001175
Iteration 13/1000 | Loss: 0.00001167
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00001148
Iteration 16/1000 | Loss: 0.00001146
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001146
Iteration 19/1000 | Loss: 0.00001141
Iteration 20/1000 | Loss: 0.00001140
Iteration 21/1000 | Loss: 0.00001139
Iteration 22/1000 | Loss: 0.00001139
Iteration 23/1000 | Loss: 0.00001137
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001136
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001133
Iteration 28/1000 | Loss: 0.00001131
Iteration 29/1000 | Loss: 0.00001131
Iteration 30/1000 | Loss: 0.00001124
Iteration 31/1000 | Loss: 0.00001123
Iteration 32/1000 | Loss: 0.00001123
Iteration 33/1000 | Loss: 0.00001121
Iteration 34/1000 | Loss: 0.00001120
Iteration 35/1000 | Loss: 0.00001120
Iteration 36/1000 | Loss: 0.00001119
Iteration 37/1000 | Loss: 0.00001119
Iteration 38/1000 | Loss: 0.00001118
Iteration 39/1000 | Loss: 0.00001118
Iteration 40/1000 | Loss: 0.00001117
Iteration 41/1000 | Loss: 0.00001116
Iteration 42/1000 | Loss: 0.00001116
Iteration 43/1000 | Loss: 0.00001115
Iteration 44/1000 | Loss: 0.00001115
Iteration 45/1000 | Loss: 0.00001114
Iteration 46/1000 | Loss: 0.00001114
Iteration 47/1000 | Loss: 0.00001114
Iteration 48/1000 | Loss: 0.00001114
Iteration 49/1000 | Loss: 0.00001113
Iteration 50/1000 | Loss: 0.00001113
Iteration 51/1000 | Loss: 0.00001112
Iteration 52/1000 | Loss: 0.00001112
Iteration 53/1000 | Loss: 0.00001112
Iteration 54/1000 | Loss: 0.00001111
Iteration 55/1000 | Loss: 0.00001111
Iteration 56/1000 | Loss: 0.00001110
Iteration 57/1000 | Loss: 0.00001110
Iteration 58/1000 | Loss: 0.00001109
Iteration 59/1000 | Loss: 0.00001109
Iteration 60/1000 | Loss: 0.00001109
Iteration 61/1000 | Loss: 0.00001108
Iteration 62/1000 | Loss: 0.00001108
Iteration 63/1000 | Loss: 0.00001108
Iteration 64/1000 | Loss: 0.00001108
Iteration 65/1000 | Loss: 0.00001108
Iteration 66/1000 | Loss: 0.00001108
Iteration 67/1000 | Loss: 0.00001107
Iteration 68/1000 | Loss: 0.00001106
Iteration 69/1000 | Loss: 0.00001106
Iteration 70/1000 | Loss: 0.00001106
Iteration 71/1000 | Loss: 0.00001105
Iteration 72/1000 | Loss: 0.00001105
Iteration 73/1000 | Loss: 0.00001105
Iteration 74/1000 | Loss: 0.00001105
Iteration 75/1000 | Loss: 0.00001104
Iteration 76/1000 | Loss: 0.00001104
Iteration 77/1000 | Loss: 0.00001103
Iteration 78/1000 | Loss: 0.00001102
Iteration 79/1000 | Loss: 0.00001102
Iteration 80/1000 | Loss: 0.00001102
Iteration 81/1000 | Loss: 0.00001102
Iteration 82/1000 | Loss: 0.00001102
Iteration 83/1000 | Loss: 0.00001102
Iteration 84/1000 | Loss: 0.00001102
Iteration 85/1000 | Loss: 0.00001101
Iteration 86/1000 | Loss: 0.00001101
Iteration 87/1000 | Loss: 0.00001101
Iteration 88/1000 | Loss: 0.00001101
Iteration 89/1000 | Loss: 0.00001101
Iteration 90/1000 | Loss: 0.00001101
Iteration 91/1000 | Loss: 0.00001101
Iteration 92/1000 | Loss: 0.00001100
Iteration 93/1000 | Loss: 0.00001100
Iteration 94/1000 | Loss: 0.00001100
Iteration 95/1000 | Loss: 0.00001100
Iteration 96/1000 | Loss: 0.00001099
Iteration 97/1000 | Loss: 0.00001099
Iteration 98/1000 | Loss: 0.00001099
Iteration 99/1000 | Loss: 0.00001098
Iteration 100/1000 | Loss: 0.00001098
Iteration 101/1000 | Loss: 0.00001098
Iteration 102/1000 | Loss: 0.00001098
Iteration 103/1000 | Loss: 0.00001098
Iteration 104/1000 | Loss: 0.00001098
Iteration 105/1000 | Loss: 0.00001097
Iteration 106/1000 | Loss: 0.00001097
Iteration 107/1000 | Loss: 0.00001097
Iteration 108/1000 | Loss: 0.00001097
Iteration 109/1000 | Loss: 0.00001097
Iteration 110/1000 | Loss: 0.00001097
Iteration 111/1000 | Loss: 0.00001097
Iteration 112/1000 | Loss: 0.00001097
Iteration 113/1000 | Loss: 0.00001097
Iteration 114/1000 | Loss: 0.00001097
Iteration 115/1000 | Loss: 0.00001097
Iteration 116/1000 | Loss: 0.00001097
Iteration 117/1000 | Loss: 0.00001097
Iteration 118/1000 | Loss: 0.00001097
Iteration 119/1000 | Loss: 0.00001097
Iteration 120/1000 | Loss: 0.00001097
Iteration 121/1000 | Loss: 0.00001097
Iteration 122/1000 | Loss: 0.00001097
Iteration 123/1000 | Loss: 0.00001097
Iteration 124/1000 | Loss: 0.00001097
Iteration 125/1000 | Loss: 0.00001097
Iteration 126/1000 | Loss: 0.00001097
Iteration 127/1000 | Loss: 0.00001097
Iteration 128/1000 | Loss: 0.00001097
Iteration 129/1000 | Loss: 0.00001097
Iteration 130/1000 | Loss: 0.00001097
Iteration 131/1000 | Loss: 0.00001097
Iteration 132/1000 | Loss: 0.00001097
Iteration 133/1000 | Loss: 0.00001097
Iteration 134/1000 | Loss: 0.00001097
Iteration 135/1000 | Loss: 0.00001097
Iteration 136/1000 | Loss: 0.00001097
Iteration 137/1000 | Loss: 0.00001097
Iteration 138/1000 | Loss: 0.00001097
Iteration 139/1000 | Loss: 0.00001097
Iteration 140/1000 | Loss: 0.00001097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.0968900824082084e-05, 1.0968900824082084e-05, 1.0968900824082084e-05, 1.0968900824082084e-05, 1.0968900824082084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0968900824082084e-05

Optimization complete. Final v2v error: 2.872835874557495 mm

Highest mean error: 3.0321731567382812 mm for frame 64

Lowest mean error: 2.740964412689209 mm for frame 2

Saving results

Total time: 42.43382430076599
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028941
Iteration 2/25 | Loss: 0.00217849
Iteration 3/25 | Loss: 0.00181181
Iteration 4/25 | Loss: 0.00184845
Iteration 5/25 | Loss: 0.00155229
Iteration 6/25 | Loss: 0.00145886
Iteration 7/25 | Loss: 0.00146082
Iteration 8/25 | Loss: 0.00129323
Iteration 9/25 | Loss: 0.00125422
Iteration 10/25 | Loss: 0.00123834
Iteration 11/25 | Loss: 0.00122757
Iteration 12/25 | Loss: 0.00122825
Iteration 13/25 | Loss: 0.00122626
Iteration 14/25 | Loss: 0.00122767
Iteration 15/25 | Loss: 0.00121780
Iteration 16/25 | Loss: 0.00122169
Iteration 17/25 | Loss: 0.00121953
Iteration 18/25 | Loss: 0.00122043
Iteration 19/25 | Loss: 0.00121910
Iteration 20/25 | Loss: 0.00121563
Iteration 21/25 | Loss: 0.00121201
Iteration 22/25 | Loss: 0.00121406
Iteration 23/25 | Loss: 0.00121269
Iteration 24/25 | Loss: 0.00121134
Iteration 25/25 | Loss: 0.00121096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37682545
Iteration 2/25 | Loss: 0.00211490
Iteration 3/25 | Loss: 0.00123392
Iteration 4/25 | Loss: 0.00123392
Iteration 5/25 | Loss: 0.00123392
Iteration 6/25 | Loss: 0.00123392
Iteration 7/25 | Loss: 0.00123391
Iteration 8/25 | Loss: 0.00123391
Iteration 9/25 | Loss: 0.00123391
Iteration 10/25 | Loss: 0.00123391
Iteration 11/25 | Loss: 0.00123391
Iteration 12/25 | Loss: 0.00123391
Iteration 13/25 | Loss: 0.00123391
Iteration 14/25 | Loss: 0.00123391
Iteration 15/25 | Loss: 0.00123391
Iteration 16/25 | Loss: 0.00123391
Iteration 17/25 | Loss: 0.00123391
Iteration 18/25 | Loss: 0.00123391
Iteration 19/25 | Loss: 0.00123391
Iteration 20/25 | Loss: 0.00123391
Iteration 21/25 | Loss: 0.00123391
Iteration 22/25 | Loss: 0.00123391
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001233913586474955, 0.001233913586474955, 0.001233913586474955, 0.001233913586474955, 0.001233913586474955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001233913586474955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123391
Iteration 2/1000 | Loss: 0.00013320
Iteration 3/1000 | Loss: 0.00116880
Iteration 4/1000 | Loss: 0.00031831
Iteration 5/1000 | Loss: 0.00003079
Iteration 6/1000 | Loss: 0.00002560
Iteration 7/1000 | Loss: 0.00006162
Iteration 8/1000 | Loss: 0.00079920
Iteration 9/1000 | Loss: 0.00003170
Iteration 10/1000 | Loss: 0.00003388
Iteration 11/1000 | Loss: 0.00001485
Iteration 12/1000 | Loss: 0.00342076
Iteration 13/1000 | Loss: 0.00020742
Iteration 14/1000 | Loss: 0.00017731
Iteration 15/1000 | Loss: 0.00011012
Iteration 16/1000 | Loss: 0.00003934
Iteration 17/1000 | Loss: 0.00005235
Iteration 18/1000 | Loss: 0.00003076
Iteration 19/1000 | Loss: 0.00014106
Iteration 20/1000 | Loss: 0.00001798
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00002503
Iteration 23/1000 | Loss: 0.00001200
Iteration 24/1000 | Loss: 0.00001183
Iteration 25/1000 | Loss: 0.00002939
Iteration 26/1000 | Loss: 0.00042165
Iteration 27/1000 | Loss: 0.00001482
Iteration 28/1000 | Loss: 0.00002048
Iteration 29/1000 | Loss: 0.00001164
Iteration 30/1000 | Loss: 0.00005399
Iteration 31/1000 | Loss: 0.00002141
Iteration 32/1000 | Loss: 0.00001404
Iteration 33/1000 | Loss: 0.00002295
Iteration 34/1000 | Loss: 0.00001148
Iteration 35/1000 | Loss: 0.00001138
Iteration 36/1000 | Loss: 0.00001138
Iteration 37/1000 | Loss: 0.00001137
Iteration 38/1000 | Loss: 0.00001137
Iteration 39/1000 | Loss: 0.00004977
Iteration 40/1000 | Loss: 0.00003884
Iteration 41/1000 | Loss: 0.00002649
Iteration 42/1000 | Loss: 0.00015367
Iteration 43/1000 | Loss: 0.00001346
Iteration 44/1000 | Loss: 0.00001122
Iteration 45/1000 | Loss: 0.00001116
Iteration 46/1000 | Loss: 0.00001116
Iteration 47/1000 | Loss: 0.00001115
Iteration 48/1000 | Loss: 0.00001115
Iteration 49/1000 | Loss: 0.00001114
Iteration 50/1000 | Loss: 0.00001114
Iteration 51/1000 | Loss: 0.00001114
Iteration 52/1000 | Loss: 0.00001113
Iteration 53/1000 | Loss: 0.00001112
Iteration 54/1000 | Loss: 0.00001112
Iteration 55/1000 | Loss: 0.00001111
Iteration 56/1000 | Loss: 0.00001111
Iteration 57/1000 | Loss: 0.00001111
Iteration 58/1000 | Loss: 0.00001111
Iteration 59/1000 | Loss: 0.00001111
Iteration 60/1000 | Loss: 0.00001111
Iteration 61/1000 | Loss: 0.00001111
Iteration 62/1000 | Loss: 0.00001111
Iteration 63/1000 | Loss: 0.00001110
Iteration 64/1000 | Loss: 0.00001110
Iteration 65/1000 | Loss: 0.00001110
Iteration 66/1000 | Loss: 0.00001110
Iteration 67/1000 | Loss: 0.00001110
Iteration 68/1000 | Loss: 0.00001110
Iteration 69/1000 | Loss: 0.00001110
Iteration 70/1000 | Loss: 0.00001110
Iteration 71/1000 | Loss: 0.00001110
Iteration 72/1000 | Loss: 0.00001110
Iteration 73/1000 | Loss: 0.00001110
Iteration 74/1000 | Loss: 0.00001110
Iteration 75/1000 | Loss: 0.00001110
Iteration 76/1000 | Loss: 0.00001110
Iteration 77/1000 | Loss: 0.00001110
Iteration 78/1000 | Loss: 0.00001110
Iteration 79/1000 | Loss: 0.00001110
Iteration 80/1000 | Loss: 0.00001110
Iteration 81/1000 | Loss: 0.00001110
Iteration 82/1000 | Loss: 0.00001110
Iteration 83/1000 | Loss: 0.00001110
Iteration 84/1000 | Loss: 0.00001110
Iteration 85/1000 | Loss: 0.00001110
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001110
Iteration 93/1000 | Loss: 0.00001110
Iteration 94/1000 | Loss: 0.00001110
Iteration 95/1000 | Loss: 0.00001110
Iteration 96/1000 | Loss: 0.00001110
Iteration 97/1000 | Loss: 0.00001110
Iteration 98/1000 | Loss: 0.00001110
Iteration 99/1000 | Loss: 0.00001110
Iteration 100/1000 | Loss: 0.00001110
Iteration 101/1000 | Loss: 0.00001110
Iteration 102/1000 | Loss: 0.00001110
Iteration 103/1000 | Loss: 0.00001110
Iteration 104/1000 | Loss: 0.00001110
Iteration 105/1000 | Loss: 0.00001110
Iteration 106/1000 | Loss: 0.00001110
Iteration 107/1000 | Loss: 0.00001110
Iteration 108/1000 | Loss: 0.00001110
Iteration 109/1000 | Loss: 0.00001110
Iteration 110/1000 | Loss: 0.00001110
Iteration 111/1000 | Loss: 0.00001110
Iteration 112/1000 | Loss: 0.00001110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.1099350558652077e-05, 1.1099350558652077e-05, 1.1099350558652077e-05, 1.1099350558652077e-05, 1.1099350558652077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1099350558652077e-05

Optimization complete. Final v2v error: 2.8274857997894287 mm

Highest mean error: 4.17333459854126 mm for frame 72

Lowest mean error: 2.4693028926849365 mm for frame 38

Saving results

Total time: 106.70615029335022
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399602
Iteration 2/25 | Loss: 0.00133154
Iteration 3/25 | Loss: 0.00126305
Iteration 4/25 | Loss: 0.00125206
Iteration 5/25 | Loss: 0.00124837
Iteration 6/25 | Loss: 0.00124668
Iteration 7/25 | Loss: 0.00124605
Iteration 8/25 | Loss: 0.00124605
Iteration 9/25 | Loss: 0.00124605
Iteration 10/25 | Loss: 0.00124605
Iteration 11/25 | Loss: 0.00124605
Iteration 12/25 | Loss: 0.00124605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012460495345294476, 0.0012460495345294476, 0.0012460495345294476, 0.0012460495345294476, 0.0012460495345294476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012460495345294476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35331964
Iteration 2/25 | Loss: 0.00161741
Iteration 3/25 | Loss: 0.00161741
Iteration 4/25 | Loss: 0.00161741
Iteration 5/25 | Loss: 0.00161741
Iteration 6/25 | Loss: 0.00161741
Iteration 7/25 | Loss: 0.00161741
Iteration 8/25 | Loss: 0.00161741
Iteration 9/25 | Loss: 0.00161741
Iteration 10/25 | Loss: 0.00161741
Iteration 11/25 | Loss: 0.00161741
Iteration 12/25 | Loss: 0.00161741
Iteration 13/25 | Loss: 0.00161741
Iteration 14/25 | Loss: 0.00161741
Iteration 15/25 | Loss: 0.00161741
Iteration 16/25 | Loss: 0.00161741
Iteration 17/25 | Loss: 0.00161741
Iteration 18/25 | Loss: 0.00161741
Iteration 19/25 | Loss: 0.00161741
Iteration 20/25 | Loss: 0.00161741
Iteration 21/25 | Loss: 0.00161741
Iteration 22/25 | Loss: 0.00161741
Iteration 23/25 | Loss: 0.00161741
Iteration 24/25 | Loss: 0.00161741
Iteration 25/25 | Loss: 0.00161741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161741
Iteration 2/1000 | Loss: 0.00004362
Iteration 3/1000 | Loss: 0.00002597
Iteration 4/1000 | Loss: 0.00002036
Iteration 5/1000 | Loss: 0.00001857
Iteration 6/1000 | Loss: 0.00001792
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001681
Iteration 9/1000 | Loss: 0.00001640
Iteration 10/1000 | Loss: 0.00001622
Iteration 11/1000 | Loss: 0.00001616
Iteration 12/1000 | Loss: 0.00001615
Iteration 13/1000 | Loss: 0.00001609
Iteration 14/1000 | Loss: 0.00001608
Iteration 15/1000 | Loss: 0.00001608
Iteration 16/1000 | Loss: 0.00001607
Iteration 17/1000 | Loss: 0.00001607
Iteration 18/1000 | Loss: 0.00001606
Iteration 19/1000 | Loss: 0.00001601
Iteration 20/1000 | Loss: 0.00001599
Iteration 21/1000 | Loss: 0.00001595
Iteration 22/1000 | Loss: 0.00001594
Iteration 23/1000 | Loss: 0.00001594
Iteration 24/1000 | Loss: 0.00001591
Iteration 25/1000 | Loss: 0.00001582
Iteration 26/1000 | Loss: 0.00001580
Iteration 27/1000 | Loss: 0.00001579
Iteration 28/1000 | Loss: 0.00001577
Iteration 29/1000 | Loss: 0.00001572
Iteration 30/1000 | Loss: 0.00001570
Iteration 31/1000 | Loss: 0.00001570
Iteration 32/1000 | Loss: 0.00001563
Iteration 33/1000 | Loss: 0.00001559
Iteration 34/1000 | Loss: 0.00001558
Iteration 35/1000 | Loss: 0.00001556
Iteration 36/1000 | Loss: 0.00001552
Iteration 37/1000 | Loss: 0.00001551
Iteration 38/1000 | Loss: 0.00001550
Iteration 39/1000 | Loss: 0.00001547
Iteration 40/1000 | Loss: 0.00001547
Iteration 41/1000 | Loss: 0.00001547
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001544
Iteration 45/1000 | Loss: 0.00001544
Iteration 46/1000 | Loss: 0.00001543
Iteration 47/1000 | Loss: 0.00001543
Iteration 48/1000 | Loss: 0.00001543
Iteration 49/1000 | Loss: 0.00001542
Iteration 50/1000 | Loss: 0.00001542
Iteration 51/1000 | Loss: 0.00001541
Iteration 52/1000 | Loss: 0.00001541
Iteration 53/1000 | Loss: 0.00001541
Iteration 54/1000 | Loss: 0.00001540
Iteration 55/1000 | Loss: 0.00001540
Iteration 56/1000 | Loss: 0.00001540
Iteration 57/1000 | Loss: 0.00001540
Iteration 58/1000 | Loss: 0.00001540
Iteration 59/1000 | Loss: 0.00001540
Iteration 60/1000 | Loss: 0.00001540
Iteration 61/1000 | Loss: 0.00001540
Iteration 62/1000 | Loss: 0.00001540
Iteration 63/1000 | Loss: 0.00001539
Iteration 64/1000 | Loss: 0.00001539
Iteration 65/1000 | Loss: 0.00001539
Iteration 66/1000 | Loss: 0.00001539
Iteration 67/1000 | Loss: 0.00001538
Iteration 68/1000 | Loss: 0.00001538
Iteration 69/1000 | Loss: 0.00001538
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001538
Iteration 73/1000 | Loss: 0.00001538
Iteration 74/1000 | Loss: 0.00001538
Iteration 75/1000 | Loss: 0.00001538
Iteration 76/1000 | Loss: 0.00001538
Iteration 77/1000 | Loss: 0.00001538
Iteration 78/1000 | Loss: 0.00001538
Iteration 79/1000 | Loss: 0.00001537
Iteration 80/1000 | Loss: 0.00001537
Iteration 81/1000 | Loss: 0.00001537
Iteration 82/1000 | Loss: 0.00001537
Iteration 83/1000 | Loss: 0.00001537
Iteration 84/1000 | Loss: 0.00001537
Iteration 85/1000 | Loss: 0.00001537
Iteration 86/1000 | Loss: 0.00001537
Iteration 87/1000 | Loss: 0.00001537
Iteration 88/1000 | Loss: 0.00001537
Iteration 89/1000 | Loss: 0.00001537
Iteration 90/1000 | Loss: 0.00001537
Iteration 91/1000 | Loss: 0.00001537
Iteration 92/1000 | Loss: 0.00001536
Iteration 93/1000 | Loss: 0.00001536
Iteration 94/1000 | Loss: 0.00001536
Iteration 95/1000 | Loss: 0.00001536
Iteration 96/1000 | Loss: 0.00001536
Iteration 97/1000 | Loss: 0.00001536
Iteration 98/1000 | Loss: 0.00001536
Iteration 99/1000 | Loss: 0.00001536
Iteration 100/1000 | Loss: 0.00001535
Iteration 101/1000 | Loss: 0.00001535
Iteration 102/1000 | Loss: 0.00001535
Iteration 103/1000 | Loss: 0.00001535
Iteration 104/1000 | Loss: 0.00001535
Iteration 105/1000 | Loss: 0.00001535
Iteration 106/1000 | Loss: 0.00001534
Iteration 107/1000 | Loss: 0.00001534
Iteration 108/1000 | Loss: 0.00001534
Iteration 109/1000 | Loss: 0.00001534
Iteration 110/1000 | Loss: 0.00001534
Iteration 111/1000 | Loss: 0.00001534
Iteration 112/1000 | Loss: 0.00001534
Iteration 113/1000 | Loss: 0.00001533
Iteration 114/1000 | Loss: 0.00001533
Iteration 115/1000 | Loss: 0.00001533
Iteration 116/1000 | Loss: 0.00001533
Iteration 117/1000 | Loss: 0.00001532
Iteration 118/1000 | Loss: 0.00001532
Iteration 119/1000 | Loss: 0.00001532
Iteration 120/1000 | Loss: 0.00001531
Iteration 121/1000 | Loss: 0.00001531
Iteration 122/1000 | Loss: 0.00001531
Iteration 123/1000 | Loss: 0.00001531
Iteration 124/1000 | Loss: 0.00001530
Iteration 125/1000 | Loss: 0.00001530
Iteration 126/1000 | Loss: 0.00001530
Iteration 127/1000 | Loss: 0.00001529
Iteration 128/1000 | Loss: 0.00001529
Iteration 129/1000 | Loss: 0.00001529
Iteration 130/1000 | Loss: 0.00001529
Iteration 131/1000 | Loss: 0.00001528
Iteration 132/1000 | Loss: 0.00001528
Iteration 133/1000 | Loss: 0.00001528
Iteration 134/1000 | Loss: 0.00001528
Iteration 135/1000 | Loss: 0.00001528
Iteration 136/1000 | Loss: 0.00001528
Iteration 137/1000 | Loss: 0.00001528
Iteration 138/1000 | Loss: 0.00001528
Iteration 139/1000 | Loss: 0.00001528
Iteration 140/1000 | Loss: 0.00001528
Iteration 141/1000 | Loss: 0.00001528
Iteration 142/1000 | Loss: 0.00001528
Iteration 143/1000 | Loss: 0.00001528
Iteration 144/1000 | Loss: 0.00001528
Iteration 145/1000 | Loss: 0.00001527
Iteration 146/1000 | Loss: 0.00001527
Iteration 147/1000 | Loss: 0.00001527
Iteration 148/1000 | Loss: 0.00001527
Iteration 149/1000 | Loss: 0.00001527
Iteration 150/1000 | Loss: 0.00001527
Iteration 151/1000 | Loss: 0.00001527
Iteration 152/1000 | Loss: 0.00001527
Iteration 153/1000 | Loss: 0.00001527
Iteration 154/1000 | Loss: 0.00001526
Iteration 155/1000 | Loss: 0.00001526
Iteration 156/1000 | Loss: 0.00001526
Iteration 157/1000 | Loss: 0.00001526
Iteration 158/1000 | Loss: 0.00001526
Iteration 159/1000 | Loss: 0.00001526
Iteration 160/1000 | Loss: 0.00001526
Iteration 161/1000 | Loss: 0.00001526
Iteration 162/1000 | Loss: 0.00001526
Iteration 163/1000 | Loss: 0.00001526
Iteration 164/1000 | Loss: 0.00001525
Iteration 165/1000 | Loss: 0.00001525
Iteration 166/1000 | Loss: 0.00001525
Iteration 167/1000 | Loss: 0.00001525
Iteration 168/1000 | Loss: 0.00001525
Iteration 169/1000 | Loss: 0.00001525
Iteration 170/1000 | Loss: 0.00001525
Iteration 171/1000 | Loss: 0.00001525
Iteration 172/1000 | Loss: 0.00001525
Iteration 173/1000 | Loss: 0.00001525
Iteration 174/1000 | Loss: 0.00001525
Iteration 175/1000 | Loss: 0.00001524
Iteration 176/1000 | Loss: 0.00001524
Iteration 177/1000 | Loss: 0.00001524
Iteration 178/1000 | Loss: 0.00001524
Iteration 179/1000 | Loss: 0.00001523
Iteration 180/1000 | Loss: 0.00001523
Iteration 181/1000 | Loss: 0.00001523
Iteration 182/1000 | Loss: 0.00001523
Iteration 183/1000 | Loss: 0.00001523
Iteration 184/1000 | Loss: 0.00001523
Iteration 185/1000 | Loss: 0.00001523
Iteration 186/1000 | Loss: 0.00001523
Iteration 187/1000 | Loss: 0.00001523
Iteration 188/1000 | Loss: 0.00001523
Iteration 189/1000 | Loss: 0.00001523
Iteration 190/1000 | Loss: 0.00001523
Iteration 191/1000 | Loss: 0.00001523
Iteration 192/1000 | Loss: 0.00001522
Iteration 193/1000 | Loss: 0.00001522
Iteration 194/1000 | Loss: 0.00001522
Iteration 195/1000 | Loss: 0.00001522
Iteration 196/1000 | Loss: 0.00001522
Iteration 197/1000 | Loss: 0.00001522
Iteration 198/1000 | Loss: 0.00001522
Iteration 199/1000 | Loss: 0.00001522
Iteration 200/1000 | Loss: 0.00001522
Iteration 201/1000 | Loss: 0.00001522
Iteration 202/1000 | Loss: 0.00001522
Iteration 203/1000 | Loss: 0.00001521
Iteration 204/1000 | Loss: 0.00001521
Iteration 205/1000 | Loss: 0.00001521
Iteration 206/1000 | Loss: 0.00001521
Iteration 207/1000 | Loss: 0.00001521
Iteration 208/1000 | Loss: 0.00001521
Iteration 209/1000 | Loss: 0.00001521
Iteration 210/1000 | Loss: 0.00001521
Iteration 211/1000 | Loss: 0.00001521
Iteration 212/1000 | Loss: 0.00001521
Iteration 213/1000 | Loss: 0.00001521
Iteration 214/1000 | Loss: 0.00001521
Iteration 215/1000 | Loss: 0.00001521
Iteration 216/1000 | Loss: 0.00001521
Iteration 217/1000 | Loss: 0.00001521
Iteration 218/1000 | Loss: 0.00001520
Iteration 219/1000 | Loss: 0.00001520
Iteration 220/1000 | Loss: 0.00001520
Iteration 221/1000 | Loss: 0.00001520
Iteration 222/1000 | Loss: 0.00001520
Iteration 223/1000 | Loss: 0.00001520
Iteration 224/1000 | Loss: 0.00001519
Iteration 225/1000 | Loss: 0.00001519
Iteration 226/1000 | Loss: 0.00001519
Iteration 227/1000 | Loss: 0.00001519
Iteration 228/1000 | Loss: 0.00001518
Iteration 229/1000 | Loss: 0.00001518
Iteration 230/1000 | Loss: 0.00001518
Iteration 231/1000 | Loss: 0.00001518
Iteration 232/1000 | Loss: 0.00001518
Iteration 233/1000 | Loss: 0.00001518
Iteration 234/1000 | Loss: 0.00001518
Iteration 235/1000 | Loss: 0.00001518
Iteration 236/1000 | Loss: 0.00001518
Iteration 237/1000 | Loss: 0.00001518
Iteration 238/1000 | Loss: 0.00001518
Iteration 239/1000 | Loss: 0.00001518
Iteration 240/1000 | Loss: 0.00001517
Iteration 241/1000 | Loss: 0.00001517
Iteration 242/1000 | Loss: 0.00001517
Iteration 243/1000 | Loss: 0.00001517
Iteration 244/1000 | Loss: 0.00001517
Iteration 245/1000 | Loss: 0.00001517
Iteration 246/1000 | Loss: 0.00001517
Iteration 247/1000 | Loss: 0.00001517
Iteration 248/1000 | Loss: 0.00001517
Iteration 249/1000 | Loss: 0.00001517
Iteration 250/1000 | Loss: 0.00001517
Iteration 251/1000 | Loss: 0.00001517
Iteration 252/1000 | Loss: 0.00001517
Iteration 253/1000 | Loss: 0.00001517
Iteration 254/1000 | Loss: 0.00001517
Iteration 255/1000 | Loss: 0.00001517
Iteration 256/1000 | Loss: 0.00001517
Iteration 257/1000 | Loss: 0.00001517
Iteration 258/1000 | Loss: 0.00001517
Iteration 259/1000 | Loss: 0.00001517
Iteration 260/1000 | Loss: 0.00001517
Iteration 261/1000 | Loss: 0.00001517
Iteration 262/1000 | Loss: 0.00001517
Iteration 263/1000 | Loss: 0.00001517
Iteration 264/1000 | Loss: 0.00001517
Iteration 265/1000 | Loss: 0.00001517
Iteration 266/1000 | Loss: 0.00001517
Iteration 267/1000 | Loss: 0.00001517
Iteration 268/1000 | Loss: 0.00001517
Iteration 269/1000 | Loss: 0.00001517
Iteration 270/1000 | Loss: 0.00001517
Iteration 271/1000 | Loss: 0.00001517
Iteration 272/1000 | Loss: 0.00001517
Iteration 273/1000 | Loss: 0.00001517
Iteration 274/1000 | Loss: 0.00001517
Iteration 275/1000 | Loss: 0.00001517
Iteration 276/1000 | Loss: 0.00001517
Iteration 277/1000 | Loss: 0.00001517
Iteration 278/1000 | Loss: 0.00001517
Iteration 279/1000 | Loss: 0.00001517
Iteration 280/1000 | Loss: 0.00001517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [1.5166961020440795e-05, 1.5166961020440795e-05, 1.5166961020440795e-05, 1.5166961020440795e-05, 1.5166961020440795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5166961020440795e-05

Optimization complete. Final v2v error: 3.2116763591766357 mm

Highest mean error: 3.66048264503479 mm for frame 2

Lowest mean error: 2.9164531230926514 mm for frame 40

Saving results

Total time: 47.35276818275452
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917988
Iteration 2/25 | Loss: 0.00168148
Iteration 3/25 | Loss: 0.00138116
Iteration 4/25 | Loss: 0.00135892
Iteration 5/25 | Loss: 0.00135283
Iteration 6/25 | Loss: 0.00135130
Iteration 7/25 | Loss: 0.00135130
Iteration 8/25 | Loss: 0.00135130
Iteration 9/25 | Loss: 0.00135130
Iteration 10/25 | Loss: 0.00135130
Iteration 11/25 | Loss: 0.00135130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013513019075617194, 0.0013513019075617194, 0.0013513019075617194, 0.0013513019075617194, 0.0013513019075617194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013513019075617194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87818265
Iteration 2/25 | Loss: 0.00105668
Iteration 3/25 | Loss: 0.00105667
Iteration 4/25 | Loss: 0.00105666
Iteration 5/25 | Loss: 0.00105666
Iteration 6/25 | Loss: 0.00105666
Iteration 7/25 | Loss: 0.00105666
Iteration 8/25 | Loss: 0.00105666
Iteration 9/25 | Loss: 0.00105666
Iteration 10/25 | Loss: 0.00105666
Iteration 11/25 | Loss: 0.00105666
Iteration 12/25 | Loss: 0.00105666
Iteration 13/25 | Loss: 0.00105666
Iteration 14/25 | Loss: 0.00105666
Iteration 15/25 | Loss: 0.00105666
Iteration 16/25 | Loss: 0.00105666
Iteration 17/25 | Loss: 0.00105666
Iteration 18/25 | Loss: 0.00105666
Iteration 19/25 | Loss: 0.00105666
Iteration 20/25 | Loss: 0.00105666
Iteration 21/25 | Loss: 0.00105666
Iteration 22/25 | Loss: 0.00105666
Iteration 23/25 | Loss: 0.00105666
Iteration 24/25 | Loss: 0.00105666
Iteration 25/25 | Loss: 0.00105666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105666
Iteration 2/1000 | Loss: 0.00006865
Iteration 3/1000 | Loss: 0.00004657
Iteration 4/1000 | Loss: 0.00003830
Iteration 5/1000 | Loss: 0.00003589
Iteration 6/1000 | Loss: 0.00003434
Iteration 7/1000 | Loss: 0.00003280
Iteration 8/1000 | Loss: 0.00003185
Iteration 9/1000 | Loss: 0.00003097
Iteration 10/1000 | Loss: 0.00003048
Iteration 11/1000 | Loss: 0.00003007
Iteration 12/1000 | Loss: 0.00002970
Iteration 13/1000 | Loss: 0.00002938
Iteration 14/1000 | Loss: 0.00002908
Iteration 15/1000 | Loss: 0.00002876
Iteration 16/1000 | Loss: 0.00002855
Iteration 17/1000 | Loss: 0.00002850
Iteration 18/1000 | Loss: 0.00002841
Iteration 19/1000 | Loss: 0.00002824
Iteration 20/1000 | Loss: 0.00002808
Iteration 21/1000 | Loss: 0.00002799
Iteration 22/1000 | Loss: 0.00002789
Iteration 23/1000 | Loss: 0.00002786
Iteration 24/1000 | Loss: 0.00002785
Iteration 25/1000 | Loss: 0.00002781
Iteration 26/1000 | Loss: 0.00002776
Iteration 27/1000 | Loss: 0.00002776
Iteration 28/1000 | Loss: 0.00002775
Iteration 29/1000 | Loss: 0.00002775
Iteration 30/1000 | Loss: 0.00002774
Iteration 31/1000 | Loss: 0.00002774
Iteration 32/1000 | Loss: 0.00002773
Iteration 33/1000 | Loss: 0.00002773
Iteration 34/1000 | Loss: 0.00002772
Iteration 35/1000 | Loss: 0.00002772
Iteration 36/1000 | Loss: 0.00002772
Iteration 37/1000 | Loss: 0.00002772
Iteration 38/1000 | Loss: 0.00002771
Iteration 39/1000 | Loss: 0.00002771
Iteration 40/1000 | Loss: 0.00002771
Iteration 41/1000 | Loss: 0.00002770
Iteration 42/1000 | Loss: 0.00002770
Iteration 43/1000 | Loss: 0.00002770
Iteration 44/1000 | Loss: 0.00002770
Iteration 45/1000 | Loss: 0.00002769
Iteration 46/1000 | Loss: 0.00002769
Iteration 47/1000 | Loss: 0.00002769
Iteration 48/1000 | Loss: 0.00002769
Iteration 49/1000 | Loss: 0.00002768
Iteration 50/1000 | Loss: 0.00002768
Iteration 51/1000 | Loss: 0.00002768
Iteration 52/1000 | Loss: 0.00002768
Iteration 53/1000 | Loss: 0.00002768
Iteration 54/1000 | Loss: 0.00002768
Iteration 55/1000 | Loss: 0.00002768
Iteration 56/1000 | Loss: 0.00002768
Iteration 57/1000 | Loss: 0.00002767
Iteration 58/1000 | Loss: 0.00002767
Iteration 59/1000 | Loss: 0.00002767
Iteration 60/1000 | Loss: 0.00002767
Iteration 61/1000 | Loss: 0.00002767
Iteration 62/1000 | Loss: 0.00002767
Iteration 63/1000 | Loss: 0.00002767
Iteration 64/1000 | Loss: 0.00002766
Iteration 65/1000 | Loss: 0.00002766
Iteration 66/1000 | Loss: 0.00002766
Iteration 67/1000 | Loss: 0.00002766
Iteration 68/1000 | Loss: 0.00002766
Iteration 69/1000 | Loss: 0.00002766
Iteration 70/1000 | Loss: 0.00002765
Iteration 71/1000 | Loss: 0.00002765
Iteration 72/1000 | Loss: 0.00002765
Iteration 73/1000 | Loss: 0.00002765
Iteration 74/1000 | Loss: 0.00002765
Iteration 75/1000 | Loss: 0.00002765
Iteration 76/1000 | Loss: 0.00002764
Iteration 77/1000 | Loss: 0.00002764
Iteration 78/1000 | Loss: 0.00002764
Iteration 79/1000 | Loss: 0.00002764
Iteration 80/1000 | Loss: 0.00002764
Iteration 81/1000 | Loss: 0.00002764
Iteration 82/1000 | Loss: 0.00002764
Iteration 83/1000 | Loss: 0.00002764
Iteration 84/1000 | Loss: 0.00002763
Iteration 85/1000 | Loss: 0.00002763
Iteration 86/1000 | Loss: 0.00002763
Iteration 87/1000 | Loss: 0.00002763
Iteration 88/1000 | Loss: 0.00002763
Iteration 89/1000 | Loss: 0.00002763
Iteration 90/1000 | Loss: 0.00002763
Iteration 91/1000 | Loss: 0.00002762
Iteration 92/1000 | Loss: 0.00002762
Iteration 93/1000 | Loss: 0.00002762
Iteration 94/1000 | Loss: 0.00002762
Iteration 95/1000 | Loss: 0.00002762
Iteration 96/1000 | Loss: 0.00002761
Iteration 97/1000 | Loss: 0.00002761
Iteration 98/1000 | Loss: 0.00002761
Iteration 99/1000 | Loss: 0.00002761
Iteration 100/1000 | Loss: 0.00002761
Iteration 101/1000 | Loss: 0.00002761
Iteration 102/1000 | Loss: 0.00002761
Iteration 103/1000 | Loss: 0.00002761
Iteration 104/1000 | Loss: 0.00002760
Iteration 105/1000 | Loss: 0.00002760
Iteration 106/1000 | Loss: 0.00002760
Iteration 107/1000 | Loss: 0.00002760
Iteration 108/1000 | Loss: 0.00002760
Iteration 109/1000 | Loss: 0.00002760
Iteration 110/1000 | Loss: 0.00002760
Iteration 111/1000 | Loss: 0.00002760
Iteration 112/1000 | Loss: 0.00002760
Iteration 113/1000 | Loss: 0.00002760
Iteration 114/1000 | Loss: 0.00002760
Iteration 115/1000 | Loss: 0.00002760
Iteration 116/1000 | Loss: 0.00002760
Iteration 117/1000 | Loss: 0.00002759
Iteration 118/1000 | Loss: 0.00002759
Iteration 119/1000 | Loss: 0.00002759
Iteration 120/1000 | Loss: 0.00002759
Iteration 121/1000 | Loss: 0.00002759
Iteration 122/1000 | Loss: 0.00002759
Iteration 123/1000 | Loss: 0.00002759
Iteration 124/1000 | Loss: 0.00002759
Iteration 125/1000 | Loss: 0.00002759
Iteration 126/1000 | Loss: 0.00002759
Iteration 127/1000 | Loss: 0.00002759
Iteration 128/1000 | Loss: 0.00002759
Iteration 129/1000 | Loss: 0.00002759
Iteration 130/1000 | Loss: 0.00002759
Iteration 131/1000 | Loss: 0.00002758
Iteration 132/1000 | Loss: 0.00002758
Iteration 133/1000 | Loss: 0.00002758
Iteration 134/1000 | Loss: 0.00002758
Iteration 135/1000 | Loss: 0.00002758
Iteration 136/1000 | Loss: 0.00002758
Iteration 137/1000 | Loss: 0.00002758
Iteration 138/1000 | Loss: 0.00002758
Iteration 139/1000 | Loss: 0.00002758
Iteration 140/1000 | Loss: 0.00002757
Iteration 141/1000 | Loss: 0.00002757
Iteration 142/1000 | Loss: 0.00002757
Iteration 143/1000 | Loss: 0.00002757
Iteration 144/1000 | Loss: 0.00002757
Iteration 145/1000 | Loss: 0.00002757
Iteration 146/1000 | Loss: 0.00002757
Iteration 147/1000 | Loss: 0.00002757
Iteration 148/1000 | Loss: 0.00002757
Iteration 149/1000 | Loss: 0.00002757
Iteration 150/1000 | Loss: 0.00002757
Iteration 151/1000 | Loss: 0.00002757
Iteration 152/1000 | Loss: 0.00002757
Iteration 153/1000 | Loss: 0.00002756
Iteration 154/1000 | Loss: 0.00002756
Iteration 155/1000 | Loss: 0.00002756
Iteration 156/1000 | Loss: 0.00002756
Iteration 157/1000 | Loss: 0.00002756
Iteration 158/1000 | Loss: 0.00002756
Iteration 159/1000 | Loss: 0.00002756
Iteration 160/1000 | Loss: 0.00002756
Iteration 161/1000 | Loss: 0.00002756
Iteration 162/1000 | Loss: 0.00002756
Iteration 163/1000 | Loss: 0.00002756
Iteration 164/1000 | Loss: 0.00002756
Iteration 165/1000 | Loss: 0.00002756
Iteration 166/1000 | Loss: 0.00002756
Iteration 167/1000 | Loss: 0.00002756
Iteration 168/1000 | Loss: 0.00002756
Iteration 169/1000 | Loss: 0.00002756
Iteration 170/1000 | Loss: 0.00002756
Iteration 171/1000 | Loss: 0.00002756
Iteration 172/1000 | Loss: 0.00002756
Iteration 173/1000 | Loss: 0.00002756
Iteration 174/1000 | Loss: 0.00002756
Iteration 175/1000 | Loss: 0.00002756
Iteration 176/1000 | Loss: 0.00002756
Iteration 177/1000 | Loss: 0.00002756
Iteration 178/1000 | Loss: 0.00002756
Iteration 179/1000 | Loss: 0.00002756
Iteration 180/1000 | Loss: 0.00002756
Iteration 181/1000 | Loss: 0.00002756
Iteration 182/1000 | Loss: 0.00002756
Iteration 183/1000 | Loss: 0.00002756
Iteration 184/1000 | Loss: 0.00002756
Iteration 185/1000 | Loss: 0.00002756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.755715286184568e-05, 2.755715286184568e-05, 2.755715286184568e-05, 2.755715286184568e-05, 2.755715286184568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.755715286184568e-05

Optimization complete. Final v2v error: 4.397684574127197 mm

Highest mean error: 5.316534519195557 mm for frame 134

Lowest mean error: 3.5454726219177246 mm for frame 26

Saving results

Total time: 49.523502588272095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831393
Iteration 2/25 | Loss: 0.00134862
Iteration 3/25 | Loss: 0.00124475
Iteration 4/25 | Loss: 0.00121787
Iteration 5/25 | Loss: 0.00120873
Iteration 6/25 | Loss: 0.00120683
Iteration 7/25 | Loss: 0.00120683
Iteration 8/25 | Loss: 0.00120683
Iteration 9/25 | Loss: 0.00120683
Iteration 10/25 | Loss: 0.00120683
Iteration 11/25 | Loss: 0.00120683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012068270007148385, 0.0012068270007148385, 0.0012068270007148385, 0.0012068270007148385, 0.0012068270007148385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012068270007148385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28024065
Iteration 2/25 | Loss: 0.00137675
Iteration 3/25 | Loss: 0.00137675
Iteration 4/25 | Loss: 0.00137675
Iteration 5/25 | Loss: 0.00137674
Iteration 6/25 | Loss: 0.00137674
Iteration 7/25 | Loss: 0.00137674
Iteration 8/25 | Loss: 0.00137674
Iteration 9/25 | Loss: 0.00137674
Iteration 10/25 | Loss: 0.00137674
Iteration 11/25 | Loss: 0.00137674
Iteration 12/25 | Loss: 0.00137674
Iteration 13/25 | Loss: 0.00137674
Iteration 14/25 | Loss: 0.00137674
Iteration 15/25 | Loss: 0.00137674
Iteration 16/25 | Loss: 0.00137674
Iteration 17/25 | Loss: 0.00137674
Iteration 18/25 | Loss: 0.00137674
Iteration 19/25 | Loss: 0.00137674
Iteration 20/25 | Loss: 0.00137674
Iteration 21/25 | Loss: 0.00137674
Iteration 22/25 | Loss: 0.00137674
Iteration 23/25 | Loss: 0.00137674
Iteration 24/25 | Loss: 0.00137674
Iteration 25/25 | Loss: 0.00137674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137674
Iteration 2/1000 | Loss: 0.00004568
Iteration 3/1000 | Loss: 0.00003310
Iteration 4/1000 | Loss: 0.00002711
Iteration 5/1000 | Loss: 0.00002523
Iteration 6/1000 | Loss: 0.00002369
Iteration 7/1000 | Loss: 0.00002254
Iteration 8/1000 | Loss: 0.00002185
Iteration 9/1000 | Loss: 0.00002113
Iteration 10/1000 | Loss: 0.00002074
Iteration 11/1000 | Loss: 0.00002028
Iteration 12/1000 | Loss: 0.00001992
Iteration 13/1000 | Loss: 0.00001983
Iteration 14/1000 | Loss: 0.00001960
Iteration 15/1000 | Loss: 0.00001943
Iteration 16/1000 | Loss: 0.00001934
Iteration 17/1000 | Loss: 0.00001926
Iteration 18/1000 | Loss: 0.00001918
Iteration 19/1000 | Loss: 0.00001914
Iteration 20/1000 | Loss: 0.00001910
Iteration 21/1000 | Loss: 0.00001902
Iteration 22/1000 | Loss: 0.00001899
Iteration 23/1000 | Loss: 0.00001899
Iteration 24/1000 | Loss: 0.00001899
Iteration 25/1000 | Loss: 0.00001895
Iteration 26/1000 | Loss: 0.00001895
Iteration 27/1000 | Loss: 0.00001895
Iteration 28/1000 | Loss: 0.00001895
Iteration 29/1000 | Loss: 0.00001894
Iteration 30/1000 | Loss: 0.00001894
Iteration 31/1000 | Loss: 0.00001893
Iteration 32/1000 | Loss: 0.00001892
Iteration 33/1000 | Loss: 0.00001892
Iteration 34/1000 | Loss: 0.00001891
Iteration 35/1000 | Loss: 0.00001891
Iteration 36/1000 | Loss: 0.00001891
Iteration 37/1000 | Loss: 0.00001891
Iteration 38/1000 | Loss: 0.00001890
Iteration 39/1000 | Loss: 0.00001890
Iteration 40/1000 | Loss: 0.00001889
Iteration 41/1000 | Loss: 0.00001889
Iteration 42/1000 | Loss: 0.00001889
Iteration 43/1000 | Loss: 0.00001888
Iteration 44/1000 | Loss: 0.00001888
Iteration 45/1000 | Loss: 0.00001888
Iteration 46/1000 | Loss: 0.00001887
Iteration 47/1000 | Loss: 0.00001887
Iteration 48/1000 | Loss: 0.00001887
Iteration 49/1000 | Loss: 0.00001887
Iteration 50/1000 | Loss: 0.00001887
Iteration 51/1000 | Loss: 0.00001886
Iteration 52/1000 | Loss: 0.00001886
Iteration 53/1000 | Loss: 0.00001886
Iteration 54/1000 | Loss: 0.00001886
Iteration 55/1000 | Loss: 0.00001886
Iteration 56/1000 | Loss: 0.00001886
Iteration 57/1000 | Loss: 0.00001886
Iteration 58/1000 | Loss: 0.00001886
Iteration 59/1000 | Loss: 0.00001885
Iteration 60/1000 | Loss: 0.00001885
Iteration 61/1000 | Loss: 0.00001885
Iteration 62/1000 | Loss: 0.00001884
Iteration 63/1000 | Loss: 0.00001884
Iteration 64/1000 | Loss: 0.00001884
Iteration 65/1000 | Loss: 0.00001883
Iteration 66/1000 | Loss: 0.00001883
Iteration 67/1000 | Loss: 0.00001883
Iteration 68/1000 | Loss: 0.00001883
Iteration 69/1000 | Loss: 0.00001883
Iteration 70/1000 | Loss: 0.00001883
Iteration 71/1000 | Loss: 0.00001883
Iteration 72/1000 | Loss: 0.00001882
Iteration 73/1000 | Loss: 0.00001882
Iteration 74/1000 | Loss: 0.00001882
Iteration 75/1000 | Loss: 0.00001882
Iteration 76/1000 | Loss: 0.00001882
Iteration 77/1000 | Loss: 0.00001882
Iteration 78/1000 | Loss: 0.00001882
Iteration 79/1000 | Loss: 0.00001882
Iteration 80/1000 | Loss: 0.00001881
Iteration 81/1000 | Loss: 0.00001881
Iteration 82/1000 | Loss: 0.00001881
Iteration 83/1000 | Loss: 0.00001881
Iteration 84/1000 | Loss: 0.00001881
Iteration 85/1000 | Loss: 0.00001880
Iteration 86/1000 | Loss: 0.00001880
Iteration 87/1000 | Loss: 0.00001880
Iteration 88/1000 | Loss: 0.00001880
Iteration 89/1000 | Loss: 0.00001880
Iteration 90/1000 | Loss: 0.00001880
Iteration 91/1000 | Loss: 0.00001880
Iteration 92/1000 | Loss: 0.00001880
Iteration 93/1000 | Loss: 0.00001879
Iteration 94/1000 | Loss: 0.00001879
Iteration 95/1000 | Loss: 0.00001879
Iteration 96/1000 | Loss: 0.00001879
Iteration 97/1000 | Loss: 0.00001879
Iteration 98/1000 | Loss: 0.00001879
Iteration 99/1000 | Loss: 0.00001879
Iteration 100/1000 | Loss: 0.00001879
Iteration 101/1000 | Loss: 0.00001879
Iteration 102/1000 | Loss: 0.00001879
Iteration 103/1000 | Loss: 0.00001878
Iteration 104/1000 | Loss: 0.00001878
Iteration 105/1000 | Loss: 0.00001878
Iteration 106/1000 | Loss: 0.00001878
Iteration 107/1000 | Loss: 0.00001878
Iteration 108/1000 | Loss: 0.00001878
Iteration 109/1000 | Loss: 0.00001877
Iteration 110/1000 | Loss: 0.00001877
Iteration 111/1000 | Loss: 0.00001877
Iteration 112/1000 | Loss: 0.00001877
Iteration 113/1000 | Loss: 0.00001877
Iteration 114/1000 | Loss: 0.00001877
Iteration 115/1000 | Loss: 0.00001877
Iteration 116/1000 | Loss: 0.00001877
Iteration 117/1000 | Loss: 0.00001877
Iteration 118/1000 | Loss: 0.00001877
Iteration 119/1000 | Loss: 0.00001877
Iteration 120/1000 | Loss: 0.00001877
Iteration 121/1000 | Loss: 0.00001877
Iteration 122/1000 | Loss: 0.00001877
Iteration 123/1000 | Loss: 0.00001877
Iteration 124/1000 | Loss: 0.00001876
Iteration 125/1000 | Loss: 0.00001876
Iteration 126/1000 | Loss: 0.00001876
Iteration 127/1000 | Loss: 0.00001876
Iteration 128/1000 | Loss: 0.00001876
Iteration 129/1000 | Loss: 0.00001876
Iteration 130/1000 | Loss: 0.00001876
Iteration 131/1000 | Loss: 0.00001876
Iteration 132/1000 | Loss: 0.00001876
Iteration 133/1000 | Loss: 0.00001876
Iteration 134/1000 | Loss: 0.00001876
Iteration 135/1000 | Loss: 0.00001876
Iteration 136/1000 | Loss: 0.00001876
Iteration 137/1000 | Loss: 0.00001876
Iteration 138/1000 | Loss: 0.00001876
Iteration 139/1000 | Loss: 0.00001876
Iteration 140/1000 | Loss: 0.00001876
Iteration 141/1000 | Loss: 0.00001876
Iteration 142/1000 | Loss: 0.00001876
Iteration 143/1000 | Loss: 0.00001876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.8757784346234985e-05, 1.8757784346234985e-05, 1.8757784346234985e-05, 1.8757784346234985e-05, 1.8757784346234985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8757784346234985e-05

Optimization complete. Final v2v error: 3.6358869075775146 mm

Highest mean error: 4.4839019775390625 mm for frame 125

Lowest mean error: 3.0268805027008057 mm for frame 56

Saving results

Total time: 48.54840707778931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957187
Iteration 2/25 | Loss: 0.00957187
Iteration 3/25 | Loss: 0.00957186
Iteration 4/25 | Loss: 0.00957186
Iteration 5/25 | Loss: 0.00957186
Iteration 6/25 | Loss: 0.00957186
Iteration 7/25 | Loss: 0.00957186
Iteration 8/25 | Loss: 0.00957186
Iteration 9/25 | Loss: 0.00957185
Iteration 10/25 | Loss: 0.00957185
Iteration 11/25 | Loss: 0.00957185
Iteration 12/25 | Loss: 0.00957185
Iteration 13/25 | Loss: 0.00957185
Iteration 14/25 | Loss: 0.00957185
Iteration 15/25 | Loss: 0.00957185
Iteration 16/25 | Loss: 0.00957184
Iteration 17/25 | Loss: 0.00957184
Iteration 18/25 | Loss: 0.00957184
Iteration 19/25 | Loss: 0.00957184
Iteration 20/25 | Loss: 0.00957184
Iteration 21/25 | Loss: 0.00957183
Iteration 22/25 | Loss: 0.00957183
Iteration 23/25 | Loss: 0.00957183
Iteration 24/25 | Loss: 0.00957183
Iteration 25/25 | Loss: 0.00957183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63094544
Iteration 2/25 | Loss: 0.20018861
Iteration 3/25 | Loss: 0.20018780
Iteration 4/25 | Loss: 0.20018777
Iteration 5/25 | Loss: 0.20018777
Iteration 6/25 | Loss: 0.20018774
Iteration 7/25 | Loss: 0.20018774
Iteration 8/25 | Loss: 0.20018774
Iteration 9/25 | Loss: 0.20018774
Iteration 10/25 | Loss: 0.20018774
Iteration 11/25 | Loss: 0.20018774
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.20018774271011353, 0.20018774271011353, 0.20018774271011353, 0.20018774271011353, 0.20018774271011353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.20018774271011353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.20018774
Iteration 2/1000 | Loss: 0.00405169
Iteration 3/1000 | Loss: 0.00105345
Iteration 4/1000 | Loss: 0.00054842
Iteration 5/1000 | Loss: 0.00030000
Iteration 6/1000 | Loss: 0.00018782
Iteration 7/1000 | Loss: 0.00055298
Iteration 8/1000 | Loss: 0.00010136
Iteration 9/1000 | Loss: 0.00007856
Iteration 10/1000 | Loss: 0.00169779
Iteration 11/1000 | Loss: 0.00028105
Iteration 12/1000 | Loss: 0.00005380
Iteration 13/1000 | Loss: 0.00011755
Iteration 14/1000 | Loss: 0.00008242
Iteration 15/1000 | Loss: 0.00007219
Iteration 16/1000 | Loss: 0.00003803
Iteration 17/1000 | Loss: 0.00005909
Iteration 18/1000 | Loss: 0.00007174
Iteration 19/1000 | Loss: 0.00003067
Iteration 20/1000 | Loss: 0.00005378
Iteration 21/1000 | Loss: 0.00002669
Iteration 22/1000 | Loss: 0.00002621
Iteration 23/1000 | Loss: 0.00002273
Iteration 24/1000 | Loss: 0.00002193
Iteration 25/1000 | Loss: 0.00010840
Iteration 26/1000 | Loss: 0.00002078
Iteration 27/1000 | Loss: 0.00002040
Iteration 28/1000 | Loss: 0.00001997
Iteration 29/1000 | Loss: 0.00001958
Iteration 30/1000 | Loss: 0.00001920
Iteration 31/1000 | Loss: 0.00001885
Iteration 32/1000 | Loss: 0.00001851
Iteration 33/1000 | Loss: 0.00001821
Iteration 34/1000 | Loss: 0.00001795
Iteration 35/1000 | Loss: 0.00001790
Iteration 36/1000 | Loss: 0.00001777
Iteration 37/1000 | Loss: 0.00001776
Iteration 38/1000 | Loss: 0.00001775
Iteration 39/1000 | Loss: 0.00001773
Iteration 40/1000 | Loss: 0.00001771
Iteration 41/1000 | Loss: 0.00001770
Iteration 42/1000 | Loss: 0.00001770
Iteration 43/1000 | Loss: 0.00001769
Iteration 44/1000 | Loss: 0.00001767
Iteration 45/1000 | Loss: 0.00001767
Iteration 46/1000 | Loss: 0.00001767
Iteration 47/1000 | Loss: 0.00001767
Iteration 48/1000 | Loss: 0.00001766
Iteration 49/1000 | Loss: 0.00001766
Iteration 50/1000 | Loss: 0.00001766
Iteration 51/1000 | Loss: 0.00001766
Iteration 52/1000 | Loss: 0.00001766
Iteration 53/1000 | Loss: 0.00001766
Iteration 54/1000 | Loss: 0.00001766
Iteration 55/1000 | Loss: 0.00001765
Iteration 56/1000 | Loss: 0.00001765
Iteration 57/1000 | Loss: 0.00001765
Iteration 58/1000 | Loss: 0.00001765
Iteration 59/1000 | Loss: 0.00001764
Iteration 60/1000 | Loss: 0.00001764
Iteration 61/1000 | Loss: 0.00001764
Iteration 62/1000 | Loss: 0.00001763
Iteration 63/1000 | Loss: 0.00001763
Iteration 64/1000 | Loss: 0.00001763
Iteration 65/1000 | Loss: 0.00001763
Iteration 66/1000 | Loss: 0.00001762
Iteration 67/1000 | Loss: 0.00001762
Iteration 68/1000 | Loss: 0.00001762
Iteration 69/1000 | Loss: 0.00001762
Iteration 70/1000 | Loss: 0.00001762
Iteration 71/1000 | Loss: 0.00001762
Iteration 72/1000 | Loss: 0.00001761
Iteration 73/1000 | Loss: 0.00001761
Iteration 74/1000 | Loss: 0.00001761
Iteration 75/1000 | Loss: 0.00001761
Iteration 76/1000 | Loss: 0.00001760
Iteration 77/1000 | Loss: 0.00001760
Iteration 78/1000 | Loss: 0.00001760
Iteration 79/1000 | Loss: 0.00001759
Iteration 80/1000 | Loss: 0.00001759
Iteration 81/1000 | Loss: 0.00001759
Iteration 82/1000 | Loss: 0.00001758
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001758
Iteration 86/1000 | Loss: 0.00001758
Iteration 87/1000 | Loss: 0.00001758
Iteration 88/1000 | Loss: 0.00001757
Iteration 89/1000 | Loss: 0.00001757
Iteration 90/1000 | Loss: 0.00001757
Iteration 91/1000 | Loss: 0.00001757
Iteration 92/1000 | Loss: 0.00001757
Iteration 93/1000 | Loss: 0.00001757
Iteration 94/1000 | Loss: 0.00001757
Iteration 95/1000 | Loss: 0.00001756
Iteration 96/1000 | Loss: 0.00001756
Iteration 97/1000 | Loss: 0.00001756
Iteration 98/1000 | Loss: 0.00001756
Iteration 99/1000 | Loss: 0.00001756
Iteration 100/1000 | Loss: 0.00001756
Iteration 101/1000 | Loss: 0.00001756
Iteration 102/1000 | Loss: 0.00001756
Iteration 103/1000 | Loss: 0.00001756
Iteration 104/1000 | Loss: 0.00001756
Iteration 105/1000 | Loss: 0.00001756
Iteration 106/1000 | Loss: 0.00001755
Iteration 107/1000 | Loss: 0.00001755
Iteration 108/1000 | Loss: 0.00001755
Iteration 109/1000 | Loss: 0.00001755
Iteration 110/1000 | Loss: 0.00001755
Iteration 111/1000 | Loss: 0.00001755
Iteration 112/1000 | Loss: 0.00001755
Iteration 113/1000 | Loss: 0.00001755
Iteration 114/1000 | Loss: 0.00001755
Iteration 115/1000 | Loss: 0.00001754
Iteration 116/1000 | Loss: 0.00001754
Iteration 117/1000 | Loss: 0.00001754
Iteration 118/1000 | Loss: 0.00001754
Iteration 119/1000 | Loss: 0.00001754
Iteration 120/1000 | Loss: 0.00001754
Iteration 121/1000 | Loss: 0.00001753
Iteration 122/1000 | Loss: 0.00001753
Iteration 123/1000 | Loss: 0.00001753
Iteration 124/1000 | Loss: 0.00001753
Iteration 125/1000 | Loss: 0.00001753
Iteration 126/1000 | Loss: 0.00001752
Iteration 127/1000 | Loss: 0.00001752
Iteration 128/1000 | Loss: 0.00001752
Iteration 129/1000 | Loss: 0.00001752
Iteration 130/1000 | Loss: 0.00001752
Iteration 131/1000 | Loss: 0.00001752
Iteration 132/1000 | Loss: 0.00001751
Iteration 133/1000 | Loss: 0.00001751
Iteration 134/1000 | Loss: 0.00001750
Iteration 135/1000 | Loss: 0.00001750
Iteration 136/1000 | Loss: 0.00001750
Iteration 137/1000 | Loss: 0.00001750
Iteration 138/1000 | Loss: 0.00001750
Iteration 139/1000 | Loss: 0.00001750
Iteration 140/1000 | Loss: 0.00001750
Iteration 141/1000 | Loss: 0.00001750
Iteration 142/1000 | Loss: 0.00001750
Iteration 143/1000 | Loss: 0.00001750
Iteration 144/1000 | Loss: 0.00001750
Iteration 145/1000 | Loss: 0.00001750
Iteration 146/1000 | Loss: 0.00001750
Iteration 147/1000 | Loss: 0.00001750
Iteration 148/1000 | Loss: 0.00001750
Iteration 149/1000 | Loss: 0.00001750
Iteration 150/1000 | Loss: 0.00001750
Iteration 151/1000 | Loss: 0.00001750
Iteration 152/1000 | Loss: 0.00001750
Iteration 153/1000 | Loss: 0.00001750
Iteration 154/1000 | Loss: 0.00001750
Iteration 155/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.749848343024496e-05, 1.749848343024496e-05, 1.749848343024496e-05, 1.749848343024496e-05, 1.749848343024496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.749848343024496e-05

Optimization complete. Final v2v error: 3.6194329261779785 mm

Highest mean error: 3.8128817081451416 mm for frame 21

Lowest mean error: 3.4395992755889893 mm for frame 123

Saving results

Total time: 73.09372854232788
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773670
Iteration 2/25 | Loss: 0.00131596
Iteration 3/25 | Loss: 0.00122254
Iteration 4/25 | Loss: 0.00121331
Iteration 5/25 | Loss: 0.00121139
Iteration 6/25 | Loss: 0.00121139
Iteration 7/25 | Loss: 0.00121139
Iteration 8/25 | Loss: 0.00121139
Iteration 9/25 | Loss: 0.00121139
Iteration 10/25 | Loss: 0.00121139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012113871052861214, 0.0012113871052861214, 0.0012113871052861214, 0.0012113871052861214, 0.0012113871052861214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012113871052861214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34979749
Iteration 2/25 | Loss: 0.00102116
Iteration 3/25 | Loss: 0.00102116
Iteration 4/25 | Loss: 0.00102116
Iteration 5/25 | Loss: 0.00102116
Iteration 6/25 | Loss: 0.00102116
Iteration 7/25 | Loss: 0.00102116
Iteration 8/25 | Loss: 0.00102116
Iteration 9/25 | Loss: 0.00102116
Iteration 10/25 | Loss: 0.00102116
Iteration 11/25 | Loss: 0.00102116
Iteration 12/25 | Loss: 0.00102116
Iteration 13/25 | Loss: 0.00102116
Iteration 14/25 | Loss: 0.00102116
Iteration 15/25 | Loss: 0.00102116
Iteration 16/25 | Loss: 0.00102116
Iteration 17/25 | Loss: 0.00102116
Iteration 18/25 | Loss: 0.00102116
Iteration 19/25 | Loss: 0.00102116
Iteration 20/25 | Loss: 0.00102116
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010211580665782094, 0.0010211580665782094, 0.0010211580665782094, 0.0010211580665782094, 0.0010211580665782094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010211580665782094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102116
Iteration 2/1000 | Loss: 0.00002308
Iteration 3/1000 | Loss: 0.00001563
Iteration 4/1000 | Loss: 0.00001370
Iteration 5/1000 | Loss: 0.00001285
Iteration 6/1000 | Loss: 0.00001225
Iteration 7/1000 | Loss: 0.00001183
Iteration 8/1000 | Loss: 0.00001154
Iteration 9/1000 | Loss: 0.00001122
Iteration 10/1000 | Loss: 0.00001102
Iteration 11/1000 | Loss: 0.00001097
Iteration 12/1000 | Loss: 0.00001082
Iteration 13/1000 | Loss: 0.00001079
Iteration 14/1000 | Loss: 0.00001079
Iteration 15/1000 | Loss: 0.00001074
Iteration 16/1000 | Loss: 0.00001069
Iteration 17/1000 | Loss: 0.00001067
Iteration 18/1000 | Loss: 0.00001066
Iteration 19/1000 | Loss: 0.00001066
Iteration 20/1000 | Loss: 0.00001066
Iteration 21/1000 | Loss: 0.00001066
Iteration 22/1000 | Loss: 0.00001066
Iteration 23/1000 | Loss: 0.00001066
Iteration 24/1000 | Loss: 0.00001066
Iteration 25/1000 | Loss: 0.00001066
Iteration 26/1000 | Loss: 0.00001065
Iteration 27/1000 | Loss: 0.00001065
Iteration 28/1000 | Loss: 0.00001063
Iteration 29/1000 | Loss: 0.00001063
Iteration 30/1000 | Loss: 0.00001062
Iteration 31/1000 | Loss: 0.00001062
Iteration 32/1000 | Loss: 0.00001062
Iteration 33/1000 | Loss: 0.00001061
Iteration 34/1000 | Loss: 0.00001061
Iteration 35/1000 | Loss: 0.00001059
Iteration 36/1000 | Loss: 0.00001059
Iteration 37/1000 | Loss: 0.00001059
Iteration 38/1000 | Loss: 0.00001059
Iteration 39/1000 | Loss: 0.00001058
Iteration 40/1000 | Loss: 0.00001058
Iteration 41/1000 | Loss: 0.00001058
Iteration 42/1000 | Loss: 0.00001058
Iteration 43/1000 | Loss: 0.00001057
Iteration 44/1000 | Loss: 0.00001057
Iteration 45/1000 | Loss: 0.00001057
Iteration 46/1000 | Loss: 0.00001057
Iteration 47/1000 | Loss: 0.00001056
Iteration 48/1000 | Loss: 0.00001056
Iteration 49/1000 | Loss: 0.00001056
Iteration 50/1000 | Loss: 0.00001056
Iteration 51/1000 | Loss: 0.00001056
Iteration 52/1000 | Loss: 0.00001055
Iteration 53/1000 | Loss: 0.00001054
Iteration 54/1000 | Loss: 0.00001054
Iteration 55/1000 | Loss: 0.00001054
Iteration 56/1000 | Loss: 0.00001054
Iteration 57/1000 | Loss: 0.00001052
Iteration 58/1000 | Loss: 0.00001052
Iteration 59/1000 | Loss: 0.00001051
Iteration 60/1000 | Loss: 0.00001051
Iteration 61/1000 | Loss: 0.00001051
Iteration 62/1000 | Loss: 0.00001050
Iteration 63/1000 | Loss: 0.00001048
Iteration 64/1000 | Loss: 0.00001046
Iteration 65/1000 | Loss: 0.00001046
Iteration 66/1000 | Loss: 0.00001046
Iteration 67/1000 | Loss: 0.00001045
Iteration 68/1000 | Loss: 0.00001044
Iteration 69/1000 | Loss: 0.00001044
Iteration 70/1000 | Loss: 0.00001042
Iteration 71/1000 | Loss: 0.00001042
Iteration 72/1000 | Loss: 0.00001042
Iteration 73/1000 | Loss: 0.00001042
Iteration 74/1000 | Loss: 0.00001042
Iteration 75/1000 | Loss: 0.00001041
Iteration 76/1000 | Loss: 0.00001041
Iteration 77/1000 | Loss: 0.00001041
Iteration 78/1000 | Loss: 0.00001041
Iteration 79/1000 | Loss: 0.00001041
Iteration 80/1000 | Loss: 0.00001041
Iteration 81/1000 | Loss: 0.00001041
Iteration 82/1000 | Loss: 0.00001040
Iteration 83/1000 | Loss: 0.00001040
Iteration 84/1000 | Loss: 0.00001039
Iteration 85/1000 | Loss: 0.00001039
Iteration 86/1000 | Loss: 0.00001039
Iteration 87/1000 | Loss: 0.00001039
Iteration 88/1000 | Loss: 0.00001038
Iteration 89/1000 | Loss: 0.00001038
Iteration 90/1000 | Loss: 0.00001038
Iteration 91/1000 | Loss: 0.00001037
Iteration 92/1000 | Loss: 0.00001036
Iteration 93/1000 | Loss: 0.00001035
Iteration 94/1000 | Loss: 0.00001035
Iteration 95/1000 | Loss: 0.00001035
Iteration 96/1000 | Loss: 0.00001035
Iteration 97/1000 | Loss: 0.00001035
Iteration 98/1000 | Loss: 0.00001035
Iteration 99/1000 | Loss: 0.00001035
Iteration 100/1000 | Loss: 0.00001035
Iteration 101/1000 | Loss: 0.00001035
Iteration 102/1000 | Loss: 0.00001035
Iteration 103/1000 | Loss: 0.00001034
Iteration 104/1000 | Loss: 0.00001034
Iteration 105/1000 | Loss: 0.00001034
Iteration 106/1000 | Loss: 0.00001033
Iteration 107/1000 | Loss: 0.00001033
Iteration 108/1000 | Loss: 0.00001032
Iteration 109/1000 | Loss: 0.00001032
Iteration 110/1000 | Loss: 0.00001032
Iteration 111/1000 | Loss: 0.00001032
Iteration 112/1000 | Loss: 0.00001031
Iteration 113/1000 | Loss: 0.00001031
Iteration 114/1000 | Loss: 0.00001031
Iteration 115/1000 | Loss: 0.00001031
Iteration 116/1000 | Loss: 0.00001031
Iteration 117/1000 | Loss: 0.00001031
Iteration 118/1000 | Loss: 0.00001031
Iteration 119/1000 | Loss: 0.00001031
Iteration 120/1000 | Loss: 0.00001030
Iteration 121/1000 | Loss: 0.00001030
Iteration 122/1000 | Loss: 0.00001030
Iteration 123/1000 | Loss: 0.00001030
Iteration 124/1000 | Loss: 0.00001029
Iteration 125/1000 | Loss: 0.00001029
Iteration 126/1000 | Loss: 0.00001029
Iteration 127/1000 | Loss: 0.00001029
Iteration 128/1000 | Loss: 0.00001029
Iteration 129/1000 | Loss: 0.00001029
Iteration 130/1000 | Loss: 0.00001029
Iteration 131/1000 | Loss: 0.00001029
Iteration 132/1000 | Loss: 0.00001029
Iteration 133/1000 | Loss: 0.00001028
Iteration 134/1000 | Loss: 0.00001028
Iteration 135/1000 | Loss: 0.00001028
Iteration 136/1000 | Loss: 0.00001028
Iteration 137/1000 | Loss: 0.00001028
Iteration 138/1000 | Loss: 0.00001028
Iteration 139/1000 | Loss: 0.00001028
Iteration 140/1000 | Loss: 0.00001028
Iteration 141/1000 | Loss: 0.00001028
Iteration 142/1000 | Loss: 0.00001028
Iteration 143/1000 | Loss: 0.00001028
Iteration 144/1000 | Loss: 0.00001028
Iteration 145/1000 | Loss: 0.00001028
Iteration 146/1000 | Loss: 0.00001028
Iteration 147/1000 | Loss: 0.00001028
Iteration 148/1000 | Loss: 0.00001028
Iteration 149/1000 | Loss: 0.00001027
Iteration 150/1000 | Loss: 0.00001027
Iteration 151/1000 | Loss: 0.00001027
Iteration 152/1000 | Loss: 0.00001027
Iteration 153/1000 | Loss: 0.00001027
Iteration 154/1000 | Loss: 0.00001027
Iteration 155/1000 | Loss: 0.00001027
Iteration 156/1000 | Loss: 0.00001027
Iteration 157/1000 | Loss: 0.00001027
Iteration 158/1000 | Loss: 0.00001027
Iteration 159/1000 | Loss: 0.00001027
Iteration 160/1000 | Loss: 0.00001027
Iteration 161/1000 | Loss: 0.00001027
Iteration 162/1000 | Loss: 0.00001027
Iteration 163/1000 | Loss: 0.00001027
Iteration 164/1000 | Loss: 0.00001027
Iteration 165/1000 | Loss: 0.00001026
Iteration 166/1000 | Loss: 0.00001026
Iteration 167/1000 | Loss: 0.00001026
Iteration 168/1000 | Loss: 0.00001026
Iteration 169/1000 | Loss: 0.00001026
Iteration 170/1000 | Loss: 0.00001026
Iteration 171/1000 | Loss: 0.00001026
Iteration 172/1000 | Loss: 0.00001026
Iteration 173/1000 | Loss: 0.00001026
Iteration 174/1000 | Loss: 0.00001026
Iteration 175/1000 | Loss: 0.00001026
Iteration 176/1000 | Loss: 0.00001026
Iteration 177/1000 | Loss: 0.00001025
Iteration 178/1000 | Loss: 0.00001025
Iteration 179/1000 | Loss: 0.00001025
Iteration 180/1000 | Loss: 0.00001025
Iteration 181/1000 | Loss: 0.00001025
Iteration 182/1000 | Loss: 0.00001025
Iteration 183/1000 | Loss: 0.00001025
Iteration 184/1000 | Loss: 0.00001025
Iteration 185/1000 | Loss: 0.00001025
Iteration 186/1000 | Loss: 0.00001025
Iteration 187/1000 | Loss: 0.00001025
Iteration 188/1000 | Loss: 0.00001025
Iteration 189/1000 | Loss: 0.00001025
Iteration 190/1000 | Loss: 0.00001025
Iteration 191/1000 | Loss: 0.00001024
Iteration 192/1000 | Loss: 0.00001024
Iteration 193/1000 | Loss: 0.00001024
Iteration 194/1000 | Loss: 0.00001024
Iteration 195/1000 | Loss: 0.00001024
Iteration 196/1000 | Loss: 0.00001024
Iteration 197/1000 | Loss: 0.00001024
Iteration 198/1000 | Loss: 0.00001024
Iteration 199/1000 | Loss: 0.00001024
Iteration 200/1000 | Loss: 0.00001024
Iteration 201/1000 | Loss: 0.00001024
Iteration 202/1000 | Loss: 0.00001024
Iteration 203/1000 | Loss: 0.00001024
Iteration 204/1000 | Loss: 0.00001024
Iteration 205/1000 | Loss: 0.00001024
Iteration 206/1000 | Loss: 0.00001024
Iteration 207/1000 | Loss: 0.00001024
Iteration 208/1000 | Loss: 0.00001024
Iteration 209/1000 | Loss: 0.00001024
Iteration 210/1000 | Loss: 0.00001024
Iteration 211/1000 | Loss: 0.00001024
Iteration 212/1000 | Loss: 0.00001024
Iteration 213/1000 | Loss: 0.00001024
Iteration 214/1000 | Loss: 0.00001024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.0241574273095466e-05, 1.0241574273095466e-05, 1.0241574273095466e-05, 1.0241574273095466e-05, 1.0241574273095466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0241574273095466e-05

Optimization complete. Final v2v error: 2.737314224243164 mm

Highest mean error: 2.8819754123687744 mm for frame 62

Lowest mean error: 2.6025009155273438 mm for frame 149

Saving results

Total time: 39.91967177391052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00585174
Iteration 2/25 | Loss: 0.00136935
Iteration 3/25 | Loss: 0.00128388
Iteration 4/25 | Loss: 0.00127579
Iteration 5/25 | Loss: 0.00127455
Iteration 6/25 | Loss: 0.00127455
Iteration 7/25 | Loss: 0.00127455
Iteration 8/25 | Loss: 0.00127455
Iteration 9/25 | Loss: 0.00127455
Iteration 10/25 | Loss: 0.00127455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012745509156957269, 0.0012745509156957269, 0.0012745509156957269, 0.0012745509156957269, 0.0012745509156957269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012745509156957269

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.72353029
Iteration 2/25 | Loss: 0.00108407
Iteration 3/25 | Loss: 0.00108403
Iteration 4/25 | Loss: 0.00108403
Iteration 5/25 | Loss: 0.00108403
Iteration 6/25 | Loss: 0.00108403
Iteration 7/25 | Loss: 0.00108403
Iteration 8/25 | Loss: 0.00108403
Iteration 9/25 | Loss: 0.00108403
Iteration 10/25 | Loss: 0.00108403
Iteration 11/25 | Loss: 0.00108403
Iteration 12/25 | Loss: 0.00108403
Iteration 13/25 | Loss: 0.00108403
Iteration 14/25 | Loss: 0.00108403
Iteration 15/25 | Loss: 0.00108403
Iteration 16/25 | Loss: 0.00108403
Iteration 17/25 | Loss: 0.00108403
Iteration 18/25 | Loss: 0.00108403
Iteration 19/25 | Loss: 0.00108403
Iteration 20/25 | Loss: 0.00108403
Iteration 21/25 | Loss: 0.00108403
Iteration 22/25 | Loss: 0.00108403
Iteration 23/25 | Loss: 0.00108403
Iteration 24/25 | Loss: 0.00108403
Iteration 25/25 | Loss: 0.00108403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108403
Iteration 2/1000 | Loss: 0.00002521
Iteration 3/1000 | Loss: 0.00001953
Iteration 4/1000 | Loss: 0.00001812
Iteration 5/1000 | Loss: 0.00001732
Iteration 6/1000 | Loss: 0.00001679
Iteration 7/1000 | Loss: 0.00001674
Iteration 8/1000 | Loss: 0.00001647
Iteration 9/1000 | Loss: 0.00001612
Iteration 10/1000 | Loss: 0.00001589
Iteration 11/1000 | Loss: 0.00001568
Iteration 12/1000 | Loss: 0.00001549
Iteration 13/1000 | Loss: 0.00001549
Iteration 14/1000 | Loss: 0.00001546
Iteration 15/1000 | Loss: 0.00001545
Iteration 16/1000 | Loss: 0.00001545
Iteration 17/1000 | Loss: 0.00001545
Iteration 18/1000 | Loss: 0.00001544
Iteration 19/1000 | Loss: 0.00001543
Iteration 20/1000 | Loss: 0.00001538
Iteration 21/1000 | Loss: 0.00001537
Iteration 22/1000 | Loss: 0.00001536
Iteration 23/1000 | Loss: 0.00001535
Iteration 24/1000 | Loss: 0.00001531
Iteration 25/1000 | Loss: 0.00001529
Iteration 26/1000 | Loss: 0.00001529
Iteration 27/1000 | Loss: 0.00001525
Iteration 28/1000 | Loss: 0.00001523
Iteration 29/1000 | Loss: 0.00001522
Iteration 30/1000 | Loss: 0.00001519
Iteration 31/1000 | Loss: 0.00001519
Iteration 32/1000 | Loss: 0.00001518
Iteration 33/1000 | Loss: 0.00001514
Iteration 34/1000 | Loss: 0.00001510
Iteration 35/1000 | Loss: 0.00001506
Iteration 36/1000 | Loss: 0.00001506
Iteration 37/1000 | Loss: 0.00001506
Iteration 38/1000 | Loss: 0.00001505
Iteration 39/1000 | Loss: 0.00001504
Iteration 40/1000 | Loss: 0.00001503
Iteration 41/1000 | Loss: 0.00001503
Iteration 42/1000 | Loss: 0.00001502
Iteration 43/1000 | Loss: 0.00001502
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001501
Iteration 46/1000 | Loss: 0.00001501
Iteration 47/1000 | Loss: 0.00001500
Iteration 48/1000 | Loss: 0.00001500
Iteration 49/1000 | Loss: 0.00001500
Iteration 50/1000 | Loss: 0.00001499
Iteration 51/1000 | Loss: 0.00001499
Iteration 52/1000 | Loss: 0.00001499
Iteration 53/1000 | Loss: 0.00001498
Iteration 54/1000 | Loss: 0.00001497
Iteration 55/1000 | Loss: 0.00001497
Iteration 56/1000 | Loss: 0.00001497
Iteration 57/1000 | Loss: 0.00001497
Iteration 58/1000 | Loss: 0.00001496
Iteration 59/1000 | Loss: 0.00001496
Iteration 60/1000 | Loss: 0.00001495
Iteration 61/1000 | Loss: 0.00001495
Iteration 62/1000 | Loss: 0.00001495
Iteration 63/1000 | Loss: 0.00001494
Iteration 64/1000 | Loss: 0.00001494
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001493
Iteration 67/1000 | Loss: 0.00001493
Iteration 68/1000 | Loss: 0.00001493
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001491
Iteration 72/1000 | Loss: 0.00001491
Iteration 73/1000 | Loss: 0.00001491
Iteration 74/1000 | Loss: 0.00001491
Iteration 75/1000 | Loss: 0.00001488
Iteration 76/1000 | Loss: 0.00001488
Iteration 77/1000 | Loss: 0.00001488
Iteration 78/1000 | Loss: 0.00001487
Iteration 79/1000 | Loss: 0.00001487
Iteration 80/1000 | Loss: 0.00001487
Iteration 81/1000 | Loss: 0.00001486
Iteration 82/1000 | Loss: 0.00001486
Iteration 83/1000 | Loss: 0.00001486
Iteration 84/1000 | Loss: 0.00001486
Iteration 85/1000 | Loss: 0.00001485
Iteration 86/1000 | Loss: 0.00001485
Iteration 87/1000 | Loss: 0.00001484
Iteration 88/1000 | Loss: 0.00001484
Iteration 89/1000 | Loss: 0.00001484
Iteration 90/1000 | Loss: 0.00001483
Iteration 91/1000 | Loss: 0.00001483
Iteration 92/1000 | Loss: 0.00001483
Iteration 93/1000 | Loss: 0.00001482
Iteration 94/1000 | Loss: 0.00001482
Iteration 95/1000 | Loss: 0.00001481
Iteration 96/1000 | Loss: 0.00001481
Iteration 97/1000 | Loss: 0.00001481
Iteration 98/1000 | Loss: 0.00001480
Iteration 99/1000 | Loss: 0.00001480
Iteration 100/1000 | Loss: 0.00001480
Iteration 101/1000 | Loss: 0.00001480
Iteration 102/1000 | Loss: 0.00001480
Iteration 103/1000 | Loss: 0.00001480
Iteration 104/1000 | Loss: 0.00001480
Iteration 105/1000 | Loss: 0.00001479
Iteration 106/1000 | Loss: 0.00001479
Iteration 107/1000 | Loss: 0.00001479
Iteration 108/1000 | Loss: 0.00001479
Iteration 109/1000 | Loss: 0.00001479
Iteration 110/1000 | Loss: 0.00001479
Iteration 111/1000 | Loss: 0.00001479
Iteration 112/1000 | Loss: 0.00001479
Iteration 113/1000 | Loss: 0.00001479
Iteration 114/1000 | Loss: 0.00001479
Iteration 115/1000 | Loss: 0.00001479
Iteration 116/1000 | Loss: 0.00001479
Iteration 117/1000 | Loss: 0.00001479
Iteration 118/1000 | Loss: 0.00001479
Iteration 119/1000 | Loss: 0.00001478
Iteration 120/1000 | Loss: 0.00001478
Iteration 121/1000 | Loss: 0.00001478
Iteration 122/1000 | Loss: 0.00001477
Iteration 123/1000 | Loss: 0.00001477
Iteration 124/1000 | Loss: 0.00001477
Iteration 125/1000 | Loss: 0.00001477
Iteration 126/1000 | Loss: 0.00001476
Iteration 127/1000 | Loss: 0.00001476
Iteration 128/1000 | Loss: 0.00001476
Iteration 129/1000 | Loss: 0.00001476
Iteration 130/1000 | Loss: 0.00001476
Iteration 131/1000 | Loss: 0.00001476
Iteration 132/1000 | Loss: 0.00001476
Iteration 133/1000 | Loss: 0.00001476
Iteration 134/1000 | Loss: 0.00001476
Iteration 135/1000 | Loss: 0.00001476
Iteration 136/1000 | Loss: 0.00001476
Iteration 137/1000 | Loss: 0.00001476
Iteration 138/1000 | Loss: 0.00001476
Iteration 139/1000 | Loss: 0.00001476
Iteration 140/1000 | Loss: 0.00001476
Iteration 141/1000 | Loss: 0.00001475
Iteration 142/1000 | Loss: 0.00001475
Iteration 143/1000 | Loss: 0.00001475
Iteration 144/1000 | Loss: 0.00001475
Iteration 145/1000 | Loss: 0.00001475
Iteration 146/1000 | Loss: 0.00001475
Iteration 147/1000 | Loss: 0.00001474
Iteration 148/1000 | Loss: 0.00001474
Iteration 149/1000 | Loss: 0.00001474
Iteration 150/1000 | Loss: 0.00001474
Iteration 151/1000 | Loss: 0.00001474
Iteration 152/1000 | Loss: 0.00001474
Iteration 153/1000 | Loss: 0.00001474
Iteration 154/1000 | Loss: 0.00001474
Iteration 155/1000 | Loss: 0.00001474
Iteration 156/1000 | Loss: 0.00001474
Iteration 157/1000 | Loss: 0.00001474
Iteration 158/1000 | Loss: 0.00001473
Iteration 159/1000 | Loss: 0.00001473
Iteration 160/1000 | Loss: 0.00001473
Iteration 161/1000 | Loss: 0.00001473
Iteration 162/1000 | Loss: 0.00001473
Iteration 163/1000 | Loss: 0.00001473
Iteration 164/1000 | Loss: 0.00001473
Iteration 165/1000 | Loss: 0.00001473
Iteration 166/1000 | Loss: 0.00001473
Iteration 167/1000 | Loss: 0.00001473
Iteration 168/1000 | Loss: 0.00001473
Iteration 169/1000 | Loss: 0.00001473
Iteration 170/1000 | Loss: 0.00001473
Iteration 171/1000 | Loss: 0.00001473
Iteration 172/1000 | Loss: 0.00001473
Iteration 173/1000 | Loss: 0.00001473
Iteration 174/1000 | Loss: 0.00001473
Iteration 175/1000 | Loss: 0.00001473
Iteration 176/1000 | Loss: 0.00001473
Iteration 177/1000 | Loss: 0.00001473
Iteration 178/1000 | Loss: 0.00001473
Iteration 179/1000 | Loss: 0.00001473
Iteration 180/1000 | Loss: 0.00001473
Iteration 181/1000 | Loss: 0.00001473
Iteration 182/1000 | Loss: 0.00001473
Iteration 183/1000 | Loss: 0.00001473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.473397969675716e-05, 1.473397969675716e-05, 1.473397969675716e-05, 1.473397969675716e-05, 1.473397969675716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.473397969675716e-05

Optimization complete. Final v2v error: 3.2528507709503174 mm

Highest mean error: 3.580719470977783 mm for frame 178

Lowest mean error: 3.001716136932373 mm for frame 201

Saving results

Total time: 45.02628207206726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414016
Iteration 2/25 | Loss: 0.00134447
Iteration 3/25 | Loss: 0.00126774
Iteration 4/25 | Loss: 0.00125617
Iteration 5/25 | Loss: 0.00125312
Iteration 6/25 | Loss: 0.00125312
Iteration 7/25 | Loss: 0.00125312
Iteration 8/25 | Loss: 0.00125312
Iteration 9/25 | Loss: 0.00125312
Iteration 10/25 | Loss: 0.00125312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001253116293810308, 0.001253116293810308, 0.001253116293810308, 0.001253116293810308, 0.001253116293810308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001253116293810308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.45762801
Iteration 2/25 | Loss: 0.00112516
Iteration 3/25 | Loss: 0.00112512
Iteration 4/25 | Loss: 0.00112512
Iteration 5/25 | Loss: 0.00112512
Iteration 6/25 | Loss: 0.00112512
Iteration 7/25 | Loss: 0.00112512
Iteration 8/25 | Loss: 0.00112512
Iteration 9/25 | Loss: 0.00112512
Iteration 10/25 | Loss: 0.00112512
Iteration 11/25 | Loss: 0.00112512
Iteration 12/25 | Loss: 0.00112512
Iteration 13/25 | Loss: 0.00112512
Iteration 14/25 | Loss: 0.00112512
Iteration 15/25 | Loss: 0.00112512
Iteration 16/25 | Loss: 0.00112512
Iteration 17/25 | Loss: 0.00112512
Iteration 18/25 | Loss: 0.00112512
Iteration 19/25 | Loss: 0.00112512
Iteration 20/25 | Loss: 0.00112512
Iteration 21/25 | Loss: 0.00112512
Iteration 22/25 | Loss: 0.00112512
Iteration 23/25 | Loss: 0.00112512
Iteration 24/25 | Loss: 0.00112512
Iteration 25/25 | Loss: 0.00112512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112512
Iteration 2/1000 | Loss: 0.00003360
Iteration 3/1000 | Loss: 0.00002265
Iteration 4/1000 | Loss: 0.00002077
Iteration 5/1000 | Loss: 0.00002000
Iteration 6/1000 | Loss: 0.00001946
Iteration 7/1000 | Loss: 0.00001899
Iteration 8/1000 | Loss: 0.00001879
Iteration 9/1000 | Loss: 0.00001843
Iteration 10/1000 | Loss: 0.00001807
Iteration 11/1000 | Loss: 0.00001779
Iteration 12/1000 | Loss: 0.00001761
Iteration 13/1000 | Loss: 0.00001754
Iteration 14/1000 | Loss: 0.00001753
Iteration 15/1000 | Loss: 0.00001750
Iteration 16/1000 | Loss: 0.00001747
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001740
Iteration 20/1000 | Loss: 0.00001740
Iteration 21/1000 | Loss: 0.00001739
Iteration 22/1000 | Loss: 0.00001738
Iteration 23/1000 | Loss: 0.00001737
Iteration 24/1000 | Loss: 0.00001737
Iteration 25/1000 | Loss: 0.00001736
Iteration 26/1000 | Loss: 0.00001736
Iteration 27/1000 | Loss: 0.00001735
Iteration 28/1000 | Loss: 0.00001735
Iteration 29/1000 | Loss: 0.00001735
Iteration 30/1000 | Loss: 0.00001734
Iteration 31/1000 | Loss: 0.00001733
Iteration 32/1000 | Loss: 0.00001733
Iteration 33/1000 | Loss: 0.00001732
Iteration 34/1000 | Loss: 0.00001732
Iteration 35/1000 | Loss: 0.00001732
Iteration 36/1000 | Loss: 0.00001731
Iteration 37/1000 | Loss: 0.00001730
Iteration 38/1000 | Loss: 0.00001729
Iteration 39/1000 | Loss: 0.00001729
Iteration 40/1000 | Loss: 0.00001728
Iteration 41/1000 | Loss: 0.00001728
Iteration 42/1000 | Loss: 0.00001728
Iteration 43/1000 | Loss: 0.00001728
Iteration 44/1000 | Loss: 0.00001724
Iteration 45/1000 | Loss: 0.00001724
Iteration 46/1000 | Loss: 0.00001723
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001722
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001720
Iteration 52/1000 | Loss: 0.00001720
Iteration 53/1000 | Loss: 0.00001720
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001720
Iteration 56/1000 | Loss: 0.00001720
Iteration 57/1000 | Loss: 0.00001719
Iteration 58/1000 | Loss: 0.00001719
Iteration 59/1000 | Loss: 0.00001719
Iteration 60/1000 | Loss: 0.00001719
Iteration 61/1000 | Loss: 0.00001719
Iteration 62/1000 | Loss: 0.00001719
Iteration 63/1000 | Loss: 0.00001719
Iteration 64/1000 | Loss: 0.00001719
Iteration 65/1000 | Loss: 0.00001719
Iteration 66/1000 | Loss: 0.00001719
Iteration 67/1000 | Loss: 0.00001719
Iteration 68/1000 | Loss: 0.00001719
Iteration 69/1000 | Loss: 0.00001718
Iteration 70/1000 | Loss: 0.00001718
Iteration 71/1000 | Loss: 0.00001717
Iteration 72/1000 | Loss: 0.00001716
Iteration 73/1000 | Loss: 0.00001716
Iteration 74/1000 | Loss: 0.00001715
Iteration 75/1000 | Loss: 0.00001715
Iteration 76/1000 | Loss: 0.00001715
Iteration 77/1000 | Loss: 0.00001714
Iteration 78/1000 | Loss: 0.00001714
Iteration 79/1000 | Loss: 0.00001714
Iteration 80/1000 | Loss: 0.00001714
Iteration 81/1000 | Loss: 0.00001714
Iteration 82/1000 | Loss: 0.00001714
Iteration 83/1000 | Loss: 0.00001714
Iteration 84/1000 | Loss: 0.00001713
Iteration 85/1000 | Loss: 0.00001713
Iteration 86/1000 | Loss: 0.00001713
Iteration 87/1000 | Loss: 0.00001713
Iteration 88/1000 | Loss: 0.00001712
Iteration 89/1000 | Loss: 0.00001712
Iteration 90/1000 | Loss: 0.00001712
Iteration 91/1000 | Loss: 0.00001712
Iteration 92/1000 | Loss: 0.00001712
Iteration 93/1000 | Loss: 0.00001712
Iteration 94/1000 | Loss: 0.00001711
Iteration 95/1000 | Loss: 0.00001711
Iteration 96/1000 | Loss: 0.00001711
Iteration 97/1000 | Loss: 0.00001711
Iteration 98/1000 | Loss: 0.00001711
Iteration 99/1000 | Loss: 0.00001710
Iteration 100/1000 | Loss: 0.00001710
Iteration 101/1000 | Loss: 0.00001709
Iteration 102/1000 | Loss: 0.00001709
Iteration 103/1000 | Loss: 0.00001709
Iteration 104/1000 | Loss: 0.00001709
Iteration 105/1000 | Loss: 0.00001709
Iteration 106/1000 | Loss: 0.00001709
Iteration 107/1000 | Loss: 0.00001709
Iteration 108/1000 | Loss: 0.00001709
Iteration 109/1000 | Loss: 0.00001708
Iteration 110/1000 | Loss: 0.00001708
Iteration 111/1000 | Loss: 0.00001708
Iteration 112/1000 | Loss: 0.00001708
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001708
Iteration 116/1000 | Loss: 0.00001707
Iteration 117/1000 | Loss: 0.00001707
Iteration 118/1000 | Loss: 0.00001707
Iteration 119/1000 | Loss: 0.00001707
Iteration 120/1000 | Loss: 0.00001707
Iteration 121/1000 | Loss: 0.00001707
Iteration 122/1000 | Loss: 0.00001707
Iteration 123/1000 | Loss: 0.00001707
Iteration 124/1000 | Loss: 0.00001706
Iteration 125/1000 | Loss: 0.00001706
Iteration 126/1000 | Loss: 0.00001706
Iteration 127/1000 | Loss: 0.00001706
Iteration 128/1000 | Loss: 0.00001706
Iteration 129/1000 | Loss: 0.00001706
Iteration 130/1000 | Loss: 0.00001706
Iteration 131/1000 | Loss: 0.00001706
Iteration 132/1000 | Loss: 0.00001706
Iteration 133/1000 | Loss: 0.00001705
Iteration 134/1000 | Loss: 0.00001705
Iteration 135/1000 | Loss: 0.00001705
Iteration 136/1000 | Loss: 0.00001705
Iteration 137/1000 | Loss: 0.00001705
Iteration 138/1000 | Loss: 0.00001705
Iteration 139/1000 | Loss: 0.00001705
Iteration 140/1000 | Loss: 0.00001705
Iteration 141/1000 | Loss: 0.00001705
Iteration 142/1000 | Loss: 0.00001705
Iteration 143/1000 | Loss: 0.00001705
Iteration 144/1000 | Loss: 0.00001705
Iteration 145/1000 | Loss: 0.00001705
Iteration 146/1000 | Loss: 0.00001704
Iteration 147/1000 | Loss: 0.00001704
Iteration 148/1000 | Loss: 0.00001704
Iteration 149/1000 | Loss: 0.00001704
Iteration 150/1000 | Loss: 0.00001704
Iteration 151/1000 | Loss: 0.00001704
Iteration 152/1000 | Loss: 0.00001704
Iteration 153/1000 | Loss: 0.00001704
Iteration 154/1000 | Loss: 0.00001704
Iteration 155/1000 | Loss: 0.00001704
Iteration 156/1000 | Loss: 0.00001703
Iteration 157/1000 | Loss: 0.00001703
Iteration 158/1000 | Loss: 0.00001703
Iteration 159/1000 | Loss: 0.00001703
Iteration 160/1000 | Loss: 0.00001703
Iteration 161/1000 | Loss: 0.00001703
Iteration 162/1000 | Loss: 0.00001703
Iteration 163/1000 | Loss: 0.00001703
Iteration 164/1000 | Loss: 0.00001703
Iteration 165/1000 | Loss: 0.00001702
Iteration 166/1000 | Loss: 0.00001702
Iteration 167/1000 | Loss: 0.00001702
Iteration 168/1000 | Loss: 0.00001702
Iteration 169/1000 | Loss: 0.00001701
Iteration 170/1000 | Loss: 0.00001701
Iteration 171/1000 | Loss: 0.00001701
Iteration 172/1000 | Loss: 0.00001701
Iteration 173/1000 | Loss: 0.00001701
Iteration 174/1000 | Loss: 0.00001700
Iteration 175/1000 | Loss: 0.00001700
Iteration 176/1000 | Loss: 0.00001700
Iteration 177/1000 | Loss: 0.00001700
Iteration 178/1000 | Loss: 0.00001700
Iteration 179/1000 | Loss: 0.00001700
Iteration 180/1000 | Loss: 0.00001700
Iteration 181/1000 | Loss: 0.00001700
Iteration 182/1000 | Loss: 0.00001700
Iteration 183/1000 | Loss: 0.00001699
Iteration 184/1000 | Loss: 0.00001699
Iteration 185/1000 | Loss: 0.00001699
Iteration 186/1000 | Loss: 0.00001699
Iteration 187/1000 | Loss: 0.00001699
Iteration 188/1000 | Loss: 0.00001699
Iteration 189/1000 | Loss: 0.00001699
Iteration 190/1000 | Loss: 0.00001699
Iteration 191/1000 | Loss: 0.00001699
Iteration 192/1000 | Loss: 0.00001699
Iteration 193/1000 | Loss: 0.00001699
Iteration 194/1000 | Loss: 0.00001699
Iteration 195/1000 | Loss: 0.00001699
Iteration 196/1000 | Loss: 0.00001699
Iteration 197/1000 | Loss: 0.00001699
Iteration 198/1000 | Loss: 0.00001699
Iteration 199/1000 | Loss: 0.00001699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.698825872153975e-05, 1.698825872153975e-05, 1.698825872153975e-05, 1.698825872153975e-05, 1.698825872153975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.698825872153975e-05

Optimization complete. Final v2v error: 3.5491931438446045 mm

Highest mean error: 3.7099764347076416 mm for frame 107

Lowest mean error: 3.3819096088409424 mm for frame 49

Saving results

Total time: 41.22515344619751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851961
Iteration 2/25 | Loss: 0.00133840
Iteration 3/25 | Loss: 0.00126128
Iteration 4/25 | Loss: 0.00124714
Iteration 5/25 | Loss: 0.00124296
Iteration 6/25 | Loss: 0.00124215
Iteration 7/25 | Loss: 0.00124215
Iteration 8/25 | Loss: 0.00124215
Iteration 9/25 | Loss: 0.00124215
Iteration 10/25 | Loss: 0.00124215
Iteration 11/25 | Loss: 0.00124215
Iteration 12/25 | Loss: 0.00124215
Iteration 13/25 | Loss: 0.00124215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012421474093571305, 0.0012421474093571305, 0.0012421474093571305, 0.0012421474093571305, 0.0012421474093571305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012421474093571305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31856477
Iteration 2/25 | Loss: 0.00132907
Iteration 3/25 | Loss: 0.00132906
Iteration 4/25 | Loss: 0.00132906
Iteration 5/25 | Loss: 0.00132906
Iteration 6/25 | Loss: 0.00132906
Iteration 7/25 | Loss: 0.00132905
Iteration 8/25 | Loss: 0.00132905
Iteration 9/25 | Loss: 0.00132905
Iteration 10/25 | Loss: 0.00132905
Iteration 11/25 | Loss: 0.00132905
Iteration 12/25 | Loss: 0.00132905
Iteration 13/25 | Loss: 0.00132905
Iteration 14/25 | Loss: 0.00132905
Iteration 15/25 | Loss: 0.00132905
Iteration 16/25 | Loss: 0.00132905
Iteration 17/25 | Loss: 0.00132905
Iteration 18/25 | Loss: 0.00132905
Iteration 19/25 | Loss: 0.00132905
Iteration 20/25 | Loss: 0.00132905
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001329053775407374, 0.001329053775407374, 0.001329053775407374, 0.001329053775407374, 0.001329053775407374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001329053775407374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132905
Iteration 2/1000 | Loss: 0.00003789
Iteration 3/1000 | Loss: 0.00002532
Iteration 4/1000 | Loss: 0.00001916
Iteration 5/1000 | Loss: 0.00001749
Iteration 6/1000 | Loss: 0.00001645
Iteration 7/1000 | Loss: 0.00001565
Iteration 8/1000 | Loss: 0.00001511
Iteration 9/1000 | Loss: 0.00001472
Iteration 10/1000 | Loss: 0.00001453
Iteration 11/1000 | Loss: 0.00001440
Iteration 12/1000 | Loss: 0.00001433
Iteration 13/1000 | Loss: 0.00001428
Iteration 14/1000 | Loss: 0.00001417
Iteration 15/1000 | Loss: 0.00001413
Iteration 16/1000 | Loss: 0.00001411
Iteration 17/1000 | Loss: 0.00001409
Iteration 18/1000 | Loss: 0.00001394
Iteration 19/1000 | Loss: 0.00001392
Iteration 20/1000 | Loss: 0.00001389
Iteration 21/1000 | Loss: 0.00001383
Iteration 22/1000 | Loss: 0.00001381
Iteration 23/1000 | Loss: 0.00001380
Iteration 24/1000 | Loss: 0.00001380
Iteration 25/1000 | Loss: 0.00001379
Iteration 26/1000 | Loss: 0.00001379
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001378
Iteration 29/1000 | Loss: 0.00001375
Iteration 30/1000 | Loss: 0.00001375
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001375
Iteration 33/1000 | Loss: 0.00001374
Iteration 34/1000 | Loss: 0.00001373
Iteration 35/1000 | Loss: 0.00001373
Iteration 36/1000 | Loss: 0.00001372
Iteration 37/1000 | Loss: 0.00001372
Iteration 38/1000 | Loss: 0.00001371
Iteration 39/1000 | Loss: 0.00001370
Iteration 40/1000 | Loss: 0.00001370
Iteration 41/1000 | Loss: 0.00001369
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001368
Iteration 44/1000 | Loss: 0.00001368
Iteration 45/1000 | Loss: 0.00001367
Iteration 46/1000 | Loss: 0.00001367
Iteration 47/1000 | Loss: 0.00001367
Iteration 48/1000 | Loss: 0.00001367
Iteration 49/1000 | Loss: 0.00001366
Iteration 50/1000 | Loss: 0.00001366
Iteration 51/1000 | Loss: 0.00001366
Iteration 52/1000 | Loss: 0.00001366
Iteration 53/1000 | Loss: 0.00001366
Iteration 54/1000 | Loss: 0.00001366
Iteration 55/1000 | Loss: 0.00001366
Iteration 56/1000 | Loss: 0.00001366
Iteration 57/1000 | Loss: 0.00001365
Iteration 58/1000 | Loss: 0.00001365
Iteration 59/1000 | Loss: 0.00001365
Iteration 60/1000 | Loss: 0.00001365
Iteration 61/1000 | Loss: 0.00001365
Iteration 62/1000 | Loss: 0.00001365
Iteration 63/1000 | Loss: 0.00001365
Iteration 64/1000 | Loss: 0.00001365
Iteration 65/1000 | Loss: 0.00001365
Iteration 66/1000 | Loss: 0.00001365
Iteration 67/1000 | Loss: 0.00001365
Iteration 68/1000 | Loss: 0.00001365
Iteration 69/1000 | Loss: 0.00001364
Iteration 70/1000 | Loss: 0.00001364
Iteration 71/1000 | Loss: 0.00001364
Iteration 72/1000 | Loss: 0.00001364
Iteration 73/1000 | Loss: 0.00001363
Iteration 74/1000 | Loss: 0.00001363
Iteration 75/1000 | Loss: 0.00001363
Iteration 76/1000 | Loss: 0.00001363
Iteration 77/1000 | Loss: 0.00001362
Iteration 78/1000 | Loss: 0.00001362
Iteration 79/1000 | Loss: 0.00001362
Iteration 80/1000 | Loss: 0.00001362
Iteration 81/1000 | Loss: 0.00001362
Iteration 82/1000 | Loss: 0.00001361
Iteration 83/1000 | Loss: 0.00001361
Iteration 84/1000 | Loss: 0.00001361
Iteration 85/1000 | Loss: 0.00001361
Iteration 86/1000 | Loss: 0.00001361
Iteration 87/1000 | Loss: 0.00001360
Iteration 88/1000 | Loss: 0.00001360
Iteration 89/1000 | Loss: 0.00001360
Iteration 90/1000 | Loss: 0.00001360
Iteration 91/1000 | Loss: 0.00001359
Iteration 92/1000 | Loss: 0.00001359
Iteration 93/1000 | Loss: 0.00001359
Iteration 94/1000 | Loss: 0.00001359
Iteration 95/1000 | Loss: 0.00001358
Iteration 96/1000 | Loss: 0.00001358
Iteration 97/1000 | Loss: 0.00001358
Iteration 98/1000 | Loss: 0.00001358
Iteration 99/1000 | Loss: 0.00001358
Iteration 100/1000 | Loss: 0.00001358
Iteration 101/1000 | Loss: 0.00001358
Iteration 102/1000 | Loss: 0.00001357
Iteration 103/1000 | Loss: 0.00001357
Iteration 104/1000 | Loss: 0.00001357
Iteration 105/1000 | Loss: 0.00001357
Iteration 106/1000 | Loss: 0.00001357
Iteration 107/1000 | Loss: 0.00001357
Iteration 108/1000 | Loss: 0.00001356
Iteration 109/1000 | Loss: 0.00001356
Iteration 110/1000 | Loss: 0.00001356
Iteration 111/1000 | Loss: 0.00001356
Iteration 112/1000 | Loss: 0.00001356
Iteration 113/1000 | Loss: 0.00001356
Iteration 114/1000 | Loss: 0.00001356
Iteration 115/1000 | Loss: 0.00001356
Iteration 116/1000 | Loss: 0.00001356
Iteration 117/1000 | Loss: 0.00001356
Iteration 118/1000 | Loss: 0.00001356
Iteration 119/1000 | Loss: 0.00001355
Iteration 120/1000 | Loss: 0.00001355
Iteration 121/1000 | Loss: 0.00001355
Iteration 122/1000 | Loss: 0.00001355
Iteration 123/1000 | Loss: 0.00001355
Iteration 124/1000 | Loss: 0.00001355
Iteration 125/1000 | Loss: 0.00001354
Iteration 126/1000 | Loss: 0.00001354
Iteration 127/1000 | Loss: 0.00001354
Iteration 128/1000 | Loss: 0.00001354
Iteration 129/1000 | Loss: 0.00001353
Iteration 130/1000 | Loss: 0.00001353
Iteration 131/1000 | Loss: 0.00001353
Iteration 132/1000 | Loss: 0.00001353
Iteration 133/1000 | Loss: 0.00001353
Iteration 134/1000 | Loss: 0.00001353
Iteration 135/1000 | Loss: 0.00001353
Iteration 136/1000 | Loss: 0.00001353
Iteration 137/1000 | Loss: 0.00001352
Iteration 138/1000 | Loss: 0.00001352
Iteration 139/1000 | Loss: 0.00001352
Iteration 140/1000 | Loss: 0.00001352
Iteration 141/1000 | Loss: 0.00001352
Iteration 142/1000 | Loss: 0.00001351
Iteration 143/1000 | Loss: 0.00001351
Iteration 144/1000 | Loss: 0.00001351
Iteration 145/1000 | Loss: 0.00001351
Iteration 146/1000 | Loss: 0.00001350
Iteration 147/1000 | Loss: 0.00001350
Iteration 148/1000 | Loss: 0.00001350
Iteration 149/1000 | Loss: 0.00001350
Iteration 150/1000 | Loss: 0.00001350
Iteration 151/1000 | Loss: 0.00001350
Iteration 152/1000 | Loss: 0.00001350
Iteration 153/1000 | Loss: 0.00001350
Iteration 154/1000 | Loss: 0.00001350
Iteration 155/1000 | Loss: 0.00001349
Iteration 156/1000 | Loss: 0.00001349
Iteration 157/1000 | Loss: 0.00001349
Iteration 158/1000 | Loss: 0.00001349
Iteration 159/1000 | Loss: 0.00001349
Iteration 160/1000 | Loss: 0.00001349
Iteration 161/1000 | Loss: 0.00001348
Iteration 162/1000 | Loss: 0.00001348
Iteration 163/1000 | Loss: 0.00001348
Iteration 164/1000 | Loss: 0.00001348
Iteration 165/1000 | Loss: 0.00001348
Iteration 166/1000 | Loss: 0.00001348
Iteration 167/1000 | Loss: 0.00001348
Iteration 168/1000 | Loss: 0.00001348
Iteration 169/1000 | Loss: 0.00001348
Iteration 170/1000 | Loss: 0.00001348
Iteration 171/1000 | Loss: 0.00001348
Iteration 172/1000 | Loss: 0.00001348
Iteration 173/1000 | Loss: 0.00001348
Iteration 174/1000 | Loss: 0.00001348
Iteration 175/1000 | Loss: 0.00001348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.3480682355293538e-05, 1.3480682355293538e-05, 1.3480682355293538e-05, 1.3480682355293538e-05, 1.3480682355293538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3480682355293538e-05

Optimization complete. Final v2v error: 3.0404598712921143 mm

Highest mean error: 4.043198585510254 mm for frame 38

Lowest mean error: 2.5429701805114746 mm for frame 68

Saving results

Total time: 40.58293080329895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966964
Iteration 2/25 | Loss: 0.00163723
Iteration 3/25 | Loss: 0.00140951
Iteration 4/25 | Loss: 0.00139241
Iteration 5/25 | Loss: 0.00141575
Iteration 6/25 | Loss: 0.00141765
Iteration 7/25 | Loss: 0.00131262
Iteration 8/25 | Loss: 0.00129271
Iteration 9/25 | Loss: 0.00129447
Iteration 10/25 | Loss: 0.00127503
Iteration 11/25 | Loss: 0.00126438
Iteration 12/25 | Loss: 0.00126010
Iteration 13/25 | Loss: 0.00125933
Iteration 14/25 | Loss: 0.00125910
Iteration 15/25 | Loss: 0.00125900
Iteration 16/25 | Loss: 0.00125899
Iteration 17/25 | Loss: 0.00125899
Iteration 18/25 | Loss: 0.00125899
Iteration 19/25 | Loss: 0.00125899
Iteration 20/25 | Loss: 0.00125899
Iteration 21/25 | Loss: 0.00125899
Iteration 22/25 | Loss: 0.00125899
Iteration 23/25 | Loss: 0.00125898
Iteration 24/25 | Loss: 0.00125898
Iteration 25/25 | Loss: 0.00125898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.96901870
Iteration 2/25 | Loss: 0.00117064
Iteration 3/25 | Loss: 0.00117063
Iteration 4/25 | Loss: 0.00117063
Iteration 5/25 | Loss: 0.00117063
Iteration 6/25 | Loss: 0.00117063
Iteration 7/25 | Loss: 0.00117063
Iteration 8/25 | Loss: 0.00117063
Iteration 9/25 | Loss: 0.00117063
Iteration 10/25 | Loss: 0.00117062
Iteration 11/25 | Loss: 0.00117062
Iteration 12/25 | Loss: 0.00117062
Iteration 13/25 | Loss: 0.00117062
Iteration 14/25 | Loss: 0.00117062
Iteration 15/25 | Loss: 0.00117062
Iteration 16/25 | Loss: 0.00117062
Iteration 17/25 | Loss: 0.00117062
Iteration 18/25 | Loss: 0.00117062
Iteration 19/25 | Loss: 0.00117062
Iteration 20/25 | Loss: 0.00117062
Iteration 21/25 | Loss: 0.00117062
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001170624396763742, 0.001170624396763742, 0.001170624396763742, 0.001170624396763742, 0.001170624396763742]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001170624396763742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117062
Iteration 2/1000 | Loss: 0.00004560
Iteration 3/1000 | Loss: 0.00137817
Iteration 4/1000 | Loss: 0.00011128
Iteration 5/1000 | Loss: 0.00031697
Iteration 6/1000 | Loss: 0.00002473
Iteration 7/1000 | Loss: 0.00002215
Iteration 8/1000 | Loss: 0.00002099
Iteration 9/1000 | Loss: 0.00002009
Iteration 10/1000 | Loss: 0.00001971
Iteration 11/1000 | Loss: 0.00001941
Iteration 12/1000 | Loss: 0.00001919
Iteration 13/1000 | Loss: 0.00001910
Iteration 14/1000 | Loss: 0.00001908
Iteration 15/1000 | Loss: 0.00001904
Iteration 16/1000 | Loss: 0.00001888
Iteration 17/1000 | Loss: 0.00001883
Iteration 18/1000 | Loss: 0.00001879
Iteration 19/1000 | Loss: 0.00001879
Iteration 20/1000 | Loss: 0.00001878
Iteration 21/1000 | Loss: 0.00001877
Iteration 22/1000 | Loss: 0.00001876
Iteration 23/1000 | Loss: 0.00001876
Iteration 24/1000 | Loss: 0.00001875
Iteration 25/1000 | Loss: 0.00001871
Iteration 26/1000 | Loss: 0.00001869
Iteration 27/1000 | Loss: 0.00001868
Iteration 28/1000 | Loss: 0.00001868
Iteration 29/1000 | Loss: 0.00001867
Iteration 30/1000 | Loss: 0.00001866
Iteration 31/1000 | Loss: 0.00001865
Iteration 32/1000 | Loss: 0.00001864
Iteration 33/1000 | Loss: 0.00001864
Iteration 34/1000 | Loss: 0.00001863
Iteration 35/1000 | Loss: 0.00001862
Iteration 36/1000 | Loss: 0.00001862
Iteration 37/1000 | Loss: 0.00001862
Iteration 38/1000 | Loss: 0.00001862
Iteration 39/1000 | Loss: 0.00001861
Iteration 40/1000 | Loss: 0.00001861
Iteration 41/1000 | Loss: 0.00001861
Iteration 42/1000 | Loss: 0.00001860
Iteration 43/1000 | Loss: 0.00001860
Iteration 44/1000 | Loss: 0.00001859
Iteration 45/1000 | Loss: 0.00001859
Iteration 46/1000 | Loss: 0.00001858
Iteration 47/1000 | Loss: 0.00001857
Iteration 48/1000 | Loss: 0.00001857
Iteration 49/1000 | Loss: 0.00001856
Iteration 50/1000 | Loss: 0.00001856
Iteration 51/1000 | Loss: 0.00001855
Iteration 52/1000 | Loss: 0.00001855
Iteration 53/1000 | Loss: 0.00001855
Iteration 54/1000 | Loss: 0.00001854
Iteration 55/1000 | Loss: 0.00001854
Iteration 56/1000 | Loss: 0.00001853
Iteration 57/1000 | Loss: 0.00001853
Iteration 58/1000 | Loss: 0.00001853
Iteration 59/1000 | Loss: 0.00001853
Iteration 60/1000 | Loss: 0.00001853
Iteration 61/1000 | Loss: 0.00001852
Iteration 62/1000 | Loss: 0.00001851
Iteration 63/1000 | Loss: 0.00001851
Iteration 64/1000 | Loss: 0.00001851
Iteration 65/1000 | Loss: 0.00001851
Iteration 66/1000 | Loss: 0.00001850
Iteration 67/1000 | Loss: 0.00001850
Iteration 68/1000 | Loss: 0.00001850
Iteration 69/1000 | Loss: 0.00001850
Iteration 70/1000 | Loss: 0.00001850
Iteration 71/1000 | Loss: 0.00001849
Iteration 72/1000 | Loss: 0.00001849
Iteration 73/1000 | Loss: 0.00001849
Iteration 74/1000 | Loss: 0.00001848
Iteration 75/1000 | Loss: 0.00001848
Iteration 76/1000 | Loss: 0.00001848
Iteration 77/1000 | Loss: 0.00001847
Iteration 78/1000 | Loss: 0.00001847
Iteration 79/1000 | Loss: 0.00001847
Iteration 80/1000 | Loss: 0.00001847
Iteration 81/1000 | Loss: 0.00001846
Iteration 82/1000 | Loss: 0.00001846
Iteration 83/1000 | Loss: 0.00001846
Iteration 84/1000 | Loss: 0.00001846
Iteration 85/1000 | Loss: 0.00001846
Iteration 86/1000 | Loss: 0.00001845
Iteration 87/1000 | Loss: 0.00001845
Iteration 88/1000 | Loss: 0.00001845
Iteration 89/1000 | Loss: 0.00001845
Iteration 90/1000 | Loss: 0.00001844
Iteration 91/1000 | Loss: 0.00001844
Iteration 92/1000 | Loss: 0.00001844
Iteration 93/1000 | Loss: 0.00001844
Iteration 94/1000 | Loss: 0.00001844
Iteration 95/1000 | Loss: 0.00001844
Iteration 96/1000 | Loss: 0.00001844
Iteration 97/1000 | Loss: 0.00001844
Iteration 98/1000 | Loss: 0.00001843
Iteration 99/1000 | Loss: 0.00001843
Iteration 100/1000 | Loss: 0.00001842
Iteration 101/1000 | Loss: 0.00001842
Iteration 102/1000 | Loss: 0.00001841
Iteration 103/1000 | Loss: 0.00001841
Iteration 104/1000 | Loss: 0.00001841
Iteration 105/1000 | Loss: 0.00001840
Iteration 106/1000 | Loss: 0.00001840
Iteration 107/1000 | Loss: 0.00001840
Iteration 108/1000 | Loss: 0.00001839
Iteration 109/1000 | Loss: 0.00001839
Iteration 110/1000 | Loss: 0.00001839
Iteration 111/1000 | Loss: 0.00001838
Iteration 112/1000 | Loss: 0.00001838
Iteration 113/1000 | Loss: 0.00001838
Iteration 114/1000 | Loss: 0.00001837
Iteration 115/1000 | Loss: 0.00001837
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001836
Iteration 123/1000 | Loss: 0.00001836
Iteration 124/1000 | Loss: 0.00001836
Iteration 125/1000 | Loss: 0.00001836
Iteration 126/1000 | Loss: 0.00001836
Iteration 127/1000 | Loss: 0.00001836
Iteration 128/1000 | Loss: 0.00001836
Iteration 129/1000 | Loss: 0.00001836
Iteration 130/1000 | Loss: 0.00001836
Iteration 131/1000 | Loss: 0.00001835
Iteration 132/1000 | Loss: 0.00001835
Iteration 133/1000 | Loss: 0.00001835
Iteration 134/1000 | Loss: 0.00001835
Iteration 135/1000 | Loss: 0.00001835
Iteration 136/1000 | Loss: 0.00001835
Iteration 137/1000 | Loss: 0.00001835
Iteration 138/1000 | Loss: 0.00001835
Iteration 139/1000 | Loss: 0.00001834
Iteration 140/1000 | Loss: 0.00001834
Iteration 141/1000 | Loss: 0.00001834
Iteration 142/1000 | Loss: 0.00001834
Iteration 143/1000 | Loss: 0.00001834
Iteration 144/1000 | Loss: 0.00001834
Iteration 145/1000 | Loss: 0.00001833
Iteration 146/1000 | Loss: 0.00001833
Iteration 147/1000 | Loss: 0.00001833
Iteration 148/1000 | Loss: 0.00001833
Iteration 149/1000 | Loss: 0.00001833
Iteration 150/1000 | Loss: 0.00001833
Iteration 151/1000 | Loss: 0.00001833
Iteration 152/1000 | Loss: 0.00001833
Iteration 153/1000 | Loss: 0.00001832
Iteration 154/1000 | Loss: 0.00001832
Iteration 155/1000 | Loss: 0.00001832
Iteration 156/1000 | Loss: 0.00001832
Iteration 157/1000 | Loss: 0.00001832
Iteration 158/1000 | Loss: 0.00001832
Iteration 159/1000 | Loss: 0.00001832
Iteration 160/1000 | Loss: 0.00001832
Iteration 161/1000 | Loss: 0.00001831
Iteration 162/1000 | Loss: 0.00001831
Iteration 163/1000 | Loss: 0.00001830
Iteration 164/1000 | Loss: 0.00001830
Iteration 165/1000 | Loss: 0.00001830
Iteration 166/1000 | Loss: 0.00001829
Iteration 167/1000 | Loss: 0.00001829
Iteration 168/1000 | Loss: 0.00001829
Iteration 169/1000 | Loss: 0.00001828
Iteration 170/1000 | Loss: 0.00001828
Iteration 171/1000 | Loss: 0.00001828
Iteration 172/1000 | Loss: 0.00001828
Iteration 173/1000 | Loss: 0.00001828
Iteration 174/1000 | Loss: 0.00001828
Iteration 175/1000 | Loss: 0.00001828
Iteration 176/1000 | Loss: 0.00001828
Iteration 177/1000 | Loss: 0.00001828
Iteration 178/1000 | Loss: 0.00001828
Iteration 179/1000 | Loss: 0.00001827
Iteration 180/1000 | Loss: 0.00001827
Iteration 181/1000 | Loss: 0.00001826
Iteration 182/1000 | Loss: 0.00001826
Iteration 183/1000 | Loss: 0.00001826
Iteration 184/1000 | Loss: 0.00001826
Iteration 185/1000 | Loss: 0.00001826
Iteration 186/1000 | Loss: 0.00001826
Iteration 187/1000 | Loss: 0.00001826
Iteration 188/1000 | Loss: 0.00001825
Iteration 189/1000 | Loss: 0.00001825
Iteration 190/1000 | Loss: 0.00001825
Iteration 191/1000 | Loss: 0.00001825
Iteration 192/1000 | Loss: 0.00001825
Iteration 193/1000 | Loss: 0.00001824
Iteration 194/1000 | Loss: 0.00001824
Iteration 195/1000 | Loss: 0.00001824
Iteration 196/1000 | Loss: 0.00001824
Iteration 197/1000 | Loss: 0.00001824
Iteration 198/1000 | Loss: 0.00001824
Iteration 199/1000 | Loss: 0.00001824
Iteration 200/1000 | Loss: 0.00001823
Iteration 201/1000 | Loss: 0.00001823
Iteration 202/1000 | Loss: 0.00001823
Iteration 203/1000 | Loss: 0.00001822
Iteration 204/1000 | Loss: 0.00001822
Iteration 205/1000 | Loss: 0.00001822
Iteration 206/1000 | Loss: 0.00001821
Iteration 207/1000 | Loss: 0.00001821
Iteration 208/1000 | Loss: 0.00001821
Iteration 209/1000 | Loss: 0.00001821
Iteration 210/1000 | Loss: 0.00001821
Iteration 211/1000 | Loss: 0.00001820
Iteration 212/1000 | Loss: 0.00001820
Iteration 213/1000 | Loss: 0.00001820
Iteration 214/1000 | Loss: 0.00001820
Iteration 215/1000 | Loss: 0.00001820
Iteration 216/1000 | Loss: 0.00001820
Iteration 217/1000 | Loss: 0.00001820
Iteration 218/1000 | Loss: 0.00001819
Iteration 219/1000 | Loss: 0.00001819
Iteration 220/1000 | Loss: 0.00001819
Iteration 221/1000 | Loss: 0.00001819
Iteration 222/1000 | Loss: 0.00001819
Iteration 223/1000 | Loss: 0.00001819
Iteration 224/1000 | Loss: 0.00001819
Iteration 225/1000 | Loss: 0.00001819
Iteration 226/1000 | Loss: 0.00001819
Iteration 227/1000 | Loss: 0.00001819
Iteration 228/1000 | Loss: 0.00001819
Iteration 229/1000 | Loss: 0.00001819
Iteration 230/1000 | Loss: 0.00001819
Iteration 231/1000 | Loss: 0.00001819
Iteration 232/1000 | Loss: 0.00001818
Iteration 233/1000 | Loss: 0.00001818
Iteration 234/1000 | Loss: 0.00001818
Iteration 235/1000 | Loss: 0.00001818
Iteration 236/1000 | Loss: 0.00001818
Iteration 237/1000 | Loss: 0.00001818
Iteration 238/1000 | Loss: 0.00001818
Iteration 239/1000 | Loss: 0.00001818
Iteration 240/1000 | Loss: 0.00001818
Iteration 241/1000 | Loss: 0.00001817
Iteration 242/1000 | Loss: 0.00001817
Iteration 243/1000 | Loss: 0.00001817
Iteration 244/1000 | Loss: 0.00001817
Iteration 245/1000 | Loss: 0.00001817
Iteration 246/1000 | Loss: 0.00001817
Iteration 247/1000 | Loss: 0.00001817
Iteration 248/1000 | Loss: 0.00001817
Iteration 249/1000 | Loss: 0.00001816
Iteration 250/1000 | Loss: 0.00001816
Iteration 251/1000 | Loss: 0.00001816
Iteration 252/1000 | Loss: 0.00001816
Iteration 253/1000 | Loss: 0.00001815
Iteration 254/1000 | Loss: 0.00001815
Iteration 255/1000 | Loss: 0.00001815
Iteration 256/1000 | Loss: 0.00001815
Iteration 257/1000 | Loss: 0.00001815
Iteration 258/1000 | Loss: 0.00001815
Iteration 259/1000 | Loss: 0.00001815
Iteration 260/1000 | Loss: 0.00001814
Iteration 261/1000 | Loss: 0.00001814
Iteration 262/1000 | Loss: 0.00001814
Iteration 263/1000 | Loss: 0.00001814
Iteration 264/1000 | Loss: 0.00001814
Iteration 265/1000 | Loss: 0.00001814
Iteration 266/1000 | Loss: 0.00001813
Iteration 267/1000 | Loss: 0.00001813
Iteration 268/1000 | Loss: 0.00001813
Iteration 269/1000 | Loss: 0.00001813
Iteration 270/1000 | Loss: 0.00001813
Iteration 271/1000 | Loss: 0.00001813
Iteration 272/1000 | Loss: 0.00001813
Iteration 273/1000 | Loss: 0.00001813
Iteration 274/1000 | Loss: 0.00001812
Iteration 275/1000 | Loss: 0.00001812
Iteration 276/1000 | Loss: 0.00001812
Iteration 277/1000 | Loss: 0.00001812
Iteration 278/1000 | Loss: 0.00001812
Iteration 279/1000 | Loss: 0.00001812
Iteration 280/1000 | Loss: 0.00001812
Iteration 281/1000 | Loss: 0.00001811
Iteration 282/1000 | Loss: 0.00001811
Iteration 283/1000 | Loss: 0.00001811
Iteration 284/1000 | Loss: 0.00001811
Iteration 285/1000 | Loss: 0.00001811
Iteration 286/1000 | Loss: 0.00001811
Iteration 287/1000 | Loss: 0.00001811
Iteration 288/1000 | Loss: 0.00001811
Iteration 289/1000 | Loss: 0.00001811
Iteration 290/1000 | Loss: 0.00001811
Iteration 291/1000 | Loss: 0.00001811
Iteration 292/1000 | Loss: 0.00001811
Iteration 293/1000 | Loss: 0.00001811
Iteration 294/1000 | Loss: 0.00001811
Iteration 295/1000 | Loss: 0.00001811
Iteration 296/1000 | Loss: 0.00001811
Iteration 297/1000 | Loss: 0.00001811
Iteration 298/1000 | Loss: 0.00001810
Iteration 299/1000 | Loss: 0.00001810
Iteration 300/1000 | Loss: 0.00001810
Iteration 301/1000 | Loss: 0.00001810
Iteration 302/1000 | Loss: 0.00001810
Iteration 303/1000 | Loss: 0.00001810
Iteration 304/1000 | Loss: 0.00001810
Iteration 305/1000 | Loss: 0.00001810
Iteration 306/1000 | Loss: 0.00001810
Iteration 307/1000 | Loss: 0.00001810
Iteration 308/1000 | Loss: 0.00001810
Iteration 309/1000 | Loss: 0.00001810
Iteration 310/1000 | Loss: 0.00001810
Iteration 311/1000 | Loss: 0.00001810
Iteration 312/1000 | Loss: 0.00001810
Iteration 313/1000 | Loss: 0.00001810
Iteration 314/1000 | Loss: 0.00001809
Iteration 315/1000 | Loss: 0.00001809
Iteration 316/1000 | Loss: 0.00001809
Iteration 317/1000 | Loss: 0.00001809
Iteration 318/1000 | Loss: 0.00001809
Iteration 319/1000 | Loss: 0.00001809
Iteration 320/1000 | Loss: 0.00001809
Iteration 321/1000 | Loss: 0.00001809
Iteration 322/1000 | Loss: 0.00001809
Iteration 323/1000 | Loss: 0.00001809
Iteration 324/1000 | Loss: 0.00001809
Iteration 325/1000 | Loss: 0.00001809
Iteration 326/1000 | Loss: 0.00001809
Iteration 327/1000 | Loss: 0.00001809
Iteration 328/1000 | Loss: 0.00001809
Iteration 329/1000 | Loss: 0.00001809
Iteration 330/1000 | Loss: 0.00001809
Iteration 331/1000 | Loss: 0.00001809
Iteration 332/1000 | Loss: 0.00001809
Iteration 333/1000 | Loss: 0.00001809
Iteration 334/1000 | Loss: 0.00001809
Iteration 335/1000 | Loss: 0.00001809
Iteration 336/1000 | Loss: 0.00001809
Iteration 337/1000 | Loss: 0.00001809
Iteration 338/1000 | Loss: 0.00001809
Iteration 339/1000 | Loss: 0.00001809
Iteration 340/1000 | Loss: 0.00001809
Iteration 341/1000 | Loss: 0.00001809
Iteration 342/1000 | Loss: 0.00001809
Iteration 343/1000 | Loss: 0.00001809
Iteration 344/1000 | Loss: 0.00001809
Iteration 345/1000 | Loss: 0.00001809
Iteration 346/1000 | Loss: 0.00001809
Iteration 347/1000 | Loss: 0.00001809
Iteration 348/1000 | Loss: 0.00001809
Iteration 349/1000 | Loss: 0.00001809
Iteration 350/1000 | Loss: 0.00001809
Iteration 351/1000 | Loss: 0.00001809
Iteration 352/1000 | Loss: 0.00001809
Iteration 353/1000 | Loss: 0.00001809
Iteration 354/1000 | Loss: 0.00001809
Iteration 355/1000 | Loss: 0.00001809
Iteration 356/1000 | Loss: 0.00001809
Iteration 357/1000 | Loss: 0.00001809
Iteration 358/1000 | Loss: 0.00001809
Iteration 359/1000 | Loss: 0.00001809
Iteration 360/1000 | Loss: 0.00001809
Iteration 361/1000 | Loss: 0.00001809
Iteration 362/1000 | Loss: 0.00001809
Iteration 363/1000 | Loss: 0.00001809
Iteration 364/1000 | Loss: 0.00001809
Iteration 365/1000 | Loss: 0.00001809
Iteration 366/1000 | Loss: 0.00001809
Iteration 367/1000 | Loss: 0.00001809
Iteration 368/1000 | Loss: 0.00001809
Iteration 369/1000 | Loss: 0.00001809
Iteration 370/1000 | Loss: 0.00001809
Iteration 371/1000 | Loss: 0.00001809
Iteration 372/1000 | Loss: 0.00001809
Iteration 373/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 373. Stopping optimization.
Last 5 losses: [1.8089014702127315e-05, 1.8089014702127315e-05, 1.8089014702127315e-05, 1.8089014702127315e-05, 1.8089014702127315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8089014702127315e-05

Optimization complete. Final v2v error: 3.3293890953063965 mm

Highest mean error: 12.005973815917969 mm for frame 88

Lowest mean error: 2.7809557914733887 mm for frame 33

Saving results

Total time: 72.28667521476746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778046
Iteration 2/25 | Loss: 0.00156456
Iteration 3/25 | Loss: 0.00151436
Iteration 4/25 | Loss: 0.00136769
Iteration 5/25 | Loss: 0.00135357
Iteration 6/25 | Loss: 0.00131718
Iteration 7/25 | Loss: 0.00133389
Iteration 8/25 | Loss: 0.00129913
Iteration 9/25 | Loss: 0.00129667
Iteration 10/25 | Loss: 0.00129630
Iteration 11/25 | Loss: 0.00129623
Iteration 12/25 | Loss: 0.00129623
Iteration 13/25 | Loss: 0.00129623
Iteration 14/25 | Loss: 0.00129623
Iteration 15/25 | Loss: 0.00129623
Iteration 16/25 | Loss: 0.00129623
Iteration 17/25 | Loss: 0.00129622
Iteration 18/25 | Loss: 0.00129622
Iteration 19/25 | Loss: 0.00129622
Iteration 20/25 | Loss: 0.00129622
Iteration 21/25 | Loss: 0.00129622
Iteration 22/25 | Loss: 0.00129622
Iteration 23/25 | Loss: 0.00129622
Iteration 24/25 | Loss: 0.00129622
Iteration 25/25 | Loss: 0.00129622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47492766
Iteration 2/25 | Loss: 0.00114053
Iteration 3/25 | Loss: 0.00114051
Iteration 4/25 | Loss: 0.00114051
Iteration 5/25 | Loss: 0.00114051
Iteration 6/25 | Loss: 0.00114051
Iteration 7/25 | Loss: 0.00114051
Iteration 8/25 | Loss: 0.00114051
Iteration 9/25 | Loss: 0.00114051
Iteration 10/25 | Loss: 0.00114051
Iteration 11/25 | Loss: 0.00114051
Iteration 12/25 | Loss: 0.00114051
Iteration 13/25 | Loss: 0.00114051
Iteration 14/25 | Loss: 0.00114051
Iteration 15/25 | Loss: 0.00114051
Iteration 16/25 | Loss: 0.00114051
Iteration 17/25 | Loss: 0.00114051
Iteration 18/25 | Loss: 0.00114051
Iteration 19/25 | Loss: 0.00114051
Iteration 20/25 | Loss: 0.00114051
Iteration 21/25 | Loss: 0.00114051
Iteration 22/25 | Loss: 0.00114051
Iteration 23/25 | Loss: 0.00114051
Iteration 24/25 | Loss: 0.00114051
Iteration 25/25 | Loss: 0.00114051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114051
Iteration 2/1000 | Loss: 0.00003906
Iteration 3/1000 | Loss: 0.00161961
Iteration 4/1000 | Loss: 0.00069815
Iteration 5/1000 | Loss: 0.00003178
Iteration 6/1000 | Loss: 0.00002625
Iteration 7/1000 | Loss: 0.00149426
Iteration 8/1000 | Loss: 0.00034767
Iteration 9/1000 | Loss: 0.00014898
Iteration 10/1000 | Loss: 0.00002630
Iteration 11/1000 | Loss: 0.00002312
Iteration 12/1000 | Loss: 0.00002152
Iteration 13/1000 | Loss: 0.00002070
Iteration 14/1000 | Loss: 0.00002013
Iteration 15/1000 | Loss: 0.00001978
Iteration 16/1000 | Loss: 0.00001953
Iteration 17/1000 | Loss: 0.00001928
Iteration 18/1000 | Loss: 0.00001913
Iteration 19/1000 | Loss: 0.00001893
Iteration 20/1000 | Loss: 0.00001887
Iteration 21/1000 | Loss: 0.00001884
Iteration 22/1000 | Loss: 0.00001871
Iteration 23/1000 | Loss: 0.00001867
Iteration 24/1000 | Loss: 0.00001867
Iteration 25/1000 | Loss: 0.00001866
Iteration 26/1000 | Loss: 0.00001866
Iteration 27/1000 | Loss: 0.00001865
Iteration 28/1000 | Loss: 0.00001865
Iteration 29/1000 | Loss: 0.00001863
Iteration 30/1000 | Loss: 0.00001863
Iteration 31/1000 | Loss: 0.00001862
Iteration 32/1000 | Loss: 0.00001862
Iteration 33/1000 | Loss: 0.00001861
Iteration 34/1000 | Loss: 0.00001861
Iteration 35/1000 | Loss: 0.00001857
Iteration 36/1000 | Loss: 0.00001856
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001853
Iteration 39/1000 | Loss: 0.00001853
Iteration 40/1000 | Loss: 0.00001852
Iteration 41/1000 | Loss: 0.00001852
Iteration 42/1000 | Loss: 0.00001852
Iteration 43/1000 | Loss: 0.00001851
Iteration 44/1000 | Loss: 0.00001851
Iteration 45/1000 | Loss: 0.00001850
Iteration 46/1000 | Loss: 0.00001850
Iteration 47/1000 | Loss: 0.00001849
Iteration 48/1000 | Loss: 0.00001849
Iteration 49/1000 | Loss: 0.00001849
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00001848
Iteration 52/1000 | Loss: 0.00001848
Iteration 53/1000 | Loss: 0.00001848
Iteration 54/1000 | Loss: 0.00001847
Iteration 55/1000 | Loss: 0.00001847
Iteration 56/1000 | Loss: 0.00001847
Iteration 57/1000 | Loss: 0.00001847
Iteration 58/1000 | Loss: 0.00001847
Iteration 59/1000 | Loss: 0.00001847
Iteration 60/1000 | Loss: 0.00001847
Iteration 61/1000 | Loss: 0.00001847
Iteration 62/1000 | Loss: 0.00001846
Iteration 63/1000 | Loss: 0.00001846
Iteration 64/1000 | Loss: 0.00001846
Iteration 65/1000 | Loss: 0.00001846
Iteration 66/1000 | Loss: 0.00001845
Iteration 67/1000 | Loss: 0.00001845
Iteration 68/1000 | Loss: 0.00001845
Iteration 69/1000 | Loss: 0.00001844
Iteration 70/1000 | Loss: 0.00001844
Iteration 71/1000 | Loss: 0.00001844
Iteration 72/1000 | Loss: 0.00001844
Iteration 73/1000 | Loss: 0.00001844
Iteration 74/1000 | Loss: 0.00001843
Iteration 75/1000 | Loss: 0.00001843
Iteration 76/1000 | Loss: 0.00001843
Iteration 77/1000 | Loss: 0.00001843
Iteration 78/1000 | Loss: 0.00001843
Iteration 79/1000 | Loss: 0.00001843
Iteration 80/1000 | Loss: 0.00001842
Iteration 81/1000 | Loss: 0.00001842
Iteration 82/1000 | Loss: 0.00001842
Iteration 83/1000 | Loss: 0.00001842
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001840
Iteration 89/1000 | Loss: 0.00001840
Iteration 90/1000 | Loss: 0.00001839
Iteration 91/1000 | Loss: 0.00001839
Iteration 92/1000 | Loss: 0.00001839
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001838
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001838
Iteration 98/1000 | Loss: 0.00001837
Iteration 99/1000 | Loss: 0.00001837
Iteration 100/1000 | Loss: 0.00001837
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001836
Iteration 105/1000 | Loss: 0.00001836
Iteration 106/1000 | Loss: 0.00001836
Iteration 107/1000 | Loss: 0.00001836
Iteration 108/1000 | Loss: 0.00001835
Iteration 109/1000 | Loss: 0.00001835
Iteration 110/1000 | Loss: 0.00001835
Iteration 111/1000 | Loss: 0.00001835
Iteration 112/1000 | Loss: 0.00001835
Iteration 113/1000 | Loss: 0.00001835
Iteration 114/1000 | Loss: 0.00001834
Iteration 115/1000 | Loss: 0.00001834
Iteration 116/1000 | Loss: 0.00001834
Iteration 117/1000 | Loss: 0.00001833
Iteration 118/1000 | Loss: 0.00001833
Iteration 119/1000 | Loss: 0.00001833
Iteration 120/1000 | Loss: 0.00001833
Iteration 121/1000 | Loss: 0.00001833
Iteration 122/1000 | Loss: 0.00001833
Iteration 123/1000 | Loss: 0.00001833
Iteration 124/1000 | Loss: 0.00001832
Iteration 125/1000 | Loss: 0.00001832
Iteration 126/1000 | Loss: 0.00001832
Iteration 127/1000 | Loss: 0.00001832
Iteration 128/1000 | Loss: 0.00001832
Iteration 129/1000 | Loss: 0.00001832
Iteration 130/1000 | Loss: 0.00001832
Iteration 131/1000 | Loss: 0.00001831
Iteration 132/1000 | Loss: 0.00001831
Iteration 133/1000 | Loss: 0.00001831
Iteration 134/1000 | Loss: 0.00001831
Iteration 135/1000 | Loss: 0.00001831
Iteration 136/1000 | Loss: 0.00001831
Iteration 137/1000 | Loss: 0.00001831
Iteration 138/1000 | Loss: 0.00001831
Iteration 139/1000 | Loss: 0.00001831
Iteration 140/1000 | Loss: 0.00001831
Iteration 141/1000 | Loss: 0.00001831
Iteration 142/1000 | Loss: 0.00001831
Iteration 143/1000 | Loss: 0.00001830
Iteration 144/1000 | Loss: 0.00001830
Iteration 145/1000 | Loss: 0.00001830
Iteration 146/1000 | Loss: 0.00001830
Iteration 147/1000 | Loss: 0.00001830
Iteration 148/1000 | Loss: 0.00001830
Iteration 149/1000 | Loss: 0.00001830
Iteration 150/1000 | Loss: 0.00001830
Iteration 151/1000 | Loss: 0.00001830
Iteration 152/1000 | Loss: 0.00001830
Iteration 153/1000 | Loss: 0.00001830
Iteration 154/1000 | Loss: 0.00001830
Iteration 155/1000 | Loss: 0.00001830
Iteration 156/1000 | Loss: 0.00001830
Iteration 157/1000 | Loss: 0.00001830
Iteration 158/1000 | Loss: 0.00001830
Iteration 159/1000 | Loss: 0.00001829
Iteration 160/1000 | Loss: 0.00001829
Iteration 161/1000 | Loss: 0.00001829
Iteration 162/1000 | Loss: 0.00001829
Iteration 163/1000 | Loss: 0.00001829
Iteration 164/1000 | Loss: 0.00001829
Iteration 165/1000 | Loss: 0.00001829
Iteration 166/1000 | Loss: 0.00001829
Iteration 167/1000 | Loss: 0.00001829
Iteration 168/1000 | Loss: 0.00001829
Iteration 169/1000 | Loss: 0.00001829
Iteration 170/1000 | Loss: 0.00001829
Iteration 171/1000 | Loss: 0.00001829
Iteration 172/1000 | Loss: 0.00001829
Iteration 173/1000 | Loss: 0.00001828
Iteration 174/1000 | Loss: 0.00001828
Iteration 175/1000 | Loss: 0.00001828
Iteration 176/1000 | Loss: 0.00001828
Iteration 177/1000 | Loss: 0.00001828
Iteration 178/1000 | Loss: 0.00001828
Iteration 179/1000 | Loss: 0.00001828
Iteration 180/1000 | Loss: 0.00001828
Iteration 181/1000 | Loss: 0.00001828
Iteration 182/1000 | Loss: 0.00001827
Iteration 183/1000 | Loss: 0.00001827
Iteration 184/1000 | Loss: 0.00001827
Iteration 185/1000 | Loss: 0.00001827
Iteration 186/1000 | Loss: 0.00001827
Iteration 187/1000 | Loss: 0.00001827
Iteration 188/1000 | Loss: 0.00001827
Iteration 189/1000 | Loss: 0.00001827
Iteration 190/1000 | Loss: 0.00001826
Iteration 191/1000 | Loss: 0.00001826
Iteration 192/1000 | Loss: 0.00001826
Iteration 193/1000 | Loss: 0.00001826
Iteration 194/1000 | Loss: 0.00001826
Iteration 195/1000 | Loss: 0.00001826
Iteration 196/1000 | Loss: 0.00001825
Iteration 197/1000 | Loss: 0.00001825
Iteration 198/1000 | Loss: 0.00001825
Iteration 199/1000 | Loss: 0.00001825
Iteration 200/1000 | Loss: 0.00001825
Iteration 201/1000 | Loss: 0.00001825
Iteration 202/1000 | Loss: 0.00001825
Iteration 203/1000 | Loss: 0.00001825
Iteration 204/1000 | Loss: 0.00001825
Iteration 205/1000 | Loss: 0.00001824
Iteration 206/1000 | Loss: 0.00001824
Iteration 207/1000 | Loss: 0.00001824
Iteration 208/1000 | Loss: 0.00001824
Iteration 209/1000 | Loss: 0.00001824
Iteration 210/1000 | Loss: 0.00001824
Iteration 211/1000 | Loss: 0.00001824
Iteration 212/1000 | Loss: 0.00001824
Iteration 213/1000 | Loss: 0.00001824
Iteration 214/1000 | Loss: 0.00001824
Iteration 215/1000 | Loss: 0.00001824
Iteration 216/1000 | Loss: 0.00001824
Iteration 217/1000 | Loss: 0.00001823
Iteration 218/1000 | Loss: 0.00001823
Iteration 219/1000 | Loss: 0.00001823
Iteration 220/1000 | Loss: 0.00001823
Iteration 221/1000 | Loss: 0.00001823
Iteration 222/1000 | Loss: 0.00001823
Iteration 223/1000 | Loss: 0.00001823
Iteration 224/1000 | Loss: 0.00001823
Iteration 225/1000 | Loss: 0.00001823
Iteration 226/1000 | Loss: 0.00001823
Iteration 227/1000 | Loss: 0.00001823
Iteration 228/1000 | Loss: 0.00001823
Iteration 229/1000 | Loss: 0.00001823
Iteration 230/1000 | Loss: 0.00001822
Iteration 231/1000 | Loss: 0.00001822
Iteration 232/1000 | Loss: 0.00001822
Iteration 233/1000 | Loss: 0.00001822
Iteration 234/1000 | Loss: 0.00001822
Iteration 235/1000 | Loss: 0.00001822
Iteration 236/1000 | Loss: 0.00001822
Iteration 237/1000 | Loss: 0.00001822
Iteration 238/1000 | Loss: 0.00001822
Iteration 239/1000 | Loss: 0.00001822
Iteration 240/1000 | Loss: 0.00001822
Iteration 241/1000 | Loss: 0.00001822
Iteration 242/1000 | Loss: 0.00001822
Iteration 243/1000 | Loss: 0.00001822
Iteration 244/1000 | Loss: 0.00001821
Iteration 245/1000 | Loss: 0.00001821
Iteration 246/1000 | Loss: 0.00001821
Iteration 247/1000 | Loss: 0.00001821
Iteration 248/1000 | Loss: 0.00001821
Iteration 249/1000 | Loss: 0.00001821
Iteration 250/1000 | Loss: 0.00001821
Iteration 251/1000 | Loss: 0.00001821
Iteration 252/1000 | Loss: 0.00001821
Iteration 253/1000 | Loss: 0.00001821
Iteration 254/1000 | Loss: 0.00001821
Iteration 255/1000 | Loss: 0.00001821
Iteration 256/1000 | Loss: 0.00001821
Iteration 257/1000 | Loss: 0.00001821
Iteration 258/1000 | Loss: 0.00001821
Iteration 259/1000 | Loss: 0.00001821
Iteration 260/1000 | Loss: 0.00001821
Iteration 261/1000 | Loss: 0.00001820
Iteration 262/1000 | Loss: 0.00001820
Iteration 263/1000 | Loss: 0.00001820
Iteration 264/1000 | Loss: 0.00001820
Iteration 265/1000 | Loss: 0.00001820
Iteration 266/1000 | Loss: 0.00001820
Iteration 267/1000 | Loss: 0.00001820
Iteration 268/1000 | Loss: 0.00001820
Iteration 269/1000 | Loss: 0.00001820
Iteration 270/1000 | Loss: 0.00001820
Iteration 271/1000 | Loss: 0.00001820
Iteration 272/1000 | Loss: 0.00001820
Iteration 273/1000 | Loss: 0.00001820
Iteration 274/1000 | Loss: 0.00001819
Iteration 275/1000 | Loss: 0.00001819
Iteration 276/1000 | Loss: 0.00001819
Iteration 277/1000 | Loss: 0.00001819
Iteration 278/1000 | Loss: 0.00001819
Iteration 279/1000 | Loss: 0.00001819
Iteration 280/1000 | Loss: 0.00001819
Iteration 281/1000 | Loss: 0.00001819
Iteration 282/1000 | Loss: 0.00001819
Iteration 283/1000 | Loss: 0.00001819
Iteration 284/1000 | Loss: 0.00001819
Iteration 285/1000 | Loss: 0.00001819
Iteration 286/1000 | Loss: 0.00001819
Iteration 287/1000 | Loss: 0.00001819
Iteration 288/1000 | Loss: 0.00001819
Iteration 289/1000 | Loss: 0.00001819
Iteration 290/1000 | Loss: 0.00001819
Iteration 291/1000 | Loss: 0.00001819
Iteration 292/1000 | Loss: 0.00001819
Iteration 293/1000 | Loss: 0.00001819
Iteration 294/1000 | Loss: 0.00001818
Iteration 295/1000 | Loss: 0.00001818
Iteration 296/1000 | Loss: 0.00001818
Iteration 297/1000 | Loss: 0.00001818
Iteration 298/1000 | Loss: 0.00001818
Iteration 299/1000 | Loss: 0.00001818
Iteration 300/1000 | Loss: 0.00001818
Iteration 301/1000 | Loss: 0.00001818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [1.818493183236569e-05, 1.818493183236569e-05, 1.818493183236569e-05, 1.818493183236569e-05, 1.818493183236569e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.818493183236569e-05

Optimization complete. Final v2v error: 3.5120015144348145 mm

Highest mean error: 4.371037006378174 mm for frame 51

Lowest mean error: 2.700446844100952 mm for frame 168

Saving results

Total time: 77.14283514022827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494931
Iteration 2/25 | Loss: 0.00141963
Iteration 3/25 | Loss: 0.00132267
Iteration 4/25 | Loss: 0.00129639
Iteration 5/25 | Loss: 0.00128664
Iteration 6/25 | Loss: 0.00128437
Iteration 7/25 | Loss: 0.00128437
Iteration 8/25 | Loss: 0.00128437
Iteration 9/25 | Loss: 0.00128437
Iteration 10/25 | Loss: 0.00128437
Iteration 11/25 | Loss: 0.00128437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012843678705394268, 0.0012843678705394268, 0.0012843678705394268, 0.0012843678705394268, 0.0012843678705394268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012843678705394268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51784956
Iteration 2/25 | Loss: 0.00139794
Iteration 3/25 | Loss: 0.00139793
Iteration 4/25 | Loss: 0.00139793
Iteration 5/25 | Loss: 0.00139793
Iteration 6/25 | Loss: 0.00139793
Iteration 7/25 | Loss: 0.00139793
Iteration 8/25 | Loss: 0.00139793
Iteration 9/25 | Loss: 0.00139793
Iteration 10/25 | Loss: 0.00139793
Iteration 11/25 | Loss: 0.00139793
Iteration 12/25 | Loss: 0.00139793
Iteration 13/25 | Loss: 0.00139793
Iteration 14/25 | Loss: 0.00139793
Iteration 15/25 | Loss: 0.00139793
Iteration 16/25 | Loss: 0.00139793
Iteration 17/25 | Loss: 0.00139793
Iteration 18/25 | Loss: 0.00139793
Iteration 19/25 | Loss: 0.00139793
Iteration 20/25 | Loss: 0.00139793
Iteration 21/25 | Loss: 0.00139793
Iteration 22/25 | Loss: 0.00139793
Iteration 23/25 | Loss: 0.00139793
Iteration 24/25 | Loss: 0.00139793
Iteration 25/25 | Loss: 0.00139793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139793
Iteration 2/1000 | Loss: 0.00005178
Iteration 3/1000 | Loss: 0.00003894
Iteration 4/1000 | Loss: 0.00003524
Iteration 5/1000 | Loss: 0.00003324
Iteration 6/1000 | Loss: 0.00003147
Iteration 7/1000 | Loss: 0.00003063
Iteration 8/1000 | Loss: 0.00002993
Iteration 9/1000 | Loss: 0.00002936
Iteration 10/1000 | Loss: 0.00002897
Iteration 11/1000 | Loss: 0.00002861
Iteration 12/1000 | Loss: 0.00002828
Iteration 13/1000 | Loss: 0.00002804
Iteration 14/1000 | Loss: 0.00002783
Iteration 15/1000 | Loss: 0.00002782
Iteration 16/1000 | Loss: 0.00002774
Iteration 17/1000 | Loss: 0.00002770
Iteration 18/1000 | Loss: 0.00002770
Iteration 19/1000 | Loss: 0.00002766
Iteration 20/1000 | Loss: 0.00002762
Iteration 21/1000 | Loss: 0.00002762
Iteration 22/1000 | Loss: 0.00002757
Iteration 23/1000 | Loss: 0.00002753
Iteration 24/1000 | Loss: 0.00002752
Iteration 25/1000 | Loss: 0.00002752
Iteration 26/1000 | Loss: 0.00002750
Iteration 27/1000 | Loss: 0.00002750
Iteration 28/1000 | Loss: 0.00002746
Iteration 29/1000 | Loss: 0.00002742
Iteration 30/1000 | Loss: 0.00002740
Iteration 31/1000 | Loss: 0.00002739
Iteration 32/1000 | Loss: 0.00002739
Iteration 33/1000 | Loss: 0.00002738
Iteration 34/1000 | Loss: 0.00002738
Iteration 35/1000 | Loss: 0.00002738
Iteration 36/1000 | Loss: 0.00002737
Iteration 37/1000 | Loss: 0.00002737
Iteration 38/1000 | Loss: 0.00002736
Iteration 39/1000 | Loss: 0.00002736
Iteration 40/1000 | Loss: 0.00002735
Iteration 41/1000 | Loss: 0.00002735
Iteration 42/1000 | Loss: 0.00002734
Iteration 43/1000 | Loss: 0.00002734
Iteration 44/1000 | Loss: 0.00002734
Iteration 45/1000 | Loss: 0.00002734
Iteration 46/1000 | Loss: 0.00002733
Iteration 47/1000 | Loss: 0.00002733
Iteration 48/1000 | Loss: 0.00002732
Iteration 49/1000 | Loss: 0.00002732
Iteration 50/1000 | Loss: 0.00002731
Iteration 51/1000 | Loss: 0.00002731
Iteration 52/1000 | Loss: 0.00002731
Iteration 53/1000 | Loss: 0.00002731
Iteration 54/1000 | Loss: 0.00002731
Iteration 55/1000 | Loss: 0.00002731
Iteration 56/1000 | Loss: 0.00002731
Iteration 57/1000 | Loss: 0.00002731
Iteration 58/1000 | Loss: 0.00002731
Iteration 59/1000 | Loss: 0.00002731
Iteration 60/1000 | Loss: 0.00002731
Iteration 61/1000 | Loss: 0.00002731
Iteration 62/1000 | Loss: 0.00002731
Iteration 63/1000 | Loss: 0.00002731
Iteration 64/1000 | Loss: 0.00002731
Iteration 65/1000 | Loss: 0.00002731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [2.7307820346322842e-05, 2.7307820346322842e-05, 2.7307820346322842e-05, 2.7307820346322842e-05, 2.7307820346322842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7307820346322842e-05

Optimization complete. Final v2v error: 4.327848434448242 mm

Highest mean error: 5.435647964477539 mm for frame 40

Lowest mean error: 3.4938721656799316 mm for frame 174

Saving results

Total time: 40.6971333026886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00605344
Iteration 2/25 | Loss: 0.00175645
Iteration 3/25 | Loss: 0.00140953
Iteration 4/25 | Loss: 0.00139210
Iteration 5/25 | Loss: 0.00138836
Iteration 6/25 | Loss: 0.00138713
Iteration 7/25 | Loss: 0.00138713
Iteration 8/25 | Loss: 0.00138713
Iteration 9/25 | Loss: 0.00138713
Iteration 10/25 | Loss: 0.00138713
Iteration 11/25 | Loss: 0.00138713
Iteration 12/25 | Loss: 0.00138713
Iteration 13/25 | Loss: 0.00138713
Iteration 14/25 | Loss: 0.00138713
Iteration 15/25 | Loss: 0.00138713
Iteration 16/25 | Loss: 0.00138713
Iteration 17/25 | Loss: 0.00138713
Iteration 18/25 | Loss: 0.00138713
Iteration 19/25 | Loss: 0.00138713
Iteration 20/25 | Loss: 0.00138713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013871286064386368, 0.0013871286064386368, 0.0013871286064386368, 0.0013871286064386368, 0.0013871286064386368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013871286064386368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02246547
Iteration 2/25 | Loss: 0.00113270
Iteration 3/25 | Loss: 0.00113269
Iteration 4/25 | Loss: 0.00113268
Iteration 5/25 | Loss: 0.00113268
Iteration 6/25 | Loss: 0.00113268
Iteration 7/25 | Loss: 0.00113268
Iteration 8/25 | Loss: 0.00113268
Iteration 9/25 | Loss: 0.00113268
Iteration 10/25 | Loss: 0.00113268
Iteration 11/25 | Loss: 0.00113268
Iteration 12/25 | Loss: 0.00113268
Iteration 13/25 | Loss: 0.00113268
Iteration 14/25 | Loss: 0.00113268
Iteration 15/25 | Loss: 0.00113268
Iteration 16/25 | Loss: 0.00113268
Iteration 17/25 | Loss: 0.00113268
Iteration 18/25 | Loss: 0.00113268
Iteration 19/25 | Loss: 0.00113268
Iteration 20/25 | Loss: 0.00113268
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011326821986585855, 0.0011326821986585855, 0.0011326821986585855, 0.0011326821986585855, 0.0011326821986585855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011326821986585855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113268
Iteration 2/1000 | Loss: 0.00007283
Iteration 3/1000 | Loss: 0.00005045
Iteration 4/1000 | Loss: 0.00004304
Iteration 5/1000 | Loss: 0.00004004
Iteration 6/1000 | Loss: 0.00003881
Iteration 7/1000 | Loss: 0.00003823
Iteration 8/1000 | Loss: 0.00003738
Iteration 9/1000 | Loss: 0.00003668
Iteration 10/1000 | Loss: 0.00003625
Iteration 11/1000 | Loss: 0.00003586
Iteration 12/1000 | Loss: 0.00003538
Iteration 13/1000 | Loss: 0.00003501
Iteration 14/1000 | Loss: 0.00003465
Iteration 15/1000 | Loss: 0.00003424
Iteration 16/1000 | Loss: 0.00003396
Iteration 17/1000 | Loss: 0.00003379
Iteration 18/1000 | Loss: 0.00003360
Iteration 19/1000 | Loss: 0.00003341
Iteration 20/1000 | Loss: 0.00003327
Iteration 21/1000 | Loss: 0.00003317
Iteration 22/1000 | Loss: 0.00003316
Iteration 23/1000 | Loss: 0.00003306
Iteration 24/1000 | Loss: 0.00003306
Iteration 25/1000 | Loss: 0.00003306
Iteration 26/1000 | Loss: 0.00003306
Iteration 27/1000 | Loss: 0.00003306
Iteration 28/1000 | Loss: 0.00003302
Iteration 29/1000 | Loss: 0.00003302
Iteration 30/1000 | Loss: 0.00003302
Iteration 31/1000 | Loss: 0.00003302
Iteration 32/1000 | Loss: 0.00003301
Iteration 33/1000 | Loss: 0.00003301
Iteration 34/1000 | Loss: 0.00003301
Iteration 35/1000 | Loss: 0.00003299
Iteration 36/1000 | Loss: 0.00003299
Iteration 37/1000 | Loss: 0.00003298
Iteration 38/1000 | Loss: 0.00003298
Iteration 39/1000 | Loss: 0.00003297
Iteration 40/1000 | Loss: 0.00003297
Iteration 41/1000 | Loss: 0.00003297
Iteration 42/1000 | Loss: 0.00003296
Iteration 43/1000 | Loss: 0.00003296
Iteration 44/1000 | Loss: 0.00003296
Iteration 45/1000 | Loss: 0.00003296
Iteration 46/1000 | Loss: 0.00003295
Iteration 47/1000 | Loss: 0.00003295
Iteration 48/1000 | Loss: 0.00003295
Iteration 49/1000 | Loss: 0.00003295
Iteration 50/1000 | Loss: 0.00003294
Iteration 51/1000 | Loss: 0.00003294
Iteration 52/1000 | Loss: 0.00003294
Iteration 53/1000 | Loss: 0.00003294
Iteration 54/1000 | Loss: 0.00003294
Iteration 55/1000 | Loss: 0.00003293
Iteration 56/1000 | Loss: 0.00003293
Iteration 57/1000 | Loss: 0.00003293
Iteration 58/1000 | Loss: 0.00003293
Iteration 59/1000 | Loss: 0.00003293
Iteration 60/1000 | Loss: 0.00003292
Iteration 61/1000 | Loss: 0.00003292
Iteration 62/1000 | Loss: 0.00003292
Iteration 63/1000 | Loss: 0.00003292
Iteration 64/1000 | Loss: 0.00003291
Iteration 65/1000 | Loss: 0.00003291
Iteration 66/1000 | Loss: 0.00003291
Iteration 67/1000 | Loss: 0.00003290
Iteration 68/1000 | Loss: 0.00003289
Iteration 69/1000 | Loss: 0.00003289
Iteration 70/1000 | Loss: 0.00003289
Iteration 71/1000 | Loss: 0.00003289
Iteration 72/1000 | Loss: 0.00003289
Iteration 73/1000 | Loss: 0.00003289
Iteration 74/1000 | Loss: 0.00003288
Iteration 75/1000 | Loss: 0.00003288
Iteration 76/1000 | Loss: 0.00003288
Iteration 77/1000 | Loss: 0.00003287
Iteration 78/1000 | Loss: 0.00003287
Iteration 79/1000 | Loss: 0.00003287
Iteration 80/1000 | Loss: 0.00003286
Iteration 81/1000 | Loss: 0.00003286
Iteration 82/1000 | Loss: 0.00003286
Iteration 83/1000 | Loss: 0.00003286
Iteration 84/1000 | Loss: 0.00003285
Iteration 85/1000 | Loss: 0.00003285
Iteration 86/1000 | Loss: 0.00003285
Iteration 87/1000 | Loss: 0.00003285
Iteration 88/1000 | Loss: 0.00003285
Iteration 89/1000 | Loss: 0.00003285
Iteration 90/1000 | Loss: 0.00003285
Iteration 91/1000 | Loss: 0.00003285
Iteration 92/1000 | Loss: 0.00003284
Iteration 93/1000 | Loss: 0.00003284
Iteration 94/1000 | Loss: 0.00003284
Iteration 95/1000 | Loss: 0.00003283
Iteration 96/1000 | Loss: 0.00003283
Iteration 97/1000 | Loss: 0.00003283
Iteration 98/1000 | Loss: 0.00003283
Iteration 99/1000 | Loss: 0.00003283
Iteration 100/1000 | Loss: 0.00003282
Iteration 101/1000 | Loss: 0.00003282
Iteration 102/1000 | Loss: 0.00003282
Iteration 103/1000 | Loss: 0.00003282
Iteration 104/1000 | Loss: 0.00003282
Iteration 105/1000 | Loss: 0.00003282
Iteration 106/1000 | Loss: 0.00003282
Iteration 107/1000 | Loss: 0.00003282
Iteration 108/1000 | Loss: 0.00003282
Iteration 109/1000 | Loss: 0.00003281
Iteration 110/1000 | Loss: 0.00003281
Iteration 111/1000 | Loss: 0.00003281
Iteration 112/1000 | Loss: 0.00003281
Iteration 113/1000 | Loss: 0.00003281
Iteration 114/1000 | Loss: 0.00003281
Iteration 115/1000 | Loss: 0.00003281
Iteration 116/1000 | Loss: 0.00003281
Iteration 117/1000 | Loss: 0.00003281
Iteration 118/1000 | Loss: 0.00003281
Iteration 119/1000 | Loss: 0.00003281
Iteration 120/1000 | Loss: 0.00003281
Iteration 121/1000 | Loss: 0.00003280
Iteration 122/1000 | Loss: 0.00003280
Iteration 123/1000 | Loss: 0.00003280
Iteration 124/1000 | Loss: 0.00003280
Iteration 125/1000 | Loss: 0.00003280
Iteration 126/1000 | Loss: 0.00003280
Iteration 127/1000 | Loss: 0.00003280
Iteration 128/1000 | Loss: 0.00003280
Iteration 129/1000 | Loss: 0.00003280
Iteration 130/1000 | Loss: 0.00003280
Iteration 131/1000 | Loss: 0.00003280
Iteration 132/1000 | Loss: 0.00003280
Iteration 133/1000 | Loss: 0.00003280
Iteration 134/1000 | Loss: 0.00003280
Iteration 135/1000 | Loss: 0.00003280
Iteration 136/1000 | Loss: 0.00003280
Iteration 137/1000 | Loss: 0.00003280
Iteration 138/1000 | Loss: 0.00003280
Iteration 139/1000 | Loss: 0.00003280
Iteration 140/1000 | Loss: 0.00003280
Iteration 141/1000 | Loss: 0.00003279
Iteration 142/1000 | Loss: 0.00003279
Iteration 143/1000 | Loss: 0.00003279
Iteration 144/1000 | Loss: 0.00003279
Iteration 145/1000 | Loss: 0.00003279
Iteration 146/1000 | Loss: 0.00003279
Iteration 147/1000 | Loss: 0.00003279
Iteration 148/1000 | Loss: 0.00003279
Iteration 149/1000 | Loss: 0.00003279
Iteration 150/1000 | Loss: 0.00003279
Iteration 151/1000 | Loss: 0.00003279
Iteration 152/1000 | Loss: 0.00003279
Iteration 153/1000 | Loss: 0.00003279
Iteration 154/1000 | Loss: 0.00003279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [3.2792286219773814e-05, 3.2792286219773814e-05, 3.2792286219773814e-05, 3.2792286219773814e-05, 3.2792286219773814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2792286219773814e-05

Optimization complete. Final v2v error: 4.355264663696289 mm

Highest mean error: 5.421138763427734 mm for frame 146

Lowest mean error: 3.3309454917907715 mm for frame 49

Saving results

Total time: 50.438478231430054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407054
Iteration 2/25 | Loss: 0.00131122
Iteration 3/25 | Loss: 0.00124376
Iteration 4/25 | Loss: 0.00123530
Iteration 5/25 | Loss: 0.00123271
Iteration 6/25 | Loss: 0.00123185
Iteration 7/25 | Loss: 0.00123183
Iteration 8/25 | Loss: 0.00123183
Iteration 9/25 | Loss: 0.00123183
Iteration 10/25 | Loss: 0.00123183
Iteration 11/25 | Loss: 0.00123183
Iteration 12/25 | Loss: 0.00123183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012318257940933108, 0.0012318257940933108, 0.0012318257940933108, 0.0012318257940933108, 0.0012318257940933108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012318257940933108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31733668
Iteration 2/25 | Loss: 0.00133059
Iteration 3/25 | Loss: 0.00133056
Iteration 4/25 | Loss: 0.00133056
Iteration 5/25 | Loss: 0.00133056
Iteration 6/25 | Loss: 0.00133056
Iteration 7/25 | Loss: 0.00133056
Iteration 8/25 | Loss: 0.00133056
Iteration 9/25 | Loss: 0.00133056
Iteration 10/25 | Loss: 0.00133056
Iteration 11/25 | Loss: 0.00133056
Iteration 12/25 | Loss: 0.00133056
Iteration 13/25 | Loss: 0.00133056
Iteration 14/25 | Loss: 0.00133056
Iteration 15/25 | Loss: 0.00133056
Iteration 16/25 | Loss: 0.00133056
Iteration 17/25 | Loss: 0.00133056
Iteration 18/25 | Loss: 0.00133056
Iteration 19/25 | Loss: 0.00133056
Iteration 20/25 | Loss: 0.00133056
Iteration 21/25 | Loss: 0.00133056
Iteration 22/25 | Loss: 0.00133056
Iteration 23/25 | Loss: 0.00133056
Iteration 24/25 | Loss: 0.00133056
Iteration 25/25 | Loss: 0.00133056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133056
Iteration 2/1000 | Loss: 0.00002496
Iteration 3/1000 | Loss: 0.00001632
Iteration 4/1000 | Loss: 0.00001443
Iteration 5/1000 | Loss: 0.00001343
Iteration 6/1000 | Loss: 0.00001290
Iteration 7/1000 | Loss: 0.00001242
Iteration 8/1000 | Loss: 0.00001242
Iteration 9/1000 | Loss: 0.00001223
Iteration 10/1000 | Loss: 0.00001207
Iteration 11/1000 | Loss: 0.00001189
Iteration 12/1000 | Loss: 0.00001175
Iteration 13/1000 | Loss: 0.00001175
Iteration 14/1000 | Loss: 0.00001175
Iteration 15/1000 | Loss: 0.00001170
Iteration 16/1000 | Loss: 0.00001169
Iteration 17/1000 | Loss: 0.00001164
Iteration 18/1000 | Loss: 0.00001161
Iteration 19/1000 | Loss: 0.00001156
Iteration 20/1000 | Loss: 0.00001156
Iteration 21/1000 | Loss: 0.00001153
Iteration 22/1000 | Loss: 0.00001152
Iteration 23/1000 | Loss: 0.00001152
Iteration 24/1000 | Loss: 0.00001151
Iteration 25/1000 | Loss: 0.00001149
Iteration 26/1000 | Loss: 0.00001147
Iteration 27/1000 | Loss: 0.00001146
Iteration 28/1000 | Loss: 0.00001145
Iteration 29/1000 | Loss: 0.00001144
Iteration 30/1000 | Loss: 0.00001143
Iteration 31/1000 | Loss: 0.00001142
Iteration 32/1000 | Loss: 0.00001141
Iteration 33/1000 | Loss: 0.00001141
Iteration 34/1000 | Loss: 0.00001140
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001137
Iteration 37/1000 | Loss: 0.00001136
Iteration 38/1000 | Loss: 0.00001135
Iteration 39/1000 | Loss: 0.00001135
Iteration 40/1000 | Loss: 0.00001134
Iteration 41/1000 | Loss: 0.00001134
Iteration 42/1000 | Loss: 0.00001134
Iteration 43/1000 | Loss: 0.00001133
Iteration 44/1000 | Loss: 0.00001133
Iteration 45/1000 | Loss: 0.00001133
Iteration 46/1000 | Loss: 0.00001133
Iteration 47/1000 | Loss: 0.00001133
Iteration 48/1000 | Loss: 0.00001133
Iteration 49/1000 | Loss: 0.00001132
Iteration 50/1000 | Loss: 0.00001132
Iteration 51/1000 | Loss: 0.00001132
Iteration 52/1000 | Loss: 0.00001132
Iteration 53/1000 | Loss: 0.00001132
Iteration 54/1000 | Loss: 0.00001131
Iteration 55/1000 | Loss: 0.00001131
Iteration 56/1000 | Loss: 0.00001130
Iteration 57/1000 | Loss: 0.00001130
Iteration 58/1000 | Loss: 0.00001129
Iteration 59/1000 | Loss: 0.00001129
Iteration 60/1000 | Loss: 0.00001128
Iteration 61/1000 | Loss: 0.00001128
Iteration 62/1000 | Loss: 0.00001128
Iteration 63/1000 | Loss: 0.00001128
Iteration 64/1000 | Loss: 0.00001127
Iteration 65/1000 | Loss: 0.00001127
Iteration 66/1000 | Loss: 0.00001127
Iteration 67/1000 | Loss: 0.00001126
Iteration 68/1000 | Loss: 0.00001125
Iteration 69/1000 | Loss: 0.00001124
Iteration 70/1000 | Loss: 0.00001124
Iteration 71/1000 | Loss: 0.00001124
Iteration 72/1000 | Loss: 0.00001124
Iteration 73/1000 | Loss: 0.00001124
Iteration 74/1000 | Loss: 0.00001124
Iteration 75/1000 | Loss: 0.00001124
Iteration 76/1000 | Loss: 0.00001124
Iteration 77/1000 | Loss: 0.00001124
Iteration 78/1000 | Loss: 0.00001124
Iteration 79/1000 | Loss: 0.00001124
Iteration 80/1000 | Loss: 0.00001124
Iteration 81/1000 | Loss: 0.00001124
Iteration 82/1000 | Loss: 0.00001124
Iteration 83/1000 | Loss: 0.00001124
Iteration 84/1000 | Loss: 0.00001124
Iteration 85/1000 | Loss: 0.00001124
Iteration 86/1000 | Loss: 0.00001124
Iteration 87/1000 | Loss: 0.00001124
Iteration 88/1000 | Loss: 0.00001124
Iteration 89/1000 | Loss: 0.00001124
Iteration 90/1000 | Loss: 0.00001124
Iteration 91/1000 | Loss: 0.00001124
Iteration 92/1000 | Loss: 0.00001124
Iteration 93/1000 | Loss: 0.00001124
Iteration 94/1000 | Loss: 0.00001124
Iteration 95/1000 | Loss: 0.00001124
Iteration 96/1000 | Loss: 0.00001124
Iteration 97/1000 | Loss: 0.00001124
Iteration 98/1000 | Loss: 0.00001124
Iteration 99/1000 | Loss: 0.00001124
Iteration 100/1000 | Loss: 0.00001124
Iteration 101/1000 | Loss: 0.00001124
Iteration 102/1000 | Loss: 0.00001124
Iteration 103/1000 | Loss: 0.00001124
Iteration 104/1000 | Loss: 0.00001124
Iteration 105/1000 | Loss: 0.00001124
Iteration 106/1000 | Loss: 0.00001124
Iteration 107/1000 | Loss: 0.00001124
Iteration 108/1000 | Loss: 0.00001124
Iteration 109/1000 | Loss: 0.00001124
Iteration 110/1000 | Loss: 0.00001124
Iteration 111/1000 | Loss: 0.00001124
Iteration 112/1000 | Loss: 0.00001124
Iteration 113/1000 | Loss: 0.00001124
Iteration 114/1000 | Loss: 0.00001124
Iteration 115/1000 | Loss: 0.00001124
Iteration 116/1000 | Loss: 0.00001124
Iteration 117/1000 | Loss: 0.00001124
Iteration 118/1000 | Loss: 0.00001124
Iteration 119/1000 | Loss: 0.00001124
Iteration 120/1000 | Loss: 0.00001124
Iteration 121/1000 | Loss: 0.00001124
Iteration 122/1000 | Loss: 0.00001124
Iteration 123/1000 | Loss: 0.00001124
Iteration 124/1000 | Loss: 0.00001124
Iteration 125/1000 | Loss: 0.00001124
Iteration 126/1000 | Loss: 0.00001124
Iteration 127/1000 | Loss: 0.00001124
Iteration 128/1000 | Loss: 0.00001124
Iteration 129/1000 | Loss: 0.00001124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.1236647878831718e-05, 1.1236647878831718e-05, 1.1236647878831718e-05, 1.1236647878831718e-05, 1.1236647878831718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1236647878831718e-05

Optimization complete. Final v2v error: 2.8781561851501465 mm

Highest mean error: 3.1194045543670654 mm for frame 84

Lowest mean error: 2.7068986892700195 mm for frame 10

Saving results

Total time: 33.46010160446167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995876
Iteration 2/25 | Loss: 0.00190920
Iteration 3/25 | Loss: 0.00151936
Iteration 4/25 | Loss: 0.00149985
Iteration 5/25 | Loss: 0.00149304
Iteration 6/25 | Loss: 0.00149169
Iteration 7/25 | Loss: 0.00149169
Iteration 8/25 | Loss: 0.00149169
Iteration 9/25 | Loss: 0.00149169
Iteration 10/25 | Loss: 0.00149169
Iteration 11/25 | Loss: 0.00149169
Iteration 12/25 | Loss: 0.00149169
Iteration 13/25 | Loss: 0.00149169
Iteration 14/25 | Loss: 0.00149169
Iteration 15/25 | Loss: 0.00149169
Iteration 16/25 | Loss: 0.00149169
Iteration 17/25 | Loss: 0.00149169
Iteration 18/25 | Loss: 0.00149169
Iteration 19/25 | Loss: 0.00149169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014916947111487389, 0.0014916947111487389, 0.0014916947111487389, 0.0014916947111487389, 0.0014916947111487389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014916947111487389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52119809
Iteration 2/25 | Loss: 0.00135954
Iteration 3/25 | Loss: 0.00135953
Iteration 4/25 | Loss: 0.00135953
Iteration 5/25 | Loss: 0.00135953
Iteration 6/25 | Loss: 0.00135953
Iteration 7/25 | Loss: 0.00135953
Iteration 8/25 | Loss: 0.00135953
Iteration 9/25 | Loss: 0.00135953
Iteration 10/25 | Loss: 0.00135953
Iteration 11/25 | Loss: 0.00135953
Iteration 12/25 | Loss: 0.00135953
Iteration 13/25 | Loss: 0.00135953
Iteration 14/25 | Loss: 0.00135953
Iteration 15/25 | Loss: 0.00135953
Iteration 16/25 | Loss: 0.00135953
Iteration 17/25 | Loss: 0.00135953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013595299096778035, 0.0013595299096778035, 0.0013595299096778035, 0.0013595299096778035, 0.0013595299096778035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013595299096778035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135953
Iteration 2/1000 | Loss: 0.00009223
Iteration 3/1000 | Loss: 0.00005910
Iteration 4/1000 | Loss: 0.00004626
Iteration 5/1000 | Loss: 0.00004129
Iteration 6/1000 | Loss: 0.00003958
Iteration 7/1000 | Loss: 0.00003880
Iteration 8/1000 | Loss: 0.00003801
Iteration 9/1000 | Loss: 0.00003749
Iteration 10/1000 | Loss: 0.00003700
Iteration 11/1000 | Loss: 0.00003651
Iteration 12/1000 | Loss: 0.00003618
Iteration 13/1000 | Loss: 0.00003581
Iteration 14/1000 | Loss: 0.00003540
Iteration 15/1000 | Loss: 0.00003506
Iteration 16/1000 | Loss: 0.00003477
Iteration 17/1000 | Loss: 0.00003448
Iteration 18/1000 | Loss: 0.00003423
Iteration 19/1000 | Loss: 0.00003411
Iteration 20/1000 | Loss: 0.00003403
Iteration 21/1000 | Loss: 0.00003400
Iteration 22/1000 | Loss: 0.00003391
Iteration 23/1000 | Loss: 0.00003388
Iteration 24/1000 | Loss: 0.00003380
Iteration 25/1000 | Loss: 0.00003373
Iteration 26/1000 | Loss: 0.00003370
Iteration 27/1000 | Loss: 0.00003369
Iteration 28/1000 | Loss: 0.00003369
Iteration 29/1000 | Loss: 0.00003369
Iteration 30/1000 | Loss: 0.00003368
Iteration 31/1000 | Loss: 0.00003364
Iteration 32/1000 | Loss: 0.00003364
Iteration 33/1000 | Loss: 0.00003364
Iteration 34/1000 | Loss: 0.00003363
Iteration 35/1000 | Loss: 0.00003361
Iteration 36/1000 | Loss: 0.00003360
Iteration 37/1000 | Loss: 0.00003360
Iteration 38/1000 | Loss: 0.00003360
Iteration 39/1000 | Loss: 0.00003360
Iteration 40/1000 | Loss: 0.00003360
Iteration 41/1000 | Loss: 0.00003360
Iteration 42/1000 | Loss: 0.00003359
Iteration 43/1000 | Loss: 0.00003359
Iteration 44/1000 | Loss: 0.00003357
Iteration 45/1000 | Loss: 0.00003357
Iteration 46/1000 | Loss: 0.00003356
Iteration 47/1000 | Loss: 0.00003356
Iteration 48/1000 | Loss: 0.00003356
Iteration 49/1000 | Loss: 0.00003356
Iteration 50/1000 | Loss: 0.00003356
Iteration 51/1000 | Loss: 0.00003356
Iteration 52/1000 | Loss: 0.00003356
Iteration 53/1000 | Loss: 0.00003356
Iteration 54/1000 | Loss: 0.00003356
Iteration 55/1000 | Loss: 0.00003355
Iteration 56/1000 | Loss: 0.00003354
Iteration 57/1000 | Loss: 0.00003353
Iteration 58/1000 | Loss: 0.00003353
Iteration 59/1000 | Loss: 0.00003353
Iteration 60/1000 | Loss: 0.00003353
Iteration 61/1000 | Loss: 0.00003353
Iteration 62/1000 | Loss: 0.00003353
Iteration 63/1000 | Loss: 0.00003353
Iteration 64/1000 | Loss: 0.00003353
Iteration 65/1000 | Loss: 0.00003353
Iteration 66/1000 | Loss: 0.00003353
Iteration 67/1000 | Loss: 0.00003353
Iteration 68/1000 | Loss: 0.00003352
Iteration 69/1000 | Loss: 0.00003352
Iteration 70/1000 | Loss: 0.00003352
Iteration 71/1000 | Loss: 0.00003352
Iteration 72/1000 | Loss: 0.00003350
Iteration 73/1000 | Loss: 0.00003350
Iteration 74/1000 | Loss: 0.00003350
Iteration 75/1000 | Loss: 0.00003350
Iteration 76/1000 | Loss: 0.00003350
Iteration 77/1000 | Loss: 0.00003350
Iteration 78/1000 | Loss: 0.00003350
Iteration 79/1000 | Loss: 0.00003349
Iteration 80/1000 | Loss: 0.00003349
Iteration 81/1000 | Loss: 0.00003348
Iteration 82/1000 | Loss: 0.00003348
Iteration 83/1000 | Loss: 0.00003348
Iteration 84/1000 | Loss: 0.00003348
Iteration 85/1000 | Loss: 0.00003348
Iteration 86/1000 | Loss: 0.00003348
Iteration 87/1000 | Loss: 0.00003348
Iteration 88/1000 | Loss: 0.00003348
Iteration 89/1000 | Loss: 0.00003348
Iteration 90/1000 | Loss: 0.00003348
Iteration 91/1000 | Loss: 0.00003348
Iteration 92/1000 | Loss: 0.00003347
Iteration 93/1000 | Loss: 0.00003347
Iteration 94/1000 | Loss: 0.00003347
Iteration 95/1000 | Loss: 0.00003347
Iteration 96/1000 | Loss: 0.00003346
Iteration 97/1000 | Loss: 0.00003346
Iteration 98/1000 | Loss: 0.00003346
Iteration 99/1000 | Loss: 0.00003346
Iteration 100/1000 | Loss: 0.00003346
Iteration 101/1000 | Loss: 0.00003346
Iteration 102/1000 | Loss: 0.00003346
Iteration 103/1000 | Loss: 0.00003345
Iteration 104/1000 | Loss: 0.00003345
Iteration 105/1000 | Loss: 0.00003345
Iteration 106/1000 | Loss: 0.00003345
Iteration 107/1000 | Loss: 0.00003345
Iteration 108/1000 | Loss: 0.00003344
Iteration 109/1000 | Loss: 0.00003344
Iteration 110/1000 | Loss: 0.00003344
Iteration 111/1000 | Loss: 0.00003344
Iteration 112/1000 | Loss: 0.00003343
Iteration 113/1000 | Loss: 0.00003343
Iteration 114/1000 | Loss: 0.00003343
Iteration 115/1000 | Loss: 0.00003343
Iteration 116/1000 | Loss: 0.00003343
Iteration 117/1000 | Loss: 0.00003343
Iteration 118/1000 | Loss: 0.00003343
Iteration 119/1000 | Loss: 0.00003343
Iteration 120/1000 | Loss: 0.00003343
Iteration 121/1000 | Loss: 0.00003343
Iteration 122/1000 | Loss: 0.00003343
Iteration 123/1000 | Loss: 0.00003343
Iteration 124/1000 | Loss: 0.00003343
Iteration 125/1000 | Loss: 0.00003343
Iteration 126/1000 | Loss: 0.00003343
Iteration 127/1000 | Loss: 0.00003343
Iteration 128/1000 | Loss: 0.00003342
Iteration 129/1000 | Loss: 0.00003342
Iteration 130/1000 | Loss: 0.00003342
Iteration 131/1000 | Loss: 0.00003342
Iteration 132/1000 | Loss: 0.00003342
Iteration 133/1000 | Loss: 0.00003341
Iteration 134/1000 | Loss: 0.00003341
Iteration 135/1000 | Loss: 0.00003341
Iteration 136/1000 | Loss: 0.00003341
Iteration 137/1000 | Loss: 0.00003341
Iteration 138/1000 | Loss: 0.00003340
Iteration 139/1000 | Loss: 0.00003340
Iteration 140/1000 | Loss: 0.00003340
Iteration 141/1000 | Loss: 0.00003340
Iteration 142/1000 | Loss: 0.00003340
Iteration 143/1000 | Loss: 0.00003339
Iteration 144/1000 | Loss: 0.00003339
Iteration 145/1000 | Loss: 0.00003339
Iteration 146/1000 | Loss: 0.00003339
Iteration 147/1000 | Loss: 0.00003339
Iteration 148/1000 | Loss: 0.00003338
Iteration 149/1000 | Loss: 0.00003338
Iteration 150/1000 | Loss: 0.00003338
Iteration 151/1000 | Loss: 0.00003338
Iteration 152/1000 | Loss: 0.00003338
Iteration 153/1000 | Loss: 0.00003338
Iteration 154/1000 | Loss: 0.00003338
Iteration 155/1000 | Loss: 0.00003338
Iteration 156/1000 | Loss: 0.00003338
Iteration 157/1000 | Loss: 0.00003337
Iteration 158/1000 | Loss: 0.00003337
Iteration 159/1000 | Loss: 0.00003337
Iteration 160/1000 | Loss: 0.00003337
Iteration 161/1000 | Loss: 0.00003337
Iteration 162/1000 | Loss: 0.00003337
Iteration 163/1000 | Loss: 0.00003337
Iteration 164/1000 | Loss: 0.00003337
Iteration 165/1000 | Loss: 0.00003337
Iteration 166/1000 | Loss: 0.00003336
Iteration 167/1000 | Loss: 0.00003336
Iteration 168/1000 | Loss: 0.00003336
Iteration 169/1000 | Loss: 0.00003336
Iteration 170/1000 | Loss: 0.00003335
Iteration 171/1000 | Loss: 0.00003335
Iteration 172/1000 | Loss: 0.00003335
Iteration 173/1000 | Loss: 0.00003335
Iteration 174/1000 | Loss: 0.00003335
Iteration 175/1000 | Loss: 0.00003335
Iteration 176/1000 | Loss: 0.00003335
Iteration 177/1000 | Loss: 0.00003335
Iteration 178/1000 | Loss: 0.00003335
Iteration 179/1000 | Loss: 0.00003335
Iteration 180/1000 | Loss: 0.00003335
Iteration 181/1000 | Loss: 0.00003335
Iteration 182/1000 | Loss: 0.00003335
Iteration 183/1000 | Loss: 0.00003335
Iteration 184/1000 | Loss: 0.00003335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [3.334824577905238e-05, 3.334824577905238e-05, 3.334824577905238e-05, 3.334824577905238e-05, 3.334824577905238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.334824577905238e-05

Optimization complete. Final v2v error: 4.740291118621826 mm

Highest mean error: 5.738248825073242 mm for frame 40

Lowest mean error: 4.186406135559082 mm for frame 95

Saving results

Total time: 54.79717254638672
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999446
Iteration 2/25 | Loss: 0.00291559
Iteration 3/25 | Loss: 0.00227297
Iteration 4/25 | Loss: 0.00217125
Iteration 5/25 | Loss: 0.00201138
Iteration 6/25 | Loss: 0.00193960
Iteration 7/25 | Loss: 0.00177313
Iteration 8/25 | Loss: 0.00171663
Iteration 9/25 | Loss: 0.00169840
Iteration 10/25 | Loss: 0.00168066
Iteration 11/25 | Loss: 0.00167395
Iteration 12/25 | Loss: 0.00166866
Iteration 13/25 | Loss: 0.00166159
Iteration 14/25 | Loss: 0.00164905
Iteration 15/25 | Loss: 0.00164465
Iteration 16/25 | Loss: 0.00164328
Iteration 17/25 | Loss: 0.00164610
Iteration 18/25 | Loss: 0.00164187
Iteration 19/25 | Loss: 0.00164081
Iteration 20/25 | Loss: 0.00164226
Iteration 21/25 | Loss: 0.00164187
Iteration 22/25 | Loss: 0.00164012
Iteration 23/25 | Loss: 0.00163721
Iteration 24/25 | Loss: 0.00163572
Iteration 25/25 | Loss: 0.00163505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33799815
Iteration 2/25 | Loss: 0.00314896
Iteration 3/25 | Loss: 0.00285706
Iteration 4/25 | Loss: 0.00285706
Iteration 5/25 | Loss: 0.00285706
Iteration 6/25 | Loss: 0.00285706
Iteration 7/25 | Loss: 0.00285706
Iteration 8/25 | Loss: 0.00285706
Iteration 9/25 | Loss: 0.00285706
Iteration 10/25 | Loss: 0.00285706
Iteration 11/25 | Loss: 0.00285706
Iteration 12/25 | Loss: 0.00285706
Iteration 13/25 | Loss: 0.00285706
Iteration 14/25 | Loss: 0.00285706
Iteration 15/25 | Loss: 0.00285706
Iteration 16/25 | Loss: 0.00285706
Iteration 17/25 | Loss: 0.00285706
Iteration 18/25 | Loss: 0.00285706
Iteration 19/25 | Loss: 0.00285706
Iteration 20/25 | Loss: 0.00285706
Iteration 21/25 | Loss: 0.00285706
Iteration 22/25 | Loss: 0.00285706
Iteration 23/25 | Loss: 0.00285706
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00285705947317183, 0.00285705947317183, 0.00285705947317183, 0.00285705947317183, 0.00285705947317183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00285705947317183

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285706
Iteration 2/1000 | Loss: 0.00039937
Iteration 3/1000 | Loss: 0.00066300
Iteration 4/1000 | Loss: 0.00055860
Iteration 5/1000 | Loss: 0.00074909
Iteration 6/1000 | Loss: 0.00137414
Iteration 7/1000 | Loss: 0.00142105
Iteration 8/1000 | Loss: 0.00076013
Iteration 9/1000 | Loss: 0.00019715
Iteration 10/1000 | Loss: 0.00020082
Iteration 11/1000 | Loss: 0.00030048
Iteration 12/1000 | Loss: 0.00034978
Iteration 13/1000 | Loss: 0.00079197
Iteration 14/1000 | Loss: 0.00033403
Iteration 15/1000 | Loss: 0.00077959
Iteration 16/1000 | Loss: 0.00093668
Iteration 17/1000 | Loss: 0.00168038
Iteration 18/1000 | Loss: 0.00223755
Iteration 19/1000 | Loss: 0.00204863
Iteration 20/1000 | Loss: 0.00670732
Iteration 21/1000 | Loss: 0.00508510
Iteration 22/1000 | Loss: 0.00790484
Iteration 23/1000 | Loss: 0.00333680
Iteration 24/1000 | Loss: 0.00272312
Iteration 25/1000 | Loss: 0.00116886
Iteration 26/1000 | Loss: 0.00133256
Iteration 27/1000 | Loss: 0.00093173
Iteration 28/1000 | Loss: 0.00080028
Iteration 29/1000 | Loss: 0.00023003
Iteration 30/1000 | Loss: 0.00012966
Iteration 31/1000 | Loss: 0.00012316
Iteration 32/1000 | Loss: 0.00060310
Iteration 33/1000 | Loss: 0.00067872
Iteration 34/1000 | Loss: 0.00052120
Iteration 35/1000 | Loss: 0.00050049
Iteration 36/1000 | Loss: 0.00061731
Iteration 37/1000 | Loss: 0.00036400
Iteration 38/1000 | Loss: 0.00008450
Iteration 39/1000 | Loss: 0.00005887
Iteration 40/1000 | Loss: 0.00011944
Iteration 41/1000 | Loss: 0.00029960
Iteration 42/1000 | Loss: 0.00018682
Iteration 43/1000 | Loss: 0.00006660
Iteration 44/1000 | Loss: 0.00014901
Iteration 45/1000 | Loss: 0.00014546
Iteration 46/1000 | Loss: 0.00013099
Iteration 47/1000 | Loss: 0.00018847
Iteration 48/1000 | Loss: 0.00005624
Iteration 49/1000 | Loss: 0.00010041
Iteration 50/1000 | Loss: 0.00042485
Iteration 51/1000 | Loss: 0.00019056
Iteration 52/1000 | Loss: 0.00017423
Iteration 53/1000 | Loss: 0.00005967
Iteration 54/1000 | Loss: 0.00037925
Iteration 55/1000 | Loss: 0.00045739
Iteration 56/1000 | Loss: 0.00036922
Iteration 57/1000 | Loss: 0.00045425
Iteration 58/1000 | Loss: 0.00055869
Iteration 59/1000 | Loss: 0.00024010
Iteration 60/1000 | Loss: 0.00039870
Iteration 61/1000 | Loss: 0.00037941
Iteration 62/1000 | Loss: 0.00027854
Iteration 63/1000 | Loss: 0.00031528
Iteration 64/1000 | Loss: 0.00043414
Iteration 65/1000 | Loss: 0.00028992
Iteration 66/1000 | Loss: 0.00004976
Iteration 67/1000 | Loss: 0.00036310
Iteration 68/1000 | Loss: 0.00011591
Iteration 69/1000 | Loss: 0.00005748
Iteration 70/1000 | Loss: 0.00041069
Iteration 71/1000 | Loss: 0.00008065
Iteration 72/1000 | Loss: 0.00005252
Iteration 73/1000 | Loss: 0.00005836
Iteration 74/1000 | Loss: 0.00004163
Iteration 75/1000 | Loss: 0.00003995
Iteration 76/1000 | Loss: 0.00006959
Iteration 77/1000 | Loss: 0.00014600
Iteration 78/1000 | Loss: 0.00033595
Iteration 79/1000 | Loss: 0.00012704
Iteration 80/1000 | Loss: 0.00021349
Iteration 81/1000 | Loss: 0.00014236
Iteration 82/1000 | Loss: 0.00021427
Iteration 83/1000 | Loss: 0.00003859
Iteration 84/1000 | Loss: 0.00003583
Iteration 85/1000 | Loss: 0.00034680
Iteration 86/1000 | Loss: 0.00017396
Iteration 87/1000 | Loss: 0.00021373
Iteration 88/1000 | Loss: 0.00022396
Iteration 89/1000 | Loss: 0.00021541
Iteration 90/1000 | Loss: 0.00003722
Iteration 91/1000 | Loss: 0.00003432
Iteration 92/1000 | Loss: 0.00003287
Iteration 93/1000 | Loss: 0.00003196
Iteration 94/1000 | Loss: 0.00003114
Iteration 95/1000 | Loss: 0.00003347
Iteration 96/1000 | Loss: 0.00003345
Iteration 97/1000 | Loss: 0.00003272
Iteration 98/1000 | Loss: 0.00003066
Iteration 99/1000 | Loss: 0.00002995
Iteration 100/1000 | Loss: 0.00002995
Iteration 101/1000 | Loss: 0.00030272
Iteration 102/1000 | Loss: 0.00016636
Iteration 103/1000 | Loss: 0.00022301
Iteration 104/1000 | Loss: 0.00049588
Iteration 105/1000 | Loss: 0.00030549
Iteration 106/1000 | Loss: 0.00003245
Iteration 107/1000 | Loss: 0.00030550
Iteration 108/1000 | Loss: 0.00016656
Iteration 109/1000 | Loss: 0.00025064
Iteration 110/1000 | Loss: 0.00022989
Iteration 111/1000 | Loss: 0.00004231
Iteration 112/1000 | Loss: 0.00003603
Iteration 113/1000 | Loss: 0.00003133
Iteration 114/1000 | Loss: 0.00002951
Iteration 115/1000 | Loss: 0.00002922
Iteration 116/1000 | Loss: 0.00002867
Iteration 117/1000 | Loss: 0.00002837
Iteration 118/1000 | Loss: 0.00021513
Iteration 119/1000 | Loss: 0.00017622
Iteration 120/1000 | Loss: 0.00021782
Iteration 121/1000 | Loss: 0.00020608
Iteration 122/1000 | Loss: 0.00003615
Iteration 123/1000 | Loss: 0.00005691
Iteration 124/1000 | Loss: 0.00003092
Iteration 125/1000 | Loss: 0.00002832
Iteration 126/1000 | Loss: 0.00002810
Iteration 127/1000 | Loss: 0.00002807
Iteration 128/1000 | Loss: 0.00002803
Iteration 129/1000 | Loss: 0.00002802
Iteration 130/1000 | Loss: 0.00002798
Iteration 131/1000 | Loss: 0.00002796
Iteration 132/1000 | Loss: 0.00002796
Iteration 133/1000 | Loss: 0.00002795
Iteration 134/1000 | Loss: 0.00002795
Iteration 135/1000 | Loss: 0.00002795
Iteration 136/1000 | Loss: 0.00002794
Iteration 137/1000 | Loss: 0.00002794
Iteration 138/1000 | Loss: 0.00002793
Iteration 139/1000 | Loss: 0.00017312
Iteration 140/1000 | Loss: 0.00005472
Iteration 141/1000 | Loss: 0.00010802
Iteration 142/1000 | Loss: 0.00004084
Iteration 143/1000 | Loss: 0.00004339
Iteration 144/1000 | Loss: 0.00002907
Iteration 145/1000 | Loss: 0.00002835
Iteration 146/1000 | Loss: 0.00002769
Iteration 147/1000 | Loss: 0.00003889
Iteration 148/1000 | Loss: 0.00002690
Iteration 149/1000 | Loss: 0.00002658
Iteration 150/1000 | Loss: 0.00002642
Iteration 151/1000 | Loss: 0.00002631
Iteration 152/1000 | Loss: 0.00002631
Iteration 153/1000 | Loss: 0.00002627
Iteration 154/1000 | Loss: 0.00002627
Iteration 155/1000 | Loss: 0.00002993
Iteration 156/1000 | Loss: 0.00002614
Iteration 157/1000 | Loss: 0.00002761
Iteration 158/1000 | Loss: 0.00011746
Iteration 159/1000 | Loss: 0.00007719
Iteration 160/1000 | Loss: 0.00003359
Iteration 161/1000 | Loss: 0.00002752
Iteration 162/1000 | Loss: 0.00002694
Iteration 163/1000 | Loss: 0.00002617
Iteration 164/1000 | Loss: 0.00002610
Iteration 165/1000 | Loss: 0.00013666
Iteration 166/1000 | Loss: 0.00004246
Iteration 167/1000 | Loss: 0.00002647
Iteration 168/1000 | Loss: 0.00011456
Iteration 169/1000 | Loss: 0.00003755
Iteration 170/1000 | Loss: 0.00002708
Iteration 171/1000 | Loss: 0.00002625
Iteration 172/1000 | Loss: 0.00012883
Iteration 173/1000 | Loss: 0.00003189
Iteration 174/1000 | Loss: 0.00003571
Iteration 175/1000 | Loss: 0.00009870
Iteration 176/1000 | Loss: 0.00003616
Iteration 177/1000 | Loss: 0.00004107
Iteration 178/1000 | Loss: 0.00003805
Iteration 179/1000 | Loss: 0.00006662
Iteration 180/1000 | Loss: 0.00003849
Iteration 181/1000 | Loss: 0.00003662
Iteration 182/1000 | Loss: 0.00004290
Iteration 183/1000 | Loss: 0.00004680
Iteration 184/1000 | Loss: 0.00002609
Iteration 185/1000 | Loss: 0.00002577
Iteration 186/1000 | Loss: 0.00002556
Iteration 187/1000 | Loss: 0.00002525
Iteration 188/1000 | Loss: 0.00002502
Iteration 189/1000 | Loss: 0.00002948
Iteration 190/1000 | Loss: 0.00002484
Iteration 191/1000 | Loss: 0.00002536
Iteration 192/1000 | Loss: 0.00002479
Iteration 193/1000 | Loss: 0.00002478
Iteration 194/1000 | Loss: 0.00002478
Iteration 195/1000 | Loss: 0.00002478
Iteration 196/1000 | Loss: 0.00002482
Iteration 197/1000 | Loss: 0.00002482
Iteration 198/1000 | Loss: 0.00002479
Iteration 199/1000 | Loss: 0.00002479
Iteration 200/1000 | Loss: 0.00002479
Iteration 201/1000 | Loss: 0.00002481
Iteration 202/1000 | Loss: 0.00002480
Iteration 203/1000 | Loss: 0.00002479
Iteration 204/1000 | Loss: 0.00002479
Iteration 205/1000 | Loss: 0.00002478
Iteration 206/1000 | Loss: 0.00002477
Iteration 207/1000 | Loss: 0.00002477
Iteration 208/1000 | Loss: 0.00002472
Iteration 209/1000 | Loss: 0.00002472
Iteration 210/1000 | Loss: 0.00002770
Iteration 211/1000 | Loss: 0.00002488
Iteration 212/1000 | Loss: 0.00002467
Iteration 213/1000 | Loss: 0.00002465
Iteration 214/1000 | Loss: 0.00002464
Iteration 215/1000 | Loss: 0.00002463
Iteration 216/1000 | Loss: 0.00006323
Iteration 217/1000 | Loss: 0.00003759
Iteration 218/1000 | Loss: 0.00002460
Iteration 219/1000 | Loss: 0.00002459
Iteration 220/1000 | Loss: 0.00002459
Iteration 221/1000 | Loss: 0.00002459
Iteration 222/1000 | Loss: 0.00002458
Iteration 223/1000 | Loss: 0.00002458
Iteration 224/1000 | Loss: 0.00002458
Iteration 225/1000 | Loss: 0.00002458
Iteration 226/1000 | Loss: 0.00002457
Iteration 227/1000 | Loss: 0.00002457
Iteration 228/1000 | Loss: 0.00002457
Iteration 229/1000 | Loss: 0.00002457
Iteration 230/1000 | Loss: 0.00002457
Iteration 231/1000 | Loss: 0.00002457
Iteration 232/1000 | Loss: 0.00002457
Iteration 233/1000 | Loss: 0.00002457
Iteration 234/1000 | Loss: 0.00002457
Iteration 235/1000 | Loss: 0.00002457
Iteration 236/1000 | Loss: 0.00002457
Iteration 237/1000 | Loss: 0.00002457
Iteration 238/1000 | Loss: 0.00002457
Iteration 239/1000 | Loss: 0.00005332
Iteration 240/1000 | Loss: 0.00005331
Iteration 241/1000 | Loss: 0.00002456
Iteration 242/1000 | Loss: 0.00002455
Iteration 243/1000 | Loss: 0.00002455
Iteration 244/1000 | Loss: 0.00002455
Iteration 245/1000 | Loss: 0.00002455
Iteration 246/1000 | Loss: 0.00002455
Iteration 247/1000 | Loss: 0.00002454
Iteration 248/1000 | Loss: 0.00002454
Iteration 249/1000 | Loss: 0.00002454
Iteration 250/1000 | Loss: 0.00002454
Iteration 251/1000 | Loss: 0.00002454
Iteration 252/1000 | Loss: 0.00002454
Iteration 253/1000 | Loss: 0.00002454
Iteration 254/1000 | Loss: 0.00002454
Iteration 255/1000 | Loss: 0.00002454
Iteration 256/1000 | Loss: 0.00002454
Iteration 257/1000 | Loss: 0.00002454
Iteration 258/1000 | Loss: 0.00002453
Iteration 259/1000 | Loss: 0.00002453
Iteration 260/1000 | Loss: 0.00002453
Iteration 261/1000 | Loss: 0.00002453
Iteration 262/1000 | Loss: 0.00002453
Iteration 263/1000 | Loss: 0.00002453
Iteration 264/1000 | Loss: 0.00002453
Iteration 265/1000 | Loss: 0.00002453
Iteration 266/1000 | Loss: 0.00002453
Iteration 267/1000 | Loss: 0.00002453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [2.453328670526389e-05, 2.453328670526389e-05, 2.453328670526389e-05, 2.453328670526389e-05, 2.453328670526389e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.453328670526389e-05

Optimization complete. Final v2v error: 3.7543210983276367 mm

Highest mean error: 10.240550994873047 mm for frame 89

Lowest mean error: 2.9318997859954834 mm for frame 15

Saving results

Total time: 346.64802384376526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786318
Iteration 2/25 | Loss: 0.00166159
Iteration 3/25 | Loss: 0.00137772
Iteration 4/25 | Loss: 0.00134503
Iteration 5/25 | Loss: 0.00133689
Iteration 6/25 | Loss: 0.00133402
Iteration 7/25 | Loss: 0.00133369
Iteration 8/25 | Loss: 0.00133369
Iteration 9/25 | Loss: 0.00133369
Iteration 10/25 | Loss: 0.00133369
Iteration 11/25 | Loss: 0.00133369
Iteration 12/25 | Loss: 0.00133369
Iteration 13/25 | Loss: 0.00133369
Iteration 14/25 | Loss: 0.00133369
Iteration 15/25 | Loss: 0.00133369
Iteration 16/25 | Loss: 0.00133369
Iteration 17/25 | Loss: 0.00133369
Iteration 18/25 | Loss: 0.00133369
Iteration 19/25 | Loss: 0.00133369
Iteration 20/25 | Loss: 0.00133369
Iteration 21/25 | Loss: 0.00133369
Iteration 22/25 | Loss: 0.00133369
Iteration 23/25 | Loss: 0.00133369
Iteration 24/25 | Loss: 0.00133369
Iteration 25/25 | Loss: 0.00133369

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12974727
Iteration 2/25 | Loss: 0.00124607
Iteration 3/25 | Loss: 0.00124607
Iteration 4/25 | Loss: 0.00124607
Iteration 5/25 | Loss: 0.00124606
Iteration 6/25 | Loss: 0.00124606
Iteration 7/25 | Loss: 0.00124606
Iteration 8/25 | Loss: 0.00124606
Iteration 9/25 | Loss: 0.00124606
Iteration 10/25 | Loss: 0.00124606
Iteration 11/25 | Loss: 0.00124606
Iteration 12/25 | Loss: 0.00124606
Iteration 13/25 | Loss: 0.00124606
Iteration 14/25 | Loss: 0.00124606
Iteration 15/25 | Loss: 0.00124606
Iteration 16/25 | Loss: 0.00124606
Iteration 17/25 | Loss: 0.00124606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001246063387952745, 0.001246063387952745, 0.001246063387952745, 0.001246063387952745, 0.001246063387952745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001246063387952745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124606
Iteration 2/1000 | Loss: 0.00007687
Iteration 3/1000 | Loss: 0.00004905
Iteration 4/1000 | Loss: 0.00004303
Iteration 5/1000 | Loss: 0.00004104
Iteration 6/1000 | Loss: 0.00003906
Iteration 7/1000 | Loss: 0.00003777
Iteration 8/1000 | Loss: 0.00003681
Iteration 9/1000 | Loss: 0.00003628
Iteration 10/1000 | Loss: 0.00003586
Iteration 11/1000 | Loss: 0.00003546
Iteration 12/1000 | Loss: 0.00003509
Iteration 13/1000 | Loss: 0.00003484
Iteration 14/1000 | Loss: 0.00003460
Iteration 15/1000 | Loss: 0.00003436
Iteration 16/1000 | Loss: 0.00003417
Iteration 17/1000 | Loss: 0.00003404
Iteration 18/1000 | Loss: 0.00003403
Iteration 19/1000 | Loss: 0.00003393
Iteration 20/1000 | Loss: 0.00003389
Iteration 21/1000 | Loss: 0.00003386
Iteration 22/1000 | Loss: 0.00003383
Iteration 23/1000 | Loss: 0.00003381
Iteration 24/1000 | Loss: 0.00003380
Iteration 25/1000 | Loss: 0.00003380
Iteration 26/1000 | Loss: 0.00003379
Iteration 27/1000 | Loss: 0.00003379
Iteration 28/1000 | Loss: 0.00003379
Iteration 29/1000 | Loss: 0.00003378
Iteration 30/1000 | Loss: 0.00003378
Iteration 31/1000 | Loss: 0.00003377
Iteration 32/1000 | Loss: 0.00003376
Iteration 33/1000 | Loss: 0.00003376
Iteration 34/1000 | Loss: 0.00003374
Iteration 35/1000 | Loss: 0.00003374
Iteration 36/1000 | Loss: 0.00003374
Iteration 37/1000 | Loss: 0.00003374
Iteration 38/1000 | Loss: 0.00003374
Iteration 39/1000 | Loss: 0.00003374
Iteration 40/1000 | Loss: 0.00003373
Iteration 41/1000 | Loss: 0.00003373
Iteration 42/1000 | Loss: 0.00003373
Iteration 43/1000 | Loss: 0.00003373
Iteration 44/1000 | Loss: 0.00003372
Iteration 45/1000 | Loss: 0.00003372
Iteration 46/1000 | Loss: 0.00003371
Iteration 47/1000 | Loss: 0.00003371
Iteration 48/1000 | Loss: 0.00003370
Iteration 49/1000 | Loss: 0.00003370
Iteration 50/1000 | Loss: 0.00003370
Iteration 51/1000 | Loss: 0.00003370
Iteration 52/1000 | Loss: 0.00003369
Iteration 53/1000 | Loss: 0.00003368
Iteration 54/1000 | Loss: 0.00003367
Iteration 55/1000 | Loss: 0.00003366
Iteration 56/1000 | Loss: 0.00003366
Iteration 57/1000 | Loss: 0.00003366
Iteration 58/1000 | Loss: 0.00003365
Iteration 59/1000 | Loss: 0.00003365
Iteration 60/1000 | Loss: 0.00003364
Iteration 61/1000 | Loss: 0.00003363
Iteration 62/1000 | Loss: 0.00003363
Iteration 63/1000 | Loss: 0.00003363
Iteration 64/1000 | Loss: 0.00003362
Iteration 65/1000 | Loss: 0.00003362
Iteration 66/1000 | Loss: 0.00003361
Iteration 67/1000 | Loss: 0.00003361
Iteration 68/1000 | Loss: 0.00003361
Iteration 69/1000 | Loss: 0.00003360
Iteration 70/1000 | Loss: 0.00003360
Iteration 71/1000 | Loss: 0.00003360
Iteration 72/1000 | Loss: 0.00003360
Iteration 73/1000 | Loss: 0.00003359
Iteration 74/1000 | Loss: 0.00003359
Iteration 75/1000 | Loss: 0.00003358
Iteration 76/1000 | Loss: 0.00003358
Iteration 77/1000 | Loss: 0.00003358
Iteration 78/1000 | Loss: 0.00003358
Iteration 79/1000 | Loss: 0.00003358
Iteration 80/1000 | Loss: 0.00003358
Iteration 81/1000 | Loss: 0.00003358
Iteration 82/1000 | Loss: 0.00003358
Iteration 83/1000 | Loss: 0.00003357
Iteration 84/1000 | Loss: 0.00003357
Iteration 85/1000 | Loss: 0.00003357
Iteration 86/1000 | Loss: 0.00003357
Iteration 87/1000 | Loss: 0.00003357
Iteration 88/1000 | Loss: 0.00003356
Iteration 89/1000 | Loss: 0.00003356
Iteration 90/1000 | Loss: 0.00003356
Iteration 91/1000 | Loss: 0.00003356
Iteration 92/1000 | Loss: 0.00003356
Iteration 93/1000 | Loss: 0.00003355
Iteration 94/1000 | Loss: 0.00003355
Iteration 95/1000 | Loss: 0.00003355
Iteration 96/1000 | Loss: 0.00003355
Iteration 97/1000 | Loss: 0.00003355
Iteration 98/1000 | Loss: 0.00003355
Iteration 99/1000 | Loss: 0.00003354
Iteration 100/1000 | Loss: 0.00003354
Iteration 101/1000 | Loss: 0.00003354
Iteration 102/1000 | Loss: 0.00003354
Iteration 103/1000 | Loss: 0.00003354
Iteration 104/1000 | Loss: 0.00003354
Iteration 105/1000 | Loss: 0.00003354
Iteration 106/1000 | Loss: 0.00003354
Iteration 107/1000 | Loss: 0.00003353
Iteration 108/1000 | Loss: 0.00003353
Iteration 109/1000 | Loss: 0.00003353
Iteration 110/1000 | Loss: 0.00003353
Iteration 111/1000 | Loss: 0.00003353
Iteration 112/1000 | Loss: 0.00003352
Iteration 113/1000 | Loss: 0.00003352
Iteration 114/1000 | Loss: 0.00003352
Iteration 115/1000 | Loss: 0.00003352
Iteration 116/1000 | Loss: 0.00003352
Iteration 117/1000 | Loss: 0.00003351
Iteration 118/1000 | Loss: 0.00003351
Iteration 119/1000 | Loss: 0.00003351
Iteration 120/1000 | Loss: 0.00003351
Iteration 121/1000 | Loss: 0.00003351
Iteration 122/1000 | Loss: 0.00003351
Iteration 123/1000 | Loss: 0.00003351
Iteration 124/1000 | Loss: 0.00003351
Iteration 125/1000 | Loss: 0.00003351
Iteration 126/1000 | Loss: 0.00003350
Iteration 127/1000 | Loss: 0.00003350
Iteration 128/1000 | Loss: 0.00003350
Iteration 129/1000 | Loss: 0.00003350
Iteration 130/1000 | Loss: 0.00003350
Iteration 131/1000 | Loss: 0.00003350
Iteration 132/1000 | Loss: 0.00003350
Iteration 133/1000 | Loss: 0.00003349
Iteration 134/1000 | Loss: 0.00003349
Iteration 135/1000 | Loss: 0.00003349
Iteration 136/1000 | Loss: 0.00003349
Iteration 137/1000 | Loss: 0.00003349
Iteration 138/1000 | Loss: 0.00003349
Iteration 139/1000 | Loss: 0.00003349
Iteration 140/1000 | Loss: 0.00003349
Iteration 141/1000 | Loss: 0.00003349
Iteration 142/1000 | Loss: 0.00003348
Iteration 143/1000 | Loss: 0.00003348
Iteration 144/1000 | Loss: 0.00003348
Iteration 145/1000 | Loss: 0.00003348
Iteration 146/1000 | Loss: 0.00003348
Iteration 147/1000 | Loss: 0.00003348
Iteration 148/1000 | Loss: 0.00003348
Iteration 149/1000 | Loss: 0.00003348
Iteration 150/1000 | Loss: 0.00003348
Iteration 151/1000 | Loss: 0.00003348
Iteration 152/1000 | Loss: 0.00003348
Iteration 153/1000 | Loss: 0.00003348
Iteration 154/1000 | Loss: 0.00003348
Iteration 155/1000 | Loss: 0.00003348
Iteration 156/1000 | Loss: 0.00003347
Iteration 157/1000 | Loss: 0.00003347
Iteration 158/1000 | Loss: 0.00003347
Iteration 159/1000 | Loss: 0.00003347
Iteration 160/1000 | Loss: 0.00003347
Iteration 161/1000 | Loss: 0.00003347
Iteration 162/1000 | Loss: 0.00003347
Iteration 163/1000 | Loss: 0.00003347
Iteration 164/1000 | Loss: 0.00003347
Iteration 165/1000 | Loss: 0.00003347
Iteration 166/1000 | Loss: 0.00003347
Iteration 167/1000 | Loss: 0.00003346
Iteration 168/1000 | Loss: 0.00003346
Iteration 169/1000 | Loss: 0.00003346
Iteration 170/1000 | Loss: 0.00003346
Iteration 171/1000 | Loss: 0.00003346
Iteration 172/1000 | Loss: 0.00003346
Iteration 173/1000 | Loss: 0.00003346
Iteration 174/1000 | Loss: 0.00003346
Iteration 175/1000 | Loss: 0.00003346
Iteration 176/1000 | Loss: 0.00003346
Iteration 177/1000 | Loss: 0.00003346
Iteration 178/1000 | Loss: 0.00003346
Iteration 179/1000 | Loss: 0.00003346
Iteration 180/1000 | Loss: 0.00003346
Iteration 181/1000 | Loss: 0.00003346
Iteration 182/1000 | Loss: 0.00003346
Iteration 183/1000 | Loss: 0.00003346
Iteration 184/1000 | Loss: 0.00003346
Iteration 185/1000 | Loss: 0.00003346
Iteration 186/1000 | Loss: 0.00003346
Iteration 187/1000 | Loss: 0.00003346
Iteration 188/1000 | Loss: 0.00003346
Iteration 189/1000 | Loss: 0.00003346
Iteration 190/1000 | Loss: 0.00003346
Iteration 191/1000 | Loss: 0.00003346
Iteration 192/1000 | Loss: 0.00003346
Iteration 193/1000 | Loss: 0.00003346
Iteration 194/1000 | Loss: 0.00003346
Iteration 195/1000 | Loss: 0.00003346
Iteration 196/1000 | Loss: 0.00003346
Iteration 197/1000 | Loss: 0.00003346
Iteration 198/1000 | Loss: 0.00003346
Iteration 199/1000 | Loss: 0.00003346
Iteration 200/1000 | Loss: 0.00003346
Iteration 201/1000 | Loss: 0.00003346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [3.3457163226557896e-05, 3.3457163226557896e-05, 3.3457163226557896e-05, 3.3457163226557896e-05, 3.3457163226557896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3457163226557896e-05

Optimization complete. Final v2v error: 4.669702053070068 mm

Highest mean error: 5.537328720092773 mm for frame 81

Lowest mean error: 3.894609212875366 mm for frame 12

Saving results

Total time: 51.01935958862305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978147
Iteration 2/25 | Loss: 0.00234147
Iteration 3/25 | Loss: 0.00140484
Iteration 4/25 | Loss: 0.00134162
Iteration 5/25 | Loss: 0.00133702
Iteration 6/25 | Loss: 0.00133700
Iteration 7/25 | Loss: 0.00133700
Iteration 8/25 | Loss: 0.00133700
Iteration 9/25 | Loss: 0.00133700
Iteration 10/25 | Loss: 0.00133700
Iteration 11/25 | Loss: 0.00133700
Iteration 12/25 | Loss: 0.00133700
Iteration 13/25 | Loss: 0.00133700
Iteration 14/25 | Loss: 0.00133700
Iteration 15/25 | Loss: 0.00133700
Iteration 16/25 | Loss: 0.00133700
Iteration 17/25 | Loss: 0.00133700
Iteration 18/25 | Loss: 0.00133700
Iteration 19/25 | Loss: 0.00133700
Iteration 20/25 | Loss: 0.00133700
Iteration 21/25 | Loss: 0.00133700
Iteration 22/25 | Loss: 0.00133700
Iteration 23/25 | Loss: 0.00133700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013369977241382003, 0.0013369977241382003, 0.0013369977241382003, 0.0013369977241382003, 0.0013369977241382003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013369977241382003

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30988300
Iteration 2/25 | Loss: 0.00125463
Iteration 3/25 | Loss: 0.00125463
Iteration 4/25 | Loss: 0.00125463
Iteration 5/25 | Loss: 0.00125463
Iteration 6/25 | Loss: 0.00125463
Iteration 7/25 | Loss: 0.00125463
Iteration 8/25 | Loss: 0.00125463
Iteration 9/25 | Loss: 0.00125463
Iteration 10/25 | Loss: 0.00125463
Iteration 11/25 | Loss: 0.00125463
Iteration 12/25 | Loss: 0.00125463
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001254625734873116, 0.001254625734873116, 0.001254625734873116, 0.001254625734873116, 0.001254625734873116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001254625734873116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125463
Iteration 2/1000 | Loss: 0.00003683
Iteration 3/1000 | Loss: 0.00002521
Iteration 4/1000 | Loss: 0.00001976
Iteration 5/1000 | Loss: 0.00001839
Iteration 6/1000 | Loss: 0.00001761
Iteration 7/1000 | Loss: 0.00001693
Iteration 8/1000 | Loss: 0.00001645
Iteration 9/1000 | Loss: 0.00001615
Iteration 10/1000 | Loss: 0.00001594
Iteration 11/1000 | Loss: 0.00001586
Iteration 12/1000 | Loss: 0.00001581
Iteration 13/1000 | Loss: 0.00001577
Iteration 14/1000 | Loss: 0.00001576
Iteration 15/1000 | Loss: 0.00001575
Iteration 16/1000 | Loss: 0.00001574
Iteration 17/1000 | Loss: 0.00001569
Iteration 18/1000 | Loss: 0.00001567
Iteration 19/1000 | Loss: 0.00001567
Iteration 20/1000 | Loss: 0.00001566
Iteration 21/1000 | Loss: 0.00001566
Iteration 22/1000 | Loss: 0.00001565
Iteration 23/1000 | Loss: 0.00001565
Iteration 24/1000 | Loss: 0.00001564
Iteration 25/1000 | Loss: 0.00001564
Iteration 26/1000 | Loss: 0.00001556
Iteration 27/1000 | Loss: 0.00001555
Iteration 28/1000 | Loss: 0.00001554
Iteration 29/1000 | Loss: 0.00001554
Iteration 30/1000 | Loss: 0.00001553
Iteration 31/1000 | Loss: 0.00001551
Iteration 32/1000 | Loss: 0.00001551
Iteration 33/1000 | Loss: 0.00001549
Iteration 34/1000 | Loss: 0.00001549
Iteration 35/1000 | Loss: 0.00001549
Iteration 36/1000 | Loss: 0.00001549
Iteration 37/1000 | Loss: 0.00001549
Iteration 38/1000 | Loss: 0.00001548
Iteration 39/1000 | Loss: 0.00001548
Iteration 40/1000 | Loss: 0.00001548
Iteration 41/1000 | Loss: 0.00001548
Iteration 42/1000 | Loss: 0.00001548
Iteration 43/1000 | Loss: 0.00001548
Iteration 44/1000 | Loss: 0.00001547
Iteration 45/1000 | Loss: 0.00001547
Iteration 46/1000 | Loss: 0.00001547
Iteration 47/1000 | Loss: 0.00001547
Iteration 48/1000 | Loss: 0.00001547
Iteration 49/1000 | Loss: 0.00001547
Iteration 50/1000 | Loss: 0.00001547
Iteration 51/1000 | Loss: 0.00001547
Iteration 52/1000 | Loss: 0.00001546
Iteration 53/1000 | Loss: 0.00001546
Iteration 54/1000 | Loss: 0.00001546
Iteration 55/1000 | Loss: 0.00001546
Iteration 56/1000 | Loss: 0.00001546
Iteration 57/1000 | Loss: 0.00001546
Iteration 58/1000 | Loss: 0.00001546
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001546
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001545
Iteration 63/1000 | Loss: 0.00001545
Iteration 64/1000 | Loss: 0.00001545
Iteration 65/1000 | Loss: 0.00001545
Iteration 66/1000 | Loss: 0.00001545
Iteration 67/1000 | Loss: 0.00001545
Iteration 68/1000 | Loss: 0.00001545
Iteration 69/1000 | Loss: 0.00001545
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001545
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001544
Iteration 77/1000 | Loss: 0.00001543
Iteration 78/1000 | Loss: 0.00001543
Iteration 79/1000 | Loss: 0.00001542
Iteration 80/1000 | Loss: 0.00001542
Iteration 81/1000 | Loss: 0.00001541
Iteration 82/1000 | Loss: 0.00001541
Iteration 83/1000 | Loss: 0.00001541
Iteration 84/1000 | Loss: 0.00001541
Iteration 85/1000 | Loss: 0.00001540
Iteration 86/1000 | Loss: 0.00001540
Iteration 87/1000 | Loss: 0.00001540
Iteration 88/1000 | Loss: 0.00001540
Iteration 89/1000 | Loss: 0.00001540
Iteration 90/1000 | Loss: 0.00001540
Iteration 91/1000 | Loss: 0.00001540
Iteration 92/1000 | Loss: 0.00001540
Iteration 93/1000 | Loss: 0.00001540
Iteration 94/1000 | Loss: 0.00001540
Iteration 95/1000 | Loss: 0.00001540
Iteration 96/1000 | Loss: 0.00001540
Iteration 97/1000 | Loss: 0.00001540
Iteration 98/1000 | Loss: 0.00001540
Iteration 99/1000 | Loss: 0.00001540
Iteration 100/1000 | Loss: 0.00001540
Iteration 101/1000 | Loss: 0.00001540
Iteration 102/1000 | Loss: 0.00001540
Iteration 103/1000 | Loss: 0.00001540
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001540
Iteration 106/1000 | Loss: 0.00001540
Iteration 107/1000 | Loss: 0.00001540
Iteration 108/1000 | Loss: 0.00001540
Iteration 109/1000 | Loss: 0.00001540
Iteration 110/1000 | Loss: 0.00001540
Iteration 111/1000 | Loss: 0.00001540
Iteration 112/1000 | Loss: 0.00001540
Iteration 113/1000 | Loss: 0.00001540
Iteration 114/1000 | Loss: 0.00001540
Iteration 115/1000 | Loss: 0.00001540
Iteration 116/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.5397550669149496e-05, 1.5397550669149496e-05, 1.5397550669149496e-05, 1.5397550669149496e-05, 1.5397550669149496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5397550669149496e-05

Optimization complete. Final v2v error: 3.275411605834961 mm

Highest mean error: 4.0121073722839355 mm for frame 238

Lowest mean error: 2.833608388900757 mm for frame 181

Saving results

Total time: 36.21939754486084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754476
Iteration 2/25 | Loss: 0.00143724
Iteration 3/25 | Loss: 0.00125461
Iteration 4/25 | Loss: 0.00122882
Iteration 5/25 | Loss: 0.00122391
Iteration 6/25 | Loss: 0.00122330
Iteration 7/25 | Loss: 0.00122330
Iteration 8/25 | Loss: 0.00122330
Iteration 9/25 | Loss: 0.00122330
Iteration 10/25 | Loss: 0.00122330
Iteration 11/25 | Loss: 0.00122330
Iteration 12/25 | Loss: 0.00122330
Iteration 13/25 | Loss: 0.00122330
Iteration 14/25 | Loss: 0.00122330
Iteration 15/25 | Loss: 0.00122330
Iteration 16/25 | Loss: 0.00122330
Iteration 17/25 | Loss: 0.00122330
Iteration 18/25 | Loss: 0.00122330
Iteration 19/25 | Loss: 0.00122330
Iteration 20/25 | Loss: 0.00122330
Iteration 21/25 | Loss: 0.00122330
Iteration 22/25 | Loss: 0.00122330
Iteration 23/25 | Loss: 0.00122330
Iteration 24/25 | Loss: 0.00122330
Iteration 25/25 | Loss: 0.00122330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012232954613864422, 0.0012232954613864422, 0.0012232954613864422, 0.0012232954613864422, 0.0012232954613864422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012232954613864422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30780709
Iteration 2/25 | Loss: 0.00088052
Iteration 3/25 | Loss: 0.00088048
Iteration 4/25 | Loss: 0.00088048
Iteration 5/25 | Loss: 0.00088048
Iteration 6/25 | Loss: 0.00088048
Iteration 7/25 | Loss: 0.00088048
Iteration 8/25 | Loss: 0.00088048
Iteration 9/25 | Loss: 0.00088048
Iteration 10/25 | Loss: 0.00088048
Iteration 11/25 | Loss: 0.00088048
Iteration 12/25 | Loss: 0.00088048
Iteration 13/25 | Loss: 0.00088048
Iteration 14/25 | Loss: 0.00088048
Iteration 15/25 | Loss: 0.00088048
Iteration 16/25 | Loss: 0.00088048
Iteration 17/25 | Loss: 0.00088048
Iteration 18/25 | Loss: 0.00088048
Iteration 19/25 | Loss: 0.00088048
Iteration 20/25 | Loss: 0.00088048
Iteration 21/25 | Loss: 0.00088048
Iteration 22/25 | Loss: 0.00088048
Iteration 23/25 | Loss: 0.00088048
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008804760873317719, 0.0008804760873317719, 0.0008804760873317719, 0.0008804760873317719, 0.0008804760873317719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008804760873317719

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088048
Iteration 2/1000 | Loss: 0.00003301
Iteration 3/1000 | Loss: 0.00002479
Iteration 4/1000 | Loss: 0.00002252
Iteration 5/1000 | Loss: 0.00002135
Iteration 6/1000 | Loss: 0.00002009
Iteration 7/1000 | Loss: 0.00001939
Iteration 8/1000 | Loss: 0.00001885
Iteration 9/1000 | Loss: 0.00001845
Iteration 10/1000 | Loss: 0.00001798
Iteration 11/1000 | Loss: 0.00001772
Iteration 12/1000 | Loss: 0.00001763
Iteration 13/1000 | Loss: 0.00001736
Iteration 14/1000 | Loss: 0.00001726
Iteration 15/1000 | Loss: 0.00001723
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001697
Iteration 18/1000 | Loss: 0.00001677
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001659
Iteration 21/1000 | Loss: 0.00001658
Iteration 22/1000 | Loss: 0.00001651
Iteration 23/1000 | Loss: 0.00001649
Iteration 24/1000 | Loss: 0.00001648
Iteration 25/1000 | Loss: 0.00001644
Iteration 26/1000 | Loss: 0.00001643
Iteration 27/1000 | Loss: 0.00001643
Iteration 28/1000 | Loss: 0.00001643
Iteration 29/1000 | Loss: 0.00001642
Iteration 30/1000 | Loss: 0.00001641
Iteration 31/1000 | Loss: 0.00001641
Iteration 32/1000 | Loss: 0.00001641
Iteration 33/1000 | Loss: 0.00001641
Iteration 34/1000 | Loss: 0.00001641
Iteration 35/1000 | Loss: 0.00001641
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001641
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001641
Iteration 40/1000 | Loss: 0.00001639
Iteration 41/1000 | Loss: 0.00001637
Iteration 42/1000 | Loss: 0.00001637
Iteration 43/1000 | Loss: 0.00001636
Iteration 44/1000 | Loss: 0.00001636
Iteration 45/1000 | Loss: 0.00001636
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001636
Iteration 48/1000 | Loss: 0.00001636
Iteration 49/1000 | Loss: 0.00001636
Iteration 50/1000 | Loss: 0.00001636
Iteration 51/1000 | Loss: 0.00001636
Iteration 52/1000 | Loss: 0.00001635
Iteration 53/1000 | Loss: 0.00001635
Iteration 54/1000 | Loss: 0.00001634
Iteration 55/1000 | Loss: 0.00001634
Iteration 56/1000 | Loss: 0.00001634
Iteration 57/1000 | Loss: 0.00001633
Iteration 58/1000 | Loss: 0.00001633
Iteration 59/1000 | Loss: 0.00001633
Iteration 60/1000 | Loss: 0.00001633
Iteration 61/1000 | Loss: 0.00001632
Iteration 62/1000 | Loss: 0.00001632
Iteration 63/1000 | Loss: 0.00001631
Iteration 64/1000 | Loss: 0.00001631
Iteration 65/1000 | Loss: 0.00001631
Iteration 66/1000 | Loss: 0.00001630
Iteration 67/1000 | Loss: 0.00001630
Iteration 68/1000 | Loss: 0.00001630
Iteration 69/1000 | Loss: 0.00001630
Iteration 70/1000 | Loss: 0.00001630
Iteration 71/1000 | Loss: 0.00001630
Iteration 72/1000 | Loss: 0.00001630
Iteration 73/1000 | Loss: 0.00001629
Iteration 74/1000 | Loss: 0.00001629
Iteration 75/1000 | Loss: 0.00001629
Iteration 76/1000 | Loss: 0.00001628
Iteration 77/1000 | Loss: 0.00001628
Iteration 78/1000 | Loss: 0.00001628
Iteration 79/1000 | Loss: 0.00001628
Iteration 80/1000 | Loss: 0.00001628
Iteration 81/1000 | Loss: 0.00001628
Iteration 82/1000 | Loss: 0.00001628
Iteration 83/1000 | Loss: 0.00001628
Iteration 84/1000 | Loss: 0.00001627
Iteration 85/1000 | Loss: 0.00001626
Iteration 86/1000 | Loss: 0.00001626
Iteration 87/1000 | Loss: 0.00001626
Iteration 88/1000 | Loss: 0.00001626
Iteration 89/1000 | Loss: 0.00001626
Iteration 90/1000 | Loss: 0.00001626
Iteration 91/1000 | Loss: 0.00001626
Iteration 92/1000 | Loss: 0.00001626
Iteration 93/1000 | Loss: 0.00001625
Iteration 94/1000 | Loss: 0.00001625
Iteration 95/1000 | Loss: 0.00001625
Iteration 96/1000 | Loss: 0.00001625
Iteration 97/1000 | Loss: 0.00001625
Iteration 98/1000 | Loss: 0.00001624
Iteration 99/1000 | Loss: 0.00001624
Iteration 100/1000 | Loss: 0.00001624
Iteration 101/1000 | Loss: 0.00001623
Iteration 102/1000 | Loss: 0.00001623
Iteration 103/1000 | Loss: 0.00001623
Iteration 104/1000 | Loss: 0.00001623
Iteration 105/1000 | Loss: 0.00001623
Iteration 106/1000 | Loss: 0.00001623
Iteration 107/1000 | Loss: 0.00001622
Iteration 108/1000 | Loss: 0.00001622
Iteration 109/1000 | Loss: 0.00001622
Iteration 110/1000 | Loss: 0.00001622
Iteration 111/1000 | Loss: 0.00001622
Iteration 112/1000 | Loss: 0.00001622
Iteration 113/1000 | Loss: 0.00001622
Iteration 114/1000 | Loss: 0.00001622
Iteration 115/1000 | Loss: 0.00001622
Iteration 116/1000 | Loss: 0.00001621
Iteration 117/1000 | Loss: 0.00001621
Iteration 118/1000 | Loss: 0.00001621
Iteration 119/1000 | Loss: 0.00001621
Iteration 120/1000 | Loss: 0.00001621
Iteration 121/1000 | Loss: 0.00001621
Iteration 122/1000 | Loss: 0.00001621
Iteration 123/1000 | Loss: 0.00001620
Iteration 124/1000 | Loss: 0.00001620
Iteration 125/1000 | Loss: 0.00001620
Iteration 126/1000 | Loss: 0.00001620
Iteration 127/1000 | Loss: 0.00001620
Iteration 128/1000 | Loss: 0.00001620
Iteration 129/1000 | Loss: 0.00001620
Iteration 130/1000 | Loss: 0.00001620
Iteration 131/1000 | Loss: 0.00001620
Iteration 132/1000 | Loss: 0.00001619
Iteration 133/1000 | Loss: 0.00001619
Iteration 134/1000 | Loss: 0.00001619
Iteration 135/1000 | Loss: 0.00001619
Iteration 136/1000 | Loss: 0.00001619
Iteration 137/1000 | Loss: 0.00001619
Iteration 138/1000 | Loss: 0.00001619
Iteration 139/1000 | Loss: 0.00001619
Iteration 140/1000 | Loss: 0.00001619
Iteration 141/1000 | Loss: 0.00001619
Iteration 142/1000 | Loss: 0.00001619
Iteration 143/1000 | Loss: 0.00001619
Iteration 144/1000 | Loss: 0.00001619
Iteration 145/1000 | Loss: 0.00001619
Iteration 146/1000 | Loss: 0.00001619
Iteration 147/1000 | Loss: 0.00001619
Iteration 148/1000 | Loss: 0.00001619
Iteration 149/1000 | Loss: 0.00001619
Iteration 150/1000 | Loss: 0.00001618
Iteration 151/1000 | Loss: 0.00001618
Iteration 152/1000 | Loss: 0.00001618
Iteration 153/1000 | Loss: 0.00001618
Iteration 154/1000 | Loss: 0.00001618
Iteration 155/1000 | Loss: 0.00001618
Iteration 156/1000 | Loss: 0.00001618
Iteration 157/1000 | Loss: 0.00001618
Iteration 158/1000 | Loss: 0.00001618
Iteration 159/1000 | Loss: 0.00001618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.6184318155865185e-05, 1.6184318155865185e-05, 1.6184318155865185e-05, 1.6184318155865185e-05, 1.6184318155865185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6184318155865185e-05

Optimization complete. Final v2v error: 3.451709747314453 mm

Highest mean error: 3.764420509338379 mm for frame 141

Lowest mean error: 3.2133235931396484 mm for frame 102

Saving results

Total time: 48.097243547439575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389852
Iteration 2/25 | Loss: 0.00130298
Iteration 3/25 | Loss: 0.00122719
Iteration 4/25 | Loss: 0.00122062
Iteration 5/25 | Loss: 0.00121840
Iteration 6/25 | Loss: 0.00121840
Iteration 7/25 | Loss: 0.00121840
Iteration 8/25 | Loss: 0.00121840
Iteration 9/25 | Loss: 0.00121840
Iteration 10/25 | Loss: 0.00121840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001218396588228643, 0.001218396588228643, 0.001218396588228643, 0.001218396588228643, 0.001218396588228643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001218396588228643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63590455
Iteration 2/25 | Loss: 0.00091879
Iteration 3/25 | Loss: 0.00091875
Iteration 4/25 | Loss: 0.00091875
Iteration 5/25 | Loss: 0.00091875
Iteration 6/25 | Loss: 0.00091875
Iteration 7/25 | Loss: 0.00091875
Iteration 8/25 | Loss: 0.00091875
Iteration 9/25 | Loss: 0.00091875
Iteration 10/25 | Loss: 0.00091875
Iteration 11/25 | Loss: 0.00091875
Iteration 12/25 | Loss: 0.00091875
Iteration 13/25 | Loss: 0.00091875
Iteration 14/25 | Loss: 0.00091875
Iteration 15/25 | Loss: 0.00091875
Iteration 16/25 | Loss: 0.00091875
Iteration 17/25 | Loss: 0.00091875
Iteration 18/25 | Loss: 0.00091875
Iteration 19/25 | Loss: 0.00091875
Iteration 20/25 | Loss: 0.00091875
Iteration 21/25 | Loss: 0.00091875
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009187498362734914, 0.0009187498362734914, 0.0009187498362734914, 0.0009187498362734914, 0.0009187498362734914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009187498362734914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091875
Iteration 2/1000 | Loss: 0.00003012
Iteration 3/1000 | Loss: 0.00001918
Iteration 4/1000 | Loss: 0.00001526
Iteration 5/1000 | Loss: 0.00001409
Iteration 6/1000 | Loss: 0.00001315
Iteration 7/1000 | Loss: 0.00001268
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001200
Iteration 10/1000 | Loss: 0.00001158
Iteration 11/1000 | Loss: 0.00001142
Iteration 12/1000 | Loss: 0.00001141
Iteration 13/1000 | Loss: 0.00001141
Iteration 14/1000 | Loss: 0.00001140
Iteration 15/1000 | Loss: 0.00001137
Iteration 16/1000 | Loss: 0.00001131
Iteration 17/1000 | Loss: 0.00001120
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001113
Iteration 20/1000 | Loss: 0.00001108
Iteration 21/1000 | Loss: 0.00001105
Iteration 22/1000 | Loss: 0.00001104
Iteration 23/1000 | Loss: 0.00001099
Iteration 24/1000 | Loss: 0.00001098
Iteration 25/1000 | Loss: 0.00001098
Iteration 26/1000 | Loss: 0.00001097
Iteration 27/1000 | Loss: 0.00001097
Iteration 28/1000 | Loss: 0.00001096
Iteration 29/1000 | Loss: 0.00001096
Iteration 30/1000 | Loss: 0.00001096
Iteration 31/1000 | Loss: 0.00001096
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001094
Iteration 34/1000 | Loss: 0.00001094
Iteration 35/1000 | Loss: 0.00001093
Iteration 36/1000 | Loss: 0.00001092
Iteration 37/1000 | Loss: 0.00001090
Iteration 38/1000 | Loss: 0.00001090
Iteration 39/1000 | Loss: 0.00001089
Iteration 40/1000 | Loss: 0.00001088
Iteration 41/1000 | Loss: 0.00001087
Iteration 42/1000 | Loss: 0.00001086
Iteration 43/1000 | Loss: 0.00001085
Iteration 44/1000 | Loss: 0.00001085
Iteration 45/1000 | Loss: 0.00001084
Iteration 46/1000 | Loss: 0.00001084
Iteration 47/1000 | Loss: 0.00001081
Iteration 48/1000 | Loss: 0.00001081
Iteration 49/1000 | Loss: 0.00001081
Iteration 50/1000 | Loss: 0.00001081
Iteration 51/1000 | Loss: 0.00001081
Iteration 52/1000 | Loss: 0.00001081
Iteration 53/1000 | Loss: 0.00001081
Iteration 54/1000 | Loss: 0.00001080
Iteration 55/1000 | Loss: 0.00001079
Iteration 56/1000 | Loss: 0.00001079
Iteration 57/1000 | Loss: 0.00001078
Iteration 58/1000 | Loss: 0.00001077
Iteration 59/1000 | Loss: 0.00001077
Iteration 60/1000 | Loss: 0.00001077
Iteration 61/1000 | Loss: 0.00001077
Iteration 62/1000 | Loss: 0.00001077
Iteration 63/1000 | Loss: 0.00001077
Iteration 64/1000 | Loss: 0.00001077
Iteration 65/1000 | Loss: 0.00001077
Iteration 66/1000 | Loss: 0.00001077
Iteration 67/1000 | Loss: 0.00001077
Iteration 68/1000 | Loss: 0.00001076
Iteration 69/1000 | Loss: 0.00001076
Iteration 70/1000 | Loss: 0.00001076
Iteration 71/1000 | Loss: 0.00001076
Iteration 72/1000 | Loss: 0.00001075
Iteration 73/1000 | Loss: 0.00001075
Iteration 74/1000 | Loss: 0.00001075
Iteration 75/1000 | Loss: 0.00001074
Iteration 76/1000 | Loss: 0.00001074
Iteration 77/1000 | Loss: 0.00001073
Iteration 78/1000 | Loss: 0.00001073
Iteration 79/1000 | Loss: 0.00001072
Iteration 80/1000 | Loss: 0.00001072
Iteration 81/1000 | Loss: 0.00001072
Iteration 82/1000 | Loss: 0.00001071
Iteration 83/1000 | Loss: 0.00001070
Iteration 84/1000 | Loss: 0.00001069
Iteration 85/1000 | Loss: 0.00001069
Iteration 86/1000 | Loss: 0.00001069
Iteration 87/1000 | Loss: 0.00001069
Iteration 88/1000 | Loss: 0.00001069
Iteration 89/1000 | Loss: 0.00001069
Iteration 90/1000 | Loss: 0.00001068
Iteration 91/1000 | Loss: 0.00001068
Iteration 92/1000 | Loss: 0.00001068
Iteration 93/1000 | Loss: 0.00001068
Iteration 94/1000 | Loss: 0.00001068
Iteration 95/1000 | Loss: 0.00001068
Iteration 96/1000 | Loss: 0.00001068
Iteration 97/1000 | Loss: 0.00001068
Iteration 98/1000 | Loss: 0.00001068
Iteration 99/1000 | Loss: 0.00001068
Iteration 100/1000 | Loss: 0.00001068
Iteration 101/1000 | Loss: 0.00001068
Iteration 102/1000 | Loss: 0.00001067
Iteration 103/1000 | Loss: 0.00001067
Iteration 104/1000 | Loss: 0.00001066
Iteration 105/1000 | Loss: 0.00001066
Iteration 106/1000 | Loss: 0.00001066
Iteration 107/1000 | Loss: 0.00001066
Iteration 108/1000 | Loss: 0.00001066
Iteration 109/1000 | Loss: 0.00001065
Iteration 110/1000 | Loss: 0.00001065
Iteration 111/1000 | Loss: 0.00001065
Iteration 112/1000 | Loss: 0.00001065
Iteration 113/1000 | Loss: 0.00001065
Iteration 114/1000 | Loss: 0.00001064
Iteration 115/1000 | Loss: 0.00001064
Iteration 116/1000 | Loss: 0.00001064
Iteration 117/1000 | Loss: 0.00001064
Iteration 118/1000 | Loss: 0.00001064
Iteration 119/1000 | Loss: 0.00001064
Iteration 120/1000 | Loss: 0.00001063
Iteration 121/1000 | Loss: 0.00001063
Iteration 122/1000 | Loss: 0.00001063
Iteration 123/1000 | Loss: 0.00001063
Iteration 124/1000 | Loss: 0.00001062
Iteration 125/1000 | Loss: 0.00001062
Iteration 126/1000 | Loss: 0.00001061
Iteration 127/1000 | Loss: 0.00001061
Iteration 128/1000 | Loss: 0.00001061
Iteration 129/1000 | Loss: 0.00001061
Iteration 130/1000 | Loss: 0.00001060
Iteration 131/1000 | Loss: 0.00001060
Iteration 132/1000 | Loss: 0.00001060
Iteration 133/1000 | Loss: 0.00001060
Iteration 134/1000 | Loss: 0.00001060
Iteration 135/1000 | Loss: 0.00001060
Iteration 136/1000 | Loss: 0.00001059
Iteration 137/1000 | Loss: 0.00001059
Iteration 138/1000 | Loss: 0.00001059
Iteration 139/1000 | Loss: 0.00001058
Iteration 140/1000 | Loss: 0.00001058
Iteration 141/1000 | Loss: 0.00001058
Iteration 142/1000 | Loss: 0.00001057
Iteration 143/1000 | Loss: 0.00001057
Iteration 144/1000 | Loss: 0.00001057
Iteration 145/1000 | Loss: 0.00001056
Iteration 146/1000 | Loss: 0.00001056
Iteration 147/1000 | Loss: 0.00001056
Iteration 148/1000 | Loss: 0.00001055
Iteration 149/1000 | Loss: 0.00001055
Iteration 150/1000 | Loss: 0.00001054
Iteration 151/1000 | Loss: 0.00001054
Iteration 152/1000 | Loss: 0.00001054
Iteration 153/1000 | Loss: 0.00001054
Iteration 154/1000 | Loss: 0.00001054
Iteration 155/1000 | Loss: 0.00001054
Iteration 156/1000 | Loss: 0.00001054
Iteration 157/1000 | Loss: 0.00001054
Iteration 158/1000 | Loss: 0.00001054
Iteration 159/1000 | Loss: 0.00001054
Iteration 160/1000 | Loss: 0.00001054
Iteration 161/1000 | Loss: 0.00001054
Iteration 162/1000 | Loss: 0.00001054
Iteration 163/1000 | Loss: 0.00001054
Iteration 164/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.053824871632969e-05, 1.053824871632969e-05, 1.053824871632969e-05, 1.053824871632969e-05, 1.053824871632969e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.053824871632969e-05

Optimization complete. Final v2v error: 2.7675297260284424 mm

Highest mean error: 3.1308984756469727 mm for frame 164

Lowest mean error: 2.5193982124328613 mm for frame 192

Saving results

Total time: 45.86455416679382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827586
Iteration 2/25 | Loss: 0.00132043
Iteration 3/25 | Loss: 0.00122945
Iteration 4/25 | Loss: 0.00122043
Iteration 5/25 | Loss: 0.00121881
Iteration 6/25 | Loss: 0.00121881
Iteration 7/25 | Loss: 0.00121881
Iteration 8/25 | Loss: 0.00121881
Iteration 9/25 | Loss: 0.00121881
Iteration 10/25 | Loss: 0.00121881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012188147520646453, 0.0012188147520646453, 0.0012188147520646453, 0.0012188147520646453, 0.0012188147520646453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012188147520646453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33936095
Iteration 2/25 | Loss: 0.00089909
Iteration 3/25 | Loss: 0.00089907
Iteration 4/25 | Loss: 0.00089907
Iteration 5/25 | Loss: 0.00089906
Iteration 6/25 | Loss: 0.00089906
Iteration 7/25 | Loss: 0.00089906
Iteration 8/25 | Loss: 0.00089906
Iteration 9/25 | Loss: 0.00089906
Iteration 10/25 | Loss: 0.00089906
Iteration 11/25 | Loss: 0.00089906
Iteration 12/25 | Loss: 0.00089906
Iteration 13/25 | Loss: 0.00089906
Iteration 14/25 | Loss: 0.00089906
Iteration 15/25 | Loss: 0.00089906
Iteration 16/25 | Loss: 0.00089906
Iteration 17/25 | Loss: 0.00089906
Iteration 18/25 | Loss: 0.00089906
Iteration 19/25 | Loss: 0.00089906
Iteration 20/25 | Loss: 0.00089906
Iteration 21/25 | Loss: 0.00089906
Iteration 22/25 | Loss: 0.00089906
Iteration 23/25 | Loss: 0.00089906
Iteration 24/25 | Loss: 0.00089906
Iteration 25/25 | Loss: 0.00089906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089906
Iteration 2/1000 | Loss: 0.00002095
Iteration 3/1000 | Loss: 0.00001551
Iteration 4/1000 | Loss: 0.00001329
Iteration 5/1000 | Loss: 0.00001224
Iteration 6/1000 | Loss: 0.00001152
Iteration 7/1000 | Loss: 0.00001117
Iteration 8/1000 | Loss: 0.00001071
Iteration 9/1000 | Loss: 0.00001058
Iteration 10/1000 | Loss: 0.00001057
Iteration 11/1000 | Loss: 0.00001047
Iteration 12/1000 | Loss: 0.00001022
Iteration 13/1000 | Loss: 0.00001022
Iteration 14/1000 | Loss: 0.00001021
Iteration 15/1000 | Loss: 0.00001019
Iteration 16/1000 | Loss: 0.00001014
Iteration 17/1000 | Loss: 0.00001013
Iteration 18/1000 | Loss: 0.00001012
Iteration 19/1000 | Loss: 0.00001011
Iteration 20/1000 | Loss: 0.00001008
Iteration 21/1000 | Loss: 0.00001007
Iteration 22/1000 | Loss: 0.00001005
Iteration 23/1000 | Loss: 0.00001005
Iteration 24/1000 | Loss: 0.00001004
Iteration 25/1000 | Loss: 0.00001003
Iteration 26/1000 | Loss: 0.00000997
Iteration 27/1000 | Loss: 0.00000987
Iteration 28/1000 | Loss: 0.00000985
Iteration 29/1000 | Loss: 0.00000984
Iteration 30/1000 | Loss: 0.00000984
Iteration 31/1000 | Loss: 0.00000978
Iteration 32/1000 | Loss: 0.00000978
Iteration 33/1000 | Loss: 0.00000977
Iteration 34/1000 | Loss: 0.00000976
Iteration 35/1000 | Loss: 0.00000976
Iteration 36/1000 | Loss: 0.00000975
Iteration 37/1000 | Loss: 0.00000975
Iteration 38/1000 | Loss: 0.00000974
Iteration 39/1000 | Loss: 0.00000974
Iteration 40/1000 | Loss: 0.00000974
Iteration 41/1000 | Loss: 0.00000974
Iteration 42/1000 | Loss: 0.00000973
Iteration 43/1000 | Loss: 0.00000973
Iteration 44/1000 | Loss: 0.00000973
Iteration 45/1000 | Loss: 0.00000973
Iteration 46/1000 | Loss: 0.00000973
Iteration 47/1000 | Loss: 0.00000972
Iteration 48/1000 | Loss: 0.00000972
Iteration 49/1000 | Loss: 0.00000972
Iteration 50/1000 | Loss: 0.00000972
Iteration 51/1000 | Loss: 0.00000972
Iteration 52/1000 | Loss: 0.00000971
Iteration 53/1000 | Loss: 0.00000971
Iteration 54/1000 | Loss: 0.00000971
Iteration 55/1000 | Loss: 0.00000971
Iteration 56/1000 | Loss: 0.00000970
Iteration 57/1000 | Loss: 0.00000970
Iteration 58/1000 | Loss: 0.00000970
Iteration 59/1000 | Loss: 0.00000970
Iteration 60/1000 | Loss: 0.00000970
Iteration 61/1000 | Loss: 0.00000969
Iteration 62/1000 | Loss: 0.00000968
Iteration 63/1000 | Loss: 0.00000968
Iteration 64/1000 | Loss: 0.00000968
Iteration 65/1000 | Loss: 0.00000967
Iteration 66/1000 | Loss: 0.00000967
Iteration 67/1000 | Loss: 0.00000967
Iteration 68/1000 | Loss: 0.00000967
Iteration 69/1000 | Loss: 0.00000966
Iteration 70/1000 | Loss: 0.00000966
Iteration 71/1000 | Loss: 0.00000966
Iteration 72/1000 | Loss: 0.00000966
Iteration 73/1000 | Loss: 0.00000966
Iteration 74/1000 | Loss: 0.00000965
Iteration 75/1000 | Loss: 0.00000964
Iteration 76/1000 | Loss: 0.00000964
Iteration 77/1000 | Loss: 0.00000963
Iteration 78/1000 | Loss: 0.00000963
Iteration 79/1000 | Loss: 0.00000963
Iteration 80/1000 | Loss: 0.00000963
Iteration 81/1000 | Loss: 0.00000963
Iteration 82/1000 | Loss: 0.00000963
Iteration 83/1000 | Loss: 0.00000963
Iteration 84/1000 | Loss: 0.00000963
Iteration 85/1000 | Loss: 0.00000963
Iteration 86/1000 | Loss: 0.00000963
Iteration 87/1000 | Loss: 0.00000962
Iteration 88/1000 | Loss: 0.00000962
Iteration 89/1000 | Loss: 0.00000962
Iteration 90/1000 | Loss: 0.00000961
Iteration 91/1000 | Loss: 0.00000961
Iteration 92/1000 | Loss: 0.00000960
Iteration 93/1000 | Loss: 0.00000960
Iteration 94/1000 | Loss: 0.00000959
Iteration 95/1000 | Loss: 0.00000959
Iteration 96/1000 | Loss: 0.00000959
Iteration 97/1000 | Loss: 0.00000959
Iteration 98/1000 | Loss: 0.00000959
Iteration 99/1000 | Loss: 0.00000959
Iteration 100/1000 | Loss: 0.00000959
Iteration 101/1000 | Loss: 0.00000958
Iteration 102/1000 | Loss: 0.00000958
Iteration 103/1000 | Loss: 0.00000958
Iteration 104/1000 | Loss: 0.00000958
Iteration 105/1000 | Loss: 0.00000958
Iteration 106/1000 | Loss: 0.00000958
Iteration 107/1000 | Loss: 0.00000958
Iteration 108/1000 | Loss: 0.00000958
Iteration 109/1000 | Loss: 0.00000958
Iteration 110/1000 | Loss: 0.00000958
Iteration 111/1000 | Loss: 0.00000957
Iteration 112/1000 | Loss: 0.00000956
Iteration 113/1000 | Loss: 0.00000955
Iteration 114/1000 | Loss: 0.00000955
Iteration 115/1000 | Loss: 0.00000955
Iteration 116/1000 | Loss: 0.00000955
Iteration 117/1000 | Loss: 0.00000955
Iteration 118/1000 | Loss: 0.00000955
Iteration 119/1000 | Loss: 0.00000955
Iteration 120/1000 | Loss: 0.00000954
Iteration 121/1000 | Loss: 0.00000954
Iteration 122/1000 | Loss: 0.00000953
Iteration 123/1000 | Loss: 0.00000952
Iteration 124/1000 | Loss: 0.00000952
Iteration 125/1000 | Loss: 0.00000952
Iteration 126/1000 | Loss: 0.00000951
Iteration 127/1000 | Loss: 0.00000951
Iteration 128/1000 | Loss: 0.00000951
Iteration 129/1000 | Loss: 0.00000950
Iteration 130/1000 | Loss: 0.00000950
Iteration 131/1000 | Loss: 0.00000950
Iteration 132/1000 | Loss: 0.00000949
Iteration 133/1000 | Loss: 0.00000949
Iteration 134/1000 | Loss: 0.00000949
Iteration 135/1000 | Loss: 0.00000949
Iteration 136/1000 | Loss: 0.00000948
Iteration 137/1000 | Loss: 0.00000948
Iteration 138/1000 | Loss: 0.00000948
Iteration 139/1000 | Loss: 0.00000948
Iteration 140/1000 | Loss: 0.00000948
Iteration 141/1000 | Loss: 0.00000948
Iteration 142/1000 | Loss: 0.00000947
Iteration 143/1000 | Loss: 0.00000947
Iteration 144/1000 | Loss: 0.00000947
Iteration 145/1000 | Loss: 0.00000947
Iteration 146/1000 | Loss: 0.00000947
Iteration 147/1000 | Loss: 0.00000947
Iteration 148/1000 | Loss: 0.00000947
Iteration 149/1000 | Loss: 0.00000947
Iteration 150/1000 | Loss: 0.00000947
Iteration 151/1000 | Loss: 0.00000947
Iteration 152/1000 | Loss: 0.00000947
Iteration 153/1000 | Loss: 0.00000947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [9.469856195210014e-06, 9.469856195210014e-06, 9.469856195210014e-06, 9.469856195210014e-06, 9.469856195210014e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.469856195210014e-06

Optimization complete. Final v2v error: 2.6540539264678955 mm

Highest mean error: 2.875619888305664 mm for frame 29

Lowest mean error: 2.521073579788208 mm for frame 105

Saving results

Total time: 37.95739936828613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873235
Iteration 2/25 | Loss: 0.00150008
Iteration 3/25 | Loss: 0.00132355
Iteration 4/25 | Loss: 0.00130195
Iteration 5/25 | Loss: 0.00129723
Iteration 6/25 | Loss: 0.00129723
Iteration 7/25 | Loss: 0.00129723
Iteration 8/25 | Loss: 0.00129723
Iteration 9/25 | Loss: 0.00129723
Iteration 10/25 | Loss: 0.00129723
Iteration 11/25 | Loss: 0.00129723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012972303666174412, 0.0012972303666174412, 0.0012972303666174412, 0.0012972303666174412, 0.0012972303666174412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012972303666174412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17238808
Iteration 2/25 | Loss: 0.00110375
Iteration 3/25 | Loss: 0.00110375
Iteration 4/25 | Loss: 0.00110374
Iteration 5/25 | Loss: 0.00110374
Iteration 6/25 | Loss: 0.00110374
Iteration 7/25 | Loss: 0.00110374
Iteration 8/25 | Loss: 0.00110374
Iteration 9/25 | Loss: 0.00110374
Iteration 10/25 | Loss: 0.00110374
Iteration 11/25 | Loss: 0.00110374
Iteration 12/25 | Loss: 0.00110374
Iteration 13/25 | Loss: 0.00110374
Iteration 14/25 | Loss: 0.00110374
Iteration 15/25 | Loss: 0.00110374
Iteration 16/25 | Loss: 0.00110374
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011037426302209496, 0.0011037426302209496, 0.0011037426302209496, 0.0011037426302209496, 0.0011037426302209496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011037426302209496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110374
Iteration 2/1000 | Loss: 0.00007059
Iteration 3/1000 | Loss: 0.00004032
Iteration 4/1000 | Loss: 0.00003254
Iteration 5/1000 | Loss: 0.00002978
Iteration 6/1000 | Loss: 0.00002793
Iteration 7/1000 | Loss: 0.00002641
Iteration 8/1000 | Loss: 0.00002566
Iteration 9/1000 | Loss: 0.00002502
Iteration 10/1000 | Loss: 0.00002461
Iteration 11/1000 | Loss: 0.00002430
Iteration 12/1000 | Loss: 0.00002404
Iteration 13/1000 | Loss: 0.00002383
Iteration 14/1000 | Loss: 0.00002361
Iteration 15/1000 | Loss: 0.00002345
Iteration 16/1000 | Loss: 0.00002342
Iteration 17/1000 | Loss: 0.00002337
Iteration 18/1000 | Loss: 0.00002333
Iteration 19/1000 | Loss: 0.00002326
Iteration 20/1000 | Loss: 0.00002326
Iteration 21/1000 | Loss: 0.00002322
Iteration 22/1000 | Loss: 0.00002321
Iteration 23/1000 | Loss: 0.00002320
Iteration 24/1000 | Loss: 0.00002319
Iteration 25/1000 | Loss: 0.00002319
Iteration 26/1000 | Loss: 0.00002318
Iteration 27/1000 | Loss: 0.00002318
Iteration 28/1000 | Loss: 0.00002317
Iteration 29/1000 | Loss: 0.00002317
Iteration 30/1000 | Loss: 0.00002317
Iteration 31/1000 | Loss: 0.00002316
Iteration 32/1000 | Loss: 0.00002315
Iteration 33/1000 | Loss: 0.00002315
Iteration 34/1000 | Loss: 0.00002314
Iteration 35/1000 | Loss: 0.00002314
Iteration 36/1000 | Loss: 0.00002314
Iteration 37/1000 | Loss: 0.00002313
Iteration 38/1000 | Loss: 0.00002313
Iteration 39/1000 | Loss: 0.00002312
Iteration 40/1000 | Loss: 0.00002312
Iteration 41/1000 | Loss: 0.00002311
Iteration 42/1000 | Loss: 0.00002311
Iteration 43/1000 | Loss: 0.00002310
Iteration 44/1000 | Loss: 0.00002309
Iteration 45/1000 | Loss: 0.00002309
Iteration 46/1000 | Loss: 0.00002308
Iteration 47/1000 | Loss: 0.00002308
Iteration 48/1000 | Loss: 0.00002308
Iteration 49/1000 | Loss: 0.00002307
Iteration 50/1000 | Loss: 0.00002307
Iteration 51/1000 | Loss: 0.00002306
Iteration 52/1000 | Loss: 0.00002305
Iteration 53/1000 | Loss: 0.00002305
Iteration 54/1000 | Loss: 0.00002305
Iteration 55/1000 | Loss: 0.00002305
Iteration 56/1000 | Loss: 0.00002305
Iteration 57/1000 | Loss: 0.00002305
Iteration 58/1000 | Loss: 0.00002304
Iteration 59/1000 | Loss: 0.00002304
Iteration 60/1000 | Loss: 0.00002303
Iteration 61/1000 | Loss: 0.00002303
Iteration 62/1000 | Loss: 0.00002302
Iteration 63/1000 | Loss: 0.00002302
Iteration 64/1000 | Loss: 0.00002301
Iteration 65/1000 | Loss: 0.00002301
Iteration 66/1000 | Loss: 0.00002301
Iteration 67/1000 | Loss: 0.00002300
Iteration 68/1000 | Loss: 0.00002299
Iteration 69/1000 | Loss: 0.00002299
Iteration 70/1000 | Loss: 0.00002299
Iteration 71/1000 | Loss: 0.00002299
Iteration 72/1000 | Loss: 0.00002299
Iteration 73/1000 | Loss: 0.00002298
Iteration 74/1000 | Loss: 0.00002298
Iteration 75/1000 | Loss: 0.00002298
Iteration 76/1000 | Loss: 0.00002297
Iteration 77/1000 | Loss: 0.00002297
Iteration 78/1000 | Loss: 0.00002297
Iteration 79/1000 | Loss: 0.00002297
Iteration 80/1000 | Loss: 0.00002296
Iteration 81/1000 | Loss: 0.00002296
Iteration 82/1000 | Loss: 0.00002296
Iteration 83/1000 | Loss: 0.00002295
Iteration 84/1000 | Loss: 0.00002295
Iteration 85/1000 | Loss: 0.00002295
Iteration 86/1000 | Loss: 0.00002294
Iteration 87/1000 | Loss: 0.00002294
Iteration 88/1000 | Loss: 0.00002294
Iteration 89/1000 | Loss: 0.00002294
Iteration 90/1000 | Loss: 0.00002293
Iteration 91/1000 | Loss: 0.00002293
Iteration 92/1000 | Loss: 0.00002293
Iteration 93/1000 | Loss: 0.00002292
Iteration 94/1000 | Loss: 0.00002292
Iteration 95/1000 | Loss: 0.00002292
Iteration 96/1000 | Loss: 0.00002292
Iteration 97/1000 | Loss: 0.00002292
Iteration 98/1000 | Loss: 0.00002291
Iteration 99/1000 | Loss: 0.00002291
Iteration 100/1000 | Loss: 0.00002291
Iteration 101/1000 | Loss: 0.00002290
Iteration 102/1000 | Loss: 0.00002290
Iteration 103/1000 | Loss: 0.00002290
Iteration 104/1000 | Loss: 0.00002290
Iteration 105/1000 | Loss: 0.00002290
Iteration 106/1000 | Loss: 0.00002290
Iteration 107/1000 | Loss: 0.00002290
Iteration 108/1000 | Loss: 0.00002290
Iteration 109/1000 | Loss: 0.00002290
Iteration 110/1000 | Loss: 0.00002290
Iteration 111/1000 | Loss: 0.00002290
Iteration 112/1000 | Loss: 0.00002290
Iteration 113/1000 | Loss: 0.00002290
Iteration 114/1000 | Loss: 0.00002290
Iteration 115/1000 | Loss: 0.00002290
Iteration 116/1000 | Loss: 0.00002290
Iteration 117/1000 | Loss: 0.00002290
Iteration 118/1000 | Loss: 0.00002290
Iteration 119/1000 | Loss: 0.00002290
Iteration 120/1000 | Loss: 0.00002290
Iteration 121/1000 | Loss: 0.00002290
Iteration 122/1000 | Loss: 0.00002290
Iteration 123/1000 | Loss: 0.00002290
Iteration 124/1000 | Loss: 0.00002290
Iteration 125/1000 | Loss: 0.00002290
Iteration 126/1000 | Loss: 0.00002290
Iteration 127/1000 | Loss: 0.00002290
Iteration 128/1000 | Loss: 0.00002290
Iteration 129/1000 | Loss: 0.00002290
Iteration 130/1000 | Loss: 0.00002290
Iteration 131/1000 | Loss: 0.00002290
Iteration 132/1000 | Loss: 0.00002290
Iteration 133/1000 | Loss: 0.00002290
Iteration 134/1000 | Loss: 0.00002290
Iteration 135/1000 | Loss: 0.00002290
Iteration 136/1000 | Loss: 0.00002290
Iteration 137/1000 | Loss: 0.00002290
Iteration 138/1000 | Loss: 0.00002290
Iteration 139/1000 | Loss: 0.00002290
Iteration 140/1000 | Loss: 0.00002290
Iteration 141/1000 | Loss: 0.00002290
Iteration 142/1000 | Loss: 0.00002290
Iteration 143/1000 | Loss: 0.00002290
Iteration 144/1000 | Loss: 0.00002290
Iteration 145/1000 | Loss: 0.00002290
Iteration 146/1000 | Loss: 0.00002290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.289799704158213e-05, 2.289799704158213e-05, 2.289799704158213e-05, 2.289799704158213e-05, 2.289799704158213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.289799704158213e-05

Optimization complete. Final v2v error: 3.964646339416504 mm

Highest mean error: 4.327547073364258 mm for frame 189

Lowest mean error: 3.1588451862335205 mm for frame 238

Saving results

Total time: 45.77116012573242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973708
Iteration 2/25 | Loss: 0.00190050
Iteration 3/25 | Loss: 0.00144577
Iteration 4/25 | Loss: 0.00136910
Iteration 5/25 | Loss: 0.00134137
Iteration 6/25 | Loss: 0.00135308
Iteration 7/25 | Loss: 0.00130110
Iteration 8/25 | Loss: 0.00128711
Iteration 9/25 | Loss: 0.00128403
Iteration 10/25 | Loss: 0.00128286
Iteration 11/25 | Loss: 0.00128218
Iteration 12/25 | Loss: 0.00128168
Iteration 13/25 | Loss: 0.00128695
Iteration 14/25 | Loss: 0.00128272
Iteration 15/25 | Loss: 0.00128134
Iteration 16/25 | Loss: 0.00128367
Iteration 17/25 | Loss: 0.00128510
Iteration 18/25 | Loss: 0.00128874
Iteration 19/25 | Loss: 0.00128341
Iteration 20/25 | Loss: 0.00127911
Iteration 21/25 | Loss: 0.00127911
Iteration 22/25 | Loss: 0.00127835
Iteration 23/25 | Loss: 0.00127696
Iteration 24/25 | Loss: 0.00127598
Iteration 25/25 | Loss: 0.00127534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35368526
Iteration 2/25 | Loss: 0.00127987
Iteration 3/25 | Loss: 0.00127085
Iteration 4/25 | Loss: 0.00127085
Iteration 5/25 | Loss: 0.00127085
Iteration 6/25 | Loss: 0.00127085
Iteration 7/25 | Loss: 0.00127085
Iteration 8/25 | Loss: 0.00127085
Iteration 9/25 | Loss: 0.00127085
Iteration 10/25 | Loss: 0.00127085
Iteration 11/25 | Loss: 0.00127085
Iteration 12/25 | Loss: 0.00127085
Iteration 13/25 | Loss: 0.00127085
Iteration 14/25 | Loss: 0.00127085
Iteration 15/25 | Loss: 0.00127085
Iteration 16/25 | Loss: 0.00127085
Iteration 17/25 | Loss: 0.00127085
Iteration 18/25 | Loss: 0.00127085
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012708455324172974, 0.0012708455324172974, 0.0012708455324172974, 0.0012708455324172974, 0.0012708455324172974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012708455324172974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127085
Iteration 2/1000 | Loss: 0.00005903
Iteration 3/1000 | Loss: 0.00018028
Iteration 4/1000 | Loss: 0.00010470
Iteration 5/1000 | Loss: 0.00015137
Iteration 6/1000 | Loss: 0.00010451
Iteration 7/1000 | Loss: 0.00013379
Iteration 8/1000 | Loss: 0.00013224
Iteration 9/1000 | Loss: 0.00010734
Iteration 10/1000 | Loss: 0.00013236
Iteration 11/1000 | Loss: 0.00009993
Iteration 12/1000 | Loss: 0.00002404
Iteration 13/1000 | Loss: 0.00002252
Iteration 14/1000 | Loss: 0.00012179
Iteration 15/1000 | Loss: 0.00010692
Iteration 16/1000 | Loss: 0.00014234
Iteration 17/1000 | Loss: 0.00003071
Iteration 18/1000 | Loss: 0.00002550
Iteration 19/1000 | Loss: 0.00002216
Iteration 20/1000 | Loss: 0.00002075
Iteration 21/1000 | Loss: 0.00015837
Iteration 22/1000 | Loss: 0.00011021
Iteration 23/1000 | Loss: 0.00014458
Iteration 24/1000 | Loss: 0.00009996
Iteration 25/1000 | Loss: 0.00014705
Iteration 26/1000 | Loss: 0.00014258
Iteration 27/1000 | Loss: 0.00004038
Iteration 28/1000 | Loss: 0.00002284
Iteration 29/1000 | Loss: 0.00002084
Iteration 30/1000 | Loss: 0.00001810
Iteration 31/1000 | Loss: 0.00001762
Iteration 32/1000 | Loss: 0.00008142
Iteration 33/1000 | Loss: 0.00029649
Iteration 34/1000 | Loss: 0.00009607
Iteration 35/1000 | Loss: 0.00003155
Iteration 36/1000 | Loss: 0.00001831
Iteration 37/1000 | Loss: 0.00001954
Iteration 38/1000 | Loss: 0.00001657
Iteration 39/1000 | Loss: 0.00001595
Iteration 40/1000 | Loss: 0.00001528
Iteration 41/1000 | Loss: 0.00002587
Iteration 42/1000 | Loss: 0.00001488
Iteration 43/1000 | Loss: 0.00001488
Iteration 44/1000 | Loss: 0.00001488
Iteration 45/1000 | Loss: 0.00001949
Iteration 46/1000 | Loss: 0.00001476
Iteration 47/1000 | Loss: 0.00001474
Iteration 48/1000 | Loss: 0.00001474
Iteration 49/1000 | Loss: 0.00001474
Iteration 50/1000 | Loss: 0.00001473
Iteration 51/1000 | Loss: 0.00001473
Iteration 52/1000 | Loss: 0.00001473
Iteration 53/1000 | Loss: 0.00001472
Iteration 54/1000 | Loss: 0.00001471
Iteration 55/1000 | Loss: 0.00001471
Iteration 56/1000 | Loss: 0.00001469
Iteration 57/1000 | Loss: 0.00001469
Iteration 58/1000 | Loss: 0.00001468
Iteration 59/1000 | Loss: 0.00001468
Iteration 60/1000 | Loss: 0.00001468
Iteration 61/1000 | Loss: 0.00001468
Iteration 62/1000 | Loss: 0.00001468
Iteration 63/1000 | Loss: 0.00001466
Iteration 64/1000 | Loss: 0.00001465
Iteration 65/1000 | Loss: 0.00001465
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001464
Iteration 68/1000 | Loss: 0.00001464
Iteration 69/1000 | Loss: 0.00001463
Iteration 70/1000 | Loss: 0.00001463
Iteration 71/1000 | Loss: 0.00001462
Iteration 72/1000 | Loss: 0.00001461
Iteration 73/1000 | Loss: 0.00001460
Iteration 74/1000 | Loss: 0.00001459
Iteration 75/1000 | Loss: 0.00001459
Iteration 76/1000 | Loss: 0.00001459
Iteration 77/1000 | Loss: 0.00001458
Iteration 78/1000 | Loss: 0.00001458
Iteration 79/1000 | Loss: 0.00001456
Iteration 80/1000 | Loss: 0.00001455
Iteration 81/1000 | Loss: 0.00001455
Iteration 82/1000 | Loss: 0.00001454
Iteration 83/1000 | Loss: 0.00001451
Iteration 84/1000 | Loss: 0.00001451
Iteration 85/1000 | Loss: 0.00001450
Iteration 86/1000 | Loss: 0.00001450
Iteration 87/1000 | Loss: 0.00001449
Iteration 88/1000 | Loss: 0.00001449
Iteration 89/1000 | Loss: 0.00001449
Iteration 90/1000 | Loss: 0.00001448
Iteration 91/1000 | Loss: 0.00001448
Iteration 92/1000 | Loss: 0.00002937
Iteration 93/1000 | Loss: 0.00001454
Iteration 94/1000 | Loss: 0.00001440
Iteration 95/1000 | Loss: 0.00001440
Iteration 96/1000 | Loss: 0.00001440
Iteration 97/1000 | Loss: 0.00001440
Iteration 98/1000 | Loss: 0.00001440
Iteration 99/1000 | Loss: 0.00001440
Iteration 100/1000 | Loss: 0.00001440
Iteration 101/1000 | Loss: 0.00001440
Iteration 102/1000 | Loss: 0.00001440
Iteration 103/1000 | Loss: 0.00001440
Iteration 104/1000 | Loss: 0.00001440
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001439
Iteration 108/1000 | Loss: 0.00001439
Iteration 109/1000 | Loss: 0.00001439
Iteration 110/1000 | Loss: 0.00001439
Iteration 111/1000 | Loss: 0.00001439
Iteration 112/1000 | Loss: 0.00001439
Iteration 113/1000 | Loss: 0.00001438
Iteration 114/1000 | Loss: 0.00001438
Iteration 115/1000 | Loss: 0.00001438
Iteration 116/1000 | Loss: 0.00001438
Iteration 117/1000 | Loss: 0.00001438
Iteration 118/1000 | Loss: 0.00001438
Iteration 119/1000 | Loss: 0.00001438
Iteration 120/1000 | Loss: 0.00001438
Iteration 121/1000 | Loss: 0.00001438
Iteration 122/1000 | Loss: 0.00001438
Iteration 123/1000 | Loss: 0.00001438
Iteration 124/1000 | Loss: 0.00001438
Iteration 125/1000 | Loss: 0.00001437
Iteration 126/1000 | Loss: 0.00001437
Iteration 127/1000 | Loss: 0.00001437
Iteration 128/1000 | Loss: 0.00001437
Iteration 129/1000 | Loss: 0.00001437
Iteration 130/1000 | Loss: 0.00001437
Iteration 131/1000 | Loss: 0.00001437
Iteration 132/1000 | Loss: 0.00001437
Iteration 133/1000 | Loss: 0.00001437
Iteration 134/1000 | Loss: 0.00001437
Iteration 135/1000 | Loss: 0.00001437
Iteration 136/1000 | Loss: 0.00001437
Iteration 137/1000 | Loss: 0.00001437
Iteration 138/1000 | Loss: 0.00001437
Iteration 139/1000 | Loss: 0.00001437
Iteration 140/1000 | Loss: 0.00001437
Iteration 141/1000 | Loss: 0.00001437
Iteration 142/1000 | Loss: 0.00001436
Iteration 143/1000 | Loss: 0.00001436
Iteration 144/1000 | Loss: 0.00001436
Iteration 145/1000 | Loss: 0.00001436
Iteration 146/1000 | Loss: 0.00001436
Iteration 147/1000 | Loss: 0.00001436
Iteration 148/1000 | Loss: 0.00001436
Iteration 149/1000 | Loss: 0.00001436
Iteration 150/1000 | Loss: 0.00001435
Iteration 151/1000 | Loss: 0.00001435
Iteration 152/1000 | Loss: 0.00001435
Iteration 153/1000 | Loss: 0.00001435
Iteration 154/1000 | Loss: 0.00002570
Iteration 155/1000 | Loss: 0.00001439
Iteration 156/1000 | Loss: 0.00001434
Iteration 157/1000 | Loss: 0.00001433
Iteration 158/1000 | Loss: 0.00001433
Iteration 159/1000 | Loss: 0.00001433
Iteration 160/1000 | Loss: 0.00001433
Iteration 161/1000 | Loss: 0.00001433
Iteration 162/1000 | Loss: 0.00001433
Iteration 163/1000 | Loss: 0.00001433
Iteration 164/1000 | Loss: 0.00001433
Iteration 165/1000 | Loss: 0.00001433
Iteration 166/1000 | Loss: 0.00001433
Iteration 167/1000 | Loss: 0.00001433
Iteration 168/1000 | Loss: 0.00001433
Iteration 169/1000 | Loss: 0.00001433
Iteration 170/1000 | Loss: 0.00001433
Iteration 171/1000 | Loss: 0.00001433
Iteration 172/1000 | Loss: 0.00001433
Iteration 173/1000 | Loss: 0.00001433
Iteration 174/1000 | Loss: 0.00001433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.4330917110783048e-05, 1.4330917110783048e-05, 1.4330917110783048e-05, 1.4330917110783048e-05, 1.4330917110783048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4330917110783048e-05

Optimization complete. Final v2v error: 3.1477396488189697 mm

Highest mean error: 6.06663179397583 mm for frame 0

Lowest mean error: 2.6041388511657715 mm for frame 236

Saving results

Total time: 138.4456024169922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968028
Iteration 2/25 | Loss: 0.00216749
Iteration 3/25 | Loss: 0.00176941
Iteration 4/25 | Loss: 0.00170891
Iteration 5/25 | Loss: 0.00167160
Iteration 6/25 | Loss: 0.00155892
Iteration 7/25 | Loss: 0.00145033
Iteration 8/25 | Loss: 0.00143115
Iteration 9/25 | Loss: 0.00139218
Iteration 10/25 | Loss: 0.00138706
Iteration 11/25 | Loss: 0.00134971
Iteration 12/25 | Loss: 0.00134288
Iteration 13/25 | Loss: 0.00133504
Iteration 14/25 | Loss: 0.00133393
Iteration 15/25 | Loss: 0.00134072
Iteration 16/25 | Loss: 0.00133933
Iteration 17/25 | Loss: 0.00133346
Iteration 18/25 | Loss: 0.00133177
Iteration 19/25 | Loss: 0.00133109
Iteration 20/25 | Loss: 0.00133091
Iteration 21/25 | Loss: 0.00133254
Iteration 22/25 | Loss: 0.00133044
Iteration 23/25 | Loss: 0.00132595
Iteration 24/25 | Loss: 0.00132454
Iteration 25/25 | Loss: 0.00132426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78404570
Iteration 2/25 | Loss: 0.00173618
Iteration 3/25 | Loss: 0.00173617
Iteration 4/25 | Loss: 0.00173617
Iteration 5/25 | Loss: 0.00173617
Iteration 6/25 | Loss: 0.00173617
Iteration 7/25 | Loss: 0.00173617
Iteration 8/25 | Loss: 0.00173617
Iteration 9/25 | Loss: 0.00173617
Iteration 10/25 | Loss: 0.00173617
Iteration 11/25 | Loss: 0.00173617
Iteration 12/25 | Loss: 0.00173617
Iteration 13/25 | Loss: 0.00173617
Iteration 14/25 | Loss: 0.00173617
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0017361706122756004, 0.0017361706122756004, 0.0017361706122756004, 0.0017361706122756004, 0.0017361706122756004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017361706122756004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173617
Iteration 2/1000 | Loss: 0.00014118
Iteration 3/1000 | Loss: 0.00009768
Iteration 4/1000 | Loss: 0.00006946
Iteration 5/1000 | Loss: 0.00005835
Iteration 6/1000 | Loss: 0.00005271
Iteration 7/1000 | Loss: 0.00011640
Iteration 8/1000 | Loss: 0.00004869
Iteration 9/1000 | Loss: 0.00016623
Iteration 10/1000 | Loss: 0.00011051
Iteration 11/1000 | Loss: 0.00044033
Iteration 12/1000 | Loss: 0.00067151
Iteration 13/1000 | Loss: 0.00005968
Iteration 14/1000 | Loss: 0.00003826
Iteration 15/1000 | Loss: 0.00003253
Iteration 16/1000 | Loss: 0.00002748
Iteration 17/1000 | Loss: 0.00002391
Iteration 18/1000 | Loss: 0.00002156
Iteration 19/1000 | Loss: 0.00001998
Iteration 20/1000 | Loss: 0.00001913
Iteration 21/1000 | Loss: 0.00001831
Iteration 22/1000 | Loss: 0.00001790
Iteration 23/1000 | Loss: 0.00024364
Iteration 24/1000 | Loss: 0.00002777
Iteration 25/1000 | Loss: 0.00002292
Iteration 26/1000 | Loss: 0.00002087
Iteration 27/1000 | Loss: 0.00030033
Iteration 28/1000 | Loss: 0.00018580
Iteration 29/1000 | Loss: 0.00028585
Iteration 30/1000 | Loss: 0.00016589
Iteration 31/1000 | Loss: 0.00026007
Iteration 32/1000 | Loss: 0.00016319
Iteration 33/1000 | Loss: 0.00020239
Iteration 34/1000 | Loss: 0.00002989
Iteration 35/1000 | Loss: 0.00002172
Iteration 36/1000 | Loss: 0.00001801
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001665
Iteration 39/1000 | Loss: 0.00001636
Iteration 40/1000 | Loss: 0.00001615
Iteration 41/1000 | Loss: 0.00001606
Iteration 42/1000 | Loss: 0.00001603
Iteration 43/1000 | Loss: 0.00001601
Iteration 44/1000 | Loss: 0.00001598
Iteration 45/1000 | Loss: 0.00001595
Iteration 46/1000 | Loss: 0.00001595
Iteration 47/1000 | Loss: 0.00001594
Iteration 48/1000 | Loss: 0.00001592
Iteration 49/1000 | Loss: 0.00001591
Iteration 50/1000 | Loss: 0.00001588
Iteration 51/1000 | Loss: 0.00001588
Iteration 52/1000 | Loss: 0.00001587
Iteration 53/1000 | Loss: 0.00001586
Iteration 54/1000 | Loss: 0.00001586
Iteration 55/1000 | Loss: 0.00001585
Iteration 56/1000 | Loss: 0.00001585
Iteration 57/1000 | Loss: 0.00001585
Iteration 58/1000 | Loss: 0.00001585
Iteration 59/1000 | Loss: 0.00001585
Iteration 60/1000 | Loss: 0.00001584
Iteration 61/1000 | Loss: 0.00001584
Iteration 62/1000 | Loss: 0.00001583
Iteration 63/1000 | Loss: 0.00001582
Iteration 64/1000 | Loss: 0.00001582
Iteration 65/1000 | Loss: 0.00001580
Iteration 66/1000 | Loss: 0.00001578
Iteration 67/1000 | Loss: 0.00001578
Iteration 68/1000 | Loss: 0.00001578
Iteration 69/1000 | Loss: 0.00001577
Iteration 70/1000 | Loss: 0.00001577
Iteration 71/1000 | Loss: 0.00001577
Iteration 72/1000 | Loss: 0.00001577
Iteration 73/1000 | Loss: 0.00001577
Iteration 74/1000 | Loss: 0.00001576
Iteration 75/1000 | Loss: 0.00001576
Iteration 76/1000 | Loss: 0.00001576
Iteration 77/1000 | Loss: 0.00001576
Iteration 78/1000 | Loss: 0.00001576
Iteration 79/1000 | Loss: 0.00001576
Iteration 80/1000 | Loss: 0.00001576
Iteration 81/1000 | Loss: 0.00001576
Iteration 82/1000 | Loss: 0.00001576
Iteration 83/1000 | Loss: 0.00001576
Iteration 84/1000 | Loss: 0.00001576
Iteration 85/1000 | Loss: 0.00001575
Iteration 86/1000 | Loss: 0.00001575
Iteration 87/1000 | Loss: 0.00001575
Iteration 88/1000 | Loss: 0.00001574
Iteration 89/1000 | Loss: 0.00001574
Iteration 90/1000 | Loss: 0.00001574
Iteration 91/1000 | Loss: 0.00001573
Iteration 92/1000 | Loss: 0.00001573
Iteration 93/1000 | Loss: 0.00001573
Iteration 94/1000 | Loss: 0.00001573
Iteration 95/1000 | Loss: 0.00001573
Iteration 96/1000 | Loss: 0.00001573
Iteration 97/1000 | Loss: 0.00001573
Iteration 98/1000 | Loss: 0.00001573
Iteration 99/1000 | Loss: 0.00001573
Iteration 100/1000 | Loss: 0.00001573
Iteration 101/1000 | Loss: 0.00001572
Iteration 102/1000 | Loss: 0.00001572
Iteration 103/1000 | Loss: 0.00001572
Iteration 104/1000 | Loss: 0.00001572
Iteration 105/1000 | Loss: 0.00001572
Iteration 106/1000 | Loss: 0.00001571
Iteration 107/1000 | Loss: 0.00001571
Iteration 108/1000 | Loss: 0.00001571
Iteration 109/1000 | Loss: 0.00001571
Iteration 110/1000 | Loss: 0.00001571
Iteration 111/1000 | Loss: 0.00001570
Iteration 112/1000 | Loss: 0.00001570
Iteration 113/1000 | Loss: 0.00001570
Iteration 114/1000 | Loss: 0.00001570
Iteration 115/1000 | Loss: 0.00001570
Iteration 116/1000 | Loss: 0.00001570
Iteration 117/1000 | Loss: 0.00001570
Iteration 118/1000 | Loss: 0.00001570
Iteration 119/1000 | Loss: 0.00001570
Iteration 120/1000 | Loss: 0.00001570
Iteration 121/1000 | Loss: 0.00001569
Iteration 122/1000 | Loss: 0.00001569
Iteration 123/1000 | Loss: 0.00001569
Iteration 124/1000 | Loss: 0.00001569
Iteration 125/1000 | Loss: 0.00001569
Iteration 126/1000 | Loss: 0.00001569
Iteration 127/1000 | Loss: 0.00001569
Iteration 128/1000 | Loss: 0.00001569
Iteration 129/1000 | Loss: 0.00001569
Iteration 130/1000 | Loss: 0.00001569
Iteration 131/1000 | Loss: 0.00001569
Iteration 132/1000 | Loss: 0.00001569
Iteration 133/1000 | Loss: 0.00001569
Iteration 134/1000 | Loss: 0.00001569
Iteration 135/1000 | Loss: 0.00001569
Iteration 136/1000 | Loss: 0.00001569
Iteration 137/1000 | Loss: 0.00001569
Iteration 138/1000 | Loss: 0.00001568
Iteration 139/1000 | Loss: 0.00001568
Iteration 140/1000 | Loss: 0.00001568
Iteration 141/1000 | Loss: 0.00001568
Iteration 142/1000 | Loss: 0.00001568
Iteration 143/1000 | Loss: 0.00001568
Iteration 144/1000 | Loss: 0.00001568
Iteration 145/1000 | Loss: 0.00001568
Iteration 146/1000 | Loss: 0.00001568
Iteration 147/1000 | Loss: 0.00001568
Iteration 148/1000 | Loss: 0.00001568
Iteration 149/1000 | Loss: 0.00001568
Iteration 150/1000 | Loss: 0.00001568
Iteration 151/1000 | Loss: 0.00001568
Iteration 152/1000 | Loss: 0.00001568
Iteration 153/1000 | Loss: 0.00001568
Iteration 154/1000 | Loss: 0.00001568
Iteration 155/1000 | Loss: 0.00001567
Iteration 156/1000 | Loss: 0.00001567
Iteration 157/1000 | Loss: 0.00001567
Iteration 158/1000 | Loss: 0.00001567
Iteration 159/1000 | Loss: 0.00001567
Iteration 160/1000 | Loss: 0.00001567
Iteration 161/1000 | Loss: 0.00001567
Iteration 162/1000 | Loss: 0.00001567
Iteration 163/1000 | Loss: 0.00001567
Iteration 164/1000 | Loss: 0.00001567
Iteration 165/1000 | Loss: 0.00001567
Iteration 166/1000 | Loss: 0.00001567
Iteration 167/1000 | Loss: 0.00001567
Iteration 168/1000 | Loss: 0.00001567
Iteration 169/1000 | Loss: 0.00001566
Iteration 170/1000 | Loss: 0.00001566
Iteration 171/1000 | Loss: 0.00001566
Iteration 172/1000 | Loss: 0.00001566
Iteration 173/1000 | Loss: 0.00001566
Iteration 174/1000 | Loss: 0.00001566
Iteration 175/1000 | Loss: 0.00001566
Iteration 176/1000 | Loss: 0.00001566
Iteration 177/1000 | Loss: 0.00001566
Iteration 178/1000 | Loss: 0.00001566
Iteration 179/1000 | Loss: 0.00001566
Iteration 180/1000 | Loss: 0.00001566
Iteration 181/1000 | Loss: 0.00001566
Iteration 182/1000 | Loss: 0.00001566
Iteration 183/1000 | Loss: 0.00001565
Iteration 184/1000 | Loss: 0.00001565
Iteration 185/1000 | Loss: 0.00001565
Iteration 186/1000 | Loss: 0.00001565
Iteration 187/1000 | Loss: 0.00001565
Iteration 188/1000 | Loss: 0.00001565
Iteration 189/1000 | Loss: 0.00001565
Iteration 190/1000 | Loss: 0.00001565
Iteration 191/1000 | Loss: 0.00001565
Iteration 192/1000 | Loss: 0.00001565
Iteration 193/1000 | Loss: 0.00001565
Iteration 194/1000 | Loss: 0.00001565
Iteration 195/1000 | Loss: 0.00001565
Iteration 196/1000 | Loss: 0.00001565
Iteration 197/1000 | Loss: 0.00001565
Iteration 198/1000 | Loss: 0.00001565
Iteration 199/1000 | Loss: 0.00001564
Iteration 200/1000 | Loss: 0.00001564
Iteration 201/1000 | Loss: 0.00001564
Iteration 202/1000 | Loss: 0.00001564
Iteration 203/1000 | Loss: 0.00001564
Iteration 204/1000 | Loss: 0.00001564
Iteration 205/1000 | Loss: 0.00001564
Iteration 206/1000 | Loss: 0.00001564
Iteration 207/1000 | Loss: 0.00001564
Iteration 208/1000 | Loss: 0.00001564
Iteration 209/1000 | Loss: 0.00001564
Iteration 210/1000 | Loss: 0.00001564
Iteration 211/1000 | Loss: 0.00001564
Iteration 212/1000 | Loss: 0.00001563
Iteration 213/1000 | Loss: 0.00001563
Iteration 214/1000 | Loss: 0.00001563
Iteration 215/1000 | Loss: 0.00001563
Iteration 216/1000 | Loss: 0.00001563
Iteration 217/1000 | Loss: 0.00001563
Iteration 218/1000 | Loss: 0.00001563
Iteration 219/1000 | Loss: 0.00001563
Iteration 220/1000 | Loss: 0.00001563
Iteration 221/1000 | Loss: 0.00001563
Iteration 222/1000 | Loss: 0.00001563
Iteration 223/1000 | Loss: 0.00001563
Iteration 224/1000 | Loss: 0.00001563
Iteration 225/1000 | Loss: 0.00001563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.563160913065076e-05, 1.563160913065076e-05, 1.563160913065076e-05, 1.563160913065076e-05, 1.563160913065076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.563160913065076e-05

Optimization complete. Final v2v error: 3.321070909500122 mm

Highest mean error: 4.450202465057373 mm for frame 69

Lowest mean error: 2.930783271789551 mm for frame 107

Saving results

Total time: 114.15177059173584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402442
Iteration 2/25 | Loss: 0.00131491
Iteration 3/25 | Loss: 0.00124178
Iteration 4/25 | Loss: 0.00123591
Iteration 5/25 | Loss: 0.00123513
Iteration 6/25 | Loss: 0.00123513
Iteration 7/25 | Loss: 0.00123513
Iteration 8/25 | Loss: 0.00123513
Iteration 9/25 | Loss: 0.00123513
Iteration 10/25 | Loss: 0.00123513
Iteration 11/25 | Loss: 0.00123513
Iteration 12/25 | Loss: 0.00123513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012351332698017359, 0.0012351332698017359, 0.0012351332698017359, 0.0012351332698017359, 0.0012351332698017359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012351332698017359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52421200
Iteration 2/25 | Loss: 0.00087880
Iteration 3/25 | Loss: 0.00087880
Iteration 4/25 | Loss: 0.00087880
Iteration 5/25 | Loss: 0.00087880
Iteration 6/25 | Loss: 0.00087880
Iteration 7/25 | Loss: 0.00087880
Iteration 8/25 | Loss: 0.00087880
Iteration 9/25 | Loss: 0.00087880
Iteration 10/25 | Loss: 0.00087880
Iteration 11/25 | Loss: 0.00087880
Iteration 12/25 | Loss: 0.00087880
Iteration 13/25 | Loss: 0.00087880
Iteration 14/25 | Loss: 0.00087880
Iteration 15/25 | Loss: 0.00087880
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008787992992438376, 0.0008787992992438376, 0.0008787992992438376, 0.0008787992992438376, 0.0008787992992438376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008787992992438376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087880
Iteration 2/1000 | Loss: 0.00002458
Iteration 3/1000 | Loss: 0.00001632
Iteration 4/1000 | Loss: 0.00001460
Iteration 5/1000 | Loss: 0.00001383
Iteration 6/1000 | Loss: 0.00001328
Iteration 7/1000 | Loss: 0.00001303
Iteration 8/1000 | Loss: 0.00001267
Iteration 9/1000 | Loss: 0.00001236
Iteration 10/1000 | Loss: 0.00001211
Iteration 11/1000 | Loss: 0.00001189
Iteration 12/1000 | Loss: 0.00001173
Iteration 13/1000 | Loss: 0.00001165
Iteration 14/1000 | Loss: 0.00001162
Iteration 15/1000 | Loss: 0.00001162
Iteration 16/1000 | Loss: 0.00001161
Iteration 17/1000 | Loss: 0.00001160
Iteration 18/1000 | Loss: 0.00001160
Iteration 19/1000 | Loss: 0.00001160
Iteration 20/1000 | Loss: 0.00001159
Iteration 21/1000 | Loss: 0.00001159
Iteration 22/1000 | Loss: 0.00001159
Iteration 23/1000 | Loss: 0.00001159
Iteration 24/1000 | Loss: 0.00001158
Iteration 25/1000 | Loss: 0.00001157
Iteration 26/1000 | Loss: 0.00001156
Iteration 27/1000 | Loss: 0.00001156
Iteration 28/1000 | Loss: 0.00001155
Iteration 29/1000 | Loss: 0.00001155
Iteration 30/1000 | Loss: 0.00001155
Iteration 31/1000 | Loss: 0.00001154
Iteration 32/1000 | Loss: 0.00001153
Iteration 33/1000 | Loss: 0.00001153
Iteration 34/1000 | Loss: 0.00001152
Iteration 35/1000 | Loss: 0.00001152
Iteration 36/1000 | Loss: 0.00001151
Iteration 37/1000 | Loss: 0.00001151
Iteration 38/1000 | Loss: 0.00001150
Iteration 39/1000 | Loss: 0.00001149
Iteration 40/1000 | Loss: 0.00001149
Iteration 41/1000 | Loss: 0.00001148
Iteration 42/1000 | Loss: 0.00001148
Iteration 43/1000 | Loss: 0.00001146
Iteration 44/1000 | Loss: 0.00001145
Iteration 45/1000 | Loss: 0.00001145
Iteration 46/1000 | Loss: 0.00001145
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001145
Iteration 49/1000 | Loss: 0.00001143
Iteration 50/1000 | Loss: 0.00001140
Iteration 51/1000 | Loss: 0.00001140
Iteration 52/1000 | Loss: 0.00001140
Iteration 53/1000 | Loss: 0.00001139
Iteration 54/1000 | Loss: 0.00001139
Iteration 55/1000 | Loss: 0.00001139
Iteration 56/1000 | Loss: 0.00001137
Iteration 57/1000 | Loss: 0.00001136
Iteration 58/1000 | Loss: 0.00001136
Iteration 59/1000 | Loss: 0.00001136
Iteration 60/1000 | Loss: 0.00001136
Iteration 61/1000 | Loss: 0.00001136
Iteration 62/1000 | Loss: 0.00001136
Iteration 63/1000 | Loss: 0.00001136
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001129
Iteration 66/1000 | Loss: 0.00001128
Iteration 67/1000 | Loss: 0.00001125
Iteration 68/1000 | Loss: 0.00001124
Iteration 69/1000 | Loss: 0.00001124
Iteration 70/1000 | Loss: 0.00001123
Iteration 71/1000 | Loss: 0.00001121
Iteration 72/1000 | Loss: 0.00001120
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00001119
Iteration 75/1000 | Loss: 0.00001119
Iteration 76/1000 | Loss: 0.00001119
Iteration 77/1000 | Loss: 0.00001118
Iteration 78/1000 | Loss: 0.00001118
Iteration 79/1000 | Loss: 0.00001117
Iteration 80/1000 | Loss: 0.00001116
Iteration 81/1000 | Loss: 0.00001115
Iteration 82/1000 | Loss: 0.00001115
Iteration 83/1000 | Loss: 0.00001115
Iteration 84/1000 | Loss: 0.00001115
Iteration 85/1000 | Loss: 0.00001115
Iteration 86/1000 | Loss: 0.00001115
Iteration 87/1000 | Loss: 0.00001115
Iteration 88/1000 | Loss: 0.00001115
Iteration 89/1000 | Loss: 0.00001115
Iteration 90/1000 | Loss: 0.00001115
Iteration 91/1000 | Loss: 0.00001115
Iteration 92/1000 | Loss: 0.00001115
Iteration 93/1000 | Loss: 0.00001115
Iteration 94/1000 | Loss: 0.00001114
Iteration 95/1000 | Loss: 0.00001114
Iteration 96/1000 | Loss: 0.00001113
Iteration 97/1000 | Loss: 0.00001113
Iteration 98/1000 | Loss: 0.00001113
Iteration 99/1000 | Loss: 0.00001112
Iteration 100/1000 | Loss: 0.00001111
Iteration 101/1000 | Loss: 0.00001111
Iteration 102/1000 | Loss: 0.00001111
Iteration 103/1000 | Loss: 0.00001111
Iteration 104/1000 | Loss: 0.00001111
Iteration 105/1000 | Loss: 0.00001111
Iteration 106/1000 | Loss: 0.00001111
Iteration 107/1000 | Loss: 0.00001110
Iteration 108/1000 | Loss: 0.00001110
Iteration 109/1000 | Loss: 0.00001110
Iteration 110/1000 | Loss: 0.00001110
Iteration 111/1000 | Loss: 0.00001110
Iteration 112/1000 | Loss: 0.00001110
Iteration 113/1000 | Loss: 0.00001110
Iteration 114/1000 | Loss: 0.00001110
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001109
Iteration 118/1000 | Loss: 0.00001109
Iteration 119/1000 | Loss: 0.00001109
Iteration 120/1000 | Loss: 0.00001109
Iteration 121/1000 | Loss: 0.00001108
Iteration 122/1000 | Loss: 0.00001108
Iteration 123/1000 | Loss: 0.00001108
Iteration 124/1000 | Loss: 0.00001108
Iteration 125/1000 | Loss: 0.00001108
Iteration 126/1000 | Loss: 0.00001108
Iteration 127/1000 | Loss: 0.00001108
Iteration 128/1000 | Loss: 0.00001108
Iteration 129/1000 | Loss: 0.00001108
Iteration 130/1000 | Loss: 0.00001107
Iteration 131/1000 | Loss: 0.00001107
Iteration 132/1000 | Loss: 0.00001107
Iteration 133/1000 | Loss: 0.00001107
Iteration 134/1000 | Loss: 0.00001107
Iteration 135/1000 | Loss: 0.00001106
Iteration 136/1000 | Loss: 0.00001106
Iteration 137/1000 | Loss: 0.00001106
Iteration 138/1000 | Loss: 0.00001105
Iteration 139/1000 | Loss: 0.00001105
Iteration 140/1000 | Loss: 0.00001105
Iteration 141/1000 | Loss: 0.00001105
Iteration 142/1000 | Loss: 0.00001105
Iteration 143/1000 | Loss: 0.00001104
Iteration 144/1000 | Loss: 0.00001104
Iteration 145/1000 | Loss: 0.00001104
Iteration 146/1000 | Loss: 0.00001104
Iteration 147/1000 | Loss: 0.00001104
Iteration 148/1000 | Loss: 0.00001104
Iteration 149/1000 | Loss: 0.00001104
Iteration 150/1000 | Loss: 0.00001104
Iteration 151/1000 | Loss: 0.00001104
Iteration 152/1000 | Loss: 0.00001103
Iteration 153/1000 | Loss: 0.00001103
Iteration 154/1000 | Loss: 0.00001103
Iteration 155/1000 | Loss: 0.00001103
Iteration 156/1000 | Loss: 0.00001102
Iteration 157/1000 | Loss: 0.00001102
Iteration 158/1000 | Loss: 0.00001102
Iteration 159/1000 | Loss: 0.00001102
Iteration 160/1000 | Loss: 0.00001102
Iteration 161/1000 | Loss: 0.00001102
Iteration 162/1000 | Loss: 0.00001102
Iteration 163/1000 | Loss: 0.00001102
Iteration 164/1000 | Loss: 0.00001101
Iteration 165/1000 | Loss: 0.00001101
Iteration 166/1000 | Loss: 0.00001101
Iteration 167/1000 | Loss: 0.00001101
Iteration 168/1000 | Loss: 0.00001101
Iteration 169/1000 | Loss: 0.00001101
Iteration 170/1000 | Loss: 0.00001101
Iteration 171/1000 | Loss: 0.00001101
Iteration 172/1000 | Loss: 0.00001101
Iteration 173/1000 | Loss: 0.00001101
Iteration 174/1000 | Loss: 0.00001100
Iteration 175/1000 | Loss: 0.00001100
Iteration 176/1000 | Loss: 0.00001100
Iteration 177/1000 | Loss: 0.00001100
Iteration 178/1000 | Loss: 0.00001100
Iteration 179/1000 | Loss: 0.00001100
Iteration 180/1000 | Loss: 0.00001100
Iteration 181/1000 | Loss: 0.00001100
Iteration 182/1000 | Loss: 0.00001100
Iteration 183/1000 | Loss: 0.00001100
Iteration 184/1000 | Loss: 0.00001100
Iteration 185/1000 | Loss: 0.00001100
Iteration 186/1000 | Loss: 0.00001100
Iteration 187/1000 | Loss: 0.00001099
Iteration 188/1000 | Loss: 0.00001099
Iteration 189/1000 | Loss: 0.00001099
Iteration 190/1000 | Loss: 0.00001099
Iteration 191/1000 | Loss: 0.00001099
Iteration 192/1000 | Loss: 0.00001099
Iteration 193/1000 | Loss: 0.00001099
Iteration 194/1000 | Loss: 0.00001098
Iteration 195/1000 | Loss: 0.00001098
Iteration 196/1000 | Loss: 0.00001098
Iteration 197/1000 | Loss: 0.00001098
Iteration 198/1000 | Loss: 0.00001098
Iteration 199/1000 | Loss: 0.00001098
Iteration 200/1000 | Loss: 0.00001098
Iteration 201/1000 | Loss: 0.00001098
Iteration 202/1000 | Loss: 0.00001098
Iteration 203/1000 | Loss: 0.00001098
Iteration 204/1000 | Loss: 0.00001098
Iteration 205/1000 | Loss: 0.00001098
Iteration 206/1000 | Loss: 0.00001097
Iteration 207/1000 | Loss: 0.00001097
Iteration 208/1000 | Loss: 0.00001097
Iteration 209/1000 | Loss: 0.00001097
Iteration 210/1000 | Loss: 0.00001097
Iteration 211/1000 | Loss: 0.00001097
Iteration 212/1000 | Loss: 0.00001097
Iteration 213/1000 | Loss: 0.00001097
Iteration 214/1000 | Loss: 0.00001097
Iteration 215/1000 | Loss: 0.00001097
Iteration 216/1000 | Loss: 0.00001097
Iteration 217/1000 | Loss: 0.00001097
Iteration 218/1000 | Loss: 0.00001097
Iteration 219/1000 | Loss: 0.00001097
Iteration 220/1000 | Loss: 0.00001097
Iteration 221/1000 | Loss: 0.00001097
Iteration 222/1000 | Loss: 0.00001097
Iteration 223/1000 | Loss: 0.00001097
Iteration 224/1000 | Loss: 0.00001097
Iteration 225/1000 | Loss: 0.00001097
Iteration 226/1000 | Loss: 0.00001097
Iteration 227/1000 | Loss: 0.00001097
Iteration 228/1000 | Loss: 0.00001097
Iteration 229/1000 | Loss: 0.00001097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.0974174983857665e-05, 1.0974174983857665e-05, 1.0974174983857665e-05, 1.0974174983857665e-05, 1.0974174983857665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0974174983857665e-05

Optimization complete. Final v2v error: 2.8373470306396484 mm

Highest mean error: 3.0602829456329346 mm for frame 153

Lowest mean error: 2.6911370754241943 mm for frame 244

Saving results

Total time: 48.76410126686096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471604
Iteration 2/25 | Loss: 0.00135883
Iteration 3/25 | Loss: 0.00125634
Iteration 4/25 | Loss: 0.00124495
Iteration 5/25 | Loss: 0.00124218
Iteration 6/25 | Loss: 0.00124218
Iteration 7/25 | Loss: 0.00124218
Iteration 8/25 | Loss: 0.00124218
Iteration 9/25 | Loss: 0.00124218
Iteration 10/25 | Loss: 0.00124218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012421789579093456, 0.0012421789579093456, 0.0012421789579093456, 0.0012421789579093456, 0.0012421789579093456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012421789579093456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.09784412
Iteration 2/25 | Loss: 0.00099394
Iteration 3/25 | Loss: 0.00099392
Iteration 4/25 | Loss: 0.00099392
Iteration 5/25 | Loss: 0.00099392
Iteration 6/25 | Loss: 0.00099392
Iteration 7/25 | Loss: 0.00099392
Iteration 8/25 | Loss: 0.00099392
Iteration 9/25 | Loss: 0.00099392
Iteration 10/25 | Loss: 0.00099392
Iteration 11/25 | Loss: 0.00099392
Iteration 12/25 | Loss: 0.00099392
Iteration 13/25 | Loss: 0.00099392
Iteration 14/25 | Loss: 0.00099392
Iteration 15/25 | Loss: 0.00099392
Iteration 16/25 | Loss: 0.00099392
Iteration 17/25 | Loss: 0.00099392
Iteration 18/25 | Loss: 0.00099392
Iteration 19/25 | Loss: 0.00099392
Iteration 20/25 | Loss: 0.00099392
Iteration 21/25 | Loss: 0.00099392
Iteration 22/25 | Loss: 0.00099392
Iteration 23/25 | Loss: 0.00099392
Iteration 24/25 | Loss: 0.00099392
Iteration 25/25 | Loss: 0.00099392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099392
Iteration 2/1000 | Loss: 0.00002458
Iteration 3/1000 | Loss: 0.00001956
Iteration 4/1000 | Loss: 0.00001792
Iteration 5/1000 | Loss: 0.00001682
Iteration 6/1000 | Loss: 0.00001619
Iteration 7/1000 | Loss: 0.00001565
Iteration 8/1000 | Loss: 0.00001525
Iteration 9/1000 | Loss: 0.00001496
Iteration 10/1000 | Loss: 0.00001460
Iteration 11/1000 | Loss: 0.00001445
Iteration 12/1000 | Loss: 0.00001425
Iteration 13/1000 | Loss: 0.00001414
Iteration 14/1000 | Loss: 0.00001413
Iteration 15/1000 | Loss: 0.00001408
Iteration 16/1000 | Loss: 0.00001408
Iteration 17/1000 | Loss: 0.00001407
Iteration 18/1000 | Loss: 0.00001407
Iteration 19/1000 | Loss: 0.00001406
Iteration 20/1000 | Loss: 0.00001405
Iteration 21/1000 | Loss: 0.00001405
Iteration 22/1000 | Loss: 0.00001399
Iteration 23/1000 | Loss: 0.00001395
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001394
Iteration 26/1000 | Loss: 0.00001393
Iteration 27/1000 | Loss: 0.00001392
Iteration 28/1000 | Loss: 0.00001392
Iteration 29/1000 | Loss: 0.00001391
Iteration 30/1000 | Loss: 0.00001391
Iteration 31/1000 | Loss: 0.00001390
Iteration 32/1000 | Loss: 0.00001390
Iteration 33/1000 | Loss: 0.00001387
Iteration 34/1000 | Loss: 0.00001387
Iteration 35/1000 | Loss: 0.00001385
Iteration 36/1000 | Loss: 0.00001385
Iteration 37/1000 | Loss: 0.00001383
Iteration 38/1000 | Loss: 0.00001383
Iteration 39/1000 | Loss: 0.00001382
Iteration 40/1000 | Loss: 0.00001382
Iteration 41/1000 | Loss: 0.00001382
Iteration 42/1000 | Loss: 0.00001382
Iteration 43/1000 | Loss: 0.00001381
Iteration 44/1000 | Loss: 0.00001380
Iteration 45/1000 | Loss: 0.00001380
Iteration 46/1000 | Loss: 0.00001379
Iteration 47/1000 | Loss: 0.00001376
Iteration 48/1000 | Loss: 0.00001376
Iteration 49/1000 | Loss: 0.00001374
Iteration 50/1000 | Loss: 0.00001373
Iteration 51/1000 | Loss: 0.00001373
Iteration 52/1000 | Loss: 0.00001373
Iteration 53/1000 | Loss: 0.00001373
Iteration 54/1000 | Loss: 0.00001373
Iteration 55/1000 | Loss: 0.00001373
Iteration 56/1000 | Loss: 0.00001372
Iteration 57/1000 | Loss: 0.00001372
Iteration 58/1000 | Loss: 0.00001372
Iteration 59/1000 | Loss: 0.00001372
Iteration 60/1000 | Loss: 0.00001372
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001371
Iteration 64/1000 | Loss: 0.00001371
Iteration 65/1000 | Loss: 0.00001370
Iteration 66/1000 | Loss: 0.00001370
Iteration 67/1000 | Loss: 0.00001369
Iteration 68/1000 | Loss: 0.00001369
Iteration 69/1000 | Loss: 0.00001369
Iteration 70/1000 | Loss: 0.00001368
Iteration 71/1000 | Loss: 0.00001368
Iteration 72/1000 | Loss: 0.00001368
Iteration 73/1000 | Loss: 0.00001368
Iteration 74/1000 | Loss: 0.00001367
Iteration 75/1000 | Loss: 0.00001366
Iteration 76/1000 | Loss: 0.00001366
Iteration 77/1000 | Loss: 0.00001365
Iteration 78/1000 | Loss: 0.00001364
Iteration 79/1000 | Loss: 0.00001364
Iteration 80/1000 | Loss: 0.00001363
Iteration 81/1000 | Loss: 0.00001363
Iteration 82/1000 | Loss: 0.00001363
Iteration 83/1000 | Loss: 0.00001363
Iteration 84/1000 | Loss: 0.00001363
Iteration 85/1000 | Loss: 0.00001362
Iteration 86/1000 | Loss: 0.00001362
Iteration 87/1000 | Loss: 0.00001362
Iteration 88/1000 | Loss: 0.00001362
Iteration 89/1000 | Loss: 0.00001361
Iteration 90/1000 | Loss: 0.00001361
Iteration 91/1000 | Loss: 0.00001361
Iteration 92/1000 | Loss: 0.00001361
Iteration 93/1000 | Loss: 0.00001361
Iteration 94/1000 | Loss: 0.00001361
Iteration 95/1000 | Loss: 0.00001361
Iteration 96/1000 | Loss: 0.00001361
Iteration 97/1000 | Loss: 0.00001360
Iteration 98/1000 | Loss: 0.00001360
Iteration 99/1000 | Loss: 0.00001360
Iteration 100/1000 | Loss: 0.00001360
Iteration 101/1000 | Loss: 0.00001360
Iteration 102/1000 | Loss: 0.00001359
Iteration 103/1000 | Loss: 0.00001359
Iteration 104/1000 | Loss: 0.00001359
Iteration 105/1000 | Loss: 0.00001358
Iteration 106/1000 | Loss: 0.00001358
Iteration 107/1000 | Loss: 0.00001358
Iteration 108/1000 | Loss: 0.00001357
Iteration 109/1000 | Loss: 0.00001357
Iteration 110/1000 | Loss: 0.00001357
Iteration 111/1000 | Loss: 0.00001357
Iteration 112/1000 | Loss: 0.00001356
Iteration 113/1000 | Loss: 0.00001356
Iteration 114/1000 | Loss: 0.00001356
Iteration 115/1000 | Loss: 0.00001355
Iteration 116/1000 | Loss: 0.00001355
Iteration 117/1000 | Loss: 0.00001355
Iteration 118/1000 | Loss: 0.00001355
Iteration 119/1000 | Loss: 0.00001355
Iteration 120/1000 | Loss: 0.00001355
Iteration 121/1000 | Loss: 0.00001354
Iteration 122/1000 | Loss: 0.00001354
Iteration 123/1000 | Loss: 0.00001354
Iteration 124/1000 | Loss: 0.00001354
Iteration 125/1000 | Loss: 0.00001354
Iteration 126/1000 | Loss: 0.00001354
Iteration 127/1000 | Loss: 0.00001353
Iteration 128/1000 | Loss: 0.00001353
Iteration 129/1000 | Loss: 0.00001353
Iteration 130/1000 | Loss: 0.00001353
Iteration 131/1000 | Loss: 0.00001353
Iteration 132/1000 | Loss: 0.00001353
Iteration 133/1000 | Loss: 0.00001352
Iteration 134/1000 | Loss: 0.00001352
Iteration 135/1000 | Loss: 0.00001352
Iteration 136/1000 | Loss: 0.00001352
Iteration 137/1000 | Loss: 0.00001352
Iteration 138/1000 | Loss: 0.00001352
Iteration 139/1000 | Loss: 0.00001352
Iteration 140/1000 | Loss: 0.00001352
Iteration 141/1000 | Loss: 0.00001352
Iteration 142/1000 | Loss: 0.00001351
Iteration 143/1000 | Loss: 0.00001351
Iteration 144/1000 | Loss: 0.00001351
Iteration 145/1000 | Loss: 0.00001350
Iteration 146/1000 | Loss: 0.00001350
Iteration 147/1000 | Loss: 0.00001350
Iteration 148/1000 | Loss: 0.00001350
Iteration 149/1000 | Loss: 0.00001350
Iteration 150/1000 | Loss: 0.00001350
Iteration 151/1000 | Loss: 0.00001350
Iteration 152/1000 | Loss: 0.00001350
Iteration 153/1000 | Loss: 0.00001350
Iteration 154/1000 | Loss: 0.00001349
Iteration 155/1000 | Loss: 0.00001349
Iteration 156/1000 | Loss: 0.00001348
Iteration 157/1000 | Loss: 0.00001348
Iteration 158/1000 | Loss: 0.00001348
Iteration 159/1000 | Loss: 0.00001348
Iteration 160/1000 | Loss: 0.00001348
Iteration 161/1000 | Loss: 0.00001348
Iteration 162/1000 | Loss: 0.00001348
Iteration 163/1000 | Loss: 0.00001348
Iteration 164/1000 | Loss: 0.00001348
Iteration 165/1000 | Loss: 0.00001348
Iteration 166/1000 | Loss: 0.00001347
Iteration 167/1000 | Loss: 0.00001347
Iteration 168/1000 | Loss: 0.00001347
Iteration 169/1000 | Loss: 0.00001347
Iteration 170/1000 | Loss: 0.00001347
Iteration 171/1000 | Loss: 0.00001346
Iteration 172/1000 | Loss: 0.00001346
Iteration 173/1000 | Loss: 0.00001346
Iteration 174/1000 | Loss: 0.00001346
Iteration 175/1000 | Loss: 0.00001346
Iteration 176/1000 | Loss: 0.00001346
Iteration 177/1000 | Loss: 0.00001346
Iteration 178/1000 | Loss: 0.00001346
Iteration 179/1000 | Loss: 0.00001346
Iteration 180/1000 | Loss: 0.00001346
Iteration 181/1000 | Loss: 0.00001346
Iteration 182/1000 | Loss: 0.00001346
Iteration 183/1000 | Loss: 0.00001346
Iteration 184/1000 | Loss: 0.00001346
Iteration 185/1000 | Loss: 0.00001345
Iteration 186/1000 | Loss: 0.00001345
Iteration 187/1000 | Loss: 0.00001345
Iteration 188/1000 | Loss: 0.00001345
Iteration 189/1000 | Loss: 0.00001345
Iteration 190/1000 | Loss: 0.00001345
Iteration 191/1000 | Loss: 0.00001345
Iteration 192/1000 | Loss: 0.00001345
Iteration 193/1000 | Loss: 0.00001345
Iteration 194/1000 | Loss: 0.00001345
Iteration 195/1000 | Loss: 0.00001344
Iteration 196/1000 | Loss: 0.00001344
Iteration 197/1000 | Loss: 0.00001344
Iteration 198/1000 | Loss: 0.00001344
Iteration 199/1000 | Loss: 0.00001344
Iteration 200/1000 | Loss: 0.00001344
Iteration 201/1000 | Loss: 0.00001344
Iteration 202/1000 | Loss: 0.00001344
Iteration 203/1000 | Loss: 0.00001344
Iteration 204/1000 | Loss: 0.00001344
Iteration 205/1000 | Loss: 0.00001344
Iteration 206/1000 | Loss: 0.00001344
Iteration 207/1000 | Loss: 0.00001344
Iteration 208/1000 | Loss: 0.00001344
Iteration 209/1000 | Loss: 0.00001344
Iteration 210/1000 | Loss: 0.00001344
Iteration 211/1000 | Loss: 0.00001344
Iteration 212/1000 | Loss: 0.00001344
Iteration 213/1000 | Loss: 0.00001344
Iteration 214/1000 | Loss: 0.00001344
Iteration 215/1000 | Loss: 0.00001343
Iteration 216/1000 | Loss: 0.00001343
Iteration 217/1000 | Loss: 0.00001343
Iteration 218/1000 | Loss: 0.00001343
Iteration 219/1000 | Loss: 0.00001343
Iteration 220/1000 | Loss: 0.00001343
Iteration 221/1000 | Loss: 0.00001343
Iteration 222/1000 | Loss: 0.00001343
Iteration 223/1000 | Loss: 0.00001343
Iteration 224/1000 | Loss: 0.00001343
Iteration 225/1000 | Loss: 0.00001343
Iteration 226/1000 | Loss: 0.00001343
Iteration 227/1000 | Loss: 0.00001343
Iteration 228/1000 | Loss: 0.00001343
Iteration 229/1000 | Loss: 0.00001343
Iteration 230/1000 | Loss: 0.00001343
Iteration 231/1000 | Loss: 0.00001343
Iteration 232/1000 | Loss: 0.00001343
Iteration 233/1000 | Loss: 0.00001343
Iteration 234/1000 | Loss: 0.00001343
Iteration 235/1000 | Loss: 0.00001343
Iteration 236/1000 | Loss: 0.00001343
Iteration 237/1000 | Loss: 0.00001343
Iteration 238/1000 | Loss: 0.00001343
Iteration 239/1000 | Loss: 0.00001343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.3429241334961262e-05, 1.3429241334961262e-05, 1.3429241334961262e-05, 1.3429241334961262e-05, 1.3429241334961262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3429241334961262e-05

Optimization complete. Final v2v error: 3.1088993549346924 mm

Highest mean error: 3.3920669555664062 mm for frame 80

Lowest mean error: 2.841535806655884 mm for frame 156

Saving results

Total time: 46.13528752326965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924273
Iteration 2/25 | Loss: 0.00293579
Iteration 3/25 | Loss: 0.00189195
Iteration 4/25 | Loss: 0.00165275
Iteration 5/25 | Loss: 0.00170411
Iteration 6/25 | Loss: 0.00166861
Iteration 7/25 | Loss: 0.00164538
Iteration 8/25 | Loss: 0.00162178
Iteration 9/25 | Loss: 0.00163313
Iteration 10/25 | Loss: 0.00163581
Iteration 11/25 | Loss: 0.00164101
Iteration 12/25 | Loss: 0.00160832
Iteration 13/25 | Loss: 0.00158444
Iteration 14/25 | Loss: 0.00159266
Iteration 15/25 | Loss: 0.00157798
Iteration 16/25 | Loss: 0.00156037
Iteration 17/25 | Loss: 0.00155214
Iteration 18/25 | Loss: 0.00154890
Iteration 19/25 | Loss: 0.00154175
Iteration 20/25 | Loss: 0.00152758
Iteration 21/25 | Loss: 0.00152630
Iteration 22/25 | Loss: 0.00152769
Iteration 23/25 | Loss: 0.00151239
Iteration 24/25 | Loss: 0.00150654
Iteration 25/25 | Loss: 0.00150614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.69435191
Iteration 2/25 | Loss: 0.00345691
Iteration 3/25 | Loss: 0.00345691
Iteration 4/25 | Loss: 0.00345691
Iteration 5/25 | Loss: 0.00345691
Iteration 6/25 | Loss: 0.00345691
Iteration 7/25 | Loss: 0.00345690
Iteration 8/25 | Loss: 0.00345690
Iteration 9/25 | Loss: 0.00345690
Iteration 10/25 | Loss: 0.00345690
Iteration 11/25 | Loss: 0.00345690
Iteration 12/25 | Loss: 0.00345690
Iteration 13/25 | Loss: 0.00345690
Iteration 14/25 | Loss: 0.00345690
Iteration 15/25 | Loss: 0.00345690
Iteration 16/25 | Loss: 0.00345690
Iteration 17/25 | Loss: 0.00345690
Iteration 18/25 | Loss: 0.00345690
Iteration 19/25 | Loss: 0.00345690
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003456903388723731, 0.003456903388723731, 0.003456903388723731, 0.003456903388723731, 0.003456903388723731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003456903388723731

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00345690
Iteration 2/1000 | Loss: 0.00035271
Iteration 3/1000 | Loss: 0.00042238
Iteration 4/1000 | Loss: 0.00046021
Iteration 5/1000 | Loss: 0.00065845
Iteration 6/1000 | Loss: 0.00090749
Iteration 7/1000 | Loss: 0.00246187
Iteration 8/1000 | Loss: 0.00287614
Iteration 9/1000 | Loss: 0.00070933
Iteration 10/1000 | Loss: 0.00033243
Iteration 11/1000 | Loss: 0.00057810
Iteration 12/1000 | Loss: 0.00039714
Iteration 13/1000 | Loss: 0.00065219
Iteration 14/1000 | Loss: 0.00031068
Iteration 15/1000 | Loss: 0.00117785
Iteration 16/1000 | Loss: 0.00033033
Iteration 17/1000 | Loss: 0.00027744
Iteration 18/1000 | Loss: 0.00021123
Iteration 19/1000 | Loss: 0.00026870
Iteration 20/1000 | Loss: 0.00028549
Iteration 21/1000 | Loss: 0.00027130
Iteration 22/1000 | Loss: 0.00037568
Iteration 23/1000 | Loss: 0.00025164
Iteration 24/1000 | Loss: 0.00022649
Iteration 25/1000 | Loss: 0.00027384
Iteration 26/1000 | Loss: 0.00024891
Iteration 27/1000 | Loss: 0.00013931
Iteration 28/1000 | Loss: 0.00014845
Iteration 29/1000 | Loss: 0.00078597
Iteration 30/1000 | Loss: 0.00411342
Iteration 31/1000 | Loss: 0.00168693
Iteration 32/1000 | Loss: 0.00059310
Iteration 33/1000 | Loss: 0.00014944
Iteration 34/1000 | Loss: 0.00014399
Iteration 35/1000 | Loss: 0.00057726
Iteration 36/1000 | Loss: 0.00411977
Iteration 37/1000 | Loss: 0.00607840
Iteration 38/1000 | Loss: 0.00221280
Iteration 39/1000 | Loss: 0.00205561
Iteration 40/1000 | Loss: 0.00023748
Iteration 41/1000 | Loss: 0.00017175
Iteration 42/1000 | Loss: 0.00014347
Iteration 43/1000 | Loss: 0.00045810
Iteration 44/1000 | Loss: 0.00060995
Iteration 45/1000 | Loss: 0.00012492
Iteration 46/1000 | Loss: 0.00067063
Iteration 47/1000 | Loss: 0.00032705
Iteration 48/1000 | Loss: 0.00015543
Iteration 49/1000 | Loss: 0.00022227
Iteration 50/1000 | Loss: 0.00090341
Iteration 51/1000 | Loss: 0.00082524
Iteration 52/1000 | Loss: 0.00021331
Iteration 53/1000 | Loss: 0.00010713
Iteration 54/1000 | Loss: 0.00072716
Iteration 55/1000 | Loss: 0.00059228
Iteration 56/1000 | Loss: 0.00022850
Iteration 57/1000 | Loss: 0.00010422
Iteration 58/1000 | Loss: 0.00027261
Iteration 59/1000 | Loss: 0.00126368
Iteration 60/1000 | Loss: 0.00066257
Iteration 61/1000 | Loss: 0.00026837
Iteration 62/1000 | Loss: 0.00024154
Iteration 63/1000 | Loss: 0.00025164
Iteration 64/1000 | Loss: 0.00019372
Iteration 65/1000 | Loss: 0.00057322
Iteration 66/1000 | Loss: 0.00167790
Iteration 67/1000 | Loss: 0.00108197
Iteration 68/1000 | Loss: 0.00032871
Iteration 69/1000 | Loss: 0.00059657
Iteration 70/1000 | Loss: 0.00073104
Iteration 71/1000 | Loss: 0.00094731
Iteration 72/1000 | Loss: 0.00016395
Iteration 73/1000 | Loss: 0.00070589
Iteration 74/1000 | Loss: 0.00070040
Iteration 75/1000 | Loss: 0.00009471
Iteration 76/1000 | Loss: 0.00008720
Iteration 77/1000 | Loss: 0.00032775
Iteration 78/1000 | Loss: 0.00062288
Iteration 79/1000 | Loss: 0.00032832
Iteration 80/1000 | Loss: 0.00070619
Iteration 81/1000 | Loss: 0.00044465
Iteration 82/1000 | Loss: 0.00008759
Iteration 83/1000 | Loss: 0.00066107
Iteration 84/1000 | Loss: 0.00009316
Iteration 85/1000 | Loss: 0.00107809
Iteration 86/1000 | Loss: 0.00036864
Iteration 87/1000 | Loss: 0.00077086
Iteration 88/1000 | Loss: 0.00061384
Iteration 89/1000 | Loss: 0.00037868
Iteration 90/1000 | Loss: 0.00008264
Iteration 91/1000 | Loss: 0.00066917
Iteration 92/1000 | Loss: 0.00068115
Iteration 93/1000 | Loss: 0.00062553
Iteration 94/1000 | Loss: 0.00008241
Iteration 95/1000 | Loss: 0.00007516
Iteration 96/1000 | Loss: 0.00062509
Iteration 97/1000 | Loss: 0.00109680
Iteration 98/1000 | Loss: 0.00057321
Iteration 99/1000 | Loss: 0.00076053
Iteration 100/1000 | Loss: 0.00103664
Iteration 101/1000 | Loss: 0.00081054
Iteration 102/1000 | Loss: 0.00007583
Iteration 103/1000 | Loss: 0.00070216
Iteration 104/1000 | Loss: 0.00084562
Iteration 105/1000 | Loss: 0.00019750
Iteration 106/1000 | Loss: 0.00040424
Iteration 107/1000 | Loss: 0.00019004
Iteration 108/1000 | Loss: 0.00007028
Iteration 109/1000 | Loss: 0.00006346
Iteration 110/1000 | Loss: 0.00005968
Iteration 111/1000 | Loss: 0.00280019
Iteration 112/1000 | Loss: 0.00222138
Iteration 113/1000 | Loss: 0.00037653
Iteration 114/1000 | Loss: 0.00032125
Iteration 115/1000 | Loss: 0.00008882
Iteration 116/1000 | Loss: 0.00037268
Iteration 117/1000 | Loss: 0.00044697
Iteration 118/1000 | Loss: 0.00018828
Iteration 119/1000 | Loss: 0.00005899
Iteration 120/1000 | Loss: 0.00157414
Iteration 121/1000 | Loss: 0.00212826
Iteration 122/1000 | Loss: 0.00097466
Iteration 123/1000 | Loss: 0.00006324
Iteration 124/1000 | Loss: 0.00005432
Iteration 125/1000 | Loss: 0.00073672
Iteration 126/1000 | Loss: 0.00065479
Iteration 127/1000 | Loss: 0.00053226
Iteration 128/1000 | Loss: 0.00006410
Iteration 129/1000 | Loss: 0.00005249
Iteration 130/1000 | Loss: 0.00033230
Iteration 131/1000 | Loss: 0.00041552
Iteration 132/1000 | Loss: 0.00081891
Iteration 133/1000 | Loss: 0.00035756
Iteration 134/1000 | Loss: 0.00005753
Iteration 135/1000 | Loss: 0.00004333
Iteration 136/1000 | Loss: 0.00032492
Iteration 137/1000 | Loss: 0.00022345
Iteration 138/1000 | Loss: 0.00007951
Iteration 139/1000 | Loss: 0.00020474
Iteration 140/1000 | Loss: 0.00096406
Iteration 141/1000 | Loss: 0.00017650
Iteration 142/1000 | Loss: 0.00003968
Iteration 143/1000 | Loss: 0.00003409
Iteration 144/1000 | Loss: 0.00003121
Iteration 145/1000 | Loss: 0.00002967
Iteration 146/1000 | Loss: 0.00026743
Iteration 147/1000 | Loss: 0.00015539
Iteration 148/1000 | Loss: 0.00025314
Iteration 149/1000 | Loss: 0.00017623
Iteration 150/1000 | Loss: 0.00006220
Iteration 151/1000 | Loss: 0.00034345
Iteration 152/1000 | Loss: 0.00028493
Iteration 153/1000 | Loss: 0.00003867
Iteration 154/1000 | Loss: 0.00053552
Iteration 155/1000 | Loss: 0.00056774
Iteration 156/1000 | Loss: 0.00004641
Iteration 157/1000 | Loss: 0.00003545
Iteration 158/1000 | Loss: 0.00003105
Iteration 159/1000 | Loss: 0.00047481
Iteration 160/1000 | Loss: 0.00003645
Iteration 161/1000 | Loss: 0.00003080
Iteration 162/1000 | Loss: 0.00002743
Iteration 163/1000 | Loss: 0.00002553
Iteration 164/1000 | Loss: 0.00002470
Iteration 165/1000 | Loss: 0.00002398
Iteration 166/1000 | Loss: 0.00030293
Iteration 167/1000 | Loss: 0.00042077
Iteration 168/1000 | Loss: 0.00029050
Iteration 169/1000 | Loss: 0.00028062
Iteration 170/1000 | Loss: 0.00028186
Iteration 171/1000 | Loss: 0.00024827
Iteration 172/1000 | Loss: 0.00070427
Iteration 173/1000 | Loss: 0.00004019
Iteration 174/1000 | Loss: 0.00002960
Iteration 175/1000 | Loss: 0.00002513
Iteration 176/1000 | Loss: 0.00002272
Iteration 177/1000 | Loss: 0.00002164
Iteration 178/1000 | Loss: 0.00002089
Iteration 179/1000 | Loss: 0.00002049
Iteration 180/1000 | Loss: 0.00002002
Iteration 181/1000 | Loss: 0.00001970
Iteration 182/1000 | Loss: 0.00001942
Iteration 183/1000 | Loss: 0.00001920
Iteration 184/1000 | Loss: 0.00001900
Iteration 185/1000 | Loss: 0.00001897
Iteration 186/1000 | Loss: 0.00001886
Iteration 187/1000 | Loss: 0.00001886
Iteration 188/1000 | Loss: 0.00001886
Iteration 189/1000 | Loss: 0.00001885
Iteration 190/1000 | Loss: 0.00001885
Iteration 191/1000 | Loss: 0.00001880
Iteration 192/1000 | Loss: 0.00004459
Iteration 193/1000 | Loss: 0.00058393
Iteration 194/1000 | Loss: 0.00013869
Iteration 195/1000 | Loss: 0.00002347
Iteration 196/1000 | Loss: 0.00002177
Iteration 197/1000 | Loss: 0.00003870
Iteration 198/1000 | Loss: 0.00002390
Iteration 199/1000 | Loss: 0.00002086
Iteration 200/1000 | Loss: 0.00003955
Iteration 201/1000 | Loss: 0.00003808
Iteration 202/1000 | Loss: 0.00003877
Iteration 203/1000 | Loss: 0.00003769
Iteration 204/1000 | Loss: 0.00003782
Iteration 205/1000 | Loss: 0.00003964
Iteration 206/1000 | Loss: 0.00002287
Iteration 207/1000 | Loss: 0.00002151
Iteration 208/1000 | Loss: 0.00002055
Iteration 209/1000 | Loss: 0.00003214
Iteration 210/1000 | Loss: 0.00003027
Iteration 211/1000 | Loss: 0.00002133
Iteration 212/1000 | Loss: 0.00002033
Iteration 213/1000 | Loss: 0.00002006
Iteration 214/1000 | Loss: 0.00001989
Iteration 215/1000 | Loss: 0.00001988
Iteration 216/1000 | Loss: 0.00001988
Iteration 217/1000 | Loss: 0.00001973
Iteration 218/1000 | Loss: 0.00001955
Iteration 219/1000 | Loss: 0.00001934
Iteration 220/1000 | Loss: 0.00001899
Iteration 221/1000 | Loss: 0.00001874
Iteration 222/1000 | Loss: 0.00001844
Iteration 223/1000 | Loss: 0.00001838
Iteration 224/1000 | Loss: 0.00001835
Iteration 225/1000 | Loss: 0.00001826
Iteration 226/1000 | Loss: 0.00001823
Iteration 227/1000 | Loss: 0.00001822
Iteration 228/1000 | Loss: 0.00001821
Iteration 229/1000 | Loss: 0.00001821
Iteration 230/1000 | Loss: 0.00001820
Iteration 231/1000 | Loss: 0.00001819
Iteration 232/1000 | Loss: 0.00001819
Iteration 233/1000 | Loss: 0.00001819
Iteration 234/1000 | Loss: 0.00001818
Iteration 235/1000 | Loss: 0.00001818
Iteration 236/1000 | Loss: 0.00001817
Iteration 237/1000 | Loss: 0.00001816
Iteration 238/1000 | Loss: 0.00001815
Iteration 239/1000 | Loss: 0.00001815
Iteration 240/1000 | Loss: 0.00001815
Iteration 241/1000 | Loss: 0.00001814
Iteration 242/1000 | Loss: 0.00001814
Iteration 243/1000 | Loss: 0.00001814
Iteration 244/1000 | Loss: 0.00001814
Iteration 245/1000 | Loss: 0.00001814
Iteration 246/1000 | Loss: 0.00001813
Iteration 247/1000 | Loss: 0.00001813
Iteration 248/1000 | Loss: 0.00001813
Iteration 249/1000 | Loss: 0.00001813
Iteration 250/1000 | Loss: 0.00001813
Iteration 251/1000 | Loss: 0.00001813
Iteration 252/1000 | Loss: 0.00001812
Iteration 253/1000 | Loss: 0.00001812
Iteration 254/1000 | Loss: 0.00001811
Iteration 255/1000 | Loss: 0.00001811
Iteration 256/1000 | Loss: 0.00001810
Iteration 257/1000 | Loss: 0.00001810
Iteration 258/1000 | Loss: 0.00001809
Iteration 259/1000 | Loss: 0.00001809
Iteration 260/1000 | Loss: 0.00001809
Iteration 261/1000 | Loss: 0.00001808
Iteration 262/1000 | Loss: 0.00001808
Iteration 263/1000 | Loss: 0.00001807
Iteration 264/1000 | Loss: 0.00001807
Iteration 265/1000 | Loss: 0.00001807
Iteration 266/1000 | Loss: 0.00001807
Iteration 267/1000 | Loss: 0.00001807
Iteration 268/1000 | Loss: 0.00001807
Iteration 269/1000 | Loss: 0.00001807
Iteration 270/1000 | Loss: 0.00001807
Iteration 271/1000 | Loss: 0.00001807
Iteration 272/1000 | Loss: 0.00001807
Iteration 273/1000 | Loss: 0.00001806
Iteration 274/1000 | Loss: 0.00001806
Iteration 275/1000 | Loss: 0.00001806
Iteration 276/1000 | Loss: 0.00001806
Iteration 277/1000 | Loss: 0.00001806
Iteration 278/1000 | Loss: 0.00001806
Iteration 279/1000 | Loss: 0.00001806
Iteration 280/1000 | Loss: 0.00001806
Iteration 281/1000 | Loss: 0.00001805
Iteration 282/1000 | Loss: 0.00001805
Iteration 283/1000 | Loss: 0.00001805
Iteration 284/1000 | Loss: 0.00001805
Iteration 285/1000 | Loss: 0.00001805
Iteration 286/1000 | Loss: 0.00001805
Iteration 287/1000 | Loss: 0.00001805
Iteration 288/1000 | Loss: 0.00001804
Iteration 289/1000 | Loss: 0.00001804
Iteration 290/1000 | Loss: 0.00001804
Iteration 291/1000 | Loss: 0.00001803
Iteration 292/1000 | Loss: 0.00001803
Iteration 293/1000 | Loss: 0.00001803
Iteration 294/1000 | Loss: 0.00001802
Iteration 295/1000 | Loss: 0.00001802
Iteration 296/1000 | Loss: 0.00001802
Iteration 297/1000 | Loss: 0.00001802
Iteration 298/1000 | Loss: 0.00001801
Iteration 299/1000 | Loss: 0.00001801
Iteration 300/1000 | Loss: 0.00001801
Iteration 301/1000 | Loss: 0.00001801
Iteration 302/1000 | Loss: 0.00001801
Iteration 303/1000 | Loss: 0.00001801
Iteration 304/1000 | Loss: 0.00001801
Iteration 305/1000 | Loss: 0.00001800
Iteration 306/1000 | Loss: 0.00001800
Iteration 307/1000 | Loss: 0.00001800
Iteration 308/1000 | Loss: 0.00001800
Iteration 309/1000 | Loss: 0.00001800
Iteration 310/1000 | Loss: 0.00001800
Iteration 311/1000 | Loss: 0.00001800
Iteration 312/1000 | Loss: 0.00001800
Iteration 313/1000 | Loss: 0.00001800
Iteration 314/1000 | Loss: 0.00001800
Iteration 315/1000 | Loss: 0.00001799
Iteration 316/1000 | Loss: 0.00001799
Iteration 317/1000 | Loss: 0.00001799
Iteration 318/1000 | Loss: 0.00001799
Iteration 319/1000 | Loss: 0.00001799
Iteration 320/1000 | Loss: 0.00001799
Iteration 321/1000 | Loss: 0.00001798
Iteration 322/1000 | Loss: 0.00001798
Iteration 323/1000 | Loss: 0.00001798
Iteration 324/1000 | Loss: 0.00001798
Iteration 325/1000 | Loss: 0.00001798
Iteration 326/1000 | Loss: 0.00001798
Iteration 327/1000 | Loss: 0.00001798
Iteration 328/1000 | Loss: 0.00001798
Iteration 329/1000 | Loss: 0.00001798
Iteration 330/1000 | Loss: 0.00001798
Iteration 331/1000 | Loss: 0.00001798
Iteration 332/1000 | Loss: 0.00001797
Iteration 333/1000 | Loss: 0.00001797
Iteration 334/1000 | Loss: 0.00001797
Iteration 335/1000 | Loss: 0.00001796
Iteration 336/1000 | Loss: 0.00001796
Iteration 337/1000 | Loss: 0.00001796
Iteration 338/1000 | Loss: 0.00001796
Iteration 339/1000 | Loss: 0.00001795
Iteration 340/1000 | Loss: 0.00001795
Iteration 341/1000 | Loss: 0.00001795
Iteration 342/1000 | Loss: 0.00001794
Iteration 343/1000 | Loss: 0.00001794
Iteration 344/1000 | Loss: 0.00001794
Iteration 345/1000 | Loss: 0.00001793
Iteration 346/1000 | Loss: 0.00001793
Iteration 347/1000 | Loss: 0.00001793
Iteration 348/1000 | Loss: 0.00001792
Iteration 349/1000 | Loss: 0.00001792
Iteration 350/1000 | Loss: 0.00001792
Iteration 351/1000 | Loss: 0.00001791
Iteration 352/1000 | Loss: 0.00001791
Iteration 353/1000 | Loss: 0.00001791
Iteration 354/1000 | Loss: 0.00001791
Iteration 355/1000 | Loss: 0.00001790
Iteration 356/1000 | Loss: 0.00001790
Iteration 357/1000 | Loss: 0.00001790
Iteration 358/1000 | Loss: 0.00001790
Iteration 359/1000 | Loss: 0.00001790
Iteration 360/1000 | Loss: 0.00001790
Iteration 361/1000 | Loss: 0.00001790
Iteration 362/1000 | Loss: 0.00001790
Iteration 363/1000 | Loss: 0.00001790
Iteration 364/1000 | Loss: 0.00001790
Iteration 365/1000 | Loss: 0.00001789
Iteration 366/1000 | Loss: 0.00001789
Iteration 367/1000 | Loss: 0.00001789
Iteration 368/1000 | Loss: 0.00001789
Iteration 369/1000 | Loss: 0.00001789
Iteration 370/1000 | Loss: 0.00001789
Iteration 371/1000 | Loss: 0.00001788
Iteration 372/1000 | Loss: 0.00001788
Iteration 373/1000 | Loss: 0.00001788
Iteration 374/1000 | Loss: 0.00001788
Iteration 375/1000 | Loss: 0.00001788
Iteration 376/1000 | Loss: 0.00001788
Iteration 377/1000 | Loss: 0.00001787
Iteration 378/1000 | Loss: 0.00001787
Iteration 379/1000 | Loss: 0.00001787
Iteration 380/1000 | Loss: 0.00001787
Iteration 381/1000 | Loss: 0.00001787
Iteration 382/1000 | Loss: 0.00001787
Iteration 383/1000 | Loss: 0.00001787
Iteration 384/1000 | Loss: 0.00001786
Iteration 385/1000 | Loss: 0.00001786
Iteration 386/1000 | Loss: 0.00001786
Iteration 387/1000 | Loss: 0.00001786
Iteration 388/1000 | Loss: 0.00001786
Iteration 389/1000 | Loss: 0.00001786
Iteration 390/1000 | Loss: 0.00001786
Iteration 391/1000 | Loss: 0.00001786
Iteration 392/1000 | Loss: 0.00001786
Iteration 393/1000 | Loss: 0.00001786
Iteration 394/1000 | Loss: 0.00001786
Iteration 395/1000 | Loss: 0.00001786
Iteration 396/1000 | Loss: 0.00001786
Iteration 397/1000 | Loss: 0.00001785
Iteration 398/1000 | Loss: 0.00001785
Iteration 399/1000 | Loss: 0.00001785
Iteration 400/1000 | Loss: 0.00001785
Iteration 401/1000 | Loss: 0.00001785
Iteration 402/1000 | Loss: 0.00001785
Iteration 403/1000 | Loss: 0.00001785
Iteration 404/1000 | Loss: 0.00001785
Iteration 405/1000 | Loss: 0.00001785
Iteration 406/1000 | Loss: 0.00001785
Iteration 407/1000 | Loss: 0.00001785
Iteration 408/1000 | Loss: 0.00001785
Iteration 409/1000 | Loss: 0.00001785
Iteration 410/1000 | Loss: 0.00001785
Iteration 411/1000 | Loss: 0.00001785
Iteration 412/1000 | Loss: 0.00001785
Iteration 413/1000 | Loss: 0.00001784
Iteration 414/1000 | Loss: 0.00001784
Iteration 415/1000 | Loss: 0.00001784
Iteration 416/1000 | Loss: 0.00001784
Iteration 417/1000 | Loss: 0.00001784
Iteration 418/1000 | Loss: 0.00001784
Iteration 419/1000 | Loss: 0.00001784
Iteration 420/1000 | Loss: 0.00001784
Iteration 421/1000 | Loss: 0.00001784
Iteration 422/1000 | Loss: 0.00001784
Iteration 423/1000 | Loss: 0.00001784
Iteration 424/1000 | Loss: 0.00001783
Iteration 425/1000 | Loss: 0.00001783
Iteration 426/1000 | Loss: 0.00001783
Iteration 427/1000 | Loss: 0.00001783
Iteration 428/1000 | Loss: 0.00001783
Iteration 429/1000 | Loss: 0.00001783
Iteration 430/1000 | Loss: 0.00001783
Iteration 431/1000 | Loss: 0.00001783
Iteration 432/1000 | Loss: 0.00001782
Iteration 433/1000 | Loss: 0.00001782
Iteration 434/1000 | Loss: 0.00001782
Iteration 435/1000 | Loss: 0.00001782
Iteration 436/1000 | Loss: 0.00001782
Iteration 437/1000 | Loss: 0.00001782
Iteration 438/1000 | Loss: 0.00001782
Iteration 439/1000 | Loss: 0.00001782
Iteration 440/1000 | Loss: 0.00001782
Iteration 441/1000 | Loss: 0.00001782
Iteration 442/1000 | Loss: 0.00001782
Iteration 443/1000 | Loss: 0.00001782
Iteration 444/1000 | Loss: 0.00001782
Iteration 445/1000 | Loss: 0.00001782
Iteration 446/1000 | Loss: 0.00001782
Iteration 447/1000 | Loss: 0.00001782
Iteration 448/1000 | Loss: 0.00001782
Iteration 449/1000 | Loss: 0.00001782
Iteration 450/1000 | Loss: 0.00001782
Iteration 451/1000 | Loss: 0.00001782
Iteration 452/1000 | Loss: 0.00001782
Iteration 453/1000 | Loss: 0.00001782
Iteration 454/1000 | Loss: 0.00001781
Iteration 455/1000 | Loss: 0.00001781
Iteration 456/1000 | Loss: 0.00001781
Iteration 457/1000 | Loss: 0.00001781
Iteration 458/1000 | Loss: 0.00001781
Iteration 459/1000 | Loss: 0.00001781
Iteration 460/1000 | Loss: 0.00001781
Iteration 461/1000 | Loss: 0.00001781
Iteration 462/1000 | Loss: 0.00001781
Iteration 463/1000 | Loss: 0.00001781
Iteration 464/1000 | Loss: 0.00001781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 464. Stopping optimization.
Last 5 losses: [1.7814825696405023e-05, 1.7814825696405023e-05, 1.7814825696405023e-05, 1.7814825696405023e-05, 1.7814825696405023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7814825696405023e-05

Optimization complete. Final v2v error: 3.4557876586914062 mm

Highest mean error: 5.4476542472839355 mm for frame 54

Lowest mean error: 2.8994970321655273 mm for frame 156

Saving results

Total time: 379.6941125392914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026490
Iteration 2/25 | Loss: 0.01026490
Iteration 3/25 | Loss: 0.00258646
Iteration 4/25 | Loss: 0.00211530
Iteration 5/25 | Loss: 0.00191719
Iteration 6/25 | Loss: 0.00176124
Iteration 7/25 | Loss: 0.00163611
Iteration 8/25 | Loss: 0.00159140
Iteration 9/25 | Loss: 0.00165543
Iteration 10/25 | Loss: 0.00148227
Iteration 11/25 | Loss: 0.00140158
Iteration 12/25 | Loss: 0.00139523
Iteration 13/25 | Loss: 0.00139384
Iteration 14/25 | Loss: 0.00135607
Iteration 15/25 | Loss: 0.00133907
Iteration 16/25 | Loss: 0.00132103
Iteration 17/25 | Loss: 0.00132302
Iteration 18/25 | Loss: 0.00131784
Iteration 19/25 | Loss: 0.00131099
Iteration 20/25 | Loss: 0.00130938
Iteration 21/25 | Loss: 0.00130891
Iteration 22/25 | Loss: 0.00130874
Iteration 23/25 | Loss: 0.00130873
Iteration 24/25 | Loss: 0.00130872
Iteration 25/25 | Loss: 0.00130872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37296259
Iteration 2/25 | Loss: 0.00210734
Iteration 3/25 | Loss: 0.00133042
Iteration 4/25 | Loss: 0.00133042
Iteration 5/25 | Loss: 0.00133042
Iteration 6/25 | Loss: 0.00133042
Iteration 7/25 | Loss: 0.00133042
Iteration 8/25 | Loss: 0.00133042
Iteration 9/25 | Loss: 0.00133042
Iteration 10/25 | Loss: 0.00133042
Iteration 11/25 | Loss: 0.00133042
Iteration 12/25 | Loss: 0.00133042
Iteration 13/25 | Loss: 0.00133042
Iteration 14/25 | Loss: 0.00133042
Iteration 15/25 | Loss: 0.00133042
Iteration 16/25 | Loss: 0.00133042
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013304187450557947, 0.0013304187450557947, 0.0013304187450557947, 0.0013304187450557947, 0.0013304187450557947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013304187450557947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133042
Iteration 2/1000 | Loss: 0.00074066
Iteration 3/1000 | Loss: 0.00109067
Iteration 4/1000 | Loss: 0.00020324
Iteration 5/1000 | Loss: 0.00012294
Iteration 6/1000 | Loss: 0.00020791
Iteration 7/1000 | Loss: 0.00002608
Iteration 8/1000 | Loss: 0.00009390
Iteration 9/1000 | Loss: 0.00105243
Iteration 10/1000 | Loss: 0.00005392
Iteration 11/1000 | Loss: 0.00005197
Iteration 12/1000 | Loss: 0.00009298
Iteration 13/1000 | Loss: 0.00052980
Iteration 14/1000 | Loss: 0.00046334
Iteration 15/1000 | Loss: 0.00008594
Iteration 16/1000 | Loss: 0.00002837
Iteration 17/1000 | Loss: 0.00002256
Iteration 18/1000 | Loss: 0.00008165
Iteration 19/1000 | Loss: 0.00017403
Iteration 20/1000 | Loss: 0.00004114
Iteration 21/1000 | Loss: 0.00002115
Iteration 22/1000 | Loss: 0.00002086
Iteration 23/1000 | Loss: 0.00002053
Iteration 24/1000 | Loss: 0.00002025
Iteration 25/1000 | Loss: 0.00002013
Iteration 26/1000 | Loss: 0.00002012
Iteration 27/1000 | Loss: 0.00002007
Iteration 28/1000 | Loss: 0.00016150
Iteration 29/1000 | Loss: 0.00002295
Iteration 30/1000 | Loss: 0.00011085
Iteration 31/1000 | Loss: 0.00002048
Iteration 32/1000 | Loss: 0.00011952
Iteration 33/1000 | Loss: 0.00085080
Iteration 34/1000 | Loss: 0.00001942
Iteration 35/1000 | Loss: 0.00006688
Iteration 36/1000 | Loss: 0.00001875
Iteration 37/1000 | Loss: 0.00007267
Iteration 38/1000 | Loss: 0.00015037
Iteration 39/1000 | Loss: 0.00002280
Iteration 40/1000 | Loss: 0.00001849
Iteration 41/1000 | Loss: 0.00001831
Iteration 42/1000 | Loss: 0.00001830
Iteration 43/1000 | Loss: 0.00001825
Iteration 44/1000 | Loss: 0.00001823
Iteration 45/1000 | Loss: 0.00001823
Iteration 46/1000 | Loss: 0.00001819
Iteration 47/1000 | Loss: 0.00001818
Iteration 48/1000 | Loss: 0.00005367
Iteration 49/1000 | Loss: 0.00001825
Iteration 50/1000 | Loss: 0.00001812
Iteration 51/1000 | Loss: 0.00001812
Iteration 52/1000 | Loss: 0.00001812
Iteration 53/1000 | Loss: 0.00001811
Iteration 54/1000 | Loss: 0.00001811
Iteration 55/1000 | Loss: 0.00001811
Iteration 56/1000 | Loss: 0.00001811
Iteration 57/1000 | Loss: 0.00001811
Iteration 58/1000 | Loss: 0.00001811
Iteration 59/1000 | Loss: 0.00001811
Iteration 60/1000 | Loss: 0.00001811
Iteration 61/1000 | Loss: 0.00001811
Iteration 62/1000 | Loss: 0.00001810
Iteration 63/1000 | Loss: 0.00001810
Iteration 64/1000 | Loss: 0.00001809
Iteration 65/1000 | Loss: 0.00001809
Iteration 66/1000 | Loss: 0.00001808
Iteration 67/1000 | Loss: 0.00001808
Iteration 68/1000 | Loss: 0.00001808
Iteration 69/1000 | Loss: 0.00001808
Iteration 70/1000 | Loss: 0.00001808
Iteration 71/1000 | Loss: 0.00001808
Iteration 72/1000 | Loss: 0.00001807
Iteration 73/1000 | Loss: 0.00001807
Iteration 74/1000 | Loss: 0.00001807
Iteration 75/1000 | Loss: 0.00001806
Iteration 76/1000 | Loss: 0.00001806
Iteration 77/1000 | Loss: 0.00001805
Iteration 78/1000 | Loss: 0.00001805
Iteration 79/1000 | Loss: 0.00001805
Iteration 80/1000 | Loss: 0.00001804
Iteration 81/1000 | Loss: 0.00001804
Iteration 82/1000 | Loss: 0.00001804
Iteration 83/1000 | Loss: 0.00001804
Iteration 84/1000 | Loss: 0.00001804
Iteration 85/1000 | Loss: 0.00001804
Iteration 86/1000 | Loss: 0.00001804
Iteration 87/1000 | Loss: 0.00001804
Iteration 88/1000 | Loss: 0.00001804
Iteration 89/1000 | Loss: 0.00001803
Iteration 90/1000 | Loss: 0.00001803
Iteration 91/1000 | Loss: 0.00001803
Iteration 92/1000 | Loss: 0.00001803
Iteration 93/1000 | Loss: 0.00001803
Iteration 94/1000 | Loss: 0.00001803
Iteration 95/1000 | Loss: 0.00001802
Iteration 96/1000 | Loss: 0.00001802
Iteration 97/1000 | Loss: 0.00001802
Iteration 98/1000 | Loss: 0.00001802
Iteration 99/1000 | Loss: 0.00001802
Iteration 100/1000 | Loss: 0.00001802
Iteration 101/1000 | Loss: 0.00001802
Iteration 102/1000 | Loss: 0.00001801
Iteration 103/1000 | Loss: 0.00001801
Iteration 104/1000 | Loss: 0.00001801
Iteration 105/1000 | Loss: 0.00001800
Iteration 106/1000 | Loss: 0.00001799
Iteration 107/1000 | Loss: 0.00001799
Iteration 108/1000 | Loss: 0.00001799
Iteration 109/1000 | Loss: 0.00001799
Iteration 110/1000 | Loss: 0.00001799
Iteration 111/1000 | Loss: 0.00001799
Iteration 112/1000 | Loss: 0.00001799
Iteration 113/1000 | Loss: 0.00001798
Iteration 114/1000 | Loss: 0.00001798
Iteration 115/1000 | Loss: 0.00001798
Iteration 116/1000 | Loss: 0.00001798
Iteration 117/1000 | Loss: 0.00001797
Iteration 118/1000 | Loss: 0.00001797
Iteration 119/1000 | Loss: 0.00001797
Iteration 120/1000 | Loss: 0.00001796
Iteration 121/1000 | Loss: 0.00001796
Iteration 122/1000 | Loss: 0.00001796
Iteration 123/1000 | Loss: 0.00001795
Iteration 124/1000 | Loss: 0.00001795
Iteration 125/1000 | Loss: 0.00001795
Iteration 126/1000 | Loss: 0.00001794
Iteration 127/1000 | Loss: 0.00001793
Iteration 128/1000 | Loss: 0.00001792
Iteration 129/1000 | Loss: 0.00001792
Iteration 130/1000 | Loss: 0.00001791
Iteration 131/1000 | Loss: 0.00001791
Iteration 132/1000 | Loss: 0.00001791
Iteration 133/1000 | Loss: 0.00001791
Iteration 134/1000 | Loss: 0.00001791
Iteration 135/1000 | Loss: 0.00001790
Iteration 136/1000 | Loss: 0.00001790
Iteration 137/1000 | Loss: 0.00001789
Iteration 138/1000 | Loss: 0.00001789
Iteration 139/1000 | Loss: 0.00001789
Iteration 140/1000 | Loss: 0.00001789
Iteration 141/1000 | Loss: 0.00001789
Iteration 142/1000 | Loss: 0.00001789
Iteration 143/1000 | Loss: 0.00001789
Iteration 144/1000 | Loss: 0.00001788
Iteration 145/1000 | Loss: 0.00001788
Iteration 146/1000 | Loss: 0.00001788
Iteration 147/1000 | Loss: 0.00001788
Iteration 148/1000 | Loss: 0.00001788
Iteration 149/1000 | Loss: 0.00001788
Iteration 150/1000 | Loss: 0.00001788
Iteration 151/1000 | Loss: 0.00001787
Iteration 152/1000 | Loss: 0.00001787
Iteration 153/1000 | Loss: 0.00001787
Iteration 154/1000 | Loss: 0.00001787
Iteration 155/1000 | Loss: 0.00001787
Iteration 156/1000 | Loss: 0.00001787
Iteration 157/1000 | Loss: 0.00001786
Iteration 158/1000 | Loss: 0.00001786
Iteration 159/1000 | Loss: 0.00001785
Iteration 160/1000 | Loss: 0.00001785
Iteration 161/1000 | Loss: 0.00001785
Iteration 162/1000 | Loss: 0.00001785
Iteration 163/1000 | Loss: 0.00001785
Iteration 164/1000 | Loss: 0.00001785
Iteration 165/1000 | Loss: 0.00001785
Iteration 166/1000 | Loss: 0.00001785
Iteration 167/1000 | Loss: 0.00001785
Iteration 168/1000 | Loss: 0.00001785
Iteration 169/1000 | Loss: 0.00001785
Iteration 170/1000 | Loss: 0.00001784
Iteration 171/1000 | Loss: 0.00001784
Iteration 172/1000 | Loss: 0.00001784
Iteration 173/1000 | Loss: 0.00001784
Iteration 174/1000 | Loss: 0.00001784
Iteration 175/1000 | Loss: 0.00001784
Iteration 176/1000 | Loss: 0.00001784
Iteration 177/1000 | Loss: 0.00001784
Iteration 178/1000 | Loss: 0.00001784
Iteration 179/1000 | Loss: 0.00001784
Iteration 180/1000 | Loss: 0.00001784
Iteration 181/1000 | Loss: 0.00001784
Iteration 182/1000 | Loss: 0.00001784
Iteration 183/1000 | Loss: 0.00001784
Iteration 184/1000 | Loss: 0.00001783
Iteration 185/1000 | Loss: 0.00001783
Iteration 186/1000 | Loss: 0.00001783
Iteration 187/1000 | Loss: 0.00001783
Iteration 188/1000 | Loss: 0.00001782
Iteration 189/1000 | Loss: 0.00001782
Iteration 190/1000 | Loss: 0.00001782
Iteration 191/1000 | Loss: 0.00001782
Iteration 192/1000 | Loss: 0.00001782
Iteration 193/1000 | Loss: 0.00001782
Iteration 194/1000 | Loss: 0.00001782
Iteration 195/1000 | Loss: 0.00001782
Iteration 196/1000 | Loss: 0.00001782
Iteration 197/1000 | Loss: 0.00001781
Iteration 198/1000 | Loss: 0.00001781
Iteration 199/1000 | Loss: 0.00001781
Iteration 200/1000 | Loss: 0.00001781
Iteration 201/1000 | Loss: 0.00001781
Iteration 202/1000 | Loss: 0.00001781
Iteration 203/1000 | Loss: 0.00001781
Iteration 204/1000 | Loss: 0.00001780
Iteration 205/1000 | Loss: 0.00001780
Iteration 206/1000 | Loss: 0.00001780
Iteration 207/1000 | Loss: 0.00001780
Iteration 208/1000 | Loss: 0.00001780
Iteration 209/1000 | Loss: 0.00001780
Iteration 210/1000 | Loss: 0.00001779
Iteration 211/1000 | Loss: 0.00001779
Iteration 212/1000 | Loss: 0.00001779
Iteration 213/1000 | Loss: 0.00001779
Iteration 214/1000 | Loss: 0.00001779
Iteration 215/1000 | Loss: 0.00001779
Iteration 216/1000 | Loss: 0.00001779
Iteration 217/1000 | Loss: 0.00001779
Iteration 218/1000 | Loss: 0.00001779
Iteration 219/1000 | Loss: 0.00001779
Iteration 220/1000 | Loss: 0.00001779
Iteration 221/1000 | Loss: 0.00001779
Iteration 222/1000 | Loss: 0.00001779
Iteration 223/1000 | Loss: 0.00001779
Iteration 224/1000 | Loss: 0.00001778
Iteration 225/1000 | Loss: 0.00001778
Iteration 226/1000 | Loss: 0.00001778
Iteration 227/1000 | Loss: 0.00001778
Iteration 228/1000 | Loss: 0.00001778
Iteration 229/1000 | Loss: 0.00001778
Iteration 230/1000 | Loss: 0.00001778
Iteration 231/1000 | Loss: 0.00001778
Iteration 232/1000 | Loss: 0.00001778
Iteration 233/1000 | Loss: 0.00001778
Iteration 234/1000 | Loss: 0.00001778
Iteration 235/1000 | Loss: 0.00001778
Iteration 236/1000 | Loss: 0.00001778
Iteration 237/1000 | Loss: 0.00001778
Iteration 238/1000 | Loss: 0.00001778
Iteration 239/1000 | Loss: 0.00001778
Iteration 240/1000 | Loss: 0.00001777
Iteration 241/1000 | Loss: 0.00001777
Iteration 242/1000 | Loss: 0.00001777
Iteration 243/1000 | Loss: 0.00001777
Iteration 244/1000 | Loss: 0.00001777
Iteration 245/1000 | Loss: 0.00001777
Iteration 246/1000 | Loss: 0.00001777
Iteration 247/1000 | Loss: 0.00001777
Iteration 248/1000 | Loss: 0.00001777
Iteration 249/1000 | Loss: 0.00001777
Iteration 250/1000 | Loss: 0.00001777
Iteration 251/1000 | Loss: 0.00001777
Iteration 252/1000 | Loss: 0.00001777
Iteration 253/1000 | Loss: 0.00001777
Iteration 254/1000 | Loss: 0.00001777
Iteration 255/1000 | Loss: 0.00001777
Iteration 256/1000 | Loss: 0.00001777
Iteration 257/1000 | Loss: 0.00001777
Iteration 258/1000 | Loss: 0.00001777
Iteration 259/1000 | Loss: 0.00001777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.7767935787560418e-05, 1.7767935787560418e-05, 1.7767935787560418e-05, 1.7767935787560418e-05, 1.7767935787560418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7767935787560418e-05

Optimization complete. Final v2v error: 3.340924024581909 mm

Highest mean error: 10.529708862304688 mm for frame 37

Lowest mean error: 3.097276449203491 mm for frame 164

Saving results

Total time: 126.33349990844727
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967740
Iteration 2/25 | Loss: 0.00967740
Iteration 3/25 | Loss: 0.00967740
Iteration 4/25 | Loss: 0.00967740
Iteration 5/25 | Loss: 0.00967739
Iteration 6/25 | Loss: 0.00967739
Iteration 7/25 | Loss: 0.00268945
Iteration 8/25 | Loss: 0.00216482
Iteration 9/25 | Loss: 0.00213584
Iteration 10/25 | Loss: 0.00201104
Iteration 11/25 | Loss: 0.00172334
Iteration 12/25 | Loss: 0.00156008
Iteration 13/25 | Loss: 0.00149260
Iteration 14/25 | Loss: 0.00146437
Iteration 15/25 | Loss: 0.00142245
Iteration 16/25 | Loss: 0.00141574
Iteration 17/25 | Loss: 0.00140419
Iteration 18/25 | Loss: 0.00138145
Iteration 19/25 | Loss: 0.00135431
Iteration 20/25 | Loss: 0.00134492
Iteration 21/25 | Loss: 0.00134519
Iteration 22/25 | Loss: 0.00134180
Iteration 23/25 | Loss: 0.00134339
Iteration 24/25 | Loss: 0.00134421
Iteration 25/25 | Loss: 0.00133825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30775321
Iteration 2/25 | Loss: 0.00136046
Iteration 3/25 | Loss: 0.00136045
Iteration 4/25 | Loss: 0.00136045
Iteration 5/25 | Loss: 0.00136044
Iteration 6/25 | Loss: 0.00136044
Iteration 7/25 | Loss: 0.00136044
Iteration 8/25 | Loss: 0.00136044
Iteration 9/25 | Loss: 0.00136044
Iteration 10/25 | Loss: 0.00136044
Iteration 11/25 | Loss: 0.00136044
Iteration 12/25 | Loss: 0.00136044
Iteration 13/25 | Loss: 0.00136044
Iteration 14/25 | Loss: 0.00136044
Iteration 15/25 | Loss: 0.00136044
Iteration 16/25 | Loss: 0.00136044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013604435371235013, 0.0013604435371235013, 0.0013604435371235013, 0.0013604435371235013, 0.0013604435371235013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013604435371235013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136044
Iteration 2/1000 | Loss: 0.00092365
Iteration 3/1000 | Loss: 0.00083635
Iteration 4/1000 | Loss: 0.00067724
Iteration 5/1000 | Loss: 0.00060258
Iteration 6/1000 | Loss: 0.00011320
Iteration 7/1000 | Loss: 0.00008536
Iteration 8/1000 | Loss: 0.00019326
Iteration 9/1000 | Loss: 0.00005820
Iteration 10/1000 | Loss: 0.00004817
Iteration 11/1000 | Loss: 0.00004087
Iteration 12/1000 | Loss: 0.00022023
Iteration 13/1000 | Loss: 0.00038662
Iteration 14/1000 | Loss: 0.00027771
Iteration 15/1000 | Loss: 0.00031171
Iteration 16/1000 | Loss: 0.00026608
Iteration 17/1000 | Loss: 0.00036145
Iteration 18/1000 | Loss: 0.00014308
Iteration 19/1000 | Loss: 0.00003991
Iteration 20/1000 | Loss: 0.00011383
Iteration 21/1000 | Loss: 0.00003675
Iteration 22/1000 | Loss: 0.00003346
Iteration 23/1000 | Loss: 0.00013371
Iteration 24/1000 | Loss: 0.00008493
Iteration 25/1000 | Loss: 0.00012495
Iteration 26/1000 | Loss: 0.00008015
Iteration 27/1000 | Loss: 0.00010312
Iteration 28/1000 | Loss: 0.00003132
Iteration 29/1000 | Loss: 0.00003206
Iteration 30/1000 | Loss: 0.00008792
Iteration 31/1000 | Loss: 0.00002737
Iteration 32/1000 | Loss: 0.00002467
Iteration 33/1000 | Loss: 0.00002364
Iteration 34/1000 | Loss: 0.00002271
Iteration 35/1000 | Loss: 0.00002216
Iteration 36/1000 | Loss: 0.00002160
Iteration 37/1000 | Loss: 0.00002120
Iteration 38/1000 | Loss: 0.00002071
Iteration 39/1000 | Loss: 0.00002041
Iteration 40/1000 | Loss: 0.00002015
Iteration 41/1000 | Loss: 0.00002007
Iteration 42/1000 | Loss: 0.00001996
Iteration 43/1000 | Loss: 0.00001991
Iteration 44/1000 | Loss: 0.00001983
Iteration 45/1000 | Loss: 0.00001981
Iteration 46/1000 | Loss: 0.00001980
Iteration 47/1000 | Loss: 0.00001976
Iteration 48/1000 | Loss: 0.00001973
Iteration 49/1000 | Loss: 0.00001973
Iteration 50/1000 | Loss: 0.00001972
Iteration 51/1000 | Loss: 0.00001972
Iteration 52/1000 | Loss: 0.00001972
Iteration 53/1000 | Loss: 0.00001972
Iteration 54/1000 | Loss: 0.00001972
Iteration 55/1000 | Loss: 0.00001971
Iteration 56/1000 | Loss: 0.00001971
Iteration 57/1000 | Loss: 0.00001969
Iteration 58/1000 | Loss: 0.00001968
Iteration 59/1000 | Loss: 0.00001968
Iteration 60/1000 | Loss: 0.00001967
Iteration 61/1000 | Loss: 0.00001967
Iteration 62/1000 | Loss: 0.00001966
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001965
Iteration 65/1000 | Loss: 0.00001965
Iteration 66/1000 | Loss: 0.00001965
Iteration 67/1000 | Loss: 0.00001964
Iteration 68/1000 | Loss: 0.00001964
Iteration 69/1000 | Loss: 0.00001964
Iteration 70/1000 | Loss: 0.00001964
Iteration 71/1000 | Loss: 0.00001964
Iteration 72/1000 | Loss: 0.00001964
Iteration 73/1000 | Loss: 0.00001964
Iteration 74/1000 | Loss: 0.00001964
Iteration 75/1000 | Loss: 0.00001963
Iteration 76/1000 | Loss: 0.00001963
Iteration 77/1000 | Loss: 0.00001963
Iteration 78/1000 | Loss: 0.00001963
Iteration 79/1000 | Loss: 0.00001963
Iteration 80/1000 | Loss: 0.00001962
Iteration 81/1000 | Loss: 0.00001962
Iteration 82/1000 | Loss: 0.00001962
Iteration 83/1000 | Loss: 0.00001962
Iteration 84/1000 | Loss: 0.00001961
Iteration 85/1000 | Loss: 0.00001961
Iteration 86/1000 | Loss: 0.00001961
Iteration 87/1000 | Loss: 0.00001961
Iteration 88/1000 | Loss: 0.00001961
Iteration 89/1000 | Loss: 0.00001961
Iteration 90/1000 | Loss: 0.00001961
Iteration 91/1000 | Loss: 0.00001961
Iteration 92/1000 | Loss: 0.00001960
Iteration 93/1000 | Loss: 0.00001960
Iteration 94/1000 | Loss: 0.00001960
Iteration 95/1000 | Loss: 0.00001960
Iteration 96/1000 | Loss: 0.00001960
Iteration 97/1000 | Loss: 0.00001959
Iteration 98/1000 | Loss: 0.00001959
Iteration 99/1000 | Loss: 0.00001959
Iteration 100/1000 | Loss: 0.00001959
Iteration 101/1000 | Loss: 0.00001959
Iteration 102/1000 | Loss: 0.00001959
Iteration 103/1000 | Loss: 0.00001959
Iteration 104/1000 | Loss: 0.00001959
Iteration 105/1000 | Loss: 0.00001958
Iteration 106/1000 | Loss: 0.00001957
Iteration 107/1000 | Loss: 0.00001957
Iteration 108/1000 | Loss: 0.00001956
Iteration 109/1000 | Loss: 0.00001955
Iteration 110/1000 | Loss: 0.00001955
Iteration 111/1000 | Loss: 0.00001955
Iteration 112/1000 | Loss: 0.00001954
Iteration 113/1000 | Loss: 0.00001954
Iteration 114/1000 | Loss: 0.00001953
Iteration 115/1000 | Loss: 0.00001953
Iteration 116/1000 | Loss: 0.00001951
Iteration 117/1000 | Loss: 0.00017983
Iteration 118/1000 | Loss: 0.00013228
Iteration 119/1000 | Loss: 0.00002201
Iteration 120/1000 | Loss: 0.00001985
Iteration 121/1000 | Loss: 0.00001882
Iteration 122/1000 | Loss: 0.00001839
Iteration 123/1000 | Loss: 0.00001820
Iteration 124/1000 | Loss: 0.00001817
Iteration 125/1000 | Loss: 0.00001814
Iteration 126/1000 | Loss: 0.00001813
Iteration 127/1000 | Loss: 0.00001811
Iteration 128/1000 | Loss: 0.00001811
Iteration 129/1000 | Loss: 0.00001810
Iteration 130/1000 | Loss: 0.00001809
Iteration 131/1000 | Loss: 0.00001804
Iteration 132/1000 | Loss: 0.00001803
Iteration 133/1000 | Loss: 0.00001802
Iteration 134/1000 | Loss: 0.00001801
Iteration 135/1000 | Loss: 0.00001801
Iteration 136/1000 | Loss: 0.00001801
Iteration 137/1000 | Loss: 0.00001800
Iteration 138/1000 | Loss: 0.00001800
Iteration 139/1000 | Loss: 0.00001800
Iteration 140/1000 | Loss: 0.00001800
Iteration 141/1000 | Loss: 0.00001799
Iteration 142/1000 | Loss: 0.00001799
Iteration 143/1000 | Loss: 0.00001799
Iteration 144/1000 | Loss: 0.00001799
Iteration 145/1000 | Loss: 0.00001799
Iteration 146/1000 | Loss: 0.00001798
Iteration 147/1000 | Loss: 0.00001798
Iteration 148/1000 | Loss: 0.00001798
Iteration 149/1000 | Loss: 0.00001797
Iteration 150/1000 | Loss: 0.00001797
Iteration 151/1000 | Loss: 0.00001797
Iteration 152/1000 | Loss: 0.00001797
Iteration 153/1000 | Loss: 0.00001796
Iteration 154/1000 | Loss: 0.00001796
Iteration 155/1000 | Loss: 0.00001796
Iteration 156/1000 | Loss: 0.00001796
Iteration 157/1000 | Loss: 0.00001796
Iteration 158/1000 | Loss: 0.00001796
Iteration 159/1000 | Loss: 0.00001796
Iteration 160/1000 | Loss: 0.00001796
Iteration 161/1000 | Loss: 0.00001795
Iteration 162/1000 | Loss: 0.00001795
Iteration 163/1000 | Loss: 0.00001795
Iteration 164/1000 | Loss: 0.00001795
Iteration 165/1000 | Loss: 0.00001795
Iteration 166/1000 | Loss: 0.00001795
Iteration 167/1000 | Loss: 0.00001795
Iteration 168/1000 | Loss: 0.00001794
Iteration 169/1000 | Loss: 0.00001794
Iteration 170/1000 | Loss: 0.00001794
Iteration 171/1000 | Loss: 0.00001794
Iteration 172/1000 | Loss: 0.00001794
Iteration 173/1000 | Loss: 0.00001794
Iteration 174/1000 | Loss: 0.00001793
Iteration 175/1000 | Loss: 0.00001793
Iteration 176/1000 | Loss: 0.00001793
Iteration 177/1000 | Loss: 0.00001793
Iteration 178/1000 | Loss: 0.00001793
Iteration 179/1000 | Loss: 0.00001793
Iteration 180/1000 | Loss: 0.00001793
Iteration 181/1000 | Loss: 0.00001792
Iteration 182/1000 | Loss: 0.00001792
Iteration 183/1000 | Loss: 0.00001792
Iteration 184/1000 | Loss: 0.00001792
Iteration 185/1000 | Loss: 0.00001792
Iteration 186/1000 | Loss: 0.00001792
Iteration 187/1000 | Loss: 0.00001792
Iteration 188/1000 | Loss: 0.00001792
Iteration 189/1000 | Loss: 0.00001792
Iteration 190/1000 | Loss: 0.00001792
Iteration 191/1000 | Loss: 0.00001792
Iteration 192/1000 | Loss: 0.00001792
Iteration 193/1000 | Loss: 0.00001792
Iteration 194/1000 | Loss: 0.00001792
Iteration 195/1000 | Loss: 0.00001791
Iteration 196/1000 | Loss: 0.00001791
Iteration 197/1000 | Loss: 0.00001791
Iteration 198/1000 | Loss: 0.00001791
Iteration 199/1000 | Loss: 0.00001791
Iteration 200/1000 | Loss: 0.00001791
Iteration 201/1000 | Loss: 0.00001791
Iteration 202/1000 | Loss: 0.00001791
Iteration 203/1000 | Loss: 0.00001791
Iteration 204/1000 | Loss: 0.00001791
Iteration 205/1000 | Loss: 0.00001791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.791004069673363e-05, 1.791004069673363e-05, 1.791004069673363e-05, 1.791004069673363e-05, 1.791004069673363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.791004069673363e-05

Optimization complete. Final v2v error: 3.5507524013519287 mm

Highest mean error: 5.606163501739502 mm for frame 108

Lowest mean error: 3.021294593811035 mm for frame 168

Saving results

Total time: 137.43469882011414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964488
Iteration 2/25 | Loss: 0.00262808
Iteration 3/25 | Loss: 0.00188778
Iteration 4/25 | Loss: 0.00183090
Iteration 5/25 | Loss: 0.00170366
Iteration 6/25 | Loss: 0.00158014
Iteration 7/25 | Loss: 0.00151590
Iteration 8/25 | Loss: 0.00144524
Iteration 9/25 | Loss: 0.00138536
Iteration 10/25 | Loss: 0.00137169
Iteration 11/25 | Loss: 0.00136856
Iteration 12/25 | Loss: 0.00137182
Iteration 13/25 | Loss: 0.00136638
Iteration 14/25 | Loss: 0.00136752
Iteration 15/25 | Loss: 0.00136308
Iteration 16/25 | Loss: 0.00136686
Iteration 17/25 | Loss: 0.00136185
Iteration 18/25 | Loss: 0.00135985
Iteration 19/25 | Loss: 0.00136675
Iteration 20/25 | Loss: 0.00136843
Iteration 21/25 | Loss: 0.00136844
Iteration 22/25 | Loss: 0.00136619
Iteration 23/25 | Loss: 0.00135940
Iteration 24/25 | Loss: 0.00135963
Iteration 25/25 | Loss: 0.00136017

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30970573
Iteration 2/25 | Loss: 0.00183760
Iteration 3/25 | Loss: 0.00182447
Iteration 4/25 | Loss: 0.00182447
Iteration 5/25 | Loss: 0.00182447
Iteration 6/25 | Loss: 0.00182447
Iteration 7/25 | Loss: 0.00182447
Iteration 8/25 | Loss: 0.00182447
Iteration 9/25 | Loss: 0.00182447
Iteration 10/25 | Loss: 0.00182447
Iteration 11/25 | Loss: 0.00182447
Iteration 12/25 | Loss: 0.00182446
Iteration 13/25 | Loss: 0.00182447
Iteration 14/25 | Loss: 0.00182446
Iteration 15/25 | Loss: 0.00182446
Iteration 16/25 | Loss: 0.00182446
Iteration 17/25 | Loss: 0.00182446
Iteration 18/25 | Loss: 0.00182446
Iteration 19/25 | Loss: 0.00182446
Iteration 20/25 | Loss: 0.00182446
Iteration 21/25 | Loss: 0.00182446
Iteration 22/25 | Loss: 0.00182446
Iteration 23/25 | Loss: 0.00182446
Iteration 24/25 | Loss: 0.00182446
Iteration 25/25 | Loss: 0.00182446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182446
Iteration 2/1000 | Loss: 0.00014103
Iteration 3/1000 | Loss: 0.00009941
Iteration 4/1000 | Loss: 0.00013194
Iteration 5/1000 | Loss: 0.00018111
Iteration 6/1000 | Loss: 0.00018246
Iteration 7/1000 | Loss: 0.00015478
Iteration 8/1000 | Loss: 0.00016208
Iteration 9/1000 | Loss: 0.00013657
Iteration 10/1000 | Loss: 0.00014013
Iteration 11/1000 | Loss: 0.00007122
Iteration 12/1000 | Loss: 0.00022245
Iteration 13/1000 | Loss: 0.00032735
Iteration 14/1000 | Loss: 0.00023827
Iteration 15/1000 | Loss: 0.00007603
Iteration 16/1000 | Loss: 0.00006742
Iteration 17/1000 | Loss: 0.00030034
Iteration 18/1000 | Loss: 0.00020448
Iteration 19/1000 | Loss: 0.00008193
Iteration 20/1000 | Loss: 0.00010423
Iteration 21/1000 | Loss: 0.00006104
Iteration 22/1000 | Loss: 0.00024069
Iteration 23/1000 | Loss: 0.00018272
Iteration 24/1000 | Loss: 0.00024208
Iteration 25/1000 | Loss: 0.00006230
Iteration 26/1000 | Loss: 0.00076043
Iteration 27/1000 | Loss: 0.00088070
Iteration 28/1000 | Loss: 0.00029210
Iteration 29/1000 | Loss: 0.00009576
Iteration 30/1000 | Loss: 0.00008596
Iteration 31/1000 | Loss: 0.00033388
Iteration 32/1000 | Loss: 0.00006096
Iteration 33/1000 | Loss: 0.00005368
Iteration 34/1000 | Loss: 0.00020525
Iteration 35/1000 | Loss: 0.00027162
Iteration 36/1000 | Loss: 0.00007606
Iteration 37/1000 | Loss: 0.00022779
Iteration 38/1000 | Loss: 0.00008233
Iteration 39/1000 | Loss: 0.00012719
Iteration 40/1000 | Loss: 0.00005550
Iteration 41/1000 | Loss: 0.00005208
Iteration 42/1000 | Loss: 0.00005335
Iteration 43/1000 | Loss: 0.00005173
Iteration 44/1000 | Loss: 0.00004602
Iteration 45/1000 | Loss: 0.00034556
Iteration 46/1000 | Loss: 0.00020738
Iteration 47/1000 | Loss: 0.00032186
Iteration 48/1000 | Loss: 0.00017641
Iteration 49/1000 | Loss: 0.00019938
Iteration 50/1000 | Loss: 0.00007083
Iteration 51/1000 | Loss: 0.00006025
Iteration 52/1000 | Loss: 0.00005907
Iteration 53/1000 | Loss: 0.00005079
Iteration 54/1000 | Loss: 0.00004909
Iteration 55/1000 | Loss: 0.00004834
Iteration 56/1000 | Loss: 0.00020776
Iteration 57/1000 | Loss: 0.00027186
Iteration 58/1000 | Loss: 0.00004755
Iteration 59/1000 | Loss: 0.00024854
Iteration 60/1000 | Loss: 0.00331538
Iteration 61/1000 | Loss: 0.00170737
Iteration 62/1000 | Loss: 0.00332718
Iteration 63/1000 | Loss: 0.00033673
Iteration 64/1000 | Loss: 0.00037556
Iteration 65/1000 | Loss: 0.00034007
Iteration 66/1000 | Loss: 0.00036438
Iteration 67/1000 | Loss: 0.00026753
Iteration 68/1000 | Loss: 0.00031098
Iteration 69/1000 | Loss: 0.00021801
Iteration 70/1000 | Loss: 0.00028632
Iteration 71/1000 | Loss: 0.00018605
Iteration 72/1000 | Loss: 0.00026440
Iteration 73/1000 | Loss: 0.00017225
Iteration 74/1000 | Loss: 0.00024704
Iteration 75/1000 | Loss: 0.00015348
Iteration 76/1000 | Loss: 0.00021902
Iteration 77/1000 | Loss: 0.00004334
Iteration 78/1000 | Loss: 0.00003762
Iteration 79/1000 | Loss: 0.00003454
Iteration 80/1000 | Loss: 0.00003289
Iteration 81/1000 | Loss: 0.00003176
Iteration 82/1000 | Loss: 0.00003073
Iteration 83/1000 | Loss: 0.00003005
Iteration 84/1000 | Loss: 0.00026038
Iteration 85/1000 | Loss: 0.00016315
Iteration 86/1000 | Loss: 0.00020959
Iteration 87/1000 | Loss: 0.00008340
Iteration 88/1000 | Loss: 0.00019389
Iteration 89/1000 | Loss: 0.00010647
Iteration 90/1000 | Loss: 0.00003160
Iteration 91/1000 | Loss: 0.00021212
Iteration 92/1000 | Loss: 0.00010305
Iteration 93/1000 | Loss: 0.00020256
Iteration 94/1000 | Loss: 0.00012662
Iteration 95/1000 | Loss: 0.00003125
Iteration 96/1000 | Loss: 0.00002953
Iteration 97/1000 | Loss: 0.00024610
Iteration 98/1000 | Loss: 0.00012814
Iteration 99/1000 | Loss: 0.00003038
Iteration 100/1000 | Loss: 0.00002917
Iteration 101/1000 | Loss: 0.00024551
Iteration 102/1000 | Loss: 0.00011873
Iteration 103/1000 | Loss: 0.00003477
Iteration 104/1000 | Loss: 0.00003066
Iteration 105/1000 | Loss: 0.00002984
Iteration 106/1000 | Loss: 0.00027283
Iteration 107/1000 | Loss: 0.00024160
Iteration 108/1000 | Loss: 0.00015070
Iteration 109/1000 | Loss: 0.00021535
Iteration 110/1000 | Loss: 0.00023216
Iteration 111/1000 | Loss: 0.00016916
Iteration 112/1000 | Loss: 0.00018135
Iteration 113/1000 | Loss: 0.00029071
Iteration 114/1000 | Loss: 0.00025155
Iteration 115/1000 | Loss: 0.00027341
Iteration 116/1000 | Loss: 0.00023721
Iteration 117/1000 | Loss: 0.00004626
Iteration 118/1000 | Loss: 0.00003163
Iteration 119/1000 | Loss: 0.00002844
Iteration 120/1000 | Loss: 0.00002730
Iteration 121/1000 | Loss: 0.00002654
Iteration 122/1000 | Loss: 0.00002627
Iteration 123/1000 | Loss: 0.00002611
Iteration 124/1000 | Loss: 0.00002607
Iteration 125/1000 | Loss: 0.00002597
Iteration 126/1000 | Loss: 0.00002592
Iteration 127/1000 | Loss: 0.00002590
Iteration 128/1000 | Loss: 0.00002588
Iteration 129/1000 | Loss: 0.00002583
Iteration 130/1000 | Loss: 0.00002581
Iteration 131/1000 | Loss: 0.00002579
Iteration 132/1000 | Loss: 0.00002575
Iteration 133/1000 | Loss: 0.00002568
Iteration 134/1000 | Loss: 0.00002565
Iteration 135/1000 | Loss: 0.00002563
Iteration 136/1000 | Loss: 0.00002563
Iteration 137/1000 | Loss: 0.00002562
Iteration 138/1000 | Loss: 0.00002562
Iteration 139/1000 | Loss: 0.00002562
Iteration 140/1000 | Loss: 0.00002561
Iteration 141/1000 | Loss: 0.00002561
Iteration 142/1000 | Loss: 0.00002560
Iteration 143/1000 | Loss: 0.00002560
Iteration 144/1000 | Loss: 0.00002560
Iteration 145/1000 | Loss: 0.00002560
Iteration 146/1000 | Loss: 0.00002560
Iteration 147/1000 | Loss: 0.00002560
Iteration 148/1000 | Loss: 0.00002560
Iteration 149/1000 | Loss: 0.00002560
Iteration 150/1000 | Loss: 0.00002560
Iteration 151/1000 | Loss: 0.00002560
Iteration 152/1000 | Loss: 0.00002559
Iteration 153/1000 | Loss: 0.00002559
Iteration 154/1000 | Loss: 0.00002559
Iteration 155/1000 | Loss: 0.00002559
Iteration 156/1000 | Loss: 0.00002559
Iteration 157/1000 | Loss: 0.00002559
Iteration 158/1000 | Loss: 0.00002559
Iteration 159/1000 | Loss: 0.00002559
Iteration 160/1000 | Loss: 0.00002559
Iteration 161/1000 | Loss: 0.00002559
Iteration 162/1000 | Loss: 0.00002559
Iteration 163/1000 | Loss: 0.00002559
Iteration 164/1000 | Loss: 0.00002559
Iteration 165/1000 | Loss: 0.00002558
Iteration 166/1000 | Loss: 0.00002558
Iteration 167/1000 | Loss: 0.00002558
Iteration 168/1000 | Loss: 0.00002558
Iteration 169/1000 | Loss: 0.00002558
Iteration 170/1000 | Loss: 0.00002558
Iteration 171/1000 | Loss: 0.00002558
Iteration 172/1000 | Loss: 0.00002558
Iteration 173/1000 | Loss: 0.00002558
Iteration 174/1000 | Loss: 0.00002558
Iteration 175/1000 | Loss: 0.00002558
Iteration 176/1000 | Loss: 0.00002558
Iteration 177/1000 | Loss: 0.00002558
Iteration 178/1000 | Loss: 0.00002558
Iteration 179/1000 | Loss: 0.00002558
Iteration 180/1000 | Loss: 0.00002557
Iteration 181/1000 | Loss: 0.00002557
Iteration 182/1000 | Loss: 0.00002557
Iteration 183/1000 | Loss: 0.00002557
Iteration 184/1000 | Loss: 0.00002557
Iteration 185/1000 | Loss: 0.00002557
Iteration 186/1000 | Loss: 0.00002557
Iteration 187/1000 | Loss: 0.00002557
Iteration 188/1000 | Loss: 0.00002557
Iteration 189/1000 | Loss: 0.00002556
Iteration 190/1000 | Loss: 0.00002556
Iteration 191/1000 | Loss: 0.00002556
Iteration 192/1000 | Loss: 0.00002555
Iteration 193/1000 | Loss: 0.00002555
Iteration 194/1000 | Loss: 0.00002555
Iteration 195/1000 | Loss: 0.00002554
Iteration 196/1000 | Loss: 0.00002554
Iteration 197/1000 | Loss: 0.00002553
Iteration 198/1000 | Loss: 0.00002553
Iteration 199/1000 | Loss: 0.00002553
Iteration 200/1000 | Loss: 0.00002553
Iteration 201/1000 | Loss: 0.00002553
Iteration 202/1000 | Loss: 0.00002553
Iteration 203/1000 | Loss: 0.00002553
Iteration 204/1000 | Loss: 0.00002553
Iteration 205/1000 | Loss: 0.00002553
Iteration 206/1000 | Loss: 0.00002552
Iteration 207/1000 | Loss: 0.00002552
Iteration 208/1000 | Loss: 0.00002552
Iteration 209/1000 | Loss: 0.00002552
Iteration 210/1000 | Loss: 0.00002551
Iteration 211/1000 | Loss: 0.00002551
Iteration 212/1000 | Loss: 0.00002551
Iteration 213/1000 | Loss: 0.00002551
Iteration 214/1000 | Loss: 0.00002551
Iteration 215/1000 | Loss: 0.00002551
Iteration 216/1000 | Loss: 0.00002551
Iteration 217/1000 | Loss: 0.00002551
Iteration 218/1000 | Loss: 0.00002550
Iteration 219/1000 | Loss: 0.00002550
Iteration 220/1000 | Loss: 0.00002550
Iteration 221/1000 | Loss: 0.00002550
Iteration 222/1000 | Loss: 0.00002549
Iteration 223/1000 | Loss: 0.00002549
Iteration 224/1000 | Loss: 0.00002549
Iteration 225/1000 | Loss: 0.00002548
Iteration 226/1000 | Loss: 0.00002548
Iteration 227/1000 | Loss: 0.00002548
Iteration 228/1000 | Loss: 0.00002548
Iteration 229/1000 | Loss: 0.00002548
Iteration 230/1000 | Loss: 0.00002548
Iteration 231/1000 | Loss: 0.00002548
Iteration 232/1000 | Loss: 0.00002548
Iteration 233/1000 | Loss: 0.00002548
Iteration 234/1000 | Loss: 0.00002548
Iteration 235/1000 | Loss: 0.00002548
Iteration 236/1000 | Loss: 0.00002547
Iteration 237/1000 | Loss: 0.00002547
Iteration 238/1000 | Loss: 0.00002547
Iteration 239/1000 | Loss: 0.00002547
Iteration 240/1000 | Loss: 0.00002547
Iteration 241/1000 | Loss: 0.00002547
Iteration 242/1000 | Loss: 0.00002547
Iteration 243/1000 | Loss: 0.00002547
Iteration 244/1000 | Loss: 0.00002547
Iteration 245/1000 | Loss: 0.00002547
Iteration 246/1000 | Loss: 0.00002547
Iteration 247/1000 | Loss: 0.00002547
Iteration 248/1000 | Loss: 0.00002547
Iteration 249/1000 | Loss: 0.00002547
Iteration 250/1000 | Loss: 0.00002547
Iteration 251/1000 | Loss: 0.00002547
Iteration 252/1000 | Loss: 0.00002547
Iteration 253/1000 | Loss: 0.00002547
Iteration 254/1000 | Loss: 0.00002547
Iteration 255/1000 | Loss: 0.00002547
Iteration 256/1000 | Loss: 0.00002547
Iteration 257/1000 | Loss: 0.00002547
Iteration 258/1000 | Loss: 0.00002547
Iteration 259/1000 | Loss: 0.00002547
Iteration 260/1000 | Loss: 0.00002547
Iteration 261/1000 | Loss: 0.00002547
Iteration 262/1000 | Loss: 0.00002547
Iteration 263/1000 | Loss: 0.00002547
Iteration 264/1000 | Loss: 0.00002547
Iteration 265/1000 | Loss: 0.00002547
Iteration 266/1000 | Loss: 0.00002547
Iteration 267/1000 | Loss: 0.00002547
Iteration 268/1000 | Loss: 0.00002547
Iteration 269/1000 | Loss: 0.00002547
Iteration 270/1000 | Loss: 0.00002547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [2.546894393162802e-05, 2.546894393162802e-05, 2.546894393162802e-05, 2.546894393162802e-05, 2.546894393162802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.546894393162802e-05

Optimization complete. Final v2v error: 4.021090030670166 mm

Highest mean error: 5.993988037109375 mm for frame 105

Lowest mean error: 2.6783154010772705 mm for frame 22

Saving results

Total time: 232.50928568840027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00732514
Iteration 2/25 | Loss: 0.00157904
Iteration 3/25 | Loss: 0.00143890
Iteration 4/25 | Loss: 0.00142829
Iteration 5/25 | Loss: 0.00142829
Iteration 6/25 | Loss: 0.00142829
Iteration 7/25 | Loss: 0.00142829
Iteration 8/25 | Loss: 0.00142829
Iteration 9/25 | Loss: 0.00142829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0014282887568697333, 0.0014282887568697333, 0.0014282887568697333, 0.0014282887568697333, 0.0014282887568697333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014282887568697333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.70748806
Iteration 2/25 | Loss: 0.00093409
Iteration 3/25 | Loss: 0.00093409
Iteration 4/25 | Loss: 0.00093409
Iteration 5/25 | Loss: 0.00093409
Iteration 6/25 | Loss: 0.00093409
Iteration 7/25 | Loss: 0.00093409
Iteration 8/25 | Loss: 0.00093409
Iteration 9/25 | Loss: 0.00093409
Iteration 10/25 | Loss: 0.00093409
Iteration 11/25 | Loss: 0.00093409
Iteration 12/25 | Loss: 0.00093409
Iteration 13/25 | Loss: 0.00093409
Iteration 14/25 | Loss: 0.00093409
Iteration 15/25 | Loss: 0.00093409
Iteration 16/25 | Loss: 0.00093409
Iteration 17/25 | Loss: 0.00093409
Iteration 18/25 | Loss: 0.00093409
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009340867982245982, 0.0009340867982245982, 0.0009340867982245982, 0.0009340867982245982, 0.0009340867982245982]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009340867982245982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093409
Iteration 2/1000 | Loss: 0.00005851
Iteration 3/1000 | Loss: 0.00003196
Iteration 4/1000 | Loss: 0.00002839
Iteration 5/1000 | Loss: 0.00002693
Iteration 6/1000 | Loss: 0.00002607
Iteration 7/1000 | Loss: 0.00002553
Iteration 8/1000 | Loss: 0.00002523
Iteration 9/1000 | Loss: 0.00002483
Iteration 10/1000 | Loss: 0.00002448
Iteration 11/1000 | Loss: 0.00002419
Iteration 12/1000 | Loss: 0.00002398
Iteration 13/1000 | Loss: 0.00002374
Iteration 14/1000 | Loss: 0.00002353
Iteration 15/1000 | Loss: 0.00002336
Iteration 16/1000 | Loss: 0.00002322
Iteration 17/1000 | Loss: 0.00002320
Iteration 18/1000 | Loss: 0.00002309
Iteration 19/1000 | Loss: 0.00002306
Iteration 20/1000 | Loss: 0.00002300
Iteration 21/1000 | Loss: 0.00002297
Iteration 22/1000 | Loss: 0.00002293
Iteration 23/1000 | Loss: 0.00002292
Iteration 24/1000 | Loss: 0.00002291
Iteration 25/1000 | Loss: 0.00002290
Iteration 26/1000 | Loss: 0.00002290
Iteration 27/1000 | Loss: 0.00002289
Iteration 28/1000 | Loss: 0.00002289
Iteration 29/1000 | Loss: 0.00002289
Iteration 30/1000 | Loss: 0.00002288
Iteration 31/1000 | Loss: 0.00002288
Iteration 32/1000 | Loss: 0.00002288
Iteration 33/1000 | Loss: 0.00002288
Iteration 34/1000 | Loss: 0.00002287
Iteration 35/1000 | Loss: 0.00002287
Iteration 36/1000 | Loss: 0.00002286
Iteration 37/1000 | Loss: 0.00002286
Iteration 38/1000 | Loss: 0.00002286
Iteration 39/1000 | Loss: 0.00002285
Iteration 40/1000 | Loss: 0.00002285
Iteration 41/1000 | Loss: 0.00002285
Iteration 42/1000 | Loss: 0.00002285
Iteration 43/1000 | Loss: 0.00002285
Iteration 44/1000 | Loss: 0.00002285
Iteration 45/1000 | Loss: 0.00002285
Iteration 46/1000 | Loss: 0.00002284
Iteration 47/1000 | Loss: 0.00002284
Iteration 48/1000 | Loss: 0.00002284
Iteration 49/1000 | Loss: 0.00002283
Iteration 50/1000 | Loss: 0.00002283
Iteration 51/1000 | Loss: 0.00002283
Iteration 52/1000 | Loss: 0.00002283
Iteration 53/1000 | Loss: 0.00002282
Iteration 54/1000 | Loss: 0.00002282
Iteration 55/1000 | Loss: 0.00002282
Iteration 56/1000 | Loss: 0.00002281
Iteration 57/1000 | Loss: 0.00002281
Iteration 58/1000 | Loss: 0.00002281
Iteration 59/1000 | Loss: 0.00002281
Iteration 60/1000 | Loss: 0.00002280
Iteration 61/1000 | Loss: 0.00002280
Iteration 62/1000 | Loss: 0.00002280
Iteration 63/1000 | Loss: 0.00002280
Iteration 64/1000 | Loss: 0.00002280
Iteration 65/1000 | Loss: 0.00002279
Iteration 66/1000 | Loss: 0.00002279
Iteration 67/1000 | Loss: 0.00002279
Iteration 68/1000 | Loss: 0.00002279
Iteration 69/1000 | Loss: 0.00002279
Iteration 70/1000 | Loss: 0.00002279
Iteration 71/1000 | Loss: 0.00002278
Iteration 72/1000 | Loss: 0.00002278
Iteration 73/1000 | Loss: 0.00002278
Iteration 74/1000 | Loss: 0.00002277
Iteration 75/1000 | Loss: 0.00002277
Iteration 76/1000 | Loss: 0.00002277
Iteration 77/1000 | Loss: 0.00002277
Iteration 78/1000 | Loss: 0.00002277
Iteration 79/1000 | Loss: 0.00002277
Iteration 80/1000 | Loss: 0.00002277
Iteration 81/1000 | Loss: 0.00002277
Iteration 82/1000 | Loss: 0.00002276
Iteration 83/1000 | Loss: 0.00002276
Iteration 84/1000 | Loss: 0.00002276
Iteration 85/1000 | Loss: 0.00002276
Iteration 86/1000 | Loss: 0.00002276
Iteration 87/1000 | Loss: 0.00002276
Iteration 88/1000 | Loss: 0.00002276
Iteration 89/1000 | Loss: 0.00002276
Iteration 90/1000 | Loss: 0.00002276
Iteration 91/1000 | Loss: 0.00002276
Iteration 92/1000 | Loss: 0.00002276
Iteration 93/1000 | Loss: 0.00002276
Iteration 94/1000 | Loss: 0.00002276
Iteration 95/1000 | Loss: 0.00002276
Iteration 96/1000 | Loss: 0.00002276
Iteration 97/1000 | Loss: 0.00002276
Iteration 98/1000 | Loss: 0.00002276
Iteration 99/1000 | Loss: 0.00002276
Iteration 100/1000 | Loss: 0.00002276
Iteration 101/1000 | Loss: 0.00002276
Iteration 102/1000 | Loss: 0.00002276
Iteration 103/1000 | Loss: 0.00002276
Iteration 104/1000 | Loss: 0.00002276
Iteration 105/1000 | Loss: 0.00002276
Iteration 106/1000 | Loss: 0.00002276
Iteration 107/1000 | Loss: 0.00002276
Iteration 108/1000 | Loss: 0.00002276
Iteration 109/1000 | Loss: 0.00002276
Iteration 110/1000 | Loss: 0.00002276
Iteration 111/1000 | Loss: 0.00002276
Iteration 112/1000 | Loss: 0.00002276
Iteration 113/1000 | Loss: 0.00002276
Iteration 114/1000 | Loss: 0.00002276
Iteration 115/1000 | Loss: 0.00002276
Iteration 116/1000 | Loss: 0.00002276
Iteration 117/1000 | Loss: 0.00002276
Iteration 118/1000 | Loss: 0.00002276
Iteration 119/1000 | Loss: 0.00002276
Iteration 120/1000 | Loss: 0.00002276
Iteration 121/1000 | Loss: 0.00002276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.2763508241041563e-05, 2.2763508241041563e-05, 2.2763508241041563e-05, 2.2763508241041563e-05, 2.2763508241041563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2763508241041563e-05

Optimization complete. Final v2v error: 3.8924927711486816 mm

Highest mean error: 4.517880439758301 mm for frame 166

Lowest mean error: 3.216501474380493 mm for frame 239

Saving results

Total time: 41.70627737045288
