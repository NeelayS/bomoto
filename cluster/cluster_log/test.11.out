Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=11, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 616-671
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404743
Iteration 2/25 | Loss: 0.00114173
Iteration 3/25 | Loss: 0.00105583
Iteration 4/25 | Loss: 0.00104388
Iteration 5/25 | Loss: 0.00104009
Iteration 6/25 | Loss: 0.00103963
Iteration 7/25 | Loss: 0.00103963
Iteration 8/25 | Loss: 0.00103963
Iteration 9/25 | Loss: 0.00103963
Iteration 10/25 | Loss: 0.00103963
Iteration 11/25 | Loss: 0.00103963
Iteration 12/25 | Loss: 0.00103963
Iteration 13/25 | Loss: 0.00103963
Iteration 14/25 | Loss: 0.00103963
Iteration 15/25 | Loss: 0.00103963
Iteration 16/25 | Loss: 0.00103963
Iteration 17/25 | Loss: 0.00103963
Iteration 18/25 | Loss: 0.00103963
Iteration 19/25 | Loss: 0.00103963
Iteration 20/25 | Loss: 0.00103963
Iteration 21/25 | Loss: 0.00103963
Iteration 22/25 | Loss: 0.00103963
Iteration 23/25 | Loss: 0.00103963
Iteration 24/25 | Loss: 0.00103963
Iteration 25/25 | Loss: 0.00103963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31570709
Iteration 2/25 | Loss: 0.00152434
Iteration 3/25 | Loss: 0.00152434
Iteration 4/25 | Loss: 0.00152434
Iteration 5/25 | Loss: 0.00152434
Iteration 6/25 | Loss: 0.00152434
Iteration 7/25 | Loss: 0.00152434
Iteration 8/25 | Loss: 0.00152434
Iteration 9/25 | Loss: 0.00152434
Iteration 10/25 | Loss: 0.00152434
Iteration 11/25 | Loss: 0.00152434
Iteration 12/25 | Loss: 0.00152434
Iteration 13/25 | Loss: 0.00152434
Iteration 14/25 | Loss: 0.00152434
Iteration 15/25 | Loss: 0.00152434
Iteration 16/25 | Loss: 0.00152434
Iteration 17/25 | Loss: 0.00152434
Iteration 18/25 | Loss: 0.00152434
Iteration 19/25 | Loss: 0.00152434
Iteration 20/25 | Loss: 0.00152434
Iteration 21/25 | Loss: 0.00152434
Iteration 22/25 | Loss: 0.00152434
Iteration 23/25 | Loss: 0.00152434
Iteration 24/25 | Loss: 0.00152434
Iteration 25/25 | Loss: 0.00152434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152434
Iteration 2/1000 | Loss: 0.00003420
Iteration 3/1000 | Loss: 0.00002094
Iteration 4/1000 | Loss: 0.00001793
Iteration 5/1000 | Loss: 0.00001682
Iteration 6/1000 | Loss: 0.00001613
Iteration 7/1000 | Loss: 0.00001574
Iteration 8/1000 | Loss: 0.00001546
Iteration 9/1000 | Loss: 0.00001539
Iteration 10/1000 | Loss: 0.00001525
Iteration 11/1000 | Loss: 0.00001516
Iteration 12/1000 | Loss: 0.00001510
Iteration 13/1000 | Loss: 0.00001505
Iteration 14/1000 | Loss: 0.00001504
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001503
Iteration 17/1000 | Loss: 0.00001502
Iteration 18/1000 | Loss: 0.00001500
Iteration 19/1000 | Loss: 0.00001500
Iteration 20/1000 | Loss: 0.00001498
Iteration 21/1000 | Loss: 0.00001498
Iteration 22/1000 | Loss: 0.00001498
Iteration 23/1000 | Loss: 0.00001497
Iteration 24/1000 | Loss: 0.00001497
Iteration 25/1000 | Loss: 0.00001497
Iteration 26/1000 | Loss: 0.00001495
Iteration 27/1000 | Loss: 0.00001493
Iteration 28/1000 | Loss: 0.00001492
Iteration 29/1000 | Loss: 0.00001492
Iteration 30/1000 | Loss: 0.00001491
Iteration 31/1000 | Loss: 0.00001490
Iteration 32/1000 | Loss: 0.00001490
Iteration 33/1000 | Loss: 0.00001490
Iteration 34/1000 | Loss: 0.00001489
Iteration 35/1000 | Loss: 0.00001489
Iteration 36/1000 | Loss: 0.00001489
Iteration 37/1000 | Loss: 0.00001488
Iteration 38/1000 | Loss: 0.00001487
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001484
Iteration 41/1000 | Loss: 0.00001483
Iteration 42/1000 | Loss: 0.00001483
Iteration 43/1000 | Loss: 0.00001483
Iteration 44/1000 | Loss: 0.00001482
Iteration 45/1000 | Loss: 0.00001482
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001481
Iteration 48/1000 | Loss: 0.00001480
Iteration 49/1000 | Loss: 0.00001480
Iteration 50/1000 | Loss: 0.00001480
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001479
Iteration 53/1000 | Loss: 0.00001478
Iteration 54/1000 | Loss: 0.00001478
Iteration 55/1000 | Loss: 0.00001478
Iteration 56/1000 | Loss: 0.00001478
Iteration 57/1000 | Loss: 0.00001477
Iteration 58/1000 | Loss: 0.00001477
Iteration 59/1000 | Loss: 0.00001477
Iteration 60/1000 | Loss: 0.00001476
Iteration 61/1000 | Loss: 0.00001476
Iteration 62/1000 | Loss: 0.00001476
Iteration 63/1000 | Loss: 0.00001476
Iteration 64/1000 | Loss: 0.00001475
Iteration 65/1000 | Loss: 0.00001475
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001474
Iteration 68/1000 | Loss: 0.00001474
Iteration 69/1000 | Loss: 0.00001474
Iteration 70/1000 | Loss: 0.00001474
Iteration 71/1000 | Loss: 0.00001473
Iteration 72/1000 | Loss: 0.00001473
Iteration 73/1000 | Loss: 0.00001473
Iteration 74/1000 | Loss: 0.00001473
Iteration 75/1000 | Loss: 0.00001473
Iteration 76/1000 | Loss: 0.00001473
Iteration 77/1000 | Loss: 0.00001472
Iteration 78/1000 | Loss: 0.00001472
Iteration 79/1000 | Loss: 0.00001472
Iteration 80/1000 | Loss: 0.00001472
Iteration 81/1000 | Loss: 0.00001472
Iteration 82/1000 | Loss: 0.00001472
Iteration 83/1000 | Loss: 0.00001472
Iteration 84/1000 | Loss: 0.00001472
Iteration 85/1000 | Loss: 0.00001472
Iteration 86/1000 | Loss: 0.00001472
Iteration 87/1000 | Loss: 0.00001472
Iteration 88/1000 | Loss: 0.00001472
Iteration 89/1000 | Loss: 0.00001471
Iteration 90/1000 | Loss: 0.00001471
Iteration 91/1000 | Loss: 0.00001471
Iteration 92/1000 | Loss: 0.00001471
Iteration 93/1000 | Loss: 0.00001470
Iteration 94/1000 | Loss: 0.00001470
Iteration 95/1000 | Loss: 0.00001470
Iteration 96/1000 | Loss: 0.00001470
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001469
Iteration 99/1000 | Loss: 0.00001469
Iteration 100/1000 | Loss: 0.00001469
Iteration 101/1000 | Loss: 0.00001469
Iteration 102/1000 | Loss: 0.00001469
Iteration 103/1000 | Loss: 0.00001469
Iteration 104/1000 | Loss: 0.00001469
Iteration 105/1000 | Loss: 0.00001469
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001469
Iteration 108/1000 | Loss: 0.00001469
Iteration 109/1000 | Loss: 0.00001468
Iteration 110/1000 | Loss: 0.00001468
Iteration 111/1000 | Loss: 0.00001468
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001468
Iteration 117/1000 | Loss: 0.00001467
Iteration 118/1000 | Loss: 0.00001467
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001466
Iteration 121/1000 | Loss: 0.00001466
Iteration 122/1000 | Loss: 0.00001466
Iteration 123/1000 | Loss: 0.00001466
Iteration 124/1000 | Loss: 0.00001466
Iteration 125/1000 | Loss: 0.00001465
Iteration 126/1000 | Loss: 0.00001465
Iteration 127/1000 | Loss: 0.00001465
Iteration 128/1000 | Loss: 0.00001465
Iteration 129/1000 | Loss: 0.00001465
Iteration 130/1000 | Loss: 0.00001465
Iteration 131/1000 | Loss: 0.00001464
Iteration 132/1000 | Loss: 0.00001464
Iteration 133/1000 | Loss: 0.00001464
Iteration 134/1000 | Loss: 0.00001464
Iteration 135/1000 | Loss: 0.00001464
Iteration 136/1000 | Loss: 0.00001464
Iteration 137/1000 | Loss: 0.00001464
Iteration 138/1000 | Loss: 0.00001464
Iteration 139/1000 | Loss: 0.00001464
Iteration 140/1000 | Loss: 0.00001463
Iteration 141/1000 | Loss: 0.00001463
Iteration 142/1000 | Loss: 0.00001463
Iteration 143/1000 | Loss: 0.00001463
Iteration 144/1000 | Loss: 0.00001463
Iteration 145/1000 | Loss: 0.00001463
Iteration 146/1000 | Loss: 0.00001463
Iteration 147/1000 | Loss: 0.00001463
Iteration 148/1000 | Loss: 0.00001462
Iteration 149/1000 | Loss: 0.00001462
Iteration 150/1000 | Loss: 0.00001462
Iteration 151/1000 | Loss: 0.00001462
Iteration 152/1000 | Loss: 0.00001462
Iteration 153/1000 | Loss: 0.00001462
Iteration 154/1000 | Loss: 0.00001462
Iteration 155/1000 | Loss: 0.00001462
Iteration 156/1000 | Loss: 0.00001462
Iteration 157/1000 | Loss: 0.00001462
Iteration 158/1000 | Loss: 0.00001461
Iteration 159/1000 | Loss: 0.00001461
Iteration 160/1000 | Loss: 0.00001461
Iteration 161/1000 | Loss: 0.00001461
Iteration 162/1000 | Loss: 0.00001461
Iteration 163/1000 | Loss: 0.00001461
Iteration 164/1000 | Loss: 0.00001461
Iteration 165/1000 | Loss: 0.00001461
Iteration 166/1000 | Loss: 0.00001461
Iteration 167/1000 | Loss: 0.00001461
Iteration 168/1000 | Loss: 0.00001461
Iteration 169/1000 | Loss: 0.00001461
Iteration 170/1000 | Loss: 0.00001461
Iteration 171/1000 | Loss: 0.00001461
Iteration 172/1000 | Loss: 0.00001461
Iteration 173/1000 | Loss: 0.00001461
Iteration 174/1000 | Loss: 0.00001461
Iteration 175/1000 | Loss: 0.00001461
Iteration 176/1000 | Loss: 0.00001461
Iteration 177/1000 | Loss: 0.00001461
Iteration 178/1000 | Loss: 0.00001461
Iteration 179/1000 | Loss: 0.00001461
Iteration 180/1000 | Loss: 0.00001461
Iteration 181/1000 | Loss: 0.00001461
Iteration 182/1000 | Loss: 0.00001461
Iteration 183/1000 | Loss: 0.00001461
Iteration 184/1000 | Loss: 0.00001461
Iteration 185/1000 | Loss: 0.00001461
Iteration 186/1000 | Loss: 0.00001461
Iteration 187/1000 | Loss: 0.00001461
Iteration 188/1000 | Loss: 0.00001461
Iteration 189/1000 | Loss: 0.00001461
Iteration 190/1000 | Loss: 0.00001461
Iteration 191/1000 | Loss: 0.00001461
Iteration 192/1000 | Loss: 0.00001461
Iteration 193/1000 | Loss: 0.00001461
Iteration 194/1000 | Loss: 0.00001461
Iteration 195/1000 | Loss: 0.00001461
Iteration 196/1000 | Loss: 0.00001461
Iteration 197/1000 | Loss: 0.00001461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.4605301657866221e-05, 1.4605301657866221e-05, 1.4605301657866221e-05, 1.4605301657866221e-05, 1.4605301657866221e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4605301657866221e-05

Optimization complete. Final v2v error: 3.1918137073516846 mm

Highest mean error: 3.593841075897217 mm for frame 238

Lowest mean error: 2.777097225189209 mm for frame 100

Saving results

Total time: 43.554311752319336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979594
Iteration 2/25 | Loss: 0.00209948
Iteration 3/25 | Loss: 0.00137386
Iteration 4/25 | Loss: 0.00127990
Iteration 5/25 | Loss: 0.00123088
Iteration 6/25 | Loss: 0.00119690
Iteration 7/25 | Loss: 0.00118124
Iteration 8/25 | Loss: 0.00116758
Iteration 9/25 | Loss: 0.00116551
Iteration 10/25 | Loss: 0.00115968
Iteration 11/25 | Loss: 0.00115702
Iteration 12/25 | Loss: 0.00115524
Iteration 13/25 | Loss: 0.00115431
Iteration 14/25 | Loss: 0.00115379
Iteration 15/25 | Loss: 0.00115555
Iteration 16/25 | Loss: 0.00115222
Iteration 17/25 | Loss: 0.00115158
Iteration 18/25 | Loss: 0.00115184
Iteration 19/25 | Loss: 0.00115190
Iteration 20/25 | Loss: 0.00115141
Iteration 21/25 | Loss: 0.00115176
Iteration 22/25 | Loss: 0.00115146
Iteration 23/25 | Loss: 0.00115209
Iteration 24/25 | Loss: 0.00115141
Iteration 25/25 | Loss: 0.00115167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84876132
Iteration 2/25 | Loss: 0.00056603
Iteration 3/25 | Loss: 0.00056599
Iteration 4/25 | Loss: 0.00056599
Iteration 5/25 | Loss: 0.00056599
Iteration 6/25 | Loss: 0.00056599
Iteration 7/25 | Loss: 0.00056599
Iteration 8/25 | Loss: 0.00056599
Iteration 9/25 | Loss: 0.00056599
Iteration 10/25 | Loss: 0.00056599
Iteration 11/25 | Loss: 0.00056599
Iteration 12/25 | Loss: 0.00056599
Iteration 13/25 | Loss: 0.00056599
Iteration 14/25 | Loss: 0.00056599
Iteration 15/25 | Loss: 0.00056599
Iteration 16/25 | Loss: 0.00056599
Iteration 17/25 | Loss: 0.00056599
Iteration 18/25 | Loss: 0.00056599
Iteration 19/25 | Loss: 0.00056599
Iteration 20/25 | Loss: 0.00056599
Iteration 21/25 | Loss: 0.00056599
Iteration 22/25 | Loss: 0.00056599
Iteration 23/25 | Loss: 0.00056599
Iteration 24/25 | Loss: 0.00056599
Iteration 25/25 | Loss: 0.00056599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056599
Iteration 2/1000 | Loss: 0.00004978
Iteration 3/1000 | Loss: 0.00003546
Iteration 4/1000 | Loss: 0.00007913
Iteration 5/1000 | Loss: 0.00004825
Iteration 6/1000 | Loss: 0.00003406
Iteration 7/1000 | Loss: 0.00002882
Iteration 8/1000 | Loss: 0.00003534
Iteration 9/1000 | Loss: 0.00002665
Iteration 10/1000 | Loss: 0.00003324
Iteration 11/1000 | Loss: 0.00003737
Iteration 12/1000 | Loss: 0.00002558
Iteration 13/1000 | Loss: 0.00003269
Iteration 14/1000 | Loss: 0.00003280
Iteration 15/1000 | Loss: 0.00002876
Iteration 16/1000 | Loss: 0.00002876
Iteration 17/1000 | Loss: 0.00003989
Iteration 18/1000 | Loss: 0.00003469
Iteration 19/1000 | Loss: 0.00002673
Iteration 20/1000 | Loss: 0.00003266
Iteration 21/1000 | Loss: 0.00003199
Iteration 22/1000 | Loss: 0.00003010
Iteration 23/1000 | Loss: 0.00003415
Iteration 24/1000 | Loss: 0.00003033
Iteration 25/1000 | Loss: 0.00003471
Iteration 26/1000 | Loss: 0.00003242
Iteration 27/1000 | Loss: 0.00003429
Iteration 28/1000 | Loss: 0.00003120
Iteration 29/1000 | Loss: 0.00003433
Iteration 30/1000 | Loss: 0.00003890
Iteration 31/1000 | Loss: 0.00003397
Iteration 32/1000 | Loss: 0.00003758
Iteration 33/1000 | Loss: 0.00002961
Iteration 34/1000 | Loss: 0.00003016
Iteration 35/1000 | Loss: 0.00002842
Iteration 36/1000 | Loss: 0.00003176
Iteration 37/1000 | Loss: 0.00003229
Iteration 38/1000 | Loss: 0.00003222
Iteration 39/1000 | Loss: 0.00002487
Iteration 40/1000 | Loss: 0.00002380
Iteration 41/1000 | Loss: 0.00002326
Iteration 42/1000 | Loss: 0.00002285
Iteration 43/1000 | Loss: 0.00002266
Iteration 44/1000 | Loss: 0.00002260
Iteration 45/1000 | Loss: 0.00002259
Iteration 46/1000 | Loss: 0.00002258
Iteration 47/1000 | Loss: 0.00002258
Iteration 48/1000 | Loss: 0.00002258
Iteration 49/1000 | Loss: 0.00002257
Iteration 50/1000 | Loss: 0.00002257
Iteration 51/1000 | Loss: 0.00002257
Iteration 52/1000 | Loss: 0.00002257
Iteration 53/1000 | Loss: 0.00002257
Iteration 54/1000 | Loss: 0.00002257
Iteration 55/1000 | Loss: 0.00002257
Iteration 56/1000 | Loss: 0.00002256
Iteration 57/1000 | Loss: 0.00002256
Iteration 58/1000 | Loss: 0.00002256
Iteration 59/1000 | Loss: 0.00002256
Iteration 60/1000 | Loss: 0.00002256
Iteration 61/1000 | Loss: 0.00002256
Iteration 62/1000 | Loss: 0.00002256
Iteration 63/1000 | Loss: 0.00002256
Iteration 64/1000 | Loss: 0.00002256
Iteration 65/1000 | Loss: 0.00002256
Iteration 66/1000 | Loss: 0.00002256
Iteration 67/1000 | Loss: 0.00002256
Iteration 68/1000 | Loss: 0.00002255
Iteration 69/1000 | Loss: 0.00002255
Iteration 70/1000 | Loss: 0.00002255
Iteration 71/1000 | Loss: 0.00002255
Iteration 72/1000 | Loss: 0.00002255
Iteration 73/1000 | Loss: 0.00002255
Iteration 74/1000 | Loss: 0.00002255
Iteration 75/1000 | Loss: 0.00002254
Iteration 76/1000 | Loss: 0.00002254
Iteration 77/1000 | Loss: 0.00002254
Iteration 78/1000 | Loss: 0.00002254
Iteration 79/1000 | Loss: 0.00002254
Iteration 80/1000 | Loss: 0.00002253
Iteration 81/1000 | Loss: 0.00002253
Iteration 82/1000 | Loss: 0.00002252
Iteration 83/1000 | Loss: 0.00002252
Iteration 84/1000 | Loss: 0.00002250
Iteration 85/1000 | Loss: 0.00002250
Iteration 86/1000 | Loss: 0.00002249
Iteration 87/1000 | Loss: 0.00002249
Iteration 88/1000 | Loss: 0.00002249
Iteration 89/1000 | Loss: 0.00002249
Iteration 90/1000 | Loss: 0.00002249
Iteration 91/1000 | Loss: 0.00002249
Iteration 92/1000 | Loss: 0.00002249
Iteration 93/1000 | Loss: 0.00002249
Iteration 94/1000 | Loss: 0.00002249
Iteration 95/1000 | Loss: 0.00002248
Iteration 96/1000 | Loss: 0.00002248
Iteration 97/1000 | Loss: 0.00002248
Iteration 98/1000 | Loss: 0.00002248
Iteration 99/1000 | Loss: 0.00002245
Iteration 100/1000 | Loss: 0.00002245
Iteration 101/1000 | Loss: 0.00002245
Iteration 102/1000 | Loss: 0.00002244
Iteration 103/1000 | Loss: 0.00002244
Iteration 104/1000 | Loss: 0.00002242
Iteration 105/1000 | Loss: 0.00002242
Iteration 106/1000 | Loss: 0.00002241
Iteration 107/1000 | Loss: 0.00002241
Iteration 108/1000 | Loss: 0.00002241
Iteration 109/1000 | Loss: 0.00002241
Iteration 110/1000 | Loss: 0.00002241
Iteration 111/1000 | Loss: 0.00002241
Iteration 112/1000 | Loss: 0.00002241
Iteration 113/1000 | Loss: 0.00002241
Iteration 114/1000 | Loss: 0.00002240
Iteration 115/1000 | Loss: 0.00002240
Iteration 116/1000 | Loss: 0.00002238
Iteration 117/1000 | Loss: 0.00002238
Iteration 118/1000 | Loss: 0.00002237
Iteration 119/1000 | Loss: 0.00002237
Iteration 120/1000 | Loss: 0.00002237
Iteration 121/1000 | Loss: 0.00002237
Iteration 122/1000 | Loss: 0.00002237
Iteration 123/1000 | Loss: 0.00002233
Iteration 124/1000 | Loss: 0.00002233
Iteration 125/1000 | Loss: 0.00002233
Iteration 126/1000 | Loss: 0.00002233
Iteration 127/1000 | Loss: 0.00002233
Iteration 128/1000 | Loss: 0.00002232
Iteration 129/1000 | Loss: 0.00002232
Iteration 130/1000 | Loss: 0.00002232
Iteration 131/1000 | Loss: 0.00002232
Iteration 132/1000 | Loss: 0.00002231
Iteration 133/1000 | Loss: 0.00002231
Iteration 134/1000 | Loss: 0.00002230
Iteration 135/1000 | Loss: 0.00002230
Iteration 136/1000 | Loss: 0.00002229
Iteration 137/1000 | Loss: 0.00002229
Iteration 138/1000 | Loss: 0.00002229
Iteration 139/1000 | Loss: 0.00002229
Iteration 140/1000 | Loss: 0.00002229
Iteration 141/1000 | Loss: 0.00002229
Iteration 142/1000 | Loss: 0.00002229
Iteration 143/1000 | Loss: 0.00002229
Iteration 144/1000 | Loss: 0.00002229
Iteration 145/1000 | Loss: 0.00002228
Iteration 146/1000 | Loss: 0.00002228
Iteration 147/1000 | Loss: 0.00002226
Iteration 148/1000 | Loss: 0.00002226
Iteration 149/1000 | Loss: 0.00002225
Iteration 150/1000 | Loss: 0.00002225
Iteration 151/1000 | Loss: 0.00002225
Iteration 152/1000 | Loss: 0.00002225
Iteration 153/1000 | Loss: 0.00002224
Iteration 154/1000 | Loss: 0.00002224
Iteration 155/1000 | Loss: 0.00002224
Iteration 156/1000 | Loss: 0.00002223
Iteration 157/1000 | Loss: 0.00002222
Iteration 158/1000 | Loss: 0.00002221
Iteration 159/1000 | Loss: 0.00002221
Iteration 160/1000 | Loss: 0.00002220
Iteration 161/1000 | Loss: 0.00002217
Iteration 162/1000 | Loss: 0.00002216
Iteration 163/1000 | Loss: 0.00002216
Iteration 164/1000 | Loss: 0.00002216
Iteration 165/1000 | Loss: 0.00002215
Iteration 166/1000 | Loss: 0.00002215
Iteration 167/1000 | Loss: 0.00002214
Iteration 168/1000 | Loss: 0.00002213
Iteration 169/1000 | Loss: 0.00002211
Iteration 170/1000 | Loss: 0.00002210
Iteration 171/1000 | Loss: 0.00002209
Iteration 172/1000 | Loss: 0.00002208
Iteration 173/1000 | Loss: 0.00002208
Iteration 174/1000 | Loss: 0.00002207
Iteration 175/1000 | Loss: 0.00002207
Iteration 176/1000 | Loss: 0.00002206
Iteration 177/1000 | Loss: 0.00002205
Iteration 178/1000 | Loss: 0.00002205
Iteration 179/1000 | Loss: 0.00002204
Iteration 180/1000 | Loss: 0.00002204
Iteration 181/1000 | Loss: 0.00002204
Iteration 182/1000 | Loss: 0.00002204
Iteration 183/1000 | Loss: 0.00002204
Iteration 184/1000 | Loss: 0.00002204
Iteration 185/1000 | Loss: 0.00002204
Iteration 186/1000 | Loss: 0.00002204
Iteration 187/1000 | Loss: 0.00002203
Iteration 188/1000 | Loss: 0.00002203
Iteration 189/1000 | Loss: 0.00002202
Iteration 190/1000 | Loss: 0.00002201
Iteration 191/1000 | Loss: 0.00002200
Iteration 192/1000 | Loss: 0.00002200
Iteration 193/1000 | Loss: 0.00002199
Iteration 194/1000 | Loss: 0.00002199
Iteration 195/1000 | Loss: 0.00002198
Iteration 196/1000 | Loss: 0.00002198
Iteration 197/1000 | Loss: 0.00002198
Iteration 198/1000 | Loss: 0.00002198
Iteration 199/1000 | Loss: 0.00002198
Iteration 200/1000 | Loss: 0.00002198
Iteration 201/1000 | Loss: 0.00002197
Iteration 202/1000 | Loss: 0.00002197
Iteration 203/1000 | Loss: 0.00002197
Iteration 204/1000 | Loss: 0.00002197
Iteration 205/1000 | Loss: 0.00002197
Iteration 206/1000 | Loss: 0.00002197
Iteration 207/1000 | Loss: 0.00002197
Iteration 208/1000 | Loss: 0.00002196
Iteration 209/1000 | Loss: 0.00002196
Iteration 210/1000 | Loss: 0.00002196
Iteration 211/1000 | Loss: 0.00002196
Iteration 212/1000 | Loss: 0.00002196
Iteration 213/1000 | Loss: 0.00002195
Iteration 214/1000 | Loss: 0.00002195
Iteration 215/1000 | Loss: 0.00002195
Iteration 216/1000 | Loss: 0.00002195
Iteration 217/1000 | Loss: 0.00002195
Iteration 218/1000 | Loss: 0.00002195
Iteration 219/1000 | Loss: 0.00002195
Iteration 220/1000 | Loss: 0.00002195
Iteration 221/1000 | Loss: 0.00002195
Iteration 222/1000 | Loss: 0.00002195
Iteration 223/1000 | Loss: 0.00002195
Iteration 224/1000 | Loss: 0.00002195
Iteration 225/1000 | Loss: 0.00002195
Iteration 226/1000 | Loss: 0.00002195
Iteration 227/1000 | Loss: 0.00002195
Iteration 228/1000 | Loss: 0.00002195
Iteration 229/1000 | Loss: 0.00002195
Iteration 230/1000 | Loss: 0.00002195
Iteration 231/1000 | Loss: 0.00002195
Iteration 232/1000 | Loss: 0.00002194
Iteration 233/1000 | Loss: 0.00002194
Iteration 234/1000 | Loss: 0.00002194
Iteration 235/1000 | Loss: 0.00002194
Iteration 236/1000 | Loss: 0.00002194
Iteration 237/1000 | Loss: 0.00002194
Iteration 238/1000 | Loss: 0.00002194
Iteration 239/1000 | Loss: 0.00002194
Iteration 240/1000 | Loss: 0.00002194
Iteration 241/1000 | Loss: 0.00002193
Iteration 242/1000 | Loss: 0.00002193
Iteration 243/1000 | Loss: 0.00002193
Iteration 244/1000 | Loss: 0.00002193
Iteration 245/1000 | Loss: 0.00002193
Iteration 246/1000 | Loss: 0.00002193
Iteration 247/1000 | Loss: 0.00002193
Iteration 248/1000 | Loss: 0.00002193
Iteration 249/1000 | Loss: 0.00002193
Iteration 250/1000 | Loss: 0.00002193
Iteration 251/1000 | Loss: 0.00002193
Iteration 252/1000 | Loss: 0.00002193
Iteration 253/1000 | Loss: 0.00002192
Iteration 254/1000 | Loss: 0.00002192
Iteration 255/1000 | Loss: 0.00002192
Iteration 256/1000 | Loss: 0.00002192
Iteration 257/1000 | Loss: 0.00002192
Iteration 258/1000 | Loss: 0.00002192
Iteration 259/1000 | Loss: 0.00002192
Iteration 260/1000 | Loss: 0.00002192
Iteration 261/1000 | Loss: 0.00002192
Iteration 262/1000 | Loss: 0.00002192
Iteration 263/1000 | Loss: 0.00002191
Iteration 264/1000 | Loss: 0.00002191
Iteration 265/1000 | Loss: 0.00002191
Iteration 266/1000 | Loss: 0.00002191
Iteration 267/1000 | Loss: 0.00002191
Iteration 268/1000 | Loss: 0.00002191
Iteration 269/1000 | Loss: 0.00002190
Iteration 270/1000 | Loss: 0.00002190
Iteration 271/1000 | Loss: 0.00002190
Iteration 272/1000 | Loss: 0.00002190
Iteration 273/1000 | Loss: 0.00002190
Iteration 274/1000 | Loss: 0.00002190
Iteration 275/1000 | Loss: 0.00002190
Iteration 276/1000 | Loss: 0.00002190
Iteration 277/1000 | Loss: 0.00002190
Iteration 278/1000 | Loss: 0.00002190
Iteration 279/1000 | Loss: 0.00002190
Iteration 280/1000 | Loss: 0.00002190
Iteration 281/1000 | Loss: 0.00002190
Iteration 282/1000 | Loss: 0.00002190
Iteration 283/1000 | Loss: 0.00002189
Iteration 284/1000 | Loss: 0.00002189
Iteration 285/1000 | Loss: 0.00002189
Iteration 286/1000 | Loss: 0.00002189
Iteration 287/1000 | Loss: 0.00002189
Iteration 288/1000 | Loss: 0.00002189
Iteration 289/1000 | Loss: 0.00002189
Iteration 290/1000 | Loss: 0.00002189
Iteration 291/1000 | Loss: 0.00002188
Iteration 292/1000 | Loss: 0.00002188
Iteration 293/1000 | Loss: 0.00002188
Iteration 294/1000 | Loss: 0.00002188
Iteration 295/1000 | Loss: 0.00002188
Iteration 296/1000 | Loss: 0.00002188
Iteration 297/1000 | Loss: 0.00002188
Iteration 298/1000 | Loss: 0.00002188
Iteration 299/1000 | Loss: 0.00002188
Iteration 300/1000 | Loss: 0.00002188
Iteration 301/1000 | Loss: 0.00002188
Iteration 302/1000 | Loss: 0.00002188
Iteration 303/1000 | Loss: 0.00002188
Iteration 304/1000 | Loss: 0.00002188
Iteration 305/1000 | Loss: 0.00002187
Iteration 306/1000 | Loss: 0.00002187
Iteration 307/1000 | Loss: 0.00002187
Iteration 308/1000 | Loss: 0.00002187
Iteration 309/1000 | Loss: 0.00002187
Iteration 310/1000 | Loss: 0.00002187
Iteration 311/1000 | Loss: 0.00002187
Iteration 312/1000 | Loss: 0.00002187
Iteration 313/1000 | Loss: 0.00002187
Iteration 314/1000 | Loss: 0.00002187
Iteration 315/1000 | Loss: 0.00002187
Iteration 316/1000 | Loss: 0.00002187
Iteration 317/1000 | Loss: 0.00002187
Iteration 318/1000 | Loss: 0.00002187
Iteration 319/1000 | Loss: 0.00002187
Iteration 320/1000 | Loss: 0.00002187
Iteration 321/1000 | Loss: 0.00002187
Iteration 322/1000 | Loss: 0.00002187
Iteration 323/1000 | Loss: 0.00002187
Iteration 324/1000 | Loss: 0.00002187
Iteration 325/1000 | Loss: 0.00002187
Iteration 326/1000 | Loss: 0.00002187
Iteration 327/1000 | Loss: 0.00002187
Iteration 328/1000 | Loss: 0.00002187
Iteration 329/1000 | Loss: 0.00002187
Iteration 330/1000 | Loss: 0.00002187
Iteration 331/1000 | Loss: 0.00002187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [2.1866422684979625e-05, 2.1866422684979625e-05, 2.1866422684979625e-05, 2.1866422684979625e-05, 2.1866422684979625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1866422684979625e-05

Optimization complete. Final v2v error: 3.571678876876831 mm

Highest mean error: 12.068528175354004 mm for frame 6

Lowest mean error: 3.23433780670166 mm for frame 149

Saving results

Total time: 146.520521402359
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00628150
Iteration 2/25 | Loss: 0.00153315
Iteration 3/25 | Loss: 0.00124937
Iteration 4/25 | Loss: 0.00120472
Iteration 5/25 | Loss: 0.00119315
Iteration 6/25 | Loss: 0.00118348
Iteration 7/25 | Loss: 0.00117369
Iteration 8/25 | Loss: 0.00116849
Iteration 9/25 | Loss: 0.00116639
Iteration 10/25 | Loss: 0.00117558
Iteration 11/25 | Loss: 0.00116626
Iteration 12/25 | Loss: 0.00115801
Iteration 13/25 | Loss: 0.00115417
Iteration 14/25 | Loss: 0.00115283
Iteration 15/25 | Loss: 0.00115228
Iteration 16/25 | Loss: 0.00115210
Iteration 17/25 | Loss: 0.00115202
Iteration 18/25 | Loss: 0.00115186
Iteration 19/25 | Loss: 0.00115171
Iteration 20/25 | Loss: 0.00115158
Iteration 21/25 | Loss: 0.00115145
Iteration 22/25 | Loss: 0.00115131
Iteration 23/25 | Loss: 0.00115121
Iteration 24/25 | Loss: 0.00115112
Iteration 25/25 | Loss: 0.00115109

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29702604
Iteration 2/25 | Loss: 0.00166284
Iteration 3/25 | Loss: 0.00166284
Iteration 4/25 | Loss: 0.00166284
Iteration 5/25 | Loss: 0.00166284
Iteration 6/25 | Loss: 0.00166284
Iteration 7/25 | Loss: 0.00166284
Iteration 8/25 | Loss: 0.00166283
Iteration 9/25 | Loss: 0.00166283
Iteration 10/25 | Loss: 0.00166283
Iteration 11/25 | Loss: 0.00166283
Iteration 12/25 | Loss: 0.00166283
Iteration 13/25 | Loss: 0.00166283
Iteration 14/25 | Loss: 0.00166283
Iteration 15/25 | Loss: 0.00166283
Iteration 16/25 | Loss: 0.00166283
Iteration 17/25 | Loss: 0.00166283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016628338489681482, 0.0016628338489681482, 0.0016628338489681482, 0.0016628338489681482, 0.0016628338489681482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016628338489681482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166283
Iteration 2/1000 | Loss: 0.00088733
Iteration 3/1000 | Loss: 0.00129795
Iteration 4/1000 | Loss: 0.00040918
Iteration 5/1000 | Loss: 0.00013677
Iteration 6/1000 | Loss: 0.00020147
Iteration 7/1000 | Loss: 0.00011429
Iteration 8/1000 | Loss: 0.00010482
Iteration 9/1000 | Loss: 0.00009770
Iteration 10/1000 | Loss: 0.00026284
Iteration 11/1000 | Loss: 0.00021977
Iteration 12/1000 | Loss: 0.00023265
Iteration 13/1000 | Loss: 0.00053294
Iteration 14/1000 | Loss: 0.00172614
Iteration 15/1000 | Loss: 0.00127355
Iteration 16/1000 | Loss: 0.00084018
Iteration 17/1000 | Loss: 0.00034024
Iteration 18/1000 | Loss: 0.00009491
Iteration 19/1000 | Loss: 0.00026320
Iteration 20/1000 | Loss: 0.00041109
Iteration 21/1000 | Loss: 0.00038667
Iteration 22/1000 | Loss: 0.00008750
Iteration 23/1000 | Loss: 0.00008451
Iteration 24/1000 | Loss: 0.00008060
Iteration 25/1000 | Loss: 0.00007603
Iteration 26/1000 | Loss: 0.00024175
Iteration 27/1000 | Loss: 0.00007733
Iteration 28/1000 | Loss: 0.00007205
Iteration 29/1000 | Loss: 0.00058654
Iteration 30/1000 | Loss: 0.00006980
Iteration 31/1000 | Loss: 0.00006615
Iteration 32/1000 | Loss: 0.00006441
Iteration 33/1000 | Loss: 0.00006308
Iteration 34/1000 | Loss: 0.00039649
Iteration 35/1000 | Loss: 0.00065368
Iteration 36/1000 | Loss: 0.00058771
Iteration 37/1000 | Loss: 0.00008115
Iteration 38/1000 | Loss: 0.00055412
Iteration 39/1000 | Loss: 0.00014720
Iteration 40/1000 | Loss: 0.00069066
Iteration 41/1000 | Loss: 0.00045419
Iteration 42/1000 | Loss: 0.00012194
Iteration 43/1000 | Loss: 0.00027972
Iteration 44/1000 | Loss: 0.00070833
Iteration 45/1000 | Loss: 0.00048355
Iteration 46/1000 | Loss: 0.00007549
Iteration 47/1000 | Loss: 0.00006617
Iteration 48/1000 | Loss: 0.00006376
Iteration 49/1000 | Loss: 0.00067233
Iteration 50/1000 | Loss: 0.00035029
Iteration 51/1000 | Loss: 0.00023355
Iteration 52/1000 | Loss: 0.00046115
Iteration 53/1000 | Loss: 0.00029586
Iteration 54/1000 | Loss: 0.00006639
Iteration 55/1000 | Loss: 0.00023616
Iteration 56/1000 | Loss: 0.00011198
Iteration 57/1000 | Loss: 0.00006676
Iteration 58/1000 | Loss: 0.00026288
Iteration 59/1000 | Loss: 0.00011003
Iteration 60/1000 | Loss: 0.00018362
Iteration 61/1000 | Loss: 0.00018422
Iteration 62/1000 | Loss: 0.00008462
Iteration 63/1000 | Loss: 0.00006039
Iteration 64/1000 | Loss: 0.00005960
Iteration 65/1000 | Loss: 0.00018970
Iteration 66/1000 | Loss: 0.00005952
Iteration 67/1000 | Loss: 0.00005761
Iteration 68/1000 | Loss: 0.00005699
Iteration 69/1000 | Loss: 0.00018473
Iteration 70/1000 | Loss: 0.00049816
Iteration 71/1000 | Loss: 0.00015840
Iteration 72/1000 | Loss: 0.00047765
Iteration 73/1000 | Loss: 0.00035514
Iteration 74/1000 | Loss: 0.00005827
Iteration 75/1000 | Loss: 0.00005668
Iteration 76/1000 | Loss: 0.00005616
Iteration 77/1000 | Loss: 0.00005580
Iteration 78/1000 | Loss: 0.00005557
Iteration 79/1000 | Loss: 0.00005540
Iteration 80/1000 | Loss: 0.00005523
Iteration 81/1000 | Loss: 0.00043323
Iteration 82/1000 | Loss: 0.00006038
Iteration 83/1000 | Loss: 0.00005763
Iteration 84/1000 | Loss: 0.00005664
Iteration 85/1000 | Loss: 0.00005611
Iteration 86/1000 | Loss: 0.00005580
Iteration 87/1000 | Loss: 0.00033806
Iteration 88/1000 | Loss: 0.00005674
Iteration 89/1000 | Loss: 0.00005502
Iteration 90/1000 | Loss: 0.00005435
Iteration 91/1000 | Loss: 0.00005375
Iteration 92/1000 | Loss: 0.00005319
Iteration 93/1000 | Loss: 0.00005281
Iteration 94/1000 | Loss: 0.00005260
Iteration 95/1000 | Loss: 0.00005245
Iteration 96/1000 | Loss: 0.00005229
Iteration 97/1000 | Loss: 0.00005229
Iteration 98/1000 | Loss: 0.00005229
Iteration 99/1000 | Loss: 0.00005228
Iteration 100/1000 | Loss: 0.00005228
Iteration 101/1000 | Loss: 0.00005228
Iteration 102/1000 | Loss: 0.00005228
Iteration 103/1000 | Loss: 0.00005228
Iteration 104/1000 | Loss: 0.00005228
Iteration 105/1000 | Loss: 0.00005228
Iteration 106/1000 | Loss: 0.00005228
Iteration 107/1000 | Loss: 0.00005228
Iteration 108/1000 | Loss: 0.00005228
Iteration 109/1000 | Loss: 0.00005227
Iteration 110/1000 | Loss: 0.00005226
Iteration 111/1000 | Loss: 0.00005226
Iteration 112/1000 | Loss: 0.00005226
Iteration 113/1000 | Loss: 0.00005225
Iteration 114/1000 | Loss: 0.00005225
Iteration 115/1000 | Loss: 0.00036381
Iteration 116/1000 | Loss: 0.00005365
Iteration 117/1000 | Loss: 0.00005199
Iteration 118/1000 | Loss: 0.00005137
Iteration 119/1000 | Loss: 0.00005081
Iteration 120/1000 | Loss: 0.00005037
Iteration 121/1000 | Loss: 0.00005010
Iteration 122/1000 | Loss: 0.00004998
Iteration 123/1000 | Loss: 0.00004996
Iteration 124/1000 | Loss: 0.00004993
Iteration 125/1000 | Loss: 0.00004985
Iteration 126/1000 | Loss: 0.00004985
Iteration 127/1000 | Loss: 0.00004984
Iteration 128/1000 | Loss: 0.00004983
Iteration 129/1000 | Loss: 0.00004983
Iteration 130/1000 | Loss: 0.00004982
Iteration 131/1000 | Loss: 0.00004981
Iteration 132/1000 | Loss: 0.00004981
Iteration 133/1000 | Loss: 0.00004980
Iteration 134/1000 | Loss: 0.00004980
Iteration 135/1000 | Loss: 0.00004980
Iteration 136/1000 | Loss: 0.00004979
Iteration 137/1000 | Loss: 0.00004979
Iteration 138/1000 | Loss: 0.00004979
Iteration 139/1000 | Loss: 0.00004979
Iteration 140/1000 | Loss: 0.00004979
Iteration 141/1000 | Loss: 0.00004979
Iteration 142/1000 | Loss: 0.00004979
Iteration 143/1000 | Loss: 0.00004978
Iteration 144/1000 | Loss: 0.00004978
Iteration 145/1000 | Loss: 0.00004978
Iteration 146/1000 | Loss: 0.00004978
Iteration 147/1000 | Loss: 0.00004978
Iteration 148/1000 | Loss: 0.00004978
Iteration 149/1000 | Loss: 0.00004977
Iteration 150/1000 | Loss: 0.00004977
Iteration 151/1000 | Loss: 0.00004977
Iteration 152/1000 | Loss: 0.00004977
Iteration 153/1000 | Loss: 0.00004977
Iteration 154/1000 | Loss: 0.00004977
Iteration 155/1000 | Loss: 0.00004977
Iteration 156/1000 | Loss: 0.00004976
Iteration 157/1000 | Loss: 0.00004976
Iteration 158/1000 | Loss: 0.00004976
Iteration 159/1000 | Loss: 0.00004976
Iteration 160/1000 | Loss: 0.00004975
Iteration 161/1000 | Loss: 0.00004975
Iteration 162/1000 | Loss: 0.00004975
Iteration 163/1000 | Loss: 0.00004975
Iteration 164/1000 | Loss: 0.00004975
Iteration 165/1000 | Loss: 0.00004975
Iteration 166/1000 | Loss: 0.00004975
Iteration 167/1000 | Loss: 0.00004975
Iteration 168/1000 | Loss: 0.00004975
Iteration 169/1000 | Loss: 0.00004975
Iteration 170/1000 | Loss: 0.00004975
Iteration 171/1000 | Loss: 0.00004974
Iteration 172/1000 | Loss: 0.00004974
Iteration 173/1000 | Loss: 0.00004974
Iteration 174/1000 | Loss: 0.00004974
Iteration 175/1000 | Loss: 0.00004974
Iteration 176/1000 | Loss: 0.00004974
Iteration 177/1000 | Loss: 0.00004974
Iteration 178/1000 | Loss: 0.00004974
Iteration 179/1000 | Loss: 0.00004973
Iteration 180/1000 | Loss: 0.00004973
Iteration 181/1000 | Loss: 0.00004973
Iteration 182/1000 | Loss: 0.00004973
Iteration 183/1000 | Loss: 0.00004973
Iteration 184/1000 | Loss: 0.00004973
Iteration 185/1000 | Loss: 0.00004973
Iteration 186/1000 | Loss: 0.00004973
Iteration 187/1000 | Loss: 0.00004973
Iteration 188/1000 | Loss: 0.00004972
Iteration 189/1000 | Loss: 0.00004972
Iteration 190/1000 | Loss: 0.00004972
Iteration 191/1000 | Loss: 0.00004972
Iteration 192/1000 | Loss: 0.00004972
Iteration 193/1000 | Loss: 0.00004972
Iteration 194/1000 | Loss: 0.00004972
Iteration 195/1000 | Loss: 0.00004972
Iteration 196/1000 | Loss: 0.00004972
Iteration 197/1000 | Loss: 0.00004971
Iteration 198/1000 | Loss: 0.00004971
Iteration 199/1000 | Loss: 0.00004971
Iteration 200/1000 | Loss: 0.00004971
Iteration 201/1000 | Loss: 0.00004971
Iteration 202/1000 | Loss: 0.00004971
Iteration 203/1000 | Loss: 0.00004971
Iteration 204/1000 | Loss: 0.00004971
Iteration 205/1000 | Loss: 0.00004971
Iteration 206/1000 | Loss: 0.00004971
Iteration 207/1000 | Loss: 0.00004971
Iteration 208/1000 | Loss: 0.00004971
Iteration 209/1000 | Loss: 0.00004971
Iteration 210/1000 | Loss: 0.00004971
Iteration 211/1000 | Loss: 0.00004971
Iteration 212/1000 | Loss: 0.00004971
Iteration 213/1000 | Loss: 0.00004971
Iteration 214/1000 | Loss: 0.00004971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [4.9709629820426926e-05, 4.9709629820426926e-05, 4.9709629820426926e-05, 4.9709629820426926e-05, 4.9709629820426926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.9709629820426926e-05

Optimization complete. Final v2v error: 3.893598794937134 mm

Highest mean error: 11.376699447631836 mm for frame 21

Lowest mean error: 2.7160677909851074 mm for frame 208

Saving results

Total time: 218.97247290611267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043662
Iteration 2/25 | Loss: 0.00214921
Iteration 3/25 | Loss: 0.00161875
Iteration 4/25 | Loss: 0.00134773
Iteration 5/25 | Loss: 0.00129394
Iteration 6/25 | Loss: 0.00125431
Iteration 7/25 | Loss: 0.00122764
Iteration 8/25 | Loss: 0.00120695
Iteration 9/25 | Loss: 0.00116053
Iteration 10/25 | Loss: 0.00113991
Iteration 11/25 | Loss: 0.00114444
Iteration 12/25 | Loss: 0.00113205
Iteration 13/25 | Loss: 0.00112233
Iteration 14/25 | Loss: 0.00112064
Iteration 15/25 | Loss: 0.00112185
Iteration 16/25 | Loss: 0.00112520
Iteration 17/25 | Loss: 0.00111548
Iteration 18/25 | Loss: 0.00111063
Iteration 19/25 | Loss: 0.00110889
Iteration 20/25 | Loss: 0.00110796
Iteration 21/25 | Loss: 0.00110779
Iteration 22/25 | Loss: 0.00110639
Iteration 23/25 | Loss: 0.00110350
Iteration 24/25 | Loss: 0.00110692
Iteration 25/25 | Loss: 0.00110657

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33171427
Iteration 2/25 | Loss: 0.00159277
Iteration 3/25 | Loss: 0.00159276
Iteration 4/25 | Loss: 0.00159276
Iteration 5/25 | Loss: 0.00159276
Iteration 6/25 | Loss: 0.00159276
Iteration 7/25 | Loss: 0.00159276
Iteration 8/25 | Loss: 0.00159276
Iteration 9/25 | Loss: 0.00159276
Iteration 10/25 | Loss: 0.00159276
Iteration 11/25 | Loss: 0.00159276
Iteration 12/25 | Loss: 0.00159276
Iteration 13/25 | Loss: 0.00159276
Iteration 14/25 | Loss: 0.00159276
Iteration 15/25 | Loss: 0.00159276
Iteration 16/25 | Loss: 0.00159276
Iteration 17/25 | Loss: 0.00159276
Iteration 18/25 | Loss: 0.00159276
Iteration 19/25 | Loss: 0.00159276
Iteration 20/25 | Loss: 0.00159276
Iteration 21/25 | Loss: 0.00159276
Iteration 22/25 | Loss: 0.00159276
Iteration 23/25 | Loss: 0.00159276
Iteration 24/25 | Loss: 0.00159276
Iteration 25/25 | Loss: 0.00159276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159276
Iteration 2/1000 | Loss: 0.00012899
Iteration 3/1000 | Loss: 0.00010117
Iteration 4/1000 | Loss: 0.00008210
Iteration 5/1000 | Loss: 0.00007400
Iteration 6/1000 | Loss: 0.00006877
Iteration 7/1000 | Loss: 0.00029108
Iteration 8/1000 | Loss: 0.00007694
Iteration 9/1000 | Loss: 0.00007090
Iteration 10/1000 | Loss: 0.00006656
Iteration 11/1000 | Loss: 0.00006470
Iteration 12/1000 | Loss: 0.00036059
Iteration 13/1000 | Loss: 0.00036770
Iteration 14/1000 | Loss: 0.00029416
Iteration 15/1000 | Loss: 0.00043438
Iteration 16/1000 | Loss: 0.00014201
Iteration 17/1000 | Loss: 0.00027350
Iteration 18/1000 | Loss: 0.00017722
Iteration 19/1000 | Loss: 0.00006839
Iteration 20/1000 | Loss: 0.00006480
Iteration 21/1000 | Loss: 0.00090210
Iteration 22/1000 | Loss: 0.00623725
Iteration 23/1000 | Loss: 0.00041439
Iteration 24/1000 | Loss: 0.00016814
Iteration 25/1000 | Loss: 0.00038699
Iteration 26/1000 | Loss: 0.00028253
Iteration 27/1000 | Loss: 0.00010632
Iteration 28/1000 | Loss: 0.00006354
Iteration 29/1000 | Loss: 0.00008155
Iteration 30/1000 | Loss: 0.00004775
Iteration 31/1000 | Loss: 0.00033316
Iteration 32/1000 | Loss: 0.00005949
Iteration 33/1000 | Loss: 0.00008890
Iteration 34/1000 | Loss: 0.00002526
Iteration 35/1000 | Loss: 0.00032452
Iteration 36/1000 | Loss: 0.00059317
Iteration 37/1000 | Loss: 0.00193618
Iteration 38/1000 | Loss: 0.00016790
Iteration 39/1000 | Loss: 0.00007332
Iteration 40/1000 | Loss: 0.00003009
Iteration 41/1000 | Loss: 0.00002626
Iteration 42/1000 | Loss: 0.00002350
Iteration 43/1000 | Loss: 0.00002127
Iteration 44/1000 | Loss: 0.00015410
Iteration 45/1000 | Loss: 0.00002578
Iteration 46/1000 | Loss: 0.00002206
Iteration 47/1000 | Loss: 0.00001881
Iteration 48/1000 | Loss: 0.00001646
Iteration 49/1000 | Loss: 0.00001487
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001313
Iteration 52/1000 | Loss: 0.00001261
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00013418
Iteration 55/1000 | Loss: 0.00001251
Iteration 56/1000 | Loss: 0.00001220
Iteration 57/1000 | Loss: 0.00001211
Iteration 58/1000 | Loss: 0.00001211
Iteration 59/1000 | Loss: 0.00001211
Iteration 60/1000 | Loss: 0.00001211
Iteration 61/1000 | Loss: 0.00001211
Iteration 62/1000 | Loss: 0.00001211
Iteration 63/1000 | Loss: 0.00001210
Iteration 64/1000 | Loss: 0.00001209
Iteration 65/1000 | Loss: 0.00001208
Iteration 66/1000 | Loss: 0.00001208
Iteration 67/1000 | Loss: 0.00001207
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001206
Iteration 71/1000 | Loss: 0.00001206
Iteration 72/1000 | Loss: 0.00001205
Iteration 73/1000 | Loss: 0.00001205
Iteration 74/1000 | Loss: 0.00001201
Iteration 75/1000 | Loss: 0.00001201
Iteration 76/1000 | Loss: 0.00001200
Iteration 77/1000 | Loss: 0.00001200
Iteration 78/1000 | Loss: 0.00001200
Iteration 79/1000 | Loss: 0.00001200
Iteration 80/1000 | Loss: 0.00001200
Iteration 81/1000 | Loss: 0.00010027
Iteration 82/1000 | Loss: 0.00001318
Iteration 83/1000 | Loss: 0.00001201
Iteration 84/1000 | Loss: 0.00001196
Iteration 85/1000 | Loss: 0.00001195
Iteration 86/1000 | Loss: 0.00001195
Iteration 87/1000 | Loss: 0.00001195
Iteration 88/1000 | Loss: 0.00001194
Iteration 89/1000 | Loss: 0.00001194
Iteration 90/1000 | Loss: 0.00001193
Iteration 91/1000 | Loss: 0.00001192
Iteration 92/1000 | Loss: 0.00001192
Iteration 93/1000 | Loss: 0.00001191
Iteration 94/1000 | Loss: 0.00001191
Iteration 95/1000 | Loss: 0.00001191
Iteration 96/1000 | Loss: 0.00001191
Iteration 97/1000 | Loss: 0.00001191
Iteration 98/1000 | Loss: 0.00001191
Iteration 99/1000 | Loss: 0.00001191
Iteration 100/1000 | Loss: 0.00001191
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001190
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001189
Iteration 106/1000 | Loss: 0.00001189
Iteration 107/1000 | Loss: 0.00001189
Iteration 108/1000 | Loss: 0.00001189
Iteration 109/1000 | Loss: 0.00001189
Iteration 110/1000 | Loss: 0.00001189
Iteration 111/1000 | Loss: 0.00001189
Iteration 112/1000 | Loss: 0.00001189
Iteration 113/1000 | Loss: 0.00001189
Iteration 114/1000 | Loss: 0.00001189
Iteration 115/1000 | Loss: 0.00001189
Iteration 116/1000 | Loss: 0.00001189
Iteration 117/1000 | Loss: 0.00001188
Iteration 118/1000 | Loss: 0.00001188
Iteration 119/1000 | Loss: 0.00001188
Iteration 120/1000 | Loss: 0.00001188
Iteration 121/1000 | Loss: 0.00001188
Iteration 122/1000 | Loss: 0.00001188
Iteration 123/1000 | Loss: 0.00001188
Iteration 124/1000 | Loss: 0.00001188
Iteration 125/1000 | Loss: 0.00001188
Iteration 126/1000 | Loss: 0.00001188
Iteration 127/1000 | Loss: 0.00001188
Iteration 128/1000 | Loss: 0.00001187
Iteration 129/1000 | Loss: 0.00001187
Iteration 130/1000 | Loss: 0.00001187
Iteration 131/1000 | Loss: 0.00001187
Iteration 132/1000 | Loss: 0.00001187
Iteration 133/1000 | Loss: 0.00001187
Iteration 134/1000 | Loss: 0.00001187
Iteration 135/1000 | Loss: 0.00001186
Iteration 136/1000 | Loss: 0.00001186
Iteration 137/1000 | Loss: 0.00001186
Iteration 138/1000 | Loss: 0.00001186
Iteration 139/1000 | Loss: 0.00001186
Iteration 140/1000 | Loss: 0.00001186
Iteration 141/1000 | Loss: 0.00001185
Iteration 142/1000 | Loss: 0.00001185
Iteration 143/1000 | Loss: 0.00001185
Iteration 144/1000 | Loss: 0.00001184
Iteration 145/1000 | Loss: 0.00001184
Iteration 146/1000 | Loss: 0.00001184
Iteration 147/1000 | Loss: 0.00001184
Iteration 148/1000 | Loss: 0.00001184
Iteration 149/1000 | Loss: 0.00001184
Iteration 150/1000 | Loss: 0.00001184
Iteration 151/1000 | Loss: 0.00001184
Iteration 152/1000 | Loss: 0.00001184
Iteration 153/1000 | Loss: 0.00001184
Iteration 154/1000 | Loss: 0.00001184
Iteration 155/1000 | Loss: 0.00001184
Iteration 156/1000 | Loss: 0.00001184
Iteration 157/1000 | Loss: 0.00001184
Iteration 158/1000 | Loss: 0.00001183
Iteration 159/1000 | Loss: 0.00001183
Iteration 160/1000 | Loss: 0.00001183
Iteration 161/1000 | Loss: 0.00001183
Iteration 162/1000 | Loss: 0.00001183
Iteration 163/1000 | Loss: 0.00001183
Iteration 164/1000 | Loss: 0.00001183
Iteration 165/1000 | Loss: 0.00001183
Iteration 166/1000 | Loss: 0.00001183
Iteration 167/1000 | Loss: 0.00001183
Iteration 168/1000 | Loss: 0.00001183
Iteration 169/1000 | Loss: 0.00001183
Iteration 170/1000 | Loss: 0.00001183
Iteration 171/1000 | Loss: 0.00001183
Iteration 172/1000 | Loss: 0.00001183
Iteration 173/1000 | Loss: 0.00001183
Iteration 174/1000 | Loss: 0.00001183
Iteration 175/1000 | Loss: 0.00001183
Iteration 176/1000 | Loss: 0.00001183
Iteration 177/1000 | Loss: 0.00001183
Iteration 178/1000 | Loss: 0.00001183
Iteration 179/1000 | Loss: 0.00001183
Iteration 180/1000 | Loss: 0.00001183
Iteration 181/1000 | Loss: 0.00001183
Iteration 182/1000 | Loss: 0.00001183
Iteration 183/1000 | Loss: 0.00001183
Iteration 184/1000 | Loss: 0.00001183
Iteration 185/1000 | Loss: 0.00001183
Iteration 186/1000 | Loss: 0.00001183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.1825262845377438e-05, 1.1825262845377438e-05, 1.1825262845377438e-05, 1.1825262845377438e-05, 1.1825262845377438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1825262845377438e-05

Optimization complete. Final v2v error: 2.8139913082122803 mm

Highest mean error: 8.230294227600098 mm for frame 21

Lowest mean error: 2.151878833770752 mm for frame 42

Saving results

Total time: 130.38241267204285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846087
Iteration 2/25 | Loss: 0.00125329
Iteration 3/25 | Loss: 0.00100411
Iteration 4/25 | Loss: 0.00097549
Iteration 5/25 | Loss: 0.00096150
Iteration 6/25 | Loss: 0.00095757
Iteration 7/25 | Loss: 0.00094907
Iteration 8/25 | Loss: 0.00094803
Iteration 9/25 | Loss: 0.00094787
Iteration 10/25 | Loss: 0.00094782
Iteration 11/25 | Loss: 0.00094782
Iteration 12/25 | Loss: 0.00094781
Iteration 13/25 | Loss: 0.00094781
Iteration 14/25 | Loss: 0.00094781
Iteration 15/25 | Loss: 0.00094781
Iteration 16/25 | Loss: 0.00094781
Iteration 17/25 | Loss: 0.00094781
Iteration 18/25 | Loss: 0.00094781
Iteration 19/25 | Loss: 0.00094781
Iteration 20/25 | Loss: 0.00094781
Iteration 21/25 | Loss: 0.00094781
Iteration 22/25 | Loss: 0.00094780
Iteration 23/25 | Loss: 0.00094780
Iteration 24/25 | Loss: 0.00094780
Iteration 25/25 | Loss: 0.00094780

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.46202946
Iteration 2/25 | Loss: 0.00096198
Iteration 3/25 | Loss: 0.00096198
Iteration 4/25 | Loss: 0.00096198
Iteration 5/25 | Loss: 0.00096198
Iteration 6/25 | Loss: 0.00096198
Iteration 7/25 | Loss: 0.00096198
Iteration 8/25 | Loss: 0.00096198
Iteration 9/25 | Loss: 0.00096198
Iteration 10/25 | Loss: 0.00096197
Iteration 11/25 | Loss: 0.00096197
Iteration 12/25 | Loss: 0.00096197
Iteration 13/25 | Loss: 0.00096197
Iteration 14/25 | Loss: 0.00096197
Iteration 15/25 | Loss: 0.00096197
Iteration 16/25 | Loss: 0.00096197
Iteration 17/25 | Loss: 0.00096197
Iteration 18/25 | Loss: 0.00096197
Iteration 19/25 | Loss: 0.00096197
Iteration 20/25 | Loss: 0.00096197
Iteration 21/25 | Loss: 0.00096197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009619742049835622, 0.0009619742049835622, 0.0009619742049835622, 0.0009619742049835622, 0.0009619742049835622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009619742049835622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096197
Iteration 2/1000 | Loss: 0.00002765
Iteration 3/1000 | Loss: 0.00001603
Iteration 4/1000 | Loss: 0.00001354
Iteration 5/1000 | Loss: 0.00001285
Iteration 6/1000 | Loss: 0.00001217
Iteration 7/1000 | Loss: 0.00001182
Iteration 8/1000 | Loss: 0.00001168
Iteration 9/1000 | Loss: 0.00001150
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001144
Iteration 12/1000 | Loss: 0.00001144
Iteration 13/1000 | Loss: 0.00001143
Iteration 14/1000 | Loss: 0.00001143
Iteration 15/1000 | Loss: 0.00001143
Iteration 16/1000 | Loss: 0.00001142
Iteration 17/1000 | Loss: 0.00001142
Iteration 18/1000 | Loss: 0.00001141
Iteration 19/1000 | Loss: 0.00001139
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001139
Iteration 22/1000 | Loss: 0.00001138
Iteration 23/1000 | Loss: 0.00001138
Iteration 24/1000 | Loss: 0.00001138
Iteration 25/1000 | Loss: 0.00001137
Iteration 26/1000 | Loss: 0.00001137
Iteration 27/1000 | Loss: 0.00001136
Iteration 28/1000 | Loss: 0.00001134
Iteration 29/1000 | Loss: 0.00001134
Iteration 30/1000 | Loss: 0.00001134
Iteration 31/1000 | Loss: 0.00001133
Iteration 32/1000 | Loss: 0.00001133
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001133
Iteration 35/1000 | Loss: 0.00001133
Iteration 36/1000 | Loss: 0.00001133
Iteration 37/1000 | Loss: 0.00001133
Iteration 38/1000 | Loss: 0.00001133
Iteration 39/1000 | Loss: 0.00001133
Iteration 40/1000 | Loss: 0.00001133
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001131
Iteration 43/1000 | Loss: 0.00001130
Iteration 44/1000 | Loss: 0.00001130
Iteration 45/1000 | Loss: 0.00001129
Iteration 46/1000 | Loss: 0.00001129
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001128
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001127
Iteration 51/1000 | Loss: 0.00001127
Iteration 52/1000 | Loss: 0.00001127
Iteration 53/1000 | Loss: 0.00001126
Iteration 54/1000 | Loss: 0.00001126
Iteration 55/1000 | Loss: 0.00001126
Iteration 56/1000 | Loss: 0.00001125
Iteration 57/1000 | Loss: 0.00001125
Iteration 58/1000 | Loss: 0.00001124
Iteration 59/1000 | Loss: 0.00001124
Iteration 60/1000 | Loss: 0.00001123
Iteration 61/1000 | Loss: 0.00001123
Iteration 62/1000 | Loss: 0.00001123
Iteration 63/1000 | Loss: 0.00001123
Iteration 64/1000 | Loss: 0.00001123
Iteration 65/1000 | Loss: 0.00001122
Iteration 66/1000 | Loss: 0.00001122
Iteration 67/1000 | Loss: 0.00001122
Iteration 68/1000 | Loss: 0.00001122
Iteration 69/1000 | Loss: 0.00001122
Iteration 70/1000 | Loss: 0.00001122
Iteration 71/1000 | Loss: 0.00001121
Iteration 72/1000 | Loss: 0.00001121
Iteration 73/1000 | Loss: 0.00001121
Iteration 74/1000 | Loss: 0.00001121
Iteration 75/1000 | Loss: 0.00001121
Iteration 76/1000 | Loss: 0.00001121
Iteration 77/1000 | Loss: 0.00001120
Iteration 78/1000 | Loss: 0.00001120
Iteration 79/1000 | Loss: 0.00001120
Iteration 80/1000 | Loss: 0.00001120
Iteration 81/1000 | Loss: 0.00001120
Iteration 82/1000 | Loss: 0.00001120
Iteration 83/1000 | Loss: 0.00001120
Iteration 84/1000 | Loss: 0.00001120
Iteration 85/1000 | Loss: 0.00001120
Iteration 86/1000 | Loss: 0.00001119
Iteration 87/1000 | Loss: 0.00001119
Iteration 88/1000 | Loss: 0.00001119
Iteration 89/1000 | Loss: 0.00001119
Iteration 90/1000 | Loss: 0.00001119
Iteration 91/1000 | Loss: 0.00001119
Iteration 92/1000 | Loss: 0.00001119
Iteration 93/1000 | Loss: 0.00001119
Iteration 94/1000 | Loss: 0.00001118
Iteration 95/1000 | Loss: 0.00001118
Iteration 96/1000 | Loss: 0.00001118
Iteration 97/1000 | Loss: 0.00001118
Iteration 98/1000 | Loss: 0.00001118
Iteration 99/1000 | Loss: 0.00001118
Iteration 100/1000 | Loss: 0.00001118
Iteration 101/1000 | Loss: 0.00001118
Iteration 102/1000 | Loss: 0.00001118
Iteration 103/1000 | Loss: 0.00001118
Iteration 104/1000 | Loss: 0.00001117
Iteration 105/1000 | Loss: 0.00001117
Iteration 106/1000 | Loss: 0.00001117
Iteration 107/1000 | Loss: 0.00001117
Iteration 108/1000 | Loss: 0.00001116
Iteration 109/1000 | Loss: 0.00001116
Iteration 110/1000 | Loss: 0.00001116
Iteration 111/1000 | Loss: 0.00001116
Iteration 112/1000 | Loss: 0.00001115
Iteration 113/1000 | Loss: 0.00001115
Iteration 114/1000 | Loss: 0.00001115
Iteration 115/1000 | Loss: 0.00001115
Iteration 116/1000 | Loss: 0.00001115
Iteration 117/1000 | Loss: 0.00001115
Iteration 118/1000 | Loss: 0.00001115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.115491431846749e-05, 1.115491431846749e-05, 1.115491431846749e-05, 1.115491431846749e-05, 1.115491431846749e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.115491431846749e-05

Optimization complete. Final v2v error: 2.833940029144287 mm

Highest mean error: 3.155811071395874 mm for frame 20

Lowest mean error: 2.472437858581543 mm for frame 72

Saving results

Total time: 36.09648084640503
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926973
Iteration 2/25 | Loss: 0.00275157
Iteration 3/25 | Loss: 0.00194766
Iteration 4/25 | Loss: 0.00187514
Iteration 5/25 | Loss: 0.00162538
Iteration 6/25 | Loss: 0.00147895
Iteration 7/25 | Loss: 0.00141160
Iteration 8/25 | Loss: 0.00137115
Iteration 9/25 | Loss: 0.00137160
Iteration 10/25 | Loss: 0.00130706
Iteration 11/25 | Loss: 0.00128108
Iteration 12/25 | Loss: 0.00128466
Iteration 13/25 | Loss: 0.00127679
Iteration 14/25 | Loss: 0.00127523
Iteration 15/25 | Loss: 0.00126449
Iteration 16/25 | Loss: 0.00126214
Iteration 17/25 | Loss: 0.00126120
Iteration 18/25 | Loss: 0.00126080
Iteration 19/25 | Loss: 0.00126041
Iteration 20/25 | Loss: 0.00126005
Iteration 21/25 | Loss: 0.00126246
Iteration 22/25 | Loss: 0.00126497
Iteration 23/25 | Loss: 0.00125956
Iteration 24/25 | Loss: 0.00125458
Iteration 25/25 | Loss: 0.00125318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.52643490
Iteration 2/25 | Loss: 0.00333505
Iteration 3/25 | Loss: 0.00333504
Iteration 4/25 | Loss: 0.00296058
Iteration 5/25 | Loss: 0.00296056
Iteration 6/25 | Loss: 0.00296056
Iteration 7/25 | Loss: 0.00296056
Iteration 8/25 | Loss: 0.00296056
Iteration 9/25 | Loss: 0.00296056
Iteration 10/25 | Loss: 0.00296056
Iteration 11/25 | Loss: 0.00296056
Iteration 12/25 | Loss: 0.00296056
Iteration 13/25 | Loss: 0.00296056
Iteration 14/25 | Loss: 0.00296056
Iteration 15/25 | Loss: 0.00296056
Iteration 16/25 | Loss: 0.00296056
Iteration 17/25 | Loss: 0.00296056
Iteration 18/25 | Loss: 0.00296056
Iteration 19/25 | Loss: 0.00296056
Iteration 20/25 | Loss: 0.00296056
Iteration 21/25 | Loss: 0.00296056
Iteration 22/25 | Loss: 0.00296056
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002960559679195285, 0.002960559679195285, 0.002960559679195285, 0.002960559679195285, 0.002960559679195285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002960559679195285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00296056
Iteration 2/1000 | Loss: 0.00153794
Iteration 3/1000 | Loss: 0.00037589
Iteration 4/1000 | Loss: 0.00333399
Iteration 5/1000 | Loss: 0.00114942
Iteration 6/1000 | Loss: 0.00274242
Iteration 7/1000 | Loss: 0.00176557
Iteration 8/1000 | Loss: 0.00043356
Iteration 9/1000 | Loss: 0.00303736
Iteration 10/1000 | Loss: 0.00058382
Iteration 11/1000 | Loss: 0.00038162
Iteration 12/1000 | Loss: 0.00084721
Iteration 13/1000 | Loss: 0.00053191
Iteration 14/1000 | Loss: 0.00226457
Iteration 15/1000 | Loss: 0.00088919
Iteration 16/1000 | Loss: 0.00186650
Iteration 17/1000 | Loss: 0.00093051
Iteration 18/1000 | Loss: 0.00019910
Iteration 19/1000 | Loss: 0.00010942
Iteration 20/1000 | Loss: 0.00012486
Iteration 21/1000 | Loss: 0.00006538
Iteration 22/1000 | Loss: 0.00006517
Iteration 23/1000 | Loss: 0.00016144
Iteration 24/1000 | Loss: 0.00043626
Iteration 25/1000 | Loss: 0.00039225
Iteration 26/1000 | Loss: 0.00021005
Iteration 27/1000 | Loss: 0.00024030
Iteration 28/1000 | Loss: 0.00014831
Iteration 29/1000 | Loss: 0.00018114
Iteration 30/1000 | Loss: 0.00004918
Iteration 31/1000 | Loss: 0.00073123
Iteration 32/1000 | Loss: 0.00063683
Iteration 33/1000 | Loss: 0.00028602
Iteration 34/1000 | Loss: 0.00027476
Iteration 35/1000 | Loss: 0.00031859
Iteration 36/1000 | Loss: 0.00006787
Iteration 37/1000 | Loss: 0.00055128
Iteration 38/1000 | Loss: 0.00023255
Iteration 39/1000 | Loss: 0.00013443
Iteration 40/1000 | Loss: 0.00031747
Iteration 41/1000 | Loss: 0.00016302
Iteration 42/1000 | Loss: 0.00011586
Iteration 43/1000 | Loss: 0.00012330
Iteration 44/1000 | Loss: 0.00008338
Iteration 45/1000 | Loss: 0.00011959
Iteration 46/1000 | Loss: 0.00007625
Iteration 47/1000 | Loss: 0.00012248
Iteration 48/1000 | Loss: 0.00007683
Iteration 49/1000 | Loss: 0.00003208
Iteration 50/1000 | Loss: 0.00011495
Iteration 51/1000 | Loss: 0.00007558
Iteration 52/1000 | Loss: 0.00011254
Iteration 53/1000 | Loss: 0.00003459
Iteration 54/1000 | Loss: 0.00018924
Iteration 55/1000 | Loss: 0.00043888
Iteration 56/1000 | Loss: 0.00034403
Iteration 57/1000 | Loss: 0.00040241
Iteration 58/1000 | Loss: 0.00049690
Iteration 59/1000 | Loss: 0.00046186
Iteration 60/1000 | Loss: 0.00045225
Iteration 61/1000 | Loss: 0.00048465
Iteration 62/1000 | Loss: 0.00048817
Iteration 63/1000 | Loss: 0.00004329
Iteration 64/1000 | Loss: 0.00012380
Iteration 65/1000 | Loss: 0.00003205
Iteration 66/1000 | Loss: 0.00028125
Iteration 67/1000 | Loss: 0.00036746
Iteration 68/1000 | Loss: 0.00216703
Iteration 69/1000 | Loss: 0.00098925
Iteration 70/1000 | Loss: 0.00062872
Iteration 71/1000 | Loss: 0.00022924
Iteration 72/1000 | Loss: 0.00003793
Iteration 73/1000 | Loss: 0.00055205
Iteration 74/1000 | Loss: 0.00032323
Iteration 75/1000 | Loss: 0.00002942
Iteration 76/1000 | Loss: 0.00002660
Iteration 77/1000 | Loss: 0.00002519
Iteration 78/1000 | Loss: 0.00028076
Iteration 79/1000 | Loss: 0.00007865
Iteration 80/1000 | Loss: 0.00002405
Iteration 81/1000 | Loss: 0.00014377
Iteration 82/1000 | Loss: 0.00007248
Iteration 83/1000 | Loss: 0.00008720
Iteration 84/1000 | Loss: 0.00059321
Iteration 85/1000 | Loss: 0.00003148
Iteration 86/1000 | Loss: 0.00037684
Iteration 87/1000 | Loss: 0.00002832
Iteration 88/1000 | Loss: 0.00002528
Iteration 89/1000 | Loss: 0.00002380
Iteration 90/1000 | Loss: 0.00002288
Iteration 91/1000 | Loss: 0.00048914
Iteration 92/1000 | Loss: 0.00004917
Iteration 93/1000 | Loss: 0.00003465
Iteration 94/1000 | Loss: 0.00002442
Iteration 95/1000 | Loss: 0.00002320
Iteration 96/1000 | Loss: 0.00002253
Iteration 97/1000 | Loss: 0.00002216
Iteration 98/1000 | Loss: 0.00002164
Iteration 99/1000 | Loss: 0.00002093
Iteration 100/1000 | Loss: 0.00039041
Iteration 101/1000 | Loss: 0.00026435
Iteration 102/1000 | Loss: 0.00009331
Iteration 103/1000 | Loss: 0.00036169
Iteration 104/1000 | Loss: 0.00242464
Iteration 105/1000 | Loss: 0.00074349
Iteration 106/1000 | Loss: 0.00010759
Iteration 107/1000 | Loss: 0.00081343
Iteration 108/1000 | Loss: 0.00040995
Iteration 109/1000 | Loss: 0.00042522
Iteration 110/1000 | Loss: 0.00003075
Iteration 111/1000 | Loss: 0.00002499
Iteration 112/1000 | Loss: 0.00002198
Iteration 113/1000 | Loss: 0.00002081
Iteration 114/1000 | Loss: 0.00001975
Iteration 115/1000 | Loss: 0.00001907
Iteration 116/1000 | Loss: 0.00020177
Iteration 117/1000 | Loss: 0.00008053
Iteration 118/1000 | Loss: 0.00001882
Iteration 119/1000 | Loss: 0.00001720
Iteration 120/1000 | Loss: 0.00001654
Iteration 121/1000 | Loss: 0.00001614
Iteration 122/1000 | Loss: 0.00001605
Iteration 123/1000 | Loss: 0.00001582
Iteration 124/1000 | Loss: 0.00001580
Iteration 125/1000 | Loss: 0.00001573
Iteration 126/1000 | Loss: 0.00001572
Iteration 127/1000 | Loss: 0.00001567
Iteration 128/1000 | Loss: 0.00001560
Iteration 129/1000 | Loss: 0.00001557
Iteration 130/1000 | Loss: 0.00001557
Iteration 131/1000 | Loss: 0.00001556
Iteration 132/1000 | Loss: 0.00001556
Iteration 133/1000 | Loss: 0.00001555
Iteration 134/1000 | Loss: 0.00001554
Iteration 135/1000 | Loss: 0.00001554
Iteration 136/1000 | Loss: 0.00001553
Iteration 137/1000 | Loss: 0.00001553
Iteration 138/1000 | Loss: 0.00001553
Iteration 139/1000 | Loss: 0.00001552
Iteration 140/1000 | Loss: 0.00001552
Iteration 141/1000 | Loss: 0.00001551
Iteration 142/1000 | Loss: 0.00001551
Iteration 143/1000 | Loss: 0.00001551
Iteration 144/1000 | Loss: 0.00001550
Iteration 145/1000 | Loss: 0.00001550
Iteration 146/1000 | Loss: 0.00001550
Iteration 147/1000 | Loss: 0.00001549
Iteration 148/1000 | Loss: 0.00001549
Iteration 149/1000 | Loss: 0.00001549
Iteration 150/1000 | Loss: 0.00001548
Iteration 151/1000 | Loss: 0.00001548
Iteration 152/1000 | Loss: 0.00001548
Iteration 153/1000 | Loss: 0.00001547
Iteration 154/1000 | Loss: 0.00001547
Iteration 155/1000 | Loss: 0.00001547
Iteration 156/1000 | Loss: 0.00001547
Iteration 157/1000 | Loss: 0.00001547
Iteration 158/1000 | Loss: 0.00001547
Iteration 159/1000 | Loss: 0.00001547
Iteration 160/1000 | Loss: 0.00001547
Iteration 161/1000 | Loss: 0.00001547
Iteration 162/1000 | Loss: 0.00001546
Iteration 163/1000 | Loss: 0.00001546
Iteration 164/1000 | Loss: 0.00001546
Iteration 165/1000 | Loss: 0.00001545
Iteration 166/1000 | Loss: 0.00001545
Iteration 167/1000 | Loss: 0.00001545
Iteration 168/1000 | Loss: 0.00001544
Iteration 169/1000 | Loss: 0.00001544
Iteration 170/1000 | Loss: 0.00001544
Iteration 171/1000 | Loss: 0.00001544
Iteration 172/1000 | Loss: 0.00001544
Iteration 173/1000 | Loss: 0.00001543
Iteration 174/1000 | Loss: 0.00001543
Iteration 175/1000 | Loss: 0.00001543
Iteration 176/1000 | Loss: 0.00001543
Iteration 177/1000 | Loss: 0.00001543
Iteration 178/1000 | Loss: 0.00001543
Iteration 179/1000 | Loss: 0.00001543
Iteration 180/1000 | Loss: 0.00001543
Iteration 181/1000 | Loss: 0.00001543
Iteration 182/1000 | Loss: 0.00001543
Iteration 183/1000 | Loss: 0.00001543
Iteration 184/1000 | Loss: 0.00001543
Iteration 185/1000 | Loss: 0.00001543
Iteration 186/1000 | Loss: 0.00001543
Iteration 187/1000 | Loss: 0.00001543
Iteration 188/1000 | Loss: 0.00001543
Iteration 189/1000 | Loss: 0.00001543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.5433033695444465e-05, 1.5433033695444465e-05, 1.5433033695444465e-05, 1.5433033695444465e-05, 1.5433033695444465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5433033695444465e-05

Optimization complete. Final v2v error: 3.2863564491271973 mm

Highest mean error: 8.54227066040039 mm for frame 120

Lowest mean error: 2.6109626293182373 mm for frame 14

Saving results

Total time: 232.886643409729
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072478
Iteration 2/25 | Loss: 0.00141974
Iteration 3/25 | Loss: 0.00112448
Iteration 4/25 | Loss: 0.00118432
Iteration 5/25 | Loss: 0.00105670
Iteration 6/25 | Loss: 0.00103945
Iteration 7/25 | Loss: 0.00103288
Iteration 8/25 | Loss: 0.00101468
Iteration 9/25 | Loss: 0.00099045
Iteration 10/25 | Loss: 0.00097281
Iteration 11/25 | Loss: 0.00097934
Iteration 12/25 | Loss: 0.00097702
Iteration 13/25 | Loss: 0.00097421
Iteration 14/25 | Loss: 0.00097053
Iteration 15/25 | Loss: 0.00096233
Iteration 16/25 | Loss: 0.00095773
Iteration 17/25 | Loss: 0.00095559
Iteration 18/25 | Loss: 0.00095507
Iteration 19/25 | Loss: 0.00095431
Iteration 20/25 | Loss: 0.00095493
Iteration 21/25 | Loss: 0.00095243
Iteration 22/25 | Loss: 0.00095415
Iteration 23/25 | Loss: 0.00095227
Iteration 24/25 | Loss: 0.00095303
Iteration 25/25 | Loss: 0.00095116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42560410
Iteration 2/25 | Loss: 0.00088231
Iteration 3/25 | Loss: 0.00088231
Iteration 4/25 | Loss: 0.00088231
Iteration 5/25 | Loss: 0.00088231
Iteration 6/25 | Loss: 0.00088231
Iteration 7/25 | Loss: 0.00088231
Iteration 8/25 | Loss: 0.00088231
Iteration 9/25 | Loss: 0.00088231
Iteration 10/25 | Loss: 0.00088231
Iteration 11/25 | Loss: 0.00088231
Iteration 12/25 | Loss: 0.00088231
Iteration 13/25 | Loss: 0.00088231
Iteration 14/25 | Loss: 0.00088231
Iteration 15/25 | Loss: 0.00088231
Iteration 16/25 | Loss: 0.00088231
Iteration 17/25 | Loss: 0.00088231
Iteration 18/25 | Loss: 0.00088231
Iteration 19/25 | Loss: 0.00088231
Iteration 20/25 | Loss: 0.00088231
Iteration 21/25 | Loss: 0.00088231
Iteration 22/25 | Loss: 0.00088231
Iteration 23/25 | Loss: 0.00088231
Iteration 24/25 | Loss: 0.00088231
Iteration 25/25 | Loss: 0.00088231

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088231
Iteration 2/1000 | Loss: 0.00003878
Iteration 3/1000 | Loss: 0.00004537
Iteration 4/1000 | Loss: 0.00002970
Iteration 5/1000 | Loss: 0.00003753
Iteration 6/1000 | Loss: 0.00003610
Iteration 7/1000 | Loss: 0.00003399
Iteration 8/1000 | Loss: 0.00002169
Iteration 9/1000 | Loss: 0.00002085
Iteration 10/1000 | Loss: 0.00003241
Iteration 11/1000 | Loss: 0.00002934
Iteration 12/1000 | Loss: 0.00002965
Iteration 13/1000 | Loss: 0.00003325
Iteration 14/1000 | Loss: 0.00002920
Iteration 15/1000 | Loss: 0.00003194
Iteration 16/1000 | Loss: 0.00003332
Iteration 17/1000 | Loss: 0.00003022
Iteration 18/1000 | Loss: 0.00003409
Iteration 19/1000 | Loss: 0.00003097
Iteration 20/1000 | Loss: 0.00003516
Iteration 21/1000 | Loss: 0.00003273
Iteration 22/1000 | Loss: 0.00003415
Iteration 23/1000 | Loss: 0.00003208
Iteration 24/1000 | Loss: 0.00003423
Iteration 25/1000 | Loss: 0.00003143
Iteration 26/1000 | Loss: 0.00003393
Iteration 27/1000 | Loss: 0.00003569
Iteration 28/1000 | Loss: 0.00004218
Iteration 29/1000 | Loss: 0.00003693
Iteration 30/1000 | Loss: 0.00003021
Iteration 31/1000 | Loss: 0.00003102
Iteration 32/1000 | Loss: 0.00003104
Iteration 33/1000 | Loss: 0.00003282
Iteration 34/1000 | Loss: 0.00003052
Iteration 35/1000 | Loss: 0.00003487
Iteration 36/1000 | Loss: 0.00003264
Iteration 37/1000 | Loss: 0.00003094
Iteration 38/1000 | Loss: 0.00003184
Iteration 39/1000 | Loss: 0.00002514
Iteration 40/1000 | Loss: 0.00003225
Iteration 41/1000 | Loss: 0.00003025
Iteration 42/1000 | Loss: 0.00003118
Iteration 43/1000 | Loss: 0.00004388
Iteration 44/1000 | Loss: 0.00003372
Iteration 45/1000 | Loss: 0.00002988
Iteration 46/1000 | Loss: 0.00003214
Iteration 47/1000 | Loss: 0.00002697
Iteration 48/1000 | Loss: 0.00003155
Iteration 49/1000 | Loss: 0.00003534
Iteration 50/1000 | Loss: 0.00003825
Iteration 51/1000 | Loss: 0.00003407
Iteration 52/1000 | Loss: 0.00003013
Iteration 53/1000 | Loss: 0.00002865
Iteration 54/1000 | Loss: 0.00003878
Iteration 55/1000 | Loss: 0.00002983
Iteration 56/1000 | Loss: 0.00003026
Iteration 57/1000 | Loss: 0.00003163
Iteration 58/1000 | Loss: 0.00003342
Iteration 59/1000 | Loss: 0.00003123
Iteration 60/1000 | Loss: 0.00003271
Iteration 61/1000 | Loss: 0.00003336
Iteration 62/1000 | Loss: 0.00003217
Iteration 63/1000 | Loss: 0.00003313
Iteration 64/1000 | Loss: 0.00002106
Iteration 65/1000 | Loss: 0.00002785
Iteration 66/1000 | Loss: 0.00003309
Iteration 67/1000 | Loss: 0.00002744
Iteration 68/1000 | Loss: 0.00003203
Iteration 69/1000 | Loss: 0.00003375
Iteration 70/1000 | Loss: 0.00003230
Iteration 71/1000 | Loss: 0.00003353
Iteration 72/1000 | Loss: 0.00003260
Iteration 73/1000 | Loss: 0.00001791
Iteration 74/1000 | Loss: 0.00003512
Iteration 75/1000 | Loss: 0.00003360
Iteration 76/1000 | Loss: 0.00003144
Iteration 77/1000 | Loss: 0.00003350
Iteration 78/1000 | Loss: 0.00003164
Iteration 79/1000 | Loss: 0.00001406
Iteration 80/1000 | Loss: 0.00003289
Iteration 81/1000 | Loss: 0.00003941
Iteration 82/1000 | Loss: 0.00002196
Iteration 83/1000 | Loss: 0.00001759
Iteration 84/1000 | Loss: 0.00001398
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001155
Iteration 87/1000 | Loss: 0.00001086
Iteration 88/1000 | Loss: 0.00001057
Iteration 89/1000 | Loss: 0.00001041
Iteration 90/1000 | Loss: 0.00001040
Iteration 91/1000 | Loss: 0.00001038
Iteration 92/1000 | Loss: 0.00001037
Iteration 93/1000 | Loss: 0.00001034
Iteration 94/1000 | Loss: 0.00001033
Iteration 95/1000 | Loss: 0.00001033
Iteration 96/1000 | Loss: 0.00001032
Iteration 97/1000 | Loss: 0.00001032
Iteration 98/1000 | Loss: 0.00001030
Iteration 99/1000 | Loss: 0.00001026
Iteration 100/1000 | Loss: 0.00001024
Iteration 101/1000 | Loss: 0.00001021
Iteration 102/1000 | Loss: 0.00001017
Iteration 103/1000 | Loss: 0.00001016
Iteration 104/1000 | Loss: 0.00001015
Iteration 105/1000 | Loss: 0.00001015
Iteration 106/1000 | Loss: 0.00001014
Iteration 107/1000 | Loss: 0.00001013
Iteration 108/1000 | Loss: 0.00001012
Iteration 109/1000 | Loss: 0.00001011
Iteration 110/1000 | Loss: 0.00001010
Iteration 111/1000 | Loss: 0.00001010
Iteration 112/1000 | Loss: 0.00001010
Iteration 113/1000 | Loss: 0.00001010
Iteration 114/1000 | Loss: 0.00001010
Iteration 115/1000 | Loss: 0.00001009
Iteration 116/1000 | Loss: 0.00001009
Iteration 117/1000 | Loss: 0.00001008
Iteration 118/1000 | Loss: 0.00001007
Iteration 119/1000 | Loss: 0.00001006
Iteration 120/1000 | Loss: 0.00001006
Iteration 121/1000 | Loss: 0.00001005
Iteration 122/1000 | Loss: 0.00001005
Iteration 123/1000 | Loss: 0.00001005
Iteration 124/1000 | Loss: 0.00001004
Iteration 125/1000 | Loss: 0.00001004
Iteration 126/1000 | Loss: 0.00001004
Iteration 127/1000 | Loss: 0.00001003
Iteration 128/1000 | Loss: 0.00001003
Iteration 129/1000 | Loss: 0.00001003
Iteration 130/1000 | Loss: 0.00001003
Iteration 131/1000 | Loss: 0.00001002
Iteration 132/1000 | Loss: 0.00001002
Iteration 133/1000 | Loss: 0.00001002
Iteration 134/1000 | Loss: 0.00001002
Iteration 135/1000 | Loss: 0.00001001
Iteration 136/1000 | Loss: 0.00001001
Iteration 137/1000 | Loss: 0.00001001
Iteration 138/1000 | Loss: 0.00001001
Iteration 139/1000 | Loss: 0.00001001
Iteration 140/1000 | Loss: 0.00001001
Iteration 141/1000 | Loss: 0.00001001
Iteration 142/1000 | Loss: 0.00001001
Iteration 143/1000 | Loss: 0.00001001
Iteration 144/1000 | Loss: 0.00001001
Iteration 145/1000 | Loss: 0.00001001
Iteration 146/1000 | Loss: 0.00001001
Iteration 147/1000 | Loss: 0.00001001
Iteration 148/1000 | Loss: 0.00001001
Iteration 149/1000 | Loss: 0.00001000
Iteration 150/1000 | Loss: 0.00001000
Iteration 151/1000 | Loss: 0.00001000
Iteration 152/1000 | Loss: 0.00001000
Iteration 153/1000 | Loss: 0.00001000
Iteration 154/1000 | Loss: 0.00001000
Iteration 155/1000 | Loss: 0.00001000
Iteration 156/1000 | Loss: 0.00001000
Iteration 157/1000 | Loss: 0.00000999
Iteration 158/1000 | Loss: 0.00000999
Iteration 159/1000 | Loss: 0.00000999
Iteration 160/1000 | Loss: 0.00000998
Iteration 161/1000 | Loss: 0.00000998
Iteration 162/1000 | Loss: 0.00000998
Iteration 163/1000 | Loss: 0.00000998
Iteration 164/1000 | Loss: 0.00000998
Iteration 165/1000 | Loss: 0.00000998
Iteration 166/1000 | Loss: 0.00000998
Iteration 167/1000 | Loss: 0.00000998
Iteration 168/1000 | Loss: 0.00000998
Iteration 169/1000 | Loss: 0.00000998
Iteration 170/1000 | Loss: 0.00000998
Iteration 171/1000 | Loss: 0.00000998
Iteration 172/1000 | Loss: 0.00000998
Iteration 173/1000 | Loss: 0.00000997
Iteration 174/1000 | Loss: 0.00000997
Iteration 175/1000 | Loss: 0.00000997
Iteration 176/1000 | Loss: 0.00000997
Iteration 177/1000 | Loss: 0.00000997
Iteration 178/1000 | Loss: 0.00000997
Iteration 179/1000 | Loss: 0.00000997
Iteration 180/1000 | Loss: 0.00000997
Iteration 181/1000 | Loss: 0.00000997
Iteration 182/1000 | Loss: 0.00000997
Iteration 183/1000 | Loss: 0.00000997
Iteration 184/1000 | Loss: 0.00000996
Iteration 185/1000 | Loss: 0.00000996
Iteration 186/1000 | Loss: 0.00000996
Iteration 187/1000 | Loss: 0.00000996
Iteration 188/1000 | Loss: 0.00000996
Iteration 189/1000 | Loss: 0.00000996
Iteration 190/1000 | Loss: 0.00000996
Iteration 191/1000 | Loss: 0.00000996
Iteration 192/1000 | Loss: 0.00000996
Iteration 193/1000 | Loss: 0.00000996
Iteration 194/1000 | Loss: 0.00000996
Iteration 195/1000 | Loss: 0.00000996
Iteration 196/1000 | Loss: 0.00000996
Iteration 197/1000 | Loss: 0.00000996
Iteration 198/1000 | Loss: 0.00000996
Iteration 199/1000 | Loss: 0.00000996
Iteration 200/1000 | Loss: 0.00000996
Iteration 201/1000 | Loss: 0.00000996
Iteration 202/1000 | Loss: 0.00000996
Iteration 203/1000 | Loss: 0.00000995
Iteration 204/1000 | Loss: 0.00000995
Iteration 205/1000 | Loss: 0.00000995
Iteration 206/1000 | Loss: 0.00000995
Iteration 207/1000 | Loss: 0.00000995
Iteration 208/1000 | Loss: 0.00000995
Iteration 209/1000 | Loss: 0.00000995
Iteration 210/1000 | Loss: 0.00000995
Iteration 211/1000 | Loss: 0.00000995
Iteration 212/1000 | Loss: 0.00000995
Iteration 213/1000 | Loss: 0.00000995
Iteration 214/1000 | Loss: 0.00000995
Iteration 215/1000 | Loss: 0.00000995
Iteration 216/1000 | Loss: 0.00000995
Iteration 217/1000 | Loss: 0.00000995
Iteration 218/1000 | Loss: 0.00000995
Iteration 219/1000 | Loss: 0.00000995
Iteration 220/1000 | Loss: 0.00000995
Iteration 221/1000 | Loss: 0.00000995
Iteration 222/1000 | Loss: 0.00000995
Iteration 223/1000 | Loss: 0.00000995
Iteration 224/1000 | Loss: 0.00000995
Iteration 225/1000 | Loss: 0.00000995
Iteration 226/1000 | Loss: 0.00000995
Iteration 227/1000 | Loss: 0.00000995
Iteration 228/1000 | Loss: 0.00000995
Iteration 229/1000 | Loss: 0.00000995
Iteration 230/1000 | Loss: 0.00000995
Iteration 231/1000 | Loss: 0.00000995
Iteration 232/1000 | Loss: 0.00000995
Iteration 233/1000 | Loss: 0.00000995
Iteration 234/1000 | Loss: 0.00000995
Iteration 235/1000 | Loss: 0.00000995
Iteration 236/1000 | Loss: 0.00000995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [9.948244951374363e-06, 9.948244951374363e-06, 9.948244951374363e-06, 9.948244951374363e-06, 9.948244951374363e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.948244951374363e-06

Optimization complete. Final v2v error: 2.6358466148376465 mm

Highest mean error: 3.4658854007720947 mm for frame 65

Lowest mean error: 2.2115774154663086 mm for frame 18

Saving results

Total time: 170.15452551841736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00744228
Iteration 2/25 | Loss: 0.00119907
Iteration 3/25 | Loss: 0.00106548
Iteration 4/25 | Loss: 0.00105695
Iteration 5/25 | Loss: 0.00105478
Iteration 6/25 | Loss: 0.00105438
Iteration 7/25 | Loss: 0.00105438
Iteration 8/25 | Loss: 0.00105438
Iteration 9/25 | Loss: 0.00105438
Iteration 10/25 | Loss: 0.00105438
Iteration 11/25 | Loss: 0.00105438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001054384745657444, 0.001054384745657444, 0.001054384745657444, 0.001054384745657444, 0.001054384745657444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001054384745657444

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26427603
Iteration 2/25 | Loss: 0.00082016
Iteration 3/25 | Loss: 0.00082015
Iteration 4/25 | Loss: 0.00082015
Iteration 5/25 | Loss: 0.00082015
Iteration 6/25 | Loss: 0.00082015
Iteration 7/25 | Loss: 0.00082015
Iteration 8/25 | Loss: 0.00082015
Iteration 9/25 | Loss: 0.00082015
Iteration 10/25 | Loss: 0.00082015
Iteration 11/25 | Loss: 0.00082015
Iteration 12/25 | Loss: 0.00082015
Iteration 13/25 | Loss: 0.00082015
Iteration 14/25 | Loss: 0.00082015
Iteration 15/25 | Loss: 0.00082015
Iteration 16/25 | Loss: 0.00082015
Iteration 17/25 | Loss: 0.00082015
Iteration 18/25 | Loss: 0.00082015
Iteration 19/25 | Loss: 0.00082015
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008201464079320431, 0.0008201464079320431, 0.0008201464079320431, 0.0008201464079320431, 0.0008201464079320431]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008201464079320431

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082015
Iteration 2/1000 | Loss: 0.00004015
Iteration 3/1000 | Loss: 0.00002639
Iteration 4/1000 | Loss: 0.00002211
Iteration 5/1000 | Loss: 0.00002082
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00001894
Iteration 8/1000 | Loss: 0.00001846
Iteration 9/1000 | Loss: 0.00001803
Iteration 10/1000 | Loss: 0.00001779
Iteration 11/1000 | Loss: 0.00001775
Iteration 12/1000 | Loss: 0.00001754
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001727
Iteration 15/1000 | Loss: 0.00001724
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001719
Iteration 18/1000 | Loss: 0.00001718
Iteration 19/1000 | Loss: 0.00001716
Iteration 20/1000 | Loss: 0.00001715
Iteration 21/1000 | Loss: 0.00001713
Iteration 22/1000 | Loss: 0.00001713
Iteration 23/1000 | Loss: 0.00001713
Iteration 24/1000 | Loss: 0.00001713
Iteration 25/1000 | Loss: 0.00001713
Iteration 26/1000 | Loss: 0.00001713
Iteration 27/1000 | Loss: 0.00001713
Iteration 28/1000 | Loss: 0.00001713
Iteration 29/1000 | Loss: 0.00001712
Iteration 30/1000 | Loss: 0.00001712
Iteration 31/1000 | Loss: 0.00001712
Iteration 32/1000 | Loss: 0.00001711
Iteration 33/1000 | Loss: 0.00001711
Iteration 34/1000 | Loss: 0.00001710
Iteration 35/1000 | Loss: 0.00001709
Iteration 36/1000 | Loss: 0.00001709
Iteration 37/1000 | Loss: 0.00001708
Iteration 38/1000 | Loss: 0.00001708
Iteration 39/1000 | Loss: 0.00001708
Iteration 40/1000 | Loss: 0.00001708
Iteration 41/1000 | Loss: 0.00001708
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001708
Iteration 44/1000 | Loss: 0.00001708
Iteration 45/1000 | Loss: 0.00001707
Iteration 46/1000 | Loss: 0.00001707
Iteration 47/1000 | Loss: 0.00001706
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001705
Iteration 53/1000 | Loss: 0.00001705
Iteration 54/1000 | Loss: 0.00001705
Iteration 55/1000 | Loss: 0.00001705
Iteration 56/1000 | Loss: 0.00001705
Iteration 57/1000 | Loss: 0.00001704
Iteration 58/1000 | Loss: 0.00001704
Iteration 59/1000 | Loss: 0.00001704
Iteration 60/1000 | Loss: 0.00001704
Iteration 61/1000 | Loss: 0.00001704
Iteration 62/1000 | Loss: 0.00001704
Iteration 63/1000 | Loss: 0.00001704
Iteration 64/1000 | Loss: 0.00001704
Iteration 65/1000 | Loss: 0.00001704
Iteration 66/1000 | Loss: 0.00001704
Iteration 67/1000 | Loss: 0.00001703
Iteration 68/1000 | Loss: 0.00001703
Iteration 69/1000 | Loss: 0.00001703
Iteration 70/1000 | Loss: 0.00001703
Iteration 71/1000 | Loss: 0.00001703
Iteration 72/1000 | Loss: 0.00001703
Iteration 73/1000 | Loss: 0.00001703
Iteration 74/1000 | Loss: 0.00001703
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001703
Iteration 78/1000 | Loss: 0.00001703
Iteration 79/1000 | Loss: 0.00001703
Iteration 80/1000 | Loss: 0.00001703
Iteration 81/1000 | Loss: 0.00001702
Iteration 82/1000 | Loss: 0.00001702
Iteration 83/1000 | Loss: 0.00001702
Iteration 84/1000 | Loss: 0.00001702
Iteration 85/1000 | Loss: 0.00001702
Iteration 86/1000 | Loss: 0.00001701
Iteration 87/1000 | Loss: 0.00001701
Iteration 88/1000 | Loss: 0.00001701
Iteration 89/1000 | Loss: 0.00001701
Iteration 90/1000 | Loss: 0.00001701
Iteration 91/1000 | Loss: 0.00001701
Iteration 92/1000 | Loss: 0.00001701
Iteration 93/1000 | Loss: 0.00001700
Iteration 94/1000 | Loss: 0.00001700
Iteration 95/1000 | Loss: 0.00001700
Iteration 96/1000 | Loss: 0.00001700
Iteration 97/1000 | Loss: 0.00001700
Iteration 98/1000 | Loss: 0.00001700
Iteration 99/1000 | Loss: 0.00001700
Iteration 100/1000 | Loss: 0.00001700
Iteration 101/1000 | Loss: 0.00001700
Iteration 102/1000 | Loss: 0.00001700
Iteration 103/1000 | Loss: 0.00001699
Iteration 104/1000 | Loss: 0.00001699
Iteration 105/1000 | Loss: 0.00001699
Iteration 106/1000 | Loss: 0.00001699
Iteration 107/1000 | Loss: 0.00001699
Iteration 108/1000 | Loss: 0.00001699
Iteration 109/1000 | Loss: 0.00001699
Iteration 110/1000 | Loss: 0.00001699
Iteration 111/1000 | Loss: 0.00001698
Iteration 112/1000 | Loss: 0.00001698
Iteration 113/1000 | Loss: 0.00001698
Iteration 114/1000 | Loss: 0.00001698
Iteration 115/1000 | Loss: 0.00001697
Iteration 116/1000 | Loss: 0.00001697
Iteration 117/1000 | Loss: 0.00001697
Iteration 118/1000 | Loss: 0.00001697
Iteration 119/1000 | Loss: 0.00001697
Iteration 120/1000 | Loss: 0.00001697
Iteration 121/1000 | Loss: 0.00001697
Iteration 122/1000 | Loss: 0.00001697
Iteration 123/1000 | Loss: 0.00001696
Iteration 124/1000 | Loss: 0.00001696
Iteration 125/1000 | Loss: 0.00001696
Iteration 126/1000 | Loss: 0.00001696
Iteration 127/1000 | Loss: 0.00001696
Iteration 128/1000 | Loss: 0.00001696
Iteration 129/1000 | Loss: 0.00001696
Iteration 130/1000 | Loss: 0.00001696
Iteration 131/1000 | Loss: 0.00001696
Iteration 132/1000 | Loss: 0.00001696
Iteration 133/1000 | Loss: 0.00001696
Iteration 134/1000 | Loss: 0.00001695
Iteration 135/1000 | Loss: 0.00001695
Iteration 136/1000 | Loss: 0.00001695
Iteration 137/1000 | Loss: 0.00001695
Iteration 138/1000 | Loss: 0.00001695
Iteration 139/1000 | Loss: 0.00001695
Iteration 140/1000 | Loss: 0.00001695
Iteration 141/1000 | Loss: 0.00001695
Iteration 142/1000 | Loss: 0.00001695
Iteration 143/1000 | Loss: 0.00001695
Iteration 144/1000 | Loss: 0.00001695
Iteration 145/1000 | Loss: 0.00001695
Iteration 146/1000 | Loss: 0.00001695
Iteration 147/1000 | Loss: 0.00001695
Iteration 148/1000 | Loss: 0.00001694
Iteration 149/1000 | Loss: 0.00001694
Iteration 150/1000 | Loss: 0.00001694
Iteration 151/1000 | Loss: 0.00001694
Iteration 152/1000 | Loss: 0.00001694
Iteration 153/1000 | Loss: 0.00001694
Iteration 154/1000 | Loss: 0.00001694
Iteration 155/1000 | Loss: 0.00001694
Iteration 156/1000 | Loss: 0.00001694
Iteration 157/1000 | Loss: 0.00001694
Iteration 158/1000 | Loss: 0.00001694
Iteration 159/1000 | Loss: 0.00001694
Iteration 160/1000 | Loss: 0.00001694
Iteration 161/1000 | Loss: 0.00001694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.6944768503890373e-05, 1.6944768503890373e-05, 1.6944768503890373e-05, 1.6944768503890373e-05, 1.6944768503890373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6944768503890373e-05

Optimization complete. Final v2v error: 3.4307589530944824 mm

Highest mean error: 4.026615619659424 mm for frame 26

Lowest mean error: 2.882909059524536 mm for frame 107

Saving results

Total time: 34.71336007118225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058471
Iteration 2/25 | Loss: 0.00241034
Iteration 3/25 | Loss: 0.00163282
Iteration 4/25 | Loss: 0.00140601
Iteration 5/25 | Loss: 0.00126076
Iteration 6/25 | Loss: 0.00119608
Iteration 7/25 | Loss: 0.00116881
Iteration 8/25 | Loss: 0.00113679
Iteration 9/25 | Loss: 0.00112536
Iteration 10/25 | Loss: 0.00111147
Iteration 11/25 | Loss: 0.00110063
Iteration 12/25 | Loss: 0.00108673
Iteration 13/25 | Loss: 0.00107677
Iteration 14/25 | Loss: 0.00107334
Iteration 15/25 | Loss: 0.00107334
Iteration 16/25 | Loss: 0.00107092
Iteration 17/25 | Loss: 0.00107006
Iteration 18/25 | Loss: 0.00106990
Iteration 19/25 | Loss: 0.00106959
Iteration 20/25 | Loss: 0.00106933
Iteration 21/25 | Loss: 0.00106863
Iteration 22/25 | Loss: 0.00106862
Iteration 23/25 | Loss: 0.00106896
Iteration 24/25 | Loss: 0.00106879
Iteration 25/25 | Loss: 0.00106888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33690274
Iteration 2/25 | Loss: 0.00214242
Iteration 3/25 | Loss: 0.00181715
Iteration 4/25 | Loss: 0.00181715
Iteration 5/25 | Loss: 0.00181714
Iteration 6/25 | Loss: 0.00181714
Iteration 7/25 | Loss: 0.00181714
Iteration 8/25 | Loss: 0.00181714
Iteration 9/25 | Loss: 0.00181714
Iteration 10/25 | Loss: 0.00181714
Iteration 11/25 | Loss: 0.00181714
Iteration 12/25 | Loss: 0.00181714
Iteration 13/25 | Loss: 0.00181714
Iteration 14/25 | Loss: 0.00181714
Iteration 15/25 | Loss: 0.00181714
Iteration 16/25 | Loss: 0.00181714
Iteration 17/25 | Loss: 0.00181714
Iteration 18/25 | Loss: 0.00181714
Iteration 19/25 | Loss: 0.00181714
Iteration 20/25 | Loss: 0.00181714
Iteration 21/25 | Loss: 0.00181714
Iteration 22/25 | Loss: 0.00181714
Iteration 23/25 | Loss: 0.00181714
Iteration 24/25 | Loss: 0.00181714
Iteration 25/25 | Loss: 0.00181714
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0018171417759731412, 0.0018171417759731412, 0.0018171417759731412, 0.0018171417759731412, 0.0018171417759731412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018171417759731412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181714
Iteration 2/1000 | Loss: 0.00019242
Iteration 3/1000 | Loss: 0.00035121
Iteration 4/1000 | Loss: 0.00025304
Iteration 5/1000 | Loss: 0.00040719
Iteration 6/1000 | Loss: 0.00017666
Iteration 7/1000 | Loss: 0.00016035
Iteration 8/1000 | Loss: 0.00022045
Iteration 9/1000 | Loss: 0.00010468
Iteration 10/1000 | Loss: 0.00049942
Iteration 11/1000 | Loss: 0.00054429
Iteration 12/1000 | Loss: 0.00055855
Iteration 13/1000 | Loss: 0.00037326
Iteration 14/1000 | Loss: 0.00068220
Iteration 15/1000 | Loss: 0.00032870
Iteration 16/1000 | Loss: 0.00027194
Iteration 17/1000 | Loss: 0.00013484
Iteration 18/1000 | Loss: 0.00011053
Iteration 19/1000 | Loss: 0.00009330
Iteration 20/1000 | Loss: 0.00025044
Iteration 21/1000 | Loss: 0.00047721
Iteration 22/1000 | Loss: 0.00045718
Iteration 23/1000 | Loss: 0.00016894
Iteration 24/1000 | Loss: 0.00011123
Iteration 25/1000 | Loss: 0.00019138
Iteration 26/1000 | Loss: 0.00035946
Iteration 27/1000 | Loss: 0.00013247
Iteration 28/1000 | Loss: 0.00017615
Iteration 29/1000 | Loss: 0.00009845
Iteration 30/1000 | Loss: 0.00011855
Iteration 31/1000 | Loss: 0.00009831
Iteration 32/1000 | Loss: 0.00048376
Iteration 33/1000 | Loss: 0.00013324
Iteration 34/1000 | Loss: 0.00016653
Iteration 35/1000 | Loss: 0.00028464
Iteration 36/1000 | Loss: 0.00016053
Iteration 37/1000 | Loss: 0.00010858
Iteration 38/1000 | Loss: 0.00009505
Iteration 39/1000 | Loss: 0.00013819
Iteration 40/1000 | Loss: 0.00009560
Iteration 41/1000 | Loss: 0.00009512
Iteration 42/1000 | Loss: 0.00011414
Iteration 43/1000 | Loss: 0.00009805
Iteration 44/1000 | Loss: 0.00058930
Iteration 45/1000 | Loss: 0.00022811
Iteration 46/1000 | Loss: 0.00039321
Iteration 47/1000 | Loss: 0.00020726
Iteration 48/1000 | Loss: 0.00009054
Iteration 49/1000 | Loss: 0.00009189
Iteration 50/1000 | Loss: 0.00011821
Iteration 51/1000 | Loss: 0.00011223
Iteration 52/1000 | Loss: 0.00036908
Iteration 53/1000 | Loss: 0.00038228
Iteration 54/1000 | Loss: 0.00009712
Iteration 55/1000 | Loss: 0.00018344
Iteration 56/1000 | Loss: 0.00012127
Iteration 57/1000 | Loss: 0.00009919
Iteration 58/1000 | Loss: 0.00015551
Iteration 59/1000 | Loss: 0.00025604
Iteration 60/1000 | Loss: 0.00074560
Iteration 61/1000 | Loss: 0.00010856
Iteration 62/1000 | Loss: 0.00011026
Iteration 63/1000 | Loss: 0.00008853
Iteration 64/1000 | Loss: 0.00016824
Iteration 65/1000 | Loss: 0.00008845
Iteration 66/1000 | Loss: 0.00010218
Iteration 67/1000 | Loss: 0.00008799
Iteration 68/1000 | Loss: 0.00008460
Iteration 69/1000 | Loss: 0.00010479
Iteration 70/1000 | Loss: 0.00010056
Iteration 71/1000 | Loss: 0.00024165
Iteration 72/1000 | Loss: 0.00011044
Iteration 73/1000 | Loss: 0.00011526
Iteration 74/1000 | Loss: 0.00009275
Iteration 75/1000 | Loss: 0.00008948
Iteration 76/1000 | Loss: 0.00009762
Iteration 77/1000 | Loss: 0.00024153
Iteration 78/1000 | Loss: 0.00018737
Iteration 79/1000 | Loss: 0.00033176
Iteration 80/1000 | Loss: 0.00008879
Iteration 81/1000 | Loss: 0.00008869
Iteration 82/1000 | Loss: 0.00014650
Iteration 83/1000 | Loss: 0.00008868
Iteration 84/1000 | Loss: 0.00008955
Iteration 85/1000 | Loss: 0.00008691
Iteration 86/1000 | Loss: 0.00023600
Iteration 87/1000 | Loss: 0.00129380
Iteration 88/1000 | Loss: 0.00023629
Iteration 89/1000 | Loss: 0.00018133
Iteration 90/1000 | Loss: 0.00044970
Iteration 91/1000 | Loss: 0.00016032
Iteration 92/1000 | Loss: 0.00008373
Iteration 93/1000 | Loss: 0.00006445
Iteration 94/1000 | Loss: 0.00006206
Iteration 95/1000 | Loss: 0.00030457
Iteration 96/1000 | Loss: 0.00018316
Iteration 97/1000 | Loss: 0.00040804
Iteration 98/1000 | Loss: 0.00019511
Iteration 99/1000 | Loss: 0.00006374
Iteration 100/1000 | Loss: 0.00012414
Iteration 101/1000 | Loss: 0.00007519
Iteration 102/1000 | Loss: 0.00006057
Iteration 103/1000 | Loss: 0.00005956
Iteration 104/1000 | Loss: 0.00035615
Iteration 105/1000 | Loss: 0.00073478
Iteration 106/1000 | Loss: 0.00080176
Iteration 107/1000 | Loss: 0.00013665
Iteration 108/1000 | Loss: 0.00019830
Iteration 109/1000 | Loss: 0.00045597
Iteration 110/1000 | Loss: 0.00034638
Iteration 111/1000 | Loss: 0.00030015
Iteration 112/1000 | Loss: 0.00014546
Iteration 113/1000 | Loss: 0.00026149
Iteration 114/1000 | Loss: 0.00019946
Iteration 115/1000 | Loss: 0.00011333
Iteration 116/1000 | Loss: 0.00008780
Iteration 117/1000 | Loss: 0.00006325
Iteration 118/1000 | Loss: 0.00007544
Iteration 119/1000 | Loss: 0.00006749
Iteration 120/1000 | Loss: 0.00010015
Iteration 121/1000 | Loss: 0.00005537
Iteration 122/1000 | Loss: 0.00007656
Iteration 123/1000 | Loss: 0.00017853
Iteration 124/1000 | Loss: 0.00051646
Iteration 125/1000 | Loss: 0.00018701
Iteration 126/1000 | Loss: 0.00018669
Iteration 127/1000 | Loss: 0.00017384
Iteration 128/1000 | Loss: 0.00006817
Iteration 129/1000 | Loss: 0.00007569
Iteration 130/1000 | Loss: 0.00006326
Iteration 131/1000 | Loss: 0.00010013
Iteration 132/1000 | Loss: 0.00005594
Iteration 133/1000 | Loss: 0.00005547
Iteration 134/1000 | Loss: 0.00024993
Iteration 135/1000 | Loss: 0.00024286
Iteration 136/1000 | Loss: 0.00034950
Iteration 137/1000 | Loss: 0.00010018
Iteration 138/1000 | Loss: 0.00014560
Iteration 139/1000 | Loss: 0.00005749
Iteration 140/1000 | Loss: 0.00005470
Iteration 141/1000 | Loss: 0.00009109
Iteration 142/1000 | Loss: 0.00021853
Iteration 143/1000 | Loss: 0.00027746
Iteration 144/1000 | Loss: 0.00020131
Iteration 145/1000 | Loss: 0.00011820
Iteration 146/1000 | Loss: 0.00005715
Iteration 147/1000 | Loss: 0.00006177
Iteration 148/1000 | Loss: 0.00005047
Iteration 149/1000 | Loss: 0.00004946
Iteration 150/1000 | Loss: 0.00008686
Iteration 151/1000 | Loss: 0.00005474
Iteration 152/1000 | Loss: 0.00015790
Iteration 153/1000 | Loss: 0.00006222
Iteration 154/1000 | Loss: 0.00022360
Iteration 155/1000 | Loss: 0.00037145
Iteration 156/1000 | Loss: 0.00052726
Iteration 157/1000 | Loss: 0.00039917
Iteration 158/1000 | Loss: 0.00034430
Iteration 159/1000 | Loss: 0.00063757
Iteration 160/1000 | Loss: 0.00033180
Iteration 161/1000 | Loss: 0.00045453
Iteration 162/1000 | Loss: 0.00029419
Iteration 163/1000 | Loss: 0.00005529
Iteration 164/1000 | Loss: 0.00019793
Iteration 165/1000 | Loss: 0.00009720
Iteration 166/1000 | Loss: 0.00005479
Iteration 167/1000 | Loss: 0.00006440
Iteration 168/1000 | Loss: 0.00005444
Iteration 169/1000 | Loss: 0.00005438
Iteration 170/1000 | Loss: 0.00005261
Iteration 171/1000 | Loss: 0.00004599
Iteration 172/1000 | Loss: 0.00004541
Iteration 173/1000 | Loss: 0.00004589
Iteration 174/1000 | Loss: 0.00004426
Iteration 175/1000 | Loss: 0.00013028
Iteration 176/1000 | Loss: 0.00020747
Iteration 177/1000 | Loss: 0.00006200
Iteration 178/1000 | Loss: 0.00006450
Iteration 179/1000 | Loss: 0.00004599
Iteration 180/1000 | Loss: 0.00006655
Iteration 181/1000 | Loss: 0.00004248
Iteration 182/1000 | Loss: 0.00004914
Iteration 183/1000 | Loss: 0.00004114
Iteration 184/1000 | Loss: 0.00006312
Iteration 185/1000 | Loss: 0.00004067
Iteration 186/1000 | Loss: 0.00004042
Iteration 187/1000 | Loss: 0.00011723
Iteration 188/1000 | Loss: 0.00005192
Iteration 189/1000 | Loss: 0.00005742
Iteration 190/1000 | Loss: 0.00004200
Iteration 191/1000 | Loss: 0.00005046
Iteration 192/1000 | Loss: 0.00004094
Iteration 193/1000 | Loss: 0.00017423
Iteration 194/1000 | Loss: 0.00005021
Iteration 195/1000 | Loss: 0.00004301
Iteration 196/1000 | Loss: 0.00004033
Iteration 197/1000 | Loss: 0.00004181
Iteration 198/1000 | Loss: 0.00003832
Iteration 199/1000 | Loss: 0.00003746
Iteration 200/1000 | Loss: 0.00011015
Iteration 201/1000 | Loss: 0.00005028
Iteration 202/1000 | Loss: 0.00004099
Iteration 203/1000 | Loss: 0.00003886
Iteration 204/1000 | Loss: 0.00003811
Iteration 205/1000 | Loss: 0.00003638
Iteration 206/1000 | Loss: 0.00003584
Iteration 207/1000 | Loss: 0.00003550
Iteration 208/1000 | Loss: 0.00004132
Iteration 209/1000 | Loss: 0.00003533
Iteration 210/1000 | Loss: 0.00003897
Iteration 211/1000 | Loss: 0.00003497
Iteration 212/1000 | Loss: 0.00003496
Iteration 213/1000 | Loss: 0.00003496
Iteration 214/1000 | Loss: 0.00003495
Iteration 215/1000 | Loss: 0.00003495
Iteration 216/1000 | Loss: 0.00003494
Iteration 217/1000 | Loss: 0.00003494
Iteration 218/1000 | Loss: 0.00003493
Iteration 219/1000 | Loss: 0.00003492
Iteration 220/1000 | Loss: 0.00003492
Iteration 221/1000 | Loss: 0.00003492
Iteration 222/1000 | Loss: 0.00003492
Iteration 223/1000 | Loss: 0.00003492
Iteration 224/1000 | Loss: 0.00003491
Iteration 225/1000 | Loss: 0.00003656
Iteration 226/1000 | Loss: 0.00003485
Iteration 227/1000 | Loss: 0.00003484
Iteration 228/1000 | Loss: 0.00003484
Iteration 229/1000 | Loss: 0.00003484
Iteration 230/1000 | Loss: 0.00003484
Iteration 231/1000 | Loss: 0.00003484
Iteration 232/1000 | Loss: 0.00003483
Iteration 233/1000 | Loss: 0.00003483
Iteration 234/1000 | Loss: 0.00003483
Iteration 235/1000 | Loss: 0.00003483
Iteration 236/1000 | Loss: 0.00003483
Iteration 237/1000 | Loss: 0.00003483
Iteration 238/1000 | Loss: 0.00003483
Iteration 239/1000 | Loss: 0.00003482
Iteration 240/1000 | Loss: 0.00003482
Iteration 241/1000 | Loss: 0.00003631
Iteration 242/1000 | Loss: 0.00003480
Iteration 243/1000 | Loss: 0.00003480
Iteration 244/1000 | Loss: 0.00003480
Iteration 245/1000 | Loss: 0.00003480
Iteration 246/1000 | Loss: 0.00003480
Iteration 247/1000 | Loss: 0.00003480
Iteration 248/1000 | Loss: 0.00003480
Iteration 249/1000 | Loss: 0.00003480
Iteration 250/1000 | Loss: 0.00003480
Iteration 251/1000 | Loss: 0.00003479
Iteration 252/1000 | Loss: 0.00003479
Iteration 253/1000 | Loss: 0.00003479
Iteration 254/1000 | Loss: 0.00003479
Iteration 255/1000 | Loss: 0.00003478
Iteration 256/1000 | Loss: 0.00003478
Iteration 257/1000 | Loss: 0.00003477
Iteration 258/1000 | Loss: 0.00003476
Iteration 259/1000 | Loss: 0.00003476
Iteration 260/1000 | Loss: 0.00003475
Iteration 261/1000 | Loss: 0.00003475
Iteration 262/1000 | Loss: 0.00003474
Iteration 263/1000 | Loss: 0.00003474
Iteration 264/1000 | Loss: 0.00003468
Iteration 265/1000 | Loss: 0.00003468
Iteration 266/1000 | Loss: 0.00003467
Iteration 267/1000 | Loss: 0.00003467
Iteration 268/1000 | Loss: 0.00003467
Iteration 269/1000 | Loss: 0.00003466
Iteration 270/1000 | Loss: 0.00003466
Iteration 271/1000 | Loss: 0.00003466
Iteration 272/1000 | Loss: 0.00003719
Iteration 273/1000 | Loss: 0.00010200
Iteration 274/1000 | Loss: 0.00022004
Iteration 275/1000 | Loss: 0.00005196
Iteration 276/1000 | Loss: 0.00004111
Iteration 277/1000 | Loss: 0.00004746
Iteration 278/1000 | Loss: 0.00003602
Iteration 279/1000 | Loss: 0.00003418
Iteration 280/1000 | Loss: 0.00003296
Iteration 281/1000 | Loss: 0.00003877
Iteration 282/1000 | Loss: 0.00003229
Iteration 283/1000 | Loss: 0.00003687
Iteration 284/1000 | Loss: 0.00015817
Iteration 285/1000 | Loss: 0.00015816
Iteration 286/1000 | Loss: 0.00027478
Iteration 287/1000 | Loss: 0.00014747
Iteration 288/1000 | Loss: 0.00025789
Iteration 289/1000 | Loss: 0.00025241
Iteration 290/1000 | Loss: 0.00030604
Iteration 291/1000 | Loss: 0.00004172
Iteration 292/1000 | Loss: 0.00004054
Iteration 293/1000 | Loss: 0.00003763
Iteration 294/1000 | Loss: 0.00026333
Iteration 295/1000 | Loss: 0.00008831
Iteration 296/1000 | Loss: 0.00021241
Iteration 297/1000 | Loss: 0.00006862
Iteration 298/1000 | Loss: 0.00005893
Iteration 299/1000 | Loss: 0.00009187
Iteration 300/1000 | Loss: 0.00003420
Iteration 301/1000 | Loss: 0.00007532
Iteration 302/1000 | Loss: 0.00003330
Iteration 303/1000 | Loss: 0.00003328
Iteration 304/1000 | Loss: 0.00003255
Iteration 305/1000 | Loss: 0.00003120
Iteration 306/1000 | Loss: 0.00003318
Iteration 307/1000 | Loss: 0.00004854
Iteration 308/1000 | Loss: 0.00003209
Iteration 309/1000 | Loss: 0.00003450
Iteration 310/1000 | Loss: 0.00003077
Iteration 311/1000 | Loss: 0.00006641
Iteration 312/1000 | Loss: 0.00003048
Iteration 313/1000 | Loss: 0.00003347
Iteration 314/1000 | Loss: 0.00003034
Iteration 315/1000 | Loss: 0.00003302
Iteration 316/1000 | Loss: 0.00003023
Iteration 317/1000 | Loss: 0.00003021
Iteration 318/1000 | Loss: 0.00003020
Iteration 319/1000 | Loss: 0.00003020
Iteration 320/1000 | Loss: 0.00003019
Iteration 321/1000 | Loss: 0.00003018
Iteration 322/1000 | Loss: 0.00003016
Iteration 323/1000 | Loss: 0.00003016
Iteration 324/1000 | Loss: 0.00003015
Iteration 325/1000 | Loss: 0.00003010
Iteration 326/1000 | Loss: 0.00003841
Iteration 327/1000 | Loss: 0.00003059
Iteration 328/1000 | Loss: 0.00003002
Iteration 329/1000 | Loss: 0.00003002
Iteration 330/1000 | Loss: 0.00003002
Iteration 331/1000 | Loss: 0.00003001
Iteration 332/1000 | Loss: 0.00003001
Iteration 333/1000 | Loss: 0.00003001
Iteration 334/1000 | Loss: 0.00003001
Iteration 335/1000 | Loss: 0.00003001
Iteration 336/1000 | Loss: 0.00003067
Iteration 337/1000 | Loss: 0.00003066
Iteration 338/1000 | Loss: 0.00002998
Iteration 339/1000 | Loss: 0.00002998
Iteration 340/1000 | Loss: 0.00002998
Iteration 341/1000 | Loss: 0.00002998
Iteration 342/1000 | Loss: 0.00002998
Iteration 343/1000 | Loss: 0.00002998
Iteration 344/1000 | Loss: 0.00002998
Iteration 345/1000 | Loss: 0.00002997
Iteration 346/1000 | Loss: 0.00003001
Iteration 347/1000 | Loss: 0.00003001
Iteration 348/1000 | Loss: 0.00003001
Iteration 349/1000 | Loss: 0.00002996
Iteration 350/1000 | Loss: 0.00002995
Iteration 351/1000 | Loss: 0.00002995
Iteration 352/1000 | Loss: 0.00002995
Iteration 353/1000 | Loss: 0.00002994
Iteration 354/1000 | Loss: 0.00002994
Iteration 355/1000 | Loss: 0.00002994
Iteration 356/1000 | Loss: 0.00002994
Iteration 357/1000 | Loss: 0.00002994
Iteration 358/1000 | Loss: 0.00002994
Iteration 359/1000 | Loss: 0.00002994
Iteration 360/1000 | Loss: 0.00002994
Iteration 361/1000 | Loss: 0.00002994
Iteration 362/1000 | Loss: 0.00002994
Iteration 363/1000 | Loss: 0.00002994
Iteration 364/1000 | Loss: 0.00002994
Iteration 365/1000 | Loss: 0.00002994
Iteration 366/1000 | Loss: 0.00002994
Iteration 367/1000 | Loss: 0.00002994
Iteration 368/1000 | Loss: 0.00002994
Iteration 369/1000 | Loss: 0.00002993
Iteration 370/1000 | Loss: 0.00002993
Iteration 371/1000 | Loss: 0.00002993
Iteration 372/1000 | Loss: 0.00002993
Iteration 373/1000 | Loss: 0.00002993
Iteration 374/1000 | Loss: 0.00002993
Iteration 375/1000 | Loss: 0.00002993
Iteration 376/1000 | Loss: 0.00002992
Iteration 377/1000 | Loss: 0.00002992
Iteration 378/1000 | Loss: 0.00002991
Iteration 379/1000 | Loss: 0.00002991
Iteration 380/1000 | Loss: 0.00002991
Iteration 381/1000 | Loss: 0.00002991
Iteration 382/1000 | Loss: 0.00002991
Iteration 383/1000 | Loss: 0.00002991
Iteration 384/1000 | Loss: 0.00002991
Iteration 385/1000 | Loss: 0.00002990
Iteration 386/1000 | Loss: 0.00002990
Iteration 387/1000 | Loss: 0.00002990
Iteration 388/1000 | Loss: 0.00002990
Iteration 389/1000 | Loss: 0.00002990
Iteration 390/1000 | Loss: 0.00002990
Iteration 391/1000 | Loss: 0.00002990
Iteration 392/1000 | Loss: 0.00002990
Iteration 393/1000 | Loss: 0.00002990
Iteration 394/1000 | Loss: 0.00002990
Iteration 395/1000 | Loss: 0.00002990
Iteration 396/1000 | Loss: 0.00002990
Iteration 397/1000 | Loss: 0.00002989
Iteration 398/1000 | Loss: 0.00002989
Iteration 399/1000 | Loss: 0.00002989
Iteration 400/1000 | Loss: 0.00002989
Iteration 401/1000 | Loss: 0.00002989
Iteration 402/1000 | Loss: 0.00002988
Iteration 403/1000 | Loss: 0.00002988
Iteration 404/1000 | Loss: 0.00002988
Iteration 405/1000 | Loss: 0.00002988
Iteration 406/1000 | Loss: 0.00002987
Iteration 407/1000 | Loss: 0.00002987
Iteration 408/1000 | Loss: 0.00002987
Iteration 409/1000 | Loss: 0.00002987
Iteration 410/1000 | Loss: 0.00002987
Iteration 411/1000 | Loss: 0.00002987
Iteration 412/1000 | Loss: 0.00002986
Iteration 413/1000 | Loss: 0.00002986
Iteration 414/1000 | Loss: 0.00002986
Iteration 415/1000 | Loss: 0.00002986
Iteration 416/1000 | Loss: 0.00002985
Iteration 417/1000 | Loss: 0.00002985
Iteration 418/1000 | Loss: 0.00002984
Iteration 419/1000 | Loss: 0.00002984
Iteration 420/1000 | Loss: 0.00004281
Iteration 421/1000 | Loss: 0.00003086
Iteration 422/1000 | Loss: 0.00003371
Iteration 423/1000 | Loss: 0.00002983
Iteration 424/1000 | Loss: 0.00002984
Iteration 425/1000 | Loss: 0.00002982
Iteration 426/1000 | Loss: 0.00002982
Iteration 427/1000 | Loss: 0.00002981
Iteration 428/1000 | Loss: 0.00002981
Iteration 429/1000 | Loss: 0.00002981
Iteration 430/1000 | Loss: 0.00002981
Iteration 431/1000 | Loss: 0.00002981
Iteration 432/1000 | Loss: 0.00002981
Iteration 433/1000 | Loss: 0.00002981
Iteration 434/1000 | Loss: 0.00002981
Iteration 435/1000 | Loss: 0.00002981
Iteration 436/1000 | Loss: 0.00002981
Iteration 437/1000 | Loss: 0.00002981
Iteration 438/1000 | Loss: 0.00002980
Iteration 439/1000 | Loss: 0.00002980
Iteration 440/1000 | Loss: 0.00002980
Iteration 441/1000 | Loss: 0.00002980
Iteration 442/1000 | Loss: 0.00002980
Iteration 443/1000 | Loss: 0.00002980
Iteration 444/1000 | Loss: 0.00002980
Iteration 445/1000 | Loss: 0.00002980
Iteration 446/1000 | Loss: 0.00002980
Iteration 447/1000 | Loss: 0.00002980
Iteration 448/1000 | Loss: 0.00002980
Iteration 449/1000 | Loss: 0.00002980
Iteration 450/1000 | Loss: 0.00002980
Iteration 451/1000 | Loss: 0.00002980
Iteration 452/1000 | Loss: 0.00002980
Iteration 453/1000 | Loss: 0.00002980
Iteration 454/1000 | Loss: 0.00002980
Iteration 455/1000 | Loss: 0.00002980
Iteration 456/1000 | Loss: 0.00002980
Iteration 457/1000 | Loss: 0.00002979
Iteration 458/1000 | Loss: 0.00002979
Iteration 459/1000 | Loss: 0.00002979
Iteration 460/1000 | Loss: 0.00002979
Iteration 461/1000 | Loss: 0.00002979
Iteration 462/1000 | Loss: 0.00002979
Iteration 463/1000 | Loss: 0.00002979
Iteration 464/1000 | Loss: 0.00002979
Iteration 465/1000 | Loss: 0.00002979
Iteration 466/1000 | Loss: 0.00002979
Iteration 467/1000 | Loss: 0.00002978
Iteration 468/1000 | Loss: 0.00002978
Iteration 469/1000 | Loss: 0.00002978
Iteration 470/1000 | Loss: 0.00002978
Iteration 471/1000 | Loss: 0.00002978
Iteration 472/1000 | Loss: 0.00002978
Iteration 473/1000 | Loss: 0.00002978
Iteration 474/1000 | Loss: 0.00002978
Iteration 475/1000 | Loss: 0.00002978
Iteration 476/1000 | Loss: 0.00002978
Iteration 477/1000 | Loss: 0.00002978
Iteration 478/1000 | Loss: 0.00002977
Iteration 479/1000 | Loss: 0.00002977
Iteration 480/1000 | Loss: 0.00002977
Iteration 481/1000 | Loss: 0.00002977
Iteration 482/1000 | Loss: 0.00002977
Iteration 483/1000 | Loss: 0.00002977
Iteration 484/1000 | Loss: 0.00002977
Iteration 485/1000 | Loss: 0.00002977
Iteration 486/1000 | Loss: 0.00002977
Iteration 487/1000 | Loss: 0.00002977
Iteration 488/1000 | Loss: 0.00002977
Iteration 489/1000 | Loss: 0.00002977
Iteration 490/1000 | Loss: 0.00002977
Iteration 491/1000 | Loss: 0.00002976
Iteration 492/1000 | Loss: 0.00002976
Iteration 493/1000 | Loss: 0.00002976
Iteration 494/1000 | Loss: 0.00002976
Iteration 495/1000 | Loss: 0.00002976
Iteration 496/1000 | Loss: 0.00002976
Iteration 497/1000 | Loss: 0.00002976
Iteration 498/1000 | Loss: 0.00002976
Iteration 499/1000 | Loss: 0.00002975
Iteration 500/1000 | Loss: 0.00002975
Iteration 501/1000 | Loss: 0.00002975
Iteration 502/1000 | Loss: 0.00002975
Iteration 503/1000 | Loss: 0.00002975
Iteration 504/1000 | Loss: 0.00002974
Iteration 505/1000 | Loss: 0.00002974
Iteration 506/1000 | Loss: 0.00002974
Iteration 507/1000 | Loss: 0.00002974
Iteration 508/1000 | Loss: 0.00002974
Iteration 509/1000 | Loss: 0.00002974
Iteration 510/1000 | Loss: 0.00002974
Iteration 511/1000 | Loss: 0.00002973
Iteration 512/1000 | Loss: 0.00002973
Iteration 513/1000 | Loss: 0.00002973
Iteration 514/1000 | Loss: 0.00002973
Iteration 515/1000 | Loss: 0.00002972
Iteration 516/1000 | Loss: 0.00002972
Iteration 517/1000 | Loss: 0.00002972
Iteration 518/1000 | Loss: 0.00002972
Iteration 519/1000 | Loss: 0.00002972
Iteration 520/1000 | Loss: 0.00002972
Iteration 521/1000 | Loss: 0.00002972
Iteration 522/1000 | Loss: 0.00002972
Iteration 523/1000 | Loss: 0.00002972
Iteration 524/1000 | Loss: 0.00002972
Iteration 525/1000 | Loss: 0.00002972
Iteration 526/1000 | Loss: 0.00002972
Iteration 527/1000 | Loss: 0.00002972
Iteration 528/1000 | Loss: 0.00002972
Iteration 529/1000 | Loss: 0.00002971
Iteration 530/1000 | Loss: 0.00002971
Iteration 531/1000 | Loss: 0.00002971
Iteration 532/1000 | Loss: 0.00002971
Iteration 533/1000 | Loss: 0.00002971
Iteration 534/1000 | Loss: 0.00002971
Iteration 535/1000 | Loss: 0.00002971
Iteration 536/1000 | Loss: 0.00002971
Iteration 537/1000 | Loss: 0.00002971
Iteration 538/1000 | Loss: 0.00002970
Iteration 539/1000 | Loss: 0.00002970
Iteration 540/1000 | Loss: 0.00002970
Iteration 541/1000 | Loss: 0.00002970
Iteration 542/1000 | Loss: 0.00002970
Iteration 543/1000 | Loss: 0.00002970
Iteration 544/1000 | Loss: 0.00002970
Iteration 545/1000 | Loss: 0.00002970
Iteration 546/1000 | Loss: 0.00002970
Iteration 547/1000 | Loss: 0.00002970
Iteration 548/1000 | Loss: 0.00002970
Iteration 549/1000 | Loss: 0.00002970
Iteration 550/1000 | Loss: 0.00002970
Iteration 551/1000 | Loss: 0.00010430
Iteration 552/1000 | Loss: 0.00004009
Iteration 553/1000 | Loss: 0.00009334
Iteration 554/1000 | Loss: 0.00003326
Iteration 555/1000 | Loss: 0.00003407
Iteration 556/1000 | Loss: 0.00003400
Iteration 557/1000 | Loss: 0.00002985
Iteration 558/1000 | Loss: 0.00003116
Iteration 559/1000 | Loss: 0.00002874
Iteration 560/1000 | Loss: 0.00002870
Iteration 561/1000 | Loss: 0.00002851
Iteration 562/1000 | Loss: 0.00002846
Iteration 563/1000 | Loss: 0.00002844
Iteration 564/1000 | Loss: 0.00002843
Iteration 565/1000 | Loss: 0.00002841
Iteration 566/1000 | Loss: 0.00004139
Iteration 567/1000 | Loss: 0.00002840
Iteration 568/1000 | Loss: 0.00002957
Iteration 569/1000 | Loss: 0.00002824
Iteration 570/1000 | Loss: 0.00002824
Iteration 571/1000 | Loss: 0.00002824
Iteration 572/1000 | Loss: 0.00002824
Iteration 573/1000 | Loss: 0.00002824
Iteration 574/1000 | Loss: 0.00002824
Iteration 575/1000 | Loss: 0.00002824
Iteration 576/1000 | Loss: 0.00002824
Iteration 577/1000 | Loss: 0.00002824
Iteration 578/1000 | Loss: 0.00002824
Iteration 579/1000 | Loss: 0.00002824
Iteration 580/1000 | Loss: 0.00002824
Iteration 581/1000 | Loss: 0.00002824
Iteration 582/1000 | Loss: 0.00002823
Iteration 583/1000 | Loss: 0.00002823
Iteration 584/1000 | Loss: 0.00002823
Iteration 585/1000 | Loss: 0.00002823
Iteration 586/1000 | Loss: 0.00002823
Iteration 587/1000 | Loss: 0.00002823
Iteration 588/1000 | Loss: 0.00002823
Iteration 589/1000 | Loss: 0.00002823
Iteration 590/1000 | Loss: 0.00002822
Iteration 591/1000 | Loss: 0.00002821
Iteration 592/1000 | Loss: 0.00002820
Iteration 593/1000 | Loss: 0.00003479
Iteration 594/1000 | Loss: 0.00002916
Iteration 595/1000 | Loss: 0.00002817
Iteration 596/1000 | Loss: 0.00002817
Iteration 597/1000 | Loss: 0.00002817
Iteration 598/1000 | Loss: 0.00002817
Iteration 599/1000 | Loss: 0.00002817
Iteration 600/1000 | Loss: 0.00002817
Iteration 601/1000 | Loss: 0.00002817
Iteration 602/1000 | Loss: 0.00002817
Iteration 603/1000 | Loss: 0.00002817
Iteration 604/1000 | Loss: 0.00002817
Iteration 605/1000 | Loss: 0.00002817
Iteration 606/1000 | Loss: 0.00002817
Iteration 607/1000 | Loss: 0.00002817
Iteration 608/1000 | Loss: 0.00002817
Iteration 609/1000 | Loss: 0.00002817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 609. Stopping optimization.
Last 5 losses: [2.816975575115066e-05, 2.816975575115066e-05, 2.816975575115066e-05, 2.816975575115066e-05, 2.816975575115066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.816975575115066e-05

Optimization complete. Final v2v error: 3.113229513168335 mm

Highest mean error: 10.42431354522705 mm for frame 206

Lowest mean error: 2.1402182579040527 mm for frame 72

Saving results

Total time: 503.68046259880066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446181
Iteration 2/25 | Loss: 0.00151385
Iteration 3/25 | Loss: 0.00142863
Iteration 4/25 | Loss: 0.00140392
Iteration 5/25 | Loss: 0.00139854
Iteration 6/25 | Loss: 0.00139729
Iteration 7/25 | Loss: 0.00139729
Iteration 8/25 | Loss: 0.00139729
Iteration 9/25 | Loss: 0.00139729
Iteration 10/25 | Loss: 0.00139729
Iteration 11/25 | Loss: 0.00139729
Iteration 12/25 | Loss: 0.00139729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013972867745906115, 0.0013972867745906115, 0.0013972867745906115, 0.0013972867745906115, 0.0013972867745906115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013972867745906115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20877254
Iteration 2/25 | Loss: 0.00189274
Iteration 3/25 | Loss: 0.00189274
Iteration 4/25 | Loss: 0.00189273
Iteration 5/25 | Loss: 0.00189273
Iteration 6/25 | Loss: 0.00189273
Iteration 7/25 | Loss: 0.00189273
Iteration 8/25 | Loss: 0.00189273
Iteration 9/25 | Loss: 0.00189273
Iteration 10/25 | Loss: 0.00189273
Iteration 11/25 | Loss: 0.00189273
Iteration 12/25 | Loss: 0.00189273
Iteration 13/25 | Loss: 0.00189273
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0018927338533103466, 0.0018927338533103466, 0.0018927338533103466, 0.0018927338533103466, 0.0018927338533103466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018927338533103466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189273
Iteration 2/1000 | Loss: 0.00005692
Iteration 3/1000 | Loss: 0.00003385
Iteration 4/1000 | Loss: 0.00002743
Iteration 5/1000 | Loss: 0.00002498
Iteration 6/1000 | Loss: 0.00002351
Iteration 7/1000 | Loss: 0.00002252
Iteration 8/1000 | Loss: 0.00002161
Iteration 9/1000 | Loss: 0.00002124
Iteration 10/1000 | Loss: 0.00002083
Iteration 11/1000 | Loss: 0.00002070
Iteration 12/1000 | Loss: 0.00002050
Iteration 13/1000 | Loss: 0.00002036
Iteration 14/1000 | Loss: 0.00002020
Iteration 15/1000 | Loss: 0.00002019
Iteration 16/1000 | Loss: 0.00002019
Iteration 17/1000 | Loss: 0.00002015
Iteration 18/1000 | Loss: 0.00002012
Iteration 19/1000 | Loss: 0.00002011
Iteration 20/1000 | Loss: 0.00002011
Iteration 21/1000 | Loss: 0.00002010
Iteration 22/1000 | Loss: 0.00002010
Iteration 23/1000 | Loss: 0.00002010
Iteration 24/1000 | Loss: 0.00002010
Iteration 25/1000 | Loss: 0.00002009
Iteration 26/1000 | Loss: 0.00002009
Iteration 27/1000 | Loss: 0.00002009
Iteration 28/1000 | Loss: 0.00002008
Iteration 29/1000 | Loss: 0.00002008
Iteration 30/1000 | Loss: 0.00002008
Iteration 31/1000 | Loss: 0.00002008
Iteration 32/1000 | Loss: 0.00002007
Iteration 33/1000 | Loss: 0.00002007
Iteration 34/1000 | Loss: 0.00002007
Iteration 35/1000 | Loss: 0.00002007
Iteration 36/1000 | Loss: 0.00002007
Iteration 37/1000 | Loss: 0.00002007
Iteration 38/1000 | Loss: 0.00002007
Iteration 39/1000 | Loss: 0.00002007
Iteration 40/1000 | Loss: 0.00002007
Iteration 41/1000 | Loss: 0.00002007
Iteration 42/1000 | Loss: 0.00002007
Iteration 43/1000 | Loss: 0.00002007
Iteration 44/1000 | Loss: 0.00002006
Iteration 45/1000 | Loss: 0.00002006
Iteration 46/1000 | Loss: 0.00002005
Iteration 47/1000 | Loss: 0.00002005
Iteration 48/1000 | Loss: 0.00002005
Iteration 49/1000 | Loss: 0.00002005
Iteration 50/1000 | Loss: 0.00002005
Iteration 51/1000 | Loss: 0.00002004
Iteration 52/1000 | Loss: 0.00002004
Iteration 53/1000 | Loss: 0.00002004
Iteration 54/1000 | Loss: 0.00002004
Iteration 55/1000 | Loss: 0.00002004
Iteration 56/1000 | Loss: 0.00002004
Iteration 57/1000 | Loss: 0.00002004
Iteration 58/1000 | Loss: 0.00002004
Iteration 59/1000 | Loss: 0.00002004
Iteration 60/1000 | Loss: 0.00002004
Iteration 61/1000 | Loss: 0.00002004
Iteration 62/1000 | Loss: 0.00002004
Iteration 63/1000 | Loss: 0.00002004
Iteration 64/1000 | Loss: 0.00002004
Iteration 65/1000 | Loss: 0.00002004
Iteration 66/1000 | Loss: 0.00002004
Iteration 67/1000 | Loss: 0.00002004
Iteration 68/1000 | Loss: 0.00002004
Iteration 69/1000 | Loss: 0.00002004
Iteration 70/1000 | Loss: 0.00002004
Iteration 71/1000 | Loss: 0.00002004
Iteration 72/1000 | Loss: 0.00002004
Iteration 73/1000 | Loss: 0.00002004
Iteration 74/1000 | Loss: 0.00002004
Iteration 75/1000 | Loss: 0.00002004
Iteration 76/1000 | Loss: 0.00002004
Iteration 77/1000 | Loss: 0.00002004
Iteration 78/1000 | Loss: 0.00002004
Iteration 79/1000 | Loss: 0.00002004
Iteration 80/1000 | Loss: 0.00002004
Iteration 81/1000 | Loss: 0.00002004
Iteration 82/1000 | Loss: 0.00002004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [2.0038822185597382e-05, 2.0038822185597382e-05, 2.0038822185597382e-05, 2.0038822185597382e-05, 2.0038822185597382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0038822185597382e-05

Optimization complete. Final v2v error: 3.8309764862060547 mm

Highest mean error: 4.214385032653809 mm for frame 120

Lowest mean error: 3.4606926441192627 mm for frame 4

Saving results

Total time: 31.36245894432068
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101203
Iteration 2/25 | Loss: 0.00325022
Iteration 3/25 | Loss: 0.00220656
Iteration 4/25 | Loss: 0.00205419
Iteration 5/25 | Loss: 0.00196577
Iteration 6/25 | Loss: 0.00190616
Iteration 7/25 | Loss: 0.00184741
Iteration 8/25 | Loss: 0.00182036
Iteration 9/25 | Loss: 0.00178474
Iteration 10/25 | Loss: 0.00176535
Iteration 11/25 | Loss: 0.00175584
Iteration 12/25 | Loss: 0.00174937
Iteration 13/25 | Loss: 0.00175020
Iteration 14/25 | Loss: 0.00174231
Iteration 15/25 | Loss: 0.00173225
Iteration 16/25 | Loss: 0.00172374
Iteration 17/25 | Loss: 0.00171611
Iteration 18/25 | Loss: 0.00171291
Iteration 19/25 | Loss: 0.00170836
Iteration 20/25 | Loss: 0.00170045
Iteration 21/25 | Loss: 0.00169689
Iteration 22/25 | Loss: 0.00169646
Iteration 23/25 | Loss: 0.00169631
Iteration 24/25 | Loss: 0.00169619
Iteration 25/25 | Loss: 0.00169602

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18616247
Iteration 2/25 | Loss: 0.00597083
Iteration 3/25 | Loss: 0.00480074
Iteration 4/25 | Loss: 0.00479868
Iteration 5/25 | Loss: 0.00479298
Iteration 6/25 | Loss: 0.00479297
Iteration 7/25 | Loss: 0.00479297
Iteration 8/25 | Loss: 0.00479297
Iteration 9/25 | Loss: 0.00479297
Iteration 10/25 | Loss: 0.00479297
Iteration 11/25 | Loss: 0.00479297
Iteration 12/25 | Loss: 0.00479297
Iteration 13/25 | Loss: 0.00479297
Iteration 14/25 | Loss: 0.00479297
Iteration 15/25 | Loss: 0.00479297
Iteration 16/25 | Loss: 0.00479297
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004792971536517143, 0.004792971536517143, 0.004792971536517143, 0.004792971536517143, 0.004792971536517143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004792971536517143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00479297
Iteration 2/1000 | Loss: 0.00196285
Iteration 3/1000 | Loss: 0.00138029
Iteration 4/1000 | Loss: 0.00319712
Iteration 5/1000 | Loss: 0.00214809
Iteration 6/1000 | Loss: 0.00175690
Iteration 7/1000 | Loss: 0.00116470
Iteration 8/1000 | Loss: 0.00154557
Iteration 9/1000 | Loss: 0.00178119
Iteration 10/1000 | Loss: 0.00084136
Iteration 11/1000 | Loss: 0.00118734
Iteration 12/1000 | Loss: 0.00052124
Iteration 13/1000 | Loss: 0.00095038
Iteration 14/1000 | Loss: 0.00069516
Iteration 15/1000 | Loss: 0.00157364
Iteration 16/1000 | Loss: 0.00158278
Iteration 17/1000 | Loss: 0.00087593
Iteration 18/1000 | Loss: 0.00149020
Iteration 19/1000 | Loss: 0.00169127
Iteration 20/1000 | Loss: 0.00151242
Iteration 21/1000 | Loss: 0.00089106
Iteration 22/1000 | Loss: 0.00037247
Iteration 23/1000 | Loss: 0.00042224
Iteration 24/1000 | Loss: 0.00048318
Iteration 25/1000 | Loss: 0.00039897
Iteration 26/1000 | Loss: 0.00092689
Iteration 27/1000 | Loss: 0.00043704
Iteration 28/1000 | Loss: 0.00066656
Iteration 29/1000 | Loss: 0.00044206
Iteration 30/1000 | Loss: 0.00082729
Iteration 31/1000 | Loss: 0.00132987
Iteration 32/1000 | Loss: 0.00058442
Iteration 33/1000 | Loss: 0.00105850
Iteration 34/1000 | Loss: 0.00075732
Iteration 35/1000 | Loss: 0.00113250
Iteration 36/1000 | Loss: 0.00086914
Iteration 37/1000 | Loss: 0.00091761
Iteration 38/1000 | Loss: 0.00116788
Iteration 39/1000 | Loss: 0.00087123
Iteration 40/1000 | Loss: 0.00124862
Iteration 41/1000 | Loss: 0.00070469
Iteration 42/1000 | Loss: 0.00060581
Iteration 43/1000 | Loss: 0.00062743
Iteration 44/1000 | Loss: 0.00054893
Iteration 45/1000 | Loss: 0.00079888
Iteration 46/1000 | Loss: 0.00058262
Iteration 47/1000 | Loss: 0.00052633
Iteration 48/1000 | Loss: 0.00030340
Iteration 49/1000 | Loss: 0.00038331
Iteration 50/1000 | Loss: 0.00077160
Iteration 51/1000 | Loss: 0.00020967
Iteration 52/1000 | Loss: 0.00020285
Iteration 53/1000 | Loss: 0.00079756
Iteration 54/1000 | Loss: 0.00017448
Iteration 55/1000 | Loss: 0.00013407
Iteration 56/1000 | Loss: 0.00014826
Iteration 57/1000 | Loss: 0.00032019
Iteration 58/1000 | Loss: 0.00023888
Iteration 59/1000 | Loss: 0.00151897
Iteration 60/1000 | Loss: 0.00147822
Iteration 61/1000 | Loss: 0.00176424
Iteration 62/1000 | Loss: 0.00040383
Iteration 63/1000 | Loss: 0.00017982
Iteration 64/1000 | Loss: 0.00040612
Iteration 65/1000 | Loss: 0.00019558
Iteration 66/1000 | Loss: 0.00036453
Iteration 67/1000 | Loss: 0.00014421
Iteration 68/1000 | Loss: 0.00026801
Iteration 69/1000 | Loss: 0.00081507
Iteration 70/1000 | Loss: 0.00051052
Iteration 71/1000 | Loss: 0.00056705
Iteration 72/1000 | Loss: 0.00036618
Iteration 73/1000 | Loss: 0.00019343
Iteration 74/1000 | Loss: 0.00022222
Iteration 75/1000 | Loss: 0.00029755
Iteration 76/1000 | Loss: 0.00030067
Iteration 77/1000 | Loss: 0.00010755
Iteration 78/1000 | Loss: 0.00017798
Iteration 79/1000 | Loss: 0.00012930
Iteration 80/1000 | Loss: 0.00011171
Iteration 81/1000 | Loss: 0.00022581
Iteration 82/1000 | Loss: 0.00017773
Iteration 83/1000 | Loss: 0.00025004
Iteration 84/1000 | Loss: 0.00022836
Iteration 85/1000 | Loss: 0.00035756
Iteration 86/1000 | Loss: 0.00013662
Iteration 87/1000 | Loss: 0.00016752
Iteration 88/1000 | Loss: 0.00033465
Iteration 89/1000 | Loss: 0.00024962
Iteration 90/1000 | Loss: 0.00042516
Iteration 91/1000 | Loss: 0.00024551
Iteration 92/1000 | Loss: 0.00024394
Iteration 93/1000 | Loss: 0.00010535
Iteration 94/1000 | Loss: 0.00014290
Iteration 95/1000 | Loss: 0.00024020
Iteration 96/1000 | Loss: 0.00041352
Iteration 97/1000 | Loss: 0.00021581
Iteration 98/1000 | Loss: 0.00011189
Iteration 99/1000 | Loss: 0.00011896
Iteration 100/1000 | Loss: 0.00008898
Iteration 101/1000 | Loss: 0.00014023
Iteration 102/1000 | Loss: 0.00007138
Iteration 103/1000 | Loss: 0.00006592
Iteration 104/1000 | Loss: 0.00006445
Iteration 105/1000 | Loss: 0.00006335
Iteration 106/1000 | Loss: 0.00007610
Iteration 107/1000 | Loss: 0.00006218
Iteration 108/1000 | Loss: 0.00013271
Iteration 109/1000 | Loss: 0.00009204
Iteration 110/1000 | Loss: 0.00006726
Iteration 111/1000 | Loss: 0.00006301
Iteration 112/1000 | Loss: 0.00006128
Iteration 113/1000 | Loss: 0.00011881
Iteration 114/1000 | Loss: 0.00006000
Iteration 115/1000 | Loss: 0.00005943
Iteration 116/1000 | Loss: 0.00005912
Iteration 117/1000 | Loss: 0.00005897
Iteration 118/1000 | Loss: 0.00005878
Iteration 119/1000 | Loss: 0.00005872
Iteration 120/1000 | Loss: 0.00005865
Iteration 121/1000 | Loss: 0.00005865
Iteration 122/1000 | Loss: 0.00005863
Iteration 123/1000 | Loss: 0.00005863
Iteration 124/1000 | Loss: 0.00005863
Iteration 125/1000 | Loss: 0.00005860
Iteration 126/1000 | Loss: 0.00005859
Iteration 127/1000 | Loss: 0.00010935
Iteration 128/1000 | Loss: 0.00006430
Iteration 129/1000 | Loss: 0.00006925
Iteration 130/1000 | Loss: 0.00005855
Iteration 131/1000 | Loss: 0.00005854
Iteration 132/1000 | Loss: 0.00005854
Iteration 133/1000 | Loss: 0.00005854
Iteration 134/1000 | Loss: 0.00005854
Iteration 135/1000 | Loss: 0.00005853
Iteration 136/1000 | Loss: 0.00005853
Iteration 137/1000 | Loss: 0.00005853
Iteration 138/1000 | Loss: 0.00005853
Iteration 139/1000 | Loss: 0.00005853
Iteration 140/1000 | Loss: 0.00005852
Iteration 141/1000 | Loss: 0.00005852
Iteration 142/1000 | Loss: 0.00005852
Iteration 143/1000 | Loss: 0.00005851
Iteration 144/1000 | Loss: 0.00005851
Iteration 145/1000 | Loss: 0.00005851
Iteration 146/1000 | Loss: 0.00005851
Iteration 147/1000 | Loss: 0.00005851
Iteration 148/1000 | Loss: 0.00005850
Iteration 149/1000 | Loss: 0.00005850
Iteration 150/1000 | Loss: 0.00005850
Iteration 151/1000 | Loss: 0.00005850
Iteration 152/1000 | Loss: 0.00005850
Iteration 153/1000 | Loss: 0.00005850
Iteration 154/1000 | Loss: 0.00005850
Iteration 155/1000 | Loss: 0.00005849
Iteration 156/1000 | Loss: 0.00005849
Iteration 157/1000 | Loss: 0.00005848
Iteration 158/1000 | Loss: 0.00005848
Iteration 159/1000 | Loss: 0.00005848
Iteration 160/1000 | Loss: 0.00005847
Iteration 161/1000 | Loss: 0.00005847
Iteration 162/1000 | Loss: 0.00005847
Iteration 163/1000 | Loss: 0.00005847
Iteration 164/1000 | Loss: 0.00005847
Iteration 165/1000 | Loss: 0.00005847
Iteration 166/1000 | Loss: 0.00005847
Iteration 167/1000 | Loss: 0.00005847
Iteration 168/1000 | Loss: 0.00005846
Iteration 169/1000 | Loss: 0.00005846
Iteration 170/1000 | Loss: 0.00005846
Iteration 171/1000 | Loss: 0.00005845
Iteration 172/1000 | Loss: 0.00005845
Iteration 173/1000 | Loss: 0.00005845
Iteration 174/1000 | Loss: 0.00005845
Iteration 175/1000 | Loss: 0.00005845
Iteration 176/1000 | Loss: 0.00005844
Iteration 177/1000 | Loss: 0.00005844
Iteration 178/1000 | Loss: 0.00005844
Iteration 179/1000 | Loss: 0.00005843
Iteration 180/1000 | Loss: 0.00005839
Iteration 181/1000 | Loss: 0.00005839
Iteration 182/1000 | Loss: 0.00005839
Iteration 183/1000 | Loss: 0.00005839
Iteration 184/1000 | Loss: 0.00005838
Iteration 185/1000 | Loss: 0.00005838
Iteration 186/1000 | Loss: 0.00005838
Iteration 187/1000 | Loss: 0.00005838
Iteration 188/1000 | Loss: 0.00005837
Iteration 189/1000 | Loss: 0.00005837
Iteration 190/1000 | Loss: 0.00005836
Iteration 191/1000 | Loss: 0.00005836
Iteration 192/1000 | Loss: 0.00005836
Iteration 193/1000 | Loss: 0.00005836
Iteration 194/1000 | Loss: 0.00005835
Iteration 195/1000 | Loss: 0.00005835
Iteration 196/1000 | Loss: 0.00005835
Iteration 197/1000 | Loss: 0.00005835
Iteration 198/1000 | Loss: 0.00005834
Iteration 199/1000 | Loss: 0.00005834
Iteration 200/1000 | Loss: 0.00005834
Iteration 201/1000 | Loss: 0.00005834
Iteration 202/1000 | Loss: 0.00005834
Iteration 203/1000 | Loss: 0.00005833
Iteration 204/1000 | Loss: 0.00005833
Iteration 205/1000 | Loss: 0.00005833
Iteration 206/1000 | Loss: 0.00005833
Iteration 207/1000 | Loss: 0.00005832
Iteration 208/1000 | Loss: 0.00005832
Iteration 209/1000 | Loss: 0.00005832
Iteration 210/1000 | Loss: 0.00005832
Iteration 211/1000 | Loss: 0.00005832
Iteration 212/1000 | Loss: 0.00005832
Iteration 213/1000 | Loss: 0.00005832
Iteration 214/1000 | Loss: 0.00005832
Iteration 215/1000 | Loss: 0.00005832
Iteration 216/1000 | Loss: 0.00005831
Iteration 217/1000 | Loss: 0.00005831
Iteration 218/1000 | Loss: 0.00005831
Iteration 219/1000 | Loss: 0.00005831
Iteration 220/1000 | Loss: 0.00005831
Iteration 221/1000 | Loss: 0.00005831
Iteration 222/1000 | Loss: 0.00032640
Iteration 223/1000 | Loss: 0.00044646
Iteration 224/1000 | Loss: 0.00056051
Iteration 225/1000 | Loss: 0.00039007
Iteration 226/1000 | Loss: 0.00021784
Iteration 227/1000 | Loss: 0.00081852
Iteration 228/1000 | Loss: 0.00042863
Iteration 229/1000 | Loss: 0.00006572
Iteration 230/1000 | Loss: 0.00006072
Iteration 231/1000 | Loss: 0.00005946
Iteration 232/1000 | Loss: 0.00017761
Iteration 233/1000 | Loss: 0.00006069
Iteration 234/1000 | Loss: 0.00005833
Iteration 235/1000 | Loss: 0.00005830
Iteration 236/1000 | Loss: 0.00005830
Iteration 237/1000 | Loss: 0.00005830
Iteration 238/1000 | Loss: 0.00005829
Iteration 239/1000 | Loss: 0.00005829
Iteration 240/1000 | Loss: 0.00005829
Iteration 241/1000 | Loss: 0.00005829
Iteration 242/1000 | Loss: 0.00005829
Iteration 243/1000 | Loss: 0.00005829
Iteration 244/1000 | Loss: 0.00005829
Iteration 245/1000 | Loss: 0.00005829
Iteration 246/1000 | Loss: 0.00005829
Iteration 247/1000 | Loss: 0.00005829
Iteration 248/1000 | Loss: 0.00005829
Iteration 249/1000 | Loss: 0.00005829
Iteration 250/1000 | Loss: 0.00005828
Iteration 251/1000 | Loss: 0.00005828
Iteration 252/1000 | Loss: 0.00005828
Iteration 253/1000 | Loss: 0.00005828
Iteration 254/1000 | Loss: 0.00005828
Iteration 255/1000 | Loss: 0.00005828
Iteration 256/1000 | Loss: 0.00005828
Iteration 257/1000 | Loss: 0.00005828
Iteration 258/1000 | Loss: 0.00005828
Iteration 259/1000 | Loss: 0.00005828
Iteration 260/1000 | Loss: 0.00005828
Iteration 261/1000 | Loss: 0.00005828
Iteration 262/1000 | Loss: 0.00005828
Iteration 263/1000 | Loss: 0.00005828
Iteration 264/1000 | Loss: 0.00005828
Iteration 265/1000 | Loss: 0.00005828
Iteration 266/1000 | Loss: 0.00005828
Iteration 267/1000 | Loss: 0.00005828
Iteration 268/1000 | Loss: 0.00005827
Iteration 269/1000 | Loss: 0.00005827
Iteration 270/1000 | Loss: 0.00005827
Iteration 271/1000 | Loss: 0.00005827
Iteration 272/1000 | Loss: 0.00005827
Iteration 273/1000 | Loss: 0.00005827
Iteration 274/1000 | Loss: 0.00005827
Iteration 275/1000 | Loss: 0.00005827
Iteration 276/1000 | Loss: 0.00005827
Iteration 277/1000 | Loss: 0.00005827
Iteration 278/1000 | Loss: 0.00005827
Iteration 279/1000 | Loss: 0.00005827
Iteration 280/1000 | Loss: 0.00005827
Iteration 281/1000 | Loss: 0.00005827
Iteration 282/1000 | Loss: 0.00005827
Iteration 283/1000 | Loss: 0.00005827
Iteration 284/1000 | Loss: 0.00005827
Iteration 285/1000 | Loss: 0.00005827
Iteration 286/1000 | Loss: 0.00005827
Iteration 287/1000 | Loss: 0.00005827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [5.8270321460440755e-05, 5.8270321460440755e-05, 5.8270321460440755e-05, 5.8270321460440755e-05, 5.8270321460440755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.8270321460440755e-05

Optimization complete. Final v2v error: 4.092337131500244 mm

Highest mean error: 13.319519996643066 mm for frame 141

Lowest mean error: 3.010615110397339 mm for frame 213

Saving results

Total time: 278.02773451805115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789970
Iteration 2/25 | Loss: 0.00162957
Iteration 3/25 | Loss: 0.00146978
Iteration 4/25 | Loss: 0.00144834
Iteration 5/25 | Loss: 0.00144247
Iteration 6/25 | Loss: 0.00144130
Iteration 7/25 | Loss: 0.00144130
Iteration 8/25 | Loss: 0.00144130
Iteration 9/25 | Loss: 0.00144130
Iteration 10/25 | Loss: 0.00144130
Iteration 11/25 | Loss: 0.00144130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001441295607946813, 0.001441295607946813, 0.001441295607946813, 0.001441295607946813, 0.001441295607946813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001441295607946813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.83462429
Iteration 2/25 | Loss: 0.00197981
Iteration 3/25 | Loss: 0.00197971
Iteration 4/25 | Loss: 0.00197971
Iteration 5/25 | Loss: 0.00197971
Iteration 6/25 | Loss: 0.00197971
Iteration 7/25 | Loss: 0.00197971
Iteration 8/25 | Loss: 0.00197971
Iteration 9/25 | Loss: 0.00197970
Iteration 10/25 | Loss: 0.00197970
Iteration 11/25 | Loss: 0.00197970
Iteration 12/25 | Loss: 0.00197970
Iteration 13/25 | Loss: 0.00197970
Iteration 14/25 | Loss: 0.00197970
Iteration 15/25 | Loss: 0.00197970
Iteration 16/25 | Loss: 0.00197970
Iteration 17/25 | Loss: 0.00197970
Iteration 18/25 | Loss: 0.00197970
Iteration 19/25 | Loss: 0.00197970
Iteration 20/25 | Loss: 0.00197970
Iteration 21/25 | Loss: 0.00197970
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001979704247787595, 0.001979704247787595, 0.001979704247787595, 0.001979704247787595, 0.001979704247787595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001979704247787595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197970
Iteration 2/1000 | Loss: 0.00006200
Iteration 3/1000 | Loss: 0.00003684
Iteration 4/1000 | Loss: 0.00003293
Iteration 5/1000 | Loss: 0.00003093
Iteration 6/1000 | Loss: 0.00002927
Iteration 7/1000 | Loss: 0.00002843
Iteration 8/1000 | Loss: 0.00002747
Iteration 9/1000 | Loss: 0.00002678
Iteration 10/1000 | Loss: 0.00002624
Iteration 11/1000 | Loss: 0.00002582
Iteration 12/1000 | Loss: 0.00002544
Iteration 13/1000 | Loss: 0.00002518
Iteration 14/1000 | Loss: 0.00002504
Iteration 15/1000 | Loss: 0.00002503
Iteration 16/1000 | Loss: 0.00002497
Iteration 17/1000 | Loss: 0.00002495
Iteration 18/1000 | Loss: 0.00002493
Iteration 19/1000 | Loss: 0.00002492
Iteration 20/1000 | Loss: 0.00002487
Iteration 21/1000 | Loss: 0.00002486
Iteration 22/1000 | Loss: 0.00002485
Iteration 23/1000 | Loss: 0.00002484
Iteration 24/1000 | Loss: 0.00002480
Iteration 25/1000 | Loss: 0.00002479
Iteration 26/1000 | Loss: 0.00002475
Iteration 27/1000 | Loss: 0.00002472
Iteration 28/1000 | Loss: 0.00002472
Iteration 29/1000 | Loss: 0.00002472
Iteration 30/1000 | Loss: 0.00002471
Iteration 31/1000 | Loss: 0.00002471
Iteration 32/1000 | Loss: 0.00002470
Iteration 33/1000 | Loss: 0.00002470
Iteration 34/1000 | Loss: 0.00002469
Iteration 35/1000 | Loss: 0.00002467
Iteration 36/1000 | Loss: 0.00002467
Iteration 37/1000 | Loss: 0.00002467
Iteration 38/1000 | Loss: 0.00002467
Iteration 39/1000 | Loss: 0.00002467
Iteration 40/1000 | Loss: 0.00002467
Iteration 41/1000 | Loss: 0.00002467
Iteration 42/1000 | Loss: 0.00002467
Iteration 43/1000 | Loss: 0.00002467
Iteration 44/1000 | Loss: 0.00002467
Iteration 45/1000 | Loss: 0.00002466
Iteration 46/1000 | Loss: 0.00002466
Iteration 47/1000 | Loss: 0.00002465
Iteration 48/1000 | Loss: 0.00002465
Iteration 49/1000 | Loss: 0.00002464
Iteration 50/1000 | Loss: 0.00002464
Iteration 51/1000 | Loss: 0.00002463
Iteration 52/1000 | Loss: 0.00002463
Iteration 53/1000 | Loss: 0.00002463
Iteration 54/1000 | Loss: 0.00002463
Iteration 55/1000 | Loss: 0.00002462
Iteration 56/1000 | Loss: 0.00002462
Iteration 57/1000 | Loss: 0.00002462
Iteration 58/1000 | Loss: 0.00002462
Iteration 59/1000 | Loss: 0.00002462
Iteration 60/1000 | Loss: 0.00002462
Iteration 61/1000 | Loss: 0.00002461
Iteration 62/1000 | Loss: 0.00002461
Iteration 63/1000 | Loss: 0.00002461
Iteration 64/1000 | Loss: 0.00002461
Iteration 65/1000 | Loss: 0.00002460
Iteration 66/1000 | Loss: 0.00002459
Iteration 67/1000 | Loss: 0.00002459
Iteration 68/1000 | Loss: 0.00002459
Iteration 69/1000 | Loss: 0.00002459
Iteration 70/1000 | Loss: 0.00002459
Iteration 71/1000 | Loss: 0.00002459
Iteration 72/1000 | Loss: 0.00002459
Iteration 73/1000 | Loss: 0.00002459
Iteration 74/1000 | Loss: 0.00002459
Iteration 75/1000 | Loss: 0.00002458
Iteration 76/1000 | Loss: 0.00002458
Iteration 77/1000 | Loss: 0.00002458
Iteration 78/1000 | Loss: 0.00002457
Iteration 79/1000 | Loss: 0.00002457
Iteration 80/1000 | Loss: 0.00002457
Iteration 81/1000 | Loss: 0.00002457
Iteration 82/1000 | Loss: 0.00002456
Iteration 83/1000 | Loss: 0.00002456
Iteration 84/1000 | Loss: 0.00002456
Iteration 85/1000 | Loss: 0.00002456
Iteration 86/1000 | Loss: 0.00002455
Iteration 87/1000 | Loss: 0.00002455
Iteration 88/1000 | Loss: 0.00002455
Iteration 89/1000 | Loss: 0.00002455
Iteration 90/1000 | Loss: 0.00002455
Iteration 91/1000 | Loss: 0.00002455
Iteration 92/1000 | Loss: 0.00002455
Iteration 93/1000 | Loss: 0.00002455
Iteration 94/1000 | Loss: 0.00002455
Iteration 95/1000 | Loss: 0.00002455
Iteration 96/1000 | Loss: 0.00002455
Iteration 97/1000 | Loss: 0.00002455
Iteration 98/1000 | Loss: 0.00002454
Iteration 99/1000 | Loss: 0.00002454
Iteration 100/1000 | Loss: 0.00002454
Iteration 101/1000 | Loss: 0.00002454
Iteration 102/1000 | Loss: 0.00002454
Iteration 103/1000 | Loss: 0.00002454
Iteration 104/1000 | Loss: 0.00002454
Iteration 105/1000 | Loss: 0.00002454
Iteration 106/1000 | Loss: 0.00002454
Iteration 107/1000 | Loss: 0.00002454
Iteration 108/1000 | Loss: 0.00002454
Iteration 109/1000 | Loss: 0.00002454
Iteration 110/1000 | Loss: 0.00002454
Iteration 111/1000 | Loss: 0.00002454
Iteration 112/1000 | Loss: 0.00002453
Iteration 113/1000 | Loss: 0.00002453
Iteration 114/1000 | Loss: 0.00002453
Iteration 115/1000 | Loss: 0.00002453
Iteration 116/1000 | Loss: 0.00002453
Iteration 117/1000 | Loss: 0.00002453
Iteration 118/1000 | Loss: 0.00002453
Iteration 119/1000 | Loss: 0.00002453
Iteration 120/1000 | Loss: 0.00002452
Iteration 121/1000 | Loss: 0.00002452
Iteration 122/1000 | Loss: 0.00002452
Iteration 123/1000 | Loss: 0.00002452
Iteration 124/1000 | Loss: 0.00002452
Iteration 125/1000 | Loss: 0.00002452
Iteration 126/1000 | Loss: 0.00002452
Iteration 127/1000 | Loss: 0.00002452
Iteration 128/1000 | Loss: 0.00002452
Iteration 129/1000 | Loss: 0.00002452
Iteration 130/1000 | Loss: 0.00002452
Iteration 131/1000 | Loss: 0.00002452
Iteration 132/1000 | Loss: 0.00002452
Iteration 133/1000 | Loss: 0.00002452
Iteration 134/1000 | Loss: 0.00002451
Iteration 135/1000 | Loss: 0.00002451
Iteration 136/1000 | Loss: 0.00002451
Iteration 137/1000 | Loss: 0.00002451
Iteration 138/1000 | Loss: 0.00002451
Iteration 139/1000 | Loss: 0.00002451
Iteration 140/1000 | Loss: 0.00002451
Iteration 141/1000 | Loss: 0.00002451
Iteration 142/1000 | Loss: 0.00002451
Iteration 143/1000 | Loss: 0.00002451
Iteration 144/1000 | Loss: 0.00002451
Iteration 145/1000 | Loss: 0.00002451
Iteration 146/1000 | Loss: 0.00002450
Iteration 147/1000 | Loss: 0.00002450
Iteration 148/1000 | Loss: 0.00002450
Iteration 149/1000 | Loss: 0.00002450
Iteration 150/1000 | Loss: 0.00002450
Iteration 151/1000 | Loss: 0.00002450
Iteration 152/1000 | Loss: 0.00002450
Iteration 153/1000 | Loss: 0.00002450
Iteration 154/1000 | Loss: 0.00002450
Iteration 155/1000 | Loss: 0.00002450
Iteration 156/1000 | Loss: 0.00002450
Iteration 157/1000 | Loss: 0.00002450
Iteration 158/1000 | Loss: 0.00002450
Iteration 159/1000 | Loss: 0.00002449
Iteration 160/1000 | Loss: 0.00002449
Iteration 161/1000 | Loss: 0.00002449
Iteration 162/1000 | Loss: 0.00002449
Iteration 163/1000 | Loss: 0.00002449
Iteration 164/1000 | Loss: 0.00002449
Iteration 165/1000 | Loss: 0.00002449
Iteration 166/1000 | Loss: 0.00002449
Iteration 167/1000 | Loss: 0.00002449
Iteration 168/1000 | Loss: 0.00002449
Iteration 169/1000 | Loss: 0.00002449
Iteration 170/1000 | Loss: 0.00002449
Iteration 171/1000 | Loss: 0.00002449
Iteration 172/1000 | Loss: 0.00002449
Iteration 173/1000 | Loss: 0.00002449
Iteration 174/1000 | Loss: 0.00002449
Iteration 175/1000 | Loss: 0.00002449
Iteration 176/1000 | Loss: 0.00002449
Iteration 177/1000 | Loss: 0.00002449
Iteration 178/1000 | Loss: 0.00002449
Iteration 179/1000 | Loss: 0.00002449
Iteration 180/1000 | Loss: 0.00002448
Iteration 181/1000 | Loss: 0.00002448
Iteration 182/1000 | Loss: 0.00002448
Iteration 183/1000 | Loss: 0.00002448
Iteration 184/1000 | Loss: 0.00002448
Iteration 185/1000 | Loss: 0.00002448
Iteration 186/1000 | Loss: 0.00002448
Iteration 187/1000 | Loss: 0.00002448
Iteration 188/1000 | Loss: 0.00002448
Iteration 189/1000 | Loss: 0.00002448
Iteration 190/1000 | Loss: 0.00002448
Iteration 191/1000 | Loss: 0.00002448
Iteration 192/1000 | Loss: 0.00002448
Iteration 193/1000 | Loss: 0.00002448
Iteration 194/1000 | Loss: 0.00002448
Iteration 195/1000 | Loss: 0.00002448
Iteration 196/1000 | Loss: 0.00002448
Iteration 197/1000 | Loss: 0.00002448
Iteration 198/1000 | Loss: 0.00002448
Iteration 199/1000 | Loss: 0.00002448
Iteration 200/1000 | Loss: 0.00002448
Iteration 201/1000 | Loss: 0.00002447
Iteration 202/1000 | Loss: 0.00002447
Iteration 203/1000 | Loss: 0.00002447
Iteration 204/1000 | Loss: 0.00002447
Iteration 205/1000 | Loss: 0.00002447
Iteration 206/1000 | Loss: 0.00002447
Iteration 207/1000 | Loss: 0.00002447
Iteration 208/1000 | Loss: 0.00002447
Iteration 209/1000 | Loss: 0.00002447
Iteration 210/1000 | Loss: 0.00002447
Iteration 211/1000 | Loss: 0.00002447
Iteration 212/1000 | Loss: 0.00002447
Iteration 213/1000 | Loss: 0.00002447
Iteration 214/1000 | Loss: 0.00002447
Iteration 215/1000 | Loss: 0.00002447
Iteration 216/1000 | Loss: 0.00002447
Iteration 217/1000 | Loss: 0.00002447
Iteration 218/1000 | Loss: 0.00002447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [2.4469067284371704e-05, 2.4469067284371704e-05, 2.4469067284371704e-05, 2.4469067284371704e-05, 2.4469067284371704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4469067284371704e-05

Optimization complete. Final v2v error: 4.176159858703613 mm

Highest mean error: 4.859519958496094 mm for frame 155

Lowest mean error: 3.534806489944458 mm for frame 141

Saving results

Total time: 49.83256459236145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00517107
Iteration 2/25 | Loss: 0.00144016
Iteration 3/25 | Loss: 0.00137740
Iteration 4/25 | Loss: 0.00136586
Iteration 5/25 | Loss: 0.00136221
Iteration 6/25 | Loss: 0.00136176
Iteration 7/25 | Loss: 0.00136176
Iteration 8/25 | Loss: 0.00136176
Iteration 9/25 | Loss: 0.00136176
Iteration 10/25 | Loss: 0.00136176
Iteration 11/25 | Loss: 0.00136176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001361758797429502, 0.001361758797429502, 0.001361758797429502, 0.001361758797429502, 0.001361758797429502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001361758797429502

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.17987370
Iteration 2/25 | Loss: 0.00181852
Iteration 3/25 | Loss: 0.00181852
Iteration 4/25 | Loss: 0.00181852
Iteration 5/25 | Loss: 0.00181852
Iteration 6/25 | Loss: 0.00181852
Iteration 7/25 | Loss: 0.00181852
Iteration 8/25 | Loss: 0.00181852
Iteration 9/25 | Loss: 0.00181852
Iteration 10/25 | Loss: 0.00181852
Iteration 11/25 | Loss: 0.00181852
Iteration 12/25 | Loss: 0.00181852
Iteration 13/25 | Loss: 0.00181852
Iteration 14/25 | Loss: 0.00181852
Iteration 15/25 | Loss: 0.00181852
Iteration 16/25 | Loss: 0.00181852
Iteration 17/25 | Loss: 0.00181852
Iteration 18/25 | Loss: 0.00181852
Iteration 19/25 | Loss: 0.00181852
Iteration 20/25 | Loss: 0.00181852
Iteration 21/25 | Loss: 0.00181852
Iteration 22/25 | Loss: 0.00181852
Iteration 23/25 | Loss: 0.00181852
Iteration 24/25 | Loss: 0.00181852
Iteration 25/25 | Loss: 0.00181852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181852
Iteration 2/1000 | Loss: 0.00003324
Iteration 3/1000 | Loss: 0.00002338
Iteration 4/1000 | Loss: 0.00002116
Iteration 5/1000 | Loss: 0.00002005
Iteration 6/1000 | Loss: 0.00001881
Iteration 7/1000 | Loss: 0.00001811
Iteration 8/1000 | Loss: 0.00001755
Iteration 9/1000 | Loss: 0.00001715
Iteration 10/1000 | Loss: 0.00001688
Iteration 11/1000 | Loss: 0.00001688
Iteration 12/1000 | Loss: 0.00001678
Iteration 13/1000 | Loss: 0.00001676
Iteration 14/1000 | Loss: 0.00001675
Iteration 15/1000 | Loss: 0.00001667
Iteration 16/1000 | Loss: 0.00001663
Iteration 17/1000 | Loss: 0.00001654
Iteration 18/1000 | Loss: 0.00001654
Iteration 19/1000 | Loss: 0.00001653
Iteration 20/1000 | Loss: 0.00001651
Iteration 21/1000 | Loss: 0.00001645
Iteration 22/1000 | Loss: 0.00001645
Iteration 23/1000 | Loss: 0.00001644
Iteration 24/1000 | Loss: 0.00001644
Iteration 25/1000 | Loss: 0.00001640
Iteration 26/1000 | Loss: 0.00001640
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001639
Iteration 30/1000 | Loss: 0.00001639
Iteration 31/1000 | Loss: 0.00001638
Iteration 32/1000 | Loss: 0.00001637
Iteration 33/1000 | Loss: 0.00001635
Iteration 34/1000 | Loss: 0.00001634
Iteration 35/1000 | Loss: 0.00001634
Iteration 36/1000 | Loss: 0.00001633
Iteration 37/1000 | Loss: 0.00001633
Iteration 38/1000 | Loss: 0.00001633
Iteration 39/1000 | Loss: 0.00001633
Iteration 40/1000 | Loss: 0.00001633
Iteration 41/1000 | Loss: 0.00001632
Iteration 42/1000 | Loss: 0.00001632
Iteration 43/1000 | Loss: 0.00001632
Iteration 44/1000 | Loss: 0.00001632
Iteration 45/1000 | Loss: 0.00001632
Iteration 46/1000 | Loss: 0.00001631
Iteration 47/1000 | Loss: 0.00001631
Iteration 48/1000 | Loss: 0.00001630
Iteration 49/1000 | Loss: 0.00001630
Iteration 50/1000 | Loss: 0.00001630
Iteration 51/1000 | Loss: 0.00001630
Iteration 52/1000 | Loss: 0.00001629
Iteration 53/1000 | Loss: 0.00001629
Iteration 54/1000 | Loss: 0.00001629
Iteration 55/1000 | Loss: 0.00001629
Iteration 56/1000 | Loss: 0.00001629
Iteration 57/1000 | Loss: 0.00001629
Iteration 58/1000 | Loss: 0.00001628
Iteration 59/1000 | Loss: 0.00001628
Iteration 60/1000 | Loss: 0.00001628
Iteration 61/1000 | Loss: 0.00001628
Iteration 62/1000 | Loss: 0.00001628
Iteration 63/1000 | Loss: 0.00001628
Iteration 64/1000 | Loss: 0.00001628
Iteration 65/1000 | Loss: 0.00001628
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001627
Iteration 68/1000 | Loss: 0.00001627
Iteration 69/1000 | Loss: 0.00001627
Iteration 70/1000 | Loss: 0.00001627
Iteration 71/1000 | Loss: 0.00001626
Iteration 72/1000 | Loss: 0.00001626
Iteration 73/1000 | Loss: 0.00001626
Iteration 74/1000 | Loss: 0.00001626
Iteration 75/1000 | Loss: 0.00001626
Iteration 76/1000 | Loss: 0.00001626
Iteration 77/1000 | Loss: 0.00001626
Iteration 78/1000 | Loss: 0.00001626
Iteration 79/1000 | Loss: 0.00001626
Iteration 80/1000 | Loss: 0.00001626
Iteration 81/1000 | Loss: 0.00001625
Iteration 82/1000 | Loss: 0.00001625
Iteration 83/1000 | Loss: 0.00001625
Iteration 84/1000 | Loss: 0.00001625
Iteration 85/1000 | Loss: 0.00001625
Iteration 86/1000 | Loss: 0.00001625
Iteration 87/1000 | Loss: 0.00001625
Iteration 88/1000 | Loss: 0.00001625
Iteration 89/1000 | Loss: 0.00001625
Iteration 90/1000 | Loss: 0.00001624
Iteration 91/1000 | Loss: 0.00001624
Iteration 92/1000 | Loss: 0.00001624
Iteration 93/1000 | Loss: 0.00001624
Iteration 94/1000 | Loss: 0.00001624
Iteration 95/1000 | Loss: 0.00001624
Iteration 96/1000 | Loss: 0.00001624
Iteration 97/1000 | Loss: 0.00001624
Iteration 98/1000 | Loss: 0.00001624
Iteration 99/1000 | Loss: 0.00001624
Iteration 100/1000 | Loss: 0.00001624
Iteration 101/1000 | Loss: 0.00001624
Iteration 102/1000 | Loss: 0.00001624
Iteration 103/1000 | Loss: 0.00001623
Iteration 104/1000 | Loss: 0.00001623
Iteration 105/1000 | Loss: 0.00001623
Iteration 106/1000 | Loss: 0.00001623
Iteration 107/1000 | Loss: 0.00001623
Iteration 108/1000 | Loss: 0.00001623
Iteration 109/1000 | Loss: 0.00001623
Iteration 110/1000 | Loss: 0.00001622
Iteration 111/1000 | Loss: 0.00001622
Iteration 112/1000 | Loss: 0.00001622
Iteration 113/1000 | Loss: 0.00001622
Iteration 114/1000 | Loss: 0.00001621
Iteration 115/1000 | Loss: 0.00001621
Iteration 116/1000 | Loss: 0.00001621
Iteration 117/1000 | Loss: 0.00001621
Iteration 118/1000 | Loss: 0.00001621
Iteration 119/1000 | Loss: 0.00001621
Iteration 120/1000 | Loss: 0.00001620
Iteration 121/1000 | Loss: 0.00001620
Iteration 122/1000 | Loss: 0.00001620
Iteration 123/1000 | Loss: 0.00001620
Iteration 124/1000 | Loss: 0.00001620
Iteration 125/1000 | Loss: 0.00001620
Iteration 126/1000 | Loss: 0.00001620
Iteration 127/1000 | Loss: 0.00001620
Iteration 128/1000 | Loss: 0.00001619
Iteration 129/1000 | Loss: 0.00001619
Iteration 130/1000 | Loss: 0.00001619
Iteration 131/1000 | Loss: 0.00001619
Iteration 132/1000 | Loss: 0.00001619
Iteration 133/1000 | Loss: 0.00001619
Iteration 134/1000 | Loss: 0.00001619
Iteration 135/1000 | Loss: 0.00001619
Iteration 136/1000 | Loss: 0.00001619
Iteration 137/1000 | Loss: 0.00001619
Iteration 138/1000 | Loss: 0.00001619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.618630085431505e-05, 1.618630085431505e-05, 1.618630085431505e-05, 1.618630085431505e-05, 1.618630085431505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.618630085431505e-05

Optimization complete. Final v2v error: 3.4156601428985596 mm

Highest mean error: 3.598874092102051 mm for frame 118

Lowest mean error: 3.2044193744659424 mm for frame 4

Saving results

Total time: 38.60851192474365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900347
Iteration 2/25 | Loss: 0.00256024
Iteration 3/25 | Loss: 0.00203560
Iteration 4/25 | Loss: 0.00195559
Iteration 5/25 | Loss: 0.00184642
Iteration 6/25 | Loss: 0.00162153
Iteration 7/25 | Loss: 0.00152038
Iteration 8/25 | Loss: 0.00149427
Iteration 9/25 | Loss: 0.00148723
Iteration 10/25 | Loss: 0.00148660
Iteration 11/25 | Loss: 0.00148647
Iteration 12/25 | Loss: 0.00148647
Iteration 13/25 | Loss: 0.00148647
Iteration 14/25 | Loss: 0.00148647
Iteration 15/25 | Loss: 0.00148647
Iteration 16/25 | Loss: 0.00148647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014864654513075948, 0.0014864654513075948, 0.0014864654513075948, 0.0014864654513075948, 0.0014864654513075948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014864654513075948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17222333
Iteration 2/25 | Loss: 0.00152034
Iteration 3/25 | Loss: 0.00152034
Iteration 4/25 | Loss: 0.00152033
Iteration 5/25 | Loss: 0.00152033
Iteration 6/25 | Loss: 0.00152033
Iteration 7/25 | Loss: 0.00152033
Iteration 8/25 | Loss: 0.00152033
Iteration 9/25 | Loss: 0.00152033
Iteration 10/25 | Loss: 0.00152033
Iteration 11/25 | Loss: 0.00152033
Iteration 12/25 | Loss: 0.00152033
Iteration 13/25 | Loss: 0.00152033
Iteration 14/25 | Loss: 0.00152033
Iteration 15/25 | Loss: 0.00152033
Iteration 16/25 | Loss: 0.00152033
Iteration 17/25 | Loss: 0.00152033
Iteration 18/25 | Loss: 0.00152033
Iteration 19/25 | Loss: 0.00152033
Iteration 20/25 | Loss: 0.00152033
Iteration 21/25 | Loss: 0.00152033
Iteration 22/25 | Loss: 0.00152033
Iteration 23/25 | Loss: 0.00152033
Iteration 24/25 | Loss: 0.00152033
Iteration 25/25 | Loss: 0.00152033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152033
Iteration 2/1000 | Loss: 0.00004862
Iteration 3/1000 | Loss: 0.00003038
Iteration 4/1000 | Loss: 0.00002446
Iteration 5/1000 | Loss: 0.00002327
Iteration 6/1000 | Loss: 0.00002248
Iteration 7/1000 | Loss: 0.00002166
Iteration 8/1000 | Loss: 0.00002101
Iteration 9/1000 | Loss: 0.00002056
Iteration 10/1000 | Loss: 0.00002018
Iteration 11/1000 | Loss: 0.00002006
Iteration 12/1000 | Loss: 0.00002005
Iteration 13/1000 | Loss: 0.00001999
Iteration 14/1000 | Loss: 0.00001992
Iteration 15/1000 | Loss: 0.00001992
Iteration 16/1000 | Loss: 0.00001991
Iteration 17/1000 | Loss: 0.00001980
Iteration 18/1000 | Loss: 0.00001978
Iteration 19/1000 | Loss: 0.00001977
Iteration 20/1000 | Loss: 0.00001976
Iteration 21/1000 | Loss: 0.00001975
Iteration 22/1000 | Loss: 0.00001974
Iteration 23/1000 | Loss: 0.00001974
Iteration 24/1000 | Loss: 0.00001974
Iteration 25/1000 | Loss: 0.00001973
Iteration 26/1000 | Loss: 0.00001973
Iteration 27/1000 | Loss: 0.00001972
Iteration 28/1000 | Loss: 0.00001972
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001972
Iteration 32/1000 | Loss: 0.00001971
Iteration 33/1000 | Loss: 0.00001971
Iteration 34/1000 | Loss: 0.00001971
Iteration 35/1000 | Loss: 0.00001970
Iteration 36/1000 | Loss: 0.00001970
Iteration 37/1000 | Loss: 0.00001969
Iteration 38/1000 | Loss: 0.00001969
Iteration 39/1000 | Loss: 0.00001969
Iteration 40/1000 | Loss: 0.00001969
Iteration 41/1000 | Loss: 0.00001969
Iteration 42/1000 | Loss: 0.00001968
Iteration 43/1000 | Loss: 0.00001968
Iteration 44/1000 | Loss: 0.00001968
Iteration 45/1000 | Loss: 0.00001968
Iteration 46/1000 | Loss: 0.00001968
Iteration 47/1000 | Loss: 0.00001967
Iteration 48/1000 | Loss: 0.00001967
Iteration 49/1000 | Loss: 0.00001967
Iteration 50/1000 | Loss: 0.00001967
Iteration 51/1000 | Loss: 0.00001967
Iteration 52/1000 | Loss: 0.00001967
Iteration 53/1000 | Loss: 0.00001967
Iteration 54/1000 | Loss: 0.00001967
Iteration 55/1000 | Loss: 0.00001967
Iteration 56/1000 | Loss: 0.00001967
Iteration 57/1000 | Loss: 0.00001967
Iteration 58/1000 | Loss: 0.00001966
Iteration 59/1000 | Loss: 0.00001966
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001966
Iteration 62/1000 | Loss: 0.00001965
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001965
Iteration 65/1000 | Loss: 0.00001965
Iteration 66/1000 | Loss: 0.00001965
Iteration 67/1000 | Loss: 0.00001964
Iteration 68/1000 | Loss: 0.00001964
Iteration 69/1000 | Loss: 0.00001963
Iteration 70/1000 | Loss: 0.00001963
Iteration 71/1000 | Loss: 0.00001963
Iteration 72/1000 | Loss: 0.00001963
Iteration 73/1000 | Loss: 0.00001963
Iteration 74/1000 | Loss: 0.00001962
Iteration 75/1000 | Loss: 0.00001962
Iteration 76/1000 | Loss: 0.00001962
Iteration 77/1000 | Loss: 0.00001962
Iteration 78/1000 | Loss: 0.00001962
Iteration 79/1000 | Loss: 0.00001962
Iteration 80/1000 | Loss: 0.00001962
Iteration 81/1000 | Loss: 0.00001962
Iteration 82/1000 | Loss: 0.00001962
Iteration 83/1000 | Loss: 0.00001962
Iteration 84/1000 | Loss: 0.00001962
Iteration 85/1000 | Loss: 0.00001962
Iteration 86/1000 | Loss: 0.00001961
Iteration 87/1000 | Loss: 0.00001961
Iteration 88/1000 | Loss: 0.00001961
Iteration 89/1000 | Loss: 0.00001961
Iteration 90/1000 | Loss: 0.00001961
Iteration 91/1000 | Loss: 0.00001961
Iteration 92/1000 | Loss: 0.00001960
Iteration 93/1000 | Loss: 0.00001960
Iteration 94/1000 | Loss: 0.00001960
Iteration 95/1000 | Loss: 0.00001960
Iteration 96/1000 | Loss: 0.00001960
Iteration 97/1000 | Loss: 0.00001960
Iteration 98/1000 | Loss: 0.00001960
Iteration 99/1000 | Loss: 0.00001960
Iteration 100/1000 | Loss: 0.00001959
Iteration 101/1000 | Loss: 0.00001959
Iteration 102/1000 | Loss: 0.00001959
Iteration 103/1000 | Loss: 0.00001958
Iteration 104/1000 | Loss: 0.00001958
Iteration 105/1000 | Loss: 0.00001958
Iteration 106/1000 | Loss: 0.00001957
Iteration 107/1000 | Loss: 0.00001957
Iteration 108/1000 | Loss: 0.00001957
Iteration 109/1000 | Loss: 0.00001957
Iteration 110/1000 | Loss: 0.00001957
Iteration 111/1000 | Loss: 0.00001956
Iteration 112/1000 | Loss: 0.00001956
Iteration 113/1000 | Loss: 0.00001956
Iteration 114/1000 | Loss: 0.00001956
Iteration 115/1000 | Loss: 0.00001956
Iteration 116/1000 | Loss: 0.00001956
Iteration 117/1000 | Loss: 0.00001956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.956388405233156e-05, 1.956388405233156e-05, 1.956388405233156e-05, 1.956388405233156e-05, 1.956388405233156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.956388405233156e-05

Optimization complete. Final v2v error: 3.8209903240203857 mm

Highest mean error: 4.092112064361572 mm for frame 220

Lowest mean error: 3.5950231552124023 mm for frame 58

Saving results

Total time: 47.09612035751343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088711
Iteration 2/25 | Loss: 0.00239220
Iteration 3/25 | Loss: 0.00184729
Iteration 4/25 | Loss: 0.00175989
Iteration 5/25 | Loss: 0.00166686
Iteration 6/25 | Loss: 0.00153207
Iteration 7/25 | Loss: 0.00147433
Iteration 8/25 | Loss: 0.00145398
Iteration 9/25 | Loss: 0.00144668
Iteration 10/25 | Loss: 0.00144328
Iteration 11/25 | Loss: 0.00143305
Iteration 12/25 | Loss: 0.00142492
Iteration 13/25 | Loss: 0.00142240
Iteration 14/25 | Loss: 0.00142117
Iteration 15/25 | Loss: 0.00142068
Iteration 16/25 | Loss: 0.00142005
Iteration 17/25 | Loss: 0.00141953
Iteration 18/25 | Loss: 0.00141896
Iteration 19/25 | Loss: 0.00142356
Iteration 20/25 | Loss: 0.00142597
Iteration 21/25 | Loss: 0.00141638
Iteration 22/25 | Loss: 0.00141436
Iteration 23/25 | Loss: 0.00141382
Iteration 24/25 | Loss: 0.00141351
Iteration 25/25 | Loss: 0.00141335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16260040
Iteration 2/25 | Loss: 0.00187143
Iteration 3/25 | Loss: 0.00187143
Iteration 4/25 | Loss: 0.00187143
Iteration 5/25 | Loss: 0.00187143
Iteration 6/25 | Loss: 0.00187143
Iteration 7/25 | Loss: 0.00187143
Iteration 8/25 | Loss: 0.00187143
Iteration 9/25 | Loss: 0.00187143
Iteration 10/25 | Loss: 0.00187143
Iteration 11/25 | Loss: 0.00187143
Iteration 12/25 | Loss: 0.00187143
Iteration 13/25 | Loss: 0.00187143
Iteration 14/25 | Loss: 0.00187143
Iteration 15/25 | Loss: 0.00187143
Iteration 16/25 | Loss: 0.00187143
Iteration 17/25 | Loss: 0.00187143
Iteration 18/25 | Loss: 0.00187143
Iteration 19/25 | Loss: 0.00187143
Iteration 20/25 | Loss: 0.00187143
Iteration 21/25 | Loss: 0.00187143
Iteration 22/25 | Loss: 0.00187143
Iteration 23/25 | Loss: 0.00187143
Iteration 24/25 | Loss: 0.00187143
Iteration 25/25 | Loss: 0.00187143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187143
Iteration 2/1000 | Loss: 0.00005786
Iteration 3/1000 | Loss: 0.00003762
Iteration 4/1000 | Loss: 0.00003431
Iteration 5/1000 | Loss: 0.00003299
Iteration 6/1000 | Loss: 0.00003213
Iteration 7/1000 | Loss: 0.00003163
Iteration 8/1000 | Loss: 0.00003127
Iteration 9/1000 | Loss: 0.00003097
Iteration 10/1000 | Loss: 0.00003073
Iteration 11/1000 | Loss: 0.00003052
Iteration 12/1000 | Loss: 0.00003038
Iteration 13/1000 | Loss: 0.00003036
Iteration 14/1000 | Loss: 0.00003035
Iteration 15/1000 | Loss: 0.00003033
Iteration 16/1000 | Loss: 0.00003033
Iteration 17/1000 | Loss: 0.00003033
Iteration 18/1000 | Loss: 0.00003032
Iteration 19/1000 | Loss: 0.00003031
Iteration 20/1000 | Loss: 0.00003030
Iteration 21/1000 | Loss: 0.00003029
Iteration 22/1000 | Loss: 0.00003029
Iteration 23/1000 | Loss: 0.00003026
Iteration 24/1000 | Loss: 0.00003026
Iteration 25/1000 | Loss: 0.00003025
Iteration 26/1000 | Loss: 0.00003024
Iteration 27/1000 | Loss: 0.00003024
Iteration 28/1000 | Loss: 0.00003024
Iteration 29/1000 | Loss: 0.00003024
Iteration 30/1000 | Loss: 0.00003023
Iteration 31/1000 | Loss: 0.00003023
Iteration 32/1000 | Loss: 0.00003023
Iteration 33/1000 | Loss: 0.00003023
Iteration 34/1000 | Loss: 0.00003023
Iteration 35/1000 | Loss: 0.00003023
Iteration 36/1000 | Loss: 0.00003023
Iteration 37/1000 | Loss: 0.00003023
Iteration 38/1000 | Loss: 0.00003023
Iteration 39/1000 | Loss: 0.00003022
Iteration 40/1000 | Loss: 0.00003021
Iteration 41/1000 | Loss: 0.00003021
Iteration 42/1000 | Loss: 0.00003021
Iteration 43/1000 | Loss: 0.00003021
Iteration 44/1000 | Loss: 0.00003020
Iteration 45/1000 | Loss: 0.00003020
Iteration 46/1000 | Loss: 0.00003020
Iteration 47/1000 | Loss: 0.00003019
Iteration 48/1000 | Loss: 0.00003019
Iteration 49/1000 | Loss: 0.00003018
Iteration 50/1000 | Loss: 0.00003018
Iteration 51/1000 | Loss: 0.00003018
Iteration 52/1000 | Loss: 0.00003018
Iteration 53/1000 | Loss: 0.00003018
Iteration 54/1000 | Loss: 0.00003018
Iteration 55/1000 | Loss: 0.00003017
Iteration 56/1000 | Loss: 0.00003017
Iteration 57/1000 | Loss: 0.00003017
Iteration 58/1000 | Loss: 0.00003017
Iteration 59/1000 | Loss: 0.00003017
Iteration 60/1000 | Loss: 0.00003017
Iteration 61/1000 | Loss: 0.00003016
Iteration 62/1000 | Loss: 0.00003016
Iteration 63/1000 | Loss: 0.00003016
Iteration 64/1000 | Loss: 0.00003016
Iteration 65/1000 | Loss: 0.00003016
Iteration 66/1000 | Loss: 0.00003016
Iteration 67/1000 | Loss: 0.00003015
Iteration 68/1000 | Loss: 0.00003015
Iteration 69/1000 | Loss: 0.00003015
Iteration 70/1000 | Loss: 0.00003015
Iteration 71/1000 | Loss: 0.00003015
Iteration 72/1000 | Loss: 0.00003015
Iteration 73/1000 | Loss: 0.00003015
Iteration 74/1000 | Loss: 0.00003015
Iteration 75/1000 | Loss: 0.00003015
Iteration 76/1000 | Loss: 0.00003015
Iteration 77/1000 | Loss: 0.00003015
Iteration 78/1000 | Loss: 0.00003015
Iteration 79/1000 | Loss: 0.00003015
Iteration 80/1000 | Loss: 0.00003015
Iteration 81/1000 | Loss: 0.00003014
Iteration 82/1000 | Loss: 0.00003014
Iteration 83/1000 | Loss: 0.00003014
Iteration 84/1000 | Loss: 0.00003014
Iteration 85/1000 | Loss: 0.00003014
Iteration 86/1000 | Loss: 0.00003014
Iteration 87/1000 | Loss: 0.00003014
Iteration 88/1000 | Loss: 0.00003014
Iteration 89/1000 | Loss: 0.00003014
Iteration 90/1000 | Loss: 0.00003014
Iteration 91/1000 | Loss: 0.00003013
Iteration 92/1000 | Loss: 0.00003013
Iteration 93/1000 | Loss: 0.00003013
Iteration 94/1000 | Loss: 0.00003013
Iteration 95/1000 | Loss: 0.00003013
Iteration 96/1000 | Loss: 0.00003013
Iteration 97/1000 | Loss: 0.00003013
Iteration 98/1000 | Loss: 0.00003013
Iteration 99/1000 | Loss: 0.00003013
Iteration 100/1000 | Loss: 0.00003013
Iteration 101/1000 | Loss: 0.00003013
Iteration 102/1000 | Loss: 0.00003013
Iteration 103/1000 | Loss: 0.00003012
Iteration 104/1000 | Loss: 0.00003012
Iteration 105/1000 | Loss: 0.00003012
Iteration 106/1000 | Loss: 0.00003012
Iteration 107/1000 | Loss: 0.00003012
Iteration 108/1000 | Loss: 0.00003012
Iteration 109/1000 | Loss: 0.00003012
Iteration 110/1000 | Loss: 0.00003012
Iteration 111/1000 | Loss: 0.00003012
Iteration 112/1000 | Loss: 0.00003012
Iteration 113/1000 | Loss: 0.00003012
Iteration 114/1000 | Loss: 0.00003012
Iteration 115/1000 | Loss: 0.00003012
Iteration 116/1000 | Loss: 0.00003012
Iteration 117/1000 | Loss: 0.00003011
Iteration 118/1000 | Loss: 0.00003011
Iteration 119/1000 | Loss: 0.00003011
Iteration 120/1000 | Loss: 0.00003011
Iteration 121/1000 | Loss: 0.00003011
Iteration 122/1000 | Loss: 0.00003011
Iteration 123/1000 | Loss: 0.00003011
Iteration 124/1000 | Loss: 0.00003011
Iteration 125/1000 | Loss: 0.00003011
Iteration 126/1000 | Loss: 0.00003011
Iteration 127/1000 | Loss: 0.00003011
Iteration 128/1000 | Loss: 0.00003011
Iteration 129/1000 | Loss: 0.00003011
Iteration 130/1000 | Loss: 0.00003011
Iteration 131/1000 | Loss: 0.00003011
Iteration 132/1000 | Loss: 0.00003011
Iteration 133/1000 | Loss: 0.00003011
Iteration 134/1000 | Loss: 0.00003011
Iteration 135/1000 | Loss: 0.00003011
Iteration 136/1000 | Loss: 0.00003011
Iteration 137/1000 | Loss: 0.00003011
Iteration 138/1000 | Loss: 0.00003011
Iteration 139/1000 | Loss: 0.00003011
Iteration 140/1000 | Loss: 0.00003011
Iteration 141/1000 | Loss: 0.00003011
Iteration 142/1000 | Loss: 0.00003011
Iteration 143/1000 | Loss: 0.00003011
Iteration 144/1000 | Loss: 0.00003011
Iteration 145/1000 | Loss: 0.00003011
Iteration 146/1000 | Loss: 0.00003011
Iteration 147/1000 | Loss: 0.00003011
Iteration 148/1000 | Loss: 0.00003011
Iteration 149/1000 | Loss: 0.00003011
Iteration 150/1000 | Loss: 0.00003011
Iteration 151/1000 | Loss: 0.00003011
Iteration 152/1000 | Loss: 0.00003011
Iteration 153/1000 | Loss: 0.00003011
Iteration 154/1000 | Loss: 0.00003011
Iteration 155/1000 | Loss: 0.00003011
Iteration 156/1000 | Loss: 0.00003011
Iteration 157/1000 | Loss: 0.00003011
Iteration 158/1000 | Loss: 0.00003011
Iteration 159/1000 | Loss: 0.00003011
Iteration 160/1000 | Loss: 0.00003011
Iteration 161/1000 | Loss: 0.00003011
Iteration 162/1000 | Loss: 0.00003011
Iteration 163/1000 | Loss: 0.00003011
Iteration 164/1000 | Loss: 0.00003011
Iteration 165/1000 | Loss: 0.00003011
Iteration 166/1000 | Loss: 0.00003011
Iteration 167/1000 | Loss: 0.00003011
Iteration 168/1000 | Loss: 0.00003011
Iteration 169/1000 | Loss: 0.00003011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [3.0112934837234206e-05, 3.0112934837234206e-05, 3.0112934837234206e-05, 3.0112934837234206e-05, 3.0112934837234206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0112934837234206e-05

Optimization complete. Final v2v error: 3.9935033321380615 mm

Highest mean error: 10.59740924835205 mm for frame 40

Lowest mean error: 3.09281063079834 mm for frame 0

Saving results

Total time: 68.6436448097229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432046
Iteration 2/25 | Loss: 0.00152430
Iteration 3/25 | Loss: 0.00141976
Iteration 4/25 | Loss: 0.00140387
Iteration 5/25 | Loss: 0.00139947
Iteration 6/25 | Loss: 0.00139843
Iteration 7/25 | Loss: 0.00139843
Iteration 8/25 | Loss: 0.00139843
Iteration 9/25 | Loss: 0.00139843
Iteration 10/25 | Loss: 0.00139843
Iteration 11/25 | Loss: 0.00139843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001398427877575159, 0.001398427877575159, 0.001398427877575159, 0.001398427877575159, 0.001398427877575159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001398427877575159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84219372
Iteration 2/25 | Loss: 0.00177022
Iteration 3/25 | Loss: 0.00177022
Iteration 4/25 | Loss: 0.00177022
Iteration 5/25 | Loss: 0.00177022
Iteration 6/25 | Loss: 0.00177022
Iteration 7/25 | Loss: 0.00177022
Iteration 8/25 | Loss: 0.00177021
Iteration 9/25 | Loss: 0.00177021
Iteration 10/25 | Loss: 0.00177021
Iteration 11/25 | Loss: 0.00177021
Iteration 12/25 | Loss: 0.00177021
Iteration 13/25 | Loss: 0.00177021
Iteration 14/25 | Loss: 0.00177021
Iteration 15/25 | Loss: 0.00177021
Iteration 16/25 | Loss: 0.00177021
Iteration 17/25 | Loss: 0.00177021
Iteration 18/25 | Loss: 0.00177021
Iteration 19/25 | Loss: 0.00177021
Iteration 20/25 | Loss: 0.00177021
Iteration 21/25 | Loss: 0.00177021
Iteration 22/25 | Loss: 0.00177021
Iteration 23/25 | Loss: 0.00177021
Iteration 24/25 | Loss: 0.00177021
Iteration 25/25 | Loss: 0.00177021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177021
Iteration 2/1000 | Loss: 0.00004791
Iteration 3/1000 | Loss: 0.00003108
Iteration 4/1000 | Loss: 0.00002872
Iteration 5/1000 | Loss: 0.00002716
Iteration 6/1000 | Loss: 0.00002584
Iteration 7/1000 | Loss: 0.00002500
Iteration 8/1000 | Loss: 0.00002438
Iteration 9/1000 | Loss: 0.00002405
Iteration 10/1000 | Loss: 0.00002378
Iteration 11/1000 | Loss: 0.00002362
Iteration 12/1000 | Loss: 0.00002357
Iteration 13/1000 | Loss: 0.00002357
Iteration 14/1000 | Loss: 0.00002355
Iteration 15/1000 | Loss: 0.00002345
Iteration 16/1000 | Loss: 0.00002339
Iteration 17/1000 | Loss: 0.00002337
Iteration 18/1000 | Loss: 0.00002333
Iteration 19/1000 | Loss: 0.00002333
Iteration 20/1000 | Loss: 0.00002332
Iteration 21/1000 | Loss: 0.00002331
Iteration 22/1000 | Loss: 0.00002331
Iteration 23/1000 | Loss: 0.00002330
Iteration 24/1000 | Loss: 0.00002330
Iteration 25/1000 | Loss: 0.00002330
Iteration 26/1000 | Loss: 0.00002329
Iteration 27/1000 | Loss: 0.00002329
Iteration 28/1000 | Loss: 0.00002328
Iteration 29/1000 | Loss: 0.00002328
Iteration 30/1000 | Loss: 0.00002328
Iteration 31/1000 | Loss: 0.00002328
Iteration 32/1000 | Loss: 0.00002328
Iteration 33/1000 | Loss: 0.00002328
Iteration 34/1000 | Loss: 0.00002327
Iteration 35/1000 | Loss: 0.00002327
Iteration 36/1000 | Loss: 0.00002327
Iteration 37/1000 | Loss: 0.00002327
Iteration 38/1000 | Loss: 0.00002327
Iteration 39/1000 | Loss: 0.00002327
Iteration 40/1000 | Loss: 0.00002327
Iteration 41/1000 | Loss: 0.00002327
Iteration 42/1000 | Loss: 0.00002327
Iteration 43/1000 | Loss: 0.00002327
Iteration 44/1000 | Loss: 0.00002327
Iteration 45/1000 | Loss: 0.00002327
Iteration 46/1000 | Loss: 0.00002327
Iteration 47/1000 | Loss: 0.00002327
Iteration 48/1000 | Loss: 0.00002327
Iteration 49/1000 | Loss: 0.00002327
Iteration 50/1000 | Loss: 0.00002327
Iteration 51/1000 | Loss: 0.00002326
Iteration 52/1000 | Loss: 0.00002326
Iteration 53/1000 | Loss: 0.00002326
Iteration 54/1000 | Loss: 0.00002326
Iteration 55/1000 | Loss: 0.00002326
Iteration 56/1000 | Loss: 0.00002326
Iteration 57/1000 | Loss: 0.00002326
Iteration 58/1000 | Loss: 0.00002326
Iteration 59/1000 | Loss: 0.00002326
Iteration 60/1000 | Loss: 0.00002326
Iteration 61/1000 | Loss: 0.00002326
Iteration 62/1000 | Loss: 0.00002326
Iteration 63/1000 | Loss: 0.00002326
Iteration 64/1000 | Loss: 0.00002326
Iteration 65/1000 | Loss: 0.00002326
Iteration 66/1000 | Loss: 0.00002326
Iteration 67/1000 | Loss: 0.00002326
Iteration 68/1000 | Loss: 0.00002326
Iteration 69/1000 | Loss: 0.00002326
Iteration 70/1000 | Loss: 0.00002326
Iteration 71/1000 | Loss: 0.00002326
Iteration 72/1000 | Loss: 0.00002326
Iteration 73/1000 | Loss: 0.00002326
Iteration 74/1000 | Loss: 0.00002326
Iteration 75/1000 | Loss: 0.00002326
Iteration 76/1000 | Loss: 0.00002326
Iteration 77/1000 | Loss: 0.00002326
Iteration 78/1000 | Loss: 0.00002326
Iteration 79/1000 | Loss: 0.00002326
Iteration 80/1000 | Loss: 0.00002326
Iteration 81/1000 | Loss: 0.00002326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.3255006453837268e-05, 2.3255006453837268e-05, 2.3255006453837268e-05, 2.3255006453837268e-05, 2.3255006453837268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3255006453837268e-05

Optimization complete. Final v2v error: 4.045679092407227 mm

Highest mean error: 4.418399333953857 mm for frame 52

Lowest mean error: 3.777601957321167 mm for frame 1

Saving results

Total time: 30.282744646072388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00584426
Iteration 2/25 | Loss: 0.00161610
Iteration 3/25 | Loss: 0.00143199
Iteration 4/25 | Loss: 0.00141918
Iteration 5/25 | Loss: 0.00141703
Iteration 6/25 | Loss: 0.00141680
Iteration 7/25 | Loss: 0.00141680
Iteration 8/25 | Loss: 0.00141680
Iteration 9/25 | Loss: 0.00141680
Iteration 10/25 | Loss: 0.00141680
Iteration 11/25 | Loss: 0.00141680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014168048510327935, 0.0014168048510327935, 0.0014168048510327935, 0.0014168048510327935, 0.0014168048510327935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014168048510327935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.32455945
Iteration 2/25 | Loss: 0.00131812
Iteration 3/25 | Loss: 0.00131804
Iteration 4/25 | Loss: 0.00131803
Iteration 5/25 | Loss: 0.00131803
Iteration 6/25 | Loss: 0.00131803
Iteration 7/25 | Loss: 0.00131803
Iteration 8/25 | Loss: 0.00131803
Iteration 9/25 | Loss: 0.00131803
Iteration 10/25 | Loss: 0.00131803
Iteration 11/25 | Loss: 0.00131803
Iteration 12/25 | Loss: 0.00131803
Iteration 13/25 | Loss: 0.00131803
Iteration 14/25 | Loss: 0.00131803
Iteration 15/25 | Loss: 0.00131803
Iteration 16/25 | Loss: 0.00131803
Iteration 17/25 | Loss: 0.00131803
Iteration 18/25 | Loss: 0.00131803
Iteration 19/25 | Loss: 0.00131803
Iteration 20/25 | Loss: 0.00131803
Iteration 21/25 | Loss: 0.00131803
Iteration 22/25 | Loss: 0.00131803
Iteration 23/25 | Loss: 0.00131803
Iteration 24/25 | Loss: 0.00131803
Iteration 25/25 | Loss: 0.00131803

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131803
Iteration 2/1000 | Loss: 0.00006832
Iteration 3/1000 | Loss: 0.00003640
Iteration 4/1000 | Loss: 0.00002743
Iteration 5/1000 | Loss: 0.00002484
Iteration 6/1000 | Loss: 0.00002357
Iteration 7/1000 | Loss: 0.00002278
Iteration 8/1000 | Loss: 0.00002217
Iteration 9/1000 | Loss: 0.00002172
Iteration 10/1000 | Loss: 0.00002140
Iteration 11/1000 | Loss: 0.00002112
Iteration 12/1000 | Loss: 0.00002089
Iteration 13/1000 | Loss: 0.00002075
Iteration 14/1000 | Loss: 0.00002074
Iteration 15/1000 | Loss: 0.00002070
Iteration 16/1000 | Loss: 0.00002065
Iteration 17/1000 | Loss: 0.00002056
Iteration 18/1000 | Loss: 0.00002055
Iteration 19/1000 | Loss: 0.00002052
Iteration 20/1000 | Loss: 0.00002051
Iteration 21/1000 | Loss: 0.00002050
Iteration 22/1000 | Loss: 0.00002049
Iteration 23/1000 | Loss: 0.00002048
Iteration 24/1000 | Loss: 0.00002048
Iteration 25/1000 | Loss: 0.00002047
Iteration 26/1000 | Loss: 0.00002047
Iteration 27/1000 | Loss: 0.00002046
Iteration 28/1000 | Loss: 0.00002046
Iteration 29/1000 | Loss: 0.00002046
Iteration 30/1000 | Loss: 0.00002045
Iteration 31/1000 | Loss: 0.00002045
Iteration 32/1000 | Loss: 0.00002045
Iteration 33/1000 | Loss: 0.00002044
Iteration 34/1000 | Loss: 0.00002043
Iteration 35/1000 | Loss: 0.00002043
Iteration 36/1000 | Loss: 0.00002042
Iteration 37/1000 | Loss: 0.00002042
Iteration 38/1000 | Loss: 0.00002041
Iteration 39/1000 | Loss: 0.00002041
Iteration 40/1000 | Loss: 0.00002041
Iteration 41/1000 | Loss: 0.00002041
Iteration 42/1000 | Loss: 0.00002041
Iteration 43/1000 | Loss: 0.00002041
Iteration 44/1000 | Loss: 0.00002041
Iteration 45/1000 | Loss: 0.00002041
Iteration 46/1000 | Loss: 0.00002041
Iteration 47/1000 | Loss: 0.00002041
Iteration 48/1000 | Loss: 0.00002040
Iteration 49/1000 | Loss: 0.00002040
Iteration 50/1000 | Loss: 0.00002039
Iteration 51/1000 | Loss: 0.00002039
Iteration 52/1000 | Loss: 0.00002039
Iteration 53/1000 | Loss: 0.00002039
Iteration 54/1000 | Loss: 0.00002038
Iteration 55/1000 | Loss: 0.00002038
Iteration 56/1000 | Loss: 0.00002038
Iteration 57/1000 | Loss: 0.00002038
Iteration 58/1000 | Loss: 0.00002038
Iteration 59/1000 | Loss: 0.00002038
Iteration 60/1000 | Loss: 0.00002038
Iteration 61/1000 | Loss: 0.00002038
Iteration 62/1000 | Loss: 0.00002038
Iteration 63/1000 | Loss: 0.00002038
Iteration 64/1000 | Loss: 0.00002038
Iteration 65/1000 | Loss: 0.00002037
Iteration 66/1000 | Loss: 0.00002037
Iteration 67/1000 | Loss: 0.00002036
Iteration 68/1000 | Loss: 0.00002036
Iteration 69/1000 | Loss: 0.00002035
Iteration 70/1000 | Loss: 0.00002035
Iteration 71/1000 | Loss: 0.00002035
Iteration 72/1000 | Loss: 0.00002035
Iteration 73/1000 | Loss: 0.00002034
Iteration 74/1000 | Loss: 0.00002034
Iteration 75/1000 | Loss: 0.00002034
Iteration 76/1000 | Loss: 0.00002034
Iteration 77/1000 | Loss: 0.00002034
Iteration 78/1000 | Loss: 0.00002034
Iteration 79/1000 | Loss: 0.00002034
Iteration 80/1000 | Loss: 0.00002034
Iteration 81/1000 | Loss: 0.00002034
Iteration 82/1000 | Loss: 0.00002033
Iteration 83/1000 | Loss: 0.00002033
Iteration 84/1000 | Loss: 0.00002033
Iteration 85/1000 | Loss: 0.00002033
Iteration 86/1000 | Loss: 0.00002033
Iteration 87/1000 | Loss: 0.00002033
Iteration 88/1000 | Loss: 0.00002033
Iteration 89/1000 | Loss: 0.00002033
Iteration 90/1000 | Loss: 0.00002033
Iteration 91/1000 | Loss: 0.00002033
Iteration 92/1000 | Loss: 0.00002033
Iteration 93/1000 | Loss: 0.00002033
Iteration 94/1000 | Loss: 0.00002033
Iteration 95/1000 | Loss: 0.00002033
Iteration 96/1000 | Loss: 0.00002032
Iteration 97/1000 | Loss: 0.00002032
Iteration 98/1000 | Loss: 0.00002031
Iteration 99/1000 | Loss: 0.00002031
Iteration 100/1000 | Loss: 0.00002031
Iteration 101/1000 | Loss: 0.00002030
Iteration 102/1000 | Loss: 0.00002030
Iteration 103/1000 | Loss: 0.00002030
Iteration 104/1000 | Loss: 0.00002030
Iteration 105/1000 | Loss: 0.00002030
Iteration 106/1000 | Loss: 0.00002030
Iteration 107/1000 | Loss: 0.00002029
Iteration 108/1000 | Loss: 0.00002029
Iteration 109/1000 | Loss: 0.00002029
Iteration 110/1000 | Loss: 0.00002029
Iteration 111/1000 | Loss: 0.00002029
Iteration 112/1000 | Loss: 0.00002028
Iteration 113/1000 | Loss: 0.00002028
Iteration 114/1000 | Loss: 0.00002028
Iteration 115/1000 | Loss: 0.00002028
Iteration 116/1000 | Loss: 0.00002028
Iteration 117/1000 | Loss: 0.00002028
Iteration 118/1000 | Loss: 0.00002027
Iteration 119/1000 | Loss: 0.00002027
Iteration 120/1000 | Loss: 0.00002027
Iteration 121/1000 | Loss: 0.00002026
Iteration 122/1000 | Loss: 0.00002026
Iteration 123/1000 | Loss: 0.00002026
Iteration 124/1000 | Loss: 0.00002026
Iteration 125/1000 | Loss: 0.00002026
Iteration 126/1000 | Loss: 0.00002026
Iteration 127/1000 | Loss: 0.00002026
Iteration 128/1000 | Loss: 0.00002026
Iteration 129/1000 | Loss: 0.00002026
Iteration 130/1000 | Loss: 0.00002026
Iteration 131/1000 | Loss: 0.00002026
Iteration 132/1000 | Loss: 0.00002026
Iteration 133/1000 | Loss: 0.00002026
Iteration 134/1000 | Loss: 0.00002025
Iteration 135/1000 | Loss: 0.00002025
Iteration 136/1000 | Loss: 0.00002025
Iteration 137/1000 | Loss: 0.00002025
Iteration 138/1000 | Loss: 0.00002025
Iteration 139/1000 | Loss: 0.00002025
Iteration 140/1000 | Loss: 0.00002024
Iteration 141/1000 | Loss: 0.00002024
Iteration 142/1000 | Loss: 0.00002024
Iteration 143/1000 | Loss: 0.00002024
Iteration 144/1000 | Loss: 0.00002024
Iteration 145/1000 | Loss: 0.00002024
Iteration 146/1000 | Loss: 0.00002024
Iteration 147/1000 | Loss: 0.00002024
Iteration 148/1000 | Loss: 0.00002023
Iteration 149/1000 | Loss: 0.00002023
Iteration 150/1000 | Loss: 0.00002023
Iteration 151/1000 | Loss: 0.00002023
Iteration 152/1000 | Loss: 0.00002022
Iteration 153/1000 | Loss: 0.00002022
Iteration 154/1000 | Loss: 0.00002022
Iteration 155/1000 | Loss: 0.00002022
Iteration 156/1000 | Loss: 0.00002022
Iteration 157/1000 | Loss: 0.00002022
Iteration 158/1000 | Loss: 0.00002022
Iteration 159/1000 | Loss: 0.00002022
Iteration 160/1000 | Loss: 0.00002022
Iteration 161/1000 | Loss: 0.00002021
Iteration 162/1000 | Loss: 0.00002021
Iteration 163/1000 | Loss: 0.00002021
Iteration 164/1000 | Loss: 0.00002021
Iteration 165/1000 | Loss: 0.00002021
Iteration 166/1000 | Loss: 0.00002020
Iteration 167/1000 | Loss: 0.00002020
Iteration 168/1000 | Loss: 0.00002020
Iteration 169/1000 | Loss: 0.00002020
Iteration 170/1000 | Loss: 0.00002020
Iteration 171/1000 | Loss: 0.00002020
Iteration 172/1000 | Loss: 0.00002020
Iteration 173/1000 | Loss: 0.00002020
Iteration 174/1000 | Loss: 0.00002020
Iteration 175/1000 | Loss: 0.00002020
Iteration 176/1000 | Loss: 0.00002020
Iteration 177/1000 | Loss: 0.00002020
Iteration 178/1000 | Loss: 0.00002019
Iteration 179/1000 | Loss: 0.00002019
Iteration 180/1000 | Loss: 0.00002019
Iteration 181/1000 | Loss: 0.00002018
Iteration 182/1000 | Loss: 0.00002018
Iteration 183/1000 | Loss: 0.00002018
Iteration 184/1000 | Loss: 0.00002018
Iteration 185/1000 | Loss: 0.00002017
Iteration 186/1000 | Loss: 0.00002017
Iteration 187/1000 | Loss: 0.00002017
Iteration 188/1000 | Loss: 0.00002017
Iteration 189/1000 | Loss: 0.00002017
Iteration 190/1000 | Loss: 0.00002017
Iteration 191/1000 | Loss: 0.00002016
Iteration 192/1000 | Loss: 0.00002016
Iteration 193/1000 | Loss: 0.00002016
Iteration 194/1000 | Loss: 0.00002016
Iteration 195/1000 | Loss: 0.00002015
Iteration 196/1000 | Loss: 0.00002015
Iteration 197/1000 | Loss: 0.00002015
Iteration 198/1000 | Loss: 0.00002015
Iteration 199/1000 | Loss: 0.00002015
Iteration 200/1000 | Loss: 0.00002014
Iteration 201/1000 | Loss: 0.00002014
Iteration 202/1000 | Loss: 0.00002014
Iteration 203/1000 | Loss: 0.00002014
Iteration 204/1000 | Loss: 0.00002014
Iteration 205/1000 | Loss: 0.00002014
Iteration 206/1000 | Loss: 0.00002014
Iteration 207/1000 | Loss: 0.00002014
Iteration 208/1000 | Loss: 0.00002014
Iteration 209/1000 | Loss: 0.00002013
Iteration 210/1000 | Loss: 0.00002013
Iteration 211/1000 | Loss: 0.00002013
Iteration 212/1000 | Loss: 0.00002013
Iteration 213/1000 | Loss: 0.00002012
Iteration 214/1000 | Loss: 0.00002012
Iteration 215/1000 | Loss: 0.00002012
Iteration 216/1000 | Loss: 0.00002012
Iteration 217/1000 | Loss: 0.00002012
Iteration 218/1000 | Loss: 0.00002012
Iteration 219/1000 | Loss: 0.00002012
Iteration 220/1000 | Loss: 0.00002011
Iteration 221/1000 | Loss: 0.00002011
Iteration 222/1000 | Loss: 0.00002011
Iteration 223/1000 | Loss: 0.00002011
Iteration 224/1000 | Loss: 0.00002011
Iteration 225/1000 | Loss: 0.00002011
Iteration 226/1000 | Loss: 0.00002011
Iteration 227/1000 | Loss: 0.00002011
Iteration 228/1000 | Loss: 0.00002011
Iteration 229/1000 | Loss: 0.00002011
Iteration 230/1000 | Loss: 0.00002010
Iteration 231/1000 | Loss: 0.00002010
Iteration 232/1000 | Loss: 0.00002010
Iteration 233/1000 | Loss: 0.00002010
Iteration 234/1000 | Loss: 0.00002010
Iteration 235/1000 | Loss: 0.00002010
Iteration 236/1000 | Loss: 0.00002010
Iteration 237/1000 | Loss: 0.00002009
Iteration 238/1000 | Loss: 0.00002009
Iteration 239/1000 | Loss: 0.00002009
Iteration 240/1000 | Loss: 0.00002009
Iteration 241/1000 | Loss: 0.00002009
Iteration 242/1000 | Loss: 0.00002009
Iteration 243/1000 | Loss: 0.00002009
Iteration 244/1000 | Loss: 0.00002008
Iteration 245/1000 | Loss: 0.00002008
Iteration 246/1000 | Loss: 0.00002008
Iteration 247/1000 | Loss: 0.00002008
Iteration 248/1000 | Loss: 0.00002008
Iteration 249/1000 | Loss: 0.00002008
Iteration 250/1000 | Loss: 0.00002008
Iteration 251/1000 | Loss: 0.00002008
Iteration 252/1000 | Loss: 0.00002008
Iteration 253/1000 | Loss: 0.00002008
Iteration 254/1000 | Loss: 0.00002008
Iteration 255/1000 | Loss: 0.00002008
Iteration 256/1000 | Loss: 0.00002008
Iteration 257/1000 | Loss: 0.00002007
Iteration 258/1000 | Loss: 0.00002007
Iteration 259/1000 | Loss: 0.00002007
Iteration 260/1000 | Loss: 0.00002007
Iteration 261/1000 | Loss: 0.00002007
Iteration 262/1000 | Loss: 0.00002007
Iteration 263/1000 | Loss: 0.00002007
Iteration 264/1000 | Loss: 0.00002007
Iteration 265/1000 | Loss: 0.00002007
Iteration 266/1000 | Loss: 0.00002007
Iteration 267/1000 | Loss: 0.00002007
Iteration 268/1000 | Loss: 0.00002007
Iteration 269/1000 | Loss: 0.00002006
Iteration 270/1000 | Loss: 0.00002006
Iteration 271/1000 | Loss: 0.00002006
Iteration 272/1000 | Loss: 0.00002006
Iteration 273/1000 | Loss: 0.00002006
Iteration 274/1000 | Loss: 0.00002006
Iteration 275/1000 | Loss: 0.00002006
Iteration 276/1000 | Loss: 0.00002006
Iteration 277/1000 | Loss: 0.00002006
Iteration 278/1000 | Loss: 0.00002006
Iteration 279/1000 | Loss: 0.00002006
Iteration 280/1000 | Loss: 0.00002006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [2.006036629609298e-05, 2.006036629609298e-05, 2.006036629609298e-05, 2.006036629609298e-05, 2.006036629609298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.006036629609298e-05

Optimization complete. Final v2v error: 3.7389090061187744 mm

Highest mean error: 5.306389331817627 mm for frame 193

Lowest mean error: 3.264411687850952 mm for frame 32

Saving results

Total time: 52.26339554786682
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00648120
Iteration 2/25 | Loss: 0.00146391
Iteration 3/25 | Loss: 0.00137732
Iteration 4/25 | Loss: 0.00136422
Iteration 5/25 | Loss: 0.00135847
Iteration 6/25 | Loss: 0.00135712
Iteration 7/25 | Loss: 0.00135712
Iteration 8/25 | Loss: 0.00135712
Iteration 9/25 | Loss: 0.00135712
Iteration 10/25 | Loss: 0.00135712
Iteration 11/25 | Loss: 0.00135712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013571211602538824, 0.0013571211602538824, 0.0013571211602538824, 0.0013571211602538824, 0.0013571211602538824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013571211602538824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.95213366
Iteration 2/25 | Loss: 0.00195345
Iteration 3/25 | Loss: 0.00195345
Iteration 4/25 | Loss: 0.00195345
Iteration 5/25 | Loss: 0.00195345
Iteration 6/25 | Loss: 0.00195345
Iteration 7/25 | Loss: 0.00195345
Iteration 8/25 | Loss: 0.00195345
Iteration 9/25 | Loss: 0.00195345
Iteration 10/25 | Loss: 0.00195345
Iteration 11/25 | Loss: 0.00195345
Iteration 12/25 | Loss: 0.00195345
Iteration 13/25 | Loss: 0.00195345
Iteration 14/25 | Loss: 0.00195345
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001953450497239828, 0.001953450497239828, 0.001953450497239828, 0.001953450497239828, 0.001953450497239828]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001953450497239828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195345
Iteration 2/1000 | Loss: 0.00004002
Iteration 3/1000 | Loss: 0.00002457
Iteration 4/1000 | Loss: 0.00002217
Iteration 5/1000 | Loss: 0.00002090
Iteration 6/1000 | Loss: 0.00002023
Iteration 7/1000 | Loss: 0.00001974
Iteration 8/1000 | Loss: 0.00001938
Iteration 9/1000 | Loss: 0.00001920
Iteration 10/1000 | Loss: 0.00001906
Iteration 11/1000 | Loss: 0.00001899
Iteration 12/1000 | Loss: 0.00001897
Iteration 13/1000 | Loss: 0.00001896
Iteration 14/1000 | Loss: 0.00001888
Iteration 15/1000 | Loss: 0.00001887
Iteration 16/1000 | Loss: 0.00001881
Iteration 17/1000 | Loss: 0.00001869
Iteration 18/1000 | Loss: 0.00001860
Iteration 19/1000 | Loss: 0.00001857
Iteration 20/1000 | Loss: 0.00001856
Iteration 21/1000 | Loss: 0.00001856
Iteration 22/1000 | Loss: 0.00001855
Iteration 23/1000 | Loss: 0.00001855
Iteration 24/1000 | Loss: 0.00001854
Iteration 25/1000 | Loss: 0.00001853
Iteration 26/1000 | Loss: 0.00001852
Iteration 27/1000 | Loss: 0.00001851
Iteration 28/1000 | Loss: 0.00001851
Iteration 29/1000 | Loss: 0.00001850
Iteration 30/1000 | Loss: 0.00001850
Iteration 31/1000 | Loss: 0.00001850
Iteration 32/1000 | Loss: 0.00001849
Iteration 33/1000 | Loss: 0.00001849
Iteration 34/1000 | Loss: 0.00001849
Iteration 35/1000 | Loss: 0.00001849
Iteration 36/1000 | Loss: 0.00001849
Iteration 37/1000 | Loss: 0.00001849
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001848
Iteration 40/1000 | Loss: 0.00001848
Iteration 41/1000 | Loss: 0.00001848
Iteration 42/1000 | Loss: 0.00001848
Iteration 43/1000 | Loss: 0.00001848
Iteration 44/1000 | Loss: 0.00001847
Iteration 45/1000 | Loss: 0.00001847
Iteration 46/1000 | Loss: 0.00001847
Iteration 47/1000 | Loss: 0.00001846
Iteration 48/1000 | Loss: 0.00001846
Iteration 49/1000 | Loss: 0.00001846
Iteration 50/1000 | Loss: 0.00001846
Iteration 51/1000 | Loss: 0.00001845
Iteration 52/1000 | Loss: 0.00001845
Iteration 53/1000 | Loss: 0.00001845
Iteration 54/1000 | Loss: 0.00001845
Iteration 55/1000 | Loss: 0.00001845
Iteration 56/1000 | Loss: 0.00001845
Iteration 57/1000 | Loss: 0.00001845
Iteration 58/1000 | Loss: 0.00001844
Iteration 59/1000 | Loss: 0.00001843
Iteration 60/1000 | Loss: 0.00001843
Iteration 61/1000 | Loss: 0.00001843
Iteration 62/1000 | Loss: 0.00001843
Iteration 63/1000 | Loss: 0.00001842
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001842
Iteration 66/1000 | Loss: 0.00001842
Iteration 67/1000 | Loss: 0.00001842
Iteration 68/1000 | Loss: 0.00001842
Iteration 69/1000 | Loss: 0.00001842
Iteration 70/1000 | Loss: 0.00001842
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001841
Iteration 74/1000 | Loss: 0.00001841
Iteration 75/1000 | Loss: 0.00001841
Iteration 76/1000 | Loss: 0.00001841
Iteration 77/1000 | Loss: 0.00001841
Iteration 78/1000 | Loss: 0.00001841
Iteration 79/1000 | Loss: 0.00001841
Iteration 80/1000 | Loss: 0.00001841
Iteration 81/1000 | Loss: 0.00001841
Iteration 82/1000 | Loss: 0.00001841
Iteration 83/1000 | Loss: 0.00001841
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001840
Iteration 89/1000 | Loss: 0.00001840
Iteration 90/1000 | Loss: 0.00001840
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00001840
Iteration 93/1000 | Loss: 0.00001840
Iteration 94/1000 | Loss: 0.00001840
Iteration 95/1000 | Loss: 0.00001840
Iteration 96/1000 | Loss: 0.00001840
Iteration 97/1000 | Loss: 0.00001840
Iteration 98/1000 | Loss: 0.00001840
Iteration 99/1000 | Loss: 0.00001839
Iteration 100/1000 | Loss: 0.00001839
Iteration 101/1000 | Loss: 0.00001839
Iteration 102/1000 | Loss: 0.00001839
Iteration 103/1000 | Loss: 0.00001839
Iteration 104/1000 | Loss: 0.00001839
Iteration 105/1000 | Loss: 0.00001839
Iteration 106/1000 | Loss: 0.00001839
Iteration 107/1000 | Loss: 0.00001839
Iteration 108/1000 | Loss: 0.00001839
Iteration 109/1000 | Loss: 0.00001838
Iteration 110/1000 | Loss: 0.00001838
Iteration 111/1000 | Loss: 0.00001838
Iteration 112/1000 | Loss: 0.00001838
Iteration 113/1000 | Loss: 0.00001838
Iteration 114/1000 | Loss: 0.00001838
Iteration 115/1000 | Loss: 0.00001838
Iteration 116/1000 | Loss: 0.00001838
Iteration 117/1000 | Loss: 0.00001838
Iteration 118/1000 | Loss: 0.00001838
Iteration 119/1000 | Loss: 0.00001838
Iteration 120/1000 | Loss: 0.00001838
Iteration 121/1000 | Loss: 0.00001838
Iteration 122/1000 | Loss: 0.00001838
Iteration 123/1000 | Loss: 0.00001838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.8382532289251685e-05, 1.8382532289251685e-05, 1.8382532289251685e-05, 1.8382532289251685e-05, 1.8382532289251685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8382532289251685e-05

Optimization complete. Final v2v error: 3.6398375034332275 mm

Highest mean error: 4.017228126525879 mm for frame 23

Lowest mean error: 3.405228853225708 mm for frame 92

Saving results

Total time: 31.1469669342041
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826399
Iteration 2/25 | Loss: 0.00167190
Iteration 3/25 | Loss: 0.00148788
Iteration 4/25 | Loss: 0.00146210
Iteration 5/25 | Loss: 0.00145575
Iteration 6/25 | Loss: 0.00145474
Iteration 7/25 | Loss: 0.00145474
Iteration 8/25 | Loss: 0.00145474
Iteration 9/25 | Loss: 0.00145474
Iteration 10/25 | Loss: 0.00145474
Iteration 11/25 | Loss: 0.00145474
Iteration 12/25 | Loss: 0.00145474
Iteration 13/25 | Loss: 0.00145474
Iteration 14/25 | Loss: 0.00145474
Iteration 15/25 | Loss: 0.00145474
Iteration 16/25 | Loss: 0.00145474
Iteration 17/25 | Loss: 0.00145474
Iteration 18/25 | Loss: 0.00145474
Iteration 19/25 | Loss: 0.00145474
Iteration 20/25 | Loss: 0.00145474
Iteration 21/25 | Loss: 0.00145474
Iteration 22/25 | Loss: 0.00145474
Iteration 23/25 | Loss: 0.00145474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014547394821420312, 0.0014547394821420312, 0.0014547394821420312, 0.0014547394821420312, 0.0014547394821420312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014547394821420312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17824769
Iteration 2/25 | Loss: 0.00244170
Iteration 3/25 | Loss: 0.00244167
Iteration 4/25 | Loss: 0.00244167
Iteration 5/25 | Loss: 0.00244167
Iteration 6/25 | Loss: 0.00244167
Iteration 7/25 | Loss: 0.00244167
Iteration 8/25 | Loss: 0.00244167
Iteration 9/25 | Loss: 0.00244167
Iteration 10/25 | Loss: 0.00244167
Iteration 11/25 | Loss: 0.00244167
Iteration 12/25 | Loss: 0.00244167
Iteration 13/25 | Loss: 0.00244167
Iteration 14/25 | Loss: 0.00244167
Iteration 15/25 | Loss: 0.00244167
Iteration 16/25 | Loss: 0.00244167
Iteration 17/25 | Loss: 0.00244167
Iteration 18/25 | Loss: 0.00244167
Iteration 19/25 | Loss: 0.00244167
Iteration 20/25 | Loss: 0.00244167
Iteration 21/25 | Loss: 0.00244167
Iteration 22/25 | Loss: 0.00244167
Iteration 23/25 | Loss: 0.00244167
Iteration 24/25 | Loss: 0.00244167
Iteration 25/25 | Loss: 0.00244167

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244167
Iteration 2/1000 | Loss: 0.00004052
Iteration 3/1000 | Loss: 0.00003009
Iteration 4/1000 | Loss: 0.00002726
Iteration 5/1000 | Loss: 0.00002532
Iteration 6/1000 | Loss: 0.00002398
Iteration 7/1000 | Loss: 0.00002334
Iteration 8/1000 | Loss: 0.00002291
Iteration 9/1000 | Loss: 0.00002261
Iteration 10/1000 | Loss: 0.00002231
Iteration 11/1000 | Loss: 0.00002221
Iteration 12/1000 | Loss: 0.00002215
Iteration 13/1000 | Loss: 0.00002193
Iteration 14/1000 | Loss: 0.00002192
Iteration 15/1000 | Loss: 0.00002182
Iteration 16/1000 | Loss: 0.00002179
Iteration 17/1000 | Loss: 0.00002179
Iteration 18/1000 | Loss: 0.00002176
Iteration 19/1000 | Loss: 0.00002176
Iteration 20/1000 | Loss: 0.00002176
Iteration 21/1000 | Loss: 0.00002176
Iteration 22/1000 | Loss: 0.00002176
Iteration 23/1000 | Loss: 0.00002176
Iteration 24/1000 | Loss: 0.00002176
Iteration 25/1000 | Loss: 0.00002174
Iteration 26/1000 | Loss: 0.00002174
Iteration 27/1000 | Loss: 0.00002172
Iteration 28/1000 | Loss: 0.00002172
Iteration 29/1000 | Loss: 0.00002171
Iteration 30/1000 | Loss: 0.00002170
Iteration 31/1000 | Loss: 0.00002170
Iteration 32/1000 | Loss: 0.00002170
Iteration 33/1000 | Loss: 0.00002170
Iteration 34/1000 | Loss: 0.00002170
Iteration 35/1000 | Loss: 0.00002169
Iteration 36/1000 | Loss: 0.00002169
Iteration 37/1000 | Loss: 0.00002169
Iteration 38/1000 | Loss: 0.00002168
Iteration 39/1000 | Loss: 0.00002168
Iteration 40/1000 | Loss: 0.00002168
Iteration 41/1000 | Loss: 0.00002168
Iteration 42/1000 | Loss: 0.00002168
Iteration 43/1000 | Loss: 0.00002168
Iteration 44/1000 | Loss: 0.00002167
Iteration 45/1000 | Loss: 0.00002167
Iteration 46/1000 | Loss: 0.00002167
Iteration 47/1000 | Loss: 0.00002167
Iteration 48/1000 | Loss: 0.00002167
Iteration 49/1000 | Loss: 0.00002167
Iteration 50/1000 | Loss: 0.00002167
Iteration 51/1000 | Loss: 0.00002167
Iteration 52/1000 | Loss: 0.00002167
Iteration 53/1000 | Loss: 0.00002167
Iteration 54/1000 | Loss: 0.00002167
Iteration 55/1000 | Loss: 0.00002167
Iteration 56/1000 | Loss: 0.00002167
Iteration 57/1000 | Loss: 0.00002167
Iteration 58/1000 | Loss: 0.00002167
Iteration 59/1000 | Loss: 0.00002167
Iteration 60/1000 | Loss: 0.00002167
Iteration 61/1000 | Loss: 0.00002167
Iteration 62/1000 | Loss: 0.00002167
Iteration 63/1000 | Loss: 0.00002167
Iteration 64/1000 | Loss: 0.00002167
Iteration 65/1000 | Loss: 0.00002167
Iteration 66/1000 | Loss: 0.00002167
Iteration 67/1000 | Loss: 0.00002167
Iteration 68/1000 | Loss: 0.00002167
Iteration 69/1000 | Loss: 0.00002167
Iteration 70/1000 | Loss: 0.00002167
Iteration 71/1000 | Loss: 0.00002167
Iteration 72/1000 | Loss: 0.00002167
Iteration 73/1000 | Loss: 0.00002167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.166579906770494e-05, 2.166579906770494e-05, 2.166579906770494e-05, 2.166579906770494e-05, 2.166579906770494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.166579906770494e-05

Optimization complete. Final v2v error: 4.009385585784912 mm

Highest mean error: 4.308530330657959 mm for frame 178

Lowest mean error: 3.609588861465454 mm for frame 56

Saving results

Total time: 34.30266976356506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00726623
Iteration 2/25 | Loss: 0.00151565
Iteration 3/25 | Loss: 0.00141159
Iteration 4/25 | Loss: 0.00139399
Iteration 5/25 | Loss: 0.00138760
Iteration 6/25 | Loss: 0.00138578
Iteration 7/25 | Loss: 0.00138567
Iteration 8/25 | Loss: 0.00138567
Iteration 9/25 | Loss: 0.00138567
Iteration 10/25 | Loss: 0.00138567
Iteration 11/25 | Loss: 0.00138567
Iteration 12/25 | Loss: 0.00138567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013856709701940417, 0.0013856709701940417, 0.0013856709701940417, 0.0013856709701940417, 0.0013856709701940417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013856709701940417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.79824877
Iteration 2/25 | Loss: 0.00184891
Iteration 3/25 | Loss: 0.00184891
Iteration 4/25 | Loss: 0.00184891
Iteration 5/25 | Loss: 0.00184891
Iteration 6/25 | Loss: 0.00184891
Iteration 7/25 | Loss: 0.00184891
Iteration 8/25 | Loss: 0.00184891
Iteration 9/25 | Loss: 0.00184891
Iteration 10/25 | Loss: 0.00184891
Iteration 11/25 | Loss: 0.00184891
Iteration 12/25 | Loss: 0.00184891
Iteration 13/25 | Loss: 0.00184891
Iteration 14/25 | Loss: 0.00184891
Iteration 15/25 | Loss: 0.00184891
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0018489076755940914, 0.0018489076755940914, 0.0018489076755940914, 0.0018489076755940914, 0.0018489076755940914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018489076755940914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184891
Iteration 2/1000 | Loss: 0.00004999
Iteration 3/1000 | Loss: 0.00002836
Iteration 4/1000 | Loss: 0.00002563
Iteration 5/1000 | Loss: 0.00002410
Iteration 6/1000 | Loss: 0.00002317
Iteration 7/1000 | Loss: 0.00002198
Iteration 8/1000 | Loss: 0.00002136
Iteration 9/1000 | Loss: 0.00002088
Iteration 10/1000 | Loss: 0.00002047
Iteration 11/1000 | Loss: 0.00002025
Iteration 12/1000 | Loss: 0.00002016
Iteration 13/1000 | Loss: 0.00002014
Iteration 14/1000 | Loss: 0.00002004
Iteration 15/1000 | Loss: 0.00001999
Iteration 16/1000 | Loss: 0.00001998
Iteration 17/1000 | Loss: 0.00001988
Iteration 18/1000 | Loss: 0.00001987
Iteration 19/1000 | Loss: 0.00001987
Iteration 20/1000 | Loss: 0.00001987
Iteration 21/1000 | Loss: 0.00001986
Iteration 22/1000 | Loss: 0.00001986
Iteration 23/1000 | Loss: 0.00001984
Iteration 24/1000 | Loss: 0.00001983
Iteration 25/1000 | Loss: 0.00001983
Iteration 26/1000 | Loss: 0.00001982
Iteration 27/1000 | Loss: 0.00001982
Iteration 28/1000 | Loss: 0.00001982
Iteration 29/1000 | Loss: 0.00001981
Iteration 30/1000 | Loss: 0.00001981
Iteration 31/1000 | Loss: 0.00001980
Iteration 32/1000 | Loss: 0.00001980
Iteration 33/1000 | Loss: 0.00001980
Iteration 34/1000 | Loss: 0.00001979
Iteration 35/1000 | Loss: 0.00001979
Iteration 36/1000 | Loss: 0.00001979
Iteration 37/1000 | Loss: 0.00001979
Iteration 38/1000 | Loss: 0.00001978
Iteration 39/1000 | Loss: 0.00001978
Iteration 40/1000 | Loss: 0.00001978
Iteration 41/1000 | Loss: 0.00001978
Iteration 42/1000 | Loss: 0.00001978
Iteration 43/1000 | Loss: 0.00001977
Iteration 44/1000 | Loss: 0.00001976
Iteration 45/1000 | Loss: 0.00001976
Iteration 46/1000 | Loss: 0.00001975
Iteration 47/1000 | Loss: 0.00001975
Iteration 48/1000 | Loss: 0.00001974
Iteration 49/1000 | Loss: 0.00001974
Iteration 50/1000 | Loss: 0.00001973
Iteration 51/1000 | Loss: 0.00001972
Iteration 52/1000 | Loss: 0.00001972
Iteration 53/1000 | Loss: 0.00001972
Iteration 54/1000 | Loss: 0.00001971
Iteration 55/1000 | Loss: 0.00001971
Iteration 56/1000 | Loss: 0.00001971
Iteration 57/1000 | Loss: 0.00001971
Iteration 58/1000 | Loss: 0.00001971
Iteration 59/1000 | Loss: 0.00001971
Iteration 60/1000 | Loss: 0.00001971
Iteration 61/1000 | Loss: 0.00001971
Iteration 62/1000 | Loss: 0.00001971
Iteration 63/1000 | Loss: 0.00001971
Iteration 64/1000 | Loss: 0.00001970
Iteration 65/1000 | Loss: 0.00001970
Iteration 66/1000 | Loss: 0.00001969
Iteration 67/1000 | Loss: 0.00001969
Iteration 68/1000 | Loss: 0.00001968
Iteration 69/1000 | Loss: 0.00001968
Iteration 70/1000 | Loss: 0.00001968
Iteration 71/1000 | Loss: 0.00001967
Iteration 72/1000 | Loss: 0.00001966
Iteration 73/1000 | Loss: 0.00001966
Iteration 74/1000 | Loss: 0.00001966
Iteration 75/1000 | Loss: 0.00001965
Iteration 76/1000 | Loss: 0.00001965
Iteration 77/1000 | Loss: 0.00001965
Iteration 78/1000 | Loss: 0.00001965
Iteration 79/1000 | Loss: 0.00001965
Iteration 80/1000 | Loss: 0.00001965
Iteration 81/1000 | Loss: 0.00001965
Iteration 82/1000 | Loss: 0.00001964
Iteration 83/1000 | Loss: 0.00001964
Iteration 84/1000 | Loss: 0.00001964
Iteration 85/1000 | Loss: 0.00001963
Iteration 86/1000 | Loss: 0.00001963
Iteration 87/1000 | Loss: 0.00001963
Iteration 88/1000 | Loss: 0.00001963
Iteration 89/1000 | Loss: 0.00001963
Iteration 90/1000 | Loss: 0.00001963
Iteration 91/1000 | Loss: 0.00001963
Iteration 92/1000 | Loss: 0.00001963
Iteration 93/1000 | Loss: 0.00001963
Iteration 94/1000 | Loss: 0.00001963
Iteration 95/1000 | Loss: 0.00001963
Iteration 96/1000 | Loss: 0.00001963
Iteration 97/1000 | Loss: 0.00001963
Iteration 98/1000 | Loss: 0.00001963
Iteration 99/1000 | Loss: 0.00001963
Iteration 100/1000 | Loss: 0.00001963
Iteration 101/1000 | Loss: 0.00001963
Iteration 102/1000 | Loss: 0.00001963
Iteration 103/1000 | Loss: 0.00001963
Iteration 104/1000 | Loss: 0.00001963
Iteration 105/1000 | Loss: 0.00001963
Iteration 106/1000 | Loss: 0.00001963
Iteration 107/1000 | Loss: 0.00001963
Iteration 108/1000 | Loss: 0.00001963
Iteration 109/1000 | Loss: 0.00001963
Iteration 110/1000 | Loss: 0.00001963
Iteration 111/1000 | Loss: 0.00001963
Iteration 112/1000 | Loss: 0.00001963
Iteration 113/1000 | Loss: 0.00001963
Iteration 114/1000 | Loss: 0.00001963
Iteration 115/1000 | Loss: 0.00001963
Iteration 116/1000 | Loss: 0.00001963
Iteration 117/1000 | Loss: 0.00001963
Iteration 118/1000 | Loss: 0.00001963
Iteration 119/1000 | Loss: 0.00001963
Iteration 120/1000 | Loss: 0.00001963
Iteration 121/1000 | Loss: 0.00001963
Iteration 122/1000 | Loss: 0.00001963
Iteration 123/1000 | Loss: 0.00001963
Iteration 124/1000 | Loss: 0.00001963
Iteration 125/1000 | Loss: 0.00001963
Iteration 126/1000 | Loss: 0.00001963
Iteration 127/1000 | Loss: 0.00001963
Iteration 128/1000 | Loss: 0.00001963
Iteration 129/1000 | Loss: 0.00001963
Iteration 130/1000 | Loss: 0.00001963
Iteration 131/1000 | Loss: 0.00001963
Iteration 132/1000 | Loss: 0.00001963
Iteration 133/1000 | Loss: 0.00001963
Iteration 134/1000 | Loss: 0.00001963
Iteration 135/1000 | Loss: 0.00001963
Iteration 136/1000 | Loss: 0.00001963
Iteration 137/1000 | Loss: 0.00001963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.962774695130065e-05, 1.962774695130065e-05, 1.962774695130065e-05, 1.962774695130065e-05, 1.962774695130065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.962774695130065e-05

Optimization complete. Final v2v error: 3.755398988723755 mm

Highest mean error: 4.382490158081055 mm for frame 115

Lowest mean error: 3.3865461349487305 mm for frame 0

Saving results

Total time: 34.413315296173096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387919
Iteration 2/25 | Loss: 0.00158330
Iteration 3/25 | Loss: 0.00147742
Iteration 4/25 | Loss: 0.00145981
Iteration 5/25 | Loss: 0.00145268
Iteration 6/25 | Loss: 0.00145033
Iteration 7/25 | Loss: 0.00145010
Iteration 8/25 | Loss: 0.00145010
Iteration 9/25 | Loss: 0.00145010
Iteration 10/25 | Loss: 0.00145010
Iteration 11/25 | Loss: 0.00145010
Iteration 12/25 | Loss: 0.00145010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001450104988180101, 0.001450104988180101, 0.001450104988180101, 0.001450104988180101, 0.001450104988180101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001450104988180101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17655659
Iteration 2/25 | Loss: 0.00294043
Iteration 3/25 | Loss: 0.00294043
Iteration 4/25 | Loss: 0.00294043
Iteration 5/25 | Loss: 0.00294043
Iteration 6/25 | Loss: 0.00294043
Iteration 7/25 | Loss: 0.00294043
Iteration 8/25 | Loss: 0.00294043
Iteration 9/25 | Loss: 0.00294043
Iteration 10/25 | Loss: 0.00294043
Iteration 11/25 | Loss: 0.00294043
Iteration 12/25 | Loss: 0.00294043
Iteration 13/25 | Loss: 0.00294043
Iteration 14/25 | Loss: 0.00294043
Iteration 15/25 | Loss: 0.00294043
Iteration 16/25 | Loss: 0.00294043
Iteration 17/25 | Loss: 0.00294043
Iteration 18/25 | Loss: 0.00294043
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002940426580607891, 0.002940426580607891, 0.002940426580607891, 0.002940426580607891, 0.002940426580607891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002940426580607891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00294043
Iteration 2/1000 | Loss: 0.00008737
Iteration 3/1000 | Loss: 0.00004173
Iteration 4/1000 | Loss: 0.00002906
Iteration 5/1000 | Loss: 0.00002545
Iteration 6/1000 | Loss: 0.00002407
Iteration 7/1000 | Loss: 0.00002289
Iteration 8/1000 | Loss: 0.00002223
Iteration 9/1000 | Loss: 0.00002168
Iteration 10/1000 | Loss: 0.00002120
Iteration 11/1000 | Loss: 0.00002085
Iteration 12/1000 | Loss: 0.00002057
Iteration 13/1000 | Loss: 0.00002047
Iteration 14/1000 | Loss: 0.00002039
Iteration 15/1000 | Loss: 0.00002031
Iteration 16/1000 | Loss: 0.00002030
Iteration 17/1000 | Loss: 0.00002023
Iteration 18/1000 | Loss: 0.00002021
Iteration 19/1000 | Loss: 0.00002019
Iteration 20/1000 | Loss: 0.00002012
Iteration 21/1000 | Loss: 0.00002010
Iteration 22/1000 | Loss: 0.00002009
Iteration 23/1000 | Loss: 0.00002009
Iteration 24/1000 | Loss: 0.00002008
Iteration 25/1000 | Loss: 0.00002008
Iteration 26/1000 | Loss: 0.00002007
Iteration 27/1000 | Loss: 0.00002007
Iteration 28/1000 | Loss: 0.00002006
Iteration 29/1000 | Loss: 0.00002005
Iteration 30/1000 | Loss: 0.00002005
Iteration 31/1000 | Loss: 0.00002004
Iteration 32/1000 | Loss: 0.00002004
Iteration 33/1000 | Loss: 0.00002003
Iteration 34/1000 | Loss: 0.00002003
Iteration 35/1000 | Loss: 0.00002002
Iteration 36/1000 | Loss: 0.00002001
Iteration 37/1000 | Loss: 0.00001999
Iteration 38/1000 | Loss: 0.00001996
Iteration 39/1000 | Loss: 0.00001996
Iteration 40/1000 | Loss: 0.00001996
Iteration 41/1000 | Loss: 0.00001995
Iteration 42/1000 | Loss: 0.00001995
Iteration 43/1000 | Loss: 0.00001995
Iteration 44/1000 | Loss: 0.00001994
Iteration 45/1000 | Loss: 0.00001994
Iteration 46/1000 | Loss: 0.00001993
Iteration 47/1000 | Loss: 0.00001993
Iteration 48/1000 | Loss: 0.00001993
Iteration 49/1000 | Loss: 0.00001991
Iteration 50/1000 | Loss: 0.00001991
Iteration 51/1000 | Loss: 0.00001991
Iteration 52/1000 | Loss: 0.00001991
Iteration 53/1000 | Loss: 0.00001990
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001987
Iteration 58/1000 | Loss: 0.00001987
Iteration 59/1000 | Loss: 0.00001987
Iteration 60/1000 | Loss: 0.00001987
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00001986
Iteration 63/1000 | Loss: 0.00001986
Iteration 64/1000 | Loss: 0.00001986
Iteration 65/1000 | Loss: 0.00001985
Iteration 66/1000 | Loss: 0.00001985
Iteration 67/1000 | Loss: 0.00001985
Iteration 68/1000 | Loss: 0.00001984
Iteration 69/1000 | Loss: 0.00001984
Iteration 70/1000 | Loss: 0.00001984
Iteration 71/1000 | Loss: 0.00001984
Iteration 72/1000 | Loss: 0.00001984
Iteration 73/1000 | Loss: 0.00001983
Iteration 74/1000 | Loss: 0.00001983
Iteration 75/1000 | Loss: 0.00001983
Iteration 76/1000 | Loss: 0.00001983
Iteration 77/1000 | Loss: 0.00001983
Iteration 78/1000 | Loss: 0.00001983
Iteration 79/1000 | Loss: 0.00001983
Iteration 80/1000 | Loss: 0.00001982
Iteration 81/1000 | Loss: 0.00001982
Iteration 82/1000 | Loss: 0.00001982
Iteration 83/1000 | Loss: 0.00001982
Iteration 84/1000 | Loss: 0.00001982
Iteration 85/1000 | Loss: 0.00001982
Iteration 86/1000 | Loss: 0.00001982
Iteration 87/1000 | Loss: 0.00001982
Iteration 88/1000 | Loss: 0.00001982
Iteration 89/1000 | Loss: 0.00001982
Iteration 90/1000 | Loss: 0.00001982
Iteration 91/1000 | Loss: 0.00001982
Iteration 92/1000 | Loss: 0.00001981
Iteration 93/1000 | Loss: 0.00001981
Iteration 94/1000 | Loss: 0.00001981
Iteration 95/1000 | Loss: 0.00001981
Iteration 96/1000 | Loss: 0.00001981
Iteration 97/1000 | Loss: 0.00001981
Iteration 98/1000 | Loss: 0.00001981
Iteration 99/1000 | Loss: 0.00001981
Iteration 100/1000 | Loss: 0.00001981
Iteration 101/1000 | Loss: 0.00001981
Iteration 102/1000 | Loss: 0.00001981
Iteration 103/1000 | Loss: 0.00001981
Iteration 104/1000 | Loss: 0.00001981
Iteration 105/1000 | Loss: 0.00001981
Iteration 106/1000 | Loss: 0.00001980
Iteration 107/1000 | Loss: 0.00001980
Iteration 108/1000 | Loss: 0.00001980
Iteration 109/1000 | Loss: 0.00001980
Iteration 110/1000 | Loss: 0.00001980
Iteration 111/1000 | Loss: 0.00001980
Iteration 112/1000 | Loss: 0.00001980
Iteration 113/1000 | Loss: 0.00001980
Iteration 114/1000 | Loss: 0.00001980
Iteration 115/1000 | Loss: 0.00001980
Iteration 116/1000 | Loss: 0.00001980
Iteration 117/1000 | Loss: 0.00001980
Iteration 118/1000 | Loss: 0.00001980
Iteration 119/1000 | Loss: 0.00001980
Iteration 120/1000 | Loss: 0.00001980
Iteration 121/1000 | Loss: 0.00001980
Iteration 122/1000 | Loss: 0.00001980
Iteration 123/1000 | Loss: 0.00001980
Iteration 124/1000 | Loss: 0.00001980
Iteration 125/1000 | Loss: 0.00001980
Iteration 126/1000 | Loss: 0.00001980
Iteration 127/1000 | Loss: 0.00001980
Iteration 128/1000 | Loss: 0.00001980
Iteration 129/1000 | Loss: 0.00001980
Iteration 130/1000 | Loss: 0.00001980
Iteration 131/1000 | Loss: 0.00001980
Iteration 132/1000 | Loss: 0.00001980
Iteration 133/1000 | Loss: 0.00001980
Iteration 134/1000 | Loss: 0.00001980
Iteration 135/1000 | Loss: 0.00001980
Iteration 136/1000 | Loss: 0.00001980
Iteration 137/1000 | Loss: 0.00001980
Iteration 138/1000 | Loss: 0.00001980
Iteration 139/1000 | Loss: 0.00001980
Iteration 140/1000 | Loss: 0.00001980
Iteration 141/1000 | Loss: 0.00001980
Iteration 142/1000 | Loss: 0.00001980
Iteration 143/1000 | Loss: 0.00001980
Iteration 144/1000 | Loss: 0.00001980
Iteration 145/1000 | Loss: 0.00001980
Iteration 146/1000 | Loss: 0.00001980
Iteration 147/1000 | Loss: 0.00001980
Iteration 148/1000 | Loss: 0.00001980
Iteration 149/1000 | Loss: 0.00001980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.9802340830210596e-05, 1.9802340830210596e-05, 1.9802340830210596e-05, 1.9802340830210596e-05, 1.9802340830210596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9802340830210596e-05

Optimization complete. Final v2v error: 3.735705852508545 mm

Highest mean error: 4.295247554779053 mm for frame 33

Lowest mean error: 3.2002341747283936 mm for frame 174

Saving results

Total time: 42.83625793457031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00659559
Iteration 2/25 | Loss: 0.00143030
Iteration 3/25 | Loss: 0.00133270
Iteration 4/25 | Loss: 0.00132486
Iteration 5/25 | Loss: 0.00132279
Iteration 6/25 | Loss: 0.00132279
Iteration 7/25 | Loss: 0.00132279
Iteration 8/25 | Loss: 0.00132279
Iteration 9/25 | Loss: 0.00132279
Iteration 10/25 | Loss: 0.00132279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013227906310930848, 0.0013227906310930848, 0.0013227906310930848, 0.0013227906310930848, 0.0013227906310930848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013227906310930848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.55105972
Iteration 2/25 | Loss: 0.00184797
Iteration 3/25 | Loss: 0.00184795
Iteration 4/25 | Loss: 0.00184795
Iteration 5/25 | Loss: 0.00184795
Iteration 6/25 | Loss: 0.00184795
Iteration 7/25 | Loss: 0.00184795
Iteration 8/25 | Loss: 0.00184795
Iteration 9/25 | Loss: 0.00184795
Iteration 10/25 | Loss: 0.00184795
Iteration 11/25 | Loss: 0.00184795
Iteration 12/25 | Loss: 0.00184795
Iteration 13/25 | Loss: 0.00184795
Iteration 14/25 | Loss: 0.00184795
Iteration 15/25 | Loss: 0.00184795
Iteration 16/25 | Loss: 0.00184795
Iteration 17/25 | Loss: 0.00184795
Iteration 18/25 | Loss: 0.00184795
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018479462014511228, 0.0018479462014511228, 0.0018479462014511228, 0.0018479462014511228, 0.0018479462014511228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018479462014511228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184795
Iteration 2/1000 | Loss: 0.00003164
Iteration 3/1000 | Loss: 0.00002115
Iteration 4/1000 | Loss: 0.00001930
Iteration 5/1000 | Loss: 0.00001847
Iteration 6/1000 | Loss: 0.00001775
Iteration 7/1000 | Loss: 0.00001741
Iteration 8/1000 | Loss: 0.00001709
Iteration 9/1000 | Loss: 0.00001685
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001670
Iteration 12/1000 | Loss: 0.00001667
Iteration 13/1000 | Loss: 0.00001664
Iteration 14/1000 | Loss: 0.00001662
Iteration 15/1000 | Loss: 0.00001658
Iteration 16/1000 | Loss: 0.00001653
Iteration 17/1000 | Loss: 0.00001653
Iteration 18/1000 | Loss: 0.00001648
Iteration 19/1000 | Loss: 0.00001645
Iteration 20/1000 | Loss: 0.00001643
Iteration 21/1000 | Loss: 0.00001643
Iteration 22/1000 | Loss: 0.00001640
Iteration 23/1000 | Loss: 0.00001640
Iteration 24/1000 | Loss: 0.00001639
Iteration 25/1000 | Loss: 0.00001639
Iteration 26/1000 | Loss: 0.00001639
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001639
Iteration 30/1000 | Loss: 0.00001639
Iteration 31/1000 | Loss: 0.00001639
Iteration 32/1000 | Loss: 0.00001639
Iteration 33/1000 | Loss: 0.00001639
Iteration 34/1000 | Loss: 0.00001638
Iteration 35/1000 | Loss: 0.00001638
Iteration 36/1000 | Loss: 0.00001638
Iteration 37/1000 | Loss: 0.00001638
Iteration 38/1000 | Loss: 0.00001638
Iteration 39/1000 | Loss: 0.00001637
Iteration 40/1000 | Loss: 0.00001636
Iteration 41/1000 | Loss: 0.00001636
Iteration 42/1000 | Loss: 0.00001636
Iteration 43/1000 | Loss: 0.00001636
Iteration 44/1000 | Loss: 0.00001636
Iteration 45/1000 | Loss: 0.00001635
Iteration 46/1000 | Loss: 0.00001635
Iteration 47/1000 | Loss: 0.00001635
Iteration 48/1000 | Loss: 0.00001635
Iteration 49/1000 | Loss: 0.00001635
Iteration 50/1000 | Loss: 0.00001635
Iteration 51/1000 | Loss: 0.00001634
Iteration 52/1000 | Loss: 0.00001634
Iteration 53/1000 | Loss: 0.00001634
Iteration 54/1000 | Loss: 0.00001633
Iteration 55/1000 | Loss: 0.00001633
Iteration 56/1000 | Loss: 0.00001633
Iteration 57/1000 | Loss: 0.00001633
Iteration 58/1000 | Loss: 0.00001633
Iteration 59/1000 | Loss: 0.00001633
Iteration 60/1000 | Loss: 0.00001633
Iteration 61/1000 | Loss: 0.00001633
Iteration 62/1000 | Loss: 0.00001633
Iteration 63/1000 | Loss: 0.00001632
Iteration 64/1000 | Loss: 0.00001632
Iteration 65/1000 | Loss: 0.00001632
Iteration 66/1000 | Loss: 0.00001632
Iteration 67/1000 | Loss: 0.00001632
Iteration 68/1000 | Loss: 0.00001631
Iteration 69/1000 | Loss: 0.00001631
Iteration 70/1000 | Loss: 0.00001631
Iteration 71/1000 | Loss: 0.00001631
Iteration 72/1000 | Loss: 0.00001631
Iteration 73/1000 | Loss: 0.00001631
Iteration 74/1000 | Loss: 0.00001631
Iteration 75/1000 | Loss: 0.00001631
Iteration 76/1000 | Loss: 0.00001631
Iteration 77/1000 | Loss: 0.00001631
Iteration 78/1000 | Loss: 0.00001631
Iteration 79/1000 | Loss: 0.00001631
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001631
Iteration 85/1000 | Loss: 0.00001631
Iteration 86/1000 | Loss: 0.00001631
Iteration 87/1000 | Loss: 0.00001631
Iteration 88/1000 | Loss: 0.00001631
Iteration 89/1000 | Loss: 0.00001631
Iteration 90/1000 | Loss: 0.00001631
Iteration 91/1000 | Loss: 0.00001631
Iteration 92/1000 | Loss: 0.00001631
Iteration 93/1000 | Loss: 0.00001631
Iteration 94/1000 | Loss: 0.00001631
Iteration 95/1000 | Loss: 0.00001631
Iteration 96/1000 | Loss: 0.00001631
Iteration 97/1000 | Loss: 0.00001631
Iteration 98/1000 | Loss: 0.00001631
Iteration 99/1000 | Loss: 0.00001631
Iteration 100/1000 | Loss: 0.00001631
Iteration 101/1000 | Loss: 0.00001631
Iteration 102/1000 | Loss: 0.00001631
Iteration 103/1000 | Loss: 0.00001631
Iteration 104/1000 | Loss: 0.00001631
Iteration 105/1000 | Loss: 0.00001631
Iteration 106/1000 | Loss: 0.00001631
Iteration 107/1000 | Loss: 0.00001631
Iteration 108/1000 | Loss: 0.00001631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.630624865356367e-05, 1.630624865356367e-05, 1.630624865356367e-05, 1.630624865356367e-05, 1.630624865356367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.630624865356367e-05

Optimization complete. Final v2v error: 3.3569846153259277 mm

Highest mean error: 3.7906570434570312 mm for frame 116

Lowest mean error: 3.0189764499664307 mm for frame 104

Saving results

Total time: 33.049399852752686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932576
Iteration 2/25 | Loss: 0.00179794
Iteration 3/25 | Loss: 0.00158354
Iteration 4/25 | Loss: 0.00155217
Iteration 5/25 | Loss: 0.00154477
Iteration 6/25 | Loss: 0.00154261
Iteration 7/25 | Loss: 0.00154248
Iteration 8/25 | Loss: 0.00154248
Iteration 9/25 | Loss: 0.00154248
Iteration 10/25 | Loss: 0.00154248
Iteration 11/25 | Loss: 0.00154248
Iteration 12/25 | Loss: 0.00154248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015424798475578427, 0.0015424798475578427, 0.0015424798475578427, 0.0015424798475578427, 0.0015424798475578427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015424798475578427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13218546
Iteration 2/25 | Loss: 0.00229415
Iteration 3/25 | Loss: 0.00229414
Iteration 4/25 | Loss: 0.00229414
Iteration 5/25 | Loss: 0.00229414
Iteration 6/25 | Loss: 0.00229414
Iteration 7/25 | Loss: 0.00229414
Iteration 8/25 | Loss: 0.00229414
Iteration 9/25 | Loss: 0.00229414
Iteration 10/25 | Loss: 0.00229414
Iteration 11/25 | Loss: 0.00229414
Iteration 12/25 | Loss: 0.00229414
Iteration 13/25 | Loss: 0.00229414
Iteration 14/25 | Loss: 0.00229414
Iteration 15/25 | Loss: 0.00229414
Iteration 16/25 | Loss: 0.00229414
Iteration 17/25 | Loss: 0.00229414
Iteration 18/25 | Loss: 0.00229414
Iteration 19/25 | Loss: 0.00229414
Iteration 20/25 | Loss: 0.00229414
Iteration 21/25 | Loss: 0.00229414
Iteration 22/25 | Loss: 0.00229414
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0022941427305340767, 0.0022941427305340767, 0.0022941427305340767, 0.0022941427305340767, 0.0022941427305340767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022941427305340767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229414
Iteration 2/1000 | Loss: 0.00010235
Iteration 3/1000 | Loss: 0.00005659
Iteration 4/1000 | Loss: 0.00004442
Iteration 5/1000 | Loss: 0.00004008
Iteration 6/1000 | Loss: 0.00003794
Iteration 7/1000 | Loss: 0.00003621
Iteration 8/1000 | Loss: 0.00003522
Iteration 9/1000 | Loss: 0.00003453
Iteration 10/1000 | Loss: 0.00003403
Iteration 11/1000 | Loss: 0.00003369
Iteration 12/1000 | Loss: 0.00003338
Iteration 13/1000 | Loss: 0.00003312
Iteration 14/1000 | Loss: 0.00003293
Iteration 15/1000 | Loss: 0.00003287
Iteration 16/1000 | Loss: 0.00003286
Iteration 17/1000 | Loss: 0.00003286
Iteration 18/1000 | Loss: 0.00003286
Iteration 19/1000 | Loss: 0.00003277
Iteration 20/1000 | Loss: 0.00003277
Iteration 21/1000 | Loss: 0.00003272
Iteration 22/1000 | Loss: 0.00003272
Iteration 23/1000 | Loss: 0.00003269
Iteration 24/1000 | Loss: 0.00003269
Iteration 25/1000 | Loss: 0.00003265
Iteration 26/1000 | Loss: 0.00003265
Iteration 27/1000 | Loss: 0.00003264
Iteration 28/1000 | Loss: 0.00003264
Iteration 29/1000 | Loss: 0.00003263
Iteration 30/1000 | Loss: 0.00003258
Iteration 31/1000 | Loss: 0.00003258
Iteration 32/1000 | Loss: 0.00003257
Iteration 33/1000 | Loss: 0.00003256
Iteration 34/1000 | Loss: 0.00003256
Iteration 35/1000 | Loss: 0.00003256
Iteration 36/1000 | Loss: 0.00003255
Iteration 37/1000 | Loss: 0.00003255
Iteration 38/1000 | Loss: 0.00003255
Iteration 39/1000 | Loss: 0.00003255
Iteration 40/1000 | Loss: 0.00003255
Iteration 41/1000 | Loss: 0.00003254
Iteration 42/1000 | Loss: 0.00003254
Iteration 43/1000 | Loss: 0.00003254
Iteration 44/1000 | Loss: 0.00003253
Iteration 45/1000 | Loss: 0.00003253
Iteration 46/1000 | Loss: 0.00003252
Iteration 47/1000 | Loss: 0.00003252
Iteration 48/1000 | Loss: 0.00003252
Iteration 49/1000 | Loss: 0.00003252
Iteration 50/1000 | Loss: 0.00003252
Iteration 51/1000 | Loss: 0.00003252
Iteration 52/1000 | Loss: 0.00003252
Iteration 53/1000 | Loss: 0.00003252
Iteration 54/1000 | Loss: 0.00003251
Iteration 55/1000 | Loss: 0.00003251
Iteration 56/1000 | Loss: 0.00003251
Iteration 57/1000 | Loss: 0.00003251
Iteration 58/1000 | Loss: 0.00003251
Iteration 59/1000 | Loss: 0.00003250
Iteration 60/1000 | Loss: 0.00003250
Iteration 61/1000 | Loss: 0.00003250
Iteration 62/1000 | Loss: 0.00003250
Iteration 63/1000 | Loss: 0.00003250
Iteration 64/1000 | Loss: 0.00003249
Iteration 65/1000 | Loss: 0.00003249
Iteration 66/1000 | Loss: 0.00003249
Iteration 67/1000 | Loss: 0.00003249
Iteration 68/1000 | Loss: 0.00003249
Iteration 69/1000 | Loss: 0.00003249
Iteration 70/1000 | Loss: 0.00003249
Iteration 71/1000 | Loss: 0.00003249
Iteration 72/1000 | Loss: 0.00003249
Iteration 73/1000 | Loss: 0.00003249
Iteration 74/1000 | Loss: 0.00003248
Iteration 75/1000 | Loss: 0.00003248
Iteration 76/1000 | Loss: 0.00003248
Iteration 77/1000 | Loss: 0.00003248
Iteration 78/1000 | Loss: 0.00003248
Iteration 79/1000 | Loss: 0.00003247
Iteration 80/1000 | Loss: 0.00003247
Iteration 81/1000 | Loss: 0.00003247
Iteration 82/1000 | Loss: 0.00003247
Iteration 83/1000 | Loss: 0.00003247
Iteration 84/1000 | Loss: 0.00003247
Iteration 85/1000 | Loss: 0.00003247
Iteration 86/1000 | Loss: 0.00003247
Iteration 87/1000 | Loss: 0.00003247
Iteration 88/1000 | Loss: 0.00003247
Iteration 89/1000 | Loss: 0.00003247
Iteration 90/1000 | Loss: 0.00003247
Iteration 91/1000 | Loss: 0.00003247
Iteration 92/1000 | Loss: 0.00003247
Iteration 93/1000 | Loss: 0.00003246
Iteration 94/1000 | Loss: 0.00003246
Iteration 95/1000 | Loss: 0.00003246
Iteration 96/1000 | Loss: 0.00003246
Iteration 97/1000 | Loss: 0.00003246
Iteration 98/1000 | Loss: 0.00003246
Iteration 99/1000 | Loss: 0.00003246
Iteration 100/1000 | Loss: 0.00003246
Iteration 101/1000 | Loss: 0.00003246
Iteration 102/1000 | Loss: 0.00003246
Iteration 103/1000 | Loss: 0.00003245
Iteration 104/1000 | Loss: 0.00003245
Iteration 105/1000 | Loss: 0.00003245
Iteration 106/1000 | Loss: 0.00003244
Iteration 107/1000 | Loss: 0.00003244
Iteration 108/1000 | Loss: 0.00003244
Iteration 109/1000 | Loss: 0.00003244
Iteration 110/1000 | Loss: 0.00003244
Iteration 111/1000 | Loss: 0.00003244
Iteration 112/1000 | Loss: 0.00003244
Iteration 113/1000 | Loss: 0.00003244
Iteration 114/1000 | Loss: 0.00003244
Iteration 115/1000 | Loss: 0.00003243
Iteration 116/1000 | Loss: 0.00003243
Iteration 117/1000 | Loss: 0.00003243
Iteration 118/1000 | Loss: 0.00003243
Iteration 119/1000 | Loss: 0.00003243
Iteration 120/1000 | Loss: 0.00003243
Iteration 121/1000 | Loss: 0.00003242
Iteration 122/1000 | Loss: 0.00003242
Iteration 123/1000 | Loss: 0.00003242
Iteration 124/1000 | Loss: 0.00003241
Iteration 125/1000 | Loss: 0.00003241
Iteration 126/1000 | Loss: 0.00003241
Iteration 127/1000 | Loss: 0.00003241
Iteration 128/1000 | Loss: 0.00003241
Iteration 129/1000 | Loss: 0.00003241
Iteration 130/1000 | Loss: 0.00003241
Iteration 131/1000 | Loss: 0.00003241
Iteration 132/1000 | Loss: 0.00003240
Iteration 133/1000 | Loss: 0.00003240
Iteration 134/1000 | Loss: 0.00003240
Iteration 135/1000 | Loss: 0.00003240
Iteration 136/1000 | Loss: 0.00003240
Iteration 137/1000 | Loss: 0.00003240
Iteration 138/1000 | Loss: 0.00003240
Iteration 139/1000 | Loss: 0.00003240
Iteration 140/1000 | Loss: 0.00003240
Iteration 141/1000 | Loss: 0.00003239
Iteration 142/1000 | Loss: 0.00003239
Iteration 143/1000 | Loss: 0.00003239
Iteration 144/1000 | Loss: 0.00003238
Iteration 145/1000 | Loss: 0.00003238
Iteration 146/1000 | Loss: 0.00003238
Iteration 147/1000 | Loss: 0.00003238
Iteration 148/1000 | Loss: 0.00003238
Iteration 149/1000 | Loss: 0.00003238
Iteration 150/1000 | Loss: 0.00003238
Iteration 151/1000 | Loss: 0.00003238
Iteration 152/1000 | Loss: 0.00003238
Iteration 153/1000 | Loss: 0.00003238
Iteration 154/1000 | Loss: 0.00003238
Iteration 155/1000 | Loss: 0.00003238
Iteration 156/1000 | Loss: 0.00003238
Iteration 157/1000 | Loss: 0.00003238
Iteration 158/1000 | Loss: 0.00003238
Iteration 159/1000 | Loss: 0.00003238
Iteration 160/1000 | Loss: 0.00003238
Iteration 161/1000 | Loss: 0.00003238
Iteration 162/1000 | Loss: 0.00003238
Iteration 163/1000 | Loss: 0.00003238
Iteration 164/1000 | Loss: 0.00003238
Iteration 165/1000 | Loss: 0.00003238
Iteration 166/1000 | Loss: 0.00003238
Iteration 167/1000 | Loss: 0.00003238
Iteration 168/1000 | Loss: 0.00003238
Iteration 169/1000 | Loss: 0.00003238
Iteration 170/1000 | Loss: 0.00003238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [3.23772183037363e-05, 3.23772183037363e-05, 3.23772183037363e-05, 3.23772183037363e-05, 3.23772183037363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.23772183037363e-05

Optimization complete. Final v2v error: 4.844266891479492 mm

Highest mean error: 5.408257961273193 mm for frame 111

Lowest mean error: 4.3214521408081055 mm for frame 60

Saving results

Total time: 40.40692210197449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906864
Iteration 2/25 | Loss: 0.00156937
Iteration 3/25 | Loss: 0.00144767
Iteration 4/25 | Loss: 0.00143407
Iteration 5/25 | Loss: 0.00143029
Iteration 6/25 | Loss: 0.00142919
Iteration 7/25 | Loss: 0.00142919
Iteration 8/25 | Loss: 0.00142919
Iteration 9/25 | Loss: 0.00142919
Iteration 10/25 | Loss: 0.00142919
Iteration 11/25 | Loss: 0.00142919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014291857369244099, 0.0014291857369244099, 0.0014291857369244099, 0.0014291857369244099, 0.0014291857369244099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014291857369244099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18905807
Iteration 2/25 | Loss: 0.00219382
Iteration 3/25 | Loss: 0.00219382
Iteration 4/25 | Loss: 0.00219382
Iteration 5/25 | Loss: 0.00219382
Iteration 6/25 | Loss: 0.00219382
Iteration 7/25 | Loss: 0.00219382
Iteration 8/25 | Loss: 0.00219382
Iteration 9/25 | Loss: 0.00219382
Iteration 10/25 | Loss: 0.00219382
Iteration 11/25 | Loss: 0.00219382
Iteration 12/25 | Loss: 0.00219382
Iteration 13/25 | Loss: 0.00219382
Iteration 14/25 | Loss: 0.00219382
Iteration 15/25 | Loss: 0.00219382
Iteration 16/25 | Loss: 0.00219382
Iteration 17/25 | Loss: 0.00219382
Iteration 18/25 | Loss: 0.00219382
Iteration 19/25 | Loss: 0.00219382
Iteration 20/25 | Loss: 0.00219382
Iteration 21/25 | Loss: 0.00219382
Iteration 22/25 | Loss: 0.00219382
Iteration 23/25 | Loss: 0.00219382
Iteration 24/25 | Loss: 0.00219382
Iteration 25/25 | Loss: 0.00219382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219382
Iteration 2/1000 | Loss: 0.00003925
Iteration 3/1000 | Loss: 0.00002391
Iteration 4/1000 | Loss: 0.00002163
Iteration 5/1000 | Loss: 0.00002045
Iteration 6/1000 | Loss: 0.00001996
Iteration 7/1000 | Loss: 0.00001953
Iteration 8/1000 | Loss: 0.00001919
Iteration 9/1000 | Loss: 0.00001889
Iteration 10/1000 | Loss: 0.00001868
Iteration 11/1000 | Loss: 0.00001860
Iteration 12/1000 | Loss: 0.00001859
Iteration 13/1000 | Loss: 0.00001858
Iteration 14/1000 | Loss: 0.00001852
Iteration 15/1000 | Loss: 0.00001834
Iteration 16/1000 | Loss: 0.00001833
Iteration 17/1000 | Loss: 0.00001827
Iteration 18/1000 | Loss: 0.00001826
Iteration 19/1000 | Loss: 0.00001822
Iteration 20/1000 | Loss: 0.00001815
Iteration 21/1000 | Loss: 0.00001815
Iteration 22/1000 | Loss: 0.00001813
Iteration 23/1000 | Loss: 0.00001813
Iteration 24/1000 | Loss: 0.00001812
Iteration 25/1000 | Loss: 0.00001812
Iteration 26/1000 | Loss: 0.00001812
Iteration 27/1000 | Loss: 0.00001812
Iteration 28/1000 | Loss: 0.00001812
Iteration 29/1000 | Loss: 0.00001811
Iteration 30/1000 | Loss: 0.00001811
Iteration 31/1000 | Loss: 0.00001811
Iteration 32/1000 | Loss: 0.00001811
Iteration 33/1000 | Loss: 0.00001811
Iteration 34/1000 | Loss: 0.00001811
Iteration 35/1000 | Loss: 0.00001810
Iteration 36/1000 | Loss: 0.00001810
Iteration 37/1000 | Loss: 0.00001810
Iteration 38/1000 | Loss: 0.00001810
Iteration 39/1000 | Loss: 0.00001810
Iteration 40/1000 | Loss: 0.00001810
Iteration 41/1000 | Loss: 0.00001810
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001809
Iteration 44/1000 | Loss: 0.00001809
Iteration 45/1000 | Loss: 0.00001809
Iteration 46/1000 | Loss: 0.00001809
Iteration 47/1000 | Loss: 0.00001808
Iteration 48/1000 | Loss: 0.00001808
Iteration 49/1000 | Loss: 0.00001808
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001808
Iteration 52/1000 | Loss: 0.00001808
Iteration 53/1000 | Loss: 0.00001807
Iteration 54/1000 | Loss: 0.00001807
Iteration 55/1000 | Loss: 0.00001807
Iteration 56/1000 | Loss: 0.00001806
Iteration 57/1000 | Loss: 0.00001806
Iteration 58/1000 | Loss: 0.00001806
Iteration 59/1000 | Loss: 0.00001806
Iteration 60/1000 | Loss: 0.00001806
Iteration 61/1000 | Loss: 0.00001806
Iteration 62/1000 | Loss: 0.00001806
Iteration 63/1000 | Loss: 0.00001805
Iteration 64/1000 | Loss: 0.00001805
Iteration 65/1000 | Loss: 0.00001805
Iteration 66/1000 | Loss: 0.00001805
Iteration 67/1000 | Loss: 0.00001805
Iteration 68/1000 | Loss: 0.00001805
Iteration 69/1000 | Loss: 0.00001804
Iteration 70/1000 | Loss: 0.00001804
Iteration 71/1000 | Loss: 0.00001804
Iteration 72/1000 | Loss: 0.00001803
Iteration 73/1000 | Loss: 0.00001803
Iteration 74/1000 | Loss: 0.00001803
Iteration 75/1000 | Loss: 0.00001803
Iteration 76/1000 | Loss: 0.00001803
Iteration 77/1000 | Loss: 0.00001803
Iteration 78/1000 | Loss: 0.00001802
Iteration 79/1000 | Loss: 0.00001802
Iteration 80/1000 | Loss: 0.00001802
Iteration 81/1000 | Loss: 0.00001801
Iteration 82/1000 | Loss: 0.00001801
Iteration 83/1000 | Loss: 0.00001801
Iteration 84/1000 | Loss: 0.00001801
Iteration 85/1000 | Loss: 0.00001800
Iteration 86/1000 | Loss: 0.00001800
Iteration 87/1000 | Loss: 0.00001800
Iteration 88/1000 | Loss: 0.00001800
Iteration 89/1000 | Loss: 0.00001799
Iteration 90/1000 | Loss: 0.00001799
Iteration 91/1000 | Loss: 0.00001799
Iteration 92/1000 | Loss: 0.00001798
Iteration 93/1000 | Loss: 0.00001798
Iteration 94/1000 | Loss: 0.00001798
Iteration 95/1000 | Loss: 0.00001798
Iteration 96/1000 | Loss: 0.00001798
Iteration 97/1000 | Loss: 0.00001798
Iteration 98/1000 | Loss: 0.00001798
Iteration 99/1000 | Loss: 0.00001798
Iteration 100/1000 | Loss: 0.00001798
Iteration 101/1000 | Loss: 0.00001798
Iteration 102/1000 | Loss: 0.00001798
Iteration 103/1000 | Loss: 0.00001798
Iteration 104/1000 | Loss: 0.00001798
Iteration 105/1000 | Loss: 0.00001798
Iteration 106/1000 | Loss: 0.00001798
Iteration 107/1000 | Loss: 0.00001798
Iteration 108/1000 | Loss: 0.00001798
Iteration 109/1000 | Loss: 0.00001798
Iteration 110/1000 | Loss: 0.00001798
Iteration 111/1000 | Loss: 0.00001798
Iteration 112/1000 | Loss: 0.00001798
Iteration 113/1000 | Loss: 0.00001798
Iteration 114/1000 | Loss: 0.00001798
Iteration 115/1000 | Loss: 0.00001798
Iteration 116/1000 | Loss: 0.00001798
Iteration 117/1000 | Loss: 0.00001798
Iteration 118/1000 | Loss: 0.00001798
Iteration 119/1000 | Loss: 0.00001798
Iteration 120/1000 | Loss: 0.00001798
Iteration 121/1000 | Loss: 0.00001798
Iteration 122/1000 | Loss: 0.00001798
Iteration 123/1000 | Loss: 0.00001798
Iteration 124/1000 | Loss: 0.00001798
Iteration 125/1000 | Loss: 0.00001798
Iteration 126/1000 | Loss: 0.00001798
Iteration 127/1000 | Loss: 0.00001798
Iteration 128/1000 | Loss: 0.00001798
Iteration 129/1000 | Loss: 0.00001798
Iteration 130/1000 | Loss: 0.00001798
Iteration 131/1000 | Loss: 0.00001798
Iteration 132/1000 | Loss: 0.00001798
Iteration 133/1000 | Loss: 0.00001798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.7980277334572747e-05, 1.7980277334572747e-05, 1.7980277334572747e-05, 1.7980277334572747e-05, 1.7980277334572747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7980277334572747e-05

Optimization complete. Final v2v error: 3.6627564430236816 mm

Highest mean error: 4.124775409698486 mm for frame 78

Lowest mean error: 3.1428864002227783 mm for frame 17

Saving results

Total time: 32.79815435409546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912901
Iteration 2/25 | Loss: 0.00150243
Iteration 3/25 | Loss: 0.00139093
Iteration 4/25 | Loss: 0.00137569
Iteration 5/25 | Loss: 0.00137091
Iteration 6/25 | Loss: 0.00136943
Iteration 7/25 | Loss: 0.00136943
Iteration 8/25 | Loss: 0.00136936
Iteration 9/25 | Loss: 0.00136936
Iteration 10/25 | Loss: 0.00136936
Iteration 11/25 | Loss: 0.00136936
Iteration 12/25 | Loss: 0.00136936
Iteration 13/25 | Loss: 0.00136936
Iteration 14/25 | Loss: 0.00136936
Iteration 15/25 | Loss: 0.00136936
Iteration 16/25 | Loss: 0.00136936
Iteration 17/25 | Loss: 0.00136936
Iteration 18/25 | Loss: 0.00136936
Iteration 19/25 | Loss: 0.00136936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013693585060536861, 0.0013693585060536861, 0.0013693585060536861, 0.0013693585060536861, 0.0013693585060536861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013693585060536861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.05986881
Iteration 2/25 | Loss: 0.00186096
Iteration 3/25 | Loss: 0.00186095
Iteration 4/25 | Loss: 0.00186095
Iteration 5/25 | Loss: 0.00186095
Iteration 6/25 | Loss: 0.00186095
Iteration 7/25 | Loss: 0.00186095
Iteration 8/25 | Loss: 0.00186095
Iteration 9/25 | Loss: 0.00186095
Iteration 10/25 | Loss: 0.00186095
Iteration 11/25 | Loss: 0.00186095
Iteration 12/25 | Loss: 0.00186095
Iteration 13/25 | Loss: 0.00186095
Iteration 14/25 | Loss: 0.00186095
Iteration 15/25 | Loss: 0.00186095
Iteration 16/25 | Loss: 0.00186095
Iteration 17/25 | Loss: 0.00186095
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018609522376209497, 0.0018609522376209497, 0.0018609522376209497, 0.0018609522376209497, 0.0018609522376209497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018609522376209497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186095
Iteration 2/1000 | Loss: 0.00003735
Iteration 3/1000 | Loss: 0.00002438
Iteration 4/1000 | Loss: 0.00002228
Iteration 5/1000 | Loss: 0.00002098
Iteration 6/1000 | Loss: 0.00002028
Iteration 7/1000 | Loss: 0.00001970
Iteration 8/1000 | Loss: 0.00001943
Iteration 9/1000 | Loss: 0.00001929
Iteration 10/1000 | Loss: 0.00001911
Iteration 11/1000 | Loss: 0.00001905
Iteration 12/1000 | Loss: 0.00001903
Iteration 13/1000 | Loss: 0.00001903
Iteration 14/1000 | Loss: 0.00001902
Iteration 15/1000 | Loss: 0.00001901
Iteration 16/1000 | Loss: 0.00001901
Iteration 17/1000 | Loss: 0.00001900
Iteration 18/1000 | Loss: 0.00001897
Iteration 19/1000 | Loss: 0.00001886
Iteration 20/1000 | Loss: 0.00001884
Iteration 21/1000 | Loss: 0.00001882
Iteration 22/1000 | Loss: 0.00001878
Iteration 23/1000 | Loss: 0.00001878
Iteration 24/1000 | Loss: 0.00001878
Iteration 25/1000 | Loss: 0.00001878
Iteration 26/1000 | Loss: 0.00001878
Iteration 27/1000 | Loss: 0.00001877
Iteration 28/1000 | Loss: 0.00001877
Iteration 29/1000 | Loss: 0.00001877
Iteration 30/1000 | Loss: 0.00001877
Iteration 31/1000 | Loss: 0.00001877
Iteration 32/1000 | Loss: 0.00001873
Iteration 33/1000 | Loss: 0.00001873
Iteration 34/1000 | Loss: 0.00001873
Iteration 35/1000 | Loss: 0.00001873
Iteration 36/1000 | Loss: 0.00001872
Iteration 37/1000 | Loss: 0.00001872
Iteration 38/1000 | Loss: 0.00001871
Iteration 39/1000 | Loss: 0.00001871
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001870
Iteration 42/1000 | Loss: 0.00001870
Iteration 43/1000 | Loss: 0.00001870
Iteration 44/1000 | Loss: 0.00001870
Iteration 45/1000 | Loss: 0.00001869
Iteration 46/1000 | Loss: 0.00001869
Iteration 47/1000 | Loss: 0.00001869
Iteration 48/1000 | Loss: 0.00001869
Iteration 49/1000 | Loss: 0.00001869
Iteration 50/1000 | Loss: 0.00001869
Iteration 51/1000 | Loss: 0.00001869
Iteration 52/1000 | Loss: 0.00001869
Iteration 53/1000 | Loss: 0.00001869
Iteration 54/1000 | Loss: 0.00001868
Iteration 55/1000 | Loss: 0.00001868
Iteration 56/1000 | Loss: 0.00001868
Iteration 57/1000 | Loss: 0.00001868
Iteration 58/1000 | Loss: 0.00001867
Iteration 59/1000 | Loss: 0.00001867
Iteration 60/1000 | Loss: 0.00001867
Iteration 61/1000 | Loss: 0.00001867
Iteration 62/1000 | Loss: 0.00001867
Iteration 63/1000 | Loss: 0.00001867
Iteration 64/1000 | Loss: 0.00001867
Iteration 65/1000 | Loss: 0.00001867
Iteration 66/1000 | Loss: 0.00001867
Iteration 67/1000 | Loss: 0.00001867
Iteration 68/1000 | Loss: 0.00001867
Iteration 69/1000 | Loss: 0.00001867
Iteration 70/1000 | Loss: 0.00001867
Iteration 71/1000 | Loss: 0.00001867
Iteration 72/1000 | Loss: 0.00001867
Iteration 73/1000 | Loss: 0.00001867
Iteration 74/1000 | Loss: 0.00001867
Iteration 75/1000 | Loss: 0.00001867
Iteration 76/1000 | Loss: 0.00001867
Iteration 77/1000 | Loss: 0.00001867
Iteration 78/1000 | Loss: 0.00001866
Iteration 79/1000 | Loss: 0.00001866
Iteration 80/1000 | Loss: 0.00001866
Iteration 81/1000 | Loss: 0.00001866
Iteration 82/1000 | Loss: 0.00001866
Iteration 83/1000 | Loss: 0.00001866
Iteration 84/1000 | Loss: 0.00001866
Iteration 85/1000 | Loss: 0.00001866
Iteration 86/1000 | Loss: 0.00001866
Iteration 87/1000 | Loss: 0.00001866
Iteration 88/1000 | Loss: 0.00001866
Iteration 89/1000 | Loss: 0.00001866
Iteration 90/1000 | Loss: 0.00001866
Iteration 91/1000 | Loss: 0.00001866
Iteration 92/1000 | Loss: 0.00001865
Iteration 93/1000 | Loss: 0.00001865
Iteration 94/1000 | Loss: 0.00001865
Iteration 95/1000 | Loss: 0.00001865
Iteration 96/1000 | Loss: 0.00001865
Iteration 97/1000 | Loss: 0.00001865
Iteration 98/1000 | Loss: 0.00001865
Iteration 99/1000 | Loss: 0.00001865
Iteration 100/1000 | Loss: 0.00001865
Iteration 101/1000 | Loss: 0.00001865
Iteration 102/1000 | Loss: 0.00001865
Iteration 103/1000 | Loss: 0.00001865
Iteration 104/1000 | Loss: 0.00001865
Iteration 105/1000 | Loss: 0.00001865
Iteration 106/1000 | Loss: 0.00001865
Iteration 107/1000 | Loss: 0.00001865
Iteration 108/1000 | Loss: 0.00001865
Iteration 109/1000 | Loss: 0.00001865
Iteration 110/1000 | Loss: 0.00001865
Iteration 111/1000 | Loss: 0.00001864
Iteration 112/1000 | Loss: 0.00001864
Iteration 113/1000 | Loss: 0.00001864
Iteration 114/1000 | Loss: 0.00001864
Iteration 115/1000 | Loss: 0.00001864
Iteration 116/1000 | Loss: 0.00001864
Iteration 117/1000 | Loss: 0.00001864
Iteration 118/1000 | Loss: 0.00001864
Iteration 119/1000 | Loss: 0.00001864
Iteration 120/1000 | Loss: 0.00001864
Iteration 121/1000 | Loss: 0.00001864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.8639095287653618e-05, 1.8639095287653618e-05, 1.8639095287653618e-05, 1.8639095287653618e-05, 1.8639095287653618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8639095287653618e-05

Optimization complete. Final v2v error: 3.613652467727661 mm

Highest mean error: 4.297515869140625 mm for frame 89

Lowest mean error: 3.124429702758789 mm for frame 5

Saving results

Total time: 31.97561478614807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468971
Iteration 2/25 | Loss: 0.00148041
Iteration 3/25 | Loss: 0.00138916
Iteration 4/25 | Loss: 0.00136778
Iteration 5/25 | Loss: 0.00135934
Iteration 6/25 | Loss: 0.00135787
Iteration 7/25 | Loss: 0.00135787
Iteration 8/25 | Loss: 0.00135787
Iteration 9/25 | Loss: 0.00135787
Iteration 10/25 | Loss: 0.00135787
Iteration 11/25 | Loss: 0.00135787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001357868779450655, 0.001357868779450655, 0.001357868779450655, 0.001357868779450655, 0.001357868779450655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001357868779450655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36094284
Iteration 2/25 | Loss: 0.00201685
Iteration 3/25 | Loss: 0.00201685
Iteration 4/25 | Loss: 0.00201684
Iteration 5/25 | Loss: 0.00201684
Iteration 6/25 | Loss: 0.00201684
Iteration 7/25 | Loss: 0.00201684
Iteration 8/25 | Loss: 0.00201684
Iteration 9/25 | Loss: 0.00201684
Iteration 10/25 | Loss: 0.00201684
Iteration 11/25 | Loss: 0.00201684
Iteration 12/25 | Loss: 0.00201684
Iteration 13/25 | Loss: 0.00201684
Iteration 14/25 | Loss: 0.00201684
Iteration 15/25 | Loss: 0.00201684
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00201684283092618, 0.00201684283092618, 0.00201684283092618, 0.00201684283092618, 0.00201684283092618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00201684283092618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201684
Iteration 2/1000 | Loss: 0.00003690
Iteration 3/1000 | Loss: 0.00002517
Iteration 4/1000 | Loss: 0.00002283
Iteration 5/1000 | Loss: 0.00002113
Iteration 6/1000 | Loss: 0.00002020
Iteration 7/1000 | Loss: 0.00001944
Iteration 8/1000 | Loss: 0.00001910
Iteration 9/1000 | Loss: 0.00001900
Iteration 10/1000 | Loss: 0.00001869
Iteration 11/1000 | Loss: 0.00001853
Iteration 12/1000 | Loss: 0.00001839
Iteration 13/1000 | Loss: 0.00001828
Iteration 14/1000 | Loss: 0.00001822
Iteration 15/1000 | Loss: 0.00001820
Iteration 16/1000 | Loss: 0.00001820
Iteration 17/1000 | Loss: 0.00001819
Iteration 18/1000 | Loss: 0.00001819
Iteration 19/1000 | Loss: 0.00001819
Iteration 20/1000 | Loss: 0.00001817
Iteration 21/1000 | Loss: 0.00001814
Iteration 22/1000 | Loss: 0.00001813
Iteration 23/1000 | Loss: 0.00001813
Iteration 24/1000 | Loss: 0.00001812
Iteration 25/1000 | Loss: 0.00001812
Iteration 26/1000 | Loss: 0.00001811
Iteration 27/1000 | Loss: 0.00001810
Iteration 28/1000 | Loss: 0.00001810
Iteration 29/1000 | Loss: 0.00001809
Iteration 30/1000 | Loss: 0.00001808
Iteration 31/1000 | Loss: 0.00001808
Iteration 32/1000 | Loss: 0.00001808
Iteration 33/1000 | Loss: 0.00001808
Iteration 34/1000 | Loss: 0.00001807
Iteration 35/1000 | Loss: 0.00001807
Iteration 36/1000 | Loss: 0.00001807
Iteration 37/1000 | Loss: 0.00001806
Iteration 38/1000 | Loss: 0.00001806
Iteration 39/1000 | Loss: 0.00001805
Iteration 40/1000 | Loss: 0.00001805
Iteration 41/1000 | Loss: 0.00001804
Iteration 42/1000 | Loss: 0.00001804
Iteration 43/1000 | Loss: 0.00001804
Iteration 44/1000 | Loss: 0.00001804
Iteration 45/1000 | Loss: 0.00001803
Iteration 46/1000 | Loss: 0.00001803
Iteration 47/1000 | Loss: 0.00001803
Iteration 48/1000 | Loss: 0.00001803
Iteration 49/1000 | Loss: 0.00001803
Iteration 50/1000 | Loss: 0.00001803
Iteration 51/1000 | Loss: 0.00001802
Iteration 52/1000 | Loss: 0.00001802
Iteration 53/1000 | Loss: 0.00001802
Iteration 54/1000 | Loss: 0.00001802
Iteration 55/1000 | Loss: 0.00001802
Iteration 56/1000 | Loss: 0.00001801
Iteration 57/1000 | Loss: 0.00001801
Iteration 58/1000 | Loss: 0.00001801
Iteration 59/1000 | Loss: 0.00001801
Iteration 60/1000 | Loss: 0.00001801
Iteration 61/1000 | Loss: 0.00001801
Iteration 62/1000 | Loss: 0.00001801
Iteration 63/1000 | Loss: 0.00001801
Iteration 64/1000 | Loss: 0.00001801
Iteration 65/1000 | Loss: 0.00001801
Iteration 66/1000 | Loss: 0.00001801
Iteration 67/1000 | Loss: 0.00001801
Iteration 68/1000 | Loss: 0.00001801
Iteration 69/1000 | Loss: 0.00001800
Iteration 70/1000 | Loss: 0.00001800
Iteration 71/1000 | Loss: 0.00001800
Iteration 72/1000 | Loss: 0.00001800
Iteration 73/1000 | Loss: 0.00001800
Iteration 74/1000 | Loss: 0.00001800
Iteration 75/1000 | Loss: 0.00001800
Iteration 76/1000 | Loss: 0.00001800
Iteration 77/1000 | Loss: 0.00001799
Iteration 78/1000 | Loss: 0.00001799
Iteration 79/1000 | Loss: 0.00001799
Iteration 80/1000 | Loss: 0.00001799
Iteration 81/1000 | Loss: 0.00001799
Iteration 82/1000 | Loss: 0.00001799
Iteration 83/1000 | Loss: 0.00001798
Iteration 84/1000 | Loss: 0.00001798
Iteration 85/1000 | Loss: 0.00001798
Iteration 86/1000 | Loss: 0.00001798
Iteration 87/1000 | Loss: 0.00001798
Iteration 88/1000 | Loss: 0.00001797
Iteration 89/1000 | Loss: 0.00001797
Iteration 90/1000 | Loss: 0.00001797
Iteration 91/1000 | Loss: 0.00001797
Iteration 92/1000 | Loss: 0.00001797
Iteration 93/1000 | Loss: 0.00001797
Iteration 94/1000 | Loss: 0.00001797
Iteration 95/1000 | Loss: 0.00001796
Iteration 96/1000 | Loss: 0.00001796
Iteration 97/1000 | Loss: 0.00001796
Iteration 98/1000 | Loss: 0.00001796
Iteration 99/1000 | Loss: 0.00001796
Iteration 100/1000 | Loss: 0.00001796
Iteration 101/1000 | Loss: 0.00001796
Iteration 102/1000 | Loss: 0.00001796
Iteration 103/1000 | Loss: 0.00001796
Iteration 104/1000 | Loss: 0.00001796
Iteration 105/1000 | Loss: 0.00001795
Iteration 106/1000 | Loss: 0.00001795
Iteration 107/1000 | Loss: 0.00001794
Iteration 108/1000 | Loss: 0.00001794
Iteration 109/1000 | Loss: 0.00001794
Iteration 110/1000 | Loss: 0.00001794
Iteration 111/1000 | Loss: 0.00001794
Iteration 112/1000 | Loss: 0.00001794
Iteration 113/1000 | Loss: 0.00001793
Iteration 114/1000 | Loss: 0.00001793
Iteration 115/1000 | Loss: 0.00001793
Iteration 116/1000 | Loss: 0.00001793
Iteration 117/1000 | Loss: 0.00001793
Iteration 118/1000 | Loss: 0.00001793
Iteration 119/1000 | Loss: 0.00001793
Iteration 120/1000 | Loss: 0.00001793
Iteration 121/1000 | Loss: 0.00001793
Iteration 122/1000 | Loss: 0.00001793
Iteration 123/1000 | Loss: 0.00001793
Iteration 124/1000 | Loss: 0.00001793
Iteration 125/1000 | Loss: 0.00001793
Iteration 126/1000 | Loss: 0.00001793
Iteration 127/1000 | Loss: 0.00001793
Iteration 128/1000 | Loss: 0.00001792
Iteration 129/1000 | Loss: 0.00001792
Iteration 130/1000 | Loss: 0.00001792
Iteration 131/1000 | Loss: 0.00001792
Iteration 132/1000 | Loss: 0.00001792
Iteration 133/1000 | Loss: 0.00001792
Iteration 134/1000 | Loss: 0.00001791
Iteration 135/1000 | Loss: 0.00001791
Iteration 136/1000 | Loss: 0.00001791
Iteration 137/1000 | Loss: 0.00001791
Iteration 138/1000 | Loss: 0.00001791
Iteration 139/1000 | Loss: 0.00001791
Iteration 140/1000 | Loss: 0.00001791
Iteration 141/1000 | Loss: 0.00001791
Iteration 142/1000 | Loss: 0.00001791
Iteration 143/1000 | Loss: 0.00001791
Iteration 144/1000 | Loss: 0.00001791
Iteration 145/1000 | Loss: 0.00001791
Iteration 146/1000 | Loss: 0.00001790
Iteration 147/1000 | Loss: 0.00001790
Iteration 148/1000 | Loss: 0.00001790
Iteration 149/1000 | Loss: 0.00001790
Iteration 150/1000 | Loss: 0.00001790
Iteration 151/1000 | Loss: 0.00001790
Iteration 152/1000 | Loss: 0.00001790
Iteration 153/1000 | Loss: 0.00001790
Iteration 154/1000 | Loss: 0.00001790
Iteration 155/1000 | Loss: 0.00001790
Iteration 156/1000 | Loss: 0.00001790
Iteration 157/1000 | Loss: 0.00001789
Iteration 158/1000 | Loss: 0.00001789
Iteration 159/1000 | Loss: 0.00001789
Iteration 160/1000 | Loss: 0.00001789
Iteration 161/1000 | Loss: 0.00001789
Iteration 162/1000 | Loss: 0.00001789
Iteration 163/1000 | Loss: 0.00001789
Iteration 164/1000 | Loss: 0.00001789
Iteration 165/1000 | Loss: 0.00001789
Iteration 166/1000 | Loss: 0.00001789
Iteration 167/1000 | Loss: 0.00001789
Iteration 168/1000 | Loss: 0.00001789
Iteration 169/1000 | Loss: 0.00001789
Iteration 170/1000 | Loss: 0.00001788
Iteration 171/1000 | Loss: 0.00001788
Iteration 172/1000 | Loss: 0.00001788
Iteration 173/1000 | Loss: 0.00001788
Iteration 174/1000 | Loss: 0.00001788
Iteration 175/1000 | Loss: 0.00001788
Iteration 176/1000 | Loss: 0.00001788
Iteration 177/1000 | Loss: 0.00001788
Iteration 178/1000 | Loss: 0.00001788
Iteration 179/1000 | Loss: 0.00001788
Iteration 180/1000 | Loss: 0.00001788
Iteration 181/1000 | Loss: 0.00001787
Iteration 182/1000 | Loss: 0.00001787
Iteration 183/1000 | Loss: 0.00001787
Iteration 184/1000 | Loss: 0.00001787
Iteration 185/1000 | Loss: 0.00001787
Iteration 186/1000 | Loss: 0.00001786
Iteration 187/1000 | Loss: 0.00001786
Iteration 188/1000 | Loss: 0.00001786
Iteration 189/1000 | Loss: 0.00001786
Iteration 190/1000 | Loss: 0.00001786
Iteration 191/1000 | Loss: 0.00001786
Iteration 192/1000 | Loss: 0.00001786
Iteration 193/1000 | Loss: 0.00001786
Iteration 194/1000 | Loss: 0.00001786
Iteration 195/1000 | Loss: 0.00001786
Iteration 196/1000 | Loss: 0.00001786
Iteration 197/1000 | Loss: 0.00001786
Iteration 198/1000 | Loss: 0.00001786
Iteration 199/1000 | Loss: 0.00001786
Iteration 200/1000 | Loss: 0.00001785
Iteration 201/1000 | Loss: 0.00001785
Iteration 202/1000 | Loss: 0.00001785
Iteration 203/1000 | Loss: 0.00001785
Iteration 204/1000 | Loss: 0.00001785
Iteration 205/1000 | Loss: 0.00001785
Iteration 206/1000 | Loss: 0.00001785
Iteration 207/1000 | Loss: 0.00001785
Iteration 208/1000 | Loss: 0.00001785
Iteration 209/1000 | Loss: 0.00001785
Iteration 210/1000 | Loss: 0.00001785
Iteration 211/1000 | Loss: 0.00001785
Iteration 212/1000 | Loss: 0.00001785
Iteration 213/1000 | Loss: 0.00001785
Iteration 214/1000 | Loss: 0.00001785
Iteration 215/1000 | Loss: 0.00001785
Iteration 216/1000 | Loss: 0.00001785
Iteration 217/1000 | Loss: 0.00001785
Iteration 218/1000 | Loss: 0.00001785
Iteration 219/1000 | Loss: 0.00001785
Iteration 220/1000 | Loss: 0.00001785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.7848355128080584e-05, 1.7848355128080584e-05, 1.7848355128080584e-05, 1.7848355128080584e-05, 1.7848355128080584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7848355128080584e-05

Optimization complete. Final v2v error: 3.5629642009735107 mm

Highest mean error: 3.7166264057159424 mm for frame 111

Lowest mean error: 3.338188886642456 mm for frame 95

Saving results

Total time: 38.042738914489746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957324
Iteration 2/25 | Loss: 0.00957324
Iteration 3/25 | Loss: 0.00957323
Iteration 4/25 | Loss: 0.00957323
Iteration 5/25 | Loss: 0.00957323
Iteration 6/25 | Loss: 0.00957323
Iteration 7/25 | Loss: 0.00957323
Iteration 8/25 | Loss: 0.00957323
Iteration 9/25 | Loss: 0.00957323
Iteration 10/25 | Loss: 0.00957323
Iteration 11/25 | Loss: 0.00957323
Iteration 12/25 | Loss: 0.00957323
Iteration 13/25 | Loss: 0.00957322
Iteration 14/25 | Loss: 0.00957322
Iteration 15/25 | Loss: 0.00957322
Iteration 16/25 | Loss: 0.00957322
Iteration 17/25 | Loss: 0.00957322
Iteration 18/25 | Loss: 0.00957322
Iteration 19/25 | Loss: 0.00957322
Iteration 20/25 | Loss: 0.00957322
Iteration 21/25 | Loss: 0.00957321
Iteration 22/25 | Loss: 0.00957321
Iteration 23/25 | Loss: 0.00957321
Iteration 24/25 | Loss: 0.00957321
Iteration 25/25 | Loss: 0.00957321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32774186
Iteration 2/25 | Loss: 0.18513924
Iteration 3/25 | Loss: 0.18500970
Iteration 4/25 | Loss: 0.18381454
Iteration 5/25 | Loss: 0.18381575
Iteration 6/25 | Loss: 0.18366824
Iteration 7/25 | Loss: 0.18366823
Iteration 8/25 | Loss: 0.18366821
Iteration 9/25 | Loss: 0.18366818
Iteration 10/25 | Loss: 0.18366818
Iteration 11/25 | Loss: 0.18366821
Iteration 12/25 | Loss: 0.18366821
Iteration 13/25 | Loss: 0.18366818
Iteration 14/25 | Loss: 0.18366818
Iteration 15/25 | Loss: 0.18366818
Iteration 16/25 | Loss: 0.18366818
Iteration 17/25 | Loss: 0.18366818
Iteration 18/25 | Loss: 0.18366818
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.18366818130016327, 0.18366818130016327, 0.18366818130016327, 0.18366818130016327, 0.18366818130016327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18366818130016327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18366818
Iteration 2/1000 | Loss: 0.00810096
Iteration 3/1000 | Loss: 0.00441339
Iteration 4/1000 | Loss: 0.00262813
Iteration 5/1000 | Loss: 0.00108582
Iteration 6/1000 | Loss: 0.00070784
Iteration 7/1000 | Loss: 0.00204473
Iteration 8/1000 | Loss: 0.00038384
Iteration 9/1000 | Loss: 0.00039791
Iteration 10/1000 | Loss: 0.00024054
Iteration 11/1000 | Loss: 0.00099914
Iteration 12/1000 | Loss: 0.00061531
Iteration 13/1000 | Loss: 0.00016195
Iteration 14/1000 | Loss: 0.00089964
Iteration 15/1000 | Loss: 0.00013831
Iteration 16/1000 | Loss: 0.00012139
Iteration 17/1000 | Loss: 0.00010785
Iteration 18/1000 | Loss: 0.00011616
Iteration 19/1000 | Loss: 0.00020960
Iteration 20/1000 | Loss: 0.00009037
Iteration 21/1000 | Loss: 0.00141230
Iteration 22/1000 | Loss: 0.00047298
Iteration 23/1000 | Loss: 0.00026654
Iteration 24/1000 | Loss: 0.00024545
Iteration 25/1000 | Loss: 0.00016860
Iteration 26/1000 | Loss: 0.00027602
Iteration 27/1000 | Loss: 0.00015859
Iteration 28/1000 | Loss: 0.00017097
Iteration 29/1000 | Loss: 0.00011095
Iteration 30/1000 | Loss: 0.00062916
Iteration 31/1000 | Loss: 0.00176157
Iteration 32/1000 | Loss: 0.00104457
Iteration 33/1000 | Loss: 0.00037307
Iteration 34/1000 | Loss: 0.00054331
Iteration 35/1000 | Loss: 0.00045588
Iteration 36/1000 | Loss: 0.00010440
Iteration 37/1000 | Loss: 0.00008884
Iteration 38/1000 | Loss: 0.00016290
Iteration 39/1000 | Loss: 0.00007587
Iteration 40/1000 | Loss: 0.00007055
Iteration 41/1000 | Loss: 0.00015352
Iteration 42/1000 | Loss: 0.00006482
Iteration 43/1000 | Loss: 0.00013842
Iteration 44/1000 | Loss: 0.00006131
Iteration 45/1000 | Loss: 0.00005939
Iteration 46/1000 | Loss: 0.00009655
Iteration 47/1000 | Loss: 0.00017519
Iteration 48/1000 | Loss: 0.00014298
Iteration 49/1000 | Loss: 0.00104453
Iteration 50/1000 | Loss: 0.00095159
Iteration 51/1000 | Loss: 0.00023189
Iteration 52/1000 | Loss: 0.00027719
Iteration 53/1000 | Loss: 0.00024164
Iteration 54/1000 | Loss: 0.00014780
Iteration 55/1000 | Loss: 0.00007893
Iteration 56/1000 | Loss: 0.00006591
Iteration 57/1000 | Loss: 0.00005908
Iteration 58/1000 | Loss: 0.00012016
Iteration 59/1000 | Loss: 0.00028950
Iteration 60/1000 | Loss: 0.00005371
Iteration 61/1000 | Loss: 0.00005086
Iteration 62/1000 | Loss: 0.00014081
Iteration 63/1000 | Loss: 0.00017572
Iteration 64/1000 | Loss: 0.00038695
Iteration 65/1000 | Loss: 0.00026021
Iteration 66/1000 | Loss: 0.00012703
Iteration 67/1000 | Loss: 0.00008945
Iteration 68/1000 | Loss: 0.00005074
Iteration 69/1000 | Loss: 0.00007244
Iteration 70/1000 | Loss: 0.00006432
Iteration 71/1000 | Loss: 0.00013896
Iteration 72/1000 | Loss: 0.00004984
Iteration 73/1000 | Loss: 0.00005000
Iteration 74/1000 | Loss: 0.00004613
Iteration 75/1000 | Loss: 0.00006297
Iteration 76/1000 | Loss: 0.00011734
Iteration 77/1000 | Loss: 0.00005584
Iteration 78/1000 | Loss: 0.00004575
Iteration 79/1000 | Loss: 0.00004521
Iteration 80/1000 | Loss: 0.00005014
Iteration 81/1000 | Loss: 0.00004471
Iteration 82/1000 | Loss: 0.00004458
Iteration 83/1000 | Loss: 0.00007266
Iteration 84/1000 | Loss: 0.00004467
Iteration 85/1000 | Loss: 0.00004418
Iteration 86/1000 | Loss: 0.00004412
Iteration 87/1000 | Loss: 0.00004408
Iteration 88/1000 | Loss: 0.00008346
Iteration 89/1000 | Loss: 0.00005218
Iteration 90/1000 | Loss: 0.00004678
Iteration 91/1000 | Loss: 0.00006232
Iteration 92/1000 | Loss: 0.00004716
Iteration 93/1000 | Loss: 0.00011134
Iteration 94/1000 | Loss: 0.00010456
Iteration 95/1000 | Loss: 0.00004385
Iteration 96/1000 | Loss: 0.00004385
Iteration 97/1000 | Loss: 0.00004384
Iteration 98/1000 | Loss: 0.00004384
Iteration 99/1000 | Loss: 0.00004822
Iteration 100/1000 | Loss: 0.00004820
Iteration 101/1000 | Loss: 0.00012117
Iteration 102/1000 | Loss: 0.00006931
Iteration 103/1000 | Loss: 0.00006147
Iteration 104/1000 | Loss: 0.00004448
Iteration 105/1000 | Loss: 0.00009148
Iteration 106/1000 | Loss: 0.00004412
Iteration 107/1000 | Loss: 0.00004394
Iteration 108/1000 | Loss: 0.00004385
Iteration 109/1000 | Loss: 0.00004384
Iteration 110/1000 | Loss: 0.00004381
Iteration 111/1000 | Loss: 0.00004380
Iteration 112/1000 | Loss: 0.00004380
Iteration 113/1000 | Loss: 0.00004378
Iteration 114/1000 | Loss: 0.00004377
Iteration 115/1000 | Loss: 0.00004377
Iteration 116/1000 | Loss: 0.00004376
Iteration 117/1000 | Loss: 0.00004375
Iteration 118/1000 | Loss: 0.00004375
Iteration 119/1000 | Loss: 0.00004374
Iteration 120/1000 | Loss: 0.00004373
Iteration 121/1000 | Loss: 0.00004373
Iteration 122/1000 | Loss: 0.00004373
Iteration 123/1000 | Loss: 0.00004373
Iteration 124/1000 | Loss: 0.00004373
Iteration 125/1000 | Loss: 0.00004372
Iteration 126/1000 | Loss: 0.00004372
Iteration 127/1000 | Loss: 0.00004372
Iteration 128/1000 | Loss: 0.00004371
Iteration 129/1000 | Loss: 0.00004371
Iteration 130/1000 | Loss: 0.00004371
Iteration 131/1000 | Loss: 0.00004370
Iteration 132/1000 | Loss: 0.00004368
Iteration 133/1000 | Loss: 0.00004367
Iteration 134/1000 | Loss: 0.00004366
Iteration 135/1000 | Loss: 0.00004365
Iteration 136/1000 | Loss: 0.00004365
Iteration 137/1000 | Loss: 0.00004364
Iteration 138/1000 | Loss: 0.00004364
Iteration 139/1000 | Loss: 0.00004363
Iteration 140/1000 | Loss: 0.00004363
Iteration 141/1000 | Loss: 0.00004363
Iteration 142/1000 | Loss: 0.00004362
Iteration 143/1000 | Loss: 0.00004362
Iteration 144/1000 | Loss: 0.00004362
Iteration 145/1000 | Loss: 0.00004361
Iteration 146/1000 | Loss: 0.00004361
Iteration 147/1000 | Loss: 0.00004361
Iteration 148/1000 | Loss: 0.00004361
Iteration 149/1000 | Loss: 0.00004361
Iteration 150/1000 | Loss: 0.00004361
Iteration 151/1000 | Loss: 0.00004360
Iteration 152/1000 | Loss: 0.00004360
Iteration 153/1000 | Loss: 0.00004360
Iteration 154/1000 | Loss: 0.00004360
Iteration 155/1000 | Loss: 0.00004360
Iteration 156/1000 | Loss: 0.00004360
Iteration 157/1000 | Loss: 0.00004360
Iteration 158/1000 | Loss: 0.00004360
Iteration 159/1000 | Loss: 0.00004360
Iteration 160/1000 | Loss: 0.00004360
Iteration 161/1000 | Loss: 0.00004360
Iteration 162/1000 | Loss: 0.00004360
Iteration 163/1000 | Loss: 0.00004360
Iteration 164/1000 | Loss: 0.00004360
Iteration 165/1000 | Loss: 0.00004360
Iteration 166/1000 | Loss: 0.00004360
Iteration 167/1000 | Loss: 0.00004360
Iteration 168/1000 | Loss: 0.00004360
Iteration 169/1000 | Loss: 0.00004360
Iteration 170/1000 | Loss: 0.00004360
Iteration 171/1000 | Loss: 0.00004360
Iteration 172/1000 | Loss: 0.00004360
Iteration 173/1000 | Loss: 0.00004360
Iteration 174/1000 | Loss: 0.00004360
Iteration 175/1000 | Loss: 0.00004360
Iteration 176/1000 | Loss: 0.00004360
Iteration 177/1000 | Loss: 0.00004360
Iteration 178/1000 | Loss: 0.00004360
Iteration 179/1000 | Loss: 0.00004360
Iteration 180/1000 | Loss: 0.00004360
Iteration 181/1000 | Loss: 0.00004360
Iteration 182/1000 | Loss: 0.00004360
Iteration 183/1000 | Loss: 0.00004360
Iteration 184/1000 | Loss: 0.00004360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [4.360169259598479e-05, 4.360169259598479e-05, 4.360169259598479e-05, 4.360169259598479e-05, 4.360169259598479e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.360169259598479e-05

Optimization complete. Final v2v error: 4.896584987640381 mm

Highest mean error: 20.46451187133789 mm for frame 187

Lowest mean error: 4.007514476776123 mm for frame 2

Saving results

Total time: 174.5437469482422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058323
Iteration 2/25 | Loss: 0.00401345
Iteration 3/25 | Loss: 0.00294903
Iteration 4/25 | Loss: 0.00231857
Iteration 5/25 | Loss: 0.00244077
Iteration 6/25 | Loss: 0.00238254
Iteration 7/25 | Loss: 0.00210857
Iteration 8/25 | Loss: 0.00193986
Iteration 9/25 | Loss: 0.00185235
Iteration 10/25 | Loss: 0.00178869
Iteration 11/25 | Loss: 0.00182746
Iteration 12/25 | Loss: 0.00178361
Iteration 13/25 | Loss: 0.00173068
Iteration 14/25 | Loss: 0.00172731
Iteration 15/25 | Loss: 0.00173316
Iteration 16/25 | Loss: 0.00171843
Iteration 17/25 | Loss: 0.00173495
Iteration 18/25 | Loss: 0.00167588
Iteration 19/25 | Loss: 0.00167145
Iteration 20/25 | Loss: 0.00163299
Iteration 21/25 | Loss: 0.00165013
Iteration 22/25 | Loss: 0.00166119
Iteration 23/25 | Loss: 0.00165591
Iteration 24/25 | Loss: 0.00168405
Iteration 25/25 | Loss: 0.00166300

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17332435
Iteration 2/25 | Loss: 0.00456599
Iteration 3/25 | Loss: 0.00400388
Iteration 4/25 | Loss: 0.00400388
Iteration 5/25 | Loss: 0.00400388
Iteration 6/25 | Loss: 0.00400388
Iteration 7/25 | Loss: 0.00400388
Iteration 8/25 | Loss: 0.00400388
Iteration 9/25 | Loss: 0.00400388
Iteration 10/25 | Loss: 0.00400388
Iteration 11/25 | Loss: 0.00400388
Iteration 12/25 | Loss: 0.00400388
Iteration 13/25 | Loss: 0.00400388
Iteration 14/25 | Loss: 0.00400388
Iteration 15/25 | Loss: 0.00400388
Iteration 16/25 | Loss: 0.00400388
Iteration 17/25 | Loss: 0.00400388
Iteration 18/25 | Loss: 0.00400388
Iteration 19/25 | Loss: 0.00400388
Iteration 20/25 | Loss: 0.00400388
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004003877751529217, 0.004003877751529217, 0.004003877751529217, 0.004003877751529217, 0.004003877751529217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004003877751529217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00400388
Iteration 2/1000 | Loss: 0.00178957
Iteration 3/1000 | Loss: 0.00098208
Iteration 4/1000 | Loss: 0.00170957
Iteration 5/1000 | Loss: 0.00128542
Iteration 6/1000 | Loss: 0.00126606
Iteration 7/1000 | Loss: 0.00100009
Iteration 8/1000 | Loss: 0.00143142
Iteration 9/1000 | Loss: 0.00094485
Iteration 10/1000 | Loss: 0.00261650
Iteration 11/1000 | Loss: 0.00110055
Iteration 12/1000 | Loss: 0.00090823
Iteration 13/1000 | Loss: 0.00069904
Iteration 14/1000 | Loss: 0.00080793
Iteration 15/1000 | Loss: 0.00064705
Iteration 16/1000 | Loss: 0.00061264
Iteration 17/1000 | Loss: 0.00054941
Iteration 18/1000 | Loss: 0.00086150
Iteration 19/1000 | Loss: 0.00064251
Iteration 20/1000 | Loss: 0.00049217
Iteration 21/1000 | Loss: 0.00041074
Iteration 22/1000 | Loss: 0.00058406
Iteration 23/1000 | Loss: 0.00071370
Iteration 24/1000 | Loss: 0.00057455
Iteration 25/1000 | Loss: 0.00073546
Iteration 26/1000 | Loss: 0.00062334
Iteration 27/1000 | Loss: 0.00067417
Iteration 28/1000 | Loss: 0.00087527
Iteration 29/1000 | Loss: 0.00044599
Iteration 30/1000 | Loss: 0.00057187
Iteration 31/1000 | Loss: 0.00064555
Iteration 32/1000 | Loss: 0.00062913
Iteration 33/1000 | Loss: 0.00053912
Iteration 34/1000 | Loss: 0.00055456
Iteration 35/1000 | Loss: 0.00087607
Iteration 36/1000 | Loss: 0.00065447
Iteration 37/1000 | Loss: 0.00045327
Iteration 38/1000 | Loss: 0.00050440
Iteration 39/1000 | Loss: 0.00044092
Iteration 40/1000 | Loss: 0.00052558
Iteration 41/1000 | Loss: 0.00082002
Iteration 42/1000 | Loss: 0.00071701
Iteration 43/1000 | Loss: 0.00073224
Iteration 44/1000 | Loss: 0.00060984
Iteration 45/1000 | Loss: 0.00093179
Iteration 46/1000 | Loss: 0.00054945
Iteration 47/1000 | Loss: 0.00047353
Iteration 48/1000 | Loss: 0.00065240
Iteration 49/1000 | Loss: 0.00074191
Iteration 50/1000 | Loss: 0.00042846
Iteration 51/1000 | Loss: 0.00034828
Iteration 52/1000 | Loss: 0.00030216
Iteration 53/1000 | Loss: 0.00033742
Iteration 54/1000 | Loss: 0.00034456
Iteration 55/1000 | Loss: 0.00047695
Iteration 56/1000 | Loss: 0.00050200
Iteration 57/1000 | Loss: 0.00051847
Iteration 58/1000 | Loss: 0.00046738
Iteration 59/1000 | Loss: 0.00100239
Iteration 60/1000 | Loss: 0.00083158
Iteration 61/1000 | Loss: 0.00078872
Iteration 62/1000 | Loss: 0.00055933
Iteration 63/1000 | Loss: 0.00066565
Iteration 64/1000 | Loss: 0.00027341
Iteration 65/1000 | Loss: 0.00021336
Iteration 66/1000 | Loss: 0.00035013
Iteration 67/1000 | Loss: 0.00023597
Iteration 68/1000 | Loss: 0.00023589
Iteration 69/1000 | Loss: 0.00026247
Iteration 70/1000 | Loss: 0.00039458
Iteration 71/1000 | Loss: 0.00037181
Iteration 72/1000 | Loss: 0.00026007
Iteration 73/1000 | Loss: 0.00053012
Iteration 74/1000 | Loss: 0.00056674
Iteration 75/1000 | Loss: 0.00037172
Iteration 76/1000 | Loss: 0.00047223
Iteration 77/1000 | Loss: 0.00075795
Iteration 78/1000 | Loss: 0.00088409
Iteration 79/1000 | Loss: 0.00073388
Iteration 80/1000 | Loss: 0.00013624
Iteration 81/1000 | Loss: 0.00020649
Iteration 82/1000 | Loss: 0.00009917
Iteration 83/1000 | Loss: 0.00012465
Iteration 84/1000 | Loss: 0.00011649
Iteration 85/1000 | Loss: 0.00045333
Iteration 86/1000 | Loss: 0.00029615
Iteration 87/1000 | Loss: 0.00023658
Iteration 88/1000 | Loss: 0.00064902
Iteration 89/1000 | Loss: 0.00022910
Iteration 90/1000 | Loss: 0.00032525
Iteration 91/1000 | Loss: 0.00009658
Iteration 92/1000 | Loss: 0.00027833
Iteration 93/1000 | Loss: 0.00030574
Iteration 94/1000 | Loss: 0.00025253
Iteration 95/1000 | Loss: 0.00010104
Iteration 96/1000 | Loss: 0.00007948
Iteration 97/1000 | Loss: 0.00006863
Iteration 98/1000 | Loss: 0.00009164
Iteration 99/1000 | Loss: 0.00007797
Iteration 100/1000 | Loss: 0.00007373
Iteration 101/1000 | Loss: 0.00015730
Iteration 102/1000 | Loss: 0.00029435
Iteration 103/1000 | Loss: 0.00016422
Iteration 104/1000 | Loss: 0.00019044
Iteration 105/1000 | Loss: 0.00024891
Iteration 106/1000 | Loss: 0.00023653
Iteration 107/1000 | Loss: 0.00011131
Iteration 108/1000 | Loss: 0.00011274
Iteration 109/1000 | Loss: 0.00008509
Iteration 110/1000 | Loss: 0.00009592
Iteration 111/1000 | Loss: 0.00026785
Iteration 112/1000 | Loss: 0.00066906
Iteration 113/1000 | Loss: 0.00110404
Iteration 114/1000 | Loss: 0.00069273
Iteration 115/1000 | Loss: 0.00034384
Iteration 116/1000 | Loss: 0.00022004
Iteration 117/1000 | Loss: 0.00008426
Iteration 118/1000 | Loss: 0.00016250
Iteration 119/1000 | Loss: 0.00009737
Iteration 120/1000 | Loss: 0.00005476
Iteration 121/1000 | Loss: 0.00012234
Iteration 122/1000 | Loss: 0.00037767
Iteration 123/1000 | Loss: 0.00020272
Iteration 124/1000 | Loss: 0.00008596
Iteration 125/1000 | Loss: 0.00004667
Iteration 126/1000 | Loss: 0.00019991
Iteration 127/1000 | Loss: 0.00014170
Iteration 128/1000 | Loss: 0.00026049
Iteration 129/1000 | Loss: 0.00028055
Iteration 130/1000 | Loss: 0.00019518
Iteration 131/1000 | Loss: 0.00040682
Iteration 132/1000 | Loss: 0.00010591
Iteration 133/1000 | Loss: 0.00006227
Iteration 134/1000 | Loss: 0.00028177
Iteration 135/1000 | Loss: 0.00037847
Iteration 136/1000 | Loss: 0.00020585
Iteration 137/1000 | Loss: 0.00029051
Iteration 138/1000 | Loss: 0.00064456
Iteration 139/1000 | Loss: 0.00037019
Iteration 140/1000 | Loss: 0.00055985
Iteration 141/1000 | Loss: 0.00024474
Iteration 142/1000 | Loss: 0.00062415
Iteration 143/1000 | Loss: 0.00009460
Iteration 144/1000 | Loss: 0.00011902
Iteration 145/1000 | Loss: 0.00010385
Iteration 146/1000 | Loss: 0.00007493
Iteration 147/1000 | Loss: 0.00010752
Iteration 148/1000 | Loss: 0.00011630
Iteration 149/1000 | Loss: 0.00008052
Iteration 150/1000 | Loss: 0.00006066
Iteration 151/1000 | Loss: 0.00008747
Iteration 152/1000 | Loss: 0.00005246
Iteration 153/1000 | Loss: 0.00008632
Iteration 154/1000 | Loss: 0.00006938
Iteration 155/1000 | Loss: 0.00010395
Iteration 156/1000 | Loss: 0.00010228
Iteration 157/1000 | Loss: 0.00009608
Iteration 158/1000 | Loss: 0.00009478
Iteration 159/1000 | Loss: 0.00010562
Iteration 160/1000 | Loss: 0.00008323
Iteration 161/1000 | Loss: 0.00007925
Iteration 162/1000 | Loss: 0.00007251
Iteration 163/1000 | Loss: 0.00009561
Iteration 164/1000 | Loss: 0.00010580
Iteration 165/1000 | Loss: 0.00008340
Iteration 166/1000 | Loss: 0.00009069
Iteration 167/1000 | Loss: 0.00008425
Iteration 168/1000 | Loss: 0.00006609
Iteration 169/1000 | Loss: 0.00009654
Iteration 170/1000 | Loss: 0.00011177
Iteration 171/1000 | Loss: 0.00006357
Iteration 172/1000 | Loss: 0.00004981
Iteration 173/1000 | Loss: 0.00004932
Iteration 174/1000 | Loss: 0.00004855
Iteration 175/1000 | Loss: 0.00006600
Iteration 176/1000 | Loss: 0.00006259
Iteration 177/1000 | Loss: 0.00004805
Iteration 178/1000 | Loss: 0.00005053
Iteration 179/1000 | Loss: 0.00006718
Iteration 180/1000 | Loss: 0.00005582
Iteration 181/1000 | Loss: 0.00007032
Iteration 182/1000 | Loss: 0.00005607
Iteration 183/1000 | Loss: 0.00006876
Iteration 184/1000 | Loss: 0.00007064
Iteration 185/1000 | Loss: 0.00006627
Iteration 186/1000 | Loss: 0.00006060
Iteration 187/1000 | Loss: 0.00007555
Iteration 188/1000 | Loss: 0.00006464
Iteration 189/1000 | Loss: 0.00007573
Iteration 190/1000 | Loss: 0.00005290
Iteration 191/1000 | Loss: 0.00006066
Iteration 192/1000 | Loss: 0.00006483
Iteration 193/1000 | Loss: 0.00006084
Iteration 194/1000 | Loss: 0.00006961
Iteration 195/1000 | Loss: 0.00006771
Iteration 196/1000 | Loss: 0.00008089
Iteration 197/1000 | Loss: 0.00004491
Iteration 198/1000 | Loss: 0.00005368
Iteration 199/1000 | Loss: 0.00004424
Iteration 200/1000 | Loss: 0.00005107
Iteration 201/1000 | Loss: 0.00005023
Iteration 202/1000 | Loss: 0.00006159
Iteration 203/1000 | Loss: 0.00004206
Iteration 204/1000 | Loss: 0.00006427
Iteration 205/1000 | Loss: 0.00007112
Iteration 206/1000 | Loss: 0.00005111
Iteration 207/1000 | Loss: 0.00006192
Iteration 208/1000 | Loss: 0.00008972
Iteration 209/1000 | Loss: 0.00007019
Iteration 210/1000 | Loss: 0.00008269
Iteration 211/1000 | Loss: 0.00006944
Iteration 212/1000 | Loss: 0.00007436
Iteration 213/1000 | Loss: 0.00005768
Iteration 214/1000 | Loss: 0.00008906
Iteration 215/1000 | Loss: 0.00006175
Iteration 216/1000 | Loss: 0.00008577
Iteration 217/1000 | Loss: 0.00005436
Iteration 218/1000 | Loss: 0.00005108
Iteration 219/1000 | Loss: 0.00008134
Iteration 220/1000 | Loss: 0.00007726
Iteration 221/1000 | Loss: 0.00007014
Iteration 222/1000 | Loss: 0.00007095
Iteration 223/1000 | Loss: 0.00005842
Iteration 224/1000 | Loss: 0.00006963
Iteration 225/1000 | Loss: 0.00006467
Iteration 226/1000 | Loss: 0.00008464
Iteration 227/1000 | Loss: 0.00006069
Iteration 228/1000 | Loss: 0.00006969
Iteration 229/1000 | Loss: 0.00005321
Iteration 230/1000 | Loss: 0.00006133
Iteration 231/1000 | Loss: 0.00006927
Iteration 232/1000 | Loss: 0.00006163
Iteration 233/1000 | Loss: 0.00028423
Iteration 234/1000 | Loss: 0.00005941
Iteration 235/1000 | Loss: 0.00006303
Iteration 236/1000 | Loss: 0.00005698
Iteration 237/1000 | Loss: 0.00008799
Iteration 238/1000 | Loss: 0.00007249
Iteration 239/1000 | Loss: 0.00009093
Iteration 240/1000 | Loss: 0.00005963
Iteration 241/1000 | Loss: 0.00006000
Iteration 242/1000 | Loss: 0.00008503
Iteration 243/1000 | Loss: 0.00008033
Iteration 244/1000 | Loss: 0.00006647
Iteration 245/1000 | Loss: 0.00006109
Iteration 246/1000 | Loss: 0.00004132
Iteration 247/1000 | Loss: 0.00003490
Iteration 248/1000 | Loss: 0.00005704
Iteration 249/1000 | Loss: 0.00004220
Iteration 250/1000 | Loss: 0.00006483
Iteration 251/1000 | Loss: 0.00007119
Iteration 252/1000 | Loss: 0.00007771
Iteration 253/1000 | Loss: 0.00006814
Iteration 254/1000 | Loss: 0.00005223
Iteration 255/1000 | Loss: 0.00006093
Iteration 256/1000 | Loss: 0.00006292
Iteration 257/1000 | Loss: 0.00007415
Iteration 258/1000 | Loss: 0.00006463
Iteration 259/1000 | Loss: 0.00006207
Iteration 260/1000 | Loss: 0.00007035
Iteration 261/1000 | Loss: 0.00007086
Iteration 262/1000 | Loss: 0.00007976
Iteration 263/1000 | Loss: 0.00006577
Iteration 264/1000 | Loss: 0.00007999
Iteration 265/1000 | Loss: 0.00007058
Iteration 266/1000 | Loss: 0.00005824
Iteration 267/1000 | Loss: 0.00006072
Iteration 268/1000 | Loss: 0.00006403
Iteration 269/1000 | Loss: 0.00005746
Iteration 270/1000 | Loss: 0.00004655
Iteration 271/1000 | Loss: 0.00005769
Iteration 272/1000 | Loss: 0.00005833
Iteration 273/1000 | Loss: 0.00006940
Iteration 274/1000 | Loss: 0.00007381
Iteration 275/1000 | Loss: 0.00028191
Iteration 276/1000 | Loss: 0.00023287
Iteration 277/1000 | Loss: 0.00007105
Iteration 278/1000 | Loss: 0.00006877
Iteration 279/1000 | Loss: 0.00007242
Iteration 280/1000 | Loss: 0.00007268
Iteration 281/1000 | Loss: 0.00007237
Iteration 282/1000 | Loss: 0.00007267
Iteration 283/1000 | Loss: 0.00007457
Iteration 284/1000 | Loss: 0.00007042
Iteration 285/1000 | Loss: 0.00007170
Iteration 286/1000 | Loss: 0.00006008
Iteration 287/1000 | Loss: 0.00006329
Iteration 288/1000 | Loss: 0.00006363
Iteration 289/1000 | Loss: 0.00028586
Iteration 290/1000 | Loss: 0.00008673
Iteration 291/1000 | Loss: 0.00007287
Iteration 292/1000 | Loss: 0.00005879
Iteration 293/1000 | Loss: 0.00005835
Iteration 294/1000 | Loss: 0.00007437
Iteration 295/1000 | Loss: 0.00007334
Iteration 296/1000 | Loss: 0.00007069
Iteration 297/1000 | Loss: 0.00007653
Iteration 298/1000 | Loss: 0.00007293
Iteration 299/1000 | Loss: 0.00007356
Iteration 300/1000 | Loss: 0.00007158
Iteration 301/1000 | Loss: 0.00008695
Iteration 302/1000 | Loss: 0.00006793
Iteration 303/1000 | Loss: 0.00008487
Iteration 304/1000 | Loss: 0.00005915
Iteration 305/1000 | Loss: 0.00005284
Iteration 306/1000 | Loss: 0.00006273
Iteration 307/1000 | Loss: 0.00007185
Iteration 308/1000 | Loss: 0.00006148
Iteration 309/1000 | Loss: 0.00005826
Iteration 310/1000 | Loss: 0.00025164
Iteration 311/1000 | Loss: 0.00015689
Iteration 312/1000 | Loss: 0.00058140
Iteration 313/1000 | Loss: 0.00044545
Iteration 314/1000 | Loss: 0.00006929
Iteration 315/1000 | Loss: 0.00014419
Iteration 316/1000 | Loss: 0.00017162
Iteration 317/1000 | Loss: 0.00005766
Iteration 318/1000 | Loss: 0.00005440
Iteration 319/1000 | Loss: 0.00004227
Iteration 320/1000 | Loss: 0.00003545
Iteration 321/1000 | Loss: 0.00003815
Iteration 322/1000 | Loss: 0.00004231
Iteration 323/1000 | Loss: 0.00003102
Iteration 324/1000 | Loss: 0.00002504
Iteration 325/1000 | Loss: 0.00003157
Iteration 326/1000 | Loss: 0.00002320
Iteration 327/1000 | Loss: 0.00003003
Iteration 328/1000 | Loss: 0.00002921
Iteration 329/1000 | Loss: 0.00002824
Iteration 330/1000 | Loss: 0.00002621
Iteration 331/1000 | Loss: 0.00003505
Iteration 332/1000 | Loss: 0.00002375
Iteration 333/1000 | Loss: 0.00004203
Iteration 334/1000 | Loss: 0.00002666
Iteration 335/1000 | Loss: 0.00002393
Iteration 336/1000 | Loss: 0.00004252
Iteration 337/1000 | Loss: 0.00003795
Iteration 338/1000 | Loss: 0.00004145
Iteration 339/1000 | Loss: 0.00013756
Iteration 340/1000 | Loss: 0.00007915
Iteration 341/1000 | Loss: 0.00010271
Iteration 342/1000 | Loss: 0.00004653
Iteration 343/1000 | Loss: 0.00004339
Iteration 344/1000 | Loss: 0.00004421
Iteration 345/1000 | Loss: 0.00002383
Iteration 346/1000 | Loss: 0.00002286
Iteration 347/1000 | Loss: 0.00002180
Iteration 348/1000 | Loss: 0.00002104
Iteration 349/1000 | Loss: 0.00002062
Iteration 350/1000 | Loss: 0.00002021
Iteration 351/1000 | Loss: 0.00001980
Iteration 352/1000 | Loss: 0.00001949
Iteration 353/1000 | Loss: 0.00001930
Iteration 354/1000 | Loss: 0.00001929
Iteration 355/1000 | Loss: 0.00001928
Iteration 356/1000 | Loss: 0.00001928
Iteration 357/1000 | Loss: 0.00001927
Iteration 358/1000 | Loss: 0.00001927
Iteration 359/1000 | Loss: 0.00001927
Iteration 360/1000 | Loss: 0.00001927
Iteration 361/1000 | Loss: 0.00001927
Iteration 362/1000 | Loss: 0.00001927
Iteration 363/1000 | Loss: 0.00001927
Iteration 364/1000 | Loss: 0.00001927
Iteration 365/1000 | Loss: 0.00001927
Iteration 366/1000 | Loss: 0.00001927
Iteration 367/1000 | Loss: 0.00001926
Iteration 368/1000 | Loss: 0.00001926
Iteration 369/1000 | Loss: 0.00001926
Iteration 370/1000 | Loss: 0.00001926
Iteration 371/1000 | Loss: 0.00001926
Iteration 372/1000 | Loss: 0.00001924
Iteration 373/1000 | Loss: 0.00001923
Iteration 374/1000 | Loss: 0.00001923
Iteration 375/1000 | Loss: 0.00001923
Iteration 376/1000 | Loss: 0.00001923
Iteration 377/1000 | Loss: 0.00001923
Iteration 378/1000 | Loss: 0.00001923
Iteration 379/1000 | Loss: 0.00001922
Iteration 380/1000 | Loss: 0.00001922
Iteration 381/1000 | Loss: 0.00001922
Iteration 382/1000 | Loss: 0.00001921
Iteration 383/1000 | Loss: 0.00001921
Iteration 384/1000 | Loss: 0.00001921
Iteration 385/1000 | Loss: 0.00001921
Iteration 386/1000 | Loss: 0.00001920
Iteration 387/1000 | Loss: 0.00001920
Iteration 388/1000 | Loss: 0.00001920
Iteration 389/1000 | Loss: 0.00001920
Iteration 390/1000 | Loss: 0.00001920
Iteration 391/1000 | Loss: 0.00001920
Iteration 392/1000 | Loss: 0.00001920
Iteration 393/1000 | Loss: 0.00001919
Iteration 394/1000 | Loss: 0.00001919
Iteration 395/1000 | Loss: 0.00001919
Iteration 396/1000 | Loss: 0.00001919
Iteration 397/1000 | Loss: 0.00001919
Iteration 398/1000 | Loss: 0.00001919
Iteration 399/1000 | Loss: 0.00001919
Iteration 400/1000 | Loss: 0.00001919
Iteration 401/1000 | Loss: 0.00001918
Iteration 402/1000 | Loss: 0.00001918
Iteration 403/1000 | Loss: 0.00001918
Iteration 404/1000 | Loss: 0.00001918
Iteration 405/1000 | Loss: 0.00001918
Iteration 406/1000 | Loss: 0.00001918
Iteration 407/1000 | Loss: 0.00001918
Iteration 408/1000 | Loss: 0.00001918
Iteration 409/1000 | Loss: 0.00001918
Iteration 410/1000 | Loss: 0.00001918
Iteration 411/1000 | Loss: 0.00001918
Iteration 412/1000 | Loss: 0.00001918
Iteration 413/1000 | Loss: 0.00001918
Iteration 414/1000 | Loss: 0.00001918
Iteration 415/1000 | Loss: 0.00001918
Iteration 416/1000 | Loss: 0.00001918
Iteration 417/1000 | Loss: 0.00001918
Iteration 418/1000 | Loss: 0.00001918
Iteration 419/1000 | Loss: 0.00001918
Iteration 420/1000 | Loss: 0.00001918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 420. Stopping optimization.
Last 5 losses: [1.917539702844806e-05, 1.917539702844806e-05, 1.917539702844806e-05, 1.917539702844806e-05, 1.917539702844806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.917539702844806e-05

Optimization complete. Final v2v error: 3.721428871154785 mm

Highest mean error: 4.164182662963867 mm for frame 121

Lowest mean error: 3.5766053199768066 mm for frame 36

Saving results

Total time: 524.9768285751343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01199739
Iteration 2/25 | Loss: 0.00241435
Iteration 3/25 | Loss: 0.00165604
Iteration 4/25 | Loss: 0.00160705
Iteration 5/25 | Loss: 0.00160238
Iteration 6/25 | Loss: 0.00159476
Iteration 7/25 | Loss: 0.00159022
Iteration 8/25 | Loss: 0.00158879
Iteration 9/25 | Loss: 0.00158713
Iteration 10/25 | Loss: 0.00159246
Iteration 11/25 | Loss: 0.00158713
Iteration 12/25 | Loss: 0.00158272
Iteration 13/25 | Loss: 0.00157936
Iteration 14/25 | Loss: 0.00157855
Iteration 15/25 | Loss: 0.00157587
Iteration 16/25 | Loss: 0.00157822
Iteration 17/25 | Loss: 0.00158012
Iteration 18/25 | Loss: 0.00157738
Iteration 19/25 | Loss: 0.00157659
Iteration 20/25 | Loss: 0.00157802
Iteration 21/25 | Loss: 0.00157874
Iteration 22/25 | Loss: 0.00158012
Iteration 23/25 | Loss: 0.00157860
Iteration 24/25 | Loss: 0.00157852
Iteration 25/25 | Loss: 0.00157701

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27339745
Iteration 2/25 | Loss: 0.00204921
Iteration 3/25 | Loss: 0.00204921
Iteration 4/25 | Loss: 0.00204921
Iteration 5/25 | Loss: 0.00204921
Iteration 6/25 | Loss: 0.00204920
Iteration 7/25 | Loss: 0.00204920
Iteration 8/25 | Loss: 0.00204920
Iteration 9/25 | Loss: 0.00204920
Iteration 10/25 | Loss: 0.00204920
Iteration 11/25 | Loss: 0.00204920
Iteration 12/25 | Loss: 0.00204920
Iteration 13/25 | Loss: 0.00204920
Iteration 14/25 | Loss: 0.00204920
Iteration 15/25 | Loss: 0.00204920
Iteration 16/25 | Loss: 0.00204920
Iteration 17/25 | Loss: 0.00204920
Iteration 18/25 | Loss: 0.00204920
Iteration 19/25 | Loss: 0.00204920
Iteration 20/25 | Loss: 0.00204920
Iteration 21/25 | Loss: 0.00204920
Iteration 22/25 | Loss: 0.00204920
Iteration 23/25 | Loss: 0.00204920
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0020492037292569876, 0.0020492037292569876, 0.0020492037292569876, 0.0020492037292569876, 0.0020492037292569876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020492037292569876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204920
Iteration 2/1000 | Loss: 0.00061969
Iteration 3/1000 | Loss: 0.00191835
Iteration 4/1000 | Loss: 0.00091078
Iteration 5/1000 | Loss: 0.00085626
Iteration 6/1000 | Loss: 0.00060928
Iteration 7/1000 | Loss: 0.00049631
Iteration 8/1000 | Loss: 0.00059340
Iteration 9/1000 | Loss: 0.00032399
Iteration 10/1000 | Loss: 0.00093454
Iteration 11/1000 | Loss: 0.00030338
Iteration 12/1000 | Loss: 0.00045824
Iteration 13/1000 | Loss: 0.00034069
Iteration 14/1000 | Loss: 0.00040203
Iteration 15/1000 | Loss: 0.00046174
Iteration 16/1000 | Loss: 0.00016254
Iteration 17/1000 | Loss: 0.00032946
Iteration 18/1000 | Loss: 0.00011974
Iteration 19/1000 | Loss: 0.00082293
Iteration 20/1000 | Loss: 0.00072482
Iteration 21/1000 | Loss: 0.00144828
Iteration 22/1000 | Loss: 0.00145176
Iteration 23/1000 | Loss: 0.00096142
Iteration 24/1000 | Loss: 0.00067786
Iteration 25/1000 | Loss: 0.00088447
Iteration 26/1000 | Loss: 0.00084170
Iteration 27/1000 | Loss: 0.00043621
Iteration 28/1000 | Loss: 0.00035626
Iteration 29/1000 | Loss: 0.00057824
Iteration 30/1000 | Loss: 0.00013968
Iteration 31/1000 | Loss: 0.00007273
Iteration 32/1000 | Loss: 0.00006879
Iteration 33/1000 | Loss: 0.00044994
Iteration 34/1000 | Loss: 0.00074723
Iteration 35/1000 | Loss: 0.00052494
Iteration 36/1000 | Loss: 0.00034506
Iteration 37/1000 | Loss: 0.00013905
Iteration 38/1000 | Loss: 0.00060446
Iteration 39/1000 | Loss: 0.00031416
Iteration 40/1000 | Loss: 0.00026217
Iteration 41/1000 | Loss: 0.00034057
Iteration 42/1000 | Loss: 0.00037681
Iteration 43/1000 | Loss: 0.00023556
Iteration 44/1000 | Loss: 0.00015173
Iteration 45/1000 | Loss: 0.00010428
Iteration 46/1000 | Loss: 0.00023321
Iteration 47/1000 | Loss: 0.00013120
Iteration 48/1000 | Loss: 0.00007012
Iteration 49/1000 | Loss: 0.00009913
Iteration 50/1000 | Loss: 0.00014925
Iteration 51/1000 | Loss: 0.00014107
Iteration 52/1000 | Loss: 0.00008431
Iteration 53/1000 | Loss: 0.00014317
Iteration 54/1000 | Loss: 0.00047016
Iteration 55/1000 | Loss: 0.00036342
Iteration 56/1000 | Loss: 0.00018910
Iteration 57/1000 | Loss: 0.00053865
Iteration 58/1000 | Loss: 0.00029674
Iteration 59/1000 | Loss: 0.00007572
Iteration 60/1000 | Loss: 0.00005658
Iteration 61/1000 | Loss: 0.00005452
Iteration 62/1000 | Loss: 0.00028365
Iteration 63/1000 | Loss: 0.00060252
Iteration 64/1000 | Loss: 0.00040033
Iteration 65/1000 | Loss: 0.00054539
Iteration 66/1000 | Loss: 0.00009676
Iteration 67/1000 | Loss: 0.00005340
Iteration 68/1000 | Loss: 0.00005129
Iteration 69/1000 | Loss: 0.00027173
Iteration 70/1000 | Loss: 0.00049245
Iteration 71/1000 | Loss: 0.00074646
Iteration 72/1000 | Loss: 0.00052270
Iteration 73/1000 | Loss: 0.00046497
Iteration 74/1000 | Loss: 0.00033764
Iteration 75/1000 | Loss: 0.00079940
Iteration 76/1000 | Loss: 0.00026636
Iteration 77/1000 | Loss: 0.00006990
Iteration 78/1000 | Loss: 0.00005830
Iteration 79/1000 | Loss: 0.00005187
Iteration 80/1000 | Loss: 0.00004929
Iteration 81/1000 | Loss: 0.00045377
Iteration 82/1000 | Loss: 0.00032324
Iteration 83/1000 | Loss: 0.00004775
Iteration 84/1000 | Loss: 0.00045095
Iteration 85/1000 | Loss: 0.00014255
Iteration 86/1000 | Loss: 0.00013091
Iteration 87/1000 | Loss: 0.00004700
Iteration 88/1000 | Loss: 0.00004552
Iteration 89/1000 | Loss: 0.00004437
Iteration 90/1000 | Loss: 0.00004358
Iteration 91/1000 | Loss: 0.00004308
Iteration 92/1000 | Loss: 0.00004272
Iteration 93/1000 | Loss: 0.00004244
Iteration 94/1000 | Loss: 0.00004223
Iteration 95/1000 | Loss: 0.00004193
Iteration 96/1000 | Loss: 0.00004156
Iteration 97/1000 | Loss: 0.00004125
Iteration 98/1000 | Loss: 0.00004100
Iteration 99/1000 | Loss: 0.00004083
Iteration 100/1000 | Loss: 0.00004069
Iteration 101/1000 | Loss: 0.00004065
Iteration 102/1000 | Loss: 0.00004065
Iteration 103/1000 | Loss: 0.00004058
Iteration 104/1000 | Loss: 0.00004053
Iteration 105/1000 | Loss: 0.00004047
Iteration 106/1000 | Loss: 0.00004047
Iteration 107/1000 | Loss: 0.00004045
Iteration 108/1000 | Loss: 0.00004044
Iteration 109/1000 | Loss: 0.00004044
Iteration 110/1000 | Loss: 0.00004044
Iteration 111/1000 | Loss: 0.00004044
Iteration 112/1000 | Loss: 0.00004044
Iteration 113/1000 | Loss: 0.00004044
Iteration 114/1000 | Loss: 0.00004044
Iteration 115/1000 | Loss: 0.00004044
Iteration 116/1000 | Loss: 0.00004044
Iteration 117/1000 | Loss: 0.00004044
Iteration 118/1000 | Loss: 0.00004044
Iteration 119/1000 | Loss: 0.00004044
Iteration 120/1000 | Loss: 0.00004043
Iteration 121/1000 | Loss: 0.00004043
Iteration 122/1000 | Loss: 0.00004043
Iteration 123/1000 | Loss: 0.00004043
Iteration 124/1000 | Loss: 0.00004043
Iteration 125/1000 | Loss: 0.00004042
Iteration 126/1000 | Loss: 0.00004042
Iteration 127/1000 | Loss: 0.00004042
Iteration 128/1000 | Loss: 0.00004042
Iteration 129/1000 | Loss: 0.00004042
Iteration 130/1000 | Loss: 0.00004041
Iteration 131/1000 | Loss: 0.00004041
Iteration 132/1000 | Loss: 0.00004041
Iteration 133/1000 | Loss: 0.00004041
Iteration 134/1000 | Loss: 0.00004040
Iteration 135/1000 | Loss: 0.00004040
Iteration 136/1000 | Loss: 0.00004040
Iteration 137/1000 | Loss: 0.00004040
Iteration 138/1000 | Loss: 0.00004039
Iteration 139/1000 | Loss: 0.00004039
Iteration 140/1000 | Loss: 0.00004039
Iteration 141/1000 | Loss: 0.00004039
Iteration 142/1000 | Loss: 0.00004039
Iteration 143/1000 | Loss: 0.00004038
Iteration 144/1000 | Loss: 0.00004038
Iteration 145/1000 | Loss: 0.00004038
Iteration 146/1000 | Loss: 0.00004038
Iteration 147/1000 | Loss: 0.00004037
Iteration 148/1000 | Loss: 0.00004037
Iteration 149/1000 | Loss: 0.00004037
Iteration 150/1000 | Loss: 0.00004037
Iteration 151/1000 | Loss: 0.00004037
Iteration 152/1000 | Loss: 0.00004037
Iteration 153/1000 | Loss: 0.00004037
Iteration 154/1000 | Loss: 0.00004037
Iteration 155/1000 | Loss: 0.00004037
Iteration 156/1000 | Loss: 0.00004037
Iteration 157/1000 | Loss: 0.00004037
Iteration 158/1000 | Loss: 0.00004037
Iteration 159/1000 | Loss: 0.00004037
Iteration 160/1000 | Loss: 0.00004037
Iteration 161/1000 | Loss: 0.00004037
Iteration 162/1000 | Loss: 0.00004036
Iteration 163/1000 | Loss: 0.00004036
Iteration 164/1000 | Loss: 0.00004036
Iteration 165/1000 | Loss: 0.00004036
Iteration 166/1000 | Loss: 0.00004036
Iteration 167/1000 | Loss: 0.00004036
Iteration 168/1000 | Loss: 0.00004036
Iteration 169/1000 | Loss: 0.00004036
Iteration 170/1000 | Loss: 0.00004036
Iteration 171/1000 | Loss: 0.00004036
Iteration 172/1000 | Loss: 0.00004036
Iteration 173/1000 | Loss: 0.00004036
Iteration 174/1000 | Loss: 0.00004036
Iteration 175/1000 | Loss: 0.00004036
Iteration 176/1000 | Loss: 0.00004036
Iteration 177/1000 | Loss: 0.00004036
Iteration 178/1000 | Loss: 0.00004036
Iteration 179/1000 | Loss: 0.00004036
Iteration 180/1000 | Loss: 0.00004036
Iteration 181/1000 | Loss: 0.00004036
Iteration 182/1000 | Loss: 0.00004035
Iteration 183/1000 | Loss: 0.00004035
Iteration 184/1000 | Loss: 0.00004035
Iteration 185/1000 | Loss: 0.00004035
Iteration 186/1000 | Loss: 0.00004035
Iteration 187/1000 | Loss: 0.00004035
Iteration 188/1000 | Loss: 0.00004035
Iteration 189/1000 | Loss: 0.00004035
Iteration 190/1000 | Loss: 0.00004035
Iteration 191/1000 | Loss: 0.00004035
Iteration 192/1000 | Loss: 0.00004035
Iteration 193/1000 | Loss: 0.00004035
Iteration 194/1000 | Loss: 0.00004035
Iteration 195/1000 | Loss: 0.00004035
Iteration 196/1000 | Loss: 0.00004034
Iteration 197/1000 | Loss: 0.00004034
Iteration 198/1000 | Loss: 0.00004034
Iteration 199/1000 | Loss: 0.00004034
Iteration 200/1000 | Loss: 0.00004034
Iteration 201/1000 | Loss: 0.00004034
Iteration 202/1000 | Loss: 0.00004034
Iteration 203/1000 | Loss: 0.00004034
Iteration 204/1000 | Loss: 0.00004034
Iteration 205/1000 | Loss: 0.00004034
Iteration 206/1000 | Loss: 0.00004034
Iteration 207/1000 | Loss: 0.00004034
Iteration 208/1000 | Loss: 0.00004033
Iteration 209/1000 | Loss: 0.00004033
Iteration 210/1000 | Loss: 0.00004033
Iteration 211/1000 | Loss: 0.00004033
Iteration 212/1000 | Loss: 0.00004033
Iteration 213/1000 | Loss: 0.00004033
Iteration 214/1000 | Loss: 0.00004033
Iteration 215/1000 | Loss: 0.00004033
Iteration 216/1000 | Loss: 0.00004033
Iteration 217/1000 | Loss: 0.00004033
Iteration 218/1000 | Loss: 0.00004033
Iteration 219/1000 | Loss: 0.00004033
Iteration 220/1000 | Loss: 0.00004033
Iteration 221/1000 | Loss: 0.00004033
Iteration 222/1000 | Loss: 0.00004033
Iteration 223/1000 | Loss: 0.00004033
Iteration 224/1000 | Loss: 0.00004033
Iteration 225/1000 | Loss: 0.00004032
Iteration 226/1000 | Loss: 0.00004032
Iteration 227/1000 | Loss: 0.00004032
Iteration 228/1000 | Loss: 0.00004032
Iteration 229/1000 | Loss: 0.00004032
Iteration 230/1000 | Loss: 0.00004032
Iteration 231/1000 | Loss: 0.00004032
Iteration 232/1000 | Loss: 0.00004032
Iteration 233/1000 | Loss: 0.00004032
Iteration 234/1000 | Loss: 0.00004032
Iteration 235/1000 | Loss: 0.00004032
Iteration 236/1000 | Loss: 0.00004032
Iteration 237/1000 | Loss: 0.00004032
Iteration 238/1000 | Loss: 0.00004032
Iteration 239/1000 | Loss: 0.00004032
Iteration 240/1000 | Loss: 0.00004032
Iteration 241/1000 | Loss: 0.00004031
Iteration 242/1000 | Loss: 0.00004031
Iteration 243/1000 | Loss: 0.00004031
Iteration 244/1000 | Loss: 0.00004031
Iteration 245/1000 | Loss: 0.00004031
Iteration 246/1000 | Loss: 0.00004031
Iteration 247/1000 | Loss: 0.00004031
Iteration 248/1000 | Loss: 0.00004031
Iteration 249/1000 | Loss: 0.00004031
Iteration 250/1000 | Loss: 0.00004031
Iteration 251/1000 | Loss: 0.00004031
Iteration 252/1000 | Loss: 0.00004031
Iteration 253/1000 | Loss: 0.00004030
Iteration 254/1000 | Loss: 0.00004030
Iteration 255/1000 | Loss: 0.00004030
Iteration 256/1000 | Loss: 0.00004030
Iteration 257/1000 | Loss: 0.00004030
Iteration 258/1000 | Loss: 0.00004030
Iteration 259/1000 | Loss: 0.00004030
Iteration 260/1000 | Loss: 0.00004030
Iteration 261/1000 | Loss: 0.00004030
Iteration 262/1000 | Loss: 0.00004030
Iteration 263/1000 | Loss: 0.00004030
Iteration 264/1000 | Loss: 0.00004030
Iteration 265/1000 | Loss: 0.00004030
Iteration 266/1000 | Loss: 0.00004030
Iteration 267/1000 | Loss: 0.00004030
Iteration 268/1000 | Loss: 0.00004030
Iteration 269/1000 | Loss: 0.00004030
Iteration 270/1000 | Loss: 0.00004030
Iteration 271/1000 | Loss: 0.00004030
Iteration 272/1000 | Loss: 0.00004030
Iteration 273/1000 | Loss: 0.00004030
Iteration 274/1000 | Loss: 0.00004030
Iteration 275/1000 | Loss: 0.00004030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [4.0298164094565436e-05, 4.0298164094565436e-05, 4.0298164094565436e-05, 4.0298164094565436e-05, 4.0298164094565436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0298164094565436e-05

Optimization complete. Final v2v error: 4.974508285522461 mm

Highest mean error: 6.2890238761901855 mm for frame 87

Lowest mean error: 4.034879684448242 mm for frame 8

Saving results

Total time: 226.83231925964355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487607
Iteration 2/25 | Loss: 0.00148521
Iteration 3/25 | Loss: 0.00141090
Iteration 4/25 | Loss: 0.00139588
Iteration 5/25 | Loss: 0.00139040
Iteration 6/25 | Loss: 0.00138968
Iteration 7/25 | Loss: 0.00138968
Iteration 8/25 | Loss: 0.00138968
Iteration 9/25 | Loss: 0.00138968
Iteration 10/25 | Loss: 0.00138968
Iteration 11/25 | Loss: 0.00138968
Iteration 12/25 | Loss: 0.00138968
Iteration 13/25 | Loss: 0.00138968
Iteration 14/25 | Loss: 0.00138968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013896763557568192, 0.0013896763557568192, 0.0013896763557568192, 0.0013896763557568192, 0.0013896763557568192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013896763557568192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21770585
Iteration 2/25 | Loss: 0.00178804
Iteration 3/25 | Loss: 0.00178804
Iteration 4/25 | Loss: 0.00178804
Iteration 5/25 | Loss: 0.00178804
Iteration 6/25 | Loss: 0.00178804
Iteration 7/25 | Loss: 0.00178804
Iteration 8/25 | Loss: 0.00178804
Iteration 9/25 | Loss: 0.00178804
Iteration 10/25 | Loss: 0.00178804
Iteration 11/25 | Loss: 0.00178804
Iteration 12/25 | Loss: 0.00178804
Iteration 13/25 | Loss: 0.00178804
Iteration 14/25 | Loss: 0.00178804
Iteration 15/25 | Loss: 0.00178804
Iteration 16/25 | Loss: 0.00178804
Iteration 17/25 | Loss: 0.00178804
Iteration 18/25 | Loss: 0.00178804
Iteration 19/25 | Loss: 0.00178804
Iteration 20/25 | Loss: 0.00178804
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0017880365485325456, 0.0017880365485325456, 0.0017880365485325456, 0.0017880365485325456, 0.0017880365485325456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017880365485325456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178804
Iteration 2/1000 | Loss: 0.00005703
Iteration 3/1000 | Loss: 0.00003058
Iteration 4/1000 | Loss: 0.00002805
Iteration 5/1000 | Loss: 0.00002707
Iteration 6/1000 | Loss: 0.00002623
Iteration 7/1000 | Loss: 0.00002542
Iteration 8/1000 | Loss: 0.00002491
Iteration 9/1000 | Loss: 0.00002453
Iteration 10/1000 | Loss: 0.00002429
Iteration 11/1000 | Loss: 0.00002427
Iteration 12/1000 | Loss: 0.00002422
Iteration 13/1000 | Loss: 0.00002420
Iteration 14/1000 | Loss: 0.00002418
Iteration 15/1000 | Loss: 0.00002412
Iteration 16/1000 | Loss: 0.00002412
Iteration 17/1000 | Loss: 0.00002410
Iteration 18/1000 | Loss: 0.00002410
Iteration 19/1000 | Loss: 0.00002410
Iteration 20/1000 | Loss: 0.00002408
Iteration 21/1000 | Loss: 0.00002408
Iteration 22/1000 | Loss: 0.00002408
Iteration 23/1000 | Loss: 0.00002407
Iteration 24/1000 | Loss: 0.00002406
Iteration 25/1000 | Loss: 0.00002406
Iteration 26/1000 | Loss: 0.00002406
Iteration 27/1000 | Loss: 0.00002405
Iteration 28/1000 | Loss: 0.00002405
Iteration 29/1000 | Loss: 0.00002403
Iteration 30/1000 | Loss: 0.00002403
Iteration 31/1000 | Loss: 0.00002399
Iteration 32/1000 | Loss: 0.00002399
Iteration 33/1000 | Loss: 0.00002398
Iteration 34/1000 | Loss: 0.00002398
Iteration 35/1000 | Loss: 0.00002397
Iteration 36/1000 | Loss: 0.00002397
Iteration 37/1000 | Loss: 0.00002396
Iteration 38/1000 | Loss: 0.00002396
Iteration 39/1000 | Loss: 0.00002395
Iteration 40/1000 | Loss: 0.00002395
Iteration 41/1000 | Loss: 0.00002395
Iteration 42/1000 | Loss: 0.00002395
Iteration 43/1000 | Loss: 0.00002395
Iteration 44/1000 | Loss: 0.00002395
Iteration 45/1000 | Loss: 0.00002395
Iteration 46/1000 | Loss: 0.00002395
Iteration 47/1000 | Loss: 0.00002394
Iteration 48/1000 | Loss: 0.00002394
Iteration 49/1000 | Loss: 0.00002394
Iteration 50/1000 | Loss: 0.00002394
Iteration 51/1000 | Loss: 0.00002392
Iteration 52/1000 | Loss: 0.00002392
Iteration 53/1000 | Loss: 0.00002392
Iteration 54/1000 | Loss: 0.00002391
Iteration 55/1000 | Loss: 0.00002391
Iteration 56/1000 | Loss: 0.00002391
Iteration 57/1000 | Loss: 0.00002391
Iteration 58/1000 | Loss: 0.00002390
Iteration 59/1000 | Loss: 0.00002390
Iteration 60/1000 | Loss: 0.00002390
Iteration 61/1000 | Loss: 0.00002390
Iteration 62/1000 | Loss: 0.00002390
Iteration 63/1000 | Loss: 0.00002390
Iteration 64/1000 | Loss: 0.00002390
Iteration 65/1000 | Loss: 0.00002390
Iteration 66/1000 | Loss: 0.00002390
Iteration 67/1000 | Loss: 0.00002390
Iteration 68/1000 | Loss: 0.00002389
Iteration 69/1000 | Loss: 0.00002389
Iteration 70/1000 | Loss: 0.00002389
Iteration 71/1000 | Loss: 0.00002389
Iteration 72/1000 | Loss: 0.00002388
Iteration 73/1000 | Loss: 0.00002388
Iteration 74/1000 | Loss: 0.00002388
Iteration 75/1000 | Loss: 0.00002388
Iteration 76/1000 | Loss: 0.00002388
Iteration 77/1000 | Loss: 0.00002388
Iteration 78/1000 | Loss: 0.00002388
Iteration 79/1000 | Loss: 0.00002388
Iteration 80/1000 | Loss: 0.00002388
Iteration 81/1000 | Loss: 0.00002388
Iteration 82/1000 | Loss: 0.00002388
Iteration 83/1000 | Loss: 0.00002388
Iteration 84/1000 | Loss: 0.00002388
Iteration 85/1000 | Loss: 0.00002388
Iteration 86/1000 | Loss: 0.00002387
Iteration 87/1000 | Loss: 0.00002387
Iteration 88/1000 | Loss: 0.00002387
Iteration 89/1000 | Loss: 0.00002387
Iteration 90/1000 | Loss: 0.00002387
Iteration 91/1000 | Loss: 0.00002387
Iteration 92/1000 | Loss: 0.00002387
Iteration 93/1000 | Loss: 0.00002387
Iteration 94/1000 | Loss: 0.00002387
Iteration 95/1000 | Loss: 0.00002387
Iteration 96/1000 | Loss: 0.00002387
Iteration 97/1000 | Loss: 0.00002387
Iteration 98/1000 | Loss: 0.00002387
Iteration 99/1000 | Loss: 0.00002387
Iteration 100/1000 | Loss: 0.00002387
Iteration 101/1000 | Loss: 0.00002387
Iteration 102/1000 | Loss: 0.00002387
Iteration 103/1000 | Loss: 0.00002386
Iteration 104/1000 | Loss: 0.00002386
Iteration 105/1000 | Loss: 0.00002386
Iteration 106/1000 | Loss: 0.00002386
Iteration 107/1000 | Loss: 0.00002386
Iteration 108/1000 | Loss: 0.00002386
Iteration 109/1000 | Loss: 0.00002386
Iteration 110/1000 | Loss: 0.00002386
Iteration 111/1000 | Loss: 0.00002386
Iteration 112/1000 | Loss: 0.00002386
Iteration 113/1000 | Loss: 0.00002386
Iteration 114/1000 | Loss: 0.00002385
Iteration 115/1000 | Loss: 0.00002385
Iteration 116/1000 | Loss: 0.00002385
Iteration 117/1000 | Loss: 0.00002385
Iteration 118/1000 | Loss: 0.00002385
Iteration 119/1000 | Loss: 0.00002385
Iteration 120/1000 | Loss: 0.00002385
Iteration 121/1000 | Loss: 0.00002385
Iteration 122/1000 | Loss: 0.00002385
Iteration 123/1000 | Loss: 0.00002384
Iteration 124/1000 | Loss: 0.00002384
Iteration 125/1000 | Loss: 0.00002384
Iteration 126/1000 | Loss: 0.00002384
Iteration 127/1000 | Loss: 0.00002384
Iteration 128/1000 | Loss: 0.00002383
Iteration 129/1000 | Loss: 0.00002383
Iteration 130/1000 | Loss: 0.00002383
Iteration 131/1000 | Loss: 0.00002383
Iteration 132/1000 | Loss: 0.00002383
Iteration 133/1000 | Loss: 0.00002383
Iteration 134/1000 | Loss: 0.00002383
Iteration 135/1000 | Loss: 0.00002383
Iteration 136/1000 | Loss: 0.00002383
Iteration 137/1000 | Loss: 0.00002383
Iteration 138/1000 | Loss: 0.00002382
Iteration 139/1000 | Loss: 0.00002382
Iteration 140/1000 | Loss: 0.00002382
Iteration 141/1000 | Loss: 0.00002382
Iteration 142/1000 | Loss: 0.00002382
Iteration 143/1000 | Loss: 0.00002382
Iteration 144/1000 | Loss: 0.00002382
Iteration 145/1000 | Loss: 0.00002382
Iteration 146/1000 | Loss: 0.00002382
Iteration 147/1000 | Loss: 0.00002382
Iteration 148/1000 | Loss: 0.00002382
Iteration 149/1000 | Loss: 0.00002382
Iteration 150/1000 | Loss: 0.00002382
Iteration 151/1000 | Loss: 0.00002382
Iteration 152/1000 | Loss: 0.00002382
Iteration 153/1000 | Loss: 0.00002382
Iteration 154/1000 | Loss: 0.00002382
Iteration 155/1000 | Loss: 0.00002382
Iteration 156/1000 | Loss: 0.00002382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.3823722585802898e-05, 2.3823722585802898e-05, 2.3823722585802898e-05, 2.3823722585802898e-05, 2.3823722585802898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3823722585802898e-05

Optimization complete. Final v2v error: 4.145468235015869 mm

Highest mean error: 4.736323356628418 mm for frame 31

Lowest mean error: 3.582702398300171 mm for frame 69

Saving results

Total time: 37.806363344192505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00931985
Iteration 2/25 | Loss: 0.00162527
Iteration 3/25 | Loss: 0.00144289
Iteration 4/25 | Loss: 0.00142058
Iteration 5/25 | Loss: 0.00141658
Iteration 6/25 | Loss: 0.00140529
Iteration 7/25 | Loss: 0.00140071
Iteration 8/25 | Loss: 0.00140017
Iteration 9/25 | Loss: 0.00139945
Iteration 10/25 | Loss: 0.00139864
Iteration 11/25 | Loss: 0.00139825
Iteration 12/25 | Loss: 0.00139796
Iteration 13/25 | Loss: 0.00139779
Iteration 14/25 | Loss: 0.00139765
Iteration 15/25 | Loss: 0.00139736
Iteration 16/25 | Loss: 0.00139559
Iteration 17/25 | Loss: 0.00140151
Iteration 18/25 | Loss: 0.00139433
Iteration 19/25 | Loss: 0.00139216
Iteration 20/25 | Loss: 0.00139196
Iteration 21/25 | Loss: 0.00139189
Iteration 22/25 | Loss: 0.00139185
Iteration 23/25 | Loss: 0.00139184
Iteration 24/25 | Loss: 0.00139184
Iteration 25/25 | Loss: 0.00139184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34780240
Iteration 2/25 | Loss: 0.00177293
Iteration 3/25 | Loss: 0.00177293
Iteration 4/25 | Loss: 0.00177293
Iteration 5/25 | Loss: 0.00177292
Iteration 6/25 | Loss: 0.00177292
Iteration 7/25 | Loss: 0.00177292
Iteration 8/25 | Loss: 0.00177292
Iteration 9/25 | Loss: 0.00177292
Iteration 10/25 | Loss: 0.00177292
Iteration 11/25 | Loss: 0.00177292
Iteration 12/25 | Loss: 0.00177292
Iteration 13/25 | Loss: 0.00177292
Iteration 14/25 | Loss: 0.00177292
Iteration 15/25 | Loss: 0.00177292
Iteration 16/25 | Loss: 0.00177292
Iteration 17/25 | Loss: 0.00177292
Iteration 18/25 | Loss: 0.00177292
Iteration 19/25 | Loss: 0.00177292
Iteration 20/25 | Loss: 0.00177292
Iteration 21/25 | Loss: 0.00177292
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0017729224637150764, 0.0017729224637150764, 0.0017729224637150764, 0.0017729224637150764, 0.0017729224637150764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017729224637150764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177292
Iteration 2/1000 | Loss: 0.00006296
Iteration 3/1000 | Loss: 0.00003701
Iteration 4/1000 | Loss: 0.00002800
Iteration 5/1000 | Loss: 0.00002581
Iteration 6/1000 | Loss: 0.00002446
Iteration 7/1000 | Loss: 0.00002353
Iteration 8/1000 | Loss: 0.00002293
Iteration 9/1000 | Loss: 0.00002244
Iteration 10/1000 | Loss: 0.00002213
Iteration 11/1000 | Loss: 0.00002189
Iteration 12/1000 | Loss: 0.00002172
Iteration 13/1000 | Loss: 0.00002153
Iteration 14/1000 | Loss: 0.00002142
Iteration 15/1000 | Loss: 0.00002134
Iteration 16/1000 | Loss: 0.00002131
Iteration 17/1000 | Loss: 0.00002131
Iteration 18/1000 | Loss: 0.00002129
Iteration 19/1000 | Loss: 0.00002129
Iteration 20/1000 | Loss: 0.00002128
Iteration 21/1000 | Loss: 0.00002127
Iteration 22/1000 | Loss: 0.00002127
Iteration 23/1000 | Loss: 0.00002127
Iteration 24/1000 | Loss: 0.00002126
Iteration 25/1000 | Loss: 0.00002125
Iteration 26/1000 | Loss: 0.00002125
Iteration 27/1000 | Loss: 0.00002125
Iteration 28/1000 | Loss: 0.00002124
Iteration 29/1000 | Loss: 0.00002124
Iteration 30/1000 | Loss: 0.00002124
Iteration 31/1000 | Loss: 0.00002123
Iteration 32/1000 | Loss: 0.00002123
Iteration 33/1000 | Loss: 0.00002123
Iteration 34/1000 | Loss: 0.00002122
Iteration 35/1000 | Loss: 0.00002122
Iteration 36/1000 | Loss: 0.00002122
Iteration 37/1000 | Loss: 0.00002122
Iteration 38/1000 | Loss: 0.00002122
Iteration 39/1000 | Loss: 0.00002121
Iteration 40/1000 | Loss: 0.00002121
Iteration 41/1000 | Loss: 0.00002121
Iteration 42/1000 | Loss: 0.00002121
Iteration 43/1000 | Loss: 0.00002121
Iteration 44/1000 | Loss: 0.00002121
Iteration 45/1000 | Loss: 0.00002121
Iteration 46/1000 | Loss: 0.00002120
Iteration 47/1000 | Loss: 0.00002120
Iteration 48/1000 | Loss: 0.00002120
Iteration 49/1000 | Loss: 0.00002120
Iteration 50/1000 | Loss: 0.00002119
Iteration 51/1000 | Loss: 0.00002119
Iteration 52/1000 | Loss: 0.00002119
Iteration 53/1000 | Loss: 0.00002119
Iteration 54/1000 | Loss: 0.00002118
Iteration 55/1000 | Loss: 0.00002118
Iteration 56/1000 | Loss: 0.00002118
Iteration 57/1000 | Loss: 0.00002118
Iteration 58/1000 | Loss: 0.00002118
Iteration 59/1000 | Loss: 0.00002117
Iteration 60/1000 | Loss: 0.00002117
Iteration 61/1000 | Loss: 0.00002117
Iteration 62/1000 | Loss: 0.00002116
Iteration 63/1000 | Loss: 0.00002116
Iteration 64/1000 | Loss: 0.00002116
Iteration 65/1000 | Loss: 0.00002116
Iteration 66/1000 | Loss: 0.00002116
Iteration 67/1000 | Loss: 0.00002116
Iteration 68/1000 | Loss: 0.00002115
Iteration 69/1000 | Loss: 0.00002115
Iteration 70/1000 | Loss: 0.00002115
Iteration 71/1000 | Loss: 0.00002115
Iteration 72/1000 | Loss: 0.00002115
Iteration 73/1000 | Loss: 0.00002114
Iteration 74/1000 | Loss: 0.00002114
Iteration 75/1000 | Loss: 0.00002114
Iteration 76/1000 | Loss: 0.00002114
Iteration 77/1000 | Loss: 0.00002113
Iteration 78/1000 | Loss: 0.00002113
Iteration 79/1000 | Loss: 0.00002113
Iteration 80/1000 | Loss: 0.00002113
Iteration 81/1000 | Loss: 0.00002113
Iteration 82/1000 | Loss: 0.00002112
Iteration 83/1000 | Loss: 0.00002112
Iteration 84/1000 | Loss: 0.00002112
Iteration 85/1000 | Loss: 0.00002112
Iteration 86/1000 | Loss: 0.00002112
Iteration 87/1000 | Loss: 0.00002112
Iteration 88/1000 | Loss: 0.00002111
Iteration 89/1000 | Loss: 0.00002111
Iteration 90/1000 | Loss: 0.00002111
Iteration 91/1000 | Loss: 0.00002111
Iteration 92/1000 | Loss: 0.00002111
Iteration 93/1000 | Loss: 0.00002111
Iteration 94/1000 | Loss: 0.00002111
Iteration 95/1000 | Loss: 0.00002111
Iteration 96/1000 | Loss: 0.00002111
Iteration 97/1000 | Loss: 0.00002111
Iteration 98/1000 | Loss: 0.00002111
Iteration 99/1000 | Loss: 0.00002111
Iteration 100/1000 | Loss: 0.00002111
Iteration 101/1000 | Loss: 0.00002111
Iteration 102/1000 | Loss: 0.00002111
Iteration 103/1000 | Loss: 0.00002111
Iteration 104/1000 | Loss: 0.00002111
Iteration 105/1000 | Loss: 0.00002111
Iteration 106/1000 | Loss: 0.00002111
Iteration 107/1000 | Loss: 0.00002111
Iteration 108/1000 | Loss: 0.00002111
Iteration 109/1000 | Loss: 0.00002111
Iteration 110/1000 | Loss: 0.00002111
Iteration 111/1000 | Loss: 0.00002111
Iteration 112/1000 | Loss: 0.00002111
Iteration 113/1000 | Loss: 0.00002111
Iteration 114/1000 | Loss: 0.00002111
Iteration 115/1000 | Loss: 0.00002111
Iteration 116/1000 | Loss: 0.00002111
Iteration 117/1000 | Loss: 0.00002111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.110576315317303e-05, 2.110576315317303e-05, 2.110576315317303e-05, 2.110576315317303e-05, 2.110576315317303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.110576315317303e-05

Optimization complete. Final v2v error: 3.852266550064087 mm

Highest mean error: 5.250494003295898 mm for frame 108

Lowest mean error: 3.3967669010162354 mm for frame 138

Saving results

Total time: 62.03510403633118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381277
Iteration 2/25 | Loss: 0.00160004
Iteration 3/25 | Loss: 0.00144903
Iteration 4/25 | Loss: 0.00143276
Iteration 5/25 | Loss: 0.00142875
Iteration 6/25 | Loss: 0.00142798
Iteration 7/25 | Loss: 0.00142798
Iteration 8/25 | Loss: 0.00142798
Iteration 9/25 | Loss: 0.00142798
Iteration 10/25 | Loss: 0.00142798
Iteration 11/25 | Loss: 0.00142798
Iteration 12/25 | Loss: 0.00142798
Iteration 13/25 | Loss: 0.00142798
Iteration 14/25 | Loss: 0.00142798
Iteration 15/25 | Loss: 0.00142798
Iteration 16/25 | Loss: 0.00142798
Iteration 17/25 | Loss: 0.00142798
Iteration 18/25 | Loss: 0.00142798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014279814204201102, 0.0014279814204201102, 0.0014279814204201102, 0.0014279814204201102, 0.0014279814204201102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014279814204201102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15813160
Iteration 2/25 | Loss: 0.00258957
Iteration 3/25 | Loss: 0.00258957
Iteration 4/25 | Loss: 0.00258957
Iteration 5/25 | Loss: 0.00258957
Iteration 6/25 | Loss: 0.00258957
Iteration 7/25 | Loss: 0.00258957
Iteration 8/25 | Loss: 0.00258957
Iteration 9/25 | Loss: 0.00258957
Iteration 10/25 | Loss: 0.00258957
Iteration 11/25 | Loss: 0.00258957
Iteration 12/25 | Loss: 0.00258957
Iteration 13/25 | Loss: 0.00258957
Iteration 14/25 | Loss: 0.00258957
Iteration 15/25 | Loss: 0.00258957
Iteration 16/25 | Loss: 0.00258957
Iteration 17/25 | Loss: 0.00258957
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0025895657017827034, 0.0025895657017827034, 0.0025895657017827034, 0.0025895657017827034, 0.0025895657017827034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025895657017827034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00258957
Iteration 2/1000 | Loss: 0.00005801
Iteration 3/1000 | Loss: 0.00003075
Iteration 4/1000 | Loss: 0.00002345
Iteration 5/1000 | Loss: 0.00002121
Iteration 6/1000 | Loss: 0.00002005
Iteration 7/1000 | Loss: 0.00001922
Iteration 8/1000 | Loss: 0.00001871
Iteration 9/1000 | Loss: 0.00001827
Iteration 10/1000 | Loss: 0.00001806
Iteration 11/1000 | Loss: 0.00001782
Iteration 12/1000 | Loss: 0.00001772
Iteration 13/1000 | Loss: 0.00001763
Iteration 14/1000 | Loss: 0.00001750
Iteration 15/1000 | Loss: 0.00001746
Iteration 16/1000 | Loss: 0.00001742
Iteration 17/1000 | Loss: 0.00001742
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001740
Iteration 20/1000 | Loss: 0.00001740
Iteration 21/1000 | Loss: 0.00001739
Iteration 22/1000 | Loss: 0.00001739
Iteration 23/1000 | Loss: 0.00001738
Iteration 24/1000 | Loss: 0.00001737
Iteration 25/1000 | Loss: 0.00001730
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001730
Iteration 28/1000 | Loss: 0.00001729
Iteration 29/1000 | Loss: 0.00001729
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001728
Iteration 32/1000 | Loss: 0.00001728
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001727
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001726
Iteration 38/1000 | Loss: 0.00001726
Iteration 39/1000 | Loss: 0.00001726
Iteration 40/1000 | Loss: 0.00001726
Iteration 41/1000 | Loss: 0.00001725
Iteration 42/1000 | Loss: 0.00001725
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001725
Iteration 46/1000 | Loss: 0.00001724
Iteration 47/1000 | Loss: 0.00001724
Iteration 48/1000 | Loss: 0.00001724
Iteration 49/1000 | Loss: 0.00001724
Iteration 50/1000 | Loss: 0.00001724
Iteration 51/1000 | Loss: 0.00001724
Iteration 52/1000 | Loss: 0.00001724
Iteration 53/1000 | Loss: 0.00001723
Iteration 54/1000 | Loss: 0.00001723
Iteration 55/1000 | Loss: 0.00001723
Iteration 56/1000 | Loss: 0.00001722
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001722
Iteration 60/1000 | Loss: 0.00001722
Iteration 61/1000 | Loss: 0.00001722
Iteration 62/1000 | Loss: 0.00001722
Iteration 63/1000 | Loss: 0.00001722
Iteration 64/1000 | Loss: 0.00001721
Iteration 65/1000 | Loss: 0.00001721
Iteration 66/1000 | Loss: 0.00001721
Iteration 67/1000 | Loss: 0.00001721
Iteration 68/1000 | Loss: 0.00001721
Iteration 69/1000 | Loss: 0.00001721
Iteration 70/1000 | Loss: 0.00001721
Iteration 71/1000 | Loss: 0.00001720
Iteration 72/1000 | Loss: 0.00001720
Iteration 73/1000 | Loss: 0.00001720
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001720
Iteration 76/1000 | Loss: 0.00001720
Iteration 77/1000 | Loss: 0.00001720
Iteration 78/1000 | Loss: 0.00001720
Iteration 79/1000 | Loss: 0.00001720
Iteration 80/1000 | Loss: 0.00001720
Iteration 81/1000 | Loss: 0.00001720
Iteration 82/1000 | Loss: 0.00001719
Iteration 83/1000 | Loss: 0.00001719
Iteration 84/1000 | Loss: 0.00001719
Iteration 85/1000 | Loss: 0.00001719
Iteration 86/1000 | Loss: 0.00001719
Iteration 87/1000 | Loss: 0.00001719
Iteration 88/1000 | Loss: 0.00001719
Iteration 89/1000 | Loss: 0.00001719
Iteration 90/1000 | Loss: 0.00001719
Iteration 91/1000 | Loss: 0.00001719
Iteration 92/1000 | Loss: 0.00001719
Iteration 93/1000 | Loss: 0.00001719
Iteration 94/1000 | Loss: 0.00001718
Iteration 95/1000 | Loss: 0.00001718
Iteration 96/1000 | Loss: 0.00001718
Iteration 97/1000 | Loss: 0.00001718
Iteration 98/1000 | Loss: 0.00001718
Iteration 99/1000 | Loss: 0.00001717
Iteration 100/1000 | Loss: 0.00001717
Iteration 101/1000 | Loss: 0.00001717
Iteration 102/1000 | Loss: 0.00001717
Iteration 103/1000 | Loss: 0.00001717
Iteration 104/1000 | Loss: 0.00001717
Iteration 105/1000 | Loss: 0.00001717
Iteration 106/1000 | Loss: 0.00001717
Iteration 107/1000 | Loss: 0.00001717
Iteration 108/1000 | Loss: 0.00001717
Iteration 109/1000 | Loss: 0.00001717
Iteration 110/1000 | Loss: 0.00001717
Iteration 111/1000 | Loss: 0.00001717
Iteration 112/1000 | Loss: 0.00001716
Iteration 113/1000 | Loss: 0.00001716
Iteration 114/1000 | Loss: 0.00001716
Iteration 115/1000 | Loss: 0.00001716
Iteration 116/1000 | Loss: 0.00001716
Iteration 117/1000 | Loss: 0.00001716
Iteration 118/1000 | Loss: 0.00001716
Iteration 119/1000 | Loss: 0.00001716
Iteration 120/1000 | Loss: 0.00001716
Iteration 121/1000 | Loss: 0.00001716
Iteration 122/1000 | Loss: 0.00001716
Iteration 123/1000 | Loss: 0.00001716
Iteration 124/1000 | Loss: 0.00001716
Iteration 125/1000 | Loss: 0.00001715
Iteration 126/1000 | Loss: 0.00001715
Iteration 127/1000 | Loss: 0.00001715
Iteration 128/1000 | Loss: 0.00001715
Iteration 129/1000 | Loss: 0.00001715
Iteration 130/1000 | Loss: 0.00001715
Iteration 131/1000 | Loss: 0.00001715
Iteration 132/1000 | Loss: 0.00001715
Iteration 133/1000 | Loss: 0.00001714
Iteration 134/1000 | Loss: 0.00001714
Iteration 135/1000 | Loss: 0.00001714
Iteration 136/1000 | Loss: 0.00001714
Iteration 137/1000 | Loss: 0.00001714
Iteration 138/1000 | Loss: 0.00001714
Iteration 139/1000 | Loss: 0.00001714
Iteration 140/1000 | Loss: 0.00001714
Iteration 141/1000 | Loss: 0.00001713
Iteration 142/1000 | Loss: 0.00001713
Iteration 143/1000 | Loss: 0.00001713
Iteration 144/1000 | Loss: 0.00001713
Iteration 145/1000 | Loss: 0.00001713
Iteration 146/1000 | Loss: 0.00001713
Iteration 147/1000 | Loss: 0.00001713
Iteration 148/1000 | Loss: 0.00001713
Iteration 149/1000 | Loss: 0.00001713
Iteration 150/1000 | Loss: 0.00001713
Iteration 151/1000 | Loss: 0.00001713
Iteration 152/1000 | Loss: 0.00001713
Iteration 153/1000 | Loss: 0.00001713
Iteration 154/1000 | Loss: 0.00001712
Iteration 155/1000 | Loss: 0.00001712
Iteration 156/1000 | Loss: 0.00001712
Iteration 157/1000 | Loss: 0.00001712
Iteration 158/1000 | Loss: 0.00001712
Iteration 159/1000 | Loss: 0.00001712
Iteration 160/1000 | Loss: 0.00001712
Iteration 161/1000 | Loss: 0.00001712
Iteration 162/1000 | Loss: 0.00001712
Iteration 163/1000 | Loss: 0.00001712
Iteration 164/1000 | Loss: 0.00001712
Iteration 165/1000 | Loss: 0.00001712
Iteration 166/1000 | Loss: 0.00001712
Iteration 167/1000 | Loss: 0.00001712
Iteration 168/1000 | Loss: 0.00001712
Iteration 169/1000 | Loss: 0.00001712
Iteration 170/1000 | Loss: 0.00001711
Iteration 171/1000 | Loss: 0.00001711
Iteration 172/1000 | Loss: 0.00001711
Iteration 173/1000 | Loss: 0.00001711
Iteration 174/1000 | Loss: 0.00001711
Iteration 175/1000 | Loss: 0.00001711
Iteration 176/1000 | Loss: 0.00001711
Iteration 177/1000 | Loss: 0.00001711
Iteration 178/1000 | Loss: 0.00001711
Iteration 179/1000 | Loss: 0.00001711
Iteration 180/1000 | Loss: 0.00001711
Iteration 181/1000 | Loss: 0.00001711
Iteration 182/1000 | Loss: 0.00001711
Iteration 183/1000 | Loss: 0.00001711
Iteration 184/1000 | Loss: 0.00001711
Iteration 185/1000 | Loss: 0.00001711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.7109952750615776e-05, 1.7109952750615776e-05, 1.7109952750615776e-05, 1.7109952750615776e-05, 1.7109952750615776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7109952750615776e-05

Optimization complete. Final v2v error: 3.5065178871154785 mm

Highest mean error: 4.120902061462402 mm for frame 125

Lowest mean error: 3.1005382537841797 mm for frame 56

Saving results

Total time: 43.88020348548889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01110333
Iteration 2/25 | Loss: 0.00281511
Iteration 3/25 | Loss: 0.00278737
Iteration 4/25 | Loss: 0.00284189
Iteration 5/25 | Loss: 0.00244880
Iteration 6/25 | Loss: 0.00220286
Iteration 7/25 | Loss: 0.00210260
Iteration 8/25 | Loss: 0.00191571
Iteration 9/25 | Loss: 0.00175560
Iteration 10/25 | Loss: 0.00166767
Iteration 11/25 | Loss: 0.00156571
Iteration 12/25 | Loss: 0.00148901
Iteration 13/25 | Loss: 0.00145659
Iteration 14/25 | Loss: 0.00144815
Iteration 15/25 | Loss: 0.00145019
Iteration 16/25 | Loss: 0.00144581
Iteration 17/25 | Loss: 0.00145280
Iteration 18/25 | Loss: 0.00144990
Iteration 19/25 | Loss: 0.00144259
Iteration 20/25 | Loss: 0.00144636
Iteration 21/25 | Loss: 0.00143629
Iteration 22/25 | Loss: 0.00143869
Iteration 23/25 | Loss: 0.00143708
Iteration 24/25 | Loss: 0.00143775
Iteration 25/25 | Loss: 0.00143720

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22031105
Iteration 2/25 | Loss: 0.00290310
Iteration 3/25 | Loss: 0.00207567
Iteration 4/25 | Loss: 0.00207567
Iteration 5/25 | Loss: 0.00207567
Iteration 6/25 | Loss: 0.00207567
Iteration 7/25 | Loss: 0.00207567
Iteration 8/25 | Loss: 0.00207567
Iteration 9/25 | Loss: 0.00207567
Iteration 10/25 | Loss: 0.00207567
Iteration 11/25 | Loss: 0.00207567
Iteration 12/25 | Loss: 0.00207567
Iteration 13/25 | Loss: 0.00207567
Iteration 14/25 | Loss: 0.00207567
Iteration 15/25 | Loss: 0.00207567
Iteration 16/25 | Loss: 0.00207567
Iteration 17/25 | Loss: 0.00207567
Iteration 18/25 | Loss: 0.00207567
Iteration 19/25 | Loss: 0.00207567
Iteration 20/25 | Loss: 0.00207567
Iteration 21/25 | Loss: 0.00207567
Iteration 22/25 | Loss: 0.00207567
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0020756672602146864, 0.0020756672602146864, 0.0020756672602146864, 0.0020756672602146864, 0.0020756672602146864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020756672602146864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207567
Iteration 2/1000 | Loss: 0.00053102
Iteration 3/1000 | Loss: 0.00028688
Iteration 4/1000 | Loss: 0.00028254
Iteration 5/1000 | Loss: 0.00007407
Iteration 6/1000 | Loss: 0.00018492
Iteration 7/1000 | Loss: 0.00025712
Iteration 8/1000 | Loss: 0.00005283
Iteration 9/1000 | Loss: 0.00004411
Iteration 10/1000 | Loss: 0.00004962
Iteration 11/1000 | Loss: 0.00005073
Iteration 12/1000 | Loss: 0.00005739
Iteration 13/1000 | Loss: 0.00005789
Iteration 14/1000 | Loss: 0.00003707
Iteration 15/1000 | Loss: 0.00003179
Iteration 16/1000 | Loss: 0.00002998
Iteration 17/1000 | Loss: 0.00003917
Iteration 18/1000 | Loss: 0.00003984
Iteration 19/1000 | Loss: 0.00004285
Iteration 20/1000 | Loss: 0.00004333
Iteration 21/1000 | Loss: 0.00057115
Iteration 22/1000 | Loss: 0.00074726
Iteration 23/1000 | Loss: 0.00005224
Iteration 24/1000 | Loss: 0.00004984
Iteration 25/1000 | Loss: 0.00012368
Iteration 26/1000 | Loss: 0.00003317
Iteration 27/1000 | Loss: 0.00003912
Iteration 28/1000 | Loss: 0.00003008
Iteration 29/1000 | Loss: 0.00003026
Iteration 30/1000 | Loss: 0.00003027
Iteration 31/1000 | Loss: 0.00004026
Iteration 32/1000 | Loss: 0.00004308
Iteration 33/1000 | Loss: 0.00003674
Iteration 34/1000 | Loss: 0.00004869
Iteration 35/1000 | Loss: 0.00005127
Iteration 36/1000 | Loss: 0.00005326
Iteration 37/1000 | Loss: 0.00003456
Iteration 38/1000 | Loss: 0.00005618
Iteration 39/1000 | Loss: 0.00025074
Iteration 40/1000 | Loss: 0.00005350
Iteration 41/1000 | Loss: 0.00003578
Iteration 42/1000 | Loss: 0.00003594
Iteration 43/1000 | Loss: 0.00004336
Iteration 44/1000 | Loss: 0.00003933
Iteration 45/1000 | Loss: 0.00002886
Iteration 46/1000 | Loss: 0.00003407
Iteration 47/1000 | Loss: 0.00003781
Iteration 48/1000 | Loss: 0.00002937
Iteration 49/1000 | Loss: 0.00003335
Iteration 50/1000 | Loss: 0.00003434
Iteration 51/1000 | Loss: 0.00021110
Iteration 52/1000 | Loss: 0.00006090
Iteration 53/1000 | Loss: 0.00003321
Iteration 54/1000 | Loss: 0.00003471
Iteration 55/1000 | Loss: 0.00003461
Iteration 56/1000 | Loss: 0.00003849
Iteration 57/1000 | Loss: 0.00003514
Iteration 58/1000 | Loss: 0.00003441
Iteration 59/1000 | Loss: 0.00003453
Iteration 60/1000 | Loss: 0.00003775
Iteration 61/1000 | Loss: 0.00003446
Iteration 62/1000 | Loss: 0.00014589
Iteration 63/1000 | Loss: 0.00003235
Iteration 64/1000 | Loss: 0.00003325
Iteration 65/1000 | Loss: 0.00003516
Iteration 66/1000 | Loss: 0.00003481
Iteration 67/1000 | Loss: 0.00003758
Iteration 68/1000 | Loss: 0.00003771
Iteration 69/1000 | Loss: 0.00003924
Iteration 70/1000 | Loss: 0.00005016
Iteration 71/1000 | Loss: 0.00022518
Iteration 72/1000 | Loss: 0.00005844
Iteration 73/1000 | Loss: 0.00002958
Iteration 74/1000 | Loss: 0.00003734
Iteration 75/1000 | Loss: 0.00003378
Iteration 76/1000 | Loss: 0.00003303
Iteration 77/1000 | Loss: 0.00003074
Iteration 78/1000 | Loss: 0.00003304
Iteration 79/1000 | Loss: 0.00003188
Iteration 80/1000 | Loss: 0.00003193
Iteration 81/1000 | Loss: 0.00003050
Iteration 82/1000 | Loss: 0.00003850
Iteration 83/1000 | Loss: 0.00005059
Iteration 84/1000 | Loss: 0.00003566
Iteration 85/1000 | Loss: 0.00003844
Iteration 86/1000 | Loss: 0.00004678
Iteration 87/1000 | Loss: 0.00002556
Iteration 88/1000 | Loss: 0.00002322
Iteration 89/1000 | Loss: 0.00002173
Iteration 90/1000 | Loss: 0.00002100
Iteration 91/1000 | Loss: 0.00002040
Iteration 92/1000 | Loss: 0.00002012
Iteration 93/1000 | Loss: 0.00001989
Iteration 94/1000 | Loss: 0.00001965
Iteration 95/1000 | Loss: 0.00001964
Iteration 96/1000 | Loss: 0.00001955
Iteration 97/1000 | Loss: 0.00001947
Iteration 98/1000 | Loss: 0.00001942
Iteration 99/1000 | Loss: 0.00001942
Iteration 100/1000 | Loss: 0.00001941
Iteration 101/1000 | Loss: 0.00001938
Iteration 102/1000 | Loss: 0.00001938
Iteration 103/1000 | Loss: 0.00001936
Iteration 104/1000 | Loss: 0.00001936
Iteration 105/1000 | Loss: 0.00001936
Iteration 106/1000 | Loss: 0.00001935
Iteration 107/1000 | Loss: 0.00001934
Iteration 108/1000 | Loss: 0.00001933
Iteration 109/1000 | Loss: 0.00001933
Iteration 110/1000 | Loss: 0.00001933
Iteration 111/1000 | Loss: 0.00001933
Iteration 112/1000 | Loss: 0.00001933
Iteration 113/1000 | Loss: 0.00001933
Iteration 114/1000 | Loss: 0.00001933
Iteration 115/1000 | Loss: 0.00001932
Iteration 116/1000 | Loss: 0.00001932
Iteration 117/1000 | Loss: 0.00001932
Iteration 118/1000 | Loss: 0.00001932
Iteration 119/1000 | Loss: 0.00001931
Iteration 120/1000 | Loss: 0.00001931
Iteration 121/1000 | Loss: 0.00001931
Iteration 122/1000 | Loss: 0.00001931
Iteration 123/1000 | Loss: 0.00001931
Iteration 124/1000 | Loss: 0.00001930
Iteration 125/1000 | Loss: 0.00001930
Iteration 126/1000 | Loss: 0.00001930
Iteration 127/1000 | Loss: 0.00001930
Iteration 128/1000 | Loss: 0.00001930
Iteration 129/1000 | Loss: 0.00001930
Iteration 130/1000 | Loss: 0.00001929
Iteration 131/1000 | Loss: 0.00001929
Iteration 132/1000 | Loss: 0.00001929
Iteration 133/1000 | Loss: 0.00001929
Iteration 134/1000 | Loss: 0.00001929
Iteration 135/1000 | Loss: 0.00001929
Iteration 136/1000 | Loss: 0.00001928
Iteration 137/1000 | Loss: 0.00001928
Iteration 138/1000 | Loss: 0.00001928
Iteration 139/1000 | Loss: 0.00001928
Iteration 140/1000 | Loss: 0.00001928
Iteration 141/1000 | Loss: 0.00001928
Iteration 142/1000 | Loss: 0.00001928
Iteration 143/1000 | Loss: 0.00001928
Iteration 144/1000 | Loss: 0.00001928
Iteration 145/1000 | Loss: 0.00001928
Iteration 146/1000 | Loss: 0.00001927
Iteration 147/1000 | Loss: 0.00001927
Iteration 148/1000 | Loss: 0.00001927
Iteration 149/1000 | Loss: 0.00001927
Iteration 150/1000 | Loss: 0.00001927
Iteration 151/1000 | Loss: 0.00001927
Iteration 152/1000 | Loss: 0.00001926
Iteration 153/1000 | Loss: 0.00001926
Iteration 154/1000 | Loss: 0.00001926
Iteration 155/1000 | Loss: 0.00001926
Iteration 156/1000 | Loss: 0.00001926
Iteration 157/1000 | Loss: 0.00001926
Iteration 158/1000 | Loss: 0.00001926
Iteration 159/1000 | Loss: 0.00001926
Iteration 160/1000 | Loss: 0.00001926
Iteration 161/1000 | Loss: 0.00001925
Iteration 162/1000 | Loss: 0.00001925
Iteration 163/1000 | Loss: 0.00001925
Iteration 164/1000 | Loss: 0.00001925
Iteration 165/1000 | Loss: 0.00001925
Iteration 166/1000 | Loss: 0.00001925
Iteration 167/1000 | Loss: 0.00001925
Iteration 168/1000 | Loss: 0.00001925
Iteration 169/1000 | Loss: 0.00001925
Iteration 170/1000 | Loss: 0.00001925
Iteration 171/1000 | Loss: 0.00001925
Iteration 172/1000 | Loss: 0.00001924
Iteration 173/1000 | Loss: 0.00001924
Iteration 174/1000 | Loss: 0.00001924
Iteration 175/1000 | Loss: 0.00001924
Iteration 176/1000 | Loss: 0.00001924
Iteration 177/1000 | Loss: 0.00001924
Iteration 178/1000 | Loss: 0.00001924
Iteration 179/1000 | Loss: 0.00001924
Iteration 180/1000 | Loss: 0.00001924
Iteration 181/1000 | Loss: 0.00001924
Iteration 182/1000 | Loss: 0.00001924
Iteration 183/1000 | Loss: 0.00001924
Iteration 184/1000 | Loss: 0.00001924
Iteration 185/1000 | Loss: 0.00001924
Iteration 186/1000 | Loss: 0.00001924
Iteration 187/1000 | Loss: 0.00001924
Iteration 188/1000 | Loss: 0.00001924
Iteration 189/1000 | Loss: 0.00001924
Iteration 190/1000 | Loss: 0.00001924
Iteration 191/1000 | Loss: 0.00001924
Iteration 192/1000 | Loss: 0.00001924
Iteration 193/1000 | Loss: 0.00001924
Iteration 194/1000 | Loss: 0.00001924
Iteration 195/1000 | Loss: 0.00001924
Iteration 196/1000 | Loss: 0.00001924
Iteration 197/1000 | Loss: 0.00001924
Iteration 198/1000 | Loss: 0.00001924
Iteration 199/1000 | Loss: 0.00001924
Iteration 200/1000 | Loss: 0.00001924
Iteration 201/1000 | Loss: 0.00001924
Iteration 202/1000 | Loss: 0.00001924
Iteration 203/1000 | Loss: 0.00001924
Iteration 204/1000 | Loss: 0.00001924
Iteration 205/1000 | Loss: 0.00001924
Iteration 206/1000 | Loss: 0.00001924
Iteration 207/1000 | Loss: 0.00001924
Iteration 208/1000 | Loss: 0.00001924
Iteration 209/1000 | Loss: 0.00001924
Iteration 210/1000 | Loss: 0.00001924
Iteration 211/1000 | Loss: 0.00001924
Iteration 212/1000 | Loss: 0.00001924
Iteration 213/1000 | Loss: 0.00001924
Iteration 214/1000 | Loss: 0.00001924
Iteration 215/1000 | Loss: 0.00001924
Iteration 216/1000 | Loss: 0.00001924
Iteration 217/1000 | Loss: 0.00001924
Iteration 218/1000 | Loss: 0.00001924
Iteration 219/1000 | Loss: 0.00001924
Iteration 220/1000 | Loss: 0.00001924
Iteration 221/1000 | Loss: 0.00001924
Iteration 222/1000 | Loss: 0.00001924
Iteration 223/1000 | Loss: 0.00001924
Iteration 224/1000 | Loss: 0.00001924
Iteration 225/1000 | Loss: 0.00001924
Iteration 226/1000 | Loss: 0.00001924
Iteration 227/1000 | Loss: 0.00001924
Iteration 228/1000 | Loss: 0.00001924
Iteration 229/1000 | Loss: 0.00001924
Iteration 230/1000 | Loss: 0.00001924
Iteration 231/1000 | Loss: 0.00001924
Iteration 232/1000 | Loss: 0.00001924
Iteration 233/1000 | Loss: 0.00001924
Iteration 234/1000 | Loss: 0.00001924
Iteration 235/1000 | Loss: 0.00001924
Iteration 236/1000 | Loss: 0.00001924
Iteration 237/1000 | Loss: 0.00001924
Iteration 238/1000 | Loss: 0.00001924
Iteration 239/1000 | Loss: 0.00001924
Iteration 240/1000 | Loss: 0.00001924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.9237151718698442e-05, 1.9237151718698442e-05, 1.9237151718698442e-05, 1.9237151718698442e-05, 1.9237151718698442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9237151718698442e-05

Optimization complete. Final v2v error: 3.7463619709014893 mm

Highest mean error: 4.504534721374512 mm for frame 77

Lowest mean error: 3.3755037784576416 mm for frame 52

Saving results

Total time: 182.2437720298767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_0017/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_0017/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903036
Iteration 2/25 | Loss: 0.00168065
Iteration 3/25 | Loss: 0.00145464
Iteration 4/25 | Loss: 0.00142832
Iteration 5/25 | Loss: 0.00142331
Iteration 6/25 | Loss: 0.00142239
Iteration 7/25 | Loss: 0.00142239
Iteration 8/25 | Loss: 0.00142239
Iteration 9/25 | Loss: 0.00142239
Iteration 10/25 | Loss: 0.00142239
Iteration 11/25 | Loss: 0.00142239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014223860343918204, 0.0014223860343918204, 0.0014223860343918204, 0.0014223860343918204, 0.0014223860343918204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014223860343918204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20682859
Iteration 2/25 | Loss: 0.00194542
Iteration 3/25 | Loss: 0.00194540
Iteration 4/25 | Loss: 0.00194540
Iteration 5/25 | Loss: 0.00194540
Iteration 6/25 | Loss: 0.00194540
Iteration 7/25 | Loss: 0.00194540
Iteration 8/25 | Loss: 0.00194540
Iteration 9/25 | Loss: 0.00194540
Iteration 10/25 | Loss: 0.00194540
Iteration 11/25 | Loss: 0.00194540
Iteration 12/25 | Loss: 0.00194540
Iteration 13/25 | Loss: 0.00194540
Iteration 14/25 | Loss: 0.00194540
Iteration 15/25 | Loss: 0.00194540
Iteration 16/25 | Loss: 0.00194540
Iteration 17/25 | Loss: 0.00194540
Iteration 18/25 | Loss: 0.00194540
Iteration 19/25 | Loss: 0.00194540
Iteration 20/25 | Loss: 0.00194540
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019453983986750245, 0.0019453983986750245, 0.0019453983986750245, 0.0019453983986750245, 0.0019453983986750245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019453983986750245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194540
Iteration 2/1000 | Loss: 0.00005541
Iteration 3/1000 | Loss: 0.00003545
Iteration 4/1000 | Loss: 0.00002779
Iteration 5/1000 | Loss: 0.00002404
Iteration 6/1000 | Loss: 0.00002215
Iteration 7/1000 | Loss: 0.00002106
Iteration 8/1000 | Loss: 0.00002003
Iteration 9/1000 | Loss: 0.00001941
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001857
Iteration 12/1000 | Loss: 0.00001840
Iteration 13/1000 | Loss: 0.00001816
Iteration 14/1000 | Loss: 0.00001804
Iteration 15/1000 | Loss: 0.00001797
Iteration 16/1000 | Loss: 0.00001796
Iteration 17/1000 | Loss: 0.00001792
Iteration 18/1000 | Loss: 0.00001792
Iteration 19/1000 | Loss: 0.00001792
Iteration 20/1000 | Loss: 0.00001792
Iteration 21/1000 | Loss: 0.00001792
Iteration 22/1000 | Loss: 0.00001791
Iteration 23/1000 | Loss: 0.00001791
Iteration 24/1000 | Loss: 0.00001791
Iteration 25/1000 | Loss: 0.00001791
Iteration 26/1000 | Loss: 0.00001791
Iteration 27/1000 | Loss: 0.00001791
Iteration 28/1000 | Loss: 0.00001790
Iteration 29/1000 | Loss: 0.00001790
Iteration 30/1000 | Loss: 0.00001790
Iteration 31/1000 | Loss: 0.00001790
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001789
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001789
Iteration 37/1000 | Loss: 0.00001789
Iteration 38/1000 | Loss: 0.00001789
Iteration 39/1000 | Loss: 0.00001789
Iteration 40/1000 | Loss: 0.00001789
Iteration 41/1000 | Loss: 0.00001789
Iteration 42/1000 | Loss: 0.00001788
Iteration 43/1000 | Loss: 0.00001788
Iteration 44/1000 | Loss: 0.00001788
Iteration 45/1000 | Loss: 0.00001788
Iteration 46/1000 | Loss: 0.00001788
Iteration 47/1000 | Loss: 0.00001788
Iteration 48/1000 | Loss: 0.00001788
Iteration 49/1000 | Loss: 0.00001788
Iteration 50/1000 | Loss: 0.00001788
Iteration 51/1000 | Loss: 0.00001788
Iteration 52/1000 | Loss: 0.00001787
Iteration 53/1000 | Loss: 0.00001787
Iteration 54/1000 | Loss: 0.00001787
Iteration 55/1000 | Loss: 0.00001787
Iteration 56/1000 | Loss: 0.00001787
Iteration 57/1000 | Loss: 0.00001786
Iteration 58/1000 | Loss: 0.00001786
Iteration 59/1000 | Loss: 0.00001786
Iteration 60/1000 | Loss: 0.00001786
Iteration 61/1000 | Loss: 0.00001786
Iteration 62/1000 | Loss: 0.00001785
Iteration 63/1000 | Loss: 0.00001785
Iteration 64/1000 | Loss: 0.00001785
Iteration 65/1000 | Loss: 0.00001785
Iteration 66/1000 | Loss: 0.00001784
Iteration 67/1000 | Loss: 0.00001784
Iteration 68/1000 | Loss: 0.00001784
Iteration 69/1000 | Loss: 0.00001783
Iteration 70/1000 | Loss: 0.00001783
Iteration 71/1000 | Loss: 0.00001783
Iteration 72/1000 | Loss: 0.00001783
Iteration 73/1000 | Loss: 0.00001783
Iteration 74/1000 | Loss: 0.00001782
Iteration 75/1000 | Loss: 0.00001782
Iteration 76/1000 | Loss: 0.00001782
Iteration 77/1000 | Loss: 0.00001782
Iteration 78/1000 | Loss: 0.00001782
Iteration 79/1000 | Loss: 0.00001782
Iteration 80/1000 | Loss: 0.00001782
Iteration 81/1000 | Loss: 0.00001782
Iteration 82/1000 | Loss: 0.00001782
Iteration 83/1000 | Loss: 0.00001782
Iteration 84/1000 | Loss: 0.00001782
Iteration 85/1000 | Loss: 0.00001782
Iteration 86/1000 | Loss: 0.00001782
Iteration 87/1000 | Loss: 0.00001782
Iteration 88/1000 | Loss: 0.00001781
Iteration 89/1000 | Loss: 0.00001781
Iteration 90/1000 | Loss: 0.00001781
Iteration 91/1000 | Loss: 0.00001781
Iteration 92/1000 | Loss: 0.00001780
Iteration 93/1000 | Loss: 0.00001780
Iteration 94/1000 | Loss: 0.00001780
Iteration 95/1000 | Loss: 0.00001780
Iteration 96/1000 | Loss: 0.00001780
Iteration 97/1000 | Loss: 0.00001780
Iteration 98/1000 | Loss: 0.00001780
Iteration 99/1000 | Loss: 0.00001780
Iteration 100/1000 | Loss: 0.00001780
Iteration 101/1000 | Loss: 0.00001779
Iteration 102/1000 | Loss: 0.00001779
Iteration 103/1000 | Loss: 0.00001779
Iteration 104/1000 | Loss: 0.00001779
Iteration 105/1000 | Loss: 0.00001778
Iteration 106/1000 | Loss: 0.00001778
Iteration 107/1000 | Loss: 0.00001778
Iteration 108/1000 | Loss: 0.00001778
Iteration 109/1000 | Loss: 0.00001778
Iteration 110/1000 | Loss: 0.00001778
Iteration 111/1000 | Loss: 0.00001778
Iteration 112/1000 | Loss: 0.00001778
Iteration 113/1000 | Loss: 0.00001778
Iteration 114/1000 | Loss: 0.00001778
Iteration 115/1000 | Loss: 0.00001778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.7779439076548442e-05, 1.7779439076548442e-05, 1.7779439076548442e-05, 1.7779439076548442e-05, 1.7779439076548442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7779439076548442e-05

Optimization complete. Final v2v error: 3.584346294403076 mm

Highest mean error: 3.876431465148926 mm for frame 11

Lowest mean error: 3.313781976699829 mm for frame 170

Saving results

Total time: 34.666258811950684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836429
Iteration 2/25 | Loss: 0.00174201
Iteration 3/25 | Loss: 0.00123443
Iteration 4/25 | Loss: 0.00113542
Iteration 5/25 | Loss: 0.00112020
Iteration 6/25 | Loss: 0.00111861
Iteration 7/25 | Loss: 0.00111861
Iteration 8/25 | Loss: 0.00111861
Iteration 9/25 | Loss: 0.00111861
Iteration 10/25 | Loss: 0.00111861
Iteration 11/25 | Loss: 0.00111861
Iteration 12/25 | Loss: 0.00111861
Iteration 13/25 | Loss: 0.00111861
Iteration 14/25 | Loss: 0.00111861
Iteration 15/25 | Loss: 0.00111861
Iteration 16/25 | Loss: 0.00111861
Iteration 17/25 | Loss: 0.00111861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011186087504029274, 0.0011186087504029274, 0.0011186087504029274, 0.0011186087504029274, 0.0011186087504029274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011186087504029274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19915974
Iteration 2/25 | Loss: 0.00200190
Iteration 3/25 | Loss: 0.00200190
Iteration 4/25 | Loss: 0.00200190
Iteration 5/25 | Loss: 0.00200190
Iteration 6/25 | Loss: 0.00200190
Iteration 7/25 | Loss: 0.00200190
Iteration 8/25 | Loss: 0.00200190
Iteration 9/25 | Loss: 0.00200190
Iteration 10/25 | Loss: 0.00200190
Iteration 11/25 | Loss: 0.00200190
Iteration 12/25 | Loss: 0.00200190
Iteration 13/25 | Loss: 0.00200190
Iteration 14/25 | Loss: 0.00200190
Iteration 15/25 | Loss: 0.00200190
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0020018992945551872, 0.0020018992945551872, 0.0020018992945551872, 0.0020018992945551872, 0.0020018992945551872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020018992945551872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200190
Iteration 2/1000 | Loss: 0.00002967
Iteration 3/1000 | Loss: 0.00002012
Iteration 4/1000 | Loss: 0.00001863
Iteration 5/1000 | Loss: 0.00001771
Iteration 6/1000 | Loss: 0.00001694
Iteration 7/1000 | Loss: 0.00001652
Iteration 8/1000 | Loss: 0.00001605
Iteration 9/1000 | Loss: 0.00001565
Iteration 10/1000 | Loss: 0.00001541
Iteration 11/1000 | Loss: 0.00001523
Iteration 12/1000 | Loss: 0.00001522
Iteration 13/1000 | Loss: 0.00001519
Iteration 14/1000 | Loss: 0.00001518
Iteration 15/1000 | Loss: 0.00001517
Iteration 16/1000 | Loss: 0.00001517
Iteration 17/1000 | Loss: 0.00001515
Iteration 18/1000 | Loss: 0.00001512
Iteration 19/1000 | Loss: 0.00001511
Iteration 20/1000 | Loss: 0.00001511
Iteration 21/1000 | Loss: 0.00001510
Iteration 22/1000 | Loss: 0.00001506
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001505
Iteration 25/1000 | Loss: 0.00001505
Iteration 26/1000 | Loss: 0.00001505
Iteration 27/1000 | Loss: 0.00001505
Iteration 28/1000 | Loss: 0.00001505
Iteration 29/1000 | Loss: 0.00001505
Iteration 30/1000 | Loss: 0.00001505
Iteration 31/1000 | Loss: 0.00001505
Iteration 32/1000 | Loss: 0.00001505
Iteration 33/1000 | Loss: 0.00001504
Iteration 34/1000 | Loss: 0.00001501
Iteration 35/1000 | Loss: 0.00001501
Iteration 36/1000 | Loss: 0.00001500
Iteration 37/1000 | Loss: 0.00001500
Iteration 38/1000 | Loss: 0.00001499
Iteration 39/1000 | Loss: 0.00001499
Iteration 40/1000 | Loss: 0.00001497
Iteration 41/1000 | Loss: 0.00001497
Iteration 42/1000 | Loss: 0.00001497
Iteration 43/1000 | Loss: 0.00001497
Iteration 44/1000 | Loss: 0.00001497
Iteration 45/1000 | Loss: 0.00001496
Iteration 46/1000 | Loss: 0.00001496
Iteration 47/1000 | Loss: 0.00001496
Iteration 48/1000 | Loss: 0.00001496
Iteration 49/1000 | Loss: 0.00001496
Iteration 50/1000 | Loss: 0.00001496
Iteration 51/1000 | Loss: 0.00001496
Iteration 52/1000 | Loss: 0.00001496
Iteration 53/1000 | Loss: 0.00001496
Iteration 54/1000 | Loss: 0.00001496
Iteration 55/1000 | Loss: 0.00001495
Iteration 56/1000 | Loss: 0.00001495
Iteration 57/1000 | Loss: 0.00001494
Iteration 58/1000 | Loss: 0.00001494
Iteration 59/1000 | Loss: 0.00001493
Iteration 60/1000 | Loss: 0.00001493
Iteration 61/1000 | Loss: 0.00001493
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001492
Iteration 65/1000 | Loss: 0.00001492
Iteration 66/1000 | Loss: 0.00001492
Iteration 67/1000 | Loss: 0.00001492
Iteration 68/1000 | Loss: 0.00001492
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001491
Iteration 71/1000 | Loss: 0.00001491
Iteration 72/1000 | Loss: 0.00001491
Iteration 73/1000 | Loss: 0.00001491
Iteration 74/1000 | Loss: 0.00001491
Iteration 75/1000 | Loss: 0.00001491
Iteration 76/1000 | Loss: 0.00001490
Iteration 77/1000 | Loss: 0.00001490
Iteration 78/1000 | Loss: 0.00001490
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001490
Iteration 82/1000 | Loss: 0.00001490
Iteration 83/1000 | Loss: 0.00001490
Iteration 84/1000 | Loss: 0.00001490
Iteration 85/1000 | Loss: 0.00001490
Iteration 86/1000 | Loss: 0.00001490
Iteration 87/1000 | Loss: 0.00001490
Iteration 88/1000 | Loss: 0.00001490
Iteration 89/1000 | Loss: 0.00001490
Iteration 90/1000 | Loss: 0.00001490
Iteration 91/1000 | Loss: 0.00001490
Iteration 92/1000 | Loss: 0.00001489
Iteration 93/1000 | Loss: 0.00001489
Iteration 94/1000 | Loss: 0.00001489
Iteration 95/1000 | Loss: 0.00001489
Iteration 96/1000 | Loss: 0.00001489
Iteration 97/1000 | Loss: 0.00001489
Iteration 98/1000 | Loss: 0.00001489
Iteration 99/1000 | Loss: 0.00001489
Iteration 100/1000 | Loss: 0.00001489
Iteration 101/1000 | Loss: 0.00001489
Iteration 102/1000 | Loss: 0.00001489
Iteration 103/1000 | Loss: 0.00001489
Iteration 104/1000 | Loss: 0.00001489
Iteration 105/1000 | Loss: 0.00001489
Iteration 106/1000 | Loss: 0.00001489
Iteration 107/1000 | Loss: 0.00001489
Iteration 108/1000 | Loss: 0.00001489
Iteration 109/1000 | Loss: 0.00001489
Iteration 110/1000 | Loss: 0.00001489
Iteration 111/1000 | Loss: 0.00001489
Iteration 112/1000 | Loss: 0.00001489
Iteration 113/1000 | Loss: 0.00001489
Iteration 114/1000 | Loss: 0.00001489
Iteration 115/1000 | Loss: 0.00001489
Iteration 116/1000 | Loss: 0.00001489
Iteration 117/1000 | Loss: 0.00001489
Iteration 118/1000 | Loss: 0.00001489
Iteration 119/1000 | Loss: 0.00001489
Iteration 120/1000 | Loss: 0.00001489
Iteration 121/1000 | Loss: 0.00001489
Iteration 122/1000 | Loss: 0.00001489
Iteration 123/1000 | Loss: 0.00001489
Iteration 124/1000 | Loss: 0.00001489
Iteration 125/1000 | Loss: 0.00001489
Iteration 126/1000 | Loss: 0.00001489
Iteration 127/1000 | Loss: 0.00001489
Iteration 128/1000 | Loss: 0.00001489
Iteration 129/1000 | Loss: 0.00001489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.4894874766469002e-05, 1.4894874766469002e-05, 1.4894874766469002e-05, 1.4894874766469002e-05, 1.4894874766469002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4894874766469002e-05

Optimization complete. Final v2v error: 3.1835641860961914 mm

Highest mean error: 3.4885056018829346 mm for frame 79

Lowest mean error: 2.8532443046569824 mm for frame 9

Saving results

Total time: 35.6345374584198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016373
Iteration 2/25 | Loss: 0.00133746
Iteration 3/25 | Loss: 0.00120602
Iteration 4/25 | Loss: 0.00114574
Iteration 5/25 | Loss: 0.00113953
Iteration 6/25 | Loss: 0.00113613
Iteration 7/25 | Loss: 0.00113515
Iteration 8/25 | Loss: 0.00113400
Iteration 9/25 | Loss: 0.00113344
Iteration 10/25 | Loss: 0.00113323
Iteration 11/25 | Loss: 0.00113311
Iteration 12/25 | Loss: 0.00113306
Iteration 13/25 | Loss: 0.00113305
Iteration 14/25 | Loss: 0.00113305
Iteration 15/25 | Loss: 0.00113305
Iteration 16/25 | Loss: 0.00113305
Iteration 17/25 | Loss: 0.00113305
Iteration 18/25 | Loss: 0.00113304
Iteration 19/25 | Loss: 0.00113304
Iteration 20/25 | Loss: 0.00113304
Iteration 21/25 | Loss: 0.00113304
Iteration 22/25 | Loss: 0.00113304
Iteration 23/25 | Loss: 0.00113304
Iteration 24/25 | Loss: 0.00113304
Iteration 25/25 | Loss: 0.00113304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.92976785
Iteration 2/25 | Loss: 0.00233617
Iteration 3/25 | Loss: 0.00233617
Iteration 4/25 | Loss: 0.00233617
Iteration 5/25 | Loss: 0.00233617
Iteration 6/25 | Loss: 0.00233617
Iteration 7/25 | Loss: 0.00233617
Iteration 8/25 | Loss: 0.00233617
Iteration 9/25 | Loss: 0.00233617
Iteration 10/25 | Loss: 0.00233617
Iteration 11/25 | Loss: 0.00233617
Iteration 12/25 | Loss: 0.00233617
Iteration 13/25 | Loss: 0.00233617
Iteration 14/25 | Loss: 0.00233617
Iteration 15/25 | Loss: 0.00233617
Iteration 16/25 | Loss: 0.00233617
Iteration 17/25 | Loss: 0.00233617
Iteration 18/25 | Loss: 0.00233617
Iteration 19/25 | Loss: 0.00233617
Iteration 20/25 | Loss: 0.00233617
Iteration 21/25 | Loss: 0.00233617
Iteration 22/25 | Loss: 0.00233617
Iteration 23/25 | Loss: 0.00233616
Iteration 24/25 | Loss: 0.00233617
Iteration 25/25 | Loss: 0.00233616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233616
Iteration 2/1000 | Loss: 0.00003073
Iteration 3/1000 | Loss: 0.00006292
Iteration 4/1000 | Loss: 0.00002593
Iteration 5/1000 | Loss: 0.00001839
Iteration 6/1000 | Loss: 0.00001614
Iteration 7/1000 | Loss: 0.00001553
Iteration 8/1000 | Loss: 0.00006154
Iteration 9/1000 | Loss: 0.00001492
Iteration 10/1000 | Loss: 0.00001457
Iteration 11/1000 | Loss: 0.00001426
Iteration 12/1000 | Loss: 0.00001407
Iteration 13/1000 | Loss: 0.00001406
Iteration 14/1000 | Loss: 0.00001391
Iteration 15/1000 | Loss: 0.00001387
Iteration 16/1000 | Loss: 0.00006361
Iteration 17/1000 | Loss: 0.00006361
Iteration 18/1000 | Loss: 0.00001815
Iteration 19/1000 | Loss: 0.00001499
Iteration 20/1000 | Loss: 0.00001384
Iteration 21/1000 | Loss: 0.00001375
Iteration 22/1000 | Loss: 0.00001375
Iteration 23/1000 | Loss: 0.00001375
Iteration 24/1000 | Loss: 0.00001375
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001374
Iteration 27/1000 | Loss: 0.00001374
Iteration 28/1000 | Loss: 0.00001374
Iteration 29/1000 | Loss: 0.00001374
Iteration 30/1000 | Loss: 0.00001374
Iteration 31/1000 | Loss: 0.00001374
Iteration 32/1000 | Loss: 0.00001374
Iteration 33/1000 | Loss: 0.00001374
Iteration 34/1000 | Loss: 0.00001374
Iteration 35/1000 | Loss: 0.00001373
Iteration 36/1000 | Loss: 0.00001373
Iteration 37/1000 | Loss: 0.00001373
Iteration 38/1000 | Loss: 0.00001372
Iteration 39/1000 | Loss: 0.00001372
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001372
Iteration 42/1000 | Loss: 0.00001372
Iteration 43/1000 | Loss: 0.00001372
Iteration 44/1000 | Loss: 0.00001371
Iteration 45/1000 | Loss: 0.00001371
Iteration 46/1000 | Loss: 0.00001370
Iteration 47/1000 | Loss: 0.00001369
Iteration 48/1000 | Loss: 0.00001369
Iteration 49/1000 | Loss: 0.00001369
Iteration 50/1000 | Loss: 0.00001369
Iteration 51/1000 | Loss: 0.00001368
Iteration 52/1000 | Loss: 0.00001368
Iteration 53/1000 | Loss: 0.00001368
Iteration 54/1000 | Loss: 0.00001368
Iteration 55/1000 | Loss: 0.00001367
Iteration 56/1000 | Loss: 0.00001367
Iteration 57/1000 | Loss: 0.00001367
Iteration 58/1000 | Loss: 0.00001366
Iteration 59/1000 | Loss: 0.00001366
Iteration 60/1000 | Loss: 0.00001366
Iteration 61/1000 | Loss: 0.00001365
Iteration 62/1000 | Loss: 0.00001365
Iteration 63/1000 | Loss: 0.00001365
Iteration 64/1000 | Loss: 0.00001364
Iteration 65/1000 | Loss: 0.00001364
Iteration 66/1000 | Loss: 0.00001364
Iteration 67/1000 | Loss: 0.00001363
Iteration 68/1000 | Loss: 0.00001363
Iteration 69/1000 | Loss: 0.00001363
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001362
Iteration 73/1000 | Loss: 0.00001362
Iteration 74/1000 | Loss: 0.00001362
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001362
Iteration 77/1000 | Loss: 0.00001362
Iteration 78/1000 | Loss: 0.00001362
Iteration 79/1000 | Loss: 0.00001362
Iteration 80/1000 | Loss: 0.00001362
Iteration 81/1000 | Loss: 0.00001362
Iteration 82/1000 | Loss: 0.00001362
Iteration 83/1000 | Loss: 0.00001362
Iteration 84/1000 | Loss: 0.00001362
Iteration 85/1000 | Loss: 0.00001362
Iteration 86/1000 | Loss: 0.00001362
Iteration 87/1000 | Loss: 0.00001362
Iteration 88/1000 | Loss: 0.00001362
Iteration 89/1000 | Loss: 0.00001362
Iteration 90/1000 | Loss: 0.00001362
Iteration 91/1000 | Loss: 0.00001362
Iteration 92/1000 | Loss: 0.00001362
Iteration 93/1000 | Loss: 0.00001362
Iteration 94/1000 | Loss: 0.00001362
Iteration 95/1000 | Loss: 0.00001361
Iteration 96/1000 | Loss: 0.00001361
Iteration 97/1000 | Loss: 0.00001361
Iteration 98/1000 | Loss: 0.00001361
Iteration 99/1000 | Loss: 0.00001361
Iteration 100/1000 | Loss: 0.00001361
Iteration 101/1000 | Loss: 0.00001360
Iteration 102/1000 | Loss: 0.00001360
Iteration 103/1000 | Loss: 0.00001360
Iteration 104/1000 | Loss: 0.00001360
Iteration 105/1000 | Loss: 0.00001359
Iteration 106/1000 | Loss: 0.00001359
Iteration 107/1000 | Loss: 0.00001359
Iteration 108/1000 | Loss: 0.00001359
Iteration 109/1000 | Loss: 0.00001359
Iteration 110/1000 | Loss: 0.00001359
Iteration 111/1000 | Loss: 0.00001359
Iteration 112/1000 | Loss: 0.00001359
Iteration 113/1000 | Loss: 0.00001359
Iteration 114/1000 | Loss: 0.00001359
Iteration 115/1000 | Loss: 0.00001359
Iteration 116/1000 | Loss: 0.00001359
Iteration 117/1000 | Loss: 0.00001359
Iteration 118/1000 | Loss: 0.00001359
Iteration 119/1000 | Loss: 0.00001359
Iteration 120/1000 | Loss: 0.00001359
Iteration 121/1000 | Loss: 0.00001358
Iteration 122/1000 | Loss: 0.00001358
Iteration 123/1000 | Loss: 0.00001358
Iteration 124/1000 | Loss: 0.00001358
Iteration 125/1000 | Loss: 0.00001358
Iteration 126/1000 | Loss: 0.00001358
Iteration 127/1000 | Loss: 0.00001358
Iteration 128/1000 | Loss: 0.00001358
Iteration 129/1000 | Loss: 0.00001358
Iteration 130/1000 | Loss: 0.00001358
Iteration 131/1000 | Loss: 0.00001358
Iteration 132/1000 | Loss: 0.00001358
Iteration 133/1000 | Loss: 0.00001357
Iteration 134/1000 | Loss: 0.00001357
Iteration 135/1000 | Loss: 0.00001357
Iteration 136/1000 | Loss: 0.00001357
Iteration 137/1000 | Loss: 0.00001357
Iteration 138/1000 | Loss: 0.00001357
Iteration 139/1000 | Loss: 0.00001357
Iteration 140/1000 | Loss: 0.00001357
Iteration 141/1000 | Loss: 0.00001357
Iteration 142/1000 | Loss: 0.00001357
Iteration 143/1000 | Loss: 0.00001357
Iteration 144/1000 | Loss: 0.00001357
Iteration 145/1000 | Loss: 0.00001357
Iteration 146/1000 | Loss: 0.00001357
Iteration 147/1000 | Loss: 0.00001357
Iteration 148/1000 | Loss: 0.00001357
Iteration 149/1000 | Loss: 0.00001357
Iteration 150/1000 | Loss: 0.00001356
Iteration 151/1000 | Loss: 0.00001356
Iteration 152/1000 | Loss: 0.00001356
Iteration 153/1000 | Loss: 0.00001356
Iteration 154/1000 | Loss: 0.00001356
Iteration 155/1000 | Loss: 0.00001356
Iteration 156/1000 | Loss: 0.00001356
Iteration 157/1000 | Loss: 0.00001356
Iteration 158/1000 | Loss: 0.00001356
Iteration 159/1000 | Loss: 0.00001356
Iteration 160/1000 | Loss: 0.00001356
Iteration 161/1000 | Loss: 0.00001356
Iteration 162/1000 | Loss: 0.00001356
Iteration 163/1000 | Loss: 0.00001355
Iteration 164/1000 | Loss: 0.00001355
Iteration 165/1000 | Loss: 0.00001355
Iteration 166/1000 | Loss: 0.00001355
Iteration 167/1000 | Loss: 0.00001355
Iteration 168/1000 | Loss: 0.00001355
Iteration 169/1000 | Loss: 0.00001355
Iteration 170/1000 | Loss: 0.00001355
Iteration 171/1000 | Loss: 0.00001355
Iteration 172/1000 | Loss: 0.00001355
Iteration 173/1000 | Loss: 0.00001355
Iteration 174/1000 | Loss: 0.00001355
Iteration 175/1000 | Loss: 0.00001355
Iteration 176/1000 | Loss: 0.00001355
Iteration 177/1000 | Loss: 0.00001355
Iteration 178/1000 | Loss: 0.00001355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.354876712866826e-05, 1.354876712866826e-05, 1.354876712866826e-05, 1.354876712866826e-05, 1.354876712866826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.354876712866826e-05

Optimization complete. Final v2v error: 3.113206148147583 mm

Highest mean error: 3.619319200515747 mm for frame 172

Lowest mean error: 2.719435453414917 mm for frame 156

Saving results

Total time: 54.82953858375549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00589670
Iteration 2/25 | Loss: 0.00139427
Iteration 3/25 | Loss: 0.00131783
Iteration 4/25 | Loss: 0.00114821
Iteration 5/25 | Loss: 0.00114568
Iteration 6/25 | Loss: 0.00113746
Iteration 7/25 | Loss: 0.00113112
Iteration 8/25 | Loss: 0.00112842
Iteration 9/25 | Loss: 0.00112684
Iteration 10/25 | Loss: 0.00112620
Iteration 11/25 | Loss: 0.00112594
Iteration 12/25 | Loss: 0.00112580
Iteration 13/25 | Loss: 0.00112564
Iteration 14/25 | Loss: 0.00112556
Iteration 15/25 | Loss: 0.00112552
Iteration 16/25 | Loss: 0.00112548
Iteration 17/25 | Loss: 0.00112548
Iteration 18/25 | Loss: 0.00112548
Iteration 19/25 | Loss: 0.00112547
Iteration 20/25 | Loss: 0.00112547
Iteration 21/25 | Loss: 0.00112546
Iteration 22/25 | Loss: 0.00112546
Iteration 23/25 | Loss: 0.00112546
Iteration 24/25 | Loss: 0.00112545
Iteration 25/25 | Loss: 0.00112545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.05883694
Iteration 2/25 | Loss: 0.00216316
Iteration 3/25 | Loss: 0.00216316
Iteration 4/25 | Loss: 0.00216316
Iteration 5/25 | Loss: 0.00216316
Iteration 6/25 | Loss: 0.00216316
Iteration 7/25 | Loss: 0.00216316
Iteration 8/25 | Loss: 0.00216316
Iteration 9/25 | Loss: 0.00216315
Iteration 10/25 | Loss: 0.00216315
Iteration 11/25 | Loss: 0.00216315
Iteration 12/25 | Loss: 0.00216315
Iteration 13/25 | Loss: 0.00216315
Iteration 14/25 | Loss: 0.00216315
Iteration 15/25 | Loss: 0.00216315
Iteration 16/25 | Loss: 0.00216315
Iteration 17/25 | Loss: 0.00216315
Iteration 18/25 | Loss: 0.00216315
Iteration 19/25 | Loss: 0.00216315
Iteration 20/25 | Loss: 0.00216315
Iteration 21/25 | Loss: 0.00216315
Iteration 22/25 | Loss: 0.00216315
Iteration 23/25 | Loss: 0.00216315
Iteration 24/25 | Loss: 0.00216315
Iteration 25/25 | Loss: 0.00216315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216315
Iteration 2/1000 | Loss: 0.00003781
Iteration 3/1000 | Loss: 0.00001803
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001427
Iteration 6/1000 | Loss: 0.00001360
Iteration 7/1000 | Loss: 0.00001293
Iteration 8/1000 | Loss: 0.00001232
Iteration 9/1000 | Loss: 0.00001183
Iteration 10/1000 | Loss: 0.00001155
Iteration 11/1000 | Loss: 0.00001136
Iteration 12/1000 | Loss: 0.00001123
Iteration 13/1000 | Loss: 0.00001109
Iteration 14/1000 | Loss: 0.00001106
Iteration 15/1000 | Loss: 0.00001104
Iteration 16/1000 | Loss: 0.00001103
Iteration 17/1000 | Loss: 0.00001102
Iteration 18/1000 | Loss: 0.00001102
Iteration 19/1000 | Loss: 0.00001102
Iteration 20/1000 | Loss: 0.00001101
Iteration 21/1000 | Loss: 0.00001101
Iteration 22/1000 | Loss: 0.00001097
Iteration 23/1000 | Loss: 0.00001096
Iteration 24/1000 | Loss: 0.00001095
Iteration 25/1000 | Loss: 0.00001094
Iteration 26/1000 | Loss: 0.00001094
Iteration 27/1000 | Loss: 0.00001093
Iteration 28/1000 | Loss: 0.00001106
Iteration 29/1000 | Loss: 0.00001091
Iteration 30/1000 | Loss: 0.00001090
Iteration 31/1000 | Loss: 0.00001089
Iteration 32/1000 | Loss: 0.00001089
Iteration 33/1000 | Loss: 0.00001094
Iteration 34/1000 | Loss: 0.00001087
Iteration 35/1000 | Loss: 0.00001086
Iteration 36/1000 | Loss: 0.00001086
Iteration 37/1000 | Loss: 0.00001095
Iteration 38/1000 | Loss: 0.00001093
Iteration 39/1000 | Loss: 0.00001092
Iteration 40/1000 | Loss: 0.00001079
Iteration 41/1000 | Loss: 0.00001081
Iteration 42/1000 | Loss: 0.00001081
Iteration 43/1000 | Loss: 0.00001078
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001077
Iteration 47/1000 | Loss: 0.00001070
Iteration 48/1000 | Loss: 0.00001070
Iteration 49/1000 | Loss: 0.00001070
Iteration 50/1000 | Loss: 0.00001070
Iteration 51/1000 | Loss: 0.00001070
Iteration 52/1000 | Loss: 0.00001070
Iteration 53/1000 | Loss: 0.00001070
Iteration 54/1000 | Loss: 0.00001069
Iteration 55/1000 | Loss: 0.00001069
Iteration 56/1000 | Loss: 0.00001069
Iteration 57/1000 | Loss: 0.00001069
Iteration 58/1000 | Loss: 0.00001069
Iteration 59/1000 | Loss: 0.00001069
Iteration 60/1000 | Loss: 0.00001069
Iteration 61/1000 | Loss: 0.00001069
Iteration 62/1000 | Loss: 0.00001069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.0693257536331657e-05, 1.0693257536331657e-05, 1.0693257536331657e-05, 1.0693257536331657e-05, 1.0693257536331657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0693257536331657e-05

Optimization complete. Final v2v error: 2.7895774841308594 mm

Highest mean error: 8.566780090332031 mm for frame 223

Lowest mean error: 2.5447685718536377 mm for frame 26

Saving results

Total time: 57.345210313797
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446754
Iteration 2/25 | Loss: 0.00125619
Iteration 3/25 | Loss: 0.00112772
Iteration 4/25 | Loss: 0.00110230
Iteration 5/25 | Loss: 0.00109635
Iteration 6/25 | Loss: 0.00109457
Iteration 7/25 | Loss: 0.00109428
Iteration 8/25 | Loss: 0.00109428
Iteration 9/25 | Loss: 0.00109428
Iteration 10/25 | Loss: 0.00109428
Iteration 11/25 | Loss: 0.00109428
Iteration 12/25 | Loss: 0.00109428
Iteration 13/25 | Loss: 0.00109428
Iteration 14/25 | Loss: 0.00109428
Iteration 15/25 | Loss: 0.00109428
Iteration 16/25 | Loss: 0.00109428
Iteration 17/25 | Loss: 0.00109428
Iteration 18/25 | Loss: 0.00109428
Iteration 19/25 | Loss: 0.00109428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010942837689071894, 0.0010942837689071894, 0.0010942837689071894, 0.0010942837689071894, 0.0010942837689071894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010942837689071894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21431470
Iteration 2/25 | Loss: 0.00199624
Iteration 3/25 | Loss: 0.00199624
Iteration 4/25 | Loss: 0.00199624
Iteration 5/25 | Loss: 0.00199624
Iteration 6/25 | Loss: 0.00199624
Iteration 7/25 | Loss: 0.00199624
Iteration 8/25 | Loss: 0.00199624
Iteration 9/25 | Loss: 0.00199624
Iteration 10/25 | Loss: 0.00199624
Iteration 11/25 | Loss: 0.00199624
Iteration 12/25 | Loss: 0.00199624
Iteration 13/25 | Loss: 0.00199624
Iteration 14/25 | Loss: 0.00199624
Iteration 15/25 | Loss: 0.00199624
Iteration 16/25 | Loss: 0.00199624
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019962370861321688, 0.0019962370861321688, 0.0019962370861321688, 0.0019962370861321688, 0.0019962370861321688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019962370861321688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199624
Iteration 2/1000 | Loss: 0.00003874
Iteration 3/1000 | Loss: 0.00002162
Iteration 4/1000 | Loss: 0.00001927
Iteration 5/1000 | Loss: 0.00001804
Iteration 6/1000 | Loss: 0.00001724
Iteration 7/1000 | Loss: 0.00001680
Iteration 8/1000 | Loss: 0.00001654
Iteration 9/1000 | Loss: 0.00001629
Iteration 10/1000 | Loss: 0.00001603
Iteration 11/1000 | Loss: 0.00001602
Iteration 12/1000 | Loss: 0.00001596
Iteration 13/1000 | Loss: 0.00001581
Iteration 14/1000 | Loss: 0.00001577
Iteration 15/1000 | Loss: 0.00001567
Iteration 16/1000 | Loss: 0.00001565
Iteration 17/1000 | Loss: 0.00001563
Iteration 18/1000 | Loss: 0.00001562
Iteration 19/1000 | Loss: 0.00001561
Iteration 20/1000 | Loss: 0.00001561
Iteration 21/1000 | Loss: 0.00001561
Iteration 22/1000 | Loss: 0.00001560
Iteration 23/1000 | Loss: 0.00001560
Iteration 24/1000 | Loss: 0.00001560
Iteration 25/1000 | Loss: 0.00001560
Iteration 26/1000 | Loss: 0.00001560
Iteration 27/1000 | Loss: 0.00001560
Iteration 28/1000 | Loss: 0.00001560
Iteration 29/1000 | Loss: 0.00001560
Iteration 30/1000 | Loss: 0.00001560
Iteration 31/1000 | Loss: 0.00001559
Iteration 32/1000 | Loss: 0.00001559
Iteration 33/1000 | Loss: 0.00001558
Iteration 34/1000 | Loss: 0.00001557
Iteration 35/1000 | Loss: 0.00001557
Iteration 36/1000 | Loss: 0.00001556
Iteration 37/1000 | Loss: 0.00001556
Iteration 38/1000 | Loss: 0.00001555
Iteration 39/1000 | Loss: 0.00001555
Iteration 40/1000 | Loss: 0.00001555
Iteration 41/1000 | Loss: 0.00001555
Iteration 42/1000 | Loss: 0.00001555
Iteration 43/1000 | Loss: 0.00001555
Iteration 44/1000 | Loss: 0.00001554
Iteration 45/1000 | Loss: 0.00001554
Iteration 46/1000 | Loss: 0.00001553
Iteration 47/1000 | Loss: 0.00001553
Iteration 48/1000 | Loss: 0.00001553
Iteration 49/1000 | Loss: 0.00001553
Iteration 50/1000 | Loss: 0.00001553
Iteration 51/1000 | Loss: 0.00001553
Iteration 52/1000 | Loss: 0.00001553
Iteration 53/1000 | Loss: 0.00001553
Iteration 54/1000 | Loss: 0.00001551
Iteration 55/1000 | Loss: 0.00001551
Iteration 56/1000 | Loss: 0.00001550
Iteration 57/1000 | Loss: 0.00001549
Iteration 58/1000 | Loss: 0.00001549
Iteration 59/1000 | Loss: 0.00001548
Iteration 60/1000 | Loss: 0.00001547
Iteration 61/1000 | Loss: 0.00001547
Iteration 62/1000 | Loss: 0.00001547
Iteration 63/1000 | Loss: 0.00001547
Iteration 64/1000 | Loss: 0.00001547
Iteration 65/1000 | Loss: 0.00001547
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001546
Iteration 68/1000 | Loss: 0.00001546
Iteration 69/1000 | Loss: 0.00001546
Iteration 70/1000 | Loss: 0.00001546
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001545
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001544
Iteration 77/1000 | Loss: 0.00001544
Iteration 78/1000 | Loss: 0.00001544
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001544
Iteration 82/1000 | Loss: 0.00001543
Iteration 83/1000 | Loss: 0.00001543
Iteration 84/1000 | Loss: 0.00001543
Iteration 85/1000 | Loss: 0.00001543
Iteration 86/1000 | Loss: 0.00001543
Iteration 87/1000 | Loss: 0.00001543
Iteration 88/1000 | Loss: 0.00001543
Iteration 89/1000 | Loss: 0.00001543
Iteration 90/1000 | Loss: 0.00001543
Iteration 91/1000 | Loss: 0.00001543
Iteration 92/1000 | Loss: 0.00001543
Iteration 93/1000 | Loss: 0.00001543
Iteration 94/1000 | Loss: 0.00001543
Iteration 95/1000 | Loss: 0.00001543
Iteration 96/1000 | Loss: 0.00001543
Iteration 97/1000 | Loss: 0.00001543
Iteration 98/1000 | Loss: 0.00001543
Iteration 99/1000 | Loss: 0.00001543
Iteration 100/1000 | Loss: 0.00001543
Iteration 101/1000 | Loss: 0.00001543
Iteration 102/1000 | Loss: 0.00001543
Iteration 103/1000 | Loss: 0.00001543
Iteration 104/1000 | Loss: 0.00001543
Iteration 105/1000 | Loss: 0.00001543
Iteration 106/1000 | Loss: 0.00001543
Iteration 107/1000 | Loss: 0.00001543
Iteration 108/1000 | Loss: 0.00001543
Iteration 109/1000 | Loss: 0.00001543
Iteration 110/1000 | Loss: 0.00001543
Iteration 111/1000 | Loss: 0.00001543
Iteration 112/1000 | Loss: 0.00001543
Iteration 113/1000 | Loss: 0.00001543
Iteration 114/1000 | Loss: 0.00001543
Iteration 115/1000 | Loss: 0.00001543
Iteration 116/1000 | Loss: 0.00001543
Iteration 117/1000 | Loss: 0.00001543
Iteration 118/1000 | Loss: 0.00001543
Iteration 119/1000 | Loss: 0.00001543
Iteration 120/1000 | Loss: 0.00001543
Iteration 121/1000 | Loss: 0.00001543
Iteration 122/1000 | Loss: 0.00001543
Iteration 123/1000 | Loss: 0.00001543
Iteration 124/1000 | Loss: 0.00001543
Iteration 125/1000 | Loss: 0.00001543
Iteration 126/1000 | Loss: 0.00001543
Iteration 127/1000 | Loss: 0.00001543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.5426187019329518e-05, 1.5426187019329518e-05, 1.5426187019329518e-05, 1.5426187019329518e-05, 1.5426187019329518e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5426187019329518e-05

Optimization complete. Final v2v error: 3.3027660846710205 mm

Highest mean error: 4.133137226104736 mm for frame 29

Lowest mean error: 3.112210988998413 mm for frame 62

Saving results

Total time: 31.812496662139893
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923891
Iteration 2/25 | Loss: 0.00125763
Iteration 3/25 | Loss: 0.00113670
Iteration 4/25 | Loss: 0.00111063
Iteration 5/25 | Loss: 0.00110248
Iteration 6/25 | Loss: 0.00109925
Iteration 7/25 | Loss: 0.00109869
Iteration 8/25 | Loss: 0.00109869
Iteration 9/25 | Loss: 0.00109869
Iteration 10/25 | Loss: 0.00109869
Iteration 11/25 | Loss: 0.00109869
Iteration 12/25 | Loss: 0.00109869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001098686596378684, 0.001098686596378684, 0.001098686596378684, 0.001098686596378684, 0.001098686596378684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001098686596378684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26918960
Iteration 2/25 | Loss: 0.00210957
Iteration 3/25 | Loss: 0.00210956
Iteration 4/25 | Loss: 0.00210955
Iteration 5/25 | Loss: 0.00210955
Iteration 6/25 | Loss: 0.00210955
Iteration 7/25 | Loss: 0.00210955
Iteration 8/25 | Loss: 0.00210955
Iteration 9/25 | Loss: 0.00210955
Iteration 10/25 | Loss: 0.00210955
Iteration 11/25 | Loss: 0.00210955
Iteration 12/25 | Loss: 0.00210955
Iteration 13/25 | Loss: 0.00210955
Iteration 14/25 | Loss: 0.00210955
Iteration 15/25 | Loss: 0.00210955
Iteration 16/25 | Loss: 0.00210955
Iteration 17/25 | Loss: 0.00210955
Iteration 18/25 | Loss: 0.00210955
Iteration 19/25 | Loss: 0.00210955
Iteration 20/25 | Loss: 0.00210955
Iteration 21/25 | Loss: 0.00210955
Iteration 22/25 | Loss: 0.00210955
Iteration 23/25 | Loss: 0.00210955
Iteration 24/25 | Loss: 0.00210955
Iteration 25/25 | Loss: 0.00210955
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0021095501724630594, 0.0021095501724630594, 0.0021095501724630594, 0.0021095501724630594, 0.0021095501724630594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021095501724630594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210955
Iteration 2/1000 | Loss: 0.00005229
Iteration 3/1000 | Loss: 0.00002854
Iteration 4/1000 | Loss: 0.00001977
Iteration 5/1000 | Loss: 0.00001776
Iteration 6/1000 | Loss: 0.00001654
Iteration 7/1000 | Loss: 0.00001569
Iteration 8/1000 | Loss: 0.00001529
Iteration 9/1000 | Loss: 0.00001497
Iteration 10/1000 | Loss: 0.00001474
Iteration 11/1000 | Loss: 0.00001467
Iteration 12/1000 | Loss: 0.00001466
Iteration 13/1000 | Loss: 0.00001455
Iteration 14/1000 | Loss: 0.00001454
Iteration 15/1000 | Loss: 0.00001454
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001435
Iteration 19/1000 | Loss: 0.00001432
Iteration 20/1000 | Loss: 0.00001430
Iteration 21/1000 | Loss: 0.00001427
Iteration 22/1000 | Loss: 0.00001424
Iteration 23/1000 | Loss: 0.00001421
Iteration 24/1000 | Loss: 0.00001421
Iteration 25/1000 | Loss: 0.00001419
Iteration 26/1000 | Loss: 0.00001418
Iteration 27/1000 | Loss: 0.00001418
Iteration 28/1000 | Loss: 0.00001417
Iteration 29/1000 | Loss: 0.00001414
Iteration 30/1000 | Loss: 0.00001413
Iteration 31/1000 | Loss: 0.00001412
Iteration 32/1000 | Loss: 0.00001410
Iteration 33/1000 | Loss: 0.00001409
Iteration 34/1000 | Loss: 0.00001409
Iteration 35/1000 | Loss: 0.00001409
Iteration 36/1000 | Loss: 0.00001408
Iteration 37/1000 | Loss: 0.00001408
Iteration 38/1000 | Loss: 0.00001408
Iteration 39/1000 | Loss: 0.00001408
Iteration 40/1000 | Loss: 0.00001407
Iteration 41/1000 | Loss: 0.00001407
Iteration 42/1000 | Loss: 0.00001406
Iteration 43/1000 | Loss: 0.00001406
Iteration 44/1000 | Loss: 0.00001406
Iteration 45/1000 | Loss: 0.00001405
Iteration 46/1000 | Loss: 0.00001405
Iteration 47/1000 | Loss: 0.00001405
Iteration 48/1000 | Loss: 0.00001405
Iteration 49/1000 | Loss: 0.00001404
Iteration 50/1000 | Loss: 0.00001404
Iteration 51/1000 | Loss: 0.00001404
Iteration 52/1000 | Loss: 0.00001403
Iteration 53/1000 | Loss: 0.00001403
Iteration 54/1000 | Loss: 0.00001403
Iteration 55/1000 | Loss: 0.00001403
Iteration 56/1000 | Loss: 0.00001403
Iteration 57/1000 | Loss: 0.00001402
Iteration 58/1000 | Loss: 0.00001402
Iteration 59/1000 | Loss: 0.00001402
Iteration 60/1000 | Loss: 0.00001402
Iteration 61/1000 | Loss: 0.00001402
Iteration 62/1000 | Loss: 0.00001402
Iteration 63/1000 | Loss: 0.00001402
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001401
Iteration 67/1000 | Loss: 0.00001401
Iteration 68/1000 | Loss: 0.00001400
Iteration 69/1000 | Loss: 0.00001400
Iteration 70/1000 | Loss: 0.00001400
Iteration 71/1000 | Loss: 0.00001400
Iteration 72/1000 | Loss: 0.00001399
Iteration 73/1000 | Loss: 0.00001399
Iteration 74/1000 | Loss: 0.00001399
Iteration 75/1000 | Loss: 0.00001399
Iteration 76/1000 | Loss: 0.00001399
Iteration 77/1000 | Loss: 0.00001399
Iteration 78/1000 | Loss: 0.00001398
Iteration 79/1000 | Loss: 0.00001398
Iteration 80/1000 | Loss: 0.00001398
Iteration 81/1000 | Loss: 0.00001398
Iteration 82/1000 | Loss: 0.00001398
Iteration 83/1000 | Loss: 0.00001397
Iteration 84/1000 | Loss: 0.00001397
Iteration 85/1000 | Loss: 0.00001397
Iteration 86/1000 | Loss: 0.00001397
Iteration 87/1000 | Loss: 0.00001397
Iteration 88/1000 | Loss: 0.00001397
Iteration 89/1000 | Loss: 0.00001397
Iteration 90/1000 | Loss: 0.00001397
Iteration 91/1000 | Loss: 0.00001397
Iteration 92/1000 | Loss: 0.00001397
Iteration 93/1000 | Loss: 0.00001397
Iteration 94/1000 | Loss: 0.00001397
Iteration 95/1000 | Loss: 0.00001396
Iteration 96/1000 | Loss: 0.00001396
Iteration 97/1000 | Loss: 0.00001396
Iteration 98/1000 | Loss: 0.00001396
Iteration 99/1000 | Loss: 0.00001396
Iteration 100/1000 | Loss: 0.00001396
Iteration 101/1000 | Loss: 0.00001396
Iteration 102/1000 | Loss: 0.00001396
Iteration 103/1000 | Loss: 0.00001395
Iteration 104/1000 | Loss: 0.00001395
Iteration 105/1000 | Loss: 0.00001395
Iteration 106/1000 | Loss: 0.00001395
Iteration 107/1000 | Loss: 0.00001395
Iteration 108/1000 | Loss: 0.00001395
Iteration 109/1000 | Loss: 0.00001395
Iteration 110/1000 | Loss: 0.00001395
Iteration 111/1000 | Loss: 0.00001395
Iteration 112/1000 | Loss: 0.00001395
Iteration 113/1000 | Loss: 0.00001395
Iteration 114/1000 | Loss: 0.00001395
Iteration 115/1000 | Loss: 0.00001395
Iteration 116/1000 | Loss: 0.00001395
Iteration 117/1000 | Loss: 0.00001395
Iteration 118/1000 | Loss: 0.00001395
Iteration 119/1000 | Loss: 0.00001395
Iteration 120/1000 | Loss: 0.00001395
Iteration 121/1000 | Loss: 0.00001395
Iteration 122/1000 | Loss: 0.00001395
Iteration 123/1000 | Loss: 0.00001395
Iteration 124/1000 | Loss: 0.00001395
Iteration 125/1000 | Loss: 0.00001395
Iteration 126/1000 | Loss: 0.00001395
Iteration 127/1000 | Loss: 0.00001395
Iteration 128/1000 | Loss: 0.00001395
Iteration 129/1000 | Loss: 0.00001395
Iteration 130/1000 | Loss: 0.00001395
Iteration 131/1000 | Loss: 0.00001395
Iteration 132/1000 | Loss: 0.00001395
Iteration 133/1000 | Loss: 0.00001395
Iteration 134/1000 | Loss: 0.00001395
Iteration 135/1000 | Loss: 0.00001395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.3948669220553711e-05, 1.3948669220553711e-05, 1.3948669220553711e-05, 1.3948669220553711e-05, 1.3948669220553711e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3948669220553711e-05

Optimization complete. Final v2v error: 3.072528839111328 mm

Highest mean error: 4.88962459564209 mm for frame 76

Lowest mean error: 2.3306832313537598 mm for frame 140

Saving results

Total time: 35.55254316329956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00619510
Iteration 2/25 | Loss: 0.00141265
Iteration 3/25 | Loss: 0.00122994
Iteration 4/25 | Loss: 0.00121365
Iteration 5/25 | Loss: 0.00120925
Iteration 6/25 | Loss: 0.00120922
Iteration 7/25 | Loss: 0.00120922
Iteration 8/25 | Loss: 0.00120922
Iteration 9/25 | Loss: 0.00120922
Iteration 10/25 | Loss: 0.00120922
Iteration 11/25 | Loss: 0.00120922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001209221431054175, 0.001209221431054175, 0.001209221431054175, 0.001209221431054175, 0.001209221431054175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001209221431054175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17957866
Iteration 2/25 | Loss: 0.00245872
Iteration 3/25 | Loss: 0.00245871
Iteration 4/25 | Loss: 0.00245871
Iteration 5/25 | Loss: 0.00245871
Iteration 6/25 | Loss: 0.00245871
Iteration 7/25 | Loss: 0.00245871
Iteration 8/25 | Loss: 0.00245871
Iteration 9/25 | Loss: 0.00245871
Iteration 10/25 | Loss: 0.00245871
Iteration 11/25 | Loss: 0.00245871
Iteration 12/25 | Loss: 0.00245871
Iteration 13/25 | Loss: 0.00245871
Iteration 14/25 | Loss: 0.00245871
Iteration 15/25 | Loss: 0.00245871
Iteration 16/25 | Loss: 0.00245871
Iteration 17/25 | Loss: 0.00245871
Iteration 18/25 | Loss: 0.00245871
Iteration 19/25 | Loss: 0.00245871
Iteration 20/25 | Loss: 0.00245871
Iteration 21/25 | Loss: 0.00245871
Iteration 22/25 | Loss: 0.00245871
Iteration 23/25 | Loss: 0.00245871
Iteration 24/25 | Loss: 0.00245871
Iteration 25/25 | Loss: 0.00245871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245871
Iteration 2/1000 | Loss: 0.00005392
Iteration 3/1000 | Loss: 0.00003537
Iteration 4/1000 | Loss: 0.00003219
Iteration 5/1000 | Loss: 0.00003089
Iteration 6/1000 | Loss: 0.00003011
Iteration 7/1000 | Loss: 0.00002956
Iteration 8/1000 | Loss: 0.00002918
Iteration 9/1000 | Loss: 0.00002887
Iteration 10/1000 | Loss: 0.00002856
Iteration 11/1000 | Loss: 0.00002850
Iteration 12/1000 | Loss: 0.00002823
Iteration 13/1000 | Loss: 0.00002804
Iteration 14/1000 | Loss: 0.00002792
Iteration 15/1000 | Loss: 0.00002789
Iteration 16/1000 | Loss: 0.00002781
Iteration 17/1000 | Loss: 0.00002781
Iteration 18/1000 | Loss: 0.00002779
Iteration 19/1000 | Loss: 0.00002779
Iteration 20/1000 | Loss: 0.00002779
Iteration 21/1000 | Loss: 0.00002777
Iteration 22/1000 | Loss: 0.00002776
Iteration 23/1000 | Loss: 0.00002776
Iteration 24/1000 | Loss: 0.00002776
Iteration 25/1000 | Loss: 0.00002776
Iteration 26/1000 | Loss: 0.00002776
Iteration 27/1000 | Loss: 0.00002776
Iteration 28/1000 | Loss: 0.00002776
Iteration 29/1000 | Loss: 0.00002776
Iteration 30/1000 | Loss: 0.00002776
Iteration 31/1000 | Loss: 0.00002776
Iteration 32/1000 | Loss: 0.00002776
Iteration 33/1000 | Loss: 0.00002776
Iteration 34/1000 | Loss: 0.00002775
Iteration 35/1000 | Loss: 0.00002775
Iteration 36/1000 | Loss: 0.00002775
Iteration 37/1000 | Loss: 0.00002774
Iteration 38/1000 | Loss: 0.00002774
Iteration 39/1000 | Loss: 0.00002774
Iteration 40/1000 | Loss: 0.00002774
Iteration 41/1000 | Loss: 0.00002774
Iteration 42/1000 | Loss: 0.00002774
Iteration 43/1000 | Loss: 0.00002774
Iteration 44/1000 | Loss: 0.00002774
Iteration 45/1000 | Loss: 0.00002774
Iteration 46/1000 | Loss: 0.00002774
Iteration 47/1000 | Loss: 0.00002774
Iteration 48/1000 | Loss: 0.00002774
Iteration 49/1000 | Loss: 0.00002774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [2.7738491553463973e-05, 2.7738491553463973e-05, 2.7738491553463973e-05, 2.7738491553463973e-05, 2.7738491553463973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7738491553463973e-05

Optimization complete. Final v2v error: 4.251989364624023 mm

Highest mean error: 4.815248489379883 mm for frame 204

Lowest mean error: 3.780355215072632 mm for frame 0

Saving results

Total time: 33.162519454956055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00716511
Iteration 2/25 | Loss: 0.00161842
Iteration 3/25 | Loss: 0.00120819
Iteration 4/25 | Loss: 0.00116423
Iteration 5/25 | Loss: 0.00115632
Iteration 6/25 | Loss: 0.00114292
Iteration 7/25 | Loss: 0.00113940
Iteration 8/25 | Loss: 0.00113962
Iteration 9/25 | Loss: 0.00113662
Iteration 10/25 | Loss: 0.00113389
Iteration 11/25 | Loss: 0.00113270
Iteration 12/25 | Loss: 0.00113233
Iteration 13/25 | Loss: 0.00113285
Iteration 14/25 | Loss: 0.00113142
Iteration 15/25 | Loss: 0.00113095
Iteration 16/25 | Loss: 0.00113082
Iteration 17/25 | Loss: 0.00113080
Iteration 18/25 | Loss: 0.00113080
Iteration 19/25 | Loss: 0.00113079
Iteration 20/25 | Loss: 0.00113079
Iteration 21/25 | Loss: 0.00113079
Iteration 22/25 | Loss: 0.00113079
Iteration 23/25 | Loss: 0.00113079
Iteration 24/25 | Loss: 0.00113079
Iteration 25/25 | Loss: 0.00113079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26527894
Iteration 2/25 | Loss: 0.00132529
Iteration 3/25 | Loss: 0.00132527
Iteration 4/25 | Loss: 0.00132527
Iteration 5/25 | Loss: 0.00132527
Iteration 6/25 | Loss: 0.00132527
Iteration 7/25 | Loss: 0.00132527
Iteration 8/25 | Loss: 0.00132527
Iteration 9/25 | Loss: 0.00132527
Iteration 10/25 | Loss: 0.00132527
Iteration 11/25 | Loss: 0.00132527
Iteration 12/25 | Loss: 0.00132527
Iteration 13/25 | Loss: 0.00132527
Iteration 14/25 | Loss: 0.00132527
Iteration 15/25 | Loss: 0.00132527
Iteration 16/25 | Loss: 0.00132527
Iteration 17/25 | Loss: 0.00132527
Iteration 18/25 | Loss: 0.00132527
Iteration 19/25 | Loss: 0.00132527
Iteration 20/25 | Loss: 0.00132527
Iteration 21/25 | Loss: 0.00132526
Iteration 22/25 | Loss: 0.00132526
Iteration 23/25 | Loss: 0.00132526
Iteration 24/25 | Loss: 0.00132526
Iteration 25/25 | Loss: 0.00132526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132526
Iteration 2/1000 | Loss: 0.00004603
Iteration 3/1000 | Loss: 0.00002434
Iteration 4/1000 | Loss: 0.00002095
Iteration 5/1000 | Loss: 0.00002186
Iteration 6/1000 | Loss: 0.00001918
Iteration 7/1000 | Loss: 0.00001864
Iteration 8/1000 | Loss: 0.00001838
Iteration 9/1000 | Loss: 0.00001817
Iteration 10/1000 | Loss: 0.00001798
Iteration 11/1000 | Loss: 0.00001788
Iteration 12/1000 | Loss: 0.00001774
Iteration 13/1000 | Loss: 0.00001765
Iteration 14/1000 | Loss: 0.00001755
Iteration 15/1000 | Loss: 0.00001747
Iteration 16/1000 | Loss: 0.00001746
Iteration 17/1000 | Loss: 0.00001742
Iteration 18/1000 | Loss: 0.00001735
Iteration 19/1000 | Loss: 0.00001735
Iteration 20/1000 | Loss: 0.00001732
Iteration 21/1000 | Loss: 0.00001732
Iteration 22/1000 | Loss: 0.00001731
Iteration 23/1000 | Loss: 0.00001728
Iteration 24/1000 | Loss: 0.00001728
Iteration 25/1000 | Loss: 0.00001728
Iteration 26/1000 | Loss: 0.00001727
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001727
Iteration 29/1000 | Loss: 0.00001724
Iteration 30/1000 | Loss: 0.00001724
Iteration 31/1000 | Loss: 0.00001723
Iteration 32/1000 | Loss: 0.00001722
Iteration 33/1000 | Loss: 0.00001721
Iteration 34/1000 | Loss: 0.00001721
Iteration 35/1000 | Loss: 0.00001721
Iteration 36/1000 | Loss: 0.00001721
Iteration 37/1000 | Loss: 0.00001721
Iteration 38/1000 | Loss: 0.00001721
Iteration 39/1000 | Loss: 0.00001721
Iteration 40/1000 | Loss: 0.00001721
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001721
Iteration 43/1000 | Loss: 0.00001721
Iteration 44/1000 | Loss: 0.00001721
Iteration 45/1000 | Loss: 0.00001721
Iteration 46/1000 | Loss: 0.00001721
Iteration 47/1000 | Loss: 0.00001721
Iteration 48/1000 | Loss: 0.00001721
Iteration 49/1000 | Loss: 0.00001721
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001721
Iteration 53/1000 | Loss: 0.00001721
Iteration 54/1000 | Loss: 0.00001721
Iteration 55/1000 | Loss: 0.00001721
Iteration 56/1000 | Loss: 0.00001721
Iteration 57/1000 | Loss: 0.00001721
Iteration 58/1000 | Loss: 0.00001721
Iteration 59/1000 | Loss: 0.00001721
Iteration 60/1000 | Loss: 0.00001721
Iteration 61/1000 | Loss: 0.00001721
Iteration 62/1000 | Loss: 0.00001721
Iteration 63/1000 | Loss: 0.00001721
Iteration 64/1000 | Loss: 0.00001721
Iteration 65/1000 | Loss: 0.00001721
Iteration 66/1000 | Loss: 0.00001721
Iteration 67/1000 | Loss: 0.00001721
Iteration 68/1000 | Loss: 0.00001721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.7212390957865864e-05, 1.7212390957865864e-05, 1.7212390957865864e-05, 1.7212390957865864e-05, 1.7212390957865864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7212390957865864e-05

Optimization complete. Final v2v error: 3.3244788646698 mm

Highest mean error: 4.195232391357422 mm for frame 122

Lowest mean error: 2.655792236328125 mm for frame 19

Saving results

Total time: 59.99172329902649
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439462
Iteration 2/25 | Loss: 0.00120533
Iteration 3/25 | Loss: 0.00110306
Iteration 4/25 | Loss: 0.00108190
Iteration 5/25 | Loss: 0.00107696
Iteration 6/25 | Loss: 0.00107544
Iteration 7/25 | Loss: 0.00107534
Iteration 8/25 | Loss: 0.00107534
Iteration 9/25 | Loss: 0.00107534
Iteration 10/25 | Loss: 0.00107534
Iteration 11/25 | Loss: 0.00107534
Iteration 12/25 | Loss: 0.00107534
Iteration 13/25 | Loss: 0.00107534
Iteration 14/25 | Loss: 0.00107534
Iteration 15/25 | Loss: 0.00107534
Iteration 16/25 | Loss: 0.00107534
Iteration 17/25 | Loss: 0.00107534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010753358947113156, 0.0010753358947113156, 0.0010753358947113156, 0.0010753358947113156, 0.0010753358947113156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010753358947113156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21630323
Iteration 2/25 | Loss: 0.00198681
Iteration 3/25 | Loss: 0.00198681
Iteration 4/25 | Loss: 0.00198681
Iteration 5/25 | Loss: 0.00198681
Iteration 6/25 | Loss: 0.00198681
Iteration 7/25 | Loss: 0.00198681
Iteration 8/25 | Loss: 0.00198681
Iteration 9/25 | Loss: 0.00198681
Iteration 10/25 | Loss: 0.00198681
Iteration 11/25 | Loss: 0.00198681
Iteration 12/25 | Loss: 0.00198681
Iteration 13/25 | Loss: 0.00198681
Iteration 14/25 | Loss: 0.00198681
Iteration 15/25 | Loss: 0.00198681
Iteration 16/25 | Loss: 0.00198681
Iteration 17/25 | Loss: 0.00198681
Iteration 18/25 | Loss: 0.00198681
Iteration 19/25 | Loss: 0.00198681
Iteration 20/25 | Loss: 0.00198681
Iteration 21/25 | Loss: 0.00198681
Iteration 22/25 | Loss: 0.00198681
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001986808143556118, 0.001986808143556118, 0.001986808143556118, 0.001986808143556118, 0.001986808143556118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001986808143556118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198681
Iteration 2/1000 | Loss: 0.00003439
Iteration 3/1000 | Loss: 0.00002218
Iteration 4/1000 | Loss: 0.00001792
Iteration 5/1000 | Loss: 0.00001650
Iteration 6/1000 | Loss: 0.00001556
Iteration 7/1000 | Loss: 0.00001488
Iteration 8/1000 | Loss: 0.00001434
Iteration 9/1000 | Loss: 0.00001409
Iteration 10/1000 | Loss: 0.00001398
Iteration 11/1000 | Loss: 0.00001392
Iteration 12/1000 | Loss: 0.00001369
Iteration 13/1000 | Loss: 0.00001366
Iteration 14/1000 | Loss: 0.00001365
Iteration 15/1000 | Loss: 0.00001365
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001357
Iteration 18/1000 | Loss: 0.00001357
Iteration 19/1000 | Loss: 0.00001356
Iteration 20/1000 | Loss: 0.00001346
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001340
Iteration 23/1000 | Loss: 0.00001339
Iteration 24/1000 | Loss: 0.00001338
Iteration 25/1000 | Loss: 0.00001337
Iteration 26/1000 | Loss: 0.00001337
Iteration 27/1000 | Loss: 0.00001335
Iteration 28/1000 | Loss: 0.00001335
Iteration 29/1000 | Loss: 0.00001334
Iteration 30/1000 | Loss: 0.00001334
Iteration 31/1000 | Loss: 0.00001333
Iteration 32/1000 | Loss: 0.00001333
Iteration 33/1000 | Loss: 0.00001332
Iteration 34/1000 | Loss: 0.00001332
Iteration 35/1000 | Loss: 0.00001332
Iteration 36/1000 | Loss: 0.00001332
Iteration 37/1000 | Loss: 0.00001332
Iteration 38/1000 | Loss: 0.00001332
Iteration 39/1000 | Loss: 0.00001331
Iteration 40/1000 | Loss: 0.00001331
Iteration 41/1000 | Loss: 0.00001331
Iteration 42/1000 | Loss: 0.00001330
Iteration 43/1000 | Loss: 0.00001330
Iteration 44/1000 | Loss: 0.00001329
Iteration 45/1000 | Loss: 0.00001329
Iteration 46/1000 | Loss: 0.00001329
Iteration 47/1000 | Loss: 0.00001328
Iteration 48/1000 | Loss: 0.00001328
Iteration 49/1000 | Loss: 0.00001328
Iteration 50/1000 | Loss: 0.00001328
Iteration 51/1000 | Loss: 0.00001327
Iteration 52/1000 | Loss: 0.00001327
Iteration 53/1000 | Loss: 0.00001327
Iteration 54/1000 | Loss: 0.00001327
Iteration 55/1000 | Loss: 0.00001327
Iteration 56/1000 | Loss: 0.00001327
Iteration 57/1000 | Loss: 0.00001327
Iteration 58/1000 | Loss: 0.00001327
Iteration 59/1000 | Loss: 0.00001327
Iteration 60/1000 | Loss: 0.00001326
Iteration 61/1000 | Loss: 0.00001326
Iteration 62/1000 | Loss: 0.00001326
Iteration 63/1000 | Loss: 0.00001326
Iteration 64/1000 | Loss: 0.00001325
Iteration 65/1000 | Loss: 0.00001325
Iteration 66/1000 | Loss: 0.00001325
Iteration 67/1000 | Loss: 0.00001325
Iteration 68/1000 | Loss: 0.00001325
Iteration 69/1000 | Loss: 0.00001324
Iteration 70/1000 | Loss: 0.00001324
Iteration 71/1000 | Loss: 0.00001324
Iteration 72/1000 | Loss: 0.00001324
Iteration 73/1000 | Loss: 0.00001324
Iteration 74/1000 | Loss: 0.00001324
Iteration 75/1000 | Loss: 0.00001324
Iteration 76/1000 | Loss: 0.00001323
Iteration 77/1000 | Loss: 0.00001323
Iteration 78/1000 | Loss: 0.00001323
Iteration 79/1000 | Loss: 0.00001323
Iteration 80/1000 | Loss: 0.00001323
Iteration 81/1000 | Loss: 0.00001322
Iteration 82/1000 | Loss: 0.00001322
Iteration 83/1000 | Loss: 0.00001322
Iteration 84/1000 | Loss: 0.00001322
Iteration 85/1000 | Loss: 0.00001322
Iteration 86/1000 | Loss: 0.00001322
Iteration 87/1000 | Loss: 0.00001322
Iteration 88/1000 | Loss: 0.00001322
Iteration 89/1000 | Loss: 0.00001321
Iteration 90/1000 | Loss: 0.00001321
Iteration 91/1000 | Loss: 0.00001321
Iteration 92/1000 | Loss: 0.00001321
Iteration 93/1000 | Loss: 0.00001321
Iteration 94/1000 | Loss: 0.00001321
Iteration 95/1000 | Loss: 0.00001321
Iteration 96/1000 | Loss: 0.00001321
Iteration 97/1000 | Loss: 0.00001321
Iteration 98/1000 | Loss: 0.00001321
Iteration 99/1000 | Loss: 0.00001321
Iteration 100/1000 | Loss: 0.00001321
Iteration 101/1000 | Loss: 0.00001321
Iteration 102/1000 | Loss: 0.00001320
Iteration 103/1000 | Loss: 0.00001320
Iteration 104/1000 | Loss: 0.00001320
Iteration 105/1000 | Loss: 0.00001320
Iteration 106/1000 | Loss: 0.00001320
Iteration 107/1000 | Loss: 0.00001320
Iteration 108/1000 | Loss: 0.00001320
Iteration 109/1000 | Loss: 0.00001320
Iteration 110/1000 | Loss: 0.00001320
Iteration 111/1000 | Loss: 0.00001320
Iteration 112/1000 | Loss: 0.00001320
Iteration 113/1000 | Loss: 0.00001320
Iteration 114/1000 | Loss: 0.00001320
Iteration 115/1000 | Loss: 0.00001320
Iteration 116/1000 | Loss: 0.00001319
Iteration 117/1000 | Loss: 0.00001319
Iteration 118/1000 | Loss: 0.00001319
Iteration 119/1000 | Loss: 0.00001319
Iteration 120/1000 | Loss: 0.00001319
Iteration 121/1000 | Loss: 0.00001319
Iteration 122/1000 | Loss: 0.00001319
Iteration 123/1000 | Loss: 0.00001319
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001319
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001319
Iteration 128/1000 | Loss: 0.00001319
Iteration 129/1000 | Loss: 0.00001319
Iteration 130/1000 | Loss: 0.00001319
Iteration 131/1000 | Loss: 0.00001319
Iteration 132/1000 | Loss: 0.00001319
Iteration 133/1000 | Loss: 0.00001319
Iteration 134/1000 | Loss: 0.00001319
Iteration 135/1000 | Loss: 0.00001319
Iteration 136/1000 | Loss: 0.00001319
Iteration 137/1000 | Loss: 0.00001318
Iteration 138/1000 | Loss: 0.00001318
Iteration 139/1000 | Loss: 0.00001318
Iteration 140/1000 | Loss: 0.00001318
Iteration 141/1000 | Loss: 0.00001318
Iteration 142/1000 | Loss: 0.00001318
Iteration 143/1000 | Loss: 0.00001318
Iteration 144/1000 | Loss: 0.00001318
Iteration 145/1000 | Loss: 0.00001318
Iteration 146/1000 | Loss: 0.00001318
Iteration 147/1000 | Loss: 0.00001318
Iteration 148/1000 | Loss: 0.00001318
Iteration 149/1000 | Loss: 0.00001318
Iteration 150/1000 | Loss: 0.00001318
Iteration 151/1000 | Loss: 0.00001317
Iteration 152/1000 | Loss: 0.00001317
Iteration 153/1000 | Loss: 0.00001317
Iteration 154/1000 | Loss: 0.00001317
Iteration 155/1000 | Loss: 0.00001317
Iteration 156/1000 | Loss: 0.00001317
Iteration 157/1000 | Loss: 0.00001317
Iteration 158/1000 | Loss: 0.00001317
Iteration 159/1000 | Loss: 0.00001317
Iteration 160/1000 | Loss: 0.00001317
Iteration 161/1000 | Loss: 0.00001317
Iteration 162/1000 | Loss: 0.00001317
Iteration 163/1000 | Loss: 0.00001317
Iteration 164/1000 | Loss: 0.00001317
Iteration 165/1000 | Loss: 0.00001317
Iteration 166/1000 | Loss: 0.00001317
Iteration 167/1000 | Loss: 0.00001317
Iteration 168/1000 | Loss: 0.00001317
Iteration 169/1000 | Loss: 0.00001317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.3170340025681071e-05, 1.3170340025681071e-05, 1.3170340025681071e-05, 1.3170340025681071e-05, 1.3170340025681071e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3170340025681071e-05

Optimization complete. Final v2v error: 3.0640709400177 mm

Highest mean error: 3.255951404571533 mm for frame 95

Lowest mean error: 2.763763666152954 mm for frame 38

Saving results

Total time: 34.645453214645386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049051
Iteration 2/25 | Loss: 0.00377892
Iteration 3/25 | Loss: 0.00274923
Iteration 4/25 | Loss: 0.00226545
Iteration 5/25 | Loss: 0.00237748
Iteration 6/25 | Loss: 0.00206432
Iteration 7/25 | Loss: 0.00184777
Iteration 8/25 | Loss: 0.00172973
Iteration 9/25 | Loss: 0.00169413
Iteration 10/25 | Loss: 0.00161578
Iteration 11/25 | Loss: 0.00157820
Iteration 12/25 | Loss: 0.00153239
Iteration 13/25 | Loss: 0.00152141
Iteration 14/25 | Loss: 0.00150417
Iteration 15/25 | Loss: 0.00150834
Iteration 16/25 | Loss: 0.00149573
Iteration 17/25 | Loss: 0.00149219
Iteration 18/25 | Loss: 0.00146737
Iteration 19/25 | Loss: 0.00146809
Iteration 20/25 | Loss: 0.00145643
Iteration 21/25 | Loss: 0.00145697
Iteration 22/25 | Loss: 0.00145545
Iteration 23/25 | Loss: 0.00145659
Iteration 24/25 | Loss: 0.00145891
Iteration 25/25 | Loss: 0.00145742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19935775
Iteration 2/25 | Loss: 0.00805959
Iteration 3/25 | Loss: 0.00662329
Iteration 4/25 | Loss: 0.00662328
Iteration 5/25 | Loss: 0.00662328
Iteration 6/25 | Loss: 0.00662328
Iteration 7/25 | Loss: 0.00662328
Iteration 8/25 | Loss: 0.00662328
Iteration 9/25 | Loss: 0.00662328
Iteration 10/25 | Loss: 0.00662328
Iteration 11/25 | Loss: 0.00662328
Iteration 12/25 | Loss: 0.00662328
Iteration 13/25 | Loss: 0.00662328
Iteration 14/25 | Loss: 0.00662328
Iteration 15/25 | Loss: 0.00662328
Iteration 16/25 | Loss: 0.00662328
Iteration 17/25 | Loss: 0.00662328
Iteration 18/25 | Loss: 0.00662328
Iteration 19/25 | Loss: 0.00662328
Iteration 20/25 | Loss: 0.00662328
Iteration 21/25 | Loss: 0.00662328
Iteration 22/25 | Loss: 0.00662328
Iteration 23/25 | Loss: 0.00662328
Iteration 24/25 | Loss: 0.00662328
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0066232820972800255, 0.0066232820972800255, 0.0066232820972800255, 0.0066232820972800255, 0.0066232820972800255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0066232820972800255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00662328
Iteration 2/1000 | Loss: 0.00124766
Iteration 3/1000 | Loss: 0.00090497
Iteration 4/1000 | Loss: 0.00066355
Iteration 5/1000 | Loss: 0.00085675
Iteration 6/1000 | Loss: 0.00028826
Iteration 7/1000 | Loss: 0.00041571
Iteration 8/1000 | Loss: 0.00055452
Iteration 9/1000 | Loss: 0.00028949
Iteration 10/1000 | Loss: 0.00049871
Iteration 11/1000 | Loss: 0.00033918
Iteration 12/1000 | Loss: 0.00026285
Iteration 13/1000 | Loss: 0.00024728
Iteration 14/1000 | Loss: 0.00083650
Iteration 15/1000 | Loss: 0.00037590
Iteration 16/1000 | Loss: 0.00024389
Iteration 17/1000 | Loss: 0.00039568
Iteration 18/1000 | Loss: 0.00030643
Iteration 19/1000 | Loss: 0.00030112
Iteration 20/1000 | Loss: 0.00026584
Iteration 21/1000 | Loss: 0.00023360
Iteration 22/1000 | Loss: 0.00043913
Iteration 23/1000 | Loss: 0.00058340
Iteration 24/1000 | Loss: 0.00053317
Iteration 25/1000 | Loss: 0.00047902
Iteration 26/1000 | Loss: 0.00022894
Iteration 27/1000 | Loss: 0.00022546
Iteration 28/1000 | Loss: 0.00063700
Iteration 29/1000 | Loss: 0.00041167
Iteration 30/1000 | Loss: 0.00038391
Iteration 31/1000 | Loss: 0.00024598
Iteration 32/1000 | Loss: 0.00022177
Iteration 33/1000 | Loss: 0.00022063
Iteration 34/1000 | Loss: 0.00031961
Iteration 35/1000 | Loss: 0.00021810
Iteration 36/1000 | Loss: 0.00056688
Iteration 37/1000 | Loss: 0.00021382
Iteration 38/1000 | Loss: 0.00047039
Iteration 39/1000 | Loss: 0.00021120
Iteration 40/1000 | Loss: 0.00081704
Iteration 41/1000 | Loss: 0.00043330
Iteration 42/1000 | Loss: 0.00022327
Iteration 43/1000 | Loss: 0.00020705
Iteration 44/1000 | Loss: 0.00019978
Iteration 45/1000 | Loss: 0.00019352
Iteration 46/1000 | Loss: 0.00018916
Iteration 47/1000 | Loss: 0.00018466
Iteration 48/1000 | Loss: 0.00049888
Iteration 49/1000 | Loss: 0.00020531
Iteration 50/1000 | Loss: 0.00018839
Iteration 51/1000 | Loss: 0.00017520
Iteration 52/1000 | Loss: 0.00037058
Iteration 53/1000 | Loss: 0.00102865
Iteration 54/1000 | Loss: 0.00061216
Iteration 55/1000 | Loss: 0.00081859
Iteration 56/1000 | Loss: 0.00054532
Iteration 57/1000 | Loss: 0.00019358
Iteration 58/1000 | Loss: 0.00018131
Iteration 59/1000 | Loss: 0.00022028
Iteration 60/1000 | Loss: 0.00016613
Iteration 61/1000 | Loss: 0.00032067
Iteration 62/1000 | Loss: 0.00018846
Iteration 63/1000 | Loss: 0.00016877
Iteration 64/1000 | Loss: 0.00016551
Iteration 65/1000 | Loss: 0.00015965
Iteration 66/1000 | Loss: 0.00026878
Iteration 67/1000 | Loss: 0.00015910
Iteration 68/1000 | Loss: 0.00015752
Iteration 69/1000 | Loss: 0.00029214
Iteration 70/1000 | Loss: 0.00015632
Iteration 71/1000 | Loss: 0.00015346
Iteration 72/1000 | Loss: 0.00031012
Iteration 73/1000 | Loss: 0.00049120
Iteration 74/1000 | Loss: 0.00020037
Iteration 75/1000 | Loss: 0.00015489
Iteration 76/1000 | Loss: 0.00015369
Iteration 77/1000 | Loss: 0.00052261
Iteration 78/1000 | Loss: 0.00065944
Iteration 79/1000 | Loss: 0.00015496
Iteration 80/1000 | Loss: 0.00036591
Iteration 81/1000 | Loss: 0.00016748
Iteration 82/1000 | Loss: 0.00029787
Iteration 83/1000 | Loss: 0.00018261
Iteration 84/1000 | Loss: 0.00017631
Iteration 85/1000 | Loss: 0.00041935
Iteration 86/1000 | Loss: 0.00018010
Iteration 87/1000 | Loss: 0.00015272
Iteration 88/1000 | Loss: 0.00042547
Iteration 89/1000 | Loss: 0.00035473
Iteration 90/1000 | Loss: 0.00015991
Iteration 91/1000 | Loss: 0.00038073
Iteration 92/1000 | Loss: 0.00037015
Iteration 93/1000 | Loss: 0.00015965
Iteration 94/1000 | Loss: 0.00023983
Iteration 95/1000 | Loss: 0.00037523
Iteration 96/1000 | Loss: 0.00022999
Iteration 97/1000 | Loss: 0.00015133
Iteration 98/1000 | Loss: 0.00014986
Iteration 99/1000 | Loss: 0.00014938
Iteration 100/1000 | Loss: 0.00059800
Iteration 101/1000 | Loss: 0.00053792
Iteration 102/1000 | Loss: 0.00101987
Iteration 103/1000 | Loss: 0.00131605
Iteration 104/1000 | Loss: 0.00253934
Iteration 105/1000 | Loss: 0.00103771
Iteration 106/1000 | Loss: 0.00149908
Iteration 107/1000 | Loss: 0.00047611
Iteration 108/1000 | Loss: 0.00018296
Iteration 109/1000 | Loss: 0.00109993
Iteration 110/1000 | Loss: 0.00058967
Iteration 111/1000 | Loss: 0.00068739
Iteration 112/1000 | Loss: 0.00016264
Iteration 113/1000 | Loss: 0.00015272
Iteration 114/1000 | Loss: 0.00031883
Iteration 115/1000 | Loss: 0.00015173
Iteration 116/1000 | Loss: 0.00036718
Iteration 117/1000 | Loss: 0.00014443
Iteration 118/1000 | Loss: 0.00024376
Iteration 119/1000 | Loss: 0.00013982
Iteration 120/1000 | Loss: 0.00013800
Iteration 121/1000 | Loss: 0.00026692
Iteration 122/1000 | Loss: 0.00013769
Iteration 123/1000 | Loss: 0.00013681
Iteration 124/1000 | Loss: 0.00035055
Iteration 125/1000 | Loss: 0.00013637
Iteration 126/1000 | Loss: 0.00013591
Iteration 127/1000 | Loss: 0.00013562
Iteration 128/1000 | Loss: 0.00013549
Iteration 129/1000 | Loss: 0.00013537
Iteration 130/1000 | Loss: 0.00013536
Iteration 131/1000 | Loss: 0.00013536
Iteration 132/1000 | Loss: 0.00013535
Iteration 133/1000 | Loss: 0.00013535
Iteration 134/1000 | Loss: 0.00013535
Iteration 135/1000 | Loss: 0.00013533
Iteration 136/1000 | Loss: 0.00013531
Iteration 137/1000 | Loss: 0.00013530
Iteration 138/1000 | Loss: 0.00013525
Iteration 139/1000 | Loss: 0.00013525
Iteration 140/1000 | Loss: 0.00013518
Iteration 141/1000 | Loss: 0.00013517
Iteration 142/1000 | Loss: 0.00013516
Iteration 143/1000 | Loss: 0.00013515
Iteration 144/1000 | Loss: 0.00013515
Iteration 145/1000 | Loss: 0.00013515
Iteration 146/1000 | Loss: 0.00013515
Iteration 147/1000 | Loss: 0.00013515
Iteration 148/1000 | Loss: 0.00013515
Iteration 149/1000 | Loss: 0.00013514
Iteration 150/1000 | Loss: 0.00013514
Iteration 151/1000 | Loss: 0.00013514
Iteration 152/1000 | Loss: 0.00013514
Iteration 153/1000 | Loss: 0.00013514
Iteration 154/1000 | Loss: 0.00013514
Iteration 155/1000 | Loss: 0.00013514
Iteration 156/1000 | Loss: 0.00013514
Iteration 157/1000 | Loss: 0.00013513
Iteration 158/1000 | Loss: 0.00013513
Iteration 159/1000 | Loss: 0.00013513
Iteration 160/1000 | Loss: 0.00013513
Iteration 161/1000 | Loss: 0.00013513
Iteration 162/1000 | Loss: 0.00013513
Iteration 163/1000 | Loss: 0.00013513
Iteration 164/1000 | Loss: 0.00013513
Iteration 165/1000 | Loss: 0.00013513
Iteration 166/1000 | Loss: 0.00013512
Iteration 167/1000 | Loss: 0.00013512
Iteration 168/1000 | Loss: 0.00013512
Iteration 169/1000 | Loss: 0.00013512
Iteration 170/1000 | Loss: 0.00013511
Iteration 171/1000 | Loss: 0.00013511
Iteration 172/1000 | Loss: 0.00013510
Iteration 173/1000 | Loss: 0.00013510
Iteration 174/1000 | Loss: 0.00013510
Iteration 175/1000 | Loss: 0.00013510
Iteration 176/1000 | Loss: 0.00013510
Iteration 177/1000 | Loss: 0.00013510
Iteration 178/1000 | Loss: 0.00013510
Iteration 179/1000 | Loss: 0.00013510
Iteration 180/1000 | Loss: 0.00013510
Iteration 181/1000 | Loss: 0.00013510
Iteration 182/1000 | Loss: 0.00013510
Iteration 183/1000 | Loss: 0.00013510
Iteration 184/1000 | Loss: 0.00013510
Iteration 185/1000 | Loss: 0.00013510
Iteration 186/1000 | Loss: 0.00013510
Iteration 187/1000 | Loss: 0.00013509
Iteration 188/1000 | Loss: 0.00013509
Iteration 189/1000 | Loss: 0.00013509
Iteration 190/1000 | Loss: 0.00013509
Iteration 191/1000 | Loss: 0.00013509
Iteration 192/1000 | Loss: 0.00013509
Iteration 193/1000 | Loss: 0.00013509
Iteration 194/1000 | Loss: 0.00013509
Iteration 195/1000 | Loss: 0.00013509
Iteration 196/1000 | Loss: 0.00013508
Iteration 197/1000 | Loss: 0.00013508
Iteration 198/1000 | Loss: 0.00013508
Iteration 199/1000 | Loss: 0.00013508
Iteration 200/1000 | Loss: 0.00013508
Iteration 201/1000 | Loss: 0.00013508
Iteration 202/1000 | Loss: 0.00013508
Iteration 203/1000 | Loss: 0.00013508
Iteration 204/1000 | Loss: 0.00013508
Iteration 205/1000 | Loss: 0.00013508
Iteration 206/1000 | Loss: 0.00013507
Iteration 207/1000 | Loss: 0.00013507
Iteration 208/1000 | Loss: 0.00013507
Iteration 209/1000 | Loss: 0.00013507
Iteration 210/1000 | Loss: 0.00013507
Iteration 211/1000 | Loss: 0.00013507
Iteration 212/1000 | Loss: 0.00013507
Iteration 213/1000 | Loss: 0.00013507
Iteration 214/1000 | Loss: 0.00013507
Iteration 215/1000 | Loss: 0.00013507
Iteration 216/1000 | Loss: 0.00013507
Iteration 217/1000 | Loss: 0.00013507
Iteration 218/1000 | Loss: 0.00013507
Iteration 219/1000 | Loss: 0.00013507
Iteration 220/1000 | Loss: 0.00013507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [0.00013507020776160061, 0.00013507020776160061, 0.00013507020776160061, 0.00013507020776160061, 0.00013507020776160061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013507020776160061

Optimization complete. Final v2v error: 5.785411357879639 mm

Highest mean error: 10.495523452758789 mm for frame 123

Lowest mean error: 3.1024928092956543 mm for frame 151

Saving results

Total time: 226.30411553382874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00617031
Iteration 2/25 | Loss: 0.00135927
Iteration 3/25 | Loss: 0.00120659
Iteration 4/25 | Loss: 0.00119461
Iteration 5/25 | Loss: 0.00119093
Iteration 6/25 | Loss: 0.00119093
Iteration 7/25 | Loss: 0.00119093
Iteration 8/25 | Loss: 0.00119093
Iteration 9/25 | Loss: 0.00119093
Iteration 10/25 | Loss: 0.00119093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011909314198419452, 0.0011909314198419452, 0.0011909314198419452, 0.0011909314198419452, 0.0011909314198419452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011909314198419452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.52678442
Iteration 2/25 | Loss: 0.00184773
Iteration 3/25 | Loss: 0.00184759
Iteration 4/25 | Loss: 0.00184759
Iteration 5/25 | Loss: 0.00184759
Iteration 6/25 | Loss: 0.00184759
Iteration 7/25 | Loss: 0.00184759
Iteration 8/25 | Loss: 0.00184759
Iteration 9/25 | Loss: 0.00184759
Iteration 10/25 | Loss: 0.00184759
Iteration 11/25 | Loss: 0.00184759
Iteration 12/25 | Loss: 0.00184759
Iteration 13/25 | Loss: 0.00184759
Iteration 14/25 | Loss: 0.00184759
Iteration 15/25 | Loss: 0.00184759
Iteration 16/25 | Loss: 0.00184759
Iteration 17/25 | Loss: 0.00184759
Iteration 18/25 | Loss: 0.00184759
Iteration 19/25 | Loss: 0.00184759
Iteration 20/25 | Loss: 0.00184759
Iteration 21/25 | Loss: 0.00184759
Iteration 22/25 | Loss: 0.00184759
Iteration 23/25 | Loss: 0.00184759
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0018475872930139303, 0.0018475872930139303, 0.0018475872930139303, 0.0018475872930139303, 0.0018475872930139303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018475872930139303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184759
Iteration 2/1000 | Loss: 0.00005273
Iteration 3/1000 | Loss: 0.00002529
Iteration 4/1000 | Loss: 0.00002152
Iteration 5/1000 | Loss: 0.00001967
Iteration 6/1000 | Loss: 0.00001861
Iteration 7/1000 | Loss: 0.00001791
Iteration 8/1000 | Loss: 0.00001741
Iteration 9/1000 | Loss: 0.00001704
Iteration 10/1000 | Loss: 0.00001679
Iteration 11/1000 | Loss: 0.00001671
Iteration 12/1000 | Loss: 0.00001664
Iteration 13/1000 | Loss: 0.00001656
Iteration 14/1000 | Loss: 0.00001645
Iteration 15/1000 | Loss: 0.00001643
Iteration 16/1000 | Loss: 0.00001643
Iteration 17/1000 | Loss: 0.00001640
Iteration 18/1000 | Loss: 0.00001640
Iteration 19/1000 | Loss: 0.00001640
Iteration 20/1000 | Loss: 0.00001640
Iteration 21/1000 | Loss: 0.00001640
Iteration 22/1000 | Loss: 0.00001640
Iteration 23/1000 | Loss: 0.00001640
Iteration 24/1000 | Loss: 0.00001640
Iteration 25/1000 | Loss: 0.00001640
Iteration 26/1000 | Loss: 0.00001639
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001639
Iteration 30/1000 | Loss: 0.00001639
Iteration 31/1000 | Loss: 0.00001638
Iteration 32/1000 | Loss: 0.00001638
Iteration 33/1000 | Loss: 0.00001637
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001637
Iteration 36/1000 | Loss: 0.00001637
Iteration 37/1000 | Loss: 0.00001636
Iteration 38/1000 | Loss: 0.00001636
Iteration 39/1000 | Loss: 0.00001636
Iteration 40/1000 | Loss: 0.00001635
Iteration 41/1000 | Loss: 0.00001635
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001634
Iteration 44/1000 | Loss: 0.00001634
Iteration 45/1000 | Loss: 0.00001633
Iteration 46/1000 | Loss: 0.00001633
Iteration 47/1000 | Loss: 0.00001632
Iteration 48/1000 | Loss: 0.00001632
Iteration 49/1000 | Loss: 0.00001632
Iteration 50/1000 | Loss: 0.00001632
Iteration 51/1000 | Loss: 0.00001632
Iteration 52/1000 | Loss: 0.00001631
Iteration 53/1000 | Loss: 0.00001631
Iteration 54/1000 | Loss: 0.00001631
Iteration 55/1000 | Loss: 0.00001631
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001631
Iteration 58/1000 | Loss: 0.00001631
Iteration 59/1000 | Loss: 0.00001631
Iteration 60/1000 | Loss: 0.00001629
Iteration 61/1000 | Loss: 0.00001629
Iteration 62/1000 | Loss: 0.00001629
Iteration 63/1000 | Loss: 0.00001629
Iteration 64/1000 | Loss: 0.00001628
Iteration 65/1000 | Loss: 0.00001628
Iteration 66/1000 | Loss: 0.00001628
Iteration 67/1000 | Loss: 0.00001627
Iteration 68/1000 | Loss: 0.00001627
Iteration 69/1000 | Loss: 0.00001627
Iteration 70/1000 | Loss: 0.00001627
Iteration 71/1000 | Loss: 0.00001627
Iteration 72/1000 | Loss: 0.00001626
Iteration 73/1000 | Loss: 0.00001626
Iteration 74/1000 | Loss: 0.00001626
Iteration 75/1000 | Loss: 0.00001625
Iteration 76/1000 | Loss: 0.00001625
Iteration 77/1000 | Loss: 0.00001625
Iteration 78/1000 | Loss: 0.00001624
Iteration 79/1000 | Loss: 0.00001624
Iteration 80/1000 | Loss: 0.00001624
Iteration 81/1000 | Loss: 0.00001624
Iteration 82/1000 | Loss: 0.00001624
Iteration 83/1000 | Loss: 0.00001624
Iteration 84/1000 | Loss: 0.00001623
Iteration 85/1000 | Loss: 0.00001623
Iteration 86/1000 | Loss: 0.00001623
Iteration 87/1000 | Loss: 0.00001623
Iteration 88/1000 | Loss: 0.00001623
Iteration 89/1000 | Loss: 0.00001623
Iteration 90/1000 | Loss: 0.00001623
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001622
Iteration 94/1000 | Loss: 0.00001622
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001622
Iteration 97/1000 | Loss: 0.00001622
Iteration 98/1000 | Loss: 0.00001622
Iteration 99/1000 | Loss: 0.00001622
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001621
Iteration 102/1000 | Loss: 0.00001621
Iteration 103/1000 | Loss: 0.00001621
Iteration 104/1000 | Loss: 0.00001621
Iteration 105/1000 | Loss: 0.00001621
Iteration 106/1000 | Loss: 0.00001621
Iteration 107/1000 | Loss: 0.00001621
Iteration 108/1000 | Loss: 0.00001621
Iteration 109/1000 | Loss: 0.00001621
Iteration 110/1000 | Loss: 0.00001621
Iteration 111/1000 | Loss: 0.00001621
Iteration 112/1000 | Loss: 0.00001621
Iteration 113/1000 | Loss: 0.00001621
Iteration 114/1000 | Loss: 0.00001621
Iteration 115/1000 | Loss: 0.00001621
Iteration 116/1000 | Loss: 0.00001621
Iteration 117/1000 | Loss: 0.00001621
Iteration 118/1000 | Loss: 0.00001621
Iteration 119/1000 | Loss: 0.00001621
Iteration 120/1000 | Loss: 0.00001621
Iteration 121/1000 | Loss: 0.00001621
Iteration 122/1000 | Loss: 0.00001621
Iteration 123/1000 | Loss: 0.00001621
Iteration 124/1000 | Loss: 0.00001621
Iteration 125/1000 | Loss: 0.00001621
Iteration 126/1000 | Loss: 0.00001621
Iteration 127/1000 | Loss: 0.00001621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.6209583918680437e-05, 1.6209583918680437e-05, 1.6209583918680437e-05, 1.6209583918680437e-05, 1.6209583918680437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6209583918680437e-05

Optimization complete. Final v2v error: 3.356233596801758 mm

Highest mean error: 3.6864938735961914 mm for frame 224

Lowest mean error: 3.0122153759002686 mm for frame 209

Saving results

Total time: 35.99336814880371
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025750
Iteration 2/25 | Loss: 0.00247244
Iteration 3/25 | Loss: 0.00180849
Iteration 4/25 | Loss: 0.00159587
Iteration 5/25 | Loss: 0.00151317
Iteration 6/25 | Loss: 0.00142626
Iteration 7/25 | Loss: 0.00138911
Iteration 8/25 | Loss: 0.00134644
Iteration 9/25 | Loss: 0.00132780
Iteration 10/25 | Loss: 0.00131535
Iteration 11/25 | Loss: 0.00131374
Iteration 12/25 | Loss: 0.00132111
Iteration 13/25 | Loss: 0.00131558
Iteration 14/25 | Loss: 0.00131318
Iteration 15/25 | Loss: 0.00132024
Iteration 16/25 | Loss: 0.00131299
Iteration 17/25 | Loss: 0.00131295
Iteration 18/25 | Loss: 0.00131294
Iteration 19/25 | Loss: 0.00131294
Iteration 20/25 | Loss: 0.00131294
Iteration 21/25 | Loss: 0.00131293
Iteration 22/25 | Loss: 0.00131293
Iteration 23/25 | Loss: 0.00131293
Iteration 24/25 | Loss: 0.00131293
Iteration 25/25 | Loss: 0.00131293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24206269
Iteration 2/25 | Loss: 0.00344179
Iteration 3/25 | Loss: 0.00253206
Iteration 4/25 | Loss: 0.00253206
Iteration 5/25 | Loss: 0.00253205
Iteration 6/25 | Loss: 0.00253205
Iteration 7/25 | Loss: 0.00253205
Iteration 8/25 | Loss: 0.00253205
Iteration 9/25 | Loss: 0.00253205
Iteration 10/25 | Loss: 0.00253205
Iteration 11/25 | Loss: 0.00253205
Iteration 12/25 | Loss: 0.00253205
Iteration 13/25 | Loss: 0.00253205
Iteration 14/25 | Loss: 0.00253205
Iteration 15/25 | Loss: 0.00253205
Iteration 16/25 | Loss: 0.00253205
Iteration 17/25 | Loss: 0.00253205
Iteration 18/25 | Loss: 0.00253205
Iteration 19/25 | Loss: 0.00253205
Iteration 20/25 | Loss: 0.00253205
Iteration 21/25 | Loss: 0.00253205
Iteration 22/25 | Loss: 0.00253205
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0025320507120341063, 0.0025320507120341063, 0.0025320507120341063, 0.0025320507120341063, 0.0025320507120341063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025320507120341063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00253205
Iteration 2/1000 | Loss: 0.00337214
Iteration 3/1000 | Loss: 0.00393382
Iteration 4/1000 | Loss: 0.00193963
Iteration 5/1000 | Loss: 0.00349258
Iteration 6/1000 | Loss: 0.00169079
Iteration 7/1000 | Loss: 0.00213585
Iteration 8/1000 | Loss: 0.00082353
Iteration 9/1000 | Loss: 0.00140878
Iteration 10/1000 | Loss: 0.00086320
Iteration 11/1000 | Loss: 0.00136524
Iteration 12/1000 | Loss: 0.00260746
Iteration 13/1000 | Loss: 0.00024889
Iteration 14/1000 | Loss: 0.00149063
Iteration 15/1000 | Loss: 0.00017010
Iteration 16/1000 | Loss: 0.00006123
Iteration 17/1000 | Loss: 0.00301700
Iteration 18/1000 | Loss: 0.00066631
Iteration 19/1000 | Loss: 0.00032449
Iteration 20/1000 | Loss: 0.00012946
Iteration 21/1000 | Loss: 0.00004581
Iteration 22/1000 | Loss: 0.00033860
Iteration 23/1000 | Loss: 0.00255425
Iteration 24/1000 | Loss: 0.00175712
Iteration 25/1000 | Loss: 0.00130446
Iteration 26/1000 | Loss: 0.00141218
Iteration 27/1000 | Loss: 0.00187292
Iteration 28/1000 | Loss: 0.00126481
Iteration 29/1000 | Loss: 0.00318241
Iteration 30/1000 | Loss: 0.00184006
Iteration 31/1000 | Loss: 0.00015873
Iteration 32/1000 | Loss: 0.00203747
Iteration 33/1000 | Loss: 0.00240791
Iteration 34/1000 | Loss: 0.00175210
Iteration 35/1000 | Loss: 0.00221153
Iteration 36/1000 | Loss: 0.00053106
Iteration 37/1000 | Loss: 0.00007368
Iteration 38/1000 | Loss: 0.00019945
Iteration 39/1000 | Loss: 0.00005265
Iteration 40/1000 | Loss: 0.00011037
Iteration 41/1000 | Loss: 0.00004423
Iteration 42/1000 | Loss: 0.00004250
Iteration 43/1000 | Loss: 0.00004125
Iteration 44/1000 | Loss: 0.00004051
Iteration 45/1000 | Loss: 0.00107411
Iteration 46/1000 | Loss: 0.00066911
Iteration 47/1000 | Loss: 0.00005136
Iteration 48/1000 | Loss: 0.00006673
Iteration 49/1000 | Loss: 0.00107013
Iteration 50/1000 | Loss: 0.00090819
Iteration 51/1000 | Loss: 0.00011793
Iteration 52/1000 | Loss: 0.00137565
Iteration 53/1000 | Loss: 0.00082160
Iteration 54/1000 | Loss: 0.00005475
Iteration 55/1000 | Loss: 0.00004149
Iteration 56/1000 | Loss: 0.00020365
Iteration 57/1000 | Loss: 0.00005746
Iteration 58/1000 | Loss: 0.00010095
Iteration 59/1000 | Loss: 0.00210987
Iteration 60/1000 | Loss: 0.00320686
Iteration 61/1000 | Loss: 0.00282658
Iteration 62/1000 | Loss: 0.00008732
Iteration 63/1000 | Loss: 0.00006529
Iteration 64/1000 | Loss: 0.00006551
Iteration 65/1000 | Loss: 0.00121500
Iteration 66/1000 | Loss: 0.00111087
Iteration 67/1000 | Loss: 0.00088302
Iteration 68/1000 | Loss: 0.00181589
Iteration 69/1000 | Loss: 0.00110979
Iteration 70/1000 | Loss: 0.00147681
Iteration 71/1000 | Loss: 0.00066482
Iteration 72/1000 | Loss: 0.00177462
Iteration 73/1000 | Loss: 0.00112146
Iteration 74/1000 | Loss: 0.00007520
Iteration 75/1000 | Loss: 0.00149328
Iteration 76/1000 | Loss: 0.00114345
Iteration 77/1000 | Loss: 0.00026177
Iteration 78/1000 | Loss: 0.00009327
Iteration 79/1000 | Loss: 0.00223430
Iteration 80/1000 | Loss: 0.00014766
Iteration 81/1000 | Loss: 0.00004761
Iteration 82/1000 | Loss: 0.00031446
Iteration 83/1000 | Loss: 0.00004298
Iteration 84/1000 | Loss: 0.00004101
Iteration 85/1000 | Loss: 0.00003949
Iteration 86/1000 | Loss: 0.00003857
Iteration 87/1000 | Loss: 0.00003801
Iteration 88/1000 | Loss: 0.00003751
Iteration 89/1000 | Loss: 0.00003717
Iteration 90/1000 | Loss: 0.00036857
Iteration 91/1000 | Loss: 0.00022586
Iteration 92/1000 | Loss: 0.00036953
Iteration 93/1000 | Loss: 0.00021669
Iteration 94/1000 | Loss: 0.00038332
Iteration 95/1000 | Loss: 0.00081221
Iteration 96/1000 | Loss: 0.00008245
Iteration 97/1000 | Loss: 0.00006124
Iteration 98/1000 | Loss: 0.00003874
Iteration 99/1000 | Loss: 0.00004485
Iteration 100/1000 | Loss: 0.00003549
Iteration 101/1000 | Loss: 0.00121130
Iteration 102/1000 | Loss: 0.00026815
Iteration 103/1000 | Loss: 0.00069652
Iteration 104/1000 | Loss: 0.00076850
Iteration 105/1000 | Loss: 0.00004418
Iteration 106/1000 | Loss: 0.00125945
Iteration 107/1000 | Loss: 0.00061138
Iteration 108/1000 | Loss: 0.00039439
Iteration 109/1000 | Loss: 0.00068668
Iteration 110/1000 | Loss: 0.00005114
Iteration 111/1000 | Loss: 0.00003942
Iteration 112/1000 | Loss: 0.00002800
Iteration 113/1000 | Loss: 0.00002525
Iteration 114/1000 | Loss: 0.00002411
Iteration 115/1000 | Loss: 0.00002309
Iteration 116/1000 | Loss: 0.00002254
Iteration 117/1000 | Loss: 0.00002201
Iteration 118/1000 | Loss: 0.00002154
Iteration 119/1000 | Loss: 0.00002117
Iteration 120/1000 | Loss: 0.00002110
Iteration 121/1000 | Loss: 0.00002094
Iteration 122/1000 | Loss: 0.00002081
Iteration 123/1000 | Loss: 0.00002080
Iteration 124/1000 | Loss: 0.00002078
Iteration 125/1000 | Loss: 0.00002063
Iteration 126/1000 | Loss: 0.00002061
Iteration 127/1000 | Loss: 0.00002060
Iteration 128/1000 | Loss: 0.00002060
Iteration 129/1000 | Loss: 0.00002060
Iteration 130/1000 | Loss: 0.00002060
Iteration 131/1000 | Loss: 0.00002060
Iteration 132/1000 | Loss: 0.00002060
Iteration 133/1000 | Loss: 0.00002060
Iteration 134/1000 | Loss: 0.00002058
Iteration 135/1000 | Loss: 0.00002054
Iteration 136/1000 | Loss: 0.00002053
Iteration 137/1000 | Loss: 0.00002053
Iteration 138/1000 | Loss: 0.00002053
Iteration 139/1000 | Loss: 0.00002052
Iteration 140/1000 | Loss: 0.00002050
Iteration 141/1000 | Loss: 0.00002049
Iteration 142/1000 | Loss: 0.00002049
Iteration 143/1000 | Loss: 0.00002046
Iteration 144/1000 | Loss: 0.00002045
Iteration 145/1000 | Loss: 0.00002043
Iteration 146/1000 | Loss: 0.00002041
Iteration 147/1000 | Loss: 0.00002040
Iteration 148/1000 | Loss: 0.00002040
Iteration 149/1000 | Loss: 0.00002038
Iteration 150/1000 | Loss: 0.00002038
Iteration 151/1000 | Loss: 0.00002037
Iteration 152/1000 | Loss: 0.00002037
Iteration 153/1000 | Loss: 0.00002037
Iteration 154/1000 | Loss: 0.00002037
Iteration 155/1000 | Loss: 0.00002037
Iteration 156/1000 | Loss: 0.00002037
Iteration 157/1000 | Loss: 0.00002037
Iteration 158/1000 | Loss: 0.00002037
Iteration 159/1000 | Loss: 0.00002037
Iteration 160/1000 | Loss: 0.00002036
Iteration 161/1000 | Loss: 0.00002036
Iteration 162/1000 | Loss: 0.00002036
Iteration 163/1000 | Loss: 0.00002036
Iteration 164/1000 | Loss: 0.00002035
Iteration 165/1000 | Loss: 0.00002035
Iteration 166/1000 | Loss: 0.00002034
Iteration 167/1000 | Loss: 0.00002034
Iteration 168/1000 | Loss: 0.00002032
Iteration 169/1000 | Loss: 0.00004793
Iteration 170/1000 | Loss: 0.00005501
Iteration 171/1000 | Loss: 0.00005230
Iteration 172/1000 | Loss: 0.00003638
Iteration 173/1000 | Loss: 0.00002270
Iteration 174/1000 | Loss: 0.00004726
Iteration 175/1000 | Loss: 0.00002032
Iteration 176/1000 | Loss: 0.00002008
Iteration 177/1000 | Loss: 0.00002008
Iteration 178/1000 | Loss: 0.00002008
Iteration 179/1000 | Loss: 0.00002007
Iteration 180/1000 | Loss: 0.00002007
Iteration 181/1000 | Loss: 0.00002007
Iteration 182/1000 | Loss: 0.00002007
Iteration 183/1000 | Loss: 0.00002007
Iteration 184/1000 | Loss: 0.00002007
Iteration 185/1000 | Loss: 0.00002007
Iteration 186/1000 | Loss: 0.00002007
Iteration 187/1000 | Loss: 0.00002007
Iteration 188/1000 | Loss: 0.00002007
Iteration 189/1000 | Loss: 0.00002006
Iteration 190/1000 | Loss: 0.00002006
Iteration 191/1000 | Loss: 0.00002006
Iteration 192/1000 | Loss: 0.00002006
Iteration 193/1000 | Loss: 0.00002006
Iteration 194/1000 | Loss: 0.00002005
Iteration 195/1000 | Loss: 0.00002005
Iteration 196/1000 | Loss: 0.00002005
Iteration 197/1000 | Loss: 0.00002005
Iteration 198/1000 | Loss: 0.00002005
Iteration 199/1000 | Loss: 0.00002005
Iteration 200/1000 | Loss: 0.00002005
Iteration 201/1000 | Loss: 0.00002005
Iteration 202/1000 | Loss: 0.00002005
Iteration 203/1000 | Loss: 0.00002005
Iteration 204/1000 | Loss: 0.00002005
Iteration 205/1000 | Loss: 0.00002005
Iteration 206/1000 | Loss: 0.00002005
Iteration 207/1000 | Loss: 0.00002004
Iteration 208/1000 | Loss: 0.00002004
Iteration 209/1000 | Loss: 0.00002004
Iteration 210/1000 | Loss: 0.00002004
Iteration 211/1000 | Loss: 0.00002004
Iteration 212/1000 | Loss: 0.00002004
Iteration 213/1000 | Loss: 0.00002004
Iteration 214/1000 | Loss: 0.00002004
Iteration 215/1000 | Loss: 0.00002004
Iteration 216/1000 | Loss: 0.00002004
Iteration 217/1000 | Loss: 0.00002004
Iteration 218/1000 | Loss: 0.00002004
Iteration 219/1000 | Loss: 0.00002003
Iteration 220/1000 | Loss: 0.00002003
Iteration 221/1000 | Loss: 0.00002003
Iteration 222/1000 | Loss: 0.00002002
Iteration 223/1000 | Loss: 0.00002002
Iteration 224/1000 | Loss: 0.00002002
Iteration 225/1000 | Loss: 0.00002001
Iteration 226/1000 | Loss: 0.00002001
Iteration 227/1000 | Loss: 0.00002001
Iteration 228/1000 | Loss: 0.00002001
Iteration 229/1000 | Loss: 0.00002001
Iteration 230/1000 | Loss: 0.00002000
Iteration 231/1000 | Loss: 0.00002000
Iteration 232/1000 | Loss: 0.00002000
Iteration 233/1000 | Loss: 0.00002000
Iteration 234/1000 | Loss: 0.00002000
Iteration 235/1000 | Loss: 0.00002000
Iteration 236/1000 | Loss: 0.00001999
Iteration 237/1000 | Loss: 0.00001999
Iteration 238/1000 | Loss: 0.00001999
Iteration 239/1000 | Loss: 0.00001999
Iteration 240/1000 | Loss: 0.00001999
Iteration 241/1000 | Loss: 0.00001999
Iteration 242/1000 | Loss: 0.00001998
Iteration 243/1000 | Loss: 0.00001998
Iteration 244/1000 | Loss: 0.00001998
Iteration 245/1000 | Loss: 0.00001998
Iteration 246/1000 | Loss: 0.00001997
Iteration 247/1000 | Loss: 0.00001997
Iteration 248/1000 | Loss: 0.00001997
Iteration 249/1000 | Loss: 0.00001996
Iteration 250/1000 | Loss: 0.00001996
Iteration 251/1000 | Loss: 0.00001995
Iteration 252/1000 | Loss: 0.00001995
Iteration 253/1000 | Loss: 0.00001995
Iteration 254/1000 | Loss: 0.00001994
Iteration 255/1000 | Loss: 0.00001994
Iteration 256/1000 | Loss: 0.00001994
Iteration 257/1000 | Loss: 0.00001994
Iteration 258/1000 | Loss: 0.00001994
Iteration 259/1000 | Loss: 0.00001994
Iteration 260/1000 | Loss: 0.00001993
Iteration 261/1000 | Loss: 0.00001993
Iteration 262/1000 | Loss: 0.00001993
Iteration 263/1000 | Loss: 0.00001992
Iteration 264/1000 | Loss: 0.00001992
Iteration 265/1000 | Loss: 0.00001992
Iteration 266/1000 | Loss: 0.00001991
Iteration 267/1000 | Loss: 0.00001991
Iteration 268/1000 | Loss: 0.00001991
Iteration 269/1000 | Loss: 0.00001991
Iteration 270/1000 | Loss: 0.00001991
Iteration 271/1000 | Loss: 0.00001991
Iteration 272/1000 | Loss: 0.00001991
Iteration 273/1000 | Loss: 0.00001991
Iteration 274/1000 | Loss: 0.00001991
Iteration 275/1000 | Loss: 0.00001991
Iteration 276/1000 | Loss: 0.00001991
Iteration 277/1000 | Loss: 0.00001991
Iteration 278/1000 | Loss: 0.00001991
Iteration 279/1000 | Loss: 0.00001991
Iteration 280/1000 | Loss: 0.00001991
Iteration 281/1000 | Loss: 0.00001991
Iteration 282/1000 | Loss: 0.00001991
Iteration 283/1000 | Loss: 0.00001991
Iteration 284/1000 | Loss: 0.00001991
Iteration 285/1000 | Loss: 0.00001991
Iteration 286/1000 | Loss: 0.00001991
Iteration 287/1000 | Loss: 0.00001991
Iteration 288/1000 | Loss: 0.00001991
Iteration 289/1000 | Loss: 0.00001991
Iteration 290/1000 | Loss: 0.00001991
Iteration 291/1000 | Loss: 0.00001991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [1.9905652152374387e-05, 1.9905652152374387e-05, 1.9905652152374387e-05, 1.9905652152374387e-05, 1.9905652152374387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9905652152374387e-05

Optimization complete. Final v2v error: 3.6462388038635254 mm

Highest mean error: 8.302591323852539 mm for frame 119

Lowest mean error: 3.0954606533050537 mm for frame 3

Saving results

Total time: 210.7859194278717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997160
Iteration 2/25 | Loss: 0.00174825
Iteration 3/25 | Loss: 0.00124242
Iteration 4/25 | Loss: 0.00119250
Iteration 5/25 | Loss: 0.00120628
Iteration 6/25 | Loss: 0.00115656
Iteration 7/25 | Loss: 0.00112581
Iteration 8/25 | Loss: 0.00112170
Iteration 9/25 | Loss: 0.00113187
Iteration 10/25 | Loss: 0.00109847
Iteration 11/25 | Loss: 0.00108865
Iteration 12/25 | Loss: 0.00107604
Iteration 13/25 | Loss: 0.00107260
Iteration 14/25 | Loss: 0.00106846
Iteration 15/25 | Loss: 0.00106742
Iteration 16/25 | Loss: 0.00106713
Iteration 17/25 | Loss: 0.00106703
Iteration 18/25 | Loss: 0.00106727
Iteration 19/25 | Loss: 0.00106697
Iteration 20/25 | Loss: 0.00106697
Iteration 21/25 | Loss: 0.00106697
Iteration 22/25 | Loss: 0.00106697
Iteration 23/25 | Loss: 0.00106696
Iteration 24/25 | Loss: 0.00106696
Iteration 25/25 | Loss: 0.00106696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32139266
Iteration 2/25 | Loss: 0.00245714
Iteration 3/25 | Loss: 0.00245714
Iteration 4/25 | Loss: 0.00245713
Iteration 5/25 | Loss: 0.00245713
Iteration 6/25 | Loss: 0.00245713
Iteration 7/25 | Loss: 0.00245713
Iteration 8/25 | Loss: 0.00245713
Iteration 9/25 | Loss: 0.00245713
Iteration 10/25 | Loss: 0.00245713
Iteration 11/25 | Loss: 0.00245713
Iteration 12/25 | Loss: 0.00245713
Iteration 13/25 | Loss: 0.00245713
Iteration 14/25 | Loss: 0.00245713
Iteration 15/25 | Loss: 0.00245713
Iteration 16/25 | Loss: 0.00245713
Iteration 17/25 | Loss: 0.00245713
Iteration 18/25 | Loss: 0.00245713
Iteration 19/25 | Loss: 0.00245713
Iteration 20/25 | Loss: 0.00245713
Iteration 21/25 | Loss: 0.00245713
Iteration 22/25 | Loss: 0.00245713
Iteration 23/25 | Loss: 0.00245713
Iteration 24/25 | Loss: 0.00245713
Iteration 25/25 | Loss: 0.00245713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245713
Iteration 2/1000 | Loss: 0.00004853
Iteration 3/1000 | Loss: 0.00002304
Iteration 4/1000 | Loss: 0.00001585
Iteration 5/1000 | Loss: 0.00001378
Iteration 6/1000 | Loss: 0.00001270
Iteration 7/1000 | Loss: 0.00005765
Iteration 8/1000 | Loss: 0.00001287
Iteration 9/1000 | Loss: 0.00001171
Iteration 10/1000 | Loss: 0.00001153
Iteration 11/1000 | Loss: 0.00001149
Iteration 12/1000 | Loss: 0.00001146
Iteration 13/1000 | Loss: 0.00001135
Iteration 14/1000 | Loss: 0.00001113
Iteration 15/1000 | Loss: 0.00001101
Iteration 16/1000 | Loss: 0.00001100
Iteration 17/1000 | Loss: 0.00001099
Iteration 18/1000 | Loss: 0.00001097
Iteration 19/1000 | Loss: 0.00001096
Iteration 20/1000 | Loss: 0.00001096
Iteration 21/1000 | Loss: 0.00001095
Iteration 22/1000 | Loss: 0.00001092
Iteration 23/1000 | Loss: 0.00001092
Iteration 24/1000 | Loss: 0.00001090
Iteration 25/1000 | Loss: 0.00001090
Iteration 26/1000 | Loss: 0.00001089
Iteration 27/1000 | Loss: 0.00001089
Iteration 28/1000 | Loss: 0.00001089
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001088
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001086
Iteration 33/1000 | Loss: 0.00001083
Iteration 34/1000 | Loss: 0.00001083
Iteration 35/1000 | Loss: 0.00001082
Iteration 36/1000 | Loss: 0.00001081
Iteration 37/1000 | Loss: 0.00001081
Iteration 38/1000 | Loss: 0.00001080
Iteration 39/1000 | Loss: 0.00001079
Iteration 40/1000 | Loss: 0.00001079
Iteration 41/1000 | Loss: 0.00001078
Iteration 42/1000 | Loss: 0.00001078
Iteration 43/1000 | Loss: 0.00001078
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001077
Iteration 47/1000 | Loss: 0.00001077
Iteration 48/1000 | Loss: 0.00001076
Iteration 49/1000 | Loss: 0.00001076
Iteration 50/1000 | Loss: 0.00001076
Iteration 51/1000 | Loss: 0.00001075
Iteration 52/1000 | Loss: 0.00001075
Iteration 53/1000 | Loss: 0.00001075
Iteration 54/1000 | Loss: 0.00001075
Iteration 55/1000 | Loss: 0.00001075
Iteration 56/1000 | Loss: 0.00001075
Iteration 57/1000 | Loss: 0.00001074
Iteration 58/1000 | Loss: 0.00001074
Iteration 59/1000 | Loss: 0.00001074
Iteration 60/1000 | Loss: 0.00001074
Iteration 61/1000 | Loss: 0.00001074
Iteration 62/1000 | Loss: 0.00001074
Iteration 63/1000 | Loss: 0.00001074
Iteration 64/1000 | Loss: 0.00001074
Iteration 65/1000 | Loss: 0.00001074
Iteration 66/1000 | Loss: 0.00001074
Iteration 67/1000 | Loss: 0.00001073
Iteration 68/1000 | Loss: 0.00001073
Iteration 69/1000 | Loss: 0.00001073
Iteration 70/1000 | Loss: 0.00001073
Iteration 71/1000 | Loss: 0.00001073
Iteration 72/1000 | Loss: 0.00001073
Iteration 73/1000 | Loss: 0.00001073
Iteration 74/1000 | Loss: 0.00001073
Iteration 75/1000 | Loss: 0.00001072
Iteration 76/1000 | Loss: 0.00001072
Iteration 77/1000 | Loss: 0.00001072
Iteration 78/1000 | Loss: 0.00001072
Iteration 79/1000 | Loss: 0.00001072
Iteration 80/1000 | Loss: 0.00001072
Iteration 81/1000 | Loss: 0.00001072
Iteration 82/1000 | Loss: 0.00001072
Iteration 83/1000 | Loss: 0.00001072
Iteration 84/1000 | Loss: 0.00001072
Iteration 85/1000 | Loss: 0.00001072
Iteration 86/1000 | Loss: 0.00001072
Iteration 87/1000 | Loss: 0.00001071
Iteration 88/1000 | Loss: 0.00001071
Iteration 89/1000 | Loss: 0.00001071
Iteration 90/1000 | Loss: 0.00001071
Iteration 91/1000 | Loss: 0.00001071
Iteration 92/1000 | Loss: 0.00001071
Iteration 93/1000 | Loss: 0.00001071
Iteration 94/1000 | Loss: 0.00001071
Iteration 95/1000 | Loss: 0.00001071
Iteration 96/1000 | Loss: 0.00001071
Iteration 97/1000 | Loss: 0.00001071
Iteration 98/1000 | Loss: 0.00001071
Iteration 99/1000 | Loss: 0.00001070
Iteration 100/1000 | Loss: 0.00001070
Iteration 101/1000 | Loss: 0.00001070
Iteration 102/1000 | Loss: 0.00001070
Iteration 103/1000 | Loss: 0.00001070
Iteration 104/1000 | Loss: 0.00001070
Iteration 105/1000 | Loss: 0.00001070
Iteration 106/1000 | Loss: 0.00001070
Iteration 107/1000 | Loss: 0.00001070
Iteration 108/1000 | Loss: 0.00001070
Iteration 109/1000 | Loss: 0.00001070
Iteration 110/1000 | Loss: 0.00001070
Iteration 111/1000 | Loss: 0.00001069
Iteration 112/1000 | Loss: 0.00001069
Iteration 113/1000 | Loss: 0.00001069
Iteration 114/1000 | Loss: 0.00001069
Iteration 115/1000 | Loss: 0.00001069
Iteration 116/1000 | Loss: 0.00001069
Iteration 117/1000 | Loss: 0.00001069
Iteration 118/1000 | Loss: 0.00001068
Iteration 119/1000 | Loss: 0.00001068
Iteration 120/1000 | Loss: 0.00001068
Iteration 121/1000 | Loss: 0.00001068
Iteration 122/1000 | Loss: 0.00001068
Iteration 123/1000 | Loss: 0.00001068
Iteration 124/1000 | Loss: 0.00001068
Iteration 125/1000 | Loss: 0.00001068
Iteration 126/1000 | Loss: 0.00001068
Iteration 127/1000 | Loss: 0.00001068
Iteration 128/1000 | Loss: 0.00001068
Iteration 129/1000 | Loss: 0.00001068
Iteration 130/1000 | Loss: 0.00001067
Iteration 131/1000 | Loss: 0.00001067
Iteration 132/1000 | Loss: 0.00001067
Iteration 133/1000 | Loss: 0.00001067
Iteration 134/1000 | Loss: 0.00001067
Iteration 135/1000 | Loss: 0.00001067
Iteration 136/1000 | Loss: 0.00001067
Iteration 137/1000 | Loss: 0.00001067
Iteration 138/1000 | Loss: 0.00001067
Iteration 139/1000 | Loss: 0.00001067
Iteration 140/1000 | Loss: 0.00001067
Iteration 141/1000 | Loss: 0.00001067
Iteration 142/1000 | Loss: 0.00001067
Iteration 143/1000 | Loss: 0.00001066
Iteration 144/1000 | Loss: 0.00001066
Iteration 145/1000 | Loss: 0.00001066
Iteration 146/1000 | Loss: 0.00001066
Iteration 147/1000 | Loss: 0.00001066
Iteration 148/1000 | Loss: 0.00001066
Iteration 149/1000 | Loss: 0.00001066
Iteration 150/1000 | Loss: 0.00001066
Iteration 151/1000 | Loss: 0.00001066
Iteration 152/1000 | Loss: 0.00001066
Iteration 153/1000 | Loss: 0.00001066
Iteration 154/1000 | Loss: 0.00001066
Iteration 155/1000 | Loss: 0.00001066
Iteration 156/1000 | Loss: 0.00001066
Iteration 157/1000 | Loss: 0.00001065
Iteration 158/1000 | Loss: 0.00001065
Iteration 159/1000 | Loss: 0.00001065
Iteration 160/1000 | Loss: 0.00001065
Iteration 161/1000 | Loss: 0.00001065
Iteration 162/1000 | Loss: 0.00001065
Iteration 163/1000 | Loss: 0.00001065
Iteration 164/1000 | Loss: 0.00001065
Iteration 165/1000 | Loss: 0.00001065
Iteration 166/1000 | Loss: 0.00001065
Iteration 167/1000 | Loss: 0.00001065
Iteration 168/1000 | Loss: 0.00001065
Iteration 169/1000 | Loss: 0.00001065
Iteration 170/1000 | Loss: 0.00001065
Iteration 171/1000 | Loss: 0.00001065
Iteration 172/1000 | Loss: 0.00001065
Iteration 173/1000 | Loss: 0.00001065
Iteration 174/1000 | Loss: 0.00001065
Iteration 175/1000 | Loss: 0.00001065
Iteration 176/1000 | Loss: 0.00001064
Iteration 177/1000 | Loss: 0.00001064
Iteration 178/1000 | Loss: 0.00001064
Iteration 179/1000 | Loss: 0.00003349
Iteration 180/1000 | Loss: 0.00003349
Iteration 181/1000 | Loss: 0.00001148
Iteration 182/1000 | Loss: 0.00001067
Iteration 183/1000 | Loss: 0.00001064
Iteration 184/1000 | Loss: 0.00001063
Iteration 185/1000 | Loss: 0.00001062
Iteration 186/1000 | Loss: 0.00001061
Iteration 187/1000 | Loss: 0.00001061
Iteration 188/1000 | Loss: 0.00001061
Iteration 189/1000 | Loss: 0.00001061
Iteration 190/1000 | Loss: 0.00001061
Iteration 191/1000 | Loss: 0.00001060
Iteration 192/1000 | Loss: 0.00001060
Iteration 193/1000 | Loss: 0.00001060
Iteration 194/1000 | Loss: 0.00001060
Iteration 195/1000 | Loss: 0.00001060
Iteration 196/1000 | Loss: 0.00001060
Iteration 197/1000 | Loss: 0.00001060
Iteration 198/1000 | Loss: 0.00001060
Iteration 199/1000 | Loss: 0.00001060
Iteration 200/1000 | Loss: 0.00001060
Iteration 201/1000 | Loss: 0.00001060
Iteration 202/1000 | Loss: 0.00001060
Iteration 203/1000 | Loss: 0.00001060
Iteration 204/1000 | Loss: 0.00001060
Iteration 205/1000 | Loss: 0.00001060
Iteration 206/1000 | Loss: 0.00001060
Iteration 207/1000 | Loss: 0.00001060
Iteration 208/1000 | Loss: 0.00001060
Iteration 209/1000 | Loss: 0.00001060
Iteration 210/1000 | Loss: 0.00001060
Iteration 211/1000 | Loss: 0.00001060
Iteration 212/1000 | Loss: 0.00001060
Iteration 213/1000 | Loss: 0.00001060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.059808892023284e-05, 1.059808892023284e-05, 1.059808892023284e-05, 1.059808892023284e-05, 1.059808892023284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.059808892023284e-05

Optimization complete. Final v2v error: 2.772068500518799 mm

Highest mean error: 4.298802852630615 mm for frame 48

Lowest mean error: 2.239665985107422 mm for frame 115

Saving results

Total time: 65.8003044128418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758278
Iteration 2/25 | Loss: 0.00132008
Iteration 3/25 | Loss: 0.00120129
Iteration 4/25 | Loss: 0.00119161
Iteration 5/25 | Loss: 0.00118983
Iteration 6/25 | Loss: 0.00118976
Iteration 7/25 | Loss: 0.00118976
Iteration 8/25 | Loss: 0.00118970
Iteration 9/25 | Loss: 0.00118970
Iteration 10/25 | Loss: 0.00118970
Iteration 11/25 | Loss: 0.00118970
Iteration 12/25 | Loss: 0.00118970
Iteration 13/25 | Loss: 0.00118970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00118970253970474, 0.00118970253970474, 0.00118970253970474, 0.00118970253970474, 0.00118970253970474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00118970253970474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28679490
Iteration 2/25 | Loss: 0.00189779
Iteration 3/25 | Loss: 0.00189779
Iteration 4/25 | Loss: 0.00189779
Iteration 5/25 | Loss: 0.00189779
Iteration 6/25 | Loss: 0.00189779
Iteration 7/25 | Loss: 0.00189779
Iteration 8/25 | Loss: 0.00189778
Iteration 9/25 | Loss: 0.00189778
Iteration 10/25 | Loss: 0.00189778
Iteration 11/25 | Loss: 0.00189778
Iteration 12/25 | Loss: 0.00189778
Iteration 13/25 | Loss: 0.00189778
Iteration 14/25 | Loss: 0.00189778
Iteration 15/25 | Loss: 0.00189778
Iteration 16/25 | Loss: 0.00189778
Iteration 17/25 | Loss: 0.00189778
Iteration 18/25 | Loss: 0.00189778
Iteration 19/25 | Loss: 0.00189778
Iteration 20/25 | Loss: 0.00189778
Iteration 21/25 | Loss: 0.00189778
Iteration 22/25 | Loss: 0.00189778
Iteration 23/25 | Loss: 0.00189778
Iteration 24/25 | Loss: 0.00189778
Iteration 25/25 | Loss: 0.00189778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189778
Iteration 2/1000 | Loss: 0.00005836
Iteration 3/1000 | Loss: 0.00003001
Iteration 4/1000 | Loss: 0.00002504
Iteration 5/1000 | Loss: 0.00002214
Iteration 6/1000 | Loss: 0.00002031
Iteration 7/1000 | Loss: 0.00001921
Iteration 8/1000 | Loss: 0.00001843
Iteration 9/1000 | Loss: 0.00001798
Iteration 10/1000 | Loss: 0.00001775
Iteration 11/1000 | Loss: 0.00001756
Iteration 12/1000 | Loss: 0.00001755
Iteration 13/1000 | Loss: 0.00001743
Iteration 14/1000 | Loss: 0.00001739
Iteration 15/1000 | Loss: 0.00001739
Iteration 16/1000 | Loss: 0.00001739
Iteration 17/1000 | Loss: 0.00001738
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00001727
Iteration 20/1000 | Loss: 0.00001727
Iteration 21/1000 | Loss: 0.00001724
Iteration 22/1000 | Loss: 0.00001723
Iteration 23/1000 | Loss: 0.00001723
Iteration 24/1000 | Loss: 0.00001723
Iteration 25/1000 | Loss: 0.00001723
Iteration 26/1000 | Loss: 0.00001722
Iteration 27/1000 | Loss: 0.00001722
Iteration 28/1000 | Loss: 0.00001722
Iteration 29/1000 | Loss: 0.00001720
Iteration 30/1000 | Loss: 0.00001720
Iteration 31/1000 | Loss: 0.00001720
Iteration 32/1000 | Loss: 0.00001720
Iteration 33/1000 | Loss: 0.00001720
Iteration 34/1000 | Loss: 0.00001719
Iteration 35/1000 | Loss: 0.00001719
Iteration 36/1000 | Loss: 0.00001719
Iteration 37/1000 | Loss: 0.00001718
Iteration 38/1000 | Loss: 0.00001718
Iteration 39/1000 | Loss: 0.00001716
Iteration 40/1000 | Loss: 0.00001716
Iteration 41/1000 | Loss: 0.00001716
Iteration 42/1000 | Loss: 0.00001716
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001715
Iteration 46/1000 | Loss: 0.00001715
Iteration 47/1000 | Loss: 0.00001715
Iteration 48/1000 | Loss: 0.00001715
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001713
Iteration 51/1000 | Loss: 0.00001713
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001712
Iteration 55/1000 | Loss: 0.00001712
Iteration 56/1000 | Loss: 0.00001711
Iteration 57/1000 | Loss: 0.00001711
Iteration 58/1000 | Loss: 0.00001711
Iteration 59/1000 | Loss: 0.00001710
Iteration 60/1000 | Loss: 0.00001710
Iteration 61/1000 | Loss: 0.00001710
Iteration 62/1000 | Loss: 0.00001710
Iteration 63/1000 | Loss: 0.00001710
Iteration 64/1000 | Loss: 0.00001710
Iteration 65/1000 | Loss: 0.00001710
Iteration 66/1000 | Loss: 0.00001710
Iteration 67/1000 | Loss: 0.00001710
Iteration 68/1000 | Loss: 0.00001709
Iteration 69/1000 | Loss: 0.00001709
Iteration 70/1000 | Loss: 0.00001709
Iteration 71/1000 | Loss: 0.00001708
Iteration 72/1000 | Loss: 0.00001707
Iteration 73/1000 | Loss: 0.00001707
Iteration 74/1000 | Loss: 0.00001707
Iteration 75/1000 | Loss: 0.00001707
Iteration 76/1000 | Loss: 0.00001707
Iteration 77/1000 | Loss: 0.00001707
Iteration 78/1000 | Loss: 0.00001707
Iteration 79/1000 | Loss: 0.00001706
Iteration 80/1000 | Loss: 0.00001706
Iteration 81/1000 | Loss: 0.00001706
Iteration 82/1000 | Loss: 0.00001706
Iteration 83/1000 | Loss: 0.00001706
Iteration 84/1000 | Loss: 0.00001706
Iteration 85/1000 | Loss: 0.00001706
Iteration 86/1000 | Loss: 0.00001706
Iteration 87/1000 | Loss: 0.00001706
Iteration 88/1000 | Loss: 0.00001705
Iteration 89/1000 | Loss: 0.00001705
Iteration 90/1000 | Loss: 0.00001705
Iteration 91/1000 | Loss: 0.00001705
Iteration 92/1000 | Loss: 0.00001705
Iteration 93/1000 | Loss: 0.00001705
Iteration 94/1000 | Loss: 0.00001705
Iteration 95/1000 | Loss: 0.00001705
Iteration 96/1000 | Loss: 0.00001705
Iteration 97/1000 | Loss: 0.00001705
Iteration 98/1000 | Loss: 0.00001705
Iteration 99/1000 | Loss: 0.00001705
Iteration 100/1000 | Loss: 0.00001705
Iteration 101/1000 | Loss: 0.00001704
Iteration 102/1000 | Loss: 0.00001704
Iteration 103/1000 | Loss: 0.00001704
Iteration 104/1000 | Loss: 0.00001703
Iteration 105/1000 | Loss: 0.00001703
Iteration 106/1000 | Loss: 0.00001703
Iteration 107/1000 | Loss: 0.00001703
Iteration 108/1000 | Loss: 0.00001703
Iteration 109/1000 | Loss: 0.00001703
Iteration 110/1000 | Loss: 0.00001703
Iteration 111/1000 | Loss: 0.00001703
Iteration 112/1000 | Loss: 0.00001702
Iteration 113/1000 | Loss: 0.00001702
Iteration 114/1000 | Loss: 0.00001702
Iteration 115/1000 | Loss: 0.00001702
Iteration 116/1000 | Loss: 0.00001702
Iteration 117/1000 | Loss: 0.00001702
Iteration 118/1000 | Loss: 0.00001702
Iteration 119/1000 | Loss: 0.00001702
Iteration 120/1000 | Loss: 0.00001702
Iteration 121/1000 | Loss: 0.00001702
Iteration 122/1000 | Loss: 0.00001701
Iteration 123/1000 | Loss: 0.00001701
Iteration 124/1000 | Loss: 0.00001701
Iteration 125/1000 | Loss: 0.00001701
Iteration 126/1000 | Loss: 0.00001701
Iteration 127/1000 | Loss: 0.00001701
Iteration 128/1000 | Loss: 0.00001700
Iteration 129/1000 | Loss: 0.00001700
Iteration 130/1000 | Loss: 0.00001700
Iteration 131/1000 | Loss: 0.00001700
Iteration 132/1000 | Loss: 0.00001700
Iteration 133/1000 | Loss: 0.00001700
Iteration 134/1000 | Loss: 0.00001700
Iteration 135/1000 | Loss: 0.00001700
Iteration 136/1000 | Loss: 0.00001699
Iteration 137/1000 | Loss: 0.00001699
Iteration 138/1000 | Loss: 0.00001699
Iteration 139/1000 | Loss: 0.00001699
Iteration 140/1000 | Loss: 0.00001699
Iteration 141/1000 | Loss: 0.00001699
Iteration 142/1000 | Loss: 0.00001699
Iteration 143/1000 | Loss: 0.00001699
Iteration 144/1000 | Loss: 0.00001699
Iteration 145/1000 | Loss: 0.00001699
Iteration 146/1000 | Loss: 0.00001699
Iteration 147/1000 | Loss: 0.00001699
Iteration 148/1000 | Loss: 0.00001699
Iteration 149/1000 | Loss: 0.00001699
Iteration 150/1000 | Loss: 0.00001698
Iteration 151/1000 | Loss: 0.00001698
Iteration 152/1000 | Loss: 0.00001698
Iteration 153/1000 | Loss: 0.00001698
Iteration 154/1000 | Loss: 0.00001698
Iteration 155/1000 | Loss: 0.00001698
Iteration 156/1000 | Loss: 0.00001698
Iteration 157/1000 | Loss: 0.00001698
Iteration 158/1000 | Loss: 0.00001698
Iteration 159/1000 | Loss: 0.00001698
Iteration 160/1000 | Loss: 0.00001698
Iteration 161/1000 | Loss: 0.00001698
Iteration 162/1000 | Loss: 0.00001698
Iteration 163/1000 | Loss: 0.00001697
Iteration 164/1000 | Loss: 0.00001697
Iteration 165/1000 | Loss: 0.00001697
Iteration 166/1000 | Loss: 0.00001697
Iteration 167/1000 | Loss: 0.00001697
Iteration 168/1000 | Loss: 0.00001697
Iteration 169/1000 | Loss: 0.00001697
Iteration 170/1000 | Loss: 0.00001697
Iteration 171/1000 | Loss: 0.00001697
Iteration 172/1000 | Loss: 0.00001697
Iteration 173/1000 | Loss: 0.00001697
Iteration 174/1000 | Loss: 0.00001697
Iteration 175/1000 | Loss: 0.00001697
Iteration 176/1000 | Loss: 0.00001697
Iteration 177/1000 | Loss: 0.00001696
Iteration 178/1000 | Loss: 0.00001696
Iteration 179/1000 | Loss: 0.00001696
Iteration 180/1000 | Loss: 0.00001696
Iteration 181/1000 | Loss: 0.00001696
Iteration 182/1000 | Loss: 0.00001696
Iteration 183/1000 | Loss: 0.00001696
Iteration 184/1000 | Loss: 0.00001696
Iteration 185/1000 | Loss: 0.00001696
Iteration 186/1000 | Loss: 0.00001696
Iteration 187/1000 | Loss: 0.00001696
Iteration 188/1000 | Loss: 0.00001696
Iteration 189/1000 | Loss: 0.00001696
Iteration 190/1000 | Loss: 0.00001696
Iteration 191/1000 | Loss: 0.00001696
Iteration 192/1000 | Loss: 0.00001696
Iteration 193/1000 | Loss: 0.00001696
Iteration 194/1000 | Loss: 0.00001696
Iteration 195/1000 | Loss: 0.00001696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.6964273527264595e-05, 1.6964273527264595e-05, 1.6964273527264595e-05, 1.6964273527264595e-05, 1.6964273527264595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6964273527264595e-05

Optimization complete. Final v2v error: 3.4535813331604004 mm

Highest mean error: 3.8834218978881836 mm for frame 53

Lowest mean error: 3.139784574508667 mm for frame 84

Saving results

Total time: 36.95174241065979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078685
Iteration 2/25 | Loss: 0.00268436
Iteration 3/25 | Loss: 0.00181985
Iteration 4/25 | Loss: 0.00161876
Iteration 5/25 | Loss: 0.00157616
Iteration 6/25 | Loss: 0.00152732
Iteration 7/25 | Loss: 0.00153627
Iteration 8/25 | Loss: 0.00149000
Iteration 9/25 | Loss: 0.00146442
Iteration 10/25 | Loss: 0.00145842
Iteration 11/25 | Loss: 0.00145836
Iteration 12/25 | Loss: 0.00145290
Iteration 13/25 | Loss: 0.00144023
Iteration 14/25 | Loss: 0.00143528
Iteration 15/25 | Loss: 0.00143167
Iteration 16/25 | Loss: 0.00143060
Iteration 17/25 | Loss: 0.00143013
Iteration 18/25 | Loss: 0.00142989
Iteration 19/25 | Loss: 0.00142972
Iteration 20/25 | Loss: 0.00142959
Iteration 21/25 | Loss: 0.00142942
Iteration 22/25 | Loss: 0.00142925
Iteration 23/25 | Loss: 0.00142910
Iteration 24/25 | Loss: 0.00142898
Iteration 25/25 | Loss: 0.00142884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08079553
Iteration 2/25 | Loss: 0.00399299
Iteration 3/25 | Loss: 0.00399299
Iteration 4/25 | Loss: 0.00399299
Iteration 5/25 | Loss: 0.00399299
Iteration 6/25 | Loss: 0.00399298
Iteration 7/25 | Loss: 0.00399298
Iteration 8/25 | Loss: 0.00399298
Iteration 9/25 | Loss: 0.00399298
Iteration 10/25 | Loss: 0.00399298
Iteration 11/25 | Loss: 0.00399298
Iteration 12/25 | Loss: 0.00399298
Iteration 13/25 | Loss: 0.00399298
Iteration 14/25 | Loss: 0.00399298
Iteration 15/25 | Loss: 0.00399298
Iteration 16/25 | Loss: 0.00399298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003992982674390078, 0.003992982674390078, 0.003992982674390078, 0.003992982674390078, 0.003992982674390078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003992982674390078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00399298
Iteration 2/1000 | Loss: 0.00068547
Iteration 3/1000 | Loss: 0.00055567
Iteration 4/1000 | Loss: 0.00218046
Iteration 5/1000 | Loss: 0.00040276
Iteration 6/1000 | Loss: 0.00220986
Iteration 7/1000 | Loss: 0.00058563
Iteration 8/1000 | Loss: 0.00129135
Iteration 9/1000 | Loss: 0.00029261
Iteration 10/1000 | Loss: 0.00036421
Iteration 11/1000 | Loss: 0.00066223
Iteration 12/1000 | Loss: 0.00044190
Iteration 13/1000 | Loss: 0.00035079
Iteration 14/1000 | Loss: 0.00019961
Iteration 15/1000 | Loss: 0.00074698
Iteration 16/1000 | Loss: 0.00023048
Iteration 17/1000 | Loss: 0.00092174
Iteration 18/1000 | Loss: 0.00021371
Iteration 19/1000 | Loss: 0.00015060
Iteration 20/1000 | Loss: 0.00055040
Iteration 21/1000 | Loss: 0.00027138
Iteration 22/1000 | Loss: 0.00014097
Iteration 23/1000 | Loss: 0.00013573
Iteration 24/1000 | Loss: 0.00850096
Iteration 25/1000 | Loss: 0.00051802
Iteration 26/1000 | Loss: 0.00035563
Iteration 27/1000 | Loss: 0.00013213
Iteration 28/1000 | Loss: 0.00010728
Iteration 29/1000 | Loss: 0.00071579
Iteration 30/1000 | Loss: 0.00006651
Iteration 31/1000 | Loss: 0.00005580
Iteration 32/1000 | Loss: 0.00045770
Iteration 33/1000 | Loss: 0.00004721
Iteration 34/1000 | Loss: 0.00004261
Iteration 35/1000 | Loss: 0.00041584
Iteration 36/1000 | Loss: 0.00052855
Iteration 37/1000 | Loss: 0.00010482
Iteration 38/1000 | Loss: 0.00003895
Iteration 39/1000 | Loss: 0.00071913
Iteration 40/1000 | Loss: 0.00016297
Iteration 41/1000 | Loss: 0.00020577
Iteration 42/1000 | Loss: 0.00003717
Iteration 43/1000 | Loss: 0.00003581
Iteration 44/1000 | Loss: 0.00003478
Iteration 45/1000 | Loss: 0.00003413
Iteration 46/1000 | Loss: 0.00045068
Iteration 47/1000 | Loss: 0.00019827
Iteration 48/1000 | Loss: 0.00005607
Iteration 49/1000 | Loss: 0.00003321
Iteration 50/1000 | Loss: 0.00003276
Iteration 51/1000 | Loss: 0.00003240
Iteration 52/1000 | Loss: 0.00003218
Iteration 53/1000 | Loss: 0.00003200
Iteration 54/1000 | Loss: 0.00003181
Iteration 55/1000 | Loss: 0.00003173
Iteration 56/1000 | Loss: 0.00003164
Iteration 57/1000 | Loss: 0.00003163
Iteration 58/1000 | Loss: 0.00003161
Iteration 59/1000 | Loss: 0.00003156
Iteration 60/1000 | Loss: 0.00003156
Iteration 61/1000 | Loss: 0.00003154
Iteration 62/1000 | Loss: 0.00003152
Iteration 63/1000 | Loss: 0.00003152
Iteration 64/1000 | Loss: 0.00003151
Iteration 65/1000 | Loss: 0.00003151
Iteration 66/1000 | Loss: 0.00003151
Iteration 67/1000 | Loss: 0.00003150
Iteration 68/1000 | Loss: 0.00003150
Iteration 69/1000 | Loss: 0.00003148
Iteration 70/1000 | Loss: 0.00003145
Iteration 71/1000 | Loss: 0.00003145
Iteration 72/1000 | Loss: 0.00003145
Iteration 73/1000 | Loss: 0.00003144
Iteration 74/1000 | Loss: 0.00003144
Iteration 75/1000 | Loss: 0.00003144
Iteration 76/1000 | Loss: 0.00003144
Iteration 77/1000 | Loss: 0.00003143
Iteration 78/1000 | Loss: 0.00003143
Iteration 79/1000 | Loss: 0.00003143
Iteration 80/1000 | Loss: 0.00003143
Iteration 81/1000 | Loss: 0.00003143
Iteration 82/1000 | Loss: 0.00003143
Iteration 83/1000 | Loss: 0.00003143
Iteration 84/1000 | Loss: 0.00003143
Iteration 85/1000 | Loss: 0.00003143
Iteration 86/1000 | Loss: 0.00003143
Iteration 87/1000 | Loss: 0.00003143
Iteration 88/1000 | Loss: 0.00003143
Iteration 89/1000 | Loss: 0.00003143
Iteration 90/1000 | Loss: 0.00003142
Iteration 91/1000 | Loss: 0.00003142
Iteration 92/1000 | Loss: 0.00003142
Iteration 93/1000 | Loss: 0.00003142
Iteration 94/1000 | Loss: 0.00003141
Iteration 95/1000 | Loss: 0.00003141
Iteration 96/1000 | Loss: 0.00003141
Iteration 97/1000 | Loss: 0.00003141
Iteration 98/1000 | Loss: 0.00003140
Iteration 99/1000 | Loss: 0.00003140
Iteration 100/1000 | Loss: 0.00003140
Iteration 101/1000 | Loss: 0.00003140
Iteration 102/1000 | Loss: 0.00003140
Iteration 103/1000 | Loss: 0.00003140
Iteration 104/1000 | Loss: 0.00003140
Iteration 105/1000 | Loss: 0.00003140
Iteration 106/1000 | Loss: 0.00003139
Iteration 107/1000 | Loss: 0.00003139
Iteration 108/1000 | Loss: 0.00003139
Iteration 109/1000 | Loss: 0.00003139
Iteration 110/1000 | Loss: 0.00003139
Iteration 111/1000 | Loss: 0.00003138
Iteration 112/1000 | Loss: 0.00003138
Iteration 113/1000 | Loss: 0.00003137
Iteration 114/1000 | Loss: 0.00003137
Iteration 115/1000 | Loss: 0.00003137
Iteration 116/1000 | Loss: 0.00003137
Iteration 117/1000 | Loss: 0.00003137
Iteration 118/1000 | Loss: 0.00003137
Iteration 119/1000 | Loss: 0.00003137
Iteration 120/1000 | Loss: 0.00003136
Iteration 121/1000 | Loss: 0.00003136
Iteration 122/1000 | Loss: 0.00003136
Iteration 123/1000 | Loss: 0.00003136
Iteration 124/1000 | Loss: 0.00003136
Iteration 125/1000 | Loss: 0.00003136
Iteration 126/1000 | Loss: 0.00003136
Iteration 127/1000 | Loss: 0.00003135
Iteration 128/1000 | Loss: 0.00003135
Iteration 129/1000 | Loss: 0.00003135
Iteration 130/1000 | Loss: 0.00003135
Iteration 131/1000 | Loss: 0.00003134
Iteration 132/1000 | Loss: 0.00003134
Iteration 133/1000 | Loss: 0.00003134
Iteration 134/1000 | Loss: 0.00003134
Iteration 135/1000 | Loss: 0.00003134
Iteration 136/1000 | Loss: 0.00003134
Iteration 137/1000 | Loss: 0.00003133
Iteration 138/1000 | Loss: 0.00003133
Iteration 139/1000 | Loss: 0.00003133
Iteration 140/1000 | Loss: 0.00003133
Iteration 141/1000 | Loss: 0.00003133
Iteration 142/1000 | Loss: 0.00003133
Iteration 143/1000 | Loss: 0.00003133
Iteration 144/1000 | Loss: 0.00003132
Iteration 145/1000 | Loss: 0.00003132
Iteration 146/1000 | Loss: 0.00003132
Iteration 147/1000 | Loss: 0.00003132
Iteration 148/1000 | Loss: 0.00003132
Iteration 149/1000 | Loss: 0.00003132
Iteration 150/1000 | Loss: 0.00003132
Iteration 151/1000 | Loss: 0.00003132
Iteration 152/1000 | Loss: 0.00003131
Iteration 153/1000 | Loss: 0.00003131
Iteration 154/1000 | Loss: 0.00003131
Iteration 155/1000 | Loss: 0.00003131
Iteration 156/1000 | Loss: 0.00003131
Iteration 157/1000 | Loss: 0.00003131
Iteration 158/1000 | Loss: 0.00003130
Iteration 159/1000 | Loss: 0.00003130
Iteration 160/1000 | Loss: 0.00003130
Iteration 161/1000 | Loss: 0.00003130
Iteration 162/1000 | Loss: 0.00003130
Iteration 163/1000 | Loss: 0.00003130
Iteration 164/1000 | Loss: 0.00003130
Iteration 165/1000 | Loss: 0.00003130
Iteration 166/1000 | Loss: 0.00003130
Iteration 167/1000 | Loss: 0.00003129
Iteration 168/1000 | Loss: 0.00003129
Iteration 169/1000 | Loss: 0.00003129
Iteration 170/1000 | Loss: 0.00003129
Iteration 171/1000 | Loss: 0.00003129
Iteration 172/1000 | Loss: 0.00003128
Iteration 173/1000 | Loss: 0.00003128
Iteration 174/1000 | Loss: 0.00003128
Iteration 175/1000 | Loss: 0.00003128
Iteration 176/1000 | Loss: 0.00003127
Iteration 177/1000 | Loss: 0.00003127
Iteration 178/1000 | Loss: 0.00003127
Iteration 179/1000 | Loss: 0.00003127
Iteration 180/1000 | Loss: 0.00003127
Iteration 181/1000 | Loss: 0.00003126
Iteration 182/1000 | Loss: 0.00003126
Iteration 183/1000 | Loss: 0.00003126
Iteration 184/1000 | Loss: 0.00003126
Iteration 185/1000 | Loss: 0.00003126
Iteration 186/1000 | Loss: 0.00003126
Iteration 187/1000 | Loss: 0.00003126
Iteration 188/1000 | Loss: 0.00003126
Iteration 189/1000 | Loss: 0.00003126
Iteration 190/1000 | Loss: 0.00003126
Iteration 191/1000 | Loss: 0.00003126
Iteration 192/1000 | Loss: 0.00003126
Iteration 193/1000 | Loss: 0.00003126
Iteration 194/1000 | Loss: 0.00003126
Iteration 195/1000 | Loss: 0.00003126
Iteration 196/1000 | Loss: 0.00003126
Iteration 197/1000 | Loss: 0.00003126
Iteration 198/1000 | Loss: 0.00003126
Iteration 199/1000 | Loss: 0.00003126
Iteration 200/1000 | Loss: 0.00003126
Iteration 201/1000 | Loss: 0.00003126
Iteration 202/1000 | Loss: 0.00003126
Iteration 203/1000 | Loss: 0.00003126
Iteration 204/1000 | Loss: 0.00003126
Iteration 205/1000 | Loss: 0.00003126
Iteration 206/1000 | Loss: 0.00003126
Iteration 207/1000 | Loss: 0.00003126
Iteration 208/1000 | Loss: 0.00003126
Iteration 209/1000 | Loss: 0.00003126
Iteration 210/1000 | Loss: 0.00003126
Iteration 211/1000 | Loss: 0.00003126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [3.125697548966855e-05, 3.125697548966855e-05, 3.125697548966855e-05, 3.125697548966855e-05, 3.125697548966855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.125697548966855e-05

Optimization complete. Final v2v error: 3.7867228984832764 mm

Highest mean error: 13.120408058166504 mm for frame 24

Lowest mean error: 2.5609662532806396 mm for frame 3

Saving results

Total time: 127.89023184776306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466340
Iteration 2/25 | Loss: 0.00120229
Iteration 3/25 | Loss: 0.00110619
Iteration 4/25 | Loss: 0.00109139
Iteration 5/25 | Loss: 0.00108810
Iteration 6/25 | Loss: 0.00108692
Iteration 7/25 | Loss: 0.00108692
Iteration 8/25 | Loss: 0.00108692
Iteration 9/25 | Loss: 0.00108692
Iteration 10/25 | Loss: 0.00108692
Iteration 11/25 | Loss: 0.00108692
Iteration 12/25 | Loss: 0.00108692
Iteration 13/25 | Loss: 0.00108692
Iteration 14/25 | Loss: 0.00108692
Iteration 15/25 | Loss: 0.00108692
Iteration 16/25 | Loss: 0.00108692
Iteration 17/25 | Loss: 0.00108692
Iteration 18/25 | Loss: 0.00108692
Iteration 19/25 | Loss: 0.00108692
Iteration 20/25 | Loss: 0.00108692
Iteration 21/25 | Loss: 0.00108692
Iteration 22/25 | Loss: 0.00108692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010869164252653718, 0.0010869164252653718, 0.0010869164252653718, 0.0010869164252653718, 0.0010869164252653718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010869164252653718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.64845371
Iteration 2/25 | Loss: 0.00199221
Iteration 3/25 | Loss: 0.00199220
Iteration 4/25 | Loss: 0.00199219
Iteration 5/25 | Loss: 0.00199219
Iteration 6/25 | Loss: 0.00199219
Iteration 7/25 | Loss: 0.00199219
Iteration 8/25 | Loss: 0.00199219
Iteration 9/25 | Loss: 0.00199219
Iteration 10/25 | Loss: 0.00199219
Iteration 11/25 | Loss: 0.00199219
Iteration 12/25 | Loss: 0.00199219
Iteration 13/25 | Loss: 0.00199219
Iteration 14/25 | Loss: 0.00199219
Iteration 15/25 | Loss: 0.00199219
Iteration 16/25 | Loss: 0.00199219
Iteration 17/25 | Loss: 0.00199219
Iteration 18/25 | Loss: 0.00199219
Iteration 19/25 | Loss: 0.00199219
Iteration 20/25 | Loss: 0.00199219
Iteration 21/25 | Loss: 0.00199219
Iteration 22/25 | Loss: 0.00199219
Iteration 23/25 | Loss: 0.00199219
Iteration 24/25 | Loss: 0.00199219
Iteration 25/25 | Loss: 0.00199219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199219
Iteration 2/1000 | Loss: 0.00002411
Iteration 3/1000 | Loss: 0.00001597
Iteration 4/1000 | Loss: 0.00001417
Iteration 5/1000 | Loss: 0.00001341
Iteration 6/1000 | Loss: 0.00001278
Iteration 7/1000 | Loss: 0.00001242
Iteration 8/1000 | Loss: 0.00001198
Iteration 9/1000 | Loss: 0.00001172
Iteration 10/1000 | Loss: 0.00001160
Iteration 11/1000 | Loss: 0.00001152
Iteration 12/1000 | Loss: 0.00001147
Iteration 13/1000 | Loss: 0.00001146
Iteration 14/1000 | Loss: 0.00001139
Iteration 15/1000 | Loss: 0.00001138
Iteration 16/1000 | Loss: 0.00001130
Iteration 17/1000 | Loss: 0.00001128
Iteration 18/1000 | Loss: 0.00001125
Iteration 19/1000 | Loss: 0.00001124
Iteration 20/1000 | Loss: 0.00001123
Iteration 21/1000 | Loss: 0.00001122
Iteration 22/1000 | Loss: 0.00001122
Iteration 23/1000 | Loss: 0.00001120
Iteration 24/1000 | Loss: 0.00001120
Iteration 25/1000 | Loss: 0.00001119
Iteration 26/1000 | Loss: 0.00001119
Iteration 27/1000 | Loss: 0.00001117
Iteration 28/1000 | Loss: 0.00001117
Iteration 29/1000 | Loss: 0.00001117
Iteration 30/1000 | Loss: 0.00001117
Iteration 31/1000 | Loss: 0.00001117
Iteration 32/1000 | Loss: 0.00001117
Iteration 33/1000 | Loss: 0.00001117
Iteration 34/1000 | Loss: 0.00001116
Iteration 35/1000 | Loss: 0.00001116
Iteration 36/1000 | Loss: 0.00001116
Iteration 37/1000 | Loss: 0.00001115
Iteration 38/1000 | Loss: 0.00001114
Iteration 39/1000 | Loss: 0.00001111
Iteration 40/1000 | Loss: 0.00001111
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001110
Iteration 43/1000 | Loss: 0.00001110
Iteration 44/1000 | Loss: 0.00001109
Iteration 45/1000 | Loss: 0.00001109
Iteration 46/1000 | Loss: 0.00001108
Iteration 47/1000 | Loss: 0.00001107
Iteration 48/1000 | Loss: 0.00001107
Iteration 49/1000 | Loss: 0.00001107
Iteration 50/1000 | Loss: 0.00001106
Iteration 51/1000 | Loss: 0.00001106
Iteration 52/1000 | Loss: 0.00001106
Iteration 53/1000 | Loss: 0.00001106
Iteration 54/1000 | Loss: 0.00001105
Iteration 55/1000 | Loss: 0.00001105
Iteration 56/1000 | Loss: 0.00001105
Iteration 57/1000 | Loss: 0.00001104
Iteration 58/1000 | Loss: 0.00001104
Iteration 59/1000 | Loss: 0.00001104
Iteration 60/1000 | Loss: 0.00001103
Iteration 61/1000 | Loss: 0.00001103
Iteration 62/1000 | Loss: 0.00001103
Iteration 63/1000 | Loss: 0.00001103
Iteration 64/1000 | Loss: 0.00001103
Iteration 65/1000 | Loss: 0.00001102
Iteration 66/1000 | Loss: 0.00001102
Iteration 67/1000 | Loss: 0.00001102
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001101
Iteration 70/1000 | Loss: 0.00001101
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001101
Iteration 74/1000 | Loss: 0.00001101
Iteration 75/1000 | Loss: 0.00001101
Iteration 76/1000 | Loss: 0.00001101
Iteration 77/1000 | Loss: 0.00001101
Iteration 78/1000 | Loss: 0.00001101
Iteration 79/1000 | Loss: 0.00001101
Iteration 80/1000 | Loss: 0.00001101
Iteration 81/1000 | Loss: 0.00001101
Iteration 82/1000 | Loss: 0.00001101
Iteration 83/1000 | Loss: 0.00001101
Iteration 84/1000 | Loss: 0.00001101
Iteration 85/1000 | Loss: 0.00001101
Iteration 86/1000 | Loss: 0.00001101
Iteration 87/1000 | Loss: 0.00001101
Iteration 88/1000 | Loss: 0.00001101
Iteration 89/1000 | Loss: 0.00001101
Iteration 90/1000 | Loss: 0.00001101
Iteration 91/1000 | Loss: 0.00001101
Iteration 92/1000 | Loss: 0.00001101
Iteration 93/1000 | Loss: 0.00001101
Iteration 94/1000 | Loss: 0.00001101
Iteration 95/1000 | Loss: 0.00001101
Iteration 96/1000 | Loss: 0.00001101
Iteration 97/1000 | Loss: 0.00001101
Iteration 98/1000 | Loss: 0.00001101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.1005037777067628e-05, 1.1005037777067628e-05, 1.1005037777067628e-05, 1.1005037777067628e-05, 1.1005037777067628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1005037777067628e-05

Optimization complete. Final v2v error: 2.892397880554199 mm

Highest mean error: 3.4981515407562256 mm for frame 85

Lowest mean error: 2.4760096073150635 mm for frame 195

Saving results

Total time: 36.74211049079895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739137
Iteration 2/25 | Loss: 0.00196606
Iteration 3/25 | Loss: 0.00157456
Iteration 4/25 | Loss: 0.00151821
Iteration 5/25 | Loss: 0.00145661
Iteration 6/25 | Loss: 0.00142949
Iteration 7/25 | Loss: 0.00142912
Iteration 8/25 | Loss: 0.00143877
Iteration 9/25 | Loss: 0.00141521
Iteration 10/25 | Loss: 0.00137779
Iteration 11/25 | Loss: 0.00135766
Iteration 12/25 | Loss: 0.00136757
Iteration 13/25 | Loss: 0.00138349
Iteration 14/25 | Loss: 0.00139763
Iteration 15/25 | Loss: 0.00137705
Iteration 16/25 | Loss: 0.00132097
Iteration 17/25 | Loss: 0.00131581
Iteration 18/25 | Loss: 0.00129151
Iteration 19/25 | Loss: 0.00129038
Iteration 20/25 | Loss: 0.00129025
Iteration 21/25 | Loss: 0.00129024
Iteration 22/25 | Loss: 0.00129024
Iteration 23/25 | Loss: 0.00129024
Iteration 24/25 | Loss: 0.00129024
Iteration 25/25 | Loss: 0.00129024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57942522
Iteration 2/25 | Loss: 0.00261043
Iteration 3/25 | Loss: 0.00261013
Iteration 4/25 | Loss: 0.00261013
Iteration 5/25 | Loss: 0.00261012
Iteration 6/25 | Loss: 0.00261012
Iteration 7/25 | Loss: 0.00261012
Iteration 8/25 | Loss: 0.00261012
Iteration 9/25 | Loss: 0.00261012
Iteration 10/25 | Loss: 0.00261012
Iteration 11/25 | Loss: 0.00261012
Iteration 12/25 | Loss: 0.00261012
Iteration 13/25 | Loss: 0.00261012
Iteration 14/25 | Loss: 0.00261012
Iteration 15/25 | Loss: 0.00261012
Iteration 16/25 | Loss: 0.00261012
Iteration 17/25 | Loss: 0.00261012
Iteration 18/25 | Loss: 0.00261012
Iteration 19/25 | Loss: 0.00261012
Iteration 20/25 | Loss: 0.00261012
Iteration 21/25 | Loss: 0.00261012
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0026101202238351107, 0.0026101202238351107, 0.0026101202238351107, 0.0026101202238351107, 0.0026101202238351107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026101202238351107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261012
Iteration 2/1000 | Loss: 0.00354647
Iteration 3/1000 | Loss: 0.00009666
Iteration 4/1000 | Loss: 0.00006786
Iteration 5/1000 | Loss: 0.00005320
Iteration 6/1000 | Loss: 0.00004564
Iteration 7/1000 | Loss: 0.00004086
Iteration 8/1000 | Loss: 0.00003845
Iteration 9/1000 | Loss: 0.00003692
Iteration 10/1000 | Loss: 0.00003532
Iteration 11/1000 | Loss: 0.00003394
Iteration 12/1000 | Loss: 0.00003296
Iteration 13/1000 | Loss: 0.00003227
Iteration 14/1000 | Loss: 0.00003167
Iteration 15/1000 | Loss: 0.00003123
Iteration 16/1000 | Loss: 0.00003084
Iteration 17/1000 | Loss: 0.00003058
Iteration 18/1000 | Loss: 0.00003038
Iteration 19/1000 | Loss: 0.00003030
Iteration 20/1000 | Loss: 0.00003020
Iteration 21/1000 | Loss: 0.00003017
Iteration 22/1000 | Loss: 0.00003015
Iteration 23/1000 | Loss: 0.00003015
Iteration 24/1000 | Loss: 0.00003014
Iteration 25/1000 | Loss: 0.00003014
Iteration 26/1000 | Loss: 0.00003013
Iteration 27/1000 | Loss: 0.00003012
Iteration 28/1000 | Loss: 0.00003012
Iteration 29/1000 | Loss: 0.00003012
Iteration 30/1000 | Loss: 0.00003011
Iteration 31/1000 | Loss: 0.00003011
Iteration 32/1000 | Loss: 0.00003010
Iteration 33/1000 | Loss: 0.00003010
Iteration 34/1000 | Loss: 0.00003008
Iteration 35/1000 | Loss: 0.00003007
Iteration 36/1000 | Loss: 0.00003007
Iteration 37/1000 | Loss: 0.00003005
Iteration 38/1000 | Loss: 0.00003002
Iteration 39/1000 | Loss: 0.00002999
Iteration 40/1000 | Loss: 0.00002999
Iteration 41/1000 | Loss: 0.00002999
Iteration 42/1000 | Loss: 0.00002998
Iteration 43/1000 | Loss: 0.00002998
Iteration 44/1000 | Loss: 0.00002998
Iteration 45/1000 | Loss: 0.00002998
Iteration 46/1000 | Loss: 0.00002998
Iteration 47/1000 | Loss: 0.00002997
Iteration 48/1000 | Loss: 0.00002997
Iteration 49/1000 | Loss: 0.00002997
Iteration 50/1000 | Loss: 0.00002997
Iteration 51/1000 | Loss: 0.00002997
Iteration 52/1000 | Loss: 0.00002996
Iteration 53/1000 | Loss: 0.00002996
Iteration 54/1000 | Loss: 0.00002995
Iteration 55/1000 | Loss: 0.00002994
Iteration 56/1000 | Loss: 0.00002994
Iteration 57/1000 | Loss: 0.00002994
Iteration 58/1000 | Loss: 0.00002993
Iteration 59/1000 | Loss: 0.00002993
Iteration 60/1000 | Loss: 0.00002993
Iteration 61/1000 | Loss: 0.00002992
Iteration 62/1000 | Loss: 0.00002992
Iteration 63/1000 | Loss: 0.00002991
Iteration 64/1000 | Loss: 0.00002991
Iteration 65/1000 | Loss: 0.00002991
Iteration 66/1000 | Loss: 0.00002990
Iteration 67/1000 | Loss: 0.00002990
Iteration 68/1000 | Loss: 0.00002989
Iteration 69/1000 | Loss: 0.00002989
Iteration 70/1000 | Loss: 0.00002989
Iteration 71/1000 | Loss: 0.00002989
Iteration 72/1000 | Loss: 0.00002988
Iteration 73/1000 | Loss: 0.00002988
Iteration 74/1000 | Loss: 0.00002988
Iteration 75/1000 | Loss: 0.00002988
Iteration 76/1000 | Loss: 0.00002988
Iteration 77/1000 | Loss: 0.00002988
Iteration 78/1000 | Loss: 0.00002988
Iteration 79/1000 | Loss: 0.00002988
Iteration 80/1000 | Loss: 0.00002988
Iteration 81/1000 | Loss: 0.00002988
Iteration 82/1000 | Loss: 0.00002987
Iteration 83/1000 | Loss: 0.00002987
Iteration 84/1000 | Loss: 0.00002987
Iteration 85/1000 | Loss: 0.00002987
Iteration 86/1000 | Loss: 0.00002987
Iteration 87/1000 | Loss: 0.00002987
Iteration 88/1000 | Loss: 0.00002987
Iteration 89/1000 | Loss: 0.00002987
Iteration 90/1000 | Loss: 0.00002987
Iteration 91/1000 | Loss: 0.00002987
Iteration 92/1000 | Loss: 0.00002986
Iteration 93/1000 | Loss: 0.00002986
Iteration 94/1000 | Loss: 0.00002986
Iteration 95/1000 | Loss: 0.00002986
Iteration 96/1000 | Loss: 0.00002986
Iteration 97/1000 | Loss: 0.00002986
Iteration 98/1000 | Loss: 0.00002986
Iteration 99/1000 | Loss: 0.00002986
Iteration 100/1000 | Loss: 0.00002986
Iteration 101/1000 | Loss: 0.00002986
Iteration 102/1000 | Loss: 0.00002986
Iteration 103/1000 | Loss: 0.00002986
Iteration 104/1000 | Loss: 0.00002986
Iteration 105/1000 | Loss: 0.00002986
Iteration 106/1000 | Loss: 0.00002986
Iteration 107/1000 | Loss: 0.00002986
Iteration 108/1000 | Loss: 0.00002986
Iteration 109/1000 | Loss: 0.00002986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.986051549669355e-05, 2.986051549669355e-05, 2.986051549669355e-05, 2.986051549669355e-05, 2.986051549669355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.986051549669355e-05

Optimization complete. Final v2v error: 4.272536754608154 mm

Highest mean error: 6.677178382873535 mm for frame 163

Lowest mean error: 2.7071971893310547 mm for frame 203

Saving results

Total time: 79.31451416015625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863565
Iteration 2/25 | Loss: 0.00125410
Iteration 3/25 | Loss: 0.00116060
Iteration 4/25 | Loss: 0.00113957
Iteration 5/25 | Loss: 0.00113074
Iteration 6/25 | Loss: 0.00112818
Iteration 7/25 | Loss: 0.00112818
Iteration 8/25 | Loss: 0.00112818
Iteration 9/25 | Loss: 0.00112818
Iteration 10/25 | Loss: 0.00112818
Iteration 11/25 | Loss: 0.00112818
Iteration 12/25 | Loss: 0.00112818
Iteration 13/25 | Loss: 0.00112818
Iteration 14/25 | Loss: 0.00112818
Iteration 15/25 | Loss: 0.00112818
Iteration 16/25 | Loss: 0.00112818
Iteration 17/25 | Loss: 0.00112818
Iteration 18/25 | Loss: 0.00112818
Iteration 19/25 | Loss: 0.00112818
Iteration 20/25 | Loss: 0.00112818
Iteration 21/25 | Loss: 0.00112818
Iteration 22/25 | Loss: 0.00112818
Iteration 23/25 | Loss: 0.00112818
Iteration 24/25 | Loss: 0.00112818
Iteration 25/25 | Loss: 0.00112818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19912148
Iteration 2/25 | Loss: 0.00238078
Iteration 3/25 | Loss: 0.00238078
Iteration 4/25 | Loss: 0.00238078
Iteration 5/25 | Loss: 0.00238078
Iteration 6/25 | Loss: 0.00238078
Iteration 7/25 | Loss: 0.00238078
Iteration 8/25 | Loss: 0.00238078
Iteration 9/25 | Loss: 0.00238078
Iteration 10/25 | Loss: 0.00238078
Iteration 11/25 | Loss: 0.00238078
Iteration 12/25 | Loss: 0.00238078
Iteration 13/25 | Loss: 0.00238078
Iteration 14/25 | Loss: 0.00238078
Iteration 15/25 | Loss: 0.00238078
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002380779944360256, 0.002380779944360256, 0.002380779944360256, 0.002380779944360256, 0.002380779944360256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002380779944360256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238078
Iteration 2/1000 | Loss: 0.00003070
Iteration 3/1000 | Loss: 0.00002161
Iteration 4/1000 | Loss: 0.00001994
Iteration 5/1000 | Loss: 0.00001901
Iteration 6/1000 | Loss: 0.00001847
Iteration 7/1000 | Loss: 0.00001791
Iteration 8/1000 | Loss: 0.00001768
Iteration 9/1000 | Loss: 0.00001742
Iteration 10/1000 | Loss: 0.00001739
Iteration 11/1000 | Loss: 0.00001739
Iteration 12/1000 | Loss: 0.00001738
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001738
Iteration 15/1000 | Loss: 0.00001737
Iteration 16/1000 | Loss: 0.00001736
Iteration 17/1000 | Loss: 0.00001732
Iteration 18/1000 | Loss: 0.00001732
Iteration 19/1000 | Loss: 0.00001731
Iteration 20/1000 | Loss: 0.00001730
Iteration 21/1000 | Loss: 0.00001730
Iteration 22/1000 | Loss: 0.00001730
Iteration 23/1000 | Loss: 0.00001730
Iteration 24/1000 | Loss: 0.00001729
Iteration 25/1000 | Loss: 0.00001729
Iteration 26/1000 | Loss: 0.00001728
Iteration 27/1000 | Loss: 0.00001728
Iteration 28/1000 | Loss: 0.00001728
Iteration 29/1000 | Loss: 0.00001727
Iteration 30/1000 | Loss: 0.00001727
Iteration 31/1000 | Loss: 0.00001727
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001727
Iteration 35/1000 | Loss: 0.00001727
Iteration 36/1000 | Loss: 0.00001727
Iteration 37/1000 | Loss: 0.00001727
Iteration 38/1000 | Loss: 0.00001727
Iteration 39/1000 | Loss: 0.00001726
Iteration 40/1000 | Loss: 0.00001725
Iteration 41/1000 | Loss: 0.00001725
Iteration 42/1000 | Loss: 0.00001722
Iteration 43/1000 | Loss: 0.00001721
Iteration 44/1000 | Loss: 0.00001721
Iteration 45/1000 | Loss: 0.00001719
Iteration 46/1000 | Loss: 0.00001719
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001717
Iteration 49/1000 | Loss: 0.00001717
Iteration 50/1000 | Loss: 0.00001716
Iteration 51/1000 | Loss: 0.00001716
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001715
Iteration 54/1000 | Loss: 0.00001715
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001713
Iteration 58/1000 | Loss: 0.00001713
Iteration 59/1000 | Loss: 0.00001712
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001712
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001711
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00001711
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001710
Iteration 79/1000 | Loss: 0.00001710
Iteration 80/1000 | Loss: 0.00001710
Iteration 81/1000 | Loss: 0.00001710
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001709
Iteration 88/1000 | Loss: 0.00001709
Iteration 89/1000 | Loss: 0.00001709
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001709
Iteration 95/1000 | Loss: 0.00001709
Iteration 96/1000 | Loss: 0.00001709
Iteration 97/1000 | Loss: 0.00001709
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001709
Iteration 101/1000 | Loss: 0.00001709
Iteration 102/1000 | Loss: 0.00001708
Iteration 103/1000 | Loss: 0.00001708
Iteration 104/1000 | Loss: 0.00001708
Iteration 105/1000 | Loss: 0.00001708
Iteration 106/1000 | Loss: 0.00001708
Iteration 107/1000 | Loss: 0.00001708
Iteration 108/1000 | Loss: 0.00001708
Iteration 109/1000 | Loss: 0.00001708
Iteration 110/1000 | Loss: 0.00001708
Iteration 111/1000 | Loss: 0.00001708
Iteration 112/1000 | Loss: 0.00001708
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001708
Iteration 116/1000 | Loss: 0.00001707
Iteration 117/1000 | Loss: 0.00001707
Iteration 118/1000 | Loss: 0.00001707
Iteration 119/1000 | Loss: 0.00001707
Iteration 120/1000 | Loss: 0.00001706
Iteration 121/1000 | Loss: 0.00001706
Iteration 122/1000 | Loss: 0.00001706
Iteration 123/1000 | Loss: 0.00001706
Iteration 124/1000 | Loss: 0.00001706
Iteration 125/1000 | Loss: 0.00001706
Iteration 126/1000 | Loss: 0.00001706
Iteration 127/1000 | Loss: 0.00001706
Iteration 128/1000 | Loss: 0.00001706
Iteration 129/1000 | Loss: 0.00001706
Iteration 130/1000 | Loss: 0.00001706
Iteration 131/1000 | Loss: 0.00001706
Iteration 132/1000 | Loss: 0.00001706
Iteration 133/1000 | Loss: 0.00001706
Iteration 134/1000 | Loss: 0.00001706
Iteration 135/1000 | Loss: 0.00001705
Iteration 136/1000 | Loss: 0.00001705
Iteration 137/1000 | Loss: 0.00001705
Iteration 138/1000 | Loss: 0.00001705
Iteration 139/1000 | Loss: 0.00001705
Iteration 140/1000 | Loss: 0.00001705
Iteration 141/1000 | Loss: 0.00001705
Iteration 142/1000 | Loss: 0.00001705
Iteration 143/1000 | Loss: 0.00001705
Iteration 144/1000 | Loss: 0.00001705
Iteration 145/1000 | Loss: 0.00001705
Iteration 146/1000 | Loss: 0.00001705
Iteration 147/1000 | Loss: 0.00001705
Iteration 148/1000 | Loss: 0.00001705
Iteration 149/1000 | Loss: 0.00001705
Iteration 150/1000 | Loss: 0.00001705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.7054910131264478e-05, 1.7054910131264478e-05, 1.7054910131264478e-05, 1.7054910131264478e-05, 1.7054910131264478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7054910131264478e-05

Optimization complete. Final v2v error: 3.466733694076538 mm

Highest mean error: 3.772857427597046 mm for frame 63

Lowest mean error: 2.9153332710266113 mm for frame 1

Saving results

Total time: 35.90359663963318
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791610
Iteration 2/25 | Loss: 0.00143867
Iteration 3/25 | Loss: 0.00127669
Iteration 4/25 | Loss: 0.00114182
Iteration 5/25 | Loss: 0.00111666
Iteration 6/25 | Loss: 0.00113418
Iteration 7/25 | Loss: 0.00110089
Iteration 8/25 | Loss: 0.00108605
Iteration 9/25 | Loss: 0.00107422
Iteration 10/25 | Loss: 0.00107987
Iteration 11/25 | Loss: 0.00107040
Iteration 12/25 | Loss: 0.00107566
Iteration 13/25 | Loss: 0.00107043
Iteration 14/25 | Loss: 0.00106971
Iteration 15/25 | Loss: 0.00106969
Iteration 16/25 | Loss: 0.00106968
Iteration 17/25 | Loss: 0.00106968
Iteration 18/25 | Loss: 0.00106966
Iteration 19/25 | Loss: 0.00106966
Iteration 20/25 | Loss: 0.00106966
Iteration 21/25 | Loss: 0.00106966
Iteration 22/25 | Loss: 0.00106966
Iteration 23/25 | Loss: 0.00106966
Iteration 24/25 | Loss: 0.00106966
Iteration 25/25 | Loss: 0.00106966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.64195061
Iteration 2/25 | Loss: 0.00214312
Iteration 3/25 | Loss: 0.00214310
Iteration 4/25 | Loss: 0.00214310
Iteration 5/25 | Loss: 0.00214310
Iteration 6/25 | Loss: 0.00214310
Iteration 7/25 | Loss: 0.00214310
Iteration 8/25 | Loss: 0.00214310
Iteration 9/25 | Loss: 0.00214310
Iteration 10/25 | Loss: 0.00214310
Iteration 11/25 | Loss: 0.00214310
Iteration 12/25 | Loss: 0.00214310
Iteration 13/25 | Loss: 0.00214310
Iteration 14/25 | Loss: 0.00214310
Iteration 15/25 | Loss: 0.00214310
Iteration 16/25 | Loss: 0.00214310
Iteration 17/25 | Loss: 0.00214310
Iteration 18/25 | Loss: 0.00214310
Iteration 19/25 | Loss: 0.00214310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0021430980414152145, 0.0021430980414152145, 0.0021430980414152145, 0.0021430980414152145, 0.0021430980414152145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021430980414152145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214310
Iteration 2/1000 | Loss: 0.00010905
Iteration 3/1000 | Loss: 0.00001862
Iteration 4/1000 | Loss: 0.00001629
Iteration 5/1000 | Loss: 0.00001525
Iteration 6/1000 | Loss: 0.00001475
Iteration 7/1000 | Loss: 0.00006255
Iteration 8/1000 | Loss: 0.00001450
Iteration 9/1000 | Loss: 0.00001402
Iteration 10/1000 | Loss: 0.00001400
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001364
Iteration 13/1000 | Loss: 0.00001354
Iteration 14/1000 | Loss: 0.00001353
Iteration 15/1000 | Loss: 0.00001349
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001348
Iteration 18/1000 | Loss: 0.00001343
Iteration 19/1000 | Loss: 0.00001342
Iteration 20/1000 | Loss: 0.00001342
Iteration 21/1000 | Loss: 0.00001377
Iteration 22/1000 | Loss: 0.00001349
Iteration 23/1000 | Loss: 0.00001346
Iteration 24/1000 | Loss: 0.00001346
Iteration 25/1000 | Loss: 0.00001339
Iteration 26/1000 | Loss: 0.00001338
Iteration 27/1000 | Loss: 0.00001338
Iteration 28/1000 | Loss: 0.00001337
Iteration 29/1000 | Loss: 0.00001337
Iteration 30/1000 | Loss: 0.00001336
Iteration 31/1000 | Loss: 0.00001335
Iteration 32/1000 | Loss: 0.00001331
Iteration 33/1000 | Loss: 0.00001330
Iteration 34/1000 | Loss: 0.00001329
Iteration 35/1000 | Loss: 0.00001329
Iteration 36/1000 | Loss: 0.00001328
Iteration 37/1000 | Loss: 0.00001326
Iteration 38/1000 | Loss: 0.00001326
Iteration 39/1000 | Loss: 0.00001325
Iteration 40/1000 | Loss: 0.00001325
Iteration 41/1000 | Loss: 0.00001324
Iteration 42/1000 | Loss: 0.00001324
Iteration 43/1000 | Loss: 0.00001324
Iteration 44/1000 | Loss: 0.00001324
Iteration 45/1000 | Loss: 0.00001324
Iteration 46/1000 | Loss: 0.00001324
Iteration 47/1000 | Loss: 0.00001324
Iteration 48/1000 | Loss: 0.00001324
Iteration 49/1000 | Loss: 0.00001324
Iteration 50/1000 | Loss: 0.00001323
Iteration 51/1000 | Loss: 0.00001323
Iteration 52/1000 | Loss: 0.00001323
Iteration 53/1000 | Loss: 0.00001323
Iteration 54/1000 | Loss: 0.00001322
Iteration 55/1000 | Loss: 0.00001322
Iteration 56/1000 | Loss: 0.00001322
Iteration 57/1000 | Loss: 0.00001322
Iteration 58/1000 | Loss: 0.00001322
Iteration 59/1000 | Loss: 0.00001321
Iteration 60/1000 | Loss: 0.00001321
Iteration 61/1000 | Loss: 0.00001321
Iteration 62/1000 | Loss: 0.00001321
Iteration 63/1000 | Loss: 0.00001321
Iteration 64/1000 | Loss: 0.00001321
Iteration 65/1000 | Loss: 0.00001321
Iteration 66/1000 | Loss: 0.00001321
Iteration 67/1000 | Loss: 0.00001321
Iteration 68/1000 | Loss: 0.00001321
Iteration 69/1000 | Loss: 0.00001321
Iteration 70/1000 | Loss: 0.00001321
Iteration 71/1000 | Loss: 0.00001321
Iteration 72/1000 | Loss: 0.00001321
Iteration 73/1000 | Loss: 0.00001321
Iteration 74/1000 | Loss: 0.00001321
Iteration 75/1000 | Loss: 0.00001321
Iteration 76/1000 | Loss: 0.00001321
Iteration 77/1000 | Loss: 0.00001321
Iteration 78/1000 | Loss: 0.00001321
Iteration 79/1000 | Loss: 0.00001321
Iteration 80/1000 | Loss: 0.00001320
Iteration 81/1000 | Loss: 0.00001320
Iteration 82/1000 | Loss: 0.00001320
Iteration 83/1000 | Loss: 0.00001320
Iteration 84/1000 | Loss: 0.00001320
Iteration 85/1000 | Loss: 0.00001320
Iteration 86/1000 | Loss: 0.00001320
Iteration 87/1000 | Loss: 0.00001320
Iteration 88/1000 | Loss: 0.00001320
Iteration 89/1000 | Loss: 0.00001320
Iteration 90/1000 | Loss: 0.00001320
Iteration 91/1000 | Loss: 0.00001320
Iteration 92/1000 | Loss: 0.00001320
Iteration 93/1000 | Loss: 0.00001320
Iteration 94/1000 | Loss: 0.00001320
Iteration 95/1000 | Loss: 0.00001320
Iteration 96/1000 | Loss: 0.00001320
Iteration 97/1000 | Loss: 0.00001320
Iteration 98/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.3201945876062382e-05, 1.3201945876062382e-05, 1.3201945876062382e-05, 1.3201945876062382e-05, 1.3201945876062382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3201945876062382e-05

Optimization complete. Final v2v error: 2.5740199089050293 mm

Highest mean error: 21.90680503845215 mm for frame 38

Lowest mean error: 2.1623165607452393 mm for frame 238

Saving results

Total time: 58.3460259437561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959350
Iteration 2/25 | Loss: 0.00172120
Iteration 3/25 | Loss: 0.00138580
Iteration 4/25 | Loss: 0.00134652
Iteration 5/25 | Loss: 0.00133884
Iteration 6/25 | Loss: 0.00133589
Iteration 7/25 | Loss: 0.00133448
Iteration 8/25 | Loss: 0.00134040
Iteration 9/25 | Loss: 0.00133733
Iteration 10/25 | Loss: 0.00133394
Iteration 11/25 | Loss: 0.00133150
Iteration 12/25 | Loss: 0.00133091
Iteration 13/25 | Loss: 0.00133070
Iteration 14/25 | Loss: 0.00133067
Iteration 15/25 | Loss: 0.00133067
Iteration 16/25 | Loss: 0.00133067
Iteration 17/25 | Loss: 0.00133067
Iteration 18/25 | Loss: 0.00133067
Iteration 19/25 | Loss: 0.00133066
Iteration 20/25 | Loss: 0.00133066
Iteration 21/25 | Loss: 0.00133066
Iteration 22/25 | Loss: 0.00133066
Iteration 23/25 | Loss: 0.00133066
Iteration 24/25 | Loss: 0.00133066
Iteration 25/25 | Loss: 0.00133066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26431823
Iteration 2/25 | Loss: 0.00231304
Iteration 3/25 | Loss: 0.00231277
Iteration 4/25 | Loss: 0.00231276
Iteration 5/25 | Loss: 0.00231276
Iteration 6/25 | Loss: 0.00231276
Iteration 7/25 | Loss: 0.00231276
Iteration 8/25 | Loss: 0.00231276
Iteration 9/25 | Loss: 0.00231276
Iteration 10/25 | Loss: 0.00231276
Iteration 11/25 | Loss: 0.00231276
Iteration 12/25 | Loss: 0.00231276
Iteration 13/25 | Loss: 0.00231276
Iteration 14/25 | Loss: 0.00231276
Iteration 15/25 | Loss: 0.00231276
Iteration 16/25 | Loss: 0.00231276
Iteration 17/25 | Loss: 0.00231276
Iteration 18/25 | Loss: 0.00231276
Iteration 19/25 | Loss: 0.00231276
Iteration 20/25 | Loss: 0.00231276
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0023127614986151457, 0.0023127614986151457, 0.0023127614986151457, 0.0023127614986151457, 0.0023127614986151457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023127614986151457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231276
Iteration 2/1000 | Loss: 0.00009769
Iteration 3/1000 | Loss: 0.00005737
Iteration 4/1000 | Loss: 0.00005027
Iteration 5/1000 | Loss: 0.00004696
Iteration 6/1000 | Loss: 0.00004524
Iteration 7/1000 | Loss: 0.00004415
Iteration 8/1000 | Loss: 0.00004321
Iteration 9/1000 | Loss: 0.00004250
Iteration 10/1000 | Loss: 0.00004195
Iteration 11/1000 | Loss: 0.00004147
Iteration 12/1000 | Loss: 0.00004108
Iteration 13/1000 | Loss: 0.00004081
Iteration 14/1000 | Loss: 0.00004058
Iteration 15/1000 | Loss: 0.00004036
Iteration 16/1000 | Loss: 0.00004017
Iteration 17/1000 | Loss: 0.00004009
Iteration 18/1000 | Loss: 0.00003994
Iteration 19/1000 | Loss: 0.00003993
Iteration 20/1000 | Loss: 0.00003979
Iteration 21/1000 | Loss: 0.00003976
Iteration 22/1000 | Loss: 0.00003972
Iteration 23/1000 | Loss: 0.00003965
Iteration 24/1000 | Loss: 0.00003959
Iteration 25/1000 | Loss: 0.00003959
Iteration 26/1000 | Loss: 0.00003957
Iteration 27/1000 | Loss: 0.00003957
Iteration 28/1000 | Loss: 0.00003953
Iteration 29/1000 | Loss: 0.00003952
Iteration 30/1000 | Loss: 0.00003952
Iteration 31/1000 | Loss: 0.00003947
Iteration 32/1000 | Loss: 0.00003946
Iteration 33/1000 | Loss: 0.00003945
Iteration 34/1000 | Loss: 0.00003944
Iteration 35/1000 | Loss: 0.00003944
Iteration 36/1000 | Loss: 0.00003944
Iteration 37/1000 | Loss: 0.00003944
Iteration 38/1000 | Loss: 0.00003943
Iteration 39/1000 | Loss: 0.00003943
Iteration 40/1000 | Loss: 0.00003943
Iteration 41/1000 | Loss: 0.00003943
Iteration 42/1000 | Loss: 0.00003943
Iteration 43/1000 | Loss: 0.00003943
Iteration 44/1000 | Loss: 0.00003943
Iteration 45/1000 | Loss: 0.00003943
Iteration 46/1000 | Loss: 0.00003943
Iteration 47/1000 | Loss: 0.00003943
Iteration 48/1000 | Loss: 0.00003941
Iteration 49/1000 | Loss: 0.00003941
Iteration 50/1000 | Loss: 0.00003941
Iteration 51/1000 | Loss: 0.00003941
Iteration 52/1000 | Loss: 0.00003941
Iteration 53/1000 | Loss: 0.00003941
Iteration 54/1000 | Loss: 0.00003941
Iteration 55/1000 | Loss: 0.00003941
Iteration 56/1000 | Loss: 0.00003941
Iteration 57/1000 | Loss: 0.00003941
Iteration 58/1000 | Loss: 0.00003941
Iteration 59/1000 | Loss: 0.00003941
Iteration 60/1000 | Loss: 0.00003940
Iteration 61/1000 | Loss: 0.00003940
Iteration 62/1000 | Loss: 0.00003940
Iteration 63/1000 | Loss: 0.00003940
Iteration 64/1000 | Loss: 0.00003939
Iteration 65/1000 | Loss: 0.00003939
Iteration 66/1000 | Loss: 0.00003939
Iteration 67/1000 | Loss: 0.00003939
Iteration 68/1000 | Loss: 0.00003938
Iteration 69/1000 | Loss: 0.00003938
Iteration 70/1000 | Loss: 0.00003938
Iteration 71/1000 | Loss: 0.00003937
Iteration 72/1000 | Loss: 0.00003937
Iteration 73/1000 | Loss: 0.00003937
Iteration 74/1000 | Loss: 0.00003937
Iteration 75/1000 | Loss: 0.00003936
Iteration 76/1000 | Loss: 0.00003936
Iteration 77/1000 | Loss: 0.00003936
Iteration 78/1000 | Loss: 0.00003935
Iteration 79/1000 | Loss: 0.00003935
Iteration 80/1000 | Loss: 0.00003935
Iteration 81/1000 | Loss: 0.00003934
Iteration 82/1000 | Loss: 0.00003934
Iteration 83/1000 | Loss: 0.00003934
Iteration 84/1000 | Loss: 0.00003934
Iteration 85/1000 | Loss: 0.00003933
Iteration 86/1000 | Loss: 0.00003933
Iteration 87/1000 | Loss: 0.00003933
Iteration 88/1000 | Loss: 0.00003933
Iteration 89/1000 | Loss: 0.00003932
Iteration 90/1000 | Loss: 0.00003932
Iteration 91/1000 | Loss: 0.00003932
Iteration 92/1000 | Loss: 0.00003932
Iteration 93/1000 | Loss: 0.00003932
Iteration 94/1000 | Loss: 0.00003932
Iteration 95/1000 | Loss: 0.00003931
Iteration 96/1000 | Loss: 0.00003931
Iteration 97/1000 | Loss: 0.00003931
Iteration 98/1000 | Loss: 0.00003931
Iteration 99/1000 | Loss: 0.00003931
Iteration 100/1000 | Loss: 0.00003930
Iteration 101/1000 | Loss: 0.00003930
Iteration 102/1000 | Loss: 0.00003930
Iteration 103/1000 | Loss: 0.00003930
Iteration 104/1000 | Loss: 0.00003930
Iteration 105/1000 | Loss: 0.00003930
Iteration 106/1000 | Loss: 0.00003930
Iteration 107/1000 | Loss: 0.00003930
Iteration 108/1000 | Loss: 0.00003929
Iteration 109/1000 | Loss: 0.00003929
Iteration 110/1000 | Loss: 0.00003929
Iteration 111/1000 | Loss: 0.00003929
Iteration 112/1000 | Loss: 0.00003929
Iteration 113/1000 | Loss: 0.00003928
Iteration 114/1000 | Loss: 0.00003928
Iteration 115/1000 | Loss: 0.00003928
Iteration 116/1000 | Loss: 0.00003928
Iteration 117/1000 | Loss: 0.00003928
Iteration 118/1000 | Loss: 0.00003928
Iteration 119/1000 | Loss: 0.00003928
Iteration 120/1000 | Loss: 0.00003928
Iteration 121/1000 | Loss: 0.00003928
Iteration 122/1000 | Loss: 0.00003927
Iteration 123/1000 | Loss: 0.00003927
Iteration 124/1000 | Loss: 0.00003927
Iteration 125/1000 | Loss: 0.00003927
Iteration 126/1000 | Loss: 0.00003927
Iteration 127/1000 | Loss: 0.00003927
Iteration 128/1000 | Loss: 0.00003927
Iteration 129/1000 | Loss: 0.00003927
Iteration 130/1000 | Loss: 0.00003927
Iteration 131/1000 | Loss: 0.00003927
Iteration 132/1000 | Loss: 0.00003927
Iteration 133/1000 | Loss: 0.00003927
Iteration 134/1000 | Loss: 0.00003926
Iteration 135/1000 | Loss: 0.00003926
Iteration 136/1000 | Loss: 0.00003926
Iteration 137/1000 | Loss: 0.00003926
Iteration 138/1000 | Loss: 0.00003926
Iteration 139/1000 | Loss: 0.00003926
Iteration 140/1000 | Loss: 0.00003926
Iteration 141/1000 | Loss: 0.00003926
Iteration 142/1000 | Loss: 0.00003926
Iteration 143/1000 | Loss: 0.00003926
Iteration 144/1000 | Loss: 0.00003926
Iteration 145/1000 | Loss: 0.00003926
Iteration 146/1000 | Loss: 0.00003926
Iteration 147/1000 | Loss: 0.00003925
Iteration 148/1000 | Loss: 0.00003925
Iteration 149/1000 | Loss: 0.00003925
Iteration 150/1000 | Loss: 0.00003925
Iteration 151/1000 | Loss: 0.00003925
Iteration 152/1000 | Loss: 0.00003925
Iteration 153/1000 | Loss: 0.00003925
Iteration 154/1000 | Loss: 0.00003925
Iteration 155/1000 | Loss: 0.00003925
Iteration 156/1000 | Loss: 0.00003925
Iteration 157/1000 | Loss: 0.00003925
Iteration 158/1000 | Loss: 0.00003925
Iteration 159/1000 | Loss: 0.00003925
Iteration 160/1000 | Loss: 0.00003925
Iteration 161/1000 | Loss: 0.00003925
Iteration 162/1000 | Loss: 0.00003925
Iteration 163/1000 | Loss: 0.00003925
Iteration 164/1000 | Loss: 0.00003925
Iteration 165/1000 | Loss: 0.00003925
Iteration 166/1000 | Loss: 0.00003925
Iteration 167/1000 | Loss: 0.00003925
Iteration 168/1000 | Loss: 0.00003925
Iteration 169/1000 | Loss: 0.00003925
Iteration 170/1000 | Loss: 0.00003925
Iteration 171/1000 | Loss: 0.00003925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [3.925450801034458e-05, 3.925450801034458e-05, 3.925450801034458e-05, 3.925450801034458e-05, 3.925450801034458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.925450801034458e-05

Optimization complete. Final v2v error: 4.863254070281982 mm

Highest mean error: 5.959545612335205 mm for frame 66

Lowest mean error: 3.624648332595825 mm for frame 234

Saving results

Total time: 70.77309608459473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401623
Iteration 2/25 | Loss: 0.00127388
Iteration 3/25 | Loss: 0.00113214
Iteration 4/25 | Loss: 0.00111383
Iteration 5/25 | Loss: 0.00110892
Iteration 6/25 | Loss: 0.00110715
Iteration 7/25 | Loss: 0.00110681
Iteration 8/25 | Loss: 0.00110681
Iteration 9/25 | Loss: 0.00110681
Iteration 10/25 | Loss: 0.00110681
Iteration 11/25 | Loss: 0.00110681
Iteration 12/25 | Loss: 0.00110681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011068062158301473, 0.0011068062158301473, 0.0011068062158301473, 0.0011068062158301473, 0.0011068062158301473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011068062158301473

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25035882
Iteration 2/25 | Loss: 0.00239285
Iteration 3/25 | Loss: 0.00239283
Iteration 4/25 | Loss: 0.00239283
Iteration 5/25 | Loss: 0.00239283
Iteration 6/25 | Loss: 0.00239283
Iteration 7/25 | Loss: 0.00239283
Iteration 8/25 | Loss: 0.00239283
Iteration 9/25 | Loss: 0.00239283
Iteration 10/25 | Loss: 0.00239283
Iteration 11/25 | Loss: 0.00239283
Iteration 12/25 | Loss: 0.00239283
Iteration 13/25 | Loss: 0.00239283
Iteration 14/25 | Loss: 0.00239283
Iteration 15/25 | Loss: 0.00239283
Iteration 16/25 | Loss: 0.00239283
Iteration 17/25 | Loss: 0.00239283
Iteration 18/25 | Loss: 0.00239283
Iteration 19/25 | Loss: 0.00239283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002392828231677413, 0.002392828231677413, 0.002392828231677413, 0.002392828231677413, 0.002392828231677413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002392828231677413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239283
Iteration 2/1000 | Loss: 0.00006173
Iteration 3/1000 | Loss: 0.00003245
Iteration 4/1000 | Loss: 0.00002233
Iteration 5/1000 | Loss: 0.00001905
Iteration 6/1000 | Loss: 0.00001783
Iteration 7/1000 | Loss: 0.00001646
Iteration 8/1000 | Loss: 0.00001572
Iteration 9/1000 | Loss: 0.00001524
Iteration 10/1000 | Loss: 0.00001481
Iteration 11/1000 | Loss: 0.00001456
Iteration 12/1000 | Loss: 0.00001432
Iteration 13/1000 | Loss: 0.00001430
Iteration 14/1000 | Loss: 0.00001413
Iteration 15/1000 | Loss: 0.00001406
Iteration 16/1000 | Loss: 0.00001405
Iteration 17/1000 | Loss: 0.00001404
Iteration 18/1000 | Loss: 0.00001404
Iteration 19/1000 | Loss: 0.00001404
Iteration 20/1000 | Loss: 0.00001403
Iteration 21/1000 | Loss: 0.00001403
Iteration 22/1000 | Loss: 0.00001403
Iteration 23/1000 | Loss: 0.00001395
Iteration 24/1000 | Loss: 0.00001394
Iteration 25/1000 | Loss: 0.00001394
Iteration 26/1000 | Loss: 0.00001394
Iteration 27/1000 | Loss: 0.00001393
Iteration 28/1000 | Loss: 0.00001392
Iteration 29/1000 | Loss: 0.00001388
Iteration 30/1000 | Loss: 0.00001387
Iteration 31/1000 | Loss: 0.00001386
Iteration 32/1000 | Loss: 0.00001385
Iteration 33/1000 | Loss: 0.00001383
Iteration 34/1000 | Loss: 0.00001383
Iteration 35/1000 | Loss: 0.00001379
Iteration 36/1000 | Loss: 0.00001379
Iteration 37/1000 | Loss: 0.00001377
Iteration 38/1000 | Loss: 0.00001373
Iteration 39/1000 | Loss: 0.00001372
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001372
Iteration 42/1000 | Loss: 0.00001372
Iteration 43/1000 | Loss: 0.00001372
Iteration 44/1000 | Loss: 0.00001371
Iteration 45/1000 | Loss: 0.00001371
Iteration 46/1000 | Loss: 0.00001371
Iteration 47/1000 | Loss: 0.00001370
Iteration 48/1000 | Loss: 0.00001370
Iteration 49/1000 | Loss: 0.00001370
Iteration 50/1000 | Loss: 0.00001370
Iteration 51/1000 | Loss: 0.00001370
Iteration 52/1000 | Loss: 0.00001370
Iteration 53/1000 | Loss: 0.00001370
Iteration 54/1000 | Loss: 0.00001369
Iteration 55/1000 | Loss: 0.00001369
Iteration 56/1000 | Loss: 0.00001369
Iteration 57/1000 | Loss: 0.00001369
Iteration 58/1000 | Loss: 0.00001369
Iteration 59/1000 | Loss: 0.00001369
Iteration 60/1000 | Loss: 0.00001369
Iteration 61/1000 | Loss: 0.00001369
Iteration 62/1000 | Loss: 0.00001368
Iteration 63/1000 | Loss: 0.00001368
Iteration 64/1000 | Loss: 0.00001368
Iteration 65/1000 | Loss: 0.00001367
Iteration 66/1000 | Loss: 0.00001367
Iteration 67/1000 | Loss: 0.00001367
Iteration 68/1000 | Loss: 0.00001366
Iteration 69/1000 | Loss: 0.00001365
Iteration 70/1000 | Loss: 0.00001365
Iteration 71/1000 | Loss: 0.00001365
Iteration 72/1000 | Loss: 0.00001365
Iteration 73/1000 | Loss: 0.00001364
Iteration 74/1000 | Loss: 0.00001364
Iteration 75/1000 | Loss: 0.00001364
Iteration 76/1000 | Loss: 0.00001364
Iteration 77/1000 | Loss: 0.00001364
Iteration 78/1000 | Loss: 0.00001364
Iteration 79/1000 | Loss: 0.00001364
Iteration 80/1000 | Loss: 0.00001364
Iteration 81/1000 | Loss: 0.00001364
Iteration 82/1000 | Loss: 0.00001364
Iteration 83/1000 | Loss: 0.00001363
Iteration 84/1000 | Loss: 0.00001363
Iteration 85/1000 | Loss: 0.00001363
Iteration 86/1000 | Loss: 0.00001363
Iteration 87/1000 | Loss: 0.00001363
Iteration 88/1000 | Loss: 0.00001363
Iteration 89/1000 | Loss: 0.00001363
Iteration 90/1000 | Loss: 0.00001363
Iteration 91/1000 | Loss: 0.00001363
Iteration 92/1000 | Loss: 0.00001363
Iteration 93/1000 | Loss: 0.00001363
Iteration 94/1000 | Loss: 0.00001363
Iteration 95/1000 | Loss: 0.00001362
Iteration 96/1000 | Loss: 0.00001362
Iteration 97/1000 | Loss: 0.00001362
Iteration 98/1000 | Loss: 0.00001362
Iteration 99/1000 | Loss: 0.00001361
Iteration 100/1000 | Loss: 0.00001361
Iteration 101/1000 | Loss: 0.00001361
Iteration 102/1000 | Loss: 0.00001361
Iteration 103/1000 | Loss: 0.00001361
Iteration 104/1000 | Loss: 0.00001361
Iteration 105/1000 | Loss: 0.00001361
Iteration 106/1000 | Loss: 0.00001361
Iteration 107/1000 | Loss: 0.00001360
Iteration 108/1000 | Loss: 0.00001360
Iteration 109/1000 | Loss: 0.00001360
Iteration 110/1000 | Loss: 0.00001360
Iteration 111/1000 | Loss: 0.00001360
Iteration 112/1000 | Loss: 0.00001360
Iteration 113/1000 | Loss: 0.00001360
Iteration 114/1000 | Loss: 0.00001360
Iteration 115/1000 | Loss: 0.00001360
Iteration 116/1000 | Loss: 0.00001360
Iteration 117/1000 | Loss: 0.00001359
Iteration 118/1000 | Loss: 0.00001359
Iteration 119/1000 | Loss: 0.00001359
Iteration 120/1000 | Loss: 0.00001359
Iteration 121/1000 | Loss: 0.00001359
Iteration 122/1000 | Loss: 0.00001359
Iteration 123/1000 | Loss: 0.00001359
Iteration 124/1000 | Loss: 0.00001359
Iteration 125/1000 | Loss: 0.00001358
Iteration 126/1000 | Loss: 0.00001358
Iteration 127/1000 | Loss: 0.00001358
Iteration 128/1000 | Loss: 0.00001358
Iteration 129/1000 | Loss: 0.00001358
Iteration 130/1000 | Loss: 0.00001358
Iteration 131/1000 | Loss: 0.00001358
Iteration 132/1000 | Loss: 0.00001358
Iteration 133/1000 | Loss: 0.00001357
Iteration 134/1000 | Loss: 0.00001357
Iteration 135/1000 | Loss: 0.00001357
Iteration 136/1000 | Loss: 0.00001357
Iteration 137/1000 | Loss: 0.00001357
Iteration 138/1000 | Loss: 0.00001356
Iteration 139/1000 | Loss: 0.00001356
Iteration 140/1000 | Loss: 0.00001356
Iteration 141/1000 | Loss: 0.00001356
Iteration 142/1000 | Loss: 0.00001356
Iteration 143/1000 | Loss: 0.00001355
Iteration 144/1000 | Loss: 0.00001355
Iteration 145/1000 | Loss: 0.00001355
Iteration 146/1000 | Loss: 0.00001355
Iteration 147/1000 | Loss: 0.00001355
Iteration 148/1000 | Loss: 0.00001355
Iteration 149/1000 | Loss: 0.00001355
Iteration 150/1000 | Loss: 0.00001355
Iteration 151/1000 | Loss: 0.00001355
Iteration 152/1000 | Loss: 0.00001355
Iteration 153/1000 | Loss: 0.00001355
Iteration 154/1000 | Loss: 0.00001355
Iteration 155/1000 | Loss: 0.00001354
Iteration 156/1000 | Loss: 0.00001354
Iteration 157/1000 | Loss: 0.00001354
Iteration 158/1000 | Loss: 0.00001354
Iteration 159/1000 | Loss: 0.00001354
Iteration 160/1000 | Loss: 0.00001354
Iteration 161/1000 | Loss: 0.00001354
Iteration 162/1000 | Loss: 0.00001354
Iteration 163/1000 | Loss: 0.00001354
Iteration 164/1000 | Loss: 0.00001354
Iteration 165/1000 | Loss: 0.00001354
Iteration 166/1000 | Loss: 0.00001354
Iteration 167/1000 | Loss: 0.00001354
Iteration 168/1000 | Loss: 0.00001354
Iteration 169/1000 | Loss: 0.00001354
Iteration 170/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.3542167835112195e-05, 1.3542167835112195e-05, 1.3542167835112195e-05, 1.3542167835112195e-05, 1.3542167835112195e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3542167835112195e-05

Optimization complete. Final v2v error: 2.904432535171509 mm

Highest mean error: 4.874841213226318 mm for frame 89

Lowest mean error: 2.1153626441955566 mm for frame 128

Saving results

Total time: 42.324930906295776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857500
Iteration 2/25 | Loss: 0.00133576
Iteration 3/25 | Loss: 0.00113153
Iteration 4/25 | Loss: 0.00110839
Iteration 5/25 | Loss: 0.00110430
Iteration 6/25 | Loss: 0.00110423
Iteration 7/25 | Loss: 0.00110423
Iteration 8/25 | Loss: 0.00110423
Iteration 9/25 | Loss: 0.00110423
Iteration 10/25 | Loss: 0.00110423
Iteration 11/25 | Loss: 0.00110423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001104234135709703, 0.001104234135709703, 0.001104234135709703, 0.001104234135709703, 0.001104234135709703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001104234135709703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80570024
Iteration 2/25 | Loss: 0.00107546
Iteration 3/25 | Loss: 0.00107546
Iteration 4/25 | Loss: 0.00107546
Iteration 5/25 | Loss: 0.00107546
Iteration 6/25 | Loss: 0.00107546
Iteration 7/25 | Loss: 0.00107546
Iteration 8/25 | Loss: 0.00107546
Iteration 9/25 | Loss: 0.00107546
Iteration 10/25 | Loss: 0.00107546
Iteration 11/25 | Loss: 0.00107546
Iteration 12/25 | Loss: 0.00107546
Iteration 13/25 | Loss: 0.00107546
Iteration 14/25 | Loss: 0.00107546
Iteration 15/25 | Loss: 0.00107546
Iteration 16/25 | Loss: 0.00107546
Iteration 17/25 | Loss: 0.00107546
Iteration 18/25 | Loss: 0.00107546
Iteration 19/25 | Loss: 0.00107546
Iteration 20/25 | Loss: 0.00107546
Iteration 21/25 | Loss: 0.00107546
Iteration 22/25 | Loss: 0.00107546
Iteration 23/25 | Loss: 0.00107546
Iteration 24/25 | Loss: 0.00107546
Iteration 25/25 | Loss: 0.00107546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107546
Iteration 2/1000 | Loss: 0.00003731
Iteration 3/1000 | Loss: 0.00002704
Iteration 4/1000 | Loss: 0.00002424
Iteration 5/1000 | Loss: 0.00002314
Iteration 6/1000 | Loss: 0.00002250
Iteration 7/1000 | Loss: 0.00002220
Iteration 8/1000 | Loss: 0.00002204
Iteration 9/1000 | Loss: 0.00002204
Iteration 10/1000 | Loss: 0.00002194
Iteration 11/1000 | Loss: 0.00002193
Iteration 12/1000 | Loss: 0.00002192
Iteration 13/1000 | Loss: 0.00002192
Iteration 14/1000 | Loss: 0.00002182
Iteration 15/1000 | Loss: 0.00002180
Iteration 16/1000 | Loss: 0.00002178
Iteration 17/1000 | Loss: 0.00002177
Iteration 18/1000 | Loss: 0.00002176
Iteration 19/1000 | Loss: 0.00002176
Iteration 20/1000 | Loss: 0.00002176
Iteration 21/1000 | Loss: 0.00002176
Iteration 22/1000 | Loss: 0.00002176
Iteration 23/1000 | Loss: 0.00002175
Iteration 24/1000 | Loss: 0.00002172
Iteration 25/1000 | Loss: 0.00002172
Iteration 26/1000 | Loss: 0.00002172
Iteration 27/1000 | Loss: 0.00002172
Iteration 28/1000 | Loss: 0.00002172
Iteration 29/1000 | Loss: 0.00002172
Iteration 30/1000 | Loss: 0.00002172
Iteration 31/1000 | Loss: 0.00002172
Iteration 32/1000 | Loss: 0.00002172
Iteration 33/1000 | Loss: 0.00002172
Iteration 34/1000 | Loss: 0.00002172
Iteration 35/1000 | Loss: 0.00002172
Iteration 36/1000 | Loss: 0.00002172
Iteration 37/1000 | Loss: 0.00002172
Iteration 38/1000 | Loss: 0.00002172
Iteration 39/1000 | Loss: 0.00002172
Iteration 40/1000 | Loss: 0.00002172
Iteration 41/1000 | Loss: 0.00002171
Iteration 42/1000 | Loss: 0.00002171
Iteration 43/1000 | Loss: 0.00002171
Iteration 44/1000 | Loss: 0.00002168
Iteration 45/1000 | Loss: 0.00002168
Iteration 46/1000 | Loss: 0.00002168
Iteration 47/1000 | Loss: 0.00002168
Iteration 48/1000 | Loss: 0.00002167
Iteration 49/1000 | Loss: 0.00002166
Iteration 50/1000 | Loss: 0.00002164
Iteration 51/1000 | Loss: 0.00002164
Iteration 52/1000 | Loss: 0.00002164
Iteration 53/1000 | Loss: 0.00002164
Iteration 54/1000 | Loss: 0.00002164
Iteration 55/1000 | Loss: 0.00002164
Iteration 56/1000 | Loss: 0.00002164
Iteration 57/1000 | Loss: 0.00002164
Iteration 58/1000 | Loss: 0.00002164
Iteration 59/1000 | Loss: 0.00002164
Iteration 60/1000 | Loss: 0.00002164
Iteration 61/1000 | Loss: 0.00002164
Iteration 62/1000 | Loss: 0.00002164
Iteration 63/1000 | Loss: 0.00002164
Iteration 64/1000 | Loss: 0.00002164
Iteration 65/1000 | Loss: 0.00002164
Iteration 66/1000 | Loss: 0.00002164
Iteration 67/1000 | Loss: 0.00002164
Iteration 68/1000 | Loss: 0.00002164
Iteration 69/1000 | Loss: 0.00002164
Iteration 70/1000 | Loss: 0.00002164
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002164
Iteration 73/1000 | Loss: 0.00002164
Iteration 74/1000 | Loss: 0.00002164
Iteration 75/1000 | Loss: 0.00002164
Iteration 76/1000 | Loss: 0.00002164
Iteration 77/1000 | Loss: 0.00002164
Iteration 78/1000 | Loss: 0.00002164
Iteration 79/1000 | Loss: 0.00002164
Iteration 80/1000 | Loss: 0.00002164
Iteration 81/1000 | Loss: 0.00002164
Iteration 82/1000 | Loss: 0.00002164
Iteration 83/1000 | Loss: 0.00002164
Iteration 84/1000 | Loss: 0.00002164
Iteration 85/1000 | Loss: 0.00002164
Iteration 86/1000 | Loss: 0.00002164
Iteration 87/1000 | Loss: 0.00002164
Iteration 88/1000 | Loss: 0.00002164
Iteration 89/1000 | Loss: 0.00002164
Iteration 90/1000 | Loss: 0.00002164
Iteration 91/1000 | Loss: 0.00002164
Iteration 92/1000 | Loss: 0.00002164
Iteration 93/1000 | Loss: 0.00002164
Iteration 94/1000 | Loss: 0.00002164
Iteration 95/1000 | Loss: 0.00002164
Iteration 96/1000 | Loss: 0.00002164
Iteration 97/1000 | Loss: 0.00002164
Iteration 98/1000 | Loss: 0.00002164
Iteration 99/1000 | Loss: 0.00002164
Iteration 100/1000 | Loss: 0.00002164
Iteration 101/1000 | Loss: 0.00002164
Iteration 102/1000 | Loss: 0.00002164
Iteration 103/1000 | Loss: 0.00002164
Iteration 104/1000 | Loss: 0.00002164
Iteration 105/1000 | Loss: 0.00002164
Iteration 106/1000 | Loss: 0.00002164
Iteration 107/1000 | Loss: 0.00002164
Iteration 108/1000 | Loss: 0.00002164
Iteration 109/1000 | Loss: 0.00002164
Iteration 110/1000 | Loss: 0.00002164
Iteration 111/1000 | Loss: 0.00002164
Iteration 112/1000 | Loss: 0.00002164
Iteration 113/1000 | Loss: 0.00002164
Iteration 114/1000 | Loss: 0.00002164
Iteration 115/1000 | Loss: 0.00002164
Iteration 116/1000 | Loss: 0.00002164
Iteration 117/1000 | Loss: 0.00002164
Iteration 118/1000 | Loss: 0.00002164
Iteration 119/1000 | Loss: 0.00002164
Iteration 120/1000 | Loss: 0.00002164
Iteration 121/1000 | Loss: 0.00002164
Iteration 122/1000 | Loss: 0.00002164
Iteration 123/1000 | Loss: 0.00002164
Iteration 124/1000 | Loss: 0.00002164
Iteration 125/1000 | Loss: 0.00002164
Iteration 126/1000 | Loss: 0.00002164
Iteration 127/1000 | Loss: 0.00002164
Iteration 128/1000 | Loss: 0.00002164
Iteration 129/1000 | Loss: 0.00002164
Iteration 130/1000 | Loss: 0.00002164
Iteration 131/1000 | Loss: 0.00002164
Iteration 132/1000 | Loss: 0.00002164
Iteration 133/1000 | Loss: 0.00002164
Iteration 134/1000 | Loss: 0.00002164
Iteration 135/1000 | Loss: 0.00002164
Iteration 136/1000 | Loss: 0.00002164
Iteration 137/1000 | Loss: 0.00002164
Iteration 138/1000 | Loss: 0.00002164
Iteration 139/1000 | Loss: 0.00002164
Iteration 140/1000 | Loss: 0.00002164
Iteration 141/1000 | Loss: 0.00002164
Iteration 142/1000 | Loss: 0.00002164
Iteration 143/1000 | Loss: 0.00002164
Iteration 144/1000 | Loss: 0.00002164
Iteration 145/1000 | Loss: 0.00002164
Iteration 146/1000 | Loss: 0.00002164
Iteration 147/1000 | Loss: 0.00002164
Iteration 148/1000 | Loss: 0.00002164
Iteration 149/1000 | Loss: 0.00002164
Iteration 150/1000 | Loss: 0.00002164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.1635736629832536e-05, 2.1635736629832536e-05, 2.1635736629832536e-05, 2.1635736629832536e-05, 2.1635736629832536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1635736629832536e-05

Optimization complete. Final v2v error: 3.8742778301239014 mm

Highest mean error: 3.9373891353607178 mm for frame 150

Lowest mean error: 3.795635938644409 mm for frame 108

Saving results

Total time: 27.620938062667847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073999
Iteration 2/25 | Loss: 0.01073999
Iteration 3/25 | Loss: 0.01073998
Iteration 4/25 | Loss: 0.01073998
Iteration 5/25 | Loss: 0.01073998
Iteration 6/25 | Loss: 0.01073998
Iteration 7/25 | Loss: 0.01073998
Iteration 8/25 | Loss: 0.01073997
Iteration 9/25 | Loss: 0.01073997
Iteration 10/25 | Loss: 0.01073997
Iteration 11/25 | Loss: 0.01073997
Iteration 12/25 | Loss: 0.01073997
Iteration 13/25 | Loss: 0.01073997
Iteration 14/25 | Loss: 0.01073997
Iteration 15/25 | Loss: 0.01073996
Iteration 16/25 | Loss: 0.01073996
Iteration 17/25 | Loss: 0.01073996
Iteration 18/25 | Loss: 0.01073996
Iteration 19/25 | Loss: 0.01073995
Iteration 20/25 | Loss: 0.01073995
Iteration 21/25 | Loss: 0.01073995
Iteration 22/25 | Loss: 0.01073995
Iteration 23/25 | Loss: 0.01073995
Iteration 24/25 | Loss: 0.01073994
Iteration 25/25 | Loss: 0.01073994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23004842
Iteration 2/25 | Loss: 0.19318683
Iteration 3/25 | Loss: 0.18448339
Iteration 4/25 | Loss: 0.18348922
Iteration 5/25 | Loss: 0.18107770
Iteration 6/25 | Loss: 0.18085866
Iteration 7/25 | Loss: 0.18085866
Iteration 8/25 | Loss: 0.18085866
Iteration 9/25 | Loss: 0.18085866
Iteration 10/25 | Loss: 0.18085864
Iteration 11/25 | Loss: 0.18085864
Iteration 12/25 | Loss: 0.18085864
Iteration 13/25 | Loss: 0.18085864
Iteration 14/25 | Loss: 0.18085864
Iteration 15/25 | Loss: 0.18085864
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.18085864186286926, 0.18085864186286926, 0.18085864186286926, 0.18085864186286926, 0.18085864186286926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18085864186286926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18085864
Iteration 2/1000 | Loss: 0.00211490
Iteration 3/1000 | Loss: 0.00205419
Iteration 4/1000 | Loss: 0.00043109
Iteration 5/1000 | Loss: 0.00010171
Iteration 6/1000 | Loss: 0.00008917
Iteration 7/1000 | Loss: 0.00007400
Iteration 8/1000 | Loss: 0.00003671
Iteration 9/1000 | Loss: 0.00003106
Iteration 10/1000 | Loss: 0.00002922
Iteration 11/1000 | Loss: 0.00005186
Iteration 12/1000 | Loss: 0.00003360
Iteration 13/1000 | Loss: 0.00002575
Iteration 14/1000 | Loss: 0.00002967
Iteration 15/1000 | Loss: 0.00002431
Iteration 16/1000 | Loss: 0.00007451
Iteration 17/1000 | Loss: 0.00003043
Iteration 18/1000 | Loss: 0.00001821
Iteration 19/1000 | Loss: 0.00001747
Iteration 20/1000 | Loss: 0.00002995
Iteration 21/1000 | Loss: 0.00002686
Iteration 22/1000 | Loss: 0.00001583
Iteration 23/1000 | Loss: 0.00001829
Iteration 24/1000 | Loss: 0.00002332
Iteration 25/1000 | Loss: 0.00001452
Iteration 26/1000 | Loss: 0.00003287
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00005151
Iteration 29/1000 | Loss: 0.00001583
Iteration 30/1000 | Loss: 0.00001362
Iteration 31/1000 | Loss: 0.00001342
Iteration 32/1000 | Loss: 0.00005544
Iteration 33/1000 | Loss: 0.00001334
Iteration 34/1000 | Loss: 0.00001615
Iteration 35/1000 | Loss: 0.00001322
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001320
Iteration 38/1000 | Loss: 0.00001320
Iteration 39/1000 | Loss: 0.00001320
Iteration 40/1000 | Loss: 0.00001317
Iteration 41/1000 | Loss: 0.00001735
Iteration 42/1000 | Loss: 0.00001299
Iteration 43/1000 | Loss: 0.00001299
Iteration 44/1000 | Loss: 0.00001297
Iteration 45/1000 | Loss: 0.00001290
Iteration 46/1000 | Loss: 0.00001289
Iteration 47/1000 | Loss: 0.00001288
Iteration 48/1000 | Loss: 0.00001288
Iteration 49/1000 | Loss: 0.00001274
Iteration 50/1000 | Loss: 0.00001273
Iteration 51/1000 | Loss: 0.00001270
Iteration 52/1000 | Loss: 0.00001269
Iteration 53/1000 | Loss: 0.00001269
Iteration 54/1000 | Loss: 0.00001269
Iteration 55/1000 | Loss: 0.00001268
Iteration 56/1000 | Loss: 0.00001268
Iteration 57/1000 | Loss: 0.00001267
Iteration 58/1000 | Loss: 0.00001267
Iteration 59/1000 | Loss: 0.00001267
Iteration 60/1000 | Loss: 0.00001266
Iteration 61/1000 | Loss: 0.00001265
Iteration 62/1000 | Loss: 0.00001264
Iteration 63/1000 | Loss: 0.00001264
Iteration 64/1000 | Loss: 0.00001264
Iteration 65/1000 | Loss: 0.00001263
Iteration 66/1000 | Loss: 0.00001263
Iteration 67/1000 | Loss: 0.00001263
Iteration 68/1000 | Loss: 0.00001262
Iteration 69/1000 | Loss: 0.00001262
Iteration 70/1000 | Loss: 0.00001261
Iteration 71/1000 | Loss: 0.00001261
Iteration 72/1000 | Loss: 0.00001261
Iteration 73/1000 | Loss: 0.00001261
Iteration 74/1000 | Loss: 0.00001260
Iteration 75/1000 | Loss: 0.00001260
Iteration 76/1000 | Loss: 0.00001260
Iteration 77/1000 | Loss: 0.00001259
Iteration 78/1000 | Loss: 0.00001254
Iteration 79/1000 | Loss: 0.00001254
Iteration 80/1000 | Loss: 0.00001254
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001252
Iteration 84/1000 | Loss: 0.00001252
Iteration 85/1000 | Loss: 0.00001252
Iteration 86/1000 | Loss: 0.00001252
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001252
Iteration 91/1000 | Loss: 0.00001252
Iteration 92/1000 | Loss: 0.00001252
Iteration 93/1000 | Loss: 0.00001252
Iteration 94/1000 | Loss: 0.00001252
Iteration 95/1000 | Loss: 0.00001252
Iteration 96/1000 | Loss: 0.00001252
Iteration 97/1000 | Loss: 0.00001252
Iteration 98/1000 | Loss: 0.00001252
Iteration 99/1000 | Loss: 0.00001252
Iteration 100/1000 | Loss: 0.00001251
Iteration 101/1000 | Loss: 0.00001251
Iteration 102/1000 | Loss: 0.00001251
Iteration 103/1000 | Loss: 0.00001251
Iteration 104/1000 | Loss: 0.00001251
Iteration 105/1000 | Loss: 0.00001251
Iteration 106/1000 | Loss: 0.00001251
Iteration 107/1000 | Loss: 0.00001251
Iteration 108/1000 | Loss: 0.00001251
Iteration 109/1000 | Loss: 0.00001251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.2514252375694923e-05, 1.2514252375694923e-05, 1.2514252375694923e-05, 1.2514252375694923e-05, 1.2514252375694923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2514252375694923e-05

Optimization complete. Final v2v error: 2.788473606109619 mm

Highest mean error: 5.504149436950684 mm for frame 63

Lowest mean error: 2.206132411956787 mm for frame 95

Saving results

Total time: 78.16599559783936
