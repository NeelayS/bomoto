Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=200, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11200-11255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786531
Iteration 2/25 | Loss: 0.00136562
Iteration 3/25 | Loss: 0.00119066
Iteration 4/25 | Loss: 0.00116653
Iteration 5/25 | Loss: 0.00116152
Iteration 6/25 | Loss: 0.00116028
Iteration 7/25 | Loss: 0.00116012
Iteration 8/25 | Loss: 0.00116012
Iteration 9/25 | Loss: 0.00116012
Iteration 10/25 | Loss: 0.00116012
Iteration 11/25 | Loss: 0.00116012
Iteration 12/25 | Loss: 0.00116012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011601236183196306, 0.0011601236183196306, 0.0011601236183196306, 0.0011601236183196306, 0.0011601236183196306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011601236183196306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35707939
Iteration 2/25 | Loss: 0.00080581
Iteration 3/25 | Loss: 0.00080581
Iteration 4/25 | Loss: 0.00080581
Iteration 5/25 | Loss: 0.00080581
Iteration 6/25 | Loss: 0.00080581
Iteration 7/25 | Loss: 0.00080581
Iteration 8/25 | Loss: 0.00080581
Iteration 9/25 | Loss: 0.00080581
Iteration 10/25 | Loss: 0.00080581
Iteration 11/25 | Loss: 0.00080581
Iteration 12/25 | Loss: 0.00080581
Iteration 13/25 | Loss: 0.00080581
Iteration 14/25 | Loss: 0.00080581
Iteration 15/25 | Loss: 0.00080581
Iteration 16/25 | Loss: 0.00080581
Iteration 17/25 | Loss: 0.00080581
Iteration 18/25 | Loss: 0.00080581
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008058098028413951, 0.0008058098028413951, 0.0008058098028413951, 0.0008058098028413951, 0.0008058098028413951]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008058098028413951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080581
Iteration 2/1000 | Loss: 0.00003659
Iteration 3/1000 | Loss: 0.00002333
Iteration 4/1000 | Loss: 0.00001978
Iteration 5/1000 | Loss: 0.00001868
Iteration 6/1000 | Loss: 0.00001765
Iteration 7/1000 | Loss: 0.00001696
Iteration 8/1000 | Loss: 0.00001640
Iteration 9/1000 | Loss: 0.00001600
Iteration 10/1000 | Loss: 0.00001570
Iteration 11/1000 | Loss: 0.00001544
Iteration 12/1000 | Loss: 0.00001529
Iteration 13/1000 | Loss: 0.00001520
Iteration 14/1000 | Loss: 0.00001516
Iteration 15/1000 | Loss: 0.00001510
Iteration 16/1000 | Loss: 0.00001507
Iteration 17/1000 | Loss: 0.00001507
Iteration 18/1000 | Loss: 0.00001506
Iteration 19/1000 | Loss: 0.00001505
Iteration 20/1000 | Loss: 0.00001505
Iteration 21/1000 | Loss: 0.00001504
Iteration 22/1000 | Loss: 0.00001501
Iteration 23/1000 | Loss: 0.00001495
Iteration 24/1000 | Loss: 0.00001495
Iteration 25/1000 | Loss: 0.00001494
Iteration 26/1000 | Loss: 0.00001493
Iteration 27/1000 | Loss: 0.00001492
Iteration 28/1000 | Loss: 0.00001492
Iteration 29/1000 | Loss: 0.00001492
Iteration 30/1000 | Loss: 0.00001491
Iteration 31/1000 | Loss: 0.00001491
Iteration 32/1000 | Loss: 0.00001491
Iteration 33/1000 | Loss: 0.00001490
Iteration 34/1000 | Loss: 0.00001490
Iteration 35/1000 | Loss: 0.00001490
Iteration 36/1000 | Loss: 0.00001489
Iteration 37/1000 | Loss: 0.00001489
Iteration 38/1000 | Loss: 0.00001489
Iteration 39/1000 | Loss: 0.00001488
Iteration 40/1000 | Loss: 0.00001488
Iteration 41/1000 | Loss: 0.00001488
Iteration 42/1000 | Loss: 0.00001488
Iteration 43/1000 | Loss: 0.00001488
Iteration 44/1000 | Loss: 0.00001487
Iteration 45/1000 | Loss: 0.00001487
Iteration 46/1000 | Loss: 0.00001487
Iteration 47/1000 | Loss: 0.00001487
Iteration 48/1000 | Loss: 0.00001486
Iteration 49/1000 | Loss: 0.00001486
Iteration 50/1000 | Loss: 0.00001486
Iteration 51/1000 | Loss: 0.00001486
Iteration 52/1000 | Loss: 0.00001486
Iteration 53/1000 | Loss: 0.00001486
Iteration 54/1000 | Loss: 0.00001486
Iteration 55/1000 | Loss: 0.00001486
Iteration 56/1000 | Loss: 0.00001486
Iteration 57/1000 | Loss: 0.00001486
Iteration 58/1000 | Loss: 0.00001486
Iteration 59/1000 | Loss: 0.00001486
Iteration 60/1000 | Loss: 0.00001485
Iteration 61/1000 | Loss: 0.00001485
Iteration 62/1000 | Loss: 0.00001485
Iteration 63/1000 | Loss: 0.00001484
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00001484
Iteration 66/1000 | Loss: 0.00001484
Iteration 67/1000 | Loss: 0.00001483
Iteration 68/1000 | Loss: 0.00001483
Iteration 69/1000 | Loss: 0.00001483
Iteration 70/1000 | Loss: 0.00001482
Iteration 71/1000 | Loss: 0.00001482
Iteration 72/1000 | Loss: 0.00001482
Iteration 73/1000 | Loss: 0.00001482
Iteration 74/1000 | Loss: 0.00001481
Iteration 75/1000 | Loss: 0.00001481
Iteration 76/1000 | Loss: 0.00001481
Iteration 77/1000 | Loss: 0.00001481
Iteration 78/1000 | Loss: 0.00001481
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001480
Iteration 81/1000 | Loss: 0.00001480
Iteration 82/1000 | Loss: 0.00001480
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001479
Iteration 86/1000 | Loss: 0.00001479
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001478
Iteration 90/1000 | Loss: 0.00001478
Iteration 91/1000 | Loss: 0.00001477
Iteration 92/1000 | Loss: 0.00001477
Iteration 93/1000 | Loss: 0.00001477
Iteration 94/1000 | Loss: 0.00001477
Iteration 95/1000 | Loss: 0.00001476
Iteration 96/1000 | Loss: 0.00001476
Iteration 97/1000 | Loss: 0.00001476
Iteration 98/1000 | Loss: 0.00001476
Iteration 99/1000 | Loss: 0.00001476
Iteration 100/1000 | Loss: 0.00001476
Iteration 101/1000 | Loss: 0.00001476
Iteration 102/1000 | Loss: 0.00001475
Iteration 103/1000 | Loss: 0.00001475
Iteration 104/1000 | Loss: 0.00001475
Iteration 105/1000 | Loss: 0.00001475
Iteration 106/1000 | Loss: 0.00001474
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001474
Iteration 109/1000 | Loss: 0.00001474
Iteration 110/1000 | Loss: 0.00001474
Iteration 111/1000 | Loss: 0.00001474
Iteration 112/1000 | Loss: 0.00001474
Iteration 113/1000 | Loss: 0.00001474
Iteration 114/1000 | Loss: 0.00001474
Iteration 115/1000 | Loss: 0.00001474
Iteration 116/1000 | Loss: 0.00001474
Iteration 117/1000 | Loss: 0.00001473
Iteration 118/1000 | Loss: 0.00001473
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001473
Iteration 121/1000 | Loss: 0.00001473
Iteration 122/1000 | Loss: 0.00001473
Iteration 123/1000 | Loss: 0.00001473
Iteration 124/1000 | Loss: 0.00001473
Iteration 125/1000 | Loss: 0.00001473
Iteration 126/1000 | Loss: 0.00001473
Iteration 127/1000 | Loss: 0.00001473
Iteration 128/1000 | Loss: 0.00001472
Iteration 129/1000 | Loss: 0.00001472
Iteration 130/1000 | Loss: 0.00001472
Iteration 131/1000 | Loss: 0.00001472
Iteration 132/1000 | Loss: 0.00001472
Iteration 133/1000 | Loss: 0.00001472
Iteration 134/1000 | Loss: 0.00001472
Iteration 135/1000 | Loss: 0.00001472
Iteration 136/1000 | Loss: 0.00001472
Iteration 137/1000 | Loss: 0.00001472
Iteration 138/1000 | Loss: 0.00001472
Iteration 139/1000 | Loss: 0.00001472
Iteration 140/1000 | Loss: 0.00001472
Iteration 141/1000 | Loss: 0.00001472
Iteration 142/1000 | Loss: 0.00001472
Iteration 143/1000 | Loss: 0.00001471
Iteration 144/1000 | Loss: 0.00001471
Iteration 145/1000 | Loss: 0.00001471
Iteration 146/1000 | Loss: 0.00001471
Iteration 147/1000 | Loss: 0.00001471
Iteration 148/1000 | Loss: 0.00001471
Iteration 149/1000 | Loss: 0.00001471
Iteration 150/1000 | Loss: 0.00001471
Iteration 151/1000 | Loss: 0.00001471
Iteration 152/1000 | Loss: 0.00001471
Iteration 153/1000 | Loss: 0.00001471
Iteration 154/1000 | Loss: 0.00001471
Iteration 155/1000 | Loss: 0.00001471
Iteration 156/1000 | Loss: 0.00001471
Iteration 157/1000 | Loss: 0.00001471
Iteration 158/1000 | Loss: 0.00001471
Iteration 159/1000 | Loss: 0.00001470
Iteration 160/1000 | Loss: 0.00001470
Iteration 161/1000 | Loss: 0.00001470
Iteration 162/1000 | Loss: 0.00001470
Iteration 163/1000 | Loss: 0.00001470
Iteration 164/1000 | Loss: 0.00001470
Iteration 165/1000 | Loss: 0.00001470
Iteration 166/1000 | Loss: 0.00001470
Iteration 167/1000 | Loss: 0.00001470
Iteration 168/1000 | Loss: 0.00001470
Iteration 169/1000 | Loss: 0.00001470
Iteration 170/1000 | Loss: 0.00001470
Iteration 171/1000 | Loss: 0.00001470
Iteration 172/1000 | Loss: 0.00001470
Iteration 173/1000 | Loss: 0.00001470
Iteration 174/1000 | Loss: 0.00001470
Iteration 175/1000 | Loss: 0.00001470
Iteration 176/1000 | Loss: 0.00001469
Iteration 177/1000 | Loss: 0.00001469
Iteration 178/1000 | Loss: 0.00001469
Iteration 179/1000 | Loss: 0.00001469
Iteration 180/1000 | Loss: 0.00001469
Iteration 181/1000 | Loss: 0.00001469
Iteration 182/1000 | Loss: 0.00001469
Iteration 183/1000 | Loss: 0.00001469
Iteration 184/1000 | Loss: 0.00001469
Iteration 185/1000 | Loss: 0.00001469
Iteration 186/1000 | Loss: 0.00001469
Iteration 187/1000 | Loss: 0.00001469
Iteration 188/1000 | Loss: 0.00001469
Iteration 189/1000 | Loss: 0.00001469
Iteration 190/1000 | Loss: 0.00001468
Iteration 191/1000 | Loss: 0.00001468
Iteration 192/1000 | Loss: 0.00001468
Iteration 193/1000 | Loss: 0.00001468
Iteration 194/1000 | Loss: 0.00001468
Iteration 195/1000 | Loss: 0.00001468
Iteration 196/1000 | Loss: 0.00001468
Iteration 197/1000 | Loss: 0.00001468
Iteration 198/1000 | Loss: 0.00001468
Iteration 199/1000 | Loss: 0.00001468
Iteration 200/1000 | Loss: 0.00001468
Iteration 201/1000 | Loss: 0.00001468
Iteration 202/1000 | Loss: 0.00001468
Iteration 203/1000 | Loss: 0.00001468
Iteration 204/1000 | Loss: 0.00001467
Iteration 205/1000 | Loss: 0.00001467
Iteration 206/1000 | Loss: 0.00001467
Iteration 207/1000 | Loss: 0.00001467
Iteration 208/1000 | Loss: 0.00001467
Iteration 209/1000 | Loss: 0.00001467
Iteration 210/1000 | Loss: 0.00001467
Iteration 211/1000 | Loss: 0.00001467
Iteration 212/1000 | Loss: 0.00001467
Iteration 213/1000 | Loss: 0.00001467
Iteration 214/1000 | Loss: 0.00001467
Iteration 215/1000 | Loss: 0.00001467
Iteration 216/1000 | Loss: 0.00001467
Iteration 217/1000 | Loss: 0.00001467
Iteration 218/1000 | Loss: 0.00001467
Iteration 219/1000 | Loss: 0.00001467
Iteration 220/1000 | Loss: 0.00001467
Iteration 221/1000 | Loss: 0.00001467
Iteration 222/1000 | Loss: 0.00001466
Iteration 223/1000 | Loss: 0.00001466
Iteration 224/1000 | Loss: 0.00001466
Iteration 225/1000 | Loss: 0.00001466
Iteration 226/1000 | Loss: 0.00001466
Iteration 227/1000 | Loss: 0.00001466
Iteration 228/1000 | Loss: 0.00001466
Iteration 229/1000 | Loss: 0.00001466
Iteration 230/1000 | Loss: 0.00001466
Iteration 231/1000 | Loss: 0.00001466
Iteration 232/1000 | Loss: 0.00001465
Iteration 233/1000 | Loss: 0.00001465
Iteration 234/1000 | Loss: 0.00001465
Iteration 235/1000 | Loss: 0.00001465
Iteration 236/1000 | Loss: 0.00001465
Iteration 237/1000 | Loss: 0.00001465
Iteration 238/1000 | Loss: 0.00001465
Iteration 239/1000 | Loss: 0.00001465
Iteration 240/1000 | Loss: 0.00001465
Iteration 241/1000 | Loss: 0.00001465
Iteration 242/1000 | Loss: 0.00001465
Iteration 243/1000 | Loss: 0.00001465
Iteration 244/1000 | Loss: 0.00001465
Iteration 245/1000 | Loss: 0.00001465
Iteration 246/1000 | Loss: 0.00001465
Iteration 247/1000 | Loss: 0.00001465
Iteration 248/1000 | Loss: 0.00001465
Iteration 249/1000 | Loss: 0.00001464
Iteration 250/1000 | Loss: 0.00001464
Iteration 251/1000 | Loss: 0.00001464
Iteration 252/1000 | Loss: 0.00001464
Iteration 253/1000 | Loss: 0.00001464
Iteration 254/1000 | Loss: 0.00001464
Iteration 255/1000 | Loss: 0.00001464
Iteration 256/1000 | Loss: 0.00001464
Iteration 257/1000 | Loss: 0.00001464
Iteration 258/1000 | Loss: 0.00001464
Iteration 259/1000 | Loss: 0.00001464
Iteration 260/1000 | Loss: 0.00001464
Iteration 261/1000 | Loss: 0.00001464
Iteration 262/1000 | Loss: 0.00001464
Iteration 263/1000 | Loss: 0.00001464
Iteration 264/1000 | Loss: 0.00001464
Iteration 265/1000 | Loss: 0.00001464
Iteration 266/1000 | Loss: 0.00001464
Iteration 267/1000 | Loss: 0.00001464
Iteration 268/1000 | Loss: 0.00001464
Iteration 269/1000 | Loss: 0.00001463
Iteration 270/1000 | Loss: 0.00001463
Iteration 271/1000 | Loss: 0.00001463
Iteration 272/1000 | Loss: 0.00001463
Iteration 273/1000 | Loss: 0.00001463
Iteration 274/1000 | Loss: 0.00001463
Iteration 275/1000 | Loss: 0.00001463
Iteration 276/1000 | Loss: 0.00001463
Iteration 277/1000 | Loss: 0.00001463
Iteration 278/1000 | Loss: 0.00001463
Iteration 279/1000 | Loss: 0.00001463
Iteration 280/1000 | Loss: 0.00001463
Iteration 281/1000 | Loss: 0.00001463
Iteration 282/1000 | Loss: 0.00001463
Iteration 283/1000 | Loss: 0.00001463
Iteration 284/1000 | Loss: 0.00001463
Iteration 285/1000 | Loss: 0.00001463
Iteration 286/1000 | Loss: 0.00001463
Iteration 287/1000 | Loss: 0.00001463
Iteration 288/1000 | Loss: 0.00001463
Iteration 289/1000 | Loss: 0.00001463
Iteration 290/1000 | Loss: 0.00001463
Iteration 291/1000 | Loss: 0.00001463
Iteration 292/1000 | Loss: 0.00001463
Iteration 293/1000 | Loss: 0.00001463
Iteration 294/1000 | Loss: 0.00001463
Iteration 295/1000 | Loss: 0.00001463
Iteration 296/1000 | Loss: 0.00001463
Iteration 297/1000 | Loss: 0.00001463
Iteration 298/1000 | Loss: 0.00001463
Iteration 299/1000 | Loss: 0.00001463
Iteration 300/1000 | Loss: 0.00001463
Iteration 301/1000 | Loss: 0.00001463
Iteration 302/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [1.4630928490078077e-05, 1.4630928490078077e-05, 1.4630928490078077e-05, 1.4630928490078077e-05, 1.4630928490078077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4630928490078077e-05

Optimization complete. Final v2v error: 3.2119951248168945 mm

Highest mean error: 3.943122386932373 mm for frame 88

Lowest mean error: 2.8156113624572754 mm for frame 56

Saving results

Total time: 47.73307752609253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509214
Iteration 2/25 | Loss: 0.00129244
Iteration 3/25 | Loss: 0.00116873
Iteration 4/25 | Loss: 0.00115410
Iteration 5/25 | Loss: 0.00114603
Iteration 6/25 | Loss: 0.00114601
Iteration 7/25 | Loss: 0.00114341
Iteration 8/25 | Loss: 0.00114491
Iteration 9/25 | Loss: 0.00114322
Iteration 10/25 | Loss: 0.00114321
Iteration 11/25 | Loss: 0.00114320
Iteration 12/25 | Loss: 0.00114319
Iteration 13/25 | Loss: 0.00114319
Iteration 14/25 | Loss: 0.00114318
Iteration 15/25 | Loss: 0.00114318
Iteration 16/25 | Loss: 0.00114318
Iteration 17/25 | Loss: 0.00114318
Iteration 18/25 | Loss: 0.00114318
Iteration 19/25 | Loss: 0.00114318
Iteration 20/25 | Loss: 0.00114318
Iteration 21/25 | Loss: 0.00114318
Iteration 22/25 | Loss: 0.00114447
Iteration 23/25 | Loss: 0.00114316
Iteration 24/25 | Loss: 0.00114316
Iteration 25/25 | Loss: 0.00114316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.83138227
Iteration 2/25 | Loss: 0.00083959
Iteration 3/25 | Loss: 0.00083055
Iteration 4/25 | Loss: 0.00083055
Iteration 5/25 | Loss: 0.00083055
Iteration 6/25 | Loss: 0.00083055
Iteration 7/25 | Loss: 0.00083055
Iteration 8/25 | Loss: 0.00083054
Iteration 9/25 | Loss: 0.00083054
Iteration 10/25 | Loss: 0.00083054
Iteration 11/25 | Loss: 0.00083054
Iteration 12/25 | Loss: 0.00083054
Iteration 13/25 | Loss: 0.00083054
Iteration 14/25 | Loss: 0.00083054
Iteration 15/25 | Loss: 0.00083054
Iteration 16/25 | Loss: 0.00083054
Iteration 17/25 | Loss: 0.00083054
Iteration 18/25 | Loss: 0.00083054
Iteration 19/25 | Loss: 0.00083054
Iteration 20/25 | Loss: 0.00083054
Iteration 21/25 | Loss: 0.00083054
Iteration 22/25 | Loss: 0.00083054
Iteration 23/25 | Loss: 0.00083054
Iteration 24/25 | Loss: 0.00083054
Iteration 25/25 | Loss: 0.00083054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083054
Iteration 2/1000 | Loss: 0.00003514
Iteration 3/1000 | Loss: 0.00003639
Iteration 4/1000 | Loss: 0.00001659
Iteration 5/1000 | Loss: 0.00002656
Iteration 6/1000 | Loss: 0.00001755
Iteration 7/1000 | Loss: 0.00004068
Iteration 8/1000 | Loss: 0.00013280
Iteration 9/1000 | Loss: 0.00001559
Iteration 10/1000 | Loss: 0.00001435
Iteration 11/1000 | Loss: 0.00001409
Iteration 12/1000 | Loss: 0.00001377
Iteration 13/1000 | Loss: 0.00001346
Iteration 14/1000 | Loss: 0.00005096
Iteration 15/1000 | Loss: 0.00002579
Iteration 16/1000 | Loss: 0.00004994
Iteration 17/1000 | Loss: 0.00009694
Iteration 18/1000 | Loss: 0.00001307
Iteration 19/1000 | Loss: 0.00001306
Iteration 20/1000 | Loss: 0.00004397
Iteration 21/1000 | Loss: 0.00001307
Iteration 22/1000 | Loss: 0.00001575
Iteration 23/1000 | Loss: 0.00001366
Iteration 24/1000 | Loss: 0.00001293
Iteration 25/1000 | Loss: 0.00001292
Iteration 26/1000 | Loss: 0.00001291
Iteration 27/1000 | Loss: 0.00001289
Iteration 28/1000 | Loss: 0.00001281
Iteration 29/1000 | Loss: 0.00001278
Iteration 30/1000 | Loss: 0.00001277
Iteration 31/1000 | Loss: 0.00001277
Iteration 32/1000 | Loss: 0.00001277
Iteration 33/1000 | Loss: 0.00001276
Iteration 34/1000 | Loss: 0.00001270
Iteration 35/1000 | Loss: 0.00001267
Iteration 36/1000 | Loss: 0.00001266
Iteration 37/1000 | Loss: 0.00001266
Iteration 38/1000 | Loss: 0.00001266
Iteration 39/1000 | Loss: 0.00001265
Iteration 40/1000 | Loss: 0.00001265
Iteration 41/1000 | Loss: 0.00001264
Iteration 42/1000 | Loss: 0.00001263
Iteration 43/1000 | Loss: 0.00001263
Iteration 44/1000 | Loss: 0.00001263
Iteration 45/1000 | Loss: 0.00001262
Iteration 46/1000 | Loss: 0.00001262
Iteration 47/1000 | Loss: 0.00001262
Iteration 48/1000 | Loss: 0.00001261
Iteration 49/1000 | Loss: 0.00001261
Iteration 50/1000 | Loss: 0.00001261
Iteration 51/1000 | Loss: 0.00001261
Iteration 52/1000 | Loss: 0.00001260
Iteration 53/1000 | Loss: 0.00001260
Iteration 54/1000 | Loss: 0.00001259
Iteration 55/1000 | Loss: 0.00001259
Iteration 56/1000 | Loss: 0.00001259
Iteration 57/1000 | Loss: 0.00001258
Iteration 58/1000 | Loss: 0.00001256
Iteration 59/1000 | Loss: 0.00001253
Iteration 60/1000 | Loss: 0.00001253
Iteration 61/1000 | Loss: 0.00001250
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001249
Iteration 64/1000 | Loss: 0.00001249
Iteration 65/1000 | Loss: 0.00001248
Iteration 66/1000 | Loss: 0.00001247
Iteration 67/1000 | Loss: 0.00001247
Iteration 68/1000 | Loss: 0.00001247
Iteration 69/1000 | Loss: 0.00004332
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001243
Iteration 72/1000 | Loss: 0.00001243
Iteration 73/1000 | Loss: 0.00001243
Iteration 74/1000 | Loss: 0.00001242
Iteration 75/1000 | Loss: 0.00001241
Iteration 76/1000 | Loss: 0.00001240
Iteration 77/1000 | Loss: 0.00001240
Iteration 78/1000 | Loss: 0.00001240
Iteration 79/1000 | Loss: 0.00001239
Iteration 80/1000 | Loss: 0.00001239
Iteration 81/1000 | Loss: 0.00001239
Iteration 82/1000 | Loss: 0.00001239
Iteration 83/1000 | Loss: 0.00001239
Iteration 84/1000 | Loss: 0.00001239
Iteration 85/1000 | Loss: 0.00001239
Iteration 86/1000 | Loss: 0.00001239
Iteration 87/1000 | Loss: 0.00001239
Iteration 88/1000 | Loss: 0.00001239
Iteration 89/1000 | Loss: 0.00001239
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001239
Iteration 94/1000 | Loss: 0.00001238
Iteration 95/1000 | Loss: 0.00001238
Iteration 96/1000 | Loss: 0.00001238
Iteration 97/1000 | Loss: 0.00001238
Iteration 98/1000 | Loss: 0.00001238
Iteration 99/1000 | Loss: 0.00001238
Iteration 100/1000 | Loss: 0.00001237
Iteration 101/1000 | Loss: 0.00001237
Iteration 102/1000 | Loss: 0.00001237
Iteration 103/1000 | Loss: 0.00001237
Iteration 104/1000 | Loss: 0.00001237
Iteration 105/1000 | Loss: 0.00001237
Iteration 106/1000 | Loss: 0.00001236
Iteration 107/1000 | Loss: 0.00001235
Iteration 108/1000 | Loss: 0.00001235
Iteration 109/1000 | Loss: 0.00001234
Iteration 110/1000 | Loss: 0.00001234
Iteration 111/1000 | Loss: 0.00001234
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001233
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001231
Iteration 120/1000 | Loss: 0.00001231
Iteration 121/1000 | Loss: 0.00001230
Iteration 122/1000 | Loss: 0.00001230
Iteration 123/1000 | Loss: 0.00001230
Iteration 124/1000 | Loss: 0.00001230
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001230
Iteration 127/1000 | Loss: 0.00001229
Iteration 128/1000 | Loss: 0.00001229
Iteration 129/1000 | Loss: 0.00001229
Iteration 130/1000 | Loss: 0.00001229
Iteration 131/1000 | Loss: 0.00001229
Iteration 132/1000 | Loss: 0.00001229
Iteration 133/1000 | Loss: 0.00001229
Iteration 134/1000 | Loss: 0.00001229
Iteration 135/1000 | Loss: 0.00001228
Iteration 136/1000 | Loss: 0.00001228
Iteration 137/1000 | Loss: 0.00001228
Iteration 138/1000 | Loss: 0.00001228
Iteration 139/1000 | Loss: 0.00001228
Iteration 140/1000 | Loss: 0.00001228
Iteration 141/1000 | Loss: 0.00001228
Iteration 142/1000 | Loss: 0.00001228
Iteration 143/1000 | Loss: 0.00001228
Iteration 144/1000 | Loss: 0.00001228
Iteration 145/1000 | Loss: 0.00001228
Iteration 146/1000 | Loss: 0.00001227
Iteration 147/1000 | Loss: 0.00001227
Iteration 148/1000 | Loss: 0.00001227
Iteration 149/1000 | Loss: 0.00001227
Iteration 150/1000 | Loss: 0.00001227
Iteration 151/1000 | Loss: 0.00001227
Iteration 152/1000 | Loss: 0.00001227
Iteration 153/1000 | Loss: 0.00001227
Iteration 154/1000 | Loss: 0.00001227
Iteration 155/1000 | Loss: 0.00001227
Iteration 156/1000 | Loss: 0.00001227
Iteration 157/1000 | Loss: 0.00001227
Iteration 158/1000 | Loss: 0.00001227
Iteration 159/1000 | Loss: 0.00001227
Iteration 160/1000 | Loss: 0.00001227
Iteration 161/1000 | Loss: 0.00001227
Iteration 162/1000 | Loss: 0.00001226
Iteration 163/1000 | Loss: 0.00001226
Iteration 164/1000 | Loss: 0.00001226
Iteration 165/1000 | Loss: 0.00001226
Iteration 166/1000 | Loss: 0.00001226
Iteration 167/1000 | Loss: 0.00001226
Iteration 168/1000 | Loss: 0.00001226
Iteration 169/1000 | Loss: 0.00001226
Iteration 170/1000 | Loss: 0.00001226
Iteration 171/1000 | Loss: 0.00001226
Iteration 172/1000 | Loss: 0.00001226
Iteration 173/1000 | Loss: 0.00001226
Iteration 174/1000 | Loss: 0.00001226
Iteration 175/1000 | Loss: 0.00001226
Iteration 176/1000 | Loss: 0.00001226
Iteration 177/1000 | Loss: 0.00001225
Iteration 178/1000 | Loss: 0.00001225
Iteration 179/1000 | Loss: 0.00001225
Iteration 180/1000 | Loss: 0.00001225
Iteration 181/1000 | Loss: 0.00001225
Iteration 182/1000 | Loss: 0.00001225
Iteration 183/1000 | Loss: 0.00001225
Iteration 184/1000 | Loss: 0.00001225
Iteration 185/1000 | Loss: 0.00001225
Iteration 186/1000 | Loss: 0.00001225
Iteration 187/1000 | Loss: 0.00001225
Iteration 188/1000 | Loss: 0.00001225
Iteration 189/1000 | Loss: 0.00001225
Iteration 190/1000 | Loss: 0.00001225
Iteration 191/1000 | Loss: 0.00001225
Iteration 192/1000 | Loss: 0.00001225
Iteration 193/1000 | Loss: 0.00001225
Iteration 194/1000 | Loss: 0.00001225
Iteration 195/1000 | Loss: 0.00001224
Iteration 196/1000 | Loss: 0.00001224
Iteration 197/1000 | Loss: 0.00001224
Iteration 198/1000 | Loss: 0.00001224
Iteration 199/1000 | Loss: 0.00001224
Iteration 200/1000 | Loss: 0.00001224
Iteration 201/1000 | Loss: 0.00001224
Iteration 202/1000 | Loss: 0.00001224
Iteration 203/1000 | Loss: 0.00001224
Iteration 204/1000 | Loss: 0.00001224
Iteration 205/1000 | Loss: 0.00001224
Iteration 206/1000 | Loss: 0.00001224
Iteration 207/1000 | Loss: 0.00001224
Iteration 208/1000 | Loss: 0.00001223
Iteration 209/1000 | Loss: 0.00001223
Iteration 210/1000 | Loss: 0.00001223
Iteration 211/1000 | Loss: 0.00001223
Iteration 212/1000 | Loss: 0.00001223
Iteration 213/1000 | Loss: 0.00001223
Iteration 214/1000 | Loss: 0.00001223
Iteration 215/1000 | Loss: 0.00001223
Iteration 216/1000 | Loss: 0.00001223
Iteration 217/1000 | Loss: 0.00001223
Iteration 218/1000 | Loss: 0.00001223
Iteration 219/1000 | Loss: 0.00001223
Iteration 220/1000 | Loss: 0.00001223
Iteration 221/1000 | Loss: 0.00001223
Iteration 222/1000 | Loss: 0.00001223
Iteration 223/1000 | Loss: 0.00001223
Iteration 224/1000 | Loss: 0.00001223
Iteration 225/1000 | Loss: 0.00001223
Iteration 226/1000 | Loss: 0.00001223
Iteration 227/1000 | Loss: 0.00001223
Iteration 228/1000 | Loss: 0.00001223
Iteration 229/1000 | Loss: 0.00001223
Iteration 230/1000 | Loss: 0.00001223
Iteration 231/1000 | Loss: 0.00001223
Iteration 232/1000 | Loss: 0.00001223
Iteration 233/1000 | Loss: 0.00001223
Iteration 234/1000 | Loss: 0.00001223
Iteration 235/1000 | Loss: 0.00001223
Iteration 236/1000 | Loss: 0.00001223
Iteration 237/1000 | Loss: 0.00001223
Iteration 238/1000 | Loss: 0.00001223
Iteration 239/1000 | Loss: 0.00001223
Iteration 240/1000 | Loss: 0.00001223
Iteration 241/1000 | Loss: 0.00001223
Iteration 242/1000 | Loss: 0.00001223
Iteration 243/1000 | Loss: 0.00001223
Iteration 244/1000 | Loss: 0.00001223
Iteration 245/1000 | Loss: 0.00001223
Iteration 246/1000 | Loss: 0.00001223
Iteration 247/1000 | Loss: 0.00001223
Iteration 248/1000 | Loss: 0.00001223
Iteration 249/1000 | Loss: 0.00001223
Iteration 250/1000 | Loss: 0.00001223
Iteration 251/1000 | Loss: 0.00001223
Iteration 252/1000 | Loss: 0.00001223
Iteration 253/1000 | Loss: 0.00001223
Iteration 254/1000 | Loss: 0.00001223
Iteration 255/1000 | Loss: 0.00001223
Iteration 256/1000 | Loss: 0.00001223
Iteration 257/1000 | Loss: 0.00001223
Iteration 258/1000 | Loss: 0.00001223
Iteration 259/1000 | Loss: 0.00001223
Iteration 260/1000 | Loss: 0.00001223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.2225090358697344e-05, 1.2225090358697344e-05, 1.2225090358697344e-05, 1.2225090358697344e-05, 1.2225090358697344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2225090358697344e-05

Optimization complete. Final v2v error: 2.9556174278259277 mm

Highest mean error: 3.7376956939697266 mm for frame 200

Lowest mean error: 2.7460267543792725 mm for frame 215

Saving results

Total time: 78.65130805969238
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999115
Iteration 2/25 | Loss: 0.00294607
Iteration 3/25 | Loss: 0.00242746
Iteration 4/25 | Loss: 0.00205832
Iteration 5/25 | Loss: 0.00192594
Iteration 6/25 | Loss: 0.00169488
Iteration 7/25 | Loss: 0.00154247
Iteration 8/25 | Loss: 0.00139756
Iteration 9/25 | Loss: 0.00132093
Iteration 10/25 | Loss: 0.00131493
Iteration 11/25 | Loss: 0.00130907
Iteration 12/25 | Loss: 0.00130524
Iteration 13/25 | Loss: 0.00130438
Iteration 14/25 | Loss: 0.00130406
Iteration 15/25 | Loss: 0.00130378
Iteration 16/25 | Loss: 0.00130362
Iteration 17/25 | Loss: 0.00130353
Iteration 18/25 | Loss: 0.00130353
Iteration 19/25 | Loss: 0.00130353
Iteration 20/25 | Loss: 0.00130353
Iteration 21/25 | Loss: 0.00130353
Iteration 22/25 | Loss: 0.00130353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013035263400524855, 0.0013035263400524855, 0.0013035263400524855, 0.0013035263400524855, 0.0013035263400524855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013035263400524855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33932376
Iteration 2/25 | Loss: 0.00097934
Iteration 3/25 | Loss: 0.00097933
Iteration 4/25 | Loss: 0.00097933
Iteration 5/25 | Loss: 0.00097933
Iteration 6/25 | Loss: 0.00097933
Iteration 7/25 | Loss: 0.00097933
Iteration 8/25 | Loss: 0.00097933
Iteration 9/25 | Loss: 0.00097933
Iteration 10/25 | Loss: 0.00097933
Iteration 11/25 | Loss: 0.00097933
Iteration 12/25 | Loss: 0.00097933
Iteration 13/25 | Loss: 0.00097933
Iteration 14/25 | Loss: 0.00097933
Iteration 15/25 | Loss: 0.00097933
Iteration 16/25 | Loss: 0.00097933
Iteration 17/25 | Loss: 0.00097933
Iteration 18/25 | Loss: 0.00097933
Iteration 19/25 | Loss: 0.00097933
Iteration 20/25 | Loss: 0.00097933
Iteration 21/25 | Loss: 0.00097933
Iteration 22/25 | Loss: 0.00097933
Iteration 23/25 | Loss: 0.00097933
Iteration 24/25 | Loss: 0.00097933
Iteration 25/25 | Loss: 0.00097933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097933
Iteration 2/1000 | Loss: 0.00007075
Iteration 3/1000 | Loss: 0.00004761
Iteration 4/1000 | Loss: 0.00004337
Iteration 5/1000 | Loss: 0.00004161
Iteration 6/1000 | Loss: 0.00004009
Iteration 7/1000 | Loss: 0.00003911
Iteration 8/1000 | Loss: 0.00003836
Iteration 9/1000 | Loss: 0.00010783
Iteration 10/1000 | Loss: 0.00023813
Iteration 11/1000 | Loss: 0.00009918
Iteration 12/1000 | Loss: 0.00004333
Iteration 13/1000 | Loss: 0.00003704
Iteration 14/1000 | Loss: 0.00003289
Iteration 15/1000 | Loss: 0.00003081
Iteration 16/1000 | Loss: 0.00002888
Iteration 17/1000 | Loss: 0.00002767
Iteration 18/1000 | Loss: 0.00002686
Iteration 19/1000 | Loss: 0.00002618
Iteration 20/1000 | Loss: 0.00002568
Iteration 21/1000 | Loss: 0.00002542
Iteration 22/1000 | Loss: 0.00002518
Iteration 23/1000 | Loss: 0.00002508
Iteration 24/1000 | Loss: 0.00002498
Iteration 25/1000 | Loss: 0.00002497
Iteration 26/1000 | Loss: 0.00002496
Iteration 27/1000 | Loss: 0.00002496
Iteration 28/1000 | Loss: 0.00002495
Iteration 29/1000 | Loss: 0.00002495
Iteration 30/1000 | Loss: 0.00002494
Iteration 31/1000 | Loss: 0.00002494
Iteration 32/1000 | Loss: 0.00002489
Iteration 33/1000 | Loss: 0.00002487
Iteration 34/1000 | Loss: 0.00002486
Iteration 35/1000 | Loss: 0.00002486
Iteration 36/1000 | Loss: 0.00002485
Iteration 37/1000 | Loss: 0.00002483
Iteration 38/1000 | Loss: 0.00002482
Iteration 39/1000 | Loss: 0.00002482
Iteration 40/1000 | Loss: 0.00002479
Iteration 41/1000 | Loss: 0.00002479
Iteration 42/1000 | Loss: 0.00002476
Iteration 43/1000 | Loss: 0.00002474
Iteration 44/1000 | Loss: 0.00002474
Iteration 45/1000 | Loss: 0.00002474
Iteration 46/1000 | Loss: 0.00002474
Iteration 47/1000 | Loss: 0.00002473
Iteration 48/1000 | Loss: 0.00002473
Iteration 49/1000 | Loss: 0.00002473
Iteration 50/1000 | Loss: 0.00002473
Iteration 51/1000 | Loss: 0.00002472
Iteration 52/1000 | Loss: 0.00002472
Iteration 53/1000 | Loss: 0.00002472
Iteration 54/1000 | Loss: 0.00002472
Iteration 55/1000 | Loss: 0.00002471
Iteration 56/1000 | Loss: 0.00002471
Iteration 57/1000 | Loss: 0.00002471
Iteration 58/1000 | Loss: 0.00002470
Iteration 59/1000 | Loss: 0.00002470
Iteration 60/1000 | Loss: 0.00002469
Iteration 61/1000 | Loss: 0.00002469
Iteration 62/1000 | Loss: 0.00002469
Iteration 63/1000 | Loss: 0.00002468
Iteration 64/1000 | Loss: 0.00002468
Iteration 65/1000 | Loss: 0.00002468
Iteration 66/1000 | Loss: 0.00002468
Iteration 67/1000 | Loss: 0.00002468
Iteration 68/1000 | Loss: 0.00002468
Iteration 69/1000 | Loss: 0.00002468
Iteration 70/1000 | Loss: 0.00002467
Iteration 71/1000 | Loss: 0.00002467
Iteration 72/1000 | Loss: 0.00002467
Iteration 73/1000 | Loss: 0.00002467
Iteration 74/1000 | Loss: 0.00002467
Iteration 75/1000 | Loss: 0.00002467
Iteration 76/1000 | Loss: 0.00002466
Iteration 77/1000 | Loss: 0.00002466
Iteration 78/1000 | Loss: 0.00002466
Iteration 79/1000 | Loss: 0.00002466
Iteration 80/1000 | Loss: 0.00002465
Iteration 81/1000 | Loss: 0.00002465
Iteration 82/1000 | Loss: 0.00002465
Iteration 83/1000 | Loss: 0.00002465
Iteration 84/1000 | Loss: 0.00002465
Iteration 85/1000 | Loss: 0.00002464
Iteration 86/1000 | Loss: 0.00002464
Iteration 87/1000 | Loss: 0.00002464
Iteration 88/1000 | Loss: 0.00002464
Iteration 89/1000 | Loss: 0.00002464
Iteration 90/1000 | Loss: 0.00002464
Iteration 91/1000 | Loss: 0.00002464
Iteration 92/1000 | Loss: 0.00002464
Iteration 93/1000 | Loss: 0.00002463
Iteration 94/1000 | Loss: 0.00002463
Iteration 95/1000 | Loss: 0.00002463
Iteration 96/1000 | Loss: 0.00002463
Iteration 97/1000 | Loss: 0.00002463
Iteration 98/1000 | Loss: 0.00002463
Iteration 99/1000 | Loss: 0.00002463
Iteration 100/1000 | Loss: 0.00002463
Iteration 101/1000 | Loss: 0.00002463
Iteration 102/1000 | Loss: 0.00002462
Iteration 103/1000 | Loss: 0.00002462
Iteration 104/1000 | Loss: 0.00002462
Iteration 105/1000 | Loss: 0.00002462
Iteration 106/1000 | Loss: 0.00002462
Iteration 107/1000 | Loss: 0.00002462
Iteration 108/1000 | Loss: 0.00002462
Iteration 109/1000 | Loss: 0.00002462
Iteration 110/1000 | Loss: 0.00002462
Iteration 111/1000 | Loss: 0.00002462
Iteration 112/1000 | Loss: 0.00002462
Iteration 113/1000 | Loss: 0.00002462
Iteration 114/1000 | Loss: 0.00002462
Iteration 115/1000 | Loss: 0.00002462
Iteration 116/1000 | Loss: 0.00002462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.4620543626951985e-05, 2.4620543626951985e-05, 2.4620543626951985e-05, 2.4620543626951985e-05, 2.4620543626951985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4620543626951985e-05

Optimization complete. Final v2v error: 4.216910362243652 mm

Highest mean error: 4.456856727600098 mm for frame 104

Lowest mean error: 4.101946830749512 mm for frame 100

Saving results

Total time: 80.33905005455017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00372616
Iteration 2/25 | Loss: 0.00117970
Iteration 3/25 | Loss: 0.00110880
Iteration 4/25 | Loss: 0.00109763
Iteration 5/25 | Loss: 0.00109320
Iteration 6/25 | Loss: 0.00109224
Iteration 7/25 | Loss: 0.00109224
Iteration 8/25 | Loss: 0.00109224
Iteration 9/25 | Loss: 0.00109224
Iteration 10/25 | Loss: 0.00109224
Iteration 11/25 | Loss: 0.00109224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010922419605776668, 0.0010922419605776668, 0.0010922419605776668, 0.0010922419605776668, 0.0010922419605776668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010922419605776668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45799053
Iteration 2/25 | Loss: 0.00084191
Iteration 3/25 | Loss: 0.00084191
Iteration 4/25 | Loss: 0.00084191
Iteration 5/25 | Loss: 0.00084191
Iteration 6/25 | Loss: 0.00084191
Iteration 7/25 | Loss: 0.00084191
Iteration 8/25 | Loss: 0.00084191
Iteration 9/25 | Loss: 0.00084191
Iteration 10/25 | Loss: 0.00084191
Iteration 11/25 | Loss: 0.00084191
Iteration 12/25 | Loss: 0.00084191
Iteration 13/25 | Loss: 0.00084191
Iteration 14/25 | Loss: 0.00084191
Iteration 15/25 | Loss: 0.00084191
Iteration 16/25 | Loss: 0.00084191
Iteration 17/25 | Loss: 0.00084191
Iteration 18/25 | Loss: 0.00084191
Iteration 19/25 | Loss: 0.00084191
Iteration 20/25 | Loss: 0.00084191
Iteration 21/25 | Loss: 0.00084191
Iteration 22/25 | Loss: 0.00084191
Iteration 23/25 | Loss: 0.00084191
Iteration 24/25 | Loss: 0.00084191
Iteration 25/25 | Loss: 0.00084191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084191
Iteration 2/1000 | Loss: 0.00002130
Iteration 3/1000 | Loss: 0.00001194
Iteration 4/1000 | Loss: 0.00001074
Iteration 5/1000 | Loss: 0.00000997
Iteration 6/1000 | Loss: 0.00000996
Iteration 7/1000 | Loss: 0.00000961
Iteration 8/1000 | Loss: 0.00000925
Iteration 9/1000 | Loss: 0.00000913
Iteration 10/1000 | Loss: 0.00000906
Iteration 11/1000 | Loss: 0.00000906
Iteration 12/1000 | Loss: 0.00000905
Iteration 13/1000 | Loss: 0.00000901
Iteration 14/1000 | Loss: 0.00000901
Iteration 15/1000 | Loss: 0.00000899
Iteration 16/1000 | Loss: 0.00000883
Iteration 17/1000 | Loss: 0.00000879
Iteration 18/1000 | Loss: 0.00000878
Iteration 19/1000 | Loss: 0.00000878
Iteration 20/1000 | Loss: 0.00000876
Iteration 21/1000 | Loss: 0.00000876
Iteration 22/1000 | Loss: 0.00000875
Iteration 23/1000 | Loss: 0.00000874
Iteration 24/1000 | Loss: 0.00000874
Iteration 25/1000 | Loss: 0.00000873
Iteration 26/1000 | Loss: 0.00000873
Iteration 27/1000 | Loss: 0.00000872
Iteration 28/1000 | Loss: 0.00000872
Iteration 29/1000 | Loss: 0.00000868
Iteration 30/1000 | Loss: 0.00000867
Iteration 31/1000 | Loss: 0.00000866
Iteration 32/1000 | Loss: 0.00000866
Iteration 33/1000 | Loss: 0.00000865
Iteration 34/1000 | Loss: 0.00000865
Iteration 35/1000 | Loss: 0.00000862
Iteration 36/1000 | Loss: 0.00000861
Iteration 37/1000 | Loss: 0.00000861
Iteration 38/1000 | Loss: 0.00000860
Iteration 39/1000 | Loss: 0.00000860
Iteration 40/1000 | Loss: 0.00000860
Iteration 41/1000 | Loss: 0.00000854
Iteration 42/1000 | Loss: 0.00000854
Iteration 43/1000 | Loss: 0.00000853
Iteration 44/1000 | Loss: 0.00000853
Iteration 45/1000 | Loss: 0.00000853
Iteration 46/1000 | Loss: 0.00000852
Iteration 47/1000 | Loss: 0.00000849
Iteration 48/1000 | Loss: 0.00000849
Iteration 49/1000 | Loss: 0.00000849
Iteration 50/1000 | Loss: 0.00000848
Iteration 51/1000 | Loss: 0.00000848
Iteration 52/1000 | Loss: 0.00000848
Iteration 53/1000 | Loss: 0.00000848
Iteration 54/1000 | Loss: 0.00000848
Iteration 55/1000 | Loss: 0.00000848
Iteration 56/1000 | Loss: 0.00000848
Iteration 57/1000 | Loss: 0.00000848
Iteration 58/1000 | Loss: 0.00000847
Iteration 59/1000 | Loss: 0.00000847
Iteration 60/1000 | Loss: 0.00000846
Iteration 61/1000 | Loss: 0.00000846
Iteration 62/1000 | Loss: 0.00000846
Iteration 63/1000 | Loss: 0.00000846
Iteration 64/1000 | Loss: 0.00000845
Iteration 65/1000 | Loss: 0.00000845
Iteration 66/1000 | Loss: 0.00000845
Iteration 67/1000 | Loss: 0.00000845
Iteration 68/1000 | Loss: 0.00000845
Iteration 69/1000 | Loss: 0.00000845
Iteration 70/1000 | Loss: 0.00000845
Iteration 71/1000 | Loss: 0.00000845
Iteration 72/1000 | Loss: 0.00000845
Iteration 73/1000 | Loss: 0.00000844
Iteration 74/1000 | Loss: 0.00000844
Iteration 75/1000 | Loss: 0.00000844
Iteration 76/1000 | Loss: 0.00000843
Iteration 77/1000 | Loss: 0.00000843
Iteration 78/1000 | Loss: 0.00000843
Iteration 79/1000 | Loss: 0.00000843
Iteration 80/1000 | Loss: 0.00000843
Iteration 81/1000 | Loss: 0.00000842
Iteration 82/1000 | Loss: 0.00000842
Iteration 83/1000 | Loss: 0.00000842
Iteration 84/1000 | Loss: 0.00000842
Iteration 85/1000 | Loss: 0.00000841
Iteration 86/1000 | Loss: 0.00000841
Iteration 87/1000 | Loss: 0.00000840
Iteration 88/1000 | Loss: 0.00000840
Iteration 89/1000 | Loss: 0.00000840
Iteration 90/1000 | Loss: 0.00000840
Iteration 91/1000 | Loss: 0.00000839
Iteration 92/1000 | Loss: 0.00000839
Iteration 93/1000 | Loss: 0.00000839
Iteration 94/1000 | Loss: 0.00000839
Iteration 95/1000 | Loss: 0.00000839
Iteration 96/1000 | Loss: 0.00000839
Iteration 97/1000 | Loss: 0.00000838
Iteration 98/1000 | Loss: 0.00000838
Iteration 99/1000 | Loss: 0.00000838
Iteration 100/1000 | Loss: 0.00000837
Iteration 101/1000 | Loss: 0.00000837
Iteration 102/1000 | Loss: 0.00000836
Iteration 103/1000 | Loss: 0.00000836
Iteration 104/1000 | Loss: 0.00000836
Iteration 105/1000 | Loss: 0.00000836
Iteration 106/1000 | Loss: 0.00000836
Iteration 107/1000 | Loss: 0.00000835
Iteration 108/1000 | Loss: 0.00000835
Iteration 109/1000 | Loss: 0.00000835
Iteration 110/1000 | Loss: 0.00000835
Iteration 111/1000 | Loss: 0.00000835
Iteration 112/1000 | Loss: 0.00000835
Iteration 113/1000 | Loss: 0.00000835
Iteration 114/1000 | Loss: 0.00000835
Iteration 115/1000 | Loss: 0.00000835
Iteration 116/1000 | Loss: 0.00000834
Iteration 117/1000 | Loss: 0.00000834
Iteration 118/1000 | Loss: 0.00000834
Iteration 119/1000 | Loss: 0.00000833
Iteration 120/1000 | Loss: 0.00000832
Iteration 121/1000 | Loss: 0.00000832
Iteration 122/1000 | Loss: 0.00000832
Iteration 123/1000 | Loss: 0.00000832
Iteration 124/1000 | Loss: 0.00000832
Iteration 125/1000 | Loss: 0.00000832
Iteration 126/1000 | Loss: 0.00000832
Iteration 127/1000 | Loss: 0.00000832
Iteration 128/1000 | Loss: 0.00000832
Iteration 129/1000 | Loss: 0.00000831
Iteration 130/1000 | Loss: 0.00000831
Iteration 131/1000 | Loss: 0.00000831
Iteration 132/1000 | Loss: 0.00000831
Iteration 133/1000 | Loss: 0.00000831
Iteration 134/1000 | Loss: 0.00000830
Iteration 135/1000 | Loss: 0.00000830
Iteration 136/1000 | Loss: 0.00000830
Iteration 137/1000 | Loss: 0.00000830
Iteration 138/1000 | Loss: 0.00000830
Iteration 139/1000 | Loss: 0.00000830
Iteration 140/1000 | Loss: 0.00000830
Iteration 141/1000 | Loss: 0.00000830
Iteration 142/1000 | Loss: 0.00000830
Iteration 143/1000 | Loss: 0.00000829
Iteration 144/1000 | Loss: 0.00000829
Iteration 145/1000 | Loss: 0.00000829
Iteration 146/1000 | Loss: 0.00000828
Iteration 147/1000 | Loss: 0.00000827
Iteration 148/1000 | Loss: 0.00000827
Iteration 149/1000 | Loss: 0.00000827
Iteration 150/1000 | Loss: 0.00000826
Iteration 151/1000 | Loss: 0.00000826
Iteration 152/1000 | Loss: 0.00000826
Iteration 153/1000 | Loss: 0.00000825
Iteration 154/1000 | Loss: 0.00000825
Iteration 155/1000 | Loss: 0.00000825
Iteration 156/1000 | Loss: 0.00000824
Iteration 157/1000 | Loss: 0.00000824
Iteration 158/1000 | Loss: 0.00000823
Iteration 159/1000 | Loss: 0.00000822
Iteration 160/1000 | Loss: 0.00000822
Iteration 161/1000 | Loss: 0.00000822
Iteration 162/1000 | Loss: 0.00000821
Iteration 163/1000 | Loss: 0.00000821
Iteration 164/1000 | Loss: 0.00000821
Iteration 165/1000 | Loss: 0.00000821
Iteration 166/1000 | Loss: 0.00000821
Iteration 167/1000 | Loss: 0.00000821
Iteration 168/1000 | Loss: 0.00000821
Iteration 169/1000 | Loss: 0.00000820
Iteration 170/1000 | Loss: 0.00000820
Iteration 171/1000 | Loss: 0.00000820
Iteration 172/1000 | Loss: 0.00000820
Iteration 173/1000 | Loss: 0.00000820
Iteration 174/1000 | Loss: 0.00000819
Iteration 175/1000 | Loss: 0.00000819
Iteration 176/1000 | Loss: 0.00000818
Iteration 177/1000 | Loss: 0.00000818
Iteration 178/1000 | Loss: 0.00000818
Iteration 179/1000 | Loss: 0.00000818
Iteration 180/1000 | Loss: 0.00000818
Iteration 181/1000 | Loss: 0.00000817
Iteration 182/1000 | Loss: 0.00000817
Iteration 183/1000 | Loss: 0.00000817
Iteration 184/1000 | Loss: 0.00000817
Iteration 185/1000 | Loss: 0.00000817
Iteration 186/1000 | Loss: 0.00000817
Iteration 187/1000 | Loss: 0.00000816
Iteration 188/1000 | Loss: 0.00000816
Iteration 189/1000 | Loss: 0.00000816
Iteration 190/1000 | Loss: 0.00000816
Iteration 191/1000 | Loss: 0.00000816
Iteration 192/1000 | Loss: 0.00000816
Iteration 193/1000 | Loss: 0.00000816
Iteration 194/1000 | Loss: 0.00000816
Iteration 195/1000 | Loss: 0.00000816
Iteration 196/1000 | Loss: 0.00000816
Iteration 197/1000 | Loss: 0.00000816
Iteration 198/1000 | Loss: 0.00000816
Iteration 199/1000 | Loss: 0.00000816
Iteration 200/1000 | Loss: 0.00000816
Iteration 201/1000 | Loss: 0.00000816
Iteration 202/1000 | Loss: 0.00000816
Iteration 203/1000 | Loss: 0.00000816
Iteration 204/1000 | Loss: 0.00000816
Iteration 205/1000 | Loss: 0.00000816
Iteration 206/1000 | Loss: 0.00000816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [8.159155186149292e-06, 8.159155186149292e-06, 8.159155186149292e-06, 8.159155186149292e-06, 8.159155186149292e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.159155186149292e-06

Optimization complete. Final v2v error: 2.4782397747039795 mm

Highest mean error: 2.684799909591675 mm for frame 27

Lowest mean error: 2.3340628147125244 mm for frame 101

Saving results

Total time: 37.03357291221619
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880856
Iteration 2/25 | Loss: 0.00159345
Iteration 3/25 | Loss: 0.00132113
Iteration 4/25 | Loss: 0.00129089
Iteration 5/25 | Loss: 0.00128961
Iteration 6/25 | Loss: 0.00128903
Iteration 7/25 | Loss: 0.00127054
Iteration 8/25 | Loss: 0.00124895
Iteration 9/25 | Loss: 0.00124151
Iteration 10/25 | Loss: 0.00124340
Iteration 11/25 | Loss: 0.00123809
Iteration 12/25 | Loss: 0.00123638
Iteration 13/25 | Loss: 0.00123533
Iteration 14/25 | Loss: 0.00123247
Iteration 15/25 | Loss: 0.00122692
Iteration 16/25 | Loss: 0.00122290
Iteration 17/25 | Loss: 0.00122070
Iteration 18/25 | Loss: 0.00122356
Iteration 19/25 | Loss: 0.00122285
Iteration 20/25 | Loss: 0.00122040
Iteration 21/25 | Loss: 0.00121895
Iteration 22/25 | Loss: 0.00121856
Iteration 23/25 | Loss: 0.00121852
Iteration 24/25 | Loss: 0.00121851
Iteration 25/25 | Loss: 0.00121851

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24774313
Iteration 2/25 | Loss: 0.00115396
Iteration 3/25 | Loss: 0.00115396
Iteration 4/25 | Loss: 0.00115396
Iteration 5/25 | Loss: 0.00115396
Iteration 6/25 | Loss: 0.00115396
Iteration 7/25 | Loss: 0.00115396
Iteration 8/25 | Loss: 0.00115396
Iteration 9/25 | Loss: 0.00115396
Iteration 10/25 | Loss: 0.00115396
Iteration 11/25 | Loss: 0.00115396
Iteration 12/25 | Loss: 0.00115396
Iteration 13/25 | Loss: 0.00115396
Iteration 14/25 | Loss: 0.00115396
Iteration 15/25 | Loss: 0.00115396
Iteration 16/25 | Loss: 0.00115396
Iteration 17/25 | Loss: 0.00115396
Iteration 18/25 | Loss: 0.00115396
Iteration 19/25 | Loss: 0.00115396
Iteration 20/25 | Loss: 0.00115396
Iteration 21/25 | Loss: 0.00115396
Iteration 22/25 | Loss: 0.00115396
Iteration 23/25 | Loss: 0.00115396
Iteration 24/25 | Loss: 0.00115396
Iteration 25/25 | Loss: 0.00115396
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011539598926901817, 0.0011539598926901817, 0.0011539598926901817, 0.0011539598926901817, 0.0011539598926901817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011539598926901817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115396
Iteration 2/1000 | Loss: 0.00014402
Iteration 3/1000 | Loss: 0.00028124
Iteration 4/1000 | Loss: 0.00003948
Iteration 5/1000 | Loss: 0.00002909
Iteration 6/1000 | Loss: 0.00002594
Iteration 7/1000 | Loss: 0.00002477
Iteration 8/1000 | Loss: 0.00002391
Iteration 9/1000 | Loss: 0.00002301
Iteration 10/1000 | Loss: 0.00002242
Iteration 11/1000 | Loss: 0.00002199
Iteration 12/1000 | Loss: 0.00002172
Iteration 13/1000 | Loss: 0.00002144
Iteration 14/1000 | Loss: 0.00002121
Iteration 15/1000 | Loss: 0.00002106
Iteration 16/1000 | Loss: 0.00002083
Iteration 17/1000 | Loss: 0.00002080
Iteration 18/1000 | Loss: 0.00002071
Iteration 19/1000 | Loss: 0.00002069
Iteration 20/1000 | Loss: 0.00002068
Iteration 21/1000 | Loss: 0.00002067
Iteration 22/1000 | Loss: 0.00002064
Iteration 23/1000 | Loss: 0.00002064
Iteration 24/1000 | Loss: 0.00002060
Iteration 25/1000 | Loss: 0.00002059
Iteration 26/1000 | Loss: 0.00002059
Iteration 27/1000 | Loss: 0.00002058
Iteration 28/1000 | Loss: 0.00002056
Iteration 29/1000 | Loss: 0.00002055
Iteration 30/1000 | Loss: 0.00002054
Iteration 31/1000 | Loss: 0.00002053
Iteration 32/1000 | Loss: 0.00002052
Iteration 33/1000 | Loss: 0.00002051
Iteration 34/1000 | Loss: 0.00002050
Iteration 35/1000 | Loss: 0.00002050
Iteration 36/1000 | Loss: 0.00002050
Iteration 37/1000 | Loss: 0.00002049
Iteration 38/1000 | Loss: 0.00002046
Iteration 39/1000 | Loss: 0.00002044
Iteration 40/1000 | Loss: 0.00002044
Iteration 41/1000 | Loss: 0.00002043
Iteration 42/1000 | Loss: 0.00002042
Iteration 43/1000 | Loss: 0.00002042
Iteration 44/1000 | Loss: 0.00002041
Iteration 45/1000 | Loss: 0.00002041
Iteration 46/1000 | Loss: 0.00002041
Iteration 47/1000 | Loss: 0.00002041
Iteration 48/1000 | Loss: 0.00002041
Iteration 49/1000 | Loss: 0.00002041
Iteration 50/1000 | Loss: 0.00002040
Iteration 51/1000 | Loss: 0.00002040
Iteration 52/1000 | Loss: 0.00002039
Iteration 53/1000 | Loss: 0.00002039
Iteration 54/1000 | Loss: 0.00002039
Iteration 55/1000 | Loss: 0.00002039
Iteration 56/1000 | Loss: 0.00002039
Iteration 57/1000 | Loss: 0.00002039
Iteration 58/1000 | Loss: 0.00002038
Iteration 59/1000 | Loss: 0.00002038
Iteration 60/1000 | Loss: 0.00002038
Iteration 61/1000 | Loss: 0.00002038
Iteration 62/1000 | Loss: 0.00002037
Iteration 63/1000 | Loss: 0.00002037
Iteration 64/1000 | Loss: 0.00002037
Iteration 65/1000 | Loss: 0.00002036
Iteration 66/1000 | Loss: 0.00002036
Iteration 67/1000 | Loss: 0.00002036
Iteration 68/1000 | Loss: 0.00002036
Iteration 69/1000 | Loss: 0.00002036
Iteration 70/1000 | Loss: 0.00002035
Iteration 71/1000 | Loss: 0.00002035
Iteration 72/1000 | Loss: 0.00002035
Iteration 73/1000 | Loss: 0.00002035
Iteration 74/1000 | Loss: 0.00002034
Iteration 75/1000 | Loss: 0.00002034
Iteration 76/1000 | Loss: 0.00002034
Iteration 77/1000 | Loss: 0.00002033
Iteration 78/1000 | Loss: 0.00002033
Iteration 79/1000 | Loss: 0.00002033
Iteration 80/1000 | Loss: 0.00002033
Iteration 81/1000 | Loss: 0.00002032
Iteration 82/1000 | Loss: 0.00002032
Iteration 83/1000 | Loss: 0.00002032
Iteration 84/1000 | Loss: 0.00002032
Iteration 85/1000 | Loss: 0.00002032
Iteration 86/1000 | Loss: 0.00002032
Iteration 87/1000 | Loss: 0.00002032
Iteration 88/1000 | Loss: 0.00002032
Iteration 89/1000 | Loss: 0.00002032
Iteration 90/1000 | Loss: 0.00002031
Iteration 91/1000 | Loss: 0.00002031
Iteration 92/1000 | Loss: 0.00002031
Iteration 93/1000 | Loss: 0.00002030
Iteration 94/1000 | Loss: 0.00002030
Iteration 95/1000 | Loss: 0.00002030
Iteration 96/1000 | Loss: 0.00002029
Iteration 97/1000 | Loss: 0.00002029
Iteration 98/1000 | Loss: 0.00002029
Iteration 99/1000 | Loss: 0.00002029
Iteration 100/1000 | Loss: 0.00002029
Iteration 101/1000 | Loss: 0.00002029
Iteration 102/1000 | Loss: 0.00002028
Iteration 103/1000 | Loss: 0.00002028
Iteration 104/1000 | Loss: 0.00002028
Iteration 105/1000 | Loss: 0.00002028
Iteration 106/1000 | Loss: 0.00002028
Iteration 107/1000 | Loss: 0.00002027
Iteration 108/1000 | Loss: 0.00002027
Iteration 109/1000 | Loss: 0.00002027
Iteration 110/1000 | Loss: 0.00002027
Iteration 111/1000 | Loss: 0.00002026
Iteration 112/1000 | Loss: 0.00002026
Iteration 113/1000 | Loss: 0.00002026
Iteration 114/1000 | Loss: 0.00002025
Iteration 115/1000 | Loss: 0.00002024
Iteration 116/1000 | Loss: 0.00002024
Iteration 117/1000 | Loss: 0.00002024
Iteration 118/1000 | Loss: 0.00002023
Iteration 119/1000 | Loss: 0.00002023
Iteration 120/1000 | Loss: 0.00002023
Iteration 121/1000 | Loss: 0.00002022
Iteration 122/1000 | Loss: 0.00002022
Iteration 123/1000 | Loss: 0.00002022
Iteration 124/1000 | Loss: 0.00002022
Iteration 125/1000 | Loss: 0.00002022
Iteration 126/1000 | Loss: 0.00002022
Iteration 127/1000 | Loss: 0.00002022
Iteration 128/1000 | Loss: 0.00002022
Iteration 129/1000 | Loss: 0.00002022
Iteration 130/1000 | Loss: 0.00002022
Iteration 131/1000 | Loss: 0.00002022
Iteration 132/1000 | Loss: 0.00002022
Iteration 133/1000 | Loss: 0.00002021
Iteration 134/1000 | Loss: 0.00002021
Iteration 135/1000 | Loss: 0.00002021
Iteration 136/1000 | Loss: 0.00002021
Iteration 137/1000 | Loss: 0.00002021
Iteration 138/1000 | Loss: 0.00002020
Iteration 139/1000 | Loss: 0.00002020
Iteration 140/1000 | Loss: 0.00002020
Iteration 141/1000 | Loss: 0.00002020
Iteration 142/1000 | Loss: 0.00002020
Iteration 143/1000 | Loss: 0.00002020
Iteration 144/1000 | Loss: 0.00002020
Iteration 145/1000 | Loss: 0.00002020
Iteration 146/1000 | Loss: 0.00002020
Iteration 147/1000 | Loss: 0.00002020
Iteration 148/1000 | Loss: 0.00002020
Iteration 149/1000 | Loss: 0.00002019
Iteration 150/1000 | Loss: 0.00002019
Iteration 151/1000 | Loss: 0.00002019
Iteration 152/1000 | Loss: 0.00002019
Iteration 153/1000 | Loss: 0.00002019
Iteration 154/1000 | Loss: 0.00002019
Iteration 155/1000 | Loss: 0.00002018
Iteration 156/1000 | Loss: 0.00002018
Iteration 157/1000 | Loss: 0.00002018
Iteration 158/1000 | Loss: 0.00002018
Iteration 159/1000 | Loss: 0.00002018
Iteration 160/1000 | Loss: 0.00002018
Iteration 161/1000 | Loss: 0.00002018
Iteration 162/1000 | Loss: 0.00002018
Iteration 163/1000 | Loss: 0.00002018
Iteration 164/1000 | Loss: 0.00002018
Iteration 165/1000 | Loss: 0.00002018
Iteration 166/1000 | Loss: 0.00002018
Iteration 167/1000 | Loss: 0.00002018
Iteration 168/1000 | Loss: 0.00002017
Iteration 169/1000 | Loss: 0.00002017
Iteration 170/1000 | Loss: 0.00002017
Iteration 171/1000 | Loss: 0.00002017
Iteration 172/1000 | Loss: 0.00002017
Iteration 173/1000 | Loss: 0.00002017
Iteration 174/1000 | Loss: 0.00002017
Iteration 175/1000 | Loss: 0.00002017
Iteration 176/1000 | Loss: 0.00002017
Iteration 177/1000 | Loss: 0.00002017
Iteration 178/1000 | Loss: 0.00002017
Iteration 179/1000 | Loss: 0.00002017
Iteration 180/1000 | Loss: 0.00002017
Iteration 181/1000 | Loss: 0.00002017
Iteration 182/1000 | Loss: 0.00002017
Iteration 183/1000 | Loss: 0.00002017
Iteration 184/1000 | Loss: 0.00002017
Iteration 185/1000 | Loss: 0.00002016
Iteration 186/1000 | Loss: 0.00002016
Iteration 187/1000 | Loss: 0.00002016
Iteration 188/1000 | Loss: 0.00002016
Iteration 189/1000 | Loss: 0.00002016
Iteration 190/1000 | Loss: 0.00002016
Iteration 191/1000 | Loss: 0.00002016
Iteration 192/1000 | Loss: 0.00002016
Iteration 193/1000 | Loss: 0.00002016
Iteration 194/1000 | Loss: 0.00002016
Iteration 195/1000 | Loss: 0.00002016
Iteration 196/1000 | Loss: 0.00002016
Iteration 197/1000 | Loss: 0.00002016
Iteration 198/1000 | Loss: 0.00002016
Iteration 199/1000 | Loss: 0.00002016
Iteration 200/1000 | Loss: 0.00002016
Iteration 201/1000 | Loss: 0.00002016
Iteration 202/1000 | Loss: 0.00002016
Iteration 203/1000 | Loss: 0.00002016
Iteration 204/1000 | Loss: 0.00002016
Iteration 205/1000 | Loss: 0.00002016
Iteration 206/1000 | Loss: 0.00002016
Iteration 207/1000 | Loss: 0.00002016
Iteration 208/1000 | Loss: 0.00002016
Iteration 209/1000 | Loss: 0.00002016
Iteration 210/1000 | Loss: 0.00002016
Iteration 211/1000 | Loss: 0.00002016
Iteration 212/1000 | Loss: 0.00002016
Iteration 213/1000 | Loss: 0.00002016
Iteration 214/1000 | Loss: 0.00002016
Iteration 215/1000 | Loss: 0.00002016
Iteration 216/1000 | Loss: 0.00002016
Iteration 217/1000 | Loss: 0.00002016
Iteration 218/1000 | Loss: 0.00002016
Iteration 219/1000 | Loss: 0.00002016
Iteration 220/1000 | Loss: 0.00002016
Iteration 221/1000 | Loss: 0.00002016
Iteration 222/1000 | Loss: 0.00002016
Iteration 223/1000 | Loss: 0.00002016
Iteration 224/1000 | Loss: 0.00002016
Iteration 225/1000 | Loss: 0.00002016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.016306643781718e-05, 2.016306643781718e-05, 2.016306643781718e-05, 2.016306643781718e-05, 2.016306643781718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.016306643781718e-05

Optimization complete. Final v2v error: 3.739983320236206 mm

Highest mean error: 5.7191901206970215 mm for frame 106

Lowest mean error: 3.1981770992279053 mm for frame 70

Saving results

Total time: 83.69824433326721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379503
Iteration 2/25 | Loss: 0.00154318
Iteration 3/25 | Loss: 0.00119948
Iteration 4/25 | Loss: 0.00115438
Iteration 5/25 | Loss: 0.00114346
Iteration 6/25 | Loss: 0.00114105
Iteration 7/25 | Loss: 0.00114054
Iteration 8/25 | Loss: 0.00114054
Iteration 9/25 | Loss: 0.00114054
Iteration 10/25 | Loss: 0.00114054
Iteration 11/25 | Loss: 0.00114054
Iteration 12/25 | Loss: 0.00114054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011405374389141798, 0.0011405374389141798, 0.0011405374389141798, 0.0011405374389141798, 0.0011405374389141798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011405374389141798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32439077
Iteration 2/25 | Loss: 0.00060972
Iteration 3/25 | Loss: 0.00060972
Iteration 4/25 | Loss: 0.00060971
Iteration 5/25 | Loss: 0.00060971
Iteration 6/25 | Loss: 0.00060971
Iteration 7/25 | Loss: 0.00060971
Iteration 8/25 | Loss: 0.00060971
Iteration 9/25 | Loss: 0.00060971
Iteration 10/25 | Loss: 0.00060971
Iteration 11/25 | Loss: 0.00060971
Iteration 12/25 | Loss: 0.00060971
Iteration 13/25 | Loss: 0.00060971
Iteration 14/25 | Loss: 0.00060971
Iteration 15/25 | Loss: 0.00060971
Iteration 16/25 | Loss: 0.00060971
Iteration 17/25 | Loss: 0.00060971
Iteration 18/25 | Loss: 0.00060971
Iteration 19/25 | Loss: 0.00060971
Iteration 20/25 | Loss: 0.00060971
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006097131408751011, 0.0006097131408751011, 0.0006097131408751011, 0.0006097131408751011, 0.0006097131408751011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006097131408751011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060971
Iteration 2/1000 | Loss: 0.00004578
Iteration 3/1000 | Loss: 0.00002663
Iteration 4/1000 | Loss: 0.00001974
Iteration 5/1000 | Loss: 0.00001697
Iteration 6/1000 | Loss: 0.00001571
Iteration 7/1000 | Loss: 0.00001501
Iteration 8/1000 | Loss: 0.00001436
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001374
Iteration 11/1000 | Loss: 0.00001348
Iteration 12/1000 | Loss: 0.00001331
Iteration 13/1000 | Loss: 0.00001328
Iteration 14/1000 | Loss: 0.00001313
Iteration 15/1000 | Loss: 0.00001310
Iteration 16/1000 | Loss: 0.00001310
Iteration 17/1000 | Loss: 0.00001309
Iteration 18/1000 | Loss: 0.00001307
Iteration 19/1000 | Loss: 0.00001306
Iteration 20/1000 | Loss: 0.00001302
Iteration 21/1000 | Loss: 0.00001301
Iteration 22/1000 | Loss: 0.00001299
Iteration 23/1000 | Loss: 0.00001299
Iteration 24/1000 | Loss: 0.00001298
Iteration 25/1000 | Loss: 0.00001298
Iteration 26/1000 | Loss: 0.00001296
Iteration 27/1000 | Loss: 0.00001296
Iteration 28/1000 | Loss: 0.00001294
Iteration 29/1000 | Loss: 0.00001294
Iteration 30/1000 | Loss: 0.00001294
Iteration 31/1000 | Loss: 0.00001294
Iteration 32/1000 | Loss: 0.00001294
Iteration 33/1000 | Loss: 0.00001294
Iteration 34/1000 | Loss: 0.00001294
Iteration 35/1000 | Loss: 0.00001293
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001293
Iteration 38/1000 | Loss: 0.00001293
Iteration 39/1000 | Loss: 0.00001292
Iteration 40/1000 | Loss: 0.00001292
Iteration 41/1000 | Loss: 0.00001292
Iteration 42/1000 | Loss: 0.00001292
Iteration 43/1000 | Loss: 0.00001292
Iteration 44/1000 | Loss: 0.00001292
Iteration 45/1000 | Loss: 0.00001292
Iteration 46/1000 | Loss: 0.00001292
Iteration 47/1000 | Loss: 0.00001292
Iteration 48/1000 | Loss: 0.00001291
Iteration 49/1000 | Loss: 0.00001291
Iteration 50/1000 | Loss: 0.00001291
Iteration 51/1000 | Loss: 0.00001291
Iteration 52/1000 | Loss: 0.00001291
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001290
Iteration 56/1000 | Loss: 0.00001290
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001289
Iteration 59/1000 | Loss: 0.00001289
Iteration 60/1000 | Loss: 0.00001288
Iteration 61/1000 | Loss: 0.00001287
Iteration 62/1000 | Loss: 0.00001287
Iteration 63/1000 | Loss: 0.00001287
Iteration 64/1000 | Loss: 0.00001287
Iteration 65/1000 | Loss: 0.00001287
Iteration 66/1000 | Loss: 0.00001287
Iteration 67/1000 | Loss: 0.00001287
Iteration 68/1000 | Loss: 0.00001287
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001286
Iteration 71/1000 | Loss: 0.00001286
Iteration 72/1000 | Loss: 0.00001286
Iteration 73/1000 | Loss: 0.00001285
Iteration 74/1000 | Loss: 0.00001285
Iteration 75/1000 | Loss: 0.00001285
Iteration 76/1000 | Loss: 0.00001285
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001285
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001284
Iteration 92/1000 | Loss: 0.00001284
Iteration 93/1000 | Loss: 0.00001284
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001283
Iteration 96/1000 | Loss: 0.00001283
Iteration 97/1000 | Loss: 0.00001283
Iteration 98/1000 | Loss: 0.00001283
Iteration 99/1000 | Loss: 0.00001283
Iteration 100/1000 | Loss: 0.00001283
Iteration 101/1000 | Loss: 0.00001283
Iteration 102/1000 | Loss: 0.00001283
Iteration 103/1000 | Loss: 0.00001283
Iteration 104/1000 | Loss: 0.00001283
Iteration 105/1000 | Loss: 0.00001283
Iteration 106/1000 | Loss: 0.00001283
Iteration 107/1000 | Loss: 0.00001283
Iteration 108/1000 | Loss: 0.00001283
Iteration 109/1000 | Loss: 0.00001283
Iteration 110/1000 | Loss: 0.00001283
Iteration 111/1000 | Loss: 0.00001283
Iteration 112/1000 | Loss: 0.00001282
Iteration 113/1000 | Loss: 0.00001282
Iteration 114/1000 | Loss: 0.00001282
Iteration 115/1000 | Loss: 0.00001282
Iteration 116/1000 | Loss: 0.00001282
Iteration 117/1000 | Loss: 0.00001282
Iteration 118/1000 | Loss: 0.00001282
Iteration 119/1000 | Loss: 0.00001282
Iteration 120/1000 | Loss: 0.00001282
Iteration 121/1000 | Loss: 0.00001282
Iteration 122/1000 | Loss: 0.00001282
Iteration 123/1000 | Loss: 0.00001282
Iteration 124/1000 | Loss: 0.00001282
Iteration 125/1000 | Loss: 0.00001282
Iteration 126/1000 | Loss: 0.00001282
Iteration 127/1000 | Loss: 0.00001282
Iteration 128/1000 | Loss: 0.00001282
Iteration 129/1000 | Loss: 0.00001282
Iteration 130/1000 | Loss: 0.00001282
Iteration 131/1000 | Loss: 0.00001282
Iteration 132/1000 | Loss: 0.00001282
Iteration 133/1000 | Loss: 0.00001281
Iteration 134/1000 | Loss: 0.00001281
Iteration 135/1000 | Loss: 0.00001281
Iteration 136/1000 | Loss: 0.00001281
Iteration 137/1000 | Loss: 0.00001281
Iteration 138/1000 | Loss: 0.00001281
Iteration 139/1000 | Loss: 0.00001281
Iteration 140/1000 | Loss: 0.00001281
Iteration 141/1000 | Loss: 0.00001281
Iteration 142/1000 | Loss: 0.00001281
Iteration 143/1000 | Loss: 0.00001281
Iteration 144/1000 | Loss: 0.00001281
Iteration 145/1000 | Loss: 0.00001281
Iteration 146/1000 | Loss: 0.00001281
Iteration 147/1000 | Loss: 0.00001281
Iteration 148/1000 | Loss: 0.00001281
Iteration 149/1000 | Loss: 0.00001280
Iteration 150/1000 | Loss: 0.00001280
Iteration 151/1000 | Loss: 0.00001280
Iteration 152/1000 | Loss: 0.00001280
Iteration 153/1000 | Loss: 0.00001280
Iteration 154/1000 | Loss: 0.00001280
Iteration 155/1000 | Loss: 0.00001280
Iteration 156/1000 | Loss: 0.00001280
Iteration 157/1000 | Loss: 0.00001280
Iteration 158/1000 | Loss: 0.00001280
Iteration 159/1000 | Loss: 0.00001280
Iteration 160/1000 | Loss: 0.00001280
Iteration 161/1000 | Loss: 0.00001280
Iteration 162/1000 | Loss: 0.00001280
Iteration 163/1000 | Loss: 0.00001280
Iteration 164/1000 | Loss: 0.00001279
Iteration 165/1000 | Loss: 0.00001279
Iteration 166/1000 | Loss: 0.00001279
Iteration 167/1000 | Loss: 0.00001279
Iteration 168/1000 | Loss: 0.00001279
Iteration 169/1000 | Loss: 0.00001279
Iteration 170/1000 | Loss: 0.00001279
Iteration 171/1000 | Loss: 0.00001279
Iteration 172/1000 | Loss: 0.00001279
Iteration 173/1000 | Loss: 0.00001279
Iteration 174/1000 | Loss: 0.00001279
Iteration 175/1000 | Loss: 0.00001279
Iteration 176/1000 | Loss: 0.00001279
Iteration 177/1000 | Loss: 0.00001279
Iteration 178/1000 | Loss: 0.00001279
Iteration 179/1000 | Loss: 0.00001279
Iteration 180/1000 | Loss: 0.00001279
Iteration 181/1000 | Loss: 0.00001279
Iteration 182/1000 | Loss: 0.00001279
Iteration 183/1000 | Loss: 0.00001279
Iteration 184/1000 | Loss: 0.00001279
Iteration 185/1000 | Loss: 0.00001279
Iteration 186/1000 | Loss: 0.00001279
Iteration 187/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.2794356734957546e-05, 1.2794356734957546e-05, 1.2794356734957546e-05, 1.2794356734957546e-05, 1.2794356734957546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2794356734957546e-05

Optimization complete. Final v2v error: 3.0369553565979004 mm

Highest mean error: 3.245340347290039 mm for frame 103

Lowest mean error: 2.883124828338623 mm for frame 25

Saving results

Total time: 39.57276940345764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430058
Iteration 2/25 | Loss: 0.00124097
Iteration 3/25 | Loss: 0.00116486
Iteration 4/25 | Loss: 0.00115580
Iteration 5/25 | Loss: 0.00115300
Iteration 6/25 | Loss: 0.00115300
Iteration 7/25 | Loss: 0.00115300
Iteration 8/25 | Loss: 0.00115300
Iteration 9/25 | Loss: 0.00115300
Iteration 10/25 | Loss: 0.00115300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011529980693012476, 0.0011529980693012476, 0.0011529980693012476, 0.0011529980693012476, 0.0011529980693012476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011529980693012476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39960825
Iteration 2/25 | Loss: 0.00073463
Iteration 3/25 | Loss: 0.00073462
Iteration 4/25 | Loss: 0.00073462
Iteration 5/25 | Loss: 0.00073462
Iteration 6/25 | Loss: 0.00073462
Iteration 7/25 | Loss: 0.00073462
Iteration 8/25 | Loss: 0.00073462
Iteration 9/25 | Loss: 0.00073462
Iteration 10/25 | Loss: 0.00073462
Iteration 11/25 | Loss: 0.00073462
Iteration 12/25 | Loss: 0.00073462
Iteration 13/25 | Loss: 0.00073462
Iteration 14/25 | Loss: 0.00073462
Iteration 15/25 | Loss: 0.00073462
Iteration 16/25 | Loss: 0.00073462
Iteration 17/25 | Loss: 0.00073462
Iteration 18/25 | Loss: 0.00073462
Iteration 19/25 | Loss: 0.00073462
Iteration 20/25 | Loss: 0.00073462
Iteration 21/25 | Loss: 0.00073462
Iteration 22/25 | Loss: 0.00073462
Iteration 23/25 | Loss: 0.00073462
Iteration 24/25 | Loss: 0.00073462
Iteration 25/25 | Loss: 0.00073462

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073462
Iteration 2/1000 | Loss: 0.00002954
Iteration 3/1000 | Loss: 0.00002160
Iteration 4/1000 | Loss: 0.00001918
Iteration 5/1000 | Loss: 0.00001774
Iteration 6/1000 | Loss: 0.00001689
Iteration 7/1000 | Loss: 0.00001626
Iteration 8/1000 | Loss: 0.00001579
Iteration 9/1000 | Loss: 0.00001555
Iteration 10/1000 | Loss: 0.00001521
Iteration 11/1000 | Loss: 0.00001513
Iteration 12/1000 | Loss: 0.00001494
Iteration 13/1000 | Loss: 0.00001492
Iteration 14/1000 | Loss: 0.00001490
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001479
Iteration 17/1000 | Loss: 0.00001477
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001469
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001453
Iteration 22/1000 | Loss: 0.00001451
Iteration 23/1000 | Loss: 0.00001450
Iteration 24/1000 | Loss: 0.00001446
Iteration 25/1000 | Loss: 0.00001445
Iteration 26/1000 | Loss: 0.00001445
Iteration 27/1000 | Loss: 0.00001444
Iteration 28/1000 | Loss: 0.00001444
Iteration 29/1000 | Loss: 0.00001444
Iteration 30/1000 | Loss: 0.00001444
Iteration 31/1000 | Loss: 0.00001442
Iteration 32/1000 | Loss: 0.00001441
Iteration 33/1000 | Loss: 0.00001437
Iteration 34/1000 | Loss: 0.00001437
Iteration 35/1000 | Loss: 0.00001436
Iteration 36/1000 | Loss: 0.00001436
Iteration 37/1000 | Loss: 0.00001432
Iteration 38/1000 | Loss: 0.00001429
Iteration 39/1000 | Loss: 0.00001429
Iteration 40/1000 | Loss: 0.00001428
Iteration 41/1000 | Loss: 0.00001428
Iteration 42/1000 | Loss: 0.00001426
Iteration 43/1000 | Loss: 0.00001426
Iteration 44/1000 | Loss: 0.00001425
Iteration 45/1000 | Loss: 0.00001425
Iteration 46/1000 | Loss: 0.00001425
Iteration 47/1000 | Loss: 0.00001424
Iteration 48/1000 | Loss: 0.00001423
Iteration 49/1000 | Loss: 0.00001420
Iteration 50/1000 | Loss: 0.00001419
Iteration 51/1000 | Loss: 0.00001418
Iteration 52/1000 | Loss: 0.00001418
Iteration 53/1000 | Loss: 0.00001416
Iteration 54/1000 | Loss: 0.00001415
Iteration 55/1000 | Loss: 0.00001415
Iteration 56/1000 | Loss: 0.00001413
Iteration 57/1000 | Loss: 0.00001413
Iteration 58/1000 | Loss: 0.00001413
Iteration 59/1000 | Loss: 0.00001413
Iteration 60/1000 | Loss: 0.00001413
Iteration 61/1000 | Loss: 0.00001413
Iteration 62/1000 | Loss: 0.00001412
Iteration 63/1000 | Loss: 0.00001412
Iteration 64/1000 | Loss: 0.00001412
Iteration 65/1000 | Loss: 0.00001411
Iteration 66/1000 | Loss: 0.00001411
Iteration 67/1000 | Loss: 0.00001411
Iteration 68/1000 | Loss: 0.00001409
Iteration 69/1000 | Loss: 0.00001409
Iteration 70/1000 | Loss: 0.00001409
Iteration 71/1000 | Loss: 0.00001409
Iteration 72/1000 | Loss: 0.00001408
Iteration 73/1000 | Loss: 0.00001408
Iteration 74/1000 | Loss: 0.00001408
Iteration 75/1000 | Loss: 0.00001408
Iteration 76/1000 | Loss: 0.00001408
Iteration 77/1000 | Loss: 0.00001407
Iteration 78/1000 | Loss: 0.00001407
Iteration 79/1000 | Loss: 0.00001407
Iteration 80/1000 | Loss: 0.00001406
Iteration 81/1000 | Loss: 0.00001406
Iteration 82/1000 | Loss: 0.00001405
Iteration 83/1000 | Loss: 0.00001405
Iteration 84/1000 | Loss: 0.00001404
Iteration 85/1000 | Loss: 0.00001404
Iteration 86/1000 | Loss: 0.00001403
Iteration 87/1000 | Loss: 0.00001403
Iteration 88/1000 | Loss: 0.00001403
Iteration 89/1000 | Loss: 0.00001402
Iteration 90/1000 | Loss: 0.00001402
Iteration 91/1000 | Loss: 0.00001401
Iteration 92/1000 | Loss: 0.00001401
Iteration 93/1000 | Loss: 0.00001401
Iteration 94/1000 | Loss: 0.00001400
Iteration 95/1000 | Loss: 0.00001400
Iteration 96/1000 | Loss: 0.00001400
Iteration 97/1000 | Loss: 0.00001399
Iteration 98/1000 | Loss: 0.00001399
Iteration 99/1000 | Loss: 0.00001399
Iteration 100/1000 | Loss: 0.00001399
Iteration 101/1000 | Loss: 0.00001399
Iteration 102/1000 | Loss: 0.00001399
Iteration 103/1000 | Loss: 0.00001398
Iteration 104/1000 | Loss: 0.00001397
Iteration 105/1000 | Loss: 0.00001397
Iteration 106/1000 | Loss: 0.00001397
Iteration 107/1000 | Loss: 0.00001396
Iteration 108/1000 | Loss: 0.00001396
Iteration 109/1000 | Loss: 0.00001396
Iteration 110/1000 | Loss: 0.00001396
Iteration 111/1000 | Loss: 0.00001396
Iteration 112/1000 | Loss: 0.00001395
Iteration 113/1000 | Loss: 0.00001395
Iteration 114/1000 | Loss: 0.00001395
Iteration 115/1000 | Loss: 0.00001395
Iteration 116/1000 | Loss: 0.00001395
Iteration 117/1000 | Loss: 0.00001395
Iteration 118/1000 | Loss: 0.00001395
Iteration 119/1000 | Loss: 0.00001395
Iteration 120/1000 | Loss: 0.00001395
Iteration 121/1000 | Loss: 0.00001395
Iteration 122/1000 | Loss: 0.00001394
Iteration 123/1000 | Loss: 0.00001394
Iteration 124/1000 | Loss: 0.00001393
Iteration 125/1000 | Loss: 0.00001393
Iteration 126/1000 | Loss: 0.00001393
Iteration 127/1000 | Loss: 0.00001393
Iteration 128/1000 | Loss: 0.00001393
Iteration 129/1000 | Loss: 0.00001392
Iteration 130/1000 | Loss: 0.00001392
Iteration 131/1000 | Loss: 0.00001392
Iteration 132/1000 | Loss: 0.00001392
Iteration 133/1000 | Loss: 0.00001392
Iteration 134/1000 | Loss: 0.00001392
Iteration 135/1000 | Loss: 0.00001392
Iteration 136/1000 | Loss: 0.00001392
Iteration 137/1000 | Loss: 0.00001391
Iteration 138/1000 | Loss: 0.00001391
Iteration 139/1000 | Loss: 0.00001391
Iteration 140/1000 | Loss: 0.00001391
Iteration 141/1000 | Loss: 0.00001391
Iteration 142/1000 | Loss: 0.00001391
Iteration 143/1000 | Loss: 0.00001391
Iteration 144/1000 | Loss: 0.00001391
Iteration 145/1000 | Loss: 0.00001391
Iteration 146/1000 | Loss: 0.00001391
Iteration 147/1000 | Loss: 0.00001391
Iteration 148/1000 | Loss: 0.00001391
Iteration 149/1000 | Loss: 0.00001391
Iteration 150/1000 | Loss: 0.00001391
Iteration 151/1000 | Loss: 0.00001391
Iteration 152/1000 | Loss: 0.00001390
Iteration 153/1000 | Loss: 0.00001390
Iteration 154/1000 | Loss: 0.00001390
Iteration 155/1000 | Loss: 0.00001390
Iteration 156/1000 | Loss: 0.00001390
Iteration 157/1000 | Loss: 0.00001390
Iteration 158/1000 | Loss: 0.00001390
Iteration 159/1000 | Loss: 0.00001390
Iteration 160/1000 | Loss: 0.00001390
Iteration 161/1000 | Loss: 0.00001390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.3902313185099047e-05, 1.3902313185099047e-05, 1.3902313185099047e-05, 1.3902313185099047e-05, 1.3902313185099047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3902313185099047e-05

Optimization complete. Final v2v error: 3.1880455017089844 mm

Highest mean error: 3.5335259437561035 mm for frame 71

Lowest mean error: 2.881335973739624 mm for frame 85

Saving results

Total time: 46.67797899246216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767760
Iteration 2/25 | Loss: 0.00156730
Iteration 3/25 | Loss: 0.00138476
Iteration 4/25 | Loss: 0.00135531
Iteration 5/25 | Loss: 0.00134583
Iteration 6/25 | Loss: 0.00134342
Iteration 7/25 | Loss: 0.00134297
Iteration 8/25 | Loss: 0.00134297
Iteration 9/25 | Loss: 0.00134297
Iteration 10/25 | Loss: 0.00134297
Iteration 11/25 | Loss: 0.00134297
Iteration 12/25 | Loss: 0.00134297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013429675018414855, 0.0013429675018414855, 0.0013429675018414855, 0.0013429675018414855, 0.0013429675018414855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013429675018414855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.85662460
Iteration 2/25 | Loss: 0.00102965
Iteration 3/25 | Loss: 0.00102932
Iteration 4/25 | Loss: 0.00102932
Iteration 5/25 | Loss: 0.00102932
Iteration 6/25 | Loss: 0.00102932
Iteration 7/25 | Loss: 0.00102932
Iteration 8/25 | Loss: 0.00102932
Iteration 9/25 | Loss: 0.00102932
Iteration 10/25 | Loss: 0.00102932
Iteration 11/25 | Loss: 0.00102932
Iteration 12/25 | Loss: 0.00102932
Iteration 13/25 | Loss: 0.00102932
Iteration 14/25 | Loss: 0.00102932
Iteration 15/25 | Loss: 0.00102932
Iteration 16/25 | Loss: 0.00102932
Iteration 17/25 | Loss: 0.00102932
Iteration 18/25 | Loss: 0.00102932
Iteration 19/25 | Loss: 0.00102932
Iteration 20/25 | Loss: 0.00102932
Iteration 21/25 | Loss: 0.00102932
Iteration 22/25 | Loss: 0.00102932
Iteration 23/25 | Loss: 0.00102932
Iteration 24/25 | Loss: 0.00102932
Iteration 25/25 | Loss: 0.00102932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001029317150823772, 0.001029317150823772, 0.001029317150823772, 0.001029317150823772, 0.001029317150823772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001029317150823772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102932
Iteration 2/1000 | Loss: 0.00009101
Iteration 3/1000 | Loss: 0.00005256
Iteration 4/1000 | Loss: 0.00004450
Iteration 5/1000 | Loss: 0.00004148
Iteration 6/1000 | Loss: 0.00004022
Iteration 7/1000 | Loss: 0.00003875
Iteration 8/1000 | Loss: 0.00003789
Iteration 9/1000 | Loss: 0.00003725
Iteration 10/1000 | Loss: 0.00003684
Iteration 11/1000 | Loss: 0.00003641
Iteration 12/1000 | Loss: 0.00003615
Iteration 13/1000 | Loss: 0.00003589
Iteration 14/1000 | Loss: 0.00003568
Iteration 15/1000 | Loss: 0.00003547
Iteration 16/1000 | Loss: 0.00003529
Iteration 17/1000 | Loss: 0.00003524
Iteration 18/1000 | Loss: 0.00003523
Iteration 19/1000 | Loss: 0.00003515
Iteration 20/1000 | Loss: 0.00003505
Iteration 21/1000 | Loss: 0.00003498
Iteration 22/1000 | Loss: 0.00003491
Iteration 23/1000 | Loss: 0.00003488
Iteration 24/1000 | Loss: 0.00003487
Iteration 25/1000 | Loss: 0.00003484
Iteration 26/1000 | Loss: 0.00003484
Iteration 27/1000 | Loss: 0.00003483
Iteration 28/1000 | Loss: 0.00003483
Iteration 29/1000 | Loss: 0.00003481
Iteration 30/1000 | Loss: 0.00003481
Iteration 31/1000 | Loss: 0.00003481
Iteration 32/1000 | Loss: 0.00003481
Iteration 33/1000 | Loss: 0.00003481
Iteration 34/1000 | Loss: 0.00003481
Iteration 35/1000 | Loss: 0.00003481
Iteration 36/1000 | Loss: 0.00003480
Iteration 37/1000 | Loss: 0.00003480
Iteration 38/1000 | Loss: 0.00003480
Iteration 39/1000 | Loss: 0.00003480
Iteration 40/1000 | Loss: 0.00003480
Iteration 41/1000 | Loss: 0.00003480
Iteration 42/1000 | Loss: 0.00003480
Iteration 43/1000 | Loss: 0.00003480
Iteration 44/1000 | Loss: 0.00003480
Iteration 45/1000 | Loss: 0.00003480
Iteration 46/1000 | Loss: 0.00003480
Iteration 47/1000 | Loss: 0.00003479
Iteration 48/1000 | Loss: 0.00003479
Iteration 49/1000 | Loss: 0.00003479
Iteration 50/1000 | Loss: 0.00003477
Iteration 51/1000 | Loss: 0.00003477
Iteration 52/1000 | Loss: 0.00003477
Iteration 53/1000 | Loss: 0.00003477
Iteration 54/1000 | Loss: 0.00003477
Iteration 55/1000 | Loss: 0.00003477
Iteration 56/1000 | Loss: 0.00003477
Iteration 57/1000 | Loss: 0.00003477
Iteration 58/1000 | Loss: 0.00003476
Iteration 59/1000 | Loss: 0.00003476
Iteration 60/1000 | Loss: 0.00003476
Iteration 61/1000 | Loss: 0.00003476
Iteration 62/1000 | Loss: 0.00003476
Iteration 63/1000 | Loss: 0.00003476
Iteration 64/1000 | Loss: 0.00003476
Iteration 65/1000 | Loss: 0.00003476
Iteration 66/1000 | Loss: 0.00003476
Iteration 67/1000 | Loss: 0.00003476
Iteration 68/1000 | Loss: 0.00003476
Iteration 69/1000 | Loss: 0.00003475
Iteration 70/1000 | Loss: 0.00003474
Iteration 71/1000 | Loss: 0.00003474
Iteration 72/1000 | Loss: 0.00003474
Iteration 73/1000 | Loss: 0.00003474
Iteration 74/1000 | Loss: 0.00003473
Iteration 75/1000 | Loss: 0.00003473
Iteration 76/1000 | Loss: 0.00003472
Iteration 77/1000 | Loss: 0.00003472
Iteration 78/1000 | Loss: 0.00003472
Iteration 79/1000 | Loss: 0.00003471
Iteration 80/1000 | Loss: 0.00003471
Iteration 81/1000 | Loss: 0.00003471
Iteration 82/1000 | Loss: 0.00003471
Iteration 83/1000 | Loss: 0.00003471
Iteration 84/1000 | Loss: 0.00003471
Iteration 85/1000 | Loss: 0.00003471
Iteration 86/1000 | Loss: 0.00003471
Iteration 87/1000 | Loss: 0.00003471
Iteration 88/1000 | Loss: 0.00003471
Iteration 89/1000 | Loss: 0.00003471
Iteration 90/1000 | Loss: 0.00003470
Iteration 91/1000 | Loss: 0.00003470
Iteration 92/1000 | Loss: 0.00003470
Iteration 93/1000 | Loss: 0.00003470
Iteration 94/1000 | Loss: 0.00003469
Iteration 95/1000 | Loss: 0.00003469
Iteration 96/1000 | Loss: 0.00003469
Iteration 97/1000 | Loss: 0.00003469
Iteration 98/1000 | Loss: 0.00003469
Iteration 99/1000 | Loss: 0.00003468
Iteration 100/1000 | Loss: 0.00003468
Iteration 101/1000 | Loss: 0.00003468
Iteration 102/1000 | Loss: 0.00003468
Iteration 103/1000 | Loss: 0.00003467
Iteration 104/1000 | Loss: 0.00003467
Iteration 105/1000 | Loss: 0.00003467
Iteration 106/1000 | Loss: 0.00003467
Iteration 107/1000 | Loss: 0.00003467
Iteration 108/1000 | Loss: 0.00003467
Iteration 109/1000 | Loss: 0.00003467
Iteration 110/1000 | Loss: 0.00003467
Iteration 111/1000 | Loss: 0.00003467
Iteration 112/1000 | Loss: 0.00003467
Iteration 113/1000 | Loss: 0.00003467
Iteration 114/1000 | Loss: 0.00003467
Iteration 115/1000 | Loss: 0.00003467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [3.4672539186431095e-05, 3.4672539186431095e-05, 3.4672539186431095e-05, 3.4672539186431095e-05, 3.4672539186431095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4672539186431095e-05

Optimization complete. Final v2v error: 4.705300807952881 mm

Highest mean error: 7.3994598388671875 mm for frame 145

Lowest mean error: 3.6019136905670166 mm for frame 14

Saving results

Total time: 52.37298130989075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900957
Iteration 2/25 | Loss: 0.00187155
Iteration 3/25 | Loss: 0.00141840
Iteration 4/25 | Loss: 0.00135567
Iteration 5/25 | Loss: 0.00136764
Iteration 6/25 | Loss: 0.00134644
Iteration 7/25 | Loss: 0.00133081
Iteration 8/25 | Loss: 0.00134600
Iteration 9/25 | Loss: 0.00132702
Iteration 10/25 | Loss: 0.00133205
Iteration 11/25 | Loss: 0.00132824
Iteration 12/25 | Loss: 0.00132336
Iteration 13/25 | Loss: 0.00132227
Iteration 14/25 | Loss: 0.00131781
Iteration 15/25 | Loss: 0.00131983
Iteration 16/25 | Loss: 0.00131010
Iteration 17/25 | Loss: 0.00130474
Iteration 18/25 | Loss: 0.00132357
Iteration 19/25 | Loss: 0.00129612
Iteration 20/25 | Loss: 0.00129434
Iteration 21/25 | Loss: 0.00129079
Iteration 22/25 | Loss: 0.00129193
Iteration 23/25 | Loss: 0.00128875
Iteration 24/25 | Loss: 0.00128920
Iteration 25/25 | Loss: 0.00128814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31177473
Iteration 2/25 | Loss: 0.00121559
Iteration 3/25 | Loss: 0.00121538
Iteration 4/25 | Loss: 0.00121538
Iteration 5/25 | Loss: 0.00121538
Iteration 6/25 | Loss: 0.00121538
Iteration 7/25 | Loss: 0.00121538
Iteration 8/25 | Loss: 0.00121538
Iteration 9/25 | Loss: 0.00121537
Iteration 10/25 | Loss: 0.00121537
Iteration 11/25 | Loss: 0.00121537
Iteration 12/25 | Loss: 0.00121537
Iteration 13/25 | Loss: 0.00121537
Iteration 14/25 | Loss: 0.00121537
Iteration 15/25 | Loss: 0.00121537
Iteration 16/25 | Loss: 0.00121537
Iteration 17/25 | Loss: 0.00121537
Iteration 18/25 | Loss: 0.00121537
Iteration 19/25 | Loss: 0.00121537
Iteration 20/25 | Loss: 0.00121537
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012153740972280502, 0.0012153740972280502, 0.0012153740972280502, 0.0012153740972280502, 0.0012153740972280502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012153740972280502

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121537
Iteration 2/1000 | Loss: 0.00014900
Iteration 3/1000 | Loss: 0.00370757
Iteration 4/1000 | Loss: 0.00146709
Iteration 5/1000 | Loss: 0.00197748
Iteration 6/1000 | Loss: 0.00100157
Iteration 7/1000 | Loss: 0.00080462
Iteration 8/1000 | Loss: 0.00104612
Iteration 9/1000 | Loss: 0.00072207
Iteration 10/1000 | Loss: 0.00052733
Iteration 11/1000 | Loss: 0.00058744
Iteration 12/1000 | Loss: 0.00073757
Iteration 13/1000 | Loss: 0.00091476
Iteration 14/1000 | Loss: 0.00069030
Iteration 15/1000 | Loss: 0.00083552
Iteration 16/1000 | Loss: 0.00069751
Iteration 17/1000 | Loss: 0.00036808
Iteration 18/1000 | Loss: 0.00048130
Iteration 19/1000 | Loss: 0.00036252
Iteration 20/1000 | Loss: 0.00035298
Iteration 21/1000 | Loss: 0.00050745
Iteration 22/1000 | Loss: 0.00046938
Iteration 23/1000 | Loss: 0.00046056
Iteration 24/1000 | Loss: 0.00021766
Iteration 25/1000 | Loss: 0.00045169
Iteration 26/1000 | Loss: 0.00040926
Iteration 27/1000 | Loss: 0.00023314
Iteration 28/1000 | Loss: 0.00035811
Iteration 29/1000 | Loss: 0.00052368
Iteration 30/1000 | Loss: 0.00025091
Iteration 31/1000 | Loss: 0.00029212
Iteration 32/1000 | Loss: 0.00050671
Iteration 33/1000 | Loss: 0.00044593
Iteration 34/1000 | Loss: 0.00038927
Iteration 35/1000 | Loss: 0.00033098
Iteration 36/1000 | Loss: 0.00047027
Iteration 37/1000 | Loss: 0.00036626
Iteration 38/1000 | Loss: 0.00029202
Iteration 39/1000 | Loss: 0.00118617
Iteration 40/1000 | Loss: 0.00063144
Iteration 41/1000 | Loss: 0.00096260
Iteration 42/1000 | Loss: 0.00116690
Iteration 43/1000 | Loss: 0.00114453
Iteration 44/1000 | Loss: 0.00098476
Iteration 45/1000 | Loss: 0.00103869
Iteration 46/1000 | Loss: 0.00054638
Iteration 47/1000 | Loss: 0.00107543
Iteration 48/1000 | Loss: 0.00046877
Iteration 49/1000 | Loss: 0.00007065
Iteration 50/1000 | Loss: 0.00009057
Iteration 51/1000 | Loss: 0.00004924
Iteration 52/1000 | Loss: 0.00015960
Iteration 53/1000 | Loss: 0.00005085
Iteration 54/1000 | Loss: 0.00006495
Iteration 55/1000 | Loss: 0.00004498
Iteration 56/1000 | Loss: 0.00004250
Iteration 57/1000 | Loss: 0.00004014
Iteration 58/1000 | Loss: 0.00016337
Iteration 59/1000 | Loss: 0.00009881
Iteration 60/1000 | Loss: 0.00003691
Iteration 61/1000 | Loss: 0.00039747
Iteration 62/1000 | Loss: 0.00039983
Iteration 63/1000 | Loss: 0.00008411
Iteration 64/1000 | Loss: 0.00007977
Iteration 65/1000 | Loss: 0.00037628
Iteration 66/1000 | Loss: 0.00007603
Iteration 67/1000 | Loss: 0.00007783
Iteration 68/1000 | Loss: 0.00003710
Iteration 69/1000 | Loss: 0.00003321
Iteration 70/1000 | Loss: 0.00003057
Iteration 71/1000 | Loss: 0.00002868
Iteration 72/1000 | Loss: 0.00002712
Iteration 73/1000 | Loss: 0.00002630
Iteration 74/1000 | Loss: 0.00002579
Iteration 75/1000 | Loss: 0.00002540
Iteration 76/1000 | Loss: 0.00002512
Iteration 77/1000 | Loss: 0.00002481
Iteration 78/1000 | Loss: 0.00002459
Iteration 79/1000 | Loss: 0.00002440
Iteration 80/1000 | Loss: 0.00002440
Iteration 81/1000 | Loss: 0.00002433
Iteration 82/1000 | Loss: 0.00002417
Iteration 83/1000 | Loss: 0.00002400
Iteration 84/1000 | Loss: 0.00002392
Iteration 85/1000 | Loss: 0.00002389
Iteration 86/1000 | Loss: 0.00002385
Iteration 87/1000 | Loss: 0.00002384
Iteration 88/1000 | Loss: 0.00002383
Iteration 89/1000 | Loss: 0.00002383
Iteration 90/1000 | Loss: 0.00002382
Iteration 91/1000 | Loss: 0.00002382
Iteration 92/1000 | Loss: 0.00002382
Iteration 93/1000 | Loss: 0.00002382
Iteration 94/1000 | Loss: 0.00002382
Iteration 95/1000 | Loss: 0.00002382
Iteration 96/1000 | Loss: 0.00002381
Iteration 97/1000 | Loss: 0.00002381
Iteration 98/1000 | Loss: 0.00002381
Iteration 99/1000 | Loss: 0.00002381
Iteration 100/1000 | Loss: 0.00002381
Iteration 101/1000 | Loss: 0.00002381
Iteration 102/1000 | Loss: 0.00002381
Iteration 103/1000 | Loss: 0.00002381
Iteration 104/1000 | Loss: 0.00002381
Iteration 105/1000 | Loss: 0.00002381
Iteration 106/1000 | Loss: 0.00002381
Iteration 107/1000 | Loss: 0.00002381
Iteration 108/1000 | Loss: 0.00002380
Iteration 109/1000 | Loss: 0.00002380
Iteration 110/1000 | Loss: 0.00002379
Iteration 111/1000 | Loss: 0.00002378
Iteration 112/1000 | Loss: 0.00002377
Iteration 113/1000 | Loss: 0.00002377
Iteration 114/1000 | Loss: 0.00002377
Iteration 115/1000 | Loss: 0.00002377
Iteration 116/1000 | Loss: 0.00002377
Iteration 117/1000 | Loss: 0.00002377
Iteration 118/1000 | Loss: 0.00002377
Iteration 119/1000 | Loss: 0.00002377
Iteration 120/1000 | Loss: 0.00002377
Iteration 121/1000 | Loss: 0.00002376
Iteration 122/1000 | Loss: 0.00002375
Iteration 123/1000 | Loss: 0.00002375
Iteration 124/1000 | Loss: 0.00002375
Iteration 125/1000 | Loss: 0.00002375
Iteration 126/1000 | Loss: 0.00002374
Iteration 127/1000 | Loss: 0.00002374
Iteration 128/1000 | Loss: 0.00002374
Iteration 129/1000 | Loss: 0.00002374
Iteration 130/1000 | Loss: 0.00002374
Iteration 131/1000 | Loss: 0.00002374
Iteration 132/1000 | Loss: 0.00002374
Iteration 133/1000 | Loss: 0.00002374
Iteration 134/1000 | Loss: 0.00002373
Iteration 135/1000 | Loss: 0.00002373
Iteration 136/1000 | Loss: 0.00002373
Iteration 137/1000 | Loss: 0.00002373
Iteration 138/1000 | Loss: 0.00002373
Iteration 139/1000 | Loss: 0.00002373
Iteration 140/1000 | Loss: 0.00002373
Iteration 141/1000 | Loss: 0.00002372
Iteration 142/1000 | Loss: 0.00002372
Iteration 143/1000 | Loss: 0.00002372
Iteration 144/1000 | Loss: 0.00002372
Iteration 145/1000 | Loss: 0.00002371
Iteration 146/1000 | Loss: 0.00002371
Iteration 147/1000 | Loss: 0.00002371
Iteration 148/1000 | Loss: 0.00002371
Iteration 149/1000 | Loss: 0.00002370
Iteration 150/1000 | Loss: 0.00002370
Iteration 151/1000 | Loss: 0.00002370
Iteration 152/1000 | Loss: 0.00002370
Iteration 153/1000 | Loss: 0.00002370
Iteration 154/1000 | Loss: 0.00002369
Iteration 155/1000 | Loss: 0.00002369
Iteration 156/1000 | Loss: 0.00002369
Iteration 157/1000 | Loss: 0.00002368
Iteration 158/1000 | Loss: 0.00002368
Iteration 159/1000 | Loss: 0.00002368
Iteration 160/1000 | Loss: 0.00002368
Iteration 161/1000 | Loss: 0.00002368
Iteration 162/1000 | Loss: 0.00002368
Iteration 163/1000 | Loss: 0.00002367
Iteration 164/1000 | Loss: 0.00002367
Iteration 165/1000 | Loss: 0.00002367
Iteration 166/1000 | Loss: 0.00002366
Iteration 167/1000 | Loss: 0.00002366
Iteration 168/1000 | Loss: 0.00002366
Iteration 169/1000 | Loss: 0.00002366
Iteration 170/1000 | Loss: 0.00002365
Iteration 171/1000 | Loss: 0.00002365
Iteration 172/1000 | Loss: 0.00002365
Iteration 173/1000 | Loss: 0.00002365
Iteration 174/1000 | Loss: 0.00002365
Iteration 175/1000 | Loss: 0.00002365
Iteration 176/1000 | Loss: 0.00002364
Iteration 177/1000 | Loss: 0.00002364
Iteration 178/1000 | Loss: 0.00002364
Iteration 179/1000 | Loss: 0.00002364
Iteration 180/1000 | Loss: 0.00002364
Iteration 181/1000 | Loss: 0.00002364
Iteration 182/1000 | Loss: 0.00002364
Iteration 183/1000 | Loss: 0.00002364
Iteration 184/1000 | Loss: 0.00002364
Iteration 185/1000 | Loss: 0.00002364
Iteration 186/1000 | Loss: 0.00002364
Iteration 187/1000 | Loss: 0.00002364
Iteration 188/1000 | Loss: 0.00002363
Iteration 189/1000 | Loss: 0.00002363
Iteration 190/1000 | Loss: 0.00002363
Iteration 191/1000 | Loss: 0.00002363
Iteration 192/1000 | Loss: 0.00002363
Iteration 193/1000 | Loss: 0.00002363
Iteration 194/1000 | Loss: 0.00002363
Iteration 195/1000 | Loss: 0.00002363
Iteration 196/1000 | Loss: 0.00002363
Iteration 197/1000 | Loss: 0.00002363
Iteration 198/1000 | Loss: 0.00002363
Iteration 199/1000 | Loss: 0.00002363
Iteration 200/1000 | Loss: 0.00002363
Iteration 201/1000 | Loss: 0.00002363
Iteration 202/1000 | Loss: 0.00002363
Iteration 203/1000 | Loss: 0.00002362
Iteration 204/1000 | Loss: 0.00002362
Iteration 205/1000 | Loss: 0.00002362
Iteration 206/1000 | Loss: 0.00002362
Iteration 207/1000 | Loss: 0.00002362
Iteration 208/1000 | Loss: 0.00002362
Iteration 209/1000 | Loss: 0.00002362
Iteration 210/1000 | Loss: 0.00002362
Iteration 211/1000 | Loss: 0.00002362
Iteration 212/1000 | Loss: 0.00002362
Iteration 213/1000 | Loss: 0.00002362
Iteration 214/1000 | Loss: 0.00002362
Iteration 215/1000 | Loss: 0.00002362
Iteration 216/1000 | Loss: 0.00002361
Iteration 217/1000 | Loss: 0.00002361
Iteration 218/1000 | Loss: 0.00002361
Iteration 219/1000 | Loss: 0.00002361
Iteration 220/1000 | Loss: 0.00002361
Iteration 221/1000 | Loss: 0.00002361
Iteration 222/1000 | Loss: 0.00002361
Iteration 223/1000 | Loss: 0.00002361
Iteration 224/1000 | Loss: 0.00002361
Iteration 225/1000 | Loss: 0.00002360
Iteration 226/1000 | Loss: 0.00002360
Iteration 227/1000 | Loss: 0.00002360
Iteration 228/1000 | Loss: 0.00002360
Iteration 229/1000 | Loss: 0.00002360
Iteration 230/1000 | Loss: 0.00002360
Iteration 231/1000 | Loss: 0.00002360
Iteration 232/1000 | Loss: 0.00002360
Iteration 233/1000 | Loss: 0.00002360
Iteration 234/1000 | Loss: 0.00002360
Iteration 235/1000 | Loss: 0.00002360
Iteration 236/1000 | Loss: 0.00002360
Iteration 237/1000 | Loss: 0.00002360
Iteration 238/1000 | Loss: 0.00002359
Iteration 239/1000 | Loss: 0.00002359
Iteration 240/1000 | Loss: 0.00002359
Iteration 241/1000 | Loss: 0.00002359
Iteration 242/1000 | Loss: 0.00002359
Iteration 243/1000 | Loss: 0.00002359
Iteration 244/1000 | Loss: 0.00002359
Iteration 245/1000 | Loss: 0.00002358
Iteration 246/1000 | Loss: 0.00002358
Iteration 247/1000 | Loss: 0.00002358
Iteration 248/1000 | Loss: 0.00002358
Iteration 249/1000 | Loss: 0.00002358
Iteration 250/1000 | Loss: 0.00002358
Iteration 251/1000 | Loss: 0.00002358
Iteration 252/1000 | Loss: 0.00002357
Iteration 253/1000 | Loss: 0.00002357
Iteration 254/1000 | Loss: 0.00002357
Iteration 255/1000 | Loss: 0.00002357
Iteration 256/1000 | Loss: 0.00002357
Iteration 257/1000 | Loss: 0.00002357
Iteration 258/1000 | Loss: 0.00002357
Iteration 259/1000 | Loss: 0.00002357
Iteration 260/1000 | Loss: 0.00002356
Iteration 261/1000 | Loss: 0.00002356
Iteration 262/1000 | Loss: 0.00002355
Iteration 263/1000 | Loss: 0.00002355
Iteration 264/1000 | Loss: 0.00002355
Iteration 265/1000 | Loss: 0.00002355
Iteration 266/1000 | Loss: 0.00002354
Iteration 267/1000 | Loss: 0.00002354
Iteration 268/1000 | Loss: 0.00002354
Iteration 269/1000 | Loss: 0.00002354
Iteration 270/1000 | Loss: 0.00002354
Iteration 271/1000 | Loss: 0.00002353
Iteration 272/1000 | Loss: 0.00002353
Iteration 273/1000 | Loss: 0.00002353
Iteration 274/1000 | Loss: 0.00002353
Iteration 275/1000 | Loss: 0.00002352
Iteration 276/1000 | Loss: 0.00002352
Iteration 277/1000 | Loss: 0.00002352
Iteration 278/1000 | Loss: 0.00002352
Iteration 279/1000 | Loss: 0.00002352
Iteration 280/1000 | Loss: 0.00002352
Iteration 281/1000 | Loss: 0.00002352
Iteration 282/1000 | Loss: 0.00002351
Iteration 283/1000 | Loss: 0.00002351
Iteration 284/1000 | Loss: 0.00002351
Iteration 285/1000 | Loss: 0.00002351
Iteration 286/1000 | Loss: 0.00002350
Iteration 287/1000 | Loss: 0.00002350
Iteration 288/1000 | Loss: 0.00002350
Iteration 289/1000 | Loss: 0.00002349
Iteration 290/1000 | Loss: 0.00002349
Iteration 291/1000 | Loss: 0.00002349
Iteration 292/1000 | Loss: 0.00002349
Iteration 293/1000 | Loss: 0.00002349
Iteration 294/1000 | Loss: 0.00002349
Iteration 295/1000 | Loss: 0.00002349
Iteration 296/1000 | Loss: 0.00002349
Iteration 297/1000 | Loss: 0.00002348
Iteration 298/1000 | Loss: 0.00002348
Iteration 299/1000 | Loss: 0.00002348
Iteration 300/1000 | Loss: 0.00002348
Iteration 301/1000 | Loss: 0.00002348
Iteration 302/1000 | Loss: 0.00002348
Iteration 303/1000 | Loss: 0.00002348
Iteration 304/1000 | Loss: 0.00002347
Iteration 305/1000 | Loss: 0.00002347
Iteration 306/1000 | Loss: 0.00002347
Iteration 307/1000 | Loss: 0.00002347
Iteration 308/1000 | Loss: 0.00002347
Iteration 309/1000 | Loss: 0.00002347
Iteration 310/1000 | Loss: 0.00002347
Iteration 311/1000 | Loss: 0.00002347
Iteration 312/1000 | Loss: 0.00002347
Iteration 313/1000 | Loss: 0.00002347
Iteration 314/1000 | Loss: 0.00002346
Iteration 315/1000 | Loss: 0.00002346
Iteration 316/1000 | Loss: 0.00002346
Iteration 317/1000 | Loss: 0.00002346
Iteration 318/1000 | Loss: 0.00002346
Iteration 319/1000 | Loss: 0.00002346
Iteration 320/1000 | Loss: 0.00002346
Iteration 321/1000 | Loss: 0.00002346
Iteration 322/1000 | Loss: 0.00002345
Iteration 323/1000 | Loss: 0.00002345
Iteration 324/1000 | Loss: 0.00002345
Iteration 325/1000 | Loss: 0.00002345
Iteration 326/1000 | Loss: 0.00002345
Iteration 327/1000 | Loss: 0.00002345
Iteration 328/1000 | Loss: 0.00002345
Iteration 329/1000 | Loss: 0.00002345
Iteration 330/1000 | Loss: 0.00002345
Iteration 331/1000 | Loss: 0.00002345
Iteration 332/1000 | Loss: 0.00002345
Iteration 333/1000 | Loss: 0.00002345
Iteration 334/1000 | Loss: 0.00002345
Iteration 335/1000 | Loss: 0.00002345
Iteration 336/1000 | Loss: 0.00002345
Iteration 337/1000 | Loss: 0.00002345
Iteration 338/1000 | Loss: 0.00002344
Iteration 339/1000 | Loss: 0.00002344
Iteration 340/1000 | Loss: 0.00002344
Iteration 341/1000 | Loss: 0.00002344
Iteration 342/1000 | Loss: 0.00002344
Iteration 343/1000 | Loss: 0.00002344
Iteration 344/1000 | Loss: 0.00002344
Iteration 345/1000 | Loss: 0.00002344
Iteration 346/1000 | Loss: 0.00002344
Iteration 347/1000 | Loss: 0.00002344
Iteration 348/1000 | Loss: 0.00002344
Iteration 349/1000 | Loss: 0.00002344
Iteration 350/1000 | Loss: 0.00002344
Iteration 351/1000 | Loss: 0.00002344
Iteration 352/1000 | Loss: 0.00002344
Iteration 353/1000 | Loss: 0.00002344
Iteration 354/1000 | Loss: 0.00002344
Iteration 355/1000 | Loss: 0.00002343
Iteration 356/1000 | Loss: 0.00002343
Iteration 357/1000 | Loss: 0.00002343
Iteration 358/1000 | Loss: 0.00002343
Iteration 359/1000 | Loss: 0.00002343
Iteration 360/1000 | Loss: 0.00002343
Iteration 361/1000 | Loss: 0.00002343
Iteration 362/1000 | Loss: 0.00002343
Iteration 363/1000 | Loss: 0.00002343
Iteration 364/1000 | Loss: 0.00002343
Iteration 365/1000 | Loss: 0.00002343
Iteration 366/1000 | Loss: 0.00002343
Iteration 367/1000 | Loss: 0.00002343
Iteration 368/1000 | Loss: 0.00002343
Iteration 369/1000 | Loss: 0.00002343
Iteration 370/1000 | Loss: 0.00002343
Iteration 371/1000 | Loss: 0.00002343
Iteration 372/1000 | Loss: 0.00002343
Iteration 373/1000 | Loss: 0.00002343
Iteration 374/1000 | Loss: 0.00002343
Iteration 375/1000 | Loss: 0.00002343
Iteration 376/1000 | Loss: 0.00002343
Iteration 377/1000 | Loss: 0.00002343
Iteration 378/1000 | Loss: 0.00002343
Iteration 379/1000 | Loss: 0.00002343
Iteration 380/1000 | Loss: 0.00002343
Iteration 381/1000 | Loss: 0.00002343
Iteration 382/1000 | Loss: 0.00002343
Iteration 383/1000 | Loss: 0.00002343
Iteration 384/1000 | Loss: 0.00002343
Iteration 385/1000 | Loss: 0.00002343
Iteration 386/1000 | Loss: 0.00002343
Iteration 387/1000 | Loss: 0.00002343
Iteration 388/1000 | Loss: 0.00002343
Iteration 389/1000 | Loss: 0.00002343
Iteration 390/1000 | Loss: 0.00002343
Iteration 391/1000 | Loss: 0.00002343
Iteration 392/1000 | Loss: 0.00002343
Iteration 393/1000 | Loss: 0.00002343
Iteration 394/1000 | Loss: 0.00002343
Iteration 395/1000 | Loss: 0.00002343
Iteration 396/1000 | Loss: 0.00002343
Iteration 397/1000 | Loss: 0.00002343
Iteration 398/1000 | Loss: 0.00002343
Iteration 399/1000 | Loss: 0.00002343
Iteration 400/1000 | Loss: 0.00002343
Iteration 401/1000 | Loss: 0.00002343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 401. Stopping optimization.
Last 5 losses: [2.3426451662089676e-05, 2.3426451662089676e-05, 2.3426451662089676e-05, 2.3426451662089676e-05, 2.3426451662089676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3426451662089676e-05

Optimization complete. Final v2v error: 3.9995195865631104 mm

Highest mean error: 5.852968692779541 mm for frame 98

Lowest mean error: 3.0134499073028564 mm for frame 144

Saving results

Total time: 178.46162676811218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455914
Iteration 2/25 | Loss: 0.00139520
Iteration 3/25 | Loss: 0.00123994
Iteration 4/25 | Loss: 0.00122876
Iteration 5/25 | Loss: 0.00122711
Iteration 6/25 | Loss: 0.00122711
Iteration 7/25 | Loss: 0.00122711
Iteration 8/25 | Loss: 0.00122711
Iteration 9/25 | Loss: 0.00122711
Iteration 10/25 | Loss: 0.00122711
Iteration 11/25 | Loss: 0.00122711
Iteration 12/25 | Loss: 0.00122711
Iteration 13/25 | Loss: 0.00122711
Iteration 14/25 | Loss: 0.00122711
Iteration 15/25 | Loss: 0.00122711
Iteration 16/25 | Loss: 0.00122711
Iteration 17/25 | Loss: 0.00122711
Iteration 18/25 | Loss: 0.00122711
Iteration 19/25 | Loss: 0.00122711
Iteration 20/25 | Loss: 0.00122711
Iteration 21/25 | Loss: 0.00122711
Iteration 22/25 | Loss: 0.00122711
Iteration 23/25 | Loss: 0.00122711
Iteration 24/25 | Loss: 0.00122711
Iteration 25/25 | Loss: 0.00122711

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37332284
Iteration 2/25 | Loss: 0.00062434
Iteration 3/25 | Loss: 0.00062434
Iteration 4/25 | Loss: 0.00062434
Iteration 5/25 | Loss: 0.00062434
Iteration 6/25 | Loss: 0.00062434
Iteration 7/25 | Loss: 0.00062434
Iteration 8/25 | Loss: 0.00062434
Iteration 9/25 | Loss: 0.00062434
Iteration 10/25 | Loss: 0.00062434
Iteration 11/25 | Loss: 0.00062434
Iteration 12/25 | Loss: 0.00062434
Iteration 13/25 | Loss: 0.00062434
Iteration 14/25 | Loss: 0.00062434
Iteration 15/25 | Loss: 0.00062434
Iteration 16/25 | Loss: 0.00062434
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006243414827622473, 0.0006243414827622473, 0.0006243414827622473, 0.0006243414827622473, 0.0006243414827622473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006243414827622473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062434
Iteration 2/1000 | Loss: 0.00003239
Iteration 3/1000 | Loss: 0.00002641
Iteration 4/1000 | Loss: 0.00002434
Iteration 5/1000 | Loss: 0.00002340
Iteration 6/1000 | Loss: 0.00002247
Iteration 7/1000 | Loss: 0.00002195
Iteration 8/1000 | Loss: 0.00002135
Iteration 9/1000 | Loss: 0.00002102
Iteration 10/1000 | Loss: 0.00002079
Iteration 11/1000 | Loss: 0.00002067
Iteration 12/1000 | Loss: 0.00002052
Iteration 13/1000 | Loss: 0.00002049
Iteration 14/1000 | Loss: 0.00002049
Iteration 15/1000 | Loss: 0.00002044
Iteration 16/1000 | Loss: 0.00002042
Iteration 17/1000 | Loss: 0.00002039
Iteration 18/1000 | Loss: 0.00002038
Iteration 19/1000 | Loss: 0.00002037
Iteration 20/1000 | Loss: 0.00002037
Iteration 21/1000 | Loss: 0.00002036
Iteration 22/1000 | Loss: 0.00002035
Iteration 23/1000 | Loss: 0.00002034
Iteration 24/1000 | Loss: 0.00002033
Iteration 25/1000 | Loss: 0.00002032
Iteration 26/1000 | Loss: 0.00002032
Iteration 27/1000 | Loss: 0.00002031
Iteration 28/1000 | Loss: 0.00002029
Iteration 29/1000 | Loss: 0.00002028
Iteration 30/1000 | Loss: 0.00002028
Iteration 31/1000 | Loss: 0.00002027
Iteration 32/1000 | Loss: 0.00002027
Iteration 33/1000 | Loss: 0.00002026
Iteration 34/1000 | Loss: 0.00002026
Iteration 35/1000 | Loss: 0.00002026
Iteration 36/1000 | Loss: 0.00002026
Iteration 37/1000 | Loss: 0.00002025
Iteration 38/1000 | Loss: 0.00002025
Iteration 39/1000 | Loss: 0.00002025
Iteration 40/1000 | Loss: 0.00002024
Iteration 41/1000 | Loss: 0.00002023
Iteration 42/1000 | Loss: 0.00002023
Iteration 43/1000 | Loss: 0.00002022
Iteration 44/1000 | Loss: 0.00002022
Iteration 45/1000 | Loss: 0.00002022
Iteration 46/1000 | Loss: 0.00002022
Iteration 47/1000 | Loss: 0.00002022
Iteration 48/1000 | Loss: 0.00002022
Iteration 49/1000 | Loss: 0.00002022
Iteration 50/1000 | Loss: 0.00002022
Iteration 51/1000 | Loss: 0.00002022
Iteration 52/1000 | Loss: 0.00002022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 52. Stopping optimization.
Last 5 losses: [2.0219160433043726e-05, 2.0219160433043726e-05, 2.0219160433043726e-05, 2.0219160433043726e-05, 2.0219160433043726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0219160433043726e-05

Optimization complete. Final v2v error: 3.766258716583252 mm

Highest mean error: 4.005547046661377 mm for frame 49

Lowest mean error: 3.5039172172546387 mm for frame 3

Saving results

Total time: 31.653532028198242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880839
Iteration 2/25 | Loss: 0.00157472
Iteration 3/25 | Loss: 0.00132710
Iteration 4/25 | Loss: 0.00129397
Iteration 5/25 | Loss: 0.00129271
Iteration 6/25 | Loss: 0.00128942
Iteration 7/25 | Loss: 0.00126356
Iteration 8/25 | Loss: 0.00124844
Iteration 9/25 | Loss: 0.00124109
Iteration 10/25 | Loss: 0.00123747
Iteration 11/25 | Loss: 0.00122774
Iteration 12/25 | Loss: 0.00122768
Iteration 13/25 | Loss: 0.00123406
Iteration 14/25 | Loss: 0.00123545
Iteration 15/25 | Loss: 0.00123215
Iteration 16/25 | Loss: 0.00122726
Iteration 17/25 | Loss: 0.00122323
Iteration 18/25 | Loss: 0.00121997
Iteration 19/25 | Loss: 0.00121906
Iteration 20/25 | Loss: 0.00121868
Iteration 21/25 | Loss: 0.00121860
Iteration 22/25 | Loss: 0.00121860
Iteration 23/25 | Loss: 0.00121859
Iteration 24/25 | Loss: 0.00121859
Iteration 25/25 | Loss: 0.00121859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24763012
Iteration 2/25 | Loss: 0.00115316
Iteration 3/25 | Loss: 0.00115315
Iteration 4/25 | Loss: 0.00115315
Iteration 5/25 | Loss: 0.00115315
Iteration 6/25 | Loss: 0.00115315
Iteration 7/25 | Loss: 0.00115315
Iteration 8/25 | Loss: 0.00115315
Iteration 9/25 | Loss: 0.00115315
Iteration 10/25 | Loss: 0.00115315
Iteration 11/25 | Loss: 0.00115315
Iteration 12/25 | Loss: 0.00115315
Iteration 13/25 | Loss: 0.00115315
Iteration 14/25 | Loss: 0.00115315
Iteration 15/25 | Loss: 0.00115315
Iteration 16/25 | Loss: 0.00115315
Iteration 17/25 | Loss: 0.00115315
Iteration 18/25 | Loss: 0.00115315
Iteration 19/25 | Loss: 0.00115315
Iteration 20/25 | Loss: 0.00115315
Iteration 21/25 | Loss: 0.00115315
Iteration 22/25 | Loss: 0.00115315
Iteration 23/25 | Loss: 0.00115315
Iteration 24/25 | Loss: 0.00115315
Iteration 25/25 | Loss: 0.00115315
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011531509226188064, 0.0011531509226188064, 0.0011531509226188064, 0.0011531509226188064, 0.0011531509226188064]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011531509226188064

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115315
Iteration 2/1000 | Loss: 0.00014658
Iteration 3/1000 | Loss: 0.00028338
Iteration 4/1000 | Loss: 0.00004075
Iteration 5/1000 | Loss: 0.00003027
Iteration 6/1000 | Loss: 0.00002606
Iteration 7/1000 | Loss: 0.00002482
Iteration 8/1000 | Loss: 0.00002401
Iteration 9/1000 | Loss: 0.00002312
Iteration 10/1000 | Loss: 0.00002255
Iteration 11/1000 | Loss: 0.00002202
Iteration 12/1000 | Loss: 0.00002176
Iteration 13/1000 | Loss: 0.00002146
Iteration 14/1000 | Loss: 0.00002126
Iteration 15/1000 | Loss: 0.00002107
Iteration 16/1000 | Loss: 0.00002089
Iteration 17/1000 | Loss: 0.00002078
Iteration 18/1000 | Loss: 0.00002071
Iteration 19/1000 | Loss: 0.00002067
Iteration 20/1000 | Loss: 0.00002067
Iteration 21/1000 | Loss: 0.00002060
Iteration 22/1000 | Loss: 0.00002059
Iteration 23/1000 | Loss: 0.00002058
Iteration 24/1000 | Loss: 0.00002055
Iteration 25/1000 | Loss: 0.00002053
Iteration 26/1000 | Loss: 0.00002052
Iteration 27/1000 | Loss: 0.00002051
Iteration 28/1000 | Loss: 0.00002051
Iteration 29/1000 | Loss: 0.00002048
Iteration 30/1000 | Loss: 0.00002047
Iteration 31/1000 | Loss: 0.00002047
Iteration 32/1000 | Loss: 0.00002046
Iteration 33/1000 | Loss: 0.00002046
Iteration 34/1000 | Loss: 0.00002045
Iteration 35/1000 | Loss: 0.00002044
Iteration 36/1000 | Loss: 0.00002043
Iteration 37/1000 | Loss: 0.00002042
Iteration 38/1000 | Loss: 0.00002042
Iteration 39/1000 | Loss: 0.00002041
Iteration 40/1000 | Loss: 0.00002040
Iteration 41/1000 | Loss: 0.00002040
Iteration 42/1000 | Loss: 0.00002040
Iteration 43/1000 | Loss: 0.00002040
Iteration 44/1000 | Loss: 0.00002039
Iteration 45/1000 | Loss: 0.00002039
Iteration 46/1000 | Loss: 0.00002039
Iteration 47/1000 | Loss: 0.00002039
Iteration 48/1000 | Loss: 0.00002039
Iteration 49/1000 | Loss: 0.00002039
Iteration 50/1000 | Loss: 0.00002039
Iteration 51/1000 | Loss: 0.00002039
Iteration 52/1000 | Loss: 0.00002038
Iteration 53/1000 | Loss: 0.00002038
Iteration 54/1000 | Loss: 0.00002038
Iteration 55/1000 | Loss: 0.00002037
Iteration 56/1000 | Loss: 0.00002037
Iteration 57/1000 | Loss: 0.00002037
Iteration 58/1000 | Loss: 0.00002037
Iteration 59/1000 | Loss: 0.00002036
Iteration 60/1000 | Loss: 0.00002036
Iteration 61/1000 | Loss: 0.00002035
Iteration 62/1000 | Loss: 0.00002035
Iteration 63/1000 | Loss: 0.00002035
Iteration 64/1000 | Loss: 0.00002035
Iteration 65/1000 | Loss: 0.00002035
Iteration 66/1000 | Loss: 0.00002035
Iteration 67/1000 | Loss: 0.00002034
Iteration 68/1000 | Loss: 0.00002034
Iteration 69/1000 | Loss: 0.00002034
Iteration 70/1000 | Loss: 0.00002034
Iteration 71/1000 | Loss: 0.00002034
Iteration 72/1000 | Loss: 0.00002034
Iteration 73/1000 | Loss: 0.00002033
Iteration 74/1000 | Loss: 0.00002033
Iteration 75/1000 | Loss: 0.00002033
Iteration 76/1000 | Loss: 0.00002032
Iteration 77/1000 | Loss: 0.00002032
Iteration 78/1000 | Loss: 0.00002032
Iteration 79/1000 | Loss: 0.00002032
Iteration 80/1000 | Loss: 0.00002032
Iteration 81/1000 | Loss: 0.00002032
Iteration 82/1000 | Loss: 0.00002032
Iteration 83/1000 | Loss: 0.00002032
Iteration 84/1000 | Loss: 0.00002032
Iteration 85/1000 | Loss: 0.00002032
Iteration 86/1000 | Loss: 0.00002031
Iteration 87/1000 | Loss: 0.00002031
Iteration 88/1000 | Loss: 0.00002031
Iteration 89/1000 | Loss: 0.00002031
Iteration 90/1000 | Loss: 0.00002030
Iteration 91/1000 | Loss: 0.00002030
Iteration 92/1000 | Loss: 0.00002030
Iteration 93/1000 | Loss: 0.00002029
Iteration 94/1000 | Loss: 0.00002029
Iteration 95/1000 | Loss: 0.00002029
Iteration 96/1000 | Loss: 0.00002028
Iteration 97/1000 | Loss: 0.00002028
Iteration 98/1000 | Loss: 0.00002028
Iteration 99/1000 | Loss: 0.00002028
Iteration 100/1000 | Loss: 0.00002028
Iteration 101/1000 | Loss: 0.00002028
Iteration 102/1000 | Loss: 0.00002028
Iteration 103/1000 | Loss: 0.00002028
Iteration 104/1000 | Loss: 0.00002028
Iteration 105/1000 | Loss: 0.00002028
Iteration 106/1000 | Loss: 0.00002028
Iteration 107/1000 | Loss: 0.00002028
Iteration 108/1000 | Loss: 0.00002028
Iteration 109/1000 | Loss: 0.00002028
Iteration 110/1000 | Loss: 0.00002028
Iteration 111/1000 | Loss: 0.00002028
Iteration 112/1000 | Loss: 0.00002028
Iteration 113/1000 | Loss: 0.00002028
Iteration 114/1000 | Loss: 0.00002028
Iteration 115/1000 | Loss: 0.00002028
Iteration 116/1000 | Loss: 0.00002028
Iteration 117/1000 | Loss: 0.00002028
Iteration 118/1000 | Loss: 0.00002028
Iteration 119/1000 | Loss: 0.00002028
Iteration 120/1000 | Loss: 0.00002028
Iteration 121/1000 | Loss: 0.00002028
Iteration 122/1000 | Loss: 0.00002028
Iteration 123/1000 | Loss: 0.00002028
Iteration 124/1000 | Loss: 0.00002028
Iteration 125/1000 | Loss: 0.00002028
Iteration 126/1000 | Loss: 0.00002028
Iteration 127/1000 | Loss: 0.00002028
Iteration 128/1000 | Loss: 0.00002027
Iteration 129/1000 | Loss: 0.00002027
Iteration 130/1000 | Loss: 0.00002027
Iteration 131/1000 | Loss: 0.00002027
Iteration 132/1000 | Loss: 0.00002027
Iteration 133/1000 | Loss: 0.00002027
Iteration 134/1000 | Loss: 0.00002027
Iteration 135/1000 | Loss: 0.00002027
Iteration 136/1000 | Loss: 0.00002027
Iteration 137/1000 | Loss: 0.00002027
Iteration 138/1000 | Loss: 0.00002027
Iteration 139/1000 | Loss: 0.00002027
Iteration 140/1000 | Loss: 0.00002027
Iteration 141/1000 | Loss: 0.00002027
Iteration 142/1000 | Loss: 0.00002027
Iteration 143/1000 | Loss: 0.00002027
Iteration 144/1000 | Loss: 0.00002027
Iteration 145/1000 | Loss: 0.00002027
Iteration 146/1000 | Loss: 0.00002027
Iteration 147/1000 | Loss: 0.00002027
Iteration 148/1000 | Loss: 0.00002027
Iteration 149/1000 | Loss: 0.00002027
Iteration 150/1000 | Loss: 0.00002027
Iteration 151/1000 | Loss: 0.00002027
Iteration 152/1000 | Loss: 0.00002027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.0274101188988425e-05, 2.0274101188988425e-05, 2.0274101188988425e-05, 2.0274101188988425e-05, 2.0274101188988425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0274101188988425e-05

Optimization complete. Final v2v error: 3.7512080669403076 mm

Highest mean error: 5.729658126831055 mm for frame 106

Lowest mean error: 3.1950111389160156 mm for frame 70

Saving results

Total time: 74.48938274383545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993583
Iteration 2/25 | Loss: 0.00993583
Iteration 3/25 | Loss: 0.00250676
Iteration 4/25 | Loss: 0.00182878
Iteration 5/25 | Loss: 0.00166358
Iteration 6/25 | Loss: 0.00178873
Iteration 7/25 | Loss: 0.00175823
Iteration 8/25 | Loss: 0.00147384
Iteration 9/25 | Loss: 0.00132225
Iteration 10/25 | Loss: 0.00130825
Iteration 11/25 | Loss: 0.00129583
Iteration 12/25 | Loss: 0.00128728
Iteration 13/25 | Loss: 0.00128973
Iteration 14/25 | Loss: 0.00128093
Iteration 15/25 | Loss: 0.00128310
Iteration 16/25 | Loss: 0.00127485
Iteration 17/25 | Loss: 0.00127241
Iteration 18/25 | Loss: 0.00127087
Iteration 19/25 | Loss: 0.00127808
Iteration 20/25 | Loss: 0.00127480
Iteration 21/25 | Loss: 0.00127239
Iteration 22/25 | Loss: 0.00127279
Iteration 23/25 | Loss: 0.00127116
Iteration 24/25 | Loss: 0.00127120
Iteration 25/25 | Loss: 0.00126941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35430896
Iteration 2/25 | Loss: 0.00105138
Iteration 3/25 | Loss: 0.00105138
Iteration 4/25 | Loss: 0.00105138
Iteration 5/25 | Loss: 0.00105138
Iteration 6/25 | Loss: 0.00105138
Iteration 7/25 | Loss: 0.00105138
Iteration 8/25 | Loss: 0.00105138
Iteration 9/25 | Loss: 0.00105137
Iteration 10/25 | Loss: 0.00105137
Iteration 11/25 | Loss: 0.00105137
Iteration 12/25 | Loss: 0.00105137
Iteration 13/25 | Loss: 0.00105137
Iteration 14/25 | Loss: 0.00105137
Iteration 15/25 | Loss: 0.00105137
Iteration 16/25 | Loss: 0.00105137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010513743618503213, 0.0010513743618503213, 0.0010513743618503213, 0.0010513743618503213, 0.0010513743618503213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010513743618503213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105137
Iteration 2/1000 | Loss: 0.00085645
Iteration 3/1000 | Loss: 0.00054078
Iteration 4/1000 | Loss: 0.00035456
Iteration 5/1000 | Loss: 0.00010309
Iteration 6/1000 | Loss: 0.00007712
Iteration 7/1000 | Loss: 0.00014695
Iteration 8/1000 | Loss: 0.00005752
Iteration 9/1000 | Loss: 0.00005006
Iteration 10/1000 | Loss: 0.00004407
Iteration 11/1000 | Loss: 0.00010369
Iteration 12/1000 | Loss: 0.00066108
Iteration 13/1000 | Loss: 0.00006611
Iteration 14/1000 | Loss: 0.00005185
Iteration 15/1000 | Loss: 0.00011360
Iteration 16/1000 | Loss: 0.00026380
Iteration 17/1000 | Loss: 0.00033560
Iteration 18/1000 | Loss: 0.00007062
Iteration 19/1000 | Loss: 0.00006896
Iteration 20/1000 | Loss: 0.00055012
Iteration 21/1000 | Loss: 0.00006825
Iteration 22/1000 | Loss: 0.00005274
Iteration 23/1000 | Loss: 0.00004761
Iteration 24/1000 | Loss: 0.00012081
Iteration 25/1000 | Loss: 0.00004090
Iteration 26/1000 | Loss: 0.00003729
Iteration 27/1000 | Loss: 0.00004770
Iteration 28/1000 | Loss: 0.00003299
Iteration 29/1000 | Loss: 0.00003100
Iteration 30/1000 | Loss: 0.00002950
Iteration 31/1000 | Loss: 0.00002821
Iteration 32/1000 | Loss: 0.00002729
Iteration 33/1000 | Loss: 0.00002642
Iteration 34/1000 | Loss: 0.00002565
Iteration 35/1000 | Loss: 0.00002511
Iteration 36/1000 | Loss: 0.00002472
Iteration 37/1000 | Loss: 0.00017317
Iteration 38/1000 | Loss: 0.00003018
Iteration 39/1000 | Loss: 0.00002705
Iteration 40/1000 | Loss: 0.00002550
Iteration 41/1000 | Loss: 0.00002412
Iteration 42/1000 | Loss: 0.00002332
Iteration 43/1000 | Loss: 0.00002287
Iteration 44/1000 | Loss: 0.00008458
Iteration 45/1000 | Loss: 0.00002703
Iteration 46/1000 | Loss: 0.00002600
Iteration 47/1000 | Loss: 0.00002538
Iteration 48/1000 | Loss: 0.00009330
Iteration 49/1000 | Loss: 0.00008879
Iteration 50/1000 | Loss: 0.00003022
Iteration 51/1000 | Loss: 0.00002554
Iteration 52/1000 | Loss: 0.00004347
Iteration 53/1000 | Loss: 0.00003071
Iteration 54/1000 | Loss: 0.00002333
Iteration 55/1000 | Loss: 0.00002290
Iteration 56/1000 | Loss: 0.00007256
Iteration 57/1000 | Loss: 0.00002475
Iteration 58/1000 | Loss: 0.00003113
Iteration 59/1000 | Loss: 0.00002423
Iteration 60/1000 | Loss: 0.00002295
Iteration 61/1000 | Loss: 0.00002228
Iteration 62/1000 | Loss: 0.00002190
Iteration 63/1000 | Loss: 0.00002163
Iteration 64/1000 | Loss: 0.00002135
Iteration 65/1000 | Loss: 0.00002123
Iteration 66/1000 | Loss: 0.00002107
Iteration 67/1000 | Loss: 0.00002103
Iteration 68/1000 | Loss: 0.00002103
Iteration 69/1000 | Loss: 0.00002102
Iteration 70/1000 | Loss: 0.00002101
Iteration 71/1000 | Loss: 0.00002100
Iteration 72/1000 | Loss: 0.00002100
Iteration 73/1000 | Loss: 0.00002100
Iteration 74/1000 | Loss: 0.00002100
Iteration 75/1000 | Loss: 0.00002100
Iteration 76/1000 | Loss: 0.00002099
Iteration 77/1000 | Loss: 0.00002099
Iteration 78/1000 | Loss: 0.00002099
Iteration 79/1000 | Loss: 0.00002099
Iteration 80/1000 | Loss: 0.00002099
Iteration 81/1000 | Loss: 0.00002099
Iteration 82/1000 | Loss: 0.00002099
Iteration 83/1000 | Loss: 0.00002097
Iteration 84/1000 | Loss: 0.00002097
Iteration 85/1000 | Loss: 0.00002097
Iteration 86/1000 | Loss: 0.00002097
Iteration 87/1000 | Loss: 0.00002096
Iteration 88/1000 | Loss: 0.00002096
Iteration 89/1000 | Loss: 0.00002096
Iteration 90/1000 | Loss: 0.00002096
Iteration 91/1000 | Loss: 0.00002096
Iteration 92/1000 | Loss: 0.00002095
Iteration 93/1000 | Loss: 0.00002095
Iteration 94/1000 | Loss: 0.00002095
Iteration 95/1000 | Loss: 0.00002095
Iteration 96/1000 | Loss: 0.00002095
Iteration 97/1000 | Loss: 0.00002095
Iteration 98/1000 | Loss: 0.00002095
Iteration 99/1000 | Loss: 0.00002095
Iteration 100/1000 | Loss: 0.00002095
Iteration 101/1000 | Loss: 0.00002095
Iteration 102/1000 | Loss: 0.00002095
Iteration 103/1000 | Loss: 0.00002095
Iteration 104/1000 | Loss: 0.00002095
Iteration 105/1000 | Loss: 0.00002094
Iteration 106/1000 | Loss: 0.00002094
Iteration 107/1000 | Loss: 0.00002094
Iteration 108/1000 | Loss: 0.00002094
Iteration 109/1000 | Loss: 0.00002094
Iteration 110/1000 | Loss: 0.00002094
Iteration 111/1000 | Loss: 0.00002094
Iteration 112/1000 | Loss: 0.00002093
Iteration 113/1000 | Loss: 0.00002093
Iteration 114/1000 | Loss: 0.00002093
Iteration 115/1000 | Loss: 0.00002093
Iteration 116/1000 | Loss: 0.00002093
Iteration 117/1000 | Loss: 0.00002093
Iteration 118/1000 | Loss: 0.00002093
Iteration 119/1000 | Loss: 0.00002093
Iteration 120/1000 | Loss: 0.00002093
Iteration 121/1000 | Loss: 0.00002093
Iteration 122/1000 | Loss: 0.00002093
Iteration 123/1000 | Loss: 0.00002093
Iteration 124/1000 | Loss: 0.00002093
Iteration 125/1000 | Loss: 0.00002093
Iteration 126/1000 | Loss: 0.00002093
Iteration 127/1000 | Loss: 0.00002093
Iteration 128/1000 | Loss: 0.00002093
Iteration 129/1000 | Loss: 0.00002093
Iteration 130/1000 | Loss: 0.00002093
Iteration 131/1000 | Loss: 0.00002093
Iteration 132/1000 | Loss: 0.00002092
Iteration 133/1000 | Loss: 0.00002092
Iteration 134/1000 | Loss: 0.00002092
Iteration 135/1000 | Loss: 0.00002092
Iteration 136/1000 | Loss: 0.00002092
Iteration 137/1000 | Loss: 0.00002092
Iteration 138/1000 | Loss: 0.00002092
Iteration 139/1000 | Loss: 0.00002092
Iteration 140/1000 | Loss: 0.00002092
Iteration 141/1000 | Loss: 0.00002092
Iteration 142/1000 | Loss: 0.00002092
Iteration 143/1000 | Loss: 0.00002092
Iteration 144/1000 | Loss: 0.00002092
Iteration 145/1000 | Loss: 0.00002092
Iteration 146/1000 | Loss: 0.00002092
Iteration 147/1000 | Loss: 0.00002091
Iteration 148/1000 | Loss: 0.00002091
Iteration 149/1000 | Loss: 0.00002091
Iteration 150/1000 | Loss: 0.00002091
Iteration 151/1000 | Loss: 0.00002091
Iteration 152/1000 | Loss: 0.00002091
Iteration 153/1000 | Loss: 0.00002091
Iteration 154/1000 | Loss: 0.00002091
Iteration 155/1000 | Loss: 0.00002091
Iteration 156/1000 | Loss: 0.00002091
Iteration 157/1000 | Loss: 0.00002091
Iteration 158/1000 | Loss: 0.00002091
Iteration 159/1000 | Loss: 0.00002091
Iteration 160/1000 | Loss: 0.00002091
Iteration 161/1000 | Loss: 0.00002091
Iteration 162/1000 | Loss: 0.00002091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.091351962008048e-05, 2.091351962008048e-05, 2.091351962008048e-05, 2.091351962008048e-05, 2.091351962008048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.091351962008048e-05

Optimization complete. Final v2v error: 3.7886974811553955 mm

Highest mean error: 5.403770446777344 mm for frame 143

Lowest mean error: 3.2318639755249023 mm for frame 64

Saving results

Total time: 158.62276530265808
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849935
Iteration 2/25 | Loss: 0.00183661
Iteration 3/25 | Loss: 0.00138193
Iteration 4/25 | Loss: 0.00132039
Iteration 5/25 | Loss: 0.00131558
Iteration 6/25 | Loss: 0.00131508
Iteration 7/25 | Loss: 0.00131508
Iteration 8/25 | Loss: 0.00131508
Iteration 9/25 | Loss: 0.00131508
Iteration 10/25 | Loss: 0.00131508
Iteration 11/25 | Loss: 0.00131508
Iteration 12/25 | Loss: 0.00131508
Iteration 13/25 | Loss: 0.00131508
Iteration 14/25 | Loss: 0.00131508
Iteration 15/25 | Loss: 0.00131508
Iteration 16/25 | Loss: 0.00131508
Iteration 17/25 | Loss: 0.00131508
Iteration 18/25 | Loss: 0.00131508
Iteration 19/25 | Loss: 0.00131508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013150781160220504, 0.0013150781160220504, 0.0013150781160220504, 0.0013150781160220504, 0.0013150781160220504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013150781160220504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18653286
Iteration 2/25 | Loss: 0.00070848
Iteration 3/25 | Loss: 0.00070848
Iteration 4/25 | Loss: 0.00070848
Iteration 5/25 | Loss: 0.00070848
Iteration 6/25 | Loss: 0.00070848
Iteration 7/25 | Loss: 0.00070848
Iteration 8/25 | Loss: 0.00070848
Iteration 9/25 | Loss: 0.00070848
Iteration 10/25 | Loss: 0.00070848
Iteration 11/25 | Loss: 0.00070848
Iteration 12/25 | Loss: 0.00070848
Iteration 13/25 | Loss: 0.00070848
Iteration 14/25 | Loss: 0.00070848
Iteration 15/25 | Loss: 0.00070848
Iteration 16/25 | Loss: 0.00070848
Iteration 17/25 | Loss: 0.00070848
Iteration 18/25 | Loss: 0.00070848
Iteration 19/25 | Loss: 0.00070848
Iteration 20/25 | Loss: 0.00070848
Iteration 21/25 | Loss: 0.00070848
Iteration 22/25 | Loss: 0.00070848
Iteration 23/25 | Loss: 0.00070848
Iteration 24/25 | Loss: 0.00070848
Iteration 25/25 | Loss: 0.00070848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070848
Iteration 2/1000 | Loss: 0.00005334
Iteration 3/1000 | Loss: 0.00003000
Iteration 4/1000 | Loss: 0.00002530
Iteration 5/1000 | Loss: 0.00002313
Iteration 6/1000 | Loss: 0.00002235
Iteration 7/1000 | Loss: 0.00002170
Iteration 8/1000 | Loss: 0.00002123
Iteration 9/1000 | Loss: 0.00002086
Iteration 10/1000 | Loss: 0.00002064
Iteration 11/1000 | Loss: 0.00002042
Iteration 12/1000 | Loss: 0.00002021
Iteration 13/1000 | Loss: 0.00002011
Iteration 14/1000 | Loss: 0.00002010
Iteration 15/1000 | Loss: 0.00001996
Iteration 16/1000 | Loss: 0.00001994
Iteration 17/1000 | Loss: 0.00001993
Iteration 18/1000 | Loss: 0.00001991
Iteration 19/1000 | Loss: 0.00001991
Iteration 20/1000 | Loss: 0.00001991
Iteration 21/1000 | Loss: 0.00001991
Iteration 22/1000 | Loss: 0.00001991
Iteration 23/1000 | Loss: 0.00001991
Iteration 24/1000 | Loss: 0.00001991
Iteration 25/1000 | Loss: 0.00001991
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001991
Iteration 28/1000 | Loss: 0.00001990
Iteration 29/1000 | Loss: 0.00001990
Iteration 30/1000 | Loss: 0.00001990
Iteration 31/1000 | Loss: 0.00001987
Iteration 32/1000 | Loss: 0.00001987
Iteration 33/1000 | Loss: 0.00001986
Iteration 34/1000 | Loss: 0.00001986
Iteration 35/1000 | Loss: 0.00001986
Iteration 36/1000 | Loss: 0.00001985
Iteration 37/1000 | Loss: 0.00001985
Iteration 38/1000 | Loss: 0.00001984
Iteration 39/1000 | Loss: 0.00001983
Iteration 40/1000 | Loss: 0.00001983
Iteration 41/1000 | Loss: 0.00001982
Iteration 42/1000 | Loss: 0.00001982
Iteration 43/1000 | Loss: 0.00001982
Iteration 44/1000 | Loss: 0.00001982
Iteration 45/1000 | Loss: 0.00001982
Iteration 46/1000 | Loss: 0.00001982
Iteration 47/1000 | Loss: 0.00001982
Iteration 48/1000 | Loss: 0.00001982
Iteration 49/1000 | Loss: 0.00001981
Iteration 50/1000 | Loss: 0.00001979
Iteration 51/1000 | Loss: 0.00001979
Iteration 52/1000 | Loss: 0.00001978
Iteration 53/1000 | Loss: 0.00001978
Iteration 54/1000 | Loss: 0.00001978
Iteration 55/1000 | Loss: 0.00001978
Iteration 56/1000 | Loss: 0.00001978
Iteration 57/1000 | Loss: 0.00001978
Iteration 58/1000 | Loss: 0.00001978
Iteration 59/1000 | Loss: 0.00001978
Iteration 60/1000 | Loss: 0.00001978
Iteration 61/1000 | Loss: 0.00001978
Iteration 62/1000 | Loss: 0.00001978
Iteration 63/1000 | Loss: 0.00001978
Iteration 64/1000 | Loss: 0.00001976
Iteration 65/1000 | Loss: 0.00001976
Iteration 66/1000 | Loss: 0.00001975
Iteration 67/1000 | Loss: 0.00001975
Iteration 68/1000 | Loss: 0.00001975
Iteration 69/1000 | Loss: 0.00001975
Iteration 70/1000 | Loss: 0.00001975
Iteration 71/1000 | Loss: 0.00001975
Iteration 72/1000 | Loss: 0.00001974
Iteration 73/1000 | Loss: 0.00001974
Iteration 74/1000 | Loss: 0.00001974
Iteration 75/1000 | Loss: 0.00001974
Iteration 76/1000 | Loss: 0.00001974
Iteration 77/1000 | Loss: 0.00001974
Iteration 78/1000 | Loss: 0.00001974
Iteration 79/1000 | Loss: 0.00001974
Iteration 80/1000 | Loss: 0.00001974
Iteration 81/1000 | Loss: 0.00001974
Iteration 82/1000 | Loss: 0.00001974
Iteration 83/1000 | Loss: 0.00001974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.974286715267226e-05, 1.974286715267226e-05, 1.974286715267226e-05, 1.974286715267226e-05, 1.974286715267226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.974286715267226e-05

Optimization complete. Final v2v error: 3.72259521484375 mm

Highest mean error: 3.906338930130005 mm for frame 67

Lowest mean error: 3.4860198497772217 mm for frame 33

Saving results

Total time: 31.924780130386353
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959795
Iteration 2/25 | Loss: 0.00144468
Iteration 3/25 | Loss: 0.00145976
Iteration 4/25 | Loss: 0.00120216
Iteration 5/25 | Loss: 0.00115850
Iteration 6/25 | Loss: 0.00114323
Iteration 7/25 | Loss: 0.00115617
Iteration 8/25 | Loss: 0.00113267
Iteration 9/25 | Loss: 0.00113148
Iteration 10/25 | Loss: 0.00113111
Iteration 11/25 | Loss: 0.00113255
Iteration 12/25 | Loss: 0.00112947
Iteration 13/25 | Loss: 0.00112884
Iteration 14/25 | Loss: 0.00112864
Iteration 15/25 | Loss: 0.00112857
Iteration 16/25 | Loss: 0.00112857
Iteration 17/25 | Loss: 0.00112857
Iteration 18/25 | Loss: 0.00112857
Iteration 19/25 | Loss: 0.00112856
Iteration 20/25 | Loss: 0.00112856
Iteration 21/25 | Loss: 0.00112856
Iteration 22/25 | Loss: 0.00112856
Iteration 23/25 | Loss: 0.00112856
Iteration 24/25 | Loss: 0.00112856
Iteration 25/25 | Loss: 0.00112856

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35741460
Iteration 2/25 | Loss: 0.00085859
Iteration 3/25 | Loss: 0.00085858
Iteration 4/25 | Loss: 0.00085858
Iteration 5/25 | Loss: 0.00085858
Iteration 6/25 | Loss: 0.00085858
Iteration 7/25 | Loss: 0.00085858
Iteration 8/25 | Loss: 0.00085858
Iteration 9/25 | Loss: 0.00085858
Iteration 10/25 | Loss: 0.00085858
Iteration 11/25 | Loss: 0.00085858
Iteration 12/25 | Loss: 0.00085858
Iteration 13/25 | Loss: 0.00085858
Iteration 14/25 | Loss: 0.00085858
Iteration 15/25 | Loss: 0.00085858
Iteration 16/25 | Loss: 0.00085858
Iteration 17/25 | Loss: 0.00085858
Iteration 18/25 | Loss: 0.00085858
Iteration 19/25 | Loss: 0.00085858
Iteration 20/25 | Loss: 0.00085858
Iteration 21/25 | Loss: 0.00085858
Iteration 22/25 | Loss: 0.00085858
Iteration 23/25 | Loss: 0.00085858
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000858579995110631, 0.000858579995110631, 0.000858579995110631, 0.000858579995110631, 0.000858579995110631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000858579995110631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085858
Iteration 2/1000 | Loss: 0.00002243
Iteration 3/1000 | Loss: 0.00001628
Iteration 4/1000 | Loss: 0.00001456
Iteration 5/1000 | Loss: 0.00001361
Iteration 6/1000 | Loss: 0.00001294
Iteration 7/1000 | Loss: 0.00001256
Iteration 8/1000 | Loss: 0.00001231
Iteration 9/1000 | Loss: 0.00001207
Iteration 10/1000 | Loss: 0.00001189
Iteration 11/1000 | Loss: 0.00001178
Iteration 12/1000 | Loss: 0.00001177
Iteration 13/1000 | Loss: 0.00001177
Iteration 14/1000 | Loss: 0.00001168
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001159
Iteration 17/1000 | Loss: 0.00001150
Iteration 18/1000 | Loss: 0.00001143
Iteration 19/1000 | Loss: 0.00001142
Iteration 20/1000 | Loss: 0.00001140
Iteration 21/1000 | Loss: 0.00001140
Iteration 22/1000 | Loss: 0.00001140
Iteration 23/1000 | Loss: 0.00001139
Iteration 24/1000 | Loss: 0.00001139
Iteration 25/1000 | Loss: 0.00001138
Iteration 26/1000 | Loss: 0.00001138
Iteration 27/1000 | Loss: 0.00001137
Iteration 28/1000 | Loss: 0.00001137
Iteration 29/1000 | Loss: 0.00001137
Iteration 30/1000 | Loss: 0.00001136
Iteration 31/1000 | Loss: 0.00001136
Iteration 32/1000 | Loss: 0.00001135
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001128
Iteration 35/1000 | Loss: 0.00001128
Iteration 36/1000 | Loss: 0.00001127
Iteration 37/1000 | Loss: 0.00001126
Iteration 38/1000 | Loss: 0.00001126
Iteration 39/1000 | Loss: 0.00001125
Iteration 40/1000 | Loss: 0.00001125
Iteration 41/1000 | Loss: 0.00001125
Iteration 42/1000 | Loss: 0.00001125
Iteration 43/1000 | Loss: 0.00001124
Iteration 44/1000 | Loss: 0.00001124
Iteration 45/1000 | Loss: 0.00001123
Iteration 46/1000 | Loss: 0.00001123
Iteration 47/1000 | Loss: 0.00001122
Iteration 48/1000 | Loss: 0.00001121
Iteration 49/1000 | Loss: 0.00001121
Iteration 50/1000 | Loss: 0.00001120
Iteration 51/1000 | Loss: 0.00001120
Iteration 52/1000 | Loss: 0.00001119
Iteration 53/1000 | Loss: 0.00001119
Iteration 54/1000 | Loss: 0.00001119
Iteration 55/1000 | Loss: 0.00001118
Iteration 56/1000 | Loss: 0.00001118
Iteration 57/1000 | Loss: 0.00001118
Iteration 58/1000 | Loss: 0.00001117
Iteration 59/1000 | Loss: 0.00001117
Iteration 60/1000 | Loss: 0.00001117
Iteration 61/1000 | Loss: 0.00001117
Iteration 62/1000 | Loss: 0.00001117
Iteration 63/1000 | Loss: 0.00001117
Iteration 64/1000 | Loss: 0.00001117
Iteration 65/1000 | Loss: 0.00001116
Iteration 66/1000 | Loss: 0.00001116
Iteration 67/1000 | Loss: 0.00001116
Iteration 68/1000 | Loss: 0.00001116
Iteration 69/1000 | Loss: 0.00001116
Iteration 70/1000 | Loss: 0.00001116
Iteration 71/1000 | Loss: 0.00001115
Iteration 72/1000 | Loss: 0.00001115
Iteration 73/1000 | Loss: 0.00001115
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001115
Iteration 76/1000 | Loss: 0.00001113
Iteration 77/1000 | Loss: 0.00001113
Iteration 78/1000 | Loss: 0.00001113
Iteration 79/1000 | Loss: 0.00001113
Iteration 80/1000 | Loss: 0.00001113
Iteration 81/1000 | Loss: 0.00001113
Iteration 82/1000 | Loss: 0.00001113
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001112
Iteration 86/1000 | Loss: 0.00001112
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001111
Iteration 89/1000 | Loss: 0.00001111
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001110
Iteration 93/1000 | Loss: 0.00001109
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001109
Iteration 96/1000 | Loss: 0.00001109
Iteration 97/1000 | Loss: 0.00001109
Iteration 98/1000 | Loss: 0.00001108
Iteration 99/1000 | Loss: 0.00001108
Iteration 100/1000 | Loss: 0.00001108
Iteration 101/1000 | Loss: 0.00001107
Iteration 102/1000 | Loss: 0.00001107
Iteration 103/1000 | Loss: 0.00001107
Iteration 104/1000 | Loss: 0.00001106
Iteration 105/1000 | Loss: 0.00001106
Iteration 106/1000 | Loss: 0.00001105
Iteration 107/1000 | Loss: 0.00001105
Iteration 108/1000 | Loss: 0.00001105
Iteration 109/1000 | Loss: 0.00001104
Iteration 110/1000 | Loss: 0.00001104
Iteration 111/1000 | Loss: 0.00001103
Iteration 112/1000 | Loss: 0.00001102
Iteration 113/1000 | Loss: 0.00001102
Iteration 114/1000 | Loss: 0.00001102
Iteration 115/1000 | Loss: 0.00001101
Iteration 116/1000 | Loss: 0.00001101
Iteration 117/1000 | Loss: 0.00001101
Iteration 118/1000 | Loss: 0.00001101
Iteration 119/1000 | Loss: 0.00001101
Iteration 120/1000 | Loss: 0.00001100
Iteration 121/1000 | Loss: 0.00001100
Iteration 122/1000 | Loss: 0.00001099
Iteration 123/1000 | Loss: 0.00001099
Iteration 124/1000 | Loss: 0.00001099
Iteration 125/1000 | Loss: 0.00001099
Iteration 126/1000 | Loss: 0.00001099
Iteration 127/1000 | Loss: 0.00001099
Iteration 128/1000 | Loss: 0.00001099
Iteration 129/1000 | Loss: 0.00001099
Iteration 130/1000 | Loss: 0.00001098
Iteration 131/1000 | Loss: 0.00001098
Iteration 132/1000 | Loss: 0.00001098
Iteration 133/1000 | Loss: 0.00001098
Iteration 134/1000 | Loss: 0.00001098
Iteration 135/1000 | Loss: 0.00001098
Iteration 136/1000 | Loss: 0.00001098
Iteration 137/1000 | Loss: 0.00001097
Iteration 138/1000 | Loss: 0.00001097
Iteration 139/1000 | Loss: 0.00001097
Iteration 140/1000 | Loss: 0.00001097
Iteration 141/1000 | Loss: 0.00001097
Iteration 142/1000 | Loss: 0.00001097
Iteration 143/1000 | Loss: 0.00001097
Iteration 144/1000 | Loss: 0.00001097
Iteration 145/1000 | Loss: 0.00001097
Iteration 146/1000 | Loss: 0.00001097
Iteration 147/1000 | Loss: 0.00001096
Iteration 148/1000 | Loss: 0.00001096
Iteration 149/1000 | Loss: 0.00001096
Iteration 150/1000 | Loss: 0.00001096
Iteration 151/1000 | Loss: 0.00001096
Iteration 152/1000 | Loss: 0.00001096
Iteration 153/1000 | Loss: 0.00001095
Iteration 154/1000 | Loss: 0.00001095
Iteration 155/1000 | Loss: 0.00001095
Iteration 156/1000 | Loss: 0.00001095
Iteration 157/1000 | Loss: 0.00001095
Iteration 158/1000 | Loss: 0.00001095
Iteration 159/1000 | Loss: 0.00001095
Iteration 160/1000 | Loss: 0.00001095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.0948318049486261e-05, 1.0948318049486261e-05, 1.0948318049486261e-05, 1.0948318049486261e-05, 1.0948318049486261e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0948318049486261e-05

Optimization complete. Final v2v error: 2.832010507583618 mm

Highest mean error: 3.066415786743164 mm for frame 114

Lowest mean error: 2.64741587638855 mm for frame 165

Saving results

Total time: 65.9448413848877
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000379
Iteration 2/25 | Loss: 0.00202223
Iteration 3/25 | Loss: 0.00159006
Iteration 4/25 | Loss: 0.00154347
Iteration 5/25 | Loss: 0.00154872
Iteration 6/25 | Loss: 0.00155758
Iteration 7/25 | Loss: 0.00152253
Iteration 8/25 | Loss: 0.00148937
Iteration 9/25 | Loss: 0.00143922
Iteration 10/25 | Loss: 0.00139775
Iteration 11/25 | Loss: 0.00135529
Iteration 12/25 | Loss: 0.00132190
Iteration 13/25 | Loss: 0.00130321
Iteration 14/25 | Loss: 0.00129145
Iteration 15/25 | Loss: 0.00128665
Iteration 16/25 | Loss: 0.00128318
Iteration 17/25 | Loss: 0.00127437
Iteration 18/25 | Loss: 0.00126633
Iteration 19/25 | Loss: 0.00126751
Iteration 20/25 | Loss: 0.00126418
Iteration 21/25 | Loss: 0.00126128
Iteration 22/25 | Loss: 0.00126747
Iteration 23/25 | Loss: 0.00126916
Iteration 24/25 | Loss: 0.00127007
Iteration 25/25 | Loss: 0.00126141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23700035
Iteration 2/25 | Loss: 0.00172324
Iteration 3/25 | Loss: 0.00167434
Iteration 4/25 | Loss: 0.00167434
Iteration 5/25 | Loss: 0.00167434
Iteration 6/25 | Loss: 0.00167434
Iteration 7/25 | Loss: 0.00167434
Iteration 8/25 | Loss: 0.00167434
Iteration 9/25 | Loss: 0.00167434
Iteration 10/25 | Loss: 0.00167434
Iteration 11/25 | Loss: 0.00167434
Iteration 12/25 | Loss: 0.00167434
Iteration 13/25 | Loss: 0.00167434
Iteration 14/25 | Loss: 0.00167434
Iteration 15/25 | Loss: 0.00167434
Iteration 16/25 | Loss: 0.00167434
Iteration 17/25 | Loss: 0.00167434
Iteration 18/25 | Loss: 0.00167434
Iteration 19/25 | Loss: 0.00167434
Iteration 20/25 | Loss: 0.00167434
Iteration 21/25 | Loss: 0.00167434
Iteration 22/25 | Loss: 0.00167434
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0016743369633331895, 0.0016743369633331895, 0.0016743369633331895, 0.0016743369633331895, 0.0016743369633331895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016743369633331895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167434
Iteration 2/1000 | Loss: 0.00082158
Iteration 3/1000 | Loss: 0.00043075
Iteration 4/1000 | Loss: 0.00027478
Iteration 5/1000 | Loss: 0.00048605
Iteration 6/1000 | Loss: 0.00057070
Iteration 7/1000 | Loss: 0.00043952
Iteration 8/1000 | Loss: 0.00020119
Iteration 9/1000 | Loss: 0.00041450
Iteration 10/1000 | Loss: 0.00056927
Iteration 11/1000 | Loss: 0.00042568
Iteration 12/1000 | Loss: 0.00016265
Iteration 13/1000 | Loss: 0.00016928
Iteration 14/1000 | Loss: 0.00017481
Iteration 15/1000 | Loss: 0.00018730
Iteration 16/1000 | Loss: 0.00018941
Iteration 17/1000 | Loss: 0.00017349
Iteration 18/1000 | Loss: 0.00028734
Iteration 19/1000 | Loss: 0.00022075
Iteration 20/1000 | Loss: 0.00025111
Iteration 21/1000 | Loss: 0.00075616
Iteration 22/1000 | Loss: 0.00051932
Iteration 23/1000 | Loss: 0.00033100
Iteration 24/1000 | Loss: 0.00015909
Iteration 25/1000 | Loss: 0.00017336
Iteration 26/1000 | Loss: 0.00024984
Iteration 27/1000 | Loss: 0.00021089
Iteration 28/1000 | Loss: 0.00030503
Iteration 29/1000 | Loss: 0.00039770
Iteration 30/1000 | Loss: 0.00060186
Iteration 31/1000 | Loss: 0.00052251
Iteration 32/1000 | Loss: 0.00063311
Iteration 33/1000 | Loss: 0.00029602
Iteration 34/1000 | Loss: 0.00016203
Iteration 35/1000 | Loss: 0.00030688
Iteration 36/1000 | Loss: 0.00029232
Iteration 37/1000 | Loss: 0.00013859
Iteration 38/1000 | Loss: 0.00014192
Iteration 39/1000 | Loss: 0.00012840
Iteration 40/1000 | Loss: 0.00022374
Iteration 41/1000 | Loss: 0.00017924
Iteration 42/1000 | Loss: 0.00026426
Iteration 43/1000 | Loss: 0.00022705
Iteration 44/1000 | Loss: 0.00019134
Iteration 45/1000 | Loss: 0.00028916
Iteration 46/1000 | Loss: 0.00040666
Iteration 47/1000 | Loss: 0.00033395
Iteration 48/1000 | Loss: 0.00033628
Iteration 49/1000 | Loss: 0.00020383
Iteration 50/1000 | Loss: 0.00036800
Iteration 51/1000 | Loss: 0.00067996
Iteration 52/1000 | Loss: 0.00025149
Iteration 53/1000 | Loss: 0.00027458
Iteration 54/1000 | Loss: 0.00013788
Iteration 55/1000 | Loss: 0.00024199
Iteration 56/1000 | Loss: 0.00019595
Iteration 57/1000 | Loss: 0.00023823
Iteration 58/1000 | Loss: 0.00032723
Iteration 59/1000 | Loss: 0.00034934
Iteration 60/1000 | Loss: 0.00016703
Iteration 61/1000 | Loss: 0.00019094
Iteration 62/1000 | Loss: 0.00050008
Iteration 63/1000 | Loss: 0.00026359
Iteration 64/1000 | Loss: 0.00021990
Iteration 65/1000 | Loss: 0.00026515
Iteration 66/1000 | Loss: 0.00011735
Iteration 67/1000 | Loss: 0.00011465
Iteration 68/1000 | Loss: 0.00009434
Iteration 69/1000 | Loss: 0.00012673
Iteration 70/1000 | Loss: 0.00010255
Iteration 71/1000 | Loss: 0.00009412
Iteration 72/1000 | Loss: 0.00012565
Iteration 73/1000 | Loss: 0.00015779
Iteration 74/1000 | Loss: 0.00018840
Iteration 75/1000 | Loss: 0.00009600
Iteration 76/1000 | Loss: 0.00011398
Iteration 77/1000 | Loss: 0.00013400
Iteration 78/1000 | Loss: 0.00013256
Iteration 79/1000 | Loss: 0.00007302
Iteration 80/1000 | Loss: 0.00007826
Iteration 81/1000 | Loss: 0.00019947
Iteration 82/1000 | Loss: 0.00017118
Iteration 83/1000 | Loss: 0.00010847
Iteration 84/1000 | Loss: 0.00022400
Iteration 85/1000 | Loss: 0.00016943
Iteration 86/1000 | Loss: 0.00008746
Iteration 87/1000 | Loss: 0.00024646
Iteration 88/1000 | Loss: 0.00017481
Iteration 89/1000 | Loss: 0.00015408
Iteration 90/1000 | Loss: 0.00015118
Iteration 91/1000 | Loss: 0.00019940
Iteration 92/1000 | Loss: 0.00014366
Iteration 93/1000 | Loss: 0.00013297
Iteration 94/1000 | Loss: 0.00007145
Iteration 95/1000 | Loss: 0.00007499
Iteration 96/1000 | Loss: 0.00007459
Iteration 97/1000 | Loss: 0.00022251
Iteration 98/1000 | Loss: 0.00007936
Iteration 99/1000 | Loss: 0.00013963
Iteration 100/1000 | Loss: 0.00014983
Iteration 101/1000 | Loss: 0.00017462
Iteration 102/1000 | Loss: 0.00023163
Iteration 103/1000 | Loss: 0.00017353
Iteration 104/1000 | Loss: 0.00019803
Iteration 105/1000 | Loss: 0.00017190
Iteration 106/1000 | Loss: 0.00015747
Iteration 107/1000 | Loss: 0.00012835
Iteration 108/1000 | Loss: 0.00015417
Iteration 109/1000 | Loss: 0.00008547
Iteration 110/1000 | Loss: 0.00007135
Iteration 111/1000 | Loss: 0.00016083
Iteration 112/1000 | Loss: 0.00015653
Iteration 113/1000 | Loss: 0.00015853
Iteration 114/1000 | Loss: 0.00011384
Iteration 115/1000 | Loss: 0.00021056
Iteration 116/1000 | Loss: 0.00016869
Iteration 117/1000 | Loss: 0.00019617
Iteration 118/1000 | Loss: 0.00014033
Iteration 119/1000 | Loss: 0.00011476
Iteration 120/1000 | Loss: 0.00016265
Iteration 121/1000 | Loss: 0.00022721
Iteration 122/1000 | Loss: 0.00018223
Iteration 123/1000 | Loss: 0.00018366
Iteration 124/1000 | Loss: 0.00012755
Iteration 125/1000 | Loss: 0.00016188
Iteration 126/1000 | Loss: 0.00008942
Iteration 127/1000 | Loss: 0.00018742
Iteration 128/1000 | Loss: 0.00014259
Iteration 129/1000 | Loss: 0.00015554
Iteration 130/1000 | Loss: 0.00016700
Iteration 131/1000 | Loss: 0.00018820
Iteration 132/1000 | Loss: 0.00017368
Iteration 133/1000 | Loss: 0.00018389
Iteration 134/1000 | Loss: 0.00008059
Iteration 135/1000 | Loss: 0.00007247
Iteration 136/1000 | Loss: 0.00006146
Iteration 137/1000 | Loss: 0.00006853
Iteration 138/1000 | Loss: 0.00006938
Iteration 139/1000 | Loss: 0.00007222
Iteration 140/1000 | Loss: 0.00007416
Iteration 141/1000 | Loss: 0.00007026
Iteration 142/1000 | Loss: 0.00006696
Iteration 143/1000 | Loss: 0.00007234
Iteration 144/1000 | Loss: 0.00007807
Iteration 145/1000 | Loss: 0.00007298
Iteration 146/1000 | Loss: 0.00007362
Iteration 147/1000 | Loss: 0.00006240
Iteration 148/1000 | Loss: 0.00007018
Iteration 149/1000 | Loss: 0.00006906
Iteration 150/1000 | Loss: 0.00007497
Iteration 151/1000 | Loss: 0.00007617
Iteration 152/1000 | Loss: 0.00007434
Iteration 153/1000 | Loss: 0.00006740
Iteration 154/1000 | Loss: 0.00008021
Iteration 155/1000 | Loss: 0.00008792
Iteration 156/1000 | Loss: 0.00006838
Iteration 157/1000 | Loss: 0.00007514
Iteration 158/1000 | Loss: 0.00008231
Iteration 159/1000 | Loss: 0.00007408
Iteration 160/1000 | Loss: 0.00007494
Iteration 161/1000 | Loss: 0.00004890
Iteration 162/1000 | Loss: 0.00005527
Iteration 163/1000 | Loss: 0.00007459
Iteration 164/1000 | Loss: 0.00007975
Iteration 165/1000 | Loss: 0.00006860
Iteration 166/1000 | Loss: 0.00006227
Iteration 167/1000 | Loss: 0.00007133
Iteration 168/1000 | Loss: 0.00006778
Iteration 169/1000 | Loss: 0.00007159
Iteration 170/1000 | Loss: 0.00007151
Iteration 171/1000 | Loss: 0.00007459
Iteration 172/1000 | Loss: 0.00007073
Iteration 173/1000 | Loss: 0.00007463
Iteration 174/1000 | Loss: 0.00005577
Iteration 175/1000 | Loss: 0.00006456
Iteration 176/1000 | Loss: 0.00006340
Iteration 177/1000 | Loss: 0.00005607
Iteration 178/1000 | Loss: 0.00006632
Iteration 179/1000 | Loss: 0.00006776
Iteration 180/1000 | Loss: 0.00006696
Iteration 181/1000 | Loss: 0.00006603
Iteration 182/1000 | Loss: 0.00007379
Iteration 183/1000 | Loss: 0.00006509
Iteration 184/1000 | Loss: 0.00006470
Iteration 185/1000 | Loss: 0.00005336
Iteration 186/1000 | Loss: 0.00005298
Iteration 187/1000 | Loss: 0.00006152
Iteration 188/1000 | Loss: 0.00006609
Iteration 189/1000 | Loss: 0.00006405
Iteration 190/1000 | Loss: 0.00006036
Iteration 191/1000 | Loss: 0.00006215
Iteration 192/1000 | Loss: 0.00005615
Iteration 193/1000 | Loss: 0.00005419
Iteration 194/1000 | Loss: 0.00005847
Iteration 195/1000 | Loss: 0.00005376
Iteration 196/1000 | Loss: 0.00005064
Iteration 197/1000 | Loss: 0.00005023
Iteration 198/1000 | Loss: 0.00005165
Iteration 199/1000 | Loss: 0.00005260
Iteration 200/1000 | Loss: 0.00005009
Iteration 201/1000 | Loss: 0.00005099
Iteration 202/1000 | Loss: 0.00005067
Iteration 203/1000 | Loss: 0.00005013
Iteration 204/1000 | Loss: 0.00004966
Iteration 205/1000 | Loss: 0.00004989
Iteration 206/1000 | Loss: 0.00004979
Iteration 207/1000 | Loss: 0.00004999
Iteration 208/1000 | Loss: 0.00005076
Iteration 209/1000 | Loss: 0.00005000
Iteration 210/1000 | Loss: 0.00004784
Iteration 211/1000 | Loss: 0.00004711
Iteration 212/1000 | Loss: 0.00005298
Iteration 213/1000 | Loss: 0.00005147
Iteration 214/1000 | Loss: 0.00005045
Iteration 215/1000 | Loss: 0.00005149
Iteration 216/1000 | Loss: 0.00005038
Iteration 217/1000 | Loss: 0.00004908
Iteration 218/1000 | Loss: 0.00004947
Iteration 219/1000 | Loss: 0.00005605
Iteration 220/1000 | Loss: 0.00005316
Iteration 221/1000 | Loss: 0.00005480
Iteration 222/1000 | Loss: 0.00006208
Iteration 223/1000 | Loss: 0.00004567
Iteration 224/1000 | Loss: 0.00004162
Iteration 225/1000 | Loss: 0.00004075
Iteration 226/1000 | Loss: 0.00003998
Iteration 227/1000 | Loss: 0.00003962
Iteration 228/1000 | Loss: 0.00003949
Iteration 229/1000 | Loss: 0.00003927
Iteration 230/1000 | Loss: 0.00003896
Iteration 231/1000 | Loss: 0.00003875
Iteration 232/1000 | Loss: 0.00003875
Iteration 233/1000 | Loss: 0.00003874
Iteration 234/1000 | Loss: 0.00003868
Iteration 235/1000 | Loss: 0.00003868
Iteration 236/1000 | Loss: 0.00003867
Iteration 237/1000 | Loss: 0.00003867
Iteration 238/1000 | Loss: 0.00003866
Iteration 239/1000 | Loss: 0.00003865
Iteration 240/1000 | Loss: 0.00003865
Iteration 241/1000 | Loss: 0.00003864
Iteration 242/1000 | Loss: 0.00003863
Iteration 243/1000 | Loss: 0.00003863
Iteration 244/1000 | Loss: 0.00003863
Iteration 245/1000 | Loss: 0.00003863
Iteration 246/1000 | Loss: 0.00003863
Iteration 247/1000 | Loss: 0.00003862
Iteration 248/1000 | Loss: 0.00003861
Iteration 249/1000 | Loss: 0.00003859
Iteration 250/1000 | Loss: 0.00003857
Iteration 251/1000 | Loss: 0.00003857
Iteration 252/1000 | Loss: 0.00003857
Iteration 253/1000 | Loss: 0.00003856
Iteration 254/1000 | Loss: 0.00003853
Iteration 255/1000 | Loss: 0.00003852
Iteration 256/1000 | Loss: 0.00003849
Iteration 257/1000 | Loss: 0.00003834
Iteration 258/1000 | Loss: 0.00003834
Iteration 259/1000 | Loss: 0.00003834
Iteration 260/1000 | Loss: 0.00003830
Iteration 261/1000 | Loss: 0.00003825
Iteration 262/1000 | Loss: 0.00003819
Iteration 263/1000 | Loss: 0.00003817
Iteration 264/1000 | Loss: 0.00003816
Iteration 265/1000 | Loss: 0.00003816
Iteration 266/1000 | Loss: 0.00003816
Iteration 267/1000 | Loss: 0.00003816
Iteration 268/1000 | Loss: 0.00003816
Iteration 269/1000 | Loss: 0.00003816
Iteration 270/1000 | Loss: 0.00003816
Iteration 271/1000 | Loss: 0.00003815
Iteration 272/1000 | Loss: 0.00003815
Iteration 273/1000 | Loss: 0.00003815
Iteration 274/1000 | Loss: 0.00003815
Iteration 275/1000 | Loss: 0.00003815
Iteration 276/1000 | Loss: 0.00003814
Iteration 277/1000 | Loss: 0.00003814
Iteration 278/1000 | Loss: 0.00003814
Iteration 279/1000 | Loss: 0.00003814
Iteration 280/1000 | Loss: 0.00003813
Iteration 281/1000 | Loss: 0.00003813
Iteration 282/1000 | Loss: 0.00003813
Iteration 283/1000 | Loss: 0.00003810
Iteration 284/1000 | Loss: 0.00003809
Iteration 285/1000 | Loss: 0.00003806
Iteration 286/1000 | Loss: 0.00003805
Iteration 287/1000 | Loss: 0.00003804
Iteration 288/1000 | Loss: 0.00003804
Iteration 289/1000 | Loss: 0.00003803
Iteration 290/1000 | Loss: 0.00003803
Iteration 291/1000 | Loss: 0.00003803
Iteration 292/1000 | Loss: 0.00003802
Iteration 293/1000 | Loss: 0.00003802
Iteration 294/1000 | Loss: 0.00003802
Iteration 295/1000 | Loss: 0.00003801
Iteration 296/1000 | Loss: 0.00003800
Iteration 297/1000 | Loss: 0.00003799
Iteration 298/1000 | Loss: 0.00003799
Iteration 299/1000 | Loss: 0.00003798
Iteration 300/1000 | Loss: 0.00003798
Iteration 301/1000 | Loss: 0.00025879
Iteration 302/1000 | Loss: 0.00010365
Iteration 303/1000 | Loss: 0.00017725
Iteration 304/1000 | Loss: 0.00004049
Iteration 305/1000 | Loss: 0.00003889
Iteration 306/1000 | Loss: 0.00003801
Iteration 307/1000 | Loss: 0.00003730
Iteration 308/1000 | Loss: 0.00003694
Iteration 309/1000 | Loss: 0.00003673
Iteration 310/1000 | Loss: 0.00003671
Iteration 311/1000 | Loss: 0.00003667
Iteration 312/1000 | Loss: 0.00003667
Iteration 313/1000 | Loss: 0.00003667
Iteration 314/1000 | Loss: 0.00003667
Iteration 315/1000 | Loss: 0.00003667
Iteration 316/1000 | Loss: 0.00003667
Iteration 317/1000 | Loss: 0.00003667
Iteration 318/1000 | Loss: 0.00003666
Iteration 319/1000 | Loss: 0.00003666
Iteration 320/1000 | Loss: 0.00003666
Iteration 321/1000 | Loss: 0.00003666
Iteration 322/1000 | Loss: 0.00003666
Iteration 323/1000 | Loss: 0.00003666
Iteration 324/1000 | Loss: 0.00003666
Iteration 325/1000 | Loss: 0.00003664
Iteration 326/1000 | Loss: 0.00003664
Iteration 327/1000 | Loss: 0.00003663
Iteration 328/1000 | Loss: 0.00003663
Iteration 329/1000 | Loss: 0.00003662
Iteration 330/1000 | Loss: 0.00003662
Iteration 331/1000 | Loss: 0.00003662
Iteration 332/1000 | Loss: 0.00003661
Iteration 333/1000 | Loss: 0.00003661
Iteration 334/1000 | Loss: 0.00003660
Iteration 335/1000 | Loss: 0.00003659
Iteration 336/1000 | Loss: 0.00003659
Iteration 337/1000 | Loss: 0.00003658
Iteration 338/1000 | Loss: 0.00003658
Iteration 339/1000 | Loss: 0.00003658
Iteration 340/1000 | Loss: 0.00003658
Iteration 341/1000 | Loss: 0.00003656
Iteration 342/1000 | Loss: 0.00003656
Iteration 343/1000 | Loss: 0.00003655
Iteration 344/1000 | Loss: 0.00003655
Iteration 345/1000 | Loss: 0.00003655
Iteration 346/1000 | Loss: 0.00003654
Iteration 347/1000 | Loss: 0.00003654
Iteration 348/1000 | Loss: 0.00003653
Iteration 349/1000 | Loss: 0.00003653
Iteration 350/1000 | Loss: 0.00003652
Iteration 351/1000 | Loss: 0.00003652
Iteration 352/1000 | Loss: 0.00003651
Iteration 353/1000 | Loss: 0.00003651
Iteration 354/1000 | Loss: 0.00003650
Iteration 355/1000 | Loss: 0.00003650
Iteration 356/1000 | Loss: 0.00003650
Iteration 357/1000 | Loss: 0.00003650
Iteration 358/1000 | Loss: 0.00003649
Iteration 359/1000 | Loss: 0.00003649
Iteration 360/1000 | Loss: 0.00003649
Iteration 361/1000 | Loss: 0.00003648
Iteration 362/1000 | Loss: 0.00003647
Iteration 363/1000 | Loss: 0.00003646
Iteration 364/1000 | Loss: 0.00003645
Iteration 365/1000 | Loss: 0.00003645
Iteration 366/1000 | Loss: 0.00003644
Iteration 367/1000 | Loss: 0.00003644
Iteration 368/1000 | Loss: 0.00003644
Iteration 369/1000 | Loss: 0.00003644
Iteration 370/1000 | Loss: 0.00003643
Iteration 371/1000 | Loss: 0.00003642
Iteration 372/1000 | Loss: 0.00003642
Iteration 373/1000 | Loss: 0.00003642
Iteration 374/1000 | Loss: 0.00003642
Iteration 375/1000 | Loss: 0.00003642
Iteration 376/1000 | Loss: 0.00003642
Iteration 377/1000 | Loss: 0.00003642
Iteration 378/1000 | Loss: 0.00003642
Iteration 379/1000 | Loss: 0.00003642
Iteration 380/1000 | Loss: 0.00003641
Iteration 381/1000 | Loss: 0.00003641
Iteration 382/1000 | Loss: 0.00003641
Iteration 383/1000 | Loss: 0.00003641
Iteration 384/1000 | Loss: 0.00003640
Iteration 385/1000 | Loss: 0.00003640
Iteration 386/1000 | Loss: 0.00003640
Iteration 387/1000 | Loss: 0.00003637
Iteration 388/1000 | Loss: 0.00003637
Iteration 389/1000 | Loss: 0.00003636
Iteration 390/1000 | Loss: 0.00003635
Iteration 391/1000 | Loss: 0.00003635
Iteration 392/1000 | Loss: 0.00003635
Iteration 393/1000 | Loss: 0.00003635
Iteration 394/1000 | Loss: 0.00003635
Iteration 395/1000 | Loss: 0.00003635
Iteration 396/1000 | Loss: 0.00003634
Iteration 397/1000 | Loss: 0.00003634
Iteration 398/1000 | Loss: 0.00003634
Iteration 399/1000 | Loss: 0.00003634
Iteration 400/1000 | Loss: 0.00003634
Iteration 401/1000 | Loss: 0.00003634
Iteration 402/1000 | Loss: 0.00003633
Iteration 403/1000 | Loss: 0.00003633
Iteration 404/1000 | Loss: 0.00003632
Iteration 405/1000 | Loss: 0.00003632
Iteration 406/1000 | Loss: 0.00003631
Iteration 407/1000 | Loss: 0.00022652
Iteration 408/1000 | Loss: 0.00068854
Iteration 409/1000 | Loss: 0.00238705
Iteration 410/1000 | Loss: 0.00032585
Iteration 411/1000 | Loss: 0.00009440
Iteration 412/1000 | Loss: 0.00005831
Iteration 413/1000 | Loss: 0.00004562
Iteration 414/1000 | Loss: 0.00003799
Iteration 415/1000 | Loss: 0.00012543
Iteration 416/1000 | Loss: 0.00013391
Iteration 417/1000 | Loss: 0.00011461
Iteration 418/1000 | Loss: 0.00014086
Iteration 419/1000 | Loss: 0.00054238
Iteration 420/1000 | Loss: 0.00039290
Iteration 421/1000 | Loss: 0.00003030
Iteration 422/1000 | Loss: 0.00033424
Iteration 423/1000 | Loss: 0.00021390
Iteration 424/1000 | Loss: 0.00022684
Iteration 425/1000 | Loss: 0.00010567
Iteration 426/1000 | Loss: 0.00005662
Iteration 427/1000 | Loss: 0.00002545
Iteration 428/1000 | Loss: 0.00016522
Iteration 429/1000 | Loss: 0.00002753
Iteration 430/1000 | Loss: 0.00015756
Iteration 431/1000 | Loss: 0.00003125
Iteration 432/1000 | Loss: 0.00013508
Iteration 433/1000 | Loss: 0.00002283
Iteration 434/1000 | Loss: 0.00002212
Iteration 435/1000 | Loss: 0.00016225
Iteration 436/1000 | Loss: 0.00002577
Iteration 437/1000 | Loss: 0.00002406
Iteration 438/1000 | Loss: 0.00017815
Iteration 439/1000 | Loss: 0.00015897
Iteration 440/1000 | Loss: 0.00016885
Iteration 441/1000 | Loss: 0.00015208
Iteration 442/1000 | Loss: 0.00015807
Iteration 443/1000 | Loss: 0.00002462
Iteration 444/1000 | Loss: 0.00002109
Iteration 445/1000 | Loss: 0.00002065
Iteration 446/1000 | Loss: 0.00002004
Iteration 447/1000 | Loss: 0.00014957
Iteration 448/1000 | Loss: 0.00004608
Iteration 449/1000 | Loss: 0.00002555
Iteration 450/1000 | Loss: 0.00014647
Iteration 451/1000 | Loss: 0.00003920
Iteration 452/1000 | Loss: 0.00001997
Iteration 453/1000 | Loss: 0.00014074
Iteration 454/1000 | Loss: 0.00002541
Iteration 455/1000 | Loss: 0.00012690
Iteration 456/1000 | Loss: 0.00004416
Iteration 457/1000 | Loss: 0.00001992
Iteration 458/1000 | Loss: 0.00014238
Iteration 459/1000 | Loss: 0.00003164
Iteration 460/1000 | Loss: 0.00012619
Iteration 461/1000 | Loss: 0.00002436
Iteration 462/1000 | Loss: 0.00006128
Iteration 463/1000 | Loss: 0.00002262
Iteration 464/1000 | Loss: 0.00005897
Iteration 465/1000 | Loss: 0.00002062
Iteration 466/1000 | Loss: 0.00005911
Iteration 467/1000 | Loss: 0.00002304
Iteration 468/1000 | Loss: 0.00005656
Iteration 469/1000 | Loss: 0.00001919
Iteration 470/1000 | Loss: 0.00001819
Iteration 471/1000 | Loss: 0.00001763
Iteration 472/1000 | Loss: 0.00001737
Iteration 473/1000 | Loss: 0.00001723
Iteration 474/1000 | Loss: 0.00001716
Iteration 475/1000 | Loss: 0.00001709
Iteration 476/1000 | Loss: 0.00001706
Iteration 477/1000 | Loss: 0.00001705
Iteration 478/1000 | Loss: 0.00001705
Iteration 479/1000 | Loss: 0.00001703
Iteration 480/1000 | Loss: 0.00001702
Iteration 481/1000 | Loss: 0.00001702
Iteration 482/1000 | Loss: 0.00001702
Iteration 483/1000 | Loss: 0.00001701
Iteration 484/1000 | Loss: 0.00001701
Iteration 485/1000 | Loss: 0.00001699
Iteration 486/1000 | Loss: 0.00001699
Iteration 487/1000 | Loss: 0.00001698
Iteration 488/1000 | Loss: 0.00001698
Iteration 489/1000 | Loss: 0.00001697
Iteration 490/1000 | Loss: 0.00001697
Iteration 491/1000 | Loss: 0.00001697
Iteration 492/1000 | Loss: 0.00001696
Iteration 493/1000 | Loss: 0.00001696
Iteration 494/1000 | Loss: 0.00001695
Iteration 495/1000 | Loss: 0.00001694
Iteration 496/1000 | Loss: 0.00001694
Iteration 497/1000 | Loss: 0.00001694
Iteration 498/1000 | Loss: 0.00001693
Iteration 499/1000 | Loss: 0.00001693
Iteration 500/1000 | Loss: 0.00001693
Iteration 501/1000 | Loss: 0.00001693
Iteration 502/1000 | Loss: 0.00001693
Iteration 503/1000 | Loss: 0.00001693
Iteration 504/1000 | Loss: 0.00001693
Iteration 505/1000 | Loss: 0.00001693
Iteration 506/1000 | Loss: 0.00001693
Iteration 507/1000 | Loss: 0.00001693
Iteration 508/1000 | Loss: 0.00001693
Iteration 509/1000 | Loss: 0.00001693
Iteration 510/1000 | Loss: 0.00001693
Iteration 511/1000 | Loss: 0.00001692
Iteration 512/1000 | Loss: 0.00001692
Iteration 513/1000 | Loss: 0.00001692
Iteration 514/1000 | Loss: 0.00001692
Iteration 515/1000 | Loss: 0.00001692
Iteration 516/1000 | Loss: 0.00001692
Iteration 517/1000 | Loss: 0.00001692
Iteration 518/1000 | Loss: 0.00001692
Iteration 519/1000 | Loss: 0.00001692
Iteration 520/1000 | Loss: 0.00001692
Iteration 521/1000 | Loss: 0.00001691
Iteration 522/1000 | Loss: 0.00001691
Iteration 523/1000 | Loss: 0.00001691
Iteration 524/1000 | Loss: 0.00001690
Iteration 525/1000 | Loss: 0.00001690
Iteration 526/1000 | Loss: 0.00001689
Iteration 527/1000 | Loss: 0.00001689
Iteration 528/1000 | Loss: 0.00001689
Iteration 529/1000 | Loss: 0.00001689
Iteration 530/1000 | Loss: 0.00001688
Iteration 531/1000 | Loss: 0.00001688
Iteration 532/1000 | Loss: 0.00001688
Iteration 533/1000 | Loss: 0.00001688
Iteration 534/1000 | Loss: 0.00001688
Iteration 535/1000 | Loss: 0.00001687
Iteration 536/1000 | Loss: 0.00001687
Iteration 537/1000 | Loss: 0.00001687
Iteration 538/1000 | Loss: 0.00001687
Iteration 539/1000 | Loss: 0.00001687
Iteration 540/1000 | Loss: 0.00001687
Iteration 541/1000 | Loss: 0.00001687
Iteration 542/1000 | Loss: 0.00001687
Iteration 543/1000 | Loss: 0.00001687
Iteration 544/1000 | Loss: 0.00001686
Iteration 545/1000 | Loss: 0.00001686
Iteration 546/1000 | Loss: 0.00001686
Iteration 547/1000 | Loss: 0.00001686
Iteration 548/1000 | Loss: 0.00001686
Iteration 549/1000 | Loss: 0.00001686
Iteration 550/1000 | Loss: 0.00001686
Iteration 551/1000 | Loss: 0.00001686
Iteration 552/1000 | Loss: 0.00001686
Iteration 553/1000 | Loss: 0.00001686
Iteration 554/1000 | Loss: 0.00001686
Iteration 555/1000 | Loss: 0.00001685
Iteration 556/1000 | Loss: 0.00001685
Iteration 557/1000 | Loss: 0.00001685
Iteration 558/1000 | Loss: 0.00001685
Iteration 559/1000 | Loss: 0.00001685
Iteration 560/1000 | Loss: 0.00001685
Iteration 561/1000 | Loss: 0.00001685
Iteration 562/1000 | Loss: 0.00001685
Iteration 563/1000 | Loss: 0.00001685
Iteration 564/1000 | Loss: 0.00001685
Iteration 565/1000 | Loss: 0.00001684
Iteration 566/1000 | Loss: 0.00001684
Iteration 567/1000 | Loss: 0.00001684
Iteration 568/1000 | Loss: 0.00001684
Iteration 569/1000 | Loss: 0.00001684
Iteration 570/1000 | Loss: 0.00001683
Iteration 571/1000 | Loss: 0.00001683
Iteration 572/1000 | Loss: 0.00001683
Iteration 573/1000 | Loss: 0.00001683
Iteration 574/1000 | Loss: 0.00001683
Iteration 575/1000 | Loss: 0.00001683
Iteration 576/1000 | Loss: 0.00001683
Iteration 577/1000 | Loss: 0.00001683
Iteration 578/1000 | Loss: 0.00001683
Iteration 579/1000 | Loss: 0.00001683
Iteration 580/1000 | Loss: 0.00001683
Iteration 581/1000 | Loss: 0.00001683
Iteration 582/1000 | Loss: 0.00001683
Iteration 583/1000 | Loss: 0.00001683
Iteration 584/1000 | Loss: 0.00001683
Iteration 585/1000 | Loss: 0.00001683
Iteration 586/1000 | Loss: 0.00001683
Iteration 587/1000 | Loss: 0.00001683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 587. Stopping optimization.
Last 5 losses: [1.6833337213029154e-05, 1.6833337213029154e-05, 1.6833337213029154e-05, 1.6833337213029154e-05, 1.6833337213029154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6833337213029154e-05

Optimization complete. Final v2v error: 3.4584743976593018 mm

Highest mean error: 4.759629249572754 mm for frame 66

Lowest mean error: 3.217738628387451 mm for frame 200

Saving results

Total time: 578.0431745052338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457618
Iteration 2/25 | Loss: 0.00139813
Iteration 3/25 | Loss: 0.00120712
Iteration 4/25 | Loss: 0.00118962
Iteration 5/25 | Loss: 0.00118600
Iteration 6/25 | Loss: 0.00118501
Iteration 7/25 | Loss: 0.00118494
Iteration 8/25 | Loss: 0.00118494
Iteration 9/25 | Loss: 0.00118494
Iteration 10/25 | Loss: 0.00118494
Iteration 11/25 | Loss: 0.00118491
Iteration 12/25 | Loss: 0.00118491
Iteration 13/25 | Loss: 0.00118491
Iteration 14/25 | Loss: 0.00118491
Iteration 15/25 | Loss: 0.00118491
Iteration 16/25 | Loss: 0.00118491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011849116999655962, 0.0011849116999655962, 0.0011849116999655962, 0.0011849116999655962, 0.0011849116999655962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011849116999655962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46671343
Iteration 2/25 | Loss: 0.00070448
Iteration 3/25 | Loss: 0.00070448
Iteration 4/25 | Loss: 0.00070448
Iteration 5/25 | Loss: 0.00070448
Iteration 6/25 | Loss: 0.00070448
Iteration 7/25 | Loss: 0.00070448
Iteration 8/25 | Loss: 0.00070448
Iteration 9/25 | Loss: 0.00070448
Iteration 10/25 | Loss: 0.00070448
Iteration 11/25 | Loss: 0.00070447
Iteration 12/25 | Loss: 0.00070447
Iteration 13/25 | Loss: 0.00070447
Iteration 14/25 | Loss: 0.00070447
Iteration 15/25 | Loss: 0.00070447
Iteration 16/25 | Loss: 0.00070447
Iteration 17/25 | Loss: 0.00070447
Iteration 18/25 | Loss: 0.00070447
Iteration 19/25 | Loss: 0.00070447
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000704474572557956, 0.000704474572557956, 0.000704474572557956, 0.000704474572557956, 0.000704474572557956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000704474572557956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070447
Iteration 2/1000 | Loss: 0.00004285
Iteration 3/1000 | Loss: 0.00002699
Iteration 4/1000 | Loss: 0.00001945
Iteration 5/1000 | Loss: 0.00001788
Iteration 6/1000 | Loss: 0.00001690
Iteration 7/1000 | Loss: 0.00001617
Iteration 8/1000 | Loss: 0.00001575
Iteration 9/1000 | Loss: 0.00001534
Iteration 10/1000 | Loss: 0.00001507
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001471
Iteration 13/1000 | Loss: 0.00001466
Iteration 14/1000 | Loss: 0.00001460
Iteration 15/1000 | Loss: 0.00001455
Iteration 16/1000 | Loss: 0.00001449
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001445
Iteration 19/1000 | Loss: 0.00001445
Iteration 20/1000 | Loss: 0.00001441
Iteration 21/1000 | Loss: 0.00001439
Iteration 22/1000 | Loss: 0.00001434
Iteration 23/1000 | Loss: 0.00001430
Iteration 24/1000 | Loss: 0.00001430
Iteration 25/1000 | Loss: 0.00001429
Iteration 26/1000 | Loss: 0.00001429
Iteration 27/1000 | Loss: 0.00001428
Iteration 28/1000 | Loss: 0.00001428
Iteration 29/1000 | Loss: 0.00001427
Iteration 30/1000 | Loss: 0.00001427
Iteration 31/1000 | Loss: 0.00001427
Iteration 32/1000 | Loss: 0.00001426
Iteration 33/1000 | Loss: 0.00001426
Iteration 34/1000 | Loss: 0.00001421
Iteration 35/1000 | Loss: 0.00001421
Iteration 36/1000 | Loss: 0.00001420
Iteration 37/1000 | Loss: 0.00001420
Iteration 38/1000 | Loss: 0.00001420
Iteration 39/1000 | Loss: 0.00001419
Iteration 40/1000 | Loss: 0.00001419
Iteration 41/1000 | Loss: 0.00001418
Iteration 42/1000 | Loss: 0.00001418
Iteration 43/1000 | Loss: 0.00001418
Iteration 44/1000 | Loss: 0.00001417
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001417
Iteration 49/1000 | Loss: 0.00001417
Iteration 50/1000 | Loss: 0.00001416
Iteration 51/1000 | Loss: 0.00001416
Iteration 52/1000 | Loss: 0.00001416
Iteration 53/1000 | Loss: 0.00001415
Iteration 54/1000 | Loss: 0.00001415
Iteration 55/1000 | Loss: 0.00001415
Iteration 56/1000 | Loss: 0.00001414
Iteration 57/1000 | Loss: 0.00001414
Iteration 58/1000 | Loss: 0.00001414
Iteration 59/1000 | Loss: 0.00001414
Iteration 60/1000 | Loss: 0.00001414
Iteration 61/1000 | Loss: 0.00001414
Iteration 62/1000 | Loss: 0.00001414
Iteration 63/1000 | Loss: 0.00001414
Iteration 64/1000 | Loss: 0.00001413
Iteration 65/1000 | Loss: 0.00001413
Iteration 66/1000 | Loss: 0.00001413
Iteration 67/1000 | Loss: 0.00001412
Iteration 68/1000 | Loss: 0.00001411
Iteration 69/1000 | Loss: 0.00001411
Iteration 70/1000 | Loss: 0.00001411
Iteration 71/1000 | Loss: 0.00001410
Iteration 72/1000 | Loss: 0.00001410
Iteration 73/1000 | Loss: 0.00001410
Iteration 74/1000 | Loss: 0.00001410
Iteration 75/1000 | Loss: 0.00001410
Iteration 76/1000 | Loss: 0.00001409
Iteration 77/1000 | Loss: 0.00001409
Iteration 78/1000 | Loss: 0.00001408
Iteration 79/1000 | Loss: 0.00001406
Iteration 80/1000 | Loss: 0.00001406
Iteration 81/1000 | Loss: 0.00001406
Iteration 82/1000 | Loss: 0.00001406
Iteration 83/1000 | Loss: 0.00001405
Iteration 84/1000 | Loss: 0.00001405
Iteration 85/1000 | Loss: 0.00001405
Iteration 86/1000 | Loss: 0.00001405
Iteration 87/1000 | Loss: 0.00001404
Iteration 88/1000 | Loss: 0.00001404
Iteration 89/1000 | Loss: 0.00001404
Iteration 90/1000 | Loss: 0.00001403
Iteration 91/1000 | Loss: 0.00001403
Iteration 92/1000 | Loss: 0.00001403
Iteration 93/1000 | Loss: 0.00001403
Iteration 94/1000 | Loss: 0.00001402
Iteration 95/1000 | Loss: 0.00001402
Iteration 96/1000 | Loss: 0.00001402
Iteration 97/1000 | Loss: 0.00001402
Iteration 98/1000 | Loss: 0.00001402
Iteration 99/1000 | Loss: 0.00001401
Iteration 100/1000 | Loss: 0.00001401
Iteration 101/1000 | Loss: 0.00001401
Iteration 102/1000 | Loss: 0.00001401
Iteration 103/1000 | Loss: 0.00001401
Iteration 104/1000 | Loss: 0.00001401
Iteration 105/1000 | Loss: 0.00001400
Iteration 106/1000 | Loss: 0.00001400
Iteration 107/1000 | Loss: 0.00001400
Iteration 108/1000 | Loss: 0.00001400
Iteration 109/1000 | Loss: 0.00001400
Iteration 110/1000 | Loss: 0.00001400
Iteration 111/1000 | Loss: 0.00001400
Iteration 112/1000 | Loss: 0.00001400
Iteration 113/1000 | Loss: 0.00001400
Iteration 114/1000 | Loss: 0.00001400
Iteration 115/1000 | Loss: 0.00001400
Iteration 116/1000 | Loss: 0.00001399
Iteration 117/1000 | Loss: 0.00001399
Iteration 118/1000 | Loss: 0.00001399
Iteration 119/1000 | Loss: 0.00001399
Iteration 120/1000 | Loss: 0.00001398
Iteration 121/1000 | Loss: 0.00001398
Iteration 122/1000 | Loss: 0.00001398
Iteration 123/1000 | Loss: 0.00001398
Iteration 124/1000 | Loss: 0.00001397
Iteration 125/1000 | Loss: 0.00001397
Iteration 126/1000 | Loss: 0.00001397
Iteration 127/1000 | Loss: 0.00001397
Iteration 128/1000 | Loss: 0.00001397
Iteration 129/1000 | Loss: 0.00001397
Iteration 130/1000 | Loss: 0.00001397
Iteration 131/1000 | Loss: 0.00001397
Iteration 132/1000 | Loss: 0.00001397
Iteration 133/1000 | Loss: 0.00001397
Iteration 134/1000 | Loss: 0.00001396
Iteration 135/1000 | Loss: 0.00001396
Iteration 136/1000 | Loss: 0.00001396
Iteration 137/1000 | Loss: 0.00001396
Iteration 138/1000 | Loss: 0.00001396
Iteration 139/1000 | Loss: 0.00001396
Iteration 140/1000 | Loss: 0.00001396
Iteration 141/1000 | Loss: 0.00001396
Iteration 142/1000 | Loss: 0.00001395
Iteration 143/1000 | Loss: 0.00001395
Iteration 144/1000 | Loss: 0.00001395
Iteration 145/1000 | Loss: 0.00001395
Iteration 146/1000 | Loss: 0.00001395
Iteration 147/1000 | Loss: 0.00001395
Iteration 148/1000 | Loss: 0.00001395
Iteration 149/1000 | Loss: 0.00001395
Iteration 150/1000 | Loss: 0.00001394
Iteration 151/1000 | Loss: 0.00001394
Iteration 152/1000 | Loss: 0.00001394
Iteration 153/1000 | Loss: 0.00001394
Iteration 154/1000 | Loss: 0.00001394
Iteration 155/1000 | Loss: 0.00001394
Iteration 156/1000 | Loss: 0.00001394
Iteration 157/1000 | Loss: 0.00001394
Iteration 158/1000 | Loss: 0.00001393
Iteration 159/1000 | Loss: 0.00001393
Iteration 160/1000 | Loss: 0.00001393
Iteration 161/1000 | Loss: 0.00001393
Iteration 162/1000 | Loss: 0.00001393
Iteration 163/1000 | Loss: 0.00001393
Iteration 164/1000 | Loss: 0.00001393
Iteration 165/1000 | Loss: 0.00001393
Iteration 166/1000 | Loss: 0.00001393
Iteration 167/1000 | Loss: 0.00001393
Iteration 168/1000 | Loss: 0.00001393
Iteration 169/1000 | Loss: 0.00001393
Iteration 170/1000 | Loss: 0.00001393
Iteration 171/1000 | Loss: 0.00001393
Iteration 172/1000 | Loss: 0.00001393
Iteration 173/1000 | Loss: 0.00001393
Iteration 174/1000 | Loss: 0.00001393
Iteration 175/1000 | Loss: 0.00001393
Iteration 176/1000 | Loss: 0.00001393
Iteration 177/1000 | Loss: 0.00001393
Iteration 178/1000 | Loss: 0.00001393
Iteration 179/1000 | Loss: 0.00001393
Iteration 180/1000 | Loss: 0.00001393
Iteration 181/1000 | Loss: 0.00001393
Iteration 182/1000 | Loss: 0.00001393
Iteration 183/1000 | Loss: 0.00001393
Iteration 184/1000 | Loss: 0.00001393
Iteration 185/1000 | Loss: 0.00001393
Iteration 186/1000 | Loss: 0.00001393
Iteration 187/1000 | Loss: 0.00001393
Iteration 188/1000 | Loss: 0.00001393
Iteration 189/1000 | Loss: 0.00001393
Iteration 190/1000 | Loss: 0.00001393
Iteration 191/1000 | Loss: 0.00001393
Iteration 192/1000 | Loss: 0.00001393
Iteration 193/1000 | Loss: 0.00001393
Iteration 194/1000 | Loss: 0.00001393
Iteration 195/1000 | Loss: 0.00001393
Iteration 196/1000 | Loss: 0.00001393
Iteration 197/1000 | Loss: 0.00001393
Iteration 198/1000 | Loss: 0.00001393
Iteration 199/1000 | Loss: 0.00001393
Iteration 200/1000 | Loss: 0.00001393
Iteration 201/1000 | Loss: 0.00001393
Iteration 202/1000 | Loss: 0.00001393
Iteration 203/1000 | Loss: 0.00001393
Iteration 204/1000 | Loss: 0.00001393
Iteration 205/1000 | Loss: 0.00001393
Iteration 206/1000 | Loss: 0.00001393
Iteration 207/1000 | Loss: 0.00001393
Iteration 208/1000 | Loss: 0.00001393
Iteration 209/1000 | Loss: 0.00001393
Iteration 210/1000 | Loss: 0.00001393
Iteration 211/1000 | Loss: 0.00001393
Iteration 212/1000 | Loss: 0.00001393
Iteration 213/1000 | Loss: 0.00001393
Iteration 214/1000 | Loss: 0.00001393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.3925208804721478e-05, 1.3925208804721478e-05, 1.3925208804721478e-05, 1.3925208804721478e-05, 1.3925208804721478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3925208804721478e-05

Optimization complete. Final v2v error: 3.119209051132202 mm

Highest mean error: 4.4420905113220215 mm for frame 67

Lowest mean error: 2.603147029876709 mm for frame 6

Saving results

Total time: 43.62842082977295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771132
Iteration 2/25 | Loss: 0.00171443
Iteration 3/25 | Loss: 0.00134156
Iteration 4/25 | Loss: 0.00130961
Iteration 5/25 | Loss: 0.00130734
Iteration 6/25 | Loss: 0.00130734
Iteration 7/25 | Loss: 0.00130734
Iteration 8/25 | Loss: 0.00130734
Iteration 9/25 | Loss: 0.00130734
Iteration 10/25 | Loss: 0.00130734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013073449954390526, 0.0013073449954390526, 0.0013073449954390526, 0.0013073449954390526, 0.0013073449954390526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013073449954390526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35694325
Iteration 2/25 | Loss: 0.00077126
Iteration 3/25 | Loss: 0.00077124
Iteration 4/25 | Loss: 0.00077124
Iteration 5/25 | Loss: 0.00077124
Iteration 6/25 | Loss: 0.00077124
Iteration 7/25 | Loss: 0.00077124
Iteration 8/25 | Loss: 0.00077124
Iteration 9/25 | Loss: 0.00077124
Iteration 10/25 | Loss: 0.00077124
Iteration 11/25 | Loss: 0.00077124
Iteration 12/25 | Loss: 0.00077124
Iteration 13/25 | Loss: 0.00077124
Iteration 14/25 | Loss: 0.00077124
Iteration 15/25 | Loss: 0.00077124
Iteration 16/25 | Loss: 0.00077124
Iteration 17/25 | Loss: 0.00077124
Iteration 18/25 | Loss: 0.00077124
Iteration 19/25 | Loss: 0.00077124
Iteration 20/25 | Loss: 0.00077124
Iteration 21/25 | Loss: 0.00077124
Iteration 22/25 | Loss: 0.00077124
Iteration 23/25 | Loss: 0.00077124
Iteration 24/25 | Loss: 0.00077124
Iteration 25/25 | Loss: 0.00077124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077124
Iteration 2/1000 | Loss: 0.00004606
Iteration 3/1000 | Loss: 0.00003030
Iteration 4/1000 | Loss: 0.00002698
Iteration 5/1000 | Loss: 0.00002590
Iteration 6/1000 | Loss: 0.00002517
Iteration 7/1000 | Loss: 0.00002481
Iteration 8/1000 | Loss: 0.00002446
Iteration 9/1000 | Loss: 0.00002423
Iteration 10/1000 | Loss: 0.00002397
Iteration 11/1000 | Loss: 0.00002386
Iteration 12/1000 | Loss: 0.00002378
Iteration 13/1000 | Loss: 0.00002362
Iteration 14/1000 | Loss: 0.00002361
Iteration 15/1000 | Loss: 0.00002356
Iteration 16/1000 | Loss: 0.00002356
Iteration 17/1000 | Loss: 0.00002349
Iteration 18/1000 | Loss: 0.00002336
Iteration 19/1000 | Loss: 0.00002336
Iteration 20/1000 | Loss: 0.00002336
Iteration 21/1000 | Loss: 0.00002335
Iteration 22/1000 | Loss: 0.00002335
Iteration 23/1000 | Loss: 0.00002329
Iteration 24/1000 | Loss: 0.00002328
Iteration 25/1000 | Loss: 0.00002327
Iteration 26/1000 | Loss: 0.00002326
Iteration 27/1000 | Loss: 0.00002326
Iteration 28/1000 | Loss: 0.00002326
Iteration 29/1000 | Loss: 0.00002326
Iteration 30/1000 | Loss: 0.00002326
Iteration 31/1000 | Loss: 0.00002326
Iteration 32/1000 | Loss: 0.00002326
Iteration 33/1000 | Loss: 0.00002326
Iteration 34/1000 | Loss: 0.00002326
Iteration 35/1000 | Loss: 0.00002326
Iteration 36/1000 | Loss: 0.00002326
Iteration 37/1000 | Loss: 0.00002325
Iteration 38/1000 | Loss: 0.00002325
Iteration 39/1000 | Loss: 0.00002325
Iteration 40/1000 | Loss: 0.00002325
Iteration 41/1000 | Loss: 0.00002324
Iteration 42/1000 | Loss: 0.00002324
Iteration 43/1000 | Loss: 0.00002324
Iteration 44/1000 | Loss: 0.00002324
Iteration 45/1000 | Loss: 0.00002324
Iteration 46/1000 | Loss: 0.00002323
Iteration 47/1000 | Loss: 0.00002323
Iteration 48/1000 | Loss: 0.00002322
Iteration 49/1000 | Loss: 0.00002322
Iteration 50/1000 | Loss: 0.00002322
Iteration 51/1000 | Loss: 0.00002322
Iteration 52/1000 | Loss: 0.00002322
Iteration 53/1000 | Loss: 0.00002322
Iteration 54/1000 | Loss: 0.00002322
Iteration 55/1000 | Loss: 0.00002322
Iteration 56/1000 | Loss: 0.00002322
Iteration 57/1000 | Loss: 0.00002322
Iteration 58/1000 | Loss: 0.00002322
Iteration 59/1000 | Loss: 0.00002322
Iteration 60/1000 | Loss: 0.00002322
Iteration 61/1000 | Loss: 0.00002321
Iteration 62/1000 | Loss: 0.00002321
Iteration 63/1000 | Loss: 0.00002321
Iteration 64/1000 | Loss: 0.00002321
Iteration 65/1000 | Loss: 0.00002321
Iteration 66/1000 | Loss: 0.00002320
Iteration 67/1000 | Loss: 0.00002320
Iteration 68/1000 | Loss: 0.00002320
Iteration 69/1000 | Loss: 0.00002320
Iteration 70/1000 | Loss: 0.00002320
Iteration 71/1000 | Loss: 0.00002320
Iteration 72/1000 | Loss: 0.00002320
Iteration 73/1000 | Loss: 0.00002320
Iteration 74/1000 | Loss: 0.00002320
Iteration 75/1000 | Loss: 0.00002320
Iteration 76/1000 | Loss: 0.00002320
Iteration 77/1000 | Loss: 0.00002320
Iteration 78/1000 | Loss: 0.00002320
Iteration 79/1000 | Loss: 0.00002320
Iteration 80/1000 | Loss: 0.00002320
Iteration 81/1000 | Loss: 0.00002320
Iteration 82/1000 | Loss: 0.00002320
Iteration 83/1000 | Loss: 0.00002320
Iteration 84/1000 | Loss: 0.00002320
Iteration 85/1000 | Loss: 0.00002320
Iteration 86/1000 | Loss: 0.00002319
Iteration 87/1000 | Loss: 0.00002319
Iteration 88/1000 | Loss: 0.00002319
Iteration 89/1000 | Loss: 0.00002319
Iteration 90/1000 | Loss: 0.00002319
Iteration 91/1000 | Loss: 0.00002319
Iteration 92/1000 | Loss: 0.00002318
Iteration 93/1000 | Loss: 0.00002318
Iteration 94/1000 | Loss: 0.00002318
Iteration 95/1000 | Loss: 0.00002318
Iteration 96/1000 | Loss: 0.00002318
Iteration 97/1000 | Loss: 0.00002318
Iteration 98/1000 | Loss: 0.00002318
Iteration 99/1000 | Loss: 0.00002318
Iteration 100/1000 | Loss: 0.00002318
Iteration 101/1000 | Loss: 0.00002318
Iteration 102/1000 | Loss: 0.00002318
Iteration 103/1000 | Loss: 0.00002318
Iteration 104/1000 | Loss: 0.00002318
Iteration 105/1000 | Loss: 0.00002318
Iteration 106/1000 | Loss: 0.00002318
Iteration 107/1000 | Loss: 0.00002318
Iteration 108/1000 | Loss: 0.00002318
Iteration 109/1000 | Loss: 0.00002317
Iteration 110/1000 | Loss: 0.00002317
Iteration 111/1000 | Loss: 0.00002317
Iteration 112/1000 | Loss: 0.00002317
Iteration 113/1000 | Loss: 0.00002317
Iteration 114/1000 | Loss: 0.00002317
Iteration 115/1000 | Loss: 0.00002317
Iteration 116/1000 | Loss: 0.00002317
Iteration 117/1000 | Loss: 0.00002317
Iteration 118/1000 | Loss: 0.00002317
Iteration 119/1000 | Loss: 0.00002317
Iteration 120/1000 | Loss: 0.00002317
Iteration 121/1000 | Loss: 0.00002316
Iteration 122/1000 | Loss: 0.00002316
Iteration 123/1000 | Loss: 0.00002316
Iteration 124/1000 | Loss: 0.00002316
Iteration 125/1000 | Loss: 0.00002316
Iteration 126/1000 | Loss: 0.00002316
Iteration 127/1000 | Loss: 0.00002316
Iteration 128/1000 | Loss: 0.00002316
Iteration 129/1000 | Loss: 0.00002316
Iteration 130/1000 | Loss: 0.00002316
Iteration 131/1000 | Loss: 0.00002316
Iteration 132/1000 | Loss: 0.00002316
Iteration 133/1000 | Loss: 0.00002316
Iteration 134/1000 | Loss: 0.00002316
Iteration 135/1000 | Loss: 0.00002316
Iteration 136/1000 | Loss: 0.00002316
Iteration 137/1000 | Loss: 0.00002316
Iteration 138/1000 | Loss: 0.00002316
Iteration 139/1000 | Loss: 0.00002316
Iteration 140/1000 | Loss: 0.00002316
Iteration 141/1000 | Loss: 0.00002316
Iteration 142/1000 | Loss: 0.00002316
Iteration 143/1000 | Loss: 0.00002316
Iteration 144/1000 | Loss: 0.00002316
Iteration 145/1000 | Loss: 0.00002316
Iteration 146/1000 | Loss: 0.00002316
Iteration 147/1000 | Loss: 0.00002316
Iteration 148/1000 | Loss: 0.00002316
Iteration 149/1000 | Loss: 0.00002316
Iteration 150/1000 | Loss: 0.00002316
Iteration 151/1000 | Loss: 0.00002316
Iteration 152/1000 | Loss: 0.00002316
Iteration 153/1000 | Loss: 0.00002316
Iteration 154/1000 | Loss: 0.00002316
Iteration 155/1000 | Loss: 0.00002316
Iteration 156/1000 | Loss: 0.00002316
Iteration 157/1000 | Loss: 0.00002316
Iteration 158/1000 | Loss: 0.00002316
Iteration 159/1000 | Loss: 0.00002316
Iteration 160/1000 | Loss: 0.00002316
Iteration 161/1000 | Loss: 0.00002316
Iteration 162/1000 | Loss: 0.00002316
Iteration 163/1000 | Loss: 0.00002316
Iteration 164/1000 | Loss: 0.00002316
Iteration 165/1000 | Loss: 0.00002316
Iteration 166/1000 | Loss: 0.00002316
Iteration 167/1000 | Loss: 0.00002316
Iteration 168/1000 | Loss: 0.00002316
Iteration 169/1000 | Loss: 0.00002316
Iteration 170/1000 | Loss: 0.00002316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.3162703655543737e-05, 2.3162703655543737e-05, 2.3162703655543737e-05, 2.3162703655543737e-05, 2.3162703655543737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3162703655543737e-05

Optimization complete. Final v2v error: 4.047879219055176 mm

Highest mean error: 4.239790439605713 mm for frame 53

Lowest mean error: 3.8889353275299072 mm for frame 25

Saving results

Total time: 35.45323872566223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799249
Iteration 2/25 | Loss: 0.00155294
Iteration 3/25 | Loss: 0.00137798
Iteration 4/25 | Loss: 0.00136298
Iteration 5/25 | Loss: 0.00135976
Iteration 6/25 | Loss: 0.00135976
Iteration 7/25 | Loss: 0.00135976
Iteration 8/25 | Loss: 0.00135976
Iteration 9/25 | Loss: 0.00135976
Iteration 10/25 | Loss: 0.00135976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013597611105069518, 0.0013597611105069518, 0.0013597611105069518, 0.0013597611105069518, 0.0013597611105069518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013597611105069518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.00177574
Iteration 2/25 | Loss: 0.00121888
Iteration 3/25 | Loss: 0.00121888
Iteration 4/25 | Loss: 0.00121888
Iteration 5/25 | Loss: 0.00121888
Iteration 6/25 | Loss: 0.00121888
Iteration 7/25 | Loss: 0.00121888
Iteration 8/25 | Loss: 0.00121888
Iteration 9/25 | Loss: 0.00121888
Iteration 10/25 | Loss: 0.00121888
Iteration 11/25 | Loss: 0.00121888
Iteration 12/25 | Loss: 0.00121888
Iteration 13/25 | Loss: 0.00121888
Iteration 14/25 | Loss: 0.00121888
Iteration 15/25 | Loss: 0.00121888
Iteration 16/25 | Loss: 0.00121888
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012188814580440521, 0.0012188814580440521, 0.0012188814580440521, 0.0012188814580440521, 0.0012188814580440521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012188814580440521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121888
Iteration 2/1000 | Loss: 0.00005824
Iteration 3/1000 | Loss: 0.00003387
Iteration 4/1000 | Loss: 0.00002815
Iteration 5/1000 | Loss: 0.00002633
Iteration 6/1000 | Loss: 0.00002487
Iteration 7/1000 | Loss: 0.00002398
Iteration 8/1000 | Loss: 0.00002350
Iteration 9/1000 | Loss: 0.00002321
Iteration 10/1000 | Loss: 0.00002300
Iteration 11/1000 | Loss: 0.00002285
Iteration 12/1000 | Loss: 0.00002277
Iteration 13/1000 | Loss: 0.00002273
Iteration 14/1000 | Loss: 0.00002273
Iteration 15/1000 | Loss: 0.00002272
Iteration 16/1000 | Loss: 0.00002271
Iteration 17/1000 | Loss: 0.00002271
Iteration 18/1000 | Loss: 0.00002270
Iteration 19/1000 | Loss: 0.00002262
Iteration 20/1000 | Loss: 0.00002260
Iteration 21/1000 | Loss: 0.00002256
Iteration 22/1000 | Loss: 0.00002256
Iteration 23/1000 | Loss: 0.00002255
Iteration 24/1000 | Loss: 0.00002253
Iteration 25/1000 | Loss: 0.00002253
Iteration 26/1000 | Loss: 0.00002247
Iteration 27/1000 | Loss: 0.00002245
Iteration 28/1000 | Loss: 0.00002244
Iteration 29/1000 | Loss: 0.00002244
Iteration 30/1000 | Loss: 0.00002243
Iteration 31/1000 | Loss: 0.00002243
Iteration 32/1000 | Loss: 0.00002243
Iteration 33/1000 | Loss: 0.00002243
Iteration 34/1000 | Loss: 0.00002243
Iteration 35/1000 | Loss: 0.00002242
Iteration 36/1000 | Loss: 0.00002240
Iteration 37/1000 | Loss: 0.00002240
Iteration 38/1000 | Loss: 0.00002239
Iteration 39/1000 | Loss: 0.00002239
Iteration 40/1000 | Loss: 0.00002239
Iteration 41/1000 | Loss: 0.00002239
Iteration 42/1000 | Loss: 0.00002239
Iteration 43/1000 | Loss: 0.00002239
Iteration 44/1000 | Loss: 0.00002238
Iteration 45/1000 | Loss: 0.00002238
Iteration 46/1000 | Loss: 0.00002238
Iteration 47/1000 | Loss: 0.00002238
Iteration 48/1000 | Loss: 0.00002238
Iteration 49/1000 | Loss: 0.00002238
Iteration 50/1000 | Loss: 0.00002238
Iteration 51/1000 | Loss: 0.00002238
Iteration 52/1000 | Loss: 0.00002238
Iteration 53/1000 | Loss: 0.00002238
Iteration 54/1000 | Loss: 0.00002238
Iteration 55/1000 | Loss: 0.00002238
Iteration 56/1000 | Loss: 0.00002238
Iteration 57/1000 | Loss: 0.00002238
Iteration 58/1000 | Loss: 0.00002238
Iteration 59/1000 | Loss: 0.00002238
Iteration 60/1000 | Loss: 0.00002238
Iteration 61/1000 | Loss: 0.00002238
Iteration 62/1000 | Loss: 0.00002238
Iteration 63/1000 | Loss: 0.00002238
Iteration 64/1000 | Loss: 0.00002238
Iteration 65/1000 | Loss: 0.00002238
Iteration 66/1000 | Loss: 0.00002238
Iteration 67/1000 | Loss: 0.00002238
Iteration 68/1000 | Loss: 0.00002238
Iteration 69/1000 | Loss: 0.00002238
Iteration 70/1000 | Loss: 0.00002238
Iteration 71/1000 | Loss: 0.00002238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [2.237888111267239e-05, 2.237888111267239e-05, 2.237888111267239e-05, 2.237888111267239e-05, 2.237888111267239e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.237888111267239e-05

Optimization complete. Final v2v error: 4.000978469848633 mm

Highest mean error: 4.5211663246154785 mm for frame 114

Lowest mean error: 3.5927681922912598 mm for frame 173

Saving results

Total time: 29.915780067443848
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818681
Iteration 2/25 | Loss: 0.00170534
Iteration 3/25 | Loss: 0.00137920
Iteration 4/25 | Loss: 0.00133600
Iteration 5/25 | Loss: 0.00130935
Iteration 6/25 | Loss: 0.00133403
Iteration 7/25 | Loss: 0.00132868
Iteration 8/25 | Loss: 0.00129277
Iteration 9/25 | Loss: 0.00127573
Iteration 10/25 | Loss: 0.00126207
Iteration 11/25 | Loss: 0.00125910
Iteration 12/25 | Loss: 0.00126230
Iteration 13/25 | Loss: 0.00126532
Iteration 14/25 | Loss: 0.00126046
Iteration 15/25 | Loss: 0.00125682
Iteration 16/25 | Loss: 0.00125469
Iteration 17/25 | Loss: 0.00125415
Iteration 18/25 | Loss: 0.00125372
Iteration 19/25 | Loss: 0.00125439
Iteration 20/25 | Loss: 0.00125326
Iteration 21/25 | Loss: 0.00125291
Iteration 22/25 | Loss: 0.00125361
Iteration 23/25 | Loss: 0.00125336
Iteration 24/25 | Loss: 0.00125395
Iteration 25/25 | Loss: 0.00125352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26823235
Iteration 2/25 | Loss: 0.00074410
Iteration 3/25 | Loss: 0.00074410
Iteration 4/25 | Loss: 0.00074410
Iteration 5/25 | Loss: 0.00074410
Iteration 6/25 | Loss: 0.00074410
Iteration 7/25 | Loss: 0.00074410
Iteration 8/25 | Loss: 0.00074410
Iteration 9/25 | Loss: 0.00074410
Iteration 10/25 | Loss: 0.00074410
Iteration 11/25 | Loss: 0.00074410
Iteration 12/25 | Loss: 0.00074410
Iteration 13/25 | Loss: 0.00074410
Iteration 14/25 | Loss: 0.00074410
Iteration 15/25 | Loss: 0.00074410
Iteration 16/25 | Loss: 0.00074410
Iteration 17/25 | Loss: 0.00074410
Iteration 18/25 | Loss: 0.00074410
Iteration 19/25 | Loss: 0.00074410
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007441006018780172, 0.0007441006018780172, 0.0007441006018780172, 0.0007441006018780172, 0.0007441006018780172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007441006018780172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074410
Iteration 2/1000 | Loss: 0.00005810
Iteration 3/1000 | Loss: 0.00003339
Iteration 4/1000 | Loss: 0.00003717
Iteration 5/1000 | Loss: 0.00002812
Iteration 6/1000 | Loss: 0.00003691
Iteration 7/1000 | Loss: 0.00003126
Iteration 8/1000 | Loss: 0.00002802
Iteration 9/1000 | Loss: 0.00003494
Iteration 10/1000 | Loss: 0.00003362
Iteration 11/1000 | Loss: 0.00002775
Iteration 12/1000 | Loss: 0.00002878
Iteration 13/1000 | Loss: 0.00003416
Iteration 14/1000 | Loss: 0.00002810
Iteration 15/1000 | Loss: 0.00002624
Iteration 16/1000 | Loss: 0.00003243
Iteration 17/1000 | Loss: 0.00003423
Iteration 18/1000 | Loss: 0.00003192
Iteration 19/1000 | Loss: 0.00003483
Iteration 20/1000 | Loss: 0.00004294
Iteration 21/1000 | Loss: 0.00003117
Iteration 22/1000 | Loss: 0.00003654
Iteration 23/1000 | Loss: 0.00003980
Iteration 24/1000 | Loss: 0.00003685
Iteration 25/1000 | Loss: 0.00004028
Iteration 26/1000 | Loss: 0.00003716
Iteration 27/1000 | Loss: 0.00004044
Iteration 28/1000 | Loss: 0.00003633
Iteration 29/1000 | Loss: 0.00004030
Iteration 30/1000 | Loss: 0.00003623
Iteration 31/1000 | Loss: 0.00004001
Iteration 32/1000 | Loss: 0.00003597
Iteration 33/1000 | Loss: 0.00003595
Iteration 34/1000 | Loss: 0.00004129
Iteration 35/1000 | Loss: 0.00004160
Iteration 36/1000 | Loss: 0.00004084
Iteration 37/1000 | Loss: 0.00003297
Iteration 38/1000 | Loss: 0.00003707
Iteration 39/1000 | Loss: 0.00003984
Iteration 40/1000 | Loss: 0.00002373
Iteration 41/1000 | Loss: 0.00002289
Iteration 42/1000 | Loss: 0.00002235
Iteration 43/1000 | Loss: 0.00002208
Iteration 44/1000 | Loss: 0.00002191
Iteration 45/1000 | Loss: 0.00002184
Iteration 46/1000 | Loss: 0.00002183
Iteration 47/1000 | Loss: 0.00002182
Iteration 48/1000 | Loss: 0.00002182
Iteration 49/1000 | Loss: 0.00002181
Iteration 50/1000 | Loss: 0.00002180
Iteration 51/1000 | Loss: 0.00002179
Iteration 52/1000 | Loss: 0.00002178
Iteration 53/1000 | Loss: 0.00002175
Iteration 54/1000 | Loss: 0.00002172
Iteration 55/1000 | Loss: 0.00002171
Iteration 56/1000 | Loss: 0.00002169
Iteration 57/1000 | Loss: 0.00002167
Iteration 58/1000 | Loss: 0.00002166
Iteration 59/1000 | Loss: 0.00002165
Iteration 60/1000 | Loss: 0.00002164
Iteration 61/1000 | Loss: 0.00002163
Iteration 62/1000 | Loss: 0.00002163
Iteration 63/1000 | Loss: 0.00002163
Iteration 64/1000 | Loss: 0.00002163
Iteration 65/1000 | Loss: 0.00002163
Iteration 66/1000 | Loss: 0.00002160
Iteration 67/1000 | Loss: 0.00002160
Iteration 68/1000 | Loss: 0.00002158
Iteration 69/1000 | Loss: 0.00002158
Iteration 70/1000 | Loss: 0.00002158
Iteration 71/1000 | Loss: 0.00002158
Iteration 72/1000 | Loss: 0.00002157
Iteration 73/1000 | Loss: 0.00002157
Iteration 74/1000 | Loss: 0.00002156
Iteration 75/1000 | Loss: 0.00002156
Iteration 76/1000 | Loss: 0.00002156
Iteration 77/1000 | Loss: 0.00002156
Iteration 78/1000 | Loss: 0.00002155
Iteration 79/1000 | Loss: 0.00002154
Iteration 80/1000 | Loss: 0.00002150
Iteration 81/1000 | Loss: 0.00002149
Iteration 82/1000 | Loss: 0.00002147
Iteration 83/1000 | Loss: 0.00002147
Iteration 84/1000 | Loss: 0.00002146
Iteration 85/1000 | Loss: 0.00002146
Iteration 86/1000 | Loss: 0.00002146
Iteration 87/1000 | Loss: 0.00002146
Iteration 88/1000 | Loss: 0.00002146
Iteration 89/1000 | Loss: 0.00002146
Iteration 90/1000 | Loss: 0.00002146
Iteration 91/1000 | Loss: 0.00002146
Iteration 92/1000 | Loss: 0.00002146
Iteration 93/1000 | Loss: 0.00002145
Iteration 94/1000 | Loss: 0.00002145
Iteration 95/1000 | Loss: 0.00002144
Iteration 96/1000 | Loss: 0.00002144
Iteration 97/1000 | Loss: 0.00002144
Iteration 98/1000 | Loss: 0.00002144
Iteration 99/1000 | Loss: 0.00002143
Iteration 100/1000 | Loss: 0.00002143
Iteration 101/1000 | Loss: 0.00002143
Iteration 102/1000 | Loss: 0.00002142
Iteration 103/1000 | Loss: 0.00002142
Iteration 104/1000 | Loss: 0.00002142
Iteration 105/1000 | Loss: 0.00002140
Iteration 106/1000 | Loss: 0.00002139
Iteration 107/1000 | Loss: 0.00002139
Iteration 108/1000 | Loss: 0.00002139
Iteration 109/1000 | Loss: 0.00002139
Iteration 110/1000 | Loss: 0.00002139
Iteration 111/1000 | Loss: 0.00002139
Iteration 112/1000 | Loss: 0.00002138
Iteration 113/1000 | Loss: 0.00002138
Iteration 114/1000 | Loss: 0.00002138
Iteration 115/1000 | Loss: 0.00002138
Iteration 116/1000 | Loss: 0.00002138
Iteration 117/1000 | Loss: 0.00002138
Iteration 118/1000 | Loss: 0.00002138
Iteration 119/1000 | Loss: 0.00002138
Iteration 120/1000 | Loss: 0.00002137
Iteration 121/1000 | Loss: 0.00002137
Iteration 122/1000 | Loss: 0.00002137
Iteration 123/1000 | Loss: 0.00002137
Iteration 124/1000 | Loss: 0.00002137
Iteration 125/1000 | Loss: 0.00002136
Iteration 126/1000 | Loss: 0.00002136
Iteration 127/1000 | Loss: 0.00002136
Iteration 128/1000 | Loss: 0.00002136
Iteration 129/1000 | Loss: 0.00002136
Iteration 130/1000 | Loss: 0.00002135
Iteration 131/1000 | Loss: 0.00002135
Iteration 132/1000 | Loss: 0.00002135
Iteration 133/1000 | Loss: 0.00002135
Iteration 134/1000 | Loss: 0.00002135
Iteration 135/1000 | Loss: 0.00002135
Iteration 136/1000 | Loss: 0.00002135
Iteration 137/1000 | Loss: 0.00002135
Iteration 138/1000 | Loss: 0.00002135
Iteration 139/1000 | Loss: 0.00002135
Iteration 140/1000 | Loss: 0.00002134
Iteration 141/1000 | Loss: 0.00002134
Iteration 142/1000 | Loss: 0.00002134
Iteration 143/1000 | Loss: 0.00002134
Iteration 144/1000 | Loss: 0.00002133
Iteration 145/1000 | Loss: 0.00002133
Iteration 146/1000 | Loss: 0.00002132
Iteration 147/1000 | Loss: 0.00002132
Iteration 148/1000 | Loss: 0.00002132
Iteration 149/1000 | Loss: 0.00002132
Iteration 150/1000 | Loss: 0.00002132
Iteration 151/1000 | Loss: 0.00002132
Iteration 152/1000 | Loss: 0.00002132
Iteration 153/1000 | Loss: 0.00002132
Iteration 154/1000 | Loss: 0.00002132
Iteration 155/1000 | Loss: 0.00002132
Iteration 156/1000 | Loss: 0.00002131
Iteration 157/1000 | Loss: 0.00002131
Iteration 158/1000 | Loss: 0.00002131
Iteration 159/1000 | Loss: 0.00002131
Iteration 160/1000 | Loss: 0.00002131
Iteration 161/1000 | Loss: 0.00002131
Iteration 162/1000 | Loss: 0.00002131
Iteration 163/1000 | Loss: 0.00002131
Iteration 164/1000 | Loss: 0.00002130
Iteration 165/1000 | Loss: 0.00002130
Iteration 166/1000 | Loss: 0.00002130
Iteration 167/1000 | Loss: 0.00002130
Iteration 168/1000 | Loss: 0.00002130
Iteration 169/1000 | Loss: 0.00002130
Iteration 170/1000 | Loss: 0.00002130
Iteration 171/1000 | Loss: 0.00002129
Iteration 172/1000 | Loss: 0.00002129
Iteration 173/1000 | Loss: 0.00002129
Iteration 174/1000 | Loss: 0.00002129
Iteration 175/1000 | Loss: 0.00002129
Iteration 176/1000 | Loss: 0.00002129
Iteration 177/1000 | Loss: 0.00002128
Iteration 178/1000 | Loss: 0.00002128
Iteration 179/1000 | Loss: 0.00002128
Iteration 180/1000 | Loss: 0.00002128
Iteration 181/1000 | Loss: 0.00002128
Iteration 182/1000 | Loss: 0.00002127
Iteration 183/1000 | Loss: 0.00002127
Iteration 184/1000 | Loss: 0.00002127
Iteration 185/1000 | Loss: 0.00002127
Iteration 186/1000 | Loss: 0.00002127
Iteration 187/1000 | Loss: 0.00002127
Iteration 188/1000 | Loss: 0.00002127
Iteration 189/1000 | Loss: 0.00002127
Iteration 190/1000 | Loss: 0.00002127
Iteration 191/1000 | Loss: 0.00002127
Iteration 192/1000 | Loss: 0.00002126
Iteration 193/1000 | Loss: 0.00002126
Iteration 194/1000 | Loss: 0.00002126
Iteration 195/1000 | Loss: 0.00002126
Iteration 196/1000 | Loss: 0.00002126
Iteration 197/1000 | Loss: 0.00002126
Iteration 198/1000 | Loss: 0.00002126
Iteration 199/1000 | Loss: 0.00002126
Iteration 200/1000 | Loss: 0.00002126
Iteration 201/1000 | Loss: 0.00002126
Iteration 202/1000 | Loss: 0.00002125
Iteration 203/1000 | Loss: 0.00002125
Iteration 204/1000 | Loss: 0.00002125
Iteration 205/1000 | Loss: 0.00002125
Iteration 206/1000 | Loss: 0.00002125
Iteration 207/1000 | Loss: 0.00002125
Iteration 208/1000 | Loss: 0.00002124
Iteration 209/1000 | Loss: 0.00002124
Iteration 210/1000 | Loss: 0.00002124
Iteration 211/1000 | Loss: 0.00002124
Iteration 212/1000 | Loss: 0.00002124
Iteration 213/1000 | Loss: 0.00002124
Iteration 214/1000 | Loss: 0.00002123
Iteration 215/1000 | Loss: 0.00002123
Iteration 216/1000 | Loss: 0.00002123
Iteration 217/1000 | Loss: 0.00002123
Iteration 218/1000 | Loss: 0.00002123
Iteration 219/1000 | Loss: 0.00002123
Iteration 220/1000 | Loss: 0.00002123
Iteration 221/1000 | Loss: 0.00002123
Iteration 222/1000 | Loss: 0.00002123
Iteration 223/1000 | Loss: 0.00002123
Iteration 224/1000 | Loss: 0.00002123
Iteration 225/1000 | Loss: 0.00002123
Iteration 226/1000 | Loss: 0.00002123
Iteration 227/1000 | Loss: 0.00002123
Iteration 228/1000 | Loss: 0.00002123
Iteration 229/1000 | Loss: 0.00002123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [2.1231371647445485e-05, 2.1231371647445485e-05, 2.1231371647445485e-05, 2.1231371647445485e-05, 2.1231371647445485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1231371647445485e-05

Optimization complete. Final v2v error: 3.7849326133728027 mm

Highest mean error: 4.661441326141357 mm for frame 168

Lowest mean error: 3.396623134613037 mm for frame 36

Saving results

Total time: 138.13336396217346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967173
Iteration 2/25 | Loss: 0.00257621
Iteration 3/25 | Loss: 0.00210317
Iteration 4/25 | Loss: 0.00199424
Iteration 5/25 | Loss: 0.00193966
Iteration 6/25 | Loss: 0.00196910
Iteration 7/25 | Loss: 0.00189631
Iteration 8/25 | Loss: 0.00181888
Iteration 9/25 | Loss: 0.00179809
Iteration 10/25 | Loss: 0.00177949
Iteration 11/25 | Loss: 0.00175630
Iteration 12/25 | Loss: 0.00174210
Iteration 13/25 | Loss: 0.00174431
Iteration 14/25 | Loss: 0.00175972
Iteration 15/25 | Loss: 0.00173598
Iteration 16/25 | Loss: 0.00173234
Iteration 17/25 | Loss: 0.00173194
Iteration 18/25 | Loss: 0.00173170
Iteration 19/25 | Loss: 0.00173166
Iteration 20/25 | Loss: 0.00173166
Iteration 21/25 | Loss: 0.00173166
Iteration 22/25 | Loss: 0.00173165
Iteration 23/25 | Loss: 0.00173165
Iteration 24/25 | Loss: 0.00173165
Iteration 25/25 | Loss: 0.00173156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34837115
Iteration 2/25 | Loss: 0.00449270
Iteration 3/25 | Loss: 0.00434619
Iteration 4/25 | Loss: 0.00434619
Iteration 5/25 | Loss: 0.00434619
Iteration 6/25 | Loss: 0.00434619
Iteration 7/25 | Loss: 0.00434618
Iteration 8/25 | Loss: 0.00434618
Iteration 9/25 | Loss: 0.00434618
Iteration 10/25 | Loss: 0.00434618
Iteration 11/25 | Loss: 0.00434618
Iteration 12/25 | Loss: 0.00434618
Iteration 13/25 | Loss: 0.00434618
Iteration 14/25 | Loss: 0.00434618
Iteration 15/25 | Loss: 0.00434618
Iteration 16/25 | Loss: 0.00434618
Iteration 17/25 | Loss: 0.00434618
Iteration 18/25 | Loss: 0.00434618
Iteration 19/25 | Loss: 0.00434618
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004346183035522699, 0.004346183035522699, 0.004346183035522699, 0.004346183035522699, 0.004346183035522699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004346183035522699

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00434618
Iteration 2/1000 | Loss: 0.00060954
Iteration 3/1000 | Loss: 0.00071022
Iteration 4/1000 | Loss: 0.00050228
Iteration 5/1000 | Loss: 0.00039819
Iteration 6/1000 | Loss: 0.01070849
Iteration 7/1000 | Loss: 0.00048490
Iteration 8/1000 | Loss: 0.00043302
Iteration 9/1000 | Loss: 0.00038558
Iteration 10/1000 | Loss: 0.00089200
Iteration 11/1000 | Loss: 0.00804746
Iteration 12/1000 | Loss: 0.00127789
Iteration 13/1000 | Loss: 0.00133529
Iteration 14/1000 | Loss: 0.00297209
Iteration 15/1000 | Loss: 0.00039885
Iteration 16/1000 | Loss: 0.00036140
Iteration 17/1000 | Loss: 0.00040985
Iteration 18/1000 | Loss: 0.00160165
Iteration 19/1000 | Loss: 0.00039910
Iteration 20/1000 | Loss: 0.00097927
Iteration 21/1000 | Loss: 0.00061732
Iteration 22/1000 | Loss: 0.00037742
Iteration 23/1000 | Loss: 0.00030324
Iteration 24/1000 | Loss: 0.00033197
Iteration 25/1000 | Loss: 0.00037628
Iteration 26/1000 | Loss: 0.00076221
Iteration 27/1000 | Loss: 0.00039173
Iteration 28/1000 | Loss: 0.00081841
Iteration 29/1000 | Loss: 0.00156941
Iteration 30/1000 | Loss: 0.00055972
Iteration 31/1000 | Loss: 0.00028976
Iteration 32/1000 | Loss: 0.00155752
Iteration 33/1000 | Loss: 0.00733024
Iteration 34/1000 | Loss: 0.01266707
Iteration 35/1000 | Loss: 0.01003355
Iteration 36/1000 | Loss: 0.00368471
Iteration 37/1000 | Loss: 0.00647381
Iteration 38/1000 | Loss: 0.00228968
Iteration 39/1000 | Loss: 0.00041879
Iteration 40/1000 | Loss: 0.00159738
Iteration 41/1000 | Loss: 0.00065126
Iteration 42/1000 | Loss: 0.00014093
Iteration 43/1000 | Loss: 0.00040986
Iteration 44/1000 | Loss: 0.00012160
Iteration 45/1000 | Loss: 0.00016886
Iteration 46/1000 | Loss: 0.00011220
Iteration 47/1000 | Loss: 0.00037369
Iteration 48/1000 | Loss: 0.00004774
Iteration 49/1000 | Loss: 0.00005380
Iteration 50/1000 | Loss: 0.00041833
Iteration 51/1000 | Loss: 0.00005888
Iteration 52/1000 | Loss: 0.00004073
Iteration 53/1000 | Loss: 0.00020970
Iteration 54/1000 | Loss: 0.00025994
Iteration 55/1000 | Loss: 0.00005567
Iteration 56/1000 | Loss: 0.00003271
Iteration 57/1000 | Loss: 0.00003121
Iteration 58/1000 | Loss: 0.00015632
Iteration 59/1000 | Loss: 0.00034938
Iteration 60/1000 | Loss: 0.00004239
Iteration 61/1000 | Loss: 0.00003043
Iteration 62/1000 | Loss: 0.00040880
Iteration 63/1000 | Loss: 0.00002629
Iteration 64/1000 | Loss: 0.00002421
Iteration 65/1000 | Loss: 0.00018626
Iteration 66/1000 | Loss: 0.00002326
Iteration 67/1000 | Loss: 0.00002166
Iteration 68/1000 | Loss: 0.00002101
Iteration 69/1000 | Loss: 0.00002059
Iteration 70/1000 | Loss: 0.00002025
Iteration 71/1000 | Loss: 0.00002002
Iteration 72/1000 | Loss: 0.00001980
Iteration 73/1000 | Loss: 0.00001968
Iteration 74/1000 | Loss: 0.00001964
Iteration 75/1000 | Loss: 0.00001959
Iteration 76/1000 | Loss: 0.00001959
Iteration 77/1000 | Loss: 0.00001957
Iteration 78/1000 | Loss: 0.00001957
Iteration 79/1000 | Loss: 0.00001957
Iteration 80/1000 | Loss: 0.00001957
Iteration 81/1000 | Loss: 0.00001956
Iteration 82/1000 | Loss: 0.00001956
Iteration 83/1000 | Loss: 0.00001956
Iteration 84/1000 | Loss: 0.00001956
Iteration 85/1000 | Loss: 0.00001956
Iteration 86/1000 | Loss: 0.00001956
Iteration 87/1000 | Loss: 0.00001955
Iteration 88/1000 | Loss: 0.00001955
Iteration 89/1000 | Loss: 0.00001955
Iteration 90/1000 | Loss: 0.00001955
Iteration 91/1000 | Loss: 0.00001955
Iteration 92/1000 | Loss: 0.00001954
Iteration 93/1000 | Loss: 0.00001951
Iteration 94/1000 | Loss: 0.00001949
Iteration 95/1000 | Loss: 0.00001949
Iteration 96/1000 | Loss: 0.00001949
Iteration 97/1000 | Loss: 0.00001949
Iteration 98/1000 | Loss: 0.00001949
Iteration 99/1000 | Loss: 0.00001948
Iteration 100/1000 | Loss: 0.00001948
Iteration 101/1000 | Loss: 0.00001948
Iteration 102/1000 | Loss: 0.00001948
Iteration 103/1000 | Loss: 0.00001948
Iteration 104/1000 | Loss: 0.00001947
Iteration 105/1000 | Loss: 0.00001947
Iteration 106/1000 | Loss: 0.00001947
Iteration 107/1000 | Loss: 0.00001943
Iteration 108/1000 | Loss: 0.00001943
Iteration 109/1000 | Loss: 0.00001943
Iteration 110/1000 | Loss: 0.00001943
Iteration 111/1000 | Loss: 0.00001943
Iteration 112/1000 | Loss: 0.00001943
Iteration 113/1000 | Loss: 0.00001943
Iteration 114/1000 | Loss: 0.00001939
Iteration 115/1000 | Loss: 0.00001939
Iteration 116/1000 | Loss: 0.00001939
Iteration 117/1000 | Loss: 0.00001939
Iteration 118/1000 | Loss: 0.00001939
Iteration 119/1000 | Loss: 0.00001938
Iteration 120/1000 | Loss: 0.00001938
Iteration 121/1000 | Loss: 0.00001938
Iteration 122/1000 | Loss: 0.00001938
Iteration 123/1000 | Loss: 0.00001938
Iteration 124/1000 | Loss: 0.00001938
Iteration 125/1000 | Loss: 0.00001938
Iteration 126/1000 | Loss: 0.00001938
Iteration 127/1000 | Loss: 0.00001938
Iteration 128/1000 | Loss: 0.00001938
Iteration 129/1000 | Loss: 0.00001938
Iteration 130/1000 | Loss: 0.00001938
Iteration 131/1000 | Loss: 0.00001938
Iteration 132/1000 | Loss: 0.00001938
Iteration 133/1000 | Loss: 0.00001937
Iteration 134/1000 | Loss: 0.00001936
Iteration 135/1000 | Loss: 0.00001936
Iteration 136/1000 | Loss: 0.00001935
Iteration 137/1000 | Loss: 0.00001935
Iteration 138/1000 | Loss: 0.00001935
Iteration 139/1000 | Loss: 0.00001934
Iteration 140/1000 | Loss: 0.00001934
Iteration 141/1000 | Loss: 0.00001934
Iteration 142/1000 | Loss: 0.00001934
Iteration 143/1000 | Loss: 0.00001933
Iteration 144/1000 | Loss: 0.00001933
Iteration 145/1000 | Loss: 0.00001933
Iteration 146/1000 | Loss: 0.00001933
Iteration 147/1000 | Loss: 0.00001932
Iteration 148/1000 | Loss: 0.00001932
Iteration 149/1000 | Loss: 0.00001932
Iteration 150/1000 | Loss: 0.00001932
Iteration 151/1000 | Loss: 0.00001932
Iteration 152/1000 | Loss: 0.00001932
Iteration 153/1000 | Loss: 0.00001932
Iteration 154/1000 | Loss: 0.00001931
Iteration 155/1000 | Loss: 0.00001931
Iteration 156/1000 | Loss: 0.00001931
Iteration 157/1000 | Loss: 0.00001931
Iteration 158/1000 | Loss: 0.00001931
Iteration 159/1000 | Loss: 0.00001931
Iteration 160/1000 | Loss: 0.00001931
Iteration 161/1000 | Loss: 0.00001931
Iteration 162/1000 | Loss: 0.00001931
Iteration 163/1000 | Loss: 0.00001931
Iteration 164/1000 | Loss: 0.00001931
Iteration 165/1000 | Loss: 0.00001931
Iteration 166/1000 | Loss: 0.00001931
Iteration 167/1000 | Loss: 0.00001931
Iteration 168/1000 | Loss: 0.00001930
Iteration 169/1000 | Loss: 0.00001930
Iteration 170/1000 | Loss: 0.00001930
Iteration 171/1000 | Loss: 0.00001930
Iteration 172/1000 | Loss: 0.00001930
Iteration 173/1000 | Loss: 0.00001930
Iteration 174/1000 | Loss: 0.00001930
Iteration 175/1000 | Loss: 0.00001930
Iteration 176/1000 | Loss: 0.00001930
Iteration 177/1000 | Loss: 0.00001930
Iteration 178/1000 | Loss: 0.00001930
Iteration 179/1000 | Loss: 0.00001930
Iteration 180/1000 | Loss: 0.00001929
Iteration 181/1000 | Loss: 0.00001929
Iteration 182/1000 | Loss: 0.00001929
Iteration 183/1000 | Loss: 0.00001929
Iteration 184/1000 | Loss: 0.00001928
Iteration 185/1000 | Loss: 0.00017959
Iteration 186/1000 | Loss: 0.00002107
Iteration 187/1000 | Loss: 0.00001969
Iteration 188/1000 | Loss: 0.00001931
Iteration 189/1000 | Loss: 0.00001927
Iteration 190/1000 | Loss: 0.00001927
Iteration 191/1000 | Loss: 0.00001927
Iteration 192/1000 | Loss: 0.00001927
Iteration 193/1000 | Loss: 0.00001927
Iteration 194/1000 | Loss: 0.00001926
Iteration 195/1000 | Loss: 0.00001926
Iteration 196/1000 | Loss: 0.00001926
Iteration 197/1000 | Loss: 0.00001926
Iteration 198/1000 | Loss: 0.00001926
Iteration 199/1000 | Loss: 0.00001925
Iteration 200/1000 | Loss: 0.00001924
Iteration 201/1000 | Loss: 0.00001924
Iteration 202/1000 | Loss: 0.00001924
Iteration 203/1000 | Loss: 0.00001923
Iteration 204/1000 | Loss: 0.00001923
Iteration 205/1000 | Loss: 0.00001923
Iteration 206/1000 | Loss: 0.00001923
Iteration 207/1000 | Loss: 0.00001923
Iteration 208/1000 | Loss: 0.00001923
Iteration 209/1000 | Loss: 0.00001923
Iteration 210/1000 | Loss: 0.00001923
Iteration 211/1000 | Loss: 0.00001923
Iteration 212/1000 | Loss: 0.00001923
Iteration 213/1000 | Loss: 0.00001923
Iteration 214/1000 | Loss: 0.00001923
Iteration 215/1000 | Loss: 0.00001923
Iteration 216/1000 | Loss: 0.00001923
Iteration 217/1000 | Loss: 0.00001923
Iteration 218/1000 | Loss: 0.00001922
Iteration 219/1000 | Loss: 0.00001922
Iteration 220/1000 | Loss: 0.00001922
Iteration 221/1000 | Loss: 0.00001922
Iteration 222/1000 | Loss: 0.00001922
Iteration 223/1000 | Loss: 0.00001922
Iteration 224/1000 | Loss: 0.00001922
Iteration 225/1000 | Loss: 0.00001922
Iteration 226/1000 | Loss: 0.00001922
Iteration 227/1000 | Loss: 0.00001922
Iteration 228/1000 | Loss: 0.00001922
Iteration 229/1000 | Loss: 0.00001922
Iteration 230/1000 | Loss: 0.00001922
Iteration 231/1000 | Loss: 0.00001922
Iteration 232/1000 | Loss: 0.00001922
Iteration 233/1000 | Loss: 0.00001922
Iteration 234/1000 | Loss: 0.00001922
Iteration 235/1000 | Loss: 0.00001922
Iteration 236/1000 | Loss: 0.00001922
Iteration 237/1000 | Loss: 0.00001922
Iteration 238/1000 | Loss: 0.00001922
Iteration 239/1000 | Loss: 0.00001921
Iteration 240/1000 | Loss: 0.00001921
Iteration 241/1000 | Loss: 0.00001921
Iteration 242/1000 | Loss: 0.00001921
Iteration 243/1000 | Loss: 0.00001921
Iteration 244/1000 | Loss: 0.00001921
Iteration 245/1000 | Loss: 0.00001921
Iteration 246/1000 | Loss: 0.00001921
Iteration 247/1000 | Loss: 0.00001921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.921468174259644e-05, 1.921468174259644e-05, 1.921468174259644e-05, 1.921468174259644e-05, 1.921468174259644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.921468174259644e-05

Optimization complete. Final v2v error: 3.5271897315979004 mm

Highest mean error: 12.67661190032959 mm for frame 99

Lowest mean error: 3.2897789478302 mm for frame 111

Saving results

Total time: 157.45506405830383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902770
Iteration 2/25 | Loss: 0.00133037
Iteration 3/25 | Loss: 0.00122394
Iteration 4/25 | Loss: 0.00120383
Iteration 5/25 | Loss: 0.00119686
Iteration 6/25 | Loss: 0.00119529
Iteration 7/25 | Loss: 0.00119529
Iteration 8/25 | Loss: 0.00119529
Iteration 9/25 | Loss: 0.00119529
Iteration 10/25 | Loss: 0.00119529
Iteration 11/25 | Loss: 0.00119529
Iteration 12/25 | Loss: 0.00119529
Iteration 13/25 | Loss: 0.00119529
Iteration 14/25 | Loss: 0.00119529
Iteration 15/25 | Loss: 0.00119529
Iteration 16/25 | Loss: 0.00119529
Iteration 17/25 | Loss: 0.00119529
Iteration 18/25 | Loss: 0.00119529
Iteration 19/25 | Loss: 0.00119529
Iteration 20/25 | Loss: 0.00119529
Iteration 21/25 | Loss: 0.00119529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001195290358737111, 0.001195290358737111, 0.001195290358737111, 0.001195290358737111, 0.001195290358737111]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001195290358737111

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37769258
Iteration 2/25 | Loss: 0.00076965
Iteration 3/25 | Loss: 0.00076963
Iteration 4/25 | Loss: 0.00076963
Iteration 5/25 | Loss: 0.00076963
Iteration 6/25 | Loss: 0.00076963
Iteration 7/25 | Loss: 0.00076963
Iteration 8/25 | Loss: 0.00076963
Iteration 9/25 | Loss: 0.00076963
Iteration 10/25 | Loss: 0.00076963
Iteration 11/25 | Loss: 0.00076963
Iteration 12/25 | Loss: 0.00076962
Iteration 13/25 | Loss: 0.00076962
Iteration 14/25 | Loss: 0.00076962
Iteration 15/25 | Loss: 0.00076962
Iteration 16/25 | Loss: 0.00076962
Iteration 17/25 | Loss: 0.00076962
Iteration 18/25 | Loss: 0.00076962
Iteration 19/25 | Loss: 0.00076962
Iteration 20/25 | Loss: 0.00076962
Iteration 21/25 | Loss: 0.00076962
Iteration 22/25 | Loss: 0.00076962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007696248940192163, 0.0007696248940192163, 0.0007696248940192163, 0.0007696248940192163, 0.0007696248940192163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007696248940192163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076962
Iteration 2/1000 | Loss: 0.00004915
Iteration 3/1000 | Loss: 0.00003285
Iteration 4/1000 | Loss: 0.00002666
Iteration 5/1000 | Loss: 0.00002481
Iteration 6/1000 | Loss: 0.00002405
Iteration 7/1000 | Loss: 0.00002317
Iteration 8/1000 | Loss: 0.00002261
Iteration 9/1000 | Loss: 0.00002192
Iteration 10/1000 | Loss: 0.00002153
Iteration 11/1000 | Loss: 0.00002124
Iteration 12/1000 | Loss: 0.00002108
Iteration 13/1000 | Loss: 0.00002093
Iteration 14/1000 | Loss: 0.00002080
Iteration 15/1000 | Loss: 0.00002076
Iteration 16/1000 | Loss: 0.00002067
Iteration 17/1000 | Loss: 0.00002066
Iteration 18/1000 | Loss: 0.00002065
Iteration 19/1000 | Loss: 0.00002064
Iteration 20/1000 | Loss: 0.00002062
Iteration 21/1000 | Loss: 0.00002058
Iteration 22/1000 | Loss: 0.00002051
Iteration 23/1000 | Loss: 0.00002051
Iteration 24/1000 | Loss: 0.00002050
Iteration 25/1000 | Loss: 0.00002049
Iteration 26/1000 | Loss: 0.00002045
Iteration 27/1000 | Loss: 0.00002044
Iteration 28/1000 | Loss: 0.00002043
Iteration 29/1000 | Loss: 0.00002042
Iteration 30/1000 | Loss: 0.00002042
Iteration 31/1000 | Loss: 0.00002041
Iteration 32/1000 | Loss: 0.00002040
Iteration 33/1000 | Loss: 0.00002040
Iteration 34/1000 | Loss: 0.00002039
Iteration 35/1000 | Loss: 0.00002039
Iteration 36/1000 | Loss: 0.00002038
Iteration 37/1000 | Loss: 0.00002038
Iteration 38/1000 | Loss: 0.00002037
Iteration 39/1000 | Loss: 0.00002036
Iteration 40/1000 | Loss: 0.00002036
Iteration 41/1000 | Loss: 0.00002036
Iteration 42/1000 | Loss: 0.00002036
Iteration 43/1000 | Loss: 0.00002035
Iteration 44/1000 | Loss: 0.00002035
Iteration 45/1000 | Loss: 0.00002035
Iteration 46/1000 | Loss: 0.00002035
Iteration 47/1000 | Loss: 0.00002035
Iteration 48/1000 | Loss: 0.00002034
Iteration 49/1000 | Loss: 0.00002034
Iteration 50/1000 | Loss: 0.00002034
Iteration 51/1000 | Loss: 0.00002033
Iteration 52/1000 | Loss: 0.00002032
Iteration 53/1000 | Loss: 0.00002031
Iteration 54/1000 | Loss: 0.00002031
Iteration 55/1000 | Loss: 0.00002030
Iteration 56/1000 | Loss: 0.00002030
Iteration 57/1000 | Loss: 0.00002030
Iteration 58/1000 | Loss: 0.00002030
Iteration 59/1000 | Loss: 0.00002029
Iteration 60/1000 | Loss: 0.00002029
Iteration 61/1000 | Loss: 0.00002029
Iteration 62/1000 | Loss: 0.00002028
Iteration 63/1000 | Loss: 0.00002028
Iteration 64/1000 | Loss: 0.00002028
Iteration 65/1000 | Loss: 0.00002028
Iteration 66/1000 | Loss: 0.00002028
Iteration 67/1000 | Loss: 0.00002028
Iteration 68/1000 | Loss: 0.00002028
Iteration 69/1000 | Loss: 0.00002028
Iteration 70/1000 | Loss: 0.00002027
Iteration 71/1000 | Loss: 0.00002027
Iteration 72/1000 | Loss: 0.00002027
Iteration 73/1000 | Loss: 0.00002027
Iteration 74/1000 | Loss: 0.00002027
Iteration 75/1000 | Loss: 0.00002027
Iteration 76/1000 | Loss: 0.00002026
Iteration 77/1000 | Loss: 0.00002026
Iteration 78/1000 | Loss: 0.00002026
Iteration 79/1000 | Loss: 0.00002025
Iteration 80/1000 | Loss: 0.00002025
Iteration 81/1000 | Loss: 0.00002025
Iteration 82/1000 | Loss: 0.00002025
Iteration 83/1000 | Loss: 0.00002025
Iteration 84/1000 | Loss: 0.00002024
Iteration 85/1000 | Loss: 0.00002023
Iteration 86/1000 | Loss: 0.00002023
Iteration 87/1000 | Loss: 0.00002022
Iteration 88/1000 | Loss: 0.00002022
Iteration 89/1000 | Loss: 0.00002021
Iteration 90/1000 | Loss: 0.00002021
Iteration 91/1000 | Loss: 0.00002019
Iteration 92/1000 | Loss: 0.00002019
Iteration 93/1000 | Loss: 0.00002018
Iteration 94/1000 | Loss: 0.00002018
Iteration 95/1000 | Loss: 0.00002018
Iteration 96/1000 | Loss: 0.00002018
Iteration 97/1000 | Loss: 0.00002018
Iteration 98/1000 | Loss: 0.00002018
Iteration 99/1000 | Loss: 0.00002018
Iteration 100/1000 | Loss: 0.00002018
Iteration 101/1000 | Loss: 0.00002018
Iteration 102/1000 | Loss: 0.00002018
Iteration 103/1000 | Loss: 0.00002018
Iteration 104/1000 | Loss: 0.00002017
Iteration 105/1000 | Loss: 0.00002017
Iteration 106/1000 | Loss: 0.00002017
Iteration 107/1000 | Loss: 0.00002016
Iteration 108/1000 | Loss: 0.00002015
Iteration 109/1000 | Loss: 0.00002015
Iteration 110/1000 | Loss: 0.00002015
Iteration 111/1000 | Loss: 0.00002014
Iteration 112/1000 | Loss: 0.00002014
Iteration 113/1000 | Loss: 0.00002014
Iteration 114/1000 | Loss: 0.00002014
Iteration 115/1000 | Loss: 0.00002014
Iteration 116/1000 | Loss: 0.00002014
Iteration 117/1000 | Loss: 0.00002014
Iteration 118/1000 | Loss: 0.00002013
Iteration 119/1000 | Loss: 0.00002013
Iteration 120/1000 | Loss: 0.00002013
Iteration 121/1000 | Loss: 0.00002013
Iteration 122/1000 | Loss: 0.00002013
Iteration 123/1000 | Loss: 0.00002013
Iteration 124/1000 | Loss: 0.00002013
Iteration 125/1000 | Loss: 0.00002012
Iteration 126/1000 | Loss: 0.00002012
Iteration 127/1000 | Loss: 0.00002012
Iteration 128/1000 | Loss: 0.00002012
Iteration 129/1000 | Loss: 0.00002012
Iteration 130/1000 | Loss: 0.00002012
Iteration 131/1000 | Loss: 0.00002012
Iteration 132/1000 | Loss: 0.00002012
Iteration 133/1000 | Loss: 0.00002011
Iteration 134/1000 | Loss: 0.00002011
Iteration 135/1000 | Loss: 0.00002011
Iteration 136/1000 | Loss: 0.00002011
Iteration 137/1000 | Loss: 0.00002011
Iteration 138/1000 | Loss: 0.00002011
Iteration 139/1000 | Loss: 0.00002011
Iteration 140/1000 | Loss: 0.00002010
Iteration 141/1000 | Loss: 0.00002010
Iteration 142/1000 | Loss: 0.00002009
Iteration 143/1000 | Loss: 0.00002009
Iteration 144/1000 | Loss: 0.00002009
Iteration 145/1000 | Loss: 0.00002008
Iteration 146/1000 | Loss: 0.00002008
Iteration 147/1000 | Loss: 0.00002008
Iteration 148/1000 | Loss: 0.00002008
Iteration 149/1000 | Loss: 0.00002008
Iteration 150/1000 | Loss: 0.00002008
Iteration 151/1000 | Loss: 0.00002008
Iteration 152/1000 | Loss: 0.00002008
Iteration 153/1000 | Loss: 0.00002008
Iteration 154/1000 | Loss: 0.00002008
Iteration 155/1000 | Loss: 0.00002008
Iteration 156/1000 | Loss: 0.00002007
Iteration 157/1000 | Loss: 0.00002007
Iteration 158/1000 | Loss: 0.00002007
Iteration 159/1000 | Loss: 0.00002006
Iteration 160/1000 | Loss: 0.00002006
Iteration 161/1000 | Loss: 0.00002006
Iteration 162/1000 | Loss: 0.00002006
Iteration 163/1000 | Loss: 0.00002006
Iteration 164/1000 | Loss: 0.00002006
Iteration 165/1000 | Loss: 0.00002006
Iteration 166/1000 | Loss: 0.00002006
Iteration 167/1000 | Loss: 0.00002006
Iteration 168/1000 | Loss: 0.00002006
Iteration 169/1000 | Loss: 0.00002005
Iteration 170/1000 | Loss: 0.00002005
Iteration 171/1000 | Loss: 0.00002005
Iteration 172/1000 | Loss: 0.00002005
Iteration 173/1000 | Loss: 0.00002005
Iteration 174/1000 | Loss: 0.00002004
Iteration 175/1000 | Loss: 0.00002004
Iteration 176/1000 | Loss: 0.00002004
Iteration 177/1000 | Loss: 0.00002004
Iteration 178/1000 | Loss: 0.00002004
Iteration 179/1000 | Loss: 0.00002004
Iteration 180/1000 | Loss: 0.00002004
Iteration 181/1000 | Loss: 0.00002004
Iteration 182/1000 | Loss: 0.00002004
Iteration 183/1000 | Loss: 0.00002004
Iteration 184/1000 | Loss: 0.00002003
Iteration 185/1000 | Loss: 0.00002003
Iteration 186/1000 | Loss: 0.00002003
Iteration 187/1000 | Loss: 0.00002003
Iteration 188/1000 | Loss: 0.00002003
Iteration 189/1000 | Loss: 0.00002003
Iteration 190/1000 | Loss: 0.00002003
Iteration 191/1000 | Loss: 0.00002003
Iteration 192/1000 | Loss: 0.00002002
Iteration 193/1000 | Loss: 0.00002002
Iteration 194/1000 | Loss: 0.00002002
Iteration 195/1000 | Loss: 0.00002002
Iteration 196/1000 | Loss: 0.00002002
Iteration 197/1000 | Loss: 0.00002002
Iteration 198/1000 | Loss: 0.00002002
Iteration 199/1000 | Loss: 0.00002002
Iteration 200/1000 | Loss: 0.00002002
Iteration 201/1000 | Loss: 0.00002002
Iteration 202/1000 | Loss: 0.00002002
Iteration 203/1000 | Loss: 0.00002002
Iteration 204/1000 | Loss: 0.00002002
Iteration 205/1000 | Loss: 0.00002002
Iteration 206/1000 | Loss: 0.00002002
Iteration 207/1000 | Loss: 0.00002002
Iteration 208/1000 | Loss: 0.00002002
Iteration 209/1000 | Loss: 0.00002002
Iteration 210/1000 | Loss: 0.00002002
Iteration 211/1000 | Loss: 0.00002002
Iteration 212/1000 | Loss: 0.00002002
Iteration 213/1000 | Loss: 0.00002002
Iteration 214/1000 | Loss: 0.00002002
Iteration 215/1000 | Loss: 0.00002002
Iteration 216/1000 | Loss: 0.00002002
Iteration 217/1000 | Loss: 0.00002002
Iteration 218/1000 | Loss: 0.00002002
Iteration 219/1000 | Loss: 0.00002002
Iteration 220/1000 | Loss: 0.00002002
Iteration 221/1000 | Loss: 0.00002002
Iteration 222/1000 | Loss: 0.00002002
Iteration 223/1000 | Loss: 0.00002002
Iteration 224/1000 | Loss: 0.00002002
Iteration 225/1000 | Loss: 0.00002002
Iteration 226/1000 | Loss: 0.00002002
Iteration 227/1000 | Loss: 0.00002002
Iteration 228/1000 | Loss: 0.00002002
Iteration 229/1000 | Loss: 0.00002002
Iteration 230/1000 | Loss: 0.00002002
Iteration 231/1000 | Loss: 0.00002002
Iteration 232/1000 | Loss: 0.00002002
Iteration 233/1000 | Loss: 0.00002002
Iteration 234/1000 | Loss: 0.00002002
Iteration 235/1000 | Loss: 0.00002002
Iteration 236/1000 | Loss: 0.00002002
Iteration 237/1000 | Loss: 0.00002002
Iteration 238/1000 | Loss: 0.00002002
Iteration 239/1000 | Loss: 0.00002002
Iteration 240/1000 | Loss: 0.00002002
Iteration 241/1000 | Loss: 0.00002002
Iteration 242/1000 | Loss: 0.00002002
Iteration 243/1000 | Loss: 0.00002002
Iteration 244/1000 | Loss: 0.00002002
Iteration 245/1000 | Loss: 0.00002002
Iteration 246/1000 | Loss: 0.00002002
Iteration 247/1000 | Loss: 0.00002002
Iteration 248/1000 | Loss: 0.00002002
Iteration 249/1000 | Loss: 0.00002002
Iteration 250/1000 | Loss: 0.00002002
Iteration 251/1000 | Loss: 0.00002002
Iteration 252/1000 | Loss: 0.00002002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [2.0016517737531103e-05, 2.0016517737531103e-05, 2.0016517737531103e-05, 2.0016517737531103e-05, 2.0016517737531103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0016517737531103e-05

Optimization complete. Final v2v error: 3.752732515335083 mm

Highest mean error: 5.230579376220703 mm for frame 68

Lowest mean error: 3.164266586303711 mm for frame 44

Saving results

Total time: 46.0651581287384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960733
Iteration 2/25 | Loss: 0.00314318
Iteration 3/25 | Loss: 0.00228694
Iteration 4/25 | Loss: 0.00217746
Iteration 5/25 | Loss: 0.00200945
Iteration 6/25 | Loss: 0.00185864
Iteration 7/25 | Loss: 0.00183127
Iteration 8/25 | Loss: 0.00235656
Iteration 9/25 | Loss: 0.00193956
Iteration 10/25 | Loss: 0.00141681
Iteration 11/25 | Loss: 0.00128571
Iteration 12/25 | Loss: 0.00126379
Iteration 13/25 | Loss: 0.00126043
Iteration 14/25 | Loss: 0.00126821
Iteration 15/25 | Loss: 0.00126068
Iteration 16/25 | Loss: 0.00125576
Iteration 17/25 | Loss: 0.00126071
Iteration 18/25 | Loss: 0.00125896
Iteration 19/25 | Loss: 0.00125646
Iteration 20/25 | Loss: 0.00125510
Iteration 21/25 | Loss: 0.00125396
Iteration 22/25 | Loss: 0.00125108
Iteration 23/25 | Loss: 0.00125260
Iteration 24/25 | Loss: 0.00125231
Iteration 25/25 | Loss: 0.00125327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.05930042
Iteration 2/25 | Loss: 0.00180076
Iteration 3/25 | Loss: 0.00067319
Iteration 4/25 | Loss: 0.00067318
Iteration 5/25 | Loss: 0.00067318
Iteration 6/25 | Loss: 0.00067318
Iteration 7/25 | Loss: 0.00067318
Iteration 8/25 | Loss: 0.00067318
Iteration 9/25 | Loss: 0.00067318
Iteration 10/25 | Loss: 0.00067318
Iteration 11/25 | Loss: 0.00067318
Iteration 12/25 | Loss: 0.00067318
Iteration 13/25 | Loss: 0.00067318
Iteration 14/25 | Loss: 0.00067318
Iteration 15/25 | Loss: 0.00067318
Iteration 16/25 | Loss: 0.00067318
Iteration 17/25 | Loss: 0.00067318
Iteration 18/25 | Loss: 0.00067318
Iteration 19/25 | Loss: 0.00067318
Iteration 20/25 | Loss: 0.00067318
Iteration 21/25 | Loss: 0.00067318
Iteration 22/25 | Loss: 0.00067318
Iteration 23/25 | Loss: 0.00067318
Iteration 24/25 | Loss: 0.00067318
Iteration 25/25 | Loss: 0.00067318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067318
Iteration 2/1000 | Loss: 0.00060588
Iteration 3/1000 | Loss: 0.00004940
Iteration 4/1000 | Loss: 0.00005652
Iteration 5/1000 | Loss: 0.00004626
Iteration 6/1000 | Loss: 0.00004256
Iteration 7/1000 | Loss: 0.00016170
Iteration 8/1000 | Loss: 0.00031603
Iteration 9/1000 | Loss: 0.00007744
Iteration 10/1000 | Loss: 0.00004329
Iteration 11/1000 | Loss: 0.00006818
Iteration 12/1000 | Loss: 0.00003224
Iteration 13/1000 | Loss: 0.00064774
Iteration 14/1000 | Loss: 0.00015103
Iteration 15/1000 | Loss: 0.00004549
Iteration 16/1000 | Loss: 0.00003190
Iteration 17/1000 | Loss: 0.00002702
Iteration 18/1000 | Loss: 0.00002662
Iteration 19/1000 | Loss: 0.00003865
Iteration 20/1000 | Loss: 0.00004988
Iteration 21/1000 | Loss: 0.00004276
Iteration 22/1000 | Loss: 0.00004812
Iteration 23/1000 | Loss: 0.00005086
Iteration 24/1000 | Loss: 0.00004670
Iteration 25/1000 | Loss: 0.00004829
Iteration 26/1000 | Loss: 0.00004137
Iteration 27/1000 | Loss: 0.00004174
Iteration 28/1000 | Loss: 0.00004389
Iteration 29/1000 | Loss: 0.00002609
Iteration 30/1000 | Loss: 0.00004356
Iteration 31/1000 | Loss: 0.00004990
Iteration 32/1000 | Loss: 0.00004051
Iteration 33/1000 | Loss: 0.00067849
Iteration 34/1000 | Loss: 0.00002799
Iteration 35/1000 | Loss: 0.00002309
Iteration 36/1000 | Loss: 0.00002093
Iteration 37/1000 | Loss: 0.00051506
Iteration 38/1000 | Loss: 0.00048068
Iteration 39/1000 | Loss: 0.00010219
Iteration 40/1000 | Loss: 0.00002145
Iteration 41/1000 | Loss: 0.00002026
Iteration 42/1000 | Loss: 0.00001989
Iteration 43/1000 | Loss: 0.00001966
Iteration 44/1000 | Loss: 0.00001952
Iteration 45/1000 | Loss: 0.00001945
Iteration 46/1000 | Loss: 0.00001945
Iteration 47/1000 | Loss: 0.00001944
Iteration 48/1000 | Loss: 0.00001944
Iteration 49/1000 | Loss: 0.00001944
Iteration 50/1000 | Loss: 0.00001944
Iteration 51/1000 | Loss: 0.00001944
Iteration 52/1000 | Loss: 0.00001944
Iteration 53/1000 | Loss: 0.00001944
Iteration 54/1000 | Loss: 0.00001944
Iteration 55/1000 | Loss: 0.00001943
Iteration 56/1000 | Loss: 0.00001943
Iteration 57/1000 | Loss: 0.00001940
Iteration 58/1000 | Loss: 0.00001938
Iteration 59/1000 | Loss: 0.00001932
Iteration 60/1000 | Loss: 0.00001926
Iteration 61/1000 | Loss: 0.00001925
Iteration 62/1000 | Loss: 0.00001922
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001915
Iteration 66/1000 | Loss: 0.00001915
Iteration 67/1000 | Loss: 0.00001914
Iteration 68/1000 | Loss: 0.00001914
Iteration 69/1000 | Loss: 0.00001913
Iteration 70/1000 | Loss: 0.00001912
Iteration 71/1000 | Loss: 0.00001912
Iteration 72/1000 | Loss: 0.00001911
Iteration 73/1000 | Loss: 0.00001911
Iteration 74/1000 | Loss: 0.00001910
Iteration 75/1000 | Loss: 0.00001910
Iteration 76/1000 | Loss: 0.00001910
Iteration 77/1000 | Loss: 0.00001909
Iteration 78/1000 | Loss: 0.00001908
Iteration 79/1000 | Loss: 0.00001906
Iteration 80/1000 | Loss: 0.00001906
Iteration 81/1000 | Loss: 0.00001905
Iteration 82/1000 | Loss: 0.00001905
Iteration 83/1000 | Loss: 0.00001904
Iteration 84/1000 | Loss: 0.00001903
Iteration 85/1000 | Loss: 0.00001903
Iteration 86/1000 | Loss: 0.00001903
Iteration 87/1000 | Loss: 0.00001903
Iteration 88/1000 | Loss: 0.00001902
Iteration 89/1000 | Loss: 0.00001902
Iteration 90/1000 | Loss: 0.00001902
Iteration 91/1000 | Loss: 0.00001902
Iteration 92/1000 | Loss: 0.00001902
Iteration 93/1000 | Loss: 0.00001902
Iteration 94/1000 | Loss: 0.00001901
Iteration 95/1000 | Loss: 0.00001901
Iteration 96/1000 | Loss: 0.00001901
Iteration 97/1000 | Loss: 0.00001900
Iteration 98/1000 | Loss: 0.00001900
Iteration 99/1000 | Loss: 0.00001900
Iteration 100/1000 | Loss: 0.00001900
Iteration 101/1000 | Loss: 0.00001900
Iteration 102/1000 | Loss: 0.00001900
Iteration 103/1000 | Loss: 0.00001900
Iteration 104/1000 | Loss: 0.00001900
Iteration 105/1000 | Loss: 0.00001900
Iteration 106/1000 | Loss: 0.00001900
Iteration 107/1000 | Loss: 0.00001900
Iteration 108/1000 | Loss: 0.00001900
Iteration 109/1000 | Loss: 0.00001900
Iteration 110/1000 | Loss: 0.00001900
Iteration 111/1000 | Loss: 0.00001900
Iteration 112/1000 | Loss: 0.00001900
Iteration 113/1000 | Loss: 0.00001900
Iteration 114/1000 | Loss: 0.00001899
Iteration 115/1000 | Loss: 0.00001899
Iteration 116/1000 | Loss: 0.00001899
Iteration 117/1000 | Loss: 0.00001899
Iteration 118/1000 | Loss: 0.00001899
Iteration 119/1000 | Loss: 0.00001899
Iteration 120/1000 | Loss: 0.00001899
Iteration 121/1000 | Loss: 0.00001899
Iteration 122/1000 | Loss: 0.00001899
Iteration 123/1000 | Loss: 0.00001899
Iteration 124/1000 | Loss: 0.00001899
Iteration 125/1000 | Loss: 0.00001899
Iteration 126/1000 | Loss: 0.00001899
Iteration 127/1000 | Loss: 0.00001899
Iteration 128/1000 | Loss: 0.00001899
Iteration 129/1000 | Loss: 0.00001898
Iteration 130/1000 | Loss: 0.00001898
Iteration 131/1000 | Loss: 0.00001898
Iteration 132/1000 | Loss: 0.00001898
Iteration 133/1000 | Loss: 0.00001898
Iteration 134/1000 | Loss: 0.00001898
Iteration 135/1000 | Loss: 0.00001898
Iteration 136/1000 | Loss: 0.00001898
Iteration 137/1000 | Loss: 0.00001898
Iteration 138/1000 | Loss: 0.00001898
Iteration 139/1000 | Loss: 0.00001898
Iteration 140/1000 | Loss: 0.00001898
Iteration 141/1000 | Loss: 0.00001898
Iteration 142/1000 | Loss: 0.00001898
Iteration 143/1000 | Loss: 0.00001898
Iteration 144/1000 | Loss: 0.00001898
Iteration 145/1000 | Loss: 0.00001898
Iteration 146/1000 | Loss: 0.00001898
Iteration 147/1000 | Loss: 0.00001898
Iteration 148/1000 | Loss: 0.00001898
Iteration 149/1000 | Loss: 0.00001898
Iteration 150/1000 | Loss: 0.00001898
Iteration 151/1000 | Loss: 0.00001898
Iteration 152/1000 | Loss: 0.00001898
Iteration 153/1000 | Loss: 0.00001898
Iteration 154/1000 | Loss: 0.00001898
Iteration 155/1000 | Loss: 0.00001898
Iteration 156/1000 | Loss: 0.00001898
Iteration 157/1000 | Loss: 0.00001898
Iteration 158/1000 | Loss: 0.00001898
Iteration 159/1000 | Loss: 0.00001898
Iteration 160/1000 | Loss: 0.00001898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.8984526832355186e-05, 1.8984526832355186e-05, 1.8984526832355186e-05, 1.8984526832355186e-05, 1.8984526832355186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8984526832355186e-05

Optimization complete. Final v2v error: 3.670531749725342 mm

Highest mean error: 4.148190498352051 mm for frame 58

Lowest mean error: 3.208773136138916 mm for frame 122

Saving results

Total time: 115.1429271697998
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374240
Iteration 2/25 | Loss: 0.00118148
Iteration 3/25 | Loss: 0.00111958
Iteration 4/25 | Loss: 0.00111426
Iteration 5/25 | Loss: 0.00111243
Iteration 6/25 | Loss: 0.00111213
Iteration 7/25 | Loss: 0.00111213
Iteration 8/25 | Loss: 0.00111213
Iteration 9/25 | Loss: 0.00111213
Iteration 10/25 | Loss: 0.00111213
Iteration 11/25 | Loss: 0.00111213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011121326824650168, 0.0011121326824650168, 0.0011121326824650168, 0.0011121326824650168, 0.0011121326824650168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011121326824650168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87986290
Iteration 2/25 | Loss: 0.00089173
Iteration 3/25 | Loss: 0.00089173
Iteration 4/25 | Loss: 0.00089173
Iteration 5/25 | Loss: 0.00089173
Iteration 6/25 | Loss: 0.00089173
Iteration 7/25 | Loss: 0.00089172
Iteration 8/25 | Loss: 0.00089172
Iteration 9/25 | Loss: 0.00089172
Iteration 10/25 | Loss: 0.00089172
Iteration 11/25 | Loss: 0.00089172
Iteration 12/25 | Loss: 0.00089172
Iteration 13/25 | Loss: 0.00089172
Iteration 14/25 | Loss: 0.00089172
Iteration 15/25 | Loss: 0.00089172
Iteration 16/25 | Loss: 0.00089172
Iteration 17/25 | Loss: 0.00089172
Iteration 18/25 | Loss: 0.00089172
Iteration 19/25 | Loss: 0.00089172
Iteration 20/25 | Loss: 0.00089172
Iteration 21/25 | Loss: 0.00089172
Iteration 22/25 | Loss: 0.00089172
Iteration 23/25 | Loss: 0.00089172
Iteration 24/25 | Loss: 0.00089172
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008917231461964548, 0.0008917231461964548, 0.0008917231461964548, 0.0008917231461964548, 0.0008917231461964548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008917231461964548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089172
Iteration 2/1000 | Loss: 0.00002497
Iteration 3/1000 | Loss: 0.00001528
Iteration 4/1000 | Loss: 0.00001242
Iteration 5/1000 | Loss: 0.00001094
Iteration 6/1000 | Loss: 0.00001029
Iteration 7/1000 | Loss: 0.00000981
Iteration 8/1000 | Loss: 0.00000949
Iteration 9/1000 | Loss: 0.00000938
Iteration 10/1000 | Loss: 0.00000937
Iteration 11/1000 | Loss: 0.00000937
Iteration 12/1000 | Loss: 0.00000914
Iteration 13/1000 | Loss: 0.00000903
Iteration 14/1000 | Loss: 0.00000896
Iteration 15/1000 | Loss: 0.00000895
Iteration 16/1000 | Loss: 0.00000894
Iteration 17/1000 | Loss: 0.00000894
Iteration 18/1000 | Loss: 0.00000889
Iteration 19/1000 | Loss: 0.00000884
Iteration 20/1000 | Loss: 0.00000884
Iteration 21/1000 | Loss: 0.00000884
Iteration 22/1000 | Loss: 0.00000883
Iteration 23/1000 | Loss: 0.00000883
Iteration 24/1000 | Loss: 0.00000877
Iteration 25/1000 | Loss: 0.00000877
Iteration 26/1000 | Loss: 0.00000876
Iteration 27/1000 | Loss: 0.00000876
Iteration 28/1000 | Loss: 0.00000876
Iteration 29/1000 | Loss: 0.00000875
Iteration 30/1000 | Loss: 0.00000875
Iteration 31/1000 | Loss: 0.00000874
Iteration 32/1000 | Loss: 0.00000874
Iteration 33/1000 | Loss: 0.00000872
Iteration 34/1000 | Loss: 0.00000872
Iteration 35/1000 | Loss: 0.00000872
Iteration 36/1000 | Loss: 0.00000872
Iteration 37/1000 | Loss: 0.00000872
Iteration 38/1000 | Loss: 0.00000872
Iteration 39/1000 | Loss: 0.00000872
Iteration 40/1000 | Loss: 0.00000872
Iteration 41/1000 | Loss: 0.00000872
Iteration 42/1000 | Loss: 0.00000871
Iteration 43/1000 | Loss: 0.00000871
Iteration 44/1000 | Loss: 0.00000871
Iteration 45/1000 | Loss: 0.00000870
Iteration 46/1000 | Loss: 0.00000870
Iteration 47/1000 | Loss: 0.00000868
Iteration 48/1000 | Loss: 0.00000868
Iteration 49/1000 | Loss: 0.00000868
Iteration 50/1000 | Loss: 0.00000868
Iteration 51/1000 | Loss: 0.00000868
Iteration 52/1000 | Loss: 0.00000868
Iteration 53/1000 | Loss: 0.00000867
Iteration 54/1000 | Loss: 0.00000867
Iteration 55/1000 | Loss: 0.00000867
Iteration 56/1000 | Loss: 0.00000867
Iteration 57/1000 | Loss: 0.00000867
Iteration 58/1000 | Loss: 0.00000867
Iteration 59/1000 | Loss: 0.00000865
Iteration 60/1000 | Loss: 0.00000865
Iteration 61/1000 | Loss: 0.00000865
Iteration 62/1000 | Loss: 0.00000864
Iteration 63/1000 | Loss: 0.00000864
Iteration 64/1000 | Loss: 0.00000864
Iteration 65/1000 | Loss: 0.00000864
Iteration 66/1000 | Loss: 0.00000864
Iteration 67/1000 | Loss: 0.00000863
Iteration 68/1000 | Loss: 0.00000863
Iteration 69/1000 | Loss: 0.00000863
Iteration 70/1000 | Loss: 0.00000863
Iteration 71/1000 | Loss: 0.00000863
Iteration 72/1000 | Loss: 0.00000863
Iteration 73/1000 | Loss: 0.00000863
Iteration 74/1000 | Loss: 0.00000863
Iteration 75/1000 | Loss: 0.00000863
Iteration 76/1000 | Loss: 0.00000863
Iteration 77/1000 | Loss: 0.00000862
Iteration 78/1000 | Loss: 0.00000862
Iteration 79/1000 | Loss: 0.00000862
Iteration 80/1000 | Loss: 0.00000862
Iteration 81/1000 | Loss: 0.00000861
Iteration 82/1000 | Loss: 0.00000861
Iteration 83/1000 | Loss: 0.00000861
Iteration 84/1000 | Loss: 0.00000861
Iteration 85/1000 | Loss: 0.00000860
Iteration 86/1000 | Loss: 0.00000860
Iteration 87/1000 | Loss: 0.00000860
Iteration 88/1000 | Loss: 0.00000860
Iteration 89/1000 | Loss: 0.00000859
Iteration 90/1000 | Loss: 0.00000859
Iteration 91/1000 | Loss: 0.00000858
Iteration 92/1000 | Loss: 0.00000858
Iteration 93/1000 | Loss: 0.00000858
Iteration 94/1000 | Loss: 0.00000857
Iteration 95/1000 | Loss: 0.00000857
Iteration 96/1000 | Loss: 0.00000857
Iteration 97/1000 | Loss: 0.00000857
Iteration 98/1000 | Loss: 0.00000856
Iteration 99/1000 | Loss: 0.00000856
Iteration 100/1000 | Loss: 0.00000856
Iteration 101/1000 | Loss: 0.00000855
Iteration 102/1000 | Loss: 0.00000855
Iteration 103/1000 | Loss: 0.00000855
Iteration 104/1000 | Loss: 0.00000854
Iteration 105/1000 | Loss: 0.00000854
Iteration 106/1000 | Loss: 0.00000854
Iteration 107/1000 | Loss: 0.00000854
Iteration 108/1000 | Loss: 0.00000853
Iteration 109/1000 | Loss: 0.00000853
Iteration 110/1000 | Loss: 0.00000853
Iteration 111/1000 | Loss: 0.00000853
Iteration 112/1000 | Loss: 0.00000853
Iteration 113/1000 | Loss: 0.00000853
Iteration 114/1000 | Loss: 0.00000853
Iteration 115/1000 | Loss: 0.00000852
Iteration 116/1000 | Loss: 0.00000852
Iteration 117/1000 | Loss: 0.00000852
Iteration 118/1000 | Loss: 0.00000852
Iteration 119/1000 | Loss: 0.00000852
Iteration 120/1000 | Loss: 0.00000852
Iteration 121/1000 | Loss: 0.00000852
Iteration 122/1000 | Loss: 0.00000852
Iteration 123/1000 | Loss: 0.00000851
Iteration 124/1000 | Loss: 0.00000851
Iteration 125/1000 | Loss: 0.00000851
Iteration 126/1000 | Loss: 0.00000851
Iteration 127/1000 | Loss: 0.00000851
Iteration 128/1000 | Loss: 0.00000850
Iteration 129/1000 | Loss: 0.00000850
Iteration 130/1000 | Loss: 0.00000850
Iteration 131/1000 | Loss: 0.00000850
Iteration 132/1000 | Loss: 0.00000850
Iteration 133/1000 | Loss: 0.00000850
Iteration 134/1000 | Loss: 0.00000850
Iteration 135/1000 | Loss: 0.00000850
Iteration 136/1000 | Loss: 0.00000849
Iteration 137/1000 | Loss: 0.00000849
Iteration 138/1000 | Loss: 0.00000849
Iteration 139/1000 | Loss: 0.00000849
Iteration 140/1000 | Loss: 0.00000849
Iteration 141/1000 | Loss: 0.00000849
Iteration 142/1000 | Loss: 0.00000848
Iteration 143/1000 | Loss: 0.00000848
Iteration 144/1000 | Loss: 0.00000848
Iteration 145/1000 | Loss: 0.00000848
Iteration 146/1000 | Loss: 0.00000848
Iteration 147/1000 | Loss: 0.00000848
Iteration 148/1000 | Loss: 0.00000848
Iteration 149/1000 | Loss: 0.00000848
Iteration 150/1000 | Loss: 0.00000847
Iteration 151/1000 | Loss: 0.00000847
Iteration 152/1000 | Loss: 0.00000847
Iteration 153/1000 | Loss: 0.00000847
Iteration 154/1000 | Loss: 0.00000847
Iteration 155/1000 | Loss: 0.00000847
Iteration 156/1000 | Loss: 0.00000847
Iteration 157/1000 | Loss: 0.00000847
Iteration 158/1000 | Loss: 0.00000847
Iteration 159/1000 | Loss: 0.00000847
Iteration 160/1000 | Loss: 0.00000846
Iteration 161/1000 | Loss: 0.00000846
Iteration 162/1000 | Loss: 0.00000846
Iteration 163/1000 | Loss: 0.00000846
Iteration 164/1000 | Loss: 0.00000846
Iteration 165/1000 | Loss: 0.00000845
Iteration 166/1000 | Loss: 0.00000845
Iteration 167/1000 | Loss: 0.00000845
Iteration 168/1000 | Loss: 0.00000845
Iteration 169/1000 | Loss: 0.00000845
Iteration 170/1000 | Loss: 0.00000845
Iteration 171/1000 | Loss: 0.00000845
Iteration 172/1000 | Loss: 0.00000845
Iteration 173/1000 | Loss: 0.00000845
Iteration 174/1000 | Loss: 0.00000844
Iteration 175/1000 | Loss: 0.00000844
Iteration 176/1000 | Loss: 0.00000844
Iteration 177/1000 | Loss: 0.00000844
Iteration 178/1000 | Loss: 0.00000844
Iteration 179/1000 | Loss: 0.00000844
Iteration 180/1000 | Loss: 0.00000844
Iteration 181/1000 | Loss: 0.00000844
Iteration 182/1000 | Loss: 0.00000844
Iteration 183/1000 | Loss: 0.00000844
Iteration 184/1000 | Loss: 0.00000844
Iteration 185/1000 | Loss: 0.00000843
Iteration 186/1000 | Loss: 0.00000843
Iteration 187/1000 | Loss: 0.00000843
Iteration 188/1000 | Loss: 0.00000843
Iteration 189/1000 | Loss: 0.00000843
Iteration 190/1000 | Loss: 0.00000843
Iteration 191/1000 | Loss: 0.00000843
Iteration 192/1000 | Loss: 0.00000843
Iteration 193/1000 | Loss: 0.00000843
Iteration 194/1000 | Loss: 0.00000843
Iteration 195/1000 | Loss: 0.00000843
Iteration 196/1000 | Loss: 0.00000843
Iteration 197/1000 | Loss: 0.00000843
Iteration 198/1000 | Loss: 0.00000843
Iteration 199/1000 | Loss: 0.00000843
Iteration 200/1000 | Loss: 0.00000842
Iteration 201/1000 | Loss: 0.00000842
Iteration 202/1000 | Loss: 0.00000842
Iteration 203/1000 | Loss: 0.00000842
Iteration 204/1000 | Loss: 0.00000842
Iteration 205/1000 | Loss: 0.00000842
Iteration 206/1000 | Loss: 0.00000842
Iteration 207/1000 | Loss: 0.00000842
Iteration 208/1000 | Loss: 0.00000842
Iteration 209/1000 | Loss: 0.00000842
Iteration 210/1000 | Loss: 0.00000842
Iteration 211/1000 | Loss: 0.00000842
Iteration 212/1000 | Loss: 0.00000842
Iteration 213/1000 | Loss: 0.00000842
Iteration 214/1000 | Loss: 0.00000842
Iteration 215/1000 | Loss: 0.00000842
Iteration 216/1000 | Loss: 0.00000841
Iteration 217/1000 | Loss: 0.00000841
Iteration 218/1000 | Loss: 0.00000841
Iteration 219/1000 | Loss: 0.00000841
Iteration 220/1000 | Loss: 0.00000841
Iteration 221/1000 | Loss: 0.00000841
Iteration 222/1000 | Loss: 0.00000841
Iteration 223/1000 | Loss: 0.00000841
Iteration 224/1000 | Loss: 0.00000841
Iteration 225/1000 | Loss: 0.00000841
Iteration 226/1000 | Loss: 0.00000841
Iteration 227/1000 | Loss: 0.00000841
Iteration 228/1000 | Loss: 0.00000841
Iteration 229/1000 | Loss: 0.00000841
Iteration 230/1000 | Loss: 0.00000841
Iteration 231/1000 | Loss: 0.00000841
Iteration 232/1000 | Loss: 0.00000841
Iteration 233/1000 | Loss: 0.00000841
Iteration 234/1000 | Loss: 0.00000841
Iteration 235/1000 | Loss: 0.00000841
Iteration 236/1000 | Loss: 0.00000841
Iteration 237/1000 | Loss: 0.00000841
Iteration 238/1000 | Loss: 0.00000841
Iteration 239/1000 | Loss: 0.00000840
Iteration 240/1000 | Loss: 0.00000840
Iteration 241/1000 | Loss: 0.00000840
Iteration 242/1000 | Loss: 0.00000840
Iteration 243/1000 | Loss: 0.00000840
Iteration 244/1000 | Loss: 0.00000840
Iteration 245/1000 | Loss: 0.00000840
Iteration 246/1000 | Loss: 0.00000840
Iteration 247/1000 | Loss: 0.00000840
Iteration 248/1000 | Loss: 0.00000840
Iteration 249/1000 | Loss: 0.00000840
Iteration 250/1000 | Loss: 0.00000840
Iteration 251/1000 | Loss: 0.00000840
Iteration 252/1000 | Loss: 0.00000840
Iteration 253/1000 | Loss: 0.00000840
Iteration 254/1000 | Loss: 0.00000840
Iteration 255/1000 | Loss: 0.00000840
Iteration 256/1000 | Loss: 0.00000840
Iteration 257/1000 | Loss: 0.00000840
Iteration 258/1000 | Loss: 0.00000840
Iteration 259/1000 | Loss: 0.00000840
Iteration 260/1000 | Loss: 0.00000840
Iteration 261/1000 | Loss: 0.00000840
Iteration 262/1000 | Loss: 0.00000840
Iteration 263/1000 | Loss: 0.00000840
Iteration 264/1000 | Loss: 0.00000840
Iteration 265/1000 | Loss: 0.00000840
Iteration 266/1000 | Loss: 0.00000840
Iteration 267/1000 | Loss: 0.00000840
Iteration 268/1000 | Loss: 0.00000840
Iteration 269/1000 | Loss: 0.00000840
Iteration 270/1000 | Loss: 0.00000840
Iteration 271/1000 | Loss: 0.00000840
Iteration 272/1000 | Loss: 0.00000840
Iteration 273/1000 | Loss: 0.00000840
Iteration 274/1000 | Loss: 0.00000840
Iteration 275/1000 | Loss: 0.00000840
Iteration 276/1000 | Loss: 0.00000840
Iteration 277/1000 | Loss: 0.00000840
Iteration 278/1000 | Loss: 0.00000840
Iteration 279/1000 | Loss: 0.00000840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [8.398074896831531e-06, 8.398074896831531e-06, 8.398074896831531e-06, 8.398074896831531e-06, 8.398074896831531e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.398074896831531e-06

Optimization complete. Final v2v error: 2.4760031700134277 mm

Highest mean error: 3.1268248558044434 mm for frame 77

Lowest mean error: 2.3073623180389404 mm for frame 151

Saving results

Total time: 43.62954378128052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791858
Iteration 2/25 | Loss: 0.00139213
Iteration 3/25 | Loss: 0.00116340
Iteration 4/25 | Loss: 0.00114873
Iteration 5/25 | Loss: 0.00114351
Iteration 6/25 | Loss: 0.00114230
Iteration 7/25 | Loss: 0.00114230
Iteration 8/25 | Loss: 0.00114230
Iteration 9/25 | Loss: 0.00114230
Iteration 10/25 | Loss: 0.00114230
Iteration 11/25 | Loss: 0.00114230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001142299734055996, 0.001142299734055996, 0.001142299734055996, 0.001142299734055996, 0.001142299734055996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001142299734055996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11800027
Iteration 2/25 | Loss: 0.00085792
Iteration 3/25 | Loss: 0.00085792
Iteration 4/25 | Loss: 0.00085792
Iteration 5/25 | Loss: 0.00085792
Iteration 6/25 | Loss: 0.00085792
Iteration 7/25 | Loss: 0.00085792
Iteration 8/25 | Loss: 0.00085792
Iteration 9/25 | Loss: 0.00085792
Iteration 10/25 | Loss: 0.00085792
Iteration 11/25 | Loss: 0.00085792
Iteration 12/25 | Loss: 0.00085792
Iteration 13/25 | Loss: 0.00085792
Iteration 14/25 | Loss: 0.00085792
Iteration 15/25 | Loss: 0.00085792
Iteration 16/25 | Loss: 0.00085792
Iteration 17/25 | Loss: 0.00085792
Iteration 18/25 | Loss: 0.00085792
Iteration 19/25 | Loss: 0.00085792
Iteration 20/25 | Loss: 0.00085792
Iteration 21/25 | Loss: 0.00085792
Iteration 22/25 | Loss: 0.00085792
Iteration 23/25 | Loss: 0.00085792
Iteration 24/25 | Loss: 0.00085792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008579177083447576, 0.0008579177083447576, 0.0008579177083447576, 0.0008579177083447576, 0.0008579177083447576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008579177083447576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085792
Iteration 2/1000 | Loss: 0.00004433
Iteration 3/1000 | Loss: 0.00002752
Iteration 4/1000 | Loss: 0.00002220
Iteration 5/1000 | Loss: 0.00001998
Iteration 6/1000 | Loss: 0.00001847
Iteration 7/1000 | Loss: 0.00001752
Iteration 8/1000 | Loss: 0.00001667
Iteration 9/1000 | Loss: 0.00001625
Iteration 10/1000 | Loss: 0.00001581
Iteration 11/1000 | Loss: 0.00001540
Iteration 12/1000 | Loss: 0.00001532
Iteration 13/1000 | Loss: 0.00001521
Iteration 14/1000 | Loss: 0.00001502
Iteration 15/1000 | Loss: 0.00001501
Iteration 16/1000 | Loss: 0.00001498
Iteration 17/1000 | Loss: 0.00001495
Iteration 18/1000 | Loss: 0.00001485
Iteration 19/1000 | Loss: 0.00001474
Iteration 20/1000 | Loss: 0.00001467
Iteration 21/1000 | Loss: 0.00001454
Iteration 22/1000 | Loss: 0.00001451
Iteration 23/1000 | Loss: 0.00001449
Iteration 24/1000 | Loss: 0.00001449
Iteration 25/1000 | Loss: 0.00001446
Iteration 26/1000 | Loss: 0.00001445
Iteration 27/1000 | Loss: 0.00001444
Iteration 28/1000 | Loss: 0.00001443
Iteration 29/1000 | Loss: 0.00001443
Iteration 30/1000 | Loss: 0.00001442
Iteration 31/1000 | Loss: 0.00001442
Iteration 32/1000 | Loss: 0.00001441
Iteration 33/1000 | Loss: 0.00001440
Iteration 34/1000 | Loss: 0.00001438
Iteration 35/1000 | Loss: 0.00001437
Iteration 36/1000 | Loss: 0.00001435
Iteration 37/1000 | Loss: 0.00001433
Iteration 38/1000 | Loss: 0.00001432
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001431
Iteration 41/1000 | Loss: 0.00001431
Iteration 42/1000 | Loss: 0.00001431
Iteration 43/1000 | Loss: 0.00001430
Iteration 44/1000 | Loss: 0.00001430
Iteration 45/1000 | Loss: 0.00001430
Iteration 46/1000 | Loss: 0.00001430
Iteration 47/1000 | Loss: 0.00001429
Iteration 48/1000 | Loss: 0.00001429
Iteration 49/1000 | Loss: 0.00001429
Iteration 50/1000 | Loss: 0.00001428
Iteration 51/1000 | Loss: 0.00001428
Iteration 52/1000 | Loss: 0.00001427
Iteration 53/1000 | Loss: 0.00001427
Iteration 54/1000 | Loss: 0.00001426
Iteration 55/1000 | Loss: 0.00001426
Iteration 56/1000 | Loss: 0.00001426
Iteration 57/1000 | Loss: 0.00001425
Iteration 58/1000 | Loss: 0.00001425
Iteration 59/1000 | Loss: 0.00001425
Iteration 60/1000 | Loss: 0.00001425
Iteration 61/1000 | Loss: 0.00001424
Iteration 62/1000 | Loss: 0.00001424
Iteration 63/1000 | Loss: 0.00001424
Iteration 64/1000 | Loss: 0.00001423
Iteration 65/1000 | Loss: 0.00001423
Iteration 66/1000 | Loss: 0.00001423
Iteration 67/1000 | Loss: 0.00001422
Iteration 68/1000 | Loss: 0.00001422
Iteration 69/1000 | Loss: 0.00001422
Iteration 70/1000 | Loss: 0.00001422
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001422
Iteration 73/1000 | Loss: 0.00001422
Iteration 74/1000 | Loss: 0.00001422
Iteration 75/1000 | Loss: 0.00001422
Iteration 76/1000 | Loss: 0.00001422
Iteration 77/1000 | Loss: 0.00001422
Iteration 78/1000 | Loss: 0.00001422
Iteration 79/1000 | Loss: 0.00001422
Iteration 80/1000 | Loss: 0.00001422
Iteration 81/1000 | Loss: 0.00001422
Iteration 82/1000 | Loss: 0.00001422
Iteration 83/1000 | Loss: 0.00001422
Iteration 84/1000 | Loss: 0.00001422
Iteration 85/1000 | Loss: 0.00001422
Iteration 86/1000 | Loss: 0.00001422
Iteration 87/1000 | Loss: 0.00001422
Iteration 88/1000 | Loss: 0.00001422
Iteration 89/1000 | Loss: 0.00001422
Iteration 90/1000 | Loss: 0.00001422
Iteration 91/1000 | Loss: 0.00001422
Iteration 92/1000 | Loss: 0.00001422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.4217936040950008e-05, 1.4217936040950008e-05, 1.4217936040950008e-05, 1.4217936040950008e-05, 1.4217936040950008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4217936040950008e-05

Optimization complete. Final v2v error: 3.0975286960601807 mm

Highest mean error: 4.2169928550720215 mm for frame 81

Lowest mean error: 2.4067938327789307 mm for frame 197

Saving results

Total time: 43.256574392318726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887421
Iteration 2/25 | Loss: 0.00166427
Iteration 3/25 | Loss: 0.00132777
Iteration 4/25 | Loss: 0.00129564
Iteration 5/25 | Loss: 0.00129234
Iteration 6/25 | Loss: 0.00127491
Iteration 7/25 | Loss: 0.00128719
Iteration 8/25 | Loss: 0.00126875
Iteration 9/25 | Loss: 0.00127421
Iteration 10/25 | Loss: 0.00126439
Iteration 11/25 | Loss: 0.00126320
Iteration 12/25 | Loss: 0.00126064
Iteration 13/25 | Loss: 0.00125984
Iteration 14/25 | Loss: 0.00125955
Iteration 15/25 | Loss: 0.00125872
Iteration 16/25 | Loss: 0.00125833
Iteration 17/25 | Loss: 0.00125815
Iteration 18/25 | Loss: 0.00125800
Iteration 19/25 | Loss: 0.00125781
Iteration 20/25 | Loss: 0.00126154
Iteration 21/25 | Loss: 0.00125744
Iteration 22/25 | Loss: 0.00125664
Iteration 23/25 | Loss: 0.00125645
Iteration 24/25 | Loss: 0.00125645
Iteration 25/25 | Loss: 0.00125645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36200762
Iteration 2/25 | Loss: 0.00078450
Iteration 3/25 | Loss: 0.00078450
Iteration 4/25 | Loss: 0.00078450
Iteration 5/25 | Loss: 0.00078450
Iteration 6/25 | Loss: 0.00078450
Iteration 7/25 | Loss: 0.00078450
Iteration 8/25 | Loss: 0.00078450
Iteration 9/25 | Loss: 0.00078450
Iteration 10/25 | Loss: 0.00078450
Iteration 11/25 | Loss: 0.00078450
Iteration 12/25 | Loss: 0.00078450
Iteration 13/25 | Loss: 0.00078450
Iteration 14/25 | Loss: 0.00078450
Iteration 15/25 | Loss: 0.00078450
Iteration 16/25 | Loss: 0.00078450
Iteration 17/25 | Loss: 0.00078450
Iteration 18/25 | Loss: 0.00078450
Iteration 19/25 | Loss: 0.00078450
Iteration 20/25 | Loss: 0.00078450
Iteration 21/25 | Loss: 0.00078450
Iteration 22/25 | Loss: 0.00078450
Iteration 23/25 | Loss: 0.00078450
Iteration 24/25 | Loss: 0.00078450
Iteration 25/25 | Loss: 0.00078450

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078450
Iteration 2/1000 | Loss: 0.00013252
Iteration 3/1000 | Loss: 0.00004832
Iteration 4/1000 | Loss: 0.00003424
Iteration 5/1000 | Loss: 0.00003007
Iteration 6/1000 | Loss: 0.00002756
Iteration 7/1000 | Loss: 0.00002624
Iteration 8/1000 | Loss: 0.00002551
Iteration 9/1000 | Loss: 0.00002492
Iteration 10/1000 | Loss: 0.00002449
Iteration 11/1000 | Loss: 0.00002409
Iteration 12/1000 | Loss: 0.00002377
Iteration 13/1000 | Loss: 0.00002350
Iteration 14/1000 | Loss: 0.00002317
Iteration 15/1000 | Loss: 0.00002298
Iteration 16/1000 | Loss: 0.00002278
Iteration 17/1000 | Loss: 0.00002273
Iteration 18/1000 | Loss: 0.00002272
Iteration 19/1000 | Loss: 0.00021246
Iteration 20/1000 | Loss: 0.00047116
Iteration 21/1000 | Loss: 0.00048785
Iteration 22/1000 | Loss: 0.00048779
Iteration 23/1000 | Loss: 0.00019710
Iteration 24/1000 | Loss: 0.00002470
Iteration 25/1000 | Loss: 0.00002303
Iteration 26/1000 | Loss: 0.00002213
Iteration 27/1000 | Loss: 0.00002153
Iteration 28/1000 | Loss: 0.00002120
Iteration 29/1000 | Loss: 0.00002093
Iteration 30/1000 | Loss: 0.00002091
Iteration 31/1000 | Loss: 0.00002090
Iteration 32/1000 | Loss: 0.00002084
Iteration 33/1000 | Loss: 0.00002080
Iteration 34/1000 | Loss: 0.00002080
Iteration 35/1000 | Loss: 0.00002079
Iteration 36/1000 | Loss: 0.00002078
Iteration 37/1000 | Loss: 0.00002078
Iteration 38/1000 | Loss: 0.00002078
Iteration 39/1000 | Loss: 0.00002077
Iteration 40/1000 | Loss: 0.00002077
Iteration 41/1000 | Loss: 0.00002077
Iteration 42/1000 | Loss: 0.00002077
Iteration 43/1000 | Loss: 0.00002077
Iteration 44/1000 | Loss: 0.00002076
Iteration 45/1000 | Loss: 0.00002076
Iteration 46/1000 | Loss: 0.00002076
Iteration 47/1000 | Loss: 0.00002076
Iteration 48/1000 | Loss: 0.00002076
Iteration 49/1000 | Loss: 0.00002076
Iteration 50/1000 | Loss: 0.00002076
Iteration 51/1000 | Loss: 0.00002075
Iteration 52/1000 | Loss: 0.00002075
Iteration 53/1000 | Loss: 0.00002075
Iteration 54/1000 | Loss: 0.00002074
Iteration 55/1000 | Loss: 0.00002074
Iteration 56/1000 | Loss: 0.00002074
Iteration 57/1000 | Loss: 0.00002074
Iteration 58/1000 | Loss: 0.00002073
Iteration 59/1000 | Loss: 0.00002073
Iteration 60/1000 | Loss: 0.00002073
Iteration 61/1000 | Loss: 0.00002073
Iteration 62/1000 | Loss: 0.00002072
Iteration 63/1000 | Loss: 0.00002072
Iteration 64/1000 | Loss: 0.00002072
Iteration 65/1000 | Loss: 0.00002072
Iteration 66/1000 | Loss: 0.00002072
Iteration 67/1000 | Loss: 0.00002071
Iteration 68/1000 | Loss: 0.00002071
Iteration 69/1000 | Loss: 0.00002071
Iteration 70/1000 | Loss: 0.00002071
Iteration 71/1000 | Loss: 0.00002071
Iteration 72/1000 | Loss: 0.00002071
Iteration 73/1000 | Loss: 0.00002071
Iteration 74/1000 | Loss: 0.00002071
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002070
Iteration 77/1000 | Loss: 0.00002070
Iteration 78/1000 | Loss: 0.00002070
Iteration 79/1000 | Loss: 0.00002070
Iteration 80/1000 | Loss: 0.00002070
Iteration 81/1000 | Loss: 0.00002070
Iteration 82/1000 | Loss: 0.00002070
Iteration 83/1000 | Loss: 0.00002070
Iteration 84/1000 | Loss: 0.00002070
Iteration 85/1000 | Loss: 0.00002070
Iteration 86/1000 | Loss: 0.00002070
Iteration 87/1000 | Loss: 0.00002070
Iteration 88/1000 | Loss: 0.00002070
Iteration 89/1000 | Loss: 0.00002070
Iteration 90/1000 | Loss: 0.00002069
Iteration 91/1000 | Loss: 0.00002069
Iteration 92/1000 | Loss: 0.00002069
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002069
Iteration 97/1000 | Loss: 0.00002069
Iteration 98/1000 | Loss: 0.00002068
Iteration 99/1000 | Loss: 0.00002068
Iteration 100/1000 | Loss: 0.00002068
Iteration 101/1000 | Loss: 0.00002068
Iteration 102/1000 | Loss: 0.00002068
Iteration 103/1000 | Loss: 0.00002068
Iteration 104/1000 | Loss: 0.00002068
Iteration 105/1000 | Loss: 0.00002068
Iteration 106/1000 | Loss: 0.00002068
Iteration 107/1000 | Loss: 0.00002068
Iteration 108/1000 | Loss: 0.00002068
Iteration 109/1000 | Loss: 0.00002068
Iteration 110/1000 | Loss: 0.00002068
Iteration 111/1000 | Loss: 0.00002068
Iteration 112/1000 | Loss: 0.00002068
Iteration 113/1000 | Loss: 0.00002068
Iteration 114/1000 | Loss: 0.00002068
Iteration 115/1000 | Loss: 0.00002068
Iteration 116/1000 | Loss: 0.00002068
Iteration 117/1000 | Loss: 0.00002067
Iteration 118/1000 | Loss: 0.00002067
Iteration 119/1000 | Loss: 0.00002067
Iteration 120/1000 | Loss: 0.00002067
Iteration 121/1000 | Loss: 0.00002067
Iteration 122/1000 | Loss: 0.00002067
Iteration 123/1000 | Loss: 0.00002067
Iteration 124/1000 | Loss: 0.00002067
Iteration 125/1000 | Loss: 0.00002067
Iteration 126/1000 | Loss: 0.00002067
Iteration 127/1000 | Loss: 0.00002067
Iteration 128/1000 | Loss: 0.00002066
Iteration 129/1000 | Loss: 0.00002066
Iteration 130/1000 | Loss: 0.00002066
Iteration 131/1000 | Loss: 0.00002066
Iteration 132/1000 | Loss: 0.00002066
Iteration 133/1000 | Loss: 0.00002066
Iteration 134/1000 | Loss: 0.00002066
Iteration 135/1000 | Loss: 0.00002066
Iteration 136/1000 | Loss: 0.00002066
Iteration 137/1000 | Loss: 0.00002066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.066283741442021e-05, 2.066283741442021e-05, 2.066283741442021e-05, 2.066283741442021e-05, 2.066283741442021e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.066283741442021e-05

Optimization complete. Final v2v error: 3.78277587890625 mm

Highest mean error: 5.359926700592041 mm for frame 222

Lowest mean error: 3.122187376022339 mm for frame 231

Saving results

Total time: 98.24503684043884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00363129
Iteration 2/25 | Loss: 0.00116355
Iteration 3/25 | Loss: 0.00111242
Iteration 4/25 | Loss: 0.00110347
Iteration 5/25 | Loss: 0.00110060
Iteration 6/25 | Loss: 0.00110060
Iteration 7/25 | Loss: 0.00110060
Iteration 8/25 | Loss: 0.00110060
Iteration 9/25 | Loss: 0.00110060
Iteration 10/25 | Loss: 0.00110060
Iteration 11/25 | Loss: 0.00110060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011006029089912772, 0.0011006029089912772, 0.0011006029089912772, 0.0011006029089912772, 0.0011006029089912772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011006029089912772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63234758
Iteration 2/25 | Loss: 0.00084031
Iteration 3/25 | Loss: 0.00084031
Iteration 4/25 | Loss: 0.00084031
Iteration 5/25 | Loss: 0.00084031
Iteration 6/25 | Loss: 0.00084031
Iteration 7/25 | Loss: 0.00084031
Iteration 8/25 | Loss: 0.00084031
Iteration 9/25 | Loss: 0.00084031
Iteration 10/25 | Loss: 0.00084031
Iteration 11/25 | Loss: 0.00084031
Iteration 12/25 | Loss: 0.00084031
Iteration 13/25 | Loss: 0.00084031
Iteration 14/25 | Loss: 0.00084031
Iteration 15/25 | Loss: 0.00084031
Iteration 16/25 | Loss: 0.00084031
Iteration 17/25 | Loss: 0.00084031
Iteration 18/25 | Loss: 0.00084031
Iteration 19/25 | Loss: 0.00084031
Iteration 20/25 | Loss: 0.00084031
Iteration 21/25 | Loss: 0.00084031
Iteration 22/25 | Loss: 0.00084031
Iteration 23/25 | Loss: 0.00084031
Iteration 24/25 | Loss: 0.00084031
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008403087849728763, 0.0008403087849728763, 0.0008403087849728763, 0.0008403087849728763, 0.0008403087849728763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008403087849728763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084031
Iteration 2/1000 | Loss: 0.00001485
Iteration 3/1000 | Loss: 0.00001172
Iteration 4/1000 | Loss: 0.00001090
Iteration 5/1000 | Loss: 0.00001035
Iteration 6/1000 | Loss: 0.00000991
Iteration 7/1000 | Loss: 0.00000983
Iteration 8/1000 | Loss: 0.00000956
Iteration 9/1000 | Loss: 0.00000944
Iteration 10/1000 | Loss: 0.00000944
Iteration 11/1000 | Loss: 0.00000938
Iteration 12/1000 | Loss: 0.00000932
Iteration 13/1000 | Loss: 0.00000927
Iteration 14/1000 | Loss: 0.00000926
Iteration 15/1000 | Loss: 0.00000925
Iteration 16/1000 | Loss: 0.00000915
Iteration 17/1000 | Loss: 0.00000915
Iteration 18/1000 | Loss: 0.00000910
Iteration 19/1000 | Loss: 0.00000910
Iteration 20/1000 | Loss: 0.00000909
Iteration 21/1000 | Loss: 0.00000908
Iteration 22/1000 | Loss: 0.00000906
Iteration 23/1000 | Loss: 0.00000906
Iteration 24/1000 | Loss: 0.00000906
Iteration 25/1000 | Loss: 0.00000904
Iteration 26/1000 | Loss: 0.00000902
Iteration 27/1000 | Loss: 0.00000902
Iteration 28/1000 | Loss: 0.00000902
Iteration 29/1000 | Loss: 0.00000901
Iteration 30/1000 | Loss: 0.00000898
Iteration 31/1000 | Loss: 0.00000898
Iteration 32/1000 | Loss: 0.00000898
Iteration 33/1000 | Loss: 0.00000898
Iteration 34/1000 | Loss: 0.00000898
Iteration 35/1000 | Loss: 0.00000898
Iteration 36/1000 | Loss: 0.00000898
Iteration 37/1000 | Loss: 0.00000898
Iteration 38/1000 | Loss: 0.00000897
Iteration 39/1000 | Loss: 0.00000897
Iteration 40/1000 | Loss: 0.00000896
Iteration 41/1000 | Loss: 0.00000896
Iteration 42/1000 | Loss: 0.00000895
Iteration 43/1000 | Loss: 0.00000894
Iteration 44/1000 | Loss: 0.00000893
Iteration 45/1000 | Loss: 0.00000892
Iteration 46/1000 | Loss: 0.00000891
Iteration 47/1000 | Loss: 0.00000891
Iteration 48/1000 | Loss: 0.00000890
Iteration 49/1000 | Loss: 0.00000889
Iteration 50/1000 | Loss: 0.00000889
Iteration 51/1000 | Loss: 0.00000889
Iteration 52/1000 | Loss: 0.00000888
Iteration 53/1000 | Loss: 0.00000888
Iteration 54/1000 | Loss: 0.00000887
Iteration 55/1000 | Loss: 0.00000887
Iteration 56/1000 | Loss: 0.00000887
Iteration 57/1000 | Loss: 0.00000886
Iteration 58/1000 | Loss: 0.00000883
Iteration 59/1000 | Loss: 0.00000883
Iteration 60/1000 | Loss: 0.00000882
Iteration 61/1000 | Loss: 0.00000882
Iteration 62/1000 | Loss: 0.00000881
Iteration 63/1000 | Loss: 0.00000881
Iteration 64/1000 | Loss: 0.00000880
Iteration 65/1000 | Loss: 0.00000879
Iteration 66/1000 | Loss: 0.00000879
Iteration 67/1000 | Loss: 0.00000878
Iteration 68/1000 | Loss: 0.00000878
Iteration 69/1000 | Loss: 0.00000878
Iteration 70/1000 | Loss: 0.00000877
Iteration 71/1000 | Loss: 0.00000877
Iteration 72/1000 | Loss: 0.00000877
Iteration 73/1000 | Loss: 0.00000877
Iteration 74/1000 | Loss: 0.00000876
Iteration 75/1000 | Loss: 0.00000876
Iteration 76/1000 | Loss: 0.00000875
Iteration 77/1000 | Loss: 0.00000875
Iteration 78/1000 | Loss: 0.00000874
Iteration 79/1000 | Loss: 0.00000874
Iteration 80/1000 | Loss: 0.00000874
Iteration 81/1000 | Loss: 0.00000873
Iteration 82/1000 | Loss: 0.00000873
Iteration 83/1000 | Loss: 0.00000873
Iteration 84/1000 | Loss: 0.00000873
Iteration 85/1000 | Loss: 0.00000873
Iteration 86/1000 | Loss: 0.00000872
Iteration 87/1000 | Loss: 0.00000872
Iteration 88/1000 | Loss: 0.00000872
Iteration 89/1000 | Loss: 0.00000871
Iteration 90/1000 | Loss: 0.00000871
Iteration 91/1000 | Loss: 0.00000871
Iteration 92/1000 | Loss: 0.00000871
Iteration 93/1000 | Loss: 0.00000871
Iteration 94/1000 | Loss: 0.00000871
Iteration 95/1000 | Loss: 0.00000871
Iteration 96/1000 | Loss: 0.00000871
Iteration 97/1000 | Loss: 0.00000871
Iteration 98/1000 | Loss: 0.00000871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [8.712985618331004e-06, 8.712985618331004e-06, 8.712985618331004e-06, 8.712985618331004e-06, 8.712985618331004e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.712985618331004e-06

Optimization complete. Final v2v error: 2.5541956424713135 mm

Highest mean error: 2.960331678390503 mm for frame 136

Lowest mean error: 2.4579813480377197 mm for frame 6

Saving results

Total time: 34.81598877906799
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450845
Iteration 2/25 | Loss: 0.00133317
Iteration 3/25 | Loss: 0.00119171
Iteration 4/25 | Loss: 0.00117747
Iteration 5/25 | Loss: 0.00117214
Iteration 6/25 | Loss: 0.00117144
Iteration 7/25 | Loss: 0.00117144
Iteration 8/25 | Loss: 0.00117144
Iteration 9/25 | Loss: 0.00117144
Iteration 10/25 | Loss: 0.00117144
Iteration 11/25 | Loss: 0.00117144
Iteration 12/25 | Loss: 0.00117144
Iteration 13/25 | Loss: 0.00117144
Iteration 14/25 | Loss: 0.00117144
Iteration 15/25 | Loss: 0.00117144
Iteration 16/25 | Loss: 0.00117144
Iteration 17/25 | Loss: 0.00117144
Iteration 18/25 | Loss: 0.00117144
Iteration 19/25 | Loss: 0.00117144
Iteration 20/25 | Loss: 0.00117144
Iteration 21/25 | Loss: 0.00117144
Iteration 22/25 | Loss: 0.00117144
Iteration 23/25 | Loss: 0.00117144
Iteration 24/25 | Loss: 0.00117144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011714386055245996, 0.0011714386055245996, 0.0011714386055245996, 0.0011714386055245996, 0.0011714386055245996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011714386055245996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36626780
Iteration 2/25 | Loss: 0.00104124
Iteration 3/25 | Loss: 0.00104124
Iteration 4/25 | Loss: 0.00104124
Iteration 5/25 | Loss: 0.00104124
Iteration 6/25 | Loss: 0.00104124
Iteration 7/25 | Loss: 0.00104124
Iteration 8/25 | Loss: 0.00104124
Iteration 9/25 | Loss: 0.00104124
Iteration 10/25 | Loss: 0.00104124
Iteration 11/25 | Loss: 0.00104124
Iteration 12/25 | Loss: 0.00104124
Iteration 13/25 | Loss: 0.00104124
Iteration 14/25 | Loss: 0.00104124
Iteration 15/25 | Loss: 0.00104124
Iteration 16/25 | Loss: 0.00104124
Iteration 17/25 | Loss: 0.00104124
Iteration 18/25 | Loss: 0.00104124
Iteration 19/25 | Loss: 0.00104124
Iteration 20/25 | Loss: 0.00104124
Iteration 21/25 | Loss: 0.00104124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010412399424239993, 0.0010412399424239993, 0.0010412399424239993, 0.0010412399424239993, 0.0010412399424239993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010412399424239993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104124
Iteration 2/1000 | Loss: 0.00002063
Iteration 3/1000 | Loss: 0.00001580
Iteration 4/1000 | Loss: 0.00001453
Iteration 5/1000 | Loss: 0.00001392
Iteration 6/1000 | Loss: 0.00001354
Iteration 7/1000 | Loss: 0.00001335
Iteration 8/1000 | Loss: 0.00001307
Iteration 9/1000 | Loss: 0.00001285
Iteration 10/1000 | Loss: 0.00001283
Iteration 11/1000 | Loss: 0.00001281
Iteration 12/1000 | Loss: 0.00001270
Iteration 13/1000 | Loss: 0.00001269
Iteration 14/1000 | Loss: 0.00001269
Iteration 15/1000 | Loss: 0.00001268
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001268
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001268
Iteration 20/1000 | Loss: 0.00001267
Iteration 21/1000 | Loss: 0.00001266
Iteration 22/1000 | Loss: 0.00001265
Iteration 23/1000 | Loss: 0.00001265
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001265
Iteration 26/1000 | Loss: 0.00001264
Iteration 27/1000 | Loss: 0.00001264
Iteration 28/1000 | Loss: 0.00001263
Iteration 29/1000 | Loss: 0.00001263
Iteration 30/1000 | Loss: 0.00001262
Iteration 31/1000 | Loss: 0.00001262
Iteration 32/1000 | Loss: 0.00001262
Iteration 33/1000 | Loss: 0.00001261
Iteration 34/1000 | Loss: 0.00001261
Iteration 35/1000 | Loss: 0.00001260
Iteration 36/1000 | Loss: 0.00001260
Iteration 37/1000 | Loss: 0.00001260
Iteration 38/1000 | Loss: 0.00001259
Iteration 39/1000 | Loss: 0.00001259
Iteration 40/1000 | Loss: 0.00001258
Iteration 41/1000 | Loss: 0.00001258
Iteration 42/1000 | Loss: 0.00001257
Iteration 43/1000 | Loss: 0.00001255
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001255
Iteration 46/1000 | Loss: 0.00001255
Iteration 47/1000 | Loss: 0.00001255
Iteration 48/1000 | Loss: 0.00001255
Iteration 49/1000 | Loss: 0.00001255
Iteration 50/1000 | Loss: 0.00001255
Iteration 51/1000 | Loss: 0.00001254
Iteration 52/1000 | Loss: 0.00001254
Iteration 53/1000 | Loss: 0.00001254
Iteration 54/1000 | Loss: 0.00001253
Iteration 55/1000 | Loss: 0.00001253
Iteration 56/1000 | Loss: 0.00001253
Iteration 57/1000 | Loss: 0.00001252
Iteration 58/1000 | Loss: 0.00001251
Iteration 59/1000 | Loss: 0.00001251
Iteration 60/1000 | Loss: 0.00001250
Iteration 61/1000 | Loss: 0.00001249
Iteration 62/1000 | Loss: 0.00001249
Iteration 63/1000 | Loss: 0.00001249
Iteration 64/1000 | Loss: 0.00001248
Iteration 65/1000 | Loss: 0.00001248
Iteration 66/1000 | Loss: 0.00001248
Iteration 67/1000 | Loss: 0.00001247
Iteration 68/1000 | Loss: 0.00001246
Iteration 69/1000 | Loss: 0.00001246
Iteration 70/1000 | Loss: 0.00001246
Iteration 71/1000 | Loss: 0.00001246
Iteration 72/1000 | Loss: 0.00001245
Iteration 73/1000 | Loss: 0.00001245
Iteration 74/1000 | Loss: 0.00001244
Iteration 75/1000 | Loss: 0.00001244
Iteration 76/1000 | Loss: 0.00001244
Iteration 77/1000 | Loss: 0.00001243
Iteration 78/1000 | Loss: 0.00001243
Iteration 79/1000 | Loss: 0.00001242
Iteration 80/1000 | Loss: 0.00001242
Iteration 81/1000 | Loss: 0.00001242
Iteration 82/1000 | Loss: 0.00001242
Iteration 83/1000 | Loss: 0.00001241
Iteration 84/1000 | Loss: 0.00001241
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001241
Iteration 88/1000 | Loss: 0.00001241
Iteration 89/1000 | Loss: 0.00001241
Iteration 90/1000 | Loss: 0.00001241
Iteration 91/1000 | Loss: 0.00001240
Iteration 92/1000 | Loss: 0.00001240
Iteration 93/1000 | Loss: 0.00001240
Iteration 94/1000 | Loss: 0.00001240
Iteration 95/1000 | Loss: 0.00001238
Iteration 96/1000 | Loss: 0.00001238
Iteration 97/1000 | Loss: 0.00001237
Iteration 98/1000 | Loss: 0.00001236
Iteration 99/1000 | Loss: 0.00001235
Iteration 100/1000 | Loss: 0.00001235
Iteration 101/1000 | Loss: 0.00001235
Iteration 102/1000 | Loss: 0.00001234
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001233
Iteration 105/1000 | Loss: 0.00001233
Iteration 106/1000 | Loss: 0.00001232
Iteration 107/1000 | Loss: 0.00001232
Iteration 108/1000 | Loss: 0.00001232
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001231
Iteration 111/1000 | Loss: 0.00001231
Iteration 112/1000 | Loss: 0.00001231
Iteration 113/1000 | Loss: 0.00001231
Iteration 114/1000 | Loss: 0.00001231
Iteration 115/1000 | Loss: 0.00001231
Iteration 116/1000 | Loss: 0.00001231
Iteration 117/1000 | Loss: 0.00001231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.2307522410992533e-05, 1.2307522410992533e-05, 1.2307522410992533e-05, 1.2307522410992533e-05, 1.2307522410992533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2307522410992533e-05

Optimization complete. Final v2v error: 2.9389877319335938 mm

Highest mean error: 3.3951480388641357 mm for frame 77

Lowest mean error: 2.6577532291412354 mm for frame 160

Saving results

Total time: 35.55974984169006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071320
Iteration 2/25 | Loss: 0.00191801
Iteration 3/25 | Loss: 0.00172695
Iteration 4/25 | Loss: 0.00147878
Iteration 5/25 | Loss: 0.00134591
Iteration 6/25 | Loss: 0.00125467
Iteration 7/25 | Loss: 0.00120870
Iteration 8/25 | Loss: 0.00118705
Iteration 9/25 | Loss: 0.00118039
Iteration 10/25 | Loss: 0.00116706
Iteration 11/25 | Loss: 0.00115696
Iteration 12/25 | Loss: 0.00115526
Iteration 13/25 | Loss: 0.00115448
Iteration 14/25 | Loss: 0.00115375
Iteration 15/25 | Loss: 0.00115345
Iteration 16/25 | Loss: 0.00115341
Iteration 17/25 | Loss: 0.00115338
Iteration 18/25 | Loss: 0.00115337
Iteration 19/25 | Loss: 0.00115337
Iteration 20/25 | Loss: 0.00115337
Iteration 21/25 | Loss: 0.00115337
Iteration 22/25 | Loss: 0.00115337
Iteration 23/25 | Loss: 0.00115337
Iteration 24/25 | Loss: 0.00115337
Iteration 25/25 | Loss: 0.00115337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37947047
Iteration 2/25 | Loss: 0.00095159
Iteration 3/25 | Loss: 0.00095159
Iteration 4/25 | Loss: 0.00095159
Iteration 5/25 | Loss: 0.00095159
Iteration 6/25 | Loss: 0.00095159
Iteration 7/25 | Loss: 0.00095159
Iteration 8/25 | Loss: 0.00095159
Iteration 9/25 | Loss: 0.00095158
Iteration 10/25 | Loss: 0.00095158
Iteration 11/25 | Loss: 0.00095158
Iteration 12/25 | Loss: 0.00095158
Iteration 13/25 | Loss: 0.00095158
Iteration 14/25 | Loss: 0.00095158
Iteration 15/25 | Loss: 0.00095158
Iteration 16/25 | Loss: 0.00095158
Iteration 17/25 | Loss: 0.00095158
Iteration 18/25 | Loss: 0.00095158
Iteration 19/25 | Loss: 0.00095158
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009515838464722037, 0.0009515838464722037, 0.0009515838464722037, 0.0009515838464722037, 0.0009515838464722037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009515838464722037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095158
Iteration 2/1000 | Loss: 0.00002981
Iteration 3/1000 | Loss: 0.00002163
Iteration 4/1000 | Loss: 0.00002025
Iteration 5/1000 | Loss: 0.00001928
Iteration 6/1000 | Loss: 0.00001876
Iteration 7/1000 | Loss: 0.00001825
Iteration 8/1000 | Loss: 0.00040313
Iteration 9/1000 | Loss: 0.00002058
Iteration 10/1000 | Loss: 0.00001822
Iteration 11/1000 | Loss: 0.00001719
Iteration 12/1000 | Loss: 0.00001645
Iteration 13/1000 | Loss: 0.00001596
Iteration 14/1000 | Loss: 0.00001571
Iteration 15/1000 | Loss: 0.00001564
Iteration 16/1000 | Loss: 0.00001556
Iteration 17/1000 | Loss: 0.00001555
Iteration 18/1000 | Loss: 0.00001547
Iteration 19/1000 | Loss: 0.00001545
Iteration 20/1000 | Loss: 0.00001539
Iteration 21/1000 | Loss: 0.00001523
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00001509
Iteration 24/1000 | Loss: 0.00001502
Iteration 25/1000 | Loss: 0.00001502
Iteration 26/1000 | Loss: 0.00001502
Iteration 27/1000 | Loss: 0.00001502
Iteration 28/1000 | Loss: 0.00001501
Iteration 29/1000 | Loss: 0.00001500
Iteration 30/1000 | Loss: 0.00001500
Iteration 31/1000 | Loss: 0.00001497
Iteration 32/1000 | Loss: 0.00001497
Iteration 33/1000 | Loss: 0.00001496
Iteration 34/1000 | Loss: 0.00001495
Iteration 35/1000 | Loss: 0.00001493
Iteration 36/1000 | Loss: 0.00001493
Iteration 37/1000 | Loss: 0.00001493
Iteration 38/1000 | Loss: 0.00001493
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001493
Iteration 41/1000 | Loss: 0.00001493
Iteration 42/1000 | Loss: 0.00001493
Iteration 43/1000 | Loss: 0.00001492
Iteration 44/1000 | Loss: 0.00001492
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001490
Iteration 47/1000 | Loss: 0.00001490
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001488
Iteration 50/1000 | Loss: 0.00001488
Iteration 51/1000 | Loss: 0.00001488
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001487
Iteration 59/1000 | Loss: 0.00001487
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001487
Iteration 62/1000 | Loss: 0.00001487
Iteration 63/1000 | Loss: 0.00001487
Iteration 64/1000 | Loss: 0.00001487
Iteration 65/1000 | Loss: 0.00001486
Iteration 66/1000 | Loss: 0.00001486
Iteration 67/1000 | Loss: 0.00001486
Iteration 68/1000 | Loss: 0.00001486
Iteration 69/1000 | Loss: 0.00001486
Iteration 70/1000 | Loss: 0.00001486
Iteration 71/1000 | Loss: 0.00001486
Iteration 72/1000 | Loss: 0.00001486
Iteration 73/1000 | Loss: 0.00001486
Iteration 74/1000 | Loss: 0.00001486
Iteration 75/1000 | Loss: 0.00001486
Iteration 76/1000 | Loss: 0.00001486
Iteration 77/1000 | Loss: 0.00001485
Iteration 78/1000 | Loss: 0.00001485
Iteration 79/1000 | Loss: 0.00001485
Iteration 80/1000 | Loss: 0.00001485
Iteration 81/1000 | Loss: 0.00001485
Iteration 82/1000 | Loss: 0.00001485
Iteration 83/1000 | Loss: 0.00001485
Iteration 84/1000 | Loss: 0.00001485
Iteration 85/1000 | Loss: 0.00001485
Iteration 86/1000 | Loss: 0.00001485
Iteration 87/1000 | Loss: 0.00001484
Iteration 88/1000 | Loss: 0.00001484
Iteration 89/1000 | Loss: 0.00001484
Iteration 90/1000 | Loss: 0.00001484
Iteration 91/1000 | Loss: 0.00001484
Iteration 92/1000 | Loss: 0.00001484
Iteration 93/1000 | Loss: 0.00001484
Iteration 94/1000 | Loss: 0.00001483
Iteration 95/1000 | Loss: 0.00001483
Iteration 96/1000 | Loss: 0.00001483
Iteration 97/1000 | Loss: 0.00001483
Iteration 98/1000 | Loss: 0.00001483
Iteration 99/1000 | Loss: 0.00001483
Iteration 100/1000 | Loss: 0.00001483
Iteration 101/1000 | Loss: 0.00001483
Iteration 102/1000 | Loss: 0.00001483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.4830245163466316e-05, 1.4830245163466316e-05, 1.4830245163466316e-05, 1.4830245163466316e-05, 1.4830245163466316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4830245163466316e-05

Optimization complete. Final v2v error: 3.260746717453003 mm

Highest mean error: 4.427058219909668 mm for frame 66

Lowest mean error: 2.9046242237091064 mm for frame 9

Saving results

Total time: 61.226733922958374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058495
Iteration 2/25 | Loss: 0.01058495
Iteration 3/25 | Loss: 0.01058494
Iteration 4/25 | Loss: 0.01058494
Iteration 5/25 | Loss: 0.01058494
Iteration 6/25 | Loss: 0.01058494
Iteration 7/25 | Loss: 0.01058494
Iteration 8/25 | Loss: 0.01058494
Iteration 9/25 | Loss: 0.01058494
Iteration 10/25 | Loss: 0.01058494
Iteration 11/25 | Loss: 0.01058494
Iteration 12/25 | Loss: 0.01058494
Iteration 13/25 | Loss: 0.01058494
Iteration 14/25 | Loss: 0.01058494
Iteration 15/25 | Loss: 0.01058494
Iteration 16/25 | Loss: 0.01058494
Iteration 17/25 | Loss: 0.01058494
Iteration 18/25 | Loss: 0.01058494
Iteration 19/25 | Loss: 0.01058494
Iteration 20/25 | Loss: 0.01058494
Iteration 21/25 | Loss: 0.01058494
Iteration 22/25 | Loss: 0.01058494
Iteration 23/25 | Loss: 0.01058494
Iteration 24/25 | Loss: 0.01058494
Iteration 25/25 | Loss: 0.01058494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74894714
Iteration 2/25 | Loss: 0.06118611
Iteration 3/25 | Loss: 0.06101053
Iteration 4/25 | Loss: 0.05980468
Iteration 5/25 | Loss: 0.05980467
Iteration 6/25 | Loss: 0.05980467
Iteration 7/25 | Loss: 0.05980467
Iteration 8/25 | Loss: 0.05980466
Iteration 9/25 | Loss: 0.05980466
Iteration 10/25 | Loss: 0.05980466
Iteration 11/25 | Loss: 0.05980466
Iteration 12/25 | Loss: 0.05980466
Iteration 13/25 | Loss: 0.05980466
Iteration 14/25 | Loss: 0.05980466
Iteration 15/25 | Loss: 0.05980466
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.059804659336805344, 0.059804659336805344, 0.059804659336805344, 0.059804659336805344, 0.059804659336805344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.059804659336805344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05980466
Iteration 2/1000 | Loss: 0.00264299
Iteration 3/1000 | Loss: 0.00081246
Iteration 4/1000 | Loss: 0.00093756
Iteration 5/1000 | Loss: 0.00024953
Iteration 6/1000 | Loss: 0.00055526
Iteration 7/1000 | Loss: 0.00463214
Iteration 8/1000 | Loss: 0.00054125
Iteration 9/1000 | Loss: 0.00009385
Iteration 10/1000 | Loss: 0.00034929
Iteration 11/1000 | Loss: 0.00005849
Iteration 12/1000 | Loss: 0.00006168
Iteration 13/1000 | Loss: 0.00012380
Iteration 14/1000 | Loss: 0.00004656
Iteration 15/1000 | Loss: 0.00040524
Iteration 16/1000 | Loss: 0.00003684
Iteration 17/1000 | Loss: 0.00008351
Iteration 18/1000 | Loss: 0.00008657
Iteration 19/1000 | Loss: 0.00010764
Iteration 20/1000 | Loss: 0.00002899
Iteration 21/1000 | Loss: 0.00005752
Iteration 22/1000 | Loss: 0.00007325
Iteration 23/1000 | Loss: 0.00050449
Iteration 24/1000 | Loss: 0.00003869
Iteration 25/1000 | Loss: 0.00002759
Iteration 26/1000 | Loss: 0.00003911
Iteration 27/1000 | Loss: 0.00004936
Iteration 28/1000 | Loss: 0.00002347
Iteration 29/1000 | Loss: 0.00002215
Iteration 30/1000 | Loss: 0.00002527
Iteration 31/1000 | Loss: 0.00015559
Iteration 32/1000 | Loss: 0.00002548
Iteration 33/1000 | Loss: 0.00002594
Iteration 34/1000 | Loss: 0.00002054
Iteration 35/1000 | Loss: 0.00004712
Iteration 36/1000 | Loss: 0.00002031
Iteration 37/1000 | Loss: 0.00004236
Iteration 38/1000 | Loss: 0.00002232
Iteration 39/1000 | Loss: 0.00009065
Iteration 40/1000 | Loss: 0.00030025
Iteration 41/1000 | Loss: 0.00002202
Iteration 42/1000 | Loss: 0.00003149
Iteration 43/1000 | Loss: 0.00001928
Iteration 44/1000 | Loss: 0.00003236
Iteration 45/1000 | Loss: 0.00008429
Iteration 46/1000 | Loss: 0.00029632
Iteration 47/1000 | Loss: 0.00002554
Iteration 48/1000 | Loss: 0.00011532
Iteration 49/1000 | Loss: 0.00004174
Iteration 50/1000 | Loss: 0.00001901
Iteration 51/1000 | Loss: 0.00002397
Iteration 52/1000 | Loss: 0.00001896
Iteration 53/1000 | Loss: 0.00001896
Iteration 54/1000 | Loss: 0.00001896
Iteration 55/1000 | Loss: 0.00001896
Iteration 56/1000 | Loss: 0.00001896
Iteration 57/1000 | Loss: 0.00001896
Iteration 58/1000 | Loss: 0.00001896
Iteration 59/1000 | Loss: 0.00001896
Iteration 60/1000 | Loss: 0.00001896
Iteration 61/1000 | Loss: 0.00001896
Iteration 62/1000 | Loss: 0.00001896
Iteration 63/1000 | Loss: 0.00001896
Iteration 64/1000 | Loss: 0.00001895
Iteration 65/1000 | Loss: 0.00001895
Iteration 66/1000 | Loss: 0.00001895
Iteration 67/1000 | Loss: 0.00003284
Iteration 68/1000 | Loss: 0.00001891
Iteration 69/1000 | Loss: 0.00001890
Iteration 70/1000 | Loss: 0.00001890
Iteration 71/1000 | Loss: 0.00001890
Iteration 72/1000 | Loss: 0.00001889
Iteration 73/1000 | Loss: 0.00001886
Iteration 74/1000 | Loss: 0.00001886
Iteration 75/1000 | Loss: 0.00001886
Iteration 76/1000 | Loss: 0.00001886
Iteration 77/1000 | Loss: 0.00001886
Iteration 78/1000 | Loss: 0.00001886
Iteration 79/1000 | Loss: 0.00001886
Iteration 80/1000 | Loss: 0.00001886
Iteration 81/1000 | Loss: 0.00001886
Iteration 82/1000 | Loss: 0.00001884
Iteration 83/1000 | Loss: 0.00001883
Iteration 84/1000 | Loss: 0.00001883
Iteration 85/1000 | Loss: 0.00004000
Iteration 86/1000 | Loss: 0.00001884
Iteration 87/1000 | Loss: 0.00001881
Iteration 88/1000 | Loss: 0.00001879
Iteration 89/1000 | Loss: 0.00001878
Iteration 90/1000 | Loss: 0.00001878
Iteration 91/1000 | Loss: 0.00001878
Iteration 92/1000 | Loss: 0.00001878
Iteration 93/1000 | Loss: 0.00001878
Iteration 94/1000 | Loss: 0.00001878
Iteration 95/1000 | Loss: 0.00001878
Iteration 96/1000 | Loss: 0.00001877
Iteration 97/1000 | Loss: 0.00001877
Iteration 98/1000 | Loss: 0.00001877
Iteration 99/1000 | Loss: 0.00001876
Iteration 100/1000 | Loss: 0.00001876
Iteration 101/1000 | Loss: 0.00001875
Iteration 102/1000 | Loss: 0.00001874
Iteration 103/1000 | Loss: 0.00001874
Iteration 104/1000 | Loss: 0.00006049
Iteration 105/1000 | Loss: 0.00002434
Iteration 106/1000 | Loss: 0.00002854
Iteration 107/1000 | Loss: 0.00001872
Iteration 108/1000 | Loss: 0.00001871
Iteration 109/1000 | Loss: 0.00001871
Iteration 110/1000 | Loss: 0.00001871
Iteration 111/1000 | Loss: 0.00001871
Iteration 112/1000 | Loss: 0.00001871
Iteration 113/1000 | Loss: 0.00001871
Iteration 114/1000 | Loss: 0.00001871
Iteration 115/1000 | Loss: 0.00001871
Iteration 116/1000 | Loss: 0.00001870
Iteration 117/1000 | Loss: 0.00001870
Iteration 118/1000 | Loss: 0.00001870
Iteration 119/1000 | Loss: 0.00001870
Iteration 120/1000 | Loss: 0.00001870
Iteration 121/1000 | Loss: 0.00001870
Iteration 122/1000 | Loss: 0.00005187
Iteration 123/1000 | Loss: 0.00006816
Iteration 124/1000 | Loss: 0.00004677
Iteration 125/1000 | Loss: 0.00002227
Iteration 126/1000 | Loss: 0.00008361
Iteration 127/1000 | Loss: 0.00008361
Iteration 128/1000 | Loss: 0.00008361
Iteration 129/1000 | Loss: 0.00032890
Iteration 130/1000 | Loss: 0.00030943
Iteration 131/1000 | Loss: 0.00083057
Iteration 132/1000 | Loss: 0.00008403
Iteration 133/1000 | Loss: 0.00003582
Iteration 134/1000 | Loss: 0.00002998
Iteration 135/1000 | Loss: 0.00001965
Iteration 136/1000 | Loss: 0.00002448
Iteration 137/1000 | Loss: 0.00002064
Iteration 138/1000 | Loss: 0.00002466
Iteration 139/1000 | Loss: 0.00002502
Iteration 140/1000 | Loss: 0.00001898
Iteration 141/1000 | Loss: 0.00005105
Iteration 142/1000 | Loss: 0.00002735
Iteration 143/1000 | Loss: 0.00001981
Iteration 144/1000 | Loss: 0.00001869
Iteration 145/1000 | Loss: 0.00001867
Iteration 146/1000 | Loss: 0.00001864
Iteration 147/1000 | Loss: 0.00001863
Iteration 148/1000 | Loss: 0.00001861
Iteration 149/1000 | Loss: 0.00001861
Iteration 150/1000 | Loss: 0.00001861
Iteration 151/1000 | Loss: 0.00001861
Iteration 152/1000 | Loss: 0.00001861
Iteration 153/1000 | Loss: 0.00001861
Iteration 154/1000 | Loss: 0.00001861
Iteration 155/1000 | Loss: 0.00001861
Iteration 156/1000 | Loss: 0.00001861
Iteration 157/1000 | Loss: 0.00001861
Iteration 158/1000 | Loss: 0.00001861
Iteration 159/1000 | Loss: 0.00001861
Iteration 160/1000 | Loss: 0.00001861
Iteration 161/1000 | Loss: 0.00001861
Iteration 162/1000 | Loss: 0.00001861
Iteration 163/1000 | Loss: 0.00001861
Iteration 164/1000 | Loss: 0.00001861
Iteration 165/1000 | Loss: 0.00001861
Iteration 166/1000 | Loss: 0.00001861
Iteration 167/1000 | Loss: 0.00001861
Iteration 168/1000 | Loss: 0.00001861
Iteration 169/1000 | Loss: 0.00001861
Iteration 170/1000 | Loss: 0.00001861
Iteration 171/1000 | Loss: 0.00001861
Iteration 172/1000 | Loss: 0.00001861
Iteration 173/1000 | Loss: 0.00001861
Iteration 174/1000 | Loss: 0.00001861
Iteration 175/1000 | Loss: 0.00001861
Iteration 176/1000 | Loss: 0.00001861
Iteration 177/1000 | Loss: 0.00001861
Iteration 178/1000 | Loss: 0.00001861
Iteration 179/1000 | Loss: 0.00001861
Iteration 180/1000 | Loss: 0.00001861
Iteration 181/1000 | Loss: 0.00001861
Iteration 182/1000 | Loss: 0.00001861
Iteration 183/1000 | Loss: 0.00001861
Iteration 184/1000 | Loss: 0.00001861
Iteration 185/1000 | Loss: 0.00001861
Iteration 186/1000 | Loss: 0.00001861
Iteration 187/1000 | Loss: 0.00001861
Iteration 188/1000 | Loss: 0.00001861
Iteration 189/1000 | Loss: 0.00001861
Iteration 190/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.8605323930387385e-05, 1.8605323930387385e-05, 1.8605323930387385e-05, 1.8605323930387385e-05, 1.8605323930387385e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8605323930387385e-05

Optimization complete. Final v2v error: 3.702853202819824 mm

Highest mean error: 4.603148937225342 mm for frame 37

Lowest mean error: 3.3487284183502197 mm for frame 167

Saving results

Total time: 137.312726020813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820866
Iteration 2/25 | Loss: 0.00120116
Iteration 3/25 | Loss: 0.00113534
Iteration 4/25 | Loss: 0.00112494
Iteration 5/25 | Loss: 0.00112152
Iteration 6/25 | Loss: 0.00112115
Iteration 7/25 | Loss: 0.00112115
Iteration 8/25 | Loss: 0.00112115
Iteration 9/25 | Loss: 0.00112115
Iteration 10/25 | Loss: 0.00112115
Iteration 11/25 | Loss: 0.00112115
Iteration 12/25 | Loss: 0.00112115
Iteration 13/25 | Loss: 0.00112115
Iteration 14/25 | Loss: 0.00112115
Iteration 15/25 | Loss: 0.00112115
Iteration 16/25 | Loss: 0.00112115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011211455566808581, 0.0011211455566808581, 0.0011211455566808581, 0.0011211455566808581, 0.0011211455566808581]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011211455566808581

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.23891640
Iteration 2/25 | Loss: 0.00083860
Iteration 3/25 | Loss: 0.00083859
Iteration 4/25 | Loss: 0.00083859
Iteration 5/25 | Loss: 0.00083859
Iteration 6/25 | Loss: 0.00083859
Iteration 7/25 | Loss: 0.00083859
Iteration 8/25 | Loss: 0.00083859
Iteration 9/25 | Loss: 0.00083859
Iteration 10/25 | Loss: 0.00083859
Iteration 11/25 | Loss: 0.00083859
Iteration 12/25 | Loss: 0.00083859
Iteration 13/25 | Loss: 0.00083859
Iteration 14/25 | Loss: 0.00083859
Iteration 15/25 | Loss: 0.00083859
Iteration 16/25 | Loss: 0.00083859
Iteration 17/25 | Loss: 0.00083859
Iteration 18/25 | Loss: 0.00083859
Iteration 19/25 | Loss: 0.00083859
Iteration 20/25 | Loss: 0.00083859
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008385870023630559, 0.0008385870023630559, 0.0008385870023630559, 0.0008385870023630559, 0.0008385870023630559]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008385870023630559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083859
Iteration 2/1000 | Loss: 0.00002216
Iteration 3/1000 | Loss: 0.00001720
Iteration 4/1000 | Loss: 0.00001499
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001382
Iteration 7/1000 | Loss: 0.00001342
Iteration 8/1000 | Loss: 0.00001309
Iteration 9/1000 | Loss: 0.00001288
Iteration 10/1000 | Loss: 0.00001259
Iteration 11/1000 | Loss: 0.00001243
Iteration 12/1000 | Loss: 0.00001231
Iteration 13/1000 | Loss: 0.00001228
Iteration 14/1000 | Loss: 0.00001216
Iteration 15/1000 | Loss: 0.00001214
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001203
Iteration 18/1000 | Loss: 0.00001203
Iteration 19/1000 | Loss: 0.00001202
Iteration 20/1000 | Loss: 0.00001201
Iteration 21/1000 | Loss: 0.00001200
Iteration 22/1000 | Loss: 0.00001200
Iteration 23/1000 | Loss: 0.00001200
Iteration 24/1000 | Loss: 0.00001199
Iteration 25/1000 | Loss: 0.00001199
Iteration 26/1000 | Loss: 0.00001198
Iteration 27/1000 | Loss: 0.00001197
Iteration 28/1000 | Loss: 0.00001197
Iteration 29/1000 | Loss: 0.00001195
Iteration 30/1000 | Loss: 0.00001195
Iteration 31/1000 | Loss: 0.00001194
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001193
Iteration 34/1000 | Loss: 0.00001193
Iteration 35/1000 | Loss: 0.00001192
Iteration 36/1000 | Loss: 0.00001192
Iteration 37/1000 | Loss: 0.00001192
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001188
Iteration 41/1000 | Loss: 0.00001188
Iteration 42/1000 | Loss: 0.00001187
Iteration 43/1000 | Loss: 0.00001187
Iteration 44/1000 | Loss: 0.00001186
Iteration 45/1000 | Loss: 0.00001186
Iteration 46/1000 | Loss: 0.00001185
Iteration 47/1000 | Loss: 0.00001181
Iteration 48/1000 | Loss: 0.00001180
Iteration 49/1000 | Loss: 0.00001178
Iteration 50/1000 | Loss: 0.00001178
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001176
Iteration 53/1000 | Loss: 0.00001175
Iteration 54/1000 | Loss: 0.00001175
Iteration 55/1000 | Loss: 0.00001175
Iteration 56/1000 | Loss: 0.00001175
Iteration 57/1000 | Loss: 0.00001174
Iteration 58/1000 | Loss: 0.00001174
Iteration 59/1000 | Loss: 0.00001174
Iteration 60/1000 | Loss: 0.00001173
Iteration 61/1000 | Loss: 0.00001173
Iteration 62/1000 | Loss: 0.00001171
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001170
Iteration 65/1000 | Loss: 0.00001170
Iteration 66/1000 | Loss: 0.00001170
Iteration 67/1000 | Loss: 0.00001170
Iteration 68/1000 | Loss: 0.00001170
Iteration 69/1000 | Loss: 0.00001170
Iteration 70/1000 | Loss: 0.00001170
Iteration 71/1000 | Loss: 0.00001169
Iteration 72/1000 | Loss: 0.00001169
Iteration 73/1000 | Loss: 0.00001169
Iteration 74/1000 | Loss: 0.00001169
Iteration 75/1000 | Loss: 0.00001169
Iteration 76/1000 | Loss: 0.00001169
Iteration 77/1000 | Loss: 0.00001169
Iteration 78/1000 | Loss: 0.00001169
Iteration 79/1000 | Loss: 0.00001169
Iteration 80/1000 | Loss: 0.00001169
Iteration 81/1000 | Loss: 0.00001169
Iteration 82/1000 | Loss: 0.00001168
Iteration 83/1000 | Loss: 0.00001168
Iteration 84/1000 | Loss: 0.00001168
Iteration 85/1000 | Loss: 0.00001168
Iteration 86/1000 | Loss: 0.00001168
Iteration 87/1000 | Loss: 0.00001168
Iteration 88/1000 | Loss: 0.00001168
Iteration 89/1000 | Loss: 0.00001168
Iteration 90/1000 | Loss: 0.00001167
Iteration 91/1000 | Loss: 0.00001167
Iteration 92/1000 | Loss: 0.00001167
Iteration 93/1000 | Loss: 0.00001167
Iteration 94/1000 | Loss: 0.00001167
Iteration 95/1000 | Loss: 0.00001167
Iteration 96/1000 | Loss: 0.00001167
Iteration 97/1000 | Loss: 0.00001166
Iteration 98/1000 | Loss: 0.00001166
Iteration 99/1000 | Loss: 0.00001166
Iteration 100/1000 | Loss: 0.00001165
Iteration 101/1000 | Loss: 0.00001165
Iteration 102/1000 | Loss: 0.00001165
Iteration 103/1000 | Loss: 0.00001165
Iteration 104/1000 | Loss: 0.00001165
Iteration 105/1000 | Loss: 0.00001164
Iteration 106/1000 | Loss: 0.00001164
Iteration 107/1000 | Loss: 0.00001164
Iteration 108/1000 | Loss: 0.00001163
Iteration 109/1000 | Loss: 0.00001163
Iteration 110/1000 | Loss: 0.00001163
Iteration 111/1000 | Loss: 0.00001162
Iteration 112/1000 | Loss: 0.00001162
Iteration 113/1000 | Loss: 0.00001162
Iteration 114/1000 | Loss: 0.00001162
Iteration 115/1000 | Loss: 0.00001162
Iteration 116/1000 | Loss: 0.00001161
Iteration 117/1000 | Loss: 0.00001161
Iteration 118/1000 | Loss: 0.00001161
Iteration 119/1000 | Loss: 0.00001161
Iteration 120/1000 | Loss: 0.00001160
Iteration 121/1000 | Loss: 0.00001160
Iteration 122/1000 | Loss: 0.00001160
Iteration 123/1000 | Loss: 0.00001160
Iteration 124/1000 | Loss: 0.00001160
Iteration 125/1000 | Loss: 0.00001160
Iteration 126/1000 | Loss: 0.00001160
Iteration 127/1000 | Loss: 0.00001160
Iteration 128/1000 | Loss: 0.00001160
Iteration 129/1000 | Loss: 0.00001159
Iteration 130/1000 | Loss: 0.00001159
Iteration 131/1000 | Loss: 0.00001159
Iteration 132/1000 | Loss: 0.00001159
Iteration 133/1000 | Loss: 0.00001159
Iteration 134/1000 | Loss: 0.00001159
Iteration 135/1000 | Loss: 0.00001159
Iteration 136/1000 | Loss: 0.00001159
Iteration 137/1000 | Loss: 0.00001159
Iteration 138/1000 | Loss: 0.00001159
Iteration 139/1000 | Loss: 0.00001158
Iteration 140/1000 | Loss: 0.00001158
Iteration 141/1000 | Loss: 0.00001158
Iteration 142/1000 | Loss: 0.00001158
Iteration 143/1000 | Loss: 0.00001158
Iteration 144/1000 | Loss: 0.00001158
Iteration 145/1000 | Loss: 0.00001158
Iteration 146/1000 | Loss: 0.00001158
Iteration 147/1000 | Loss: 0.00001158
Iteration 148/1000 | Loss: 0.00001158
Iteration 149/1000 | Loss: 0.00001158
Iteration 150/1000 | Loss: 0.00001158
Iteration 151/1000 | Loss: 0.00001158
Iteration 152/1000 | Loss: 0.00001158
Iteration 153/1000 | Loss: 0.00001158
Iteration 154/1000 | Loss: 0.00001158
Iteration 155/1000 | Loss: 0.00001158
Iteration 156/1000 | Loss: 0.00001158
Iteration 157/1000 | Loss: 0.00001158
Iteration 158/1000 | Loss: 0.00001157
Iteration 159/1000 | Loss: 0.00001157
Iteration 160/1000 | Loss: 0.00001157
Iteration 161/1000 | Loss: 0.00001157
Iteration 162/1000 | Loss: 0.00001157
Iteration 163/1000 | Loss: 0.00001157
Iteration 164/1000 | Loss: 0.00001157
Iteration 165/1000 | Loss: 0.00001157
Iteration 166/1000 | Loss: 0.00001157
Iteration 167/1000 | Loss: 0.00001157
Iteration 168/1000 | Loss: 0.00001157
Iteration 169/1000 | Loss: 0.00001157
Iteration 170/1000 | Loss: 0.00001157
Iteration 171/1000 | Loss: 0.00001157
Iteration 172/1000 | Loss: 0.00001157
Iteration 173/1000 | Loss: 0.00001157
Iteration 174/1000 | Loss: 0.00001157
Iteration 175/1000 | Loss: 0.00001157
Iteration 176/1000 | Loss: 0.00001157
Iteration 177/1000 | Loss: 0.00001157
Iteration 178/1000 | Loss: 0.00001157
Iteration 179/1000 | Loss: 0.00001156
Iteration 180/1000 | Loss: 0.00001156
Iteration 181/1000 | Loss: 0.00001156
Iteration 182/1000 | Loss: 0.00001156
Iteration 183/1000 | Loss: 0.00001156
Iteration 184/1000 | Loss: 0.00001156
Iteration 185/1000 | Loss: 0.00001156
Iteration 186/1000 | Loss: 0.00001156
Iteration 187/1000 | Loss: 0.00001156
Iteration 188/1000 | Loss: 0.00001156
Iteration 189/1000 | Loss: 0.00001156
Iteration 190/1000 | Loss: 0.00001156
Iteration 191/1000 | Loss: 0.00001156
Iteration 192/1000 | Loss: 0.00001156
Iteration 193/1000 | Loss: 0.00001156
Iteration 194/1000 | Loss: 0.00001156
Iteration 195/1000 | Loss: 0.00001156
Iteration 196/1000 | Loss: 0.00001156
Iteration 197/1000 | Loss: 0.00001156
Iteration 198/1000 | Loss: 0.00001155
Iteration 199/1000 | Loss: 0.00001155
Iteration 200/1000 | Loss: 0.00001155
Iteration 201/1000 | Loss: 0.00001155
Iteration 202/1000 | Loss: 0.00001155
Iteration 203/1000 | Loss: 0.00001155
Iteration 204/1000 | Loss: 0.00001155
Iteration 205/1000 | Loss: 0.00001155
Iteration 206/1000 | Loss: 0.00001155
Iteration 207/1000 | Loss: 0.00001155
Iteration 208/1000 | Loss: 0.00001155
Iteration 209/1000 | Loss: 0.00001155
Iteration 210/1000 | Loss: 0.00001155
Iteration 211/1000 | Loss: 0.00001155
Iteration 212/1000 | Loss: 0.00001155
Iteration 213/1000 | Loss: 0.00001155
Iteration 214/1000 | Loss: 0.00001155
Iteration 215/1000 | Loss: 0.00001155
Iteration 216/1000 | Loss: 0.00001155
Iteration 217/1000 | Loss: 0.00001155
Iteration 218/1000 | Loss: 0.00001155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.154769051936455e-05, 1.154769051936455e-05, 1.154769051936455e-05, 1.154769051936455e-05, 1.154769051936455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.154769051936455e-05

Optimization complete. Final v2v error: 2.908418893814087 mm

Highest mean error: 3.221419334411621 mm for frame 113

Lowest mean error: 2.59298038482666 mm for frame 138

Saving results

Total time: 41.65385341644287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813106
Iteration 2/25 | Loss: 0.00232276
Iteration 3/25 | Loss: 0.00183655
Iteration 4/25 | Loss: 0.00166752
Iteration 5/25 | Loss: 0.00139054
Iteration 6/25 | Loss: 0.00133084
Iteration 7/25 | Loss: 0.00132951
Iteration 8/25 | Loss: 0.00132892
Iteration 9/25 | Loss: 0.00132854
Iteration 10/25 | Loss: 0.00132831
Iteration 11/25 | Loss: 0.00132888
Iteration 12/25 | Loss: 0.00132880
Iteration 13/25 | Loss: 0.00132860
Iteration 14/25 | Loss: 0.00132864
Iteration 15/25 | Loss: 0.00132852
Iteration 16/25 | Loss: 0.00132772
Iteration 17/25 | Loss: 0.00132853
Iteration 18/25 | Loss: 0.00132934
Iteration 19/25 | Loss: 0.00132835
Iteration 20/25 | Loss: 0.00132695
Iteration 21/25 | Loss: 0.00132767
Iteration 22/25 | Loss: 0.00132665
Iteration 23/25 | Loss: 0.00132667
Iteration 24/25 | Loss: 0.00132686
Iteration 25/25 | Loss: 0.00132663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35511351
Iteration 2/25 | Loss: 0.00067722
Iteration 3/25 | Loss: 0.00067721
Iteration 4/25 | Loss: 0.00067721
Iteration 5/25 | Loss: 0.00067721
Iteration 6/25 | Loss: 0.00067721
Iteration 7/25 | Loss: 0.00067721
Iteration 8/25 | Loss: 0.00067721
Iteration 9/25 | Loss: 0.00067721
Iteration 10/25 | Loss: 0.00067721
Iteration 11/25 | Loss: 0.00067721
Iteration 12/25 | Loss: 0.00067721
Iteration 13/25 | Loss: 0.00067721
Iteration 14/25 | Loss: 0.00067721
Iteration 15/25 | Loss: 0.00067721
Iteration 16/25 | Loss: 0.00067721
Iteration 17/25 | Loss: 0.00067721
Iteration 18/25 | Loss: 0.00067721
Iteration 19/25 | Loss: 0.00067721
Iteration 20/25 | Loss: 0.00067721
Iteration 21/25 | Loss: 0.00067721
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006772070191800594, 0.0006772070191800594, 0.0006772070191800594, 0.0006772070191800594, 0.0006772070191800594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006772070191800594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067721
Iteration 2/1000 | Loss: 0.00003651
Iteration 3/1000 | Loss: 0.00002406
Iteration 4/1000 | Loss: 0.00002194
Iteration 5/1000 | Loss: 0.00002092
Iteration 6/1000 | Loss: 0.00002033
Iteration 7/1000 | Loss: 0.00002006
Iteration 8/1000 | Loss: 0.00001979
Iteration 9/1000 | Loss: 0.00001966
Iteration 10/1000 | Loss: 0.00001964
Iteration 11/1000 | Loss: 0.00001964
Iteration 12/1000 | Loss: 0.00001963
Iteration 13/1000 | Loss: 0.00001954
Iteration 14/1000 | Loss: 0.00001953
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001944
Iteration 17/1000 | Loss: 0.00001939
Iteration 18/1000 | Loss: 0.00001938
Iteration 19/1000 | Loss: 0.00001938
Iteration 20/1000 | Loss: 0.00001937
Iteration 21/1000 | Loss: 0.00001937
Iteration 22/1000 | Loss: 0.00001937
Iteration 23/1000 | Loss: 0.00001936
Iteration 24/1000 | Loss: 0.00001936
Iteration 25/1000 | Loss: 0.00001936
Iteration 26/1000 | Loss: 0.00001936
Iteration 27/1000 | Loss: 0.00001936
Iteration 28/1000 | Loss: 0.00001936
Iteration 29/1000 | Loss: 0.00001936
Iteration 30/1000 | Loss: 0.00001935
Iteration 31/1000 | Loss: 0.00001935
Iteration 32/1000 | Loss: 0.00001934
Iteration 33/1000 | Loss: 0.00001934
Iteration 34/1000 | Loss: 0.00001933
Iteration 35/1000 | Loss: 0.00001932
Iteration 36/1000 | Loss: 0.00001931
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001930
Iteration 40/1000 | Loss: 0.00001928
Iteration 41/1000 | Loss: 0.00001928
Iteration 42/1000 | Loss: 0.00001927
Iteration 43/1000 | Loss: 0.00001927
Iteration 44/1000 | Loss: 0.00001927
Iteration 45/1000 | Loss: 0.00001927
Iteration 46/1000 | Loss: 0.00002198
Iteration 47/1000 | Loss: 0.00002198
Iteration 48/1000 | Loss: 0.00002020
Iteration 49/1000 | Loss: 0.00003191
Iteration 50/1000 | Loss: 0.00003287
Iteration 51/1000 | Loss: 0.00002348
Iteration 52/1000 | Loss: 0.00002732
Iteration 53/1000 | Loss: 0.00002503
Iteration 54/1000 | Loss: 0.00003366
Iteration 55/1000 | Loss: 0.00003245
Iteration 56/1000 | Loss: 0.00002191
Iteration 57/1000 | Loss: 0.00002826
Iteration 58/1000 | Loss: 0.00002326
Iteration 59/1000 | Loss: 0.00002292
Iteration 60/1000 | Loss: 0.00002860
Iteration 61/1000 | Loss: 0.00003660
Iteration 62/1000 | Loss: 0.00002799
Iteration 63/1000 | Loss: 0.00003934
Iteration 64/1000 | Loss: 0.00002526
Iteration 65/1000 | Loss: 0.00003328
Iteration 66/1000 | Loss: 0.00003121
Iteration 67/1000 | Loss: 0.00003532
Iteration 68/1000 | Loss: 0.00003743
Iteration 69/1000 | Loss: 0.00003629
Iteration 70/1000 | Loss: 0.00002615
Iteration 71/1000 | Loss: 0.00002749
Iteration 72/1000 | Loss: 0.00003542
Iteration 73/1000 | Loss: 0.00002227
Iteration 74/1000 | Loss: 0.00003335
Iteration 75/1000 | Loss: 0.00003782
Iteration 76/1000 | Loss: 0.00003369
Iteration 77/1000 | Loss: 0.00003273
Iteration 78/1000 | Loss: 0.00003771
Iteration 79/1000 | Loss: 0.00003298
Iteration 80/1000 | Loss: 0.00003702
Iteration 81/1000 | Loss: 0.00003291
Iteration 82/1000 | Loss: 0.00002177
Iteration 83/1000 | Loss: 0.00002065
Iteration 84/1000 | Loss: 0.00002005
Iteration 85/1000 | Loss: 0.00001947
Iteration 86/1000 | Loss: 0.00001924
Iteration 87/1000 | Loss: 0.00001905
Iteration 88/1000 | Loss: 0.00001901
Iteration 89/1000 | Loss: 0.00001900
Iteration 90/1000 | Loss: 0.00001897
Iteration 91/1000 | Loss: 0.00001892
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001890
Iteration 95/1000 | Loss: 0.00001890
Iteration 96/1000 | Loss: 0.00001888
Iteration 97/1000 | Loss: 0.00001885
Iteration 98/1000 | Loss: 0.00001885
Iteration 99/1000 | Loss: 0.00001883
Iteration 100/1000 | Loss: 0.00001883
Iteration 101/1000 | Loss: 0.00001883
Iteration 102/1000 | Loss: 0.00001883
Iteration 103/1000 | Loss: 0.00001883
Iteration 104/1000 | Loss: 0.00001883
Iteration 105/1000 | Loss: 0.00001883
Iteration 106/1000 | Loss: 0.00001883
Iteration 107/1000 | Loss: 0.00001883
Iteration 108/1000 | Loss: 0.00001883
Iteration 109/1000 | Loss: 0.00001882
Iteration 110/1000 | Loss: 0.00001882
Iteration 111/1000 | Loss: 0.00001882
Iteration 112/1000 | Loss: 0.00001882
Iteration 113/1000 | Loss: 0.00001882
Iteration 114/1000 | Loss: 0.00001882
Iteration 115/1000 | Loss: 0.00001882
Iteration 116/1000 | Loss: 0.00001882
Iteration 117/1000 | Loss: 0.00001882
Iteration 118/1000 | Loss: 0.00001882
Iteration 119/1000 | Loss: 0.00001882
Iteration 120/1000 | Loss: 0.00001882
Iteration 121/1000 | Loss: 0.00001882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.8818984244717285e-05, 1.8818984244717285e-05, 1.8818984244717285e-05, 1.8818984244717285e-05, 1.8818984244717285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8818984244717285e-05

Optimization complete. Final v2v error: 3.6881179809570312 mm

Highest mean error: 5.539494037628174 mm for frame 88

Lowest mean error: 3.502596616744995 mm for frame 53

Saving results

Total time: 133.8646912574768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018218
Iteration 2/25 | Loss: 0.01018218
Iteration 3/25 | Loss: 0.01018217
Iteration 4/25 | Loss: 0.00171888
Iteration 5/25 | Loss: 0.00128288
Iteration 6/25 | Loss: 0.00120484
Iteration 7/25 | Loss: 0.00119753
Iteration 8/25 | Loss: 0.00119544
Iteration 9/25 | Loss: 0.00119492
Iteration 10/25 | Loss: 0.00119488
Iteration 11/25 | Loss: 0.00119488
Iteration 12/25 | Loss: 0.00119488
Iteration 13/25 | Loss: 0.00119488
Iteration 14/25 | Loss: 0.00119488
Iteration 15/25 | Loss: 0.00119488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011948796454817057, 0.0011948796454817057, 0.0011948796454817057, 0.0011948796454817057, 0.0011948796454817057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011948796454817057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46610439
Iteration 2/25 | Loss: 0.00111832
Iteration 3/25 | Loss: 0.00111832
Iteration 4/25 | Loss: 0.00111832
Iteration 5/25 | Loss: 0.00111831
Iteration 6/25 | Loss: 0.00111831
Iteration 7/25 | Loss: 0.00111831
Iteration 8/25 | Loss: 0.00111831
Iteration 9/25 | Loss: 0.00111831
Iteration 10/25 | Loss: 0.00111831
Iteration 11/25 | Loss: 0.00111831
Iteration 12/25 | Loss: 0.00111831
Iteration 13/25 | Loss: 0.00111831
Iteration 14/25 | Loss: 0.00111831
Iteration 15/25 | Loss: 0.00111831
Iteration 16/25 | Loss: 0.00111831
Iteration 17/25 | Loss: 0.00111831
Iteration 18/25 | Loss: 0.00111831
Iteration 19/25 | Loss: 0.00111831
Iteration 20/25 | Loss: 0.00111831
Iteration 21/25 | Loss: 0.00111831
Iteration 22/25 | Loss: 0.00111831
Iteration 23/25 | Loss: 0.00111831
Iteration 24/25 | Loss: 0.00111831
Iteration 25/25 | Loss: 0.00111831

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111831
Iteration 2/1000 | Loss: 0.00004050
Iteration 3/1000 | Loss: 0.00002668
Iteration 4/1000 | Loss: 0.00002238
Iteration 5/1000 | Loss: 0.00002012
Iteration 6/1000 | Loss: 0.00001897
Iteration 7/1000 | Loss: 0.00001827
Iteration 8/1000 | Loss: 0.00001779
Iteration 9/1000 | Loss: 0.00001747
Iteration 10/1000 | Loss: 0.00001711
Iteration 11/1000 | Loss: 0.00001683
Iteration 12/1000 | Loss: 0.00001663
Iteration 13/1000 | Loss: 0.00001649
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001640
Iteration 16/1000 | Loss: 0.00001632
Iteration 17/1000 | Loss: 0.00001632
Iteration 18/1000 | Loss: 0.00001631
Iteration 19/1000 | Loss: 0.00001631
Iteration 20/1000 | Loss: 0.00001630
Iteration 21/1000 | Loss: 0.00001629
Iteration 22/1000 | Loss: 0.00001628
Iteration 23/1000 | Loss: 0.00001628
Iteration 24/1000 | Loss: 0.00001627
Iteration 25/1000 | Loss: 0.00001627
Iteration 26/1000 | Loss: 0.00001627
Iteration 27/1000 | Loss: 0.00001627
Iteration 28/1000 | Loss: 0.00001626
Iteration 29/1000 | Loss: 0.00001626
Iteration 30/1000 | Loss: 0.00001626
Iteration 31/1000 | Loss: 0.00001625
Iteration 32/1000 | Loss: 0.00001625
Iteration 33/1000 | Loss: 0.00001624
Iteration 34/1000 | Loss: 0.00001624
Iteration 35/1000 | Loss: 0.00001624
Iteration 36/1000 | Loss: 0.00001623
Iteration 37/1000 | Loss: 0.00001621
Iteration 38/1000 | Loss: 0.00001621
Iteration 39/1000 | Loss: 0.00001620
Iteration 40/1000 | Loss: 0.00001620
Iteration 41/1000 | Loss: 0.00001619
Iteration 42/1000 | Loss: 0.00001618
Iteration 43/1000 | Loss: 0.00001617
Iteration 44/1000 | Loss: 0.00001617
Iteration 45/1000 | Loss: 0.00001616
Iteration 46/1000 | Loss: 0.00001616
Iteration 47/1000 | Loss: 0.00001616
Iteration 48/1000 | Loss: 0.00001615
Iteration 49/1000 | Loss: 0.00001614
Iteration 50/1000 | Loss: 0.00001614
Iteration 51/1000 | Loss: 0.00001614
Iteration 52/1000 | Loss: 0.00001614
Iteration 53/1000 | Loss: 0.00001614
Iteration 54/1000 | Loss: 0.00001614
Iteration 55/1000 | Loss: 0.00001614
Iteration 56/1000 | Loss: 0.00001614
Iteration 57/1000 | Loss: 0.00001614
Iteration 58/1000 | Loss: 0.00001613
Iteration 59/1000 | Loss: 0.00001613
Iteration 60/1000 | Loss: 0.00001612
Iteration 61/1000 | Loss: 0.00001612
Iteration 62/1000 | Loss: 0.00001612
Iteration 63/1000 | Loss: 0.00001612
Iteration 64/1000 | Loss: 0.00001612
Iteration 65/1000 | Loss: 0.00001611
Iteration 66/1000 | Loss: 0.00001611
Iteration 67/1000 | Loss: 0.00001611
Iteration 68/1000 | Loss: 0.00001611
Iteration 69/1000 | Loss: 0.00001611
Iteration 70/1000 | Loss: 0.00001611
Iteration 71/1000 | Loss: 0.00001611
Iteration 72/1000 | Loss: 0.00001611
Iteration 73/1000 | Loss: 0.00001611
Iteration 74/1000 | Loss: 0.00001611
Iteration 75/1000 | Loss: 0.00001611
Iteration 76/1000 | Loss: 0.00001611
Iteration 77/1000 | Loss: 0.00001611
Iteration 78/1000 | Loss: 0.00001610
Iteration 79/1000 | Loss: 0.00001610
Iteration 80/1000 | Loss: 0.00001610
Iteration 81/1000 | Loss: 0.00001609
Iteration 82/1000 | Loss: 0.00001609
Iteration 83/1000 | Loss: 0.00001609
Iteration 84/1000 | Loss: 0.00001609
Iteration 85/1000 | Loss: 0.00001608
Iteration 86/1000 | Loss: 0.00001608
Iteration 87/1000 | Loss: 0.00001608
Iteration 88/1000 | Loss: 0.00001608
Iteration 89/1000 | Loss: 0.00001608
Iteration 90/1000 | Loss: 0.00001608
Iteration 91/1000 | Loss: 0.00001607
Iteration 92/1000 | Loss: 0.00001607
Iteration 93/1000 | Loss: 0.00001607
Iteration 94/1000 | Loss: 0.00001607
Iteration 95/1000 | Loss: 0.00001607
Iteration 96/1000 | Loss: 0.00001606
Iteration 97/1000 | Loss: 0.00001606
Iteration 98/1000 | Loss: 0.00001606
Iteration 99/1000 | Loss: 0.00001606
Iteration 100/1000 | Loss: 0.00001606
Iteration 101/1000 | Loss: 0.00001606
Iteration 102/1000 | Loss: 0.00001606
Iteration 103/1000 | Loss: 0.00001606
Iteration 104/1000 | Loss: 0.00001606
Iteration 105/1000 | Loss: 0.00001606
Iteration 106/1000 | Loss: 0.00001605
Iteration 107/1000 | Loss: 0.00001605
Iteration 108/1000 | Loss: 0.00001605
Iteration 109/1000 | Loss: 0.00001605
Iteration 110/1000 | Loss: 0.00001605
Iteration 111/1000 | Loss: 0.00001605
Iteration 112/1000 | Loss: 0.00001605
Iteration 113/1000 | Loss: 0.00001605
Iteration 114/1000 | Loss: 0.00001605
Iteration 115/1000 | Loss: 0.00001605
Iteration 116/1000 | Loss: 0.00001605
Iteration 117/1000 | Loss: 0.00001604
Iteration 118/1000 | Loss: 0.00001604
Iteration 119/1000 | Loss: 0.00001604
Iteration 120/1000 | Loss: 0.00001604
Iteration 121/1000 | Loss: 0.00001604
Iteration 122/1000 | Loss: 0.00001604
Iteration 123/1000 | Loss: 0.00001604
Iteration 124/1000 | Loss: 0.00001604
Iteration 125/1000 | Loss: 0.00001604
Iteration 126/1000 | Loss: 0.00001604
Iteration 127/1000 | Loss: 0.00001604
Iteration 128/1000 | Loss: 0.00001604
Iteration 129/1000 | Loss: 0.00001604
Iteration 130/1000 | Loss: 0.00001604
Iteration 131/1000 | Loss: 0.00001604
Iteration 132/1000 | Loss: 0.00001603
Iteration 133/1000 | Loss: 0.00001603
Iteration 134/1000 | Loss: 0.00001603
Iteration 135/1000 | Loss: 0.00001603
Iteration 136/1000 | Loss: 0.00001603
Iteration 137/1000 | Loss: 0.00001603
Iteration 138/1000 | Loss: 0.00001603
Iteration 139/1000 | Loss: 0.00001603
Iteration 140/1000 | Loss: 0.00001603
Iteration 141/1000 | Loss: 0.00001603
Iteration 142/1000 | Loss: 0.00001603
Iteration 143/1000 | Loss: 0.00001603
Iteration 144/1000 | Loss: 0.00001603
Iteration 145/1000 | Loss: 0.00001603
Iteration 146/1000 | Loss: 0.00001603
Iteration 147/1000 | Loss: 0.00001603
Iteration 148/1000 | Loss: 0.00001603
Iteration 149/1000 | Loss: 0.00001603
Iteration 150/1000 | Loss: 0.00001603
Iteration 151/1000 | Loss: 0.00001603
Iteration 152/1000 | Loss: 0.00001603
Iteration 153/1000 | Loss: 0.00001603
Iteration 154/1000 | Loss: 0.00001603
Iteration 155/1000 | Loss: 0.00001603
Iteration 156/1000 | Loss: 0.00001603
Iteration 157/1000 | Loss: 0.00001603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.6026480807340704e-05, 1.6026480807340704e-05, 1.6026480807340704e-05, 1.6026480807340704e-05, 1.6026480807340704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6026480807340704e-05

Optimization complete. Final v2v error: 3.489593029022217 mm

Highest mean error: 3.6814913749694824 mm for frame 140

Lowest mean error: 3.2638559341430664 mm for frame 90

Saving results

Total time: 45.55587148666382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426574
Iteration 2/25 | Loss: 0.00126763
Iteration 3/25 | Loss: 0.00115623
Iteration 4/25 | Loss: 0.00114417
Iteration 5/25 | Loss: 0.00114108
Iteration 6/25 | Loss: 0.00114024
Iteration 7/25 | Loss: 0.00114024
Iteration 8/25 | Loss: 0.00114024
Iteration 9/25 | Loss: 0.00114024
Iteration 10/25 | Loss: 0.00114024
Iteration 11/25 | Loss: 0.00114024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011402431409806013, 0.0011402431409806013, 0.0011402431409806013, 0.0011402431409806013, 0.0011402431409806013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011402431409806013

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.59718323
Iteration 2/25 | Loss: 0.00079884
Iteration 3/25 | Loss: 0.00079883
Iteration 4/25 | Loss: 0.00079883
Iteration 5/25 | Loss: 0.00079883
Iteration 6/25 | Loss: 0.00079882
Iteration 7/25 | Loss: 0.00079882
Iteration 8/25 | Loss: 0.00079882
Iteration 9/25 | Loss: 0.00079882
Iteration 10/25 | Loss: 0.00079882
Iteration 11/25 | Loss: 0.00079882
Iteration 12/25 | Loss: 0.00079882
Iteration 13/25 | Loss: 0.00079882
Iteration 14/25 | Loss: 0.00079882
Iteration 15/25 | Loss: 0.00079882
Iteration 16/25 | Loss: 0.00079882
Iteration 17/25 | Loss: 0.00079882
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007988239522092044, 0.0007988239522092044, 0.0007988239522092044, 0.0007988239522092044, 0.0007988239522092044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007988239522092044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079882
Iteration 2/1000 | Loss: 0.00002671
Iteration 3/1000 | Loss: 0.00001840
Iteration 4/1000 | Loss: 0.00001556
Iteration 5/1000 | Loss: 0.00001459
Iteration 6/1000 | Loss: 0.00001393
Iteration 7/1000 | Loss: 0.00001352
Iteration 8/1000 | Loss: 0.00001313
Iteration 9/1000 | Loss: 0.00001286
Iteration 10/1000 | Loss: 0.00001263
Iteration 11/1000 | Loss: 0.00001244
Iteration 12/1000 | Loss: 0.00001242
Iteration 13/1000 | Loss: 0.00001241
Iteration 14/1000 | Loss: 0.00001241
Iteration 15/1000 | Loss: 0.00001234
Iteration 16/1000 | Loss: 0.00001231
Iteration 17/1000 | Loss: 0.00001230
Iteration 18/1000 | Loss: 0.00001229
Iteration 19/1000 | Loss: 0.00001228
Iteration 20/1000 | Loss: 0.00001228
Iteration 21/1000 | Loss: 0.00001227
Iteration 22/1000 | Loss: 0.00001224
Iteration 23/1000 | Loss: 0.00001220
Iteration 24/1000 | Loss: 0.00001217
Iteration 25/1000 | Loss: 0.00001217
Iteration 26/1000 | Loss: 0.00001216
Iteration 27/1000 | Loss: 0.00001216
Iteration 28/1000 | Loss: 0.00001211
Iteration 29/1000 | Loss: 0.00001211
Iteration 30/1000 | Loss: 0.00001211
Iteration 31/1000 | Loss: 0.00001210
Iteration 32/1000 | Loss: 0.00001210
Iteration 33/1000 | Loss: 0.00001207
Iteration 34/1000 | Loss: 0.00001207
Iteration 35/1000 | Loss: 0.00001207
Iteration 36/1000 | Loss: 0.00001207
Iteration 37/1000 | Loss: 0.00001207
Iteration 38/1000 | Loss: 0.00001207
Iteration 39/1000 | Loss: 0.00001207
Iteration 40/1000 | Loss: 0.00001207
Iteration 41/1000 | Loss: 0.00001207
Iteration 42/1000 | Loss: 0.00001207
Iteration 43/1000 | Loss: 0.00001207
Iteration 44/1000 | Loss: 0.00001206
Iteration 45/1000 | Loss: 0.00001206
Iteration 46/1000 | Loss: 0.00001206
Iteration 47/1000 | Loss: 0.00001206
Iteration 48/1000 | Loss: 0.00001206
Iteration 49/1000 | Loss: 0.00001206
Iteration 50/1000 | Loss: 0.00001206
Iteration 51/1000 | Loss: 0.00001204
Iteration 52/1000 | Loss: 0.00001204
Iteration 53/1000 | Loss: 0.00001202
Iteration 54/1000 | Loss: 0.00001201
Iteration 55/1000 | Loss: 0.00001201
Iteration 56/1000 | Loss: 0.00001201
Iteration 57/1000 | Loss: 0.00001201
Iteration 58/1000 | Loss: 0.00001199
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001198
Iteration 61/1000 | Loss: 0.00001198
Iteration 62/1000 | Loss: 0.00001197
Iteration 63/1000 | Loss: 0.00001197
Iteration 64/1000 | Loss: 0.00001196
Iteration 65/1000 | Loss: 0.00001196
Iteration 66/1000 | Loss: 0.00001196
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001192
Iteration 77/1000 | Loss: 0.00001192
Iteration 78/1000 | Loss: 0.00001192
Iteration 79/1000 | Loss: 0.00001192
Iteration 80/1000 | Loss: 0.00001191
Iteration 81/1000 | Loss: 0.00001191
Iteration 82/1000 | Loss: 0.00001191
Iteration 83/1000 | Loss: 0.00001191
Iteration 84/1000 | Loss: 0.00001191
Iteration 85/1000 | Loss: 0.00001191
Iteration 86/1000 | Loss: 0.00001191
Iteration 87/1000 | Loss: 0.00001191
Iteration 88/1000 | Loss: 0.00001191
Iteration 89/1000 | Loss: 0.00001191
Iteration 90/1000 | Loss: 0.00001190
Iteration 91/1000 | Loss: 0.00001190
Iteration 92/1000 | Loss: 0.00001190
Iteration 93/1000 | Loss: 0.00001189
Iteration 94/1000 | Loss: 0.00001189
Iteration 95/1000 | Loss: 0.00001188
Iteration 96/1000 | Loss: 0.00001188
Iteration 97/1000 | Loss: 0.00001187
Iteration 98/1000 | Loss: 0.00001187
Iteration 99/1000 | Loss: 0.00001187
Iteration 100/1000 | Loss: 0.00001186
Iteration 101/1000 | Loss: 0.00001186
Iteration 102/1000 | Loss: 0.00001186
Iteration 103/1000 | Loss: 0.00001186
Iteration 104/1000 | Loss: 0.00001185
Iteration 105/1000 | Loss: 0.00001185
Iteration 106/1000 | Loss: 0.00001185
Iteration 107/1000 | Loss: 0.00001185
Iteration 108/1000 | Loss: 0.00001184
Iteration 109/1000 | Loss: 0.00001184
Iteration 110/1000 | Loss: 0.00001184
Iteration 111/1000 | Loss: 0.00001183
Iteration 112/1000 | Loss: 0.00001183
Iteration 113/1000 | Loss: 0.00001183
Iteration 114/1000 | Loss: 0.00001183
Iteration 115/1000 | Loss: 0.00001183
Iteration 116/1000 | Loss: 0.00001182
Iteration 117/1000 | Loss: 0.00001182
Iteration 118/1000 | Loss: 0.00001182
Iteration 119/1000 | Loss: 0.00001182
Iteration 120/1000 | Loss: 0.00001182
Iteration 121/1000 | Loss: 0.00001181
Iteration 122/1000 | Loss: 0.00001181
Iteration 123/1000 | Loss: 0.00001181
Iteration 124/1000 | Loss: 0.00001180
Iteration 125/1000 | Loss: 0.00001180
Iteration 126/1000 | Loss: 0.00001180
Iteration 127/1000 | Loss: 0.00001179
Iteration 128/1000 | Loss: 0.00001179
Iteration 129/1000 | Loss: 0.00001179
Iteration 130/1000 | Loss: 0.00001179
Iteration 131/1000 | Loss: 0.00001179
Iteration 132/1000 | Loss: 0.00001178
Iteration 133/1000 | Loss: 0.00001178
Iteration 134/1000 | Loss: 0.00001178
Iteration 135/1000 | Loss: 0.00001178
Iteration 136/1000 | Loss: 0.00001178
Iteration 137/1000 | Loss: 0.00001177
Iteration 138/1000 | Loss: 0.00001177
Iteration 139/1000 | Loss: 0.00001177
Iteration 140/1000 | Loss: 0.00001177
Iteration 141/1000 | Loss: 0.00001177
Iteration 142/1000 | Loss: 0.00001177
Iteration 143/1000 | Loss: 0.00001177
Iteration 144/1000 | Loss: 0.00001176
Iteration 145/1000 | Loss: 0.00001176
Iteration 146/1000 | Loss: 0.00001176
Iteration 147/1000 | Loss: 0.00001176
Iteration 148/1000 | Loss: 0.00001176
Iteration 149/1000 | Loss: 0.00001176
Iteration 150/1000 | Loss: 0.00001176
Iteration 151/1000 | Loss: 0.00001176
Iteration 152/1000 | Loss: 0.00001176
Iteration 153/1000 | Loss: 0.00001176
Iteration 154/1000 | Loss: 0.00001176
Iteration 155/1000 | Loss: 0.00001176
Iteration 156/1000 | Loss: 0.00001176
Iteration 157/1000 | Loss: 0.00001175
Iteration 158/1000 | Loss: 0.00001175
Iteration 159/1000 | Loss: 0.00001175
Iteration 160/1000 | Loss: 0.00001175
Iteration 161/1000 | Loss: 0.00001175
Iteration 162/1000 | Loss: 0.00001175
Iteration 163/1000 | Loss: 0.00001175
Iteration 164/1000 | Loss: 0.00001175
Iteration 165/1000 | Loss: 0.00001174
Iteration 166/1000 | Loss: 0.00001174
Iteration 167/1000 | Loss: 0.00001174
Iteration 168/1000 | Loss: 0.00001174
Iteration 169/1000 | Loss: 0.00001174
Iteration 170/1000 | Loss: 0.00001174
Iteration 171/1000 | Loss: 0.00001174
Iteration 172/1000 | Loss: 0.00001174
Iteration 173/1000 | Loss: 0.00001174
Iteration 174/1000 | Loss: 0.00001174
Iteration 175/1000 | Loss: 0.00001174
Iteration 176/1000 | Loss: 0.00001173
Iteration 177/1000 | Loss: 0.00001173
Iteration 178/1000 | Loss: 0.00001173
Iteration 179/1000 | Loss: 0.00001173
Iteration 180/1000 | Loss: 0.00001173
Iteration 181/1000 | Loss: 0.00001173
Iteration 182/1000 | Loss: 0.00001173
Iteration 183/1000 | Loss: 0.00001173
Iteration 184/1000 | Loss: 0.00001173
Iteration 185/1000 | Loss: 0.00001173
Iteration 186/1000 | Loss: 0.00001173
Iteration 187/1000 | Loss: 0.00001173
Iteration 188/1000 | Loss: 0.00001173
Iteration 189/1000 | Loss: 0.00001173
Iteration 190/1000 | Loss: 0.00001173
Iteration 191/1000 | Loss: 0.00001173
Iteration 192/1000 | Loss: 0.00001173
Iteration 193/1000 | Loss: 0.00001173
Iteration 194/1000 | Loss: 0.00001172
Iteration 195/1000 | Loss: 0.00001172
Iteration 196/1000 | Loss: 0.00001172
Iteration 197/1000 | Loss: 0.00001172
Iteration 198/1000 | Loss: 0.00001172
Iteration 199/1000 | Loss: 0.00001172
Iteration 200/1000 | Loss: 0.00001172
Iteration 201/1000 | Loss: 0.00001172
Iteration 202/1000 | Loss: 0.00001172
Iteration 203/1000 | Loss: 0.00001172
Iteration 204/1000 | Loss: 0.00001172
Iteration 205/1000 | Loss: 0.00001172
Iteration 206/1000 | Loss: 0.00001172
Iteration 207/1000 | Loss: 0.00001172
Iteration 208/1000 | Loss: 0.00001172
Iteration 209/1000 | Loss: 0.00001172
Iteration 210/1000 | Loss: 0.00001172
Iteration 211/1000 | Loss: 0.00001172
Iteration 212/1000 | Loss: 0.00001172
Iteration 213/1000 | Loss: 0.00001172
Iteration 214/1000 | Loss: 0.00001172
Iteration 215/1000 | Loss: 0.00001172
Iteration 216/1000 | Loss: 0.00001172
Iteration 217/1000 | Loss: 0.00001172
Iteration 218/1000 | Loss: 0.00001172
Iteration 219/1000 | Loss: 0.00001172
Iteration 220/1000 | Loss: 0.00001172
Iteration 221/1000 | Loss: 0.00001172
Iteration 222/1000 | Loss: 0.00001172
Iteration 223/1000 | Loss: 0.00001172
Iteration 224/1000 | Loss: 0.00001172
Iteration 225/1000 | Loss: 0.00001172
Iteration 226/1000 | Loss: 0.00001172
Iteration 227/1000 | Loss: 0.00001172
Iteration 228/1000 | Loss: 0.00001172
Iteration 229/1000 | Loss: 0.00001172
Iteration 230/1000 | Loss: 0.00001172
Iteration 231/1000 | Loss: 0.00001172
Iteration 232/1000 | Loss: 0.00001172
Iteration 233/1000 | Loss: 0.00001172
Iteration 234/1000 | Loss: 0.00001172
Iteration 235/1000 | Loss: 0.00001172
Iteration 236/1000 | Loss: 0.00001172
Iteration 237/1000 | Loss: 0.00001172
Iteration 238/1000 | Loss: 0.00001172
Iteration 239/1000 | Loss: 0.00001172
Iteration 240/1000 | Loss: 0.00001172
Iteration 241/1000 | Loss: 0.00001172
Iteration 242/1000 | Loss: 0.00001172
Iteration 243/1000 | Loss: 0.00001172
Iteration 244/1000 | Loss: 0.00001172
Iteration 245/1000 | Loss: 0.00001172
Iteration 246/1000 | Loss: 0.00001172
Iteration 247/1000 | Loss: 0.00001172
Iteration 248/1000 | Loss: 0.00001172
Iteration 249/1000 | Loss: 0.00001172
Iteration 250/1000 | Loss: 0.00001172
Iteration 251/1000 | Loss: 0.00001172
Iteration 252/1000 | Loss: 0.00001172
Iteration 253/1000 | Loss: 0.00001172
Iteration 254/1000 | Loss: 0.00001172
Iteration 255/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.1716923836502247e-05, 1.1716923836502247e-05, 1.1716923836502247e-05, 1.1716923836502247e-05, 1.1716923836502247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1716923836502247e-05

Optimization complete. Final v2v error: 2.9244565963745117 mm

Highest mean error: 3.50860333442688 mm for frame 82

Lowest mean error: 2.599581241607666 mm for frame 34

Saving results

Total time: 42.08860993385315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379330
Iteration 2/25 | Loss: 0.00122185
Iteration 3/25 | Loss: 0.00113560
Iteration 4/25 | Loss: 0.00112693
Iteration 5/25 | Loss: 0.00112365
Iteration 6/25 | Loss: 0.00112365
Iteration 7/25 | Loss: 0.00112365
Iteration 8/25 | Loss: 0.00112365
Iteration 9/25 | Loss: 0.00112365
Iteration 10/25 | Loss: 0.00112365
Iteration 11/25 | Loss: 0.00112365
Iteration 12/25 | Loss: 0.00112365
Iteration 13/25 | Loss: 0.00112365
Iteration 14/25 | Loss: 0.00112365
Iteration 15/25 | Loss: 0.00112365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011236489517614245, 0.0011236489517614245, 0.0011236489517614245, 0.0011236489517614245, 0.0011236489517614245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011236489517614245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51500750
Iteration 2/25 | Loss: 0.00080686
Iteration 3/25 | Loss: 0.00080686
Iteration 4/25 | Loss: 0.00080686
Iteration 5/25 | Loss: 0.00080686
Iteration 6/25 | Loss: 0.00080686
Iteration 7/25 | Loss: 0.00080686
Iteration 8/25 | Loss: 0.00080686
Iteration 9/25 | Loss: 0.00080686
Iteration 10/25 | Loss: 0.00080685
Iteration 11/25 | Loss: 0.00080685
Iteration 12/25 | Loss: 0.00080685
Iteration 13/25 | Loss: 0.00080685
Iteration 14/25 | Loss: 0.00080685
Iteration 15/25 | Loss: 0.00080685
Iteration 16/25 | Loss: 0.00080685
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008068546303547919, 0.0008068546303547919, 0.0008068546303547919, 0.0008068546303547919, 0.0008068546303547919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008068546303547919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080685
Iteration 2/1000 | Loss: 0.00002605
Iteration 3/1000 | Loss: 0.00001620
Iteration 4/1000 | Loss: 0.00001378
Iteration 5/1000 | Loss: 0.00001253
Iteration 6/1000 | Loss: 0.00001160
Iteration 7/1000 | Loss: 0.00001114
Iteration 8/1000 | Loss: 0.00001080
Iteration 9/1000 | Loss: 0.00001053
Iteration 10/1000 | Loss: 0.00001015
Iteration 11/1000 | Loss: 0.00001002
Iteration 12/1000 | Loss: 0.00000999
Iteration 13/1000 | Loss: 0.00000992
Iteration 14/1000 | Loss: 0.00000989
Iteration 15/1000 | Loss: 0.00000989
Iteration 16/1000 | Loss: 0.00000989
Iteration 17/1000 | Loss: 0.00000988
Iteration 18/1000 | Loss: 0.00000987
Iteration 19/1000 | Loss: 0.00000987
Iteration 20/1000 | Loss: 0.00000986
Iteration 21/1000 | Loss: 0.00000985
Iteration 22/1000 | Loss: 0.00000982
Iteration 23/1000 | Loss: 0.00000981
Iteration 24/1000 | Loss: 0.00000981
Iteration 25/1000 | Loss: 0.00000980
Iteration 26/1000 | Loss: 0.00000980
Iteration 27/1000 | Loss: 0.00000980
Iteration 28/1000 | Loss: 0.00000980
Iteration 29/1000 | Loss: 0.00000980
Iteration 30/1000 | Loss: 0.00000979
Iteration 31/1000 | Loss: 0.00000979
Iteration 32/1000 | Loss: 0.00000979
Iteration 33/1000 | Loss: 0.00000979
Iteration 34/1000 | Loss: 0.00000978
Iteration 35/1000 | Loss: 0.00000978
Iteration 36/1000 | Loss: 0.00000978
Iteration 37/1000 | Loss: 0.00000977
Iteration 38/1000 | Loss: 0.00000977
Iteration 39/1000 | Loss: 0.00000977
Iteration 40/1000 | Loss: 0.00000976
Iteration 41/1000 | Loss: 0.00000976
Iteration 42/1000 | Loss: 0.00000976
Iteration 43/1000 | Loss: 0.00000976
Iteration 44/1000 | Loss: 0.00000976
Iteration 45/1000 | Loss: 0.00000976
Iteration 46/1000 | Loss: 0.00000976
Iteration 47/1000 | Loss: 0.00000976
Iteration 48/1000 | Loss: 0.00000976
Iteration 49/1000 | Loss: 0.00000976
Iteration 50/1000 | Loss: 0.00000976
Iteration 51/1000 | Loss: 0.00000976
Iteration 52/1000 | Loss: 0.00000976
Iteration 53/1000 | Loss: 0.00000976
Iteration 54/1000 | Loss: 0.00000976
Iteration 55/1000 | Loss: 0.00000976
Iteration 56/1000 | Loss: 0.00000976
Iteration 57/1000 | Loss: 0.00000976
Iteration 58/1000 | Loss: 0.00000976
Iteration 59/1000 | Loss: 0.00000976
Iteration 60/1000 | Loss: 0.00000976
Iteration 61/1000 | Loss: 0.00000976
Iteration 62/1000 | Loss: 0.00000976
Iteration 63/1000 | Loss: 0.00000976
Iteration 64/1000 | Loss: 0.00000976
Iteration 65/1000 | Loss: 0.00000976
Iteration 66/1000 | Loss: 0.00000976
Iteration 67/1000 | Loss: 0.00000976
Iteration 68/1000 | Loss: 0.00000976
Iteration 69/1000 | Loss: 0.00000976
Iteration 70/1000 | Loss: 0.00000976
Iteration 71/1000 | Loss: 0.00000976
Iteration 72/1000 | Loss: 0.00000976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [9.764645255927462e-06, 9.764645255927462e-06, 9.764645255927462e-06, 9.764645255927462e-06, 9.764645255927462e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.764645255927462e-06

Optimization complete. Final v2v error: 2.7040982246398926 mm

Highest mean error: 3.1026268005371094 mm for frame 131

Lowest mean error: 2.4837870597839355 mm for frame 37

Saving results

Total time: 30.46090817451477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993468
Iteration 2/25 | Loss: 0.00189019
Iteration 3/25 | Loss: 0.00150236
Iteration 4/25 | Loss: 0.00142233
Iteration 5/25 | Loss: 0.00142951
Iteration 6/25 | Loss: 0.00142427
Iteration 7/25 | Loss: 0.00136312
Iteration 8/25 | Loss: 0.00131339
Iteration 9/25 | Loss: 0.00130111
Iteration 10/25 | Loss: 0.00129824
Iteration 11/25 | Loss: 0.00129896
Iteration 12/25 | Loss: 0.00129391
Iteration 13/25 | Loss: 0.00128942
Iteration 14/25 | Loss: 0.00128655
Iteration 15/25 | Loss: 0.00128939
Iteration 16/25 | Loss: 0.00128798
Iteration 17/25 | Loss: 0.00128717
Iteration 18/25 | Loss: 0.00129037
Iteration 19/25 | Loss: 0.00128800
Iteration 20/25 | Loss: 0.00128507
Iteration 21/25 | Loss: 0.00128570
Iteration 22/25 | Loss: 0.00128026
Iteration 23/25 | Loss: 0.00127568
Iteration 24/25 | Loss: 0.00127245
Iteration 25/25 | Loss: 0.00127154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90792465
Iteration 2/25 | Loss: 0.00086504
Iteration 3/25 | Loss: 0.00086504
Iteration 4/25 | Loss: 0.00086504
Iteration 5/25 | Loss: 0.00086504
Iteration 6/25 | Loss: 0.00086504
Iteration 7/25 | Loss: 0.00086504
Iteration 8/25 | Loss: 0.00086504
Iteration 9/25 | Loss: 0.00086504
Iteration 10/25 | Loss: 0.00086504
Iteration 11/25 | Loss: 0.00086504
Iteration 12/25 | Loss: 0.00086504
Iteration 13/25 | Loss: 0.00086504
Iteration 14/25 | Loss: 0.00086504
Iteration 15/25 | Loss: 0.00086504
Iteration 16/25 | Loss: 0.00086504
Iteration 17/25 | Loss: 0.00086504
Iteration 18/25 | Loss: 0.00086504
Iteration 19/25 | Loss: 0.00086504
Iteration 20/25 | Loss: 0.00086504
Iteration 21/25 | Loss: 0.00086504
Iteration 22/25 | Loss: 0.00086504
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008650358067825437, 0.0008650358067825437, 0.0008650358067825437, 0.0008650358067825437, 0.0008650358067825437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008650358067825437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086504
Iteration 2/1000 | Loss: 0.00004072
Iteration 3/1000 | Loss: 0.00022513
Iteration 4/1000 | Loss: 0.00024357
Iteration 5/1000 | Loss: 0.00004749
Iteration 6/1000 | Loss: 0.00026290
Iteration 7/1000 | Loss: 0.00003603
Iteration 8/1000 | Loss: 0.00022418
Iteration 9/1000 | Loss: 0.00024318
Iteration 10/1000 | Loss: 0.00022008
Iteration 11/1000 | Loss: 0.00003032
Iteration 12/1000 | Loss: 0.00002824
Iteration 13/1000 | Loss: 0.00002698
Iteration 14/1000 | Loss: 0.00024287
Iteration 15/1000 | Loss: 0.00040840
Iteration 16/1000 | Loss: 0.00005096
Iteration 17/1000 | Loss: 0.00003617
Iteration 18/1000 | Loss: 0.00028102
Iteration 19/1000 | Loss: 0.00025985
Iteration 20/1000 | Loss: 0.00027716
Iteration 21/1000 | Loss: 0.00025539
Iteration 22/1000 | Loss: 0.00003300
Iteration 23/1000 | Loss: 0.00018497
Iteration 24/1000 | Loss: 0.00021786
Iteration 25/1000 | Loss: 0.00013602
Iteration 26/1000 | Loss: 0.00003069
Iteration 27/1000 | Loss: 0.00002797
Iteration 28/1000 | Loss: 0.00002637
Iteration 29/1000 | Loss: 0.00002856
Iteration 30/1000 | Loss: 0.00002543
Iteration 31/1000 | Loss: 0.00002499
Iteration 32/1000 | Loss: 0.00002570
Iteration 33/1000 | Loss: 0.00002445
Iteration 34/1000 | Loss: 0.00002411
Iteration 35/1000 | Loss: 0.00002406
Iteration 36/1000 | Loss: 0.00002403
Iteration 37/1000 | Loss: 0.00002402
Iteration 38/1000 | Loss: 0.00002386
Iteration 39/1000 | Loss: 0.00002378
Iteration 40/1000 | Loss: 0.00002370
Iteration 41/1000 | Loss: 0.00002361
Iteration 42/1000 | Loss: 0.00002360
Iteration 43/1000 | Loss: 0.00002355
Iteration 44/1000 | Loss: 0.00002355
Iteration 45/1000 | Loss: 0.00002354
Iteration 46/1000 | Loss: 0.00002353
Iteration 47/1000 | Loss: 0.00002353
Iteration 48/1000 | Loss: 0.00002352
Iteration 49/1000 | Loss: 0.00002351
Iteration 50/1000 | Loss: 0.00002350
Iteration 51/1000 | Loss: 0.00002350
Iteration 52/1000 | Loss: 0.00002348
Iteration 53/1000 | Loss: 0.00002338
Iteration 54/1000 | Loss: 0.00002336
Iteration 55/1000 | Loss: 0.00002335
Iteration 56/1000 | Loss: 0.00002335
Iteration 57/1000 | Loss: 0.00002334
Iteration 58/1000 | Loss: 0.00002334
Iteration 59/1000 | Loss: 0.00002334
Iteration 60/1000 | Loss: 0.00002333
Iteration 61/1000 | Loss: 0.00002333
Iteration 62/1000 | Loss: 0.00002333
Iteration 63/1000 | Loss: 0.00002333
Iteration 64/1000 | Loss: 0.00002332
Iteration 65/1000 | Loss: 0.00002332
Iteration 66/1000 | Loss: 0.00002331
Iteration 67/1000 | Loss: 0.00002331
Iteration 68/1000 | Loss: 0.00002330
Iteration 69/1000 | Loss: 0.00002330
Iteration 70/1000 | Loss: 0.00002329
Iteration 71/1000 | Loss: 0.00002328
Iteration 72/1000 | Loss: 0.00002328
Iteration 73/1000 | Loss: 0.00002328
Iteration 74/1000 | Loss: 0.00002327
Iteration 75/1000 | Loss: 0.00002327
Iteration 76/1000 | Loss: 0.00002327
Iteration 77/1000 | Loss: 0.00002327
Iteration 78/1000 | Loss: 0.00002326
Iteration 79/1000 | Loss: 0.00002326
Iteration 80/1000 | Loss: 0.00002326
Iteration 81/1000 | Loss: 0.00002326
Iteration 82/1000 | Loss: 0.00002326
Iteration 83/1000 | Loss: 0.00002326
Iteration 84/1000 | Loss: 0.00002325
Iteration 85/1000 | Loss: 0.00002325
Iteration 86/1000 | Loss: 0.00002324
Iteration 87/1000 | Loss: 0.00002324
Iteration 88/1000 | Loss: 0.00002324
Iteration 89/1000 | Loss: 0.00002323
Iteration 90/1000 | Loss: 0.00002323
Iteration 91/1000 | Loss: 0.00002322
Iteration 92/1000 | Loss: 0.00002321
Iteration 93/1000 | Loss: 0.00002321
Iteration 94/1000 | Loss: 0.00002321
Iteration 95/1000 | Loss: 0.00002320
Iteration 96/1000 | Loss: 0.00002320
Iteration 97/1000 | Loss: 0.00002319
Iteration 98/1000 | Loss: 0.00002319
Iteration 99/1000 | Loss: 0.00002319
Iteration 100/1000 | Loss: 0.00002318
Iteration 101/1000 | Loss: 0.00002315
Iteration 102/1000 | Loss: 0.00002315
Iteration 103/1000 | Loss: 0.00002315
Iteration 104/1000 | Loss: 0.00002315
Iteration 105/1000 | Loss: 0.00002314
Iteration 106/1000 | Loss: 0.00002313
Iteration 107/1000 | Loss: 0.00002313
Iteration 108/1000 | Loss: 0.00002312
Iteration 109/1000 | Loss: 0.00002312
Iteration 110/1000 | Loss: 0.00002311
Iteration 111/1000 | Loss: 0.00002311
Iteration 112/1000 | Loss: 0.00002310
Iteration 113/1000 | Loss: 0.00002310
Iteration 114/1000 | Loss: 0.00002310
Iteration 115/1000 | Loss: 0.00002310
Iteration 116/1000 | Loss: 0.00002310
Iteration 117/1000 | Loss: 0.00002310
Iteration 118/1000 | Loss: 0.00002310
Iteration 119/1000 | Loss: 0.00002310
Iteration 120/1000 | Loss: 0.00002310
Iteration 121/1000 | Loss: 0.00002310
Iteration 122/1000 | Loss: 0.00002310
Iteration 123/1000 | Loss: 0.00002309
Iteration 124/1000 | Loss: 0.00002309
Iteration 125/1000 | Loss: 0.00002309
Iteration 126/1000 | Loss: 0.00002309
Iteration 127/1000 | Loss: 0.00002309
Iteration 128/1000 | Loss: 0.00002309
Iteration 129/1000 | Loss: 0.00002309
Iteration 130/1000 | Loss: 0.00002308
Iteration 131/1000 | Loss: 0.00002308
Iteration 132/1000 | Loss: 0.00002308
Iteration 133/1000 | Loss: 0.00002307
Iteration 134/1000 | Loss: 0.00002307
Iteration 135/1000 | Loss: 0.00002306
Iteration 136/1000 | Loss: 0.00002306
Iteration 137/1000 | Loss: 0.00002306
Iteration 138/1000 | Loss: 0.00002306
Iteration 139/1000 | Loss: 0.00002306
Iteration 140/1000 | Loss: 0.00002305
Iteration 141/1000 | Loss: 0.00002305
Iteration 142/1000 | Loss: 0.00002304
Iteration 143/1000 | Loss: 0.00002304
Iteration 144/1000 | Loss: 0.00002303
Iteration 145/1000 | Loss: 0.00002303
Iteration 146/1000 | Loss: 0.00002303
Iteration 147/1000 | Loss: 0.00002303
Iteration 148/1000 | Loss: 0.00002303
Iteration 149/1000 | Loss: 0.00002303
Iteration 150/1000 | Loss: 0.00002302
Iteration 151/1000 | Loss: 0.00002302
Iteration 152/1000 | Loss: 0.00002302
Iteration 153/1000 | Loss: 0.00002302
Iteration 154/1000 | Loss: 0.00002302
Iteration 155/1000 | Loss: 0.00002302
Iteration 156/1000 | Loss: 0.00002301
Iteration 157/1000 | Loss: 0.00002301
Iteration 158/1000 | Loss: 0.00002301
Iteration 159/1000 | Loss: 0.00002300
Iteration 160/1000 | Loss: 0.00002300
Iteration 161/1000 | Loss: 0.00002300
Iteration 162/1000 | Loss: 0.00002299
Iteration 163/1000 | Loss: 0.00002299
Iteration 164/1000 | Loss: 0.00002299
Iteration 165/1000 | Loss: 0.00002298
Iteration 166/1000 | Loss: 0.00002298
Iteration 167/1000 | Loss: 0.00002298
Iteration 168/1000 | Loss: 0.00002298
Iteration 169/1000 | Loss: 0.00002297
Iteration 170/1000 | Loss: 0.00002297
Iteration 171/1000 | Loss: 0.00002297
Iteration 172/1000 | Loss: 0.00002297
Iteration 173/1000 | Loss: 0.00002297
Iteration 174/1000 | Loss: 0.00002296
Iteration 175/1000 | Loss: 0.00002296
Iteration 176/1000 | Loss: 0.00002296
Iteration 177/1000 | Loss: 0.00002296
Iteration 178/1000 | Loss: 0.00002296
Iteration 179/1000 | Loss: 0.00002296
Iteration 180/1000 | Loss: 0.00002296
Iteration 181/1000 | Loss: 0.00002296
Iteration 182/1000 | Loss: 0.00002295
Iteration 183/1000 | Loss: 0.00002295
Iteration 184/1000 | Loss: 0.00002295
Iteration 185/1000 | Loss: 0.00002295
Iteration 186/1000 | Loss: 0.00002294
Iteration 187/1000 | Loss: 0.00002294
Iteration 188/1000 | Loss: 0.00002294
Iteration 189/1000 | Loss: 0.00002294
Iteration 190/1000 | Loss: 0.00002294
Iteration 191/1000 | Loss: 0.00002294
Iteration 192/1000 | Loss: 0.00002294
Iteration 193/1000 | Loss: 0.00002293
Iteration 194/1000 | Loss: 0.00002293
Iteration 195/1000 | Loss: 0.00002293
Iteration 196/1000 | Loss: 0.00002293
Iteration 197/1000 | Loss: 0.00002293
Iteration 198/1000 | Loss: 0.00002293
Iteration 199/1000 | Loss: 0.00002293
Iteration 200/1000 | Loss: 0.00002293
Iteration 201/1000 | Loss: 0.00002293
Iteration 202/1000 | Loss: 0.00002293
Iteration 203/1000 | Loss: 0.00002293
Iteration 204/1000 | Loss: 0.00002293
Iteration 205/1000 | Loss: 0.00002293
Iteration 206/1000 | Loss: 0.00002293
Iteration 207/1000 | Loss: 0.00002293
Iteration 208/1000 | Loss: 0.00002293
Iteration 209/1000 | Loss: 0.00002293
Iteration 210/1000 | Loss: 0.00002293
Iteration 211/1000 | Loss: 0.00002293
Iteration 212/1000 | Loss: 0.00002293
Iteration 213/1000 | Loss: 0.00002293
Iteration 214/1000 | Loss: 0.00002293
Iteration 215/1000 | Loss: 0.00002293
Iteration 216/1000 | Loss: 0.00002293
Iteration 217/1000 | Loss: 0.00002293
Iteration 218/1000 | Loss: 0.00002293
Iteration 219/1000 | Loss: 0.00002293
Iteration 220/1000 | Loss: 0.00002293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.2928061298443936e-05, 2.2928061298443936e-05, 2.2928061298443936e-05, 2.2928061298443936e-05, 2.2928061298443936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2928061298443936e-05

Optimization complete. Final v2v error: 3.76254940032959 mm

Highest mean error: 5.81251859664917 mm for frame 87

Lowest mean error: 3.2810304164886475 mm for frame 182

Saving results

Total time: 118.65795803070068
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449594
Iteration 2/25 | Loss: 0.00130801
Iteration 3/25 | Loss: 0.00121326
Iteration 4/25 | Loss: 0.00120306
Iteration 5/25 | Loss: 0.00120044
Iteration 6/25 | Loss: 0.00119999
Iteration 7/25 | Loss: 0.00119999
Iteration 8/25 | Loss: 0.00119999
Iteration 9/25 | Loss: 0.00119999
Iteration 10/25 | Loss: 0.00119999
Iteration 11/25 | Loss: 0.00119999
Iteration 12/25 | Loss: 0.00119999
Iteration 13/25 | Loss: 0.00119999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011999938869848847, 0.0011999938869848847, 0.0011999938869848847, 0.0011999938869848847, 0.0011999938869848847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011999938869848847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36395335
Iteration 2/25 | Loss: 0.00091343
Iteration 3/25 | Loss: 0.00091343
Iteration 4/25 | Loss: 0.00091343
Iteration 5/25 | Loss: 0.00091343
Iteration 6/25 | Loss: 0.00091343
Iteration 7/25 | Loss: 0.00091343
Iteration 8/25 | Loss: 0.00091343
Iteration 9/25 | Loss: 0.00091343
Iteration 10/25 | Loss: 0.00091343
Iteration 11/25 | Loss: 0.00091343
Iteration 12/25 | Loss: 0.00091343
Iteration 13/25 | Loss: 0.00091343
Iteration 14/25 | Loss: 0.00091343
Iteration 15/25 | Loss: 0.00091343
Iteration 16/25 | Loss: 0.00091343
Iteration 17/25 | Loss: 0.00091343
Iteration 18/25 | Loss: 0.00091343
Iteration 19/25 | Loss: 0.00091343
Iteration 20/25 | Loss: 0.00091343
Iteration 21/25 | Loss: 0.00091343
Iteration 22/25 | Loss: 0.00091343
Iteration 23/25 | Loss: 0.00091343
Iteration 24/25 | Loss: 0.00091343
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009134312276728451, 0.0009134312276728451, 0.0009134312276728451, 0.0009134312276728451, 0.0009134312276728451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009134312276728451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091343
Iteration 2/1000 | Loss: 0.00003030
Iteration 3/1000 | Loss: 0.00002009
Iteration 4/1000 | Loss: 0.00001810
Iteration 5/1000 | Loss: 0.00001726
Iteration 6/1000 | Loss: 0.00001681
Iteration 7/1000 | Loss: 0.00001649
Iteration 8/1000 | Loss: 0.00001628
Iteration 9/1000 | Loss: 0.00001610
Iteration 10/1000 | Loss: 0.00001609
Iteration 11/1000 | Loss: 0.00001603
Iteration 12/1000 | Loss: 0.00001602
Iteration 13/1000 | Loss: 0.00001601
Iteration 14/1000 | Loss: 0.00001601
Iteration 15/1000 | Loss: 0.00001597
Iteration 16/1000 | Loss: 0.00001594
Iteration 17/1000 | Loss: 0.00001584
Iteration 18/1000 | Loss: 0.00001577
Iteration 19/1000 | Loss: 0.00001575
Iteration 20/1000 | Loss: 0.00001573
Iteration 21/1000 | Loss: 0.00001571
Iteration 22/1000 | Loss: 0.00001571
Iteration 23/1000 | Loss: 0.00001571
Iteration 24/1000 | Loss: 0.00001571
Iteration 25/1000 | Loss: 0.00001571
Iteration 26/1000 | Loss: 0.00001571
Iteration 27/1000 | Loss: 0.00001571
Iteration 28/1000 | Loss: 0.00001571
Iteration 29/1000 | Loss: 0.00001570
Iteration 30/1000 | Loss: 0.00001570
Iteration 31/1000 | Loss: 0.00001565
Iteration 32/1000 | Loss: 0.00001559
Iteration 33/1000 | Loss: 0.00001558
Iteration 34/1000 | Loss: 0.00001556
Iteration 35/1000 | Loss: 0.00001556
Iteration 36/1000 | Loss: 0.00001556
Iteration 37/1000 | Loss: 0.00001556
Iteration 38/1000 | Loss: 0.00001556
Iteration 39/1000 | Loss: 0.00001556
Iteration 40/1000 | Loss: 0.00001556
Iteration 41/1000 | Loss: 0.00001553
Iteration 42/1000 | Loss: 0.00001553
Iteration 43/1000 | Loss: 0.00001553
Iteration 44/1000 | Loss: 0.00001552
Iteration 45/1000 | Loss: 0.00001551
Iteration 46/1000 | Loss: 0.00001551
Iteration 47/1000 | Loss: 0.00001551
Iteration 48/1000 | Loss: 0.00001551
Iteration 49/1000 | Loss: 0.00001551
Iteration 50/1000 | Loss: 0.00001550
Iteration 51/1000 | Loss: 0.00001549
Iteration 52/1000 | Loss: 0.00001549
Iteration 53/1000 | Loss: 0.00001549
Iteration 54/1000 | Loss: 0.00001548
Iteration 55/1000 | Loss: 0.00001548
Iteration 56/1000 | Loss: 0.00001547
Iteration 57/1000 | Loss: 0.00001546
Iteration 58/1000 | Loss: 0.00001546
Iteration 59/1000 | Loss: 0.00001545
Iteration 60/1000 | Loss: 0.00001545
Iteration 61/1000 | Loss: 0.00001545
Iteration 62/1000 | Loss: 0.00001544
Iteration 63/1000 | Loss: 0.00001544
Iteration 64/1000 | Loss: 0.00001544
Iteration 65/1000 | Loss: 0.00001544
Iteration 66/1000 | Loss: 0.00001543
Iteration 67/1000 | Loss: 0.00001543
Iteration 68/1000 | Loss: 0.00001542
Iteration 69/1000 | Loss: 0.00001541
Iteration 70/1000 | Loss: 0.00001540
Iteration 71/1000 | Loss: 0.00001540
Iteration 72/1000 | Loss: 0.00001540
Iteration 73/1000 | Loss: 0.00001540
Iteration 74/1000 | Loss: 0.00001539
Iteration 75/1000 | Loss: 0.00001539
Iteration 76/1000 | Loss: 0.00001538
Iteration 77/1000 | Loss: 0.00001537
Iteration 78/1000 | Loss: 0.00001537
Iteration 79/1000 | Loss: 0.00001537
Iteration 80/1000 | Loss: 0.00001537
Iteration 81/1000 | Loss: 0.00001537
Iteration 82/1000 | Loss: 0.00001537
Iteration 83/1000 | Loss: 0.00001537
Iteration 84/1000 | Loss: 0.00001537
Iteration 85/1000 | Loss: 0.00001536
Iteration 86/1000 | Loss: 0.00001536
Iteration 87/1000 | Loss: 0.00001536
Iteration 88/1000 | Loss: 0.00001536
Iteration 89/1000 | Loss: 0.00001536
Iteration 90/1000 | Loss: 0.00001536
Iteration 91/1000 | Loss: 0.00001536
Iteration 92/1000 | Loss: 0.00001536
Iteration 93/1000 | Loss: 0.00001536
Iteration 94/1000 | Loss: 0.00001536
Iteration 95/1000 | Loss: 0.00001536
Iteration 96/1000 | Loss: 0.00001536
Iteration 97/1000 | Loss: 0.00001536
Iteration 98/1000 | Loss: 0.00001536
Iteration 99/1000 | Loss: 0.00001536
Iteration 100/1000 | Loss: 0.00001536
Iteration 101/1000 | Loss: 0.00001536
Iteration 102/1000 | Loss: 0.00001536
Iteration 103/1000 | Loss: 0.00001536
Iteration 104/1000 | Loss: 0.00001536
Iteration 105/1000 | Loss: 0.00001536
Iteration 106/1000 | Loss: 0.00001536
Iteration 107/1000 | Loss: 0.00001536
Iteration 108/1000 | Loss: 0.00001536
Iteration 109/1000 | Loss: 0.00001536
Iteration 110/1000 | Loss: 0.00001536
Iteration 111/1000 | Loss: 0.00001536
Iteration 112/1000 | Loss: 0.00001536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.5364759747171775e-05, 1.5364759747171775e-05, 1.5364759747171775e-05, 1.5364759747171775e-05, 1.5364759747171775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5364759747171775e-05

Optimization complete. Final v2v error: 3.235836982727051 mm

Highest mean error: 3.5598998069763184 mm for frame 160

Lowest mean error: 2.9717421531677246 mm for frame 2

Saving results

Total time: 32.66216826438904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880237
Iteration 2/25 | Loss: 0.00170084
Iteration 3/25 | Loss: 0.00136303
Iteration 4/25 | Loss: 0.00136689
Iteration 5/25 | Loss: 0.00130427
Iteration 6/25 | Loss: 0.00129672
Iteration 7/25 | Loss: 0.00129322
Iteration 8/25 | Loss: 0.00127466
Iteration 9/25 | Loss: 0.00126012
Iteration 10/25 | Loss: 0.00125863
Iteration 11/25 | Loss: 0.00124639
Iteration 12/25 | Loss: 0.00124801
Iteration 13/25 | Loss: 0.00124023
Iteration 14/25 | Loss: 0.00123853
Iteration 15/25 | Loss: 0.00123563
Iteration 16/25 | Loss: 0.00123491
Iteration 17/25 | Loss: 0.00124083
Iteration 18/25 | Loss: 0.00123627
Iteration 19/25 | Loss: 0.00123225
Iteration 20/25 | Loss: 0.00123198
Iteration 21/25 | Loss: 0.00123396
Iteration 22/25 | Loss: 0.00123333
Iteration 23/25 | Loss: 0.00123379
Iteration 24/25 | Loss: 0.00123597
Iteration 25/25 | Loss: 0.00123340

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.66976118
Iteration 2/25 | Loss: 0.00140531
Iteration 3/25 | Loss: 0.00140531
Iteration 4/25 | Loss: 0.00140531
Iteration 5/25 | Loss: 0.00140531
Iteration 6/25 | Loss: 0.00140531
Iteration 7/25 | Loss: 0.00140531
Iteration 8/25 | Loss: 0.00140531
Iteration 9/25 | Loss: 0.00140531
Iteration 10/25 | Loss: 0.00140531
Iteration 11/25 | Loss: 0.00140530
Iteration 12/25 | Loss: 0.00140530
Iteration 13/25 | Loss: 0.00140530
Iteration 14/25 | Loss: 0.00140530
Iteration 15/25 | Loss: 0.00140530
Iteration 16/25 | Loss: 0.00140530
Iteration 17/25 | Loss: 0.00140530
Iteration 18/25 | Loss: 0.00140530
Iteration 19/25 | Loss: 0.00140530
Iteration 20/25 | Loss: 0.00140530
Iteration 21/25 | Loss: 0.00140530
Iteration 22/25 | Loss: 0.00140530
Iteration 23/25 | Loss: 0.00140530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014053047634661198, 0.0014053047634661198, 0.0014053047634661198, 0.0014053047634661198, 0.0014053047634661198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014053047634661198

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140530
Iteration 2/1000 | Loss: 0.00031498
Iteration 3/1000 | Loss: 0.00020699
Iteration 4/1000 | Loss: 0.00024411
Iteration 5/1000 | Loss: 0.00052678
Iteration 6/1000 | Loss: 0.00391549
Iteration 7/1000 | Loss: 0.00015061
Iteration 8/1000 | Loss: 0.00008335
Iteration 9/1000 | Loss: 0.00007923
Iteration 10/1000 | Loss: 0.00004738
Iteration 11/1000 | Loss: 0.00007174
Iteration 12/1000 | Loss: 0.00005601
Iteration 13/1000 | Loss: 0.00114537
Iteration 14/1000 | Loss: 0.00029831
Iteration 15/1000 | Loss: 0.00006373
Iteration 16/1000 | Loss: 0.00073623
Iteration 17/1000 | Loss: 0.00003846
Iteration 18/1000 | Loss: 0.00006477
Iteration 19/1000 | Loss: 0.00004641
Iteration 20/1000 | Loss: 0.00003994
Iteration 21/1000 | Loss: 0.00004881
Iteration 22/1000 | Loss: 0.00003449
Iteration 23/1000 | Loss: 0.00005679
Iteration 24/1000 | Loss: 0.00003404
Iteration 25/1000 | Loss: 0.00004992
Iteration 26/1000 | Loss: 0.00005173
Iteration 27/1000 | Loss: 0.00004487
Iteration 28/1000 | Loss: 0.00004918
Iteration 29/1000 | Loss: 0.00004522
Iteration 30/1000 | Loss: 0.00004707
Iteration 31/1000 | Loss: 0.00003738
Iteration 32/1000 | Loss: 0.00004387
Iteration 33/1000 | Loss: 0.00003722
Iteration 34/1000 | Loss: 0.00004784
Iteration 35/1000 | Loss: 0.00003680
Iteration 36/1000 | Loss: 0.00004722
Iteration 37/1000 | Loss: 0.00003716
Iteration 38/1000 | Loss: 0.00004851
Iteration 39/1000 | Loss: 0.00003662
Iteration 40/1000 | Loss: 0.00005252
Iteration 41/1000 | Loss: 0.00004561
Iteration 42/1000 | Loss: 0.00004560
Iteration 43/1000 | Loss: 0.00005318
Iteration 44/1000 | Loss: 0.00004629
Iteration 45/1000 | Loss: 0.00004738
Iteration 46/1000 | Loss: 0.00003305
Iteration 47/1000 | Loss: 0.00004275
Iteration 48/1000 | Loss: 0.00003441
Iteration 49/1000 | Loss: 0.00005271
Iteration 50/1000 | Loss: 0.00003280
Iteration 51/1000 | Loss: 0.00003135
Iteration 52/1000 | Loss: 0.00003060
Iteration 53/1000 | Loss: 0.00003008
Iteration 54/1000 | Loss: 0.00002950
Iteration 55/1000 | Loss: 0.00002907
Iteration 56/1000 | Loss: 0.00002882
Iteration 57/1000 | Loss: 0.00002862
Iteration 58/1000 | Loss: 0.00002844
Iteration 59/1000 | Loss: 0.00002835
Iteration 60/1000 | Loss: 0.00002834
Iteration 61/1000 | Loss: 0.00002833
Iteration 62/1000 | Loss: 0.00002829
Iteration 63/1000 | Loss: 0.00002828
Iteration 64/1000 | Loss: 0.00002824
Iteration 65/1000 | Loss: 0.00002824
Iteration 66/1000 | Loss: 0.00002823
Iteration 67/1000 | Loss: 0.00002822
Iteration 68/1000 | Loss: 0.00002821
Iteration 69/1000 | Loss: 0.00002818
Iteration 70/1000 | Loss: 0.00002813
Iteration 71/1000 | Loss: 0.00002802
Iteration 72/1000 | Loss: 0.00002797
Iteration 73/1000 | Loss: 0.00002792
Iteration 74/1000 | Loss: 0.00002788
Iteration 75/1000 | Loss: 0.00002788
Iteration 76/1000 | Loss: 0.00002787
Iteration 77/1000 | Loss: 0.00002786
Iteration 78/1000 | Loss: 0.00002785
Iteration 79/1000 | Loss: 0.00002785
Iteration 80/1000 | Loss: 0.00002784
Iteration 81/1000 | Loss: 0.00002784
Iteration 82/1000 | Loss: 0.00002782
Iteration 83/1000 | Loss: 0.00002781
Iteration 84/1000 | Loss: 0.00002780
Iteration 85/1000 | Loss: 0.00002780
Iteration 86/1000 | Loss: 0.00002780
Iteration 87/1000 | Loss: 0.00002778
Iteration 88/1000 | Loss: 0.00002778
Iteration 89/1000 | Loss: 0.00002777
Iteration 90/1000 | Loss: 0.00002777
Iteration 91/1000 | Loss: 0.00002776
Iteration 92/1000 | Loss: 0.00002776
Iteration 93/1000 | Loss: 0.00002774
Iteration 94/1000 | Loss: 0.00002773
Iteration 95/1000 | Loss: 0.00002771
Iteration 96/1000 | Loss: 0.00002769
Iteration 97/1000 | Loss: 0.00002769
Iteration 98/1000 | Loss: 0.00002769
Iteration 99/1000 | Loss: 0.00002769
Iteration 100/1000 | Loss: 0.00002769
Iteration 101/1000 | Loss: 0.00002768
Iteration 102/1000 | Loss: 0.00002768
Iteration 103/1000 | Loss: 0.00002767
Iteration 104/1000 | Loss: 0.00002767
Iteration 105/1000 | Loss: 0.00002767
Iteration 106/1000 | Loss: 0.00002767
Iteration 107/1000 | Loss: 0.00002766
Iteration 108/1000 | Loss: 0.00002766
Iteration 109/1000 | Loss: 0.00002766
Iteration 110/1000 | Loss: 0.00002766
Iteration 111/1000 | Loss: 0.00002765
Iteration 112/1000 | Loss: 0.00002765
Iteration 113/1000 | Loss: 0.00002765
Iteration 114/1000 | Loss: 0.00002764
Iteration 115/1000 | Loss: 0.00002764
Iteration 116/1000 | Loss: 0.00002764
Iteration 117/1000 | Loss: 0.00002764
Iteration 118/1000 | Loss: 0.00002764
Iteration 119/1000 | Loss: 0.00002764
Iteration 120/1000 | Loss: 0.00002764
Iteration 121/1000 | Loss: 0.00002764
Iteration 122/1000 | Loss: 0.00002764
Iteration 123/1000 | Loss: 0.00002764
Iteration 124/1000 | Loss: 0.00002764
Iteration 125/1000 | Loss: 0.00002764
Iteration 126/1000 | Loss: 0.00002763
Iteration 127/1000 | Loss: 0.00002762
Iteration 128/1000 | Loss: 0.00002762
Iteration 129/1000 | Loss: 0.00002762
Iteration 130/1000 | Loss: 0.00002762
Iteration 131/1000 | Loss: 0.00002762
Iteration 132/1000 | Loss: 0.00002762
Iteration 133/1000 | Loss: 0.00002762
Iteration 134/1000 | Loss: 0.00002762
Iteration 135/1000 | Loss: 0.00002762
Iteration 136/1000 | Loss: 0.00002762
Iteration 137/1000 | Loss: 0.00002761
Iteration 138/1000 | Loss: 0.00002761
Iteration 139/1000 | Loss: 0.00002761
Iteration 140/1000 | Loss: 0.00002761
Iteration 141/1000 | Loss: 0.00002760
Iteration 142/1000 | Loss: 0.00002760
Iteration 143/1000 | Loss: 0.00002760
Iteration 144/1000 | Loss: 0.00002760
Iteration 145/1000 | Loss: 0.00002759
Iteration 146/1000 | Loss: 0.00002759
Iteration 147/1000 | Loss: 0.00002759
Iteration 148/1000 | Loss: 0.00002759
Iteration 149/1000 | Loss: 0.00002758
Iteration 150/1000 | Loss: 0.00002758
Iteration 151/1000 | Loss: 0.00002758
Iteration 152/1000 | Loss: 0.00002758
Iteration 153/1000 | Loss: 0.00002758
Iteration 154/1000 | Loss: 0.00002757
Iteration 155/1000 | Loss: 0.00002757
Iteration 156/1000 | Loss: 0.00002757
Iteration 157/1000 | Loss: 0.00002757
Iteration 158/1000 | Loss: 0.00002756
Iteration 159/1000 | Loss: 0.00002756
Iteration 160/1000 | Loss: 0.00002756
Iteration 161/1000 | Loss: 0.00002756
Iteration 162/1000 | Loss: 0.00002755
Iteration 163/1000 | Loss: 0.00002755
Iteration 164/1000 | Loss: 0.00002755
Iteration 165/1000 | Loss: 0.00002755
Iteration 166/1000 | Loss: 0.00002755
Iteration 167/1000 | Loss: 0.00002755
Iteration 168/1000 | Loss: 0.00002754
Iteration 169/1000 | Loss: 0.00002754
Iteration 170/1000 | Loss: 0.00002754
Iteration 171/1000 | Loss: 0.00002754
Iteration 172/1000 | Loss: 0.00002753
Iteration 173/1000 | Loss: 0.00002753
Iteration 174/1000 | Loss: 0.00002753
Iteration 175/1000 | Loss: 0.00002752
Iteration 176/1000 | Loss: 0.00002752
Iteration 177/1000 | Loss: 0.00002752
Iteration 178/1000 | Loss: 0.00002752
Iteration 179/1000 | Loss: 0.00002752
Iteration 180/1000 | Loss: 0.00002752
Iteration 181/1000 | Loss: 0.00002752
Iteration 182/1000 | Loss: 0.00002752
Iteration 183/1000 | Loss: 0.00002752
Iteration 184/1000 | Loss: 0.00002752
Iteration 185/1000 | Loss: 0.00002752
Iteration 186/1000 | Loss: 0.00002751
Iteration 187/1000 | Loss: 0.00002751
Iteration 188/1000 | Loss: 0.00002751
Iteration 189/1000 | Loss: 0.00002751
Iteration 190/1000 | Loss: 0.00002751
Iteration 191/1000 | Loss: 0.00002751
Iteration 192/1000 | Loss: 0.00002751
Iteration 193/1000 | Loss: 0.00002751
Iteration 194/1000 | Loss: 0.00002750
Iteration 195/1000 | Loss: 0.00002750
Iteration 196/1000 | Loss: 0.00002750
Iteration 197/1000 | Loss: 0.00002750
Iteration 198/1000 | Loss: 0.00002750
Iteration 199/1000 | Loss: 0.00002750
Iteration 200/1000 | Loss: 0.00002750
Iteration 201/1000 | Loss: 0.00002750
Iteration 202/1000 | Loss: 0.00002750
Iteration 203/1000 | Loss: 0.00002750
Iteration 204/1000 | Loss: 0.00002750
Iteration 205/1000 | Loss: 0.00002750
Iteration 206/1000 | Loss: 0.00002750
Iteration 207/1000 | Loss: 0.00002750
Iteration 208/1000 | Loss: 0.00002750
Iteration 209/1000 | Loss: 0.00002749
Iteration 210/1000 | Loss: 0.00002749
Iteration 211/1000 | Loss: 0.00002749
Iteration 212/1000 | Loss: 0.00002749
Iteration 213/1000 | Loss: 0.00002749
Iteration 214/1000 | Loss: 0.00002749
Iteration 215/1000 | Loss: 0.00002749
Iteration 216/1000 | Loss: 0.00002749
Iteration 217/1000 | Loss: 0.00002749
Iteration 218/1000 | Loss: 0.00002749
Iteration 219/1000 | Loss: 0.00002748
Iteration 220/1000 | Loss: 0.00002748
Iteration 221/1000 | Loss: 0.00002748
Iteration 222/1000 | Loss: 0.00002748
Iteration 223/1000 | Loss: 0.00002748
Iteration 224/1000 | Loss: 0.00002748
Iteration 225/1000 | Loss: 0.00002748
Iteration 226/1000 | Loss: 0.00002747
Iteration 227/1000 | Loss: 0.00002747
Iteration 228/1000 | Loss: 0.00002747
Iteration 229/1000 | Loss: 0.00002747
Iteration 230/1000 | Loss: 0.00002747
Iteration 231/1000 | Loss: 0.00002747
Iteration 232/1000 | Loss: 0.00002747
Iteration 233/1000 | Loss: 0.00002747
Iteration 234/1000 | Loss: 0.00002747
Iteration 235/1000 | Loss: 0.00002747
Iteration 236/1000 | Loss: 0.00002746
Iteration 237/1000 | Loss: 0.00002746
Iteration 238/1000 | Loss: 0.00002746
Iteration 239/1000 | Loss: 0.00002746
Iteration 240/1000 | Loss: 0.00002746
Iteration 241/1000 | Loss: 0.00002746
Iteration 242/1000 | Loss: 0.00002746
Iteration 243/1000 | Loss: 0.00002746
Iteration 244/1000 | Loss: 0.00002746
Iteration 245/1000 | Loss: 0.00002746
Iteration 246/1000 | Loss: 0.00002746
Iteration 247/1000 | Loss: 0.00002745
Iteration 248/1000 | Loss: 0.00002745
Iteration 249/1000 | Loss: 0.00002745
Iteration 250/1000 | Loss: 0.00002745
Iteration 251/1000 | Loss: 0.00002745
Iteration 252/1000 | Loss: 0.00002745
Iteration 253/1000 | Loss: 0.00002745
Iteration 254/1000 | Loss: 0.00002745
Iteration 255/1000 | Loss: 0.00002745
Iteration 256/1000 | Loss: 0.00002745
Iteration 257/1000 | Loss: 0.00002745
Iteration 258/1000 | Loss: 0.00002745
Iteration 259/1000 | Loss: 0.00002745
Iteration 260/1000 | Loss: 0.00002744
Iteration 261/1000 | Loss: 0.00002744
Iteration 262/1000 | Loss: 0.00002744
Iteration 263/1000 | Loss: 0.00002744
Iteration 264/1000 | Loss: 0.00002744
Iteration 265/1000 | Loss: 0.00002744
Iteration 266/1000 | Loss: 0.00002744
Iteration 267/1000 | Loss: 0.00002744
Iteration 268/1000 | Loss: 0.00002743
Iteration 269/1000 | Loss: 0.00002743
Iteration 270/1000 | Loss: 0.00002743
Iteration 271/1000 | Loss: 0.00002743
Iteration 272/1000 | Loss: 0.00002743
Iteration 273/1000 | Loss: 0.00002743
Iteration 274/1000 | Loss: 0.00002743
Iteration 275/1000 | Loss: 0.00002743
Iteration 276/1000 | Loss: 0.00002743
Iteration 277/1000 | Loss: 0.00002743
Iteration 278/1000 | Loss: 0.00002743
Iteration 279/1000 | Loss: 0.00002742
Iteration 280/1000 | Loss: 0.00002742
Iteration 281/1000 | Loss: 0.00002742
Iteration 282/1000 | Loss: 0.00002742
Iteration 283/1000 | Loss: 0.00002742
Iteration 284/1000 | Loss: 0.00002742
Iteration 285/1000 | Loss: 0.00002742
Iteration 286/1000 | Loss: 0.00002742
Iteration 287/1000 | Loss: 0.00002742
Iteration 288/1000 | Loss: 0.00002742
Iteration 289/1000 | Loss: 0.00002742
Iteration 290/1000 | Loss: 0.00002742
Iteration 291/1000 | Loss: 0.00002742
Iteration 292/1000 | Loss: 0.00002742
Iteration 293/1000 | Loss: 0.00002742
Iteration 294/1000 | Loss: 0.00002741
Iteration 295/1000 | Loss: 0.00002741
Iteration 296/1000 | Loss: 0.00002741
Iteration 297/1000 | Loss: 0.00002741
Iteration 298/1000 | Loss: 0.00002741
Iteration 299/1000 | Loss: 0.00002741
Iteration 300/1000 | Loss: 0.00002741
Iteration 301/1000 | Loss: 0.00002741
Iteration 302/1000 | Loss: 0.00002741
Iteration 303/1000 | Loss: 0.00002741
Iteration 304/1000 | Loss: 0.00002741
Iteration 305/1000 | Loss: 0.00002741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 305. Stopping optimization.
Last 5 losses: [2.7413836505729705e-05, 2.7413836505729705e-05, 2.7413836505729705e-05, 2.7413836505729705e-05, 2.7413836505729705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7413836505729705e-05

Optimization complete. Final v2v error: 4.281117916107178 mm

Highest mean error: 6.864054203033447 mm for frame 41

Lowest mean error: 2.9842114448547363 mm for frame 136

Saving results

Total time: 168.36417055130005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463355
Iteration 2/25 | Loss: 0.00138500
Iteration 3/25 | Loss: 0.00118673
Iteration 4/25 | Loss: 0.00115789
Iteration 5/25 | Loss: 0.00115418
Iteration 6/25 | Loss: 0.00115292
Iteration 7/25 | Loss: 0.00115291
Iteration 8/25 | Loss: 0.00115291
Iteration 9/25 | Loss: 0.00115291
Iteration 10/25 | Loss: 0.00115291
Iteration 11/25 | Loss: 0.00115291
Iteration 12/25 | Loss: 0.00115291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011529078474268317, 0.0011529078474268317, 0.0011529078474268317, 0.0011529078474268317, 0.0011529078474268317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011529078474268317

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37002003
Iteration 2/25 | Loss: 0.00078631
Iteration 3/25 | Loss: 0.00078631
Iteration 4/25 | Loss: 0.00078631
Iteration 5/25 | Loss: 0.00078631
Iteration 6/25 | Loss: 0.00078631
Iteration 7/25 | Loss: 0.00078630
Iteration 8/25 | Loss: 0.00078630
Iteration 9/25 | Loss: 0.00078630
Iteration 10/25 | Loss: 0.00078630
Iteration 11/25 | Loss: 0.00078630
Iteration 12/25 | Loss: 0.00078630
Iteration 13/25 | Loss: 0.00078630
Iteration 14/25 | Loss: 0.00078630
Iteration 15/25 | Loss: 0.00078630
Iteration 16/25 | Loss: 0.00078630
Iteration 17/25 | Loss: 0.00078630
Iteration 18/25 | Loss: 0.00078630
Iteration 19/25 | Loss: 0.00078630
Iteration 20/25 | Loss: 0.00078630
Iteration 21/25 | Loss: 0.00078630
Iteration 22/25 | Loss: 0.00078630
Iteration 23/25 | Loss: 0.00078630
Iteration 24/25 | Loss: 0.00078630
Iteration 25/25 | Loss: 0.00078630

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078630
Iteration 2/1000 | Loss: 0.00002719
Iteration 3/1000 | Loss: 0.00001799
Iteration 4/1000 | Loss: 0.00001438
Iteration 5/1000 | Loss: 0.00001341
Iteration 6/1000 | Loss: 0.00001267
Iteration 7/1000 | Loss: 0.00001225
Iteration 8/1000 | Loss: 0.00001190
Iteration 9/1000 | Loss: 0.00001185
Iteration 10/1000 | Loss: 0.00001165
Iteration 11/1000 | Loss: 0.00001146
Iteration 12/1000 | Loss: 0.00001140
Iteration 13/1000 | Loss: 0.00001136
Iteration 14/1000 | Loss: 0.00001135
Iteration 15/1000 | Loss: 0.00001134
Iteration 16/1000 | Loss: 0.00001133
Iteration 17/1000 | Loss: 0.00001133
Iteration 18/1000 | Loss: 0.00001129
Iteration 19/1000 | Loss: 0.00001127
Iteration 20/1000 | Loss: 0.00001125
Iteration 21/1000 | Loss: 0.00001123
Iteration 22/1000 | Loss: 0.00001123
Iteration 23/1000 | Loss: 0.00001122
Iteration 24/1000 | Loss: 0.00001122
Iteration 25/1000 | Loss: 0.00001120
Iteration 26/1000 | Loss: 0.00001120
Iteration 27/1000 | Loss: 0.00001116
Iteration 28/1000 | Loss: 0.00001116
Iteration 29/1000 | Loss: 0.00001116
Iteration 30/1000 | Loss: 0.00001114
Iteration 31/1000 | Loss: 0.00001114
Iteration 32/1000 | Loss: 0.00001114
Iteration 33/1000 | Loss: 0.00001114
Iteration 34/1000 | Loss: 0.00001114
Iteration 35/1000 | Loss: 0.00001114
Iteration 36/1000 | Loss: 0.00001114
Iteration 37/1000 | Loss: 0.00001113
Iteration 38/1000 | Loss: 0.00001113
Iteration 39/1000 | Loss: 0.00001113
Iteration 40/1000 | Loss: 0.00001113
Iteration 41/1000 | Loss: 0.00001113
Iteration 42/1000 | Loss: 0.00001112
Iteration 43/1000 | Loss: 0.00001112
Iteration 44/1000 | Loss: 0.00001111
Iteration 45/1000 | Loss: 0.00001111
Iteration 46/1000 | Loss: 0.00001111
Iteration 47/1000 | Loss: 0.00001111
Iteration 48/1000 | Loss: 0.00001111
Iteration 49/1000 | Loss: 0.00001111
Iteration 50/1000 | Loss: 0.00001110
Iteration 51/1000 | Loss: 0.00001110
Iteration 52/1000 | Loss: 0.00001110
Iteration 53/1000 | Loss: 0.00001110
Iteration 54/1000 | Loss: 0.00001109
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001108
Iteration 57/1000 | Loss: 0.00001108
Iteration 58/1000 | Loss: 0.00001108
Iteration 59/1000 | Loss: 0.00001107
Iteration 60/1000 | Loss: 0.00001107
Iteration 61/1000 | Loss: 0.00001107
Iteration 62/1000 | Loss: 0.00001107
Iteration 63/1000 | Loss: 0.00001106
Iteration 64/1000 | Loss: 0.00001106
Iteration 65/1000 | Loss: 0.00001106
Iteration 66/1000 | Loss: 0.00001105
Iteration 67/1000 | Loss: 0.00001105
Iteration 68/1000 | Loss: 0.00001104
Iteration 69/1000 | Loss: 0.00001104
Iteration 70/1000 | Loss: 0.00001104
Iteration 71/1000 | Loss: 0.00001103
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001103
Iteration 76/1000 | Loss: 0.00001102
Iteration 77/1000 | Loss: 0.00001102
Iteration 78/1000 | Loss: 0.00001102
Iteration 79/1000 | Loss: 0.00001102
Iteration 80/1000 | Loss: 0.00001101
Iteration 81/1000 | Loss: 0.00001101
Iteration 82/1000 | Loss: 0.00001101
Iteration 83/1000 | Loss: 0.00001101
Iteration 84/1000 | Loss: 0.00001101
Iteration 85/1000 | Loss: 0.00001101
Iteration 86/1000 | Loss: 0.00001100
Iteration 87/1000 | Loss: 0.00001100
Iteration 88/1000 | Loss: 0.00001100
Iteration 89/1000 | Loss: 0.00001100
Iteration 90/1000 | Loss: 0.00001100
Iteration 91/1000 | Loss: 0.00001100
Iteration 92/1000 | Loss: 0.00001099
Iteration 93/1000 | Loss: 0.00001099
Iteration 94/1000 | Loss: 0.00001099
Iteration 95/1000 | Loss: 0.00001099
Iteration 96/1000 | Loss: 0.00001099
Iteration 97/1000 | Loss: 0.00001099
Iteration 98/1000 | Loss: 0.00001099
Iteration 99/1000 | Loss: 0.00001099
Iteration 100/1000 | Loss: 0.00001099
Iteration 101/1000 | Loss: 0.00001099
Iteration 102/1000 | Loss: 0.00001099
Iteration 103/1000 | Loss: 0.00001099
Iteration 104/1000 | Loss: 0.00001099
Iteration 105/1000 | Loss: 0.00001099
Iteration 106/1000 | Loss: 0.00001099
Iteration 107/1000 | Loss: 0.00001098
Iteration 108/1000 | Loss: 0.00001098
Iteration 109/1000 | Loss: 0.00001098
Iteration 110/1000 | Loss: 0.00001098
Iteration 111/1000 | Loss: 0.00001098
Iteration 112/1000 | Loss: 0.00001098
Iteration 113/1000 | Loss: 0.00001098
Iteration 114/1000 | Loss: 0.00001097
Iteration 115/1000 | Loss: 0.00001097
Iteration 116/1000 | Loss: 0.00001097
Iteration 117/1000 | Loss: 0.00001097
Iteration 118/1000 | Loss: 0.00001097
Iteration 119/1000 | Loss: 0.00001096
Iteration 120/1000 | Loss: 0.00001096
Iteration 121/1000 | Loss: 0.00001096
Iteration 122/1000 | Loss: 0.00001096
Iteration 123/1000 | Loss: 0.00001096
Iteration 124/1000 | Loss: 0.00001096
Iteration 125/1000 | Loss: 0.00001096
Iteration 126/1000 | Loss: 0.00001096
Iteration 127/1000 | Loss: 0.00001095
Iteration 128/1000 | Loss: 0.00001095
Iteration 129/1000 | Loss: 0.00001095
Iteration 130/1000 | Loss: 0.00001095
Iteration 131/1000 | Loss: 0.00001094
Iteration 132/1000 | Loss: 0.00001094
Iteration 133/1000 | Loss: 0.00001094
Iteration 134/1000 | Loss: 0.00001094
Iteration 135/1000 | Loss: 0.00001093
Iteration 136/1000 | Loss: 0.00001093
Iteration 137/1000 | Loss: 0.00001093
Iteration 138/1000 | Loss: 0.00001092
Iteration 139/1000 | Loss: 0.00001092
Iteration 140/1000 | Loss: 0.00001092
Iteration 141/1000 | Loss: 0.00001092
Iteration 142/1000 | Loss: 0.00001091
Iteration 143/1000 | Loss: 0.00001091
Iteration 144/1000 | Loss: 0.00001091
Iteration 145/1000 | Loss: 0.00001091
Iteration 146/1000 | Loss: 0.00001091
Iteration 147/1000 | Loss: 0.00001091
Iteration 148/1000 | Loss: 0.00001091
Iteration 149/1000 | Loss: 0.00001091
Iteration 150/1000 | Loss: 0.00001091
Iteration 151/1000 | Loss: 0.00001090
Iteration 152/1000 | Loss: 0.00001090
Iteration 153/1000 | Loss: 0.00001090
Iteration 154/1000 | Loss: 0.00001090
Iteration 155/1000 | Loss: 0.00001089
Iteration 156/1000 | Loss: 0.00001089
Iteration 157/1000 | Loss: 0.00001089
Iteration 158/1000 | Loss: 0.00001089
Iteration 159/1000 | Loss: 0.00001089
Iteration 160/1000 | Loss: 0.00001089
Iteration 161/1000 | Loss: 0.00001089
Iteration 162/1000 | Loss: 0.00001089
Iteration 163/1000 | Loss: 0.00001088
Iteration 164/1000 | Loss: 0.00001088
Iteration 165/1000 | Loss: 0.00001088
Iteration 166/1000 | Loss: 0.00001087
Iteration 167/1000 | Loss: 0.00001087
Iteration 168/1000 | Loss: 0.00001087
Iteration 169/1000 | Loss: 0.00001087
Iteration 170/1000 | Loss: 0.00001087
Iteration 171/1000 | Loss: 0.00001087
Iteration 172/1000 | Loss: 0.00001087
Iteration 173/1000 | Loss: 0.00001087
Iteration 174/1000 | Loss: 0.00001086
Iteration 175/1000 | Loss: 0.00001086
Iteration 176/1000 | Loss: 0.00001086
Iteration 177/1000 | Loss: 0.00001086
Iteration 178/1000 | Loss: 0.00001086
Iteration 179/1000 | Loss: 0.00001086
Iteration 180/1000 | Loss: 0.00001086
Iteration 181/1000 | Loss: 0.00001086
Iteration 182/1000 | Loss: 0.00001085
Iteration 183/1000 | Loss: 0.00001085
Iteration 184/1000 | Loss: 0.00001085
Iteration 185/1000 | Loss: 0.00001085
Iteration 186/1000 | Loss: 0.00001085
Iteration 187/1000 | Loss: 0.00001085
Iteration 188/1000 | Loss: 0.00001085
Iteration 189/1000 | Loss: 0.00001085
Iteration 190/1000 | Loss: 0.00001085
Iteration 191/1000 | Loss: 0.00001084
Iteration 192/1000 | Loss: 0.00001084
Iteration 193/1000 | Loss: 0.00001084
Iteration 194/1000 | Loss: 0.00001084
Iteration 195/1000 | Loss: 0.00001084
Iteration 196/1000 | Loss: 0.00001084
Iteration 197/1000 | Loss: 0.00001084
Iteration 198/1000 | Loss: 0.00001084
Iteration 199/1000 | Loss: 0.00001084
Iteration 200/1000 | Loss: 0.00001084
Iteration 201/1000 | Loss: 0.00001084
Iteration 202/1000 | Loss: 0.00001083
Iteration 203/1000 | Loss: 0.00001083
Iteration 204/1000 | Loss: 0.00001083
Iteration 205/1000 | Loss: 0.00001083
Iteration 206/1000 | Loss: 0.00001083
Iteration 207/1000 | Loss: 0.00001083
Iteration 208/1000 | Loss: 0.00001083
Iteration 209/1000 | Loss: 0.00001083
Iteration 210/1000 | Loss: 0.00001083
Iteration 211/1000 | Loss: 0.00001083
Iteration 212/1000 | Loss: 0.00001082
Iteration 213/1000 | Loss: 0.00001082
Iteration 214/1000 | Loss: 0.00001082
Iteration 215/1000 | Loss: 0.00001082
Iteration 216/1000 | Loss: 0.00001082
Iteration 217/1000 | Loss: 0.00001082
Iteration 218/1000 | Loss: 0.00001082
Iteration 219/1000 | Loss: 0.00001082
Iteration 220/1000 | Loss: 0.00001082
Iteration 221/1000 | Loss: 0.00001082
Iteration 222/1000 | Loss: 0.00001081
Iteration 223/1000 | Loss: 0.00001081
Iteration 224/1000 | Loss: 0.00001081
Iteration 225/1000 | Loss: 0.00001081
Iteration 226/1000 | Loss: 0.00001081
Iteration 227/1000 | Loss: 0.00001081
Iteration 228/1000 | Loss: 0.00001081
Iteration 229/1000 | Loss: 0.00001081
Iteration 230/1000 | Loss: 0.00001080
Iteration 231/1000 | Loss: 0.00001080
Iteration 232/1000 | Loss: 0.00001080
Iteration 233/1000 | Loss: 0.00001080
Iteration 234/1000 | Loss: 0.00001080
Iteration 235/1000 | Loss: 0.00001080
Iteration 236/1000 | Loss: 0.00001080
Iteration 237/1000 | Loss: 0.00001080
Iteration 238/1000 | Loss: 0.00001080
Iteration 239/1000 | Loss: 0.00001080
Iteration 240/1000 | Loss: 0.00001080
Iteration 241/1000 | Loss: 0.00001080
Iteration 242/1000 | Loss: 0.00001079
Iteration 243/1000 | Loss: 0.00001079
Iteration 244/1000 | Loss: 0.00001079
Iteration 245/1000 | Loss: 0.00001079
Iteration 246/1000 | Loss: 0.00001079
Iteration 247/1000 | Loss: 0.00001079
Iteration 248/1000 | Loss: 0.00001079
Iteration 249/1000 | Loss: 0.00001079
Iteration 250/1000 | Loss: 0.00001079
Iteration 251/1000 | Loss: 0.00001079
Iteration 252/1000 | Loss: 0.00001079
Iteration 253/1000 | Loss: 0.00001079
Iteration 254/1000 | Loss: 0.00001079
Iteration 255/1000 | Loss: 0.00001079
Iteration 256/1000 | Loss: 0.00001079
Iteration 257/1000 | Loss: 0.00001079
Iteration 258/1000 | Loss: 0.00001079
Iteration 259/1000 | Loss: 0.00001079
Iteration 260/1000 | Loss: 0.00001079
Iteration 261/1000 | Loss: 0.00001079
Iteration 262/1000 | Loss: 0.00001079
Iteration 263/1000 | Loss: 0.00001079
Iteration 264/1000 | Loss: 0.00001079
Iteration 265/1000 | Loss: 0.00001079
Iteration 266/1000 | Loss: 0.00001079
Iteration 267/1000 | Loss: 0.00001079
Iteration 268/1000 | Loss: 0.00001079
Iteration 269/1000 | Loss: 0.00001079
Iteration 270/1000 | Loss: 0.00001079
Iteration 271/1000 | Loss: 0.00001079
Iteration 272/1000 | Loss: 0.00001079
Iteration 273/1000 | Loss: 0.00001079
Iteration 274/1000 | Loss: 0.00001079
Iteration 275/1000 | Loss: 0.00001079
Iteration 276/1000 | Loss: 0.00001079
Iteration 277/1000 | Loss: 0.00001079
Iteration 278/1000 | Loss: 0.00001079
Iteration 279/1000 | Loss: 0.00001079
Iteration 280/1000 | Loss: 0.00001079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [1.0791447493829764e-05, 1.0791447493829764e-05, 1.0791447493829764e-05, 1.0791447493829764e-05, 1.0791447493829764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0791447493829764e-05

Optimization complete. Final v2v error: 2.762247085571289 mm

Highest mean error: 3.5310540199279785 mm for frame 80

Lowest mean error: 2.449965000152588 mm for frame 163

Saving results

Total time: 43.67111897468567
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406975
Iteration 2/25 | Loss: 0.00122327
Iteration 3/25 | Loss: 0.00112251
Iteration 4/25 | Loss: 0.00111152
Iteration 5/25 | Loss: 0.00110911
Iteration 6/25 | Loss: 0.00110876
Iteration 7/25 | Loss: 0.00110876
Iteration 8/25 | Loss: 0.00110876
Iteration 9/25 | Loss: 0.00110876
Iteration 10/25 | Loss: 0.00110876
Iteration 11/25 | Loss: 0.00110876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011087582679465413, 0.0011087582679465413, 0.0011087582679465413, 0.0011087582679465413, 0.0011087582679465413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011087582679465413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37045360
Iteration 2/25 | Loss: 0.00090775
Iteration 3/25 | Loss: 0.00090774
Iteration 4/25 | Loss: 0.00090774
Iteration 5/25 | Loss: 0.00090774
Iteration 6/25 | Loss: 0.00090774
Iteration 7/25 | Loss: 0.00090774
Iteration 8/25 | Loss: 0.00090774
Iteration 9/25 | Loss: 0.00090774
Iteration 10/25 | Loss: 0.00090774
Iteration 11/25 | Loss: 0.00090774
Iteration 12/25 | Loss: 0.00090774
Iteration 13/25 | Loss: 0.00090774
Iteration 14/25 | Loss: 0.00090774
Iteration 15/25 | Loss: 0.00090774
Iteration 16/25 | Loss: 0.00090774
Iteration 17/25 | Loss: 0.00090774
Iteration 18/25 | Loss: 0.00090774
Iteration 19/25 | Loss: 0.00090774
Iteration 20/25 | Loss: 0.00090774
Iteration 21/25 | Loss: 0.00090774
Iteration 22/25 | Loss: 0.00090774
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009077399154193699, 0.0009077399154193699, 0.0009077399154193699, 0.0009077399154193699, 0.0009077399154193699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009077399154193699

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090774
Iteration 2/1000 | Loss: 0.00002552
Iteration 3/1000 | Loss: 0.00001592
Iteration 4/1000 | Loss: 0.00001254
Iteration 5/1000 | Loss: 0.00001138
Iteration 6/1000 | Loss: 0.00001064
Iteration 7/1000 | Loss: 0.00001019
Iteration 8/1000 | Loss: 0.00000990
Iteration 9/1000 | Loss: 0.00000980
Iteration 10/1000 | Loss: 0.00000972
Iteration 11/1000 | Loss: 0.00000953
Iteration 12/1000 | Loss: 0.00000949
Iteration 13/1000 | Loss: 0.00000948
Iteration 14/1000 | Loss: 0.00000947
Iteration 15/1000 | Loss: 0.00000946
Iteration 16/1000 | Loss: 0.00000941
Iteration 17/1000 | Loss: 0.00000936
Iteration 18/1000 | Loss: 0.00000936
Iteration 19/1000 | Loss: 0.00000932
Iteration 20/1000 | Loss: 0.00000930
Iteration 21/1000 | Loss: 0.00000923
Iteration 22/1000 | Loss: 0.00000923
Iteration 23/1000 | Loss: 0.00000922
Iteration 24/1000 | Loss: 0.00000922
Iteration 25/1000 | Loss: 0.00000921
Iteration 26/1000 | Loss: 0.00000920
Iteration 27/1000 | Loss: 0.00000920
Iteration 28/1000 | Loss: 0.00000920
Iteration 29/1000 | Loss: 0.00000919
Iteration 30/1000 | Loss: 0.00000918
Iteration 31/1000 | Loss: 0.00000918
Iteration 32/1000 | Loss: 0.00000918
Iteration 33/1000 | Loss: 0.00000918
Iteration 34/1000 | Loss: 0.00000918
Iteration 35/1000 | Loss: 0.00000917
Iteration 36/1000 | Loss: 0.00000917
Iteration 37/1000 | Loss: 0.00000917
Iteration 38/1000 | Loss: 0.00000917
Iteration 39/1000 | Loss: 0.00000917
Iteration 40/1000 | Loss: 0.00000917
Iteration 41/1000 | Loss: 0.00000917
Iteration 42/1000 | Loss: 0.00000917
Iteration 43/1000 | Loss: 0.00000917
Iteration 44/1000 | Loss: 0.00000917
Iteration 45/1000 | Loss: 0.00000916
Iteration 46/1000 | Loss: 0.00000916
Iteration 47/1000 | Loss: 0.00000916
Iteration 48/1000 | Loss: 0.00000916
Iteration 49/1000 | Loss: 0.00000916
Iteration 50/1000 | Loss: 0.00000916
Iteration 51/1000 | Loss: 0.00000916
Iteration 52/1000 | Loss: 0.00000916
Iteration 53/1000 | Loss: 0.00000916
Iteration 54/1000 | Loss: 0.00000916
Iteration 55/1000 | Loss: 0.00000916
Iteration 56/1000 | Loss: 0.00000916
Iteration 57/1000 | Loss: 0.00000916
Iteration 58/1000 | Loss: 0.00000916
Iteration 59/1000 | Loss: 0.00000916
Iteration 60/1000 | Loss: 0.00000916
Iteration 61/1000 | Loss: 0.00000916
Iteration 62/1000 | Loss: 0.00000916
Iteration 63/1000 | Loss: 0.00000916
Iteration 64/1000 | Loss: 0.00000915
Iteration 65/1000 | Loss: 0.00000915
Iteration 66/1000 | Loss: 0.00000915
Iteration 67/1000 | Loss: 0.00000915
Iteration 68/1000 | Loss: 0.00000914
Iteration 69/1000 | Loss: 0.00000914
Iteration 70/1000 | Loss: 0.00000913
Iteration 71/1000 | Loss: 0.00000913
Iteration 72/1000 | Loss: 0.00000913
Iteration 73/1000 | Loss: 0.00000912
Iteration 74/1000 | Loss: 0.00000912
Iteration 75/1000 | Loss: 0.00000912
Iteration 76/1000 | Loss: 0.00000911
Iteration 77/1000 | Loss: 0.00000911
Iteration 78/1000 | Loss: 0.00000910
Iteration 79/1000 | Loss: 0.00000910
Iteration 80/1000 | Loss: 0.00000910
Iteration 81/1000 | Loss: 0.00000909
Iteration 82/1000 | Loss: 0.00000909
Iteration 83/1000 | Loss: 0.00000908
Iteration 84/1000 | Loss: 0.00000907
Iteration 85/1000 | Loss: 0.00000907
Iteration 86/1000 | Loss: 0.00000906
Iteration 87/1000 | Loss: 0.00000906
Iteration 88/1000 | Loss: 0.00000906
Iteration 89/1000 | Loss: 0.00000905
Iteration 90/1000 | Loss: 0.00000905
Iteration 91/1000 | Loss: 0.00000905
Iteration 92/1000 | Loss: 0.00000905
Iteration 93/1000 | Loss: 0.00000905
Iteration 94/1000 | Loss: 0.00000905
Iteration 95/1000 | Loss: 0.00000904
Iteration 96/1000 | Loss: 0.00000904
Iteration 97/1000 | Loss: 0.00000904
Iteration 98/1000 | Loss: 0.00000904
Iteration 99/1000 | Loss: 0.00000903
Iteration 100/1000 | Loss: 0.00000903
Iteration 101/1000 | Loss: 0.00000902
Iteration 102/1000 | Loss: 0.00000902
Iteration 103/1000 | Loss: 0.00000902
Iteration 104/1000 | Loss: 0.00000902
Iteration 105/1000 | Loss: 0.00000901
Iteration 106/1000 | Loss: 0.00000901
Iteration 107/1000 | Loss: 0.00000901
Iteration 108/1000 | Loss: 0.00000901
Iteration 109/1000 | Loss: 0.00000900
Iteration 110/1000 | Loss: 0.00000900
Iteration 111/1000 | Loss: 0.00000900
Iteration 112/1000 | Loss: 0.00000900
Iteration 113/1000 | Loss: 0.00000900
Iteration 114/1000 | Loss: 0.00000900
Iteration 115/1000 | Loss: 0.00000900
Iteration 116/1000 | Loss: 0.00000900
Iteration 117/1000 | Loss: 0.00000900
Iteration 118/1000 | Loss: 0.00000900
Iteration 119/1000 | Loss: 0.00000899
Iteration 120/1000 | Loss: 0.00000899
Iteration 121/1000 | Loss: 0.00000899
Iteration 122/1000 | Loss: 0.00000899
Iteration 123/1000 | Loss: 0.00000899
Iteration 124/1000 | Loss: 0.00000898
Iteration 125/1000 | Loss: 0.00000898
Iteration 126/1000 | Loss: 0.00000898
Iteration 127/1000 | Loss: 0.00000898
Iteration 128/1000 | Loss: 0.00000898
Iteration 129/1000 | Loss: 0.00000898
Iteration 130/1000 | Loss: 0.00000898
Iteration 131/1000 | Loss: 0.00000897
Iteration 132/1000 | Loss: 0.00000897
Iteration 133/1000 | Loss: 0.00000897
Iteration 134/1000 | Loss: 0.00000897
Iteration 135/1000 | Loss: 0.00000897
Iteration 136/1000 | Loss: 0.00000896
Iteration 137/1000 | Loss: 0.00000896
Iteration 138/1000 | Loss: 0.00000896
Iteration 139/1000 | Loss: 0.00000896
Iteration 140/1000 | Loss: 0.00000896
Iteration 141/1000 | Loss: 0.00000896
Iteration 142/1000 | Loss: 0.00000895
Iteration 143/1000 | Loss: 0.00000895
Iteration 144/1000 | Loss: 0.00000895
Iteration 145/1000 | Loss: 0.00000895
Iteration 146/1000 | Loss: 0.00000895
Iteration 147/1000 | Loss: 0.00000894
Iteration 148/1000 | Loss: 0.00000894
Iteration 149/1000 | Loss: 0.00000894
Iteration 150/1000 | Loss: 0.00000894
Iteration 151/1000 | Loss: 0.00000894
Iteration 152/1000 | Loss: 0.00000893
Iteration 153/1000 | Loss: 0.00000893
Iteration 154/1000 | Loss: 0.00000893
Iteration 155/1000 | Loss: 0.00000893
Iteration 156/1000 | Loss: 0.00000893
Iteration 157/1000 | Loss: 0.00000893
Iteration 158/1000 | Loss: 0.00000893
Iteration 159/1000 | Loss: 0.00000892
Iteration 160/1000 | Loss: 0.00000892
Iteration 161/1000 | Loss: 0.00000892
Iteration 162/1000 | Loss: 0.00000892
Iteration 163/1000 | Loss: 0.00000892
Iteration 164/1000 | Loss: 0.00000892
Iteration 165/1000 | Loss: 0.00000892
Iteration 166/1000 | Loss: 0.00000892
Iteration 167/1000 | Loss: 0.00000892
Iteration 168/1000 | Loss: 0.00000892
Iteration 169/1000 | Loss: 0.00000891
Iteration 170/1000 | Loss: 0.00000891
Iteration 171/1000 | Loss: 0.00000891
Iteration 172/1000 | Loss: 0.00000891
Iteration 173/1000 | Loss: 0.00000891
Iteration 174/1000 | Loss: 0.00000891
Iteration 175/1000 | Loss: 0.00000891
Iteration 176/1000 | Loss: 0.00000891
Iteration 177/1000 | Loss: 0.00000891
Iteration 178/1000 | Loss: 0.00000890
Iteration 179/1000 | Loss: 0.00000890
Iteration 180/1000 | Loss: 0.00000890
Iteration 181/1000 | Loss: 0.00000890
Iteration 182/1000 | Loss: 0.00000890
Iteration 183/1000 | Loss: 0.00000890
Iteration 184/1000 | Loss: 0.00000890
Iteration 185/1000 | Loss: 0.00000890
Iteration 186/1000 | Loss: 0.00000890
Iteration 187/1000 | Loss: 0.00000890
Iteration 188/1000 | Loss: 0.00000890
Iteration 189/1000 | Loss: 0.00000890
Iteration 190/1000 | Loss: 0.00000889
Iteration 191/1000 | Loss: 0.00000889
Iteration 192/1000 | Loss: 0.00000889
Iteration 193/1000 | Loss: 0.00000889
Iteration 194/1000 | Loss: 0.00000888
Iteration 195/1000 | Loss: 0.00000888
Iteration 196/1000 | Loss: 0.00000888
Iteration 197/1000 | Loss: 0.00000888
Iteration 198/1000 | Loss: 0.00000888
Iteration 199/1000 | Loss: 0.00000888
Iteration 200/1000 | Loss: 0.00000887
Iteration 201/1000 | Loss: 0.00000887
Iteration 202/1000 | Loss: 0.00000887
Iteration 203/1000 | Loss: 0.00000887
Iteration 204/1000 | Loss: 0.00000887
Iteration 205/1000 | Loss: 0.00000887
Iteration 206/1000 | Loss: 0.00000887
Iteration 207/1000 | Loss: 0.00000887
Iteration 208/1000 | Loss: 0.00000887
Iteration 209/1000 | Loss: 0.00000887
Iteration 210/1000 | Loss: 0.00000887
Iteration 211/1000 | Loss: 0.00000887
Iteration 212/1000 | Loss: 0.00000887
Iteration 213/1000 | Loss: 0.00000887
Iteration 214/1000 | Loss: 0.00000887
Iteration 215/1000 | Loss: 0.00000887
Iteration 216/1000 | Loss: 0.00000887
Iteration 217/1000 | Loss: 0.00000887
Iteration 218/1000 | Loss: 0.00000887
Iteration 219/1000 | Loss: 0.00000887
Iteration 220/1000 | Loss: 0.00000887
Iteration 221/1000 | Loss: 0.00000887
Iteration 222/1000 | Loss: 0.00000887
Iteration 223/1000 | Loss: 0.00000887
Iteration 224/1000 | Loss: 0.00000887
Iteration 225/1000 | Loss: 0.00000887
Iteration 226/1000 | Loss: 0.00000887
Iteration 227/1000 | Loss: 0.00000887
Iteration 228/1000 | Loss: 0.00000887
Iteration 229/1000 | Loss: 0.00000887
Iteration 230/1000 | Loss: 0.00000887
Iteration 231/1000 | Loss: 0.00000887
Iteration 232/1000 | Loss: 0.00000887
Iteration 233/1000 | Loss: 0.00000887
Iteration 234/1000 | Loss: 0.00000887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [8.866325515555218e-06, 8.866325515555218e-06, 8.866325515555218e-06, 8.866325515555218e-06, 8.866325515555218e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.866325515555218e-06

Optimization complete. Final v2v error: 2.5422277450561523 mm

Highest mean error: 3.4446511268615723 mm for frame 54

Lowest mean error: 2.373790740966797 mm for frame 156

Saving results

Total time: 40.85233283042908
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051675
Iteration 2/25 | Loss: 0.00577300
Iteration 3/25 | Loss: 0.00382057
Iteration 4/25 | Loss: 0.00408973
Iteration 5/25 | Loss: 0.00376529
Iteration 6/25 | Loss: 0.00272845
Iteration 7/25 | Loss: 0.00258022
Iteration 8/25 | Loss: 0.00248030
Iteration 9/25 | Loss: 0.00239864
Iteration 10/25 | Loss: 0.00231127
Iteration 11/25 | Loss: 0.00253933
Iteration 12/25 | Loss: 0.00245279
Iteration 13/25 | Loss: 0.00232414
Iteration 14/25 | Loss: 0.00218105
Iteration 15/25 | Loss: 0.00212348
Iteration 16/25 | Loss: 0.00205765
Iteration 17/25 | Loss: 0.00195028
Iteration 18/25 | Loss: 0.00189177
Iteration 19/25 | Loss: 0.00187468
Iteration 20/25 | Loss: 0.00185737
Iteration 21/25 | Loss: 0.00184175
Iteration 22/25 | Loss: 0.00183238
Iteration 23/25 | Loss: 0.00181582
Iteration 24/25 | Loss: 0.00179857
Iteration 25/25 | Loss: 0.00179943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.42664054
Iteration 2/25 | Loss: 0.02477286
Iteration 3/25 | Loss: 0.00530402
Iteration 4/25 | Loss: 0.00530396
Iteration 5/25 | Loss: 0.00530396
Iteration 6/25 | Loss: 0.00530396
Iteration 7/25 | Loss: 0.00530396
Iteration 8/25 | Loss: 0.00530396
Iteration 9/25 | Loss: 0.00530396
Iteration 10/25 | Loss: 0.00530396
Iteration 11/25 | Loss: 0.00530395
Iteration 12/25 | Loss: 0.00530395
Iteration 13/25 | Loss: 0.00530395
Iteration 14/25 | Loss: 0.00530395
Iteration 15/25 | Loss: 0.00530395
Iteration 16/25 | Loss: 0.00530395
Iteration 17/25 | Loss: 0.00530395
Iteration 18/25 | Loss: 0.00530395
Iteration 19/25 | Loss: 0.00530395
Iteration 20/25 | Loss: 0.00530395
Iteration 21/25 | Loss: 0.00530395
Iteration 22/25 | Loss: 0.00530395
Iteration 23/25 | Loss: 0.00530395
Iteration 24/25 | Loss: 0.00530395
Iteration 25/25 | Loss: 0.00530395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00530395
Iteration 2/1000 | Loss: 0.00164831
Iteration 3/1000 | Loss: 0.00043172
Iteration 4/1000 | Loss: 0.00066219
Iteration 5/1000 | Loss: 0.00089326
Iteration 6/1000 | Loss: 0.00023226
Iteration 7/1000 | Loss: 0.00049970
Iteration 8/1000 | Loss: 0.00040739
Iteration 9/1000 | Loss: 0.00071133
Iteration 10/1000 | Loss: 0.00030838
Iteration 11/1000 | Loss: 0.00026596
Iteration 12/1000 | Loss: 0.00025248
Iteration 13/1000 | Loss: 0.00017806
Iteration 14/1000 | Loss: 0.00059377
Iteration 15/1000 | Loss: 0.00052987
Iteration 16/1000 | Loss: 0.00031262
Iteration 17/1000 | Loss: 0.00066285
Iteration 18/1000 | Loss: 0.00095568
Iteration 19/1000 | Loss: 0.00092086
Iteration 20/1000 | Loss: 0.00049785
Iteration 21/1000 | Loss: 0.00085784
Iteration 22/1000 | Loss: 0.00052154
Iteration 23/1000 | Loss: 0.00047190
Iteration 24/1000 | Loss: 0.00041451
Iteration 25/1000 | Loss: 0.00029429
Iteration 26/1000 | Loss: 0.00089992
Iteration 27/1000 | Loss: 0.00056131
Iteration 28/1000 | Loss: 0.00062250
Iteration 29/1000 | Loss: 0.00064250
Iteration 30/1000 | Loss: 0.00058301
Iteration 31/1000 | Loss: 0.00061493
Iteration 32/1000 | Loss: 0.00046530
Iteration 33/1000 | Loss: 0.00069143
Iteration 34/1000 | Loss: 0.00073188
Iteration 35/1000 | Loss: 0.00044165
Iteration 36/1000 | Loss: 0.00049842
Iteration 37/1000 | Loss: 0.00049599
Iteration 38/1000 | Loss: 0.00035268
Iteration 39/1000 | Loss: 0.00026631
Iteration 40/1000 | Loss: 0.00017326
Iteration 41/1000 | Loss: 0.00016372
Iteration 42/1000 | Loss: 0.00017051
Iteration 43/1000 | Loss: 0.00016947
Iteration 44/1000 | Loss: 0.00015937
Iteration 45/1000 | Loss: 0.00015545
Iteration 46/1000 | Loss: 0.00028202
Iteration 47/1000 | Loss: 0.00029244
Iteration 48/1000 | Loss: 0.00027629
Iteration 49/1000 | Loss: 0.00035405
Iteration 50/1000 | Loss: 0.00031345
Iteration 51/1000 | Loss: 0.00024811
Iteration 52/1000 | Loss: 0.00016613
Iteration 53/1000 | Loss: 0.00039721
Iteration 54/1000 | Loss: 0.00023780
Iteration 55/1000 | Loss: 0.00014387
Iteration 56/1000 | Loss: 0.00016229
Iteration 57/1000 | Loss: 0.00028265
Iteration 58/1000 | Loss: 0.00014189
Iteration 59/1000 | Loss: 0.00014873
Iteration 60/1000 | Loss: 0.00015847
Iteration 61/1000 | Loss: 0.00015117
Iteration 62/1000 | Loss: 0.00041019
Iteration 63/1000 | Loss: 0.00033108
Iteration 64/1000 | Loss: 0.00014822
Iteration 65/1000 | Loss: 0.00016401
Iteration 66/1000 | Loss: 0.00035473
Iteration 67/1000 | Loss: 0.00016082
Iteration 68/1000 | Loss: 0.00015220
Iteration 69/1000 | Loss: 0.00016237
Iteration 70/1000 | Loss: 0.00019294
Iteration 71/1000 | Loss: 0.00015254
Iteration 72/1000 | Loss: 0.00015774
Iteration 73/1000 | Loss: 0.00015354
Iteration 74/1000 | Loss: 0.00015391
Iteration 75/1000 | Loss: 0.00014714
Iteration 76/1000 | Loss: 0.00014819
Iteration 77/1000 | Loss: 0.00013956
Iteration 78/1000 | Loss: 0.00014770
Iteration 79/1000 | Loss: 0.00037965
Iteration 80/1000 | Loss: 0.00033625
Iteration 81/1000 | Loss: 0.00051687
Iteration 82/1000 | Loss: 0.00031276
Iteration 83/1000 | Loss: 0.00048293
Iteration 84/1000 | Loss: 0.00028912
Iteration 85/1000 | Loss: 0.00015913
Iteration 86/1000 | Loss: 0.00027897
Iteration 87/1000 | Loss: 0.00027621
Iteration 88/1000 | Loss: 0.00029433
Iteration 89/1000 | Loss: 0.00028356
Iteration 90/1000 | Loss: 0.00058400
Iteration 91/1000 | Loss: 0.00110379
Iteration 92/1000 | Loss: 0.00062405
Iteration 93/1000 | Loss: 0.00035002
Iteration 94/1000 | Loss: 0.00038535
Iteration 95/1000 | Loss: 0.00050745
Iteration 96/1000 | Loss: 0.00014119
Iteration 97/1000 | Loss: 0.00014027
Iteration 98/1000 | Loss: 0.00059952
Iteration 99/1000 | Loss: 0.00030615
Iteration 100/1000 | Loss: 0.00015900
Iteration 101/1000 | Loss: 0.00014307
Iteration 102/1000 | Loss: 0.00012518
Iteration 103/1000 | Loss: 0.00012731
Iteration 104/1000 | Loss: 0.00025710
Iteration 105/1000 | Loss: 0.00047820
Iteration 106/1000 | Loss: 0.00034427
Iteration 107/1000 | Loss: 0.00018409
Iteration 108/1000 | Loss: 0.00013455
Iteration 109/1000 | Loss: 0.00013600
Iteration 110/1000 | Loss: 0.00014665
Iteration 111/1000 | Loss: 0.00012639
Iteration 112/1000 | Loss: 0.00012022
Iteration 113/1000 | Loss: 0.00012211
Iteration 114/1000 | Loss: 0.00022521
Iteration 115/1000 | Loss: 0.00037228
Iteration 116/1000 | Loss: 0.00035008
Iteration 117/1000 | Loss: 0.00035253
Iteration 118/1000 | Loss: 0.00024849
Iteration 119/1000 | Loss: 0.00013250
Iteration 120/1000 | Loss: 0.00028749
Iteration 121/1000 | Loss: 0.00026290
Iteration 122/1000 | Loss: 0.00013876
Iteration 123/1000 | Loss: 0.00013795
Iteration 124/1000 | Loss: 0.00011362
Iteration 125/1000 | Loss: 0.00012631
Iteration 126/1000 | Loss: 0.00012444
Iteration 127/1000 | Loss: 0.00027836
Iteration 128/1000 | Loss: 0.00023921
Iteration 129/1000 | Loss: 0.00028044
Iteration 130/1000 | Loss: 0.00023749
Iteration 131/1000 | Loss: 0.00026515
Iteration 132/1000 | Loss: 0.00021221
Iteration 133/1000 | Loss: 0.00013368
Iteration 134/1000 | Loss: 0.00013232
Iteration 135/1000 | Loss: 0.00016547
Iteration 136/1000 | Loss: 0.00011398
Iteration 137/1000 | Loss: 0.00012824
Iteration 138/1000 | Loss: 0.00012351
Iteration 139/1000 | Loss: 0.00029567
Iteration 140/1000 | Loss: 0.00027209
Iteration 141/1000 | Loss: 0.00031245
Iteration 142/1000 | Loss: 0.00029000
Iteration 143/1000 | Loss: 0.00025774
Iteration 144/1000 | Loss: 0.00012585
Iteration 145/1000 | Loss: 0.00012888
Iteration 146/1000 | Loss: 0.00013172
Iteration 147/1000 | Loss: 0.00012525
Iteration 148/1000 | Loss: 0.00012541
Iteration 149/1000 | Loss: 0.00013348
Iteration 150/1000 | Loss: 0.00028807
Iteration 151/1000 | Loss: 0.00020234
Iteration 152/1000 | Loss: 0.00026207
Iteration 153/1000 | Loss: 0.00021305
Iteration 154/1000 | Loss: 0.00011328
Iteration 155/1000 | Loss: 0.00010661
Iteration 156/1000 | Loss: 0.00010759
Iteration 157/1000 | Loss: 0.00029174
Iteration 158/1000 | Loss: 0.00024917
Iteration 159/1000 | Loss: 0.00025799
Iteration 160/1000 | Loss: 0.00026675
Iteration 161/1000 | Loss: 0.00025526
Iteration 162/1000 | Loss: 0.00044092
Iteration 163/1000 | Loss: 0.00033951
Iteration 164/1000 | Loss: 0.00058106
Iteration 165/1000 | Loss: 0.00036227
Iteration 166/1000 | Loss: 0.00016317
Iteration 167/1000 | Loss: 0.00035165
Iteration 168/1000 | Loss: 0.00034349
Iteration 169/1000 | Loss: 0.00035586
Iteration 170/1000 | Loss: 0.00027978
Iteration 171/1000 | Loss: 0.00031408
Iteration 172/1000 | Loss: 0.00022874
Iteration 173/1000 | Loss: 0.00026553
Iteration 174/1000 | Loss: 0.00014386
Iteration 175/1000 | Loss: 0.00013431
Iteration 176/1000 | Loss: 0.00031290
Iteration 177/1000 | Loss: 0.00024988
Iteration 178/1000 | Loss: 0.00026020
Iteration 179/1000 | Loss: 0.00018964
Iteration 180/1000 | Loss: 0.00020130
Iteration 181/1000 | Loss: 0.00017115
Iteration 182/1000 | Loss: 0.00018713
Iteration 183/1000 | Loss: 0.00021211
Iteration 184/1000 | Loss: 0.00015142
Iteration 185/1000 | Loss: 0.00012361
Iteration 186/1000 | Loss: 0.00020017
Iteration 187/1000 | Loss: 0.00020704
Iteration 188/1000 | Loss: 0.00020829
Iteration 189/1000 | Loss: 0.00017628
Iteration 190/1000 | Loss: 0.00012306
Iteration 191/1000 | Loss: 0.00023055
Iteration 192/1000 | Loss: 0.00010999
Iteration 193/1000 | Loss: 0.00013202
Iteration 194/1000 | Loss: 0.00011589
Iteration 195/1000 | Loss: 0.00010970
Iteration 196/1000 | Loss: 0.00011593
Iteration 197/1000 | Loss: 0.00010888
Iteration 198/1000 | Loss: 0.00025725
Iteration 199/1000 | Loss: 0.00015787
Iteration 200/1000 | Loss: 0.00012718
Iteration 201/1000 | Loss: 0.00011794
Iteration 202/1000 | Loss: 0.00012547
Iteration 203/1000 | Loss: 0.00011283
Iteration 204/1000 | Loss: 0.00013350
Iteration 205/1000 | Loss: 0.00013928
Iteration 206/1000 | Loss: 0.00020546
Iteration 207/1000 | Loss: 0.00017973
Iteration 208/1000 | Loss: 0.00012020
Iteration 209/1000 | Loss: 0.00011294
Iteration 210/1000 | Loss: 0.00011435
Iteration 211/1000 | Loss: 0.00011235
Iteration 212/1000 | Loss: 0.00011228
Iteration 213/1000 | Loss: 0.00011248
Iteration 214/1000 | Loss: 0.00011220
Iteration 215/1000 | Loss: 0.00011220
Iteration 216/1000 | Loss: 0.00011781
Iteration 217/1000 | Loss: 0.00011730
Iteration 218/1000 | Loss: 0.00010898
Iteration 219/1000 | Loss: 0.00011275
Iteration 220/1000 | Loss: 0.00010250
Iteration 221/1000 | Loss: 0.00010917
Iteration 222/1000 | Loss: 0.00010742
Iteration 223/1000 | Loss: 0.00011345
Iteration 224/1000 | Loss: 0.00011325
Iteration 225/1000 | Loss: 0.00010029
Iteration 226/1000 | Loss: 0.00011707
Iteration 227/1000 | Loss: 0.00010848
Iteration 228/1000 | Loss: 0.00025504
Iteration 229/1000 | Loss: 0.00022970
Iteration 230/1000 | Loss: 0.00031493
Iteration 231/1000 | Loss: 0.00020982
Iteration 232/1000 | Loss: 0.00023218
Iteration 233/1000 | Loss: 0.00021096
Iteration 234/1000 | Loss: 0.00019716
Iteration 235/1000 | Loss: 0.00012394
Iteration 236/1000 | Loss: 0.00009940
Iteration 237/1000 | Loss: 0.00009755
Iteration 238/1000 | Loss: 0.00009663
Iteration 239/1000 | Loss: 0.00009590
Iteration 240/1000 | Loss: 0.00009527
Iteration 241/1000 | Loss: 0.00010685
Iteration 242/1000 | Loss: 0.00009467
Iteration 243/1000 | Loss: 0.00009459
Iteration 244/1000 | Loss: 0.00009446
Iteration 245/1000 | Loss: 0.00009425
Iteration 246/1000 | Loss: 0.00011245
Iteration 247/1000 | Loss: 0.00009397
Iteration 248/1000 | Loss: 0.00009383
Iteration 249/1000 | Loss: 0.00018035
Iteration 250/1000 | Loss: 0.00018477
Iteration 251/1000 | Loss: 0.00009764
Iteration 252/1000 | Loss: 0.00009355
Iteration 253/1000 | Loss: 0.00010491
Iteration 254/1000 | Loss: 0.00019041
Iteration 255/1000 | Loss: 0.00017302
Iteration 256/1000 | Loss: 0.00018167
Iteration 257/1000 | Loss: 0.00012552
Iteration 258/1000 | Loss: 0.00009758
Iteration 259/1000 | Loss: 0.00010231
Iteration 260/1000 | Loss: 0.00009625
Iteration 261/1000 | Loss: 0.00017107
Iteration 262/1000 | Loss: 0.00010061
Iteration 263/1000 | Loss: 0.00009753
Iteration 264/1000 | Loss: 0.00009926
Iteration 265/1000 | Loss: 0.00009598
Iteration 266/1000 | Loss: 0.00009671
Iteration 267/1000 | Loss: 0.00009415
Iteration 268/1000 | Loss: 0.00009893
Iteration 269/1000 | Loss: 0.00009388
Iteration 270/1000 | Loss: 0.00009384
Iteration 271/1000 | Loss: 0.00009383
Iteration 272/1000 | Loss: 0.00009381
Iteration 273/1000 | Loss: 0.00009381
Iteration 274/1000 | Loss: 0.00009380
Iteration 275/1000 | Loss: 0.00009380
Iteration 276/1000 | Loss: 0.00009380
Iteration 277/1000 | Loss: 0.00009380
Iteration 278/1000 | Loss: 0.00009380
Iteration 279/1000 | Loss: 0.00009380
Iteration 280/1000 | Loss: 0.00009379
Iteration 281/1000 | Loss: 0.00009379
Iteration 282/1000 | Loss: 0.00009379
Iteration 283/1000 | Loss: 0.00009379
Iteration 284/1000 | Loss: 0.00009379
Iteration 285/1000 | Loss: 0.00009378
Iteration 286/1000 | Loss: 0.00009376
Iteration 287/1000 | Loss: 0.00009372
Iteration 288/1000 | Loss: 0.00009371
Iteration 289/1000 | Loss: 0.00009370
Iteration 290/1000 | Loss: 0.00009368
Iteration 291/1000 | Loss: 0.00009368
Iteration 292/1000 | Loss: 0.00009368
Iteration 293/1000 | Loss: 0.00009368
Iteration 294/1000 | Loss: 0.00009368
Iteration 295/1000 | Loss: 0.00009368
Iteration 296/1000 | Loss: 0.00009368
Iteration 297/1000 | Loss: 0.00009368
Iteration 298/1000 | Loss: 0.00009368
Iteration 299/1000 | Loss: 0.00009368
Iteration 300/1000 | Loss: 0.00009368
Iteration 301/1000 | Loss: 0.00009368
Iteration 302/1000 | Loss: 0.00009368
Iteration 303/1000 | Loss: 0.00009368
Iteration 304/1000 | Loss: 0.00009368
Iteration 305/1000 | Loss: 0.00010905
Iteration 306/1000 | Loss: 0.00026098
Iteration 307/1000 | Loss: 0.00512936
Iteration 308/1000 | Loss: 0.00278158
Iteration 309/1000 | Loss: 0.00135064
Iteration 310/1000 | Loss: 0.00061575
Iteration 311/1000 | Loss: 0.00108845
Iteration 312/1000 | Loss: 0.00110635
Iteration 313/1000 | Loss: 0.00088836
Iteration 314/1000 | Loss: 0.00049120
Iteration 315/1000 | Loss: 0.00035364
Iteration 316/1000 | Loss: 0.00017718
Iteration 317/1000 | Loss: 0.00152108
Iteration 318/1000 | Loss: 0.00062006
Iteration 319/1000 | Loss: 0.00091252
Iteration 320/1000 | Loss: 0.00066210
Iteration 321/1000 | Loss: 0.00019662
Iteration 322/1000 | Loss: 0.00022169
Iteration 323/1000 | Loss: 0.00022215
Iteration 324/1000 | Loss: 0.00013224
Iteration 325/1000 | Loss: 0.00151274
Iteration 326/1000 | Loss: 0.00050097
Iteration 327/1000 | Loss: 0.00087825
Iteration 328/1000 | Loss: 0.00101050
Iteration 329/1000 | Loss: 0.00085709
Iteration 330/1000 | Loss: 0.00128125
Iteration 331/1000 | Loss: 0.00048739
Iteration 332/1000 | Loss: 0.00143093
Iteration 333/1000 | Loss: 0.00028073
Iteration 334/1000 | Loss: 0.00063950
Iteration 335/1000 | Loss: 0.00034401
Iteration 336/1000 | Loss: 0.00021268
Iteration 337/1000 | Loss: 0.00013578
Iteration 338/1000 | Loss: 0.00068505
Iteration 339/1000 | Loss: 0.00018113
Iteration 340/1000 | Loss: 0.00059563
Iteration 341/1000 | Loss: 0.00020071
Iteration 342/1000 | Loss: 0.00021639
Iteration 343/1000 | Loss: 0.00011957
Iteration 344/1000 | Loss: 0.00030200
Iteration 345/1000 | Loss: 0.00021606
Iteration 346/1000 | Loss: 0.00011364
Iteration 347/1000 | Loss: 0.00016081
Iteration 348/1000 | Loss: 0.00014319
Iteration 349/1000 | Loss: 0.00011238
Iteration 350/1000 | Loss: 0.00016299
Iteration 351/1000 | Loss: 0.00034313
Iteration 352/1000 | Loss: 0.00028844
Iteration 353/1000 | Loss: 0.00086621
Iteration 354/1000 | Loss: 0.00070432
Iteration 355/1000 | Loss: 0.00018951
Iteration 356/1000 | Loss: 0.00023530
Iteration 357/1000 | Loss: 0.00082975
Iteration 358/1000 | Loss: 0.00127221
Iteration 359/1000 | Loss: 0.00106111
Iteration 360/1000 | Loss: 0.00027500
Iteration 361/1000 | Loss: 0.00120893
Iteration 362/1000 | Loss: 0.00011156
Iteration 363/1000 | Loss: 0.00010521
Iteration 364/1000 | Loss: 0.00010281
Iteration 365/1000 | Loss: 0.00010080
Iteration 366/1000 | Loss: 0.00028920
Iteration 367/1000 | Loss: 0.00159876
Iteration 368/1000 | Loss: 0.00013331
Iteration 369/1000 | Loss: 0.00011222
Iteration 370/1000 | Loss: 0.00024826
Iteration 371/1000 | Loss: 0.00025129
Iteration 372/1000 | Loss: 0.00021217
Iteration 373/1000 | Loss: 0.00020424
Iteration 374/1000 | Loss: 0.00028435
Iteration 375/1000 | Loss: 0.00010189
Iteration 376/1000 | Loss: 0.00028851
Iteration 377/1000 | Loss: 0.00026971
Iteration 378/1000 | Loss: 0.00015030
Iteration 379/1000 | Loss: 0.00010262
Iteration 380/1000 | Loss: 0.00012584
Iteration 381/1000 | Loss: 0.00009709
Iteration 382/1000 | Loss: 0.00009613
Iteration 383/1000 | Loss: 0.00009360
Iteration 384/1000 | Loss: 0.00023094
Iteration 385/1000 | Loss: 0.00017026
Iteration 386/1000 | Loss: 0.00010705
Iteration 387/1000 | Loss: 0.00009359
Iteration 388/1000 | Loss: 0.00028743
Iteration 389/1000 | Loss: 0.00032889
Iteration 390/1000 | Loss: 0.00020564
Iteration 391/1000 | Loss: 0.00017919
Iteration 392/1000 | Loss: 0.00012338
Iteration 393/1000 | Loss: 0.00026240
Iteration 394/1000 | Loss: 0.00010920
Iteration 395/1000 | Loss: 0.00011704
Iteration 396/1000 | Loss: 0.00011156
Iteration 397/1000 | Loss: 0.00060101
Iteration 398/1000 | Loss: 0.00015070
Iteration 399/1000 | Loss: 0.00013151
Iteration 400/1000 | Loss: 0.00021893
Iteration 401/1000 | Loss: 0.00065797
Iteration 402/1000 | Loss: 0.00034771
Iteration 403/1000 | Loss: 0.00012470
Iteration 404/1000 | Loss: 0.00038051
Iteration 405/1000 | Loss: 0.00049915
Iteration 406/1000 | Loss: 0.00022195
Iteration 407/1000 | Loss: 0.00029276
Iteration 408/1000 | Loss: 0.00011400
Iteration 409/1000 | Loss: 0.00009700
Iteration 410/1000 | Loss: 0.00010156
Iteration 411/1000 | Loss: 0.00011260
Iteration 412/1000 | Loss: 0.00009267
Iteration 413/1000 | Loss: 0.00009122
Iteration 414/1000 | Loss: 0.00009816
Iteration 415/1000 | Loss: 0.00028763
Iteration 416/1000 | Loss: 0.00010507
Iteration 417/1000 | Loss: 0.00009381
Iteration 418/1000 | Loss: 0.00009624
Iteration 419/1000 | Loss: 0.00009473
Iteration 420/1000 | Loss: 0.00036076
Iteration 421/1000 | Loss: 0.00016060
Iteration 422/1000 | Loss: 0.00010769
Iteration 423/1000 | Loss: 0.00032349
Iteration 424/1000 | Loss: 0.00016802
Iteration 425/1000 | Loss: 0.00050107
Iteration 426/1000 | Loss: 0.00017435
Iteration 427/1000 | Loss: 0.00026075
Iteration 428/1000 | Loss: 0.00030429
Iteration 429/1000 | Loss: 0.00026508
Iteration 430/1000 | Loss: 0.00009685
Iteration 431/1000 | Loss: 0.00009265
Iteration 432/1000 | Loss: 0.00009069
Iteration 433/1000 | Loss: 0.00008975
Iteration 434/1000 | Loss: 0.00008879
Iteration 435/1000 | Loss: 0.00011167
Iteration 436/1000 | Loss: 0.00009354
Iteration 437/1000 | Loss: 0.00008987
Iteration 438/1000 | Loss: 0.00008842
Iteration 439/1000 | Loss: 0.00008782
Iteration 440/1000 | Loss: 0.00008754
Iteration 441/1000 | Loss: 0.00008740
Iteration 442/1000 | Loss: 0.00008728
Iteration 443/1000 | Loss: 0.00008727
Iteration 444/1000 | Loss: 0.00008727
Iteration 445/1000 | Loss: 0.00010075
Iteration 446/1000 | Loss: 0.00008712
Iteration 447/1000 | Loss: 0.00008651
Iteration 448/1000 | Loss: 0.00008554
Iteration 449/1000 | Loss: 0.00008510
Iteration 450/1000 | Loss: 0.00010456
Iteration 451/1000 | Loss: 0.00008498
Iteration 452/1000 | Loss: 0.00009202
Iteration 453/1000 | Loss: 0.00008496
Iteration 454/1000 | Loss: 0.00008485
Iteration 455/1000 | Loss: 0.00008485
Iteration 456/1000 | Loss: 0.00008485
Iteration 457/1000 | Loss: 0.00008485
Iteration 458/1000 | Loss: 0.00008485
Iteration 459/1000 | Loss: 0.00008485
Iteration 460/1000 | Loss: 0.00008485
Iteration 461/1000 | Loss: 0.00008485
Iteration 462/1000 | Loss: 0.00008485
Iteration 463/1000 | Loss: 0.00008485
Iteration 464/1000 | Loss: 0.00008485
Iteration 465/1000 | Loss: 0.00008485
Iteration 466/1000 | Loss: 0.00008484
Iteration 467/1000 | Loss: 0.00008484
Iteration 468/1000 | Loss: 0.00008484
Iteration 469/1000 | Loss: 0.00008484
Iteration 470/1000 | Loss: 0.00008484
Iteration 471/1000 | Loss: 0.00008484
Iteration 472/1000 | Loss: 0.00008484
Iteration 473/1000 | Loss: 0.00008484
Iteration 474/1000 | Loss: 0.00008484
Iteration 475/1000 | Loss: 0.00008484
Iteration 476/1000 | Loss: 0.00008484
Iteration 477/1000 | Loss: 0.00008484
Iteration 478/1000 | Loss: 0.00008483
Iteration 479/1000 | Loss: 0.00008479
Iteration 480/1000 | Loss: 0.00008479
Iteration 481/1000 | Loss: 0.00008478
Iteration 482/1000 | Loss: 0.00008477
Iteration 483/1000 | Loss: 0.00008477
Iteration 484/1000 | Loss: 0.00008477
Iteration 485/1000 | Loss: 0.00008477
Iteration 486/1000 | Loss: 0.00008477
Iteration 487/1000 | Loss: 0.00008477
Iteration 488/1000 | Loss: 0.00008477
Iteration 489/1000 | Loss: 0.00008476
Iteration 490/1000 | Loss: 0.00008476
Iteration 491/1000 | Loss: 0.00009340
Iteration 492/1000 | Loss: 0.00008478
Iteration 493/1000 | Loss: 0.00008474
Iteration 494/1000 | Loss: 0.00008474
Iteration 495/1000 | Loss: 0.00008474
Iteration 496/1000 | Loss: 0.00008474
Iteration 497/1000 | Loss: 0.00008473
Iteration 498/1000 | Loss: 0.00008473
Iteration 499/1000 | Loss: 0.00008473
Iteration 500/1000 | Loss: 0.00008473
Iteration 501/1000 | Loss: 0.00008473
Iteration 502/1000 | Loss: 0.00008473
Iteration 503/1000 | Loss: 0.00008473
Iteration 504/1000 | Loss: 0.00008473
Iteration 505/1000 | Loss: 0.00008473
Iteration 506/1000 | Loss: 0.00008473
Iteration 507/1000 | Loss: 0.00008473
Iteration 508/1000 | Loss: 0.00008473
Iteration 509/1000 | Loss: 0.00008472
Iteration 510/1000 | Loss: 0.00008472
Iteration 511/1000 | Loss: 0.00008472
Iteration 512/1000 | Loss: 0.00008472
Iteration 513/1000 | Loss: 0.00008471
Iteration 514/1000 | Loss: 0.00008471
Iteration 515/1000 | Loss: 0.00008471
Iteration 516/1000 | Loss: 0.00008471
Iteration 517/1000 | Loss: 0.00008471
Iteration 518/1000 | Loss: 0.00008471
Iteration 519/1000 | Loss: 0.00008471
Iteration 520/1000 | Loss: 0.00009077
Iteration 521/1000 | Loss: 0.00008469
Iteration 522/1000 | Loss: 0.00008468
Iteration 523/1000 | Loss: 0.00008468
Iteration 524/1000 | Loss: 0.00008468
Iteration 525/1000 | Loss: 0.00008468
Iteration 526/1000 | Loss: 0.00008468
Iteration 527/1000 | Loss: 0.00008468
Iteration 528/1000 | Loss: 0.00008468
Iteration 529/1000 | Loss: 0.00008468
Iteration 530/1000 | Loss: 0.00008467
Iteration 531/1000 | Loss: 0.00008467
Iteration 532/1000 | Loss: 0.00008467
Iteration 533/1000 | Loss: 0.00008467
Iteration 534/1000 | Loss: 0.00008467
Iteration 535/1000 | Loss: 0.00008467
Iteration 536/1000 | Loss: 0.00008467
Iteration 537/1000 | Loss: 0.00008467
Iteration 538/1000 | Loss: 0.00008467
Iteration 539/1000 | Loss: 0.00008467
Iteration 540/1000 | Loss: 0.00008467
Iteration 541/1000 | Loss: 0.00008467
Iteration 542/1000 | Loss: 0.00008467
Iteration 543/1000 | Loss: 0.00008467
Iteration 544/1000 | Loss: 0.00008467
Iteration 545/1000 | Loss: 0.00008467
Iteration 546/1000 | Loss: 0.00008467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 546. Stopping optimization.
Last 5 losses: [8.467108273180202e-05, 8.467108273180202e-05, 8.467108273180202e-05, 8.467108273180202e-05, 8.467108273180202e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.467108273180202e-05

Optimization complete. Final v2v error: 6.305536270141602 mm

Highest mean error: 11.681326866149902 mm for frame 63

Lowest mean error: 4.17933464050293 mm for frame 11

Saving results

Total time: 681.3686714172363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951486
Iteration 2/25 | Loss: 0.00140818
Iteration 3/25 | Loss: 0.00120075
Iteration 4/25 | Loss: 0.00118123
Iteration 5/25 | Loss: 0.00117610
Iteration 6/25 | Loss: 0.00117522
Iteration 7/25 | Loss: 0.00117522
Iteration 8/25 | Loss: 0.00117522
Iteration 9/25 | Loss: 0.00117522
Iteration 10/25 | Loss: 0.00117522
Iteration 11/25 | Loss: 0.00117522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001175224082544446, 0.001175224082544446, 0.001175224082544446, 0.001175224082544446, 0.001175224082544446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001175224082544446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66029763
Iteration 2/25 | Loss: 0.00078794
Iteration 3/25 | Loss: 0.00078794
Iteration 4/25 | Loss: 0.00078794
Iteration 5/25 | Loss: 0.00078794
Iteration 6/25 | Loss: 0.00078794
Iteration 7/25 | Loss: 0.00078794
Iteration 8/25 | Loss: 0.00078794
Iteration 9/25 | Loss: 0.00078793
Iteration 10/25 | Loss: 0.00078793
Iteration 11/25 | Loss: 0.00078793
Iteration 12/25 | Loss: 0.00078793
Iteration 13/25 | Loss: 0.00078793
Iteration 14/25 | Loss: 0.00078793
Iteration 15/25 | Loss: 0.00078793
Iteration 16/25 | Loss: 0.00078793
Iteration 17/25 | Loss: 0.00078793
Iteration 18/25 | Loss: 0.00078793
Iteration 19/25 | Loss: 0.00078793
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000787934404797852, 0.000787934404797852, 0.000787934404797852, 0.000787934404797852, 0.000787934404797852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000787934404797852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078793
Iteration 2/1000 | Loss: 0.00002538
Iteration 3/1000 | Loss: 0.00001692
Iteration 4/1000 | Loss: 0.00001569
Iteration 5/1000 | Loss: 0.00001494
Iteration 6/1000 | Loss: 0.00001477
Iteration 7/1000 | Loss: 0.00001450
Iteration 8/1000 | Loss: 0.00001425
Iteration 9/1000 | Loss: 0.00001405
Iteration 10/1000 | Loss: 0.00001404
Iteration 11/1000 | Loss: 0.00001396
Iteration 12/1000 | Loss: 0.00001395
Iteration 13/1000 | Loss: 0.00001394
Iteration 14/1000 | Loss: 0.00001382
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001377
Iteration 17/1000 | Loss: 0.00001375
Iteration 18/1000 | Loss: 0.00001375
Iteration 19/1000 | Loss: 0.00001371
Iteration 20/1000 | Loss: 0.00001370
Iteration 21/1000 | Loss: 0.00001368
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001362
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001360
Iteration 26/1000 | Loss: 0.00001359
Iteration 27/1000 | Loss: 0.00001358
Iteration 28/1000 | Loss: 0.00001355
Iteration 29/1000 | Loss: 0.00001355
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001349
Iteration 32/1000 | Loss: 0.00001348
Iteration 33/1000 | Loss: 0.00001348
Iteration 34/1000 | Loss: 0.00001348
Iteration 35/1000 | Loss: 0.00001348
Iteration 36/1000 | Loss: 0.00001348
Iteration 37/1000 | Loss: 0.00001348
Iteration 38/1000 | Loss: 0.00001347
Iteration 39/1000 | Loss: 0.00001347
Iteration 40/1000 | Loss: 0.00001347
Iteration 41/1000 | Loss: 0.00001347
Iteration 42/1000 | Loss: 0.00001347
Iteration 43/1000 | Loss: 0.00001346
Iteration 44/1000 | Loss: 0.00001345
Iteration 45/1000 | Loss: 0.00001344
Iteration 46/1000 | Loss: 0.00001343
Iteration 47/1000 | Loss: 0.00001343
Iteration 48/1000 | Loss: 0.00001343
Iteration 49/1000 | Loss: 0.00001343
Iteration 50/1000 | Loss: 0.00001342
Iteration 51/1000 | Loss: 0.00001342
Iteration 52/1000 | Loss: 0.00001342
Iteration 53/1000 | Loss: 0.00001342
Iteration 54/1000 | Loss: 0.00001342
Iteration 55/1000 | Loss: 0.00001342
Iteration 56/1000 | Loss: 0.00001341
Iteration 57/1000 | Loss: 0.00001341
Iteration 58/1000 | Loss: 0.00001340
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00001338
Iteration 62/1000 | Loss: 0.00001338
Iteration 63/1000 | Loss: 0.00001338
Iteration 64/1000 | Loss: 0.00001338
Iteration 65/1000 | Loss: 0.00001338
Iteration 66/1000 | Loss: 0.00001338
Iteration 67/1000 | Loss: 0.00001338
Iteration 68/1000 | Loss: 0.00001337
Iteration 69/1000 | Loss: 0.00001337
Iteration 70/1000 | Loss: 0.00001337
Iteration 71/1000 | Loss: 0.00001337
Iteration 72/1000 | Loss: 0.00001337
Iteration 73/1000 | Loss: 0.00001337
Iteration 74/1000 | Loss: 0.00001336
Iteration 75/1000 | Loss: 0.00001336
Iteration 76/1000 | Loss: 0.00001336
Iteration 77/1000 | Loss: 0.00001336
Iteration 78/1000 | Loss: 0.00001335
Iteration 79/1000 | Loss: 0.00001335
Iteration 80/1000 | Loss: 0.00001335
Iteration 81/1000 | Loss: 0.00001335
Iteration 82/1000 | Loss: 0.00001335
Iteration 83/1000 | Loss: 0.00001335
Iteration 84/1000 | Loss: 0.00001335
Iteration 85/1000 | Loss: 0.00001334
Iteration 86/1000 | Loss: 0.00001334
Iteration 87/1000 | Loss: 0.00001333
Iteration 88/1000 | Loss: 0.00001333
Iteration 89/1000 | Loss: 0.00001333
Iteration 90/1000 | Loss: 0.00001333
Iteration 91/1000 | Loss: 0.00001333
Iteration 92/1000 | Loss: 0.00001333
Iteration 93/1000 | Loss: 0.00001333
Iteration 94/1000 | Loss: 0.00001333
Iteration 95/1000 | Loss: 0.00001333
Iteration 96/1000 | Loss: 0.00001333
Iteration 97/1000 | Loss: 0.00001333
Iteration 98/1000 | Loss: 0.00001332
Iteration 99/1000 | Loss: 0.00001332
Iteration 100/1000 | Loss: 0.00001332
Iteration 101/1000 | Loss: 0.00001332
Iteration 102/1000 | Loss: 0.00001332
Iteration 103/1000 | Loss: 0.00001332
Iteration 104/1000 | Loss: 0.00001332
Iteration 105/1000 | Loss: 0.00001332
Iteration 106/1000 | Loss: 0.00001332
Iteration 107/1000 | Loss: 0.00001332
Iteration 108/1000 | Loss: 0.00001332
Iteration 109/1000 | Loss: 0.00001332
Iteration 110/1000 | Loss: 0.00001332
Iteration 111/1000 | Loss: 0.00001331
Iteration 112/1000 | Loss: 0.00001331
Iteration 113/1000 | Loss: 0.00001331
Iteration 114/1000 | Loss: 0.00001331
Iteration 115/1000 | Loss: 0.00001330
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001330
Iteration 118/1000 | Loss: 0.00001330
Iteration 119/1000 | Loss: 0.00001329
Iteration 120/1000 | Loss: 0.00001329
Iteration 121/1000 | Loss: 0.00001329
Iteration 122/1000 | Loss: 0.00001329
Iteration 123/1000 | Loss: 0.00001329
Iteration 124/1000 | Loss: 0.00001328
Iteration 125/1000 | Loss: 0.00001328
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001327
Iteration 128/1000 | Loss: 0.00001326
Iteration 129/1000 | Loss: 0.00001326
Iteration 130/1000 | Loss: 0.00001326
Iteration 131/1000 | Loss: 0.00001326
Iteration 132/1000 | Loss: 0.00001325
Iteration 133/1000 | Loss: 0.00001325
Iteration 134/1000 | Loss: 0.00001324
Iteration 135/1000 | Loss: 0.00001324
Iteration 136/1000 | Loss: 0.00001324
Iteration 137/1000 | Loss: 0.00001324
Iteration 138/1000 | Loss: 0.00001323
Iteration 139/1000 | Loss: 0.00001323
Iteration 140/1000 | Loss: 0.00001323
Iteration 141/1000 | Loss: 0.00001323
Iteration 142/1000 | Loss: 0.00001323
Iteration 143/1000 | Loss: 0.00001323
Iteration 144/1000 | Loss: 0.00001323
Iteration 145/1000 | Loss: 0.00001323
Iteration 146/1000 | Loss: 0.00001323
Iteration 147/1000 | Loss: 0.00001322
Iteration 148/1000 | Loss: 0.00001322
Iteration 149/1000 | Loss: 0.00001321
Iteration 150/1000 | Loss: 0.00001321
Iteration 151/1000 | Loss: 0.00001321
Iteration 152/1000 | Loss: 0.00001320
Iteration 153/1000 | Loss: 0.00001320
Iteration 154/1000 | Loss: 0.00001320
Iteration 155/1000 | Loss: 0.00001320
Iteration 156/1000 | Loss: 0.00001320
Iteration 157/1000 | Loss: 0.00001319
Iteration 158/1000 | Loss: 0.00001319
Iteration 159/1000 | Loss: 0.00001319
Iteration 160/1000 | Loss: 0.00001319
Iteration 161/1000 | Loss: 0.00001318
Iteration 162/1000 | Loss: 0.00001318
Iteration 163/1000 | Loss: 0.00001318
Iteration 164/1000 | Loss: 0.00001318
Iteration 165/1000 | Loss: 0.00001318
Iteration 166/1000 | Loss: 0.00001318
Iteration 167/1000 | Loss: 0.00001318
Iteration 168/1000 | Loss: 0.00001318
Iteration 169/1000 | Loss: 0.00001318
Iteration 170/1000 | Loss: 0.00001318
Iteration 171/1000 | Loss: 0.00001318
Iteration 172/1000 | Loss: 0.00001318
Iteration 173/1000 | Loss: 0.00001318
Iteration 174/1000 | Loss: 0.00001318
Iteration 175/1000 | Loss: 0.00001318
Iteration 176/1000 | Loss: 0.00001317
Iteration 177/1000 | Loss: 0.00001317
Iteration 178/1000 | Loss: 0.00001317
Iteration 179/1000 | Loss: 0.00001317
Iteration 180/1000 | Loss: 0.00001317
Iteration 181/1000 | Loss: 0.00001317
Iteration 182/1000 | Loss: 0.00001317
Iteration 183/1000 | Loss: 0.00001317
Iteration 184/1000 | Loss: 0.00001317
Iteration 185/1000 | Loss: 0.00001317
Iteration 186/1000 | Loss: 0.00001317
Iteration 187/1000 | Loss: 0.00001317
Iteration 188/1000 | Loss: 0.00001317
Iteration 189/1000 | Loss: 0.00001317
Iteration 190/1000 | Loss: 0.00001317
Iteration 191/1000 | Loss: 0.00001317
Iteration 192/1000 | Loss: 0.00001317
Iteration 193/1000 | Loss: 0.00001317
Iteration 194/1000 | Loss: 0.00001317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.3173589650250506e-05, 1.3173589650250506e-05, 1.3173589650250506e-05, 1.3173589650250506e-05, 1.3173589650250506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3173589650250506e-05

Optimization complete. Final v2v error: 3.06766676902771 mm

Highest mean error: 3.2153966426849365 mm for frame 88

Lowest mean error: 2.8859405517578125 mm for frame 111

Saving results

Total time: 36.27626299858093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00651736
Iteration 2/25 | Loss: 0.00124981
Iteration 3/25 | Loss: 0.00118089
Iteration 4/25 | Loss: 0.00117204
Iteration 5/25 | Loss: 0.00116942
Iteration 6/25 | Loss: 0.00116914
Iteration 7/25 | Loss: 0.00116914
Iteration 8/25 | Loss: 0.00116914
Iteration 9/25 | Loss: 0.00116914
Iteration 10/25 | Loss: 0.00116914
Iteration 11/25 | Loss: 0.00116914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011691410327330232, 0.0011691410327330232, 0.0011691410327330232, 0.0011691410327330232, 0.0011691410327330232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011691410327330232

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.07294846
Iteration 2/25 | Loss: 0.00082250
Iteration 3/25 | Loss: 0.00082250
Iteration 4/25 | Loss: 0.00082250
Iteration 5/25 | Loss: 0.00082250
Iteration 6/25 | Loss: 0.00082250
Iteration 7/25 | Loss: 0.00082250
Iteration 8/25 | Loss: 0.00082250
Iteration 9/25 | Loss: 0.00082250
Iteration 10/25 | Loss: 0.00082250
Iteration 11/25 | Loss: 0.00082250
Iteration 12/25 | Loss: 0.00082250
Iteration 13/25 | Loss: 0.00082250
Iteration 14/25 | Loss: 0.00082250
Iteration 15/25 | Loss: 0.00082250
Iteration 16/25 | Loss: 0.00082250
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008224964840337634, 0.0008224964840337634, 0.0008224964840337634, 0.0008224964840337634, 0.0008224964840337634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008224964840337634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082250
Iteration 2/1000 | Loss: 0.00003003
Iteration 3/1000 | Loss: 0.00002264
Iteration 4/1000 | Loss: 0.00001902
Iteration 5/1000 | Loss: 0.00001819
Iteration 6/1000 | Loss: 0.00001755
Iteration 7/1000 | Loss: 0.00001709
Iteration 8/1000 | Loss: 0.00001669
Iteration 9/1000 | Loss: 0.00001658
Iteration 10/1000 | Loss: 0.00001618
Iteration 11/1000 | Loss: 0.00001591
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001562
Iteration 14/1000 | Loss: 0.00001556
Iteration 15/1000 | Loss: 0.00001550
Iteration 16/1000 | Loss: 0.00001539
Iteration 17/1000 | Loss: 0.00001538
Iteration 18/1000 | Loss: 0.00001534
Iteration 19/1000 | Loss: 0.00001532
Iteration 20/1000 | Loss: 0.00001532
Iteration 21/1000 | Loss: 0.00001531
Iteration 22/1000 | Loss: 0.00001531
Iteration 23/1000 | Loss: 0.00001531
Iteration 24/1000 | Loss: 0.00001530
Iteration 25/1000 | Loss: 0.00001529
Iteration 26/1000 | Loss: 0.00001529
Iteration 27/1000 | Loss: 0.00001528
Iteration 28/1000 | Loss: 0.00001528
Iteration 29/1000 | Loss: 0.00001527
Iteration 30/1000 | Loss: 0.00001527
Iteration 31/1000 | Loss: 0.00001527
Iteration 32/1000 | Loss: 0.00001527
Iteration 33/1000 | Loss: 0.00001526
Iteration 34/1000 | Loss: 0.00001526
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001525
Iteration 37/1000 | Loss: 0.00001525
Iteration 38/1000 | Loss: 0.00001524
Iteration 39/1000 | Loss: 0.00001524
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001523
Iteration 42/1000 | Loss: 0.00001522
Iteration 43/1000 | Loss: 0.00001522
Iteration 44/1000 | Loss: 0.00001522
Iteration 45/1000 | Loss: 0.00001522
Iteration 46/1000 | Loss: 0.00001522
Iteration 47/1000 | Loss: 0.00001522
Iteration 48/1000 | Loss: 0.00001521
Iteration 49/1000 | Loss: 0.00001521
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001520
Iteration 52/1000 | Loss: 0.00001517
Iteration 53/1000 | Loss: 0.00001517
Iteration 54/1000 | Loss: 0.00001517
Iteration 55/1000 | Loss: 0.00001516
Iteration 56/1000 | Loss: 0.00001515
Iteration 57/1000 | Loss: 0.00001515
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001513
Iteration 60/1000 | Loss: 0.00001513
Iteration 61/1000 | Loss: 0.00001513
Iteration 62/1000 | Loss: 0.00001513
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001513
Iteration 65/1000 | Loss: 0.00001512
Iteration 66/1000 | Loss: 0.00001512
Iteration 67/1000 | Loss: 0.00001512
Iteration 68/1000 | Loss: 0.00001512
Iteration 69/1000 | Loss: 0.00001512
Iteration 70/1000 | Loss: 0.00001512
Iteration 71/1000 | Loss: 0.00001512
Iteration 72/1000 | Loss: 0.00001512
Iteration 73/1000 | Loss: 0.00001512
Iteration 74/1000 | Loss: 0.00001511
Iteration 75/1000 | Loss: 0.00001511
Iteration 76/1000 | Loss: 0.00001511
Iteration 77/1000 | Loss: 0.00001511
Iteration 78/1000 | Loss: 0.00001511
Iteration 79/1000 | Loss: 0.00001511
Iteration 80/1000 | Loss: 0.00001511
Iteration 81/1000 | Loss: 0.00001511
Iteration 82/1000 | Loss: 0.00001511
Iteration 83/1000 | Loss: 0.00001511
Iteration 84/1000 | Loss: 0.00001511
Iteration 85/1000 | Loss: 0.00001510
Iteration 86/1000 | Loss: 0.00001510
Iteration 87/1000 | Loss: 0.00001510
Iteration 88/1000 | Loss: 0.00001510
Iteration 89/1000 | Loss: 0.00001510
Iteration 90/1000 | Loss: 0.00001509
Iteration 91/1000 | Loss: 0.00001509
Iteration 92/1000 | Loss: 0.00001508
Iteration 93/1000 | Loss: 0.00001508
Iteration 94/1000 | Loss: 0.00001507
Iteration 95/1000 | Loss: 0.00001507
Iteration 96/1000 | Loss: 0.00001507
Iteration 97/1000 | Loss: 0.00001507
Iteration 98/1000 | Loss: 0.00001507
Iteration 99/1000 | Loss: 0.00001507
Iteration 100/1000 | Loss: 0.00001507
Iteration 101/1000 | Loss: 0.00001506
Iteration 102/1000 | Loss: 0.00001506
Iteration 103/1000 | Loss: 0.00001506
Iteration 104/1000 | Loss: 0.00001506
Iteration 105/1000 | Loss: 0.00001506
Iteration 106/1000 | Loss: 0.00001506
Iteration 107/1000 | Loss: 0.00001505
Iteration 108/1000 | Loss: 0.00001505
Iteration 109/1000 | Loss: 0.00001505
Iteration 110/1000 | Loss: 0.00001504
Iteration 111/1000 | Loss: 0.00001504
Iteration 112/1000 | Loss: 0.00001504
Iteration 113/1000 | Loss: 0.00001504
Iteration 114/1000 | Loss: 0.00001504
Iteration 115/1000 | Loss: 0.00001503
Iteration 116/1000 | Loss: 0.00001503
Iteration 117/1000 | Loss: 0.00001503
Iteration 118/1000 | Loss: 0.00001503
Iteration 119/1000 | Loss: 0.00001502
Iteration 120/1000 | Loss: 0.00001502
Iteration 121/1000 | Loss: 0.00001501
Iteration 122/1000 | Loss: 0.00001501
Iteration 123/1000 | Loss: 0.00001501
Iteration 124/1000 | Loss: 0.00001500
Iteration 125/1000 | Loss: 0.00001500
Iteration 126/1000 | Loss: 0.00001500
Iteration 127/1000 | Loss: 0.00001499
Iteration 128/1000 | Loss: 0.00001499
Iteration 129/1000 | Loss: 0.00001499
Iteration 130/1000 | Loss: 0.00001498
Iteration 131/1000 | Loss: 0.00001498
Iteration 132/1000 | Loss: 0.00001498
Iteration 133/1000 | Loss: 0.00001498
Iteration 134/1000 | Loss: 0.00001497
Iteration 135/1000 | Loss: 0.00001497
Iteration 136/1000 | Loss: 0.00001497
Iteration 137/1000 | Loss: 0.00001497
Iteration 138/1000 | Loss: 0.00001497
Iteration 139/1000 | Loss: 0.00001497
Iteration 140/1000 | Loss: 0.00001497
Iteration 141/1000 | Loss: 0.00001497
Iteration 142/1000 | Loss: 0.00001497
Iteration 143/1000 | Loss: 0.00001497
Iteration 144/1000 | Loss: 0.00001497
Iteration 145/1000 | Loss: 0.00001497
Iteration 146/1000 | Loss: 0.00001497
Iteration 147/1000 | Loss: 0.00001497
Iteration 148/1000 | Loss: 0.00001497
Iteration 149/1000 | Loss: 0.00001496
Iteration 150/1000 | Loss: 0.00001496
Iteration 151/1000 | Loss: 0.00001496
Iteration 152/1000 | Loss: 0.00001496
Iteration 153/1000 | Loss: 0.00001496
Iteration 154/1000 | Loss: 0.00001495
Iteration 155/1000 | Loss: 0.00001495
Iteration 156/1000 | Loss: 0.00001495
Iteration 157/1000 | Loss: 0.00001495
Iteration 158/1000 | Loss: 0.00001495
Iteration 159/1000 | Loss: 0.00001495
Iteration 160/1000 | Loss: 0.00001495
Iteration 161/1000 | Loss: 0.00001495
Iteration 162/1000 | Loss: 0.00001495
Iteration 163/1000 | Loss: 0.00001494
Iteration 164/1000 | Loss: 0.00001494
Iteration 165/1000 | Loss: 0.00001494
Iteration 166/1000 | Loss: 0.00001494
Iteration 167/1000 | Loss: 0.00001494
Iteration 168/1000 | Loss: 0.00001494
Iteration 169/1000 | Loss: 0.00001494
Iteration 170/1000 | Loss: 0.00001494
Iteration 171/1000 | Loss: 0.00001494
Iteration 172/1000 | Loss: 0.00001494
Iteration 173/1000 | Loss: 0.00001494
Iteration 174/1000 | Loss: 0.00001494
Iteration 175/1000 | Loss: 0.00001493
Iteration 176/1000 | Loss: 0.00001493
Iteration 177/1000 | Loss: 0.00001493
Iteration 178/1000 | Loss: 0.00001493
Iteration 179/1000 | Loss: 0.00001493
Iteration 180/1000 | Loss: 0.00001493
Iteration 181/1000 | Loss: 0.00001493
Iteration 182/1000 | Loss: 0.00001493
Iteration 183/1000 | Loss: 0.00001493
Iteration 184/1000 | Loss: 0.00001493
Iteration 185/1000 | Loss: 0.00001493
Iteration 186/1000 | Loss: 0.00001493
Iteration 187/1000 | Loss: 0.00001493
Iteration 188/1000 | Loss: 0.00001493
Iteration 189/1000 | Loss: 0.00001493
Iteration 190/1000 | Loss: 0.00001493
Iteration 191/1000 | Loss: 0.00001493
Iteration 192/1000 | Loss: 0.00001493
Iteration 193/1000 | Loss: 0.00001493
Iteration 194/1000 | Loss: 0.00001493
Iteration 195/1000 | Loss: 0.00001493
Iteration 196/1000 | Loss: 0.00001493
Iteration 197/1000 | Loss: 0.00001493
Iteration 198/1000 | Loss: 0.00001493
Iteration 199/1000 | Loss: 0.00001493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.4931825717212632e-05, 1.4931825717212632e-05, 1.4931825717212632e-05, 1.4931825717212632e-05, 1.4931825717212632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4931825717212632e-05

Optimization complete. Final v2v error: 3.3198344707489014 mm

Highest mean error: 3.7626168727874756 mm for frame 52

Lowest mean error: 3.0557637214660645 mm for frame 155

Saving results

Total time: 39.126649141311646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935045
Iteration 2/25 | Loss: 0.00185593
Iteration 3/25 | Loss: 0.00158172
Iteration 4/25 | Loss: 0.00155302
Iteration 5/25 | Loss: 0.00156322
Iteration 6/25 | Loss: 0.00147106
Iteration 7/25 | Loss: 0.00145613
Iteration 8/25 | Loss: 0.00144998
Iteration 9/25 | Loss: 0.00144507
Iteration 10/25 | Loss: 0.00144688
Iteration 11/25 | Loss: 0.00144138
Iteration 12/25 | Loss: 0.00143795
Iteration 13/25 | Loss: 0.00143782
Iteration 14/25 | Loss: 0.00143769
Iteration 15/25 | Loss: 0.00143740
Iteration 16/25 | Loss: 0.00143713
Iteration 17/25 | Loss: 0.00143701
Iteration 18/25 | Loss: 0.00143698
Iteration 19/25 | Loss: 0.00143698
Iteration 20/25 | Loss: 0.00143698
Iteration 21/25 | Loss: 0.00143698
Iteration 22/25 | Loss: 0.00143698
Iteration 23/25 | Loss: 0.00143698
Iteration 24/25 | Loss: 0.00143698
Iteration 25/25 | Loss: 0.00143697

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87978303
Iteration 2/25 | Loss: 0.00234829
Iteration 3/25 | Loss: 0.00217807
Iteration 4/25 | Loss: 0.00217807
Iteration 5/25 | Loss: 0.00217807
Iteration 6/25 | Loss: 0.00217807
Iteration 7/25 | Loss: 0.00217807
Iteration 8/25 | Loss: 0.00217807
Iteration 9/25 | Loss: 0.00217807
Iteration 10/25 | Loss: 0.00217807
Iteration 11/25 | Loss: 0.00217807
Iteration 12/25 | Loss: 0.00217807
Iteration 13/25 | Loss: 0.00217807
Iteration 14/25 | Loss: 0.00217807
Iteration 15/25 | Loss: 0.00217807
Iteration 16/25 | Loss: 0.00217807
Iteration 17/25 | Loss: 0.00217807
Iteration 18/25 | Loss: 0.00217807
Iteration 19/25 | Loss: 0.00217807
Iteration 20/25 | Loss: 0.00217807
Iteration 21/25 | Loss: 0.00217807
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0021780668757855892, 0.0021780668757855892, 0.0021780668757855892, 0.0021780668757855892, 0.0021780668757855892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021780668757855892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217807
Iteration 2/1000 | Loss: 0.00027805
Iteration 3/1000 | Loss: 0.00152441
Iteration 4/1000 | Loss: 0.00105453
Iteration 5/1000 | Loss: 0.00013827
Iteration 6/1000 | Loss: 0.00145914
Iteration 7/1000 | Loss: 0.00041166
Iteration 8/1000 | Loss: 0.00054214
Iteration 9/1000 | Loss: 0.00042765
Iteration 10/1000 | Loss: 0.00040859
Iteration 11/1000 | Loss: 0.00007915
Iteration 12/1000 | Loss: 0.00016417
Iteration 13/1000 | Loss: 0.00067196
Iteration 14/1000 | Loss: 0.00072090
Iteration 15/1000 | Loss: 0.00013122
Iteration 16/1000 | Loss: 0.00010482
Iteration 17/1000 | Loss: 0.00005968
Iteration 18/1000 | Loss: 0.00046425
Iteration 19/1000 | Loss: 0.00048200
Iteration 20/1000 | Loss: 0.00031667
Iteration 21/1000 | Loss: 0.00129556
Iteration 22/1000 | Loss: 0.00100752
Iteration 23/1000 | Loss: 0.00032507
Iteration 24/1000 | Loss: 0.00117482
Iteration 25/1000 | Loss: 0.00065810
Iteration 26/1000 | Loss: 0.00052232
Iteration 27/1000 | Loss: 0.00032584
Iteration 28/1000 | Loss: 0.00040265
Iteration 29/1000 | Loss: 0.00005674
Iteration 30/1000 | Loss: 0.00046235
Iteration 31/1000 | Loss: 0.00005862
Iteration 32/1000 | Loss: 0.00026698
Iteration 33/1000 | Loss: 0.00005045
Iteration 34/1000 | Loss: 0.00004522
Iteration 35/1000 | Loss: 0.00004292
Iteration 36/1000 | Loss: 0.00029091
Iteration 37/1000 | Loss: 0.00004802
Iteration 38/1000 | Loss: 0.00038291
Iteration 39/1000 | Loss: 0.00006126
Iteration 40/1000 | Loss: 0.00048646
Iteration 41/1000 | Loss: 0.00059515
Iteration 42/1000 | Loss: 0.00031363
Iteration 43/1000 | Loss: 0.00016299
Iteration 44/1000 | Loss: 0.00018120
Iteration 45/1000 | Loss: 0.00004811
Iteration 46/1000 | Loss: 0.00004277
Iteration 47/1000 | Loss: 0.00003925
Iteration 48/1000 | Loss: 0.00003755
Iteration 49/1000 | Loss: 0.00003611
Iteration 50/1000 | Loss: 0.00003479
Iteration 51/1000 | Loss: 0.00003398
Iteration 52/1000 | Loss: 0.00003347
Iteration 53/1000 | Loss: 0.00003305
Iteration 54/1000 | Loss: 0.00003267
Iteration 55/1000 | Loss: 0.00003241
Iteration 56/1000 | Loss: 0.00003216
Iteration 57/1000 | Loss: 0.00003194
Iteration 58/1000 | Loss: 0.00003169
Iteration 59/1000 | Loss: 0.00003148
Iteration 60/1000 | Loss: 0.00003137
Iteration 61/1000 | Loss: 0.00003136
Iteration 62/1000 | Loss: 0.00003136
Iteration 63/1000 | Loss: 0.00003134
Iteration 64/1000 | Loss: 0.00003133
Iteration 65/1000 | Loss: 0.00003129
Iteration 66/1000 | Loss: 0.00003129
Iteration 67/1000 | Loss: 0.00003126
Iteration 68/1000 | Loss: 0.00003124
Iteration 69/1000 | Loss: 0.00003120
Iteration 70/1000 | Loss: 0.00003118
Iteration 71/1000 | Loss: 0.00003114
Iteration 72/1000 | Loss: 0.00003114
Iteration 73/1000 | Loss: 0.00003111
Iteration 74/1000 | Loss: 0.00003111
Iteration 75/1000 | Loss: 0.00003111
Iteration 76/1000 | Loss: 0.00003109
Iteration 77/1000 | Loss: 0.00003108
Iteration 78/1000 | Loss: 0.00003108
Iteration 79/1000 | Loss: 0.00003104
Iteration 80/1000 | Loss: 0.00003103
Iteration 81/1000 | Loss: 0.00003098
Iteration 82/1000 | Loss: 0.00003097
Iteration 83/1000 | Loss: 0.00003095
Iteration 84/1000 | Loss: 0.00003094
Iteration 85/1000 | Loss: 0.00003094
Iteration 86/1000 | Loss: 0.00003093
Iteration 87/1000 | Loss: 0.00003092
Iteration 88/1000 | Loss: 0.00003092
Iteration 89/1000 | Loss: 0.00003092
Iteration 90/1000 | Loss: 0.00003091
Iteration 91/1000 | Loss: 0.00003091
Iteration 92/1000 | Loss: 0.00003090
Iteration 93/1000 | Loss: 0.00003090
Iteration 94/1000 | Loss: 0.00003089
Iteration 95/1000 | Loss: 0.00003089
Iteration 96/1000 | Loss: 0.00003088
Iteration 97/1000 | Loss: 0.00003088
Iteration 98/1000 | Loss: 0.00003087
Iteration 99/1000 | Loss: 0.00003087
Iteration 100/1000 | Loss: 0.00003086
Iteration 101/1000 | Loss: 0.00003086
Iteration 102/1000 | Loss: 0.00003086
Iteration 103/1000 | Loss: 0.00003086
Iteration 104/1000 | Loss: 0.00003085
Iteration 105/1000 | Loss: 0.00003085
Iteration 106/1000 | Loss: 0.00003085
Iteration 107/1000 | Loss: 0.00003085
Iteration 108/1000 | Loss: 0.00003084
Iteration 109/1000 | Loss: 0.00003084
Iteration 110/1000 | Loss: 0.00003084
Iteration 111/1000 | Loss: 0.00003083
Iteration 112/1000 | Loss: 0.00003083
Iteration 113/1000 | Loss: 0.00003083
Iteration 114/1000 | Loss: 0.00003082
Iteration 115/1000 | Loss: 0.00003082
Iteration 116/1000 | Loss: 0.00003081
Iteration 117/1000 | Loss: 0.00003081
Iteration 118/1000 | Loss: 0.00003081
Iteration 119/1000 | Loss: 0.00003080
Iteration 120/1000 | Loss: 0.00003080
Iteration 121/1000 | Loss: 0.00003079
Iteration 122/1000 | Loss: 0.00003079
Iteration 123/1000 | Loss: 0.00003079
Iteration 124/1000 | Loss: 0.00003079
Iteration 125/1000 | Loss: 0.00003079
Iteration 126/1000 | Loss: 0.00003079
Iteration 127/1000 | Loss: 0.00003079
Iteration 128/1000 | Loss: 0.00003077
Iteration 129/1000 | Loss: 0.00003077
Iteration 130/1000 | Loss: 0.00003077
Iteration 131/1000 | Loss: 0.00003077
Iteration 132/1000 | Loss: 0.00003077
Iteration 133/1000 | Loss: 0.00003077
Iteration 134/1000 | Loss: 0.00003077
Iteration 135/1000 | Loss: 0.00003077
Iteration 136/1000 | Loss: 0.00003076
Iteration 137/1000 | Loss: 0.00003076
Iteration 138/1000 | Loss: 0.00003076
Iteration 139/1000 | Loss: 0.00003076
Iteration 140/1000 | Loss: 0.00003076
Iteration 141/1000 | Loss: 0.00003076
Iteration 142/1000 | Loss: 0.00003076
Iteration 143/1000 | Loss: 0.00003076
Iteration 144/1000 | Loss: 0.00003076
Iteration 145/1000 | Loss: 0.00003076
Iteration 146/1000 | Loss: 0.00003076
Iteration 147/1000 | Loss: 0.00003075
Iteration 148/1000 | Loss: 0.00003075
Iteration 149/1000 | Loss: 0.00003075
Iteration 150/1000 | Loss: 0.00003074
Iteration 151/1000 | Loss: 0.00003074
Iteration 152/1000 | Loss: 0.00003073
Iteration 153/1000 | Loss: 0.00003072
Iteration 154/1000 | Loss: 0.00003072
Iteration 155/1000 | Loss: 0.00003072
Iteration 156/1000 | Loss: 0.00003072
Iteration 157/1000 | Loss: 0.00003072
Iteration 158/1000 | Loss: 0.00003072
Iteration 159/1000 | Loss: 0.00003072
Iteration 160/1000 | Loss: 0.00003071
Iteration 161/1000 | Loss: 0.00003071
Iteration 162/1000 | Loss: 0.00003071
Iteration 163/1000 | Loss: 0.00003071
Iteration 164/1000 | Loss: 0.00029134
Iteration 165/1000 | Loss: 0.00003083
Iteration 166/1000 | Loss: 0.00003001
Iteration 167/1000 | Loss: 0.00002959
Iteration 168/1000 | Loss: 0.00002930
Iteration 169/1000 | Loss: 0.00002905
Iteration 170/1000 | Loss: 0.00002895
Iteration 171/1000 | Loss: 0.00002890
Iteration 172/1000 | Loss: 0.00002886
Iteration 173/1000 | Loss: 0.00002885
Iteration 174/1000 | Loss: 0.00002885
Iteration 175/1000 | Loss: 0.00002884
Iteration 176/1000 | Loss: 0.00002883
Iteration 177/1000 | Loss: 0.00002880
Iteration 178/1000 | Loss: 0.00002879
Iteration 179/1000 | Loss: 0.00002879
Iteration 180/1000 | Loss: 0.00002878
Iteration 181/1000 | Loss: 0.00002878
Iteration 182/1000 | Loss: 0.00002878
Iteration 183/1000 | Loss: 0.00002877
Iteration 184/1000 | Loss: 0.00002876
Iteration 185/1000 | Loss: 0.00002876
Iteration 186/1000 | Loss: 0.00002876
Iteration 187/1000 | Loss: 0.00002875
Iteration 188/1000 | Loss: 0.00002875
Iteration 189/1000 | Loss: 0.00002875
Iteration 190/1000 | Loss: 0.00002875
Iteration 191/1000 | Loss: 0.00002874
Iteration 192/1000 | Loss: 0.00002874
Iteration 193/1000 | Loss: 0.00002874
Iteration 194/1000 | Loss: 0.00002874
Iteration 195/1000 | Loss: 0.00002874
Iteration 196/1000 | Loss: 0.00002874
Iteration 197/1000 | Loss: 0.00002873
Iteration 198/1000 | Loss: 0.00002873
Iteration 199/1000 | Loss: 0.00002873
Iteration 200/1000 | Loss: 0.00002872
Iteration 201/1000 | Loss: 0.00002872
Iteration 202/1000 | Loss: 0.00002872
Iteration 203/1000 | Loss: 0.00002872
Iteration 204/1000 | Loss: 0.00002872
Iteration 205/1000 | Loss: 0.00002871
Iteration 206/1000 | Loss: 0.00002871
Iteration 207/1000 | Loss: 0.00002871
Iteration 208/1000 | Loss: 0.00002871
Iteration 209/1000 | Loss: 0.00002871
Iteration 210/1000 | Loss: 0.00002870
Iteration 211/1000 | Loss: 0.00002870
Iteration 212/1000 | Loss: 0.00002870
Iteration 213/1000 | Loss: 0.00002870
Iteration 214/1000 | Loss: 0.00002870
Iteration 215/1000 | Loss: 0.00002870
Iteration 216/1000 | Loss: 0.00002870
Iteration 217/1000 | Loss: 0.00002870
Iteration 218/1000 | Loss: 0.00002870
Iteration 219/1000 | Loss: 0.00002870
Iteration 220/1000 | Loss: 0.00002870
Iteration 221/1000 | Loss: 0.00002870
Iteration 222/1000 | Loss: 0.00002869
Iteration 223/1000 | Loss: 0.00002869
Iteration 224/1000 | Loss: 0.00002869
Iteration 225/1000 | Loss: 0.00002869
Iteration 226/1000 | Loss: 0.00002869
Iteration 227/1000 | Loss: 0.00002869
Iteration 228/1000 | Loss: 0.00002868
Iteration 229/1000 | Loss: 0.00002868
Iteration 230/1000 | Loss: 0.00002868
Iteration 231/1000 | Loss: 0.00002868
Iteration 232/1000 | Loss: 0.00002868
Iteration 233/1000 | Loss: 0.00002867
Iteration 234/1000 | Loss: 0.00002867
Iteration 235/1000 | Loss: 0.00002867
Iteration 236/1000 | Loss: 0.00002867
Iteration 237/1000 | Loss: 0.00002867
Iteration 238/1000 | Loss: 0.00002867
Iteration 239/1000 | Loss: 0.00002867
Iteration 240/1000 | Loss: 0.00002867
Iteration 241/1000 | Loss: 0.00002867
Iteration 242/1000 | Loss: 0.00002867
Iteration 243/1000 | Loss: 0.00002867
Iteration 244/1000 | Loss: 0.00002867
Iteration 245/1000 | Loss: 0.00002867
Iteration 246/1000 | Loss: 0.00002867
Iteration 247/1000 | Loss: 0.00002867
Iteration 248/1000 | Loss: 0.00002867
Iteration 249/1000 | Loss: 0.00002866
Iteration 250/1000 | Loss: 0.00002866
Iteration 251/1000 | Loss: 0.00002866
Iteration 252/1000 | Loss: 0.00002866
Iteration 253/1000 | Loss: 0.00002866
Iteration 254/1000 | Loss: 0.00002866
Iteration 255/1000 | Loss: 0.00002866
Iteration 256/1000 | Loss: 0.00002866
Iteration 257/1000 | Loss: 0.00002866
Iteration 258/1000 | Loss: 0.00002866
Iteration 259/1000 | Loss: 0.00002866
Iteration 260/1000 | Loss: 0.00002866
Iteration 261/1000 | Loss: 0.00002866
Iteration 262/1000 | Loss: 0.00002866
Iteration 263/1000 | Loss: 0.00002866
Iteration 264/1000 | Loss: 0.00002866
Iteration 265/1000 | Loss: 0.00002866
Iteration 266/1000 | Loss: 0.00002866
Iteration 267/1000 | Loss: 0.00002866
Iteration 268/1000 | Loss: 0.00002866
Iteration 269/1000 | Loss: 0.00002866
Iteration 270/1000 | Loss: 0.00002866
Iteration 271/1000 | Loss: 0.00002866
Iteration 272/1000 | Loss: 0.00002865
Iteration 273/1000 | Loss: 0.00002865
Iteration 274/1000 | Loss: 0.00002865
Iteration 275/1000 | Loss: 0.00002865
Iteration 276/1000 | Loss: 0.00002865
Iteration 277/1000 | Loss: 0.00002865
Iteration 278/1000 | Loss: 0.00002865
Iteration 279/1000 | Loss: 0.00002865
Iteration 280/1000 | Loss: 0.00002865
Iteration 281/1000 | Loss: 0.00002865
Iteration 282/1000 | Loss: 0.00002865
Iteration 283/1000 | Loss: 0.00002865
Iteration 284/1000 | Loss: 0.00002865
Iteration 285/1000 | Loss: 0.00002865
Iteration 286/1000 | Loss: 0.00002865
Iteration 287/1000 | Loss: 0.00002865
Iteration 288/1000 | Loss: 0.00002865
Iteration 289/1000 | Loss: 0.00002865
Iteration 290/1000 | Loss: 0.00002865
Iteration 291/1000 | Loss: 0.00002865
Iteration 292/1000 | Loss: 0.00002865
Iteration 293/1000 | Loss: 0.00002865
Iteration 294/1000 | Loss: 0.00002865
Iteration 295/1000 | Loss: 0.00002865
Iteration 296/1000 | Loss: 0.00002865
Iteration 297/1000 | Loss: 0.00002865
Iteration 298/1000 | Loss: 0.00002865
Iteration 299/1000 | Loss: 0.00002865
Iteration 300/1000 | Loss: 0.00002865
Iteration 301/1000 | Loss: 0.00002865
Iteration 302/1000 | Loss: 0.00002865
Iteration 303/1000 | Loss: 0.00002865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 303. Stopping optimization.
Last 5 losses: [2.865156602638308e-05, 2.865156602638308e-05, 2.865156602638308e-05, 2.865156602638308e-05, 2.865156602638308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.865156602638308e-05

Optimization complete. Final v2v error: 4.418060302734375 mm

Highest mean error: 5.6283087730407715 mm for frame 22

Lowest mean error: 3.3069260120391846 mm for frame 207

Saving results

Total time: 164.84465050697327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00519311
Iteration 2/25 | Loss: 0.00130934
Iteration 3/25 | Loss: 0.00118886
Iteration 4/25 | Loss: 0.00116420
Iteration 5/25 | Loss: 0.00115936
Iteration 6/25 | Loss: 0.00115909
Iteration 7/25 | Loss: 0.00115909
Iteration 8/25 | Loss: 0.00115909
Iteration 9/25 | Loss: 0.00115909
Iteration 10/25 | Loss: 0.00115909
Iteration 11/25 | Loss: 0.00115909
Iteration 12/25 | Loss: 0.00115909
Iteration 13/25 | Loss: 0.00115909
Iteration 14/25 | Loss: 0.00115909
Iteration 15/25 | Loss: 0.00115909
Iteration 16/25 | Loss: 0.00115909
Iteration 17/25 | Loss: 0.00115909
Iteration 18/25 | Loss: 0.00115909
Iteration 19/25 | Loss: 0.00115909
Iteration 20/25 | Loss: 0.00115909
Iteration 21/25 | Loss: 0.00115909
Iteration 22/25 | Loss: 0.00115909
Iteration 23/25 | Loss: 0.00115909
Iteration 24/25 | Loss: 0.00115909
Iteration 25/25 | Loss: 0.00115909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.77427363
Iteration 2/25 | Loss: 0.00084012
Iteration 3/25 | Loss: 0.00084011
Iteration 4/25 | Loss: 0.00084011
Iteration 5/25 | Loss: 0.00084011
Iteration 6/25 | Loss: 0.00084011
Iteration 7/25 | Loss: 0.00084011
Iteration 8/25 | Loss: 0.00084011
Iteration 9/25 | Loss: 0.00084010
Iteration 10/25 | Loss: 0.00084010
Iteration 11/25 | Loss: 0.00084010
Iteration 12/25 | Loss: 0.00084010
Iteration 13/25 | Loss: 0.00084010
Iteration 14/25 | Loss: 0.00084010
Iteration 15/25 | Loss: 0.00084010
Iteration 16/25 | Loss: 0.00084010
Iteration 17/25 | Loss: 0.00084010
Iteration 18/25 | Loss: 0.00084010
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008401044178754091, 0.0008401044178754091, 0.0008401044178754091, 0.0008401044178754091, 0.0008401044178754091]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008401044178754091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084010
Iteration 2/1000 | Loss: 0.00002983
Iteration 3/1000 | Loss: 0.00002042
Iteration 4/1000 | Loss: 0.00001782
Iteration 5/1000 | Loss: 0.00001673
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001533
Iteration 8/1000 | Loss: 0.00001493
Iteration 9/1000 | Loss: 0.00001453
Iteration 10/1000 | Loss: 0.00001433
Iteration 11/1000 | Loss: 0.00001411
Iteration 12/1000 | Loss: 0.00001394
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001389
Iteration 15/1000 | Loss: 0.00001386
Iteration 16/1000 | Loss: 0.00001381
Iteration 17/1000 | Loss: 0.00001377
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001376
Iteration 20/1000 | Loss: 0.00001374
Iteration 21/1000 | Loss: 0.00001370
Iteration 22/1000 | Loss: 0.00001365
Iteration 23/1000 | Loss: 0.00001365
Iteration 24/1000 | Loss: 0.00001364
Iteration 25/1000 | Loss: 0.00001363
Iteration 26/1000 | Loss: 0.00001362
Iteration 27/1000 | Loss: 0.00001361
Iteration 28/1000 | Loss: 0.00001361
Iteration 29/1000 | Loss: 0.00001361
Iteration 30/1000 | Loss: 0.00001361
Iteration 31/1000 | Loss: 0.00001360
Iteration 32/1000 | Loss: 0.00001360
Iteration 33/1000 | Loss: 0.00001360
Iteration 34/1000 | Loss: 0.00001359
Iteration 35/1000 | Loss: 0.00001359
Iteration 36/1000 | Loss: 0.00001358
Iteration 37/1000 | Loss: 0.00001358
Iteration 38/1000 | Loss: 0.00001358
Iteration 39/1000 | Loss: 0.00001357
Iteration 40/1000 | Loss: 0.00001357
Iteration 41/1000 | Loss: 0.00001357
Iteration 42/1000 | Loss: 0.00001356
Iteration 43/1000 | Loss: 0.00001356
Iteration 44/1000 | Loss: 0.00001355
Iteration 45/1000 | Loss: 0.00001355
Iteration 46/1000 | Loss: 0.00001355
Iteration 47/1000 | Loss: 0.00001355
Iteration 48/1000 | Loss: 0.00001355
Iteration 49/1000 | Loss: 0.00001354
Iteration 50/1000 | Loss: 0.00001354
Iteration 51/1000 | Loss: 0.00001354
Iteration 52/1000 | Loss: 0.00001354
Iteration 53/1000 | Loss: 0.00001353
Iteration 54/1000 | Loss: 0.00001353
Iteration 55/1000 | Loss: 0.00001353
Iteration 56/1000 | Loss: 0.00001352
Iteration 57/1000 | Loss: 0.00001352
Iteration 58/1000 | Loss: 0.00001352
Iteration 59/1000 | Loss: 0.00001351
Iteration 60/1000 | Loss: 0.00001351
Iteration 61/1000 | Loss: 0.00001350
Iteration 62/1000 | Loss: 0.00001350
Iteration 63/1000 | Loss: 0.00001350
Iteration 64/1000 | Loss: 0.00001350
Iteration 65/1000 | Loss: 0.00001350
Iteration 66/1000 | Loss: 0.00001350
Iteration 67/1000 | Loss: 0.00001350
Iteration 68/1000 | Loss: 0.00001350
Iteration 69/1000 | Loss: 0.00001350
Iteration 70/1000 | Loss: 0.00001350
Iteration 71/1000 | Loss: 0.00001350
Iteration 72/1000 | Loss: 0.00001350
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001349
Iteration 76/1000 | Loss: 0.00001349
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001349
Iteration 80/1000 | Loss: 0.00001349
Iteration 81/1000 | Loss: 0.00001349
Iteration 82/1000 | Loss: 0.00001349
Iteration 83/1000 | Loss: 0.00001349
Iteration 84/1000 | Loss: 0.00001348
Iteration 85/1000 | Loss: 0.00001348
Iteration 86/1000 | Loss: 0.00001348
Iteration 87/1000 | Loss: 0.00001348
Iteration 88/1000 | Loss: 0.00001348
Iteration 89/1000 | Loss: 0.00001348
Iteration 90/1000 | Loss: 0.00001348
Iteration 91/1000 | Loss: 0.00001347
Iteration 92/1000 | Loss: 0.00001347
Iteration 93/1000 | Loss: 0.00001347
Iteration 94/1000 | Loss: 0.00001347
Iteration 95/1000 | Loss: 0.00001347
Iteration 96/1000 | Loss: 0.00001346
Iteration 97/1000 | Loss: 0.00001346
Iteration 98/1000 | Loss: 0.00001345
Iteration 99/1000 | Loss: 0.00001345
Iteration 100/1000 | Loss: 0.00001345
Iteration 101/1000 | Loss: 0.00001345
Iteration 102/1000 | Loss: 0.00001345
Iteration 103/1000 | Loss: 0.00001344
Iteration 104/1000 | Loss: 0.00001344
Iteration 105/1000 | Loss: 0.00001344
Iteration 106/1000 | Loss: 0.00001343
Iteration 107/1000 | Loss: 0.00001343
Iteration 108/1000 | Loss: 0.00001343
Iteration 109/1000 | Loss: 0.00001343
Iteration 110/1000 | Loss: 0.00001343
Iteration 111/1000 | Loss: 0.00001343
Iteration 112/1000 | Loss: 0.00001342
Iteration 113/1000 | Loss: 0.00001342
Iteration 114/1000 | Loss: 0.00001342
Iteration 115/1000 | Loss: 0.00001342
Iteration 116/1000 | Loss: 0.00001341
Iteration 117/1000 | Loss: 0.00001341
Iteration 118/1000 | Loss: 0.00001341
Iteration 119/1000 | Loss: 0.00001341
Iteration 120/1000 | Loss: 0.00001341
Iteration 121/1000 | Loss: 0.00001341
Iteration 122/1000 | Loss: 0.00001340
Iteration 123/1000 | Loss: 0.00001340
Iteration 124/1000 | Loss: 0.00001340
Iteration 125/1000 | Loss: 0.00001340
Iteration 126/1000 | Loss: 0.00001340
Iteration 127/1000 | Loss: 0.00001340
Iteration 128/1000 | Loss: 0.00001340
Iteration 129/1000 | Loss: 0.00001340
Iteration 130/1000 | Loss: 0.00001340
Iteration 131/1000 | Loss: 0.00001340
Iteration 132/1000 | Loss: 0.00001340
Iteration 133/1000 | Loss: 0.00001339
Iteration 134/1000 | Loss: 0.00001339
Iteration 135/1000 | Loss: 0.00001339
Iteration 136/1000 | Loss: 0.00001339
Iteration 137/1000 | Loss: 0.00001339
Iteration 138/1000 | Loss: 0.00001339
Iteration 139/1000 | Loss: 0.00001338
Iteration 140/1000 | Loss: 0.00001338
Iteration 141/1000 | Loss: 0.00001338
Iteration 142/1000 | Loss: 0.00001338
Iteration 143/1000 | Loss: 0.00001338
Iteration 144/1000 | Loss: 0.00001337
Iteration 145/1000 | Loss: 0.00001337
Iteration 146/1000 | Loss: 0.00001337
Iteration 147/1000 | Loss: 0.00001337
Iteration 148/1000 | Loss: 0.00001337
Iteration 149/1000 | Loss: 0.00001337
Iteration 150/1000 | Loss: 0.00001337
Iteration 151/1000 | Loss: 0.00001337
Iteration 152/1000 | Loss: 0.00001337
Iteration 153/1000 | Loss: 0.00001337
Iteration 154/1000 | Loss: 0.00001337
Iteration 155/1000 | Loss: 0.00001337
Iteration 156/1000 | Loss: 0.00001337
Iteration 157/1000 | Loss: 0.00001337
Iteration 158/1000 | Loss: 0.00001337
Iteration 159/1000 | Loss: 0.00001337
Iteration 160/1000 | Loss: 0.00001337
Iteration 161/1000 | Loss: 0.00001336
Iteration 162/1000 | Loss: 0.00001336
Iteration 163/1000 | Loss: 0.00001336
Iteration 164/1000 | Loss: 0.00001336
Iteration 165/1000 | Loss: 0.00001336
Iteration 166/1000 | Loss: 0.00001336
Iteration 167/1000 | Loss: 0.00001336
Iteration 168/1000 | Loss: 0.00001336
Iteration 169/1000 | Loss: 0.00001336
Iteration 170/1000 | Loss: 0.00001336
Iteration 171/1000 | Loss: 0.00001336
Iteration 172/1000 | Loss: 0.00001336
Iteration 173/1000 | Loss: 0.00001336
Iteration 174/1000 | Loss: 0.00001336
Iteration 175/1000 | Loss: 0.00001336
Iteration 176/1000 | Loss: 0.00001336
Iteration 177/1000 | Loss: 0.00001336
Iteration 178/1000 | Loss: 0.00001336
Iteration 179/1000 | Loss: 0.00001336
Iteration 180/1000 | Loss: 0.00001336
Iteration 181/1000 | Loss: 0.00001336
Iteration 182/1000 | Loss: 0.00001336
Iteration 183/1000 | Loss: 0.00001336
Iteration 184/1000 | Loss: 0.00001336
Iteration 185/1000 | Loss: 0.00001336
Iteration 186/1000 | Loss: 0.00001336
Iteration 187/1000 | Loss: 0.00001336
Iteration 188/1000 | Loss: 0.00001336
Iteration 189/1000 | Loss: 0.00001336
Iteration 190/1000 | Loss: 0.00001336
Iteration 191/1000 | Loss: 0.00001336
Iteration 192/1000 | Loss: 0.00001336
Iteration 193/1000 | Loss: 0.00001336
Iteration 194/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.3355326700548176e-05, 1.3355326700548176e-05, 1.3355326700548176e-05, 1.3355326700548176e-05, 1.3355326700548176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3355326700548176e-05

Optimization complete. Final v2v error: 3.0883233547210693 mm

Highest mean error: 3.398397922515869 mm for frame 125

Lowest mean error: 2.8493988513946533 mm for frame 254

Saving results

Total time: 46.02445340156555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795142
Iteration 2/25 | Loss: 0.00119956
Iteration 3/25 | Loss: 0.00111902
Iteration 4/25 | Loss: 0.00111276
Iteration 5/25 | Loss: 0.00111206
Iteration 6/25 | Loss: 0.00111206
Iteration 7/25 | Loss: 0.00111206
Iteration 8/25 | Loss: 0.00111206
Iteration 9/25 | Loss: 0.00111206
Iteration 10/25 | Loss: 0.00111206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011120624840259552, 0.0011120624840259552, 0.0011120624840259552, 0.0011120624840259552, 0.0011120624840259552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011120624840259552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36627173
Iteration 2/25 | Loss: 0.00084529
Iteration 3/25 | Loss: 0.00084528
Iteration 4/25 | Loss: 0.00084528
Iteration 5/25 | Loss: 0.00084528
Iteration 6/25 | Loss: 0.00084528
Iteration 7/25 | Loss: 0.00084527
Iteration 8/25 | Loss: 0.00084527
Iteration 9/25 | Loss: 0.00084527
Iteration 10/25 | Loss: 0.00084527
Iteration 11/25 | Loss: 0.00084527
Iteration 12/25 | Loss: 0.00084527
Iteration 13/25 | Loss: 0.00084527
Iteration 14/25 | Loss: 0.00084527
Iteration 15/25 | Loss: 0.00084527
Iteration 16/25 | Loss: 0.00084527
Iteration 17/25 | Loss: 0.00084527
Iteration 18/25 | Loss: 0.00084527
Iteration 19/25 | Loss: 0.00084527
Iteration 20/25 | Loss: 0.00084527
Iteration 21/25 | Loss: 0.00084527
Iteration 22/25 | Loss: 0.00084527
Iteration 23/25 | Loss: 0.00084527
Iteration 24/25 | Loss: 0.00084527
Iteration 25/25 | Loss: 0.00084527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084527
Iteration 2/1000 | Loss: 0.00002039
Iteration 3/1000 | Loss: 0.00001372
Iteration 4/1000 | Loss: 0.00001139
Iteration 5/1000 | Loss: 0.00001058
Iteration 6/1000 | Loss: 0.00000991
Iteration 7/1000 | Loss: 0.00000965
Iteration 8/1000 | Loss: 0.00000965
Iteration 9/1000 | Loss: 0.00000958
Iteration 10/1000 | Loss: 0.00000932
Iteration 11/1000 | Loss: 0.00000920
Iteration 12/1000 | Loss: 0.00000909
Iteration 13/1000 | Loss: 0.00000902
Iteration 14/1000 | Loss: 0.00000899
Iteration 15/1000 | Loss: 0.00000896
Iteration 16/1000 | Loss: 0.00000895
Iteration 17/1000 | Loss: 0.00000894
Iteration 18/1000 | Loss: 0.00000888
Iteration 19/1000 | Loss: 0.00000888
Iteration 20/1000 | Loss: 0.00000883
Iteration 21/1000 | Loss: 0.00000881
Iteration 22/1000 | Loss: 0.00000879
Iteration 23/1000 | Loss: 0.00000878
Iteration 24/1000 | Loss: 0.00000878
Iteration 25/1000 | Loss: 0.00000877
Iteration 26/1000 | Loss: 0.00000877
Iteration 27/1000 | Loss: 0.00000877
Iteration 28/1000 | Loss: 0.00000876
Iteration 29/1000 | Loss: 0.00000876
Iteration 30/1000 | Loss: 0.00000876
Iteration 31/1000 | Loss: 0.00000875
Iteration 32/1000 | Loss: 0.00000875
Iteration 33/1000 | Loss: 0.00000874
Iteration 34/1000 | Loss: 0.00000874
Iteration 35/1000 | Loss: 0.00000874
Iteration 36/1000 | Loss: 0.00000874
Iteration 37/1000 | Loss: 0.00000874
Iteration 38/1000 | Loss: 0.00000873
Iteration 39/1000 | Loss: 0.00000873
Iteration 40/1000 | Loss: 0.00000873
Iteration 41/1000 | Loss: 0.00000873
Iteration 42/1000 | Loss: 0.00000872
Iteration 43/1000 | Loss: 0.00000872
Iteration 44/1000 | Loss: 0.00000872
Iteration 45/1000 | Loss: 0.00000871
Iteration 46/1000 | Loss: 0.00000871
Iteration 47/1000 | Loss: 0.00000871
Iteration 48/1000 | Loss: 0.00000871
Iteration 49/1000 | Loss: 0.00000870
Iteration 50/1000 | Loss: 0.00000870
Iteration 51/1000 | Loss: 0.00000870
Iteration 52/1000 | Loss: 0.00000870
Iteration 53/1000 | Loss: 0.00000870
Iteration 54/1000 | Loss: 0.00000869
Iteration 55/1000 | Loss: 0.00000869
Iteration 56/1000 | Loss: 0.00000869
Iteration 57/1000 | Loss: 0.00000869
Iteration 58/1000 | Loss: 0.00000869
Iteration 59/1000 | Loss: 0.00000868
Iteration 60/1000 | Loss: 0.00000868
Iteration 61/1000 | Loss: 0.00000868
Iteration 62/1000 | Loss: 0.00000867
Iteration 63/1000 | Loss: 0.00000866
Iteration 64/1000 | Loss: 0.00000866
Iteration 65/1000 | Loss: 0.00000865
Iteration 66/1000 | Loss: 0.00000865
Iteration 67/1000 | Loss: 0.00000865
Iteration 68/1000 | Loss: 0.00000865
Iteration 69/1000 | Loss: 0.00000865
Iteration 70/1000 | Loss: 0.00000865
Iteration 71/1000 | Loss: 0.00000864
Iteration 72/1000 | Loss: 0.00000864
Iteration 73/1000 | Loss: 0.00000864
Iteration 74/1000 | Loss: 0.00000864
Iteration 75/1000 | Loss: 0.00000863
Iteration 76/1000 | Loss: 0.00000863
Iteration 77/1000 | Loss: 0.00000863
Iteration 78/1000 | Loss: 0.00000862
Iteration 79/1000 | Loss: 0.00000862
Iteration 80/1000 | Loss: 0.00000861
Iteration 81/1000 | Loss: 0.00000860
Iteration 82/1000 | Loss: 0.00000860
Iteration 83/1000 | Loss: 0.00000859
Iteration 84/1000 | Loss: 0.00000855
Iteration 85/1000 | Loss: 0.00000855
Iteration 86/1000 | Loss: 0.00000855
Iteration 87/1000 | Loss: 0.00000855
Iteration 88/1000 | Loss: 0.00000854
Iteration 89/1000 | Loss: 0.00000854
Iteration 90/1000 | Loss: 0.00000853
Iteration 91/1000 | Loss: 0.00000853
Iteration 92/1000 | Loss: 0.00000852
Iteration 93/1000 | Loss: 0.00000852
Iteration 94/1000 | Loss: 0.00000852
Iteration 95/1000 | Loss: 0.00000851
Iteration 96/1000 | Loss: 0.00000851
Iteration 97/1000 | Loss: 0.00000851
Iteration 98/1000 | Loss: 0.00000850
Iteration 99/1000 | Loss: 0.00000850
Iteration 100/1000 | Loss: 0.00000850
Iteration 101/1000 | Loss: 0.00000849
Iteration 102/1000 | Loss: 0.00000849
Iteration 103/1000 | Loss: 0.00000848
Iteration 104/1000 | Loss: 0.00000848
Iteration 105/1000 | Loss: 0.00000847
Iteration 106/1000 | Loss: 0.00000847
Iteration 107/1000 | Loss: 0.00000847
Iteration 108/1000 | Loss: 0.00000847
Iteration 109/1000 | Loss: 0.00000847
Iteration 110/1000 | Loss: 0.00000847
Iteration 111/1000 | Loss: 0.00000847
Iteration 112/1000 | Loss: 0.00000846
Iteration 113/1000 | Loss: 0.00000846
Iteration 114/1000 | Loss: 0.00000846
Iteration 115/1000 | Loss: 0.00000846
Iteration 116/1000 | Loss: 0.00000846
Iteration 117/1000 | Loss: 0.00000846
Iteration 118/1000 | Loss: 0.00000846
Iteration 119/1000 | Loss: 0.00000846
Iteration 120/1000 | Loss: 0.00000846
Iteration 121/1000 | Loss: 0.00000845
Iteration 122/1000 | Loss: 0.00000845
Iteration 123/1000 | Loss: 0.00000845
Iteration 124/1000 | Loss: 0.00000845
Iteration 125/1000 | Loss: 0.00000845
Iteration 126/1000 | Loss: 0.00000844
Iteration 127/1000 | Loss: 0.00000844
Iteration 128/1000 | Loss: 0.00000844
Iteration 129/1000 | Loss: 0.00000844
Iteration 130/1000 | Loss: 0.00000844
Iteration 131/1000 | Loss: 0.00000843
Iteration 132/1000 | Loss: 0.00000843
Iteration 133/1000 | Loss: 0.00000843
Iteration 134/1000 | Loss: 0.00000843
Iteration 135/1000 | Loss: 0.00000843
Iteration 136/1000 | Loss: 0.00000843
Iteration 137/1000 | Loss: 0.00000842
Iteration 138/1000 | Loss: 0.00000842
Iteration 139/1000 | Loss: 0.00000842
Iteration 140/1000 | Loss: 0.00000842
Iteration 141/1000 | Loss: 0.00000842
Iteration 142/1000 | Loss: 0.00000842
Iteration 143/1000 | Loss: 0.00000842
Iteration 144/1000 | Loss: 0.00000842
Iteration 145/1000 | Loss: 0.00000842
Iteration 146/1000 | Loss: 0.00000841
Iteration 147/1000 | Loss: 0.00000841
Iteration 148/1000 | Loss: 0.00000841
Iteration 149/1000 | Loss: 0.00000841
Iteration 150/1000 | Loss: 0.00000841
Iteration 151/1000 | Loss: 0.00000841
Iteration 152/1000 | Loss: 0.00000841
Iteration 153/1000 | Loss: 0.00000841
Iteration 154/1000 | Loss: 0.00000840
Iteration 155/1000 | Loss: 0.00000840
Iteration 156/1000 | Loss: 0.00000840
Iteration 157/1000 | Loss: 0.00000840
Iteration 158/1000 | Loss: 0.00000840
Iteration 159/1000 | Loss: 0.00000840
Iteration 160/1000 | Loss: 0.00000840
Iteration 161/1000 | Loss: 0.00000840
Iteration 162/1000 | Loss: 0.00000840
Iteration 163/1000 | Loss: 0.00000840
Iteration 164/1000 | Loss: 0.00000840
Iteration 165/1000 | Loss: 0.00000840
Iteration 166/1000 | Loss: 0.00000840
Iteration 167/1000 | Loss: 0.00000840
Iteration 168/1000 | Loss: 0.00000840
Iteration 169/1000 | Loss: 0.00000840
Iteration 170/1000 | Loss: 0.00000839
Iteration 171/1000 | Loss: 0.00000839
Iteration 172/1000 | Loss: 0.00000839
Iteration 173/1000 | Loss: 0.00000839
Iteration 174/1000 | Loss: 0.00000839
Iteration 175/1000 | Loss: 0.00000839
Iteration 176/1000 | Loss: 0.00000839
Iteration 177/1000 | Loss: 0.00000839
Iteration 178/1000 | Loss: 0.00000838
Iteration 179/1000 | Loss: 0.00000838
Iteration 180/1000 | Loss: 0.00000838
Iteration 181/1000 | Loss: 0.00000838
Iteration 182/1000 | Loss: 0.00000838
Iteration 183/1000 | Loss: 0.00000838
Iteration 184/1000 | Loss: 0.00000838
Iteration 185/1000 | Loss: 0.00000838
Iteration 186/1000 | Loss: 0.00000838
Iteration 187/1000 | Loss: 0.00000837
Iteration 188/1000 | Loss: 0.00000837
Iteration 189/1000 | Loss: 0.00000837
Iteration 190/1000 | Loss: 0.00000837
Iteration 191/1000 | Loss: 0.00000837
Iteration 192/1000 | Loss: 0.00000837
Iteration 193/1000 | Loss: 0.00000837
Iteration 194/1000 | Loss: 0.00000837
Iteration 195/1000 | Loss: 0.00000837
Iteration 196/1000 | Loss: 0.00000836
Iteration 197/1000 | Loss: 0.00000836
Iteration 198/1000 | Loss: 0.00000836
Iteration 199/1000 | Loss: 0.00000836
Iteration 200/1000 | Loss: 0.00000836
Iteration 201/1000 | Loss: 0.00000836
Iteration 202/1000 | Loss: 0.00000836
Iteration 203/1000 | Loss: 0.00000836
Iteration 204/1000 | Loss: 0.00000836
Iteration 205/1000 | Loss: 0.00000836
Iteration 206/1000 | Loss: 0.00000836
Iteration 207/1000 | Loss: 0.00000836
Iteration 208/1000 | Loss: 0.00000835
Iteration 209/1000 | Loss: 0.00000835
Iteration 210/1000 | Loss: 0.00000835
Iteration 211/1000 | Loss: 0.00000835
Iteration 212/1000 | Loss: 0.00000835
Iteration 213/1000 | Loss: 0.00000835
Iteration 214/1000 | Loss: 0.00000835
Iteration 215/1000 | Loss: 0.00000835
Iteration 216/1000 | Loss: 0.00000834
Iteration 217/1000 | Loss: 0.00000834
Iteration 218/1000 | Loss: 0.00000834
Iteration 219/1000 | Loss: 0.00000834
Iteration 220/1000 | Loss: 0.00000834
Iteration 221/1000 | Loss: 0.00000834
Iteration 222/1000 | Loss: 0.00000834
Iteration 223/1000 | Loss: 0.00000834
Iteration 224/1000 | Loss: 0.00000833
Iteration 225/1000 | Loss: 0.00000833
Iteration 226/1000 | Loss: 0.00000833
Iteration 227/1000 | Loss: 0.00000833
Iteration 228/1000 | Loss: 0.00000833
Iteration 229/1000 | Loss: 0.00000833
Iteration 230/1000 | Loss: 0.00000833
Iteration 231/1000 | Loss: 0.00000833
Iteration 232/1000 | Loss: 0.00000833
Iteration 233/1000 | Loss: 0.00000833
Iteration 234/1000 | Loss: 0.00000833
Iteration 235/1000 | Loss: 0.00000833
Iteration 236/1000 | Loss: 0.00000833
Iteration 237/1000 | Loss: 0.00000833
Iteration 238/1000 | Loss: 0.00000833
Iteration 239/1000 | Loss: 0.00000833
Iteration 240/1000 | Loss: 0.00000832
Iteration 241/1000 | Loss: 0.00000832
Iteration 242/1000 | Loss: 0.00000832
Iteration 243/1000 | Loss: 0.00000832
Iteration 244/1000 | Loss: 0.00000832
Iteration 245/1000 | Loss: 0.00000832
Iteration 246/1000 | Loss: 0.00000832
Iteration 247/1000 | Loss: 0.00000832
Iteration 248/1000 | Loss: 0.00000832
Iteration 249/1000 | Loss: 0.00000832
Iteration 250/1000 | Loss: 0.00000832
Iteration 251/1000 | Loss: 0.00000832
Iteration 252/1000 | Loss: 0.00000832
Iteration 253/1000 | Loss: 0.00000832
Iteration 254/1000 | Loss: 0.00000832
Iteration 255/1000 | Loss: 0.00000832
Iteration 256/1000 | Loss: 0.00000832
Iteration 257/1000 | Loss: 0.00000832
Iteration 258/1000 | Loss: 0.00000832
Iteration 259/1000 | Loss: 0.00000832
Iteration 260/1000 | Loss: 0.00000832
Iteration 261/1000 | Loss: 0.00000832
Iteration 262/1000 | Loss: 0.00000832
Iteration 263/1000 | Loss: 0.00000832
Iteration 264/1000 | Loss: 0.00000832
Iteration 265/1000 | Loss: 0.00000832
Iteration 266/1000 | Loss: 0.00000832
Iteration 267/1000 | Loss: 0.00000832
Iteration 268/1000 | Loss: 0.00000832
Iteration 269/1000 | Loss: 0.00000832
Iteration 270/1000 | Loss: 0.00000832
Iteration 271/1000 | Loss: 0.00000832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [8.315716513607185e-06, 8.315716513607185e-06, 8.315716513607185e-06, 8.315716513607185e-06, 8.315716513607185e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.315716513607185e-06

Optimization complete. Final v2v error: 2.469572067260742 mm

Highest mean error: 2.6650922298431396 mm for frame 107

Lowest mean error: 2.3046951293945312 mm for frame 159

Saving results

Total time: 46.67258834838867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061248
Iteration 2/25 | Loss: 0.00166592
Iteration 3/25 | Loss: 0.00134621
Iteration 4/25 | Loss: 0.00132349
Iteration 5/25 | Loss: 0.00131719
Iteration 6/25 | Loss: 0.00131652
Iteration 7/25 | Loss: 0.00131652
Iteration 8/25 | Loss: 0.00131652
Iteration 9/25 | Loss: 0.00131652
Iteration 10/25 | Loss: 0.00131652
Iteration 11/25 | Loss: 0.00131652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001316517242230475, 0.001316517242230475, 0.001316517242230475, 0.001316517242230475, 0.001316517242230475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001316517242230475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93502426
Iteration 2/25 | Loss: 0.00095331
Iteration 3/25 | Loss: 0.00095331
Iteration 4/25 | Loss: 0.00095331
Iteration 5/25 | Loss: 0.00095331
Iteration 6/25 | Loss: 0.00095331
Iteration 7/25 | Loss: 0.00095331
Iteration 8/25 | Loss: 0.00095331
Iteration 9/25 | Loss: 0.00095331
Iteration 10/25 | Loss: 0.00095331
Iteration 11/25 | Loss: 0.00095331
Iteration 12/25 | Loss: 0.00095331
Iteration 13/25 | Loss: 0.00095331
Iteration 14/25 | Loss: 0.00095331
Iteration 15/25 | Loss: 0.00095331
Iteration 16/25 | Loss: 0.00095331
Iteration 17/25 | Loss: 0.00095331
Iteration 18/25 | Loss: 0.00095331
Iteration 19/25 | Loss: 0.00095331
Iteration 20/25 | Loss: 0.00095331
Iteration 21/25 | Loss: 0.00095331
Iteration 22/25 | Loss: 0.00095331
Iteration 23/25 | Loss: 0.00095331
Iteration 24/25 | Loss: 0.00095331
Iteration 25/25 | Loss: 0.00095331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009533092961646616, 0.0009533092961646616, 0.0009533092961646616, 0.0009533092961646616, 0.0009533092961646616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009533092961646616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095331
Iteration 2/1000 | Loss: 0.00004890
Iteration 3/1000 | Loss: 0.00003463
Iteration 4/1000 | Loss: 0.00002952
Iteration 5/1000 | Loss: 0.00002796
Iteration 6/1000 | Loss: 0.00002705
Iteration 7/1000 | Loss: 0.00002644
Iteration 8/1000 | Loss: 0.00002601
Iteration 9/1000 | Loss: 0.00002565
Iteration 10/1000 | Loss: 0.00002531
Iteration 11/1000 | Loss: 0.00002500
Iteration 12/1000 | Loss: 0.00002478
Iteration 13/1000 | Loss: 0.00002459
Iteration 14/1000 | Loss: 0.00002442
Iteration 15/1000 | Loss: 0.00002422
Iteration 16/1000 | Loss: 0.00002415
Iteration 17/1000 | Loss: 0.00002412
Iteration 18/1000 | Loss: 0.00002408
Iteration 19/1000 | Loss: 0.00002403
Iteration 20/1000 | Loss: 0.00002403
Iteration 21/1000 | Loss: 0.00002399
Iteration 22/1000 | Loss: 0.00002394
Iteration 23/1000 | Loss: 0.00002391
Iteration 24/1000 | Loss: 0.00002387
Iteration 25/1000 | Loss: 0.00002383
Iteration 26/1000 | Loss: 0.00002382
Iteration 27/1000 | Loss: 0.00002382
Iteration 28/1000 | Loss: 0.00002382
Iteration 29/1000 | Loss: 0.00002382
Iteration 30/1000 | Loss: 0.00002381
Iteration 31/1000 | Loss: 0.00002378
Iteration 32/1000 | Loss: 0.00002378
Iteration 33/1000 | Loss: 0.00002378
Iteration 34/1000 | Loss: 0.00002378
Iteration 35/1000 | Loss: 0.00002378
Iteration 36/1000 | Loss: 0.00002378
Iteration 37/1000 | Loss: 0.00002376
Iteration 38/1000 | Loss: 0.00002375
Iteration 39/1000 | Loss: 0.00002375
Iteration 40/1000 | Loss: 0.00002375
Iteration 41/1000 | Loss: 0.00002375
Iteration 42/1000 | Loss: 0.00002374
Iteration 43/1000 | Loss: 0.00002374
Iteration 44/1000 | Loss: 0.00002374
Iteration 45/1000 | Loss: 0.00002374
Iteration 46/1000 | Loss: 0.00002374
Iteration 47/1000 | Loss: 0.00002374
Iteration 48/1000 | Loss: 0.00002374
Iteration 49/1000 | Loss: 0.00002374
Iteration 50/1000 | Loss: 0.00002374
Iteration 51/1000 | Loss: 0.00002373
Iteration 52/1000 | Loss: 0.00002373
Iteration 53/1000 | Loss: 0.00002373
Iteration 54/1000 | Loss: 0.00002373
Iteration 55/1000 | Loss: 0.00002372
Iteration 56/1000 | Loss: 0.00002372
Iteration 57/1000 | Loss: 0.00002372
Iteration 58/1000 | Loss: 0.00002372
Iteration 59/1000 | Loss: 0.00002371
Iteration 60/1000 | Loss: 0.00002371
Iteration 61/1000 | Loss: 0.00002371
Iteration 62/1000 | Loss: 0.00002371
Iteration 63/1000 | Loss: 0.00002371
Iteration 64/1000 | Loss: 0.00002371
Iteration 65/1000 | Loss: 0.00002370
Iteration 66/1000 | Loss: 0.00002370
Iteration 67/1000 | Loss: 0.00002370
Iteration 68/1000 | Loss: 0.00002369
Iteration 69/1000 | Loss: 0.00002369
Iteration 70/1000 | Loss: 0.00002369
Iteration 71/1000 | Loss: 0.00002369
Iteration 72/1000 | Loss: 0.00002369
Iteration 73/1000 | Loss: 0.00002368
Iteration 74/1000 | Loss: 0.00002368
Iteration 75/1000 | Loss: 0.00002368
Iteration 76/1000 | Loss: 0.00002368
Iteration 77/1000 | Loss: 0.00002368
Iteration 78/1000 | Loss: 0.00002368
Iteration 79/1000 | Loss: 0.00002368
Iteration 80/1000 | Loss: 0.00002368
Iteration 81/1000 | Loss: 0.00002368
Iteration 82/1000 | Loss: 0.00002367
Iteration 83/1000 | Loss: 0.00002367
Iteration 84/1000 | Loss: 0.00002367
Iteration 85/1000 | Loss: 0.00002367
Iteration 86/1000 | Loss: 0.00002366
Iteration 87/1000 | Loss: 0.00002366
Iteration 88/1000 | Loss: 0.00002366
Iteration 89/1000 | Loss: 0.00002366
Iteration 90/1000 | Loss: 0.00002366
Iteration 91/1000 | Loss: 0.00002366
Iteration 92/1000 | Loss: 0.00002366
Iteration 93/1000 | Loss: 0.00002366
Iteration 94/1000 | Loss: 0.00002365
Iteration 95/1000 | Loss: 0.00002365
Iteration 96/1000 | Loss: 0.00002365
Iteration 97/1000 | Loss: 0.00002365
Iteration 98/1000 | Loss: 0.00002365
Iteration 99/1000 | Loss: 0.00002365
Iteration 100/1000 | Loss: 0.00002365
Iteration 101/1000 | Loss: 0.00002365
Iteration 102/1000 | Loss: 0.00002365
Iteration 103/1000 | Loss: 0.00002365
Iteration 104/1000 | Loss: 0.00002365
Iteration 105/1000 | Loss: 0.00002364
Iteration 106/1000 | Loss: 0.00002364
Iteration 107/1000 | Loss: 0.00002364
Iteration 108/1000 | Loss: 0.00002364
Iteration 109/1000 | Loss: 0.00002364
Iteration 110/1000 | Loss: 0.00002364
Iteration 111/1000 | Loss: 0.00002364
Iteration 112/1000 | Loss: 0.00002364
Iteration 113/1000 | Loss: 0.00002364
Iteration 114/1000 | Loss: 0.00002363
Iteration 115/1000 | Loss: 0.00002363
Iteration 116/1000 | Loss: 0.00002363
Iteration 117/1000 | Loss: 0.00002363
Iteration 118/1000 | Loss: 0.00002363
Iteration 119/1000 | Loss: 0.00002363
Iteration 120/1000 | Loss: 0.00002363
Iteration 121/1000 | Loss: 0.00002363
Iteration 122/1000 | Loss: 0.00002363
Iteration 123/1000 | Loss: 0.00002363
Iteration 124/1000 | Loss: 0.00002363
Iteration 125/1000 | Loss: 0.00002363
Iteration 126/1000 | Loss: 0.00002363
Iteration 127/1000 | Loss: 0.00002363
Iteration 128/1000 | Loss: 0.00002363
Iteration 129/1000 | Loss: 0.00002363
Iteration 130/1000 | Loss: 0.00002362
Iteration 131/1000 | Loss: 0.00002362
Iteration 132/1000 | Loss: 0.00002362
Iteration 133/1000 | Loss: 0.00002362
Iteration 134/1000 | Loss: 0.00002362
Iteration 135/1000 | Loss: 0.00002362
Iteration 136/1000 | Loss: 0.00002362
Iteration 137/1000 | Loss: 0.00002362
Iteration 138/1000 | Loss: 0.00002362
Iteration 139/1000 | Loss: 0.00002362
Iteration 140/1000 | Loss: 0.00002362
Iteration 141/1000 | Loss: 0.00002362
Iteration 142/1000 | Loss: 0.00002362
Iteration 143/1000 | Loss: 0.00002362
Iteration 144/1000 | Loss: 0.00002362
Iteration 145/1000 | Loss: 0.00002362
Iteration 146/1000 | Loss: 0.00002362
Iteration 147/1000 | Loss: 0.00002362
Iteration 148/1000 | Loss: 0.00002362
Iteration 149/1000 | Loss: 0.00002362
Iteration 150/1000 | Loss: 0.00002361
Iteration 151/1000 | Loss: 0.00002361
Iteration 152/1000 | Loss: 0.00002361
Iteration 153/1000 | Loss: 0.00002361
Iteration 154/1000 | Loss: 0.00002361
Iteration 155/1000 | Loss: 0.00002361
Iteration 156/1000 | Loss: 0.00002361
Iteration 157/1000 | Loss: 0.00002361
Iteration 158/1000 | Loss: 0.00002361
Iteration 159/1000 | Loss: 0.00002361
Iteration 160/1000 | Loss: 0.00002361
Iteration 161/1000 | Loss: 0.00002361
Iteration 162/1000 | Loss: 0.00002361
Iteration 163/1000 | Loss: 0.00002361
Iteration 164/1000 | Loss: 0.00002361
Iteration 165/1000 | Loss: 0.00002361
Iteration 166/1000 | Loss: 0.00002361
Iteration 167/1000 | Loss: 0.00002361
Iteration 168/1000 | Loss: 0.00002361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.3610558855580166e-05, 2.3610558855580166e-05, 2.3610558855580166e-05, 2.3610558855580166e-05, 2.3610558855580166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3610558855580166e-05

Optimization complete. Final v2v error: 4.018564224243164 mm

Highest mean error: 4.910037994384766 mm for frame 139

Lowest mean error: 3.330274820327759 mm for frame 25

Saving results

Total time: 45.41541123390198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014180
Iteration 2/25 | Loss: 0.00220559
Iteration 3/25 | Loss: 0.00155231
Iteration 4/25 | Loss: 0.00150963
Iteration 5/25 | Loss: 0.00142160
Iteration 6/25 | Loss: 0.00142635
Iteration 7/25 | Loss: 0.00134484
Iteration 8/25 | Loss: 0.00138688
Iteration 9/25 | Loss: 0.00130282
Iteration 10/25 | Loss: 0.00128515
Iteration 11/25 | Loss: 0.00130037
Iteration 12/25 | Loss: 0.00124539
Iteration 13/25 | Loss: 0.00122988
Iteration 14/25 | Loss: 0.00121865
Iteration 15/25 | Loss: 0.00122551
Iteration 16/25 | Loss: 0.00122183
Iteration 17/25 | Loss: 0.00122024
Iteration 18/25 | Loss: 0.00121833
Iteration 19/25 | Loss: 0.00121836
Iteration 20/25 | Loss: 0.00120978
Iteration 21/25 | Loss: 0.00119995
Iteration 22/25 | Loss: 0.00119598
Iteration 23/25 | Loss: 0.00120467
Iteration 24/25 | Loss: 0.00119716
Iteration 25/25 | Loss: 0.00118964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44572604
Iteration 2/25 | Loss: 0.00145550
Iteration 3/25 | Loss: 0.00145549
Iteration 4/25 | Loss: 0.00145549
Iteration 5/25 | Loss: 0.00145549
Iteration 6/25 | Loss: 0.00145549
Iteration 7/25 | Loss: 0.00145549
Iteration 8/25 | Loss: 0.00145549
Iteration 9/25 | Loss: 0.00145549
Iteration 10/25 | Loss: 0.00145549
Iteration 11/25 | Loss: 0.00145549
Iteration 12/25 | Loss: 0.00145549
Iteration 13/25 | Loss: 0.00145549
Iteration 14/25 | Loss: 0.00145549
Iteration 15/25 | Loss: 0.00145549
Iteration 16/25 | Loss: 0.00145549
Iteration 17/25 | Loss: 0.00145549
Iteration 18/25 | Loss: 0.00145549
Iteration 19/25 | Loss: 0.00145549
Iteration 20/25 | Loss: 0.00145549
Iteration 21/25 | Loss: 0.00145549
Iteration 22/25 | Loss: 0.00145549
Iteration 23/25 | Loss: 0.00145549
Iteration 24/25 | Loss: 0.00145549
Iteration 25/25 | Loss: 0.00145549

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145549
Iteration 2/1000 | Loss: 0.00035492
Iteration 3/1000 | Loss: 0.00021350
Iteration 4/1000 | Loss: 0.00005914
Iteration 5/1000 | Loss: 0.00006073
Iteration 6/1000 | Loss: 0.00004632
Iteration 7/1000 | Loss: 0.00004410
Iteration 8/1000 | Loss: 0.00009139
Iteration 9/1000 | Loss: 0.00024801
Iteration 10/1000 | Loss: 0.00023718
Iteration 11/1000 | Loss: 0.00004865
Iteration 12/1000 | Loss: 0.00013091
Iteration 13/1000 | Loss: 0.00023065
Iteration 14/1000 | Loss: 0.00003838
Iteration 15/1000 | Loss: 0.00004585
Iteration 16/1000 | Loss: 0.00003067
Iteration 17/1000 | Loss: 0.00021210
Iteration 18/1000 | Loss: 0.00012059
Iteration 19/1000 | Loss: 0.00012328
Iteration 20/1000 | Loss: 0.00027083
Iteration 21/1000 | Loss: 0.00010806
Iteration 22/1000 | Loss: 0.00040325
Iteration 23/1000 | Loss: 0.00003535
Iteration 24/1000 | Loss: 0.00011819
Iteration 25/1000 | Loss: 0.00002558
Iteration 26/1000 | Loss: 0.00017493
Iteration 27/1000 | Loss: 0.00003732
Iteration 28/1000 | Loss: 0.00002586
Iteration 29/1000 | Loss: 0.00004392
Iteration 30/1000 | Loss: 0.00002560
Iteration 31/1000 | Loss: 0.00002303
Iteration 32/1000 | Loss: 0.00002303
Iteration 33/1000 | Loss: 0.00002092
Iteration 34/1000 | Loss: 0.00002258
Iteration 35/1000 | Loss: 0.00001622
Iteration 36/1000 | Loss: 0.00001849
Iteration 37/1000 | Loss: 0.00004008
Iteration 38/1000 | Loss: 0.00001544
Iteration 39/1000 | Loss: 0.00003473
Iteration 40/1000 | Loss: 0.00005164
Iteration 41/1000 | Loss: 0.00001496
Iteration 42/1000 | Loss: 0.00001476
Iteration 43/1000 | Loss: 0.00001733
Iteration 44/1000 | Loss: 0.00001498
Iteration 45/1000 | Loss: 0.00001463
Iteration 46/1000 | Loss: 0.00001462
Iteration 47/1000 | Loss: 0.00001462
Iteration 48/1000 | Loss: 0.00001462
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001468
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001448
Iteration 53/1000 | Loss: 0.00001448
Iteration 54/1000 | Loss: 0.00001447
Iteration 55/1000 | Loss: 0.00001446
Iteration 56/1000 | Loss: 0.00001446
Iteration 57/1000 | Loss: 0.00001446
Iteration 58/1000 | Loss: 0.00001442
Iteration 59/1000 | Loss: 0.00001548
Iteration 60/1000 | Loss: 0.00001440
Iteration 61/1000 | Loss: 0.00001440
Iteration 62/1000 | Loss: 0.00001440
Iteration 63/1000 | Loss: 0.00001440
Iteration 64/1000 | Loss: 0.00001440
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001451
Iteration 67/1000 | Loss: 0.00001435
Iteration 68/1000 | Loss: 0.00001435
Iteration 69/1000 | Loss: 0.00001435
Iteration 70/1000 | Loss: 0.00001435
Iteration 71/1000 | Loss: 0.00001435
Iteration 72/1000 | Loss: 0.00001435
Iteration 73/1000 | Loss: 0.00001435
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001435
Iteration 77/1000 | Loss: 0.00001435
Iteration 78/1000 | Loss: 0.00001434
Iteration 79/1000 | Loss: 0.00001434
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001431
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001429
Iteration 86/1000 | Loss: 0.00001428
Iteration 87/1000 | Loss: 0.00001428
Iteration 88/1000 | Loss: 0.00001427
Iteration 89/1000 | Loss: 0.00001427
Iteration 90/1000 | Loss: 0.00001427
Iteration 91/1000 | Loss: 0.00001427
Iteration 92/1000 | Loss: 0.00001427
Iteration 93/1000 | Loss: 0.00001427
Iteration 94/1000 | Loss: 0.00001426
Iteration 95/1000 | Loss: 0.00001426
Iteration 96/1000 | Loss: 0.00001426
Iteration 97/1000 | Loss: 0.00001426
Iteration 98/1000 | Loss: 0.00001426
Iteration 99/1000 | Loss: 0.00001426
Iteration 100/1000 | Loss: 0.00001426
Iteration 101/1000 | Loss: 0.00001426
Iteration 102/1000 | Loss: 0.00001426
Iteration 103/1000 | Loss: 0.00001426
Iteration 104/1000 | Loss: 0.00001426
Iteration 105/1000 | Loss: 0.00001426
Iteration 106/1000 | Loss: 0.00001426
Iteration 107/1000 | Loss: 0.00002276
Iteration 108/1000 | Loss: 0.00002148
Iteration 109/1000 | Loss: 0.00001598
Iteration 110/1000 | Loss: 0.00001553
Iteration 111/1000 | Loss: 0.00001696
Iteration 112/1000 | Loss: 0.00001424
Iteration 113/1000 | Loss: 0.00001422
Iteration 114/1000 | Loss: 0.00001421
Iteration 115/1000 | Loss: 0.00001421
Iteration 116/1000 | Loss: 0.00001421
Iteration 117/1000 | Loss: 0.00001421
Iteration 118/1000 | Loss: 0.00001421
Iteration 119/1000 | Loss: 0.00001421
Iteration 120/1000 | Loss: 0.00001421
Iteration 121/1000 | Loss: 0.00001421
Iteration 122/1000 | Loss: 0.00001421
Iteration 123/1000 | Loss: 0.00001421
Iteration 124/1000 | Loss: 0.00001421
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001420
Iteration 127/1000 | Loss: 0.00001420
Iteration 128/1000 | Loss: 0.00001420
Iteration 129/1000 | Loss: 0.00001420
Iteration 130/1000 | Loss: 0.00001420
Iteration 131/1000 | Loss: 0.00001420
Iteration 132/1000 | Loss: 0.00001420
Iteration 133/1000 | Loss: 0.00001420
Iteration 134/1000 | Loss: 0.00001420
Iteration 135/1000 | Loss: 0.00001420
Iteration 136/1000 | Loss: 0.00001420
Iteration 137/1000 | Loss: 0.00001420
Iteration 138/1000 | Loss: 0.00001420
Iteration 139/1000 | Loss: 0.00001420
Iteration 140/1000 | Loss: 0.00001420
Iteration 141/1000 | Loss: 0.00001420
Iteration 142/1000 | Loss: 0.00001420
Iteration 143/1000 | Loss: 0.00001420
Iteration 144/1000 | Loss: 0.00001420
Iteration 145/1000 | Loss: 0.00001420
Iteration 146/1000 | Loss: 0.00001420
Iteration 147/1000 | Loss: 0.00001420
Iteration 148/1000 | Loss: 0.00001420
Iteration 149/1000 | Loss: 0.00001420
Iteration 150/1000 | Loss: 0.00001420
Iteration 151/1000 | Loss: 0.00001420
Iteration 152/1000 | Loss: 0.00001420
Iteration 153/1000 | Loss: 0.00001420
Iteration 154/1000 | Loss: 0.00001420
Iteration 155/1000 | Loss: 0.00001420
Iteration 156/1000 | Loss: 0.00001420
Iteration 157/1000 | Loss: 0.00001420
Iteration 158/1000 | Loss: 0.00001420
Iteration 159/1000 | Loss: 0.00001420
Iteration 160/1000 | Loss: 0.00001420
Iteration 161/1000 | Loss: 0.00001420
Iteration 162/1000 | Loss: 0.00001420
Iteration 163/1000 | Loss: 0.00001420
Iteration 164/1000 | Loss: 0.00001420
Iteration 165/1000 | Loss: 0.00001420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.4199422366800718e-05, 1.4199422366800718e-05, 1.4199422366800718e-05, 1.4199422366800718e-05, 1.4199422366800718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4199422366800718e-05

Optimization complete. Final v2v error: 2.9536643028259277 mm

Highest mean error: 11.056038856506348 mm for frame 95

Lowest mean error: 2.5092523097991943 mm for frame 44

Saving results

Total time: 123.38012933731079
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00628205
Iteration 2/25 | Loss: 0.00131648
Iteration 3/25 | Loss: 0.00117559
Iteration 4/25 | Loss: 0.00115095
Iteration 5/25 | Loss: 0.00114350
Iteration 6/25 | Loss: 0.00114172
Iteration 7/25 | Loss: 0.00114172
Iteration 8/25 | Loss: 0.00114172
Iteration 9/25 | Loss: 0.00114172
Iteration 10/25 | Loss: 0.00114172
Iteration 11/25 | Loss: 0.00114172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011417183559387922, 0.0011417183559387922, 0.0011417183559387922, 0.0011417183559387922, 0.0011417183559387922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011417183559387922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38677526
Iteration 2/25 | Loss: 0.00110900
Iteration 3/25 | Loss: 0.00110900
Iteration 4/25 | Loss: 0.00110900
Iteration 5/25 | Loss: 0.00110900
Iteration 6/25 | Loss: 0.00110900
Iteration 7/25 | Loss: 0.00110900
Iteration 8/25 | Loss: 0.00110900
Iteration 9/25 | Loss: 0.00110900
Iteration 10/25 | Loss: 0.00110900
Iteration 11/25 | Loss: 0.00110900
Iteration 12/25 | Loss: 0.00110900
Iteration 13/25 | Loss: 0.00110900
Iteration 14/25 | Loss: 0.00110900
Iteration 15/25 | Loss: 0.00110900
Iteration 16/25 | Loss: 0.00110900
Iteration 17/25 | Loss: 0.00110900
Iteration 18/25 | Loss: 0.00110900
Iteration 19/25 | Loss: 0.00110900
Iteration 20/25 | Loss: 0.00110900
Iteration 21/25 | Loss: 0.00110900
Iteration 22/25 | Loss: 0.00110900
Iteration 23/25 | Loss: 0.00110900
Iteration 24/25 | Loss: 0.00110900
Iteration 25/25 | Loss: 0.00110900

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110900
Iteration 2/1000 | Loss: 0.00003779
Iteration 3/1000 | Loss: 0.00002275
Iteration 4/1000 | Loss: 0.00001718
Iteration 5/1000 | Loss: 0.00001553
Iteration 6/1000 | Loss: 0.00001431
Iteration 7/1000 | Loss: 0.00001355
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001258
Iteration 10/1000 | Loss: 0.00001234
Iteration 11/1000 | Loss: 0.00001232
Iteration 12/1000 | Loss: 0.00001212
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001197
Iteration 15/1000 | Loss: 0.00001196
Iteration 16/1000 | Loss: 0.00001196
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001186
Iteration 19/1000 | Loss: 0.00001185
Iteration 20/1000 | Loss: 0.00001183
Iteration 21/1000 | Loss: 0.00001182
Iteration 22/1000 | Loss: 0.00001181
Iteration 23/1000 | Loss: 0.00001181
Iteration 24/1000 | Loss: 0.00001180
Iteration 25/1000 | Loss: 0.00001178
Iteration 26/1000 | Loss: 0.00001177
Iteration 27/1000 | Loss: 0.00001177
Iteration 28/1000 | Loss: 0.00001176
Iteration 29/1000 | Loss: 0.00001175
Iteration 30/1000 | Loss: 0.00001174
Iteration 31/1000 | Loss: 0.00001172
Iteration 32/1000 | Loss: 0.00001171
Iteration 33/1000 | Loss: 0.00001170
Iteration 34/1000 | Loss: 0.00001170
Iteration 35/1000 | Loss: 0.00001169
Iteration 36/1000 | Loss: 0.00001169
Iteration 37/1000 | Loss: 0.00001168
Iteration 38/1000 | Loss: 0.00001168
Iteration 39/1000 | Loss: 0.00001165
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001163
Iteration 42/1000 | Loss: 0.00001163
Iteration 43/1000 | Loss: 0.00001163
Iteration 44/1000 | Loss: 0.00001163
Iteration 45/1000 | Loss: 0.00001162
Iteration 46/1000 | Loss: 0.00001162
Iteration 47/1000 | Loss: 0.00001162
Iteration 48/1000 | Loss: 0.00001161
Iteration 49/1000 | Loss: 0.00001161
Iteration 50/1000 | Loss: 0.00001160
Iteration 51/1000 | Loss: 0.00001160
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00001160
Iteration 54/1000 | Loss: 0.00001159
Iteration 55/1000 | Loss: 0.00001159
Iteration 56/1000 | Loss: 0.00001159
Iteration 57/1000 | Loss: 0.00001159
Iteration 58/1000 | Loss: 0.00001159
Iteration 59/1000 | Loss: 0.00001158
Iteration 60/1000 | Loss: 0.00001158
Iteration 61/1000 | Loss: 0.00001158
Iteration 62/1000 | Loss: 0.00001158
Iteration 63/1000 | Loss: 0.00001158
Iteration 64/1000 | Loss: 0.00001158
Iteration 65/1000 | Loss: 0.00001158
Iteration 66/1000 | Loss: 0.00001157
Iteration 67/1000 | Loss: 0.00001157
Iteration 68/1000 | Loss: 0.00001157
Iteration 69/1000 | Loss: 0.00001157
Iteration 70/1000 | Loss: 0.00001156
Iteration 71/1000 | Loss: 0.00001156
Iteration 72/1000 | Loss: 0.00001156
Iteration 73/1000 | Loss: 0.00001156
Iteration 74/1000 | Loss: 0.00001155
Iteration 75/1000 | Loss: 0.00001155
Iteration 76/1000 | Loss: 0.00001155
Iteration 77/1000 | Loss: 0.00001155
Iteration 78/1000 | Loss: 0.00001155
Iteration 79/1000 | Loss: 0.00001154
Iteration 80/1000 | Loss: 0.00001154
Iteration 81/1000 | Loss: 0.00001154
Iteration 82/1000 | Loss: 0.00001154
Iteration 83/1000 | Loss: 0.00001154
Iteration 84/1000 | Loss: 0.00001154
Iteration 85/1000 | Loss: 0.00001154
Iteration 86/1000 | Loss: 0.00001154
Iteration 87/1000 | Loss: 0.00001154
Iteration 88/1000 | Loss: 0.00001154
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001153
Iteration 93/1000 | Loss: 0.00001153
Iteration 94/1000 | Loss: 0.00001153
Iteration 95/1000 | Loss: 0.00001153
Iteration 96/1000 | Loss: 0.00001152
Iteration 97/1000 | Loss: 0.00001152
Iteration 98/1000 | Loss: 0.00001152
Iteration 99/1000 | Loss: 0.00001152
Iteration 100/1000 | Loss: 0.00001152
Iteration 101/1000 | Loss: 0.00001151
Iteration 102/1000 | Loss: 0.00001151
Iteration 103/1000 | Loss: 0.00001151
Iteration 104/1000 | Loss: 0.00001151
Iteration 105/1000 | Loss: 0.00001150
Iteration 106/1000 | Loss: 0.00001150
Iteration 107/1000 | Loss: 0.00001150
Iteration 108/1000 | Loss: 0.00001150
Iteration 109/1000 | Loss: 0.00001150
Iteration 110/1000 | Loss: 0.00001150
Iteration 111/1000 | Loss: 0.00001150
Iteration 112/1000 | Loss: 0.00001150
Iteration 113/1000 | Loss: 0.00001149
Iteration 114/1000 | Loss: 0.00001149
Iteration 115/1000 | Loss: 0.00001149
Iteration 116/1000 | Loss: 0.00001149
Iteration 117/1000 | Loss: 0.00001149
Iteration 118/1000 | Loss: 0.00001148
Iteration 119/1000 | Loss: 0.00001148
Iteration 120/1000 | Loss: 0.00001148
Iteration 121/1000 | Loss: 0.00001148
Iteration 122/1000 | Loss: 0.00001148
Iteration 123/1000 | Loss: 0.00001148
Iteration 124/1000 | Loss: 0.00001148
Iteration 125/1000 | Loss: 0.00001147
Iteration 126/1000 | Loss: 0.00001147
Iteration 127/1000 | Loss: 0.00001147
Iteration 128/1000 | Loss: 0.00001147
Iteration 129/1000 | Loss: 0.00001147
Iteration 130/1000 | Loss: 0.00001147
Iteration 131/1000 | Loss: 0.00001147
Iteration 132/1000 | Loss: 0.00001147
Iteration 133/1000 | Loss: 0.00001147
Iteration 134/1000 | Loss: 0.00001147
Iteration 135/1000 | Loss: 0.00001147
Iteration 136/1000 | Loss: 0.00001146
Iteration 137/1000 | Loss: 0.00001146
Iteration 138/1000 | Loss: 0.00001146
Iteration 139/1000 | Loss: 0.00001146
Iteration 140/1000 | Loss: 0.00001146
Iteration 141/1000 | Loss: 0.00001146
Iteration 142/1000 | Loss: 0.00001146
Iteration 143/1000 | Loss: 0.00001146
Iteration 144/1000 | Loss: 0.00001146
Iteration 145/1000 | Loss: 0.00001146
Iteration 146/1000 | Loss: 0.00001145
Iteration 147/1000 | Loss: 0.00001145
Iteration 148/1000 | Loss: 0.00001145
Iteration 149/1000 | Loss: 0.00001145
Iteration 150/1000 | Loss: 0.00001145
Iteration 151/1000 | Loss: 0.00001145
Iteration 152/1000 | Loss: 0.00001144
Iteration 153/1000 | Loss: 0.00001144
Iteration 154/1000 | Loss: 0.00001144
Iteration 155/1000 | Loss: 0.00001144
Iteration 156/1000 | Loss: 0.00001144
Iteration 157/1000 | Loss: 0.00001144
Iteration 158/1000 | Loss: 0.00001144
Iteration 159/1000 | Loss: 0.00001143
Iteration 160/1000 | Loss: 0.00001143
Iteration 161/1000 | Loss: 0.00001143
Iteration 162/1000 | Loss: 0.00001143
Iteration 163/1000 | Loss: 0.00001142
Iteration 164/1000 | Loss: 0.00001142
Iteration 165/1000 | Loss: 0.00001142
Iteration 166/1000 | Loss: 0.00001142
Iteration 167/1000 | Loss: 0.00001142
Iteration 168/1000 | Loss: 0.00001142
Iteration 169/1000 | Loss: 0.00001142
Iteration 170/1000 | Loss: 0.00001142
Iteration 171/1000 | Loss: 0.00001142
Iteration 172/1000 | Loss: 0.00001142
Iteration 173/1000 | Loss: 0.00001142
Iteration 174/1000 | Loss: 0.00001142
Iteration 175/1000 | Loss: 0.00001142
Iteration 176/1000 | Loss: 0.00001142
Iteration 177/1000 | Loss: 0.00001142
Iteration 178/1000 | Loss: 0.00001142
Iteration 179/1000 | Loss: 0.00001142
Iteration 180/1000 | Loss: 0.00001142
Iteration 181/1000 | Loss: 0.00001142
Iteration 182/1000 | Loss: 0.00001142
Iteration 183/1000 | Loss: 0.00001142
Iteration 184/1000 | Loss: 0.00001142
Iteration 185/1000 | Loss: 0.00001142
Iteration 186/1000 | Loss: 0.00001142
Iteration 187/1000 | Loss: 0.00001142
Iteration 188/1000 | Loss: 0.00001142
Iteration 189/1000 | Loss: 0.00001142
Iteration 190/1000 | Loss: 0.00001142
Iteration 191/1000 | Loss: 0.00001142
Iteration 192/1000 | Loss: 0.00001142
Iteration 193/1000 | Loss: 0.00001142
Iteration 194/1000 | Loss: 0.00001142
Iteration 195/1000 | Loss: 0.00001142
Iteration 196/1000 | Loss: 0.00001142
Iteration 197/1000 | Loss: 0.00001142
Iteration 198/1000 | Loss: 0.00001142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.1419280781410635e-05, 1.1419280781410635e-05, 1.1419280781410635e-05, 1.1419280781410635e-05, 1.1419280781410635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1419280781410635e-05

Optimization complete. Final v2v error: 2.867781639099121 mm

Highest mean error: 3.4877569675445557 mm for frame 74

Lowest mean error: 2.4844436645507812 mm for frame 28

Saving results

Total time: 40.59602499008179
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531383
Iteration 2/25 | Loss: 0.00128187
Iteration 3/25 | Loss: 0.00118934
Iteration 4/25 | Loss: 0.00117959
Iteration 5/25 | Loss: 0.00117774
Iteration 6/25 | Loss: 0.00117712
Iteration 7/25 | Loss: 0.00117708
Iteration 8/25 | Loss: 0.00117708
Iteration 9/25 | Loss: 0.00117708
Iteration 10/25 | Loss: 0.00117708
Iteration 11/25 | Loss: 0.00117708
Iteration 12/25 | Loss: 0.00117708
Iteration 13/25 | Loss: 0.00117708
Iteration 14/25 | Loss: 0.00117708
Iteration 15/25 | Loss: 0.00117708
Iteration 16/25 | Loss: 0.00117708
Iteration 17/25 | Loss: 0.00117708
Iteration 18/25 | Loss: 0.00117708
Iteration 19/25 | Loss: 0.00117708
Iteration 20/25 | Loss: 0.00117708
Iteration 21/25 | Loss: 0.00117708
Iteration 22/25 | Loss: 0.00117708
Iteration 23/25 | Loss: 0.00117708
Iteration 24/25 | Loss: 0.00117708
Iteration 25/25 | Loss: 0.00117708

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.08701301
Iteration 2/25 | Loss: 0.00101818
Iteration 3/25 | Loss: 0.00101818
Iteration 4/25 | Loss: 0.00101818
Iteration 5/25 | Loss: 0.00101818
Iteration 6/25 | Loss: 0.00101818
Iteration 7/25 | Loss: 0.00101818
Iteration 8/25 | Loss: 0.00101818
Iteration 9/25 | Loss: 0.00101818
Iteration 10/25 | Loss: 0.00101818
Iteration 11/25 | Loss: 0.00101818
Iteration 12/25 | Loss: 0.00101818
Iteration 13/25 | Loss: 0.00101818
Iteration 14/25 | Loss: 0.00101818
Iteration 15/25 | Loss: 0.00101818
Iteration 16/25 | Loss: 0.00101818
Iteration 17/25 | Loss: 0.00101818
Iteration 18/25 | Loss: 0.00101818
Iteration 19/25 | Loss: 0.00101818
Iteration 20/25 | Loss: 0.00101818
Iteration 21/25 | Loss: 0.00101818
Iteration 22/25 | Loss: 0.00101818
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001018176437355578, 0.001018176437355578, 0.001018176437355578, 0.001018176437355578, 0.001018176437355578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001018176437355578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101818
Iteration 2/1000 | Loss: 0.00003094
Iteration 3/1000 | Loss: 0.00001943
Iteration 4/1000 | Loss: 0.00001508
Iteration 5/1000 | Loss: 0.00001420
Iteration 6/1000 | Loss: 0.00001367
Iteration 7/1000 | Loss: 0.00001332
Iteration 8/1000 | Loss: 0.00001301
Iteration 9/1000 | Loss: 0.00001282
Iteration 10/1000 | Loss: 0.00001282
Iteration 11/1000 | Loss: 0.00001277
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001275
Iteration 14/1000 | Loss: 0.00001274
Iteration 15/1000 | Loss: 0.00001274
Iteration 16/1000 | Loss: 0.00001273
Iteration 17/1000 | Loss: 0.00001255
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001252
Iteration 20/1000 | Loss: 0.00001251
Iteration 21/1000 | Loss: 0.00001251
Iteration 22/1000 | Loss: 0.00001250
Iteration 23/1000 | Loss: 0.00001247
Iteration 24/1000 | Loss: 0.00001247
Iteration 25/1000 | Loss: 0.00001243
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001235
Iteration 28/1000 | Loss: 0.00001233
Iteration 29/1000 | Loss: 0.00001233
Iteration 30/1000 | Loss: 0.00001233
Iteration 31/1000 | Loss: 0.00001232
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001231
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001229
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001228
Iteration 38/1000 | Loss: 0.00001228
Iteration 39/1000 | Loss: 0.00001227
Iteration 40/1000 | Loss: 0.00001225
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001222
Iteration 43/1000 | Loss: 0.00001222
Iteration 44/1000 | Loss: 0.00001221
Iteration 45/1000 | Loss: 0.00001221
Iteration 46/1000 | Loss: 0.00001220
Iteration 47/1000 | Loss: 0.00001219
Iteration 48/1000 | Loss: 0.00001219
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001219
Iteration 51/1000 | Loss: 0.00001219
Iteration 52/1000 | Loss: 0.00001219
Iteration 53/1000 | Loss: 0.00001219
Iteration 54/1000 | Loss: 0.00001219
Iteration 55/1000 | Loss: 0.00001219
Iteration 56/1000 | Loss: 0.00001219
Iteration 57/1000 | Loss: 0.00001218
Iteration 58/1000 | Loss: 0.00001218
Iteration 59/1000 | Loss: 0.00001218
Iteration 60/1000 | Loss: 0.00001217
Iteration 61/1000 | Loss: 0.00001217
Iteration 62/1000 | Loss: 0.00001216
Iteration 63/1000 | Loss: 0.00001216
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001216
Iteration 69/1000 | Loss: 0.00001216
Iteration 70/1000 | Loss: 0.00001215
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001215
Iteration 73/1000 | Loss: 0.00001215
Iteration 74/1000 | Loss: 0.00001215
Iteration 75/1000 | Loss: 0.00001215
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001214
Iteration 79/1000 | Loss: 0.00001214
Iteration 80/1000 | Loss: 0.00001214
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001213
Iteration 83/1000 | Loss: 0.00001213
Iteration 84/1000 | Loss: 0.00001213
Iteration 85/1000 | Loss: 0.00001213
Iteration 86/1000 | Loss: 0.00001213
Iteration 87/1000 | Loss: 0.00001213
Iteration 88/1000 | Loss: 0.00001213
Iteration 89/1000 | Loss: 0.00001213
Iteration 90/1000 | Loss: 0.00001213
Iteration 91/1000 | Loss: 0.00001212
Iteration 92/1000 | Loss: 0.00001212
Iteration 93/1000 | Loss: 0.00001212
Iteration 94/1000 | Loss: 0.00001212
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001211
Iteration 97/1000 | Loss: 0.00001211
Iteration 98/1000 | Loss: 0.00001211
Iteration 99/1000 | Loss: 0.00001211
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001210
Iteration 103/1000 | Loss: 0.00001210
Iteration 104/1000 | Loss: 0.00001210
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001210
Iteration 109/1000 | Loss: 0.00001210
Iteration 110/1000 | Loss: 0.00001210
Iteration 111/1000 | Loss: 0.00001210
Iteration 112/1000 | Loss: 0.00001210
Iteration 113/1000 | Loss: 0.00001210
Iteration 114/1000 | Loss: 0.00001210
Iteration 115/1000 | Loss: 0.00001209
Iteration 116/1000 | Loss: 0.00001209
Iteration 117/1000 | Loss: 0.00001209
Iteration 118/1000 | Loss: 0.00001209
Iteration 119/1000 | Loss: 0.00001209
Iteration 120/1000 | Loss: 0.00001209
Iteration 121/1000 | Loss: 0.00001209
Iteration 122/1000 | Loss: 0.00001209
Iteration 123/1000 | Loss: 0.00001208
Iteration 124/1000 | Loss: 0.00001208
Iteration 125/1000 | Loss: 0.00001208
Iteration 126/1000 | Loss: 0.00001208
Iteration 127/1000 | Loss: 0.00001208
Iteration 128/1000 | Loss: 0.00001208
Iteration 129/1000 | Loss: 0.00001208
Iteration 130/1000 | Loss: 0.00001207
Iteration 131/1000 | Loss: 0.00001207
Iteration 132/1000 | Loss: 0.00001207
Iteration 133/1000 | Loss: 0.00001207
Iteration 134/1000 | Loss: 0.00001207
Iteration 135/1000 | Loss: 0.00001207
Iteration 136/1000 | Loss: 0.00001207
Iteration 137/1000 | Loss: 0.00001207
Iteration 138/1000 | Loss: 0.00001207
Iteration 139/1000 | Loss: 0.00001207
Iteration 140/1000 | Loss: 0.00001207
Iteration 141/1000 | Loss: 0.00001207
Iteration 142/1000 | Loss: 0.00001207
Iteration 143/1000 | Loss: 0.00001206
Iteration 144/1000 | Loss: 0.00001206
Iteration 145/1000 | Loss: 0.00001206
Iteration 146/1000 | Loss: 0.00001206
Iteration 147/1000 | Loss: 0.00001206
Iteration 148/1000 | Loss: 0.00001206
Iteration 149/1000 | Loss: 0.00001206
Iteration 150/1000 | Loss: 0.00001206
Iteration 151/1000 | Loss: 0.00001206
Iteration 152/1000 | Loss: 0.00001206
Iteration 153/1000 | Loss: 0.00001206
Iteration 154/1000 | Loss: 0.00001206
Iteration 155/1000 | Loss: 0.00001206
Iteration 156/1000 | Loss: 0.00001205
Iteration 157/1000 | Loss: 0.00001205
Iteration 158/1000 | Loss: 0.00001205
Iteration 159/1000 | Loss: 0.00001205
Iteration 160/1000 | Loss: 0.00001205
Iteration 161/1000 | Loss: 0.00001205
Iteration 162/1000 | Loss: 0.00001205
Iteration 163/1000 | Loss: 0.00001205
Iteration 164/1000 | Loss: 0.00001205
Iteration 165/1000 | Loss: 0.00001205
Iteration 166/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.205214448418701e-05, 1.205214448418701e-05, 1.205214448418701e-05, 1.205214448418701e-05, 1.205214448418701e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.205214448418701e-05

Optimization complete. Final v2v error: 2.9408040046691895 mm

Highest mean error: 3.5824153423309326 mm for frame 63

Lowest mean error: 2.65690279006958 mm for frame 1

Saving results

Total time: 37.042619466781616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00693426
Iteration 2/25 | Loss: 0.00150165
Iteration 3/25 | Loss: 0.00126352
Iteration 4/25 | Loss: 0.00124118
Iteration 5/25 | Loss: 0.00126296
Iteration 6/25 | Loss: 0.00124102
Iteration 7/25 | Loss: 0.00123038
Iteration 8/25 | Loss: 0.00121560
Iteration 9/25 | Loss: 0.00121002
Iteration 10/25 | Loss: 0.00121636
Iteration 11/25 | Loss: 0.00121226
Iteration 12/25 | Loss: 0.00120787
Iteration 13/25 | Loss: 0.00120740
Iteration 14/25 | Loss: 0.00120726
Iteration 15/25 | Loss: 0.00120726
Iteration 16/25 | Loss: 0.00120726
Iteration 17/25 | Loss: 0.00120726
Iteration 18/25 | Loss: 0.00120726
Iteration 19/25 | Loss: 0.00120726
Iteration 20/25 | Loss: 0.00120726
Iteration 21/25 | Loss: 0.00120726
Iteration 22/25 | Loss: 0.00120726
Iteration 23/25 | Loss: 0.00120726
Iteration 24/25 | Loss: 0.00120726
Iteration 25/25 | Loss: 0.00120726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.63896847
Iteration 2/25 | Loss: 0.00065228
Iteration 3/25 | Loss: 0.00065219
Iteration 4/25 | Loss: 0.00065219
Iteration 5/25 | Loss: 0.00065219
Iteration 6/25 | Loss: 0.00065219
Iteration 7/25 | Loss: 0.00065219
Iteration 8/25 | Loss: 0.00065219
Iteration 9/25 | Loss: 0.00065219
Iteration 10/25 | Loss: 0.00065219
Iteration 11/25 | Loss: 0.00065219
Iteration 12/25 | Loss: 0.00065219
Iteration 13/25 | Loss: 0.00065219
Iteration 14/25 | Loss: 0.00065219
Iteration 15/25 | Loss: 0.00065219
Iteration 16/25 | Loss: 0.00065219
Iteration 17/25 | Loss: 0.00065219
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006521912873722613, 0.0006521912873722613, 0.0006521912873722613, 0.0006521912873722613, 0.0006521912873722613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006521912873722613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065219
Iteration 2/1000 | Loss: 0.00003213
Iteration 3/1000 | Loss: 0.00001943
Iteration 4/1000 | Loss: 0.00001687
Iteration 5/1000 | Loss: 0.00001607
Iteration 6/1000 | Loss: 0.00001540
Iteration 7/1000 | Loss: 0.00001495
Iteration 8/1000 | Loss: 0.00001463
Iteration 9/1000 | Loss: 0.00001444
Iteration 10/1000 | Loss: 0.00001426
Iteration 11/1000 | Loss: 0.00001413
Iteration 12/1000 | Loss: 0.00001411
Iteration 13/1000 | Loss: 0.00001408
Iteration 14/1000 | Loss: 0.00001407
Iteration 15/1000 | Loss: 0.00001407
Iteration 16/1000 | Loss: 0.00001404
Iteration 17/1000 | Loss: 0.00001403
Iteration 18/1000 | Loss: 0.00001398
Iteration 19/1000 | Loss: 0.00001396
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001396
Iteration 22/1000 | Loss: 0.00001395
Iteration 23/1000 | Loss: 0.00001395
Iteration 24/1000 | Loss: 0.00001393
Iteration 25/1000 | Loss: 0.00001393
Iteration 26/1000 | Loss: 0.00001392
Iteration 27/1000 | Loss: 0.00001392
Iteration 28/1000 | Loss: 0.00001388
Iteration 29/1000 | Loss: 0.00001388
Iteration 30/1000 | Loss: 0.00001388
Iteration 31/1000 | Loss: 0.00001387
Iteration 32/1000 | Loss: 0.00001387
Iteration 33/1000 | Loss: 0.00001386
Iteration 34/1000 | Loss: 0.00001386
Iteration 35/1000 | Loss: 0.00001385
Iteration 36/1000 | Loss: 0.00001385
Iteration 37/1000 | Loss: 0.00001384
Iteration 38/1000 | Loss: 0.00001384
Iteration 39/1000 | Loss: 0.00001384
Iteration 40/1000 | Loss: 0.00001384
Iteration 41/1000 | Loss: 0.00001384
Iteration 42/1000 | Loss: 0.00001384
Iteration 43/1000 | Loss: 0.00001383
Iteration 44/1000 | Loss: 0.00001383
Iteration 45/1000 | Loss: 0.00001383
Iteration 46/1000 | Loss: 0.00001383
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001382
Iteration 49/1000 | Loss: 0.00001382
Iteration 50/1000 | Loss: 0.00001382
Iteration 51/1000 | Loss: 0.00001381
Iteration 52/1000 | Loss: 0.00001381
Iteration 53/1000 | Loss: 0.00001380
Iteration 54/1000 | Loss: 0.00001380
Iteration 55/1000 | Loss: 0.00001380
Iteration 56/1000 | Loss: 0.00001380
Iteration 57/1000 | Loss: 0.00001380
Iteration 58/1000 | Loss: 0.00001380
Iteration 59/1000 | Loss: 0.00001379
Iteration 60/1000 | Loss: 0.00001379
Iteration 61/1000 | Loss: 0.00001378
Iteration 62/1000 | Loss: 0.00001378
Iteration 63/1000 | Loss: 0.00001377
Iteration 64/1000 | Loss: 0.00001377
Iteration 65/1000 | Loss: 0.00001377
Iteration 66/1000 | Loss: 0.00001377
Iteration 67/1000 | Loss: 0.00001377
Iteration 68/1000 | Loss: 0.00001376
Iteration 69/1000 | Loss: 0.00001376
Iteration 70/1000 | Loss: 0.00001376
Iteration 71/1000 | Loss: 0.00001376
Iteration 72/1000 | Loss: 0.00001376
Iteration 73/1000 | Loss: 0.00001376
Iteration 74/1000 | Loss: 0.00001375
Iteration 75/1000 | Loss: 0.00001375
Iteration 76/1000 | Loss: 0.00001374
Iteration 77/1000 | Loss: 0.00001373
Iteration 78/1000 | Loss: 0.00001373
Iteration 79/1000 | Loss: 0.00001373
Iteration 80/1000 | Loss: 0.00001373
Iteration 81/1000 | Loss: 0.00001373
Iteration 82/1000 | Loss: 0.00001373
Iteration 83/1000 | Loss: 0.00001373
Iteration 84/1000 | Loss: 0.00001372
Iteration 85/1000 | Loss: 0.00001372
Iteration 86/1000 | Loss: 0.00001372
Iteration 87/1000 | Loss: 0.00001372
Iteration 88/1000 | Loss: 0.00001372
Iteration 89/1000 | Loss: 0.00001371
Iteration 90/1000 | Loss: 0.00001371
Iteration 91/1000 | Loss: 0.00001371
Iteration 92/1000 | Loss: 0.00001371
Iteration 93/1000 | Loss: 0.00001371
Iteration 94/1000 | Loss: 0.00001370
Iteration 95/1000 | Loss: 0.00001370
Iteration 96/1000 | Loss: 0.00001370
Iteration 97/1000 | Loss: 0.00001370
Iteration 98/1000 | Loss: 0.00001370
Iteration 99/1000 | Loss: 0.00001370
Iteration 100/1000 | Loss: 0.00001370
Iteration 101/1000 | Loss: 0.00001370
Iteration 102/1000 | Loss: 0.00001370
Iteration 103/1000 | Loss: 0.00001370
Iteration 104/1000 | Loss: 0.00001370
Iteration 105/1000 | Loss: 0.00001370
Iteration 106/1000 | Loss: 0.00001370
Iteration 107/1000 | Loss: 0.00001370
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001370
Iteration 111/1000 | Loss: 0.00001370
Iteration 112/1000 | Loss: 0.00001370
Iteration 113/1000 | Loss: 0.00001370
Iteration 114/1000 | Loss: 0.00001370
Iteration 115/1000 | Loss: 0.00001370
Iteration 116/1000 | Loss: 0.00001370
Iteration 117/1000 | Loss: 0.00001370
Iteration 118/1000 | Loss: 0.00001370
Iteration 119/1000 | Loss: 0.00001370
Iteration 120/1000 | Loss: 0.00001370
Iteration 121/1000 | Loss: 0.00001370
Iteration 122/1000 | Loss: 0.00001370
Iteration 123/1000 | Loss: 0.00001370
Iteration 124/1000 | Loss: 0.00001370
Iteration 125/1000 | Loss: 0.00001370
Iteration 126/1000 | Loss: 0.00001370
Iteration 127/1000 | Loss: 0.00001370
Iteration 128/1000 | Loss: 0.00001370
Iteration 129/1000 | Loss: 0.00001370
Iteration 130/1000 | Loss: 0.00001370
Iteration 131/1000 | Loss: 0.00001370
Iteration 132/1000 | Loss: 0.00001370
Iteration 133/1000 | Loss: 0.00001370
Iteration 134/1000 | Loss: 0.00001370
Iteration 135/1000 | Loss: 0.00001370
Iteration 136/1000 | Loss: 0.00001370
Iteration 137/1000 | Loss: 0.00001370
Iteration 138/1000 | Loss: 0.00001370
Iteration 139/1000 | Loss: 0.00001370
Iteration 140/1000 | Loss: 0.00001370
Iteration 141/1000 | Loss: 0.00001370
Iteration 142/1000 | Loss: 0.00001370
Iteration 143/1000 | Loss: 0.00001370
Iteration 144/1000 | Loss: 0.00001370
Iteration 145/1000 | Loss: 0.00001370
Iteration 146/1000 | Loss: 0.00001370
Iteration 147/1000 | Loss: 0.00001370
Iteration 148/1000 | Loss: 0.00001370
Iteration 149/1000 | Loss: 0.00001370
Iteration 150/1000 | Loss: 0.00001370
Iteration 151/1000 | Loss: 0.00001370
Iteration 152/1000 | Loss: 0.00001370
Iteration 153/1000 | Loss: 0.00001370
Iteration 154/1000 | Loss: 0.00001370
Iteration 155/1000 | Loss: 0.00001370
Iteration 156/1000 | Loss: 0.00001370
Iteration 157/1000 | Loss: 0.00001370
Iteration 158/1000 | Loss: 0.00001370
Iteration 159/1000 | Loss: 0.00001370
Iteration 160/1000 | Loss: 0.00001370
Iteration 161/1000 | Loss: 0.00001370
Iteration 162/1000 | Loss: 0.00001370
Iteration 163/1000 | Loss: 0.00001370
Iteration 164/1000 | Loss: 0.00001370
Iteration 165/1000 | Loss: 0.00001370
Iteration 166/1000 | Loss: 0.00001370
Iteration 167/1000 | Loss: 0.00001370
Iteration 168/1000 | Loss: 0.00001370
Iteration 169/1000 | Loss: 0.00001370
Iteration 170/1000 | Loss: 0.00001370
Iteration 171/1000 | Loss: 0.00001370
Iteration 172/1000 | Loss: 0.00001370
Iteration 173/1000 | Loss: 0.00001370
Iteration 174/1000 | Loss: 0.00001370
Iteration 175/1000 | Loss: 0.00001370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.3700608178623952e-05, 1.3700608178623952e-05, 1.3700608178623952e-05, 1.3700608178623952e-05, 1.3700608178623952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3700608178623952e-05

Optimization complete. Final v2v error: 3.143298387527466 mm

Highest mean error: 3.955497980117798 mm for frame 3

Lowest mean error: 2.8541526794433594 mm for frame 94

Saving results

Total time: 49.8387234210968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876498
Iteration 2/25 | Loss: 0.00288572
Iteration 3/25 | Loss: 0.00189218
Iteration 4/25 | Loss: 0.00158297
Iteration 5/25 | Loss: 0.00152002
Iteration 6/25 | Loss: 0.00152650
Iteration 7/25 | Loss: 0.00151990
Iteration 8/25 | Loss: 0.00148836
Iteration 9/25 | Loss: 0.00149192
Iteration 10/25 | Loss: 0.00148278
Iteration 11/25 | Loss: 0.00148772
Iteration 12/25 | Loss: 0.00147875
Iteration 13/25 | Loss: 0.00147144
Iteration 14/25 | Loss: 0.00147093
Iteration 15/25 | Loss: 0.00146636
Iteration 16/25 | Loss: 0.00145492
Iteration 17/25 | Loss: 0.00145626
Iteration 18/25 | Loss: 0.00144358
Iteration 19/25 | Loss: 0.00144463
Iteration 20/25 | Loss: 0.00144225
Iteration 21/25 | Loss: 0.00144689
Iteration 22/25 | Loss: 0.00144688
Iteration 23/25 | Loss: 0.00144026
Iteration 24/25 | Loss: 0.00143560
Iteration 25/25 | Loss: 0.00143081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.04314137
Iteration 2/25 | Loss: 0.00414773
Iteration 3/25 | Loss: 0.00386213
Iteration 4/25 | Loss: 0.00386210
Iteration 5/25 | Loss: 0.00386210
Iteration 6/25 | Loss: 0.00386210
Iteration 7/25 | Loss: 0.00386210
Iteration 8/25 | Loss: 0.00386210
Iteration 9/25 | Loss: 0.00386210
Iteration 10/25 | Loss: 0.00386210
Iteration 11/25 | Loss: 0.00386210
Iteration 12/25 | Loss: 0.00386210
Iteration 13/25 | Loss: 0.00386210
Iteration 14/25 | Loss: 0.00386210
Iteration 15/25 | Loss: 0.00386210
Iteration 16/25 | Loss: 0.00386210
Iteration 17/25 | Loss: 0.00386210
Iteration 18/25 | Loss: 0.00386210
Iteration 19/25 | Loss: 0.00386210
Iteration 20/25 | Loss: 0.00386210
Iteration 21/25 | Loss: 0.00386210
Iteration 22/25 | Loss: 0.00386210
Iteration 23/25 | Loss: 0.00386210
Iteration 24/25 | Loss: 0.00386210
Iteration 25/25 | Loss: 0.00386210

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00386210
Iteration 2/1000 | Loss: 0.00047075
Iteration 3/1000 | Loss: 0.00129363
Iteration 4/1000 | Loss: 0.00083715
Iteration 5/1000 | Loss: 0.00150855
Iteration 6/1000 | Loss: 0.00325754
Iteration 7/1000 | Loss: 0.00142528
Iteration 8/1000 | Loss: 0.00027034
Iteration 9/1000 | Loss: 0.00155878
Iteration 10/1000 | Loss: 0.00022570
Iteration 11/1000 | Loss: 0.00021631
Iteration 12/1000 | Loss: 0.00054769
Iteration 13/1000 | Loss: 0.00075531
Iteration 14/1000 | Loss: 0.00068301
Iteration 15/1000 | Loss: 0.00007935
Iteration 16/1000 | Loss: 0.00053472
Iteration 17/1000 | Loss: 0.00118016
Iteration 18/1000 | Loss: 0.00030239
Iteration 19/1000 | Loss: 0.00124790
Iteration 20/1000 | Loss: 0.00041225
Iteration 21/1000 | Loss: 0.00030490
Iteration 22/1000 | Loss: 0.00006648
Iteration 23/1000 | Loss: 0.00005972
Iteration 24/1000 | Loss: 0.00020601
Iteration 25/1000 | Loss: 0.00116100
Iteration 26/1000 | Loss: 0.00023495
Iteration 27/1000 | Loss: 0.00025453
Iteration 28/1000 | Loss: 0.00006884
Iteration 29/1000 | Loss: 0.00032021
Iteration 30/1000 | Loss: 0.00011879
Iteration 31/1000 | Loss: 0.00010932
Iteration 32/1000 | Loss: 0.00017373
Iteration 33/1000 | Loss: 0.00016401
Iteration 34/1000 | Loss: 0.00021207
Iteration 35/1000 | Loss: 0.00014854
Iteration 36/1000 | Loss: 0.00022449
Iteration 37/1000 | Loss: 0.00036222
Iteration 38/1000 | Loss: 0.00007789
Iteration 39/1000 | Loss: 0.00007080
Iteration 40/1000 | Loss: 0.00006263
Iteration 41/1000 | Loss: 0.00004451
Iteration 42/1000 | Loss: 0.00004190
Iteration 43/1000 | Loss: 0.00003982
Iteration 44/1000 | Loss: 0.00091758
Iteration 45/1000 | Loss: 0.00016484
Iteration 46/1000 | Loss: 0.00022856
Iteration 47/1000 | Loss: 0.00005459
Iteration 48/1000 | Loss: 0.00008767
Iteration 49/1000 | Loss: 0.00003763
Iteration 50/1000 | Loss: 0.00005972
Iteration 51/1000 | Loss: 0.00010110
Iteration 52/1000 | Loss: 0.00072363
Iteration 53/1000 | Loss: 0.00017270
Iteration 54/1000 | Loss: 0.00003637
Iteration 55/1000 | Loss: 0.00003465
Iteration 56/1000 | Loss: 0.00014450
Iteration 57/1000 | Loss: 0.00003845
Iteration 58/1000 | Loss: 0.00009865
Iteration 59/1000 | Loss: 0.00003422
Iteration 60/1000 | Loss: 0.00075982
Iteration 61/1000 | Loss: 0.00008028
Iteration 62/1000 | Loss: 0.00003250
Iteration 63/1000 | Loss: 0.00005070
Iteration 64/1000 | Loss: 0.00004118
Iteration 65/1000 | Loss: 0.00003003
Iteration 66/1000 | Loss: 0.00003007
Iteration 67/1000 | Loss: 0.00003155
Iteration 68/1000 | Loss: 0.00009155
Iteration 69/1000 | Loss: 0.00002890
Iteration 70/1000 | Loss: 0.00002860
Iteration 71/1000 | Loss: 0.00008002
Iteration 72/1000 | Loss: 0.00008636
Iteration 73/1000 | Loss: 0.00002837
Iteration 74/1000 | Loss: 0.00002832
Iteration 75/1000 | Loss: 0.00002818
Iteration 76/1000 | Loss: 0.00002811
Iteration 77/1000 | Loss: 0.00002808
Iteration 78/1000 | Loss: 0.00002805
Iteration 79/1000 | Loss: 0.00009901
Iteration 80/1000 | Loss: 0.00002984
Iteration 81/1000 | Loss: 0.00002792
Iteration 82/1000 | Loss: 0.00002791
Iteration 83/1000 | Loss: 0.00002785
Iteration 84/1000 | Loss: 0.00002785
Iteration 85/1000 | Loss: 0.00002784
Iteration 86/1000 | Loss: 0.00002783
Iteration 87/1000 | Loss: 0.00002783
Iteration 88/1000 | Loss: 0.00002783
Iteration 89/1000 | Loss: 0.00002783
Iteration 90/1000 | Loss: 0.00002783
Iteration 91/1000 | Loss: 0.00002783
Iteration 92/1000 | Loss: 0.00002783
Iteration 93/1000 | Loss: 0.00002783
Iteration 94/1000 | Loss: 0.00002782
Iteration 95/1000 | Loss: 0.00002782
Iteration 96/1000 | Loss: 0.00002782
Iteration 97/1000 | Loss: 0.00002781
Iteration 98/1000 | Loss: 0.00002781
Iteration 99/1000 | Loss: 0.00002781
Iteration 100/1000 | Loss: 0.00002781
Iteration 101/1000 | Loss: 0.00002781
Iteration 102/1000 | Loss: 0.00002781
Iteration 103/1000 | Loss: 0.00002781
Iteration 104/1000 | Loss: 0.00002781
Iteration 105/1000 | Loss: 0.00002781
Iteration 106/1000 | Loss: 0.00002781
Iteration 107/1000 | Loss: 0.00002781
Iteration 108/1000 | Loss: 0.00002780
Iteration 109/1000 | Loss: 0.00002780
Iteration 110/1000 | Loss: 0.00002780
Iteration 111/1000 | Loss: 0.00002780
Iteration 112/1000 | Loss: 0.00002780
Iteration 113/1000 | Loss: 0.00002780
Iteration 114/1000 | Loss: 0.00002780
Iteration 115/1000 | Loss: 0.00002780
Iteration 116/1000 | Loss: 0.00002780
Iteration 117/1000 | Loss: 0.00002780
Iteration 118/1000 | Loss: 0.00002780
Iteration 119/1000 | Loss: 0.00002780
Iteration 120/1000 | Loss: 0.00002780
Iteration 121/1000 | Loss: 0.00002780
Iteration 122/1000 | Loss: 0.00002780
Iteration 123/1000 | Loss: 0.00002780
Iteration 124/1000 | Loss: 0.00002779
Iteration 125/1000 | Loss: 0.00053521
Iteration 126/1000 | Loss: 0.00032393
Iteration 127/1000 | Loss: 0.00003036
Iteration 128/1000 | Loss: 0.00002813
Iteration 129/1000 | Loss: 0.00002786
Iteration 130/1000 | Loss: 0.00008643
Iteration 131/1000 | Loss: 0.00002802
Iteration 132/1000 | Loss: 0.00005786
Iteration 133/1000 | Loss: 0.00003290
Iteration 134/1000 | Loss: 0.00002770
Iteration 135/1000 | Loss: 0.00002770
Iteration 136/1000 | Loss: 0.00002769
Iteration 137/1000 | Loss: 0.00002769
Iteration 138/1000 | Loss: 0.00002769
Iteration 139/1000 | Loss: 0.00002769
Iteration 140/1000 | Loss: 0.00002769
Iteration 141/1000 | Loss: 0.00002768
Iteration 142/1000 | Loss: 0.00002768
Iteration 143/1000 | Loss: 0.00002768
Iteration 144/1000 | Loss: 0.00002768
Iteration 145/1000 | Loss: 0.00004495
Iteration 146/1000 | Loss: 0.00003375
Iteration 147/1000 | Loss: 0.00002764
Iteration 148/1000 | Loss: 0.00002764
Iteration 149/1000 | Loss: 0.00002763
Iteration 150/1000 | Loss: 0.00002762
Iteration 151/1000 | Loss: 0.00002762
Iteration 152/1000 | Loss: 0.00002761
Iteration 153/1000 | Loss: 0.00002761
Iteration 154/1000 | Loss: 0.00002761
Iteration 155/1000 | Loss: 0.00002761
Iteration 156/1000 | Loss: 0.00002761
Iteration 157/1000 | Loss: 0.00002761
Iteration 158/1000 | Loss: 0.00002761
Iteration 159/1000 | Loss: 0.00002761
Iteration 160/1000 | Loss: 0.00002761
Iteration 161/1000 | Loss: 0.00002761
Iteration 162/1000 | Loss: 0.00002760
Iteration 163/1000 | Loss: 0.00002760
Iteration 164/1000 | Loss: 0.00002760
Iteration 165/1000 | Loss: 0.00002760
Iteration 166/1000 | Loss: 0.00002760
Iteration 167/1000 | Loss: 0.00002760
Iteration 168/1000 | Loss: 0.00002760
Iteration 169/1000 | Loss: 0.00007411
Iteration 170/1000 | Loss: 0.00002809
Iteration 171/1000 | Loss: 0.00006133
Iteration 172/1000 | Loss: 0.00005124
Iteration 173/1000 | Loss: 0.00003137
Iteration 174/1000 | Loss: 0.00002777
Iteration 175/1000 | Loss: 0.00002777
Iteration 176/1000 | Loss: 0.00002776
Iteration 177/1000 | Loss: 0.00002776
Iteration 178/1000 | Loss: 0.00002775
Iteration 179/1000 | Loss: 0.00002923
Iteration 180/1000 | Loss: 0.00002763
Iteration 181/1000 | Loss: 0.00002762
Iteration 182/1000 | Loss: 0.00002757
Iteration 183/1000 | Loss: 0.00002748
Iteration 184/1000 | Loss: 0.00002748
Iteration 185/1000 | Loss: 0.00002747
Iteration 186/1000 | Loss: 0.00002747
Iteration 187/1000 | Loss: 0.00051553
Iteration 188/1000 | Loss: 0.00036650
Iteration 189/1000 | Loss: 0.00002942
Iteration 190/1000 | Loss: 0.00003911
Iteration 191/1000 | Loss: 0.00002776
Iteration 192/1000 | Loss: 0.00004382
Iteration 193/1000 | Loss: 0.00002752
Iteration 194/1000 | Loss: 0.00002746
Iteration 195/1000 | Loss: 0.00002746
Iteration 196/1000 | Loss: 0.00002745
Iteration 197/1000 | Loss: 0.00002744
Iteration 198/1000 | Loss: 0.00002744
Iteration 199/1000 | Loss: 0.00002743
Iteration 200/1000 | Loss: 0.00002742
Iteration 201/1000 | Loss: 0.00005001
Iteration 202/1000 | Loss: 0.00004699
Iteration 203/1000 | Loss: 0.00050213
Iteration 204/1000 | Loss: 0.00027833
Iteration 205/1000 | Loss: 0.00029320
Iteration 206/1000 | Loss: 0.00018753
Iteration 207/1000 | Loss: 0.00016674
Iteration 208/1000 | Loss: 0.00006299
Iteration 209/1000 | Loss: 0.00002625
Iteration 210/1000 | Loss: 0.00018277
Iteration 211/1000 | Loss: 0.00011871
Iteration 212/1000 | Loss: 0.00002983
Iteration 213/1000 | Loss: 0.00002511
Iteration 214/1000 | Loss: 0.00002409
Iteration 215/1000 | Loss: 0.00002339
Iteration 216/1000 | Loss: 0.00008253
Iteration 217/1000 | Loss: 0.00002708
Iteration 218/1000 | Loss: 0.00002261
Iteration 219/1000 | Loss: 0.00002227
Iteration 220/1000 | Loss: 0.00002213
Iteration 221/1000 | Loss: 0.00002211
Iteration 222/1000 | Loss: 0.00002206
Iteration 223/1000 | Loss: 0.00002197
Iteration 224/1000 | Loss: 0.00002193
Iteration 225/1000 | Loss: 0.00002192
Iteration 226/1000 | Loss: 0.00002192
Iteration 227/1000 | Loss: 0.00002188
Iteration 228/1000 | Loss: 0.00002187
Iteration 229/1000 | Loss: 0.00002187
Iteration 230/1000 | Loss: 0.00002186
Iteration 231/1000 | Loss: 0.00002185
Iteration 232/1000 | Loss: 0.00002185
Iteration 233/1000 | Loss: 0.00002185
Iteration 234/1000 | Loss: 0.00002185
Iteration 235/1000 | Loss: 0.00002184
Iteration 236/1000 | Loss: 0.00002183
Iteration 237/1000 | Loss: 0.00002183
Iteration 238/1000 | Loss: 0.00002182
Iteration 239/1000 | Loss: 0.00002182
Iteration 240/1000 | Loss: 0.00002182
Iteration 241/1000 | Loss: 0.00002181
Iteration 242/1000 | Loss: 0.00002181
Iteration 243/1000 | Loss: 0.00002179
Iteration 244/1000 | Loss: 0.00002178
Iteration 245/1000 | Loss: 0.00002178
Iteration 246/1000 | Loss: 0.00002178
Iteration 247/1000 | Loss: 0.00002177
Iteration 248/1000 | Loss: 0.00002177
Iteration 249/1000 | Loss: 0.00002177
Iteration 250/1000 | Loss: 0.00002177
Iteration 251/1000 | Loss: 0.00002176
Iteration 252/1000 | Loss: 0.00002176
Iteration 253/1000 | Loss: 0.00002173
Iteration 254/1000 | Loss: 0.00002171
Iteration 255/1000 | Loss: 0.00002171
Iteration 256/1000 | Loss: 0.00002170
Iteration 257/1000 | Loss: 0.00002170
Iteration 258/1000 | Loss: 0.00002170
Iteration 259/1000 | Loss: 0.00003497
Iteration 260/1000 | Loss: 0.00002216
Iteration 261/1000 | Loss: 0.00002688
Iteration 262/1000 | Loss: 0.00002553
Iteration 263/1000 | Loss: 0.00002167
Iteration 264/1000 | Loss: 0.00002566
Iteration 265/1000 | Loss: 0.00002165
Iteration 266/1000 | Loss: 0.00002165
Iteration 267/1000 | Loss: 0.00002165
Iteration 268/1000 | Loss: 0.00002165
Iteration 269/1000 | Loss: 0.00002165
Iteration 270/1000 | Loss: 0.00002165
Iteration 271/1000 | Loss: 0.00002165
Iteration 272/1000 | Loss: 0.00002165
Iteration 273/1000 | Loss: 0.00002165
Iteration 274/1000 | Loss: 0.00002165
Iteration 275/1000 | Loss: 0.00002165
Iteration 276/1000 | Loss: 0.00002165
Iteration 277/1000 | Loss: 0.00002165
Iteration 278/1000 | Loss: 0.00002565
Iteration 279/1000 | Loss: 0.00002694
Iteration 280/1000 | Loss: 0.00002164
Iteration 281/1000 | Loss: 0.00002164
Iteration 282/1000 | Loss: 0.00002164
Iteration 283/1000 | Loss: 0.00002164
Iteration 284/1000 | Loss: 0.00002164
Iteration 285/1000 | Loss: 0.00002163
Iteration 286/1000 | Loss: 0.00002163
Iteration 287/1000 | Loss: 0.00002163
Iteration 288/1000 | Loss: 0.00002163
Iteration 289/1000 | Loss: 0.00002163
Iteration 290/1000 | Loss: 0.00002163
Iteration 291/1000 | Loss: 0.00002163
Iteration 292/1000 | Loss: 0.00002163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [2.1633733922499232e-05, 2.1633733922499232e-05, 2.1633733922499232e-05, 2.1633733922499232e-05, 2.1633733922499232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1633733922499232e-05

Optimization complete. Final v2v error: 3.271672248840332 mm

Highest mean error: 12.486538887023926 mm for frame 76

Lowest mean error: 2.566598415374756 mm for frame 112

Saving results

Total time: 240.35279178619385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00564772
Iteration 2/25 | Loss: 0.00119655
Iteration 3/25 | Loss: 0.00112186
Iteration 4/25 | Loss: 0.00111197
Iteration 5/25 | Loss: 0.00110863
Iteration 6/25 | Loss: 0.00110831
Iteration 7/25 | Loss: 0.00110831
Iteration 8/25 | Loss: 0.00110831
Iteration 9/25 | Loss: 0.00110831
Iteration 10/25 | Loss: 0.00110831
Iteration 11/25 | Loss: 0.00110831
Iteration 12/25 | Loss: 0.00110831
Iteration 13/25 | Loss: 0.00110831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001108313794247806, 0.001108313794247806, 0.001108313794247806, 0.001108313794247806, 0.001108313794247806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001108313794247806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.85683036
Iteration 2/25 | Loss: 0.00082674
Iteration 3/25 | Loss: 0.00082673
Iteration 4/25 | Loss: 0.00082673
Iteration 5/25 | Loss: 0.00082673
Iteration 6/25 | Loss: 0.00082673
Iteration 7/25 | Loss: 0.00082673
Iteration 8/25 | Loss: 0.00082673
Iteration 9/25 | Loss: 0.00082673
Iteration 10/25 | Loss: 0.00082673
Iteration 11/25 | Loss: 0.00082673
Iteration 12/25 | Loss: 0.00082673
Iteration 13/25 | Loss: 0.00082673
Iteration 14/25 | Loss: 0.00082673
Iteration 15/25 | Loss: 0.00082673
Iteration 16/25 | Loss: 0.00082673
Iteration 17/25 | Loss: 0.00082673
Iteration 18/25 | Loss: 0.00082673
Iteration 19/25 | Loss: 0.00082673
Iteration 20/25 | Loss: 0.00082673
Iteration 21/25 | Loss: 0.00082673
Iteration 22/25 | Loss: 0.00082673
Iteration 23/25 | Loss: 0.00082673
Iteration 24/25 | Loss: 0.00082673
Iteration 25/25 | Loss: 0.00082673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082673
Iteration 2/1000 | Loss: 0.00002138
Iteration 3/1000 | Loss: 0.00001445
Iteration 4/1000 | Loss: 0.00001242
Iteration 5/1000 | Loss: 0.00001133
Iteration 6/1000 | Loss: 0.00001079
Iteration 7/1000 | Loss: 0.00001032
Iteration 8/1000 | Loss: 0.00001014
Iteration 9/1000 | Loss: 0.00001014
Iteration 10/1000 | Loss: 0.00001000
Iteration 11/1000 | Loss: 0.00000983
Iteration 12/1000 | Loss: 0.00000968
Iteration 13/1000 | Loss: 0.00000962
Iteration 14/1000 | Loss: 0.00000961
Iteration 15/1000 | Loss: 0.00000961
Iteration 16/1000 | Loss: 0.00000960
Iteration 17/1000 | Loss: 0.00000960
Iteration 18/1000 | Loss: 0.00000952
Iteration 19/1000 | Loss: 0.00000948
Iteration 20/1000 | Loss: 0.00000944
Iteration 21/1000 | Loss: 0.00000944
Iteration 22/1000 | Loss: 0.00000943
Iteration 23/1000 | Loss: 0.00000942
Iteration 24/1000 | Loss: 0.00000942
Iteration 25/1000 | Loss: 0.00000942
Iteration 26/1000 | Loss: 0.00000942
Iteration 27/1000 | Loss: 0.00000936
Iteration 28/1000 | Loss: 0.00000936
Iteration 29/1000 | Loss: 0.00000935
Iteration 30/1000 | Loss: 0.00000935
Iteration 31/1000 | Loss: 0.00000934
Iteration 32/1000 | Loss: 0.00000933
Iteration 33/1000 | Loss: 0.00000932
Iteration 34/1000 | Loss: 0.00000932
Iteration 35/1000 | Loss: 0.00000931
Iteration 36/1000 | Loss: 0.00000927
Iteration 37/1000 | Loss: 0.00000927
Iteration 38/1000 | Loss: 0.00000926
Iteration 39/1000 | Loss: 0.00000926
Iteration 40/1000 | Loss: 0.00000926
Iteration 41/1000 | Loss: 0.00000925
Iteration 42/1000 | Loss: 0.00000925
Iteration 43/1000 | Loss: 0.00000925
Iteration 44/1000 | Loss: 0.00000925
Iteration 45/1000 | Loss: 0.00000924
Iteration 46/1000 | Loss: 0.00000924
Iteration 47/1000 | Loss: 0.00000924
Iteration 48/1000 | Loss: 0.00000924
Iteration 49/1000 | Loss: 0.00000924
Iteration 50/1000 | Loss: 0.00000923
Iteration 51/1000 | Loss: 0.00000919
Iteration 52/1000 | Loss: 0.00000919
Iteration 53/1000 | Loss: 0.00000917
Iteration 54/1000 | Loss: 0.00000916
Iteration 55/1000 | Loss: 0.00000915
Iteration 56/1000 | Loss: 0.00000915
Iteration 57/1000 | Loss: 0.00000914
Iteration 58/1000 | Loss: 0.00000913
Iteration 59/1000 | Loss: 0.00000913
Iteration 60/1000 | Loss: 0.00000912
Iteration 61/1000 | Loss: 0.00000912
Iteration 62/1000 | Loss: 0.00000911
Iteration 63/1000 | Loss: 0.00000911
Iteration 64/1000 | Loss: 0.00000911
Iteration 65/1000 | Loss: 0.00000910
Iteration 66/1000 | Loss: 0.00000910
Iteration 67/1000 | Loss: 0.00000910
Iteration 68/1000 | Loss: 0.00000909
Iteration 69/1000 | Loss: 0.00000909
Iteration 70/1000 | Loss: 0.00000909
Iteration 71/1000 | Loss: 0.00000908
Iteration 72/1000 | Loss: 0.00000908
Iteration 73/1000 | Loss: 0.00000908
Iteration 74/1000 | Loss: 0.00000907
Iteration 75/1000 | Loss: 0.00000907
Iteration 76/1000 | Loss: 0.00000907
Iteration 77/1000 | Loss: 0.00000907
Iteration 78/1000 | Loss: 0.00000907
Iteration 79/1000 | Loss: 0.00000906
Iteration 80/1000 | Loss: 0.00000906
Iteration 81/1000 | Loss: 0.00000906
Iteration 82/1000 | Loss: 0.00000906
Iteration 83/1000 | Loss: 0.00000906
Iteration 84/1000 | Loss: 0.00000906
Iteration 85/1000 | Loss: 0.00000906
Iteration 86/1000 | Loss: 0.00000905
Iteration 87/1000 | Loss: 0.00000905
Iteration 88/1000 | Loss: 0.00000904
Iteration 89/1000 | Loss: 0.00000904
Iteration 90/1000 | Loss: 0.00000904
Iteration 91/1000 | Loss: 0.00000904
Iteration 92/1000 | Loss: 0.00000904
Iteration 93/1000 | Loss: 0.00000903
Iteration 94/1000 | Loss: 0.00000903
Iteration 95/1000 | Loss: 0.00000903
Iteration 96/1000 | Loss: 0.00000903
Iteration 97/1000 | Loss: 0.00000903
Iteration 98/1000 | Loss: 0.00000903
Iteration 99/1000 | Loss: 0.00000903
Iteration 100/1000 | Loss: 0.00000903
Iteration 101/1000 | Loss: 0.00000902
Iteration 102/1000 | Loss: 0.00000902
Iteration 103/1000 | Loss: 0.00000902
Iteration 104/1000 | Loss: 0.00000902
Iteration 105/1000 | Loss: 0.00000902
Iteration 106/1000 | Loss: 0.00000902
Iteration 107/1000 | Loss: 0.00000902
Iteration 108/1000 | Loss: 0.00000902
Iteration 109/1000 | Loss: 0.00000902
Iteration 110/1000 | Loss: 0.00000902
Iteration 111/1000 | Loss: 0.00000902
Iteration 112/1000 | Loss: 0.00000902
Iteration 113/1000 | Loss: 0.00000902
Iteration 114/1000 | Loss: 0.00000901
Iteration 115/1000 | Loss: 0.00000901
Iteration 116/1000 | Loss: 0.00000901
Iteration 117/1000 | Loss: 0.00000901
Iteration 118/1000 | Loss: 0.00000900
Iteration 119/1000 | Loss: 0.00000900
Iteration 120/1000 | Loss: 0.00000900
Iteration 121/1000 | Loss: 0.00000900
Iteration 122/1000 | Loss: 0.00000900
Iteration 123/1000 | Loss: 0.00000900
Iteration 124/1000 | Loss: 0.00000900
Iteration 125/1000 | Loss: 0.00000900
Iteration 126/1000 | Loss: 0.00000900
Iteration 127/1000 | Loss: 0.00000900
Iteration 128/1000 | Loss: 0.00000900
Iteration 129/1000 | Loss: 0.00000900
Iteration 130/1000 | Loss: 0.00000900
Iteration 131/1000 | Loss: 0.00000900
Iteration 132/1000 | Loss: 0.00000900
Iteration 133/1000 | Loss: 0.00000900
Iteration 134/1000 | Loss: 0.00000900
Iteration 135/1000 | Loss: 0.00000900
Iteration 136/1000 | Loss: 0.00000900
Iteration 137/1000 | Loss: 0.00000900
Iteration 138/1000 | Loss: 0.00000900
Iteration 139/1000 | Loss: 0.00000900
Iteration 140/1000 | Loss: 0.00000900
Iteration 141/1000 | Loss: 0.00000900
Iteration 142/1000 | Loss: 0.00000900
Iteration 143/1000 | Loss: 0.00000900
Iteration 144/1000 | Loss: 0.00000900
Iteration 145/1000 | Loss: 0.00000900
Iteration 146/1000 | Loss: 0.00000900
Iteration 147/1000 | Loss: 0.00000900
Iteration 148/1000 | Loss: 0.00000900
Iteration 149/1000 | Loss: 0.00000900
Iteration 150/1000 | Loss: 0.00000900
Iteration 151/1000 | Loss: 0.00000900
Iteration 152/1000 | Loss: 0.00000900
Iteration 153/1000 | Loss: 0.00000900
Iteration 154/1000 | Loss: 0.00000900
Iteration 155/1000 | Loss: 0.00000900
Iteration 156/1000 | Loss: 0.00000900
Iteration 157/1000 | Loss: 0.00000900
Iteration 158/1000 | Loss: 0.00000900
Iteration 159/1000 | Loss: 0.00000900
Iteration 160/1000 | Loss: 0.00000900
Iteration 161/1000 | Loss: 0.00000900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [8.996613360068295e-06, 8.996613360068295e-06, 8.996613360068295e-06, 8.996613360068295e-06, 8.996613360068295e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.996613360068295e-06

Optimization complete. Final v2v error: 2.6026625633239746 mm

Highest mean error: 2.9831128120422363 mm for frame 118

Lowest mean error: 2.4832217693328857 mm for frame 45

Saving results

Total time: 35.022923946380615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874544
Iteration 2/25 | Loss: 0.00134676
Iteration 3/25 | Loss: 0.00116934
Iteration 4/25 | Loss: 0.00115503
Iteration 5/25 | Loss: 0.00115018
Iteration 6/25 | Loss: 0.00114956
Iteration 7/25 | Loss: 0.00114956
Iteration 8/25 | Loss: 0.00114956
Iteration 9/25 | Loss: 0.00114956
Iteration 10/25 | Loss: 0.00114956
Iteration 11/25 | Loss: 0.00114956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011495582293719053, 0.0011495582293719053, 0.0011495582293719053, 0.0011495582293719053, 0.0011495582293719053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011495582293719053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39232433
Iteration 2/25 | Loss: 0.00092621
Iteration 3/25 | Loss: 0.00092621
Iteration 4/25 | Loss: 0.00092621
Iteration 5/25 | Loss: 0.00092621
Iteration 6/25 | Loss: 0.00092621
Iteration 7/25 | Loss: 0.00092621
Iteration 8/25 | Loss: 0.00092620
Iteration 9/25 | Loss: 0.00092620
Iteration 10/25 | Loss: 0.00092620
Iteration 11/25 | Loss: 0.00092620
Iteration 12/25 | Loss: 0.00092620
Iteration 13/25 | Loss: 0.00092620
Iteration 14/25 | Loss: 0.00092620
Iteration 15/25 | Loss: 0.00092620
Iteration 16/25 | Loss: 0.00092620
Iteration 17/25 | Loss: 0.00092620
Iteration 18/25 | Loss: 0.00092620
Iteration 19/25 | Loss: 0.00092620
Iteration 20/25 | Loss: 0.00092620
Iteration 21/25 | Loss: 0.00092620
Iteration 22/25 | Loss: 0.00092620
Iteration 23/25 | Loss: 0.00092620
Iteration 24/25 | Loss: 0.00092620
Iteration 25/25 | Loss: 0.00092620

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092620
Iteration 2/1000 | Loss: 0.00002649
Iteration 3/1000 | Loss: 0.00001992
Iteration 4/1000 | Loss: 0.00001833
Iteration 5/1000 | Loss: 0.00001762
Iteration 6/1000 | Loss: 0.00001697
Iteration 7/1000 | Loss: 0.00001660
Iteration 8/1000 | Loss: 0.00001635
Iteration 9/1000 | Loss: 0.00001607
Iteration 10/1000 | Loss: 0.00001589
Iteration 11/1000 | Loss: 0.00001580
Iteration 12/1000 | Loss: 0.00001573
Iteration 13/1000 | Loss: 0.00001564
Iteration 14/1000 | Loss: 0.00001563
Iteration 15/1000 | Loss: 0.00001563
Iteration 16/1000 | Loss: 0.00001562
Iteration 17/1000 | Loss: 0.00001561
Iteration 18/1000 | Loss: 0.00001560
Iteration 19/1000 | Loss: 0.00001559
Iteration 20/1000 | Loss: 0.00001558
Iteration 21/1000 | Loss: 0.00001558
Iteration 22/1000 | Loss: 0.00001558
Iteration 23/1000 | Loss: 0.00001558
Iteration 24/1000 | Loss: 0.00001557
Iteration 25/1000 | Loss: 0.00001557
Iteration 26/1000 | Loss: 0.00001557
Iteration 27/1000 | Loss: 0.00001557
Iteration 28/1000 | Loss: 0.00001557
Iteration 29/1000 | Loss: 0.00001557
Iteration 30/1000 | Loss: 0.00001556
Iteration 31/1000 | Loss: 0.00001556
Iteration 32/1000 | Loss: 0.00001553
Iteration 33/1000 | Loss: 0.00001548
Iteration 34/1000 | Loss: 0.00001548
Iteration 35/1000 | Loss: 0.00001548
Iteration 36/1000 | Loss: 0.00001548
Iteration 37/1000 | Loss: 0.00001548
Iteration 38/1000 | Loss: 0.00001548
Iteration 39/1000 | Loss: 0.00001547
Iteration 40/1000 | Loss: 0.00001547
Iteration 41/1000 | Loss: 0.00001547
Iteration 42/1000 | Loss: 0.00001547
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001545
Iteration 45/1000 | Loss: 0.00001545
Iteration 46/1000 | Loss: 0.00001544
Iteration 47/1000 | Loss: 0.00001543
Iteration 48/1000 | Loss: 0.00001543
Iteration 49/1000 | Loss: 0.00001543
Iteration 50/1000 | Loss: 0.00001543
Iteration 51/1000 | Loss: 0.00001543
Iteration 52/1000 | Loss: 0.00001543
Iteration 53/1000 | Loss: 0.00001543
Iteration 54/1000 | Loss: 0.00001542
Iteration 55/1000 | Loss: 0.00001542
Iteration 56/1000 | Loss: 0.00001542
Iteration 57/1000 | Loss: 0.00001541
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001541
Iteration 60/1000 | Loss: 0.00001541
Iteration 61/1000 | Loss: 0.00001541
Iteration 62/1000 | Loss: 0.00001541
Iteration 63/1000 | Loss: 0.00001541
Iteration 64/1000 | Loss: 0.00001541
Iteration 65/1000 | Loss: 0.00001541
Iteration 66/1000 | Loss: 0.00001541
Iteration 67/1000 | Loss: 0.00001541
Iteration 68/1000 | Loss: 0.00001541
Iteration 69/1000 | Loss: 0.00001541
Iteration 70/1000 | Loss: 0.00001541
Iteration 71/1000 | Loss: 0.00001541
Iteration 72/1000 | Loss: 0.00001541
Iteration 73/1000 | Loss: 0.00001541
Iteration 74/1000 | Loss: 0.00001541
Iteration 75/1000 | Loss: 0.00001541
Iteration 76/1000 | Loss: 0.00001541
Iteration 77/1000 | Loss: 0.00001540
Iteration 78/1000 | Loss: 0.00001540
Iteration 79/1000 | Loss: 0.00001540
Iteration 80/1000 | Loss: 0.00001539
Iteration 81/1000 | Loss: 0.00001538
Iteration 82/1000 | Loss: 0.00001538
Iteration 83/1000 | Loss: 0.00001538
Iteration 84/1000 | Loss: 0.00001538
Iteration 85/1000 | Loss: 0.00001538
Iteration 86/1000 | Loss: 0.00001538
Iteration 87/1000 | Loss: 0.00001538
Iteration 88/1000 | Loss: 0.00001538
Iteration 89/1000 | Loss: 0.00001537
Iteration 90/1000 | Loss: 0.00001537
Iteration 91/1000 | Loss: 0.00001536
Iteration 92/1000 | Loss: 0.00001536
Iteration 93/1000 | Loss: 0.00001535
Iteration 94/1000 | Loss: 0.00001535
Iteration 95/1000 | Loss: 0.00001535
Iteration 96/1000 | Loss: 0.00001535
Iteration 97/1000 | Loss: 0.00001535
Iteration 98/1000 | Loss: 0.00001535
Iteration 99/1000 | Loss: 0.00001535
Iteration 100/1000 | Loss: 0.00001534
Iteration 101/1000 | Loss: 0.00001534
Iteration 102/1000 | Loss: 0.00001534
Iteration 103/1000 | Loss: 0.00001534
Iteration 104/1000 | Loss: 0.00001534
Iteration 105/1000 | Loss: 0.00001534
Iteration 106/1000 | Loss: 0.00001533
Iteration 107/1000 | Loss: 0.00001533
Iteration 108/1000 | Loss: 0.00001533
Iteration 109/1000 | Loss: 0.00001533
Iteration 110/1000 | Loss: 0.00001533
Iteration 111/1000 | Loss: 0.00001532
Iteration 112/1000 | Loss: 0.00001532
Iteration 113/1000 | Loss: 0.00001532
Iteration 114/1000 | Loss: 0.00001532
Iteration 115/1000 | Loss: 0.00001532
Iteration 116/1000 | Loss: 0.00001532
Iteration 117/1000 | Loss: 0.00001532
Iteration 118/1000 | Loss: 0.00001532
Iteration 119/1000 | Loss: 0.00001532
Iteration 120/1000 | Loss: 0.00001532
Iteration 121/1000 | Loss: 0.00001532
Iteration 122/1000 | Loss: 0.00001532
Iteration 123/1000 | Loss: 0.00001532
Iteration 124/1000 | Loss: 0.00001532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.5322213585022837e-05, 1.5322213585022837e-05, 1.5322213585022837e-05, 1.5322213585022837e-05, 1.5322213585022837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5322213585022837e-05

Optimization complete. Final v2v error: 3.185591220855713 mm

Highest mean error: 3.860527276992798 mm for frame 184

Lowest mean error: 2.5736072063446045 mm for frame 213

Saving results

Total time: 37.34553647041321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928923
Iteration 2/25 | Loss: 0.00142125
Iteration 3/25 | Loss: 0.00127192
Iteration 4/25 | Loss: 0.00124750
Iteration 5/25 | Loss: 0.00123874
Iteration 6/25 | Loss: 0.00123658
Iteration 7/25 | Loss: 0.00123616
Iteration 8/25 | Loss: 0.00123616
Iteration 9/25 | Loss: 0.00123616
Iteration 10/25 | Loss: 0.00123616
Iteration 11/25 | Loss: 0.00123616
Iteration 12/25 | Loss: 0.00123616
Iteration 13/25 | Loss: 0.00123616
Iteration 14/25 | Loss: 0.00123616
Iteration 15/25 | Loss: 0.00123616
Iteration 16/25 | Loss: 0.00123616
Iteration 17/25 | Loss: 0.00123616
Iteration 18/25 | Loss: 0.00123616
Iteration 19/25 | Loss: 0.00123616
Iteration 20/25 | Loss: 0.00123616
Iteration 21/25 | Loss: 0.00123616
Iteration 22/25 | Loss: 0.00123616
Iteration 23/25 | Loss: 0.00123616
Iteration 24/25 | Loss: 0.00123616
Iteration 25/25 | Loss: 0.00123616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.16223946
Iteration 2/25 | Loss: 0.00086253
Iteration 3/25 | Loss: 0.00086253
Iteration 4/25 | Loss: 0.00086253
Iteration 5/25 | Loss: 0.00086253
Iteration 6/25 | Loss: 0.00086253
Iteration 7/25 | Loss: 0.00086253
Iteration 8/25 | Loss: 0.00086253
Iteration 9/25 | Loss: 0.00086253
Iteration 10/25 | Loss: 0.00086253
Iteration 11/25 | Loss: 0.00086253
Iteration 12/25 | Loss: 0.00086253
Iteration 13/25 | Loss: 0.00086253
Iteration 14/25 | Loss: 0.00086253
Iteration 15/25 | Loss: 0.00086253
Iteration 16/25 | Loss: 0.00086253
Iteration 17/25 | Loss: 0.00086253
Iteration 18/25 | Loss: 0.00086253
Iteration 19/25 | Loss: 0.00086253
Iteration 20/25 | Loss: 0.00086253
Iteration 21/25 | Loss: 0.00086253
Iteration 22/25 | Loss: 0.00086253
Iteration 23/25 | Loss: 0.00086253
Iteration 24/25 | Loss: 0.00086253
Iteration 25/25 | Loss: 0.00086253
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008625275222584605, 0.0008625275222584605, 0.0008625275222584605, 0.0008625275222584605, 0.0008625275222584605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008625275222584605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086253
Iteration 2/1000 | Loss: 0.00006000
Iteration 3/1000 | Loss: 0.00004195
Iteration 4/1000 | Loss: 0.00003527
Iteration 5/1000 | Loss: 0.00003198
Iteration 6/1000 | Loss: 0.00003048
Iteration 7/1000 | Loss: 0.00002956
Iteration 8/1000 | Loss: 0.00002901
Iteration 9/1000 | Loss: 0.00002846
Iteration 10/1000 | Loss: 0.00002802
Iteration 11/1000 | Loss: 0.00002764
Iteration 12/1000 | Loss: 0.00002738
Iteration 13/1000 | Loss: 0.00002734
Iteration 14/1000 | Loss: 0.00002734
Iteration 15/1000 | Loss: 0.00002711
Iteration 16/1000 | Loss: 0.00002695
Iteration 17/1000 | Loss: 0.00002677
Iteration 18/1000 | Loss: 0.00002676
Iteration 19/1000 | Loss: 0.00002656
Iteration 20/1000 | Loss: 0.00002655
Iteration 21/1000 | Loss: 0.00002647
Iteration 22/1000 | Loss: 0.00002640
Iteration 23/1000 | Loss: 0.00002639
Iteration 24/1000 | Loss: 0.00002637
Iteration 25/1000 | Loss: 0.00002628
Iteration 26/1000 | Loss: 0.00002626
Iteration 27/1000 | Loss: 0.00002626
Iteration 28/1000 | Loss: 0.00002624
Iteration 29/1000 | Loss: 0.00002623
Iteration 30/1000 | Loss: 0.00002623
Iteration 31/1000 | Loss: 0.00002623
Iteration 32/1000 | Loss: 0.00002622
Iteration 33/1000 | Loss: 0.00002622
Iteration 34/1000 | Loss: 0.00002622
Iteration 35/1000 | Loss: 0.00002620
Iteration 36/1000 | Loss: 0.00002618
Iteration 37/1000 | Loss: 0.00002618
Iteration 38/1000 | Loss: 0.00002617
Iteration 39/1000 | Loss: 0.00002617
Iteration 40/1000 | Loss: 0.00002617
Iteration 41/1000 | Loss: 0.00002616
Iteration 42/1000 | Loss: 0.00002616
Iteration 43/1000 | Loss: 0.00002616
Iteration 44/1000 | Loss: 0.00002615
Iteration 45/1000 | Loss: 0.00002615
Iteration 46/1000 | Loss: 0.00002615
Iteration 47/1000 | Loss: 0.00002614
Iteration 48/1000 | Loss: 0.00002614
Iteration 49/1000 | Loss: 0.00002614
Iteration 50/1000 | Loss: 0.00002614
Iteration 51/1000 | Loss: 0.00002614
Iteration 52/1000 | Loss: 0.00002614
Iteration 53/1000 | Loss: 0.00002614
Iteration 54/1000 | Loss: 0.00002614
Iteration 55/1000 | Loss: 0.00002614
Iteration 56/1000 | Loss: 0.00002613
Iteration 57/1000 | Loss: 0.00002613
Iteration 58/1000 | Loss: 0.00002613
Iteration 59/1000 | Loss: 0.00002613
Iteration 60/1000 | Loss: 0.00002613
Iteration 61/1000 | Loss: 0.00002613
Iteration 62/1000 | Loss: 0.00002612
Iteration 63/1000 | Loss: 0.00002612
Iteration 64/1000 | Loss: 0.00002612
Iteration 65/1000 | Loss: 0.00002612
Iteration 66/1000 | Loss: 0.00002611
Iteration 67/1000 | Loss: 0.00002611
Iteration 68/1000 | Loss: 0.00002611
Iteration 69/1000 | Loss: 0.00002611
Iteration 70/1000 | Loss: 0.00002610
Iteration 71/1000 | Loss: 0.00002610
Iteration 72/1000 | Loss: 0.00002610
Iteration 73/1000 | Loss: 0.00002610
Iteration 74/1000 | Loss: 0.00002610
Iteration 75/1000 | Loss: 0.00002610
Iteration 76/1000 | Loss: 0.00002610
Iteration 77/1000 | Loss: 0.00002610
Iteration 78/1000 | Loss: 0.00002610
Iteration 79/1000 | Loss: 0.00002610
Iteration 80/1000 | Loss: 0.00002610
Iteration 81/1000 | Loss: 0.00002610
Iteration 82/1000 | Loss: 0.00002610
Iteration 83/1000 | Loss: 0.00002610
Iteration 84/1000 | Loss: 0.00002610
Iteration 85/1000 | Loss: 0.00002610
Iteration 86/1000 | Loss: 0.00002610
Iteration 87/1000 | Loss: 0.00002610
Iteration 88/1000 | Loss: 0.00002610
Iteration 89/1000 | Loss: 0.00002610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [2.6102479750989005e-05, 2.6102479750989005e-05, 2.6102479750989005e-05, 2.6102479750989005e-05, 2.6102479750989005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6102479750989005e-05

Optimization complete. Final v2v error: 4.213716983795166 mm

Highest mean error: 4.435434818267822 mm for frame 50

Lowest mean error: 3.8046114444732666 mm for frame 8

Saving results

Total time: 39.959577798843384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383589
Iteration 2/25 | Loss: 0.00122555
Iteration 3/25 | Loss: 0.00114302
Iteration 4/25 | Loss: 0.00113543
Iteration 5/25 | Loss: 0.00113338
Iteration 6/25 | Loss: 0.00113313
Iteration 7/25 | Loss: 0.00113311
Iteration 8/25 | Loss: 0.00113311
Iteration 9/25 | Loss: 0.00113311
Iteration 10/25 | Loss: 0.00113311
Iteration 11/25 | Loss: 0.00113311
Iteration 12/25 | Loss: 0.00113311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011331104906275868, 0.0011331104906275868, 0.0011331104906275868, 0.0011331104906275868, 0.0011331104906275868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011331104906275868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39088690
Iteration 2/25 | Loss: 0.00084029
Iteration 3/25 | Loss: 0.00084029
Iteration 4/25 | Loss: 0.00084029
Iteration 5/25 | Loss: 0.00084029
Iteration 6/25 | Loss: 0.00084029
Iteration 7/25 | Loss: 0.00084029
Iteration 8/25 | Loss: 0.00084029
Iteration 9/25 | Loss: 0.00084029
Iteration 10/25 | Loss: 0.00084029
Iteration 11/25 | Loss: 0.00084029
Iteration 12/25 | Loss: 0.00084029
Iteration 13/25 | Loss: 0.00084029
Iteration 14/25 | Loss: 0.00084029
Iteration 15/25 | Loss: 0.00084029
Iteration 16/25 | Loss: 0.00084029
Iteration 17/25 | Loss: 0.00084029
Iteration 18/25 | Loss: 0.00084029
Iteration 19/25 | Loss: 0.00084029
Iteration 20/25 | Loss: 0.00084029
Iteration 21/25 | Loss: 0.00084029
Iteration 22/25 | Loss: 0.00084029
Iteration 23/25 | Loss: 0.00084029
Iteration 24/25 | Loss: 0.00084029
Iteration 25/25 | Loss: 0.00084029

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084029
Iteration 2/1000 | Loss: 0.00002149
Iteration 3/1000 | Loss: 0.00001449
Iteration 4/1000 | Loss: 0.00001316
Iteration 5/1000 | Loss: 0.00001255
Iteration 6/1000 | Loss: 0.00001217
Iteration 7/1000 | Loss: 0.00001184
Iteration 8/1000 | Loss: 0.00001180
Iteration 9/1000 | Loss: 0.00001176
Iteration 10/1000 | Loss: 0.00001174
Iteration 11/1000 | Loss: 0.00001173
Iteration 12/1000 | Loss: 0.00001158
Iteration 13/1000 | Loss: 0.00001156
Iteration 14/1000 | Loss: 0.00001146
Iteration 15/1000 | Loss: 0.00001142
Iteration 16/1000 | Loss: 0.00001141
Iteration 17/1000 | Loss: 0.00001133
Iteration 18/1000 | Loss: 0.00001129
Iteration 19/1000 | Loss: 0.00001129
Iteration 20/1000 | Loss: 0.00001128
Iteration 21/1000 | Loss: 0.00001127
Iteration 22/1000 | Loss: 0.00001126
Iteration 23/1000 | Loss: 0.00001126
Iteration 24/1000 | Loss: 0.00001124
Iteration 25/1000 | Loss: 0.00001124
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001123
Iteration 28/1000 | Loss: 0.00001123
Iteration 29/1000 | Loss: 0.00001123
Iteration 30/1000 | Loss: 0.00001123
Iteration 31/1000 | Loss: 0.00001122
Iteration 32/1000 | Loss: 0.00001122
Iteration 33/1000 | Loss: 0.00001122
Iteration 34/1000 | Loss: 0.00001122
Iteration 35/1000 | Loss: 0.00001121
Iteration 36/1000 | Loss: 0.00001120
Iteration 37/1000 | Loss: 0.00001120
Iteration 38/1000 | Loss: 0.00001120
Iteration 39/1000 | Loss: 0.00001119
Iteration 40/1000 | Loss: 0.00001119
Iteration 41/1000 | Loss: 0.00001118
Iteration 42/1000 | Loss: 0.00001115
Iteration 43/1000 | Loss: 0.00001114
Iteration 44/1000 | Loss: 0.00001114
Iteration 45/1000 | Loss: 0.00001113
Iteration 46/1000 | Loss: 0.00001113
Iteration 47/1000 | Loss: 0.00001113
Iteration 48/1000 | Loss: 0.00001113
Iteration 49/1000 | Loss: 0.00001112
Iteration 50/1000 | Loss: 0.00001112
Iteration 51/1000 | Loss: 0.00001111
Iteration 52/1000 | Loss: 0.00001111
Iteration 53/1000 | Loss: 0.00001110
Iteration 54/1000 | Loss: 0.00001110
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001109
Iteration 57/1000 | Loss: 0.00001109
Iteration 58/1000 | Loss: 0.00001108
Iteration 59/1000 | Loss: 0.00001108
Iteration 60/1000 | Loss: 0.00001108
Iteration 61/1000 | Loss: 0.00001108
Iteration 62/1000 | Loss: 0.00001107
Iteration 63/1000 | Loss: 0.00001107
Iteration 64/1000 | Loss: 0.00001107
Iteration 65/1000 | Loss: 0.00001107
Iteration 66/1000 | Loss: 0.00001107
Iteration 67/1000 | Loss: 0.00001106
Iteration 68/1000 | Loss: 0.00001106
Iteration 69/1000 | Loss: 0.00001106
Iteration 70/1000 | Loss: 0.00001105
Iteration 71/1000 | Loss: 0.00001105
Iteration 72/1000 | Loss: 0.00001105
Iteration 73/1000 | Loss: 0.00001105
Iteration 74/1000 | Loss: 0.00001105
Iteration 75/1000 | Loss: 0.00001105
Iteration 76/1000 | Loss: 0.00001104
Iteration 77/1000 | Loss: 0.00001104
Iteration 78/1000 | Loss: 0.00001104
Iteration 79/1000 | Loss: 0.00001104
Iteration 80/1000 | Loss: 0.00001104
Iteration 81/1000 | Loss: 0.00001104
Iteration 82/1000 | Loss: 0.00001104
Iteration 83/1000 | Loss: 0.00001104
Iteration 84/1000 | Loss: 0.00001103
Iteration 85/1000 | Loss: 0.00001103
Iteration 86/1000 | Loss: 0.00001103
Iteration 87/1000 | Loss: 0.00001103
Iteration 88/1000 | Loss: 0.00001103
Iteration 89/1000 | Loss: 0.00001102
Iteration 90/1000 | Loss: 0.00001102
Iteration 91/1000 | Loss: 0.00001102
Iteration 92/1000 | Loss: 0.00001102
Iteration 93/1000 | Loss: 0.00001102
Iteration 94/1000 | Loss: 0.00001102
Iteration 95/1000 | Loss: 0.00001102
Iteration 96/1000 | Loss: 0.00001102
Iteration 97/1000 | Loss: 0.00001102
Iteration 98/1000 | Loss: 0.00001102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.1017988981620874e-05, 1.1017988981620874e-05, 1.1017988981620874e-05, 1.1017988981620874e-05, 1.1017988981620874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1017988981620874e-05

Optimization complete. Final v2v error: 2.8320634365081787 mm

Highest mean error: 3.0295443534851074 mm for frame 30

Lowest mean error: 2.678577423095703 mm for frame 67

Saving results

Total time: 28.801940441131592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409701
Iteration 2/25 | Loss: 0.00123509
Iteration 3/25 | Loss: 0.00115730
Iteration 4/25 | Loss: 0.00113745
Iteration 5/25 | Loss: 0.00113103
Iteration 6/25 | Loss: 0.00113048
Iteration 7/25 | Loss: 0.00113048
Iteration 8/25 | Loss: 0.00113048
Iteration 9/25 | Loss: 0.00113048
Iteration 10/25 | Loss: 0.00113048
Iteration 11/25 | Loss: 0.00113048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011304771760478616, 0.0011304771760478616, 0.0011304771760478616, 0.0011304771760478616, 0.0011304771760478616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011304771760478616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35905516
Iteration 2/25 | Loss: 0.00075157
Iteration 3/25 | Loss: 0.00075157
Iteration 4/25 | Loss: 0.00075157
Iteration 5/25 | Loss: 0.00075157
Iteration 6/25 | Loss: 0.00075157
Iteration 7/25 | Loss: 0.00075157
Iteration 8/25 | Loss: 0.00075157
Iteration 9/25 | Loss: 0.00075156
Iteration 10/25 | Loss: 0.00075156
Iteration 11/25 | Loss: 0.00075156
Iteration 12/25 | Loss: 0.00075156
Iteration 13/25 | Loss: 0.00075156
Iteration 14/25 | Loss: 0.00075156
Iteration 15/25 | Loss: 0.00075156
Iteration 16/25 | Loss: 0.00075156
Iteration 17/25 | Loss: 0.00075156
Iteration 18/25 | Loss: 0.00075156
Iteration 19/25 | Loss: 0.00075156
Iteration 20/25 | Loss: 0.00075156
Iteration 21/25 | Loss: 0.00075156
Iteration 22/25 | Loss: 0.00075156
Iteration 23/25 | Loss: 0.00075156
Iteration 24/25 | Loss: 0.00075156
Iteration 25/25 | Loss: 0.00075156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075156
Iteration 2/1000 | Loss: 0.00002549
Iteration 3/1000 | Loss: 0.00001614
Iteration 4/1000 | Loss: 0.00001523
Iteration 5/1000 | Loss: 0.00001454
Iteration 6/1000 | Loss: 0.00001421
Iteration 7/1000 | Loss: 0.00001373
Iteration 8/1000 | Loss: 0.00001345
Iteration 9/1000 | Loss: 0.00001343
Iteration 10/1000 | Loss: 0.00001316
Iteration 11/1000 | Loss: 0.00001311
Iteration 12/1000 | Loss: 0.00001302
Iteration 13/1000 | Loss: 0.00001290
Iteration 14/1000 | Loss: 0.00001287
Iteration 15/1000 | Loss: 0.00001287
Iteration 16/1000 | Loss: 0.00001285
Iteration 17/1000 | Loss: 0.00001284
Iteration 18/1000 | Loss: 0.00001283
Iteration 19/1000 | Loss: 0.00001282
Iteration 20/1000 | Loss: 0.00001275
Iteration 21/1000 | Loss: 0.00001273
Iteration 22/1000 | Loss: 0.00001273
Iteration 23/1000 | Loss: 0.00001272
Iteration 24/1000 | Loss: 0.00001270
Iteration 25/1000 | Loss: 0.00001265
Iteration 26/1000 | Loss: 0.00001264
Iteration 27/1000 | Loss: 0.00001262
Iteration 28/1000 | Loss: 0.00001260
Iteration 29/1000 | Loss: 0.00001259
Iteration 30/1000 | Loss: 0.00001259
Iteration 31/1000 | Loss: 0.00001258
Iteration 32/1000 | Loss: 0.00001257
Iteration 33/1000 | Loss: 0.00001255
Iteration 34/1000 | Loss: 0.00001254
Iteration 35/1000 | Loss: 0.00001254
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001254
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001254
Iteration 40/1000 | Loss: 0.00001254
Iteration 41/1000 | Loss: 0.00001254
Iteration 42/1000 | Loss: 0.00001253
Iteration 43/1000 | Loss: 0.00001253
Iteration 44/1000 | Loss: 0.00001253
Iteration 45/1000 | Loss: 0.00001253
Iteration 46/1000 | Loss: 0.00001253
Iteration 47/1000 | Loss: 0.00001252
Iteration 48/1000 | Loss: 0.00001252
Iteration 49/1000 | Loss: 0.00001251
Iteration 50/1000 | Loss: 0.00001250
Iteration 51/1000 | Loss: 0.00001250
Iteration 52/1000 | Loss: 0.00001249
Iteration 53/1000 | Loss: 0.00001248
Iteration 54/1000 | Loss: 0.00001248
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001248
Iteration 57/1000 | Loss: 0.00001248
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001247
Iteration 60/1000 | Loss: 0.00001247
Iteration 61/1000 | Loss: 0.00001246
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001237
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001237
Iteration 66/1000 | Loss: 0.00001236
Iteration 67/1000 | Loss: 0.00001236
Iteration 68/1000 | Loss: 0.00001236
Iteration 69/1000 | Loss: 0.00001236
Iteration 70/1000 | Loss: 0.00001234
Iteration 71/1000 | Loss: 0.00001232
Iteration 72/1000 | Loss: 0.00001231
Iteration 73/1000 | Loss: 0.00001231
Iteration 74/1000 | Loss: 0.00001231
Iteration 75/1000 | Loss: 0.00001231
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001230
Iteration 78/1000 | Loss: 0.00001230
Iteration 79/1000 | Loss: 0.00001230
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001230
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001230
Iteration 85/1000 | Loss: 0.00001230
Iteration 86/1000 | Loss: 0.00001230
Iteration 87/1000 | Loss: 0.00001229
Iteration 88/1000 | Loss: 0.00001229
Iteration 89/1000 | Loss: 0.00001229
Iteration 90/1000 | Loss: 0.00001229
Iteration 91/1000 | Loss: 0.00001228
Iteration 92/1000 | Loss: 0.00001228
Iteration 93/1000 | Loss: 0.00001228
Iteration 94/1000 | Loss: 0.00001227
Iteration 95/1000 | Loss: 0.00001227
Iteration 96/1000 | Loss: 0.00001227
Iteration 97/1000 | Loss: 0.00001226
Iteration 98/1000 | Loss: 0.00001226
Iteration 99/1000 | Loss: 0.00001225
Iteration 100/1000 | Loss: 0.00001225
Iteration 101/1000 | Loss: 0.00001224
Iteration 102/1000 | Loss: 0.00001224
Iteration 103/1000 | Loss: 0.00001224
Iteration 104/1000 | Loss: 0.00001224
Iteration 105/1000 | Loss: 0.00001224
Iteration 106/1000 | Loss: 0.00001223
Iteration 107/1000 | Loss: 0.00001223
Iteration 108/1000 | Loss: 0.00001223
Iteration 109/1000 | Loss: 0.00001223
Iteration 110/1000 | Loss: 0.00001223
Iteration 111/1000 | Loss: 0.00001223
Iteration 112/1000 | Loss: 0.00001223
Iteration 113/1000 | Loss: 0.00001223
Iteration 114/1000 | Loss: 0.00001223
Iteration 115/1000 | Loss: 0.00001222
Iteration 116/1000 | Loss: 0.00001222
Iteration 117/1000 | Loss: 0.00001222
Iteration 118/1000 | Loss: 0.00001221
Iteration 119/1000 | Loss: 0.00001221
Iteration 120/1000 | Loss: 0.00001220
Iteration 121/1000 | Loss: 0.00001220
Iteration 122/1000 | Loss: 0.00001220
Iteration 123/1000 | Loss: 0.00001220
Iteration 124/1000 | Loss: 0.00001219
Iteration 125/1000 | Loss: 0.00001219
Iteration 126/1000 | Loss: 0.00001219
Iteration 127/1000 | Loss: 0.00001219
Iteration 128/1000 | Loss: 0.00001219
Iteration 129/1000 | Loss: 0.00001219
Iteration 130/1000 | Loss: 0.00001219
Iteration 131/1000 | Loss: 0.00001219
Iteration 132/1000 | Loss: 0.00001219
Iteration 133/1000 | Loss: 0.00001219
Iteration 134/1000 | Loss: 0.00001219
Iteration 135/1000 | Loss: 0.00001219
Iteration 136/1000 | Loss: 0.00001219
Iteration 137/1000 | Loss: 0.00001219
Iteration 138/1000 | Loss: 0.00001219
Iteration 139/1000 | Loss: 0.00001218
Iteration 140/1000 | Loss: 0.00001218
Iteration 141/1000 | Loss: 0.00001218
Iteration 142/1000 | Loss: 0.00001218
Iteration 143/1000 | Loss: 0.00001218
Iteration 144/1000 | Loss: 0.00001218
Iteration 145/1000 | Loss: 0.00001218
Iteration 146/1000 | Loss: 0.00001218
Iteration 147/1000 | Loss: 0.00001218
Iteration 148/1000 | Loss: 0.00001218
Iteration 149/1000 | Loss: 0.00001218
Iteration 150/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.2182906175439712e-05, 1.2182906175439712e-05, 1.2182906175439712e-05, 1.2182906175439712e-05, 1.2182906175439712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2182906175439712e-05

Optimization complete. Final v2v error: 3.0245304107666016 mm

Highest mean error: 3.053546905517578 mm for frame 136

Lowest mean error: 2.968580961227417 mm for frame 122

Saving results

Total time: 36.14551067352295
