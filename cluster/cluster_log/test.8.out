Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=8, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 448-503
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_nl_4036/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_nl_4036/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_nl_4036/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893291
Iteration 2/25 | Loss: 0.00112860
Iteration 3/25 | Loss: 0.00084356
Iteration 4/25 | Loss: 0.00081682
Iteration 5/25 | Loss: 0.00080767
Iteration 6/25 | Loss: 0.00080589
Iteration 7/25 | Loss: 0.00080565
Iteration 8/25 | Loss: 0.00080565
Iteration 9/25 | Loss: 0.00080565
Iteration 10/25 | Loss: 0.00080565
Iteration 11/25 | Loss: 0.00080565
Iteration 12/25 | Loss: 0.00080565
Iteration 13/25 | Loss: 0.00080565
Iteration 14/25 | Loss: 0.00080565
Iteration 15/25 | Loss: 0.00080565
Iteration 16/25 | Loss: 0.00080565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008056478691287339, 0.0008056478691287339, 0.0008056478691287339, 0.0008056478691287339, 0.0008056478691287339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008056478691287339

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99347115
Iteration 2/25 | Loss: 0.00048058
Iteration 3/25 | Loss: 0.00048057
Iteration 4/25 | Loss: 0.00048057
Iteration 5/25 | Loss: 0.00048057
Iteration 6/25 | Loss: 0.00048057
Iteration 7/25 | Loss: 0.00048057
Iteration 8/25 | Loss: 0.00048057
Iteration 9/25 | Loss: 0.00048057
Iteration 10/25 | Loss: 0.00048057
Iteration 11/25 | Loss: 0.00048057
Iteration 12/25 | Loss: 0.00048057
Iteration 13/25 | Loss: 0.00048057
Iteration 14/25 | Loss: 0.00048057
Iteration 15/25 | Loss: 0.00048057
Iteration 16/25 | Loss: 0.00048057
Iteration 17/25 | Loss: 0.00048057
Iteration 18/25 | Loss: 0.00048057
Iteration 19/25 | Loss: 0.00048057
Iteration 20/25 | Loss: 0.00048057
Iteration 21/25 | Loss: 0.00048057
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004805695789400488, 0.0004805695789400488, 0.0004805695789400488, 0.0004805695789400488, 0.0004805695789400488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004805695789400488

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048057
Iteration 2/1000 | Loss: 0.00004861
Iteration 3/1000 | Loss: 0.00003695
Iteration 4/1000 | Loss: 0.00003319
Iteration 5/1000 | Loss: 0.00003122
Iteration 6/1000 | Loss: 0.00003025
Iteration 7/1000 | Loss: 0.00002935
Iteration 8/1000 | Loss: 0.00002888
Iteration 9/1000 | Loss: 0.00002862
Iteration 10/1000 | Loss: 0.00002839
Iteration 11/1000 | Loss: 0.00002836
Iteration 12/1000 | Loss: 0.00002835
Iteration 13/1000 | Loss: 0.00002835
Iteration 14/1000 | Loss: 0.00002835
Iteration 15/1000 | Loss: 0.00002835
Iteration 16/1000 | Loss: 0.00002835
Iteration 17/1000 | Loss: 0.00002835
Iteration 18/1000 | Loss: 0.00002835
Iteration 19/1000 | Loss: 0.00002835
Iteration 20/1000 | Loss: 0.00002835
Iteration 21/1000 | Loss: 0.00002835
Iteration 22/1000 | Loss: 0.00002834
Iteration 23/1000 | Loss: 0.00002823
Iteration 24/1000 | Loss: 0.00002823
Iteration 25/1000 | Loss: 0.00002823
Iteration 26/1000 | Loss: 0.00002822
Iteration 27/1000 | Loss: 0.00002822
Iteration 28/1000 | Loss: 0.00002822
Iteration 29/1000 | Loss: 0.00002821
Iteration 30/1000 | Loss: 0.00002820
Iteration 31/1000 | Loss: 0.00002819
Iteration 32/1000 | Loss: 0.00002819
Iteration 33/1000 | Loss: 0.00002819
Iteration 34/1000 | Loss: 0.00002819
Iteration 35/1000 | Loss: 0.00002819
Iteration 36/1000 | Loss: 0.00002819
Iteration 37/1000 | Loss: 0.00002819
Iteration 38/1000 | Loss: 0.00002819
Iteration 39/1000 | Loss: 0.00002819
Iteration 40/1000 | Loss: 0.00002818
Iteration 41/1000 | Loss: 0.00002818
Iteration 42/1000 | Loss: 0.00002818
Iteration 43/1000 | Loss: 0.00002818
Iteration 44/1000 | Loss: 0.00002818
Iteration 45/1000 | Loss: 0.00002818
Iteration 46/1000 | Loss: 0.00002817
Iteration 47/1000 | Loss: 0.00002817
Iteration 48/1000 | Loss: 0.00002817
Iteration 49/1000 | Loss: 0.00002817
Iteration 50/1000 | Loss: 0.00002817
Iteration 51/1000 | Loss: 0.00002817
Iteration 52/1000 | Loss: 0.00002816
Iteration 53/1000 | Loss: 0.00002816
Iteration 54/1000 | Loss: 0.00002816
Iteration 55/1000 | Loss: 0.00002816
Iteration 56/1000 | Loss: 0.00002816
Iteration 57/1000 | Loss: 0.00002816
Iteration 58/1000 | Loss: 0.00002816
Iteration 59/1000 | Loss: 0.00002816
Iteration 60/1000 | Loss: 0.00002816
Iteration 61/1000 | Loss: 0.00002816
Iteration 62/1000 | Loss: 0.00002816
Iteration 63/1000 | Loss: 0.00002816
Iteration 64/1000 | Loss: 0.00002815
Iteration 65/1000 | Loss: 0.00002815
Iteration 66/1000 | Loss: 0.00002815
Iteration 67/1000 | Loss: 0.00002815
Iteration 68/1000 | Loss: 0.00002815
Iteration 69/1000 | Loss: 0.00002815
Iteration 70/1000 | Loss: 0.00002814
Iteration 71/1000 | Loss: 0.00002814
Iteration 72/1000 | Loss: 0.00002814
Iteration 73/1000 | Loss: 0.00002814
Iteration 74/1000 | Loss: 0.00002814
Iteration 75/1000 | Loss: 0.00002813
Iteration 76/1000 | Loss: 0.00002813
Iteration 77/1000 | Loss: 0.00002813
Iteration 78/1000 | Loss: 0.00002813
Iteration 79/1000 | Loss: 0.00002813
Iteration 80/1000 | Loss: 0.00002813
Iteration 81/1000 | Loss: 0.00002813
Iteration 82/1000 | Loss: 0.00002813
Iteration 83/1000 | Loss: 0.00002813
Iteration 84/1000 | Loss: 0.00002813
Iteration 85/1000 | Loss: 0.00002813
Iteration 86/1000 | Loss: 0.00002813
Iteration 87/1000 | Loss: 0.00002812
Iteration 88/1000 | Loss: 0.00002812
Iteration 89/1000 | Loss: 0.00002812
Iteration 90/1000 | Loss: 0.00002812
Iteration 91/1000 | Loss: 0.00002812
Iteration 92/1000 | Loss: 0.00002812
Iteration 93/1000 | Loss: 0.00002812
Iteration 94/1000 | Loss: 0.00002812
Iteration 95/1000 | Loss: 0.00002812
Iteration 96/1000 | Loss: 0.00002812
Iteration 97/1000 | Loss: 0.00002812
Iteration 98/1000 | Loss: 0.00002812
Iteration 99/1000 | Loss: 0.00002812
Iteration 100/1000 | Loss: 0.00002812
Iteration 101/1000 | Loss: 0.00002812
Iteration 102/1000 | Loss: 0.00002811
Iteration 103/1000 | Loss: 0.00002811
Iteration 104/1000 | Loss: 0.00002811
Iteration 105/1000 | Loss: 0.00002811
Iteration 106/1000 | Loss: 0.00002810
Iteration 107/1000 | Loss: 0.00002810
Iteration 108/1000 | Loss: 0.00002810
Iteration 109/1000 | Loss: 0.00002810
Iteration 110/1000 | Loss: 0.00002810
Iteration 111/1000 | Loss: 0.00002810
Iteration 112/1000 | Loss: 0.00002810
Iteration 113/1000 | Loss: 0.00002810
Iteration 114/1000 | Loss: 0.00002810
Iteration 115/1000 | Loss: 0.00002809
Iteration 116/1000 | Loss: 0.00002809
Iteration 117/1000 | Loss: 0.00002808
Iteration 118/1000 | Loss: 0.00002808
Iteration 119/1000 | Loss: 0.00002808
Iteration 120/1000 | Loss: 0.00002808
Iteration 121/1000 | Loss: 0.00002808
Iteration 122/1000 | Loss: 0.00002808
Iteration 123/1000 | Loss: 0.00002807
Iteration 124/1000 | Loss: 0.00002807
Iteration 125/1000 | Loss: 0.00002807
Iteration 126/1000 | Loss: 0.00002807
Iteration 127/1000 | Loss: 0.00002807
Iteration 128/1000 | Loss: 0.00002807
Iteration 129/1000 | Loss: 0.00002807
Iteration 130/1000 | Loss: 0.00002807
Iteration 131/1000 | Loss: 0.00002807
Iteration 132/1000 | Loss: 0.00002807
Iteration 133/1000 | Loss: 0.00002807
Iteration 134/1000 | Loss: 0.00002807
Iteration 135/1000 | Loss: 0.00002807
Iteration 136/1000 | Loss: 0.00002807
Iteration 137/1000 | Loss: 0.00002807
Iteration 138/1000 | Loss: 0.00002807
Iteration 139/1000 | Loss: 0.00002807
Iteration 140/1000 | Loss: 0.00002807
Iteration 141/1000 | Loss: 0.00002807
Iteration 142/1000 | Loss: 0.00002807
Iteration 143/1000 | Loss: 0.00002807
Iteration 144/1000 | Loss: 0.00002807
Iteration 145/1000 | Loss: 0.00002807
Iteration 146/1000 | Loss: 0.00002807
Iteration 147/1000 | Loss: 0.00002807
Iteration 148/1000 | Loss: 0.00002807
Iteration 149/1000 | Loss: 0.00002807
Iteration 150/1000 | Loss: 0.00002807
Iteration 151/1000 | Loss: 0.00002807
Iteration 152/1000 | Loss: 0.00002807
Iteration 153/1000 | Loss: 0.00002807
Iteration 154/1000 | Loss: 0.00002807
Iteration 155/1000 | Loss: 0.00002807
Iteration 156/1000 | Loss: 0.00002807
Iteration 157/1000 | Loss: 0.00002807
Iteration 158/1000 | Loss: 0.00002807
Iteration 159/1000 | Loss: 0.00002807
Iteration 160/1000 | Loss: 0.00002807
Iteration 161/1000 | Loss: 0.00002807
Iteration 162/1000 | Loss: 0.00002807
Iteration 163/1000 | Loss: 0.00002807
Iteration 164/1000 | Loss: 0.00002807
Iteration 165/1000 | Loss: 0.00002807
Iteration 166/1000 | Loss: 0.00002807
Iteration 167/1000 | Loss: 0.00002807
Iteration 168/1000 | Loss: 0.00002807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.8072463464923203e-05, 2.8072463464923203e-05, 2.8072463464923203e-05, 2.8072463464923203e-05, 2.8072463464923203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8072463464923203e-05

Optimization complete. Final v2v error: 4.361254692077637 mm

Highest mean error: 4.55901575088501 mm for frame 1

Lowest mean error: 4.21875524520874 mm for frame 85

Saving results

Total time: 34.90348839759827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_nl_4036/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_nl_4036/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_nl_4036/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444723
Iteration 2/25 | Loss: 0.00078845
Iteration 3/25 | Loss: 0.00064663
Iteration 4/25 | Loss: 0.00062941
Iteration 5/25 | Loss: 0.00062661
Iteration 6/25 | Loss: 0.00062578
Iteration 7/25 | Loss: 0.00062578
Iteration 8/25 | Loss: 0.00062578
Iteration 9/25 | Loss: 0.00062578
Iteration 10/25 | Loss: 0.00062578
Iteration 11/25 | Loss: 0.00062578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006257848581299186, 0.0006257848581299186, 0.0006257848581299186, 0.0006257848581299186, 0.0006257848581299186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006257848581299186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49937940
Iteration 2/25 | Loss: 0.00029902
Iteration 3/25 | Loss: 0.00029902
Iteration 4/25 | Loss: 0.00029902
Iteration 5/25 | Loss: 0.00029902
Iteration 6/25 | Loss: 0.00029902
Iteration 7/25 | Loss: 0.00029902
Iteration 8/25 | Loss: 0.00029902
Iteration 9/25 | Loss: 0.00029902
Iteration 10/25 | Loss: 0.00029902
Iteration 11/25 | Loss: 0.00029902
Iteration 12/25 | Loss: 0.00029902
Iteration 13/25 | Loss: 0.00029902
Iteration 14/25 | Loss: 0.00029902
Iteration 15/25 | Loss: 0.00029902
Iteration 16/25 | Loss: 0.00029902
Iteration 17/25 | Loss: 0.00029902
Iteration 18/25 | Loss: 0.00029902
Iteration 19/25 | Loss: 0.00029902
Iteration 20/25 | Loss: 0.00029902
Iteration 21/25 | Loss: 0.00029902
Iteration 22/25 | Loss: 0.00029902
Iteration 23/25 | Loss: 0.00029902
Iteration 24/25 | Loss: 0.00029902
Iteration 25/25 | Loss: 0.00029902

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029902
Iteration 2/1000 | Loss: 0.00002553
Iteration 3/1000 | Loss: 0.00001791
Iteration 4/1000 | Loss: 0.00001658
Iteration 5/1000 | Loss: 0.00001592
Iteration 6/1000 | Loss: 0.00001546
Iteration 7/1000 | Loss: 0.00001531
Iteration 8/1000 | Loss: 0.00001506
Iteration 9/1000 | Loss: 0.00001494
Iteration 10/1000 | Loss: 0.00001493
Iteration 11/1000 | Loss: 0.00001486
Iteration 12/1000 | Loss: 0.00001483
Iteration 13/1000 | Loss: 0.00001483
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001481
Iteration 16/1000 | Loss: 0.00001480
Iteration 17/1000 | Loss: 0.00001480
Iteration 18/1000 | Loss: 0.00001480
Iteration 19/1000 | Loss: 0.00001480
Iteration 20/1000 | Loss: 0.00001480
Iteration 21/1000 | Loss: 0.00001479
Iteration 22/1000 | Loss: 0.00001478
Iteration 23/1000 | Loss: 0.00001477
Iteration 24/1000 | Loss: 0.00001476
Iteration 25/1000 | Loss: 0.00001475
Iteration 26/1000 | Loss: 0.00001475
Iteration 27/1000 | Loss: 0.00001475
Iteration 28/1000 | Loss: 0.00001474
Iteration 29/1000 | Loss: 0.00001474
Iteration 30/1000 | Loss: 0.00001473
Iteration 31/1000 | Loss: 0.00001472
Iteration 32/1000 | Loss: 0.00001472
Iteration 33/1000 | Loss: 0.00001471
Iteration 34/1000 | Loss: 0.00001470
Iteration 35/1000 | Loss: 0.00001470
Iteration 36/1000 | Loss: 0.00001470
Iteration 37/1000 | Loss: 0.00001469
Iteration 38/1000 | Loss: 0.00001469
Iteration 39/1000 | Loss: 0.00001468
Iteration 40/1000 | Loss: 0.00001468
Iteration 41/1000 | Loss: 0.00001467
Iteration 42/1000 | Loss: 0.00001467
Iteration 43/1000 | Loss: 0.00001467
Iteration 44/1000 | Loss: 0.00001466
Iteration 45/1000 | Loss: 0.00001465
Iteration 46/1000 | Loss: 0.00001465
Iteration 47/1000 | Loss: 0.00001464
Iteration 48/1000 | Loss: 0.00001463
Iteration 49/1000 | Loss: 0.00001463
Iteration 50/1000 | Loss: 0.00001463
Iteration 51/1000 | Loss: 0.00001463
Iteration 52/1000 | Loss: 0.00001462
Iteration 53/1000 | Loss: 0.00001462
Iteration 54/1000 | Loss: 0.00001462
Iteration 55/1000 | Loss: 0.00001462
Iteration 56/1000 | Loss: 0.00001461
Iteration 57/1000 | Loss: 0.00001461
Iteration 58/1000 | Loss: 0.00001461
Iteration 59/1000 | Loss: 0.00001461
Iteration 60/1000 | Loss: 0.00001461
Iteration 61/1000 | Loss: 0.00001461
Iteration 62/1000 | Loss: 0.00001461
Iteration 63/1000 | Loss: 0.00001461
Iteration 64/1000 | Loss: 0.00001460
Iteration 65/1000 | Loss: 0.00001460
Iteration 66/1000 | Loss: 0.00001460
Iteration 67/1000 | Loss: 0.00001460
Iteration 68/1000 | Loss: 0.00001459
Iteration 69/1000 | Loss: 0.00001459
Iteration 70/1000 | Loss: 0.00001458
Iteration 71/1000 | Loss: 0.00001458
Iteration 72/1000 | Loss: 0.00001458
Iteration 73/1000 | Loss: 0.00001457
Iteration 74/1000 | Loss: 0.00001457
Iteration 75/1000 | Loss: 0.00001457
Iteration 76/1000 | Loss: 0.00001457
Iteration 77/1000 | Loss: 0.00001456
Iteration 78/1000 | Loss: 0.00001456
Iteration 79/1000 | Loss: 0.00001456
Iteration 80/1000 | Loss: 0.00001456
Iteration 81/1000 | Loss: 0.00001455
Iteration 82/1000 | Loss: 0.00001455
Iteration 83/1000 | Loss: 0.00001455
Iteration 84/1000 | Loss: 0.00001455
Iteration 85/1000 | Loss: 0.00001455
Iteration 86/1000 | Loss: 0.00001455
Iteration 87/1000 | Loss: 0.00001455
Iteration 88/1000 | Loss: 0.00001455
Iteration 89/1000 | Loss: 0.00001455
Iteration 90/1000 | Loss: 0.00001455
Iteration 91/1000 | Loss: 0.00001455
Iteration 92/1000 | Loss: 0.00001454
Iteration 93/1000 | Loss: 0.00001454
Iteration 94/1000 | Loss: 0.00001454
Iteration 95/1000 | Loss: 0.00001454
Iteration 96/1000 | Loss: 0.00001454
Iteration 97/1000 | Loss: 0.00001454
Iteration 98/1000 | Loss: 0.00001453
Iteration 99/1000 | Loss: 0.00001453
Iteration 100/1000 | Loss: 0.00001453
Iteration 101/1000 | Loss: 0.00001452
Iteration 102/1000 | Loss: 0.00001452
Iteration 103/1000 | Loss: 0.00001452
Iteration 104/1000 | Loss: 0.00001452
Iteration 105/1000 | Loss: 0.00001452
Iteration 106/1000 | Loss: 0.00001452
Iteration 107/1000 | Loss: 0.00001452
Iteration 108/1000 | Loss: 0.00001452
Iteration 109/1000 | Loss: 0.00001452
Iteration 110/1000 | Loss: 0.00001452
Iteration 111/1000 | Loss: 0.00001452
Iteration 112/1000 | Loss: 0.00001452
Iteration 113/1000 | Loss: 0.00001452
Iteration 114/1000 | Loss: 0.00001451
Iteration 115/1000 | Loss: 0.00001451
Iteration 116/1000 | Loss: 0.00001451
Iteration 117/1000 | Loss: 0.00001451
Iteration 118/1000 | Loss: 0.00001451
Iteration 119/1000 | Loss: 0.00001451
Iteration 120/1000 | Loss: 0.00001451
Iteration 121/1000 | Loss: 0.00001451
Iteration 122/1000 | Loss: 0.00001451
Iteration 123/1000 | Loss: 0.00001451
Iteration 124/1000 | Loss: 0.00001450
Iteration 125/1000 | Loss: 0.00001450
Iteration 126/1000 | Loss: 0.00001450
Iteration 127/1000 | Loss: 0.00001450
Iteration 128/1000 | Loss: 0.00001450
Iteration 129/1000 | Loss: 0.00001449
Iteration 130/1000 | Loss: 0.00001449
Iteration 131/1000 | Loss: 0.00001449
Iteration 132/1000 | Loss: 0.00001449
Iteration 133/1000 | Loss: 0.00001449
Iteration 134/1000 | Loss: 0.00001449
Iteration 135/1000 | Loss: 0.00001449
Iteration 136/1000 | Loss: 0.00001449
Iteration 137/1000 | Loss: 0.00001449
Iteration 138/1000 | Loss: 0.00001449
Iteration 139/1000 | Loss: 0.00001449
Iteration 140/1000 | Loss: 0.00001449
Iteration 141/1000 | Loss: 0.00001449
Iteration 142/1000 | Loss: 0.00001449
Iteration 143/1000 | Loss: 0.00001449
Iteration 144/1000 | Loss: 0.00001449
Iteration 145/1000 | Loss: 0.00001449
Iteration 146/1000 | Loss: 0.00001448
Iteration 147/1000 | Loss: 0.00001448
Iteration 148/1000 | Loss: 0.00001448
Iteration 149/1000 | Loss: 0.00001448
Iteration 150/1000 | Loss: 0.00001448
Iteration 151/1000 | Loss: 0.00001448
Iteration 152/1000 | Loss: 0.00001448
Iteration 153/1000 | Loss: 0.00001448
Iteration 154/1000 | Loss: 0.00001448
Iteration 155/1000 | Loss: 0.00001448
Iteration 156/1000 | Loss: 0.00001448
Iteration 157/1000 | Loss: 0.00001448
Iteration 158/1000 | Loss: 0.00001448
Iteration 159/1000 | Loss: 0.00001448
Iteration 160/1000 | Loss: 0.00001448
Iteration 161/1000 | Loss: 0.00001448
Iteration 162/1000 | Loss: 0.00001448
Iteration 163/1000 | Loss: 0.00001448
Iteration 164/1000 | Loss: 0.00001448
Iteration 165/1000 | Loss: 0.00001448
Iteration 166/1000 | Loss: 0.00001448
Iteration 167/1000 | Loss: 0.00001448
Iteration 168/1000 | Loss: 0.00001448
Iteration 169/1000 | Loss: 0.00001448
Iteration 170/1000 | Loss: 0.00001448
Iteration 171/1000 | Loss: 0.00001448
Iteration 172/1000 | Loss: 0.00001448
Iteration 173/1000 | Loss: 0.00001448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.4476151591225062e-05, 1.4476151591225062e-05, 1.4476151591225062e-05, 1.4476151591225062e-05, 1.4476151591225062e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4476151591225062e-05

Optimization complete. Final v2v error: 3.1129019260406494 mm

Highest mean error: 3.653308391571045 mm for frame 5

Lowest mean error: 2.589022636413574 mm for frame 204

Saving results

Total time: 36.93786692619324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596452
Iteration 2/25 | Loss: 0.00116115
Iteration 3/25 | Loss: 0.00106520
Iteration 4/25 | Loss: 0.00104393
Iteration 5/25 | Loss: 0.00103606
Iteration 6/25 | Loss: 0.00103400
Iteration 7/25 | Loss: 0.00103347
Iteration 8/25 | Loss: 0.00103347
Iteration 9/25 | Loss: 0.00103347
Iteration 10/25 | Loss: 0.00103347
Iteration 11/25 | Loss: 0.00103347
Iteration 12/25 | Loss: 0.00103347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010334717808291316, 0.0010334717808291316, 0.0010334717808291316, 0.0010334717808291316, 0.0010334717808291316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010334717808291316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.98886061
Iteration 2/25 | Loss: 0.00072225
Iteration 3/25 | Loss: 0.00072224
Iteration 4/25 | Loss: 0.00072224
Iteration 5/25 | Loss: 0.00072224
Iteration 6/25 | Loss: 0.00072224
Iteration 7/25 | Loss: 0.00072224
Iteration 8/25 | Loss: 0.00072224
Iteration 9/25 | Loss: 0.00072224
Iteration 10/25 | Loss: 0.00072224
Iteration 11/25 | Loss: 0.00072224
Iteration 12/25 | Loss: 0.00072224
Iteration 13/25 | Loss: 0.00072224
Iteration 14/25 | Loss: 0.00072224
Iteration 15/25 | Loss: 0.00072224
Iteration 16/25 | Loss: 0.00072224
Iteration 17/25 | Loss: 0.00072224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007222355343401432, 0.0007222355343401432, 0.0007222355343401432, 0.0007222355343401432, 0.0007222355343401432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007222355343401432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072224
Iteration 2/1000 | Loss: 0.00003867
Iteration 3/1000 | Loss: 0.00002758
Iteration 4/1000 | Loss: 0.00002586
Iteration 5/1000 | Loss: 0.00002473
Iteration 6/1000 | Loss: 0.00002411
Iteration 7/1000 | Loss: 0.00002345
Iteration 8/1000 | Loss: 0.00002339
Iteration 9/1000 | Loss: 0.00002315
Iteration 10/1000 | Loss: 0.00002303
Iteration 11/1000 | Loss: 0.00002303
Iteration 12/1000 | Loss: 0.00002303
Iteration 13/1000 | Loss: 0.00002303
Iteration 14/1000 | Loss: 0.00002302
Iteration 15/1000 | Loss: 0.00002302
Iteration 16/1000 | Loss: 0.00002302
Iteration 17/1000 | Loss: 0.00002299
Iteration 18/1000 | Loss: 0.00002299
Iteration 19/1000 | Loss: 0.00002298
Iteration 20/1000 | Loss: 0.00002298
Iteration 21/1000 | Loss: 0.00002297
Iteration 22/1000 | Loss: 0.00002296
Iteration 23/1000 | Loss: 0.00002296
Iteration 24/1000 | Loss: 0.00002295
Iteration 25/1000 | Loss: 0.00002294
Iteration 26/1000 | Loss: 0.00002294
Iteration 27/1000 | Loss: 0.00002294
Iteration 28/1000 | Loss: 0.00002294
Iteration 29/1000 | Loss: 0.00002293
Iteration 30/1000 | Loss: 0.00002293
Iteration 31/1000 | Loss: 0.00002293
Iteration 32/1000 | Loss: 0.00002292
Iteration 33/1000 | Loss: 0.00002292
Iteration 34/1000 | Loss: 0.00002291
Iteration 35/1000 | Loss: 0.00002291
Iteration 36/1000 | Loss: 0.00002291
Iteration 37/1000 | Loss: 0.00002291
Iteration 38/1000 | Loss: 0.00002291
Iteration 39/1000 | Loss: 0.00002291
Iteration 40/1000 | Loss: 0.00002291
Iteration 41/1000 | Loss: 0.00002291
Iteration 42/1000 | Loss: 0.00002291
Iteration 43/1000 | Loss: 0.00002290
Iteration 44/1000 | Loss: 0.00002290
Iteration 45/1000 | Loss: 0.00002290
Iteration 46/1000 | Loss: 0.00002290
Iteration 47/1000 | Loss: 0.00002290
Iteration 48/1000 | Loss: 0.00002290
Iteration 49/1000 | Loss: 0.00002289
Iteration 50/1000 | Loss: 0.00002289
Iteration 51/1000 | Loss: 0.00002289
Iteration 52/1000 | Loss: 0.00002289
Iteration 53/1000 | Loss: 0.00002289
Iteration 54/1000 | Loss: 0.00002288
Iteration 55/1000 | Loss: 0.00002288
Iteration 56/1000 | Loss: 0.00002288
Iteration 57/1000 | Loss: 0.00002288
Iteration 58/1000 | Loss: 0.00002288
Iteration 59/1000 | Loss: 0.00002288
Iteration 60/1000 | Loss: 0.00002288
Iteration 61/1000 | Loss: 0.00002288
Iteration 62/1000 | Loss: 0.00002288
Iteration 63/1000 | Loss: 0.00002288
Iteration 64/1000 | Loss: 0.00002287
Iteration 65/1000 | Loss: 0.00002287
Iteration 66/1000 | Loss: 0.00002287
Iteration 67/1000 | Loss: 0.00002287
Iteration 68/1000 | Loss: 0.00002287
Iteration 69/1000 | Loss: 0.00002287
Iteration 70/1000 | Loss: 0.00002287
Iteration 71/1000 | Loss: 0.00002287
Iteration 72/1000 | Loss: 0.00002287
Iteration 73/1000 | Loss: 0.00002287
Iteration 74/1000 | Loss: 0.00002287
Iteration 75/1000 | Loss: 0.00002287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.2870754037285224e-05, 2.2870754037285224e-05, 2.2870754037285224e-05, 2.2870754037285224e-05, 2.2870754037285224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2870754037285224e-05

Optimization complete. Final v2v error: 4.086207866668701 mm

Highest mean error: 4.545236110687256 mm for frame 87

Lowest mean error: 3.7450544834136963 mm for frame 45

Saving results

Total time: 26.899192810058594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950420
Iteration 2/25 | Loss: 0.00149222
Iteration 3/25 | Loss: 0.00122561
Iteration 4/25 | Loss: 0.00118423
Iteration 5/25 | Loss: 0.00117152
Iteration 6/25 | Loss: 0.00116888
Iteration 7/25 | Loss: 0.00116815
Iteration 8/25 | Loss: 0.00116815
Iteration 9/25 | Loss: 0.00116815
Iteration 10/25 | Loss: 0.00116815
Iteration 11/25 | Loss: 0.00116815
Iteration 12/25 | Loss: 0.00116815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011681491741910577, 0.0011681491741910577, 0.0011681491741910577, 0.0011681491741910577, 0.0011681491741910577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011681491741910577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46705556
Iteration 2/25 | Loss: 0.00122787
Iteration 3/25 | Loss: 0.00122787
Iteration 4/25 | Loss: 0.00122786
Iteration 5/25 | Loss: 0.00122786
Iteration 6/25 | Loss: 0.00122786
Iteration 7/25 | Loss: 0.00122786
Iteration 8/25 | Loss: 0.00122786
Iteration 9/25 | Loss: 0.00122786
Iteration 10/25 | Loss: 0.00122786
Iteration 11/25 | Loss: 0.00122786
Iteration 12/25 | Loss: 0.00122786
Iteration 13/25 | Loss: 0.00122786
Iteration 14/25 | Loss: 0.00122786
Iteration 15/25 | Loss: 0.00122786
Iteration 16/25 | Loss: 0.00122786
Iteration 17/25 | Loss: 0.00122786
Iteration 18/25 | Loss: 0.00122786
Iteration 19/25 | Loss: 0.00122786
Iteration 20/25 | Loss: 0.00122786
Iteration 21/25 | Loss: 0.00122786
Iteration 22/25 | Loss: 0.00122786
Iteration 23/25 | Loss: 0.00122786
Iteration 24/25 | Loss: 0.00122786
Iteration 25/25 | Loss: 0.00122786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122786
Iteration 2/1000 | Loss: 0.00006729
Iteration 3/1000 | Loss: 0.00005388
Iteration 4/1000 | Loss: 0.00004677
Iteration 5/1000 | Loss: 0.00004324
Iteration 6/1000 | Loss: 0.00004033
Iteration 7/1000 | Loss: 0.00003874
Iteration 8/1000 | Loss: 0.00003771
Iteration 9/1000 | Loss: 0.00003707
Iteration 10/1000 | Loss: 0.00003667
Iteration 11/1000 | Loss: 0.00003639
Iteration 12/1000 | Loss: 0.00003615
Iteration 13/1000 | Loss: 0.00003595
Iteration 14/1000 | Loss: 0.00003588
Iteration 15/1000 | Loss: 0.00003587
Iteration 16/1000 | Loss: 0.00003568
Iteration 17/1000 | Loss: 0.00003563
Iteration 18/1000 | Loss: 0.00003556
Iteration 19/1000 | Loss: 0.00003556
Iteration 20/1000 | Loss: 0.00003555
Iteration 21/1000 | Loss: 0.00003553
Iteration 22/1000 | Loss: 0.00003553
Iteration 23/1000 | Loss: 0.00003553
Iteration 24/1000 | Loss: 0.00003552
Iteration 25/1000 | Loss: 0.00003552
Iteration 26/1000 | Loss: 0.00003552
Iteration 27/1000 | Loss: 0.00003550
Iteration 28/1000 | Loss: 0.00003550
Iteration 29/1000 | Loss: 0.00003548
Iteration 30/1000 | Loss: 0.00003548
Iteration 31/1000 | Loss: 0.00003547
Iteration 32/1000 | Loss: 0.00003546
Iteration 33/1000 | Loss: 0.00003545
Iteration 34/1000 | Loss: 0.00003545
Iteration 35/1000 | Loss: 0.00003545
Iteration 36/1000 | Loss: 0.00003541
Iteration 37/1000 | Loss: 0.00003541
Iteration 38/1000 | Loss: 0.00003541
Iteration 39/1000 | Loss: 0.00003540
Iteration 40/1000 | Loss: 0.00003540
Iteration 41/1000 | Loss: 0.00003540
Iteration 42/1000 | Loss: 0.00003539
Iteration 43/1000 | Loss: 0.00003538
Iteration 44/1000 | Loss: 0.00003538
Iteration 45/1000 | Loss: 0.00003537
Iteration 46/1000 | Loss: 0.00003537
Iteration 47/1000 | Loss: 0.00003537
Iteration 48/1000 | Loss: 0.00003537
Iteration 49/1000 | Loss: 0.00003537
Iteration 50/1000 | Loss: 0.00003537
Iteration 51/1000 | Loss: 0.00003537
Iteration 52/1000 | Loss: 0.00003537
Iteration 53/1000 | Loss: 0.00003536
Iteration 54/1000 | Loss: 0.00003536
Iteration 55/1000 | Loss: 0.00003536
Iteration 56/1000 | Loss: 0.00003536
Iteration 57/1000 | Loss: 0.00003535
Iteration 58/1000 | Loss: 0.00003534
Iteration 59/1000 | Loss: 0.00003534
Iteration 60/1000 | Loss: 0.00003533
Iteration 61/1000 | Loss: 0.00003533
Iteration 62/1000 | Loss: 0.00003532
Iteration 63/1000 | Loss: 0.00003532
Iteration 64/1000 | Loss: 0.00003532
Iteration 65/1000 | Loss: 0.00003531
Iteration 66/1000 | Loss: 0.00003531
Iteration 67/1000 | Loss: 0.00003531
Iteration 68/1000 | Loss: 0.00003531
Iteration 69/1000 | Loss: 0.00003531
Iteration 70/1000 | Loss: 0.00003531
Iteration 71/1000 | Loss: 0.00003530
Iteration 72/1000 | Loss: 0.00003530
Iteration 73/1000 | Loss: 0.00003529
Iteration 74/1000 | Loss: 0.00003529
Iteration 75/1000 | Loss: 0.00003529
Iteration 76/1000 | Loss: 0.00003529
Iteration 77/1000 | Loss: 0.00003529
Iteration 78/1000 | Loss: 0.00003529
Iteration 79/1000 | Loss: 0.00003529
Iteration 80/1000 | Loss: 0.00003529
Iteration 81/1000 | Loss: 0.00003529
Iteration 82/1000 | Loss: 0.00003528
Iteration 83/1000 | Loss: 0.00003528
Iteration 84/1000 | Loss: 0.00003528
Iteration 85/1000 | Loss: 0.00003528
Iteration 86/1000 | Loss: 0.00003527
Iteration 87/1000 | Loss: 0.00003527
Iteration 88/1000 | Loss: 0.00003527
Iteration 89/1000 | Loss: 0.00003526
Iteration 90/1000 | Loss: 0.00003526
Iteration 91/1000 | Loss: 0.00003526
Iteration 92/1000 | Loss: 0.00003526
Iteration 93/1000 | Loss: 0.00003526
Iteration 94/1000 | Loss: 0.00003526
Iteration 95/1000 | Loss: 0.00003526
Iteration 96/1000 | Loss: 0.00003526
Iteration 97/1000 | Loss: 0.00003525
Iteration 98/1000 | Loss: 0.00003525
Iteration 99/1000 | Loss: 0.00003525
Iteration 100/1000 | Loss: 0.00003524
Iteration 101/1000 | Loss: 0.00003524
Iteration 102/1000 | Loss: 0.00003524
Iteration 103/1000 | Loss: 0.00003524
Iteration 104/1000 | Loss: 0.00003524
Iteration 105/1000 | Loss: 0.00003523
Iteration 106/1000 | Loss: 0.00003523
Iteration 107/1000 | Loss: 0.00003523
Iteration 108/1000 | Loss: 0.00003523
Iteration 109/1000 | Loss: 0.00003523
Iteration 110/1000 | Loss: 0.00003523
Iteration 111/1000 | Loss: 0.00003522
Iteration 112/1000 | Loss: 0.00003522
Iteration 113/1000 | Loss: 0.00003522
Iteration 114/1000 | Loss: 0.00003522
Iteration 115/1000 | Loss: 0.00003522
Iteration 116/1000 | Loss: 0.00003522
Iteration 117/1000 | Loss: 0.00003522
Iteration 118/1000 | Loss: 0.00003522
Iteration 119/1000 | Loss: 0.00003522
Iteration 120/1000 | Loss: 0.00003522
Iteration 121/1000 | Loss: 0.00003522
Iteration 122/1000 | Loss: 0.00003522
Iteration 123/1000 | Loss: 0.00003522
Iteration 124/1000 | Loss: 0.00003521
Iteration 125/1000 | Loss: 0.00003521
Iteration 126/1000 | Loss: 0.00003521
Iteration 127/1000 | Loss: 0.00003521
Iteration 128/1000 | Loss: 0.00003521
Iteration 129/1000 | Loss: 0.00003521
Iteration 130/1000 | Loss: 0.00003521
Iteration 131/1000 | Loss: 0.00003521
Iteration 132/1000 | Loss: 0.00003521
Iteration 133/1000 | Loss: 0.00003521
Iteration 134/1000 | Loss: 0.00003520
Iteration 135/1000 | Loss: 0.00003520
Iteration 136/1000 | Loss: 0.00003520
Iteration 137/1000 | Loss: 0.00003520
Iteration 138/1000 | Loss: 0.00003520
Iteration 139/1000 | Loss: 0.00003520
Iteration 140/1000 | Loss: 0.00003520
Iteration 141/1000 | Loss: 0.00003520
Iteration 142/1000 | Loss: 0.00003519
Iteration 143/1000 | Loss: 0.00003519
Iteration 144/1000 | Loss: 0.00003519
Iteration 145/1000 | Loss: 0.00003519
Iteration 146/1000 | Loss: 0.00003519
Iteration 147/1000 | Loss: 0.00003519
Iteration 148/1000 | Loss: 0.00003519
Iteration 149/1000 | Loss: 0.00003519
Iteration 150/1000 | Loss: 0.00003519
Iteration 151/1000 | Loss: 0.00003519
Iteration 152/1000 | Loss: 0.00003518
Iteration 153/1000 | Loss: 0.00003518
Iteration 154/1000 | Loss: 0.00003518
Iteration 155/1000 | Loss: 0.00003518
Iteration 156/1000 | Loss: 0.00003518
Iteration 157/1000 | Loss: 0.00003517
Iteration 158/1000 | Loss: 0.00003517
Iteration 159/1000 | Loss: 0.00003517
Iteration 160/1000 | Loss: 0.00003517
Iteration 161/1000 | Loss: 0.00003517
Iteration 162/1000 | Loss: 0.00003517
Iteration 163/1000 | Loss: 0.00003517
Iteration 164/1000 | Loss: 0.00003517
Iteration 165/1000 | Loss: 0.00003517
Iteration 166/1000 | Loss: 0.00003517
Iteration 167/1000 | Loss: 0.00003517
Iteration 168/1000 | Loss: 0.00003517
Iteration 169/1000 | Loss: 0.00003517
Iteration 170/1000 | Loss: 0.00003517
Iteration 171/1000 | Loss: 0.00003517
Iteration 172/1000 | Loss: 0.00003517
Iteration 173/1000 | Loss: 0.00003517
Iteration 174/1000 | Loss: 0.00003517
Iteration 175/1000 | Loss: 0.00003517
Iteration 176/1000 | Loss: 0.00003517
Iteration 177/1000 | Loss: 0.00003517
Iteration 178/1000 | Loss: 0.00003517
Iteration 179/1000 | Loss: 0.00003517
Iteration 180/1000 | Loss: 0.00003517
Iteration 181/1000 | Loss: 0.00003517
Iteration 182/1000 | Loss: 0.00003517
Iteration 183/1000 | Loss: 0.00003517
Iteration 184/1000 | Loss: 0.00003517
Iteration 185/1000 | Loss: 0.00003517
Iteration 186/1000 | Loss: 0.00003517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [3.516804281389341e-05, 3.516804281389341e-05, 3.516804281389341e-05, 3.516804281389341e-05, 3.516804281389341e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.516804281389341e-05

Optimization complete. Final v2v error: 5.069763660430908 mm

Highest mean error: 5.4698872566223145 mm for frame 124

Lowest mean error: 4.720267295837402 mm for frame 81

Saving results

Total time: 41.89510130882263
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470924
Iteration 2/25 | Loss: 0.00124872
Iteration 3/25 | Loss: 0.00111311
Iteration 4/25 | Loss: 0.00108815
Iteration 5/25 | Loss: 0.00107632
Iteration 6/25 | Loss: 0.00107392
Iteration 7/25 | Loss: 0.00107314
Iteration 8/25 | Loss: 0.00107314
Iteration 9/25 | Loss: 0.00107314
Iteration 10/25 | Loss: 0.00107314
Iteration 11/25 | Loss: 0.00107314
Iteration 12/25 | Loss: 0.00107314
Iteration 13/25 | Loss: 0.00107314
Iteration 14/25 | Loss: 0.00107314
Iteration 15/25 | Loss: 0.00107314
Iteration 16/25 | Loss: 0.00107314
Iteration 17/25 | Loss: 0.00107314
Iteration 18/25 | Loss: 0.00107314
Iteration 19/25 | Loss: 0.00107314
Iteration 20/25 | Loss: 0.00107314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001073140068911016, 0.001073140068911016, 0.001073140068911016, 0.001073140068911016, 0.001073140068911016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001073140068911016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40039098
Iteration 2/25 | Loss: 0.00080858
Iteration 3/25 | Loss: 0.00080858
Iteration 4/25 | Loss: 0.00080858
Iteration 5/25 | Loss: 0.00080858
Iteration 6/25 | Loss: 0.00080858
Iteration 7/25 | Loss: 0.00080858
Iteration 8/25 | Loss: 0.00080858
Iteration 9/25 | Loss: 0.00080858
Iteration 10/25 | Loss: 0.00080858
Iteration 11/25 | Loss: 0.00080858
Iteration 12/25 | Loss: 0.00080858
Iteration 13/25 | Loss: 0.00080858
Iteration 14/25 | Loss: 0.00080858
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008085828740149736, 0.0008085828740149736, 0.0008085828740149736, 0.0008085828740149736, 0.0008085828740149736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008085828740149736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080858
Iteration 2/1000 | Loss: 0.00006409
Iteration 3/1000 | Loss: 0.00004186
Iteration 4/1000 | Loss: 0.00003443
Iteration 5/1000 | Loss: 0.00003205
Iteration 6/1000 | Loss: 0.00003055
Iteration 7/1000 | Loss: 0.00002941
Iteration 8/1000 | Loss: 0.00002866
Iteration 9/1000 | Loss: 0.00002808
Iteration 10/1000 | Loss: 0.00002774
Iteration 11/1000 | Loss: 0.00002752
Iteration 12/1000 | Loss: 0.00002752
Iteration 13/1000 | Loss: 0.00002740
Iteration 14/1000 | Loss: 0.00002739
Iteration 15/1000 | Loss: 0.00002738
Iteration 16/1000 | Loss: 0.00002737
Iteration 17/1000 | Loss: 0.00002737
Iteration 18/1000 | Loss: 0.00002736
Iteration 19/1000 | Loss: 0.00002735
Iteration 20/1000 | Loss: 0.00002729
Iteration 21/1000 | Loss: 0.00002729
Iteration 22/1000 | Loss: 0.00002728
Iteration 23/1000 | Loss: 0.00002727
Iteration 24/1000 | Loss: 0.00002726
Iteration 25/1000 | Loss: 0.00002726
Iteration 26/1000 | Loss: 0.00002726
Iteration 27/1000 | Loss: 0.00002725
Iteration 28/1000 | Loss: 0.00002725
Iteration 29/1000 | Loss: 0.00002725
Iteration 30/1000 | Loss: 0.00002725
Iteration 31/1000 | Loss: 0.00002725
Iteration 32/1000 | Loss: 0.00002724
Iteration 33/1000 | Loss: 0.00002724
Iteration 34/1000 | Loss: 0.00002724
Iteration 35/1000 | Loss: 0.00002723
Iteration 36/1000 | Loss: 0.00002723
Iteration 37/1000 | Loss: 0.00002723
Iteration 38/1000 | Loss: 0.00002722
Iteration 39/1000 | Loss: 0.00002722
Iteration 40/1000 | Loss: 0.00002722
Iteration 41/1000 | Loss: 0.00002722
Iteration 42/1000 | Loss: 0.00002722
Iteration 43/1000 | Loss: 0.00002722
Iteration 44/1000 | Loss: 0.00002722
Iteration 45/1000 | Loss: 0.00002722
Iteration 46/1000 | Loss: 0.00002722
Iteration 47/1000 | Loss: 0.00002722
Iteration 48/1000 | Loss: 0.00002721
Iteration 49/1000 | Loss: 0.00002721
Iteration 50/1000 | Loss: 0.00002721
Iteration 51/1000 | Loss: 0.00002721
Iteration 52/1000 | Loss: 0.00002721
Iteration 53/1000 | Loss: 0.00002721
Iteration 54/1000 | Loss: 0.00002721
Iteration 55/1000 | Loss: 0.00002720
Iteration 56/1000 | Loss: 0.00002720
Iteration 57/1000 | Loss: 0.00002720
Iteration 58/1000 | Loss: 0.00002720
Iteration 59/1000 | Loss: 0.00002720
Iteration 60/1000 | Loss: 0.00002720
Iteration 61/1000 | Loss: 0.00002719
Iteration 62/1000 | Loss: 0.00002719
Iteration 63/1000 | Loss: 0.00002719
Iteration 64/1000 | Loss: 0.00002719
Iteration 65/1000 | Loss: 0.00002719
Iteration 66/1000 | Loss: 0.00002718
Iteration 67/1000 | Loss: 0.00002718
Iteration 68/1000 | Loss: 0.00002718
Iteration 69/1000 | Loss: 0.00002718
Iteration 70/1000 | Loss: 0.00002718
Iteration 71/1000 | Loss: 0.00002718
Iteration 72/1000 | Loss: 0.00002718
Iteration 73/1000 | Loss: 0.00002718
Iteration 74/1000 | Loss: 0.00002718
Iteration 75/1000 | Loss: 0.00002718
Iteration 76/1000 | Loss: 0.00002718
Iteration 77/1000 | Loss: 0.00002718
Iteration 78/1000 | Loss: 0.00002718
Iteration 79/1000 | Loss: 0.00002718
Iteration 80/1000 | Loss: 0.00002718
Iteration 81/1000 | Loss: 0.00002718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.7176025469088927e-05, 2.7176025469088927e-05, 2.7176025469088927e-05, 2.7176025469088927e-05, 2.7176025469088927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7176025469088927e-05

Optimization complete. Final v2v error: 4.311913967132568 mm

Highest mean error: 4.769699573516846 mm for frame 14

Lowest mean error: 3.7283456325531006 mm for frame 78

Saving results

Total time: 31.551172733306885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881222
Iteration 2/25 | Loss: 0.00109860
Iteration 3/25 | Loss: 0.00099382
Iteration 4/25 | Loss: 0.00097709
Iteration 5/25 | Loss: 0.00097410
Iteration 6/25 | Loss: 0.00097326
Iteration 7/25 | Loss: 0.00097326
Iteration 8/25 | Loss: 0.00097326
Iteration 9/25 | Loss: 0.00097326
Iteration 10/25 | Loss: 0.00097326
Iteration 11/25 | Loss: 0.00097326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000973262416664511, 0.000973262416664511, 0.000973262416664511, 0.000973262416664511, 0.000973262416664511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000973262416664511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46756113
Iteration 2/25 | Loss: 0.00065888
Iteration 3/25 | Loss: 0.00065888
Iteration 4/25 | Loss: 0.00065888
Iteration 5/25 | Loss: 0.00065888
Iteration 6/25 | Loss: 0.00065888
Iteration 7/25 | Loss: 0.00065888
Iteration 8/25 | Loss: 0.00065888
Iteration 9/25 | Loss: 0.00065888
Iteration 10/25 | Loss: 0.00065888
Iteration 11/25 | Loss: 0.00065888
Iteration 12/25 | Loss: 0.00065888
Iteration 13/25 | Loss: 0.00065888
Iteration 14/25 | Loss: 0.00065888
Iteration 15/25 | Loss: 0.00065888
Iteration 16/25 | Loss: 0.00065888
Iteration 17/25 | Loss: 0.00065888
Iteration 18/25 | Loss: 0.00065888
Iteration 19/25 | Loss: 0.00065888
Iteration 20/25 | Loss: 0.00065888
Iteration 21/25 | Loss: 0.00065888
Iteration 22/25 | Loss: 0.00065888
Iteration 23/25 | Loss: 0.00065888
Iteration 24/25 | Loss: 0.00065888
Iteration 25/25 | Loss: 0.00065888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065888
Iteration 2/1000 | Loss: 0.00003674
Iteration 3/1000 | Loss: 0.00002141
Iteration 4/1000 | Loss: 0.00001952
Iteration 5/1000 | Loss: 0.00001839
Iteration 6/1000 | Loss: 0.00001769
Iteration 7/1000 | Loss: 0.00001729
Iteration 8/1000 | Loss: 0.00001704
Iteration 9/1000 | Loss: 0.00001704
Iteration 10/1000 | Loss: 0.00001694
Iteration 11/1000 | Loss: 0.00001691
Iteration 12/1000 | Loss: 0.00001691
Iteration 13/1000 | Loss: 0.00001690
Iteration 14/1000 | Loss: 0.00001689
Iteration 15/1000 | Loss: 0.00001688
Iteration 16/1000 | Loss: 0.00001688
Iteration 17/1000 | Loss: 0.00001687
Iteration 18/1000 | Loss: 0.00001685
Iteration 19/1000 | Loss: 0.00001685
Iteration 20/1000 | Loss: 0.00001685
Iteration 21/1000 | Loss: 0.00001684
Iteration 22/1000 | Loss: 0.00001684
Iteration 23/1000 | Loss: 0.00001684
Iteration 24/1000 | Loss: 0.00001684
Iteration 25/1000 | Loss: 0.00001683
Iteration 26/1000 | Loss: 0.00001683
Iteration 27/1000 | Loss: 0.00001683
Iteration 28/1000 | Loss: 0.00001683
Iteration 29/1000 | Loss: 0.00001683
Iteration 30/1000 | Loss: 0.00001683
Iteration 31/1000 | Loss: 0.00001682
Iteration 32/1000 | Loss: 0.00001682
Iteration 33/1000 | Loss: 0.00001681
Iteration 34/1000 | Loss: 0.00001681
Iteration 35/1000 | Loss: 0.00001680
Iteration 36/1000 | Loss: 0.00001680
Iteration 37/1000 | Loss: 0.00001680
Iteration 38/1000 | Loss: 0.00001678
Iteration 39/1000 | Loss: 0.00001677
Iteration 40/1000 | Loss: 0.00001677
Iteration 41/1000 | Loss: 0.00001677
Iteration 42/1000 | Loss: 0.00001676
Iteration 43/1000 | Loss: 0.00001676
Iteration 44/1000 | Loss: 0.00001675
Iteration 45/1000 | Loss: 0.00001675
Iteration 46/1000 | Loss: 0.00001675
Iteration 47/1000 | Loss: 0.00001674
Iteration 48/1000 | Loss: 0.00001673
Iteration 49/1000 | Loss: 0.00001673
Iteration 50/1000 | Loss: 0.00001673
Iteration 51/1000 | Loss: 0.00001673
Iteration 52/1000 | Loss: 0.00001672
Iteration 53/1000 | Loss: 0.00001672
Iteration 54/1000 | Loss: 0.00001672
Iteration 55/1000 | Loss: 0.00001672
Iteration 56/1000 | Loss: 0.00001672
Iteration 57/1000 | Loss: 0.00001672
Iteration 58/1000 | Loss: 0.00001672
Iteration 59/1000 | Loss: 0.00001672
Iteration 60/1000 | Loss: 0.00001672
Iteration 61/1000 | Loss: 0.00001670
Iteration 62/1000 | Loss: 0.00001669
Iteration 63/1000 | Loss: 0.00001669
Iteration 64/1000 | Loss: 0.00001668
Iteration 65/1000 | Loss: 0.00001668
Iteration 66/1000 | Loss: 0.00001668
Iteration 67/1000 | Loss: 0.00001668
Iteration 68/1000 | Loss: 0.00001667
Iteration 69/1000 | Loss: 0.00001667
Iteration 70/1000 | Loss: 0.00001667
Iteration 71/1000 | Loss: 0.00001667
Iteration 72/1000 | Loss: 0.00001666
Iteration 73/1000 | Loss: 0.00001666
Iteration 74/1000 | Loss: 0.00001666
Iteration 75/1000 | Loss: 0.00001666
Iteration 76/1000 | Loss: 0.00001666
Iteration 77/1000 | Loss: 0.00001665
Iteration 78/1000 | Loss: 0.00001665
Iteration 79/1000 | Loss: 0.00001665
Iteration 80/1000 | Loss: 0.00001665
Iteration 81/1000 | Loss: 0.00001665
Iteration 82/1000 | Loss: 0.00001665
Iteration 83/1000 | Loss: 0.00001665
Iteration 84/1000 | Loss: 0.00001664
Iteration 85/1000 | Loss: 0.00001664
Iteration 86/1000 | Loss: 0.00001664
Iteration 87/1000 | Loss: 0.00001664
Iteration 88/1000 | Loss: 0.00001664
Iteration 89/1000 | Loss: 0.00001664
Iteration 90/1000 | Loss: 0.00001664
Iteration 91/1000 | Loss: 0.00001664
Iteration 92/1000 | Loss: 0.00001663
Iteration 93/1000 | Loss: 0.00001663
Iteration 94/1000 | Loss: 0.00001663
Iteration 95/1000 | Loss: 0.00001663
Iteration 96/1000 | Loss: 0.00001663
Iteration 97/1000 | Loss: 0.00001663
Iteration 98/1000 | Loss: 0.00001663
Iteration 99/1000 | Loss: 0.00001663
Iteration 100/1000 | Loss: 0.00001663
Iteration 101/1000 | Loss: 0.00001663
Iteration 102/1000 | Loss: 0.00001662
Iteration 103/1000 | Loss: 0.00001662
Iteration 104/1000 | Loss: 0.00001662
Iteration 105/1000 | Loss: 0.00001662
Iteration 106/1000 | Loss: 0.00001662
Iteration 107/1000 | Loss: 0.00001662
Iteration 108/1000 | Loss: 0.00001662
Iteration 109/1000 | Loss: 0.00001662
Iteration 110/1000 | Loss: 0.00001662
Iteration 111/1000 | Loss: 0.00001662
Iteration 112/1000 | Loss: 0.00001662
Iteration 113/1000 | Loss: 0.00001661
Iteration 114/1000 | Loss: 0.00001661
Iteration 115/1000 | Loss: 0.00001661
Iteration 116/1000 | Loss: 0.00001661
Iteration 117/1000 | Loss: 0.00001661
Iteration 118/1000 | Loss: 0.00001661
Iteration 119/1000 | Loss: 0.00001661
Iteration 120/1000 | Loss: 0.00001661
Iteration 121/1000 | Loss: 0.00001661
Iteration 122/1000 | Loss: 0.00001661
Iteration 123/1000 | Loss: 0.00001661
Iteration 124/1000 | Loss: 0.00001660
Iteration 125/1000 | Loss: 0.00001660
Iteration 126/1000 | Loss: 0.00001660
Iteration 127/1000 | Loss: 0.00001660
Iteration 128/1000 | Loss: 0.00001660
Iteration 129/1000 | Loss: 0.00001660
Iteration 130/1000 | Loss: 0.00001660
Iteration 131/1000 | Loss: 0.00001660
Iteration 132/1000 | Loss: 0.00001660
Iteration 133/1000 | Loss: 0.00001660
Iteration 134/1000 | Loss: 0.00001660
Iteration 135/1000 | Loss: 0.00001660
Iteration 136/1000 | Loss: 0.00001660
Iteration 137/1000 | Loss: 0.00001660
Iteration 138/1000 | Loss: 0.00001659
Iteration 139/1000 | Loss: 0.00001659
Iteration 140/1000 | Loss: 0.00001659
Iteration 141/1000 | Loss: 0.00001659
Iteration 142/1000 | Loss: 0.00001659
Iteration 143/1000 | Loss: 0.00001659
Iteration 144/1000 | Loss: 0.00001659
Iteration 145/1000 | Loss: 0.00001659
Iteration 146/1000 | Loss: 0.00001659
Iteration 147/1000 | Loss: 0.00001659
Iteration 148/1000 | Loss: 0.00001659
Iteration 149/1000 | Loss: 0.00001659
Iteration 150/1000 | Loss: 0.00001659
Iteration 151/1000 | Loss: 0.00001659
Iteration 152/1000 | Loss: 0.00001659
Iteration 153/1000 | Loss: 0.00001659
Iteration 154/1000 | Loss: 0.00001659
Iteration 155/1000 | Loss: 0.00001658
Iteration 156/1000 | Loss: 0.00001658
Iteration 157/1000 | Loss: 0.00001658
Iteration 158/1000 | Loss: 0.00001658
Iteration 159/1000 | Loss: 0.00001658
Iteration 160/1000 | Loss: 0.00001658
Iteration 161/1000 | Loss: 0.00001658
Iteration 162/1000 | Loss: 0.00001657
Iteration 163/1000 | Loss: 0.00001657
Iteration 164/1000 | Loss: 0.00001657
Iteration 165/1000 | Loss: 0.00001657
Iteration 166/1000 | Loss: 0.00001657
Iteration 167/1000 | Loss: 0.00001657
Iteration 168/1000 | Loss: 0.00001657
Iteration 169/1000 | Loss: 0.00001657
Iteration 170/1000 | Loss: 0.00001657
Iteration 171/1000 | Loss: 0.00001657
Iteration 172/1000 | Loss: 0.00001657
Iteration 173/1000 | Loss: 0.00001657
Iteration 174/1000 | Loss: 0.00001657
Iteration 175/1000 | Loss: 0.00001657
Iteration 176/1000 | Loss: 0.00001657
Iteration 177/1000 | Loss: 0.00001657
Iteration 178/1000 | Loss: 0.00001656
Iteration 179/1000 | Loss: 0.00001656
Iteration 180/1000 | Loss: 0.00001656
Iteration 181/1000 | Loss: 0.00001656
Iteration 182/1000 | Loss: 0.00001656
Iteration 183/1000 | Loss: 0.00001656
Iteration 184/1000 | Loss: 0.00001656
Iteration 185/1000 | Loss: 0.00001656
Iteration 186/1000 | Loss: 0.00001656
Iteration 187/1000 | Loss: 0.00001656
Iteration 188/1000 | Loss: 0.00001656
Iteration 189/1000 | Loss: 0.00001656
Iteration 190/1000 | Loss: 0.00001656
Iteration 191/1000 | Loss: 0.00001656
Iteration 192/1000 | Loss: 0.00001656
Iteration 193/1000 | Loss: 0.00001656
Iteration 194/1000 | Loss: 0.00001656
Iteration 195/1000 | Loss: 0.00001656
Iteration 196/1000 | Loss: 0.00001655
Iteration 197/1000 | Loss: 0.00001655
Iteration 198/1000 | Loss: 0.00001655
Iteration 199/1000 | Loss: 0.00001655
Iteration 200/1000 | Loss: 0.00001655
Iteration 201/1000 | Loss: 0.00001655
Iteration 202/1000 | Loss: 0.00001655
Iteration 203/1000 | Loss: 0.00001655
Iteration 204/1000 | Loss: 0.00001655
Iteration 205/1000 | Loss: 0.00001655
Iteration 206/1000 | Loss: 0.00001655
Iteration 207/1000 | Loss: 0.00001655
Iteration 208/1000 | Loss: 0.00001655
Iteration 209/1000 | Loss: 0.00001655
Iteration 210/1000 | Loss: 0.00001655
Iteration 211/1000 | Loss: 0.00001655
Iteration 212/1000 | Loss: 0.00001655
Iteration 213/1000 | Loss: 0.00001655
Iteration 214/1000 | Loss: 0.00001655
Iteration 215/1000 | Loss: 0.00001655
Iteration 216/1000 | Loss: 0.00001654
Iteration 217/1000 | Loss: 0.00001654
Iteration 218/1000 | Loss: 0.00001654
Iteration 219/1000 | Loss: 0.00001654
Iteration 220/1000 | Loss: 0.00001654
Iteration 221/1000 | Loss: 0.00001654
Iteration 222/1000 | Loss: 0.00001654
Iteration 223/1000 | Loss: 0.00001654
Iteration 224/1000 | Loss: 0.00001654
Iteration 225/1000 | Loss: 0.00001654
Iteration 226/1000 | Loss: 0.00001654
Iteration 227/1000 | Loss: 0.00001654
Iteration 228/1000 | Loss: 0.00001654
Iteration 229/1000 | Loss: 0.00001654
Iteration 230/1000 | Loss: 0.00001654
Iteration 231/1000 | Loss: 0.00001654
Iteration 232/1000 | Loss: 0.00001654
Iteration 233/1000 | Loss: 0.00001654
Iteration 234/1000 | Loss: 0.00001654
Iteration 235/1000 | Loss: 0.00001654
Iteration 236/1000 | Loss: 0.00001654
Iteration 237/1000 | Loss: 0.00001654
Iteration 238/1000 | Loss: 0.00001654
Iteration 239/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.6537564079044387e-05, 1.6537564079044387e-05, 1.6537564079044387e-05, 1.6537564079044387e-05, 1.6537564079044387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6537564079044387e-05

Optimization complete. Final v2v error: 3.548734664916992 mm

Highest mean error: 3.761643171310425 mm for frame 105

Lowest mean error: 3.3548429012298584 mm for frame 13

Saving results

Total time: 36.82239270210266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918418
Iteration 2/25 | Loss: 0.00143287
Iteration 3/25 | Loss: 0.00125874
Iteration 4/25 | Loss: 0.00119418
Iteration 5/25 | Loss: 0.00118670
Iteration 6/25 | Loss: 0.00118525
Iteration 7/25 | Loss: 0.00118505
Iteration 8/25 | Loss: 0.00118505
Iteration 9/25 | Loss: 0.00118505
Iteration 10/25 | Loss: 0.00118505
Iteration 11/25 | Loss: 0.00118505
Iteration 12/25 | Loss: 0.00118505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011850533774122596, 0.0011850533774122596, 0.0011850533774122596, 0.0011850533774122596, 0.0011850533774122596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011850533774122596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42694736
Iteration 2/25 | Loss: 0.00086822
Iteration 3/25 | Loss: 0.00086822
Iteration 4/25 | Loss: 0.00086822
Iteration 5/25 | Loss: 0.00086822
Iteration 6/25 | Loss: 0.00086822
Iteration 7/25 | Loss: 0.00086822
Iteration 8/25 | Loss: 0.00086822
Iteration 9/25 | Loss: 0.00086822
Iteration 10/25 | Loss: 0.00086822
Iteration 11/25 | Loss: 0.00086822
Iteration 12/25 | Loss: 0.00086822
Iteration 13/25 | Loss: 0.00086822
Iteration 14/25 | Loss: 0.00086822
Iteration 15/25 | Loss: 0.00086822
Iteration 16/25 | Loss: 0.00086822
Iteration 17/25 | Loss: 0.00086822
Iteration 18/25 | Loss: 0.00086822
Iteration 19/25 | Loss: 0.00086822
Iteration 20/25 | Loss: 0.00086822
Iteration 21/25 | Loss: 0.00086822
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008682216284796596, 0.0008682216284796596, 0.0008682216284796596, 0.0008682216284796596, 0.0008682216284796596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008682216284796596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086822
Iteration 2/1000 | Loss: 0.00006360
Iteration 3/1000 | Loss: 0.00004183
Iteration 4/1000 | Loss: 0.00003785
Iteration 5/1000 | Loss: 0.00003562
Iteration 6/1000 | Loss: 0.00003384
Iteration 7/1000 | Loss: 0.00003270
Iteration 8/1000 | Loss: 0.00003169
Iteration 9/1000 | Loss: 0.00003120
Iteration 10/1000 | Loss: 0.00003092
Iteration 11/1000 | Loss: 0.00003068
Iteration 12/1000 | Loss: 0.00003067
Iteration 13/1000 | Loss: 0.00003059
Iteration 14/1000 | Loss: 0.00003054
Iteration 15/1000 | Loss: 0.00003054
Iteration 16/1000 | Loss: 0.00003053
Iteration 17/1000 | Loss: 0.00003052
Iteration 18/1000 | Loss: 0.00003050
Iteration 19/1000 | Loss: 0.00003050
Iteration 20/1000 | Loss: 0.00003050
Iteration 21/1000 | Loss: 0.00003050
Iteration 22/1000 | Loss: 0.00003050
Iteration 23/1000 | Loss: 0.00003050
Iteration 24/1000 | Loss: 0.00003050
Iteration 25/1000 | Loss: 0.00003049
Iteration 26/1000 | Loss: 0.00003049
Iteration 27/1000 | Loss: 0.00003049
Iteration 28/1000 | Loss: 0.00003049
Iteration 29/1000 | Loss: 0.00003049
Iteration 30/1000 | Loss: 0.00003049
Iteration 31/1000 | Loss: 0.00003049
Iteration 32/1000 | Loss: 0.00003048
Iteration 33/1000 | Loss: 0.00003047
Iteration 34/1000 | Loss: 0.00003047
Iteration 35/1000 | Loss: 0.00003046
Iteration 36/1000 | Loss: 0.00003046
Iteration 37/1000 | Loss: 0.00003046
Iteration 38/1000 | Loss: 0.00003046
Iteration 39/1000 | Loss: 0.00003046
Iteration 40/1000 | Loss: 0.00003046
Iteration 41/1000 | Loss: 0.00003046
Iteration 42/1000 | Loss: 0.00003045
Iteration 43/1000 | Loss: 0.00003045
Iteration 44/1000 | Loss: 0.00003045
Iteration 45/1000 | Loss: 0.00003045
Iteration 46/1000 | Loss: 0.00003045
Iteration 47/1000 | Loss: 0.00003045
Iteration 48/1000 | Loss: 0.00003045
Iteration 49/1000 | Loss: 0.00003045
Iteration 50/1000 | Loss: 0.00003045
Iteration 51/1000 | Loss: 0.00003045
Iteration 52/1000 | Loss: 0.00003045
Iteration 53/1000 | Loss: 0.00003045
Iteration 54/1000 | Loss: 0.00003045
Iteration 55/1000 | Loss: 0.00003045
Iteration 56/1000 | Loss: 0.00003045
Iteration 57/1000 | Loss: 0.00003045
Iteration 58/1000 | Loss: 0.00003045
Iteration 59/1000 | Loss: 0.00003045
Iteration 60/1000 | Loss: 0.00003045
Iteration 61/1000 | Loss: 0.00003045
Iteration 62/1000 | Loss: 0.00003045
Iteration 63/1000 | Loss: 0.00003045
Iteration 64/1000 | Loss: 0.00003045
Iteration 65/1000 | Loss: 0.00003045
Iteration 66/1000 | Loss: 0.00003045
Iteration 67/1000 | Loss: 0.00003045
Iteration 68/1000 | Loss: 0.00003045
Iteration 69/1000 | Loss: 0.00003045
Iteration 70/1000 | Loss: 0.00003045
Iteration 71/1000 | Loss: 0.00003045
Iteration 72/1000 | Loss: 0.00003045
Iteration 73/1000 | Loss: 0.00003045
Iteration 74/1000 | Loss: 0.00003045
Iteration 75/1000 | Loss: 0.00003045
Iteration 76/1000 | Loss: 0.00003045
Iteration 77/1000 | Loss: 0.00003045
Iteration 78/1000 | Loss: 0.00003045
Iteration 79/1000 | Loss: 0.00003045
Iteration 80/1000 | Loss: 0.00003045
Iteration 81/1000 | Loss: 0.00003045
Iteration 82/1000 | Loss: 0.00003045
Iteration 83/1000 | Loss: 0.00003045
Iteration 84/1000 | Loss: 0.00003045
Iteration 85/1000 | Loss: 0.00003045
Iteration 86/1000 | Loss: 0.00003045
Iteration 87/1000 | Loss: 0.00003045
Iteration 88/1000 | Loss: 0.00003045
Iteration 89/1000 | Loss: 0.00003045
Iteration 90/1000 | Loss: 0.00003045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [3.0450932172243483e-05, 3.0450932172243483e-05, 3.0450932172243483e-05, 3.0450932172243483e-05, 3.0450932172243483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0450932172243483e-05

Optimization complete. Final v2v error: 4.637026309967041 mm

Highest mean error: 4.8574395179748535 mm for frame 144

Lowest mean error: 4.52739953994751 mm for frame 41

Saving results

Total time: 29.363661289215088
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843757
Iteration 2/25 | Loss: 0.00167275
Iteration 3/25 | Loss: 0.00121754
Iteration 4/25 | Loss: 0.00122665
Iteration 5/25 | Loss: 0.00119637
Iteration 6/25 | Loss: 0.00112446
Iteration 7/25 | Loss: 0.00109562
Iteration 8/25 | Loss: 0.00108422
Iteration 9/25 | Loss: 0.00107240
Iteration 10/25 | Loss: 0.00106829
Iteration 11/25 | Loss: 0.00106263
Iteration 12/25 | Loss: 0.00105614
Iteration 13/25 | Loss: 0.00105562
Iteration 14/25 | Loss: 0.00104980
Iteration 15/25 | Loss: 0.00104762
Iteration 16/25 | Loss: 0.00104643
Iteration 17/25 | Loss: 0.00104745
Iteration 18/25 | Loss: 0.00105230
Iteration 19/25 | Loss: 0.00105226
Iteration 20/25 | Loss: 0.00104411
Iteration 21/25 | Loss: 0.00103810
Iteration 22/25 | Loss: 0.00103695
Iteration 23/25 | Loss: 0.00104335
Iteration 24/25 | Loss: 0.00103724
Iteration 25/25 | Loss: 0.00103356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72262812
Iteration 2/25 | Loss: 0.00117057
Iteration 3/25 | Loss: 0.00112358
Iteration 4/25 | Loss: 0.00112358
Iteration 5/25 | Loss: 0.00112358
Iteration 6/25 | Loss: 0.00112358
Iteration 7/25 | Loss: 0.00112357
Iteration 8/25 | Loss: 0.00112357
Iteration 9/25 | Loss: 0.00112357
Iteration 10/25 | Loss: 0.00112357
Iteration 11/25 | Loss: 0.00112357
Iteration 12/25 | Loss: 0.00112357
Iteration 13/25 | Loss: 0.00112357
Iteration 14/25 | Loss: 0.00112357
Iteration 15/25 | Loss: 0.00112357
Iteration 16/25 | Loss: 0.00112357
Iteration 17/25 | Loss: 0.00112357
Iteration 18/25 | Loss: 0.00112357
Iteration 19/25 | Loss: 0.00112357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011235743295401335, 0.0011235743295401335, 0.0011235743295401335, 0.0011235743295401335, 0.0011235743295401335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011235743295401335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112357
Iteration 2/1000 | Loss: 0.00028999
Iteration 3/1000 | Loss: 0.00017274
Iteration 4/1000 | Loss: 0.00016456
Iteration 5/1000 | Loss: 0.00013625
Iteration 6/1000 | Loss: 0.00027475
Iteration 7/1000 | Loss: 0.00024737
Iteration 8/1000 | Loss: 0.00019319
Iteration 9/1000 | Loss: 0.00015569
Iteration 10/1000 | Loss: 0.00047207
Iteration 11/1000 | Loss: 0.00031368
Iteration 12/1000 | Loss: 0.00044565
Iteration 13/1000 | Loss: 0.00055609
Iteration 14/1000 | Loss: 0.00035830
Iteration 15/1000 | Loss: 0.00067726
Iteration 16/1000 | Loss: 0.00070891
Iteration 17/1000 | Loss: 0.00113591
Iteration 18/1000 | Loss: 0.00090649
Iteration 19/1000 | Loss: 0.00076858
Iteration 20/1000 | Loss: 0.00039941
Iteration 21/1000 | Loss: 0.00032938
Iteration 22/1000 | Loss: 0.00003836
Iteration 23/1000 | Loss: 0.00058557
Iteration 24/1000 | Loss: 0.00040579
Iteration 25/1000 | Loss: 0.00029557
Iteration 26/1000 | Loss: 0.00004334
Iteration 27/1000 | Loss: 0.00004346
Iteration 28/1000 | Loss: 0.00003953
Iteration 29/1000 | Loss: 0.00004255
Iteration 30/1000 | Loss: 0.00003355
Iteration 31/1000 | Loss: 0.00002512
Iteration 32/1000 | Loss: 0.00055010
Iteration 33/1000 | Loss: 0.00020585
Iteration 34/1000 | Loss: 0.00028951
Iteration 35/1000 | Loss: 0.00003106
Iteration 36/1000 | Loss: 0.00002573
Iteration 37/1000 | Loss: 0.00002346
Iteration 38/1000 | Loss: 0.00002186
Iteration 39/1000 | Loss: 0.00002094
Iteration 40/1000 | Loss: 0.00002049
Iteration 41/1000 | Loss: 0.00002027
Iteration 42/1000 | Loss: 0.00002000
Iteration 43/1000 | Loss: 0.00001973
Iteration 44/1000 | Loss: 0.00001945
Iteration 45/1000 | Loss: 0.00001938
Iteration 46/1000 | Loss: 0.00001919
Iteration 47/1000 | Loss: 0.00001918
Iteration 48/1000 | Loss: 0.00001914
Iteration 49/1000 | Loss: 0.00001914
Iteration 50/1000 | Loss: 0.00001914
Iteration 51/1000 | Loss: 0.00001913
Iteration 52/1000 | Loss: 0.00001913
Iteration 53/1000 | Loss: 0.00001913
Iteration 54/1000 | Loss: 0.00001912
Iteration 55/1000 | Loss: 0.00001912
Iteration 56/1000 | Loss: 0.00001912
Iteration 57/1000 | Loss: 0.00001911
Iteration 58/1000 | Loss: 0.00001911
Iteration 59/1000 | Loss: 0.00001910
Iteration 60/1000 | Loss: 0.00001910
Iteration 61/1000 | Loss: 0.00001910
Iteration 62/1000 | Loss: 0.00001909
Iteration 63/1000 | Loss: 0.00001909
Iteration 64/1000 | Loss: 0.00001908
Iteration 65/1000 | Loss: 0.00001908
Iteration 66/1000 | Loss: 0.00001907
Iteration 67/1000 | Loss: 0.00001907
Iteration 68/1000 | Loss: 0.00001902
Iteration 69/1000 | Loss: 0.00001902
Iteration 70/1000 | Loss: 0.00001901
Iteration 71/1000 | Loss: 0.00001901
Iteration 72/1000 | Loss: 0.00001901
Iteration 73/1000 | Loss: 0.00001901
Iteration 74/1000 | Loss: 0.00001901
Iteration 75/1000 | Loss: 0.00001901
Iteration 76/1000 | Loss: 0.00001901
Iteration 77/1000 | Loss: 0.00001901
Iteration 78/1000 | Loss: 0.00001900
Iteration 79/1000 | Loss: 0.00001900
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001899
Iteration 82/1000 | Loss: 0.00001899
Iteration 83/1000 | Loss: 0.00001899
Iteration 84/1000 | Loss: 0.00001898
Iteration 85/1000 | Loss: 0.00001898
Iteration 86/1000 | Loss: 0.00001898
Iteration 87/1000 | Loss: 0.00001898
Iteration 88/1000 | Loss: 0.00001898
Iteration 89/1000 | Loss: 0.00001898
Iteration 90/1000 | Loss: 0.00001898
Iteration 91/1000 | Loss: 0.00001898
Iteration 92/1000 | Loss: 0.00001898
Iteration 93/1000 | Loss: 0.00001897
Iteration 94/1000 | Loss: 0.00001897
Iteration 95/1000 | Loss: 0.00001897
Iteration 96/1000 | Loss: 0.00001897
Iteration 97/1000 | Loss: 0.00001897
Iteration 98/1000 | Loss: 0.00001897
Iteration 99/1000 | Loss: 0.00001897
Iteration 100/1000 | Loss: 0.00001897
Iteration 101/1000 | Loss: 0.00001897
Iteration 102/1000 | Loss: 0.00001896
Iteration 103/1000 | Loss: 0.00001896
Iteration 104/1000 | Loss: 0.00001896
Iteration 105/1000 | Loss: 0.00001896
Iteration 106/1000 | Loss: 0.00001896
Iteration 107/1000 | Loss: 0.00001896
Iteration 108/1000 | Loss: 0.00001896
Iteration 109/1000 | Loss: 0.00001896
Iteration 110/1000 | Loss: 0.00001896
Iteration 111/1000 | Loss: 0.00001896
Iteration 112/1000 | Loss: 0.00001896
Iteration 113/1000 | Loss: 0.00001896
Iteration 114/1000 | Loss: 0.00001896
Iteration 115/1000 | Loss: 0.00001896
Iteration 116/1000 | Loss: 0.00001896
Iteration 117/1000 | Loss: 0.00001896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.89593974937452e-05, 1.89593974937452e-05, 1.89593974937452e-05, 1.89593974937452e-05, 1.89593974937452e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.89593974937452e-05

Optimization complete. Final v2v error: 3.804950714111328 mm

Highest mean error: 5.133901119232178 mm for frame 208

Lowest mean error: 3.221999168395996 mm for frame 124

Saving results

Total time: 130.54693579673767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868129
Iteration 2/25 | Loss: 0.00143983
Iteration 3/25 | Loss: 0.00121829
Iteration 4/25 | Loss: 0.00115885
Iteration 5/25 | Loss: 0.00111662
Iteration 6/25 | Loss: 0.00110009
Iteration 7/25 | Loss: 0.00108610
Iteration 8/25 | Loss: 0.00107884
Iteration 9/25 | Loss: 0.00107734
Iteration 10/25 | Loss: 0.00107705
Iteration 11/25 | Loss: 0.00107696
Iteration 12/25 | Loss: 0.00107696
Iteration 13/25 | Loss: 0.00107696
Iteration 14/25 | Loss: 0.00107696
Iteration 15/25 | Loss: 0.00107696
Iteration 16/25 | Loss: 0.00107696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010769566288217902, 0.0010769566288217902, 0.0010769566288217902, 0.0010769566288217902, 0.0010769566288217902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010769566288217902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44863033
Iteration 2/25 | Loss: 0.00094945
Iteration 3/25 | Loss: 0.00094945
Iteration 4/25 | Loss: 0.00094945
Iteration 5/25 | Loss: 0.00094945
Iteration 6/25 | Loss: 0.00094945
Iteration 7/25 | Loss: 0.00094945
Iteration 8/25 | Loss: 0.00094945
Iteration 9/25 | Loss: 0.00094945
Iteration 10/25 | Loss: 0.00094945
Iteration 11/25 | Loss: 0.00094945
Iteration 12/25 | Loss: 0.00094945
Iteration 13/25 | Loss: 0.00094945
Iteration 14/25 | Loss: 0.00094945
Iteration 15/25 | Loss: 0.00094945
Iteration 16/25 | Loss: 0.00094945
Iteration 17/25 | Loss: 0.00094945
Iteration 18/25 | Loss: 0.00094945
Iteration 19/25 | Loss: 0.00094945
Iteration 20/25 | Loss: 0.00094945
Iteration 21/25 | Loss: 0.00094945
Iteration 22/25 | Loss: 0.00094945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009494498372077942, 0.0009494498372077942, 0.0009494498372077942, 0.0009494498372077942, 0.0009494498372077942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009494498372077942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094945
Iteration 2/1000 | Loss: 0.00005992
Iteration 3/1000 | Loss: 0.00003595
Iteration 4/1000 | Loss: 0.00003061
Iteration 5/1000 | Loss: 0.00002851
Iteration 6/1000 | Loss: 0.00002685
Iteration 7/1000 | Loss: 0.00002604
Iteration 8/1000 | Loss: 0.00002543
Iteration 9/1000 | Loss: 0.00002494
Iteration 10/1000 | Loss: 0.00002468
Iteration 11/1000 | Loss: 0.00002445
Iteration 12/1000 | Loss: 0.00002432
Iteration 13/1000 | Loss: 0.00002428
Iteration 14/1000 | Loss: 0.00002418
Iteration 15/1000 | Loss: 0.00002417
Iteration 16/1000 | Loss: 0.00002416
Iteration 17/1000 | Loss: 0.00002415
Iteration 18/1000 | Loss: 0.00002415
Iteration 19/1000 | Loss: 0.00002414
Iteration 20/1000 | Loss: 0.00002413
Iteration 21/1000 | Loss: 0.00002413
Iteration 22/1000 | Loss: 0.00002413
Iteration 23/1000 | Loss: 0.00002412
Iteration 24/1000 | Loss: 0.00002412
Iteration 25/1000 | Loss: 0.00002412
Iteration 26/1000 | Loss: 0.00002412
Iteration 27/1000 | Loss: 0.00002412
Iteration 28/1000 | Loss: 0.00002411
Iteration 29/1000 | Loss: 0.00002411
Iteration 30/1000 | Loss: 0.00002411
Iteration 31/1000 | Loss: 0.00002410
Iteration 32/1000 | Loss: 0.00002410
Iteration 33/1000 | Loss: 0.00002410
Iteration 34/1000 | Loss: 0.00002410
Iteration 35/1000 | Loss: 0.00002409
Iteration 36/1000 | Loss: 0.00002409
Iteration 37/1000 | Loss: 0.00002409
Iteration 38/1000 | Loss: 0.00002409
Iteration 39/1000 | Loss: 0.00002408
Iteration 40/1000 | Loss: 0.00002408
Iteration 41/1000 | Loss: 0.00002408
Iteration 42/1000 | Loss: 0.00002408
Iteration 43/1000 | Loss: 0.00002408
Iteration 44/1000 | Loss: 0.00002408
Iteration 45/1000 | Loss: 0.00002407
Iteration 46/1000 | Loss: 0.00002407
Iteration 47/1000 | Loss: 0.00002407
Iteration 48/1000 | Loss: 0.00002407
Iteration 49/1000 | Loss: 0.00002407
Iteration 50/1000 | Loss: 0.00002406
Iteration 51/1000 | Loss: 0.00002406
Iteration 52/1000 | Loss: 0.00002406
Iteration 53/1000 | Loss: 0.00002405
Iteration 54/1000 | Loss: 0.00002405
Iteration 55/1000 | Loss: 0.00002405
Iteration 56/1000 | Loss: 0.00002404
Iteration 57/1000 | Loss: 0.00002404
Iteration 58/1000 | Loss: 0.00002403
Iteration 59/1000 | Loss: 0.00002403
Iteration 60/1000 | Loss: 0.00002403
Iteration 61/1000 | Loss: 0.00002403
Iteration 62/1000 | Loss: 0.00002403
Iteration 63/1000 | Loss: 0.00002403
Iteration 64/1000 | Loss: 0.00002402
Iteration 65/1000 | Loss: 0.00002402
Iteration 66/1000 | Loss: 0.00002402
Iteration 67/1000 | Loss: 0.00002402
Iteration 68/1000 | Loss: 0.00002402
Iteration 69/1000 | Loss: 0.00002402
Iteration 70/1000 | Loss: 0.00002401
Iteration 71/1000 | Loss: 0.00002401
Iteration 72/1000 | Loss: 0.00002401
Iteration 73/1000 | Loss: 0.00002401
Iteration 74/1000 | Loss: 0.00002401
Iteration 75/1000 | Loss: 0.00002400
Iteration 76/1000 | Loss: 0.00002400
Iteration 77/1000 | Loss: 0.00002400
Iteration 78/1000 | Loss: 0.00002400
Iteration 79/1000 | Loss: 0.00002400
Iteration 80/1000 | Loss: 0.00002400
Iteration 81/1000 | Loss: 0.00002400
Iteration 82/1000 | Loss: 0.00002400
Iteration 83/1000 | Loss: 0.00002400
Iteration 84/1000 | Loss: 0.00002400
Iteration 85/1000 | Loss: 0.00002399
Iteration 86/1000 | Loss: 0.00002399
Iteration 87/1000 | Loss: 0.00002399
Iteration 88/1000 | Loss: 0.00002399
Iteration 89/1000 | Loss: 0.00002399
Iteration 90/1000 | Loss: 0.00002398
Iteration 91/1000 | Loss: 0.00002398
Iteration 92/1000 | Loss: 0.00002398
Iteration 93/1000 | Loss: 0.00002398
Iteration 94/1000 | Loss: 0.00002398
Iteration 95/1000 | Loss: 0.00002398
Iteration 96/1000 | Loss: 0.00002397
Iteration 97/1000 | Loss: 0.00002397
Iteration 98/1000 | Loss: 0.00002397
Iteration 99/1000 | Loss: 0.00002397
Iteration 100/1000 | Loss: 0.00002397
Iteration 101/1000 | Loss: 0.00002397
Iteration 102/1000 | Loss: 0.00002397
Iteration 103/1000 | Loss: 0.00002397
Iteration 104/1000 | Loss: 0.00002396
Iteration 105/1000 | Loss: 0.00002396
Iteration 106/1000 | Loss: 0.00002396
Iteration 107/1000 | Loss: 0.00002396
Iteration 108/1000 | Loss: 0.00002396
Iteration 109/1000 | Loss: 0.00002395
Iteration 110/1000 | Loss: 0.00002395
Iteration 111/1000 | Loss: 0.00002395
Iteration 112/1000 | Loss: 0.00002395
Iteration 113/1000 | Loss: 0.00002395
Iteration 114/1000 | Loss: 0.00002394
Iteration 115/1000 | Loss: 0.00002394
Iteration 116/1000 | Loss: 0.00002394
Iteration 117/1000 | Loss: 0.00002394
Iteration 118/1000 | Loss: 0.00002394
Iteration 119/1000 | Loss: 0.00002394
Iteration 120/1000 | Loss: 0.00002394
Iteration 121/1000 | Loss: 0.00002394
Iteration 122/1000 | Loss: 0.00002394
Iteration 123/1000 | Loss: 0.00002394
Iteration 124/1000 | Loss: 0.00002394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.3936388970469125e-05, 2.3936388970469125e-05, 2.3936388970469125e-05, 2.3936388970469125e-05, 2.3936388970469125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3936388970469125e-05

Optimization complete. Final v2v error: 4.22744083404541 mm

Highest mean error: 4.732487678527832 mm for frame 19

Lowest mean error: 3.630128860473633 mm for frame 81

Saving results

Total time: 50.104907751083374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059073
Iteration 2/25 | Loss: 0.00175951
Iteration 3/25 | Loss: 0.00120652
Iteration 4/25 | Loss: 0.00114914
Iteration 5/25 | Loss: 0.00113578
Iteration 6/25 | Loss: 0.00113335
Iteration 7/25 | Loss: 0.00113321
Iteration 8/25 | Loss: 0.00113321
Iteration 9/25 | Loss: 0.00113321
Iteration 10/25 | Loss: 0.00113321
Iteration 11/25 | Loss: 0.00113321
Iteration 12/25 | Loss: 0.00113321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011332075810059905, 0.0011332075810059905, 0.0011332075810059905, 0.0011332075810059905, 0.0011332075810059905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011332075810059905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.90104151
Iteration 2/25 | Loss: 0.00094575
Iteration 3/25 | Loss: 0.00094571
Iteration 4/25 | Loss: 0.00094571
Iteration 5/25 | Loss: 0.00094571
Iteration 6/25 | Loss: 0.00094571
Iteration 7/25 | Loss: 0.00094571
Iteration 8/25 | Loss: 0.00094571
Iteration 9/25 | Loss: 0.00094571
Iteration 10/25 | Loss: 0.00094571
Iteration 11/25 | Loss: 0.00094571
Iteration 12/25 | Loss: 0.00094571
Iteration 13/25 | Loss: 0.00094571
Iteration 14/25 | Loss: 0.00094571
Iteration 15/25 | Loss: 0.00094571
Iteration 16/25 | Loss: 0.00094571
Iteration 17/25 | Loss: 0.00094571
Iteration 18/25 | Loss: 0.00094571
Iteration 19/25 | Loss: 0.00094571
Iteration 20/25 | Loss: 0.00094571
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009457100532017648, 0.0009457100532017648, 0.0009457100532017648, 0.0009457100532017648, 0.0009457100532017648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009457100532017648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094571
Iteration 2/1000 | Loss: 0.00005255
Iteration 3/1000 | Loss: 0.00003908
Iteration 4/1000 | Loss: 0.00003539
Iteration 5/1000 | Loss: 0.00003384
Iteration 6/1000 | Loss: 0.00003273
Iteration 7/1000 | Loss: 0.00003208
Iteration 8/1000 | Loss: 0.00003147
Iteration 9/1000 | Loss: 0.00003110
Iteration 10/1000 | Loss: 0.00003086
Iteration 11/1000 | Loss: 0.00003062
Iteration 12/1000 | Loss: 0.00003039
Iteration 13/1000 | Loss: 0.00003024
Iteration 14/1000 | Loss: 0.00003011
Iteration 15/1000 | Loss: 0.00003008
Iteration 16/1000 | Loss: 0.00003001
Iteration 17/1000 | Loss: 0.00002995
Iteration 18/1000 | Loss: 0.00002987
Iteration 19/1000 | Loss: 0.00002983
Iteration 20/1000 | Loss: 0.00002982
Iteration 21/1000 | Loss: 0.00002977
Iteration 22/1000 | Loss: 0.00002977
Iteration 23/1000 | Loss: 0.00002976
Iteration 24/1000 | Loss: 0.00002974
Iteration 25/1000 | Loss: 0.00002973
Iteration 26/1000 | Loss: 0.00002972
Iteration 27/1000 | Loss: 0.00002972
Iteration 28/1000 | Loss: 0.00002972
Iteration 29/1000 | Loss: 0.00002971
Iteration 30/1000 | Loss: 0.00002969
Iteration 31/1000 | Loss: 0.00002968
Iteration 32/1000 | Loss: 0.00002968
Iteration 33/1000 | Loss: 0.00002966
Iteration 34/1000 | Loss: 0.00002965
Iteration 35/1000 | Loss: 0.00002965
Iteration 36/1000 | Loss: 0.00002964
Iteration 37/1000 | Loss: 0.00002964
Iteration 38/1000 | Loss: 0.00002964
Iteration 39/1000 | Loss: 0.00002964
Iteration 40/1000 | Loss: 0.00002964
Iteration 41/1000 | Loss: 0.00002964
Iteration 42/1000 | Loss: 0.00002964
Iteration 43/1000 | Loss: 0.00002964
Iteration 44/1000 | Loss: 0.00002964
Iteration 45/1000 | Loss: 0.00002963
Iteration 46/1000 | Loss: 0.00002963
Iteration 47/1000 | Loss: 0.00002963
Iteration 48/1000 | Loss: 0.00002963
Iteration 49/1000 | Loss: 0.00002963
Iteration 50/1000 | Loss: 0.00002963
Iteration 51/1000 | Loss: 0.00002963
Iteration 52/1000 | Loss: 0.00002962
Iteration 53/1000 | Loss: 0.00002962
Iteration 54/1000 | Loss: 0.00002962
Iteration 55/1000 | Loss: 0.00002962
Iteration 56/1000 | Loss: 0.00002962
Iteration 57/1000 | Loss: 0.00002962
Iteration 58/1000 | Loss: 0.00002962
Iteration 59/1000 | Loss: 0.00002962
Iteration 60/1000 | Loss: 0.00002962
Iteration 61/1000 | Loss: 0.00002962
Iteration 62/1000 | Loss: 0.00002962
Iteration 63/1000 | Loss: 0.00002961
Iteration 64/1000 | Loss: 0.00002961
Iteration 65/1000 | Loss: 0.00002961
Iteration 66/1000 | Loss: 0.00002961
Iteration 67/1000 | Loss: 0.00002961
Iteration 68/1000 | Loss: 0.00002961
Iteration 69/1000 | Loss: 0.00002958
Iteration 70/1000 | Loss: 0.00002958
Iteration 71/1000 | Loss: 0.00002956
Iteration 72/1000 | Loss: 0.00002956
Iteration 73/1000 | Loss: 0.00002956
Iteration 74/1000 | Loss: 0.00002956
Iteration 75/1000 | Loss: 0.00002956
Iteration 76/1000 | Loss: 0.00002955
Iteration 77/1000 | Loss: 0.00002955
Iteration 78/1000 | Loss: 0.00002955
Iteration 79/1000 | Loss: 0.00002955
Iteration 80/1000 | Loss: 0.00002955
Iteration 81/1000 | Loss: 0.00002953
Iteration 82/1000 | Loss: 0.00002953
Iteration 83/1000 | Loss: 0.00002951
Iteration 84/1000 | Loss: 0.00002951
Iteration 85/1000 | Loss: 0.00002948
Iteration 86/1000 | Loss: 0.00002947
Iteration 87/1000 | Loss: 0.00002946
Iteration 88/1000 | Loss: 0.00002946
Iteration 89/1000 | Loss: 0.00002945
Iteration 90/1000 | Loss: 0.00002944
Iteration 91/1000 | Loss: 0.00002943
Iteration 92/1000 | Loss: 0.00002943
Iteration 93/1000 | Loss: 0.00002942
Iteration 94/1000 | Loss: 0.00002940
Iteration 95/1000 | Loss: 0.00002940
Iteration 96/1000 | Loss: 0.00002939
Iteration 97/1000 | Loss: 0.00002938
Iteration 98/1000 | Loss: 0.00002938
Iteration 99/1000 | Loss: 0.00002937
Iteration 100/1000 | Loss: 0.00002937
Iteration 101/1000 | Loss: 0.00002937
Iteration 102/1000 | Loss: 0.00002936
Iteration 103/1000 | Loss: 0.00002936
Iteration 104/1000 | Loss: 0.00002936
Iteration 105/1000 | Loss: 0.00002935
Iteration 106/1000 | Loss: 0.00002935
Iteration 107/1000 | Loss: 0.00002935
Iteration 108/1000 | Loss: 0.00002935
Iteration 109/1000 | Loss: 0.00002935
Iteration 110/1000 | Loss: 0.00002934
Iteration 111/1000 | Loss: 0.00002934
Iteration 112/1000 | Loss: 0.00002933
Iteration 113/1000 | Loss: 0.00002932
Iteration 114/1000 | Loss: 0.00002932
Iteration 115/1000 | Loss: 0.00002932
Iteration 116/1000 | Loss: 0.00002932
Iteration 117/1000 | Loss: 0.00002931
Iteration 118/1000 | Loss: 0.00002931
Iteration 119/1000 | Loss: 0.00002931
Iteration 120/1000 | Loss: 0.00002931
Iteration 121/1000 | Loss: 0.00002931
Iteration 122/1000 | Loss: 0.00002931
Iteration 123/1000 | Loss: 0.00002931
Iteration 124/1000 | Loss: 0.00002931
Iteration 125/1000 | Loss: 0.00002931
Iteration 126/1000 | Loss: 0.00002931
Iteration 127/1000 | Loss: 0.00002931
Iteration 128/1000 | Loss: 0.00002931
Iteration 129/1000 | Loss: 0.00002931
Iteration 130/1000 | Loss: 0.00002931
Iteration 131/1000 | Loss: 0.00002929
Iteration 132/1000 | Loss: 0.00002929
Iteration 133/1000 | Loss: 0.00002929
Iteration 134/1000 | Loss: 0.00002929
Iteration 135/1000 | Loss: 0.00002929
Iteration 136/1000 | Loss: 0.00002928
Iteration 137/1000 | Loss: 0.00002928
Iteration 138/1000 | Loss: 0.00002928
Iteration 139/1000 | Loss: 0.00002928
Iteration 140/1000 | Loss: 0.00002928
Iteration 141/1000 | Loss: 0.00002928
Iteration 142/1000 | Loss: 0.00002928
Iteration 143/1000 | Loss: 0.00002928
Iteration 144/1000 | Loss: 0.00002928
Iteration 145/1000 | Loss: 0.00002928
Iteration 146/1000 | Loss: 0.00002928
Iteration 147/1000 | Loss: 0.00002927
Iteration 148/1000 | Loss: 0.00002927
Iteration 149/1000 | Loss: 0.00002926
Iteration 150/1000 | Loss: 0.00002926
Iteration 151/1000 | Loss: 0.00002926
Iteration 152/1000 | Loss: 0.00002926
Iteration 153/1000 | Loss: 0.00002926
Iteration 154/1000 | Loss: 0.00002926
Iteration 155/1000 | Loss: 0.00002926
Iteration 156/1000 | Loss: 0.00002926
Iteration 157/1000 | Loss: 0.00002926
Iteration 158/1000 | Loss: 0.00002926
Iteration 159/1000 | Loss: 0.00002926
Iteration 160/1000 | Loss: 0.00002926
Iteration 161/1000 | Loss: 0.00002926
Iteration 162/1000 | Loss: 0.00002925
Iteration 163/1000 | Loss: 0.00002925
Iteration 164/1000 | Loss: 0.00002925
Iteration 165/1000 | Loss: 0.00002925
Iteration 166/1000 | Loss: 0.00002925
Iteration 167/1000 | Loss: 0.00002924
Iteration 168/1000 | Loss: 0.00002924
Iteration 169/1000 | Loss: 0.00002924
Iteration 170/1000 | Loss: 0.00002924
Iteration 171/1000 | Loss: 0.00002924
Iteration 172/1000 | Loss: 0.00002923
Iteration 173/1000 | Loss: 0.00002923
Iteration 174/1000 | Loss: 0.00002923
Iteration 175/1000 | Loss: 0.00002923
Iteration 176/1000 | Loss: 0.00002923
Iteration 177/1000 | Loss: 0.00002922
Iteration 178/1000 | Loss: 0.00002922
Iteration 179/1000 | Loss: 0.00002922
Iteration 180/1000 | Loss: 0.00002921
Iteration 181/1000 | Loss: 0.00002921
Iteration 182/1000 | Loss: 0.00002921
Iteration 183/1000 | Loss: 0.00002921
Iteration 184/1000 | Loss: 0.00002921
Iteration 185/1000 | Loss: 0.00002921
Iteration 186/1000 | Loss: 0.00002921
Iteration 187/1000 | Loss: 0.00002921
Iteration 188/1000 | Loss: 0.00002921
Iteration 189/1000 | Loss: 0.00002921
Iteration 190/1000 | Loss: 0.00002921
Iteration 191/1000 | Loss: 0.00002920
Iteration 192/1000 | Loss: 0.00002920
Iteration 193/1000 | Loss: 0.00002920
Iteration 194/1000 | Loss: 0.00002920
Iteration 195/1000 | Loss: 0.00002919
Iteration 196/1000 | Loss: 0.00002919
Iteration 197/1000 | Loss: 0.00002919
Iteration 198/1000 | Loss: 0.00002919
Iteration 199/1000 | Loss: 0.00002918
Iteration 200/1000 | Loss: 0.00002918
Iteration 201/1000 | Loss: 0.00002918
Iteration 202/1000 | Loss: 0.00002918
Iteration 203/1000 | Loss: 0.00002917
Iteration 204/1000 | Loss: 0.00002917
Iteration 205/1000 | Loss: 0.00002917
Iteration 206/1000 | Loss: 0.00002917
Iteration 207/1000 | Loss: 0.00002916
Iteration 208/1000 | Loss: 0.00002916
Iteration 209/1000 | Loss: 0.00002916
Iteration 210/1000 | Loss: 0.00002916
Iteration 211/1000 | Loss: 0.00002916
Iteration 212/1000 | Loss: 0.00002916
Iteration 213/1000 | Loss: 0.00002916
Iteration 214/1000 | Loss: 0.00002916
Iteration 215/1000 | Loss: 0.00002915
Iteration 216/1000 | Loss: 0.00002915
Iteration 217/1000 | Loss: 0.00002915
Iteration 218/1000 | Loss: 0.00002915
Iteration 219/1000 | Loss: 0.00002915
Iteration 220/1000 | Loss: 0.00002915
Iteration 221/1000 | Loss: 0.00002915
Iteration 222/1000 | Loss: 0.00002915
Iteration 223/1000 | Loss: 0.00002915
Iteration 224/1000 | Loss: 0.00002914
Iteration 225/1000 | Loss: 0.00002914
Iteration 226/1000 | Loss: 0.00002914
Iteration 227/1000 | Loss: 0.00002914
Iteration 228/1000 | Loss: 0.00002914
Iteration 229/1000 | Loss: 0.00002914
Iteration 230/1000 | Loss: 0.00002914
Iteration 231/1000 | Loss: 0.00002914
Iteration 232/1000 | Loss: 0.00002914
Iteration 233/1000 | Loss: 0.00002914
Iteration 234/1000 | Loss: 0.00002914
Iteration 235/1000 | Loss: 0.00002913
Iteration 236/1000 | Loss: 0.00002913
Iteration 237/1000 | Loss: 0.00002913
Iteration 238/1000 | Loss: 0.00002913
Iteration 239/1000 | Loss: 0.00002913
Iteration 240/1000 | Loss: 0.00002913
Iteration 241/1000 | Loss: 0.00002913
Iteration 242/1000 | Loss: 0.00002913
Iteration 243/1000 | Loss: 0.00002913
Iteration 244/1000 | Loss: 0.00002913
Iteration 245/1000 | Loss: 0.00002913
Iteration 246/1000 | Loss: 0.00002913
Iteration 247/1000 | Loss: 0.00002913
Iteration 248/1000 | Loss: 0.00002913
Iteration 249/1000 | Loss: 0.00002913
Iteration 250/1000 | Loss: 0.00002913
Iteration 251/1000 | Loss: 0.00002913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [2.9131557312211953e-05, 2.9131557312211953e-05, 2.9131557312211953e-05, 2.9131557312211953e-05, 2.9131557312211953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9131557312211953e-05

Optimization complete. Final v2v error: 4.376467227935791 mm

Highest mean error: 5.5833587646484375 mm for frame 25

Lowest mean error: 3.3321311473846436 mm for frame 156

Saving results

Total time: 58.255449533462524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467766
Iteration 2/25 | Loss: 0.00117972
Iteration 3/25 | Loss: 0.00103337
Iteration 4/25 | Loss: 0.00101063
Iteration 5/25 | Loss: 0.00100281
Iteration 6/25 | Loss: 0.00100129
Iteration 7/25 | Loss: 0.00100129
Iteration 8/25 | Loss: 0.00100129
Iteration 9/25 | Loss: 0.00100129
Iteration 10/25 | Loss: 0.00100129
Iteration 11/25 | Loss: 0.00100129
Iteration 12/25 | Loss: 0.00100129
Iteration 13/25 | Loss: 0.00100129
Iteration 14/25 | Loss: 0.00100129
Iteration 15/25 | Loss: 0.00100129
Iteration 16/25 | Loss: 0.00100129
Iteration 17/25 | Loss: 0.00100129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010012905113399029, 0.0010012905113399029, 0.0010012905113399029, 0.0010012905113399029, 0.0010012905113399029]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010012905113399029

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46072781
Iteration 2/25 | Loss: 0.00072029
Iteration 3/25 | Loss: 0.00072029
Iteration 4/25 | Loss: 0.00072029
Iteration 5/25 | Loss: 0.00072029
Iteration 6/25 | Loss: 0.00072029
Iteration 7/25 | Loss: 0.00072029
Iteration 8/25 | Loss: 0.00072029
Iteration 9/25 | Loss: 0.00072029
Iteration 10/25 | Loss: 0.00072029
Iteration 11/25 | Loss: 0.00072029
Iteration 12/25 | Loss: 0.00072029
Iteration 13/25 | Loss: 0.00072029
Iteration 14/25 | Loss: 0.00072029
Iteration 15/25 | Loss: 0.00072029
Iteration 16/25 | Loss: 0.00072029
Iteration 17/25 | Loss: 0.00072029
Iteration 18/25 | Loss: 0.00072029
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007202858687378466, 0.0007202858687378466, 0.0007202858687378466, 0.0007202858687378466, 0.0007202858687378466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007202858687378466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072029
Iteration 2/1000 | Loss: 0.00003648
Iteration 3/1000 | Loss: 0.00002840
Iteration 4/1000 | Loss: 0.00002662
Iteration 5/1000 | Loss: 0.00002547
Iteration 6/1000 | Loss: 0.00002529
Iteration 7/1000 | Loss: 0.00002488
Iteration 8/1000 | Loss: 0.00002456
Iteration 9/1000 | Loss: 0.00002438
Iteration 10/1000 | Loss: 0.00002430
Iteration 11/1000 | Loss: 0.00002428
Iteration 12/1000 | Loss: 0.00002426
Iteration 13/1000 | Loss: 0.00002425
Iteration 14/1000 | Loss: 0.00002424
Iteration 15/1000 | Loss: 0.00002424
Iteration 16/1000 | Loss: 0.00002424
Iteration 17/1000 | Loss: 0.00002423
Iteration 18/1000 | Loss: 0.00002423
Iteration 19/1000 | Loss: 0.00002422
Iteration 20/1000 | Loss: 0.00002421
Iteration 21/1000 | Loss: 0.00002421
Iteration 22/1000 | Loss: 0.00002421
Iteration 23/1000 | Loss: 0.00002421
Iteration 24/1000 | Loss: 0.00002420
Iteration 25/1000 | Loss: 0.00002420
Iteration 26/1000 | Loss: 0.00002420
Iteration 27/1000 | Loss: 0.00002420
Iteration 28/1000 | Loss: 0.00002419
Iteration 29/1000 | Loss: 0.00002419
Iteration 30/1000 | Loss: 0.00002419
Iteration 31/1000 | Loss: 0.00002418
Iteration 32/1000 | Loss: 0.00002418
Iteration 33/1000 | Loss: 0.00002418
Iteration 34/1000 | Loss: 0.00002417
Iteration 35/1000 | Loss: 0.00002417
Iteration 36/1000 | Loss: 0.00002417
Iteration 37/1000 | Loss: 0.00002417
Iteration 38/1000 | Loss: 0.00002417
Iteration 39/1000 | Loss: 0.00002416
Iteration 40/1000 | Loss: 0.00002416
Iteration 41/1000 | Loss: 0.00002416
Iteration 42/1000 | Loss: 0.00002416
Iteration 43/1000 | Loss: 0.00002416
Iteration 44/1000 | Loss: 0.00002416
Iteration 45/1000 | Loss: 0.00002416
Iteration 46/1000 | Loss: 0.00002416
Iteration 47/1000 | Loss: 0.00002416
Iteration 48/1000 | Loss: 0.00002416
Iteration 49/1000 | Loss: 0.00002415
Iteration 50/1000 | Loss: 0.00002415
Iteration 51/1000 | Loss: 0.00002415
Iteration 52/1000 | Loss: 0.00002415
Iteration 53/1000 | Loss: 0.00002415
Iteration 54/1000 | Loss: 0.00002415
Iteration 55/1000 | Loss: 0.00002415
Iteration 56/1000 | Loss: 0.00002415
Iteration 57/1000 | Loss: 0.00002415
Iteration 58/1000 | Loss: 0.00002414
Iteration 59/1000 | Loss: 0.00002414
Iteration 60/1000 | Loss: 0.00002414
Iteration 61/1000 | Loss: 0.00002414
Iteration 62/1000 | Loss: 0.00002414
Iteration 63/1000 | Loss: 0.00002413
Iteration 64/1000 | Loss: 0.00002413
Iteration 65/1000 | Loss: 0.00002413
Iteration 66/1000 | Loss: 0.00002413
Iteration 67/1000 | Loss: 0.00002413
Iteration 68/1000 | Loss: 0.00002413
Iteration 69/1000 | Loss: 0.00002413
Iteration 70/1000 | Loss: 0.00002413
Iteration 71/1000 | Loss: 0.00002413
Iteration 72/1000 | Loss: 0.00002413
Iteration 73/1000 | Loss: 0.00002413
Iteration 74/1000 | Loss: 0.00002413
Iteration 75/1000 | Loss: 0.00002413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.413320544292219e-05, 2.413320544292219e-05, 2.413320544292219e-05, 2.413320544292219e-05, 2.413320544292219e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.413320544292219e-05

Optimization complete. Final v2v error: 4.2300920486450195 mm

Highest mean error: 4.66516637802124 mm for frame 197

Lowest mean error: 3.915221929550171 mm for frame 24

Saving results

Total time: 29.53507685661316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044581
Iteration 2/25 | Loss: 0.00183912
Iteration 3/25 | Loss: 0.00128069
Iteration 4/25 | Loss: 0.00123655
Iteration 5/25 | Loss: 0.00121799
Iteration 6/25 | Loss: 0.00121358
Iteration 7/25 | Loss: 0.00121267
Iteration 8/25 | Loss: 0.00121267
Iteration 9/25 | Loss: 0.00121267
Iteration 10/25 | Loss: 0.00121267
Iteration 11/25 | Loss: 0.00121267
Iteration 12/25 | Loss: 0.00121267
Iteration 13/25 | Loss: 0.00121267
Iteration 14/25 | Loss: 0.00121267
Iteration 15/25 | Loss: 0.00121267
Iteration 16/25 | Loss: 0.00121267
Iteration 17/25 | Loss: 0.00121267
Iteration 18/25 | Loss: 0.00121267
Iteration 19/25 | Loss: 0.00121267
Iteration 20/25 | Loss: 0.00121267
Iteration 21/25 | Loss: 0.00121267
Iteration 22/25 | Loss: 0.00121267
Iteration 23/25 | Loss: 0.00121267
Iteration 24/25 | Loss: 0.00121267
Iteration 25/25 | Loss: 0.00121267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92679638
Iteration 2/25 | Loss: 0.00078215
Iteration 3/25 | Loss: 0.00078214
Iteration 4/25 | Loss: 0.00078214
Iteration 5/25 | Loss: 0.00078214
Iteration 6/25 | Loss: 0.00078214
Iteration 7/25 | Loss: 0.00078214
Iteration 8/25 | Loss: 0.00078214
Iteration 9/25 | Loss: 0.00078214
Iteration 10/25 | Loss: 0.00078214
Iteration 11/25 | Loss: 0.00078214
Iteration 12/25 | Loss: 0.00078214
Iteration 13/25 | Loss: 0.00078214
Iteration 14/25 | Loss: 0.00078214
Iteration 15/25 | Loss: 0.00078214
Iteration 16/25 | Loss: 0.00078214
Iteration 17/25 | Loss: 0.00078214
Iteration 18/25 | Loss: 0.00078214
Iteration 19/25 | Loss: 0.00078214
Iteration 20/25 | Loss: 0.00078214
Iteration 21/25 | Loss: 0.00078214
Iteration 22/25 | Loss: 0.00078214
Iteration 23/25 | Loss: 0.00078214
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007821412873454392, 0.0007821412873454392, 0.0007821412873454392, 0.0007821412873454392, 0.0007821412873454392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007821412873454392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078214
Iteration 2/1000 | Loss: 0.00007958
Iteration 3/1000 | Loss: 0.00006043
Iteration 4/1000 | Loss: 0.00005687
Iteration 5/1000 | Loss: 0.00005394
Iteration 6/1000 | Loss: 0.00005246
Iteration 7/1000 | Loss: 0.00005154
Iteration 8/1000 | Loss: 0.00005098
Iteration 9/1000 | Loss: 0.00005031
Iteration 10/1000 | Loss: 0.00004991
Iteration 11/1000 | Loss: 0.00004980
Iteration 12/1000 | Loss: 0.00004978
Iteration 13/1000 | Loss: 0.00004964
Iteration 14/1000 | Loss: 0.00004962
Iteration 15/1000 | Loss: 0.00004949
Iteration 16/1000 | Loss: 0.00004943
Iteration 17/1000 | Loss: 0.00004939
Iteration 18/1000 | Loss: 0.00004939
Iteration 19/1000 | Loss: 0.00004935
Iteration 20/1000 | Loss: 0.00004929
Iteration 21/1000 | Loss: 0.00004928
Iteration 22/1000 | Loss: 0.00004926
Iteration 23/1000 | Loss: 0.00004926
Iteration 24/1000 | Loss: 0.00004925
Iteration 25/1000 | Loss: 0.00004925
Iteration 26/1000 | Loss: 0.00004925
Iteration 27/1000 | Loss: 0.00004925
Iteration 28/1000 | Loss: 0.00004924
Iteration 29/1000 | Loss: 0.00004924
Iteration 30/1000 | Loss: 0.00004924
Iteration 31/1000 | Loss: 0.00004924
Iteration 32/1000 | Loss: 0.00004924
Iteration 33/1000 | Loss: 0.00004924
Iteration 34/1000 | Loss: 0.00004924
Iteration 35/1000 | Loss: 0.00004924
Iteration 36/1000 | Loss: 0.00004924
Iteration 37/1000 | Loss: 0.00004924
Iteration 38/1000 | Loss: 0.00004924
Iteration 39/1000 | Loss: 0.00004924
Iteration 40/1000 | Loss: 0.00004924
Iteration 41/1000 | Loss: 0.00004924
Iteration 42/1000 | Loss: 0.00004923
Iteration 43/1000 | Loss: 0.00004923
Iteration 44/1000 | Loss: 0.00004923
Iteration 45/1000 | Loss: 0.00004923
Iteration 46/1000 | Loss: 0.00004923
Iteration 47/1000 | Loss: 0.00004923
Iteration 48/1000 | Loss: 0.00004922
Iteration 49/1000 | Loss: 0.00004922
Iteration 50/1000 | Loss: 0.00004922
Iteration 51/1000 | Loss: 0.00004922
Iteration 52/1000 | Loss: 0.00004922
Iteration 53/1000 | Loss: 0.00004921
Iteration 54/1000 | Loss: 0.00004921
Iteration 55/1000 | Loss: 0.00004921
Iteration 56/1000 | Loss: 0.00004921
Iteration 57/1000 | Loss: 0.00004921
Iteration 58/1000 | Loss: 0.00004921
Iteration 59/1000 | Loss: 0.00004921
Iteration 60/1000 | Loss: 0.00004921
Iteration 61/1000 | Loss: 0.00004921
Iteration 62/1000 | Loss: 0.00004921
Iteration 63/1000 | Loss: 0.00004921
Iteration 64/1000 | Loss: 0.00004920
Iteration 65/1000 | Loss: 0.00004920
Iteration 66/1000 | Loss: 0.00004920
Iteration 67/1000 | Loss: 0.00004920
Iteration 68/1000 | Loss: 0.00004920
Iteration 69/1000 | Loss: 0.00004920
Iteration 70/1000 | Loss: 0.00004920
Iteration 71/1000 | Loss: 0.00004920
Iteration 72/1000 | Loss: 0.00004920
Iteration 73/1000 | Loss: 0.00004919
Iteration 74/1000 | Loss: 0.00004918
Iteration 75/1000 | Loss: 0.00004918
Iteration 76/1000 | Loss: 0.00004918
Iteration 77/1000 | Loss: 0.00004917
Iteration 78/1000 | Loss: 0.00004917
Iteration 79/1000 | Loss: 0.00004917
Iteration 80/1000 | Loss: 0.00004916
Iteration 81/1000 | Loss: 0.00004916
Iteration 82/1000 | Loss: 0.00004916
Iteration 83/1000 | Loss: 0.00004916
Iteration 84/1000 | Loss: 0.00004916
Iteration 85/1000 | Loss: 0.00004916
Iteration 86/1000 | Loss: 0.00004916
Iteration 87/1000 | Loss: 0.00004916
Iteration 88/1000 | Loss: 0.00004915
Iteration 89/1000 | Loss: 0.00004915
Iteration 90/1000 | Loss: 0.00004915
Iteration 91/1000 | Loss: 0.00004915
Iteration 92/1000 | Loss: 0.00004914
Iteration 93/1000 | Loss: 0.00004914
Iteration 94/1000 | Loss: 0.00004914
Iteration 95/1000 | Loss: 0.00004914
Iteration 96/1000 | Loss: 0.00004914
Iteration 97/1000 | Loss: 0.00004914
Iteration 98/1000 | Loss: 0.00004914
Iteration 99/1000 | Loss: 0.00004914
Iteration 100/1000 | Loss: 0.00004914
Iteration 101/1000 | Loss: 0.00004914
Iteration 102/1000 | Loss: 0.00004914
Iteration 103/1000 | Loss: 0.00004914
Iteration 104/1000 | Loss: 0.00004914
Iteration 105/1000 | Loss: 0.00004914
Iteration 106/1000 | Loss: 0.00004914
Iteration 107/1000 | Loss: 0.00004914
Iteration 108/1000 | Loss: 0.00004914
Iteration 109/1000 | Loss: 0.00004914
Iteration 110/1000 | Loss: 0.00004914
Iteration 111/1000 | Loss: 0.00004914
Iteration 112/1000 | Loss: 0.00004914
Iteration 113/1000 | Loss: 0.00004914
Iteration 114/1000 | Loss: 0.00004914
Iteration 115/1000 | Loss: 0.00004914
Iteration 116/1000 | Loss: 0.00004914
Iteration 117/1000 | Loss: 0.00004914
Iteration 118/1000 | Loss: 0.00004914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [4.914027522318065e-05, 4.914027522318065e-05, 4.914027522318065e-05, 4.914027522318065e-05, 4.914027522318065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.914027522318065e-05

Optimization complete. Final v2v error: 5.733952045440674 mm

Highest mean error: 6.977570533752441 mm for frame 88

Lowest mean error: 4.764504909515381 mm for frame 59

Saving results

Total time: 41.446292877197266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00699227
Iteration 2/25 | Loss: 0.00135810
Iteration 3/25 | Loss: 0.00109663
Iteration 4/25 | Loss: 0.00105978
Iteration 5/25 | Loss: 0.00105175
Iteration 6/25 | Loss: 0.00104952
Iteration 7/25 | Loss: 0.00104924
Iteration 8/25 | Loss: 0.00104924
Iteration 9/25 | Loss: 0.00104924
Iteration 10/25 | Loss: 0.00104924
Iteration 11/25 | Loss: 0.00104924
Iteration 12/25 | Loss: 0.00104924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001049235463142395, 0.001049235463142395, 0.001049235463142395, 0.001049235463142395, 0.001049235463142395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001049235463142395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66330230
Iteration 2/25 | Loss: 0.00082194
Iteration 3/25 | Loss: 0.00082193
Iteration 4/25 | Loss: 0.00082193
Iteration 5/25 | Loss: 0.00082193
Iteration 6/25 | Loss: 0.00082193
Iteration 7/25 | Loss: 0.00082193
Iteration 8/25 | Loss: 0.00082193
Iteration 9/25 | Loss: 0.00082193
Iteration 10/25 | Loss: 0.00082193
Iteration 11/25 | Loss: 0.00082193
Iteration 12/25 | Loss: 0.00082193
Iteration 13/25 | Loss: 0.00082193
Iteration 14/25 | Loss: 0.00082193
Iteration 15/25 | Loss: 0.00082193
Iteration 16/25 | Loss: 0.00082193
Iteration 17/25 | Loss: 0.00082193
Iteration 18/25 | Loss: 0.00082193
Iteration 19/25 | Loss: 0.00082193
Iteration 20/25 | Loss: 0.00082193
Iteration 21/25 | Loss: 0.00082193
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008219301817007363, 0.0008219301817007363, 0.0008219301817007363, 0.0008219301817007363, 0.0008219301817007363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008219301817007363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082193
Iteration 2/1000 | Loss: 0.00006291
Iteration 3/1000 | Loss: 0.00003628
Iteration 4/1000 | Loss: 0.00002992
Iteration 5/1000 | Loss: 0.00002791
Iteration 6/1000 | Loss: 0.00002662
Iteration 7/1000 | Loss: 0.00002584
Iteration 8/1000 | Loss: 0.00002527
Iteration 9/1000 | Loss: 0.00002487
Iteration 10/1000 | Loss: 0.00002456
Iteration 11/1000 | Loss: 0.00002446
Iteration 12/1000 | Loss: 0.00002440
Iteration 13/1000 | Loss: 0.00002429
Iteration 14/1000 | Loss: 0.00002422
Iteration 15/1000 | Loss: 0.00002415
Iteration 16/1000 | Loss: 0.00002411
Iteration 17/1000 | Loss: 0.00002396
Iteration 18/1000 | Loss: 0.00002396
Iteration 19/1000 | Loss: 0.00002396
Iteration 20/1000 | Loss: 0.00002394
Iteration 21/1000 | Loss: 0.00002390
Iteration 22/1000 | Loss: 0.00002389
Iteration 23/1000 | Loss: 0.00002387
Iteration 24/1000 | Loss: 0.00002386
Iteration 25/1000 | Loss: 0.00002386
Iteration 26/1000 | Loss: 0.00002384
Iteration 27/1000 | Loss: 0.00002383
Iteration 28/1000 | Loss: 0.00002381
Iteration 29/1000 | Loss: 0.00002378
Iteration 30/1000 | Loss: 0.00002375
Iteration 31/1000 | Loss: 0.00002374
Iteration 32/1000 | Loss: 0.00002374
Iteration 33/1000 | Loss: 0.00002373
Iteration 34/1000 | Loss: 0.00002372
Iteration 35/1000 | Loss: 0.00002371
Iteration 36/1000 | Loss: 0.00002371
Iteration 37/1000 | Loss: 0.00002370
Iteration 38/1000 | Loss: 0.00002369
Iteration 39/1000 | Loss: 0.00002369
Iteration 40/1000 | Loss: 0.00002369
Iteration 41/1000 | Loss: 0.00002369
Iteration 42/1000 | Loss: 0.00002369
Iteration 43/1000 | Loss: 0.00002369
Iteration 44/1000 | Loss: 0.00002369
Iteration 45/1000 | Loss: 0.00002368
Iteration 46/1000 | Loss: 0.00002367
Iteration 47/1000 | Loss: 0.00002366
Iteration 48/1000 | Loss: 0.00002366
Iteration 49/1000 | Loss: 0.00002365
Iteration 50/1000 | Loss: 0.00002365
Iteration 51/1000 | Loss: 0.00002365
Iteration 52/1000 | Loss: 0.00002364
Iteration 53/1000 | Loss: 0.00002364
Iteration 54/1000 | Loss: 0.00002363
Iteration 55/1000 | Loss: 0.00002363
Iteration 56/1000 | Loss: 0.00002363
Iteration 57/1000 | Loss: 0.00002363
Iteration 58/1000 | Loss: 0.00002362
Iteration 59/1000 | Loss: 0.00002362
Iteration 60/1000 | Loss: 0.00002362
Iteration 61/1000 | Loss: 0.00002361
Iteration 62/1000 | Loss: 0.00002361
Iteration 63/1000 | Loss: 0.00002361
Iteration 64/1000 | Loss: 0.00002361
Iteration 65/1000 | Loss: 0.00002361
Iteration 66/1000 | Loss: 0.00002360
Iteration 67/1000 | Loss: 0.00002360
Iteration 68/1000 | Loss: 0.00002360
Iteration 69/1000 | Loss: 0.00002359
Iteration 70/1000 | Loss: 0.00002359
Iteration 71/1000 | Loss: 0.00002359
Iteration 72/1000 | Loss: 0.00002358
Iteration 73/1000 | Loss: 0.00002358
Iteration 74/1000 | Loss: 0.00002358
Iteration 75/1000 | Loss: 0.00002358
Iteration 76/1000 | Loss: 0.00002358
Iteration 77/1000 | Loss: 0.00002357
Iteration 78/1000 | Loss: 0.00002357
Iteration 79/1000 | Loss: 0.00002357
Iteration 80/1000 | Loss: 0.00002357
Iteration 81/1000 | Loss: 0.00002357
Iteration 82/1000 | Loss: 0.00002357
Iteration 83/1000 | Loss: 0.00002357
Iteration 84/1000 | Loss: 0.00002357
Iteration 85/1000 | Loss: 0.00002357
Iteration 86/1000 | Loss: 0.00002357
Iteration 87/1000 | Loss: 0.00002357
Iteration 88/1000 | Loss: 0.00002357
Iteration 89/1000 | Loss: 0.00002357
Iteration 90/1000 | Loss: 0.00002357
Iteration 91/1000 | Loss: 0.00002357
Iteration 92/1000 | Loss: 0.00002357
Iteration 93/1000 | Loss: 0.00002357
Iteration 94/1000 | Loss: 0.00002357
Iteration 95/1000 | Loss: 0.00002357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.35720508499071e-05, 2.35720508499071e-05, 2.35720508499071e-05, 2.35720508499071e-05, 2.35720508499071e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.35720508499071e-05

Optimization complete. Final v2v error: 4.087776184082031 mm

Highest mean error: 5.20095157623291 mm for frame 14

Lowest mean error: 3.4019100666046143 mm for frame 35

Saving results

Total time: 41.00924348831177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00931005
Iteration 2/25 | Loss: 0.00137888
Iteration 3/25 | Loss: 0.00118246
Iteration 4/25 | Loss: 0.00115963
Iteration 5/25 | Loss: 0.00115227
Iteration 6/25 | Loss: 0.00115056
Iteration 7/25 | Loss: 0.00115056
Iteration 8/25 | Loss: 0.00115056
Iteration 9/25 | Loss: 0.00115056
Iteration 10/25 | Loss: 0.00115056
Iteration 11/25 | Loss: 0.00115056
Iteration 12/25 | Loss: 0.00115056
Iteration 13/25 | Loss: 0.00115056
Iteration 14/25 | Loss: 0.00115056
Iteration 15/25 | Loss: 0.00115056
Iteration 16/25 | Loss: 0.00115056
Iteration 17/25 | Loss: 0.00115056
Iteration 18/25 | Loss: 0.00115056
Iteration 19/25 | Loss: 0.00115056
Iteration 20/25 | Loss: 0.00115056
Iteration 21/25 | Loss: 0.00115056
Iteration 22/25 | Loss: 0.00115056
Iteration 23/25 | Loss: 0.00115056
Iteration 24/25 | Loss: 0.00115056
Iteration 25/25 | Loss: 0.00115056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97179359
Iteration 2/25 | Loss: 0.00091315
Iteration 3/25 | Loss: 0.00091315
Iteration 4/25 | Loss: 0.00091315
Iteration 5/25 | Loss: 0.00091315
Iteration 6/25 | Loss: 0.00091315
Iteration 7/25 | Loss: 0.00091315
Iteration 8/25 | Loss: 0.00091315
Iteration 9/25 | Loss: 0.00091315
Iteration 10/25 | Loss: 0.00091315
Iteration 11/25 | Loss: 0.00091315
Iteration 12/25 | Loss: 0.00091315
Iteration 13/25 | Loss: 0.00091315
Iteration 14/25 | Loss: 0.00091315
Iteration 15/25 | Loss: 0.00091315
Iteration 16/25 | Loss: 0.00091315
Iteration 17/25 | Loss: 0.00091315
Iteration 18/25 | Loss: 0.00091315
Iteration 19/25 | Loss: 0.00091315
Iteration 20/25 | Loss: 0.00091315
Iteration 21/25 | Loss: 0.00091315
Iteration 22/25 | Loss: 0.00091315
Iteration 23/25 | Loss: 0.00091315
Iteration 24/25 | Loss: 0.00091315
Iteration 25/25 | Loss: 0.00091315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091315
Iteration 2/1000 | Loss: 0.00005041
Iteration 3/1000 | Loss: 0.00003979
Iteration 4/1000 | Loss: 0.00003565
Iteration 5/1000 | Loss: 0.00003401
Iteration 6/1000 | Loss: 0.00003262
Iteration 7/1000 | Loss: 0.00003164
Iteration 8/1000 | Loss: 0.00003117
Iteration 9/1000 | Loss: 0.00003088
Iteration 10/1000 | Loss: 0.00003068
Iteration 11/1000 | Loss: 0.00003066
Iteration 12/1000 | Loss: 0.00003063
Iteration 13/1000 | Loss: 0.00003062
Iteration 14/1000 | Loss: 0.00003058
Iteration 15/1000 | Loss: 0.00003054
Iteration 16/1000 | Loss: 0.00003054
Iteration 17/1000 | Loss: 0.00003054
Iteration 18/1000 | Loss: 0.00003054
Iteration 19/1000 | Loss: 0.00003054
Iteration 20/1000 | Loss: 0.00003053
Iteration 21/1000 | Loss: 0.00003053
Iteration 22/1000 | Loss: 0.00003053
Iteration 23/1000 | Loss: 0.00003053
Iteration 24/1000 | Loss: 0.00003053
Iteration 25/1000 | Loss: 0.00003053
Iteration 26/1000 | Loss: 0.00003052
Iteration 27/1000 | Loss: 0.00003052
Iteration 28/1000 | Loss: 0.00003052
Iteration 29/1000 | Loss: 0.00003051
Iteration 30/1000 | Loss: 0.00003051
Iteration 31/1000 | Loss: 0.00003051
Iteration 32/1000 | Loss: 0.00003051
Iteration 33/1000 | Loss: 0.00003051
Iteration 34/1000 | Loss: 0.00003050
Iteration 35/1000 | Loss: 0.00003050
Iteration 36/1000 | Loss: 0.00003050
Iteration 37/1000 | Loss: 0.00003050
Iteration 38/1000 | Loss: 0.00003050
Iteration 39/1000 | Loss: 0.00003050
Iteration 40/1000 | Loss: 0.00003050
Iteration 41/1000 | Loss: 0.00003050
Iteration 42/1000 | Loss: 0.00003050
Iteration 43/1000 | Loss: 0.00003050
Iteration 44/1000 | Loss: 0.00003049
Iteration 45/1000 | Loss: 0.00003049
Iteration 46/1000 | Loss: 0.00003049
Iteration 47/1000 | Loss: 0.00003049
Iteration 48/1000 | Loss: 0.00003049
Iteration 49/1000 | Loss: 0.00003049
Iteration 50/1000 | Loss: 0.00003049
Iteration 51/1000 | Loss: 0.00003049
Iteration 52/1000 | Loss: 0.00003048
Iteration 53/1000 | Loss: 0.00003048
Iteration 54/1000 | Loss: 0.00003047
Iteration 55/1000 | Loss: 0.00003047
Iteration 56/1000 | Loss: 0.00003047
Iteration 57/1000 | Loss: 0.00003046
Iteration 58/1000 | Loss: 0.00003046
Iteration 59/1000 | Loss: 0.00003046
Iteration 60/1000 | Loss: 0.00003046
Iteration 61/1000 | Loss: 0.00003046
Iteration 62/1000 | Loss: 0.00003046
Iteration 63/1000 | Loss: 0.00003046
Iteration 64/1000 | Loss: 0.00003046
Iteration 65/1000 | Loss: 0.00003045
Iteration 66/1000 | Loss: 0.00003045
Iteration 67/1000 | Loss: 0.00003045
Iteration 68/1000 | Loss: 0.00003045
Iteration 69/1000 | Loss: 0.00003045
Iteration 70/1000 | Loss: 0.00003045
Iteration 71/1000 | Loss: 0.00003045
Iteration 72/1000 | Loss: 0.00003045
Iteration 73/1000 | Loss: 0.00003045
Iteration 74/1000 | Loss: 0.00003045
Iteration 75/1000 | Loss: 0.00003045
Iteration 76/1000 | Loss: 0.00003045
Iteration 77/1000 | Loss: 0.00003045
Iteration 78/1000 | Loss: 0.00003045
Iteration 79/1000 | Loss: 0.00003045
Iteration 80/1000 | Loss: 0.00003045
Iteration 81/1000 | Loss: 0.00003045
Iteration 82/1000 | Loss: 0.00003045
Iteration 83/1000 | Loss: 0.00003045
Iteration 84/1000 | Loss: 0.00003045
Iteration 85/1000 | Loss: 0.00003045
Iteration 86/1000 | Loss: 0.00003045
Iteration 87/1000 | Loss: 0.00003045
Iteration 88/1000 | Loss: 0.00003045
Iteration 89/1000 | Loss: 0.00003045
Iteration 90/1000 | Loss: 0.00003045
Iteration 91/1000 | Loss: 0.00003045
Iteration 92/1000 | Loss: 0.00003045
Iteration 93/1000 | Loss: 0.00003045
Iteration 94/1000 | Loss: 0.00003045
Iteration 95/1000 | Loss: 0.00003045
Iteration 96/1000 | Loss: 0.00003045
Iteration 97/1000 | Loss: 0.00003045
Iteration 98/1000 | Loss: 0.00003045
Iteration 99/1000 | Loss: 0.00003045
Iteration 100/1000 | Loss: 0.00003045
Iteration 101/1000 | Loss: 0.00003045
Iteration 102/1000 | Loss: 0.00003045
Iteration 103/1000 | Loss: 0.00003045
Iteration 104/1000 | Loss: 0.00003045
Iteration 105/1000 | Loss: 0.00003045
Iteration 106/1000 | Loss: 0.00003045
Iteration 107/1000 | Loss: 0.00003045
Iteration 108/1000 | Loss: 0.00003045
Iteration 109/1000 | Loss: 0.00003045
Iteration 110/1000 | Loss: 0.00003045
Iteration 111/1000 | Loss: 0.00003045
Iteration 112/1000 | Loss: 0.00003045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [3.0453871659119613e-05, 3.0453871659119613e-05, 3.0453871659119613e-05, 3.0453871659119613e-05, 3.0453871659119613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0453871659119613e-05

Optimization complete. Final v2v error: 4.586239337921143 mm

Highest mean error: 4.772358417510986 mm for frame 0

Lowest mean error: 4.480185031890869 mm for frame 91

Saving results

Total time: 29.080958604812622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436978
Iteration 2/25 | Loss: 0.00130005
Iteration 3/25 | Loss: 0.00103750
Iteration 4/25 | Loss: 0.00100783
Iteration 5/25 | Loss: 0.00100016
Iteration 6/25 | Loss: 0.00099846
Iteration 7/25 | Loss: 0.00099813
Iteration 8/25 | Loss: 0.00099813
Iteration 9/25 | Loss: 0.00099813
Iteration 10/25 | Loss: 0.00099813
Iteration 11/25 | Loss: 0.00099812
Iteration 12/25 | Loss: 0.00099813
Iteration 13/25 | Loss: 0.00099813
Iteration 14/25 | Loss: 0.00099813
Iteration 15/25 | Loss: 0.00099813
Iteration 16/25 | Loss: 0.00099813
Iteration 17/25 | Loss: 0.00099813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009981250623241067, 0.0009981250623241067, 0.0009981250623241067, 0.0009981250623241067, 0.0009981250623241067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009981250623241067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46035469
Iteration 2/25 | Loss: 0.00074156
Iteration 3/25 | Loss: 0.00074156
Iteration 4/25 | Loss: 0.00074156
Iteration 5/25 | Loss: 0.00074156
Iteration 6/25 | Loss: 0.00074156
Iteration 7/25 | Loss: 0.00074156
Iteration 8/25 | Loss: 0.00074156
Iteration 9/25 | Loss: 0.00074156
Iteration 10/25 | Loss: 0.00074156
Iteration 11/25 | Loss: 0.00074156
Iteration 12/25 | Loss: 0.00074156
Iteration 13/25 | Loss: 0.00074156
Iteration 14/25 | Loss: 0.00074156
Iteration 15/25 | Loss: 0.00074156
Iteration 16/25 | Loss: 0.00074156
Iteration 17/25 | Loss: 0.00074156
Iteration 18/25 | Loss: 0.00074156
Iteration 19/25 | Loss: 0.00074156
Iteration 20/25 | Loss: 0.00074156
Iteration 21/25 | Loss: 0.00074156
Iteration 22/25 | Loss: 0.00074156
Iteration 23/25 | Loss: 0.00074156
Iteration 24/25 | Loss: 0.00074156
Iteration 25/25 | Loss: 0.00074156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074156
Iteration 2/1000 | Loss: 0.00002857
Iteration 3/1000 | Loss: 0.00002086
Iteration 4/1000 | Loss: 0.00001916
Iteration 5/1000 | Loss: 0.00001827
Iteration 6/1000 | Loss: 0.00001788
Iteration 7/1000 | Loss: 0.00001762
Iteration 8/1000 | Loss: 0.00001750
Iteration 9/1000 | Loss: 0.00001748
Iteration 10/1000 | Loss: 0.00001741
Iteration 11/1000 | Loss: 0.00001737
Iteration 12/1000 | Loss: 0.00001737
Iteration 13/1000 | Loss: 0.00001736
Iteration 14/1000 | Loss: 0.00001736
Iteration 15/1000 | Loss: 0.00001735
Iteration 16/1000 | Loss: 0.00001734
Iteration 17/1000 | Loss: 0.00001732
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001731
Iteration 20/1000 | Loss: 0.00001729
Iteration 21/1000 | Loss: 0.00001728
Iteration 22/1000 | Loss: 0.00001725
Iteration 23/1000 | Loss: 0.00001722
Iteration 24/1000 | Loss: 0.00001722
Iteration 25/1000 | Loss: 0.00001721
Iteration 26/1000 | Loss: 0.00001720
Iteration 27/1000 | Loss: 0.00001719
Iteration 28/1000 | Loss: 0.00001715
Iteration 29/1000 | Loss: 0.00001715
Iteration 30/1000 | Loss: 0.00001713
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001712
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001712
Iteration 36/1000 | Loss: 0.00001711
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001711
Iteration 39/1000 | Loss: 0.00001711
Iteration 40/1000 | Loss: 0.00001711
Iteration 41/1000 | Loss: 0.00001711
Iteration 42/1000 | Loss: 0.00001710
Iteration 43/1000 | Loss: 0.00001710
Iteration 44/1000 | Loss: 0.00001710
Iteration 45/1000 | Loss: 0.00001710
Iteration 46/1000 | Loss: 0.00001709
Iteration 47/1000 | Loss: 0.00001709
Iteration 48/1000 | Loss: 0.00001709
Iteration 49/1000 | Loss: 0.00001709
Iteration 50/1000 | Loss: 0.00001709
Iteration 51/1000 | Loss: 0.00001709
Iteration 52/1000 | Loss: 0.00001709
Iteration 53/1000 | Loss: 0.00001708
Iteration 54/1000 | Loss: 0.00001708
Iteration 55/1000 | Loss: 0.00001708
Iteration 56/1000 | Loss: 0.00001707
Iteration 57/1000 | Loss: 0.00001707
Iteration 58/1000 | Loss: 0.00001706
Iteration 59/1000 | Loss: 0.00001706
Iteration 60/1000 | Loss: 0.00001706
Iteration 61/1000 | Loss: 0.00001706
Iteration 62/1000 | Loss: 0.00001705
Iteration 63/1000 | Loss: 0.00001705
Iteration 64/1000 | Loss: 0.00001705
Iteration 65/1000 | Loss: 0.00001705
Iteration 66/1000 | Loss: 0.00001705
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001704
Iteration 69/1000 | Loss: 0.00001704
Iteration 70/1000 | Loss: 0.00001704
Iteration 71/1000 | Loss: 0.00001704
Iteration 72/1000 | Loss: 0.00001703
Iteration 73/1000 | Loss: 0.00001703
Iteration 74/1000 | Loss: 0.00001703
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001703
Iteration 78/1000 | Loss: 0.00001703
Iteration 79/1000 | Loss: 0.00001702
Iteration 80/1000 | Loss: 0.00001702
Iteration 81/1000 | Loss: 0.00001702
Iteration 82/1000 | Loss: 0.00001702
Iteration 83/1000 | Loss: 0.00001701
Iteration 84/1000 | Loss: 0.00001701
Iteration 85/1000 | Loss: 0.00001701
Iteration 86/1000 | Loss: 0.00001700
Iteration 87/1000 | Loss: 0.00001700
Iteration 88/1000 | Loss: 0.00001699
Iteration 89/1000 | Loss: 0.00001699
Iteration 90/1000 | Loss: 0.00001699
Iteration 91/1000 | Loss: 0.00001699
Iteration 92/1000 | Loss: 0.00001699
Iteration 93/1000 | Loss: 0.00001699
Iteration 94/1000 | Loss: 0.00001699
Iteration 95/1000 | Loss: 0.00001699
Iteration 96/1000 | Loss: 0.00001698
Iteration 97/1000 | Loss: 0.00001698
Iteration 98/1000 | Loss: 0.00001698
Iteration 99/1000 | Loss: 0.00001698
Iteration 100/1000 | Loss: 0.00001698
Iteration 101/1000 | Loss: 0.00001698
Iteration 102/1000 | Loss: 0.00001698
Iteration 103/1000 | Loss: 0.00001698
Iteration 104/1000 | Loss: 0.00001698
Iteration 105/1000 | Loss: 0.00001698
Iteration 106/1000 | Loss: 0.00001697
Iteration 107/1000 | Loss: 0.00001697
Iteration 108/1000 | Loss: 0.00001697
Iteration 109/1000 | Loss: 0.00001697
Iteration 110/1000 | Loss: 0.00001697
Iteration 111/1000 | Loss: 0.00001697
Iteration 112/1000 | Loss: 0.00001697
Iteration 113/1000 | Loss: 0.00001697
Iteration 114/1000 | Loss: 0.00001697
Iteration 115/1000 | Loss: 0.00001697
Iteration 116/1000 | Loss: 0.00001697
Iteration 117/1000 | Loss: 0.00001697
Iteration 118/1000 | Loss: 0.00001697
Iteration 119/1000 | Loss: 0.00001696
Iteration 120/1000 | Loss: 0.00001696
Iteration 121/1000 | Loss: 0.00001696
Iteration 122/1000 | Loss: 0.00001696
Iteration 123/1000 | Loss: 0.00001696
Iteration 124/1000 | Loss: 0.00001696
Iteration 125/1000 | Loss: 0.00001696
Iteration 126/1000 | Loss: 0.00001695
Iteration 127/1000 | Loss: 0.00001695
Iteration 128/1000 | Loss: 0.00001695
Iteration 129/1000 | Loss: 0.00001695
Iteration 130/1000 | Loss: 0.00001695
Iteration 131/1000 | Loss: 0.00001695
Iteration 132/1000 | Loss: 0.00001695
Iteration 133/1000 | Loss: 0.00001695
Iteration 134/1000 | Loss: 0.00001695
Iteration 135/1000 | Loss: 0.00001695
Iteration 136/1000 | Loss: 0.00001695
Iteration 137/1000 | Loss: 0.00001695
Iteration 138/1000 | Loss: 0.00001695
Iteration 139/1000 | Loss: 0.00001695
Iteration 140/1000 | Loss: 0.00001695
Iteration 141/1000 | Loss: 0.00001695
Iteration 142/1000 | Loss: 0.00001695
Iteration 143/1000 | Loss: 0.00001695
Iteration 144/1000 | Loss: 0.00001695
Iteration 145/1000 | Loss: 0.00001695
Iteration 146/1000 | Loss: 0.00001695
Iteration 147/1000 | Loss: 0.00001695
Iteration 148/1000 | Loss: 0.00001695
Iteration 149/1000 | Loss: 0.00001695
Iteration 150/1000 | Loss: 0.00001695
Iteration 151/1000 | Loss: 0.00001695
Iteration 152/1000 | Loss: 0.00001695
Iteration 153/1000 | Loss: 0.00001695
Iteration 154/1000 | Loss: 0.00001695
Iteration 155/1000 | Loss: 0.00001695
Iteration 156/1000 | Loss: 0.00001695
Iteration 157/1000 | Loss: 0.00001695
Iteration 158/1000 | Loss: 0.00001695
Iteration 159/1000 | Loss: 0.00001695
Iteration 160/1000 | Loss: 0.00001695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.6946005416684784e-05, 1.6946005416684784e-05, 1.6946005416684784e-05, 1.6946005416684784e-05, 1.6946005416684784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6946005416684784e-05

Optimization complete. Final v2v error: 3.6099178791046143 mm

Highest mean error: 3.9254562854766846 mm for frame 19

Lowest mean error: 3.465374708175659 mm for frame 55

Saving results

Total time: 32.64334464073181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531320
Iteration 2/25 | Loss: 0.00143146
Iteration 3/25 | Loss: 0.00114948
Iteration 4/25 | Loss: 0.00110234
Iteration 5/25 | Loss: 0.00108375
Iteration 6/25 | Loss: 0.00108010
Iteration 7/25 | Loss: 0.00107893
Iteration 8/25 | Loss: 0.00107893
Iteration 9/25 | Loss: 0.00107893
Iteration 10/25 | Loss: 0.00107893
Iteration 11/25 | Loss: 0.00107893
Iteration 12/25 | Loss: 0.00107893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010789254447445273, 0.0010789254447445273, 0.0010789254447445273, 0.0010789254447445273, 0.0010789254447445273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010789254447445273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78975087
Iteration 2/25 | Loss: 0.00093275
Iteration 3/25 | Loss: 0.00093275
Iteration 4/25 | Loss: 0.00093275
Iteration 5/25 | Loss: 0.00093274
Iteration 6/25 | Loss: 0.00093274
Iteration 7/25 | Loss: 0.00093274
Iteration 8/25 | Loss: 0.00093274
Iteration 9/25 | Loss: 0.00093274
Iteration 10/25 | Loss: 0.00093274
Iteration 11/25 | Loss: 0.00093274
Iteration 12/25 | Loss: 0.00093274
Iteration 13/25 | Loss: 0.00093274
Iteration 14/25 | Loss: 0.00093274
Iteration 15/25 | Loss: 0.00093274
Iteration 16/25 | Loss: 0.00093274
Iteration 17/25 | Loss: 0.00093274
Iteration 18/25 | Loss: 0.00093274
Iteration 19/25 | Loss: 0.00093274
Iteration 20/25 | Loss: 0.00093274
Iteration 21/25 | Loss: 0.00093274
Iteration 22/25 | Loss: 0.00093274
Iteration 23/25 | Loss: 0.00093274
Iteration 24/25 | Loss: 0.00093274
Iteration 25/25 | Loss: 0.00093274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093274
Iteration 2/1000 | Loss: 0.00006266
Iteration 3/1000 | Loss: 0.00003623
Iteration 4/1000 | Loss: 0.00003325
Iteration 5/1000 | Loss: 0.00003177
Iteration 6/1000 | Loss: 0.00003072
Iteration 7/1000 | Loss: 0.00003006
Iteration 8/1000 | Loss: 0.00002951
Iteration 9/1000 | Loss: 0.00002916
Iteration 10/1000 | Loss: 0.00002884
Iteration 11/1000 | Loss: 0.00002869
Iteration 12/1000 | Loss: 0.00002862
Iteration 13/1000 | Loss: 0.00002860
Iteration 14/1000 | Loss: 0.00002854
Iteration 15/1000 | Loss: 0.00002849
Iteration 16/1000 | Loss: 0.00002849
Iteration 17/1000 | Loss: 0.00002847
Iteration 18/1000 | Loss: 0.00002847
Iteration 19/1000 | Loss: 0.00002847
Iteration 20/1000 | Loss: 0.00002846
Iteration 21/1000 | Loss: 0.00002846
Iteration 22/1000 | Loss: 0.00002846
Iteration 23/1000 | Loss: 0.00002846
Iteration 24/1000 | Loss: 0.00002846
Iteration 25/1000 | Loss: 0.00002845
Iteration 26/1000 | Loss: 0.00002845
Iteration 27/1000 | Loss: 0.00002845
Iteration 28/1000 | Loss: 0.00002845
Iteration 29/1000 | Loss: 0.00002844
Iteration 30/1000 | Loss: 0.00002844
Iteration 31/1000 | Loss: 0.00002844
Iteration 32/1000 | Loss: 0.00002843
Iteration 33/1000 | Loss: 0.00002843
Iteration 34/1000 | Loss: 0.00002843
Iteration 35/1000 | Loss: 0.00002842
Iteration 36/1000 | Loss: 0.00002842
Iteration 37/1000 | Loss: 0.00002842
Iteration 38/1000 | Loss: 0.00002842
Iteration 39/1000 | Loss: 0.00002842
Iteration 40/1000 | Loss: 0.00002842
Iteration 41/1000 | Loss: 0.00002841
Iteration 42/1000 | Loss: 0.00002841
Iteration 43/1000 | Loss: 0.00002841
Iteration 44/1000 | Loss: 0.00002840
Iteration 45/1000 | Loss: 0.00002839
Iteration 46/1000 | Loss: 0.00002837
Iteration 47/1000 | Loss: 0.00002837
Iteration 48/1000 | Loss: 0.00002837
Iteration 49/1000 | Loss: 0.00002836
Iteration 50/1000 | Loss: 0.00002836
Iteration 51/1000 | Loss: 0.00002836
Iteration 52/1000 | Loss: 0.00002836
Iteration 53/1000 | Loss: 0.00002835
Iteration 54/1000 | Loss: 0.00002835
Iteration 55/1000 | Loss: 0.00002835
Iteration 56/1000 | Loss: 0.00002835
Iteration 57/1000 | Loss: 0.00002835
Iteration 58/1000 | Loss: 0.00002832
Iteration 59/1000 | Loss: 0.00002832
Iteration 60/1000 | Loss: 0.00002832
Iteration 61/1000 | Loss: 0.00002831
Iteration 62/1000 | Loss: 0.00002830
Iteration 63/1000 | Loss: 0.00002830
Iteration 64/1000 | Loss: 0.00002830
Iteration 65/1000 | Loss: 0.00002829
Iteration 66/1000 | Loss: 0.00002829
Iteration 67/1000 | Loss: 0.00002829
Iteration 68/1000 | Loss: 0.00002829
Iteration 69/1000 | Loss: 0.00002829
Iteration 70/1000 | Loss: 0.00002829
Iteration 71/1000 | Loss: 0.00002829
Iteration 72/1000 | Loss: 0.00002828
Iteration 73/1000 | Loss: 0.00002828
Iteration 74/1000 | Loss: 0.00002828
Iteration 75/1000 | Loss: 0.00002828
Iteration 76/1000 | Loss: 0.00002827
Iteration 77/1000 | Loss: 0.00002827
Iteration 78/1000 | Loss: 0.00002827
Iteration 79/1000 | Loss: 0.00002827
Iteration 80/1000 | Loss: 0.00002827
Iteration 81/1000 | Loss: 0.00002827
Iteration 82/1000 | Loss: 0.00002827
Iteration 83/1000 | Loss: 0.00002826
Iteration 84/1000 | Loss: 0.00002826
Iteration 85/1000 | Loss: 0.00002826
Iteration 86/1000 | Loss: 0.00002826
Iteration 87/1000 | Loss: 0.00002825
Iteration 88/1000 | Loss: 0.00002825
Iteration 89/1000 | Loss: 0.00002825
Iteration 90/1000 | Loss: 0.00002824
Iteration 91/1000 | Loss: 0.00002824
Iteration 92/1000 | Loss: 0.00002824
Iteration 93/1000 | Loss: 0.00002824
Iteration 94/1000 | Loss: 0.00002824
Iteration 95/1000 | Loss: 0.00002824
Iteration 96/1000 | Loss: 0.00002824
Iteration 97/1000 | Loss: 0.00002824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.8239037419552915e-05, 2.8239037419552915e-05, 2.8239037419552915e-05, 2.8239037419552915e-05, 2.8239037419552915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8239037419552915e-05

Optimization complete. Final v2v error: 4.530462265014648 mm

Highest mean error: 5.210827350616455 mm for frame 252

Lowest mean error: 4.199134826660156 mm for frame 75

Saving results

Total time: 40.630929470062256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537771
Iteration 2/25 | Loss: 0.00137133
Iteration 3/25 | Loss: 0.00106751
Iteration 4/25 | Loss: 0.00102740
Iteration 5/25 | Loss: 0.00101703
Iteration 6/25 | Loss: 0.00101353
Iteration 7/25 | Loss: 0.00101218
Iteration 8/25 | Loss: 0.00101189
Iteration 9/25 | Loss: 0.00101189
Iteration 10/25 | Loss: 0.00101189
Iteration 11/25 | Loss: 0.00101189
Iteration 12/25 | Loss: 0.00101189
Iteration 13/25 | Loss: 0.00101189
Iteration 14/25 | Loss: 0.00101189
Iteration 15/25 | Loss: 0.00101189
Iteration 16/25 | Loss: 0.00101189
Iteration 17/25 | Loss: 0.00101189
Iteration 18/25 | Loss: 0.00101189
Iteration 19/25 | Loss: 0.00101189
Iteration 20/25 | Loss: 0.00101189
Iteration 21/25 | Loss: 0.00101189
Iteration 22/25 | Loss: 0.00101189
Iteration 23/25 | Loss: 0.00101189
Iteration 24/25 | Loss: 0.00101189
Iteration 25/25 | Loss: 0.00101189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48679483
Iteration 2/25 | Loss: 0.00066431
Iteration 3/25 | Loss: 0.00066431
Iteration 4/25 | Loss: 0.00066431
Iteration 5/25 | Loss: 0.00066431
Iteration 6/25 | Loss: 0.00066431
Iteration 7/25 | Loss: 0.00066431
Iteration 8/25 | Loss: 0.00066431
Iteration 9/25 | Loss: 0.00066431
Iteration 10/25 | Loss: 0.00066431
Iteration 11/25 | Loss: 0.00066431
Iteration 12/25 | Loss: 0.00066431
Iteration 13/25 | Loss: 0.00066431
Iteration 14/25 | Loss: 0.00066431
Iteration 15/25 | Loss: 0.00066431
Iteration 16/25 | Loss: 0.00066431
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006643082015216351, 0.0006643082015216351, 0.0006643082015216351, 0.0006643082015216351, 0.0006643082015216351]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006643082015216351

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066431
Iteration 2/1000 | Loss: 0.00003843
Iteration 3/1000 | Loss: 0.00002689
Iteration 4/1000 | Loss: 0.00002494
Iteration 5/1000 | Loss: 0.00002403
Iteration 6/1000 | Loss: 0.00002323
Iteration 7/1000 | Loss: 0.00002277
Iteration 8/1000 | Loss: 0.00002243
Iteration 9/1000 | Loss: 0.00002219
Iteration 10/1000 | Loss: 0.00002202
Iteration 11/1000 | Loss: 0.00002188
Iteration 12/1000 | Loss: 0.00002187
Iteration 13/1000 | Loss: 0.00002177
Iteration 14/1000 | Loss: 0.00002175
Iteration 15/1000 | Loss: 0.00002173
Iteration 16/1000 | Loss: 0.00002165
Iteration 17/1000 | Loss: 0.00002163
Iteration 18/1000 | Loss: 0.00002163
Iteration 19/1000 | Loss: 0.00002161
Iteration 20/1000 | Loss: 0.00002161
Iteration 21/1000 | Loss: 0.00002161
Iteration 22/1000 | Loss: 0.00002161
Iteration 23/1000 | Loss: 0.00002161
Iteration 24/1000 | Loss: 0.00002161
Iteration 25/1000 | Loss: 0.00002161
Iteration 26/1000 | Loss: 0.00002161
Iteration 27/1000 | Loss: 0.00002160
Iteration 28/1000 | Loss: 0.00002160
Iteration 29/1000 | Loss: 0.00002160
Iteration 30/1000 | Loss: 0.00002159
Iteration 31/1000 | Loss: 0.00002159
Iteration 32/1000 | Loss: 0.00002159
Iteration 33/1000 | Loss: 0.00002158
Iteration 34/1000 | Loss: 0.00002158
Iteration 35/1000 | Loss: 0.00002158
Iteration 36/1000 | Loss: 0.00002157
Iteration 37/1000 | Loss: 0.00002157
Iteration 38/1000 | Loss: 0.00002157
Iteration 39/1000 | Loss: 0.00002157
Iteration 40/1000 | Loss: 0.00002156
Iteration 41/1000 | Loss: 0.00002156
Iteration 42/1000 | Loss: 0.00002156
Iteration 43/1000 | Loss: 0.00002156
Iteration 44/1000 | Loss: 0.00002155
Iteration 45/1000 | Loss: 0.00002155
Iteration 46/1000 | Loss: 0.00002155
Iteration 47/1000 | Loss: 0.00002154
Iteration 48/1000 | Loss: 0.00002154
Iteration 49/1000 | Loss: 0.00002154
Iteration 50/1000 | Loss: 0.00002154
Iteration 51/1000 | Loss: 0.00002153
Iteration 52/1000 | Loss: 0.00002153
Iteration 53/1000 | Loss: 0.00002153
Iteration 54/1000 | Loss: 0.00002153
Iteration 55/1000 | Loss: 0.00002152
Iteration 56/1000 | Loss: 0.00002152
Iteration 57/1000 | Loss: 0.00002152
Iteration 58/1000 | Loss: 0.00002152
Iteration 59/1000 | Loss: 0.00002152
Iteration 60/1000 | Loss: 0.00002152
Iteration 61/1000 | Loss: 0.00002152
Iteration 62/1000 | Loss: 0.00002151
Iteration 63/1000 | Loss: 0.00002151
Iteration 64/1000 | Loss: 0.00002151
Iteration 65/1000 | Loss: 0.00002151
Iteration 66/1000 | Loss: 0.00002151
Iteration 67/1000 | Loss: 0.00002151
Iteration 68/1000 | Loss: 0.00002150
Iteration 69/1000 | Loss: 0.00002150
Iteration 70/1000 | Loss: 0.00002150
Iteration 71/1000 | Loss: 0.00002150
Iteration 72/1000 | Loss: 0.00002150
Iteration 73/1000 | Loss: 0.00002150
Iteration 74/1000 | Loss: 0.00002150
Iteration 75/1000 | Loss: 0.00002149
Iteration 76/1000 | Loss: 0.00002149
Iteration 77/1000 | Loss: 0.00002149
Iteration 78/1000 | Loss: 0.00002149
Iteration 79/1000 | Loss: 0.00002148
Iteration 80/1000 | Loss: 0.00002148
Iteration 81/1000 | Loss: 0.00002148
Iteration 82/1000 | Loss: 0.00002148
Iteration 83/1000 | Loss: 0.00002147
Iteration 84/1000 | Loss: 0.00002147
Iteration 85/1000 | Loss: 0.00002147
Iteration 86/1000 | Loss: 0.00002147
Iteration 87/1000 | Loss: 0.00002147
Iteration 88/1000 | Loss: 0.00002147
Iteration 89/1000 | Loss: 0.00002147
Iteration 90/1000 | Loss: 0.00002147
Iteration 91/1000 | Loss: 0.00002146
Iteration 92/1000 | Loss: 0.00002146
Iteration 93/1000 | Loss: 0.00002146
Iteration 94/1000 | Loss: 0.00002146
Iteration 95/1000 | Loss: 0.00002146
Iteration 96/1000 | Loss: 0.00002146
Iteration 97/1000 | Loss: 0.00002146
Iteration 98/1000 | Loss: 0.00002146
Iteration 99/1000 | Loss: 0.00002146
Iteration 100/1000 | Loss: 0.00002146
Iteration 101/1000 | Loss: 0.00002146
Iteration 102/1000 | Loss: 0.00002145
Iteration 103/1000 | Loss: 0.00002145
Iteration 104/1000 | Loss: 0.00002145
Iteration 105/1000 | Loss: 0.00002145
Iteration 106/1000 | Loss: 0.00002145
Iteration 107/1000 | Loss: 0.00002145
Iteration 108/1000 | Loss: 0.00002145
Iteration 109/1000 | Loss: 0.00002145
Iteration 110/1000 | Loss: 0.00002145
Iteration 111/1000 | Loss: 0.00002145
Iteration 112/1000 | Loss: 0.00002145
Iteration 113/1000 | Loss: 0.00002145
Iteration 114/1000 | Loss: 0.00002144
Iteration 115/1000 | Loss: 0.00002144
Iteration 116/1000 | Loss: 0.00002144
Iteration 117/1000 | Loss: 0.00002144
Iteration 118/1000 | Loss: 0.00002144
Iteration 119/1000 | Loss: 0.00002144
Iteration 120/1000 | Loss: 0.00002144
Iteration 121/1000 | Loss: 0.00002144
Iteration 122/1000 | Loss: 0.00002144
Iteration 123/1000 | Loss: 0.00002144
Iteration 124/1000 | Loss: 0.00002144
Iteration 125/1000 | Loss: 0.00002143
Iteration 126/1000 | Loss: 0.00002143
Iteration 127/1000 | Loss: 0.00002143
Iteration 128/1000 | Loss: 0.00002143
Iteration 129/1000 | Loss: 0.00002143
Iteration 130/1000 | Loss: 0.00002143
Iteration 131/1000 | Loss: 0.00002142
Iteration 132/1000 | Loss: 0.00002142
Iteration 133/1000 | Loss: 0.00002142
Iteration 134/1000 | Loss: 0.00002142
Iteration 135/1000 | Loss: 0.00002142
Iteration 136/1000 | Loss: 0.00002142
Iteration 137/1000 | Loss: 0.00002142
Iteration 138/1000 | Loss: 0.00002142
Iteration 139/1000 | Loss: 0.00002142
Iteration 140/1000 | Loss: 0.00002142
Iteration 141/1000 | Loss: 0.00002141
Iteration 142/1000 | Loss: 0.00002141
Iteration 143/1000 | Loss: 0.00002141
Iteration 144/1000 | Loss: 0.00002141
Iteration 145/1000 | Loss: 0.00002141
Iteration 146/1000 | Loss: 0.00002141
Iteration 147/1000 | Loss: 0.00002141
Iteration 148/1000 | Loss: 0.00002141
Iteration 149/1000 | Loss: 0.00002140
Iteration 150/1000 | Loss: 0.00002140
Iteration 151/1000 | Loss: 0.00002140
Iteration 152/1000 | Loss: 0.00002140
Iteration 153/1000 | Loss: 0.00002140
Iteration 154/1000 | Loss: 0.00002140
Iteration 155/1000 | Loss: 0.00002139
Iteration 156/1000 | Loss: 0.00002139
Iteration 157/1000 | Loss: 0.00002139
Iteration 158/1000 | Loss: 0.00002139
Iteration 159/1000 | Loss: 0.00002139
Iteration 160/1000 | Loss: 0.00002139
Iteration 161/1000 | Loss: 0.00002139
Iteration 162/1000 | Loss: 0.00002138
Iteration 163/1000 | Loss: 0.00002138
Iteration 164/1000 | Loss: 0.00002138
Iteration 165/1000 | Loss: 0.00002138
Iteration 166/1000 | Loss: 0.00002138
Iteration 167/1000 | Loss: 0.00002138
Iteration 168/1000 | Loss: 0.00002138
Iteration 169/1000 | Loss: 0.00002138
Iteration 170/1000 | Loss: 0.00002137
Iteration 171/1000 | Loss: 0.00002137
Iteration 172/1000 | Loss: 0.00002137
Iteration 173/1000 | Loss: 0.00002137
Iteration 174/1000 | Loss: 0.00002137
Iteration 175/1000 | Loss: 0.00002137
Iteration 176/1000 | Loss: 0.00002137
Iteration 177/1000 | Loss: 0.00002137
Iteration 178/1000 | Loss: 0.00002137
Iteration 179/1000 | Loss: 0.00002137
Iteration 180/1000 | Loss: 0.00002137
Iteration 181/1000 | Loss: 0.00002137
Iteration 182/1000 | Loss: 0.00002137
Iteration 183/1000 | Loss: 0.00002137
Iteration 184/1000 | Loss: 0.00002137
Iteration 185/1000 | Loss: 0.00002136
Iteration 186/1000 | Loss: 0.00002136
Iteration 187/1000 | Loss: 0.00002136
Iteration 188/1000 | Loss: 0.00002136
Iteration 189/1000 | Loss: 0.00002136
Iteration 190/1000 | Loss: 0.00002136
Iteration 191/1000 | Loss: 0.00002136
Iteration 192/1000 | Loss: 0.00002136
Iteration 193/1000 | Loss: 0.00002136
Iteration 194/1000 | Loss: 0.00002136
Iteration 195/1000 | Loss: 0.00002136
Iteration 196/1000 | Loss: 0.00002136
Iteration 197/1000 | Loss: 0.00002136
Iteration 198/1000 | Loss: 0.00002136
Iteration 199/1000 | Loss: 0.00002136
Iteration 200/1000 | Loss: 0.00002136
Iteration 201/1000 | Loss: 0.00002136
Iteration 202/1000 | Loss: 0.00002136
Iteration 203/1000 | Loss: 0.00002135
Iteration 204/1000 | Loss: 0.00002135
Iteration 205/1000 | Loss: 0.00002135
Iteration 206/1000 | Loss: 0.00002135
Iteration 207/1000 | Loss: 0.00002135
Iteration 208/1000 | Loss: 0.00002135
Iteration 209/1000 | Loss: 0.00002135
Iteration 210/1000 | Loss: 0.00002135
Iteration 211/1000 | Loss: 0.00002135
Iteration 212/1000 | Loss: 0.00002135
Iteration 213/1000 | Loss: 0.00002135
Iteration 214/1000 | Loss: 0.00002135
Iteration 215/1000 | Loss: 0.00002135
Iteration 216/1000 | Loss: 0.00002135
Iteration 217/1000 | Loss: 0.00002135
Iteration 218/1000 | Loss: 0.00002135
Iteration 219/1000 | Loss: 0.00002135
Iteration 220/1000 | Loss: 0.00002135
Iteration 221/1000 | Loss: 0.00002135
Iteration 222/1000 | Loss: 0.00002135
Iteration 223/1000 | Loss: 0.00002135
Iteration 224/1000 | Loss: 0.00002135
Iteration 225/1000 | Loss: 0.00002135
Iteration 226/1000 | Loss: 0.00002135
Iteration 227/1000 | Loss: 0.00002135
Iteration 228/1000 | Loss: 0.00002135
Iteration 229/1000 | Loss: 0.00002135
Iteration 230/1000 | Loss: 0.00002135
Iteration 231/1000 | Loss: 0.00002135
Iteration 232/1000 | Loss: 0.00002135
Iteration 233/1000 | Loss: 0.00002135
Iteration 234/1000 | Loss: 0.00002135
Iteration 235/1000 | Loss: 0.00002135
Iteration 236/1000 | Loss: 0.00002135
Iteration 237/1000 | Loss: 0.00002135
Iteration 238/1000 | Loss: 0.00002135
Iteration 239/1000 | Loss: 0.00002135
Iteration 240/1000 | Loss: 0.00002135
Iteration 241/1000 | Loss: 0.00002135
Iteration 242/1000 | Loss: 0.00002135
Iteration 243/1000 | Loss: 0.00002135
Iteration 244/1000 | Loss: 0.00002135
Iteration 245/1000 | Loss: 0.00002135
Iteration 246/1000 | Loss: 0.00002135
Iteration 247/1000 | Loss: 0.00002135
Iteration 248/1000 | Loss: 0.00002135
Iteration 249/1000 | Loss: 0.00002135
Iteration 250/1000 | Loss: 0.00002135
Iteration 251/1000 | Loss: 0.00002135
Iteration 252/1000 | Loss: 0.00002135
Iteration 253/1000 | Loss: 0.00002135
Iteration 254/1000 | Loss: 0.00002135
Iteration 255/1000 | Loss: 0.00002135
Iteration 256/1000 | Loss: 0.00002135
Iteration 257/1000 | Loss: 0.00002135
Iteration 258/1000 | Loss: 0.00002135
Iteration 259/1000 | Loss: 0.00002135
Iteration 260/1000 | Loss: 0.00002135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [2.1354233467718586e-05, 2.1354233467718586e-05, 2.1354233467718586e-05, 2.1354233467718586e-05, 2.1354233467718586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1354233467718586e-05

Optimization complete. Final v2v error: 3.7911102771759033 mm

Highest mean error: 5.431344032287598 mm for frame 64

Lowest mean error: 3.2479677200317383 mm for frame 26

Saving results

Total time: 42.17520570755005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099078
Iteration 2/25 | Loss: 0.01099077
Iteration 3/25 | Loss: 0.00431070
Iteration 4/25 | Loss: 0.00214862
Iteration 5/25 | Loss: 0.00191843
Iteration 6/25 | Loss: 0.00180272
Iteration 7/25 | Loss: 0.00176857
Iteration 8/25 | Loss: 0.00171815
Iteration 9/25 | Loss: 0.00163555
Iteration 10/25 | Loss: 0.00154820
Iteration 11/25 | Loss: 0.00152537
Iteration 12/25 | Loss: 0.00147854
Iteration 13/25 | Loss: 0.00143809
Iteration 14/25 | Loss: 0.00140031
Iteration 15/25 | Loss: 0.00139718
Iteration 16/25 | Loss: 0.00137642
Iteration 17/25 | Loss: 0.00135599
Iteration 18/25 | Loss: 0.00135060
Iteration 19/25 | Loss: 0.00133638
Iteration 20/25 | Loss: 0.00133197
Iteration 21/25 | Loss: 0.00133242
Iteration 22/25 | Loss: 0.00133269
Iteration 23/25 | Loss: 0.00133272
Iteration 24/25 | Loss: 0.00132611
Iteration 25/25 | Loss: 0.00132635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51956105
Iteration 2/25 | Loss: 0.00401499
Iteration 3/25 | Loss: 0.00388860
Iteration 4/25 | Loss: 0.00385837
Iteration 5/25 | Loss: 0.00385837
Iteration 6/25 | Loss: 0.00385837
Iteration 7/25 | Loss: 0.00385837
Iteration 8/25 | Loss: 0.00385837
Iteration 9/25 | Loss: 0.00385837
Iteration 10/25 | Loss: 0.00385837
Iteration 11/25 | Loss: 0.00385837
Iteration 12/25 | Loss: 0.00385837
Iteration 13/25 | Loss: 0.00385837
Iteration 14/25 | Loss: 0.00385837
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003858366748318076, 0.003858366748318076, 0.003858366748318076, 0.003858366748318076, 0.003858366748318076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003858366748318076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00385837
Iteration 2/1000 | Loss: 0.00126488
Iteration 3/1000 | Loss: 0.00083248
Iteration 4/1000 | Loss: 0.00068054
Iteration 5/1000 | Loss: 0.00140603
Iteration 6/1000 | Loss: 0.00171898
Iteration 7/1000 | Loss: 0.00124138
Iteration 8/1000 | Loss: 0.00217656
Iteration 9/1000 | Loss: 0.00262602
Iteration 10/1000 | Loss: 0.00710821
Iteration 11/1000 | Loss: 0.00166460
Iteration 12/1000 | Loss: 0.00120546
Iteration 13/1000 | Loss: 0.00209012
Iteration 14/1000 | Loss: 0.00290618
Iteration 15/1000 | Loss: 0.00135462
Iteration 16/1000 | Loss: 0.00332062
Iteration 17/1000 | Loss: 0.00295421
Iteration 18/1000 | Loss: 0.00211999
Iteration 19/1000 | Loss: 0.00288173
Iteration 20/1000 | Loss: 0.00439037
Iteration 21/1000 | Loss: 0.00331379
Iteration 22/1000 | Loss: 0.00402229
Iteration 23/1000 | Loss: 0.00214332
Iteration 24/1000 | Loss: 0.00156564
Iteration 25/1000 | Loss: 0.00113920
Iteration 26/1000 | Loss: 0.00112811
Iteration 27/1000 | Loss: 0.00126664
Iteration 28/1000 | Loss: 0.00125918
Iteration 29/1000 | Loss: 0.00147763
Iteration 30/1000 | Loss: 0.00136520
Iteration 31/1000 | Loss: 0.00139798
Iteration 32/1000 | Loss: 0.00350332
Iteration 33/1000 | Loss: 0.00195088
Iteration 34/1000 | Loss: 0.00383698
Iteration 35/1000 | Loss: 0.00094650
Iteration 36/1000 | Loss: 0.00090977
Iteration 37/1000 | Loss: 0.00115998
Iteration 38/1000 | Loss: 0.00106434
Iteration 39/1000 | Loss: 0.00081103
Iteration 40/1000 | Loss: 0.00037669
Iteration 41/1000 | Loss: 0.00046816
Iteration 42/1000 | Loss: 0.00065604
Iteration 43/1000 | Loss: 0.00051382
Iteration 44/1000 | Loss: 0.00196693
Iteration 45/1000 | Loss: 0.00095501
Iteration 46/1000 | Loss: 0.00096380
Iteration 47/1000 | Loss: 0.00065981
Iteration 48/1000 | Loss: 0.00127788
Iteration 49/1000 | Loss: 0.00092579
Iteration 50/1000 | Loss: 0.00095467
Iteration 51/1000 | Loss: 0.00107274
Iteration 52/1000 | Loss: 0.00136874
Iteration 53/1000 | Loss: 0.00086449
Iteration 54/1000 | Loss: 0.00116273
Iteration 55/1000 | Loss: 0.00196098
Iteration 56/1000 | Loss: 0.00228905
Iteration 57/1000 | Loss: 0.00247723
Iteration 58/1000 | Loss: 0.00180727
Iteration 59/1000 | Loss: 0.00179057
Iteration 60/1000 | Loss: 0.00086994
Iteration 61/1000 | Loss: 0.00056111
Iteration 62/1000 | Loss: 0.00142038
Iteration 63/1000 | Loss: 0.00088283
Iteration 64/1000 | Loss: 0.00082955
Iteration 65/1000 | Loss: 0.00101288
Iteration 66/1000 | Loss: 0.00103493
Iteration 67/1000 | Loss: 0.00106282
Iteration 68/1000 | Loss: 0.00086995
Iteration 69/1000 | Loss: 0.00094978
Iteration 70/1000 | Loss: 0.00083778
Iteration 71/1000 | Loss: 0.00155600
Iteration 72/1000 | Loss: 0.00125061
Iteration 73/1000 | Loss: 0.00160881
Iteration 74/1000 | Loss: 0.00074930
Iteration 75/1000 | Loss: 0.00141994
Iteration 76/1000 | Loss: 0.00076164
Iteration 77/1000 | Loss: 0.00044213
Iteration 78/1000 | Loss: 0.00042142
Iteration 79/1000 | Loss: 0.00044858
Iteration 80/1000 | Loss: 0.00078854
Iteration 81/1000 | Loss: 0.00056839
Iteration 82/1000 | Loss: 0.00060658
Iteration 83/1000 | Loss: 0.00072506
Iteration 84/1000 | Loss: 0.00066384
Iteration 85/1000 | Loss: 0.00089775
Iteration 86/1000 | Loss: 0.00102713
Iteration 87/1000 | Loss: 0.00107067
Iteration 88/1000 | Loss: 0.00092063
Iteration 89/1000 | Loss: 0.00108272
Iteration 90/1000 | Loss: 0.00051952
Iteration 91/1000 | Loss: 0.00079525
Iteration 92/1000 | Loss: 0.00111574
Iteration 93/1000 | Loss: 0.00313586
Iteration 94/1000 | Loss: 0.00113284
Iteration 95/1000 | Loss: 0.00106164
Iteration 96/1000 | Loss: 0.00024817
Iteration 97/1000 | Loss: 0.00048979
Iteration 98/1000 | Loss: 0.00052964
Iteration 99/1000 | Loss: 0.00083753
Iteration 100/1000 | Loss: 0.00042935
Iteration 101/1000 | Loss: 0.00022726
Iteration 102/1000 | Loss: 0.00069191
Iteration 103/1000 | Loss: 0.00021210
Iteration 104/1000 | Loss: 0.00034954
Iteration 105/1000 | Loss: 0.00042824
Iteration 106/1000 | Loss: 0.00025246
Iteration 107/1000 | Loss: 0.00123944
Iteration 108/1000 | Loss: 0.00052708
Iteration 109/1000 | Loss: 0.00106165
Iteration 110/1000 | Loss: 0.00019052
Iteration 111/1000 | Loss: 0.00022313
Iteration 112/1000 | Loss: 0.00016488
Iteration 113/1000 | Loss: 0.00076914
Iteration 114/1000 | Loss: 0.00043397
Iteration 115/1000 | Loss: 0.00033960
Iteration 116/1000 | Loss: 0.00037190
Iteration 117/1000 | Loss: 0.00070723
Iteration 118/1000 | Loss: 0.00078930
Iteration 119/1000 | Loss: 0.00079668
Iteration 120/1000 | Loss: 0.00079220
Iteration 121/1000 | Loss: 0.00070948
Iteration 122/1000 | Loss: 0.00050943
Iteration 123/1000 | Loss: 0.00053282
Iteration 124/1000 | Loss: 0.00059541
Iteration 125/1000 | Loss: 0.00062524
Iteration 126/1000 | Loss: 0.00058901
Iteration 127/1000 | Loss: 0.00034925
Iteration 128/1000 | Loss: 0.00023750
Iteration 129/1000 | Loss: 0.00047498
Iteration 130/1000 | Loss: 0.00031959
Iteration 131/1000 | Loss: 0.00028658
Iteration 132/1000 | Loss: 0.00072474
Iteration 133/1000 | Loss: 0.00017624
Iteration 134/1000 | Loss: 0.00010887
Iteration 135/1000 | Loss: 0.00011785
Iteration 136/1000 | Loss: 0.00008698
Iteration 137/1000 | Loss: 0.00007729
Iteration 138/1000 | Loss: 0.00009219
Iteration 139/1000 | Loss: 0.00010085
Iteration 140/1000 | Loss: 0.00020911
Iteration 141/1000 | Loss: 0.00009970
Iteration 142/1000 | Loss: 0.00030540
Iteration 143/1000 | Loss: 0.00038175
Iteration 144/1000 | Loss: 0.00008436
Iteration 145/1000 | Loss: 0.00025801
Iteration 146/1000 | Loss: 0.00048708
Iteration 147/1000 | Loss: 0.00035346
Iteration 148/1000 | Loss: 0.00034307
Iteration 149/1000 | Loss: 0.00012781
Iteration 150/1000 | Loss: 0.00009417
Iteration 151/1000 | Loss: 0.00008218
Iteration 152/1000 | Loss: 0.00031144
Iteration 153/1000 | Loss: 0.00048296
Iteration 154/1000 | Loss: 0.00048827
Iteration 155/1000 | Loss: 0.00030368
Iteration 156/1000 | Loss: 0.00032478
Iteration 157/1000 | Loss: 0.00010667
Iteration 158/1000 | Loss: 0.00006054
Iteration 159/1000 | Loss: 0.00005806
Iteration 160/1000 | Loss: 0.00006288
Iteration 161/1000 | Loss: 0.00006027
Iteration 162/1000 | Loss: 0.00006922
Iteration 163/1000 | Loss: 0.00005094
Iteration 164/1000 | Loss: 0.00004512
Iteration 165/1000 | Loss: 0.00005807
Iteration 166/1000 | Loss: 0.00004124
Iteration 167/1000 | Loss: 0.00010003
Iteration 168/1000 | Loss: 0.00004913
Iteration 169/1000 | Loss: 0.00007088
Iteration 170/1000 | Loss: 0.00004763
Iteration 171/1000 | Loss: 0.00004520
Iteration 172/1000 | Loss: 0.00056508
Iteration 173/1000 | Loss: 0.00033142
Iteration 174/1000 | Loss: 0.00004879
Iteration 175/1000 | Loss: 0.00124233
Iteration 176/1000 | Loss: 0.00041369
Iteration 177/1000 | Loss: 0.00029910
Iteration 178/1000 | Loss: 0.00026295
Iteration 179/1000 | Loss: 0.00030768
Iteration 180/1000 | Loss: 0.00013046
Iteration 181/1000 | Loss: 0.00007088
Iteration 182/1000 | Loss: 0.00038080
Iteration 183/1000 | Loss: 0.00062021
Iteration 184/1000 | Loss: 0.00058766
Iteration 185/1000 | Loss: 0.00051069
Iteration 186/1000 | Loss: 0.00051680
Iteration 187/1000 | Loss: 0.00056570
Iteration 188/1000 | Loss: 0.00061671
Iteration 189/1000 | Loss: 0.00041598
Iteration 190/1000 | Loss: 0.00013829
Iteration 191/1000 | Loss: 0.00010800
Iteration 192/1000 | Loss: 0.00029616
Iteration 193/1000 | Loss: 0.00043324
Iteration 194/1000 | Loss: 0.00068495
Iteration 195/1000 | Loss: 0.00028264
Iteration 196/1000 | Loss: 0.00034946
Iteration 197/1000 | Loss: 0.00009423
Iteration 198/1000 | Loss: 0.00028670
Iteration 199/1000 | Loss: 0.00009822
Iteration 200/1000 | Loss: 0.00003533
Iteration 201/1000 | Loss: 0.00061507
Iteration 202/1000 | Loss: 0.00004527
Iteration 203/1000 | Loss: 0.00003537
Iteration 204/1000 | Loss: 0.00003163
Iteration 205/1000 | Loss: 0.00003207
Iteration 206/1000 | Loss: 0.00002775
Iteration 207/1000 | Loss: 0.00003300
Iteration 208/1000 | Loss: 0.00002616
Iteration 209/1000 | Loss: 0.00036727
Iteration 210/1000 | Loss: 0.00003822
Iteration 211/1000 | Loss: 0.00003464
Iteration 212/1000 | Loss: 0.00003549
Iteration 213/1000 | Loss: 0.00003585
Iteration 214/1000 | Loss: 0.00002658
Iteration 215/1000 | Loss: 0.00002944
Iteration 216/1000 | Loss: 0.00002394
Iteration 217/1000 | Loss: 0.00002329
Iteration 218/1000 | Loss: 0.00002300
Iteration 219/1000 | Loss: 0.00002298
Iteration 220/1000 | Loss: 0.00002295
Iteration 221/1000 | Loss: 0.00002294
Iteration 222/1000 | Loss: 0.00002294
Iteration 223/1000 | Loss: 0.00002293
Iteration 224/1000 | Loss: 0.00002293
Iteration 225/1000 | Loss: 0.00002289
Iteration 226/1000 | Loss: 0.00002283
Iteration 227/1000 | Loss: 0.00002283
Iteration 228/1000 | Loss: 0.00002283
Iteration 229/1000 | Loss: 0.00002282
Iteration 230/1000 | Loss: 0.00002282
Iteration 231/1000 | Loss: 0.00002281
Iteration 232/1000 | Loss: 0.00002281
Iteration 233/1000 | Loss: 0.00002280
Iteration 234/1000 | Loss: 0.00002280
Iteration 235/1000 | Loss: 0.00002279
Iteration 236/1000 | Loss: 0.00002276
Iteration 237/1000 | Loss: 0.00002276
Iteration 238/1000 | Loss: 0.00002273
Iteration 239/1000 | Loss: 0.00002273
Iteration 240/1000 | Loss: 0.00002273
Iteration 241/1000 | Loss: 0.00002272
Iteration 242/1000 | Loss: 0.00002266
Iteration 243/1000 | Loss: 0.00002264
Iteration 244/1000 | Loss: 0.00002264
Iteration 245/1000 | Loss: 0.00002264
Iteration 246/1000 | Loss: 0.00002263
Iteration 247/1000 | Loss: 0.00002263
Iteration 248/1000 | Loss: 0.00002263
Iteration 249/1000 | Loss: 0.00002263
Iteration 250/1000 | Loss: 0.00002263
Iteration 251/1000 | Loss: 0.00002263
Iteration 252/1000 | Loss: 0.00002263
Iteration 253/1000 | Loss: 0.00002262
Iteration 254/1000 | Loss: 0.00002262
Iteration 255/1000 | Loss: 0.00002262
Iteration 256/1000 | Loss: 0.00002262
Iteration 257/1000 | Loss: 0.00002262
Iteration 258/1000 | Loss: 0.00002262
Iteration 259/1000 | Loss: 0.00003461
Iteration 260/1000 | Loss: 0.00002264
Iteration 261/1000 | Loss: 0.00002260
Iteration 262/1000 | Loss: 0.00002260
Iteration 263/1000 | Loss: 0.00002260
Iteration 264/1000 | Loss: 0.00002259
Iteration 265/1000 | Loss: 0.00002259
Iteration 266/1000 | Loss: 0.00002259
Iteration 267/1000 | Loss: 0.00002259
Iteration 268/1000 | Loss: 0.00002258
Iteration 269/1000 | Loss: 0.00002258
Iteration 270/1000 | Loss: 0.00002258
Iteration 271/1000 | Loss: 0.00002258
Iteration 272/1000 | Loss: 0.00002258
Iteration 273/1000 | Loss: 0.00002258
Iteration 274/1000 | Loss: 0.00002258
Iteration 275/1000 | Loss: 0.00002258
Iteration 276/1000 | Loss: 0.00002258
Iteration 277/1000 | Loss: 0.00002258
Iteration 278/1000 | Loss: 0.00002258
Iteration 279/1000 | Loss: 0.00002258
Iteration 280/1000 | Loss: 0.00002258
Iteration 281/1000 | Loss: 0.00002258
Iteration 282/1000 | Loss: 0.00002258
Iteration 283/1000 | Loss: 0.00002258
Iteration 284/1000 | Loss: 0.00002258
Iteration 285/1000 | Loss: 0.00002258
Iteration 286/1000 | Loss: 0.00002258
Iteration 287/1000 | Loss: 0.00002258
Iteration 288/1000 | Loss: 0.00002258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [2.2579837605007924e-05, 2.2579837605007924e-05, 2.2579837605007924e-05, 2.2579837605007924e-05, 2.2579837605007924e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2579837605007924e-05

Optimization complete. Final v2v error: 3.944441318511963 mm

Highest mean error: 14.013867378234863 mm for frame 143

Lowest mean error: 3.172294855117798 mm for frame 176

Saving results

Total time: 403.7773108482361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01113406
Iteration 2/25 | Loss: 0.00185673
Iteration 3/25 | Loss: 0.00172508
Iteration 4/25 | Loss: 0.00136570
Iteration 5/25 | Loss: 0.00136660
Iteration 6/25 | Loss: 0.00144755
Iteration 7/25 | Loss: 0.00134705
Iteration 8/25 | Loss: 0.00130878
Iteration 9/25 | Loss: 0.00125377
Iteration 10/25 | Loss: 0.00126875
Iteration 11/25 | Loss: 0.00132857
Iteration 12/25 | Loss: 0.00128375
Iteration 13/25 | Loss: 0.00130426
Iteration 14/25 | Loss: 0.00118904
Iteration 15/25 | Loss: 0.00110732
Iteration 16/25 | Loss: 0.00110297
Iteration 17/25 | Loss: 0.00109871
Iteration 18/25 | Loss: 0.00110738
Iteration 19/25 | Loss: 0.00114203
Iteration 20/25 | Loss: 0.00109510
Iteration 21/25 | Loss: 0.00112239
Iteration 22/25 | Loss: 0.00111826
Iteration 23/25 | Loss: 0.00112889
Iteration 24/25 | Loss: 0.00107596
Iteration 25/25 | Loss: 0.00105807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47714627
Iteration 2/25 | Loss: 0.00143930
Iteration 3/25 | Loss: 0.00143055
Iteration 4/25 | Loss: 0.00143055
Iteration 5/25 | Loss: 0.00143055
Iteration 6/25 | Loss: 0.00143055
Iteration 7/25 | Loss: 0.00143054
Iteration 8/25 | Loss: 0.00143054
Iteration 9/25 | Loss: 0.00143054
Iteration 10/25 | Loss: 0.00143054
Iteration 11/25 | Loss: 0.00143054
Iteration 12/25 | Loss: 0.00143054
Iteration 13/25 | Loss: 0.00143054
Iteration 14/25 | Loss: 0.00143054
Iteration 15/25 | Loss: 0.00143054
Iteration 16/25 | Loss: 0.00143054
Iteration 17/25 | Loss: 0.00143054
Iteration 18/25 | Loss: 0.00143054
Iteration 19/25 | Loss: 0.00143054
Iteration 20/25 | Loss: 0.00143054
Iteration 21/25 | Loss: 0.00143054
Iteration 22/25 | Loss: 0.00143054
Iteration 23/25 | Loss: 0.00143054
Iteration 24/25 | Loss: 0.00143054
Iteration 25/25 | Loss: 0.00143054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143054
Iteration 2/1000 | Loss: 0.00338630
Iteration 3/1000 | Loss: 0.00107708
Iteration 4/1000 | Loss: 0.00131919
Iteration 5/1000 | Loss: 0.00104684
Iteration 6/1000 | Loss: 0.00012803
Iteration 7/1000 | Loss: 0.00311941
Iteration 8/1000 | Loss: 0.00142972
Iteration 9/1000 | Loss: 0.00101836
Iteration 10/1000 | Loss: 0.00276477
Iteration 11/1000 | Loss: 0.00129047
Iteration 12/1000 | Loss: 0.00140327
Iteration 13/1000 | Loss: 0.00164524
Iteration 14/1000 | Loss: 0.00101793
Iteration 15/1000 | Loss: 0.00283613
Iteration 16/1000 | Loss: 0.00132826
Iteration 17/1000 | Loss: 0.00204893
Iteration 18/1000 | Loss: 0.00085225
Iteration 19/1000 | Loss: 0.00163293
Iteration 20/1000 | Loss: 0.00206909
Iteration 21/1000 | Loss: 0.00113526
Iteration 22/1000 | Loss: 0.00026352
Iteration 23/1000 | Loss: 0.00117730
Iteration 24/1000 | Loss: 0.00199404
Iteration 25/1000 | Loss: 0.00104146
Iteration 26/1000 | Loss: 0.00015637
Iteration 27/1000 | Loss: 0.00111286
Iteration 28/1000 | Loss: 0.00087941
Iteration 29/1000 | Loss: 0.00008403
Iteration 30/1000 | Loss: 0.00015946
Iteration 31/1000 | Loss: 0.00172672
Iteration 32/1000 | Loss: 0.00239819
Iteration 33/1000 | Loss: 0.00134411
Iteration 34/1000 | Loss: 0.00133877
Iteration 35/1000 | Loss: 0.00110489
Iteration 36/1000 | Loss: 0.00041984
Iteration 37/1000 | Loss: 0.00181428
Iteration 38/1000 | Loss: 0.00115005
Iteration 39/1000 | Loss: 0.00207606
Iteration 40/1000 | Loss: 0.00244729
Iteration 41/1000 | Loss: 0.00182503
Iteration 42/1000 | Loss: 0.00217268
Iteration 43/1000 | Loss: 0.00013613
Iteration 44/1000 | Loss: 0.00178495
Iteration 45/1000 | Loss: 0.00170059
Iteration 46/1000 | Loss: 0.00226031
Iteration 47/1000 | Loss: 0.00061647
Iteration 48/1000 | Loss: 0.00330658
Iteration 49/1000 | Loss: 0.00098331
Iteration 50/1000 | Loss: 0.00098064
Iteration 51/1000 | Loss: 0.00061170
Iteration 52/1000 | Loss: 0.00013634
Iteration 53/1000 | Loss: 0.00007686
Iteration 54/1000 | Loss: 0.00011473
Iteration 55/1000 | Loss: 0.00012044
Iteration 56/1000 | Loss: 0.00009426
Iteration 57/1000 | Loss: 0.00004799
Iteration 58/1000 | Loss: 0.00007148
Iteration 59/1000 | Loss: 0.00006102
Iteration 60/1000 | Loss: 0.00046672
Iteration 61/1000 | Loss: 0.00030986
Iteration 62/1000 | Loss: 0.00035059
Iteration 63/1000 | Loss: 0.00041460
Iteration 64/1000 | Loss: 0.00050263
Iteration 65/1000 | Loss: 0.00018745
Iteration 66/1000 | Loss: 0.00007057
Iteration 67/1000 | Loss: 0.00006164
Iteration 68/1000 | Loss: 0.00005256
Iteration 69/1000 | Loss: 0.00004824
Iteration 70/1000 | Loss: 0.00004449
Iteration 71/1000 | Loss: 0.00004390
Iteration 72/1000 | Loss: 0.00005158
Iteration 73/1000 | Loss: 0.00003836
Iteration 74/1000 | Loss: 0.00007285
Iteration 75/1000 | Loss: 0.00003284
Iteration 76/1000 | Loss: 0.00003038
Iteration 77/1000 | Loss: 0.00002911
Iteration 78/1000 | Loss: 0.00002812
Iteration 79/1000 | Loss: 0.00002747
Iteration 80/1000 | Loss: 0.00002701
Iteration 81/1000 | Loss: 0.00002645
Iteration 82/1000 | Loss: 0.00002597
Iteration 83/1000 | Loss: 0.00002568
Iteration 84/1000 | Loss: 0.00002547
Iteration 85/1000 | Loss: 0.00002540
Iteration 86/1000 | Loss: 0.00002539
Iteration 87/1000 | Loss: 0.00002539
Iteration 88/1000 | Loss: 0.00002538
Iteration 89/1000 | Loss: 0.00002536
Iteration 90/1000 | Loss: 0.00002536
Iteration 91/1000 | Loss: 0.00002535
Iteration 92/1000 | Loss: 0.00002534
Iteration 93/1000 | Loss: 0.00002533
Iteration 94/1000 | Loss: 0.00002533
Iteration 95/1000 | Loss: 0.00002532
Iteration 96/1000 | Loss: 0.00002532
Iteration 97/1000 | Loss: 0.00002532
Iteration 98/1000 | Loss: 0.00002531
Iteration 99/1000 | Loss: 0.00002531
Iteration 100/1000 | Loss: 0.00002527
Iteration 101/1000 | Loss: 0.00002527
Iteration 102/1000 | Loss: 0.00002526
Iteration 103/1000 | Loss: 0.00002526
Iteration 104/1000 | Loss: 0.00002526
Iteration 105/1000 | Loss: 0.00002525
Iteration 106/1000 | Loss: 0.00002525
Iteration 107/1000 | Loss: 0.00002525
Iteration 108/1000 | Loss: 0.00002525
Iteration 109/1000 | Loss: 0.00002525
Iteration 110/1000 | Loss: 0.00002525
Iteration 111/1000 | Loss: 0.00002524
Iteration 112/1000 | Loss: 0.00002524
Iteration 113/1000 | Loss: 0.00002524
Iteration 114/1000 | Loss: 0.00002523
Iteration 115/1000 | Loss: 0.00002523
Iteration 116/1000 | Loss: 0.00002523
Iteration 117/1000 | Loss: 0.00002523
Iteration 118/1000 | Loss: 0.00002523
Iteration 119/1000 | Loss: 0.00002523
Iteration 120/1000 | Loss: 0.00002523
Iteration 121/1000 | Loss: 0.00002523
Iteration 122/1000 | Loss: 0.00002523
Iteration 123/1000 | Loss: 0.00002523
Iteration 124/1000 | Loss: 0.00002523
Iteration 125/1000 | Loss: 0.00002522
Iteration 126/1000 | Loss: 0.00002522
Iteration 127/1000 | Loss: 0.00002522
Iteration 128/1000 | Loss: 0.00002521
Iteration 129/1000 | Loss: 0.00002521
Iteration 130/1000 | Loss: 0.00002521
Iteration 131/1000 | Loss: 0.00002520
Iteration 132/1000 | Loss: 0.00002520
Iteration 133/1000 | Loss: 0.00002520
Iteration 134/1000 | Loss: 0.00002520
Iteration 135/1000 | Loss: 0.00002519
Iteration 136/1000 | Loss: 0.00002519
Iteration 137/1000 | Loss: 0.00002519
Iteration 138/1000 | Loss: 0.00002519
Iteration 139/1000 | Loss: 0.00002519
Iteration 140/1000 | Loss: 0.00002518
Iteration 141/1000 | Loss: 0.00002518
Iteration 142/1000 | Loss: 0.00002517
Iteration 143/1000 | Loss: 0.00002517
Iteration 144/1000 | Loss: 0.00002517
Iteration 145/1000 | Loss: 0.00002516
Iteration 146/1000 | Loss: 0.00002516
Iteration 147/1000 | Loss: 0.00002516
Iteration 148/1000 | Loss: 0.00002515
Iteration 149/1000 | Loss: 0.00002515
Iteration 150/1000 | Loss: 0.00002515
Iteration 151/1000 | Loss: 0.00002515
Iteration 152/1000 | Loss: 0.00002515
Iteration 153/1000 | Loss: 0.00002515
Iteration 154/1000 | Loss: 0.00002515
Iteration 155/1000 | Loss: 0.00002515
Iteration 156/1000 | Loss: 0.00002515
Iteration 157/1000 | Loss: 0.00002515
Iteration 158/1000 | Loss: 0.00002515
Iteration 159/1000 | Loss: 0.00002514
Iteration 160/1000 | Loss: 0.00002514
Iteration 161/1000 | Loss: 0.00002514
Iteration 162/1000 | Loss: 0.00002513
Iteration 163/1000 | Loss: 0.00002513
Iteration 164/1000 | Loss: 0.00002513
Iteration 165/1000 | Loss: 0.00002513
Iteration 166/1000 | Loss: 0.00002513
Iteration 167/1000 | Loss: 0.00002513
Iteration 168/1000 | Loss: 0.00002513
Iteration 169/1000 | Loss: 0.00002513
Iteration 170/1000 | Loss: 0.00002513
Iteration 171/1000 | Loss: 0.00002513
Iteration 172/1000 | Loss: 0.00002512
Iteration 173/1000 | Loss: 0.00002512
Iteration 174/1000 | Loss: 0.00002512
Iteration 175/1000 | Loss: 0.00002512
Iteration 176/1000 | Loss: 0.00002512
Iteration 177/1000 | Loss: 0.00002512
Iteration 178/1000 | Loss: 0.00002512
Iteration 179/1000 | Loss: 0.00002512
Iteration 180/1000 | Loss: 0.00002512
Iteration 181/1000 | Loss: 0.00002512
Iteration 182/1000 | Loss: 0.00002511
Iteration 183/1000 | Loss: 0.00002511
Iteration 184/1000 | Loss: 0.00002511
Iteration 185/1000 | Loss: 0.00002511
Iteration 186/1000 | Loss: 0.00002511
Iteration 187/1000 | Loss: 0.00002511
Iteration 188/1000 | Loss: 0.00002511
Iteration 189/1000 | Loss: 0.00002511
Iteration 190/1000 | Loss: 0.00002511
Iteration 191/1000 | Loss: 0.00002511
Iteration 192/1000 | Loss: 0.00002510
Iteration 193/1000 | Loss: 0.00002510
Iteration 194/1000 | Loss: 0.00002510
Iteration 195/1000 | Loss: 0.00002510
Iteration 196/1000 | Loss: 0.00002510
Iteration 197/1000 | Loss: 0.00002510
Iteration 198/1000 | Loss: 0.00002510
Iteration 199/1000 | Loss: 0.00002510
Iteration 200/1000 | Loss: 0.00002510
Iteration 201/1000 | Loss: 0.00002510
Iteration 202/1000 | Loss: 0.00002510
Iteration 203/1000 | Loss: 0.00002510
Iteration 204/1000 | Loss: 0.00002510
Iteration 205/1000 | Loss: 0.00002510
Iteration 206/1000 | Loss: 0.00002510
Iteration 207/1000 | Loss: 0.00002510
Iteration 208/1000 | Loss: 0.00002510
Iteration 209/1000 | Loss: 0.00002510
Iteration 210/1000 | Loss: 0.00002510
Iteration 211/1000 | Loss: 0.00002510
Iteration 212/1000 | Loss: 0.00002510
Iteration 213/1000 | Loss: 0.00002510
Iteration 214/1000 | Loss: 0.00002510
Iteration 215/1000 | Loss: 0.00002510
Iteration 216/1000 | Loss: 0.00002510
Iteration 217/1000 | Loss: 0.00002510
Iteration 218/1000 | Loss: 0.00002510
Iteration 219/1000 | Loss: 0.00002510
Iteration 220/1000 | Loss: 0.00002510
Iteration 221/1000 | Loss: 0.00002510
Iteration 222/1000 | Loss: 0.00002510
Iteration 223/1000 | Loss: 0.00002510
Iteration 224/1000 | Loss: 0.00002510
Iteration 225/1000 | Loss: 0.00002510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.5099941922235303e-05, 2.5099941922235303e-05, 2.5099941922235303e-05, 2.5099941922235303e-05, 2.5099941922235303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5099941922235303e-05

Optimization complete. Final v2v error: 4.250436305999756 mm

Highest mean error: 10.093965530395508 mm for frame 86

Lowest mean error: 3.758220911026001 mm for frame 25

Saving results

Total time: 170.69282364845276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870453
Iteration 2/25 | Loss: 0.00128064
Iteration 3/25 | Loss: 0.00110516
Iteration 4/25 | Loss: 0.00106727
Iteration 5/25 | Loss: 0.00105619
Iteration 6/25 | Loss: 0.00105682
Iteration 7/25 | Loss: 0.00105267
Iteration 8/25 | Loss: 0.00104978
Iteration 9/25 | Loss: 0.00104887
Iteration 10/25 | Loss: 0.00104694
Iteration 11/25 | Loss: 0.00104643
Iteration 12/25 | Loss: 0.00104408
Iteration 13/25 | Loss: 0.00104726
Iteration 14/25 | Loss: 0.00104445
Iteration 15/25 | Loss: 0.00104129
Iteration 16/25 | Loss: 0.00104050
Iteration 17/25 | Loss: 0.00104008
Iteration 18/25 | Loss: 0.00104005
Iteration 19/25 | Loss: 0.00104005
Iteration 20/25 | Loss: 0.00104005
Iteration 21/25 | Loss: 0.00104005
Iteration 22/25 | Loss: 0.00104005
Iteration 23/25 | Loss: 0.00104005
Iteration 24/25 | Loss: 0.00104005
Iteration 25/25 | Loss: 0.00104005

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.68244433
Iteration 2/25 | Loss: 0.00074389
Iteration 3/25 | Loss: 0.00074388
Iteration 4/25 | Loss: 0.00074389
Iteration 5/25 | Loss: 0.00074389
Iteration 6/25 | Loss: 0.00074389
Iteration 7/25 | Loss: 0.00074389
Iteration 8/25 | Loss: 0.00074389
Iteration 9/25 | Loss: 0.00074389
Iteration 10/25 | Loss: 0.00074389
Iteration 11/25 | Loss: 0.00074389
Iteration 12/25 | Loss: 0.00074389
Iteration 13/25 | Loss: 0.00074389
Iteration 14/25 | Loss: 0.00074389
Iteration 15/25 | Loss: 0.00074389
Iteration 16/25 | Loss: 0.00074389
Iteration 17/25 | Loss: 0.00074389
Iteration 18/25 | Loss: 0.00074389
Iteration 19/25 | Loss: 0.00074389
Iteration 20/25 | Loss: 0.00074389
Iteration 21/25 | Loss: 0.00074389
Iteration 22/25 | Loss: 0.00074389
Iteration 23/25 | Loss: 0.00074389
Iteration 24/25 | Loss: 0.00074389
Iteration 25/25 | Loss: 0.00074389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074389
Iteration 2/1000 | Loss: 0.00003237
Iteration 3/1000 | Loss: 0.00002188
Iteration 4/1000 | Loss: 0.00002040
Iteration 5/1000 | Loss: 0.00001958
Iteration 6/1000 | Loss: 0.00001911
Iteration 7/1000 | Loss: 0.00001882
Iteration 8/1000 | Loss: 0.00001869
Iteration 9/1000 | Loss: 0.00001863
Iteration 10/1000 | Loss: 0.00001862
Iteration 11/1000 | Loss: 0.00001861
Iteration 12/1000 | Loss: 0.00001857
Iteration 13/1000 | Loss: 0.00001856
Iteration 14/1000 | Loss: 0.00001856
Iteration 15/1000 | Loss: 0.00001855
Iteration 16/1000 | Loss: 0.00001852
Iteration 17/1000 | Loss: 0.00001852
Iteration 18/1000 | Loss: 0.00001851
Iteration 19/1000 | Loss: 0.00001851
Iteration 20/1000 | Loss: 0.00001851
Iteration 21/1000 | Loss: 0.00001850
Iteration 22/1000 | Loss: 0.00001849
Iteration 23/1000 | Loss: 0.00001849
Iteration 24/1000 | Loss: 0.00001848
Iteration 25/1000 | Loss: 0.00001848
Iteration 26/1000 | Loss: 0.00001848
Iteration 27/1000 | Loss: 0.00001847
Iteration 28/1000 | Loss: 0.00001847
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00001847
Iteration 31/1000 | Loss: 0.00001846
Iteration 32/1000 | Loss: 0.00001846
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001845
Iteration 35/1000 | Loss: 0.00001845
Iteration 36/1000 | Loss: 0.00001845
Iteration 37/1000 | Loss: 0.00001845
Iteration 38/1000 | Loss: 0.00001844
Iteration 39/1000 | Loss: 0.00001844
Iteration 40/1000 | Loss: 0.00001844
Iteration 41/1000 | Loss: 0.00001843
Iteration 42/1000 | Loss: 0.00001843
Iteration 43/1000 | Loss: 0.00001843
Iteration 44/1000 | Loss: 0.00001843
Iteration 45/1000 | Loss: 0.00001843
Iteration 46/1000 | Loss: 0.00001843
Iteration 47/1000 | Loss: 0.00001843
Iteration 48/1000 | Loss: 0.00001843
Iteration 49/1000 | Loss: 0.00001843
Iteration 50/1000 | Loss: 0.00001842
Iteration 51/1000 | Loss: 0.00001842
Iteration 52/1000 | Loss: 0.00001842
Iteration 53/1000 | Loss: 0.00001842
Iteration 54/1000 | Loss: 0.00001842
Iteration 55/1000 | Loss: 0.00001841
Iteration 56/1000 | Loss: 0.00001841
Iteration 57/1000 | Loss: 0.00001841
Iteration 58/1000 | Loss: 0.00001841
Iteration 59/1000 | Loss: 0.00001841
Iteration 60/1000 | Loss: 0.00001841
Iteration 61/1000 | Loss: 0.00001841
Iteration 62/1000 | Loss: 0.00001841
Iteration 63/1000 | Loss: 0.00001841
Iteration 64/1000 | Loss: 0.00001840
Iteration 65/1000 | Loss: 0.00001840
Iteration 66/1000 | Loss: 0.00001840
Iteration 67/1000 | Loss: 0.00001840
Iteration 68/1000 | Loss: 0.00001840
Iteration 69/1000 | Loss: 0.00001839
Iteration 70/1000 | Loss: 0.00001839
Iteration 71/1000 | Loss: 0.00001839
Iteration 72/1000 | Loss: 0.00001839
Iteration 73/1000 | Loss: 0.00001839
Iteration 74/1000 | Loss: 0.00001839
Iteration 75/1000 | Loss: 0.00001839
Iteration 76/1000 | Loss: 0.00001839
Iteration 77/1000 | Loss: 0.00001839
Iteration 78/1000 | Loss: 0.00001839
Iteration 79/1000 | Loss: 0.00001839
Iteration 80/1000 | Loss: 0.00001839
Iteration 81/1000 | Loss: 0.00001838
Iteration 82/1000 | Loss: 0.00001838
Iteration 83/1000 | Loss: 0.00001838
Iteration 84/1000 | Loss: 0.00001838
Iteration 85/1000 | Loss: 0.00001838
Iteration 86/1000 | Loss: 0.00001838
Iteration 87/1000 | Loss: 0.00001838
Iteration 88/1000 | Loss: 0.00001837
Iteration 89/1000 | Loss: 0.00001837
Iteration 90/1000 | Loss: 0.00001837
Iteration 91/1000 | Loss: 0.00001837
Iteration 92/1000 | Loss: 0.00001837
Iteration 93/1000 | Loss: 0.00001837
Iteration 94/1000 | Loss: 0.00001837
Iteration 95/1000 | Loss: 0.00001837
Iteration 96/1000 | Loss: 0.00001837
Iteration 97/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.8367476513958536e-05, 1.8367476513958536e-05, 1.8367476513958536e-05, 1.8367476513958536e-05, 1.8367476513958536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8367476513958536e-05

Optimization complete. Final v2v error: 3.5908308029174805 mm

Highest mean error: 10.086090087890625 mm for frame 33

Lowest mean error: 3.172889232635498 mm for frame 148

Saving results

Total time: 54.58773183822632
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01130385
Iteration 2/25 | Loss: 0.01130384
Iteration 3/25 | Loss: 0.00281249
Iteration 4/25 | Loss: 0.00202172
Iteration 5/25 | Loss: 0.00171599
Iteration 6/25 | Loss: 0.00149449
Iteration 7/25 | Loss: 0.00141746
Iteration 8/25 | Loss: 0.00132815
Iteration 9/25 | Loss: 0.00125446
Iteration 10/25 | Loss: 0.00116982
Iteration 11/25 | Loss: 0.00112387
Iteration 12/25 | Loss: 0.00109217
Iteration 13/25 | Loss: 0.00107609
Iteration 14/25 | Loss: 0.00106793
Iteration 15/25 | Loss: 0.00106429
Iteration 16/25 | Loss: 0.00105837
Iteration 17/25 | Loss: 0.00105638
Iteration 18/25 | Loss: 0.00105505
Iteration 19/25 | Loss: 0.00105452
Iteration 20/25 | Loss: 0.00105431
Iteration 21/25 | Loss: 0.00105419
Iteration 22/25 | Loss: 0.00105417
Iteration 23/25 | Loss: 0.00105417
Iteration 24/25 | Loss: 0.00105417
Iteration 25/25 | Loss: 0.00105417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53270531
Iteration 2/25 | Loss: 0.00082636
Iteration 3/25 | Loss: 0.00082636
Iteration 4/25 | Loss: 0.00082636
Iteration 5/25 | Loss: 0.00082636
Iteration 6/25 | Loss: 0.00082636
Iteration 7/25 | Loss: 0.00082636
Iteration 8/25 | Loss: 0.00082636
Iteration 9/25 | Loss: 0.00082636
Iteration 10/25 | Loss: 0.00082636
Iteration 11/25 | Loss: 0.00082636
Iteration 12/25 | Loss: 0.00082636
Iteration 13/25 | Loss: 0.00082636
Iteration 14/25 | Loss: 0.00082636
Iteration 15/25 | Loss: 0.00082636
Iteration 16/25 | Loss: 0.00082636
Iteration 17/25 | Loss: 0.00082636
Iteration 18/25 | Loss: 0.00082636
Iteration 19/25 | Loss: 0.00082636
Iteration 20/25 | Loss: 0.00082636
Iteration 21/25 | Loss: 0.00082636
Iteration 22/25 | Loss: 0.00082636
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008263591444119811, 0.0008263591444119811, 0.0008263591444119811, 0.0008263591444119811, 0.0008263591444119811]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008263591444119811

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082636
Iteration 2/1000 | Loss: 0.00007436
Iteration 3/1000 | Loss: 0.00005572
Iteration 4/1000 | Loss: 0.00009562
Iteration 5/1000 | Loss: 0.00170912
Iteration 6/1000 | Loss: 0.00058418
Iteration 7/1000 | Loss: 0.00024918
Iteration 8/1000 | Loss: 0.00009148
Iteration 9/1000 | Loss: 0.00003544
Iteration 10/1000 | Loss: 0.00005440
Iteration 11/1000 | Loss: 0.00002727
Iteration 12/1000 | Loss: 0.00002509
Iteration 13/1000 | Loss: 0.00002391
Iteration 14/1000 | Loss: 0.00002322
Iteration 15/1000 | Loss: 0.00002276
Iteration 16/1000 | Loss: 0.00002235
Iteration 17/1000 | Loss: 0.00005377
Iteration 18/1000 | Loss: 0.00002207
Iteration 19/1000 | Loss: 0.00002185
Iteration 20/1000 | Loss: 0.00002180
Iteration 21/1000 | Loss: 0.00002175
Iteration 22/1000 | Loss: 0.00006460
Iteration 23/1000 | Loss: 0.00002763
Iteration 24/1000 | Loss: 0.00003593
Iteration 25/1000 | Loss: 0.00002172
Iteration 26/1000 | Loss: 0.00002163
Iteration 27/1000 | Loss: 0.00002158
Iteration 28/1000 | Loss: 0.00002158
Iteration 29/1000 | Loss: 0.00002158
Iteration 30/1000 | Loss: 0.00002158
Iteration 31/1000 | Loss: 0.00002158
Iteration 32/1000 | Loss: 0.00002158
Iteration 33/1000 | Loss: 0.00002157
Iteration 34/1000 | Loss: 0.00002157
Iteration 35/1000 | Loss: 0.00002157
Iteration 36/1000 | Loss: 0.00002157
Iteration 37/1000 | Loss: 0.00002157
Iteration 38/1000 | Loss: 0.00002156
Iteration 39/1000 | Loss: 0.00002156
Iteration 40/1000 | Loss: 0.00002155
Iteration 41/1000 | Loss: 0.00002154
Iteration 42/1000 | Loss: 0.00002154
Iteration 43/1000 | Loss: 0.00002153
Iteration 44/1000 | Loss: 0.00005709
Iteration 45/1000 | Loss: 0.00002464
Iteration 46/1000 | Loss: 0.00002844
Iteration 47/1000 | Loss: 0.00002158
Iteration 48/1000 | Loss: 0.00002156
Iteration 49/1000 | Loss: 0.00002156
Iteration 50/1000 | Loss: 0.00002155
Iteration 51/1000 | Loss: 0.00002155
Iteration 52/1000 | Loss: 0.00002155
Iteration 53/1000 | Loss: 0.00002155
Iteration 54/1000 | Loss: 0.00002155
Iteration 55/1000 | Loss: 0.00002155
Iteration 56/1000 | Loss: 0.00002155
Iteration 57/1000 | Loss: 0.00002155
Iteration 58/1000 | Loss: 0.00002154
Iteration 59/1000 | Loss: 0.00002154
Iteration 60/1000 | Loss: 0.00002153
Iteration 61/1000 | Loss: 0.00002153
Iteration 62/1000 | Loss: 0.00002152
Iteration 63/1000 | Loss: 0.00002152
Iteration 64/1000 | Loss: 0.00002152
Iteration 65/1000 | Loss: 0.00002152
Iteration 66/1000 | Loss: 0.00002152
Iteration 67/1000 | Loss: 0.00002152
Iteration 68/1000 | Loss: 0.00002151
Iteration 69/1000 | Loss: 0.00002151
Iteration 70/1000 | Loss: 0.00002151
Iteration 71/1000 | Loss: 0.00002151
Iteration 72/1000 | Loss: 0.00002151
Iteration 73/1000 | Loss: 0.00003423
Iteration 74/1000 | Loss: 0.00002156
Iteration 75/1000 | Loss: 0.00002155
Iteration 76/1000 | Loss: 0.00002153
Iteration 77/1000 | Loss: 0.00002153
Iteration 78/1000 | Loss: 0.00002153
Iteration 79/1000 | Loss: 0.00002152
Iteration 80/1000 | Loss: 0.00002152
Iteration 81/1000 | Loss: 0.00002152
Iteration 82/1000 | Loss: 0.00002152
Iteration 83/1000 | Loss: 0.00002152
Iteration 84/1000 | Loss: 0.00002152
Iteration 85/1000 | Loss: 0.00002152
Iteration 86/1000 | Loss: 0.00002151
Iteration 87/1000 | Loss: 0.00002151
Iteration 88/1000 | Loss: 0.00002151
Iteration 89/1000 | Loss: 0.00002151
Iteration 90/1000 | Loss: 0.00002150
Iteration 91/1000 | Loss: 0.00002150
Iteration 92/1000 | Loss: 0.00003007
Iteration 93/1000 | Loss: 0.00002151
Iteration 94/1000 | Loss: 0.00002149
Iteration 95/1000 | Loss: 0.00002149
Iteration 96/1000 | Loss: 0.00002149
Iteration 97/1000 | Loss: 0.00002149
Iteration 98/1000 | Loss: 0.00002149
Iteration 99/1000 | Loss: 0.00002149
Iteration 100/1000 | Loss: 0.00002149
Iteration 101/1000 | Loss: 0.00002149
Iteration 102/1000 | Loss: 0.00002149
Iteration 103/1000 | Loss: 0.00002149
Iteration 104/1000 | Loss: 0.00002148
Iteration 105/1000 | Loss: 0.00002148
Iteration 106/1000 | Loss: 0.00002148
Iteration 107/1000 | Loss: 0.00002148
Iteration 108/1000 | Loss: 0.00002147
Iteration 109/1000 | Loss: 0.00004297
Iteration 110/1000 | Loss: 0.00002358
Iteration 111/1000 | Loss: 0.00002156
Iteration 112/1000 | Loss: 0.00002156
Iteration 113/1000 | Loss: 0.00002156
Iteration 114/1000 | Loss: 0.00002156
Iteration 115/1000 | Loss: 0.00002156
Iteration 116/1000 | Loss: 0.00002156
Iteration 117/1000 | Loss: 0.00002156
Iteration 118/1000 | Loss: 0.00002156
Iteration 119/1000 | Loss: 0.00002155
Iteration 120/1000 | Loss: 0.00002155
Iteration 121/1000 | Loss: 0.00002155
Iteration 122/1000 | Loss: 0.00002201
Iteration 123/1000 | Loss: 0.00002169
Iteration 124/1000 | Loss: 0.00002175
Iteration 125/1000 | Loss: 0.00002156
Iteration 126/1000 | Loss: 0.00002156
Iteration 127/1000 | Loss: 0.00002155
Iteration 128/1000 | Loss: 0.00002155
Iteration 129/1000 | Loss: 0.00002155
Iteration 130/1000 | Loss: 0.00002155
Iteration 131/1000 | Loss: 0.00002155
Iteration 132/1000 | Loss: 0.00002155
Iteration 133/1000 | Loss: 0.00002155
Iteration 134/1000 | Loss: 0.00002155
Iteration 135/1000 | Loss: 0.00002155
Iteration 136/1000 | Loss: 0.00002155
Iteration 137/1000 | Loss: 0.00002155
Iteration 138/1000 | Loss: 0.00002155
Iteration 139/1000 | Loss: 0.00002155
Iteration 140/1000 | Loss: 0.00002155
Iteration 141/1000 | Loss: 0.00002155
Iteration 142/1000 | Loss: 0.00002155
Iteration 143/1000 | Loss: 0.00002155
Iteration 144/1000 | Loss: 0.00002155
Iteration 145/1000 | Loss: 0.00002155
Iteration 146/1000 | Loss: 0.00002155
Iteration 147/1000 | Loss: 0.00002155
Iteration 148/1000 | Loss: 0.00002155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.1548295990214683e-05, 2.1548295990214683e-05, 2.1548295990214683e-05, 2.1548295990214683e-05, 2.1548295990214683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1548295990214683e-05

Optimization complete. Final v2v error: 3.9019455909729004 mm

Highest mean error: 9.526253700256348 mm for frame 238

Lowest mean error: 3.5241942405700684 mm for frame 0

Saving results

Total time: 98.61800050735474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00671845
Iteration 2/25 | Loss: 0.00143174
Iteration 3/25 | Loss: 0.00120723
Iteration 4/25 | Loss: 0.00116543
Iteration 5/25 | Loss: 0.00115156
Iteration 6/25 | Loss: 0.00114887
Iteration 7/25 | Loss: 0.00114835
Iteration 8/25 | Loss: 0.00114835
Iteration 9/25 | Loss: 0.00114835
Iteration 10/25 | Loss: 0.00114835
Iteration 11/25 | Loss: 0.00114835
Iteration 12/25 | Loss: 0.00114835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011483457637950778, 0.0011483457637950778, 0.0011483457637950778, 0.0011483457637950778, 0.0011483457637950778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011483457637950778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.86618900
Iteration 2/25 | Loss: 0.00091785
Iteration 3/25 | Loss: 0.00091785
Iteration 4/25 | Loss: 0.00091785
Iteration 5/25 | Loss: 0.00091785
Iteration 6/25 | Loss: 0.00091785
Iteration 7/25 | Loss: 0.00091785
Iteration 8/25 | Loss: 0.00091785
Iteration 9/25 | Loss: 0.00091785
Iteration 10/25 | Loss: 0.00091785
Iteration 11/25 | Loss: 0.00091785
Iteration 12/25 | Loss: 0.00091785
Iteration 13/25 | Loss: 0.00091785
Iteration 14/25 | Loss: 0.00091785
Iteration 15/25 | Loss: 0.00091785
Iteration 16/25 | Loss: 0.00091785
Iteration 17/25 | Loss: 0.00091785
Iteration 18/25 | Loss: 0.00091785
Iteration 19/25 | Loss: 0.00091785
Iteration 20/25 | Loss: 0.00091785
Iteration 21/25 | Loss: 0.00091785
Iteration 22/25 | Loss: 0.00091785
Iteration 23/25 | Loss: 0.00091785
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009178504114970565, 0.0009178504114970565, 0.0009178504114970565, 0.0009178504114970565, 0.0009178504114970565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009178504114970565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091785
Iteration 2/1000 | Loss: 0.00005581
Iteration 3/1000 | Loss: 0.00003804
Iteration 4/1000 | Loss: 0.00003362
Iteration 5/1000 | Loss: 0.00003204
Iteration 6/1000 | Loss: 0.00003111
Iteration 7/1000 | Loss: 0.00003062
Iteration 8/1000 | Loss: 0.00003015
Iteration 9/1000 | Loss: 0.00002977
Iteration 10/1000 | Loss: 0.00002954
Iteration 11/1000 | Loss: 0.00002933
Iteration 12/1000 | Loss: 0.00002921
Iteration 13/1000 | Loss: 0.00002919
Iteration 14/1000 | Loss: 0.00002915
Iteration 15/1000 | Loss: 0.00002915
Iteration 16/1000 | Loss: 0.00002914
Iteration 17/1000 | Loss: 0.00002914
Iteration 18/1000 | Loss: 0.00002909
Iteration 19/1000 | Loss: 0.00002907
Iteration 20/1000 | Loss: 0.00002906
Iteration 21/1000 | Loss: 0.00002905
Iteration 22/1000 | Loss: 0.00002905
Iteration 23/1000 | Loss: 0.00002905
Iteration 24/1000 | Loss: 0.00002905
Iteration 25/1000 | Loss: 0.00002905
Iteration 26/1000 | Loss: 0.00002905
Iteration 27/1000 | Loss: 0.00002905
Iteration 28/1000 | Loss: 0.00002904
Iteration 29/1000 | Loss: 0.00002904
Iteration 30/1000 | Loss: 0.00002903
Iteration 31/1000 | Loss: 0.00002902
Iteration 32/1000 | Loss: 0.00002902
Iteration 33/1000 | Loss: 0.00002902
Iteration 34/1000 | Loss: 0.00002902
Iteration 35/1000 | Loss: 0.00002901
Iteration 36/1000 | Loss: 0.00002901
Iteration 37/1000 | Loss: 0.00002901
Iteration 38/1000 | Loss: 0.00002901
Iteration 39/1000 | Loss: 0.00002901
Iteration 40/1000 | Loss: 0.00002900
Iteration 41/1000 | Loss: 0.00002900
Iteration 42/1000 | Loss: 0.00002900
Iteration 43/1000 | Loss: 0.00002900
Iteration 44/1000 | Loss: 0.00002899
Iteration 45/1000 | Loss: 0.00002899
Iteration 46/1000 | Loss: 0.00002899
Iteration 47/1000 | Loss: 0.00002899
Iteration 48/1000 | Loss: 0.00002899
Iteration 49/1000 | Loss: 0.00002898
Iteration 50/1000 | Loss: 0.00002898
Iteration 51/1000 | Loss: 0.00002898
Iteration 52/1000 | Loss: 0.00002898
Iteration 53/1000 | Loss: 0.00002898
Iteration 54/1000 | Loss: 0.00002897
Iteration 55/1000 | Loss: 0.00002897
Iteration 56/1000 | Loss: 0.00002897
Iteration 57/1000 | Loss: 0.00002897
Iteration 58/1000 | Loss: 0.00002897
Iteration 59/1000 | Loss: 0.00002897
Iteration 60/1000 | Loss: 0.00002897
Iteration 61/1000 | Loss: 0.00002897
Iteration 62/1000 | Loss: 0.00002897
Iteration 63/1000 | Loss: 0.00002897
Iteration 64/1000 | Loss: 0.00002897
Iteration 65/1000 | Loss: 0.00002897
Iteration 66/1000 | Loss: 0.00002897
Iteration 67/1000 | Loss: 0.00002897
Iteration 68/1000 | Loss: 0.00002897
Iteration 69/1000 | Loss: 0.00002897
Iteration 70/1000 | Loss: 0.00002897
Iteration 71/1000 | Loss: 0.00002897
Iteration 72/1000 | Loss: 0.00002897
Iteration 73/1000 | Loss: 0.00002897
Iteration 74/1000 | Loss: 0.00002897
Iteration 75/1000 | Loss: 0.00002897
Iteration 76/1000 | Loss: 0.00002897
Iteration 77/1000 | Loss: 0.00002897
Iteration 78/1000 | Loss: 0.00002897
Iteration 79/1000 | Loss: 0.00002897
Iteration 80/1000 | Loss: 0.00002897
Iteration 81/1000 | Loss: 0.00002897
Iteration 82/1000 | Loss: 0.00002897
Iteration 83/1000 | Loss: 0.00002897
Iteration 84/1000 | Loss: 0.00002897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.8967195248696953e-05, 2.8967195248696953e-05, 2.8967195248696953e-05, 2.8967195248696953e-05, 2.8967195248696953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8967195248696953e-05

Optimization complete. Final v2v error: 4.656651020050049 mm

Highest mean error: 4.908108711242676 mm for frame 57

Lowest mean error: 4.301202297210693 mm for frame 211

Saving results

Total time: 36.292558670043945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00637690
Iteration 2/25 | Loss: 0.00133431
Iteration 3/25 | Loss: 0.00116318
Iteration 4/25 | Loss: 0.00104019
Iteration 5/25 | Loss: 0.00098320
Iteration 6/25 | Loss: 0.00096822
Iteration 7/25 | Loss: 0.00096035
Iteration 8/25 | Loss: 0.00095800
Iteration 9/25 | Loss: 0.00095959
Iteration 10/25 | Loss: 0.00095487
Iteration 11/25 | Loss: 0.00095282
Iteration 12/25 | Loss: 0.00095195
Iteration 13/25 | Loss: 0.00095165
Iteration 14/25 | Loss: 0.00095176
Iteration 15/25 | Loss: 0.00095161
Iteration 16/25 | Loss: 0.00095149
Iteration 17/25 | Loss: 0.00095138
Iteration 18/25 | Loss: 0.00095119
Iteration 19/25 | Loss: 0.00095110
Iteration 20/25 | Loss: 0.00095102
Iteration 21/25 | Loss: 0.00095101
Iteration 22/25 | Loss: 0.00095101
Iteration 23/25 | Loss: 0.00095100
Iteration 24/25 | Loss: 0.00095100
Iteration 25/25 | Loss: 0.00095100

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.28588820
Iteration 2/25 | Loss: 0.00066379
Iteration 3/25 | Loss: 0.00066379
Iteration 4/25 | Loss: 0.00066379
Iteration 5/25 | Loss: 0.00066379
Iteration 6/25 | Loss: 0.00066379
Iteration 7/25 | Loss: 0.00066379
Iteration 8/25 | Loss: 0.00066379
Iteration 9/25 | Loss: 0.00066379
Iteration 10/25 | Loss: 0.00066379
Iteration 11/25 | Loss: 0.00066379
Iteration 12/25 | Loss: 0.00066379
Iteration 13/25 | Loss: 0.00066379
Iteration 14/25 | Loss: 0.00066379
Iteration 15/25 | Loss: 0.00066379
Iteration 16/25 | Loss: 0.00066379
Iteration 17/25 | Loss: 0.00066379
Iteration 18/25 | Loss: 0.00066379
Iteration 19/25 | Loss: 0.00066379
Iteration 20/25 | Loss: 0.00066379
Iteration 21/25 | Loss: 0.00066379
Iteration 22/25 | Loss: 0.00066379
Iteration 23/25 | Loss: 0.00066379
Iteration 24/25 | Loss: 0.00066379
Iteration 25/25 | Loss: 0.00066379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066379
Iteration 2/1000 | Loss: 0.00002754
Iteration 3/1000 | Loss: 0.00002108
Iteration 4/1000 | Loss: 0.00001994
Iteration 5/1000 | Loss: 0.00001909
Iteration 6/1000 | Loss: 0.00001877
Iteration 7/1000 | Loss: 0.00001839
Iteration 8/1000 | Loss: 0.00001825
Iteration 9/1000 | Loss: 0.00001815
Iteration 10/1000 | Loss: 0.00001805
Iteration 11/1000 | Loss: 0.00001805
Iteration 12/1000 | Loss: 0.00001801
Iteration 13/1000 | Loss: 0.00001798
Iteration 14/1000 | Loss: 0.00001798
Iteration 15/1000 | Loss: 0.00001798
Iteration 16/1000 | Loss: 0.00001797
Iteration 17/1000 | Loss: 0.00001797
Iteration 18/1000 | Loss: 0.00001797
Iteration 19/1000 | Loss: 0.00001797
Iteration 20/1000 | Loss: 0.00001796
Iteration 21/1000 | Loss: 0.00001796
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00001794
Iteration 24/1000 | Loss: 0.00001793
Iteration 25/1000 | Loss: 0.00001793
Iteration 26/1000 | Loss: 0.00001793
Iteration 27/1000 | Loss: 0.00001793
Iteration 28/1000 | Loss: 0.00001793
Iteration 29/1000 | Loss: 0.00001793
Iteration 30/1000 | Loss: 0.00001792
Iteration 31/1000 | Loss: 0.00001792
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001790
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001789
Iteration 37/1000 | Loss: 0.00001789
Iteration 38/1000 | Loss: 0.00001789
Iteration 39/1000 | Loss: 0.00001789
Iteration 40/1000 | Loss: 0.00001789
Iteration 41/1000 | Loss: 0.00001789
Iteration 42/1000 | Loss: 0.00001788
Iteration 43/1000 | Loss: 0.00001788
Iteration 44/1000 | Loss: 0.00001788
Iteration 45/1000 | Loss: 0.00001787
Iteration 46/1000 | Loss: 0.00001787
Iteration 47/1000 | Loss: 0.00001787
Iteration 48/1000 | Loss: 0.00001787
Iteration 49/1000 | Loss: 0.00001787
Iteration 50/1000 | Loss: 0.00001786
Iteration 51/1000 | Loss: 0.00001786
Iteration 52/1000 | Loss: 0.00001786
Iteration 53/1000 | Loss: 0.00001786
Iteration 54/1000 | Loss: 0.00001786
Iteration 55/1000 | Loss: 0.00001785
Iteration 56/1000 | Loss: 0.00001785
Iteration 57/1000 | Loss: 0.00001785
Iteration 58/1000 | Loss: 0.00001785
Iteration 59/1000 | Loss: 0.00001785
Iteration 60/1000 | Loss: 0.00001785
Iteration 61/1000 | Loss: 0.00001784
Iteration 62/1000 | Loss: 0.00001784
Iteration 63/1000 | Loss: 0.00001784
Iteration 64/1000 | Loss: 0.00001784
Iteration 65/1000 | Loss: 0.00001784
Iteration 66/1000 | Loss: 0.00001783
Iteration 67/1000 | Loss: 0.00001783
Iteration 68/1000 | Loss: 0.00001783
Iteration 69/1000 | Loss: 0.00001783
Iteration 70/1000 | Loss: 0.00001783
Iteration 71/1000 | Loss: 0.00001783
Iteration 72/1000 | Loss: 0.00001783
Iteration 73/1000 | Loss: 0.00001783
Iteration 74/1000 | Loss: 0.00001782
Iteration 75/1000 | Loss: 0.00001782
Iteration 76/1000 | Loss: 0.00001782
Iteration 77/1000 | Loss: 0.00001782
Iteration 78/1000 | Loss: 0.00001782
Iteration 79/1000 | Loss: 0.00001782
Iteration 80/1000 | Loss: 0.00001782
Iteration 81/1000 | Loss: 0.00001782
Iteration 82/1000 | Loss: 0.00001782
Iteration 83/1000 | Loss: 0.00001782
Iteration 84/1000 | Loss: 0.00001782
Iteration 85/1000 | Loss: 0.00001782
Iteration 86/1000 | Loss: 0.00001781
Iteration 87/1000 | Loss: 0.00001781
Iteration 88/1000 | Loss: 0.00001781
Iteration 89/1000 | Loss: 0.00001781
Iteration 90/1000 | Loss: 0.00001781
Iteration 91/1000 | Loss: 0.00001781
Iteration 92/1000 | Loss: 0.00001781
Iteration 93/1000 | Loss: 0.00001781
Iteration 94/1000 | Loss: 0.00001781
Iteration 95/1000 | Loss: 0.00001781
Iteration 96/1000 | Loss: 0.00001781
Iteration 97/1000 | Loss: 0.00001781
Iteration 98/1000 | Loss: 0.00001781
Iteration 99/1000 | Loss: 0.00001781
Iteration 100/1000 | Loss: 0.00001781
Iteration 101/1000 | Loss: 0.00001781
Iteration 102/1000 | Loss: 0.00001781
Iteration 103/1000 | Loss: 0.00001781
Iteration 104/1000 | Loss: 0.00001781
Iteration 105/1000 | Loss: 0.00001781
Iteration 106/1000 | Loss: 0.00001781
Iteration 107/1000 | Loss: 0.00001781
Iteration 108/1000 | Loss: 0.00001781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.7806620235205628e-05, 1.7806620235205628e-05, 1.7806620235205628e-05, 1.7806620235205628e-05, 1.7806620235205628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7806620235205628e-05

Optimization complete. Final v2v error: 3.674238443374634 mm

Highest mean error: 9.591331481933594 mm for frame 213

Lowest mean error: 3.369947671890259 mm for frame 217

Saving results

Total time: 60.54882454872131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477524
Iteration 2/25 | Loss: 0.00120537
Iteration 3/25 | Loss: 0.00106987
Iteration 4/25 | Loss: 0.00104502
Iteration 5/25 | Loss: 0.00103308
Iteration 6/25 | Loss: 0.00103006
Iteration 7/25 | Loss: 0.00102918
Iteration 8/25 | Loss: 0.00102918
Iteration 9/25 | Loss: 0.00102918
Iteration 10/25 | Loss: 0.00102918
Iteration 11/25 | Loss: 0.00102918
Iteration 12/25 | Loss: 0.00102918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010291819926351309, 0.0010291819926351309, 0.0010291819926351309, 0.0010291819926351309, 0.0010291819926351309]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010291819926351309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53361058
Iteration 2/25 | Loss: 0.00080350
Iteration 3/25 | Loss: 0.00080350
Iteration 4/25 | Loss: 0.00080350
Iteration 5/25 | Loss: 0.00080350
Iteration 6/25 | Loss: 0.00080349
Iteration 7/25 | Loss: 0.00080349
Iteration 8/25 | Loss: 0.00080349
Iteration 9/25 | Loss: 0.00080349
Iteration 10/25 | Loss: 0.00080349
Iteration 11/25 | Loss: 0.00080349
Iteration 12/25 | Loss: 0.00080349
Iteration 13/25 | Loss: 0.00080349
Iteration 14/25 | Loss: 0.00080349
Iteration 15/25 | Loss: 0.00080349
Iteration 16/25 | Loss: 0.00080349
Iteration 17/25 | Loss: 0.00080349
Iteration 18/25 | Loss: 0.00080349
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008034938364289701, 0.0008034938364289701, 0.0008034938364289701, 0.0008034938364289701, 0.0008034938364289701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008034938364289701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080349
Iteration 2/1000 | Loss: 0.00003600
Iteration 3/1000 | Loss: 0.00002651
Iteration 4/1000 | Loss: 0.00002457
Iteration 5/1000 | Loss: 0.00002358
Iteration 6/1000 | Loss: 0.00002302
Iteration 7/1000 | Loss: 0.00002277
Iteration 8/1000 | Loss: 0.00002251
Iteration 9/1000 | Loss: 0.00002248
Iteration 10/1000 | Loss: 0.00002246
Iteration 11/1000 | Loss: 0.00002240
Iteration 12/1000 | Loss: 0.00002232
Iteration 13/1000 | Loss: 0.00002223
Iteration 14/1000 | Loss: 0.00002223
Iteration 15/1000 | Loss: 0.00002212
Iteration 16/1000 | Loss: 0.00002206
Iteration 17/1000 | Loss: 0.00002205
Iteration 18/1000 | Loss: 0.00002205
Iteration 19/1000 | Loss: 0.00002204
Iteration 20/1000 | Loss: 0.00002204
Iteration 21/1000 | Loss: 0.00002204
Iteration 22/1000 | Loss: 0.00002204
Iteration 23/1000 | Loss: 0.00002204
Iteration 24/1000 | Loss: 0.00002203
Iteration 25/1000 | Loss: 0.00002203
Iteration 26/1000 | Loss: 0.00002203
Iteration 27/1000 | Loss: 0.00002202
Iteration 28/1000 | Loss: 0.00002202
Iteration 29/1000 | Loss: 0.00002202
Iteration 30/1000 | Loss: 0.00002202
Iteration 31/1000 | Loss: 0.00002202
Iteration 32/1000 | Loss: 0.00002201
Iteration 33/1000 | Loss: 0.00002201
Iteration 34/1000 | Loss: 0.00002201
Iteration 35/1000 | Loss: 0.00002201
Iteration 36/1000 | Loss: 0.00002201
Iteration 37/1000 | Loss: 0.00002201
Iteration 38/1000 | Loss: 0.00002201
Iteration 39/1000 | Loss: 0.00002201
Iteration 40/1000 | Loss: 0.00002200
Iteration 41/1000 | Loss: 0.00002200
Iteration 42/1000 | Loss: 0.00002200
Iteration 43/1000 | Loss: 0.00002200
Iteration 44/1000 | Loss: 0.00002200
Iteration 45/1000 | Loss: 0.00002200
Iteration 46/1000 | Loss: 0.00002200
Iteration 47/1000 | Loss: 0.00002200
Iteration 48/1000 | Loss: 0.00002200
Iteration 49/1000 | Loss: 0.00002200
Iteration 50/1000 | Loss: 0.00002199
Iteration 51/1000 | Loss: 0.00002199
Iteration 52/1000 | Loss: 0.00002199
Iteration 53/1000 | Loss: 0.00002199
Iteration 54/1000 | Loss: 0.00002199
Iteration 55/1000 | Loss: 0.00002198
Iteration 56/1000 | Loss: 0.00002198
Iteration 57/1000 | Loss: 0.00002198
Iteration 58/1000 | Loss: 0.00002198
Iteration 59/1000 | Loss: 0.00002197
Iteration 60/1000 | Loss: 0.00002197
Iteration 61/1000 | Loss: 0.00002197
Iteration 62/1000 | Loss: 0.00002197
Iteration 63/1000 | Loss: 0.00002197
Iteration 64/1000 | Loss: 0.00002196
Iteration 65/1000 | Loss: 0.00002196
Iteration 66/1000 | Loss: 0.00002196
Iteration 67/1000 | Loss: 0.00002196
Iteration 68/1000 | Loss: 0.00002196
Iteration 69/1000 | Loss: 0.00002195
Iteration 70/1000 | Loss: 0.00002195
Iteration 71/1000 | Loss: 0.00002195
Iteration 72/1000 | Loss: 0.00002195
Iteration 73/1000 | Loss: 0.00002195
Iteration 74/1000 | Loss: 0.00002195
Iteration 75/1000 | Loss: 0.00002195
Iteration 76/1000 | Loss: 0.00002194
Iteration 77/1000 | Loss: 0.00002194
Iteration 78/1000 | Loss: 0.00002194
Iteration 79/1000 | Loss: 0.00002194
Iteration 80/1000 | Loss: 0.00002194
Iteration 81/1000 | Loss: 0.00002194
Iteration 82/1000 | Loss: 0.00002194
Iteration 83/1000 | Loss: 0.00002194
Iteration 84/1000 | Loss: 0.00002194
Iteration 85/1000 | Loss: 0.00002194
Iteration 86/1000 | Loss: 0.00002194
Iteration 87/1000 | Loss: 0.00002194
Iteration 88/1000 | Loss: 0.00002194
Iteration 89/1000 | Loss: 0.00002194
Iteration 90/1000 | Loss: 0.00002193
Iteration 91/1000 | Loss: 0.00002193
Iteration 92/1000 | Loss: 0.00002193
Iteration 93/1000 | Loss: 0.00002193
Iteration 94/1000 | Loss: 0.00002193
Iteration 95/1000 | Loss: 0.00002193
Iteration 96/1000 | Loss: 0.00002193
Iteration 97/1000 | Loss: 0.00002193
Iteration 98/1000 | Loss: 0.00002193
Iteration 99/1000 | Loss: 0.00002193
Iteration 100/1000 | Loss: 0.00002193
Iteration 101/1000 | Loss: 0.00002193
Iteration 102/1000 | Loss: 0.00002193
Iteration 103/1000 | Loss: 0.00002192
Iteration 104/1000 | Loss: 0.00002192
Iteration 105/1000 | Loss: 0.00002192
Iteration 106/1000 | Loss: 0.00002192
Iteration 107/1000 | Loss: 0.00002192
Iteration 108/1000 | Loss: 0.00002192
Iteration 109/1000 | Loss: 0.00002192
Iteration 110/1000 | Loss: 0.00002192
Iteration 111/1000 | Loss: 0.00002192
Iteration 112/1000 | Loss: 0.00002192
Iteration 113/1000 | Loss: 0.00002192
Iteration 114/1000 | Loss: 0.00002192
Iteration 115/1000 | Loss: 0.00002192
Iteration 116/1000 | Loss: 0.00002192
Iteration 117/1000 | Loss: 0.00002192
Iteration 118/1000 | Loss: 0.00002192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.1919460778008215e-05, 2.1919460778008215e-05, 2.1919460778008215e-05, 2.1919460778008215e-05, 2.1919460778008215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1919460778008215e-05

Optimization complete. Final v2v error: 4.175224304199219 mm

Highest mean error: 4.441742420196533 mm for frame 53

Lowest mean error: 3.415705919265747 mm for frame 1

Saving results

Total time: 33.7870397567749
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00536249
Iteration 2/25 | Loss: 0.00136822
Iteration 3/25 | Loss: 0.00106165
Iteration 4/25 | Loss: 0.00101996
Iteration 5/25 | Loss: 0.00100973
Iteration 6/25 | Loss: 0.00100601
Iteration 7/25 | Loss: 0.00100449
Iteration 8/25 | Loss: 0.00100416
Iteration 9/25 | Loss: 0.00100416
Iteration 10/25 | Loss: 0.00100416
Iteration 11/25 | Loss: 0.00100416
Iteration 12/25 | Loss: 0.00100416
Iteration 13/25 | Loss: 0.00100416
Iteration 14/25 | Loss: 0.00100416
Iteration 15/25 | Loss: 0.00100416
Iteration 16/25 | Loss: 0.00100416
Iteration 17/25 | Loss: 0.00100416
Iteration 18/25 | Loss: 0.00100416
Iteration 19/25 | Loss: 0.00100416
Iteration 20/25 | Loss: 0.00100416
Iteration 21/25 | Loss: 0.00100416
Iteration 22/25 | Loss: 0.00100416
Iteration 23/25 | Loss: 0.00100416
Iteration 24/25 | Loss: 0.00100416
Iteration 25/25 | Loss: 0.00100416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48531389
Iteration 2/25 | Loss: 0.00066962
Iteration 3/25 | Loss: 0.00066961
Iteration 4/25 | Loss: 0.00066961
Iteration 5/25 | Loss: 0.00066961
Iteration 6/25 | Loss: 0.00066961
Iteration 7/25 | Loss: 0.00066961
Iteration 8/25 | Loss: 0.00066961
Iteration 9/25 | Loss: 0.00066961
Iteration 10/25 | Loss: 0.00066961
Iteration 11/25 | Loss: 0.00066961
Iteration 12/25 | Loss: 0.00066961
Iteration 13/25 | Loss: 0.00066961
Iteration 14/25 | Loss: 0.00066961
Iteration 15/25 | Loss: 0.00066961
Iteration 16/25 | Loss: 0.00066961
Iteration 17/25 | Loss: 0.00066961
Iteration 18/25 | Loss: 0.00066961
Iteration 19/25 | Loss: 0.00066961
Iteration 20/25 | Loss: 0.00066961
Iteration 21/25 | Loss: 0.00066961
Iteration 22/25 | Loss: 0.00066961
Iteration 23/25 | Loss: 0.00066961
Iteration 24/25 | Loss: 0.00066961
Iteration 25/25 | Loss: 0.00066961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066961
Iteration 2/1000 | Loss: 0.00004211
Iteration 3/1000 | Loss: 0.00002734
Iteration 4/1000 | Loss: 0.00002387
Iteration 5/1000 | Loss: 0.00002298
Iteration 6/1000 | Loss: 0.00002214
Iteration 7/1000 | Loss: 0.00002174
Iteration 8/1000 | Loss: 0.00002135
Iteration 9/1000 | Loss: 0.00002107
Iteration 10/1000 | Loss: 0.00002090
Iteration 11/1000 | Loss: 0.00002074
Iteration 12/1000 | Loss: 0.00002068
Iteration 13/1000 | Loss: 0.00002066
Iteration 14/1000 | Loss: 0.00002060
Iteration 15/1000 | Loss: 0.00002054
Iteration 16/1000 | Loss: 0.00002053
Iteration 17/1000 | Loss: 0.00002051
Iteration 18/1000 | Loss: 0.00002050
Iteration 19/1000 | Loss: 0.00002049
Iteration 20/1000 | Loss: 0.00002049
Iteration 21/1000 | Loss: 0.00002045
Iteration 22/1000 | Loss: 0.00002045
Iteration 23/1000 | Loss: 0.00002044
Iteration 24/1000 | Loss: 0.00002044
Iteration 25/1000 | Loss: 0.00002044
Iteration 26/1000 | Loss: 0.00002043
Iteration 27/1000 | Loss: 0.00002042
Iteration 28/1000 | Loss: 0.00002042
Iteration 29/1000 | Loss: 0.00002042
Iteration 30/1000 | Loss: 0.00002041
Iteration 31/1000 | Loss: 0.00002041
Iteration 32/1000 | Loss: 0.00002041
Iteration 33/1000 | Loss: 0.00002041
Iteration 34/1000 | Loss: 0.00002041
Iteration 35/1000 | Loss: 0.00002041
Iteration 36/1000 | Loss: 0.00002040
Iteration 37/1000 | Loss: 0.00002039
Iteration 38/1000 | Loss: 0.00002039
Iteration 39/1000 | Loss: 0.00002039
Iteration 40/1000 | Loss: 0.00002039
Iteration 41/1000 | Loss: 0.00002039
Iteration 42/1000 | Loss: 0.00002038
Iteration 43/1000 | Loss: 0.00002038
Iteration 44/1000 | Loss: 0.00002038
Iteration 45/1000 | Loss: 0.00002038
Iteration 46/1000 | Loss: 0.00002038
Iteration 47/1000 | Loss: 0.00002038
Iteration 48/1000 | Loss: 0.00002038
Iteration 49/1000 | Loss: 0.00002038
Iteration 50/1000 | Loss: 0.00002037
Iteration 51/1000 | Loss: 0.00002037
Iteration 52/1000 | Loss: 0.00002037
Iteration 53/1000 | Loss: 0.00002037
Iteration 54/1000 | Loss: 0.00002036
Iteration 55/1000 | Loss: 0.00002036
Iteration 56/1000 | Loss: 0.00002036
Iteration 57/1000 | Loss: 0.00002036
Iteration 58/1000 | Loss: 0.00002036
Iteration 59/1000 | Loss: 0.00002036
Iteration 60/1000 | Loss: 0.00002036
Iteration 61/1000 | Loss: 0.00002036
Iteration 62/1000 | Loss: 0.00002036
Iteration 63/1000 | Loss: 0.00002036
Iteration 64/1000 | Loss: 0.00002035
Iteration 65/1000 | Loss: 0.00002035
Iteration 66/1000 | Loss: 0.00002035
Iteration 67/1000 | Loss: 0.00002034
Iteration 68/1000 | Loss: 0.00002034
Iteration 69/1000 | Loss: 0.00002034
Iteration 70/1000 | Loss: 0.00002034
Iteration 71/1000 | Loss: 0.00002034
Iteration 72/1000 | Loss: 0.00002033
Iteration 73/1000 | Loss: 0.00002033
Iteration 74/1000 | Loss: 0.00002033
Iteration 75/1000 | Loss: 0.00002033
Iteration 76/1000 | Loss: 0.00002032
Iteration 77/1000 | Loss: 0.00002032
Iteration 78/1000 | Loss: 0.00002032
Iteration 79/1000 | Loss: 0.00002032
Iteration 80/1000 | Loss: 0.00002031
Iteration 81/1000 | Loss: 0.00002031
Iteration 82/1000 | Loss: 0.00002031
Iteration 83/1000 | Loss: 0.00002031
Iteration 84/1000 | Loss: 0.00002030
Iteration 85/1000 | Loss: 0.00002030
Iteration 86/1000 | Loss: 0.00002030
Iteration 87/1000 | Loss: 0.00002030
Iteration 88/1000 | Loss: 0.00002029
Iteration 89/1000 | Loss: 0.00002029
Iteration 90/1000 | Loss: 0.00002029
Iteration 91/1000 | Loss: 0.00002029
Iteration 92/1000 | Loss: 0.00002029
Iteration 93/1000 | Loss: 0.00002029
Iteration 94/1000 | Loss: 0.00002029
Iteration 95/1000 | Loss: 0.00002029
Iteration 96/1000 | Loss: 0.00002029
Iteration 97/1000 | Loss: 0.00002029
Iteration 98/1000 | Loss: 0.00002028
Iteration 99/1000 | Loss: 0.00002028
Iteration 100/1000 | Loss: 0.00002028
Iteration 101/1000 | Loss: 0.00002028
Iteration 102/1000 | Loss: 0.00002028
Iteration 103/1000 | Loss: 0.00002028
Iteration 104/1000 | Loss: 0.00002027
Iteration 105/1000 | Loss: 0.00002027
Iteration 106/1000 | Loss: 0.00002027
Iteration 107/1000 | Loss: 0.00002027
Iteration 108/1000 | Loss: 0.00002027
Iteration 109/1000 | Loss: 0.00002027
Iteration 110/1000 | Loss: 0.00002027
Iteration 111/1000 | Loss: 0.00002027
Iteration 112/1000 | Loss: 0.00002027
Iteration 113/1000 | Loss: 0.00002026
Iteration 114/1000 | Loss: 0.00002026
Iteration 115/1000 | Loss: 0.00002026
Iteration 116/1000 | Loss: 0.00002026
Iteration 117/1000 | Loss: 0.00002026
Iteration 118/1000 | Loss: 0.00002026
Iteration 119/1000 | Loss: 0.00002026
Iteration 120/1000 | Loss: 0.00002026
Iteration 121/1000 | Loss: 0.00002026
Iteration 122/1000 | Loss: 0.00002026
Iteration 123/1000 | Loss: 0.00002025
Iteration 124/1000 | Loss: 0.00002025
Iteration 125/1000 | Loss: 0.00002025
Iteration 126/1000 | Loss: 0.00002025
Iteration 127/1000 | Loss: 0.00002025
Iteration 128/1000 | Loss: 0.00002025
Iteration 129/1000 | Loss: 0.00002025
Iteration 130/1000 | Loss: 0.00002024
Iteration 131/1000 | Loss: 0.00002024
Iteration 132/1000 | Loss: 0.00002024
Iteration 133/1000 | Loss: 0.00002024
Iteration 134/1000 | Loss: 0.00002024
Iteration 135/1000 | Loss: 0.00002023
Iteration 136/1000 | Loss: 0.00002023
Iteration 137/1000 | Loss: 0.00002023
Iteration 138/1000 | Loss: 0.00002023
Iteration 139/1000 | Loss: 0.00002023
Iteration 140/1000 | Loss: 0.00002023
Iteration 141/1000 | Loss: 0.00002023
Iteration 142/1000 | Loss: 0.00002023
Iteration 143/1000 | Loss: 0.00002023
Iteration 144/1000 | Loss: 0.00002023
Iteration 145/1000 | Loss: 0.00002023
Iteration 146/1000 | Loss: 0.00002023
Iteration 147/1000 | Loss: 0.00002023
Iteration 148/1000 | Loss: 0.00002023
Iteration 149/1000 | Loss: 0.00002022
Iteration 150/1000 | Loss: 0.00002022
Iteration 151/1000 | Loss: 0.00002022
Iteration 152/1000 | Loss: 0.00002022
Iteration 153/1000 | Loss: 0.00002021
Iteration 154/1000 | Loss: 0.00002021
Iteration 155/1000 | Loss: 0.00002021
Iteration 156/1000 | Loss: 0.00002021
Iteration 157/1000 | Loss: 0.00002021
Iteration 158/1000 | Loss: 0.00002021
Iteration 159/1000 | Loss: 0.00002021
Iteration 160/1000 | Loss: 0.00002021
Iteration 161/1000 | Loss: 0.00002020
Iteration 162/1000 | Loss: 0.00002020
Iteration 163/1000 | Loss: 0.00002020
Iteration 164/1000 | Loss: 0.00002020
Iteration 165/1000 | Loss: 0.00002020
Iteration 166/1000 | Loss: 0.00002020
Iteration 167/1000 | Loss: 0.00002020
Iteration 168/1000 | Loss: 0.00002019
Iteration 169/1000 | Loss: 0.00002019
Iteration 170/1000 | Loss: 0.00002019
Iteration 171/1000 | Loss: 0.00002019
Iteration 172/1000 | Loss: 0.00002019
Iteration 173/1000 | Loss: 0.00002019
Iteration 174/1000 | Loss: 0.00002019
Iteration 175/1000 | Loss: 0.00002019
Iteration 176/1000 | Loss: 0.00002019
Iteration 177/1000 | Loss: 0.00002019
Iteration 178/1000 | Loss: 0.00002019
Iteration 179/1000 | Loss: 0.00002019
Iteration 180/1000 | Loss: 0.00002019
Iteration 181/1000 | Loss: 0.00002018
Iteration 182/1000 | Loss: 0.00002018
Iteration 183/1000 | Loss: 0.00002018
Iteration 184/1000 | Loss: 0.00002018
Iteration 185/1000 | Loss: 0.00002018
Iteration 186/1000 | Loss: 0.00002018
Iteration 187/1000 | Loss: 0.00002018
Iteration 188/1000 | Loss: 0.00002018
Iteration 189/1000 | Loss: 0.00002018
Iteration 190/1000 | Loss: 0.00002018
Iteration 191/1000 | Loss: 0.00002018
Iteration 192/1000 | Loss: 0.00002018
Iteration 193/1000 | Loss: 0.00002018
Iteration 194/1000 | Loss: 0.00002017
Iteration 195/1000 | Loss: 0.00002017
Iteration 196/1000 | Loss: 0.00002017
Iteration 197/1000 | Loss: 0.00002017
Iteration 198/1000 | Loss: 0.00002017
Iteration 199/1000 | Loss: 0.00002017
Iteration 200/1000 | Loss: 0.00002017
Iteration 201/1000 | Loss: 0.00002017
Iteration 202/1000 | Loss: 0.00002017
Iteration 203/1000 | Loss: 0.00002017
Iteration 204/1000 | Loss: 0.00002017
Iteration 205/1000 | Loss: 0.00002017
Iteration 206/1000 | Loss: 0.00002017
Iteration 207/1000 | Loss: 0.00002017
Iteration 208/1000 | Loss: 0.00002017
Iteration 209/1000 | Loss: 0.00002016
Iteration 210/1000 | Loss: 0.00002016
Iteration 211/1000 | Loss: 0.00002016
Iteration 212/1000 | Loss: 0.00002016
Iteration 213/1000 | Loss: 0.00002016
Iteration 214/1000 | Loss: 0.00002016
Iteration 215/1000 | Loss: 0.00002016
Iteration 216/1000 | Loss: 0.00002016
Iteration 217/1000 | Loss: 0.00002016
Iteration 218/1000 | Loss: 0.00002016
Iteration 219/1000 | Loss: 0.00002016
Iteration 220/1000 | Loss: 0.00002016
Iteration 221/1000 | Loss: 0.00002015
Iteration 222/1000 | Loss: 0.00002015
Iteration 223/1000 | Loss: 0.00002015
Iteration 224/1000 | Loss: 0.00002015
Iteration 225/1000 | Loss: 0.00002015
Iteration 226/1000 | Loss: 0.00002015
Iteration 227/1000 | Loss: 0.00002015
Iteration 228/1000 | Loss: 0.00002015
Iteration 229/1000 | Loss: 0.00002015
Iteration 230/1000 | Loss: 0.00002015
Iteration 231/1000 | Loss: 0.00002015
Iteration 232/1000 | Loss: 0.00002015
Iteration 233/1000 | Loss: 0.00002015
Iteration 234/1000 | Loss: 0.00002015
Iteration 235/1000 | Loss: 0.00002015
Iteration 236/1000 | Loss: 0.00002014
Iteration 237/1000 | Loss: 0.00002014
Iteration 238/1000 | Loss: 0.00002014
Iteration 239/1000 | Loss: 0.00002014
Iteration 240/1000 | Loss: 0.00002014
Iteration 241/1000 | Loss: 0.00002014
Iteration 242/1000 | Loss: 0.00002014
Iteration 243/1000 | Loss: 0.00002014
Iteration 244/1000 | Loss: 0.00002014
Iteration 245/1000 | Loss: 0.00002014
Iteration 246/1000 | Loss: 0.00002014
Iteration 247/1000 | Loss: 0.00002014
Iteration 248/1000 | Loss: 0.00002014
Iteration 249/1000 | Loss: 0.00002014
Iteration 250/1000 | Loss: 0.00002014
Iteration 251/1000 | Loss: 0.00002014
Iteration 252/1000 | Loss: 0.00002014
Iteration 253/1000 | Loss: 0.00002014
Iteration 254/1000 | Loss: 0.00002014
Iteration 255/1000 | Loss: 0.00002014
Iteration 256/1000 | Loss: 0.00002014
Iteration 257/1000 | Loss: 0.00002014
Iteration 258/1000 | Loss: 0.00002014
Iteration 259/1000 | Loss: 0.00002013
Iteration 260/1000 | Loss: 0.00002013
Iteration 261/1000 | Loss: 0.00002013
Iteration 262/1000 | Loss: 0.00002013
Iteration 263/1000 | Loss: 0.00002013
Iteration 264/1000 | Loss: 0.00002013
Iteration 265/1000 | Loss: 0.00002013
Iteration 266/1000 | Loss: 0.00002013
Iteration 267/1000 | Loss: 0.00002013
Iteration 268/1000 | Loss: 0.00002013
Iteration 269/1000 | Loss: 0.00002013
Iteration 270/1000 | Loss: 0.00002013
Iteration 271/1000 | Loss: 0.00002013
Iteration 272/1000 | Loss: 0.00002013
Iteration 273/1000 | Loss: 0.00002013
Iteration 274/1000 | Loss: 0.00002013
Iteration 275/1000 | Loss: 0.00002013
Iteration 276/1000 | Loss: 0.00002013
Iteration 277/1000 | Loss: 0.00002012
Iteration 278/1000 | Loss: 0.00002012
Iteration 279/1000 | Loss: 0.00002012
Iteration 280/1000 | Loss: 0.00002012
Iteration 281/1000 | Loss: 0.00002012
Iteration 282/1000 | Loss: 0.00002012
Iteration 283/1000 | Loss: 0.00002012
Iteration 284/1000 | Loss: 0.00002012
Iteration 285/1000 | Loss: 0.00002012
Iteration 286/1000 | Loss: 0.00002012
Iteration 287/1000 | Loss: 0.00002012
Iteration 288/1000 | Loss: 0.00002012
Iteration 289/1000 | Loss: 0.00002012
Iteration 290/1000 | Loss: 0.00002012
Iteration 291/1000 | Loss: 0.00002011
Iteration 292/1000 | Loss: 0.00002011
Iteration 293/1000 | Loss: 0.00002011
Iteration 294/1000 | Loss: 0.00002011
Iteration 295/1000 | Loss: 0.00002011
Iteration 296/1000 | Loss: 0.00002011
Iteration 297/1000 | Loss: 0.00002011
Iteration 298/1000 | Loss: 0.00002011
Iteration 299/1000 | Loss: 0.00002011
Iteration 300/1000 | Loss: 0.00002011
Iteration 301/1000 | Loss: 0.00002011
Iteration 302/1000 | Loss: 0.00002011
Iteration 303/1000 | Loss: 0.00002011
Iteration 304/1000 | Loss: 0.00002011
Iteration 305/1000 | Loss: 0.00002011
Iteration 306/1000 | Loss: 0.00002011
Iteration 307/1000 | Loss: 0.00002011
Iteration 308/1000 | Loss: 0.00002011
Iteration 309/1000 | Loss: 0.00002011
Iteration 310/1000 | Loss: 0.00002011
Iteration 311/1000 | Loss: 0.00002011
Iteration 312/1000 | Loss: 0.00002011
Iteration 313/1000 | Loss: 0.00002011
Iteration 314/1000 | Loss: 0.00002011
Iteration 315/1000 | Loss: 0.00002011
Iteration 316/1000 | Loss: 0.00002011
Iteration 317/1000 | Loss: 0.00002011
Iteration 318/1000 | Loss: 0.00002011
Iteration 319/1000 | Loss: 0.00002011
Iteration 320/1000 | Loss: 0.00002011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 320. Stopping optimization.
Last 5 losses: [2.0113320715609007e-05, 2.0113320715609007e-05, 2.0113320715609007e-05, 2.0113320715609007e-05, 2.0113320715609007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0113320715609007e-05

Optimization complete. Final v2v error: 3.6174674034118652 mm

Highest mean error: 5.356834888458252 mm for frame 68

Lowest mean error: 3.0020437240600586 mm for frame 119

Saving results

Total time: 47.21019721031189
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00654432
Iteration 2/25 | Loss: 0.00180252
Iteration 3/25 | Loss: 0.00121218
Iteration 4/25 | Loss: 0.00114743
Iteration 5/25 | Loss: 0.00113354
Iteration 6/25 | Loss: 0.00112729
Iteration 7/25 | Loss: 0.00112637
Iteration 8/25 | Loss: 0.00112621
Iteration 9/25 | Loss: 0.00112621
Iteration 10/25 | Loss: 0.00112621
Iteration 11/25 | Loss: 0.00112621
Iteration 12/25 | Loss: 0.00112621
Iteration 13/25 | Loss: 0.00112621
Iteration 14/25 | Loss: 0.00112621
Iteration 15/25 | Loss: 0.00112621
Iteration 16/25 | Loss: 0.00112621
Iteration 17/25 | Loss: 0.00112621
Iteration 18/25 | Loss: 0.00112621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001126205432228744, 0.001126205432228744, 0.001126205432228744, 0.001126205432228744, 0.001126205432228744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001126205432228744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14127123
Iteration 2/25 | Loss: 0.00068476
Iteration 3/25 | Loss: 0.00068474
Iteration 4/25 | Loss: 0.00068474
Iteration 5/25 | Loss: 0.00068474
Iteration 6/25 | Loss: 0.00068474
Iteration 7/25 | Loss: 0.00068474
Iteration 8/25 | Loss: 0.00068474
Iteration 9/25 | Loss: 0.00068474
Iteration 10/25 | Loss: 0.00068474
Iteration 11/25 | Loss: 0.00068474
Iteration 12/25 | Loss: 0.00068474
Iteration 13/25 | Loss: 0.00068474
Iteration 14/25 | Loss: 0.00068474
Iteration 15/25 | Loss: 0.00068474
Iteration 16/25 | Loss: 0.00068474
Iteration 17/25 | Loss: 0.00068474
Iteration 18/25 | Loss: 0.00068474
Iteration 19/25 | Loss: 0.00068474
Iteration 20/25 | Loss: 0.00068474
Iteration 21/25 | Loss: 0.00068474
Iteration 22/25 | Loss: 0.00068474
Iteration 23/25 | Loss: 0.00068474
Iteration 24/25 | Loss: 0.00068474
Iteration 25/25 | Loss: 0.00068474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068474
Iteration 2/1000 | Loss: 0.00006963
Iteration 3/1000 | Loss: 0.00005482
Iteration 4/1000 | Loss: 0.00005081
Iteration 5/1000 | Loss: 0.00004855
Iteration 6/1000 | Loss: 0.00004734
Iteration 7/1000 | Loss: 0.00004624
Iteration 8/1000 | Loss: 0.00004541
Iteration 9/1000 | Loss: 0.00004487
Iteration 10/1000 | Loss: 0.00004441
Iteration 11/1000 | Loss: 0.00004402
Iteration 12/1000 | Loss: 0.00004378
Iteration 13/1000 | Loss: 0.00004349
Iteration 14/1000 | Loss: 0.00004327
Iteration 15/1000 | Loss: 0.00004309
Iteration 16/1000 | Loss: 0.00004290
Iteration 17/1000 | Loss: 0.00004269
Iteration 18/1000 | Loss: 0.00004250
Iteration 19/1000 | Loss: 0.00004237
Iteration 20/1000 | Loss: 0.00004226
Iteration 21/1000 | Loss: 0.00004223
Iteration 22/1000 | Loss: 0.00004223
Iteration 23/1000 | Loss: 0.00004221
Iteration 24/1000 | Loss: 0.00004220
Iteration 25/1000 | Loss: 0.00004219
Iteration 26/1000 | Loss: 0.00004219
Iteration 27/1000 | Loss: 0.00004218
Iteration 28/1000 | Loss: 0.00004217
Iteration 29/1000 | Loss: 0.00004216
Iteration 30/1000 | Loss: 0.00004215
Iteration 31/1000 | Loss: 0.00004214
Iteration 32/1000 | Loss: 0.00004214
Iteration 33/1000 | Loss: 0.00004212
Iteration 34/1000 | Loss: 0.00004212
Iteration 35/1000 | Loss: 0.00004211
Iteration 36/1000 | Loss: 0.00004211
Iteration 37/1000 | Loss: 0.00004211
Iteration 38/1000 | Loss: 0.00004209
Iteration 39/1000 | Loss: 0.00004209
Iteration 40/1000 | Loss: 0.00004209
Iteration 41/1000 | Loss: 0.00004209
Iteration 42/1000 | Loss: 0.00004208
Iteration 43/1000 | Loss: 0.00004208
Iteration 44/1000 | Loss: 0.00004208
Iteration 45/1000 | Loss: 0.00004208
Iteration 46/1000 | Loss: 0.00004208
Iteration 47/1000 | Loss: 0.00004208
Iteration 48/1000 | Loss: 0.00004208
Iteration 49/1000 | Loss: 0.00004208
Iteration 50/1000 | Loss: 0.00004208
Iteration 51/1000 | Loss: 0.00004207
Iteration 52/1000 | Loss: 0.00004206
Iteration 53/1000 | Loss: 0.00004206
Iteration 54/1000 | Loss: 0.00004205
Iteration 55/1000 | Loss: 0.00004205
Iteration 56/1000 | Loss: 0.00004204
Iteration 57/1000 | Loss: 0.00004204
Iteration 58/1000 | Loss: 0.00004203
Iteration 59/1000 | Loss: 0.00004203
Iteration 60/1000 | Loss: 0.00004201
Iteration 61/1000 | Loss: 0.00004200
Iteration 62/1000 | Loss: 0.00004200
Iteration 63/1000 | Loss: 0.00004200
Iteration 64/1000 | Loss: 0.00004199
Iteration 65/1000 | Loss: 0.00004199
Iteration 66/1000 | Loss: 0.00004199
Iteration 67/1000 | Loss: 0.00004198
Iteration 68/1000 | Loss: 0.00004198
Iteration 69/1000 | Loss: 0.00004198
Iteration 70/1000 | Loss: 0.00004196
Iteration 71/1000 | Loss: 0.00004196
Iteration 72/1000 | Loss: 0.00004195
Iteration 73/1000 | Loss: 0.00004195
Iteration 74/1000 | Loss: 0.00004195
Iteration 75/1000 | Loss: 0.00004195
Iteration 76/1000 | Loss: 0.00004195
Iteration 77/1000 | Loss: 0.00004195
Iteration 78/1000 | Loss: 0.00004195
Iteration 79/1000 | Loss: 0.00004195
Iteration 80/1000 | Loss: 0.00004195
Iteration 81/1000 | Loss: 0.00004195
Iteration 82/1000 | Loss: 0.00004195
Iteration 83/1000 | Loss: 0.00004194
Iteration 84/1000 | Loss: 0.00004194
Iteration 85/1000 | Loss: 0.00004193
Iteration 86/1000 | Loss: 0.00004193
Iteration 87/1000 | Loss: 0.00004193
Iteration 88/1000 | Loss: 0.00004193
Iteration 89/1000 | Loss: 0.00004193
Iteration 90/1000 | Loss: 0.00004192
Iteration 91/1000 | Loss: 0.00004192
Iteration 92/1000 | Loss: 0.00004192
Iteration 93/1000 | Loss: 0.00004192
Iteration 94/1000 | Loss: 0.00004191
Iteration 95/1000 | Loss: 0.00004191
Iteration 96/1000 | Loss: 0.00004191
Iteration 97/1000 | Loss: 0.00004191
Iteration 98/1000 | Loss: 0.00004190
Iteration 99/1000 | Loss: 0.00004190
Iteration 100/1000 | Loss: 0.00004190
Iteration 101/1000 | Loss: 0.00004190
Iteration 102/1000 | Loss: 0.00004190
Iteration 103/1000 | Loss: 0.00004190
Iteration 104/1000 | Loss: 0.00004190
Iteration 105/1000 | Loss: 0.00004189
Iteration 106/1000 | Loss: 0.00004189
Iteration 107/1000 | Loss: 0.00004189
Iteration 108/1000 | Loss: 0.00004189
Iteration 109/1000 | Loss: 0.00004189
Iteration 110/1000 | Loss: 0.00004189
Iteration 111/1000 | Loss: 0.00004189
Iteration 112/1000 | Loss: 0.00004189
Iteration 113/1000 | Loss: 0.00004188
Iteration 114/1000 | Loss: 0.00004188
Iteration 115/1000 | Loss: 0.00004188
Iteration 116/1000 | Loss: 0.00004188
Iteration 117/1000 | Loss: 0.00004188
Iteration 118/1000 | Loss: 0.00004188
Iteration 119/1000 | Loss: 0.00004188
Iteration 120/1000 | Loss: 0.00004188
Iteration 121/1000 | Loss: 0.00004188
Iteration 122/1000 | Loss: 0.00004188
Iteration 123/1000 | Loss: 0.00004188
Iteration 124/1000 | Loss: 0.00004188
Iteration 125/1000 | Loss: 0.00004188
Iteration 126/1000 | Loss: 0.00004188
Iteration 127/1000 | Loss: 0.00004188
Iteration 128/1000 | Loss: 0.00004188
Iteration 129/1000 | Loss: 0.00004188
Iteration 130/1000 | Loss: 0.00004188
Iteration 131/1000 | Loss: 0.00004188
Iteration 132/1000 | Loss: 0.00004188
Iteration 133/1000 | Loss: 0.00004188
Iteration 134/1000 | Loss: 0.00004188
Iteration 135/1000 | Loss: 0.00004188
Iteration 136/1000 | Loss: 0.00004188
Iteration 137/1000 | Loss: 0.00004188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [4.188102320767939e-05, 4.188102320767939e-05, 4.188102320767939e-05, 4.188102320767939e-05, 4.188102320767939e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.188102320767939e-05

Optimization complete. Final v2v error: 5.20461893081665 mm

Highest mean error: 6.414395809173584 mm for frame 151

Lowest mean error: 3.9580893516540527 mm for frame 0

Saving results

Total time: 48.710824966430664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_2351/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_2351/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00641376
Iteration 2/25 | Loss: 0.00118360
Iteration 3/25 | Loss: 0.00105462
Iteration 4/25 | Loss: 0.00103093
Iteration 5/25 | Loss: 0.00102022
Iteration 6/25 | Loss: 0.00101790
Iteration 7/25 | Loss: 0.00101738
Iteration 8/25 | Loss: 0.00101738
Iteration 9/25 | Loss: 0.00101738
Iteration 10/25 | Loss: 0.00101738
Iteration 11/25 | Loss: 0.00101738
Iteration 12/25 | Loss: 0.00101738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010173844639211893, 0.0010173844639211893, 0.0010173844639211893, 0.0010173844639211893, 0.0010173844639211893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010173844639211893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.52512312
Iteration 2/25 | Loss: 0.00082966
Iteration 3/25 | Loss: 0.00082963
Iteration 4/25 | Loss: 0.00082963
Iteration 5/25 | Loss: 0.00082963
Iteration 6/25 | Loss: 0.00082963
Iteration 7/25 | Loss: 0.00082963
Iteration 8/25 | Loss: 0.00082963
Iteration 9/25 | Loss: 0.00082963
Iteration 10/25 | Loss: 0.00082963
Iteration 11/25 | Loss: 0.00082963
Iteration 12/25 | Loss: 0.00082963
Iteration 13/25 | Loss: 0.00082963
Iteration 14/25 | Loss: 0.00082963
Iteration 15/25 | Loss: 0.00082963
Iteration 16/25 | Loss: 0.00082963
Iteration 17/25 | Loss: 0.00082963
Iteration 18/25 | Loss: 0.00082963
Iteration 19/25 | Loss: 0.00082963
Iteration 20/25 | Loss: 0.00082963
Iteration 21/25 | Loss: 0.00082963
Iteration 22/25 | Loss: 0.00082963
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008296284358948469, 0.0008296284358948469, 0.0008296284358948469, 0.0008296284358948469, 0.0008296284358948469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008296284358948469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082963
Iteration 2/1000 | Loss: 0.00004630
Iteration 3/1000 | Loss: 0.00002491
Iteration 4/1000 | Loss: 0.00002216
Iteration 5/1000 | Loss: 0.00002073
Iteration 6/1000 | Loss: 0.00001976
Iteration 7/1000 | Loss: 0.00001932
Iteration 8/1000 | Loss: 0.00001903
Iteration 9/1000 | Loss: 0.00001887
Iteration 10/1000 | Loss: 0.00001885
Iteration 11/1000 | Loss: 0.00001883
Iteration 12/1000 | Loss: 0.00001877
Iteration 13/1000 | Loss: 0.00001870
Iteration 14/1000 | Loss: 0.00001867
Iteration 15/1000 | Loss: 0.00001860
Iteration 16/1000 | Loss: 0.00001855
Iteration 17/1000 | Loss: 0.00001854
Iteration 18/1000 | Loss: 0.00001854
Iteration 19/1000 | Loss: 0.00001853
Iteration 20/1000 | Loss: 0.00001853
Iteration 21/1000 | Loss: 0.00001852
Iteration 22/1000 | Loss: 0.00001852
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001851
Iteration 25/1000 | Loss: 0.00001851
Iteration 26/1000 | Loss: 0.00001851
Iteration 27/1000 | Loss: 0.00001850
Iteration 28/1000 | Loss: 0.00001850
Iteration 29/1000 | Loss: 0.00001850
Iteration 30/1000 | Loss: 0.00001849
Iteration 31/1000 | Loss: 0.00001849
Iteration 32/1000 | Loss: 0.00001848
Iteration 33/1000 | Loss: 0.00001848
Iteration 34/1000 | Loss: 0.00001848
Iteration 35/1000 | Loss: 0.00001848
Iteration 36/1000 | Loss: 0.00001848
Iteration 37/1000 | Loss: 0.00001848
Iteration 38/1000 | Loss: 0.00001848
Iteration 39/1000 | Loss: 0.00001848
Iteration 40/1000 | Loss: 0.00001848
Iteration 41/1000 | Loss: 0.00001847
Iteration 42/1000 | Loss: 0.00001847
Iteration 43/1000 | Loss: 0.00001847
Iteration 44/1000 | Loss: 0.00001846
Iteration 45/1000 | Loss: 0.00001846
Iteration 46/1000 | Loss: 0.00001846
Iteration 47/1000 | Loss: 0.00001846
Iteration 48/1000 | Loss: 0.00001845
Iteration 49/1000 | Loss: 0.00001845
Iteration 50/1000 | Loss: 0.00001845
Iteration 51/1000 | Loss: 0.00001845
Iteration 52/1000 | Loss: 0.00001845
Iteration 53/1000 | Loss: 0.00001845
Iteration 54/1000 | Loss: 0.00001844
Iteration 55/1000 | Loss: 0.00001844
Iteration 56/1000 | Loss: 0.00001844
Iteration 57/1000 | Loss: 0.00001844
Iteration 58/1000 | Loss: 0.00001844
Iteration 59/1000 | Loss: 0.00001844
Iteration 60/1000 | Loss: 0.00001844
Iteration 61/1000 | Loss: 0.00001844
Iteration 62/1000 | Loss: 0.00001844
Iteration 63/1000 | Loss: 0.00001844
Iteration 64/1000 | Loss: 0.00001843
Iteration 65/1000 | Loss: 0.00001843
Iteration 66/1000 | Loss: 0.00001843
Iteration 67/1000 | Loss: 0.00001843
Iteration 68/1000 | Loss: 0.00001843
Iteration 69/1000 | Loss: 0.00001843
Iteration 70/1000 | Loss: 0.00001843
Iteration 71/1000 | Loss: 0.00001843
Iteration 72/1000 | Loss: 0.00001843
Iteration 73/1000 | Loss: 0.00001843
Iteration 74/1000 | Loss: 0.00001842
Iteration 75/1000 | Loss: 0.00001842
Iteration 76/1000 | Loss: 0.00001842
Iteration 77/1000 | Loss: 0.00001842
Iteration 78/1000 | Loss: 0.00001842
Iteration 79/1000 | Loss: 0.00001842
Iteration 80/1000 | Loss: 0.00001842
Iteration 81/1000 | Loss: 0.00001842
Iteration 82/1000 | Loss: 0.00001842
Iteration 83/1000 | Loss: 0.00001841
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001841
Iteration 88/1000 | Loss: 0.00001841
Iteration 89/1000 | Loss: 0.00001841
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001841
Iteration 92/1000 | Loss: 0.00001841
Iteration 93/1000 | Loss: 0.00001840
Iteration 94/1000 | Loss: 0.00001840
Iteration 95/1000 | Loss: 0.00001840
Iteration 96/1000 | Loss: 0.00001840
Iteration 97/1000 | Loss: 0.00001840
Iteration 98/1000 | Loss: 0.00001840
Iteration 99/1000 | Loss: 0.00001840
Iteration 100/1000 | Loss: 0.00001840
Iteration 101/1000 | Loss: 0.00001840
Iteration 102/1000 | Loss: 0.00001840
Iteration 103/1000 | Loss: 0.00001840
Iteration 104/1000 | Loss: 0.00001840
Iteration 105/1000 | Loss: 0.00001840
Iteration 106/1000 | Loss: 0.00001840
Iteration 107/1000 | Loss: 0.00001840
Iteration 108/1000 | Loss: 0.00001840
Iteration 109/1000 | Loss: 0.00001840
Iteration 110/1000 | Loss: 0.00001840
Iteration 111/1000 | Loss: 0.00001840
Iteration 112/1000 | Loss: 0.00001840
Iteration 113/1000 | Loss: 0.00001840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.839889955590479e-05, 1.839889955590479e-05, 1.839889955590479e-05, 1.839889955590479e-05, 1.839889955590479e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.839889955590479e-05

Optimization complete. Final v2v error: 3.7719571590423584 mm

Highest mean error: 4.454442501068115 mm for frame 154

Lowest mean error: 3.4425814151763916 mm for frame 86

Saving results

Total time: 31.718157052993774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495630
Iteration 2/25 | Loss: 0.00111030
Iteration 3/25 | Loss: 0.00101800
Iteration 4/25 | Loss: 0.00100577
Iteration 5/25 | Loss: 0.00100249
Iteration 6/25 | Loss: 0.00100166
Iteration 7/25 | Loss: 0.00100166
Iteration 8/25 | Loss: 0.00100166
Iteration 9/25 | Loss: 0.00100166
Iteration 10/25 | Loss: 0.00100166
Iteration 11/25 | Loss: 0.00100166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001001655007712543, 0.001001655007712543, 0.001001655007712543, 0.001001655007712543, 0.001001655007712543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001001655007712543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29163563
Iteration 2/25 | Loss: 0.00075036
Iteration 3/25 | Loss: 0.00075034
Iteration 4/25 | Loss: 0.00075033
Iteration 5/25 | Loss: 0.00075033
Iteration 6/25 | Loss: 0.00075033
Iteration 7/25 | Loss: 0.00075033
Iteration 8/25 | Loss: 0.00075033
Iteration 9/25 | Loss: 0.00075033
Iteration 10/25 | Loss: 0.00075033
Iteration 11/25 | Loss: 0.00075033
Iteration 12/25 | Loss: 0.00075033
Iteration 13/25 | Loss: 0.00075033
Iteration 14/25 | Loss: 0.00075033
Iteration 15/25 | Loss: 0.00075033
Iteration 16/25 | Loss: 0.00075033
Iteration 17/25 | Loss: 0.00075033
Iteration 18/25 | Loss: 0.00075033
Iteration 19/25 | Loss: 0.00075033
Iteration 20/25 | Loss: 0.00075033
Iteration 21/25 | Loss: 0.00075033
Iteration 22/25 | Loss: 0.00075033
Iteration 23/25 | Loss: 0.00075033
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007503321976400912, 0.0007503321976400912, 0.0007503321976400912, 0.0007503321976400912, 0.0007503321976400912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007503321976400912

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075033
Iteration 2/1000 | Loss: 0.00002608
Iteration 3/1000 | Loss: 0.00001886
Iteration 4/1000 | Loss: 0.00001714
Iteration 5/1000 | Loss: 0.00001614
Iteration 6/1000 | Loss: 0.00001535
Iteration 7/1000 | Loss: 0.00001502
Iteration 8/1000 | Loss: 0.00001469
Iteration 9/1000 | Loss: 0.00001449
Iteration 10/1000 | Loss: 0.00001436
Iteration 11/1000 | Loss: 0.00001435
Iteration 12/1000 | Loss: 0.00001434
Iteration 13/1000 | Loss: 0.00001429
Iteration 14/1000 | Loss: 0.00001427
Iteration 15/1000 | Loss: 0.00001426
Iteration 16/1000 | Loss: 0.00001425
Iteration 17/1000 | Loss: 0.00001425
Iteration 18/1000 | Loss: 0.00001421
Iteration 19/1000 | Loss: 0.00001420
Iteration 20/1000 | Loss: 0.00001419
Iteration 21/1000 | Loss: 0.00001419
Iteration 22/1000 | Loss: 0.00001419
Iteration 23/1000 | Loss: 0.00001418
Iteration 24/1000 | Loss: 0.00001418
Iteration 25/1000 | Loss: 0.00001417
Iteration 26/1000 | Loss: 0.00001417
Iteration 27/1000 | Loss: 0.00001416
Iteration 28/1000 | Loss: 0.00001416
Iteration 29/1000 | Loss: 0.00001415
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001415
Iteration 32/1000 | Loss: 0.00001412
Iteration 33/1000 | Loss: 0.00001412
Iteration 34/1000 | Loss: 0.00001411
Iteration 35/1000 | Loss: 0.00001411
Iteration 36/1000 | Loss: 0.00001410
Iteration 37/1000 | Loss: 0.00001410
Iteration 38/1000 | Loss: 0.00001410
Iteration 39/1000 | Loss: 0.00001410
Iteration 40/1000 | Loss: 0.00001410
Iteration 41/1000 | Loss: 0.00001410
Iteration 42/1000 | Loss: 0.00001409
Iteration 43/1000 | Loss: 0.00001409
Iteration 44/1000 | Loss: 0.00001409
Iteration 45/1000 | Loss: 0.00001408
Iteration 46/1000 | Loss: 0.00001408
Iteration 47/1000 | Loss: 0.00001407
Iteration 48/1000 | Loss: 0.00001407
Iteration 49/1000 | Loss: 0.00001407
Iteration 50/1000 | Loss: 0.00001407
Iteration 51/1000 | Loss: 0.00001407
Iteration 52/1000 | Loss: 0.00001407
Iteration 53/1000 | Loss: 0.00001407
Iteration 54/1000 | Loss: 0.00001407
Iteration 55/1000 | Loss: 0.00001407
Iteration 56/1000 | Loss: 0.00001407
Iteration 57/1000 | Loss: 0.00001406
Iteration 58/1000 | Loss: 0.00001406
Iteration 59/1000 | Loss: 0.00001406
Iteration 60/1000 | Loss: 0.00001406
Iteration 61/1000 | Loss: 0.00001406
Iteration 62/1000 | Loss: 0.00001406
Iteration 63/1000 | Loss: 0.00001406
Iteration 64/1000 | Loss: 0.00001406
Iteration 65/1000 | Loss: 0.00001406
Iteration 66/1000 | Loss: 0.00001406
Iteration 67/1000 | Loss: 0.00001405
Iteration 68/1000 | Loss: 0.00001405
Iteration 69/1000 | Loss: 0.00001405
Iteration 70/1000 | Loss: 0.00001405
Iteration 71/1000 | Loss: 0.00001405
Iteration 72/1000 | Loss: 0.00001405
Iteration 73/1000 | Loss: 0.00001405
Iteration 74/1000 | Loss: 0.00001405
Iteration 75/1000 | Loss: 0.00001405
Iteration 76/1000 | Loss: 0.00001405
Iteration 77/1000 | Loss: 0.00001405
Iteration 78/1000 | Loss: 0.00001405
Iteration 79/1000 | Loss: 0.00001405
Iteration 80/1000 | Loss: 0.00001405
Iteration 81/1000 | Loss: 0.00001405
Iteration 82/1000 | Loss: 0.00001405
Iteration 83/1000 | Loss: 0.00001405
Iteration 84/1000 | Loss: 0.00001405
Iteration 85/1000 | Loss: 0.00001405
Iteration 86/1000 | Loss: 0.00001405
Iteration 87/1000 | Loss: 0.00001405
Iteration 88/1000 | Loss: 0.00001405
Iteration 89/1000 | Loss: 0.00001405
Iteration 90/1000 | Loss: 0.00001405
Iteration 91/1000 | Loss: 0.00001405
Iteration 92/1000 | Loss: 0.00001405
Iteration 93/1000 | Loss: 0.00001405
Iteration 94/1000 | Loss: 0.00001405
Iteration 95/1000 | Loss: 0.00001405
Iteration 96/1000 | Loss: 0.00001405
Iteration 97/1000 | Loss: 0.00001405
Iteration 98/1000 | Loss: 0.00001405
Iteration 99/1000 | Loss: 0.00001405
Iteration 100/1000 | Loss: 0.00001405
Iteration 101/1000 | Loss: 0.00001405
Iteration 102/1000 | Loss: 0.00001405
Iteration 103/1000 | Loss: 0.00001405
Iteration 104/1000 | Loss: 0.00001405
Iteration 105/1000 | Loss: 0.00001405
Iteration 106/1000 | Loss: 0.00001405
Iteration 107/1000 | Loss: 0.00001405
Iteration 108/1000 | Loss: 0.00001405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.4046220712771174e-05, 1.4046220712771174e-05, 1.4046220712771174e-05, 1.4046220712771174e-05, 1.4046220712771174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4046220712771174e-05

Optimization complete. Final v2v error: 3.12677001953125 mm

Highest mean error: 3.4729881286621094 mm for frame 101

Lowest mean error: 2.649973154067993 mm for frame 12

Saving results

Total time: 29.317564487457275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840547
Iteration 2/25 | Loss: 0.00136880
Iteration 3/25 | Loss: 0.00111676
Iteration 4/25 | Loss: 0.00107943
Iteration 5/25 | Loss: 0.00105199
Iteration 6/25 | Loss: 0.00104587
Iteration 7/25 | Loss: 0.00103280
Iteration 8/25 | Loss: 0.00102897
Iteration 9/25 | Loss: 0.00102756
Iteration 10/25 | Loss: 0.00102685
Iteration 11/25 | Loss: 0.00102647
Iteration 12/25 | Loss: 0.00103411
Iteration 13/25 | Loss: 0.00102664
Iteration 14/25 | Loss: 0.00102086
Iteration 15/25 | Loss: 0.00101973
Iteration 16/25 | Loss: 0.00101929
Iteration 17/25 | Loss: 0.00101916
Iteration 18/25 | Loss: 0.00101908
Iteration 19/25 | Loss: 0.00101907
Iteration 20/25 | Loss: 0.00101907
Iteration 21/25 | Loss: 0.00101907
Iteration 22/25 | Loss: 0.00101907
Iteration 23/25 | Loss: 0.00101907
Iteration 24/25 | Loss: 0.00101907
Iteration 25/25 | Loss: 0.00101906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75202715
Iteration 2/25 | Loss: 0.00095479
Iteration 3/25 | Loss: 0.00095477
Iteration 4/25 | Loss: 0.00095477
Iteration 5/25 | Loss: 0.00095477
Iteration 6/25 | Loss: 0.00095476
Iteration 7/25 | Loss: 0.00095476
Iteration 8/25 | Loss: 0.00095476
Iteration 9/25 | Loss: 0.00095476
Iteration 10/25 | Loss: 0.00095476
Iteration 11/25 | Loss: 0.00095476
Iteration 12/25 | Loss: 0.00095476
Iteration 13/25 | Loss: 0.00095476
Iteration 14/25 | Loss: 0.00095476
Iteration 15/25 | Loss: 0.00095476
Iteration 16/25 | Loss: 0.00095476
Iteration 17/25 | Loss: 0.00095476
Iteration 18/25 | Loss: 0.00095476
Iteration 19/25 | Loss: 0.00095476
Iteration 20/25 | Loss: 0.00095476
Iteration 21/25 | Loss: 0.00095476
Iteration 22/25 | Loss: 0.00095476
Iteration 23/25 | Loss: 0.00095476
Iteration 24/25 | Loss: 0.00095476
Iteration 25/25 | Loss: 0.00095476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095476
Iteration 2/1000 | Loss: 0.00004927
Iteration 3/1000 | Loss: 0.00002934
Iteration 4/1000 | Loss: 0.00002436
Iteration 5/1000 | Loss: 0.00007917
Iteration 6/1000 | Loss: 0.00007238
Iteration 7/1000 | Loss: 0.00002141
Iteration 8/1000 | Loss: 0.00002064
Iteration 9/1000 | Loss: 0.00002132
Iteration 10/1000 | Loss: 0.00001935
Iteration 11/1000 | Loss: 0.00001884
Iteration 12/1000 | Loss: 0.00001840
Iteration 13/1000 | Loss: 0.00001810
Iteration 14/1000 | Loss: 0.00005304
Iteration 15/1000 | Loss: 0.00002418
Iteration 16/1000 | Loss: 0.00002043
Iteration 17/1000 | Loss: 0.00001796
Iteration 18/1000 | Loss: 0.00001776
Iteration 19/1000 | Loss: 0.00001772
Iteration 20/1000 | Loss: 0.00001772
Iteration 21/1000 | Loss: 0.00001771
Iteration 22/1000 | Loss: 0.00001770
Iteration 23/1000 | Loss: 0.00001770
Iteration 24/1000 | Loss: 0.00001768
Iteration 25/1000 | Loss: 0.00001767
Iteration 26/1000 | Loss: 0.00001767
Iteration 27/1000 | Loss: 0.00001762
Iteration 28/1000 | Loss: 0.00001760
Iteration 29/1000 | Loss: 0.00004673
Iteration 30/1000 | Loss: 0.00001757
Iteration 31/1000 | Loss: 0.00001757
Iteration 32/1000 | Loss: 0.00001754
Iteration 33/1000 | Loss: 0.00001754
Iteration 34/1000 | Loss: 0.00002858
Iteration 35/1000 | Loss: 0.00001759
Iteration 36/1000 | Loss: 0.00001753
Iteration 37/1000 | Loss: 0.00001753
Iteration 38/1000 | Loss: 0.00001753
Iteration 39/1000 | Loss: 0.00001753
Iteration 40/1000 | Loss: 0.00001753
Iteration 41/1000 | Loss: 0.00001753
Iteration 42/1000 | Loss: 0.00001753
Iteration 43/1000 | Loss: 0.00001753
Iteration 44/1000 | Loss: 0.00001753
Iteration 45/1000 | Loss: 0.00001752
Iteration 46/1000 | Loss: 0.00001752
Iteration 47/1000 | Loss: 0.00001752
Iteration 48/1000 | Loss: 0.00001752
Iteration 49/1000 | Loss: 0.00001752
Iteration 50/1000 | Loss: 0.00001752
Iteration 51/1000 | Loss: 0.00001752
Iteration 52/1000 | Loss: 0.00001751
Iteration 53/1000 | Loss: 0.00001751
Iteration 54/1000 | Loss: 0.00001751
Iteration 55/1000 | Loss: 0.00001750
Iteration 56/1000 | Loss: 0.00001748
Iteration 57/1000 | Loss: 0.00001748
Iteration 58/1000 | Loss: 0.00001748
Iteration 59/1000 | Loss: 0.00001748
Iteration 60/1000 | Loss: 0.00001748
Iteration 61/1000 | Loss: 0.00001747
Iteration 62/1000 | Loss: 0.00001747
Iteration 63/1000 | Loss: 0.00001747
Iteration 64/1000 | Loss: 0.00001747
Iteration 65/1000 | Loss: 0.00001747
Iteration 66/1000 | Loss: 0.00001746
Iteration 67/1000 | Loss: 0.00001746
Iteration 68/1000 | Loss: 0.00001746
Iteration 69/1000 | Loss: 0.00001746
Iteration 70/1000 | Loss: 0.00001746
Iteration 71/1000 | Loss: 0.00001746
Iteration 72/1000 | Loss: 0.00001746
Iteration 73/1000 | Loss: 0.00001745
Iteration 74/1000 | Loss: 0.00001745
Iteration 75/1000 | Loss: 0.00001745
Iteration 76/1000 | Loss: 0.00001745
Iteration 77/1000 | Loss: 0.00001745
Iteration 78/1000 | Loss: 0.00001745
Iteration 79/1000 | Loss: 0.00001745
Iteration 80/1000 | Loss: 0.00001744
Iteration 81/1000 | Loss: 0.00001744
Iteration 82/1000 | Loss: 0.00001744
Iteration 83/1000 | Loss: 0.00001744
Iteration 84/1000 | Loss: 0.00001743
Iteration 85/1000 | Loss: 0.00001743
Iteration 86/1000 | Loss: 0.00001743
Iteration 87/1000 | Loss: 0.00001743
Iteration 88/1000 | Loss: 0.00001743
Iteration 89/1000 | Loss: 0.00001743
Iteration 90/1000 | Loss: 0.00001743
Iteration 91/1000 | Loss: 0.00001743
Iteration 92/1000 | Loss: 0.00001743
Iteration 93/1000 | Loss: 0.00001743
Iteration 94/1000 | Loss: 0.00001742
Iteration 95/1000 | Loss: 0.00003746
Iteration 96/1000 | Loss: 0.00003746
Iteration 97/1000 | Loss: 0.00001994
Iteration 98/1000 | Loss: 0.00001861
Iteration 99/1000 | Loss: 0.00002486
Iteration 100/1000 | Loss: 0.00001782
Iteration 101/1000 | Loss: 0.00001766
Iteration 102/1000 | Loss: 0.00001766
Iteration 103/1000 | Loss: 0.00001744
Iteration 104/1000 | Loss: 0.00001744
Iteration 105/1000 | Loss: 0.00001744
Iteration 106/1000 | Loss: 0.00001743
Iteration 107/1000 | Loss: 0.00001743
Iteration 108/1000 | Loss: 0.00001743
Iteration 109/1000 | Loss: 0.00001743
Iteration 110/1000 | Loss: 0.00001743
Iteration 111/1000 | Loss: 0.00001743
Iteration 112/1000 | Loss: 0.00001743
Iteration 113/1000 | Loss: 0.00001743
Iteration 114/1000 | Loss: 0.00001743
Iteration 115/1000 | Loss: 0.00001743
Iteration 116/1000 | Loss: 0.00001743
Iteration 117/1000 | Loss: 0.00001742
Iteration 118/1000 | Loss: 0.00001742
Iteration 119/1000 | Loss: 0.00001742
Iteration 120/1000 | Loss: 0.00001742
Iteration 121/1000 | Loss: 0.00001742
Iteration 122/1000 | Loss: 0.00001742
Iteration 123/1000 | Loss: 0.00001742
Iteration 124/1000 | Loss: 0.00001742
Iteration 125/1000 | Loss: 0.00001742
Iteration 126/1000 | Loss: 0.00001742
Iteration 127/1000 | Loss: 0.00001742
Iteration 128/1000 | Loss: 0.00001742
Iteration 129/1000 | Loss: 0.00001742
Iteration 130/1000 | Loss: 0.00001742
Iteration 131/1000 | Loss: 0.00001742
Iteration 132/1000 | Loss: 0.00001742
Iteration 133/1000 | Loss: 0.00001742
Iteration 134/1000 | Loss: 0.00001742
Iteration 135/1000 | Loss: 0.00001742
Iteration 136/1000 | Loss: 0.00001742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.742374115565326e-05, 1.742374115565326e-05, 1.742374115565326e-05, 1.742374115565326e-05, 1.742374115565326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.742374115565326e-05

Optimization complete. Final v2v error: 3.1764042377471924 mm

Highest mean error: 13.129666328430176 mm for frame 48

Lowest mean error: 2.8129665851593018 mm for frame 129

Saving results

Total time: 71.2221360206604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451611
Iteration 2/25 | Loss: 0.00115021
Iteration 3/25 | Loss: 0.00101524
Iteration 4/25 | Loss: 0.00100771
Iteration 5/25 | Loss: 0.00100525
Iteration 6/25 | Loss: 0.00100482
Iteration 7/25 | Loss: 0.00100482
Iteration 8/25 | Loss: 0.00100482
Iteration 9/25 | Loss: 0.00100482
Iteration 10/25 | Loss: 0.00100482
Iteration 11/25 | Loss: 0.00100482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010048223193734884, 0.0010048223193734884, 0.0010048223193734884, 0.0010048223193734884, 0.0010048223193734884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010048223193734884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30541384
Iteration 2/25 | Loss: 0.00092164
Iteration 3/25 | Loss: 0.00092164
Iteration 4/25 | Loss: 0.00092164
Iteration 5/25 | Loss: 0.00092164
Iteration 6/25 | Loss: 0.00092164
Iteration 7/25 | Loss: 0.00092164
Iteration 8/25 | Loss: 0.00092164
Iteration 9/25 | Loss: 0.00092164
Iteration 10/25 | Loss: 0.00092164
Iteration 11/25 | Loss: 0.00092164
Iteration 12/25 | Loss: 0.00092164
Iteration 13/25 | Loss: 0.00092164
Iteration 14/25 | Loss: 0.00092164
Iteration 15/25 | Loss: 0.00092164
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009216350154019892, 0.0009216350154019892, 0.0009216350154019892, 0.0009216350154019892, 0.0009216350154019892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009216350154019892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092164
Iteration 2/1000 | Loss: 0.00002426
Iteration 3/1000 | Loss: 0.00001414
Iteration 4/1000 | Loss: 0.00001262
Iteration 5/1000 | Loss: 0.00001199
Iteration 6/1000 | Loss: 0.00001184
Iteration 7/1000 | Loss: 0.00001165
Iteration 8/1000 | Loss: 0.00001152
Iteration 9/1000 | Loss: 0.00001151
Iteration 10/1000 | Loss: 0.00001138
Iteration 11/1000 | Loss: 0.00001133
Iteration 12/1000 | Loss: 0.00001132
Iteration 13/1000 | Loss: 0.00001130
Iteration 14/1000 | Loss: 0.00001129
Iteration 15/1000 | Loss: 0.00001118
Iteration 16/1000 | Loss: 0.00001106
Iteration 17/1000 | Loss: 0.00001103
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001102
Iteration 20/1000 | Loss: 0.00001100
Iteration 21/1000 | Loss: 0.00001099
Iteration 22/1000 | Loss: 0.00001098
Iteration 23/1000 | Loss: 0.00001097
Iteration 24/1000 | Loss: 0.00001097
Iteration 25/1000 | Loss: 0.00001096
Iteration 26/1000 | Loss: 0.00001095
Iteration 27/1000 | Loss: 0.00001094
Iteration 28/1000 | Loss: 0.00001094
Iteration 29/1000 | Loss: 0.00001093
Iteration 30/1000 | Loss: 0.00001092
Iteration 31/1000 | Loss: 0.00001092
Iteration 32/1000 | Loss: 0.00001092
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001091
Iteration 35/1000 | Loss: 0.00001091
Iteration 36/1000 | Loss: 0.00001090
Iteration 37/1000 | Loss: 0.00001090
Iteration 38/1000 | Loss: 0.00001088
Iteration 39/1000 | Loss: 0.00001088
Iteration 40/1000 | Loss: 0.00001087
Iteration 41/1000 | Loss: 0.00001087
Iteration 42/1000 | Loss: 0.00001086
Iteration 43/1000 | Loss: 0.00001086
Iteration 44/1000 | Loss: 0.00001085
Iteration 45/1000 | Loss: 0.00001085
Iteration 46/1000 | Loss: 0.00001085
Iteration 47/1000 | Loss: 0.00001084
Iteration 48/1000 | Loss: 0.00001084
Iteration 49/1000 | Loss: 0.00001084
Iteration 50/1000 | Loss: 0.00001083
Iteration 51/1000 | Loss: 0.00001083
Iteration 52/1000 | Loss: 0.00001082
Iteration 53/1000 | Loss: 0.00001082
Iteration 54/1000 | Loss: 0.00001082
Iteration 55/1000 | Loss: 0.00001082
Iteration 56/1000 | Loss: 0.00001082
Iteration 57/1000 | Loss: 0.00001081
Iteration 58/1000 | Loss: 0.00001081
Iteration 59/1000 | Loss: 0.00001081
Iteration 60/1000 | Loss: 0.00001081
Iteration 61/1000 | Loss: 0.00001081
Iteration 62/1000 | Loss: 0.00001081
Iteration 63/1000 | Loss: 0.00001081
Iteration 64/1000 | Loss: 0.00001081
Iteration 65/1000 | Loss: 0.00001080
Iteration 66/1000 | Loss: 0.00001080
Iteration 67/1000 | Loss: 0.00001080
Iteration 68/1000 | Loss: 0.00001080
Iteration 69/1000 | Loss: 0.00001080
Iteration 70/1000 | Loss: 0.00001080
Iteration 71/1000 | Loss: 0.00001080
Iteration 72/1000 | Loss: 0.00001080
Iteration 73/1000 | Loss: 0.00001080
Iteration 74/1000 | Loss: 0.00001079
Iteration 75/1000 | Loss: 0.00001079
Iteration 76/1000 | Loss: 0.00001079
Iteration 77/1000 | Loss: 0.00001079
Iteration 78/1000 | Loss: 0.00001079
Iteration 79/1000 | Loss: 0.00001078
Iteration 80/1000 | Loss: 0.00001078
Iteration 81/1000 | Loss: 0.00001078
Iteration 82/1000 | Loss: 0.00001078
Iteration 83/1000 | Loss: 0.00001078
Iteration 84/1000 | Loss: 0.00001078
Iteration 85/1000 | Loss: 0.00001078
Iteration 86/1000 | Loss: 0.00001078
Iteration 87/1000 | Loss: 0.00001078
Iteration 88/1000 | Loss: 0.00001078
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001077
Iteration 91/1000 | Loss: 0.00001077
Iteration 92/1000 | Loss: 0.00001077
Iteration 93/1000 | Loss: 0.00001077
Iteration 94/1000 | Loss: 0.00001077
Iteration 95/1000 | Loss: 0.00001077
Iteration 96/1000 | Loss: 0.00001076
Iteration 97/1000 | Loss: 0.00001076
Iteration 98/1000 | Loss: 0.00001076
Iteration 99/1000 | Loss: 0.00001076
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001076
Iteration 102/1000 | Loss: 0.00001076
Iteration 103/1000 | Loss: 0.00001076
Iteration 104/1000 | Loss: 0.00001076
Iteration 105/1000 | Loss: 0.00001076
Iteration 106/1000 | Loss: 0.00001076
Iteration 107/1000 | Loss: 0.00001076
Iteration 108/1000 | Loss: 0.00001076
Iteration 109/1000 | Loss: 0.00001076
Iteration 110/1000 | Loss: 0.00001076
Iteration 111/1000 | Loss: 0.00001076
Iteration 112/1000 | Loss: 0.00001076
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001075
Iteration 115/1000 | Loss: 0.00001075
Iteration 116/1000 | Loss: 0.00001075
Iteration 117/1000 | Loss: 0.00001075
Iteration 118/1000 | Loss: 0.00001075
Iteration 119/1000 | Loss: 0.00001075
Iteration 120/1000 | Loss: 0.00001075
Iteration 121/1000 | Loss: 0.00001075
Iteration 122/1000 | Loss: 0.00001075
Iteration 123/1000 | Loss: 0.00001075
Iteration 124/1000 | Loss: 0.00001075
Iteration 125/1000 | Loss: 0.00001075
Iteration 126/1000 | Loss: 0.00001075
Iteration 127/1000 | Loss: 0.00001075
Iteration 128/1000 | Loss: 0.00001075
Iteration 129/1000 | Loss: 0.00001075
Iteration 130/1000 | Loss: 0.00001075
Iteration 131/1000 | Loss: 0.00001075
Iteration 132/1000 | Loss: 0.00001074
Iteration 133/1000 | Loss: 0.00001074
Iteration 134/1000 | Loss: 0.00001074
Iteration 135/1000 | Loss: 0.00001074
Iteration 136/1000 | Loss: 0.00001074
Iteration 137/1000 | Loss: 0.00001074
Iteration 138/1000 | Loss: 0.00001074
Iteration 139/1000 | Loss: 0.00001074
Iteration 140/1000 | Loss: 0.00001074
Iteration 141/1000 | Loss: 0.00001073
Iteration 142/1000 | Loss: 0.00001073
Iteration 143/1000 | Loss: 0.00001073
Iteration 144/1000 | Loss: 0.00001073
Iteration 145/1000 | Loss: 0.00001073
Iteration 146/1000 | Loss: 0.00001073
Iteration 147/1000 | Loss: 0.00001073
Iteration 148/1000 | Loss: 0.00001073
Iteration 149/1000 | Loss: 0.00001073
Iteration 150/1000 | Loss: 0.00001073
Iteration 151/1000 | Loss: 0.00001073
Iteration 152/1000 | Loss: 0.00001072
Iteration 153/1000 | Loss: 0.00001072
Iteration 154/1000 | Loss: 0.00001072
Iteration 155/1000 | Loss: 0.00001072
Iteration 156/1000 | Loss: 0.00001072
Iteration 157/1000 | Loss: 0.00001072
Iteration 158/1000 | Loss: 0.00001072
Iteration 159/1000 | Loss: 0.00001072
Iteration 160/1000 | Loss: 0.00001072
Iteration 161/1000 | Loss: 0.00001072
Iteration 162/1000 | Loss: 0.00001072
Iteration 163/1000 | Loss: 0.00001072
Iteration 164/1000 | Loss: 0.00001072
Iteration 165/1000 | Loss: 0.00001072
Iteration 166/1000 | Loss: 0.00001072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.0723631930886768e-05, 1.0723631930886768e-05, 1.0723631930886768e-05, 1.0723631930886768e-05, 1.0723631930886768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0723631930886768e-05

Optimization complete. Final v2v error: 2.6898863315582275 mm

Highest mean error: 2.9173998832702637 mm for frame 61

Lowest mean error: 2.4365108013153076 mm for frame 81

Saving results

Total time: 32.1551513671875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00689526
Iteration 2/25 | Loss: 0.00156633
Iteration 3/25 | Loss: 0.00124714
Iteration 4/25 | Loss: 0.00123352
Iteration 5/25 | Loss: 0.00123182
Iteration 6/25 | Loss: 0.00123182
Iteration 7/25 | Loss: 0.00123182
Iteration 8/25 | Loss: 0.00123182
Iteration 9/25 | Loss: 0.00123182
Iteration 10/25 | Loss: 0.00123182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012318207882344723, 0.0012318207882344723, 0.0012318207882344723, 0.0012318207882344723, 0.0012318207882344723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012318207882344723

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.57094884
Iteration 2/25 | Loss: 0.00058905
Iteration 3/25 | Loss: 0.00058904
Iteration 4/25 | Loss: 0.00058904
Iteration 5/25 | Loss: 0.00058904
Iteration 6/25 | Loss: 0.00058904
Iteration 7/25 | Loss: 0.00058904
Iteration 8/25 | Loss: 0.00058904
Iteration 9/25 | Loss: 0.00058904
Iteration 10/25 | Loss: 0.00058904
Iteration 11/25 | Loss: 0.00058904
Iteration 12/25 | Loss: 0.00058904
Iteration 13/25 | Loss: 0.00058904
Iteration 14/25 | Loss: 0.00058904
Iteration 15/25 | Loss: 0.00058904
Iteration 16/25 | Loss: 0.00058904
Iteration 17/25 | Loss: 0.00058904
Iteration 18/25 | Loss: 0.00058904
Iteration 19/25 | Loss: 0.00058904
Iteration 20/25 | Loss: 0.00058904
Iteration 21/25 | Loss: 0.00058904
Iteration 22/25 | Loss: 0.00058904
Iteration 23/25 | Loss: 0.00058904
Iteration 24/25 | Loss: 0.00058904
Iteration 25/25 | Loss: 0.00058904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058904
Iteration 2/1000 | Loss: 0.00008880
Iteration 3/1000 | Loss: 0.00006084
Iteration 4/1000 | Loss: 0.00005372
Iteration 5/1000 | Loss: 0.00005117
Iteration 6/1000 | Loss: 0.00004981
Iteration 7/1000 | Loss: 0.00004867
Iteration 8/1000 | Loss: 0.00004760
Iteration 9/1000 | Loss: 0.00004657
Iteration 10/1000 | Loss: 0.00004565
Iteration 11/1000 | Loss: 0.00004500
Iteration 12/1000 | Loss: 0.00004423
Iteration 13/1000 | Loss: 0.00004367
Iteration 14/1000 | Loss: 0.00004310
Iteration 15/1000 | Loss: 0.00004277
Iteration 16/1000 | Loss: 0.00004238
Iteration 17/1000 | Loss: 0.00004202
Iteration 18/1000 | Loss: 0.00004173
Iteration 19/1000 | Loss: 0.00004138
Iteration 20/1000 | Loss: 0.00004114
Iteration 21/1000 | Loss: 0.00004091
Iteration 22/1000 | Loss: 0.00004072
Iteration 23/1000 | Loss: 0.00004055
Iteration 24/1000 | Loss: 0.00004052
Iteration 25/1000 | Loss: 0.00004051
Iteration 26/1000 | Loss: 0.00004044
Iteration 27/1000 | Loss: 0.00004044
Iteration 28/1000 | Loss: 0.00004044
Iteration 29/1000 | Loss: 0.00004043
Iteration 30/1000 | Loss: 0.00004043
Iteration 31/1000 | Loss: 0.00004043
Iteration 32/1000 | Loss: 0.00004043
Iteration 33/1000 | Loss: 0.00004043
Iteration 34/1000 | Loss: 0.00004043
Iteration 35/1000 | Loss: 0.00004043
Iteration 36/1000 | Loss: 0.00004043
Iteration 37/1000 | Loss: 0.00004043
Iteration 38/1000 | Loss: 0.00004043
Iteration 39/1000 | Loss: 0.00004043
Iteration 40/1000 | Loss: 0.00004043
Iteration 41/1000 | Loss: 0.00004043
Iteration 42/1000 | Loss: 0.00004042
Iteration 43/1000 | Loss: 0.00004042
Iteration 44/1000 | Loss: 0.00004042
Iteration 45/1000 | Loss: 0.00004042
Iteration 46/1000 | Loss: 0.00004041
Iteration 47/1000 | Loss: 0.00004040
Iteration 48/1000 | Loss: 0.00004040
Iteration 49/1000 | Loss: 0.00004040
Iteration 50/1000 | Loss: 0.00004040
Iteration 51/1000 | Loss: 0.00004040
Iteration 52/1000 | Loss: 0.00004040
Iteration 53/1000 | Loss: 0.00004040
Iteration 54/1000 | Loss: 0.00004039
Iteration 55/1000 | Loss: 0.00004039
Iteration 56/1000 | Loss: 0.00004039
Iteration 57/1000 | Loss: 0.00004039
Iteration 58/1000 | Loss: 0.00004039
Iteration 59/1000 | Loss: 0.00004039
Iteration 60/1000 | Loss: 0.00004039
Iteration 61/1000 | Loss: 0.00004039
Iteration 62/1000 | Loss: 0.00004039
Iteration 63/1000 | Loss: 0.00004038
Iteration 64/1000 | Loss: 0.00004038
Iteration 65/1000 | Loss: 0.00004038
Iteration 66/1000 | Loss: 0.00004038
Iteration 67/1000 | Loss: 0.00004038
Iteration 68/1000 | Loss: 0.00004037
Iteration 69/1000 | Loss: 0.00004037
Iteration 70/1000 | Loss: 0.00004036
Iteration 71/1000 | Loss: 0.00004036
Iteration 72/1000 | Loss: 0.00004036
Iteration 73/1000 | Loss: 0.00004036
Iteration 74/1000 | Loss: 0.00004036
Iteration 75/1000 | Loss: 0.00004035
Iteration 76/1000 | Loss: 0.00004035
Iteration 77/1000 | Loss: 0.00004035
Iteration 78/1000 | Loss: 0.00004034
Iteration 79/1000 | Loss: 0.00004034
Iteration 80/1000 | Loss: 0.00004034
Iteration 81/1000 | Loss: 0.00004034
Iteration 82/1000 | Loss: 0.00004034
Iteration 83/1000 | Loss: 0.00004034
Iteration 84/1000 | Loss: 0.00004034
Iteration 85/1000 | Loss: 0.00004034
Iteration 86/1000 | Loss: 0.00004034
Iteration 87/1000 | Loss: 0.00004034
Iteration 88/1000 | Loss: 0.00004034
Iteration 89/1000 | Loss: 0.00004034
Iteration 90/1000 | Loss: 0.00004034
Iteration 91/1000 | Loss: 0.00004033
Iteration 92/1000 | Loss: 0.00004033
Iteration 93/1000 | Loss: 0.00004033
Iteration 94/1000 | Loss: 0.00004033
Iteration 95/1000 | Loss: 0.00004033
Iteration 96/1000 | Loss: 0.00004032
Iteration 97/1000 | Loss: 0.00004032
Iteration 98/1000 | Loss: 0.00004032
Iteration 99/1000 | Loss: 0.00004032
Iteration 100/1000 | Loss: 0.00004032
Iteration 101/1000 | Loss: 0.00004032
Iteration 102/1000 | Loss: 0.00004032
Iteration 103/1000 | Loss: 0.00004031
Iteration 104/1000 | Loss: 0.00004031
Iteration 105/1000 | Loss: 0.00004031
Iteration 106/1000 | Loss: 0.00004031
Iteration 107/1000 | Loss: 0.00004031
Iteration 108/1000 | Loss: 0.00004031
Iteration 109/1000 | Loss: 0.00004031
Iteration 110/1000 | Loss: 0.00004031
Iteration 111/1000 | Loss: 0.00004031
Iteration 112/1000 | Loss: 0.00004031
Iteration 113/1000 | Loss: 0.00004031
Iteration 114/1000 | Loss: 0.00004031
Iteration 115/1000 | Loss: 0.00004031
Iteration 116/1000 | Loss: 0.00004031
Iteration 117/1000 | Loss: 0.00004030
Iteration 118/1000 | Loss: 0.00004030
Iteration 119/1000 | Loss: 0.00004030
Iteration 120/1000 | Loss: 0.00004030
Iteration 121/1000 | Loss: 0.00004030
Iteration 122/1000 | Loss: 0.00004030
Iteration 123/1000 | Loss: 0.00004030
Iteration 124/1000 | Loss: 0.00004030
Iteration 125/1000 | Loss: 0.00004030
Iteration 126/1000 | Loss: 0.00004029
Iteration 127/1000 | Loss: 0.00004029
Iteration 128/1000 | Loss: 0.00004029
Iteration 129/1000 | Loss: 0.00004029
Iteration 130/1000 | Loss: 0.00004029
Iteration 131/1000 | Loss: 0.00004029
Iteration 132/1000 | Loss: 0.00004029
Iteration 133/1000 | Loss: 0.00004028
Iteration 134/1000 | Loss: 0.00004028
Iteration 135/1000 | Loss: 0.00004028
Iteration 136/1000 | Loss: 0.00004028
Iteration 137/1000 | Loss: 0.00004028
Iteration 138/1000 | Loss: 0.00004027
Iteration 139/1000 | Loss: 0.00004027
Iteration 140/1000 | Loss: 0.00004027
Iteration 141/1000 | Loss: 0.00004027
Iteration 142/1000 | Loss: 0.00004027
Iteration 143/1000 | Loss: 0.00004027
Iteration 144/1000 | Loss: 0.00004026
Iteration 145/1000 | Loss: 0.00004026
Iteration 146/1000 | Loss: 0.00004026
Iteration 147/1000 | Loss: 0.00004026
Iteration 148/1000 | Loss: 0.00004026
Iteration 149/1000 | Loss: 0.00004026
Iteration 150/1000 | Loss: 0.00004026
Iteration 151/1000 | Loss: 0.00004026
Iteration 152/1000 | Loss: 0.00004026
Iteration 153/1000 | Loss: 0.00004025
Iteration 154/1000 | Loss: 0.00004025
Iteration 155/1000 | Loss: 0.00004025
Iteration 156/1000 | Loss: 0.00004025
Iteration 157/1000 | Loss: 0.00004025
Iteration 158/1000 | Loss: 0.00004025
Iteration 159/1000 | Loss: 0.00004025
Iteration 160/1000 | Loss: 0.00004025
Iteration 161/1000 | Loss: 0.00004025
Iteration 162/1000 | Loss: 0.00004025
Iteration 163/1000 | Loss: 0.00004025
Iteration 164/1000 | Loss: 0.00004025
Iteration 165/1000 | Loss: 0.00004025
Iteration 166/1000 | Loss: 0.00004025
Iteration 167/1000 | Loss: 0.00004025
Iteration 168/1000 | Loss: 0.00004025
Iteration 169/1000 | Loss: 0.00004025
Iteration 170/1000 | Loss: 0.00004025
Iteration 171/1000 | Loss: 0.00004025
Iteration 172/1000 | Loss: 0.00004025
Iteration 173/1000 | Loss: 0.00004025
Iteration 174/1000 | Loss: 0.00004025
Iteration 175/1000 | Loss: 0.00004025
Iteration 176/1000 | Loss: 0.00004025
Iteration 177/1000 | Loss: 0.00004025
Iteration 178/1000 | Loss: 0.00004025
Iteration 179/1000 | Loss: 0.00004025
Iteration 180/1000 | Loss: 0.00004025
Iteration 181/1000 | Loss: 0.00004025
Iteration 182/1000 | Loss: 0.00004025
Iteration 183/1000 | Loss: 0.00004025
Iteration 184/1000 | Loss: 0.00004025
Iteration 185/1000 | Loss: 0.00004025
Iteration 186/1000 | Loss: 0.00004025
Iteration 187/1000 | Loss: 0.00004025
Iteration 188/1000 | Loss: 0.00004025
Iteration 189/1000 | Loss: 0.00004025
Iteration 190/1000 | Loss: 0.00004025
Iteration 191/1000 | Loss: 0.00004025
Iteration 192/1000 | Loss: 0.00004025
Iteration 193/1000 | Loss: 0.00004025
Iteration 194/1000 | Loss: 0.00004025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [4.02484365622513e-05, 4.02484365622513e-05, 4.02484365622513e-05, 4.02484365622513e-05, 4.02484365622513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.02484365622513e-05

Optimization complete. Final v2v error: 4.982222080230713 mm

Highest mean error: 5.531916618347168 mm for frame 13

Lowest mean error: 4.470770835876465 mm for frame 106

Saving results

Total time: 48.13988161087036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058670
Iteration 2/25 | Loss: 0.01058670
Iteration 3/25 | Loss: 0.01058670
Iteration 4/25 | Loss: 0.01058670
Iteration 5/25 | Loss: 0.01058670
Iteration 6/25 | Loss: 0.01058670
Iteration 7/25 | Loss: 0.01058670
Iteration 8/25 | Loss: 0.01058670
Iteration 9/25 | Loss: 0.01058669
Iteration 10/25 | Loss: 0.01058669
Iteration 11/25 | Loss: 0.01058669
Iteration 12/25 | Loss: 0.01058669
Iteration 13/25 | Loss: 0.01058669
Iteration 14/25 | Loss: 0.01058669
Iteration 15/25 | Loss: 0.01058669
Iteration 16/25 | Loss: 0.01058669
Iteration 17/25 | Loss: 0.01058669
Iteration 18/25 | Loss: 0.01058669
Iteration 19/25 | Loss: 0.01058669
Iteration 20/25 | Loss: 0.01058669
Iteration 21/25 | Loss: 0.01058669
Iteration 22/25 | Loss: 0.01058668
Iteration 23/25 | Loss: 0.01058668
Iteration 24/25 | Loss: 0.01058668
Iteration 25/25 | Loss: 0.01058668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54612470
Iteration 2/25 | Loss: 0.11048871
Iteration 3/25 | Loss: 0.11048240
Iteration 4/25 | Loss: 0.11040051
Iteration 5/25 | Loss: 0.11040048
Iteration 6/25 | Loss: 0.11040047
Iteration 7/25 | Loss: 0.11040047
Iteration 8/25 | Loss: 0.11040047
Iteration 9/25 | Loss: 0.11040047
Iteration 10/25 | Loss: 0.11040047
Iteration 11/25 | Loss: 0.11040047
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.11040046811103821, 0.11040046811103821, 0.11040046811103821, 0.11040046811103821, 0.11040046811103821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11040046811103821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11040047
Iteration 2/1000 | Loss: 0.00363389
Iteration 3/1000 | Loss: 0.00129161
Iteration 4/1000 | Loss: 0.00083316
Iteration 5/1000 | Loss: 0.00340458
Iteration 6/1000 | Loss: 0.00204039
Iteration 7/1000 | Loss: 0.00045633
Iteration 8/1000 | Loss: 0.00248887
Iteration 9/1000 | Loss: 0.00021486
Iteration 10/1000 | Loss: 0.00018433
Iteration 11/1000 | Loss: 0.00017141
Iteration 12/1000 | Loss: 0.00022488
Iteration 13/1000 | Loss: 0.00052180
Iteration 14/1000 | Loss: 0.00004498
Iteration 15/1000 | Loss: 0.00014537
Iteration 16/1000 | Loss: 0.00012979
Iteration 17/1000 | Loss: 0.00035983
Iteration 18/1000 | Loss: 0.00032966
Iteration 19/1000 | Loss: 0.00003080
Iteration 20/1000 | Loss: 0.00005230
Iteration 21/1000 | Loss: 0.00019358
Iteration 22/1000 | Loss: 0.00005094
Iteration 23/1000 | Loss: 0.00008332
Iteration 24/1000 | Loss: 0.00002119
Iteration 25/1000 | Loss: 0.00017757
Iteration 26/1000 | Loss: 0.00001868
Iteration 27/1000 | Loss: 0.00014393
Iteration 28/1000 | Loss: 0.00021183
Iteration 29/1000 | Loss: 0.00001727
Iteration 30/1000 | Loss: 0.00006623
Iteration 31/1000 | Loss: 0.00002388
Iteration 32/1000 | Loss: 0.00001564
Iteration 33/1000 | Loss: 0.00009140
Iteration 34/1000 | Loss: 0.00001774
Iteration 35/1000 | Loss: 0.00001458
Iteration 36/1000 | Loss: 0.00001714
Iteration 37/1000 | Loss: 0.00009262
Iteration 38/1000 | Loss: 0.00001447
Iteration 39/1000 | Loss: 0.00001392
Iteration 40/1000 | Loss: 0.00005812
Iteration 41/1000 | Loss: 0.00002494
Iteration 42/1000 | Loss: 0.00002223
Iteration 43/1000 | Loss: 0.00002053
Iteration 44/1000 | Loss: 0.00001351
Iteration 45/1000 | Loss: 0.00003694
Iteration 46/1000 | Loss: 0.00003694
Iteration 47/1000 | Loss: 0.00010482
Iteration 48/1000 | Loss: 0.00002593
Iteration 49/1000 | Loss: 0.00002101
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001595
Iteration 54/1000 | Loss: 0.00001307
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00002664
Iteration 57/1000 | Loss: 0.00034967
Iteration 58/1000 | Loss: 0.00002563
Iteration 59/1000 | Loss: 0.00003731
Iteration 60/1000 | Loss: 0.00004559
Iteration 61/1000 | Loss: 0.00008628
Iteration 62/1000 | Loss: 0.00002902
Iteration 63/1000 | Loss: 0.00001296
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001287
Iteration 66/1000 | Loss: 0.00001286
Iteration 67/1000 | Loss: 0.00001286
Iteration 68/1000 | Loss: 0.00001962
Iteration 69/1000 | Loss: 0.00003124
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001284
Iteration 72/1000 | Loss: 0.00001284
Iteration 73/1000 | Loss: 0.00001284
Iteration 74/1000 | Loss: 0.00001284
Iteration 75/1000 | Loss: 0.00001284
Iteration 76/1000 | Loss: 0.00001284
Iteration 77/1000 | Loss: 0.00001284
Iteration 78/1000 | Loss: 0.00001284
Iteration 79/1000 | Loss: 0.00001284
Iteration 80/1000 | Loss: 0.00001284
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001283
Iteration 89/1000 | Loss: 0.00001283
Iteration 90/1000 | Loss: 0.00001283
Iteration 91/1000 | Loss: 0.00001283
Iteration 92/1000 | Loss: 0.00001283
Iteration 93/1000 | Loss: 0.00001283
Iteration 94/1000 | Loss: 0.00001283
Iteration 95/1000 | Loss: 0.00001282
Iteration 96/1000 | Loss: 0.00001282
Iteration 97/1000 | Loss: 0.00001282
Iteration 98/1000 | Loss: 0.00001282
Iteration 99/1000 | Loss: 0.00001281
Iteration 100/1000 | Loss: 0.00001281
Iteration 101/1000 | Loss: 0.00001281
Iteration 102/1000 | Loss: 0.00001280
Iteration 103/1000 | Loss: 0.00001280
Iteration 104/1000 | Loss: 0.00001280
Iteration 105/1000 | Loss: 0.00001279
Iteration 106/1000 | Loss: 0.00001279
Iteration 107/1000 | Loss: 0.00001279
Iteration 108/1000 | Loss: 0.00001279
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001279
Iteration 111/1000 | Loss: 0.00001279
Iteration 112/1000 | Loss: 0.00001279
Iteration 113/1000 | Loss: 0.00001279
Iteration 114/1000 | Loss: 0.00001279
Iteration 115/1000 | Loss: 0.00001279
Iteration 116/1000 | Loss: 0.00001279
Iteration 117/1000 | Loss: 0.00001279
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001279
Iteration 120/1000 | Loss: 0.00001279
Iteration 121/1000 | Loss: 0.00001279
Iteration 122/1000 | Loss: 0.00001279
Iteration 123/1000 | Loss: 0.00001279
Iteration 124/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.2789080756192561e-05, 1.2789080756192561e-05, 1.2789080756192561e-05, 1.2789080756192561e-05, 1.2789080756192561e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2789080756192561e-05

Optimization complete. Final v2v error: 2.9863758087158203 mm

Highest mean error: 3.220069408416748 mm for frame 30

Lowest mean error: 2.611867904663086 mm for frame 1

Saving results

Total time: 106.26936388015747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00629374
Iteration 2/25 | Loss: 0.00137632
Iteration 3/25 | Loss: 0.00121757
Iteration 4/25 | Loss: 0.00117822
Iteration 5/25 | Loss: 0.00119490
Iteration 6/25 | Loss: 0.00113086
Iteration 7/25 | Loss: 0.00111421
Iteration 8/25 | Loss: 0.00111218
Iteration 9/25 | Loss: 0.00111356
Iteration 10/25 | Loss: 0.00111219
Iteration 11/25 | Loss: 0.00110830
Iteration 12/25 | Loss: 0.00110683
Iteration 13/25 | Loss: 0.00110636
Iteration 14/25 | Loss: 0.00110966
Iteration 15/25 | Loss: 0.00110338
Iteration 16/25 | Loss: 0.00110143
Iteration 17/25 | Loss: 0.00110111
Iteration 18/25 | Loss: 0.00110105
Iteration 19/25 | Loss: 0.00110105
Iteration 20/25 | Loss: 0.00110105
Iteration 21/25 | Loss: 0.00110105
Iteration 22/25 | Loss: 0.00110104
Iteration 23/25 | Loss: 0.00110104
Iteration 24/25 | Loss: 0.00110104
Iteration 25/25 | Loss: 0.00110104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24527276
Iteration 2/25 | Loss: 0.00090635
Iteration 3/25 | Loss: 0.00090627
Iteration 4/25 | Loss: 0.00090627
Iteration 5/25 | Loss: 0.00090627
Iteration 6/25 | Loss: 0.00090627
Iteration 7/25 | Loss: 0.00090627
Iteration 8/25 | Loss: 0.00090627
Iteration 9/25 | Loss: 0.00090627
Iteration 10/25 | Loss: 0.00090627
Iteration 11/25 | Loss: 0.00090627
Iteration 12/25 | Loss: 0.00090626
Iteration 13/25 | Loss: 0.00090626
Iteration 14/25 | Loss: 0.00090626
Iteration 15/25 | Loss: 0.00090626
Iteration 16/25 | Loss: 0.00090626
Iteration 17/25 | Loss: 0.00090626
Iteration 18/25 | Loss: 0.00090626
Iteration 19/25 | Loss: 0.00090626
Iteration 20/25 | Loss: 0.00090626
Iteration 21/25 | Loss: 0.00090626
Iteration 22/25 | Loss: 0.00090626
Iteration 23/25 | Loss: 0.00090626
Iteration 24/25 | Loss: 0.00090626
Iteration 25/25 | Loss: 0.00090626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090626
Iteration 2/1000 | Loss: 0.00006743
Iteration 3/1000 | Loss: 0.00003701
Iteration 4/1000 | Loss: 0.00003119
Iteration 5/1000 | Loss: 0.00002957
Iteration 6/1000 | Loss: 0.00002838
Iteration 7/1000 | Loss: 0.00002759
Iteration 8/1000 | Loss: 0.00002673
Iteration 9/1000 | Loss: 0.00002620
Iteration 10/1000 | Loss: 0.00002584
Iteration 11/1000 | Loss: 0.00002550
Iteration 12/1000 | Loss: 0.00002521
Iteration 13/1000 | Loss: 0.00002499
Iteration 14/1000 | Loss: 0.00002489
Iteration 15/1000 | Loss: 0.00002473
Iteration 16/1000 | Loss: 0.00002472
Iteration 17/1000 | Loss: 0.00002470
Iteration 18/1000 | Loss: 0.00002467
Iteration 19/1000 | Loss: 0.00002465
Iteration 20/1000 | Loss: 0.00002464
Iteration 21/1000 | Loss: 0.00002464
Iteration 22/1000 | Loss: 0.00002463
Iteration 23/1000 | Loss: 0.00002463
Iteration 24/1000 | Loss: 0.00002461
Iteration 25/1000 | Loss: 0.00002460
Iteration 26/1000 | Loss: 0.00002459
Iteration 27/1000 | Loss: 0.00002459
Iteration 28/1000 | Loss: 0.00002459
Iteration 29/1000 | Loss: 0.00002458
Iteration 30/1000 | Loss: 0.00002458
Iteration 31/1000 | Loss: 0.00002457
Iteration 32/1000 | Loss: 0.00002456
Iteration 33/1000 | Loss: 0.00002453
Iteration 34/1000 | Loss: 0.00002452
Iteration 35/1000 | Loss: 0.00002449
Iteration 36/1000 | Loss: 0.00002449
Iteration 37/1000 | Loss: 0.00002449
Iteration 38/1000 | Loss: 0.00002446
Iteration 39/1000 | Loss: 0.00002445
Iteration 40/1000 | Loss: 0.00002445
Iteration 41/1000 | Loss: 0.00002445
Iteration 42/1000 | Loss: 0.00002445
Iteration 43/1000 | Loss: 0.00002444
Iteration 44/1000 | Loss: 0.00002444
Iteration 45/1000 | Loss: 0.00002443
Iteration 46/1000 | Loss: 0.00002443
Iteration 47/1000 | Loss: 0.00002441
Iteration 48/1000 | Loss: 0.00002441
Iteration 49/1000 | Loss: 0.00002441
Iteration 50/1000 | Loss: 0.00002441
Iteration 51/1000 | Loss: 0.00002441
Iteration 52/1000 | Loss: 0.00002440
Iteration 53/1000 | Loss: 0.00002439
Iteration 54/1000 | Loss: 0.00002439
Iteration 55/1000 | Loss: 0.00002438
Iteration 56/1000 | Loss: 0.00002438
Iteration 57/1000 | Loss: 0.00002438
Iteration 58/1000 | Loss: 0.00002437
Iteration 59/1000 | Loss: 0.00002437
Iteration 60/1000 | Loss: 0.00002437
Iteration 61/1000 | Loss: 0.00002437
Iteration 62/1000 | Loss: 0.00002437
Iteration 63/1000 | Loss: 0.00002437
Iteration 64/1000 | Loss: 0.00002437
Iteration 65/1000 | Loss: 0.00002437
Iteration 66/1000 | Loss: 0.00002437
Iteration 67/1000 | Loss: 0.00002436
Iteration 68/1000 | Loss: 0.00002436
Iteration 69/1000 | Loss: 0.00002436
Iteration 70/1000 | Loss: 0.00002436
Iteration 71/1000 | Loss: 0.00002436
Iteration 72/1000 | Loss: 0.00002436
Iteration 73/1000 | Loss: 0.00002435
Iteration 74/1000 | Loss: 0.00002435
Iteration 75/1000 | Loss: 0.00002435
Iteration 76/1000 | Loss: 0.00002435
Iteration 77/1000 | Loss: 0.00002434
Iteration 78/1000 | Loss: 0.00002434
Iteration 79/1000 | Loss: 0.00002434
Iteration 80/1000 | Loss: 0.00002434
Iteration 81/1000 | Loss: 0.00002433
Iteration 82/1000 | Loss: 0.00002433
Iteration 83/1000 | Loss: 0.00002433
Iteration 84/1000 | Loss: 0.00002433
Iteration 85/1000 | Loss: 0.00002433
Iteration 86/1000 | Loss: 0.00002433
Iteration 87/1000 | Loss: 0.00002433
Iteration 88/1000 | Loss: 0.00002433
Iteration 89/1000 | Loss: 0.00002433
Iteration 90/1000 | Loss: 0.00002433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [2.4330307496711612e-05, 2.4330307496711612e-05, 2.4330307496711612e-05, 2.4330307496711612e-05, 2.4330307496711612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4330307496711612e-05

Optimization complete. Final v2v error: 3.7831475734710693 mm

Highest mean error: 5.730067729949951 mm for frame 0

Lowest mean error: 3.0331103801727295 mm for frame 171

Saving results

Total time: 60.81934833526611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907683
Iteration 2/25 | Loss: 0.00123304
Iteration 3/25 | Loss: 0.00106150
Iteration 4/25 | Loss: 0.00103207
Iteration 5/25 | Loss: 0.00102309
Iteration 6/25 | Loss: 0.00102001
Iteration 7/25 | Loss: 0.00101956
Iteration 8/25 | Loss: 0.00101956
Iteration 9/25 | Loss: 0.00101956
Iteration 10/25 | Loss: 0.00101956
Iteration 11/25 | Loss: 0.00101956
Iteration 12/25 | Loss: 0.00101956
Iteration 13/25 | Loss: 0.00101956
Iteration 14/25 | Loss: 0.00101956
Iteration 15/25 | Loss: 0.00101956
Iteration 16/25 | Loss: 0.00101956
Iteration 17/25 | Loss: 0.00101956
Iteration 18/25 | Loss: 0.00101956
Iteration 19/25 | Loss: 0.00101956
Iteration 20/25 | Loss: 0.00101956
Iteration 21/25 | Loss: 0.00101956
Iteration 22/25 | Loss: 0.00101956
Iteration 23/25 | Loss: 0.00101956
Iteration 24/25 | Loss: 0.00101956
Iteration 25/25 | Loss: 0.00101956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33289742
Iteration 2/25 | Loss: 0.00076538
Iteration 3/25 | Loss: 0.00076535
Iteration 4/25 | Loss: 0.00076535
Iteration 5/25 | Loss: 0.00076535
Iteration 6/25 | Loss: 0.00076535
Iteration 7/25 | Loss: 0.00076534
Iteration 8/25 | Loss: 0.00076534
Iteration 9/25 | Loss: 0.00076534
Iteration 10/25 | Loss: 0.00076534
Iteration 11/25 | Loss: 0.00076534
Iteration 12/25 | Loss: 0.00076534
Iteration 13/25 | Loss: 0.00076534
Iteration 14/25 | Loss: 0.00076534
Iteration 15/25 | Loss: 0.00076534
Iteration 16/25 | Loss: 0.00076534
Iteration 17/25 | Loss: 0.00076534
Iteration 18/25 | Loss: 0.00076534
Iteration 19/25 | Loss: 0.00076534
Iteration 20/25 | Loss: 0.00076534
Iteration 21/25 | Loss: 0.00076534
Iteration 22/25 | Loss: 0.00076534
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007653439533896744, 0.0007653439533896744, 0.0007653439533896744, 0.0007653439533896744, 0.0007653439533896744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007653439533896744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076534
Iteration 2/1000 | Loss: 0.00004869
Iteration 3/1000 | Loss: 0.00002848
Iteration 4/1000 | Loss: 0.00002277
Iteration 5/1000 | Loss: 0.00002130
Iteration 6/1000 | Loss: 0.00002035
Iteration 7/1000 | Loss: 0.00001947
Iteration 8/1000 | Loss: 0.00001898
Iteration 9/1000 | Loss: 0.00001856
Iteration 10/1000 | Loss: 0.00001820
Iteration 11/1000 | Loss: 0.00001809
Iteration 12/1000 | Loss: 0.00001806
Iteration 13/1000 | Loss: 0.00001799
Iteration 14/1000 | Loss: 0.00001798
Iteration 15/1000 | Loss: 0.00001797
Iteration 16/1000 | Loss: 0.00001796
Iteration 17/1000 | Loss: 0.00001786
Iteration 18/1000 | Loss: 0.00001785
Iteration 19/1000 | Loss: 0.00001778
Iteration 20/1000 | Loss: 0.00001778
Iteration 21/1000 | Loss: 0.00001775
Iteration 22/1000 | Loss: 0.00001774
Iteration 23/1000 | Loss: 0.00001774
Iteration 24/1000 | Loss: 0.00001773
Iteration 25/1000 | Loss: 0.00001773
Iteration 26/1000 | Loss: 0.00001773
Iteration 27/1000 | Loss: 0.00001773
Iteration 28/1000 | Loss: 0.00001773
Iteration 29/1000 | Loss: 0.00001772
Iteration 30/1000 | Loss: 0.00001772
Iteration 31/1000 | Loss: 0.00001771
Iteration 32/1000 | Loss: 0.00001770
Iteration 33/1000 | Loss: 0.00001770
Iteration 34/1000 | Loss: 0.00001770
Iteration 35/1000 | Loss: 0.00001770
Iteration 36/1000 | Loss: 0.00001770
Iteration 37/1000 | Loss: 0.00001770
Iteration 38/1000 | Loss: 0.00001770
Iteration 39/1000 | Loss: 0.00001770
Iteration 40/1000 | Loss: 0.00001770
Iteration 41/1000 | Loss: 0.00001770
Iteration 42/1000 | Loss: 0.00001770
Iteration 43/1000 | Loss: 0.00001769
Iteration 44/1000 | Loss: 0.00001769
Iteration 45/1000 | Loss: 0.00001769
Iteration 46/1000 | Loss: 0.00001769
Iteration 47/1000 | Loss: 0.00001769
Iteration 48/1000 | Loss: 0.00001769
Iteration 49/1000 | Loss: 0.00001769
Iteration 50/1000 | Loss: 0.00001769
Iteration 51/1000 | Loss: 0.00001767
Iteration 52/1000 | Loss: 0.00001767
Iteration 53/1000 | Loss: 0.00001767
Iteration 54/1000 | Loss: 0.00001767
Iteration 55/1000 | Loss: 0.00001767
Iteration 56/1000 | Loss: 0.00001767
Iteration 57/1000 | Loss: 0.00001767
Iteration 58/1000 | Loss: 0.00001767
Iteration 59/1000 | Loss: 0.00001766
Iteration 60/1000 | Loss: 0.00001766
Iteration 61/1000 | Loss: 0.00001766
Iteration 62/1000 | Loss: 0.00001766
Iteration 63/1000 | Loss: 0.00001766
Iteration 64/1000 | Loss: 0.00001765
Iteration 65/1000 | Loss: 0.00001765
Iteration 66/1000 | Loss: 0.00001765
Iteration 67/1000 | Loss: 0.00001765
Iteration 68/1000 | Loss: 0.00001765
Iteration 69/1000 | Loss: 0.00001765
Iteration 70/1000 | Loss: 0.00001765
Iteration 71/1000 | Loss: 0.00001764
Iteration 72/1000 | Loss: 0.00001764
Iteration 73/1000 | Loss: 0.00001764
Iteration 74/1000 | Loss: 0.00001763
Iteration 75/1000 | Loss: 0.00001763
Iteration 76/1000 | Loss: 0.00001763
Iteration 77/1000 | Loss: 0.00001763
Iteration 78/1000 | Loss: 0.00001763
Iteration 79/1000 | Loss: 0.00001763
Iteration 80/1000 | Loss: 0.00001763
Iteration 81/1000 | Loss: 0.00001762
Iteration 82/1000 | Loss: 0.00001762
Iteration 83/1000 | Loss: 0.00001762
Iteration 84/1000 | Loss: 0.00001762
Iteration 85/1000 | Loss: 0.00001762
Iteration 86/1000 | Loss: 0.00001761
Iteration 87/1000 | Loss: 0.00001761
Iteration 88/1000 | Loss: 0.00001761
Iteration 89/1000 | Loss: 0.00001761
Iteration 90/1000 | Loss: 0.00001761
Iteration 91/1000 | Loss: 0.00001761
Iteration 92/1000 | Loss: 0.00001761
Iteration 93/1000 | Loss: 0.00001761
Iteration 94/1000 | Loss: 0.00001761
Iteration 95/1000 | Loss: 0.00001761
Iteration 96/1000 | Loss: 0.00001761
Iteration 97/1000 | Loss: 0.00001761
Iteration 98/1000 | Loss: 0.00001761
Iteration 99/1000 | Loss: 0.00001761
Iteration 100/1000 | Loss: 0.00001761
Iteration 101/1000 | Loss: 0.00001761
Iteration 102/1000 | Loss: 0.00001761
Iteration 103/1000 | Loss: 0.00001760
Iteration 104/1000 | Loss: 0.00001760
Iteration 105/1000 | Loss: 0.00001760
Iteration 106/1000 | Loss: 0.00001760
Iteration 107/1000 | Loss: 0.00001760
Iteration 108/1000 | Loss: 0.00001760
Iteration 109/1000 | Loss: 0.00001760
Iteration 110/1000 | Loss: 0.00001760
Iteration 111/1000 | Loss: 0.00001760
Iteration 112/1000 | Loss: 0.00001760
Iteration 113/1000 | Loss: 0.00001760
Iteration 114/1000 | Loss: 0.00001760
Iteration 115/1000 | Loss: 0.00001760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.760321538313292e-05, 1.760321538313292e-05, 1.760321538313292e-05, 1.760321538313292e-05, 1.760321538313292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.760321538313292e-05

Optimization complete. Final v2v error: 3.42665433883667 mm

Highest mean error: 5.064916133880615 mm for frame 69

Lowest mean error: 2.5580084323883057 mm for frame 2

Saving results

Total time: 33.406561851501465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360168
Iteration 2/25 | Loss: 0.00107034
Iteration 3/25 | Loss: 0.00098248
Iteration 4/25 | Loss: 0.00097092
Iteration 5/25 | Loss: 0.00096696
Iteration 6/25 | Loss: 0.00096585
Iteration 7/25 | Loss: 0.00096585
Iteration 8/25 | Loss: 0.00096585
Iteration 9/25 | Loss: 0.00096585
Iteration 10/25 | Loss: 0.00096585
Iteration 11/25 | Loss: 0.00096585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009658480994403362, 0.0009658480994403362, 0.0009658480994403362, 0.0009658480994403362, 0.0009658480994403362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009658480994403362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56554174
Iteration 2/25 | Loss: 0.00094278
Iteration 3/25 | Loss: 0.00094278
Iteration 4/25 | Loss: 0.00094278
Iteration 5/25 | Loss: 0.00094278
Iteration 6/25 | Loss: 0.00094278
Iteration 7/25 | Loss: 0.00094278
Iteration 8/25 | Loss: 0.00094278
Iteration 9/25 | Loss: 0.00094278
Iteration 10/25 | Loss: 0.00094278
Iteration 11/25 | Loss: 0.00094278
Iteration 12/25 | Loss: 0.00094278
Iteration 13/25 | Loss: 0.00094278
Iteration 14/25 | Loss: 0.00094278
Iteration 15/25 | Loss: 0.00094278
Iteration 16/25 | Loss: 0.00094278
Iteration 17/25 | Loss: 0.00094278
Iteration 18/25 | Loss: 0.00094278
Iteration 19/25 | Loss: 0.00094278
Iteration 20/25 | Loss: 0.00094278
Iteration 21/25 | Loss: 0.00094278
Iteration 22/25 | Loss: 0.00094278
Iteration 23/25 | Loss: 0.00094278
Iteration 24/25 | Loss: 0.00094278
Iteration 25/25 | Loss: 0.00094278

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094278
Iteration 2/1000 | Loss: 0.00001594
Iteration 3/1000 | Loss: 0.00001174
Iteration 4/1000 | Loss: 0.00001094
Iteration 5/1000 | Loss: 0.00001037
Iteration 6/1000 | Loss: 0.00000994
Iteration 7/1000 | Loss: 0.00000975
Iteration 8/1000 | Loss: 0.00000950
Iteration 9/1000 | Loss: 0.00000949
Iteration 10/1000 | Loss: 0.00000948
Iteration 11/1000 | Loss: 0.00000947
Iteration 12/1000 | Loss: 0.00000946
Iteration 13/1000 | Loss: 0.00000945
Iteration 14/1000 | Loss: 0.00000944
Iteration 15/1000 | Loss: 0.00000943
Iteration 16/1000 | Loss: 0.00000943
Iteration 17/1000 | Loss: 0.00000941
Iteration 18/1000 | Loss: 0.00000940
Iteration 19/1000 | Loss: 0.00000936
Iteration 20/1000 | Loss: 0.00000935
Iteration 21/1000 | Loss: 0.00000932
Iteration 22/1000 | Loss: 0.00000931
Iteration 23/1000 | Loss: 0.00000928
Iteration 24/1000 | Loss: 0.00000927
Iteration 25/1000 | Loss: 0.00000927
Iteration 26/1000 | Loss: 0.00000927
Iteration 27/1000 | Loss: 0.00000927
Iteration 28/1000 | Loss: 0.00000927
Iteration 29/1000 | Loss: 0.00000927
Iteration 30/1000 | Loss: 0.00000927
Iteration 31/1000 | Loss: 0.00000926
Iteration 32/1000 | Loss: 0.00000926
Iteration 33/1000 | Loss: 0.00000926
Iteration 34/1000 | Loss: 0.00000926
Iteration 35/1000 | Loss: 0.00000925
Iteration 36/1000 | Loss: 0.00000925
Iteration 37/1000 | Loss: 0.00000925
Iteration 38/1000 | Loss: 0.00000925
Iteration 39/1000 | Loss: 0.00000925
Iteration 40/1000 | Loss: 0.00000924
Iteration 41/1000 | Loss: 0.00000924
Iteration 42/1000 | Loss: 0.00000924
Iteration 43/1000 | Loss: 0.00000924
Iteration 44/1000 | Loss: 0.00000923
Iteration 45/1000 | Loss: 0.00000923
Iteration 46/1000 | Loss: 0.00000922
Iteration 47/1000 | Loss: 0.00000922
Iteration 48/1000 | Loss: 0.00000922
Iteration 49/1000 | Loss: 0.00000921
Iteration 50/1000 | Loss: 0.00000921
Iteration 51/1000 | Loss: 0.00000921
Iteration 52/1000 | Loss: 0.00000921
Iteration 53/1000 | Loss: 0.00000921
Iteration 54/1000 | Loss: 0.00000920
Iteration 55/1000 | Loss: 0.00000920
Iteration 56/1000 | Loss: 0.00000920
Iteration 57/1000 | Loss: 0.00000919
Iteration 58/1000 | Loss: 0.00000919
Iteration 59/1000 | Loss: 0.00000919
Iteration 60/1000 | Loss: 0.00000918
Iteration 61/1000 | Loss: 0.00000918
Iteration 62/1000 | Loss: 0.00000918
Iteration 63/1000 | Loss: 0.00000918
Iteration 64/1000 | Loss: 0.00000917
Iteration 65/1000 | Loss: 0.00000917
Iteration 66/1000 | Loss: 0.00000917
Iteration 67/1000 | Loss: 0.00000917
Iteration 68/1000 | Loss: 0.00000917
Iteration 69/1000 | Loss: 0.00000917
Iteration 70/1000 | Loss: 0.00000916
Iteration 71/1000 | Loss: 0.00000916
Iteration 72/1000 | Loss: 0.00000916
Iteration 73/1000 | Loss: 0.00000915
Iteration 74/1000 | Loss: 0.00000915
Iteration 75/1000 | Loss: 0.00000915
Iteration 76/1000 | Loss: 0.00000915
Iteration 77/1000 | Loss: 0.00000914
Iteration 78/1000 | Loss: 0.00000914
Iteration 79/1000 | Loss: 0.00000914
Iteration 80/1000 | Loss: 0.00000914
Iteration 81/1000 | Loss: 0.00000914
Iteration 82/1000 | Loss: 0.00000914
Iteration 83/1000 | Loss: 0.00000914
Iteration 84/1000 | Loss: 0.00000913
Iteration 85/1000 | Loss: 0.00000913
Iteration 86/1000 | Loss: 0.00000913
Iteration 87/1000 | Loss: 0.00000913
Iteration 88/1000 | Loss: 0.00000913
Iteration 89/1000 | Loss: 0.00000913
Iteration 90/1000 | Loss: 0.00000913
Iteration 91/1000 | Loss: 0.00000913
Iteration 92/1000 | Loss: 0.00000913
Iteration 93/1000 | Loss: 0.00000913
Iteration 94/1000 | Loss: 0.00000913
Iteration 95/1000 | Loss: 0.00000913
Iteration 96/1000 | Loss: 0.00000913
Iteration 97/1000 | Loss: 0.00000913
Iteration 98/1000 | Loss: 0.00000913
Iteration 99/1000 | Loss: 0.00000913
Iteration 100/1000 | Loss: 0.00000912
Iteration 101/1000 | Loss: 0.00000912
Iteration 102/1000 | Loss: 0.00000912
Iteration 103/1000 | Loss: 0.00000912
Iteration 104/1000 | Loss: 0.00000912
Iteration 105/1000 | Loss: 0.00000912
Iteration 106/1000 | Loss: 0.00000912
Iteration 107/1000 | Loss: 0.00000912
Iteration 108/1000 | Loss: 0.00000912
Iteration 109/1000 | Loss: 0.00000912
Iteration 110/1000 | Loss: 0.00000912
Iteration 111/1000 | Loss: 0.00000912
Iteration 112/1000 | Loss: 0.00000912
Iteration 113/1000 | Loss: 0.00000912
Iteration 114/1000 | Loss: 0.00000912
Iteration 115/1000 | Loss: 0.00000912
Iteration 116/1000 | Loss: 0.00000912
Iteration 117/1000 | Loss: 0.00000912
Iteration 118/1000 | Loss: 0.00000912
Iteration 119/1000 | Loss: 0.00000912
Iteration 120/1000 | Loss: 0.00000912
Iteration 121/1000 | Loss: 0.00000912
Iteration 122/1000 | Loss: 0.00000912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [9.12379164219601e-06, 9.12379164219601e-06, 9.12379164219601e-06, 9.12379164219601e-06, 9.12379164219601e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.12379164219601e-06

Optimization complete. Final v2v error: 2.576565742492676 mm

Highest mean error: 3.1086225509643555 mm for frame 131

Lowest mean error: 2.392143487930298 mm for frame 260

Saving results

Total time: 33.94646763801575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405465
Iteration 2/25 | Loss: 0.00126767
Iteration 3/25 | Loss: 0.00101325
Iteration 4/25 | Loss: 0.00098694
Iteration 5/25 | Loss: 0.00098379
Iteration 6/25 | Loss: 0.00098316
Iteration 7/25 | Loss: 0.00098316
Iteration 8/25 | Loss: 0.00098316
Iteration 9/25 | Loss: 0.00098316
Iteration 10/25 | Loss: 0.00098316
Iteration 11/25 | Loss: 0.00098316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009831569623202085, 0.0009831569623202085, 0.0009831569623202085, 0.0009831569623202085, 0.0009831569623202085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009831569623202085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29389024
Iteration 2/25 | Loss: 0.00068682
Iteration 3/25 | Loss: 0.00068681
Iteration 4/25 | Loss: 0.00068681
Iteration 5/25 | Loss: 0.00068681
Iteration 6/25 | Loss: 0.00068681
Iteration 7/25 | Loss: 0.00068681
Iteration 8/25 | Loss: 0.00068681
Iteration 9/25 | Loss: 0.00068681
Iteration 10/25 | Loss: 0.00068681
Iteration 11/25 | Loss: 0.00068681
Iteration 12/25 | Loss: 0.00068681
Iteration 13/25 | Loss: 0.00068681
Iteration 14/25 | Loss: 0.00068681
Iteration 15/25 | Loss: 0.00068681
Iteration 16/25 | Loss: 0.00068681
Iteration 17/25 | Loss: 0.00068681
Iteration 18/25 | Loss: 0.00068681
Iteration 19/25 | Loss: 0.00068681
Iteration 20/25 | Loss: 0.00068681
Iteration 21/25 | Loss: 0.00068681
Iteration 22/25 | Loss: 0.00068681
Iteration 23/25 | Loss: 0.00068681
Iteration 24/25 | Loss: 0.00068681
Iteration 25/25 | Loss: 0.00068681

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068681
Iteration 2/1000 | Loss: 0.00002095
Iteration 3/1000 | Loss: 0.00001172
Iteration 4/1000 | Loss: 0.00001024
Iteration 5/1000 | Loss: 0.00000945
Iteration 6/1000 | Loss: 0.00000902
Iteration 7/1000 | Loss: 0.00000868
Iteration 8/1000 | Loss: 0.00000844
Iteration 9/1000 | Loss: 0.00000823
Iteration 10/1000 | Loss: 0.00000820
Iteration 11/1000 | Loss: 0.00000813
Iteration 12/1000 | Loss: 0.00000812
Iteration 13/1000 | Loss: 0.00000809
Iteration 14/1000 | Loss: 0.00000806
Iteration 15/1000 | Loss: 0.00000805
Iteration 16/1000 | Loss: 0.00000805
Iteration 17/1000 | Loss: 0.00000804
Iteration 18/1000 | Loss: 0.00000803
Iteration 19/1000 | Loss: 0.00000803
Iteration 20/1000 | Loss: 0.00000803
Iteration 21/1000 | Loss: 0.00000803
Iteration 22/1000 | Loss: 0.00000803
Iteration 23/1000 | Loss: 0.00000803
Iteration 24/1000 | Loss: 0.00000803
Iteration 25/1000 | Loss: 0.00000803
Iteration 26/1000 | Loss: 0.00000803
Iteration 27/1000 | Loss: 0.00000802
Iteration 28/1000 | Loss: 0.00000802
Iteration 29/1000 | Loss: 0.00000801
Iteration 30/1000 | Loss: 0.00000801
Iteration 31/1000 | Loss: 0.00000801
Iteration 32/1000 | Loss: 0.00000800
Iteration 33/1000 | Loss: 0.00000799
Iteration 34/1000 | Loss: 0.00000799
Iteration 35/1000 | Loss: 0.00000799
Iteration 36/1000 | Loss: 0.00000799
Iteration 37/1000 | Loss: 0.00000799
Iteration 38/1000 | Loss: 0.00000798
Iteration 39/1000 | Loss: 0.00000798
Iteration 40/1000 | Loss: 0.00000798
Iteration 41/1000 | Loss: 0.00000798
Iteration 42/1000 | Loss: 0.00000798
Iteration 43/1000 | Loss: 0.00000797
Iteration 44/1000 | Loss: 0.00000797
Iteration 45/1000 | Loss: 0.00000797
Iteration 46/1000 | Loss: 0.00000797
Iteration 47/1000 | Loss: 0.00000796
Iteration 48/1000 | Loss: 0.00000796
Iteration 49/1000 | Loss: 0.00000796
Iteration 50/1000 | Loss: 0.00000795
Iteration 51/1000 | Loss: 0.00000795
Iteration 52/1000 | Loss: 0.00000795
Iteration 53/1000 | Loss: 0.00000795
Iteration 54/1000 | Loss: 0.00000795
Iteration 55/1000 | Loss: 0.00000795
Iteration 56/1000 | Loss: 0.00000794
Iteration 57/1000 | Loss: 0.00000794
Iteration 58/1000 | Loss: 0.00000794
Iteration 59/1000 | Loss: 0.00000793
Iteration 60/1000 | Loss: 0.00000793
Iteration 61/1000 | Loss: 0.00000793
Iteration 62/1000 | Loss: 0.00000793
Iteration 63/1000 | Loss: 0.00000793
Iteration 64/1000 | Loss: 0.00000793
Iteration 65/1000 | Loss: 0.00000793
Iteration 66/1000 | Loss: 0.00000793
Iteration 67/1000 | Loss: 0.00000793
Iteration 68/1000 | Loss: 0.00000793
Iteration 69/1000 | Loss: 0.00000793
Iteration 70/1000 | Loss: 0.00000793
Iteration 71/1000 | Loss: 0.00000793
Iteration 72/1000 | Loss: 0.00000793
Iteration 73/1000 | Loss: 0.00000793
Iteration 74/1000 | Loss: 0.00000793
Iteration 75/1000 | Loss: 0.00000793
Iteration 76/1000 | Loss: 0.00000793
Iteration 77/1000 | Loss: 0.00000793
Iteration 78/1000 | Loss: 0.00000793
Iteration 79/1000 | Loss: 0.00000793
Iteration 80/1000 | Loss: 0.00000793
Iteration 81/1000 | Loss: 0.00000793
Iteration 82/1000 | Loss: 0.00000793
Iteration 83/1000 | Loss: 0.00000793
Iteration 84/1000 | Loss: 0.00000793
Iteration 85/1000 | Loss: 0.00000793
Iteration 86/1000 | Loss: 0.00000793
Iteration 87/1000 | Loss: 0.00000793
Iteration 88/1000 | Loss: 0.00000793
Iteration 89/1000 | Loss: 0.00000793
Iteration 90/1000 | Loss: 0.00000793
Iteration 91/1000 | Loss: 0.00000793
Iteration 92/1000 | Loss: 0.00000793
Iteration 93/1000 | Loss: 0.00000793
Iteration 94/1000 | Loss: 0.00000793
Iteration 95/1000 | Loss: 0.00000793
Iteration 96/1000 | Loss: 0.00000793
Iteration 97/1000 | Loss: 0.00000793
Iteration 98/1000 | Loss: 0.00000793
Iteration 99/1000 | Loss: 0.00000793
Iteration 100/1000 | Loss: 0.00000793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [7.926235412014648e-06, 7.926235412014648e-06, 7.926235412014648e-06, 7.926235412014648e-06, 7.926235412014648e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.926235412014648e-06

Optimization complete. Final v2v error: 2.4011096954345703 mm

Highest mean error: 2.680272102355957 mm for frame 31

Lowest mean error: 2.116307258605957 mm for frame 49

Saving results

Total time: 26.919469594955444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471752
Iteration 2/25 | Loss: 0.00118002
Iteration 3/25 | Loss: 0.00102961
Iteration 4/25 | Loss: 0.00100772
Iteration 5/25 | Loss: 0.00100318
Iteration 6/25 | Loss: 0.00100131
Iteration 7/25 | Loss: 0.00100131
Iteration 8/25 | Loss: 0.00100131
Iteration 9/25 | Loss: 0.00100131
Iteration 10/25 | Loss: 0.00100131
Iteration 11/25 | Loss: 0.00100131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010013134451583028, 0.0010013134451583028, 0.0010013134451583028, 0.0010013134451583028, 0.0010013134451583028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010013134451583028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28043604
Iteration 2/25 | Loss: 0.00081551
Iteration 3/25 | Loss: 0.00081551
Iteration 4/25 | Loss: 0.00081551
Iteration 5/25 | Loss: 0.00081551
Iteration 6/25 | Loss: 0.00081551
Iteration 7/25 | Loss: 0.00081551
Iteration 8/25 | Loss: 0.00081551
Iteration 9/25 | Loss: 0.00081551
Iteration 10/25 | Loss: 0.00081551
Iteration 11/25 | Loss: 0.00081551
Iteration 12/25 | Loss: 0.00081551
Iteration 13/25 | Loss: 0.00081551
Iteration 14/25 | Loss: 0.00081551
Iteration 15/25 | Loss: 0.00081551
Iteration 16/25 | Loss: 0.00081551
Iteration 17/25 | Loss: 0.00081551
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008155083633027971, 0.0008155083633027971, 0.0008155083633027971, 0.0008155083633027971, 0.0008155083633027971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008155083633027971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081551
Iteration 2/1000 | Loss: 0.00002135
Iteration 3/1000 | Loss: 0.00001585
Iteration 4/1000 | Loss: 0.00001493
Iteration 5/1000 | Loss: 0.00001432
Iteration 6/1000 | Loss: 0.00001387
Iteration 7/1000 | Loss: 0.00001370
Iteration 8/1000 | Loss: 0.00001353
Iteration 9/1000 | Loss: 0.00001348
Iteration 10/1000 | Loss: 0.00001348
Iteration 11/1000 | Loss: 0.00001347
Iteration 12/1000 | Loss: 0.00001347
Iteration 13/1000 | Loss: 0.00001347
Iteration 14/1000 | Loss: 0.00001347
Iteration 15/1000 | Loss: 0.00001347
Iteration 16/1000 | Loss: 0.00001346
Iteration 17/1000 | Loss: 0.00001346
Iteration 18/1000 | Loss: 0.00001346
Iteration 19/1000 | Loss: 0.00001346
Iteration 20/1000 | Loss: 0.00001346
Iteration 21/1000 | Loss: 0.00001345
Iteration 22/1000 | Loss: 0.00001343
Iteration 23/1000 | Loss: 0.00001342
Iteration 24/1000 | Loss: 0.00001342
Iteration 25/1000 | Loss: 0.00001342
Iteration 26/1000 | Loss: 0.00001342
Iteration 27/1000 | Loss: 0.00001342
Iteration 28/1000 | Loss: 0.00001342
Iteration 29/1000 | Loss: 0.00001342
Iteration 30/1000 | Loss: 0.00001342
Iteration 31/1000 | Loss: 0.00001342
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001341
Iteration 34/1000 | Loss: 0.00001341
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001340
Iteration 37/1000 | Loss: 0.00001339
Iteration 38/1000 | Loss: 0.00001339
Iteration 39/1000 | Loss: 0.00001339
Iteration 40/1000 | Loss: 0.00001339
Iteration 41/1000 | Loss: 0.00001339
Iteration 42/1000 | Loss: 0.00001339
Iteration 43/1000 | Loss: 0.00001339
Iteration 44/1000 | Loss: 0.00001338
Iteration 45/1000 | Loss: 0.00001337
Iteration 46/1000 | Loss: 0.00001336
Iteration 47/1000 | Loss: 0.00001336
Iteration 48/1000 | Loss: 0.00001336
Iteration 49/1000 | Loss: 0.00001335
Iteration 50/1000 | Loss: 0.00001335
Iteration 51/1000 | Loss: 0.00001335
Iteration 52/1000 | Loss: 0.00001335
Iteration 53/1000 | Loss: 0.00001334
Iteration 54/1000 | Loss: 0.00001334
Iteration 55/1000 | Loss: 0.00001334
Iteration 56/1000 | Loss: 0.00001333
Iteration 57/1000 | Loss: 0.00001333
Iteration 58/1000 | Loss: 0.00001332
Iteration 59/1000 | Loss: 0.00001332
Iteration 60/1000 | Loss: 0.00001331
Iteration 61/1000 | Loss: 0.00001331
Iteration 62/1000 | Loss: 0.00001330
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001329
Iteration 66/1000 | Loss: 0.00001329
Iteration 67/1000 | Loss: 0.00001329
Iteration 68/1000 | Loss: 0.00001329
Iteration 69/1000 | Loss: 0.00001329
Iteration 70/1000 | Loss: 0.00001328
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001326
Iteration 75/1000 | Loss: 0.00001323
Iteration 76/1000 | Loss: 0.00001323
Iteration 77/1000 | Loss: 0.00001323
Iteration 78/1000 | Loss: 0.00001322
Iteration 79/1000 | Loss: 0.00001322
Iteration 80/1000 | Loss: 0.00001321
Iteration 81/1000 | Loss: 0.00001321
Iteration 82/1000 | Loss: 0.00001321
Iteration 83/1000 | Loss: 0.00001320
Iteration 84/1000 | Loss: 0.00001320
Iteration 85/1000 | Loss: 0.00001320
Iteration 86/1000 | Loss: 0.00001320
Iteration 87/1000 | Loss: 0.00001319
Iteration 88/1000 | Loss: 0.00001319
Iteration 89/1000 | Loss: 0.00001319
Iteration 90/1000 | Loss: 0.00001319
Iteration 91/1000 | Loss: 0.00001319
Iteration 92/1000 | Loss: 0.00001319
Iteration 93/1000 | Loss: 0.00001319
Iteration 94/1000 | Loss: 0.00001319
Iteration 95/1000 | Loss: 0.00001319
Iteration 96/1000 | Loss: 0.00001319
Iteration 97/1000 | Loss: 0.00001319
Iteration 98/1000 | Loss: 0.00001319
Iteration 99/1000 | Loss: 0.00001319
Iteration 100/1000 | Loss: 0.00001319
Iteration 101/1000 | Loss: 0.00001319
Iteration 102/1000 | Loss: 0.00001319
Iteration 103/1000 | Loss: 0.00001319
Iteration 104/1000 | Loss: 0.00001319
Iteration 105/1000 | Loss: 0.00001319
Iteration 106/1000 | Loss: 0.00001319
Iteration 107/1000 | Loss: 0.00001319
Iteration 108/1000 | Loss: 0.00001319
Iteration 109/1000 | Loss: 0.00001319
Iteration 110/1000 | Loss: 0.00001319
Iteration 111/1000 | Loss: 0.00001319
Iteration 112/1000 | Loss: 0.00001319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.318823251494905e-05, 1.318823251494905e-05, 1.318823251494905e-05, 1.318823251494905e-05, 1.318823251494905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.318823251494905e-05

Optimization complete. Final v2v error: 3.0314691066741943 mm

Highest mean error: 3.3368496894836426 mm for frame 106

Lowest mean error: 2.6824233531951904 mm for frame 4

Saving results

Total time: 31.023221492767334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475005
Iteration 2/25 | Loss: 0.00102028
Iteration 3/25 | Loss: 0.00094020
Iteration 4/25 | Loss: 0.00093007
Iteration 5/25 | Loss: 0.00092708
Iteration 6/25 | Loss: 0.00092629
Iteration 7/25 | Loss: 0.00092629
Iteration 8/25 | Loss: 0.00092629
Iteration 9/25 | Loss: 0.00092629
Iteration 10/25 | Loss: 0.00092629
Iteration 11/25 | Loss: 0.00092629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009262897074222565, 0.0009262897074222565, 0.0009262897074222565, 0.0009262897074222565, 0.0009262897074222565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009262897074222565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.72179127
Iteration 2/25 | Loss: 0.00075492
Iteration 3/25 | Loss: 0.00075491
Iteration 4/25 | Loss: 0.00075491
Iteration 5/25 | Loss: 0.00075491
Iteration 6/25 | Loss: 0.00075491
Iteration 7/25 | Loss: 0.00075491
Iteration 8/25 | Loss: 0.00075491
Iteration 9/25 | Loss: 0.00075491
Iteration 10/25 | Loss: 0.00075491
Iteration 11/25 | Loss: 0.00075491
Iteration 12/25 | Loss: 0.00075491
Iteration 13/25 | Loss: 0.00075491
Iteration 14/25 | Loss: 0.00075491
Iteration 15/25 | Loss: 0.00075491
Iteration 16/25 | Loss: 0.00075491
Iteration 17/25 | Loss: 0.00075491
Iteration 18/25 | Loss: 0.00075491
Iteration 19/25 | Loss: 0.00075491
Iteration 20/25 | Loss: 0.00075491
Iteration 21/25 | Loss: 0.00075491
Iteration 22/25 | Loss: 0.00075491
Iteration 23/25 | Loss: 0.00075491
Iteration 24/25 | Loss: 0.00075491
Iteration 25/25 | Loss: 0.00075491

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075491
Iteration 2/1000 | Loss: 0.00001508
Iteration 3/1000 | Loss: 0.00001023
Iteration 4/1000 | Loss: 0.00000935
Iteration 5/1000 | Loss: 0.00000873
Iteration 6/1000 | Loss: 0.00000846
Iteration 7/1000 | Loss: 0.00000826
Iteration 8/1000 | Loss: 0.00000826
Iteration 9/1000 | Loss: 0.00000825
Iteration 10/1000 | Loss: 0.00000821
Iteration 11/1000 | Loss: 0.00000820
Iteration 12/1000 | Loss: 0.00000818
Iteration 13/1000 | Loss: 0.00000818
Iteration 14/1000 | Loss: 0.00000810
Iteration 15/1000 | Loss: 0.00000808
Iteration 16/1000 | Loss: 0.00000803
Iteration 17/1000 | Loss: 0.00000803
Iteration 18/1000 | Loss: 0.00000803
Iteration 19/1000 | Loss: 0.00000803
Iteration 20/1000 | Loss: 0.00000803
Iteration 21/1000 | Loss: 0.00000803
Iteration 22/1000 | Loss: 0.00000800
Iteration 23/1000 | Loss: 0.00000799
Iteration 24/1000 | Loss: 0.00000799
Iteration 25/1000 | Loss: 0.00000799
Iteration 26/1000 | Loss: 0.00000799
Iteration 27/1000 | Loss: 0.00000799
Iteration 28/1000 | Loss: 0.00000799
Iteration 29/1000 | Loss: 0.00000799
Iteration 30/1000 | Loss: 0.00000799
Iteration 31/1000 | Loss: 0.00000798
Iteration 32/1000 | Loss: 0.00000798
Iteration 33/1000 | Loss: 0.00000797
Iteration 34/1000 | Loss: 0.00000797
Iteration 35/1000 | Loss: 0.00000797
Iteration 36/1000 | Loss: 0.00000796
Iteration 37/1000 | Loss: 0.00000796
Iteration 38/1000 | Loss: 0.00000796
Iteration 39/1000 | Loss: 0.00000795
Iteration 40/1000 | Loss: 0.00000795
Iteration 41/1000 | Loss: 0.00000794
Iteration 42/1000 | Loss: 0.00000794
Iteration 43/1000 | Loss: 0.00000794
Iteration 44/1000 | Loss: 0.00000793
Iteration 45/1000 | Loss: 0.00000793
Iteration 46/1000 | Loss: 0.00000793
Iteration 47/1000 | Loss: 0.00000793
Iteration 48/1000 | Loss: 0.00000793
Iteration 49/1000 | Loss: 0.00000793
Iteration 50/1000 | Loss: 0.00000793
Iteration 51/1000 | Loss: 0.00000793
Iteration 52/1000 | Loss: 0.00000793
Iteration 53/1000 | Loss: 0.00000792
Iteration 54/1000 | Loss: 0.00000792
Iteration 55/1000 | Loss: 0.00000792
Iteration 56/1000 | Loss: 0.00000792
Iteration 57/1000 | Loss: 0.00000792
Iteration 58/1000 | Loss: 0.00000792
Iteration 59/1000 | Loss: 0.00000792
Iteration 60/1000 | Loss: 0.00000792
Iteration 61/1000 | Loss: 0.00000792
Iteration 62/1000 | Loss: 0.00000791
Iteration 63/1000 | Loss: 0.00000790
Iteration 64/1000 | Loss: 0.00000790
Iteration 65/1000 | Loss: 0.00000790
Iteration 66/1000 | Loss: 0.00000790
Iteration 67/1000 | Loss: 0.00000790
Iteration 68/1000 | Loss: 0.00000790
Iteration 69/1000 | Loss: 0.00000790
Iteration 70/1000 | Loss: 0.00000790
Iteration 71/1000 | Loss: 0.00000789
Iteration 72/1000 | Loss: 0.00000789
Iteration 73/1000 | Loss: 0.00000789
Iteration 74/1000 | Loss: 0.00000789
Iteration 75/1000 | Loss: 0.00000788
Iteration 76/1000 | Loss: 0.00000787
Iteration 77/1000 | Loss: 0.00000787
Iteration 78/1000 | Loss: 0.00000787
Iteration 79/1000 | Loss: 0.00000787
Iteration 80/1000 | Loss: 0.00000787
Iteration 81/1000 | Loss: 0.00000787
Iteration 82/1000 | Loss: 0.00000787
Iteration 83/1000 | Loss: 0.00000787
Iteration 84/1000 | Loss: 0.00000786
Iteration 85/1000 | Loss: 0.00000786
Iteration 86/1000 | Loss: 0.00000786
Iteration 87/1000 | Loss: 0.00000786
Iteration 88/1000 | Loss: 0.00000786
Iteration 89/1000 | Loss: 0.00000785
Iteration 90/1000 | Loss: 0.00000785
Iteration 91/1000 | Loss: 0.00000785
Iteration 92/1000 | Loss: 0.00000785
Iteration 93/1000 | Loss: 0.00000785
Iteration 94/1000 | Loss: 0.00000785
Iteration 95/1000 | Loss: 0.00000784
Iteration 96/1000 | Loss: 0.00000784
Iteration 97/1000 | Loss: 0.00000784
Iteration 98/1000 | Loss: 0.00000784
Iteration 99/1000 | Loss: 0.00000784
Iteration 100/1000 | Loss: 0.00000784
Iteration 101/1000 | Loss: 0.00000784
Iteration 102/1000 | Loss: 0.00000784
Iteration 103/1000 | Loss: 0.00000784
Iteration 104/1000 | Loss: 0.00000784
Iteration 105/1000 | Loss: 0.00000784
Iteration 106/1000 | Loss: 0.00000783
Iteration 107/1000 | Loss: 0.00000783
Iteration 108/1000 | Loss: 0.00000783
Iteration 109/1000 | Loss: 0.00000783
Iteration 110/1000 | Loss: 0.00000783
Iteration 111/1000 | Loss: 0.00000783
Iteration 112/1000 | Loss: 0.00000783
Iteration 113/1000 | Loss: 0.00000783
Iteration 114/1000 | Loss: 0.00000783
Iteration 115/1000 | Loss: 0.00000783
Iteration 116/1000 | Loss: 0.00000783
Iteration 117/1000 | Loss: 0.00000783
Iteration 118/1000 | Loss: 0.00000783
Iteration 119/1000 | Loss: 0.00000783
Iteration 120/1000 | Loss: 0.00000783
Iteration 121/1000 | Loss: 0.00000783
Iteration 122/1000 | Loss: 0.00000783
Iteration 123/1000 | Loss: 0.00000783
Iteration 124/1000 | Loss: 0.00000783
Iteration 125/1000 | Loss: 0.00000783
Iteration 126/1000 | Loss: 0.00000783
Iteration 127/1000 | Loss: 0.00000783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [7.834866664779838e-06, 7.834866664779838e-06, 7.834866664779838e-06, 7.834866664779838e-06, 7.834866664779838e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.834866664779838e-06

Optimization complete. Final v2v error: 2.3699090480804443 mm

Highest mean error: 2.8914451599121094 mm for frame 187

Lowest mean error: 1.9787440299987793 mm for frame 6

Saving results

Total time: 31.31051993370056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860382
Iteration 2/25 | Loss: 0.00122043
Iteration 3/25 | Loss: 0.00103968
Iteration 4/25 | Loss: 0.00100800
Iteration 5/25 | Loss: 0.00099960
Iteration 6/25 | Loss: 0.00099878
Iteration 7/25 | Loss: 0.00099878
Iteration 8/25 | Loss: 0.00099878
Iteration 9/25 | Loss: 0.00099878
Iteration 10/25 | Loss: 0.00099878
Iteration 11/25 | Loss: 0.00099878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009987764060497284, 0.0009987764060497284, 0.0009987764060497284, 0.0009987764060497284, 0.0009987764060497284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009987764060497284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92654932
Iteration 2/25 | Loss: 0.00039474
Iteration 3/25 | Loss: 0.00039473
Iteration 4/25 | Loss: 0.00039473
Iteration 5/25 | Loss: 0.00039473
Iteration 6/25 | Loss: 0.00039473
Iteration 7/25 | Loss: 0.00039473
Iteration 8/25 | Loss: 0.00039473
Iteration 9/25 | Loss: 0.00039473
Iteration 10/25 | Loss: 0.00039473
Iteration 11/25 | Loss: 0.00039473
Iteration 12/25 | Loss: 0.00039473
Iteration 13/25 | Loss: 0.00039473
Iteration 14/25 | Loss: 0.00039473
Iteration 15/25 | Loss: 0.00039473
Iteration 16/25 | Loss: 0.00039473
Iteration 17/25 | Loss: 0.00039473
Iteration 18/25 | Loss: 0.00039473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00039472818025387824, 0.00039472818025387824, 0.00039472818025387824, 0.00039472818025387824, 0.00039472818025387824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039472818025387824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039473
Iteration 2/1000 | Loss: 0.00003211
Iteration 3/1000 | Loss: 0.00002545
Iteration 4/1000 | Loss: 0.00002386
Iteration 5/1000 | Loss: 0.00002205
Iteration 6/1000 | Loss: 0.00002114
Iteration 7/1000 | Loss: 0.00002051
Iteration 8/1000 | Loss: 0.00002019
Iteration 9/1000 | Loss: 0.00001985
Iteration 10/1000 | Loss: 0.00001969
Iteration 11/1000 | Loss: 0.00001957
Iteration 12/1000 | Loss: 0.00001956
Iteration 13/1000 | Loss: 0.00001949
Iteration 14/1000 | Loss: 0.00001947
Iteration 15/1000 | Loss: 0.00001938
Iteration 16/1000 | Loss: 0.00001938
Iteration 17/1000 | Loss: 0.00001937
Iteration 18/1000 | Loss: 0.00001932
Iteration 19/1000 | Loss: 0.00001932
Iteration 20/1000 | Loss: 0.00001931
Iteration 21/1000 | Loss: 0.00001930
Iteration 22/1000 | Loss: 0.00001930
Iteration 23/1000 | Loss: 0.00001930
Iteration 24/1000 | Loss: 0.00001930
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001929
Iteration 27/1000 | Loss: 0.00001929
Iteration 28/1000 | Loss: 0.00001929
Iteration 29/1000 | Loss: 0.00001929
Iteration 30/1000 | Loss: 0.00001928
Iteration 31/1000 | Loss: 0.00001928
Iteration 32/1000 | Loss: 0.00001928
Iteration 33/1000 | Loss: 0.00001928
Iteration 34/1000 | Loss: 0.00001928
Iteration 35/1000 | Loss: 0.00001928
Iteration 36/1000 | Loss: 0.00001928
Iteration 37/1000 | Loss: 0.00001927
Iteration 38/1000 | Loss: 0.00001927
Iteration 39/1000 | Loss: 0.00001927
Iteration 40/1000 | Loss: 0.00001927
Iteration 41/1000 | Loss: 0.00001927
Iteration 42/1000 | Loss: 0.00001926
Iteration 43/1000 | Loss: 0.00001926
Iteration 44/1000 | Loss: 0.00001926
Iteration 45/1000 | Loss: 0.00001926
Iteration 46/1000 | Loss: 0.00001926
Iteration 47/1000 | Loss: 0.00001926
Iteration 48/1000 | Loss: 0.00001926
Iteration 49/1000 | Loss: 0.00001926
Iteration 50/1000 | Loss: 0.00001925
Iteration 51/1000 | Loss: 0.00001925
Iteration 52/1000 | Loss: 0.00001925
Iteration 53/1000 | Loss: 0.00001925
Iteration 54/1000 | Loss: 0.00001925
Iteration 55/1000 | Loss: 0.00001925
Iteration 56/1000 | Loss: 0.00001925
Iteration 57/1000 | Loss: 0.00001925
Iteration 58/1000 | Loss: 0.00001925
Iteration 59/1000 | Loss: 0.00001925
Iteration 60/1000 | Loss: 0.00001925
Iteration 61/1000 | Loss: 0.00001925
Iteration 62/1000 | Loss: 0.00001925
Iteration 63/1000 | Loss: 0.00001925
Iteration 64/1000 | Loss: 0.00001925
Iteration 65/1000 | Loss: 0.00001925
Iteration 66/1000 | Loss: 0.00001925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [1.9247976524638943e-05, 1.9247976524638943e-05, 1.9247976524638943e-05, 1.9247976524638943e-05, 1.9247976524638943e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9247976524638943e-05

Optimization complete. Final v2v error: 3.7075016498565674 mm

Highest mean error: 4.278308868408203 mm for frame 239

Lowest mean error: 3.261707067489624 mm for frame 3

Saving results

Total time: 32.748536109924316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025388
Iteration 2/25 | Loss: 0.00266067
Iteration 3/25 | Loss: 0.00168517
Iteration 4/25 | Loss: 0.00156594
Iteration 5/25 | Loss: 0.00155177
Iteration 6/25 | Loss: 0.00146724
Iteration 7/25 | Loss: 0.00121251
Iteration 8/25 | Loss: 0.00112411
Iteration 9/25 | Loss: 0.00109459
Iteration 10/25 | Loss: 0.00108729
Iteration 11/25 | Loss: 0.00107933
Iteration 12/25 | Loss: 0.00107845
Iteration 13/25 | Loss: 0.00107764
Iteration 14/25 | Loss: 0.00107958
Iteration 15/25 | Loss: 0.00107934
Iteration 16/25 | Loss: 0.00107594
Iteration 17/25 | Loss: 0.00107448
Iteration 18/25 | Loss: 0.00107199
Iteration 19/25 | Loss: 0.00107412
Iteration 20/25 | Loss: 0.00107148
Iteration 21/25 | Loss: 0.00107121
Iteration 22/25 | Loss: 0.00107116
Iteration 23/25 | Loss: 0.00107116
Iteration 24/25 | Loss: 0.00107116
Iteration 25/25 | Loss: 0.00107115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30708575
Iteration 2/25 | Loss: 0.00153295
Iteration 3/25 | Loss: 0.00125802
Iteration 4/25 | Loss: 0.00125802
Iteration 5/25 | Loss: 0.00125802
Iteration 6/25 | Loss: 0.00125802
Iteration 7/25 | Loss: 0.00125802
Iteration 8/25 | Loss: 0.00125802
Iteration 9/25 | Loss: 0.00125802
Iteration 10/25 | Loss: 0.00125802
Iteration 11/25 | Loss: 0.00125802
Iteration 12/25 | Loss: 0.00125802
Iteration 13/25 | Loss: 0.00125802
Iteration 14/25 | Loss: 0.00125802
Iteration 15/25 | Loss: 0.00125802
Iteration 16/25 | Loss: 0.00125802
Iteration 17/25 | Loss: 0.00125802
Iteration 18/25 | Loss: 0.00125802
Iteration 19/25 | Loss: 0.00125802
Iteration 20/25 | Loss: 0.00125802
Iteration 21/25 | Loss: 0.00125802
Iteration 22/25 | Loss: 0.00125802
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012580197071656585, 0.0012580197071656585, 0.0012580197071656585, 0.0012580197071656585, 0.0012580197071656585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012580197071656585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125802
Iteration 2/1000 | Loss: 0.00033591
Iteration 3/1000 | Loss: 0.00020524
Iteration 4/1000 | Loss: 0.00011675
Iteration 5/1000 | Loss: 0.00008477
Iteration 6/1000 | Loss: 0.00019011
Iteration 7/1000 | Loss: 0.00007928
Iteration 8/1000 | Loss: 0.00010134
Iteration 9/1000 | Loss: 0.00003744
Iteration 10/1000 | Loss: 0.00003470
Iteration 11/1000 | Loss: 0.00027711
Iteration 12/1000 | Loss: 0.00004166
Iteration 13/1000 | Loss: 0.00027330
Iteration 14/1000 | Loss: 0.00018079
Iteration 15/1000 | Loss: 0.00016432
Iteration 16/1000 | Loss: 0.00004534
Iteration 17/1000 | Loss: 0.00004576
Iteration 18/1000 | Loss: 0.00004954
Iteration 19/1000 | Loss: 0.00003059
Iteration 20/1000 | Loss: 0.00027581
Iteration 21/1000 | Loss: 0.00010261
Iteration 22/1000 | Loss: 0.00003165
Iteration 23/1000 | Loss: 0.00002785
Iteration 24/1000 | Loss: 0.00002668
Iteration 25/1000 | Loss: 0.00212818
Iteration 26/1000 | Loss: 0.00226609
Iteration 27/1000 | Loss: 0.00009744
Iteration 28/1000 | Loss: 0.00002557
Iteration 29/1000 | Loss: 0.00002227
Iteration 30/1000 | Loss: 0.00005366
Iteration 31/1000 | Loss: 0.00002096
Iteration 32/1000 | Loss: 0.00002035
Iteration 33/1000 | Loss: 0.00001999
Iteration 34/1000 | Loss: 0.00001968
Iteration 35/1000 | Loss: 0.00001950
Iteration 36/1000 | Loss: 0.00001946
Iteration 37/1000 | Loss: 0.00001941
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001921
Iteration 40/1000 | Loss: 0.00001921
Iteration 41/1000 | Loss: 0.00001920
Iteration 42/1000 | Loss: 0.00001919
Iteration 43/1000 | Loss: 0.00001918
Iteration 44/1000 | Loss: 0.00001917
Iteration 45/1000 | Loss: 0.00001917
Iteration 46/1000 | Loss: 0.00001917
Iteration 47/1000 | Loss: 0.00001916
Iteration 48/1000 | Loss: 0.00001916
Iteration 49/1000 | Loss: 0.00001916
Iteration 50/1000 | Loss: 0.00001915
Iteration 51/1000 | Loss: 0.00001915
Iteration 52/1000 | Loss: 0.00001915
Iteration 53/1000 | Loss: 0.00001914
Iteration 54/1000 | Loss: 0.00001914
Iteration 55/1000 | Loss: 0.00001913
Iteration 56/1000 | Loss: 0.00001913
Iteration 57/1000 | Loss: 0.00001913
Iteration 58/1000 | Loss: 0.00001913
Iteration 59/1000 | Loss: 0.00001913
Iteration 60/1000 | Loss: 0.00001912
Iteration 61/1000 | Loss: 0.00001912
Iteration 62/1000 | Loss: 0.00001911
Iteration 63/1000 | Loss: 0.00001911
Iteration 64/1000 | Loss: 0.00001911
Iteration 65/1000 | Loss: 0.00001910
Iteration 66/1000 | Loss: 0.00001910
Iteration 67/1000 | Loss: 0.00001910
Iteration 68/1000 | Loss: 0.00001909
Iteration 69/1000 | Loss: 0.00001909
Iteration 70/1000 | Loss: 0.00001909
Iteration 71/1000 | Loss: 0.00001908
Iteration 72/1000 | Loss: 0.00001908
Iteration 73/1000 | Loss: 0.00001908
Iteration 74/1000 | Loss: 0.00001908
Iteration 75/1000 | Loss: 0.00001907
Iteration 76/1000 | Loss: 0.00001907
Iteration 77/1000 | Loss: 0.00001907
Iteration 78/1000 | Loss: 0.00001907
Iteration 79/1000 | Loss: 0.00001907
Iteration 80/1000 | Loss: 0.00001906
Iteration 81/1000 | Loss: 0.00001906
Iteration 82/1000 | Loss: 0.00001906
Iteration 83/1000 | Loss: 0.00001906
Iteration 84/1000 | Loss: 0.00001906
Iteration 85/1000 | Loss: 0.00001905
Iteration 86/1000 | Loss: 0.00001905
Iteration 87/1000 | Loss: 0.00001905
Iteration 88/1000 | Loss: 0.00001905
Iteration 89/1000 | Loss: 0.00001905
Iteration 90/1000 | Loss: 0.00001905
Iteration 91/1000 | Loss: 0.00001905
Iteration 92/1000 | Loss: 0.00001905
Iteration 93/1000 | Loss: 0.00001904
Iteration 94/1000 | Loss: 0.00001904
Iteration 95/1000 | Loss: 0.00001904
Iteration 96/1000 | Loss: 0.00001903
Iteration 97/1000 | Loss: 0.00001902
Iteration 98/1000 | Loss: 0.00001902
Iteration 99/1000 | Loss: 0.00001902
Iteration 100/1000 | Loss: 0.00001902
Iteration 101/1000 | Loss: 0.00001901
Iteration 102/1000 | Loss: 0.00001901
Iteration 103/1000 | Loss: 0.00001901
Iteration 104/1000 | Loss: 0.00001900
Iteration 105/1000 | Loss: 0.00001900
Iteration 106/1000 | Loss: 0.00001900
Iteration 107/1000 | Loss: 0.00001899
Iteration 108/1000 | Loss: 0.00001899
Iteration 109/1000 | Loss: 0.00001899
Iteration 110/1000 | Loss: 0.00001898
Iteration 111/1000 | Loss: 0.00001898
Iteration 112/1000 | Loss: 0.00001898
Iteration 113/1000 | Loss: 0.00001898
Iteration 114/1000 | Loss: 0.00001897
Iteration 115/1000 | Loss: 0.00001897
Iteration 116/1000 | Loss: 0.00001897
Iteration 117/1000 | Loss: 0.00001897
Iteration 118/1000 | Loss: 0.00001897
Iteration 119/1000 | Loss: 0.00001897
Iteration 120/1000 | Loss: 0.00001897
Iteration 121/1000 | Loss: 0.00001897
Iteration 122/1000 | Loss: 0.00001897
Iteration 123/1000 | Loss: 0.00001896
Iteration 124/1000 | Loss: 0.00001896
Iteration 125/1000 | Loss: 0.00001896
Iteration 126/1000 | Loss: 0.00001896
Iteration 127/1000 | Loss: 0.00001896
Iteration 128/1000 | Loss: 0.00001896
Iteration 129/1000 | Loss: 0.00001896
Iteration 130/1000 | Loss: 0.00001896
Iteration 131/1000 | Loss: 0.00001895
Iteration 132/1000 | Loss: 0.00001895
Iteration 133/1000 | Loss: 0.00001895
Iteration 134/1000 | Loss: 0.00001895
Iteration 135/1000 | Loss: 0.00001895
Iteration 136/1000 | Loss: 0.00001895
Iteration 137/1000 | Loss: 0.00001895
Iteration 138/1000 | Loss: 0.00001895
Iteration 139/1000 | Loss: 0.00001894
Iteration 140/1000 | Loss: 0.00001894
Iteration 141/1000 | Loss: 0.00001894
Iteration 142/1000 | Loss: 0.00001894
Iteration 143/1000 | Loss: 0.00001894
Iteration 144/1000 | Loss: 0.00001893
Iteration 145/1000 | Loss: 0.00001893
Iteration 146/1000 | Loss: 0.00001893
Iteration 147/1000 | Loss: 0.00001893
Iteration 148/1000 | Loss: 0.00001892
Iteration 149/1000 | Loss: 0.00001892
Iteration 150/1000 | Loss: 0.00001892
Iteration 151/1000 | Loss: 0.00001892
Iteration 152/1000 | Loss: 0.00001892
Iteration 153/1000 | Loss: 0.00001892
Iteration 154/1000 | Loss: 0.00001892
Iteration 155/1000 | Loss: 0.00001892
Iteration 156/1000 | Loss: 0.00001892
Iteration 157/1000 | Loss: 0.00001892
Iteration 158/1000 | Loss: 0.00001891
Iteration 159/1000 | Loss: 0.00001891
Iteration 160/1000 | Loss: 0.00001891
Iteration 161/1000 | Loss: 0.00001891
Iteration 162/1000 | Loss: 0.00001891
Iteration 163/1000 | Loss: 0.00001891
Iteration 164/1000 | Loss: 0.00001891
Iteration 165/1000 | Loss: 0.00001891
Iteration 166/1000 | Loss: 0.00001891
Iteration 167/1000 | Loss: 0.00001891
Iteration 168/1000 | Loss: 0.00001891
Iteration 169/1000 | Loss: 0.00001891
Iteration 170/1000 | Loss: 0.00001891
Iteration 171/1000 | Loss: 0.00001890
Iteration 172/1000 | Loss: 0.00001890
Iteration 173/1000 | Loss: 0.00001890
Iteration 174/1000 | Loss: 0.00001890
Iteration 175/1000 | Loss: 0.00001890
Iteration 176/1000 | Loss: 0.00001890
Iteration 177/1000 | Loss: 0.00001890
Iteration 178/1000 | Loss: 0.00001890
Iteration 179/1000 | Loss: 0.00001889
Iteration 180/1000 | Loss: 0.00001889
Iteration 181/1000 | Loss: 0.00001889
Iteration 182/1000 | Loss: 0.00001889
Iteration 183/1000 | Loss: 0.00001889
Iteration 184/1000 | Loss: 0.00001889
Iteration 185/1000 | Loss: 0.00001889
Iteration 186/1000 | Loss: 0.00001889
Iteration 187/1000 | Loss: 0.00001889
Iteration 188/1000 | Loss: 0.00001889
Iteration 189/1000 | Loss: 0.00001889
Iteration 190/1000 | Loss: 0.00001889
Iteration 191/1000 | Loss: 0.00001888
Iteration 192/1000 | Loss: 0.00001888
Iteration 193/1000 | Loss: 0.00001888
Iteration 194/1000 | Loss: 0.00001888
Iteration 195/1000 | Loss: 0.00001888
Iteration 196/1000 | Loss: 0.00001888
Iteration 197/1000 | Loss: 0.00001888
Iteration 198/1000 | Loss: 0.00001888
Iteration 199/1000 | Loss: 0.00001888
Iteration 200/1000 | Loss: 0.00001888
Iteration 201/1000 | Loss: 0.00001888
Iteration 202/1000 | Loss: 0.00001888
Iteration 203/1000 | Loss: 0.00001888
Iteration 204/1000 | Loss: 0.00001888
Iteration 205/1000 | Loss: 0.00001888
Iteration 206/1000 | Loss: 0.00001888
Iteration 207/1000 | Loss: 0.00001887
Iteration 208/1000 | Loss: 0.00001887
Iteration 209/1000 | Loss: 0.00001887
Iteration 210/1000 | Loss: 0.00001887
Iteration 211/1000 | Loss: 0.00001887
Iteration 212/1000 | Loss: 0.00001887
Iteration 213/1000 | Loss: 0.00001887
Iteration 214/1000 | Loss: 0.00001887
Iteration 215/1000 | Loss: 0.00001887
Iteration 216/1000 | Loss: 0.00001887
Iteration 217/1000 | Loss: 0.00001887
Iteration 218/1000 | Loss: 0.00001887
Iteration 219/1000 | Loss: 0.00001887
Iteration 220/1000 | Loss: 0.00001887
Iteration 221/1000 | Loss: 0.00001887
Iteration 222/1000 | Loss: 0.00001887
Iteration 223/1000 | Loss: 0.00001887
Iteration 224/1000 | Loss: 0.00001886
Iteration 225/1000 | Loss: 0.00001886
Iteration 226/1000 | Loss: 0.00001886
Iteration 227/1000 | Loss: 0.00001886
Iteration 228/1000 | Loss: 0.00001886
Iteration 229/1000 | Loss: 0.00001886
Iteration 230/1000 | Loss: 0.00001886
Iteration 231/1000 | Loss: 0.00001886
Iteration 232/1000 | Loss: 0.00001886
Iteration 233/1000 | Loss: 0.00001886
Iteration 234/1000 | Loss: 0.00001886
Iteration 235/1000 | Loss: 0.00001886
Iteration 236/1000 | Loss: 0.00001886
Iteration 237/1000 | Loss: 0.00001886
Iteration 238/1000 | Loss: 0.00001885
Iteration 239/1000 | Loss: 0.00001885
Iteration 240/1000 | Loss: 0.00001885
Iteration 241/1000 | Loss: 0.00001885
Iteration 242/1000 | Loss: 0.00001885
Iteration 243/1000 | Loss: 0.00001885
Iteration 244/1000 | Loss: 0.00001885
Iteration 245/1000 | Loss: 0.00001885
Iteration 246/1000 | Loss: 0.00001885
Iteration 247/1000 | Loss: 0.00001885
Iteration 248/1000 | Loss: 0.00001885
Iteration 249/1000 | Loss: 0.00001885
Iteration 250/1000 | Loss: 0.00001885
Iteration 251/1000 | Loss: 0.00001885
Iteration 252/1000 | Loss: 0.00001885
Iteration 253/1000 | Loss: 0.00001885
Iteration 254/1000 | Loss: 0.00001885
Iteration 255/1000 | Loss: 0.00001885
Iteration 256/1000 | Loss: 0.00001885
Iteration 257/1000 | Loss: 0.00001885
Iteration 258/1000 | Loss: 0.00001885
Iteration 259/1000 | Loss: 0.00001885
Iteration 260/1000 | Loss: 0.00001884
Iteration 261/1000 | Loss: 0.00001884
Iteration 262/1000 | Loss: 0.00001884
Iteration 263/1000 | Loss: 0.00001884
Iteration 264/1000 | Loss: 0.00001884
Iteration 265/1000 | Loss: 0.00001884
Iteration 266/1000 | Loss: 0.00001884
Iteration 267/1000 | Loss: 0.00001884
Iteration 268/1000 | Loss: 0.00001884
Iteration 269/1000 | Loss: 0.00001884
Iteration 270/1000 | Loss: 0.00001884
Iteration 271/1000 | Loss: 0.00001884
Iteration 272/1000 | Loss: 0.00001884
Iteration 273/1000 | Loss: 0.00001884
Iteration 274/1000 | Loss: 0.00001884
Iteration 275/1000 | Loss: 0.00001884
Iteration 276/1000 | Loss: 0.00001884
Iteration 277/1000 | Loss: 0.00001884
Iteration 278/1000 | Loss: 0.00001884
Iteration 279/1000 | Loss: 0.00001883
Iteration 280/1000 | Loss: 0.00001883
Iteration 281/1000 | Loss: 0.00001883
Iteration 282/1000 | Loss: 0.00001883
Iteration 283/1000 | Loss: 0.00001883
Iteration 284/1000 | Loss: 0.00001883
Iteration 285/1000 | Loss: 0.00001883
Iteration 286/1000 | Loss: 0.00001883
Iteration 287/1000 | Loss: 0.00001883
Iteration 288/1000 | Loss: 0.00001883
Iteration 289/1000 | Loss: 0.00001883
Iteration 290/1000 | Loss: 0.00001883
Iteration 291/1000 | Loss: 0.00001883
Iteration 292/1000 | Loss: 0.00001883
Iteration 293/1000 | Loss: 0.00001883
Iteration 294/1000 | Loss: 0.00001883
Iteration 295/1000 | Loss: 0.00001883
Iteration 296/1000 | Loss: 0.00001882
Iteration 297/1000 | Loss: 0.00001882
Iteration 298/1000 | Loss: 0.00001882
Iteration 299/1000 | Loss: 0.00001882
Iteration 300/1000 | Loss: 0.00001882
Iteration 301/1000 | Loss: 0.00001882
Iteration 302/1000 | Loss: 0.00001882
Iteration 303/1000 | Loss: 0.00001882
Iteration 304/1000 | Loss: 0.00001882
Iteration 305/1000 | Loss: 0.00001882
Iteration 306/1000 | Loss: 0.00001882
Iteration 307/1000 | Loss: 0.00001882
Iteration 308/1000 | Loss: 0.00001882
Iteration 309/1000 | Loss: 0.00001882
Iteration 310/1000 | Loss: 0.00001881
Iteration 311/1000 | Loss: 0.00001881
Iteration 312/1000 | Loss: 0.00001881
Iteration 313/1000 | Loss: 0.00001881
Iteration 314/1000 | Loss: 0.00001881
Iteration 315/1000 | Loss: 0.00001881
Iteration 316/1000 | Loss: 0.00001881
Iteration 317/1000 | Loss: 0.00001881
Iteration 318/1000 | Loss: 0.00001880
Iteration 319/1000 | Loss: 0.00001880
Iteration 320/1000 | Loss: 0.00001880
Iteration 321/1000 | Loss: 0.00001880
Iteration 322/1000 | Loss: 0.00001880
Iteration 323/1000 | Loss: 0.00001880
Iteration 324/1000 | Loss: 0.00001880
Iteration 325/1000 | Loss: 0.00001880
Iteration 326/1000 | Loss: 0.00001880
Iteration 327/1000 | Loss: 0.00001880
Iteration 328/1000 | Loss: 0.00001880
Iteration 329/1000 | Loss: 0.00001880
Iteration 330/1000 | Loss: 0.00001880
Iteration 331/1000 | Loss: 0.00001880
Iteration 332/1000 | Loss: 0.00001880
Iteration 333/1000 | Loss: 0.00001880
Iteration 334/1000 | Loss: 0.00001880
Iteration 335/1000 | Loss: 0.00001880
Iteration 336/1000 | Loss: 0.00001880
Iteration 337/1000 | Loss: 0.00001880
Iteration 338/1000 | Loss: 0.00001880
Iteration 339/1000 | Loss: 0.00001879
Iteration 340/1000 | Loss: 0.00001879
Iteration 341/1000 | Loss: 0.00001879
Iteration 342/1000 | Loss: 0.00001879
Iteration 343/1000 | Loss: 0.00001879
Iteration 344/1000 | Loss: 0.00001879
Iteration 345/1000 | Loss: 0.00001879
Iteration 346/1000 | Loss: 0.00001879
Iteration 347/1000 | Loss: 0.00001879
Iteration 348/1000 | Loss: 0.00001879
Iteration 349/1000 | Loss: 0.00001879
Iteration 350/1000 | Loss: 0.00001879
Iteration 351/1000 | Loss: 0.00001879
Iteration 352/1000 | Loss: 0.00001879
Iteration 353/1000 | Loss: 0.00001879
Iteration 354/1000 | Loss: 0.00001879
Iteration 355/1000 | Loss: 0.00001879
Iteration 356/1000 | Loss: 0.00001879
Iteration 357/1000 | Loss: 0.00001879
Iteration 358/1000 | Loss: 0.00001879
Iteration 359/1000 | Loss: 0.00001879
Iteration 360/1000 | Loss: 0.00001879
Iteration 361/1000 | Loss: 0.00001879
Iteration 362/1000 | Loss: 0.00001879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 362. Stopping optimization.
Last 5 losses: [1.8792217815644108e-05, 1.8792217815644108e-05, 1.8792217815644108e-05, 1.8792217815644108e-05, 1.8792217815644108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8792217815644108e-05

Optimization complete. Final v2v error: 3.092899799346924 mm

Highest mean error: 19.7987003326416 mm for frame 92

Lowest mean error: 2.6438629627227783 mm for frame 122

Saving results

Total time: 109.07100558280945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382730
Iteration 2/25 | Loss: 0.00106652
Iteration 3/25 | Loss: 0.00097762
Iteration 4/25 | Loss: 0.00096819
Iteration 5/25 | Loss: 0.00096556
Iteration 6/25 | Loss: 0.00096458
Iteration 7/25 | Loss: 0.00096458
Iteration 8/25 | Loss: 0.00096458
Iteration 9/25 | Loss: 0.00096458
Iteration 10/25 | Loss: 0.00096458
Iteration 11/25 | Loss: 0.00096458
Iteration 12/25 | Loss: 0.00096458
Iteration 13/25 | Loss: 0.00096458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009645774844102561, 0.0009645774844102561, 0.0009645774844102561, 0.0009645774844102561, 0.0009645774844102561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009645774844102561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32134187
Iteration 2/25 | Loss: 0.00091133
Iteration 3/25 | Loss: 0.00091133
Iteration 4/25 | Loss: 0.00091133
Iteration 5/25 | Loss: 0.00091133
Iteration 6/25 | Loss: 0.00091132
Iteration 7/25 | Loss: 0.00091132
Iteration 8/25 | Loss: 0.00091132
Iteration 9/25 | Loss: 0.00091132
Iteration 10/25 | Loss: 0.00091132
Iteration 11/25 | Loss: 0.00091132
Iteration 12/25 | Loss: 0.00091132
Iteration 13/25 | Loss: 0.00091132
Iteration 14/25 | Loss: 0.00091132
Iteration 15/25 | Loss: 0.00091132
Iteration 16/25 | Loss: 0.00091132
Iteration 17/25 | Loss: 0.00091132
Iteration 18/25 | Loss: 0.00091132
Iteration 19/25 | Loss: 0.00091132
Iteration 20/25 | Loss: 0.00091132
Iteration 21/25 | Loss: 0.00091132
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000911323179025203, 0.000911323179025203, 0.000911323179025203, 0.000911323179025203, 0.000911323179025203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000911323179025203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091132
Iteration 2/1000 | Loss: 0.00003093
Iteration 3/1000 | Loss: 0.00001454
Iteration 4/1000 | Loss: 0.00001282
Iteration 5/1000 | Loss: 0.00001209
Iteration 6/1000 | Loss: 0.00001162
Iteration 7/1000 | Loss: 0.00001119
Iteration 8/1000 | Loss: 0.00001096
Iteration 9/1000 | Loss: 0.00001077
Iteration 10/1000 | Loss: 0.00001067
Iteration 11/1000 | Loss: 0.00001065
Iteration 12/1000 | Loss: 0.00001065
Iteration 13/1000 | Loss: 0.00001063
Iteration 14/1000 | Loss: 0.00001062
Iteration 15/1000 | Loss: 0.00001061
Iteration 16/1000 | Loss: 0.00001061
Iteration 17/1000 | Loss: 0.00001060
Iteration 18/1000 | Loss: 0.00001060
Iteration 19/1000 | Loss: 0.00001060
Iteration 20/1000 | Loss: 0.00001060
Iteration 21/1000 | Loss: 0.00001059
Iteration 22/1000 | Loss: 0.00001059
Iteration 23/1000 | Loss: 0.00001059
Iteration 24/1000 | Loss: 0.00001058
Iteration 25/1000 | Loss: 0.00001057
Iteration 26/1000 | Loss: 0.00001056
Iteration 27/1000 | Loss: 0.00001056
Iteration 28/1000 | Loss: 0.00001056
Iteration 29/1000 | Loss: 0.00001055
Iteration 30/1000 | Loss: 0.00001055
Iteration 31/1000 | Loss: 0.00001055
Iteration 32/1000 | Loss: 0.00001054
Iteration 33/1000 | Loss: 0.00001054
Iteration 34/1000 | Loss: 0.00001054
Iteration 35/1000 | Loss: 0.00001054
Iteration 36/1000 | Loss: 0.00001053
Iteration 37/1000 | Loss: 0.00001053
Iteration 38/1000 | Loss: 0.00001053
Iteration 39/1000 | Loss: 0.00001052
Iteration 40/1000 | Loss: 0.00001052
Iteration 41/1000 | Loss: 0.00001052
Iteration 42/1000 | Loss: 0.00001052
Iteration 43/1000 | Loss: 0.00001052
Iteration 44/1000 | Loss: 0.00001051
Iteration 45/1000 | Loss: 0.00001051
Iteration 46/1000 | Loss: 0.00001051
Iteration 47/1000 | Loss: 0.00001051
Iteration 48/1000 | Loss: 0.00001050
Iteration 49/1000 | Loss: 0.00001050
Iteration 50/1000 | Loss: 0.00001049
Iteration 51/1000 | Loss: 0.00001049
Iteration 52/1000 | Loss: 0.00001049
Iteration 53/1000 | Loss: 0.00001049
Iteration 54/1000 | Loss: 0.00001049
Iteration 55/1000 | Loss: 0.00001049
Iteration 56/1000 | Loss: 0.00001049
Iteration 57/1000 | Loss: 0.00001049
Iteration 58/1000 | Loss: 0.00001048
Iteration 59/1000 | Loss: 0.00001048
Iteration 60/1000 | Loss: 0.00001048
Iteration 61/1000 | Loss: 0.00001048
Iteration 62/1000 | Loss: 0.00001048
Iteration 63/1000 | Loss: 0.00001048
Iteration 64/1000 | Loss: 0.00001048
Iteration 65/1000 | Loss: 0.00001048
Iteration 66/1000 | Loss: 0.00001047
Iteration 67/1000 | Loss: 0.00001047
Iteration 68/1000 | Loss: 0.00001047
Iteration 69/1000 | Loss: 0.00001047
Iteration 70/1000 | Loss: 0.00001047
Iteration 71/1000 | Loss: 0.00001047
Iteration 72/1000 | Loss: 0.00001047
Iteration 73/1000 | Loss: 0.00001046
Iteration 74/1000 | Loss: 0.00001046
Iteration 75/1000 | Loss: 0.00001046
Iteration 76/1000 | Loss: 0.00001046
Iteration 77/1000 | Loss: 0.00001045
Iteration 78/1000 | Loss: 0.00001045
Iteration 79/1000 | Loss: 0.00001045
Iteration 80/1000 | Loss: 0.00001045
Iteration 81/1000 | Loss: 0.00001045
Iteration 82/1000 | Loss: 0.00001045
Iteration 83/1000 | Loss: 0.00001045
Iteration 84/1000 | Loss: 0.00001044
Iteration 85/1000 | Loss: 0.00001044
Iteration 86/1000 | Loss: 0.00001044
Iteration 87/1000 | Loss: 0.00001044
Iteration 88/1000 | Loss: 0.00001044
Iteration 89/1000 | Loss: 0.00001044
Iteration 90/1000 | Loss: 0.00001044
Iteration 91/1000 | Loss: 0.00001044
Iteration 92/1000 | Loss: 0.00001044
Iteration 93/1000 | Loss: 0.00001044
Iteration 94/1000 | Loss: 0.00001044
Iteration 95/1000 | Loss: 0.00001044
Iteration 96/1000 | Loss: 0.00001043
Iteration 97/1000 | Loss: 0.00001043
Iteration 98/1000 | Loss: 0.00001043
Iteration 99/1000 | Loss: 0.00001043
Iteration 100/1000 | Loss: 0.00001043
Iteration 101/1000 | Loss: 0.00001043
Iteration 102/1000 | Loss: 0.00001043
Iteration 103/1000 | Loss: 0.00001043
Iteration 104/1000 | Loss: 0.00001043
Iteration 105/1000 | Loss: 0.00001042
Iteration 106/1000 | Loss: 0.00001042
Iteration 107/1000 | Loss: 0.00001042
Iteration 108/1000 | Loss: 0.00001042
Iteration 109/1000 | Loss: 0.00001042
Iteration 110/1000 | Loss: 0.00001042
Iteration 111/1000 | Loss: 0.00001042
Iteration 112/1000 | Loss: 0.00001042
Iteration 113/1000 | Loss: 0.00001042
Iteration 114/1000 | Loss: 0.00001042
Iteration 115/1000 | Loss: 0.00001042
Iteration 116/1000 | Loss: 0.00001042
Iteration 117/1000 | Loss: 0.00001042
Iteration 118/1000 | Loss: 0.00001042
Iteration 119/1000 | Loss: 0.00001041
Iteration 120/1000 | Loss: 0.00001041
Iteration 121/1000 | Loss: 0.00001041
Iteration 122/1000 | Loss: 0.00001041
Iteration 123/1000 | Loss: 0.00001041
Iteration 124/1000 | Loss: 0.00001041
Iteration 125/1000 | Loss: 0.00001041
Iteration 126/1000 | Loss: 0.00001041
Iteration 127/1000 | Loss: 0.00001041
Iteration 128/1000 | Loss: 0.00001040
Iteration 129/1000 | Loss: 0.00001040
Iteration 130/1000 | Loss: 0.00001040
Iteration 131/1000 | Loss: 0.00001040
Iteration 132/1000 | Loss: 0.00001040
Iteration 133/1000 | Loss: 0.00001040
Iteration 134/1000 | Loss: 0.00001040
Iteration 135/1000 | Loss: 0.00001040
Iteration 136/1000 | Loss: 0.00001040
Iteration 137/1000 | Loss: 0.00001040
Iteration 138/1000 | Loss: 0.00001040
Iteration 139/1000 | Loss: 0.00001040
Iteration 140/1000 | Loss: 0.00001040
Iteration 141/1000 | Loss: 0.00001040
Iteration 142/1000 | Loss: 0.00001040
Iteration 143/1000 | Loss: 0.00001040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.040139341057511e-05, 1.040139341057511e-05, 1.040139341057511e-05, 1.040139341057511e-05, 1.040139341057511e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.040139341057511e-05

Optimization complete. Final v2v error: 2.727386474609375 mm

Highest mean error: 3.1863999366760254 mm for frame 63

Lowest mean error: 2.358980894088745 mm for frame 100

Saving results

Total time: 32.645198822021484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395290
Iteration 2/25 | Loss: 0.00120096
Iteration 3/25 | Loss: 0.00100608
Iteration 4/25 | Loss: 0.00095466
Iteration 5/25 | Loss: 0.00094794
Iteration 6/25 | Loss: 0.00094623
Iteration 7/25 | Loss: 0.00094594
Iteration 8/25 | Loss: 0.00094594
Iteration 9/25 | Loss: 0.00094594
Iteration 10/25 | Loss: 0.00094594
Iteration 11/25 | Loss: 0.00094594
Iteration 12/25 | Loss: 0.00094594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009459374123252928, 0.0009459374123252928, 0.0009459374123252928, 0.0009459374123252928, 0.0009459374123252928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009459374123252928

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29499066
Iteration 2/25 | Loss: 0.00067772
Iteration 3/25 | Loss: 0.00067771
Iteration 4/25 | Loss: 0.00067771
Iteration 5/25 | Loss: 0.00067771
Iteration 6/25 | Loss: 0.00067771
Iteration 7/25 | Loss: 0.00067771
Iteration 8/25 | Loss: 0.00067771
Iteration 9/25 | Loss: 0.00067771
Iteration 10/25 | Loss: 0.00067771
Iteration 11/25 | Loss: 0.00067771
Iteration 12/25 | Loss: 0.00067771
Iteration 13/25 | Loss: 0.00067771
Iteration 14/25 | Loss: 0.00067771
Iteration 15/25 | Loss: 0.00067771
Iteration 16/25 | Loss: 0.00067771
Iteration 17/25 | Loss: 0.00067771
Iteration 18/25 | Loss: 0.00067771
Iteration 19/25 | Loss: 0.00067771
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006777118542231619, 0.0006777118542231619, 0.0006777118542231619, 0.0006777118542231619, 0.0006777118542231619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006777118542231619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067771
Iteration 2/1000 | Loss: 0.00001378
Iteration 3/1000 | Loss: 0.00001039
Iteration 4/1000 | Loss: 0.00000935
Iteration 5/1000 | Loss: 0.00000879
Iteration 6/1000 | Loss: 0.00000847
Iteration 7/1000 | Loss: 0.00000832
Iteration 8/1000 | Loss: 0.00000818
Iteration 9/1000 | Loss: 0.00000814
Iteration 10/1000 | Loss: 0.00000812
Iteration 11/1000 | Loss: 0.00000810
Iteration 12/1000 | Loss: 0.00000809
Iteration 13/1000 | Loss: 0.00000808
Iteration 14/1000 | Loss: 0.00000805
Iteration 15/1000 | Loss: 0.00000805
Iteration 16/1000 | Loss: 0.00000804
Iteration 17/1000 | Loss: 0.00000804
Iteration 18/1000 | Loss: 0.00000803
Iteration 19/1000 | Loss: 0.00000803
Iteration 20/1000 | Loss: 0.00000803
Iteration 21/1000 | Loss: 0.00000803
Iteration 22/1000 | Loss: 0.00000802
Iteration 23/1000 | Loss: 0.00000802
Iteration 24/1000 | Loss: 0.00000801
Iteration 25/1000 | Loss: 0.00000800
Iteration 26/1000 | Loss: 0.00000799
Iteration 27/1000 | Loss: 0.00000799
Iteration 28/1000 | Loss: 0.00000799
Iteration 29/1000 | Loss: 0.00000799
Iteration 30/1000 | Loss: 0.00000799
Iteration 31/1000 | Loss: 0.00000799
Iteration 32/1000 | Loss: 0.00000799
Iteration 33/1000 | Loss: 0.00000798
Iteration 34/1000 | Loss: 0.00000798
Iteration 35/1000 | Loss: 0.00000798
Iteration 36/1000 | Loss: 0.00000798
Iteration 37/1000 | Loss: 0.00000798
Iteration 38/1000 | Loss: 0.00000798
Iteration 39/1000 | Loss: 0.00000798
Iteration 40/1000 | Loss: 0.00000797
Iteration 41/1000 | Loss: 0.00000797
Iteration 42/1000 | Loss: 0.00000797
Iteration 43/1000 | Loss: 0.00000797
Iteration 44/1000 | Loss: 0.00000796
Iteration 45/1000 | Loss: 0.00000796
Iteration 46/1000 | Loss: 0.00000796
Iteration 47/1000 | Loss: 0.00000796
Iteration 48/1000 | Loss: 0.00000796
Iteration 49/1000 | Loss: 0.00000796
Iteration 50/1000 | Loss: 0.00000796
Iteration 51/1000 | Loss: 0.00000796
Iteration 52/1000 | Loss: 0.00000796
Iteration 53/1000 | Loss: 0.00000795
Iteration 54/1000 | Loss: 0.00000795
Iteration 55/1000 | Loss: 0.00000795
Iteration 56/1000 | Loss: 0.00000795
Iteration 57/1000 | Loss: 0.00000795
Iteration 58/1000 | Loss: 0.00000795
Iteration 59/1000 | Loss: 0.00000795
Iteration 60/1000 | Loss: 0.00000795
Iteration 61/1000 | Loss: 0.00000794
Iteration 62/1000 | Loss: 0.00000794
Iteration 63/1000 | Loss: 0.00000794
Iteration 64/1000 | Loss: 0.00000794
Iteration 65/1000 | Loss: 0.00000793
Iteration 66/1000 | Loss: 0.00000793
Iteration 67/1000 | Loss: 0.00000793
Iteration 68/1000 | Loss: 0.00000793
Iteration 69/1000 | Loss: 0.00000793
Iteration 70/1000 | Loss: 0.00000792
Iteration 71/1000 | Loss: 0.00000792
Iteration 72/1000 | Loss: 0.00000792
Iteration 73/1000 | Loss: 0.00000792
Iteration 74/1000 | Loss: 0.00000791
Iteration 75/1000 | Loss: 0.00000791
Iteration 76/1000 | Loss: 0.00000791
Iteration 77/1000 | Loss: 0.00000790
Iteration 78/1000 | Loss: 0.00000790
Iteration 79/1000 | Loss: 0.00000790
Iteration 80/1000 | Loss: 0.00000790
Iteration 81/1000 | Loss: 0.00000790
Iteration 82/1000 | Loss: 0.00000790
Iteration 83/1000 | Loss: 0.00000790
Iteration 84/1000 | Loss: 0.00000790
Iteration 85/1000 | Loss: 0.00000790
Iteration 86/1000 | Loss: 0.00000790
Iteration 87/1000 | Loss: 0.00000790
Iteration 88/1000 | Loss: 0.00000789
Iteration 89/1000 | Loss: 0.00000789
Iteration 90/1000 | Loss: 0.00000789
Iteration 91/1000 | Loss: 0.00000789
Iteration 92/1000 | Loss: 0.00000789
Iteration 93/1000 | Loss: 0.00000789
Iteration 94/1000 | Loss: 0.00000788
Iteration 95/1000 | Loss: 0.00000788
Iteration 96/1000 | Loss: 0.00000788
Iteration 97/1000 | Loss: 0.00000788
Iteration 98/1000 | Loss: 0.00000788
Iteration 99/1000 | Loss: 0.00000788
Iteration 100/1000 | Loss: 0.00000788
Iteration 101/1000 | Loss: 0.00000788
Iteration 102/1000 | Loss: 0.00000787
Iteration 103/1000 | Loss: 0.00000787
Iteration 104/1000 | Loss: 0.00000787
Iteration 105/1000 | Loss: 0.00000787
Iteration 106/1000 | Loss: 0.00000787
Iteration 107/1000 | Loss: 0.00000787
Iteration 108/1000 | Loss: 0.00000787
Iteration 109/1000 | Loss: 0.00000787
Iteration 110/1000 | Loss: 0.00000787
Iteration 111/1000 | Loss: 0.00000787
Iteration 112/1000 | Loss: 0.00000787
Iteration 113/1000 | Loss: 0.00000787
Iteration 114/1000 | Loss: 0.00000787
Iteration 115/1000 | Loss: 0.00000787
Iteration 116/1000 | Loss: 0.00000787
Iteration 117/1000 | Loss: 0.00000787
Iteration 118/1000 | Loss: 0.00000787
Iteration 119/1000 | Loss: 0.00000787
Iteration 120/1000 | Loss: 0.00000787
Iteration 121/1000 | Loss: 0.00000787
Iteration 122/1000 | Loss: 0.00000787
Iteration 123/1000 | Loss: 0.00000787
Iteration 124/1000 | Loss: 0.00000787
Iteration 125/1000 | Loss: 0.00000787
Iteration 126/1000 | Loss: 0.00000787
Iteration 127/1000 | Loss: 0.00000787
Iteration 128/1000 | Loss: 0.00000787
Iteration 129/1000 | Loss: 0.00000787
Iteration 130/1000 | Loss: 0.00000787
Iteration 131/1000 | Loss: 0.00000787
Iteration 132/1000 | Loss: 0.00000787
Iteration 133/1000 | Loss: 0.00000787
Iteration 134/1000 | Loss: 0.00000787
Iteration 135/1000 | Loss: 0.00000787
Iteration 136/1000 | Loss: 0.00000787
Iteration 137/1000 | Loss: 0.00000787
Iteration 138/1000 | Loss: 0.00000787
Iteration 139/1000 | Loss: 0.00000787
Iteration 140/1000 | Loss: 0.00000787
Iteration 141/1000 | Loss: 0.00000787
Iteration 142/1000 | Loss: 0.00000787
Iteration 143/1000 | Loss: 0.00000787
Iteration 144/1000 | Loss: 0.00000787
Iteration 145/1000 | Loss: 0.00000787
Iteration 146/1000 | Loss: 0.00000787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [7.871744855947327e-06, 7.871744855947327e-06, 7.871744855947327e-06, 7.871744855947327e-06, 7.871744855947327e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.871744855947327e-06

Optimization complete. Final v2v error: 2.373875379562378 mm

Highest mean error: 2.7044758796691895 mm for frame 39

Lowest mean error: 1.8982369899749756 mm for frame 247

Saving results

Total time: 33.71162009239197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01089079
Iteration 2/25 | Loss: 0.00161034
Iteration 3/25 | Loss: 0.00203346
Iteration 4/25 | Loss: 0.00139522
Iteration 5/25 | Loss: 0.00129373
Iteration 6/25 | Loss: 0.00128620
Iteration 7/25 | Loss: 0.00115597
Iteration 8/25 | Loss: 0.00105067
Iteration 9/25 | Loss: 0.00105809
Iteration 10/25 | Loss: 0.00099946
Iteration 11/25 | Loss: 0.00097816
Iteration 12/25 | Loss: 0.00097722
Iteration 13/25 | Loss: 0.00097717
Iteration 14/25 | Loss: 0.00097534
Iteration 15/25 | Loss: 0.00097496
Iteration 16/25 | Loss: 0.00097347
Iteration 17/25 | Loss: 0.00098109
Iteration 18/25 | Loss: 0.00097087
Iteration 19/25 | Loss: 0.00097744
Iteration 20/25 | Loss: 0.00096989
Iteration 21/25 | Loss: 0.00096214
Iteration 22/25 | Loss: 0.00096015
Iteration 23/25 | Loss: 0.00095990
Iteration 24/25 | Loss: 0.00095990
Iteration 25/25 | Loss: 0.00095990

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31323183
Iteration 2/25 | Loss: 0.00096034
Iteration 3/25 | Loss: 0.00096033
Iteration 4/25 | Loss: 0.00096033
Iteration 5/25 | Loss: 0.00096033
Iteration 6/25 | Loss: 0.00096033
Iteration 7/25 | Loss: 0.00096033
Iteration 8/25 | Loss: 0.00096033
Iteration 9/25 | Loss: 0.00096033
Iteration 10/25 | Loss: 0.00096033
Iteration 11/25 | Loss: 0.00096033
Iteration 12/25 | Loss: 0.00096033
Iteration 13/25 | Loss: 0.00096033
Iteration 14/25 | Loss: 0.00096033
Iteration 15/25 | Loss: 0.00096033
Iteration 16/25 | Loss: 0.00096033
Iteration 17/25 | Loss: 0.00096033
Iteration 18/25 | Loss: 0.00096033
Iteration 19/25 | Loss: 0.00096033
Iteration 20/25 | Loss: 0.00096033
Iteration 21/25 | Loss: 0.00096033
Iteration 22/25 | Loss: 0.00096033
Iteration 23/25 | Loss: 0.00096033
Iteration 24/25 | Loss: 0.00096033
Iteration 25/25 | Loss: 0.00096033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096033
Iteration 2/1000 | Loss: 0.00002215
Iteration 3/1000 | Loss: 0.00042013
Iteration 4/1000 | Loss: 0.00124958
Iteration 5/1000 | Loss: 0.00041268
Iteration 6/1000 | Loss: 0.00003260
Iteration 7/1000 | Loss: 0.00009605
Iteration 8/1000 | Loss: 0.00011812
Iteration 9/1000 | Loss: 0.00029484
Iteration 10/1000 | Loss: 0.00001228
Iteration 11/1000 | Loss: 0.00001114
Iteration 12/1000 | Loss: 0.00001038
Iteration 13/1000 | Loss: 0.00000993
Iteration 14/1000 | Loss: 0.00000972
Iteration 15/1000 | Loss: 0.00000972
Iteration 16/1000 | Loss: 0.00000958
Iteration 17/1000 | Loss: 0.00000940
Iteration 18/1000 | Loss: 0.00000936
Iteration 19/1000 | Loss: 0.00000935
Iteration 20/1000 | Loss: 0.00000932
Iteration 21/1000 | Loss: 0.00000931
Iteration 22/1000 | Loss: 0.00000931
Iteration 23/1000 | Loss: 0.00000930
Iteration 24/1000 | Loss: 0.00000930
Iteration 25/1000 | Loss: 0.00000929
Iteration 26/1000 | Loss: 0.00000929
Iteration 27/1000 | Loss: 0.00000929
Iteration 28/1000 | Loss: 0.00000928
Iteration 29/1000 | Loss: 0.00000928
Iteration 30/1000 | Loss: 0.00000926
Iteration 31/1000 | Loss: 0.00000925
Iteration 32/1000 | Loss: 0.00000925
Iteration 33/1000 | Loss: 0.00000925
Iteration 34/1000 | Loss: 0.00000925
Iteration 35/1000 | Loss: 0.00000925
Iteration 36/1000 | Loss: 0.00000924
Iteration 37/1000 | Loss: 0.00000924
Iteration 38/1000 | Loss: 0.00000924
Iteration 39/1000 | Loss: 0.00000923
Iteration 40/1000 | Loss: 0.00000923
Iteration 41/1000 | Loss: 0.00000922
Iteration 42/1000 | Loss: 0.00000922
Iteration 43/1000 | Loss: 0.00000921
Iteration 44/1000 | Loss: 0.00000921
Iteration 45/1000 | Loss: 0.00000920
Iteration 46/1000 | Loss: 0.00000919
Iteration 47/1000 | Loss: 0.00000919
Iteration 48/1000 | Loss: 0.00000917
Iteration 49/1000 | Loss: 0.00000917
Iteration 50/1000 | Loss: 0.00000917
Iteration 51/1000 | Loss: 0.00000917
Iteration 52/1000 | Loss: 0.00000917
Iteration 53/1000 | Loss: 0.00000917
Iteration 54/1000 | Loss: 0.00000916
Iteration 55/1000 | Loss: 0.00000916
Iteration 56/1000 | Loss: 0.00000916
Iteration 57/1000 | Loss: 0.00000915
Iteration 58/1000 | Loss: 0.00000915
Iteration 59/1000 | Loss: 0.00000915
Iteration 60/1000 | Loss: 0.00000914
Iteration 61/1000 | Loss: 0.00000914
Iteration 62/1000 | Loss: 0.00000914
Iteration 63/1000 | Loss: 0.00000914
Iteration 64/1000 | Loss: 0.00000914
Iteration 65/1000 | Loss: 0.00000914
Iteration 66/1000 | Loss: 0.00000914
Iteration 67/1000 | Loss: 0.00000914
Iteration 68/1000 | Loss: 0.00000914
Iteration 69/1000 | Loss: 0.00000913
Iteration 70/1000 | Loss: 0.00000913
Iteration 71/1000 | Loss: 0.00000913
Iteration 72/1000 | Loss: 0.00000913
Iteration 73/1000 | Loss: 0.00000913
Iteration 74/1000 | Loss: 0.00000913
Iteration 75/1000 | Loss: 0.00000913
Iteration 76/1000 | Loss: 0.00000913
Iteration 77/1000 | Loss: 0.00000913
Iteration 78/1000 | Loss: 0.00000913
Iteration 79/1000 | Loss: 0.00000913
Iteration 80/1000 | Loss: 0.00000913
Iteration 81/1000 | Loss: 0.00000913
Iteration 82/1000 | Loss: 0.00000913
Iteration 83/1000 | Loss: 0.00000913
Iteration 84/1000 | Loss: 0.00000913
Iteration 85/1000 | Loss: 0.00000913
Iteration 86/1000 | Loss: 0.00000913
Iteration 87/1000 | Loss: 0.00000913
Iteration 88/1000 | Loss: 0.00000913
Iteration 89/1000 | Loss: 0.00000913
Iteration 90/1000 | Loss: 0.00000913
Iteration 91/1000 | Loss: 0.00000913
Iteration 92/1000 | Loss: 0.00000913
Iteration 93/1000 | Loss: 0.00000913
Iteration 94/1000 | Loss: 0.00000913
Iteration 95/1000 | Loss: 0.00000913
Iteration 96/1000 | Loss: 0.00000913
Iteration 97/1000 | Loss: 0.00000913
Iteration 98/1000 | Loss: 0.00000913
Iteration 99/1000 | Loss: 0.00000913
Iteration 100/1000 | Loss: 0.00000913
Iteration 101/1000 | Loss: 0.00000913
Iteration 102/1000 | Loss: 0.00000913
Iteration 103/1000 | Loss: 0.00000913
Iteration 104/1000 | Loss: 0.00000913
Iteration 105/1000 | Loss: 0.00000913
Iteration 106/1000 | Loss: 0.00000913
Iteration 107/1000 | Loss: 0.00000913
Iteration 108/1000 | Loss: 0.00000913
Iteration 109/1000 | Loss: 0.00000913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [9.133581443165895e-06, 9.133581443165895e-06, 9.133581443165895e-06, 9.133581443165895e-06, 9.133581443165895e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.133581443165895e-06

Optimization complete. Final v2v error: 2.4835212230682373 mm

Highest mean error: 3.5350582599639893 mm for frame 69

Lowest mean error: 2.124655246734619 mm for frame 118

Saving results

Total time: 66.00969433784485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403594
Iteration 2/25 | Loss: 0.00105863
Iteration 3/25 | Loss: 0.00095553
Iteration 4/25 | Loss: 0.00094177
Iteration 5/25 | Loss: 0.00094011
Iteration 6/25 | Loss: 0.00094011
Iteration 7/25 | Loss: 0.00094011
Iteration 8/25 | Loss: 0.00094011
Iteration 9/25 | Loss: 0.00094011
Iteration 10/25 | Loss: 0.00094011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00094010861357674, 0.00094010861357674, 0.00094010861357674, 0.00094010861357674, 0.00094010861357674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00094010861357674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59097266
Iteration 2/25 | Loss: 0.00068507
Iteration 3/25 | Loss: 0.00068503
Iteration 4/25 | Loss: 0.00068503
Iteration 5/25 | Loss: 0.00068503
Iteration 6/25 | Loss: 0.00068503
Iteration 7/25 | Loss: 0.00068502
Iteration 8/25 | Loss: 0.00068502
Iteration 9/25 | Loss: 0.00068502
Iteration 10/25 | Loss: 0.00068502
Iteration 11/25 | Loss: 0.00068502
Iteration 12/25 | Loss: 0.00068502
Iteration 13/25 | Loss: 0.00068502
Iteration 14/25 | Loss: 0.00068502
Iteration 15/25 | Loss: 0.00068502
Iteration 16/25 | Loss: 0.00068502
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006850239005871117, 0.0006850239005871117, 0.0006850239005871117, 0.0006850239005871117, 0.0006850239005871117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006850239005871117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068502
Iteration 2/1000 | Loss: 0.00002046
Iteration 3/1000 | Loss: 0.00001262
Iteration 4/1000 | Loss: 0.00001004
Iteration 5/1000 | Loss: 0.00000924
Iteration 6/1000 | Loss: 0.00000871
Iteration 7/1000 | Loss: 0.00000851
Iteration 8/1000 | Loss: 0.00000839
Iteration 9/1000 | Loss: 0.00000838
Iteration 10/1000 | Loss: 0.00000837
Iteration 11/1000 | Loss: 0.00000835
Iteration 12/1000 | Loss: 0.00000831
Iteration 13/1000 | Loss: 0.00000813
Iteration 14/1000 | Loss: 0.00000808
Iteration 15/1000 | Loss: 0.00000805
Iteration 16/1000 | Loss: 0.00000805
Iteration 17/1000 | Loss: 0.00000805
Iteration 18/1000 | Loss: 0.00000804
Iteration 19/1000 | Loss: 0.00000804
Iteration 20/1000 | Loss: 0.00000804
Iteration 21/1000 | Loss: 0.00000804
Iteration 22/1000 | Loss: 0.00000804
Iteration 23/1000 | Loss: 0.00000804
Iteration 24/1000 | Loss: 0.00000803
Iteration 25/1000 | Loss: 0.00000803
Iteration 26/1000 | Loss: 0.00000802
Iteration 27/1000 | Loss: 0.00000802
Iteration 28/1000 | Loss: 0.00000802
Iteration 29/1000 | Loss: 0.00000801
Iteration 30/1000 | Loss: 0.00000801
Iteration 31/1000 | Loss: 0.00000801
Iteration 32/1000 | Loss: 0.00000800
Iteration 33/1000 | Loss: 0.00000800
Iteration 34/1000 | Loss: 0.00000800
Iteration 35/1000 | Loss: 0.00000799
Iteration 36/1000 | Loss: 0.00000799
Iteration 37/1000 | Loss: 0.00000799
Iteration 38/1000 | Loss: 0.00000799
Iteration 39/1000 | Loss: 0.00000799
Iteration 40/1000 | Loss: 0.00000798
Iteration 41/1000 | Loss: 0.00000798
Iteration 42/1000 | Loss: 0.00000798
Iteration 43/1000 | Loss: 0.00000798
Iteration 44/1000 | Loss: 0.00000798
Iteration 45/1000 | Loss: 0.00000798
Iteration 46/1000 | Loss: 0.00000798
Iteration 47/1000 | Loss: 0.00000798
Iteration 48/1000 | Loss: 0.00000798
Iteration 49/1000 | Loss: 0.00000798
Iteration 50/1000 | Loss: 0.00000798
Iteration 51/1000 | Loss: 0.00000798
Iteration 52/1000 | Loss: 0.00000797
Iteration 53/1000 | Loss: 0.00000797
Iteration 54/1000 | Loss: 0.00000797
Iteration 55/1000 | Loss: 0.00000797
Iteration 56/1000 | Loss: 0.00000797
Iteration 57/1000 | Loss: 0.00000797
Iteration 58/1000 | Loss: 0.00000797
Iteration 59/1000 | Loss: 0.00000797
Iteration 60/1000 | Loss: 0.00000796
Iteration 61/1000 | Loss: 0.00000796
Iteration 62/1000 | Loss: 0.00000795
Iteration 63/1000 | Loss: 0.00000795
Iteration 64/1000 | Loss: 0.00000795
Iteration 65/1000 | Loss: 0.00000795
Iteration 66/1000 | Loss: 0.00000795
Iteration 67/1000 | Loss: 0.00000795
Iteration 68/1000 | Loss: 0.00000795
Iteration 69/1000 | Loss: 0.00000795
Iteration 70/1000 | Loss: 0.00000795
Iteration 71/1000 | Loss: 0.00000794
Iteration 72/1000 | Loss: 0.00000794
Iteration 73/1000 | Loss: 0.00000794
Iteration 74/1000 | Loss: 0.00000794
Iteration 75/1000 | Loss: 0.00000794
Iteration 76/1000 | Loss: 0.00000794
Iteration 77/1000 | Loss: 0.00000794
Iteration 78/1000 | Loss: 0.00000794
Iteration 79/1000 | Loss: 0.00000794
Iteration 80/1000 | Loss: 0.00000794
Iteration 81/1000 | Loss: 0.00000794
Iteration 82/1000 | Loss: 0.00000794
Iteration 83/1000 | Loss: 0.00000793
Iteration 84/1000 | Loss: 0.00000793
Iteration 85/1000 | Loss: 0.00000793
Iteration 86/1000 | Loss: 0.00000793
Iteration 87/1000 | Loss: 0.00000793
Iteration 88/1000 | Loss: 0.00000792
Iteration 89/1000 | Loss: 0.00000792
Iteration 90/1000 | Loss: 0.00000792
Iteration 91/1000 | Loss: 0.00000792
Iteration 92/1000 | Loss: 0.00000792
Iteration 93/1000 | Loss: 0.00000792
Iteration 94/1000 | Loss: 0.00000792
Iteration 95/1000 | Loss: 0.00000792
Iteration 96/1000 | Loss: 0.00000792
Iteration 97/1000 | Loss: 0.00000792
Iteration 98/1000 | Loss: 0.00000792
Iteration 99/1000 | Loss: 0.00000792
Iteration 100/1000 | Loss: 0.00000792
Iteration 101/1000 | Loss: 0.00000792
Iteration 102/1000 | Loss: 0.00000792
Iteration 103/1000 | Loss: 0.00000792
Iteration 104/1000 | Loss: 0.00000792
Iteration 105/1000 | Loss: 0.00000792
Iteration 106/1000 | Loss: 0.00000792
Iteration 107/1000 | Loss: 0.00000792
Iteration 108/1000 | Loss: 0.00000792
Iteration 109/1000 | Loss: 0.00000792
Iteration 110/1000 | Loss: 0.00000791
Iteration 111/1000 | Loss: 0.00000791
Iteration 112/1000 | Loss: 0.00000791
Iteration 113/1000 | Loss: 0.00000791
Iteration 114/1000 | Loss: 0.00000791
Iteration 115/1000 | Loss: 0.00000790
Iteration 116/1000 | Loss: 0.00000790
Iteration 117/1000 | Loss: 0.00000789
Iteration 118/1000 | Loss: 0.00000789
Iteration 119/1000 | Loss: 0.00000789
Iteration 120/1000 | Loss: 0.00000789
Iteration 121/1000 | Loss: 0.00000789
Iteration 122/1000 | Loss: 0.00000789
Iteration 123/1000 | Loss: 0.00000789
Iteration 124/1000 | Loss: 0.00000789
Iteration 125/1000 | Loss: 0.00000789
Iteration 126/1000 | Loss: 0.00000789
Iteration 127/1000 | Loss: 0.00000789
Iteration 128/1000 | Loss: 0.00000788
Iteration 129/1000 | Loss: 0.00000788
Iteration 130/1000 | Loss: 0.00000788
Iteration 131/1000 | Loss: 0.00000787
Iteration 132/1000 | Loss: 0.00000787
Iteration 133/1000 | Loss: 0.00000787
Iteration 134/1000 | Loss: 0.00000787
Iteration 135/1000 | Loss: 0.00000787
Iteration 136/1000 | Loss: 0.00000787
Iteration 137/1000 | Loss: 0.00000787
Iteration 138/1000 | Loss: 0.00000787
Iteration 139/1000 | Loss: 0.00000787
Iteration 140/1000 | Loss: 0.00000787
Iteration 141/1000 | Loss: 0.00000787
Iteration 142/1000 | Loss: 0.00000787
Iteration 143/1000 | Loss: 0.00000786
Iteration 144/1000 | Loss: 0.00000786
Iteration 145/1000 | Loss: 0.00000786
Iteration 146/1000 | Loss: 0.00000786
Iteration 147/1000 | Loss: 0.00000786
Iteration 148/1000 | Loss: 0.00000786
Iteration 149/1000 | Loss: 0.00000786
Iteration 150/1000 | Loss: 0.00000786
Iteration 151/1000 | Loss: 0.00000786
Iteration 152/1000 | Loss: 0.00000786
Iteration 153/1000 | Loss: 0.00000785
Iteration 154/1000 | Loss: 0.00000785
Iteration 155/1000 | Loss: 0.00000785
Iteration 156/1000 | Loss: 0.00000785
Iteration 157/1000 | Loss: 0.00000785
Iteration 158/1000 | Loss: 0.00000785
Iteration 159/1000 | Loss: 0.00000785
Iteration 160/1000 | Loss: 0.00000785
Iteration 161/1000 | Loss: 0.00000785
Iteration 162/1000 | Loss: 0.00000785
Iteration 163/1000 | Loss: 0.00000784
Iteration 164/1000 | Loss: 0.00000784
Iteration 165/1000 | Loss: 0.00000784
Iteration 166/1000 | Loss: 0.00000784
Iteration 167/1000 | Loss: 0.00000784
Iteration 168/1000 | Loss: 0.00000784
Iteration 169/1000 | Loss: 0.00000784
Iteration 170/1000 | Loss: 0.00000784
Iteration 171/1000 | Loss: 0.00000784
Iteration 172/1000 | Loss: 0.00000784
Iteration 173/1000 | Loss: 0.00000783
Iteration 174/1000 | Loss: 0.00000783
Iteration 175/1000 | Loss: 0.00000783
Iteration 176/1000 | Loss: 0.00000783
Iteration 177/1000 | Loss: 0.00000782
Iteration 178/1000 | Loss: 0.00000782
Iteration 179/1000 | Loss: 0.00000782
Iteration 180/1000 | Loss: 0.00000782
Iteration 181/1000 | Loss: 0.00000782
Iteration 182/1000 | Loss: 0.00000782
Iteration 183/1000 | Loss: 0.00000782
Iteration 184/1000 | Loss: 0.00000781
Iteration 185/1000 | Loss: 0.00000781
Iteration 186/1000 | Loss: 0.00000781
Iteration 187/1000 | Loss: 0.00000781
Iteration 188/1000 | Loss: 0.00000781
Iteration 189/1000 | Loss: 0.00000781
Iteration 190/1000 | Loss: 0.00000781
Iteration 191/1000 | Loss: 0.00000781
Iteration 192/1000 | Loss: 0.00000780
Iteration 193/1000 | Loss: 0.00000780
Iteration 194/1000 | Loss: 0.00000780
Iteration 195/1000 | Loss: 0.00000780
Iteration 196/1000 | Loss: 0.00000780
Iteration 197/1000 | Loss: 0.00000780
Iteration 198/1000 | Loss: 0.00000780
Iteration 199/1000 | Loss: 0.00000779
Iteration 200/1000 | Loss: 0.00000779
Iteration 201/1000 | Loss: 0.00000779
Iteration 202/1000 | Loss: 0.00000779
Iteration 203/1000 | Loss: 0.00000779
Iteration 204/1000 | Loss: 0.00000779
Iteration 205/1000 | Loss: 0.00000779
Iteration 206/1000 | Loss: 0.00000779
Iteration 207/1000 | Loss: 0.00000779
Iteration 208/1000 | Loss: 0.00000779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [7.79330548539292e-06, 7.79330548539292e-06, 7.79330548539292e-06, 7.79330548539292e-06, 7.79330548539292e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.79330548539292e-06

Optimization complete. Final v2v error: 2.369436502456665 mm

Highest mean error: 2.5471649169921875 mm for frame 125

Lowest mean error: 2.1356818675994873 mm for frame 265

Saving results

Total time: 37.418696880340576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930906
Iteration 2/25 | Loss: 0.00204601
Iteration 3/25 | Loss: 0.00138370
Iteration 4/25 | Loss: 0.00134123
Iteration 5/25 | Loss: 0.00128987
Iteration 6/25 | Loss: 0.00123357
Iteration 7/25 | Loss: 0.00122132
Iteration 8/25 | Loss: 0.00118536
Iteration 9/25 | Loss: 0.00117432
Iteration 10/25 | Loss: 0.00117299
Iteration 11/25 | Loss: 0.00116600
Iteration 12/25 | Loss: 0.00115782
Iteration 13/25 | Loss: 0.00114983
Iteration 14/25 | Loss: 0.00115844
Iteration 15/25 | Loss: 0.00118621
Iteration 16/25 | Loss: 0.00118426
Iteration 17/25 | Loss: 0.00112374
Iteration 18/25 | Loss: 0.00111086
Iteration 19/25 | Loss: 0.00110606
Iteration 20/25 | Loss: 0.00111206
Iteration 21/25 | Loss: 0.00110398
Iteration 22/25 | Loss: 0.00110190
Iteration 23/25 | Loss: 0.00110019
Iteration 24/25 | Loss: 0.00110035
Iteration 25/25 | Loss: 0.00110029

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62270856
Iteration 2/25 | Loss: 0.00140213
Iteration 3/25 | Loss: 0.00140192
Iteration 4/25 | Loss: 0.00140192
Iteration 5/25 | Loss: 0.00140191
Iteration 6/25 | Loss: 0.00140191
Iteration 7/25 | Loss: 0.00140191
Iteration 8/25 | Loss: 0.00140191
Iteration 9/25 | Loss: 0.00140191
Iteration 10/25 | Loss: 0.00140191
Iteration 11/25 | Loss: 0.00140191
Iteration 12/25 | Loss: 0.00140191
Iteration 13/25 | Loss: 0.00140191
Iteration 14/25 | Loss: 0.00140191
Iteration 15/25 | Loss: 0.00140191
Iteration 16/25 | Loss: 0.00140191
Iteration 17/25 | Loss: 0.00140191
Iteration 18/25 | Loss: 0.00140191
Iteration 19/25 | Loss: 0.00140191
Iteration 20/25 | Loss: 0.00140191
Iteration 21/25 | Loss: 0.00140191
Iteration 22/25 | Loss: 0.00140191
Iteration 23/25 | Loss: 0.00140191
Iteration 24/25 | Loss: 0.00140191
Iteration 25/25 | Loss: 0.00140191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140191
Iteration 2/1000 | Loss: 0.00059829
Iteration 3/1000 | Loss: 0.00056432
Iteration 4/1000 | Loss: 0.00049867
Iteration 5/1000 | Loss: 0.00037642
Iteration 6/1000 | Loss: 0.00038926
Iteration 7/1000 | Loss: 0.00053595
Iteration 8/1000 | Loss: 0.00037374
Iteration 9/1000 | Loss: 0.00064577
Iteration 10/1000 | Loss: 0.00043332
Iteration 11/1000 | Loss: 0.00258749
Iteration 12/1000 | Loss: 0.00026380
Iteration 13/1000 | Loss: 0.00023893
Iteration 14/1000 | Loss: 0.00022338
Iteration 15/1000 | Loss: 0.00018316
Iteration 16/1000 | Loss: 0.00017415
Iteration 17/1000 | Loss: 0.00011300
Iteration 18/1000 | Loss: 0.00018931
Iteration 19/1000 | Loss: 0.00033687
Iteration 20/1000 | Loss: 0.00054188
Iteration 21/1000 | Loss: 0.00036143
Iteration 22/1000 | Loss: 0.00036414
Iteration 23/1000 | Loss: 0.00036245
Iteration 24/1000 | Loss: 0.00034813
Iteration 25/1000 | Loss: 0.00004253
Iteration 26/1000 | Loss: 0.00022507
Iteration 27/1000 | Loss: 0.00005151
Iteration 28/1000 | Loss: 0.00004071
Iteration 29/1000 | Loss: 0.00003850
Iteration 30/1000 | Loss: 0.00003696
Iteration 31/1000 | Loss: 0.00003422
Iteration 32/1000 | Loss: 0.00003113
Iteration 33/1000 | Loss: 0.00002989
Iteration 34/1000 | Loss: 0.00002908
Iteration 35/1000 | Loss: 0.00002873
Iteration 36/1000 | Loss: 0.00002846
Iteration 37/1000 | Loss: 0.00002809
Iteration 38/1000 | Loss: 0.00002776
Iteration 39/1000 | Loss: 0.00002746
Iteration 40/1000 | Loss: 0.00002726
Iteration 41/1000 | Loss: 0.00002712
Iteration 42/1000 | Loss: 0.00002710
Iteration 43/1000 | Loss: 0.00002705
Iteration 44/1000 | Loss: 0.00002703
Iteration 45/1000 | Loss: 0.00002703
Iteration 46/1000 | Loss: 0.00002702
Iteration 47/1000 | Loss: 0.00002697
Iteration 48/1000 | Loss: 0.00002693
Iteration 49/1000 | Loss: 0.00002693
Iteration 50/1000 | Loss: 0.00002693
Iteration 51/1000 | Loss: 0.00002692
Iteration 52/1000 | Loss: 0.00002691
Iteration 53/1000 | Loss: 0.00002691
Iteration 54/1000 | Loss: 0.00002690
Iteration 55/1000 | Loss: 0.00002690
Iteration 56/1000 | Loss: 0.00002684
Iteration 57/1000 | Loss: 0.00002682
Iteration 58/1000 | Loss: 0.00002682
Iteration 59/1000 | Loss: 0.00002682
Iteration 60/1000 | Loss: 0.00002682
Iteration 61/1000 | Loss: 0.00002682
Iteration 62/1000 | Loss: 0.00002681
Iteration 63/1000 | Loss: 0.00002681
Iteration 64/1000 | Loss: 0.00002681
Iteration 65/1000 | Loss: 0.00002681
Iteration 66/1000 | Loss: 0.00002681
Iteration 67/1000 | Loss: 0.00002681
Iteration 68/1000 | Loss: 0.00002681
Iteration 69/1000 | Loss: 0.00002681
Iteration 70/1000 | Loss: 0.00002681
Iteration 71/1000 | Loss: 0.00002681
Iteration 72/1000 | Loss: 0.00002680
Iteration 73/1000 | Loss: 0.00002680
Iteration 74/1000 | Loss: 0.00002680
Iteration 75/1000 | Loss: 0.00002680
Iteration 76/1000 | Loss: 0.00002680
Iteration 77/1000 | Loss: 0.00002680
Iteration 78/1000 | Loss: 0.00002680
Iteration 79/1000 | Loss: 0.00002680
Iteration 80/1000 | Loss: 0.00002680
Iteration 81/1000 | Loss: 0.00002679
Iteration 82/1000 | Loss: 0.00002679
Iteration 83/1000 | Loss: 0.00002678
Iteration 84/1000 | Loss: 0.00002678
Iteration 85/1000 | Loss: 0.00002677
Iteration 86/1000 | Loss: 0.00002677
Iteration 87/1000 | Loss: 0.00002677
Iteration 88/1000 | Loss: 0.00002676
Iteration 89/1000 | Loss: 0.00002676
Iteration 90/1000 | Loss: 0.00002676
Iteration 91/1000 | Loss: 0.00002676
Iteration 92/1000 | Loss: 0.00002676
Iteration 93/1000 | Loss: 0.00002676
Iteration 94/1000 | Loss: 0.00002676
Iteration 95/1000 | Loss: 0.00002676
Iteration 96/1000 | Loss: 0.00002676
Iteration 97/1000 | Loss: 0.00002676
Iteration 98/1000 | Loss: 0.00002676
Iteration 99/1000 | Loss: 0.00002676
Iteration 100/1000 | Loss: 0.00002675
Iteration 101/1000 | Loss: 0.00002675
Iteration 102/1000 | Loss: 0.00002675
Iteration 103/1000 | Loss: 0.00002675
Iteration 104/1000 | Loss: 0.00002675
Iteration 105/1000 | Loss: 0.00002674
Iteration 106/1000 | Loss: 0.00002674
Iteration 107/1000 | Loss: 0.00002674
Iteration 108/1000 | Loss: 0.00002674
Iteration 109/1000 | Loss: 0.00002674
Iteration 110/1000 | Loss: 0.00002673
Iteration 111/1000 | Loss: 0.00002673
Iteration 112/1000 | Loss: 0.00002672
Iteration 113/1000 | Loss: 0.00002672
Iteration 114/1000 | Loss: 0.00002672
Iteration 115/1000 | Loss: 0.00002672
Iteration 116/1000 | Loss: 0.00002672
Iteration 117/1000 | Loss: 0.00002672
Iteration 118/1000 | Loss: 0.00002672
Iteration 119/1000 | Loss: 0.00002672
Iteration 120/1000 | Loss: 0.00002672
Iteration 121/1000 | Loss: 0.00002672
Iteration 122/1000 | Loss: 0.00002672
Iteration 123/1000 | Loss: 0.00002672
Iteration 124/1000 | Loss: 0.00002671
Iteration 125/1000 | Loss: 0.00002671
Iteration 126/1000 | Loss: 0.00002670
Iteration 127/1000 | Loss: 0.00002670
Iteration 128/1000 | Loss: 0.00002670
Iteration 129/1000 | Loss: 0.00002669
Iteration 130/1000 | Loss: 0.00002669
Iteration 131/1000 | Loss: 0.00002669
Iteration 132/1000 | Loss: 0.00002668
Iteration 133/1000 | Loss: 0.00002668
Iteration 134/1000 | Loss: 0.00002668
Iteration 135/1000 | Loss: 0.00002668
Iteration 136/1000 | Loss: 0.00002667
Iteration 137/1000 | Loss: 0.00002667
Iteration 138/1000 | Loss: 0.00002667
Iteration 139/1000 | Loss: 0.00002667
Iteration 140/1000 | Loss: 0.00002666
Iteration 141/1000 | Loss: 0.00002666
Iteration 142/1000 | Loss: 0.00002666
Iteration 143/1000 | Loss: 0.00002665
Iteration 144/1000 | Loss: 0.00002665
Iteration 145/1000 | Loss: 0.00002665
Iteration 146/1000 | Loss: 0.00002664
Iteration 147/1000 | Loss: 0.00002664
Iteration 148/1000 | Loss: 0.00002663
Iteration 149/1000 | Loss: 0.00002663
Iteration 150/1000 | Loss: 0.00002663
Iteration 151/1000 | Loss: 0.00002663
Iteration 152/1000 | Loss: 0.00002662
Iteration 153/1000 | Loss: 0.00002662
Iteration 154/1000 | Loss: 0.00002662
Iteration 155/1000 | Loss: 0.00002662
Iteration 156/1000 | Loss: 0.00002661
Iteration 157/1000 | Loss: 0.00002661
Iteration 158/1000 | Loss: 0.00002661
Iteration 159/1000 | Loss: 0.00002661
Iteration 160/1000 | Loss: 0.00002661
Iteration 161/1000 | Loss: 0.00002661
Iteration 162/1000 | Loss: 0.00002661
Iteration 163/1000 | Loss: 0.00002661
Iteration 164/1000 | Loss: 0.00002660
Iteration 165/1000 | Loss: 0.00002660
Iteration 166/1000 | Loss: 0.00002660
Iteration 167/1000 | Loss: 0.00002660
Iteration 168/1000 | Loss: 0.00002660
Iteration 169/1000 | Loss: 0.00002660
Iteration 170/1000 | Loss: 0.00002660
Iteration 171/1000 | Loss: 0.00002660
Iteration 172/1000 | Loss: 0.00002660
Iteration 173/1000 | Loss: 0.00002660
Iteration 174/1000 | Loss: 0.00002659
Iteration 175/1000 | Loss: 0.00002659
Iteration 176/1000 | Loss: 0.00002659
Iteration 177/1000 | Loss: 0.00002659
Iteration 178/1000 | Loss: 0.00002659
Iteration 179/1000 | Loss: 0.00002659
Iteration 180/1000 | Loss: 0.00002658
Iteration 181/1000 | Loss: 0.00002658
Iteration 182/1000 | Loss: 0.00002658
Iteration 183/1000 | Loss: 0.00002658
Iteration 184/1000 | Loss: 0.00002657
Iteration 185/1000 | Loss: 0.00002657
Iteration 186/1000 | Loss: 0.00002657
Iteration 187/1000 | Loss: 0.00002657
Iteration 188/1000 | Loss: 0.00002657
Iteration 189/1000 | Loss: 0.00002656
Iteration 190/1000 | Loss: 0.00002656
Iteration 191/1000 | Loss: 0.00002656
Iteration 192/1000 | Loss: 0.00002655
Iteration 193/1000 | Loss: 0.00002655
Iteration 194/1000 | Loss: 0.00002655
Iteration 195/1000 | Loss: 0.00002655
Iteration 196/1000 | Loss: 0.00002655
Iteration 197/1000 | Loss: 0.00002655
Iteration 198/1000 | Loss: 0.00002654
Iteration 199/1000 | Loss: 0.00002654
Iteration 200/1000 | Loss: 0.00002654
Iteration 201/1000 | Loss: 0.00002654
Iteration 202/1000 | Loss: 0.00002654
Iteration 203/1000 | Loss: 0.00002653
Iteration 204/1000 | Loss: 0.00002653
Iteration 205/1000 | Loss: 0.00002653
Iteration 206/1000 | Loss: 0.00002653
Iteration 207/1000 | Loss: 0.00002652
Iteration 208/1000 | Loss: 0.00002652
Iteration 209/1000 | Loss: 0.00002652
Iteration 210/1000 | Loss: 0.00002652
Iteration 211/1000 | Loss: 0.00002652
Iteration 212/1000 | Loss: 0.00002652
Iteration 213/1000 | Loss: 0.00002651
Iteration 214/1000 | Loss: 0.00002651
Iteration 215/1000 | Loss: 0.00002651
Iteration 216/1000 | Loss: 0.00002651
Iteration 217/1000 | Loss: 0.00002651
Iteration 218/1000 | Loss: 0.00002651
Iteration 219/1000 | Loss: 0.00002650
Iteration 220/1000 | Loss: 0.00002650
Iteration 221/1000 | Loss: 0.00002650
Iteration 222/1000 | Loss: 0.00002650
Iteration 223/1000 | Loss: 0.00002649
Iteration 224/1000 | Loss: 0.00002649
Iteration 225/1000 | Loss: 0.00002649
Iteration 226/1000 | Loss: 0.00002649
Iteration 227/1000 | Loss: 0.00002648
Iteration 228/1000 | Loss: 0.00002648
Iteration 229/1000 | Loss: 0.00002648
Iteration 230/1000 | Loss: 0.00002647
Iteration 231/1000 | Loss: 0.00002647
Iteration 232/1000 | Loss: 0.00002647
Iteration 233/1000 | Loss: 0.00002646
Iteration 234/1000 | Loss: 0.00002646
Iteration 235/1000 | Loss: 0.00002646
Iteration 236/1000 | Loss: 0.00002645
Iteration 237/1000 | Loss: 0.00002645
Iteration 238/1000 | Loss: 0.00002645
Iteration 239/1000 | Loss: 0.00002644
Iteration 240/1000 | Loss: 0.00002644
Iteration 241/1000 | Loss: 0.00002644
Iteration 242/1000 | Loss: 0.00002644
Iteration 243/1000 | Loss: 0.00002644
Iteration 244/1000 | Loss: 0.00002643
Iteration 245/1000 | Loss: 0.00002643
Iteration 246/1000 | Loss: 0.00002643
Iteration 247/1000 | Loss: 0.00002643
Iteration 248/1000 | Loss: 0.00002643
Iteration 249/1000 | Loss: 0.00002643
Iteration 250/1000 | Loss: 0.00002643
Iteration 251/1000 | Loss: 0.00002643
Iteration 252/1000 | Loss: 0.00002643
Iteration 253/1000 | Loss: 0.00002643
Iteration 254/1000 | Loss: 0.00002643
Iteration 255/1000 | Loss: 0.00002643
Iteration 256/1000 | Loss: 0.00002643
Iteration 257/1000 | Loss: 0.00002643
Iteration 258/1000 | Loss: 0.00002643
Iteration 259/1000 | Loss: 0.00002643
Iteration 260/1000 | Loss: 0.00002643
Iteration 261/1000 | Loss: 0.00002643
Iteration 262/1000 | Loss: 0.00002643
Iteration 263/1000 | Loss: 0.00002643
Iteration 264/1000 | Loss: 0.00002643
Iteration 265/1000 | Loss: 0.00002643
Iteration 266/1000 | Loss: 0.00002643
Iteration 267/1000 | Loss: 0.00002643
Iteration 268/1000 | Loss: 0.00002643
Iteration 269/1000 | Loss: 0.00002643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [2.6425284886499867e-05, 2.6425284886499867e-05, 2.6425284886499867e-05, 2.6425284886499867e-05, 2.6425284886499867e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6425284886499867e-05

Optimization complete. Final v2v error: 4.142111301422119 mm

Highest mean error: 6.552189350128174 mm for frame 139

Lowest mean error: 2.7845096588134766 mm for frame 46

Saving results

Total time: 138.68138885498047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802664
Iteration 2/25 | Loss: 0.00125870
Iteration 3/25 | Loss: 0.00104527
Iteration 4/25 | Loss: 0.00103199
Iteration 5/25 | Loss: 0.00096922
Iteration 6/25 | Loss: 0.00095163
Iteration 7/25 | Loss: 0.00094055
Iteration 8/25 | Loss: 0.00093667
Iteration 9/25 | Loss: 0.00093423
Iteration 10/25 | Loss: 0.00093328
Iteration 11/25 | Loss: 0.00093308
Iteration 12/25 | Loss: 0.00093295
Iteration 13/25 | Loss: 0.00093295
Iteration 14/25 | Loss: 0.00093295
Iteration 15/25 | Loss: 0.00093295
Iteration 16/25 | Loss: 0.00093295
Iteration 17/25 | Loss: 0.00093294
Iteration 18/25 | Loss: 0.00093294
Iteration 19/25 | Loss: 0.00093294
Iteration 20/25 | Loss: 0.00093294
Iteration 21/25 | Loss: 0.00093294
Iteration 22/25 | Loss: 0.00093294
Iteration 23/25 | Loss: 0.00093294
Iteration 24/25 | Loss: 0.00093294
Iteration 25/25 | Loss: 0.00093294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87043715
Iteration 2/25 | Loss: 0.00089287
Iteration 3/25 | Loss: 0.00089286
Iteration 4/25 | Loss: 0.00089286
Iteration 5/25 | Loss: 0.00089286
Iteration 6/25 | Loss: 0.00089286
Iteration 7/25 | Loss: 0.00089286
Iteration 8/25 | Loss: 0.00089286
Iteration 9/25 | Loss: 0.00089286
Iteration 10/25 | Loss: 0.00089286
Iteration 11/25 | Loss: 0.00089286
Iteration 12/25 | Loss: 0.00089286
Iteration 13/25 | Loss: 0.00089286
Iteration 14/25 | Loss: 0.00089286
Iteration 15/25 | Loss: 0.00089286
Iteration 16/25 | Loss: 0.00089286
Iteration 17/25 | Loss: 0.00089286
Iteration 18/25 | Loss: 0.00089286
Iteration 19/25 | Loss: 0.00089286
Iteration 20/25 | Loss: 0.00089286
Iteration 21/25 | Loss: 0.00089286
Iteration 22/25 | Loss: 0.00089286
Iteration 23/25 | Loss: 0.00089286
Iteration 24/25 | Loss: 0.00089286
Iteration 25/25 | Loss: 0.00089286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089286
Iteration 2/1000 | Loss: 0.00001814
Iteration 3/1000 | Loss: 0.00001145
Iteration 4/1000 | Loss: 0.00001043
Iteration 5/1000 | Loss: 0.00000967
Iteration 6/1000 | Loss: 0.00000936
Iteration 7/1000 | Loss: 0.00000908
Iteration 8/1000 | Loss: 0.00000898
Iteration 9/1000 | Loss: 0.00000897
Iteration 10/1000 | Loss: 0.00000895
Iteration 11/1000 | Loss: 0.00000894
Iteration 12/1000 | Loss: 0.00000894
Iteration 13/1000 | Loss: 0.00000892
Iteration 14/1000 | Loss: 0.00000891
Iteration 15/1000 | Loss: 0.00000885
Iteration 16/1000 | Loss: 0.00000883
Iteration 17/1000 | Loss: 0.00000882
Iteration 18/1000 | Loss: 0.00000882
Iteration 19/1000 | Loss: 0.00000881
Iteration 20/1000 | Loss: 0.00000881
Iteration 21/1000 | Loss: 0.00000881
Iteration 22/1000 | Loss: 0.00000880
Iteration 23/1000 | Loss: 0.00000878
Iteration 24/1000 | Loss: 0.00000878
Iteration 25/1000 | Loss: 0.00000878
Iteration 26/1000 | Loss: 0.00000878
Iteration 27/1000 | Loss: 0.00000878
Iteration 28/1000 | Loss: 0.00000878
Iteration 29/1000 | Loss: 0.00000878
Iteration 30/1000 | Loss: 0.00000878
Iteration 31/1000 | Loss: 0.00000878
Iteration 32/1000 | Loss: 0.00000878
Iteration 33/1000 | Loss: 0.00000878
Iteration 34/1000 | Loss: 0.00000878
Iteration 35/1000 | Loss: 0.00000878
Iteration 36/1000 | Loss: 0.00000878
Iteration 37/1000 | Loss: 0.00000878
Iteration 38/1000 | Loss: 0.00000878
Iteration 39/1000 | Loss: 0.00000878
Iteration 40/1000 | Loss: 0.00000878
Iteration 41/1000 | Loss: 0.00000878
Iteration 42/1000 | Loss: 0.00000878
Iteration 43/1000 | Loss: 0.00000878
Iteration 44/1000 | Loss: 0.00000878
Iteration 45/1000 | Loss: 0.00000878
Iteration 46/1000 | Loss: 0.00000878
Iteration 47/1000 | Loss: 0.00000878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 47. Stopping optimization.
Last 5 losses: [8.77773345564492e-06, 8.77773345564492e-06, 8.77773345564492e-06, 8.77773345564492e-06, 8.77773345564492e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.77773345564492e-06

Optimization complete. Final v2v error: 2.513169050216675 mm

Highest mean error: 2.8524255752563477 mm for frame 79

Lowest mean error: 2.188322067260742 mm for frame 138

Saving results

Total time: 34.76704216003418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812661
Iteration 2/25 | Loss: 0.00155703
Iteration 3/25 | Loss: 0.00118883
Iteration 4/25 | Loss: 0.00113973
Iteration 5/25 | Loss: 0.00112365
Iteration 6/25 | Loss: 0.00111608
Iteration 7/25 | Loss: 0.00111575
Iteration 8/25 | Loss: 0.00110820
Iteration 9/25 | Loss: 0.00110222
Iteration 10/25 | Loss: 0.00109002
Iteration 11/25 | Loss: 0.00108905
Iteration 12/25 | Loss: 0.00108542
Iteration 13/25 | Loss: 0.00108029
Iteration 14/25 | Loss: 0.00108271
Iteration 15/25 | Loss: 0.00108515
Iteration 16/25 | Loss: 0.00108005
Iteration 17/25 | Loss: 0.00107807
Iteration 18/25 | Loss: 0.00106973
Iteration 19/25 | Loss: 0.00106186
Iteration 20/25 | Loss: 0.00106556
Iteration 21/25 | Loss: 0.00106156
Iteration 22/25 | Loss: 0.00105870
Iteration 23/25 | Loss: 0.00105853
Iteration 24/25 | Loss: 0.00105823
Iteration 25/25 | Loss: 0.00105874

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26695192
Iteration 2/25 | Loss: 0.00072463
Iteration 3/25 | Loss: 0.00070640
Iteration 4/25 | Loss: 0.00070640
Iteration 5/25 | Loss: 0.00070640
Iteration 6/25 | Loss: 0.00070640
Iteration 7/25 | Loss: 0.00070640
Iteration 8/25 | Loss: 0.00070640
Iteration 9/25 | Loss: 0.00070640
Iteration 10/25 | Loss: 0.00070640
Iteration 11/25 | Loss: 0.00070640
Iteration 12/25 | Loss: 0.00070640
Iteration 13/25 | Loss: 0.00070640
Iteration 14/25 | Loss: 0.00070640
Iteration 15/25 | Loss: 0.00070640
Iteration 16/25 | Loss: 0.00070640
Iteration 17/25 | Loss: 0.00070640
Iteration 18/25 | Loss: 0.00070640
Iteration 19/25 | Loss: 0.00070640
Iteration 20/25 | Loss: 0.00070640
Iteration 21/25 | Loss: 0.00070640
Iteration 22/25 | Loss: 0.00070640
Iteration 23/25 | Loss: 0.00070640
Iteration 24/25 | Loss: 0.00070640
Iteration 25/25 | Loss: 0.00070640

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070640
Iteration 2/1000 | Loss: 0.00005412
Iteration 3/1000 | Loss: 0.00002835
Iteration 4/1000 | Loss: 0.00003448
Iteration 5/1000 | Loss: 0.00003150
Iteration 6/1000 | Loss: 0.00002690
Iteration 7/1000 | Loss: 0.00002438
Iteration 8/1000 | Loss: 0.00003770
Iteration 9/1000 | Loss: 0.00003634
Iteration 10/1000 | Loss: 0.00003647
Iteration 11/1000 | Loss: 0.00002699
Iteration 12/1000 | Loss: 0.00003665
Iteration 13/1000 | Loss: 0.00002985
Iteration 14/1000 | Loss: 0.00002749
Iteration 15/1000 | Loss: 0.00002948
Iteration 16/1000 | Loss: 0.00003201
Iteration 17/1000 | Loss: 0.00002998
Iteration 18/1000 | Loss: 0.00002884
Iteration 19/1000 | Loss: 0.00027167
Iteration 20/1000 | Loss: 0.00014074
Iteration 21/1000 | Loss: 0.00022927
Iteration 22/1000 | Loss: 0.00028499
Iteration 23/1000 | Loss: 0.00040263
Iteration 24/1000 | Loss: 0.00004339
Iteration 25/1000 | Loss: 0.00003694
Iteration 26/1000 | Loss: 0.00003635
Iteration 27/1000 | Loss: 0.00003261
Iteration 28/1000 | Loss: 0.00002498
Iteration 29/1000 | Loss: 0.00003051
Iteration 30/1000 | Loss: 0.00003061
Iteration 31/1000 | Loss: 0.00002538
Iteration 32/1000 | Loss: 0.00002289
Iteration 33/1000 | Loss: 0.00003228
Iteration 34/1000 | Loss: 0.00003164
Iteration 35/1000 | Loss: 0.00003017
Iteration 36/1000 | Loss: 0.00003084
Iteration 37/1000 | Loss: 0.00003288
Iteration 38/1000 | Loss: 0.00002886
Iteration 39/1000 | Loss: 0.00003142
Iteration 40/1000 | Loss: 0.00003196
Iteration 41/1000 | Loss: 0.00003225
Iteration 42/1000 | Loss: 0.00002802
Iteration 43/1000 | Loss: 0.00003113
Iteration 44/1000 | Loss: 0.00002832
Iteration 45/1000 | Loss: 0.00002278
Iteration 46/1000 | Loss: 0.00004352
Iteration 47/1000 | Loss: 0.00003799
Iteration 48/1000 | Loss: 0.00002715
Iteration 49/1000 | Loss: 0.00002441
Iteration 50/1000 | Loss: 0.00003683
Iteration 51/1000 | Loss: 0.00002633
Iteration 52/1000 | Loss: 0.00002325
Iteration 53/1000 | Loss: 0.00003600
Iteration 54/1000 | Loss: 0.00002686
Iteration 55/1000 | Loss: 0.00002327
Iteration 56/1000 | Loss: 0.00003430
Iteration 57/1000 | Loss: 0.00004481
Iteration 58/1000 | Loss: 0.00003432
Iteration 59/1000 | Loss: 0.00003210
Iteration 60/1000 | Loss: 0.00003276
Iteration 61/1000 | Loss: 0.00003245
Iteration 62/1000 | Loss: 0.00002578
Iteration 63/1000 | Loss: 0.00003074
Iteration 64/1000 | Loss: 0.00002546
Iteration 65/1000 | Loss: 0.00003042
Iteration 66/1000 | Loss: 0.00003106
Iteration 67/1000 | Loss: 0.00003397
Iteration 68/1000 | Loss: 0.00002873
Iteration 69/1000 | Loss: 0.00003698
Iteration 70/1000 | Loss: 0.00002783
Iteration 71/1000 | Loss: 0.00003702
Iteration 72/1000 | Loss: 0.00002033
Iteration 73/1000 | Loss: 0.00001962
Iteration 74/1000 | Loss: 0.00001905
Iteration 75/1000 | Loss: 0.00001873
Iteration 76/1000 | Loss: 0.00001873
Iteration 77/1000 | Loss: 0.00001872
Iteration 78/1000 | Loss: 0.00001871
Iteration 79/1000 | Loss: 0.00001864
Iteration 80/1000 | Loss: 0.00001846
Iteration 81/1000 | Loss: 0.00001843
Iteration 82/1000 | Loss: 0.00001840
Iteration 83/1000 | Loss: 0.00001840
Iteration 84/1000 | Loss: 0.00001840
Iteration 85/1000 | Loss: 0.00001840
Iteration 86/1000 | Loss: 0.00001840
Iteration 87/1000 | Loss: 0.00001839
Iteration 88/1000 | Loss: 0.00001839
Iteration 89/1000 | Loss: 0.00001839
Iteration 90/1000 | Loss: 0.00001839
Iteration 91/1000 | Loss: 0.00001839
Iteration 92/1000 | Loss: 0.00001839
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001839
Iteration 95/1000 | Loss: 0.00001839
Iteration 96/1000 | Loss: 0.00001839
Iteration 97/1000 | Loss: 0.00001839
Iteration 98/1000 | Loss: 0.00001838
Iteration 99/1000 | Loss: 0.00001838
Iteration 100/1000 | Loss: 0.00001838
Iteration 101/1000 | Loss: 0.00001838
Iteration 102/1000 | Loss: 0.00001838
Iteration 103/1000 | Loss: 0.00001838
Iteration 104/1000 | Loss: 0.00001838
Iteration 105/1000 | Loss: 0.00001838
Iteration 106/1000 | Loss: 0.00001838
Iteration 107/1000 | Loss: 0.00001838
Iteration 108/1000 | Loss: 0.00001837
Iteration 109/1000 | Loss: 0.00001837
Iteration 110/1000 | Loss: 0.00001837
Iteration 111/1000 | Loss: 0.00001837
Iteration 112/1000 | Loss: 0.00001837
Iteration 113/1000 | Loss: 0.00001837
Iteration 114/1000 | Loss: 0.00001836
Iteration 115/1000 | Loss: 0.00001836
Iteration 116/1000 | Loss: 0.00001836
Iteration 117/1000 | Loss: 0.00001836
Iteration 118/1000 | Loss: 0.00001836
Iteration 119/1000 | Loss: 0.00001836
Iteration 120/1000 | Loss: 0.00001835
Iteration 121/1000 | Loss: 0.00001835
Iteration 122/1000 | Loss: 0.00001835
Iteration 123/1000 | Loss: 0.00001835
Iteration 124/1000 | Loss: 0.00001835
Iteration 125/1000 | Loss: 0.00001835
Iteration 126/1000 | Loss: 0.00001834
Iteration 127/1000 | Loss: 0.00001834
Iteration 128/1000 | Loss: 0.00001834
Iteration 129/1000 | Loss: 0.00001834
Iteration 130/1000 | Loss: 0.00001834
Iteration 131/1000 | Loss: 0.00001834
Iteration 132/1000 | Loss: 0.00001834
Iteration 133/1000 | Loss: 0.00001833
Iteration 134/1000 | Loss: 0.00001833
Iteration 135/1000 | Loss: 0.00001833
Iteration 136/1000 | Loss: 0.00001833
Iteration 137/1000 | Loss: 0.00001833
Iteration 138/1000 | Loss: 0.00001833
Iteration 139/1000 | Loss: 0.00001833
Iteration 140/1000 | Loss: 0.00001832
Iteration 141/1000 | Loss: 0.00001832
Iteration 142/1000 | Loss: 0.00001832
Iteration 143/1000 | Loss: 0.00001831
Iteration 144/1000 | Loss: 0.00001831
Iteration 145/1000 | Loss: 0.00001831
Iteration 146/1000 | Loss: 0.00001831
Iteration 147/1000 | Loss: 0.00001831
Iteration 148/1000 | Loss: 0.00001831
Iteration 149/1000 | Loss: 0.00001831
Iteration 150/1000 | Loss: 0.00001831
Iteration 151/1000 | Loss: 0.00001831
Iteration 152/1000 | Loss: 0.00001831
Iteration 153/1000 | Loss: 0.00001831
Iteration 154/1000 | Loss: 0.00001830
Iteration 155/1000 | Loss: 0.00001830
Iteration 156/1000 | Loss: 0.00001830
Iteration 157/1000 | Loss: 0.00001829
Iteration 158/1000 | Loss: 0.00001829
Iteration 159/1000 | Loss: 0.00001829
Iteration 160/1000 | Loss: 0.00001829
Iteration 161/1000 | Loss: 0.00001829
Iteration 162/1000 | Loss: 0.00001829
Iteration 163/1000 | Loss: 0.00001829
Iteration 164/1000 | Loss: 0.00001828
Iteration 165/1000 | Loss: 0.00001828
Iteration 166/1000 | Loss: 0.00001827
Iteration 167/1000 | Loss: 0.00001827
Iteration 168/1000 | Loss: 0.00001826
Iteration 169/1000 | Loss: 0.00001826
Iteration 170/1000 | Loss: 0.00001825
Iteration 171/1000 | Loss: 0.00001824
Iteration 172/1000 | Loss: 0.00001824
Iteration 173/1000 | Loss: 0.00001824
Iteration 174/1000 | Loss: 0.00001823
Iteration 175/1000 | Loss: 0.00001823
Iteration 176/1000 | Loss: 0.00001823
Iteration 177/1000 | Loss: 0.00001823
Iteration 178/1000 | Loss: 0.00001823
Iteration 179/1000 | Loss: 0.00001822
Iteration 180/1000 | Loss: 0.00001822
Iteration 181/1000 | Loss: 0.00001822
Iteration 182/1000 | Loss: 0.00001821
Iteration 183/1000 | Loss: 0.00001821
Iteration 184/1000 | Loss: 0.00001821
Iteration 185/1000 | Loss: 0.00001821
Iteration 186/1000 | Loss: 0.00001821
Iteration 187/1000 | Loss: 0.00001821
Iteration 188/1000 | Loss: 0.00001821
Iteration 189/1000 | Loss: 0.00001820
Iteration 190/1000 | Loss: 0.00001820
Iteration 191/1000 | Loss: 0.00001820
Iteration 192/1000 | Loss: 0.00001820
Iteration 193/1000 | Loss: 0.00001820
Iteration 194/1000 | Loss: 0.00001820
Iteration 195/1000 | Loss: 0.00001820
Iteration 196/1000 | Loss: 0.00001820
Iteration 197/1000 | Loss: 0.00001819
Iteration 198/1000 | Loss: 0.00001819
Iteration 199/1000 | Loss: 0.00001819
Iteration 200/1000 | Loss: 0.00001818
Iteration 201/1000 | Loss: 0.00001818
Iteration 202/1000 | Loss: 0.00001818
Iteration 203/1000 | Loss: 0.00001818
Iteration 204/1000 | Loss: 0.00001817
Iteration 205/1000 | Loss: 0.00001817
Iteration 206/1000 | Loss: 0.00001817
Iteration 207/1000 | Loss: 0.00001817
Iteration 208/1000 | Loss: 0.00001817
Iteration 209/1000 | Loss: 0.00001817
Iteration 210/1000 | Loss: 0.00001817
Iteration 211/1000 | Loss: 0.00001817
Iteration 212/1000 | Loss: 0.00001817
Iteration 213/1000 | Loss: 0.00001817
Iteration 214/1000 | Loss: 0.00001816
Iteration 215/1000 | Loss: 0.00001816
Iteration 216/1000 | Loss: 0.00001816
Iteration 217/1000 | Loss: 0.00001816
Iteration 218/1000 | Loss: 0.00001816
Iteration 219/1000 | Loss: 0.00001816
Iteration 220/1000 | Loss: 0.00001816
Iteration 221/1000 | Loss: 0.00001816
Iteration 222/1000 | Loss: 0.00001816
Iteration 223/1000 | Loss: 0.00001815
Iteration 224/1000 | Loss: 0.00001815
Iteration 225/1000 | Loss: 0.00001815
Iteration 226/1000 | Loss: 0.00001815
Iteration 227/1000 | Loss: 0.00001815
Iteration 228/1000 | Loss: 0.00001815
Iteration 229/1000 | Loss: 0.00001815
Iteration 230/1000 | Loss: 0.00001815
Iteration 231/1000 | Loss: 0.00001815
Iteration 232/1000 | Loss: 0.00001815
Iteration 233/1000 | Loss: 0.00001815
Iteration 234/1000 | Loss: 0.00001815
Iteration 235/1000 | Loss: 0.00001815
Iteration 236/1000 | Loss: 0.00001814
Iteration 237/1000 | Loss: 0.00001814
Iteration 238/1000 | Loss: 0.00001814
Iteration 239/1000 | Loss: 0.00001814
Iteration 240/1000 | Loss: 0.00001814
Iteration 241/1000 | Loss: 0.00001814
Iteration 242/1000 | Loss: 0.00001814
Iteration 243/1000 | Loss: 0.00001814
Iteration 244/1000 | Loss: 0.00001814
Iteration 245/1000 | Loss: 0.00001814
Iteration 246/1000 | Loss: 0.00001814
Iteration 247/1000 | Loss: 0.00001814
Iteration 248/1000 | Loss: 0.00001814
Iteration 249/1000 | Loss: 0.00001814
Iteration 250/1000 | Loss: 0.00001814
Iteration 251/1000 | Loss: 0.00001813
Iteration 252/1000 | Loss: 0.00001813
Iteration 253/1000 | Loss: 0.00001813
Iteration 254/1000 | Loss: 0.00001813
Iteration 255/1000 | Loss: 0.00001813
Iteration 256/1000 | Loss: 0.00001813
Iteration 257/1000 | Loss: 0.00001812
Iteration 258/1000 | Loss: 0.00001812
Iteration 259/1000 | Loss: 0.00001812
Iteration 260/1000 | Loss: 0.00001812
Iteration 261/1000 | Loss: 0.00001812
Iteration 262/1000 | Loss: 0.00001812
Iteration 263/1000 | Loss: 0.00001812
Iteration 264/1000 | Loss: 0.00001812
Iteration 265/1000 | Loss: 0.00001812
Iteration 266/1000 | Loss: 0.00001812
Iteration 267/1000 | Loss: 0.00001812
Iteration 268/1000 | Loss: 0.00001812
Iteration 269/1000 | Loss: 0.00001812
Iteration 270/1000 | Loss: 0.00001812
Iteration 271/1000 | Loss: 0.00001812
Iteration 272/1000 | Loss: 0.00001812
Iteration 273/1000 | Loss: 0.00001812
Iteration 274/1000 | Loss: 0.00001812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [1.8123491827282123e-05, 1.8123491827282123e-05, 1.8123491827282123e-05, 1.8123491827282123e-05, 1.8123491827282123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8123491827282123e-05

Optimization complete. Final v2v error: 3.485353708267212 mm

Highest mean error: 4.751523494720459 mm for frame 124

Lowest mean error: 2.905951976776123 mm for frame 209

Saving results

Total time: 187.0420618057251
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856776
Iteration 2/25 | Loss: 0.00116371
Iteration 3/25 | Loss: 0.00099832
Iteration 4/25 | Loss: 0.00097314
Iteration 5/25 | Loss: 0.00096608
Iteration 6/25 | Loss: 0.00096302
Iteration 7/25 | Loss: 0.00096174
Iteration 8/25 | Loss: 0.00095918
Iteration 9/25 | Loss: 0.00095706
Iteration 10/25 | Loss: 0.00095638
Iteration 11/25 | Loss: 0.00095613
Iteration 12/25 | Loss: 0.00095603
Iteration 13/25 | Loss: 0.00095597
Iteration 14/25 | Loss: 0.00095596
Iteration 15/25 | Loss: 0.00095596
Iteration 16/25 | Loss: 0.00095596
Iteration 17/25 | Loss: 0.00095596
Iteration 18/25 | Loss: 0.00095596
Iteration 19/25 | Loss: 0.00095596
Iteration 20/25 | Loss: 0.00095596
Iteration 21/25 | Loss: 0.00095596
Iteration 22/25 | Loss: 0.00095596
Iteration 23/25 | Loss: 0.00095596
Iteration 24/25 | Loss: 0.00095596
Iteration 25/25 | Loss: 0.00095595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.57655001
Iteration 2/25 | Loss: 0.00067621
Iteration 3/25 | Loss: 0.00067621
Iteration 4/25 | Loss: 0.00067621
Iteration 5/25 | Loss: 0.00067621
Iteration 6/25 | Loss: 0.00067621
Iteration 7/25 | Loss: 0.00067621
Iteration 8/25 | Loss: 0.00067621
Iteration 9/25 | Loss: 0.00067621
Iteration 10/25 | Loss: 0.00067621
Iteration 11/25 | Loss: 0.00067621
Iteration 12/25 | Loss: 0.00067621
Iteration 13/25 | Loss: 0.00067621
Iteration 14/25 | Loss: 0.00067621
Iteration 15/25 | Loss: 0.00067621
Iteration 16/25 | Loss: 0.00067621
Iteration 17/25 | Loss: 0.00067621
Iteration 18/25 | Loss: 0.00067621
Iteration 19/25 | Loss: 0.00067621
Iteration 20/25 | Loss: 0.00067621
Iteration 21/25 | Loss: 0.00067621
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006762086995877326, 0.0006762086995877326, 0.0006762086995877326, 0.0006762086995877326, 0.0006762086995877326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006762086995877326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067621
Iteration 2/1000 | Loss: 0.00002386
Iteration 3/1000 | Loss: 0.00001558
Iteration 4/1000 | Loss: 0.00001413
Iteration 5/1000 | Loss: 0.00001323
Iteration 6/1000 | Loss: 0.00001281
Iteration 7/1000 | Loss: 0.00001235
Iteration 8/1000 | Loss: 0.00001206
Iteration 9/1000 | Loss: 0.00001185
Iteration 10/1000 | Loss: 0.00001175
Iteration 11/1000 | Loss: 0.00001170
Iteration 12/1000 | Loss: 0.00001168
Iteration 13/1000 | Loss: 0.00001168
Iteration 14/1000 | Loss: 0.00001167
Iteration 15/1000 | Loss: 0.00001166
Iteration 16/1000 | Loss: 0.00001166
Iteration 17/1000 | Loss: 0.00001165
Iteration 18/1000 | Loss: 0.00001160
Iteration 19/1000 | Loss: 0.00001160
Iteration 20/1000 | Loss: 0.00001160
Iteration 21/1000 | Loss: 0.00001160
Iteration 22/1000 | Loss: 0.00001159
Iteration 23/1000 | Loss: 0.00001159
Iteration 24/1000 | Loss: 0.00001158
Iteration 25/1000 | Loss: 0.00001157
Iteration 26/1000 | Loss: 0.00001157
Iteration 27/1000 | Loss: 0.00001156
Iteration 28/1000 | Loss: 0.00001156
Iteration 29/1000 | Loss: 0.00001156
Iteration 30/1000 | Loss: 0.00001156
Iteration 31/1000 | Loss: 0.00001155
Iteration 32/1000 | Loss: 0.00001154
Iteration 33/1000 | Loss: 0.00001153
Iteration 34/1000 | Loss: 0.00001153
Iteration 35/1000 | Loss: 0.00001152
Iteration 36/1000 | Loss: 0.00001151
Iteration 37/1000 | Loss: 0.00001151
Iteration 38/1000 | Loss: 0.00001150
Iteration 39/1000 | Loss: 0.00001150
Iteration 40/1000 | Loss: 0.00001149
Iteration 41/1000 | Loss: 0.00001149
Iteration 42/1000 | Loss: 0.00001149
Iteration 43/1000 | Loss: 0.00001148
Iteration 44/1000 | Loss: 0.00001148
Iteration 45/1000 | Loss: 0.00001148
Iteration 46/1000 | Loss: 0.00001148
Iteration 47/1000 | Loss: 0.00001147
Iteration 48/1000 | Loss: 0.00001147
Iteration 49/1000 | Loss: 0.00001147
Iteration 50/1000 | Loss: 0.00001146
Iteration 51/1000 | Loss: 0.00001146
Iteration 52/1000 | Loss: 0.00001146
Iteration 53/1000 | Loss: 0.00001145
Iteration 54/1000 | Loss: 0.00001145
Iteration 55/1000 | Loss: 0.00001145
Iteration 56/1000 | Loss: 0.00001145
Iteration 57/1000 | Loss: 0.00001145
Iteration 58/1000 | Loss: 0.00001145
Iteration 59/1000 | Loss: 0.00001145
Iteration 60/1000 | Loss: 0.00001145
Iteration 61/1000 | Loss: 0.00001145
Iteration 62/1000 | Loss: 0.00001144
Iteration 63/1000 | Loss: 0.00001144
Iteration 64/1000 | Loss: 0.00001144
Iteration 65/1000 | Loss: 0.00001144
Iteration 66/1000 | Loss: 0.00001144
Iteration 67/1000 | Loss: 0.00001144
Iteration 68/1000 | Loss: 0.00001143
Iteration 69/1000 | Loss: 0.00001143
Iteration 70/1000 | Loss: 0.00001143
Iteration 71/1000 | Loss: 0.00001143
Iteration 72/1000 | Loss: 0.00001142
Iteration 73/1000 | Loss: 0.00001142
Iteration 74/1000 | Loss: 0.00001142
Iteration 75/1000 | Loss: 0.00001142
Iteration 76/1000 | Loss: 0.00001142
Iteration 77/1000 | Loss: 0.00001142
Iteration 78/1000 | Loss: 0.00001142
Iteration 79/1000 | Loss: 0.00001142
Iteration 80/1000 | Loss: 0.00001142
Iteration 81/1000 | Loss: 0.00001142
Iteration 82/1000 | Loss: 0.00001142
Iteration 83/1000 | Loss: 0.00001142
Iteration 84/1000 | Loss: 0.00001142
Iteration 85/1000 | Loss: 0.00001142
Iteration 86/1000 | Loss: 0.00001142
Iteration 87/1000 | Loss: 0.00001142
Iteration 88/1000 | Loss: 0.00001142
Iteration 89/1000 | Loss: 0.00001141
Iteration 90/1000 | Loss: 0.00001141
Iteration 91/1000 | Loss: 0.00001141
Iteration 92/1000 | Loss: 0.00001141
Iteration 93/1000 | Loss: 0.00001141
Iteration 94/1000 | Loss: 0.00001141
Iteration 95/1000 | Loss: 0.00001141
Iteration 96/1000 | Loss: 0.00001141
Iteration 97/1000 | Loss: 0.00001140
Iteration 98/1000 | Loss: 0.00001140
Iteration 99/1000 | Loss: 0.00001140
Iteration 100/1000 | Loss: 0.00001140
Iteration 101/1000 | Loss: 0.00001140
Iteration 102/1000 | Loss: 0.00001140
Iteration 103/1000 | Loss: 0.00001140
Iteration 104/1000 | Loss: 0.00001140
Iteration 105/1000 | Loss: 0.00001140
Iteration 106/1000 | Loss: 0.00001140
Iteration 107/1000 | Loss: 0.00001140
Iteration 108/1000 | Loss: 0.00001140
Iteration 109/1000 | Loss: 0.00001140
Iteration 110/1000 | Loss: 0.00001139
Iteration 111/1000 | Loss: 0.00001139
Iteration 112/1000 | Loss: 0.00001139
Iteration 113/1000 | Loss: 0.00001139
Iteration 114/1000 | Loss: 0.00001139
Iteration 115/1000 | Loss: 0.00001139
Iteration 116/1000 | Loss: 0.00001139
Iteration 117/1000 | Loss: 0.00001139
Iteration 118/1000 | Loss: 0.00001139
Iteration 119/1000 | Loss: 0.00001139
Iteration 120/1000 | Loss: 0.00001139
Iteration 121/1000 | Loss: 0.00001139
Iteration 122/1000 | Loss: 0.00001139
Iteration 123/1000 | Loss: 0.00001139
Iteration 124/1000 | Loss: 0.00001138
Iteration 125/1000 | Loss: 0.00001138
Iteration 126/1000 | Loss: 0.00001138
Iteration 127/1000 | Loss: 0.00001138
Iteration 128/1000 | Loss: 0.00001138
Iteration 129/1000 | Loss: 0.00001138
Iteration 130/1000 | Loss: 0.00001138
Iteration 131/1000 | Loss: 0.00001138
Iteration 132/1000 | Loss: 0.00001138
Iteration 133/1000 | Loss: 0.00001138
Iteration 134/1000 | Loss: 0.00001138
Iteration 135/1000 | Loss: 0.00001138
Iteration 136/1000 | Loss: 0.00001138
Iteration 137/1000 | Loss: 0.00001138
Iteration 138/1000 | Loss: 0.00001138
Iteration 139/1000 | Loss: 0.00001138
Iteration 140/1000 | Loss: 0.00001138
Iteration 141/1000 | Loss: 0.00001138
Iteration 142/1000 | Loss: 0.00001138
Iteration 143/1000 | Loss: 0.00001137
Iteration 144/1000 | Loss: 0.00001137
Iteration 145/1000 | Loss: 0.00001137
Iteration 146/1000 | Loss: 0.00001137
Iteration 147/1000 | Loss: 0.00001137
Iteration 148/1000 | Loss: 0.00001137
Iteration 149/1000 | Loss: 0.00001137
Iteration 150/1000 | Loss: 0.00001137
Iteration 151/1000 | Loss: 0.00001137
Iteration 152/1000 | Loss: 0.00001137
Iteration 153/1000 | Loss: 0.00001137
Iteration 154/1000 | Loss: 0.00001137
Iteration 155/1000 | Loss: 0.00001137
Iteration 156/1000 | Loss: 0.00001137
Iteration 157/1000 | Loss: 0.00001137
Iteration 158/1000 | Loss: 0.00001137
Iteration 159/1000 | Loss: 0.00001137
Iteration 160/1000 | Loss: 0.00001137
Iteration 161/1000 | Loss: 0.00001137
Iteration 162/1000 | Loss: 0.00001137
Iteration 163/1000 | Loss: 0.00001137
Iteration 164/1000 | Loss: 0.00001137
Iteration 165/1000 | Loss: 0.00001137
Iteration 166/1000 | Loss: 0.00001137
Iteration 167/1000 | Loss: 0.00001137
Iteration 168/1000 | Loss: 0.00001137
Iteration 169/1000 | Loss: 0.00001137
Iteration 170/1000 | Loss: 0.00001137
Iteration 171/1000 | Loss: 0.00001137
Iteration 172/1000 | Loss: 0.00001137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.1371650543878786e-05, 1.1371650543878786e-05, 1.1371650543878786e-05, 1.1371650543878786e-05, 1.1371650543878786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1371650543878786e-05

Optimization complete. Final v2v error: 2.8156754970550537 mm

Highest mean error: 3.2247629165649414 mm for frame 143

Lowest mean error: 2.5214977264404297 mm for frame 6

Saving results

Total time: 52.25440859794617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392730
Iteration 2/25 | Loss: 0.00106494
Iteration 3/25 | Loss: 0.00097316
Iteration 4/25 | Loss: 0.00096568
Iteration 5/25 | Loss: 0.00096339
Iteration 6/25 | Loss: 0.00096311
Iteration 7/25 | Loss: 0.00096311
Iteration 8/25 | Loss: 0.00096311
Iteration 9/25 | Loss: 0.00096311
Iteration 10/25 | Loss: 0.00096311
Iteration 11/25 | Loss: 0.00096311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009631115244701505, 0.0009631115244701505, 0.0009631115244701505, 0.0009631115244701505, 0.0009631115244701505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009631115244701505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31241751
Iteration 2/25 | Loss: 0.00093023
Iteration 3/25 | Loss: 0.00093023
Iteration 4/25 | Loss: 0.00093023
Iteration 5/25 | Loss: 0.00093023
Iteration 6/25 | Loss: 0.00093023
Iteration 7/25 | Loss: 0.00093023
Iteration 8/25 | Loss: 0.00093023
Iteration 9/25 | Loss: 0.00093023
Iteration 10/25 | Loss: 0.00093023
Iteration 11/25 | Loss: 0.00093023
Iteration 12/25 | Loss: 0.00093023
Iteration 13/25 | Loss: 0.00093023
Iteration 14/25 | Loss: 0.00093023
Iteration 15/25 | Loss: 0.00093023
Iteration 16/25 | Loss: 0.00093023
Iteration 17/25 | Loss: 0.00093023
Iteration 18/25 | Loss: 0.00093023
Iteration 19/25 | Loss: 0.00093023
Iteration 20/25 | Loss: 0.00093023
Iteration 21/25 | Loss: 0.00093023
Iteration 22/25 | Loss: 0.00093023
Iteration 23/25 | Loss: 0.00093023
Iteration 24/25 | Loss: 0.00093023
Iteration 25/25 | Loss: 0.00093023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093023
Iteration 2/1000 | Loss: 0.00003637
Iteration 3/1000 | Loss: 0.00001959
Iteration 4/1000 | Loss: 0.00001424
Iteration 5/1000 | Loss: 0.00001273
Iteration 6/1000 | Loss: 0.00001205
Iteration 7/1000 | Loss: 0.00001165
Iteration 8/1000 | Loss: 0.00001128
Iteration 9/1000 | Loss: 0.00001104
Iteration 10/1000 | Loss: 0.00001098
Iteration 11/1000 | Loss: 0.00001092
Iteration 12/1000 | Loss: 0.00001082
Iteration 13/1000 | Loss: 0.00001075
Iteration 14/1000 | Loss: 0.00001072
Iteration 15/1000 | Loss: 0.00001071
Iteration 16/1000 | Loss: 0.00001070
Iteration 17/1000 | Loss: 0.00001063
Iteration 18/1000 | Loss: 0.00001060
Iteration 19/1000 | Loss: 0.00001059
Iteration 20/1000 | Loss: 0.00001059
Iteration 21/1000 | Loss: 0.00001057
Iteration 22/1000 | Loss: 0.00001054
Iteration 23/1000 | Loss: 0.00001053
Iteration 24/1000 | Loss: 0.00001053
Iteration 25/1000 | Loss: 0.00001053
Iteration 26/1000 | Loss: 0.00001053
Iteration 27/1000 | Loss: 0.00001052
Iteration 28/1000 | Loss: 0.00001051
Iteration 29/1000 | Loss: 0.00001050
Iteration 30/1000 | Loss: 0.00001049
Iteration 31/1000 | Loss: 0.00001049
Iteration 32/1000 | Loss: 0.00001048
Iteration 33/1000 | Loss: 0.00001048
Iteration 34/1000 | Loss: 0.00001048
Iteration 35/1000 | Loss: 0.00001048
Iteration 36/1000 | Loss: 0.00001048
Iteration 37/1000 | Loss: 0.00001047
Iteration 38/1000 | Loss: 0.00001047
Iteration 39/1000 | Loss: 0.00001047
Iteration 40/1000 | Loss: 0.00001047
Iteration 41/1000 | Loss: 0.00001047
Iteration 42/1000 | Loss: 0.00001047
Iteration 43/1000 | Loss: 0.00001047
Iteration 44/1000 | Loss: 0.00001046
Iteration 45/1000 | Loss: 0.00001046
Iteration 46/1000 | Loss: 0.00001046
Iteration 47/1000 | Loss: 0.00001046
Iteration 48/1000 | Loss: 0.00001045
Iteration 49/1000 | Loss: 0.00001045
Iteration 50/1000 | Loss: 0.00001044
Iteration 51/1000 | Loss: 0.00001043
Iteration 52/1000 | Loss: 0.00001043
Iteration 53/1000 | Loss: 0.00001043
Iteration 54/1000 | Loss: 0.00001043
Iteration 55/1000 | Loss: 0.00001043
Iteration 56/1000 | Loss: 0.00001043
Iteration 57/1000 | Loss: 0.00001042
Iteration 58/1000 | Loss: 0.00001042
Iteration 59/1000 | Loss: 0.00001042
Iteration 60/1000 | Loss: 0.00001042
Iteration 61/1000 | Loss: 0.00001041
Iteration 62/1000 | Loss: 0.00001041
Iteration 63/1000 | Loss: 0.00001040
Iteration 64/1000 | Loss: 0.00001040
Iteration 65/1000 | Loss: 0.00001040
Iteration 66/1000 | Loss: 0.00001039
Iteration 67/1000 | Loss: 0.00001039
Iteration 68/1000 | Loss: 0.00001039
Iteration 69/1000 | Loss: 0.00001039
Iteration 70/1000 | Loss: 0.00001038
Iteration 71/1000 | Loss: 0.00001038
Iteration 72/1000 | Loss: 0.00001038
Iteration 73/1000 | Loss: 0.00001038
Iteration 74/1000 | Loss: 0.00001038
Iteration 75/1000 | Loss: 0.00001038
Iteration 76/1000 | Loss: 0.00001038
Iteration 77/1000 | Loss: 0.00001038
Iteration 78/1000 | Loss: 0.00001038
Iteration 79/1000 | Loss: 0.00001038
Iteration 80/1000 | Loss: 0.00001038
Iteration 81/1000 | Loss: 0.00001038
Iteration 82/1000 | Loss: 0.00001038
Iteration 83/1000 | Loss: 0.00001038
Iteration 84/1000 | Loss: 0.00001038
Iteration 85/1000 | Loss: 0.00001038
Iteration 86/1000 | Loss: 0.00001038
Iteration 87/1000 | Loss: 0.00001038
Iteration 88/1000 | Loss: 0.00001038
Iteration 89/1000 | Loss: 0.00001038
Iteration 90/1000 | Loss: 0.00001038
Iteration 91/1000 | Loss: 0.00001038
Iteration 92/1000 | Loss: 0.00001038
Iteration 93/1000 | Loss: 0.00001038
Iteration 94/1000 | Loss: 0.00001038
Iteration 95/1000 | Loss: 0.00001038
Iteration 96/1000 | Loss: 0.00001038
Iteration 97/1000 | Loss: 0.00001038
Iteration 98/1000 | Loss: 0.00001038
Iteration 99/1000 | Loss: 0.00001038
Iteration 100/1000 | Loss: 0.00001038
Iteration 101/1000 | Loss: 0.00001038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.0376888894825242e-05, 1.0376888894825242e-05, 1.0376888894825242e-05, 1.0376888894825242e-05, 1.0376888894825242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0376888894825242e-05

Optimization complete. Final v2v error: 2.673283100128174 mm

Highest mean error: 3.8012828826904297 mm for frame 88

Lowest mean error: 2.2765204906463623 mm for frame 12

Saving results

Total time: 31.63544988632202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398316
Iteration 2/25 | Loss: 0.00114576
Iteration 3/25 | Loss: 0.00097954
Iteration 4/25 | Loss: 0.00096087
Iteration 5/25 | Loss: 0.00095846
Iteration 6/25 | Loss: 0.00095810
Iteration 7/25 | Loss: 0.00095810
Iteration 8/25 | Loss: 0.00095810
Iteration 9/25 | Loss: 0.00095810
Iteration 10/25 | Loss: 0.00095810
Iteration 11/25 | Loss: 0.00095810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009580978658050299, 0.0009580978658050299, 0.0009580978658050299, 0.0009580978658050299, 0.0009580978658050299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009580978658050299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29699385
Iteration 2/25 | Loss: 0.00069031
Iteration 3/25 | Loss: 0.00069030
Iteration 4/25 | Loss: 0.00069030
Iteration 5/25 | Loss: 0.00069030
Iteration 6/25 | Loss: 0.00069030
Iteration 7/25 | Loss: 0.00069030
Iteration 8/25 | Loss: 0.00069030
Iteration 9/25 | Loss: 0.00069030
Iteration 10/25 | Loss: 0.00069030
Iteration 11/25 | Loss: 0.00069030
Iteration 12/25 | Loss: 0.00069030
Iteration 13/25 | Loss: 0.00069030
Iteration 14/25 | Loss: 0.00069030
Iteration 15/25 | Loss: 0.00069030
Iteration 16/25 | Loss: 0.00069030
Iteration 17/25 | Loss: 0.00069030
Iteration 18/25 | Loss: 0.00069030
Iteration 19/25 | Loss: 0.00069030
Iteration 20/25 | Loss: 0.00069030
Iteration 21/25 | Loss: 0.00069030
Iteration 22/25 | Loss: 0.00069030
Iteration 23/25 | Loss: 0.00069030
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000690299435518682, 0.000690299435518682, 0.000690299435518682, 0.000690299435518682, 0.000690299435518682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000690299435518682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069030
Iteration 2/1000 | Loss: 0.00001806
Iteration 3/1000 | Loss: 0.00001221
Iteration 4/1000 | Loss: 0.00001115
Iteration 5/1000 | Loss: 0.00001054
Iteration 6/1000 | Loss: 0.00001013
Iteration 7/1000 | Loss: 0.00000983
Iteration 8/1000 | Loss: 0.00000973
Iteration 9/1000 | Loss: 0.00000951
Iteration 10/1000 | Loss: 0.00000948
Iteration 11/1000 | Loss: 0.00000938
Iteration 12/1000 | Loss: 0.00000929
Iteration 13/1000 | Loss: 0.00000927
Iteration 14/1000 | Loss: 0.00000926
Iteration 15/1000 | Loss: 0.00000925
Iteration 16/1000 | Loss: 0.00000924
Iteration 17/1000 | Loss: 0.00000924
Iteration 18/1000 | Loss: 0.00000924
Iteration 19/1000 | Loss: 0.00000924
Iteration 20/1000 | Loss: 0.00000924
Iteration 21/1000 | Loss: 0.00000924
Iteration 22/1000 | Loss: 0.00000924
Iteration 23/1000 | Loss: 0.00000924
Iteration 24/1000 | Loss: 0.00000924
Iteration 25/1000 | Loss: 0.00000924
Iteration 26/1000 | Loss: 0.00000924
Iteration 27/1000 | Loss: 0.00000924
Iteration 28/1000 | Loss: 0.00000923
Iteration 29/1000 | Loss: 0.00000921
Iteration 30/1000 | Loss: 0.00000920
Iteration 31/1000 | Loss: 0.00000919
Iteration 32/1000 | Loss: 0.00000919
Iteration 33/1000 | Loss: 0.00000918
Iteration 34/1000 | Loss: 0.00000917
Iteration 35/1000 | Loss: 0.00000916
Iteration 36/1000 | Loss: 0.00000915
Iteration 37/1000 | Loss: 0.00000914
Iteration 38/1000 | Loss: 0.00000914
Iteration 39/1000 | Loss: 0.00000914
Iteration 40/1000 | Loss: 0.00000913
Iteration 41/1000 | Loss: 0.00000913
Iteration 42/1000 | Loss: 0.00000913
Iteration 43/1000 | Loss: 0.00000913
Iteration 44/1000 | Loss: 0.00000912
Iteration 45/1000 | Loss: 0.00000911
Iteration 46/1000 | Loss: 0.00000910
Iteration 47/1000 | Loss: 0.00000909
Iteration 48/1000 | Loss: 0.00000909
Iteration 49/1000 | Loss: 0.00000909
Iteration 50/1000 | Loss: 0.00000908
Iteration 51/1000 | Loss: 0.00000908
Iteration 52/1000 | Loss: 0.00000908
Iteration 53/1000 | Loss: 0.00000907
Iteration 54/1000 | Loss: 0.00000907
Iteration 55/1000 | Loss: 0.00000907
Iteration 56/1000 | Loss: 0.00000907
Iteration 57/1000 | Loss: 0.00000907
Iteration 58/1000 | Loss: 0.00000906
Iteration 59/1000 | Loss: 0.00000906
Iteration 60/1000 | Loss: 0.00000906
Iteration 61/1000 | Loss: 0.00000906
Iteration 62/1000 | Loss: 0.00000906
Iteration 63/1000 | Loss: 0.00000906
Iteration 64/1000 | Loss: 0.00000906
Iteration 65/1000 | Loss: 0.00000906
Iteration 66/1000 | Loss: 0.00000906
Iteration 67/1000 | Loss: 0.00000906
Iteration 68/1000 | Loss: 0.00000906
Iteration 69/1000 | Loss: 0.00000906
Iteration 70/1000 | Loss: 0.00000906
Iteration 71/1000 | Loss: 0.00000906
Iteration 72/1000 | Loss: 0.00000906
Iteration 73/1000 | Loss: 0.00000905
Iteration 74/1000 | Loss: 0.00000905
Iteration 75/1000 | Loss: 0.00000905
Iteration 76/1000 | Loss: 0.00000905
Iteration 77/1000 | Loss: 0.00000905
Iteration 78/1000 | Loss: 0.00000905
Iteration 79/1000 | Loss: 0.00000905
Iteration 80/1000 | Loss: 0.00000905
Iteration 81/1000 | Loss: 0.00000905
Iteration 82/1000 | Loss: 0.00000905
Iteration 83/1000 | Loss: 0.00000904
Iteration 84/1000 | Loss: 0.00000904
Iteration 85/1000 | Loss: 0.00000904
Iteration 86/1000 | Loss: 0.00000904
Iteration 87/1000 | Loss: 0.00000904
Iteration 88/1000 | Loss: 0.00000904
Iteration 89/1000 | Loss: 0.00000904
Iteration 90/1000 | Loss: 0.00000904
Iteration 91/1000 | Loss: 0.00000904
Iteration 92/1000 | Loss: 0.00000903
Iteration 93/1000 | Loss: 0.00000903
Iteration 94/1000 | Loss: 0.00000903
Iteration 95/1000 | Loss: 0.00000903
Iteration 96/1000 | Loss: 0.00000903
Iteration 97/1000 | Loss: 0.00000903
Iteration 98/1000 | Loss: 0.00000903
Iteration 99/1000 | Loss: 0.00000903
Iteration 100/1000 | Loss: 0.00000903
Iteration 101/1000 | Loss: 0.00000903
Iteration 102/1000 | Loss: 0.00000902
Iteration 103/1000 | Loss: 0.00000902
Iteration 104/1000 | Loss: 0.00000902
Iteration 105/1000 | Loss: 0.00000902
Iteration 106/1000 | Loss: 0.00000902
Iteration 107/1000 | Loss: 0.00000902
Iteration 108/1000 | Loss: 0.00000902
Iteration 109/1000 | Loss: 0.00000902
Iteration 110/1000 | Loss: 0.00000902
Iteration 111/1000 | Loss: 0.00000902
Iteration 112/1000 | Loss: 0.00000902
Iteration 113/1000 | Loss: 0.00000902
Iteration 114/1000 | Loss: 0.00000902
Iteration 115/1000 | Loss: 0.00000901
Iteration 116/1000 | Loss: 0.00000901
Iteration 117/1000 | Loss: 0.00000901
Iteration 118/1000 | Loss: 0.00000901
Iteration 119/1000 | Loss: 0.00000901
Iteration 120/1000 | Loss: 0.00000901
Iteration 121/1000 | Loss: 0.00000901
Iteration 122/1000 | Loss: 0.00000901
Iteration 123/1000 | Loss: 0.00000901
Iteration 124/1000 | Loss: 0.00000901
Iteration 125/1000 | Loss: 0.00000900
Iteration 126/1000 | Loss: 0.00000900
Iteration 127/1000 | Loss: 0.00000900
Iteration 128/1000 | Loss: 0.00000900
Iteration 129/1000 | Loss: 0.00000900
Iteration 130/1000 | Loss: 0.00000900
Iteration 131/1000 | Loss: 0.00000900
Iteration 132/1000 | Loss: 0.00000900
Iteration 133/1000 | Loss: 0.00000900
Iteration 134/1000 | Loss: 0.00000900
Iteration 135/1000 | Loss: 0.00000900
Iteration 136/1000 | Loss: 0.00000900
Iteration 137/1000 | Loss: 0.00000899
Iteration 138/1000 | Loss: 0.00000899
Iteration 139/1000 | Loss: 0.00000899
Iteration 140/1000 | Loss: 0.00000899
Iteration 141/1000 | Loss: 0.00000899
Iteration 142/1000 | Loss: 0.00000899
Iteration 143/1000 | Loss: 0.00000899
Iteration 144/1000 | Loss: 0.00000899
Iteration 145/1000 | Loss: 0.00000899
Iteration 146/1000 | Loss: 0.00000899
Iteration 147/1000 | Loss: 0.00000899
Iteration 148/1000 | Loss: 0.00000899
Iteration 149/1000 | Loss: 0.00000899
Iteration 150/1000 | Loss: 0.00000899
Iteration 151/1000 | Loss: 0.00000899
Iteration 152/1000 | Loss: 0.00000898
Iteration 153/1000 | Loss: 0.00000898
Iteration 154/1000 | Loss: 0.00000898
Iteration 155/1000 | Loss: 0.00000898
Iteration 156/1000 | Loss: 0.00000898
Iteration 157/1000 | Loss: 0.00000898
Iteration 158/1000 | Loss: 0.00000898
Iteration 159/1000 | Loss: 0.00000898
Iteration 160/1000 | Loss: 0.00000898
Iteration 161/1000 | Loss: 0.00000898
Iteration 162/1000 | Loss: 0.00000898
Iteration 163/1000 | Loss: 0.00000897
Iteration 164/1000 | Loss: 0.00000897
Iteration 165/1000 | Loss: 0.00000897
Iteration 166/1000 | Loss: 0.00000897
Iteration 167/1000 | Loss: 0.00000897
Iteration 168/1000 | Loss: 0.00000897
Iteration 169/1000 | Loss: 0.00000897
Iteration 170/1000 | Loss: 0.00000897
Iteration 171/1000 | Loss: 0.00000896
Iteration 172/1000 | Loss: 0.00000896
Iteration 173/1000 | Loss: 0.00000896
Iteration 174/1000 | Loss: 0.00000896
Iteration 175/1000 | Loss: 0.00000896
Iteration 176/1000 | Loss: 0.00000896
Iteration 177/1000 | Loss: 0.00000896
Iteration 178/1000 | Loss: 0.00000896
Iteration 179/1000 | Loss: 0.00000896
Iteration 180/1000 | Loss: 0.00000896
Iteration 181/1000 | Loss: 0.00000896
Iteration 182/1000 | Loss: 0.00000896
Iteration 183/1000 | Loss: 0.00000896
Iteration 184/1000 | Loss: 0.00000896
Iteration 185/1000 | Loss: 0.00000896
Iteration 186/1000 | Loss: 0.00000896
Iteration 187/1000 | Loss: 0.00000896
Iteration 188/1000 | Loss: 0.00000896
Iteration 189/1000 | Loss: 0.00000896
Iteration 190/1000 | Loss: 0.00000896
Iteration 191/1000 | Loss: 0.00000896
Iteration 192/1000 | Loss: 0.00000895
Iteration 193/1000 | Loss: 0.00000895
Iteration 194/1000 | Loss: 0.00000895
Iteration 195/1000 | Loss: 0.00000895
Iteration 196/1000 | Loss: 0.00000895
Iteration 197/1000 | Loss: 0.00000895
Iteration 198/1000 | Loss: 0.00000895
Iteration 199/1000 | Loss: 0.00000895
Iteration 200/1000 | Loss: 0.00000895
Iteration 201/1000 | Loss: 0.00000894
Iteration 202/1000 | Loss: 0.00000894
Iteration 203/1000 | Loss: 0.00000894
Iteration 204/1000 | Loss: 0.00000894
Iteration 205/1000 | Loss: 0.00000894
Iteration 206/1000 | Loss: 0.00000894
Iteration 207/1000 | Loss: 0.00000894
Iteration 208/1000 | Loss: 0.00000894
Iteration 209/1000 | Loss: 0.00000894
Iteration 210/1000 | Loss: 0.00000893
Iteration 211/1000 | Loss: 0.00000893
Iteration 212/1000 | Loss: 0.00000893
Iteration 213/1000 | Loss: 0.00000893
Iteration 214/1000 | Loss: 0.00000893
Iteration 215/1000 | Loss: 0.00000893
Iteration 216/1000 | Loss: 0.00000893
Iteration 217/1000 | Loss: 0.00000893
Iteration 218/1000 | Loss: 0.00000893
Iteration 219/1000 | Loss: 0.00000893
Iteration 220/1000 | Loss: 0.00000893
Iteration 221/1000 | Loss: 0.00000893
Iteration 222/1000 | Loss: 0.00000893
Iteration 223/1000 | Loss: 0.00000893
Iteration 224/1000 | Loss: 0.00000893
Iteration 225/1000 | Loss: 0.00000893
Iteration 226/1000 | Loss: 0.00000893
Iteration 227/1000 | Loss: 0.00000893
Iteration 228/1000 | Loss: 0.00000893
Iteration 229/1000 | Loss: 0.00000893
Iteration 230/1000 | Loss: 0.00000893
Iteration 231/1000 | Loss: 0.00000893
Iteration 232/1000 | Loss: 0.00000893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [8.931869160733186e-06, 8.931869160733186e-06, 8.931869160733186e-06, 8.931869160733186e-06, 8.931869160733186e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.931869160733186e-06

Optimization complete. Final v2v error: 2.5467119216918945 mm

Highest mean error: 2.9132485389709473 mm for frame 63

Lowest mean error: 2.091660499572754 mm for frame 8

Saving results

Total time: 35.4655487537384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394718
Iteration 2/25 | Loss: 0.00107045
Iteration 3/25 | Loss: 0.00095333
Iteration 4/25 | Loss: 0.00094033
Iteration 5/25 | Loss: 0.00093740
Iteration 6/25 | Loss: 0.00093740
Iteration 7/25 | Loss: 0.00093740
Iteration 8/25 | Loss: 0.00093740
Iteration 9/25 | Loss: 0.00093740
Iteration 10/25 | Loss: 0.00093740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009373996290378273, 0.0009373996290378273, 0.0009373996290378273, 0.0009373996290378273, 0.0009373996290378273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009373996290378273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55782652
Iteration 2/25 | Loss: 0.00070644
Iteration 3/25 | Loss: 0.00070644
Iteration 4/25 | Loss: 0.00070644
Iteration 5/25 | Loss: 0.00070643
Iteration 6/25 | Loss: 0.00070643
Iteration 7/25 | Loss: 0.00070643
Iteration 8/25 | Loss: 0.00070643
Iteration 9/25 | Loss: 0.00070643
Iteration 10/25 | Loss: 0.00070643
Iteration 11/25 | Loss: 0.00070643
Iteration 12/25 | Loss: 0.00070643
Iteration 13/25 | Loss: 0.00070643
Iteration 14/25 | Loss: 0.00070643
Iteration 15/25 | Loss: 0.00070643
Iteration 16/25 | Loss: 0.00070643
Iteration 17/25 | Loss: 0.00070643
Iteration 18/25 | Loss: 0.00070643
Iteration 19/25 | Loss: 0.00070643
Iteration 20/25 | Loss: 0.00070643
Iteration 21/25 | Loss: 0.00070643
Iteration 22/25 | Loss: 0.00070643
Iteration 23/25 | Loss: 0.00070643
Iteration 24/25 | Loss: 0.00070643
Iteration 25/25 | Loss: 0.00070643
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007064330275170505, 0.0007064330275170505, 0.0007064330275170505, 0.0007064330275170505, 0.0007064330275170505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007064330275170505

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070643
Iteration 2/1000 | Loss: 0.00002275
Iteration 3/1000 | Loss: 0.00001321
Iteration 4/1000 | Loss: 0.00001105
Iteration 5/1000 | Loss: 0.00001022
Iteration 6/1000 | Loss: 0.00000978
Iteration 7/1000 | Loss: 0.00000942
Iteration 8/1000 | Loss: 0.00000927
Iteration 9/1000 | Loss: 0.00000925
Iteration 10/1000 | Loss: 0.00000924
Iteration 11/1000 | Loss: 0.00000924
Iteration 12/1000 | Loss: 0.00000916
Iteration 13/1000 | Loss: 0.00000908
Iteration 14/1000 | Loss: 0.00000906
Iteration 15/1000 | Loss: 0.00000899
Iteration 16/1000 | Loss: 0.00000898
Iteration 17/1000 | Loss: 0.00000896
Iteration 18/1000 | Loss: 0.00000896
Iteration 19/1000 | Loss: 0.00000895
Iteration 20/1000 | Loss: 0.00000895
Iteration 21/1000 | Loss: 0.00000894
Iteration 22/1000 | Loss: 0.00000892
Iteration 23/1000 | Loss: 0.00000890
Iteration 24/1000 | Loss: 0.00000890
Iteration 25/1000 | Loss: 0.00000890
Iteration 26/1000 | Loss: 0.00000889
Iteration 27/1000 | Loss: 0.00000889
Iteration 28/1000 | Loss: 0.00000889
Iteration 29/1000 | Loss: 0.00000889
Iteration 30/1000 | Loss: 0.00000888
Iteration 31/1000 | Loss: 0.00000888
Iteration 32/1000 | Loss: 0.00000887
Iteration 33/1000 | Loss: 0.00000887
Iteration 34/1000 | Loss: 0.00000887
Iteration 35/1000 | Loss: 0.00000886
Iteration 36/1000 | Loss: 0.00000886
Iteration 37/1000 | Loss: 0.00000885
Iteration 38/1000 | Loss: 0.00000885
Iteration 39/1000 | Loss: 0.00000885
Iteration 40/1000 | Loss: 0.00000885
Iteration 41/1000 | Loss: 0.00000885
Iteration 42/1000 | Loss: 0.00000885
Iteration 43/1000 | Loss: 0.00000885
Iteration 44/1000 | Loss: 0.00000885
Iteration 45/1000 | Loss: 0.00000884
Iteration 46/1000 | Loss: 0.00000884
Iteration 47/1000 | Loss: 0.00000884
Iteration 48/1000 | Loss: 0.00000883
Iteration 49/1000 | Loss: 0.00000883
Iteration 50/1000 | Loss: 0.00000881
Iteration 51/1000 | Loss: 0.00000881
Iteration 52/1000 | Loss: 0.00000881
Iteration 53/1000 | Loss: 0.00000881
Iteration 54/1000 | Loss: 0.00000881
Iteration 55/1000 | Loss: 0.00000881
Iteration 56/1000 | Loss: 0.00000881
Iteration 57/1000 | Loss: 0.00000881
Iteration 58/1000 | Loss: 0.00000881
Iteration 59/1000 | Loss: 0.00000881
Iteration 60/1000 | Loss: 0.00000881
Iteration 61/1000 | Loss: 0.00000880
Iteration 62/1000 | Loss: 0.00000880
Iteration 63/1000 | Loss: 0.00000880
Iteration 64/1000 | Loss: 0.00000880
Iteration 65/1000 | Loss: 0.00000880
Iteration 66/1000 | Loss: 0.00000880
Iteration 67/1000 | Loss: 0.00000880
Iteration 68/1000 | Loss: 0.00000880
Iteration 69/1000 | Loss: 0.00000879
Iteration 70/1000 | Loss: 0.00000876
Iteration 71/1000 | Loss: 0.00000876
Iteration 72/1000 | Loss: 0.00000875
Iteration 73/1000 | Loss: 0.00000875
Iteration 74/1000 | Loss: 0.00000875
Iteration 75/1000 | Loss: 0.00000875
Iteration 76/1000 | Loss: 0.00000875
Iteration 77/1000 | Loss: 0.00000872
Iteration 78/1000 | Loss: 0.00000872
Iteration 79/1000 | Loss: 0.00000872
Iteration 80/1000 | Loss: 0.00000872
Iteration 81/1000 | Loss: 0.00000872
Iteration 82/1000 | Loss: 0.00000871
Iteration 83/1000 | Loss: 0.00000871
Iteration 84/1000 | Loss: 0.00000871
Iteration 85/1000 | Loss: 0.00000871
Iteration 86/1000 | Loss: 0.00000871
Iteration 87/1000 | Loss: 0.00000871
Iteration 88/1000 | Loss: 0.00000871
Iteration 89/1000 | Loss: 0.00000871
Iteration 90/1000 | Loss: 0.00000870
Iteration 91/1000 | Loss: 0.00000870
Iteration 92/1000 | Loss: 0.00000870
Iteration 93/1000 | Loss: 0.00000869
Iteration 94/1000 | Loss: 0.00000869
Iteration 95/1000 | Loss: 0.00000869
Iteration 96/1000 | Loss: 0.00000868
Iteration 97/1000 | Loss: 0.00000868
Iteration 98/1000 | Loss: 0.00000868
Iteration 99/1000 | Loss: 0.00000868
Iteration 100/1000 | Loss: 0.00000867
Iteration 101/1000 | Loss: 0.00000867
Iteration 102/1000 | Loss: 0.00000866
Iteration 103/1000 | Loss: 0.00000866
Iteration 104/1000 | Loss: 0.00000866
Iteration 105/1000 | Loss: 0.00000865
Iteration 106/1000 | Loss: 0.00000865
Iteration 107/1000 | Loss: 0.00000865
Iteration 108/1000 | Loss: 0.00000865
Iteration 109/1000 | Loss: 0.00000865
Iteration 110/1000 | Loss: 0.00000865
Iteration 111/1000 | Loss: 0.00000865
Iteration 112/1000 | Loss: 0.00000865
Iteration 113/1000 | Loss: 0.00000865
Iteration 114/1000 | Loss: 0.00000865
Iteration 115/1000 | Loss: 0.00000865
Iteration 116/1000 | Loss: 0.00000865
Iteration 117/1000 | Loss: 0.00000865
Iteration 118/1000 | Loss: 0.00000864
Iteration 119/1000 | Loss: 0.00000864
Iteration 120/1000 | Loss: 0.00000864
Iteration 121/1000 | Loss: 0.00000864
Iteration 122/1000 | Loss: 0.00000863
Iteration 123/1000 | Loss: 0.00000863
Iteration 124/1000 | Loss: 0.00000863
Iteration 125/1000 | Loss: 0.00000863
Iteration 126/1000 | Loss: 0.00000863
Iteration 127/1000 | Loss: 0.00000863
Iteration 128/1000 | Loss: 0.00000863
Iteration 129/1000 | Loss: 0.00000863
Iteration 130/1000 | Loss: 0.00000863
Iteration 131/1000 | Loss: 0.00000863
Iteration 132/1000 | Loss: 0.00000863
Iteration 133/1000 | Loss: 0.00000863
Iteration 134/1000 | Loss: 0.00000863
Iteration 135/1000 | Loss: 0.00000862
Iteration 136/1000 | Loss: 0.00000862
Iteration 137/1000 | Loss: 0.00000862
Iteration 138/1000 | Loss: 0.00000862
Iteration 139/1000 | Loss: 0.00000862
Iteration 140/1000 | Loss: 0.00000862
Iteration 141/1000 | Loss: 0.00000862
Iteration 142/1000 | Loss: 0.00000862
Iteration 143/1000 | Loss: 0.00000862
Iteration 144/1000 | Loss: 0.00000862
Iteration 145/1000 | Loss: 0.00000862
Iteration 146/1000 | Loss: 0.00000862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [8.62461092765443e-06, 8.62461092765443e-06, 8.62461092765443e-06, 8.62461092765443e-06, 8.62461092765443e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.62461092765443e-06

Optimization complete. Final v2v error: 2.4640402793884277 mm

Highest mean error: 3.0960474014282227 mm for frame 46

Lowest mean error: 2.0414586067199707 mm for frame 266

Saving results

Total time: 35.48727989196777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0269/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0269/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793693
Iteration 2/25 | Loss: 0.00154446
Iteration 3/25 | Loss: 0.00104591
Iteration 4/25 | Loss: 0.00099523
Iteration 5/25 | Loss: 0.00098853
Iteration 6/25 | Loss: 0.00100189
Iteration 7/25 | Loss: 0.00099143
Iteration 8/25 | Loss: 0.00098891
Iteration 9/25 | Loss: 0.00099108
Iteration 10/25 | Loss: 0.00098812
Iteration 11/25 | Loss: 0.00098213
Iteration 12/25 | Loss: 0.00097670
Iteration 13/25 | Loss: 0.00097983
Iteration 14/25 | Loss: 0.00097924
Iteration 15/25 | Loss: 0.00097773
Iteration 16/25 | Loss: 0.00097482
Iteration 17/25 | Loss: 0.00097443
Iteration 18/25 | Loss: 0.00097425
Iteration 19/25 | Loss: 0.00097425
Iteration 20/25 | Loss: 0.00097425
Iteration 21/25 | Loss: 0.00097425
Iteration 22/25 | Loss: 0.00097425
Iteration 23/25 | Loss: 0.00097425
Iteration 24/25 | Loss: 0.00097425
Iteration 25/25 | Loss: 0.00097425

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 15.43531132
Iteration 2/25 | Loss: 0.00046979
Iteration 3/25 | Loss: 0.00046951
Iteration 4/25 | Loss: 0.00046950
Iteration 5/25 | Loss: 0.00046950
Iteration 6/25 | Loss: 0.00046950
Iteration 7/25 | Loss: 0.00046950
Iteration 8/25 | Loss: 0.00046950
Iteration 9/25 | Loss: 0.00046950
Iteration 10/25 | Loss: 0.00046950
Iteration 11/25 | Loss: 0.00046950
Iteration 12/25 | Loss: 0.00046950
Iteration 13/25 | Loss: 0.00046950
Iteration 14/25 | Loss: 0.00046950
Iteration 15/25 | Loss: 0.00046950
Iteration 16/25 | Loss: 0.00046950
Iteration 17/25 | Loss: 0.00046950
Iteration 18/25 | Loss: 0.00046950
Iteration 19/25 | Loss: 0.00046950
Iteration 20/25 | Loss: 0.00046950
Iteration 21/25 | Loss: 0.00046950
Iteration 22/25 | Loss: 0.00046950
Iteration 23/25 | Loss: 0.00046950
Iteration 24/25 | Loss: 0.00046950
Iteration 25/25 | Loss: 0.00046950

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046950
Iteration 2/1000 | Loss: 0.00001918
Iteration 3/1000 | Loss: 0.00001215
Iteration 4/1000 | Loss: 0.00001024
Iteration 5/1000 | Loss: 0.00000972
Iteration 6/1000 | Loss: 0.00000937
Iteration 7/1000 | Loss: 0.00000917
Iteration 8/1000 | Loss: 0.00000904
Iteration 9/1000 | Loss: 0.00000904
Iteration 10/1000 | Loss: 0.00000904
Iteration 11/1000 | Loss: 0.00000904
Iteration 12/1000 | Loss: 0.00000904
Iteration 13/1000 | Loss: 0.00000903
Iteration 14/1000 | Loss: 0.00000900
Iteration 15/1000 | Loss: 0.00000897
Iteration 16/1000 | Loss: 0.00000897
Iteration 17/1000 | Loss: 0.00000896
Iteration 18/1000 | Loss: 0.00000896
Iteration 19/1000 | Loss: 0.00000895
Iteration 20/1000 | Loss: 0.00000895
Iteration 21/1000 | Loss: 0.00000894
Iteration 22/1000 | Loss: 0.00000894
Iteration 23/1000 | Loss: 0.00000893
Iteration 24/1000 | Loss: 0.00000893
Iteration 25/1000 | Loss: 0.00000893
Iteration 26/1000 | Loss: 0.00000893
Iteration 27/1000 | Loss: 0.00000892
Iteration 28/1000 | Loss: 0.00000892
Iteration 29/1000 | Loss: 0.00000892
Iteration 30/1000 | Loss: 0.00000892
Iteration 31/1000 | Loss: 0.00000891
Iteration 32/1000 | Loss: 0.00000891
Iteration 33/1000 | Loss: 0.00000891
Iteration 34/1000 | Loss: 0.00000891
Iteration 35/1000 | Loss: 0.00000891
Iteration 36/1000 | Loss: 0.00000891
Iteration 37/1000 | Loss: 0.00000890
Iteration 38/1000 | Loss: 0.00000890
Iteration 39/1000 | Loss: 0.00000890
Iteration 40/1000 | Loss: 0.00000890
Iteration 41/1000 | Loss: 0.00000890
Iteration 42/1000 | Loss: 0.00000890
Iteration 43/1000 | Loss: 0.00000889
Iteration 44/1000 | Loss: 0.00000889
Iteration 45/1000 | Loss: 0.00000889
Iteration 46/1000 | Loss: 0.00000889
Iteration 47/1000 | Loss: 0.00000889
Iteration 48/1000 | Loss: 0.00000889
Iteration 49/1000 | Loss: 0.00000889
Iteration 50/1000 | Loss: 0.00000889
Iteration 51/1000 | Loss: 0.00000889
Iteration 52/1000 | Loss: 0.00000888
Iteration 53/1000 | Loss: 0.00000888
Iteration 54/1000 | Loss: 0.00000888
Iteration 55/1000 | Loss: 0.00000888
Iteration 56/1000 | Loss: 0.00000888
Iteration 57/1000 | Loss: 0.00000888
Iteration 58/1000 | Loss: 0.00000888
Iteration 59/1000 | Loss: 0.00000888
Iteration 60/1000 | Loss: 0.00000888
Iteration 61/1000 | Loss: 0.00000888
Iteration 62/1000 | Loss: 0.00000888
Iteration 63/1000 | Loss: 0.00000888
Iteration 64/1000 | Loss: 0.00000887
Iteration 65/1000 | Loss: 0.00000887
Iteration 66/1000 | Loss: 0.00000887
Iteration 67/1000 | Loss: 0.00000887
Iteration 68/1000 | Loss: 0.00000887
Iteration 69/1000 | Loss: 0.00000887
Iteration 70/1000 | Loss: 0.00000886
Iteration 71/1000 | Loss: 0.00000885
Iteration 72/1000 | Loss: 0.00000885
Iteration 73/1000 | Loss: 0.00000885
Iteration 74/1000 | Loss: 0.00000884
Iteration 75/1000 | Loss: 0.00000884
Iteration 76/1000 | Loss: 0.00000884
Iteration 77/1000 | Loss: 0.00000884
Iteration 78/1000 | Loss: 0.00000884
Iteration 79/1000 | Loss: 0.00000884
Iteration 80/1000 | Loss: 0.00000884
Iteration 81/1000 | Loss: 0.00000884
Iteration 82/1000 | Loss: 0.00000884
Iteration 83/1000 | Loss: 0.00000884
Iteration 84/1000 | Loss: 0.00000884
Iteration 85/1000 | Loss: 0.00000884
Iteration 86/1000 | Loss: 0.00000884
Iteration 87/1000 | Loss: 0.00000884
Iteration 88/1000 | Loss: 0.00000884
Iteration 89/1000 | Loss: 0.00000884
Iteration 90/1000 | Loss: 0.00000884
Iteration 91/1000 | Loss: 0.00000884
Iteration 92/1000 | Loss: 0.00000884
Iteration 93/1000 | Loss: 0.00000884
Iteration 94/1000 | Loss: 0.00000884
Iteration 95/1000 | Loss: 0.00000884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [8.844225703796837e-06, 8.844225703796837e-06, 8.844225703796837e-06, 8.844225703796837e-06, 8.844225703796837e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.844225703796837e-06

Optimization complete. Final v2v error: 2.5144381523132324 mm

Highest mean error: 2.8149521350860596 mm for frame 9

Lowest mean error: 2.3275716304779053 mm for frame 134

Saving results

Total time: 53.5751519203186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378233
Iteration 2/25 | Loss: 0.00080120
Iteration 3/25 | Loss: 0.00065578
Iteration 4/25 | Loss: 0.00064181
Iteration 5/25 | Loss: 0.00063299
Iteration 6/25 | Loss: 0.00063099
Iteration 7/25 | Loss: 0.00063086
Iteration 8/25 | Loss: 0.00063086
Iteration 9/25 | Loss: 0.00063086
Iteration 10/25 | Loss: 0.00063086
Iteration 11/25 | Loss: 0.00063086
Iteration 12/25 | Loss: 0.00063086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006308587617240846, 0.0006308587617240846, 0.0006308587617240846, 0.0006308587617240846, 0.0006308587617240846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006308587617240846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75230455
Iteration 2/25 | Loss: 0.00033056
Iteration 3/25 | Loss: 0.00033053
Iteration 4/25 | Loss: 0.00033052
Iteration 5/25 | Loss: 0.00033052
Iteration 6/25 | Loss: 0.00033052
Iteration 7/25 | Loss: 0.00033052
Iteration 8/25 | Loss: 0.00033052
Iteration 9/25 | Loss: 0.00033052
Iteration 10/25 | Loss: 0.00033052
Iteration 11/25 | Loss: 0.00033052
Iteration 12/25 | Loss: 0.00033052
Iteration 13/25 | Loss: 0.00033052
Iteration 14/25 | Loss: 0.00033052
Iteration 15/25 | Loss: 0.00033052
Iteration 16/25 | Loss: 0.00033052
Iteration 17/25 | Loss: 0.00033052
Iteration 18/25 | Loss: 0.00033052
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003305229765828699, 0.0003305229765828699, 0.0003305229765828699, 0.0003305229765828699, 0.0003305229765828699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003305229765828699

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033052
Iteration 2/1000 | Loss: 0.00002667
Iteration 3/1000 | Loss: 0.00001620
Iteration 4/1000 | Loss: 0.00001461
Iteration 5/1000 | Loss: 0.00001362
Iteration 6/1000 | Loss: 0.00001316
Iteration 7/1000 | Loss: 0.00001287
Iteration 8/1000 | Loss: 0.00001262
Iteration 9/1000 | Loss: 0.00001252
Iteration 10/1000 | Loss: 0.00001238
Iteration 11/1000 | Loss: 0.00001228
Iteration 12/1000 | Loss: 0.00001227
Iteration 13/1000 | Loss: 0.00001225
Iteration 14/1000 | Loss: 0.00001222
Iteration 15/1000 | Loss: 0.00001217
Iteration 16/1000 | Loss: 0.00001213
Iteration 17/1000 | Loss: 0.00001209
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001207
Iteration 20/1000 | Loss: 0.00001206
Iteration 21/1000 | Loss: 0.00001206
Iteration 22/1000 | Loss: 0.00001206
Iteration 23/1000 | Loss: 0.00001206
Iteration 24/1000 | Loss: 0.00001205
Iteration 25/1000 | Loss: 0.00001205
Iteration 26/1000 | Loss: 0.00001204
Iteration 27/1000 | Loss: 0.00001204
Iteration 28/1000 | Loss: 0.00001204
Iteration 29/1000 | Loss: 0.00001203
Iteration 30/1000 | Loss: 0.00001203
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001202
Iteration 33/1000 | Loss: 0.00001202
Iteration 34/1000 | Loss: 0.00001202
Iteration 35/1000 | Loss: 0.00001202
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001201
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001201
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001200
Iteration 46/1000 | Loss: 0.00001200
Iteration 47/1000 | Loss: 0.00001200
Iteration 48/1000 | Loss: 0.00001199
Iteration 49/1000 | Loss: 0.00001199
Iteration 50/1000 | Loss: 0.00001199
Iteration 51/1000 | Loss: 0.00001198
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001196
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001195
Iteration 65/1000 | Loss: 0.00001194
Iteration 66/1000 | Loss: 0.00001194
Iteration 67/1000 | Loss: 0.00001194
Iteration 68/1000 | Loss: 0.00001194
Iteration 69/1000 | Loss: 0.00001194
Iteration 70/1000 | Loss: 0.00001194
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001194
Iteration 76/1000 | Loss: 0.00001194
Iteration 77/1000 | Loss: 0.00001194
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001193
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001193
Iteration 86/1000 | Loss: 0.00001193
Iteration 87/1000 | Loss: 0.00001193
Iteration 88/1000 | Loss: 0.00001193
Iteration 89/1000 | Loss: 0.00001193
Iteration 90/1000 | Loss: 0.00001193
Iteration 91/1000 | Loss: 0.00001193
Iteration 92/1000 | Loss: 0.00001193
Iteration 93/1000 | Loss: 0.00001193
Iteration 94/1000 | Loss: 0.00001193
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001192
Iteration 97/1000 | Loss: 0.00001192
Iteration 98/1000 | Loss: 0.00001192
Iteration 99/1000 | Loss: 0.00001192
Iteration 100/1000 | Loss: 0.00001192
Iteration 101/1000 | Loss: 0.00001192
Iteration 102/1000 | Loss: 0.00001192
Iteration 103/1000 | Loss: 0.00001192
Iteration 104/1000 | Loss: 0.00001192
Iteration 105/1000 | Loss: 0.00001192
Iteration 106/1000 | Loss: 0.00001192
Iteration 107/1000 | Loss: 0.00001192
Iteration 108/1000 | Loss: 0.00001192
Iteration 109/1000 | Loss: 0.00001192
Iteration 110/1000 | Loss: 0.00001192
Iteration 111/1000 | Loss: 0.00001192
Iteration 112/1000 | Loss: 0.00001191
Iteration 113/1000 | Loss: 0.00001191
Iteration 114/1000 | Loss: 0.00001191
Iteration 115/1000 | Loss: 0.00001191
Iteration 116/1000 | Loss: 0.00001191
Iteration 117/1000 | Loss: 0.00001191
Iteration 118/1000 | Loss: 0.00001191
Iteration 119/1000 | Loss: 0.00001191
Iteration 120/1000 | Loss: 0.00001191
Iteration 121/1000 | Loss: 0.00001190
Iteration 122/1000 | Loss: 0.00001190
Iteration 123/1000 | Loss: 0.00001190
Iteration 124/1000 | Loss: 0.00001190
Iteration 125/1000 | Loss: 0.00001190
Iteration 126/1000 | Loss: 0.00001190
Iteration 127/1000 | Loss: 0.00001190
Iteration 128/1000 | Loss: 0.00001190
Iteration 129/1000 | Loss: 0.00001190
Iteration 130/1000 | Loss: 0.00001189
Iteration 131/1000 | Loss: 0.00001189
Iteration 132/1000 | Loss: 0.00001189
Iteration 133/1000 | Loss: 0.00001189
Iteration 134/1000 | Loss: 0.00001189
Iteration 135/1000 | Loss: 0.00001189
Iteration 136/1000 | Loss: 0.00001189
Iteration 137/1000 | Loss: 0.00001189
Iteration 138/1000 | Loss: 0.00001189
Iteration 139/1000 | Loss: 0.00001189
Iteration 140/1000 | Loss: 0.00001189
Iteration 141/1000 | Loss: 0.00001189
Iteration 142/1000 | Loss: 0.00001189
Iteration 143/1000 | Loss: 0.00001189
Iteration 144/1000 | Loss: 0.00001189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.1887851542269345e-05, 1.1887851542269345e-05, 1.1887851542269345e-05, 1.1887851542269345e-05, 1.1887851542269345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1887851542269345e-05

Optimization complete. Final v2v error: 2.9404633045196533 mm

Highest mean error: 3.184410333633423 mm for frame 100

Lowest mean error: 2.734043598175049 mm for frame 121

Saving results

Total time: 39.21222925186157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832532
Iteration 2/25 | Loss: 0.00113014
Iteration 3/25 | Loss: 0.00082869
Iteration 4/25 | Loss: 0.00076353
Iteration 5/25 | Loss: 0.00074410
Iteration 6/25 | Loss: 0.00073964
Iteration 7/25 | Loss: 0.00073843
Iteration 8/25 | Loss: 0.00073843
Iteration 9/25 | Loss: 0.00073843
Iteration 10/25 | Loss: 0.00073843
Iteration 11/25 | Loss: 0.00073843
Iteration 12/25 | Loss: 0.00073843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007384270429611206, 0.0007384270429611206, 0.0007384270429611206, 0.0007384270429611206, 0.0007384270429611206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007384270429611206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22783780
Iteration 2/25 | Loss: 0.00025332
Iteration 3/25 | Loss: 0.00025332
Iteration 4/25 | Loss: 0.00025332
Iteration 5/25 | Loss: 0.00025332
Iteration 6/25 | Loss: 0.00025332
Iteration 7/25 | Loss: 0.00025332
Iteration 8/25 | Loss: 0.00025332
Iteration 9/25 | Loss: 0.00025332
Iteration 10/25 | Loss: 0.00025332
Iteration 11/25 | Loss: 0.00025332
Iteration 12/25 | Loss: 0.00025332
Iteration 13/25 | Loss: 0.00025332
Iteration 14/25 | Loss: 0.00025332
Iteration 15/25 | Loss: 0.00025332
Iteration 16/25 | Loss: 0.00025332
Iteration 17/25 | Loss: 0.00025332
Iteration 18/25 | Loss: 0.00025332
Iteration 19/25 | Loss: 0.00025332
Iteration 20/25 | Loss: 0.00025332
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00025331942015327513, 0.00025331942015327513, 0.00025331942015327513, 0.00025331942015327513, 0.00025331942015327513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025331942015327513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025332
Iteration 2/1000 | Loss: 0.00005210
Iteration 3/1000 | Loss: 0.00003613
Iteration 4/1000 | Loss: 0.00003212
Iteration 5/1000 | Loss: 0.00003050
Iteration 6/1000 | Loss: 0.00002932
Iteration 7/1000 | Loss: 0.00002858
Iteration 8/1000 | Loss: 0.00002792
Iteration 9/1000 | Loss: 0.00002741
Iteration 10/1000 | Loss: 0.00002699
Iteration 11/1000 | Loss: 0.00002662
Iteration 12/1000 | Loss: 0.00002636
Iteration 13/1000 | Loss: 0.00002616
Iteration 14/1000 | Loss: 0.00002610
Iteration 15/1000 | Loss: 0.00002608
Iteration 16/1000 | Loss: 0.00002607
Iteration 17/1000 | Loss: 0.00002606
Iteration 18/1000 | Loss: 0.00002602
Iteration 19/1000 | Loss: 0.00002594
Iteration 20/1000 | Loss: 0.00002590
Iteration 21/1000 | Loss: 0.00002589
Iteration 22/1000 | Loss: 0.00002586
Iteration 23/1000 | Loss: 0.00002585
Iteration 24/1000 | Loss: 0.00002585
Iteration 25/1000 | Loss: 0.00002584
Iteration 26/1000 | Loss: 0.00002584
Iteration 27/1000 | Loss: 0.00002583
Iteration 28/1000 | Loss: 0.00002582
Iteration 29/1000 | Loss: 0.00002582
Iteration 30/1000 | Loss: 0.00002582
Iteration 31/1000 | Loss: 0.00002582
Iteration 32/1000 | Loss: 0.00002582
Iteration 33/1000 | Loss: 0.00002582
Iteration 34/1000 | Loss: 0.00002582
Iteration 35/1000 | Loss: 0.00002582
Iteration 36/1000 | Loss: 0.00002582
Iteration 37/1000 | Loss: 0.00002582
Iteration 38/1000 | Loss: 0.00002582
Iteration 39/1000 | Loss: 0.00002582
Iteration 40/1000 | Loss: 0.00002581
Iteration 41/1000 | Loss: 0.00002581
Iteration 42/1000 | Loss: 0.00002581
Iteration 43/1000 | Loss: 0.00002581
Iteration 44/1000 | Loss: 0.00002581
Iteration 45/1000 | Loss: 0.00002581
Iteration 46/1000 | Loss: 0.00002581
Iteration 47/1000 | Loss: 0.00002581
Iteration 48/1000 | Loss: 0.00002580
Iteration 49/1000 | Loss: 0.00002580
Iteration 50/1000 | Loss: 0.00002579
Iteration 51/1000 | Loss: 0.00002579
Iteration 52/1000 | Loss: 0.00002579
Iteration 53/1000 | Loss: 0.00002579
Iteration 54/1000 | Loss: 0.00002579
Iteration 55/1000 | Loss: 0.00002579
Iteration 56/1000 | Loss: 0.00002578
Iteration 57/1000 | Loss: 0.00002578
Iteration 58/1000 | Loss: 0.00002578
Iteration 59/1000 | Loss: 0.00002577
Iteration 60/1000 | Loss: 0.00002577
Iteration 61/1000 | Loss: 0.00002577
Iteration 62/1000 | Loss: 0.00002577
Iteration 63/1000 | Loss: 0.00002577
Iteration 64/1000 | Loss: 0.00002576
Iteration 65/1000 | Loss: 0.00002576
Iteration 66/1000 | Loss: 0.00002576
Iteration 67/1000 | Loss: 0.00002575
Iteration 68/1000 | Loss: 0.00002575
Iteration 69/1000 | Loss: 0.00002575
Iteration 70/1000 | Loss: 0.00002575
Iteration 71/1000 | Loss: 0.00002574
Iteration 72/1000 | Loss: 0.00002574
Iteration 73/1000 | Loss: 0.00002574
Iteration 74/1000 | Loss: 0.00002574
Iteration 75/1000 | Loss: 0.00002574
Iteration 76/1000 | Loss: 0.00002573
Iteration 77/1000 | Loss: 0.00002573
Iteration 78/1000 | Loss: 0.00002573
Iteration 79/1000 | Loss: 0.00002573
Iteration 80/1000 | Loss: 0.00002572
Iteration 81/1000 | Loss: 0.00002572
Iteration 82/1000 | Loss: 0.00002572
Iteration 83/1000 | Loss: 0.00002572
Iteration 84/1000 | Loss: 0.00002572
Iteration 85/1000 | Loss: 0.00002572
Iteration 86/1000 | Loss: 0.00002571
Iteration 87/1000 | Loss: 0.00002571
Iteration 88/1000 | Loss: 0.00002571
Iteration 89/1000 | Loss: 0.00002571
Iteration 90/1000 | Loss: 0.00002571
Iteration 91/1000 | Loss: 0.00002571
Iteration 92/1000 | Loss: 0.00002571
Iteration 93/1000 | Loss: 0.00002571
Iteration 94/1000 | Loss: 0.00002571
Iteration 95/1000 | Loss: 0.00002571
Iteration 96/1000 | Loss: 0.00002571
Iteration 97/1000 | Loss: 0.00002571
Iteration 98/1000 | Loss: 0.00002571
Iteration 99/1000 | Loss: 0.00002571
Iteration 100/1000 | Loss: 0.00002571
Iteration 101/1000 | Loss: 0.00002571
Iteration 102/1000 | Loss: 0.00002571
Iteration 103/1000 | Loss: 0.00002571
Iteration 104/1000 | Loss: 0.00002571
Iteration 105/1000 | Loss: 0.00002571
Iteration 106/1000 | Loss: 0.00002571
Iteration 107/1000 | Loss: 0.00002571
Iteration 108/1000 | Loss: 0.00002571
Iteration 109/1000 | Loss: 0.00002571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.5706533051561564e-05, 2.5706533051561564e-05, 2.5706533051561564e-05, 2.5706533051561564e-05, 2.5706533051561564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5706533051561564e-05

Optimization complete. Final v2v error: 4.176029205322266 mm

Highest mean error: 5.392011642456055 mm for frame 11

Lowest mean error: 3.2532296180725098 mm for frame 214

Saving results

Total time: 41.89506649971008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827469
Iteration 2/25 | Loss: 0.00073183
Iteration 3/25 | Loss: 0.00060036
Iteration 4/25 | Loss: 0.00058419
Iteration 5/25 | Loss: 0.00057896
Iteration 6/25 | Loss: 0.00057765
Iteration 7/25 | Loss: 0.00057765
Iteration 8/25 | Loss: 0.00057765
Iteration 9/25 | Loss: 0.00057765
Iteration 10/25 | Loss: 0.00057765
Iteration 11/25 | Loss: 0.00057765
Iteration 12/25 | Loss: 0.00057765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005776543403044343, 0.0005776543403044343, 0.0005776543403044343, 0.0005776543403044343, 0.0005776543403044343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005776543403044343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46471214
Iteration 2/25 | Loss: 0.00026151
Iteration 3/25 | Loss: 0.00026151
Iteration 4/25 | Loss: 0.00026151
Iteration 5/25 | Loss: 0.00026151
Iteration 6/25 | Loss: 0.00026151
Iteration 7/25 | Loss: 0.00026151
Iteration 8/25 | Loss: 0.00026151
Iteration 9/25 | Loss: 0.00026151
Iteration 10/25 | Loss: 0.00026151
Iteration 11/25 | Loss: 0.00026151
Iteration 12/25 | Loss: 0.00026151
Iteration 13/25 | Loss: 0.00026151
Iteration 14/25 | Loss: 0.00026151
Iteration 15/25 | Loss: 0.00026151
Iteration 16/25 | Loss: 0.00026151
Iteration 17/25 | Loss: 0.00026151
Iteration 18/25 | Loss: 0.00026151
Iteration 19/25 | Loss: 0.00026151
Iteration 20/25 | Loss: 0.00026151
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002615051926113665, 0.0002615051926113665, 0.0002615051926113665, 0.0002615051926113665, 0.0002615051926113665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002615051926113665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026151
Iteration 2/1000 | Loss: 0.00001638
Iteration 3/1000 | Loss: 0.00001180
Iteration 4/1000 | Loss: 0.00001078
Iteration 5/1000 | Loss: 0.00001016
Iteration 6/1000 | Loss: 0.00000984
Iteration 7/1000 | Loss: 0.00000972
Iteration 8/1000 | Loss: 0.00000972
Iteration 9/1000 | Loss: 0.00000965
Iteration 10/1000 | Loss: 0.00000964
Iteration 11/1000 | Loss: 0.00000955
Iteration 12/1000 | Loss: 0.00000954
Iteration 13/1000 | Loss: 0.00000954
Iteration 14/1000 | Loss: 0.00000954
Iteration 15/1000 | Loss: 0.00000952
Iteration 16/1000 | Loss: 0.00000948
Iteration 17/1000 | Loss: 0.00000947
Iteration 18/1000 | Loss: 0.00000947
Iteration 19/1000 | Loss: 0.00000947
Iteration 20/1000 | Loss: 0.00000946
Iteration 21/1000 | Loss: 0.00000941
Iteration 22/1000 | Loss: 0.00000940
Iteration 23/1000 | Loss: 0.00000940
Iteration 24/1000 | Loss: 0.00000940
Iteration 25/1000 | Loss: 0.00000940
Iteration 26/1000 | Loss: 0.00000938
Iteration 27/1000 | Loss: 0.00000937
Iteration 28/1000 | Loss: 0.00000936
Iteration 29/1000 | Loss: 0.00000936
Iteration 30/1000 | Loss: 0.00000936
Iteration 31/1000 | Loss: 0.00000936
Iteration 32/1000 | Loss: 0.00000936
Iteration 33/1000 | Loss: 0.00000935
Iteration 34/1000 | Loss: 0.00000935
Iteration 35/1000 | Loss: 0.00000934
Iteration 36/1000 | Loss: 0.00000934
Iteration 37/1000 | Loss: 0.00000933
Iteration 38/1000 | Loss: 0.00000932
Iteration 39/1000 | Loss: 0.00000932
Iteration 40/1000 | Loss: 0.00000932
Iteration 41/1000 | Loss: 0.00000932
Iteration 42/1000 | Loss: 0.00000932
Iteration 43/1000 | Loss: 0.00000932
Iteration 44/1000 | Loss: 0.00000932
Iteration 45/1000 | Loss: 0.00000932
Iteration 46/1000 | Loss: 0.00000932
Iteration 47/1000 | Loss: 0.00000932
Iteration 48/1000 | Loss: 0.00000931
Iteration 49/1000 | Loss: 0.00000931
Iteration 50/1000 | Loss: 0.00000929
Iteration 51/1000 | Loss: 0.00000928
Iteration 52/1000 | Loss: 0.00000928
Iteration 53/1000 | Loss: 0.00000928
Iteration 54/1000 | Loss: 0.00000928
Iteration 55/1000 | Loss: 0.00000928
Iteration 56/1000 | Loss: 0.00000928
Iteration 57/1000 | Loss: 0.00000928
Iteration 58/1000 | Loss: 0.00000928
Iteration 59/1000 | Loss: 0.00000928
Iteration 60/1000 | Loss: 0.00000927
Iteration 61/1000 | Loss: 0.00000927
Iteration 62/1000 | Loss: 0.00000927
Iteration 63/1000 | Loss: 0.00000927
Iteration 64/1000 | Loss: 0.00000927
Iteration 65/1000 | Loss: 0.00000926
Iteration 66/1000 | Loss: 0.00000926
Iteration 67/1000 | Loss: 0.00000925
Iteration 68/1000 | Loss: 0.00000925
Iteration 69/1000 | Loss: 0.00000925
Iteration 70/1000 | Loss: 0.00000925
Iteration 71/1000 | Loss: 0.00000925
Iteration 72/1000 | Loss: 0.00000925
Iteration 73/1000 | Loss: 0.00000925
Iteration 74/1000 | Loss: 0.00000925
Iteration 75/1000 | Loss: 0.00000925
Iteration 76/1000 | Loss: 0.00000925
Iteration 77/1000 | Loss: 0.00000925
Iteration 78/1000 | Loss: 0.00000925
Iteration 79/1000 | Loss: 0.00000924
Iteration 80/1000 | Loss: 0.00000924
Iteration 81/1000 | Loss: 0.00000924
Iteration 82/1000 | Loss: 0.00000924
Iteration 83/1000 | Loss: 0.00000924
Iteration 84/1000 | Loss: 0.00000924
Iteration 85/1000 | Loss: 0.00000924
Iteration 86/1000 | Loss: 0.00000924
Iteration 87/1000 | Loss: 0.00000924
Iteration 88/1000 | Loss: 0.00000924
Iteration 89/1000 | Loss: 0.00000924
Iteration 90/1000 | Loss: 0.00000923
Iteration 91/1000 | Loss: 0.00000923
Iteration 92/1000 | Loss: 0.00000923
Iteration 93/1000 | Loss: 0.00000923
Iteration 94/1000 | Loss: 0.00000923
Iteration 95/1000 | Loss: 0.00000923
Iteration 96/1000 | Loss: 0.00000923
Iteration 97/1000 | Loss: 0.00000923
Iteration 98/1000 | Loss: 0.00000923
Iteration 99/1000 | Loss: 0.00000923
Iteration 100/1000 | Loss: 0.00000923
Iteration 101/1000 | Loss: 0.00000923
Iteration 102/1000 | Loss: 0.00000923
Iteration 103/1000 | Loss: 0.00000923
Iteration 104/1000 | Loss: 0.00000923
Iteration 105/1000 | Loss: 0.00000923
Iteration 106/1000 | Loss: 0.00000923
Iteration 107/1000 | Loss: 0.00000923
Iteration 108/1000 | Loss: 0.00000923
Iteration 109/1000 | Loss: 0.00000923
Iteration 110/1000 | Loss: 0.00000923
Iteration 111/1000 | Loss: 0.00000923
Iteration 112/1000 | Loss: 0.00000923
Iteration 113/1000 | Loss: 0.00000923
Iteration 114/1000 | Loss: 0.00000923
Iteration 115/1000 | Loss: 0.00000923
Iteration 116/1000 | Loss: 0.00000923
Iteration 117/1000 | Loss: 0.00000923
Iteration 118/1000 | Loss: 0.00000923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [9.225997018802445e-06, 9.225997018802445e-06, 9.225997018802445e-06, 9.225997018802445e-06, 9.225997018802445e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.225997018802445e-06

Optimization complete. Final v2v error: 2.577714443206787 mm

Highest mean error: 2.7466471195220947 mm for frame 105

Lowest mean error: 2.4014806747436523 mm for frame 158

Saving results

Total time: 30.458080530166626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064411
Iteration 2/25 | Loss: 0.00142160
Iteration 3/25 | Loss: 0.00092391
Iteration 4/25 | Loss: 0.00086616
Iteration 5/25 | Loss: 0.00084914
Iteration 6/25 | Loss: 0.00084638
Iteration 7/25 | Loss: 0.00084542
Iteration 8/25 | Loss: 0.00084542
Iteration 9/25 | Loss: 0.00084542
Iteration 10/25 | Loss: 0.00084542
Iteration 11/25 | Loss: 0.00084542
Iteration 12/25 | Loss: 0.00084542
Iteration 13/25 | Loss: 0.00084542
Iteration 14/25 | Loss: 0.00084542
Iteration 15/25 | Loss: 0.00084542
Iteration 16/25 | Loss: 0.00084542
Iteration 17/25 | Loss: 0.00084542
Iteration 18/25 | Loss: 0.00084542
Iteration 19/25 | Loss: 0.00084542
Iteration 20/25 | Loss: 0.00084542
Iteration 21/25 | Loss: 0.00084542
Iteration 22/25 | Loss: 0.00084542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008454154012724757, 0.0008454154012724757, 0.0008454154012724757, 0.0008454154012724757, 0.0008454154012724757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008454154012724757

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02953267
Iteration 2/25 | Loss: 0.00024406
Iteration 3/25 | Loss: 0.00024403
Iteration 4/25 | Loss: 0.00024403
Iteration 5/25 | Loss: 0.00024403
Iteration 6/25 | Loss: 0.00024403
Iteration 7/25 | Loss: 0.00024403
Iteration 8/25 | Loss: 0.00024403
Iteration 9/25 | Loss: 0.00024403
Iteration 10/25 | Loss: 0.00024403
Iteration 11/25 | Loss: 0.00024403
Iteration 12/25 | Loss: 0.00024403
Iteration 13/25 | Loss: 0.00024403
Iteration 14/25 | Loss: 0.00024403
Iteration 15/25 | Loss: 0.00024403
Iteration 16/25 | Loss: 0.00024403
Iteration 17/25 | Loss: 0.00024403
Iteration 18/25 | Loss: 0.00024403
Iteration 19/25 | Loss: 0.00024403
Iteration 20/25 | Loss: 0.00024403
Iteration 21/25 | Loss: 0.00024403
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00024402837152592838, 0.00024402837152592838, 0.00024402837152592838, 0.00024402837152592838, 0.00024402837152592838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024402837152592838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024403
Iteration 2/1000 | Loss: 0.00005701
Iteration 3/1000 | Loss: 0.00004163
Iteration 4/1000 | Loss: 0.00003663
Iteration 5/1000 | Loss: 0.00003504
Iteration 6/1000 | Loss: 0.00003418
Iteration 7/1000 | Loss: 0.00003371
Iteration 8/1000 | Loss: 0.00003320
Iteration 9/1000 | Loss: 0.00003279
Iteration 10/1000 | Loss: 0.00003251
Iteration 11/1000 | Loss: 0.00003234
Iteration 12/1000 | Loss: 0.00003216
Iteration 13/1000 | Loss: 0.00003203
Iteration 14/1000 | Loss: 0.00003202
Iteration 15/1000 | Loss: 0.00003192
Iteration 16/1000 | Loss: 0.00003191
Iteration 17/1000 | Loss: 0.00003187
Iteration 18/1000 | Loss: 0.00003187
Iteration 19/1000 | Loss: 0.00003186
Iteration 20/1000 | Loss: 0.00003184
Iteration 21/1000 | Loss: 0.00003184
Iteration 22/1000 | Loss: 0.00003184
Iteration 23/1000 | Loss: 0.00003184
Iteration 24/1000 | Loss: 0.00003183
Iteration 25/1000 | Loss: 0.00003183
Iteration 26/1000 | Loss: 0.00003182
Iteration 27/1000 | Loss: 0.00003181
Iteration 28/1000 | Loss: 0.00003181
Iteration 29/1000 | Loss: 0.00003181
Iteration 30/1000 | Loss: 0.00003180
Iteration 31/1000 | Loss: 0.00003180
Iteration 32/1000 | Loss: 0.00003180
Iteration 33/1000 | Loss: 0.00003180
Iteration 34/1000 | Loss: 0.00003179
Iteration 35/1000 | Loss: 0.00003179
Iteration 36/1000 | Loss: 0.00003178
Iteration 37/1000 | Loss: 0.00003178
Iteration 38/1000 | Loss: 0.00003178
Iteration 39/1000 | Loss: 0.00003177
Iteration 40/1000 | Loss: 0.00003177
Iteration 41/1000 | Loss: 0.00003176
Iteration 42/1000 | Loss: 0.00003175
Iteration 43/1000 | Loss: 0.00003175
Iteration 44/1000 | Loss: 0.00003174
Iteration 45/1000 | Loss: 0.00003174
Iteration 46/1000 | Loss: 0.00003173
Iteration 47/1000 | Loss: 0.00003173
Iteration 48/1000 | Loss: 0.00003173
Iteration 49/1000 | Loss: 0.00003173
Iteration 50/1000 | Loss: 0.00003173
Iteration 51/1000 | Loss: 0.00003173
Iteration 52/1000 | Loss: 0.00003173
Iteration 53/1000 | Loss: 0.00003173
Iteration 54/1000 | Loss: 0.00003173
Iteration 55/1000 | Loss: 0.00003172
Iteration 56/1000 | Loss: 0.00003172
Iteration 57/1000 | Loss: 0.00003172
Iteration 58/1000 | Loss: 0.00003172
Iteration 59/1000 | Loss: 0.00003172
Iteration 60/1000 | Loss: 0.00003172
Iteration 61/1000 | Loss: 0.00003172
Iteration 62/1000 | Loss: 0.00003169
Iteration 63/1000 | Loss: 0.00003169
Iteration 64/1000 | Loss: 0.00003169
Iteration 65/1000 | Loss: 0.00003168
Iteration 66/1000 | Loss: 0.00003167
Iteration 67/1000 | Loss: 0.00003167
Iteration 68/1000 | Loss: 0.00003167
Iteration 69/1000 | Loss: 0.00003166
Iteration 70/1000 | Loss: 0.00003166
Iteration 71/1000 | Loss: 0.00003165
Iteration 72/1000 | Loss: 0.00003165
Iteration 73/1000 | Loss: 0.00003165
Iteration 74/1000 | Loss: 0.00003165
Iteration 75/1000 | Loss: 0.00003165
Iteration 76/1000 | Loss: 0.00003165
Iteration 77/1000 | Loss: 0.00003165
Iteration 78/1000 | Loss: 0.00003165
Iteration 79/1000 | Loss: 0.00003165
Iteration 80/1000 | Loss: 0.00003164
Iteration 81/1000 | Loss: 0.00003164
Iteration 82/1000 | Loss: 0.00003164
Iteration 83/1000 | Loss: 0.00003164
Iteration 84/1000 | Loss: 0.00003164
Iteration 85/1000 | Loss: 0.00003164
Iteration 86/1000 | Loss: 0.00003164
Iteration 87/1000 | Loss: 0.00003164
Iteration 88/1000 | Loss: 0.00003163
Iteration 89/1000 | Loss: 0.00003163
Iteration 90/1000 | Loss: 0.00003163
Iteration 91/1000 | Loss: 0.00003163
Iteration 92/1000 | Loss: 0.00003163
Iteration 93/1000 | Loss: 0.00003163
Iteration 94/1000 | Loss: 0.00003162
Iteration 95/1000 | Loss: 0.00003162
Iteration 96/1000 | Loss: 0.00003162
Iteration 97/1000 | Loss: 0.00003162
Iteration 98/1000 | Loss: 0.00003162
Iteration 99/1000 | Loss: 0.00003162
Iteration 100/1000 | Loss: 0.00003162
Iteration 101/1000 | Loss: 0.00003162
Iteration 102/1000 | Loss: 0.00003162
Iteration 103/1000 | Loss: 0.00003162
Iteration 104/1000 | Loss: 0.00003162
Iteration 105/1000 | Loss: 0.00003162
Iteration 106/1000 | Loss: 0.00003162
Iteration 107/1000 | Loss: 0.00003161
Iteration 108/1000 | Loss: 0.00003161
Iteration 109/1000 | Loss: 0.00003161
Iteration 110/1000 | Loss: 0.00003161
Iteration 111/1000 | Loss: 0.00003161
Iteration 112/1000 | Loss: 0.00003161
Iteration 113/1000 | Loss: 0.00003161
Iteration 114/1000 | Loss: 0.00003160
Iteration 115/1000 | Loss: 0.00003160
Iteration 116/1000 | Loss: 0.00003160
Iteration 117/1000 | Loss: 0.00003160
Iteration 118/1000 | Loss: 0.00003160
Iteration 119/1000 | Loss: 0.00003160
Iteration 120/1000 | Loss: 0.00003160
Iteration 121/1000 | Loss: 0.00003160
Iteration 122/1000 | Loss: 0.00003160
Iteration 123/1000 | Loss: 0.00003160
Iteration 124/1000 | Loss: 0.00003160
Iteration 125/1000 | Loss: 0.00003160
Iteration 126/1000 | Loss: 0.00003159
Iteration 127/1000 | Loss: 0.00003159
Iteration 128/1000 | Loss: 0.00003158
Iteration 129/1000 | Loss: 0.00003158
Iteration 130/1000 | Loss: 0.00003158
Iteration 131/1000 | Loss: 0.00003158
Iteration 132/1000 | Loss: 0.00003158
Iteration 133/1000 | Loss: 0.00003158
Iteration 134/1000 | Loss: 0.00003157
Iteration 135/1000 | Loss: 0.00003157
Iteration 136/1000 | Loss: 0.00003157
Iteration 137/1000 | Loss: 0.00003157
Iteration 138/1000 | Loss: 0.00003156
Iteration 139/1000 | Loss: 0.00003156
Iteration 140/1000 | Loss: 0.00003156
Iteration 141/1000 | Loss: 0.00003156
Iteration 142/1000 | Loss: 0.00003156
Iteration 143/1000 | Loss: 0.00003155
Iteration 144/1000 | Loss: 0.00003155
Iteration 145/1000 | Loss: 0.00003155
Iteration 146/1000 | Loss: 0.00003154
Iteration 147/1000 | Loss: 0.00003154
Iteration 148/1000 | Loss: 0.00003154
Iteration 149/1000 | Loss: 0.00003154
Iteration 150/1000 | Loss: 0.00003154
Iteration 151/1000 | Loss: 0.00003153
Iteration 152/1000 | Loss: 0.00003153
Iteration 153/1000 | Loss: 0.00003153
Iteration 154/1000 | Loss: 0.00003153
Iteration 155/1000 | Loss: 0.00003153
Iteration 156/1000 | Loss: 0.00003153
Iteration 157/1000 | Loss: 0.00003153
Iteration 158/1000 | Loss: 0.00003152
Iteration 159/1000 | Loss: 0.00003152
Iteration 160/1000 | Loss: 0.00003152
Iteration 161/1000 | Loss: 0.00003152
Iteration 162/1000 | Loss: 0.00003152
Iteration 163/1000 | Loss: 0.00003152
Iteration 164/1000 | Loss: 0.00003152
Iteration 165/1000 | Loss: 0.00003152
Iteration 166/1000 | Loss: 0.00003152
Iteration 167/1000 | Loss: 0.00003152
Iteration 168/1000 | Loss: 0.00003151
Iteration 169/1000 | Loss: 0.00003151
Iteration 170/1000 | Loss: 0.00003151
Iteration 171/1000 | Loss: 0.00003151
Iteration 172/1000 | Loss: 0.00003151
Iteration 173/1000 | Loss: 0.00003151
Iteration 174/1000 | Loss: 0.00003151
Iteration 175/1000 | Loss: 0.00003151
Iteration 176/1000 | Loss: 0.00003150
Iteration 177/1000 | Loss: 0.00003150
Iteration 178/1000 | Loss: 0.00003150
Iteration 179/1000 | Loss: 0.00003150
Iteration 180/1000 | Loss: 0.00003149
Iteration 181/1000 | Loss: 0.00003149
Iteration 182/1000 | Loss: 0.00003149
Iteration 183/1000 | Loss: 0.00003149
Iteration 184/1000 | Loss: 0.00003149
Iteration 185/1000 | Loss: 0.00003149
Iteration 186/1000 | Loss: 0.00003148
Iteration 187/1000 | Loss: 0.00003148
Iteration 188/1000 | Loss: 0.00003148
Iteration 189/1000 | Loss: 0.00003148
Iteration 190/1000 | Loss: 0.00003148
Iteration 191/1000 | Loss: 0.00003147
Iteration 192/1000 | Loss: 0.00003147
Iteration 193/1000 | Loss: 0.00003147
Iteration 194/1000 | Loss: 0.00003147
Iteration 195/1000 | Loss: 0.00003147
Iteration 196/1000 | Loss: 0.00003147
Iteration 197/1000 | Loss: 0.00003147
Iteration 198/1000 | Loss: 0.00003147
Iteration 199/1000 | Loss: 0.00003147
Iteration 200/1000 | Loss: 0.00003147
Iteration 201/1000 | Loss: 0.00003146
Iteration 202/1000 | Loss: 0.00003146
Iteration 203/1000 | Loss: 0.00003146
Iteration 204/1000 | Loss: 0.00003146
Iteration 205/1000 | Loss: 0.00003146
Iteration 206/1000 | Loss: 0.00003146
Iteration 207/1000 | Loss: 0.00003146
Iteration 208/1000 | Loss: 0.00003146
Iteration 209/1000 | Loss: 0.00003146
Iteration 210/1000 | Loss: 0.00003146
Iteration 211/1000 | Loss: 0.00003146
Iteration 212/1000 | Loss: 0.00003145
Iteration 213/1000 | Loss: 0.00003145
Iteration 214/1000 | Loss: 0.00003145
Iteration 215/1000 | Loss: 0.00003145
Iteration 216/1000 | Loss: 0.00003145
Iteration 217/1000 | Loss: 0.00003145
Iteration 218/1000 | Loss: 0.00003145
Iteration 219/1000 | Loss: 0.00003145
Iteration 220/1000 | Loss: 0.00003145
Iteration 221/1000 | Loss: 0.00003145
Iteration 222/1000 | Loss: 0.00003145
Iteration 223/1000 | Loss: 0.00003145
Iteration 224/1000 | Loss: 0.00003145
Iteration 225/1000 | Loss: 0.00003145
Iteration 226/1000 | Loss: 0.00003145
Iteration 227/1000 | Loss: 0.00003144
Iteration 228/1000 | Loss: 0.00003144
Iteration 229/1000 | Loss: 0.00003144
Iteration 230/1000 | Loss: 0.00003144
Iteration 231/1000 | Loss: 0.00003144
Iteration 232/1000 | Loss: 0.00003144
Iteration 233/1000 | Loss: 0.00003144
Iteration 234/1000 | Loss: 0.00003144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [3.144286893075332e-05, 3.144286893075332e-05, 3.144286893075332e-05, 3.144286893075332e-05, 3.144286893075332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.144286893075332e-05

Optimization complete. Final v2v error: 4.453570365905762 mm

Highest mean error: 5.167651653289795 mm for frame 74

Lowest mean error: 3.8925678730010986 mm for frame 35

Saving results

Total time: 45.672799587249756
