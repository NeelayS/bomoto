Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=33, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1848-1903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767971
Iteration 2/25 | Loss: 0.00141469
Iteration 3/25 | Loss: 0.00124816
Iteration 4/25 | Loss: 0.00122912
Iteration 5/25 | Loss: 0.00122401
Iteration 6/25 | Loss: 0.00122295
Iteration 7/25 | Loss: 0.00122295
Iteration 8/25 | Loss: 0.00122295
Iteration 9/25 | Loss: 0.00122295
Iteration 10/25 | Loss: 0.00122295
Iteration 11/25 | Loss: 0.00122295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012229521526023746, 0.0012229521526023746, 0.0012229521526023746, 0.0012229521526023746, 0.0012229521526023746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012229521526023746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26406741
Iteration 2/25 | Loss: 0.00085826
Iteration 3/25 | Loss: 0.00085823
Iteration 4/25 | Loss: 0.00085823
Iteration 5/25 | Loss: 0.00085823
Iteration 6/25 | Loss: 0.00085823
Iteration 7/25 | Loss: 0.00085823
Iteration 8/25 | Loss: 0.00085823
Iteration 9/25 | Loss: 0.00085823
Iteration 10/25 | Loss: 0.00085823
Iteration 11/25 | Loss: 0.00085823
Iteration 12/25 | Loss: 0.00085823
Iteration 13/25 | Loss: 0.00085823
Iteration 14/25 | Loss: 0.00085823
Iteration 15/25 | Loss: 0.00085823
Iteration 16/25 | Loss: 0.00085823
Iteration 17/25 | Loss: 0.00085823
Iteration 18/25 | Loss: 0.00085823
Iteration 19/25 | Loss: 0.00085823
Iteration 20/25 | Loss: 0.00085823
Iteration 21/25 | Loss: 0.00085823
Iteration 22/25 | Loss: 0.00085823
Iteration 23/25 | Loss: 0.00085823
Iteration 24/25 | Loss: 0.00085823
Iteration 25/25 | Loss: 0.00085823
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008582310983911157, 0.0008582310983911157, 0.0008582310983911157, 0.0008582310983911157, 0.0008582310983911157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008582310983911157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085823
Iteration 2/1000 | Loss: 0.00003710
Iteration 3/1000 | Loss: 0.00002795
Iteration 4/1000 | Loss: 0.00002263
Iteration 5/1000 | Loss: 0.00002098
Iteration 6/1000 | Loss: 0.00001980
Iteration 7/1000 | Loss: 0.00001907
Iteration 8/1000 | Loss: 0.00001857
Iteration 9/1000 | Loss: 0.00001811
Iteration 10/1000 | Loss: 0.00001784
Iteration 11/1000 | Loss: 0.00001757
Iteration 12/1000 | Loss: 0.00001740
Iteration 13/1000 | Loss: 0.00001735
Iteration 14/1000 | Loss: 0.00001729
Iteration 15/1000 | Loss: 0.00001726
Iteration 16/1000 | Loss: 0.00001723
Iteration 17/1000 | Loss: 0.00001715
Iteration 18/1000 | Loss: 0.00001710
Iteration 19/1000 | Loss: 0.00001709
Iteration 20/1000 | Loss: 0.00001706
Iteration 21/1000 | Loss: 0.00001705
Iteration 22/1000 | Loss: 0.00001697
Iteration 23/1000 | Loss: 0.00001693
Iteration 24/1000 | Loss: 0.00001690
Iteration 25/1000 | Loss: 0.00001689
Iteration 26/1000 | Loss: 0.00001689
Iteration 27/1000 | Loss: 0.00001688
Iteration 28/1000 | Loss: 0.00001682
Iteration 29/1000 | Loss: 0.00001682
Iteration 30/1000 | Loss: 0.00001681
Iteration 31/1000 | Loss: 0.00001681
Iteration 32/1000 | Loss: 0.00001680
Iteration 33/1000 | Loss: 0.00001680
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00001679
Iteration 36/1000 | Loss: 0.00001678
Iteration 37/1000 | Loss: 0.00001675
Iteration 38/1000 | Loss: 0.00001675
Iteration 39/1000 | Loss: 0.00001673
Iteration 40/1000 | Loss: 0.00001672
Iteration 41/1000 | Loss: 0.00001671
Iteration 42/1000 | Loss: 0.00001670
Iteration 43/1000 | Loss: 0.00001670
Iteration 44/1000 | Loss: 0.00001670
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001669
Iteration 47/1000 | Loss: 0.00001669
Iteration 48/1000 | Loss: 0.00001668
Iteration 49/1000 | Loss: 0.00001667
Iteration 50/1000 | Loss: 0.00001667
Iteration 51/1000 | Loss: 0.00001665
Iteration 52/1000 | Loss: 0.00001664
Iteration 53/1000 | Loss: 0.00001664
Iteration 54/1000 | Loss: 0.00001663
Iteration 55/1000 | Loss: 0.00001663
Iteration 56/1000 | Loss: 0.00001662
Iteration 57/1000 | Loss: 0.00001662
Iteration 58/1000 | Loss: 0.00001662
Iteration 59/1000 | Loss: 0.00001662
Iteration 60/1000 | Loss: 0.00001662
Iteration 61/1000 | Loss: 0.00001662
Iteration 62/1000 | Loss: 0.00001662
Iteration 63/1000 | Loss: 0.00001662
Iteration 64/1000 | Loss: 0.00001662
Iteration 65/1000 | Loss: 0.00001662
Iteration 66/1000 | Loss: 0.00001662
Iteration 67/1000 | Loss: 0.00001661
Iteration 68/1000 | Loss: 0.00001661
Iteration 69/1000 | Loss: 0.00001661
Iteration 70/1000 | Loss: 0.00001661
Iteration 71/1000 | Loss: 0.00001661
Iteration 72/1000 | Loss: 0.00001660
Iteration 73/1000 | Loss: 0.00001660
Iteration 74/1000 | Loss: 0.00001660
Iteration 75/1000 | Loss: 0.00001660
Iteration 76/1000 | Loss: 0.00001659
Iteration 77/1000 | Loss: 0.00001659
Iteration 78/1000 | Loss: 0.00001659
Iteration 79/1000 | Loss: 0.00001659
Iteration 80/1000 | Loss: 0.00001659
Iteration 81/1000 | Loss: 0.00001659
Iteration 82/1000 | Loss: 0.00001659
Iteration 83/1000 | Loss: 0.00001659
Iteration 84/1000 | Loss: 0.00001659
Iteration 85/1000 | Loss: 0.00001658
Iteration 86/1000 | Loss: 0.00001658
Iteration 87/1000 | Loss: 0.00001658
Iteration 88/1000 | Loss: 0.00001658
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001657
Iteration 92/1000 | Loss: 0.00001657
Iteration 93/1000 | Loss: 0.00001657
Iteration 94/1000 | Loss: 0.00001656
Iteration 95/1000 | Loss: 0.00001656
Iteration 96/1000 | Loss: 0.00001656
Iteration 97/1000 | Loss: 0.00001655
Iteration 98/1000 | Loss: 0.00001655
Iteration 99/1000 | Loss: 0.00001655
Iteration 100/1000 | Loss: 0.00001655
Iteration 101/1000 | Loss: 0.00001655
Iteration 102/1000 | Loss: 0.00001655
Iteration 103/1000 | Loss: 0.00001655
Iteration 104/1000 | Loss: 0.00001654
Iteration 105/1000 | Loss: 0.00001654
Iteration 106/1000 | Loss: 0.00001654
Iteration 107/1000 | Loss: 0.00001654
Iteration 108/1000 | Loss: 0.00001654
Iteration 109/1000 | Loss: 0.00001654
Iteration 110/1000 | Loss: 0.00001654
Iteration 111/1000 | Loss: 0.00001654
Iteration 112/1000 | Loss: 0.00001653
Iteration 113/1000 | Loss: 0.00001653
Iteration 114/1000 | Loss: 0.00001653
Iteration 115/1000 | Loss: 0.00001653
Iteration 116/1000 | Loss: 0.00001653
Iteration 117/1000 | Loss: 0.00001653
Iteration 118/1000 | Loss: 0.00001653
Iteration 119/1000 | Loss: 0.00001653
Iteration 120/1000 | Loss: 0.00001653
Iteration 121/1000 | Loss: 0.00001653
Iteration 122/1000 | Loss: 0.00001652
Iteration 123/1000 | Loss: 0.00001652
Iteration 124/1000 | Loss: 0.00001652
Iteration 125/1000 | Loss: 0.00001652
Iteration 126/1000 | Loss: 0.00001652
Iteration 127/1000 | Loss: 0.00001652
Iteration 128/1000 | Loss: 0.00001652
Iteration 129/1000 | Loss: 0.00001652
Iteration 130/1000 | Loss: 0.00001652
Iteration 131/1000 | Loss: 0.00001652
Iteration 132/1000 | Loss: 0.00001652
Iteration 133/1000 | Loss: 0.00001652
Iteration 134/1000 | Loss: 0.00001652
Iteration 135/1000 | Loss: 0.00001652
Iteration 136/1000 | Loss: 0.00001652
Iteration 137/1000 | Loss: 0.00001652
Iteration 138/1000 | Loss: 0.00001652
Iteration 139/1000 | Loss: 0.00001652
Iteration 140/1000 | Loss: 0.00001652
Iteration 141/1000 | Loss: 0.00001652
Iteration 142/1000 | Loss: 0.00001651
Iteration 143/1000 | Loss: 0.00001651
Iteration 144/1000 | Loss: 0.00001651
Iteration 145/1000 | Loss: 0.00001651
Iteration 146/1000 | Loss: 0.00001651
Iteration 147/1000 | Loss: 0.00001651
Iteration 148/1000 | Loss: 0.00001651
Iteration 149/1000 | Loss: 0.00001651
Iteration 150/1000 | Loss: 0.00001651
Iteration 151/1000 | Loss: 0.00001651
Iteration 152/1000 | Loss: 0.00001651
Iteration 153/1000 | Loss: 0.00001651
Iteration 154/1000 | Loss: 0.00001650
Iteration 155/1000 | Loss: 0.00001650
Iteration 156/1000 | Loss: 0.00001650
Iteration 157/1000 | Loss: 0.00001650
Iteration 158/1000 | Loss: 0.00001650
Iteration 159/1000 | Loss: 0.00001650
Iteration 160/1000 | Loss: 0.00001650
Iteration 161/1000 | Loss: 0.00001650
Iteration 162/1000 | Loss: 0.00001650
Iteration 163/1000 | Loss: 0.00001650
Iteration 164/1000 | Loss: 0.00001650
Iteration 165/1000 | Loss: 0.00001649
Iteration 166/1000 | Loss: 0.00001649
Iteration 167/1000 | Loss: 0.00001649
Iteration 168/1000 | Loss: 0.00001649
Iteration 169/1000 | Loss: 0.00001649
Iteration 170/1000 | Loss: 0.00001649
Iteration 171/1000 | Loss: 0.00001649
Iteration 172/1000 | Loss: 0.00001648
Iteration 173/1000 | Loss: 0.00001648
Iteration 174/1000 | Loss: 0.00001648
Iteration 175/1000 | Loss: 0.00001648
Iteration 176/1000 | Loss: 0.00001648
Iteration 177/1000 | Loss: 0.00001648
Iteration 178/1000 | Loss: 0.00001648
Iteration 179/1000 | Loss: 0.00001648
Iteration 180/1000 | Loss: 0.00001648
Iteration 181/1000 | Loss: 0.00001648
Iteration 182/1000 | Loss: 0.00001648
Iteration 183/1000 | Loss: 0.00001647
Iteration 184/1000 | Loss: 0.00001647
Iteration 185/1000 | Loss: 0.00001647
Iteration 186/1000 | Loss: 0.00001647
Iteration 187/1000 | Loss: 0.00001647
Iteration 188/1000 | Loss: 0.00001647
Iteration 189/1000 | Loss: 0.00001646
Iteration 190/1000 | Loss: 0.00001646
Iteration 191/1000 | Loss: 0.00001646
Iteration 192/1000 | Loss: 0.00001646
Iteration 193/1000 | Loss: 0.00001646
Iteration 194/1000 | Loss: 0.00001646
Iteration 195/1000 | Loss: 0.00001646
Iteration 196/1000 | Loss: 0.00001646
Iteration 197/1000 | Loss: 0.00001646
Iteration 198/1000 | Loss: 0.00001646
Iteration 199/1000 | Loss: 0.00001646
Iteration 200/1000 | Loss: 0.00001646
Iteration 201/1000 | Loss: 0.00001645
Iteration 202/1000 | Loss: 0.00001645
Iteration 203/1000 | Loss: 0.00001645
Iteration 204/1000 | Loss: 0.00001645
Iteration 205/1000 | Loss: 0.00001645
Iteration 206/1000 | Loss: 0.00001645
Iteration 207/1000 | Loss: 0.00001645
Iteration 208/1000 | Loss: 0.00001645
Iteration 209/1000 | Loss: 0.00001645
Iteration 210/1000 | Loss: 0.00001645
Iteration 211/1000 | Loss: 0.00001645
Iteration 212/1000 | Loss: 0.00001644
Iteration 213/1000 | Loss: 0.00001644
Iteration 214/1000 | Loss: 0.00001644
Iteration 215/1000 | Loss: 0.00001644
Iteration 216/1000 | Loss: 0.00001644
Iteration 217/1000 | Loss: 0.00001644
Iteration 218/1000 | Loss: 0.00001644
Iteration 219/1000 | Loss: 0.00001644
Iteration 220/1000 | Loss: 0.00001644
Iteration 221/1000 | Loss: 0.00001644
Iteration 222/1000 | Loss: 0.00001644
Iteration 223/1000 | Loss: 0.00001644
Iteration 224/1000 | Loss: 0.00001644
Iteration 225/1000 | Loss: 0.00001644
Iteration 226/1000 | Loss: 0.00001644
Iteration 227/1000 | Loss: 0.00001644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.643990799493622e-05, 1.643990799493622e-05, 1.643990799493622e-05, 1.643990799493622e-05, 1.643990799493622e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.643990799493622e-05

Optimization complete. Final v2v error: 3.437965154647827 mm

Highest mean error: 3.8362956047058105 mm for frame 106

Lowest mean error: 2.8308658599853516 mm for frame 0

Saving results

Total time: 46.23584270477295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055269
Iteration 2/25 | Loss: 0.00173922
Iteration 3/25 | Loss: 0.00142498
Iteration 4/25 | Loss: 0.00139983
Iteration 5/25 | Loss: 0.00139301
Iteration 6/25 | Loss: 0.00139100
Iteration 7/25 | Loss: 0.00139057
Iteration 8/25 | Loss: 0.00139053
Iteration 9/25 | Loss: 0.00139053
Iteration 10/25 | Loss: 0.00139053
Iteration 11/25 | Loss: 0.00139053
Iteration 12/25 | Loss: 0.00139053
Iteration 13/25 | Loss: 0.00139053
Iteration 14/25 | Loss: 0.00139053
Iteration 15/25 | Loss: 0.00139053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013905338710173965, 0.0013905338710173965, 0.0013905338710173965, 0.0013905338710173965, 0.0013905338710173965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013905338710173965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92672259
Iteration 2/25 | Loss: 0.00119084
Iteration 3/25 | Loss: 0.00119084
Iteration 4/25 | Loss: 0.00119084
Iteration 5/25 | Loss: 0.00119084
Iteration 6/25 | Loss: 0.00119083
Iteration 7/25 | Loss: 0.00119083
Iteration 8/25 | Loss: 0.00119083
Iteration 9/25 | Loss: 0.00119083
Iteration 10/25 | Loss: 0.00119083
Iteration 11/25 | Loss: 0.00119083
Iteration 12/25 | Loss: 0.00119083
Iteration 13/25 | Loss: 0.00119083
Iteration 14/25 | Loss: 0.00119083
Iteration 15/25 | Loss: 0.00119083
Iteration 16/25 | Loss: 0.00119083
Iteration 17/25 | Loss: 0.00119083
Iteration 18/25 | Loss: 0.00119083
Iteration 19/25 | Loss: 0.00119083
Iteration 20/25 | Loss: 0.00119083
Iteration 21/25 | Loss: 0.00119083
Iteration 22/25 | Loss: 0.00119083
Iteration 23/25 | Loss: 0.00119083
Iteration 24/25 | Loss: 0.00119083
Iteration 25/25 | Loss: 0.00119083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119083
Iteration 2/1000 | Loss: 0.00005064
Iteration 3/1000 | Loss: 0.00003624
Iteration 4/1000 | Loss: 0.00003089
Iteration 5/1000 | Loss: 0.00002949
Iteration 6/1000 | Loss: 0.00002840
Iteration 7/1000 | Loss: 0.00002781
Iteration 8/1000 | Loss: 0.00002735
Iteration 9/1000 | Loss: 0.00002709
Iteration 10/1000 | Loss: 0.00002672
Iteration 11/1000 | Loss: 0.00002644
Iteration 12/1000 | Loss: 0.00002613
Iteration 13/1000 | Loss: 0.00002589
Iteration 14/1000 | Loss: 0.00002566
Iteration 15/1000 | Loss: 0.00002548
Iteration 16/1000 | Loss: 0.00002525
Iteration 17/1000 | Loss: 0.00002520
Iteration 18/1000 | Loss: 0.00002515
Iteration 19/1000 | Loss: 0.00002510
Iteration 20/1000 | Loss: 0.00002505
Iteration 21/1000 | Loss: 0.00002501
Iteration 22/1000 | Loss: 0.00002501
Iteration 23/1000 | Loss: 0.00002494
Iteration 24/1000 | Loss: 0.00002488
Iteration 25/1000 | Loss: 0.00002487
Iteration 26/1000 | Loss: 0.00002485
Iteration 27/1000 | Loss: 0.00002482
Iteration 28/1000 | Loss: 0.00002481
Iteration 29/1000 | Loss: 0.00002480
Iteration 30/1000 | Loss: 0.00002480
Iteration 31/1000 | Loss: 0.00002479
Iteration 32/1000 | Loss: 0.00002479
Iteration 33/1000 | Loss: 0.00002478
Iteration 34/1000 | Loss: 0.00002478
Iteration 35/1000 | Loss: 0.00002478
Iteration 36/1000 | Loss: 0.00002477
Iteration 37/1000 | Loss: 0.00002477
Iteration 38/1000 | Loss: 0.00002477
Iteration 39/1000 | Loss: 0.00002477
Iteration 40/1000 | Loss: 0.00002477
Iteration 41/1000 | Loss: 0.00002477
Iteration 42/1000 | Loss: 0.00002477
Iteration 43/1000 | Loss: 0.00002476
Iteration 44/1000 | Loss: 0.00002476
Iteration 45/1000 | Loss: 0.00002476
Iteration 46/1000 | Loss: 0.00002476
Iteration 47/1000 | Loss: 0.00002476
Iteration 48/1000 | Loss: 0.00002476
Iteration 49/1000 | Loss: 0.00002475
Iteration 50/1000 | Loss: 0.00002475
Iteration 51/1000 | Loss: 0.00002475
Iteration 52/1000 | Loss: 0.00002474
Iteration 53/1000 | Loss: 0.00002474
Iteration 54/1000 | Loss: 0.00002473
Iteration 55/1000 | Loss: 0.00002473
Iteration 56/1000 | Loss: 0.00002473
Iteration 57/1000 | Loss: 0.00002472
Iteration 58/1000 | Loss: 0.00002472
Iteration 59/1000 | Loss: 0.00002472
Iteration 60/1000 | Loss: 0.00002472
Iteration 61/1000 | Loss: 0.00002471
Iteration 62/1000 | Loss: 0.00002471
Iteration 63/1000 | Loss: 0.00002471
Iteration 64/1000 | Loss: 0.00002471
Iteration 65/1000 | Loss: 0.00002471
Iteration 66/1000 | Loss: 0.00002471
Iteration 67/1000 | Loss: 0.00002471
Iteration 68/1000 | Loss: 0.00002471
Iteration 69/1000 | Loss: 0.00002471
Iteration 70/1000 | Loss: 0.00002471
Iteration 71/1000 | Loss: 0.00002471
Iteration 72/1000 | Loss: 0.00002471
Iteration 73/1000 | Loss: 0.00002470
Iteration 74/1000 | Loss: 0.00002470
Iteration 75/1000 | Loss: 0.00002470
Iteration 76/1000 | Loss: 0.00002470
Iteration 77/1000 | Loss: 0.00002470
Iteration 78/1000 | Loss: 0.00002469
Iteration 79/1000 | Loss: 0.00002469
Iteration 80/1000 | Loss: 0.00002469
Iteration 81/1000 | Loss: 0.00002469
Iteration 82/1000 | Loss: 0.00002469
Iteration 83/1000 | Loss: 0.00002469
Iteration 84/1000 | Loss: 0.00002469
Iteration 85/1000 | Loss: 0.00002469
Iteration 86/1000 | Loss: 0.00002469
Iteration 87/1000 | Loss: 0.00002469
Iteration 88/1000 | Loss: 0.00002468
Iteration 89/1000 | Loss: 0.00002468
Iteration 90/1000 | Loss: 0.00002468
Iteration 91/1000 | Loss: 0.00002468
Iteration 92/1000 | Loss: 0.00002467
Iteration 93/1000 | Loss: 0.00002467
Iteration 94/1000 | Loss: 0.00002467
Iteration 95/1000 | Loss: 0.00002467
Iteration 96/1000 | Loss: 0.00002467
Iteration 97/1000 | Loss: 0.00002467
Iteration 98/1000 | Loss: 0.00002467
Iteration 99/1000 | Loss: 0.00002467
Iteration 100/1000 | Loss: 0.00002467
Iteration 101/1000 | Loss: 0.00002467
Iteration 102/1000 | Loss: 0.00002467
Iteration 103/1000 | Loss: 0.00002466
Iteration 104/1000 | Loss: 0.00002466
Iteration 105/1000 | Loss: 0.00002466
Iteration 106/1000 | Loss: 0.00002466
Iteration 107/1000 | Loss: 0.00002466
Iteration 108/1000 | Loss: 0.00002466
Iteration 109/1000 | Loss: 0.00002466
Iteration 110/1000 | Loss: 0.00002466
Iteration 111/1000 | Loss: 0.00002465
Iteration 112/1000 | Loss: 0.00002465
Iteration 113/1000 | Loss: 0.00002465
Iteration 114/1000 | Loss: 0.00002465
Iteration 115/1000 | Loss: 0.00002465
Iteration 116/1000 | Loss: 0.00002465
Iteration 117/1000 | Loss: 0.00002465
Iteration 118/1000 | Loss: 0.00002465
Iteration 119/1000 | Loss: 0.00002465
Iteration 120/1000 | Loss: 0.00002464
Iteration 121/1000 | Loss: 0.00002464
Iteration 122/1000 | Loss: 0.00002464
Iteration 123/1000 | Loss: 0.00002464
Iteration 124/1000 | Loss: 0.00002464
Iteration 125/1000 | Loss: 0.00002464
Iteration 126/1000 | Loss: 0.00002464
Iteration 127/1000 | Loss: 0.00002464
Iteration 128/1000 | Loss: 0.00002464
Iteration 129/1000 | Loss: 0.00002464
Iteration 130/1000 | Loss: 0.00002464
Iteration 131/1000 | Loss: 0.00002464
Iteration 132/1000 | Loss: 0.00002464
Iteration 133/1000 | Loss: 0.00002464
Iteration 134/1000 | Loss: 0.00002464
Iteration 135/1000 | Loss: 0.00002463
Iteration 136/1000 | Loss: 0.00002463
Iteration 137/1000 | Loss: 0.00002463
Iteration 138/1000 | Loss: 0.00002463
Iteration 139/1000 | Loss: 0.00002463
Iteration 140/1000 | Loss: 0.00002463
Iteration 141/1000 | Loss: 0.00002463
Iteration 142/1000 | Loss: 0.00002463
Iteration 143/1000 | Loss: 0.00002463
Iteration 144/1000 | Loss: 0.00002463
Iteration 145/1000 | Loss: 0.00002463
Iteration 146/1000 | Loss: 0.00002463
Iteration 147/1000 | Loss: 0.00002463
Iteration 148/1000 | Loss: 0.00002463
Iteration 149/1000 | Loss: 0.00002463
Iteration 150/1000 | Loss: 0.00002463
Iteration 151/1000 | Loss: 0.00002463
Iteration 152/1000 | Loss: 0.00002463
Iteration 153/1000 | Loss: 0.00002463
Iteration 154/1000 | Loss: 0.00002463
Iteration 155/1000 | Loss: 0.00002463
Iteration 156/1000 | Loss: 0.00002463
Iteration 157/1000 | Loss: 0.00002463
Iteration 158/1000 | Loss: 0.00002463
Iteration 159/1000 | Loss: 0.00002463
Iteration 160/1000 | Loss: 0.00002463
Iteration 161/1000 | Loss: 0.00002463
Iteration 162/1000 | Loss: 0.00002463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.462546035530977e-05, 2.462546035530977e-05, 2.462546035530977e-05, 2.462546035530977e-05, 2.462546035530977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.462546035530977e-05

Optimization complete. Final v2v error: 4.118541240692139 mm

Highest mean error: 4.979615688323975 mm for frame 139

Lowest mean error: 3.4294912815093994 mm for frame 25

Saving results

Total time: 50.080528020858765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074287
Iteration 2/25 | Loss: 0.00203198
Iteration 3/25 | Loss: 0.00155618
Iteration 4/25 | Loss: 0.00145581
Iteration 5/25 | Loss: 0.00145740
Iteration 6/25 | Loss: 0.00133151
Iteration 7/25 | Loss: 0.00129365
Iteration 8/25 | Loss: 0.00127548
Iteration 9/25 | Loss: 0.00124942
Iteration 10/25 | Loss: 0.00124444
Iteration 11/25 | Loss: 0.00123970
Iteration 12/25 | Loss: 0.00123623
Iteration 13/25 | Loss: 0.00123198
Iteration 14/25 | Loss: 0.00123088
Iteration 15/25 | Loss: 0.00123045
Iteration 16/25 | Loss: 0.00123034
Iteration 17/25 | Loss: 0.00123033
Iteration 18/25 | Loss: 0.00123032
Iteration 19/25 | Loss: 0.00123032
Iteration 20/25 | Loss: 0.00123032
Iteration 21/25 | Loss: 0.00123032
Iteration 22/25 | Loss: 0.00123032
Iteration 23/25 | Loss: 0.00123032
Iteration 24/25 | Loss: 0.00123032
Iteration 25/25 | Loss: 0.00123032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07399511
Iteration 2/25 | Loss: 0.00364719
Iteration 3/25 | Loss: 0.00364719
Iteration 4/25 | Loss: 0.00364719
Iteration 5/25 | Loss: 0.00364719
Iteration 6/25 | Loss: 0.00364719
Iteration 7/25 | Loss: 0.00364719
Iteration 8/25 | Loss: 0.00364719
Iteration 9/25 | Loss: 0.00364719
Iteration 10/25 | Loss: 0.00364719
Iteration 11/25 | Loss: 0.00364719
Iteration 12/25 | Loss: 0.00364719
Iteration 13/25 | Loss: 0.00364719
Iteration 14/25 | Loss: 0.00364719
Iteration 15/25 | Loss: 0.00364719
Iteration 16/25 | Loss: 0.00364719
Iteration 17/25 | Loss: 0.00364719
Iteration 18/25 | Loss: 0.00364719
Iteration 19/25 | Loss: 0.00364719
Iteration 20/25 | Loss: 0.00364719
Iteration 21/25 | Loss: 0.00364719
Iteration 22/25 | Loss: 0.00364719
Iteration 23/25 | Loss: 0.00364719
Iteration 24/25 | Loss: 0.00364719
Iteration 25/25 | Loss: 0.00364719

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00364719
Iteration 2/1000 | Loss: 0.00004295
Iteration 3/1000 | Loss: 0.00003007
Iteration 4/1000 | Loss: 0.00002762
Iteration 5/1000 | Loss: 0.00002601
Iteration 6/1000 | Loss: 0.00002512
Iteration 7/1000 | Loss: 0.00002456
Iteration 8/1000 | Loss: 0.00002421
Iteration 9/1000 | Loss: 0.00002403
Iteration 10/1000 | Loss: 0.00002393
Iteration 11/1000 | Loss: 0.00002375
Iteration 12/1000 | Loss: 0.00002355
Iteration 13/1000 | Loss: 0.00002335
Iteration 14/1000 | Loss: 0.00002322
Iteration 15/1000 | Loss: 0.00002320
Iteration 16/1000 | Loss: 0.00002318
Iteration 17/1000 | Loss: 0.00002318
Iteration 18/1000 | Loss: 0.00002317
Iteration 19/1000 | Loss: 0.00002317
Iteration 20/1000 | Loss: 0.00002317
Iteration 21/1000 | Loss: 0.00002317
Iteration 22/1000 | Loss: 0.00002317
Iteration 23/1000 | Loss: 0.00002317
Iteration 24/1000 | Loss: 0.00002313
Iteration 25/1000 | Loss: 0.00002312
Iteration 26/1000 | Loss: 0.00002312
Iteration 27/1000 | Loss: 0.00002312
Iteration 28/1000 | Loss: 0.00002312
Iteration 29/1000 | Loss: 0.00002312
Iteration 30/1000 | Loss: 0.00002312
Iteration 31/1000 | Loss: 0.00002312
Iteration 32/1000 | Loss: 0.00002311
Iteration 33/1000 | Loss: 0.00002310
Iteration 34/1000 | Loss: 0.00002306
Iteration 35/1000 | Loss: 0.00002306
Iteration 36/1000 | Loss: 0.00002306
Iteration 37/1000 | Loss: 0.00002306
Iteration 38/1000 | Loss: 0.00002304
Iteration 39/1000 | Loss: 0.00002304
Iteration 40/1000 | Loss: 0.00002302
Iteration 41/1000 | Loss: 0.00002302
Iteration 42/1000 | Loss: 0.00002301
Iteration 43/1000 | Loss: 0.00002301
Iteration 44/1000 | Loss: 0.00002300
Iteration 45/1000 | Loss: 0.00002300
Iteration 46/1000 | Loss: 0.00002300
Iteration 47/1000 | Loss: 0.00002300
Iteration 48/1000 | Loss: 0.00002299
Iteration 49/1000 | Loss: 0.00002299
Iteration 50/1000 | Loss: 0.00002299
Iteration 51/1000 | Loss: 0.00002299
Iteration 52/1000 | Loss: 0.00002298
Iteration 53/1000 | Loss: 0.00002296
Iteration 54/1000 | Loss: 0.00002296
Iteration 55/1000 | Loss: 0.00002293
Iteration 56/1000 | Loss: 0.00002293
Iteration 57/1000 | Loss: 0.00002293
Iteration 58/1000 | Loss: 0.00002292
Iteration 59/1000 | Loss: 0.00002292
Iteration 60/1000 | Loss: 0.00002292
Iteration 61/1000 | Loss: 0.00002291
Iteration 62/1000 | Loss: 0.00002291
Iteration 63/1000 | Loss: 0.00002291
Iteration 64/1000 | Loss: 0.00002291
Iteration 65/1000 | Loss: 0.00002291
Iteration 66/1000 | Loss: 0.00002291
Iteration 67/1000 | Loss: 0.00002291
Iteration 68/1000 | Loss: 0.00002290
Iteration 69/1000 | Loss: 0.00002290
Iteration 70/1000 | Loss: 0.00002289
Iteration 71/1000 | Loss: 0.00002288
Iteration 72/1000 | Loss: 0.00002288
Iteration 73/1000 | Loss: 0.00002288
Iteration 74/1000 | Loss: 0.00002288
Iteration 75/1000 | Loss: 0.00002288
Iteration 76/1000 | Loss: 0.00002288
Iteration 77/1000 | Loss: 0.00002287
Iteration 78/1000 | Loss: 0.00002287
Iteration 79/1000 | Loss: 0.00002287
Iteration 80/1000 | Loss: 0.00002287
Iteration 81/1000 | Loss: 0.00002287
Iteration 82/1000 | Loss: 0.00002287
Iteration 83/1000 | Loss: 0.00002287
Iteration 84/1000 | Loss: 0.00002286
Iteration 85/1000 | Loss: 0.00002286
Iteration 86/1000 | Loss: 0.00002285
Iteration 87/1000 | Loss: 0.00002285
Iteration 88/1000 | Loss: 0.00002285
Iteration 89/1000 | Loss: 0.00002284
Iteration 90/1000 | Loss: 0.00002284
Iteration 91/1000 | Loss: 0.00002284
Iteration 92/1000 | Loss: 0.00002284
Iteration 93/1000 | Loss: 0.00002284
Iteration 94/1000 | Loss: 0.00002284
Iteration 95/1000 | Loss: 0.00002284
Iteration 96/1000 | Loss: 0.00002283
Iteration 97/1000 | Loss: 0.00002283
Iteration 98/1000 | Loss: 0.00002283
Iteration 99/1000 | Loss: 0.00002283
Iteration 100/1000 | Loss: 0.00002283
Iteration 101/1000 | Loss: 0.00002283
Iteration 102/1000 | Loss: 0.00002283
Iteration 103/1000 | Loss: 0.00002283
Iteration 104/1000 | Loss: 0.00002281
Iteration 105/1000 | Loss: 0.00002281
Iteration 106/1000 | Loss: 0.00002280
Iteration 107/1000 | Loss: 0.00002280
Iteration 108/1000 | Loss: 0.00002280
Iteration 109/1000 | Loss: 0.00002280
Iteration 110/1000 | Loss: 0.00002280
Iteration 111/1000 | Loss: 0.00002279
Iteration 112/1000 | Loss: 0.00002279
Iteration 113/1000 | Loss: 0.00002278
Iteration 114/1000 | Loss: 0.00002277
Iteration 115/1000 | Loss: 0.00002277
Iteration 116/1000 | Loss: 0.00002277
Iteration 117/1000 | Loss: 0.00002277
Iteration 118/1000 | Loss: 0.00002277
Iteration 119/1000 | Loss: 0.00002277
Iteration 120/1000 | Loss: 0.00002277
Iteration 121/1000 | Loss: 0.00002277
Iteration 122/1000 | Loss: 0.00002277
Iteration 123/1000 | Loss: 0.00002276
Iteration 124/1000 | Loss: 0.00002276
Iteration 125/1000 | Loss: 0.00002275
Iteration 126/1000 | Loss: 0.00002275
Iteration 127/1000 | Loss: 0.00002275
Iteration 128/1000 | Loss: 0.00002275
Iteration 129/1000 | Loss: 0.00002275
Iteration 130/1000 | Loss: 0.00002275
Iteration 131/1000 | Loss: 0.00002274
Iteration 132/1000 | Loss: 0.00002274
Iteration 133/1000 | Loss: 0.00002274
Iteration 134/1000 | Loss: 0.00002274
Iteration 135/1000 | Loss: 0.00002274
Iteration 136/1000 | Loss: 0.00002274
Iteration 137/1000 | Loss: 0.00002273
Iteration 138/1000 | Loss: 0.00002273
Iteration 139/1000 | Loss: 0.00002273
Iteration 140/1000 | Loss: 0.00002272
Iteration 141/1000 | Loss: 0.00002272
Iteration 142/1000 | Loss: 0.00002272
Iteration 143/1000 | Loss: 0.00002272
Iteration 144/1000 | Loss: 0.00002272
Iteration 145/1000 | Loss: 0.00002272
Iteration 146/1000 | Loss: 0.00002272
Iteration 147/1000 | Loss: 0.00002272
Iteration 148/1000 | Loss: 0.00002272
Iteration 149/1000 | Loss: 0.00002272
Iteration 150/1000 | Loss: 0.00002272
Iteration 151/1000 | Loss: 0.00002272
Iteration 152/1000 | Loss: 0.00002271
Iteration 153/1000 | Loss: 0.00002271
Iteration 154/1000 | Loss: 0.00002271
Iteration 155/1000 | Loss: 0.00002271
Iteration 156/1000 | Loss: 0.00002271
Iteration 157/1000 | Loss: 0.00002271
Iteration 158/1000 | Loss: 0.00002271
Iteration 159/1000 | Loss: 0.00002271
Iteration 160/1000 | Loss: 0.00002271
Iteration 161/1000 | Loss: 0.00002270
Iteration 162/1000 | Loss: 0.00002270
Iteration 163/1000 | Loss: 0.00002270
Iteration 164/1000 | Loss: 0.00002270
Iteration 165/1000 | Loss: 0.00002270
Iteration 166/1000 | Loss: 0.00002270
Iteration 167/1000 | Loss: 0.00002270
Iteration 168/1000 | Loss: 0.00002270
Iteration 169/1000 | Loss: 0.00002270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.2703032300341874e-05, 2.2703032300341874e-05, 2.2703032300341874e-05, 2.2703032300341874e-05, 2.2703032300341874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2703032300341874e-05

Optimization complete. Final v2v error: 3.9758613109588623 mm

Highest mean error: 4.264030933380127 mm for frame 117

Lowest mean error: 3.7990479469299316 mm for frame 40

Saving results

Total time: 61.6245014667511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470782
Iteration 2/25 | Loss: 0.00143500
Iteration 3/25 | Loss: 0.00116209
Iteration 4/25 | Loss: 0.00112055
Iteration 5/25 | Loss: 0.00111499
Iteration 6/25 | Loss: 0.00111377
Iteration 7/25 | Loss: 0.00111371
Iteration 8/25 | Loss: 0.00111371
Iteration 9/25 | Loss: 0.00111371
Iteration 10/25 | Loss: 0.00111371
Iteration 11/25 | Loss: 0.00111371
Iteration 12/25 | Loss: 0.00111371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011137091787531972, 0.0011137091787531972, 0.0011137091787531972, 0.0011137091787531972, 0.0011137091787531972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011137091787531972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15197003
Iteration 2/25 | Loss: 0.00282023
Iteration 3/25 | Loss: 0.00282022
Iteration 4/25 | Loss: 0.00282022
Iteration 5/25 | Loss: 0.00282022
Iteration 6/25 | Loss: 0.00282022
Iteration 7/25 | Loss: 0.00282022
Iteration 8/25 | Loss: 0.00282022
Iteration 9/25 | Loss: 0.00282022
Iteration 10/25 | Loss: 0.00282022
Iteration 11/25 | Loss: 0.00282022
Iteration 12/25 | Loss: 0.00282022
Iteration 13/25 | Loss: 0.00282022
Iteration 14/25 | Loss: 0.00282022
Iteration 15/25 | Loss: 0.00282022
Iteration 16/25 | Loss: 0.00282022
Iteration 17/25 | Loss: 0.00282022
Iteration 18/25 | Loss: 0.00282022
Iteration 19/25 | Loss: 0.00282022
Iteration 20/25 | Loss: 0.00282022
Iteration 21/25 | Loss: 0.00282022
Iteration 22/25 | Loss: 0.00282022
Iteration 23/25 | Loss: 0.00282022
Iteration 24/25 | Loss: 0.00282022
Iteration 25/25 | Loss: 0.00282022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00282022
Iteration 2/1000 | Loss: 0.00003506
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001699
Iteration 5/1000 | Loss: 0.00001585
Iteration 6/1000 | Loss: 0.00001497
Iteration 7/1000 | Loss: 0.00001440
Iteration 8/1000 | Loss: 0.00001382
Iteration 9/1000 | Loss: 0.00001343
Iteration 10/1000 | Loss: 0.00001321
Iteration 11/1000 | Loss: 0.00001302
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001285
Iteration 14/1000 | Loss: 0.00001277
Iteration 15/1000 | Loss: 0.00001275
Iteration 16/1000 | Loss: 0.00001273
Iteration 17/1000 | Loss: 0.00001272
Iteration 18/1000 | Loss: 0.00001270
Iteration 19/1000 | Loss: 0.00001270
Iteration 20/1000 | Loss: 0.00001269
Iteration 21/1000 | Loss: 0.00001269
Iteration 22/1000 | Loss: 0.00001269
Iteration 23/1000 | Loss: 0.00001269
Iteration 24/1000 | Loss: 0.00001268
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001267
Iteration 27/1000 | Loss: 0.00001266
Iteration 28/1000 | Loss: 0.00001265
Iteration 29/1000 | Loss: 0.00001265
Iteration 30/1000 | Loss: 0.00001264
Iteration 31/1000 | Loss: 0.00001264
Iteration 32/1000 | Loss: 0.00001264
Iteration 33/1000 | Loss: 0.00001264
Iteration 34/1000 | Loss: 0.00001263
Iteration 35/1000 | Loss: 0.00001263
Iteration 36/1000 | Loss: 0.00001263
Iteration 37/1000 | Loss: 0.00001263
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001263
Iteration 40/1000 | Loss: 0.00001261
Iteration 41/1000 | Loss: 0.00001259
Iteration 42/1000 | Loss: 0.00001258
Iteration 43/1000 | Loss: 0.00001258
Iteration 44/1000 | Loss: 0.00001257
Iteration 45/1000 | Loss: 0.00001257
Iteration 46/1000 | Loss: 0.00001255
Iteration 47/1000 | Loss: 0.00001254
Iteration 48/1000 | Loss: 0.00001254
Iteration 49/1000 | Loss: 0.00001254
Iteration 50/1000 | Loss: 0.00001254
Iteration 51/1000 | Loss: 0.00001254
Iteration 52/1000 | Loss: 0.00001254
Iteration 53/1000 | Loss: 0.00001254
Iteration 54/1000 | Loss: 0.00001254
Iteration 55/1000 | Loss: 0.00001254
Iteration 56/1000 | Loss: 0.00001253
Iteration 57/1000 | Loss: 0.00001252
Iteration 58/1000 | Loss: 0.00001252
Iteration 59/1000 | Loss: 0.00001251
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001250
Iteration 65/1000 | Loss: 0.00001250
Iteration 66/1000 | Loss: 0.00001250
Iteration 67/1000 | Loss: 0.00001250
Iteration 68/1000 | Loss: 0.00001250
Iteration 69/1000 | Loss: 0.00001249
Iteration 70/1000 | Loss: 0.00001249
Iteration 71/1000 | Loss: 0.00001249
Iteration 72/1000 | Loss: 0.00001248
Iteration 73/1000 | Loss: 0.00001248
Iteration 74/1000 | Loss: 0.00001248
Iteration 75/1000 | Loss: 0.00001248
Iteration 76/1000 | Loss: 0.00001247
Iteration 77/1000 | Loss: 0.00001247
Iteration 78/1000 | Loss: 0.00001247
Iteration 79/1000 | Loss: 0.00001247
Iteration 80/1000 | Loss: 0.00001247
Iteration 81/1000 | Loss: 0.00001247
Iteration 82/1000 | Loss: 0.00001246
Iteration 83/1000 | Loss: 0.00001246
Iteration 84/1000 | Loss: 0.00001246
Iteration 85/1000 | Loss: 0.00001246
Iteration 86/1000 | Loss: 0.00001246
Iteration 87/1000 | Loss: 0.00001246
Iteration 88/1000 | Loss: 0.00001246
Iteration 89/1000 | Loss: 0.00001246
Iteration 90/1000 | Loss: 0.00001245
Iteration 91/1000 | Loss: 0.00001245
Iteration 92/1000 | Loss: 0.00001245
Iteration 93/1000 | Loss: 0.00001245
Iteration 94/1000 | Loss: 0.00001245
Iteration 95/1000 | Loss: 0.00001245
Iteration 96/1000 | Loss: 0.00001244
Iteration 97/1000 | Loss: 0.00001244
Iteration 98/1000 | Loss: 0.00001244
Iteration 99/1000 | Loss: 0.00001244
Iteration 100/1000 | Loss: 0.00001244
Iteration 101/1000 | Loss: 0.00001244
Iteration 102/1000 | Loss: 0.00001244
Iteration 103/1000 | Loss: 0.00001243
Iteration 104/1000 | Loss: 0.00001243
Iteration 105/1000 | Loss: 0.00001243
Iteration 106/1000 | Loss: 0.00001243
Iteration 107/1000 | Loss: 0.00001243
Iteration 108/1000 | Loss: 0.00001243
Iteration 109/1000 | Loss: 0.00001243
Iteration 110/1000 | Loss: 0.00001243
Iteration 111/1000 | Loss: 0.00001243
Iteration 112/1000 | Loss: 0.00001243
Iteration 113/1000 | Loss: 0.00001243
Iteration 114/1000 | Loss: 0.00001243
Iteration 115/1000 | Loss: 0.00001243
Iteration 116/1000 | Loss: 0.00001242
Iteration 117/1000 | Loss: 0.00001242
Iteration 118/1000 | Loss: 0.00001242
Iteration 119/1000 | Loss: 0.00001242
Iteration 120/1000 | Loss: 0.00001242
Iteration 121/1000 | Loss: 0.00001242
Iteration 122/1000 | Loss: 0.00001242
Iteration 123/1000 | Loss: 0.00001242
Iteration 124/1000 | Loss: 0.00001242
Iteration 125/1000 | Loss: 0.00001242
Iteration 126/1000 | Loss: 0.00001242
Iteration 127/1000 | Loss: 0.00001242
Iteration 128/1000 | Loss: 0.00001242
Iteration 129/1000 | Loss: 0.00001242
Iteration 130/1000 | Loss: 0.00001242
Iteration 131/1000 | Loss: 0.00001241
Iteration 132/1000 | Loss: 0.00001241
Iteration 133/1000 | Loss: 0.00001241
Iteration 134/1000 | Loss: 0.00001241
Iteration 135/1000 | Loss: 0.00001241
Iteration 136/1000 | Loss: 0.00001241
Iteration 137/1000 | Loss: 0.00001241
Iteration 138/1000 | Loss: 0.00001241
Iteration 139/1000 | Loss: 0.00001241
Iteration 140/1000 | Loss: 0.00001241
Iteration 141/1000 | Loss: 0.00001241
Iteration 142/1000 | Loss: 0.00001241
Iteration 143/1000 | Loss: 0.00001241
Iteration 144/1000 | Loss: 0.00001241
Iteration 145/1000 | Loss: 0.00001241
Iteration 146/1000 | Loss: 0.00001241
Iteration 147/1000 | Loss: 0.00001241
Iteration 148/1000 | Loss: 0.00001241
Iteration 149/1000 | Loss: 0.00001241
Iteration 150/1000 | Loss: 0.00001241
Iteration 151/1000 | Loss: 0.00001240
Iteration 152/1000 | Loss: 0.00001240
Iteration 153/1000 | Loss: 0.00001240
Iteration 154/1000 | Loss: 0.00001240
Iteration 155/1000 | Loss: 0.00001240
Iteration 156/1000 | Loss: 0.00001240
Iteration 157/1000 | Loss: 0.00001240
Iteration 158/1000 | Loss: 0.00001240
Iteration 159/1000 | Loss: 0.00001240
Iteration 160/1000 | Loss: 0.00001240
Iteration 161/1000 | Loss: 0.00001240
Iteration 162/1000 | Loss: 0.00001240
Iteration 163/1000 | Loss: 0.00001240
Iteration 164/1000 | Loss: 0.00001240
Iteration 165/1000 | Loss: 0.00001240
Iteration 166/1000 | Loss: 0.00001240
Iteration 167/1000 | Loss: 0.00001240
Iteration 168/1000 | Loss: 0.00001240
Iteration 169/1000 | Loss: 0.00001240
Iteration 170/1000 | Loss: 0.00001240
Iteration 171/1000 | Loss: 0.00001240
Iteration 172/1000 | Loss: 0.00001240
Iteration 173/1000 | Loss: 0.00001240
Iteration 174/1000 | Loss: 0.00001240
Iteration 175/1000 | Loss: 0.00001240
Iteration 176/1000 | Loss: 0.00001240
Iteration 177/1000 | Loss: 0.00001240
Iteration 178/1000 | Loss: 0.00001240
Iteration 179/1000 | Loss: 0.00001240
Iteration 180/1000 | Loss: 0.00001240
Iteration 181/1000 | Loss: 0.00001240
Iteration 182/1000 | Loss: 0.00001240
Iteration 183/1000 | Loss: 0.00001240
Iteration 184/1000 | Loss: 0.00001240
Iteration 185/1000 | Loss: 0.00001240
Iteration 186/1000 | Loss: 0.00001240
Iteration 187/1000 | Loss: 0.00001240
Iteration 188/1000 | Loss: 0.00001240
Iteration 189/1000 | Loss: 0.00001240
Iteration 190/1000 | Loss: 0.00001240
Iteration 191/1000 | Loss: 0.00001240
Iteration 192/1000 | Loss: 0.00001240
Iteration 193/1000 | Loss: 0.00001240
Iteration 194/1000 | Loss: 0.00001240
Iteration 195/1000 | Loss: 0.00001240
Iteration 196/1000 | Loss: 0.00001240
Iteration 197/1000 | Loss: 0.00001240
Iteration 198/1000 | Loss: 0.00001240
Iteration 199/1000 | Loss: 0.00001240
Iteration 200/1000 | Loss: 0.00001240
Iteration 201/1000 | Loss: 0.00001240
Iteration 202/1000 | Loss: 0.00001240
Iteration 203/1000 | Loss: 0.00001240
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.2396789315971546e-05, 1.2396789315971546e-05, 1.2396789315971546e-05, 1.2396789315971546e-05, 1.2396789315971546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2396789315971546e-05

Optimization complete. Final v2v error: 2.913396120071411 mm

Highest mean error: 3.8341803550720215 mm for frame 77

Lowest mean error: 2.4827470779418945 mm for frame 168

Saving results

Total time: 40.260390281677246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462221
Iteration 2/25 | Loss: 0.00148388
Iteration 3/25 | Loss: 0.00122188
Iteration 4/25 | Loss: 0.00119632
Iteration 5/25 | Loss: 0.00119394
Iteration 6/25 | Loss: 0.00119378
Iteration 7/25 | Loss: 0.00119378
Iteration 8/25 | Loss: 0.00119378
Iteration 9/25 | Loss: 0.00119378
Iteration 10/25 | Loss: 0.00119378
Iteration 11/25 | Loss: 0.00119378
Iteration 12/25 | Loss: 0.00119378
Iteration 13/25 | Loss: 0.00119378
Iteration 14/25 | Loss: 0.00119378
Iteration 15/25 | Loss: 0.00119378
Iteration 16/25 | Loss: 0.00119378
Iteration 17/25 | Loss: 0.00119378
Iteration 18/25 | Loss: 0.00119378
Iteration 19/25 | Loss: 0.00119378
Iteration 20/25 | Loss: 0.00119378
Iteration 21/25 | Loss: 0.00119378
Iteration 22/25 | Loss: 0.00119378
Iteration 23/25 | Loss: 0.00119378
Iteration 24/25 | Loss: 0.00119378
Iteration 25/25 | Loss: 0.00119378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10568655
Iteration 2/25 | Loss: 0.00292645
Iteration 3/25 | Loss: 0.00292645
Iteration 4/25 | Loss: 0.00292645
Iteration 5/25 | Loss: 0.00292645
Iteration 6/25 | Loss: 0.00292645
Iteration 7/25 | Loss: 0.00292645
Iteration 8/25 | Loss: 0.00292645
Iteration 9/25 | Loss: 0.00292645
Iteration 10/25 | Loss: 0.00292645
Iteration 11/25 | Loss: 0.00292645
Iteration 12/25 | Loss: 0.00292645
Iteration 13/25 | Loss: 0.00292645
Iteration 14/25 | Loss: 0.00292645
Iteration 15/25 | Loss: 0.00292645
Iteration 16/25 | Loss: 0.00292645
Iteration 17/25 | Loss: 0.00292645
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0029264476615935564, 0.0029264476615935564, 0.0029264476615935564, 0.0029264476615935564, 0.0029264476615935564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029264476615935564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00292645
Iteration 2/1000 | Loss: 0.00004866
Iteration 3/1000 | Loss: 0.00003496
Iteration 4/1000 | Loss: 0.00003210
Iteration 5/1000 | Loss: 0.00003067
Iteration 6/1000 | Loss: 0.00002960
Iteration 7/1000 | Loss: 0.00002869
Iteration 8/1000 | Loss: 0.00002804
Iteration 9/1000 | Loss: 0.00002765
Iteration 10/1000 | Loss: 0.00002736
Iteration 11/1000 | Loss: 0.00002717
Iteration 12/1000 | Loss: 0.00002702
Iteration 13/1000 | Loss: 0.00002692
Iteration 14/1000 | Loss: 0.00002682
Iteration 15/1000 | Loss: 0.00002665
Iteration 16/1000 | Loss: 0.00002656
Iteration 17/1000 | Loss: 0.00002643
Iteration 18/1000 | Loss: 0.00002641
Iteration 19/1000 | Loss: 0.00002641
Iteration 20/1000 | Loss: 0.00002639
Iteration 21/1000 | Loss: 0.00002639
Iteration 22/1000 | Loss: 0.00002639
Iteration 23/1000 | Loss: 0.00002639
Iteration 24/1000 | Loss: 0.00002638
Iteration 25/1000 | Loss: 0.00002638
Iteration 26/1000 | Loss: 0.00002638
Iteration 27/1000 | Loss: 0.00002638
Iteration 28/1000 | Loss: 0.00002636
Iteration 29/1000 | Loss: 0.00002636
Iteration 30/1000 | Loss: 0.00002636
Iteration 31/1000 | Loss: 0.00002634
Iteration 32/1000 | Loss: 0.00002634
Iteration 33/1000 | Loss: 0.00002633
Iteration 34/1000 | Loss: 0.00002632
Iteration 35/1000 | Loss: 0.00002632
Iteration 36/1000 | Loss: 0.00002631
Iteration 37/1000 | Loss: 0.00002630
Iteration 38/1000 | Loss: 0.00002630
Iteration 39/1000 | Loss: 0.00002629
Iteration 40/1000 | Loss: 0.00002628
Iteration 41/1000 | Loss: 0.00002628
Iteration 42/1000 | Loss: 0.00002628
Iteration 43/1000 | Loss: 0.00002628
Iteration 44/1000 | Loss: 0.00002628
Iteration 45/1000 | Loss: 0.00002628
Iteration 46/1000 | Loss: 0.00002628
Iteration 47/1000 | Loss: 0.00002627
Iteration 48/1000 | Loss: 0.00002627
Iteration 49/1000 | Loss: 0.00002627
Iteration 50/1000 | Loss: 0.00002627
Iteration 51/1000 | Loss: 0.00002626
Iteration 52/1000 | Loss: 0.00002626
Iteration 53/1000 | Loss: 0.00002625
Iteration 54/1000 | Loss: 0.00002625
Iteration 55/1000 | Loss: 0.00002625
Iteration 56/1000 | Loss: 0.00002625
Iteration 57/1000 | Loss: 0.00002624
Iteration 58/1000 | Loss: 0.00002624
Iteration 59/1000 | Loss: 0.00002623
Iteration 60/1000 | Loss: 0.00002622
Iteration 61/1000 | Loss: 0.00002622
Iteration 62/1000 | Loss: 0.00002622
Iteration 63/1000 | Loss: 0.00002622
Iteration 64/1000 | Loss: 0.00002622
Iteration 65/1000 | Loss: 0.00002622
Iteration 66/1000 | Loss: 0.00002622
Iteration 67/1000 | Loss: 0.00002622
Iteration 68/1000 | Loss: 0.00002622
Iteration 69/1000 | Loss: 0.00002622
Iteration 70/1000 | Loss: 0.00002621
Iteration 71/1000 | Loss: 0.00002621
Iteration 72/1000 | Loss: 0.00002620
Iteration 73/1000 | Loss: 0.00002620
Iteration 74/1000 | Loss: 0.00002620
Iteration 75/1000 | Loss: 0.00002620
Iteration 76/1000 | Loss: 0.00002620
Iteration 77/1000 | Loss: 0.00002620
Iteration 78/1000 | Loss: 0.00002619
Iteration 79/1000 | Loss: 0.00002619
Iteration 80/1000 | Loss: 0.00002618
Iteration 81/1000 | Loss: 0.00002617
Iteration 82/1000 | Loss: 0.00002617
Iteration 83/1000 | Loss: 0.00002617
Iteration 84/1000 | Loss: 0.00002617
Iteration 85/1000 | Loss: 0.00002616
Iteration 86/1000 | Loss: 0.00002616
Iteration 87/1000 | Loss: 0.00002616
Iteration 88/1000 | Loss: 0.00002616
Iteration 89/1000 | Loss: 0.00002616
Iteration 90/1000 | Loss: 0.00002616
Iteration 91/1000 | Loss: 0.00002616
Iteration 92/1000 | Loss: 0.00002616
Iteration 93/1000 | Loss: 0.00002615
Iteration 94/1000 | Loss: 0.00002615
Iteration 95/1000 | Loss: 0.00002615
Iteration 96/1000 | Loss: 0.00002615
Iteration 97/1000 | Loss: 0.00002615
Iteration 98/1000 | Loss: 0.00002615
Iteration 99/1000 | Loss: 0.00002615
Iteration 100/1000 | Loss: 0.00002614
Iteration 101/1000 | Loss: 0.00002614
Iteration 102/1000 | Loss: 0.00002614
Iteration 103/1000 | Loss: 0.00002614
Iteration 104/1000 | Loss: 0.00002614
Iteration 105/1000 | Loss: 0.00002614
Iteration 106/1000 | Loss: 0.00002613
Iteration 107/1000 | Loss: 0.00002613
Iteration 108/1000 | Loss: 0.00002613
Iteration 109/1000 | Loss: 0.00002613
Iteration 110/1000 | Loss: 0.00002613
Iteration 111/1000 | Loss: 0.00002613
Iteration 112/1000 | Loss: 0.00002613
Iteration 113/1000 | Loss: 0.00002613
Iteration 114/1000 | Loss: 0.00002612
Iteration 115/1000 | Loss: 0.00002612
Iteration 116/1000 | Loss: 0.00002612
Iteration 117/1000 | Loss: 0.00002612
Iteration 118/1000 | Loss: 0.00002611
Iteration 119/1000 | Loss: 0.00002611
Iteration 120/1000 | Loss: 0.00002610
Iteration 121/1000 | Loss: 0.00002610
Iteration 122/1000 | Loss: 0.00002610
Iteration 123/1000 | Loss: 0.00002610
Iteration 124/1000 | Loss: 0.00002610
Iteration 125/1000 | Loss: 0.00002610
Iteration 126/1000 | Loss: 0.00002610
Iteration 127/1000 | Loss: 0.00002610
Iteration 128/1000 | Loss: 0.00002609
Iteration 129/1000 | Loss: 0.00002609
Iteration 130/1000 | Loss: 0.00002609
Iteration 131/1000 | Loss: 0.00002609
Iteration 132/1000 | Loss: 0.00002609
Iteration 133/1000 | Loss: 0.00002609
Iteration 134/1000 | Loss: 0.00002608
Iteration 135/1000 | Loss: 0.00002608
Iteration 136/1000 | Loss: 0.00002608
Iteration 137/1000 | Loss: 0.00002608
Iteration 138/1000 | Loss: 0.00002608
Iteration 139/1000 | Loss: 0.00002608
Iteration 140/1000 | Loss: 0.00002607
Iteration 141/1000 | Loss: 0.00002607
Iteration 142/1000 | Loss: 0.00002607
Iteration 143/1000 | Loss: 0.00002607
Iteration 144/1000 | Loss: 0.00002607
Iteration 145/1000 | Loss: 0.00002607
Iteration 146/1000 | Loss: 0.00002607
Iteration 147/1000 | Loss: 0.00002607
Iteration 148/1000 | Loss: 0.00002607
Iteration 149/1000 | Loss: 0.00002607
Iteration 150/1000 | Loss: 0.00002607
Iteration 151/1000 | Loss: 0.00002607
Iteration 152/1000 | Loss: 0.00002607
Iteration 153/1000 | Loss: 0.00002607
Iteration 154/1000 | Loss: 0.00002607
Iteration 155/1000 | Loss: 0.00002607
Iteration 156/1000 | Loss: 0.00002606
Iteration 157/1000 | Loss: 0.00002606
Iteration 158/1000 | Loss: 0.00002606
Iteration 159/1000 | Loss: 0.00002606
Iteration 160/1000 | Loss: 0.00002606
Iteration 161/1000 | Loss: 0.00002605
Iteration 162/1000 | Loss: 0.00002605
Iteration 163/1000 | Loss: 0.00002605
Iteration 164/1000 | Loss: 0.00002605
Iteration 165/1000 | Loss: 0.00002605
Iteration 166/1000 | Loss: 0.00002605
Iteration 167/1000 | Loss: 0.00002605
Iteration 168/1000 | Loss: 0.00002605
Iteration 169/1000 | Loss: 0.00002605
Iteration 170/1000 | Loss: 0.00002605
Iteration 171/1000 | Loss: 0.00002605
Iteration 172/1000 | Loss: 0.00002605
Iteration 173/1000 | Loss: 0.00002605
Iteration 174/1000 | Loss: 0.00002604
Iteration 175/1000 | Loss: 0.00002604
Iteration 176/1000 | Loss: 0.00002604
Iteration 177/1000 | Loss: 0.00002603
Iteration 178/1000 | Loss: 0.00002603
Iteration 179/1000 | Loss: 0.00002603
Iteration 180/1000 | Loss: 0.00002603
Iteration 181/1000 | Loss: 0.00002603
Iteration 182/1000 | Loss: 0.00002603
Iteration 183/1000 | Loss: 0.00002603
Iteration 184/1000 | Loss: 0.00002603
Iteration 185/1000 | Loss: 0.00002603
Iteration 186/1000 | Loss: 0.00002603
Iteration 187/1000 | Loss: 0.00002603
Iteration 188/1000 | Loss: 0.00002602
Iteration 189/1000 | Loss: 0.00002602
Iteration 190/1000 | Loss: 0.00002602
Iteration 191/1000 | Loss: 0.00002602
Iteration 192/1000 | Loss: 0.00002602
Iteration 193/1000 | Loss: 0.00002602
Iteration 194/1000 | Loss: 0.00002602
Iteration 195/1000 | Loss: 0.00002602
Iteration 196/1000 | Loss: 0.00002602
Iteration 197/1000 | Loss: 0.00002602
Iteration 198/1000 | Loss: 0.00002602
Iteration 199/1000 | Loss: 0.00002602
Iteration 200/1000 | Loss: 0.00002602
Iteration 201/1000 | Loss: 0.00002602
Iteration 202/1000 | Loss: 0.00002602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [2.6022857127827592e-05, 2.6022857127827592e-05, 2.6022857127827592e-05, 2.6022857127827592e-05, 2.6022857127827592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6022857127827592e-05

Optimization complete. Final v2v error: 4.0744218826293945 mm

Highest mean error: 4.7182488441467285 mm for frame 160

Lowest mean error: 3.490140438079834 mm for frame 115

Saving results

Total time: 50.007267236709595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044124
Iteration 2/25 | Loss: 0.01044124
Iteration 3/25 | Loss: 0.01044123
Iteration 4/25 | Loss: 0.01044123
Iteration 5/25 | Loss: 0.00272079
Iteration 6/25 | Loss: 0.00190412
Iteration 7/25 | Loss: 0.00172946
Iteration 8/25 | Loss: 0.00173386
Iteration 9/25 | Loss: 0.00176890
Iteration 10/25 | Loss: 0.00164281
Iteration 11/25 | Loss: 0.00153232
Iteration 12/25 | Loss: 0.00148719
Iteration 13/25 | Loss: 0.00148223
Iteration 14/25 | Loss: 0.00142908
Iteration 15/25 | Loss: 0.00134222
Iteration 16/25 | Loss: 0.00129501
Iteration 17/25 | Loss: 0.00127485
Iteration 18/25 | Loss: 0.00125653
Iteration 19/25 | Loss: 0.00124568
Iteration 20/25 | Loss: 0.00124744
Iteration 21/25 | Loss: 0.00123343
Iteration 22/25 | Loss: 0.00122775
Iteration 23/25 | Loss: 0.00123021
Iteration 24/25 | Loss: 0.00122568
Iteration 25/25 | Loss: 0.00122346

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11523151
Iteration 2/25 | Loss: 0.00491317
Iteration 3/25 | Loss: 0.00479887
Iteration 4/25 | Loss: 0.00479887
Iteration 5/25 | Loss: 0.00479887
Iteration 6/25 | Loss: 0.00479887
Iteration 7/25 | Loss: 0.00479886
Iteration 8/25 | Loss: 0.00479886
Iteration 9/25 | Loss: 0.00479886
Iteration 10/25 | Loss: 0.00479886
Iteration 11/25 | Loss: 0.00479886
Iteration 12/25 | Loss: 0.00479886
Iteration 13/25 | Loss: 0.00479886
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.004798864480108023, 0.004798864480108023, 0.004798864480108023, 0.004798864480108023, 0.004798864480108023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004798864480108023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00479886
Iteration 2/1000 | Loss: 0.00056853
Iteration 3/1000 | Loss: 0.00037553
Iteration 4/1000 | Loss: 0.00052639
Iteration 5/1000 | Loss: 0.00028479
Iteration 6/1000 | Loss: 0.00024058
Iteration 7/1000 | Loss: 0.00033139
Iteration 8/1000 | Loss: 0.00035676
Iteration 9/1000 | Loss: 0.00094471
Iteration 10/1000 | Loss: 0.00060477
Iteration 11/1000 | Loss: 0.00115338
Iteration 12/1000 | Loss: 0.00062514
Iteration 13/1000 | Loss: 0.00086019
Iteration 14/1000 | Loss: 0.00065548
Iteration 15/1000 | Loss: 0.00027320
Iteration 16/1000 | Loss: 0.00032213
Iteration 17/1000 | Loss: 0.00023692
Iteration 18/1000 | Loss: 0.00015140
Iteration 19/1000 | Loss: 0.00020625
Iteration 20/1000 | Loss: 0.00018268
Iteration 21/1000 | Loss: 0.00016898
Iteration 22/1000 | Loss: 0.00013075
Iteration 23/1000 | Loss: 0.00085966
Iteration 24/1000 | Loss: 0.00044772
Iteration 25/1000 | Loss: 0.00038854
Iteration 26/1000 | Loss: 0.00023324
Iteration 27/1000 | Loss: 0.00008536
Iteration 28/1000 | Loss: 0.00014130
Iteration 29/1000 | Loss: 0.00008734
Iteration 30/1000 | Loss: 0.00019106
Iteration 31/1000 | Loss: 0.00023313
Iteration 32/1000 | Loss: 0.00044714
Iteration 33/1000 | Loss: 0.00029248
Iteration 34/1000 | Loss: 0.00025763
Iteration 35/1000 | Loss: 0.00020097
Iteration 36/1000 | Loss: 0.00023844
Iteration 37/1000 | Loss: 0.00015671
Iteration 38/1000 | Loss: 0.00027258
Iteration 39/1000 | Loss: 0.00006446
Iteration 40/1000 | Loss: 0.00027163
Iteration 41/1000 | Loss: 0.00023113
Iteration 42/1000 | Loss: 0.00106931
Iteration 43/1000 | Loss: 0.00072700
Iteration 44/1000 | Loss: 0.00046581
Iteration 45/1000 | Loss: 0.00028801
Iteration 46/1000 | Loss: 0.00017071
Iteration 47/1000 | Loss: 0.00024158
Iteration 48/1000 | Loss: 0.00007892
Iteration 49/1000 | Loss: 0.00011526
Iteration 50/1000 | Loss: 0.00010990
Iteration 51/1000 | Loss: 0.00008041
Iteration 52/1000 | Loss: 0.00007137
Iteration 53/1000 | Loss: 0.00016312
Iteration 54/1000 | Loss: 0.00048447
Iteration 55/1000 | Loss: 0.00025110
Iteration 56/1000 | Loss: 0.00022285
Iteration 57/1000 | Loss: 0.00048842
Iteration 58/1000 | Loss: 0.00029739
Iteration 59/1000 | Loss: 0.00028230
Iteration 60/1000 | Loss: 0.00014694
Iteration 61/1000 | Loss: 0.00058780
Iteration 62/1000 | Loss: 0.00059861
Iteration 63/1000 | Loss: 0.00037175
Iteration 64/1000 | Loss: 0.00050632
Iteration 65/1000 | Loss: 0.00019404
Iteration 66/1000 | Loss: 0.00014569
Iteration 67/1000 | Loss: 0.00007197
Iteration 68/1000 | Loss: 0.00031167
Iteration 69/1000 | Loss: 0.00017675
Iteration 70/1000 | Loss: 0.00014234
Iteration 71/1000 | Loss: 0.00010564
Iteration 72/1000 | Loss: 0.00010153
Iteration 73/1000 | Loss: 0.00007483
Iteration 74/1000 | Loss: 0.00008447
Iteration 75/1000 | Loss: 0.00029070
Iteration 76/1000 | Loss: 0.00019627
Iteration 77/1000 | Loss: 0.00022184
Iteration 78/1000 | Loss: 0.00010894
Iteration 79/1000 | Loss: 0.00011155
Iteration 80/1000 | Loss: 0.00005418
Iteration 81/1000 | Loss: 0.00004920
Iteration 82/1000 | Loss: 0.00004606
Iteration 83/1000 | Loss: 0.00003999
Iteration 84/1000 | Loss: 0.00004990
Iteration 85/1000 | Loss: 0.00004226
Iteration 86/1000 | Loss: 0.00003772
Iteration 87/1000 | Loss: 0.00004540
Iteration 88/1000 | Loss: 0.00003674
Iteration 89/1000 | Loss: 0.00004217
Iteration 90/1000 | Loss: 0.00027784
Iteration 91/1000 | Loss: 0.00060150
Iteration 92/1000 | Loss: 0.00048832
Iteration 93/1000 | Loss: 0.00014529
Iteration 94/1000 | Loss: 0.00011379
Iteration 95/1000 | Loss: 0.00004419
Iteration 96/1000 | Loss: 0.00003583
Iteration 97/1000 | Loss: 0.00003345
Iteration 98/1000 | Loss: 0.00003188
Iteration 99/1000 | Loss: 0.00035476
Iteration 100/1000 | Loss: 0.00016489
Iteration 101/1000 | Loss: 0.00032425
Iteration 102/1000 | Loss: 0.00006022
Iteration 103/1000 | Loss: 0.00003478
Iteration 104/1000 | Loss: 0.00003162
Iteration 105/1000 | Loss: 0.00003081
Iteration 106/1000 | Loss: 0.00003002
Iteration 107/1000 | Loss: 0.00002924
Iteration 108/1000 | Loss: 0.00002873
Iteration 109/1000 | Loss: 0.00002842
Iteration 110/1000 | Loss: 0.00002815
Iteration 111/1000 | Loss: 0.00002806
Iteration 112/1000 | Loss: 0.00002806
Iteration 113/1000 | Loss: 0.00002799
Iteration 114/1000 | Loss: 0.00048685
Iteration 115/1000 | Loss: 0.00036186
Iteration 116/1000 | Loss: 0.00003941
Iteration 117/1000 | Loss: 0.00048125
Iteration 118/1000 | Loss: 0.00031037
Iteration 119/1000 | Loss: 0.00004942
Iteration 120/1000 | Loss: 0.00003827
Iteration 121/1000 | Loss: 0.00034812
Iteration 122/1000 | Loss: 0.00003479
Iteration 123/1000 | Loss: 0.00002873
Iteration 124/1000 | Loss: 0.00002646
Iteration 125/1000 | Loss: 0.00002373
Iteration 126/1000 | Loss: 0.00036950
Iteration 127/1000 | Loss: 0.00046567
Iteration 128/1000 | Loss: 0.00002749
Iteration 129/1000 | Loss: 0.00002180
Iteration 130/1000 | Loss: 0.00033608
Iteration 131/1000 | Loss: 0.00011150
Iteration 132/1000 | Loss: 0.00019648
Iteration 133/1000 | Loss: 0.00015705
Iteration 134/1000 | Loss: 0.00018418
Iteration 135/1000 | Loss: 0.00004594
Iteration 136/1000 | Loss: 0.00013065
Iteration 137/1000 | Loss: 0.00001772
Iteration 138/1000 | Loss: 0.00001674
Iteration 139/1000 | Loss: 0.00001600
Iteration 140/1000 | Loss: 0.00001560
Iteration 141/1000 | Loss: 0.00001538
Iteration 142/1000 | Loss: 0.00001518
Iteration 143/1000 | Loss: 0.00001509
Iteration 144/1000 | Loss: 0.00001505
Iteration 145/1000 | Loss: 0.00001504
Iteration 146/1000 | Loss: 0.00001503
Iteration 147/1000 | Loss: 0.00001503
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001502
Iteration 150/1000 | Loss: 0.00001501
Iteration 151/1000 | Loss: 0.00001499
Iteration 152/1000 | Loss: 0.00001499
Iteration 153/1000 | Loss: 0.00001498
Iteration 154/1000 | Loss: 0.00001498
Iteration 155/1000 | Loss: 0.00001498
Iteration 156/1000 | Loss: 0.00001497
Iteration 157/1000 | Loss: 0.00001493
Iteration 158/1000 | Loss: 0.00001492
Iteration 159/1000 | Loss: 0.00001491
Iteration 160/1000 | Loss: 0.00001491
Iteration 161/1000 | Loss: 0.00001484
Iteration 162/1000 | Loss: 0.00001484
Iteration 163/1000 | Loss: 0.00001484
Iteration 164/1000 | Loss: 0.00001484
Iteration 165/1000 | Loss: 0.00001484
Iteration 166/1000 | Loss: 0.00001484
Iteration 167/1000 | Loss: 0.00001484
Iteration 168/1000 | Loss: 0.00001484
Iteration 169/1000 | Loss: 0.00001483
Iteration 170/1000 | Loss: 0.00001483
Iteration 171/1000 | Loss: 0.00001483
Iteration 172/1000 | Loss: 0.00001482
Iteration 173/1000 | Loss: 0.00001482
Iteration 174/1000 | Loss: 0.00001482
Iteration 175/1000 | Loss: 0.00001482
Iteration 176/1000 | Loss: 0.00001482
Iteration 177/1000 | Loss: 0.00001482
Iteration 178/1000 | Loss: 0.00001482
Iteration 179/1000 | Loss: 0.00001481
Iteration 180/1000 | Loss: 0.00001481
Iteration 181/1000 | Loss: 0.00001481
Iteration 182/1000 | Loss: 0.00001481
Iteration 183/1000 | Loss: 0.00001481
Iteration 184/1000 | Loss: 0.00001480
Iteration 185/1000 | Loss: 0.00001480
Iteration 186/1000 | Loss: 0.00001480
Iteration 187/1000 | Loss: 0.00001479
Iteration 188/1000 | Loss: 0.00001479
Iteration 189/1000 | Loss: 0.00001478
Iteration 190/1000 | Loss: 0.00001478
Iteration 191/1000 | Loss: 0.00001478
Iteration 192/1000 | Loss: 0.00001477
Iteration 193/1000 | Loss: 0.00001477
Iteration 194/1000 | Loss: 0.00001477
Iteration 195/1000 | Loss: 0.00001477
Iteration 196/1000 | Loss: 0.00001477
Iteration 197/1000 | Loss: 0.00001477
Iteration 198/1000 | Loss: 0.00001477
Iteration 199/1000 | Loss: 0.00001476
Iteration 200/1000 | Loss: 0.00001476
Iteration 201/1000 | Loss: 0.00001476
Iteration 202/1000 | Loss: 0.00001475
Iteration 203/1000 | Loss: 0.00001475
Iteration 204/1000 | Loss: 0.00001475
Iteration 205/1000 | Loss: 0.00001475
Iteration 206/1000 | Loss: 0.00001475
Iteration 207/1000 | Loss: 0.00001475
Iteration 208/1000 | Loss: 0.00001474
Iteration 209/1000 | Loss: 0.00001474
Iteration 210/1000 | Loss: 0.00001474
Iteration 211/1000 | Loss: 0.00001474
Iteration 212/1000 | Loss: 0.00001474
Iteration 213/1000 | Loss: 0.00001474
Iteration 214/1000 | Loss: 0.00001474
Iteration 215/1000 | Loss: 0.00001474
Iteration 216/1000 | Loss: 0.00001474
Iteration 217/1000 | Loss: 0.00001473
Iteration 218/1000 | Loss: 0.00001473
Iteration 219/1000 | Loss: 0.00001473
Iteration 220/1000 | Loss: 0.00001473
Iteration 221/1000 | Loss: 0.00001473
Iteration 222/1000 | Loss: 0.00001473
Iteration 223/1000 | Loss: 0.00001473
Iteration 224/1000 | Loss: 0.00001473
Iteration 225/1000 | Loss: 0.00001473
Iteration 226/1000 | Loss: 0.00001472
Iteration 227/1000 | Loss: 0.00001472
Iteration 228/1000 | Loss: 0.00001472
Iteration 229/1000 | Loss: 0.00001472
Iteration 230/1000 | Loss: 0.00001472
Iteration 231/1000 | Loss: 0.00001472
Iteration 232/1000 | Loss: 0.00001472
Iteration 233/1000 | Loss: 0.00001472
Iteration 234/1000 | Loss: 0.00001472
Iteration 235/1000 | Loss: 0.00001471
Iteration 236/1000 | Loss: 0.00001471
Iteration 237/1000 | Loss: 0.00001471
Iteration 238/1000 | Loss: 0.00001471
Iteration 239/1000 | Loss: 0.00001471
Iteration 240/1000 | Loss: 0.00001470
Iteration 241/1000 | Loss: 0.00001470
Iteration 242/1000 | Loss: 0.00001470
Iteration 243/1000 | Loss: 0.00001470
Iteration 244/1000 | Loss: 0.00001470
Iteration 245/1000 | Loss: 0.00001470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.47048622238799e-05, 1.47048622238799e-05, 1.47048622238799e-05, 1.47048622238799e-05, 1.47048622238799e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.47048622238799e-05

Optimization complete. Final v2v error: 3.0696167945861816 mm

Highest mean error: 9.351926803588867 mm for frame 50

Lowest mean error: 2.776750087738037 mm for frame 231

Saving results

Total time: 280.5492362976074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534925
Iteration 2/25 | Loss: 0.00132110
Iteration 3/25 | Loss: 0.00117137
Iteration 4/25 | Loss: 0.00115068
Iteration 5/25 | Loss: 0.00114716
Iteration 6/25 | Loss: 0.00114687
Iteration 7/25 | Loss: 0.00114687
Iteration 8/25 | Loss: 0.00114687
Iteration 9/25 | Loss: 0.00114687
Iteration 10/25 | Loss: 0.00114687
Iteration 11/25 | Loss: 0.00114687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011468743905425072, 0.0011468743905425072, 0.0011468743905425072, 0.0011468743905425072, 0.0011468743905425072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011468743905425072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.94595861
Iteration 2/25 | Loss: 0.00210662
Iteration 3/25 | Loss: 0.00210660
Iteration 4/25 | Loss: 0.00210660
Iteration 5/25 | Loss: 0.00210660
Iteration 6/25 | Loss: 0.00210660
Iteration 7/25 | Loss: 0.00210660
Iteration 8/25 | Loss: 0.00210660
Iteration 9/25 | Loss: 0.00210660
Iteration 10/25 | Loss: 0.00210660
Iteration 11/25 | Loss: 0.00210660
Iteration 12/25 | Loss: 0.00210660
Iteration 13/25 | Loss: 0.00210660
Iteration 14/25 | Loss: 0.00210660
Iteration 15/25 | Loss: 0.00210660
Iteration 16/25 | Loss: 0.00210660
Iteration 17/25 | Loss: 0.00210660
Iteration 18/25 | Loss: 0.00210660
Iteration 19/25 | Loss: 0.00210660
Iteration 20/25 | Loss: 0.00210660
Iteration 21/25 | Loss: 0.00210660
Iteration 22/25 | Loss: 0.00210660
Iteration 23/25 | Loss: 0.00210660
Iteration 24/25 | Loss: 0.00210660
Iteration 25/25 | Loss: 0.00210660

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210660
Iteration 2/1000 | Loss: 0.00005062
Iteration 3/1000 | Loss: 0.00002802
Iteration 4/1000 | Loss: 0.00002153
Iteration 5/1000 | Loss: 0.00001926
Iteration 6/1000 | Loss: 0.00001814
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001699
Iteration 9/1000 | Loss: 0.00001666
Iteration 10/1000 | Loss: 0.00001629
Iteration 11/1000 | Loss: 0.00001600
Iteration 12/1000 | Loss: 0.00001579
Iteration 13/1000 | Loss: 0.00001572
Iteration 14/1000 | Loss: 0.00001568
Iteration 15/1000 | Loss: 0.00001568
Iteration 16/1000 | Loss: 0.00001568
Iteration 17/1000 | Loss: 0.00001568
Iteration 18/1000 | Loss: 0.00001568
Iteration 19/1000 | Loss: 0.00001568
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001567
Iteration 22/1000 | Loss: 0.00001567
Iteration 23/1000 | Loss: 0.00001565
Iteration 24/1000 | Loss: 0.00001564
Iteration 25/1000 | Loss: 0.00001563
Iteration 26/1000 | Loss: 0.00001562
Iteration 27/1000 | Loss: 0.00001561
Iteration 28/1000 | Loss: 0.00001561
Iteration 29/1000 | Loss: 0.00001560
Iteration 30/1000 | Loss: 0.00001559
Iteration 31/1000 | Loss: 0.00001556
Iteration 32/1000 | Loss: 0.00001553
Iteration 33/1000 | Loss: 0.00001548
Iteration 34/1000 | Loss: 0.00001548
Iteration 35/1000 | Loss: 0.00001547
Iteration 36/1000 | Loss: 0.00001547
Iteration 37/1000 | Loss: 0.00001547
Iteration 38/1000 | Loss: 0.00001547
Iteration 39/1000 | Loss: 0.00001547
Iteration 40/1000 | Loss: 0.00001547
Iteration 41/1000 | Loss: 0.00001547
Iteration 42/1000 | Loss: 0.00001547
Iteration 43/1000 | Loss: 0.00001547
Iteration 44/1000 | Loss: 0.00001547
Iteration 45/1000 | Loss: 0.00001546
Iteration 46/1000 | Loss: 0.00001546
Iteration 47/1000 | Loss: 0.00001546
Iteration 48/1000 | Loss: 0.00001545
Iteration 49/1000 | Loss: 0.00001545
Iteration 50/1000 | Loss: 0.00001544
Iteration 51/1000 | Loss: 0.00001544
Iteration 52/1000 | Loss: 0.00001544
Iteration 53/1000 | Loss: 0.00001543
Iteration 54/1000 | Loss: 0.00001543
Iteration 55/1000 | Loss: 0.00001543
Iteration 56/1000 | Loss: 0.00001543
Iteration 57/1000 | Loss: 0.00001543
Iteration 58/1000 | Loss: 0.00001542
Iteration 59/1000 | Loss: 0.00001542
Iteration 60/1000 | Loss: 0.00001542
Iteration 61/1000 | Loss: 0.00001542
Iteration 62/1000 | Loss: 0.00001541
Iteration 63/1000 | Loss: 0.00001541
Iteration 64/1000 | Loss: 0.00001541
Iteration 65/1000 | Loss: 0.00001541
Iteration 66/1000 | Loss: 0.00001541
Iteration 67/1000 | Loss: 0.00001541
Iteration 68/1000 | Loss: 0.00001541
Iteration 69/1000 | Loss: 0.00001540
Iteration 70/1000 | Loss: 0.00001540
Iteration 71/1000 | Loss: 0.00001540
Iteration 72/1000 | Loss: 0.00001540
Iteration 73/1000 | Loss: 0.00001540
Iteration 74/1000 | Loss: 0.00001540
Iteration 75/1000 | Loss: 0.00001540
Iteration 76/1000 | Loss: 0.00001540
Iteration 77/1000 | Loss: 0.00001540
Iteration 78/1000 | Loss: 0.00001540
Iteration 79/1000 | Loss: 0.00001540
Iteration 80/1000 | Loss: 0.00001540
Iteration 81/1000 | Loss: 0.00001540
Iteration 82/1000 | Loss: 0.00001540
Iteration 83/1000 | Loss: 0.00001539
Iteration 84/1000 | Loss: 0.00001539
Iteration 85/1000 | Loss: 0.00001539
Iteration 86/1000 | Loss: 0.00001538
Iteration 87/1000 | Loss: 0.00001538
Iteration 88/1000 | Loss: 0.00001538
Iteration 89/1000 | Loss: 0.00001538
Iteration 90/1000 | Loss: 0.00001538
Iteration 91/1000 | Loss: 0.00001538
Iteration 92/1000 | Loss: 0.00001538
Iteration 93/1000 | Loss: 0.00001538
Iteration 94/1000 | Loss: 0.00001538
Iteration 95/1000 | Loss: 0.00001538
Iteration 96/1000 | Loss: 0.00001538
Iteration 97/1000 | Loss: 0.00001537
Iteration 98/1000 | Loss: 0.00001537
Iteration 99/1000 | Loss: 0.00001537
Iteration 100/1000 | Loss: 0.00001537
Iteration 101/1000 | Loss: 0.00001537
Iteration 102/1000 | Loss: 0.00001537
Iteration 103/1000 | Loss: 0.00001537
Iteration 104/1000 | Loss: 0.00001537
Iteration 105/1000 | Loss: 0.00001537
Iteration 106/1000 | Loss: 0.00001537
Iteration 107/1000 | Loss: 0.00001537
Iteration 108/1000 | Loss: 0.00001536
Iteration 109/1000 | Loss: 0.00001536
Iteration 110/1000 | Loss: 0.00001536
Iteration 111/1000 | Loss: 0.00001536
Iteration 112/1000 | Loss: 0.00001536
Iteration 113/1000 | Loss: 0.00001536
Iteration 114/1000 | Loss: 0.00001536
Iteration 115/1000 | Loss: 0.00001536
Iteration 116/1000 | Loss: 0.00001536
Iteration 117/1000 | Loss: 0.00001536
Iteration 118/1000 | Loss: 0.00001536
Iteration 119/1000 | Loss: 0.00001536
Iteration 120/1000 | Loss: 0.00001536
Iteration 121/1000 | Loss: 0.00001536
Iteration 122/1000 | Loss: 0.00001535
Iteration 123/1000 | Loss: 0.00001535
Iteration 124/1000 | Loss: 0.00001535
Iteration 125/1000 | Loss: 0.00001535
Iteration 126/1000 | Loss: 0.00001535
Iteration 127/1000 | Loss: 0.00001535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.5354755305452272e-05, 1.5354755305452272e-05, 1.5354755305452272e-05, 1.5354755305452272e-05, 1.5354755305452272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5354755305452272e-05

Optimization complete. Final v2v error: 3.298942804336548 mm

Highest mean error: 3.5871219635009766 mm for frame 136

Lowest mean error: 3.0804502964019775 mm for frame 232

Saving results

Total time: 39.98923325538635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440176
Iteration 2/25 | Loss: 0.00127885
Iteration 3/25 | Loss: 0.00115930
Iteration 4/25 | Loss: 0.00113926
Iteration 5/25 | Loss: 0.00113484
Iteration 6/25 | Loss: 0.00113320
Iteration 7/25 | Loss: 0.00113307
Iteration 8/25 | Loss: 0.00113307
Iteration 9/25 | Loss: 0.00113307
Iteration 10/25 | Loss: 0.00113307
Iteration 11/25 | Loss: 0.00113307
Iteration 12/25 | Loss: 0.00113307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011330656707286835, 0.0011330656707286835, 0.0011330656707286835, 0.0011330656707286835, 0.0011330656707286835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011330656707286835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08754086
Iteration 2/25 | Loss: 0.00358919
Iteration 3/25 | Loss: 0.00358919
Iteration 4/25 | Loss: 0.00358919
Iteration 5/25 | Loss: 0.00358919
Iteration 6/25 | Loss: 0.00358919
Iteration 7/25 | Loss: 0.00358919
Iteration 8/25 | Loss: 0.00358919
Iteration 9/25 | Loss: 0.00358919
Iteration 10/25 | Loss: 0.00358919
Iteration 11/25 | Loss: 0.00358919
Iteration 12/25 | Loss: 0.00358919
Iteration 13/25 | Loss: 0.00358919
Iteration 14/25 | Loss: 0.00358919
Iteration 15/25 | Loss: 0.00358919
Iteration 16/25 | Loss: 0.00358919
Iteration 17/25 | Loss: 0.00358919
Iteration 18/25 | Loss: 0.00358919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0035891891457140446, 0.0035891891457140446, 0.0035891891457140446, 0.0035891891457140446, 0.0035891891457140446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035891891457140446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00358919
Iteration 2/1000 | Loss: 0.00003960
Iteration 3/1000 | Loss: 0.00002607
Iteration 4/1000 | Loss: 0.00002145
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001878
Iteration 7/1000 | Loss: 0.00001774
Iteration 8/1000 | Loss: 0.00001721
Iteration 9/1000 | Loss: 0.00001676
Iteration 10/1000 | Loss: 0.00001636
Iteration 11/1000 | Loss: 0.00001614
Iteration 12/1000 | Loss: 0.00001601
Iteration 13/1000 | Loss: 0.00001594
Iteration 14/1000 | Loss: 0.00001588
Iteration 15/1000 | Loss: 0.00001584
Iteration 16/1000 | Loss: 0.00001582
Iteration 17/1000 | Loss: 0.00001580
Iteration 18/1000 | Loss: 0.00001574
Iteration 19/1000 | Loss: 0.00001568
Iteration 20/1000 | Loss: 0.00001565
Iteration 21/1000 | Loss: 0.00001564
Iteration 22/1000 | Loss: 0.00001564
Iteration 23/1000 | Loss: 0.00001563
Iteration 24/1000 | Loss: 0.00001563
Iteration 25/1000 | Loss: 0.00001563
Iteration 26/1000 | Loss: 0.00001562
Iteration 27/1000 | Loss: 0.00001562
Iteration 28/1000 | Loss: 0.00001562
Iteration 29/1000 | Loss: 0.00001562
Iteration 30/1000 | Loss: 0.00001562
Iteration 31/1000 | Loss: 0.00001562
Iteration 32/1000 | Loss: 0.00001562
Iteration 33/1000 | Loss: 0.00001561
Iteration 34/1000 | Loss: 0.00001561
Iteration 35/1000 | Loss: 0.00001560
Iteration 36/1000 | Loss: 0.00001560
Iteration 37/1000 | Loss: 0.00001560
Iteration 38/1000 | Loss: 0.00001559
Iteration 39/1000 | Loss: 0.00001559
Iteration 40/1000 | Loss: 0.00001558
Iteration 41/1000 | Loss: 0.00001558
Iteration 42/1000 | Loss: 0.00001557
Iteration 43/1000 | Loss: 0.00001557
Iteration 44/1000 | Loss: 0.00001557
Iteration 45/1000 | Loss: 0.00001555
Iteration 46/1000 | Loss: 0.00001555
Iteration 47/1000 | Loss: 0.00001555
Iteration 48/1000 | Loss: 0.00001554
Iteration 49/1000 | Loss: 0.00001554
Iteration 50/1000 | Loss: 0.00001553
Iteration 51/1000 | Loss: 0.00001553
Iteration 52/1000 | Loss: 0.00001553
Iteration 53/1000 | Loss: 0.00001553
Iteration 54/1000 | Loss: 0.00001552
Iteration 55/1000 | Loss: 0.00001552
Iteration 56/1000 | Loss: 0.00001552
Iteration 57/1000 | Loss: 0.00001552
Iteration 58/1000 | Loss: 0.00001551
Iteration 59/1000 | Loss: 0.00001551
Iteration 60/1000 | Loss: 0.00001551
Iteration 61/1000 | Loss: 0.00001551
Iteration 62/1000 | Loss: 0.00001550
Iteration 63/1000 | Loss: 0.00001550
Iteration 64/1000 | Loss: 0.00001550
Iteration 65/1000 | Loss: 0.00001550
Iteration 66/1000 | Loss: 0.00001550
Iteration 67/1000 | Loss: 0.00001550
Iteration 68/1000 | Loss: 0.00001549
Iteration 69/1000 | Loss: 0.00001549
Iteration 70/1000 | Loss: 0.00001549
Iteration 71/1000 | Loss: 0.00001549
Iteration 72/1000 | Loss: 0.00001548
Iteration 73/1000 | Loss: 0.00001548
Iteration 74/1000 | Loss: 0.00001548
Iteration 75/1000 | Loss: 0.00001548
Iteration 76/1000 | Loss: 0.00001548
Iteration 77/1000 | Loss: 0.00001548
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001548
Iteration 80/1000 | Loss: 0.00001548
Iteration 81/1000 | Loss: 0.00001548
Iteration 82/1000 | Loss: 0.00001548
Iteration 83/1000 | Loss: 0.00001548
Iteration 84/1000 | Loss: 0.00001548
Iteration 85/1000 | Loss: 0.00001548
Iteration 86/1000 | Loss: 0.00001548
Iteration 87/1000 | Loss: 0.00001548
Iteration 88/1000 | Loss: 0.00001547
Iteration 89/1000 | Loss: 0.00001547
Iteration 90/1000 | Loss: 0.00001547
Iteration 91/1000 | Loss: 0.00001547
Iteration 92/1000 | Loss: 0.00001547
Iteration 93/1000 | Loss: 0.00001547
Iteration 94/1000 | Loss: 0.00001547
Iteration 95/1000 | Loss: 0.00001547
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001547
Iteration 99/1000 | Loss: 0.00001547
Iteration 100/1000 | Loss: 0.00001546
Iteration 101/1000 | Loss: 0.00001546
Iteration 102/1000 | Loss: 0.00001546
Iteration 103/1000 | Loss: 0.00001546
Iteration 104/1000 | Loss: 0.00001546
Iteration 105/1000 | Loss: 0.00001546
Iteration 106/1000 | Loss: 0.00001546
Iteration 107/1000 | Loss: 0.00001546
Iteration 108/1000 | Loss: 0.00001546
Iteration 109/1000 | Loss: 0.00001546
Iteration 110/1000 | Loss: 0.00001546
Iteration 111/1000 | Loss: 0.00001546
Iteration 112/1000 | Loss: 0.00001545
Iteration 113/1000 | Loss: 0.00001545
Iteration 114/1000 | Loss: 0.00001545
Iteration 115/1000 | Loss: 0.00001545
Iteration 116/1000 | Loss: 0.00001545
Iteration 117/1000 | Loss: 0.00001545
Iteration 118/1000 | Loss: 0.00001545
Iteration 119/1000 | Loss: 0.00001545
Iteration 120/1000 | Loss: 0.00001545
Iteration 121/1000 | Loss: 0.00001545
Iteration 122/1000 | Loss: 0.00001545
Iteration 123/1000 | Loss: 0.00001545
Iteration 124/1000 | Loss: 0.00001545
Iteration 125/1000 | Loss: 0.00001545
Iteration 126/1000 | Loss: 0.00001544
Iteration 127/1000 | Loss: 0.00001544
Iteration 128/1000 | Loss: 0.00001544
Iteration 129/1000 | Loss: 0.00001544
Iteration 130/1000 | Loss: 0.00001544
Iteration 131/1000 | Loss: 0.00001544
Iteration 132/1000 | Loss: 0.00001544
Iteration 133/1000 | Loss: 0.00001544
Iteration 134/1000 | Loss: 0.00001544
Iteration 135/1000 | Loss: 0.00001544
Iteration 136/1000 | Loss: 0.00001544
Iteration 137/1000 | Loss: 0.00001544
Iteration 138/1000 | Loss: 0.00001544
Iteration 139/1000 | Loss: 0.00001544
Iteration 140/1000 | Loss: 0.00001544
Iteration 141/1000 | Loss: 0.00001544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.5442083167727105e-05, 1.5442083167727105e-05, 1.5442083167727105e-05, 1.5442083167727105e-05, 1.5442083167727105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5442083167727105e-05

Optimization complete. Final v2v error: 3.300241470336914 mm

Highest mean error: 3.572011709213257 mm for frame 70

Lowest mean error: 2.9282422065734863 mm for frame 23

Saving results

Total time: 36.68520975112915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00715223
Iteration 2/25 | Loss: 0.00151972
Iteration 3/25 | Loss: 0.00133341
Iteration 4/25 | Loss: 0.00120805
Iteration 5/25 | Loss: 0.00119228
Iteration 6/25 | Loss: 0.00118425
Iteration 7/25 | Loss: 0.00118143
Iteration 8/25 | Loss: 0.00118112
Iteration 9/25 | Loss: 0.00118100
Iteration 10/25 | Loss: 0.00118095
Iteration 11/25 | Loss: 0.00118094
Iteration 12/25 | Loss: 0.00118094
Iteration 13/25 | Loss: 0.00118094
Iteration 14/25 | Loss: 0.00118094
Iteration 15/25 | Loss: 0.00118094
Iteration 16/25 | Loss: 0.00118094
Iteration 17/25 | Loss: 0.00118094
Iteration 18/25 | Loss: 0.00118094
Iteration 19/25 | Loss: 0.00118094
Iteration 20/25 | Loss: 0.00118093
Iteration 21/25 | Loss: 0.00118093
Iteration 22/25 | Loss: 0.00118093
Iteration 23/25 | Loss: 0.00118093
Iteration 24/25 | Loss: 0.00118093
Iteration 25/25 | Loss: 0.00118093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47706985
Iteration 2/25 | Loss: 0.00326168
Iteration 3/25 | Loss: 0.00326168
Iteration 4/25 | Loss: 0.00326168
Iteration 5/25 | Loss: 0.00326168
Iteration 6/25 | Loss: 0.00326168
Iteration 7/25 | Loss: 0.00326168
Iteration 8/25 | Loss: 0.00326168
Iteration 9/25 | Loss: 0.00326168
Iteration 10/25 | Loss: 0.00326168
Iteration 11/25 | Loss: 0.00326168
Iteration 12/25 | Loss: 0.00326168
Iteration 13/25 | Loss: 0.00326168
Iteration 14/25 | Loss: 0.00326168
Iteration 15/25 | Loss: 0.00326168
Iteration 16/25 | Loss: 0.00326168
Iteration 17/25 | Loss: 0.00326168
Iteration 18/25 | Loss: 0.00326168
Iteration 19/25 | Loss: 0.00326168
Iteration 20/25 | Loss: 0.00326168
Iteration 21/25 | Loss: 0.00326168
Iteration 22/25 | Loss: 0.00326168
Iteration 23/25 | Loss: 0.00326168
Iteration 24/25 | Loss: 0.00326168
Iteration 25/25 | Loss: 0.00326168

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00326168
Iteration 2/1000 | Loss: 0.00004560
Iteration 3/1000 | Loss: 0.00002399
Iteration 4/1000 | Loss: 0.00002166
Iteration 5/1000 | Loss: 0.00002047
Iteration 6/1000 | Loss: 0.00001962
Iteration 7/1000 | Loss: 0.00001891
Iteration 8/1000 | Loss: 0.00001840
Iteration 9/1000 | Loss: 0.00001805
Iteration 10/1000 | Loss: 0.00001776
Iteration 11/1000 | Loss: 0.00001770
Iteration 12/1000 | Loss: 0.00001764
Iteration 13/1000 | Loss: 0.00001747
Iteration 14/1000 | Loss: 0.00001730
Iteration 15/1000 | Loss: 0.00001728
Iteration 16/1000 | Loss: 0.00001723
Iteration 17/1000 | Loss: 0.00001723
Iteration 18/1000 | Loss: 0.00001723
Iteration 19/1000 | Loss: 0.00001723
Iteration 20/1000 | Loss: 0.00001722
Iteration 21/1000 | Loss: 0.00001722
Iteration 22/1000 | Loss: 0.00001722
Iteration 23/1000 | Loss: 0.00001722
Iteration 24/1000 | Loss: 0.00001722
Iteration 25/1000 | Loss: 0.00001722
Iteration 26/1000 | Loss: 0.00001722
Iteration 27/1000 | Loss: 0.00001721
Iteration 28/1000 | Loss: 0.00001719
Iteration 29/1000 | Loss: 0.00001718
Iteration 30/1000 | Loss: 0.00001717
Iteration 31/1000 | Loss: 0.00001717
Iteration 32/1000 | Loss: 0.00001717
Iteration 33/1000 | Loss: 0.00001717
Iteration 34/1000 | Loss: 0.00001717
Iteration 35/1000 | Loss: 0.00001717
Iteration 36/1000 | Loss: 0.00001716
Iteration 37/1000 | Loss: 0.00001716
Iteration 38/1000 | Loss: 0.00001716
Iteration 39/1000 | Loss: 0.00001716
Iteration 40/1000 | Loss: 0.00001716
Iteration 41/1000 | Loss: 0.00001716
Iteration 42/1000 | Loss: 0.00001716
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001716
Iteration 46/1000 | Loss: 0.00001715
Iteration 47/1000 | Loss: 0.00001714
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001713
Iteration 51/1000 | Loss: 0.00001713
Iteration 52/1000 | Loss: 0.00001713
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001712
Iteration 55/1000 | Loss: 0.00001712
Iteration 56/1000 | Loss: 0.00001712
Iteration 57/1000 | Loss: 0.00001712
Iteration 58/1000 | Loss: 0.00001711
Iteration 59/1000 | Loss: 0.00001711
Iteration 60/1000 | Loss: 0.00001710
Iteration 61/1000 | Loss: 0.00001710
Iteration 62/1000 | Loss: 0.00001710
Iteration 63/1000 | Loss: 0.00001710
Iteration 64/1000 | Loss: 0.00001709
Iteration 65/1000 | Loss: 0.00001709
Iteration 66/1000 | Loss: 0.00001709
Iteration 67/1000 | Loss: 0.00001709
Iteration 68/1000 | Loss: 0.00001709
Iteration 69/1000 | Loss: 0.00001709
Iteration 70/1000 | Loss: 0.00001709
Iteration 71/1000 | Loss: 0.00001709
Iteration 72/1000 | Loss: 0.00001708
Iteration 73/1000 | Loss: 0.00001708
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001708
Iteration 76/1000 | Loss: 0.00001708
Iteration 77/1000 | Loss: 0.00001708
Iteration 78/1000 | Loss: 0.00001708
Iteration 79/1000 | Loss: 0.00001707
Iteration 80/1000 | Loss: 0.00001707
Iteration 81/1000 | Loss: 0.00001707
Iteration 82/1000 | Loss: 0.00001707
Iteration 83/1000 | Loss: 0.00001706
Iteration 84/1000 | Loss: 0.00001706
Iteration 85/1000 | Loss: 0.00001706
Iteration 86/1000 | Loss: 0.00001706
Iteration 87/1000 | Loss: 0.00001706
Iteration 88/1000 | Loss: 0.00001706
Iteration 89/1000 | Loss: 0.00001706
Iteration 90/1000 | Loss: 0.00001705
Iteration 91/1000 | Loss: 0.00001705
Iteration 92/1000 | Loss: 0.00001705
Iteration 93/1000 | Loss: 0.00001705
Iteration 94/1000 | Loss: 0.00001705
Iteration 95/1000 | Loss: 0.00001705
Iteration 96/1000 | Loss: 0.00001705
Iteration 97/1000 | Loss: 0.00001705
Iteration 98/1000 | Loss: 0.00001705
Iteration 99/1000 | Loss: 0.00001705
Iteration 100/1000 | Loss: 0.00001705
Iteration 101/1000 | Loss: 0.00001705
Iteration 102/1000 | Loss: 0.00001704
Iteration 103/1000 | Loss: 0.00001704
Iteration 104/1000 | Loss: 0.00001704
Iteration 105/1000 | Loss: 0.00001704
Iteration 106/1000 | Loss: 0.00001704
Iteration 107/1000 | Loss: 0.00001704
Iteration 108/1000 | Loss: 0.00001704
Iteration 109/1000 | Loss: 0.00001704
Iteration 110/1000 | Loss: 0.00001704
Iteration 111/1000 | Loss: 0.00001704
Iteration 112/1000 | Loss: 0.00001704
Iteration 113/1000 | Loss: 0.00001704
Iteration 114/1000 | Loss: 0.00001704
Iteration 115/1000 | Loss: 0.00001704
Iteration 116/1000 | Loss: 0.00001704
Iteration 117/1000 | Loss: 0.00001704
Iteration 118/1000 | Loss: 0.00001704
Iteration 119/1000 | Loss: 0.00001704
Iteration 120/1000 | Loss: 0.00001703
Iteration 121/1000 | Loss: 0.00001703
Iteration 122/1000 | Loss: 0.00001703
Iteration 123/1000 | Loss: 0.00001703
Iteration 124/1000 | Loss: 0.00001703
Iteration 125/1000 | Loss: 0.00001703
Iteration 126/1000 | Loss: 0.00001703
Iteration 127/1000 | Loss: 0.00001703
Iteration 128/1000 | Loss: 0.00001703
Iteration 129/1000 | Loss: 0.00001703
Iteration 130/1000 | Loss: 0.00001703
Iteration 131/1000 | Loss: 0.00001703
Iteration 132/1000 | Loss: 0.00001703
Iteration 133/1000 | Loss: 0.00001703
Iteration 134/1000 | Loss: 0.00001703
Iteration 135/1000 | Loss: 0.00001703
Iteration 136/1000 | Loss: 0.00001703
Iteration 137/1000 | Loss: 0.00001703
Iteration 138/1000 | Loss: 0.00001703
Iteration 139/1000 | Loss: 0.00001703
Iteration 140/1000 | Loss: 0.00001703
Iteration 141/1000 | Loss: 0.00001703
Iteration 142/1000 | Loss: 0.00001703
Iteration 143/1000 | Loss: 0.00001703
Iteration 144/1000 | Loss: 0.00001703
Iteration 145/1000 | Loss: 0.00001703
Iteration 146/1000 | Loss: 0.00001703
Iteration 147/1000 | Loss: 0.00001703
Iteration 148/1000 | Loss: 0.00001703
Iteration 149/1000 | Loss: 0.00001703
Iteration 150/1000 | Loss: 0.00001703
Iteration 151/1000 | Loss: 0.00001703
Iteration 152/1000 | Loss: 0.00001703
Iteration 153/1000 | Loss: 0.00001703
Iteration 154/1000 | Loss: 0.00001703
Iteration 155/1000 | Loss: 0.00001703
Iteration 156/1000 | Loss: 0.00001703
Iteration 157/1000 | Loss: 0.00001703
Iteration 158/1000 | Loss: 0.00001703
Iteration 159/1000 | Loss: 0.00001703
Iteration 160/1000 | Loss: 0.00001703
Iteration 161/1000 | Loss: 0.00001703
Iteration 162/1000 | Loss: 0.00001703
Iteration 163/1000 | Loss: 0.00001703
Iteration 164/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.7028254660544917e-05, 1.7028254660544917e-05, 1.7028254660544917e-05, 1.7028254660544917e-05, 1.7028254660544917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7028254660544917e-05

Optimization complete. Final v2v error: 3.57959246635437 mm

Highest mean error: 3.9528887271881104 mm for frame 61

Lowest mean error: 3.0900449752807617 mm for frame 2

Saving results

Total time: 43.70671820640564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063857
Iteration 2/25 | Loss: 0.00303901
Iteration 3/25 | Loss: 0.00211052
Iteration 4/25 | Loss: 0.00200170
Iteration 5/25 | Loss: 0.00196333
Iteration 6/25 | Loss: 0.00194608
Iteration 7/25 | Loss: 0.00194434
Iteration 8/25 | Loss: 0.00193178
Iteration 9/25 | Loss: 0.00192766
Iteration 10/25 | Loss: 0.00192767
Iteration 11/25 | Loss: 0.00192274
Iteration 12/25 | Loss: 0.00192231
Iteration 13/25 | Loss: 0.00192200
Iteration 14/25 | Loss: 0.00192162
Iteration 15/25 | Loss: 0.00192078
Iteration 16/25 | Loss: 0.00192044
Iteration 17/25 | Loss: 0.00192033
Iteration 18/25 | Loss: 0.00192032
Iteration 19/25 | Loss: 0.00192032
Iteration 20/25 | Loss: 0.00192032
Iteration 21/25 | Loss: 0.00192032
Iteration 22/25 | Loss: 0.00192032
Iteration 23/25 | Loss: 0.00192032
Iteration 24/25 | Loss: 0.00192032
Iteration 25/25 | Loss: 0.00192032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09942317
Iteration 2/25 | Loss: 0.00954902
Iteration 3/25 | Loss: 0.00954902
Iteration 4/25 | Loss: 0.00954902
Iteration 5/25 | Loss: 0.00954902
Iteration 6/25 | Loss: 0.00954902
Iteration 7/25 | Loss: 0.00954902
Iteration 8/25 | Loss: 0.00954901
Iteration 9/25 | Loss: 0.00954901
Iteration 10/25 | Loss: 0.00954901
Iteration 11/25 | Loss: 0.00954902
Iteration 12/25 | Loss: 0.00954901
Iteration 13/25 | Loss: 0.00954901
Iteration 14/25 | Loss: 0.00954901
Iteration 15/25 | Loss: 0.00954901
Iteration 16/25 | Loss: 0.00954901
Iteration 17/25 | Loss: 0.00954901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.009549014270305634, 0.009549014270305634, 0.009549014270305634, 0.009549014270305634, 0.009549014270305634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009549014270305634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00954901
Iteration 2/1000 | Loss: 0.00091467
Iteration 3/1000 | Loss: 0.00065714
Iteration 4/1000 | Loss: 0.00056189
Iteration 5/1000 | Loss: 0.00049522
Iteration 6/1000 | Loss: 0.00044778
Iteration 7/1000 | Loss: 0.00040887
Iteration 8/1000 | Loss: 0.00037733
Iteration 9/1000 | Loss: 0.00035925
Iteration 10/1000 | Loss: 0.01782891
Iteration 11/1000 | Loss: 0.00643398
Iteration 12/1000 | Loss: 0.00039518
Iteration 13/1000 | Loss: 0.00025436
Iteration 14/1000 | Loss: 0.00018940
Iteration 15/1000 | Loss: 0.00014758
Iteration 16/1000 | Loss: 0.00009901
Iteration 17/1000 | Loss: 0.00007303
Iteration 18/1000 | Loss: 0.00005656
Iteration 19/1000 | Loss: 0.00004464
Iteration 20/1000 | Loss: 0.00003766
Iteration 21/1000 | Loss: 0.00003149
Iteration 22/1000 | Loss: 0.00002760
Iteration 23/1000 | Loss: 0.00002421
Iteration 24/1000 | Loss: 0.00002130
Iteration 25/1000 | Loss: 0.00001907
Iteration 26/1000 | Loss: 0.00001771
Iteration 27/1000 | Loss: 0.00001678
Iteration 28/1000 | Loss: 0.00001592
Iteration 29/1000 | Loss: 0.00001530
Iteration 30/1000 | Loss: 0.00001488
Iteration 31/1000 | Loss: 0.00001458
Iteration 32/1000 | Loss: 0.00001440
Iteration 33/1000 | Loss: 0.00001435
Iteration 34/1000 | Loss: 0.00001425
Iteration 35/1000 | Loss: 0.00001425
Iteration 36/1000 | Loss: 0.00001424
Iteration 37/1000 | Loss: 0.00001424
Iteration 38/1000 | Loss: 0.00001423
Iteration 39/1000 | Loss: 0.00001422
Iteration 40/1000 | Loss: 0.00001422
Iteration 41/1000 | Loss: 0.00001421
Iteration 42/1000 | Loss: 0.00001420
Iteration 43/1000 | Loss: 0.00001417
Iteration 44/1000 | Loss: 0.00001417
Iteration 45/1000 | Loss: 0.00001416
Iteration 46/1000 | Loss: 0.00001415
Iteration 47/1000 | Loss: 0.00001414
Iteration 48/1000 | Loss: 0.00001412
Iteration 49/1000 | Loss: 0.00001411
Iteration 50/1000 | Loss: 0.00001411
Iteration 51/1000 | Loss: 0.00001410
Iteration 52/1000 | Loss: 0.00001410
Iteration 53/1000 | Loss: 0.00001409
Iteration 54/1000 | Loss: 0.00001409
Iteration 55/1000 | Loss: 0.00001409
Iteration 56/1000 | Loss: 0.00001409
Iteration 57/1000 | Loss: 0.00001409
Iteration 58/1000 | Loss: 0.00001409
Iteration 59/1000 | Loss: 0.00001408
Iteration 60/1000 | Loss: 0.00001408
Iteration 61/1000 | Loss: 0.00001408
Iteration 62/1000 | Loss: 0.00001407
Iteration 63/1000 | Loss: 0.00001407
Iteration 64/1000 | Loss: 0.00001407
Iteration 65/1000 | Loss: 0.00001406
Iteration 66/1000 | Loss: 0.00001406
Iteration 67/1000 | Loss: 0.00001406
Iteration 68/1000 | Loss: 0.00001406
Iteration 69/1000 | Loss: 0.00001406
Iteration 70/1000 | Loss: 0.00001405
Iteration 71/1000 | Loss: 0.00001404
Iteration 72/1000 | Loss: 0.00001404
Iteration 73/1000 | Loss: 0.00001404
Iteration 74/1000 | Loss: 0.00001404
Iteration 75/1000 | Loss: 0.00001404
Iteration 76/1000 | Loss: 0.00001404
Iteration 77/1000 | Loss: 0.00001404
Iteration 78/1000 | Loss: 0.00001404
Iteration 79/1000 | Loss: 0.00001404
Iteration 80/1000 | Loss: 0.00001404
Iteration 81/1000 | Loss: 0.00001404
Iteration 82/1000 | Loss: 0.00001404
Iteration 83/1000 | Loss: 0.00001404
Iteration 84/1000 | Loss: 0.00001404
Iteration 85/1000 | Loss: 0.00001404
Iteration 86/1000 | Loss: 0.00001404
Iteration 87/1000 | Loss: 0.00001404
Iteration 88/1000 | Loss: 0.00001404
Iteration 89/1000 | Loss: 0.00001404
Iteration 90/1000 | Loss: 0.00001404
Iteration 91/1000 | Loss: 0.00001404
Iteration 92/1000 | Loss: 0.00001404
Iteration 93/1000 | Loss: 0.00001404
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001404
Iteration 96/1000 | Loss: 0.00001404
Iteration 97/1000 | Loss: 0.00001404
Iteration 98/1000 | Loss: 0.00001404
Iteration 99/1000 | Loss: 0.00001404
Iteration 100/1000 | Loss: 0.00001404
Iteration 101/1000 | Loss: 0.00001404
Iteration 102/1000 | Loss: 0.00001404
Iteration 103/1000 | Loss: 0.00001404
Iteration 104/1000 | Loss: 0.00001404
Iteration 105/1000 | Loss: 0.00001404
Iteration 106/1000 | Loss: 0.00001404
Iteration 107/1000 | Loss: 0.00001404
Iteration 108/1000 | Loss: 0.00001404
Iteration 109/1000 | Loss: 0.00001404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.4035228559805546e-05, 1.4035228559805546e-05, 1.4035228559805546e-05, 1.4035228559805546e-05, 1.4035228559805546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4035228559805546e-05

Optimization complete. Final v2v error: 3.1635243892669678 mm

Highest mean error: 3.745906352996826 mm for frame 57

Lowest mean error: 2.958216905593872 mm for frame 8

Saving results

Total time: 77.67413640022278
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00631658
Iteration 2/25 | Loss: 0.00146258
Iteration 3/25 | Loss: 0.00127439
Iteration 4/25 | Loss: 0.00125026
Iteration 5/25 | Loss: 0.00124435
Iteration 6/25 | Loss: 0.00124336
Iteration 7/25 | Loss: 0.00124336
Iteration 8/25 | Loss: 0.00124336
Iteration 9/25 | Loss: 0.00124336
Iteration 10/25 | Loss: 0.00124336
Iteration 11/25 | Loss: 0.00124336
Iteration 12/25 | Loss: 0.00124336
Iteration 13/25 | Loss: 0.00124336
Iteration 14/25 | Loss: 0.00124336
Iteration 15/25 | Loss: 0.00124336
Iteration 16/25 | Loss: 0.00124336
Iteration 17/25 | Loss: 0.00124336
Iteration 18/25 | Loss: 0.00124336
Iteration 19/25 | Loss: 0.00124336
Iteration 20/25 | Loss: 0.00124336
Iteration 21/25 | Loss: 0.00124336
Iteration 22/25 | Loss: 0.00124336
Iteration 23/25 | Loss: 0.00124336
Iteration 24/25 | Loss: 0.00124336
Iteration 25/25 | Loss: 0.00124336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.50775290
Iteration 2/25 | Loss: 0.00205752
Iteration 3/25 | Loss: 0.00205751
Iteration 4/25 | Loss: 0.00205751
Iteration 5/25 | Loss: 0.00205751
Iteration 6/25 | Loss: 0.00205751
Iteration 7/25 | Loss: 0.00205751
Iteration 8/25 | Loss: 0.00205751
Iteration 9/25 | Loss: 0.00205751
Iteration 10/25 | Loss: 0.00205751
Iteration 11/25 | Loss: 0.00205751
Iteration 12/25 | Loss: 0.00205751
Iteration 13/25 | Loss: 0.00205751
Iteration 14/25 | Loss: 0.00205751
Iteration 15/25 | Loss: 0.00205751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0020575106609612703, 0.0020575106609612703, 0.0020575106609612703, 0.0020575106609612703, 0.0020575106609612703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020575106609612703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205751
Iteration 2/1000 | Loss: 0.00004672
Iteration 3/1000 | Loss: 0.00002969
Iteration 4/1000 | Loss: 0.00002442
Iteration 5/1000 | Loss: 0.00002336
Iteration 6/1000 | Loss: 0.00002264
Iteration 7/1000 | Loss: 0.00002207
Iteration 8/1000 | Loss: 0.00002156
Iteration 9/1000 | Loss: 0.00002104
Iteration 10/1000 | Loss: 0.00002075
Iteration 11/1000 | Loss: 0.00002049
Iteration 12/1000 | Loss: 0.00002032
Iteration 13/1000 | Loss: 0.00002031
Iteration 14/1000 | Loss: 0.00002026
Iteration 15/1000 | Loss: 0.00002024
Iteration 16/1000 | Loss: 0.00002021
Iteration 17/1000 | Loss: 0.00002016
Iteration 18/1000 | Loss: 0.00002009
Iteration 19/1000 | Loss: 0.00002009
Iteration 20/1000 | Loss: 0.00002007
Iteration 21/1000 | Loss: 0.00002006
Iteration 22/1000 | Loss: 0.00002006
Iteration 23/1000 | Loss: 0.00002004
Iteration 24/1000 | Loss: 0.00002003
Iteration 25/1000 | Loss: 0.00002003
Iteration 26/1000 | Loss: 0.00002000
Iteration 27/1000 | Loss: 0.00001998
Iteration 28/1000 | Loss: 0.00001998
Iteration 29/1000 | Loss: 0.00001997
Iteration 30/1000 | Loss: 0.00001994
Iteration 31/1000 | Loss: 0.00001993
Iteration 32/1000 | Loss: 0.00001993
Iteration 33/1000 | Loss: 0.00001993
Iteration 34/1000 | Loss: 0.00001993
Iteration 35/1000 | Loss: 0.00001993
Iteration 36/1000 | Loss: 0.00001993
Iteration 37/1000 | Loss: 0.00001993
Iteration 38/1000 | Loss: 0.00001992
Iteration 39/1000 | Loss: 0.00001991
Iteration 40/1000 | Loss: 0.00001990
Iteration 41/1000 | Loss: 0.00001990
Iteration 42/1000 | Loss: 0.00001990
Iteration 43/1000 | Loss: 0.00001990
Iteration 44/1000 | Loss: 0.00001990
Iteration 45/1000 | Loss: 0.00001990
Iteration 46/1000 | Loss: 0.00001990
Iteration 47/1000 | Loss: 0.00001990
Iteration 48/1000 | Loss: 0.00001990
Iteration 49/1000 | Loss: 0.00001990
Iteration 50/1000 | Loss: 0.00001989
Iteration 51/1000 | Loss: 0.00001989
Iteration 52/1000 | Loss: 0.00001989
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001987
Iteration 58/1000 | Loss: 0.00001987
Iteration 59/1000 | Loss: 0.00001987
Iteration 60/1000 | Loss: 0.00001987
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00001987
Iteration 63/1000 | Loss: 0.00001987
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001986
Iteration 66/1000 | Loss: 0.00001986
Iteration 67/1000 | Loss: 0.00001986
Iteration 68/1000 | Loss: 0.00001986
Iteration 69/1000 | Loss: 0.00001986
Iteration 70/1000 | Loss: 0.00001986
Iteration 71/1000 | Loss: 0.00001986
Iteration 72/1000 | Loss: 0.00001986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [1.9862789486069232e-05, 1.9862789486069232e-05, 1.9862789486069232e-05, 1.9862789486069232e-05, 1.9862789486069232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9862789486069232e-05

Optimization complete. Final v2v error: 3.710611343383789 mm

Highest mean error: 4.0101518630981445 mm for frame 57

Lowest mean error: 3.2735595703125 mm for frame 216

Saving results

Total time: 37.1373028755188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009247
Iteration 2/25 | Loss: 0.00548491
Iteration 3/25 | Loss: 0.00349826
Iteration 4/25 | Loss: 0.00302414
Iteration 5/25 | Loss: 0.00251817
Iteration 6/25 | Loss: 0.00234149
Iteration 7/25 | Loss: 0.00206817
Iteration 8/25 | Loss: 0.00193773
Iteration 9/25 | Loss: 0.00191884
Iteration 10/25 | Loss: 0.00190449
Iteration 11/25 | Loss: 0.00190033
Iteration 12/25 | Loss: 0.00187330
Iteration 13/25 | Loss: 0.00184329
Iteration 14/25 | Loss: 0.00184043
Iteration 15/25 | Loss: 0.00183356
Iteration 16/25 | Loss: 0.00182287
Iteration 17/25 | Loss: 0.00182006
Iteration 18/25 | Loss: 0.00181924
Iteration 19/25 | Loss: 0.00182200
Iteration 20/25 | Loss: 0.00181898
Iteration 21/25 | Loss: 0.00181404
Iteration 22/25 | Loss: 0.00181325
Iteration 23/25 | Loss: 0.00181306
Iteration 24/25 | Loss: 0.00181301
Iteration 25/25 | Loss: 0.00181301

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09666526
Iteration 2/25 | Loss: 0.00823755
Iteration 3/25 | Loss: 0.00819066
Iteration 4/25 | Loss: 0.00819066
Iteration 5/25 | Loss: 0.00819066
Iteration 6/25 | Loss: 0.00819066
Iteration 7/25 | Loss: 0.00819066
Iteration 8/25 | Loss: 0.00819066
Iteration 9/25 | Loss: 0.00819066
Iteration 10/25 | Loss: 0.00819066
Iteration 11/25 | Loss: 0.00819066
Iteration 12/25 | Loss: 0.00819066
Iteration 13/25 | Loss: 0.00819066
Iteration 14/25 | Loss: 0.00819066
Iteration 15/25 | Loss: 0.00819066
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.008190655149519444, 0.008190655149519444, 0.008190655149519444, 0.008190655149519444, 0.008190655149519444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008190655149519444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00819066
Iteration 2/1000 | Loss: 0.00088128
Iteration 3/1000 | Loss: 0.00062569
Iteration 4/1000 | Loss: 0.00051076
Iteration 5/1000 | Loss: 0.00045377
Iteration 6/1000 | Loss: 0.00041194
Iteration 7/1000 | Loss: 0.00038285
Iteration 8/1000 | Loss: 0.00037016
Iteration 9/1000 | Loss: 0.00036189
Iteration 10/1000 | Loss: 0.00035351
Iteration 11/1000 | Loss: 0.00034806
Iteration 12/1000 | Loss: 0.00034356
Iteration 13/1000 | Loss: 0.00094686
Iteration 14/1000 | Loss: 0.01031554
Iteration 15/1000 | Loss: 0.01634468
Iteration 16/1000 | Loss: 0.00321004
Iteration 17/1000 | Loss: 0.00056911
Iteration 18/1000 | Loss: 0.00054115
Iteration 19/1000 | Loss: 0.00101754
Iteration 20/1000 | Loss: 0.00050136
Iteration 21/1000 | Loss: 0.00192535
Iteration 22/1000 | Loss: 0.00035767
Iteration 23/1000 | Loss: 0.00027140
Iteration 24/1000 | Loss: 0.00016805
Iteration 25/1000 | Loss: 0.00015406
Iteration 26/1000 | Loss: 0.00014313
Iteration 27/1000 | Loss: 0.00031500
Iteration 28/1000 | Loss: 0.00013622
Iteration 29/1000 | Loss: 0.00012976
Iteration 30/1000 | Loss: 0.00012584
Iteration 31/1000 | Loss: 0.00012300
Iteration 32/1000 | Loss: 0.00028403
Iteration 33/1000 | Loss: 0.00028584
Iteration 34/1000 | Loss: 0.00013006
Iteration 35/1000 | Loss: 0.00042889
Iteration 36/1000 | Loss: 0.00013608
Iteration 37/1000 | Loss: 0.00012182
Iteration 38/1000 | Loss: 0.00011546
Iteration 39/1000 | Loss: 0.00028606
Iteration 40/1000 | Loss: 0.00029484
Iteration 41/1000 | Loss: 0.00045249
Iteration 42/1000 | Loss: 0.00031383
Iteration 43/1000 | Loss: 0.00013184
Iteration 44/1000 | Loss: 0.00011868
Iteration 45/1000 | Loss: 0.00011211
Iteration 46/1000 | Loss: 0.00027805
Iteration 47/1000 | Loss: 0.00053959
Iteration 48/1000 | Loss: 0.00041208
Iteration 49/1000 | Loss: 0.00013811
Iteration 50/1000 | Loss: 0.00011475
Iteration 51/1000 | Loss: 0.00010898
Iteration 52/1000 | Loss: 0.00042360
Iteration 53/1000 | Loss: 0.00027726
Iteration 54/1000 | Loss: 0.00018685
Iteration 55/1000 | Loss: 0.00010279
Iteration 56/1000 | Loss: 0.00026170
Iteration 57/1000 | Loss: 0.00010433
Iteration 58/1000 | Loss: 0.00052712
Iteration 59/1000 | Loss: 0.00011105
Iteration 60/1000 | Loss: 0.00009727
Iteration 61/1000 | Loss: 0.00009311
Iteration 62/1000 | Loss: 0.00009185
Iteration 63/1000 | Loss: 0.00034274
Iteration 64/1000 | Loss: 0.00043753
Iteration 65/1000 | Loss: 0.00017503
Iteration 66/1000 | Loss: 0.00010743
Iteration 67/1000 | Loss: 0.00009898
Iteration 68/1000 | Loss: 0.00009691
Iteration 69/1000 | Loss: 0.00031681
Iteration 70/1000 | Loss: 0.00011145
Iteration 71/1000 | Loss: 0.00012813
Iteration 72/1000 | Loss: 0.00015085
Iteration 73/1000 | Loss: 0.00014803
Iteration 74/1000 | Loss: 0.00014510
Iteration 75/1000 | Loss: 0.00014211
Iteration 76/1000 | Loss: 0.00013815
Iteration 77/1000 | Loss: 0.00013219
Iteration 78/1000 | Loss: 0.00011840
Iteration 79/1000 | Loss: 0.00012131
Iteration 80/1000 | Loss: 0.00009696
Iteration 81/1000 | Loss: 0.00012972
Iteration 82/1000 | Loss: 0.00015118
Iteration 83/1000 | Loss: 0.00012240
Iteration 84/1000 | Loss: 0.00010251
Iteration 85/1000 | Loss: 0.00012167
Iteration 86/1000 | Loss: 0.00012976
Iteration 87/1000 | Loss: 0.00015261
Iteration 88/1000 | Loss: 0.00010617
Iteration 89/1000 | Loss: 0.00010741
Iteration 90/1000 | Loss: 0.00012093
Iteration 91/1000 | Loss: 0.00013881
Iteration 92/1000 | Loss: 0.00012929
Iteration 93/1000 | Loss: 0.00013746
Iteration 94/1000 | Loss: 0.00010106
Iteration 95/1000 | Loss: 0.00011106
Iteration 96/1000 | Loss: 0.00008987
Iteration 97/1000 | Loss: 0.00012907
Iteration 98/1000 | Loss: 0.00016596
Iteration 99/1000 | Loss: 0.00014328
Iteration 100/1000 | Loss: 0.00028430
Iteration 101/1000 | Loss: 0.00020033
Iteration 102/1000 | Loss: 0.00010881
Iteration 103/1000 | Loss: 0.00009521
Iteration 104/1000 | Loss: 0.00013144
Iteration 105/1000 | Loss: 0.00009045
Iteration 106/1000 | Loss: 0.00008780
Iteration 107/1000 | Loss: 0.00008585
Iteration 108/1000 | Loss: 0.00008420
Iteration 109/1000 | Loss: 0.00008330
Iteration 110/1000 | Loss: 0.00013712
Iteration 111/1000 | Loss: 0.00012417
Iteration 112/1000 | Loss: 0.00013545
Iteration 113/1000 | Loss: 0.00011990
Iteration 114/1000 | Loss: 0.00013966
Iteration 115/1000 | Loss: 0.00012767
Iteration 116/1000 | Loss: 0.00013757
Iteration 117/1000 | Loss: 0.00013246
Iteration 118/1000 | Loss: 0.00015643
Iteration 119/1000 | Loss: 0.00018902
Iteration 120/1000 | Loss: 0.00020480
Iteration 121/1000 | Loss: 0.00018388
Iteration 122/1000 | Loss: 0.00014504
Iteration 123/1000 | Loss: 0.00015451
Iteration 124/1000 | Loss: 0.00008795
Iteration 125/1000 | Loss: 0.00013356
Iteration 126/1000 | Loss: 0.00012374
Iteration 127/1000 | Loss: 0.00014195
Iteration 128/1000 | Loss: 0.00011193
Iteration 129/1000 | Loss: 0.00015530
Iteration 130/1000 | Loss: 0.00010075
Iteration 131/1000 | Loss: 0.00021836
Iteration 132/1000 | Loss: 0.00011020
Iteration 133/1000 | Loss: 0.00017330
Iteration 134/1000 | Loss: 0.00013178
Iteration 135/1000 | Loss: 0.00012513
Iteration 136/1000 | Loss: 0.00019716
Iteration 137/1000 | Loss: 0.00015439
Iteration 138/1000 | Loss: 0.00012413
Iteration 139/1000 | Loss: 0.00013711
Iteration 140/1000 | Loss: 0.00013367
Iteration 141/1000 | Loss: 0.00012911
Iteration 142/1000 | Loss: 0.00012474
Iteration 143/1000 | Loss: 0.00015686
Iteration 144/1000 | Loss: 0.00016282
Iteration 145/1000 | Loss: 0.00016002
Iteration 146/1000 | Loss: 0.00017247
Iteration 147/1000 | Loss: 0.00010025
Iteration 148/1000 | Loss: 0.00015241
Iteration 149/1000 | Loss: 0.00008746
Iteration 150/1000 | Loss: 0.00015178
Iteration 151/1000 | Loss: 0.00008701
Iteration 152/1000 | Loss: 0.00017524
Iteration 153/1000 | Loss: 0.00013538
Iteration 154/1000 | Loss: 0.00010599
Iteration 155/1000 | Loss: 0.00016242
Iteration 156/1000 | Loss: 0.00013250
Iteration 157/1000 | Loss: 0.00008708
Iteration 158/1000 | Loss: 0.00011847
Iteration 159/1000 | Loss: 0.00018584
Iteration 160/1000 | Loss: 0.00012860
Iteration 161/1000 | Loss: 0.00011709
Iteration 162/1000 | Loss: 0.00008857
Iteration 163/1000 | Loss: 0.00012048
Iteration 164/1000 | Loss: 0.00012240
Iteration 165/1000 | Loss: 0.00015801
Iteration 166/1000 | Loss: 0.00012799
Iteration 167/1000 | Loss: 0.00011156
Iteration 168/1000 | Loss: 0.00016925
Iteration 169/1000 | Loss: 0.00010421
Iteration 170/1000 | Loss: 0.00012677
Iteration 171/1000 | Loss: 0.00010560
Iteration 172/1000 | Loss: 0.00015086
Iteration 173/1000 | Loss: 0.00014482
Iteration 174/1000 | Loss: 0.00012848
Iteration 175/1000 | Loss: 0.00011901
Iteration 176/1000 | Loss: 0.00015429
Iteration 177/1000 | Loss: 0.00014199
Iteration 178/1000 | Loss: 0.00012490
Iteration 179/1000 | Loss: 0.00013319
Iteration 180/1000 | Loss: 0.00008470
Iteration 181/1000 | Loss: 0.00014480
Iteration 182/1000 | Loss: 0.00008969
Iteration 183/1000 | Loss: 0.00008569
Iteration 184/1000 | Loss: 0.00009930
Iteration 185/1000 | Loss: 0.00009011
Iteration 186/1000 | Loss: 0.00015562
Iteration 187/1000 | Loss: 0.00016712
Iteration 188/1000 | Loss: 0.00008758
Iteration 189/1000 | Loss: 0.00008484
Iteration 190/1000 | Loss: 0.00008322
Iteration 191/1000 | Loss: 0.00025572
Iteration 192/1000 | Loss: 0.00008672
Iteration 193/1000 | Loss: 0.00008278
Iteration 194/1000 | Loss: 0.00008149
Iteration 195/1000 | Loss: 0.00008026
Iteration 196/1000 | Loss: 0.00007952
Iteration 197/1000 | Loss: 0.00007899
Iteration 198/1000 | Loss: 0.00008653
Iteration 199/1000 | Loss: 0.00020062
Iteration 200/1000 | Loss: 0.00008328
Iteration 201/1000 | Loss: 0.00008020
Iteration 202/1000 | Loss: 0.00007864
Iteration 203/1000 | Loss: 0.00007751
Iteration 204/1000 | Loss: 0.00007688
Iteration 205/1000 | Loss: 0.00007653
Iteration 206/1000 | Loss: 0.00007629
Iteration 207/1000 | Loss: 0.00007622
Iteration 208/1000 | Loss: 0.00007605
Iteration 209/1000 | Loss: 0.00007602
Iteration 210/1000 | Loss: 0.00007595
Iteration 211/1000 | Loss: 0.00007595
Iteration 212/1000 | Loss: 0.00007595
Iteration 213/1000 | Loss: 0.00007595
Iteration 214/1000 | Loss: 0.00007595
Iteration 215/1000 | Loss: 0.00007595
Iteration 216/1000 | Loss: 0.00007595
Iteration 217/1000 | Loss: 0.00007594
Iteration 218/1000 | Loss: 0.00007594
Iteration 219/1000 | Loss: 0.00007594
Iteration 220/1000 | Loss: 0.00007594
Iteration 221/1000 | Loss: 0.00007592
Iteration 222/1000 | Loss: 0.00007587
Iteration 223/1000 | Loss: 0.00007587
Iteration 224/1000 | Loss: 0.00007587
Iteration 225/1000 | Loss: 0.00007587
Iteration 226/1000 | Loss: 0.00007587
Iteration 227/1000 | Loss: 0.00007587
Iteration 228/1000 | Loss: 0.00007587
Iteration 229/1000 | Loss: 0.00007586
Iteration 230/1000 | Loss: 0.00007586
Iteration 231/1000 | Loss: 0.00007586
Iteration 232/1000 | Loss: 0.00007586
Iteration 233/1000 | Loss: 0.00007586
Iteration 234/1000 | Loss: 0.00007584
Iteration 235/1000 | Loss: 0.00007584
Iteration 236/1000 | Loss: 0.00007584
Iteration 237/1000 | Loss: 0.00007584
Iteration 238/1000 | Loss: 0.00007584
Iteration 239/1000 | Loss: 0.00007584
Iteration 240/1000 | Loss: 0.00007583
Iteration 241/1000 | Loss: 0.00007583
Iteration 242/1000 | Loss: 0.00007583
Iteration 243/1000 | Loss: 0.00007583
Iteration 244/1000 | Loss: 0.00007583
Iteration 245/1000 | Loss: 0.00007583
Iteration 246/1000 | Loss: 0.00007583
Iteration 247/1000 | Loss: 0.00007582
Iteration 248/1000 | Loss: 0.00007579
Iteration 249/1000 | Loss: 0.00007579
Iteration 250/1000 | Loss: 0.00007579
Iteration 251/1000 | Loss: 0.00007579
Iteration 252/1000 | Loss: 0.00007578
Iteration 253/1000 | Loss: 0.00007578
Iteration 254/1000 | Loss: 0.00007577
Iteration 255/1000 | Loss: 0.00007577
Iteration 256/1000 | Loss: 0.00007577
Iteration 257/1000 | Loss: 0.00007576
Iteration 258/1000 | Loss: 0.00007576
Iteration 259/1000 | Loss: 0.00007576
Iteration 260/1000 | Loss: 0.00007576
Iteration 261/1000 | Loss: 0.00007576
Iteration 262/1000 | Loss: 0.00007576
Iteration 263/1000 | Loss: 0.00007576
Iteration 264/1000 | Loss: 0.00007576
Iteration 265/1000 | Loss: 0.00007575
Iteration 266/1000 | Loss: 0.00007575
Iteration 267/1000 | Loss: 0.00007575
Iteration 268/1000 | Loss: 0.00007575
Iteration 269/1000 | Loss: 0.00007575
Iteration 270/1000 | Loss: 0.00007575
Iteration 271/1000 | Loss: 0.00007575
Iteration 272/1000 | Loss: 0.00007575
Iteration 273/1000 | Loss: 0.00007575
Iteration 274/1000 | Loss: 0.00007574
Iteration 275/1000 | Loss: 0.00007574
Iteration 276/1000 | Loss: 0.00007574
Iteration 277/1000 | Loss: 0.00007574
Iteration 278/1000 | Loss: 0.00007574
Iteration 279/1000 | Loss: 0.00007574
Iteration 280/1000 | Loss: 0.00007573
Iteration 281/1000 | Loss: 0.00007573
Iteration 282/1000 | Loss: 0.00007573
Iteration 283/1000 | Loss: 0.00007573
Iteration 284/1000 | Loss: 0.00007573
Iteration 285/1000 | Loss: 0.00007573
Iteration 286/1000 | Loss: 0.00007573
Iteration 287/1000 | Loss: 0.00007573
Iteration 288/1000 | Loss: 0.00007573
Iteration 289/1000 | Loss: 0.00007573
Iteration 290/1000 | Loss: 0.00007573
Iteration 291/1000 | Loss: 0.00007573
Iteration 292/1000 | Loss: 0.00007572
Iteration 293/1000 | Loss: 0.00007572
Iteration 294/1000 | Loss: 0.00007572
Iteration 295/1000 | Loss: 0.00007572
Iteration 296/1000 | Loss: 0.00007572
Iteration 297/1000 | Loss: 0.00007572
Iteration 298/1000 | Loss: 0.00007572
Iteration 299/1000 | Loss: 0.00007572
Iteration 300/1000 | Loss: 0.00007572
Iteration 301/1000 | Loss: 0.00007572
Iteration 302/1000 | Loss: 0.00007572
Iteration 303/1000 | Loss: 0.00007572
Iteration 304/1000 | Loss: 0.00007572
Iteration 305/1000 | Loss: 0.00007572
Iteration 306/1000 | Loss: 0.00007572
Iteration 307/1000 | Loss: 0.00007571
Iteration 308/1000 | Loss: 0.00007571
Iteration 309/1000 | Loss: 0.00007571
Iteration 310/1000 | Loss: 0.00007571
Iteration 311/1000 | Loss: 0.00007571
Iteration 312/1000 | Loss: 0.00007571
Iteration 313/1000 | Loss: 0.00007571
Iteration 314/1000 | Loss: 0.00007570
Iteration 315/1000 | Loss: 0.00007570
Iteration 316/1000 | Loss: 0.00007570
Iteration 317/1000 | Loss: 0.00007570
Iteration 318/1000 | Loss: 0.00007570
Iteration 319/1000 | Loss: 0.00007569
Iteration 320/1000 | Loss: 0.00007569
Iteration 321/1000 | Loss: 0.00007569
Iteration 322/1000 | Loss: 0.00007569
Iteration 323/1000 | Loss: 0.00007569
Iteration 324/1000 | Loss: 0.00007569
Iteration 325/1000 | Loss: 0.00007569
Iteration 326/1000 | Loss: 0.00007569
Iteration 327/1000 | Loss: 0.00007569
Iteration 328/1000 | Loss: 0.00007569
Iteration 329/1000 | Loss: 0.00007569
Iteration 330/1000 | Loss: 0.00007569
Iteration 331/1000 | Loss: 0.00007569
Iteration 332/1000 | Loss: 0.00007568
Iteration 333/1000 | Loss: 0.00007568
Iteration 334/1000 | Loss: 0.00007568
Iteration 335/1000 | Loss: 0.00007568
Iteration 336/1000 | Loss: 0.00007568
Iteration 337/1000 | Loss: 0.00007568
Iteration 338/1000 | Loss: 0.00007568
Iteration 339/1000 | Loss: 0.00007568
Iteration 340/1000 | Loss: 0.00007568
Iteration 341/1000 | Loss: 0.00007568
Iteration 342/1000 | Loss: 0.00007568
Iteration 343/1000 | Loss: 0.00007568
Iteration 344/1000 | Loss: 0.00007568
Iteration 345/1000 | Loss: 0.00007568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 345. Stopping optimization.
Last 5 losses: [7.567957800347358e-05, 7.567957800347358e-05, 7.567957800347358e-05, 7.567957800347358e-05, 7.567957800347358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.567957800347358e-05

Optimization complete. Final v2v error: 4.7717485427856445 mm

Highest mean error: 13.156161308288574 mm for frame 160

Lowest mean error: 3.354172468185425 mm for frame 4

Saving results

Total time: 387.4786021709442
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048967
Iteration 2/25 | Loss: 0.00257780
Iteration 3/25 | Loss: 0.00195435
Iteration 4/25 | Loss: 0.00175247
Iteration 5/25 | Loss: 0.00152877
Iteration 6/25 | Loss: 0.00150331
Iteration 7/25 | Loss: 0.00145843
Iteration 8/25 | Loss: 0.00134818
Iteration 9/25 | Loss: 0.00130723
Iteration 10/25 | Loss: 0.00128200
Iteration 11/25 | Loss: 0.00127582
Iteration 12/25 | Loss: 0.00126333
Iteration 13/25 | Loss: 0.00125752
Iteration 14/25 | Loss: 0.00125649
Iteration 15/25 | Loss: 0.00125461
Iteration 16/25 | Loss: 0.00125309
Iteration 17/25 | Loss: 0.00125232
Iteration 18/25 | Loss: 0.00125189
Iteration 19/25 | Loss: 0.00125399
Iteration 20/25 | Loss: 0.00125233
Iteration 21/25 | Loss: 0.00124853
Iteration 22/25 | Loss: 0.00124452
Iteration 23/25 | Loss: 0.00124362
Iteration 24/25 | Loss: 0.00124348
Iteration 25/25 | Loss: 0.00124347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15453088
Iteration 2/25 | Loss: 0.00341006
Iteration 3/25 | Loss: 0.00341006
Iteration 4/25 | Loss: 0.00341006
Iteration 5/25 | Loss: 0.00341006
Iteration 6/25 | Loss: 0.00341006
Iteration 7/25 | Loss: 0.00341006
Iteration 8/25 | Loss: 0.00341006
Iteration 9/25 | Loss: 0.00341005
Iteration 10/25 | Loss: 0.00341005
Iteration 11/25 | Loss: 0.00341005
Iteration 12/25 | Loss: 0.00341005
Iteration 13/25 | Loss: 0.00341005
Iteration 14/25 | Loss: 0.00341005
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0034100546035915613, 0.0034100546035915613, 0.0034100546035915613, 0.0034100546035915613, 0.0034100546035915613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034100546035915613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00341005
Iteration 2/1000 | Loss: 0.00007441
Iteration 3/1000 | Loss: 0.00004936
Iteration 4/1000 | Loss: 0.00003850
Iteration 5/1000 | Loss: 0.00003623
Iteration 6/1000 | Loss: 0.00003426
Iteration 7/1000 | Loss: 0.00003306
Iteration 8/1000 | Loss: 0.00003173
Iteration 9/1000 | Loss: 0.00003096
Iteration 10/1000 | Loss: 0.00003046
Iteration 11/1000 | Loss: 0.00003016
Iteration 12/1000 | Loss: 0.00002990
Iteration 13/1000 | Loss: 0.00002961
Iteration 14/1000 | Loss: 0.00002944
Iteration 15/1000 | Loss: 0.00002931
Iteration 16/1000 | Loss: 0.00002929
Iteration 17/1000 | Loss: 0.00002924
Iteration 18/1000 | Loss: 0.00002924
Iteration 19/1000 | Loss: 0.00002923
Iteration 20/1000 | Loss: 0.00002922
Iteration 21/1000 | Loss: 0.00002921
Iteration 22/1000 | Loss: 0.00002919
Iteration 23/1000 | Loss: 0.00002919
Iteration 24/1000 | Loss: 0.00002918
Iteration 25/1000 | Loss: 0.00002918
Iteration 26/1000 | Loss: 0.00002917
Iteration 27/1000 | Loss: 0.00002916
Iteration 28/1000 | Loss: 0.00002916
Iteration 29/1000 | Loss: 0.00002913
Iteration 30/1000 | Loss: 0.00002913
Iteration 31/1000 | Loss: 0.00002913
Iteration 32/1000 | Loss: 0.00002913
Iteration 33/1000 | Loss: 0.00002912
Iteration 34/1000 | Loss: 0.00002911
Iteration 35/1000 | Loss: 0.00002910
Iteration 36/1000 | Loss: 0.00002910
Iteration 37/1000 | Loss: 0.00002910
Iteration 38/1000 | Loss: 0.00002910
Iteration 39/1000 | Loss: 0.00002909
Iteration 40/1000 | Loss: 0.00002909
Iteration 41/1000 | Loss: 0.00002909
Iteration 42/1000 | Loss: 0.00002906
Iteration 43/1000 | Loss: 0.00002906
Iteration 44/1000 | Loss: 0.00002906
Iteration 45/1000 | Loss: 0.00002906
Iteration 46/1000 | Loss: 0.00002906
Iteration 47/1000 | Loss: 0.00002906
Iteration 48/1000 | Loss: 0.00002906
Iteration 49/1000 | Loss: 0.00002905
Iteration 50/1000 | Loss: 0.00002905
Iteration 51/1000 | Loss: 0.00002904
Iteration 52/1000 | Loss: 0.00002904
Iteration 53/1000 | Loss: 0.00002903
Iteration 54/1000 | Loss: 0.00002903
Iteration 55/1000 | Loss: 0.00002903
Iteration 56/1000 | Loss: 0.00002903
Iteration 57/1000 | Loss: 0.00002903
Iteration 58/1000 | Loss: 0.00002902
Iteration 59/1000 | Loss: 0.00002902
Iteration 60/1000 | Loss: 0.00002902
Iteration 61/1000 | Loss: 0.00002902
Iteration 62/1000 | Loss: 0.00002901
Iteration 63/1000 | Loss: 0.00002900
Iteration 64/1000 | Loss: 0.00002900
Iteration 65/1000 | Loss: 0.00002900
Iteration 66/1000 | Loss: 0.00002900
Iteration 67/1000 | Loss: 0.00002900
Iteration 68/1000 | Loss: 0.00002900
Iteration 69/1000 | Loss: 0.00002900
Iteration 70/1000 | Loss: 0.00002899
Iteration 71/1000 | Loss: 0.00002899
Iteration 72/1000 | Loss: 0.00002899
Iteration 73/1000 | Loss: 0.00002898
Iteration 74/1000 | Loss: 0.00002898
Iteration 75/1000 | Loss: 0.00002897
Iteration 76/1000 | Loss: 0.00002897
Iteration 77/1000 | Loss: 0.00002897
Iteration 78/1000 | Loss: 0.00002897
Iteration 79/1000 | Loss: 0.00002897
Iteration 80/1000 | Loss: 0.00002896
Iteration 81/1000 | Loss: 0.00002896
Iteration 82/1000 | Loss: 0.00002896
Iteration 83/1000 | Loss: 0.00002896
Iteration 84/1000 | Loss: 0.00002896
Iteration 85/1000 | Loss: 0.00002896
Iteration 86/1000 | Loss: 0.00002896
Iteration 87/1000 | Loss: 0.00002896
Iteration 88/1000 | Loss: 0.00002896
Iteration 89/1000 | Loss: 0.00002896
Iteration 90/1000 | Loss: 0.00002896
Iteration 91/1000 | Loss: 0.00002896
Iteration 92/1000 | Loss: 0.00002895
Iteration 93/1000 | Loss: 0.00002895
Iteration 94/1000 | Loss: 0.00002894
Iteration 95/1000 | Loss: 0.00002894
Iteration 96/1000 | Loss: 0.00002893
Iteration 97/1000 | Loss: 0.00002893
Iteration 98/1000 | Loss: 0.00002893
Iteration 99/1000 | Loss: 0.00002893
Iteration 100/1000 | Loss: 0.00002893
Iteration 101/1000 | Loss: 0.00002893
Iteration 102/1000 | Loss: 0.00002892
Iteration 103/1000 | Loss: 0.00002892
Iteration 104/1000 | Loss: 0.00002892
Iteration 105/1000 | Loss: 0.00002892
Iteration 106/1000 | Loss: 0.00002892
Iteration 107/1000 | Loss: 0.00002892
Iteration 108/1000 | Loss: 0.00002892
Iteration 109/1000 | Loss: 0.00002892
Iteration 110/1000 | Loss: 0.00002892
Iteration 111/1000 | Loss: 0.00002892
Iteration 112/1000 | Loss: 0.00002892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [2.8922571800649166e-05, 2.8922571800649166e-05, 2.8922571800649166e-05, 2.8922571800649166e-05, 2.8922571800649166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8922571800649166e-05

Optimization complete. Final v2v error: 4.138635635375977 mm

Highest mean error: 13.6327543258667 mm for frame 110

Lowest mean error: 3.2658159732818604 mm for frame 239

Saving results

Total time: 80.390784740448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476117
Iteration 2/25 | Loss: 0.00136771
Iteration 3/25 | Loss: 0.00120165
Iteration 4/25 | Loss: 0.00118453
Iteration 5/25 | Loss: 0.00118049
Iteration 6/25 | Loss: 0.00118022
Iteration 7/25 | Loss: 0.00118022
Iteration 8/25 | Loss: 0.00118022
Iteration 9/25 | Loss: 0.00118022
Iteration 10/25 | Loss: 0.00118022
Iteration 11/25 | Loss: 0.00118022
Iteration 12/25 | Loss: 0.00118022
Iteration 13/25 | Loss: 0.00118022
Iteration 14/25 | Loss: 0.00118022
Iteration 15/25 | Loss: 0.00118022
Iteration 16/25 | Loss: 0.00118022
Iteration 17/25 | Loss: 0.00118022
Iteration 18/25 | Loss: 0.00118022
Iteration 19/25 | Loss: 0.00118022
Iteration 20/25 | Loss: 0.00118022
Iteration 21/25 | Loss: 0.00118022
Iteration 22/25 | Loss: 0.00118022
Iteration 23/25 | Loss: 0.00118022
Iteration 24/25 | Loss: 0.00118022
Iteration 25/25 | Loss: 0.00118022

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12150478
Iteration 2/25 | Loss: 0.00298339
Iteration 3/25 | Loss: 0.00298338
Iteration 4/25 | Loss: 0.00298338
Iteration 5/25 | Loss: 0.00298338
Iteration 6/25 | Loss: 0.00298338
Iteration 7/25 | Loss: 0.00298337
Iteration 8/25 | Loss: 0.00298337
Iteration 9/25 | Loss: 0.00298337
Iteration 10/25 | Loss: 0.00298337
Iteration 11/25 | Loss: 0.00298337
Iteration 12/25 | Loss: 0.00298337
Iteration 13/25 | Loss: 0.00298337
Iteration 14/25 | Loss: 0.00298337
Iteration 15/25 | Loss: 0.00298337
Iteration 16/25 | Loss: 0.00298337
Iteration 17/25 | Loss: 0.00298337
Iteration 18/25 | Loss: 0.00298337
Iteration 19/25 | Loss: 0.00298337
Iteration 20/25 | Loss: 0.00298337
Iteration 21/25 | Loss: 0.00298337
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002983373822644353, 0.002983373822644353, 0.002983373822644353, 0.002983373822644353, 0.002983373822644353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002983373822644353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00298337
Iteration 2/1000 | Loss: 0.00004927
Iteration 3/1000 | Loss: 0.00002922
Iteration 4/1000 | Loss: 0.00002292
Iteration 5/1000 | Loss: 0.00002001
Iteration 6/1000 | Loss: 0.00001861
Iteration 7/1000 | Loss: 0.00001786
Iteration 8/1000 | Loss: 0.00001710
Iteration 9/1000 | Loss: 0.00001662
Iteration 10/1000 | Loss: 0.00001619
Iteration 11/1000 | Loss: 0.00001586
Iteration 12/1000 | Loss: 0.00001558
Iteration 13/1000 | Loss: 0.00001544
Iteration 14/1000 | Loss: 0.00001539
Iteration 15/1000 | Loss: 0.00001537
Iteration 16/1000 | Loss: 0.00001524
Iteration 17/1000 | Loss: 0.00001516
Iteration 18/1000 | Loss: 0.00001511
Iteration 19/1000 | Loss: 0.00001511
Iteration 20/1000 | Loss: 0.00001510
Iteration 21/1000 | Loss: 0.00001508
Iteration 22/1000 | Loss: 0.00001505
Iteration 23/1000 | Loss: 0.00001504
Iteration 24/1000 | Loss: 0.00001503
Iteration 25/1000 | Loss: 0.00001502
Iteration 26/1000 | Loss: 0.00001502
Iteration 27/1000 | Loss: 0.00001494
Iteration 28/1000 | Loss: 0.00001492
Iteration 29/1000 | Loss: 0.00001491
Iteration 30/1000 | Loss: 0.00001491
Iteration 31/1000 | Loss: 0.00001490
Iteration 32/1000 | Loss: 0.00001490
Iteration 33/1000 | Loss: 0.00001490
Iteration 34/1000 | Loss: 0.00001489
Iteration 35/1000 | Loss: 0.00001489
Iteration 36/1000 | Loss: 0.00001488
Iteration 37/1000 | Loss: 0.00001486
Iteration 38/1000 | Loss: 0.00001486
Iteration 39/1000 | Loss: 0.00001486
Iteration 40/1000 | Loss: 0.00001485
Iteration 41/1000 | Loss: 0.00001485
Iteration 42/1000 | Loss: 0.00001485
Iteration 43/1000 | Loss: 0.00001485
Iteration 44/1000 | Loss: 0.00001484
Iteration 45/1000 | Loss: 0.00001484
Iteration 46/1000 | Loss: 0.00001484
Iteration 47/1000 | Loss: 0.00001484
Iteration 48/1000 | Loss: 0.00001484
Iteration 49/1000 | Loss: 0.00001484
Iteration 50/1000 | Loss: 0.00001483
Iteration 51/1000 | Loss: 0.00001483
Iteration 52/1000 | Loss: 0.00001482
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001482
Iteration 55/1000 | Loss: 0.00001482
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001481
Iteration 59/1000 | Loss: 0.00001480
Iteration 60/1000 | Loss: 0.00001479
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001478
Iteration 65/1000 | Loss: 0.00001478
Iteration 66/1000 | Loss: 0.00001478
Iteration 67/1000 | Loss: 0.00001477
Iteration 68/1000 | Loss: 0.00001477
Iteration 69/1000 | Loss: 0.00001477
Iteration 70/1000 | Loss: 0.00001477
Iteration 71/1000 | Loss: 0.00001477
Iteration 72/1000 | Loss: 0.00001477
Iteration 73/1000 | Loss: 0.00001477
Iteration 74/1000 | Loss: 0.00001477
Iteration 75/1000 | Loss: 0.00001477
Iteration 76/1000 | Loss: 0.00001476
Iteration 77/1000 | Loss: 0.00001476
Iteration 78/1000 | Loss: 0.00001475
Iteration 79/1000 | Loss: 0.00001475
Iteration 80/1000 | Loss: 0.00001475
Iteration 81/1000 | Loss: 0.00001475
Iteration 82/1000 | Loss: 0.00001475
Iteration 83/1000 | Loss: 0.00001475
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001474
Iteration 86/1000 | Loss: 0.00001474
Iteration 87/1000 | Loss: 0.00001474
Iteration 88/1000 | Loss: 0.00001474
Iteration 89/1000 | Loss: 0.00001473
Iteration 90/1000 | Loss: 0.00001473
Iteration 91/1000 | Loss: 0.00001473
Iteration 92/1000 | Loss: 0.00001473
Iteration 93/1000 | Loss: 0.00001472
Iteration 94/1000 | Loss: 0.00001472
Iteration 95/1000 | Loss: 0.00001472
Iteration 96/1000 | Loss: 0.00001471
Iteration 97/1000 | Loss: 0.00001471
Iteration 98/1000 | Loss: 0.00001471
Iteration 99/1000 | Loss: 0.00001470
Iteration 100/1000 | Loss: 0.00001470
Iteration 101/1000 | Loss: 0.00001470
Iteration 102/1000 | Loss: 0.00001470
Iteration 103/1000 | Loss: 0.00001470
Iteration 104/1000 | Loss: 0.00001469
Iteration 105/1000 | Loss: 0.00001469
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001468
Iteration 108/1000 | Loss: 0.00001468
Iteration 109/1000 | Loss: 0.00001468
Iteration 110/1000 | Loss: 0.00001468
Iteration 111/1000 | Loss: 0.00001468
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001468
Iteration 117/1000 | Loss: 0.00001468
Iteration 118/1000 | Loss: 0.00001467
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001467
Iteration 121/1000 | Loss: 0.00001467
Iteration 122/1000 | Loss: 0.00001467
Iteration 123/1000 | Loss: 0.00001467
Iteration 124/1000 | Loss: 0.00001467
Iteration 125/1000 | Loss: 0.00001467
Iteration 126/1000 | Loss: 0.00001467
Iteration 127/1000 | Loss: 0.00001467
Iteration 128/1000 | Loss: 0.00001467
Iteration 129/1000 | Loss: 0.00001467
Iteration 130/1000 | Loss: 0.00001467
Iteration 131/1000 | Loss: 0.00001467
Iteration 132/1000 | Loss: 0.00001467
Iteration 133/1000 | Loss: 0.00001467
Iteration 134/1000 | Loss: 0.00001467
Iteration 135/1000 | Loss: 0.00001467
Iteration 136/1000 | Loss: 0.00001467
Iteration 137/1000 | Loss: 0.00001467
Iteration 138/1000 | Loss: 0.00001467
Iteration 139/1000 | Loss: 0.00001467
Iteration 140/1000 | Loss: 0.00001467
Iteration 141/1000 | Loss: 0.00001467
Iteration 142/1000 | Loss: 0.00001467
Iteration 143/1000 | Loss: 0.00001467
Iteration 144/1000 | Loss: 0.00001467
Iteration 145/1000 | Loss: 0.00001467
Iteration 146/1000 | Loss: 0.00001467
Iteration 147/1000 | Loss: 0.00001467
Iteration 148/1000 | Loss: 0.00001467
Iteration 149/1000 | Loss: 0.00001467
Iteration 150/1000 | Loss: 0.00001467
Iteration 151/1000 | Loss: 0.00001467
Iteration 152/1000 | Loss: 0.00001467
Iteration 153/1000 | Loss: 0.00001467
Iteration 154/1000 | Loss: 0.00001467
Iteration 155/1000 | Loss: 0.00001467
Iteration 156/1000 | Loss: 0.00001467
Iteration 157/1000 | Loss: 0.00001467
Iteration 158/1000 | Loss: 0.00001467
Iteration 159/1000 | Loss: 0.00001467
Iteration 160/1000 | Loss: 0.00001467
Iteration 161/1000 | Loss: 0.00001467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.467461152060423e-05, 1.467461152060423e-05, 1.467461152060423e-05, 1.467461152060423e-05, 1.467461152060423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.467461152060423e-05

Optimization complete. Final v2v error: 3.3124091625213623 mm

Highest mean error: 3.6472861766815186 mm for frame 63

Lowest mean error: 2.9123618602752686 mm for frame 13

Saving results

Total time: 45.097060203552246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00582651
Iteration 2/25 | Loss: 0.00118826
Iteration 3/25 | Loss: 0.00111992
Iteration 4/25 | Loss: 0.00111243
Iteration 5/25 | Loss: 0.00111012
Iteration 6/25 | Loss: 0.00110952
Iteration 7/25 | Loss: 0.00110952
Iteration 8/25 | Loss: 0.00110952
Iteration 9/25 | Loss: 0.00110952
Iteration 10/25 | Loss: 0.00110952
Iteration 11/25 | Loss: 0.00110952
Iteration 12/25 | Loss: 0.00110952
Iteration 13/25 | Loss: 0.00110952
Iteration 14/25 | Loss: 0.00110952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001109523349441588, 0.001109523349441588, 0.001109523349441588, 0.001109523349441588, 0.001109523349441588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001109523349441588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63839173
Iteration 2/25 | Loss: 0.00312727
Iteration 3/25 | Loss: 0.00312727
Iteration 4/25 | Loss: 0.00312727
Iteration 5/25 | Loss: 0.00312726
Iteration 6/25 | Loss: 0.00312726
Iteration 7/25 | Loss: 0.00312726
Iteration 8/25 | Loss: 0.00312726
Iteration 9/25 | Loss: 0.00312726
Iteration 10/25 | Loss: 0.00312726
Iteration 11/25 | Loss: 0.00312726
Iteration 12/25 | Loss: 0.00312726
Iteration 13/25 | Loss: 0.00312726
Iteration 14/25 | Loss: 0.00312726
Iteration 15/25 | Loss: 0.00312726
Iteration 16/25 | Loss: 0.00312726
Iteration 17/25 | Loss: 0.00312726
Iteration 18/25 | Loss: 0.00312726
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0031272629275918007, 0.0031272629275918007, 0.0031272629275918007, 0.0031272629275918007, 0.0031272629275918007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031272629275918007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00312726
Iteration 2/1000 | Loss: 0.00002655
Iteration 3/1000 | Loss: 0.00001789
Iteration 4/1000 | Loss: 0.00001633
Iteration 5/1000 | Loss: 0.00001497
Iteration 6/1000 | Loss: 0.00001408
Iteration 7/1000 | Loss: 0.00001361
Iteration 8/1000 | Loss: 0.00001323
Iteration 9/1000 | Loss: 0.00001301
Iteration 10/1000 | Loss: 0.00001292
Iteration 11/1000 | Loss: 0.00001289
Iteration 12/1000 | Loss: 0.00001282
Iteration 13/1000 | Loss: 0.00001281
Iteration 14/1000 | Loss: 0.00001281
Iteration 15/1000 | Loss: 0.00001280
Iteration 16/1000 | Loss: 0.00001277
Iteration 17/1000 | Loss: 0.00001271
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001260
Iteration 20/1000 | Loss: 0.00001259
Iteration 21/1000 | Loss: 0.00001250
Iteration 22/1000 | Loss: 0.00001249
Iteration 23/1000 | Loss: 0.00001249
Iteration 24/1000 | Loss: 0.00001248
Iteration 25/1000 | Loss: 0.00001247
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001244
Iteration 28/1000 | Loss: 0.00001243
Iteration 29/1000 | Loss: 0.00001242
Iteration 30/1000 | Loss: 0.00001242
Iteration 31/1000 | Loss: 0.00001241
Iteration 32/1000 | Loss: 0.00001239
Iteration 33/1000 | Loss: 0.00001237
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001235
Iteration 37/1000 | Loss: 0.00001234
Iteration 38/1000 | Loss: 0.00001234
Iteration 39/1000 | Loss: 0.00001233
Iteration 40/1000 | Loss: 0.00001233
Iteration 41/1000 | Loss: 0.00001233
Iteration 42/1000 | Loss: 0.00001233
Iteration 43/1000 | Loss: 0.00001233
Iteration 44/1000 | Loss: 0.00001232
Iteration 45/1000 | Loss: 0.00001232
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001231
Iteration 48/1000 | Loss: 0.00001231
Iteration 49/1000 | Loss: 0.00001231
Iteration 50/1000 | Loss: 0.00001230
Iteration 51/1000 | Loss: 0.00001230
Iteration 52/1000 | Loss: 0.00001230
Iteration 53/1000 | Loss: 0.00001229
Iteration 54/1000 | Loss: 0.00001229
Iteration 55/1000 | Loss: 0.00001229
Iteration 56/1000 | Loss: 0.00001229
Iteration 57/1000 | Loss: 0.00001228
Iteration 58/1000 | Loss: 0.00001228
Iteration 59/1000 | Loss: 0.00001228
Iteration 60/1000 | Loss: 0.00001228
Iteration 61/1000 | Loss: 0.00001228
Iteration 62/1000 | Loss: 0.00001228
Iteration 63/1000 | Loss: 0.00001228
Iteration 64/1000 | Loss: 0.00001228
Iteration 65/1000 | Loss: 0.00001228
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001228
Iteration 68/1000 | Loss: 0.00001228
Iteration 69/1000 | Loss: 0.00001228
Iteration 70/1000 | Loss: 0.00001228
Iteration 71/1000 | Loss: 0.00001228
Iteration 72/1000 | Loss: 0.00001228
Iteration 73/1000 | Loss: 0.00001228
Iteration 74/1000 | Loss: 0.00001228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.2282953321118839e-05, 1.2282953321118839e-05, 1.2282953321118839e-05, 1.2282953321118839e-05, 1.2282953321118839e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2282953321118839e-05

Optimization complete. Final v2v error: 3.0281310081481934 mm

Highest mean error: 3.28501558303833 mm for frame 143

Lowest mean error: 2.754694700241089 mm for frame 6

Saving results

Total time: 28.78099822998047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829339
Iteration 2/25 | Loss: 0.00146752
Iteration 3/25 | Loss: 0.00122584
Iteration 4/25 | Loss: 0.00120816
Iteration 5/25 | Loss: 0.00120711
Iteration 6/25 | Loss: 0.00120711
Iteration 7/25 | Loss: 0.00120711
Iteration 8/25 | Loss: 0.00120711
Iteration 9/25 | Loss: 0.00120711
Iteration 10/25 | Loss: 0.00120711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012071066303178668, 0.0012071066303178668, 0.0012071066303178668, 0.0012071066303178668, 0.0012071066303178668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012071066303178668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10766482
Iteration 2/25 | Loss: 0.00216668
Iteration 3/25 | Loss: 0.00216667
Iteration 4/25 | Loss: 0.00216667
Iteration 5/25 | Loss: 0.00216667
Iteration 6/25 | Loss: 0.00216667
Iteration 7/25 | Loss: 0.00216666
Iteration 8/25 | Loss: 0.00216666
Iteration 9/25 | Loss: 0.00216666
Iteration 10/25 | Loss: 0.00216666
Iteration 11/25 | Loss: 0.00216666
Iteration 12/25 | Loss: 0.00216666
Iteration 13/25 | Loss: 0.00216666
Iteration 14/25 | Loss: 0.00216666
Iteration 15/25 | Loss: 0.00216666
Iteration 16/25 | Loss: 0.00216666
Iteration 17/25 | Loss: 0.00216666
Iteration 18/25 | Loss: 0.00216666
Iteration 19/25 | Loss: 0.00216666
Iteration 20/25 | Loss: 0.00216666
Iteration 21/25 | Loss: 0.00216666
Iteration 22/25 | Loss: 0.00216666
Iteration 23/25 | Loss: 0.00216666
Iteration 24/25 | Loss: 0.00216666
Iteration 25/25 | Loss: 0.00216666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216666
Iteration 2/1000 | Loss: 0.00004034
Iteration 3/1000 | Loss: 0.00002679
Iteration 4/1000 | Loss: 0.00002370
Iteration 5/1000 | Loss: 0.00002263
Iteration 6/1000 | Loss: 0.00002172
Iteration 7/1000 | Loss: 0.00002103
Iteration 8/1000 | Loss: 0.00002034
Iteration 9/1000 | Loss: 0.00001990
Iteration 10/1000 | Loss: 0.00001983
Iteration 11/1000 | Loss: 0.00001964
Iteration 12/1000 | Loss: 0.00001950
Iteration 13/1000 | Loss: 0.00001938
Iteration 14/1000 | Loss: 0.00001936
Iteration 15/1000 | Loss: 0.00001936
Iteration 16/1000 | Loss: 0.00001935
Iteration 17/1000 | Loss: 0.00001935
Iteration 18/1000 | Loss: 0.00001934
Iteration 19/1000 | Loss: 0.00001932
Iteration 20/1000 | Loss: 0.00001921
Iteration 21/1000 | Loss: 0.00001921
Iteration 22/1000 | Loss: 0.00001919
Iteration 23/1000 | Loss: 0.00001919
Iteration 24/1000 | Loss: 0.00001918
Iteration 25/1000 | Loss: 0.00001916
Iteration 26/1000 | Loss: 0.00001916
Iteration 27/1000 | Loss: 0.00001916
Iteration 28/1000 | Loss: 0.00001916
Iteration 29/1000 | Loss: 0.00001915
Iteration 30/1000 | Loss: 0.00001915
Iteration 31/1000 | Loss: 0.00001914
Iteration 32/1000 | Loss: 0.00001912
Iteration 33/1000 | Loss: 0.00001912
Iteration 34/1000 | Loss: 0.00001912
Iteration 35/1000 | Loss: 0.00001912
Iteration 36/1000 | Loss: 0.00001911
Iteration 37/1000 | Loss: 0.00001911
Iteration 38/1000 | Loss: 0.00001910
Iteration 39/1000 | Loss: 0.00001910
Iteration 40/1000 | Loss: 0.00001910
Iteration 41/1000 | Loss: 0.00001910
Iteration 42/1000 | Loss: 0.00001909
Iteration 43/1000 | Loss: 0.00001909
Iteration 44/1000 | Loss: 0.00001909
Iteration 45/1000 | Loss: 0.00001909
Iteration 46/1000 | Loss: 0.00001909
Iteration 47/1000 | Loss: 0.00001908
Iteration 48/1000 | Loss: 0.00001908
Iteration 49/1000 | Loss: 0.00001907
Iteration 50/1000 | Loss: 0.00001907
Iteration 51/1000 | Loss: 0.00001906
Iteration 52/1000 | Loss: 0.00001906
Iteration 53/1000 | Loss: 0.00001906
Iteration 54/1000 | Loss: 0.00001906
Iteration 55/1000 | Loss: 0.00001906
Iteration 56/1000 | Loss: 0.00001906
Iteration 57/1000 | Loss: 0.00001906
Iteration 58/1000 | Loss: 0.00001906
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001906
Iteration 63/1000 | Loss: 0.00001906
Iteration 64/1000 | Loss: 0.00001906
Iteration 65/1000 | Loss: 0.00001906
Iteration 66/1000 | Loss: 0.00001906
Iteration 67/1000 | Loss: 0.00001906
Iteration 68/1000 | Loss: 0.00001906
Iteration 69/1000 | Loss: 0.00001906
Iteration 70/1000 | Loss: 0.00001906
Iteration 71/1000 | Loss: 0.00001906
Iteration 72/1000 | Loss: 0.00001906
Iteration 73/1000 | Loss: 0.00001906
Iteration 74/1000 | Loss: 0.00001906
Iteration 75/1000 | Loss: 0.00001906
Iteration 76/1000 | Loss: 0.00001906
Iteration 77/1000 | Loss: 0.00001906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.906068246171344e-05, 1.906068246171344e-05, 1.906068246171344e-05, 1.906068246171344e-05, 1.906068246171344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.906068246171344e-05

Optimization complete. Final v2v error: 3.7911243438720703 mm

Highest mean error: 4.478097915649414 mm for frame 96

Lowest mean error: 3.3265955448150635 mm for frame 239

Saving results

Total time: 31.47366738319397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421864
Iteration 2/25 | Loss: 0.00131752
Iteration 3/25 | Loss: 0.00115330
Iteration 4/25 | Loss: 0.00113239
Iteration 5/25 | Loss: 0.00112705
Iteration 6/25 | Loss: 0.00112510
Iteration 7/25 | Loss: 0.00112510
Iteration 8/25 | Loss: 0.00112510
Iteration 9/25 | Loss: 0.00112510
Iteration 10/25 | Loss: 0.00112510
Iteration 11/25 | Loss: 0.00112510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011250951793044806, 0.0011250951793044806, 0.0011250951793044806, 0.0011250951793044806, 0.0011250951793044806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011250951793044806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15484560
Iteration 2/25 | Loss: 0.00329831
Iteration 3/25 | Loss: 0.00329831
Iteration 4/25 | Loss: 0.00329831
Iteration 5/25 | Loss: 0.00329830
Iteration 6/25 | Loss: 0.00329830
Iteration 7/25 | Loss: 0.00329830
Iteration 8/25 | Loss: 0.00329830
Iteration 9/25 | Loss: 0.00329830
Iteration 10/25 | Loss: 0.00329830
Iteration 11/25 | Loss: 0.00329830
Iteration 12/25 | Loss: 0.00329830
Iteration 13/25 | Loss: 0.00329830
Iteration 14/25 | Loss: 0.00329830
Iteration 15/25 | Loss: 0.00329830
Iteration 16/25 | Loss: 0.00329830
Iteration 17/25 | Loss: 0.00329830
Iteration 18/25 | Loss: 0.00329830
Iteration 19/25 | Loss: 0.00329830
Iteration 20/25 | Loss: 0.00329830
Iteration 21/25 | Loss: 0.00329830
Iteration 22/25 | Loss: 0.00329830
Iteration 23/25 | Loss: 0.00329830
Iteration 24/25 | Loss: 0.00329830
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0032983017154037952, 0.0032983017154037952, 0.0032983017154037952, 0.0032983017154037952, 0.0032983017154037952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032983017154037952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00329830
Iteration 2/1000 | Loss: 0.00003800
Iteration 3/1000 | Loss: 0.00002480
Iteration 4/1000 | Loss: 0.00002212
Iteration 5/1000 | Loss: 0.00002065
Iteration 6/1000 | Loss: 0.00001985
Iteration 7/1000 | Loss: 0.00001915
Iteration 8/1000 | Loss: 0.00001864
Iteration 9/1000 | Loss: 0.00001825
Iteration 10/1000 | Loss: 0.00001798
Iteration 11/1000 | Loss: 0.00001777
Iteration 12/1000 | Loss: 0.00001771
Iteration 13/1000 | Loss: 0.00001768
Iteration 14/1000 | Loss: 0.00001758
Iteration 15/1000 | Loss: 0.00001752
Iteration 16/1000 | Loss: 0.00001748
Iteration 17/1000 | Loss: 0.00001744
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001739
Iteration 20/1000 | Loss: 0.00001739
Iteration 21/1000 | Loss: 0.00001735
Iteration 22/1000 | Loss: 0.00001733
Iteration 23/1000 | Loss: 0.00001733
Iteration 24/1000 | Loss: 0.00001732
Iteration 25/1000 | Loss: 0.00001732
Iteration 26/1000 | Loss: 0.00001731
Iteration 27/1000 | Loss: 0.00001731
Iteration 28/1000 | Loss: 0.00001731
Iteration 29/1000 | Loss: 0.00001729
Iteration 30/1000 | Loss: 0.00001729
Iteration 31/1000 | Loss: 0.00001729
Iteration 32/1000 | Loss: 0.00001729
Iteration 33/1000 | Loss: 0.00001729
Iteration 34/1000 | Loss: 0.00001729
Iteration 35/1000 | Loss: 0.00001728
Iteration 36/1000 | Loss: 0.00001727
Iteration 37/1000 | Loss: 0.00001727
Iteration 38/1000 | Loss: 0.00001727
Iteration 39/1000 | Loss: 0.00001726
Iteration 40/1000 | Loss: 0.00001726
Iteration 41/1000 | Loss: 0.00001725
Iteration 42/1000 | Loss: 0.00001725
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001725
Iteration 46/1000 | Loss: 0.00001725
Iteration 47/1000 | Loss: 0.00001725
Iteration 48/1000 | Loss: 0.00001725
Iteration 49/1000 | Loss: 0.00001725
Iteration 50/1000 | Loss: 0.00001725
Iteration 51/1000 | Loss: 0.00001724
Iteration 52/1000 | Loss: 0.00001724
Iteration 53/1000 | Loss: 0.00001724
Iteration 54/1000 | Loss: 0.00001723
Iteration 55/1000 | Loss: 0.00001723
Iteration 56/1000 | Loss: 0.00001723
Iteration 57/1000 | Loss: 0.00001723
Iteration 58/1000 | Loss: 0.00001723
Iteration 59/1000 | Loss: 0.00001723
Iteration 60/1000 | Loss: 0.00001723
Iteration 61/1000 | Loss: 0.00001723
Iteration 62/1000 | Loss: 0.00001723
Iteration 63/1000 | Loss: 0.00001723
Iteration 64/1000 | Loss: 0.00001723
Iteration 65/1000 | Loss: 0.00001722
Iteration 66/1000 | Loss: 0.00001722
Iteration 67/1000 | Loss: 0.00001722
Iteration 68/1000 | Loss: 0.00001721
Iteration 69/1000 | Loss: 0.00001721
Iteration 70/1000 | Loss: 0.00001721
Iteration 71/1000 | Loss: 0.00001721
Iteration 72/1000 | Loss: 0.00001720
Iteration 73/1000 | Loss: 0.00001720
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001720
Iteration 76/1000 | Loss: 0.00001719
Iteration 77/1000 | Loss: 0.00001719
Iteration 78/1000 | Loss: 0.00001719
Iteration 79/1000 | Loss: 0.00001719
Iteration 80/1000 | Loss: 0.00001719
Iteration 81/1000 | Loss: 0.00001719
Iteration 82/1000 | Loss: 0.00001719
Iteration 83/1000 | Loss: 0.00001719
Iteration 84/1000 | Loss: 0.00001719
Iteration 85/1000 | Loss: 0.00001719
Iteration 86/1000 | Loss: 0.00001719
Iteration 87/1000 | Loss: 0.00001719
Iteration 88/1000 | Loss: 0.00001719
Iteration 89/1000 | Loss: 0.00001719
Iteration 90/1000 | Loss: 0.00001719
Iteration 91/1000 | Loss: 0.00001718
Iteration 92/1000 | Loss: 0.00001718
Iteration 93/1000 | Loss: 0.00001718
Iteration 94/1000 | Loss: 0.00001718
Iteration 95/1000 | Loss: 0.00001718
Iteration 96/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.718482053547632e-05, 1.718482053547632e-05, 1.718482053547632e-05, 1.718482053547632e-05, 1.718482053547632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.718482053547632e-05

Optimization complete. Final v2v error: 3.4146740436553955 mm

Highest mean error: 4.218286514282227 mm for frame 78

Lowest mean error: 2.634587049484253 mm for frame 229

Saving results

Total time: 39.8177387714386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076984
Iteration 2/25 | Loss: 0.00265448
Iteration 3/25 | Loss: 0.00169962
Iteration 4/25 | Loss: 0.00156796
Iteration 5/25 | Loss: 0.00153667
Iteration 6/25 | Loss: 0.00157833
Iteration 7/25 | Loss: 0.00150968
Iteration 8/25 | Loss: 0.00139451
Iteration 9/25 | Loss: 0.00132951
Iteration 10/25 | Loss: 0.00130026
Iteration 11/25 | Loss: 0.00128607
Iteration 12/25 | Loss: 0.00130471
Iteration 13/25 | Loss: 0.00126590
Iteration 14/25 | Loss: 0.00124748
Iteration 15/25 | Loss: 0.00123336
Iteration 16/25 | Loss: 0.00123496
Iteration 17/25 | Loss: 0.00123415
Iteration 18/25 | Loss: 0.00122767
Iteration 19/25 | Loss: 0.00122769
Iteration 20/25 | Loss: 0.00122641
Iteration 21/25 | Loss: 0.00122339
Iteration 22/25 | Loss: 0.00122020
Iteration 23/25 | Loss: 0.00121898
Iteration 24/25 | Loss: 0.00122953
Iteration 25/25 | Loss: 0.00122104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17311633
Iteration 2/25 | Loss: 0.00680689
Iteration 3/25 | Loss: 0.00430487
Iteration 4/25 | Loss: 0.00430487
Iteration 5/25 | Loss: 0.00430487
Iteration 6/25 | Loss: 0.00430487
Iteration 7/25 | Loss: 0.00430487
Iteration 8/25 | Loss: 0.00430487
Iteration 9/25 | Loss: 0.00430487
Iteration 10/25 | Loss: 0.00430487
Iteration 11/25 | Loss: 0.00430487
Iteration 12/25 | Loss: 0.00430487
Iteration 13/25 | Loss: 0.00430487
Iteration 14/25 | Loss: 0.00430487
Iteration 15/25 | Loss: 0.00430487
Iteration 16/25 | Loss: 0.00430487
Iteration 17/25 | Loss: 0.00430487
Iteration 18/25 | Loss: 0.00430487
Iteration 19/25 | Loss: 0.00430487
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004304865375161171, 0.004304865375161171, 0.004304865375161171, 0.004304865375161171, 0.004304865375161171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004304865375161171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00430487
Iteration 2/1000 | Loss: 0.00064697
Iteration 3/1000 | Loss: 0.00063455
Iteration 4/1000 | Loss: 0.00061527
Iteration 5/1000 | Loss: 0.00109086
Iteration 6/1000 | Loss: 0.00088465
Iteration 7/1000 | Loss: 0.00085341
Iteration 8/1000 | Loss: 0.00086417
Iteration 9/1000 | Loss: 0.00089503
Iteration 10/1000 | Loss: 0.00162615
Iteration 11/1000 | Loss: 0.00046202
Iteration 12/1000 | Loss: 0.00049929
Iteration 13/1000 | Loss: 0.00058543
Iteration 14/1000 | Loss: 0.00057093
Iteration 15/1000 | Loss: 0.00084972
Iteration 16/1000 | Loss: 0.00058033
Iteration 17/1000 | Loss: 0.00107679
Iteration 18/1000 | Loss: 0.00147302
Iteration 19/1000 | Loss: 0.00074000
Iteration 20/1000 | Loss: 0.00074910
Iteration 21/1000 | Loss: 0.00085029
Iteration 22/1000 | Loss: 0.00080607
Iteration 23/1000 | Loss: 0.00053079
Iteration 24/1000 | Loss: 0.00085333
Iteration 25/1000 | Loss: 0.00088633
Iteration 26/1000 | Loss: 0.00052288
Iteration 27/1000 | Loss: 0.00046883
Iteration 28/1000 | Loss: 0.00045260
Iteration 29/1000 | Loss: 0.00038689
Iteration 30/1000 | Loss: 0.00049671
Iteration 31/1000 | Loss: 0.00051051
Iteration 32/1000 | Loss: 0.00075947
Iteration 33/1000 | Loss: 0.00055917
Iteration 34/1000 | Loss: 0.00072943
Iteration 35/1000 | Loss: 0.00037722
Iteration 36/1000 | Loss: 0.00015665
Iteration 37/1000 | Loss: 0.00024555
Iteration 38/1000 | Loss: 0.00028194
Iteration 39/1000 | Loss: 0.00018983
Iteration 40/1000 | Loss: 0.00027703
Iteration 41/1000 | Loss: 0.00015666
Iteration 42/1000 | Loss: 0.00028453
Iteration 43/1000 | Loss: 0.00039091
Iteration 44/1000 | Loss: 0.00034095
Iteration 45/1000 | Loss: 0.00024949
Iteration 46/1000 | Loss: 0.00020412
Iteration 47/1000 | Loss: 0.00022048
Iteration 48/1000 | Loss: 0.00045512
Iteration 49/1000 | Loss: 0.00012678
Iteration 50/1000 | Loss: 0.00011032
Iteration 51/1000 | Loss: 0.00018964
Iteration 52/1000 | Loss: 0.00015872
Iteration 53/1000 | Loss: 0.00024770
Iteration 54/1000 | Loss: 0.00024625
Iteration 55/1000 | Loss: 0.00025713
Iteration 56/1000 | Loss: 0.00016452
Iteration 57/1000 | Loss: 0.00036188
Iteration 58/1000 | Loss: 0.00027135
Iteration 59/1000 | Loss: 0.00012789
Iteration 60/1000 | Loss: 0.00026253
Iteration 61/1000 | Loss: 0.00022964
Iteration 62/1000 | Loss: 0.00019230
Iteration 63/1000 | Loss: 0.00011141
Iteration 64/1000 | Loss: 0.00021043
Iteration 65/1000 | Loss: 0.00019010
Iteration 66/1000 | Loss: 0.00018404
Iteration 67/1000 | Loss: 0.00017689
Iteration 68/1000 | Loss: 0.00025374
Iteration 69/1000 | Loss: 0.00020962
Iteration 70/1000 | Loss: 0.00012705
Iteration 71/1000 | Loss: 0.00025935
Iteration 72/1000 | Loss: 0.00019151
Iteration 73/1000 | Loss: 0.00029691
Iteration 74/1000 | Loss: 0.00035816
Iteration 75/1000 | Loss: 0.00038718
Iteration 76/1000 | Loss: 0.00035316
Iteration 77/1000 | Loss: 0.00016248
Iteration 78/1000 | Loss: 0.00028183
Iteration 79/1000 | Loss: 0.00010676
Iteration 80/1000 | Loss: 0.00014344
Iteration 81/1000 | Loss: 0.00010683
Iteration 82/1000 | Loss: 0.00013519
Iteration 83/1000 | Loss: 0.00016584
Iteration 84/1000 | Loss: 0.00011365
Iteration 85/1000 | Loss: 0.00012606
Iteration 86/1000 | Loss: 0.00006326
Iteration 87/1000 | Loss: 0.00007525
Iteration 88/1000 | Loss: 0.00005272
Iteration 89/1000 | Loss: 0.00006685
Iteration 90/1000 | Loss: 0.00020834
Iteration 91/1000 | Loss: 0.00017791
Iteration 92/1000 | Loss: 0.00019885
Iteration 93/1000 | Loss: 0.00014800
Iteration 94/1000 | Loss: 0.00023446
Iteration 95/1000 | Loss: 0.00006430
Iteration 96/1000 | Loss: 0.00016852
Iteration 97/1000 | Loss: 0.00018274
Iteration 98/1000 | Loss: 0.00022253
Iteration 99/1000 | Loss: 0.00008282
Iteration 100/1000 | Loss: 0.00022613
Iteration 101/1000 | Loss: 0.00018588
Iteration 102/1000 | Loss: 0.00016026
Iteration 103/1000 | Loss: 0.00018537
Iteration 104/1000 | Loss: 0.00016190
Iteration 105/1000 | Loss: 0.00015327
Iteration 106/1000 | Loss: 0.00017955
Iteration 107/1000 | Loss: 0.00004491
Iteration 108/1000 | Loss: 0.00012680
Iteration 109/1000 | Loss: 0.00016446
Iteration 110/1000 | Loss: 0.00015649
Iteration 111/1000 | Loss: 0.00017492
Iteration 112/1000 | Loss: 0.00016146
Iteration 113/1000 | Loss: 0.00020678
Iteration 114/1000 | Loss: 0.00016336
Iteration 115/1000 | Loss: 0.00016663
Iteration 116/1000 | Loss: 0.00018114
Iteration 117/1000 | Loss: 0.00015573
Iteration 118/1000 | Loss: 0.00016961
Iteration 119/1000 | Loss: 0.00018012
Iteration 120/1000 | Loss: 0.00017484
Iteration 121/1000 | Loss: 0.00023669
Iteration 122/1000 | Loss: 0.00009066
Iteration 123/1000 | Loss: 0.00009356
Iteration 124/1000 | Loss: 0.00010377
Iteration 125/1000 | Loss: 0.00019681
Iteration 126/1000 | Loss: 0.00010981
Iteration 127/1000 | Loss: 0.00017380
Iteration 128/1000 | Loss: 0.00018473
Iteration 129/1000 | Loss: 0.00017490
Iteration 130/1000 | Loss: 0.00011760
Iteration 131/1000 | Loss: 0.00015229
Iteration 132/1000 | Loss: 0.00016869
Iteration 133/1000 | Loss: 0.00019849
Iteration 134/1000 | Loss: 0.00017694
Iteration 135/1000 | Loss: 0.00018706
Iteration 136/1000 | Loss: 0.00018538
Iteration 137/1000 | Loss: 0.00017925
Iteration 138/1000 | Loss: 0.00012743
Iteration 139/1000 | Loss: 0.00019941
Iteration 140/1000 | Loss: 0.00015192
Iteration 141/1000 | Loss: 0.00063929
Iteration 142/1000 | Loss: 0.00015221
Iteration 143/1000 | Loss: 0.00030425
Iteration 144/1000 | Loss: 0.00017498
Iteration 145/1000 | Loss: 0.00012894
Iteration 146/1000 | Loss: 0.00007367
Iteration 147/1000 | Loss: 0.00017183
Iteration 148/1000 | Loss: 0.00014706
Iteration 149/1000 | Loss: 0.00011175
Iteration 150/1000 | Loss: 0.00018324
Iteration 151/1000 | Loss: 0.00017923
Iteration 152/1000 | Loss: 0.00018096
Iteration 153/1000 | Loss: 0.00018044
Iteration 154/1000 | Loss: 0.00019317
Iteration 155/1000 | Loss: 0.00029313
Iteration 156/1000 | Loss: 0.00027307
Iteration 157/1000 | Loss: 0.00028366
Iteration 158/1000 | Loss: 0.00029003
Iteration 159/1000 | Loss: 0.00030497
Iteration 160/1000 | Loss: 0.00020815
Iteration 161/1000 | Loss: 0.00017315
Iteration 162/1000 | Loss: 0.00011328
Iteration 163/1000 | Loss: 0.00014177
Iteration 164/1000 | Loss: 0.00014351
Iteration 165/1000 | Loss: 0.00016223
Iteration 166/1000 | Loss: 0.00017249
Iteration 167/1000 | Loss: 0.00021363
Iteration 168/1000 | Loss: 0.00019115
Iteration 169/1000 | Loss: 0.00030885
Iteration 170/1000 | Loss: 0.00022528
Iteration 171/1000 | Loss: 0.00016862
Iteration 172/1000 | Loss: 0.00013858
Iteration 173/1000 | Loss: 0.00032778
Iteration 174/1000 | Loss: 0.00017453
Iteration 175/1000 | Loss: 0.00008295
Iteration 176/1000 | Loss: 0.00006660
Iteration 177/1000 | Loss: 0.00004875
Iteration 178/1000 | Loss: 0.00005077
Iteration 179/1000 | Loss: 0.00004241
Iteration 180/1000 | Loss: 0.00005061
Iteration 181/1000 | Loss: 0.00008435
Iteration 182/1000 | Loss: 0.00014889
Iteration 183/1000 | Loss: 0.00016384
Iteration 184/1000 | Loss: 0.00010261
Iteration 185/1000 | Loss: 0.00005099
Iteration 186/1000 | Loss: 0.00019674
Iteration 187/1000 | Loss: 0.00011453
Iteration 188/1000 | Loss: 0.00041072
Iteration 189/1000 | Loss: 0.00020388
Iteration 190/1000 | Loss: 0.00044360
Iteration 191/1000 | Loss: 0.00017645
Iteration 192/1000 | Loss: 0.00021606
Iteration 193/1000 | Loss: 0.00029205
Iteration 194/1000 | Loss: 0.00038392
Iteration 195/1000 | Loss: 0.00006414
Iteration 196/1000 | Loss: 0.00003338
Iteration 197/1000 | Loss: 0.00008765
Iteration 198/1000 | Loss: 0.00008160
Iteration 199/1000 | Loss: 0.00029772
Iteration 200/1000 | Loss: 0.00026077
Iteration 201/1000 | Loss: 0.00025142
Iteration 202/1000 | Loss: 0.00029441
Iteration 203/1000 | Loss: 0.00020773
Iteration 204/1000 | Loss: 0.00030016
Iteration 205/1000 | Loss: 0.00022495
Iteration 206/1000 | Loss: 0.00028173
Iteration 207/1000 | Loss: 0.00022726
Iteration 208/1000 | Loss: 0.00021282
Iteration 209/1000 | Loss: 0.00027696
Iteration 210/1000 | Loss: 0.00028845
Iteration 211/1000 | Loss: 0.00027473
Iteration 212/1000 | Loss: 0.00025338
Iteration 213/1000 | Loss: 0.00022401
Iteration 214/1000 | Loss: 0.00022466
Iteration 215/1000 | Loss: 0.00019390
Iteration 216/1000 | Loss: 0.00016249
Iteration 217/1000 | Loss: 0.00027883
Iteration 218/1000 | Loss: 0.00024541
Iteration 219/1000 | Loss: 0.00025841
Iteration 220/1000 | Loss: 0.00016318
Iteration 221/1000 | Loss: 0.00018381
Iteration 222/1000 | Loss: 0.00027464
Iteration 223/1000 | Loss: 0.00013276
Iteration 224/1000 | Loss: 0.00028340
Iteration 225/1000 | Loss: 0.00029255
Iteration 226/1000 | Loss: 0.00025232
Iteration 227/1000 | Loss: 0.00023375
Iteration 228/1000 | Loss: 0.00032276
Iteration 229/1000 | Loss: 0.00031494
Iteration 230/1000 | Loss: 0.00029800
Iteration 231/1000 | Loss: 0.00009870
Iteration 232/1000 | Loss: 0.00031560
Iteration 233/1000 | Loss: 0.00024253
Iteration 234/1000 | Loss: 0.00014046
Iteration 235/1000 | Loss: 0.00031092
Iteration 236/1000 | Loss: 0.00021007
Iteration 237/1000 | Loss: 0.00033854
Iteration 238/1000 | Loss: 0.00017932
Iteration 239/1000 | Loss: 0.00018280
Iteration 240/1000 | Loss: 0.00025742
Iteration 241/1000 | Loss: 0.00007889
Iteration 242/1000 | Loss: 0.00014205
Iteration 243/1000 | Loss: 0.00021950
Iteration 244/1000 | Loss: 0.00035860
Iteration 245/1000 | Loss: 0.00023649
Iteration 246/1000 | Loss: 0.00014102
Iteration 247/1000 | Loss: 0.00009862
Iteration 248/1000 | Loss: 0.00014209
Iteration 249/1000 | Loss: 0.00026409
Iteration 250/1000 | Loss: 0.00048953
Iteration 251/1000 | Loss: 0.00038014
Iteration 252/1000 | Loss: 0.00030286
Iteration 253/1000 | Loss: 0.00017303
Iteration 254/1000 | Loss: 0.00016685
Iteration 255/1000 | Loss: 0.00023705
Iteration 256/1000 | Loss: 0.00038536
Iteration 257/1000 | Loss: 0.00024639
Iteration 258/1000 | Loss: 0.00021498
Iteration 259/1000 | Loss: 0.00026702
Iteration 260/1000 | Loss: 0.00032409
Iteration 261/1000 | Loss: 0.00012335
Iteration 262/1000 | Loss: 0.00020467
Iteration 263/1000 | Loss: 0.00016146
Iteration 264/1000 | Loss: 0.00016842
Iteration 265/1000 | Loss: 0.00012179
Iteration 266/1000 | Loss: 0.00014234
Iteration 267/1000 | Loss: 0.00009603
Iteration 268/1000 | Loss: 0.00004044
Iteration 269/1000 | Loss: 0.00003622
Iteration 270/1000 | Loss: 0.00013499
Iteration 271/1000 | Loss: 0.00012529
Iteration 272/1000 | Loss: 0.00017833
Iteration 273/1000 | Loss: 0.00014053
Iteration 274/1000 | Loss: 0.00012105
Iteration 275/1000 | Loss: 0.00016502
Iteration 276/1000 | Loss: 0.00025849
Iteration 277/1000 | Loss: 0.00028978
Iteration 278/1000 | Loss: 0.00025559
Iteration 279/1000 | Loss: 0.00012385
Iteration 280/1000 | Loss: 0.00004947
Iteration 281/1000 | Loss: 0.00013073
Iteration 282/1000 | Loss: 0.00005304
Iteration 283/1000 | Loss: 0.00004638
Iteration 284/1000 | Loss: 0.00008562
Iteration 285/1000 | Loss: 0.00013859
Iteration 286/1000 | Loss: 0.00031261
Iteration 287/1000 | Loss: 0.00025805
Iteration 288/1000 | Loss: 0.00024799
Iteration 289/1000 | Loss: 0.00025037
Iteration 290/1000 | Loss: 0.00027491
Iteration 291/1000 | Loss: 0.00021731
Iteration 292/1000 | Loss: 0.00008073
Iteration 293/1000 | Loss: 0.00009059
Iteration 294/1000 | Loss: 0.00005373
Iteration 295/1000 | Loss: 0.00027751
Iteration 296/1000 | Loss: 0.00018267
Iteration 297/1000 | Loss: 0.00007629
Iteration 298/1000 | Loss: 0.00005918
Iteration 299/1000 | Loss: 0.00016316
Iteration 300/1000 | Loss: 0.00017139
Iteration 301/1000 | Loss: 0.00016644
Iteration 302/1000 | Loss: 0.00017152
Iteration 303/1000 | Loss: 0.00015254
Iteration 304/1000 | Loss: 0.00099514
Iteration 305/1000 | Loss: 0.00026016
Iteration 306/1000 | Loss: 0.00010411
Iteration 307/1000 | Loss: 0.00006231
Iteration 308/1000 | Loss: 0.00011206
Iteration 309/1000 | Loss: 0.00011043
Iteration 310/1000 | Loss: 0.00014049
Iteration 311/1000 | Loss: 0.00016698
Iteration 312/1000 | Loss: 0.00014053
Iteration 313/1000 | Loss: 0.00034261
Iteration 314/1000 | Loss: 0.00020837
Iteration 315/1000 | Loss: 0.00017082
Iteration 316/1000 | Loss: 0.00016182
Iteration 317/1000 | Loss: 0.00021466
Iteration 318/1000 | Loss: 0.00040971
Iteration 319/1000 | Loss: 0.00015006
Iteration 320/1000 | Loss: 0.00004919
Iteration 321/1000 | Loss: 0.00004990
Iteration 322/1000 | Loss: 0.00004376
Iteration 323/1000 | Loss: 0.00006290
Iteration 324/1000 | Loss: 0.00005702
Iteration 325/1000 | Loss: 0.00004485
Iteration 326/1000 | Loss: 0.00006588
Iteration 327/1000 | Loss: 0.00008846
Iteration 328/1000 | Loss: 0.00007540
Iteration 329/1000 | Loss: 0.00004696
Iteration 330/1000 | Loss: 0.00019603
Iteration 331/1000 | Loss: 0.00027627
Iteration 332/1000 | Loss: 0.00011135
Iteration 333/1000 | Loss: 0.00029957
Iteration 334/1000 | Loss: 0.00040455
Iteration 335/1000 | Loss: 0.00017353
Iteration 336/1000 | Loss: 0.00022800
Iteration 337/1000 | Loss: 0.00026188
Iteration 338/1000 | Loss: 0.00030757
Iteration 339/1000 | Loss: 0.00024082
Iteration 340/1000 | Loss: 0.00024166
Iteration 341/1000 | Loss: 0.00027092
Iteration 342/1000 | Loss: 0.00027760
Iteration 343/1000 | Loss: 0.00027672
Iteration 344/1000 | Loss: 0.00029162
Iteration 345/1000 | Loss: 0.00027059
Iteration 346/1000 | Loss: 0.00029947
Iteration 347/1000 | Loss: 0.00021471
Iteration 348/1000 | Loss: 0.00019220
Iteration 349/1000 | Loss: 0.00017092
Iteration 350/1000 | Loss: 0.00006117
Iteration 351/1000 | Loss: 0.00006769
Iteration 352/1000 | Loss: 0.00014741
Iteration 353/1000 | Loss: 0.00012027
Iteration 354/1000 | Loss: 0.00004761
Iteration 355/1000 | Loss: 0.00005371
Iteration 356/1000 | Loss: 0.00004560
Iteration 357/1000 | Loss: 0.00009454
Iteration 358/1000 | Loss: 0.00011493
Iteration 359/1000 | Loss: 0.00014719
Iteration 360/1000 | Loss: 0.00012936
Iteration 361/1000 | Loss: 0.00011464
Iteration 362/1000 | Loss: 0.00016600
Iteration 363/1000 | Loss: 0.00016747
Iteration 364/1000 | Loss: 0.00015552
Iteration 365/1000 | Loss: 0.00016794
Iteration 366/1000 | Loss: 0.00016522
Iteration 367/1000 | Loss: 0.00007419
Iteration 368/1000 | Loss: 0.00014730
Iteration 369/1000 | Loss: 0.00015952
Iteration 370/1000 | Loss: 0.00016259
Iteration 371/1000 | Loss: 0.00030441
Iteration 372/1000 | Loss: 0.00024359
Iteration 373/1000 | Loss: 0.00016406
Iteration 374/1000 | Loss: 0.00034347
Iteration 375/1000 | Loss: 0.00005026
Iteration 376/1000 | Loss: 0.00016646
Iteration 377/1000 | Loss: 0.00017448
Iteration 378/1000 | Loss: 0.00007580
Iteration 379/1000 | Loss: 0.00012101
Iteration 380/1000 | Loss: 0.00004942
Iteration 381/1000 | Loss: 0.00012757
Iteration 382/1000 | Loss: 0.00008336
Iteration 383/1000 | Loss: 0.00026288
Iteration 384/1000 | Loss: 0.00022051
Iteration 385/1000 | Loss: 0.00008862
Iteration 386/1000 | Loss: 0.00032017
Iteration 387/1000 | Loss: 0.00027998
Iteration 388/1000 | Loss: 0.00022272
Iteration 389/1000 | Loss: 0.00020273
Iteration 390/1000 | Loss: 0.00024782
Iteration 391/1000 | Loss: 0.00017616
Iteration 392/1000 | Loss: 0.00025397
Iteration 393/1000 | Loss: 0.00016749
Iteration 394/1000 | Loss: 0.00012463
Iteration 395/1000 | Loss: 0.00006804
Iteration 396/1000 | Loss: 0.00018532
Iteration 397/1000 | Loss: 0.00020859
Iteration 398/1000 | Loss: 0.00025276
Iteration 399/1000 | Loss: 0.00014110
Iteration 400/1000 | Loss: 0.00011682
Iteration 401/1000 | Loss: 0.00022038
Iteration 402/1000 | Loss: 0.00017023
Iteration 403/1000 | Loss: 0.00015286
Iteration 404/1000 | Loss: 0.00020000
Iteration 405/1000 | Loss: 0.00025708
Iteration 406/1000 | Loss: 0.00024989
Iteration 407/1000 | Loss: 0.00024158
Iteration 408/1000 | Loss: 0.00024284
Iteration 409/1000 | Loss: 0.00023816
Iteration 410/1000 | Loss: 0.00025327
Iteration 411/1000 | Loss: 0.00010192
Iteration 412/1000 | Loss: 0.00018910
Iteration 413/1000 | Loss: 0.00022721
Iteration 414/1000 | Loss: 0.00021237
Iteration 415/1000 | Loss: 0.00021528
Iteration 416/1000 | Loss: 0.00008275
Iteration 417/1000 | Loss: 0.00005097
Iteration 418/1000 | Loss: 0.00005965
Iteration 419/1000 | Loss: 0.00018045
Iteration 420/1000 | Loss: 0.00006187
Iteration 421/1000 | Loss: 0.00006042
Iteration 422/1000 | Loss: 0.00005212
Iteration 423/1000 | Loss: 0.00012317
Iteration 424/1000 | Loss: 0.00017236
Iteration 425/1000 | Loss: 0.00009911
Iteration 426/1000 | Loss: 0.00008814
Iteration 427/1000 | Loss: 0.00006728
Iteration 428/1000 | Loss: 0.00015107
Iteration 429/1000 | Loss: 0.00015125
Iteration 430/1000 | Loss: 0.00009488
Iteration 431/1000 | Loss: 0.00008635
Iteration 432/1000 | Loss: 0.00018181
Iteration 433/1000 | Loss: 0.00017979
Iteration 434/1000 | Loss: 0.00017023
Iteration 435/1000 | Loss: 0.00019073
Iteration 436/1000 | Loss: 0.00017131
Iteration 437/1000 | Loss: 0.00018709
Iteration 438/1000 | Loss: 0.00020854
Iteration 439/1000 | Loss: 0.00018507
Iteration 440/1000 | Loss: 0.00020057
Iteration 441/1000 | Loss: 0.00004040
Iteration 442/1000 | Loss: 0.00011635
Iteration 443/1000 | Loss: 0.00032404
Iteration 444/1000 | Loss: 0.00025580
Iteration 445/1000 | Loss: 0.00042373
Iteration 446/1000 | Loss: 0.00049961
Iteration 447/1000 | Loss: 0.00017725
Iteration 448/1000 | Loss: 0.00004073
Iteration 449/1000 | Loss: 0.00003874
Iteration 450/1000 | Loss: 0.00004637
Iteration 451/1000 | Loss: 0.00004548
Iteration 452/1000 | Loss: 0.00004489
Iteration 453/1000 | Loss: 0.00004737
Iteration 454/1000 | Loss: 0.00011833
Iteration 455/1000 | Loss: 0.00009643
Iteration 456/1000 | Loss: 0.00005257
Iteration 457/1000 | Loss: 0.00022259
Iteration 458/1000 | Loss: 0.00021728
Iteration 459/1000 | Loss: 0.00009002
Iteration 460/1000 | Loss: 0.00022207
Iteration 461/1000 | Loss: 0.00018630
Iteration 462/1000 | Loss: 0.00013777
Iteration 463/1000 | Loss: 0.00020320
Iteration 464/1000 | Loss: 0.00016715
Iteration 465/1000 | Loss: 0.00019025
Iteration 466/1000 | Loss: 0.00015489
Iteration 467/1000 | Loss: 0.00029663
Iteration 468/1000 | Loss: 0.00021553
Iteration 469/1000 | Loss: 0.00017195
Iteration 470/1000 | Loss: 0.00007013
Iteration 471/1000 | Loss: 0.00004683
Iteration 472/1000 | Loss: 0.00052528
Iteration 473/1000 | Loss: 0.00033911
Iteration 474/1000 | Loss: 0.00025912
Iteration 475/1000 | Loss: 0.00024166
Iteration 476/1000 | Loss: 0.00024876
Iteration 477/1000 | Loss: 0.00026095
Iteration 478/1000 | Loss: 0.00008164
Iteration 479/1000 | Loss: 0.00004322
Iteration 480/1000 | Loss: 0.00073424
Iteration 481/1000 | Loss: 0.00053192
Iteration 482/1000 | Loss: 0.00055184
Iteration 483/1000 | Loss: 0.00047733
Iteration 484/1000 | Loss: 0.00050002
Iteration 485/1000 | Loss: 0.00033169
Iteration 486/1000 | Loss: 0.00008489
Iteration 487/1000 | Loss: 0.00076166
Iteration 488/1000 | Loss: 0.00017755
Iteration 489/1000 | Loss: 0.00011539
Iteration 490/1000 | Loss: 0.00005272
Iteration 491/1000 | Loss: 0.00003876
Iteration 492/1000 | Loss: 0.00005014
Iteration 493/1000 | Loss: 0.00005627
Iteration 494/1000 | Loss: 0.00004082
Iteration 495/1000 | Loss: 0.00006217
Iteration 496/1000 | Loss: 0.00004055
Iteration 497/1000 | Loss: 0.00005488
Iteration 498/1000 | Loss: 0.00005085
Iteration 499/1000 | Loss: 0.00005297
Iteration 500/1000 | Loss: 0.00006181
Iteration 501/1000 | Loss: 0.00006630
Iteration 502/1000 | Loss: 0.00005110
Iteration 503/1000 | Loss: 0.00006202
Iteration 504/1000 | Loss: 0.00008266
Iteration 505/1000 | Loss: 0.00005563
Iteration 506/1000 | Loss: 0.00005017
Iteration 507/1000 | Loss: 0.00004387
Iteration 508/1000 | Loss: 0.00004838
Iteration 509/1000 | Loss: 0.00004167
Iteration 510/1000 | Loss: 0.00071560
Iteration 511/1000 | Loss: 0.00041589
Iteration 512/1000 | Loss: 0.00060256
Iteration 513/1000 | Loss: 0.00034702
Iteration 514/1000 | Loss: 0.00073605
Iteration 515/1000 | Loss: 0.00027245
Iteration 516/1000 | Loss: 0.00004136
Iteration 517/1000 | Loss: 0.00042450
Iteration 518/1000 | Loss: 0.00048296
Iteration 519/1000 | Loss: 0.00042776
Iteration 520/1000 | Loss: 0.00002709
Iteration 521/1000 | Loss: 0.00002081
Iteration 522/1000 | Loss: 0.00001891
Iteration 523/1000 | Loss: 0.00001709
Iteration 524/1000 | Loss: 0.00001627
Iteration 525/1000 | Loss: 0.00001551
Iteration 526/1000 | Loss: 0.00001515
Iteration 527/1000 | Loss: 0.00001473
Iteration 528/1000 | Loss: 0.00001448
Iteration 529/1000 | Loss: 0.00001425
Iteration 530/1000 | Loss: 0.00001418
Iteration 531/1000 | Loss: 0.00001404
Iteration 532/1000 | Loss: 0.00001400
Iteration 533/1000 | Loss: 0.00001384
Iteration 534/1000 | Loss: 0.00001379
Iteration 535/1000 | Loss: 0.00001372
Iteration 536/1000 | Loss: 0.00001372
Iteration 537/1000 | Loss: 0.00001372
Iteration 538/1000 | Loss: 0.00001371
Iteration 539/1000 | Loss: 0.00001371
Iteration 540/1000 | Loss: 0.00001371
Iteration 541/1000 | Loss: 0.00001370
Iteration 542/1000 | Loss: 0.00001370
Iteration 543/1000 | Loss: 0.00001370
Iteration 544/1000 | Loss: 0.00001370
Iteration 545/1000 | Loss: 0.00001370
Iteration 546/1000 | Loss: 0.00001370
Iteration 547/1000 | Loss: 0.00001369
Iteration 548/1000 | Loss: 0.00001369
Iteration 549/1000 | Loss: 0.00001369
Iteration 550/1000 | Loss: 0.00001369
Iteration 551/1000 | Loss: 0.00001369
Iteration 552/1000 | Loss: 0.00001369
Iteration 553/1000 | Loss: 0.00001369
Iteration 554/1000 | Loss: 0.00001369
Iteration 555/1000 | Loss: 0.00001369
Iteration 556/1000 | Loss: 0.00001369
Iteration 557/1000 | Loss: 0.00001369
Iteration 558/1000 | Loss: 0.00001368
Iteration 559/1000 | Loss: 0.00001368
Iteration 560/1000 | Loss: 0.00001368
Iteration 561/1000 | Loss: 0.00001368
Iteration 562/1000 | Loss: 0.00001368
Iteration 563/1000 | Loss: 0.00001368
Iteration 564/1000 | Loss: 0.00001367
Iteration 565/1000 | Loss: 0.00001367
Iteration 566/1000 | Loss: 0.00001367
Iteration 567/1000 | Loss: 0.00001365
Iteration 568/1000 | Loss: 0.00001365
Iteration 569/1000 | Loss: 0.00001365
Iteration 570/1000 | Loss: 0.00001365
Iteration 571/1000 | Loss: 0.00001365
Iteration 572/1000 | Loss: 0.00001365
Iteration 573/1000 | Loss: 0.00001365
Iteration 574/1000 | Loss: 0.00001364
Iteration 575/1000 | Loss: 0.00001364
Iteration 576/1000 | Loss: 0.00001364
Iteration 577/1000 | Loss: 0.00001364
Iteration 578/1000 | Loss: 0.00001364
Iteration 579/1000 | Loss: 0.00001364
Iteration 580/1000 | Loss: 0.00001364
Iteration 581/1000 | Loss: 0.00001364
Iteration 582/1000 | Loss: 0.00001364
Iteration 583/1000 | Loss: 0.00001364
Iteration 584/1000 | Loss: 0.00001363
Iteration 585/1000 | Loss: 0.00001363
Iteration 586/1000 | Loss: 0.00001363
Iteration 587/1000 | Loss: 0.00001363
Iteration 588/1000 | Loss: 0.00001363
Iteration 589/1000 | Loss: 0.00001363
Iteration 590/1000 | Loss: 0.00001363
Iteration 591/1000 | Loss: 0.00001363
Iteration 592/1000 | Loss: 0.00001363
Iteration 593/1000 | Loss: 0.00001362
Iteration 594/1000 | Loss: 0.00001362
Iteration 595/1000 | Loss: 0.00001362
Iteration 596/1000 | Loss: 0.00001361
Iteration 597/1000 | Loss: 0.00001361
Iteration 598/1000 | Loss: 0.00001361
Iteration 599/1000 | Loss: 0.00001361
Iteration 600/1000 | Loss: 0.00001361
Iteration 601/1000 | Loss: 0.00001361
Iteration 602/1000 | Loss: 0.00001361
Iteration 603/1000 | Loss: 0.00001361
Iteration 604/1000 | Loss: 0.00001360
Iteration 605/1000 | Loss: 0.00001360
Iteration 606/1000 | Loss: 0.00001360
Iteration 607/1000 | Loss: 0.00001359
Iteration 608/1000 | Loss: 0.00001359
Iteration 609/1000 | Loss: 0.00001359
Iteration 610/1000 | Loss: 0.00001359
Iteration 611/1000 | Loss: 0.00001359
Iteration 612/1000 | Loss: 0.00001358
Iteration 613/1000 | Loss: 0.00001358
Iteration 614/1000 | Loss: 0.00001358
Iteration 615/1000 | Loss: 0.00001358
Iteration 616/1000 | Loss: 0.00001357
Iteration 617/1000 | Loss: 0.00001357
Iteration 618/1000 | Loss: 0.00001357
Iteration 619/1000 | Loss: 0.00001357
Iteration 620/1000 | Loss: 0.00001357
Iteration 621/1000 | Loss: 0.00001357
Iteration 622/1000 | Loss: 0.00001357
Iteration 623/1000 | Loss: 0.00001357
Iteration 624/1000 | Loss: 0.00001357
Iteration 625/1000 | Loss: 0.00001357
Iteration 626/1000 | Loss: 0.00001357
Iteration 627/1000 | Loss: 0.00001357
Iteration 628/1000 | Loss: 0.00001357
Iteration 629/1000 | Loss: 0.00001356
Iteration 630/1000 | Loss: 0.00001356
Iteration 631/1000 | Loss: 0.00001356
Iteration 632/1000 | Loss: 0.00001356
Iteration 633/1000 | Loss: 0.00001356
Iteration 634/1000 | Loss: 0.00001356
Iteration 635/1000 | Loss: 0.00001356
Iteration 636/1000 | Loss: 0.00001356
Iteration 637/1000 | Loss: 0.00001356
Iteration 638/1000 | Loss: 0.00001356
Iteration 639/1000 | Loss: 0.00001356
Iteration 640/1000 | Loss: 0.00001356
Iteration 641/1000 | Loss: 0.00001356
Iteration 642/1000 | Loss: 0.00001356
Iteration 643/1000 | Loss: 0.00001356
Iteration 644/1000 | Loss: 0.00001356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 644. Stopping optimization.
Last 5 losses: [1.356494158244459e-05, 1.356494158244459e-05, 1.356494158244459e-05, 1.356494158244459e-05, 1.356494158244459e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.356494158244459e-05

Optimization complete. Final v2v error: 3.113206624984741 mm

Highest mean error: 4.473878383636475 mm for frame 78

Lowest mean error: 2.544440507888794 mm for frame 1

Saving results

Total time: 785.6313259601593
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051350
Iteration 2/25 | Loss: 0.01051350
Iteration 3/25 | Loss: 0.00275294
Iteration 4/25 | Loss: 0.00221463
Iteration 5/25 | Loss: 0.00226929
Iteration 6/25 | Loss: 0.00170961
Iteration 7/25 | Loss: 0.00155335
Iteration 8/25 | Loss: 0.00148214
Iteration 9/25 | Loss: 0.00147135
Iteration 10/25 | Loss: 0.00145587
Iteration 11/25 | Loss: 0.00144399
Iteration 12/25 | Loss: 0.00144088
Iteration 13/25 | Loss: 0.00143807
Iteration 14/25 | Loss: 0.00143934
Iteration 15/25 | Loss: 0.00143292
Iteration 16/25 | Loss: 0.00143312
Iteration 17/25 | Loss: 0.00142473
Iteration 18/25 | Loss: 0.00141840
Iteration 19/25 | Loss: 0.00142536
Iteration 20/25 | Loss: 0.00142456
Iteration 21/25 | Loss: 0.00142084
Iteration 22/25 | Loss: 0.00142418
Iteration 23/25 | Loss: 0.00142107
Iteration 24/25 | Loss: 0.00142400
Iteration 25/25 | Loss: 0.00142203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02291799
Iteration 2/25 | Loss: 0.00275204
Iteration 3/25 | Loss: 0.00275203
Iteration 4/25 | Loss: 0.00275203
Iteration 5/25 | Loss: 0.00275203
Iteration 6/25 | Loss: 0.00275203
Iteration 7/25 | Loss: 0.00275203
Iteration 8/25 | Loss: 0.00275203
Iteration 9/25 | Loss: 0.00275203
Iteration 10/25 | Loss: 0.00275203
Iteration 11/25 | Loss: 0.00275203
Iteration 12/25 | Loss: 0.00275203
Iteration 13/25 | Loss: 0.00275203
Iteration 14/25 | Loss: 0.00275203
Iteration 15/25 | Loss: 0.00275203
Iteration 16/25 | Loss: 0.00275203
Iteration 17/25 | Loss: 0.00275203
Iteration 18/25 | Loss: 0.00275203
Iteration 19/25 | Loss: 0.00275203
Iteration 20/25 | Loss: 0.00275203
Iteration 21/25 | Loss: 0.00275203
Iteration 22/25 | Loss: 0.00275203
Iteration 23/25 | Loss: 0.00275203
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002752029336988926, 0.002752029336988926, 0.002752029336988926, 0.002752029336988926, 0.002752029336988926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002752029336988926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275203
Iteration 2/1000 | Loss: 0.00020640
Iteration 3/1000 | Loss: 0.00013670
Iteration 4/1000 | Loss: 0.00014053
Iteration 5/1000 | Loss: 0.00009041
Iteration 6/1000 | Loss: 0.00008988
Iteration 7/1000 | Loss: 0.00022223
Iteration 8/1000 | Loss: 0.00012192
Iteration 9/1000 | Loss: 0.00009120
Iteration 10/1000 | Loss: 0.00009060
Iteration 11/1000 | Loss: 0.00015091
Iteration 12/1000 | Loss: 0.00009679
Iteration 13/1000 | Loss: 0.00009995
Iteration 14/1000 | Loss: 0.00010767
Iteration 15/1000 | Loss: 0.00013567
Iteration 16/1000 | Loss: 0.00014413
Iteration 17/1000 | Loss: 0.00027907
Iteration 18/1000 | Loss: 0.00011042
Iteration 19/1000 | Loss: 0.00037566
Iteration 20/1000 | Loss: 0.00011098
Iteration 21/1000 | Loss: 0.00006956
Iteration 22/1000 | Loss: 0.00007493
Iteration 23/1000 | Loss: 0.00006679
Iteration 24/1000 | Loss: 0.00007930
Iteration 25/1000 | Loss: 0.00008155
Iteration 26/1000 | Loss: 0.00007754
Iteration 27/1000 | Loss: 0.00008430
Iteration 28/1000 | Loss: 0.00008483
Iteration 29/1000 | Loss: 0.00011412
Iteration 30/1000 | Loss: 0.00009173
Iteration 31/1000 | Loss: 0.00009963
Iteration 32/1000 | Loss: 0.00009578
Iteration 33/1000 | Loss: 0.00014792
Iteration 34/1000 | Loss: 0.00014350
Iteration 35/1000 | Loss: 0.00013925
Iteration 36/1000 | Loss: 0.00007956
Iteration 37/1000 | Loss: 0.00009801
Iteration 38/1000 | Loss: 0.00018688
Iteration 39/1000 | Loss: 0.00013763
Iteration 40/1000 | Loss: 0.00014028
Iteration 41/1000 | Loss: 0.00021006
Iteration 42/1000 | Loss: 0.00016817
Iteration 43/1000 | Loss: 0.00019137
Iteration 44/1000 | Loss: 0.00010485
Iteration 45/1000 | Loss: 0.00012941
Iteration 46/1000 | Loss: 0.00011793
Iteration 47/1000 | Loss: 0.00020964
Iteration 48/1000 | Loss: 0.00014857
Iteration 49/1000 | Loss: 0.00020839
Iteration 50/1000 | Loss: 0.00010387
Iteration 51/1000 | Loss: 0.00012053
Iteration 52/1000 | Loss: 0.00016695
Iteration 53/1000 | Loss: 0.00013058
Iteration 54/1000 | Loss: 0.00007571
Iteration 55/1000 | Loss: 0.00011997
Iteration 56/1000 | Loss: 0.00019660
Iteration 57/1000 | Loss: 0.00010082
Iteration 58/1000 | Loss: 0.00008426
Iteration 59/1000 | Loss: 0.00008556
Iteration 60/1000 | Loss: 0.00007992
Iteration 61/1000 | Loss: 0.00009806
Iteration 62/1000 | Loss: 0.00049041
Iteration 63/1000 | Loss: 0.00032262
Iteration 64/1000 | Loss: 0.00020516
Iteration 65/1000 | Loss: 0.00007486
Iteration 66/1000 | Loss: 0.00013419
Iteration 67/1000 | Loss: 0.00008937
Iteration 68/1000 | Loss: 0.00016369
Iteration 69/1000 | Loss: 0.00021333
Iteration 70/1000 | Loss: 0.00020271
Iteration 71/1000 | Loss: 0.00008696
Iteration 72/1000 | Loss: 0.00008772
Iteration 73/1000 | Loss: 0.00008256
Iteration 74/1000 | Loss: 0.00008089
Iteration 75/1000 | Loss: 0.00006351
Iteration 76/1000 | Loss: 0.00006573
Iteration 77/1000 | Loss: 0.00007193
Iteration 78/1000 | Loss: 0.00007842
Iteration 79/1000 | Loss: 0.00006188
Iteration 80/1000 | Loss: 0.00007181
Iteration 81/1000 | Loss: 0.00007248
Iteration 82/1000 | Loss: 0.00007226
Iteration 83/1000 | Loss: 0.00012327
Iteration 84/1000 | Loss: 0.00008548
Iteration 85/1000 | Loss: 0.00009512
Iteration 86/1000 | Loss: 0.00007038
Iteration 87/1000 | Loss: 0.00008129
Iteration 88/1000 | Loss: 0.00006004
Iteration 89/1000 | Loss: 0.00006188
Iteration 90/1000 | Loss: 0.00006359
Iteration 91/1000 | Loss: 0.00005067
Iteration 92/1000 | Loss: 0.00006051
Iteration 93/1000 | Loss: 0.00006063
Iteration 94/1000 | Loss: 0.00005656
Iteration 95/1000 | Loss: 0.00005096
Iteration 96/1000 | Loss: 0.00006706
Iteration 97/1000 | Loss: 0.00007398
Iteration 98/1000 | Loss: 0.00007268
Iteration 99/1000 | Loss: 0.00005677
Iteration 100/1000 | Loss: 0.00004685
Iteration 101/1000 | Loss: 0.00004491
Iteration 102/1000 | Loss: 0.00004689
Iteration 103/1000 | Loss: 0.00004200
Iteration 104/1000 | Loss: 0.00004598
Iteration 105/1000 | Loss: 0.00004073
Iteration 106/1000 | Loss: 0.00004547
Iteration 107/1000 | Loss: 0.00004897
Iteration 108/1000 | Loss: 0.00004255
Iteration 109/1000 | Loss: 0.00004742
Iteration 110/1000 | Loss: 0.00004983
Iteration 111/1000 | Loss: 0.00005085
Iteration 112/1000 | Loss: 0.00004940
Iteration 113/1000 | Loss: 0.00003956
Iteration 114/1000 | Loss: 0.00004990
Iteration 115/1000 | Loss: 0.00004293
Iteration 116/1000 | Loss: 0.00004512
Iteration 117/1000 | Loss: 0.00005050
Iteration 118/1000 | Loss: 0.00004656
Iteration 119/1000 | Loss: 0.00004501
Iteration 120/1000 | Loss: 0.00004567
Iteration 121/1000 | Loss: 0.00004543
Iteration 122/1000 | Loss: 0.00004617
Iteration 123/1000 | Loss: 0.00004553
Iteration 124/1000 | Loss: 0.00004526
Iteration 125/1000 | Loss: 0.00004454
Iteration 126/1000 | Loss: 0.00004660
Iteration 127/1000 | Loss: 0.00004097
Iteration 128/1000 | Loss: 0.00004346
Iteration 129/1000 | Loss: 0.00005471
Iteration 130/1000 | Loss: 0.00003974
Iteration 131/1000 | Loss: 0.00003680
Iteration 132/1000 | Loss: 0.00003614
Iteration 133/1000 | Loss: 0.00003586
Iteration 134/1000 | Loss: 0.00003572
Iteration 135/1000 | Loss: 0.00003569
Iteration 136/1000 | Loss: 0.00003568
Iteration 137/1000 | Loss: 0.00003557
Iteration 138/1000 | Loss: 0.00003552
Iteration 139/1000 | Loss: 0.00003551
Iteration 140/1000 | Loss: 0.00003545
Iteration 141/1000 | Loss: 0.00003544
Iteration 142/1000 | Loss: 0.00003544
Iteration 143/1000 | Loss: 0.00003543
Iteration 144/1000 | Loss: 0.00003543
Iteration 145/1000 | Loss: 0.00003542
Iteration 146/1000 | Loss: 0.00003541
Iteration 147/1000 | Loss: 0.00003540
Iteration 148/1000 | Loss: 0.00003540
Iteration 149/1000 | Loss: 0.00003540
Iteration 150/1000 | Loss: 0.00003539
Iteration 151/1000 | Loss: 0.00003538
Iteration 152/1000 | Loss: 0.00003538
Iteration 153/1000 | Loss: 0.00003537
Iteration 154/1000 | Loss: 0.00003534
Iteration 155/1000 | Loss: 0.00003534
Iteration 156/1000 | Loss: 0.00003534
Iteration 157/1000 | Loss: 0.00003533
Iteration 158/1000 | Loss: 0.00003533
Iteration 159/1000 | Loss: 0.00003533
Iteration 160/1000 | Loss: 0.00003533
Iteration 161/1000 | Loss: 0.00003532
Iteration 162/1000 | Loss: 0.00003532
Iteration 163/1000 | Loss: 0.00003532
Iteration 164/1000 | Loss: 0.00003532
Iteration 165/1000 | Loss: 0.00003532
Iteration 166/1000 | Loss: 0.00003532
Iteration 167/1000 | Loss: 0.00003532
Iteration 168/1000 | Loss: 0.00003532
Iteration 169/1000 | Loss: 0.00003532
Iteration 170/1000 | Loss: 0.00003532
Iteration 171/1000 | Loss: 0.00003532
Iteration 172/1000 | Loss: 0.00003532
Iteration 173/1000 | Loss: 0.00003532
Iteration 174/1000 | Loss: 0.00003532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [3.531566835590638e-05, 3.531566835590638e-05, 3.531566835590638e-05, 3.531566835590638e-05, 3.531566835590638e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.531566835590638e-05

Optimization complete. Final v2v error: 4.451213359832764 mm

Highest mean error: 19.637266159057617 mm for frame 85

Lowest mean error: 3.55936336517334 mm for frame 11

Saving results

Total time: 266.17302656173706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046835
Iteration 2/25 | Loss: 0.01046835
Iteration 3/25 | Loss: 0.01046835
Iteration 4/25 | Loss: 0.01046835
Iteration 5/25 | Loss: 0.01046835
Iteration 6/25 | Loss: 0.01046834
Iteration 7/25 | Loss: 0.01046834
Iteration 8/25 | Loss: 0.01046834
Iteration 9/25 | Loss: 0.01046834
Iteration 10/25 | Loss: 0.01046834
Iteration 11/25 | Loss: 0.01046834
Iteration 12/25 | Loss: 0.01046834
Iteration 13/25 | Loss: 0.01046834
Iteration 14/25 | Loss: 0.01046834
Iteration 15/25 | Loss: 0.01046834
Iteration 16/25 | Loss: 0.01046834
Iteration 17/25 | Loss: 0.01046833
Iteration 18/25 | Loss: 0.01046833
Iteration 19/25 | Loss: 0.01046833
Iteration 20/25 | Loss: 0.01046833
Iteration 21/25 | Loss: 0.01046833
Iteration 22/25 | Loss: 0.01046833
Iteration 23/25 | Loss: 0.01046833
Iteration 24/25 | Loss: 0.01046833
Iteration 25/25 | Loss: 0.01046833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50672305
Iteration 2/25 | Loss: 0.08494800
Iteration 3/25 | Loss: 0.08232772
Iteration 4/25 | Loss: 0.08140521
Iteration 5/25 | Loss: 0.08076386
Iteration 6/25 | Loss: 0.08076386
Iteration 7/25 | Loss: 0.08076385
Iteration 8/25 | Loss: 0.08076384
Iteration 9/25 | Loss: 0.08076384
Iteration 10/25 | Loss: 0.08076384
Iteration 11/25 | Loss: 0.08076384
Iteration 12/25 | Loss: 0.08076384
Iteration 13/25 | Loss: 0.08076384
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.08076383918523788, 0.08076383918523788, 0.08076383918523788, 0.08076383918523788, 0.08076383918523788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08076383918523788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08076384
Iteration 2/1000 | Loss: 0.01302487
Iteration 3/1000 | Loss: 0.00742244
Iteration 4/1000 | Loss: 0.00379690
Iteration 5/1000 | Loss: 0.00356156
Iteration 6/1000 | Loss: 0.00137466
Iteration 7/1000 | Loss: 0.00145347
Iteration 8/1000 | Loss: 0.00064114
Iteration 9/1000 | Loss: 0.00038832
Iteration 10/1000 | Loss: 0.00077581
Iteration 11/1000 | Loss: 0.00075815
Iteration 12/1000 | Loss: 0.00032368
Iteration 13/1000 | Loss: 0.00078906
Iteration 14/1000 | Loss: 0.00046280
Iteration 15/1000 | Loss: 0.00021011
Iteration 16/1000 | Loss: 0.00048757
Iteration 17/1000 | Loss: 0.00106014
Iteration 18/1000 | Loss: 0.00081435
Iteration 19/1000 | Loss: 0.00121688
Iteration 20/1000 | Loss: 0.00047431
Iteration 21/1000 | Loss: 0.00022976
Iteration 22/1000 | Loss: 0.00030968
Iteration 23/1000 | Loss: 0.00021335
Iteration 24/1000 | Loss: 0.00038342
Iteration 25/1000 | Loss: 0.00047828
Iteration 26/1000 | Loss: 0.00041428
Iteration 27/1000 | Loss: 0.00019271
Iteration 28/1000 | Loss: 0.00014993
Iteration 29/1000 | Loss: 0.00030614
Iteration 30/1000 | Loss: 0.00038045
Iteration 31/1000 | Loss: 0.00074925
Iteration 32/1000 | Loss: 0.00014783
Iteration 33/1000 | Loss: 0.00012221
Iteration 34/1000 | Loss: 0.00049592
Iteration 35/1000 | Loss: 0.00018569
Iteration 36/1000 | Loss: 0.00012047
Iteration 37/1000 | Loss: 0.00012784
Iteration 38/1000 | Loss: 0.00033769
Iteration 39/1000 | Loss: 0.00010157
Iteration 40/1000 | Loss: 0.00009828
Iteration 41/1000 | Loss: 0.00011357
Iteration 42/1000 | Loss: 0.00014410
Iteration 43/1000 | Loss: 0.00061660
Iteration 44/1000 | Loss: 0.00173135
Iteration 45/1000 | Loss: 0.00192224
Iteration 46/1000 | Loss: 0.00091460
Iteration 47/1000 | Loss: 0.00098950
Iteration 48/1000 | Loss: 0.00066485
Iteration 49/1000 | Loss: 0.00039775
Iteration 50/1000 | Loss: 0.00040984
Iteration 51/1000 | Loss: 0.00044698
Iteration 52/1000 | Loss: 0.00036056
Iteration 53/1000 | Loss: 0.00045716
Iteration 54/1000 | Loss: 0.00030221
Iteration 55/1000 | Loss: 0.00011980
Iteration 56/1000 | Loss: 0.00012161
Iteration 57/1000 | Loss: 0.00020950
Iteration 58/1000 | Loss: 0.00024278
Iteration 59/1000 | Loss: 0.00016478
Iteration 60/1000 | Loss: 0.00014403
Iteration 61/1000 | Loss: 0.00009975
Iteration 62/1000 | Loss: 0.00022196
Iteration 63/1000 | Loss: 0.00010058
Iteration 64/1000 | Loss: 0.00011711
Iteration 65/1000 | Loss: 0.00008826
Iteration 66/1000 | Loss: 0.00023819
Iteration 67/1000 | Loss: 0.00023559
Iteration 68/1000 | Loss: 0.00012627
Iteration 69/1000 | Loss: 0.00011489
Iteration 70/1000 | Loss: 0.00008970
Iteration 71/1000 | Loss: 0.00017508
Iteration 72/1000 | Loss: 0.00007409
Iteration 73/1000 | Loss: 0.00010180
Iteration 74/1000 | Loss: 0.00048804
Iteration 75/1000 | Loss: 0.00006826
Iteration 76/1000 | Loss: 0.00006434
Iteration 77/1000 | Loss: 0.00009357
Iteration 78/1000 | Loss: 0.00020183
Iteration 79/1000 | Loss: 0.00010972
Iteration 80/1000 | Loss: 0.00026517
Iteration 81/1000 | Loss: 0.00016912
Iteration 82/1000 | Loss: 0.00008605
Iteration 83/1000 | Loss: 0.00017030
Iteration 84/1000 | Loss: 0.00018161
Iteration 85/1000 | Loss: 0.00006157
Iteration 86/1000 | Loss: 0.00005903
Iteration 87/1000 | Loss: 0.00028063
Iteration 88/1000 | Loss: 0.00018719
Iteration 89/1000 | Loss: 0.00007947
Iteration 90/1000 | Loss: 0.00005762
Iteration 91/1000 | Loss: 0.00008462
Iteration 92/1000 | Loss: 0.00005714
Iteration 93/1000 | Loss: 0.00005509
Iteration 94/1000 | Loss: 0.00010461
Iteration 95/1000 | Loss: 0.00008692
Iteration 96/1000 | Loss: 0.00006813
Iteration 97/1000 | Loss: 0.00013952
Iteration 98/1000 | Loss: 0.00005694
Iteration 99/1000 | Loss: 0.00028338
Iteration 100/1000 | Loss: 0.00017216
Iteration 101/1000 | Loss: 0.00009232
Iteration 102/1000 | Loss: 0.00008181
Iteration 103/1000 | Loss: 0.00006164
Iteration 104/1000 | Loss: 0.00009096
Iteration 105/1000 | Loss: 0.00006908
Iteration 106/1000 | Loss: 0.00006500
Iteration 107/1000 | Loss: 0.00007019
Iteration 108/1000 | Loss: 0.00005727
Iteration 109/1000 | Loss: 0.00021699
Iteration 110/1000 | Loss: 0.00017864
Iteration 111/1000 | Loss: 0.00009837
Iteration 112/1000 | Loss: 0.00007123
Iteration 113/1000 | Loss: 0.00021040
Iteration 114/1000 | Loss: 0.00029315
Iteration 115/1000 | Loss: 0.00028254
Iteration 116/1000 | Loss: 0.00008524
Iteration 117/1000 | Loss: 0.00012550
Iteration 118/1000 | Loss: 0.00020494
Iteration 119/1000 | Loss: 0.00005966
Iteration 120/1000 | Loss: 0.00017345
Iteration 121/1000 | Loss: 0.00037006
Iteration 122/1000 | Loss: 0.00032418
Iteration 123/1000 | Loss: 0.00011749
Iteration 124/1000 | Loss: 0.00007654
Iteration 125/1000 | Loss: 0.00006913
Iteration 126/1000 | Loss: 0.00006431
Iteration 127/1000 | Loss: 0.00014321
Iteration 128/1000 | Loss: 0.00006430
Iteration 129/1000 | Loss: 0.00010034
Iteration 130/1000 | Loss: 0.00005557
Iteration 131/1000 | Loss: 0.00011907
Iteration 132/1000 | Loss: 0.00005395
Iteration 133/1000 | Loss: 0.00005304
Iteration 134/1000 | Loss: 0.00005222
Iteration 135/1000 | Loss: 0.00024730
Iteration 136/1000 | Loss: 0.00016955
Iteration 137/1000 | Loss: 0.00009332
Iteration 138/1000 | Loss: 0.00036844
Iteration 139/1000 | Loss: 0.00006009
Iteration 140/1000 | Loss: 0.00005152
Iteration 141/1000 | Loss: 0.00010516
Iteration 142/1000 | Loss: 0.00005070
Iteration 143/1000 | Loss: 0.00004950
Iteration 144/1000 | Loss: 0.00004908
Iteration 145/1000 | Loss: 0.00004878
Iteration 146/1000 | Loss: 0.00006992
Iteration 147/1000 | Loss: 0.00004868
Iteration 148/1000 | Loss: 0.00004831
Iteration 149/1000 | Loss: 0.00007354
Iteration 150/1000 | Loss: 0.00005343
Iteration 151/1000 | Loss: 0.00006003
Iteration 152/1000 | Loss: 0.00004798
Iteration 153/1000 | Loss: 0.00004797
Iteration 154/1000 | Loss: 0.00006807
Iteration 155/1000 | Loss: 0.00004790
Iteration 156/1000 | Loss: 0.00007153
Iteration 157/1000 | Loss: 0.00004780
Iteration 158/1000 | Loss: 0.00004840
Iteration 159/1000 | Loss: 0.00004840
Iteration 160/1000 | Loss: 0.00004840
Iteration 161/1000 | Loss: 0.00004840
Iteration 162/1000 | Loss: 0.00004840
Iteration 163/1000 | Loss: 0.00008283
Iteration 164/1000 | Loss: 0.00005018
Iteration 165/1000 | Loss: 0.00004786
Iteration 166/1000 | Loss: 0.00005122
Iteration 167/1000 | Loss: 0.00005254
Iteration 168/1000 | Loss: 0.00004785
Iteration 169/1000 | Loss: 0.00004764
Iteration 170/1000 | Loss: 0.00004763
Iteration 171/1000 | Loss: 0.00004763
Iteration 172/1000 | Loss: 0.00004762
Iteration 173/1000 | Loss: 0.00004761
Iteration 174/1000 | Loss: 0.00004761
Iteration 175/1000 | Loss: 0.00004761
Iteration 176/1000 | Loss: 0.00004760
Iteration 177/1000 | Loss: 0.00004759
Iteration 178/1000 | Loss: 0.00004759
Iteration 179/1000 | Loss: 0.00004759
Iteration 180/1000 | Loss: 0.00004759
Iteration 181/1000 | Loss: 0.00004759
Iteration 182/1000 | Loss: 0.00004759
Iteration 183/1000 | Loss: 0.00004759
Iteration 184/1000 | Loss: 0.00004758
Iteration 185/1000 | Loss: 0.00004758
Iteration 186/1000 | Loss: 0.00004758
Iteration 187/1000 | Loss: 0.00004758
Iteration 188/1000 | Loss: 0.00004758
Iteration 189/1000 | Loss: 0.00004758
Iteration 190/1000 | Loss: 0.00004758
Iteration 191/1000 | Loss: 0.00004758
Iteration 192/1000 | Loss: 0.00004758
Iteration 193/1000 | Loss: 0.00004758
Iteration 194/1000 | Loss: 0.00004758
Iteration 195/1000 | Loss: 0.00004757
Iteration 196/1000 | Loss: 0.00004757
Iteration 197/1000 | Loss: 0.00004754
Iteration 198/1000 | Loss: 0.00004754
Iteration 199/1000 | Loss: 0.00004754
Iteration 200/1000 | Loss: 0.00004754
Iteration 201/1000 | Loss: 0.00004754
Iteration 202/1000 | Loss: 0.00004754
Iteration 203/1000 | Loss: 0.00004803
Iteration 204/1000 | Loss: 0.00006764
Iteration 205/1000 | Loss: 0.00004857
Iteration 206/1000 | Loss: 0.00006842
Iteration 207/1000 | Loss: 0.00005000
Iteration 208/1000 | Loss: 0.00007278
Iteration 209/1000 | Loss: 0.00004905
Iteration 210/1000 | Loss: 0.00004753
Iteration 211/1000 | Loss: 0.00004753
Iteration 212/1000 | Loss: 0.00004750
Iteration 213/1000 | Loss: 0.00004750
Iteration 214/1000 | Loss: 0.00004749
Iteration 215/1000 | Loss: 0.00004749
Iteration 216/1000 | Loss: 0.00004748
Iteration 217/1000 | Loss: 0.00004747
Iteration 218/1000 | Loss: 0.00004747
Iteration 219/1000 | Loss: 0.00004747
Iteration 220/1000 | Loss: 0.00004746
Iteration 221/1000 | Loss: 0.00004750
Iteration 222/1000 | Loss: 0.00004750
Iteration 223/1000 | Loss: 0.00004750
Iteration 224/1000 | Loss: 0.00004749
Iteration 225/1000 | Loss: 0.00004749
Iteration 226/1000 | Loss: 0.00004803
Iteration 227/1000 | Loss: 0.00004764
Iteration 228/1000 | Loss: 0.00004744
Iteration 229/1000 | Loss: 0.00004746
Iteration 230/1000 | Loss: 0.00004746
Iteration 231/1000 | Loss: 0.00004746
Iteration 232/1000 | Loss: 0.00004746
Iteration 233/1000 | Loss: 0.00004744
Iteration 234/1000 | Loss: 0.00004744
Iteration 235/1000 | Loss: 0.00004743
Iteration 236/1000 | Loss: 0.00004743
Iteration 237/1000 | Loss: 0.00004743
Iteration 238/1000 | Loss: 0.00004743
Iteration 239/1000 | Loss: 0.00004743
Iteration 240/1000 | Loss: 0.00004742
Iteration 241/1000 | Loss: 0.00004742
Iteration 242/1000 | Loss: 0.00004742
Iteration 243/1000 | Loss: 0.00004742
Iteration 244/1000 | Loss: 0.00004742
Iteration 245/1000 | Loss: 0.00004742
Iteration 246/1000 | Loss: 0.00006427
Iteration 247/1000 | Loss: 0.00005002
Iteration 248/1000 | Loss: 0.00006275
Iteration 249/1000 | Loss: 0.00004918
Iteration 250/1000 | Loss: 0.00005199
Iteration 251/1000 | Loss: 0.00004753
Iteration 252/1000 | Loss: 0.00004864
Iteration 253/1000 | Loss: 0.00004990
Iteration 254/1000 | Loss: 0.00004872
Iteration 255/1000 | Loss: 0.00005214
Iteration 256/1000 | Loss: 0.00005161
Iteration 257/1000 | Loss: 0.00004857
Iteration 258/1000 | Loss: 0.00004811
Iteration 259/1000 | Loss: 0.00005611
Iteration 260/1000 | Loss: 0.00004820
Iteration 261/1000 | Loss: 0.00005157
Iteration 262/1000 | Loss: 0.00004811
Iteration 263/1000 | Loss: 0.00005461
Iteration 264/1000 | Loss: 0.00005119
Iteration 265/1000 | Loss: 0.00004917
Iteration 266/1000 | Loss: 0.00004997
Iteration 267/1000 | Loss: 0.00005139
Iteration 268/1000 | Loss: 0.00004784
Iteration 269/1000 | Loss: 0.00004832
Iteration 270/1000 | Loss: 0.00004745
Iteration 271/1000 | Loss: 0.00004745
Iteration 272/1000 | Loss: 0.00004742
Iteration 273/1000 | Loss: 0.00004741
Iteration 274/1000 | Loss: 0.00004741
Iteration 275/1000 | Loss: 0.00004741
Iteration 276/1000 | Loss: 0.00004741
Iteration 277/1000 | Loss: 0.00004741
Iteration 278/1000 | Loss: 0.00004741
Iteration 279/1000 | Loss: 0.00004741
Iteration 280/1000 | Loss: 0.00004741
Iteration 281/1000 | Loss: 0.00004740
Iteration 282/1000 | Loss: 0.00004740
Iteration 283/1000 | Loss: 0.00004740
Iteration 284/1000 | Loss: 0.00004740
Iteration 285/1000 | Loss: 0.00004759
Iteration 286/1000 | Loss: 0.00004743
Iteration 287/1000 | Loss: 0.00004742
Iteration 288/1000 | Loss: 0.00004740
Iteration 289/1000 | Loss: 0.00004740
Iteration 290/1000 | Loss: 0.00004740
Iteration 291/1000 | Loss: 0.00004740
Iteration 292/1000 | Loss: 0.00004740
Iteration 293/1000 | Loss: 0.00004740
Iteration 294/1000 | Loss: 0.00004740
Iteration 295/1000 | Loss: 0.00004740
Iteration 296/1000 | Loss: 0.00004740
Iteration 297/1000 | Loss: 0.00004740
Iteration 298/1000 | Loss: 0.00004740
Iteration 299/1000 | Loss: 0.00004740
Iteration 300/1000 | Loss: 0.00004740
Iteration 301/1000 | Loss: 0.00004740
Iteration 302/1000 | Loss: 0.00004740
Iteration 303/1000 | Loss: 0.00004740
Iteration 304/1000 | Loss: 0.00004740
Iteration 305/1000 | Loss: 0.00004740
Iteration 306/1000 | Loss: 0.00004740
Iteration 307/1000 | Loss: 0.00004740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [4.739548603538424e-05, 4.739548603538424e-05, 4.739548603538424e-05, 4.739548603538424e-05, 4.739548603538424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.739548603538424e-05

Optimization complete. Final v2v error: 4.140705108642578 mm

Highest mean error: 20.421525955200195 mm for frame 206

Lowest mean error: 2.6374502182006836 mm for frame 90

Saving results

Total time: 322.76643204689026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067215
Iteration 2/25 | Loss: 0.01067215
Iteration 3/25 | Loss: 0.00334659
Iteration 4/25 | Loss: 0.00235626
Iteration 5/25 | Loss: 0.00193994
Iteration 6/25 | Loss: 0.00181196
Iteration 7/25 | Loss: 0.00153325
Iteration 8/25 | Loss: 0.00136745
Iteration 9/25 | Loss: 0.00127359
Iteration 10/25 | Loss: 0.00124598
Iteration 11/25 | Loss: 0.00122893
Iteration 12/25 | Loss: 0.00121853
Iteration 13/25 | Loss: 0.00121024
Iteration 14/25 | Loss: 0.00120543
Iteration 15/25 | Loss: 0.00120022
Iteration 16/25 | Loss: 0.00119935
Iteration 17/25 | Loss: 0.00119932
Iteration 18/25 | Loss: 0.00119892
Iteration 19/25 | Loss: 0.00119873
Iteration 20/25 | Loss: 0.00119899
Iteration 21/25 | Loss: 0.00119474
Iteration 22/25 | Loss: 0.00119492
Iteration 23/25 | Loss: 0.00119451
Iteration 24/25 | Loss: 0.00119465
Iteration 25/25 | Loss: 0.00119440

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14413202
Iteration 2/25 | Loss: 0.00310772
Iteration 3/25 | Loss: 0.00310772
Iteration 4/25 | Loss: 0.00310771
Iteration 5/25 | Loss: 0.00310771
Iteration 6/25 | Loss: 0.00310771
Iteration 7/25 | Loss: 0.00310771
Iteration 8/25 | Loss: 0.00310771
Iteration 9/25 | Loss: 0.00310771
Iteration 10/25 | Loss: 0.00310771
Iteration 11/25 | Loss: 0.00310771
Iteration 12/25 | Loss: 0.00310771
Iteration 13/25 | Loss: 0.00310771
Iteration 14/25 | Loss: 0.00310771
Iteration 15/25 | Loss: 0.00310771
Iteration 16/25 | Loss: 0.00310771
Iteration 17/25 | Loss: 0.00310771
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003107712371274829, 0.003107712371274829, 0.003107712371274829, 0.003107712371274829, 0.003107712371274829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003107712371274829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00310771
Iteration 2/1000 | Loss: 0.00008804
Iteration 3/1000 | Loss: 0.00006692
Iteration 4/1000 | Loss: 0.00005878
Iteration 5/1000 | Loss: 0.00005391
Iteration 6/1000 | Loss: 0.00005172
Iteration 7/1000 | Loss: 0.00004928
Iteration 8/1000 | Loss: 0.00004769
Iteration 9/1000 | Loss: 0.00058751
Iteration 10/1000 | Loss: 0.00184668
Iteration 11/1000 | Loss: 0.00021274
Iteration 12/1000 | Loss: 0.00011661
Iteration 13/1000 | Loss: 0.00004129
Iteration 14/1000 | Loss: 0.00003289
Iteration 15/1000 | Loss: 0.00002805
Iteration 16/1000 | Loss: 0.00002486
Iteration 17/1000 | Loss: 0.00002235
Iteration 18/1000 | Loss: 0.00002098
Iteration 19/1000 | Loss: 0.00001982
Iteration 20/1000 | Loss: 0.00001896
Iteration 21/1000 | Loss: 0.00001847
Iteration 22/1000 | Loss: 0.00001803
Iteration 23/1000 | Loss: 0.00001810
Iteration 24/1000 | Loss: 0.00001745
Iteration 25/1000 | Loss: 0.00001738
Iteration 26/1000 | Loss: 0.00001717
Iteration 27/1000 | Loss: 0.00001704
Iteration 28/1000 | Loss: 0.00001698
Iteration 29/1000 | Loss: 0.00001695
Iteration 30/1000 | Loss: 0.00001690
Iteration 31/1000 | Loss: 0.00001688
Iteration 32/1000 | Loss: 0.00001688
Iteration 33/1000 | Loss: 0.00001687
Iteration 34/1000 | Loss: 0.00001687
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001685
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001684
Iteration 39/1000 | Loss: 0.00001684
Iteration 40/1000 | Loss: 0.00001683
Iteration 41/1000 | Loss: 0.00001682
Iteration 42/1000 | Loss: 0.00001682
Iteration 43/1000 | Loss: 0.00001682
Iteration 44/1000 | Loss: 0.00001682
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00001682
Iteration 47/1000 | Loss: 0.00001682
Iteration 48/1000 | Loss: 0.00001682
Iteration 49/1000 | Loss: 0.00001681
Iteration 50/1000 | Loss: 0.00001681
Iteration 51/1000 | Loss: 0.00001681
Iteration 52/1000 | Loss: 0.00001681
Iteration 53/1000 | Loss: 0.00001681
Iteration 54/1000 | Loss: 0.00001681
Iteration 55/1000 | Loss: 0.00001681
Iteration 56/1000 | Loss: 0.00001681
Iteration 57/1000 | Loss: 0.00001681
Iteration 58/1000 | Loss: 0.00001680
Iteration 59/1000 | Loss: 0.00001680
Iteration 60/1000 | Loss: 0.00001680
Iteration 61/1000 | Loss: 0.00001679
Iteration 62/1000 | Loss: 0.00001679
Iteration 63/1000 | Loss: 0.00001678
Iteration 64/1000 | Loss: 0.00001678
Iteration 65/1000 | Loss: 0.00001678
Iteration 66/1000 | Loss: 0.00001678
Iteration 67/1000 | Loss: 0.00001677
Iteration 68/1000 | Loss: 0.00001677
Iteration 69/1000 | Loss: 0.00001677
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001676
Iteration 72/1000 | Loss: 0.00001676
Iteration 73/1000 | Loss: 0.00001676
Iteration 74/1000 | Loss: 0.00001676
Iteration 75/1000 | Loss: 0.00001676
Iteration 76/1000 | Loss: 0.00001676
Iteration 77/1000 | Loss: 0.00001676
Iteration 78/1000 | Loss: 0.00001676
Iteration 79/1000 | Loss: 0.00001675
Iteration 80/1000 | Loss: 0.00001675
Iteration 81/1000 | Loss: 0.00001675
Iteration 82/1000 | Loss: 0.00001675
Iteration 83/1000 | Loss: 0.00001675
Iteration 84/1000 | Loss: 0.00001675
Iteration 85/1000 | Loss: 0.00001675
Iteration 86/1000 | Loss: 0.00001675
Iteration 87/1000 | Loss: 0.00001675
Iteration 88/1000 | Loss: 0.00001675
Iteration 89/1000 | Loss: 0.00001675
Iteration 90/1000 | Loss: 0.00001675
Iteration 91/1000 | Loss: 0.00001675
Iteration 92/1000 | Loss: 0.00001675
Iteration 93/1000 | Loss: 0.00001675
Iteration 94/1000 | Loss: 0.00001675
Iteration 95/1000 | Loss: 0.00001675
Iteration 96/1000 | Loss: 0.00001675
Iteration 97/1000 | Loss: 0.00001675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.6747369954828173e-05, 1.6747369954828173e-05, 1.6747369954828173e-05, 1.6747369954828173e-05, 1.6747369954828173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6747369954828173e-05

Optimization complete. Final v2v error: 3.4130396842956543 mm

Highest mean error: 8.418919563293457 mm for frame 218

Lowest mean error: 3.0613467693328857 mm for frame 239

Saving results

Total time: 96.66616082191467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820050
Iteration 2/25 | Loss: 0.00155715
Iteration 3/25 | Loss: 0.00125378
Iteration 4/25 | Loss: 0.00121598
Iteration 5/25 | Loss: 0.00120801
Iteration 6/25 | Loss: 0.00120568
Iteration 7/25 | Loss: 0.00120568
Iteration 8/25 | Loss: 0.00120568
Iteration 9/25 | Loss: 0.00120568
Iteration 10/25 | Loss: 0.00120568
Iteration 11/25 | Loss: 0.00120568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012056803097948432, 0.0012056803097948432, 0.0012056803097948432, 0.0012056803097948432, 0.0012056803097948432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012056803097948432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17848444
Iteration 2/25 | Loss: 0.00328624
Iteration 3/25 | Loss: 0.00328624
Iteration 4/25 | Loss: 0.00328624
Iteration 5/25 | Loss: 0.00328624
Iteration 6/25 | Loss: 0.00328624
Iteration 7/25 | Loss: 0.00328624
Iteration 8/25 | Loss: 0.00328624
Iteration 9/25 | Loss: 0.00328623
Iteration 10/25 | Loss: 0.00328624
Iteration 11/25 | Loss: 0.00328624
Iteration 12/25 | Loss: 0.00328623
Iteration 13/25 | Loss: 0.00328623
Iteration 14/25 | Loss: 0.00328623
Iteration 15/25 | Loss: 0.00328623
Iteration 16/25 | Loss: 0.00328623
Iteration 17/25 | Loss: 0.00328623
Iteration 18/25 | Loss: 0.00328623
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003286234335973859, 0.003286234335973859, 0.003286234335973859, 0.003286234335973859, 0.003286234335973859]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003286234335973859

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00328623
Iteration 2/1000 | Loss: 0.00005754
Iteration 3/1000 | Loss: 0.00003005
Iteration 4/1000 | Loss: 0.00002510
Iteration 5/1000 | Loss: 0.00002304
Iteration 6/1000 | Loss: 0.00002173
Iteration 7/1000 | Loss: 0.00002099
Iteration 8/1000 | Loss: 0.00002051
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00001983
Iteration 11/1000 | Loss: 0.00001958
Iteration 12/1000 | Loss: 0.00001936
Iteration 13/1000 | Loss: 0.00001920
Iteration 14/1000 | Loss: 0.00001903
Iteration 15/1000 | Loss: 0.00001892
Iteration 16/1000 | Loss: 0.00001883
Iteration 17/1000 | Loss: 0.00001876
Iteration 18/1000 | Loss: 0.00001875
Iteration 19/1000 | Loss: 0.00001874
Iteration 20/1000 | Loss: 0.00001874
Iteration 21/1000 | Loss: 0.00001873
Iteration 22/1000 | Loss: 0.00001872
Iteration 23/1000 | Loss: 0.00001872
Iteration 24/1000 | Loss: 0.00001871
Iteration 25/1000 | Loss: 0.00001871
Iteration 26/1000 | Loss: 0.00001870
Iteration 27/1000 | Loss: 0.00001870
Iteration 28/1000 | Loss: 0.00001867
Iteration 29/1000 | Loss: 0.00001867
Iteration 30/1000 | Loss: 0.00001865
Iteration 31/1000 | Loss: 0.00001865
Iteration 32/1000 | Loss: 0.00001864
Iteration 33/1000 | Loss: 0.00001864
Iteration 34/1000 | Loss: 0.00001864
Iteration 35/1000 | Loss: 0.00001863
Iteration 36/1000 | Loss: 0.00001863
Iteration 37/1000 | Loss: 0.00001863
Iteration 38/1000 | Loss: 0.00001862
Iteration 39/1000 | Loss: 0.00001861
Iteration 40/1000 | Loss: 0.00001861
Iteration 41/1000 | Loss: 0.00001861
Iteration 42/1000 | Loss: 0.00001861
Iteration 43/1000 | Loss: 0.00001860
Iteration 44/1000 | Loss: 0.00001860
Iteration 45/1000 | Loss: 0.00001859
Iteration 46/1000 | Loss: 0.00001859
Iteration 47/1000 | Loss: 0.00001859
Iteration 48/1000 | Loss: 0.00001858
Iteration 49/1000 | Loss: 0.00001858
Iteration 50/1000 | Loss: 0.00001858
Iteration 51/1000 | Loss: 0.00001857
Iteration 52/1000 | Loss: 0.00001857
Iteration 53/1000 | Loss: 0.00001857
Iteration 54/1000 | Loss: 0.00001857
Iteration 55/1000 | Loss: 0.00001857
Iteration 56/1000 | Loss: 0.00001856
Iteration 57/1000 | Loss: 0.00001856
Iteration 58/1000 | Loss: 0.00001856
Iteration 59/1000 | Loss: 0.00001856
Iteration 60/1000 | Loss: 0.00001855
Iteration 61/1000 | Loss: 0.00001855
Iteration 62/1000 | Loss: 0.00001854
Iteration 63/1000 | Loss: 0.00001854
Iteration 64/1000 | Loss: 0.00001854
Iteration 65/1000 | Loss: 0.00001854
Iteration 66/1000 | Loss: 0.00001853
Iteration 67/1000 | Loss: 0.00001853
Iteration 68/1000 | Loss: 0.00001853
Iteration 69/1000 | Loss: 0.00001852
Iteration 70/1000 | Loss: 0.00001852
Iteration 71/1000 | Loss: 0.00001852
Iteration 72/1000 | Loss: 0.00001851
Iteration 73/1000 | Loss: 0.00001851
Iteration 74/1000 | Loss: 0.00001851
Iteration 75/1000 | Loss: 0.00001851
Iteration 76/1000 | Loss: 0.00001851
Iteration 77/1000 | Loss: 0.00001851
Iteration 78/1000 | Loss: 0.00001851
Iteration 79/1000 | Loss: 0.00001851
Iteration 80/1000 | Loss: 0.00001851
Iteration 81/1000 | Loss: 0.00001850
Iteration 82/1000 | Loss: 0.00001850
Iteration 83/1000 | Loss: 0.00001850
Iteration 84/1000 | Loss: 0.00001850
Iteration 85/1000 | Loss: 0.00001849
Iteration 86/1000 | Loss: 0.00001849
Iteration 87/1000 | Loss: 0.00001849
Iteration 88/1000 | Loss: 0.00001849
Iteration 89/1000 | Loss: 0.00001849
Iteration 90/1000 | Loss: 0.00001849
Iteration 91/1000 | Loss: 0.00001849
Iteration 92/1000 | Loss: 0.00001849
Iteration 93/1000 | Loss: 0.00001848
Iteration 94/1000 | Loss: 0.00001848
Iteration 95/1000 | Loss: 0.00001848
Iteration 96/1000 | Loss: 0.00001848
Iteration 97/1000 | Loss: 0.00001848
Iteration 98/1000 | Loss: 0.00001848
Iteration 99/1000 | Loss: 0.00001847
Iteration 100/1000 | Loss: 0.00001847
Iteration 101/1000 | Loss: 0.00001847
Iteration 102/1000 | Loss: 0.00001847
Iteration 103/1000 | Loss: 0.00001847
Iteration 104/1000 | Loss: 0.00001847
Iteration 105/1000 | Loss: 0.00001847
Iteration 106/1000 | Loss: 0.00001847
Iteration 107/1000 | Loss: 0.00001847
Iteration 108/1000 | Loss: 0.00001847
Iteration 109/1000 | Loss: 0.00001847
Iteration 110/1000 | Loss: 0.00001847
Iteration 111/1000 | Loss: 0.00001847
Iteration 112/1000 | Loss: 0.00001847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.8470116629032418e-05, 1.8470116629032418e-05, 1.8470116629032418e-05, 1.8470116629032418e-05, 1.8470116629032418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8470116629032418e-05

Optimization complete. Final v2v error: 3.6490066051483154 mm

Highest mean error: 4.3796820640563965 mm for frame 167

Lowest mean error: 2.936236619949341 mm for frame 201

Saving results

Total time: 45.6526153087616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496268
Iteration 2/25 | Loss: 0.00128814
Iteration 3/25 | Loss: 0.00117209
Iteration 4/25 | Loss: 0.00115792
Iteration 5/25 | Loss: 0.00115403
Iteration 6/25 | Loss: 0.00115293
Iteration 7/25 | Loss: 0.00115293
Iteration 8/25 | Loss: 0.00115293
Iteration 9/25 | Loss: 0.00115293
Iteration 10/25 | Loss: 0.00115293
Iteration 11/25 | Loss: 0.00115293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011529292678460479, 0.0011529292678460479, 0.0011529292678460479, 0.0011529292678460479, 0.0011529292678460479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011529292678460479

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14830852
Iteration 2/25 | Loss: 0.00292319
Iteration 3/25 | Loss: 0.00292318
Iteration 4/25 | Loss: 0.00292318
Iteration 5/25 | Loss: 0.00292318
Iteration 6/25 | Loss: 0.00292318
Iteration 7/25 | Loss: 0.00292318
Iteration 8/25 | Loss: 0.00292318
Iteration 9/25 | Loss: 0.00292318
Iteration 10/25 | Loss: 0.00292318
Iteration 11/25 | Loss: 0.00292318
Iteration 12/25 | Loss: 0.00292318
Iteration 13/25 | Loss: 0.00292318
Iteration 14/25 | Loss: 0.00292318
Iteration 15/25 | Loss: 0.00292318
Iteration 16/25 | Loss: 0.00292318
Iteration 17/25 | Loss: 0.00292318
Iteration 18/25 | Loss: 0.00292318
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0029231777880340815, 0.0029231777880340815, 0.0029231777880340815, 0.0029231777880340815, 0.0029231777880340815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029231777880340815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00292318
Iteration 2/1000 | Loss: 0.00003812
Iteration 3/1000 | Loss: 0.00003044
Iteration 4/1000 | Loss: 0.00002790
Iteration 5/1000 | Loss: 0.00002633
Iteration 6/1000 | Loss: 0.00002535
Iteration 7/1000 | Loss: 0.00002467
Iteration 8/1000 | Loss: 0.00002429
Iteration 9/1000 | Loss: 0.00002400
Iteration 10/1000 | Loss: 0.00002377
Iteration 11/1000 | Loss: 0.00002374
Iteration 12/1000 | Loss: 0.00002369
Iteration 13/1000 | Loss: 0.00002356
Iteration 14/1000 | Loss: 0.00002349
Iteration 15/1000 | Loss: 0.00002348
Iteration 16/1000 | Loss: 0.00002340
Iteration 17/1000 | Loss: 0.00002334
Iteration 18/1000 | Loss: 0.00002334
Iteration 19/1000 | Loss: 0.00002333
Iteration 20/1000 | Loss: 0.00002332
Iteration 21/1000 | Loss: 0.00002332
Iteration 22/1000 | Loss: 0.00002330
Iteration 23/1000 | Loss: 0.00002330
Iteration 24/1000 | Loss: 0.00002329
Iteration 25/1000 | Loss: 0.00002328
Iteration 26/1000 | Loss: 0.00002328
Iteration 27/1000 | Loss: 0.00002327
Iteration 28/1000 | Loss: 0.00002327
Iteration 29/1000 | Loss: 0.00002324
Iteration 30/1000 | Loss: 0.00002324
Iteration 31/1000 | Loss: 0.00002323
Iteration 32/1000 | Loss: 0.00002323
Iteration 33/1000 | Loss: 0.00002323
Iteration 34/1000 | Loss: 0.00002323
Iteration 35/1000 | Loss: 0.00002322
Iteration 36/1000 | Loss: 0.00002322
Iteration 37/1000 | Loss: 0.00002322
Iteration 38/1000 | Loss: 0.00002322
Iteration 39/1000 | Loss: 0.00002322
Iteration 40/1000 | Loss: 0.00002322
Iteration 41/1000 | Loss: 0.00002322
Iteration 42/1000 | Loss: 0.00002322
Iteration 43/1000 | Loss: 0.00002322
Iteration 44/1000 | Loss: 0.00002322
Iteration 45/1000 | Loss: 0.00002322
Iteration 46/1000 | Loss: 0.00002322
Iteration 47/1000 | Loss: 0.00002321
Iteration 48/1000 | Loss: 0.00002321
Iteration 49/1000 | Loss: 0.00002321
Iteration 50/1000 | Loss: 0.00002321
Iteration 51/1000 | Loss: 0.00002321
Iteration 52/1000 | Loss: 0.00002321
Iteration 53/1000 | Loss: 0.00002321
Iteration 54/1000 | Loss: 0.00002321
Iteration 55/1000 | Loss: 0.00002321
Iteration 56/1000 | Loss: 0.00002321
Iteration 57/1000 | Loss: 0.00002321
Iteration 58/1000 | Loss: 0.00002321
Iteration 59/1000 | Loss: 0.00002320
Iteration 60/1000 | Loss: 0.00002320
Iteration 61/1000 | Loss: 0.00002320
Iteration 62/1000 | Loss: 0.00002320
Iteration 63/1000 | Loss: 0.00002320
Iteration 64/1000 | Loss: 0.00002320
Iteration 65/1000 | Loss: 0.00002320
Iteration 66/1000 | Loss: 0.00002320
Iteration 67/1000 | Loss: 0.00002320
Iteration 68/1000 | Loss: 0.00002319
Iteration 69/1000 | Loss: 0.00002319
Iteration 70/1000 | Loss: 0.00002319
Iteration 71/1000 | Loss: 0.00002319
Iteration 72/1000 | Loss: 0.00002319
Iteration 73/1000 | Loss: 0.00002319
Iteration 74/1000 | Loss: 0.00002319
Iteration 75/1000 | Loss: 0.00002319
Iteration 76/1000 | Loss: 0.00002319
Iteration 77/1000 | Loss: 0.00002318
Iteration 78/1000 | Loss: 0.00002318
Iteration 79/1000 | Loss: 0.00002318
Iteration 80/1000 | Loss: 0.00002318
Iteration 81/1000 | Loss: 0.00002318
Iteration 82/1000 | Loss: 0.00002318
Iteration 83/1000 | Loss: 0.00002317
Iteration 84/1000 | Loss: 0.00002317
Iteration 85/1000 | Loss: 0.00002317
Iteration 86/1000 | Loss: 0.00002317
Iteration 87/1000 | Loss: 0.00002317
Iteration 88/1000 | Loss: 0.00002317
Iteration 89/1000 | Loss: 0.00002317
Iteration 90/1000 | Loss: 0.00002317
Iteration 91/1000 | Loss: 0.00002317
Iteration 92/1000 | Loss: 0.00002317
Iteration 93/1000 | Loss: 0.00002316
Iteration 94/1000 | Loss: 0.00002316
Iteration 95/1000 | Loss: 0.00002316
Iteration 96/1000 | Loss: 0.00002316
Iteration 97/1000 | Loss: 0.00002316
Iteration 98/1000 | Loss: 0.00002316
Iteration 99/1000 | Loss: 0.00002316
Iteration 100/1000 | Loss: 0.00002316
Iteration 101/1000 | Loss: 0.00002316
Iteration 102/1000 | Loss: 0.00002316
Iteration 103/1000 | Loss: 0.00002316
Iteration 104/1000 | Loss: 0.00002316
Iteration 105/1000 | Loss: 0.00002316
Iteration 106/1000 | Loss: 0.00002316
Iteration 107/1000 | Loss: 0.00002316
Iteration 108/1000 | Loss: 0.00002316
Iteration 109/1000 | Loss: 0.00002315
Iteration 110/1000 | Loss: 0.00002315
Iteration 111/1000 | Loss: 0.00002315
Iteration 112/1000 | Loss: 0.00002315
Iteration 113/1000 | Loss: 0.00002315
Iteration 114/1000 | Loss: 0.00002315
Iteration 115/1000 | Loss: 0.00002315
Iteration 116/1000 | Loss: 0.00002314
Iteration 117/1000 | Loss: 0.00002314
Iteration 118/1000 | Loss: 0.00002314
Iteration 119/1000 | Loss: 0.00002314
Iteration 120/1000 | Loss: 0.00002314
Iteration 121/1000 | Loss: 0.00002314
Iteration 122/1000 | Loss: 0.00002314
Iteration 123/1000 | Loss: 0.00002314
Iteration 124/1000 | Loss: 0.00002314
Iteration 125/1000 | Loss: 0.00002314
Iteration 126/1000 | Loss: 0.00002314
Iteration 127/1000 | Loss: 0.00002314
Iteration 128/1000 | Loss: 0.00002313
Iteration 129/1000 | Loss: 0.00002313
Iteration 130/1000 | Loss: 0.00002313
Iteration 131/1000 | Loss: 0.00002313
Iteration 132/1000 | Loss: 0.00002313
Iteration 133/1000 | Loss: 0.00002313
Iteration 134/1000 | Loss: 0.00002313
Iteration 135/1000 | Loss: 0.00002313
Iteration 136/1000 | Loss: 0.00002313
Iteration 137/1000 | Loss: 0.00002313
Iteration 138/1000 | Loss: 0.00002313
Iteration 139/1000 | Loss: 0.00002313
Iteration 140/1000 | Loss: 0.00002313
Iteration 141/1000 | Loss: 0.00002312
Iteration 142/1000 | Loss: 0.00002312
Iteration 143/1000 | Loss: 0.00002312
Iteration 144/1000 | Loss: 0.00002312
Iteration 145/1000 | Loss: 0.00002312
Iteration 146/1000 | Loss: 0.00002312
Iteration 147/1000 | Loss: 0.00002312
Iteration 148/1000 | Loss: 0.00002312
Iteration 149/1000 | Loss: 0.00002312
Iteration 150/1000 | Loss: 0.00002312
Iteration 151/1000 | Loss: 0.00002311
Iteration 152/1000 | Loss: 0.00002311
Iteration 153/1000 | Loss: 0.00002311
Iteration 154/1000 | Loss: 0.00002311
Iteration 155/1000 | Loss: 0.00002311
Iteration 156/1000 | Loss: 0.00002310
Iteration 157/1000 | Loss: 0.00002310
Iteration 158/1000 | Loss: 0.00002310
Iteration 159/1000 | Loss: 0.00002310
Iteration 160/1000 | Loss: 0.00002310
Iteration 161/1000 | Loss: 0.00002310
Iteration 162/1000 | Loss: 0.00002310
Iteration 163/1000 | Loss: 0.00002310
Iteration 164/1000 | Loss: 0.00002310
Iteration 165/1000 | Loss: 0.00002310
Iteration 166/1000 | Loss: 0.00002310
Iteration 167/1000 | Loss: 0.00002310
Iteration 168/1000 | Loss: 0.00002310
Iteration 169/1000 | Loss: 0.00002310
Iteration 170/1000 | Loss: 0.00002310
Iteration 171/1000 | Loss: 0.00002310
Iteration 172/1000 | Loss: 0.00002309
Iteration 173/1000 | Loss: 0.00002309
Iteration 174/1000 | Loss: 0.00002309
Iteration 175/1000 | Loss: 0.00002309
Iteration 176/1000 | Loss: 0.00002309
Iteration 177/1000 | Loss: 0.00002309
Iteration 178/1000 | Loss: 0.00002309
Iteration 179/1000 | Loss: 0.00002309
Iteration 180/1000 | Loss: 0.00002309
Iteration 181/1000 | Loss: 0.00002309
Iteration 182/1000 | Loss: 0.00002309
Iteration 183/1000 | Loss: 0.00002309
Iteration 184/1000 | Loss: 0.00002309
Iteration 185/1000 | Loss: 0.00002308
Iteration 186/1000 | Loss: 0.00002308
Iteration 187/1000 | Loss: 0.00002308
Iteration 188/1000 | Loss: 0.00002308
Iteration 189/1000 | Loss: 0.00002308
Iteration 190/1000 | Loss: 0.00002308
Iteration 191/1000 | Loss: 0.00002308
Iteration 192/1000 | Loss: 0.00002308
Iteration 193/1000 | Loss: 0.00002308
Iteration 194/1000 | Loss: 0.00002308
Iteration 195/1000 | Loss: 0.00002308
Iteration 196/1000 | Loss: 0.00002308
Iteration 197/1000 | Loss: 0.00002308
Iteration 198/1000 | Loss: 0.00002308
Iteration 199/1000 | Loss: 0.00002308
Iteration 200/1000 | Loss: 0.00002308
Iteration 201/1000 | Loss: 0.00002308
Iteration 202/1000 | Loss: 0.00002307
Iteration 203/1000 | Loss: 0.00002307
Iteration 204/1000 | Loss: 0.00002307
Iteration 205/1000 | Loss: 0.00002307
Iteration 206/1000 | Loss: 0.00002307
Iteration 207/1000 | Loss: 0.00002307
Iteration 208/1000 | Loss: 0.00002307
Iteration 209/1000 | Loss: 0.00002307
Iteration 210/1000 | Loss: 0.00002307
Iteration 211/1000 | Loss: 0.00002307
Iteration 212/1000 | Loss: 0.00002307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.307225986442063e-05, 2.307225986442063e-05, 2.307225986442063e-05, 2.307225986442063e-05, 2.307225986442063e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.307225986442063e-05

Optimization complete. Final v2v error: 3.757071018218994 mm

Highest mean error: 5.128268718719482 mm for frame 83

Lowest mean error: 3.2041592597961426 mm for frame 107

Saving results

Total time: 39.54649472236633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565065
Iteration 2/25 | Loss: 0.00135812
Iteration 3/25 | Loss: 0.00118370
Iteration 4/25 | Loss: 0.00117247
Iteration 5/25 | Loss: 0.00117053
Iteration 6/25 | Loss: 0.00117053
Iteration 7/25 | Loss: 0.00117053
Iteration 8/25 | Loss: 0.00117053
Iteration 9/25 | Loss: 0.00117053
Iteration 10/25 | Loss: 0.00117053
Iteration 11/25 | Loss: 0.00117053
Iteration 12/25 | Loss: 0.00117053
Iteration 13/25 | Loss: 0.00117053
Iteration 14/25 | Loss: 0.00117053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011705296346917748, 0.0011705296346917748, 0.0011705296346917748, 0.0011705296346917748, 0.0011705296346917748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011705296346917748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51430273
Iteration 2/25 | Loss: 0.00349267
Iteration 3/25 | Loss: 0.00349265
Iteration 4/25 | Loss: 0.00349265
Iteration 5/25 | Loss: 0.00349265
Iteration 6/25 | Loss: 0.00349265
Iteration 7/25 | Loss: 0.00349265
Iteration 8/25 | Loss: 0.00349265
Iteration 9/25 | Loss: 0.00349265
Iteration 10/25 | Loss: 0.00349265
Iteration 11/25 | Loss: 0.00349265
Iteration 12/25 | Loss: 0.00349265
Iteration 13/25 | Loss: 0.00349265
Iteration 14/25 | Loss: 0.00349265
Iteration 15/25 | Loss: 0.00349265
Iteration 16/25 | Loss: 0.00349265
Iteration 17/25 | Loss: 0.00349265
Iteration 18/25 | Loss: 0.00349265
Iteration 19/25 | Loss: 0.00349265
Iteration 20/25 | Loss: 0.00349265
Iteration 21/25 | Loss: 0.00349265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0034926480147987604, 0.0034926480147987604, 0.0034926480147987604, 0.0034926480147987604, 0.0034926480147987604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034926480147987604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00349265
Iteration 2/1000 | Loss: 0.00005459
Iteration 3/1000 | Loss: 0.00003003
Iteration 4/1000 | Loss: 0.00002549
Iteration 5/1000 | Loss: 0.00002384
Iteration 6/1000 | Loss: 0.00002273
Iteration 7/1000 | Loss: 0.00002209
Iteration 8/1000 | Loss: 0.00002168
Iteration 9/1000 | Loss: 0.00002136
Iteration 10/1000 | Loss: 0.00002109
Iteration 11/1000 | Loss: 0.00002087
Iteration 12/1000 | Loss: 0.00002067
Iteration 13/1000 | Loss: 0.00002050
Iteration 14/1000 | Loss: 0.00002039
Iteration 15/1000 | Loss: 0.00002035
Iteration 16/1000 | Loss: 0.00002031
Iteration 17/1000 | Loss: 0.00002028
Iteration 18/1000 | Loss: 0.00002028
Iteration 19/1000 | Loss: 0.00002028
Iteration 20/1000 | Loss: 0.00002027
Iteration 21/1000 | Loss: 0.00002026
Iteration 22/1000 | Loss: 0.00002025
Iteration 23/1000 | Loss: 0.00002020
Iteration 24/1000 | Loss: 0.00002015
Iteration 25/1000 | Loss: 0.00002013
Iteration 26/1000 | Loss: 0.00002012
Iteration 27/1000 | Loss: 0.00002010
Iteration 28/1000 | Loss: 0.00002009
Iteration 29/1000 | Loss: 0.00002009
Iteration 30/1000 | Loss: 0.00002008
Iteration 31/1000 | Loss: 0.00002006
Iteration 32/1000 | Loss: 0.00002005
Iteration 33/1000 | Loss: 0.00002005
Iteration 34/1000 | Loss: 0.00002004
Iteration 35/1000 | Loss: 0.00002004
Iteration 36/1000 | Loss: 0.00002003
Iteration 37/1000 | Loss: 0.00002002
Iteration 38/1000 | Loss: 0.00002002
Iteration 39/1000 | Loss: 0.00001997
Iteration 40/1000 | Loss: 0.00001997
Iteration 41/1000 | Loss: 0.00001995
Iteration 42/1000 | Loss: 0.00001995
Iteration 43/1000 | Loss: 0.00001991
Iteration 44/1000 | Loss: 0.00001989
Iteration 45/1000 | Loss: 0.00001985
Iteration 46/1000 | Loss: 0.00001985
Iteration 47/1000 | Loss: 0.00001983
Iteration 48/1000 | Loss: 0.00001982
Iteration 49/1000 | Loss: 0.00001982
Iteration 50/1000 | Loss: 0.00001981
Iteration 51/1000 | Loss: 0.00001981
Iteration 52/1000 | Loss: 0.00001981
Iteration 53/1000 | Loss: 0.00001981
Iteration 54/1000 | Loss: 0.00001980
Iteration 55/1000 | Loss: 0.00001980
Iteration 56/1000 | Loss: 0.00001980
Iteration 57/1000 | Loss: 0.00001980
Iteration 58/1000 | Loss: 0.00001979
Iteration 59/1000 | Loss: 0.00001979
Iteration 60/1000 | Loss: 0.00001978
Iteration 61/1000 | Loss: 0.00001978
Iteration 62/1000 | Loss: 0.00001978
Iteration 63/1000 | Loss: 0.00001977
Iteration 64/1000 | Loss: 0.00001977
Iteration 65/1000 | Loss: 0.00001977
Iteration 66/1000 | Loss: 0.00001976
Iteration 67/1000 | Loss: 0.00001976
Iteration 68/1000 | Loss: 0.00001976
Iteration 69/1000 | Loss: 0.00001975
Iteration 70/1000 | Loss: 0.00001975
Iteration 71/1000 | Loss: 0.00001974
Iteration 72/1000 | Loss: 0.00001974
Iteration 73/1000 | Loss: 0.00001974
Iteration 74/1000 | Loss: 0.00001973
Iteration 75/1000 | Loss: 0.00001973
Iteration 76/1000 | Loss: 0.00001973
Iteration 77/1000 | Loss: 0.00001973
Iteration 78/1000 | Loss: 0.00001972
Iteration 79/1000 | Loss: 0.00001972
Iteration 80/1000 | Loss: 0.00001971
Iteration 81/1000 | Loss: 0.00001971
Iteration 82/1000 | Loss: 0.00001971
Iteration 83/1000 | Loss: 0.00001970
Iteration 84/1000 | Loss: 0.00001970
Iteration 85/1000 | Loss: 0.00001970
Iteration 86/1000 | Loss: 0.00001970
Iteration 87/1000 | Loss: 0.00001970
Iteration 88/1000 | Loss: 0.00001970
Iteration 89/1000 | Loss: 0.00001970
Iteration 90/1000 | Loss: 0.00001970
Iteration 91/1000 | Loss: 0.00001969
Iteration 92/1000 | Loss: 0.00001968
Iteration 93/1000 | Loss: 0.00001968
Iteration 94/1000 | Loss: 0.00001968
Iteration 95/1000 | Loss: 0.00001968
Iteration 96/1000 | Loss: 0.00001968
Iteration 97/1000 | Loss: 0.00001968
Iteration 98/1000 | Loss: 0.00001968
Iteration 99/1000 | Loss: 0.00001968
Iteration 100/1000 | Loss: 0.00001967
Iteration 101/1000 | Loss: 0.00001967
Iteration 102/1000 | Loss: 0.00001967
Iteration 103/1000 | Loss: 0.00001966
Iteration 104/1000 | Loss: 0.00001965
Iteration 105/1000 | Loss: 0.00001965
Iteration 106/1000 | Loss: 0.00001964
Iteration 107/1000 | Loss: 0.00001964
Iteration 108/1000 | Loss: 0.00001963
Iteration 109/1000 | Loss: 0.00001963
Iteration 110/1000 | Loss: 0.00001962
Iteration 111/1000 | Loss: 0.00001962
Iteration 112/1000 | Loss: 0.00001962
Iteration 113/1000 | Loss: 0.00001961
Iteration 114/1000 | Loss: 0.00001961
Iteration 115/1000 | Loss: 0.00001961
Iteration 116/1000 | Loss: 0.00001961
Iteration 117/1000 | Loss: 0.00001961
Iteration 118/1000 | Loss: 0.00001961
Iteration 119/1000 | Loss: 0.00001961
Iteration 120/1000 | Loss: 0.00001961
Iteration 121/1000 | Loss: 0.00001960
Iteration 122/1000 | Loss: 0.00001960
Iteration 123/1000 | Loss: 0.00001960
Iteration 124/1000 | Loss: 0.00001960
Iteration 125/1000 | Loss: 0.00001960
Iteration 126/1000 | Loss: 0.00001959
Iteration 127/1000 | Loss: 0.00001959
Iteration 128/1000 | Loss: 0.00001958
Iteration 129/1000 | Loss: 0.00001958
Iteration 130/1000 | Loss: 0.00001958
Iteration 131/1000 | Loss: 0.00001957
Iteration 132/1000 | Loss: 0.00001957
Iteration 133/1000 | Loss: 0.00001957
Iteration 134/1000 | Loss: 0.00001957
Iteration 135/1000 | Loss: 0.00001957
Iteration 136/1000 | Loss: 0.00001956
Iteration 137/1000 | Loss: 0.00001956
Iteration 138/1000 | Loss: 0.00001956
Iteration 139/1000 | Loss: 0.00001956
Iteration 140/1000 | Loss: 0.00001956
Iteration 141/1000 | Loss: 0.00001956
Iteration 142/1000 | Loss: 0.00001956
Iteration 143/1000 | Loss: 0.00001956
Iteration 144/1000 | Loss: 0.00001956
Iteration 145/1000 | Loss: 0.00001956
Iteration 146/1000 | Loss: 0.00001956
Iteration 147/1000 | Loss: 0.00001956
Iteration 148/1000 | Loss: 0.00001956
Iteration 149/1000 | Loss: 0.00001955
Iteration 150/1000 | Loss: 0.00001955
Iteration 151/1000 | Loss: 0.00001955
Iteration 152/1000 | Loss: 0.00001955
Iteration 153/1000 | Loss: 0.00001955
Iteration 154/1000 | Loss: 0.00001955
Iteration 155/1000 | Loss: 0.00001955
Iteration 156/1000 | Loss: 0.00001955
Iteration 157/1000 | Loss: 0.00001955
Iteration 158/1000 | Loss: 0.00001955
Iteration 159/1000 | Loss: 0.00001955
Iteration 160/1000 | Loss: 0.00001955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.9545957911759615e-05, 1.9545957911759615e-05, 1.9545957911759615e-05, 1.9545957911759615e-05, 1.9545957911759615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9545957911759615e-05

Optimization complete. Final v2v error: 3.5349316596984863 mm

Highest mean error: 4.263782978057861 mm for frame 232

Lowest mean error: 3.031832456588745 mm for frame 4

Saving results

Total time: 48.54616093635559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835878
Iteration 2/25 | Loss: 0.00168529
Iteration 3/25 | Loss: 0.00129728
Iteration 4/25 | Loss: 0.00126709
Iteration 5/25 | Loss: 0.00125758
Iteration 6/25 | Loss: 0.00125513
Iteration 7/25 | Loss: 0.00125513
Iteration 8/25 | Loss: 0.00125513
Iteration 9/25 | Loss: 0.00125513
Iteration 10/25 | Loss: 0.00125513
Iteration 11/25 | Loss: 0.00125513
Iteration 12/25 | Loss: 0.00125513
Iteration 13/25 | Loss: 0.00125513
Iteration 14/25 | Loss: 0.00125513
Iteration 15/25 | Loss: 0.00125513
Iteration 16/25 | Loss: 0.00125513
Iteration 17/25 | Loss: 0.00125513
Iteration 18/25 | Loss: 0.00125513
Iteration 19/25 | Loss: 0.00125513
Iteration 20/25 | Loss: 0.00125513
Iteration 21/25 | Loss: 0.00125513
Iteration 22/25 | Loss: 0.00125513
Iteration 23/25 | Loss: 0.00125513
Iteration 24/25 | Loss: 0.00125513
Iteration 25/25 | Loss: 0.00125513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01277244
Iteration 2/25 | Loss: 0.00338173
Iteration 3/25 | Loss: 0.00338173
Iteration 4/25 | Loss: 0.00338173
Iteration 5/25 | Loss: 0.00338173
Iteration 6/25 | Loss: 0.00338173
Iteration 7/25 | Loss: 0.00338173
Iteration 8/25 | Loss: 0.00338173
Iteration 9/25 | Loss: 0.00338173
Iteration 10/25 | Loss: 0.00338173
Iteration 11/25 | Loss: 0.00338173
Iteration 12/25 | Loss: 0.00338173
Iteration 13/25 | Loss: 0.00338173
Iteration 14/25 | Loss: 0.00338173
Iteration 15/25 | Loss: 0.00338173
Iteration 16/25 | Loss: 0.00338173
Iteration 17/25 | Loss: 0.00338173
Iteration 18/25 | Loss: 0.00338173
Iteration 19/25 | Loss: 0.00338173
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003381728893145919, 0.003381728893145919, 0.003381728893145919, 0.003381728893145919, 0.003381728893145919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003381728893145919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00338173
Iteration 2/1000 | Loss: 0.00007856
Iteration 3/1000 | Loss: 0.00005021
Iteration 4/1000 | Loss: 0.00003982
Iteration 5/1000 | Loss: 0.00003564
Iteration 6/1000 | Loss: 0.00003391
Iteration 7/1000 | Loss: 0.00003257
Iteration 8/1000 | Loss: 0.00003163
Iteration 9/1000 | Loss: 0.00003084
Iteration 10/1000 | Loss: 0.00003035
Iteration 11/1000 | Loss: 0.00002987
Iteration 12/1000 | Loss: 0.00002944
Iteration 13/1000 | Loss: 0.00002912
Iteration 14/1000 | Loss: 0.00002890
Iteration 15/1000 | Loss: 0.00002882
Iteration 16/1000 | Loss: 0.00002867
Iteration 17/1000 | Loss: 0.00002862
Iteration 18/1000 | Loss: 0.00002849
Iteration 19/1000 | Loss: 0.00002847
Iteration 20/1000 | Loss: 0.00002847
Iteration 21/1000 | Loss: 0.00002846
Iteration 22/1000 | Loss: 0.00002845
Iteration 23/1000 | Loss: 0.00002845
Iteration 24/1000 | Loss: 0.00002844
Iteration 25/1000 | Loss: 0.00002844
Iteration 26/1000 | Loss: 0.00002843
Iteration 27/1000 | Loss: 0.00002843
Iteration 28/1000 | Loss: 0.00002843
Iteration 29/1000 | Loss: 0.00002841
Iteration 30/1000 | Loss: 0.00002841
Iteration 31/1000 | Loss: 0.00002841
Iteration 32/1000 | Loss: 0.00002841
Iteration 33/1000 | Loss: 0.00002840
Iteration 34/1000 | Loss: 0.00002840
Iteration 35/1000 | Loss: 0.00002840
Iteration 36/1000 | Loss: 0.00002840
Iteration 37/1000 | Loss: 0.00002840
Iteration 38/1000 | Loss: 0.00002840
Iteration 39/1000 | Loss: 0.00002840
Iteration 40/1000 | Loss: 0.00002840
Iteration 41/1000 | Loss: 0.00002839
Iteration 42/1000 | Loss: 0.00002839
Iteration 43/1000 | Loss: 0.00002836
Iteration 44/1000 | Loss: 0.00002836
Iteration 45/1000 | Loss: 0.00002835
Iteration 46/1000 | Loss: 0.00002835
Iteration 47/1000 | Loss: 0.00002833
Iteration 48/1000 | Loss: 0.00002833
Iteration 49/1000 | Loss: 0.00002832
Iteration 50/1000 | Loss: 0.00002832
Iteration 51/1000 | Loss: 0.00002832
Iteration 52/1000 | Loss: 0.00002832
Iteration 53/1000 | Loss: 0.00002832
Iteration 54/1000 | Loss: 0.00002831
Iteration 55/1000 | Loss: 0.00002829
Iteration 56/1000 | Loss: 0.00002829
Iteration 57/1000 | Loss: 0.00002829
Iteration 58/1000 | Loss: 0.00002828
Iteration 59/1000 | Loss: 0.00002828
Iteration 60/1000 | Loss: 0.00002828
Iteration 61/1000 | Loss: 0.00002828
Iteration 62/1000 | Loss: 0.00002828
Iteration 63/1000 | Loss: 0.00002828
Iteration 64/1000 | Loss: 0.00002828
Iteration 65/1000 | Loss: 0.00002828
Iteration 66/1000 | Loss: 0.00002827
Iteration 67/1000 | Loss: 0.00002827
Iteration 68/1000 | Loss: 0.00002826
Iteration 69/1000 | Loss: 0.00002826
Iteration 70/1000 | Loss: 0.00002826
Iteration 71/1000 | Loss: 0.00002825
Iteration 72/1000 | Loss: 0.00002825
Iteration 73/1000 | Loss: 0.00002825
Iteration 74/1000 | Loss: 0.00002824
Iteration 75/1000 | Loss: 0.00002824
Iteration 76/1000 | Loss: 0.00002824
Iteration 77/1000 | Loss: 0.00002823
Iteration 78/1000 | Loss: 0.00002823
Iteration 79/1000 | Loss: 0.00002823
Iteration 80/1000 | Loss: 0.00002822
Iteration 81/1000 | Loss: 0.00002822
Iteration 82/1000 | Loss: 0.00002822
Iteration 83/1000 | Loss: 0.00002822
Iteration 84/1000 | Loss: 0.00002821
Iteration 85/1000 | Loss: 0.00002821
Iteration 86/1000 | Loss: 0.00002821
Iteration 87/1000 | Loss: 0.00002821
Iteration 88/1000 | Loss: 0.00002821
Iteration 89/1000 | Loss: 0.00002821
Iteration 90/1000 | Loss: 0.00002821
Iteration 91/1000 | Loss: 0.00002821
Iteration 92/1000 | Loss: 0.00002820
Iteration 93/1000 | Loss: 0.00002820
Iteration 94/1000 | Loss: 0.00002820
Iteration 95/1000 | Loss: 0.00002818
Iteration 96/1000 | Loss: 0.00002818
Iteration 97/1000 | Loss: 0.00002818
Iteration 98/1000 | Loss: 0.00002818
Iteration 99/1000 | Loss: 0.00002818
Iteration 100/1000 | Loss: 0.00002818
Iteration 101/1000 | Loss: 0.00002818
Iteration 102/1000 | Loss: 0.00002818
Iteration 103/1000 | Loss: 0.00002818
Iteration 104/1000 | Loss: 0.00002818
Iteration 105/1000 | Loss: 0.00002818
Iteration 106/1000 | Loss: 0.00002818
Iteration 107/1000 | Loss: 0.00002818
Iteration 108/1000 | Loss: 0.00002818
Iteration 109/1000 | Loss: 0.00002818
Iteration 110/1000 | Loss: 0.00002817
Iteration 111/1000 | Loss: 0.00002816
Iteration 112/1000 | Loss: 0.00002816
Iteration 113/1000 | Loss: 0.00002816
Iteration 114/1000 | Loss: 0.00002816
Iteration 115/1000 | Loss: 0.00002815
Iteration 116/1000 | Loss: 0.00002815
Iteration 117/1000 | Loss: 0.00002815
Iteration 118/1000 | Loss: 0.00002814
Iteration 119/1000 | Loss: 0.00002814
Iteration 120/1000 | Loss: 0.00002814
Iteration 121/1000 | Loss: 0.00002814
Iteration 122/1000 | Loss: 0.00002814
Iteration 123/1000 | Loss: 0.00002814
Iteration 124/1000 | Loss: 0.00002813
Iteration 125/1000 | Loss: 0.00002813
Iteration 126/1000 | Loss: 0.00002813
Iteration 127/1000 | Loss: 0.00002813
Iteration 128/1000 | Loss: 0.00002813
Iteration 129/1000 | Loss: 0.00002813
Iteration 130/1000 | Loss: 0.00002813
Iteration 131/1000 | Loss: 0.00002813
Iteration 132/1000 | Loss: 0.00002812
Iteration 133/1000 | Loss: 0.00002812
Iteration 134/1000 | Loss: 0.00002811
Iteration 135/1000 | Loss: 0.00002811
Iteration 136/1000 | Loss: 0.00002811
Iteration 137/1000 | Loss: 0.00002810
Iteration 138/1000 | Loss: 0.00002810
Iteration 139/1000 | Loss: 0.00002809
Iteration 140/1000 | Loss: 0.00002809
Iteration 141/1000 | Loss: 0.00002809
Iteration 142/1000 | Loss: 0.00002809
Iteration 143/1000 | Loss: 0.00002808
Iteration 144/1000 | Loss: 0.00002808
Iteration 145/1000 | Loss: 0.00002808
Iteration 146/1000 | Loss: 0.00002808
Iteration 147/1000 | Loss: 0.00002808
Iteration 148/1000 | Loss: 0.00002808
Iteration 149/1000 | Loss: 0.00002807
Iteration 150/1000 | Loss: 0.00002807
Iteration 151/1000 | Loss: 0.00002806
Iteration 152/1000 | Loss: 0.00002806
Iteration 153/1000 | Loss: 0.00002806
Iteration 154/1000 | Loss: 0.00002805
Iteration 155/1000 | Loss: 0.00002805
Iteration 156/1000 | Loss: 0.00002804
Iteration 157/1000 | Loss: 0.00002804
Iteration 158/1000 | Loss: 0.00002804
Iteration 159/1000 | Loss: 0.00002804
Iteration 160/1000 | Loss: 0.00002804
Iteration 161/1000 | Loss: 0.00002804
Iteration 162/1000 | Loss: 0.00002804
Iteration 163/1000 | Loss: 0.00002803
Iteration 164/1000 | Loss: 0.00002803
Iteration 165/1000 | Loss: 0.00002803
Iteration 166/1000 | Loss: 0.00002803
Iteration 167/1000 | Loss: 0.00002803
Iteration 168/1000 | Loss: 0.00002803
Iteration 169/1000 | Loss: 0.00002803
Iteration 170/1000 | Loss: 0.00002803
Iteration 171/1000 | Loss: 0.00002803
Iteration 172/1000 | Loss: 0.00002803
Iteration 173/1000 | Loss: 0.00002803
Iteration 174/1000 | Loss: 0.00002802
Iteration 175/1000 | Loss: 0.00002802
Iteration 176/1000 | Loss: 0.00002802
Iteration 177/1000 | Loss: 0.00002802
Iteration 178/1000 | Loss: 0.00002802
Iteration 179/1000 | Loss: 0.00002802
Iteration 180/1000 | Loss: 0.00002802
Iteration 181/1000 | Loss: 0.00002801
Iteration 182/1000 | Loss: 0.00002801
Iteration 183/1000 | Loss: 0.00002801
Iteration 184/1000 | Loss: 0.00002801
Iteration 185/1000 | Loss: 0.00002801
Iteration 186/1000 | Loss: 0.00002801
Iteration 187/1000 | Loss: 0.00002801
Iteration 188/1000 | Loss: 0.00002801
Iteration 189/1000 | Loss: 0.00002801
Iteration 190/1000 | Loss: 0.00002801
Iteration 191/1000 | Loss: 0.00002801
Iteration 192/1000 | Loss: 0.00002801
Iteration 193/1000 | Loss: 0.00002801
Iteration 194/1000 | Loss: 0.00002801
Iteration 195/1000 | Loss: 0.00002800
Iteration 196/1000 | Loss: 0.00002800
Iteration 197/1000 | Loss: 0.00002800
Iteration 198/1000 | Loss: 0.00002800
Iteration 199/1000 | Loss: 0.00002800
Iteration 200/1000 | Loss: 0.00002800
Iteration 201/1000 | Loss: 0.00002800
Iteration 202/1000 | Loss: 0.00002800
Iteration 203/1000 | Loss: 0.00002800
Iteration 204/1000 | Loss: 0.00002800
Iteration 205/1000 | Loss: 0.00002800
Iteration 206/1000 | Loss: 0.00002800
Iteration 207/1000 | Loss: 0.00002800
Iteration 208/1000 | Loss: 0.00002800
Iteration 209/1000 | Loss: 0.00002800
Iteration 210/1000 | Loss: 0.00002800
Iteration 211/1000 | Loss: 0.00002800
Iteration 212/1000 | Loss: 0.00002800
Iteration 213/1000 | Loss: 0.00002800
Iteration 214/1000 | Loss: 0.00002800
Iteration 215/1000 | Loss: 0.00002800
Iteration 216/1000 | Loss: 0.00002800
Iteration 217/1000 | Loss: 0.00002800
Iteration 218/1000 | Loss: 0.00002800
Iteration 219/1000 | Loss: 0.00002800
Iteration 220/1000 | Loss: 0.00002800
Iteration 221/1000 | Loss: 0.00002800
Iteration 222/1000 | Loss: 0.00002800
Iteration 223/1000 | Loss: 0.00002800
Iteration 224/1000 | Loss: 0.00002800
Iteration 225/1000 | Loss: 0.00002800
Iteration 226/1000 | Loss: 0.00002800
Iteration 227/1000 | Loss: 0.00002800
Iteration 228/1000 | Loss: 0.00002800
Iteration 229/1000 | Loss: 0.00002800
Iteration 230/1000 | Loss: 0.00002800
Iteration 231/1000 | Loss: 0.00002800
Iteration 232/1000 | Loss: 0.00002800
Iteration 233/1000 | Loss: 0.00002800
Iteration 234/1000 | Loss: 0.00002800
Iteration 235/1000 | Loss: 0.00002800
Iteration 236/1000 | Loss: 0.00002800
Iteration 237/1000 | Loss: 0.00002800
Iteration 238/1000 | Loss: 0.00002800
Iteration 239/1000 | Loss: 0.00002800
Iteration 240/1000 | Loss: 0.00002800
Iteration 241/1000 | Loss: 0.00002800
Iteration 242/1000 | Loss: 0.00002800
Iteration 243/1000 | Loss: 0.00002800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [2.7997823053738102e-05, 2.7997823053738102e-05, 2.7997823053738102e-05, 2.7997823053738102e-05, 2.7997823053738102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7997823053738102e-05

Optimization complete. Final v2v error: 4.205674171447754 mm

Highest mean error: 5.114411354064941 mm for frame 146

Lowest mean error: 3.5471065044403076 mm for frame 31

Saving results

Total time: 48.93528747558594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408514
Iteration 2/25 | Loss: 0.00127489
Iteration 3/25 | Loss: 0.00116845
Iteration 4/25 | Loss: 0.00115064
Iteration 5/25 | Loss: 0.00114633
Iteration 6/25 | Loss: 0.00114503
Iteration 7/25 | Loss: 0.00114485
Iteration 8/25 | Loss: 0.00114485
Iteration 9/25 | Loss: 0.00114485
Iteration 10/25 | Loss: 0.00114485
Iteration 11/25 | Loss: 0.00114485
Iteration 12/25 | Loss: 0.00114485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011448515579104424, 0.0011448515579104424, 0.0011448515579104424, 0.0011448515579104424, 0.0011448515579104424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011448515579104424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78647482
Iteration 2/25 | Loss: 0.00299887
Iteration 3/25 | Loss: 0.00299886
Iteration 4/25 | Loss: 0.00299886
Iteration 5/25 | Loss: 0.00299886
Iteration 6/25 | Loss: 0.00299886
Iteration 7/25 | Loss: 0.00299886
Iteration 8/25 | Loss: 0.00299886
Iteration 9/25 | Loss: 0.00299886
Iteration 10/25 | Loss: 0.00299886
Iteration 11/25 | Loss: 0.00299886
Iteration 12/25 | Loss: 0.00299886
Iteration 13/25 | Loss: 0.00299886
Iteration 14/25 | Loss: 0.00299886
Iteration 15/25 | Loss: 0.00299886
Iteration 16/25 | Loss: 0.00299886
Iteration 17/25 | Loss: 0.00299886
Iteration 18/25 | Loss: 0.00299886
Iteration 19/25 | Loss: 0.00299886
Iteration 20/25 | Loss: 0.00299886
Iteration 21/25 | Loss: 0.00299886
Iteration 22/25 | Loss: 0.00299886
Iteration 23/25 | Loss: 0.00299886
Iteration 24/25 | Loss: 0.00299886
Iteration 25/25 | Loss: 0.00299886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00299886
Iteration 2/1000 | Loss: 0.00003310
Iteration 3/1000 | Loss: 0.00002544
Iteration 4/1000 | Loss: 0.00002343
Iteration 5/1000 | Loss: 0.00002181
Iteration 6/1000 | Loss: 0.00002062
Iteration 7/1000 | Loss: 0.00002002
Iteration 8/1000 | Loss: 0.00001941
Iteration 9/1000 | Loss: 0.00001894
Iteration 10/1000 | Loss: 0.00001861
Iteration 11/1000 | Loss: 0.00001837
Iteration 12/1000 | Loss: 0.00001818
Iteration 13/1000 | Loss: 0.00001813
Iteration 14/1000 | Loss: 0.00001809
Iteration 15/1000 | Loss: 0.00001806
Iteration 16/1000 | Loss: 0.00001806
Iteration 17/1000 | Loss: 0.00001805
Iteration 18/1000 | Loss: 0.00001805
Iteration 19/1000 | Loss: 0.00001804
Iteration 20/1000 | Loss: 0.00001802
Iteration 21/1000 | Loss: 0.00001802
Iteration 22/1000 | Loss: 0.00001799
Iteration 23/1000 | Loss: 0.00001799
Iteration 24/1000 | Loss: 0.00001798
Iteration 25/1000 | Loss: 0.00001798
Iteration 26/1000 | Loss: 0.00001798
Iteration 27/1000 | Loss: 0.00001797
Iteration 28/1000 | Loss: 0.00001793
Iteration 29/1000 | Loss: 0.00001793
Iteration 30/1000 | Loss: 0.00001793
Iteration 31/1000 | Loss: 0.00001793
Iteration 32/1000 | Loss: 0.00001792
Iteration 33/1000 | Loss: 0.00001792
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001789
Iteration 37/1000 | Loss: 0.00001788
Iteration 38/1000 | Loss: 0.00001788
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001787
Iteration 42/1000 | Loss: 0.00001787
Iteration 43/1000 | Loss: 0.00001786
Iteration 44/1000 | Loss: 0.00001786
Iteration 45/1000 | Loss: 0.00001785
Iteration 46/1000 | Loss: 0.00001785
Iteration 47/1000 | Loss: 0.00001785
Iteration 48/1000 | Loss: 0.00001785
Iteration 49/1000 | Loss: 0.00001785
Iteration 50/1000 | Loss: 0.00001785
Iteration 51/1000 | Loss: 0.00001785
Iteration 52/1000 | Loss: 0.00001785
Iteration 53/1000 | Loss: 0.00001785
Iteration 54/1000 | Loss: 0.00001785
Iteration 55/1000 | Loss: 0.00001784
Iteration 56/1000 | Loss: 0.00001784
Iteration 57/1000 | Loss: 0.00001784
Iteration 58/1000 | Loss: 0.00001784
Iteration 59/1000 | Loss: 0.00001784
Iteration 60/1000 | Loss: 0.00001784
Iteration 61/1000 | Loss: 0.00001783
Iteration 62/1000 | Loss: 0.00001783
Iteration 63/1000 | Loss: 0.00001783
Iteration 64/1000 | Loss: 0.00001783
Iteration 65/1000 | Loss: 0.00001783
Iteration 66/1000 | Loss: 0.00001783
Iteration 67/1000 | Loss: 0.00001782
Iteration 68/1000 | Loss: 0.00001782
Iteration 69/1000 | Loss: 0.00001782
Iteration 70/1000 | Loss: 0.00001782
Iteration 71/1000 | Loss: 0.00001782
Iteration 72/1000 | Loss: 0.00001782
Iteration 73/1000 | Loss: 0.00001782
Iteration 74/1000 | Loss: 0.00001782
Iteration 75/1000 | Loss: 0.00001782
Iteration 76/1000 | Loss: 0.00001782
Iteration 77/1000 | Loss: 0.00001782
Iteration 78/1000 | Loss: 0.00001782
Iteration 79/1000 | Loss: 0.00001782
Iteration 80/1000 | Loss: 0.00001782
Iteration 81/1000 | Loss: 0.00001782
Iteration 82/1000 | Loss: 0.00001781
Iteration 83/1000 | Loss: 0.00001781
Iteration 84/1000 | Loss: 0.00001781
Iteration 85/1000 | Loss: 0.00001781
Iteration 86/1000 | Loss: 0.00001781
Iteration 87/1000 | Loss: 0.00001781
Iteration 88/1000 | Loss: 0.00001781
Iteration 89/1000 | Loss: 0.00001781
Iteration 90/1000 | Loss: 0.00001781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.7814103557611816e-05, 1.7814103557611816e-05, 1.7814103557611816e-05, 1.7814103557611816e-05, 1.7814103557611816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7814103557611816e-05

Optimization complete. Final v2v error: 3.531054735183716 mm

Highest mean error: 4.124216556549072 mm for frame 133

Lowest mean error: 3.0510740280151367 mm for frame 44

Saving results

Total time: 32.737754583358765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_1683/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_1683/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892919
Iteration 2/25 | Loss: 0.00126050
Iteration 3/25 | Loss: 0.00117131
Iteration 4/25 | Loss: 0.00115280
Iteration 5/25 | Loss: 0.00114805
Iteration 6/25 | Loss: 0.00114734
Iteration 7/25 | Loss: 0.00114734
Iteration 8/25 | Loss: 0.00114734
Iteration 9/25 | Loss: 0.00114734
Iteration 10/25 | Loss: 0.00114734
Iteration 11/25 | Loss: 0.00114734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011473430786281824, 0.0011473430786281824, 0.0011473430786281824, 0.0011473430786281824, 0.0011473430786281824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011473430786281824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12105441
Iteration 2/25 | Loss: 0.00305534
Iteration 3/25 | Loss: 0.00305534
Iteration 4/25 | Loss: 0.00305533
Iteration 5/25 | Loss: 0.00305533
Iteration 6/25 | Loss: 0.00305533
Iteration 7/25 | Loss: 0.00305533
Iteration 8/25 | Loss: 0.00305533
Iteration 9/25 | Loss: 0.00305533
Iteration 10/25 | Loss: 0.00305533
Iteration 11/25 | Loss: 0.00305533
Iteration 12/25 | Loss: 0.00305533
Iteration 13/25 | Loss: 0.00305533
Iteration 14/25 | Loss: 0.00305533
Iteration 15/25 | Loss: 0.00305533
Iteration 16/25 | Loss: 0.00305533
Iteration 17/25 | Loss: 0.00305533
Iteration 18/25 | Loss: 0.00305533
Iteration 19/25 | Loss: 0.00305533
Iteration 20/25 | Loss: 0.00305533
Iteration 21/25 | Loss: 0.00305533
Iteration 22/25 | Loss: 0.00305533
Iteration 23/25 | Loss: 0.00305533
Iteration 24/25 | Loss: 0.00305533
Iteration 25/25 | Loss: 0.00305533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00305533
Iteration 2/1000 | Loss: 0.00002162
Iteration 3/1000 | Loss: 0.00001490
Iteration 4/1000 | Loss: 0.00001316
Iteration 5/1000 | Loss: 0.00001223
Iteration 6/1000 | Loss: 0.00001174
Iteration 7/1000 | Loss: 0.00001145
Iteration 8/1000 | Loss: 0.00001141
Iteration 9/1000 | Loss: 0.00001118
Iteration 10/1000 | Loss: 0.00001102
Iteration 11/1000 | Loss: 0.00001097
Iteration 12/1000 | Loss: 0.00001092
Iteration 13/1000 | Loss: 0.00001084
Iteration 14/1000 | Loss: 0.00001084
Iteration 15/1000 | Loss: 0.00001084
Iteration 16/1000 | Loss: 0.00001083
Iteration 17/1000 | Loss: 0.00001083
Iteration 18/1000 | Loss: 0.00001083
Iteration 19/1000 | Loss: 0.00001079
Iteration 20/1000 | Loss: 0.00001079
Iteration 21/1000 | Loss: 0.00001078
Iteration 22/1000 | Loss: 0.00001078
Iteration 23/1000 | Loss: 0.00001077
Iteration 24/1000 | Loss: 0.00001077
Iteration 25/1000 | Loss: 0.00001077
Iteration 26/1000 | Loss: 0.00001077
Iteration 27/1000 | Loss: 0.00001076
Iteration 28/1000 | Loss: 0.00001076
Iteration 29/1000 | Loss: 0.00001076
Iteration 30/1000 | Loss: 0.00001076
Iteration 31/1000 | Loss: 0.00001076
Iteration 32/1000 | Loss: 0.00001076
Iteration 33/1000 | Loss: 0.00001076
Iteration 34/1000 | Loss: 0.00001075
Iteration 35/1000 | Loss: 0.00001074
Iteration 36/1000 | Loss: 0.00001074
Iteration 37/1000 | Loss: 0.00001074
Iteration 38/1000 | Loss: 0.00001074
Iteration 39/1000 | Loss: 0.00001074
Iteration 40/1000 | Loss: 0.00001073
Iteration 41/1000 | Loss: 0.00001073
Iteration 42/1000 | Loss: 0.00001073
Iteration 43/1000 | Loss: 0.00001073
Iteration 44/1000 | Loss: 0.00001073
Iteration 45/1000 | Loss: 0.00001073
Iteration 46/1000 | Loss: 0.00001073
Iteration 47/1000 | Loss: 0.00001073
Iteration 48/1000 | Loss: 0.00001073
Iteration 49/1000 | Loss: 0.00001073
Iteration 50/1000 | Loss: 0.00001072
Iteration 51/1000 | Loss: 0.00001072
Iteration 52/1000 | Loss: 0.00001072
Iteration 53/1000 | Loss: 0.00001072
Iteration 54/1000 | Loss: 0.00001072
Iteration 55/1000 | Loss: 0.00001072
Iteration 56/1000 | Loss: 0.00001072
Iteration 57/1000 | Loss: 0.00001072
Iteration 58/1000 | Loss: 0.00001072
Iteration 59/1000 | Loss: 0.00001071
Iteration 60/1000 | Loss: 0.00001070
Iteration 61/1000 | Loss: 0.00001070
Iteration 62/1000 | Loss: 0.00001070
Iteration 63/1000 | Loss: 0.00001069
Iteration 64/1000 | Loss: 0.00001069
Iteration 65/1000 | Loss: 0.00001069
Iteration 66/1000 | Loss: 0.00001068
Iteration 67/1000 | Loss: 0.00001068
Iteration 68/1000 | Loss: 0.00001068
Iteration 69/1000 | Loss: 0.00001068
Iteration 70/1000 | Loss: 0.00001068
Iteration 71/1000 | Loss: 0.00001068
Iteration 72/1000 | Loss: 0.00001068
Iteration 73/1000 | Loss: 0.00001068
Iteration 74/1000 | Loss: 0.00001068
Iteration 75/1000 | Loss: 0.00001068
Iteration 76/1000 | Loss: 0.00001068
Iteration 77/1000 | Loss: 0.00001068
Iteration 78/1000 | Loss: 0.00001068
Iteration 79/1000 | Loss: 0.00001068
Iteration 80/1000 | Loss: 0.00001068
Iteration 81/1000 | Loss: 0.00001068
Iteration 82/1000 | Loss: 0.00001068
Iteration 83/1000 | Loss: 0.00001068
Iteration 84/1000 | Loss: 0.00001068
Iteration 85/1000 | Loss: 0.00001068
Iteration 86/1000 | Loss: 0.00001068
Iteration 87/1000 | Loss: 0.00001068
Iteration 88/1000 | Loss: 0.00001068
Iteration 89/1000 | Loss: 0.00001068
Iteration 90/1000 | Loss: 0.00001067
Iteration 91/1000 | Loss: 0.00001067
Iteration 92/1000 | Loss: 0.00001067
Iteration 93/1000 | Loss: 0.00001067
Iteration 94/1000 | Loss: 0.00001067
Iteration 95/1000 | Loss: 0.00001067
Iteration 96/1000 | Loss: 0.00001067
Iteration 97/1000 | Loss: 0.00001067
Iteration 98/1000 | Loss: 0.00001067
Iteration 99/1000 | Loss: 0.00001067
Iteration 100/1000 | Loss: 0.00001067
Iteration 101/1000 | Loss: 0.00001067
Iteration 102/1000 | Loss: 0.00001067
Iteration 103/1000 | Loss: 0.00001067
Iteration 104/1000 | Loss: 0.00001067
Iteration 105/1000 | Loss: 0.00001067
Iteration 106/1000 | Loss: 0.00001067
Iteration 107/1000 | Loss: 0.00001067
Iteration 108/1000 | Loss: 0.00001067
Iteration 109/1000 | Loss: 0.00001067
Iteration 110/1000 | Loss: 0.00001067
Iteration 111/1000 | Loss: 0.00001067
Iteration 112/1000 | Loss: 0.00001067
Iteration 113/1000 | Loss: 0.00001067
Iteration 114/1000 | Loss: 0.00001067
Iteration 115/1000 | Loss: 0.00001067
Iteration 116/1000 | Loss: 0.00001067
Iteration 117/1000 | Loss: 0.00001067
Iteration 118/1000 | Loss: 0.00001067
Iteration 119/1000 | Loss: 0.00001067
Iteration 120/1000 | Loss: 0.00001067
Iteration 121/1000 | Loss: 0.00001067
Iteration 122/1000 | Loss: 0.00001067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.0665647096175235e-05, 1.0665647096175235e-05, 1.0665647096175235e-05, 1.0665647096175235e-05, 1.0665647096175235e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0665647096175235e-05

Optimization complete. Final v2v error: 2.8264596462249756 mm

Highest mean error: 3.1862082481384277 mm for frame 206

Lowest mean error: 2.5772860050201416 mm for frame 0

Saving results

Total time: 32.68939971923828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084999
Iteration 2/25 | Loss: 0.00173237
Iteration 3/25 | Loss: 0.00110392
Iteration 4/25 | Loss: 0.00089775
Iteration 5/25 | Loss: 0.00079185
Iteration 6/25 | Loss: 0.00077916
Iteration 7/25 | Loss: 0.00076155
Iteration 8/25 | Loss: 0.00075976
Iteration 9/25 | Loss: 0.00069545
Iteration 10/25 | Loss: 0.00068895
Iteration 11/25 | Loss: 0.00068118
Iteration 12/25 | Loss: 0.00068377
Iteration 13/25 | Loss: 0.00068130
Iteration 14/25 | Loss: 0.00068551
Iteration 15/25 | Loss: 0.00067968
Iteration 16/25 | Loss: 0.00068211
Iteration 17/25 | Loss: 0.00067911
Iteration 18/25 | Loss: 0.00068202
Iteration 19/25 | Loss: 0.00067973
Iteration 20/25 | Loss: 0.00068145
Iteration 21/25 | Loss: 0.00067844
Iteration 22/25 | Loss: 0.00068061
Iteration 23/25 | Loss: 0.00067557
Iteration 24/25 | Loss: 0.00068205
Iteration 25/25 | Loss: 0.00067290

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47771609
Iteration 2/25 | Loss: 0.00064657
Iteration 3/25 | Loss: 0.00064657
Iteration 4/25 | Loss: 0.00064657
Iteration 5/25 | Loss: 0.00064657
Iteration 6/25 | Loss: 0.00063365
Iteration 7/25 | Loss: 0.00063365
Iteration 8/25 | Loss: 0.00063364
Iteration 9/25 | Loss: 0.00063364
Iteration 10/25 | Loss: 0.00063364
Iteration 11/25 | Loss: 0.00063364
Iteration 12/25 | Loss: 0.00063364
Iteration 13/25 | Loss: 0.00063364
Iteration 14/25 | Loss: 0.00063364
Iteration 15/25 | Loss: 0.00063364
Iteration 16/25 | Loss: 0.00063364
Iteration 17/25 | Loss: 0.00063364
Iteration 18/25 | Loss: 0.00063364
Iteration 19/25 | Loss: 0.00063364
Iteration 20/25 | Loss: 0.00063364
Iteration 21/25 | Loss: 0.00063364
Iteration 22/25 | Loss: 0.00063364
Iteration 23/25 | Loss: 0.00063364
Iteration 24/25 | Loss: 0.00063364
Iteration 25/25 | Loss: 0.00063364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063364
Iteration 2/1000 | Loss: 0.00007521
Iteration 3/1000 | Loss: 0.00064884
Iteration 4/1000 | Loss: 0.00035313
Iteration 5/1000 | Loss: 0.00056282
Iteration 6/1000 | Loss: 0.00031940
Iteration 7/1000 | Loss: 0.00059515
Iteration 8/1000 | Loss: 0.00305842
Iteration 9/1000 | Loss: 0.00079046
Iteration 10/1000 | Loss: 0.00207847
Iteration 11/1000 | Loss: 0.00138472
Iteration 12/1000 | Loss: 0.00005576
Iteration 13/1000 | Loss: 0.00047720
Iteration 14/1000 | Loss: 0.00010692
Iteration 15/1000 | Loss: 0.00046646
Iteration 16/1000 | Loss: 0.00087997
Iteration 17/1000 | Loss: 0.00012886
Iteration 18/1000 | Loss: 0.00002761
Iteration 19/1000 | Loss: 0.00040481
Iteration 20/1000 | Loss: 0.00026991
Iteration 21/1000 | Loss: 0.00050952
Iteration 22/1000 | Loss: 0.00018302
Iteration 23/1000 | Loss: 0.00038840
Iteration 24/1000 | Loss: 0.00034378
Iteration 25/1000 | Loss: 0.00057435
Iteration 26/1000 | Loss: 0.00021432
Iteration 27/1000 | Loss: 0.00015153
Iteration 28/1000 | Loss: 0.00035489
Iteration 29/1000 | Loss: 0.00023182
Iteration 30/1000 | Loss: 0.00031176
Iteration 31/1000 | Loss: 0.00029890
Iteration 32/1000 | Loss: 0.00034625
Iteration 33/1000 | Loss: 0.00050449
Iteration 34/1000 | Loss: 0.00040017
Iteration 35/1000 | Loss: 0.00095347
Iteration 36/1000 | Loss: 0.00041059
Iteration 37/1000 | Loss: 0.00070040
Iteration 38/1000 | Loss: 0.00040598
Iteration 39/1000 | Loss: 0.00087783
Iteration 40/1000 | Loss: 0.00038472
Iteration 41/1000 | Loss: 0.00076026
Iteration 42/1000 | Loss: 0.00043937
Iteration 43/1000 | Loss: 0.00062200
Iteration 44/1000 | Loss: 0.00032080
Iteration 45/1000 | Loss: 0.00243950
Iteration 46/1000 | Loss: 0.00027317
Iteration 47/1000 | Loss: 0.00006105
Iteration 48/1000 | Loss: 0.00003643
Iteration 49/1000 | Loss: 0.00080341
Iteration 50/1000 | Loss: 0.00025342
Iteration 51/1000 | Loss: 0.00091946
Iteration 52/1000 | Loss: 0.00053841
Iteration 53/1000 | Loss: 0.00016240
Iteration 54/1000 | Loss: 0.00002631
Iteration 55/1000 | Loss: 0.00002885
Iteration 56/1000 | Loss: 0.00002745
Iteration 57/1000 | Loss: 0.00003597
Iteration 58/1000 | Loss: 0.00001812
Iteration 59/1000 | Loss: 0.00001749
Iteration 60/1000 | Loss: 0.00003213
Iteration 61/1000 | Loss: 0.00001632
Iteration 62/1000 | Loss: 0.00001611
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00004134
Iteration 65/1000 | Loss: 0.00001625
Iteration 66/1000 | Loss: 0.00004644
Iteration 67/1000 | Loss: 0.00002103
Iteration 68/1000 | Loss: 0.00002784
Iteration 69/1000 | Loss: 0.00001581
Iteration 70/1000 | Loss: 0.00001581
Iteration 71/1000 | Loss: 0.00001581
Iteration 72/1000 | Loss: 0.00001581
Iteration 73/1000 | Loss: 0.00001581
Iteration 74/1000 | Loss: 0.00001581
Iteration 75/1000 | Loss: 0.00001580
Iteration 76/1000 | Loss: 0.00001580
Iteration 77/1000 | Loss: 0.00001575
Iteration 78/1000 | Loss: 0.00001575
Iteration 79/1000 | Loss: 0.00001575
Iteration 80/1000 | Loss: 0.00001574
Iteration 81/1000 | Loss: 0.00001574
Iteration 82/1000 | Loss: 0.00001573
Iteration 83/1000 | Loss: 0.00001573
Iteration 84/1000 | Loss: 0.00001573
Iteration 85/1000 | Loss: 0.00001573
Iteration 86/1000 | Loss: 0.00001572
Iteration 87/1000 | Loss: 0.00006113
Iteration 88/1000 | Loss: 0.00001571
Iteration 89/1000 | Loss: 0.00001565
Iteration 90/1000 | Loss: 0.00001565
Iteration 91/1000 | Loss: 0.00001565
Iteration 92/1000 | Loss: 0.00001564
Iteration 93/1000 | Loss: 0.00001564
Iteration 94/1000 | Loss: 0.00001563
Iteration 95/1000 | Loss: 0.00001563
Iteration 96/1000 | Loss: 0.00001563
Iteration 97/1000 | Loss: 0.00001562
Iteration 98/1000 | Loss: 0.00001562
Iteration 99/1000 | Loss: 0.00001561
Iteration 100/1000 | Loss: 0.00001561
Iteration 101/1000 | Loss: 0.00001561
Iteration 102/1000 | Loss: 0.00001561
Iteration 103/1000 | Loss: 0.00001561
Iteration 104/1000 | Loss: 0.00001560
Iteration 105/1000 | Loss: 0.00001560
Iteration 106/1000 | Loss: 0.00001560
Iteration 107/1000 | Loss: 0.00001560
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001559
Iteration 111/1000 | Loss: 0.00001559
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001558
Iteration 114/1000 | Loss: 0.00001557
Iteration 115/1000 | Loss: 0.00001557
Iteration 116/1000 | Loss: 0.00001557
Iteration 117/1000 | Loss: 0.00001556
Iteration 118/1000 | Loss: 0.00001556
Iteration 119/1000 | Loss: 0.00001555
Iteration 120/1000 | Loss: 0.00001555
Iteration 121/1000 | Loss: 0.00001554
Iteration 122/1000 | Loss: 0.00001554
Iteration 123/1000 | Loss: 0.00001553
Iteration 124/1000 | Loss: 0.00001553
Iteration 125/1000 | Loss: 0.00001553
Iteration 126/1000 | Loss: 0.00001552
Iteration 127/1000 | Loss: 0.00001552
Iteration 128/1000 | Loss: 0.00001552
Iteration 129/1000 | Loss: 0.00001551
Iteration 130/1000 | Loss: 0.00001551
Iteration 131/1000 | Loss: 0.00001551
Iteration 132/1000 | Loss: 0.00001550
Iteration 133/1000 | Loss: 0.00001550
Iteration 134/1000 | Loss: 0.00001550
Iteration 135/1000 | Loss: 0.00001550
Iteration 136/1000 | Loss: 0.00001550
Iteration 137/1000 | Loss: 0.00001550
Iteration 138/1000 | Loss: 0.00001550
Iteration 139/1000 | Loss: 0.00001550
Iteration 140/1000 | Loss: 0.00001550
Iteration 141/1000 | Loss: 0.00001550
Iteration 142/1000 | Loss: 0.00001550
Iteration 143/1000 | Loss: 0.00001550
Iteration 144/1000 | Loss: 0.00001550
Iteration 145/1000 | Loss: 0.00001550
Iteration 146/1000 | Loss: 0.00001550
Iteration 147/1000 | Loss: 0.00001550
Iteration 148/1000 | Loss: 0.00001550
Iteration 149/1000 | Loss: 0.00001550
Iteration 150/1000 | Loss: 0.00001550
Iteration 151/1000 | Loss: 0.00001550
Iteration 152/1000 | Loss: 0.00001550
Iteration 153/1000 | Loss: 0.00001550
Iteration 154/1000 | Loss: 0.00001550
Iteration 155/1000 | Loss: 0.00001550
Iteration 156/1000 | Loss: 0.00001550
Iteration 157/1000 | Loss: 0.00001550
Iteration 158/1000 | Loss: 0.00001550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.5497018466703594e-05, 1.5497018466703594e-05, 1.5497018466703594e-05, 1.5497018466703594e-05, 1.5497018466703594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5497018466703594e-05

Optimization complete. Final v2v error: 3.3138015270233154 mm

Highest mean error: 4.226862907409668 mm for frame 83

Lowest mean error: 3.1013553142547607 mm for frame 3

Saving results

Total time: 149.0424406528473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803342
Iteration 2/25 | Loss: 0.00096363
Iteration 3/25 | Loss: 0.00071631
Iteration 4/25 | Loss: 0.00066655
Iteration 5/25 | Loss: 0.00065699
Iteration 6/25 | Loss: 0.00065389
Iteration 7/25 | Loss: 0.00065358
Iteration 8/25 | Loss: 0.00065358
Iteration 9/25 | Loss: 0.00065358
Iteration 10/25 | Loss: 0.00065358
Iteration 11/25 | Loss: 0.00065358
Iteration 12/25 | Loss: 0.00065358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006535821012221277, 0.0006535821012221277, 0.0006535821012221277, 0.0006535821012221277, 0.0006535821012221277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006535821012221277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44323444
Iteration 2/25 | Loss: 0.00031037
Iteration 3/25 | Loss: 0.00031035
Iteration 4/25 | Loss: 0.00031035
Iteration 5/25 | Loss: 0.00031035
Iteration 6/25 | Loss: 0.00031035
Iteration 7/25 | Loss: 0.00031035
Iteration 8/25 | Loss: 0.00031035
Iteration 9/25 | Loss: 0.00031035
Iteration 10/25 | Loss: 0.00031035
Iteration 11/25 | Loss: 0.00031035
Iteration 12/25 | Loss: 0.00031035
Iteration 13/25 | Loss: 0.00031035
Iteration 14/25 | Loss: 0.00031035
Iteration 15/25 | Loss: 0.00031035
Iteration 16/25 | Loss: 0.00031035
Iteration 17/25 | Loss: 0.00031035
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000310345902107656, 0.000310345902107656, 0.000310345902107656, 0.000310345902107656, 0.000310345902107656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000310345902107656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031035
Iteration 2/1000 | Loss: 0.00002695
Iteration 3/1000 | Loss: 0.00002050
Iteration 4/1000 | Loss: 0.00001906
Iteration 5/1000 | Loss: 0.00001787
Iteration 6/1000 | Loss: 0.00001730
Iteration 7/1000 | Loss: 0.00001681
Iteration 8/1000 | Loss: 0.00001646
Iteration 9/1000 | Loss: 0.00001624
Iteration 10/1000 | Loss: 0.00001604
Iteration 11/1000 | Loss: 0.00001592
Iteration 12/1000 | Loss: 0.00001586
Iteration 13/1000 | Loss: 0.00001586
Iteration 14/1000 | Loss: 0.00001585
Iteration 15/1000 | Loss: 0.00001585
Iteration 16/1000 | Loss: 0.00001585
Iteration 17/1000 | Loss: 0.00001584
Iteration 18/1000 | Loss: 0.00001577
Iteration 19/1000 | Loss: 0.00001576
Iteration 20/1000 | Loss: 0.00001576
Iteration 21/1000 | Loss: 0.00001575
Iteration 22/1000 | Loss: 0.00001575
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00001574
Iteration 25/1000 | Loss: 0.00001573
Iteration 26/1000 | Loss: 0.00001573
Iteration 27/1000 | Loss: 0.00001572
Iteration 28/1000 | Loss: 0.00001572
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001571
Iteration 34/1000 | Loss: 0.00001571
Iteration 35/1000 | Loss: 0.00001570
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00001569
Iteration 38/1000 | Loss: 0.00001569
Iteration 39/1000 | Loss: 0.00001569
Iteration 40/1000 | Loss: 0.00001569
Iteration 41/1000 | Loss: 0.00001568
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001568
Iteration 44/1000 | Loss: 0.00001567
Iteration 45/1000 | Loss: 0.00001567
Iteration 46/1000 | Loss: 0.00001566
Iteration 47/1000 | Loss: 0.00001566
Iteration 48/1000 | Loss: 0.00001566
Iteration 49/1000 | Loss: 0.00001566
Iteration 50/1000 | Loss: 0.00001566
Iteration 51/1000 | Loss: 0.00001566
Iteration 52/1000 | Loss: 0.00001566
Iteration 53/1000 | Loss: 0.00001566
Iteration 54/1000 | Loss: 0.00001566
Iteration 55/1000 | Loss: 0.00001566
Iteration 56/1000 | Loss: 0.00001566
Iteration 57/1000 | Loss: 0.00001566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [1.565666934766341e-05, 1.565666934766341e-05, 1.565666934766341e-05, 1.565666934766341e-05, 1.565666934766341e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.565666934766341e-05

Optimization complete. Final v2v error: 3.3020925521850586 mm

Highest mean error: 3.7745893001556396 mm for frame 236

Lowest mean error: 2.8894295692443848 mm for frame 79

Saving results

Total time: 34.129491567611694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390305
Iteration 2/25 | Loss: 0.00083061
Iteration 3/25 | Loss: 0.00068905
Iteration 4/25 | Loss: 0.00064830
Iteration 5/25 | Loss: 0.00063624
Iteration 6/25 | Loss: 0.00063335
Iteration 7/25 | Loss: 0.00063212
Iteration 8/25 | Loss: 0.00063168
Iteration 9/25 | Loss: 0.00063168
Iteration 10/25 | Loss: 0.00063168
Iteration 11/25 | Loss: 0.00063168
Iteration 12/25 | Loss: 0.00063168
Iteration 13/25 | Loss: 0.00063168
Iteration 14/25 | Loss: 0.00063168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006316780927591026, 0.0006316780927591026, 0.0006316780927591026, 0.0006316780927591026, 0.0006316780927591026]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006316780927591026

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48765469
Iteration 2/25 | Loss: 0.00034545
Iteration 3/25 | Loss: 0.00034545
Iteration 4/25 | Loss: 0.00034544
Iteration 5/25 | Loss: 0.00034544
Iteration 6/25 | Loss: 0.00034544
Iteration 7/25 | Loss: 0.00034544
Iteration 8/25 | Loss: 0.00034544
Iteration 9/25 | Loss: 0.00034544
Iteration 10/25 | Loss: 0.00034544
Iteration 11/25 | Loss: 0.00034544
Iteration 12/25 | Loss: 0.00034544
Iteration 13/25 | Loss: 0.00034544
Iteration 14/25 | Loss: 0.00034544
Iteration 15/25 | Loss: 0.00034544
Iteration 16/25 | Loss: 0.00034544
Iteration 17/25 | Loss: 0.00034544
Iteration 18/25 | Loss: 0.00034544
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003454428806435317, 0.0003454428806435317, 0.0003454428806435317, 0.0003454428806435317, 0.0003454428806435317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003454428806435317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034544
Iteration 2/1000 | Loss: 0.00003252
Iteration 3/1000 | Loss: 0.00001974
Iteration 4/1000 | Loss: 0.00001503
Iteration 5/1000 | Loss: 0.00001415
Iteration 6/1000 | Loss: 0.00001358
Iteration 7/1000 | Loss: 0.00001315
Iteration 8/1000 | Loss: 0.00001285
Iteration 9/1000 | Loss: 0.00001282
Iteration 10/1000 | Loss: 0.00001274
Iteration 11/1000 | Loss: 0.00001261
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001252
Iteration 14/1000 | Loss: 0.00001251
Iteration 15/1000 | Loss: 0.00001246
Iteration 16/1000 | Loss: 0.00001246
Iteration 17/1000 | Loss: 0.00001245
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001244
Iteration 20/1000 | Loss: 0.00001241
Iteration 21/1000 | Loss: 0.00001241
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001240
Iteration 24/1000 | Loss: 0.00001239
Iteration 25/1000 | Loss: 0.00001239
Iteration 26/1000 | Loss: 0.00001239
Iteration 27/1000 | Loss: 0.00001239
Iteration 28/1000 | Loss: 0.00001238
Iteration 29/1000 | Loss: 0.00001238
Iteration 30/1000 | Loss: 0.00001237
Iteration 31/1000 | Loss: 0.00001237
Iteration 32/1000 | Loss: 0.00001236
Iteration 33/1000 | Loss: 0.00001236
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001236
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001236
Iteration 39/1000 | Loss: 0.00001235
Iteration 40/1000 | Loss: 0.00001235
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00001234
Iteration 43/1000 | Loss: 0.00001232
Iteration 44/1000 | Loss: 0.00001231
Iteration 45/1000 | Loss: 0.00001231
Iteration 46/1000 | Loss: 0.00001230
Iteration 47/1000 | Loss: 0.00001230
Iteration 48/1000 | Loss: 0.00001230
Iteration 49/1000 | Loss: 0.00001230
Iteration 50/1000 | Loss: 0.00001230
Iteration 51/1000 | Loss: 0.00001230
Iteration 52/1000 | Loss: 0.00001229
Iteration 53/1000 | Loss: 0.00001229
Iteration 54/1000 | Loss: 0.00001229
Iteration 55/1000 | Loss: 0.00001229
Iteration 56/1000 | Loss: 0.00001229
Iteration 57/1000 | Loss: 0.00001229
Iteration 58/1000 | Loss: 0.00001228
Iteration 59/1000 | Loss: 0.00001227
Iteration 60/1000 | Loss: 0.00001227
Iteration 61/1000 | Loss: 0.00001226
Iteration 62/1000 | Loss: 0.00001226
Iteration 63/1000 | Loss: 0.00001226
Iteration 64/1000 | Loss: 0.00001226
Iteration 65/1000 | Loss: 0.00001226
Iteration 66/1000 | Loss: 0.00001225
Iteration 67/1000 | Loss: 0.00001225
Iteration 68/1000 | Loss: 0.00001225
Iteration 69/1000 | Loss: 0.00001224
Iteration 70/1000 | Loss: 0.00001224
Iteration 71/1000 | Loss: 0.00001223
Iteration 72/1000 | Loss: 0.00001223
Iteration 73/1000 | Loss: 0.00001223
Iteration 74/1000 | Loss: 0.00001223
Iteration 75/1000 | Loss: 0.00001223
Iteration 76/1000 | Loss: 0.00001223
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001223
Iteration 79/1000 | Loss: 0.00001223
Iteration 80/1000 | Loss: 0.00001222
Iteration 81/1000 | Loss: 0.00001222
Iteration 82/1000 | Loss: 0.00001222
Iteration 83/1000 | Loss: 0.00001221
Iteration 84/1000 | Loss: 0.00001221
Iteration 85/1000 | Loss: 0.00001221
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001220
Iteration 88/1000 | Loss: 0.00001219
Iteration 89/1000 | Loss: 0.00001219
Iteration 90/1000 | Loss: 0.00001219
Iteration 91/1000 | Loss: 0.00001219
Iteration 92/1000 | Loss: 0.00001218
Iteration 93/1000 | Loss: 0.00001218
Iteration 94/1000 | Loss: 0.00001218
Iteration 95/1000 | Loss: 0.00001218
Iteration 96/1000 | Loss: 0.00001218
Iteration 97/1000 | Loss: 0.00001217
Iteration 98/1000 | Loss: 0.00001217
Iteration 99/1000 | Loss: 0.00001217
Iteration 100/1000 | Loss: 0.00001217
Iteration 101/1000 | Loss: 0.00001217
Iteration 102/1000 | Loss: 0.00001217
Iteration 103/1000 | Loss: 0.00001217
Iteration 104/1000 | Loss: 0.00001216
Iteration 105/1000 | Loss: 0.00001216
Iteration 106/1000 | Loss: 0.00001216
Iteration 107/1000 | Loss: 0.00001215
Iteration 108/1000 | Loss: 0.00001215
Iteration 109/1000 | Loss: 0.00001215
Iteration 110/1000 | Loss: 0.00001215
Iteration 111/1000 | Loss: 0.00001214
Iteration 112/1000 | Loss: 0.00001214
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001213
Iteration 115/1000 | Loss: 0.00001213
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001213
Iteration 118/1000 | Loss: 0.00001213
Iteration 119/1000 | Loss: 0.00001213
Iteration 120/1000 | Loss: 0.00001212
Iteration 121/1000 | Loss: 0.00001212
Iteration 122/1000 | Loss: 0.00001212
Iteration 123/1000 | Loss: 0.00001212
Iteration 124/1000 | Loss: 0.00001212
Iteration 125/1000 | Loss: 0.00001212
Iteration 126/1000 | Loss: 0.00001212
Iteration 127/1000 | Loss: 0.00001212
Iteration 128/1000 | Loss: 0.00001211
Iteration 129/1000 | Loss: 0.00001211
Iteration 130/1000 | Loss: 0.00001211
Iteration 131/1000 | Loss: 0.00001211
Iteration 132/1000 | Loss: 0.00001211
Iteration 133/1000 | Loss: 0.00001211
Iteration 134/1000 | Loss: 0.00001211
Iteration 135/1000 | Loss: 0.00001211
Iteration 136/1000 | Loss: 0.00001211
Iteration 137/1000 | Loss: 0.00001211
Iteration 138/1000 | Loss: 0.00001211
Iteration 139/1000 | Loss: 0.00001211
Iteration 140/1000 | Loss: 0.00001211
Iteration 141/1000 | Loss: 0.00001211
Iteration 142/1000 | Loss: 0.00001211
Iteration 143/1000 | Loss: 0.00001211
Iteration 144/1000 | Loss: 0.00001211
Iteration 145/1000 | Loss: 0.00001211
Iteration 146/1000 | Loss: 0.00001211
Iteration 147/1000 | Loss: 0.00001211
Iteration 148/1000 | Loss: 0.00001211
Iteration 149/1000 | Loss: 0.00001211
Iteration 150/1000 | Loss: 0.00001211
Iteration 151/1000 | Loss: 0.00001211
Iteration 152/1000 | Loss: 0.00001211
Iteration 153/1000 | Loss: 0.00001211
Iteration 154/1000 | Loss: 0.00001211
Iteration 155/1000 | Loss: 0.00001211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.2108233022445347e-05, 1.2108233022445347e-05, 1.2108233022445347e-05, 1.2108233022445347e-05, 1.2108233022445347e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2108233022445347e-05

Optimization complete. Final v2v error: 2.861525058746338 mm

Highest mean error: 3.243800163269043 mm for frame 2

Lowest mean error: 2.623830795288086 mm for frame 96

Saving results

Total time: 36.16685199737549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01123916
Iteration 2/25 | Loss: 0.00225887
Iteration 3/25 | Loss: 0.00122670
Iteration 4/25 | Loss: 0.00100446
Iteration 5/25 | Loss: 0.00091599
Iteration 6/25 | Loss: 0.00089557
Iteration 7/25 | Loss: 0.00088923
Iteration 8/25 | Loss: 0.00088227
Iteration 9/25 | Loss: 0.00087899
Iteration 10/25 | Loss: 0.00087804
Iteration 11/25 | Loss: 0.00087492
Iteration 12/25 | Loss: 0.00087447
Iteration 13/25 | Loss: 0.00087437
Iteration 14/25 | Loss: 0.00087431
Iteration 15/25 | Loss: 0.00087431
Iteration 16/25 | Loss: 0.00087431
Iteration 17/25 | Loss: 0.00087431
Iteration 18/25 | Loss: 0.00087431
Iteration 19/25 | Loss: 0.00087431
Iteration 20/25 | Loss: 0.00087431
Iteration 21/25 | Loss: 0.00087431
Iteration 22/25 | Loss: 0.00087430
Iteration 23/25 | Loss: 0.00087430
Iteration 24/25 | Loss: 0.00087430
Iteration 25/25 | Loss: 0.00087430

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.69933772
Iteration 2/25 | Loss: 0.00098094
Iteration 3/25 | Loss: 0.00098094
Iteration 4/25 | Loss: 0.00098094
Iteration 5/25 | Loss: 0.00098094
Iteration 6/25 | Loss: 0.00098094
Iteration 7/25 | Loss: 0.00098094
Iteration 8/25 | Loss: 0.00098094
Iteration 9/25 | Loss: 0.00098094
Iteration 10/25 | Loss: 0.00098094
Iteration 11/25 | Loss: 0.00098094
Iteration 12/25 | Loss: 0.00098094
Iteration 13/25 | Loss: 0.00098094
Iteration 14/25 | Loss: 0.00098094
Iteration 15/25 | Loss: 0.00098094
Iteration 16/25 | Loss: 0.00098094
Iteration 17/25 | Loss: 0.00098094
Iteration 18/25 | Loss: 0.00098094
Iteration 19/25 | Loss: 0.00098094
Iteration 20/25 | Loss: 0.00098094
Iteration 21/25 | Loss: 0.00098094
Iteration 22/25 | Loss: 0.00098094
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009809377370402217, 0.0009809377370402217, 0.0009809377370402217, 0.0009809377370402217, 0.0009809377370402217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009809377370402217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098094
Iteration 2/1000 | Loss: 0.00016110
Iteration 3/1000 | Loss: 0.00009029
Iteration 4/1000 | Loss: 0.00010316
Iteration 5/1000 | Loss: 0.00008780
Iteration 6/1000 | Loss: 0.00006361
Iteration 7/1000 | Loss: 0.00006335
Iteration 8/1000 | Loss: 0.00005784
Iteration 9/1000 | Loss: 0.00009210
Iteration 10/1000 | Loss: 0.00015744
Iteration 11/1000 | Loss: 0.00010727
Iteration 12/1000 | Loss: 0.00016324
Iteration 13/1000 | Loss: 0.00012818
Iteration 14/1000 | Loss: 0.00017217
Iteration 15/1000 | Loss: 0.00010907
Iteration 16/1000 | Loss: 0.00008487
Iteration 17/1000 | Loss: 0.00008834
Iteration 18/1000 | Loss: 0.00006990
Iteration 19/1000 | Loss: 0.00006765
Iteration 20/1000 | Loss: 0.00008947
Iteration 21/1000 | Loss: 0.00008639
Iteration 22/1000 | Loss: 0.00008493
Iteration 23/1000 | Loss: 0.00005424
Iteration 24/1000 | Loss: 0.00008562
Iteration 25/1000 | Loss: 0.00009201
Iteration 26/1000 | Loss: 0.00012070
Iteration 27/1000 | Loss: 0.00005611
Iteration 28/1000 | Loss: 0.00006427
Iteration 29/1000 | Loss: 0.00005042
Iteration 30/1000 | Loss: 0.00005852
Iteration 31/1000 | Loss: 0.00004820
Iteration 32/1000 | Loss: 0.00005006
Iteration 33/1000 | Loss: 0.00007144
Iteration 34/1000 | Loss: 0.00005427
Iteration 35/1000 | Loss: 0.00004450
Iteration 36/1000 | Loss: 0.00004761
Iteration 37/1000 | Loss: 0.00005743
Iteration 38/1000 | Loss: 0.00006068
Iteration 39/1000 | Loss: 0.00004893
Iteration 40/1000 | Loss: 0.00004981
Iteration 41/1000 | Loss: 0.00004429
Iteration 42/1000 | Loss: 0.00004708
Iteration 43/1000 | Loss: 0.00006502
Iteration 44/1000 | Loss: 0.00005831
Iteration 45/1000 | Loss: 0.00006170
Iteration 46/1000 | Loss: 0.00005510
Iteration 47/1000 | Loss: 0.00006320
Iteration 48/1000 | Loss: 0.00008869
Iteration 49/1000 | Loss: 0.00013913
Iteration 50/1000 | Loss: 0.00007825
Iteration 51/1000 | Loss: 0.00005432
Iteration 52/1000 | Loss: 0.00004578
Iteration 53/1000 | Loss: 0.00006195
Iteration 54/1000 | Loss: 0.00004109
Iteration 55/1000 | Loss: 0.00005318
Iteration 56/1000 | Loss: 0.00003930
Iteration 57/1000 | Loss: 0.00005266
Iteration 58/1000 | Loss: 0.00006093
Iteration 59/1000 | Loss: 0.00004732
Iteration 60/1000 | Loss: 0.00007344
Iteration 61/1000 | Loss: 0.00006407
Iteration 62/1000 | Loss: 0.00005888
Iteration 63/1000 | Loss: 0.00003767
Iteration 64/1000 | Loss: 0.00004456
Iteration 65/1000 | Loss: 0.00004411
Iteration 66/1000 | Loss: 0.00004851
Iteration 67/1000 | Loss: 0.00005169
Iteration 68/1000 | Loss: 0.00005140
Iteration 69/1000 | Loss: 0.00004540
Iteration 70/1000 | Loss: 0.00004855
Iteration 71/1000 | Loss: 0.00005760
Iteration 72/1000 | Loss: 0.00006433
Iteration 73/1000 | Loss: 0.00005334
Iteration 74/1000 | Loss: 0.00005795
Iteration 75/1000 | Loss: 0.00004900
Iteration 76/1000 | Loss: 0.00005136
Iteration 77/1000 | Loss: 0.00007407
Iteration 78/1000 | Loss: 0.00006312
Iteration 79/1000 | Loss: 0.00008460
Iteration 80/1000 | Loss: 0.00003764
Iteration 81/1000 | Loss: 0.00006403
Iteration 82/1000 | Loss: 0.00005919
Iteration 83/1000 | Loss: 0.00006068
Iteration 84/1000 | Loss: 0.00005630
Iteration 85/1000 | Loss: 0.00004701
Iteration 86/1000 | Loss: 0.00005697
Iteration 87/1000 | Loss: 0.00005111
Iteration 88/1000 | Loss: 0.00007096
Iteration 89/1000 | Loss: 0.00004991
Iteration 90/1000 | Loss: 0.00005190
Iteration 91/1000 | Loss: 0.00004961
Iteration 92/1000 | Loss: 0.00005389
Iteration 93/1000 | Loss: 0.00014498
Iteration 94/1000 | Loss: 0.00004638
Iteration 95/1000 | Loss: 0.00004162
Iteration 96/1000 | Loss: 0.00005335
Iteration 97/1000 | Loss: 0.00005177
Iteration 98/1000 | Loss: 0.00004816
Iteration 99/1000 | Loss: 0.00004969
Iteration 100/1000 | Loss: 0.00004730
Iteration 101/1000 | Loss: 0.00004254
Iteration 102/1000 | Loss: 0.00004399
Iteration 103/1000 | Loss: 0.00004536
Iteration 104/1000 | Loss: 0.00004857
Iteration 105/1000 | Loss: 0.00005041
Iteration 106/1000 | Loss: 0.00005724
Iteration 107/1000 | Loss: 0.00005718
Iteration 108/1000 | Loss: 0.00005173
Iteration 109/1000 | Loss: 0.00005548
Iteration 110/1000 | Loss: 0.00004872
Iteration 111/1000 | Loss: 0.00005220
Iteration 112/1000 | Loss: 0.00004708
Iteration 113/1000 | Loss: 0.00004862
Iteration 114/1000 | Loss: 0.00004786
Iteration 115/1000 | Loss: 0.00005539
Iteration 116/1000 | Loss: 0.00004362
Iteration 117/1000 | Loss: 0.00004837
Iteration 118/1000 | Loss: 0.00004884
Iteration 119/1000 | Loss: 0.00005075
Iteration 120/1000 | Loss: 0.00004455
Iteration 121/1000 | Loss: 0.00004954
Iteration 122/1000 | Loss: 0.00004447
Iteration 123/1000 | Loss: 0.00004919
Iteration 124/1000 | Loss: 0.00004305
Iteration 125/1000 | Loss: 0.00004555
Iteration 126/1000 | Loss: 0.00005471
Iteration 127/1000 | Loss: 0.00004632
Iteration 128/1000 | Loss: 0.00003923
Iteration 129/1000 | Loss: 0.00004805
Iteration 130/1000 | Loss: 0.00005086
Iteration 131/1000 | Loss: 0.00004725
Iteration 132/1000 | Loss: 0.00005078
Iteration 133/1000 | Loss: 0.00004428
Iteration 134/1000 | Loss: 0.00004680
Iteration 135/1000 | Loss: 0.00003670
Iteration 136/1000 | Loss: 0.00003780
Iteration 137/1000 | Loss: 0.00004609
Iteration 138/1000 | Loss: 0.00004369
Iteration 139/1000 | Loss: 0.00003896
Iteration 140/1000 | Loss: 0.00003946
Iteration 141/1000 | Loss: 0.00004899
Iteration 142/1000 | Loss: 0.00004926
Iteration 143/1000 | Loss: 0.00005090
Iteration 144/1000 | Loss: 0.00003755
Iteration 145/1000 | Loss: 0.00003514
Iteration 146/1000 | Loss: 0.00003432
Iteration 147/1000 | Loss: 0.00003367
Iteration 148/1000 | Loss: 0.00003337
Iteration 149/1000 | Loss: 0.00003320
Iteration 150/1000 | Loss: 0.00003312
Iteration 151/1000 | Loss: 0.00003310
Iteration 152/1000 | Loss: 0.00003293
Iteration 153/1000 | Loss: 0.00003278
Iteration 154/1000 | Loss: 0.00003274
Iteration 155/1000 | Loss: 0.00003271
Iteration 156/1000 | Loss: 0.00003256
Iteration 157/1000 | Loss: 0.00003246
Iteration 158/1000 | Loss: 0.00003239
Iteration 159/1000 | Loss: 0.00003232
Iteration 160/1000 | Loss: 0.00003220
Iteration 161/1000 | Loss: 0.00003215
Iteration 162/1000 | Loss: 0.00004503
Iteration 163/1000 | Loss: 0.00003872
Iteration 164/1000 | Loss: 0.00003267
Iteration 165/1000 | Loss: 0.00003219
Iteration 166/1000 | Loss: 0.00004334
Iteration 167/1000 | Loss: 0.00003421
Iteration 168/1000 | Loss: 0.00003312
Iteration 169/1000 | Loss: 0.00003243
Iteration 170/1000 | Loss: 0.00003206
Iteration 171/1000 | Loss: 0.00003173
Iteration 172/1000 | Loss: 0.00003158
Iteration 173/1000 | Loss: 0.00003154
Iteration 174/1000 | Loss: 0.00003150
Iteration 175/1000 | Loss: 0.00003149
Iteration 176/1000 | Loss: 0.00003149
Iteration 177/1000 | Loss: 0.00003149
Iteration 178/1000 | Loss: 0.00003149
Iteration 179/1000 | Loss: 0.00003147
Iteration 180/1000 | Loss: 0.00003147
Iteration 181/1000 | Loss: 0.00003147
Iteration 182/1000 | Loss: 0.00003147
Iteration 183/1000 | Loss: 0.00003147
Iteration 184/1000 | Loss: 0.00003147
Iteration 185/1000 | Loss: 0.00003147
Iteration 186/1000 | Loss: 0.00003147
Iteration 187/1000 | Loss: 0.00003146
Iteration 188/1000 | Loss: 0.00003146
Iteration 189/1000 | Loss: 0.00003146
Iteration 190/1000 | Loss: 0.00003146
Iteration 191/1000 | Loss: 0.00003146
Iteration 192/1000 | Loss: 0.00003146
Iteration 193/1000 | Loss: 0.00003146
Iteration 194/1000 | Loss: 0.00003145
Iteration 195/1000 | Loss: 0.00003145
Iteration 196/1000 | Loss: 0.00003145
Iteration 197/1000 | Loss: 0.00003145
Iteration 198/1000 | Loss: 0.00003145
Iteration 199/1000 | Loss: 0.00003145
Iteration 200/1000 | Loss: 0.00003145
Iteration 201/1000 | Loss: 0.00003144
Iteration 202/1000 | Loss: 0.00003144
Iteration 203/1000 | Loss: 0.00003144
Iteration 204/1000 | Loss: 0.00003144
Iteration 205/1000 | Loss: 0.00003144
Iteration 206/1000 | Loss: 0.00003144
Iteration 207/1000 | Loss: 0.00003143
Iteration 208/1000 | Loss: 0.00003143
Iteration 209/1000 | Loss: 0.00003143
Iteration 210/1000 | Loss: 0.00003143
Iteration 211/1000 | Loss: 0.00003143
Iteration 212/1000 | Loss: 0.00003142
Iteration 213/1000 | Loss: 0.00003142
Iteration 214/1000 | Loss: 0.00003142
Iteration 215/1000 | Loss: 0.00003142
Iteration 216/1000 | Loss: 0.00003142
Iteration 217/1000 | Loss: 0.00003142
Iteration 218/1000 | Loss: 0.00003142
Iteration 219/1000 | Loss: 0.00003141
Iteration 220/1000 | Loss: 0.00003141
Iteration 221/1000 | Loss: 0.00003141
Iteration 222/1000 | Loss: 0.00003141
Iteration 223/1000 | Loss: 0.00003141
Iteration 224/1000 | Loss: 0.00003140
Iteration 225/1000 | Loss: 0.00003140
Iteration 226/1000 | Loss: 0.00003140
Iteration 227/1000 | Loss: 0.00003140
Iteration 228/1000 | Loss: 0.00003140
Iteration 229/1000 | Loss: 0.00003139
Iteration 230/1000 | Loss: 0.00003139
Iteration 231/1000 | Loss: 0.00003139
Iteration 232/1000 | Loss: 0.00003139
Iteration 233/1000 | Loss: 0.00003139
Iteration 234/1000 | Loss: 0.00003139
Iteration 235/1000 | Loss: 0.00003139
Iteration 236/1000 | Loss: 0.00003138
Iteration 237/1000 | Loss: 0.00003138
Iteration 238/1000 | Loss: 0.00003138
Iteration 239/1000 | Loss: 0.00003138
Iteration 240/1000 | Loss: 0.00003138
Iteration 241/1000 | Loss: 0.00003138
Iteration 242/1000 | Loss: 0.00003138
Iteration 243/1000 | Loss: 0.00003137
Iteration 244/1000 | Loss: 0.00003136
Iteration 245/1000 | Loss: 0.00003136
Iteration 246/1000 | Loss: 0.00003135
Iteration 247/1000 | Loss: 0.00003135
Iteration 248/1000 | Loss: 0.00003134
Iteration 249/1000 | Loss: 0.00003134
Iteration 250/1000 | Loss: 0.00003134
Iteration 251/1000 | Loss: 0.00003134
Iteration 252/1000 | Loss: 0.00003134
Iteration 253/1000 | Loss: 0.00003133
Iteration 254/1000 | Loss: 0.00003133
Iteration 255/1000 | Loss: 0.00003133
Iteration 256/1000 | Loss: 0.00003133
Iteration 257/1000 | Loss: 0.00003133
Iteration 258/1000 | Loss: 0.00003133
Iteration 259/1000 | Loss: 0.00003133
Iteration 260/1000 | Loss: 0.00003133
Iteration 261/1000 | Loss: 0.00003132
Iteration 262/1000 | Loss: 0.00003132
Iteration 263/1000 | Loss: 0.00003132
Iteration 264/1000 | Loss: 0.00003131
Iteration 265/1000 | Loss: 0.00003131
Iteration 266/1000 | Loss: 0.00003131
Iteration 267/1000 | Loss: 0.00003131
Iteration 268/1000 | Loss: 0.00003130
Iteration 269/1000 | Loss: 0.00003130
Iteration 270/1000 | Loss: 0.00003130
Iteration 271/1000 | Loss: 0.00003130
Iteration 272/1000 | Loss: 0.00003130
Iteration 273/1000 | Loss: 0.00003129
Iteration 274/1000 | Loss: 0.00003129
Iteration 275/1000 | Loss: 0.00003129
Iteration 276/1000 | Loss: 0.00003129
Iteration 277/1000 | Loss: 0.00003129
Iteration 278/1000 | Loss: 0.00003129
Iteration 279/1000 | Loss: 0.00003129
Iteration 280/1000 | Loss: 0.00003129
Iteration 281/1000 | Loss: 0.00003128
Iteration 282/1000 | Loss: 0.00003128
Iteration 283/1000 | Loss: 0.00003128
Iteration 284/1000 | Loss: 0.00003128
Iteration 285/1000 | Loss: 0.00003128
Iteration 286/1000 | Loss: 0.00003128
Iteration 287/1000 | Loss: 0.00003128
Iteration 288/1000 | Loss: 0.00003128
Iteration 289/1000 | Loss: 0.00003128
Iteration 290/1000 | Loss: 0.00003128
Iteration 291/1000 | Loss: 0.00003128
Iteration 292/1000 | Loss: 0.00003128
Iteration 293/1000 | Loss: 0.00003127
Iteration 294/1000 | Loss: 0.00003127
Iteration 295/1000 | Loss: 0.00003127
Iteration 296/1000 | Loss: 0.00003127
Iteration 297/1000 | Loss: 0.00003127
Iteration 298/1000 | Loss: 0.00003127
Iteration 299/1000 | Loss: 0.00003127
Iteration 300/1000 | Loss: 0.00003127
Iteration 301/1000 | Loss: 0.00003127
Iteration 302/1000 | Loss: 0.00003127
Iteration 303/1000 | Loss: 0.00003127
Iteration 304/1000 | Loss: 0.00003127
Iteration 305/1000 | Loss: 0.00003127
Iteration 306/1000 | Loss: 0.00003127
Iteration 307/1000 | Loss: 0.00003127
Iteration 308/1000 | Loss: 0.00003127
Iteration 309/1000 | Loss: 0.00003127
Iteration 310/1000 | Loss: 0.00003127
Iteration 311/1000 | Loss: 0.00003127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [3.127110539935529e-05, 3.127110539935529e-05, 3.127110539935529e-05, 3.127110539935529e-05, 3.127110539935529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.127110539935529e-05

Optimization complete. Final v2v error: 4.451832294464111 mm

Highest mean error: 6.141425132751465 mm for frame 206

Lowest mean error: 3.1963133811950684 mm for frame 20

Saving results

Total time: 303.2472517490387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861320
Iteration 2/25 | Loss: 0.00142470
Iteration 3/25 | Loss: 0.00113162
Iteration 4/25 | Loss: 0.00101806
Iteration 5/25 | Loss: 0.00099371
Iteration 6/25 | Loss: 0.00103775
Iteration 7/25 | Loss: 0.00101128
Iteration 8/25 | Loss: 0.00093464
Iteration 9/25 | Loss: 0.00088032
Iteration 10/25 | Loss: 0.00083925
Iteration 11/25 | Loss: 0.00081602
Iteration 12/25 | Loss: 0.00080792
Iteration 13/25 | Loss: 0.00079743
Iteration 14/25 | Loss: 0.00079254
Iteration 15/25 | Loss: 0.00078759
Iteration 16/25 | Loss: 0.00078557
Iteration 17/25 | Loss: 0.00078397
Iteration 18/25 | Loss: 0.00078318
Iteration 19/25 | Loss: 0.00079764
Iteration 20/25 | Loss: 0.00080151
Iteration 21/25 | Loss: 0.00081185
Iteration 22/25 | Loss: 0.00081333
Iteration 23/25 | Loss: 0.00082432
Iteration 24/25 | Loss: 0.00080845
Iteration 25/25 | Loss: 0.00081903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46572471
Iteration 2/25 | Loss: 0.00140917
Iteration 3/25 | Loss: 0.00140913
Iteration 4/25 | Loss: 0.00140913
Iteration 5/25 | Loss: 0.00140913
Iteration 6/25 | Loss: 0.00140913
Iteration 7/25 | Loss: 0.00140913
Iteration 8/25 | Loss: 0.00140913
Iteration 9/25 | Loss: 0.00140913
Iteration 10/25 | Loss: 0.00140913
Iteration 11/25 | Loss: 0.00140913
Iteration 12/25 | Loss: 0.00140913
Iteration 13/25 | Loss: 0.00140913
Iteration 14/25 | Loss: 0.00140913
Iteration 15/25 | Loss: 0.00140913
Iteration 16/25 | Loss: 0.00140913
Iteration 17/25 | Loss: 0.00140913
Iteration 18/25 | Loss: 0.00140913
Iteration 19/25 | Loss: 0.00140913
Iteration 20/25 | Loss: 0.00140913
Iteration 21/25 | Loss: 0.00140913
Iteration 22/25 | Loss: 0.00140913
Iteration 23/25 | Loss: 0.00140913
Iteration 24/25 | Loss: 0.00140913
Iteration 25/25 | Loss: 0.00140913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140913
Iteration 2/1000 | Loss: 0.00131838
Iteration 3/1000 | Loss: 0.00052564
Iteration 4/1000 | Loss: 0.00067261
Iteration 5/1000 | Loss: 0.00047146
Iteration 6/1000 | Loss: 0.00035768
Iteration 7/1000 | Loss: 0.00019436
Iteration 8/1000 | Loss: 0.00019181
Iteration 9/1000 | Loss: 0.00012702
Iteration 10/1000 | Loss: 0.00026627
Iteration 11/1000 | Loss: 0.00112874
Iteration 12/1000 | Loss: 0.00099926
Iteration 13/1000 | Loss: 0.00020441
Iteration 14/1000 | Loss: 0.00105677
Iteration 15/1000 | Loss: 0.00058484
Iteration 16/1000 | Loss: 0.00030283
Iteration 17/1000 | Loss: 0.00026910
Iteration 18/1000 | Loss: 0.00019844
Iteration 19/1000 | Loss: 0.00034708
Iteration 20/1000 | Loss: 0.00018509
Iteration 21/1000 | Loss: 0.00029847
Iteration 22/1000 | Loss: 0.00047986
Iteration 23/1000 | Loss: 0.00050959
Iteration 24/1000 | Loss: 0.00055703
Iteration 25/1000 | Loss: 0.00034532
Iteration 26/1000 | Loss: 0.00039369
Iteration 27/1000 | Loss: 0.00025745
Iteration 28/1000 | Loss: 0.00025127
Iteration 29/1000 | Loss: 0.00027141
Iteration 30/1000 | Loss: 0.00022639
Iteration 31/1000 | Loss: 0.00022221
Iteration 32/1000 | Loss: 0.00012360
Iteration 33/1000 | Loss: 0.00008856
Iteration 34/1000 | Loss: 0.00007882
Iteration 35/1000 | Loss: 0.00006574
Iteration 36/1000 | Loss: 0.00020124
Iteration 37/1000 | Loss: 0.00054968
Iteration 38/1000 | Loss: 0.00200505
Iteration 39/1000 | Loss: 0.00049638
Iteration 40/1000 | Loss: 0.00033345
Iteration 41/1000 | Loss: 0.00029079
Iteration 42/1000 | Loss: 0.00038170
Iteration 43/1000 | Loss: 0.00005883
Iteration 44/1000 | Loss: 0.00031670
Iteration 45/1000 | Loss: 0.00099103
Iteration 46/1000 | Loss: 0.00008759
Iteration 47/1000 | Loss: 0.00004990
Iteration 48/1000 | Loss: 0.00004348
Iteration 49/1000 | Loss: 0.00005274
Iteration 50/1000 | Loss: 0.00005010
Iteration 51/1000 | Loss: 0.00003752
Iteration 52/1000 | Loss: 0.00004713
Iteration 53/1000 | Loss: 0.00003327
Iteration 54/1000 | Loss: 0.00004063
Iteration 55/1000 | Loss: 0.00003346
Iteration 56/1000 | Loss: 0.00003171
Iteration 57/1000 | Loss: 0.00003846
Iteration 58/1000 | Loss: 0.00002932
Iteration 59/1000 | Loss: 0.00003541
Iteration 60/1000 | Loss: 0.00003578
Iteration 61/1000 | Loss: 0.00002819
Iteration 62/1000 | Loss: 0.00002911
Iteration 63/1000 | Loss: 0.00002900
Iteration 64/1000 | Loss: 0.00025588
Iteration 65/1000 | Loss: 0.00009707
Iteration 66/1000 | Loss: 0.00003946
Iteration 67/1000 | Loss: 0.00004130
Iteration 68/1000 | Loss: 0.00004081
Iteration 69/1000 | Loss: 0.00003863
Iteration 70/1000 | Loss: 0.00003975
Iteration 71/1000 | Loss: 0.00004042
Iteration 72/1000 | Loss: 0.00003644
Iteration 73/1000 | Loss: 0.00003681
Iteration 74/1000 | Loss: 0.00004121
Iteration 75/1000 | Loss: 0.00004238
Iteration 76/1000 | Loss: 0.00003913
Iteration 77/1000 | Loss: 0.00004542
Iteration 78/1000 | Loss: 0.00003909
Iteration 79/1000 | Loss: 0.00003821
Iteration 80/1000 | Loss: 0.00003406
Iteration 81/1000 | Loss: 0.00003678
Iteration 82/1000 | Loss: 0.00004031
Iteration 83/1000 | Loss: 0.00004119
Iteration 84/1000 | Loss: 0.00003547
Iteration 85/1000 | Loss: 0.00004281
Iteration 86/1000 | Loss: 0.00004560
Iteration 87/1000 | Loss: 0.00004149
Iteration 88/1000 | Loss: 0.00004305
Iteration 89/1000 | Loss: 0.00004006
Iteration 90/1000 | Loss: 0.00003612
Iteration 91/1000 | Loss: 0.00003794
Iteration 92/1000 | Loss: 0.00003527
Iteration 93/1000 | Loss: 0.00003792
Iteration 94/1000 | Loss: 0.00003972
Iteration 95/1000 | Loss: 0.00004202
Iteration 96/1000 | Loss: 0.00003647
Iteration 97/1000 | Loss: 0.00003796
Iteration 98/1000 | Loss: 0.00003617
Iteration 99/1000 | Loss: 0.00004263
Iteration 100/1000 | Loss: 0.00003710
Iteration 101/1000 | Loss: 0.00003241
Iteration 102/1000 | Loss: 0.00003523
Iteration 103/1000 | Loss: 0.00003862
Iteration 104/1000 | Loss: 0.00003714
Iteration 105/1000 | Loss: 0.00004440
Iteration 106/1000 | Loss: 0.00004070
Iteration 107/1000 | Loss: 0.00003325
Iteration 108/1000 | Loss: 0.00003256
Iteration 109/1000 | Loss: 0.00003045
Iteration 110/1000 | Loss: 0.00003862
Iteration 111/1000 | Loss: 0.00002830
Iteration 112/1000 | Loss: 0.00003889
Iteration 113/1000 | Loss: 0.00003760
Iteration 114/1000 | Loss: 0.00004351
Iteration 115/1000 | Loss: 0.00003972
Iteration 116/1000 | Loss: 0.00003674
Iteration 117/1000 | Loss: 0.00003762
Iteration 118/1000 | Loss: 0.00004680
Iteration 119/1000 | Loss: 0.00004149
Iteration 120/1000 | Loss: 0.00003087
Iteration 121/1000 | Loss: 0.00004374
Iteration 122/1000 | Loss: 0.00006296
Iteration 123/1000 | Loss: 0.00006581
Iteration 124/1000 | Loss: 0.00006190
Iteration 125/1000 | Loss: 0.00007298
Iteration 126/1000 | Loss: 0.00005371
Iteration 127/1000 | Loss: 0.00006305
Iteration 128/1000 | Loss: 0.00005483
Iteration 129/1000 | Loss: 0.00005027
Iteration 130/1000 | Loss: 0.00003351
Iteration 131/1000 | Loss: 0.00005393
Iteration 132/1000 | Loss: 0.00004388
Iteration 133/1000 | Loss: 0.00003373
Iteration 134/1000 | Loss: 0.00003056
Iteration 135/1000 | Loss: 0.00002889
Iteration 136/1000 | Loss: 0.00004207
Iteration 137/1000 | Loss: 0.00003526
Iteration 138/1000 | Loss: 0.00003956
Iteration 139/1000 | Loss: 0.00003928
Iteration 140/1000 | Loss: 0.00005402
Iteration 141/1000 | Loss: 0.00004883
Iteration 142/1000 | Loss: 0.00005341
Iteration 143/1000 | Loss: 0.00006133
Iteration 144/1000 | Loss: 0.00007516
Iteration 145/1000 | Loss: 0.00006907
Iteration 146/1000 | Loss: 0.00007187
Iteration 147/1000 | Loss: 0.00006752
Iteration 148/1000 | Loss: 0.00004207
Iteration 149/1000 | Loss: 0.00004325
Iteration 150/1000 | Loss: 0.00004065
Iteration 151/1000 | Loss: 0.00004688
Iteration 152/1000 | Loss: 0.00004510
Iteration 153/1000 | Loss: 0.00006515
Iteration 154/1000 | Loss: 0.00005347
Iteration 155/1000 | Loss: 0.00004579
Iteration 156/1000 | Loss: 0.00006441
Iteration 157/1000 | Loss: 0.00004362
Iteration 158/1000 | Loss: 0.00003377
Iteration 159/1000 | Loss: 0.00003857
Iteration 160/1000 | Loss: 0.00006098
Iteration 161/1000 | Loss: 0.00008821
Iteration 162/1000 | Loss: 0.00004933
Iteration 163/1000 | Loss: 0.00005877
Iteration 164/1000 | Loss: 0.00008292
Iteration 165/1000 | Loss: 0.00006445
Iteration 166/1000 | Loss: 0.00005949
Iteration 167/1000 | Loss: 0.00005222
Iteration 168/1000 | Loss: 0.00004213
Iteration 169/1000 | Loss: 0.00006685
Iteration 170/1000 | Loss: 0.00005108
Iteration 171/1000 | Loss: 0.00006331
Iteration 172/1000 | Loss: 0.00007147
Iteration 173/1000 | Loss: 0.00006143
Iteration 174/1000 | Loss: 0.00006386
Iteration 175/1000 | Loss: 0.00004073
Iteration 176/1000 | Loss: 0.00005490
Iteration 177/1000 | Loss: 0.00006454
Iteration 178/1000 | Loss: 0.00003790
Iteration 179/1000 | Loss: 0.00003249
Iteration 180/1000 | Loss: 0.00003130
Iteration 181/1000 | Loss: 0.00005578
Iteration 182/1000 | Loss: 0.00002827
Iteration 183/1000 | Loss: 0.00002641
Iteration 184/1000 | Loss: 0.00004313
Iteration 185/1000 | Loss: 0.00005291
Iteration 186/1000 | Loss: 0.00004300
Iteration 187/1000 | Loss: 0.00002533
Iteration 188/1000 | Loss: 0.00002455
Iteration 189/1000 | Loss: 0.00004470
Iteration 190/1000 | Loss: 0.00004381
Iteration 191/1000 | Loss: 0.00003414
Iteration 192/1000 | Loss: 0.00003156
Iteration 193/1000 | Loss: 0.00003966
Iteration 194/1000 | Loss: 0.00005237
Iteration 195/1000 | Loss: 0.00004366
Iteration 196/1000 | Loss: 0.00004904
Iteration 197/1000 | Loss: 0.00004425
Iteration 198/1000 | Loss: 0.00013123
Iteration 199/1000 | Loss: 0.00023119
Iteration 200/1000 | Loss: 0.00005510
Iteration 201/1000 | Loss: 0.00002723
Iteration 202/1000 | Loss: 0.00002631
Iteration 203/1000 | Loss: 0.00002574
Iteration 204/1000 | Loss: 0.00002539
Iteration 205/1000 | Loss: 0.00002506
Iteration 206/1000 | Loss: 0.00002469
Iteration 207/1000 | Loss: 0.00003103
Iteration 208/1000 | Loss: 0.00002878
Iteration 209/1000 | Loss: 0.00003053
Iteration 210/1000 | Loss: 0.00002926
Iteration 211/1000 | Loss: 0.00003085
Iteration 212/1000 | Loss: 0.00002493
Iteration 213/1000 | Loss: 0.00003026
Iteration 214/1000 | Loss: 0.00004270
Iteration 215/1000 | Loss: 0.00002645
Iteration 216/1000 | Loss: 0.00002554
Iteration 217/1000 | Loss: 0.00002408
Iteration 218/1000 | Loss: 0.00002312
Iteration 219/1000 | Loss: 0.00002279
Iteration 220/1000 | Loss: 0.00002251
Iteration 221/1000 | Loss: 0.00002231
Iteration 222/1000 | Loss: 0.00002230
Iteration 223/1000 | Loss: 0.00002224
Iteration 224/1000 | Loss: 0.00002222
Iteration 225/1000 | Loss: 0.00002221
Iteration 226/1000 | Loss: 0.00002220
Iteration 227/1000 | Loss: 0.00002220
Iteration 228/1000 | Loss: 0.00002218
Iteration 229/1000 | Loss: 0.00002218
Iteration 230/1000 | Loss: 0.00002218
Iteration 231/1000 | Loss: 0.00002218
Iteration 232/1000 | Loss: 0.00002218
Iteration 233/1000 | Loss: 0.00002218
Iteration 234/1000 | Loss: 0.00002218
Iteration 235/1000 | Loss: 0.00002217
Iteration 236/1000 | Loss: 0.00002217
Iteration 237/1000 | Loss: 0.00002217
Iteration 238/1000 | Loss: 0.00002217
Iteration 239/1000 | Loss: 0.00002217
Iteration 240/1000 | Loss: 0.00002217
Iteration 241/1000 | Loss: 0.00002216
Iteration 242/1000 | Loss: 0.00002216
Iteration 243/1000 | Loss: 0.00002216
Iteration 244/1000 | Loss: 0.00002215
Iteration 245/1000 | Loss: 0.00002215
Iteration 246/1000 | Loss: 0.00002214
Iteration 247/1000 | Loss: 0.00002214
Iteration 248/1000 | Loss: 0.00002213
Iteration 249/1000 | Loss: 0.00002213
Iteration 250/1000 | Loss: 0.00002213
Iteration 251/1000 | Loss: 0.00002212
Iteration 252/1000 | Loss: 0.00002212
Iteration 253/1000 | Loss: 0.00002212
Iteration 254/1000 | Loss: 0.00002212
Iteration 255/1000 | Loss: 0.00002212
Iteration 256/1000 | Loss: 0.00002212
Iteration 257/1000 | Loss: 0.00002211
Iteration 258/1000 | Loss: 0.00002211
Iteration 259/1000 | Loss: 0.00002211
Iteration 260/1000 | Loss: 0.00002211
Iteration 261/1000 | Loss: 0.00002211
Iteration 262/1000 | Loss: 0.00002210
Iteration 263/1000 | Loss: 0.00002210
Iteration 264/1000 | Loss: 0.00002209
Iteration 265/1000 | Loss: 0.00002208
Iteration 266/1000 | Loss: 0.00002208
Iteration 267/1000 | Loss: 0.00002207
Iteration 268/1000 | Loss: 0.00002207
Iteration 269/1000 | Loss: 0.00002207
Iteration 270/1000 | Loss: 0.00002206
Iteration 271/1000 | Loss: 0.00002206
Iteration 272/1000 | Loss: 0.00002206
Iteration 273/1000 | Loss: 0.00002206
Iteration 274/1000 | Loss: 0.00002205
Iteration 275/1000 | Loss: 0.00002205
Iteration 276/1000 | Loss: 0.00002205
Iteration 277/1000 | Loss: 0.00002205
Iteration 278/1000 | Loss: 0.00002204
Iteration 279/1000 | Loss: 0.00002204
Iteration 280/1000 | Loss: 0.00002204
Iteration 281/1000 | Loss: 0.00002203
Iteration 282/1000 | Loss: 0.00002203
Iteration 283/1000 | Loss: 0.00002203
Iteration 284/1000 | Loss: 0.00002203
Iteration 285/1000 | Loss: 0.00002203
Iteration 286/1000 | Loss: 0.00002203
Iteration 287/1000 | Loss: 0.00002203
Iteration 288/1000 | Loss: 0.00002203
Iteration 289/1000 | Loss: 0.00002203
Iteration 290/1000 | Loss: 0.00002203
Iteration 291/1000 | Loss: 0.00002203
Iteration 292/1000 | Loss: 0.00002202
Iteration 293/1000 | Loss: 0.00002202
Iteration 294/1000 | Loss: 0.00002202
Iteration 295/1000 | Loss: 0.00002202
Iteration 296/1000 | Loss: 0.00002202
Iteration 297/1000 | Loss: 0.00002202
Iteration 298/1000 | Loss: 0.00002201
Iteration 299/1000 | Loss: 0.00002201
Iteration 300/1000 | Loss: 0.00002201
Iteration 301/1000 | Loss: 0.00002201
Iteration 302/1000 | Loss: 0.00002201
Iteration 303/1000 | Loss: 0.00002201
Iteration 304/1000 | Loss: 0.00002201
Iteration 305/1000 | Loss: 0.00002201
Iteration 306/1000 | Loss: 0.00002200
Iteration 307/1000 | Loss: 0.00002200
Iteration 308/1000 | Loss: 0.00002200
Iteration 309/1000 | Loss: 0.00002200
Iteration 310/1000 | Loss: 0.00002200
Iteration 311/1000 | Loss: 0.00002200
Iteration 312/1000 | Loss: 0.00002200
Iteration 313/1000 | Loss: 0.00002200
Iteration 314/1000 | Loss: 0.00002200
Iteration 315/1000 | Loss: 0.00002200
Iteration 316/1000 | Loss: 0.00002200
Iteration 317/1000 | Loss: 0.00002200
Iteration 318/1000 | Loss: 0.00002199
Iteration 319/1000 | Loss: 0.00002199
Iteration 320/1000 | Loss: 0.00002199
Iteration 321/1000 | Loss: 0.00002199
Iteration 322/1000 | Loss: 0.00002199
Iteration 323/1000 | Loss: 0.00002198
Iteration 324/1000 | Loss: 0.00002198
Iteration 325/1000 | Loss: 0.00002198
Iteration 326/1000 | Loss: 0.00002198
Iteration 327/1000 | Loss: 0.00002197
Iteration 328/1000 | Loss: 0.00002197
Iteration 329/1000 | Loss: 0.00002197
Iteration 330/1000 | Loss: 0.00002197
Iteration 331/1000 | Loss: 0.00002197
Iteration 332/1000 | Loss: 0.00002197
Iteration 333/1000 | Loss: 0.00002197
Iteration 334/1000 | Loss: 0.00002197
Iteration 335/1000 | Loss: 0.00002197
Iteration 336/1000 | Loss: 0.00002197
Iteration 337/1000 | Loss: 0.00002197
Iteration 338/1000 | Loss: 0.00002197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 338. Stopping optimization.
Last 5 losses: [2.197018329752609e-05, 2.197018329752609e-05, 2.197018329752609e-05, 2.197018329752609e-05, 2.197018329752609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.197018329752609e-05

Optimization complete. Final v2v error: 3.8944058418273926 mm

Highest mean error: 6.5466108322143555 mm for frame 89

Lowest mean error: 3.5023610591888428 mm for frame 60

Saving results

Total time: 415.0052721500397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075049
Iteration 2/25 | Loss: 0.00206757
Iteration 3/25 | Loss: 0.00136169
Iteration 4/25 | Loss: 0.00132819
Iteration 5/25 | Loss: 0.00106220
Iteration 6/25 | Loss: 0.00100251
Iteration 7/25 | Loss: 0.00098715
Iteration 8/25 | Loss: 0.00091380
Iteration 9/25 | Loss: 0.00087096
Iteration 10/25 | Loss: 0.00083395
Iteration 11/25 | Loss: 0.00079986
Iteration 12/25 | Loss: 0.00077404
Iteration 13/25 | Loss: 0.00076291
Iteration 14/25 | Loss: 0.00076059
Iteration 15/25 | Loss: 0.00073964
Iteration 16/25 | Loss: 0.00073138
Iteration 17/25 | Loss: 0.00073454
Iteration 18/25 | Loss: 0.00072844
Iteration 19/25 | Loss: 0.00071916
Iteration 20/25 | Loss: 0.00072102
Iteration 21/25 | Loss: 0.00070915
Iteration 22/25 | Loss: 0.00071916
Iteration 23/25 | Loss: 0.00071162
Iteration 24/25 | Loss: 0.00072110
Iteration 25/25 | Loss: 0.00071895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65189111
Iteration 2/25 | Loss: 0.00156120
Iteration 3/25 | Loss: 0.00122283
Iteration 4/25 | Loss: 0.00122283
Iteration 5/25 | Loss: 0.00122283
Iteration 6/25 | Loss: 0.00122283
Iteration 7/25 | Loss: 0.00122283
Iteration 8/25 | Loss: 0.00122283
Iteration 9/25 | Loss: 0.00122283
Iteration 10/25 | Loss: 0.00122283
Iteration 11/25 | Loss: 0.00122283
Iteration 12/25 | Loss: 0.00122283
Iteration 13/25 | Loss: 0.00122283
Iteration 14/25 | Loss: 0.00122283
Iteration 15/25 | Loss: 0.00122283
Iteration 16/25 | Loss: 0.00122283
Iteration 17/25 | Loss: 0.00122283
Iteration 18/25 | Loss: 0.00122283
Iteration 19/25 | Loss: 0.00122283
Iteration 20/25 | Loss: 0.00122283
Iteration 21/25 | Loss: 0.00122283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001222829450853169, 0.001222829450853169, 0.001222829450853169, 0.001222829450853169, 0.001222829450853169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001222829450853169

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122283
Iteration 2/1000 | Loss: 0.00105895
Iteration 3/1000 | Loss: 0.00096210
Iteration 4/1000 | Loss: 0.00096872
Iteration 5/1000 | Loss: 0.00119690
Iteration 6/1000 | Loss: 0.00091490
Iteration 7/1000 | Loss: 0.00087807
Iteration 8/1000 | Loss: 0.00055354
Iteration 9/1000 | Loss: 0.00065141
Iteration 10/1000 | Loss: 0.00105128
Iteration 11/1000 | Loss: 0.00060485
Iteration 12/1000 | Loss: 0.00054648
Iteration 13/1000 | Loss: 0.00071455
Iteration 14/1000 | Loss: 0.00068103
Iteration 15/1000 | Loss: 0.00057776
Iteration 16/1000 | Loss: 0.00045418
Iteration 17/1000 | Loss: 0.00056136
Iteration 18/1000 | Loss: 0.00057728
Iteration 19/1000 | Loss: 0.00076309
Iteration 20/1000 | Loss: 0.00093430
Iteration 21/1000 | Loss: 0.00093065
Iteration 22/1000 | Loss: 0.00060107
Iteration 23/1000 | Loss: 0.00047637
Iteration 24/1000 | Loss: 0.00050190
Iteration 25/1000 | Loss: 0.00076147
Iteration 26/1000 | Loss: 0.00071766
Iteration 27/1000 | Loss: 0.00063250
Iteration 28/1000 | Loss: 0.00117534
Iteration 29/1000 | Loss: 0.00042084
Iteration 30/1000 | Loss: 0.00061591
Iteration 31/1000 | Loss: 0.00044931
Iteration 32/1000 | Loss: 0.00060143
Iteration 33/1000 | Loss: 0.00063589
Iteration 34/1000 | Loss: 0.00070005
Iteration 35/1000 | Loss: 0.00062326
Iteration 36/1000 | Loss: 0.00063054
Iteration 37/1000 | Loss: 0.00059203
Iteration 38/1000 | Loss: 0.00068205
Iteration 39/1000 | Loss: 0.00090730
Iteration 40/1000 | Loss: 0.00075272
Iteration 41/1000 | Loss: 0.00068147
Iteration 42/1000 | Loss: 0.00068299
Iteration 43/1000 | Loss: 0.00036573
Iteration 44/1000 | Loss: 0.00041706
Iteration 45/1000 | Loss: 0.00109803
Iteration 46/1000 | Loss: 0.00064285
Iteration 47/1000 | Loss: 0.00051779
Iteration 48/1000 | Loss: 0.00046243
Iteration 49/1000 | Loss: 0.00054380
Iteration 50/1000 | Loss: 0.00059308
Iteration 51/1000 | Loss: 0.00048968
Iteration 52/1000 | Loss: 0.00047172
Iteration 53/1000 | Loss: 0.00066290
Iteration 54/1000 | Loss: 0.00092054
Iteration 55/1000 | Loss: 0.00047659
Iteration 56/1000 | Loss: 0.00028215
Iteration 57/1000 | Loss: 0.00060059
Iteration 58/1000 | Loss: 0.00057777
Iteration 59/1000 | Loss: 0.00073349
Iteration 60/1000 | Loss: 0.00072784
Iteration 61/1000 | Loss: 0.00072626
Iteration 62/1000 | Loss: 0.00029282
Iteration 63/1000 | Loss: 0.00079408
Iteration 64/1000 | Loss: 0.00130025
Iteration 65/1000 | Loss: 0.00068176
Iteration 66/1000 | Loss: 0.00077166
Iteration 67/1000 | Loss: 0.00100444
Iteration 68/1000 | Loss: 0.00094536
Iteration 69/1000 | Loss: 0.00073140
Iteration 70/1000 | Loss: 0.00133059
Iteration 71/1000 | Loss: 0.00057400
Iteration 72/1000 | Loss: 0.00074586
Iteration 73/1000 | Loss: 0.00089497
Iteration 74/1000 | Loss: 0.00072450
Iteration 75/1000 | Loss: 0.00081803
Iteration 76/1000 | Loss: 0.00082487
Iteration 77/1000 | Loss: 0.00078438
Iteration 78/1000 | Loss: 0.00034066
Iteration 79/1000 | Loss: 0.00057239
Iteration 80/1000 | Loss: 0.00058995
Iteration 81/1000 | Loss: 0.00075552
Iteration 82/1000 | Loss: 0.00063532
Iteration 83/1000 | Loss: 0.00042409
Iteration 84/1000 | Loss: 0.00032954
Iteration 85/1000 | Loss: 0.00023456
Iteration 86/1000 | Loss: 0.00142516
Iteration 87/1000 | Loss: 0.00070548
Iteration 88/1000 | Loss: 0.00050959
Iteration 89/1000 | Loss: 0.00053619
Iteration 90/1000 | Loss: 0.00053539
Iteration 91/1000 | Loss: 0.00063658
Iteration 92/1000 | Loss: 0.00059348
Iteration 93/1000 | Loss: 0.00060835
Iteration 94/1000 | Loss: 0.00055031
Iteration 95/1000 | Loss: 0.00057102
Iteration 96/1000 | Loss: 0.00038981
Iteration 97/1000 | Loss: 0.00055864
Iteration 98/1000 | Loss: 0.00050492
Iteration 99/1000 | Loss: 0.00064314
Iteration 100/1000 | Loss: 0.00057254
Iteration 101/1000 | Loss: 0.00052140
Iteration 102/1000 | Loss: 0.00065273
Iteration 103/1000 | Loss: 0.00056060
Iteration 104/1000 | Loss: 0.00051900
Iteration 105/1000 | Loss: 0.00030655
Iteration 106/1000 | Loss: 0.00030619
Iteration 107/1000 | Loss: 0.00053415
Iteration 108/1000 | Loss: 0.00030331
Iteration 109/1000 | Loss: 0.00040435
Iteration 110/1000 | Loss: 0.00041430
Iteration 111/1000 | Loss: 0.00048349
Iteration 112/1000 | Loss: 0.00032898
Iteration 113/1000 | Loss: 0.00052539
Iteration 114/1000 | Loss: 0.00061551
Iteration 115/1000 | Loss: 0.00057734
Iteration 116/1000 | Loss: 0.00040103
Iteration 117/1000 | Loss: 0.00052061
Iteration 118/1000 | Loss: 0.00042195
Iteration 119/1000 | Loss: 0.00024320
Iteration 120/1000 | Loss: 0.00037452
Iteration 121/1000 | Loss: 0.00050756
Iteration 122/1000 | Loss: 0.00034153
Iteration 123/1000 | Loss: 0.00035807
Iteration 124/1000 | Loss: 0.00022716
Iteration 125/1000 | Loss: 0.00027906
Iteration 126/1000 | Loss: 0.00043954
Iteration 127/1000 | Loss: 0.00044791
Iteration 128/1000 | Loss: 0.00037865
Iteration 129/1000 | Loss: 0.00033325
Iteration 130/1000 | Loss: 0.00053594
Iteration 131/1000 | Loss: 0.00045744
Iteration 132/1000 | Loss: 0.00043470
Iteration 133/1000 | Loss: 0.00038035
Iteration 134/1000 | Loss: 0.00038907
Iteration 135/1000 | Loss: 0.00027184
Iteration 136/1000 | Loss: 0.00034551
Iteration 137/1000 | Loss: 0.00032484
Iteration 138/1000 | Loss: 0.00016472
Iteration 139/1000 | Loss: 0.00016197
Iteration 140/1000 | Loss: 0.00025661
Iteration 141/1000 | Loss: 0.00022523
Iteration 142/1000 | Loss: 0.00054100
Iteration 143/1000 | Loss: 0.00028574
Iteration 144/1000 | Loss: 0.00028593
Iteration 145/1000 | Loss: 0.00043379
Iteration 146/1000 | Loss: 0.00068824
Iteration 147/1000 | Loss: 0.00033330
Iteration 148/1000 | Loss: 0.00048114
Iteration 149/1000 | Loss: 0.00046230
Iteration 150/1000 | Loss: 0.00069914
Iteration 151/1000 | Loss: 0.00055758
Iteration 152/1000 | Loss: 0.00041147
Iteration 153/1000 | Loss: 0.00053078
Iteration 154/1000 | Loss: 0.00035200
Iteration 155/1000 | Loss: 0.00038866
Iteration 156/1000 | Loss: 0.00041240
Iteration 157/1000 | Loss: 0.00054707
Iteration 158/1000 | Loss: 0.00078002
Iteration 159/1000 | Loss: 0.00051238
Iteration 160/1000 | Loss: 0.00059884
Iteration 161/1000 | Loss: 0.00085133
Iteration 162/1000 | Loss: 0.00063159
Iteration 163/1000 | Loss: 0.00011772
Iteration 164/1000 | Loss: 0.00024921
Iteration 165/1000 | Loss: 0.00017950
Iteration 166/1000 | Loss: 0.00040823
Iteration 167/1000 | Loss: 0.00101003
Iteration 168/1000 | Loss: 0.00033022
Iteration 169/1000 | Loss: 0.00057912
Iteration 170/1000 | Loss: 0.00050535
Iteration 171/1000 | Loss: 0.00063604
Iteration 172/1000 | Loss: 0.00056819
Iteration 173/1000 | Loss: 0.00047627
Iteration 174/1000 | Loss: 0.00063360
Iteration 175/1000 | Loss: 0.00015641
Iteration 176/1000 | Loss: 0.00013193
Iteration 177/1000 | Loss: 0.00014734
Iteration 178/1000 | Loss: 0.00011244
Iteration 179/1000 | Loss: 0.00052061
Iteration 180/1000 | Loss: 0.00065964
Iteration 181/1000 | Loss: 0.00005726
Iteration 182/1000 | Loss: 0.00023816
Iteration 183/1000 | Loss: 0.00017897
Iteration 184/1000 | Loss: 0.00022637
Iteration 185/1000 | Loss: 0.00023188
Iteration 186/1000 | Loss: 0.00023323
Iteration 187/1000 | Loss: 0.00016682
Iteration 188/1000 | Loss: 0.00037053
Iteration 189/1000 | Loss: 0.00022078
Iteration 190/1000 | Loss: 0.00021039
Iteration 191/1000 | Loss: 0.00043600
Iteration 192/1000 | Loss: 0.00038761
Iteration 193/1000 | Loss: 0.00046685
Iteration 194/1000 | Loss: 0.00074395
Iteration 195/1000 | Loss: 0.00050605
Iteration 196/1000 | Loss: 0.00037787
Iteration 197/1000 | Loss: 0.00044141
Iteration 198/1000 | Loss: 0.00049710
Iteration 199/1000 | Loss: 0.00010762
Iteration 200/1000 | Loss: 0.00014924
Iteration 201/1000 | Loss: 0.00023623
Iteration 202/1000 | Loss: 0.00032791
Iteration 203/1000 | Loss: 0.00018777
Iteration 204/1000 | Loss: 0.00017813
Iteration 205/1000 | Loss: 0.00027836
Iteration 206/1000 | Loss: 0.00039870
Iteration 207/1000 | Loss: 0.00052301
Iteration 208/1000 | Loss: 0.00024491
Iteration 209/1000 | Loss: 0.00050120
Iteration 210/1000 | Loss: 0.00050873
Iteration 211/1000 | Loss: 0.00031484
Iteration 212/1000 | Loss: 0.00062273
Iteration 213/1000 | Loss: 0.00035483
Iteration 214/1000 | Loss: 0.00025908
Iteration 215/1000 | Loss: 0.00045180
Iteration 216/1000 | Loss: 0.00049819
Iteration 217/1000 | Loss: 0.00066790
Iteration 218/1000 | Loss: 0.00038865
Iteration 219/1000 | Loss: 0.00034733
Iteration 220/1000 | Loss: 0.00019576
Iteration 221/1000 | Loss: 0.00013789
Iteration 222/1000 | Loss: 0.00005881
Iteration 223/1000 | Loss: 0.00037640
Iteration 224/1000 | Loss: 0.00064061
Iteration 225/1000 | Loss: 0.00010320
Iteration 226/1000 | Loss: 0.00032038
Iteration 227/1000 | Loss: 0.00016102
Iteration 228/1000 | Loss: 0.00023797
Iteration 229/1000 | Loss: 0.00014959
Iteration 230/1000 | Loss: 0.00033582
Iteration 231/1000 | Loss: 0.00030631
Iteration 232/1000 | Loss: 0.00110346
Iteration 233/1000 | Loss: 0.00034909
Iteration 234/1000 | Loss: 0.00046591
Iteration 235/1000 | Loss: 0.00022342
Iteration 236/1000 | Loss: 0.00046564
Iteration 237/1000 | Loss: 0.00013833
Iteration 238/1000 | Loss: 0.00009359
Iteration 239/1000 | Loss: 0.00011371
Iteration 240/1000 | Loss: 0.00015045
Iteration 241/1000 | Loss: 0.00047667
Iteration 242/1000 | Loss: 0.00025245
Iteration 243/1000 | Loss: 0.00055135
Iteration 244/1000 | Loss: 0.00015032
Iteration 245/1000 | Loss: 0.00004280
Iteration 246/1000 | Loss: 0.00008036
Iteration 247/1000 | Loss: 0.00014839
Iteration 248/1000 | Loss: 0.00031853
Iteration 249/1000 | Loss: 0.00031554
Iteration 250/1000 | Loss: 0.00011866
Iteration 251/1000 | Loss: 0.00006837
Iteration 252/1000 | Loss: 0.00006022
Iteration 253/1000 | Loss: 0.00042283
Iteration 254/1000 | Loss: 0.00027286
Iteration 255/1000 | Loss: 0.00008550
Iteration 256/1000 | Loss: 0.00008256
Iteration 257/1000 | Loss: 0.00026795
Iteration 258/1000 | Loss: 0.00008208
Iteration 259/1000 | Loss: 0.00025628
Iteration 260/1000 | Loss: 0.00013826
Iteration 261/1000 | Loss: 0.00034958
Iteration 262/1000 | Loss: 0.00024428
Iteration 263/1000 | Loss: 0.00014162
Iteration 264/1000 | Loss: 0.00021938
Iteration 265/1000 | Loss: 0.00010043
Iteration 266/1000 | Loss: 0.00011214
Iteration 267/1000 | Loss: 0.00006865
Iteration 268/1000 | Loss: 0.00006242
Iteration 269/1000 | Loss: 0.00006038
Iteration 270/1000 | Loss: 0.00009096
Iteration 271/1000 | Loss: 0.00008356
Iteration 272/1000 | Loss: 0.00006127
Iteration 273/1000 | Loss: 0.00022631
Iteration 274/1000 | Loss: 0.00015500
Iteration 275/1000 | Loss: 0.00009775
Iteration 276/1000 | Loss: 0.00007533
Iteration 277/1000 | Loss: 0.00034797
Iteration 278/1000 | Loss: 0.00020600
Iteration 279/1000 | Loss: 0.00038285
Iteration 280/1000 | Loss: 0.00026678
Iteration 281/1000 | Loss: 0.00025736
Iteration 282/1000 | Loss: 0.00023035
Iteration 283/1000 | Loss: 0.00022349
Iteration 284/1000 | Loss: 0.00018079
Iteration 285/1000 | Loss: 0.00019732
Iteration 286/1000 | Loss: 0.00004735
Iteration 287/1000 | Loss: 0.00003522
Iteration 288/1000 | Loss: 0.00010189
Iteration 289/1000 | Loss: 0.00008586
Iteration 290/1000 | Loss: 0.00008787
Iteration 291/1000 | Loss: 0.00007596
Iteration 292/1000 | Loss: 0.00014554
Iteration 293/1000 | Loss: 0.00009505
Iteration 294/1000 | Loss: 0.00012629
Iteration 295/1000 | Loss: 0.00005632
Iteration 296/1000 | Loss: 0.00010659
Iteration 297/1000 | Loss: 0.00023890
Iteration 298/1000 | Loss: 0.00024673
Iteration 299/1000 | Loss: 0.00035751
Iteration 300/1000 | Loss: 0.00015437
Iteration 301/1000 | Loss: 0.00011152
Iteration 302/1000 | Loss: 0.00009032
Iteration 303/1000 | Loss: 0.00010822
Iteration 304/1000 | Loss: 0.00009804
Iteration 305/1000 | Loss: 0.00020206
Iteration 306/1000 | Loss: 0.00020196
Iteration 307/1000 | Loss: 0.00005772
Iteration 308/1000 | Loss: 0.00010393
Iteration 309/1000 | Loss: 0.00018521
Iteration 310/1000 | Loss: 0.00011128
Iteration 311/1000 | Loss: 0.00016773
Iteration 312/1000 | Loss: 0.00024224
Iteration 313/1000 | Loss: 0.00019461
Iteration 314/1000 | Loss: 0.00008001
Iteration 315/1000 | Loss: 0.00013392
Iteration 316/1000 | Loss: 0.00015660
Iteration 317/1000 | Loss: 0.00023765
Iteration 318/1000 | Loss: 0.00020887
Iteration 319/1000 | Loss: 0.00024476
Iteration 320/1000 | Loss: 0.00033154
Iteration 321/1000 | Loss: 0.00028912
Iteration 322/1000 | Loss: 0.00101315
Iteration 323/1000 | Loss: 0.00051677
Iteration 324/1000 | Loss: 0.00039663
Iteration 325/1000 | Loss: 0.00003089
Iteration 326/1000 | Loss: 0.00004561
Iteration 327/1000 | Loss: 0.00004859
Iteration 328/1000 | Loss: 0.00004122
Iteration 329/1000 | Loss: 0.00004515
Iteration 330/1000 | Loss: 0.00003522
Iteration 331/1000 | Loss: 0.00004475
Iteration 332/1000 | Loss: 0.00006241
Iteration 333/1000 | Loss: 0.00004296
Iteration 334/1000 | Loss: 0.00006814
Iteration 335/1000 | Loss: 0.00002713
Iteration 336/1000 | Loss: 0.00002266
Iteration 337/1000 | Loss: 0.00001922
Iteration 338/1000 | Loss: 0.00001774
Iteration 339/1000 | Loss: 0.00001703
Iteration 340/1000 | Loss: 0.00001657
Iteration 341/1000 | Loss: 0.00001626
Iteration 342/1000 | Loss: 0.00001626
Iteration 343/1000 | Loss: 0.00001602
Iteration 344/1000 | Loss: 0.00001587
Iteration 345/1000 | Loss: 0.00001585
Iteration 346/1000 | Loss: 0.00001584
Iteration 347/1000 | Loss: 0.00001584
Iteration 348/1000 | Loss: 0.00001573
Iteration 349/1000 | Loss: 0.00001572
Iteration 350/1000 | Loss: 0.00001570
Iteration 351/1000 | Loss: 0.00001568
Iteration 352/1000 | Loss: 0.00001563
Iteration 353/1000 | Loss: 0.00001563
Iteration 354/1000 | Loss: 0.00001562
Iteration 355/1000 | Loss: 0.00001562
Iteration 356/1000 | Loss: 0.00001558
Iteration 357/1000 | Loss: 0.00001558
Iteration 358/1000 | Loss: 0.00001558
Iteration 359/1000 | Loss: 0.00001558
Iteration 360/1000 | Loss: 0.00001558
Iteration 361/1000 | Loss: 0.00001558
Iteration 362/1000 | Loss: 0.00001558
Iteration 363/1000 | Loss: 0.00001558
Iteration 364/1000 | Loss: 0.00001556
Iteration 365/1000 | Loss: 0.00001556
Iteration 366/1000 | Loss: 0.00001556
Iteration 367/1000 | Loss: 0.00001556
Iteration 368/1000 | Loss: 0.00001556
Iteration 369/1000 | Loss: 0.00001556
Iteration 370/1000 | Loss: 0.00001556
Iteration 371/1000 | Loss: 0.00001555
Iteration 372/1000 | Loss: 0.00001555
Iteration 373/1000 | Loss: 0.00001555
Iteration 374/1000 | Loss: 0.00001555
Iteration 375/1000 | Loss: 0.00001555
Iteration 376/1000 | Loss: 0.00001555
Iteration 377/1000 | Loss: 0.00001555
Iteration 378/1000 | Loss: 0.00001555
Iteration 379/1000 | Loss: 0.00001555
Iteration 380/1000 | Loss: 0.00001554
Iteration 381/1000 | Loss: 0.00001554
Iteration 382/1000 | Loss: 0.00001554
Iteration 383/1000 | Loss: 0.00001554
Iteration 384/1000 | Loss: 0.00001554
Iteration 385/1000 | Loss: 0.00001554
Iteration 386/1000 | Loss: 0.00001554
Iteration 387/1000 | Loss: 0.00001554
Iteration 388/1000 | Loss: 0.00001554
Iteration 389/1000 | Loss: 0.00001554
Iteration 390/1000 | Loss: 0.00001554
Iteration 391/1000 | Loss: 0.00001554
Iteration 392/1000 | Loss: 0.00001553
Iteration 393/1000 | Loss: 0.00001553
Iteration 394/1000 | Loss: 0.00001553
Iteration 395/1000 | Loss: 0.00001553
Iteration 396/1000 | Loss: 0.00001553
Iteration 397/1000 | Loss: 0.00001553
Iteration 398/1000 | Loss: 0.00001553
Iteration 399/1000 | Loss: 0.00001553
Iteration 400/1000 | Loss: 0.00001553
Iteration 401/1000 | Loss: 0.00001553
Iteration 402/1000 | Loss: 0.00001553
Iteration 403/1000 | Loss: 0.00001553
Iteration 404/1000 | Loss: 0.00001553
Iteration 405/1000 | Loss: 0.00001553
Iteration 406/1000 | Loss: 0.00001552
Iteration 407/1000 | Loss: 0.00001552
Iteration 408/1000 | Loss: 0.00001552
Iteration 409/1000 | Loss: 0.00001552
Iteration 410/1000 | Loss: 0.00001552
Iteration 411/1000 | Loss: 0.00001552
Iteration 412/1000 | Loss: 0.00001552
Iteration 413/1000 | Loss: 0.00001552
Iteration 414/1000 | Loss: 0.00001552
Iteration 415/1000 | Loss: 0.00001552
Iteration 416/1000 | Loss: 0.00001552
Iteration 417/1000 | Loss: 0.00001552
Iteration 418/1000 | Loss: 0.00001552
Iteration 419/1000 | Loss: 0.00001552
Iteration 420/1000 | Loss: 0.00001552
Iteration 421/1000 | Loss: 0.00001551
Iteration 422/1000 | Loss: 0.00001551
Iteration 423/1000 | Loss: 0.00001551
Iteration 424/1000 | Loss: 0.00001551
Iteration 425/1000 | Loss: 0.00001551
Iteration 426/1000 | Loss: 0.00001551
Iteration 427/1000 | Loss: 0.00001551
Iteration 428/1000 | Loss: 0.00001551
Iteration 429/1000 | Loss: 0.00001551
Iteration 430/1000 | Loss: 0.00001551
Iteration 431/1000 | Loss: 0.00001551
Iteration 432/1000 | Loss: 0.00001551
Iteration 433/1000 | Loss: 0.00001551
Iteration 434/1000 | Loss: 0.00001551
Iteration 435/1000 | Loss: 0.00001551
Iteration 436/1000 | Loss: 0.00001551
Iteration 437/1000 | Loss: 0.00001551
Iteration 438/1000 | Loss: 0.00001551
Iteration 439/1000 | Loss: 0.00001550
Iteration 440/1000 | Loss: 0.00001550
Iteration 441/1000 | Loss: 0.00001550
Iteration 442/1000 | Loss: 0.00001550
Iteration 443/1000 | Loss: 0.00001550
Iteration 444/1000 | Loss: 0.00001550
Iteration 445/1000 | Loss: 0.00001550
Iteration 446/1000 | Loss: 0.00001550
Iteration 447/1000 | Loss: 0.00001550
Iteration 448/1000 | Loss: 0.00001550
Iteration 449/1000 | Loss: 0.00001550
Iteration 450/1000 | Loss: 0.00001550
Iteration 451/1000 | Loss: 0.00001550
Iteration 452/1000 | Loss: 0.00001550
Iteration 453/1000 | Loss: 0.00001550
Iteration 454/1000 | Loss: 0.00001550
Iteration 455/1000 | Loss: 0.00001550
Iteration 456/1000 | Loss: 0.00001550
Iteration 457/1000 | Loss: 0.00001550
Iteration 458/1000 | Loss: 0.00001550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 458. Stopping optimization.
Last 5 losses: [1.5499004803132266e-05, 1.5499004803132266e-05, 1.5499004803132266e-05, 1.5499004803132266e-05, 1.5499004803132266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5499004803132266e-05

Optimization complete. Final v2v error: 3.334181308746338 mm

Highest mean error: 3.9290688037872314 mm for frame 62

Lowest mean error: 2.9712347984313965 mm for frame 91

Saving results

Total time: 518.0974299907684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007591
Iteration 2/25 | Loss: 0.00404590
Iteration 3/25 | Loss: 0.00261578
Iteration 4/25 | Loss: 0.00269559
Iteration 5/25 | Loss: 0.00160621
Iteration 6/25 | Loss: 0.00175354
Iteration 7/25 | Loss: 0.00168713
Iteration 8/25 | Loss: 0.00157889
Iteration 9/25 | Loss: 0.00158850
Iteration 10/25 | Loss: 0.00144400
Iteration 11/25 | Loss: 0.00143374
Iteration 12/25 | Loss: 0.00142558
Iteration 13/25 | Loss: 0.00142709
Iteration 14/25 | Loss: 0.00142430
Iteration 15/25 | Loss: 0.00142505
Iteration 16/25 | Loss: 0.00142078
Iteration 17/25 | Loss: 0.00141931
Iteration 18/25 | Loss: 0.00141550
Iteration 19/25 | Loss: 0.00141898
Iteration 20/25 | Loss: 0.00141814
Iteration 21/25 | Loss: 0.00141716
Iteration 22/25 | Loss: 0.00141100
Iteration 23/25 | Loss: 0.00141524
Iteration 24/25 | Loss: 0.00141246
Iteration 25/25 | Loss: 0.00140761

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44404149
Iteration 2/25 | Loss: 0.01566686
Iteration 3/25 | Loss: 0.00933700
Iteration 4/25 | Loss: 0.00933695
Iteration 5/25 | Loss: 0.00933694
Iteration 6/25 | Loss: 0.00933694
Iteration 7/25 | Loss: 0.00933694
Iteration 8/25 | Loss: 0.00933694
Iteration 9/25 | Loss: 0.00933694
Iteration 10/25 | Loss: 0.00933694
Iteration 11/25 | Loss: 0.00933694
Iteration 12/25 | Loss: 0.00933694
Iteration 13/25 | Loss: 0.00933694
Iteration 14/25 | Loss: 0.00933694
Iteration 15/25 | Loss: 0.00933694
Iteration 16/25 | Loss: 0.00933694
Iteration 17/25 | Loss: 0.00933694
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.009336940012872219, 0.009336940012872219, 0.009336940012872219, 0.009336940012872219, 0.009336940012872219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009336940012872219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00933694
Iteration 2/1000 | Loss: 0.00644984
Iteration 3/1000 | Loss: 0.00592671
Iteration 4/1000 | Loss: 0.00094362
Iteration 5/1000 | Loss: 0.00129214
Iteration 6/1000 | Loss: 0.00339033
Iteration 7/1000 | Loss: 0.00553210
Iteration 8/1000 | Loss: 0.00199099
Iteration 9/1000 | Loss: 0.00064062
Iteration 10/1000 | Loss: 0.00053385
Iteration 11/1000 | Loss: 0.00045943
Iteration 12/1000 | Loss: 0.00098162
Iteration 13/1000 | Loss: 0.00200236
Iteration 14/1000 | Loss: 0.00119514
Iteration 15/1000 | Loss: 0.00049952
Iteration 16/1000 | Loss: 0.00041930
Iteration 17/1000 | Loss: 0.00052744
Iteration 18/1000 | Loss: 0.00037652
Iteration 19/1000 | Loss: 0.00050897
Iteration 20/1000 | Loss: 0.00045687
Iteration 21/1000 | Loss: 0.00035001
Iteration 22/1000 | Loss: 0.00051226
Iteration 23/1000 | Loss: 0.00057577
Iteration 24/1000 | Loss: 0.00043147
Iteration 25/1000 | Loss: 0.00031983
Iteration 26/1000 | Loss: 0.00055175
Iteration 27/1000 | Loss: 0.00053798
Iteration 28/1000 | Loss: 0.00063805
Iteration 29/1000 | Loss: 0.00056900
Iteration 30/1000 | Loss: 0.00115111
Iteration 31/1000 | Loss: 0.00054394
Iteration 32/1000 | Loss: 0.00029549
Iteration 33/1000 | Loss: 0.00028876
Iteration 34/1000 | Loss: 0.00028364
Iteration 35/1000 | Loss: 0.00049318
Iteration 36/1000 | Loss: 0.00050975
Iteration 37/1000 | Loss: 0.00079182
Iteration 38/1000 | Loss: 0.00053656
Iteration 39/1000 | Loss: 0.00096286
Iteration 40/1000 | Loss: 0.00035914
Iteration 41/1000 | Loss: 0.00054904
Iteration 42/1000 | Loss: 0.00035424
Iteration 43/1000 | Loss: 0.00041433
Iteration 44/1000 | Loss: 0.00079311
Iteration 45/1000 | Loss: 0.00270526
Iteration 46/1000 | Loss: 0.00972902
Iteration 47/1000 | Loss: 0.00589854
Iteration 48/1000 | Loss: 0.00297752
Iteration 49/1000 | Loss: 0.00139868
Iteration 50/1000 | Loss: 0.00227273
Iteration 51/1000 | Loss: 0.00340756
Iteration 52/1000 | Loss: 0.00190859
Iteration 53/1000 | Loss: 0.00763941
Iteration 54/1000 | Loss: 0.00372788
Iteration 55/1000 | Loss: 0.00343545
Iteration 56/1000 | Loss: 0.00095581
Iteration 57/1000 | Loss: 0.00078641
Iteration 58/1000 | Loss: 0.00086770
Iteration 59/1000 | Loss: 0.00053139
Iteration 60/1000 | Loss: 0.00046304
Iteration 61/1000 | Loss: 0.00257565
Iteration 62/1000 | Loss: 0.00411508
Iteration 63/1000 | Loss: 0.00314731
Iteration 64/1000 | Loss: 0.00224429
Iteration 65/1000 | Loss: 0.00108375
Iteration 66/1000 | Loss: 0.00058304
Iteration 67/1000 | Loss: 0.00123338
Iteration 68/1000 | Loss: 0.00149062
Iteration 69/1000 | Loss: 0.00064567
Iteration 70/1000 | Loss: 0.00022461
Iteration 71/1000 | Loss: 0.00138172
Iteration 72/1000 | Loss: 0.00060649
Iteration 73/1000 | Loss: 0.00087839
Iteration 74/1000 | Loss: 0.00035175
Iteration 75/1000 | Loss: 0.00181403
Iteration 76/1000 | Loss: 0.00056825
Iteration 77/1000 | Loss: 0.00012772
Iteration 78/1000 | Loss: 0.00018140
Iteration 79/1000 | Loss: 0.00011132
Iteration 80/1000 | Loss: 0.00045788
Iteration 81/1000 | Loss: 0.00096089
Iteration 82/1000 | Loss: 0.00181619
Iteration 83/1000 | Loss: 0.00364117
Iteration 84/1000 | Loss: 0.00088282
Iteration 85/1000 | Loss: 0.00167674
Iteration 86/1000 | Loss: 0.00066564
Iteration 87/1000 | Loss: 0.00077127
Iteration 88/1000 | Loss: 0.00019752
Iteration 89/1000 | Loss: 0.00019768
Iteration 90/1000 | Loss: 0.00053626
Iteration 91/1000 | Loss: 0.00007701
Iteration 92/1000 | Loss: 0.00012798
Iteration 93/1000 | Loss: 0.00006777
Iteration 94/1000 | Loss: 0.00017339
Iteration 95/1000 | Loss: 0.00006481
Iteration 96/1000 | Loss: 0.00014757
Iteration 97/1000 | Loss: 0.00102045
Iteration 98/1000 | Loss: 0.00010630
Iteration 99/1000 | Loss: 0.00007900
Iteration 100/1000 | Loss: 0.00010264
Iteration 101/1000 | Loss: 0.00008309
Iteration 102/1000 | Loss: 0.00006072
Iteration 103/1000 | Loss: 0.00006030
Iteration 104/1000 | Loss: 0.00029457
Iteration 105/1000 | Loss: 0.00013279
Iteration 106/1000 | Loss: 0.00006025
Iteration 107/1000 | Loss: 0.00005931
Iteration 108/1000 | Loss: 0.00013470
Iteration 109/1000 | Loss: 0.00005926
Iteration 110/1000 | Loss: 0.00012619
Iteration 111/1000 | Loss: 0.00028208
Iteration 112/1000 | Loss: 0.00010319
Iteration 113/1000 | Loss: 0.00016569
Iteration 114/1000 | Loss: 0.00008321
Iteration 115/1000 | Loss: 0.00005857
Iteration 116/1000 | Loss: 0.00005846
Iteration 117/1000 | Loss: 0.00005831
Iteration 118/1000 | Loss: 0.00005825
Iteration 119/1000 | Loss: 0.00016903
Iteration 120/1000 | Loss: 0.00005885
Iteration 121/1000 | Loss: 0.00005819
Iteration 122/1000 | Loss: 0.00005817
Iteration 123/1000 | Loss: 0.00005813
Iteration 124/1000 | Loss: 0.00005811
Iteration 125/1000 | Loss: 0.00005811
Iteration 126/1000 | Loss: 0.00005811
Iteration 127/1000 | Loss: 0.00005811
Iteration 128/1000 | Loss: 0.00005811
Iteration 129/1000 | Loss: 0.00005811
Iteration 130/1000 | Loss: 0.00005810
Iteration 131/1000 | Loss: 0.00005810
Iteration 132/1000 | Loss: 0.00005810
Iteration 133/1000 | Loss: 0.00005810
Iteration 134/1000 | Loss: 0.00005809
Iteration 135/1000 | Loss: 0.00005809
Iteration 136/1000 | Loss: 0.00005809
Iteration 137/1000 | Loss: 0.00005809
Iteration 138/1000 | Loss: 0.00005809
Iteration 139/1000 | Loss: 0.00005809
Iteration 140/1000 | Loss: 0.00005809
Iteration 141/1000 | Loss: 0.00005809
Iteration 142/1000 | Loss: 0.00005809
Iteration 143/1000 | Loss: 0.00005809
Iteration 144/1000 | Loss: 0.00005808
Iteration 145/1000 | Loss: 0.00005808
Iteration 146/1000 | Loss: 0.00005808
Iteration 147/1000 | Loss: 0.00013243
Iteration 148/1000 | Loss: 0.00005835
Iteration 149/1000 | Loss: 0.00005806
Iteration 150/1000 | Loss: 0.00005805
Iteration 151/1000 | Loss: 0.00005805
Iteration 152/1000 | Loss: 0.00005804
Iteration 153/1000 | Loss: 0.00005804
Iteration 154/1000 | Loss: 0.00005803
Iteration 155/1000 | Loss: 0.00005803
Iteration 156/1000 | Loss: 0.00005802
Iteration 157/1000 | Loss: 0.00005802
Iteration 158/1000 | Loss: 0.00005802
Iteration 159/1000 | Loss: 0.00005802
Iteration 160/1000 | Loss: 0.00005802
Iteration 161/1000 | Loss: 0.00005802
Iteration 162/1000 | Loss: 0.00005802
Iteration 163/1000 | Loss: 0.00005802
Iteration 164/1000 | Loss: 0.00005801
Iteration 165/1000 | Loss: 0.00005801
Iteration 166/1000 | Loss: 0.00005801
Iteration 167/1000 | Loss: 0.00005801
Iteration 168/1000 | Loss: 0.00005800
Iteration 169/1000 | Loss: 0.00005800
Iteration 170/1000 | Loss: 0.00005800
Iteration 171/1000 | Loss: 0.00005800
Iteration 172/1000 | Loss: 0.00005800
Iteration 173/1000 | Loss: 0.00005800
Iteration 174/1000 | Loss: 0.00005800
Iteration 175/1000 | Loss: 0.00005800
Iteration 176/1000 | Loss: 0.00005799
Iteration 177/1000 | Loss: 0.00005799
Iteration 178/1000 | Loss: 0.00005799
Iteration 179/1000 | Loss: 0.00005799
Iteration 180/1000 | Loss: 0.00005799
Iteration 181/1000 | Loss: 0.00005799
Iteration 182/1000 | Loss: 0.00005799
Iteration 183/1000 | Loss: 0.00005799
Iteration 184/1000 | Loss: 0.00005799
Iteration 185/1000 | Loss: 0.00005799
Iteration 186/1000 | Loss: 0.00005799
Iteration 187/1000 | Loss: 0.00005799
Iteration 188/1000 | Loss: 0.00005799
Iteration 189/1000 | Loss: 0.00005799
Iteration 190/1000 | Loss: 0.00005799
Iteration 191/1000 | Loss: 0.00005798
Iteration 192/1000 | Loss: 0.00005798
Iteration 193/1000 | Loss: 0.00005798
Iteration 194/1000 | Loss: 0.00005798
Iteration 195/1000 | Loss: 0.00005798
Iteration 196/1000 | Loss: 0.00005798
Iteration 197/1000 | Loss: 0.00005798
Iteration 198/1000 | Loss: 0.00005798
Iteration 199/1000 | Loss: 0.00005798
Iteration 200/1000 | Loss: 0.00005798
Iteration 201/1000 | Loss: 0.00005798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [5.798056008643471e-05, 5.798056008643471e-05, 5.798056008643471e-05, 5.798056008643471e-05, 5.798056008643471e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.798056008643471e-05

Optimization complete. Final v2v error: 4.769467353820801 mm

Highest mean error: 11.504950523376465 mm for frame 50

Lowest mean error: 3.7836902141571045 mm for frame 89

Saving results

Total time: 250.55755233764648
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080114
Iteration 2/25 | Loss: 0.01080114
Iteration 3/25 | Loss: 0.01080114
Iteration 4/25 | Loss: 0.01080114
Iteration 5/25 | Loss: 0.01080113
Iteration 6/25 | Loss: 0.01080113
Iteration 7/25 | Loss: 0.01080113
Iteration 8/25 | Loss: 0.01080113
Iteration 9/25 | Loss: 0.01080112
Iteration 10/25 | Loss: 0.01080112
Iteration 11/25 | Loss: 0.01080112
Iteration 12/25 | Loss: 0.01080112
Iteration 13/25 | Loss: 0.01080112
Iteration 14/25 | Loss: 0.01080112
Iteration 15/25 | Loss: 0.01080112
Iteration 16/25 | Loss: 0.01080112
Iteration 17/25 | Loss: 0.01080111
Iteration 18/25 | Loss: 0.01080111
Iteration 19/25 | Loss: 0.01080111
Iteration 20/25 | Loss: 0.01080111
Iteration 21/25 | Loss: 0.01080111
Iteration 22/25 | Loss: 0.01080111
Iteration 23/25 | Loss: 0.01080111
Iteration 24/25 | Loss: 0.01080110
Iteration 25/25 | Loss: 0.01080110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.17259002
Iteration 2/25 | Loss: 0.11169651
Iteration 3/25 | Loss: 0.10842541
Iteration 4/25 | Loss: 0.10739674
Iteration 5/25 | Loss: 0.10739671
Iteration 6/25 | Loss: 0.10739671
Iteration 7/25 | Loss: 0.10739671
Iteration 8/25 | Loss: 0.10739671
Iteration 9/25 | Loss: 0.10739671
Iteration 10/25 | Loss: 0.10739671
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.10739671438932419, 0.10739671438932419, 0.10739671438932419, 0.10739671438932419, 0.10739671438932419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10739671438932419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10739671
Iteration 2/1000 | Loss: 0.00186470
Iteration 3/1000 | Loss: 0.00216352
Iteration 4/1000 | Loss: 0.00053917
Iteration 5/1000 | Loss: 0.00027411
Iteration 6/1000 | Loss: 0.00112082
Iteration 7/1000 | Loss: 0.00009947
Iteration 8/1000 | Loss: 0.00023593
Iteration 9/1000 | Loss: 0.00011028
Iteration 10/1000 | Loss: 0.00010519
Iteration 11/1000 | Loss: 0.00006041
Iteration 12/1000 | Loss: 0.00008203
Iteration 13/1000 | Loss: 0.00020147
Iteration 14/1000 | Loss: 0.00012107
Iteration 15/1000 | Loss: 0.00045976
Iteration 16/1000 | Loss: 0.00005507
Iteration 17/1000 | Loss: 0.00006146
Iteration 18/1000 | Loss: 0.00020823
Iteration 19/1000 | Loss: 0.00004165
Iteration 20/1000 | Loss: 0.00030557
Iteration 21/1000 | Loss: 0.00057697
Iteration 22/1000 | Loss: 0.00011879
Iteration 23/1000 | Loss: 0.00005806
Iteration 24/1000 | Loss: 0.00003984
Iteration 25/1000 | Loss: 0.00010025
Iteration 26/1000 | Loss: 0.00005032
Iteration 27/1000 | Loss: 0.00007724
Iteration 28/1000 | Loss: 0.00009931
Iteration 29/1000 | Loss: 0.00003775
Iteration 30/1000 | Loss: 0.00003594
Iteration 31/1000 | Loss: 0.00004172
Iteration 32/1000 | Loss: 0.00003108
Iteration 33/1000 | Loss: 0.00005259
Iteration 34/1000 | Loss: 0.00003080
Iteration 35/1000 | Loss: 0.00035376
Iteration 36/1000 | Loss: 0.00003492
Iteration 37/1000 | Loss: 0.00003015
Iteration 38/1000 | Loss: 0.00006940
Iteration 39/1000 | Loss: 0.00002799
Iteration 40/1000 | Loss: 0.00010097
Iteration 41/1000 | Loss: 0.00003319
Iteration 42/1000 | Loss: 0.00014383
Iteration 43/1000 | Loss: 0.00008495
Iteration 44/1000 | Loss: 0.00002667
Iteration 45/1000 | Loss: 0.00008793
Iteration 46/1000 | Loss: 0.00003177
Iteration 47/1000 | Loss: 0.00005105
Iteration 48/1000 | Loss: 0.00003300
Iteration 49/1000 | Loss: 0.00002616
Iteration 50/1000 | Loss: 0.00002616
Iteration 51/1000 | Loss: 0.00002616
Iteration 52/1000 | Loss: 0.00002615
Iteration 53/1000 | Loss: 0.00002615
Iteration 54/1000 | Loss: 0.00002614
Iteration 55/1000 | Loss: 0.00002614
Iteration 56/1000 | Loss: 0.00002614
Iteration 57/1000 | Loss: 0.00002614
Iteration 58/1000 | Loss: 0.00002613
Iteration 59/1000 | Loss: 0.00002612
Iteration 60/1000 | Loss: 0.00003329
Iteration 61/1000 | Loss: 0.00002596
Iteration 62/1000 | Loss: 0.00002595
Iteration 63/1000 | Loss: 0.00002595
Iteration 64/1000 | Loss: 0.00002595
Iteration 65/1000 | Loss: 0.00002595
Iteration 66/1000 | Loss: 0.00002595
Iteration 67/1000 | Loss: 0.00003551
Iteration 68/1000 | Loss: 0.00002582
Iteration 69/1000 | Loss: 0.00002582
Iteration 70/1000 | Loss: 0.00002581
Iteration 71/1000 | Loss: 0.00002580
Iteration 72/1000 | Loss: 0.00002580
Iteration 73/1000 | Loss: 0.00002580
Iteration 74/1000 | Loss: 0.00002579
Iteration 75/1000 | Loss: 0.00002579
Iteration 76/1000 | Loss: 0.00002579
Iteration 77/1000 | Loss: 0.00002578
Iteration 78/1000 | Loss: 0.00003253
Iteration 79/1000 | Loss: 0.00003577
Iteration 80/1000 | Loss: 0.00002572
Iteration 81/1000 | Loss: 0.00002569
Iteration 82/1000 | Loss: 0.00002569
Iteration 83/1000 | Loss: 0.00002569
Iteration 84/1000 | Loss: 0.00002569
Iteration 85/1000 | Loss: 0.00002569
Iteration 86/1000 | Loss: 0.00002569
Iteration 87/1000 | Loss: 0.00002569
Iteration 88/1000 | Loss: 0.00002569
Iteration 89/1000 | Loss: 0.00002569
Iteration 90/1000 | Loss: 0.00002569
Iteration 91/1000 | Loss: 0.00002569
Iteration 92/1000 | Loss: 0.00002569
Iteration 93/1000 | Loss: 0.00002568
Iteration 94/1000 | Loss: 0.00002568
Iteration 95/1000 | Loss: 0.00002568
Iteration 96/1000 | Loss: 0.00002568
Iteration 97/1000 | Loss: 0.00002568
Iteration 98/1000 | Loss: 0.00002568
Iteration 99/1000 | Loss: 0.00002568
Iteration 100/1000 | Loss: 0.00002567
Iteration 101/1000 | Loss: 0.00002567
Iteration 102/1000 | Loss: 0.00002567
Iteration 103/1000 | Loss: 0.00002567
Iteration 104/1000 | Loss: 0.00002567
Iteration 105/1000 | Loss: 0.00008129
Iteration 106/1000 | Loss: 0.00008818
Iteration 107/1000 | Loss: 0.00015123
Iteration 108/1000 | Loss: 0.00002677
Iteration 109/1000 | Loss: 0.00002820
Iteration 110/1000 | Loss: 0.00003493
Iteration 111/1000 | Loss: 0.00029043
Iteration 112/1000 | Loss: 0.00003804
Iteration 113/1000 | Loss: 0.00002755
Iteration 114/1000 | Loss: 0.00003190
Iteration 115/1000 | Loss: 0.00002804
Iteration 116/1000 | Loss: 0.00006979
Iteration 117/1000 | Loss: 0.00003034
Iteration 118/1000 | Loss: 0.00004395
Iteration 119/1000 | Loss: 0.00002555
Iteration 120/1000 | Loss: 0.00002554
Iteration 121/1000 | Loss: 0.00002554
Iteration 122/1000 | Loss: 0.00002553
Iteration 123/1000 | Loss: 0.00002553
Iteration 124/1000 | Loss: 0.00002553
Iteration 125/1000 | Loss: 0.00002553
Iteration 126/1000 | Loss: 0.00002553
Iteration 127/1000 | Loss: 0.00002553
Iteration 128/1000 | Loss: 0.00002553
Iteration 129/1000 | Loss: 0.00002553
Iteration 130/1000 | Loss: 0.00002553
Iteration 131/1000 | Loss: 0.00002553
Iteration 132/1000 | Loss: 0.00002553
Iteration 133/1000 | Loss: 0.00002553
Iteration 134/1000 | Loss: 0.00002552
Iteration 135/1000 | Loss: 0.00002552
Iteration 136/1000 | Loss: 0.00002552
Iteration 137/1000 | Loss: 0.00002552
Iteration 138/1000 | Loss: 0.00002552
Iteration 139/1000 | Loss: 0.00002551
Iteration 140/1000 | Loss: 0.00002551
Iteration 141/1000 | Loss: 0.00002551
Iteration 142/1000 | Loss: 0.00002551
Iteration 143/1000 | Loss: 0.00002551
Iteration 144/1000 | Loss: 0.00002551
Iteration 145/1000 | Loss: 0.00002551
Iteration 146/1000 | Loss: 0.00002551
Iteration 147/1000 | Loss: 0.00002551
Iteration 148/1000 | Loss: 0.00002551
Iteration 149/1000 | Loss: 0.00002551
Iteration 150/1000 | Loss: 0.00002551
Iteration 151/1000 | Loss: 0.00002551
Iteration 152/1000 | Loss: 0.00002551
Iteration 153/1000 | Loss: 0.00002551
Iteration 154/1000 | Loss: 0.00002551
Iteration 155/1000 | Loss: 0.00002551
Iteration 156/1000 | Loss: 0.00002551
Iteration 157/1000 | Loss: 0.00002551
Iteration 158/1000 | Loss: 0.00002551
Iteration 159/1000 | Loss: 0.00002551
Iteration 160/1000 | Loss: 0.00002551
Iteration 161/1000 | Loss: 0.00002551
Iteration 162/1000 | Loss: 0.00002551
Iteration 163/1000 | Loss: 0.00002551
Iteration 164/1000 | Loss: 0.00002551
Iteration 165/1000 | Loss: 0.00002551
Iteration 166/1000 | Loss: 0.00002551
Iteration 167/1000 | Loss: 0.00002551
Iteration 168/1000 | Loss: 0.00002551
Iteration 169/1000 | Loss: 0.00002551
Iteration 170/1000 | Loss: 0.00002551
Iteration 171/1000 | Loss: 0.00002551
Iteration 172/1000 | Loss: 0.00002551
Iteration 173/1000 | Loss: 0.00002551
Iteration 174/1000 | Loss: 0.00002551
Iteration 175/1000 | Loss: 0.00002551
Iteration 176/1000 | Loss: 0.00002551
Iteration 177/1000 | Loss: 0.00002551
Iteration 178/1000 | Loss: 0.00002551
Iteration 179/1000 | Loss: 0.00002551
Iteration 180/1000 | Loss: 0.00002551
Iteration 181/1000 | Loss: 0.00002551
Iteration 182/1000 | Loss: 0.00002551
Iteration 183/1000 | Loss: 0.00002551
Iteration 184/1000 | Loss: 0.00002551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.5506522433715872e-05, 2.5506522433715872e-05, 2.5506522433715872e-05, 2.5506522433715872e-05, 2.5506522433715872e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5506522433715872e-05

Optimization complete. Final v2v error: 4.2193522453308105 mm

Highest mean error: 5.952991962432861 mm for frame 99

Lowest mean error: 3.4085097312927246 mm for frame 180

Saving results

Total time: 120.49719548225403
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045870
Iteration 2/25 | Loss: 0.00336909
Iteration 3/25 | Loss: 0.00181601
Iteration 4/25 | Loss: 0.00159406
Iteration 5/25 | Loss: 0.00140707
Iteration 6/25 | Loss: 0.00135899
Iteration 7/25 | Loss: 0.00127206
Iteration 8/25 | Loss: 0.00121565
Iteration 9/25 | Loss: 0.00119535
Iteration 10/25 | Loss: 0.00115898
Iteration 11/25 | Loss: 0.00113027
Iteration 12/25 | Loss: 0.00110896
Iteration 13/25 | Loss: 0.00108001
Iteration 14/25 | Loss: 0.00106547
Iteration 15/25 | Loss: 0.00105985
Iteration 16/25 | Loss: 0.00105453
Iteration 17/25 | Loss: 0.00104445
Iteration 18/25 | Loss: 0.00104060
Iteration 19/25 | Loss: 0.00103743
Iteration 20/25 | Loss: 0.00103721
Iteration 21/25 | Loss: 0.00104521
Iteration 22/25 | Loss: 0.00104306
Iteration 23/25 | Loss: 0.00103593
Iteration 24/25 | Loss: 0.00103150
Iteration 25/25 | Loss: 0.00102521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44730890
Iteration 2/25 | Loss: 0.00736116
Iteration 3/25 | Loss: 0.00452559
Iteration 4/25 | Loss: 0.00422871
Iteration 5/25 | Loss: 0.00387316
Iteration 6/25 | Loss: 0.00386901
Iteration 7/25 | Loss: 0.00386901
Iteration 8/25 | Loss: 0.00386901
Iteration 9/25 | Loss: 0.00386901
Iteration 10/25 | Loss: 0.00386901
Iteration 11/25 | Loss: 0.00386901
Iteration 12/25 | Loss: 0.00386901
Iteration 13/25 | Loss: 0.00386901
Iteration 14/25 | Loss: 0.00386901
Iteration 15/25 | Loss: 0.00386901
Iteration 16/25 | Loss: 0.00386901
Iteration 17/25 | Loss: 0.00386901
Iteration 18/25 | Loss: 0.00386901
Iteration 19/25 | Loss: 0.00386901
Iteration 20/25 | Loss: 0.00386901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003869010601192713, 0.003869010601192713, 0.003869010601192713, 0.003869010601192713, 0.003869010601192713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003869010601192713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00386901
Iteration 2/1000 | Loss: 0.00542433
Iteration 3/1000 | Loss: 0.00160578
Iteration 4/1000 | Loss: 0.00104929
Iteration 5/1000 | Loss: 0.00099565
Iteration 6/1000 | Loss: 0.00211126
Iteration 7/1000 | Loss: 0.00107346
Iteration 8/1000 | Loss: 0.00169751
Iteration 9/1000 | Loss: 0.00066190
Iteration 10/1000 | Loss: 0.00510064
Iteration 11/1000 | Loss: 0.00061695
Iteration 12/1000 | Loss: 0.00346764
Iteration 13/1000 | Loss: 0.00078261
Iteration 14/1000 | Loss: 0.00089113
Iteration 15/1000 | Loss: 0.00251761
Iteration 16/1000 | Loss: 0.00071678
Iteration 17/1000 | Loss: 0.00076334
Iteration 18/1000 | Loss: 0.00101899
Iteration 19/1000 | Loss: 0.00082615
Iteration 20/1000 | Loss: 0.00086122
Iteration 21/1000 | Loss: 0.00128357
Iteration 22/1000 | Loss: 0.00227801
Iteration 23/1000 | Loss: 0.00108368
Iteration 24/1000 | Loss: 0.00172320
Iteration 25/1000 | Loss: 0.00186787
Iteration 26/1000 | Loss: 0.00047438
Iteration 27/1000 | Loss: 0.00142488
Iteration 28/1000 | Loss: 0.00083436
Iteration 29/1000 | Loss: 0.00321621
Iteration 30/1000 | Loss: 0.00205629
Iteration 31/1000 | Loss: 0.00092862
Iteration 32/1000 | Loss: 0.00146393
Iteration 33/1000 | Loss: 0.00185785
Iteration 34/1000 | Loss: 0.00100833
Iteration 35/1000 | Loss: 0.00074562
Iteration 36/1000 | Loss: 0.00087233
Iteration 37/1000 | Loss: 0.00343375
Iteration 38/1000 | Loss: 0.00101035
Iteration 39/1000 | Loss: 0.00125366
Iteration 40/1000 | Loss: 0.00144162
Iteration 41/1000 | Loss: 0.00080536
Iteration 42/1000 | Loss: 0.00053265
Iteration 43/1000 | Loss: 0.00061204
Iteration 44/1000 | Loss: 0.00132926
Iteration 45/1000 | Loss: 0.00051761
Iteration 46/1000 | Loss: 0.00069335
Iteration 47/1000 | Loss: 0.00042108
Iteration 48/1000 | Loss: 0.00039373
Iteration 49/1000 | Loss: 0.00049429
Iteration 50/1000 | Loss: 0.00039978
Iteration 51/1000 | Loss: 0.00033940
Iteration 52/1000 | Loss: 0.00022902
Iteration 53/1000 | Loss: 0.00115812
Iteration 54/1000 | Loss: 0.00037992
Iteration 55/1000 | Loss: 0.00066607
Iteration 56/1000 | Loss: 0.00031700
Iteration 57/1000 | Loss: 0.00021906
Iteration 58/1000 | Loss: 0.00034085
Iteration 59/1000 | Loss: 0.00025584
Iteration 60/1000 | Loss: 0.00074014
Iteration 61/1000 | Loss: 0.00040222
Iteration 62/1000 | Loss: 0.00033839
Iteration 63/1000 | Loss: 0.00124945
Iteration 64/1000 | Loss: 0.00169912
Iteration 65/1000 | Loss: 0.00032849
Iteration 66/1000 | Loss: 0.00083262
Iteration 67/1000 | Loss: 0.00011079
Iteration 68/1000 | Loss: 0.00010731
Iteration 69/1000 | Loss: 0.00007478
Iteration 70/1000 | Loss: 0.00076965
Iteration 71/1000 | Loss: 0.00025222
Iteration 72/1000 | Loss: 0.00013617
Iteration 73/1000 | Loss: 0.00009710
Iteration 74/1000 | Loss: 0.00009795
Iteration 75/1000 | Loss: 0.00029347
Iteration 76/1000 | Loss: 0.00009470
Iteration 77/1000 | Loss: 0.00020385
Iteration 78/1000 | Loss: 0.00008591
Iteration 79/1000 | Loss: 0.00061168
Iteration 80/1000 | Loss: 0.00011808
Iteration 81/1000 | Loss: 0.00015401
Iteration 82/1000 | Loss: 0.00039313
Iteration 83/1000 | Loss: 0.00023750
Iteration 84/1000 | Loss: 0.00034544
Iteration 85/1000 | Loss: 0.00085962
Iteration 86/1000 | Loss: 0.00011850
Iteration 87/1000 | Loss: 0.00037763
Iteration 88/1000 | Loss: 0.00003609
Iteration 89/1000 | Loss: 0.00007842
Iteration 90/1000 | Loss: 0.00006598
Iteration 91/1000 | Loss: 0.00004641
Iteration 92/1000 | Loss: 0.00014007
Iteration 93/1000 | Loss: 0.00003788
Iteration 94/1000 | Loss: 0.00007031
Iteration 95/1000 | Loss: 0.00003950
Iteration 96/1000 | Loss: 0.00004911
Iteration 97/1000 | Loss: 0.00004665
Iteration 98/1000 | Loss: 0.00006456
Iteration 99/1000 | Loss: 0.00008877
Iteration 100/1000 | Loss: 0.00004520
Iteration 101/1000 | Loss: 0.00004283
Iteration 102/1000 | Loss: 0.00004753
Iteration 103/1000 | Loss: 0.00004218
Iteration 104/1000 | Loss: 0.00003912
Iteration 105/1000 | Loss: 0.00003843
Iteration 106/1000 | Loss: 0.00007195
Iteration 107/1000 | Loss: 0.00007018
Iteration 108/1000 | Loss: 0.00019846
Iteration 109/1000 | Loss: 0.00012637
Iteration 110/1000 | Loss: 0.00005772
Iteration 111/1000 | Loss: 0.00003876
Iteration 112/1000 | Loss: 0.00003299
Iteration 113/1000 | Loss: 0.00003471
Iteration 114/1000 | Loss: 0.00003788
Iteration 115/1000 | Loss: 0.00010153
Iteration 116/1000 | Loss: 0.00004880
Iteration 117/1000 | Loss: 0.00005885
Iteration 118/1000 | Loss: 0.00005812
Iteration 119/1000 | Loss: 0.00006914
Iteration 120/1000 | Loss: 0.00015893
Iteration 121/1000 | Loss: 0.00006660
Iteration 122/1000 | Loss: 0.00010769
Iteration 123/1000 | Loss: 0.00004067
Iteration 124/1000 | Loss: 0.00007154
Iteration 125/1000 | Loss: 0.00003843
Iteration 126/1000 | Loss: 0.00004324
Iteration 127/1000 | Loss: 0.00006727
Iteration 128/1000 | Loss: 0.00004216
Iteration 129/1000 | Loss: 0.00005506
Iteration 130/1000 | Loss: 0.00006213
Iteration 131/1000 | Loss: 0.00003987
Iteration 132/1000 | Loss: 0.00003999
Iteration 133/1000 | Loss: 0.00006743
Iteration 134/1000 | Loss: 0.00004156
Iteration 135/1000 | Loss: 0.00007234
Iteration 136/1000 | Loss: 0.00004492
Iteration 137/1000 | Loss: 0.00011436
Iteration 138/1000 | Loss: 0.00004783
Iteration 139/1000 | Loss: 0.00005310
Iteration 140/1000 | Loss: 0.00010304
Iteration 141/1000 | Loss: 0.00004077
Iteration 142/1000 | Loss: 0.00004875
Iteration 143/1000 | Loss: 0.00003790
Iteration 144/1000 | Loss: 0.00003991
Iteration 145/1000 | Loss: 0.00003787
Iteration 146/1000 | Loss: 0.00003828
Iteration 147/1000 | Loss: 0.00003754
Iteration 148/1000 | Loss: 0.00003795
Iteration 149/1000 | Loss: 0.00004601
Iteration 150/1000 | Loss: 0.00003214
Iteration 151/1000 | Loss: 0.00007777
Iteration 152/1000 | Loss: 0.00004093
Iteration 153/1000 | Loss: 0.00003745
Iteration 154/1000 | Loss: 0.00004014
Iteration 155/1000 | Loss: 0.00004119
Iteration 156/1000 | Loss: 0.00004012
Iteration 157/1000 | Loss: 0.00006200
Iteration 158/1000 | Loss: 0.00004265
Iteration 159/1000 | Loss: 0.00003807
Iteration 160/1000 | Loss: 0.00003732
Iteration 161/1000 | Loss: 0.00003732
Iteration 162/1000 | Loss: 0.00004691
Iteration 163/1000 | Loss: 0.00003748
Iteration 164/1000 | Loss: 0.00003920
Iteration 165/1000 | Loss: 0.00003639
Iteration 166/1000 | Loss: 0.00005367
Iteration 167/1000 | Loss: 0.00011900
Iteration 168/1000 | Loss: 0.00004116
Iteration 169/1000 | Loss: 0.00004877
Iteration 170/1000 | Loss: 0.00004051
Iteration 171/1000 | Loss: 0.00003961
Iteration 172/1000 | Loss: 0.00005304
Iteration 173/1000 | Loss: 0.00003071
Iteration 174/1000 | Loss: 0.00004238
Iteration 175/1000 | Loss: 0.00004910
Iteration 176/1000 | Loss: 0.00003584
Iteration 177/1000 | Loss: 0.00003773
Iteration 178/1000 | Loss: 0.00003930
Iteration 179/1000 | Loss: 0.00003677
Iteration 180/1000 | Loss: 0.00003810
Iteration 181/1000 | Loss: 0.00003768
Iteration 182/1000 | Loss: 0.00003792
Iteration 183/1000 | Loss: 0.00009025
Iteration 184/1000 | Loss: 0.00004220
Iteration 185/1000 | Loss: 0.00006438
Iteration 186/1000 | Loss: 0.00004534
Iteration 187/1000 | Loss: 0.00005431
Iteration 188/1000 | Loss: 0.00007530
Iteration 189/1000 | Loss: 0.00004999
Iteration 190/1000 | Loss: 0.00008694
Iteration 191/1000 | Loss: 0.00004317
Iteration 192/1000 | Loss: 0.00004044
Iteration 193/1000 | Loss: 0.00003878
Iteration 194/1000 | Loss: 0.00003876
Iteration 195/1000 | Loss: 0.00003838
Iteration 196/1000 | Loss: 0.00005204
Iteration 197/1000 | Loss: 0.00003830
Iteration 198/1000 | Loss: 0.00004368
Iteration 199/1000 | Loss: 0.00003993
Iteration 200/1000 | Loss: 0.00004245
Iteration 201/1000 | Loss: 0.00006975
Iteration 202/1000 | Loss: 0.00050532
Iteration 203/1000 | Loss: 0.00003216
Iteration 204/1000 | Loss: 0.00003133
Iteration 205/1000 | Loss: 0.00002640
Iteration 206/1000 | Loss: 0.00010177
Iteration 207/1000 | Loss: 0.00002446
Iteration 208/1000 | Loss: 0.00006082
Iteration 209/1000 | Loss: 0.00002347
Iteration 210/1000 | Loss: 0.00005941
Iteration 211/1000 | Loss: 0.00002301
Iteration 212/1000 | Loss: 0.00006871
Iteration 213/1000 | Loss: 0.00002293
Iteration 214/1000 | Loss: 0.00003790
Iteration 215/1000 | Loss: 0.00002470
Iteration 216/1000 | Loss: 0.00002249
Iteration 217/1000 | Loss: 0.00002249
Iteration 218/1000 | Loss: 0.00002249
Iteration 219/1000 | Loss: 0.00002248
Iteration 220/1000 | Loss: 0.00002248
Iteration 221/1000 | Loss: 0.00002247
Iteration 222/1000 | Loss: 0.00002831
Iteration 223/1000 | Loss: 0.00002245
Iteration 224/1000 | Loss: 0.00002230
Iteration 225/1000 | Loss: 0.00002229
Iteration 226/1000 | Loss: 0.00002229
Iteration 227/1000 | Loss: 0.00002228
Iteration 228/1000 | Loss: 0.00002228
Iteration 229/1000 | Loss: 0.00002228
Iteration 230/1000 | Loss: 0.00002227
Iteration 231/1000 | Loss: 0.00002227
Iteration 232/1000 | Loss: 0.00002227
Iteration 233/1000 | Loss: 0.00002227
Iteration 234/1000 | Loss: 0.00002227
Iteration 235/1000 | Loss: 0.00002227
Iteration 236/1000 | Loss: 0.00002226
Iteration 237/1000 | Loss: 0.00002226
Iteration 238/1000 | Loss: 0.00002226
Iteration 239/1000 | Loss: 0.00002225
Iteration 240/1000 | Loss: 0.00002225
Iteration 241/1000 | Loss: 0.00002225
Iteration 242/1000 | Loss: 0.00002224
Iteration 243/1000 | Loss: 0.00008259
Iteration 244/1000 | Loss: 0.00003481
Iteration 245/1000 | Loss: 0.00002222
Iteration 246/1000 | Loss: 0.00002215
Iteration 247/1000 | Loss: 0.00002214
Iteration 248/1000 | Loss: 0.00002214
Iteration 249/1000 | Loss: 0.00002214
Iteration 250/1000 | Loss: 0.00002214
Iteration 251/1000 | Loss: 0.00002214
Iteration 252/1000 | Loss: 0.00002214
Iteration 253/1000 | Loss: 0.00002214
Iteration 254/1000 | Loss: 0.00002214
Iteration 255/1000 | Loss: 0.00002214
Iteration 256/1000 | Loss: 0.00002214
Iteration 257/1000 | Loss: 0.00002214
Iteration 258/1000 | Loss: 0.00002214
Iteration 259/1000 | Loss: 0.00002214
Iteration 260/1000 | Loss: 0.00002214
Iteration 261/1000 | Loss: 0.00002213
Iteration 262/1000 | Loss: 0.00002213
Iteration 263/1000 | Loss: 0.00002213
Iteration 264/1000 | Loss: 0.00002213
Iteration 265/1000 | Loss: 0.00002213
Iteration 266/1000 | Loss: 0.00002212
Iteration 267/1000 | Loss: 0.00002212
Iteration 268/1000 | Loss: 0.00002212
Iteration 269/1000 | Loss: 0.00002212
Iteration 270/1000 | Loss: 0.00002211
Iteration 271/1000 | Loss: 0.00002211
Iteration 272/1000 | Loss: 0.00002211
Iteration 273/1000 | Loss: 0.00002211
Iteration 274/1000 | Loss: 0.00002211
Iteration 275/1000 | Loss: 0.00002211
Iteration 276/1000 | Loss: 0.00002211
Iteration 277/1000 | Loss: 0.00002211
Iteration 278/1000 | Loss: 0.00002211
Iteration 279/1000 | Loss: 0.00002211
Iteration 280/1000 | Loss: 0.00002211
Iteration 281/1000 | Loss: 0.00002211
Iteration 282/1000 | Loss: 0.00002211
Iteration 283/1000 | Loss: 0.00002211
Iteration 284/1000 | Loss: 0.00002211
Iteration 285/1000 | Loss: 0.00002211
Iteration 286/1000 | Loss: 0.00002211
Iteration 287/1000 | Loss: 0.00002210
Iteration 288/1000 | Loss: 0.00002210
Iteration 289/1000 | Loss: 0.00002210
Iteration 290/1000 | Loss: 0.00002210
Iteration 291/1000 | Loss: 0.00002210
Iteration 292/1000 | Loss: 0.00002210
Iteration 293/1000 | Loss: 0.00002210
Iteration 294/1000 | Loss: 0.00002210
Iteration 295/1000 | Loss: 0.00002209
Iteration 296/1000 | Loss: 0.00002209
Iteration 297/1000 | Loss: 0.00002209
Iteration 298/1000 | Loss: 0.00002208
Iteration 299/1000 | Loss: 0.00002208
Iteration 300/1000 | Loss: 0.00008818
Iteration 301/1000 | Loss: 0.00018625
Iteration 302/1000 | Loss: 0.00004048
Iteration 303/1000 | Loss: 0.00002826
Iteration 304/1000 | Loss: 0.00002211
Iteration 305/1000 | Loss: 0.00002610
Iteration 306/1000 | Loss: 0.00002342
Iteration 307/1000 | Loss: 0.00002454
Iteration 308/1000 | Loss: 0.00005520
Iteration 309/1000 | Loss: 0.00013570
Iteration 310/1000 | Loss: 0.00014852
Iteration 311/1000 | Loss: 0.00004767
Iteration 312/1000 | Loss: 0.00004019
Iteration 313/1000 | Loss: 0.00002642
Iteration 314/1000 | Loss: 0.00012150
Iteration 315/1000 | Loss: 0.00002248
Iteration 316/1000 | Loss: 0.00002161
Iteration 317/1000 | Loss: 0.00003676
Iteration 318/1000 | Loss: 0.00005726
Iteration 319/1000 | Loss: 0.00002083
Iteration 320/1000 | Loss: 0.00002056
Iteration 321/1000 | Loss: 0.00002055
Iteration 322/1000 | Loss: 0.00007135
Iteration 323/1000 | Loss: 0.00002250
Iteration 324/1000 | Loss: 0.00003277
Iteration 325/1000 | Loss: 0.00002031
Iteration 326/1000 | Loss: 0.00002031
Iteration 327/1000 | Loss: 0.00002031
Iteration 328/1000 | Loss: 0.00002030
Iteration 329/1000 | Loss: 0.00002030
Iteration 330/1000 | Loss: 0.00002030
Iteration 331/1000 | Loss: 0.00002029
Iteration 332/1000 | Loss: 0.00002029
Iteration 333/1000 | Loss: 0.00003210
Iteration 334/1000 | Loss: 0.00002329
Iteration 335/1000 | Loss: 0.00002023
Iteration 336/1000 | Loss: 0.00002023
Iteration 337/1000 | Loss: 0.00002023
Iteration 338/1000 | Loss: 0.00002023
Iteration 339/1000 | Loss: 0.00002023
Iteration 340/1000 | Loss: 0.00002023
Iteration 341/1000 | Loss: 0.00002023
Iteration 342/1000 | Loss: 0.00002023
Iteration 343/1000 | Loss: 0.00002023
Iteration 344/1000 | Loss: 0.00002023
Iteration 345/1000 | Loss: 0.00002023
Iteration 346/1000 | Loss: 0.00002023
Iteration 347/1000 | Loss: 0.00002023
Iteration 348/1000 | Loss: 0.00002022
Iteration 349/1000 | Loss: 0.00002022
Iteration 350/1000 | Loss: 0.00002022
Iteration 351/1000 | Loss: 0.00002020
Iteration 352/1000 | Loss: 0.00002020
Iteration 353/1000 | Loss: 0.00002020
Iteration 354/1000 | Loss: 0.00002020
Iteration 355/1000 | Loss: 0.00002020
Iteration 356/1000 | Loss: 0.00002020
Iteration 357/1000 | Loss: 0.00002020
Iteration 358/1000 | Loss: 0.00002020
Iteration 359/1000 | Loss: 0.00002020
Iteration 360/1000 | Loss: 0.00002019
Iteration 361/1000 | Loss: 0.00002019
Iteration 362/1000 | Loss: 0.00005730
Iteration 363/1000 | Loss: 0.00002019
Iteration 364/1000 | Loss: 0.00002018
Iteration 365/1000 | Loss: 0.00002018
Iteration 366/1000 | Loss: 0.00002017
Iteration 367/1000 | Loss: 0.00002017
Iteration 368/1000 | Loss: 0.00002017
Iteration 369/1000 | Loss: 0.00002017
Iteration 370/1000 | Loss: 0.00002017
Iteration 371/1000 | Loss: 0.00002017
Iteration 372/1000 | Loss: 0.00002017
Iteration 373/1000 | Loss: 0.00002017
Iteration 374/1000 | Loss: 0.00002017
Iteration 375/1000 | Loss: 0.00002016
Iteration 376/1000 | Loss: 0.00002016
Iteration 377/1000 | Loss: 0.00002016
Iteration 378/1000 | Loss: 0.00002016
Iteration 379/1000 | Loss: 0.00002016
Iteration 380/1000 | Loss: 0.00002016
Iteration 381/1000 | Loss: 0.00002015
Iteration 382/1000 | Loss: 0.00002015
Iteration 383/1000 | Loss: 0.00002015
Iteration 384/1000 | Loss: 0.00002015
Iteration 385/1000 | Loss: 0.00002015
Iteration 386/1000 | Loss: 0.00002014
Iteration 387/1000 | Loss: 0.00002014
Iteration 388/1000 | Loss: 0.00002014
Iteration 389/1000 | Loss: 0.00002014
Iteration 390/1000 | Loss: 0.00002014
Iteration 391/1000 | Loss: 0.00002013
Iteration 392/1000 | Loss: 0.00002013
Iteration 393/1000 | Loss: 0.00002013
Iteration 394/1000 | Loss: 0.00002013
Iteration 395/1000 | Loss: 0.00002013
Iteration 396/1000 | Loss: 0.00002013
Iteration 397/1000 | Loss: 0.00002013
Iteration 398/1000 | Loss: 0.00002013
Iteration 399/1000 | Loss: 0.00002013
Iteration 400/1000 | Loss: 0.00002012
Iteration 401/1000 | Loss: 0.00002012
Iteration 402/1000 | Loss: 0.00002012
Iteration 403/1000 | Loss: 0.00002012
Iteration 404/1000 | Loss: 0.00002012
Iteration 405/1000 | Loss: 0.00002012
Iteration 406/1000 | Loss: 0.00002012
Iteration 407/1000 | Loss: 0.00002012
Iteration 408/1000 | Loss: 0.00002012
Iteration 409/1000 | Loss: 0.00002012
Iteration 410/1000 | Loss: 0.00002012
Iteration 411/1000 | Loss: 0.00002012
Iteration 412/1000 | Loss: 0.00002012
Iteration 413/1000 | Loss: 0.00002012
Iteration 414/1000 | Loss: 0.00002012
Iteration 415/1000 | Loss: 0.00002012
Iteration 416/1000 | Loss: 0.00002012
Iteration 417/1000 | Loss: 0.00002012
Iteration 418/1000 | Loss: 0.00002012
Iteration 419/1000 | Loss: 0.00002012
Iteration 420/1000 | Loss: 0.00002012
Iteration 421/1000 | Loss: 0.00002012
Iteration 422/1000 | Loss: 0.00002012
Iteration 423/1000 | Loss: 0.00002011
Iteration 424/1000 | Loss: 0.00002011
Iteration 425/1000 | Loss: 0.00002011
Iteration 426/1000 | Loss: 0.00002011
Iteration 427/1000 | Loss: 0.00004495
Iteration 428/1000 | Loss: 0.00002013
Iteration 429/1000 | Loss: 0.00002013
Iteration 430/1000 | Loss: 0.00002013
Iteration 431/1000 | Loss: 0.00002013
Iteration 432/1000 | Loss: 0.00002013
Iteration 433/1000 | Loss: 0.00002013
Iteration 434/1000 | Loss: 0.00002013
Iteration 435/1000 | Loss: 0.00002013
Iteration 436/1000 | Loss: 0.00002013
Iteration 437/1000 | Loss: 0.00002013
Iteration 438/1000 | Loss: 0.00002013
Iteration 439/1000 | Loss: 0.00002012
Iteration 440/1000 | Loss: 0.00002012
Iteration 441/1000 | Loss: 0.00002012
Iteration 442/1000 | Loss: 0.00002012
Iteration 443/1000 | Loss: 0.00002012
Iteration 444/1000 | Loss: 0.00002012
Iteration 445/1000 | Loss: 0.00002012
Iteration 446/1000 | Loss: 0.00002012
Iteration 447/1000 | Loss: 0.00002011
Iteration 448/1000 | Loss: 0.00002011
Iteration 449/1000 | Loss: 0.00002011
Iteration 450/1000 | Loss: 0.00002011
Iteration 451/1000 | Loss: 0.00002010
Iteration 452/1000 | Loss: 0.00002010
Iteration 453/1000 | Loss: 0.00002010
Iteration 454/1000 | Loss: 0.00002010
Iteration 455/1000 | Loss: 0.00002010
Iteration 456/1000 | Loss: 0.00002010
Iteration 457/1000 | Loss: 0.00002010
Iteration 458/1000 | Loss: 0.00002010
Iteration 459/1000 | Loss: 0.00002010
Iteration 460/1000 | Loss: 0.00002010
Iteration 461/1000 | Loss: 0.00002010
Iteration 462/1000 | Loss: 0.00002010
Iteration 463/1000 | Loss: 0.00002010
Iteration 464/1000 | Loss: 0.00002010
Iteration 465/1000 | Loss: 0.00002010
Iteration 466/1000 | Loss: 0.00002010
Iteration 467/1000 | Loss: 0.00002010
Iteration 468/1000 | Loss: 0.00002010
Iteration 469/1000 | Loss: 0.00002010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 469. Stopping optimization.
Last 5 losses: [2.0102736016269773e-05, 2.0102736016269773e-05, 2.0102736016269773e-05, 2.0102736016269773e-05, 2.0102736016269773e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0102736016269773e-05

Optimization complete. Final v2v error: 3.3618085384368896 mm

Highest mean error: 11.926499366760254 mm for frame 144

Lowest mean error: 2.6382360458374023 mm for frame 11

Saving results

Total time: 458.52583837509155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854704
Iteration 2/25 | Loss: 0.00097709
Iteration 3/25 | Loss: 0.00072159
Iteration 4/25 | Loss: 0.00067938
Iteration 5/25 | Loss: 0.00066786
Iteration 6/25 | Loss: 0.00066638
Iteration 7/25 | Loss: 0.00066638
Iteration 8/25 | Loss: 0.00066638
Iteration 9/25 | Loss: 0.00066638
Iteration 10/25 | Loss: 0.00066638
Iteration 11/25 | Loss: 0.00066638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006663805106654763, 0.0006663805106654763, 0.0006663805106654763, 0.0006663805106654763, 0.0006663805106654763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006663805106654763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43816936
Iteration 2/25 | Loss: 0.00023558
Iteration 3/25 | Loss: 0.00023554
Iteration 4/25 | Loss: 0.00023554
Iteration 5/25 | Loss: 0.00023554
Iteration 6/25 | Loss: 0.00023554
Iteration 7/25 | Loss: 0.00023554
Iteration 8/25 | Loss: 0.00023554
Iteration 9/25 | Loss: 0.00023554
Iteration 10/25 | Loss: 0.00023554
Iteration 11/25 | Loss: 0.00023554
Iteration 12/25 | Loss: 0.00023554
Iteration 13/25 | Loss: 0.00023554
Iteration 14/25 | Loss: 0.00023554
Iteration 15/25 | Loss: 0.00023554
Iteration 16/25 | Loss: 0.00023554
Iteration 17/25 | Loss: 0.00023554
Iteration 18/25 | Loss: 0.00023554
Iteration 19/25 | Loss: 0.00023554
Iteration 20/25 | Loss: 0.00023554
Iteration 21/25 | Loss: 0.00023554
Iteration 22/25 | Loss: 0.00023554
Iteration 23/25 | Loss: 0.00023554
Iteration 24/25 | Loss: 0.00023554
Iteration 25/25 | Loss: 0.00023554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023554
Iteration 2/1000 | Loss: 0.00003491
Iteration 3/1000 | Loss: 0.00002498
Iteration 4/1000 | Loss: 0.00002283
Iteration 5/1000 | Loss: 0.00002183
Iteration 6/1000 | Loss: 0.00002067
Iteration 7/1000 | Loss: 0.00001999
Iteration 8/1000 | Loss: 0.00001946
Iteration 9/1000 | Loss: 0.00001913
Iteration 10/1000 | Loss: 0.00001913
Iteration 11/1000 | Loss: 0.00001912
Iteration 12/1000 | Loss: 0.00001912
Iteration 13/1000 | Loss: 0.00001898
Iteration 14/1000 | Loss: 0.00001890
Iteration 15/1000 | Loss: 0.00001890
Iteration 16/1000 | Loss: 0.00001889
Iteration 17/1000 | Loss: 0.00001880
Iteration 18/1000 | Loss: 0.00001876
Iteration 19/1000 | Loss: 0.00001873
Iteration 20/1000 | Loss: 0.00001866
Iteration 21/1000 | Loss: 0.00001858
Iteration 22/1000 | Loss: 0.00001857
Iteration 23/1000 | Loss: 0.00001856
Iteration 24/1000 | Loss: 0.00001854
Iteration 25/1000 | Loss: 0.00001852
Iteration 26/1000 | Loss: 0.00001852
Iteration 27/1000 | Loss: 0.00001851
Iteration 28/1000 | Loss: 0.00001850
Iteration 29/1000 | Loss: 0.00001849
Iteration 30/1000 | Loss: 0.00001848
Iteration 31/1000 | Loss: 0.00001847
Iteration 32/1000 | Loss: 0.00001845
Iteration 33/1000 | Loss: 0.00001844
Iteration 34/1000 | Loss: 0.00001844
Iteration 35/1000 | Loss: 0.00001842
Iteration 36/1000 | Loss: 0.00001842
Iteration 37/1000 | Loss: 0.00001842
Iteration 38/1000 | Loss: 0.00001841
Iteration 39/1000 | Loss: 0.00001841
Iteration 40/1000 | Loss: 0.00001841
Iteration 41/1000 | Loss: 0.00001840
Iteration 42/1000 | Loss: 0.00001840
Iteration 43/1000 | Loss: 0.00001839
Iteration 44/1000 | Loss: 0.00001839
Iteration 45/1000 | Loss: 0.00001839
Iteration 46/1000 | Loss: 0.00001839
Iteration 47/1000 | Loss: 0.00001838
Iteration 48/1000 | Loss: 0.00001838
Iteration 49/1000 | Loss: 0.00001838
Iteration 50/1000 | Loss: 0.00001837
Iteration 51/1000 | Loss: 0.00001837
Iteration 52/1000 | Loss: 0.00001837
Iteration 53/1000 | Loss: 0.00001837
Iteration 54/1000 | Loss: 0.00001837
Iteration 55/1000 | Loss: 0.00001837
Iteration 56/1000 | Loss: 0.00001837
Iteration 57/1000 | Loss: 0.00001837
Iteration 58/1000 | Loss: 0.00001837
Iteration 59/1000 | Loss: 0.00001836
Iteration 60/1000 | Loss: 0.00001836
Iteration 61/1000 | Loss: 0.00001836
Iteration 62/1000 | Loss: 0.00001836
Iteration 63/1000 | Loss: 0.00001835
Iteration 64/1000 | Loss: 0.00001835
Iteration 65/1000 | Loss: 0.00001835
Iteration 66/1000 | Loss: 0.00001835
Iteration 67/1000 | Loss: 0.00001834
Iteration 68/1000 | Loss: 0.00001834
Iteration 69/1000 | Loss: 0.00001834
Iteration 70/1000 | Loss: 0.00001834
Iteration 71/1000 | Loss: 0.00001834
Iteration 72/1000 | Loss: 0.00001834
Iteration 73/1000 | Loss: 0.00001834
Iteration 74/1000 | Loss: 0.00001834
Iteration 75/1000 | Loss: 0.00001834
Iteration 76/1000 | Loss: 0.00001834
Iteration 77/1000 | Loss: 0.00001834
Iteration 78/1000 | Loss: 0.00001834
Iteration 79/1000 | Loss: 0.00001834
Iteration 80/1000 | Loss: 0.00001834
Iteration 81/1000 | Loss: 0.00001834
Iteration 82/1000 | Loss: 0.00001834
Iteration 83/1000 | Loss: 0.00001834
Iteration 84/1000 | Loss: 0.00001834
Iteration 85/1000 | Loss: 0.00001834
Iteration 86/1000 | Loss: 0.00001834
Iteration 87/1000 | Loss: 0.00001834
Iteration 88/1000 | Loss: 0.00001834
Iteration 89/1000 | Loss: 0.00001834
Iteration 90/1000 | Loss: 0.00001834
Iteration 91/1000 | Loss: 0.00001834
Iteration 92/1000 | Loss: 0.00001834
Iteration 93/1000 | Loss: 0.00001834
Iteration 94/1000 | Loss: 0.00001834
Iteration 95/1000 | Loss: 0.00001834
Iteration 96/1000 | Loss: 0.00001834
Iteration 97/1000 | Loss: 0.00001834
Iteration 98/1000 | Loss: 0.00001834
Iteration 99/1000 | Loss: 0.00001834
Iteration 100/1000 | Loss: 0.00001834
Iteration 101/1000 | Loss: 0.00001834
Iteration 102/1000 | Loss: 0.00001834
Iteration 103/1000 | Loss: 0.00001834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.834117210819386e-05, 1.834117210819386e-05, 1.834117210819386e-05, 1.834117210819386e-05, 1.834117210819386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.834117210819386e-05

Optimization complete. Final v2v error: 3.6693339347839355 mm

Highest mean error: 4.530691146850586 mm for frame 36

Lowest mean error: 3.3716652393341064 mm for frame 7

Saving results

Total time: 33.16551470756531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026355
Iteration 2/25 | Loss: 0.00268898
Iteration 3/25 | Loss: 0.00177562
Iteration 4/25 | Loss: 0.00150057
Iteration 5/25 | Loss: 0.00130187
Iteration 6/25 | Loss: 0.00123889
Iteration 7/25 | Loss: 0.00117844
Iteration 8/25 | Loss: 0.00114908
Iteration 9/25 | Loss: 0.00113617
Iteration 10/25 | Loss: 0.00103440
Iteration 11/25 | Loss: 0.00099785
Iteration 12/25 | Loss: 0.00095828
Iteration 13/25 | Loss: 0.00095051
Iteration 14/25 | Loss: 0.00089196
Iteration 15/25 | Loss: 0.00084504
Iteration 16/25 | Loss: 0.00082556
Iteration 17/25 | Loss: 0.00081919
Iteration 18/25 | Loss: 0.00081753
Iteration 19/25 | Loss: 0.00082332
Iteration 20/25 | Loss: 0.00081752
Iteration 21/25 | Loss: 0.00081524
Iteration 22/25 | Loss: 0.00081443
Iteration 23/25 | Loss: 0.00081425
Iteration 24/25 | Loss: 0.00081424
Iteration 25/25 | Loss: 0.00081424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46153188
Iteration 2/25 | Loss: 0.00089501
Iteration 3/25 | Loss: 0.00072411
Iteration 4/25 | Loss: 0.00072411
Iteration 5/25 | Loss: 0.00072411
Iteration 6/25 | Loss: 0.00072411
Iteration 7/25 | Loss: 0.00072411
Iteration 8/25 | Loss: 0.00072411
Iteration 9/25 | Loss: 0.00072411
Iteration 10/25 | Loss: 0.00072411
Iteration 11/25 | Loss: 0.00072411
Iteration 12/25 | Loss: 0.00072411
Iteration 13/25 | Loss: 0.00072411
Iteration 14/25 | Loss: 0.00072411
Iteration 15/25 | Loss: 0.00072411
Iteration 16/25 | Loss: 0.00072411
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000724107027053833, 0.000724107027053833, 0.000724107027053833, 0.000724107027053833, 0.000724107027053833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000724107027053833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072411
Iteration 2/1000 | Loss: 0.00027901
Iteration 3/1000 | Loss: 0.00024396
Iteration 4/1000 | Loss: 0.00166259
Iteration 5/1000 | Loss: 0.00030412
Iteration 6/1000 | Loss: 0.00025971
Iteration 7/1000 | Loss: 0.00004870
Iteration 8/1000 | Loss: 0.00006500
Iteration 9/1000 | Loss: 0.00004150
Iteration 10/1000 | Loss: 0.00015302
Iteration 11/1000 | Loss: 0.00003931
Iteration 12/1000 | Loss: 0.00003823
Iteration 13/1000 | Loss: 0.00003718
Iteration 14/1000 | Loss: 0.00003640
Iteration 15/1000 | Loss: 0.00003578
Iteration 16/1000 | Loss: 0.00003544
Iteration 17/1000 | Loss: 0.00003515
Iteration 18/1000 | Loss: 0.00003491
Iteration 19/1000 | Loss: 0.00003477
Iteration 20/1000 | Loss: 0.00003468
Iteration 21/1000 | Loss: 0.00003457
Iteration 22/1000 | Loss: 0.00003456
Iteration 23/1000 | Loss: 0.00003455
Iteration 24/1000 | Loss: 0.00003455
Iteration 25/1000 | Loss: 0.00003455
Iteration 26/1000 | Loss: 0.00003454
Iteration 27/1000 | Loss: 0.00003454
Iteration 28/1000 | Loss: 0.00003454
Iteration 29/1000 | Loss: 0.00003453
Iteration 30/1000 | Loss: 0.00003453
Iteration 31/1000 | Loss: 0.00003452
Iteration 32/1000 | Loss: 0.00003452
Iteration 33/1000 | Loss: 0.00003451
Iteration 34/1000 | Loss: 0.00003451
Iteration 35/1000 | Loss: 0.00003450
Iteration 36/1000 | Loss: 0.00003450
Iteration 37/1000 | Loss: 0.00003449
Iteration 38/1000 | Loss: 0.00003449
Iteration 39/1000 | Loss: 0.00003448
Iteration 40/1000 | Loss: 0.00003448
Iteration 41/1000 | Loss: 0.00003448
Iteration 42/1000 | Loss: 0.00003447
Iteration 43/1000 | Loss: 0.00003447
Iteration 44/1000 | Loss: 0.00003447
Iteration 45/1000 | Loss: 0.00003447
Iteration 46/1000 | Loss: 0.00003447
Iteration 47/1000 | Loss: 0.00003447
Iteration 48/1000 | Loss: 0.00003447
Iteration 49/1000 | Loss: 0.00003446
Iteration 50/1000 | Loss: 0.00003446
Iteration 51/1000 | Loss: 0.00003446
Iteration 52/1000 | Loss: 0.00003446
Iteration 53/1000 | Loss: 0.00003446
Iteration 54/1000 | Loss: 0.00003446
Iteration 55/1000 | Loss: 0.00003446
Iteration 56/1000 | Loss: 0.00003446
Iteration 57/1000 | Loss: 0.00003446
Iteration 58/1000 | Loss: 0.00003446
Iteration 59/1000 | Loss: 0.00003446
Iteration 60/1000 | Loss: 0.00003445
Iteration 61/1000 | Loss: 0.00003445
Iteration 62/1000 | Loss: 0.00003445
Iteration 63/1000 | Loss: 0.00003445
Iteration 64/1000 | Loss: 0.00003445
Iteration 65/1000 | Loss: 0.00003444
Iteration 66/1000 | Loss: 0.00003444
Iteration 67/1000 | Loss: 0.00003444
Iteration 68/1000 | Loss: 0.00003444
Iteration 69/1000 | Loss: 0.00003444
Iteration 70/1000 | Loss: 0.00003444
Iteration 71/1000 | Loss: 0.00003444
Iteration 72/1000 | Loss: 0.00003443
Iteration 73/1000 | Loss: 0.00003443
Iteration 74/1000 | Loss: 0.00003443
Iteration 75/1000 | Loss: 0.00003443
Iteration 76/1000 | Loss: 0.00003443
Iteration 77/1000 | Loss: 0.00003443
Iteration 78/1000 | Loss: 0.00003443
Iteration 79/1000 | Loss: 0.00003443
Iteration 80/1000 | Loss: 0.00003443
Iteration 81/1000 | Loss: 0.00003443
Iteration 82/1000 | Loss: 0.00003443
Iteration 83/1000 | Loss: 0.00003443
Iteration 84/1000 | Loss: 0.00003443
Iteration 85/1000 | Loss: 0.00003443
Iteration 86/1000 | Loss: 0.00003442
Iteration 87/1000 | Loss: 0.00003442
Iteration 88/1000 | Loss: 0.00003442
Iteration 89/1000 | Loss: 0.00003442
Iteration 90/1000 | Loss: 0.00003442
Iteration 91/1000 | Loss: 0.00003442
Iteration 92/1000 | Loss: 0.00003442
Iteration 93/1000 | Loss: 0.00003442
Iteration 94/1000 | Loss: 0.00003442
Iteration 95/1000 | Loss: 0.00003442
Iteration 96/1000 | Loss: 0.00003442
Iteration 97/1000 | Loss: 0.00003442
Iteration 98/1000 | Loss: 0.00003442
Iteration 99/1000 | Loss: 0.00003441
Iteration 100/1000 | Loss: 0.00003441
Iteration 101/1000 | Loss: 0.00003441
Iteration 102/1000 | Loss: 0.00003441
Iteration 103/1000 | Loss: 0.00003441
Iteration 104/1000 | Loss: 0.00003441
Iteration 105/1000 | Loss: 0.00003441
Iteration 106/1000 | Loss: 0.00003441
Iteration 107/1000 | Loss: 0.00003441
Iteration 108/1000 | Loss: 0.00003441
Iteration 109/1000 | Loss: 0.00003441
Iteration 110/1000 | Loss: 0.00003441
Iteration 111/1000 | Loss: 0.00003441
Iteration 112/1000 | Loss: 0.00003441
Iteration 113/1000 | Loss: 0.00003441
Iteration 114/1000 | Loss: 0.00003441
Iteration 115/1000 | Loss: 0.00003441
Iteration 116/1000 | Loss: 0.00003441
Iteration 117/1000 | Loss: 0.00003441
Iteration 118/1000 | Loss: 0.00003441
Iteration 119/1000 | Loss: 0.00003440
Iteration 120/1000 | Loss: 0.00003440
Iteration 121/1000 | Loss: 0.00003440
Iteration 122/1000 | Loss: 0.00003440
Iteration 123/1000 | Loss: 0.00003440
Iteration 124/1000 | Loss: 0.00003440
Iteration 125/1000 | Loss: 0.00003440
Iteration 126/1000 | Loss: 0.00003440
Iteration 127/1000 | Loss: 0.00003440
Iteration 128/1000 | Loss: 0.00003440
Iteration 129/1000 | Loss: 0.00003440
Iteration 130/1000 | Loss: 0.00003440
Iteration 131/1000 | Loss: 0.00003440
Iteration 132/1000 | Loss: 0.00003440
Iteration 133/1000 | Loss: 0.00003440
Iteration 134/1000 | Loss: 0.00003440
Iteration 135/1000 | Loss: 0.00003440
Iteration 136/1000 | Loss: 0.00003440
Iteration 137/1000 | Loss: 0.00003440
Iteration 138/1000 | Loss: 0.00003439
Iteration 139/1000 | Loss: 0.00003439
Iteration 140/1000 | Loss: 0.00003439
Iteration 141/1000 | Loss: 0.00003439
Iteration 142/1000 | Loss: 0.00003439
Iteration 143/1000 | Loss: 0.00003439
Iteration 144/1000 | Loss: 0.00003439
Iteration 145/1000 | Loss: 0.00003439
Iteration 146/1000 | Loss: 0.00003439
Iteration 147/1000 | Loss: 0.00003439
Iteration 148/1000 | Loss: 0.00003439
Iteration 149/1000 | Loss: 0.00003439
Iteration 150/1000 | Loss: 0.00003439
Iteration 151/1000 | Loss: 0.00003439
Iteration 152/1000 | Loss: 0.00003439
Iteration 153/1000 | Loss: 0.00003438
Iteration 154/1000 | Loss: 0.00003438
Iteration 155/1000 | Loss: 0.00003438
Iteration 156/1000 | Loss: 0.00003438
Iteration 157/1000 | Loss: 0.00003438
Iteration 158/1000 | Loss: 0.00003438
Iteration 159/1000 | Loss: 0.00003438
Iteration 160/1000 | Loss: 0.00003438
Iteration 161/1000 | Loss: 0.00003438
Iteration 162/1000 | Loss: 0.00003438
Iteration 163/1000 | Loss: 0.00003438
Iteration 164/1000 | Loss: 0.00003438
Iteration 165/1000 | Loss: 0.00003438
Iteration 166/1000 | Loss: 0.00003438
Iteration 167/1000 | Loss: 0.00003438
Iteration 168/1000 | Loss: 0.00003438
Iteration 169/1000 | Loss: 0.00003438
Iteration 170/1000 | Loss: 0.00003438
Iteration 171/1000 | Loss: 0.00003438
Iteration 172/1000 | Loss: 0.00003438
Iteration 173/1000 | Loss: 0.00003438
Iteration 174/1000 | Loss: 0.00003438
Iteration 175/1000 | Loss: 0.00003438
Iteration 176/1000 | Loss: 0.00003438
Iteration 177/1000 | Loss: 0.00003438
Iteration 178/1000 | Loss: 0.00003438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [3.43769570463337e-05, 3.43769570463337e-05, 3.43769570463337e-05, 3.43769570463337e-05, 3.43769570463337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.43769570463337e-05

Optimization complete. Final v2v error: 3.966262102127075 mm

Highest mean error: 10.302755355834961 mm for frame 122

Lowest mean error: 2.775357723236084 mm for frame 150

Saving results

Total time: 77.92551279067993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00351167
Iteration 2/25 | Loss: 0.00068312
Iteration 3/25 | Loss: 0.00057878
Iteration 4/25 | Loss: 0.00056617
Iteration 5/25 | Loss: 0.00056229
Iteration 6/25 | Loss: 0.00056129
Iteration 7/25 | Loss: 0.00056103
Iteration 8/25 | Loss: 0.00056103
Iteration 9/25 | Loss: 0.00056103
Iteration 10/25 | Loss: 0.00056103
Iteration 11/25 | Loss: 0.00056103
Iteration 12/25 | Loss: 0.00056103
Iteration 13/25 | Loss: 0.00056103
Iteration 14/25 | Loss: 0.00056103
Iteration 15/25 | Loss: 0.00056103
Iteration 16/25 | Loss: 0.00056103
Iteration 17/25 | Loss: 0.00056103
Iteration 18/25 | Loss: 0.00056103
Iteration 19/25 | Loss: 0.00056103
Iteration 20/25 | Loss: 0.00056103
Iteration 21/25 | Loss: 0.00056103
Iteration 22/25 | Loss: 0.00056103
Iteration 23/25 | Loss: 0.00056103
Iteration 24/25 | Loss: 0.00056103
Iteration 25/25 | Loss: 0.00056103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46758950
Iteration 2/25 | Loss: 0.00026571
Iteration 3/25 | Loss: 0.00026571
Iteration 4/25 | Loss: 0.00026571
Iteration 5/25 | Loss: 0.00026571
Iteration 6/25 | Loss: 0.00026571
Iteration 7/25 | Loss: 0.00026571
Iteration 8/25 | Loss: 0.00026571
Iteration 9/25 | Loss: 0.00026571
Iteration 10/25 | Loss: 0.00026571
Iteration 11/25 | Loss: 0.00026571
Iteration 12/25 | Loss: 0.00026571
Iteration 13/25 | Loss: 0.00026571
Iteration 14/25 | Loss: 0.00026571
Iteration 15/25 | Loss: 0.00026571
Iteration 16/25 | Loss: 0.00026571
Iteration 17/25 | Loss: 0.00026571
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000265711743850261, 0.000265711743850261, 0.000265711743850261, 0.000265711743850261, 0.000265711743850261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000265711743850261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026571
Iteration 2/1000 | Loss: 0.00001821
Iteration 3/1000 | Loss: 0.00001260
Iteration 4/1000 | Loss: 0.00001149
Iteration 5/1000 | Loss: 0.00001108
Iteration 6/1000 | Loss: 0.00001067
Iteration 7/1000 | Loss: 0.00001049
Iteration 8/1000 | Loss: 0.00001044
Iteration 9/1000 | Loss: 0.00001032
Iteration 10/1000 | Loss: 0.00001028
Iteration 11/1000 | Loss: 0.00001028
Iteration 12/1000 | Loss: 0.00001028
Iteration 13/1000 | Loss: 0.00001028
Iteration 14/1000 | Loss: 0.00001028
Iteration 15/1000 | Loss: 0.00001027
Iteration 16/1000 | Loss: 0.00001027
Iteration 17/1000 | Loss: 0.00001027
Iteration 18/1000 | Loss: 0.00001027
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001027
Iteration 21/1000 | Loss: 0.00001026
Iteration 22/1000 | Loss: 0.00001026
Iteration 23/1000 | Loss: 0.00001023
Iteration 24/1000 | Loss: 0.00001022
Iteration 25/1000 | Loss: 0.00001021
Iteration 26/1000 | Loss: 0.00001021
Iteration 27/1000 | Loss: 0.00001018
Iteration 28/1000 | Loss: 0.00001018
Iteration 29/1000 | Loss: 0.00001017
Iteration 30/1000 | Loss: 0.00001016
Iteration 31/1000 | Loss: 0.00001016
Iteration 32/1000 | Loss: 0.00001016
Iteration 33/1000 | Loss: 0.00001016
Iteration 34/1000 | Loss: 0.00001015
Iteration 35/1000 | Loss: 0.00001011
Iteration 36/1000 | Loss: 0.00001010
Iteration 37/1000 | Loss: 0.00001007
Iteration 38/1000 | Loss: 0.00001007
Iteration 39/1000 | Loss: 0.00001006
Iteration 40/1000 | Loss: 0.00001006
Iteration 41/1000 | Loss: 0.00001006
Iteration 42/1000 | Loss: 0.00001005
Iteration 43/1000 | Loss: 0.00001004
Iteration 44/1000 | Loss: 0.00001004
Iteration 45/1000 | Loss: 0.00001004
Iteration 46/1000 | Loss: 0.00001004
Iteration 47/1000 | Loss: 0.00001004
Iteration 48/1000 | Loss: 0.00001004
Iteration 49/1000 | Loss: 0.00001004
Iteration 50/1000 | Loss: 0.00001004
Iteration 51/1000 | Loss: 0.00001004
Iteration 52/1000 | Loss: 0.00001004
Iteration 53/1000 | Loss: 0.00001004
Iteration 54/1000 | Loss: 0.00001004
Iteration 55/1000 | Loss: 0.00001004
Iteration 56/1000 | Loss: 0.00001003
Iteration 57/1000 | Loss: 0.00001003
Iteration 58/1000 | Loss: 0.00001003
Iteration 59/1000 | Loss: 0.00001003
Iteration 60/1000 | Loss: 0.00001003
Iteration 61/1000 | Loss: 0.00001003
Iteration 62/1000 | Loss: 0.00001003
Iteration 63/1000 | Loss: 0.00001002
Iteration 64/1000 | Loss: 0.00001002
Iteration 65/1000 | Loss: 0.00001002
Iteration 66/1000 | Loss: 0.00001002
Iteration 67/1000 | Loss: 0.00001001
Iteration 68/1000 | Loss: 0.00001001
Iteration 69/1000 | Loss: 0.00001001
Iteration 70/1000 | Loss: 0.00001001
Iteration 71/1000 | Loss: 0.00001001
Iteration 72/1000 | Loss: 0.00001001
Iteration 73/1000 | Loss: 0.00001000
Iteration 74/1000 | Loss: 0.00001000
Iteration 75/1000 | Loss: 0.00001000
Iteration 76/1000 | Loss: 0.00001000
Iteration 77/1000 | Loss: 0.00001000
Iteration 78/1000 | Loss: 0.00000998
Iteration 79/1000 | Loss: 0.00000998
Iteration 80/1000 | Loss: 0.00000998
Iteration 81/1000 | Loss: 0.00000997
Iteration 82/1000 | Loss: 0.00000997
Iteration 83/1000 | Loss: 0.00000996
Iteration 84/1000 | Loss: 0.00000995
Iteration 85/1000 | Loss: 0.00000995
Iteration 86/1000 | Loss: 0.00000994
Iteration 87/1000 | Loss: 0.00000994
Iteration 88/1000 | Loss: 0.00000994
Iteration 89/1000 | Loss: 0.00000993
Iteration 90/1000 | Loss: 0.00000993
Iteration 91/1000 | Loss: 0.00000992
Iteration 92/1000 | Loss: 0.00000991
Iteration 93/1000 | Loss: 0.00000991
Iteration 94/1000 | Loss: 0.00000990
Iteration 95/1000 | Loss: 0.00000990
Iteration 96/1000 | Loss: 0.00000989
Iteration 97/1000 | Loss: 0.00000989
Iteration 98/1000 | Loss: 0.00000989
Iteration 99/1000 | Loss: 0.00000989
Iteration 100/1000 | Loss: 0.00000988
Iteration 101/1000 | Loss: 0.00000988
Iteration 102/1000 | Loss: 0.00000988
Iteration 103/1000 | Loss: 0.00000988
Iteration 104/1000 | Loss: 0.00000988
Iteration 105/1000 | Loss: 0.00000988
Iteration 106/1000 | Loss: 0.00000988
Iteration 107/1000 | Loss: 0.00000988
Iteration 108/1000 | Loss: 0.00000988
Iteration 109/1000 | Loss: 0.00000988
Iteration 110/1000 | Loss: 0.00000988
Iteration 111/1000 | Loss: 0.00000988
Iteration 112/1000 | Loss: 0.00000988
Iteration 113/1000 | Loss: 0.00000988
Iteration 114/1000 | Loss: 0.00000987
Iteration 115/1000 | Loss: 0.00000987
Iteration 116/1000 | Loss: 0.00000987
Iteration 117/1000 | Loss: 0.00000987
Iteration 118/1000 | Loss: 0.00000987
Iteration 119/1000 | Loss: 0.00000987
Iteration 120/1000 | Loss: 0.00000987
Iteration 121/1000 | Loss: 0.00000987
Iteration 122/1000 | Loss: 0.00000987
Iteration 123/1000 | Loss: 0.00000987
Iteration 124/1000 | Loss: 0.00000987
Iteration 125/1000 | Loss: 0.00000987
Iteration 126/1000 | Loss: 0.00000987
Iteration 127/1000 | Loss: 0.00000987
Iteration 128/1000 | Loss: 0.00000987
Iteration 129/1000 | Loss: 0.00000987
Iteration 130/1000 | Loss: 0.00000987
Iteration 131/1000 | Loss: 0.00000987
Iteration 132/1000 | Loss: 0.00000987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [9.872236660157796e-06, 9.872236660157796e-06, 9.872236660157796e-06, 9.872236660157796e-06, 9.872236660157796e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.872236660157796e-06

Optimization complete. Final v2v error: 2.687972068786621 mm

Highest mean error: 2.840426206588745 mm for frame 146

Lowest mean error: 2.651035785675049 mm for frame 13

Saving results

Total time: 32.79788780212402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441803
Iteration 2/25 | Loss: 0.00110110
Iteration 3/25 | Loss: 0.00074414
Iteration 4/25 | Loss: 0.00064762
Iteration 5/25 | Loss: 0.00062251
Iteration 6/25 | Loss: 0.00061660
Iteration 7/25 | Loss: 0.00061515
Iteration 8/25 | Loss: 0.00061510
Iteration 9/25 | Loss: 0.00061510
Iteration 10/25 | Loss: 0.00061510
Iteration 11/25 | Loss: 0.00061510
Iteration 12/25 | Loss: 0.00061510
Iteration 13/25 | Loss: 0.00061510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006150964763946831, 0.0006150964763946831, 0.0006150964763946831, 0.0006150964763946831, 0.0006150964763946831]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006150964763946831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56655288
Iteration 2/25 | Loss: 0.00027163
Iteration 3/25 | Loss: 0.00027163
Iteration 4/25 | Loss: 0.00027163
Iteration 5/25 | Loss: 0.00027163
Iteration 6/25 | Loss: 0.00027163
Iteration 7/25 | Loss: 0.00027163
Iteration 8/25 | Loss: 0.00027163
Iteration 9/25 | Loss: 0.00027163
Iteration 10/25 | Loss: 0.00027163
Iteration 11/25 | Loss: 0.00027163
Iteration 12/25 | Loss: 0.00027163
Iteration 13/25 | Loss: 0.00027163
Iteration 14/25 | Loss: 0.00027163
Iteration 15/25 | Loss: 0.00027163
Iteration 16/25 | Loss: 0.00027163
Iteration 17/25 | Loss: 0.00027163
Iteration 18/25 | Loss: 0.00027163
Iteration 19/25 | Loss: 0.00027163
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002716304734349251, 0.0002716304734349251, 0.0002716304734349251, 0.0002716304734349251, 0.0002716304734349251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002716304734349251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027163
Iteration 2/1000 | Loss: 0.00002168
Iteration 3/1000 | Loss: 0.00001638
Iteration 4/1000 | Loss: 0.00001530
Iteration 5/1000 | Loss: 0.00001448
Iteration 6/1000 | Loss: 0.00001406
Iteration 7/1000 | Loss: 0.00001373
Iteration 8/1000 | Loss: 0.00001354
Iteration 9/1000 | Loss: 0.00001340
Iteration 10/1000 | Loss: 0.00001335
Iteration 11/1000 | Loss: 0.00001325
Iteration 12/1000 | Loss: 0.00001320
Iteration 13/1000 | Loss: 0.00001318
Iteration 14/1000 | Loss: 0.00001316
Iteration 15/1000 | Loss: 0.00001315
Iteration 16/1000 | Loss: 0.00001315
Iteration 17/1000 | Loss: 0.00001314
Iteration 18/1000 | Loss: 0.00001313
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001312
Iteration 22/1000 | Loss: 0.00001311
Iteration 23/1000 | Loss: 0.00001305
Iteration 24/1000 | Loss: 0.00001303
Iteration 25/1000 | Loss: 0.00001302
Iteration 26/1000 | Loss: 0.00001302
Iteration 27/1000 | Loss: 0.00001302
Iteration 28/1000 | Loss: 0.00001300
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001298
Iteration 31/1000 | Loss: 0.00001298
Iteration 32/1000 | Loss: 0.00001297
Iteration 33/1000 | Loss: 0.00001297
Iteration 34/1000 | Loss: 0.00001296
Iteration 35/1000 | Loss: 0.00001296
Iteration 36/1000 | Loss: 0.00001295
Iteration 37/1000 | Loss: 0.00001295
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001294
Iteration 40/1000 | Loss: 0.00001294
Iteration 41/1000 | Loss: 0.00001294
Iteration 42/1000 | Loss: 0.00001293
Iteration 43/1000 | Loss: 0.00001293
Iteration 44/1000 | Loss: 0.00001293
Iteration 45/1000 | Loss: 0.00001293
Iteration 46/1000 | Loss: 0.00001293
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001292
Iteration 49/1000 | Loss: 0.00001292
Iteration 50/1000 | Loss: 0.00001292
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001291
Iteration 53/1000 | Loss: 0.00001291
Iteration 54/1000 | Loss: 0.00001291
Iteration 55/1000 | Loss: 0.00001291
Iteration 56/1000 | Loss: 0.00001290
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001290
Iteration 60/1000 | Loss: 0.00001290
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001289
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001289
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001288
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001287
Iteration 73/1000 | Loss: 0.00001287
Iteration 74/1000 | Loss: 0.00001287
Iteration 75/1000 | Loss: 0.00001287
Iteration 76/1000 | Loss: 0.00001287
Iteration 77/1000 | Loss: 0.00001287
Iteration 78/1000 | Loss: 0.00001287
Iteration 79/1000 | Loss: 0.00001287
Iteration 80/1000 | Loss: 0.00001287
Iteration 81/1000 | Loss: 0.00001286
Iteration 82/1000 | Loss: 0.00001286
Iteration 83/1000 | Loss: 0.00001286
Iteration 84/1000 | Loss: 0.00001286
Iteration 85/1000 | Loss: 0.00001285
Iteration 86/1000 | Loss: 0.00001285
Iteration 87/1000 | Loss: 0.00001285
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001284
Iteration 92/1000 | Loss: 0.00001284
Iteration 93/1000 | Loss: 0.00001284
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001283
Iteration 96/1000 | Loss: 0.00001283
Iteration 97/1000 | Loss: 0.00001283
Iteration 98/1000 | Loss: 0.00001283
Iteration 99/1000 | Loss: 0.00001283
Iteration 100/1000 | Loss: 0.00001283
Iteration 101/1000 | Loss: 0.00001282
Iteration 102/1000 | Loss: 0.00001282
Iteration 103/1000 | Loss: 0.00001282
Iteration 104/1000 | Loss: 0.00001282
Iteration 105/1000 | Loss: 0.00001282
Iteration 106/1000 | Loss: 0.00001282
Iteration 107/1000 | Loss: 0.00001282
Iteration 108/1000 | Loss: 0.00001282
Iteration 109/1000 | Loss: 0.00001282
Iteration 110/1000 | Loss: 0.00001282
Iteration 111/1000 | Loss: 0.00001281
Iteration 112/1000 | Loss: 0.00001281
Iteration 113/1000 | Loss: 0.00001281
Iteration 114/1000 | Loss: 0.00001281
Iteration 115/1000 | Loss: 0.00001281
Iteration 116/1000 | Loss: 0.00001281
Iteration 117/1000 | Loss: 0.00001281
Iteration 118/1000 | Loss: 0.00001280
Iteration 119/1000 | Loss: 0.00001280
Iteration 120/1000 | Loss: 0.00001280
Iteration 121/1000 | Loss: 0.00001280
Iteration 122/1000 | Loss: 0.00001280
Iteration 123/1000 | Loss: 0.00001280
Iteration 124/1000 | Loss: 0.00001280
Iteration 125/1000 | Loss: 0.00001280
Iteration 126/1000 | Loss: 0.00001280
Iteration 127/1000 | Loss: 0.00001280
Iteration 128/1000 | Loss: 0.00001279
Iteration 129/1000 | Loss: 0.00001279
Iteration 130/1000 | Loss: 0.00001279
Iteration 131/1000 | Loss: 0.00001278
Iteration 132/1000 | Loss: 0.00001278
Iteration 133/1000 | Loss: 0.00001278
Iteration 134/1000 | Loss: 0.00001278
Iteration 135/1000 | Loss: 0.00001277
Iteration 136/1000 | Loss: 0.00001277
Iteration 137/1000 | Loss: 0.00001277
Iteration 138/1000 | Loss: 0.00001277
Iteration 139/1000 | Loss: 0.00001277
Iteration 140/1000 | Loss: 0.00001277
Iteration 141/1000 | Loss: 0.00001277
Iteration 142/1000 | Loss: 0.00001277
Iteration 143/1000 | Loss: 0.00001277
Iteration 144/1000 | Loss: 0.00001277
Iteration 145/1000 | Loss: 0.00001276
Iteration 146/1000 | Loss: 0.00001276
Iteration 147/1000 | Loss: 0.00001276
Iteration 148/1000 | Loss: 0.00001276
Iteration 149/1000 | Loss: 0.00001276
Iteration 150/1000 | Loss: 0.00001276
Iteration 151/1000 | Loss: 0.00001276
Iteration 152/1000 | Loss: 0.00001276
Iteration 153/1000 | Loss: 0.00001276
Iteration 154/1000 | Loss: 0.00001275
Iteration 155/1000 | Loss: 0.00001275
Iteration 156/1000 | Loss: 0.00001275
Iteration 157/1000 | Loss: 0.00001275
Iteration 158/1000 | Loss: 0.00001275
Iteration 159/1000 | Loss: 0.00001275
Iteration 160/1000 | Loss: 0.00001275
Iteration 161/1000 | Loss: 0.00001274
Iteration 162/1000 | Loss: 0.00001274
Iteration 163/1000 | Loss: 0.00001274
Iteration 164/1000 | Loss: 0.00001274
Iteration 165/1000 | Loss: 0.00001274
Iteration 166/1000 | Loss: 0.00001274
Iteration 167/1000 | Loss: 0.00001274
Iteration 168/1000 | Loss: 0.00001274
Iteration 169/1000 | Loss: 0.00001274
Iteration 170/1000 | Loss: 0.00001274
Iteration 171/1000 | Loss: 0.00001274
Iteration 172/1000 | Loss: 0.00001274
Iteration 173/1000 | Loss: 0.00001274
Iteration 174/1000 | Loss: 0.00001274
Iteration 175/1000 | Loss: 0.00001274
Iteration 176/1000 | Loss: 0.00001274
Iteration 177/1000 | Loss: 0.00001274
Iteration 178/1000 | Loss: 0.00001274
Iteration 179/1000 | Loss: 0.00001274
Iteration 180/1000 | Loss: 0.00001274
Iteration 181/1000 | Loss: 0.00001274
Iteration 182/1000 | Loss: 0.00001274
Iteration 183/1000 | Loss: 0.00001274
Iteration 184/1000 | Loss: 0.00001274
Iteration 185/1000 | Loss: 0.00001274
Iteration 186/1000 | Loss: 0.00001274
Iteration 187/1000 | Loss: 0.00001274
Iteration 188/1000 | Loss: 0.00001274
Iteration 189/1000 | Loss: 0.00001274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.2744661944452673e-05, 1.2744661944452673e-05, 1.2744661944452673e-05, 1.2744661944452673e-05, 1.2744661944452673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2744661944452673e-05

Optimization complete. Final v2v error: 2.9970765113830566 mm

Highest mean error: 4.029760837554932 mm for frame 104

Lowest mean error: 2.8251771926879883 mm for frame 179

Saving results

Total time: 42.460734367370605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791436
Iteration 2/25 | Loss: 0.00204536
Iteration 3/25 | Loss: 0.00110754
Iteration 4/25 | Loss: 0.00088817
Iteration 5/25 | Loss: 0.00085390
Iteration 6/25 | Loss: 0.00074190
Iteration 7/25 | Loss: 0.00074360
Iteration 8/25 | Loss: 0.00078079
Iteration 9/25 | Loss: 0.00071056
Iteration 10/25 | Loss: 0.00073559
Iteration 11/25 | Loss: 0.00064023
Iteration 12/25 | Loss: 0.00066008
Iteration 13/25 | Loss: 0.00062425
Iteration 14/25 | Loss: 0.00062343
Iteration 15/25 | Loss: 0.00062127
Iteration 16/25 | Loss: 0.00062700
Iteration 17/25 | Loss: 0.00061491
Iteration 18/25 | Loss: 0.00061586
Iteration 19/25 | Loss: 0.00061217
Iteration 20/25 | Loss: 0.00061430
Iteration 21/25 | Loss: 0.00061163
Iteration 22/25 | Loss: 0.00061161
Iteration 23/25 | Loss: 0.00061161
Iteration 24/25 | Loss: 0.00061161
Iteration 25/25 | Loss: 0.00061161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11952996
Iteration 2/25 | Loss: 0.00032164
Iteration 3/25 | Loss: 0.00032164
Iteration 4/25 | Loss: 0.00032164
Iteration 5/25 | Loss: 0.00034228
Iteration 6/25 | Loss: 0.00028997
Iteration 7/25 | Loss: 0.00028997
Iteration 8/25 | Loss: 0.00028997
Iteration 9/25 | Loss: 0.00028997
Iteration 10/25 | Loss: 0.00028997
Iteration 11/25 | Loss: 0.00028997
Iteration 12/25 | Loss: 0.00028997
Iteration 13/25 | Loss: 0.00028997
Iteration 14/25 | Loss: 0.00028997
Iteration 15/25 | Loss: 0.00028997
Iteration 16/25 | Loss: 0.00028997
Iteration 17/25 | Loss: 0.00028997
Iteration 18/25 | Loss: 0.00028997
Iteration 19/25 | Loss: 0.00028997
Iteration 20/25 | Loss: 0.00028997
Iteration 21/25 | Loss: 0.00028997
Iteration 22/25 | Loss: 0.00028997
Iteration 23/25 | Loss: 0.00028997
Iteration 24/25 | Loss: 0.00028997
Iteration 25/25 | Loss: 0.00028997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028997
Iteration 2/1000 | Loss: 0.00003069
Iteration 3/1000 | Loss: 0.00001993
Iteration 4/1000 | Loss: 0.00009148
Iteration 5/1000 | Loss: 0.00055151
Iteration 6/1000 | Loss: 0.00006846
Iteration 7/1000 | Loss: 0.00003084
Iteration 8/1000 | Loss: 0.00001618
Iteration 9/1000 | Loss: 0.00002087
Iteration 10/1000 | Loss: 0.00007835
Iteration 11/1000 | Loss: 0.00003078
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001490
Iteration 14/1000 | Loss: 0.00001360
Iteration 15/1000 | Loss: 0.00001787
Iteration 16/1000 | Loss: 0.00010234
Iteration 17/1000 | Loss: 0.00001343
Iteration 18/1000 | Loss: 0.00003349
Iteration 19/1000 | Loss: 0.00009162
Iteration 20/1000 | Loss: 0.00001598
Iteration 21/1000 | Loss: 0.00002724
Iteration 22/1000 | Loss: 0.00001315
Iteration 23/1000 | Loss: 0.00001311
Iteration 24/1000 | Loss: 0.00001311
Iteration 25/1000 | Loss: 0.00001311
Iteration 26/1000 | Loss: 0.00001310
Iteration 27/1000 | Loss: 0.00001310
Iteration 28/1000 | Loss: 0.00001310
Iteration 29/1000 | Loss: 0.00001310
Iteration 30/1000 | Loss: 0.00001310
Iteration 31/1000 | Loss: 0.00001309
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001308
Iteration 35/1000 | Loss: 0.00001307
Iteration 36/1000 | Loss: 0.00001306
Iteration 37/1000 | Loss: 0.00001305
Iteration 38/1000 | Loss: 0.00001305
Iteration 39/1000 | Loss: 0.00001305
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001289
Iteration 42/1000 | Loss: 0.00001285
Iteration 43/1000 | Loss: 0.00001283
Iteration 44/1000 | Loss: 0.00001283
Iteration 45/1000 | Loss: 0.00001278
Iteration 46/1000 | Loss: 0.00001277
Iteration 47/1000 | Loss: 0.00001273
Iteration 48/1000 | Loss: 0.00001273
Iteration 49/1000 | Loss: 0.00001272
Iteration 50/1000 | Loss: 0.00001272
Iteration 51/1000 | Loss: 0.00001272
Iteration 52/1000 | Loss: 0.00001271
Iteration 53/1000 | Loss: 0.00001271
Iteration 54/1000 | Loss: 0.00002555
Iteration 55/1000 | Loss: 0.00001261
Iteration 56/1000 | Loss: 0.00001261
Iteration 57/1000 | Loss: 0.00001261
Iteration 58/1000 | Loss: 0.00001260
Iteration 59/1000 | Loss: 0.00001260
Iteration 60/1000 | Loss: 0.00001259
Iteration 61/1000 | Loss: 0.00001259
Iteration 62/1000 | Loss: 0.00001259
Iteration 63/1000 | Loss: 0.00001259
Iteration 64/1000 | Loss: 0.00001259
Iteration 65/1000 | Loss: 0.00001259
Iteration 66/1000 | Loss: 0.00001259
Iteration 67/1000 | Loss: 0.00001259
Iteration 68/1000 | Loss: 0.00001259
Iteration 69/1000 | Loss: 0.00001259
Iteration 70/1000 | Loss: 0.00001259
Iteration 71/1000 | Loss: 0.00001259
Iteration 72/1000 | Loss: 0.00001259
Iteration 73/1000 | Loss: 0.00001259
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001258
Iteration 76/1000 | Loss: 0.00001258
Iteration 77/1000 | Loss: 0.00001258
Iteration 78/1000 | Loss: 0.00001258
Iteration 79/1000 | Loss: 0.00001257
Iteration 80/1000 | Loss: 0.00001257
Iteration 81/1000 | Loss: 0.00001257
Iteration 82/1000 | Loss: 0.00001257
Iteration 83/1000 | Loss: 0.00001257
Iteration 84/1000 | Loss: 0.00001256
Iteration 85/1000 | Loss: 0.00001256
Iteration 86/1000 | Loss: 0.00001256
Iteration 87/1000 | Loss: 0.00001256
Iteration 88/1000 | Loss: 0.00001256
Iteration 89/1000 | Loss: 0.00001256
Iteration 90/1000 | Loss: 0.00001256
Iteration 91/1000 | Loss: 0.00001256
Iteration 92/1000 | Loss: 0.00001256
Iteration 93/1000 | Loss: 0.00001256
Iteration 94/1000 | Loss: 0.00001256
Iteration 95/1000 | Loss: 0.00001255
Iteration 96/1000 | Loss: 0.00001255
Iteration 97/1000 | Loss: 0.00001255
Iteration 98/1000 | Loss: 0.00001254
Iteration 99/1000 | Loss: 0.00001254
Iteration 100/1000 | Loss: 0.00001254
Iteration 101/1000 | Loss: 0.00001254
Iteration 102/1000 | Loss: 0.00001254
Iteration 103/1000 | Loss: 0.00001254
Iteration 104/1000 | Loss: 0.00001254
Iteration 105/1000 | Loss: 0.00001254
Iteration 106/1000 | Loss: 0.00001253
Iteration 107/1000 | Loss: 0.00001253
Iteration 108/1000 | Loss: 0.00001253
Iteration 109/1000 | Loss: 0.00001253
Iteration 110/1000 | Loss: 0.00001253
Iteration 111/1000 | Loss: 0.00001253
Iteration 112/1000 | Loss: 0.00001252
Iteration 113/1000 | Loss: 0.00001252
Iteration 114/1000 | Loss: 0.00001252
Iteration 115/1000 | Loss: 0.00001252
Iteration 116/1000 | Loss: 0.00001252
Iteration 117/1000 | Loss: 0.00001252
Iteration 118/1000 | Loss: 0.00001252
Iteration 119/1000 | Loss: 0.00001252
Iteration 120/1000 | Loss: 0.00001252
Iteration 121/1000 | Loss: 0.00001252
Iteration 122/1000 | Loss: 0.00001252
Iteration 123/1000 | Loss: 0.00001252
Iteration 124/1000 | Loss: 0.00001252
Iteration 125/1000 | Loss: 0.00001252
Iteration 126/1000 | Loss: 0.00001252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.2522232282208279e-05, 1.2522232282208279e-05, 1.2522232282208279e-05, 1.2522232282208279e-05, 1.2522232282208279e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2522232282208279e-05

Optimization complete. Final v2v error: 3.01035737991333 mm

Highest mean error: 3.7857134342193604 mm for frame 138

Lowest mean error: 2.79103946685791 mm for frame 148

Saving results

Total time: 83.85011529922485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827902
Iteration 2/25 | Loss: 0.00075584
Iteration 3/25 | Loss: 0.00061209
Iteration 4/25 | Loss: 0.00059314
Iteration 5/25 | Loss: 0.00058758
Iteration 6/25 | Loss: 0.00058655
Iteration 7/25 | Loss: 0.00058642
Iteration 8/25 | Loss: 0.00058642
Iteration 9/25 | Loss: 0.00058642
Iteration 10/25 | Loss: 0.00058642
Iteration 11/25 | Loss: 0.00058642
Iteration 12/25 | Loss: 0.00058642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005864213453605771, 0.0005864213453605771, 0.0005864213453605771, 0.0005864213453605771, 0.0005864213453605771]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005864213453605771

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46747422
Iteration 2/25 | Loss: 0.00026638
Iteration 3/25 | Loss: 0.00026637
Iteration 4/25 | Loss: 0.00026637
Iteration 5/25 | Loss: 0.00026637
Iteration 6/25 | Loss: 0.00026637
Iteration 7/25 | Loss: 0.00026637
Iteration 8/25 | Loss: 0.00026637
Iteration 9/25 | Loss: 0.00026637
Iteration 10/25 | Loss: 0.00026637
Iteration 11/25 | Loss: 0.00026637
Iteration 12/25 | Loss: 0.00026637
Iteration 13/25 | Loss: 0.00026637
Iteration 14/25 | Loss: 0.00026637
Iteration 15/25 | Loss: 0.00026637
Iteration 16/25 | Loss: 0.00026637
Iteration 17/25 | Loss: 0.00026637
Iteration 18/25 | Loss: 0.00026637
Iteration 19/25 | Loss: 0.00026637
Iteration 20/25 | Loss: 0.00026637
Iteration 21/25 | Loss: 0.00026637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002663712075445801, 0.0002663712075445801, 0.0002663712075445801, 0.0002663712075445801, 0.0002663712075445801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002663712075445801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026637
Iteration 2/1000 | Loss: 0.00002081
Iteration 3/1000 | Loss: 0.00001284
Iteration 4/1000 | Loss: 0.00001157
Iteration 5/1000 | Loss: 0.00001085
Iteration 6/1000 | Loss: 0.00001046
Iteration 7/1000 | Loss: 0.00001014
Iteration 8/1000 | Loss: 0.00001009
Iteration 9/1000 | Loss: 0.00001006
Iteration 10/1000 | Loss: 0.00001003
Iteration 11/1000 | Loss: 0.00001000
Iteration 12/1000 | Loss: 0.00001000
Iteration 13/1000 | Loss: 0.00000998
Iteration 14/1000 | Loss: 0.00000997
Iteration 15/1000 | Loss: 0.00000994
Iteration 16/1000 | Loss: 0.00000994
Iteration 17/1000 | Loss: 0.00000993
Iteration 18/1000 | Loss: 0.00000993
Iteration 19/1000 | Loss: 0.00000992
Iteration 20/1000 | Loss: 0.00000992
Iteration 21/1000 | Loss: 0.00000992
Iteration 22/1000 | Loss: 0.00000992
Iteration 23/1000 | Loss: 0.00000991
Iteration 24/1000 | Loss: 0.00000991
Iteration 25/1000 | Loss: 0.00000991
Iteration 26/1000 | Loss: 0.00000990
Iteration 27/1000 | Loss: 0.00000990
Iteration 28/1000 | Loss: 0.00000990
Iteration 29/1000 | Loss: 0.00000989
Iteration 30/1000 | Loss: 0.00000989
Iteration 31/1000 | Loss: 0.00000988
Iteration 32/1000 | Loss: 0.00000987
Iteration 33/1000 | Loss: 0.00000984
Iteration 34/1000 | Loss: 0.00000982
Iteration 35/1000 | Loss: 0.00000982
Iteration 36/1000 | Loss: 0.00000981
Iteration 37/1000 | Loss: 0.00000981
Iteration 38/1000 | Loss: 0.00000981
Iteration 39/1000 | Loss: 0.00000981
Iteration 40/1000 | Loss: 0.00000981
Iteration 41/1000 | Loss: 0.00000981
Iteration 42/1000 | Loss: 0.00000980
Iteration 43/1000 | Loss: 0.00000979
Iteration 44/1000 | Loss: 0.00000979
Iteration 45/1000 | Loss: 0.00000979
Iteration 46/1000 | Loss: 0.00000979
Iteration 47/1000 | Loss: 0.00000978
Iteration 48/1000 | Loss: 0.00000978
Iteration 49/1000 | Loss: 0.00000977
Iteration 50/1000 | Loss: 0.00000977
Iteration 51/1000 | Loss: 0.00000977
Iteration 52/1000 | Loss: 0.00000977
Iteration 53/1000 | Loss: 0.00000976
Iteration 54/1000 | Loss: 0.00000976
Iteration 55/1000 | Loss: 0.00000976
Iteration 56/1000 | Loss: 0.00000975
Iteration 57/1000 | Loss: 0.00000974
Iteration 58/1000 | Loss: 0.00000974
Iteration 59/1000 | Loss: 0.00000974
Iteration 60/1000 | Loss: 0.00000974
Iteration 61/1000 | Loss: 0.00000974
Iteration 62/1000 | Loss: 0.00000974
Iteration 63/1000 | Loss: 0.00000974
Iteration 64/1000 | Loss: 0.00000974
Iteration 65/1000 | Loss: 0.00000974
Iteration 66/1000 | Loss: 0.00000974
Iteration 67/1000 | Loss: 0.00000974
Iteration 68/1000 | Loss: 0.00000974
Iteration 69/1000 | Loss: 0.00000974
Iteration 70/1000 | Loss: 0.00000974
Iteration 71/1000 | Loss: 0.00000974
Iteration 72/1000 | Loss: 0.00000974
Iteration 73/1000 | Loss: 0.00000974
Iteration 74/1000 | Loss: 0.00000974
Iteration 75/1000 | Loss: 0.00000974
Iteration 76/1000 | Loss: 0.00000973
Iteration 77/1000 | Loss: 0.00000973
Iteration 78/1000 | Loss: 0.00000973
Iteration 79/1000 | Loss: 0.00000973
Iteration 80/1000 | Loss: 0.00000973
Iteration 81/1000 | Loss: 0.00000973
Iteration 82/1000 | Loss: 0.00000973
Iteration 83/1000 | Loss: 0.00000973
Iteration 84/1000 | Loss: 0.00000973
Iteration 85/1000 | Loss: 0.00000973
Iteration 86/1000 | Loss: 0.00000973
Iteration 87/1000 | Loss: 0.00000973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [9.734627383295447e-06, 9.734627383295447e-06, 9.734627383295447e-06, 9.734627383295447e-06, 9.734627383295447e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.734627383295447e-06

Optimization complete. Final v2v error: 2.640878915786743 mm

Highest mean error: 2.7656612396240234 mm for frame 41

Lowest mean error: 2.560706615447998 mm for frame 18

Saving results

Total time: 26.754845142364502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429483
Iteration 2/25 | Loss: 0.00086152
Iteration 3/25 | Loss: 0.00069110
Iteration 4/25 | Loss: 0.00065484
Iteration 5/25 | Loss: 0.00065083
Iteration 6/25 | Loss: 0.00065041
Iteration 7/25 | Loss: 0.00065041
Iteration 8/25 | Loss: 0.00065041
Iteration 9/25 | Loss: 0.00065041
Iteration 10/25 | Loss: 0.00065041
Iteration 11/25 | Loss: 0.00065041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006504069315269589, 0.0006504069315269589, 0.0006504069315269589, 0.0006504069315269589, 0.0006504069315269589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006504069315269589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16189551
Iteration 2/25 | Loss: 0.00030013
Iteration 3/25 | Loss: 0.00030012
Iteration 4/25 | Loss: 0.00030012
Iteration 5/25 | Loss: 0.00030011
Iteration 6/25 | Loss: 0.00030011
Iteration 7/25 | Loss: 0.00030011
Iteration 8/25 | Loss: 0.00030011
Iteration 9/25 | Loss: 0.00030011
Iteration 10/25 | Loss: 0.00030011
Iteration 11/25 | Loss: 0.00030011
Iteration 12/25 | Loss: 0.00030011
Iteration 13/25 | Loss: 0.00030011
Iteration 14/25 | Loss: 0.00030011
Iteration 15/25 | Loss: 0.00030011
Iteration 16/25 | Loss: 0.00030011
Iteration 17/25 | Loss: 0.00030011
Iteration 18/25 | Loss: 0.00030011
Iteration 19/25 | Loss: 0.00030011
Iteration 20/25 | Loss: 0.00030011
Iteration 21/25 | Loss: 0.00030011
Iteration 22/25 | Loss: 0.00030011
Iteration 23/25 | Loss: 0.00030011
Iteration 24/25 | Loss: 0.00030011
Iteration 25/25 | Loss: 0.00030011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030011
Iteration 2/1000 | Loss: 0.00003569
Iteration 3/1000 | Loss: 0.00002476
Iteration 4/1000 | Loss: 0.00002198
Iteration 5/1000 | Loss: 0.00002090
Iteration 6/1000 | Loss: 0.00002009
Iteration 7/1000 | Loss: 0.00001953
Iteration 8/1000 | Loss: 0.00001923
Iteration 9/1000 | Loss: 0.00001907
Iteration 10/1000 | Loss: 0.00001900
Iteration 11/1000 | Loss: 0.00001900
Iteration 12/1000 | Loss: 0.00001898
Iteration 13/1000 | Loss: 0.00001897
Iteration 14/1000 | Loss: 0.00001897
Iteration 15/1000 | Loss: 0.00001896
Iteration 16/1000 | Loss: 0.00001896
Iteration 17/1000 | Loss: 0.00001896
Iteration 18/1000 | Loss: 0.00001895
Iteration 19/1000 | Loss: 0.00001889
Iteration 20/1000 | Loss: 0.00001886
Iteration 21/1000 | Loss: 0.00001885
Iteration 22/1000 | Loss: 0.00001874
Iteration 23/1000 | Loss: 0.00001869
Iteration 24/1000 | Loss: 0.00001867
Iteration 25/1000 | Loss: 0.00001867
Iteration 26/1000 | Loss: 0.00001867
Iteration 27/1000 | Loss: 0.00001867
Iteration 28/1000 | Loss: 0.00001867
Iteration 29/1000 | Loss: 0.00001867
Iteration 30/1000 | Loss: 0.00001867
Iteration 31/1000 | Loss: 0.00001867
Iteration 32/1000 | Loss: 0.00001866
Iteration 33/1000 | Loss: 0.00001866
Iteration 34/1000 | Loss: 0.00001866
Iteration 35/1000 | Loss: 0.00001866
Iteration 36/1000 | Loss: 0.00001866
Iteration 37/1000 | Loss: 0.00001866
Iteration 38/1000 | Loss: 0.00001866
Iteration 39/1000 | Loss: 0.00001866
Iteration 40/1000 | Loss: 0.00001865
Iteration 41/1000 | Loss: 0.00001865
Iteration 42/1000 | Loss: 0.00001865
Iteration 43/1000 | Loss: 0.00001865
Iteration 44/1000 | Loss: 0.00001865
Iteration 45/1000 | Loss: 0.00001865
Iteration 46/1000 | Loss: 0.00001865
Iteration 47/1000 | Loss: 0.00001864
Iteration 48/1000 | Loss: 0.00001864
Iteration 49/1000 | Loss: 0.00001864
Iteration 50/1000 | Loss: 0.00001864
Iteration 51/1000 | Loss: 0.00001863
Iteration 52/1000 | Loss: 0.00001862
Iteration 53/1000 | Loss: 0.00001862
Iteration 54/1000 | Loss: 0.00001861
Iteration 55/1000 | Loss: 0.00001861
Iteration 56/1000 | Loss: 0.00001861
Iteration 57/1000 | Loss: 0.00001860
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001860
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001860
Iteration 63/1000 | Loss: 0.00001860
Iteration 64/1000 | Loss: 0.00001860
Iteration 65/1000 | Loss: 0.00001860
Iteration 66/1000 | Loss: 0.00001860
Iteration 67/1000 | Loss: 0.00001860
Iteration 68/1000 | Loss: 0.00001860
Iteration 69/1000 | Loss: 0.00001860
Iteration 70/1000 | Loss: 0.00001860
Iteration 71/1000 | Loss: 0.00001860
Iteration 72/1000 | Loss: 0.00001860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [1.8596176232676953e-05, 1.8596176232676953e-05, 1.8596176232676953e-05, 1.8596176232676953e-05, 1.8596176232676953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8596176232676953e-05

Optimization complete. Final v2v error: 3.6204535961151123 mm

Highest mean error: 3.648375988006592 mm for frame 89

Lowest mean error: 3.572763204574585 mm for frame 21

Saving results

Total time: 28.061630725860596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00366773
Iteration 2/25 | Loss: 0.00076212
Iteration 3/25 | Loss: 0.00062783
Iteration 4/25 | Loss: 0.00060682
Iteration 5/25 | Loss: 0.00060141
Iteration 6/25 | Loss: 0.00059981
Iteration 7/25 | Loss: 0.00059951
Iteration 8/25 | Loss: 0.00059951
Iteration 9/25 | Loss: 0.00059951
Iteration 10/25 | Loss: 0.00059951
Iteration 11/25 | Loss: 0.00059951
Iteration 12/25 | Loss: 0.00059951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005995114333927631, 0.0005995114333927631, 0.0005995114333927631, 0.0005995114333927631, 0.0005995114333927631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005995114333927631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49854100
Iteration 2/25 | Loss: 0.00033492
Iteration 3/25 | Loss: 0.00033492
Iteration 4/25 | Loss: 0.00033492
Iteration 5/25 | Loss: 0.00033492
Iteration 6/25 | Loss: 0.00033492
Iteration 7/25 | Loss: 0.00033492
Iteration 8/25 | Loss: 0.00033492
Iteration 9/25 | Loss: 0.00033492
Iteration 10/25 | Loss: 0.00033492
Iteration 11/25 | Loss: 0.00033492
Iteration 12/25 | Loss: 0.00033492
Iteration 13/25 | Loss: 0.00033492
Iteration 14/25 | Loss: 0.00033492
Iteration 15/25 | Loss: 0.00033492
Iteration 16/25 | Loss: 0.00033492
Iteration 17/25 | Loss: 0.00033492
Iteration 18/25 | Loss: 0.00033492
Iteration 19/25 | Loss: 0.00033492
Iteration 20/25 | Loss: 0.00033492
Iteration 21/25 | Loss: 0.00033492
Iteration 22/25 | Loss: 0.00033492
Iteration 23/25 | Loss: 0.00033492
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00033492111833766103, 0.00033492111833766103, 0.00033492111833766103, 0.00033492111833766103, 0.00033492111833766103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033492111833766103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033492
Iteration 2/1000 | Loss: 0.00001805
Iteration 3/1000 | Loss: 0.00001291
Iteration 4/1000 | Loss: 0.00001217
Iteration 5/1000 | Loss: 0.00001173
Iteration 6/1000 | Loss: 0.00001155
Iteration 7/1000 | Loss: 0.00001154
Iteration 8/1000 | Loss: 0.00001131
Iteration 9/1000 | Loss: 0.00001120
Iteration 10/1000 | Loss: 0.00001118
Iteration 11/1000 | Loss: 0.00001115
Iteration 12/1000 | Loss: 0.00001111
Iteration 13/1000 | Loss: 0.00001110
Iteration 14/1000 | Loss: 0.00001109
Iteration 15/1000 | Loss: 0.00001109
Iteration 16/1000 | Loss: 0.00001109
Iteration 17/1000 | Loss: 0.00001108
Iteration 18/1000 | Loss: 0.00001108
Iteration 19/1000 | Loss: 0.00001108
Iteration 20/1000 | Loss: 0.00001106
Iteration 21/1000 | Loss: 0.00001104
Iteration 22/1000 | Loss: 0.00001104
Iteration 23/1000 | Loss: 0.00001104
Iteration 24/1000 | Loss: 0.00001103
Iteration 25/1000 | Loss: 0.00001103
Iteration 26/1000 | Loss: 0.00001103
Iteration 27/1000 | Loss: 0.00001098
Iteration 28/1000 | Loss: 0.00001097
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001097
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001097
Iteration 34/1000 | Loss: 0.00001097
Iteration 35/1000 | Loss: 0.00001097
Iteration 36/1000 | Loss: 0.00001097
Iteration 37/1000 | Loss: 0.00001097
Iteration 38/1000 | Loss: 0.00001096
Iteration 39/1000 | Loss: 0.00001093
Iteration 40/1000 | Loss: 0.00001093
Iteration 41/1000 | Loss: 0.00001092
Iteration 42/1000 | Loss: 0.00001092
Iteration 43/1000 | Loss: 0.00001091
Iteration 44/1000 | Loss: 0.00001089
Iteration 45/1000 | Loss: 0.00001088
Iteration 46/1000 | Loss: 0.00001088
Iteration 47/1000 | Loss: 0.00001088
Iteration 48/1000 | Loss: 0.00001087
Iteration 49/1000 | Loss: 0.00001087
Iteration 50/1000 | Loss: 0.00001086
Iteration 51/1000 | Loss: 0.00001086
Iteration 52/1000 | Loss: 0.00001086
Iteration 53/1000 | Loss: 0.00001086
Iteration 54/1000 | Loss: 0.00001085
Iteration 55/1000 | Loss: 0.00001085
Iteration 56/1000 | Loss: 0.00001084
Iteration 57/1000 | Loss: 0.00001084
Iteration 58/1000 | Loss: 0.00001084
Iteration 59/1000 | Loss: 0.00001083
Iteration 60/1000 | Loss: 0.00001083
Iteration 61/1000 | Loss: 0.00001082
Iteration 62/1000 | Loss: 0.00001082
Iteration 63/1000 | Loss: 0.00001082
Iteration 64/1000 | Loss: 0.00001081
Iteration 65/1000 | Loss: 0.00001081
Iteration 66/1000 | Loss: 0.00001080
Iteration 67/1000 | Loss: 0.00001080
Iteration 68/1000 | Loss: 0.00001080
Iteration 69/1000 | Loss: 0.00001080
Iteration 70/1000 | Loss: 0.00001079
Iteration 71/1000 | Loss: 0.00001079
Iteration 72/1000 | Loss: 0.00001078
Iteration 73/1000 | Loss: 0.00001078
Iteration 74/1000 | Loss: 0.00001078
Iteration 75/1000 | Loss: 0.00001078
Iteration 76/1000 | Loss: 0.00001078
Iteration 77/1000 | Loss: 0.00001077
Iteration 78/1000 | Loss: 0.00001077
Iteration 79/1000 | Loss: 0.00001077
Iteration 80/1000 | Loss: 0.00001077
Iteration 81/1000 | Loss: 0.00001076
Iteration 82/1000 | Loss: 0.00001076
Iteration 83/1000 | Loss: 0.00001076
Iteration 84/1000 | Loss: 0.00001076
Iteration 85/1000 | Loss: 0.00001076
Iteration 86/1000 | Loss: 0.00001075
Iteration 87/1000 | Loss: 0.00001075
Iteration 88/1000 | Loss: 0.00001075
Iteration 89/1000 | Loss: 0.00001075
Iteration 90/1000 | Loss: 0.00001075
Iteration 91/1000 | Loss: 0.00001075
Iteration 92/1000 | Loss: 0.00001075
Iteration 93/1000 | Loss: 0.00001074
Iteration 94/1000 | Loss: 0.00001074
Iteration 95/1000 | Loss: 0.00001074
Iteration 96/1000 | Loss: 0.00001074
Iteration 97/1000 | Loss: 0.00001074
Iteration 98/1000 | Loss: 0.00001074
Iteration 99/1000 | Loss: 0.00001074
Iteration 100/1000 | Loss: 0.00001074
Iteration 101/1000 | Loss: 0.00001074
Iteration 102/1000 | Loss: 0.00001074
Iteration 103/1000 | Loss: 0.00001073
Iteration 104/1000 | Loss: 0.00001073
Iteration 105/1000 | Loss: 0.00001073
Iteration 106/1000 | Loss: 0.00001073
Iteration 107/1000 | Loss: 0.00001073
Iteration 108/1000 | Loss: 0.00001073
Iteration 109/1000 | Loss: 0.00001073
Iteration 110/1000 | Loss: 0.00001073
Iteration 111/1000 | Loss: 0.00001073
Iteration 112/1000 | Loss: 0.00001072
Iteration 113/1000 | Loss: 0.00001072
Iteration 114/1000 | Loss: 0.00001072
Iteration 115/1000 | Loss: 0.00001072
Iteration 116/1000 | Loss: 0.00001072
Iteration 117/1000 | Loss: 0.00001072
Iteration 118/1000 | Loss: 0.00001072
Iteration 119/1000 | Loss: 0.00001072
Iteration 120/1000 | Loss: 0.00001072
Iteration 121/1000 | Loss: 0.00001072
Iteration 122/1000 | Loss: 0.00001072
Iteration 123/1000 | Loss: 0.00001072
Iteration 124/1000 | Loss: 0.00001072
Iteration 125/1000 | Loss: 0.00001072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.072065242624376e-05, 1.072065242624376e-05, 1.072065242624376e-05, 1.072065242624376e-05, 1.072065242624376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.072065242624376e-05

Optimization complete. Final v2v error: 2.749272346496582 mm

Highest mean error: 2.9578404426574707 mm for frame 12

Lowest mean error: 2.5628249645233154 mm for frame 133

Saving results

Total time: 30.76421594619751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019416
Iteration 2/25 | Loss: 0.00100973
Iteration 3/25 | Loss: 0.00080529
Iteration 4/25 | Loss: 0.00076873
Iteration 5/25 | Loss: 0.00075832
Iteration 6/25 | Loss: 0.00075694
Iteration 7/25 | Loss: 0.00075656
Iteration 8/25 | Loss: 0.00075656
Iteration 9/25 | Loss: 0.00075656
Iteration 10/25 | Loss: 0.00075656
Iteration 11/25 | Loss: 0.00075656
Iteration 12/25 | Loss: 0.00075656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007565644336864352, 0.0007565644336864352, 0.0007565644336864352, 0.0007565644336864352, 0.0007565644336864352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007565644336864352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.70010138
Iteration 2/25 | Loss: 0.00037486
Iteration 3/25 | Loss: 0.00037484
Iteration 4/25 | Loss: 0.00037484
Iteration 5/25 | Loss: 0.00037484
Iteration 6/25 | Loss: 0.00037484
Iteration 7/25 | Loss: 0.00037484
Iteration 8/25 | Loss: 0.00037484
Iteration 9/25 | Loss: 0.00037484
Iteration 10/25 | Loss: 0.00037484
Iteration 11/25 | Loss: 0.00037484
Iteration 12/25 | Loss: 0.00037484
Iteration 13/25 | Loss: 0.00037484
Iteration 14/25 | Loss: 0.00037484
Iteration 15/25 | Loss: 0.00037484
Iteration 16/25 | Loss: 0.00037484
Iteration 17/25 | Loss: 0.00037484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00037483920459635556, 0.00037483920459635556, 0.00037483920459635556, 0.00037483920459635556, 0.00037483920459635556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037483920459635556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037484
Iteration 2/1000 | Loss: 0.00004111
Iteration 3/1000 | Loss: 0.00002594
Iteration 4/1000 | Loss: 0.00002330
Iteration 5/1000 | Loss: 0.00002235
Iteration 6/1000 | Loss: 0.00002178
Iteration 7/1000 | Loss: 0.00002141
Iteration 8/1000 | Loss: 0.00002114
Iteration 9/1000 | Loss: 0.00002094
Iteration 10/1000 | Loss: 0.00002093
Iteration 11/1000 | Loss: 0.00002080
Iteration 12/1000 | Loss: 0.00002080
Iteration 13/1000 | Loss: 0.00002080
Iteration 14/1000 | Loss: 0.00002078
Iteration 15/1000 | Loss: 0.00002077
Iteration 16/1000 | Loss: 0.00002075
Iteration 17/1000 | Loss: 0.00002074
Iteration 18/1000 | Loss: 0.00002074
Iteration 19/1000 | Loss: 0.00002073
Iteration 20/1000 | Loss: 0.00002073
Iteration 21/1000 | Loss: 0.00002068
Iteration 22/1000 | Loss: 0.00002065
Iteration 23/1000 | Loss: 0.00002064
Iteration 24/1000 | Loss: 0.00002064
Iteration 25/1000 | Loss: 0.00002063
Iteration 26/1000 | Loss: 0.00002063
Iteration 27/1000 | Loss: 0.00002062
Iteration 28/1000 | Loss: 0.00002061
Iteration 29/1000 | Loss: 0.00002060
Iteration 30/1000 | Loss: 0.00002056
Iteration 31/1000 | Loss: 0.00002056
Iteration 32/1000 | Loss: 0.00002056
Iteration 33/1000 | Loss: 0.00002055
Iteration 34/1000 | Loss: 0.00002055
Iteration 35/1000 | Loss: 0.00002055
Iteration 36/1000 | Loss: 0.00002054
Iteration 37/1000 | Loss: 0.00002054
Iteration 38/1000 | Loss: 0.00002053
Iteration 39/1000 | Loss: 0.00002051
Iteration 40/1000 | Loss: 0.00002050
Iteration 41/1000 | Loss: 0.00002049
Iteration 42/1000 | Loss: 0.00002048
Iteration 43/1000 | Loss: 0.00002048
Iteration 44/1000 | Loss: 0.00002047
Iteration 45/1000 | Loss: 0.00002047
Iteration 46/1000 | Loss: 0.00002047
Iteration 47/1000 | Loss: 0.00002047
Iteration 48/1000 | Loss: 0.00002046
Iteration 49/1000 | Loss: 0.00002046
Iteration 50/1000 | Loss: 0.00002046
Iteration 51/1000 | Loss: 0.00002046
Iteration 52/1000 | Loss: 0.00002046
Iteration 53/1000 | Loss: 0.00002046
Iteration 54/1000 | Loss: 0.00002046
Iteration 55/1000 | Loss: 0.00002046
Iteration 56/1000 | Loss: 0.00002046
Iteration 57/1000 | Loss: 0.00002046
Iteration 58/1000 | Loss: 0.00002046
Iteration 59/1000 | Loss: 0.00002046
Iteration 60/1000 | Loss: 0.00002046
Iteration 61/1000 | Loss: 0.00002046
Iteration 62/1000 | Loss: 0.00002046
Iteration 63/1000 | Loss: 0.00002046
Iteration 64/1000 | Loss: 0.00002046
Iteration 65/1000 | Loss: 0.00002046
Iteration 66/1000 | Loss: 0.00002046
Iteration 67/1000 | Loss: 0.00002044
Iteration 68/1000 | Loss: 0.00002043
Iteration 69/1000 | Loss: 0.00002043
Iteration 70/1000 | Loss: 0.00002043
Iteration 71/1000 | Loss: 0.00002043
Iteration 72/1000 | Loss: 0.00002043
Iteration 73/1000 | Loss: 0.00002043
Iteration 74/1000 | Loss: 0.00002043
Iteration 75/1000 | Loss: 0.00002043
Iteration 76/1000 | Loss: 0.00002043
Iteration 77/1000 | Loss: 0.00002043
Iteration 78/1000 | Loss: 0.00002043
Iteration 79/1000 | Loss: 0.00002042
Iteration 80/1000 | Loss: 0.00002042
Iteration 81/1000 | Loss: 0.00002042
Iteration 82/1000 | Loss: 0.00002041
Iteration 83/1000 | Loss: 0.00002040
Iteration 84/1000 | Loss: 0.00002039
Iteration 85/1000 | Loss: 0.00002039
Iteration 86/1000 | Loss: 0.00002038
Iteration 87/1000 | Loss: 0.00002037
Iteration 88/1000 | Loss: 0.00002037
Iteration 89/1000 | Loss: 0.00002037
Iteration 90/1000 | Loss: 0.00002037
Iteration 91/1000 | Loss: 0.00002036
Iteration 92/1000 | Loss: 0.00002036
Iteration 93/1000 | Loss: 0.00002036
Iteration 94/1000 | Loss: 0.00002036
Iteration 95/1000 | Loss: 0.00002036
Iteration 96/1000 | Loss: 0.00002035
Iteration 97/1000 | Loss: 0.00002035
Iteration 98/1000 | Loss: 0.00002035
Iteration 99/1000 | Loss: 0.00002035
Iteration 100/1000 | Loss: 0.00002035
Iteration 101/1000 | Loss: 0.00002034
Iteration 102/1000 | Loss: 0.00002034
Iteration 103/1000 | Loss: 0.00002034
Iteration 104/1000 | Loss: 0.00002034
Iteration 105/1000 | Loss: 0.00002033
Iteration 106/1000 | Loss: 0.00002033
Iteration 107/1000 | Loss: 0.00002033
Iteration 108/1000 | Loss: 0.00002033
Iteration 109/1000 | Loss: 0.00002033
Iteration 110/1000 | Loss: 0.00002032
Iteration 111/1000 | Loss: 0.00002032
Iteration 112/1000 | Loss: 0.00002032
Iteration 113/1000 | Loss: 0.00002032
Iteration 114/1000 | Loss: 0.00002032
Iteration 115/1000 | Loss: 0.00002032
Iteration 116/1000 | Loss: 0.00002032
Iteration 117/1000 | Loss: 0.00002032
Iteration 118/1000 | Loss: 0.00002032
Iteration 119/1000 | Loss: 0.00002032
Iteration 120/1000 | Loss: 0.00002032
Iteration 121/1000 | Loss: 0.00002032
Iteration 122/1000 | Loss: 0.00002031
Iteration 123/1000 | Loss: 0.00002031
Iteration 124/1000 | Loss: 0.00002031
Iteration 125/1000 | Loss: 0.00002031
Iteration 126/1000 | Loss: 0.00002031
Iteration 127/1000 | Loss: 0.00002031
Iteration 128/1000 | Loss: 0.00002031
Iteration 129/1000 | Loss: 0.00002031
Iteration 130/1000 | Loss: 0.00002031
Iteration 131/1000 | Loss: 0.00002031
Iteration 132/1000 | Loss: 0.00002031
Iteration 133/1000 | Loss: 0.00002030
Iteration 134/1000 | Loss: 0.00002030
Iteration 135/1000 | Loss: 0.00002030
Iteration 136/1000 | Loss: 0.00002030
Iteration 137/1000 | Loss: 0.00002030
Iteration 138/1000 | Loss: 0.00002030
Iteration 139/1000 | Loss: 0.00002030
Iteration 140/1000 | Loss: 0.00002030
Iteration 141/1000 | Loss: 0.00002029
Iteration 142/1000 | Loss: 0.00002029
Iteration 143/1000 | Loss: 0.00002029
Iteration 144/1000 | Loss: 0.00002029
Iteration 145/1000 | Loss: 0.00002029
Iteration 146/1000 | Loss: 0.00002029
Iteration 147/1000 | Loss: 0.00002029
Iteration 148/1000 | Loss: 0.00002029
Iteration 149/1000 | Loss: 0.00002028
Iteration 150/1000 | Loss: 0.00002028
Iteration 151/1000 | Loss: 0.00002028
Iteration 152/1000 | Loss: 0.00002028
Iteration 153/1000 | Loss: 0.00002028
Iteration 154/1000 | Loss: 0.00002028
Iteration 155/1000 | Loss: 0.00002028
Iteration 156/1000 | Loss: 0.00002028
Iteration 157/1000 | Loss: 0.00002028
Iteration 158/1000 | Loss: 0.00002028
Iteration 159/1000 | Loss: 0.00002028
Iteration 160/1000 | Loss: 0.00002028
Iteration 161/1000 | Loss: 0.00002028
Iteration 162/1000 | Loss: 0.00002028
Iteration 163/1000 | Loss: 0.00002028
Iteration 164/1000 | Loss: 0.00002028
Iteration 165/1000 | Loss: 0.00002028
Iteration 166/1000 | Loss: 0.00002028
Iteration 167/1000 | Loss: 0.00002028
Iteration 168/1000 | Loss: 0.00002028
Iteration 169/1000 | Loss: 0.00002028
Iteration 170/1000 | Loss: 0.00002028
Iteration 171/1000 | Loss: 0.00002028
Iteration 172/1000 | Loss: 0.00002028
Iteration 173/1000 | Loss: 0.00002028
Iteration 174/1000 | Loss: 0.00002028
Iteration 175/1000 | Loss: 0.00002028
Iteration 176/1000 | Loss: 0.00002028
Iteration 177/1000 | Loss: 0.00002028
Iteration 178/1000 | Loss: 0.00002028
Iteration 179/1000 | Loss: 0.00002028
Iteration 180/1000 | Loss: 0.00002028
Iteration 181/1000 | Loss: 0.00002028
Iteration 182/1000 | Loss: 0.00002028
Iteration 183/1000 | Loss: 0.00002028
Iteration 184/1000 | Loss: 0.00002028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.027990012720693e-05, 2.027990012720693e-05, 2.027990012720693e-05, 2.027990012720693e-05, 2.027990012720693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.027990012720693e-05

Optimization complete. Final v2v error: 3.824897050857544 mm

Highest mean error: 4.183987140655518 mm for frame 52

Lowest mean error: 3.5342395305633545 mm for frame 121

Saving results

Total time: 36.53287220001221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907813
Iteration 2/25 | Loss: 0.00088142
Iteration 3/25 | Loss: 0.00063995
Iteration 4/25 | Loss: 0.00060624
Iteration 5/25 | Loss: 0.00059590
Iteration 6/25 | Loss: 0.00059377
Iteration 7/25 | Loss: 0.00059318
Iteration 8/25 | Loss: 0.00059318
Iteration 9/25 | Loss: 0.00059318
Iteration 10/25 | Loss: 0.00059318
Iteration 11/25 | Loss: 0.00059318
Iteration 12/25 | Loss: 0.00059318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005931846681050956, 0.0005931846681050956, 0.0005931846681050956, 0.0005931846681050956, 0.0005931846681050956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005931846681050956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43158293
Iteration 2/25 | Loss: 0.00020568
Iteration 3/25 | Loss: 0.00020567
Iteration 4/25 | Loss: 0.00020567
Iteration 5/25 | Loss: 0.00020567
Iteration 6/25 | Loss: 0.00020567
Iteration 7/25 | Loss: 0.00020567
Iteration 8/25 | Loss: 0.00020567
Iteration 9/25 | Loss: 0.00020567
Iteration 10/25 | Loss: 0.00020567
Iteration 11/25 | Loss: 0.00020567
Iteration 12/25 | Loss: 0.00020567
Iteration 13/25 | Loss: 0.00020567
Iteration 14/25 | Loss: 0.00020567
Iteration 15/25 | Loss: 0.00020567
Iteration 16/25 | Loss: 0.00020567
Iteration 17/25 | Loss: 0.00020567
Iteration 18/25 | Loss: 0.00020567
Iteration 19/25 | Loss: 0.00020567
Iteration 20/25 | Loss: 0.00020567
Iteration 21/25 | Loss: 0.00020567
Iteration 22/25 | Loss: 0.00020567
Iteration 23/25 | Loss: 0.00020567
Iteration 24/25 | Loss: 0.00020567
Iteration 25/25 | Loss: 0.00020567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020567
Iteration 2/1000 | Loss: 0.00002389
Iteration 3/1000 | Loss: 0.00001508
Iteration 4/1000 | Loss: 0.00001317
Iteration 5/1000 | Loss: 0.00001252
Iteration 6/1000 | Loss: 0.00001206
Iteration 7/1000 | Loss: 0.00001176
Iteration 8/1000 | Loss: 0.00001154
Iteration 9/1000 | Loss: 0.00001145
Iteration 10/1000 | Loss: 0.00001142
Iteration 11/1000 | Loss: 0.00001142
Iteration 12/1000 | Loss: 0.00001141
Iteration 13/1000 | Loss: 0.00001141
Iteration 14/1000 | Loss: 0.00001140
Iteration 15/1000 | Loss: 0.00001135
Iteration 16/1000 | Loss: 0.00001131
Iteration 17/1000 | Loss: 0.00001130
Iteration 18/1000 | Loss: 0.00001129
Iteration 19/1000 | Loss: 0.00001128
Iteration 20/1000 | Loss: 0.00001128
Iteration 21/1000 | Loss: 0.00001127
Iteration 22/1000 | Loss: 0.00001126
Iteration 23/1000 | Loss: 0.00001125
Iteration 24/1000 | Loss: 0.00001125
Iteration 25/1000 | Loss: 0.00001125
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001124
Iteration 28/1000 | Loss: 0.00001123
Iteration 29/1000 | Loss: 0.00001123
Iteration 30/1000 | Loss: 0.00001123
Iteration 31/1000 | Loss: 0.00001123
Iteration 32/1000 | Loss: 0.00001123
Iteration 33/1000 | Loss: 0.00001123
Iteration 34/1000 | Loss: 0.00001123
Iteration 35/1000 | Loss: 0.00001123
Iteration 36/1000 | Loss: 0.00001123
Iteration 37/1000 | Loss: 0.00001123
Iteration 38/1000 | Loss: 0.00001123
Iteration 39/1000 | Loss: 0.00001123
Iteration 40/1000 | Loss: 0.00001122
Iteration 41/1000 | Loss: 0.00001122
Iteration 42/1000 | Loss: 0.00001122
Iteration 43/1000 | Loss: 0.00001122
Iteration 44/1000 | Loss: 0.00001122
Iteration 45/1000 | Loss: 0.00001121
Iteration 46/1000 | Loss: 0.00001121
Iteration 47/1000 | Loss: 0.00001120
Iteration 48/1000 | Loss: 0.00001119
Iteration 49/1000 | Loss: 0.00001119
Iteration 50/1000 | Loss: 0.00001119
Iteration 51/1000 | Loss: 0.00001119
Iteration 52/1000 | Loss: 0.00001118
Iteration 53/1000 | Loss: 0.00001118
Iteration 54/1000 | Loss: 0.00001117
Iteration 55/1000 | Loss: 0.00001117
Iteration 56/1000 | Loss: 0.00001116
Iteration 57/1000 | Loss: 0.00001116
Iteration 58/1000 | Loss: 0.00001116
Iteration 59/1000 | Loss: 0.00001115
Iteration 60/1000 | Loss: 0.00001115
Iteration 61/1000 | Loss: 0.00001115
Iteration 62/1000 | Loss: 0.00001114
Iteration 63/1000 | Loss: 0.00001114
Iteration 64/1000 | Loss: 0.00001114
Iteration 65/1000 | Loss: 0.00001114
Iteration 66/1000 | Loss: 0.00001114
Iteration 67/1000 | Loss: 0.00001113
Iteration 68/1000 | Loss: 0.00001112
Iteration 69/1000 | Loss: 0.00001111
Iteration 70/1000 | Loss: 0.00001111
Iteration 71/1000 | Loss: 0.00001110
Iteration 72/1000 | Loss: 0.00001109
Iteration 73/1000 | Loss: 0.00001108
Iteration 74/1000 | Loss: 0.00001108
Iteration 75/1000 | Loss: 0.00001108
Iteration 76/1000 | Loss: 0.00001108
Iteration 77/1000 | Loss: 0.00001107
Iteration 78/1000 | Loss: 0.00001107
Iteration 79/1000 | Loss: 0.00001107
Iteration 80/1000 | Loss: 0.00001106
Iteration 81/1000 | Loss: 0.00001106
Iteration 82/1000 | Loss: 0.00001106
Iteration 83/1000 | Loss: 0.00001105
Iteration 84/1000 | Loss: 0.00001105
Iteration 85/1000 | Loss: 0.00001105
Iteration 86/1000 | Loss: 0.00001104
Iteration 87/1000 | Loss: 0.00001104
Iteration 88/1000 | Loss: 0.00001104
Iteration 89/1000 | Loss: 0.00001104
Iteration 90/1000 | Loss: 0.00001104
Iteration 91/1000 | Loss: 0.00001103
Iteration 92/1000 | Loss: 0.00001103
Iteration 93/1000 | Loss: 0.00001103
Iteration 94/1000 | Loss: 0.00001103
Iteration 95/1000 | Loss: 0.00001103
Iteration 96/1000 | Loss: 0.00001103
Iteration 97/1000 | Loss: 0.00001102
Iteration 98/1000 | Loss: 0.00001102
Iteration 99/1000 | Loss: 0.00001102
Iteration 100/1000 | Loss: 0.00001102
Iteration 101/1000 | Loss: 0.00001102
Iteration 102/1000 | Loss: 0.00001102
Iteration 103/1000 | Loss: 0.00001101
Iteration 104/1000 | Loss: 0.00001101
Iteration 105/1000 | Loss: 0.00001101
Iteration 106/1000 | Loss: 0.00001101
Iteration 107/1000 | Loss: 0.00001101
Iteration 108/1000 | Loss: 0.00001101
Iteration 109/1000 | Loss: 0.00001101
Iteration 110/1000 | Loss: 0.00001101
Iteration 111/1000 | Loss: 0.00001101
Iteration 112/1000 | Loss: 0.00001100
Iteration 113/1000 | Loss: 0.00001100
Iteration 114/1000 | Loss: 0.00001100
Iteration 115/1000 | Loss: 0.00001100
Iteration 116/1000 | Loss: 0.00001100
Iteration 117/1000 | Loss: 0.00001100
Iteration 118/1000 | Loss: 0.00001100
Iteration 119/1000 | Loss: 0.00001099
Iteration 120/1000 | Loss: 0.00001099
Iteration 121/1000 | Loss: 0.00001099
Iteration 122/1000 | Loss: 0.00001099
Iteration 123/1000 | Loss: 0.00001099
Iteration 124/1000 | Loss: 0.00001099
Iteration 125/1000 | Loss: 0.00001099
Iteration 126/1000 | Loss: 0.00001099
Iteration 127/1000 | Loss: 0.00001099
Iteration 128/1000 | Loss: 0.00001099
Iteration 129/1000 | Loss: 0.00001099
Iteration 130/1000 | Loss: 0.00001099
Iteration 131/1000 | Loss: 0.00001098
Iteration 132/1000 | Loss: 0.00001098
Iteration 133/1000 | Loss: 0.00001098
Iteration 134/1000 | Loss: 0.00001098
Iteration 135/1000 | Loss: 0.00001098
Iteration 136/1000 | Loss: 0.00001098
Iteration 137/1000 | Loss: 0.00001098
Iteration 138/1000 | Loss: 0.00001097
Iteration 139/1000 | Loss: 0.00001097
Iteration 140/1000 | Loss: 0.00001097
Iteration 141/1000 | Loss: 0.00001097
Iteration 142/1000 | Loss: 0.00001097
Iteration 143/1000 | Loss: 0.00001097
Iteration 144/1000 | Loss: 0.00001097
Iteration 145/1000 | Loss: 0.00001097
Iteration 146/1000 | Loss: 0.00001097
Iteration 147/1000 | Loss: 0.00001097
Iteration 148/1000 | Loss: 0.00001097
Iteration 149/1000 | Loss: 0.00001097
Iteration 150/1000 | Loss: 0.00001096
Iteration 151/1000 | Loss: 0.00001096
Iteration 152/1000 | Loss: 0.00001096
Iteration 153/1000 | Loss: 0.00001096
Iteration 154/1000 | Loss: 0.00001096
Iteration 155/1000 | Loss: 0.00001096
Iteration 156/1000 | Loss: 0.00001096
Iteration 157/1000 | Loss: 0.00001095
Iteration 158/1000 | Loss: 0.00001095
Iteration 159/1000 | Loss: 0.00001095
Iteration 160/1000 | Loss: 0.00001095
Iteration 161/1000 | Loss: 0.00001095
Iteration 162/1000 | Loss: 0.00001094
Iteration 163/1000 | Loss: 0.00001094
Iteration 164/1000 | Loss: 0.00001094
Iteration 165/1000 | Loss: 0.00001094
Iteration 166/1000 | Loss: 0.00001094
Iteration 167/1000 | Loss: 0.00001094
Iteration 168/1000 | Loss: 0.00001094
Iteration 169/1000 | Loss: 0.00001094
Iteration 170/1000 | Loss: 0.00001094
Iteration 171/1000 | Loss: 0.00001093
Iteration 172/1000 | Loss: 0.00001093
Iteration 173/1000 | Loss: 0.00001093
Iteration 174/1000 | Loss: 0.00001093
Iteration 175/1000 | Loss: 0.00001093
Iteration 176/1000 | Loss: 0.00001093
Iteration 177/1000 | Loss: 0.00001093
Iteration 178/1000 | Loss: 0.00001093
Iteration 179/1000 | Loss: 0.00001093
Iteration 180/1000 | Loss: 0.00001093
Iteration 181/1000 | Loss: 0.00001093
Iteration 182/1000 | Loss: 0.00001093
Iteration 183/1000 | Loss: 0.00001093
Iteration 184/1000 | Loss: 0.00001092
Iteration 185/1000 | Loss: 0.00001092
Iteration 186/1000 | Loss: 0.00001092
Iteration 187/1000 | Loss: 0.00001092
Iteration 188/1000 | Loss: 0.00001092
Iteration 189/1000 | Loss: 0.00001092
Iteration 190/1000 | Loss: 0.00001092
Iteration 191/1000 | Loss: 0.00001092
Iteration 192/1000 | Loss: 0.00001092
Iteration 193/1000 | Loss: 0.00001092
Iteration 194/1000 | Loss: 0.00001092
Iteration 195/1000 | Loss: 0.00001092
Iteration 196/1000 | Loss: 0.00001092
Iteration 197/1000 | Loss: 0.00001092
Iteration 198/1000 | Loss: 0.00001092
Iteration 199/1000 | Loss: 0.00001092
Iteration 200/1000 | Loss: 0.00001092
Iteration 201/1000 | Loss: 0.00001092
Iteration 202/1000 | Loss: 0.00001092
Iteration 203/1000 | Loss: 0.00001092
Iteration 204/1000 | Loss: 0.00001092
Iteration 205/1000 | Loss: 0.00001092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.0920683962467592e-05, 1.0920683962467592e-05, 1.0920683962467592e-05, 1.0920683962467592e-05, 1.0920683962467592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0920683962467592e-05

Optimization complete. Final v2v error: 2.8113300800323486 mm

Highest mean error: 3.6667659282684326 mm for frame 59

Lowest mean error: 2.6241002082824707 mm for frame 90

Saving results

Total time: 37.228976011276245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759879
Iteration 2/25 | Loss: 0.00090809
Iteration 3/25 | Loss: 0.00064689
Iteration 4/25 | Loss: 0.00061298
Iteration 5/25 | Loss: 0.00060449
Iteration 6/25 | Loss: 0.00060224
Iteration 7/25 | Loss: 0.00060172
Iteration 8/25 | Loss: 0.00060172
Iteration 9/25 | Loss: 0.00060172
Iteration 10/25 | Loss: 0.00060172
Iteration 11/25 | Loss: 0.00060172
Iteration 12/25 | Loss: 0.00060172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000601723266299814, 0.000601723266299814, 0.000601723266299814, 0.000601723266299814, 0.000601723266299814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000601723266299814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87830210
Iteration 2/25 | Loss: 0.00027683
Iteration 3/25 | Loss: 0.00027683
Iteration 4/25 | Loss: 0.00027683
Iteration 5/25 | Loss: 0.00027683
Iteration 6/25 | Loss: 0.00027683
Iteration 7/25 | Loss: 0.00027683
Iteration 8/25 | Loss: 0.00027683
Iteration 9/25 | Loss: 0.00027683
Iteration 10/25 | Loss: 0.00027683
Iteration 11/25 | Loss: 0.00027683
Iteration 12/25 | Loss: 0.00027683
Iteration 13/25 | Loss: 0.00027683
Iteration 14/25 | Loss: 0.00027683
Iteration 15/25 | Loss: 0.00027683
Iteration 16/25 | Loss: 0.00027683
Iteration 17/25 | Loss: 0.00027683
Iteration 18/25 | Loss: 0.00027683
Iteration 19/25 | Loss: 0.00027683
Iteration 20/25 | Loss: 0.00027683
Iteration 21/25 | Loss: 0.00027683
Iteration 22/25 | Loss: 0.00027683
Iteration 23/25 | Loss: 0.00027683
Iteration 24/25 | Loss: 0.00027683
Iteration 25/25 | Loss: 0.00027683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027683
Iteration 2/1000 | Loss: 0.00002186
Iteration 3/1000 | Loss: 0.00001607
Iteration 4/1000 | Loss: 0.00001512
Iteration 5/1000 | Loss: 0.00001433
Iteration 6/1000 | Loss: 0.00001394
Iteration 7/1000 | Loss: 0.00001365
Iteration 8/1000 | Loss: 0.00001355
Iteration 9/1000 | Loss: 0.00001339
Iteration 10/1000 | Loss: 0.00001329
Iteration 11/1000 | Loss: 0.00001328
Iteration 12/1000 | Loss: 0.00001328
Iteration 13/1000 | Loss: 0.00001324
Iteration 14/1000 | Loss: 0.00001324
Iteration 15/1000 | Loss: 0.00001324
Iteration 16/1000 | Loss: 0.00001323
Iteration 17/1000 | Loss: 0.00001322
Iteration 18/1000 | Loss: 0.00001317
Iteration 19/1000 | Loss: 0.00001316
Iteration 20/1000 | Loss: 0.00001316
Iteration 21/1000 | Loss: 0.00001316
Iteration 22/1000 | Loss: 0.00001315
Iteration 23/1000 | Loss: 0.00001311
Iteration 24/1000 | Loss: 0.00001310
Iteration 25/1000 | Loss: 0.00001310
Iteration 26/1000 | Loss: 0.00001309
Iteration 27/1000 | Loss: 0.00001309
Iteration 28/1000 | Loss: 0.00001306
Iteration 29/1000 | Loss: 0.00001306
Iteration 30/1000 | Loss: 0.00001304
Iteration 31/1000 | Loss: 0.00001303
Iteration 32/1000 | Loss: 0.00001302
Iteration 33/1000 | Loss: 0.00001299
Iteration 34/1000 | Loss: 0.00001299
Iteration 35/1000 | Loss: 0.00001299
Iteration 36/1000 | Loss: 0.00001299
Iteration 37/1000 | Loss: 0.00001299
Iteration 38/1000 | Loss: 0.00001299
Iteration 39/1000 | Loss: 0.00001299
Iteration 40/1000 | Loss: 0.00001299
Iteration 41/1000 | Loss: 0.00001299
Iteration 42/1000 | Loss: 0.00001299
Iteration 43/1000 | Loss: 0.00001298
Iteration 44/1000 | Loss: 0.00001298
Iteration 45/1000 | Loss: 0.00001298
Iteration 46/1000 | Loss: 0.00001298
Iteration 47/1000 | Loss: 0.00001298
Iteration 48/1000 | Loss: 0.00001298
Iteration 49/1000 | Loss: 0.00001298
Iteration 50/1000 | Loss: 0.00001297
Iteration 51/1000 | Loss: 0.00001297
Iteration 52/1000 | Loss: 0.00001297
Iteration 53/1000 | Loss: 0.00001296
Iteration 54/1000 | Loss: 0.00001296
Iteration 55/1000 | Loss: 0.00001296
Iteration 56/1000 | Loss: 0.00001296
Iteration 57/1000 | Loss: 0.00001296
Iteration 58/1000 | Loss: 0.00001296
Iteration 59/1000 | Loss: 0.00001296
Iteration 60/1000 | Loss: 0.00001295
Iteration 61/1000 | Loss: 0.00001295
Iteration 62/1000 | Loss: 0.00001294
Iteration 63/1000 | Loss: 0.00001294
Iteration 64/1000 | Loss: 0.00001294
Iteration 65/1000 | Loss: 0.00001294
Iteration 66/1000 | Loss: 0.00001293
Iteration 67/1000 | Loss: 0.00001293
Iteration 68/1000 | Loss: 0.00001293
Iteration 69/1000 | Loss: 0.00001292
Iteration 70/1000 | Loss: 0.00001292
Iteration 71/1000 | Loss: 0.00001291
Iteration 72/1000 | Loss: 0.00001291
Iteration 73/1000 | Loss: 0.00001290
Iteration 74/1000 | Loss: 0.00001290
Iteration 75/1000 | Loss: 0.00001289
Iteration 76/1000 | Loss: 0.00001289
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001286
Iteration 79/1000 | Loss: 0.00001286
Iteration 80/1000 | Loss: 0.00001286
Iteration 81/1000 | Loss: 0.00001286
Iteration 82/1000 | Loss: 0.00001286
Iteration 83/1000 | Loss: 0.00001286
Iteration 84/1000 | Loss: 0.00001286
Iteration 85/1000 | Loss: 0.00001285
Iteration 86/1000 | Loss: 0.00001285
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001282
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001280
Iteration 93/1000 | Loss: 0.00001280
Iteration 94/1000 | Loss: 0.00001280
Iteration 95/1000 | Loss: 0.00001280
Iteration 96/1000 | Loss: 0.00001280
Iteration 97/1000 | Loss: 0.00001280
Iteration 98/1000 | Loss: 0.00001280
Iteration 99/1000 | Loss: 0.00001279
Iteration 100/1000 | Loss: 0.00001279
Iteration 101/1000 | Loss: 0.00001279
Iteration 102/1000 | Loss: 0.00001279
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001278
Iteration 105/1000 | Loss: 0.00001278
Iteration 106/1000 | Loss: 0.00001278
Iteration 107/1000 | Loss: 0.00001278
Iteration 108/1000 | Loss: 0.00001278
Iteration 109/1000 | Loss: 0.00001278
Iteration 110/1000 | Loss: 0.00001278
Iteration 111/1000 | Loss: 0.00001278
Iteration 112/1000 | Loss: 0.00001278
Iteration 113/1000 | Loss: 0.00001278
Iteration 114/1000 | Loss: 0.00001278
Iteration 115/1000 | Loss: 0.00001278
Iteration 116/1000 | Loss: 0.00001278
Iteration 117/1000 | Loss: 0.00001278
Iteration 118/1000 | Loss: 0.00001278
Iteration 119/1000 | Loss: 0.00001277
Iteration 120/1000 | Loss: 0.00001277
Iteration 121/1000 | Loss: 0.00001277
Iteration 122/1000 | Loss: 0.00001277
Iteration 123/1000 | Loss: 0.00001277
Iteration 124/1000 | Loss: 0.00001277
Iteration 125/1000 | Loss: 0.00001277
Iteration 126/1000 | Loss: 0.00001277
Iteration 127/1000 | Loss: 0.00001276
Iteration 128/1000 | Loss: 0.00001276
Iteration 129/1000 | Loss: 0.00001276
Iteration 130/1000 | Loss: 0.00001276
Iteration 131/1000 | Loss: 0.00001276
Iteration 132/1000 | Loss: 0.00001276
Iteration 133/1000 | Loss: 0.00001276
Iteration 134/1000 | Loss: 0.00001276
Iteration 135/1000 | Loss: 0.00001276
Iteration 136/1000 | Loss: 0.00001276
Iteration 137/1000 | Loss: 0.00001276
Iteration 138/1000 | Loss: 0.00001276
Iteration 139/1000 | Loss: 0.00001276
Iteration 140/1000 | Loss: 0.00001276
Iteration 141/1000 | Loss: 0.00001276
Iteration 142/1000 | Loss: 0.00001275
Iteration 143/1000 | Loss: 0.00001275
Iteration 144/1000 | Loss: 0.00001275
Iteration 145/1000 | Loss: 0.00001275
Iteration 146/1000 | Loss: 0.00001275
Iteration 147/1000 | Loss: 0.00001275
Iteration 148/1000 | Loss: 0.00001275
Iteration 149/1000 | Loss: 0.00001275
Iteration 150/1000 | Loss: 0.00001275
Iteration 151/1000 | Loss: 0.00001275
Iteration 152/1000 | Loss: 0.00001275
Iteration 153/1000 | Loss: 0.00001275
Iteration 154/1000 | Loss: 0.00001275
Iteration 155/1000 | Loss: 0.00001274
Iteration 156/1000 | Loss: 0.00001274
Iteration 157/1000 | Loss: 0.00001274
Iteration 158/1000 | Loss: 0.00001274
Iteration 159/1000 | Loss: 0.00001274
Iteration 160/1000 | Loss: 0.00001274
Iteration 161/1000 | Loss: 0.00001274
Iteration 162/1000 | Loss: 0.00001274
Iteration 163/1000 | Loss: 0.00001274
Iteration 164/1000 | Loss: 0.00001274
Iteration 165/1000 | Loss: 0.00001274
Iteration 166/1000 | Loss: 0.00001274
Iteration 167/1000 | Loss: 0.00001273
Iteration 168/1000 | Loss: 0.00001273
Iteration 169/1000 | Loss: 0.00001273
Iteration 170/1000 | Loss: 0.00001273
Iteration 171/1000 | Loss: 0.00001273
Iteration 172/1000 | Loss: 0.00001273
Iteration 173/1000 | Loss: 0.00001273
Iteration 174/1000 | Loss: 0.00001273
Iteration 175/1000 | Loss: 0.00001273
Iteration 176/1000 | Loss: 0.00001273
Iteration 177/1000 | Loss: 0.00001273
Iteration 178/1000 | Loss: 0.00001273
Iteration 179/1000 | Loss: 0.00001273
Iteration 180/1000 | Loss: 0.00001273
Iteration 181/1000 | Loss: 0.00001273
Iteration 182/1000 | Loss: 0.00001273
Iteration 183/1000 | Loss: 0.00001273
Iteration 184/1000 | Loss: 0.00001273
Iteration 185/1000 | Loss: 0.00001273
Iteration 186/1000 | Loss: 0.00001273
Iteration 187/1000 | Loss: 0.00001273
Iteration 188/1000 | Loss: 0.00001273
Iteration 189/1000 | Loss: 0.00001273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.272511508432217e-05, 1.272511508432217e-05, 1.272511508432217e-05, 1.272511508432217e-05, 1.272511508432217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.272511508432217e-05

Optimization complete. Final v2v error: 3.0425121784210205 mm

Highest mean error: 3.342548370361328 mm for frame 98

Lowest mean error: 2.8894317150115967 mm for frame 194

Saving results

Total time: 37.831080198287964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840351
Iteration 2/25 | Loss: 0.00102927
Iteration 3/25 | Loss: 0.00075178
Iteration 4/25 | Loss: 0.00069539
Iteration 5/25 | Loss: 0.00068077
Iteration 6/25 | Loss: 0.00067880
Iteration 7/25 | Loss: 0.00067860
Iteration 8/25 | Loss: 0.00067860
Iteration 9/25 | Loss: 0.00067860
Iteration 10/25 | Loss: 0.00067860
Iteration 11/25 | Loss: 0.00067860
Iteration 12/25 | Loss: 0.00067860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006785995210520923, 0.0006785995210520923, 0.0006785995210520923, 0.0006785995210520923, 0.0006785995210520923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006785995210520923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36932647
Iteration 2/25 | Loss: 0.00026281
Iteration 3/25 | Loss: 0.00026276
Iteration 4/25 | Loss: 0.00026276
Iteration 5/25 | Loss: 0.00026276
Iteration 6/25 | Loss: 0.00026276
Iteration 7/25 | Loss: 0.00026276
Iteration 8/25 | Loss: 0.00026276
Iteration 9/25 | Loss: 0.00026276
Iteration 10/25 | Loss: 0.00026276
Iteration 11/25 | Loss: 0.00026276
Iteration 12/25 | Loss: 0.00026276
Iteration 13/25 | Loss: 0.00026276
Iteration 14/25 | Loss: 0.00026276
Iteration 15/25 | Loss: 0.00026276
Iteration 16/25 | Loss: 0.00026276
Iteration 17/25 | Loss: 0.00026276
Iteration 18/25 | Loss: 0.00026276
Iteration 19/25 | Loss: 0.00026276
Iteration 20/25 | Loss: 0.00026276
Iteration 21/25 | Loss: 0.00026276
Iteration 22/25 | Loss: 0.00026276
Iteration 23/25 | Loss: 0.00026276
Iteration 24/25 | Loss: 0.00026276
Iteration 25/25 | Loss: 0.00026276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026276
Iteration 2/1000 | Loss: 0.00003341
Iteration 3/1000 | Loss: 0.00002456
Iteration 4/1000 | Loss: 0.00002250
Iteration 5/1000 | Loss: 0.00002155
Iteration 6/1000 | Loss: 0.00002070
Iteration 7/1000 | Loss: 0.00002009
Iteration 8/1000 | Loss: 0.00001967
Iteration 9/1000 | Loss: 0.00001948
Iteration 10/1000 | Loss: 0.00001930
Iteration 11/1000 | Loss: 0.00001917
Iteration 12/1000 | Loss: 0.00001916
Iteration 13/1000 | Loss: 0.00001914
Iteration 14/1000 | Loss: 0.00001913
Iteration 15/1000 | Loss: 0.00001902
Iteration 16/1000 | Loss: 0.00001890
Iteration 17/1000 | Loss: 0.00001890
Iteration 18/1000 | Loss: 0.00001888
Iteration 19/1000 | Loss: 0.00001888
Iteration 20/1000 | Loss: 0.00001888
Iteration 21/1000 | Loss: 0.00001887
Iteration 22/1000 | Loss: 0.00001886
Iteration 23/1000 | Loss: 0.00001886
Iteration 24/1000 | Loss: 0.00001885
Iteration 25/1000 | Loss: 0.00001883
Iteration 26/1000 | Loss: 0.00001882
Iteration 27/1000 | Loss: 0.00001882
Iteration 28/1000 | Loss: 0.00001881
Iteration 29/1000 | Loss: 0.00001881
Iteration 30/1000 | Loss: 0.00001881
Iteration 31/1000 | Loss: 0.00001880
Iteration 32/1000 | Loss: 0.00001880
Iteration 33/1000 | Loss: 0.00001880
Iteration 34/1000 | Loss: 0.00001880
Iteration 35/1000 | Loss: 0.00001880
Iteration 36/1000 | Loss: 0.00001879
Iteration 37/1000 | Loss: 0.00001879
Iteration 38/1000 | Loss: 0.00001879
Iteration 39/1000 | Loss: 0.00001878
Iteration 40/1000 | Loss: 0.00001878
Iteration 41/1000 | Loss: 0.00001878
Iteration 42/1000 | Loss: 0.00001878
Iteration 43/1000 | Loss: 0.00001877
Iteration 44/1000 | Loss: 0.00001877
Iteration 45/1000 | Loss: 0.00001877
Iteration 46/1000 | Loss: 0.00001877
Iteration 47/1000 | Loss: 0.00001876
Iteration 48/1000 | Loss: 0.00001876
Iteration 49/1000 | Loss: 0.00001875
Iteration 50/1000 | Loss: 0.00001875
Iteration 51/1000 | Loss: 0.00001875
Iteration 52/1000 | Loss: 0.00001875
Iteration 53/1000 | Loss: 0.00001874
Iteration 54/1000 | Loss: 0.00001874
Iteration 55/1000 | Loss: 0.00001874
Iteration 56/1000 | Loss: 0.00001874
Iteration 57/1000 | Loss: 0.00001874
Iteration 58/1000 | Loss: 0.00001874
Iteration 59/1000 | Loss: 0.00001873
Iteration 60/1000 | Loss: 0.00001873
Iteration 61/1000 | Loss: 0.00001873
Iteration 62/1000 | Loss: 0.00001873
Iteration 63/1000 | Loss: 0.00001873
Iteration 64/1000 | Loss: 0.00001873
Iteration 65/1000 | Loss: 0.00001873
Iteration 66/1000 | Loss: 0.00001873
Iteration 67/1000 | Loss: 0.00001873
Iteration 68/1000 | Loss: 0.00001873
Iteration 69/1000 | Loss: 0.00001872
Iteration 70/1000 | Loss: 0.00001872
Iteration 71/1000 | Loss: 0.00001872
Iteration 72/1000 | Loss: 0.00001872
Iteration 73/1000 | Loss: 0.00001872
Iteration 74/1000 | Loss: 0.00001872
Iteration 75/1000 | Loss: 0.00001872
Iteration 76/1000 | Loss: 0.00001872
Iteration 77/1000 | Loss: 0.00001872
Iteration 78/1000 | Loss: 0.00001872
Iteration 79/1000 | Loss: 0.00001872
Iteration 80/1000 | Loss: 0.00001872
Iteration 81/1000 | Loss: 0.00001872
Iteration 82/1000 | Loss: 0.00001871
Iteration 83/1000 | Loss: 0.00001871
Iteration 84/1000 | Loss: 0.00001871
Iteration 85/1000 | Loss: 0.00001871
Iteration 86/1000 | Loss: 0.00001871
Iteration 87/1000 | Loss: 0.00001871
Iteration 88/1000 | Loss: 0.00001871
Iteration 89/1000 | Loss: 0.00001871
Iteration 90/1000 | Loss: 0.00001871
Iteration 91/1000 | Loss: 0.00001871
Iteration 92/1000 | Loss: 0.00001871
Iteration 93/1000 | Loss: 0.00001871
Iteration 94/1000 | Loss: 0.00001871
Iteration 95/1000 | Loss: 0.00001871
Iteration 96/1000 | Loss: 0.00001871
Iteration 97/1000 | Loss: 0.00001871
Iteration 98/1000 | Loss: 0.00001871
Iteration 99/1000 | Loss: 0.00001871
Iteration 100/1000 | Loss: 0.00001871
Iteration 101/1000 | Loss: 0.00001871
Iteration 102/1000 | Loss: 0.00001871
Iteration 103/1000 | Loss: 0.00001871
Iteration 104/1000 | Loss: 0.00001871
Iteration 105/1000 | Loss: 0.00001871
Iteration 106/1000 | Loss: 0.00001871
Iteration 107/1000 | Loss: 0.00001871
Iteration 108/1000 | Loss: 0.00001870
Iteration 109/1000 | Loss: 0.00001870
Iteration 110/1000 | Loss: 0.00001870
Iteration 111/1000 | Loss: 0.00001870
Iteration 112/1000 | Loss: 0.00001870
Iteration 113/1000 | Loss: 0.00001870
Iteration 114/1000 | Loss: 0.00001870
Iteration 115/1000 | Loss: 0.00001870
Iteration 116/1000 | Loss: 0.00001870
Iteration 117/1000 | Loss: 0.00001870
Iteration 118/1000 | Loss: 0.00001870
Iteration 119/1000 | Loss: 0.00001870
Iteration 120/1000 | Loss: 0.00001870
Iteration 121/1000 | Loss: 0.00001870
Iteration 122/1000 | Loss: 0.00001870
Iteration 123/1000 | Loss: 0.00001870
Iteration 124/1000 | Loss: 0.00001870
Iteration 125/1000 | Loss: 0.00001870
Iteration 126/1000 | Loss: 0.00001870
Iteration 127/1000 | Loss: 0.00001870
Iteration 128/1000 | Loss: 0.00001870
Iteration 129/1000 | Loss: 0.00001870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.8703467503655702e-05, 1.8703467503655702e-05, 1.8703467503655702e-05, 1.8703467503655702e-05, 1.8703467503655702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8703467503655702e-05

Optimization complete. Final v2v error: 3.691418170928955 mm

Highest mean error: 4.10616397857666 mm for frame 89

Lowest mean error: 3.4237864017486572 mm for frame 26

Saving results

Total time: 34.65690636634827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832839
Iteration 2/25 | Loss: 0.00086522
Iteration 3/25 | Loss: 0.00071251
Iteration 4/25 | Loss: 0.00068513
Iteration 5/25 | Loss: 0.00067658
Iteration 6/25 | Loss: 0.00067597
Iteration 7/25 | Loss: 0.00067597
Iteration 8/25 | Loss: 0.00067597
Iteration 9/25 | Loss: 0.00067597
Iteration 10/25 | Loss: 0.00067597
Iteration 11/25 | Loss: 0.00067597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006759717362001538, 0.0006759717362001538, 0.0006759717362001538, 0.0006759717362001538, 0.0006759717362001538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006759717362001538

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44337666
Iteration 2/25 | Loss: 0.00028747
Iteration 3/25 | Loss: 0.00028747
Iteration 4/25 | Loss: 0.00028747
Iteration 5/25 | Loss: 0.00028746
Iteration 6/25 | Loss: 0.00028746
Iteration 7/25 | Loss: 0.00028746
Iteration 8/25 | Loss: 0.00028746
Iteration 9/25 | Loss: 0.00028746
Iteration 10/25 | Loss: 0.00028746
Iteration 11/25 | Loss: 0.00028746
Iteration 12/25 | Loss: 0.00028746
Iteration 13/25 | Loss: 0.00028746
Iteration 14/25 | Loss: 0.00028746
Iteration 15/25 | Loss: 0.00028746
Iteration 16/25 | Loss: 0.00028746
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00028746292809955776, 0.00028746292809955776, 0.00028746292809955776, 0.00028746292809955776, 0.00028746292809955776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028746292809955776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028746
Iteration 2/1000 | Loss: 0.00002492
Iteration 3/1000 | Loss: 0.00002128
Iteration 4/1000 | Loss: 0.00001997
Iteration 5/1000 | Loss: 0.00001896
Iteration 6/1000 | Loss: 0.00001841
Iteration 7/1000 | Loss: 0.00001801
Iteration 8/1000 | Loss: 0.00001769
Iteration 9/1000 | Loss: 0.00001747
Iteration 10/1000 | Loss: 0.00001747
Iteration 11/1000 | Loss: 0.00001744
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001742
Iteration 14/1000 | Loss: 0.00001742
Iteration 15/1000 | Loss: 0.00001741
Iteration 16/1000 | Loss: 0.00001740
Iteration 17/1000 | Loss: 0.00001737
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00001735
Iteration 20/1000 | Loss: 0.00001735
Iteration 21/1000 | Loss: 0.00001735
Iteration 22/1000 | Loss: 0.00001735
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001734
Iteration 25/1000 | Loss: 0.00001733
Iteration 26/1000 | Loss: 0.00001733
Iteration 27/1000 | Loss: 0.00001732
Iteration 28/1000 | Loss: 0.00001728
Iteration 29/1000 | Loss: 0.00001727
Iteration 30/1000 | Loss: 0.00001726
Iteration 31/1000 | Loss: 0.00001725
Iteration 32/1000 | Loss: 0.00001724
Iteration 33/1000 | Loss: 0.00001724
Iteration 34/1000 | Loss: 0.00001724
Iteration 35/1000 | Loss: 0.00001724
Iteration 36/1000 | Loss: 0.00001724
Iteration 37/1000 | Loss: 0.00001724
Iteration 38/1000 | Loss: 0.00001723
Iteration 39/1000 | Loss: 0.00001723
Iteration 40/1000 | Loss: 0.00001723
Iteration 41/1000 | Loss: 0.00001723
Iteration 42/1000 | Loss: 0.00001723
Iteration 43/1000 | Loss: 0.00001723
Iteration 44/1000 | Loss: 0.00001723
Iteration 45/1000 | Loss: 0.00001723
Iteration 46/1000 | Loss: 0.00001722
Iteration 47/1000 | Loss: 0.00001722
Iteration 48/1000 | Loss: 0.00001720
Iteration 49/1000 | Loss: 0.00001720
Iteration 50/1000 | Loss: 0.00001720
Iteration 51/1000 | Loss: 0.00001720
Iteration 52/1000 | Loss: 0.00001720
Iteration 53/1000 | Loss: 0.00001719
Iteration 54/1000 | Loss: 0.00001719
Iteration 55/1000 | Loss: 0.00001719
Iteration 56/1000 | Loss: 0.00001719
Iteration 57/1000 | Loss: 0.00001719
Iteration 58/1000 | Loss: 0.00001719
Iteration 59/1000 | Loss: 0.00001719
Iteration 60/1000 | Loss: 0.00001718
Iteration 61/1000 | Loss: 0.00001718
Iteration 62/1000 | Loss: 0.00001718
Iteration 63/1000 | Loss: 0.00001717
Iteration 64/1000 | Loss: 0.00001717
Iteration 65/1000 | Loss: 0.00001717
Iteration 66/1000 | Loss: 0.00001716
Iteration 67/1000 | Loss: 0.00001716
Iteration 68/1000 | Loss: 0.00001716
Iteration 69/1000 | Loss: 0.00001716
Iteration 70/1000 | Loss: 0.00001716
Iteration 71/1000 | Loss: 0.00001716
Iteration 72/1000 | Loss: 0.00001715
Iteration 73/1000 | Loss: 0.00001715
Iteration 74/1000 | Loss: 0.00001715
Iteration 75/1000 | Loss: 0.00001715
Iteration 76/1000 | Loss: 0.00001714
Iteration 77/1000 | Loss: 0.00001714
Iteration 78/1000 | Loss: 0.00001714
Iteration 79/1000 | Loss: 0.00001714
Iteration 80/1000 | Loss: 0.00001714
Iteration 81/1000 | Loss: 0.00001714
Iteration 82/1000 | Loss: 0.00001714
Iteration 83/1000 | Loss: 0.00001714
Iteration 84/1000 | Loss: 0.00001714
Iteration 85/1000 | Loss: 0.00001714
Iteration 86/1000 | Loss: 0.00001714
Iteration 87/1000 | Loss: 0.00001714
Iteration 88/1000 | Loss: 0.00001714
Iteration 89/1000 | Loss: 0.00001713
Iteration 90/1000 | Loss: 0.00001713
Iteration 91/1000 | Loss: 0.00001713
Iteration 92/1000 | Loss: 0.00001713
Iteration 93/1000 | Loss: 0.00001713
Iteration 94/1000 | Loss: 0.00001712
Iteration 95/1000 | Loss: 0.00001712
Iteration 96/1000 | Loss: 0.00001712
Iteration 97/1000 | Loss: 0.00001712
Iteration 98/1000 | Loss: 0.00001712
Iteration 99/1000 | Loss: 0.00001712
Iteration 100/1000 | Loss: 0.00001711
Iteration 101/1000 | Loss: 0.00001711
Iteration 102/1000 | Loss: 0.00001711
Iteration 103/1000 | Loss: 0.00001711
Iteration 104/1000 | Loss: 0.00001711
Iteration 105/1000 | Loss: 0.00001711
Iteration 106/1000 | Loss: 0.00001711
Iteration 107/1000 | Loss: 0.00001711
Iteration 108/1000 | Loss: 0.00001711
Iteration 109/1000 | Loss: 0.00001711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.711277218419127e-05, 1.711277218419127e-05, 1.711277218419127e-05, 1.711277218419127e-05, 1.711277218419127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.711277218419127e-05

Optimization complete. Final v2v error: 3.4777204990386963 mm

Highest mean error: 3.5998692512512207 mm for frame 235

Lowest mean error: 3.3788909912109375 mm for frame 43

Saving results

Total time: 32.64064264297485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388362
Iteration 2/25 | Loss: 0.00073820
Iteration 3/25 | Loss: 0.00063066
Iteration 4/25 | Loss: 0.00061022
Iteration 5/25 | Loss: 0.00060104
Iteration 6/25 | Loss: 0.00059907
Iteration 7/25 | Loss: 0.00059854
Iteration 8/25 | Loss: 0.00059854
Iteration 9/25 | Loss: 0.00059854
Iteration 10/25 | Loss: 0.00059854
Iteration 11/25 | Loss: 0.00059854
Iteration 12/25 | Loss: 0.00059854
Iteration 13/25 | Loss: 0.00059854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005985418683849275, 0.0005985418683849275, 0.0005985418683849275, 0.0005985418683849275, 0.0005985418683849275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005985418683849275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99193132
Iteration 2/25 | Loss: 0.00027043
Iteration 3/25 | Loss: 0.00027043
Iteration 4/25 | Loss: 0.00027043
Iteration 5/25 | Loss: 0.00027043
Iteration 6/25 | Loss: 0.00027043
Iteration 7/25 | Loss: 0.00027042
Iteration 8/25 | Loss: 0.00027042
Iteration 9/25 | Loss: 0.00027042
Iteration 10/25 | Loss: 0.00027042
Iteration 11/25 | Loss: 0.00027042
Iteration 12/25 | Loss: 0.00027042
Iteration 13/25 | Loss: 0.00027042
Iteration 14/25 | Loss: 0.00027042
Iteration 15/25 | Loss: 0.00027042
Iteration 16/25 | Loss: 0.00027042
Iteration 17/25 | Loss: 0.00027042
Iteration 18/25 | Loss: 0.00027042
Iteration 19/25 | Loss: 0.00027042
Iteration 20/25 | Loss: 0.00027042
Iteration 21/25 | Loss: 0.00027042
Iteration 22/25 | Loss: 0.00027042
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002704239741433412, 0.0002704239741433412, 0.0002704239741433412, 0.0002704239741433412, 0.0002704239741433412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002704239741433412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027042
Iteration 2/1000 | Loss: 0.00002850
Iteration 3/1000 | Loss: 0.00001777
Iteration 4/1000 | Loss: 0.00001599
Iteration 5/1000 | Loss: 0.00001496
Iteration 6/1000 | Loss: 0.00001421
Iteration 7/1000 | Loss: 0.00001382
Iteration 8/1000 | Loss: 0.00001342
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00001317
Iteration 11/1000 | Loss: 0.00001310
Iteration 12/1000 | Loss: 0.00001303
Iteration 13/1000 | Loss: 0.00001297
Iteration 14/1000 | Loss: 0.00001295
Iteration 15/1000 | Loss: 0.00001294
Iteration 16/1000 | Loss: 0.00001294
Iteration 17/1000 | Loss: 0.00001293
Iteration 18/1000 | Loss: 0.00001293
Iteration 19/1000 | Loss: 0.00001291
Iteration 20/1000 | Loss: 0.00001288
Iteration 21/1000 | Loss: 0.00001288
Iteration 22/1000 | Loss: 0.00001287
Iteration 23/1000 | Loss: 0.00001285
Iteration 24/1000 | Loss: 0.00001284
Iteration 25/1000 | Loss: 0.00001284
Iteration 26/1000 | Loss: 0.00001283
Iteration 27/1000 | Loss: 0.00001283
Iteration 28/1000 | Loss: 0.00001282
Iteration 29/1000 | Loss: 0.00001282
Iteration 30/1000 | Loss: 0.00001282
Iteration 31/1000 | Loss: 0.00001281
Iteration 32/1000 | Loss: 0.00001280
Iteration 33/1000 | Loss: 0.00001279
Iteration 34/1000 | Loss: 0.00001278
Iteration 35/1000 | Loss: 0.00001278
Iteration 36/1000 | Loss: 0.00001277
Iteration 37/1000 | Loss: 0.00001277
Iteration 38/1000 | Loss: 0.00001277
Iteration 39/1000 | Loss: 0.00001277
Iteration 40/1000 | Loss: 0.00001277
Iteration 41/1000 | Loss: 0.00001277
Iteration 42/1000 | Loss: 0.00001277
Iteration 43/1000 | Loss: 0.00001276
Iteration 44/1000 | Loss: 0.00001276
Iteration 45/1000 | Loss: 0.00001276
Iteration 46/1000 | Loss: 0.00001275
Iteration 47/1000 | Loss: 0.00001275
Iteration 48/1000 | Loss: 0.00001275
Iteration 49/1000 | Loss: 0.00001274
Iteration 50/1000 | Loss: 0.00001274
Iteration 51/1000 | Loss: 0.00001274
Iteration 52/1000 | Loss: 0.00001274
Iteration 53/1000 | Loss: 0.00001274
Iteration 54/1000 | Loss: 0.00001274
Iteration 55/1000 | Loss: 0.00001273
Iteration 56/1000 | Loss: 0.00001273
Iteration 57/1000 | Loss: 0.00001273
Iteration 58/1000 | Loss: 0.00001273
Iteration 59/1000 | Loss: 0.00001273
Iteration 60/1000 | Loss: 0.00001273
Iteration 61/1000 | Loss: 0.00001273
Iteration 62/1000 | Loss: 0.00001273
Iteration 63/1000 | Loss: 0.00001273
Iteration 64/1000 | Loss: 0.00001273
Iteration 65/1000 | Loss: 0.00001272
Iteration 66/1000 | Loss: 0.00001272
Iteration 67/1000 | Loss: 0.00001272
Iteration 68/1000 | Loss: 0.00001272
Iteration 69/1000 | Loss: 0.00001272
Iteration 70/1000 | Loss: 0.00001272
Iteration 71/1000 | Loss: 0.00001271
Iteration 72/1000 | Loss: 0.00001271
Iteration 73/1000 | Loss: 0.00001271
Iteration 74/1000 | Loss: 0.00001271
Iteration 75/1000 | Loss: 0.00001271
Iteration 76/1000 | Loss: 0.00001270
Iteration 77/1000 | Loss: 0.00001270
Iteration 78/1000 | Loss: 0.00001270
Iteration 79/1000 | Loss: 0.00001270
Iteration 80/1000 | Loss: 0.00001270
Iteration 81/1000 | Loss: 0.00001269
Iteration 82/1000 | Loss: 0.00001269
Iteration 83/1000 | Loss: 0.00001269
Iteration 84/1000 | Loss: 0.00001269
Iteration 85/1000 | Loss: 0.00001269
Iteration 86/1000 | Loss: 0.00001269
Iteration 87/1000 | Loss: 0.00001269
Iteration 88/1000 | Loss: 0.00001269
Iteration 89/1000 | Loss: 0.00001268
Iteration 90/1000 | Loss: 0.00001268
Iteration 91/1000 | Loss: 0.00001268
Iteration 92/1000 | Loss: 0.00001267
Iteration 93/1000 | Loss: 0.00001267
Iteration 94/1000 | Loss: 0.00001267
Iteration 95/1000 | Loss: 0.00001267
Iteration 96/1000 | Loss: 0.00001267
Iteration 97/1000 | Loss: 0.00001266
Iteration 98/1000 | Loss: 0.00001266
Iteration 99/1000 | Loss: 0.00001266
Iteration 100/1000 | Loss: 0.00001266
Iteration 101/1000 | Loss: 0.00001266
Iteration 102/1000 | Loss: 0.00001265
Iteration 103/1000 | Loss: 0.00001265
Iteration 104/1000 | Loss: 0.00001265
Iteration 105/1000 | Loss: 0.00001264
Iteration 106/1000 | Loss: 0.00001264
Iteration 107/1000 | Loss: 0.00001264
Iteration 108/1000 | Loss: 0.00001264
Iteration 109/1000 | Loss: 0.00001264
Iteration 110/1000 | Loss: 0.00001263
Iteration 111/1000 | Loss: 0.00001263
Iteration 112/1000 | Loss: 0.00001263
Iteration 113/1000 | Loss: 0.00001263
Iteration 114/1000 | Loss: 0.00001263
Iteration 115/1000 | Loss: 0.00001263
Iteration 116/1000 | Loss: 0.00001262
Iteration 117/1000 | Loss: 0.00001262
Iteration 118/1000 | Loss: 0.00001262
Iteration 119/1000 | Loss: 0.00001262
Iteration 120/1000 | Loss: 0.00001262
Iteration 121/1000 | Loss: 0.00001262
Iteration 122/1000 | Loss: 0.00001261
Iteration 123/1000 | Loss: 0.00001261
Iteration 124/1000 | Loss: 0.00001261
Iteration 125/1000 | Loss: 0.00001261
Iteration 126/1000 | Loss: 0.00001261
Iteration 127/1000 | Loss: 0.00001261
Iteration 128/1000 | Loss: 0.00001261
Iteration 129/1000 | Loss: 0.00001261
Iteration 130/1000 | Loss: 0.00001260
Iteration 131/1000 | Loss: 0.00001260
Iteration 132/1000 | Loss: 0.00001260
Iteration 133/1000 | Loss: 0.00001260
Iteration 134/1000 | Loss: 0.00001260
Iteration 135/1000 | Loss: 0.00001259
Iteration 136/1000 | Loss: 0.00001259
Iteration 137/1000 | Loss: 0.00001259
Iteration 138/1000 | Loss: 0.00001258
Iteration 139/1000 | Loss: 0.00001258
Iteration 140/1000 | Loss: 0.00001258
Iteration 141/1000 | Loss: 0.00001258
Iteration 142/1000 | Loss: 0.00001258
Iteration 143/1000 | Loss: 0.00001258
Iteration 144/1000 | Loss: 0.00001257
Iteration 145/1000 | Loss: 0.00001257
Iteration 146/1000 | Loss: 0.00001257
Iteration 147/1000 | Loss: 0.00001257
Iteration 148/1000 | Loss: 0.00001257
Iteration 149/1000 | Loss: 0.00001257
Iteration 150/1000 | Loss: 0.00001257
Iteration 151/1000 | Loss: 0.00001257
Iteration 152/1000 | Loss: 0.00001257
Iteration 153/1000 | Loss: 0.00001256
Iteration 154/1000 | Loss: 0.00001256
Iteration 155/1000 | Loss: 0.00001256
Iteration 156/1000 | Loss: 0.00001256
Iteration 157/1000 | Loss: 0.00001256
Iteration 158/1000 | Loss: 0.00001256
Iteration 159/1000 | Loss: 0.00001256
Iteration 160/1000 | Loss: 0.00001256
Iteration 161/1000 | Loss: 0.00001256
Iteration 162/1000 | Loss: 0.00001255
Iteration 163/1000 | Loss: 0.00001255
Iteration 164/1000 | Loss: 0.00001255
Iteration 165/1000 | Loss: 0.00001255
Iteration 166/1000 | Loss: 0.00001255
Iteration 167/1000 | Loss: 0.00001255
Iteration 168/1000 | Loss: 0.00001255
Iteration 169/1000 | Loss: 0.00001255
Iteration 170/1000 | Loss: 0.00001255
Iteration 171/1000 | Loss: 0.00001255
Iteration 172/1000 | Loss: 0.00001255
Iteration 173/1000 | Loss: 0.00001255
Iteration 174/1000 | Loss: 0.00001255
Iteration 175/1000 | Loss: 0.00001255
Iteration 176/1000 | Loss: 0.00001255
Iteration 177/1000 | Loss: 0.00001255
Iteration 178/1000 | Loss: 0.00001255
Iteration 179/1000 | Loss: 0.00001255
Iteration 180/1000 | Loss: 0.00001255
Iteration 181/1000 | Loss: 0.00001255
Iteration 182/1000 | Loss: 0.00001255
Iteration 183/1000 | Loss: 0.00001255
Iteration 184/1000 | Loss: 0.00001255
Iteration 185/1000 | Loss: 0.00001255
Iteration 186/1000 | Loss: 0.00001255
Iteration 187/1000 | Loss: 0.00001255
Iteration 188/1000 | Loss: 0.00001255
Iteration 189/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.2550984138215426e-05, 1.2550984138215426e-05, 1.2550984138215426e-05, 1.2550984138215426e-05, 1.2550984138215426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2550984138215426e-05

Optimization complete. Final v2v error: 3.001582145690918 mm

Highest mean error: 3.7598183155059814 mm for frame 72

Lowest mean error: 2.706040620803833 mm for frame 86

Saving results

Total time: 39.618159770965576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042599
Iteration 2/25 | Loss: 0.00247811
Iteration 3/25 | Loss: 0.00132218
Iteration 4/25 | Loss: 0.00106679
Iteration 5/25 | Loss: 0.00093658
Iteration 6/25 | Loss: 0.00091570
Iteration 7/25 | Loss: 0.00076810
Iteration 8/25 | Loss: 0.00072371
Iteration 9/25 | Loss: 0.00069240
Iteration 10/25 | Loss: 0.00068370
Iteration 11/25 | Loss: 0.00068500
Iteration 12/25 | Loss: 0.00067084
Iteration 13/25 | Loss: 0.00066550
Iteration 14/25 | Loss: 0.00066283
Iteration 15/25 | Loss: 0.00066177
Iteration 16/25 | Loss: 0.00065888
Iteration 17/25 | Loss: 0.00065932
Iteration 18/25 | Loss: 0.00066276
Iteration 19/25 | Loss: 0.00065992
Iteration 20/25 | Loss: 0.00065879
Iteration 21/25 | Loss: 0.00065660
Iteration 22/25 | Loss: 0.00065714
Iteration 23/25 | Loss: 0.00066051
Iteration 24/25 | Loss: 0.00065405
Iteration 25/25 | Loss: 0.00065442

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57529438
Iteration 2/25 | Loss: 0.00056341
Iteration 3/25 | Loss: 0.00055592
Iteration 4/25 | Loss: 0.00055592
Iteration 5/25 | Loss: 0.00055592
Iteration 6/25 | Loss: 0.00055592
Iteration 7/25 | Loss: 0.00055592
Iteration 8/25 | Loss: 0.00055592
Iteration 9/25 | Loss: 0.00055592
Iteration 10/25 | Loss: 0.00055592
Iteration 11/25 | Loss: 0.00055592
Iteration 12/25 | Loss: 0.00055592
Iteration 13/25 | Loss: 0.00055592
Iteration 14/25 | Loss: 0.00055592
Iteration 15/25 | Loss: 0.00055592
Iteration 16/25 | Loss: 0.00055592
Iteration 17/25 | Loss: 0.00055592
Iteration 18/25 | Loss: 0.00055592
Iteration 19/25 | Loss: 0.00055592
Iteration 20/25 | Loss: 0.00055592
Iteration 21/25 | Loss: 0.00055592
Iteration 22/25 | Loss: 0.00055592
Iteration 23/25 | Loss: 0.00055592
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005559164565056562, 0.0005559164565056562, 0.0005559164565056562, 0.0005559164565056562, 0.0005559164565056562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005559164565056562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055592
Iteration 2/1000 | Loss: 0.00009627
Iteration 3/1000 | Loss: 0.00012182
Iteration 4/1000 | Loss: 0.00036323
Iteration 5/1000 | Loss: 0.00013853
Iteration 6/1000 | Loss: 0.00064672
Iteration 7/1000 | Loss: 0.00021347
Iteration 8/1000 | Loss: 0.00060326
Iteration 9/1000 | Loss: 0.00004661
Iteration 10/1000 | Loss: 0.00056116
Iteration 11/1000 | Loss: 0.00064249
Iteration 12/1000 | Loss: 0.00019024
Iteration 13/1000 | Loss: 0.00019657
Iteration 14/1000 | Loss: 0.00003950
Iteration 15/1000 | Loss: 0.00006440
Iteration 16/1000 | Loss: 0.00016326
Iteration 17/1000 | Loss: 0.00012643
Iteration 18/1000 | Loss: 0.00003694
Iteration 19/1000 | Loss: 0.00002896
Iteration 20/1000 | Loss: 0.00061672
Iteration 21/1000 | Loss: 0.00015120
Iteration 22/1000 | Loss: 0.00023743
Iteration 23/1000 | Loss: 0.00031488
Iteration 24/1000 | Loss: 0.00013651
Iteration 25/1000 | Loss: 0.00002961
Iteration 26/1000 | Loss: 0.00008188
Iteration 27/1000 | Loss: 0.00025330
Iteration 28/1000 | Loss: 0.00018960
Iteration 29/1000 | Loss: 0.00025165
Iteration 30/1000 | Loss: 0.00020030
Iteration 31/1000 | Loss: 0.00004126
Iteration 32/1000 | Loss: 0.00003854
Iteration 33/1000 | Loss: 0.00002251
Iteration 34/1000 | Loss: 0.00054546
Iteration 35/1000 | Loss: 0.00002858
Iteration 36/1000 | Loss: 0.00002321
Iteration 37/1000 | Loss: 0.00001972
Iteration 38/1000 | Loss: 0.00001802
Iteration 39/1000 | Loss: 0.00001614
Iteration 40/1000 | Loss: 0.00001513
Iteration 41/1000 | Loss: 0.00001440
Iteration 42/1000 | Loss: 0.00001870
Iteration 43/1000 | Loss: 0.00001530
Iteration 44/1000 | Loss: 0.00001393
Iteration 45/1000 | Loss: 0.00001355
Iteration 46/1000 | Loss: 0.00001323
Iteration 47/1000 | Loss: 0.00001310
Iteration 48/1000 | Loss: 0.00001304
Iteration 49/1000 | Loss: 0.00001303
Iteration 50/1000 | Loss: 0.00001303
Iteration 51/1000 | Loss: 0.00001303
Iteration 52/1000 | Loss: 0.00001302
Iteration 53/1000 | Loss: 0.00001301
Iteration 54/1000 | Loss: 0.00001300
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001297
Iteration 58/1000 | Loss: 0.00001297
Iteration 59/1000 | Loss: 0.00001295
Iteration 60/1000 | Loss: 0.00001291
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001278
Iteration 64/1000 | Loss: 0.00001275
Iteration 65/1000 | Loss: 0.00001275
Iteration 66/1000 | Loss: 0.00001274
Iteration 67/1000 | Loss: 0.00001273
Iteration 68/1000 | Loss: 0.00001273
Iteration 69/1000 | Loss: 0.00001273
Iteration 70/1000 | Loss: 0.00001272
Iteration 71/1000 | Loss: 0.00001271
Iteration 72/1000 | Loss: 0.00001271
Iteration 73/1000 | Loss: 0.00001271
Iteration 74/1000 | Loss: 0.00001271
Iteration 75/1000 | Loss: 0.00001271
Iteration 76/1000 | Loss: 0.00001271
Iteration 77/1000 | Loss: 0.00001270
Iteration 78/1000 | Loss: 0.00001270
Iteration 79/1000 | Loss: 0.00001270
Iteration 80/1000 | Loss: 0.00001270
Iteration 81/1000 | Loss: 0.00001270
Iteration 82/1000 | Loss: 0.00001269
Iteration 83/1000 | Loss: 0.00001269
Iteration 84/1000 | Loss: 0.00001269
Iteration 85/1000 | Loss: 0.00001268
Iteration 86/1000 | Loss: 0.00001268
Iteration 87/1000 | Loss: 0.00001268
Iteration 88/1000 | Loss: 0.00001268
Iteration 89/1000 | Loss: 0.00001268
Iteration 90/1000 | Loss: 0.00001268
Iteration 91/1000 | Loss: 0.00001268
Iteration 92/1000 | Loss: 0.00001267
Iteration 93/1000 | Loss: 0.00001267
Iteration 94/1000 | Loss: 0.00001267
Iteration 95/1000 | Loss: 0.00001266
Iteration 96/1000 | Loss: 0.00001266
Iteration 97/1000 | Loss: 0.00001266
Iteration 98/1000 | Loss: 0.00001266
Iteration 99/1000 | Loss: 0.00001266
Iteration 100/1000 | Loss: 0.00001266
Iteration 101/1000 | Loss: 0.00001266
Iteration 102/1000 | Loss: 0.00001266
Iteration 103/1000 | Loss: 0.00001266
Iteration 104/1000 | Loss: 0.00001265
Iteration 105/1000 | Loss: 0.00001265
Iteration 106/1000 | Loss: 0.00001265
Iteration 107/1000 | Loss: 0.00001265
Iteration 108/1000 | Loss: 0.00001265
Iteration 109/1000 | Loss: 0.00001265
Iteration 110/1000 | Loss: 0.00001265
Iteration 111/1000 | Loss: 0.00001265
Iteration 112/1000 | Loss: 0.00001265
Iteration 113/1000 | Loss: 0.00001264
Iteration 114/1000 | Loss: 0.00001264
Iteration 115/1000 | Loss: 0.00001264
Iteration 116/1000 | Loss: 0.00001263
Iteration 117/1000 | Loss: 0.00001263
Iteration 118/1000 | Loss: 0.00001263
Iteration 119/1000 | Loss: 0.00001262
Iteration 120/1000 | Loss: 0.00001262
Iteration 121/1000 | Loss: 0.00001262
Iteration 122/1000 | Loss: 0.00001261
Iteration 123/1000 | Loss: 0.00001261
Iteration 124/1000 | Loss: 0.00001261
Iteration 125/1000 | Loss: 0.00001260
Iteration 126/1000 | Loss: 0.00001260
Iteration 127/1000 | Loss: 0.00001260
Iteration 128/1000 | Loss: 0.00001260
Iteration 129/1000 | Loss: 0.00001260
Iteration 130/1000 | Loss: 0.00001260
Iteration 131/1000 | Loss: 0.00001260
Iteration 132/1000 | Loss: 0.00001260
Iteration 133/1000 | Loss: 0.00001259
Iteration 134/1000 | Loss: 0.00001259
Iteration 135/1000 | Loss: 0.00001259
Iteration 136/1000 | Loss: 0.00001259
Iteration 137/1000 | Loss: 0.00001259
Iteration 138/1000 | Loss: 0.00001259
Iteration 139/1000 | Loss: 0.00001259
Iteration 140/1000 | Loss: 0.00001259
Iteration 141/1000 | Loss: 0.00001259
Iteration 142/1000 | Loss: 0.00001259
Iteration 143/1000 | Loss: 0.00001258
Iteration 144/1000 | Loss: 0.00001258
Iteration 145/1000 | Loss: 0.00001258
Iteration 146/1000 | Loss: 0.00001258
Iteration 147/1000 | Loss: 0.00001258
Iteration 148/1000 | Loss: 0.00001258
Iteration 149/1000 | Loss: 0.00001258
Iteration 150/1000 | Loss: 0.00001258
Iteration 151/1000 | Loss: 0.00001257
Iteration 152/1000 | Loss: 0.00001257
Iteration 153/1000 | Loss: 0.00001257
Iteration 154/1000 | Loss: 0.00001257
Iteration 155/1000 | Loss: 0.00001257
Iteration 156/1000 | Loss: 0.00001257
Iteration 157/1000 | Loss: 0.00001257
Iteration 158/1000 | Loss: 0.00001257
Iteration 159/1000 | Loss: 0.00001257
Iteration 160/1000 | Loss: 0.00001257
Iteration 161/1000 | Loss: 0.00001257
Iteration 162/1000 | Loss: 0.00001257
Iteration 163/1000 | Loss: 0.00001257
Iteration 164/1000 | Loss: 0.00001257
Iteration 165/1000 | Loss: 0.00001256
Iteration 166/1000 | Loss: 0.00001256
Iteration 167/1000 | Loss: 0.00001256
Iteration 168/1000 | Loss: 0.00001256
Iteration 169/1000 | Loss: 0.00001256
Iteration 170/1000 | Loss: 0.00001256
Iteration 171/1000 | Loss: 0.00001256
Iteration 172/1000 | Loss: 0.00001256
Iteration 173/1000 | Loss: 0.00001256
Iteration 174/1000 | Loss: 0.00001256
Iteration 175/1000 | Loss: 0.00001256
Iteration 176/1000 | Loss: 0.00001255
Iteration 177/1000 | Loss: 0.00001255
Iteration 178/1000 | Loss: 0.00001255
Iteration 179/1000 | Loss: 0.00001255
Iteration 180/1000 | Loss: 0.00001255
Iteration 181/1000 | Loss: 0.00001255
Iteration 182/1000 | Loss: 0.00001255
Iteration 183/1000 | Loss: 0.00001255
Iteration 184/1000 | Loss: 0.00001254
Iteration 185/1000 | Loss: 0.00001254
Iteration 186/1000 | Loss: 0.00001254
Iteration 187/1000 | Loss: 0.00001254
Iteration 188/1000 | Loss: 0.00001254
Iteration 189/1000 | Loss: 0.00001254
Iteration 190/1000 | Loss: 0.00001254
Iteration 191/1000 | Loss: 0.00001254
Iteration 192/1000 | Loss: 0.00001253
Iteration 193/1000 | Loss: 0.00001253
Iteration 194/1000 | Loss: 0.00001253
Iteration 195/1000 | Loss: 0.00001253
Iteration 196/1000 | Loss: 0.00001253
Iteration 197/1000 | Loss: 0.00001253
Iteration 198/1000 | Loss: 0.00001253
Iteration 199/1000 | Loss: 0.00001253
Iteration 200/1000 | Loss: 0.00001253
Iteration 201/1000 | Loss: 0.00001253
Iteration 202/1000 | Loss: 0.00001252
Iteration 203/1000 | Loss: 0.00001252
Iteration 204/1000 | Loss: 0.00001252
Iteration 205/1000 | Loss: 0.00001252
Iteration 206/1000 | Loss: 0.00001252
Iteration 207/1000 | Loss: 0.00001252
Iteration 208/1000 | Loss: 0.00001252
Iteration 209/1000 | Loss: 0.00001252
Iteration 210/1000 | Loss: 0.00001252
Iteration 211/1000 | Loss: 0.00001252
Iteration 212/1000 | Loss: 0.00001252
Iteration 213/1000 | Loss: 0.00001252
Iteration 214/1000 | Loss: 0.00001252
Iteration 215/1000 | Loss: 0.00001252
Iteration 216/1000 | Loss: 0.00001252
Iteration 217/1000 | Loss: 0.00001252
Iteration 218/1000 | Loss: 0.00001252
Iteration 219/1000 | Loss: 0.00001252
Iteration 220/1000 | Loss: 0.00001252
Iteration 221/1000 | Loss: 0.00001251
Iteration 222/1000 | Loss: 0.00001251
Iteration 223/1000 | Loss: 0.00001251
Iteration 224/1000 | Loss: 0.00001251
Iteration 225/1000 | Loss: 0.00001251
Iteration 226/1000 | Loss: 0.00001251
Iteration 227/1000 | Loss: 0.00001251
Iteration 228/1000 | Loss: 0.00001251
Iteration 229/1000 | Loss: 0.00001251
Iteration 230/1000 | Loss: 0.00001251
Iteration 231/1000 | Loss: 0.00001251
Iteration 232/1000 | Loss: 0.00001251
Iteration 233/1000 | Loss: 0.00001251
Iteration 234/1000 | Loss: 0.00001251
Iteration 235/1000 | Loss: 0.00001251
Iteration 236/1000 | Loss: 0.00001251
Iteration 237/1000 | Loss: 0.00001251
Iteration 238/1000 | Loss: 0.00001251
Iteration 239/1000 | Loss: 0.00001251
Iteration 240/1000 | Loss: 0.00001251
Iteration 241/1000 | Loss: 0.00001251
Iteration 242/1000 | Loss: 0.00001251
Iteration 243/1000 | Loss: 0.00001251
Iteration 244/1000 | Loss: 0.00001251
Iteration 245/1000 | Loss: 0.00001251
Iteration 246/1000 | Loss: 0.00001251
Iteration 247/1000 | Loss: 0.00001251
Iteration 248/1000 | Loss: 0.00001251
Iteration 249/1000 | Loss: 0.00001251
Iteration 250/1000 | Loss: 0.00001251
Iteration 251/1000 | Loss: 0.00001251
Iteration 252/1000 | Loss: 0.00001251
Iteration 253/1000 | Loss: 0.00001251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.2510853593994398e-05, 1.2510853593994398e-05, 1.2510853593994398e-05, 1.2510853593994398e-05, 1.2510853593994398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2510853593994398e-05

Optimization complete. Final v2v error: 3.0242230892181396 mm

Highest mean error: 4.049880027770996 mm for frame 182

Lowest mean error: 2.769097328186035 mm for frame 81

Saving results

Total time: 144.1931836605072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842705
Iteration 2/25 | Loss: 0.00083573
Iteration 3/25 | Loss: 0.00071798
Iteration 4/25 | Loss: 0.00066623
Iteration 5/25 | Loss: 0.00065548
Iteration 6/25 | Loss: 0.00065309
Iteration 7/25 | Loss: 0.00065185
Iteration 8/25 | Loss: 0.00065178
Iteration 9/25 | Loss: 0.00065178
Iteration 10/25 | Loss: 0.00065178
Iteration 11/25 | Loss: 0.00065178
Iteration 12/25 | Loss: 0.00065178
Iteration 13/25 | Loss: 0.00065178
Iteration 14/25 | Loss: 0.00065178
Iteration 15/25 | Loss: 0.00065178
Iteration 16/25 | Loss: 0.00065178
Iteration 17/25 | Loss: 0.00065178
Iteration 18/25 | Loss: 0.00065178
Iteration 19/25 | Loss: 0.00065178
Iteration 20/25 | Loss: 0.00065178
Iteration 21/25 | Loss: 0.00065178
Iteration 22/25 | Loss: 0.00065178
Iteration 23/25 | Loss: 0.00065178
Iteration 24/25 | Loss: 0.00065178
Iteration 25/25 | Loss: 0.00065178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.57998133
Iteration 2/25 | Loss: 0.00029685
Iteration 3/25 | Loss: 0.00029679
Iteration 4/25 | Loss: 0.00029679
Iteration 5/25 | Loss: 0.00029679
Iteration 6/25 | Loss: 0.00029679
Iteration 7/25 | Loss: 0.00029679
Iteration 8/25 | Loss: 0.00029679
Iteration 9/25 | Loss: 0.00029679
Iteration 10/25 | Loss: 0.00029679
Iteration 11/25 | Loss: 0.00029679
Iteration 12/25 | Loss: 0.00029679
Iteration 13/25 | Loss: 0.00029679
Iteration 14/25 | Loss: 0.00029679
Iteration 15/25 | Loss: 0.00029679
Iteration 16/25 | Loss: 0.00029679
Iteration 17/25 | Loss: 0.00029679
Iteration 18/25 | Loss: 0.00029679
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00029678671853616834, 0.00029678671853616834, 0.00029678671853616834, 0.00029678671853616834, 0.00029678671853616834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029678671853616834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029679
Iteration 2/1000 | Loss: 0.00003099
Iteration 3/1000 | Loss: 0.00002191
Iteration 4/1000 | Loss: 0.00001984
Iteration 5/1000 | Loss: 0.00001863
Iteration 6/1000 | Loss: 0.00001817
Iteration 7/1000 | Loss: 0.00001767
Iteration 8/1000 | Loss: 0.00001745
Iteration 9/1000 | Loss: 0.00001717
Iteration 10/1000 | Loss: 0.00001692
Iteration 11/1000 | Loss: 0.00001686
Iteration 12/1000 | Loss: 0.00001682
Iteration 13/1000 | Loss: 0.00001673
Iteration 14/1000 | Loss: 0.00001669
Iteration 15/1000 | Loss: 0.00001662
Iteration 16/1000 | Loss: 0.00001662
Iteration 17/1000 | Loss: 0.00001660
Iteration 18/1000 | Loss: 0.00001659
Iteration 19/1000 | Loss: 0.00001656
Iteration 20/1000 | Loss: 0.00001656
Iteration 21/1000 | Loss: 0.00001656
Iteration 22/1000 | Loss: 0.00001656
Iteration 23/1000 | Loss: 0.00001656
Iteration 24/1000 | Loss: 0.00001656
Iteration 25/1000 | Loss: 0.00001656
Iteration 26/1000 | Loss: 0.00001656
Iteration 27/1000 | Loss: 0.00001656
Iteration 28/1000 | Loss: 0.00001655
Iteration 29/1000 | Loss: 0.00001655
Iteration 30/1000 | Loss: 0.00001655
Iteration 31/1000 | Loss: 0.00001653
Iteration 32/1000 | Loss: 0.00001652
Iteration 33/1000 | Loss: 0.00001652
Iteration 34/1000 | Loss: 0.00001652
Iteration 35/1000 | Loss: 0.00001652
Iteration 36/1000 | Loss: 0.00001652
Iteration 37/1000 | Loss: 0.00001652
Iteration 38/1000 | Loss: 0.00001652
Iteration 39/1000 | Loss: 0.00001652
Iteration 40/1000 | Loss: 0.00001652
Iteration 41/1000 | Loss: 0.00001652
Iteration 42/1000 | Loss: 0.00001651
Iteration 43/1000 | Loss: 0.00001650
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001650
Iteration 46/1000 | Loss: 0.00001649
Iteration 47/1000 | Loss: 0.00001649
Iteration 48/1000 | Loss: 0.00001649
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001648
Iteration 52/1000 | Loss: 0.00001648
Iteration 53/1000 | Loss: 0.00001648
Iteration 54/1000 | Loss: 0.00001647
Iteration 55/1000 | Loss: 0.00001647
Iteration 56/1000 | Loss: 0.00001647
Iteration 57/1000 | Loss: 0.00001647
Iteration 58/1000 | Loss: 0.00001647
Iteration 59/1000 | Loss: 0.00001647
Iteration 60/1000 | Loss: 0.00001647
Iteration 61/1000 | Loss: 0.00001647
Iteration 62/1000 | Loss: 0.00001646
Iteration 63/1000 | Loss: 0.00001646
Iteration 64/1000 | Loss: 0.00001646
Iteration 65/1000 | Loss: 0.00001646
Iteration 66/1000 | Loss: 0.00001646
Iteration 67/1000 | Loss: 0.00001646
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001645
Iteration 71/1000 | Loss: 0.00001645
Iteration 72/1000 | Loss: 0.00001645
Iteration 73/1000 | Loss: 0.00001645
Iteration 74/1000 | Loss: 0.00001644
Iteration 75/1000 | Loss: 0.00001644
Iteration 76/1000 | Loss: 0.00001644
Iteration 77/1000 | Loss: 0.00001644
Iteration 78/1000 | Loss: 0.00001644
Iteration 79/1000 | Loss: 0.00001643
Iteration 80/1000 | Loss: 0.00001643
Iteration 81/1000 | Loss: 0.00001643
Iteration 82/1000 | Loss: 0.00001643
Iteration 83/1000 | Loss: 0.00001643
Iteration 84/1000 | Loss: 0.00001643
Iteration 85/1000 | Loss: 0.00001642
Iteration 86/1000 | Loss: 0.00001642
Iteration 87/1000 | Loss: 0.00001642
Iteration 88/1000 | Loss: 0.00001642
Iteration 89/1000 | Loss: 0.00001642
Iteration 90/1000 | Loss: 0.00001642
Iteration 91/1000 | Loss: 0.00001641
Iteration 92/1000 | Loss: 0.00001641
Iteration 93/1000 | Loss: 0.00001641
Iteration 94/1000 | Loss: 0.00001641
Iteration 95/1000 | Loss: 0.00001641
Iteration 96/1000 | Loss: 0.00001641
Iteration 97/1000 | Loss: 0.00001641
Iteration 98/1000 | Loss: 0.00001641
Iteration 99/1000 | Loss: 0.00001641
Iteration 100/1000 | Loss: 0.00001641
Iteration 101/1000 | Loss: 0.00001641
Iteration 102/1000 | Loss: 0.00001641
Iteration 103/1000 | Loss: 0.00001641
Iteration 104/1000 | Loss: 0.00001641
Iteration 105/1000 | Loss: 0.00001640
Iteration 106/1000 | Loss: 0.00001640
Iteration 107/1000 | Loss: 0.00001640
Iteration 108/1000 | Loss: 0.00001640
Iteration 109/1000 | Loss: 0.00001640
Iteration 110/1000 | Loss: 0.00001639
Iteration 111/1000 | Loss: 0.00001639
Iteration 112/1000 | Loss: 0.00001639
Iteration 113/1000 | Loss: 0.00001639
Iteration 114/1000 | Loss: 0.00001639
Iteration 115/1000 | Loss: 0.00001639
Iteration 116/1000 | Loss: 0.00001639
Iteration 117/1000 | Loss: 0.00001639
Iteration 118/1000 | Loss: 0.00001639
Iteration 119/1000 | Loss: 0.00001639
Iteration 120/1000 | Loss: 0.00001639
Iteration 121/1000 | Loss: 0.00001639
Iteration 122/1000 | Loss: 0.00001638
Iteration 123/1000 | Loss: 0.00001638
Iteration 124/1000 | Loss: 0.00001638
Iteration 125/1000 | Loss: 0.00001638
Iteration 126/1000 | Loss: 0.00001638
Iteration 127/1000 | Loss: 0.00001638
Iteration 128/1000 | Loss: 0.00001637
Iteration 129/1000 | Loss: 0.00001637
Iteration 130/1000 | Loss: 0.00001637
Iteration 131/1000 | Loss: 0.00001637
Iteration 132/1000 | Loss: 0.00001637
Iteration 133/1000 | Loss: 0.00001637
Iteration 134/1000 | Loss: 0.00001637
Iteration 135/1000 | Loss: 0.00001637
Iteration 136/1000 | Loss: 0.00001637
Iteration 137/1000 | Loss: 0.00001637
Iteration 138/1000 | Loss: 0.00001637
Iteration 139/1000 | Loss: 0.00001637
Iteration 140/1000 | Loss: 0.00001637
Iteration 141/1000 | Loss: 0.00001637
Iteration 142/1000 | Loss: 0.00001636
Iteration 143/1000 | Loss: 0.00001636
Iteration 144/1000 | Loss: 0.00001636
Iteration 145/1000 | Loss: 0.00001636
Iteration 146/1000 | Loss: 0.00001636
Iteration 147/1000 | Loss: 0.00001636
Iteration 148/1000 | Loss: 0.00001636
Iteration 149/1000 | Loss: 0.00001636
Iteration 150/1000 | Loss: 0.00001635
Iteration 151/1000 | Loss: 0.00001635
Iteration 152/1000 | Loss: 0.00001635
Iteration 153/1000 | Loss: 0.00001635
Iteration 154/1000 | Loss: 0.00001635
Iteration 155/1000 | Loss: 0.00001635
Iteration 156/1000 | Loss: 0.00001635
Iteration 157/1000 | Loss: 0.00001635
Iteration 158/1000 | Loss: 0.00001635
Iteration 159/1000 | Loss: 0.00001635
Iteration 160/1000 | Loss: 0.00001635
Iteration 161/1000 | Loss: 0.00001635
Iteration 162/1000 | Loss: 0.00001635
Iteration 163/1000 | Loss: 0.00001634
Iteration 164/1000 | Loss: 0.00001634
Iteration 165/1000 | Loss: 0.00001634
Iteration 166/1000 | Loss: 0.00001634
Iteration 167/1000 | Loss: 0.00001634
Iteration 168/1000 | Loss: 0.00001634
Iteration 169/1000 | Loss: 0.00001634
Iteration 170/1000 | Loss: 0.00001634
Iteration 171/1000 | Loss: 0.00001634
Iteration 172/1000 | Loss: 0.00001634
Iteration 173/1000 | Loss: 0.00001634
Iteration 174/1000 | Loss: 0.00001634
Iteration 175/1000 | Loss: 0.00001634
Iteration 176/1000 | Loss: 0.00001634
Iteration 177/1000 | Loss: 0.00001634
Iteration 178/1000 | Loss: 0.00001634
Iteration 179/1000 | Loss: 0.00001634
Iteration 180/1000 | Loss: 0.00001634
Iteration 181/1000 | Loss: 0.00001634
Iteration 182/1000 | Loss: 0.00001634
Iteration 183/1000 | Loss: 0.00001634
Iteration 184/1000 | Loss: 0.00001634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.633660758670885e-05, 1.633660758670885e-05, 1.633660758670885e-05, 1.633660758670885e-05, 1.633660758670885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.633660758670885e-05

Optimization complete. Final v2v error: 3.4234585762023926 mm

Highest mean error: 3.864487648010254 mm for frame 50

Lowest mean error: 3.0520591735839844 mm for frame 218

Saving results

Total time: 44.085530281066895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493489
Iteration 2/25 | Loss: 0.00092002
Iteration 3/25 | Loss: 0.00069331
Iteration 4/25 | Loss: 0.00066153
Iteration 5/25 | Loss: 0.00065496
Iteration 6/25 | Loss: 0.00065349
Iteration 7/25 | Loss: 0.00065294
Iteration 8/25 | Loss: 0.00065294
Iteration 9/25 | Loss: 0.00065294
Iteration 10/25 | Loss: 0.00065294
Iteration 11/25 | Loss: 0.00065294
Iteration 12/25 | Loss: 0.00065294
Iteration 13/25 | Loss: 0.00065294
Iteration 14/25 | Loss: 0.00065294
Iteration 15/25 | Loss: 0.00065294
Iteration 16/25 | Loss: 0.00065294
Iteration 17/25 | Loss: 0.00065294
Iteration 18/25 | Loss: 0.00065294
Iteration 19/25 | Loss: 0.00065294
Iteration 20/25 | Loss: 0.00065294
Iteration 21/25 | Loss: 0.00065294
Iteration 22/25 | Loss: 0.00065294
Iteration 23/25 | Loss: 0.00065294
Iteration 24/25 | Loss: 0.00065294
Iteration 25/25 | Loss: 0.00065294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79052746
Iteration 2/25 | Loss: 0.00021202
Iteration 3/25 | Loss: 0.00021202
Iteration 4/25 | Loss: 0.00021202
Iteration 5/25 | Loss: 0.00021202
Iteration 6/25 | Loss: 0.00021202
Iteration 7/25 | Loss: 0.00021202
Iteration 8/25 | Loss: 0.00021202
Iteration 9/25 | Loss: 0.00021202
Iteration 10/25 | Loss: 0.00021202
Iteration 11/25 | Loss: 0.00021202
Iteration 12/25 | Loss: 0.00021202
Iteration 13/25 | Loss: 0.00021202
Iteration 14/25 | Loss: 0.00021202
Iteration 15/25 | Loss: 0.00021202
Iteration 16/25 | Loss: 0.00021202
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00021201647177804261, 0.00021201647177804261, 0.00021201647177804261, 0.00021201647177804261, 0.00021201647177804261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021201647177804261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021202
Iteration 2/1000 | Loss: 0.00004082
Iteration 3/1000 | Loss: 0.00002765
Iteration 4/1000 | Loss: 0.00002580
Iteration 5/1000 | Loss: 0.00002461
Iteration 6/1000 | Loss: 0.00002379
Iteration 7/1000 | Loss: 0.00002317
Iteration 8/1000 | Loss: 0.00002258
Iteration 9/1000 | Loss: 0.00002217
Iteration 10/1000 | Loss: 0.00002196
Iteration 11/1000 | Loss: 0.00002195
Iteration 12/1000 | Loss: 0.00002170
Iteration 13/1000 | Loss: 0.00002154
Iteration 14/1000 | Loss: 0.00002142
Iteration 15/1000 | Loss: 0.00002141
Iteration 16/1000 | Loss: 0.00002141
Iteration 17/1000 | Loss: 0.00002133
Iteration 18/1000 | Loss: 0.00002131
Iteration 19/1000 | Loss: 0.00002131
Iteration 20/1000 | Loss: 0.00002130
Iteration 21/1000 | Loss: 0.00002130
Iteration 22/1000 | Loss: 0.00002130
Iteration 23/1000 | Loss: 0.00002129
Iteration 24/1000 | Loss: 0.00002129
Iteration 25/1000 | Loss: 0.00002129
Iteration 26/1000 | Loss: 0.00002129
Iteration 27/1000 | Loss: 0.00002129
Iteration 28/1000 | Loss: 0.00002129
Iteration 29/1000 | Loss: 0.00002129
Iteration 30/1000 | Loss: 0.00002129
Iteration 31/1000 | Loss: 0.00002129
Iteration 32/1000 | Loss: 0.00002128
Iteration 33/1000 | Loss: 0.00002127
Iteration 34/1000 | Loss: 0.00002127
Iteration 35/1000 | Loss: 0.00002127
Iteration 36/1000 | Loss: 0.00002127
Iteration 37/1000 | Loss: 0.00002126
Iteration 38/1000 | Loss: 0.00002125
Iteration 39/1000 | Loss: 0.00002120
Iteration 40/1000 | Loss: 0.00002119
Iteration 41/1000 | Loss: 0.00002119
Iteration 42/1000 | Loss: 0.00002118
Iteration 43/1000 | Loss: 0.00002116
Iteration 44/1000 | Loss: 0.00002116
Iteration 45/1000 | Loss: 0.00002115
Iteration 46/1000 | Loss: 0.00002115
Iteration 47/1000 | Loss: 0.00002115
Iteration 48/1000 | Loss: 0.00002114
Iteration 49/1000 | Loss: 0.00002114
Iteration 50/1000 | Loss: 0.00002113
Iteration 51/1000 | Loss: 0.00002113
Iteration 52/1000 | Loss: 0.00002113
Iteration 53/1000 | Loss: 0.00002113
Iteration 54/1000 | Loss: 0.00002113
Iteration 55/1000 | Loss: 0.00002112
Iteration 56/1000 | Loss: 0.00002112
Iteration 57/1000 | Loss: 0.00002112
Iteration 58/1000 | Loss: 0.00002112
Iteration 59/1000 | Loss: 0.00002112
Iteration 60/1000 | Loss: 0.00002111
Iteration 61/1000 | Loss: 0.00002111
Iteration 62/1000 | Loss: 0.00002111
Iteration 63/1000 | Loss: 0.00002110
Iteration 64/1000 | Loss: 0.00002110
Iteration 65/1000 | Loss: 0.00002110
Iteration 66/1000 | Loss: 0.00002109
Iteration 67/1000 | Loss: 0.00002109
Iteration 68/1000 | Loss: 0.00002109
Iteration 69/1000 | Loss: 0.00002109
Iteration 70/1000 | Loss: 0.00002108
Iteration 71/1000 | Loss: 0.00002108
Iteration 72/1000 | Loss: 0.00002107
Iteration 73/1000 | Loss: 0.00002107
Iteration 74/1000 | Loss: 0.00002106
Iteration 75/1000 | Loss: 0.00002106
Iteration 76/1000 | Loss: 0.00002106
Iteration 77/1000 | Loss: 0.00002106
Iteration 78/1000 | Loss: 0.00002105
Iteration 79/1000 | Loss: 0.00002105
Iteration 80/1000 | Loss: 0.00002105
Iteration 81/1000 | Loss: 0.00002105
Iteration 82/1000 | Loss: 0.00002104
Iteration 83/1000 | Loss: 0.00002104
Iteration 84/1000 | Loss: 0.00002104
Iteration 85/1000 | Loss: 0.00002103
Iteration 86/1000 | Loss: 0.00002103
Iteration 87/1000 | Loss: 0.00002103
Iteration 88/1000 | Loss: 0.00002103
Iteration 89/1000 | Loss: 0.00002103
Iteration 90/1000 | Loss: 0.00002103
Iteration 91/1000 | Loss: 0.00002103
Iteration 92/1000 | Loss: 0.00002102
Iteration 93/1000 | Loss: 0.00002102
Iteration 94/1000 | Loss: 0.00002102
Iteration 95/1000 | Loss: 0.00002102
Iteration 96/1000 | Loss: 0.00002102
Iteration 97/1000 | Loss: 0.00002102
Iteration 98/1000 | Loss: 0.00002102
Iteration 99/1000 | Loss: 0.00002102
Iteration 100/1000 | Loss: 0.00002102
Iteration 101/1000 | Loss: 0.00002102
Iteration 102/1000 | Loss: 0.00002102
Iteration 103/1000 | Loss: 0.00002102
Iteration 104/1000 | Loss: 0.00002101
Iteration 105/1000 | Loss: 0.00002101
Iteration 106/1000 | Loss: 0.00002101
Iteration 107/1000 | Loss: 0.00002101
Iteration 108/1000 | Loss: 0.00002101
Iteration 109/1000 | Loss: 0.00002100
Iteration 110/1000 | Loss: 0.00002100
Iteration 111/1000 | Loss: 0.00002099
Iteration 112/1000 | Loss: 0.00002099
Iteration 113/1000 | Loss: 0.00002099
Iteration 114/1000 | Loss: 0.00002099
Iteration 115/1000 | Loss: 0.00002099
Iteration 116/1000 | Loss: 0.00002099
Iteration 117/1000 | Loss: 0.00002099
Iteration 118/1000 | Loss: 0.00002099
Iteration 119/1000 | Loss: 0.00002099
Iteration 120/1000 | Loss: 0.00002098
Iteration 121/1000 | Loss: 0.00002098
Iteration 122/1000 | Loss: 0.00002098
Iteration 123/1000 | Loss: 0.00002098
Iteration 124/1000 | Loss: 0.00002098
Iteration 125/1000 | Loss: 0.00002098
Iteration 126/1000 | Loss: 0.00002098
Iteration 127/1000 | Loss: 0.00002098
Iteration 128/1000 | Loss: 0.00002098
Iteration 129/1000 | Loss: 0.00002098
Iteration 130/1000 | Loss: 0.00002098
Iteration 131/1000 | Loss: 0.00002097
Iteration 132/1000 | Loss: 0.00002097
Iteration 133/1000 | Loss: 0.00002097
Iteration 134/1000 | Loss: 0.00002097
Iteration 135/1000 | Loss: 0.00002097
Iteration 136/1000 | Loss: 0.00002097
Iteration 137/1000 | Loss: 0.00002097
Iteration 138/1000 | Loss: 0.00002097
Iteration 139/1000 | Loss: 0.00002097
Iteration 140/1000 | Loss: 0.00002097
Iteration 141/1000 | Loss: 0.00002097
Iteration 142/1000 | Loss: 0.00002096
Iteration 143/1000 | Loss: 0.00002096
Iteration 144/1000 | Loss: 0.00002096
Iteration 145/1000 | Loss: 0.00002096
Iteration 146/1000 | Loss: 0.00002096
Iteration 147/1000 | Loss: 0.00002096
Iteration 148/1000 | Loss: 0.00002096
Iteration 149/1000 | Loss: 0.00002096
Iteration 150/1000 | Loss: 0.00002096
Iteration 151/1000 | Loss: 0.00002095
Iteration 152/1000 | Loss: 0.00002095
Iteration 153/1000 | Loss: 0.00002095
Iteration 154/1000 | Loss: 0.00002095
Iteration 155/1000 | Loss: 0.00002095
Iteration 156/1000 | Loss: 0.00002095
Iteration 157/1000 | Loss: 0.00002095
Iteration 158/1000 | Loss: 0.00002095
Iteration 159/1000 | Loss: 0.00002094
Iteration 160/1000 | Loss: 0.00002094
Iteration 161/1000 | Loss: 0.00002094
Iteration 162/1000 | Loss: 0.00002094
Iteration 163/1000 | Loss: 0.00002094
Iteration 164/1000 | Loss: 0.00002094
Iteration 165/1000 | Loss: 0.00002094
Iteration 166/1000 | Loss: 0.00002094
Iteration 167/1000 | Loss: 0.00002094
Iteration 168/1000 | Loss: 0.00002094
Iteration 169/1000 | Loss: 0.00002094
Iteration 170/1000 | Loss: 0.00002094
Iteration 171/1000 | Loss: 0.00002094
Iteration 172/1000 | Loss: 0.00002093
Iteration 173/1000 | Loss: 0.00002093
Iteration 174/1000 | Loss: 0.00002093
Iteration 175/1000 | Loss: 0.00002093
Iteration 176/1000 | Loss: 0.00002093
Iteration 177/1000 | Loss: 0.00002093
Iteration 178/1000 | Loss: 0.00002093
Iteration 179/1000 | Loss: 0.00002093
Iteration 180/1000 | Loss: 0.00002093
Iteration 181/1000 | Loss: 0.00002093
Iteration 182/1000 | Loss: 0.00002093
Iteration 183/1000 | Loss: 0.00002093
Iteration 184/1000 | Loss: 0.00002093
Iteration 185/1000 | Loss: 0.00002092
Iteration 186/1000 | Loss: 0.00002092
Iteration 187/1000 | Loss: 0.00002092
Iteration 188/1000 | Loss: 0.00002092
Iteration 189/1000 | Loss: 0.00002092
Iteration 190/1000 | Loss: 0.00002092
Iteration 191/1000 | Loss: 0.00002092
Iteration 192/1000 | Loss: 0.00002092
Iteration 193/1000 | Loss: 0.00002092
Iteration 194/1000 | Loss: 0.00002092
Iteration 195/1000 | Loss: 0.00002092
Iteration 196/1000 | Loss: 0.00002091
Iteration 197/1000 | Loss: 0.00002091
Iteration 198/1000 | Loss: 0.00002091
Iteration 199/1000 | Loss: 0.00002091
Iteration 200/1000 | Loss: 0.00002091
Iteration 201/1000 | Loss: 0.00002091
Iteration 202/1000 | Loss: 0.00002091
Iteration 203/1000 | Loss: 0.00002091
Iteration 204/1000 | Loss: 0.00002091
Iteration 205/1000 | Loss: 0.00002091
Iteration 206/1000 | Loss: 0.00002091
Iteration 207/1000 | Loss: 0.00002091
Iteration 208/1000 | Loss: 0.00002091
Iteration 209/1000 | Loss: 0.00002090
Iteration 210/1000 | Loss: 0.00002090
Iteration 211/1000 | Loss: 0.00002090
Iteration 212/1000 | Loss: 0.00002090
Iteration 213/1000 | Loss: 0.00002090
Iteration 214/1000 | Loss: 0.00002090
Iteration 215/1000 | Loss: 0.00002090
Iteration 216/1000 | Loss: 0.00002090
Iteration 217/1000 | Loss: 0.00002090
Iteration 218/1000 | Loss: 0.00002090
Iteration 219/1000 | Loss: 0.00002089
Iteration 220/1000 | Loss: 0.00002089
Iteration 221/1000 | Loss: 0.00002089
Iteration 222/1000 | Loss: 0.00002089
Iteration 223/1000 | Loss: 0.00002089
Iteration 224/1000 | Loss: 0.00002089
Iteration 225/1000 | Loss: 0.00002089
Iteration 226/1000 | Loss: 0.00002089
Iteration 227/1000 | Loss: 0.00002088
Iteration 228/1000 | Loss: 0.00002088
Iteration 229/1000 | Loss: 0.00002088
Iteration 230/1000 | Loss: 0.00002088
Iteration 231/1000 | Loss: 0.00002088
Iteration 232/1000 | Loss: 0.00002088
Iteration 233/1000 | Loss: 0.00002088
Iteration 234/1000 | Loss: 0.00002088
Iteration 235/1000 | Loss: 0.00002088
Iteration 236/1000 | Loss: 0.00002088
Iteration 237/1000 | Loss: 0.00002088
Iteration 238/1000 | Loss: 0.00002088
Iteration 239/1000 | Loss: 0.00002088
Iteration 240/1000 | Loss: 0.00002088
Iteration 241/1000 | Loss: 0.00002087
Iteration 242/1000 | Loss: 0.00002087
Iteration 243/1000 | Loss: 0.00002087
Iteration 244/1000 | Loss: 0.00002087
Iteration 245/1000 | Loss: 0.00002087
Iteration 246/1000 | Loss: 0.00002087
Iteration 247/1000 | Loss: 0.00002087
Iteration 248/1000 | Loss: 0.00002087
Iteration 249/1000 | Loss: 0.00002087
Iteration 250/1000 | Loss: 0.00002087
Iteration 251/1000 | Loss: 0.00002087
Iteration 252/1000 | Loss: 0.00002087
Iteration 253/1000 | Loss: 0.00002087
Iteration 254/1000 | Loss: 0.00002087
Iteration 255/1000 | Loss: 0.00002087
Iteration 256/1000 | Loss: 0.00002087
Iteration 257/1000 | Loss: 0.00002087
Iteration 258/1000 | Loss: 0.00002087
Iteration 259/1000 | Loss: 0.00002087
Iteration 260/1000 | Loss: 0.00002087
Iteration 261/1000 | Loss: 0.00002087
Iteration 262/1000 | Loss: 0.00002087
Iteration 263/1000 | Loss: 0.00002087
Iteration 264/1000 | Loss: 0.00002087
Iteration 265/1000 | Loss: 0.00002087
Iteration 266/1000 | Loss: 0.00002087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [2.086824861180503e-05, 2.086824861180503e-05, 2.086824861180503e-05, 2.086824861180503e-05, 2.086824861180503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.086824861180503e-05

Optimization complete. Final v2v error: 3.8898494243621826 mm

Highest mean error: 4.689101696014404 mm for frame 246

Lowest mean error: 3.698539972305298 mm for frame 25

Saving results

Total time: 53.84410262107849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813247
Iteration 2/25 | Loss: 0.00085607
Iteration 3/25 | Loss: 0.00065229
Iteration 4/25 | Loss: 0.00061954
Iteration 5/25 | Loss: 0.00060899
Iteration 6/25 | Loss: 0.00060649
Iteration 7/25 | Loss: 0.00060624
Iteration 8/25 | Loss: 0.00060624
Iteration 9/25 | Loss: 0.00060624
Iteration 10/25 | Loss: 0.00060624
Iteration 11/25 | Loss: 0.00060624
Iteration 12/25 | Loss: 0.00060624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006062376196496189, 0.0006062376196496189, 0.0006062376196496189, 0.0006062376196496189, 0.0006062376196496189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006062376196496189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45729625
Iteration 2/25 | Loss: 0.00029336
Iteration 3/25 | Loss: 0.00029336
Iteration 4/25 | Loss: 0.00029336
Iteration 5/25 | Loss: 0.00029335
Iteration 6/25 | Loss: 0.00029335
Iteration 7/25 | Loss: 0.00029335
Iteration 8/25 | Loss: 0.00029335
Iteration 9/25 | Loss: 0.00029335
Iteration 10/25 | Loss: 0.00029335
Iteration 11/25 | Loss: 0.00029335
Iteration 12/25 | Loss: 0.00029335
Iteration 13/25 | Loss: 0.00029335
Iteration 14/25 | Loss: 0.00029335
Iteration 15/25 | Loss: 0.00029335
Iteration 16/25 | Loss: 0.00029335
Iteration 17/25 | Loss: 0.00029335
Iteration 18/25 | Loss: 0.00029335
Iteration 19/25 | Loss: 0.00029335
Iteration 20/25 | Loss: 0.00029335
Iteration 21/25 | Loss: 0.00029335
Iteration 22/25 | Loss: 0.00029335
Iteration 23/25 | Loss: 0.00029335
Iteration 24/25 | Loss: 0.00029335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002933529030997306, 0.0002933529030997306, 0.0002933529030997306, 0.0002933529030997306, 0.0002933529030997306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002933529030997306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029335
Iteration 2/1000 | Loss: 0.00001940
Iteration 3/1000 | Loss: 0.00001305
Iteration 4/1000 | Loss: 0.00001221
Iteration 5/1000 | Loss: 0.00001160
Iteration 6/1000 | Loss: 0.00001131
Iteration 7/1000 | Loss: 0.00001107
Iteration 8/1000 | Loss: 0.00001099
Iteration 9/1000 | Loss: 0.00001088
Iteration 10/1000 | Loss: 0.00001085
Iteration 11/1000 | Loss: 0.00001083
Iteration 12/1000 | Loss: 0.00001083
Iteration 13/1000 | Loss: 0.00001082
Iteration 14/1000 | Loss: 0.00001077
Iteration 15/1000 | Loss: 0.00001077
Iteration 16/1000 | Loss: 0.00001074
Iteration 17/1000 | Loss: 0.00001067
Iteration 18/1000 | Loss: 0.00001067
Iteration 19/1000 | Loss: 0.00001066
Iteration 20/1000 | Loss: 0.00001065
Iteration 21/1000 | Loss: 0.00001065
Iteration 22/1000 | Loss: 0.00001065
Iteration 23/1000 | Loss: 0.00001065
Iteration 24/1000 | Loss: 0.00001064
Iteration 25/1000 | Loss: 0.00001062
Iteration 26/1000 | Loss: 0.00001062
Iteration 27/1000 | Loss: 0.00001062
Iteration 28/1000 | Loss: 0.00001062
Iteration 29/1000 | Loss: 0.00001062
Iteration 30/1000 | Loss: 0.00001062
Iteration 31/1000 | Loss: 0.00001062
Iteration 32/1000 | Loss: 0.00001062
Iteration 33/1000 | Loss: 0.00001062
Iteration 34/1000 | Loss: 0.00001062
Iteration 35/1000 | Loss: 0.00001061
Iteration 36/1000 | Loss: 0.00001061
Iteration 37/1000 | Loss: 0.00001061
Iteration 38/1000 | Loss: 0.00001061
Iteration 39/1000 | Loss: 0.00001061
Iteration 40/1000 | Loss: 0.00001060
Iteration 41/1000 | Loss: 0.00001059
Iteration 42/1000 | Loss: 0.00001059
Iteration 43/1000 | Loss: 0.00001058
Iteration 44/1000 | Loss: 0.00001057
Iteration 45/1000 | Loss: 0.00001057
Iteration 46/1000 | Loss: 0.00001056
Iteration 47/1000 | Loss: 0.00001056
Iteration 48/1000 | Loss: 0.00001055
Iteration 49/1000 | Loss: 0.00001055
Iteration 50/1000 | Loss: 0.00001052
Iteration 51/1000 | Loss: 0.00001052
Iteration 52/1000 | Loss: 0.00001051
Iteration 53/1000 | Loss: 0.00001051
Iteration 54/1000 | Loss: 0.00001051
Iteration 55/1000 | Loss: 0.00001050
Iteration 56/1000 | Loss: 0.00001050
Iteration 57/1000 | Loss: 0.00001050
Iteration 58/1000 | Loss: 0.00001050
Iteration 59/1000 | Loss: 0.00001050
Iteration 60/1000 | Loss: 0.00001050
Iteration 61/1000 | Loss: 0.00001050
Iteration 62/1000 | Loss: 0.00001050
Iteration 63/1000 | Loss: 0.00001050
Iteration 64/1000 | Loss: 0.00001050
Iteration 65/1000 | Loss: 0.00001050
Iteration 66/1000 | Loss: 0.00001050
Iteration 67/1000 | Loss: 0.00001049
Iteration 68/1000 | Loss: 0.00001049
Iteration 69/1000 | Loss: 0.00001049
Iteration 70/1000 | Loss: 0.00001049
Iteration 71/1000 | Loss: 0.00001049
Iteration 72/1000 | Loss: 0.00001049
Iteration 73/1000 | Loss: 0.00001049
Iteration 74/1000 | Loss: 0.00001049
Iteration 75/1000 | Loss: 0.00001049
Iteration 76/1000 | Loss: 0.00001049
Iteration 77/1000 | Loss: 0.00001049
Iteration 78/1000 | Loss: 0.00001049
Iteration 79/1000 | Loss: 0.00001049
Iteration 80/1000 | Loss: 0.00001049
Iteration 81/1000 | Loss: 0.00001049
Iteration 82/1000 | Loss: 0.00001048
Iteration 83/1000 | Loss: 0.00001048
Iteration 84/1000 | Loss: 0.00001048
Iteration 85/1000 | Loss: 0.00001048
Iteration 86/1000 | Loss: 0.00001048
Iteration 87/1000 | Loss: 0.00001048
Iteration 88/1000 | Loss: 0.00001048
Iteration 89/1000 | Loss: 0.00001048
Iteration 90/1000 | Loss: 0.00001048
Iteration 91/1000 | Loss: 0.00001048
Iteration 92/1000 | Loss: 0.00001048
Iteration 93/1000 | Loss: 0.00001047
Iteration 94/1000 | Loss: 0.00001047
Iteration 95/1000 | Loss: 0.00001047
Iteration 96/1000 | Loss: 0.00001047
Iteration 97/1000 | Loss: 0.00001047
Iteration 98/1000 | Loss: 0.00001047
Iteration 99/1000 | Loss: 0.00001046
Iteration 100/1000 | Loss: 0.00001046
Iteration 101/1000 | Loss: 0.00001046
Iteration 102/1000 | Loss: 0.00001046
Iteration 103/1000 | Loss: 0.00001046
Iteration 104/1000 | Loss: 0.00001046
Iteration 105/1000 | Loss: 0.00001046
Iteration 106/1000 | Loss: 0.00001046
Iteration 107/1000 | Loss: 0.00001046
Iteration 108/1000 | Loss: 0.00001045
Iteration 109/1000 | Loss: 0.00001045
Iteration 110/1000 | Loss: 0.00001045
Iteration 111/1000 | Loss: 0.00001045
Iteration 112/1000 | Loss: 0.00001045
Iteration 113/1000 | Loss: 0.00001045
Iteration 114/1000 | Loss: 0.00001045
Iteration 115/1000 | Loss: 0.00001045
Iteration 116/1000 | Loss: 0.00001045
Iteration 117/1000 | Loss: 0.00001045
Iteration 118/1000 | Loss: 0.00001045
Iteration 119/1000 | Loss: 0.00001045
Iteration 120/1000 | Loss: 0.00001044
Iteration 121/1000 | Loss: 0.00001044
Iteration 122/1000 | Loss: 0.00001044
Iteration 123/1000 | Loss: 0.00001044
Iteration 124/1000 | Loss: 0.00001044
Iteration 125/1000 | Loss: 0.00001044
Iteration 126/1000 | Loss: 0.00001044
Iteration 127/1000 | Loss: 0.00001044
Iteration 128/1000 | Loss: 0.00001044
Iteration 129/1000 | Loss: 0.00001044
Iteration 130/1000 | Loss: 0.00001043
Iteration 131/1000 | Loss: 0.00001043
Iteration 132/1000 | Loss: 0.00001043
Iteration 133/1000 | Loss: 0.00001043
Iteration 134/1000 | Loss: 0.00001043
Iteration 135/1000 | Loss: 0.00001043
Iteration 136/1000 | Loss: 0.00001043
Iteration 137/1000 | Loss: 0.00001042
Iteration 138/1000 | Loss: 0.00001042
Iteration 139/1000 | Loss: 0.00001042
Iteration 140/1000 | Loss: 0.00001042
Iteration 141/1000 | Loss: 0.00001042
Iteration 142/1000 | Loss: 0.00001042
Iteration 143/1000 | Loss: 0.00001042
Iteration 144/1000 | Loss: 0.00001042
Iteration 145/1000 | Loss: 0.00001041
Iteration 146/1000 | Loss: 0.00001041
Iteration 147/1000 | Loss: 0.00001041
Iteration 148/1000 | Loss: 0.00001041
Iteration 149/1000 | Loss: 0.00001041
Iteration 150/1000 | Loss: 0.00001041
Iteration 151/1000 | Loss: 0.00001041
Iteration 152/1000 | Loss: 0.00001041
Iteration 153/1000 | Loss: 0.00001041
Iteration 154/1000 | Loss: 0.00001041
Iteration 155/1000 | Loss: 0.00001041
Iteration 156/1000 | Loss: 0.00001041
Iteration 157/1000 | Loss: 0.00001040
Iteration 158/1000 | Loss: 0.00001040
Iteration 159/1000 | Loss: 0.00001040
Iteration 160/1000 | Loss: 0.00001040
Iteration 161/1000 | Loss: 0.00001040
Iteration 162/1000 | Loss: 0.00001040
Iteration 163/1000 | Loss: 0.00001040
Iteration 164/1000 | Loss: 0.00001040
Iteration 165/1000 | Loss: 0.00001040
Iteration 166/1000 | Loss: 0.00001040
Iteration 167/1000 | Loss: 0.00001040
Iteration 168/1000 | Loss: 0.00001040
Iteration 169/1000 | Loss: 0.00001040
Iteration 170/1000 | Loss: 0.00001040
Iteration 171/1000 | Loss: 0.00001040
Iteration 172/1000 | Loss: 0.00001040
Iteration 173/1000 | Loss: 0.00001040
Iteration 174/1000 | Loss: 0.00001040
Iteration 175/1000 | Loss: 0.00001040
Iteration 176/1000 | Loss: 0.00001040
Iteration 177/1000 | Loss: 0.00001040
Iteration 178/1000 | Loss: 0.00001040
Iteration 179/1000 | Loss: 0.00001040
Iteration 180/1000 | Loss: 0.00001040
Iteration 181/1000 | Loss: 0.00001040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.0399500752100721e-05, 1.0399500752100721e-05, 1.0399500752100721e-05, 1.0399500752100721e-05, 1.0399500752100721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0399500752100721e-05

Optimization complete. Final v2v error: 2.698324203491211 mm

Highest mean error: 2.9137930870056152 mm for frame 116

Lowest mean error: 2.553893804550171 mm for frame 209

Saving results

Total time: 37.08551478385925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00652079
Iteration 2/25 | Loss: 0.00108566
Iteration 3/25 | Loss: 0.00085835
Iteration 4/25 | Loss: 0.00083267
Iteration 5/25 | Loss: 0.00082566
Iteration 6/25 | Loss: 0.00082518
Iteration 7/25 | Loss: 0.00082518
Iteration 8/25 | Loss: 0.00082518
Iteration 9/25 | Loss: 0.00082518
Iteration 10/25 | Loss: 0.00082518
Iteration 11/25 | Loss: 0.00082518
Iteration 12/25 | Loss: 0.00082518
Iteration 13/25 | Loss: 0.00082518
Iteration 14/25 | Loss: 0.00082518
Iteration 15/25 | Loss: 0.00082518
Iteration 16/25 | Loss: 0.00082518
Iteration 17/25 | Loss: 0.00082518
Iteration 18/25 | Loss: 0.00082518
Iteration 19/25 | Loss: 0.00082518
Iteration 20/25 | Loss: 0.00082518
Iteration 21/25 | Loss: 0.00082518
Iteration 22/25 | Loss: 0.00082518
Iteration 23/25 | Loss: 0.00082518
Iteration 24/25 | Loss: 0.00082518
Iteration 25/25 | Loss: 0.00082518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67463213
Iteration 2/25 | Loss: 0.00037694
Iteration 3/25 | Loss: 0.00037694
Iteration 4/25 | Loss: 0.00037694
Iteration 5/25 | Loss: 0.00037694
Iteration 6/25 | Loss: 0.00037694
Iteration 7/25 | Loss: 0.00037694
Iteration 8/25 | Loss: 0.00037694
Iteration 9/25 | Loss: 0.00037694
Iteration 10/25 | Loss: 0.00037694
Iteration 11/25 | Loss: 0.00037694
Iteration 12/25 | Loss: 0.00037694
Iteration 13/25 | Loss: 0.00037694
Iteration 14/25 | Loss: 0.00037694
Iteration 15/25 | Loss: 0.00037694
Iteration 16/25 | Loss: 0.00037694
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00037693549529649317, 0.00037693549529649317, 0.00037693549529649317, 0.00037693549529649317, 0.00037693549529649317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037693549529649317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037694
Iteration 2/1000 | Loss: 0.00003475
Iteration 3/1000 | Loss: 0.00002816
Iteration 4/1000 | Loss: 0.00002696
Iteration 5/1000 | Loss: 0.00002594
Iteration 6/1000 | Loss: 0.00002538
Iteration 7/1000 | Loss: 0.00002511
Iteration 8/1000 | Loss: 0.00002463
Iteration 9/1000 | Loss: 0.00002433
Iteration 10/1000 | Loss: 0.00002412
Iteration 11/1000 | Loss: 0.00002408
Iteration 12/1000 | Loss: 0.00002404
Iteration 13/1000 | Loss: 0.00002403
Iteration 14/1000 | Loss: 0.00002403
Iteration 15/1000 | Loss: 0.00002402
Iteration 16/1000 | Loss: 0.00002401
Iteration 17/1000 | Loss: 0.00002398
Iteration 18/1000 | Loss: 0.00002398
Iteration 19/1000 | Loss: 0.00002397
Iteration 20/1000 | Loss: 0.00002384
Iteration 21/1000 | Loss: 0.00002376
Iteration 22/1000 | Loss: 0.00002375
Iteration 23/1000 | Loss: 0.00002373
Iteration 24/1000 | Loss: 0.00002373
Iteration 25/1000 | Loss: 0.00002373
Iteration 26/1000 | Loss: 0.00002372
Iteration 27/1000 | Loss: 0.00002372
Iteration 28/1000 | Loss: 0.00002372
Iteration 29/1000 | Loss: 0.00002372
Iteration 30/1000 | Loss: 0.00002367
Iteration 31/1000 | Loss: 0.00002367
Iteration 32/1000 | Loss: 0.00002367
Iteration 33/1000 | Loss: 0.00002367
Iteration 34/1000 | Loss: 0.00002367
Iteration 35/1000 | Loss: 0.00002366
Iteration 36/1000 | Loss: 0.00002366
Iteration 37/1000 | Loss: 0.00002366
Iteration 38/1000 | Loss: 0.00002366
Iteration 39/1000 | Loss: 0.00002366
Iteration 40/1000 | Loss: 0.00002365
Iteration 41/1000 | Loss: 0.00002364
Iteration 42/1000 | Loss: 0.00002363
Iteration 43/1000 | Loss: 0.00002363
Iteration 44/1000 | Loss: 0.00002363
Iteration 45/1000 | Loss: 0.00002361
Iteration 46/1000 | Loss: 0.00002361
Iteration 47/1000 | Loss: 0.00002361
Iteration 48/1000 | Loss: 0.00002360
Iteration 49/1000 | Loss: 0.00002359
Iteration 50/1000 | Loss: 0.00002358
Iteration 51/1000 | Loss: 0.00002358
Iteration 52/1000 | Loss: 0.00002358
Iteration 53/1000 | Loss: 0.00002358
Iteration 54/1000 | Loss: 0.00002358
Iteration 55/1000 | Loss: 0.00002357
Iteration 56/1000 | Loss: 0.00002357
Iteration 57/1000 | Loss: 0.00002357
Iteration 58/1000 | Loss: 0.00002357
Iteration 59/1000 | Loss: 0.00002357
Iteration 60/1000 | Loss: 0.00002357
Iteration 61/1000 | Loss: 0.00002357
Iteration 62/1000 | Loss: 0.00002357
Iteration 63/1000 | Loss: 0.00002357
Iteration 64/1000 | Loss: 0.00002357
Iteration 65/1000 | Loss: 0.00002357
Iteration 66/1000 | Loss: 0.00002356
Iteration 67/1000 | Loss: 0.00002356
Iteration 68/1000 | Loss: 0.00002356
Iteration 69/1000 | Loss: 0.00002356
Iteration 70/1000 | Loss: 0.00002356
Iteration 71/1000 | Loss: 0.00002356
Iteration 72/1000 | Loss: 0.00002355
Iteration 73/1000 | Loss: 0.00002355
Iteration 74/1000 | Loss: 0.00002355
Iteration 75/1000 | Loss: 0.00002355
Iteration 76/1000 | Loss: 0.00002355
Iteration 77/1000 | Loss: 0.00002355
Iteration 78/1000 | Loss: 0.00002355
Iteration 79/1000 | Loss: 0.00002355
Iteration 80/1000 | Loss: 0.00002355
Iteration 81/1000 | Loss: 0.00002355
Iteration 82/1000 | Loss: 0.00002355
Iteration 83/1000 | Loss: 0.00002354
Iteration 84/1000 | Loss: 0.00002354
Iteration 85/1000 | Loss: 0.00002354
Iteration 86/1000 | Loss: 0.00002354
Iteration 87/1000 | Loss: 0.00002354
Iteration 88/1000 | Loss: 0.00002354
Iteration 89/1000 | Loss: 0.00002354
Iteration 90/1000 | Loss: 0.00002353
Iteration 91/1000 | Loss: 0.00002353
Iteration 92/1000 | Loss: 0.00002353
Iteration 93/1000 | Loss: 0.00002353
Iteration 94/1000 | Loss: 0.00002353
Iteration 95/1000 | Loss: 0.00002353
Iteration 96/1000 | Loss: 0.00002353
Iteration 97/1000 | Loss: 0.00002353
Iteration 98/1000 | Loss: 0.00002353
Iteration 99/1000 | Loss: 0.00002352
Iteration 100/1000 | Loss: 0.00002352
Iteration 101/1000 | Loss: 0.00002352
Iteration 102/1000 | Loss: 0.00002352
Iteration 103/1000 | Loss: 0.00002352
Iteration 104/1000 | Loss: 0.00002352
Iteration 105/1000 | Loss: 0.00002352
Iteration 106/1000 | Loss: 0.00002352
Iteration 107/1000 | Loss: 0.00002352
Iteration 108/1000 | Loss: 0.00002352
Iteration 109/1000 | Loss: 0.00002352
Iteration 110/1000 | Loss: 0.00002352
Iteration 111/1000 | Loss: 0.00002352
Iteration 112/1000 | Loss: 0.00002352
Iteration 113/1000 | Loss: 0.00002352
Iteration 114/1000 | Loss: 0.00002352
Iteration 115/1000 | Loss: 0.00002352
Iteration 116/1000 | Loss: 0.00002352
Iteration 117/1000 | Loss: 0.00002352
Iteration 118/1000 | Loss: 0.00002352
Iteration 119/1000 | Loss: 0.00002352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.3520104150520638e-05, 2.3520104150520638e-05, 2.3520104150520638e-05, 2.3520104150520638e-05, 2.3520104150520638e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3520104150520638e-05

Optimization complete. Final v2v error: 4.028106689453125 mm

Highest mean error: 4.68897819519043 mm for frame 172

Lowest mean error: 3.690932273864746 mm for frame 104

Saving results

Total time: 37.73613357543945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_026/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_026/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033055
Iteration 2/25 | Loss: 0.01033055
Iteration 3/25 | Loss: 0.01033054
Iteration 4/25 | Loss: 0.01033054
Iteration 5/25 | Loss: 0.01033054
Iteration 6/25 | Loss: 0.01033054
Iteration 7/25 | Loss: 0.01033054
Iteration 8/25 | Loss: 0.01033053
Iteration 9/25 | Loss: 0.01033053
Iteration 10/25 | Loss: 0.01033053
Iteration 11/25 | Loss: 0.01033053
Iteration 12/25 | Loss: 0.01033052
Iteration 13/25 | Loss: 0.01033052
Iteration 14/25 | Loss: 0.01033052
Iteration 15/25 | Loss: 0.01033052
Iteration 16/25 | Loss: 0.01033052
Iteration 17/25 | Loss: 0.01033051
Iteration 18/25 | Loss: 0.01033051
Iteration 19/25 | Loss: 0.01033051
Iteration 20/25 | Loss: 0.01033051
Iteration 21/25 | Loss: 0.01033050
Iteration 22/25 | Loss: 0.01033050
Iteration 23/25 | Loss: 0.01033050
Iteration 24/25 | Loss: 0.01033050
Iteration 25/25 | Loss: 0.01033050

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59773827
Iteration 2/25 | Loss: 0.17584567
Iteration 3/25 | Loss: 0.17347500
Iteration 4/25 | Loss: 0.17267133
Iteration 5/25 | Loss: 0.17267133
Iteration 6/25 | Loss: 0.17267130
Iteration 7/25 | Loss: 0.17267129
Iteration 8/25 | Loss: 0.17267129
Iteration 9/25 | Loss: 0.17267129
Iteration 10/25 | Loss: 0.17267129
Iteration 11/25 | Loss: 0.17267129
Iteration 12/25 | Loss: 0.17267127
Iteration 13/25 | Loss: 0.17267127
Iteration 14/25 | Loss: 0.17267127
Iteration 15/25 | Loss: 0.17267127
Iteration 16/25 | Loss: 0.17267127
Iteration 17/25 | Loss: 0.17267127
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.17267127335071564, 0.17267127335071564, 0.17267127335071564, 0.17267127335071564, 0.17267127335071564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17267127335071564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17267127
Iteration 2/1000 | Loss: 0.00658986
Iteration 3/1000 | Loss: 0.00244123
Iteration 4/1000 | Loss: 0.00106037
Iteration 5/1000 | Loss: 0.00073681
Iteration 6/1000 | Loss: 0.00104757
Iteration 7/1000 | Loss: 0.00048976
Iteration 8/1000 | Loss: 0.00041035
Iteration 9/1000 | Loss: 0.00036614
Iteration 10/1000 | Loss: 0.00086870
Iteration 11/1000 | Loss: 0.00278275
Iteration 12/1000 | Loss: 0.00167659
Iteration 13/1000 | Loss: 0.00333741
Iteration 14/1000 | Loss: 0.00316121
Iteration 15/1000 | Loss: 0.00032149
Iteration 16/1000 | Loss: 0.00015277
Iteration 17/1000 | Loss: 0.00011374
Iteration 18/1000 | Loss: 0.00014806
Iteration 19/1000 | Loss: 0.00056136
Iteration 20/1000 | Loss: 0.00005370
Iteration 21/1000 | Loss: 0.00032625
Iteration 22/1000 | Loss: 0.00027711
Iteration 23/1000 | Loss: 0.00003195
Iteration 24/1000 | Loss: 0.00002862
Iteration 25/1000 | Loss: 0.00002655
Iteration 26/1000 | Loss: 0.00006456
Iteration 27/1000 | Loss: 0.00006021
Iteration 28/1000 | Loss: 0.00002275
Iteration 29/1000 | Loss: 0.00008180
Iteration 30/1000 | Loss: 0.00004933
Iteration 31/1000 | Loss: 0.00008530
Iteration 32/1000 | Loss: 0.00002083
Iteration 33/1000 | Loss: 0.00001951
Iteration 34/1000 | Loss: 0.00015750
Iteration 35/1000 | Loss: 0.00002920
Iteration 36/1000 | Loss: 0.00002937
Iteration 37/1000 | Loss: 0.00001887
Iteration 38/1000 | Loss: 0.00001869
Iteration 39/1000 | Loss: 0.00017820
Iteration 40/1000 | Loss: 0.00002417
Iteration 41/1000 | Loss: 0.00003812
Iteration 42/1000 | Loss: 0.00001872
Iteration 43/1000 | Loss: 0.00001844
Iteration 44/1000 | Loss: 0.00001834
Iteration 45/1000 | Loss: 0.00005882
Iteration 46/1000 | Loss: 0.00002354
Iteration 47/1000 | Loss: 0.00004470
Iteration 48/1000 | Loss: 0.00001825
Iteration 49/1000 | Loss: 0.00001823
Iteration 50/1000 | Loss: 0.00001823
Iteration 51/1000 | Loss: 0.00001823
Iteration 52/1000 | Loss: 0.00001823
Iteration 53/1000 | Loss: 0.00001823
Iteration 54/1000 | Loss: 0.00001822
Iteration 55/1000 | Loss: 0.00001822
Iteration 56/1000 | Loss: 0.00001822
Iteration 57/1000 | Loss: 0.00001820
Iteration 58/1000 | Loss: 0.00001820
Iteration 59/1000 | Loss: 0.00001820
Iteration 60/1000 | Loss: 0.00001819
Iteration 61/1000 | Loss: 0.00001819
Iteration 62/1000 | Loss: 0.00001819
Iteration 63/1000 | Loss: 0.00001818
Iteration 64/1000 | Loss: 0.00001818
Iteration 65/1000 | Loss: 0.00001818
Iteration 66/1000 | Loss: 0.00001817
Iteration 67/1000 | Loss: 0.00001817
Iteration 68/1000 | Loss: 0.00001817
Iteration 69/1000 | Loss: 0.00001817
Iteration 70/1000 | Loss: 0.00001816
Iteration 71/1000 | Loss: 0.00001816
Iteration 72/1000 | Loss: 0.00001816
Iteration 73/1000 | Loss: 0.00001816
Iteration 74/1000 | Loss: 0.00001816
Iteration 75/1000 | Loss: 0.00001816
Iteration 76/1000 | Loss: 0.00001815
Iteration 77/1000 | Loss: 0.00001815
Iteration 78/1000 | Loss: 0.00001815
Iteration 79/1000 | Loss: 0.00001815
Iteration 80/1000 | Loss: 0.00001813
Iteration 81/1000 | Loss: 0.00001813
Iteration 82/1000 | Loss: 0.00001813
Iteration 83/1000 | Loss: 0.00001812
Iteration 84/1000 | Loss: 0.00001812
Iteration 85/1000 | Loss: 0.00001811
Iteration 86/1000 | Loss: 0.00001811
Iteration 87/1000 | Loss: 0.00001811
Iteration 88/1000 | Loss: 0.00001811
Iteration 89/1000 | Loss: 0.00001810
Iteration 90/1000 | Loss: 0.00001810
Iteration 91/1000 | Loss: 0.00001810
Iteration 92/1000 | Loss: 0.00001810
Iteration 93/1000 | Loss: 0.00001810
Iteration 94/1000 | Loss: 0.00001809
Iteration 95/1000 | Loss: 0.00001809
Iteration 96/1000 | Loss: 0.00001809
Iteration 97/1000 | Loss: 0.00001809
Iteration 98/1000 | Loss: 0.00001809
Iteration 99/1000 | Loss: 0.00001809
Iteration 100/1000 | Loss: 0.00001809
Iteration 101/1000 | Loss: 0.00001808
Iteration 102/1000 | Loss: 0.00001808
Iteration 103/1000 | Loss: 0.00001808
Iteration 104/1000 | Loss: 0.00001808
Iteration 105/1000 | Loss: 0.00001808
Iteration 106/1000 | Loss: 0.00001808
Iteration 107/1000 | Loss: 0.00001808
Iteration 108/1000 | Loss: 0.00001808
Iteration 109/1000 | Loss: 0.00001808
Iteration 110/1000 | Loss: 0.00001808
Iteration 111/1000 | Loss: 0.00001808
Iteration 112/1000 | Loss: 0.00001807
Iteration 113/1000 | Loss: 0.00001807
Iteration 114/1000 | Loss: 0.00001806
Iteration 115/1000 | Loss: 0.00001806
Iteration 116/1000 | Loss: 0.00001806
Iteration 117/1000 | Loss: 0.00001806
Iteration 118/1000 | Loss: 0.00001806
Iteration 119/1000 | Loss: 0.00001805
Iteration 120/1000 | Loss: 0.00001805
Iteration 121/1000 | Loss: 0.00001805
Iteration 122/1000 | Loss: 0.00001805
Iteration 123/1000 | Loss: 0.00001805
Iteration 124/1000 | Loss: 0.00001805
Iteration 125/1000 | Loss: 0.00001805
Iteration 126/1000 | Loss: 0.00001805
Iteration 127/1000 | Loss: 0.00001805
Iteration 128/1000 | Loss: 0.00001804
Iteration 129/1000 | Loss: 0.00001804
Iteration 130/1000 | Loss: 0.00001804
Iteration 131/1000 | Loss: 0.00001804
Iteration 132/1000 | Loss: 0.00001804
Iteration 133/1000 | Loss: 0.00001804
Iteration 134/1000 | Loss: 0.00001804
Iteration 135/1000 | Loss: 0.00001804
Iteration 136/1000 | Loss: 0.00001804
Iteration 137/1000 | Loss: 0.00001803
Iteration 138/1000 | Loss: 0.00001803
Iteration 139/1000 | Loss: 0.00001803
Iteration 140/1000 | Loss: 0.00001803
Iteration 141/1000 | Loss: 0.00001803
Iteration 142/1000 | Loss: 0.00001803
Iteration 143/1000 | Loss: 0.00001803
Iteration 144/1000 | Loss: 0.00001803
Iteration 145/1000 | Loss: 0.00001803
Iteration 146/1000 | Loss: 0.00001802
Iteration 147/1000 | Loss: 0.00001802
Iteration 148/1000 | Loss: 0.00001802
Iteration 149/1000 | Loss: 0.00001802
Iteration 150/1000 | Loss: 0.00001802
Iteration 151/1000 | Loss: 0.00001802
Iteration 152/1000 | Loss: 0.00001802
Iteration 153/1000 | Loss: 0.00001802
Iteration 154/1000 | Loss: 0.00001802
Iteration 155/1000 | Loss: 0.00001802
Iteration 156/1000 | Loss: 0.00001802
Iteration 157/1000 | Loss: 0.00001801
Iteration 158/1000 | Loss: 0.00001801
Iteration 159/1000 | Loss: 0.00001801
Iteration 160/1000 | Loss: 0.00001801
Iteration 161/1000 | Loss: 0.00001801
Iteration 162/1000 | Loss: 0.00001801
Iteration 163/1000 | Loss: 0.00001801
Iteration 164/1000 | Loss: 0.00001801
Iteration 165/1000 | Loss: 0.00001801
Iteration 166/1000 | Loss: 0.00001801
Iteration 167/1000 | Loss: 0.00001801
Iteration 168/1000 | Loss: 0.00001801
Iteration 169/1000 | Loss: 0.00001801
Iteration 170/1000 | Loss: 0.00001800
Iteration 171/1000 | Loss: 0.00001800
Iteration 172/1000 | Loss: 0.00001800
Iteration 173/1000 | Loss: 0.00001800
Iteration 174/1000 | Loss: 0.00001800
Iteration 175/1000 | Loss: 0.00001800
Iteration 176/1000 | Loss: 0.00001799
Iteration 177/1000 | Loss: 0.00001799
Iteration 178/1000 | Loss: 0.00001799
Iteration 179/1000 | Loss: 0.00001799
Iteration 180/1000 | Loss: 0.00001799
Iteration 181/1000 | Loss: 0.00001799
Iteration 182/1000 | Loss: 0.00001799
Iteration 183/1000 | Loss: 0.00001798
Iteration 184/1000 | Loss: 0.00001798
Iteration 185/1000 | Loss: 0.00001798
Iteration 186/1000 | Loss: 0.00001798
Iteration 187/1000 | Loss: 0.00001798
Iteration 188/1000 | Loss: 0.00001798
Iteration 189/1000 | Loss: 0.00001798
Iteration 190/1000 | Loss: 0.00001798
Iteration 191/1000 | Loss: 0.00001798
Iteration 192/1000 | Loss: 0.00001798
Iteration 193/1000 | Loss: 0.00001798
Iteration 194/1000 | Loss: 0.00001798
Iteration 195/1000 | Loss: 0.00001797
Iteration 196/1000 | Loss: 0.00001797
Iteration 197/1000 | Loss: 0.00001797
Iteration 198/1000 | Loss: 0.00001797
Iteration 199/1000 | Loss: 0.00001797
Iteration 200/1000 | Loss: 0.00001797
Iteration 201/1000 | Loss: 0.00001797
Iteration 202/1000 | Loss: 0.00001797
Iteration 203/1000 | Loss: 0.00001797
Iteration 204/1000 | Loss: 0.00001797
Iteration 205/1000 | Loss: 0.00001797
Iteration 206/1000 | Loss: 0.00001797
Iteration 207/1000 | Loss: 0.00001797
Iteration 208/1000 | Loss: 0.00001797
Iteration 209/1000 | Loss: 0.00001797
Iteration 210/1000 | Loss: 0.00001797
Iteration 211/1000 | Loss: 0.00001797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.797039294615388e-05, 1.797039294615388e-05, 1.797039294615388e-05, 1.797039294615388e-05, 1.797039294615388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.797039294615388e-05

Optimization complete. Final v2v error: 3.6160426139831543 mm

Highest mean error: 3.993398904800415 mm for frame 18

Lowest mean error: 3.341827392578125 mm for frame 158

Saving results

Total time: 95.92045187950134
