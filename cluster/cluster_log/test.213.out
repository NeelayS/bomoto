Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=213, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11928-11983
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411427
Iteration 2/25 | Loss: 0.00131548
Iteration 3/25 | Loss: 0.00114022
Iteration 4/25 | Loss: 0.00110682
Iteration 5/25 | Loss: 0.00109902
Iteration 6/25 | Loss: 0.00109842
Iteration 7/25 | Loss: 0.00109842
Iteration 8/25 | Loss: 0.00109842
Iteration 9/25 | Loss: 0.00109842
Iteration 10/25 | Loss: 0.00109842
Iteration 11/25 | Loss: 0.00109842
Iteration 12/25 | Loss: 0.00109842
Iteration 13/25 | Loss: 0.00109842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010984156979247928, 0.0010984156979247928, 0.0010984156979247928, 0.0010984156979247928, 0.0010984156979247928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010984156979247928

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56826651
Iteration 2/25 | Loss: 0.00057599
Iteration 3/25 | Loss: 0.00057599
Iteration 4/25 | Loss: 0.00057599
Iteration 5/25 | Loss: 0.00057599
Iteration 6/25 | Loss: 0.00057599
Iteration 7/25 | Loss: 0.00057599
Iteration 8/25 | Loss: 0.00057599
Iteration 9/25 | Loss: 0.00057599
Iteration 10/25 | Loss: 0.00057599
Iteration 11/25 | Loss: 0.00057599
Iteration 12/25 | Loss: 0.00057599
Iteration 13/25 | Loss: 0.00057599
Iteration 14/25 | Loss: 0.00057599
Iteration 15/25 | Loss: 0.00057599
Iteration 16/25 | Loss: 0.00057599
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005759858759120107, 0.0005759858759120107, 0.0005759858759120107, 0.0005759858759120107, 0.0005759858759120107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005759858759120107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057599
Iteration 2/1000 | Loss: 0.00004501
Iteration 3/1000 | Loss: 0.00002671
Iteration 4/1000 | Loss: 0.00002053
Iteration 5/1000 | Loss: 0.00001745
Iteration 6/1000 | Loss: 0.00001619
Iteration 7/1000 | Loss: 0.00001553
Iteration 8/1000 | Loss: 0.00001510
Iteration 9/1000 | Loss: 0.00001477
Iteration 10/1000 | Loss: 0.00001470
Iteration 11/1000 | Loss: 0.00001447
Iteration 12/1000 | Loss: 0.00001433
Iteration 13/1000 | Loss: 0.00001429
Iteration 14/1000 | Loss: 0.00001418
Iteration 15/1000 | Loss: 0.00001413
Iteration 16/1000 | Loss: 0.00001403
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001388
Iteration 19/1000 | Loss: 0.00001384
Iteration 20/1000 | Loss: 0.00001383
Iteration 21/1000 | Loss: 0.00001381
Iteration 22/1000 | Loss: 0.00001371
Iteration 23/1000 | Loss: 0.00001371
Iteration 24/1000 | Loss: 0.00001370
Iteration 25/1000 | Loss: 0.00001368
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001367
Iteration 28/1000 | Loss: 0.00001367
Iteration 29/1000 | Loss: 0.00001367
Iteration 30/1000 | Loss: 0.00001366
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001366
Iteration 33/1000 | Loss: 0.00001365
Iteration 34/1000 | Loss: 0.00001365
Iteration 35/1000 | Loss: 0.00001365
Iteration 36/1000 | Loss: 0.00001364
Iteration 37/1000 | Loss: 0.00001364
Iteration 38/1000 | Loss: 0.00001364
Iteration 39/1000 | Loss: 0.00001364
Iteration 40/1000 | Loss: 0.00001364
Iteration 41/1000 | Loss: 0.00001364
Iteration 42/1000 | Loss: 0.00001364
Iteration 43/1000 | Loss: 0.00001364
Iteration 44/1000 | Loss: 0.00001364
Iteration 45/1000 | Loss: 0.00001364
Iteration 46/1000 | Loss: 0.00001363
Iteration 47/1000 | Loss: 0.00001363
Iteration 48/1000 | Loss: 0.00001363
Iteration 49/1000 | Loss: 0.00001363
Iteration 50/1000 | Loss: 0.00001362
Iteration 51/1000 | Loss: 0.00001362
Iteration 52/1000 | Loss: 0.00001362
Iteration 53/1000 | Loss: 0.00001362
Iteration 54/1000 | Loss: 0.00001362
Iteration 55/1000 | Loss: 0.00001362
Iteration 56/1000 | Loss: 0.00001362
Iteration 57/1000 | Loss: 0.00001362
Iteration 58/1000 | Loss: 0.00001362
Iteration 59/1000 | Loss: 0.00001361
Iteration 60/1000 | Loss: 0.00001361
Iteration 61/1000 | Loss: 0.00001361
Iteration 62/1000 | Loss: 0.00001361
Iteration 63/1000 | Loss: 0.00001361
Iteration 64/1000 | Loss: 0.00001360
Iteration 65/1000 | Loss: 0.00001360
Iteration 66/1000 | Loss: 0.00001360
Iteration 67/1000 | Loss: 0.00001359
Iteration 68/1000 | Loss: 0.00001359
Iteration 69/1000 | Loss: 0.00001358
Iteration 70/1000 | Loss: 0.00001358
Iteration 71/1000 | Loss: 0.00001358
Iteration 72/1000 | Loss: 0.00001358
Iteration 73/1000 | Loss: 0.00001358
Iteration 74/1000 | Loss: 0.00001358
Iteration 75/1000 | Loss: 0.00001357
Iteration 76/1000 | Loss: 0.00001357
Iteration 77/1000 | Loss: 0.00001357
Iteration 78/1000 | Loss: 0.00001357
Iteration 79/1000 | Loss: 0.00001356
Iteration 80/1000 | Loss: 0.00001356
Iteration 81/1000 | Loss: 0.00001356
Iteration 82/1000 | Loss: 0.00001356
Iteration 83/1000 | Loss: 0.00001356
Iteration 84/1000 | Loss: 0.00001356
Iteration 85/1000 | Loss: 0.00001356
Iteration 86/1000 | Loss: 0.00001356
Iteration 87/1000 | Loss: 0.00001356
Iteration 88/1000 | Loss: 0.00001355
Iteration 89/1000 | Loss: 0.00001355
Iteration 90/1000 | Loss: 0.00001355
Iteration 91/1000 | Loss: 0.00001355
Iteration 92/1000 | Loss: 0.00001355
Iteration 93/1000 | Loss: 0.00001355
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001355
Iteration 100/1000 | Loss: 0.00001354
Iteration 101/1000 | Loss: 0.00001354
Iteration 102/1000 | Loss: 0.00001354
Iteration 103/1000 | Loss: 0.00001353
Iteration 104/1000 | Loss: 0.00001353
Iteration 105/1000 | Loss: 0.00001353
Iteration 106/1000 | Loss: 0.00001352
Iteration 107/1000 | Loss: 0.00001352
Iteration 108/1000 | Loss: 0.00001352
Iteration 109/1000 | Loss: 0.00001351
Iteration 110/1000 | Loss: 0.00001351
Iteration 111/1000 | Loss: 0.00001351
Iteration 112/1000 | Loss: 0.00001351
Iteration 113/1000 | Loss: 0.00001351
Iteration 114/1000 | Loss: 0.00001351
Iteration 115/1000 | Loss: 0.00001351
Iteration 116/1000 | Loss: 0.00001351
Iteration 117/1000 | Loss: 0.00001350
Iteration 118/1000 | Loss: 0.00001350
Iteration 119/1000 | Loss: 0.00001350
Iteration 120/1000 | Loss: 0.00001350
Iteration 121/1000 | Loss: 0.00001350
Iteration 122/1000 | Loss: 0.00001350
Iteration 123/1000 | Loss: 0.00001350
Iteration 124/1000 | Loss: 0.00001350
Iteration 125/1000 | Loss: 0.00001350
Iteration 126/1000 | Loss: 0.00001350
Iteration 127/1000 | Loss: 0.00001350
Iteration 128/1000 | Loss: 0.00001350
Iteration 129/1000 | Loss: 0.00001350
Iteration 130/1000 | Loss: 0.00001349
Iteration 131/1000 | Loss: 0.00001349
Iteration 132/1000 | Loss: 0.00001349
Iteration 133/1000 | Loss: 0.00001349
Iteration 134/1000 | Loss: 0.00001349
Iteration 135/1000 | Loss: 0.00001349
Iteration 136/1000 | Loss: 0.00001348
Iteration 137/1000 | Loss: 0.00001348
Iteration 138/1000 | Loss: 0.00001348
Iteration 139/1000 | Loss: 0.00001348
Iteration 140/1000 | Loss: 0.00001348
Iteration 141/1000 | Loss: 0.00001348
Iteration 142/1000 | Loss: 0.00001348
Iteration 143/1000 | Loss: 0.00001347
Iteration 144/1000 | Loss: 0.00001347
Iteration 145/1000 | Loss: 0.00001347
Iteration 146/1000 | Loss: 0.00001347
Iteration 147/1000 | Loss: 0.00001347
Iteration 148/1000 | Loss: 0.00001347
Iteration 149/1000 | Loss: 0.00001347
Iteration 150/1000 | Loss: 0.00001347
Iteration 151/1000 | Loss: 0.00001347
Iteration 152/1000 | Loss: 0.00001347
Iteration 153/1000 | Loss: 0.00001346
Iteration 154/1000 | Loss: 0.00001346
Iteration 155/1000 | Loss: 0.00001346
Iteration 156/1000 | Loss: 0.00001346
Iteration 157/1000 | Loss: 0.00001346
Iteration 158/1000 | Loss: 0.00001346
Iteration 159/1000 | Loss: 0.00001346
Iteration 160/1000 | Loss: 0.00001346
Iteration 161/1000 | Loss: 0.00001346
Iteration 162/1000 | Loss: 0.00001345
Iteration 163/1000 | Loss: 0.00001345
Iteration 164/1000 | Loss: 0.00001345
Iteration 165/1000 | Loss: 0.00001345
Iteration 166/1000 | Loss: 0.00001345
Iteration 167/1000 | Loss: 0.00001345
Iteration 168/1000 | Loss: 0.00001345
Iteration 169/1000 | Loss: 0.00001345
Iteration 170/1000 | Loss: 0.00001345
Iteration 171/1000 | Loss: 0.00001345
Iteration 172/1000 | Loss: 0.00001345
Iteration 173/1000 | Loss: 0.00001345
Iteration 174/1000 | Loss: 0.00001345
Iteration 175/1000 | Loss: 0.00001345
Iteration 176/1000 | Loss: 0.00001345
Iteration 177/1000 | Loss: 0.00001344
Iteration 178/1000 | Loss: 0.00001344
Iteration 179/1000 | Loss: 0.00001344
Iteration 180/1000 | Loss: 0.00001344
Iteration 181/1000 | Loss: 0.00001344
Iteration 182/1000 | Loss: 0.00001344
Iteration 183/1000 | Loss: 0.00001344
Iteration 184/1000 | Loss: 0.00001344
Iteration 185/1000 | Loss: 0.00001344
Iteration 186/1000 | Loss: 0.00001344
Iteration 187/1000 | Loss: 0.00001344
Iteration 188/1000 | Loss: 0.00001344
Iteration 189/1000 | Loss: 0.00001344
Iteration 190/1000 | Loss: 0.00001344
Iteration 191/1000 | Loss: 0.00001344
Iteration 192/1000 | Loss: 0.00001344
Iteration 193/1000 | Loss: 0.00001344
Iteration 194/1000 | Loss: 0.00001344
Iteration 195/1000 | Loss: 0.00001344
Iteration 196/1000 | Loss: 0.00001344
Iteration 197/1000 | Loss: 0.00001344
Iteration 198/1000 | Loss: 0.00001344
Iteration 199/1000 | Loss: 0.00001344
Iteration 200/1000 | Loss: 0.00001344
Iteration 201/1000 | Loss: 0.00001344
Iteration 202/1000 | Loss: 0.00001344
Iteration 203/1000 | Loss: 0.00001344
Iteration 204/1000 | Loss: 0.00001344
Iteration 205/1000 | Loss: 0.00001344
Iteration 206/1000 | Loss: 0.00001344
Iteration 207/1000 | Loss: 0.00001344
Iteration 208/1000 | Loss: 0.00001344
Iteration 209/1000 | Loss: 0.00001344
Iteration 210/1000 | Loss: 0.00001344
Iteration 211/1000 | Loss: 0.00001344
Iteration 212/1000 | Loss: 0.00001344
Iteration 213/1000 | Loss: 0.00001344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.3444289834296796e-05, 1.3444289834296796e-05, 1.3444289834296796e-05, 1.3444289834296796e-05, 1.3444289834296796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3444289834296796e-05

Optimization complete. Final v2v error: 3.240755081176758 mm

Highest mean error: 3.6503424644470215 mm for frame 218

Lowest mean error: 2.856642961502075 mm for frame 263

Saving results

Total time: 48.530842781066895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973569
Iteration 2/25 | Loss: 0.00226404
Iteration 3/25 | Loss: 0.00174757
Iteration 4/25 | Loss: 0.00168012
Iteration 5/25 | Loss: 0.00153248
Iteration 6/25 | Loss: 0.00126774
Iteration 7/25 | Loss: 0.00115455
Iteration 8/25 | Loss: 0.00112112
Iteration 9/25 | Loss: 0.00111676
Iteration 10/25 | Loss: 0.00111560
Iteration 11/25 | Loss: 0.00111530
Iteration 12/25 | Loss: 0.00111514
Iteration 13/25 | Loss: 0.00111510
Iteration 14/25 | Loss: 0.00111510
Iteration 15/25 | Loss: 0.00111510
Iteration 16/25 | Loss: 0.00111510
Iteration 17/25 | Loss: 0.00111510
Iteration 18/25 | Loss: 0.00111510
Iteration 19/25 | Loss: 0.00111510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011151038343086839, 0.0011151038343086839, 0.0011151038343086839, 0.0011151038343086839, 0.0011151038343086839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011151038343086839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37022996
Iteration 2/25 | Loss: 0.00062558
Iteration 3/25 | Loss: 0.00062558
Iteration 4/25 | Loss: 0.00062558
Iteration 5/25 | Loss: 0.00062558
Iteration 6/25 | Loss: 0.00062558
Iteration 7/25 | Loss: 0.00062558
Iteration 8/25 | Loss: 0.00062558
Iteration 9/25 | Loss: 0.00062558
Iteration 10/25 | Loss: 0.00062558
Iteration 11/25 | Loss: 0.00062558
Iteration 12/25 | Loss: 0.00062558
Iteration 13/25 | Loss: 0.00062558
Iteration 14/25 | Loss: 0.00062558
Iteration 15/25 | Loss: 0.00062558
Iteration 16/25 | Loss: 0.00062558
Iteration 17/25 | Loss: 0.00062558
Iteration 18/25 | Loss: 0.00062558
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006255761836655438, 0.0006255761836655438, 0.0006255761836655438, 0.0006255761836655438, 0.0006255761836655438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006255761836655438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062558
Iteration 2/1000 | Loss: 0.00007224
Iteration 3/1000 | Loss: 0.00004697
Iteration 4/1000 | Loss: 0.00040110
Iteration 5/1000 | Loss: 0.00005918
Iteration 6/1000 | Loss: 0.00003970
Iteration 7/1000 | Loss: 0.00003253
Iteration 8/1000 | Loss: 0.00002828
Iteration 9/1000 | Loss: 0.00002497
Iteration 10/1000 | Loss: 0.00002272
Iteration 11/1000 | Loss: 0.00002150
Iteration 12/1000 | Loss: 0.00002031
Iteration 13/1000 | Loss: 0.00001965
Iteration 14/1000 | Loss: 0.00001916
Iteration 15/1000 | Loss: 0.00001888
Iteration 16/1000 | Loss: 0.00001857
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001800
Iteration 19/1000 | Loss: 0.00001790
Iteration 20/1000 | Loss: 0.00001778
Iteration 21/1000 | Loss: 0.00001757
Iteration 22/1000 | Loss: 0.00001757
Iteration 23/1000 | Loss: 0.00001752
Iteration 24/1000 | Loss: 0.00001747
Iteration 25/1000 | Loss: 0.00001746
Iteration 26/1000 | Loss: 0.00001741
Iteration 27/1000 | Loss: 0.00001739
Iteration 28/1000 | Loss: 0.00001739
Iteration 29/1000 | Loss: 0.00001739
Iteration 30/1000 | Loss: 0.00001738
Iteration 31/1000 | Loss: 0.00001738
Iteration 32/1000 | Loss: 0.00001737
Iteration 33/1000 | Loss: 0.00001737
Iteration 34/1000 | Loss: 0.00001737
Iteration 35/1000 | Loss: 0.00001735
Iteration 36/1000 | Loss: 0.00001735
Iteration 37/1000 | Loss: 0.00001734
Iteration 38/1000 | Loss: 0.00001733
Iteration 39/1000 | Loss: 0.00001732
Iteration 40/1000 | Loss: 0.00001726
Iteration 41/1000 | Loss: 0.00001726
Iteration 42/1000 | Loss: 0.00001726
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001724
Iteration 45/1000 | Loss: 0.00001724
Iteration 46/1000 | Loss: 0.00001724
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001722
Iteration 50/1000 | Loss: 0.00001722
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001721
Iteration 53/1000 | Loss: 0.00001720
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001719
Iteration 56/1000 | Loss: 0.00001719
Iteration 57/1000 | Loss: 0.00001718
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001715
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001713
Iteration 64/1000 | Loss: 0.00001713
Iteration 65/1000 | Loss: 0.00001713
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001712
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001712
Iteration 74/1000 | Loss: 0.00001712
Iteration 75/1000 | Loss: 0.00001712
Iteration 76/1000 | Loss: 0.00001712
Iteration 77/1000 | Loss: 0.00001712
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001711
Iteration 82/1000 | Loss: 0.00001711
Iteration 83/1000 | Loss: 0.00001711
Iteration 84/1000 | Loss: 0.00001711
Iteration 85/1000 | Loss: 0.00001711
Iteration 86/1000 | Loss: 0.00001711
Iteration 87/1000 | Loss: 0.00001711
Iteration 88/1000 | Loss: 0.00001711
Iteration 89/1000 | Loss: 0.00001711
Iteration 90/1000 | Loss: 0.00001711
Iteration 91/1000 | Loss: 0.00001711
Iteration 92/1000 | Loss: 0.00001710
Iteration 93/1000 | Loss: 0.00001710
Iteration 94/1000 | Loss: 0.00001710
Iteration 95/1000 | Loss: 0.00001710
Iteration 96/1000 | Loss: 0.00001709
Iteration 97/1000 | Loss: 0.00001709
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001708
Iteration 101/1000 | Loss: 0.00001708
Iteration 102/1000 | Loss: 0.00001708
Iteration 103/1000 | Loss: 0.00001708
Iteration 104/1000 | Loss: 0.00001707
Iteration 105/1000 | Loss: 0.00001707
Iteration 106/1000 | Loss: 0.00001707
Iteration 107/1000 | Loss: 0.00001707
Iteration 108/1000 | Loss: 0.00001707
Iteration 109/1000 | Loss: 0.00001707
Iteration 110/1000 | Loss: 0.00001707
Iteration 111/1000 | Loss: 0.00001707
Iteration 112/1000 | Loss: 0.00001707
Iteration 113/1000 | Loss: 0.00001707
Iteration 114/1000 | Loss: 0.00001707
Iteration 115/1000 | Loss: 0.00001707
Iteration 116/1000 | Loss: 0.00001707
Iteration 117/1000 | Loss: 0.00001707
Iteration 118/1000 | Loss: 0.00001707
Iteration 119/1000 | Loss: 0.00001706
Iteration 120/1000 | Loss: 0.00001706
Iteration 121/1000 | Loss: 0.00001706
Iteration 122/1000 | Loss: 0.00001706
Iteration 123/1000 | Loss: 0.00001706
Iteration 124/1000 | Loss: 0.00001706
Iteration 125/1000 | Loss: 0.00001705
Iteration 126/1000 | Loss: 0.00001705
Iteration 127/1000 | Loss: 0.00001705
Iteration 128/1000 | Loss: 0.00001704
Iteration 129/1000 | Loss: 0.00001704
Iteration 130/1000 | Loss: 0.00001704
Iteration 131/1000 | Loss: 0.00001704
Iteration 132/1000 | Loss: 0.00001703
Iteration 133/1000 | Loss: 0.00001702
Iteration 134/1000 | Loss: 0.00001702
Iteration 135/1000 | Loss: 0.00001702
Iteration 136/1000 | Loss: 0.00001701
Iteration 137/1000 | Loss: 0.00001701
Iteration 138/1000 | Loss: 0.00001701
Iteration 139/1000 | Loss: 0.00001701
Iteration 140/1000 | Loss: 0.00001701
Iteration 141/1000 | Loss: 0.00001701
Iteration 142/1000 | Loss: 0.00001701
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001701
Iteration 145/1000 | Loss: 0.00001700
Iteration 146/1000 | Loss: 0.00001700
Iteration 147/1000 | Loss: 0.00001700
Iteration 148/1000 | Loss: 0.00001700
Iteration 149/1000 | Loss: 0.00001700
Iteration 150/1000 | Loss: 0.00001700
Iteration 151/1000 | Loss: 0.00001700
Iteration 152/1000 | Loss: 0.00001699
Iteration 153/1000 | Loss: 0.00001699
Iteration 154/1000 | Loss: 0.00001699
Iteration 155/1000 | Loss: 0.00001699
Iteration 156/1000 | Loss: 0.00001699
Iteration 157/1000 | Loss: 0.00001699
Iteration 158/1000 | Loss: 0.00001699
Iteration 159/1000 | Loss: 0.00001699
Iteration 160/1000 | Loss: 0.00001699
Iteration 161/1000 | Loss: 0.00001699
Iteration 162/1000 | Loss: 0.00001699
Iteration 163/1000 | Loss: 0.00001699
Iteration 164/1000 | Loss: 0.00001699
Iteration 165/1000 | Loss: 0.00001699
Iteration 166/1000 | Loss: 0.00001699
Iteration 167/1000 | Loss: 0.00001698
Iteration 168/1000 | Loss: 0.00001698
Iteration 169/1000 | Loss: 0.00001698
Iteration 170/1000 | Loss: 0.00001698
Iteration 171/1000 | Loss: 0.00001698
Iteration 172/1000 | Loss: 0.00001698
Iteration 173/1000 | Loss: 0.00001698
Iteration 174/1000 | Loss: 0.00001698
Iteration 175/1000 | Loss: 0.00001698
Iteration 176/1000 | Loss: 0.00001698
Iteration 177/1000 | Loss: 0.00001698
Iteration 178/1000 | Loss: 0.00001698
Iteration 179/1000 | Loss: 0.00001698
Iteration 180/1000 | Loss: 0.00001698
Iteration 181/1000 | Loss: 0.00001698
Iteration 182/1000 | Loss: 0.00001698
Iteration 183/1000 | Loss: 0.00001698
Iteration 184/1000 | Loss: 0.00001698
Iteration 185/1000 | Loss: 0.00001698
Iteration 186/1000 | Loss: 0.00001698
Iteration 187/1000 | Loss: 0.00001698
Iteration 188/1000 | Loss: 0.00001698
Iteration 189/1000 | Loss: 0.00001698
Iteration 190/1000 | Loss: 0.00001698
Iteration 191/1000 | Loss: 0.00001698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.6980842701741494e-05, 1.6980842701741494e-05, 1.6980842701741494e-05, 1.6980842701741494e-05, 1.6980842701741494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6980842701741494e-05

Optimization complete. Final v2v error: 3.426071882247925 mm

Highest mean error: 6.1885786056518555 mm for frame 45

Lowest mean error: 3.156470775604248 mm for frame 1

Saving results

Total time: 62.62525653839111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413190
Iteration 2/25 | Loss: 0.00115250
Iteration 3/25 | Loss: 0.00102720
Iteration 4/25 | Loss: 0.00101837
Iteration 5/25 | Loss: 0.00101658
Iteration 6/25 | Loss: 0.00101658
Iteration 7/25 | Loss: 0.00101658
Iteration 8/25 | Loss: 0.00101658
Iteration 9/25 | Loss: 0.00101658
Iteration 10/25 | Loss: 0.00101658
Iteration 11/25 | Loss: 0.00101658
Iteration 12/25 | Loss: 0.00101658
Iteration 13/25 | Loss: 0.00101658
Iteration 14/25 | Loss: 0.00101658
Iteration 15/25 | Loss: 0.00101658
Iteration 16/25 | Loss: 0.00101658
Iteration 17/25 | Loss: 0.00101658
Iteration 18/25 | Loss: 0.00101658
Iteration 19/25 | Loss: 0.00101658
Iteration 20/25 | Loss: 0.00101658
Iteration 21/25 | Loss: 0.00101658
Iteration 22/25 | Loss: 0.00101658
Iteration 23/25 | Loss: 0.00101658
Iteration 24/25 | Loss: 0.00101658
Iteration 25/25 | Loss: 0.00101658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37588596
Iteration 2/25 | Loss: 0.00050577
Iteration 3/25 | Loss: 0.00050577
Iteration 4/25 | Loss: 0.00050576
Iteration 5/25 | Loss: 0.00050576
Iteration 6/25 | Loss: 0.00050576
Iteration 7/25 | Loss: 0.00050576
Iteration 8/25 | Loss: 0.00050576
Iteration 9/25 | Loss: 0.00050576
Iteration 10/25 | Loss: 0.00050576
Iteration 11/25 | Loss: 0.00050576
Iteration 12/25 | Loss: 0.00050576
Iteration 13/25 | Loss: 0.00050576
Iteration 14/25 | Loss: 0.00050576
Iteration 15/25 | Loss: 0.00050576
Iteration 16/25 | Loss: 0.00050576
Iteration 17/25 | Loss: 0.00050576
Iteration 18/25 | Loss: 0.00050576
Iteration 19/25 | Loss: 0.00050576
Iteration 20/25 | Loss: 0.00050576
Iteration 21/25 | Loss: 0.00050576
Iteration 22/25 | Loss: 0.00050576
Iteration 23/25 | Loss: 0.00050576
Iteration 24/25 | Loss: 0.00050576
Iteration 25/25 | Loss: 0.00050576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050576
Iteration 2/1000 | Loss: 0.00003584
Iteration 3/1000 | Loss: 0.00001962
Iteration 4/1000 | Loss: 0.00001678
Iteration 5/1000 | Loss: 0.00001504
Iteration 6/1000 | Loss: 0.00001430
Iteration 7/1000 | Loss: 0.00001398
Iteration 8/1000 | Loss: 0.00001372
Iteration 9/1000 | Loss: 0.00001347
Iteration 10/1000 | Loss: 0.00001334
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001321
Iteration 13/1000 | Loss: 0.00001311
Iteration 14/1000 | Loss: 0.00001310
Iteration 15/1000 | Loss: 0.00001310
Iteration 16/1000 | Loss: 0.00001309
Iteration 17/1000 | Loss: 0.00001308
Iteration 18/1000 | Loss: 0.00001307
Iteration 19/1000 | Loss: 0.00001307
Iteration 20/1000 | Loss: 0.00001307
Iteration 21/1000 | Loss: 0.00001307
Iteration 22/1000 | Loss: 0.00001306
Iteration 23/1000 | Loss: 0.00001306
Iteration 24/1000 | Loss: 0.00001306
Iteration 25/1000 | Loss: 0.00001305
Iteration 26/1000 | Loss: 0.00001305
Iteration 27/1000 | Loss: 0.00001305
Iteration 28/1000 | Loss: 0.00001305
Iteration 29/1000 | Loss: 0.00001305
Iteration 30/1000 | Loss: 0.00001304
Iteration 31/1000 | Loss: 0.00001304
Iteration 32/1000 | Loss: 0.00001304
Iteration 33/1000 | Loss: 0.00001304
Iteration 34/1000 | Loss: 0.00001304
Iteration 35/1000 | Loss: 0.00001304
Iteration 36/1000 | Loss: 0.00001304
Iteration 37/1000 | Loss: 0.00001304
Iteration 38/1000 | Loss: 0.00001304
Iteration 39/1000 | Loss: 0.00001304
Iteration 40/1000 | Loss: 0.00001304
Iteration 41/1000 | Loss: 0.00001303
Iteration 42/1000 | Loss: 0.00001303
Iteration 43/1000 | Loss: 0.00001303
Iteration 44/1000 | Loss: 0.00001303
Iteration 45/1000 | Loss: 0.00001303
Iteration 46/1000 | Loss: 0.00001302
Iteration 47/1000 | Loss: 0.00001302
Iteration 48/1000 | Loss: 0.00001302
Iteration 49/1000 | Loss: 0.00001302
Iteration 50/1000 | Loss: 0.00001302
Iteration 51/1000 | Loss: 0.00001302
Iteration 52/1000 | Loss: 0.00001302
Iteration 53/1000 | Loss: 0.00001302
Iteration 54/1000 | Loss: 0.00001302
Iteration 55/1000 | Loss: 0.00001302
Iteration 56/1000 | Loss: 0.00001302
Iteration 57/1000 | Loss: 0.00001302
Iteration 58/1000 | Loss: 0.00001302
Iteration 59/1000 | Loss: 0.00001301
Iteration 60/1000 | Loss: 0.00001301
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001301
Iteration 63/1000 | Loss: 0.00001301
Iteration 64/1000 | Loss: 0.00001301
Iteration 65/1000 | Loss: 0.00001301
Iteration 66/1000 | Loss: 0.00001301
Iteration 67/1000 | Loss: 0.00001301
Iteration 68/1000 | Loss: 0.00001301
Iteration 69/1000 | Loss: 0.00001301
Iteration 70/1000 | Loss: 0.00001300
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001300
Iteration 73/1000 | Loss: 0.00001300
Iteration 74/1000 | Loss: 0.00001300
Iteration 75/1000 | Loss: 0.00001300
Iteration 76/1000 | Loss: 0.00001300
Iteration 77/1000 | Loss: 0.00001300
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001300
Iteration 81/1000 | Loss: 0.00001300
Iteration 82/1000 | Loss: 0.00001300
Iteration 83/1000 | Loss: 0.00001300
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001299
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001299
Iteration 89/1000 | Loss: 0.00001299
Iteration 90/1000 | Loss: 0.00001299
Iteration 91/1000 | Loss: 0.00001299
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001299
Iteration 94/1000 | Loss: 0.00001299
Iteration 95/1000 | Loss: 0.00001299
Iteration 96/1000 | Loss: 0.00001299
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001298
Iteration 100/1000 | Loss: 0.00001298
Iteration 101/1000 | Loss: 0.00001298
Iteration 102/1000 | Loss: 0.00001298
Iteration 103/1000 | Loss: 0.00001298
Iteration 104/1000 | Loss: 0.00001298
Iteration 105/1000 | Loss: 0.00001298
Iteration 106/1000 | Loss: 0.00001297
Iteration 107/1000 | Loss: 0.00001297
Iteration 108/1000 | Loss: 0.00001297
Iteration 109/1000 | Loss: 0.00001297
Iteration 110/1000 | Loss: 0.00001297
Iteration 111/1000 | Loss: 0.00001296
Iteration 112/1000 | Loss: 0.00001296
Iteration 113/1000 | Loss: 0.00001296
Iteration 114/1000 | Loss: 0.00001296
Iteration 115/1000 | Loss: 0.00001296
Iteration 116/1000 | Loss: 0.00001295
Iteration 117/1000 | Loss: 0.00001295
Iteration 118/1000 | Loss: 0.00001295
Iteration 119/1000 | Loss: 0.00001294
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001294
Iteration 123/1000 | Loss: 0.00001294
Iteration 124/1000 | Loss: 0.00001294
Iteration 125/1000 | Loss: 0.00001294
Iteration 126/1000 | Loss: 0.00001294
Iteration 127/1000 | Loss: 0.00001293
Iteration 128/1000 | Loss: 0.00001293
Iteration 129/1000 | Loss: 0.00001293
Iteration 130/1000 | Loss: 0.00001292
Iteration 131/1000 | Loss: 0.00001292
Iteration 132/1000 | Loss: 0.00001292
Iteration 133/1000 | Loss: 0.00001292
Iteration 134/1000 | Loss: 0.00001292
Iteration 135/1000 | Loss: 0.00001292
Iteration 136/1000 | Loss: 0.00001292
Iteration 137/1000 | Loss: 0.00001292
Iteration 138/1000 | Loss: 0.00001292
Iteration 139/1000 | Loss: 0.00001291
Iteration 140/1000 | Loss: 0.00001291
Iteration 141/1000 | Loss: 0.00001291
Iteration 142/1000 | Loss: 0.00001291
Iteration 143/1000 | Loss: 0.00001291
Iteration 144/1000 | Loss: 0.00001291
Iteration 145/1000 | Loss: 0.00001291
Iteration 146/1000 | Loss: 0.00001291
Iteration 147/1000 | Loss: 0.00001291
Iteration 148/1000 | Loss: 0.00001291
Iteration 149/1000 | Loss: 0.00001291
Iteration 150/1000 | Loss: 0.00001291
Iteration 151/1000 | Loss: 0.00001290
Iteration 152/1000 | Loss: 0.00001290
Iteration 153/1000 | Loss: 0.00001290
Iteration 154/1000 | Loss: 0.00001290
Iteration 155/1000 | Loss: 0.00001290
Iteration 156/1000 | Loss: 0.00001290
Iteration 157/1000 | Loss: 0.00001290
Iteration 158/1000 | Loss: 0.00001290
Iteration 159/1000 | Loss: 0.00001290
Iteration 160/1000 | Loss: 0.00001290
Iteration 161/1000 | Loss: 0.00001290
Iteration 162/1000 | Loss: 0.00001290
Iteration 163/1000 | Loss: 0.00001290
Iteration 164/1000 | Loss: 0.00001290
Iteration 165/1000 | Loss: 0.00001290
Iteration 166/1000 | Loss: 0.00001290
Iteration 167/1000 | Loss: 0.00001290
Iteration 168/1000 | Loss: 0.00001290
Iteration 169/1000 | Loss: 0.00001290
Iteration 170/1000 | Loss: 0.00001290
Iteration 171/1000 | Loss: 0.00001289
Iteration 172/1000 | Loss: 0.00001289
Iteration 173/1000 | Loss: 0.00001289
Iteration 174/1000 | Loss: 0.00001289
Iteration 175/1000 | Loss: 0.00001289
Iteration 176/1000 | Loss: 0.00001289
Iteration 177/1000 | Loss: 0.00001289
Iteration 178/1000 | Loss: 0.00001289
Iteration 179/1000 | Loss: 0.00001289
Iteration 180/1000 | Loss: 0.00001289
Iteration 181/1000 | Loss: 0.00001289
Iteration 182/1000 | Loss: 0.00001289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.2891297956230119e-05, 1.2891297956230119e-05, 1.2891297956230119e-05, 1.2891297956230119e-05, 1.2891297956230119e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2891297956230119e-05

Optimization complete. Final v2v error: 3.122051954269409 mm

Highest mean error: 3.3445258140563965 mm for frame 65

Lowest mean error: 2.8123586177825928 mm for frame 146

Saving results

Total time: 34.79605150222778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909350
Iteration 2/25 | Loss: 0.00165771
Iteration 3/25 | Loss: 0.00146040
Iteration 4/25 | Loss: 0.00139641
Iteration 5/25 | Loss: 0.00139923
Iteration 6/25 | Loss: 0.00140424
Iteration 7/25 | Loss: 0.00138276
Iteration 8/25 | Loss: 0.00136792
Iteration 9/25 | Loss: 0.00134536
Iteration 10/25 | Loss: 0.00133828
Iteration 11/25 | Loss: 0.00132623
Iteration 12/25 | Loss: 0.00131995
Iteration 13/25 | Loss: 0.00131105
Iteration 14/25 | Loss: 0.00130408
Iteration 15/25 | Loss: 0.00130635
Iteration 16/25 | Loss: 0.00129909
Iteration 17/25 | Loss: 0.00129699
Iteration 18/25 | Loss: 0.00129532
Iteration 19/25 | Loss: 0.00129269
Iteration 20/25 | Loss: 0.00129015
Iteration 21/25 | Loss: 0.00128631
Iteration 22/25 | Loss: 0.00128637
Iteration 23/25 | Loss: 0.00128854
Iteration 24/25 | Loss: 0.00128702
Iteration 25/25 | Loss: 0.00128219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38726473
Iteration 2/25 | Loss: 0.00241479
Iteration 3/25 | Loss: 0.00241477
Iteration 4/25 | Loss: 0.00241477
Iteration 5/25 | Loss: 0.00241477
Iteration 6/25 | Loss: 0.00241477
Iteration 7/25 | Loss: 0.00241476
Iteration 8/25 | Loss: 0.00241476
Iteration 9/25 | Loss: 0.00241476
Iteration 10/25 | Loss: 0.00241476
Iteration 11/25 | Loss: 0.00241476
Iteration 12/25 | Loss: 0.00241476
Iteration 13/25 | Loss: 0.00241476
Iteration 14/25 | Loss: 0.00241476
Iteration 15/25 | Loss: 0.00241476
Iteration 16/25 | Loss: 0.00241476
Iteration 17/25 | Loss: 0.00241476
Iteration 18/25 | Loss: 0.00241476
Iteration 19/25 | Loss: 0.00241476
Iteration 20/25 | Loss: 0.00241476
Iteration 21/25 | Loss: 0.00241476
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0024147641379386187, 0.0024147641379386187, 0.0024147641379386187, 0.0024147641379386187, 0.0024147641379386187]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024147641379386187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00241476
Iteration 2/1000 | Loss: 0.00025151
Iteration 3/1000 | Loss: 0.00017730
Iteration 4/1000 | Loss: 0.00017799
Iteration 5/1000 | Loss: 0.00015811
Iteration 6/1000 | Loss: 0.00014994
Iteration 7/1000 | Loss: 0.00052579
Iteration 8/1000 | Loss: 0.00037751
Iteration 9/1000 | Loss: 0.00024386
Iteration 10/1000 | Loss: 0.00014276
Iteration 11/1000 | Loss: 0.00014569
Iteration 12/1000 | Loss: 0.00027052
Iteration 13/1000 | Loss: 0.00019753
Iteration 14/1000 | Loss: 0.00011183
Iteration 15/1000 | Loss: 0.00015948
Iteration 16/1000 | Loss: 0.00013471
Iteration 17/1000 | Loss: 0.00009534
Iteration 18/1000 | Loss: 0.00011834
Iteration 19/1000 | Loss: 0.00020669
Iteration 20/1000 | Loss: 0.00007840
Iteration 21/1000 | Loss: 0.00007338
Iteration 22/1000 | Loss: 0.00006904
Iteration 23/1000 | Loss: 0.00006712
Iteration 24/1000 | Loss: 0.00006457
Iteration 25/1000 | Loss: 0.00052051
Iteration 26/1000 | Loss: 0.00021880
Iteration 27/1000 | Loss: 0.00006289
Iteration 28/1000 | Loss: 0.00051432
Iteration 29/1000 | Loss: 0.00288108
Iteration 30/1000 | Loss: 0.00053627
Iteration 31/1000 | Loss: 0.00078379
Iteration 32/1000 | Loss: 0.00026142
Iteration 33/1000 | Loss: 0.00020805
Iteration 34/1000 | Loss: 0.00007919
Iteration 35/1000 | Loss: 0.00006435
Iteration 36/1000 | Loss: 0.00005672
Iteration 37/1000 | Loss: 0.00005289
Iteration 38/1000 | Loss: 0.00004881
Iteration 39/1000 | Loss: 0.00052609
Iteration 40/1000 | Loss: 0.00025297
Iteration 41/1000 | Loss: 0.00004627
Iteration 42/1000 | Loss: 0.00045000
Iteration 43/1000 | Loss: 0.00029539
Iteration 44/1000 | Loss: 0.00033073
Iteration 45/1000 | Loss: 0.00004788
Iteration 46/1000 | Loss: 0.00004398
Iteration 47/1000 | Loss: 0.00004156
Iteration 48/1000 | Loss: 0.00004050
Iteration 49/1000 | Loss: 0.00003965
Iteration 50/1000 | Loss: 0.00003924
Iteration 51/1000 | Loss: 0.00003885
Iteration 52/1000 | Loss: 0.00003852
Iteration 53/1000 | Loss: 0.00003832
Iteration 54/1000 | Loss: 0.00003806
Iteration 55/1000 | Loss: 0.00003789
Iteration 56/1000 | Loss: 0.00003781
Iteration 57/1000 | Loss: 0.00003768
Iteration 58/1000 | Loss: 0.00003766
Iteration 59/1000 | Loss: 0.00003766
Iteration 60/1000 | Loss: 0.00003765
Iteration 61/1000 | Loss: 0.00003764
Iteration 62/1000 | Loss: 0.00003764
Iteration 63/1000 | Loss: 0.00003763
Iteration 64/1000 | Loss: 0.00003763
Iteration 65/1000 | Loss: 0.00003763
Iteration 66/1000 | Loss: 0.00003762
Iteration 67/1000 | Loss: 0.00003762
Iteration 68/1000 | Loss: 0.00003761
Iteration 69/1000 | Loss: 0.00003761
Iteration 70/1000 | Loss: 0.00003759
Iteration 71/1000 | Loss: 0.00003758
Iteration 72/1000 | Loss: 0.00003758
Iteration 73/1000 | Loss: 0.00003757
Iteration 74/1000 | Loss: 0.00003757
Iteration 75/1000 | Loss: 0.00003757
Iteration 76/1000 | Loss: 0.00003756
Iteration 77/1000 | Loss: 0.00003755
Iteration 78/1000 | Loss: 0.00003755
Iteration 79/1000 | Loss: 0.00003755
Iteration 80/1000 | Loss: 0.00003755
Iteration 81/1000 | Loss: 0.00003755
Iteration 82/1000 | Loss: 0.00003755
Iteration 83/1000 | Loss: 0.00003752
Iteration 84/1000 | Loss: 0.00003751
Iteration 85/1000 | Loss: 0.00003751
Iteration 86/1000 | Loss: 0.00003750
Iteration 87/1000 | Loss: 0.00003750
Iteration 88/1000 | Loss: 0.00003749
Iteration 89/1000 | Loss: 0.00003749
Iteration 90/1000 | Loss: 0.00003749
Iteration 91/1000 | Loss: 0.00003749
Iteration 92/1000 | Loss: 0.00003749
Iteration 93/1000 | Loss: 0.00003749
Iteration 94/1000 | Loss: 0.00003749
Iteration 95/1000 | Loss: 0.00003749
Iteration 96/1000 | Loss: 0.00003749
Iteration 97/1000 | Loss: 0.00003749
Iteration 98/1000 | Loss: 0.00003749
Iteration 99/1000 | Loss: 0.00003749
Iteration 100/1000 | Loss: 0.00003749
Iteration 101/1000 | Loss: 0.00003748
Iteration 102/1000 | Loss: 0.00003748
Iteration 103/1000 | Loss: 0.00003748
Iteration 104/1000 | Loss: 0.00003748
Iteration 105/1000 | Loss: 0.00003748
Iteration 106/1000 | Loss: 0.00003748
Iteration 107/1000 | Loss: 0.00003748
Iteration 108/1000 | Loss: 0.00003748
Iteration 109/1000 | Loss: 0.00003748
Iteration 110/1000 | Loss: 0.00003748
Iteration 111/1000 | Loss: 0.00003747
Iteration 112/1000 | Loss: 0.00003747
Iteration 113/1000 | Loss: 0.00003747
Iteration 114/1000 | Loss: 0.00003746
Iteration 115/1000 | Loss: 0.00003746
Iteration 116/1000 | Loss: 0.00003746
Iteration 117/1000 | Loss: 0.00003745
Iteration 118/1000 | Loss: 0.00003745
Iteration 119/1000 | Loss: 0.00003745
Iteration 120/1000 | Loss: 0.00003745
Iteration 121/1000 | Loss: 0.00003745
Iteration 122/1000 | Loss: 0.00003744
Iteration 123/1000 | Loss: 0.00003744
Iteration 124/1000 | Loss: 0.00003744
Iteration 125/1000 | Loss: 0.00003743
Iteration 126/1000 | Loss: 0.00003743
Iteration 127/1000 | Loss: 0.00003743
Iteration 128/1000 | Loss: 0.00003743
Iteration 129/1000 | Loss: 0.00003742
Iteration 130/1000 | Loss: 0.00003742
Iteration 131/1000 | Loss: 0.00003742
Iteration 132/1000 | Loss: 0.00003742
Iteration 133/1000 | Loss: 0.00003742
Iteration 134/1000 | Loss: 0.00003741
Iteration 135/1000 | Loss: 0.00003741
Iteration 136/1000 | Loss: 0.00003741
Iteration 137/1000 | Loss: 0.00003741
Iteration 138/1000 | Loss: 0.00003741
Iteration 139/1000 | Loss: 0.00003741
Iteration 140/1000 | Loss: 0.00003741
Iteration 141/1000 | Loss: 0.00003741
Iteration 142/1000 | Loss: 0.00003741
Iteration 143/1000 | Loss: 0.00003740
Iteration 144/1000 | Loss: 0.00003740
Iteration 145/1000 | Loss: 0.00003740
Iteration 146/1000 | Loss: 0.00003740
Iteration 147/1000 | Loss: 0.00003740
Iteration 148/1000 | Loss: 0.00003740
Iteration 149/1000 | Loss: 0.00003740
Iteration 150/1000 | Loss: 0.00003740
Iteration 151/1000 | Loss: 0.00003740
Iteration 152/1000 | Loss: 0.00003740
Iteration 153/1000 | Loss: 0.00003740
Iteration 154/1000 | Loss: 0.00003740
Iteration 155/1000 | Loss: 0.00003739
Iteration 156/1000 | Loss: 0.00003739
Iteration 157/1000 | Loss: 0.00003739
Iteration 158/1000 | Loss: 0.00003739
Iteration 159/1000 | Loss: 0.00003739
Iteration 160/1000 | Loss: 0.00003738
Iteration 161/1000 | Loss: 0.00003738
Iteration 162/1000 | Loss: 0.00003738
Iteration 163/1000 | Loss: 0.00003738
Iteration 164/1000 | Loss: 0.00003738
Iteration 165/1000 | Loss: 0.00003738
Iteration 166/1000 | Loss: 0.00003738
Iteration 167/1000 | Loss: 0.00003737
Iteration 168/1000 | Loss: 0.00003737
Iteration 169/1000 | Loss: 0.00003737
Iteration 170/1000 | Loss: 0.00003737
Iteration 171/1000 | Loss: 0.00003737
Iteration 172/1000 | Loss: 0.00003737
Iteration 173/1000 | Loss: 0.00003737
Iteration 174/1000 | Loss: 0.00003737
Iteration 175/1000 | Loss: 0.00003737
Iteration 176/1000 | Loss: 0.00003736
Iteration 177/1000 | Loss: 0.00003736
Iteration 178/1000 | Loss: 0.00003736
Iteration 179/1000 | Loss: 0.00003736
Iteration 180/1000 | Loss: 0.00003736
Iteration 181/1000 | Loss: 0.00003736
Iteration 182/1000 | Loss: 0.00003736
Iteration 183/1000 | Loss: 0.00003736
Iteration 184/1000 | Loss: 0.00003736
Iteration 185/1000 | Loss: 0.00003736
Iteration 186/1000 | Loss: 0.00003736
Iteration 187/1000 | Loss: 0.00003735
Iteration 188/1000 | Loss: 0.00003735
Iteration 189/1000 | Loss: 0.00003735
Iteration 190/1000 | Loss: 0.00003735
Iteration 191/1000 | Loss: 0.00003735
Iteration 192/1000 | Loss: 0.00003735
Iteration 193/1000 | Loss: 0.00003735
Iteration 194/1000 | Loss: 0.00003735
Iteration 195/1000 | Loss: 0.00003734
Iteration 196/1000 | Loss: 0.00003734
Iteration 197/1000 | Loss: 0.00003734
Iteration 198/1000 | Loss: 0.00003734
Iteration 199/1000 | Loss: 0.00003734
Iteration 200/1000 | Loss: 0.00003734
Iteration 201/1000 | Loss: 0.00003734
Iteration 202/1000 | Loss: 0.00003734
Iteration 203/1000 | Loss: 0.00003734
Iteration 204/1000 | Loss: 0.00003734
Iteration 205/1000 | Loss: 0.00003734
Iteration 206/1000 | Loss: 0.00003734
Iteration 207/1000 | Loss: 0.00003734
Iteration 208/1000 | Loss: 0.00003733
Iteration 209/1000 | Loss: 0.00003733
Iteration 210/1000 | Loss: 0.00003733
Iteration 211/1000 | Loss: 0.00003733
Iteration 212/1000 | Loss: 0.00003733
Iteration 213/1000 | Loss: 0.00003733
Iteration 214/1000 | Loss: 0.00003733
Iteration 215/1000 | Loss: 0.00003733
Iteration 216/1000 | Loss: 0.00003733
Iteration 217/1000 | Loss: 0.00003733
Iteration 218/1000 | Loss: 0.00003733
Iteration 219/1000 | Loss: 0.00003733
Iteration 220/1000 | Loss: 0.00003733
Iteration 221/1000 | Loss: 0.00003733
Iteration 222/1000 | Loss: 0.00003733
Iteration 223/1000 | Loss: 0.00003733
Iteration 224/1000 | Loss: 0.00003733
Iteration 225/1000 | Loss: 0.00003733
Iteration 226/1000 | Loss: 0.00003733
Iteration 227/1000 | Loss: 0.00003733
Iteration 228/1000 | Loss: 0.00003733
Iteration 229/1000 | Loss: 0.00003733
Iteration 230/1000 | Loss: 0.00003733
Iteration 231/1000 | Loss: 0.00003733
Iteration 232/1000 | Loss: 0.00003733
Iteration 233/1000 | Loss: 0.00003733
Iteration 234/1000 | Loss: 0.00003733
Iteration 235/1000 | Loss: 0.00003733
Iteration 236/1000 | Loss: 0.00003733
Iteration 237/1000 | Loss: 0.00003733
Iteration 238/1000 | Loss: 0.00003733
Iteration 239/1000 | Loss: 0.00003733
Iteration 240/1000 | Loss: 0.00003733
Iteration 241/1000 | Loss: 0.00003733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [3.7326852179830894e-05, 3.7326852179830894e-05, 3.7326852179830894e-05, 3.7326852179830894e-05, 3.7326852179830894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7326852179830894e-05

Optimization complete. Final v2v error: 4.123172283172607 mm

Highest mean error: 12.60018253326416 mm for frame 111

Lowest mean error: 3.469066619873047 mm for frame 26

Saving results

Total time: 143.5299003124237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085979
Iteration 2/25 | Loss: 0.01085979
Iteration 3/25 | Loss: 0.01085979
Iteration 4/25 | Loss: 0.01085978
Iteration 5/25 | Loss: 0.01085978
Iteration 6/25 | Loss: 0.00338210
Iteration 7/25 | Loss: 0.00227627
Iteration 8/25 | Loss: 0.00180617
Iteration 9/25 | Loss: 0.00145824
Iteration 10/25 | Loss: 0.00135807
Iteration 11/25 | Loss: 0.00132043
Iteration 12/25 | Loss: 0.00130455
Iteration 13/25 | Loss: 0.00129285
Iteration 14/25 | Loss: 0.00128438
Iteration 15/25 | Loss: 0.00127504
Iteration 16/25 | Loss: 0.00126969
Iteration 17/25 | Loss: 0.00127492
Iteration 18/25 | Loss: 0.00126138
Iteration 19/25 | Loss: 0.00126008
Iteration 20/25 | Loss: 0.00125368
Iteration 21/25 | Loss: 0.00125268
Iteration 22/25 | Loss: 0.00125200
Iteration 23/25 | Loss: 0.00125128
Iteration 24/25 | Loss: 0.00125163
Iteration 25/25 | Loss: 0.00125147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35022616
Iteration 2/25 | Loss: 0.00109121
Iteration 3/25 | Loss: 0.00109121
Iteration 4/25 | Loss: 0.00109121
Iteration 5/25 | Loss: 0.00109121
Iteration 6/25 | Loss: 0.00109121
Iteration 7/25 | Loss: 0.00109121
Iteration 8/25 | Loss: 0.00109121
Iteration 9/25 | Loss: 0.00109121
Iteration 10/25 | Loss: 0.00109121
Iteration 11/25 | Loss: 0.00109121
Iteration 12/25 | Loss: 0.00109121
Iteration 13/25 | Loss: 0.00109121
Iteration 14/25 | Loss: 0.00109121
Iteration 15/25 | Loss: 0.00109121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010912104044109583, 0.0010912104044109583, 0.0010912104044109583, 0.0010912104044109583, 0.0010912104044109583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010912104044109583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109121
Iteration 2/1000 | Loss: 0.00040445
Iteration 3/1000 | Loss: 0.00022893
Iteration 4/1000 | Loss: 0.00090723
Iteration 5/1000 | Loss: 0.00025331
Iteration 6/1000 | Loss: 0.00010824
Iteration 7/1000 | Loss: 0.00008870
Iteration 8/1000 | Loss: 0.00057220
Iteration 9/1000 | Loss: 0.00060367
Iteration 10/1000 | Loss: 0.00096220
Iteration 11/1000 | Loss: 0.00012991
Iteration 12/1000 | Loss: 0.00054971
Iteration 13/1000 | Loss: 0.00020929
Iteration 14/1000 | Loss: 0.00009094
Iteration 15/1000 | Loss: 0.00007163
Iteration 16/1000 | Loss: 0.00006369
Iteration 17/1000 | Loss: 0.00005973
Iteration 18/1000 | Loss: 0.00006902
Iteration 19/1000 | Loss: 0.00005819
Iteration 20/1000 | Loss: 0.00005710
Iteration 21/1000 | Loss: 0.00015638
Iteration 22/1000 | Loss: 0.00007072
Iteration 23/1000 | Loss: 0.00006309
Iteration 24/1000 | Loss: 0.00005707
Iteration 25/1000 | Loss: 0.00006616
Iteration 26/1000 | Loss: 0.00014074
Iteration 27/1000 | Loss: 0.00013919
Iteration 28/1000 | Loss: 0.00025501
Iteration 29/1000 | Loss: 0.00021888
Iteration 30/1000 | Loss: 0.00009957
Iteration 31/1000 | Loss: 0.00006324
Iteration 32/1000 | Loss: 0.00005245
Iteration 33/1000 | Loss: 0.00015920
Iteration 34/1000 | Loss: 0.00059933
Iteration 35/1000 | Loss: 0.00024888
Iteration 36/1000 | Loss: 0.00006848
Iteration 37/1000 | Loss: 0.00011364
Iteration 38/1000 | Loss: 0.00008786
Iteration 39/1000 | Loss: 0.00026638
Iteration 40/1000 | Loss: 0.00014061
Iteration 41/1000 | Loss: 0.00007427
Iteration 42/1000 | Loss: 0.00028706
Iteration 43/1000 | Loss: 0.00015187
Iteration 44/1000 | Loss: 0.00029202
Iteration 45/1000 | Loss: 0.00006327
Iteration 46/1000 | Loss: 0.00004039
Iteration 47/1000 | Loss: 0.00004810
Iteration 48/1000 | Loss: 0.00004725
Iteration 49/1000 | Loss: 0.00003753
Iteration 50/1000 | Loss: 0.00021150
Iteration 51/1000 | Loss: 0.00012803
Iteration 52/1000 | Loss: 0.00005301
Iteration 53/1000 | Loss: 0.00004273
Iteration 54/1000 | Loss: 0.00003808
Iteration 55/1000 | Loss: 0.00003480
Iteration 56/1000 | Loss: 0.00003366
Iteration 57/1000 | Loss: 0.00003275
Iteration 58/1000 | Loss: 0.00003199
Iteration 59/1000 | Loss: 0.00003141
Iteration 60/1000 | Loss: 0.00011883
Iteration 61/1000 | Loss: 0.00003314
Iteration 62/1000 | Loss: 0.00003037
Iteration 63/1000 | Loss: 0.00003001
Iteration 64/1000 | Loss: 0.00002970
Iteration 65/1000 | Loss: 0.00002955
Iteration 66/1000 | Loss: 0.00002951
Iteration 67/1000 | Loss: 0.00002942
Iteration 68/1000 | Loss: 0.00002925
Iteration 69/1000 | Loss: 0.00002922
Iteration 70/1000 | Loss: 0.00002921
Iteration 71/1000 | Loss: 0.00002916
Iteration 72/1000 | Loss: 0.00002915
Iteration 73/1000 | Loss: 0.00002912
Iteration 74/1000 | Loss: 0.00002911
Iteration 75/1000 | Loss: 0.00002910
Iteration 76/1000 | Loss: 0.00002910
Iteration 77/1000 | Loss: 0.00002909
Iteration 78/1000 | Loss: 0.00002909
Iteration 79/1000 | Loss: 0.00002906
Iteration 80/1000 | Loss: 0.00002905
Iteration 81/1000 | Loss: 0.00002905
Iteration 82/1000 | Loss: 0.00002901
Iteration 83/1000 | Loss: 0.00002900
Iteration 84/1000 | Loss: 0.00002900
Iteration 85/1000 | Loss: 0.00002899
Iteration 86/1000 | Loss: 0.00002898
Iteration 87/1000 | Loss: 0.00002898
Iteration 88/1000 | Loss: 0.00002898
Iteration 89/1000 | Loss: 0.00002898
Iteration 90/1000 | Loss: 0.00002898
Iteration 91/1000 | Loss: 0.00002897
Iteration 92/1000 | Loss: 0.00002897
Iteration 93/1000 | Loss: 0.00002897
Iteration 94/1000 | Loss: 0.00002896
Iteration 95/1000 | Loss: 0.00002896
Iteration 96/1000 | Loss: 0.00002896
Iteration 97/1000 | Loss: 0.00002895
Iteration 98/1000 | Loss: 0.00002895
Iteration 99/1000 | Loss: 0.00002895
Iteration 100/1000 | Loss: 0.00002894
Iteration 101/1000 | Loss: 0.00002894
Iteration 102/1000 | Loss: 0.00002894
Iteration 103/1000 | Loss: 0.00002893
Iteration 104/1000 | Loss: 0.00002893
Iteration 105/1000 | Loss: 0.00002893
Iteration 106/1000 | Loss: 0.00002893
Iteration 107/1000 | Loss: 0.00002892
Iteration 108/1000 | Loss: 0.00002892
Iteration 109/1000 | Loss: 0.00002892
Iteration 110/1000 | Loss: 0.00002892
Iteration 111/1000 | Loss: 0.00002892
Iteration 112/1000 | Loss: 0.00002892
Iteration 113/1000 | Loss: 0.00002892
Iteration 114/1000 | Loss: 0.00002891
Iteration 115/1000 | Loss: 0.00002891
Iteration 116/1000 | Loss: 0.00002891
Iteration 117/1000 | Loss: 0.00002891
Iteration 118/1000 | Loss: 0.00002891
Iteration 119/1000 | Loss: 0.00002891
Iteration 120/1000 | Loss: 0.00002891
Iteration 121/1000 | Loss: 0.00002891
Iteration 122/1000 | Loss: 0.00002891
Iteration 123/1000 | Loss: 0.00002891
Iteration 124/1000 | Loss: 0.00002891
Iteration 125/1000 | Loss: 0.00002891
Iteration 126/1000 | Loss: 0.00002891
Iteration 127/1000 | Loss: 0.00002890
Iteration 128/1000 | Loss: 0.00002890
Iteration 129/1000 | Loss: 0.00002890
Iteration 130/1000 | Loss: 0.00002890
Iteration 131/1000 | Loss: 0.00002890
Iteration 132/1000 | Loss: 0.00002890
Iteration 133/1000 | Loss: 0.00002890
Iteration 134/1000 | Loss: 0.00002890
Iteration 135/1000 | Loss: 0.00002890
Iteration 136/1000 | Loss: 0.00002890
Iteration 137/1000 | Loss: 0.00002890
Iteration 138/1000 | Loss: 0.00002889
Iteration 139/1000 | Loss: 0.00002889
Iteration 140/1000 | Loss: 0.00002889
Iteration 141/1000 | Loss: 0.00002889
Iteration 142/1000 | Loss: 0.00002889
Iteration 143/1000 | Loss: 0.00002889
Iteration 144/1000 | Loss: 0.00002889
Iteration 145/1000 | Loss: 0.00002889
Iteration 146/1000 | Loss: 0.00002889
Iteration 147/1000 | Loss: 0.00002889
Iteration 148/1000 | Loss: 0.00002889
Iteration 149/1000 | Loss: 0.00002889
Iteration 150/1000 | Loss: 0.00002889
Iteration 151/1000 | Loss: 0.00002889
Iteration 152/1000 | Loss: 0.00002889
Iteration 153/1000 | Loss: 0.00002889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.8890079192933626e-05, 2.8890079192933626e-05, 2.8890079192933626e-05, 2.8890079192933626e-05, 2.8890079192933626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8890079192933626e-05

Optimization complete. Final v2v error: 4.54994535446167 mm

Highest mean error: 5.925472736358643 mm for frame 124

Lowest mean error: 3.9868555068969727 mm for frame 230

Saving results

Total time: 161.30803966522217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447216
Iteration 2/25 | Loss: 0.00131548
Iteration 3/25 | Loss: 0.00110677
Iteration 4/25 | Loss: 0.00107613
Iteration 5/25 | Loss: 0.00107010
Iteration 6/25 | Loss: 0.00106847
Iteration 7/25 | Loss: 0.00106840
Iteration 8/25 | Loss: 0.00106840
Iteration 9/25 | Loss: 0.00106840
Iteration 10/25 | Loss: 0.00106840
Iteration 11/25 | Loss: 0.00106840
Iteration 12/25 | Loss: 0.00106840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010684027802199125, 0.0010684027802199125, 0.0010684027802199125, 0.0010684027802199125, 0.0010684027802199125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010684027802199125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38776672
Iteration 2/25 | Loss: 0.00067327
Iteration 3/25 | Loss: 0.00067327
Iteration 4/25 | Loss: 0.00067327
Iteration 5/25 | Loss: 0.00067326
Iteration 6/25 | Loss: 0.00067326
Iteration 7/25 | Loss: 0.00067326
Iteration 8/25 | Loss: 0.00067326
Iteration 9/25 | Loss: 0.00067326
Iteration 10/25 | Loss: 0.00067326
Iteration 11/25 | Loss: 0.00067326
Iteration 12/25 | Loss: 0.00067326
Iteration 13/25 | Loss: 0.00067326
Iteration 14/25 | Loss: 0.00067326
Iteration 15/25 | Loss: 0.00067326
Iteration 16/25 | Loss: 0.00067326
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006732632173225284, 0.0006732632173225284, 0.0006732632173225284, 0.0006732632173225284, 0.0006732632173225284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006732632173225284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067326
Iteration 2/1000 | Loss: 0.00005128
Iteration 3/1000 | Loss: 0.00002528
Iteration 4/1000 | Loss: 0.00002067
Iteration 5/1000 | Loss: 0.00001809
Iteration 6/1000 | Loss: 0.00001658
Iteration 7/1000 | Loss: 0.00001583
Iteration 8/1000 | Loss: 0.00001545
Iteration 9/1000 | Loss: 0.00001520
Iteration 10/1000 | Loss: 0.00001481
Iteration 11/1000 | Loss: 0.00001452
Iteration 12/1000 | Loss: 0.00001451
Iteration 13/1000 | Loss: 0.00001448
Iteration 14/1000 | Loss: 0.00001447
Iteration 15/1000 | Loss: 0.00001440
Iteration 16/1000 | Loss: 0.00001435
Iteration 17/1000 | Loss: 0.00001418
Iteration 18/1000 | Loss: 0.00001418
Iteration 19/1000 | Loss: 0.00001414
Iteration 20/1000 | Loss: 0.00001412
Iteration 21/1000 | Loss: 0.00001411
Iteration 22/1000 | Loss: 0.00001410
Iteration 23/1000 | Loss: 0.00001410
Iteration 24/1000 | Loss: 0.00001409
Iteration 25/1000 | Loss: 0.00001408
Iteration 26/1000 | Loss: 0.00001406
Iteration 27/1000 | Loss: 0.00001405
Iteration 28/1000 | Loss: 0.00001405
Iteration 29/1000 | Loss: 0.00001405
Iteration 30/1000 | Loss: 0.00001405
Iteration 31/1000 | Loss: 0.00001405
Iteration 32/1000 | Loss: 0.00001405
Iteration 33/1000 | Loss: 0.00001405
Iteration 34/1000 | Loss: 0.00001405
Iteration 35/1000 | Loss: 0.00001405
Iteration 36/1000 | Loss: 0.00001403
Iteration 37/1000 | Loss: 0.00001402
Iteration 38/1000 | Loss: 0.00001402
Iteration 39/1000 | Loss: 0.00001401
Iteration 40/1000 | Loss: 0.00001401
Iteration 41/1000 | Loss: 0.00001400
Iteration 42/1000 | Loss: 0.00001400
Iteration 43/1000 | Loss: 0.00001400
Iteration 44/1000 | Loss: 0.00001400
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001399
Iteration 47/1000 | Loss: 0.00001399
Iteration 48/1000 | Loss: 0.00001398
Iteration 49/1000 | Loss: 0.00001398
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001398
Iteration 52/1000 | Loss: 0.00001397
Iteration 53/1000 | Loss: 0.00001397
Iteration 54/1000 | Loss: 0.00001397
Iteration 55/1000 | Loss: 0.00001397
Iteration 56/1000 | Loss: 0.00001397
Iteration 57/1000 | Loss: 0.00001397
Iteration 58/1000 | Loss: 0.00001397
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001396
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001395
Iteration 63/1000 | Loss: 0.00001395
Iteration 64/1000 | Loss: 0.00001395
Iteration 65/1000 | Loss: 0.00001394
Iteration 66/1000 | Loss: 0.00001394
Iteration 67/1000 | Loss: 0.00001394
Iteration 68/1000 | Loss: 0.00001394
Iteration 69/1000 | Loss: 0.00001393
Iteration 70/1000 | Loss: 0.00001393
Iteration 71/1000 | Loss: 0.00001393
Iteration 72/1000 | Loss: 0.00001392
Iteration 73/1000 | Loss: 0.00001392
Iteration 74/1000 | Loss: 0.00001392
Iteration 75/1000 | Loss: 0.00001391
Iteration 76/1000 | Loss: 0.00001391
Iteration 77/1000 | Loss: 0.00001391
Iteration 78/1000 | Loss: 0.00001391
Iteration 79/1000 | Loss: 0.00001391
Iteration 80/1000 | Loss: 0.00001391
Iteration 81/1000 | Loss: 0.00001390
Iteration 82/1000 | Loss: 0.00001390
Iteration 83/1000 | Loss: 0.00001390
Iteration 84/1000 | Loss: 0.00001390
Iteration 85/1000 | Loss: 0.00001390
Iteration 86/1000 | Loss: 0.00001390
Iteration 87/1000 | Loss: 0.00001390
Iteration 88/1000 | Loss: 0.00001389
Iteration 89/1000 | Loss: 0.00001389
Iteration 90/1000 | Loss: 0.00001389
Iteration 91/1000 | Loss: 0.00001389
Iteration 92/1000 | Loss: 0.00001389
Iteration 93/1000 | Loss: 0.00001388
Iteration 94/1000 | Loss: 0.00001388
Iteration 95/1000 | Loss: 0.00001388
Iteration 96/1000 | Loss: 0.00001388
Iteration 97/1000 | Loss: 0.00001388
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.3883591236663051e-05, 1.3883591236663051e-05, 1.3883591236663051e-05, 1.3883591236663051e-05, 1.3883591236663051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3883591236663051e-05

Optimization complete. Final v2v error: 3.19974684715271 mm

Highest mean error: 4.047152996063232 mm for frame 218

Lowest mean error: 2.7573039531707764 mm for frame 5

Saving results

Total time: 38.98392868041992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985104
Iteration 2/25 | Loss: 0.00135067
Iteration 3/25 | Loss: 0.00119346
Iteration 4/25 | Loss: 0.00115528
Iteration 5/25 | Loss: 0.00114398
Iteration 6/25 | Loss: 0.00114041
Iteration 7/25 | Loss: 0.00113893
Iteration 8/25 | Loss: 0.00113878
Iteration 9/25 | Loss: 0.00113878
Iteration 10/25 | Loss: 0.00113878
Iteration 11/25 | Loss: 0.00113878
Iteration 12/25 | Loss: 0.00113878
Iteration 13/25 | Loss: 0.00113878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011387760750949383, 0.0011387760750949383, 0.0011387760750949383, 0.0011387760750949383, 0.0011387760750949383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011387760750949383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.18739395
Iteration 2/25 | Loss: 0.00097062
Iteration 3/25 | Loss: 0.00097062
Iteration 4/25 | Loss: 0.00097062
Iteration 5/25 | Loss: 0.00097062
Iteration 6/25 | Loss: 0.00097062
Iteration 7/25 | Loss: 0.00097062
Iteration 8/25 | Loss: 0.00097062
Iteration 9/25 | Loss: 0.00097062
Iteration 10/25 | Loss: 0.00097062
Iteration 11/25 | Loss: 0.00097062
Iteration 12/25 | Loss: 0.00097062
Iteration 13/25 | Loss: 0.00097062
Iteration 14/25 | Loss: 0.00097062
Iteration 15/25 | Loss: 0.00097062
Iteration 16/25 | Loss: 0.00097062
Iteration 17/25 | Loss: 0.00097062
Iteration 18/25 | Loss: 0.00097062
Iteration 19/25 | Loss: 0.00097062
Iteration 20/25 | Loss: 0.00097062
Iteration 21/25 | Loss: 0.00097062
Iteration 22/25 | Loss: 0.00097062
Iteration 23/25 | Loss: 0.00097062
Iteration 24/25 | Loss: 0.00097062
Iteration 25/25 | Loss: 0.00097062

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097062
Iteration 2/1000 | Loss: 0.00006820
Iteration 3/1000 | Loss: 0.00005097
Iteration 4/1000 | Loss: 0.00004140
Iteration 5/1000 | Loss: 0.00003691
Iteration 6/1000 | Loss: 0.00003449
Iteration 7/1000 | Loss: 0.00003310
Iteration 8/1000 | Loss: 0.00003195
Iteration 9/1000 | Loss: 0.00003120
Iteration 10/1000 | Loss: 0.00003085
Iteration 11/1000 | Loss: 0.00003042
Iteration 12/1000 | Loss: 0.00003004
Iteration 13/1000 | Loss: 0.00002967
Iteration 14/1000 | Loss: 0.00002933
Iteration 15/1000 | Loss: 0.00002930
Iteration 16/1000 | Loss: 0.00002909
Iteration 17/1000 | Loss: 0.00002904
Iteration 18/1000 | Loss: 0.00002897
Iteration 19/1000 | Loss: 0.00002889
Iteration 20/1000 | Loss: 0.00002889
Iteration 21/1000 | Loss: 0.00002872
Iteration 22/1000 | Loss: 0.00002869
Iteration 23/1000 | Loss: 0.00002869
Iteration 24/1000 | Loss: 0.00002867
Iteration 25/1000 | Loss: 0.00002866
Iteration 26/1000 | Loss: 0.00002866
Iteration 27/1000 | Loss: 0.00002865
Iteration 28/1000 | Loss: 0.00002865
Iteration 29/1000 | Loss: 0.00002865
Iteration 30/1000 | Loss: 0.00002862
Iteration 31/1000 | Loss: 0.00002857
Iteration 32/1000 | Loss: 0.00002856
Iteration 33/1000 | Loss: 0.00002855
Iteration 34/1000 | Loss: 0.00002855
Iteration 35/1000 | Loss: 0.00002855
Iteration 36/1000 | Loss: 0.00002854
Iteration 37/1000 | Loss: 0.00002854
Iteration 38/1000 | Loss: 0.00002854
Iteration 39/1000 | Loss: 0.00002854
Iteration 40/1000 | Loss: 0.00002854
Iteration 41/1000 | Loss: 0.00002854
Iteration 42/1000 | Loss: 0.00002854
Iteration 43/1000 | Loss: 0.00002853
Iteration 44/1000 | Loss: 0.00002853
Iteration 45/1000 | Loss: 0.00002852
Iteration 46/1000 | Loss: 0.00002851
Iteration 47/1000 | Loss: 0.00002851
Iteration 48/1000 | Loss: 0.00002850
Iteration 49/1000 | Loss: 0.00002850
Iteration 50/1000 | Loss: 0.00002850
Iteration 51/1000 | Loss: 0.00002850
Iteration 52/1000 | Loss: 0.00002849
Iteration 53/1000 | Loss: 0.00002849
Iteration 54/1000 | Loss: 0.00002848
Iteration 55/1000 | Loss: 0.00002848
Iteration 56/1000 | Loss: 0.00002847
Iteration 57/1000 | Loss: 0.00002847
Iteration 58/1000 | Loss: 0.00002847
Iteration 59/1000 | Loss: 0.00002847
Iteration 60/1000 | Loss: 0.00002847
Iteration 61/1000 | Loss: 0.00002846
Iteration 62/1000 | Loss: 0.00002846
Iteration 63/1000 | Loss: 0.00002846
Iteration 64/1000 | Loss: 0.00002846
Iteration 65/1000 | Loss: 0.00002846
Iteration 66/1000 | Loss: 0.00002846
Iteration 67/1000 | Loss: 0.00002846
Iteration 68/1000 | Loss: 0.00002845
Iteration 69/1000 | Loss: 0.00002845
Iteration 70/1000 | Loss: 0.00002845
Iteration 71/1000 | Loss: 0.00002845
Iteration 72/1000 | Loss: 0.00002844
Iteration 73/1000 | Loss: 0.00002844
Iteration 74/1000 | Loss: 0.00002844
Iteration 75/1000 | Loss: 0.00002843
Iteration 76/1000 | Loss: 0.00002843
Iteration 77/1000 | Loss: 0.00002843
Iteration 78/1000 | Loss: 0.00002843
Iteration 79/1000 | Loss: 0.00002843
Iteration 80/1000 | Loss: 0.00002843
Iteration 81/1000 | Loss: 0.00002843
Iteration 82/1000 | Loss: 0.00002842
Iteration 83/1000 | Loss: 0.00002842
Iteration 84/1000 | Loss: 0.00002842
Iteration 85/1000 | Loss: 0.00002842
Iteration 86/1000 | Loss: 0.00002842
Iteration 87/1000 | Loss: 0.00002842
Iteration 88/1000 | Loss: 0.00002842
Iteration 89/1000 | Loss: 0.00002842
Iteration 90/1000 | Loss: 0.00002842
Iteration 91/1000 | Loss: 0.00002842
Iteration 92/1000 | Loss: 0.00002842
Iteration 93/1000 | Loss: 0.00002842
Iteration 94/1000 | Loss: 0.00002842
Iteration 95/1000 | Loss: 0.00002842
Iteration 96/1000 | Loss: 0.00002842
Iteration 97/1000 | Loss: 0.00002842
Iteration 98/1000 | Loss: 0.00002842
Iteration 99/1000 | Loss: 0.00002842
Iteration 100/1000 | Loss: 0.00002842
Iteration 101/1000 | Loss: 0.00002842
Iteration 102/1000 | Loss: 0.00002842
Iteration 103/1000 | Loss: 0.00002842
Iteration 104/1000 | Loss: 0.00002842
Iteration 105/1000 | Loss: 0.00002842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.8420528906281106e-05, 2.8420528906281106e-05, 2.8420528906281106e-05, 2.8420528906281106e-05, 2.8420528906281106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8420528906281106e-05

Optimization complete. Final v2v error: 4.549696445465088 mm

Highest mean error: 4.880019187927246 mm for frame 54

Lowest mean error: 4.249541759490967 mm for frame 12

Saving results

Total time: 41.79632115364075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430961
Iteration 2/25 | Loss: 0.00116451
Iteration 3/25 | Loss: 0.00105696
Iteration 4/25 | Loss: 0.00104489
Iteration 5/25 | Loss: 0.00104079
Iteration 6/25 | Loss: 0.00103961
Iteration 7/25 | Loss: 0.00103940
Iteration 8/25 | Loss: 0.00103940
Iteration 9/25 | Loss: 0.00103940
Iteration 10/25 | Loss: 0.00103940
Iteration 11/25 | Loss: 0.00103940
Iteration 12/25 | Loss: 0.00103940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010393951088190079, 0.0010393951088190079, 0.0010393951088190079, 0.0010393951088190079, 0.0010393951088190079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010393951088190079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50969183
Iteration 2/25 | Loss: 0.00055279
Iteration 3/25 | Loss: 0.00055279
Iteration 4/25 | Loss: 0.00055279
Iteration 5/25 | Loss: 0.00055279
Iteration 6/25 | Loss: 0.00055279
Iteration 7/25 | Loss: 0.00055279
Iteration 8/25 | Loss: 0.00055279
Iteration 9/25 | Loss: 0.00055278
Iteration 10/25 | Loss: 0.00055278
Iteration 11/25 | Loss: 0.00055278
Iteration 12/25 | Loss: 0.00055278
Iteration 13/25 | Loss: 0.00055278
Iteration 14/25 | Loss: 0.00055278
Iteration 15/25 | Loss: 0.00055278
Iteration 16/25 | Loss: 0.00055278
Iteration 17/25 | Loss: 0.00055278
Iteration 18/25 | Loss: 0.00055278
Iteration 19/25 | Loss: 0.00055278
Iteration 20/25 | Loss: 0.00055278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005527845351025462, 0.0005527845351025462, 0.0005527845351025462, 0.0005527845351025462, 0.0005527845351025462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005527845351025462

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055278
Iteration 2/1000 | Loss: 0.00007163
Iteration 3/1000 | Loss: 0.00003970
Iteration 4/1000 | Loss: 0.00002496
Iteration 5/1000 | Loss: 0.00002145
Iteration 6/1000 | Loss: 0.00001914
Iteration 7/1000 | Loss: 0.00001802
Iteration 8/1000 | Loss: 0.00001720
Iteration 9/1000 | Loss: 0.00001675
Iteration 10/1000 | Loss: 0.00001648
Iteration 11/1000 | Loss: 0.00001622
Iteration 12/1000 | Loss: 0.00001600
Iteration 13/1000 | Loss: 0.00001578
Iteration 14/1000 | Loss: 0.00001569
Iteration 15/1000 | Loss: 0.00001564
Iteration 16/1000 | Loss: 0.00001562
Iteration 17/1000 | Loss: 0.00001561
Iteration 18/1000 | Loss: 0.00001561
Iteration 19/1000 | Loss: 0.00001560
Iteration 20/1000 | Loss: 0.00001560
Iteration 21/1000 | Loss: 0.00001559
Iteration 22/1000 | Loss: 0.00001559
Iteration 23/1000 | Loss: 0.00001559
Iteration 24/1000 | Loss: 0.00001558
Iteration 25/1000 | Loss: 0.00001558
Iteration 26/1000 | Loss: 0.00001558
Iteration 27/1000 | Loss: 0.00001557
Iteration 28/1000 | Loss: 0.00001557
Iteration 29/1000 | Loss: 0.00001557
Iteration 30/1000 | Loss: 0.00001557
Iteration 31/1000 | Loss: 0.00001556
Iteration 32/1000 | Loss: 0.00001556
Iteration 33/1000 | Loss: 0.00001556
Iteration 34/1000 | Loss: 0.00001556
Iteration 35/1000 | Loss: 0.00001556
Iteration 36/1000 | Loss: 0.00001555
Iteration 37/1000 | Loss: 0.00001555
Iteration 38/1000 | Loss: 0.00001555
Iteration 39/1000 | Loss: 0.00001555
Iteration 40/1000 | Loss: 0.00001555
Iteration 41/1000 | Loss: 0.00001554
Iteration 42/1000 | Loss: 0.00001553
Iteration 43/1000 | Loss: 0.00001553
Iteration 44/1000 | Loss: 0.00001553
Iteration 45/1000 | Loss: 0.00001553
Iteration 46/1000 | Loss: 0.00001552
Iteration 47/1000 | Loss: 0.00001552
Iteration 48/1000 | Loss: 0.00001552
Iteration 49/1000 | Loss: 0.00001552
Iteration 50/1000 | Loss: 0.00001552
Iteration 51/1000 | Loss: 0.00001552
Iteration 52/1000 | Loss: 0.00001552
Iteration 53/1000 | Loss: 0.00001551
Iteration 54/1000 | Loss: 0.00001551
Iteration 55/1000 | Loss: 0.00001551
Iteration 56/1000 | Loss: 0.00001551
Iteration 57/1000 | Loss: 0.00001550
Iteration 58/1000 | Loss: 0.00001550
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001550
Iteration 61/1000 | Loss: 0.00001550
Iteration 62/1000 | Loss: 0.00001550
Iteration 63/1000 | Loss: 0.00001550
Iteration 64/1000 | Loss: 0.00001550
Iteration 65/1000 | Loss: 0.00001550
Iteration 66/1000 | Loss: 0.00001550
Iteration 67/1000 | Loss: 0.00001550
Iteration 68/1000 | Loss: 0.00001549
Iteration 69/1000 | Loss: 0.00001549
Iteration 70/1000 | Loss: 0.00001549
Iteration 71/1000 | Loss: 0.00001549
Iteration 72/1000 | Loss: 0.00001549
Iteration 73/1000 | Loss: 0.00001549
Iteration 74/1000 | Loss: 0.00001549
Iteration 75/1000 | Loss: 0.00001549
Iteration 76/1000 | Loss: 0.00001549
Iteration 77/1000 | Loss: 0.00001549
Iteration 78/1000 | Loss: 0.00001549
Iteration 79/1000 | Loss: 0.00001549
Iteration 80/1000 | Loss: 0.00001549
Iteration 81/1000 | Loss: 0.00001548
Iteration 82/1000 | Loss: 0.00001548
Iteration 83/1000 | Loss: 0.00001548
Iteration 84/1000 | Loss: 0.00001548
Iteration 85/1000 | Loss: 0.00001548
Iteration 86/1000 | Loss: 0.00001548
Iteration 87/1000 | Loss: 0.00001548
Iteration 88/1000 | Loss: 0.00001548
Iteration 89/1000 | Loss: 0.00001548
Iteration 90/1000 | Loss: 0.00001548
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001548
Iteration 94/1000 | Loss: 0.00001548
Iteration 95/1000 | Loss: 0.00001548
Iteration 96/1000 | Loss: 0.00001548
Iteration 97/1000 | Loss: 0.00001548
Iteration 98/1000 | Loss: 0.00001548
Iteration 99/1000 | Loss: 0.00001548
Iteration 100/1000 | Loss: 0.00001548
Iteration 101/1000 | Loss: 0.00001548
Iteration 102/1000 | Loss: 0.00001548
Iteration 103/1000 | Loss: 0.00001548
Iteration 104/1000 | Loss: 0.00001548
Iteration 105/1000 | Loss: 0.00001548
Iteration 106/1000 | Loss: 0.00001548
Iteration 107/1000 | Loss: 0.00001548
Iteration 108/1000 | Loss: 0.00001548
Iteration 109/1000 | Loss: 0.00001548
Iteration 110/1000 | Loss: 0.00001548
Iteration 111/1000 | Loss: 0.00001548
Iteration 112/1000 | Loss: 0.00001548
Iteration 113/1000 | Loss: 0.00001548
Iteration 114/1000 | Loss: 0.00001548
Iteration 115/1000 | Loss: 0.00001548
Iteration 116/1000 | Loss: 0.00001548
Iteration 117/1000 | Loss: 0.00001548
Iteration 118/1000 | Loss: 0.00001548
Iteration 119/1000 | Loss: 0.00001548
Iteration 120/1000 | Loss: 0.00001548
Iteration 121/1000 | Loss: 0.00001548
Iteration 122/1000 | Loss: 0.00001548
Iteration 123/1000 | Loss: 0.00001548
Iteration 124/1000 | Loss: 0.00001548
Iteration 125/1000 | Loss: 0.00001548
Iteration 126/1000 | Loss: 0.00001548
Iteration 127/1000 | Loss: 0.00001548
Iteration 128/1000 | Loss: 0.00001548
Iteration 129/1000 | Loss: 0.00001548
Iteration 130/1000 | Loss: 0.00001548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.548081672808621e-05, 1.548081672808621e-05, 1.548081672808621e-05, 1.548081672808621e-05, 1.548081672808621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.548081672808621e-05

Optimization complete. Final v2v error: 3.3819942474365234 mm

Highest mean error: 4.3786797523498535 mm for frame 29

Lowest mean error: 2.8677966594696045 mm for frame 72

Saving results

Total time: 34.854076862335205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869499
Iteration 2/25 | Loss: 0.00112025
Iteration 3/25 | Loss: 0.00099280
Iteration 4/25 | Loss: 0.00098510
Iteration 5/25 | Loss: 0.00098320
Iteration 6/25 | Loss: 0.00098307
Iteration 7/25 | Loss: 0.00098307
Iteration 8/25 | Loss: 0.00098307
Iteration 9/25 | Loss: 0.00098307
Iteration 10/25 | Loss: 0.00098307
Iteration 11/25 | Loss: 0.00098307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009830670896917582, 0.0009830670896917582, 0.0009830670896917582, 0.0009830670896917582, 0.0009830670896917582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009830670896917582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39089775
Iteration 2/25 | Loss: 0.00053234
Iteration 3/25 | Loss: 0.00053234
Iteration 4/25 | Loss: 0.00053234
Iteration 5/25 | Loss: 0.00053234
Iteration 6/25 | Loss: 0.00053234
Iteration 7/25 | Loss: 0.00053233
Iteration 8/25 | Loss: 0.00053233
Iteration 9/25 | Loss: 0.00053233
Iteration 10/25 | Loss: 0.00053233
Iteration 11/25 | Loss: 0.00053233
Iteration 12/25 | Loss: 0.00053233
Iteration 13/25 | Loss: 0.00053233
Iteration 14/25 | Loss: 0.00053233
Iteration 15/25 | Loss: 0.00053233
Iteration 16/25 | Loss: 0.00053233
Iteration 17/25 | Loss: 0.00053233
Iteration 18/25 | Loss: 0.00053233
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005323340301401913, 0.0005323340301401913, 0.0005323340301401913, 0.0005323340301401913, 0.0005323340301401913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005323340301401913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053233
Iteration 2/1000 | Loss: 0.00003137
Iteration 3/1000 | Loss: 0.00001660
Iteration 4/1000 | Loss: 0.00001433
Iteration 5/1000 | Loss: 0.00001264
Iteration 6/1000 | Loss: 0.00001207
Iteration 7/1000 | Loss: 0.00001171
Iteration 8/1000 | Loss: 0.00001166
Iteration 9/1000 | Loss: 0.00001144
Iteration 10/1000 | Loss: 0.00001138
Iteration 11/1000 | Loss: 0.00001131
Iteration 12/1000 | Loss: 0.00001123
Iteration 13/1000 | Loss: 0.00001123
Iteration 14/1000 | Loss: 0.00001113
Iteration 15/1000 | Loss: 0.00001112
Iteration 16/1000 | Loss: 0.00001112
Iteration 17/1000 | Loss: 0.00001110
Iteration 18/1000 | Loss: 0.00001110
Iteration 19/1000 | Loss: 0.00001109
Iteration 20/1000 | Loss: 0.00001109
Iteration 21/1000 | Loss: 0.00001108
Iteration 22/1000 | Loss: 0.00001107
Iteration 23/1000 | Loss: 0.00001106
Iteration 24/1000 | Loss: 0.00001105
Iteration 25/1000 | Loss: 0.00001105
Iteration 26/1000 | Loss: 0.00001104
Iteration 27/1000 | Loss: 0.00001104
Iteration 28/1000 | Loss: 0.00001104
Iteration 29/1000 | Loss: 0.00001103
Iteration 30/1000 | Loss: 0.00001103
Iteration 31/1000 | Loss: 0.00001103
Iteration 32/1000 | Loss: 0.00001103
Iteration 33/1000 | Loss: 0.00001102
Iteration 34/1000 | Loss: 0.00001102
Iteration 35/1000 | Loss: 0.00001101
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001101
Iteration 38/1000 | Loss: 0.00001101
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001101
Iteration 41/1000 | Loss: 0.00001101
Iteration 42/1000 | Loss: 0.00001101
Iteration 43/1000 | Loss: 0.00001101
Iteration 44/1000 | Loss: 0.00001101
Iteration 45/1000 | Loss: 0.00001101
Iteration 46/1000 | Loss: 0.00001100
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001100
Iteration 49/1000 | Loss: 0.00001100
Iteration 50/1000 | Loss: 0.00001100
Iteration 51/1000 | Loss: 0.00001100
Iteration 52/1000 | Loss: 0.00001100
Iteration 53/1000 | Loss: 0.00001100
Iteration 54/1000 | Loss: 0.00001099
Iteration 55/1000 | Loss: 0.00001099
Iteration 56/1000 | Loss: 0.00001099
Iteration 57/1000 | Loss: 0.00001099
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001099
Iteration 60/1000 | Loss: 0.00001099
Iteration 61/1000 | Loss: 0.00001099
Iteration 62/1000 | Loss: 0.00001099
Iteration 63/1000 | Loss: 0.00001099
Iteration 64/1000 | Loss: 0.00001099
Iteration 65/1000 | Loss: 0.00001098
Iteration 66/1000 | Loss: 0.00001098
Iteration 67/1000 | Loss: 0.00001098
Iteration 68/1000 | Loss: 0.00001098
Iteration 69/1000 | Loss: 0.00001098
Iteration 70/1000 | Loss: 0.00001098
Iteration 71/1000 | Loss: 0.00001098
Iteration 72/1000 | Loss: 0.00001097
Iteration 73/1000 | Loss: 0.00001097
Iteration 74/1000 | Loss: 0.00001097
Iteration 75/1000 | Loss: 0.00001097
Iteration 76/1000 | Loss: 0.00001097
Iteration 77/1000 | Loss: 0.00001097
Iteration 78/1000 | Loss: 0.00001097
Iteration 79/1000 | Loss: 0.00001097
Iteration 80/1000 | Loss: 0.00001097
Iteration 81/1000 | Loss: 0.00001097
Iteration 82/1000 | Loss: 0.00001097
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001097
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001096
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001096
Iteration 91/1000 | Loss: 0.00001096
Iteration 92/1000 | Loss: 0.00001096
Iteration 93/1000 | Loss: 0.00001096
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001096
Iteration 97/1000 | Loss: 0.00001096
Iteration 98/1000 | Loss: 0.00001096
Iteration 99/1000 | Loss: 0.00001096
Iteration 100/1000 | Loss: 0.00001096
Iteration 101/1000 | Loss: 0.00001096
Iteration 102/1000 | Loss: 0.00001096
Iteration 103/1000 | Loss: 0.00001096
Iteration 104/1000 | Loss: 0.00001096
Iteration 105/1000 | Loss: 0.00001096
Iteration 106/1000 | Loss: 0.00001096
Iteration 107/1000 | Loss: 0.00001096
Iteration 108/1000 | Loss: 0.00001096
Iteration 109/1000 | Loss: 0.00001096
Iteration 110/1000 | Loss: 0.00001096
Iteration 111/1000 | Loss: 0.00001096
Iteration 112/1000 | Loss: 0.00001096
Iteration 113/1000 | Loss: 0.00001096
Iteration 114/1000 | Loss: 0.00001096
Iteration 115/1000 | Loss: 0.00001096
Iteration 116/1000 | Loss: 0.00001096
Iteration 117/1000 | Loss: 0.00001096
Iteration 118/1000 | Loss: 0.00001096
Iteration 119/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.0959180144709535e-05, 1.0959180144709535e-05, 1.0959180144709535e-05, 1.0959180144709535e-05, 1.0959180144709535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0959180144709535e-05

Optimization complete. Final v2v error: 2.8185133934020996 mm

Highest mean error: 3.297948122024536 mm for frame 76

Lowest mean error: 2.5791101455688477 mm for frame 27

Saving results

Total time: 29.194700956344604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890339
Iteration 2/25 | Loss: 0.00131941
Iteration 3/25 | Loss: 0.00109868
Iteration 4/25 | Loss: 0.00103504
Iteration 5/25 | Loss: 0.00103439
Iteration 6/25 | Loss: 0.00102084
Iteration 7/25 | Loss: 0.00101826
Iteration 8/25 | Loss: 0.00101886
Iteration 9/25 | Loss: 0.00101793
Iteration 10/25 | Loss: 0.00101791
Iteration 11/25 | Loss: 0.00101791
Iteration 12/25 | Loss: 0.00101789
Iteration 13/25 | Loss: 0.00101789
Iteration 14/25 | Loss: 0.00101789
Iteration 15/25 | Loss: 0.00101788
Iteration 16/25 | Loss: 0.00101788
Iteration 17/25 | Loss: 0.00101788
Iteration 18/25 | Loss: 0.00101788
Iteration 19/25 | Loss: 0.00101788
Iteration 20/25 | Loss: 0.00101788
Iteration 21/25 | Loss: 0.00101788
Iteration 22/25 | Loss: 0.00101788
Iteration 23/25 | Loss: 0.00101788
Iteration 24/25 | Loss: 0.00101788
Iteration 25/25 | Loss: 0.00101788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.76110363
Iteration 2/25 | Loss: 0.00056344
Iteration 3/25 | Loss: 0.00056344
Iteration 4/25 | Loss: 0.00056344
Iteration 5/25 | Loss: 0.00056344
Iteration 6/25 | Loss: 0.00056344
Iteration 7/25 | Loss: 0.00056344
Iteration 8/25 | Loss: 0.00056344
Iteration 9/25 | Loss: 0.00056344
Iteration 10/25 | Loss: 0.00056344
Iteration 11/25 | Loss: 0.00056344
Iteration 12/25 | Loss: 0.00056344
Iteration 13/25 | Loss: 0.00056344
Iteration 14/25 | Loss: 0.00056344
Iteration 15/25 | Loss: 0.00056344
Iteration 16/25 | Loss: 0.00056344
Iteration 17/25 | Loss: 0.00056344
Iteration 18/25 | Loss: 0.00056344
Iteration 19/25 | Loss: 0.00056344
Iteration 20/25 | Loss: 0.00056344
Iteration 21/25 | Loss: 0.00056344
Iteration 22/25 | Loss: 0.00056344
Iteration 23/25 | Loss: 0.00056344
Iteration 24/25 | Loss: 0.00056344
Iteration 25/25 | Loss: 0.00056344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056344
Iteration 2/1000 | Loss: 0.00004161
Iteration 3/1000 | Loss: 0.00002205
Iteration 4/1000 | Loss: 0.00001718
Iteration 5/1000 | Loss: 0.00001864
Iteration 6/1000 | Loss: 0.00001552
Iteration 7/1000 | Loss: 0.00001514
Iteration 8/1000 | Loss: 0.00001480
Iteration 9/1000 | Loss: 0.00001549
Iteration 10/1000 | Loss: 0.00001418
Iteration 11/1000 | Loss: 0.00001414
Iteration 12/1000 | Loss: 0.00001413
Iteration 13/1000 | Loss: 0.00001413
Iteration 14/1000 | Loss: 0.00001517
Iteration 15/1000 | Loss: 0.00001399
Iteration 16/1000 | Loss: 0.00001398
Iteration 17/1000 | Loss: 0.00001398
Iteration 18/1000 | Loss: 0.00001398
Iteration 19/1000 | Loss: 0.00001398
Iteration 20/1000 | Loss: 0.00001398
Iteration 21/1000 | Loss: 0.00001398
Iteration 22/1000 | Loss: 0.00001398
Iteration 23/1000 | Loss: 0.00001398
Iteration 24/1000 | Loss: 0.00001398
Iteration 25/1000 | Loss: 0.00001398
Iteration 26/1000 | Loss: 0.00001398
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001395
Iteration 29/1000 | Loss: 0.00001394
Iteration 30/1000 | Loss: 0.00001394
Iteration 31/1000 | Loss: 0.00001409
Iteration 32/1000 | Loss: 0.00001398
Iteration 33/1000 | Loss: 0.00001390
Iteration 34/1000 | Loss: 0.00001389
Iteration 35/1000 | Loss: 0.00001389
Iteration 36/1000 | Loss: 0.00001389
Iteration 37/1000 | Loss: 0.00001389
Iteration 38/1000 | Loss: 0.00001389
Iteration 39/1000 | Loss: 0.00001388
Iteration 40/1000 | Loss: 0.00001388
Iteration 41/1000 | Loss: 0.00001388
Iteration 42/1000 | Loss: 0.00001388
Iteration 43/1000 | Loss: 0.00001388
Iteration 44/1000 | Loss: 0.00001388
Iteration 45/1000 | Loss: 0.00001388
Iteration 46/1000 | Loss: 0.00001388
Iteration 47/1000 | Loss: 0.00001388
Iteration 48/1000 | Loss: 0.00001388
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001387
Iteration 51/1000 | Loss: 0.00001387
Iteration 52/1000 | Loss: 0.00001387
Iteration 53/1000 | Loss: 0.00001387
Iteration 54/1000 | Loss: 0.00001386
Iteration 55/1000 | Loss: 0.00001386
Iteration 56/1000 | Loss: 0.00001386
Iteration 57/1000 | Loss: 0.00001385
Iteration 58/1000 | Loss: 0.00001385
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001384
Iteration 61/1000 | Loss: 0.00001384
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001384
Iteration 64/1000 | Loss: 0.00001384
Iteration 65/1000 | Loss: 0.00001384
Iteration 66/1000 | Loss: 0.00001383
Iteration 67/1000 | Loss: 0.00001383
Iteration 68/1000 | Loss: 0.00001383
Iteration 69/1000 | Loss: 0.00001383
Iteration 70/1000 | Loss: 0.00001383
Iteration 71/1000 | Loss: 0.00001382
Iteration 72/1000 | Loss: 0.00001382
Iteration 73/1000 | Loss: 0.00001382
Iteration 74/1000 | Loss: 0.00001382
Iteration 75/1000 | Loss: 0.00001382
Iteration 76/1000 | Loss: 0.00001382
Iteration 77/1000 | Loss: 0.00001382
Iteration 78/1000 | Loss: 0.00001382
Iteration 79/1000 | Loss: 0.00001382
Iteration 80/1000 | Loss: 0.00001382
Iteration 81/1000 | Loss: 0.00001382
Iteration 82/1000 | Loss: 0.00001382
Iteration 83/1000 | Loss: 0.00001381
Iteration 84/1000 | Loss: 0.00001381
Iteration 85/1000 | Loss: 0.00001381
Iteration 86/1000 | Loss: 0.00001381
Iteration 87/1000 | Loss: 0.00001381
Iteration 88/1000 | Loss: 0.00001380
Iteration 89/1000 | Loss: 0.00001380
Iteration 90/1000 | Loss: 0.00001379
Iteration 91/1000 | Loss: 0.00001379
Iteration 92/1000 | Loss: 0.00001379
Iteration 93/1000 | Loss: 0.00001379
Iteration 94/1000 | Loss: 0.00001379
Iteration 95/1000 | Loss: 0.00001379
Iteration 96/1000 | Loss: 0.00001379
Iteration 97/1000 | Loss: 0.00001379
Iteration 98/1000 | Loss: 0.00001379
Iteration 99/1000 | Loss: 0.00001379
Iteration 100/1000 | Loss: 0.00001379
Iteration 101/1000 | Loss: 0.00001379
Iteration 102/1000 | Loss: 0.00001379
Iteration 103/1000 | Loss: 0.00001379
Iteration 104/1000 | Loss: 0.00001379
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.3792123354505748e-05, 1.3792123354505748e-05, 1.3792123354505748e-05, 1.3792123354505748e-05, 1.3792123354505748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3792123354505748e-05

Optimization complete. Final v2v error: 3.2353105545043945 mm

Highest mean error: 3.771700143814087 mm for frame 115

Lowest mean error: 2.918562889099121 mm for frame 9

Saving results

Total time: 44.26212501525879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027872
Iteration 2/25 | Loss: 0.00453085
Iteration 3/25 | Loss: 0.00351996
Iteration 4/25 | Loss: 0.00303293
Iteration 5/25 | Loss: 0.00266431
Iteration 6/25 | Loss: 0.00235608
Iteration 7/25 | Loss: 0.00205817
Iteration 8/25 | Loss: 0.00196861
Iteration 9/25 | Loss: 0.00185239
Iteration 10/25 | Loss: 0.00182002
Iteration 11/25 | Loss: 0.00177822
Iteration 12/25 | Loss: 0.00180613
Iteration 13/25 | Loss: 0.00176334
Iteration 14/25 | Loss: 0.00172501
Iteration 15/25 | Loss: 0.00171793
Iteration 16/25 | Loss: 0.00171734
Iteration 17/25 | Loss: 0.00170954
Iteration 18/25 | Loss: 0.00169956
Iteration 19/25 | Loss: 0.00169732
Iteration 20/25 | Loss: 0.00169062
Iteration 21/25 | Loss: 0.00168964
Iteration 22/25 | Loss: 0.00168929
Iteration 23/25 | Loss: 0.00168900
Iteration 24/25 | Loss: 0.00168875
Iteration 25/25 | Loss: 0.00168866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31858730
Iteration 2/25 | Loss: 0.02154112
Iteration 3/25 | Loss: 0.00467556
Iteration 4/25 | Loss: 0.00444062
Iteration 5/25 | Loss: 0.00444054
Iteration 6/25 | Loss: 0.00444053
Iteration 7/25 | Loss: 0.00444053
Iteration 8/25 | Loss: 0.00444053
Iteration 9/25 | Loss: 0.00444053
Iteration 10/25 | Loss: 0.00444053
Iteration 11/25 | Loss: 0.00444053
Iteration 12/25 | Loss: 0.00444053
Iteration 13/25 | Loss: 0.00444053
Iteration 14/25 | Loss: 0.00444053
Iteration 15/25 | Loss: 0.00444053
Iteration 16/25 | Loss: 0.00444053
Iteration 17/25 | Loss: 0.00444053
Iteration 18/25 | Loss: 0.00444053
Iteration 19/25 | Loss: 0.00444053
Iteration 20/25 | Loss: 0.00444053
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004440528806298971, 0.004440528806298971, 0.004440528806298971, 0.004440528806298971, 0.004440528806298971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004440528806298971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00444053
Iteration 2/1000 | Loss: 0.00568728
Iteration 3/1000 | Loss: 0.00999045
Iteration 4/1000 | Loss: 0.00061683
Iteration 5/1000 | Loss: 0.00053369
Iteration 6/1000 | Loss: 0.00063597
Iteration 7/1000 | Loss: 0.00270208
Iteration 8/1000 | Loss: 0.00362298
Iteration 9/1000 | Loss: 0.00362913
Iteration 10/1000 | Loss: 0.00151889
Iteration 11/1000 | Loss: 0.00069515
Iteration 12/1000 | Loss: 0.00079869
Iteration 13/1000 | Loss: 0.00060952
Iteration 14/1000 | Loss: 0.00067240
Iteration 15/1000 | Loss: 0.00036697
Iteration 16/1000 | Loss: 0.00041396
Iteration 17/1000 | Loss: 0.00065783
Iteration 18/1000 | Loss: 0.00591341
Iteration 19/1000 | Loss: 0.01544237
Iteration 20/1000 | Loss: 0.00888708
Iteration 21/1000 | Loss: 0.01507757
Iteration 22/1000 | Loss: 0.00457638
Iteration 23/1000 | Loss: 0.00083989
Iteration 24/1000 | Loss: 0.00227684
Iteration 25/1000 | Loss: 0.00059308
Iteration 26/1000 | Loss: 0.00075728
Iteration 27/1000 | Loss: 0.00258721
Iteration 28/1000 | Loss: 0.00248668
Iteration 29/1000 | Loss: 0.00027812
Iteration 30/1000 | Loss: 0.00038452
Iteration 31/1000 | Loss: 0.00077941
Iteration 32/1000 | Loss: 0.00037274
Iteration 33/1000 | Loss: 0.00040271
Iteration 34/1000 | Loss: 0.00074333
Iteration 35/1000 | Loss: 0.00018344
Iteration 36/1000 | Loss: 0.00017810
Iteration 37/1000 | Loss: 0.00250434
Iteration 38/1000 | Loss: 0.00033181
Iteration 39/1000 | Loss: 0.00024520
Iteration 40/1000 | Loss: 0.00422105
Iteration 41/1000 | Loss: 0.00008335
Iteration 42/1000 | Loss: 0.00018845
Iteration 43/1000 | Loss: 0.00014218
Iteration 44/1000 | Loss: 0.00349903
Iteration 45/1000 | Loss: 0.00041379
Iteration 46/1000 | Loss: 0.00011106
Iteration 47/1000 | Loss: 0.00017009
Iteration 48/1000 | Loss: 0.00068100
Iteration 49/1000 | Loss: 0.00035825
Iteration 50/1000 | Loss: 0.00022175
Iteration 51/1000 | Loss: 0.00036412
Iteration 52/1000 | Loss: 0.00025141
Iteration 53/1000 | Loss: 0.00007037
Iteration 54/1000 | Loss: 0.00003179
Iteration 55/1000 | Loss: 0.00037706
Iteration 56/1000 | Loss: 0.00002866
Iteration 57/1000 | Loss: 0.00002624
Iteration 58/1000 | Loss: 0.00003250
Iteration 59/1000 | Loss: 0.00002511
Iteration 60/1000 | Loss: 0.00002352
Iteration 61/1000 | Loss: 0.00025751
Iteration 62/1000 | Loss: 0.00021059
Iteration 63/1000 | Loss: 0.00009042
Iteration 64/1000 | Loss: 0.00009043
Iteration 65/1000 | Loss: 0.00002347
Iteration 66/1000 | Loss: 0.00002164
Iteration 67/1000 | Loss: 0.00023540
Iteration 68/1000 | Loss: 0.00002025
Iteration 69/1000 | Loss: 0.00001900
Iteration 70/1000 | Loss: 0.00001854
Iteration 71/1000 | Loss: 0.00001823
Iteration 72/1000 | Loss: 0.00001799
Iteration 73/1000 | Loss: 0.00001780
Iteration 74/1000 | Loss: 0.00001779
Iteration 75/1000 | Loss: 0.00001777
Iteration 76/1000 | Loss: 0.00001776
Iteration 77/1000 | Loss: 0.00001776
Iteration 78/1000 | Loss: 0.00001776
Iteration 79/1000 | Loss: 0.00001776
Iteration 80/1000 | Loss: 0.00001775
Iteration 81/1000 | Loss: 0.00001775
Iteration 82/1000 | Loss: 0.00001774
Iteration 83/1000 | Loss: 0.00001771
Iteration 84/1000 | Loss: 0.00001771
Iteration 85/1000 | Loss: 0.00001769
Iteration 86/1000 | Loss: 0.00001769
Iteration 87/1000 | Loss: 0.00001769
Iteration 88/1000 | Loss: 0.00001768
Iteration 89/1000 | Loss: 0.00001768
Iteration 90/1000 | Loss: 0.00001768
Iteration 91/1000 | Loss: 0.00001767
Iteration 92/1000 | Loss: 0.00001767
Iteration 93/1000 | Loss: 0.00001766
Iteration 94/1000 | Loss: 0.00001764
Iteration 95/1000 | Loss: 0.00001762
Iteration 96/1000 | Loss: 0.00001762
Iteration 97/1000 | Loss: 0.00001761
Iteration 98/1000 | Loss: 0.00001761
Iteration 99/1000 | Loss: 0.00001761
Iteration 100/1000 | Loss: 0.00001761
Iteration 101/1000 | Loss: 0.00001761
Iteration 102/1000 | Loss: 0.00001761
Iteration 103/1000 | Loss: 0.00001760
Iteration 104/1000 | Loss: 0.00001760
Iteration 105/1000 | Loss: 0.00001760
Iteration 106/1000 | Loss: 0.00001760
Iteration 107/1000 | Loss: 0.00001760
Iteration 108/1000 | Loss: 0.00001760
Iteration 109/1000 | Loss: 0.00001760
Iteration 110/1000 | Loss: 0.00001759
Iteration 111/1000 | Loss: 0.00001759
Iteration 112/1000 | Loss: 0.00001759
Iteration 113/1000 | Loss: 0.00001759
Iteration 114/1000 | Loss: 0.00001759
Iteration 115/1000 | Loss: 0.00001759
Iteration 116/1000 | Loss: 0.00001758
Iteration 117/1000 | Loss: 0.00001758
Iteration 118/1000 | Loss: 0.00001758
Iteration 119/1000 | Loss: 0.00001758
Iteration 120/1000 | Loss: 0.00001757
Iteration 121/1000 | Loss: 0.00001757
Iteration 122/1000 | Loss: 0.00001757
Iteration 123/1000 | Loss: 0.00001757
Iteration 124/1000 | Loss: 0.00001756
Iteration 125/1000 | Loss: 0.00001756
Iteration 126/1000 | Loss: 0.00001756
Iteration 127/1000 | Loss: 0.00001756
Iteration 128/1000 | Loss: 0.00001756
Iteration 129/1000 | Loss: 0.00001756
Iteration 130/1000 | Loss: 0.00001756
Iteration 131/1000 | Loss: 0.00001755
Iteration 132/1000 | Loss: 0.00001755
Iteration 133/1000 | Loss: 0.00001755
Iteration 134/1000 | Loss: 0.00001755
Iteration 135/1000 | Loss: 0.00001755
Iteration 136/1000 | Loss: 0.00001755
Iteration 137/1000 | Loss: 0.00001755
Iteration 138/1000 | Loss: 0.00010777
Iteration 139/1000 | Loss: 0.00012395
Iteration 140/1000 | Loss: 0.00007512
Iteration 141/1000 | Loss: 0.00002695
Iteration 142/1000 | Loss: 0.00001768
Iteration 143/1000 | Loss: 0.00001622
Iteration 144/1000 | Loss: 0.00001569
Iteration 145/1000 | Loss: 0.00001533
Iteration 146/1000 | Loss: 0.00001526
Iteration 147/1000 | Loss: 0.00001524
Iteration 148/1000 | Loss: 0.00001524
Iteration 149/1000 | Loss: 0.00001523
Iteration 150/1000 | Loss: 0.00001523
Iteration 151/1000 | Loss: 0.00001522
Iteration 152/1000 | Loss: 0.00001522
Iteration 153/1000 | Loss: 0.00001522
Iteration 154/1000 | Loss: 0.00001522
Iteration 155/1000 | Loss: 0.00001522
Iteration 156/1000 | Loss: 0.00001521
Iteration 157/1000 | Loss: 0.00001521
Iteration 158/1000 | Loss: 0.00001521
Iteration 159/1000 | Loss: 0.00001521
Iteration 160/1000 | Loss: 0.00001520
Iteration 161/1000 | Loss: 0.00001520
Iteration 162/1000 | Loss: 0.00001519
Iteration 163/1000 | Loss: 0.00001519
Iteration 164/1000 | Loss: 0.00001518
Iteration 165/1000 | Loss: 0.00001518
Iteration 166/1000 | Loss: 0.00001517
Iteration 167/1000 | Loss: 0.00001516
Iteration 168/1000 | Loss: 0.00001516
Iteration 169/1000 | Loss: 0.00001515
Iteration 170/1000 | Loss: 0.00001515
Iteration 171/1000 | Loss: 0.00001514
Iteration 172/1000 | Loss: 0.00001514
Iteration 173/1000 | Loss: 0.00001513
Iteration 174/1000 | Loss: 0.00001513
Iteration 175/1000 | Loss: 0.00001513
Iteration 176/1000 | Loss: 0.00001512
Iteration 177/1000 | Loss: 0.00001512
Iteration 178/1000 | Loss: 0.00001511
Iteration 179/1000 | Loss: 0.00001509
Iteration 180/1000 | Loss: 0.00001508
Iteration 181/1000 | Loss: 0.00001508
Iteration 182/1000 | Loss: 0.00001508
Iteration 183/1000 | Loss: 0.00001508
Iteration 184/1000 | Loss: 0.00001508
Iteration 185/1000 | Loss: 0.00001508
Iteration 186/1000 | Loss: 0.00001508
Iteration 187/1000 | Loss: 0.00001508
Iteration 188/1000 | Loss: 0.00001508
Iteration 189/1000 | Loss: 0.00001508
Iteration 190/1000 | Loss: 0.00001508
Iteration 191/1000 | Loss: 0.00001507
Iteration 192/1000 | Loss: 0.00001507
Iteration 193/1000 | Loss: 0.00001507
Iteration 194/1000 | Loss: 0.00001506
Iteration 195/1000 | Loss: 0.00001506
Iteration 196/1000 | Loss: 0.00001506
Iteration 197/1000 | Loss: 0.00001506
Iteration 198/1000 | Loss: 0.00001506
Iteration 199/1000 | Loss: 0.00001505
Iteration 200/1000 | Loss: 0.00001505
Iteration 201/1000 | Loss: 0.00001505
Iteration 202/1000 | Loss: 0.00001505
Iteration 203/1000 | Loss: 0.00001505
Iteration 204/1000 | Loss: 0.00001505
Iteration 205/1000 | Loss: 0.00001505
Iteration 206/1000 | Loss: 0.00001505
Iteration 207/1000 | Loss: 0.00001505
Iteration 208/1000 | Loss: 0.00001505
Iteration 209/1000 | Loss: 0.00001505
Iteration 210/1000 | Loss: 0.00001505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.5052893104439136e-05, 1.5052893104439136e-05, 1.5052893104439136e-05, 1.5052893104439136e-05, 1.5052893104439136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5052893104439136e-05

Optimization complete. Final v2v error: 3.2757694721221924 mm

Highest mean error: 10.99520492553711 mm for frame 153

Lowest mean error: 2.898132801055908 mm for frame 127

Saving results

Total time: 194.76823091506958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990769
Iteration 2/25 | Loss: 0.00146971
Iteration 3/25 | Loss: 0.00123504
Iteration 4/25 | Loss: 0.00110260
Iteration 5/25 | Loss: 0.00104375
Iteration 6/25 | Loss: 0.00103311
Iteration 7/25 | Loss: 0.00103662
Iteration 8/25 | Loss: 0.00103059
Iteration 9/25 | Loss: 0.00102714
Iteration 10/25 | Loss: 0.00102457
Iteration 11/25 | Loss: 0.00102351
Iteration 12/25 | Loss: 0.00102255
Iteration 13/25 | Loss: 0.00102086
Iteration 14/25 | Loss: 0.00101901
Iteration 15/25 | Loss: 0.00101861
Iteration 16/25 | Loss: 0.00101845
Iteration 17/25 | Loss: 0.00101841
Iteration 18/25 | Loss: 0.00101837
Iteration 19/25 | Loss: 0.00101836
Iteration 20/25 | Loss: 0.00101836
Iteration 21/25 | Loss: 0.00101836
Iteration 22/25 | Loss: 0.00101836
Iteration 23/25 | Loss: 0.00101836
Iteration 24/25 | Loss: 0.00101836
Iteration 25/25 | Loss: 0.00101835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50973654
Iteration 2/25 | Loss: 0.00058150
Iteration 3/25 | Loss: 0.00058149
Iteration 4/25 | Loss: 0.00058149
Iteration 5/25 | Loss: 0.00058149
Iteration 6/25 | Loss: 0.00058148
Iteration 7/25 | Loss: 0.00058148
Iteration 8/25 | Loss: 0.00058148
Iteration 9/25 | Loss: 0.00058148
Iteration 10/25 | Loss: 0.00058148
Iteration 11/25 | Loss: 0.00058148
Iteration 12/25 | Loss: 0.00058148
Iteration 13/25 | Loss: 0.00058148
Iteration 14/25 | Loss: 0.00058148
Iteration 15/25 | Loss: 0.00058148
Iteration 16/25 | Loss: 0.00058148
Iteration 17/25 | Loss: 0.00058148
Iteration 18/25 | Loss: 0.00058148
Iteration 19/25 | Loss: 0.00058148
Iteration 20/25 | Loss: 0.00058148
Iteration 21/25 | Loss: 0.00058148
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005814834730699658, 0.0005814834730699658, 0.0005814834730699658, 0.0005814834730699658, 0.0005814834730699658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005814834730699658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058148
Iteration 2/1000 | Loss: 0.00009175
Iteration 3/1000 | Loss: 0.00003966
Iteration 4/1000 | Loss: 0.00002720
Iteration 5/1000 | Loss: 0.00002320
Iteration 6/1000 | Loss: 0.00004425
Iteration 7/1000 | Loss: 0.00002010
Iteration 8/1000 | Loss: 0.00003193
Iteration 9/1000 | Loss: 0.00002447
Iteration 10/1000 | Loss: 0.00001884
Iteration 11/1000 | Loss: 0.00001882
Iteration 12/1000 | Loss: 0.00001859
Iteration 13/1000 | Loss: 0.00009473
Iteration 14/1000 | Loss: 0.00004050
Iteration 15/1000 | Loss: 0.00001830
Iteration 16/1000 | Loss: 0.00001814
Iteration 17/1000 | Loss: 0.00001811
Iteration 18/1000 | Loss: 0.00001809
Iteration 19/1000 | Loss: 0.00001809
Iteration 20/1000 | Loss: 0.00001809
Iteration 21/1000 | Loss: 0.00001799
Iteration 22/1000 | Loss: 0.00001798
Iteration 23/1000 | Loss: 0.00001796
Iteration 24/1000 | Loss: 0.00001793
Iteration 25/1000 | Loss: 0.00001792
Iteration 26/1000 | Loss: 0.00001792
Iteration 27/1000 | Loss: 0.00001791
Iteration 28/1000 | Loss: 0.00001791
Iteration 29/1000 | Loss: 0.00001790
Iteration 30/1000 | Loss: 0.00001790
Iteration 31/1000 | Loss: 0.00001790
Iteration 32/1000 | Loss: 0.00001789
Iteration 33/1000 | Loss: 0.00001789
Iteration 34/1000 | Loss: 0.00001788
Iteration 35/1000 | Loss: 0.00001788
Iteration 36/1000 | Loss: 0.00001787
Iteration 37/1000 | Loss: 0.00001787
Iteration 38/1000 | Loss: 0.00001786
Iteration 39/1000 | Loss: 0.00001786
Iteration 40/1000 | Loss: 0.00001785
Iteration 41/1000 | Loss: 0.00001785
Iteration 42/1000 | Loss: 0.00001784
Iteration 43/1000 | Loss: 0.00001783
Iteration 44/1000 | Loss: 0.00001783
Iteration 45/1000 | Loss: 0.00001782
Iteration 46/1000 | Loss: 0.00001782
Iteration 47/1000 | Loss: 0.00004493
Iteration 48/1000 | Loss: 0.00001786
Iteration 49/1000 | Loss: 0.00002120
Iteration 50/1000 | Loss: 0.00001782
Iteration 51/1000 | Loss: 0.00001781
Iteration 52/1000 | Loss: 0.00001780
Iteration 53/1000 | Loss: 0.00001780
Iteration 54/1000 | Loss: 0.00001779
Iteration 55/1000 | Loss: 0.00001779
Iteration 56/1000 | Loss: 0.00001779
Iteration 57/1000 | Loss: 0.00001778
Iteration 58/1000 | Loss: 0.00001778
Iteration 59/1000 | Loss: 0.00001778
Iteration 60/1000 | Loss: 0.00001778
Iteration 61/1000 | Loss: 0.00001777
Iteration 62/1000 | Loss: 0.00002487
Iteration 63/1000 | Loss: 0.00001775
Iteration 64/1000 | Loss: 0.00001775
Iteration 65/1000 | Loss: 0.00001775
Iteration 66/1000 | Loss: 0.00001775
Iteration 67/1000 | Loss: 0.00001775
Iteration 68/1000 | Loss: 0.00001775
Iteration 69/1000 | Loss: 0.00001775
Iteration 70/1000 | Loss: 0.00001775
Iteration 71/1000 | Loss: 0.00001775
Iteration 72/1000 | Loss: 0.00001775
Iteration 73/1000 | Loss: 0.00001774
Iteration 74/1000 | Loss: 0.00001774
Iteration 75/1000 | Loss: 0.00001774
Iteration 76/1000 | Loss: 0.00001774
Iteration 77/1000 | Loss: 0.00001774
Iteration 78/1000 | Loss: 0.00001774
Iteration 79/1000 | Loss: 0.00001774
Iteration 80/1000 | Loss: 0.00001774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.7744147044140846e-05, 1.7744147044140846e-05, 1.7744147044140846e-05, 1.7744147044140846e-05, 1.7744147044140846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7744147044140846e-05

Optimization complete. Final v2v error: 3.527907609939575 mm

Highest mean error: 9.649101257324219 mm for frame 23

Lowest mean error: 2.9070825576782227 mm for frame 121

Saving results

Total time: 65.15272188186646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00620861
Iteration 2/25 | Loss: 0.00159789
Iteration 3/25 | Loss: 0.00126899
Iteration 4/25 | Loss: 0.00122296
Iteration 5/25 | Loss: 0.00122276
Iteration 6/25 | Loss: 0.00120318
Iteration 7/25 | Loss: 0.00117555
Iteration 8/25 | Loss: 0.00116822
Iteration 9/25 | Loss: 0.00116494
Iteration 10/25 | Loss: 0.00115663
Iteration 11/25 | Loss: 0.00115267
Iteration 12/25 | Loss: 0.00115156
Iteration 13/25 | Loss: 0.00115124
Iteration 14/25 | Loss: 0.00115108
Iteration 15/25 | Loss: 0.00115100
Iteration 16/25 | Loss: 0.00115100
Iteration 17/25 | Loss: 0.00115100
Iteration 18/25 | Loss: 0.00115100
Iteration 19/25 | Loss: 0.00115100
Iteration 20/25 | Loss: 0.00115100
Iteration 21/25 | Loss: 0.00115100
Iteration 22/25 | Loss: 0.00115100
Iteration 23/25 | Loss: 0.00115100
Iteration 24/25 | Loss: 0.00115100
Iteration 25/25 | Loss: 0.00115099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86960626
Iteration 2/25 | Loss: 0.00089840
Iteration 3/25 | Loss: 0.00081852
Iteration 4/25 | Loss: 0.00081852
Iteration 5/25 | Loss: 0.00081852
Iteration 6/25 | Loss: 0.00081852
Iteration 7/25 | Loss: 0.00081852
Iteration 8/25 | Loss: 0.00081852
Iteration 9/25 | Loss: 0.00081852
Iteration 10/25 | Loss: 0.00081852
Iteration 11/25 | Loss: 0.00081852
Iteration 12/25 | Loss: 0.00081852
Iteration 13/25 | Loss: 0.00081852
Iteration 14/25 | Loss: 0.00081852
Iteration 15/25 | Loss: 0.00081852
Iteration 16/25 | Loss: 0.00081852
Iteration 17/25 | Loss: 0.00081852
Iteration 18/25 | Loss: 0.00081852
Iteration 19/25 | Loss: 0.00081852
Iteration 20/25 | Loss: 0.00081852
Iteration 21/25 | Loss: 0.00081852
Iteration 22/25 | Loss: 0.00081852
Iteration 23/25 | Loss: 0.00081852
Iteration 24/25 | Loss: 0.00081852
Iteration 25/25 | Loss: 0.00081852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081852
Iteration 2/1000 | Loss: 0.00012054
Iteration 3/1000 | Loss: 0.00005626
Iteration 4/1000 | Loss: 0.00018107
Iteration 5/1000 | Loss: 0.00009956
Iteration 6/1000 | Loss: 0.00004167
Iteration 7/1000 | Loss: 0.00003754
Iteration 8/1000 | Loss: 0.00003583
Iteration 9/1000 | Loss: 0.00040527
Iteration 10/1000 | Loss: 0.00021203
Iteration 11/1000 | Loss: 0.00038331
Iteration 12/1000 | Loss: 0.00004893
Iteration 13/1000 | Loss: 0.00003871
Iteration 14/1000 | Loss: 0.00003336
Iteration 15/1000 | Loss: 0.00003059
Iteration 16/1000 | Loss: 0.00023827
Iteration 17/1000 | Loss: 0.00007184
Iteration 18/1000 | Loss: 0.00019092
Iteration 19/1000 | Loss: 0.00010532
Iteration 20/1000 | Loss: 0.00017872
Iteration 21/1000 | Loss: 0.00003487
Iteration 22/1000 | Loss: 0.00007603
Iteration 23/1000 | Loss: 0.00031289
Iteration 24/1000 | Loss: 0.00002964
Iteration 25/1000 | Loss: 0.00002526
Iteration 26/1000 | Loss: 0.00002415
Iteration 27/1000 | Loss: 0.00002333
Iteration 28/1000 | Loss: 0.00002284
Iteration 29/1000 | Loss: 0.00002242
Iteration 30/1000 | Loss: 0.00036509
Iteration 31/1000 | Loss: 0.00012242
Iteration 32/1000 | Loss: 0.00015821
Iteration 33/1000 | Loss: 0.00002650
Iteration 34/1000 | Loss: 0.00002364
Iteration 35/1000 | Loss: 0.00002214
Iteration 36/1000 | Loss: 0.00008953
Iteration 37/1000 | Loss: 0.00008787
Iteration 38/1000 | Loss: 0.00006969
Iteration 39/1000 | Loss: 0.00002122
Iteration 40/1000 | Loss: 0.00002055
Iteration 41/1000 | Loss: 0.00002016
Iteration 42/1000 | Loss: 0.00001992
Iteration 43/1000 | Loss: 0.00001985
Iteration 44/1000 | Loss: 0.00001985
Iteration 45/1000 | Loss: 0.00001984
Iteration 46/1000 | Loss: 0.00010237
Iteration 47/1000 | Loss: 0.00002438
Iteration 48/1000 | Loss: 0.00003260
Iteration 49/1000 | Loss: 0.00001982
Iteration 50/1000 | Loss: 0.00001975
Iteration 51/1000 | Loss: 0.00001971
Iteration 52/1000 | Loss: 0.00001965
Iteration 53/1000 | Loss: 0.00001965
Iteration 54/1000 | Loss: 0.00001965
Iteration 55/1000 | Loss: 0.00001965
Iteration 56/1000 | Loss: 0.00001965
Iteration 57/1000 | Loss: 0.00001965
Iteration 58/1000 | Loss: 0.00001965
Iteration 59/1000 | Loss: 0.00001965
Iteration 60/1000 | Loss: 0.00001965
Iteration 61/1000 | Loss: 0.00001964
Iteration 62/1000 | Loss: 0.00001964
Iteration 63/1000 | Loss: 0.00001964
Iteration 64/1000 | Loss: 0.00001963
Iteration 65/1000 | Loss: 0.00001963
Iteration 66/1000 | Loss: 0.00001963
Iteration 67/1000 | Loss: 0.00001963
Iteration 68/1000 | Loss: 0.00001963
Iteration 69/1000 | Loss: 0.00001963
Iteration 70/1000 | Loss: 0.00001957
Iteration 71/1000 | Loss: 0.00001957
Iteration 72/1000 | Loss: 0.00001956
Iteration 73/1000 | Loss: 0.00001956
Iteration 74/1000 | Loss: 0.00001955
Iteration 75/1000 | Loss: 0.00001955
Iteration 76/1000 | Loss: 0.00001954
Iteration 77/1000 | Loss: 0.00001954
Iteration 78/1000 | Loss: 0.00001954
Iteration 79/1000 | Loss: 0.00001954
Iteration 80/1000 | Loss: 0.00001953
Iteration 81/1000 | Loss: 0.00001953
Iteration 82/1000 | Loss: 0.00001953
Iteration 83/1000 | Loss: 0.00001953
Iteration 84/1000 | Loss: 0.00001953
Iteration 85/1000 | Loss: 0.00001953
Iteration 86/1000 | Loss: 0.00001953
Iteration 87/1000 | Loss: 0.00001953
Iteration 88/1000 | Loss: 0.00001953
Iteration 89/1000 | Loss: 0.00001952
Iteration 90/1000 | Loss: 0.00001952
Iteration 91/1000 | Loss: 0.00001951
Iteration 92/1000 | Loss: 0.00001951
Iteration 93/1000 | Loss: 0.00001951
Iteration 94/1000 | Loss: 0.00001951
Iteration 95/1000 | Loss: 0.00001951
Iteration 96/1000 | Loss: 0.00001951
Iteration 97/1000 | Loss: 0.00001951
Iteration 98/1000 | Loss: 0.00001951
Iteration 99/1000 | Loss: 0.00001950
Iteration 100/1000 | Loss: 0.00001950
Iteration 101/1000 | Loss: 0.00001950
Iteration 102/1000 | Loss: 0.00001950
Iteration 103/1000 | Loss: 0.00001950
Iteration 104/1000 | Loss: 0.00001950
Iteration 105/1000 | Loss: 0.00001950
Iteration 106/1000 | Loss: 0.00001950
Iteration 107/1000 | Loss: 0.00001950
Iteration 108/1000 | Loss: 0.00001950
Iteration 109/1000 | Loss: 0.00001950
Iteration 110/1000 | Loss: 0.00001950
Iteration 111/1000 | Loss: 0.00001950
Iteration 112/1000 | Loss: 0.00001950
Iteration 113/1000 | Loss: 0.00001950
Iteration 114/1000 | Loss: 0.00001950
Iteration 115/1000 | Loss: 0.00001950
Iteration 116/1000 | Loss: 0.00001950
Iteration 117/1000 | Loss: 0.00001950
Iteration 118/1000 | Loss: 0.00001950
Iteration 119/1000 | Loss: 0.00001950
Iteration 120/1000 | Loss: 0.00001950
Iteration 121/1000 | Loss: 0.00001949
Iteration 122/1000 | Loss: 0.00001949
Iteration 123/1000 | Loss: 0.00001949
Iteration 124/1000 | Loss: 0.00001949
Iteration 125/1000 | Loss: 0.00001949
Iteration 126/1000 | Loss: 0.00001949
Iteration 127/1000 | Loss: 0.00001949
Iteration 128/1000 | Loss: 0.00001949
Iteration 129/1000 | Loss: 0.00001949
Iteration 130/1000 | Loss: 0.00001949
Iteration 131/1000 | Loss: 0.00001949
Iteration 132/1000 | Loss: 0.00001949
Iteration 133/1000 | Loss: 0.00001949
Iteration 134/1000 | Loss: 0.00001949
Iteration 135/1000 | Loss: 0.00001949
Iteration 136/1000 | Loss: 0.00001949
Iteration 137/1000 | Loss: 0.00001949
Iteration 138/1000 | Loss: 0.00001949
Iteration 139/1000 | Loss: 0.00001949
Iteration 140/1000 | Loss: 0.00001949
Iteration 141/1000 | Loss: 0.00001949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.9487837562337518e-05, 1.9487837562337518e-05, 1.9487837562337518e-05, 1.9487837562337518e-05, 1.9487837562337518e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9487837562337518e-05

Optimization complete. Final v2v error: 3.7228446006774902 mm

Highest mean error: 4.697082042694092 mm for frame 59

Lowest mean error: 2.970383882522583 mm for frame 233

Saving results

Total time: 113.38568091392517
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845253
Iteration 2/25 | Loss: 0.00131338
Iteration 3/25 | Loss: 0.00111116
Iteration 4/25 | Loss: 0.00108500
Iteration 5/25 | Loss: 0.00107995
Iteration 6/25 | Loss: 0.00107855
Iteration 7/25 | Loss: 0.00107843
Iteration 8/25 | Loss: 0.00107843
Iteration 9/25 | Loss: 0.00107843
Iteration 10/25 | Loss: 0.00107843
Iteration 11/25 | Loss: 0.00107843
Iteration 12/25 | Loss: 0.00107843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00107842858415097, 0.00107842858415097, 0.00107842858415097, 0.00107842858415097, 0.00107842858415097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00107842858415097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37656403
Iteration 2/25 | Loss: 0.00052188
Iteration 3/25 | Loss: 0.00052188
Iteration 4/25 | Loss: 0.00052188
Iteration 5/25 | Loss: 0.00052188
Iteration 6/25 | Loss: 0.00052188
Iteration 7/25 | Loss: 0.00052188
Iteration 8/25 | Loss: 0.00052188
Iteration 9/25 | Loss: 0.00052188
Iteration 10/25 | Loss: 0.00052188
Iteration 11/25 | Loss: 0.00052188
Iteration 12/25 | Loss: 0.00052188
Iteration 13/25 | Loss: 0.00052188
Iteration 14/25 | Loss: 0.00052188
Iteration 15/25 | Loss: 0.00052188
Iteration 16/25 | Loss: 0.00052188
Iteration 17/25 | Loss: 0.00052188
Iteration 18/25 | Loss: 0.00052188
Iteration 19/25 | Loss: 0.00052188
Iteration 20/25 | Loss: 0.00052188
Iteration 21/25 | Loss: 0.00052188
Iteration 22/25 | Loss: 0.00052188
Iteration 23/25 | Loss: 0.00052188
Iteration 24/25 | Loss: 0.00052188
Iteration 25/25 | Loss: 0.00052188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052188
Iteration 2/1000 | Loss: 0.00004894
Iteration 3/1000 | Loss: 0.00002997
Iteration 4/1000 | Loss: 0.00002521
Iteration 5/1000 | Loss: 0.00002312
Iteration 6/1000 | Loss: 0.00002150
Iteration 7/1000 | Loss: 0.00002077
Iteration 8/1000 | Loss: 0.00002020
Iteration 9/1000 | Loss: 0.00001970
Iteration 10/1000 | Loss: 0.00001938
Iteration 11/1000 | Loss: 0.00001912
Iteration 12/1000 | Loss: 0.00001908
Iteration 13/1000 | Loss: 0.00001893
Iteration 14/1000 | Loss: 0.00001885
Iteration 15/1000 | Loss: 0.00001876
Iteration 16/1000 | Loss: 0.00001876
Iteration 17/1000 | Loss: 0.00001875
Iteration 18/1000 | Loss: 0.00001874
Iteration 19/1000 | Loss: 0.00001874
Iteration 20/1000 | Loss: 0.00001874
Iteration 21/1000 | Loss: 0.00001873
Iteration 22/1000 | Loss: 0.00001873
Iteration 23/1000 | Loss: 0.00001872
Iteration 24/1000 | Loss: 0.00001872
Iteration 25/1000 | Loss: 0.00001872
Iteration 26/1000 | Loss: 0.00001871
Iteration 27/1000 | Loss: 0.00001871
Iteration 28/1000 | Loss: 0.00001871
Iteration 29/1000 | Loss: 0.00001871
Iteration 30/1000 | Loss: 0.00001871
Iteration 31/1000 | Loss: 0.00001870
Iteration 32/1000 | Loss: 0.00001870
Iteration 33/1000 | Loss: 0.00001869
Iteration 34/1000 | Loss: 0.00001869
Iteration 35/1000 | Loss: 0.00001868
Iteration 36/1000 | Loss: 0.00001868
Iteration 37/1000 | Loss: 0.00001868
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001866
Iteration 41/1000 | Loss: 0.00001866
Iteration 42/1000 | Loss: 0.00001866
Iteration 43/1000 | Loss: 0.00001866
Iteration 44/1000 | Loss: 0.00001865
Iteration 45/1000 | Loss: 0.00001865
Iteration 46/1000 | Loss: 0.00001865
Iteration 47/1000 | Loss: 0.00001865
Iteration 48/1000 | Loss: 0.00001865
Iteration 49/1000 | Loss: 0.00001865
Iteration 50/1000 | Loss: 0.00001864
Iteration 51/1000 | Loss: 0.00001864
Iteration 52/1000 | Loss: 0.00001864
Iteration 53/1000 | Loss: 0.00001864
Iteration 54/1000 | Loss: 0.00001864
Iteration 55/1000 | Loss: 0.00001864
Iteration 56/1000 | Loss: 0.00001864
Iteration 57/1000 | Loss: 0.00001864
Iteration 58/1000 | Loss: 0.00001864
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001864
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001863
Iteration 65/1000 | Loss: 0.00001863
Iteration 66/1000 | Loss: 0.00001863
Iteration 67/1000 | Loss: 0.00001863
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001863
Iteration 71/1000 | Loss: 0.00001863
Iteration 72/1000 | Loss: 0.00001863
Iteration 73/1000 | Loss: 0.00001863
Iteration 74/1000 | Loss: 0.00001863
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001863
Iteration 77/1000 | Loss: 0.00001863
Iteration 78/1000 | Loss: 0.00001863
Iteration 79/1000 | Loss: 0.00001863
Iteration 80/1000 | Loss: 0.00001862
Iteration 81/1000 | Loss: 0.00001862
Iteration 82/1000 | Loss: 0.00001862
Iteration 83/1000 | Loss: 0.00001862
Iteration 84/1000 | Loss: 0.00001862
Iteration 85/1000 | Loss: 0.00001862
Iteration 86/1000 | Loss: 0.00001862
Iteration 87/1000 | Loss: 0.00001862
Iteration 88/1000 | Loss: 0.00001862
Iteration 89/1000 | Loss: 0.00001862
Iteration 90/1000 | Loss: 0.00001862
Iteration 91/1000 | Loss: 0.00001862
Iteration 92/1000 | Loss: 0.00001862
Iteration 93/1000 | Loss: 0.00001862
Iteration 94/1000 | Loss: 0.00001861
Iteration 95/1000 | Loss: 0.00001861
Iteration 96/1000 | Loss: 0.00001861
Iteration 97/1000 | Loss: 0.00001861
Iteration 98/1000 | Loss: 0.00001861
Iteration 99/1000 | Loss: 0.00001861
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001861
Iteration 102/1000 | Loss: 0.00001861
Iteration 103/1000 | Loss: 0.00001861
Iteration 104/1000 | Loss: 0.00001861
Iteration 105/1000 | Loss: 0.00001861
Iteration 106/1000 | Loss: 0.00001861
Iteration 107/1000 | Loss: 0.00001861
Iteration 108/1000 | Loss: 0.00001861
Iteration 109/1000 | Loss: 0.00001860
Iteration 110/1000 | Loss: 0.00001860
Iteration 111/1000 | Loss: 0.00001860
Iteration 112/1000 | Loss: 0.00001860
Iteration 113/1000 | Loss: 0.00001860
Iteration 114/1000 | Loss: 0.00001860
Iteration 115/1000 | Loss: 0.00001860
Iteration 116/1000 | Loss: 0.00001860
Iteration 117/1000 | Loss: 0.00001860
Iteration 118/1000 | Loss: 0.00001860
Iteration 119/1000 | Loss: 0.00001860
Iteration 120/1000 | Loss: 0.00001860
Iteration 121/1000 | Loss: 0.00001860
Iteration 122/1000 | Loss: 0.00001859
Iteration 123/1000 | Loss: 0.00001859
Iteration 124/1000 | Loss: 0.00001859
Iteration 125/1000 | Loss: 0.00001859
Iteration 126/1000 | Loss: 0.00001859
Iteration 127/1000 | Loss: 0.00001859
Iteration 128/1000 | Loss: 0.00001859
Iteration 129/1000 | Loss: 0.00001859
Iteration 130/1000 | Loss: 0.00001859
Iteration 131/1000 | Loss: 0.00001859
Iteration 132/1000 | Loss: 0.00001859
Iteration 133/1000 | Loss: 0.00001859
Iteration 134/1000 | Loss: 0.00001859
Iteration 135/1000 | Loss: 0.00001858
Iteration 136/1000 | Loss: 0.00001858
Iteration 137/1000 | Loss: 0.00001858
Iteration 138/1000 | Loss: 0.00001858
Iteration 139/1000 | Loss: 0.00001858
Iteration 140/1000 | Loss: 0.00001858
Iteration 141/1000 | Loss: 0.00001858
Iteration 142/1000 | Loss: 0.00001858
Iteration 143/1000 | Loss: 0.00001857
Iteration 144/1000 | Loss: 0.00001857
Iteration 145/1000 | Loss: 0.00001857
Iteration 146/1000 | Loss: 0.00001857
Iteration 147/1000 | Loss: 0.00001857
Iteration 148/1000 | Loss: 0.00001857
Iteration 149/1000 | Loss: 0.00001857
Iteration 150/1000 | Loss: 0.00001857
Iteration 151/1000 | Loss: 0.00001857
Iteration 152/1000 | Loss: 0.00001857
Iteration 153/1000 | Loss: 0.00001857
Iteration 154/1000 | Loss: 0.00001856
Iteration 155/1000 | Loss: 0.00001856
Iteration 156/1000 | Loss: 0.00001856
Iteration 157/1000 | Loss: 0.00001856
Iteration 158/1000 | Loss: 0.00001856
Iteration 159/1000 | Loss: 0.00001856
Iteration 160/1000 | Loss: 0.00001856
Iteration 161/1000 | Loss: 0.00001856
Iteration 162/1000 | Loss: 0.00001856
Iteration 163/1000 | Loss: 0.00001856
Iteration 164/1000 | Loss: 0.00001856
Iteration 165/1000 | Loss: 0.00001856
Iteration 166/1000 | Loss: 0.00001856
Iteration 167/1000 | Loss: 0.00001855
Iteration 168/1000 | Loss: 0.00001855
Iteration 169/1000 | Loss: 0.00001855
Iteration 170/1000 | Loss: 0.00001855
Iteration 171/1000 | Loss: 0.00001855
Iteration 172/1000 | Loss: 0.00001855
Iteration 173/1000 | Loss: 0.00001855
Iteration 174/1000 | Loss: 0.00001855
Iteration 175/1000 | Loss: 0.00001855
Iteration 176/1000 | Loss: 0.00001855
Iteration 177/1000 | Loss: 0.00001855
Iteration 178/1000 | Loss: 0.00001855
Iteration 179/1000 | Loss: 0.00001855
Iteration 180/1000 | Loss: 0.00001855
Iteration 181/1000 | Loss: 0.00001855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.854880974860862e-05, 1.854880974860862e-05, 1.854880974860862e-05, 1.854880974860862e-05, 1.854880974860862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.854880974860862e-05

Optimization complete. Final v2v error: 3.611788511276245 mm

Highest mean error: 3.968383550643921 mm for frame 88

Lowest mean error: 3.299293279647827 mm for frame 55

Saving results

Total time: 37.03771734237671
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928395
Iteration 2/25 | Loss: 0.00124504
Iteration 3/25 | Loss: 0.00109525
Iteration 4/25 | Loss: 0.00107133
Iteration 5/25 | Loss: 0.00106649
Iteration 6/25 | Loss: 0.00106571
Iteration 7/25 | Loss: 0.00106571
Iteration 8/25 | Loss: 0.00106571
Iteration 9/25 | Loss: 0.00106571
Iteration 10/25 | Loss: 0.00106571
Iteration 11/25 | Loss: 0.00106571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010657116072252393, 0.0010657116072252393, 0.0010657116072252393, 0.0010657116072252393, 0.0010657116072252393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010657116072252393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61826277
Iteration 2/25 | Loss: 0.00064140
Iteration 3/25 | Loss: 0.00064138
Iteration 4/25 | Loss: 0.00064138
Iteration 5/25 | Loss: 0.00064138
Iteration 6/25 | Loss: 0.00064138
Iteration 7/25 | Loss: 0.00064138
Iteration 8/25 | Loss: 0.00064138
Iteration 9/25 | Loss: 0.00064138
Iteration 10/25 | Loss: 0.00064138
Iteration 11/25 | Loss: 0.00064138
Iteration 12/25 | Loss: 0.00064138
Iteration 13/25 | Loss: 0.00064138
Iteration 14/25 | Loss: 0.00064138
Iteration 15/25 | Loss: 0.00064138
Iteration 16/25 | Loss: 0.00064138
Iteration 17/25 | Loss: 0.00064138
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006413778755813837, 0.0006413778755813837, 0.0006413778755813837, 0.0006413778755813837, 0.0006413778755813837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006413778755813837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064138
Iteration 2/1000 | Loss: 0.00004489
Iteration 3/1000 | Loss: 0.00002435
Iteration 4/1000 | Loss: 0.00002128
Iteration 5/1000 | Loss: 0.00001875
Iteration 6/1000 | Loss: 0.00001805
Iteration 7/1000 | Loss: 0.00001762
Iteration 8/1000 | Loss: 0.00001745
Iteration 9/1000 | Loss: 0.00001728
Iteration 10/1000 | Loss: 0.00001715
Iteration 11/1000 | Loss: 0.00001712
Iteration 12/1000 | Loss: 0.00001709
Iteration 13/1000 | Loss: 0.00001702
Iteration 14/1000 | Loss: 0.00001701
Iteration 15/1000 | Loss: 0.00001700
Iteration 16/1000 | Loss: 0.00001699
Iteration 17/1000 | Loss: 0.00001698
Iteration 18/1000 | Loss: 0.00001697
Iteration 19/1000 | Loss: 0.00001697
Iteration 20/1000 | Loss: 0.00001695
Iteration 21/1000 | Loss: 0.00001694
Iteration 22/1000 | Loss: 0.00001694
Iteration 23/1000 | Loss: 0.00001690
Iteration 24/1000 | Loss: 0.00001690
Iteration 25/1000 | Loss: 0.00001689
Iteration 26/1000 | Loss: 0.00001687
Iteration 27/1000 | Loss: 0.00001686
Iteration 28/1000 | Loss: 0.00001685
Iteration 29/1000 | Loss: 0.00001685
Iteration 30/1000 | Loss: 0.00001685
Iteration 31/1000 | Loss: 0.00001685
Iteration 32/1000 | Loss: 0.00001685
Iteration 33/1000 | Loss: 0.00001684
Iteration 34/1000 | Loss: 0.00001683
Iteration 35/1000 | Loss: 0.00001682
Iteration 36/1000 | Loss: 0.00001682
Iteration 37/1000 | Loss: 0.00001682
Iteration 38/1000 | Loss: 0.00001682
Iteration 39/1000 | Loss: 0.00001681
Iteration 40/1000 | Loss: 0.00001681
Iteration 41/1000 | Loss: 0.00001681
Iteration 42/1000 | Loss: 0.00001681
Iteration 43/1000 | Loss: 0.00001680
Iteration 44/1000 | Loss: 0.00001680
Iteration 45/1000 | Loss: 0.00001680
Iteration 46/1000 | Loss: 0.00001680
Iteration 47/1000 | Loss: 0.00001680
Iteration 48/1000 | Loss: 0.00001680
Iteration 49/1000 | Loss: 0.00001680
Iteration 50/1000 | Loss: 0.00001679
Iteration 51/1000 | Loss: 0.00001679
Iteration 52/1000 | Loss: 0.00001679
Iteration 53/1000 | Loss: 0.00001679
Iteration 54/1000 | Loss: 0.00001679
Iteration 55/1000 | Loss: 0.00001679
Iteration 56/1000 | Loss: 0.00001679
Iteration 57/1000 | Loss: 0.00001679
Iteration 58/1000 | Loss: 0.00001678
Iteration 59/1000 | Loss: 0.00001678
Iteration 60/1000 | Loss: 0.00001678
Iteration 61/1000 | Loss: 0.00001678
Iteration 62/1000 | Loss: 0.00001678
Iteration 63/1000 | Loss: 0.00001678
Iteration 64/1000 | Loss: 0.00001678
Iteration 65/1000 | Loss: 0.00001678
Iteration 66/1000 | Loss: 0.00001678
Iteration 67/1000 | Loss: 0.00001678
Iteration 68/1000 | Loss: 0.00001678
Iteration 69/1000 | Loss: 0.00001678
Iteration 70/1000 | Loss: 0.00001678
Iteration 71/1000 | Loss: 0.00001677
Iteration 72/1000 | Loss: 0.00001677
Iteration 73/1000 | Loss: 0.00001677
Iteration 74/1000 | Loss: 0.00001677
Iteration 75/1000 | Loss: 0.00001677
Iteration 76/1000 | Loss: 0.00001677
Iteration 77/1000 | Loss: 0.00001677
Iteration 78/1000 | Loss: 0.00001677
Iteration 79/1000 | Loss: 0.00001677
Iteration 80/1000 | Loss: 0.00001677
Iteration 81/1000 | Loss: 0.00001677
Iteration 82/1000 | Loss: 0.00001677
Iteration 83/1000 | Loss: 0.00001677
Iteration 84/1000 | Loss: 0.00001677
Iteration 85/1000 | Loss: 0.00001676
Iteration 86/1000 | Loss: 0.00001676
Iteration 87/1000 | Loss: 0.00001676
Iteration 88/1000 | Loss: 0.00001676
Iteration 89/1000 | Loss: 0.00001676
Iteration 90/1000 | Loss: 0.00001676
Iteration 91/1000 | Loss: 0.00001676
Iteration 92/1000 | Loss: 0.00001676
Iteration 93/1000 | Loss: 0.00001676
Iteration 94/1000 | Loss: 0.00001676
Iteration 95/1000 | Loss: 0.00001676
Iteration 96/1000 | Loss: 0.00001676
Iteration 97/1000 | Loss: 0.00001676
Iteration 98/1000 | Loss: 0.00001676
Iteration 99/1000 | Loss: 0.00001676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.6762329323682934e-05, 1.6762329323682934e-05, 1.6762329323682934e-05, 1.6762329323682934e-05, 1.6762329323682934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6762329323682934e-05

Optimization complete. Final v2v error: 3.459918737411499 mm

Highest mean error: 3.736771583557129 mm for frame 31

Lowest mean error: 3.2703943252563477 mm for frame 47

Saving results

Total time: 29.739211559295654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065692
Iteration 2/25 | Loss: 0.00202002
Iteration 3/25 | Loss: 0.00131001
Iteration 4/25 | Loss: 0.00120311
Iteration 5/25 | Loss: 0.00118723
Iteration 6/25 | Loss: 0.00113952
Iteration 7/25 | Loss: 0.00118442
Iteration 8/25 | Loss: 0.00115450
Iteration 9/25 | Loss: 0.00106834
Iteration 10/25 | Loss: 0.00104035
Iteration 11/25 | Loss: 0.00097899
Iteration 12/25 | Loss: 0.00096852
Iteration 13/25 | Loss: 0.00096682
Iteration 14/25 | Loss: 0.00095581
Iteration 15/25 | Loss: 0.00094736
Iteration 16/25 | Loss: 0.00094708
Iteration 17/25 | Loss: 0.00094377
Iteration 18/25 | Loss: 0.00093221
Iteration 19/25 | Loss: 0.00092878
Iteration 20/25 | Loss: 0.00092851
Iteration 21/25 | Loss: 0.00092838
Iteration 22/25 | Loss: 0.00092818
Iteration 23/25 | Loss: 0.00093128
Iteration 24/25 | Loss: 0.00092596
Iteration 25/25 | Loss: 0.00092532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43209350
Iteration 2/25 | Loss: 0.00058212
Iteration 3/25 | Loss: 0.00058212
Iteration 4/25 | Loss: 0.00058212
Iteration 5/25 | Loss: 0.00058212
Iteration 6/25 | Loss: 0.00058212
Iteration 7/25 | Loss: 0.00058212
Iteration 8/25 | Loss: 0.00058212
Iteration 9/25 | Loss: 0.00058212
Iteration 10/25 | Loss: 0.00058212
Iteration 11/25 | Loss: 0.00058212
Iteration 12/25 | Loss: 0.00058212
Iteration 13/25 | Loss: 0.00058212
Iteration 14/25 | Loss: 0.00058212
Iteration 15/25 | Loss: 0.00058212
Iteration 16/25 | Loss: 0.00058212
Iteration 17/25 | Loss: 0.00058212
Iteration 18/25 | Loss: 0.00058212
Iteration 19/25 | Loss: 0.00058212
Iteration 20/25 | Loss: 0.00058212
Iteration 21/25 | Loss: 0.00058212
Iteration 22/25 | Loss: 0.00058212
Iteration 23/25 | Loss: 0.00058212
Iteration 24/25 | Loss: 0.00058212
Iteration 25/25 | Loss: 0.00058212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058212
Iteration 2/1000 | Loss: 0.00004752
Iteration 3/1000 | Loss: 0.00002419
Iteration 4/1000 | Loss: 0.00002084
Iteration 5/1000 | Loss: 0.00001883
Iteration 6/1000 | Loss: 0.00001782
Iteration 7/1000 | Loss: 0.00009246
Iteration 8/1000 | Loss: 0.00009597
Iteration 9/1000 | Loss: 0.00001814
Iteration 10/1000 | Loss: 0.00001687
Iteration 11/1000 | Loss: 0.00001662
Iteration 12/1000 | Loss: 0.00001655
Iteration 13/1000 | Loss: 0.00001652
Iteration 14/1000 | Loss: 0.00001637
Iteration 15/1000 | Loss: 0.00001633
Iteration 16/1000 | Loss: 0.00010408
Iteration 17/1000 | Loss: 0.00001632
Iteration 18/1000 | Loss: 0.00001631
Iteration 19/1000 | Loss: 0.00001622
Iteration 20/1000 | Loss: 0.00001621
Iteration 21/1000 | Loss: 0.00001621
Iteration 22/1000 | Loss: 0.00001620
Iteration 23/1000 | Loss: 0.00001620
Iteration 24/1000 | Loss: 0.00001619
Iteration 25/1000 | Loss: 0.00001618
Iteration 26/1000 | Loss: 0.00001618
Iteration 27/1000 | Loss: 0.00001618
Iteration 28/1000 | Loss: 0.00001618
Iteration 29/1000 | Loss: 0.00001618
Iteration 30/1000 | Loss: 0.00001618
Iteration 31/1000 | Loss: 0.00001618
Iteration 32/1000 | Loss: 0.00001618
Iteration 33/1000 | Loss: 0.00001618
Iteration 34/1000 | Loss: 0.00001618
Iteration 35/1000 | Loss: 0.00001618
Iteration 36/1000 | Loss: 0.00001618
Iteration 37/1000 | Loss: 0.00001618
Iteration 38/1000 | Loss: 0.00001617
Iteration 39/1000 | Loss: 0.00001617
Iteration 40/1000 | Loss: 0.00001617
Iteration 41/1000 | Loss: 0.00001616
Iteration 42/1000 | Loss: 0.00001616
Iteration 43/1000 | Loss: 0.00001615
Iteration 44/1000 | Loss: 0.00001615
Iteration 45/1000 | Loss: 0.00001615
Iteration 46/1000 | Loss: 0.00001614
Iteration 47/1000 | Loss: 0.00001614
Iteration 48/1000 | Loss: 0.00001614
Iteration 49/1000 | Loss: 0.00001614
Iteration 50/1000 | Loss: 0.00001614
Iteration 51/1000 | Loss: 0.00001614
Iteration 52/1000 | Loss: 0.00001613
Iteration 53/1000 | Loss: 0.00001613
Iteration 54/1000 | Loss: 0.00001613
Iteration 55/1000 | Loss: 0.00001613
Iteration 56/1000 | Loss: 0.00001613
Iteration 57/1000 | Loss: 0.00001613
Iteration 58/1000 | Loss: 0.00001613
Iteration 59/1000 | Loss: 0.00001613
Iteration 60/1000 | Loss: 0.00001613
Iteration 61/1000 | Loss: 0.00001613
Iteration 62/1000 | Loss: 0.00001613
Iteration 63/1000 | Loss: 0.00001613
Iteration 64/1000 | Loss: 0.00001613
Iteration 65/1000 | Loss: 0.00001612
Iteration 66/1000 | Loss: 0.00001612
Iteration 67/1000 | Loss: 0.00001612
Iteration 68/1000 | Loss: 0.00001612
Iteration 69/1000 | Loss: 0.00001612
Iteration 70/1000 | Loss: 0.00001612
Iteration 71/1000 | Loss: 0.00001612
Iteration 72/1000 | Loss: 0.00001612
Iteration 73/1000 | Loss: 0.00001612
Iteration 74/1000 | Loss: 0.00001612
Iteration 75/1000 | Loss: 0.00001612
Iteration 76/1000 | Loss: 0.00001612
Iteration 77/1000 | Loss: 0.00001612
Iteration 78/1000 | Loss: 0.00001612
Iteration 79/1000 | Loss: 0.00001612
Iteration 80/1000 | Loss: 0.00001612
Iteration 81/1000 | Loss: 0.00001612
Iteration 82/1000 | Loss: 0.00001612
Iteration 83/1000 | Loss: 0.00001612
Iteration 84/1000 | Loss: 0.00001612
Iteration 85/1000 | Loss: 0.00001612
Iteration 86/1000 | Loss: 0.00001612
Iteration 87/1000 | Loss: 0.00001612
Iteration 88/1000 | Loss: 0.00001612
Iteration 89/1000 | Loss: 0.00001612
Iteration 90/1000 | Loss: 0.00001612
Iteration 91/1000 | Loss: 0.00001612
Iteration 92/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.6119831343530677e-05, 1.6119831343530677e-05, 1.6119831343530677e-05, 1.6119831343530677e-05, 1.6119831343530677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6119831343530677e-05

Optimization complete. Final v2v error: 3.1428616046905518 mm

Highest mean error: 9.05501651763916 mm for frame 66

Lowest mean error: 2.4939353466033936 mm for frame 143

Saving results

Total time: 68.83318042755127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867622
Iteration 2/25 | Loss: 0.00118498
Iteration 3/25 | Loss: 0.00109245
Iteration 4/25 | Loss: 0.00107015
Iteration 5/25 | Loss: 0.00106020
Iteration 6/25 | Loss: 0.00105799
Iteration 7/25 | Loss: 0.00105799
Iteration 8/25 | Loss: 0.00105799
Iteration 9/25 | Loss: 0.00105799
Iteration 10/25 | Loss: 0.00105799
Iteration 11/25 | Loss: 0.00105799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010579890804365277, 0.0010579890804365277, 0.0010579890804365277, 0.0010579890804365277, 0.0010579890804365277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010579890804365277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.44959784
Iteration 2/25 | Loss: 0.00070473
Iteration 3/25 | Loss: 0.00070467
Iteration 4/25 | Loss: 0.00070467
Iteration 5/25 | Loss: 0.00070467
Iteration 6/25 | Loss: 0.00070467
Iteration 7/25 | Loss: 0.00070467
Iteration 8/25 | Loss: 0.00070466
Iteration 9/25 | Loss: 0.00070466
Iteration 10/25 | Loss: 0.00070466
Iteration 11/25 | Loss: 0.00070466
Iteration 12/25 | Loss: 0.00070466
Iteration 13/25 | Loss: 0.00070466
Iteration 14/25 | Loss: 0.00070466
Iteration 15/25 | Loss: 0.00070466
Iteration 16/25 | Loss: 0.00070466
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007046639802865684, 0.0007046639802865684, 0.0007046639802865684, 0.0007046639802865684, 0.0007046639802865684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007046639802865684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070466
Iteration 2/1000 | Loss: 0.00004816
Iteration 3/1000 | Loss: 0.00002705
Iteration 4/1000 | Loss: 0.00002290
Iteration 5/1000 | Loss: 0.00002106
Iteration 6/1000 | Loss: 0.00001997
Iteration 7/1000 | Loss: 0.00001961
Iteration 8/1000 | Loss: 0.00001924
Iteration 9/1000 | Loss: 0.00001897
Iteration 10/1000 | Loss: 0.00001863
Iteration 11/1000 | Loss: 0.00001855
Iteration 12/1000 | Loss: 0.00001841
Iteration 13/1000 | Loss: 0.00001831
Iteration 14/1000 | Loss: 0.00001826
Iteration 15/1000 | Loss: 0.00001822
Iteration 16/1000 | Loss: 0.00001822
Iteration 17/1000 | Loss: 0.00001818
Iteration 18/1000 | Loss: 0.00001814
Iteration 19/1000 | Loss: 0.00001814
Iteration 20/1000 | Loss: 0.00001814
Iteration 21/1000 | Loss: 0.00001814
Iteration 22/1000 | Loss: 0.00001814
Iteration 23/1000 | Loss: 0.00001814
Iteration 24/1000 | Loss: 0.00001813
Iteration 25/1000 | Loss: 0.00001812
Iteration 26/1000 | Loss: 0.00001811
Iteration 27/1000 | Loss: 0.00001810
Iteration 28/1000 | Loss: 0.00001809
Iteration 29/1000 | Loss: 0.00001809
Iteration 30/1000 | Loss: 0.00001809
Iteration 31/1000 | Loss: 0.00001808
Iteration 32/1000 | Loss: 0.00001808
Iteration 33/1000 | Loss: 0.00001808
Iteration 34/1000 | Loss: 0.00001807
Iteration 35/1000 | Loss: 0.00001806
Iteration 36/1000 | Loss: 0.00001805
Iteration 37/1000 | Loss: 0.00001805
Iteration 38/1000 | Loss: 0.00001804
Iteration 39/1000 | Loss: 0.00001804
Iteration 40/1000 | Loss: 0.00001802
Iteration 41/1000 | Loss: 0.00001801
Iteration 42/1000 | Loss: 0.00001801
Iteration 43/1000 | Loss: 0.00001800
Iteration 44/1000 | Loss: 0.00001800
Iteration 45/1000 | Loss: 0.00001800
Iteration 46/1000 | Loss: 0.00001800
Iteration 47/1000 | Loss: 0.00001800
Iteration 48/1000 | Loss: 0.00001800
Iteration 49/1000 | Loss: 0.00001799
Iteration 50/1000 | Loss: 0.00001799
Iteration 51/1000 | Loss: 0.00001799
Iteration 52/1000 | Loss: 0.00001799
Iteration 53/1000 | Loss: 0.00001798
Iteration 54/1000 | Loss: 0.00001798
Iteration 55/1000 | Loss: 0.00001798
Iteration 56/1000 | Loss: 0.00001798
Iteration 57/1000 | Loss: 0.00001798
Iteration 58/1000 | Loss: 0.00001797
Iteration 59/1000 | Loss: 0.00001797
Iteration 60/1000 | Loss: 0.00001797
Iteration 61/1000 | Loss: 0.00001797
Iteration 62/1000 | Loss: 0.00001797
Iteration 63/1000 | Loss: 0.00001797
Iteration 64/1000 | Loss: 0.00001797
Iteration 65/1000 | Loss: 0.00001796
Iteration 66/1000 | Loss: 0.00001796
Iteration 67/1000 | Loss: 0.00001796
Iteration 68/1000 | Loss: 0.00001796
Iteration 69/1000 | Loss: 0.00001796
Iteration 70/1000 | Loss: 0.00001796
Iteration 71/1000 | Loss: 0.00001796
Iteration 72/1000 | Loss: 0.00001796
Iteration 73/1000 | Loss: 0.00001796
Iteration 74/1000 | Loss: 0.00001796
Iteration 75/1000 | Loss: 0.00001796
Iteration 76/1000 | Loss: 0.00001796
Iteration 77/1000 | Loss: 0.00001796
Iteration 78/1000 | Loss: 0.00001796
Iteration 79/1000 | Loss: 0.00001796
Iteration 80/1000 | Loss: 0.00001796
Iteration 81/1000 | Loss: 0.00001796
Iteration 82/1000 | Loss: 0.00001796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.7958340322365984e-05, 1.7958340322365984e-05, 1.7958340322365984e-05, 1.7958340322365984e-05, 1.7958340322365984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7958340322365984e-05

Optimization complete. Final v2v error: 3.697221040725708 mm

Highest mean error: 4.3991780281066895 mm for frame 179

Lowest mean error: 3.217486619949341 mm for frame 206

Saving results

Total time: 38.34533429145813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084711
Iteration 2/25 | Loss: 0.01084711
Iteration 3/25 | Loss: 0.00253302
Iteration 4/25 | Loss: 0.00191295
Iteration 5/25 | Loss: 0.00168673
Iteration 6/25 | Loss: 0.00174587
Iteration 7/25 | Loss: 0.00165209
Iteration 8/25 | Loss: 0.00136237
Iteration 9/25 | Loss: 0.00119321
Iteration 10/25 | Loss: 0.00115124
Iteration 11/25 | Loss: 0.00113572
Iteration 12/25 | Loss: 0.00111525
Iteration 13/25 | Loss: 0.00111295
Iteration 14/25 | Loss: 0.00110836
Iteration 15/25 | Loss: 0.00109677
Iteration 16/25 | Loss: 0.00109281
Iteration 17/25 | Loss: 0.00109170
Iteration 18/25 | Loss: 0.00109133
Iteration 19/25 | Loss: 0.00109123
Iteration 20/25 | Loss: 0.00109120
Iteration 21/25 | Loss: 0.00109120
Iteration 22/25 | Loss: 0.00109120
Iteration 23/25 | Loss: 0.00109120
Iteration 24/25 | Loss: 0.00109120
Iteration 25/25 | Loss: 0.00109120

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37287462
Iteration 2/25 | Loss: 0.00073223
Iteration 3/25 | Loss: 0.00073223
Iteration 4/25 | Loss: 0.00073223
Iteration 5/25 | Loss: 0.00073223
Iteration 6/25 | Loss: 0.00073222
Iteration 7/25 | Loss: 0.00073222
Iteration 8/25 | Loss: 0.00073222
Iteration 9/25 | Loss: 0.00073222
Iteration 10/25 | Loss: 0.00073222
Iteration 11/25 | Loss: 0.00073222
Iteration 12/25 | Loss: 0.00073222
Iteration 13/25 | Loss: 0.00073222
Iteration 14/25 | Loss: 0.00073222
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007322243181988597, 0.0007322243181988597, 0.0007322243181988597, 0.0007322243181988597, 0.0007322243181988597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007322243181988597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073222
Iteration 2/1000 | Loss: 0.00004689
Iteration 3/1000 | Loss: 0.00002917
Iteration 4/1000 | Loss: 0.00002338
Iteration 5/1000 | Loss: 0.00002141
Iteration 6/1000 | Loss: 0.00002064
Iteration 7/1000 | Loss: 0.00001978
Iteration 8/1000 | Loss: 0.00001907
Iteration 9/1000 | Loss: 0.00001864
Iteration 10/1000 | Loss: 0.00001824
Iteration 11/1000 | Loss: 0.00051061
Iteration 12/1000 | Loss: 0.00002006
Iteration 13/1000 | Loss: 0.00001773
Iteration 14/1000 | Loss: 0.00001683
Iteration 15/1000 | Loss: 0.00001637
Iteration 16/1000 | Loss: 0.00001601
Iteration 17/1000 | Loss: 0.00001592
Iteration 18/1000 | Loss: 0.00001584
Iteration 19/1000 | Loss: 0.00001581
Iteration 20/1000 | Loss: 0.00001580
Iteration 21/1000 | Loss: 0.00001579
Iteration 22/1000 | Loss: 0.00001578
Iteration 23/1000 | Loss: 0.00001576
Iteration 24/1000 | Loss: 0.00001575
Iteration 25/1000 | Loss: 0.00001575
Iteration 26/1000 | Loss: 0.00001575
Iteration 27/1000 | Loss: 0.00001575
Iteration 28/1000 | Loss: 0.00001575
Iteration 29/1000 | Loss: 0.00001575
Iteration 30/1000 | Loss: 0.00001575
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00001574
Iteration 33/1000 | Loss: 0.00001574
Iteration 34/1000 | Loss: 0.00001574
Iteration 35/1000 | Loss: 0.00001574
Iteration 36/1000 | Loss: 0.00001574
Iteration 37/1000 | Loss: 0.00001574
Iteration 38/1000 | Loss: 0.00001573
Iteration 39/1000 | Loss: 0.00001573
Iteration 40/1000 | Loss: 0.00001573
Iteration 41/1000 | Loss: 0.00001573
Iteration 42/1000 | Loss: 0.00001573
Iteration 43/1000 | Loss: 0.00001573
Iteration 44/1000 | Loss: 0.00001573
Iteration 45/1000 | Loss: 0.00001573
Iteration 46/1000 | Loss: 0.00001573
Iteration 47/1000 | Loss: 0.00001573
Iteration 48/1000 | Loss: 0.00001573
Iteration 49/1000 | Loss: 0.00001573
Iteration 50/1000 | Loss: 0.00001573
Iteration 51/1000 | Loss: 0.00001573
Iteration 52/1000 | Loss: 0.00001573
Iteration 53/1000 | Loss: 0.00001572
Iteration 54/1000 | Loss: 0.00001572
Iteration 55/1000 | Loss: 0.00001572
Iteration 56/1000 | Loss: 0.00001572
Iteration 57/1000 | Loss: 0.00001572
Iteration 58/1000 | Loss: 0.00001572
Iteration 59/1000 | Loss: 0.00001572
Iteration 60/1000 | Loss: 0.00001572
Iteration 61/1000 | Loss: 0.00001572
Iteration 62/1000 | Loss: 0.00001572
Iteration 63/1000 | Loss: 0.00001572
Iteration 64/1000 | Loss: 0.00001572
Iteration 65/1000 | Loss: 0.00001572
Iteration 66/1000 | Loss: 0.00001572
Iteration 67/1000 | Loss: 0.00001572
Iteration 68/1000 | Loss: 0.00001571
Iteration 69/1000 | Loss: 0.00001571
Iteration 70/1000 | Loss: 0.00001571
Iteration 71/1000 | Loss: 0.00001571
Iteration 72/1000 | Loss: 0.00001571
Iteration 73/1000 | Loss: 0.00001571
Iteration 74/1000 | Loss: 0.00001571
Iteration 75/1000 | Loss: 0.00001571
Iteration 76/1000 | Loss: 0.00001571
Iteration 77/1000 | Loss: 0.00001571
Iteration 78/1000 | Loss: 0.00001571
Iteration 79/1000 | Loss: 0.00001571
Iteration 80/1000 | Loss: 0.00001571
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001571
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.571316533954814e-05, 1.571316533954814e-05, 1.571316533954814e-05, 1.571316533954814e-05, 1.571316533954814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.571316533954814e-05

Optimization complete. Final v2v error: 3.4277586936950684 mm

Highest mean error: 4.31492280960083 mm for frame 238

Lowest mean error: 3.0695676803588867 mm for frame 23

Saving results

Total time: 68.47695779800415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423600
Iteration 2/25 | Loss: 0.00109892
Iteration 3/25 | Loss: 0.00099847
Iteration 4/25 | Loss: 0.00098252
Iteration 5/25 | Loss: 0.00097939
Iteration 6/25 | Loss: 0.00097908
Iteration 7/25 | Loss: 0.00097908
Iteration 8/25 | Loss: 0.00097908
Iteration 9/25 | Loss: 0.00097908
Iteration 10/25 | Loss: 0.00097908
Iteration 11/25 | Loss: 0.00097908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009790841722860932, 0.0009790841722860932, 0.0009790841722860932, 0.0009790841722860932, 0.0009790841722860932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009790841722860932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38789988
Iteration 2/25 | Loss: 0.00049282
Iteration 3/25 | Loss: 0.00049282
Iteration 4/25 | Loss: 0.00049282
Iteration 5/25 | Loss: 0.00049282
Iteration 6/25 | Loss: 0.00049282
Iteration 7/25 | Loss: 0.00049282
Iteration 8/25 | Loss: 0.00049282
Iteration 9/25 | Loss: 0.00049282
Iteration 10/25 | Loss: 0.00049282
Iteration 11/25 | Loss: 0.00049282
Iteration 12/25 | Loss: 0.00049282
Iteration 13/25 | Loss: 0.00049282
Iteration 14/25 | Loss: 0.00049282
Iteration 15/25 | Loss: 0.00049282
Iteration 16/25 | Loss: 0.00049282
Iteration 17/25 | Loss: 0.00049282
Iteration 18/25 | Loss: 0.00049282
Iteration 19/25 | Loss: 0.00049282
Iteration 20/25 | Loss: 0.00049282
Iteration 21/25 | Loss: 0.00049282
Iteration 22/25 | Loss: 0.00049282
Iteration 23/25 | Loss: 0.00049282
Iteration 24/25 | Loss: 0.00049282
Iteration 25/25 | Loss: 0.00049282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049282
Iteration 2/1000 | Loss: 0.00002597
Iteration 3/1000 | Loss: 0.00001645
Iteration 4/1000 | Loss: 0.00001382
Iteration 5/1000 | Loss: 0.00001292
Iteration 6/1000 | Loss: 0.00001265
Iteration 7/1000 | Loss: 0.00001264
Iteration 8/1000 | Loss: 0.00001264
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001251
Iteration 11/1000 | Loss: 0.00001223
Iteration 12/1000 | Loss: 0.00001218
Iteration 13/1000 | Loss: 0.00001208
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001203
Iteration 16/1000 | Loss: 0.00001202
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001201
Iteration 19/1000 | Loss: 0.00001200
Iteration 20/1000 | Loss: 0.00001199
Iteration 21/1000 | Loss: 0.00001199
Iteration 22/1000 | Loss: 0.00001199
Iteration 23/1000 | Loss: 0.00001198
Iteration 24/1000 | Loss: 0.00001198
Iteration 25/1000 | Loss: 0.00001197
Iteration 26/1000 | Loss: 0.00001195
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001195
Iteration 29/1000 | Loss: 0.00001195
Iteration 30/1000 | Loss: 0.00001195
Iteration 31/1000 | Loss: 0.00001195
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001194
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001194
Iteration 37/1000 | Loss: 0.00001194
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001190
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001189
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001188
Iteration 49/1000 | Loss: 0.00001187
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001186
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001186
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001186
Iteration 58/1000 | Loss: 0.00001186
Iteration 59/1000 | Loss: 0.00001186
Iteration 60/1000 | Loss: 0.00001185
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001185
Iteration 64/1000 | Loss: 0.00001185
Iteration 65/1000 | Loss: 0.00001185
Iteration 66/1000 | Loss: 0.00001185
Iteration 67/1000 | Loss: 0.00001185
Iteration 68/1000 | Loss: 0.00001185
Iteration 69/1000 | Loss: 0.00001185
Iteration 70/1000 | Loss: 0.00001185
Iteration 71/1000 | Loss: 0.00001185
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001185
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001185
Iteration 77/1000 | Loss: 0.00001185
Iteration 78/1000 | Loss: 0.00001185
Iteration 79/1000 | Loss: 0.00001185
Iteration 80/1000 | Loss: 0.00001185
Iteration 81/1000 | Loss: 0.00001185
Iteration 82/1000 | Loss: 0.00001185
Iteration 83/1000 | Loss: 0.00001185
Iteration 84/1000 | Loss: 0.00001185
Iteration 85/1000 | Loss: 0.00001185
Iteration 86/1000 | Loss: 0.00001185
Iteration 87/1000 | Loss: 0.00001185
Iteration 88/1000 | Loss: 0.00001185
Iteration 89/1000 | Loss: 0.00001185
Iteration 90/1000 | Loss: 0.00001185
Iteration 91/1000 | Loss: 0.00001185
Iteration 92/1000 | Loss: 0.00001185
Iteration 93/1000 | Loss: 0.00001185
Iteration 94/1000 | Loss: 0.00001185
Iteration 95/1000 | Loss: 0.00001185
Iteration 96/1000 | Loss: 0.00001185
Iteration 97/1000 | Loss: 0.00001185
Iteration 98/1000 | Loss: 0.00001185
Iteration 99/1000 | Loss: 0.00001185
Iteration 100/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.1850052942463662e-05, 1.1850052942463662e-05, 1.1850052942463662e-05, 1.1850052942463662e-05, 1.1850052942463662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1850052942463662e-05

Optimization complete. Final v2v error: 2.889495849609375 mm

Highest mean error: 3.0598156452178955 mm for frame 81

Lowest mean error: 2.7465925216674805 mm for frame 0

Saving results

Total time: 27.790136575698853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431827
Iteration 2/25 | Loss: 0.00150489
Iteration 3/25 | Loss: 0.00115322
Iteration 4/25 | Loss: 0.00109467
Iteration 5/25 | Loss: 0.00108404
Iteration 6/25 | Loss: 0.00108237
Iteration 7/25 | Loss: 0.00108237
Iteration 8/25 | Loss: 0.00108237
Iteration 9/25 | Loss: 0.00108237
Iteration 10/25 | Loss: 0.00108237
Iteration 11/25 | Loss: 0.00108237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010823680786415935, 0.0010823680786415935, 0.0010823680786415935, 0.0010823680786415935, 0.0010823680786415935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010823680786415935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38790715
Iteration 2/25 | Loss: 0.00039929
Iteration 3/25 | Loss: 0.00039929
Iteration 4/25 | Loss: 0.00039929
Iteration 5/25 | Loss: 0.00039929
Iteration 6/25 | Loss: 0.00039929
Iteration 7/25 | Loss: 0.00039929
Iteration 8/25 | Loss: 0.00039929
Iteration 9/25 | Loss: 0.00039929
Iteration 10/25 | Loss: 0.00039929
Iteration 11/25 | Loss: 0.00039929
Iteration 12/25 | Loss: 0.00039929
Iteration 13/25 | Loss: 0.00039929
Iteration 14/25 | Loss: 0.00039929
Iteration 15/25 | Loss: 0.00039929
Iteration 16/25 | Loss: 0.00039929
Iteration 17/25 | Loss: 0.00039929
Iteration 18/25 | Loss: 0.00039929
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003992869460489601, 0.0003992869460489601, 0.0003992869460489601, 0.0003992869460489601, 0.0003992869460489601]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003992869460489601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039929
Iteration 2/1000 | Loss: 0.00004933
Iteration 3/1000 | Loss: 0.00003247
Iteration 4/1000 | Loss: 0.00002803
Iteration 5/1000 | Loss: 0.00002621
Iteration 6/1000 | Loss: 0.00002529
Iteration 7/1000 | Loss: 0.00002478
Iteration 8/1000 | Loss: 0.00002419
Iteration 9/1000 | Loss: 0.00002374
Iteration 10/1000 | Loss: 0.00002346
Iteration 11/1000 | Loss: 0.00002325
Iteration 12/1000 | Loss: 0.00002324
Iteration 13/1000 | Loss: 0.00002324
Iteration 14/1000 | Loss: 0.00002324
Iteration 15/1000 | Loss: 0.00002322
Iteration 16/1000 | Loss: 0.00002319
Iteration 17/1000 | Loss: 0.00002319
Iteration 18/1000 | Loss: 0.00002319
Iteration 19/1000 | Loss: 0.00002318
Iteration 20/1000 | Loss: 0.00002318
Iteration 21/1000 | Loss: 0.00002316
Iteration 22/1000 | Loss: 0.00002315
Iteration 23/1000 | Loss: 0.00002315
Iteration 24/1000 | Loss: 0.00002314
Iteration 25/1000 | Loss: 0.00002314
Iteration 26/1000 | Loss: 0.00002313
Iteration 27/1000 | Loss: 0.00002313
Iteration 28/1000 | Loss: 0.00002312
Iteration 29/1000 | Loss: 0.00002311
Iteration 30/1000 | Loss: 0.00002310
Iteration 31/1000 | Loss: 0.00002310
Iteration 32/1000 | Loss: 0.00002310
Iteration 33/1000 | Loss: 0.00002310
Iteration 34/1000 | Loss: 0.00002310
Iteration 35/1000 | Loss: 0.00002310
Iteration 36/1000 | Loss: 0.00002309
Iteration 37/1000 | Loss: 0.00002309
Iteration 38/1000 | Loss: 0.00002309
Iteration 39/1000 | Loss: 0.00002308
Iteration 40/1000 | Loss: 0.00002308
Iteration 41/1000 | Loss: 0.00002308
Iteration 42/1000 | Loss: 0.00002308
Iteration 43/1000 | Loss: 0.00002308
Iteration 44/1000 | Loss: 0.00002308
Iteration 45/1000 | Loss: 0.00002308
Iteration 46/1000 | Loss: 0.00002307
Iteration 47/1000 | Loss: 0.00002307
Iteration 48/1000 | Loss: 0.00002307
Iteration 49/1000 | Loss: 0.00002307
Iteration 50/1000 | Loss: 0.00002307
Iteration 51/1000 | Loss: 0.00002307
Iteration 52/1000 | Loss: 0.00002307
Iteration 53/1000 | Loss: 0.00002307
Iteration 54/1000 | Loss: 0.00002307
Iteration 55/1000 | Loss: 0.00002306
Iteration 56/1000 | Loss: 0.00002306
Iteration 57/1000 | Loss: 0.00002306
Iteration 58/1000 | Loss: 0.00002306
Iteration 59/1000 | Loss: 0.00002306
Iteration 60/1000 | Loss: 0.00002306
Iteration 61/1000 | Loss: 0.00002306
Iteration 62/1000 | Loss: 0.00002306
Iteration 63/1000 | Loss: 0.00002306
Iteration 64/1000 | Loss: 0.00002306
Iteration 65/1000 | Loss: 0.00002306
Iteration 66/1000 | Loss: 0.00002306
Iteration 67/1000 | Loss: 0.00002305
Iteration 68/1000 | Loss: 0.00002305
Iteration 69/1000 | Loss: 0.00002305
Iteration 70/1000 | Loss: 0.00002305
Iteration 71/1000 | Loss: 0.00002304
Iteration 72/1000 | Loss: 0.00002304
Iteration 73/1000 | Loss: 0.00002303
Iteration 74/1000 | Loss: 0.00002303
Iteration 75/1000 | Loss: 0.00002303
Iteration 76/1000 | Loss: 0.00002303
Iteration 77/1000 | Loss: 0.00002303
Iteration 78/1000 | Loss: 0.00002302
Iteration 79/1000 | Loss: 0.00002302
Iteration 80/1000 | Loss: 0.00002302
Iteration 81/1000 | Loss: 0.00002302
Iteration 82/1000 | Loss: 0.00002302
Iteration 83/1000 | Loss: 0.00002301
Iteration 84/1000 | Loss: 0.00002301
Iteration 85/1000 | Loss: 0.00002301
Iteration 86/1000 | Loss: 0.00002301
Iteration 87/1000 | Loss: 0.00002301
Iteration 88/1000 | Loss: 0.00002301
Iteration 89/1000 | Loss: 0.00002301
Iteration 90/1000 | Loss: 0.00002301
Iteration 91/1000 | Loss: 0.00002301
Iteration 92/1000 | Loss: 0.00002301
Iteration 93/1000 | Loss: 0.00002301
Iteration 94/1000 | Loss: 0.00002301
Iteration 95/1000 | Loss: 0.00002301
Iteration 96/1000 | Loss: 0.00002301
Iteration 97/1000 | Loss: 0.00002301
Iteration 98/1000 | Loss: 0.00002301
Iteration 99/1000 | Loss: 0.00002301
Iteration 100/1000 | Loss: 0.00002301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.301127096870914e-05, 2.301127096870914e-05, 2.301127096870914e-05, 2.301127096870914e-05, 2.301127096870914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.301127096870914e-05

Optimization complete. Final v2v error: 4.025967121124268 mm

Highest mean error: 4.308186054229736 mm for frame 69

Lowest mean error: 3.598337411880493 mm for frame 0

Saving results

Total time: 32.7763729095459
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831044
Iteration 2/25 | Loss: 0.00133553
Iteration 3/25 | Loss: 0.00116742
Iteration 4/25 | Loss: 0.00113563
Iteration 5/25 | Loss: 0.00113575
Iteration 6/25 | Loss: 0.00110185
Iteration 7/25 | Loss: 0.00109661
Iteration 8/25 | Loss: 0.00109535
Iteration 9/25 | Loss: 0.00109509
Iteration 10/25 | Loss: 0.00110081
Iteration 11/25 | Loss: 0.00109135
Iteration 12/25 | Loss: 0.00108954
Iteration 13/25 | Loss: 0.00108910
Iteration 14/25 | Loss: 0.00108901
Iteration 15/25 | Loss: 0.00108900
Iteration 16/25 | Loss: 0.00108900
Iteration 17/25 | Loss: 0.00108900
Iteration 18/25 | Loss: 0.00108900
Iteration 19/25 | Loss: 0.00108900
Iteration 20/25 | Loss: 0.00108900
Iteration 21/25 | Loss: 0.00108900
Iteration 22/25 | Loss: 0.00108900
Iteration 23/25 | Loss: 0.00108900
Iteration 24/25 | Loss: 0.00108900
Iteration 25/25 | Loss: 0.00108900

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.91004848
Iteration 2/25 | Loss: 0.00061546
Iteration 3/25 | Loss: 0.00061546
Iteration 4/25 | Loss: 0.00061546
Iteration 5/25 | Loss: 0.00061546
Iteration 6/25 | Loss: 0.00061546
Iteration 7/25 | Loss: 0.00061545
Iteration 8/25 | Loss: 0.00061546
Iteration 9/25 | Loss: 0.00061545
Iteration 10/25 | Loss: 0.00061545
Iteration 11/25 | Loss: 0.00061545
Iteration 12/25 | Loss: 0.00061545
Iteration 13/25 | Loss: 0.00061545
Iteration 14/25 | Loss: 0.00061545
Iteration 15/25 | Loss: 0.00061545
Iteration 16/25 | Loss: 0.00061545
Iteration 17/25 | Loss: 0.00061545
Iteration 18/25 | Loss: 0.00061545
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006154549191705883, 0.0006154549191705883, 0.0006154549191705883, 0.0006154549191705883, 0.0006154549191705883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006154549191705883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061545
Iteration 2/1000 | Loss: 0.00004785
Iteration 3/1000 | Loss: 0.00002572
Iteration 4/1000 | Loss: 0.00002201
Iteration 5/1000 | Loss: 0.00001894
Iteration 6/1000 | Loss: 0.00001788
Iteration 7/1000 | Loss: 0.00001747
Iteration 8/1000 | Loss: 0.00001718
Iteration 9/1000 | Loss: 0.00001696
Iteration 10/1000 | Loss: 0.00001686
Iteration 11/1000 | Loss: 0.00001667
Iteration 12/1000 | Loss: 0.00001662
Iteration 13/1000 | Loss: 0.00001656
Iteration 14/1000 | Loss: 0.00001645
Iteration 15/1000 | Loss: 0.00001642
Iteration 16/1000 | Loss: 0.00001642
Iteration 17/1000 | Loss: 0.00001642
Iteration 18/1000 | Loss: 0.00001641
Iteration 19/1000 | Loss: 0.00001641
Iteration 20/1000 | Loss: 0.00001638
Iteration 21/1000 | Loss: 0.00001638
Iteration 22/1000 | Loss: 0.00001638
Iteration 23/1000 | Loss: 0.00001637
Iteration 24/1000 | Loss: 0.00001637
Iteration 25/1000 | Loss: 0.00001636
Iteration 26/1000 | Loss: 0.00001635
Iteration 27/1000 | Loss: 0.00001635
Iteration 28/1000 | Loss: 0.00001635
Iteration 29/1000 | Loss: 0.00001635
Iteration 30/1000 | Loss: 0.00001635
Iteration 31/1000 | Loss: 0.00001635
Iteration 32/1000 | Loss: 0.00001635
Iteration 33/1000 | Loss: 0.00001635
Iteration 34/1000 | Loss: 0.00001634
Iteration 35/1000 | Loss: 0.00001634
Iteration 36/1000 | Loss: 0.00001634
Iteration 37/1000 | Loss: 0.00001634
Iteration 38/1000 | Loss: 0.00001634
Iteration 39/1000 | Loss: 0.00001633
Iteration 40/1000 | Loss: 0.00001633
Iteration 41/1000 | Loss: 0.00001633
Iteration 42/1000 | Loss: 0.00001633
Iteration 43/1000 | Loss: 0.00001633
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 45. Stopping optimization.
Last 5 losses: [1.6333227904397063e-05, 1.6333227904397063e-05, 1.6333227904397063e-05, 1.6333227904397063e-05, 1.6333227904397063e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6333227904397063e-05

Optimization complete. Final v2v error: 3.447340726852417 mm

Highest mean error: 4.1191487312316895 mm for frame 192

Lowest mean error: 2.7701542377471924 mm for frame 110

Saving results

Total time: 44.528294801712036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_1389/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_1389/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00624841
Iteration 2/25 | Loss: 0.00116390
Iteration 3/25 | Loss: 0.00107656
Iteration 4/25 | Loss: 0.00106703
Iteration 5/25 | Loss: 0.00106472
Iteration 6/25 | Loss: 0.00106458
Iteration 7/25 | Loss: 0.00106458
Iteration 8/25 | Loss: 0.00106458
Iteration 9/25 | Loss: 0.00106458
Iteration 10/25 | Loss: 0.00106458
Iteration 11/25 | Loss: 0.00106458
Iteration 12/25 | Loss: 0.00106458
Iteration 13/25 | Loss: 0.00106458
Iteration 14/25 | Loss: 0.00106458
Iteration 15/25 | Loss: 0.00106458
Iteration 16/25 | Loss: 0.00106458
Iteration 17/25 | Loss: 0.00106458
Iteration 18/25 | Loss: 0.00106458
Iteration 19/25 | Loss: 0.00106458
Iteration 20/25 | Loss: 0.00106458
Iteration 21/25 | Loss: 0.00106458
Iteration 22/25 | Loss: 0.00106458
Iteration 23/25 | Loss: 0.00106458
Iteration 24/25 | Loss: 0.00106458
Iteration 25/25 | Loss: 0.00106458

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.11166382
Iteration 2/25 | Loss: 0.00061966
Iteration 3/25 | Loss: 0.00061965
Iteration 4/25 | Loss: 0.00061965
Iteration 5/25 | Loss: 0.00061965
Iteration 6/25 | Loss: 0.00061965
Iteration 7/25 | Loss: 0.00061965
Iteration 8/25 | Loss: 0.00061965
Iteration 9/25 | Loss: 0.00061965
Iteration 10/25 | Loss: 0.00061965
Iteration 11/25 | Loss: 0.00061965
Iteration 12/25 | Loss: 0.00061965
Iteration 13/25 | Loss: 0.00061965
Iteration 14/25 | Loss: 0.00061965
Iteration 15/25 | Loss: 0.00061965
Iteration 16/25 | Loss: 0.00061965
Iteration 17/25 | Loss: 0.00061965
Iteration 18/25 | Loss: 0.00061965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006196466856636107, 0.0006196466856636107, 0.0006196466856636107, 0.0006196466856636107, 0.0006196466856636107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006196466856636107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061965
Iteration 2/1000 | Loss: 0.00004880
Iteration 3/1000 | Loss: 0.00002335
Iteration 4/1000 | Loss: 0.00001971
Iteration 5/1000 | Loss: 0.00001713
Iteration 6/1000 | Loss: 0.00001599
Iteration 7/1000 | Loss: 0.00001559
Iteration 8/1000 | Loss: 0.00001537
Iteration 9/1000 | Loss: 0.00001523
Iteration 10/1000 | Loss: 0.00001523
Iteration 11/1000 | Loss: 0.00001517
Iteration 12/1000 | Loss: 0.00001505
Iteration 13/1000 | Loss: 0.00001497
Iteration 14/1000 | Loss: 0.00001495
Iteration 15/1000 | Loss: 0.00001492
Iteration 16/1000 | Loss: 0.00001491
Iteration 17/1000 | Loss: 0.00001490
Iteration 18/1000 | Loss: 0.00001490
Iteration 19/1000 | Loss: 0.00001490
Iteration 20/1000 | Loss: 0.00001487
Iteration 21/1000 | Loss: 0.00001486
Iteration 22/1000 | Loss: 0.00001486
Iteration 23/1000 | Loss: 0.00001486
Iteration 24/1000 | Loss: 0.00001486
Iteration 25/1000 | Loss: 0.00001486
Iteration 26/1000 | Loss: 0.00001486
Iteration 27/1000 | Loss: 0.00001485
Iteration 28/1000 | Loss: 0.00001484
Iteration 29/1000 | Loss: 0.00001484
Iteration 30/1000 | Loss: 0.00001484
Iteration 31/1000 | Loss: 0.00001484
Iteration 32/1000 | Loss: 0.00001483
Iteration 33/1000 | Loss: 0.00001483
Iteration 34/1000 | Loss: 0.00001483
Iteration 35/1000 | Loss: 0.00001483
Iteration 36/1000 | Loss: 0.00001483
Iteration 37/1000 | Loss: 0.00001483
Iteration 38/1000 | Loss: 0.00001483
Iteration 39/1000 | Loss: 0.00001483
Iteration 40/1000 | Loss: 0.00001483
Iteration 41/1000 | Loss: 0.00001483
Iteration 42/1000 | Loss: 0.00001483
Iteration 43/1000 | Loss: 0.00001483
Iteration 44/1000 | Loss: 0.00001483
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001482
Iteration 48/1000 | Loss: 0.00001482
Iteration 49/1000 | Loss: 0.00001482
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001481
Iteration 52/1000 | Loss: 0.00001481
Iteration 53/1000 | Loss: 0.00001481
Iteration 54/1000 | Loss: 0.00001481
Iteration 55/1000 | Loss: 0.00001481
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001481
Iteration 59/1000 | Loss: 0.00001481
Iteration 60/1000 | Loss: 0.00001481
Iteration 61/1000 | Loss: 0.00001481
Iteration 62/1000 | Loss: 0.00001481
Iteration 63/1000 | Loss: 0.00001481
Iteration 64/1000 | Loss: 0.00001481
Iteration 65/1000 | Loss: 0.00001481
Iteration 66/1000 | Loss: 0.00001481
Iteration 67/1000 | Loss: 0.00001481
Iteration 68/1000 | Loss: 0.00001481
Iteration 69/1000 | Loss: 0.00001481
Iteration 70/1000 | Loss: 0.00001481
Iteration 71/1000 | Loss: 0.00001481
Iteration 72/1000 | Loss: 0.00001481
Iteration 73/1000 | Loss: 0.00001481
Iteration 74/1000 | Loss: 0.00001481
Iteration 75/1000 | Loss: 0.00001481
Iteration 76/1000 | Loss: 0.00001481
Iteration 77/1000 | Loss: 0.00001481
Iteration 78/1000 | Loss: 0.00001481
Iteration 79/1000 | Loss: 0.00001481
Iteration 80/1000 | Loss: 0.00001481
Iteration 81/1000 | Loss: 0.00001481
Iteration 82/1000 | Loss: 0.00001481
Iteration 83/1000 | Loss: 0.00001481
Iteration 84/1000 | Loss: 0.00001481
Iteration 85/1000 | Loss: 0.00001481
Iteration 86/1000 | Loss: 0.00001481
Iteration 87/1000 | Loss: 0.00001481
Iteration 88/1000 | Loss: 0.00001481
Iteration 89/1000 | Loss: 0.00001481
Iteration 90/1000 | Loss: 0.00001481
Iteration 91/1000 | Loss: 0.00001481
Iteration 92/1000 | Loss: 0.00001481
Iteration 93/1000 | Loss: 0.00001481
Iteration 94/1000 | Loss: 0.00001481
Iteration 95/1000 | Loss: 0.00001481
Iteration 96/1000 | Loss: 0.00001481
Iteration 97/1000 | Loss: 0.00001481
Iteration 98/1000 | Loss: 0.00001481
Iteration 99/1000 | Loss: 0.00001481
Iteration 100/1000 | Loss: 0.00001481
Iteration 101/1000 | Loss: 0.00001481
Iteration 102/1000 | Loss: 0.00001481
Iteration 103/1000 | Loss: 0.00001481
Iteration 104/1000 | Loss: 0.00001481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.4806169929215685e-05, 1.4806169929215685e-05, 1.4806169929215685e-05, 1.4806169929215685e-05, 1.4806169929215685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4806169929215685e-05

Optimization complete. Final v2v error: 3.3083605766296387 mm

Highest mean error: 3.5818960666656494 mm for frame 184

Lowest mean error: 3.0047643184661865 mm for frame 77

Saving results

Total time: 29.187100172042847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404102
Iteration 2/25 | Loss: 0.00118816
Iteration 3/25 | Loss: 0.00102612
Iteration 4/25 | Loss: 0.00100887
Iteration 5/25 | Loss: 0.00100352
Iteration 6/25 | Loss: 0.00100110
Iteration 7/25 | Loss: 0.00100045
Iteration 8/25 | Loss: 0.00100045
Iteration 9/25 | Loss: 0.00100045
Iteration 10/25 | Loss: 0.00100045
Iteration 11/25 | Loss: 0.00100045
Iteration 12/25 | Loss: 0.00100045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010004490613937378, 0.0010004490613937378, 0.0010004490613937378, 0.0010004490613937378, 0.0010004490613937378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010004490613937378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32452953
Iteration 2/25 | Loss: 0.00086667
Iteration 3/25 | Loss: 0.00086664
Iteration 4/25 | Loss: 0.00086664
Iteration 5/25 | Loss: 0.00086664
Iteration 6/25 | Loss: 0.00086664
Iteration 7/25 | Loss: 0.00086664
Iteration 8/25 | Loss: 0.00086664
Iteration 9/25 | Loss: 0.00086664
Iteration 10/25 | Loss: 0.00086664
Iteration 11/25 | Loss: 0.00086664
Iteration 12/25 | Loss: 0.00086664
Iteration 13/25 | Loss: 0.00086664
Iteration 14/25 | Loss: 0.00086664
Iteration 15/25 | Loss: 0.00086664
Iteration 16/25 | Loss: 0.00086664
Iteration 17/25 | Loss: 0.00086664
Iteration 18/25 | Loss: 0.00086664
Iteration 19/25 | Loss: 0.00086664
Iteration 20/25 | Loss: 0.00086664
Iteration 21/25 | Loss: 0.00086664
Iteration 22/25 | Loss: 0.00086664
Iteration 23/25 | Loss: 0.00086664
Iteration 24/25 | Loss: 0.00086664
Iteration 25/25 | Loss: 0.00086664

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086664
Iteration 2/1000 | Loss: 0.00005953
Iteration 3/1000 | Loss: 0.00002778
Iteration 4/1000 | Loss: 0.00001931
Iteration 5/1000 | Loss: 0.00001746
Iteration 6/1000 | Loss: 0.00001656
Iteration 7/1000 | Loss: 0.00001570
Iteration 8/1000 | Loss: 0.00001523
Iteration 9/1000 | Loss: 0.00001484
Iteration 10/1000 | Loss: 0.00001449
Iteration 11/1000 | Loss: 0.00001443
Iteration 12/1000 | Loss: 0.00001423
Iteration 13/1000 | Loss: 0.00001405
Iteration 14/1000 | Loss: 0.00001404
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001402
Iteration 17/1000 | Loss: 0.00001402
Iteration 18/1000 | Loss: 0.00001400
Iteration 19/1000 | Loss: 0.00001399
Iteration 20/1000 | Loss: 0.00001399
Iteration 21/1000 | Loss: 0.00001397
Iteration 22/1000 | Loss: 0.00001396
Iteration 23/1000 | Loss: 0.00001395
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001393
Iteration 26/1000 | Loss: 0.00001386
Iteration 27/1000 | Loss: 0.00001383
Iteration 28/1000 | Loss: 0.00001383
Iteration 29/1000 | Loss: 0.00001383
Iteration 30/1000 | Loss: 0.00001383
Iteration 31/1000 | Loss: 0.00001383
Iteration 32/1000 | Loss: 0.00001382
Iteration 33/1000 | Loss: 0.00001382
Iteration 34/1000 | Loss: 0.00001382
Iteration 35/1000 | Loss: 0.00001382
Iteration 36/1000 | Loss: 0.00001382
Iteration 37/1000 | Loss: 0.00001382
Iteration 38/1000 | Loss: 0.00001382
Iteration 39/1000 | Loss: 0.00001382
Iteration 40/1000 | Loss: 0.00001382
Iteration 41/1000 | Loss: 0.00001382
Iteration 42/1000 | Loss: 0.00001382
Iteration 43/1000 | Loss: 0.00001382
Iteration 44/1000 | Loss: 0.00001380
Iteration 45/1000 | Loss: 0.00001379
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001377
Iteration 48/1000 | Loss: 0.00001377
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001371
Iteration 51/1000 | Loss: 0.00001371
Iteration 52/1000 | Loss: 0.00001370
Iteration 53/1000 | Loss: 0.00001370
Iteration 54/1000 | Loss: 0.00001370
Iteration 55/1000 | Loss: 0.00001370
Iteration 56/1000 | Loss: 0.00001370
Iteration 57/1000 | Loss: 0.00001370
Iteration 58/1000 | Loss: 0.00001370
Iteration 59/1000 | Loss: 0.00001370
Iteration 60/1000 | Loss: 0.00001370
Iteration 61/1000 | Loss: 0.00001370
Iteration 62/1000 | Loss: 0.00001370
Iteration 63/1000 | Loss: 0.00001369
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001368
Iteration 68/1000 | Loss: 0.00001367
Iteration 69/1000 | Loss: 0.00001367
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001366
Iteration 72/1000 | Loss: 0.00001366
Iteration 73/1000 | Loss: 0.00001366
Iteration 74/1000 | Loss: 0.00001365
Iteration 75/1000 | Loss: 0.00001365
Iteration 76/1000 | Loss: 0.00001364
Iteration 77/1000 | Loss: 0.00001364
Iteration 78/1000 | Loss: 0.00001364
Iteration 79/1000 | Loss: 0.00001361
Iteration 80/1000 | Loss: 0.00001360
Iteration 81/1000 | Loss: 0.00001360
Iteration 82/1000 | Loss: 0.00001359
Iteration 83/1000 | Loss: 0.00001359
Iteration 84/1000 | Loss: 0.00001359
Iteration 85/1000 | Loss: 0.00001359
Iteration 86/1000 | Loss: 0.00001358
Iteration 87/1000 | Loss: 0.00001358
Iteration 88/1000 | Loss: 0.00001358
Iteration 89/1000 | Loss: 0.00001358
Iteration 90/1000 | Loss: 0.00001357
Iteration 91/1000 | Loss: 0.00001357
Iteration 92/1000 | Loss: 0.00001357
Iteration 93/1000 | Loss: 0.00001357
Iteration 94/1000 | Loss: 0.00001357
Iteration 95/1000 | Loss: 0.00001357
Iteration 96/1000 | Loss: 0.00001357
Iteration 97/1000 | Loss: 0.00001357
Iteration 98/1000 | Loss: 0.00001357
Iteration 99/1000 | Loss: 0.00001357
Iteration 100/1000 | Loss: 0.00001357
Iteration 101/1000 | Loss: 0.00001357
Iteration 102/1000 | Loss: 0.00001356
Iteration 103/1000 | Loss: 0.00001356
Iteration 104/1000 | Loss: 0.00001356
Iteration 105/1000 | Loss: 0.00001356
Iteration 106/1000 | Loss: 0.00001356
Iteration 107/1000 | Loss: 0.00001356
Iteration 108/1000 | Loss: 0.00001356
Iteration 109/1000 | Loss: 0.00001356
Iteration 110/1000 | Loss: 0.00001356
Iteration 111/1000 | Loss: 0.00001356
Iteration 112/1000 | Loss: 0.00001356
Iteration 113/1000 | Loss: 0.00001356
Iteration 114/1000 | Loss: 0.00001356
Iteration 115/1000 | Loss: 0.00001355
Iteration 116/1000 | Loss: 0.00001355
Iteration 117/1000 | Loss: 0.00001355
Iteration 118/1000 | Loss: 0.00001355
Iteration 119/1000 | Loss: 0.00001355
Iteration 120/1000 | Loss: 0.00001355
Iteration 121/1000 | Loss: 0.00001355
Iteration 122/1000 | Loss: 0.00001355
Iteration 123/1000 | Loss: 0.00001354
Iteration 124/1000 | Loss: 0.00001354
Iteration 125/1000 | Loss: 0.00001354
Iteration 126/1000 | Loss: 0.00001354
Iteration 127/1000 | Loss: 0.00001354
Iteration 128/1000 | Loss: 0.00001354
Iteration 129/1000 | Loss: 0.00001354
Iteration 130/1000 | Loss: 0.00001354
Iteration 131/1000 | Loss: 0.00001354
Iteration 132/1000 | Loss: 0.00001354
Iteration 133/1000 | Loss: 0.00001354
Iteration 134/1000 | Loss: 0.00001354
Iteration 135/1000 | Loss: 0.00001354
Iteration 136/1000 | Loss: 0.00001354
Iteration 137/1000 | Loss: 0.00001354
Iteration 138/1000 | Loss: 0.00001354
Iteration 139/1000 | Loss: 0.00001354
Iteration 140/1000 | Loss: 0.00001354
Iteration 141/1000 | Loss: 0.00001354
Iteration 142/1000 | Loss: 0.00001354
Iteration 143/1000 | Loss: 0.00001354
Iteration 144/1000 | Loss: 0.00001354
Iteration 145/1000 | Loss: 0.00001354
Iteration 146/1000 | Loss: 0.00001354
Iteration 147/1000 | Loss: 0.00001354
Iteration 148/1000 | Loss: 0.00001354
Iteration 149/1000 | Loss: 0.00001354
Iteration 150/1000 | Loss: 0.00001354
Iteration 151/1000 | Loss: 0.00001354
Iteration 152/1000 | Loss: 0.00001354
Iteration 153/1000 | Loss: 0.00001354
Iteration 154/1000 | Loss: 0.00001354
Iteration 155/1000 | Loss: 0.00001354
Iteration 156/1000 | Loss: 0.00001354
Iteration 157/1000 | Loss: 0.00001354
Iteration 158/1000 | Loss: 0.00001354
Iteration 159/1000 | Loss: 0.00001354
Iteration 160/1000 | Loss: 0.00001354
Iteration 161/1000 | Loss: 0.00001354
Iteration 162/1000 | Loss: 0.00001354
Iteration 163/1000 | Loss: 0.00001354
Iteration 164/1000 | Loss: 0.00001354
Iteration 165/1000 | Loss: 0.00001354
Iteration 166/1000 | Loss: 0.00001354
Iteration 167/1000 | Loss: 0.00001354
Iteration 168/1000 | Loss: 0.00001354
Iteration 169/1000 | Loss: 0.00001354
Iteration 170/1000 | Loss: 0.00001354
Iteration 171/1000 | Loss: 0.00001354
Iteration 172/1000 | Loss: 0.00001354
Iteration 173/1000 | Loss: 0.00001354
Iteration 174/1000 | Loss: 0.00001354
Iteration 175/1000 | Loss: 0.00001354
Iteration 176/1000 | Loss: 0.00001354
Iteration 177/1000 | Loss: 0.00001354
Iteration 178/1000 | Loss: 0.00001354
Iteration 179/1000 | Loss: 0.00001354
Iteration 180/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.3537951417674776e-05, 1.3537951417674776e-05, 1.3537951417674776e-05, 1.3537951417674776e-05, 1.3537951417674776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3537951417674776e-05

Optimization complete. Final v2v error: 2.9518940448760986 mm

Highest mean error: 5.1256608963012695 mm for frame 76

Lowest mean error: 2.367666244506836 mm for frame 119

Saving results

Total time: 40.40606665611267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088259
Iteration 2/25 | Loss: 0.00156157
Iteration 3/25 | Loss: 0.00153237
Iteration 4/25 | Loss: 0.00133145
Iteration 5/25 | Loss: 0.00124300
Iteration 6/25 | Loss: 0.00132085
Iteration 7/25 | Loss: 0.00118123
Iteration 8/25 | Loss: 0.00112717
Iteration 9/25 | Loss: 0.00111486
Iteration 10/25 | Loss: 0.00106244
Iteration 11/25 | Loss: 0.00103452
Iteration 12/25 | Loss: 0.00100633
Iteration 13/25 | Loss: 0.00098469
Iteration 14/25 | Loss: 0.00097978
Iteration 15/25 | Loss: 0.00096311
Iteration 16/25 | Loss: 0.00096623
Iteration 17/25 | Loss: 0.00096966
Iteration 18/25 | Loss: 0.00096567
Iteration 19/25 | Loss: 0.00096252
Iteration 20/25 | Loss: 0.00096634
Iteration 21/25 | Loss: 0.00096680
Iteration 22/25 | Loss: 0.00096574
Iteration 23/25 | Loss: 0.00096503
Iteration 24/25 | Loss: 0.00096202
Iteration 25/25 | Loss: 0.00095939

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36127007
Iteration 2/25 | Loss: 0.00130082
Iteration 3/25 | Loss: 0.00119408
Iteration 4/25 | Loss: 0.00119408
Iteration 5/25 | Loss: 0.00119408
Iteration 6/25 | Loss: 0.00119407
Iteration 7/25 | Loss: 0.00119407
Iteration 8/25 | Loss: 0.00119407
Iteration 9/25 | Loss: 0.00119407
Iteration 10/25 | Loss: 0.00119407
Iteration 11/25 | Loss: 0.00119407
Iteration 12/25 | Loss: 0.00119407
Iteration 13/25 | Loss: 0.00119407
Iteration 14/25 | Loss: 0.00119407
Iteration 15/25 | Loss: 0.00119407
Iteration 16/25 | Loss: 0.00119407
Iteration 17/25 | Loss: 0.00119407
Iteration 18/25 | Loss: 0.00119407
Iteration 19/25 | Loss: 0.00119407
Iteration 20/25 | Loss: 0.00119407
Iteration 21/25 | Loss: 0.00119407
Iteration 22/25 | Loss: 0.00119407
Iteration 23/25 | Loss: 0.00119407
Iteration 24/25 | Loss: 0.00119407
Iteration 25/25 | Loss: 0.00119407
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011940733529627323, 0.0011940733529627323, 0.0011940733529627323, 0.0011940733529627323, 0.0011940733529627323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011940733529627323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119407
Iteration 2/1000 | Loss: 0.00070568
Iteration 3/1000 | Loss: 0.00025827
Iteration 4/1000 | Loss: 0.00014578
Iteration 5/1000 | Loss: 0.00013733
Iteration 6/1000 | Loss: 0.00017756
Iteration 7/1000 | Loss: 0.00012070
Iteration 8/1000 | Loss: 0.00025838
Iteration 9/1000 | Loss: 0.00011995
Iteration 10/1000 | Loss: 0.00007125
Iteration 11/1000 | Loss: 0.00018002
Iteration 12/1000 | Loss: 0.00010261
Iteration 13/1000 | Loss: 0.00001753
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00020707
Iteration 16/1000 | Loss: 0.00011802
Iteration 17/1000 | Loss: 0.00001563
Iteration 18/1000 | Loss: 0.00028731
Iteration 19/1000 | Loss: 0.00026672
Iteration 20/1000 | Loss: 0.00029298
Iteration 21/1000 | Loss: 0.00040472
Iteration 22/1000 | Loss: 0.00013240
Iteration 23/1000 | Loss: 0.00010193
Iteration 24/1000 | Loss: 0.00013584
Iteration 25/1000 | Loss: 0.00011258
Iteration 26/1000 | Loss: 0.00013263
Iteration 27/1000 | Loss: 0.00028505
Iteration 28/1000 | Loss: 0.00026231
Iteration 29/1000 | Loss: 0.00006366
Iteration 30/1000 | Loss: 0.00002812
Iteration 31/1000 | Loss: 0.00009056
Iteration 32/1000 | Loss: 0.00033171
Iteration 33/1000 | Loss: 0.00039676
Iteration 34/1000 | Loss: 0.00011599
Iteration 35/1000 | Loss: 0.00002127
Iteration 36/1000 | Loss: 0.00001945
Iteration 37/1000 | Loss: 0.00001739
Iteration 38/1000 | Loss: 0.00001565
Iteration 39/1000 | Loss: 0.00001457
Iteration 40/1000 | Loss: 0.00035114
Iteration 41/1000 | Loss: 0.00017226
Iteration 42/1000 | Loss: 0.00003815
Iteration 43/1000 | Loss: 0.00003846
Iteration 44/1000 | Loss: 0.00006296
Iteration 45/1000 | Loss: 0.00001220
Iteration 46/1000 | Loss: 0.00001106
Iteration 47/1000 | Loss: 0.00010160
Iteration 48/1000 | Loss: 0.00022555
Iteration 49/1000 | Loss: 0.00005015
Iteration 50/1000 | Loss: 0.00001019
Iteration 51/1000 | Loss: 0.00000988
Iteration 52/1000 | Loss: 0.00000967
Iteration 53/1000 | Loss: 0.00011591
Iteration 54/1000 | Loss: 0.00007280
Iteration 55/1000 | Loss: 0.00018111
Iteration 56/1000 | Loss: 0.00007098
Iteration 57/1000 | Loss: 0.00000994
Iteration 58/1000 | Loss: 0.00000969
Iteration 59/1000 | Loss: 0.00000954
Iteration 60/1000 | Loss: 0.00000949
Iteration 61/1000 | Loss: 0.00026890
Iteration 62/1000 | Loss: 0.00003362
Iteration 63/1000 | Loss: 0.00001784
Iteration 64/1000 | Loss: 0.00015784
Iteration 65/1000 | Loss: 0.00001388
Iteration 66/1000 | Loss: 0.00001262
Iteration 67/1000 | Loss: 0.00001189
Iteration 68/1000 | Loss: 0.00001140
Iteration 69/1000 | Loss: 0.00001070
Iteration 70/1000 | Loss: 0.00000977
Iteration 71/1000 | Loss: 0.00000925
Iteration 72/1000 | Loss: 0.00000893
Iteration 73/1000 | Loss: 0.00009386
Iteration 74/1000 | Loss: 0.00015821
Iteration 75/1000 | Loss: 0.00000868
Iteration 76/1000 | Loss: 0.00000866
Iteration 77/1000 | Loss: 0.00000858
Iteration 78/1000 | Loss: 0.00000858
Iteration 79/1000 | Loss: 0.00000858
Iteration 80/1000 | Loss: 0.00000858
Iteration 81/1000 | Loss: 0.00000858
Iteration 82/1000 | Loss: 0.00000858
Iteration 83/1000 | Loss: 0.00000858
Iteration 84/1000 | Loss: 0.00000857
Iteration 85/1000 | Loss: 0.00000857
Iteration 86/1000 | Loss: 0.00000856
Iteration 87/1000 | Loss: 0.00000855
Iteration 88/1000 | Loss: 0.00000855
Iteration 89/1000 | Loss: 0.00000855
Iteration 90/1000 | Loss: 0.00000855
Iteration 91/1000 | Loss: 0.00000855
Iteration 92/1000 | Loss: 0.00000855
Iteration 93/1000 | Loss: 0.00000855
Iteration 94/1000 | Loss: 0.00000855
Iteration 95/1000 | Loss: 0.00000854
Iteration 96/1000 | Loss: 0.00000854
Iteration 97/1000 | Loss: 0.00000854
Iteration 98/1000 | Loss: 0.00000854
Iteration 99/1000 | Loss: 0.00000854
Iteration 100/1000 | Loss: 0.00000854
Iteration 101/1000 | Loss: 0.00000854
Iteration 102/1000 | Loss: 0.00000854
Iteration 103/1000 | Loss: 0.00000854
Iteration 104/1000 | Loss: 0.00000854
Iteration 105/1000 | Loss: 0.00000854
Iteration 106/1000 | Loss: 0.00000853
Iteration 107/1000 | Loss: 0.00000853
Iteration 108/1000 | Loss: 0.00000853
Iteration 109/1000 | Loss: 0.00000853
Iteration 110/1000 | Loss: 0.00000853
Iteration 111/1000 | Loss: 0.00000853
Iteration 112/1000 | Loss: 0.00000853
Iteration 113/1000 | Loss: 0.00009732
Iteration 114/1000 | Loss: 0.00016675
Iteration 115/1000 | Loss: 0.00000867
Iteration 116/1000 | Loss: 0.00009409
Iteration 117/1000 | Loss: 0.00000878
Iteration 118/1000 | Loss: 0.00000856
Iteration 119/1000 | Loss: 0.00000853
Iteration 120/1000 | Loss: 0.00000853
Iteration 121/1000 | Loss: 0.00000853
Iteration 122/1000 | Loss: 0.00000852
Iteration 123/1000 | Loss: 0.00000852
Iteration 124/1000 | Loss: 0.00000852
Iteration 125/1000 | Loss: 0.00000852
Iteration 126/1000 | Loss: 0.00000852
Iteration 127/1000 | Loss: 0.00000852
Iteration 128/1000 | Loss: 0.00000851
Iteration 129/1000 | Loss: 0.00000851
Iteration 130/1000 | Loss: 0.00000851
Iteration 131/1000 | Loss: 0.00000851
Iteration 132/1000 | Loss: 0.00000850
Iteration 133/1000 | Loss: 0.00000850
Iteration 134/1000 | Loss: 0.00000850
Iteration 135/1000 | Loss: 0.00000850
Iteration 136/1000 | Loss: 0.00000850
Iteration 137/1000 | Loss: 0.00000850
Iteration 138/1000 | Loss: 0.00000850
Iteration 139/1000 | Loss: 0.00000849
Iteration 140/1000 | Loss: 0.00000849
Iteration 141/1000 | Loss: 0.00000849
Iteration 142/1000 | Loss: 0.00000849
Iteration 143/1000 | Loss: 0.00000849
Iteration 144/1000 | Loss: 0.00000849
Iteration 145/1000 | Loss: 0.00000849
Iteration 146/1000 | Loss: 0.00000849
Iteration 147/1000 | Loss: 0.00000849
Iteration 148/1000 | Loss: 0.00000849
Iteration 149/1000 | Loss: 0.00000849
Iteration 150/1000 | Loss: 0.00000849
Iteration 151/1000 | Loss: 0.00000849
Iteration 152/1000 | Loss: 0.00000849
Iteration 153/1000 | Loss: 0.00000849
Iteration 154/1000 | Loss: 0.00000848
Iteration 155/1000 | Loss: 0.00000848
Iteration 156/1000 | Loss: 0.00000848
Iteration 157/1000 | Loss: 0.00000848
Iteration 158/1000 | Loss: 0.00000848
Iteration 159/1000 | Loss: 0.00000848
Iteration 160/1000 | Loss: 0.00000848
Iteration 161/1000 | Loss: 0.00000848
Iteration 162/1000 | Loss: 0.00000848
Iteration 163/1000 | Loss: 0.00000847
Iteration 164/1000 | Loss: 0.00000847
Iteration 165/1000 | Loss: 0.00000847
Iteration 166/1000 | Loss: 0.00000847
Iteration 167/1000 | Loss: 0.00000847
Iteration 168/1000 | Loss: 0.00000847
Iteration 169/1000 | Loss: 0.00000847
Iteration 170/1000 | Loss: 0.00000847
Iteration 171/1000 | Loss: 0.00000847
Iteration 172/1000 | Loss: 0.00000847
Iteration 173/1000 | Loss: 0.00000847
Iteration 174/1000 | Loss: 0.00000847
Iteration 175/1000 | Loss: 0.00000847
Iteration 176/1000 | Loss: 0.00000847
Iteration 177/1000 | Loss: 0.00000847
Iteration 178/1000 | Loss: 0.00000847
Iteration 179/1000 | Loss: 0.00000847
Iteration 180/1000 | Loss: 0.00000847
Iteration 181/1000 | Loss: 0.00000847
Iteration 182/1000 | Loss: 0.00000847
Iteration 183/1000 | Loss: 0.00000847
Iteration 184/1000 | Loss: 0.00000847
Iteration 185/1000 | Loss: 0.00000847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [8.46882630867185e-06, 8.46882630867185e-06, 8.46882630867185e-06, 8.46882630867185e-06, 8.46882630867185e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.46882630867185e-06

Optimization complete. Final v2v error: 2.375840663909912 mm

Highest mean error: 4.610744953155518 mm for frame 89

Lowest mean error: 1.9901978969573975 mm for frame 135

Saving results

Total time: 161.42876887321472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923316
Iteration 2/25 | Loss: 0.00112424
Iteration 3/25 | Loss: 0.00101108
Iteration 4/25 | Loss: 0.00099821
Iteration 5/25 | Loss: 0.00099451
Iteration 6/25 | Loss: 0.00099319
Iteration 7/25 | Loss: 0.00099319
Iteration 8/25 | Loss: 0.00099319
Iteration 9/25 | Loss: 0.00099319
Iteration 10/25 | Loss: 0.00099319
Iteration 11/25 | Loss: 0.00099319
Iteration 12/25 | Loss: 0.00099319
Iteration 13/25 | Loss: 0.00099319
Iteration 14/25 | Loss: 0.00099319
Iteration 15/25 | Loss: 0.00099319
Iteration 16/25 | Loss: 0.00099319
Iteration 17/25 | Loss: 0.00099319
Iteration 18/25 | Loss: 0.00099319
Iteration 19/25 | Loss: 0.00099319
Iteration 20/25 | Loss: 0.00099319
Iteration 21/25 | Loss: 0.00099319
Iteration 22/25 | Loss: 0.00099319
Iteration 23/25 | Loss: 0.00099319
Iteration 24/25 | Loss: 0.00099319
Iteration 25/25 | Loss: 0.00099319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66790187
Iteration 2/25 | Loss: 0.00081436
Iteration 3/25 | Loss: 0.00081436
Iteration 4/25 | Loss: 0.00081436
Iteration 5/25 | Loss: 0.00081436
Iteration 6/25 | Loss: 0.00081436
Iteration 7/25 | Loss: 0.00081436
Iteration 8/25 | Loss: 0.00081436
Iteration 9/25 | Loss: 0.00081436
Iteration 10/25 | Loss: 0.00081436
Iteration 11/25 | Loss: 0.00081436
Iteration 12/25 | Loss: 0.00081436
Iteration 13/25 | Loss: 0.00081436
Iteration 14/25 | Loss: 0.00081436
Iteration 15/25 | Loss: 0.00081436
Iteration 16/25 | Loss: 0.00081436
Iteration 17/25 | Loss: 0.00081436
Iteration 18/25 | Loss: 0.00081436
Iteration 19/25 | Loss: 0.00081436
Iteration 20/25 | Loss: 0.00081436
Iteration 21/25 | Loss: 0.00081436
Iteration 22/25 | Loss: 0.00081436
Iteration 23/25 | Loss: 0.00081436
Iteration 24/25 | Loss: 0.00081436
Iteration 25/25 | Loss: 0.00081436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081436
Iteration 2/1000 | Loss: 0.00002804
Iteration 3/1000 | Loss: 0.00001724
Iteration 4/1000 | Loss: 0.00001598
Iteration 5/1000 | Loss: 0.00001522
Iteration 6/1000 | Loss: 0.00001477
Iteration 7/1000 | Loss: 0.00001431
Iteration 8/1000 | Loss: 0.00001403
Iteration 9/1000 | Loss: 0.00001390
Iteration 10/1000 | Loss: 0.00001382
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001369
Iteration 13/1000 | Loss: 0.00001364
Iteration 14/1000 | Loss: 0.00001364
Iteration 15/1000 | Loss: 0.00001364
Iteration 16/1000 | Loss: 0.00001364
Iteration 17/1000 | Loss: 0.00001363
Iteration 18/1000 | Loss: 0.00001362
Iteration 19/1000 | Loss: 0.00001362
Iteration 20/1000 | Loss: 0.00001362
Iteration 21/1000 | Loss: 0.00001362
Iteration 22/1000 | Loss: 0.00001361
Iteration 23/1000 | Loss: 0.00001360
Iteration 24/1000 | Loss: 0.00001360
Iteration 25/1000 | Loss: 0.00001359
Iteration 26/1000 | Loss: 0.00001359
Iteration 27/1000 | Loss: 0.00001359
Iteration 28/1000 | Loss: 0.00001359
Iteration 29/1000 | Loss: 0.00001359
Iteration 30/1000 | Loss: 0.00001359
Iteration 31/1000 | Loss: 0.00001358
Iteration 32/1000 | Loss: 0.00001358
Iteration 33/1000 | Loss: 0.00001357
Iteration 34/1000 | Loss: 0.00001357
Iteration 35/1000 | Loss: 0.00001356
Iteration 36/1000 | Loss: 0.00001355
Iteration 37/1000 | Loss: 0.00001354
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001350
Iteration 41/1000 | Loss: 0.00001350
Iteration 42/1000 | Loss: 0.00001350
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001346
Iteration 51/1000 | Loss: 0.00001346
Iteration 52/1000 | Loss: 0.00001345
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001345
Iteration 55/1000 | Loss: 0.00001344
Iteration 56/1000 | Loss: 0.00001343
Iteration 57/1000 | Loss: 0.00001343
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001343
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001342
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001340
Iteration 68/1000 | Loss: 0.00001340
Iteration 69/1000 | Loss: 0.00001340
Iteration 70/1000 | Loss: 0.00001340
Iteration 71/1000 | Loss: 0.00001340
Iteration 72/1000 | Loss: 0.00001340
Iteration 73/1000 | Loss: 0.00001340
Iteration 74/1000 | Loss: 0.00001339
Iteration 75/1000 | Loss: 0.00001339
Iteration 76/1000 | Loss: 0.00001339
Iteration 77/1000 | Loss: 0.00001339
Iteration 78/1000 | Loss: 0.00001339
Iteration 79/1000 | Loss: 0.00001339
Iteration 80/1000 | Loss: 0.00001339
Iteration 81/1000 | Loss: 0.00001339
Iteration 82/1000 | Loss: 0.00001339
Iteration 83/1000 | Loss: 0.00001339
Iteration 84/1000 | Loss: 0.00001339
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001338
Iteration 87/1000 | Loss: 0.00001338
Iteration 88/1000 | Loss: 0.00001338
Iteration 89/1000 | Loss: 0.00001338
Iteration 90/1000 | Loss: 0.00001338
Iteration 91/1000 | Loss: 0.00001338
Iteration 92/1000 | Loss: 0.00001338
Iteration 93/1000 | Loss: 0.00001338
Iteration 94/1000 | Loss: 0.00001338
Iteration 95/1000 | Loss: 0.00001338
Iteration 96/1000 | Loss: 0.00001338
Iteration 97/1000 | Loss: 0.00001338
Iteration 98/1000 | Loss: 0.00001338
Iteration 99/1000 | Loss: 0.00001337
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001337
Iteration 102/1000 | Loss: 0.00001337
Iteration 103/1000 | Loss: 0.00001337
Iteration 104/1000 | Loss: 0.00001337
Iteration 105/1000 | Loss: 0.00001337
Iteration 106/1000 | Loss: 0.00001337
Iteration 107/1000 | Loss: 0.00001337
Iteration 108/1000 | Loss: 0.00001337
Iteration 109/1000 | Loss: 0.00001337
Iteration 110/1000 | Loss: 0.00001337
Iteration 111/1000 | Loss: 0.00001337
Iteration 112/1000 | Loss: 0.00001337
Iteration 113/1000 | Loss: 0.00001337
Iteration 114/1000 | Loss: 0.00001337
Iteration 115/1000 | Loss: 0.00001337
Iteration 116/1000 | Loss: 0.00001337
Iteration 117/1000 | Loss: 0.00001337
Iteration 118/1000 | Loss: 0.00001336
Iteration 119/1000 | Loss: 0.00001336
Iteration 120/1000 | Loss: 0.00001336
Iteration 121/1000 | Loss: 0.00001336
Iteration 122/1000 | Loss: 0.00001336
Iteration 123/1000 | Loss: 0.00001336
Iteration 124/1000 | Loss: 0.00001336
Iteration 125/1000 | Loss: 0.00001336
Iteration 126/1000 | Loss: 0.00001336
Iteration 127/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.3364202459342778e-05, 1.3364202459342778e-05, 1.3364202459342778e-05, 1.3364202459342778e-05, 1.3364202459342778e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3364202459342778e-05

Optimization complete. Final v2v error: 3.0827789306640625 mm

Highest mean error: 4.160388946533203 mm for frame 92

Lowest mean error: 2.733778715133667 mm for frame 123

Saving results

Total time: 31.704124927520752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039940
Iteration 2/25 | Loss: 0.00127403
Iteration 3/25 | Loss: 0.00112115
Iteration 4/25 | Loss: 0.00109844
Iteration 5/25 | Loss: 0.00109167
Iteration 6/25 | Loss: 0.00109041
Iteration 7/25 | Loss: 0.00109041
Iteration 8/25 | Loss: 0.00109041
Iteration 9/25 | Loss: 0.00109041
Iteration 10/25 | Loss: 0.00109041
Iteration 11/25 | Loss: 0.00109041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010904133087024093, 0.0010904133087024093, 0.0010904133087024093, 0.0010904133087024093, 0.0010904133087024093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010904133087024093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60358572
Iteration 2/25 | Loss: 0.00080468
Iteration 3/25 | Loss: 0.00080467
Iteration 4/25 | Loss: 0.00080467
Iteration 5/25 | Loss: 0.00080467
Iteration 6/25 | Loss: 0.00080467
Iteration 7/25 | Loss: 0.00080467
Iteration 8/25 | Loss: 0.00080467
Iteration 9/25 | Loss: 0.00080467
Iteration 10/25 | Loss: 0.00080467
Iteration 11/25 | Loss: 0.00080467
Iteration 12/25 | Loss: 0.00080467
Iteration 13/25 | Loss: 0.00080467
Iteration 14/25 | Loss: 0.00080467
Iteration 15/25 | Loss: 0.00080467
Iteration 16/25 | Loss: 0.00080467
Iteration 17/25 | Loss: 0.00080467
Iteration 18/25 | Loss: 0.00080467
Iteration 19/25 | Loss: 0.00080467
Iteration 20/25 | Loss: 0.00080467
Iteration 21/25 | Loss: 0.00080467
Iteration 22/25 | Loss: 0.00080467
Iteration 23/25 | Loss: 0.00080467
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008046705625019968, 0.0008046705625019968, 0.0008046705625019968, 0.0008046705625019968, 0.0008046705625019968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008046705625019968

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080467
Iteration 2/1000 | Loss: 0.00006089
Iteration 3/1000 | Loss: 0.00002926
Iteration 4/1000 | Loss: 0.00002400
Iteration 5/1000 | Loss: 0.00002230
Iteration 6/1000 | Loss: 0.00002131
Iteration 7/1000 | Loss: 0.00002084
Iteration 8/1000 | Loss: 0.00002044
Iteration 9/1000 | Loss: 0.00002016
Iteration 10/1000 | Loss: 0.00001997
Iteration 11/1000 | Loss: 0.00001985
Iteration 12/1000 | Loss: 0.00001980
Iteration 13/1000 | Loss: 0.00001979
Iteration 14/1000 | Loss: 0.00001976
Iteration 15/1000 | Loss: 0.00001973
Iteration 16/1000 | Loss: 0.00001973
Iteration 17/1000 | Loss: 0.00001972
Iteration 18/1000 | Loss: 0.00001972
Iteration 19/1000 | Loss: 0.00001971
Iteration 20/1000 | Loss: 0.00001971
Iteration 21/1000 | Loss: 0.00001971
Iteration 22/1000 | Loss: 0.00001970
Iteration 23/1000 | Loss: 0.00001970
Iteration 24/1000 | Loss: 0.00001969
Iteration 25/1000 | Loss: 0.00001969
Iteration 26/1000 | Loss: 0.00001969
Iteration 27/1000 | Loss: 0.00001967
Iteration 28/1000 | Loss: 0.00001967
Iteration 29/1000 | Loss: 0.00001966
Iteration 30/1000 | Loss: 0.00001966
Iteration 31/1000 | Loss: 0.00001966
Iteration 32/1000 | Loss: 0.00001966
Iteration 33/1000 | Loss: 0.00001966
Iteration 34/1000 | Loss: 0.00001966
Iteration 35/1000 | Loss: 0.00001966
Iteration 36/1000 | Loss: 0.00001966
Iteration 37/1000 | Loss: 0.00001966
Iteration 38/1000 | Loss: 0.00001966
Iteration 39/1000 | Loss: 0.00001965
Iteration 40/1000 | Loss: 0.00001965
Iteration 41/1000 | Loss: 0.00001963
Iteration 42/1000 | Loss: 0.00001963
Iteration 43/1000 | Loss: 0.00001960
Iteration 44/1000 | Loss: 0.00001960
Iteration 45/1000 | Loss: 0.00001959
Iteration 46/1000 | Loss: 0.00001959
Iteration 47/1000 | Loss: 0.00001959
Iteration 48/1000 | Loss: 0.00001959
Iteration 49/1000 | Loss: 0.00001959
Iteration 50/1000 | Loss: 0.00001959
Iteration 51/1000 | Loss: 0.00001958
Iteration 52/1000 | Loss: 0.00001958
Iteration 53/1000 | Loss: 0.00001958
Iteration 54/1000 | Loss: 0.00001958
Iteration 55/1000 | Loss: 0.00001958
Iteration 56/1000 | Loss: 0.00001958
Iteration 57/1000 | Loss: 0.00001958
Iteration 58/1000 | Loss: 0.00001958
Iteration 59/1000 | Loss: 0.00001957
Iteration 60/1000 | Loss: 0.00001957
Iteration 61/1000 | Loss: 0.00001957
Iteration 62/1000 | Loss: 0.00001957
Iteration 63/1000 | Loss: 0.00001956
Iteration 64/1000 | Loss: 0.00001956
Iteration 65/1000 | Loss: 0.00001956
Iteration 66/1000 | Loss: 0.00001955
Iteration 67/1000 | Loss: 0.00001955
Iteration 68/1000 | Loss: 0.00001955
Iteration 69/1000 | Loss: 0.00001955
Iteration 70/1000 | Loss: 0.00001955
Iteration 71/1000 | Loss: 0.00001954
Iteration 72/1000 | Loss: 0.00001954
Iteration 73/1000 | Loss: 0.00001954
Iteration 74/1000 | Loss: 0.00001954
Iteration 75/1000 | Loss: 0.00001954
Iteration 76/1000 | Loss: 0.00001954
Iteration 77/1000 | Loss: 0.00001954
Iteration 78/1000 | Loss: 0.00001954
Iteration 79/1000 | Loss: 0.00001954
Iteration 80/1000 | Loss: 0.00001954
Iteration 81/1000 | Loss: 0.00001953
Iteration 82/1000 | Loss: 0.00001953
Iteration 83/1000 | Loss: 0.00001953
Iteration 84/1000 | Loss: 0.00001953
Iteration 85/1000 | Loss: 0.00001953
Iteration 86/1000 | Loss: 0.00001952
Iteration 87/1000 | Loss: 0.00001952
Iteration 88/1000 | Loss: 0.00001952
Iteration 89/1000 | Loss: 0.00001952
Iteration 90/1000 | Loss: 0.00001952
Iteration 91/1000 | Loss: 0.00001952
Iteration 92/1000 | Loss: 0.00001952
Iteration 93/1000 | Loss: 0.00001952
Iteration 94/1000 | Loss: 0.00001952
Iteration 95/1000 | Loss: 0.00001952
Iteration 96/1000 | Loss: 0.00001952
Iteration 97/1000 | Loss: 0.00001952
Iteration 98/1000 | Loss: 0.00001951
Iteration 99/1000 | Loss: 0.00001951
Iteration 100/1000 | Loss: 0.00001951
Iteration 101/1000 | Loss: 0.00001950
Iteration 102/1000 | Loss: 0.00001950
Iteration 103/1000 | Loss: 0.00001950
Iteration 104/1000 | Loss: 0.00001950
Iteration 105/1000 | Loss: 0.00001950
Iteration 106/1000 | Loss: 0.00001950
Iteration 107/1000 | Loss: 0.00001950
Iteration 108/1000 | Loss: 0.00001950
Iteration 109/1000 | Loss: 0.00001950
Iteration 110/1000 | Loss: 0.00001950
Iteration 111/1000 | Loss: 0.00001950
Iteration 112/1000 | Loss: 0.00001950
Iteration 113/1000 | Loss: 0.00001950
Iteration 114/1000 | Loss: 0.00001950
Iteration 115/1000 | Loss: 0.00001950
Iteration 116/1000 | Loss: 0.00001950
Iteration 117/1000 | Loss: 0.00001949
Iteration 118/1000 | Loss: 0.00001949
Iteration 119/1000 | Loss: 0.00001949
Iteration 120/1000 | Loss: 0.00001949
Iteration 121/1000 | Loss: 0.00001949
Iteration 122/1000 | Loss: 0.00001949
Iteration 123/1000 | Loss: 0.00001949
Iteration 124/1000 | Loss: 0.00001949
Iteration 125/1000 | Loss: 0.00001949
Iteration 126/1000 | Loss: 0.00001949
Iteration 127/1000 | Loss: 0.00001949
Iteration 128/1000 | Loss: 0.00001949
Iteration 129/1000 | Loss: 0.00001949
Iteration 130/1000 | Loss: 0.00001948
Iteration 131/1000 | Loss: 0.00001948
Iteration 132/1000 | Loss: 0.00001948
Iteration 133/1000 | Loss: 0.00001948
Iteration 134/1000 | Loss: 0.00001948
Iteration 135/1000 | Loss: 0.00001948
Iteration 136/1000 | Loss: 0.00001948
Iteration 137/1000 | Loss: 0.00001948
Iteration 138/1000 | Loss: 0.00001948
Iteration 139/1000 | Loss: 0.00001948
Iteration 140/1000 | Loss: 0.00001948
Iteration 141/1000 | Loss: 0.00001948
Iteration 142/1000 | Loss: 0.00001948
Iteration 143/1000 | Loss: 0.00001947
Iteration 144/1000 | Loss: 0.00001947
Iteration 145/1000 | Loss: 0.00001947
Iteration 146/1000 | Loss: 0.00001947
Iteration 147/1000 | Loss: 0.00001947
Iteration 148/1000 | Loss: 0.00001947
Iteration 149/1000 | Loss: 0.00001947
Iteration 150/1000 | Loss: 0.00001947
Iteration 151/1000 | Loss: 0.00001947
Iteration 152/1000 | Loss: 0.00001947
Iteration 153/1000 | Loss: 0.00001947
Iteration 154/1000 | Loss: 0.00001947
Iteration 155/1000 | Loss: 0.00001947
Iteration 156/1000 | Loss: 0.00001947
Iteration 157/1000 | Loss: 0.00001946
Iteration 158/1000 | Loss: 0.00001946
Iteration 159/1000 | Loss: 0.00001946
Iteration 160/1000 | Loss: 0.00001946
Iteration 161/1000 | Loss: 0.00001946
Iteration 162/1000 | Loss: 0.00001946
Iteration 163/1000 | Loss: 0.00001946
Iteration 164/1000 | Loss: 0.00001946
Iteration 165/1000 | Loss: 0.00001946
Iteration 166/1000 | Loss: 0.00001946
Iteration 167/1000 | Loss: 0.00001946
Iteration 168/1000 | Loss: 0.00001946
Iteration 169/1000 | Loss: 0.00001946
Iteration 170/1000 | Loss: 0.00001946
Iteration 171/1000 | Loss: 0.00001946
Iteration 172/1000 | Loss: 0.00001946
Iteration 173/1000 | Loss: 0.00001946
Iteration 174/1000 | Loss: 0.00001946
Iteration 175/1000 | Loss: 0.00001946
Iteration 176/1000 | Loss: 0.00001946
Iteration 177/1000 | Loss: 0.00001946
Iteration 178/1000 | Loss: 0.00001946
Iteration 179/1000 | Loss: 0.00001946
Iteration 180/1000 | Loss: 0.00001946
Iteration 181/1000 | Loss: 0.00001946
Iteration 182/1000 | Loss: 0.00001946
Iteration 183/1000 | Loss: 0.00001946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.9459333998383954e-05, 1.9459333998383954e-05, 1.9459333998383954e-05, 1.9459333998383954e-05, 1.9459333998383954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9459333998383954e-05

Optimization complete. Final v2v error: 3.720148801803589 mm

Highest mean error: 4.06229305267334 mm for frame 22

Lowest mean error: 3.1897358894348145 mm for frame 121

Saving results

Total time: 34.72317624092102
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569721
Iteration 2/25 | Loss: 0.00110736
Iteration 3/25 | Loss: 0.00100003
Iteration 4/25 | Loss: 0.00098795
Iteration 5/25 | Loss: 0.00098476
Iteration 6/25 | Loss: 0.00098414
Iteration 7/25 | Loss: 0.00098414
Iteration 8/25 | Loss: 0.00098414
Iteration 9/25 | Loss: 0.00098414
Iteration 10/25 | Loss: 0.00098414
Iteration 11/25 | Loss: 0.00098414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009841430000960827, 0.0009841430000960827, 0.0009841430000960827, 0.0009841430000960827, 0.0009841430000960827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009841430000960827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29673684
Iteration 2/25 | Loss: 0.00075365
Iteration 3/25 | Loss: 0.00075361
Iteration 4/25 | Loss: 0.00075361
Iteration 5/25 | Loss: 0.00075361
Iteration 6/25 | Loss: 0.00075361
Iteration 7/25 | Loss: 0.00075361
Iteration 8/25 | Loss: 0.00075361
Iteration 9/25 | Loss: 0.00075361
Iteration 10/25 | Loss: 0.00075361
Iteration 11/25 | Loss: 0.00075361
Iteration 12/25 | Loss: 0.00075361
Iteration 13/25 | Loss: 0.00075361
Iteration 14/25 | Loss: 0.00075361
Iteration 15/25 | Loss: 0.00075361
Iteration 16/25 | Loss: 0.00075361
Iteration 17/25 | Loss: 0.00075361
Iteration 18/25 | Loss: 0.00075361
Iteration 19/25 | Loss: 0.00075361
Iteration 20/25 | Loss: 0.00075361
Iteration 21/25 | Loss: 0.00075361
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007536060875281692, 0.0007536060875281692, 0.0007536060875281692, 0.0007536060875281692, 0.0007536060875281692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007536060875281692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075361
Iteration 2/1000 | Loss: 0.00002276
Iteration 3/1000 | Loss: 0.00001313
Iteration 4/1000 | Loss: 0.00001179
Iteration 5/1000 | Loss: 0.00001102
Iteration 6/1000 | Loss: 0.00001062
Iteration 7/1000 | Loss: 0.00001032
Iteration 8/1000 | Loss: 0.00001007
Iteration 9/1000 | Loss: 0.00000983
Iteration 10/1000 | Loss: 0.00000971
Iteration 11/1000 | Loss: 0.00000966
Iteration 12/1000 | Loss: 0.00000965
Iteration 13/1000 | Loss: 0.00000964
Iteration 14/1000 | Loss: 0.00000963
Iteration 15/1000 | Loss: 0.00000962
Iteration 16/1000 | Loss: 0.00000962
Iteration 17/1000 | Loss: 0.00000962
Iteration 18/1000 | Loss: 0.00000962
Iteration 19/1000 | Loss: 0.00000962
Iteration 20/1000 | Loss: 0.00000962
Iteration 21/1000 | Loss: 0.00000962
Iteration 22/1000 | Loss: 0.00000962
Iteration 23/1000 | Loss: 0.00000962
Iteration 24/1000 | Loss: 0.00000962
Iteration 25/1000 | Loss: 0.00000960
Iteration 26/1000 | Loss: 0.00000960
Iteration 27/1000 | Loss: 0.00000959
Iteration 28/1000 | Loss: 0.00000952
Iteration 29/1000 | Loss: 0.00000952
Iteration 30/1000 | Loss: 0.00000952
Iteration 31/1000 | Loss: 0.00000952
Iteration 32/1000 | Loss: 0.00000952
Iteration 33/1000 | Loss: 0.00000952
Iteration 34/1000 | Loss: 0.00000952
Iteration 35/1000 | Loss: 0.00000951
Iteration 36/1000 | Loss: 0.00000951
Iteration 37/1000 | Loss: 0.00000951
Iteration 38/1000 | Loss: 0.00000951
Iteration 39/1000 | Loss: 0.00000951
Iteration 40/1000 | Loss: 0.00000951
Iteration 41/1000 | Loss: 0.00000951
Iteration 42/1000 | Loss: 0.00000951
Iteration 43/1000 | Loss: 0.00000951
Iteration 44/1000 | Loss: 0.00000950
Iteration 45/1000 | Loss: 0.00000950
Iteration 46/1000 | Loss: 0.00000950
Iteration 47/1000 | Loss: 0.00000950
Iteration 48/1000 | Loss: 0.00000950
Iteration 49/1000 | Loss: 0.00000950
Iteration 50/1000 | Loss: 0.00000950
Iteration 51/1000 | Loss: 0.00000949
Iteration 52/1000 | Loss: 0.00000949
Iteration 53/1000 | Loss: 0.00000949
Iteration 54/1000 | Loss: 0.00000949
Iteration 55/1000 | Loss: 0.00000949
Iteration 56/1000 | Loss: 0.00000949
Iteration 57/1000 | Loss: 0.00000949
Iteration 58/1000 | Loss: 0.00000949
Iteration 59/1000 | Loss: 0.00000949
Iteration 60/1000 | Loss: 0.00000949
Iteration 61/1000 | Loss: 0.00000949
Iteration 62/1000 | Loss: 0.00000949
Iteration 63/1000 | Loss: 0.00000949
Iteration 64/1000 | Loss: 0.00000948
Iteration 65/1000 | Loss: 0.00000948
Iteration 66/1000 | Loss: 0.00000947
Iteration 67/1000 | Loss: 0.00000947
Iteration 68/1000 | Loss: 0.00000947
Iteration 69/1000 | Loss: 0.00000946
Iteration 70/1000 | Loss: 0.00000946
Iteration 71/1000 | Loss: 0.00000946
Iteration 72/1000 | Loss: 0.00000946
Iteration 73/1000 | Loss: 0.00000945
Iteration 74/1000 | Loss: 0.00000945
Iteration 75/1000 | Loss: 0.00000945
Iteration 76/1000 | Loss: 0.00000944
Iteration 77/1000 | Loss: 0.00000944
Iteration 78/1000 | Loss: 0.00000944
Iteration 79/1000 | Loss: 0.00000944
Iteration 80/1000 | Loss: 0.00000944
Iteration 81/1000 | Loss: 0.00000944
Iteration 82/1000 | Loss: 0.00000944
Iteration 83/1000 | Loss: 0.00000944
Iteration 84/1000 | Loss: 0.00000944
Iteration 85/1000 | Loss: 0.00000944
Iteration 86/1000 | Loss: 0.00000944
Iteration 87/1000 | Loss: 0.00000943
Iteration 88/1000 | Loss: 0.00000943
Iteration 89/1000 | Loss: 0.00000943
Iteration 90/1000 | Loss: 0.00000943
Iteration 91/1000 | Loss: 0.00000943
Iteration 92/1000 | Loss: 0.00000943
Iteration 93/1000 | Loss: 0.00000943
Iteration 94/1000 | Loss: 0.00000943
Iteration 95/1000 | Loss: 0.00000943
Iteration 96/1000 | Loss: 0.00000943
Iteration 97/1000 | Loss: 0.00000943
Iteration 98/1000 | Loss: 0.00000943
Iteration 99/1000 | Loss: 0.00000943
Iteration 100/1000 | Loss: 0.00000943
Iteration 101/1000 | Loss: 0.00000943
Iteration 102/1000 | Loss: 0.00000943
Iteration 103/1000 | Loss: 0.00000943
Iteration 104/1000 | Loss: 0.00000942
Iteration 105/1000 | Loss: 0.00000942
Iteration 106/1000 | Loss: 0.00000942
Iteration 107/1000 | Loss: 0.00000942
Iteration 108/1000 | Loss: 0.00000942
Iteration 109/1000 | Loss: 0.00000942
Iteration 110/1000 | Loss: 0.00000942
Iteration 111/1000 | Loss: 0.00000942
Iteration 112/1000 | Loss: 0.00000942
Iteration 113/1000 | Loss: 0.00000942
Iteration 114/1000 | Loss: 0.00000942
Iteration 115/1000 | Loss: 0.00000942
Iteration 116/1000 | Loss: 0.00000942
Iteration 117/1000 | Loss: 0.00000942
Iteration 118/1000 | Loss: 0.00000942
Iteration 119/1000 | Loss: 0.00000942
Iteration 120/1000 | Loss: 0.00000942
Iteration 121/1000 | Loss: 0.00000942
Iteration 122/1000 | Loss: 0.00000942
Iteration 123/1000 | Loss: 0.00000942
Iteration 124/1000 | Loss: 0.00000942
Iteration 125/1000 | Loss: 0.00000941
Iteration 126/1000 | Loss: 0.00000941
Iteration 127/1000 | Loss: 0.00000941
Iteration 128/1000 | Loss: 0.00000941
Iteration 129/1000 | Loss: 0.00000941
Iteration 130/1000 | Loss: 0.00000941
Iteration 131/1000 | Loss: 0.00000941
Iteration 132/1000 | Loss: 0.00000941
Iteration 133/1000 | Loss: 0.00000941
Iteration 134/1000 | Loss: 0.00000941
Iteration 135/1000 | Loss: 0.00000941
Iteration 136/1000 | Loss: 0.00000941
Iteration 137/1000 | Loss: 0.00000941
Iteration 138/1000 | Loss: 0.00000941
Iteration 139/1000 | Loss: 0.00000941
Iteration 140/1000 | Loss: 0.00000941
Iteration 141/1000 | Loss: 0.00000941
Iteration 142/1000 | Loss: 0.00000941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [9.409181984665338e-06, 9.409181984665338e-06, 9.409181984665338e-06, 9.409181984665338e-06, 9.409181984665338e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.409181984665338e-06

Optimization complete. Final v2v error: 2.6575162410736084 mm

Highest mean error: 2.7357001304626465 mm for frame 149

Lowest mean error: 2.5967283248901367 mm for frame 177

Saving results

Total time: 32.28288555145264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886734
Iteration 2/25 | Loss: 0.00139563
Iteration 3/25 | Loss: 0.00116116
Iteration 4/25 | Loss: 0.00113611
Iteration 5/25 | Loss: 0.00113357
Iteration 6/25 | Loss: 0.00113356
Iteration 7/25 | Loss: 0.00113356
Iteration 8/25 | Loss: 0.00113356
Iteration 9/25 | Loss: 0.00113356
Iteration 10/25 | Loss: 0.00113356
Iteration 11/25 | Loss: 0.00113356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001133557758294046, 0.001133557758294046, 0.001133557758294046, 0.001133557758294046, 0.001133557758294046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001133557758294046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91466141
Iteration 2/25 | Loss: 0.00045173
Iteration 3/25 | Loss: 0.00045173
Iteration 4/25 | Loss: 0.00045172
Iteration 5/25 | Loss: 0.00045172
Iteration 6/25 | Loss: 0.00045172
Iteration 7/25 | Loss: 0.00045172
Iteration 8/25 | Loss: 0.00045172
Iteration 9/25 | Loss: 0.00045172
Iteration 10/25 | Loss: 0.00045172
Iteration 11/25 | Loss: 0.00045172
Iteration 12/25 | Loss: 0.00045172
Iteration 13/25 | Loss: 0.00045172
Iteration 14/25 | Loss: 0.00045172
Iteration 15/25 | Loss: 0.00045172
Iteration 16/25 | Loss: 0.00045172
Iteration 17/25 | Loss: 0.00045172
Iteration 18/25 | Loss: 0.00045172
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004517228517215699, 0.0004517228517215699, 0.0004517228517215699, 0.0004517228517215699, 0.0004517228517215699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004517228517215699

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045172
Iteration 2/1000 | Loss: 0.00004545
Iteration 3/1000 | Loss: 0.00003127
Iteration 4/1000 | Loss: 0.00002722
Iteration 5/1000 | Loss: 0.00002543
Iteration 6/1000 | Loss: 0.00002488
Iteration 7/1000 | Loss: 0.00002433
Iteration 8/1000 | Loss: 0.00002394
Iteration 9/1000 | Loss: 0.00002365
Iteration 10/1000 | Loss: 0.00002348
Iteration 11/1000 | Loss: 0.00002343
Iteration 12/1000 | Loss: 0.00002332
Iteration 13/1000 | Loss: 0.00002331
Iteration 14/1000 | Loss: 0.00002326
Iteration 15/1000 | Loss: 0.00002324
Iteration 16/1000 | Loss: 0.00002319
Iteration 17/1000 | Loss: 0.00002319
Iteration 18/1000 | Loss: 0.00002317
Iteration 19/1000 | Loss: 0.00002315
Iteration 20/1000 | Loss: 0.00002315
Iteration 21/1000 | Loss: 0.00002315
Iteration 22/1000 | Loss: 0.00002315
Iteration 23/1000 | Loss: 0.00002315
Iteration 24/1000 | Loss: 0.00002315
Iteration 25/1000 | Loss: 0.00002315
Iteration 26/1000 | Loss: 0.00002315
Iteration 27/1000 | Loss: 0.00002315
Iteration 28/1000 | Loss: 0.00002314
Iteration 29/1000 | Loss: 0.00002314
Iteration 30/1000 | Loss: 0.00002314
Iteration 31/1000 | Loss: 0.00002311
Iteration 32/1000 | Loss: 0.00002311
Iteration 33/1000 | Loss: 0.00002311
Iteration 34/1000 | Loss: 0.00002311
Iteration 35/1000 | Loss: 0.00002311
Iteration 36/1000 | Loss: 0.00002311
Iteration 37/1000 | Loss: 0.00002311
Iteration 38/1000 | Loss: 0.00002311
Iteration 39/1000 | Loss: 0.00002311
Iteration 40/1000 | Loss: 0.00002311
Iteration 41/1000 | Loss: 0.00002308
Iteration 42/1000 | Loss: 0.00002308
Iteration 43/1000 | Loss: 0.00002307
Iteration 44/1000 | Loss: 0.00002307
Iteration 45/1000 | Loss: 0.00002307
Iteration 46/1000 | Loss: 0.00002307
Iteration 47/1000 | Loss: 0.00002306
Iteration 48/1000 | Loss: 0.00002306
Iteration 49/1000 | Loss: 0.00002306
Iteration 50/1000 | Loss: 0.00002306
Iteration 51/1000 | Loss: 0.00002306
Iteration 52/1000 | Loss: 0.00002306
Iteration 53/1000 | Loss: 0.00002306
Iteration 54/1000 | Loss: 0.00002306
Iteration 55/1000 | Loss: 0.00002306
Iteration 56/1000 | Loss: 0.00002306
Iteration 57/1000 | Loss: 0.00002306
Iteration 58/1000 | Loss: 0.00002305
Iteration 59/1000 | Loss: 0.00002305
Iteration 60/1000 | Loss: 0.00002305
Iteration 61/1000 | Loss: 0.00002305
Iteration 62/1000 | Loss: 0.00002305
Iteration 63/1000 | Loss: 0.00002305
Iteration 64/1000 | Loss: 0.00002305
Iteration 65/1000 | Loss: 0.00002305
Iteration 66/1000 | Loss: 0.00002305
Iteration 67/1000 | Loss: 0.00002304
Iteration 68/1000 | Loss: 0.00002304
Iteration 69/1000 | Loss: 0.00002304
Iteration 70/1000 | Loss: 0.00002303
Iteration 71/1000 | Loss: 0.00002303
Iteration 72/1000 | Loss: 0.00002303
Iteration 73/1000 | Loss: 0.00002303
Iteration 74/1000 | Loss: 0.00002303
Iteration 75/1000 | Loss: 0.00002303
Iteration 76/1000 | Loss: 0.00002302
Iteration 77/1000 | Loss: 0.00002302
Iteration 78/1000 | Loss: 0.00002302
Iteration 79/1000 | Loss: 0.00002301
Iteration 80/1000 | Loss: 0.00002301
Iteration 81/1000 | Loss: 0.00002301
Iteration 82/1000 | Loss: 0.00002301
Iteration 83/1000 | Loss: 0.00002301
Iteration 84/1000 | Loss: 0.00002301
Iteration 85/1000 | Loss: 0.00002301
Iteration 86/1000 | Loss: 0.00002301
Iteration 87/1000 | Loss: 0.00002301
Iteration 88/1000 | Loss: 0.00002300
Iteration 89/1000 | Loss: 0.00002300
Iteration 90/1000 | Loss: 0.00002300
Iteration 91/1000 | Loss: 0.00002300
Iteration 92/1000 | Loss: 0.00002300
Iteration 93/1000 | Loss: 0.00002300
Iteration 94/1000 | Loss: 0.00002300
Iteration 95/1000 | Loss: 0.00002297
Iteration 96/1000 | Loss: 0.00002297
Iteration 97/1000 | Loss: 0.00002297
Iteration 98/1000 | Loss: 0.00002297
Iteration 99/1000 | Loss: 0.00002297
Iteration 100/1000 | Loss: 0.00002297
Iteration 101/1000 | Loss: 0.00002297
Iteration 102/1000 | Loss: 0.00002297
Iteration 103/1000 | Loss: 0.00002297
Iteration 104/1000 | Loss: 0.00002297
Iteration 105/1000 | Loss: 0.00002296
Iteration 106/1000 | Loss: 0.00002296
Iteration 107/1000 | Loss: 0.00002296
Iteration 108/1000 | Loss: 0.00002296
Iteration 109/1000 | Loss: 0.00002296
Iteration 110/1000 | Loss: 0.00002296
Iteration 111/1000 | Loss: 0.00002296
Iteration 112/1000 | Loss: 0.00002296
Iteration 113/1000 | Loss: 0.00002296
Iteration 114/1000 | Loss: 0.00002296
Iteration 115/1000 | Loss: 0.00002295
Iteration 116/1000 | Loss: 0.00002295
Iteration 117/1000 | Loss: 0.00002295
Iteration 118/1000 | Loss: 0.00002295
Iteration 119/1000 | Loss: 0.00002295
Iteration 120/1000 | Loss: 0.00002295
Iteration 121/1000 | Loss: 0.00002295
Iteration 122/1000 | Loss: 0.00002295
Iteration 123/1000 | Loss: 0.00002295
Iteration 124/1000 | Loss: 0.00002295
Iteration 125/1000 | Loss: 0.00002295
Iteration 126/1000 | Loss: 0.00002295
Iteration 127/1000 | Loss: 0.00002295
Iteration 128/1000 | Loss: 0.00002295
Iteration 129/1000 | Loss: 0.00002294
Iteration 130/1000 | Loss: 0.00002294
Iteration 131/1000 | Loss: 0.00002294
Iteration 132/1000 | Loss: 0.00002294
Iteration 133/1000 | Loss: 0.00002293
Iteration 134/1000 | Loss: 0.00002293
Iteration 135/1000 | Loss: 0.00002292
Iteration 136/1000 | Loss: 0.00002292
Iteration 137/1000 | Loss: 0.00002291
Iteration 138/1000 | Loss: 0.00002291
Iteration 139/1000 | Loss: 0.00002291
Iteration 140/1000 | Loss: 0.00002290
Iteration 141/1000 | Loss: 0.00002290
Iteration 142/1000 | Loss: 0.00002290
Iteration 143/1000 | Loss: 0.00002290
Iteration 144/1000 | Loss: 0.00002289
Iteration 145/1000 | Loss: 0.00002289
Iteration 146/1000 | Loss: 0.00002289
Iteration 147/1000 | Loss: 0.00002289
Iteration 148/1000 | Loss: 0.00002289
Iteration 149/1000 | Loss: 0.00002289
Iteration 150/1000 | Loss: 0.00002289
Iteration 151/1000 | Loss: 0.00002289
Iteration 152/1000 | Loss: 0.00002288
Iteration 153/1000 | Loss: 0.00002288
Iteration 154/1000 | Loss: 0.00002288
Iteration 155/1000 | Loss: 0.00002288
Iteration 156/1000 | Loss: 0.00002288
Iteration 157/1000 | Loss: 0.00002288
Iteration 158/1000 | Loss: 0.00002288
Iteration 159/1000 | Loss: 0.00002288
Iteration 160/1000 | Loss: 0.00002288
Iteration 161/1000 | Loss: 0.00002288
Iteration 162/1000 | Loss: 0.00002287
Iteration 163/1000 | Loss: 0.00002287
Iteration 164/1000 | Loss: 0.00002286
Iteration 165/1000 | Loss: 0.00002286
Iteration 166/1000 | Loss: 0.00002286
Iteration 167/1000 | Loss: 0.00002286
Iteration 168/1000 | Loss: 0.00002286
Iteration 169/1000 | Loss: 0.00002286
Iteration 170/1000 | Loss: 0.00002285
Iteration 171/1000 | Loss: 0.00002285
Iteration 172/1000 | Loss: 0.00002285
Iteration 173/1000 | Loss: 0.00002284
Iteration 174/1000 | Loss: 0.00002283
Iteration 175/1000 | Loss: 0.00002283
Iteration 176/1000 | Loss: 0.00002283
Iteration 177/1000 | Loss: 0.00002283
Iteration 178/1000 | Loss: 0.00002283
Iteration 179/1000 | Loss: 0.00002283
Iteration 180/1000 | Loss: 0.00002282
Iteration 181/1000 | Loss: 0.00002282
Iteration 182/1000 | Loss: 0.00002282
Iteration 183/1000 | Loss: 0.00002282
Iteration 184/1000 | Loss: 0.00002282
Iteration 185/1000 | Loss: 0.00002282
Iteration 186/1000 | Loss: 0.00002282
Iteration 187/1000 | Loss: 0.00002282
Iteration 188/1000 | Loss: 0.00002281
Iteration 189/1000 | Loss: 0.00002281
Iteration 190/1000 | Loss: 0.00002280
Iteration 191/1000 | Loss: 0.00002280
Iteration 192/1000 | Loss: 0.00002280
Iteration 193/1000 | Loss: 0.00002280
Iteration 194/1000 | Loss: 0.00002280
Iteration 195/1000 | Loss: 0.00002280
Iteration 196/1000 | Loss: 0.00002280
Iteration 197/1000 | Loss: 0.00002280
Iteration 198/1000 | Loss: 0.00002280
Iteration 199/1000 | Loss: 0.00002280
Iteration 200/1000 | Loss: 0.00002280
Iteration 201/1000 | Loss: 0.00002280
Iteration 202/1000 | Loss: 0.00002280
Iteration 203/1000 | Loss: 0.00002280
Iteration 204/1000 | Loss: 0.00002280
Iteration 205/1000 | Loss: 0.00002280
Iteration 206/1000 | Loss: 0.00002280
Iteration 207/1000 | Loss: 0.00002280
Iteration 208/1000 | Loss: 0.00002280
Iteration 209/1000 | Loss: 0.00002280
Iteration 210/1000 | Loss: 0.00002280
Iteration 211/1000 | Loss: 0.00002280
Iteration 212/1000 | Loss: 0.00002280
Iteration 213/1000 | Loss: 0.00002280
Iteration 214/1000 | Loss: 0.00002280
Iteration 215/1000 | Loss: 0.00002280
Iteration 216/1000 | Loss: 0.00002280
Iteration 217/1000 | Loss: 0.00002280
Iteration 218/1000 | Loss: 0.00002280
Iteration 219/1000 | Loss: 0.00002280
Iteration 220/1000 | Loss: 0.00002280
Iteration 221/1000 | Loss: 0.00002280
Iteration 222/1000 | Loss: 0.00002280
Iteration 223/1000 | Loss: 0.00002280
Iteration 224/1000 | Loss: 0.00002280
Iteration 225/1000 | Loss: 0.00002280
Iteration 226/1000 | Loss: 0.00002280
Iteration 227/1000 | Loss: 0.00002280
Iteration 228/1000 | Loss: 0.00002280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [2.2795120457885787e-05, 2.2795120457885787e-05, 2.2795120457885787e-05, 2.2795120457885787e-05, 2.2795120457885787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2795120457885787e-05

Optimization complete. Final v2v error: 4.0269575119018555 mm

Highest mean error: 4.107367515563965 mm for frame 126

Lowest mean error: 3.964141845703125 mm for frame 142

Saving results

Total time: 37.74188280105591
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379143
Iteration 2/25 | Loss: 0.00115764
Iteration 3/25 | Loss: 0.00104401
Iteration 4/25 | Loss: 0.00102989
Iteration 5/25 | Loss: 0.00102804
Iteration 6/25 | Loss: 0.00102804
Iteration 7/25 | Loss: 0.00102804
Iteration 8/25 | Loss: 0.00102804
Iteration 9/25 | Loss: 0.00102804
Iteration 10/25 | Loss: 0.00102804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010280442656949162, 0.0010280442656949162, 0.0010280442656949162, 0.0010280442656949162, 0.0010280442656949162]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010280442656949162

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81116986
Iteration 2/25 | Loss: 0.00086568
Iteration 3/25 | Loss: 0.00086567
Iteration 4/25 | Loss: 0.00086567
Iteration 5/25 | Loss: 0.00086567
Iteration 6/25 | Loss: 0.00086567
Iteration 7/25 | Loss: 0.00086567
Iteration 8/25 | Loss: 0.00086567
Iteration 9/25 | Loss: 0.00086567
Iteration 10/25 | Loss: 0.00086567
Iteration 11/25 | Loss: 0.00086567
Iteration 12/25 | Loss: 0.00086567
Iteration 13/25 | Loss: 0.00086567
Iteration 14/25 | Loss: 0.00086567
Iteration 15/25 | Loss: 0.00086567
Iteration 16/25 | Loss: 0.00086567
Iteration 17/25 | Loss: 0.00086567
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008656696882098913, 0.0008656696882098913, 0.0008656696882098913, 0.0008656696882098913, 0.0008656696882098913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008656696882098913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086567
Iteration 2/1000 | Loss: 0.00004729
Iteration 3/1000 | Loss: 0.00002782
Iteration 4/1000 | Loss: 0.00002221
Iteration 5/1000 | Loss: 0.00002090
Iteration 6/1000 | Loss: 0.00002010
Iteration 7/1000 | Loss: 0.00001980
Iteration 8/1000 | Loss: 0.00001949
Iteration 9/1000 | Loss: 0.00001926
Iteration 10/1000 | Loss: 0.00001906
Iteration 11/1000 | Loss: 0.00001905
Iteration 12/1000 | Loss: 0.00001900
Iteration 13/1000 | Loss: 0.00001896
Iteration 14/1000 | Loss: 0.00001885
Iteration 15/1000 | Loss: 0.00001869
Iteration 16/1000 | Loss: 0.00001853
Iteration 17/1000 | Loss: 0.00001841
Iteration 18/1000 | Loss: 0.00001822
Iteration 19/1000 | Loss: 0.00001803
Iteration 20/1000 | Loss: 0.00001788
Iteration 21/1000 | Loss: 0.00001787
Iteration 22/1000 | Loss: 0.00001778
Iteration 23/1000 | Loss: 0.00001772
Iteration 24/1000 | Loss: 0.00001768
Iteration 25/1000 | Loss: 0.00001767
Iteration 26/1000 | Loss: 0.00001767
Iteration 27/1000 | Loss: 0.00001764
Iteration 28/1000 | Loss: 0.00001764
Iteration 29/1000 | Loss: 0.00001761
Iteration 30/1000 | Loss: 0.00001760
Iteration 31/1000 | Loss: 0.00001759
Iteration 32/1000 | Loss: 0.00001746
Iteration 33/1000 | Loss: 0.00001745
Iteration 34/1000 | Loss: 0.00001743
Iteration 35/1000 | Loss: 0.00001739
Iteration 36/1000 | Loss: 0.00001739
Iteration 37/1000 | Loss: 0.00001739
Iteration 38/1000 | Loss: 0.00001738
Iteration 39/1000 | Loss: 0.00001738
Iteration 40/1000 | Loss: 0.00001738
Iteration 41/1000 | Loss: 0.00001737
Iteration 42/1000 | Loss: 0.00001737
Iteration 43/1000 | Loss: 0.00001737
Iteration 44/1000 | Loss: 0.00001737
Iteration 45/1000 | Loss: 0.00001736
Iteration 46/1000 | Loss: 0.00001736
Iteration 47/1000 | Loss: 0.00001736
Iteration 48/1000 | Loss: 0.00001736
Iteration 49/1000 | Loss: 0.00001735
Iteration 50/1000 | Loss: 0.00001735
Iteration 51/1000 | Loss: 0.00001735
Iteration 52/1000 | Loss: 0.00001735
Iteration 53/1000 | Loss: 0.00001735
Iteration 54/1000 | Loss: 0.00001735
Iteration 55/1000 | Loss: 0.00001735
Iteration 56/1000 | Loss: 0.00001735
Iteration 57/1000 | Loss: 0.00001735
Iteration 58/1000 | Loss: 0.00001735
Iteration 59/1000 | Loss: 0.00001735
Iteration 60/1000 | Loss: 0.00001735
Iteration 61/1000 | Loss: 0.00001734
Iteration 62/1000 | Loss: 0.00001734
Iteration 63/1000 | Loss: 0.00001734
Iteration 64/1000 | Loss: 0.00001734
Iteration 65/1000 | Loss: 0.00001734
Iteration 66/1000 | Loss: 0.00001734
Iteration 67/1000 | Loss: 0.00001734
Iteration 68/1000 | Loss: 0.00001734
Iteration 69/1000 | Loss: 0.00001733
Iteration 70/1000 | Loss: 0.00001733
Iteration 71/1000 | Loss: 0.00001733
Iteration 72/1000 | Loss: 0.00001733
Iteration 73/1000 | Loss: 0.00001733
Iteration 74/1000 | Loss: 0.00001733
Iteration 75/1000 | Loss: 0.00001733
Iteration 76/1000 | Loss: 0.00001732
Iteration 77/1000 | Loss: 0.00001732
Iteration 78/1000 | Loss: 0.00001732
Iteration 79/1000 | Loss: 0.00001732
Iteration 80/1000 | Loss: 0.00001732
Iteration 81/1000 | Loss: 0.00001732
Iteration 82/1000 | Loss: 0.00001732
Iteration 83/1000 | Loss: 0.00001731
Iteration 84/1000 | Loss: 0.00001731
Iteration 85/1000 | Loss: 0.00001731
Iteration 86/1000 | Loss: 0.00001731
Iteration 87/1000 | Loss: 0.00001731
Iteration 88/1000 | Loss: 0.00001731
Iteration 89/1000 | Loss: 0.00001731
Iteration 90/1000 | Loss: 0.00001730
Iteration 91/1000 | Loss: 0.00001730
Iteration 92/1000 | Loss: 0.00001730
Iteration 93/1000 | Loss: 0.00001730
Iteration 94/1000 | Loss: 0.00001729
Iteration 95/1000 | Loss: 0.00001729
Iteration 96/1000 | Loss: 0.00001729
Iteration 97/1000 | Loss: 0.00001729
Iteration 98/1000 | Loss: 0.00001729
Iteration 99/1000 | Loss: 0.00001729
Iteration 100/1000 | Loss: 0.00001729
Iteration 101/1000 | Loss: 0.00001728
Iteration 102/1000 | Loss: 0.00001728
Iteration 103/1000 | Loss: 0.00001728
Iteration 104/1000 | Loss: 0.00001728
Iteration 105/1000 | Loss: 0.00001728
Iteration 106/1000 | Loss: 0.00001728
Iteration 107/1000 | Loss: 0.00001728
Iteration 108/1000 | Loss: 0.00001728
Iteration 109/1000 | Loss: 0.00001728
Iteration 110/1000 | Loss: 0.00001728
Iteration 111/1000 | Loss: 0.00001728
Iteration 112/1000 | Loss: 0.00001728
Iteration 113/1000 | Loss: 0.00001728
Iteration 114/1000 | Loss: 0.00001728
Iteration 115/1000 | Loss: 0.00001728
Iteration 116/1000 | Loss: 0.00001728
Iteration 117/1000 | Loss: 0.00001728
Iteration 118/1000 | Loss: 0.00001728
Iteration 119/1000 | Loss: 0.00001728
Iteration 120/1000 | Loss: 0.00001728
Iteration 121/1000 | Loss: 0.00001728
Iteration 122/1000 | Loss: 0.00001728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.7281578038819134e-05, 1.7281578038819134e-05, 1.7281578038819134e-05, 1.7281578038819134e-05, 1.7281578038819134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7281578038819134e-05

Optimization complete. Final v2v error: 3.5387728214263916 mm

Highest mean error: 3.989985704421997 mm for frame 5

Lowest mean error: 3.3184974193573 mm for frame 32

Saving results

Total time: 45.1680052280426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831525
Iteration 2/25 | Loss: 0.00190554
Iteration 3/25 | Loss: 0.00119867
Iteration 4/25 | Loss: 0.00112977
Iteration 5/25 | Loss: 0.00108281
Iteration 6/25 | Loss: 0.00107478
Iteration 7/25 | Loss: 0.00106312
Iteration 8/25 | Loss: 0.00105822
Iteration 9/25 | Loss: 0.00105743
Iteration 10/25 | Loss: 0.00105417
Iteration 11/25 | Loss: 0.00105180
Iteration 12/25 | Loss: 0.00105129
Iteration 13/25 | Loss: 0.00105117
Iteration 14/25 | Loss: 0.00105117
Iteration 15/25 | Loss: 0.00105117
Iteration 16/25 | Loss: 0.00105117
Iteration 17/25 | Loss: 0.00105117
Iteration 18/25 | Loss: 0.00105116
Iteration 19/25 | Loss: 0.00105116
Iteration 20/25 | Loss: 0.00105116
Iteration 21/25 | Loss: 0.00105116
Iteration 22/25 | Loss: 0.00105116
Iteration 23/25 | Loss: 0.00105116
Iteration 24/25 | Loss: 0.00105116
Iteration 25/25 | Loss: 0.00105116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.66122079
Iteration 2/25 | Loss: 0.00046286
Iteration 3/25 | Loss: 0.00046283
Iteration 4/25 | Loss: 0.00046283
Iteration 5/25 | Loss: 0.00046283
Iteration 6/25 | Loss: 0.00046283
Iteration 7/25 | Loss: 0.00046283
Iteration 8/25 | Loss: 0.00046283
Iteration 9/25 | Loss: 0.00046283
Iteration 10/25 | Loss: 0.00046283
Iteration 11/25 | Loss: 0.00046283
Iteration 12/25 | Loss: 0.00046283
Iteration 13/25 | Loss: 0.00046283
Iteration 14/25 | Loss: 0.00046283
Iteration 15/25 | Loss: 0.00046283
Iteration 16/25 | Loss: 0.00046283
Iteration 17/25 | Loss: 0.00046283
Iteration 18/25 | Loss: 0.00046283
Iteration 19/25 | Loss: 0.00046283
Iteration 20/25 | Loss: 0.00046283
Iteration 21/25 | Loss: 0.00046283
Iteration 22/25 | Loss: 0.00046283
Iteration 23/25 | Loss: 0.00046283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004628317547030747, 0.0004628317547030747, 0.0004628317547030747, 0.0004628317547030747, 0.0004628317547030747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004628317547030747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046283
Iteration 2/1000 | Loss: 0.00004069
Iteration 3/1000 | Loss: 0.00002436
Iteration 4/1000 | Loss: 0.00002081
Iteration 5/1000 | Loss: 0.00012343
Iteration 6/1000 | Loss: 0.00022733
Iteration 7/1000 | Loss: 0.00001890
Iteration 8/1000 | Loss: 0.00010654
Iteration 9/1000 | Loss: 0.00001851
Iteration 10/1000 | Loss: 0.00001793
Iteration 11/1000 | Loss: 0.00001755
Iteration 12/1000 | Loss: 0.00001737
Iteration 13/1000 | Loss: 0.00001734
Iteration 14/1000 | Loss: 0.00001726
Iteration 15/1000 | Loss: 0.00001720
Iteration 16/1000 | Loss: 0.00001719
Iteration 17/1000 | Loss: 0.00001710
Iteration 18/1000 | Loss: 0.00001709
Iteration 19/1000 | Loss: 0.00001709
Iteration 20/1000 | Loss: 0.00001709
Iteration 21/1000 | Loss: 0.00001706
Iteration 22/1000 | Loss: 0.00001706
Iteration 23/1000 | Loss: 0.00001706
Iteration 24/1000 | Loss: 0.00001706
Iteration 25/1000 | Loss: 0.00001706
Iteration 26/1000 | Loss: 0.00001706
Iteration 27/1000 | Loss: 0.00001706
Iteration 28/1000 | Loss: 0.00001706
Iteration 29/1000 | Loss: 0.00001706
Iteration 30/1000 | Loss: 0.00001705
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001702
Iteration 33/1000 | Loss: 0.00001699
Iteration 34/1000 | Loss: 0.00001699
Iteration 35/1000 | Loss: 0.00001699
Iteration 36/1000 | Loss: 0.00001698
Iteration 37/1000 | Loss: 0.00001698
Iteration 38/1000 | Loss: 0.00001697
Iteration 39/1000 | Loss: 0.00001697
Iteration 40/1000 | Loss: 0.00001697
Iteration 41/1000 | Loss: 0.00001697
Iteration 42/1000 | Loss: 0.00001697
Iteration 43/1000 | Loss: 0.00001697
Iteration 44/1000 | Loss: 0.00001697
Iteration 45/1000 | Loss: 0.00001697
Iteration 46/1000 | Loss: 0.00001697
Iteration 47/1000 | Loss: 0.00001697
Iteration 48/1000 | Loss: 0.00001697
Iteration 49/1000 | Loss: 0.00001697
Iteration 50/1000 | Loss: 0.00001697
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001697
Iteration 53/1000 | Loss: 0.00001697
Iteration 54/1000 | Loss: 0.00001697
Iteration 55/1000 | Loss: 0.00001697
Iteration 56/1000 | Loss: 0.00001697
Iteration 57/1000 | Loss: 0.00001697
Iteration 58/1000 | Loss: 0.00001697
Iteration 59/1000 | Loss: 0.00001697
Iteration 60/1000 | Loss: 0.00001697
Iteration 61/1000 | Loss: 0.00001697
Iteration 62/1000 | Loss: 0.00001697
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001697
Iteration 65/1000 | Loss: 0.00001697
Iteration 66/1000 | Loss: 0.00001697
Iteration 67/1000 | Loss: 0.00001697
Iteration 68/1000 | Loss: 0.00001697
Iteration 69/1000 | Loss: 0.00001697
Iteration 70/1000 | Loss: 0.00001697
Iteration 71/1000 | Loss: 0.00001697
Iteration 72/1000 | Loss: 0.00001697
Iteration 73/1000 | Loss: 0.00001697
Iteration 74/1000 | Loss: 0.00001697
Iteration 75/1000 | Loss: 0.00001697
Iteration 76/1000 | Loss: 0.00001697
Iteration 77/1000 | Loss: 0.00001697
Iteration 78/1000 | Loss: 0.00001697
Iteration 79/1000 | Loss: 0.00001697
Iteration 80/1000 | Loss: 0.00001697
Iteration 81/1000 | Loss: 0.00001697
Iteration 82/1000 | Loss: 0.00001697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.696933941275347e-05, 1.696933941275347e-05, 1.696933941275347e-05, 1.696933941275347e-05, 1.696933941275347e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.696933941275347e-05

Optimization complete. Final v2v error: 3.5463480949401855 mm

Highest mean error: 4.103390693664551 mm for frame 7

Lowest mean error: 2.78964900970459 mm for frame 178

Saving results

Total time: 46.80338191986084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463761
Iteration 2/25 | Loss: 0.00124926
Iteration 3/25 | Loss: 0.00107561
Iteration 4/25 | Loss: 0.00104400
Iteration 5/25 | Loss: 0.00103552
Iteration 6/25 | Loss: 0.00103340
Iteration 7/25 | Loss: 0.00103302
Iteration 8/25 | Loss: 0.00103302
Iteration 9/25 | Loss: 0.00103302
Iteration 10/25 | Loss: 0.00103302
Iteration 11/25 | Loss: 0.00103302
Iteration 12/25 | Loss: 0.00103302
Iteration 13/25 | Loss: 0.00103302
Iteration 14/25 | Loss: 0.00103302
Iteration 15/25 | Loss: 0.00103302
Iteration 16/25 | Loss: 0.00103302
Iteration 17/25 | Loss: 0.00103302
Iteration 18/25 | Loss: 0.00103302
Iteration 19/25 | Loss: 0.00103302
Iteration 20/25 | Loss: 0.00103302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001033018808811903, 0.001033018808811903, 0.001033018808811903, 0.001033018808811903, 0.001033018808811903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001033018808811903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26142097
Iteration 2/25 | Loss: 0.00072141
Iteration 3/25 | Loss: 0.00072140
Iteration 4/25 | Loss: 0.00072140
Iteration 5/25 | Loss: 0.00072140
Iteration 6/25 | Loss: 0.00072140
Iteration 7/25 | Loss: 0.00072140
Iteration 8/25 | Loss: 0.00072140
Iteration 9/25 | Loss: 0.00072140
Iteration 10/25 | Loss: 0.00072140
Iteration 11/25 | Loss: 0.00072140
Iteration 12/25 | Loss: 0.00072140
Iteration 13/25 | Loss: 0.00072140
Iteration 14/25 | Loss: 0.00072140
Iteration 15/25 | Loss: 0.00072140
Iteration 16/25 | Loss: 0.00072140
Iteration 17/25 | Loss: 0.00072140
Iteration 18/25 | Loss: 0.00072140
Iteration 19/25 | Loss: 0.00072140
Iteration 20/25 | Loss: 0.00072140
Iteration 21/25 | Loss: 0.00072140
Iteration 22/25 | Loss: 0.00072140
Iteration 23/25 | Loss: 0.00072140
Iteration 24/25 | Loss: 0.00072140
Iteration 25/25 | Loss: 0.00072140

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072140
Iteration 2/1000 | Loss: 0.00003939
Iteration 3/1000 | Loss: 0.00002233
Iteration 4/1000 | Loss: 0.00001945
Iteration 5/1000 | Loss: 0.00001856
Iteration 6/1000 | Loss: 0.00001755
Iteration 7/1000 | Loss: 0.00001702
Iteration 8/1000 | Loss: 0.00001663
Iteration 9/1000 | Loss: 0.00001634
Iteration 10/1000 | Loss: 0.00001616
Iteration 11/1000 | Loss: 0.00001615
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001608
Iteration 14/1000 | Loss: 0.00001603
Iteration 15/1000 | Loss: 0.00001596
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001595
Iteration 18/1000 | Loss: 0.00001594
Iteration 19/1000 | Loss: 0.00001589
Iteration 20/1000 | Loss: 0.00001589
Iteration 21/1000 | Loss: 0.00001585
Iteration 22/1000 | Loss: 0.00001584
Iteration 23/1000 | Loss: 0.00001583
Iteration 24/1000 | Loss: 0.00001583
Iteration 25/1000 | Loss: 0.00001583
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001581
Iteration 28/1000 | Loss: 0.00001580
Iteration 29/1000 | Loss: 0.00001580
Iteration 30/1000 | Loss: 0.00001580
Iteration 31/1000 | Loss: 0.00001580
Iteration 32/1000 | Loss: 0.00001580
Iteration 33/1000 | Loss: 0.00001580
Iteration 34/1000 | Loss: 0.00001580
Iteration 35/1000 | Loss: 0.00001580
Iteration 36/1000 | Loss: 0.00001580
Iteration 37/1000 | Loss: 0.00001580
Iteration 38/1000 | Loss: 0.00001580
Iteration 39/1000 | Loss: 0.00001580
Iteration 40/1000 | Loss: 0.00001580
Iteration 41/1000 | Loss: 0.00001579
Iteration 42/1000 | Loss: 0.00001579
Iteration 43/1000 | Loss: 0.00001579
Iteration 44/1000 | Loss: 0.00001578
Iteration 45/1000 | Loss: 0.00001578
Iteration 46/1000 | Loss: 0.00001578
Iteration 47/1000 | Loss: 0.00001578
Iteration 48/1000 | Loss: 0.00001578
Iteration 49/1000 | Loss: 0.00001578
Iteration 50/1000 | Loss: 0.00001577
Iteration 51/1000 | Loss: 0.00001577
Iteration 52/1000 | Loss: 0.00001577
Iteration 53/1000 | Loss: 0.00001577
Iteration 54/1000 | Loss: 0.00001577
Iteration 55/1000 | Loss: 0.00001577
Iteration 56/1000 | Loss: 0.00001577
Iteration 57/1000 | Loss: 0.00001577
Iteration 58/1000 | Loss: 0.00001577
Iteration 59/1000 | Loss: 0.00001577
Iteration 60/1000 | Loss: 0.00001577
Iteration 61/1000 | Loss: 0.00001576
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001576
Iteration 65/1000 | Loss: 0.00001576
Iteration 66/1000 | Loss: 0.00001576
Iteration 67/1000 | Loss: 0.00001576
Iteration 68/1000 | Loss: 0.00001576
Iteration 69/1000 | Loss: 0.00001576
Iteration 70/1000 | Loss: 0.00001576
Iteration 71/1000 | Loss: 0.00001575
Iteration 72/1000 | Loss: 0.00001575
Iteration 73/1000 | Loss: 0.00001575
Iteration 74/1000 | Loss: 0.00001575
Iteration 75/1000 | Loss: 0.00001575
Iteration 76/1000 | Loss: 0.00001575
Iteration 77/1000 | Loss: 0.00001574
Iteration 78/1000 | Loss: 0.00001574
Iteration 79/1000 | Loss: 0.00001574
Iteration 80/1000 | Loss: 0.00001574
Iteration 81/1000 | Loss: 0.00001574
Iteration 82/1000 | Loss: 0.00001574
Iteration 83/1000 | Loss: 0.00001574
Iteration 84/1000 | Loss: 0.00001574
Iteration 85/1000 | Loss: 0.00001574
Iteration 86/1000 | Loss: 0.00001574
Iteration 87/1000 | Loss: 0.00001574
Iteration 88/1000 | Loss: 0.00001574
Iteration 89/1000 | Loss: 0.00001574
Iteration 90/1000 | Loss: 0.00001574
Iteration 91/1000 | Loss: 0.00001574
Iteration 92/1000 | Loss: 0.00001574
Iteration 93/1000 | Loss: 0.00001574
Iteration 94/1000 | Loss: 0.00001574
Iteration 95/1000 | Loss: 0.00001574
Iteration 96/1000 | Loss: 0.00001574
Iteration 97/1000 | Loss: 0.00001574
Iteration 98/1000 | Loss: 0.00001574
Iteration 99/1000 | Loss: 0.00001574
Iteration 100/1000 | Loss: 0.00001574
Iteration 101/1000 | Loss: 0.00001574
Iteration 102/1000 | Loss: 0.00001574
Iteration 103/1000 | Loss: 0.00001574
Iteration 104/1000 | Loss: 0.00001574
Iteration 105/1000 | Loss: 0.00001574
Iteration 106/1000 | Loss: 0.00001574
Iteration 107/1000 | Loss: 0.00001574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.5735646229586564e-05, 1.5735646229586564e-05, 1.5735646229586564e-05, 1.5735646229586564e-05, 1.5735646229586564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5735646229586564e-05

Optimization complete. Final v2v error: 3.4237701892852783 mm

Highest mean error: 4.041705131530762 mm for frame 42

Lowest mean error: 2.999908208847046 mm for frame 82

Saving results

Total time: 32.51448941230774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421355
Iteration 2/25 | Loss: 0.00121559
Iteration 3/25 | Loss: 0.00104706
Iteration 4/25 | Loss: 0.00101393
Iteration 5/25 | Loss: 0.00100279
Iteration 6/25 | Loss: 0.00099969
Iteration 7/25 | Loss: 0.00099896
Iteration 8/25 | Loss: 0.00099859
Iteration 9/25 | Loss: 0.00099858
Iteration 10/25 | Loss: 0.00099858
Iteration 11/25 | Loss: 0.00099858
Iteration 12/25 | Loss: 0.00099858
Iteration 13/25 | Loss: 0.00099858
Iteration 14/25 | Loss: 0.00099858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009985774522647262, 0.0009985774522647262, 0.0009985774522647262, 0.0009985774522647262, 0.0009985774522647262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009985774522647262

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35413194
Iteration 2/25 | Loss: 0.00082278
Iteration 3/25 | Loss: 0.00082277
Iteration 4/25 | Loss: 0.00082277
Iteration 5/25 | Loss: 0.00082277
Iteration 6/25 | Loss: 0.00082277
Iteration 7/25 | Loss: 0.00082277
Iteration 8/25 | Loss: 0.00082277
Iteration 9/25 | Loss: 0.00082277
Iteration 10/25 | Loss: 0.00082277
Iteration 11/25 | Loss: 0.00082277
Iteration 12/25 | Loss: 0.00082277
Iteration 13/25 | Loss: 0.00082277
Iteration 14/25 | Loss: 0.00082277
Iteration 15/25 | Loss: 0.00082277
Iteration 16/25 | Loss: 0.00082277
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008227697690017521, 0.0008227697690017521, 0.0008227697690017521, 0.0008227697690017521, 0.0008227697690017521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008227697690017521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082277
Iteration 2/1000 | Loss: 0.00005702
Iteration 3/1000 | Loss: 0.00002503
Iteration 4/1000 | Loss: 0.00001844
Iteration 5/1000 | Loss: 0.00001680
Iteration 6/1000 | Loss: 0.00001587
Iteration 7/1000 | Loss: 0.00001524
Iteration 8/1000 | Loss: 0.00001490
Iteration 9/1000 | Loss: 0.00001462
Iteration 10/1000 | Loss: 0.00001442
Iteration 11/1000 | Loss: 0.00001435
Iteration 12/1000 | Loss: 0.00001422
Iteration 13/1000 | Loss: 0.00001420
Iteration 14/1000 | Loss: 0.00001416
Iteration 15/1000 | Loss: 0.00001415
Iteration 16/1000 | Loss: 0.00001414
Iteration 17/1000 | Loss: 0.00001412
Iteration 18/1000 | Loss: 0.00001409
Iteration 19/1000 | Loss: 0.00001406
Iteration 20/1000 | Loss: 0.00001403
Iteration 21/1000 | Loss: 0.00001403
Iteration 22/1000 | Loss: 0.00001399
Iteration 23/1000 | Loss: 0.00001399
Iteration 24/1000 | Loss: 0.00001399
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001399
Iteration 27/1000 | Loss: 0.00001399
Iteration 28/1000 | Loss: 0.00001399
Iteration 29/1000 | Loss: 0.00001399
Iteration 30/1000 | Loss: 0.00001399
Iteration 31/1000 | Loss: 0.00001399
Iteration 32/1000 | Loss: 0.00001398
Iteration 33/1000 | Loss: 0.00001398
Iteration 34/1000 | Loss: 0.00001398
Iteration 35/1000 | Loss: 0.00001398
Iteration 36/1000 | Loss: 0.00001398
Iteration 37/1000 | Loss: 0.00001398
Iteration 38/1000 | Loss: 0.00001397
Iteration 39/1000 | Loss: 0.00001396
Iteration 40/1000 | Loss: 0.00001396
Iteration 41/1000 | Loss: 0.00001396
Iteration 42/1000 | Loss: 0.00001395
Iteration 43/1000 | Loss: 0.00001395
Iteration 44/1000 | Loss: 0.00001395
Iteration 45/1000 | Loss: 0.00001395
Iteration 46/1000 | Loss: 0.00001395
Iteration 47/1000 | Loss: 0.00001394
Iteration 48/1000 | Loss: 0.00001394
Iteration 49/1000 | Loss: 0.00001394
Iteration 50/1000 | Loss: 0.00001394
Iteration 51/1000 | Loss: 0.00001394
Iteration 52/1000 | Loss: 0.00001394
Iteration 53/1000 | Loss: 0.00001394
Iteration 54/1000 | Loss: 0.00001393
Iteration 55/1000 | Loss: 0.00001393
Iteration 56/1000 | Loss: 0.00001393
Iteration 57/1000 | Loss: 0.00001393
Iteration 58/1000 | Loss: 0.00001393
Iteration 59/1000 | Loss: 0.00001393
Iteration 60/1000 | Loss: 0.00001393
Iteration 61/1000 | Loss: 0.00001393
Iteration 62/1000 | Loss: 0.00001393
Iteration 63/1000 | Loss: 0.00001393
Iteration 64/1000 | Loss: 0.00001392
Iteration 65/1000 | Loss: 0.00001392
Iteration 66/1000 | Loss: 0.00001392
Iteration 67/1000 | Loss: 0.00001391
Iteration 68/1000 | Loss: 0.00001391
Iteration 69/1000 | Loss: 0.00001391
Iteration 70/1000 | Loss: 0.00001391
Iteration 71/1000 | Loss: 0.00001391
Iteration 72/1000 | Loss: 0.00001390
Iteration 73/1000 | Loss: 0.00001390
Iteration 74/1000 | Loss: 0.00001390
Iteration 75/1000 | Loss: 0.00001390
Iteration 76/1000 | Loss: 0.00001390
Iteration 77/1000 | Loss: 0.00001390
Iteration 78/1000 | Loss: 0.00001390
Iteration 79/1000 | Loss: 0.00001390
Iteration 80/1000 | Loss: 0.00001389
Iteration 81/1000 | Loss: 0.00001389
Iteration 82/1000 | Loss: 0.00001389
Iteration 83/1000 | Loss: 0.00001389
Iteration 84/1000 | Loss: 0.00001389
Iteration 85/1000 | Loss: 0.00001389
Iteration 86/1000 | Loss: 0.00001389
Iteration 87/1000 | Loss: 0.00001389
Iteration 88/1000 | Loss: 0.00001388
Iteration 89/1000 | Loss: 0.00001388
Iteration 90/1000 | Loss: 0.00001388
Iteration 91/1000 | Loss: 0.00001388
Iteration 92/1000 | Loss: 0.00001388
Iteration 93/1000 | Loss: 0.00001388
Iteration 94/1000 | Loss: 0.00001388
Iteration 95/1000 | Loss: 0.00001388
Iteration 96/1000 | Loss: 0.00001388
Iteration 97/1000 | Loss: 0.00001388
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001388
Iteration 101/1000 | Loss: 0.00001388
Iteration 102/1000 | Loss: 0.00001387
Iteration 103/1000 | Loss: 0.00001387
Iteration 104/1000 | Loss: 0.00001387
Iteration 105/1000 | Loss: 0.00001387
Iteration 106/1000 | Loss: 0.00001387
Iteration 107/1000 | Loss: 0.00001387
Iteration 108/1000 | Loss: 0.00001387
Iteration 109/1000 | Loss: 0.00001387
Iteration 110/1000 | Loss: 0.00001386
Iteration 111/1000 | Loss: 0.00001386
Iteration 112/1000 | Loss: 0.00001386
Iteration 113/1000 | Loss: 0.00001386
Iteration 114/1000 | Loss: 0.00001386
Iteration 115/1000 | Loss: 0.00001386
Iteration 116/1000 | Loss: 0.00001386
Iteration 117/1000 | Loss: 0.00001386
Iteration 118/1000 | Loss: 0.00001386
Iteration 119/1000 | Loss: 0.00001386
Iteration 120/1000 | Loss: 0.00001386
Iteration 121/1000 | Loss: 0.00001386
Iteration 122/1000 | Loss: 0.00001386
Iteration 123/1000 | Loss: 0.00001386
Iteration 124/1000 | Loss: 0.00001386
Iteration 125/1000 | Loss: 0.00001386
Iteration 126/1000 | Loss: 0.00001386
Iteration 127/1000 | Loss: 0.00001386
Iteration 128/1000 | Loss: 0.00001386
Iteration 129/1000 | Loss: 0.00001386
Iteration 130/1000 | Loss: 0.00001386
Iteration 131/1000 | Loss: 0.00001385
Iteration 132/1000 | Loss: 0.00001385
Iteration 133/1000 | Loss: 0.00001385
Iteration 134/1000 | Loss: 0.00001385
Iteration 135/1000 | Loss: 0.00001385
Iteration 136/1000 | Loss: 0.00001385
Iteration 137/1000 | Loss: 0.00001385
Iteration 138/1000 | Loss: 0.00001385
Iteration 139/1000 | Loss: 0.00001385
Iteration 140/1000 | Loss: 0.00001385
Iteration 141/1000 | Loss: 0.00001385
Iteration 142/1000 | Loss: 0.00001385
Iteration 143/1000 | Loss: 0.00001385
Iteration 144/1000 | Loss: 0.00001385
Iteration 145/1000 | Loss: 0.00001385
Iteration 146/1000 | Loss: 0.00001385
Iteration 147/1000 | Loss: 0.00001385
Iteration 148/1000 | Loss: 0.00001385
Iteration 149/1000 | Loss: 0.00001385
Iteration 150/1000 | Loss: 0.00001385
Iteration 151/1000 | Loss: 0.00001385
Iteration 152/1000 | Loss: 0.00001385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.3853460586688016e-05, 1.3853460586688016e-05, 1.3853460586688016e-05, 1.3853460586688016e-05, 1.3853460586688016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3853460586688016e-05

Optimization complete. Final v2v error: 3.1521522998809814 mm

Highest mean error: 4.399860382080078 mm for frame 45

Lowest mean error: 2.5017552375793457 mm for frame 104

Saving results

Total time: 37.54705810546875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911992
Iteration 2/25 | Loss: 0.00110561
Iteration 3/25 | Loss: 0.00101391
Iteration 4/25 | Loss: 0.00099958
Iteration 5/25 | Loss: 0.00099486
Iteration 6/25 | Loss: 0.00099360
Iteration 7/25 | Loss: 0.00099356
Iteration 8/25 | Loss: 0.00099356
Iteration 9/25 | Loss: 0.00099356
Iteration 10/25 | Loss: 0.00099356
Iteration 11/25 | Loss: 0.00099356
Iteration 12/25 | Loss: 0.00099356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009935598354786634, 0.0009935598354786634, 0.0009935598354786634, 0.0009935598354786634, 0.0009935598354786634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009935598354786634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27841187
Iteration 2/25 | Loss: 0.00083637
Iteration 3/25 | Loss: 0.00083635
Iteration 4/25 | Loss: 0.00083635
Iteration 5/25 | Loss: 0.00083635
Iteration 6/25 | Loss: 0.00083635
Iteration 7/25 | Loss: 0.00083635
Iteration 8/25 | Loss: 0.00083635
Iteration 9/25 | Loss: 0.00083635
Iteration 10/25 | Loss: 0.00083635
Iteration 11/25 | Loss: 0.00083635
Iteration 12/25 | Loss: 0.00083634
Iteration 13/25 | Loss: 0.00083634
Iteration 14/25 | Loss: 0.00083634
Iteration 15/25 | Loss: 0.00083635
Iteration 16/25 | Loss: 0.00083635
Iteration 17/25 | Loss: 0.00083635
Iteration 18/25 | Loss: 0.00083635
Iteration 19/25 | Loss: 0.00083635
Iteration 20/25 | Loss: 0.00083635
Iteration 21/25 | Loss: 0.00083635
Iteration 22/25 | Loss: 0.00083635
Iteration 23/25 | Loss: 0.00083635
Iteration 24/25 | Loss: 0.00083635
Iteration 25/25 | Loss: 0.00083635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083635
Iteration 2/1000 | Loss: 0.00003927
Iteration 3/1000 | Loss: 0.00001967
Iteration 4/1000 | Loss: 0.00001620
Iteration 5/1000 | Loss: 0.00001521
Iteration 6/1000 | Loss: 0.00001456
Iteration 7/1000 | Loss: 0.00001413
Iteration 8/1000 | Loss: 0.00001377
Iteration 9/1000 | Loss: 0.00001345
Iteration 10/1000 | Loss: 0.00001319
Iteration 11/1000 | Loss: 0.00001304
Iteration 12/1000 | Loss: 0.00001296
Iteration 13/1000 | Loss: 0.00001289
Iteration 14/1000 | Loss: 0.00001283
Iteration 15/1000 | Loss: 0.00001283
Iteration 16/1000 | Loss: 0.00001282
Iteration 17/1000 | Loss: 0.00001280
Iteration 18/1000 | Loss: 0.00001280
Iteration 19/1000 | Loss: 0.00001279
Iteration 20/1000 | Loss: 0.00001275
Iteration 21/1000 | Loss: 0.00001275
Iteration 22/1000 | Loss: 0.00001273
Iteration 23/1000 | Loss: 0.00001272
Iteration 24/1000 | Loss: 0.00001272
Iteration 25/1000 | Loss: 0.00001271
Iteration 26/1000 | Loss: 0.00001271
Iteration 27/1000 | Loss: 0.00001270
Iteration 28/1000 | Loss: 0.00001270
Iteration 29/1000 | Loss: 0.00001270
Iteration 30/1000 | Loss: 0.00001270
Iteration 31/1000 | Loss: 0.00001269
Iteration 32/1000 | Loss: 0.00001269
Iteration 33/1000 | Loss: 0.00001269
Iteration 34/1000 | Loss: 0.00001269
Iteration 35/1000 | Loss: 0.00001269
Iteration 36/1000 | Loss: 0.00001269
Iteration 37/1000 | Loss: 0.00001268
Iteration 38/1000 | Loss: 0.00001268
Iteration 39/1000 | Loss: 0.00001268
Iteration 40/1000 | Loss: 0.00001267
Iteration 41/1000 | Loss: 0.00001267
Iteration 42/1000 | Loss: 0.00001266
Iteration 43/1000 | Loss: 0.00001265
Iteration 44/1000 | Loss: 0.00001265
Iteration 45/1000 | Loss: 0.00001265
Iteration 46/1000 | Loss: 0.00001265
Iteration 47/1000 | Loss: 0.00001264
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001264
Iteration 50/1000 | Loss: 0.00001264
Iteration 51/1000 | Loss: 0.00001264
Iteration 52/1000 | Loss: 0.00001263
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001263
Iteration 56/1000 | Loss: 0.00001262
Iteration 57/1000 | Loss: 0.00001262
Iteration 58/1000 | Loss: 0.00001262
Iteration 59/1000 | Loss: 0.00001262
Iteration 60/1000 | Loss: 0.00001262
Iteration 61/1000 | Loss: 0.00001261
Iteration 62/1000 | Loss: 0.00001261
Iteration 63/1000 | Loss: 0.00001260
Iteration 64/1000 | Loss: 0.00001260
Iteration 65/1000 | Loss: 0.00001260
Iteration 66/1000 | Loss: 0.00001259
Iteration 67/1000 | Loss: 0.00001259
Iteration 68/1000 | Loss: 0.00001259
Iteration 69/1000 | Loss: 0.00001259
Iteration 70/1000 | Loss: 0.00001258
Iteration 71/1000 | Loss: 0.00001258
Iteration 72/1000 | Loss: 0.00001257
Iteration 73/1000 | Loss: 0.00001257
Iteration 74/1000 | Loss: 0.00001257
Iteration 75/1000 | Loss: 0.00001257
Iteration 76/1000 | Loss: 0.00001256
Iteration 77/1000 | Loss: 0.00001256
Iteration 78/1000 | Loss: 0.00001256
Iteration 79/1000 | Loss: 0.00001256
Iteration 80/1000 | Loss: 0.00001256
Iteration 81/1000 | Loss: 0.00001256
Iteration 82/1000 | Loss: 0.00001255
Iteration 83/1000 | Loss: 0.00001255
Iteration 84/1000 | Loss: 0.00001255
Iteration 85/1000 | Loss: 0.00001255
Iteration 86/1000 | Loss: 0.00001255
Iteration 87/1000 | Loss: 0.00001255
Iteration 88/1000 | Loss: 0.00001255
Iteration 89/1000 | Loss: 0.00001255
Iteration 90/1000 | Loss: 0.00001255
Iteration 91/1000 | Loss: 0.00001255
Iteration 92/1000 | Loss: 0.00001255
Iteration 93/1000 | Loss: 0.00001255
Iteration 94/1000 | Loss: 0.00001255
Iteration 95/1000 | Loss: 0.00001255
Iteration 96/1000 | Loss: 0.00001255
Iteration 97/1000 | Loss: 0.00001255
Iteration 98/1000 | Loss: 0.00001254
Iteration 99/1000 | Loss: 0.00001254
Iteration 100/1000 | Loss: 0.00001254
Iteration 101/1000 | Loss: 0.00001254
Iteration 102/1000 | Loss: 0.00001254
Iteration 103/1000 | Loss: 0.00001254
Iteration 104/1000 | Loss: 0.00001254
Iteration 105/1000 | Loss: 0.00001254
Iteration 106/1000 | Loss: 0.00001254
Iteration 107/1000 | Loss: 0.00001254
Iteration 108/1000 | Loss: 0.00001254
Iteration 109/1000 | Loss: 0.00001254
Iteration 110/1000 | Loss: 0.00001254
Iteration 111/1000 | Loss: 0.00001254
Iteration 112/1000 | Loss: 0.00001254
Iteration 113/1000 | Loss: 0.00001254
Iteration 114/1000 | Loss: 0.00001254
Iteration 115/1000 | Loss: 0.00001254
Iteration 116/1000 | Loss: 0.00001254
Iteration 117/1000 | Loss: 0.00001254
Iteration 118/1000 | Loss: 0.00001254
Iteration 119/1000 | Loss: 0.00001254
Iteration 120/1000 | Loss: 0.00001254
Iteration 121/1000 | Loss: 0.00001254
Iteration 122/1000 | Loss: 0.00001254
Iteration 123/1000 | Loss: 0.00001254
Iteration 124/1000 | Loss: 0.00001254
Iteration 125/1000 | Loss: 0.00001254
Iteration 126/1000 | Loss: 0.00001254
Iteration 127/1000 | Loss: 0.00001254
Iteration 128/1000 | Loss: 0.00001254
Iteration 129/1000 | Loss: 0.00001254
Iteration 130/1000 | Loss: 0.00001254
Iteration 131/1000 | Loss: 0.00001254
Iteration 132/1000 | Loss: 0.00001254
Iteration 133/1000 | Loss: 0.00001254
Iteration 134/1000 | Loss: 0.00001254
Iteration 135/1000 | Loss: 0.00001254
Iteration 136/1000 | Loss: 0.00001254
Iteration 137/1000 | Loss: 0.00001254
Iteration 138/1000 | Loss: 0.00001254
Iteration 139/1000 | Loss: 0.00001254
Iteration 140/1000 | Loss: 0.00001254
Iteration 141/1000 | Loss: 0.00001254
Iteration 142/1000 | Loss: 0.00001254
Iteration 143/1000 | Loss: 0.00001254
Iteration 144/1000 | Loss: 0.00001254
Iteration 145/1000 | Loss: 0.00001254
Iteration 146/1000 | Loss: 0.00001254
Iteration 147/1000 | Loss: 0.00001254
Iteration 148/1000 | Loss: 0.00001254
Iteration 149/1000 | Loss: 0.00001254
Iteration 150/1000 | Loss: 0.00001254
Iteration 151/1000 | Loss: 0.00001254
Iteration 152/1000 | Loss: 0.00001254
Iteration 153/1000 | Loss: 0.00001254
Iteration 154/1000 | Loss: 0.00001254
Iteration 155/1000 | Loss: 0.00001254
Iteration 156/1000 | Loss: 0.00001254
Iteration 157/1000 | Loss: 0.00001254
Iteration 158/1000 | Loss: 0.00001254
Iteration 159/1000 | Loss: 0.00001254
Iteration 160/1000 | Loss: 0.00001254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.2541555406642146e-05, 1.2541555406642146e-05, 1.2541555406642146e-05, 1.2541555406642146e-05, 1.2541555406642146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2541555406642146e-05

Optimization complete. Final v2v error: 2.940788745880127 mm

Highest mean error: 3.9687235355377197 mm for frame 40

Lowest mean error: 2.3791470527648926 mm for frame 79

Saving results

Total time: 34.619966983795166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408428
Iteration 2/25 | Loss: 0.00109312
Iteration 3/25 | Loss: 0.00100469
Iteration 4/25 | Loss: 0.00098797
Iteration 5/25 | Loss: 0.00098438
Iteration 6/25 | Loss: 0.00098366
Iteration 7/25 | Loss: 0.00098366
Iteration 8/25 | Loss: 0.00098366
Iteration 9/25 | Loss: 0.00098366
Iteration 10/25 | Loss: 0.00098366
Iteration 11/25 | Loss: 0.00098366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009836559183895588, 0.0009836559183895588, 0.0009836559183895588, 0.0009836559183895588, 0.0009836559183895588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009836559183895588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37289929
Iteration 2/25 | Loss: 0.00078208
Iteration 3/25 | Loss: 0.00078208
Iteration 4/25 | Loss: 0.00078208
Iteration 5/25 | Loss: 0.00078208
Iteration 6/25 | Loss: 0.00078208
Iteration 7/25 | Loss: 0.00078208
Iteration 8/25 | Loss: 0.00078207
Iteration 9/25 | Loss: 0.00078207
Iteration 10/25 | Loss: 0.00078207
Iteration 11/25 | Loss: 0.00078207
Iteration 12/25 | Loss: 0.00078207
Iteration 13/25 | Loss: 0.00078207
Iteration 14/25 | Loss: 0.00078207
Iteration 15/25 | Loss: 0.00078207
Iteration 16/25 | Loss: 0.00078207
Iteration 17/25 | Loss: 0.00078207
Iteration 18/25 | Loss: 0.00078207
Iteration 19/25 | Loss: 0.00078207
Iteration 20/25 | Loss: 0.00078207
Iteration 21/25 | Loss: 0.00078207
Iteration 22/25 | Loss: 0.00078207
Iteration 23/25 | Loss: 0.00078207
Iteration 24/25 | Loss: 0.00078207
Iteration 25/25 | Loss: 0.00078207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078207
Iteration 2/1000 | Loss: 0.00003880
Iteration 3/1000 | Loss: 0.00001773
Iteration 4/1000 | Loss: 0.00001599
Iteration 5/1000 | Loss: 0.00001503
Iteration 6/1000 | Loss: 0.00001421
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001314
Iteration 9/1000 | Loss: 0.00001314
Iteration 10/1000 | Loss: 0.00001292
Iteration 11/1000 | Loss: 0.00001290
Iteration 12/1000 | Loss: 0.00001283
Iteration 13/1000 | Loss: 0.00001266
Iteration 14/1000 | Loss: 0.00001257
Iteration 15/1000 | Loss: 0.00001248
Iteration 16/1000 | Loss: 0.00001244
Iteration 17/1000 | Loss: 0.00001242
Iteration 18/1000 | Loss: 0.00001242
Iteration 19/1000 | Loss: 0.00001241
Iteration 20/1000 | Loss: 0.00001241
Iteration 21/1000 | Loss: 0.00001240
Iteration 22/1000 | Loss: 0.00001240
Iteration 23/1000 | Loss: 0.00001240
Iteration 24/1000 | Loss: 0.00001240
Iteration 25/1000 | Loss: 0.00001240
Iteration 26/1000 | Loss: 0.00001239
Iteration 27/1000 | Loss: 0.00001239
Iteration 28/1000 | Loss: 0.00001239
Iteration 29/1000 | Loss: 0.00001239
Iteration 30/1000 | Loss: 0.00001237
Iteration 31/1000 | Loss: 0.00001236
Iteration 32/1000 | Loss: 0.00001236
Iteration 33/1000 | Loss: 0.00001236
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001235
Iteration 37/1000 | Loss: 0.00001235
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001235
Iteration 40/1000 | Loss: 0.00001235
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00001235
Iteration 43/1000 | Loss: 0.00001234
Iteration 44/1000 | Loss: 0.00001234
Iteration 45/1000 | Loss: 0.00001233
Iteration 46/1000 | Loss: 0.00001233
Iteration 47/1000 | Loss: 0.00001233
Iteration 48/1000 | Loss: 0.00001233
Iteration 49/1000 | Loss: 0.00001233
Iteration 50/1000 | Loss: 0.00001232
Iteration 51/1000 | Loss: 0.00001232
Iteration 52/1000 | Loss: 0.00001232
Iteration 53/1000 | Loss: 0.00001232
Iteration 54/1000 | Loss: 0.00001232
Iteration 55/1000 | Loss: 0.00001232
Iteration 56/1000 | Loss: 0.00001231
Iteration 57/1000 | Loss: 0.00001231
Iteration 58/1000 | Loss: 0.00001231
Iteration 59/1000 | Loss: 0.00001231
Iteration 60/1000 | Loss: 0.00001231
Iteration 61/1000 | Loss: 0.00001231
Iteration 62/1000 | Loss: 0.00001231
Iteration 63/1000 | Loss: 0.00001231
Iteration 64/1000 | Loss: 0.00001231
Iteration 65/1000 | Loss: 0.00001231
Iteration 66/1000 | Loss: 0.00001231
Iteration 67/1000 | Loss: 0.00001231
Iteration 68/1000 | Loss: 0.00001230
Iteration 69/1000 | Loss: 0.00001230
Iteration 70/1000 | Loss: 0.00001230
Iteration 71/1000 | Loss: 0.00001230
Iteration 72/1000 | Loss: 0.00001230
Iteration 73/1000 | Loss: 0.00001230
Iteration 74/1000 | Loss: 0.00001230
Iteration 75/1000 | Loss: 0.00001230
Iteration 76/1000 | Loss: 0.00001230
Iteration 77/1000 | Loss: 0.00001230
Iteration 78/1000 | Loss: 0.00001230
Iteration 79/1000 | Loss: 0.00001230
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001230
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001230
Iteration 85/1000 | Loss: 0.00001230
Iteration 86/1000 | Loss: 0.00001230
Iteration 87/1000 | Loss: 0.00001230
Iteration 88/1000 | Loss: 0.00001230
Iteration 89/1000 | Loss: 0.00001230
Iteration 90/1000 | Loss: 0.00001230
Iteration 91/1000 | Loss: 0.00001230
Iteration 92/1000 | Loss: 0.00001230
Iteration 93/1000 | Loss: 0.00001230
Iteration 94/1000 | Loss: 0.00001230
Iteration 95/1000 | Loss: 0.00001230
Iteration 96/1000 | Loss: 0.00001230
Iteration 97/1000 | Loss: 0.00001230
Iteration 98/1000 | Loss: 0.00001230
Iteration 99/1000 | Loss: 0.00001230
Iteration 100/1000 | Loss: 0.00001230
Iteration 101/1000 | Loss: 0.00001230
Iteration 102/1000 | Loss: 0.00001230
Iteration 103/1000 | Loss: 0.00001230
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001230
Iteration 108/1000 | Loss: 0.00001230
Iteration 109/1000 | Loss: 0.00001230
Iteration 110/1000 | Loss: 0.00001230
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.230034922627965e-05, 1.230034922627965e-05, 1.230034922627965e-05, 1.230034922627965e-05, 1.230034922627965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.230034922627965e-05

Optimization complete. Final v2v error: 3.0449821949005127 mm

Highest mean error: 3.312950611114502 mm for frame 10

Lowest mean error: 2.8264613151550293 mm for frame 104

Saving results

Total time: 31.297566175460815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083049
Iteration 2/25 | Loss: 0.00273595
Iteration 3/25 | Loss: 0.00337443
Iteration 4/25 | Loss: 0.00138527
Iteration 5/25 | Loss: 0.00121653
Iteration 6/25 | Loss: 0.00112515
Iteration 7/25 | Loss: 0.00108710
Iteration 8/25 | Loss: 0.00108140
Iteration 9/25 | Loss: 0.00107069
Iteration 10/25 | Loss: 0.00106473
Iteration 11/25 | Loss: 0.00105779
Iteration 12/25 | Loss: 0.00105070
Iteration 13/25 | Loss: 0.00104421
Iteration 14/25 | Loss: 0.00104491
Iteration 15/25 | Loss: 0.00104097
Iteration 16/25 | Loss: 0.00104266
Iteration 17/25 | Loss: 0.00104252
Iteration 18/25 | Loss: 0.00104323
Iteration 19/25 | Loss: 0.00104658
Iteration 20/25 | Loss: 0.00103766
Iteration 21/25 | Loss: 0.00103306
Iteration 22/25 | Loss: 0.00103106
Iteration 23/25 | Loss: 0.00103066
Iteration 24/25 | Loss: 0.00103048
Iteration 25/25 | Loss: 0.00103044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38941181
Iteration 2/25 | Loss: 0.00067716
Iteration 3/25 | Loss: 0.00067716
Iteration 4/25 | Loss: 0.00067716
Iteration 5/25 | Loss: 0.00067716
Iteration 6/25 | Loss: 0.00067716
Iteration 7/25 | Loss: 0.00067716
Iteration 8/25 | Loss: 0.00067716
Iteration 9/25 | Loss: 0.00067716
Iteration 10/25 | Loss: 0.00067716
Iteration 11/25 | Loss: 0.00067716
Iteration 12/25 | Loss: 0.00067716
Iteration 13/25 | Loss: 0.00067716
Iteration 14/25 | Loss: 0.00067716
Iteration 15/25 | Loss: 0.00067716
Iteration 16/25 | Loss: 0.00067716
Iteration 17/25 | Loss: 0.00067716
Iteration 18/25 | Loss: 0.00067716
Iteration 19/25 | Loss: 0.00067716
Iteration 20/25 | Loss: 0.00067716
Iteration 21/25 | Loss: 0.00067716
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006771552143618464, 0.0006771552143618464, 0.0006771552143618464, 0.0006771552143618464, 0.0006771552143618464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006771552143618464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067716
Iteration 2/1000 | Loss: 0.00008789
Iteration 3/1000 | Loss: 0.00029139
Iteration 4/1000 | Loss: 0.00003255
Iteration 5/1000 | Loss: 0.00002744
Iteration 6/1000 | Loss: 0.00014001
Iteration 7/1000 | Loss: 0.00013074
Iteration 8/1000 | Loss: 0.00002336
Iteration 9/1000 | Loss: 0.00002249
Iteration 10/1000 | Loss: 0.00002150
Iteration 11/1000 | Loss: 0.00002045
Iteration 12/1000 | Loss: 0.00016185
Iteration 13/1000 | Loss: 0.00020792
Iteration 14/1000 | Loss: 0.00022176
Iteration 15/1000 | Loss: 0.00076338
Iteration 16/1000 | Loss: 0.00022235
Iteration 17/1000 | Loss: 0.00004552
Iteration 18/1000 | Loss: 0.00003037
Iteration 19/1000 | Loss: 0.00002478
Iteration 20/1000 | Loss: 0.00004956
Iteration 21/1000 | Loss: 0.00019771
Iteration 22/1000 | Loss: 0.00019280
Iteration 23/1000 | Loss: 0.00019535
Iteration 24/1000 | Loss: 0.00015900
Iteration 25/1000 | Loss: 0.00015798
Iteration 26/1000 | Loss: 0.00014067
Iteration 27/1000 | Loss: 0.00011909
Iteration 28/1000 | Loss: 0.00003891
Iteration 29/1000 | Loss: 0.00010217
Iteration 30/1000 | Loss: 0.00003310
Iteration 31/1000 | Loss: 0.00018215
Iteration 32/1000 | Loss: 0.00012216
Iteration 33/1000 | Loss: 0.00016754
Iteration 34/1000 | Loss: 0.00012577
Iteration 35/1000 | Loss: 0.00018381
Iteration 36/1000 | Loss: 0.00012647
Iteration 37/1000 | Loss: 0.00002540
Iteration 38/1000 | Loss: 0.00002118
Iteration 39/1000 | Loss: 0.00001909
Iteration 40/1000 | Loss: 0.00001824
Iteration 41/1000 | Loss: 0.00001767
Iteration 42/1000 | Loss: 0.00001734
Iteration 43/1000 | Loss: 0.00001717
Iteration 44/1000 | Loss: 0.00001697
Iteration 45/1000 | Loss: 0.00001695
Iteration 46/1000 | Loss: 0.00001679
Iteration 47/1000 | Loss: 0.00001677
Iteration 48/1000 | Loss: 0.00001671
Iteration 49/1000 | Loss: 0.00001671
Iteration 50/1000 | Loss: 0.00001670
Iteration 51/1000 | Loss: 0.00001665
Iteration 52/1000 | Loss: 0.00001665
Iteration 53/1000 | Loss: 0.00001663
Iteration 54/1000 | Loss: 0.00001663
Iteration 55/1000 | Loss: 0.00001662
Iteration 56/1000 | Loss: 0.00001661
Iteration 57/1000 | Loss: 0.00001660
Iteration 58/1000 | Loss: 0.00001659
Iteration 59/1000 | Loss: 0.00001659
Iteration 60/1000 | Loss: 0.00001658
Iteration 61/1000 | Loss: 0.00001658
Iteration 62/1000 | Loss: 0.00001657
Iteration 63/1000 | Loss: 0.00001656
Iteration 64/1000 | Loss: 0.00001655
Iteration 65/1000 | Loss: 0.00001655
Iteration 66/1000 | Loss: 0.00001655
Iteration 67/1000 | Loss: 0.00001654
Iteration 68/1000 | Loss: 0.00001654
Iteration 69/1000 | Loss: 0.00001654
Iteration 70/1000 | Loss: 0.00001654
Iteration 71/1000 | Loss: 0.00001653
Iteration 72/1000 | Loss: 0.00001653
Iteration 73/1000 | Loss: 0.00001653
Iteration 74/1000 | Loss: 0.00001653
Iteration 75/1000 | Loss: 0.00001652
Iteration 76/1000 | Loss: 0.00001651
Iteration 77/1000 | Loss: 0.00001651
Iteration 78/1000 | Loss: 0.00001650
Iteration 79/1000 | Loss: 0.00001650
Iteration 80/1000 | Loss: 0.00001650
Iteration 81/1000 | Loss: 0.00001650
Iteration 82/1000 | Loss: 0.00001650
Iteration 83/1000 | Loss: 0.00001649
Iteration 84/1000 | Loss: 0.00001649
Iteration 85/1000 | Loss: 0.00001649
Iteration 86/1000 | Loss: 0.00001649
Iteration 87/1000 | Loss: 0.00001648
Iteration 88/1000 | Loss: 0.00001648
Iteration 89/1000 | Loss: 0.00001648
Iteration 90/1000 | Loss: 0.00001648
Iteration 91/1000 | Loss: 0.00001648
Iteration 92/1000 | Loss: 0.00001648
Iteration 93/1000 | Loss: 0.00001648
Iteration 94/1000 | Loss: 0.00001647
Iteration 95/1000 | Loss: 0.00001647
Iteration 96/1000 | Loss: 0.00001647
Iteration 97/1000 | Loss: 0.00001647
Iteration 98/1000 | Loss: 0.00001647
Iteration 99/1000 | Loss: 0.00001647
Iteration 100/1000 | Loss: 0.00001647
Iteration 101/1000 | Loss: 0.00001646
Iteration 102/1000 | Loss: 0.00001646
Iteration 103/1000 | Loss: 0.00001646
Iteration 104/1000 | Loss: 0.00001646
Iteration 105/1000 | Loss: 0.00001646
Iteration 106/1000 | Loss: 0.00001646
Iteration 107/1000 | Loss: 0.00001646
Iteration 108/1000 | Loss: 0.00001646
Iteration 109/1000 | Loss: 0.00001646
Iteration 110/1000 | Loss: 0.00001646
Iteration 111/1000 | Loss: 0.00001645
Iteration 112/1000 | Loss: 0.00001645
Iteration 113/1000 | Loss: 0.00001645
Iteration 114/1000 | Loss: 0.00001645
Iteration 115/1000 | Loss: 0.00001645
Iteration 116/1000 | Loss: 0.00001645
Iteration 117/1000 | Loss: 0.00001644
Iteration 118/1000 | Loss: 0.00001644
Iteration 119/1000 | Loss: 0.00001644
Iteration 120/1000 | Loss: 0.00001643
Iteration 121/1000 | Loss: 0.00001643
Iteration 122/1000 | Loss: 0.00001643
Iteration 123/1000 | Loss: 0.00001643
Iteration 124/1000 | Loss: 0.00001642
Iteration 125/1000 | Loss: 0.00001642
Iteration 126/1000 | Loss: 0.00001642
Iteration 127/1000 | Loss: 0.00001642
Iteration 128/1000 | Loss: 0.00001642
Iteration 129/1000 | Loss: 0.00001642
Iteration 130/1000 | Loss: 0.00001641
Iteration 131/1000 | Loss: 0.00001641
Iteration 132/1000 | Loss: 0.00001641
Iteration 133/1000 | Loss: 0.00001641
Iteration 134/1000 | Loss: 0.00001641
Iteration 135/1000 | Loss: 0.00001641
Iteration 136/1000 | Loss: 0.00001641
Iteration 137/1000 | Loss: 0.00001640
Iteration 138/1000 | Loss: 0.00001640
Iteration 139/1000 | Loss: 0.00001640
Iteration 140/1000 | Loss: 0.00001640
Iteration 141/1000 | Loss: 0.00001640
Iteration 142/1000 | Loss: 0.00001639
Iteration 143/1000 | Loss: 0.00001639
Iteration 144/1000 | Loss: 0.00001639
Iteration 145/1000 | Loss: 0.00001639
Iteration 146/1000 | Loss: 0.00001639
Iteration 147/1000 | Loss: 0.00001639
Iteration 148/1000 | Loss: 0.00001639
Iteration 149/1000 | Loss: 0.00001639
Iteration 150/1000 | Loss: 0.00001638
Iteration 151/1000 | Loss: 0.00001638
Iteration 152/1000 | Loss: 0.00001638
Iteration 153/1000 | Loss: 0.00001638
Iteration 154/1000 | Loss: 0.00001638
Iteration 155/1000 | Loss: 0.00001638
Iteration 156/1000 | Loss: 0.00001638
Iteration 157/1000 | Loss: 0.00001637
Iteration 158/1000 | Loss: 0.00001637
Iteration 159/1000 | Loss: 0.00001637
Iteration 160/1000 | Loss: 0.00001637
Iteration 161/1000 | Loss: 0.00001637
Iteration 162/1000 | Loss: 0.00001637
Iteration 163/1000 | Loss: 0.00001637
Iteration 164/1000 | Loss: 0.00001637
Iteration 165/1000 | Loss: 0.00001637
Iteration 166/1000 | Loss: 0.00001637
Iteration 167/1000 | Loss: 0.00001637
Iteration 168/1000 | Loss: 0.00001637
Iteration 169/1000 | Loss: 0.00001637
Iteration 170/1000 | Loss: 0.00001636
Iteration 171/1000 | Loss: 0.00001636
Iteration 172/1000 | Loss: 0.00001636
Iteration 173/1000 | Loss: 0.00001636
Iteration 174/1000 | Loss: 0.00001636
Iteration 175/1000 | Loss: 0.00001636
Iteration 176/1000 | Loss: 0.00001636
Iteration 177/1000 | Loss: 0.00001636
Iteration 178/1000 | Loss: 0.00001636
Iteration 179/1000 | Loss: 0.00001636
Iteration 180/1000 | Loss: 0.00001636
Iteration 181/1000 | Loss: 0.00001636
Iteration 182/1000 | Loss: 0.00001636
Iteration 183/1000 | Loss: 0.00001636
Iteration 184/1000 | Loss: 0.00001636
Iteration 185/1000 | Loss: 0.00001636
Iteration 186/1000 | Loss: 0.00001636
Iteration 187/1000 | Loss: 0.00001636
Iteration 188/1000 | Loss: 0.00001636
Iteration 189/1000 | Loss: 0.00001636
Iteration 190/1000 | Loss: 0.00001636
Iteration 191/1000 | Loss: 0.00001636
Iteration 192/1000 | Loss: 0.00001636
Iteration 193/1000 | Loss: 0.00001636
Iteration 194/1000 | Loss: 0.00001636
Iteration 195/1000 | Loss: 0.00001636
Iteration 196/1000 | Loss: 0.00001636
Iteration 197/1000 | Loss: 0.00001636
Iteration 198/1000 | Loss: 0.00001636
Iteration 199/1000 | Loss: 0.00001636
Iteration 200/1000 | Loss: 0.00001636
Iteration 201/1000 | Loss: 0.00001636
Iteration 202/1000 | Loss: 0.00001636
Iteration 203/1000 | Loss: 0.00001636
Iteration 204/1000 | Loss: 0.00001636
Iteration 205/1000 | Loss: 0.00001636
Iteration 206/1000 | Loss: 0.00001636
Iteration 207/1000 | Loss: 0.00001636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.6356898413505405e-05, 1.6356898413505405e-05, 1.6356898413505405e-05, 1.6356898413505405e-05, 1.6356898413505405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6356898413505405e-05

Optimization complete. Final v2v error: 3.4339942932128906 mm

Highest mean error: 5.198794364929199 mm for frame 102

Lowest mean error: 2.571716070175171 mm for frame 132

Saving results

Total time: 116.73041582107544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086597
Iteration 2/25 | Loss: 0.00157010
Iteration 3/25 | Loss: 0.00123184
Iteration 4/25 | Loss: 0.00119011
Iteration 5/25 | Loss: 0.00118161
Iteration 6/25 | Loss: 0.00117891
Iteration 7/25 | Loss: 0.00117891
Iteration 8/25 | Loss: 0.00117891
Iteration 9/25 | Loss: 0.00117891
Iteration 10/25 | Loss: 0.00117891
Iteration 11/25 | Loss: 0.00117891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011789132840931416, 0.0011789132840931416, 0.0011789132840931416, 0.0011789132840931416, 0.0011789132840931416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011789132840931416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12051225
Iteration 2/25 | Loss: 0.00086232
Iteration 3/25 | Loss: 0.00086227
Iteration 4/25 | Loss: 0.00086227
Iteration 5/25 | Loss: 0.00086227
Iteration 6/25 | Loss: 0.00086227
Iteration 7/25 | Loss: 0.00086227
Iteration 8/25 | Loss: 0.00086227
Iteration 9/25 | Loss: 0.00086227
Iteration 10/25 | Loss: 0.00086227
Iteration 11/25 | Loss: 0.00086227
Iteration 12/25 | Loss: 0.00086227
Iteration 13/25 | Loss: 0.00086227
Iteration 14/25 | Loss: 0.00086227
Iteration 15/25 | Loss: 0.00086227
Iteration 16/25 | Loss: 0.00086227
Iteration 17/25 | Loss: 0.00086227
Iteration 18/25 | Loss: 0.00086227
Iteration 19/25 | Loss: 0.00086227
Iteration 20/25 | Loss: 0.00086227
Iteration 21/25 | Loss: 0.00086227
Iteration 22/25 | Loss: 0.00086227
Iteration 23/25 | Loss: 0.00086227
Iteration 24/25 | Loss: 0.00086227
Iteration 25/25 | Loss: 0.00086227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086227
Iteration 2/1000 | Loss: 0.00006975
Iteration 3/1000 | Loss: 0.00004489
Iteration 4/1000 | Loss: 0.00003825
Iteration 5/1000 | Loss: 0.00003563
Iteration 6/1000 | Loss: 0.00003423
Iteration 7/1000 | Loss: 0.00003336
Iteration 8/1000 | Loss: 0.00003275
Iteration 9/1000 | Loss: 0.00003208
Iteration 10/1000 | Loss: 0.00003140
Iteration 11/1000 | Loss: 0.00003096
Iteration 12/1000 | Loss: 0.00003050
Iteration 13/1000 | Loss: 0.00003023
Iteration 14/1000 | Loss: 0.00003004
Iteration 15/1000 | Loss: 0.00003001
Iteration 16/1000 | Loss: 0.00002995
Iteration 17/1000 | Loss: 0.00002985
Iteration 18/1000 | Loss: 0.00002972
Iteration 19/1000 | Loss: 0.00002971
Iteration 20/1000 | Loss: 0.00002968
Iteration 21/1000 | Loss: 0.00002967
Iteration 22/1000 | Loss: 0.00002966
Iteration 23/1000 | Loss: 0.00002966
Iteration 24/1000 | Loss: 0.00002965
Iteration 25/1000 | Loss: 0.00002965
Iteration 26/1000 | Loss: 0.00002964
Iteration 27/1000 | Loss: 0.00002962
Iteration 28/1000 | Loss: 0.00002962
Iteration 29/1000 | Loss: 0.00002962
Iteration 30/1000 | Loss: 0.00002962
Iteration 31/1000 | Loss: 0.00002962
Iteration 32/1000 | Loss: 0.00002962
Iteration 33/1000 | Loss: 0.00002962
Iteration 34/1000 | Loss: 0.00002962
Iteration 35/1000 | Loss: 0.00002962
Iteration 36/1000 | Loss: 0.00002962
Iteration 37/1000 | Loss: 0.00002962
Iteration 38/1000 | Loss: 0.00002962
Iteration 39/1000 | Loss: 0.00002962
Iteration 40/1000 | Loss: 0.00002961
Iteration 41/1000 | Loss: 0.00002961
Iteration 42/1000 | Loss: 0.00002961
Iteration 43/1000 | Loss: 0.00002960
Iteration 44/1000 | Loss: 0.00002960
Iteration 45/1000 | Loss: 0.00002960
Iteration 46/1000 | Loss: 0.00002960
Iteration 47/1000 | Loss: 0.00002960
Iteration 48/1000 | Loss: 0.00002959
Iteration 49/1000 | Loss: 0.00002959
Iteration 50/1000 | Loss: 0.00002959
Iteration 51/1000 | Loss: 0.00002959
Iteration 52/1000 | Loss: 0.00002959
Iteration 53/1000 | Loss: 0.00002959
Iteration 54/1000 | Loss: 0.00002959
Iteration 55/1000 | Loss: 0.00002959
Iteration 56/1000 | Loss: 0.00002958
Iteration 57/1000 | Loss: 0.00002958
Iteration 58/1000 | Loss: 0.00002957
Iteration 59/1000 | Loss: 0.00002957
Iteration 60/1000 | Loss: 0.00002957
Iteration 61/1000 | Loss: 0.00002956
Iteration 62/1000 | Loss: 0.00002956
Iteration 63/1000 | Loss: 0.00002956
Iteration 64/1000 | Loss: 0.00002956
Iteration 65/1000 | Loss: 0.00002956
Iteration 66/1000 | Loss: 0.00002955
Iteration 67/1000 | Loss: 0.00002955
Iteration 68/1000 | Loss: 0.00002955
Iteration 69/1000 | Loss: 0.00002955
Iteration 70/1000 | Loss: 0.00002955
Iteration 71/1000 | Loss: 0.00002955
Iteration 72/1000 | Loss: 0.00002955
Iteration 73/1000 | Loss: 0.00002955
Iteration 74/1000 | Loss: 0.00002955
Iteration 75/1000 | Loss: 0.00002954
Iteration 76/1000 | Loss: 0.00002954
Iteration 77/1000 | Loss: 0.00002954
Iteration 78/1000 | Loss: 0.00002954
Iteration 79/1000 | Loss: 0.00002953
Iteration 80/1000 | Loss: 0.00002953
Iteration 81/1000 | Loss: 0.00002953
Iteration 82/1000 | Loss: 0.00002952
Iteration 83/1000 | Loss: 0.00002952
Iteration 84/1000 | Loss: 0.00002952
Iteration 85/1000 | Loss: 0.00002952
Iteration 86/1000 | Loss: 0.00002952
Iteration 87/1000 | Loss: 0.00002951
Iteration 88/1000 | Loss: 0.00002951
Iteration 89/1000 | Loss: 0.00002951
Iteration 90/1000 | Loss: 0.00002951
Iteration 91/1000 | Loss: 0.00002950
Iteration 92/1000 | Loss: 0.00002950
Iteration 93/1000 | Loss: 0.00002950
Iteration 94/1000 | Loss: 0.00002950
Iteration 95/1000 | Loss: 0.00002950
Iteration 96/1000 | Loss: 0.00002949
Iteration 97/1000 | Loss: 0.00002949
Iteration 98/1000 | Loss: 0.00002949
Iteration 99/1000 | Loss: 0.00002949
Iteration 100/1000 | Loss: 0.00002949
Iteration 101/1000 | Loss: 0.00002949
Iteration 102/1000 | Loss: 0.00002949
Iteration 103/1000 | Loss: 0.00002949
Iteration 104/1000 | Loss: 0.00002949
Iteration 105/1000 | Loss: 0.00002948
Iteration 106/1000 | Loss: 0.00002948
Iteration 107/1000 | Loss: 0.00002948
Iteration 108/1000 | Loss: 0.00002948
Iteration 109/1000 | Loss: 0.00002948
Iteration 110/1000 | Loss: 0.00002948
Iteration 111/1000 | Loss: 0.00002948
Iteration 112/1000 | Loss: 0.00002948
Iteration 113/1000 | Loss: 0.00002948
Iteration 114/1000 | Loss: 0.00002948
Iteration 115/1000 | Loss: 0.00002948
Iteration 116/1000 | Loss: 0.00002948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.9479064323822968e-05, 2.9479064323822968e-05, 2.9479064323822968e-05, 2.9479064323822968e-05, 2.9479064323822968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9479064323822968e-05

Optimization complete. Final v2v error: 4.550927639007568 mm

Highest mean error: 5.8690314292907715 mm for frame 155

Lowest mean error: 3.610706090927124 mm for frame 220

Saving results

Total time: 45.15156149864197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01116367
Iteration 2/25 | Loss: 0.00182035
Iteration 3/25 | Loss: 0.00137581
Iteration 4/25 | Loss: 0.00127548
Iteration 5/25 | Loss: 0.00127706
Iteration 6/25 | Loss: 0.00130520
Iteration 7/25 | Loss: 0.00125743
Iteration 8/25 | Loss: 0.00120645
Iteration 9/25 | Loss: 0.00121062
Iteration 10/25 | Loss: 0.00120423
Iteration 11/25 | Loss: 0.00120173
Iteration 12/25 | Loss: 0.00120597
Iteration 13/25 | Loss: 0.00118523
Iteration 14/25 | Loss: 0.00117096
Iteration 15/25 | Loss: 0.00115347
Iteration 16/25 | Loss: 0.00114982
Iteration 17/25 | Loss: 0.00114430
Iteration 18/25 | Loss: 0.00113959
Iteration 19/25 | Loss: 0.00114131
Iteration 20/25 | Loss: 0.00114012
Iteration 21/25 | Loss: 0.00114896
Iteration 22/25 | Loss: 0.00115050
Iteration 23/25 | Loss: 0.00115079
Iteration 24/25 | Loss: 0.00115419
Iteration 25/25 | Loss: 0.00115387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08815491
Iteration 2/25 | Loss: 0.00143177
Iteration 3/25 | Loss: 0.00143176
Iteration 4/25 | Loss: 0.00143176
Iteration 5/25 | Loss: 0.00143176
Iteration 6/25 | Loss: 0.00143176
Iteration 7/25 | Loss: 0.00143176
Iteration 8/25 | Loss: 0.00143176
Iteration 9/25 | Loss: 0.00143176
Iteration 10/25 | Loss: 0.00143176
Iteration 11/25 | Loss: 0.00143176
Iteration 12/25 | Loss: 0.00143176
Iteration 13/25 | Loss: 0.00143176
Iteration 14/25 | Loss: 0.00143176
Iteration 15/25 | Loss: 0.00143176
Iteration 16/25 | Loss: 0.00143176
Iteration 17/25 | Loss: 0.00143176
Iteration 18/25 | Loss: 0.00143176
Iteration 19/25 | Loss: 0.00143176
Iteration 20/25 | Loss: 0.00143176
Iteration 21/25 | Loss: 0.00143176
Iteration 22/25 | Loss: 0.00143176
Iteration 23/25 | Loss: 0.00143176
Iteration 24/25 | Loss: 0.00143176
Iteration 25/25 | Loss: 0.00143176
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014317574677988887, 0.0014317574677988887, 0.0014317574677988887, 0.0014317574677988887, 0.0014317574677988887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014317574677988887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143176
Iteration 2/1000 | Loss: 0.00066792
Iteration 3/1000 | Loss: 0.00082221
Iteration 4/1000 | Loss: 0.00044161
Iteration 5/1000 | Loss: 0.00065938
Iteration 6/1000 | Loss: 0.00066090
Iteration 7/1000 | Loss: 0.00055880
Iteration 8/1000 | Loss: 0.00037383
Iteration 9/1000 | Loss: 0.00046790
Iteration 10/1000 | Loss: 0.00054793
Iteration 11/1000 | Loss: 0.00053596
Iteration 12/1000 | Loss: 0.00072841
Iteration 13/1000 | Loss: 0.00072288
Iteration 14/1000 | Loss: 0.00071111
Iteration 15/1000 | Loss: 0.00070215
Iteration 16/1000 | Loss: 0.00054394
Iteration 17/1000 | Loss: 0.00044705
Iteration 18/1000 | Loss: 0.00043855
Iteration 19/1000 | Loss: 0.00057911
Iteration 20/1000 | Loss: 0.00058303
Iteration 21/1000 | Loss: 0.00055468
Iteration 22/1000 | Loss: 0.00065811
Iteration 23/1000 | Loss: 0.00067054
Iteration 24/1000 | Loss: 0.00066297
Iteration 25/1000 | Loss: 0.00036302
Iteration 26/1000 | Loss: 0.00034699
Iteration 27/1000 | Loss: 0.00074062
Iteration 28/1000 | Loss: 0.00057178
Iteration 29/1000 | Loss: 0.00099132
Iteration 30/1000 | Loss: 0.00080690
Iteration 31/1000 | Loss: 0.00118396
Iteration 32/1000 | Loss: 0.00081138
Iteration 33/1000 | Loss: 0.00056920
Iteration 34/1000 | Loss: 0.00029361
Iteration 35/1000 | Loss: 0.00033372
Iteration 36/1000 | Loss: 0.00051832
Iteration 37/1000 | Loss: 0.00037605
Iteration 38/1000 | Loss: 0.00024908
Iteration 39/1000 | Loss: 0.00022610
Iteration 40/1000 | Loss: 0.00011416
Iteration 41/1000 | Loss: 0.00019766
Iteration 42/1000 | Loss: 0.00022121
Iteration 43/1000 | Loss: 0.00016154
Iteration 44/1000 | Loss: 0.00010904
Iteration 45/1000 | Loss: 0.00014762
Iteration 46/1000 | Loss: 0.00053105
Iteration 47/1000 | Loss: 0.00018151
Iteration 48/1000 | Loss: 0.00037515
Iteration 49/1000 | Loss: 0.00028650
Iteration 50/1000 | Loss: 0.00009240
Iteration 51/1000 | Loss: 0.00017687
Iteration 52/1000 | Loss: 0.00012111
Iteration 53/1000 | Loss: 0.00012041
Iteration 54/1000 | Loss: 0.00008089
Iteration 55/1000 | Loss: 0.00010166
Iteration 56/1000 | Loss: 0.00011091
Iteration 57/1000 | Loss: 0.00012283
Iteration 58/1000 | Loss: 0.00018081
Iteration 59/1000 | Loss: 0.00023983
Iteration 60/1000 | Loss: 0.00072550
Iteration 61/1000 | Loss: 0.00037050
Iteration 62/1000 | Loss: 0.00028553
Iteration 63/1000 | Loss: 0.00030978
Iteration 64/1000 | Loss: 0.00033745
Iteration 65/1000 | Loss: 0.00045798
Iteration 66/1000 | Loss: 0.00030382
Iteration 67/1000 | Loss: 0.00025191
Iteration 68/1000 | Loss: 0.00097223
Iteration 69/1000 | Loss: 0.00050226
Iteration 70/1000 | Loss: 0.00070536
Iteration 71/1000 | Loss: 0.00025142
Iteration 72/1000 | Loss: 0.00081302
Iteration 73/1000 | Loss: 0.00115475
Iteration 74/1000 | Loss: 0.00069398
Iteration 75/1000 | Loss: 0.00025187
Iteration 76/1000 | Loss: 0.00019871
Iteration 77/1000 | Loss: 0.00020463
Iteration 78/1000 | Loss: 0.00006861
Iteration 79/1000 | Loss: 0.00006513
Iteration 80/1000 | Loss: 0.00010613
Iteration 81/1000 | Loss: 0.00005707
Iteration 82/1000 | Loss: 0.00014450
Iteration 83/1000 | Loss: 0.00009501
Iteration 84/1000 | Loss: 0.00016070
Iteration 85/1000 | Loss: 0.00012003
Iteration 86/1000 | Loss: 0.00005662
Iteration 87/1000 | Loss: 0.00011576
Iteration 88/1000 | Loss: 0.00010693
Iteration 89/1000 | Loss: 0.00009951
Iteration 90/1000 | Loss: 0.00009197
Iteration 91/1000 | Loss: 0.00008653
Iteration 92/1000 | Loss: 0.00008810
Iteration 93/1000 | Loss: 0.00010336
Iteration 94/1000 | Loss: 0.00017116
Iteration 95/1000 | Loss: 0.00009125
Iteration 96/1000 | Loss: 0.00013276
Iteration 97/1000 | Loss: 0.00009359
Iteration 98/1000 | Loss: 0.00012877
Iteration 99/1000 | Loss: 0.00005064
Iteration 100/1000 | Loss: 0.00018419
Iteration 101/1000 | Loss: 0.00018591
Iteration 102/1000 | Loss: 0.00020795
Iteration 103/1000 | Loss: 0.00041036
Iteration 104/1000 | Loss: 0.00032302
Iteration 105/1000 | Loss: 0.00022696
Iteration 106/1000 | Loss: 0.00016374
Iteration 107/1000 | Loss: 0.00020186
Iteration 108/1000 | Loss: 0.00013054
Iteration 109/1000 | Loss: 0.00003684
Iteration 110/1000 | Loss: 0.00006619
Iteration 111/1000 | Loss: 0.00007428
Iteration 112/1000 | Loss: 0.00003322
Iteration 113/1000 | Loss: 0.00004055
Iteration 114/1000 | Loss: 0.00003188
Iteration 115/1000 | Loss: 0.00004259
Iteration 116/1000 | Loss: 0.00013812
Iteration 117/1000 | Loss: 0.00010557
Iteration 118/1000 | Loss: 0.00005111
Iteration 119/1000 | Loss: 0.00004089
Iteration 120/1000 | Loss: 0.00003748
Iteration 121/1000 | Loss: 0.00004118
Iteration 122/1000 | Loss: 0.00003693
Iteration 123/1000 | Loss: 0.00003891
Iteration 124/1000 | Loss: 0.00003599
Iteration 125/1000 | Loss: 0.00003866
Iteration 126/1000 | Loss: 0.00003959
Iteration 127/1000 | Loss: 0.00003830
Iteration 128/1000 | Loss: 0.00004002
Iteration 129/1000 | Loss: 0.00003102
Iteration 130/1000 | Loss: 0.00004200
Iteration 131/1000 | Loss: 0.00003531
Iteration 132/1000 | Loss: 0.00003777
Iteration 133/1000 | Loss: 0.00005104
Iteration 134/1000 | Loss: 0.00003414
Iteration 135/1000 | Loss: 0.00003710
Iteration 136/1000 | Loss: 0.00004135
Iteration 137/1000 | Loss: 0.00003732
Iteration 138/1000 | Loss: 0.00014180
Iteration 139/1000 | Loss: 0.00019496
Iteration 140/1000 | Loss: 0.00018194
Iteration 141/1000 | Loss: 0.00005320
Iteration 142/1000 | Loss: 0.00004912
Iteration 143/1000 | Loss: 0.00017488
Iteration 144/1000 | Loss: 0.00010453
Iteration 145/1000 | Loss: 0.00016878
Iteration 146/1000 | Loss: 0.00004118
Iteration 147/1000 | Loss: 0.00013761
Iteration 148/1000 | Loss: 0.00013497
Iteration 149/1000 | Loss: 0.00003672
Iteration 150/1000 | Loss: 0.00011430
Iteration 151/1000 | Loss: 0.00011832
Iteration 152/1000 | Loss: 0.00014435
Iteration 153/1000 | Loss: 0.00010592
Iteration 154/1000 | Loss: 0.00003690
Iteration 155/1000 | Loss: 0.00011674
Iteration 156/1000 | Loss: 0.00009386
Iteration 157/1000 | Loss: 0.00016161
Iteration 158/1000 | Loss: 0.00008871
Iteration 159/1000 | Loss: 0.00008349
Iteration 160/1000 | Loss: 0.00005886
Iteration 161/1000 | Loss: 0.00014620
Iteration 162/1000 | Loss: 0.00010252
Iteration 163/1000 | Loss: 0.00014500
Iteration 164/1000 | Loss: 0.00014887
Iteration 165/1000 | Loss: 0.00003145
Iteration 166/1000 | Loss: 0.00009876
Iteration 167/1000 | Loss: 0.00011886
Iteration 168/1000 | Loss: 0.00012442
Iteration 169/1000 | Loss: 0.00010594
Iteration 170/1000 | Loss: 0.00003016
Iteration 171/1000 | Loss: 0.00002809
Iteration 172/1000 | Loss: 0.00002744
Iteration 173/1000 | Loss: 0.00002717
Iteration 174/1000 | Loss: 0.00002693
Iteration 175/1000 | Loss: 0.00002669
Iteration 176/1000 | Loss: 0.00002648
Iteration 177/1000 | Loss: 0.00002633
Iteration 178/1000 | Loss: 0.00002629
Iteration 179/1000 | Loss: 0.00002626
Iteration 180/1000 | Loss: 0.00002619
Iteration 181/1000 | Loss: 0.00002619
Iteration 182/1000 | Loss: 0.00002617
Iteration 183/1000 | Loss: 0.00002617
Iteration 184/1000 | Loss: 0.00002616
Iteration 185/1000 | Loss: 0.00002616
Iteration 186/1000 | Loss: 0.00002616
Iteration 187/1000 | Loss: 0.00002616
Iteration 188/1000 | Loss: 0.00002616
Iteration 189/1000 | Loss: 0.00002616
Iteration 190/1000 | Loss: 0.00002616
Iteration 191/1000 | Loss: 0.00002616
Iteration 192/1000 | Loss: 0.00002615
Iteration 193/1000 | Loss: 0.00002615
Iteration 194/1000 | Loss: 0.00002614
Iteration 195/1000 | Loss: 0.00002614
Iteration 196/1000 | Loss: 0.00002613
Iteration 197/1000 | Loss: 0.00002610
Iteration 198/1000 | Loss: 0.00002610
Iteration 199/1000 | Loss: 0.00002610
Iteration 200/1000 | Loss: 0.00002609
Iteration 201/1000 | Loss: 0.00002609
Iteration 202/1000 | Loss: 0.00002608
Iteration 203/1000 | Loss: 0.00002608
Iteration 204/1000 | Loss: 0.00002607
Iteration 205/1000 | Loss: 0.00002607
Iteration 206/1000 | Loss: 0.00002607
Iteration 207/1000 | Loss: 0.00002606
Iteration 208/1000 | Loss: 0.00002606
Iteration 209/1000 | Loss: 0.00002606
Iteration 210/1000 | Loss: 0.00002605
Iteration 211/1000 | Loss: 0.00002605
Iteration 212/1000 | Loss: 0.00002605
Iteration 213/1000 | Loss: 0.00002604
Iteration 214/1000 | Loss: 0.00002604
Iteration 215/1000 | Loss: 0.00002604
Iteration 216/1000 | Loss: 0.00002604
Iteration 217/1000 | Loss: 0.00002603
Iteration 218/1000 | Loss: 0.00002603
Iteration 219/1000 | Loss: 0.00002603
Iteration 220/1000 | Loss: 0.00002603
Iteration 221/1000 | Loss: 0.00002602
Iteration 222/1000 | Loss: 0.00002602
Iteration 223/1000 | Loss: 0.00002602
Iteration 224/1000 | Loss: 0.00002602
Iteration 225/1000 | Loss: 0.00002602
Iteration 226/1000 | Loss: 0.00002601
Iteration 227/1000 | Loss: 0.00002601
Iteration 228/1000 | Loss: 0.00002601
Iteration 229/1000 | Loss: 0.00002601
Iteration 230/1000 | Loss: 0.00002601
Iteration 231/1000 | Loss: 0.00002601
Iteration 232/1000 | Loss: 0.00002601
Iteration 233/1000 | Loss: 0.00002601
Iteration 234/1000 | Loss: 0.00002601
Iteration 235/1000 | Loss: 0.00002601
Iteration 236/1000 | Loss: 0.00002601
Iteration 237/1000 | Loss: 0.00002600
Iteration 238/1000 | Loss: 0.00002600
Iteration 239/1000 | Loss: 0.00002600
Iteration 240/1000 | Loss: 0.00002600
Iteration 241/1000 | Loss: 0.00002600
Iteration 242/1000 | Loss: 0.00002600
Iteration 243/1000 | Loss: 0.00002600
Iteration 244/1000 | Loss: 0.00002600
Iteration 245/1000 | Loss: 0.00002600
Iteration 246/1000 | Loss: 0.00002599
Iteration 247/1000 | Loss: 0.00002599
Iteration 248/1000 | Loss: 0.00002599
Iteration 249/1000 | Loss: 0.00002598
Iteration 250/1000 | Loss: 0.00002598
Iteration 251/1000 | Loss: 0.00002598
Iteration 252/1000 | Loss: 0.00002598
Iteration 253/1000 | Loss: 0.00002598
Iteration 254/1000 | Loss: 0.00002598
Iteration 255/1000 | Loss: 0.00002597
Iteration 256/1000 | Loss: 0.00002597
Iteration 257/1000 | Loss: 0.00002597
Iteration 258/1000 | Loss: 0.00002597
Iteration 259/1000 | Loss: 0.00002597
Iteration 260/1000 | Loss: 0.00002596
Iteration 261/1000 | Loss: 0.00002596
Iteration 262/1000 | Loss: 0.00002596
Iteration 263/1000 | Loss: 0.00002596
Iteration 264/1000 | Loss: 0.00002596
Iteration 265/1000 | Loss: 0.00002596
Iteration 266/1000 | Loss: 0.00002596
Iteration 267/1000 | Loss: 0.00002596
Iteration 268/1000 | Loss: 0.00002596
Iteration 269/1000 | Loss: 0.00002595
Iteration 270/1000 | Loss: 0.00002595
Iteration 271/1000 | Loss: 0.00002595
Iteration 272/1000 | Loss: 0.00002595
Iteration 273/1000 | Loss: 0.00002595
Iteration 274/1000 | Loss: 0.00002595
Iteration 275/1000 | Loss: 0.00002595
Iteration 276/1000 | Loss: 0.00002595
Iteration 277/1000 | Loss: 0.00002595
Iteration 278/1000 | Loss: 0.00002595
Iteration 279/1000 | Loss: 0.00002594
Iteration 280/1000 | Loss: 0.00002594
Iteration 281/1000 | Loss: 0.00002594
Iteration 282/1000 | Loss: 0.00002594
Iteration 283/1000 | Loss: 0.00002594
Iteration 284/1000 | Loss: 0.00002593
Iteration 285/1000 | Loss: 0.00002593
Iteration 286/1000 | Loss: 0.00002593
Iteration 287/1000 | Loss: 0.00002593
Iteration 288/1000 | Loss: 0.00002592
Iteration 289/1000 | Loss: 0.00002592
Iteration 290/1000 | Loss: 0.00002592
Iteration 291/1000 | Loss: 0.00002592
Iteration 292/1000 | Loss: 0.00002592
Iteration 293/1000 | Loss: 0.00002592
Iteration 294/1000 | Loss: 0.00002591
Iteration 295/1000 | Loss: 0.00002591
Iteration 296/1000 | Loss: 0.00002591
Iteration 297/1000 | Loss: 0.00002591
Iteration 298/1000 | Loss: 0.00002591
Iteration 299/1000 | Loss: 0.00002591
Iteration 300/1000 | Loss: 0.00002590
Iteration 301/1000 | Loss: 0.00002590
Iteration 302/1000 | Loss: 0.00002590
Iteration 303/1000 | Loss: 0.00002590
Iteration 304/1000 | Loss: 0.00002590
Iteration 305/1000 | Loss: 0.00002590
Iteration 306/1000 | Loss: 0.00002590
Iteration 307/1000 | Loss: 0.00002590
Iteration 308/1000 | Loss: 0.00002590
Iteration 309/1000 | Loss: 0.00002590
Iteration 310/1000 | Loss: 0.00002590
Iteration 311/1000 | Loss: 0.00002590
Iteration 312/1000 | Loss: 0.00002590
Iteration 313/1000 | Loss: 0.00002590
Iteration 314/1000 | Loss: 0.00002590
Iteration 315/1000 | Loss: 0.00002590
Iteration 316/1000 | Loss: 0.00002590
Iteration 317/1000 | Loss: 0.00002590
Iteration 318/1000 | Loss: 0.00002590
Iteration 319/1000 | Loss: 0.00002590
Iteration 320/1000 | Loss: 0.00002590
Iteration 321/1000 | Loss: 0.00002590
Iteration 322/1000 | Loss: 0.00002590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 322. Stopping optimization.
Last 5 losses: [2.5897001250996254e-05, 2.5897001250996254e-05, 2.5897001250996254e-05, 2.5897001250996254e-05, 2.5897001250996254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5897001250996254e-05

Optimization complete. Final v2v error: 3.978605270385742 mm

Highest mean error: 7.462833881378174 mm for frame 56

Lowest mean error: 3.351451873779297 mm for frame 116

Saving results

Total time: 324.18550062179565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859641
Iteration 2/25 | Loss: 0.00126783
Iteration 3/25 | Loss: 0.00110203
Iteration 4/25 | Loss: 0.00108154
Iteration 5/25 | Loss: 0.00107749
Iteration 6/25 | Loss: 0.00107641
Iteration 7/25 | Loss: 0.00107606
Iteration 8/25 | Loss: 0.00107596
Iteration 9/25 | Loss: 0.00107596
Iteration 10/25 | Loss: 0.00107596
Iteration 11/25 | Loss: 0.00107596
Iteration 12/25 | Loss: 0.00107596
Iteration 13/25 | Loss: 0.00107596
Iteration 14/25 | Loss: 0.00107596
Iteration 15/25 | Loss: 0.00107596
Iteration 16/25 | Loss: 0.00107596
Iteration 17/25 | Loss: 0.00107596
Iteration 18/25 | Loss: 0.00107596
Iteration 19/25 | Loss: 0.00107596
Iteration 20/25 | Loss: 0.00107596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010759581346064806, 0.0010759581346064806, 0.0010759581346064806, 0.0010759581346064806, 0.0010759581346064806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010759581346064806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30448687
Iteration 2/25 | Loss: 0.00201088
Iteration 3/25 | Loss: 0.00201088
Iteration 4/25 | Loss: 0.00201088
Iteration 5/25 | Loss: 0.00201088
Iteration 6/25 | Loss: 0.00201087
Iteration 7/25 | Loss: 0.00201087
Iteration 8/25 | Loss: 0.00201087
Iteration 9/25 | Loss: 0.00201087
Iteration 10/25 | Loss: 0.00201087
Iteration 11/25 | Loss: 0.00201087
Iteration 12/25 | Loss: 0.00201087
Iteration 13/25 | Loss: 0.00201087
Iteration 14/25 | Loss: 0.00201087
Iteration 15/25 | Loss: 0.00201087
Iteration 16/25 | Loss: 0.00201087
Iteration 17/25 | Loss: 0.00201087
Iteration 18/25 | Loss: 0.00201087
Iteration 19/25 | Loss: 0.00201087
Iteration 20/25 | Loss: 0.00201087
Iteration 21/25 | Loss: 0.00201087
Iteration 22/25 | Loss: 0.00201087
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002010873518884182, 0.002010873518884182, 0.002010873518884182, 0.002010873518884182, 0.002010873518884182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002010873518884182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201087
Iteration 2/1000 | Loss: 0.00033920
Iteration 3/1000 | Loss: 0.00012006
Iteration 4/1000 | Loss: 0.00024735
Iteration 5/1000 | Loss: 0.00024872
Iteration 6/1000 | Loss: 0.00009373
Iteration 7/1000 | Loss: 0.00021748
Iteration 8/1000 | Loss: 0.00020103
Iteration 9/1000 | Loss: 0.00019238
Iteration 10/1000 | Loss: 0.00018305
Iteration 11/1000 | Loss: 0.00018856
Iteration 12/1000 | Loss: 0.00017440
Iteration 13/1000 | Loss: 0.00018905
Iteration 14/1000 | Loss: 0.00008077
Iteration 15/1000 | Loss: 0.00007432
Iteration 16/1000 | Loss: 0.00007121
Iteration 17/1000 | Loss: 0.00006726
Iteration 18/1000 | Loss: 0.00006544
Iteration 19/1000 | Loss: 0.00006410
Iteration 20/1000 | Loss: 0.00006244
Iteration 21/1000 | Loss: 0.00006125
Iteration 22/1000 | Loss: 0.00006053
Iteration 23/1000 | Loss: 0.00005973
Iteration 24/1000 | Loss: 0.00005896
Iteration 25/1000 | Loss: 0.00005847
Iteration 26/1000 | Loss: 0.00005805
Iteration 27/1000 | Loss: 0.00005778
Iteration 28/1000 | Loss: 0.00005753
Iteration 29/1000 | Loss: 0.00005744
Iteration 30/1000 | Loss: 0.00005741
Iteration 31/1000 | Loss: 0.00005738
Iteration 32/1000 | Loss: 0.00005737
Iteration 33/1000 | Loss: 0.00005729
Iteration 34/1000 | Loss: 0.00005714
Iteration 35/1000 | Loss: 0.00005712
Iteration 36/1000 | Loss: 0.00005711
Iteration 37/1000 | Loss: 0.00005709
Iteration 38/1000 | Loss: 0.00005708
Iteration 39/1000 | Loss: 0.00005707
Iteration 40/1000 | Loss: 0.00005707
Iteration 41/1000 | Loss: 0.00005707
Iteration 42/1000 | Loss: 0.00005701
Iteration 43/1000 | Loss: 0.00005695
Iteration 44/1000 | Loss: 0.00005695
Iteration 45/1000 | Loss: 0.00005693
Iteration 46/1000 | Loss: 0.00005691
Iteration 47/1000 | Loss: 0.00005691
Iteration 48/1000 | Loss: 0.00005688
Iteration 49/1000 | Loss: 0.00005687
Iteration 50/1000 | Loss: 0.00005686
Iteration 51/1000 | Loss: 0.00005685
Iteration 52/1000 | Loss: 0.00005685
Iteration 53/1000 | Loss: 0.00005684
Iteration 54/1000 | Loss: 0.00005683
Iteration 55/1000 | Loss: 0.00005682
Iteration 56/1000 | Loss: 0.00005677
Iteration 57/1000 | Loss: 0.00005677
Iteration 58/1000 | Loss: 0.00005675
Iteration 59/1000 | Loss: 0.00005675
Iteration 60/1000 | Loss: 0.00005674
Iteration 61/1000 | Loss: 0.00005674
Iteration 62/1000 | Loss: 0.00005674
Iteration 63/1000 | Loss: 0.00005674
Iteration 64/1000 | Loss: 0.00005673
Iteration 65/1000 | Loss: 0.00005673
Iteration 66/1000 | Loss: 0.00005671
Iteration 67/1000 | Loss: 0.00005671
Iteration 68/1000 | Loss: 0.00005670
Iteration 69/1000 | Loss: 0.00005670
Iteration 70/1000 | Loss: 0.00005669
Iteration 71/1000 | Loss: 0.00005669
Iteration 72/1000 | Loss: 0.00005669
Iteration 73/1000 | Loss: 0.00005669
Iteration 74/1000 | Loss: 0.00005668
Iteration 75/1000 | Loss: 0.00005668
Iteration 76/1000 | Loss: 0.00005668
Iteration 77/1000 | Loss: 0.00005667
Iteration 78/1000 | Loss: 0.00005667
Iteration 79/1000 | Loss: 0.00005667
Iteration 80/1000 | Loss: 0.00005667
Iteration 81/1000 | Loss: 0.00005666
Iteration 82/1000 | Loss: 0.00005666
Iteration 83/1000 | Loss: 0.00005666
Iteration 84/1000 | Loss: 0.00005666
Iteration 85/1000 | Loss: 0.00005666
Iteration 86/1000 | Loss: 0.00005666
Iteration 87/1000 | Loss: 0.00005666
Iteration 88/1000 | Loss: 0.00005665
Iteration 89/1000 | Loss: 0.00005665
Iteration 90/1000 | Loss: 0.00005665
Iteration 91/1000 | Loss: 0.00005664
Iteration 92/1000 | Loss: 0.00005664
Iteration 93/1000 | Loss: 0.00005664
Iteration 94/1000 | Loss: 0.00005664
Iteration 95/1000 | Loss: 0.00005664
Iteration 96/1000 | Loss: 0.00005663
Iteration 97/1000 | Loss: 0.00005663
Iteration 98/1000 | Loss: 0.00005663
Iteration 99/1000 | Loss: 0.00005663
Iteration 100/1000 | Loss: 0.00005662
Iteration 101/1000 | Loss: 0.00005662
Iteration 102/1000 | Loss: 0.00005662
Iteration 103/1000 | Loss: 0.00005662
Iteration 104/1000 | Loss: 0.00005662
Iteration 105/1000 | Loss: 0.00005661
Iteration 106/1000 | Loss: 0.00005661
Iteration 107/1000 | Loss: 0.00005661
Iteration 108/1000 | Loss: 0.00005661
Iteration 109/1000 | Loss: 0.00005660
Iteration 110/1000 | Loss: 0.00005660
Iteration 111/1000 | Loss: 0.00005660
Iteration 112/1000 | Loss: 0.00005660
Iteration 113/1000 | Loss: 0.00005660
Iteration 114/1000 | Loss: 0.00005659
Iteration 115/1000 | Loss: 0.00005659
Iteration 116/1000 | Loss: 0.00005657
Iteration 117/1000 | Loss: 0.00005657
Iteration 118/1000 | Loss: 0.00005657
Iteration 119/1000 | Loss: 0.00005656
Iteration 120/1000 | Loss: 0.00005656
Iteration 121/1000 | Loss: 0.00005656
Iteration 122/1000 | Loss: 0.00005656
Iteration 123/1000 | Loss: 0.00005656
Iteration 124/1000 | Loss: 0.00005655
Iteration 125/1000 | Loss: 0.00005655
Iteration 126/1000 | Loss: 0.00005655
Iteration 127/1000 | Loss: 0.00005655
Iteration 128/1000 | Loss: 0.00005654
Iteration 129/1000 | Loss: 0.00005654
Iteration 130/1000 | Loss: 0.00005654
Iteration 131/1000 | Loss: 0.00005654
Iteration 132/1000 | Loss: 0.00005654
Iteration 133/1000 | Loss: 0.00005653
Iteration 134/1000 | Loss: 0.00005653
Iteration 135/1000 | Loss: 0.00005653
Iteration 136/1000 | Loss: 0.00005653
Iteration 137/1000 | Loss: 0.00005653
Iteration 138/1000 | Loss: 0.00005653
Iteration 139/1000 | Loss: 0.00005653
Iteration 140/1000 | Loss: 0.00005653
Iteration 141/1000 | Loss: 0.00005653
Iteration 142/1000 | Loss: 0.00005653
Iteration 143/1000 | Loss: 0.00005653
Iteration 144/1000 | Loss: 0.00005653
Iteration 145/1000 | Loss: 0.00005653
Iteration 146/1000 | Loss: 0.00005652
Iteration 147/1000 | Loss: 0.00005652
Iteration 148/1000 | Loss: 0.00005652
Iteration 149/1000 | Loss: 0.00005652
Iteration 150/1000 | Loss: 0.00005652
Iteration 151/1000 | Loss: 0.00005652
Iteration 152/1000 | Loss: 0.00005652
Iteration 153/1000 | Loss: 0.00005652
Iteration 154/1000 | Loss: 0.00005652
Iteration 155/1000 | Loss: 0.00005652
Iteration 156/1000 | Loss: 0.00005651
Iteration 157/1000 | Loss: 0.00005651
Iteration 158/1000 | Loss: 0.00005651
Iteration 159/1000 | Loss: 0.00005651
Iteration 160/1000 | Loss: 0.00005651
Iteration 161/1000 | Loss: 0.00005651
Iteration 162/1000 | Loss: 0.00005651
Iteration 163/1000 | Loss: 0.00005651
Iteration 164/1000 | Loss: 0.00005651
Iteration 165/1000 | Loss: 0.00005651
Iteration 166/1000 | Loss: 0.00005651
Iteration 167/1000 | Loss: 0.00005651
Iteration 168/1000 | Loss: 0.00005651
Iteration 169/1000 | Loss: 0.00005650
Iteration 170/1000 | Loss: 0.00005650
Iteration 171/1000 | Loss: 0.00005650
Iteration 172/1000 | Loss: 0.00005650
Iteration 173/1000 | Loss: 0.00005650
Iteration 174/1000 | Loss: 0.00005650
Iteration 175/1000 | Loss: 0.00005650
Iteration 176/1000 | Loss: 0.00005650
Iteration 177/1000 | Loss: 0.00005650
Iteration 178/1000 | Loss: 0.00005650
Iteration 179/1000 | Loss: 0.00005650
Iteration 180/1000 | Loss: 0.00005650
Iteration 181/1000 | Loss: 0.00005649
Iteration 182/1000 | Loss: 0.00005649
Iteration 183/1000 | Loss: 0.00005649
Iteration 184/1000 | Loss: 0.00005649
Iteration 185/1000 | Loss: 0.00005649
Iteration 186/1000 | Loss: 0.00005649
Iteration 187/1000 | Loss: 0.00005649
Iteration 188/1000 | Loss: 0.00005649
Iteration 189/1000 | Loss: 0.00005649
Iteration 190/1000 | Loss: 0.00005649
Iteration 191/1000 | Loss: 0.00005648
Iteration 192/1000 | Loss: 0.00005648
Iteration 193/1000 | Loss: 0.00005648
Iteration 194/1000 | Loss: 0.00005648
Iteration 195/1000 | Loss: 0.00005648
Iteration 196/1000 | Loss: 0.00005648
Iteration 197/1000 | Loss: 0.00005648
Iteration 198/1000 | Loss: 0.00005648
Iteration 199/1000 | Loss: 0.00005648
Iteration 200/1000 | Loss: 0.00005648
Iteration 201/1000 | Loss: 0.00005648
Iteration 202/1000 | Loss: 0.00005647
Iteration 203/1000 | Loss: 0.00005647
Iteration 204/1000 | Loss: 0.00005647
Iteration 205/1000 | Loss: 0.00005647
Iteration 206/1000 | Loss: 0.00005647
Iteration 207/1000 | Loss: 0.00005647
Iteration 208/1000 | Loss: 0.00005647
Iteration 209/1000 | Loss: 0.00005647
Iteration 210/1000 | Loss: 0.00005647
Iteration 211/1000 | Loss: 0.00005647
Iteration 212/1000 | Loss: 0.00005647
Iteration 213/1000 | Loss: 0.00005647
Iteration 214/1000 | Loss: 0.00005647
Iteration 215/1000 | Loss: 0.00005646
Iteration 216/1000 | Loss: 0.00005646
Iteration 217/1000 | Loss: 0.00005646
Iteration 218/1000 | Loss: 0.00005646
Iteration 219/1000 | Loss: 0.00005646
Iteration 220/1000 | Loss: 0.00005646
Iteration 221/1000 | Loss: 0.00005646
Iteration 222/1000 | Loss: 0.00005646
Iteration 223/1000 | Loss: 0.00005646
Iteration 224/1000 | Loss: 0.00005646
Iteration 225/1000 | Loss: 0.00005646
Iteration 226/1000 | Loss: 0.00005645
Iteration 227/1000 | Loss: 0.00005645
Iteration 228/1000 | Loss: 0.00005645
Iteration 229/1000 | Loss: 0.00005645
Iteration 230/1000 | Loss: 0.00005645
Iteration 231/1000 | Loss: 0.00005645
Iteration 232/1000 | Loss: 0.00005645
Iteration 233/1000 | Loss: 0.00005645
Iteration 234/1000 | Loss: 0.00005644
Iteration 235/1000 | Loss: 0.00005644
Iteration 236/1000 | Loss: 0.00005644
Iteration 237/1000 | Loss: 0.00005644
Iteration 238/1000 | Loss: 0.00005644
Iteration 239/1000 | Loss: 0.00005644
Iteration 240/1000 | Loss: 0.00005644
Iteration 241/1000 | Loss: 0.00005644
Iteration 242/1000 | Loss: 0.00005644
Iteration 243/1000 | Loss: 0.00005644
Iteration 244/1000 | Loss: 0.00005644
Iteration 245/1000 | Loss: 0.00005644
Iteration 246/1000 | Loss: 0.00005644
Iteration 247/1000 | Loss: 0.00005643
Iteration 248/1000 | Loss: 0.00005643
Iteration 249/1000 | Loss: 0.00005643
Iteration 250/1000 | Loss: 0.00005643
Iteration 251/1000 | Loss: 0.00005643
Iteration 252/1000 | Loss: 0.00005642
Iteration 253/1000 | Loss: 0.00005642
Iteration 254/1000 | Loss: 0.00005642
Iteration 255/1000 | Loss: 0.00005642
Iteration 256/1000 | Loss: 0.00005642
Iteration 257/1000 | Loss: 0.00005642
Iteration 258/1000 | Loss: 0.00005642
Iteration 259/1000 | Loss: 0.00005642
Iteration 260/1000 | Loss: 0.00005642
Iteration 261/1000 | Loss: 0.00005642
Iteration 262/1000 | Loss: 0.00005641
Iteration 263/1000 | Loss: 0.00005641
Iteration 264/1000 | Loss: 0.00005641
Iteration 265/1000 | Loss: 0.00005641
Iteration 266/1000 | Loss: 0.00005641
Iteration 267/1000 | Loss: 0.00005641
Iteration 268/1000 | Loss: 0.00005641
Iteration 269/1000 | Loss: 0.00005640
Iteration 270/1000 | Loss: 0.00005640
Iteration 271/1000 | Loss: 0.00005640
Iteration 272/1000 | Loss: 0.00005640
Iteration 273/1000 | Loss: 0.00005640
Iteration 274/1000 | Loss: 0.00005640
Iteration 275/1000 | Loss: 0.00005640
Iteration 276/1000 | Loss: 0.00005640
Iteration 277/1000 | Loss: 0.00005640
Iteration 278/1000 | Loss: 0.00005640
Iteration 279/1000 | Loss: 0.00005640
Iteration 280/1000 | Loss: 0.00005639
Iteration 281/1000 | Loss: 0.00005639
Iteration 282/1000 | Loss: 0.00005639
Iteration 283/1000 | Loss: 0.00005639
Iteration 284/1000 | Loss: 0.00005639
Iteration 285/1000 | Loss: 0.00005639
Iteration 286/1000 | Loss: 0.00005639
Iteration 287/1000 | Loss: 0.00005639
Iteration 288/1000 | Loss: 0.00005639
Iteration 289/1000 | Loss: 0.00005639
Iteration 290/1000 | Loss: 0.00005639
Iteration 291/1000 | Loss: 0.00005639
Iteration 292/1000 | Loss: 0.00005639
Iteration 293/1000 | Loss: 0.00005639
Iteration 294/1000 | Loss: 0.00005639
Iteration 295/1000 | Loss: 0.00005639
Iteration 296/1000 | Loss: 0.00005639
Iteration 297/1000 | Loss: 0.00005639
Iteration 298/1000 | Loss: 0.00005639
Iteration 299/1000 | Loss: 0.00005639
Iteration 300/1000 | Loss: 0.00005639
Iteration 301/1000 | Loss: 0.00005639
Iteration 302/1000 | Loss: 0.00005639
Iteration 303/1000 | Loss: 0.00005639
Iteration 304/1000 | Loss: 0.00005639
Iteration 305/1000 | Loss: 0.00005639
Iteration 306/1000 | Loss: 0.00005639
Iteration 307/1000 | Loss: 0.00005639
Iteration 308/1000 | Loss: 0.00005639
Iteration 309/1000 | Loss: 0.00005639
Iteration 310/1000 | Loss: 0.00005639
Iteration 311/1000 | Loss: 0.00005639
Iteration 312/1000 | Loss: 0.00005639
Iteration 313/1000 | Loss: 0.00005639
Iteration 314/1000 | Loss: 0.00005639
Iteration 315/1000 | Loss: 0.00005639
Iteration 316/1000 | Loss: 0.00005639
Iteration 317/1000 | Loss: 0.00005639
Iteration 318/1000 | Loss: 0.00005639
Iteration 319/1000 | Loss: 0.00005639
Iteration 320/1000 | Loss: 0.00005639
Iteration 321/1000 | Loss: 0.00005639
Iteration 322/1000 | Loss: 0.00005639
Iteration 323/1000 | Loss: 0.00005639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 323. Stopping optimization.
Last 5 losses: [5.638801303575747e-05, 5.638801303575747e-05, 5.638801303575747e-05, 5.638801303575747e-05, 5.638801303575747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.638801303575747e-05

Optimization complete. Final v2v error: 3.550447702407837 mm

Highest mean error: 13.066459655761719 mm for frame 82

Lowest mean error: 2.3033316135406494 mm for frame 23

Saving results

Total time: 82.04606032371521
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502038
Iteration 2/25 | Loss: 0.00133012
Iteration 3/25 | Loss: 0.00109776
Iteration 4/25 | Loss: 0.00107764
Iteration 5/25 | Loss: 0.00107330
Iteration 6/25 | Loss: 0.00107135
Iteration 7/25 | Loss: 0.00107130
Iteration 8/25 | Loss: 0.00107130
Iteration 9/25 | Loss: 0.00107130
Iteration 10/25 | Loss: 0.00107130
Iteration 11/25 | Loss: 0.00107130
Iteration 12/25 | Loss: 0.00107130
Iteration 13/25 | Loss: 0.00107130
Iteration 14/25 | Loss: 0.00107130
Iteration 15/25 | Loss: 0.00107130
Iteration 16/25 | Loss: 0.00107130
Iteration 17/25 | Loss: 0.00107130
Iteration 18/25 | Loss: 0.00107130
Iteration 19/25 | Loss: 0.00107130
Iteration 20/25 | Loss: 0.00107130
Iteration 21/25 | Loss: 0.00107130
Iteration 22/25 | Loss: 0.00107130
Iteration 23/25 | Loss: 0.00107130
Iteration 24/25 | Loss: 0.00107130
Iteration 25/25 | Loss: 0.00107130

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29867291
Iteration 2/25 | Loss: 0.00077359
Iteration 3/25 | Loss: 0.00077359
Iteration 4/25 | Loss: 0.00077359
Iteration 5/25 | Loss: 0.00077359
Iteration 6/25 | Loss: 0.00077359
Iteration 7/25 | Loss: 0.00077359
Iteration 8/25 | Loss: 0.00077359
Iteration 9/25 | Loss: 0.00077359
Iteration 10/25 | Loss: 0.00077359
Iteration 11/25 | Loss: 0.00077359
Iteration 12/25 | Loss: 0.00077359
Iteration 13/25 | Loss: 0.00077359
Iteration 14/25 | Loss: 0.00077359
Iteration 15/25 | Loss: 0.00077359
Iteration 16/25 | Loss: 0.00077359
Iteration 17/25 | Loss: 0.00077359
Iteration 18/25 | Loss: 0.00077359
Iteration 19/25 | Loss: 0.00077359
Iteration 20/25 | Loss: 0.00077359
Iteration 21/25 | Loss: 0.00077359
Iteration 22/25 | Loss: 0.00077359
Iteration 23/25 | Loss: 0.00077359
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007735873805359006, 0.0007735873805359006, 0.0007735873805359006, 0.0007735873805359006, 0.0007735873805359006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007735873805359006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077359
Iteration 2/1000 | Loss: 0.00006398
Iteration 3/1000 | Loss: 0.00004000
Iteration 4/1000 | Loss: 0.00003627
Iteration 5/1000 | Loss: 0.00003449
Iteration 6/1000 | Loss: 0.00003355
Iteration 7/1000 | Loss: 0.00003298
Iteration 8/1000 | Loss: 0.00003238
Iteration 9/1000 | Loss: 0.00003191
Iteration 10/1000 | Loss: 0.00003164
Iteration 11/1000 | Loss: 0.00003144
Iteration 12/1000 | Loss: 0.00003131
Iteration 13/1000 | Loss: 0.00003111
Iteration 14/1000 | Loss: 0.00003095
Iteration 15/1000 | Loss: 0.00003078
Iteration 16/1000 | Loss: 0.00003065
Iteration 17/1000 | Loss: 0.00003065
Iteration 18/1000 | Loss: 0.00003064
Iteration 19/1000 | Loss: 0.00003057
Iteration 20/1000 | Loss: 0.00003057
Iteration 21/1000 | Loss: 0.00003044
Iteration 22/1000 | Loss: 0.00003042
Iteration 23/1000 | Loss: 0.00003036
Iteration 24/1000 | Loss: 0.00003030
Iteration 25/1000 | Loss: 0.00003022
Iteration 26/1000 | Loss: 0.00003015
Iteration 27/1000 | Loss: 0.00003012
Iteration 28/1000 | Loss: 0.00003012
Iteration 29/1000 | Loss: 0.00003012
Iteration 30/1000 | Loss: 0.00003011
Iteration 31/1000 | Loss: 0.00003011
Iteration 32/1000 | Loss: 0.00003008
Iteration 33/1000 | Loss: 0.00003007
Iteration 34/1000 | Loss: 0.00003007
Iteration 35/1000 | Loss: 0.00003006
Iteration 36/1000 | Loss: 0.00003005
Iteration 37/1000 | Loss: 0.00003001
Iteration 38/1000 | Loss: 0.00003001
Iteration 39/1000 | Loss: 0.00003000
Iteration 40/1000 | Loss: 0.00002999
Iteration 41/1000 | Loss: 0.00002997
Iteration 42/1000 | Loss: 0.00002995
Iteration 43/1000 | Loss: 0.00002995
Iteration 44/1000 | Loss: 0.00002995
Iteration 45/1000 | Loss: 0.00002995
Iteration 46/1000 | Loss: 0.00002995
Iteration 47/1000 | Loss: 0.00002995
Iteration 48/1000 | Loss: 0.00002995
Iteration 49/1000 | Loss: 0.00002994
Iteration 50/1000 | Loss: 0.00002994
Iteration 51/1000 | Loss: 0.00002992
Iteration 52/1000 | Loss: 0.00002992
Iteration 53/1000 | Loss: 0.00002992
Iteration 54/1000 | Loss: 0.00002992
Iteration 55/1000 | Loss: 0.00002992
Iteration 56/1000 | Loss: 0.00002992
Iteration 57/1000 | Loss: 0.00002992
Iteration 58/1000 | Loss: 0.00002991
Iteration 59/1000 | Loss: 0.00002991
Iteration 60/1000 | Loss: 0.00002991
Iteration 61/1000 | Loss: 0.00002991
Iteration 62/1000 | Loss: 0.00002991
Iteration 63/1000 | Loss: 0.00002991
Iteration 64/1000 | Loss: 0.00002991
Iteration 65/1000 | Loss: 0.00002991
Iteration 66/1000 | Loss: 0.00002991
Iteration 67/1000 | Loss: 0.00002991
Iteration 68/1000 | Loss: 0.00002991
Iteration 69/1000 | Loss: 0.00002991
Iteration 70/1000 | Loss: 0.00002991
Iteration 71/1000 | Loss: 0.00002990
Iteration 72/1000 | Loss: 0.00002990
Iteration 73/1000 | Loss: 0.00002990
Iteration 74/1000 | Loss: 0.00002989
Iteration 75/1000 | Loss: 0.00002989
Iteration 76/1000 | Loss: 0.00002988
Iteration 77/1000 | Loss: 0.00002988
Iteration 78/1000 | Loss: 0.00002988
Iteration 79/1000 | Loss: 0.00002988
Iteration 80/1000 | Loss: 0.00002988
Iteration 81/1000 | Loss: 0.00002988
Iteration 82/1000 | Loss: 0.00002988
Iteration 83/1000 | Loss: 0.00002988
Iteration 84/1000 | Loss: 0.00002987
Iteration 85/1000 | Loss: 0.00002987
Iteration 86/1000 | Loss: 0.00002987
Iteration 87/1000 | Loss: 0.00002987
Iteration 88/1000 | Loss: 0.00002986
Iteration 89/1000 | Loss: 0.00002986
Iteration 90/1000 | Loss: 0.00002986
Iteration 91/1000 | Loss: 0.00002986
Iteration 92/1000 | Loss: 0.00002986
Iteration 93/1000 | Loss: 0.00002986
Iteration 94/1000 | Loss: 0.00002986
Iteration 95/1000 | Loss: 0.00002986
Iteration 96/1000 | Loss: 0.00002986
Iteration 97/1000 | Loss: 0.00002985
Iteration 98/1000 | Loss: 0.00002985
Iteration 99/1000 | Loss: 0.00002985
Iteration 100/1000 | Loss: 0.00002985
Iteration 101/1000 | Loss: 0.00002985
Iteration 102/1000 | Loss: 0.00002985
Iteration 103/1000 | Loss: 0.00002985
Iteration 104/1000 | Loss: 0.00002985
Iteration 105/1000 | Loss: 0.00002985
Iteration 106/1000 | Loss: 0.00002984
Iteration 107/1000 | Loss: 0.00002984
Iteration 108/1000 | Loss: 0.00002984
Iteration 109/1000 | Loss: 0.00002984
Iteration 110/1000 | Loss: 0.00002984
Iteration 111/1000 | Loss: 0.00002984
Iteration 112/1000 | Loss: 0.00002984
Iteration 113/1000 | Loss: 0.00002984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.984488855872769e-05, 2.984488855872769e-05, 2.984488855872769e-05, 2.984488855872769e-05, 2.984488855872769e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.984488855872769e-05

Optimization complete. Final v2v error: 4.428501129150391 mm

Highest mean error: 5.372714519500732 mm for frame 179

Lowest mean error: 3.687905788421631 mm for frame 6

Saving results

Total time: 53.19475340843201
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568405
Iteration 2/25 | Loss: 0.00129952
Iteration 3/25 | Loss: 0.00110174
Iteration 4/25 | Loss: 0.00108509
Iteration 5/25 | Loss: 0.00108258
Iteration 6/25 | Loss: 0.00108213
Iteration 7/25 | Loss: 0.00108213
Iteration 8/25 | Loss: 0.00108213
Iteration 9/25 | Loss: 0.00108213
Iteration 10/25 | Loss: 0.00108213
Iteration 11/25 | Loss: 0.00108213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001082127564586699, 0.001082127564586699, 0.001082127564586699, 0.001082127564586699, 0.001082127564586699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001082127564586699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39703441
Iteration 2/25 | Loss: 0.00056443
Iteration 3/25 | Loss: 0.00056434
Iteration 4/25 | Loss: 0.00056434
Iteration 5/25 | Loss: 0.00056434
Iteration 6/25 | Loss: 0.00056434
Iteration 7/25 | Loss: 0.00056434
Iteration 8/25 | Loss: 0.00056434
Iteration 9/25 | Loss: 0.00056434
Iteration 10/25 | Loss: 0.00056434
Iteration 11/25 | Loss: 0.00056434
Iteration 12/25 | Loss: 0.00056434
Iteration 13/25 | Loss: 0.00056434
Iteration 14/25 | Loss: 0.00056434
Iteration 15/25 | Loss: 0.00056434
Iteration 16/25 | Loss: 0.00056434
Iteration 17/25 | Loss: 0.00056434
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005643378244712949, 0.0005643378244712949, 0.0005643378244712949, 0.0005643378244712949, 0.0005643378244712949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005643378244712949

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056434
Iteration 2/1000 | Loss: 0.00005577
Iteration 3/1000 | Loss: 0.00002639
Iteration 4/1000 | Loss: 0.00002173
Iteration 5/1000 | Loss: 0.00001949
Iteration 6/1000 | Loss: 0.00001883
Iteration 7/1000 | Loss: 0.00001834
Iteration 8/1000 | Loss: 0.00001798
Iteration 9/1000 | Loss: 0.00001762
Iteration 10/1000 | Loss: 0.00001738
Iteration 11/1000 | Loss: 0.00001707
Iteration 12/1000 | Loss: 0.00001691
Iteration 13/1000 | Loss: 0.00001679
Iteration 14/1000 | Loss: 0.00001679
Iteration 15/1000 | Loss: 0.00001669
Iteration 16/1000 | Loss: 0.00001666
Iteration 17/1000 | Loss: 0.00001664
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001662
Iteration 20/1000 | Loss: 0.00001660
Iteration 21/1000 | Loss: 0.00001659
Iteration 22/1000 | Loss: 0.00001657
Iteration 23/1000 | Loss: 0.00001654
Iteration 24/1000 | Loss: 0.00001653
Iteration 25/1000 | Loss: 0.00001653
Iteration 26/1000 | Loss: 0.00001653
Iteration 27/1000 | Loss: 0.00001652
Iteration 28/1000 | Loss: 0.00001651
Iteration 29/1000 | Loss: 0.00001651
Iteration 30/1000 | Loss: 0.00001651
Iteration 31/1000 | Loss: 0.00001651
Iteration 32/1000 | Loss: 0.00001650
Iteration 33/1000 | Loss: 0.00001650
Iteration 34/1000 | Loss: 0.00001650
Iteration 35/1000 | Loss: 0.00001650
Iteration 36/1000 | Loss: 0.00001649
Iteration 37/1000 | Loss: 0.00001649
Iteration 38/1000 | Loss: 0.00001649
Iteration 39/1000 | Loss: 0.00001649
Iteration 40/1000 | Loss: 0.00001649
Iteration 41/1000 | Loss: 0.00001648
Iteration 42/1000 | Loss: 0.00001648
Iteration 43/1000 | Loss: 0.00001648
Iteration 44/1000 | Loss: 0.00001648
Iteration 45/1000 | Loss: 0.00001647
Iteration 46/1000 | Loss: 0.00001647
Iteration 47/1000 | Loss: 0.00001647
Iteration 48/1000 | Loss: 0.00001647
Iteration 49/1000 | Loss: 0.00001647
Iteration 50/1000 | Loss: 0.00001647
Iteration 51/1000 | Loss: 0.00001646
Iteration 52/1000 | Loss: 0.00001646
Iteration 53/1000 | Loss: 0.00001646
Iteration 54/1000 | Loss: 0.00001645
Iteration 55/1000 | Loss: 0.00001645
Iteration 56/1000 | Loss: 0.00001644
Iteration 57/1000 | Loss: 0.00001644
Iteration 58/1000 | Loss: 0.00001643
Iteration 59/1000 | Loss: 0.00001643
Iteration 60/1000 | Loss: 0.00001643
Iteration 61/1000 | Loss: 0.00001642
Iteration 62/1000 | Loss: 0.00001642
Iteration 63/1000 | Loss: 0.00001642
Iteration 64/1000 | Loss: 0.00001642
Iteration 65/1000 | Loss: 0.00001642
Iteration 66/1000 | Loss: 0.00001642
Iteration 67/1000 | Loss: 0.00001642
Iteration 68/1000 | Loss: 0.00001642
Iteration 69/1000 | Loss: 0.00001641
Iteration 70/1000 | Loss: 0.00001641
Iteration 71/1000 | Loss: 0.00001641
Iteration 72/1000 | Loss: 0.00001641
Iteration 73/1000 | Loss: 0.00001641
Iteration 74/1000 | Loss: 0.00001641
Iteration 75/1000 | Loss: 0.00001640
Iteration 76/1000 | Loss: 0.00001640
Iteration 77/1000 | Loss: 0.00001640
Iteration 78/1000 | Loss: 0.00001639
Iteration 79/1000 | Loss: 0.00001639
Iteration 80/1000 | Loss: 0.00001639
Iteration 81/1000 | Loss: 0.00001639
Iteration 82/1000 | Loss: 0.00001638
Iteration 83/1000 | Loss: 0.00001638
Iteration 84/1000 | Loss: 0.00001638
Iteration 85/1000 | Loss: 0.00001638
Iteration 86/1000 | Loss: 0.00001638
Iteration 87/1000 | Loss: 0.00001638
Iteration 88/1000 | Loss: 0.00001637
Iteration 89/1000 | Loss: 0.00001637
Iteration 90/1000 | Loss: 0.00001637
Iteration 91/1000 | Loss: 0.00001637
Iteration 92/1000 | Loss: 0.00001637
Iteration 93/1000 | Loss: 0.00001637
Iteration 94/1000 | Loss: 0.00001637
Iteration 95/1000 | Loss: 0.00001637
Iteration 96/1000 | Loss: 0.00001637
Iteration 97/1000 | Loss: 0.00001637
Iteration 98/1000 | Loss: 0.00001637
Iteration 99/1000 | Loss: 0.00001636
Iteration 100/1000 | Loss: 0.00001636
Iteration 101/1000 | Loss: 0.00001636
Iteration 102/1000 | Loss: 0.00001636
Iteration 103/1000 | Loss: 0.00001635
Iteration 104/1000 | Loss: 0.00001635
Iteration 105/1000 | Loss: 0.00001635
Iteration 106/1000 | Loss: 0.00001635
Iteration 107/1000 | Loss: 0.00001634
Iteration 108/1000 | Loss: 0.00001634
Iteration 109/1000 | Loss: 0.00001634
Iteration 110/1000 | Loss: 0.00001633
Iteration 111/1000 | Loss: 0.00001633
Iteration 112/1000 | Loss: 0.00001633
Iteration 113/1000 | Loss: 0.00001633
Iteration 114/1000 | Loss: 0.00001633
Iteration 115/1000 | Loss: 0.00001633
Iteration 116/1000 | Loss: 0.00001633
Iteration 117/1000 | Loss: 0.00001633
Iteration 118/1000 | Loss: 0.00001633
Iteration 119/1000 | Loss: 0.00001633
Iteration 120/1000 | Loss: 0.00001632
Iteration 121/1000 | Loss: 0.00001632
Iteration 122/1000 | Loss: 0.00001632
Iteration 123/1000 | Loss: 0.00001632
Iteration 124/1000 | Loss: 0.00001632
Iteration 125/1000 | Loss: 0.00001631
Iteration 126/1000 | Loss: 0.00001631
Iteration 127/1000 | Loss: 0.00001631
Iteration 128/1000 | Loss: 0.00001631
Iteration 129/1000 | Loss: 0.00001631
Iteration 130/1000 | Loss: 0.00001631
Iteration 131/1000 | Loss: 0.00001631
Iteration 132/1000 | Loss: 0.00001631
Iteration 133/1000 | Loss: 0.00001631
Iteration 134/1000 | Loss: 0.00001631
Iteration 135/1000 | Loss: 0.00001631
Iteration 136/1000 | Loss: 0.00001631
Iteration 137/1000 | Loss: 0.00001631
Iteration 138/1000 | Loss: 0.00001630
Iteration 139/1000 | Loss: 0.00001630
Iteration 140/1000 | Loss: 0.00001630
Iteration 141/1000 | Loss: 0.00001630
Iteration 142/1000 | Loss: 0.00001630
Iteration 143/1000 | Loss: 0.00001630
Iteration 144/1000 | Loss: 0.00001629
Iteration 145/1000 | Loss: 0.00001629
Iteration 146/1000 | Loss: 0.00001629
Iteration 147/1000 | Loss: 0.00001629
Iteration 148/1000 | Loss: 0.00001629
Iteration 149/1000 | Loss: 0.00001629
Iteration 150/1000 | Loss: 0.00001629
Iteration 151/1000 | Loss: 0.00001629
Iteration 152/1000 | Loss: 0.00001629
Iteration 153/1000 | Loss: 0.00001629
Iteration 154/1000 | Loss: 0.00001629
Iteration 155/1000 | Loss: 0.00001629
Iteration 156/1000 | Loss: 0.00001629
Iteration 157/1000 | Loss: 0.00001629
Iteration 158/1000 | Loss: 0.00001629
Iteration 159/1000 | Loss: 0.00001628
Iteration 160/1000 | Loss: 0.00001628
Iteration 161/1000 | Loss: 0.00001628
Iteration 162/1000 | Loss: 0.00001628
Iteration 163/1000 | Loss: 0.00001628
Iteration 164/1000 | Loss: 0.00001628
Iteration 165/1000 | Loss: 0.00001628
Iteration 166/1000 | Loss: 0.00001628
Iteration 167/1000 | Loss: 0.00001628
Iteration 168/1000 | Loss: 0.00001628
Iteration 169/1000 | Loss: 0.00001628
Iteration 170/1000 | Loss: 0.00001628
Iteration 171/1000 | Loss: 0.00001628
Iteration 172/1000 | Loss: 0.00001628
Iteration 173/1000 | Loss: 0.00001628
Iteration 174/1000 | Loss: 0.00001628
Iteration 175/1000 | Loss: 0.00001628
Iteration 176/1000 | Loss: 0.00001628
Iteration 177/1000 | Loss: 0.00001628
Iteration 178/1000 | Loss: 0.00001628
Iteration 179/1000 | Loss: 0.00001628
Iteration 180/1000 | Loss: 0.00001628
Iteration 181/1000 | Loss: 0.00001628
Iteration 182/1000 | Loss: 0.00001628
Iteration 183/1000 | Loss: 0.00001628
Iteration 184/1000 | Loss: 0.00001628
Iteration 185/1000 | Loss: 0.00001628
Iteration 186/1000 | Loss: 0.00001628
Iteration 187/1000 | Loss: 0.00001628
Iteration 188/1000 | Loss: 0.00001628
Iteration 189/1000 | Loss: 0.00001628
Iteration 190/1000 | Loss: 0.00001628
Iteration 191/1000 | Loss: 0.00001628
Iteration 192/1000 | Loss: 0.00001628
Iteration 193/1000 | Loss: 0.00001628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.627757228561677e-05, 1.627757228561677e-05, 1.627757228561677e-05, 1.627757228561677e-05, 1.627757228561677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.627757228561677e-05

Optimization complete. Final v2v error: 3.450178384780884 mm

Highest mean error: 5.094674587249756 mm for frame 192

Lowest mean error: 2.876723527908325 mm for frame 35

Saving results

Total time: 46.33331751823425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815853
Iteration 2/25 | Loss: 0.00127287
Iteration 3/25 | Loss: 0.00112498
Iteration 4/25 | Loss: 0.00111123
Iteration 5/25 | Loss: 0.00110859
Iteration 6/25 | Loss: 0.00110770
Iteration 7/25 | Loss: 0.00110768
Iteration 8/25 | Loss: 0.00110768
Iteration 9/25 | Loss: 0.00110768
Iteration 10/25 | Loss: 0.00110768
Iteration 11/25 | Loss: 0.00110768
Iteration 12/25 | Loss: 0.00110768
Iteration 13/25 | Loss: 0.00110768
Iteration 14/25 | Loss: 0.00110768
Iteration 15/25 | Loss: 0.00110768
Iteration 16/25 | Loss: 0.00110768
Iteration 17/25 | Loss: 0.00110768
Iteration 18/25 | Loss: 0.00110768
Iteration 19/25 | Loss: 0.00110768
Iteration 20/25 | Loss: 0.00110768
Iteration 21/25 | Loss: 0.00110768
Iteration 22/25 | Loss: 0.00110768
Iteration 23/25 | Loss: 0.00110768
Iteration 24/25 | Loss: 0.00110768
Iteration 25/25 | Loss: 0.00110768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09164321
Iteration 2/25 | Loss: 0.00058019
Iteration 3/25 | Loss: 0.00058015
Iteration 4/25 | Loss: 0.00058015
Iteration 5/25 | Loss: 0.00058015
Iteration 6/25 | Loss: 0.00058015
Iteration 7/25 | Loss: 0.00058015
Iteration 8/25 | Loss: 0.00058015
Iteration 9/25 | Loss: 0.00058015
Iteration 10/25 | Loss: 0.00058015
Iteration 11/25 | Loss: 0.00058015
Iteration 12/25 | Loss: 0.00058015
Iteration 13/25 | Loss: 0.00058015
Iteration 14/25 | Loss: 0.00058015
Iteration 15/25 | Loss: 0.00058015
Iteration 16/25 | Loss: 0.00058015
Iteration 17/25 | Loss: 0.00058015
Iteration 18/25 | Loss: 0.00058015
Iteration 19/25 | Loss: 0.00058015
Iteration 20/25 | Loss: 0.00058015
Iteration 21/25 | Loss: 0.00058015
Iteration 22/25 | Loss: 0.00058015
Iteration 23/25 | Loss: 0.00058015
Iteration 24/25 | Loss: 0.00058015
Iteration 25/25 | Loss: 0.00058015

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058015
Iteration 2/1000 | Loss: 0.00005177
Iteration 3/1000 | Loss: 0.00003155
Iteration 4/1000 | Loss: 0.00002772
Iteration 5/1000 | Loss: 0.00002622
Iteration 6/1000 | Loss: 0.00002545
Iteration 7/1000 | Loss: 0.00002504
Iteration 8/1000 | Loss: 0.00002459
Iteration 9/1000 | Loss: 0.00002424
Iteration 10/1000 | Loss: 0.00002396
Iteration 11/1000 | Loss: 0.00002380
Iteration 12/1000 | Loss: 0.00002366
Iteration 13/1000 | Loss: 0.00002357
Iteration 14/1000 | Loss: 0.00002354
Iteration 15/1000 | Loss: 0.00002354
Iteration 16/1000 | Loss: 0.00002348
Iteration 17/1000 | Loss: 0.00002348
Iteration 18/1000 | Loss: 0.00002347
Iteration 19/1000 | Loss: 0.00002346
Iteration 20/1000 | Loss: 0.00002345
Iteration 21/1000 | Loss: 0.00002345
Iteration 22/1000 | Loss: 0.00002344
Iteration 23/1000 | Loss: 0.00002344
Iteration 24/1000 | Loss: 0.00002341
Iteration 25/1000 | Loss: 0.00002337
Iteration 26/1000 | Loss: 0.00002336
Iteration 27/1000 | Loss: 0.00002336
Iteration 28/1000 | Loss: 0.00002336
Iteration 29/1000 | Loss: 0.00002335
Iteration 30/1000 | Loss: 0.00002335
Iteration 31/1000 | Loss: 0.00002335
Iteration 32/1000 | Loss: 0.00002334
Iteration 33/1000 | Loss: 0.00002334
Iteration 34/1000 | Loss: 0.00002334
Iteration 35/1000 | Loss: 0.00002332
Iteration 36/1000 | Loss: 0.00002332
Iteration 37/1000 | Loss: 0.00002332
Iteration 38/1000 | Loss: 0.00002332
Iteration 39/1000 | Loss: 0.00002332
Iteration 40/1000 | Loss: 0.00002331
Iteration 41/1000 | Loss: 0.00002331
Iteration 42/1000 | Loss: 0.00002331
Iteration 43/1000 | Loss: 0.00002331
Iteration 44/1000 | Loss: 0.00002331
Iteration 45/1000 | Loss: 0.00002331
Iteration 46/1000 | Loss: 0.00002330
Iteration 47/1000 | Loss: 0.00002330
Iteration 48/1000 | Loss: 0.00002330
Iteration 49/1000 | Loss: 0.00002329
Iteration 50/1000 | Loss: 0.00002329
Iteration 51/1000 | Loss: 0.00002328
Iteration 52/1000 | Loss: 0.00002328
Iteration 53/1000 | Loss: 0.00002327
Iteration 54/1000 | Loss: 0.00002327
Iteration 55/1000 | Loss: 0.00002327
Iteration 56/1000 | Loss: 0.00002327
Iteration 57/1000 | Loss: 0.00002326
Iteration 58/1000 | Loss: 0.00002326
Iteration 59/1000 | Loss: 0.00002326
Iteration 60/1000 | Loss: 0.00002326
Iteration 61/1000 | Loss: 0.00002326
Iteration 62/1000 | Loss: 0.00002326
Iteration 63/1000 | Loss: 0.00002325
Iteration 64/1000 | Loss: 0.00002325
Iteration 65/1000 | Loss: 0.00002325
Iteration 66/1000 | Loss: 0.00002325
Iteration 67/1000 | Loss: 0.00002325
Iteration 68/1000 | Loss: 0.00002324
Iteration 69/1000 | Loss: 0.00002324
Iteration 70/1000 | Loss: 0.00002324
Iteration 71/1000 | Loss: 0.00002324
Iteration 72/1000 | Loss: 0.00002324
Iteration 73/1000 | Loss: 0.00002324
Iteration 74/1000 | Loss: 0.00002324
Iteration 75/1000 | Loss: 0.00002324
Iteration 76/1000 | Loss: 0.00002324
Iteration 77/1000 | Loss: 0.00002324
Iteration 78/1000 | Loss: 0.00002324
Iteration 79/1000 | Loss: 0.00002324
Iteration 80/1000 | Loss: 0.00002324
Iteration 81/1000 | Loss: 0.00002323
Iteration 82/1000 | Loss: 0.00002323
Iteration 83/1000 | Loss: 0.00002323
Iteration 84/1000 | Loss: 0.00002323
Iteration 85/1000 | Loss: 0.00002323
Iteration 86/1000 | Loss: 0.00002323
Iteration 87/1000 | Loss: 0.00002323
Iteration 88/1000 | Loss: 0.00002323
Iteration 89/1000 | Loss: 0.00002323
Iteration 90/1000 | Loss: 0.00002323
Iteration 91/1000 | Loss: 0.00002323
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002323
Iteration 94/1000 | Loss: 0.00002323
Iteration 95/1000 | Loss: 0.00002323
Iteration 96/1000 | Loss: 0.00002323
Iteration 97/1000 | Loss: 0.00002323
Iteration 98/1000 | Loss: 0.00002322
Iteration 99/1000 | Loss: 0.00002322
Iteration 100/1000 | Loss: 0.00002322
Iteration 101/1000 | Loss: 0.00002322
Iteration 102/1000 | Loss: 0.00002322
Iteration 103/1000 | Loss: 0.00002322
Iteration 104/1000 | Loss: 0.00002322
Iteration 105/1000 | Loss: 0.00002322
Iteration 106/1000 | Loss: 0.00002322
Iteration 107/1000 | Loss: 0.00002322
Iteration 108/1000 | Loss: 0.00002322
Iteration 109/1000 | Loss: 0.00002322
Iteration 110/1000 | Loss: 0.00002322
Iteration 111/1000 | Loss: 0.00002322
Iteration 112/1000 | Loss: 0.00002322
Iteration 113/1000 | Loss: 0.00002321
Iteration 114/1000 | Loss: 0.00002321
Iteration 115/1000 | Loss: 0.00002321
Iteration 116/1000 | Loss: 0.00002321
Iteration 117/1000 | Loss: 0.00002321
Iteration 118/1000 | Loss: 0.00002321
Iteration 119/1000 | Loss: 0.00002321
Iteration 120/1000 | Loss: 0.00002321
Iteration 121/1000 | Loss: 0.00002321
Iteration 122/1000 | Loss: 0.00002321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.321170541108586e-05, 2.321170541108586e-05, 2.321170541108586e-05, 2.321170541108586e-05, 2.321170541108586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.321170541108586e-05

Optimization complete. Final v2v error: 4.055895805358887 mm

Highest mean error: 4.446147918701172 mm for frame 40

Lowest mean error: 3.620741367340088 mm for frame 21

Saving results

Total time: 35.97047019004822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01141378
Iteration 2/25 | Loss: 0.00367156
Iteration 3/25 | Loss: 0.00287176
Iteration 4/25 | Loss: 0.00277561
Iteration 5/25 | Loss: 0.00247396
Iteration 6/25 | Loss: 0.00223808
Iteration 7/25 | Loss: 0.00199704
Iteration 8/25 | Loss: 0.00185036
Iteration 9/25 | Loss: 0.00175288
Iteration 10/25 | Loss: 0.00171388
Iteration 11/25 | Loss: 0.00168051
Iteration 12/25 | Loss: 0.00164108
Iteration 13/25 | Loss: 0.00162236
Iteration 14/25 | Loss: 0.00158429
Iteration 15/25 | Loss: 0.00157484
Iteration 16/25 | Loss: 0.00156226
Iteration 17/25 | Loss: 0.00156344
Iteration 18/25 | Loss: 0.00155386
Iteration 19/25 | Loss: 0.00152860
Iteration 20/25 | Loss: 0.00152841
Iteration 21/25 | Loss: 0.00151810
Iteration 22/25 | Loss: 0.00150462
Iteration 23/25 | Loss: 0.00150078
Iteration 24/25 | Loss: 0.00150159
Iteration 25/25 | Loss: 0.00149546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11613333
Iteration 2/25 | Loss: 0.00531350
Iteration 3/25 | Loss: 0.00531350
Iteration 4/25 | Loss: 0.00531350
Iteration 5/25 | Loss: 0.00527798
Iteration 6/25 | Loss: 0.00527798
Iteration 7/25 | Loss: 0.00527798
Iteration 8/25 | Loss: 0.00527798
Iteration 9/25 | Loss: 0.00527798
Iteration 10/25 | Loss: 0.00527798
Iteration 11/25 | Loss: 0.00527798
Iteration 12/25 | Loss: 0.00527798
Iteration 13/25 | Loss: 0.00527798
Iteration 14/25 | Loss: 0.00527798
Iteration 15/25 | Loss: 0.00527798
Iteration 16/25 | Loss: 0.00527798
Iteration 17/25 | Loss: 0.00527798
Iteration 18/25 | Loss: 0.00527798
Iteration 19/25 | Loss: 0.00527798
Iteration 20/25 | Loss: 0.00527798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.005277975928038359, 0.005277975928038359, 0.005277975928038359, 0.005277975928038359, 0.005277975928038359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005277975928038359

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00527798
Iteration 2/1000 | Loss: 0.00116309
Iteration 3/1000 | Loss: 0.00074427
Iteration 4/1000 | Loss: 0.00068673
Iteration 5/1000 | Loss: 0.00096360
Iteration 6/1000 | Loss: 0.00054576
Iteration 7/1000 | Loss: 0.00058883
Iteration 8/1000 | Loss: 0.00045619
Iteration 9/1000 | Loss: 0.00056714
Iteration 10/1000 | Loss: 0.00062125
Iteration 11/1000 | Loss: 0.00058645
Iteration 12/1000 | Loss: 0.00046980
Iteration 13/1000 | Loss: 0.00037739
Iteration 14/1000 | Loss: 0.00035052
Iteration 15/1000 | Loss: 0.00025341
Iteration 16/1000 | Loss: 0.00042266
Iteration 17/1000 | Loss: 0.00030601
Iteration 18/1000 | Loss: 0.00032231
Iteration 19/1000 | Loss: 0.00031557
Iteration 20/1000 | Loss: 0.00027345
Iteration 21/1000 | Loss: 0.00059198
Iteration 22/1000 | Loss: 0.00199810
Iteration 23/1000 | Loss: 0.00200405
Iteration 24/1000 | Loss: 0.00187050
Iteration 25/1000 | Loss: 0.00096336
Iteration 26/1000 | Loss: 0.00041893
Iteration 27/1000 | Loss: 0.00056218
Iteration 28/1000 | Loss: 0.00031466
Iteration 29/1000 | Loss: 0.00064212
Iteration 30/1000 | Loss: 0.00089189
Iteration 31/1000 | Loss: 0.00153480
Iteration 32/1000 | Loss: 0.00145912
Iteration 33/1000 | Loss: 0.00130010
Iteration 34/1000 | Loss: 0.00167668
Iteration 35/1000 | Loss: 0.00122593
Iteration 36/1000 | Loss: 0.00098379
Iteration 37/1000 | Loss: 0.00092874
Iteration 38/1000 | Loss: 0.00113014
Iteration 39/1000 | Loss: 0.00059699
Iteration 40/1000 | Loss: 0.00041098
Iteration 41/1000 | Loss: 0.00096785
Iteration 42/1000 | Loss: 0.00067084
Iteration 43/1000 | Loss: 0.00027879
Iteration 44/1000 | Loss: 0.00048114
Iteration 45/1000 | Loss: 0.00042440
Iteration 46/1000 | Loss: 0.00035162
Iteration 47/1000 | Loss: 0.00028782
Iteration 48/1000 | Loss: 0.00038701
Iteration 49/1000 | Loss: 0.00023379
Iteration 50/1000 | Loss: 0.00029547
Iteration 51/1000 | Loss: 0.00071810
Iteration 52/1000 | Loss: 0.00029619
Iteration 53/1000 | Loss: 0.00054855
Iteration 54/1000 | Loss: 0.00086411
Iteration 55/1000 | Loss: 0.00038869
Iteration 56/1000 | Loss: 0.00037794
Iteration 57/1000 | Loss: 0.00022009
Iteration 58/1000 | Loss: 0.00030064
Iteration 59/1000 | Loss: 0.00058126
Iteration 60/1000 | Loss: 0.00016354
Iteration 61/1000 | Loss: 0.00029783
Iteration 62/1000 | Loss: 0.00026928
Iteration 63/1000 | Loss: 0.00022937
Iteration 64/1000 | Loss: 0.00094690
Iteration 65/1000 | Loss: 0.00146422
Iteration 66/1000 | Loss: 0.00140771
Iteration 67/1000 | Loss: 0.00260704
Iteration 68/1000 | Loss: 0.00103135
Iteration 69/1000 | Loss: 0.00056037
Iteration 70/1000 | Loss: 0.00037548
Iteration 71/1000 | Loss: 0.00019246
Iteration 72/1000 | Loss: 0.00015640
Iteration 73/1000 | Loss: 0.00062021
Iteration 74/1000 | Loss: 0.00014582
Iteration 75/1000 | Loss: 0.00016286
Iteration 76/1000 | Loss: 0.00014171
Iteration 77/1000 | Loss: 0.00013946
Iteration 78/1000 | Loss: 0.00015220
Iteration 79/1000 | Loss: 0.00051038
Iteration 80/1000 | Loss: 0.00051550
Iteration 81/1000 | Loss: 0.00010766
Iteration 82/1000 | Loss: 0.00057177
Iteration 83/1000 | Loss: 0.00014414
Iteration 84/1000 | Loss: 0.00014663
Iteration 85/1000 | Loss: 0.00065232
Iteration 86/1000 | Loss: 0.00088022
Iteration 87/1000 | Loss: 0.00102893
Iteration 88/1000 | Loss: 0.00045997
Iteration 89/1000 | Loss: 0.00037123
Iteration 90/1000 | Loss: 0.00089498
Iteration 91/1000 | Loss: 0.00128828
Iteration 92/1000 | Loss: 0.00114164
Iteration 93/1000 | Loss: 0.00049014
Iteration 94/1000 | Loss: 0.00063990
Iteration 95/1000 | Loss: 0.00052808
Iteration 96/1000 | Loss: 0.00041921
Iteration 97/1000 | Loss: 0.00018310
Iteration 98/1000 | Loss: 0.00032607
Iteration 99/1000 | Loss: 0.00036507
Iteration 100/1000 | Loss: 0.00058217
Iteration 101/1000 | Loss: 0.00036735
Iteration 102/1000 | Loss: 0.00039363
Iteration 103/1000 | Loss: 0.00033920
Iteration 104/1000 | Loss: 0.00014557
Iteration 105/1000 | Loss: 0.00009240
Iteration 106/1000 | Loss: 0.00009818
Iteration 107/1000 | Loss: 0.00009205
Iteration 108/1000 | Loss: 0.00008090
Iteration 109/1000 | Loss: 0.00008285
Iteration 110/1000 | Loss: 0.00008190
Iteration 111/1000 | Loss: 0.00009058
Iteration 112/1000 | Loss: 0.00007576
Iteration 113/1000 | Loss: 0.00009105
Iteration 114/1000 | Loss: 0.00111658
Iteration 115/1000 | Loss: 0.00078562
Iteration 116/1000 | Loss: 0.00056038
Iteration 117/1000 | Loss: 0.00046774
Iteration 118/1000 | Loss: 0.00021857
Iteration 119/1000 | Loss: 0.00014547
Iteration 120/1000 | Loss: 0.00007247
Iteration 121/1000 | Loss: 0.00006404
Iteration 122/1000 | Loss: 0.00006017
Iteration 123/1000 | Loss: 0.00006384
Iteration 124/1000 | Loss: 0.00005503
Iteration 125/1000 | Loss: 0.00005258
Iteration 126/1000 | Loss: 0.00006805
Iteration 127/1000 | Loss: 0.00006443
Iteration 128/1000 | Loss: 0.00005530
Iteration 129/1000 | Loss: 0.00005861
Iteration 130/1000 | Loss: 0.00005199
Iteration 131/1000 | Loss: 0.00005008
Iteration 132/1000 | Loss: 0.00005749
Iteration 133/1000 | Loss: 0.00005696
Iteration 134/1000 | Loss: 0.00005542
Iteration 135/1000 | Loss: 0.00005659
Iteration 136/1000 | Loss: 0.00005034
Iteration 137/1000 | Loss: 0.00005499
Iteration 138/1000 | Loss: 0.00005562
Iteration 139/1000 | Loss: 0.00005560
Iteration 140/1000 | Loss: 0.00005856
Iteration 141/1000 | Loss: 0.00006296
Iteration 142/1000 | Loss: 0.00005937
Iteration 143/1000 | Loss: 0.00033488
Iteration 144/1000 | Loss: 0.00035041
Iteration 145/1000 | Loss: 0.00008789
Iteration 146/1000 | Loss: 0.00006506
Iteration 147/1000 | Loss: 0.00005320
Iteration 148/1000 | Loss: 0.00004652
Iteration 149/1000 | Loss: 0.00004449
Iteration 150/1000 | Loss: 0.00004332
Iteration 151/1000 | Loss: 0.00004268
Iteration 152/1000 | Loss: 0.00004219
Iteration 153/1000 | Loss: 0.00004193
Iteration 154/1000 | Loss: 0.00004170
Iteration 155/1000 | Loss: 0.00004159
Iteration 156/1000 | Loss: 0.00004153
Iteration 157/1000 | Loss: 0.00004147
Iteration 158/1000 | Loss: 0.00004141
Iteration 159/1000 | Loss: 0.00004133
Iteration 160/1000 | Loss: 0.00004129
Iteration 161/1000 | Loss: 0.00004128
Iteration 162/1000 | Loss: 0.00027189
Iteration 163/1000 | Loss: 0.00004320
Iteration 164/1000 | Loss: 0.00004111
Iteration 165/1000 | Loss: 0.00004024
Iteration 166/1000 | Loss: 0.00003961
Iteration 167/1000 | Loss: 0.00003925
Iteration 168/1000 | Loss: 0.00003905
Iteration 169/1000 | Loss: 0.00003893
Iteration 170/1000 | Loss: 0.00003892
Iteration 171/1000 | Loss: 0.00003891
Iteration 172/1000 | Loss: 0.00003891
Iteration 173/1000 | Loss: 0.00003882
Iteration 174/1000 | Loss: 0.00003882
Iteration 175/1000 | Loss: 0.00003879
Iteration 176/1000 | Loss: 0.00003879
Iteration 177/1000 | Loss: 0.00003879
Iteration 178/1000 | Loss: 0.00003879
Iteration 179/1000 | Loss: 0.00003879
Iteration 180/1000 | Loss: 0.00003879
Iteration 181/1000 | Loss: 0.00003879
Iteration 182/1000 | Loss: 0.00003878
Iteration 183/1000 | Loss: 0.00003878
Iteration 184/1000 | Loss: 0.00003878
Iteration 185/1000 | Loss: 0.00003877
Iteration 186/1000 | Loss: 0.00003877
Iteration 187/1000 | Loss: 0.00003876
Iteration 188/1000 | Loss: 0.00003876
Iteration 189/1000 | Loss: 0.00003876
Iteration 190/1000 | Loss: 0.00003876
Iteration 191/1000 | Loss: 0.00003876
Iteration 192/1000 | Loss: 0.00003876
Iteration 193/1000 | Loss: 0.00003876
Iteration 194/1000 | Loss: 0.00003876
Iteration 195/1000 | Loss: 0.00003876
Iteration 196/1000 | Loss: 0.00003876
Iteration 197/1000 | Loss: 0.00003876
Iteration 198/1000 | Loss: 0.00003876
Iteration 199/1000 | Loss: 0.00003876
Iteration 200/1000 | Loss: 0.00003876
Iteration 201/1000 | Loss: 0.00003876
Iteration 202/1000 | Loss: 0.00003876
Iteration 203/1000 | Loss: 0.00003876
Iteration 204/1000 | Loss: 0.00003876
Iteration 205/1000 | Loss: 0.00003876
Iteration 206/1000 | Loss: 0.00003876
Iteration 207/1000 | Loss: 0.00003876
Iteration 208/1000 | Loss: 0.00003876
Iteration 209/1000 | Loss: 0.00003876
Iteration 210/1000 | Loss: 0.00003876
Iteration 211/1000 | Loss: 0.00003876
Iteration 212/1000 | Loss: 0.00003876
Iteration 213/1000 | Loss: 0.00003876
Iteration 214/1000 | Loss: 0.00003876
Iteration 215/1000 | Loss: 0.00003876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [3.875728725688532e-05, 3.875728725688532e-05, 3.875728725688532e-05, 3.875728725688532e-05, 3.875728725688532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.875728725688532e-05

Optimization complete. Final v2v error: 4.197103500366211 mm

Highest mean error: 13.699865341186523 mm for frame 180

Lowest mean error: 3.4319639205932617 mm for frame 17

Saving results

Total time: 322.30803751945496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874743
Iteration 2/25 | Loss: 0.00164219
Iteration 3/25 | Loss: 0.00114335
Iteration 4/25 | Loss: 0.00110230
Iteration 5/25 | Loss: 0.00109848
Iteration 6/25 | Loss: 0.00109788
Iteration 7/25 | Loss: 0.00109788
Iteration 8/25 | Loss: 0.00109788
Iteration 9/25 | Loss: 0.00109788
Iteration 10/25 | Loss: 0.00109788
Iteration 11/25 | Loss: 0.00109788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001097884145565331, 0.001097884145565331, 0.001097884145565331, 0.001097884145565331, 0.001097884145565331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001097884145565331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19673836
Iteration 2/25 | Loss: 0.00065705
Iteration 3/25 | Loss: 0.00065705
Iteration 4/25 | Loss: 0.00065705
Iteration 5/25 | Loss: 0.00065705
Iteration 6/25 | Loss: 0.00065705
Iteration 7/25 | Loss: 0.00065705
Iteration 8/25 | Loss: 0.00065705
Iteration 9/25 | Loss: 0.00065705
Iteration 10/25 | Loss: 0.00065705
Iteration 11/25 | Loss: 0.00065705
Iteration 12/25 | Loss: 0.00065705
Iteration 13/25 | Loss: 0.00065705
Iteration 14/25 | Loss: 0.00065705
Iteration 15/25 | Loss: 0.00065705
Iteration 16/25 | Loss: 0.00065705
Iteration 17/25 | Loss: 0.00065705
Iteration 18/25 | Loss: 0.00065705
Iteration 19/25 | Loss: 0.00065705
Iteration 20/25 | Loss: 0.00065705
Iteration 21/25 | Loss: 0.00065705
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006570491823367774, 0.0006570491823367774, 0.0006570491823367774, 0.0006570491823367774, 0.0006570491823367774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006570491823367774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065705
Iteration 2/1000 | Loss: 0.00004101
Iteration 3/1000 | Loss: 0.00002578
Iteration 4/1000 | Loss: 0.00002133
Iteration 5/1000 | Loss: 0.00001994
Iteration 6/1000 | Loss: 0.00001938
Iteration 7/1000 | Loss: 0.00001899
Iteration 8/1000 | Loss: 0.00001870
Iteration 9/1000 | Loss: 0.00001837
Iteration 10/1000 | Loss: 0.00001810
Iteration 11/1000 | Loss: 0.00001794
Iteration 12/1000 | Loss: 0.00001790
Iteration 13/1000 | Loss: 0.00001778
Iteration 14/1000 | Loss: 0.00001777
Iteration 15/1000 | Loss: 0.00001770
Iteration 16/1000 | Loss: 0.00001770
Iteration 17/1000 | Loss: 0.00001770
Iteration 18/1000 | Loss: 0.00001769
Iteration 19/1000 | Loss: 0.00001769
Iteration 20/1000 | Loss: 0.00001769
Iteration 21/1000 | Loss: 0.00001769
Iteration 22/1000 | Loss: 0.00001769
Iteration 23/1000 | Loss: 0.00001769
Iteration 24/1000 | Loss: 0.00001769
Iteration 25/1000 | Loss: 0.00001769
Iteration 26/1000 | Loss: 0.00001769
Iteration 27/1000 | Loss: 0.00001769
Iteration 28/1000 | Loss: 0.00001769
Iteration 29/1000 | Loss: 0.00001769
Iteration 30/1000 | Loss: 0.00001769
Iteration 31/1000 | Loss: 0.00001769
Iteration 32/1000 | Loss: 0.00001767
Iteration 33/1000 | Loss: 0.00001767
Iteration 34/1000 | Loss: 0.00001766
Iteration 35/1000 | Loss: 0.00001766
Iteration 36/1000 | Loss: 0.00001765
Iteration 37/1000 | Loss: 0.00001764
Iteration 38/1000 | Loss: 0.00001764
Iteration 39/1000 | Loss: 0.00001763
Iteration 40/1000 | Loss: 0.00001763
Iteration 41/1000 | Loss: 0.00001761
Iteration 42/1000 | Loss: 0.00001761
Iteration 43/1000 | Loss: 0.00001761
Iteration 44/1000 | Loss: 0.00001761
Iteration 45/1000 | Loss: 0.00001761
Iteration 46/1000 | Loss: 0.00001761
Iteration 47/1000 | Loss: 0.00001761
Iteration 48/1000 | Loss: 0.00001761
Iteration 49/1000 | Loss: 0.00001761
Iteration 50/1000 | Loss: 0.00001761
Iteration 51/1000 | Loss: 0.00001761
Iteration 52/1000 | Loss: 0.00001761
Iteration 53/1000 | Loss: 0.00001761
Iteration 54/1000 | Loss: 0.00001761
Iteration 55/1000 | Loss: 0.00001761
Iteration 56/1000 | Loss: 0.00001761
Iteration 57/1000 | Loss: 0.00001761
Iteration 58/1000 | Loss: 0.00001761
Iteration 59/1000 | Loss: 0.00001761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [1.760704799380619e-05, 1.760704799380619e-05, 1.760704799380619e-05, 1.760704799380619e-05, 1.760704799380619e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.760704799380619e-05

Optimization complete. Final v2v error: 3.6358273029327393 mm

Highest mean error: 4.342602729797363 mm for frame 197

Lowest mean error: 3.273622989654541 mm for frame 233

Saving results

Total time: 32.76167869567871
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121439
Iteration 2/25 | Loss: 0.00234578
Iteration 3/25 | Loss: 0.00158867
Iteration 4/25 | Loss: 0.00146873
Iteration 5/25 | Loss: 0.00153689
Iteration 6/25 | Loss: 0.00157453
Iteration 7/25 | Loss: 0.00148666
Iteration 8/25 | Loss: 0.00142013
Iteration 9/25 | Loss: 0.00144415
Iteration 10/25 | Loss: 0.00143520
Iteration 11/25 | Loss: 0.00137176
Iteration 12/25 | Loss: 0.00136915
Iteration 13/25 | Loss: 0.00134528
Iteration 14/25 | Loss: 0.00133479
Iteration 15/25 | Loss: 0.00130895
Iteration 16/25 | Loss: 0.00125470
Iteration 17/25 | Loss: 0.00122709
Iteration 18/25 | Loss: 0.00123205
Iteration 19/25 | Loss: 0.00125942
Iteration 20/25 | Loss: 0.00123836
Iteration 21/25 | Loss: 0.00122726
Iteration 22/25 | Loss: 0.00120154
Iteration 23/25 | Loss: 0.00116666
Iteration 24/25 | Loss: 0.00115147
Iteration 25/25 | Loss: 0.00114937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19203341
Iteration 2/25 | Loss: 0.00307515
Iteration 3/25 | Loss: 0.00258003
Iteration 4/25 | Loss: 0.00258003
Iteration 5/25 | Loss: 0.00258003
Iteration 6/25 | Loss: 0.00258003
Iteration 7/25 | Loss: 0.00258003
Iteration 8/25 | Loss: 0.00258003
Iteration 9/25 | Loss: 0.00258003
Iteration 10/25 | Loss: 0.00258003
Iteration 11/25 | Loss: 0.00258003
Iteration 12/25 | Loss: 0.00258003
Iteration 13/25 | Loss: 0.00258003
Iteration 14/25 | Loss: 0.00258003
Iteration 15/25 | Loss: 0.00258003
Iteration 16/25 | Loss: 0.00258003
Iteration 17/25 | Loss: 0.00258003
Iteration 18/25 | Loss: 0.00258003
Iteration 19/25 | Loss: 0.00258003
Iteration 20/25 | Loss: 0.00258003
Iteration 21/25 | Loss: 0.00258003
Iteration 22/25 | Loss: 0.00258003
Iteration 23/25 | Loss: 0.00258003
Iteration 24/25 | Loss: 0.00258003
Iteration 25/25 | Loss: 0.00258003

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00258003
Iteration 2/1000 | Loss: 0.00106363
Iteration 3/1000 | Loss: 0.00072412
Iteration 4/1000 | Loss: 0.00072435
Iteration 5/1000 | Loss: 0.00082666
Iteration 6/1000 | Loss: 0.00098244
Iteration 7/1000 | Loss: 0.00106143
Iteration 8/1000 | Loss: 0.00094009
Iteration 9/1000 | Loss: 0.00101521
Iteration 10/1000 | Loss: 0.00136074
Iteration 11/1000 | Loss: 0.00074807
Iteration 12/1000 | Loss: 0.00067184
Iteration 13/1000 | Loss: 0.00057248
Iteration 14/1000 | Loss: 0.00043233
Iteration 15/1000 | Loss: 0.00040494
Iteration 16/1000 | Loss: 0.00043447
Iteration 17/1000 | Loss: 0.00051629
Iteration 18/1000 | Loss: 0.00051587
Iteration 19/1000 | Loss: 0.00065461
Iteration 20/1000 | Loss: 0.00043484
Iteration 21/1000 | Loss: 0.00077046
Iteration 22/1000 | Loss: 0.00061893
Iteration 23/1000 | Loss: 0.00035851
Iteration 24/1000 | Loss: 0.00026726
Iteration 25/1000 | Loss: 0.00012189
Iteration 26/1000 | Loss: 0.00009282
Iteration 27/1000 | Loss: 0.00012106
Iteration 28/1000 | Loss: 0.00033012
Iteration 29/1000 | Loss: 0.00057012
Iteration 30/1000 | Loss: 0.00060928
Iteration 31/1000 | Loss: 0.00032827
Iteration 32/1000 | Loss: 0.00059113
Iteration 33/1000 | Loss: 0.00027435
Iteration 34/1000 | Loss: 0.00008430
Iteration 35/1000 | Loss: 0.00029144
Iteration 36/1000 | Loss: 0.00008315
Iteration 37/1000 | Loss: 0.00036711
Iteration 38/1000 | Loss: 0.00121723
Iteration 39/1000 | Loss: 0.00047115
Iteration 40/1000 | Loss: 0.00011649
Iteration 41/1000 | Loss: 0.00033565
Iteration 42/1000 | Loss: 0.00023184
Iteration 43/1000 | Loss: 0.00015010
Iteration 44/1000 | Loss: 0.00020749
Iteration 45/1000 | Loss: 0.00006881
Iteration 46/1000 | Loss: 0.00063935
Iteration 47/1000 | Loss: 0.00013819
Iteration 48/1000 | Loss: 0.00024202
Iteration 49/1000 | Loss: 0.00031863
Iteration 50/1000 | Loss: 0.00029836
Iteration 51/1000 | Loss: 0.00029787
Iteration 52/1000 | Loss: 0.00029701
Iteration 53/1000 | Loss: 0.00049213
Iteration 54/1000 | Loss: 0.00040673
Iteration 55/1000 | Loss: 0.00029454
Iteration 56/1000 | Loss: 0.00026623
Iteration 57/1000 | Loss: 0.00026686
Iteration 58/1000 | Loss: 0.00051759
Iteration 59/1000 | Loss: 0.00047098
Iteration 60/1000 | Loss: 0.00049125
Iteration 61/1000 | Loss: 0.00018830
Iteration 62/1000 | Loss: 0.00045160
Iteration 63/1000 | Loss: 0.00027778
Iteration 64/1000 | Loss: 0.00026789
Iteration 65/1000 | Loss: 0.00025782
Iteration 66/1000 | Loss: 0.00022263
Iteration 67/1000 | Loss: 0.00025637
Iteration 68/1000 | Loss: 0.00019673
Iteration 69/1000 | Loss: 0.00021841
Iteration 70/1000 | Loss: 0.00019644
Iteration 71/1000 | Loss: 0.00024080
Iteration 72/1000 | Loss: 0.00021001
Iteration 73/1000 | Loss: 0.00020842
Iteration 74/1000 | Loss: 0.00018219
Iteration 75/1000 | Loss: 0.00026156
Iteration 76/1000 | Loss: 0.00005622
Iteration 77/1000 | Loss: 0.00004588
Iteration 78/1000 | Loss: 0.00028313
Iteration 79/1000 | Loss: 0.00020939
Iteration 80/1000 | Loss: 0.00004146
Iteration 81/1000 | Loss: 0.00004590
Iteration 82/1000 | Loss: 0.00056253
Iteration 83/1000 | Loss: 0.00016344
Iteration 84/1000 | Loss: 0.00006339
Iteration 85/1000 | Loss: 0.00016495
Iteration 86/1000 | Loss: 0.00015147
Iteration 87/1000 | Loss: 0.00021794
Iteration 88/1000 | Loss: 0.00042267
Iteration 89/1000 | Loss: 0.00028643
Iteration 90/1000 | Loss: 0.00008338
Iteration 91/1000 | Loss: 0.00015786
Iteration 92/1000 | Loss: 0.00062696
Iteration 93/1000 | Loss: 0.00034485
Iteration 94/1000 | Loss: 0.00044517
Iteration 95/1000 | Loss: 0.00039288
Iteration 96/1000 | Loss: 0.00035217
Iteration 97/1000 | Loss: 0.00030331
Iteration 98/1000 | Loss: 0.00010489
Iteration 99/1000 | Loss: 0.00045043
Iteration 100/1000 | Loss: 0.00049936
Iteration 101/1000 | Loss: 0.00008595
Iteration 102/1000 | Loss: 0.00021905
Iteration 103/1000 | Loss: 0.00028373
Iteration 104/1000 | Loss: 0.00041145
Iteration 105/1000 | Loss: 0.00033926
Iteration 106/1000 | Loss: 0.00078700
Iteration 107/1000 | Loss: 0.00089262
Iteration 108/1000 | Loss: 0.00020940
Iteration 109/1000 | Loss: 0.00022619
Iteration 110/1000 | Loss: 0.00005670
Iteration 111/1000 | Loss: 0.00080124
Iteration 112/1000 | Loss: 0.00058959
Iteration 113/1000 | Loss: 0.00004821
Iteration 114/1000 | Loss: 0.00019883
Iteration 115/1000 | Loss: 0.00004708
Iteration 116/1000 | Loss: 0.00048248
Iteration 117/1000 | Loss: 0.00025266
Iteration 118/1000 | Loss: 0.00025987
Iteration 119/1000 | Loss: 0.00025412
Iteration 120/1000 | Loss: 0.00019234
Iteration 121/1000 | Loss: 0.00019212
Iteration 122/1000 | Loss: 0.00003981
Iteration 123/1000 | Loss: 0.00003657
Iteration 124/1000 | Loss: 0.00003301
Iteration 125/1000 | Loss: 0.00003069
Iteration 126/1000 | Loss: 0.00002924
Iteration 127/1000 | Loss: 0.00026410
Iteration 128/1000 | Loss: 0.00018438
Iteration 129/1000 | Loss: 0.00009603
Iteration 130/1000 | Loss: 0.00003705
Iteration 131/1000 | Loss: 0.00003359
Iteration 132/1000 | Loss: 0.00003147
Iteration 133/1000 | Loss: 0.00003002
Iteration 134/1000 | Loss: 0.00033837
Iteration 135/1000 | Loss: 0.00015010
Iteration 136/1000 | Loss: 0.00031333
Iteration 137/1000 | Loss: 0.00030519
Iteration 138/1000 | Loss: 0.00013797
Iteration 139/1000 | Loss: 0.00004286
Iteration 140/1000 | Loss: 0.00003742
Iteration 141/1000 | Loss: 0.00003443
Iteration 142/1000 | Loss: 0.00021716
Iteration 143/1000 | Loss: 0.00029299
Iteration 144/1000 | Loss: 0.00069266
Iteration 145/1000 | Loss: 0.00024862
Iteration 146/1000 | Loss: 0.00004756
Iteration 147/1000 | Loss: 0.00034905
Iteration 148/1000 | Loss: 0.00027061
Iteration 149/1000 | Loss: 0.00006254
Iteration 150/1000 | Loss: 0.00003769
Iteration 151/1000 | Loss: 0.00003264
Iteration 152/1000 | Loss: 0.00025413
Iteration 153/1000 | Loss: 0.00003396
Iteration 154/1000 | Loss: 0.00002930
Iteration 155/1000 | Loss: 0.00002810
Iteration 156/1000 | Loss: 0.00002721
Iteration 157/1000 | Loss: 0.00048160
Iteration 158/1000 | Loss: 0.00032507
Iteration 159/1000 | Loss: 0.00026928
Iteration 160/1000 | Loss: 0.00004119
Iteration 161/1000 | Loss: 0.00037973
Iteration 162/1000 | Loss: 0.00050009
Iteration 163/1000 | Loss: 0.00014199
Iteration 164/1000 | Loss: 0.00025298
Iteration 165/1000 | Loss: 0.00038271
Iteration 166/1000 | Loss: 0.00004704
Iteration 167/1000 | Loss: 0.00003654
Iteration 168/1000 | Loss: 0.00005133
Iteration 169/1000 | Loss: 0.00003339
Iteration 170/1000 | Loss: 0.00003146
Iteration 171/1000 | Loss: 0.00019638
Iteration 172/1000 | Loss: 0.00003006
Iteration 173/1000 | Loss: 0.00002768
Iteration 174/1000 | Loss: 0.00002596
Iteration 175/1000 | Loss: 0.00002484
Iteration 176/1000 | Loss: 0.00002436
Iteration 177/1000 | Loss: 0.00002391
Iteration 178/1000 | Loss: 0.00002375
Iteration 179/1000 | Loss: 0.00002361
Iteration 180/1000 | Loss: 0.00002350
Iteration 181/1000 | Loss: 0.00002348
Iteration 182/1000 | Loss: 0.00002347
Iteration 183/1000 | Loss: 0.00002344
Iteration 184/1000 | Loss: 0.00002344
Iteration 185/1000 | Loss: 0.00002344
Iteration 186/1000 | Loss: 0.00002344
Iteration 187/1000 | Loss: 0.00002344
Iteration 188/1000 | Loss: 0.00002344
Iteration 189/1000 | Loss: 0.00002344
Iteration 190/1000 | Loss: 0.00002344
Iteration 191/1000 | Loss: 0.00002343
Iteration 192/1000 | Loss: 0.00002343
Iteration 193/1000 | Loss: 0.00002342
Iteration 194/1000 | Loss: 0.00002342
Iteration 195/1000 | Loss: 0.00002341
Iteration 196/1000 | Loss: 0.00002340
Iteration 197/1000 | Loss: 0.00002340
Iteration 198/1000 | Loss: 0.00002340
Iteration 199/1000 | Loss: 0.00002339
Iteration 200/1000 | Loss: 0.00002339
Iteration 201/1000 | Loss: 0.00002338
Iteration 202/1000 | Loss: 0.00002338
Iteration 203/1000 | Loss: 0.00002337
Iteration 204/1000 | Loss: 0.00002336
Iteration 205/1000 | Loss: 0.00002335
Iteration 206/1000 | Loss: 0.00002335
Iteration 207/1000 | Loss: 0.00002335
Iteration 208/1000 | Loss: 0.00002335
Iteration 209/1000 | Loss: 0.00002334
Iteration 210/1000 | Loss: 0.00002334
Iteration 211/1000 | Loss: 0.00002333
Iteration 212/1000 | Loss: 0.00002333
Iteration 213/1000 | Loss: 0.00002333
Iteration 214/1000 | Loss: 0.00002332
Iteration 215/1000 | Loss: 0.00002331
Iteration 216/1000 | Loss: 0.00002330
Iteration 217/1000 | Loss: 0.00002329
Iteration 218/1000 | Loss: 0.00002329
Iteration 219/1000 | Loss: 0.00002329
Iteration 220/1000 | Loss: 0.00002329
Iteration 221/1000 | Loss: 0.00002329
Iteration 222/1000 | Loss: 0.00002329
Iteration 223/1000 | Loss: 0.00002329
Iteration 224/1000 | Loss: 0.00002329
Iteration 225/1000 | Loss: 0.00002327
Iteration 226/1000 | Loss: 0.00002327
Iteration 227/1000 | Loss: 0.00002325
Iteration 228/1000 | Loss: 0.00002325
Iteration 229/1000 | Loss: 0.00002324
Iteration 230/1000 | Loss: 0.00002324
Iteration 231/1000 | Loss: 0.00002324
Iteration 232/1000 | Loss: 0.00002323
Iteration 233/1000 | Loss: 0.00002323
Iteration 234/1000 | Loss: 0.00002323
Iteration 235/1000 | Loss: 0.00002323
Iteration 236/1000 | Loss: 0.00002323
Iteration 237/1000 | Loss: 0.00002323
Iteration 238/1000 | Loss: 0.00002323
Iteration 239/1000 | Loss: 0.00002323
Iteration 240/1000 | Loss: 0.00002323
Iteration 241/1000 | Loss: 0.00002323
Iteration 242/1000 | Loss: 0.00002323
Iteration 243/1000 | Loss: 0.00002322
Iteration 244/1000 | Loss: 0.00002322
Iteration 245/1000 | Loss: 0.00002322
Iteration 246/1000 | Loss: 0.00002322
Iteration 247/1000 | Loss: 0.00002322
Iteration 248/1000 | Loss: 0.00002322
Iteration 249/1000 | Loss: 0.00002322
Iteration 250/1000 | Loss: 0.00002321
Iteration 251/1000 | Loss: 0.00002321
Iteration 252/1000 | Loss: 0.00002321
Iteration 253/1000 | Loss: 0.00002321
Iteration 254/1000 | Loss: 0.00002321
Iteration 255/1000 | Loss: 0.00002321
Iteration 256/1000 | Loss: 0.00002321
Iteration 257/1000 | Loss: 0.00002321
Iteration 258/1000 | Loss: 0.00002321
Iteration 259/1000 | Loss: 0.00002321
Iteration 260/1000 | Loss: 0.00002321
Iteration 261/1000 | Loss: 0.00002321
Iteration 262/1000 | Loss: 0.00002321
Iteration 263/1000 | Loss: 0.00002321
Iteration 264/1000 | Loss: 0.00002321
Iteration 265/1000 | Loss: 0.00002321
Iteration 266/1000 | Loss: 0.00002321
Iteration 267/1000 | Loss: 0.00002321
Iteration 268/1000 | Loss: 0.00002321
Iteration 269/1000 | Loss: 0.00002321
Iteration 270/1000 | Loss: 0.00002321
Iteration 271/1000 | Loss: 0.00002321
Iteration 272/1000 | Loss: 0.00002321
Iteration 273/1000 | Loss: 0.00002321
Iteration 274/1000 | Loss: 0.00002321
Iteration 275/1000 | Loss: 0.00002321
Iteration 276/1000 | Loss: 0.00002321
Iteration 277/1000 | Loss: 0.00002321
Iteration 278/1000 | Loss: 0.00002321
Iteration 279/1000 | Loss: 0.00002321
Iteration 280/1000 | Loss: 0.00002321
Iteration 281/1000 | Loss: 0.00002321
Iteration 282/1000 | Loss: 0.00002321
Iteration 283/1000 | Loss: 0.00002321
Iteration 284/1000 | Loss: 0.00002321
Iteration 285/1000 | Loss: 0.00002321
Iteration 286/1000 | Loss: 0.00002321
Iteration 287/1000 | Loss: 0.00002321
Iteration 288/1000 | Loss: 0.00002321
Iteration 289/1000 | Loss: 0.00002321
Iteration 290/1000 | Loss: 0.00002321
Iteration 291/1000 | Loss: 0.00002321
Iteration 292/1000 | Loss: 0.00002321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [2.320704334124457e-05, 2.320704334124457e-05, 2.320704334124457e-05, 2.320704334124457e-05, 2.320704334124457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.320704334124457e-05

Optimization complete. Final v2v error: 4.008174419403076 mm

Highest mean error: 9.558749198913574 mm for frame 21

Lowest mean error: 3.3751027584075928 mm for frame 5

Saving results

Total time: 298.9472141265869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810438
Iteration 2/25 | Loss: 0.00121590
Iteration 3/25 | Loss: 0.00108497
Iteration 4/25 | Loss: 0.00105251
Iteration 5/25 | Loss: 0.00105075
Iteration 6/25 | Loss: 0.00101764
Iteration 7/25 | Loss: 0.00101131
Iteration 8/25 | Loss: 0.00101076
Iteration 9/25 | Loss: 0.00101057
Iteration 10/25 | Loss: 0.00101053
Iteration 11/25 | Loss: 0.00101053
Iteration 12/25 | Loss: 0.00101053
Iteration 13/25 | Loss: 0.00101053
Iteration 14/25 | Loss: 0.00101053
Iteration 15/25 | Loss: 0.00101053
Iteration 16/25 | Loss: 0.00101053
Iteration 17/25 | Loss: 0.00101053
Iteration 18/25 | Loss: 0.00101053
Iteration 19/25 | Loss: 0.00101053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010105252731591463, 0.0010105252731591463, 0.0010105252731591463, 0.0010105252731591463, 0.0010105252731591463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010105252731591463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28716731
Iteration 2/25 | Loss: 0.00105356
Iteration 3/25 | Loss: 0.00105356
Iteration 4/25 | Loss: 0.00105356
Iteration 5/25 | Loss: 0.00105356
Iteration 6/25 | Loss: 0.00105356
Iteration 7/25 | Loss: 0.00105356
Iteration 8/25 | Loss: 0.00105356
Iteration 9/25 | Loss: 0.00105356
Iteration 10/25 | Loss: 0.00105356
Iteration 11/25 | Loss: 0.00105356
Iteration 12/25 | Loss: 0.00105356
Iteration 13/25 | Loss: 0.00105356
Iteration 14/25 | Loss: 0.00105356
Iteration 15/25 | Loss: 0.00105356
Iteration 16/25 | Loss: 0.00105356
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010535565670579672, 0.0010535565670579672, 0.0010535565670579672, 0.0010535565670579672, 0.0010535565670579672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010535565670579672

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105356
Iteration 2/1000 | Loss: 0.00003914
Iteration 3/1000 | Loss: 0.00002221
Iteration 4/1000 | Loss: 0.00001699
Iteration 5/1000 | Loss: 0.00001590
Iteration 6/1000 | Loss: 0.00001501
Iteration 7/1000 | Loss: 0.00001455
Iteration 8/1000 | Loss: 0.00001410
Iteration 9/1000 | Loss: 0.00001382
Iteration 10/1000 | Loss: 0.00001355
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001341
Iteration 14/1000 | Loss: 0.00001340
Iteration 15/1000 | Loss: 0.00001340
Iteration 16/1000 | Loss: 0.00001339
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001328
Iteration 19/1000 | Loss: 0.00001322
Iteration 20/1000 | Loss: 0.00001320
Iteration 21/1000 | Loss: 0.00001319
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001317
Iteration 24/1000 | Loss: 0.00001317
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001317
Iteration 27/1000 | Loss: 0.00001317
Iteration 28/1000 | Loss: 0.00001317
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001317
Iteration 31/1000 | Loss: 0.00001317
Iteration 32/1000 | Loss: 0.00001317
Iteration 33/1000 | Loss: 0.00001317
Iteration 34/1000 | Loss: 0.00001316
Iteration 35/1000 | Loss: 0.00001316
Iteration 36/1000 | Loss: 0.00001316
Iteration 37/1000 | Loss: 0.00001316
Iteration 38/1000 | Loss: 0.00001316
Iteration 39/1000 | Loss: 0.00001316
Iteration 40/1000 | Loss: 0.00001315
Iteration 41/1000 | Loss: 0.00001315
Iteration 42/1000 | Loss: 0.00001312
Iteration 43/1000 | Loss: 0.00001311
Iteration 44/1000 | Loss: 0.00001311
Iteration 45/1000 | Loss: 0.00001310
Iteration 46/1000 | Loss: 0.00001310
Iteration 47/1000 | Loss: 0.00001309
Iteration 48/1000 | Loss: 0.00001309
Iteration 49/1000 | Loss: 0.00001308
Iteration 50/1000 | Loss: 0.00001308
Iteration 51/1000 | Loss: 0.00001307
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001306
Iteration 55/1000 | Loss: 0.00001305
Iteration 56/1000 | Loss: 0.00001305
Iteration 57/1000 | Loss: 0.00001305
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001304
Iteration 60/1000 | Loss: 0.00001304
Iteration 61/1000 | Loss: 0.00001304
Iteration 62/1000 | Loss: 0.00001304
Iteration 63/1000 | Loss: 0.00001303
Iteration 64/1000 | Loss: 0.00001303
Iteration 65/1000 | Loss: 0.00001303
Iteration 66/1000 | Loss: 0.00001303
Iteration 67/1000 | Loss: 0.00001303
Iteration 68/1000 | Loss: 0.00001303
Iteration 69/1000 | Loss: 0.00001303
Iteration 70/1000 | Loss: 0.00001302
Iteration 71/1000 | Loss: 0.00001302
Iteration 72/1000 | Loss: 0.00001302
Iteration 73/1000 | Loss: 0.00001302
Iteration 74/1000 | Loss: 0.00001302
Iteration 75/1000 | Loss: 0.00001302
Iteration 76/1000 | Loss: 0.00001302
Iteration 77/1000 | Loss: 0.00001302
Iteration 78/1000 | Loss: 0.00001302
Iteration 79/1000 | Loss: 0.00001302
Iteration 80/1000 | Loss: 0.00001302
Iteration 81/1000 | Loss: 0.00001302
Iteration 82/1000 | Loss: 0.00001302
Iteration 83/1000 | Loss: 0.00001301
Iteration 84/1000 | Loss: 0.00001301
Iteration 85/1000 | Loss: 0.00001301
Iteration 86/1000 | Loss: 0.00001301
Iteration 87/1000 | Loss: 0.00001301
Iteration 88/1000 | Loss: 0.00001301
Iteration 89/1000 | Loss: 0.00001301
Iteration 90/1000 | Loss: 0.00001300
Iteration 91/1000 | Loss: 0.00001300
Iteration 92/1000 | Loss: 0.00001300
Iteration 93/1000 | Loss: 0.00001300
Iteration 94/1000 | Loss: 0.00001300
Iteration 95/1000 | Loss: 0.00001300
Iteration 96/1000 | Loss: 0.00001300
Iteration 97/1000 | Loss: 0.00001300
Iteration 98/1000 | Loss: 0.00001300
Iteration 99/1000 | Loss: 0.00001299
Iteration 100/1000 | Loss: 0.00001299
Iteration 101/1000 | Loss: 0.00001299
Iteration 102/1000 | Loss: 0.00001299
Iteration 103/1000 | Loss: 0.00001299
Iteration 104/1000 | Loss: 0.00001299
Iteration 105/1000 | Loss: 0.00001298
Iteration 106/1000 | Loss: 0.00001298
Iteration 107/1000 | Loss: 0.00001298
Iteration 108/1000 | Loss: 0.00001298
Iteration 109/1000 | Loss: 0.00001298
Iteration 110/1000 | Loss: 0.00001298
Iteration 111/1000 | Loss: 0.00001298
Iteration 112/1000 | Loss: 0.00001298
Iteration 113/1000 | Loss: 0.00001298
Iteration 114/1000 | Loss: 0.00001298
Iteration 115/1000 | Loss: 0.00001298
Iteration 116/1000 | Loss: 0.00001298
Iteration 117/1000 | Loss: 0.00001298
Iteration 118/1000 | Loss: 0.00001298
Iteration 119/1000 | Loss: 0.00001298
Iteration 120/1000 | Loss: 0.00001298
Iteration 121/1000 | Loss: 0.00001298
Iteration 122/1000 | Loss: 0.00001298
Iteration 123/1000 | Loss: 0.00001298
Iteration 124/1000 | Loss: 0.00001298
Iteration 125/1000 | Loss: 0.00001298
Iteration 126/1000 | Loss: 0.00001298
Iteration 127/1000 | Loss: 0.00001298
Iteration 128/1000 | Loss: 0.00001298
Iteration 129/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.2979463463125285e-05, 1.2979463463125285e-05, 1.2979463463125285e-05, 1.2979463463125285e-05, 1.2979463463125285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2979463463125285e-05

Optimization complete. Final v2v error: 3.147209882736206 mm

Highest mean error: 3.6943321228027344 mm for frame 121

Lowest mean error: 2.5490622520446777 mm for frame 239

Saving results

Total time: 46.24343490600586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01207086
Iteration 2/25 | Loss: 0.00279816
Iteration 3/25 | Loss: 0.00181272
Iteration 4/25 | Loss: 0.00188030
Iteration 5/25 | Loss: 0.00181113
Iteration 6/25 | Loss: 0.00173198
Iteration 7/25 | Loss: 0.00166729
Iteration 8/25 | Loss: 0.00162671
Iteration 9/25 | Loss: 0.00158205
Iteration 10/25 | Loss: 0.00157766
Iteration 11/25 | Loss: 0.00155858
Iteration 12/25 | Loss: 0.00150729
Iteration 13/25 | Loss: 0.00145295
Iteration 14/25 | Loss: 0.00144217
Iteration 15/25 | Loss: 0.00143321
Iteration 16/25 | Loss: 0.00142303
Iteration 17/25 | Loss: 0.00141678
Iteration 18/25 | Loss: 0.00141311
Iteration 19/25 | Loss: 0.00141212
Iteration 20/25 | Loss: 0.00141319
Iteration 21/25 | Loss: 0.00141119
Iteration 22/25 | Loss: 0.00141087
Iteration 23/25 | Loss: 0.00141072
Iteration 24/25 | Loss: 0.00141065
Iteration 25/25 | Loss: 0.00141063

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.34818554
Iteration 2/25 | Loss: 0.00075948
Iteration 3/25 | Loss: 0.00075947
Iteration 4/25 | Loss: 0.00075947
Iteration 5/25 | Loss: 0.00075947
Iteration 6/25 | Loss: 0.00075947
Iteration 7/25 | Loss: 0.00075947
Iteration 8/25 | Loss: 0.00075947
Iteration 9/25 | Loss: 0.00075947
Iteration 10/25 | Loss: 0.00075947
Iteration 11/25 | Loss: 0.00075947
Iteration 12/25 | Loss: 0.00075947
Iteration 13/25 | Loss: 0.00075947
Iteration 14/25 | Loss: 0.00075947
Iteration 15/25 | Loss: 0.00075947
Iteration 16/25 | Loss: 0.00075947
Iteration 17/25 | Loss: 0.00075947
Iteration 18/25 | Loss: 0.00075947
Iteration 19/25 | Loss: 0.00075947
Iteration 20/25 | Loss: 0.00075947
Iteration 21/25 | Loss: 0.00075947
Iteration 22/25 | Loss: 0.00075947
Iteration 23/25 | Loss: 0.00075947
Iteration 24/25 | Loss: 0.00075947
Iteration 25/25 | Loss: 0.00075947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075947
Iteration 2/1000 | Loss: 0.00011123
Iteration 3/1000 | Loss: 0.00008852
Iteration 4/1000 | Loss: 0.00008094
Iteration 5/1000 | Loss: 0.00007808
Iteration 6/1000 | Loss: 0.00007668
Iteration 7/1000 | Loss: 0.00007520
Iteration 8/1000 | Loss: 0.00007393
Iteration 9/1000 | Loss: 0.00007267
Iteration 10/1000 | Loss: 0.00007136
Iteration 11/1000 | Loss: 0.00007031
Iteration 12/1000 | Loss: 0.00006960
Iteration 13/1000 | Loss: 0.00006888
Iteration 14/1000 | Loss: 0.00006828
Iteration 15/1000 | Loss: 0.00006766
Iteration 16/1000 | Loss: 0.00006692
Iteration 17/1000 | Loss: 0.00006656
Iteration 18/1000 | Loss: 0.00006617
Iteration 19/1000 | Loss: 0.00006581
Iteration 20/1000 | Loss: 0.00006549
Iteration 21/1000 | Loss: 0.00006528
Iteration 22/1000 | Loss: 0.00006508
Iteration 23/1000 | Loss: 0.00006488
Iteration 24/1000 | Loss: 0.00006476
Iteration 25/1000 | Loss: 0.00006472
Iteration 26/1000 | Loss: 0.00006472
Iteration 27/1000 | Loss: 0.00006472
Iteration 28/1000 | Loss: 0.00006472
Iteration 29/1000 | Loss: 0.00006472
Iteration 30/1000 | Loss: 0.00006472
Iteration 31/1000 | Loss: 0.00006471
Iteration 32/1000 | Loss: 0.00006471
Iteration 33/1000 | Loss: 0.00006471
Iteration 34/1000 | Loss: 0.00006470
Iteration 35/1000 | Loss: 0.00006470
Iteration 36/1000 | Loss: 0.00006468
Iteration 37/1000 | Loss: 0.00006468
Iteration 38/1000 | Loss: 0.00006468
Iteration 39/1000 | Loss: 0.00006468
Iteration 40/1000 | Loss: 0.00006468
Iteration 41/1000 | Loss: 0.00006468
Iteration 42/1000 | Loss: 0.00006468
Iteration 43/1000 | Loss: 0.00006468
Iteration 44/1000 | Loss: 0.00006468
Iteration 45/1000 | Loss: 0.00006468
Iteration 46/1000 | Loss: 0.00006468
Iteration 47/1000 | Loss: 0.00006468
Iteration 48/1000 | Loss: 0.00006468
Iteration 49/1000 | Loss: 0.00006466
Iteration 50/1000 | Loss: 0.00006466
Iteration 51/1000 | Loss: 0.00006466
Iteration 52/1000 | Loss: 0.00006466
Iteration 53/1000 | Loss: 0.00006466
Iteration 54/1000 | Loss: 0.00006466
Iteration 55/1000 | Loss: 0.00006466
Iteration 56/1000 | Loss: 0.00006466
Iteration 57/1000 | Loss: 0.00006465
Iteration 58/1000 | Loss: 0.00006465
Iteration 59/1000 | Loss: 0.00006465
Iteration 60/1000 | Loss: 0.00006465
Iteration 61/1000 | Loss: 0.00006464
Iteration 62/1000 | Loss: 0.00006463
Iteration 63/1000 | Loss: 0.00006463
Iteration 64/1000 | Loss: 0.00006462
Iteration 65/1000 | Loss: 0.00006462
Iteration 66/1000 | Loss: 0.00006459
Iteration 67/1000 | Loss: 0.00006459
Iteration 68/1000 | Loss: 0.00006458
Iteration 69/1000 | Loss: 0.00006458
Iteration 70/1000 | Loss: 0.00006458
Iteration 71/1000 | Loss: 0.00006458
Iteration 72/1000 | Loss: 0.00006458
Iteration 73/1000 | Loss: 0.00006458
Iteration 74/1000 | Loss: 0.00006458
Iteration 75/1000 | Loss: 0.00006458
Iteration 76/1000 | Loss: 0.00006458
Iteration 77/1000 | Loss: 0.00006458
Iteration 78/1000 | Loss: 0.00006457
Iteration 79/1000 | Loss: 0.00006457
Iteration 80/1000 | Loss: 0.00006457
Iteration 81/1000 | Loss: 0.00006457
Iteration 82/1000 | Loss: 0.00006457
Iteration 83/1000 | Loss: 0.00006457
Iteration 84/1000 | Loss: 0.00006456
Iteration 85/1000 | Loss: 0.00006456
Iteration 86/1000 | Loss: 0.00006455
Iteration 87/1000 | Loss: 0.00006455
Iteration 88/1000 | Loss: 0.00006455
Iteration 89/1000 | Loss: 0.00006455
Iteration 90/1000 | Loss: 0.00006455
Iteration 91/1000 | Loss: 0.00006454
Iteration 92/1000 | Loss: 0.00006454
Iteration 93/1000 | Loss: 0.00006454
Iteration 94/1000 | Loss: 0.00006454
Iteration 95/1000 | Loss: 0.00006453
Iteration 96/1000 | Loss: 0.00006453
Iteration 97/1000 | Loss: 0.00006452
Iteration 98/1000 | Loss: 0.00006451
Iteration 99/1000 | Loss: 0.00006451
Iteration 100/1000 | Loss: 0.00006451
Iteration 101/1000 | Loss: 0.00006451
Iteration 102/1000 | Loss: 0.00006451
Iteration 103/1000 | Loss: 0.00006451
Iteration 104/1000 | Loss: 0.00006450
Iteration 105/1000 | Loss: 0.00006450
Iteration 106/1000 | Loss: 0.00006450
Iteration 107/1000 | Loss: 0.00006450
Iteration 108/1000 | Loss: 0.00006450
Iteration 109/1000 | Loss: 0.00006450
Iteration 110/1000 | Loss: 0.00006450
Iteration 111/1000 | Loss: 0.00006450
Iteration 112/1000 | Loss: 0.00006450
Iteration 113/1000 | Loss: 0.00006450
Iteration 114/1000 | Loss: 0.00006449
Iteration 115/1000 | Loss: 0.00006449
Iteration 116/1000 | Loss: 0.00006449
Iteration 117/1000 | Loss: 0.00006449
Iteration 118/1000 | Loss: 0.00006449
Iteration 119/1000 | Loss: 0.00006449
Iteration 120/1000 | Loss: 0.00006449
Iteration 121/1000 | Loss: 0.00006449
Iteration 122/1000 | Loss: 0.00006449
Iteration 123/1000 | Loss: 0.00006449
Iteration 124/1000 | Loss: 0.00006449
Iteration 125/1000 | Loss: 0.00006449
Iteration 126/1000 | Loss: 0.00006449
Iteration 127/1000 | Loss: 0.00006449
Iteration 128/1000 | Loss: 0.00006448
Iteration 129/1000 | Loss: 0.00006448
Iteration 130/1000 | Loss: 0.00006448
Iteration 131/1000 | Loss: 0.00006448
Iteration 132/1000 | Loss: 0.00006448
Iteration 133/1000 | Loss: 0.00006448
Iteration 134/1000 | Loss: 0.00006448
Iteration 135/1000 | Loss: 0.00006448
Iteration 136/1000 | Loss: 0.00006448
Iteration 137/1000 | Loss: 0.00006448
Iteration 138/1000 | Loss: 0.00006448
Iteration 139/1000 | Loss: 0.00006448
Iteration 140/1000 | Loss: 0.00006448
Iteration 141/1000 | Loss: 0.00006448
Iteration 142/1000 | Loss: 0.00006448
Iteration 143/1000 | Loss: 0.00006448
Iteration 144/1000 | Loss: 0.00006448
Iteration 145/1000 | Loss: 0.00006448
Iteration 146/1000 | Loss: 0.00006448
Iteration 147/1000 | Loss: 0.00006448
Iteration 148/1000 | Loss: 0.00006448
Iteration 149/1000 | Loss: 0.00006448
Iteration 150/1000 | Loss: 0.00006448
Iteration 151/1000 | Loss: 0.00006448
Iteration 152/1000 | Loss: 0.00006448
Iteration 153/1000 | Loss: 0.00006448
Iteration 154/1000 | Loss: 0.00006448
Iteration 155/1000 | Loss: 0.00006448
Iteration 156/1000 | Loss: 0.00006448
Iteration 157/1000 | Loss: 0.00006448
Iteration 158/1000 | Loss: 0.00006448
Iteration 159/1000 | Loss: 0.00006448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [6.44786050543189e-05, 6.44786050543189e-05, 6.44786050543189e-05, 6.44786050543189e-05, 6.44786050543189e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.44786050543189e-05

Optimization complete. Final v2v error: 6.449588298797607 mm

Highest mean error: 6.713872909545898 mm for frame 79

Lowest mean error: 6.25700044631958 mm for frame 142

Saving results

Total time: 99.1026873588562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_33_us_2100/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_33_us_2100/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392238
Iteration 2/25 | Loss: 0.00106405
Iteration 3/25 | Loss: 0.00097837
Iteration 4/25 | Loss: 0.00096775
Iteration 5/25 | Loss: 0.00096444
Iteration 6/25 | Loss: 0.00096339
Iteration 7/25 | Loss: 0.00096339
Iteration 8/25 | Loss: 0.00096339
Iteration 9/25 | Loss: 0.00096339
Iteration 10/25 | Loss: 0.00096339
Iteration 11/25 | Loss: 0.00096339
Iteration 12/25 | Loss: 0.00096339
Iteration 13/25 | Loss: 0.00096339
Iteration 14/25 | Loss: 0.00096339
Iteration 15/25 | Loss: 0.00096339
Iteration 16/25 | Loss: 0.00096339
Iteration 17/25 | Loss: 0.00096339
Iteration 18/25 | Loss: 0.00096339
Iteration 19/25 | Loss: 0.00096339
Iteration 20/25 | Loss: 0.00096339
Iteration 21/25 | Loss: 0.00096339
Iteration 22/25 | Loss: 0.00096339
Iteration 23/25 | Loss: 0.00096339
Iteration 24/25 | Loss: 0.00096339
Iteration 25/25 | Loss: 0.00096339

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32200170
Iteration 2/25 | Loss: 0.00077972
Iteration 3/25 | Loss: 0.00077972
Iteration 4/25 | Loss: 0.00077972
Iteration 5/25 | Loss: 0.00077972
Iteration 6/25 | Loss: 0.00077972
Iteration 7/25 | Loss: 0.00077972
Iteration 8/25 | Loss: 0.00077972
Iteration 9/25 | Loss: 0.00077972
Iteration 10/25 | Loss: 0.00077972
Iteration 11/25 | Loss: 0.00077972
Iteration 12/25 | Loss: 0.00077972
Iteration 13/25 | Loss: 0.00077972
Iteration 14/25 | Loss: 0.00077972
Iteration 15/25 | Loss: 0.00077972
Iteration 16/25 | Loss: 0.00077972
Iteration 17/25 | Loss: 0.00077972
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007797175785526633, 0.0007797175785526633, 0.0007797175785526633, 0.0007797175785526633, 0.0007797175785526633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007797175785526633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077972
Iteration 2/1000 | Loss: 0.00002267
Iteration 3/1000 | Loss: 0.00001422
Iteration 4/1000 | Loss: 0.00001322
Iteration 5/1000 | Loss: 0.00001250
Iteration 6/1000 | Loss: 0.00001211
Iteration 7/1000 | Loss: 0.00001172
Iteration 8/1000 | Loss: 0.00001149
Iteration 9/1000 | Loss: 0.00001125
Iteration 10/1000 | Loss: 0.00001124
Iteration 11/1000 | Loss: 0.00001107
Iteration 12/1000 | Loss: 0.00001103
Iteration 13/1000 | Loss: 0.00001102
Iteration 14/1000 | Loss: 0.00001101
Iteration 15/1000 | Loss: 0.00001100
Iteration 16/1000 | Loss: 0.00001100
Iteration 17/1000 | Loss: 0.00001100
Iteration 18/1000 | Loss: 0.00001100
Iteration 19/1000 | Loss: 0.00001099
Iteration 20/1000 | Loss: 0.00001099
Iteration 21/1000 | Loss: 0.00001099
Iteration 22/1000 | Loss: 0.00001098
Iteration 23/1000 | Loss: 0.00001097
Iteration 24/1000 | Loss: 0.00001097
Iteration 25/1000 | Loss: 0.00001096
Iteration 26/1000 | Loss: 0.00001096
Iteration 27/1000 | Loss: 0.00001096
Iteration 28/1000 | Loss: 0.00001096
Iteration 29/1000 | Loss: 0.00001096
Iteration 30/1000 | Loss: 0.00001095
Iteration 31/1000 | Loss: 0.00001094
Iteration 32/1000 | Loss: 0.00001093
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001092
Iteration 35/1000 | Loss: 0.00001091
Iteration 36/1000 | Loss: 0.00001090
Iteration 37/1000 | Loss: 0.00001088
Iteration 38/1000 | Loss: 0.00001087
Iteration 39/1000 | Loss: 0.00001087
Iteration 40/1000 | Loss: 0.00001084
Iteration 41/1000 | Loss: 0.00001083
Iteration 42/1000 | Loss: 0.00001082
Iteration 43/1000 | Loss: 0.00001081
Iteration 44/1000 | Loss: 0.00001081
Iteration 45/1000 | Loss: 0.00001081
Iteration 46/1000 | Loss: 0.00001081
Iteration 47/1000 | Loss: 0.00001081
Iteration 48/1000 | Loss: 0.00001081
Iteration 49/1000 | Loss: 0.00001081
Iteration 50/1000 | Loss: 0.00001080
Iteration 51/1000 | Loss: 0.00001080
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001079
Iteration 54/1000 | Loss: 0.00001078
Iteration 55/1000 | Loss: 0.00001078
Iteration 56/1000 | Loss: 0.00001078
Iteration 57/1000 | Loss: 0.00001077
Iteration 58/1000 | Loss: 0.00001077
Iteration 59/1000 | Loss: 0.00001077
Iteration 60/1000 | Loss: 0.00001077
Iteration 61/1000 | Loss: 0.00001077
Iteration 62/1000 | Loss: 0.00001077
Iteration 63/1000 | Loss: 0.00001077
Iteration 64/1000 | Loss: 0.00001077
Iteration 65/1000 | Loss: 0.00001076
Iteration 66/1000 | Loss: 0.00001076
Iteration 67/1000 | Loss: 0.00001076
Iteration 68/1000 | Loss: 0.00001076
Iteration 69/1000 | Loss: 0.00001076
Iteration 70/1000 | Loss: 0.00001075
Iteration 71/1000 | Loss: 0.00001075
Iteration 72/1000 | Loss: 0.00001075
Iteration 73/1000 | Loss: 0.00001075
Iteration 74/1000 | Loss: 0.00001075
Iteration 75/1000 | Loss: 0.00001074
Iteration 76/1000 | Loss: 0.00001074
Iteration 77/1000 | Loss: 0.00001074
Iteration 78/1000 | Loss: 0.00001074
Iteration 79/1000 | Loss: 0.00001074
Iteration 80/1000 | Loss: 0.00001074
Iteration 81/1000 | Loss: 0.00001074
Iteration 82/1000 | Loss: 0.00001073
Iteration 83/1000 | Loss: 0.00001073
Iteration 84/1000 | Loss: 0.00001073
Iteration 85/1000 | Loss: 0.00001073
Iteration 86/1000 | Loss: 0.00001073
Iteration 87/1000 | Loss: 0.00001072
Iteration 88/1000 | Loss: 0.00001072
Iteration 89/1000 | Loss: 0.00001072
Iteration 90/1000 | Loss: 0.00001072
Iteration 91/1000 | Loss: 0.00001071
Iteration 92/1000 | Loss: 0.00001071
Iteration 93/1000 | Loss: 0.00001071
Iteration 94/1000 | Loss: 0.00001071
Iteration 95/1000 | Loss: 0.00001071
Iteration 96/1000 | Loss: 0.00001070
Iteration 97/1000 | Loss: 0.00001070
Iteration 98/1000 | Loss: 0.00001070
Iteration 99/1000 | Loss: 0.00001070
Iteration 100/1000 | Loss: 0.00001070
Iteration 101/1000 | Loss: 0.00001070
Iteration 102/1000 | Loss: 0.00001070
Iteration 103/1000 | Loss: 0.00001070
Iteration 104/1000 | Loss: 0.00001070
Iteration 105/1000 | Loss: 0.00001070
Iteration 106/1000 | Loss: 0.00001070
Iteration 107/1000 | Loss: 0.00001070
Iteration 108/1000 | Loss: 0.00001070
Iteration 109/1000 | Loss: 0.00001070
Iteration 110/1000 | Loss: 0.00001070
Iteration 111/1000 | Loss: 0.00001070
Iteration 112/1000 | Loss: 0.00001070
Iteration 113/1000 | Loss: 0.00001070
Iteration 114/1000 | Loss: 0.00001070
Iteration 115/1000 | Loss: 0.00001070
Iteration 116/1000 | Loss: 0.00001070
Iteration 117/1000 | Loss: 0.00001070
Iteration 118/1000 | Loss: 0.00001070
Iteration 119/1000 | Loss: 0.00001070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.0699637641664594e-05, 1.0699637641664594e-05, 1.0699637641664594e-05, 1.0699637641664594e-05, 1.0699637641664594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0699637641664594e-05

Optimization complete. Final v2v error: 2.885744333267212 mm

Highest mean error: 3.199226140975952 mm for frame 3

Lowest mean error: 2.4845848083496094 mm for frame 140

Saving results

Total time: 30.75882887840271
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00939575
Iteration 2/25 | Loss: 0.00226765
Iteration 3/25 | Loss: 0.00174639
Iteration 4/25 | Loss: 0.00167021
Iteration 5/25 | Loss: 0.00163094
Iteration 6/25 | Loss: 0.00158865
Iteration 7/25 | Loss: 0.00150611
Iteration 8/25 | Loss: 0.00147372
Iteration 9/25 | Loss: 0.00146623
Iteration 10/25 | Loss: 0.00146176
Iteration 11/25 | Loss: 0.00146065
Iteration 12/25 | Loss: 0.00146014
Iteration 13/25 | Loss: 0.00146353
Iteration 14/25 | Loss: 0.00145637
Iteration 15/25 | Loss: 0.00145460
Iteration 16/25 | Loss: 0.00145419
Iteration 17/25 | Loss: 0.00145400
Iteration 18/25 | Loss: 0.00145372
Iteration 19/25 | Loss: 0.00145906
Iteration 20/25 | Loss: 0.00145155
Iteration 21/25 | Loss: 0.00144897
Iteration 22/25 | Loss: 0.00144817
Iteration 23/25 | Loss: 0.00144804
Iteration 24/25 | Loss: 0.00144803
Iteration 25/25 | Loss: 0.00144803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41955781
Iteration 2/25 | Loss: 0.00221571
Iteration 3/25 | Loss: 0.00221570
Iteration 4/25 | Loss: 0.00221570
Iteration 5/25 | Loss: 0.00221570
Iteration 6/25 | Loss: 0.00221570
Iteration 7/25 | Loss: 0.00221570
Iteration 8/25 | Loss: 0.00221570
Iteration 9/25 | Loss: 0.00221570
Iteration 10/25 | Loss: 0.00221570
Iteration 11/25 | Loss: 0.00221570
Iteration 12/25 | Loss: 0.00221570
Iteration 13/25 | Loss: 0.00221570
Iteration 14/25 | Loss: 0.00221570
Iteration 15/25 | Loss: 0.00221570
Iteration 16/25 | Loss: 0.00221570
Iteration 17/25 | Loss: 0.00221570
Iteration 18/25 | Loss: 0.00221570
Iteration 19/25 | Loss: 0.00221570
Iteration 20/25 | Loss: 0.00221570
Iteration 21/25 | Loss: 0.00221570
Iteration 22/25 | Loss: 0.00221570
Iteration 23/25 | Loss: 0.00221570
Iteration 24/25 | Loss: 0.00221570
Iteration 25/25 | Loss: 0.00221570

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221570
Iteration 2/1000 | Loss: 0.00021511
Iteration 3/1000 | Loss: 0.00013709
Iteration 4/1000 | Loss: 0.00011704
Iteration 5/1000 | Loss: 0.00010706
Iteration 6/1000 | Loss: 0.00010249
Iteration 7/1000 | Loss: 0.00009945
Iteration 8/1000 | Loss: 0.00009596
Iteration 9/1000 | Loss: 0.00009345
Iteration 10/1000 | Loss: 0.00009124
Iteration 11/1000 | Loss: 0.00041059
Iteration 12/1000 | Loss: 0.00009423
Iteration 13/1000 | Loss: 0.00008930
Iteration 14/1000 | Loss: 0.00030201
Iteration 15/1000 | Loss: 0.00008853
Iteration 16/1000 | Loss: 0.00008570
Iteration 17/1000 | Loss: 0.00008388
Iteration 18/1000 | Loss: 0.00008288
Iteration 19/1000 | Loss: 0.00008184
Iteration 20/1000 | Loss: 0.00008106
Iteration 21/1000 | Loss: 0.00008051
Iteration 22/1000 | Loss: 0.00007994
Iteration 23/1000 | Loss: 0.00007941
Iteration 24/1000 | Loss: 0.00007896
Iteration 25/1000 | Loss: 0.00007866
Iteration 26/1000 | Loss: 0.00007839
Iteration 27/1000 | Loss: 0.00007816
Iteration 28/1000 | Loss: 0.00007799
Iteration 29/1000 | Loss: 0.00007798
Iteration 30/1000 | Loss: 0.00007783
Iteration 31/1000 | Loss: 0.00007783
Iteration 32/1000 | Loss: 0.00007783
Iteration 33/1000 | Loss: 0.00007782
Iteration 34/1000 | Loss: 0.00008735
Iteration 35/1000 | Loss: 0.00007768
Iteration 36/1000 | Loss: 0.00007760
Iteration 37/1000 | Loss: 0.00007759
Iteration 38/1000 | Loss: 0.00007754
Iteration 39/1000 | Loss: 0.00007754
Iteration 40/1000 | Loss: 0.00007754
Iteration 41/1000 | Loss: 0.00007754
Iteration 42/1000 | Loss: 0.00053417
Iteration 43/1000 | Loss: 0.00583388
Iteration 44/1000 | Loss: 0.00250105
Iteration 45/1000 | Loss: 0.00046231
Iteration 46/1000 | Loss: 0.00104613
Iteration 47/1000 | Loss: 0.00009136
Iteration 48/1000 | Loss: 0.00046476
Iteration 49/1000 | Loss: 0.00022792
Iteration 50/1000 | Loss: 0.00006981
Iteration 51/1000 | Loss: 0.00005628
Iteration 52/1000 | Loss: 0.00004946
Iteration 53/1000 | Loss: 0.00004435
Iteration 54/1000 | Loss: 0.00004142
Iteration 55/1000 | Loss: 0.00003885
Iteration 56/1000 | Loss: 0.00003735
Iteration 57/1000 | Loss: 0.00003578
Iteration 58/1000 | Loss: 0.00003460
Iteration 59/1000 | Loss: 0.00003377
Iteration 60/1000 | Loss: 0.00003313
Iteration 61/1000 | Loss: 0.00003267
Iteration 62/1000 | Loss: 0.00003217
Iteration 63/1000 | Loss: 0.00090400
Iteration 64/1000 | Loss: 0.00055060
Iteration 65/1000 | Loss: 0.00003467
Iteration 66/1000 | Loss: 0.00054497
Iteration 67/1000 | Loss: 0.00020371
Iteration 68/1000 | Loss: 0.00005221
Iteration 69/1000 | Loss: 0.00003996
Iteration 70/1000 | Loss: 0.00004255
Iteration 71/1000 | Loss: 0.00018772
Iteration 72/1000 | Loss: 0.00010326
Iteration 73/1000 | Loss: 0.00007096
Iteration 74/1000 | Loss: 0.00003423
Iteration 75/1000 | Loss: 0.00003063
Iteration 76/1000 | Loss: 0.00002977
Iteration 77/1000 | Loss: 0.00002914
Iteration 78/1000 | Loss: 0.00002869
Iteration 79/1000 | Loss: 0.00002820
Iteration 80/1000 | Loss: 0.00008460
Iteration 81/1000 | Loss: 0.00003014
Iteration 82/1000 | Loss: 0.00004821
Iteration 83/1000 | Loss: 0.00002731
Iteration 84/1000 | Loss: 0.00002720
Iteration 85/1000 | Loss: 0.00002701
Iteration 86/1000 | Loss: 0.00002692
Iteration 87/1000 | Loss: 0.00002687
Iteration 88/1000 | Loss: 0.00002686
Iteration 89/1000 | Loss: 0.00002686
Iteration 90/1000 | Loss: 0.00002685
Iteration 91/1000 | Loss: 0.00002684
Iteration 92/1000 | Loss: 0.00002681
Iteration 93/1000 | Loss: 0.00002681
Iteration 94/1000 | Loss: 0.00002680
Iteration 95/1000 | Loss: 0.00002678
Iteration 96/1000 | Loss: 0.00002677
Iteration 97/1000 | Loss: 0.00002676
Iteration 98/1000 | Loss: 0.00002675
Iteration 99/1000 | Loss: 0.00002675
Iteration 100/1000 | Loss: 0.00002675
Iteration 101/1000 | Loss: 0.00002675
Iteration 102/1000 | Loss: 0.00002675
Iteration 103/1000 | Loss: 0.00002675
Iteration 104/1000 | Loss: 0.00002675
Iteration 105/1000 | Loss: 0.00002675
Iteration 106/1000 | Loss: 0.00002675
Iteration 107/1000 | Loss: 0.00002675
Iteration 108/1000 | Loss: 0.00002674
Iteration 109/1000 | Loss: 0.00002674
Iteration 110/1000 | Loss: 0.00002674
Iteration 111/1000 | Loss: 0.00002674
Iteration 112/1000 | Loss: 0.00002674
Iteration 113/1000 | Loss: 0.00002674
Iteration 114/1000 | Loss: 0.00002674
Iteration 115/1000 | Loss: 0.00002674
Iteration 116/1000 | Loss: 0.00002674
Iteration 117/1000 | Loss: 0.00002674
Iteration 118/1000 | Loss: 0.00002674
Iteration 119/1000 | Loss: 0.00002674
Iteration 120/1000 | Loss: 0.00002674
Iteration 121/1000 | Loss: 0.00002674
Iteration 122/1000 | Loss: 0.00002674
Iteration 123/1000 | Loss: 0.00002674
Iteration 124/1000 | Loss: 0.00002674
Iteration 125/1000 | Loss: 0.00002674
Iteration 126/1000 | Loss: 0.00002674
Iteration 127/1000 | Loss: 0.00002674
Iteration 128/1000 | Loss: 0.00002674
Iteration 129/1000 | Loss: 0.00002674
Iteration 130/1000 | Loss: 0.00002674
Iteration 131/1000 | Loss: 0.00002674
Iteration 132/1000 | Loss: 0.00002674
Iteration 133/1000 | Loss: 0.00002674
Iteration 134/1000 | Loss: 0.00002674
Iteration 135/1000 | Loss: 0.00002674
Iteration 136/1000 | Loss: 0.00002674
Iteration 137/1000 | Loss: 0.00002674
Iteration 138/1000 | Loss: 0.00002674
Iteration 139/1000 | Loss: 0.00002674
Iteration 140/1000 | Loss: 0.00002674
Iteration 141/1000 | Loss: 0.00002674
Iteration 142/1000 | Loss: 0.00002674
Iteration 143/1000 | Loss: 0.00002674
Iteration 144/1000 | Loss: 0.00002674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.6743446142063476e-05, 2.6743446142063476e-05, 2.6743446142063476e-05, 2.6743446142063476e-05, 2.6743446142063476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6743446142063476e-05

Optimization complete. Final v2v error: 4.183101177215576 mm

Highest mean error: 6.024630069732666 mm for frame 47

Lowest mean error: 2.8612937927246094 mm for frame 17

Saving results

Total time: 149.1116213798523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004471
Iteration 2/25 | Loss: 0.00164080
Iteration 3/25 | Loss: 0.00143454
Iteration 4/25 | Loss: 0.00140338
Iteration 5/25 | Loss: 0.00134375
Iteration 6/25 | Loss: 0.00131148
Iteration 7/25 | Loss: 0.00130361
Iteration 8/25 | Loss: 0.00130358
Iteration 9/25 | Loss: 0.00130542
Iteration 10/25 | Loss: 0.00130327
Iteration 11/25 | Loss: 0.00130350
Iteration 12/25 | Loss: 0.00130071
Iteration 13/25 | Loss: 0.00129864
Iteration 14/25 | Loss: 0.00130134
Iteration 15/25 | Loss: 0.00130043
Iteration 16/25 | Loss: 0.00129888
Iteration 17/25 | Loss: 0.00130147
Iteration 18/25 | Loss: 0.00130032
Iteration 19/25 | Loss: 0.00130016
Iteration 20/25 | Loss: 0.00130203
Iteration 21/25 | Loss: 0.00129882
Iteration 22/25 | Loss: 0.00129675
Iteration 23/25 | Loss: 0.00129637
Iteration 24/25 | Loss: 0.00129863
Iteration 25/25 | Loss: 0.00129820

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.71962905
Iteration 2/25 | Loss: 0.00092335
Iteration 3/25 | Loss: 0.00092335
Iteration 4/25 | Loss: 0.00092335
Iteration 5/25 | Loss: 0.00092335
Iteration 6/25 | Loss: 0.00092335
Iteration 7/25 | Loss: 0.00092335
Iteration 8/25 | Loss: 0.00092334
Iteration 9/25 | Loss: 0.00092334
Iteration 10/25 | Loss: 0.00092334
Iteration 11/25 | Loss: 0.00092334
Iteration 12/25 | Loss: 0.00092334
Iteration 13/25 | Loss: 0.00092334
Iteration 14/25 | Loss: 0.00092334
Iteration 15/25 | Loss: 0.00092334
Iteration 16/25 | Loss: 0.00092334
Iteration 17/25 | Loss: 0.00092334
Iteration 18/25 | Loss: 0.00092334
Iteration 19/25 | Loss: 0.00092334
Iteration 20/25 | Loss: 0.00092334
Iteration 21/25 | Loss: 0.00092334
Iteration 22/25 | Loss: 0.00092334
Iteration 23/25 | Loss: 0.00092334
Iteration 24/25 | Loss: 0.00092334
Iteration 25/25 | Loss: 0.00092334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092334
Iteration 2/1000 | Loss: 0.00010102
Iteration 3/1000 | Loss: 0.00002903
Iteration 4/1000 | Loss: 0.00002429
Iteration 5/1000 | Loss: 0.00002170
Iteration 6/1000 | Loss: 0.00001969
Iteration 7/1000 | Loss: 0.00001842
Iteration 8/1000 | Loss: 0.00001741
Iteration 9/1000 | Loss: 0.00001678
Iteration 10/1000 | Loss: 0.00001625
Iteration 11/1000 | Loss: 0.00001588
Iteration 12/1000 | Loss: 0.00001569
Iteration 13/1000 | Loss: 0.00001563
Iteration 14/1000 | Loss: 0.00001549
Iteration 15/1000 | Loss: 0.00001546
Iteration 16/1000 | Loss: 0.00001538
Iteration 17/1000 | Loss: 0.00001535
Iteration 18/1000 | Loss: 0.00001534
Iteration 19/1000 | Loss: 0.00001533
Iteration 20/1000 | Loss: 0.00001533
Iteration 21/1000 | Loss: 0.00001532
Iteration 22/1000 | Loss: 0.00001532
Iteration 23/1000 | Loss: 0.00001532
Iteration 24/1000 | Loss: 0.00001529
Iteration 25/1000 | Loss: 0.00001529
Iteration 26/1000 | Loss: 0.00001528
Iteration 27/1000 | Loss: 0.00001526
Iteration 28/1000 | Loss: 0.00001526
Iteration 29/1000 | Loss: 0.00001526
Iteration 30/1000 | Loss: 0.00001526
Iteration 31/1000 | Loss: 0.00001525
Iteration 32/1000 | Loss: 0.00001524
Iteration 33/1000 | Loss: 0.00001524
Iteration 34/1000 | Loss: 0.00001523
Iteration 35/1000 | Loss: 0.00001523
Iteration 36/1000 | Loss: 0.00001522
Iteration 37/1000 | Loss: 0.00001522
Iteration 38/1000 | Loss: 0.00001521
Iteration 39/1000 | Loss: 0.00001521
Iteration 40/1000 | Loss: 0.00001520
Iteration 41/1000 | Loss: 0.00001520
Iteration 42/1000 | Loss: 0.00001520
Iteration 43/1000 | Loss: 0.00001519
Iteration 44/1000 | Loss: 0.00001519
Iteration 45/1000 | Loss: 0.00001518
Iteration 46/1000 | Loss: 0.00001518
Iteration 47/1000 | Loss: 0.00001518
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001516
Iteration 52/1000 | Loss: 0.00001516
Iteration 53/1000 | Loss: 0.00001516
Iteration 54/1000 | Loss: 0.00001515
Iteration 55/1000 | Loss: 0.00001515
Iteration 56/1000 | Loss: 0.00001515
Iteration 57/1000 | Loss: 0.00001515
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001515
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001511
Iteration 64/1000 | Loss: 0.00001510
Iteration 65/1000 | Loss: 0.00001510
Iteration 66/1000 | Loss: 0.00001510
Iteration 67/1000 | Loss: 0.00001509
Iteration 68/1000 | Loss: 0.00001509
Iteration 69/1000 | Loss: 0.00001509
Iteration 70/1000 | Loss: 0.00001509
Iteration 71/1000 | Loss: 0.00001508
Iteration 72/1000 | Loss: 0.00001508
Iteration 73/1000 | Loss: 0.00001507
Iteration 74/1000 | Loss: 0.00001507
Iteration 75/1000 | Loss: 0.00001506
Iteration 76/1000 | Loss: 0.00001506
Iteration 77/1000 | Loss: 0.00001506
Iteration 78/1000 | Loss: 0.00001506
Iteration 79/1000 | Loss: 0.00001506
Iteration 80/1000 | Loss: 0.00001505
Iteration 81/1000 | Loss: 0.00001505
Iteration 82/1000 | Loss: 0.00001505
Iteration 83/1000 | Loss: 0.00001505
Iteration 84/1000 | Loss: 0.00001505
Iteration 85/1000 | Loss: 0.00001505
Iteration 86/1000 | Loss: 0.00001505
Iteration 87/1000 | Loss: 0.00001504
Iteration 88/1000 | Loss: 0.00001504
Iteration 89/1000 | Loss: 0.00001504
Iteration 90/1000 | Loss: 0.00001504
Iteration 91/1000 | Loss: 0.00001503
Iteration 92/1000 | Loss: 0.00001503
Iteration 93/1000 | Loss: 0.00001503
Iteration 94/1000 | Loss: 0.00001503
Iteration 95/1000 | Loss: 0.00001502
Iteration 96/1000 | Loss: 0.00001502
Iteration 97/1000 | Loss: 0.00001501
Iteration 98/1000 | Loss: 0.00001501
Iteration 99/1000 | Loss: 0.00001501
Iteration 100/1000 | Loss: 0.00001500
Iteration 101/1000 | Loss: 0.00001500
Iteration 102/1000 | Loss: 0.00001499
Iteration 103/1000 | Loss: 0.00001499
Iteration 104/1000 | Loss: 0.00001499
Iteration 105/1000 | Loss: 0.00001499
Iteration 106/1000 | Loss: 0.00001499
Iteration 107/1000 | Loss: 0.00001499
Iteration 108/1000 | Loss: 0.00001499
Iteration 109/1000 | Loss: 0.00001499
Iteration 110/1000 | Loss: 0.00001499
Iteration 111/1000 | Loss: 0.00001498
Iteration 112/1000 | Loss: 0.00001498
Iteration 113/1000 | Loss: 0.00001498
Iteration 114/1000 | Loss: 0.00001498
Iteration 115/1000 | Loss: 0.00001498
Iteration 116/1000 | Loss: 0.00001498
Iteration 117/1000 | Loss: 0.00001498
Iteration 118/1000 | Loss: 0.00001498
Iteration 119/1000 | Loss: 0.00001498
Iteration 120/1000 | Loss: 0.00001498
Iteration 121/1000 | Loss: 0.00001497
Iteration 122/1000 | Loss: 0.00001497
Iteration 123/1000 | Loss: 0.00001497
Iteration 124/1000 | Loss: 0.00001497
Iteration 125/1000 | Loss: 0.00001496
Iteration 126/1000 | Loss: 0.00001496
Iteration 127/1000 | Loss: 0.00001496
Iteration 128/1000 | Loss: 0.00001496
Iteration 129/1000 | Loss: 0.00001496
Iteration 130/1000 | Loss: 0.00001496
Iteration 131/1000 | Loss: 0.00001496
Iteration 132/1000 | Loss: 0.00001496
Iteration 133/1000 | Loss: 0.00001496
Iteration 134/1000 | Loss: 0.00001496
Iteration 135/1000 | Loss: 0.00001496
Iteration 136/1000 | Loss: 0.00001495
Iteration 137/1000 | Loss: 0.00001495
Iteration 138/1000 | Loss: 0.00001495
Iteration 139/1000 | Loss: 0.00001495
Iteration 140/1000 | Loss: 0.00001495
Iteration 141/1000 | Loss: 0.00001495
Iteration 142/1000 | Loss: 0.00001495
Iteration 143/1000 | Loss: 0.00001495
Iteration 144/1000 | Loss: 0.00001494
Iteration 145/1000 | Loss: 0.00001494
Iteration 146/1000 | Loss: 0.00001494
Iteration 147/1000 | Loss: 0.00001494
Iteration 148/1000 | Loss: 0.00001494
Iteration 149/1000 | Loss: 0.00001494
Iteration 150/1000 | Loss: 0.00001494
Iteration 151/1000 | Loss: 0.00001494
Iteration 152/1000 | Loss: 0.00001494
Iteration 153/1000 | Loss: 0.00001493
Iteration 154/1000 | Loss: 0.00001493
Iteration 155/1000 | Loss: 0.00001493
Iteration 156/1000 | Loss: 0.00001493
Iteration 157/1000 | Loss: 0.00001493
Iteration 158/1000 | Loss: 0.00001493
Iteration 159/1000 | Loss: 0.00001493
Iteration 160/1000 | Loss: 0.00001492
Iteration 161/1000 | Loss: 0.00001492
Iteration 162/1000 | Loss: 0.00001492
Iteration 163/1000 | Loss: 0.00001492
Iteration 164/1000 | Loss: 0.00001492
Iteration 165/1000 | Loss: 0.00001492
Iteration 166/1000 | Loss: 0.00001492
Iteration 167/1000 | Loss: 0.00001492
Iteration 168/1000 | Loss: 0.00001492
Iteration 169/1000 | Loss: 0.00001491
Iteration 170/1000 | Loss: 0.00001491
Iteration 171/1000 | Loss: 0.00001491
Iteration 172/1000 | Loss: 0.00001491
Iteration 173/1000 | Loss: 0.00001491
Iteration 174/1000 | Loss: 0.00001491
Iteration 175/1000 | Loss: 0.00001491
Iteration 176/1000 | Loss: 0.00001491
Iteration 177/1000 | Loss: 0.00001491
Iteration 178/1000 | Loss: 0.00001491
Iteration 179/1000 | Loss: 0.00001491
Iteration 180/1000 | Loss: 0.00001491
Iteration 181/1000 | Loss: 0.00001491
Iteration 182/1000 | Loss: 0.00001491
Iteration 183/1000 | Loss: 0.00001491
Iteration 184/1000 | Loss: 0.00001491
Iteration 185/1000 | Loss: 0.00001491
Iteration 186/1000 | Loss: 0.00001491
Iteration 187/1000 | Loss: 0.00001491
Iteration 188/1000 | Loss: 0.00001491
Iteration 189/1000 | Loss: 0.00001491
Iteration 190/1000 | Loss: 0.00001491
Iteration 191/1000 | Loss: 0.00001491
Iteration 192/1000 | Loss: 0.00001491
Iteration 193/1000 | Loss: 0.00001491
Iteration 194/1000 | Loss: 0.00001491
Iteration 195/1000 | Loss: 0.00001491
Iteration 196/1000 | Loss: 0.00001491
Iteration 197/1000 | Loss: 0.00001491
Iteration 198/1000 | Loss: 0.00001491
Iteration 199/1000 | Loss: 0.00001491
Iteration 200/1000 | Loss: 0.00001491
Iteration 201/1000 | Loss: 0.00001491
Iteration 202/1000 | Loss: 0.00001491
Iteration 203/1000 | Loss: 0.00001491
Iteration 204/1000 | Loss: 0.00001491
Iteration 205/1000 | Loss: 0.00001491
Iteration 206/1000 | Loss: 0.00001491
Iteration 207/1000 | Loss: 0.00001491
Iteration 208/1000 | Loss: 0.00001491
Iteration 209/1000 | Loss: 0.00001491
Iteration 210/1000 | Loss: 0.00001491
Iteration 211/1000 | Loss: 0.00001491
Iteration 212/1000 | Loss: 0.00001491
Iteration 213/1000 | Loss: 0.00001491
Iteration 214/1000 | Loss: 0.00001491
Iteration 215/1000 | Loss: 0.00001491
Iteration 216/1000 | Loss: 0.00001491
Iteration 217/1000 | Loss: 0.00001491
Iteration 218/1000 | Loss: 0.00001491
Iteration 219/1000 | Loss: 0.00001491
Iteration 220/1000 | Loss: 0.00001491
Iteration 221/1000 | Loss: 0.00001491
Iteration 222/1000 | Loss: 0.00001491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.4909116544004064e-05, 1.4909116544004064e-05, 1.4909116544004064e-05, 1.4909116544004064e-05, 1.4909116544004064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4909116544004064e-05

Optimization complete. Final v2v error: 3.187373161315918 mm

Highest mean error: 5.583014965057373 mm for frame 13

Lowest mean error: 2.962333917617798 mm for frame 243

Saving results

Total time: 91.93594431877136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953206
Iteration 2/25 | Loss: 0.00953205
Iteration 3/25 | Loss: 0.00953205
Iteration 4/25 | Loss: 0.00953204
Iteration 5/25 | Loss: 0.00953204
Iteration 6/25 | Loss: 0.00416612
Iteration 7/25 | Loss: 0.00395672
Iteration 8/25 | Loss: 0.00328632
Iteration 9/25 | Loss: 0.00258040
Iteration 10/25 | Loss: 0.00228603
Iteration 11/25 | Loss: 0.00218309
Iteration 12/25 | Loss: 0.00202521
Iteration 13/25 | Loss: 0.00198902
Iteration 14/25 | Loss: 0.00192343
Iteration 15/25 | Loss: 0.00190540
Iteration 16/25 | Loss: 0.00188332
Iteration 17/25 | Loss: 0.00186006
Iteration 18/25 | Loss: 0.00184793
Iteration 19/25 | Loss: 0.00183670
Iteration 20/25 | Loss: 0.00185376
Iteration 21/25 | Loss: 0.00183027
Iteration 22/25 | Loss: 0.00182929
Iteration 23/25 | Loss: 0.00182900
Iteration 24/25 | Loss: 0.00182840
Iteration 25/25 | Loss: 0.00182776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37750900
Iteration 2/25 | Loss: 0.01357431
Iteration 3/25 | Loss: 0.00335330
Iteration 4/25 | Loss: 0.00335256
Iteration 5/25 | Loss: 0.00335255
Iteration 6/25 | Loss: 0.00335255
Iteration 7/25 | Loss: 0.00335255
Iteration 8/25 | Loss: 0.00335255
Iteration 9/25 | Loss: 0.00335255
Iteration 10/25 | Loss: 0.00335255
Iteration 11/25 | Loss: 0.00335255
Iteration 12/25 | Loss: 0.00335255
Iteration 13/25 | Loss: 0.00335255
Iteration 14/25 | Loss: 0.00335255
Iteration 15/25 | Loss: 0.00335255
Iteration 16/25 | Loss: 0.00335255
Iteration 17/25 | Loss: 0.00335255
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003352551022544503, 0.003352551022544503, 0.003352551022544503, 0.003352551022544503, 0.003352551022544503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003352551022544503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00335255
Iteration 2/1000 | Loss: 0.00287530
Iteration 3/1000 | Loss: 0.00055175
Iteration 4/1000 | Loss: 0.00043756
Iteration 5/1000 | Loss: 0.00079578
Iteration 6/1000 | Loss: 0.00215393
Iteration 7/1000 | Loss: 0.00029660
Iteration 8/1000 | Loss: 0.00114491
Iteration 9/1000 | Loss: 0.00042133
Iteration 10/1000 | Loss: 0.00024852
Iteration 11/1000 | Loss: 0.00057992
Iteration 12/1000 | Loss: 0.00023593
Iteration 13/1000 | Loss: 0.00099887
Iteration 14/1000 | Loss: 0.01713520
Iteration 15/1000 | Loss: 0.00626909
Iteration 16/1000 | Loss: 0.02113284
Iteration 17/1000 | Loss: 0.00550757
Iteration 18/1000 | Loss: 0.00547052
Iteration 19/1000 | Loss: 0.00128441
Iteration 20/1000 | Loss: 0.00497097
Iteration 21/1000 | Loss: 0.00100320
Iteration 22/1000 | Loss: 0.00050038
Iteration 23/1000 | Loss: 0.00116132
Iteration 24/1000 | Loss: 0.00038245
Iteration 25/1000 | Loss: 0.00489304
Iteration 26/1000 | Loss: 0.00120310
Iteration 27/1000 | Loss: 0.00217375
Iteration 28/1000 | Loss: 0.00034416
Iteration 29/1000 | Loss: 0.00022044
Iteration 30/1000 | Loss: 0.00025701
Iteration 31/1000 | Loss: 0.00013890
Iteration 32/1000 | Loss: 0.00119923
Iteration 33/1000 | Loss: 0.00073535
Iteration 34/1000 | Loss: 0.00012818
Iteration 35/1000 | Loss: 0.00009874
Iteration 36/1000 | Loss: 0.00005132
Iteration 37/1000 | Loss: 0.00013305
Iteration 38/1000 | Loss: 0.00004174
Iteration 39/1000 | Loss: 0.00038066
Iteration 40/1000 | Loss: 0.00003644
Iteration 41/1000 | Loss: 0.00003185
Iteration 42/1000 | Loss: 0.00119310
Iteration 43/1000 | Loss: 0.00044946
Iteration 44/1000 | Loss: 0.00460571
Iteration 45/1000 | Loss: 0.00406595
Iteration 46/1000 | Loss: 0.00241987
Iteration 47/1000 | Loss: 0.00016954
Iteration 48/1000 | Loss: 0.00189648
Iteration 49/1000 | Loss: 0.00004254
Iteration 50/1000 | Loss: 0.00002991
Iteration 51/1000 | Loss: 0.00002573
Iteration 52/1000 | Loss: 0.00004260
Iteration 53/1000 | Loss: 0.00002464
Iteration 54/1000 | Loss: 0.00031736
Iteration 55/1000 | Loss: 0.00057983
Iteration 56/1000 | Loss: 0.00002893
Iteration 57/1000 | Loss: 0.00003016
Iteration 58/1000 | Loss: 0.00048380
Iteration 59/1000 | Loss: 0.00155094
Iteration 60/1000 | Loss: 0.00236471
Iteration 61/1000 | Loss: 0.00013987
Iteration 62/1000 | Loss: 0.00046110
Iteration 63/1000 | Loss: 0.00079889
Iteration 64/1000 | Loss: 0.00068016
Iteration 65/1000 | Loss: 0.00080705
Iteration 66/1000 | Loss: 0.00058638
Iteration 67/1000 | Loss: 0.00021266
Iteration 68/1000 | Loss: 0.00051196
Iteration 69/1000 | Loss: 0.00039700
Iteration 70/1000 | Loss: 0.00078772
Iteration 71/1000 | Loss: 0.00073906
Iteration 72/1000 | Loss: 0.00035755
Iteration 73/1000 | Loss: 0.00007907
Iteration 74/1000 | Loss: 0.00078356
Iteration 75/1000 | Loss: 0.00005901
Iteration 76/1000 | Loss: 0.00004050
Iteration 77/1000 | Loss: 0.00010333
Iteration 78/1000 | Loss: 0.00003447
Iteration 79/1000 | Loss: 0.00069629
Iteration 80/1000 | Loss: 0.00063240
Iteration 81/1000 | Loss: 0.00026608
Iteration 82/1000 | Loss: 0.00032400
Iteration 83/1000 | Loss: 0.00003440
Iteration 84/1000 | Loss: 0.00002797
Iteration 85/1000 | Loss: 0.00049115
Iteration 86/1000 | Loss: 0.00030583
Iteration 87/1000 | Loss: 0.00056800
Iteration 88/1000 | Loss: 0.00036478
Iteration 89/1000 | Loss: 0.00052200
Iteration 90/1000 | Loss: 0.00085532
Iteration 91/1000 | Loss: 0.00149367
Iteration 92/1000 | Loss: 0.00047752
Iteration 93/1000 | Loss: 0.00043969
Iteration 94/1000 | Loss: 0.00020141
Iteration 95/1000 | Loss: 0.00004405
Iteration 96/1000 | Loss: 0.00006178
Iteration 97/1000 | Loss: 0.00002808
Iteration 98/1000 | Loss: 0.00002848
Iteration 99/1000 | Loss: 0.00002565
Iteration 100/1000 | Loss: 0.00087597
Iteration 101/1000 | Loss: 0.00047756
Iteration 102/1000 | Loss: 0.00077869
Iteration 103/1000 | Loss: 0.00129836
Iteration 104/1000 | Loss: 0.00358373
Iteration 105/1000 | Loss: 0.00031153
Iteration 106/1000 | Loss: 0.00041473
Iteration 107/1000 | Loss: 0.00044882
Iteration 108/1000 | Loss: 0.00036357
Iteration 109/1000 | Loss: 0.00063291
Iteration 110/1000 | Loss: 0.00026398
Iteration 111/1000 | Loss: 0.00048365
Iteration 112/1000 | Loss: 0.00185835
Iteration 113/1000 | Loss: 0.00124571
Iteration 114/1000 | Loss: 0.00007123
Iteration 115/1000 | Loss: 0.00033231
Iteration 116/1000 | Loss: 0.00004405
Iteration 117/1000 | Loss: 0.00005488
Iteration 118/1000 | Loss: 0.00005514
Iteration 119/1000 | Loss: 0.00002599
Iteration 120/1000 | Loss: 0.00006552
Iteration 121/1000 | Loss: 0.00002441
Iteration 122/1000 | Loss: 0.00002375
Iteration 123/1000 | Loss: 0.00002346
Iteration 124/1000 | Loss: 0.00002346
Iteration 125/1000 | Loss: 0.00019471
Iteration 126/1000 | Loss: 0.00003065
Iteration 127/1000 | Loss: 0.00002687
Iteration 128/1000 | Loss: 0.00002278
Iteration 129/1000 | Loss: 0.00123420
Iteration 130/1000 | Loss: 0.00027987
Iteration 131/1000 | Loss: 0.00148434
Iteration 132/1000 | Loss: 0.00012023
Iteration 133/1000 | Loss: 0.00004637
Iteration 134/1000 | Loss: 0.00024305
Iteration 135/1000 | Loss: 0.00003349
Iteration 136/1000 | Loss: 0.00016226
Iteration 137/1000 | Loss: 0.00024054
Iteration 138/1000 | Loss: 0.00004621
Iteration 139/1000 | Loss: 0.00004254
Iteration 140/1000 | Loss: 0.00006880
Iteration 141/1000 | Loss: 0.00010033
Iteration 142/1000 | Loss: 0.00004957
Iteration 143/1000 | Loss: 0.00002786
Iteration 144/1000 | Loss: 0.00011385
Iteration 145/1000 | Loss: 0.00029107
Iteration 146/1000 | Loss: 0.00008424
Iteration 147/1000 | Loss: 0.00003093
Iteration 148/1000 | Loss: 0.00002348
Iteration 149/1000 | Loss: 0.00018764
Iteration 150/1000 | Loss: 0.00002184
Iteration 151/1000 | Loss: 0.00011518
Iteration 152/1000 | Loss: 0.00002129
Iteration 153/1000 | Loss: 0.00002079
Iteration 154/1000 | Loss: 0.00002055
Iteration 155/1000 | Loss: 0.00002043
Iteration 156/1000 | Loss: 0.00002042
Iteration 157/1000 | Loss: 0.00008772
Iteration 158/1000 | Loss: 0.00002294
Iteration 159/1000 | Loss: 0.00015236
Iteration 160/1000 | Loss: 0.00064812
Iteration 161/1000 | Loss: 0.00051468
Iteration 162/1000 | Loss: 0.00010420
Iteration 163/1000 | Loss: 0.00002022
Iteration 164/1000 | Loss: 0.00001939
Iteration 165/1000 | Loss: 0.00001905
Iteration 166/1000 | Loss: 0.00001894
Iteration 167/1000 | Loss: 0.00001888
Iteration 168/1000 | Loss: 0.00001879
Iteration 169/1000 | Loss: 0.00015868
Iteration 170/1000 | Loss: 0.00011312
Iteration 171/1000 | Loss: 0.00001962
Iteration 172/1000 | Loss: 0.00001876
Iteration 173/1000 | Loss: 0.00001853
Iteration 174/1000 | Loss: 0.00001851
Iteration 175/1000 | Loss: 0.00001846
Iteration 176/1000 | Loss: 0.00001846
Iteration 177/1000 | Loss: 0.00001845
Iteration 178/1000 | Loss: 0.00001844
Iteration 179/1000 | Loss: 0.00001844
Iteration 180/1000 | Loss: 0.00001844
Iteration 181/1000 | Loss: 0.00001843
Iteration 182/1000 | Loss: 0.00001843
Iteration 183/1000 | Loss: 0.00001843
Iteration 184/1000 | Loss: 0.00001842
Iteration 185/1000 | Loss: 0.00001842
Iteration 186/1000 | Loss: 0.00001842
Iteration 187/1000 | Loss: 0.00001842
Iteration 188/1000 | Loss: 0.00001842
Iteration 189/1000 | Loss: 0.00001842
Iteration 190/1000 | Loss: 0.00001841
Iteration 191/1000 | Loss: 0.00001841
Iteration 192/1000 | Loss: 0.00001841
Iteration 193/1000 | Loss: 0.00001841
Iteration 194/1000 | Loss: 0.00001841
Iteration 195/1000 | Loss: 0.00001841
Iteration 196/1000 | Loss: 0.00001838
Iteration 197/1000 | Loss: 0.00001837
Iteration 198/1000 | Loss: 0.00001837
Iteration 199/1000 | Loss: 0.00001837
Iteration 200/1000 | Loss: 0.00001836
Iteration 201/1000 | Loss: 0.00001836
Iteration 202/1000 | Loss: 0.00001835
Iteration 203/1000 | Loss: 0.00001834
Iteration 204/1000 | Loss: 0.00001834
Iteration 205/1000 | Loss: 0.00001834
Iteration 206/1000 | Loss: 0.00001834
Iteration 207/1000 | Loss: 0.00001833
Iteration 208/1000 | Loss: 0.00001833
Iteration 209/1000 | Loss: 0.00001833
Iteration 210/1000 | Loss: 0.00001833
Iteration 211/1000 | Loss: 0.00001833
Iteration 212/1000 | Loss: 0.00001833
Iteration 213/1000 | Loss: 0.00001833
Iteration 214/1000 | Loss: 0.00001832
Iteration 215/1000 | Loss: 0.00001831
Iteration 216/1000 | Loss: 0.00001830
Iteration 217/1000 | Loss: 0.00001830
Iteration 218/1000 | Loss: 0.00001830
Iteration 219/1000 | Loss: 0.00001829
Iteration 220/1000 | Loss: 0.00001829
Iteration 221/1000 | Loss: 0.00001828
Iteration 222/1000 | Loss: 0.00001828
Iteration 223/1000 | Loss: 0.00001828
Iteration 224/1000 | Loss: 0.00001828
Iteration 225/1000 | Loss: 0.00001828
Iteration 226/1000 | Loss: 0.00001827
Iteration 227/1000 | Loss: 0.00001827
Iteration 228/1000 | Loss: 0.00001827
Iteration 229/1000 | Loss: 0.00001827
Iteration 230/1000 | Loss: 0.00001827
Iteration 231/1000 | Loss: 0.00001826
Iteration 232/1000 | Loss: 0.00001826
Iteration 233/1000 | Loss: 0.00001826
Iteration 234/1000 | Loss: 0.00001825
Iteration 235/1000 | Loss: 0.00001825
Iteration 236/1000 | Loss: 0.00001824
Iteration 237/1000 | Loss: 0.00001824
Iteration 238/1000 | Loss: 0.00001824
Iteration 239/1000 | Loss: 0.00001824
Iteration 240/1000 | Loss: 0.00001824
Iteration 241/1000 | Loss: 0.00001824
Iteration 242/1000 | Loss: 0.00001824
Iteration 243/1000 | Loss: 0.00001824
Iteration 244/1000 | Loss: 0.00001823
Iteration 245/1000 | Loss: 0.00001822
Iteration 246/1000 | Loss: 0.00017041
Iteration 247/1000 | Loss: 0.00002962
Iteration 248/1000 | Loss: 0.00003294
Iteration 249/1000 | Loss: 0.00001834
Iteration 250/1000 | Loss: 0.00001822
Iteration 251/1000 | Loss: 0.00001818
Iteration 252/1000 | Loss: 0.00001817
Iteration 253/1000 | Loss: 0.00001817
Iteration 254/1000 | Loss: 0.00001816
Iteration 255/1000 | Loss: 0.00001815
Iteration 256/1000 | Loss: 0.00001815
Iteration 257/1000 | Loss: 0.00001815
Iteration 258/1000 | Loss: 0.00001815
Iteration 259/1000 | Loss: 0.00001815
Iteration 260/1000 | Loss: 0.00001815
Iteration 261/1000 | Loss: 0.00001815
Iteration 262/1000 | Loss: 0.00001814
Iteration 263/1000 | Loss: 0.00001814
Iteration 264/1000 | Loss: 0.00001813
Iteration 265/1000 | Loss: 0.00001813
Iteration 266/1000 | Loss: 0.00001812
Iteration 267/1000 | Loss: 0.00001812
Iteration 268/1000 | Loss: 0.00001811
Iteration 269/1000 | Loss: 0.00001811
Iteration 270/1000 | Loss: 0.00001810
Iteration 271/1000 | Loss: 0.00001810
Iteration 272/1000 | Loss: 0.00001809
Iteration 273/1000 | Loss: 0.00001809
Iteration 274/1000 | Loss: 0.00001809
Iteration 275/1000 | Loss: 0.00001809
Iteration 276/1000 | Loss: 0.00001809
Iteration 277/1000 | Loss: 0.00001809
Iteration 278/1000 | Loss: 0.00001808
Iteration 279/1000 | Loss: 0.00001808
Iteration 280/1000 | Loss: 0.00001808
Iteration 281/1000 | Loss: 0.00001808
Iteration 282/1000 | Loss: 0.00001808
Iteration 283/1000 | Loss: 0.00001808
Iteration 284/1000 | Loss: 0.00001808
Iteration 285/1000 | Loss: 0.00001808
Iteration 286/1000 | Loss: 0.00001808
Iteration 287/1000 | Loss: 0.00001808
Iteration 288/1000 | Loss: 0.00001808
Iteration 289/1000 | Loss: 0.00001808
Iteration 290/1000 | Loss: 0.00001808
Iteration 291/1000 | Loss: 0.00001808
Iteration 292/1000 | Loss: 0.00001808
Iteration 293/1000 | Loss: 0.00001808
Iteration 294/1000 | Loss: 0.00001807
Iteration 295/1000 | Loss: 0.00001807
Iteration 296/1000 | Loss: 0.00001807
Iteration 297/1000 | Loss: 0.00001807
Iteration 298/1000 | Loss: 0.00001807
Iteration 299/1000 | Loss: 0.00001806
Iteration 300/1000 | Loss: 0.00001806
Iteration 301/1000 | Loss: 0.00001806
Iteration 302/1000 | Loss: 0.00001806
Iteration 303/1000 | Loss: 0.00001806
Iteration 304/1000 | Loss: 0.00001806
Iteration 305/1000 | Loss: 0.00001806
Iteration 306/1000 | Loss: 0.00001806
Iteration 307/1000 | Loss: 0.00001805
Iteration 308/1000 | Loss: 0.00001805
Iteration 309/1000 | Loss: 0.00001805
Iteration 310/1000 | Loss: 0.00001805
Iteration 311/1000 | Loss: 0.00001805
Iteration 312/1000 | Loss: 0.00001805
Iteration 313/1000 | Loss: 0.00001805
Iteration 314/1000 | Loss: 0.00001805
Iteration 315/1000 | Loss: 0.00001805
Iteration 316/1000 | Loss: 0.00001805
Iteration 317/1000 | Loss: 0.00001804
Iteration 318/1000 | Loss: 0.00001804
Iteration 319/1000 | Loss: 0.00001804
Iteration 320/1000 | Loss: 0.00001804
Iteration 321/1000 | Loss: 0.00001804
Iteration 322/1000 | Loss: 0.00001804
Iteration 323/1000 | Loss: 0.00001804
Iteration 324/1000 | Loss: 0.00001803
Iteration 325/1000 | Loss: 0.00001803
Iteration 326/1000 | Loss: 0.00001803
Iteration 327/1000 | Loss: 0.00001803
Iteration 328/1000 | Loss: 0.00001803
Iteration 329/1000 | Loss: 0.00001803
Iteration 330/1000 | Loss: 0.00001803
Iteration 331/1000 | Loss: 0.00001803
Iteration 332/1000 | Loss: 0.00001803
Iteration 333/1000 | Loss: 0.00001803
Iteration 334/1000 | Loss: 0.00001803
Iteration 335/1000 | Loss: 0.00001803
Iteration 336/1000 | Loss: 0.00001803
Iteration 337/1000 | Loss: 0.00001803
Iteration 338/1000 | Loss: 0.00001803
Iteration 339/1000 | Loss: 0.00001802
Iteration 340/1000 | Loss: 0.00001802
Iteration 341/1000 | Loss: 0.00001802
Iteration 342/1000 | Loss: 0.00001802
Iteration 343/1000 | Loss: 0.00001802
Iteration 344/1000 | Loss: 0.00001802
Iteration 345/1000 | Loss: 0.00001802
Iteration 346/1000 | Loss: 0.00001802
Iteration 347/1000 | Loss: 0.00001802
Iteration 348/1000 | Loss: 0.00001802
Iteration 349/1000 | Loss: 0.00001802
Iteration 350/1000 | Loss: 0.00001802
Iteration 351/1000 | Loss: 0.00001802
Iteration 352/1000 | Loss: 0.00001802
Iteration 353/1000 | Loss: 0.00001801
Iteration 354/1000 | Loss: 0.00001801
Iteration 355/1000 | Loss: 0.00001801
Iteration 356/1000 | Loss: 0.00001801
Iteration 357/1000 | Loss: 0.00001801
Iteration 358/1000 | Loss: 0.00001801
Iteration 359/1000 | Loss: 0.00001800
Iteration 360/1000 | Loss: 0.00001800
Iteration 361/1000 | Loss: 0.00001800
Iteration 362/1000 | Loss: 0.00001800
Iteration 363/1000 | Loss: 0.00001800
Iteration 364/1000 | Loss: 0.00001800
Iteration 365/1000 | Loss: 0.00001800
Iteration 366/1000 | Loss: 0.00001800
Iteration 367/1000 | Loss: 0.00001800
Iteration 368/1000 | Loss: 0.00001800
Iteration 369/1000 | Loss: 0.00001800
Iteration 370/1000 | Loss: 0.00001800
Iteration 371/1000 | Loss: 0.00001799
Iteration 372/1000 | Loss: 0.00001799
Iteration 373/1000 | Loss: 0.00001799
Iteration 374/1000 | Loss: 0.00001799
Iteration 375/1000 | Loss: 0.00001799
Iteration 376/1000 | Loss: 0.00001799
Iteration 377/1000 | Loss: 0.00001799
Iteration 378/1000 | Loss: 0.00001799
Iteration 379/1000 | Loss: 0.00001798
Iteration 380/1000 | Loss: 0.00001798
Iteration 381/1000 | Loss: 0.00001798
Iteration 382/1000 | Loss: 0.00001798
Iteration 383/1000 | Loss: 0.00001798
Iteration 384/1000 | Loss: 0.00001798
Iteration 385/1000 | Loss: 0.00001798
Iteration 386/1000 | Loss: 0.00001798
Iteration 387/1000 | Loss: 0.00001798
Iteration 388/1000 | Loss: 0.00001798
Iteration 389/1000 | Loss: 0.00001797
Iteration 390/1000 | Loss: 0.00001797
Iteration 391/1000 | Loss: 0.00001797
Iteration 392/1000 | Loss: 0.00001796
Iteration 393/1000 | Loss: 0.00001796
Iteration 394/1000 | Loss: 0.00001796
Iteration 395/1000 | Loss: 0.00001796
Iteration 396/1000 | Loss: 0.00001796
Iteration 397/1000 | Loss: 0.00001795
Iteration 398/1000 | Loss: 0.00001795
Iteration 399/1000 | Loss: 0.00001795
Iteration 400/1000 | Loss: 0.00001795
Iteration 401/1000 | Loss: 0.00001795
Iteration 402/1000 | Loss: 0.00001795
Iteration 403/1000 | Loss: 0.00001795
Iteration 404/1000 | Loss: 0.00001794
Iteration 405/1000 | Loss: 0.00001793
Iteration 406/1000 | Loss: 0.00001793
Iteration 407/1000 | Loss: 0.00001792
Iteration 408/1000 | Loss: 0.00001792
Iteration 409/1000 | Loss: 0.00001792
Iteration 410/1000 | Loss: 0.00001792
Iteration 411/1000 | Loss: 0.00001792
Iteration 412/1000 | Loss: 0.00001791
Iteration 413/1000 | Loss: 0.00001791
Iteration 414/1000 | Loss: 0.00001791
Iteration 415/1000 | Loss: 0.00001791
Iteration 416/1000 | Loss: 0.00001791
Iteration 417/1000 | Loss: 0.00001791
Iteration 418/1000 | Loss: 0.00001791
Iteration 419/1000 | Loss: 0.00001791
Iteration 420/1000 | Loss: 0.00001791
Iteration 421/1000 | Loss: 0.00019182
Iteration 422/1000 | Loss: 0.00001815
Iteration 423/1000 | Loss: 0.00001792
Iteration 424/1000 | Loss: 0.00001792
Iteration 425/1000 | Loss: 0.00001789
Iteration 426/1000 | Loss: 0.00001789
Iteration 427/1000 | Loss: 0.00001789
Iteration 428/1000 | Loss: 0.00001789
Iteration 429/1000 | Loss: 0.00001789
Iteration 430/1000 | Loss: 0.00001789
Iteration 431/1000 | Loss: 0.00001789
Iteration 432/1000 | Loss: 0.00001788
Iteration 433/1000 | Loss: 0.00001788
Iteration 434/1000 | Loss: 0.00001788
Iteration 435/1000 | Loss: 0.00001788
Iteration 436/1000 | Loss: 0.00001788
Iteration 437/1000 | Loss: 0.00001788
Iteration 438/1000 | Loss: 0.00001788
Iteration 439/1000 | Loss: 0.00001788
Iteration 440/1000 | Loss: 0.00001788
Iteration 441/1000 | Loss: 0.00001787
Iteration 442/1000 | Loss: 0.00001787
Iteration 443/1000 | Loss: 0.00001787
Iteration 444/1000 | Loss: 0.00001786
Iteration 445/1000 | Loss: 0.00001786
Iteration 446/1000 | Loss: 0.00001786
Iteration 447/1000 | Loss: 0.00001786
Iteration 448/1000 | Loss: 0.00001786
Iteration 449/1000 | Loss: 0.00001786
Iteration 450/1000 | Loss: 0.00001786
Iteration 451/1000 | Loss: 0.00001786
Iteration 452/1000 | Loss: 0.00001786
Iteration 453/1000 | Loss: 0.00001786
Iteration 454/1000 | Loss: 0.00001786
Iteration 455/1000 | Loss: 0.00001786
Iteration 456/1000 | Loss: 0.00001786
Iteration 457/1000 | Loss: 0.00001786
Iteration 458/1000 | Loss: 0.00001786
Iteration 459/1000 | Loss: 0.00001786
Iteration 460/1000 | Loss: 0.00001786
Iteration 461/1000 | Loss: 0.00001786
Iteration 462/1000 | Loss: 0.00001786
Iteration 463/1000 | Loss: 0.00001786
Iteration 464/1000 | Loss: 0.00001786
Iteration 465/1000 | Loss: 0.00001786
Iteration 466/1000 | Loss: 0.00001786
Iteration 467/1000 | Loss: 0.00001786
Iteration 468/1000 | Loss: 0.00001786
Iteration 469/1000 | Loss: 0.00001786
Iteration 470/1000 | Loss: 0.00001786
Iteration 471/1000 | Loss: 0.00001786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 471. Stopping optimization.
Last 5 losses: [1.7857426428236067e-05, 1.7857426428236067e-05, 1.7857426428236067e-05, 1.7857426428236067e-05, 1.7857426428236067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7857426428236067e-05

Optimization complete. Final v2v error: 3.5295870304107666 mm

Highest mean error: 4.4674859046936035 mm for frame 30

Lowest mean error: 3.4311561584472656 mm for frame 129

Saving results

Total time: 351.88489151000977
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360608
Iteration 2/25 | Loss: 0.00142430
Iteration 3/25 | Loss: 0.00127942
Iteration 4/25 | Loss: 0.00126559
Iteration 5/25 | Loss: 0.00126125
Iteration 6/25 | Loss: 0.00126050
Iteration 7/25 | Loss: 0.00126050
Iteration 8/25 | Loss: 0.00126050
Iteration 9/25 | Loss: 0.00126050
Iteration 10/25 | Loss: 0.00126050
Iteration 11/25 | Loss: 0.00126050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012605029623955488, 0.0012605029623955488, 0.0012605029623955488, 0.0012605029623955488, 0.0012605029623955488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012605029623955488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40162241
Iteration 2/25 | Loss: 0.00075637
Iteration 3/25 | Loss: 0.00075637
Iteration 4/25 | Loss: 0.00075637
Iteration 5/25 | Loss: 0.00075637
Iteration 6/25 | Loss: 0.00075637
Iteration 7/25 | Loss: 0.00075637
Iteration 8/25 | Loss: 0.00075637
Iteration 9/25 | Loss: 0.00075637
Iteration 10/25 | Loss: 0.00075637
Iteration 11/25 | Loss: 0.00075637
Iteration 12/25 | Loss: 0.00075637
Iteration 13/25 | Loss: 0.00075637
Iteration 14/25 | Loss: 0.00075637
Iteration 15/25 | Loss: 0.00075637
Iteration 16/25 | Loss: 0.00075637
Iteration 17/25 | Loss: 0.00075637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000756368855945766, 0.000756368855945766, 0.000756368855945766, 0.000756368855945766, 0.000756368855945766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000756368855945766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075637
Iteration 2/1000 | Loss: 0.00003713
Iteration 3/1000 | Loss: 0.00002504
Iteration 4/1000 | Loss: 0.00002077
Iteration 5/1000 | Loss: 0.00001934
Iteration 6/1000 | Loss: 0.00001829
Iteration 7/1000 | Loss: 0.00001754
Iteration 8/1000 | Loss: 0.00001705
Iteration 9/1000 | Loss: 0.00001676
Iteration 10/1000 | Loss: 0.00001649
Iteration 11/1000 | Loss: 0.00001616
Iteration 12/1000 | Loss: 0.00001600
Iteration 13/1000 | Loss: 0.00001591
Iteration 14/1000 | Loss: 0.00001591
Iteration 15/1000 | Loss: 0.00001579
Iteration 16/1000 | Loss: 0.00001577
Iteration 17/1000 | Loss: 0.00001573
Iteration 18/1000 | Loss: 0.00001567
Iteration 19/1000 | Loss: 0.00001564
Iteration 20/1000 | Loss: 0.00001561
Iteration 21/1000 | Loss: 0.00001557
Iteration 22/1000 | Loss: 0.00001557
Iteration 23/1000 | Loss: 0.00001555
Iteration 24/1000 | Loss: 0.00001555
Iteration 25/1000 | Loss: 0.00001554
Iteration 26/1000 | Loss: 0.00001551
Iteration 27/1000 | Loss: 0.00001550
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001546
Iteration 30/1000 | Loss: 0.00001545
Iteration 31/1000 | Loss: 0.00001543
Iteration 32/1000 | Loss: 0.00001543
Iteration 33/1000 | Loss: 0.00001542
Iteration 34/1000 | Loss: 0.00001540
Iteration 35/1000 | Loss: 0.00001540
Iteration 36/1000 | Loss: 0.00001539
Iteration 37/1000 | Loss: 0.00001538
Iteration 38/1000 | Loss: 0.00001538
Iteration 39/1000 | Loss: 0.00001537
Iteration 40/1000 | Loss: 0.00001537
Iteration 41/1000 | Loss: 0.00001537
Iteration 42/1000 | Loss: 0.00001537
Iteration 43/1000 | Loss: 0.00001536
Iteration 44/1000 | Loss: 0.00001536
Iteration 45/1000 | Loss: 0.00001536
Iteration 46/1000 | Loss: 0.00001536
Iteration 47/1000 | Loss: 0.00001535
Iteration 48/1000 | Loss: 0.00001535
Iteration 49/1000 | Loss: 0.00001535
Iteration 50/1000 | Loss: 0.00001534
Iteration 51/1000 | Loss: 0.00001534
Iteration 52/1000 | Loss: 0.00001534
Iteration 53/1000 | Loss: 0.00001534
Iteration 54/1000 | Loss: 0.00001534
Iteration 55/1000 | Loss: 0.00001534
Iteration 56/1000 | Loss: 0.00001533
Iteration 57/1000 | Loss: 0.00001533
Iteration 58/1000 | Loss: 0.00001533
Iteration 59/1000 | Loss: 0.00001533
Iteration 60/1000 | Loss: 0.00001533
Iteration 61/1000 | Loss: 0.00001533
Iteration 62/1000 | Loss: 0.00001533
Iteration 63/1000 | Loss: 0.00001533
Iteration 64/1000 | Loss: 0.00001532
Iteration 65/1000 | Loss: 0.00001532
Iteration 66/1000 | Loss: 0.00001532
Iteration 67/1000 | Loss: 0.00001532
Iteration 68/1000 | Loss: 0.00001532
Iteration 69/1000 | Loss: 0.00001532
Iteration 70/1000 | Loss: 0.00001532
Iteration 71/1000 | Loss: 0.00001532
Iteration 72/1000 | Loss: 0.00001532
Iteration 73/1000 | Loss: 0.00001532
Iteration 74/1000 | Loss: 0.00001532
Iteration 75/1000 | Loss: 0.00001532
Iteration 76/1000 | Loss: 0.00001532
Iteration 77/1000 | Loss: 0.00001532
Iteration 78/1000 | Loss: 0.00001532
Iteration 79/1000 | Loss: 0.00001532
Iteration 80/1000 | Loss: 0.00001532
Iteration 81/1000 | Loss: 0.00001532
Iteration 82/1000 | Loss: 0.00001532
Iteration 83/1000 | Loss: 0.00001532
Iteration 84/1000 | Loss: 0.00001532
Iteration 85/1000 | Loss: 0.00001532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.531573616375681e-05, 1.531573616375681e-05, 1.531573616375681e-05, 1.531573616375681e-05, 1.531573616375681e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.531573616375681e-05

Optimization complete. Final v2v error: 3.313488245010376 mm

Highest mean error: 3.7141900062561035 mm for frame 203

Lowest mean error: 2.9874820709228516 mm for frame 220

Saving results

Total time: 39.84414100646973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404657
Iteration 2/25 | Loss: 0.00133601
Iteration 3/25 | Loss: 0.00128017
Iteration 4/25 | Loss: 0.00126786
Iteration 5/25 | Loss: 0.00126512
Iteration 6/25 | Loss: 0.00126512
Iteration 7/25 | Loss: 0.00126512
Iteration 8/25 | Loss: 0.00126512
Iteration 9/25 | Loss: 0.00126512
Iteration 10/25 | Loss: 0.00126512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012651243014261127, 0.0012651243014261127, 0.0012651243014261127, 0.0012651243014261127, 0.0012651243014261127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012651243014261127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56330848
Iteration 2/25 | Loss: 0.00080790
Iteration 3/25 | Loss: 0.00080790
Iteration 4/25 | Loss: 0.00080790
Iteration 5/25 | Loss: 0.00080789
Iteration 6/25 | Loss: 0.00080789
Iteration 7/25 | Loss: 0.00080789
Iteration 8/25 | Loss: 0.00080789
Iteration 9/25 | Loss: 0.00080789
Iteration 10/25 | Loss: 0.00080789
Iteration 11/25 | Loss: 0.00080789
Iteration 12/25 | Loss: 0.00080789
Iteration 13/25 | Loss: 0.00080789
Iteration 14/25 | Loss: 0.00080789
Iteration 15/25 | Loss: 0.00080789
Iteration 16/25 | Loss: 0.00080789
Iteration 17/25 | Loss: 0.00080789
Iteration 18/25 | Loss: 0.00080789
Iteration 19/25 | Loss: 0.00080789
Iteration 20/25 | Loss: 0.00080789
Iteration 21/25 | Loss: 0.00080789
Iteration 22/25 | Loss: 0.00080789
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008078927639871836, 0.0008078927639871836, 0.0008078927639871836, 0.0008078927639871836, 0.0008078927639871836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008078927639871836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080789
Iteration 2/1000 | Loss: 0.00002419
Iteration 3/1000 | Loss: 0.00001868
Iteration 4/1000 | Loss: 0.00001747
Iteration 5/1000 | Loss: 0.00001680
Iteration 6/1000 | Loss: 0.00001633
Iteration 7/1000 | Loss: 0.00001609
Iteration 8/1000 | Loss: 0.00001567
Iteration 9/1000 | Loss: 0.00001541
Iteration 10/1000 | Loss: 0.00001535
Iteration 11/1000 | Loss: 0.00001534
Iteration 12/1000 | Loss: 0.00001527
Iteration 13/1000 | Loss: 0.00001525
Iteration 14/1000 | Loss: 0.00001525
Iteration 15/1000 | Loss: 0.00001524
Iteration 16/1000 | Loss: 0.00001516
Iteration 17/1000 | Loss: 0.00001507
Iteration 18/1000 | Loss: 0.00001507
Iteration 19/1000 | Loss: 0.00001503
Iteration 20/1000 | Loss: 0.00001502
Iteration 21/1000 | Loss: 0.00001502
Iteration 22/1000 | Loss: 0.00001501
Iteration 23/1000 | Loss: 0.00001500
Iteration 24/1000 | Loss: 0.00001499
Iteration 25/1000 | Loss: 0.00001499
Iteration 26/1000 | Loss: 0.00001498
Iteration 27/1000 | Loss: 0.00001498
Iteration 28/1000 | Loss: 0.00001498
Iteration 29/1000 | Loss: 0.00001497
Iteration 30/1000 | Loss: 0.00001497
Iteration 31/1000 | Loss: 0.00001496
Iteration 32/1000 | Loss: 0.00001495
Iteration 33/1000 | Loss: 0.00001494
Iteration 34/1000 | Loss: 0.00001494
Iteration 35/1000 | Loss: 0.00001494
Iteration 36/1000 | Loss: 0.00001493
Iteration 37/1000 | Loss: 0.00001493
Iteration 38/1000 | Loss: 0.00001493
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001492
Iteration 41/1000 | Loss: 0.00001492
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001491
Iteration 44/1000 | Loss: 0.00001491
Iteration 45/1000 | Loss: 0.00001490
Iteration 46/1000 | Loss: 0.00001489
Iteration 47/1000 | Loss: 0.00001489
Iteration 48/1000 | Loss: 0.00001488
Iteration 49/1000 | Loss: 0.00001487
Iteration 50/1000 | Loss: 0.00001486
Iteration 51/1000 | Loss: 0.00001485
Iteration 52/1000 | Loss: 0.00001481
Iteration 53/1000 | Loss: 0.00001477
Iteration 54/1000 | Loss: 0.00001477
Iteration 55/1000 | Loss: 0.00001477
Iteration 56/1000 | Loss: 0.00001477
Iteration 57/1000 | Loss: 0.00001476
Iteration 58/1000 | Loss: 0.00001475
Iteration 59/1000 | Loss: 0.00001475
Iteration 60/1000 | Loss: 0.00001475
Iteration 61/1000 | Loss: 0.00001475
Iteration 62/1000 | Loss: 0.00001475
Iteration 63/1000 | Loss: 0.00001475
Iteration 64/1000 | Loss: 0.00001474
Iteration 65/1000 | Loss: 0.00001474
Iteration 66/1000 | Loss: 0.00001474
Iteration 67/1000 | Loss: 0.00001474
Iteration 68/1000 | Loss: 0.00001473
Iteration 69/1000 | Loss: 0.00001473
Iteration 70/1000 | Loss: 0.00001472
Iteration 71/1000 | Loss: 0.00001472
Iteration 72/1000 | Loss: 0.00001472
Iteration 73/1000 | Loss: 0.00001471
Iteration 74/1000 | Loss: 0.00001470
Iteration 75/1000 | Loss: 0.00001470
Iteration 76/1000 | Loss: 0.00001470
Iteration 77/1000 | Loss: 0.00001470
Iteration 78/1000 | Loss: 0.00001470
Iteration 79/1000 | Loss: 0.00001469
Iteration 80/1000 | Loss: 0.00001469
Iteration 81/1000 | Loss: 0.00001469
Iteration 82/1000 | Loss: 0.00001469
Iteration 83/1000 | Loss: 0.00001468
Iteration 84/1000 | Loss: 0.00001468
Iteration 85/1000 | Loss: 0.00001467
Iteration 86/1000 | Loss: 0.00001466
Iteration 87/1000 | Loss: 0.00001465
Iteration 88/1000 | Loss: 0.00001465
Iteration 89/1000 | Loss: 0.00001464
Iteration 90/1000 | Loss: 0.00001463
Iteration 91/1000 | Loss: 0.00001462
Iteration 92/1000 | Loss: 0.00001462
Iteration 93/1000 | Loss: 0.00001462
Iteration 94/1000 | Loss: 0.00001462
Iteration 95/1000 | Loss: 0.00001461
Iteration 96/1000 | Loss: 0.00001461
Iteration 97/1000 | Loss: 0.00001460
Iteration 98/1000 | Loss: 0.00001460
Iteration 99/1000 | Loss: 0.00001459
Iteration 100/1000 | Loss: 0.00001459
Iteration 101/1000 | Loss: 0.00001459
Iteration 102/1000 | Loss: 0.00001459
Iteration 103/1000 | Loss: 0.00001459
Iteration 104/1000 | Loss: 0.00001459
Iteration 105/1000 | Loss: 0.00001459
Iteration 106/1000 | Loss: 0.00001459
Iteration 107/1000 | Loss: 0.00001458
Iteration 108/1000 | Loss: 0.00001458
Iteration 109/1000 | Loss: 0.00001458
Iteration 110/1000 | Loss: 0.00001458
Iteration 111/1000 | Loss: 0.00001458
Iteration 112/1000 | Loss: 0.00001458
Iteration 113/1000 | Loss: 0.00001457
Iteration 114/1000 | Loss: 0.00001457
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001456
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001456
Iteration 120/1000 | Loss: 0.00001456
Iteration 121/1000 | Loss: 0.00001456
Iteration 122/1000 | Loss: 0.00001456
Iteration 123/1000 | Loss: 0.00001456
Iteration 124/1000 | Loss: 0.00001456
Iteration 125/1000 | Loss: 0.00001456
Iteration 126/1000 | Loss: 0.00001455
Iteration 127/1000 | Loss: 0.00001455
Iteration 128/1000 | Loss: 0.00001455
Iteration 129/1000 | Loss: 0.00001455
Iteration 130/1000 | Loss: 0.00001455
Iteration 131/1000 | Loss: 0.00001455
Iteration 132/1000 | Loss: 0.00001455
Iteration 133/1000 | Loss: 0.00001454
Iteration 134/1000 | Loss: 0.00001454
Iteration 135/1000 | Loss: 0.00001454
Iteration 136/1000 | Loss: 0.00001454
Iteration 137/1000 | Loss: 0.00001454
Iteration 138/1000 | Loss: 0.00001454
Iteration 139/1000 | Loss: 0.00001453
Iteration 140/1000 | Loss: 0.00001453
Iteration 141/1000 | Loss: 0.00001453
Iteration 142/1000 | Loss: 0.00001453
Iteration 143/1000 | Loss: 0.00001453
Iteration 144/1000 | Loss: 0.00001453
Iteration 145/1000 | Loss: 0.00001452
Iteration 146/1000 | Loss: 0.00001452
Iteration 147/1000 | Loss: 0.00001452
Iteration 148/1000 | Loss: 0.00001452
Iteration 149/1000 | Loss: 0.00001452
Iteration 150/1000 | Loss: 0.00001452
Iteration 151/1000 | Loss: 0.00001452
Iteration 152/1000 | Loss: 0.00001452
Iteration 153/1000 | Loss: 0.00001452
Iteration 154/1000 | Loss: 0.00001452
Iteration 155/1000 | Loss: 0.00001452
Iteration 156/1000 | Loss: 0.00001452
Iteration 157/1000 | Loss: 0.00001452
Iteration 158/1000 | Loss: 0.00001451
Iteration 159/1000 | Loss: 0.00001451
Iteration 160/1000 | Loss: 0.00001451
Iteration 161/1000 | Loss: 0.00001451
Iteration 162/1000 | Loss: 0.00001451
Iteration 163/1000 | Loss: 0.00001451
Iteration 164/1000 | Loss: 0.00001451
Iteration 165/1000 | Loss: 0.00001451
Iteration 166/1000 | Loss: 0.00001451
Iteration 167/1000 | Loss: 0.00001451
Iteration 168/1000 | Loss: 0.00001451
Iteration 169/1000 | Loss: 0.00001451
Iteration 170/1000 | Loss: 0.00001451
Iteration 171/1000 | Loss: 0.00001451
Iteration 172/1000 | Loss: 0.00001451
Iteration 173/1000 | Loss: 0.00001451
Iteration 174/1000 | Loss: 0.00001451
Iteration 175/1000 | Loss: 0.00001451
Iteration 176/1000 | Loss: 0.00001451
Iteration 177/1000 | Loss: 0.00001451
Iteration 178/1000 | Loss: 0.00001451
Iteration 179/1000 | Loss: 0.00001451
Iteration 180/1000 | Loss: 0.00001451
Iteration 181/1000 | Loss: 0.00001451
Iteration 182/1000 | Loss: 0.00001451
Iteration 183/1000 | Loss: 0.00001451
Iteration 184/1000 | Loss: 0.00001451
Iteration 185/1000 | Loss: 0.00001451
Iteration 186/1000 | Loss: 0.00001451
Iteration 187/1000 | Loss: 0.00001451
Iteration 188/1000 | Loss: 0.00001451
Iteration 189/1000 | Loss: 0.00001451
Iteration 190/1000 | Loss: 0.00001451
Iteration 191/1000 | Loss: 0.00001451
Iteration 192/1000 | Loss: 0.00001451
Iteration 193/1000 | Loss: 0.00001451
Iteration 194/1000 | Loss: 0.00001451
Iteration 195/1000 | Loss: 0.00001451
Iteration 196/1000 | Loss: 0.00001451
Iteration 197/1000 | Loss: 0.00001451
Iteration 198/1000 | Loss: 0.00001451
Iteration 199/1000 | Loss: 0.00001451
Iteration 200/1000 | Loss: 0.00001451
Iteration 201/1000 | Loss: 0.00001451
Iteration 202/1000 | Loss: 0.00001451
Iteration 203/1000 | Loss: 0.00001451
Iteration 204/1000 | Loss: 0.00001451
Iteration 205/1000 | Loss: 0.00001451
Iteration 206/1000 | Loss: 0.00001451
Iteration 207/1000 | Loss: 0.00001451
Iteration 208/1000 | Loss: 0.00001451
Iteration 209/1000 | Loss: 0.00001451
Iteration 210/1000 | Loss: 0.00001451
Iteration 211/1000 | Loss: 0.00001451
Iteration 212/1000 | Loss: 0.00001451
Iteration 213/1000 | Loss: 0.00001451
Iteration 214/1000 | Loss: 0.00001451
Iteration 215/1000 | Loss: 0.00001451
Iteration 216/1000 | Loss: 0.00001451
Iteration 217/1000 | Loss: 0.00001451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.45065405376954e-05, 1.45065405376954e-05, 1.45065405376954e-05, 1.45065405376954e-05, 1.45065405376954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.45065405376954e-05

Optimization complete. Final v2v error: 3.2677395343780518 mm

Highest mean error: 3.61686110496521 mm for frame 188

Lowest mean error: 3.1165976524353027 mm for frame 210

Saving results

Total time: 43.443328619003296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930059
Iteration 2/25 | Loss: 0.00457605
Iteration 3/25 | Loss: 0.00372195
Iteration 4/25 | Loss: 0.00317619
Iteration 5/25 | Loss: 0.00256466
Iteration 6/25 | Loss: 0.00242524
Iteration 7/25 | Loss: 0.00207208
Iteration 8/25 | Loss: 0.00190794
Iteration 9/25 | Loss: 0.00187445
Iteration 10/25 | Loss: 0.00182716
Iteration 11/25 | Loss: 0.00177338
Iteration 12/25 | Loss: 0.00170052
Iteration 13/25 | Loss: 0.00164720
Iteration 14/25 | Loss: 0.00162933
Iteration 15/25 | Loss: 0.00160832
Iteration 16/25 | Loss: 0.00160101
Iteration 17/25 | Loss: 0.00157979
Iteration 18/25 | Loss: 0.00159371
Iteration 19/25 | Loss: 0.00156126
Iteration 20/25 | Loss: 0.00155643
Iteration 21/25 | Loss: 0.00153360
Iteration 22/25 | Loss: 0.00152720
Iteration 23/25 | Loss: 0.00151999
Iteration 24/25 | Loss: 0.00151099
Iteration 25/25 | Loss: 0.00151815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40058482
Iteration 2/25 | Loss: 0.00157199
Iteration 3/25 | Loss: 0.00148944
Iteration 4/25 | Loss: 0.00148944
Iteration 5/25 | Loss: 0.00148944
Iteration 6/25 | Loss: 0.00148944
Iteration 7/25 | Loss: 0.00148944
Iteration 8/25 | Loss: 0.00148944
Iteration 9/25 | Loss: 0.00148943
Iteration 10/25 | Loss: 0.00148943
Iteration 11/25 | Loss: 0.00148943
Iteration 12/25 | Loss: 0.00148943
Iteration 13/25 | Loss: 0.00148943
Iteration 14/25 | Loss: 0.00148943
Iteration 15/25 | Loss: 0.00148943
Iteration 16/25 | Loss: 0.00148943
Iteration 17/25 | Loss: 0.00148943
Iteration 18/25 | Loss: 0.00148943
Iteration 19/25 | Loss: 0.00148943
Iteration 20/25 | Loss: 0.00148943
Iteration 21/25 | Loss: 0.00148943
Iteration 22/25 | Loss: 0.00148943
Iteration 23/25 | Loss: 0.00148943
Iteration 24/25 | Loss: 0.00148943
Iteration 25/25 | Loss: 0.00148943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148943
Iteration 2/1000 | Loss: 0.00042781
Iteration 3/1000 | Loss: 0.00016749
Iteration 4/1000 | Loss: 0.00013831
Iteration 5/1000 | Loss: 0.00012800
Iteration 6/1000 | Loss: 0.00014245
Iteration 7/1000 | Loss: 0.00013494
Iteration 8/1000 | Loss: 0.00013243
Iteration 9/1000 | Loss: 0.00175126
Iteration 10/1000 | Loss: 0.00124106
Iteration 11/1000 | Loss: 0.00038542
Iteration 12/1000 | Loss: 0.00013936
Iteration 13/1000 | Loss: 0.00029393
Iteration 14/1000 | Loss: 0.00019273
Iteration 15/1000 | Loss: 0.00007645
Iteration 16/1000 | Loss: 0.00008053
Iteration 17/1000 | Loss: 0.00005391
Iteration 18/1000 | Loss: 0.00017665
Iteration 19/1000 | Loss: 0.00005304
Iteration 20/1000 | Loss: 0.00004652
Iteration 21/1000 | Loss: 0.00004765
Iteration 22/1000 | Loss: 0.00004403
Iteration 23/1000 | Loss: 0.00004411
Iteration 24/1000 | Loss: 0.00003934
Iteration 25/1000 | Loss: 0.00004516
Iteration 26/1000 | Loss: 0.00003870
Iteration 27/1000 | Loss: 0.00003971
Iteration 28/1000 | Loss: 0.00004709
Iteration 29/1000 | Loss: 0.00017713
Iteration 30/1000 | Loss: 0.00004331
Iteration 31/1000 | Loss: 0.00003510
Iteration 32/1000 | Loss: 0.00004512
Iteration 33/1000 | Loss: 0.00012432
Iteration 34/1000 | Loss: 0.00004756
Iteration 35/1000 | Loss: 0.00004482
Iteration 36/1000 | Loss: 0.00003372
Iteration 37/1000 | Loss: 0.00003138
Iteration 38/1000 | Loss: 0.00002837
Iteration 39/1000 | Loss: 0.00003410
Iteration 40/1000 | Loss: 0.00003718
Iteration 41/1000 | Loss: 0.00003173
Iteration 42/1000 | Loss: 0.00003298
Iteration 43/1000 | Loss: 0.00003766
Iteration 44/1000 | Loss: 0.00003470
Iteration 45/1000 | Loss: 0.00013552
Iteration 46/1000 | Loss: 0.00005288
Iteration 47/1000 | Loss: 0.00004503
Iteration 48/1000 | Loss: 0.00003225
Iteration 49/1000 | Loss: 0.00002876
Iteration 50/1000 | Loss: 0.00002695
Iteration 51/1000 | Loss: 0.00002629
Iteration 52/1000 | Loss: 0.00002568
Iteration 53/1000 | Loss: 0.00002534
Iteration 54/1000 | Loss: 0.00002511
Iteration 55/1000 | Loss: 0.00002504
Iteration 56/1000 | Loss: 0.00002503
Iteration 57/1000 | Loss: 0.00002496
Iteration 58/1000 | Loss: 0.00002493
Iteration 59/1000 | Loss: 0.00002488
Iteration 60/1000 | Loss: 0.00002488
Iteration 61/1000 | Loss: 0.00002488
Iteration 62/1000 | Loss: 0.00002487
Iteration 63/1000 | Loss: 0.00002487
Iteration 64/1000 | Loss: 0.00002487
Iteration 65/1000 | Loss: 0.00002486
Iteration 66/1000 | Loss: 0.00002486
Iteration 67/1000 | Loss: 0.00002485
Iteration 68/1000 | Loss: 0.00002485
Iteration 69/1000 | Loss: 0.00002485
Iteration 70/1000 | Loss: 0.00002485
Iteration 71/1000 | Loss: 0.00002485
Iteration 72/1000 | Loss: 0.00002485
Iteration 73/1000 | Loss: 0.00002485
Iteration 74/1000 | Loss: 0.00002485
Iteration 75/1000 | Loss: 0.00002485
Iteration 76/1000 | Loss: 0.00002485
Iteration 77/1000 | Loss: 0.00002485
Iteration 78/1000 | Loss: 0.00002485
Iteration 79/1000 | Loss: 0.00002485
Iteration 80/1000 | Loss: 0.00002485
Iteration 81/1000 | Loss: 0.00002484
Iteration 82/1000 | Loss: 0.00002484
Iteration 83/1000 | Loss: 0.00002484
Iteration 84/1000 | Loss: 0.00002483
Iteration 85/1000 | Loss: 0.00004473
Iteration 86/1000 | Loss: 0.00002915
Iteration 87/1000 | Loss: 0.00004185
Iteration 88/1000 | Loss: 0.00004529
Iteration 89/1000 | Loss: 0.00003646
Iteration 90/1000 | Loss: 0.00004278
Iteration 91/1000 | Loss: 0.00003917
Iteration 92/1000 | Loss: 0.00004241
Iteration 93/1000 | Loss: 0.00002662
Iteration 94/1000 | Loss: 0.00002633
Iteration 95/1000 | Loss: 0.00002546
Iteration 96/1000 | Loss: 0.00002509
Iteration 97/1000 | Loss: 0.00002475
Iteration 98/1000 | Loss: 0.00002465
Iteration 99/1000 | Loss: 0.00002462
Iteration 100/1000 | Loss: 0.00002459
Iteration 101/1000 | Loss: 0.00002447
Iteration 102/1000 | Loss: 0.00002444
Iteration 103/1000 | Loss: 0.00002442
Iteration 104/1000 | Loss: 0.00002441
Iteration 105/1000 | Loss: 0.00002441
Iteration 106/1000 | Loss: 0.00002441
Iteration 107/1000 | Loss: 0.00002440
Iteration 108/1000 | Loss: 0.00002440
Iteration 109/1000 | Loss: 0.00002439
Iteration 110/1000 | Loss: 0.00002439
Iteration 111/1000 | Loss: 0.00002439
Iteration 112/1000 | Loss: 0.00002439
Iteration 113/1000 | Loss: 0.00002439
Iteration 114/1000 | Loss: 0.00002439
Iteration 115/1000 | Loss: 0.00002439
Iteration 116/1000 | Loss: 0.00002438
Iteration 117/1000 | Loss: 0.00002438
Iteration 118/1000 | Loss: 0.00002437
Iteration 119/1000 | Loss: 0.00002437
Iteration 120/1000 | Loss: 0.00002437
Iteration 121/1000 | Loss: 0.00002436
Iteration 122/1000 | Loss: 0.00002436
Iteration 123/1000 | Loss: 0.00002436
Iteration 124/1000 | Loss: 0.00002436
Iteration 125/1000 | Loss: 0.00002436
Iteration 126/1000 | Loss: 0.00002436
Iteration 127/1000 | Loss: 0.00002436
Iteration 128/1000 | Loss: 0.00002436
Iteration 129/1000 | Loss: 0.00002436
Iteration 130/1000 | Loss: 0.00002436
Iteration 131/1000 | Loss: 0.00002436
Iteration 132/1000 | Loss: 0.00002436
Iteration 133/1000 | Loss: 0.00002436
Iteration 134/1000 | Loss: 0.00002436
Iteration 135/1000 | Loss: 0.00002435
Iteration 136/1000 | Loss: 0.00002435
Iteration 137/1000 | Loss: 0.00002435
Iteration 138/1000 | Loss: 0.00002435
Iteration 139/1000 | Loss: 0.00002435
Iteration 140/1000 | Loss: 0.00002435
Iteration 141/1000 | Loss: 0.00002435
Iteration 142/1000 | Loss: 0.00002434
Iteration 143/1000 | Loss: 0.00002434
Iteration 144/1000 | Loss: 0.00002434
Iteration 145/1000 | Loss: 0.00002434
Iteration 146/1000 | Loss: 0.00002434
Iteration 147/1000 | Loss: 0.00002433
Iteration 148/1000 | Loss: 0.00002433
Iteration 149/1000 | Loss: 0.00002433
Iteration 150/1000 | Loss: 0.00002433
Iteration 151/1000 | Loss: 0.00002433
Iteration 152/1000 | Loss: 0.00002433
Iteration 153/1000 | Loss: 0.00002433
Iteration 154/1000 | Loss: 0.00002433
Iteration 155/1000 | Loss: 0.00002433
Iteration 156/1000 | Loss: 0.00002432
Iteration 157/1000 | Loss: 0.00002432
Iteration 158/1000 | Loss: 0.00002432
Iteration 159/1000 | Loss: 0.00002432
Iteration 160/1000 | Loss: 0.00002432
Iteration 161/1000 | Loss: 0.00002431
Iteration 162/1000 | Loss: 0.00002431
Iteration 163/1000 | Loss: 0.00002430
Iteration 164/1000 | Loss: 0.00002430
Iteration 165/1000 | Loss: 0.00002430
Iteration 166/1000 | Loss: 0.00002429
Iteration 167/1000 | Loss: 0.00002429
Iteration 168/1000 | Loss: 0.00002429
Iteration 169/1000 | Loss: 0.00002428
Iteration 170/1000 | Loss: 0.00002428
Iteration 171/1000 | Loss: 0.00002428
Iteration 172/1000 | Loss: 0.00002428
Iteration 173/1000 | Loss: 0.00002427
Iteration 174/1000 | Loss: 0.00002427
Iteration 175/1000 | Loss: 0.00002427
Iteration 176/1000 | Loss: 0.00002427
Iteration 177/1000 | Loss: 0.00002427
Iteration 178/1000 | Loss: 0.00002427
Iteration 179/1000 | Loss: 0.00002427
Iteration 180/1000 | Loss: 0.00002427
Iteration 181/1000 | Loss: 0.00002426
Iteration 182/1000 | Loss: 0.00002426
Iteration 183/1000 | Loss: 0.00002426
Iteration 184/1000 | Loss: 0.00002426
Iteration 185/1000 | Loss: 0.00002426
Iteration 186/1000 | Loss: 0.00002426
Iteration 187/1000 | Loss: 0.00002426
Iteration 188/1000 | Loss: 0.00002425
Iteration 189/1000 | Loss: 0.00002425
Iteration 190/1000 | Loss: 0.00002425
Iteration 191/1000 | Loss: 0.00002425
Iteration 192/1000 | Loss: 0.00002425
Iteration 193/1000 | Loss: 0.00002425
Iteration 194/1000 | Loss: 0.00002424
Iteration 195/1000 | Loss: 0.00002424
Iteration 196/1000 | Loss: 0.00002424
Iteration 197/1000 | Loss: 0.00002423
Iteration 198/1000 | Loss: 0.00002423
Iteration 199/1000 | Loss: 0.00002423
Iteration 200/1000 | Loss: 0.00002423
Iteration 201/1000 | Loss: 0.00002423
Iteration 202/1000 | Loss: 0.00002422
Iteration 203/1000 | Loss: 0.00002422
Iteration 204/1000 | Loss: 0.00002422
Iteration 205/1000 | Loss: 0.00002422
Iteration 206/1000 | Loss: 0.00002422
Iteration 207/1000 | Loss: 0.00002422
Iteration 208/1000 | Loss: 0.00002422
Iteration 209/1000 | Loss: 0.00002422
Iteration 210/1000 | Loss: 0.00002422
Iteration 211/1000 | Loss: 0.00002422
Iteration 212/1000 | Loss: 0.00002422
Iteration 213/1000 | Loss: 0.00002422
Iteration 214/1000 | Loss: 0.00002422
Iteration 215/1000 | Loss: 0.00002422
Iteration 216/1000 | Loss: 0.00002422
Iteration 217/1000 | Loss: 0.00002421
Iteration 218/1000 | Loss: 0.00002421
Iteration 219/1000 | Loss: 0.00002421
Iteration 220/1000 | Loss: 0.00002421
Iteration 221/1000 | Loss: 0.00002421
Iteration 222/1000 | Loss: 0.00002421
Iteration 223/1000 | Loss: 0.00002421
Iteration 224/1000 | Loss: 0.00002421
Iteration 225/1000 | Loss: 0.00002421
Iteration 226/1000 | Loss: 0.00002421
Iteration 227/1000 | Loss: 0.00002421
Iteration 228/1000 | Loss: 0.00002421
Iteration 229/1000 | Loss: 0.00002421
Iteration 230/1000 | Loss: 0.00002421
Iteration 231/1000 | Loss: 0.00002421
Iteration 232/1000 | Loss: 0.00002421
Iteration 233/1000 | Loss: 0.00002421
Iteration 234/1000 | Loss: 0.00002420
Iteration 235/1000 | Loss: 0.00002420
Iteration 236/1000 | Loss: 0.00002420
Iteration 237/1000 | Loss: 0.00002420
Iteration 238/1000 | Loss: 0.00002420
Iteration 239/1000 | Loss: 0.00002420
Iteration 240/1000 | Loss: 0.00002420
Iteration 241/1000 | Loss: 0.00002420
Iteration 242/1000 | Loss: 0.00002420
Iteration 243/1000 | Loss: 0.00002419
Iteration 244/1000 | Loss: 0.00002419
Iteration 245/1000 | Loss: 0.00002419
Iteration 246/1000 | Loss: 0.00002419
Iteration 247/1000 | Loss: 0.00002419
Iteration 248/1000 | Loss: 0.00002419
Iteration 249/1000 | Loss: 0.00002419
Iteration 250/1000 | Loss: 0.00002419
Iteration 251/1000 | Loss: 0.00002419
Iteration 252/1000 | Loss: 0.00002419
Iteration 253/1000 | Loss: 0.00002419
Iteration 254/1000 | Loss: 0.00002419
Iteration 255/1000 | Loss: 0.00002419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [2.4192069759010337e-05, 2.4192069759010337e-05, 2.4192069759010337e-05, 2.4192069759010337e-05, 2.4192069759010337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4192069759010337e-05

Optimization complete. Final v2v error: 4.176085472106934 mm

Highest mean error: 5.233900547027588 mm for frame 141

Lowest mean error: 3.6382858753204346 mm for frame 27

Saving results

Total time: 168.68156003952026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000370
Iteration 2/25 | Loss: 0.01000370
Iteration 3/25 | Loss: 0.01000370
Iteration 4/25 | Loss: 0.01000370
Iteration 5/25 | Loss: 0.01000370
Iteration 6/25 | Loss: 0.01000370
Iteration 7/25 | Loss: 0.01000369
Iteration 8/25 | Loss: 0.01000369
Iteration 9/25 | Loss: 0.01000369
Iteration 10/25 | Loss: 0.01000369
Iteration 11/25 | Loss: 0.01000369
Iteration 12/25 | Loss: 0.01000369
Iteration 13/25 | Loss: 0.01000369
Iteration 14/25 | Loss: 0.01000369
Iteration 15/25 | Loss: 0.01000369
Iteration 16/25 | Loss: 0.01000369
Iteration 17/25 | Loss: 0.01000369
Iteration 18/25 | Loss: 0.01000369
Iteration 19/25 | Loss: 0.01000369
Iteration 20/25 | Loss: 0.01000369
Iteration 21/25 | Loss: 0.01000369
Iteration 22/25 | Loss: 0.01000369
Iteration 23/25 | Loss: 0.01000369
Iteration 24/25 | Loss: 0.01000368
Iteration 25/25 | Loss: 0.01000368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07898045
Iteration 2/25 | Loss: 0.00426156
Iteration 3/25 | Loss: 0.00421512
Iteration 4/25 | Loss: 0.00421511
Iteration 5/25 | Loss: 0.00421511
Iteration 6/25 | Loss: 0.00421512
Iteration 7/25 | Loss: 0.00421511
Iteration 8/25 | Loss: 0.00421511
Iteration 9/25 | Loss: 0.00421511
Iteration 10/25 | Loss: 0.00421511
Iteration 11/25 | Loss: 0.00421511
Iteration 12/25 | Loss: 0.00421511
Iteration 13/25 | Loss: 0.00421511
Iteration 14/25 | Loss: 0.00421511
Iteration 15/25 | Loss: 0.00421511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00421511335298419, 0.00421511335298419, 0.00421511335298419, 0.00421511335298419, 0.00421511335298419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00421511335298419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00421511
Iteration 2/1000 | Loss: 0.00897048
Iteration 3/1000 | Loss: 0.00434513
Iteration 4/1000 | Loss: 0.00435563
Iteration 5/1000 | Loss: 0.00270888
Iteration 6/1000 | Loss: 0.00790749
Iteration 7/1000 | Loss: 0.00035488
Iteration 8/1000 | Loss: 0.00123654
Iteration 9/1000 | Loss: 0.00468140
Iteration 10/1000 | Loss: 0.00089240
Iteration 11/1000 | Loss: 0.00185604
Iteration 12/1000 | Loss: 0.00047651
Iteration 13/1000 | Loss: 0.00074234
Iteration 14/1000 | Loss: 0.00147893
Iteration 15/1000 | Loss: 0.00014476
Iteration 16/1000 | Loss: 0.00015001
Iteration 17/1000 | Loss: 0.00158040
Iteration 18/1000 | Loss: 0.00034455
Iteration 19/1000 | Loss: 0.00007950
Iteration 20/1000 | Loss: 0.00014597
Iteration 21/1000 | Loss: 0.00024619
Iteration 22/1000 | Loss: 0.00017287
Iteration 23/1000 | Loss: 0.00021024
Iteration 24/1000 | Loss: 0.00064553
Iteration 25/1000 | Loss: 0.00008241
Iteration 26/1000 | Loss: 0.00018032
Iteration 27/1000 | Loss: 0.00037808
Iteration 28/1000 | Loss: 0.00015936
Iteration 29/1000 | Loss: 0.00099781
Iteration 30/1000 | Loss: 0.00016836
Iteration 31/1000 | Loss: 0.00006675
Iteration 32/1000 | Loss: 0.00014972
Iteration 33/1000 | Loss: 0.00025475
Iteration 34/1000 | Loss: 0.00005085
Iteration 35/1000 | Loss: 0.00033546
Iteration 36/1000 | Loss: 0.00265081
Iteration 37/1000 | Loss: 0.00033451
Iteration 38/1000 | Loss: 0.00015499
Iteration 39/1000 | Loss: 0.00014782
Iteration 40/1000 | Loss: 0.00012990
Iteration 41/1000 | Loss: 0.00027671
Iteration 42/1000 | Loss: 0.00005024
Iteration 43/1000 | Loss: 0.00027690
Iteration 44/1000 | Loss: 0.00021276
Iteration 45/1000 | Loss: 0.00004555
Iteration 46/1000 | Loss: 0.00034720
Iteration 47/1000 | Loss: 0.00285671
Iteration 48/1000 | Loss: 0.00250989
Iteration 49/1000 | Loss: 0.00023406
Iteration 50/1000 | Loss: 0.00057031
Iteration 51/1000 | Loss: 0.00013165
Iteration 52/1000 | Loss: 0.00027304
Iteration 53/1000 | Loss: 0.00011108
Iteration 54/1000 | Loss: 0.00012543
Iteration 55/1000 | Loss: 0.00027455
Iteration 56/1000 | Loss: 0.00010098
Iteration 57/1000 | Loss: 0.00015681
Iteration 58/1000 | Loss: 0.00014593
Iteration 59/1000 | Loss: 0.00009986
Iteration 60/1000 | Loss: 0.00027881
Iteration 61/1000 | Loss: 0.00004583
Iteration 62/1000 | Loss: 0.00026521
Iteration 63/1000 | Loss: 0.00018923
Iteration 64/1000 | Loss: 0.00009196
Iteration 65/1000 | Loss: 0.00024013
Iteration 66/1000 | Loss: 0.00007954
Iteration 67/1000 | Loss: 0.00007752
Iteration 68/1000 | Loss: 0.00015032
Iteration 69/1000 | Loss: 0.00019177
Iteration 70/1000 | Loss: 0.00016132
Iteration 71/1000 | Loss: 0.00011370
Iteration 72/1000 | Loss: 0.00014896
Iteration 73/1000 | Loss: 0.00016104
Iteration 74/1000 | Loss: 0.00145730
Iteration 75/1000 | Loss: 0.00021403
Iteration 76/1000 | Loss: 0.00022967
Iteration 77/1000 | Loss: 0.00013609
Iteration 78/1000 | Loss: 0.00002617
Iteration 79/1000 | Loss: 0.00036219
Iteration 80/1000 | Loss: 0.00008252
Iteration 81/1000 | Loss: 0.00004179
Iteration 82/1000 | Loss: 0.00017149
Iteration 83/1000 | Loss: 0.00019434
Iteration 84/1000 | Loss: 0.00013793
Iteration 85/1000 | Loss: 0.00003233
Iteration 86/1000 | Loss: 0.00024143
Iteration 87/1000 | Loss: 0.00004686
Iteration 88/1000 | Loss: 0.00026755
Iteration 89/1000 | Loss: 0.00061858
Iteration 90/1000 | Loss: 0.00495515
Iteration 91/1000 | Loss: 0.00046307
Iteration 92/1000 | Loss: 0.00074976
Iteration 93/1000 | Loss: 0.00017258
Iteration 94/1000 | Loss: 0.00004717
Iteration 95/1000 | Loss: 0.00004438
Iteration 96/1000 | Loss: 0.00012624
Iteration 97/1000 | Loss: 0.00003322
Iteration 98/1000 | Loss: 0.00014975
Iteration 99/1000 | Loss: 0.00003190
Iteration 100/1000 | Loss: 0.00011449
Iteration 101/1000 | Loss: 0.00018356
Iteration 102/1000 | Loss: 0.00003345
Iteration 103/1000 | Loss: 0.00021088
Iteration 104/1000 | Loss: 0.00019310
Iteration 105/1000 | Loss: 0.00004968
Iteration 106/1000 | Loss: 0.00022006
Iteration 107/1000 | Loss: 0.00013638
Iteration 108/1000 | Loss: 0.00012322
Iteration 109/1000 | Loss: 0.00223220
Iteration 110/1000 | Loss: 0.00012891
Iteration 111/1000 | Loss: 0.00021567
Iteration 112/1000 | Loss: 0.00005887
Iteration 113/1000 | Loss: 0.00003090
Iteration 114/1000 | Loss: 0.00008694
Iteration 115/1000 | Loss: 0.00002987
Iteration 116/1000 | Loss: 0.00005800
Iteration 117/1000 | Loss: 0.00006278
Iteration 118/1000 | Loss: 0.00005150
Iteration 119/1000 | Loss: 0.00002090
Iteration 120/1000 | Loss: 0.00018751
Iteration 121/1000 | Loss: 0.00018564
Iteration 122/1000 | Loss: 0.00003826
Iteration 123/1000 | Loss: 0.00010579
Iteration 124/1000 | Loss: 0.00032170
Iteration 125/1000 | Loss: 0.00008428
Iteration 126/1000 | Loss: 0.00008507
Iteration 127/1000 | Loss: 0.00034116
Iteration 128/1000 | Loss: 0.00008073
Iteration 129/1000 | Loss: 0.00003877
Iteration 130/1000 | Loss: 0.00004900
Iteration 131/1000 | Loss: 0.00030554
Iteration 132/1000 | Loss: 0.00013372
Iteration 133/1000 | Loss: 0.00013807
Iteration 134/1000 | Loss: 0.00004004
Iteration 135/1000 | Loss: 0.00006778
Iteration 136/1000 | Loss: 0.00002299
Iteration 137/1000 | Loss: 0.00010768
Iteration 138/1000 | Loss: 0.00005796
Iteration 139/1000 | Loss: 0.00006817
Iteration 140/1000 | Loss: 0.00004844
Iteration 141/1000 | Loss: 0.00007396
Iteration 142/1000 | Loss: 0.00010621
Iteration 143/1000 | Loss: 0.00011336
Iteration 144/1000 | Loss: 0.00010241
Iteration 145/1000 | Loss: 0.00002883
Iteration 146/1000 | Loss: 0.00002175
Iteration 147/1000 | Loss: 0.00001825
Iteration 148/1000 | Loss: 0.00001825
Iteration 149/1000 | Loss: 0.00001825
Iteration 150/1000 | Loss: 0.00003902
Iteration 151/1000 | Loss: 0.00015983
Iteration 152/1000 | Loss: 0.00004111
Iteration 153/1000 | Loss: 0.00002174
Iteration 154/1000 | Loss: 0.00002933
Iteration 155/1000 | Loss: 0.00007960
Iteration 156/1000 | Loss: 0.00003221
Iteration 157/1000 | Loss: 0.00003164
Iteration 158/1000 | Loss: 0.00004793
Iteration 159/1000 | Loss: 0.00002910
Iteration 160/1000 | Loss: 0.00001927
Iteration 161/1000 | Loss: 0.00002832
Iteration 162/1000 | Loss: 0.00001860
Iteration 163/1000 | Loss: 0.00002494
Iteration 164/1000 | Loss: 0.00006867
Iteration 165/1000 | Loss: 0.00042017
Iteration 166/1000 | Loss: 0.00009213
Iteration 167/1000 | Loss: 0.00003455
Iteration 168/1000 | Loss: 0.00003300
Iteration 169/1000 | Loss: 0.00003342
Iteration 170/1000 | Loss: 0.00003237
Iteration 171/1000 | Loss: 0.00007703
Iteration 172/1000 | Loss: 0.00004979
Iteration 173/1000 | Loss: 0.00004060
Iteration 174/1000 | Loss: 0.00005791
Iteration 175/1000 | Loss: 0.00005293
Iteration 176/1000 | Loss: 0.00002041
Iteration 177/1000 | Loss: 0.00008634
Iteration 178/1000 | Loss: 0.00002800
Iteration 179/1000 | Loss: 0.00004121
Iteration 180/1000 | Loss: 0.00158929
Iteration 181/1000 | Loss: 0.00009441
Iteration 182/1000 | Loss: 0.00004822
Iteration 183/1000 | Loss: 0.00019566
Iteration 184/1000 | Loss: 0.00005912
Iteration 185/1000 | Loss: 0.00001864
Iteration 186/1000 | Loss: 0.00009333
Iteration 187/1000 | Loss: 0.00002445
Iteration 188/1000 | Loss: 0.00006366
Iteration 189/1000 | Loss: 0.00003666
Iteration 190/1000 | Loss: 0.00003715
Iteration 191/1000 | Loss: 0.00004465
Iteration 192/1000 | Loss: 0.00004084
Iteration 193/1000 | Loss: 0.00002022
Iteration 194/1000 | Loss: 0.00008845
Iteration 195/1000 | Loss: 0.00003479
Iteration 196/1000 | Loss: 0.00002348
Iteration 197/1000 | Loss: 0.00001811
Iteration 198/1000 | Loss: 0.00001809
Iteration 199/1000 | Loss: 0.00001809
Iteration 200/1000 | Loss: 0.00001809
Iteration 201/1000 | Loss: 0.00001809
Iteration 202/1000 | Loss: 0.00001809
Iteration 203/1000 | Loss: 0.00001809
Iteration 204/1000 | Loss: 0.00001809
Iteration 205/1000 | Loss: 0.00001808
Iteration 206/1000 | Loss: 0.00001808
Iteration 207/1000 | Loss: 0.00001808
Iteration 208/1000 | Loss: 0.00001808
Iteration 209/1000 | Loss: 0.00001807
Iteration 210/1000 | Loss: 0.00008552
Iteration 211/1000 | Loss: 0.00002644
Iteration 212/1000 | Loss: 0.00006497
Iteration 213/1000 | Loss: 0.00003462
Iteration 214/1000 | Loss: 0.00001907
Iteration 215/1000 | Loss: 0.00001808
Iteration 216/1000 | Loss: 0.00001806
Iteration 217/1000 | Loss: 0.00001805
Iteration 218/1000 | Loss: 0.00003869
Iteration 219/1000 | Loss: 0.00006274
Iteration 220/1000 | Loss: 0.00004943
Iteration 221/1000 | Loss: 0.00004507
Iteration 222/1000 | Loss: 0.00014067
Iteration 223/1000 | Loss: 0.00002730
Iteration 224/1000 | Loss: 0.00004600
Iteration 225/1000 | Loss: 0.00002241
Iteration 226/1000 | Loss: 0.00001804
Iteration 227/1000 | Loss: 0.00001803
Iteration 228/1000 | Loss: 0.00006725
Iteration 229/1000 | Loss: 0.00001838
Iteration 230/1000 | Loss: 0.00006003
Iteration 231/1000 | Loss: 0.00002302
Iteration 232/1000 | Loss: 0.00012692
Iteration 233/1000 | Loss: 0.00002982
Iteration 234/1000 | Loss: 0.00003994
Iteration 235/1000 | Loss: 0.00005402
Iteration 236/1000 | Loss: 0.00002971
Iteration 237/1000 | Loss: 0.00004148
Iteration 238/1000 | Loss: 0.00005578
Iteration 239/1000 | Loss: 0.00004805
Iteration 240/1000 | Loss: 0.00010259
Iteration 241/1000 | Loss: 0.00003779
Iteration 242/1000 | Loss: 0.00002817
Iteration 243/1000 | Loss: 0.00005959
Iteration 244/1000 | Loss: 0.00004972
Iteration 245/1000 | Loss: 0.00006507
Iteration 246/1000 | Loss: 0.00002754
Iteration 247/1000 | Loss: 0.00003173
Iteration 248/1000 | Loss: 0.00002743
Iteration 249/1000 | Loss: 0.00002707
Iteration 250/1000 | Loss: 0.00001960
Iteration 251/1000 | Loss: 0.00004010
Iteration 252/1000 | Loss: 0.00002031
Iteration 253/1000 | Loss: 0.00001805
Iteration 254/1000 | Loss: 0.00001805
Iteration 255/1000 | Loss: 0.00001804
Iteration 256/1000 | Loss: 0.00004214
Iteration 257/1000 | Loss: 0.00002803
Iteration 258/1000 | Loss: 0.00001802
Iteration 259/1000 | Loss: 0.00001801
Iteration 260/1000 | Loss: 0.00001800
Iteration 261/1000 | Loss: 0.00001800
Iteration 262/1000 | Loss: 0.00001800
Iteration 263/1000 | Loss: 0.00001800
Iteration 264/1000 | Loss: 0.00001800
Iteration 265/1000 | Loss: 0.00001800
Iteration 266/1000 | Loss: 0.00001800
Iteration 267/1000 | Loss: 0.00001800
Iteration 268/1000 | Loss: 0.00001800
Iteration 269/1000 | Loss: 0.00003950
Iteration 270/1000 | Loss: 0.00008334
Iteration 271/1000 | Loss: 0.00004346
Iteration 272/1000 | Loss: 0.00002307
Iteration 273/1000 | Loss: 0.00005043
Iteration 274/1000 | Loss: 0.00002087
Iteration 275/1000 | Loss: 0.00002851
Iteration 276/1000 | Loss: 0.00001908
Iteration 277/1000 | Loss: 0.00001797
Iteration 278/1000 | Loss: 0.00001797
Iteration 279/1000 | Loss: 0.00001797
Iteration 280/1000 | Loss: 0.00001797
Iteration 281/1000 | Loss: 0.00008115
Iteration 282/1000 | Loss: 0.00002478
Iteration 283/1000 | Loss: 0.00001800
Iteration 284/1000 | Loss: 0.00001796
Iteration 285/1000 | Loss: 0.00001796
Iteration 286/1000 | Loss: 0.00003234
Iteration 287/1000 | Loss: 0.00001865
Iteration 288/1000 | Loss: 0.00007523
Iteration 289/1000 | Loss: 0.00017055
Iteration 290/1000 | Loss: 0.00007778
Iteration 291/1000 | Loss: 0.00002466
Iteration 292/1000 | Loss: 0.00001799
Iteration 293/1000 | Loss: 0.00001798
Iteration 294/1000 | Loss: 0.00007719
Iteration 295/1000 | Loss: 0.00005380
Iteration 296/1000 | Loss: 0.00002268
Iteration 297/1000 | Loss: 0.00025758
Iteration 298/1000 | Loss: 0.00016760
Iteration 299/1000 | Loss: 0.00003397
Iteration 300/1000 | Loss: 0.00004653
Iteration 301/1000 | Loss: 0.00003866
Iteration 302/1000 | Loss: 0.00016407
Iteration 303/1000 | Loss: 0.00021844
Iteration 304/1000 | Loss: 0.00029465
Iteration 305/1000 | Loss: 0.00295161
Iteration 306/1000 | Loss: 0.00168411
Iteration 307/1000 | Loss: 0.00083335
Iteration 308/1000 | Loss: 0.00016518
Iteration 309/1000 | Loss: 0.00008710
Iteration 310/1000 | Loss: 0.00010818
Iteration 311/1000 | Loss: 0.00002182
Iteration 312/1000 | Loss: 0.00003734
Iteration 313/1000 | Loss: 0.00009099
Iteration 314/1000 | Loss: 0.00004402
Iteration 315/1000 | Loss: 0.00003102
Iteration 316/1000 | Loss: 0.00001919
Iteration 317/1000 | Loss: 0.00001808
Iteration 318/1000 | Loss: 0.00001807
Iteration 319/1000 | Loss: 0.00001806
Iteration 320/1000 | Loss: 0.00001806
Iteration 321/1000 | Loss: 0.00001806
Iteration 322/1000 | Loss: 0.00001806
Iteration 323/1000 | Loss: 0.00001806
Iteration 324/1000 | Loss: 0.00001806
Iteration 325/1000 | Loss: 0.00001805
Iteration 326/1000 | Loss: 0.00001805
Iteration 327/1000 | Loss: 0.00001805
Iteration 328/1000 | Loss: 0.00001805
Iteration 329/1000 | Loss: 0.00001804
Iteration 330/1000 | Loss: 0.00001804
Iteration 331/1000 | Loss: 0.00005341
Iteration 332/1000 | Loss: 0.00001896
Iteration 333/1000 | Loss: 0.00001795
Iteration 334/1000 | Loss: 0.00001794
Iteration 335/1000 | Loss: 0.00001793
Iteration 336/1000 | Loss: 0.00001793
Iteration 337/1000 | Loss: 0.00001793
Iteration 338/1000 | Loss: 0.00001793
Iteration 339/1000 | Loss: 0.00001793
Iteration 340/1000 | Loss: 0.00001793
Iteration 341/1000 | Loss: 0.00001793
Iteration 342/1000 | Loss: 0.00001792
Iteration 343/1000 | Loss: 0.00001792
Iteration 344/1000 | Loss: 0.00001792
Iteration 345/1000 | Loss: 0.00001792
Iteration 346/1000 | Loss: 0.00001792
Iteration 347/1000 | Loss: 0.00001792
Iteration 348/1000 | Loss: 0.00001792
Iteration 349/1000 | Loss: 0.00001791
Iteration 350/1000 | Loss: 0.00001791
Iteration 351/1000 | Loss: 0.00001791
Iteration 352/1000 | Loss: 0.00001791
Iteration 353/1000 | Loss: 0.00001791
Iteration 354/1000 | Loss: 0.00001791
Iteration 355/1000 | Loss: 0.00001791
Iteration 356/1000 | Loss: 0.00001791
Iteration 357/1000 | Loss: 0.00001791
Iteration 358/1000 | Loss: 0.00001791
Iteration 359/1000 | Loss: 0.00001790
Iteration 360/1000 | Loss: 0.00002706
Iteration 361/1000 | Loss: 0.00001907
Iteration 362/1000 | Loss: 0.00001792
Iteration 363/1000 | Loss: 0.00001792
Iteration 364/1000 | Loss: 0.00001792
Iteration 365/1000 | Loss: 0.00001791
Iteration 366/1000 | Loss: 0.00001791
Iteration 367/1000 | Loss: 0.00001791
Iteration 368/1000 | Loss: 0.00001791
Iteration 369/1000 | Loss: 0.00001791
Iteration 370/1000 | Loss: 0.00001791
Iteration 371/1000 | Loss: 0.00001791
Iteration 372/1000 | Loss: 0.00001791
Iteration 373/1000 | Loss: 0.00001791
Iteration 374/1000 | Loss: 0.00001791
Iteration 375/1000 | Loss: 0.00001791
Iteration 376/1000 | Loss: 0.00001791
Iteration 377/1000 | Loss: 0.00001791
Iteration 378/1000 | Loss: 0.00001791
Iteration 379/1000 | Loss: 0.00001791
Iteration 380/1000 | Loss: 0.00001790
Iteration 381/1000 | Loss: 0.00001790
Iteration 382/1000 | Loss: 0.00001790
Iteration 383/1000 | Loss: 0.00001790
Iteration 384/1000 | Loss: 0.00001790
Iteration 385/1000 | Loss: 0.00001790
Iteration 386/1000 | Loss: 0.00001790
Iteration 387/1000 | Loss: 0.00001789
Iteration 388/1000 | Loss: 0.00001789
Iteration 389/1000 | Loss: 0.00001789
Iteration 390/1000 | Loss: 0.00001789
Iteration 391/1000 | Loss: 0.00001789
Iteration 392/1000 | Loss: 0.00001789
Iteration 393/1000 | Loss: 0.00001789
Iteration 394/1000 | Loss: 0.00001789
Iteration 395/1000 | Loss: 0.00001789
Iteration 396/1000 | Loss: 0.00001789
Iteration 397/1000 | Loss: 0.00001788
Iteration 398/1000 | Loss: 0.00001788
Iteration 399/1000 | Loss: 0.00001788
Iteration 400/1000 | Loss: 0.00001788
Iteration 401/1000 | Loss: 0.00001788
Iteration 402/1000 | Loss: 0.00001788
Iteration 403/1000 | Loss: 0.00001788
Iteration 404/1000 | Loss: 0.00001788
Iteration 405/1000 | Loss: 0.00001788
Iteration 406/1000 | Loss: 0.00001788
Iteration 407/1000 | Loss: 0.00001788
Iteration 408/1000 | Loss: 0.00001788
Iteration 409/1000 | Loss: 0.00001788
Iteration 410/1000 | Loss: 0.00007396
Iteration 411/1000 | Loss: 0.00053825
Iteration 412/1000 | Loss: 0.00010989
Iteration 413/1000 | Loss: 0.00023250
Iteration 414/1000 | Loss: 0.00033883
Iteration 415/1000 | Loss: 0.00052938
Iteration 416/1000 | Loss: 0.00050386
Iteration 417/1000 | Loss: 0.00012734
Iteration 418/1000 | Loss: 0.00028098
Iteration 419/1000 | Loss: 0.00084266
Iteration 420/1000 | Loss: 0.00022546
Iteration 421/1000 | Loss: 0.00020475
Iteration 422/1000 | Loss: 0.00016107
Iteration 423/1000 | Loss: 0.00003012
Iteration 424/1000 | Loss: 0.00001811
Iteration 425/1000 | Loss: 0.00001809
Iteration 426/1000 | Loss: 0.00001805
Iteration 427/1000 | Loss: 0.00003211
Iteration 428/1000 | Loss: 0.00004509
Iteration 429/1000 | Loss: 0.00001997
Iteration 430/1000 | Loss: 0.00001791
Iteration 431/1000 | Loss: 0.00001791
Iteration 432/1000 | Loss: 0.00001791
Iteration 433/1000 | Loss: 0.00001791
Iteration 434/1000 | Loss: 0.00001791
Iteration 435/1000 | Loss: 0.00001791
Iteration 436/1000 | Loss: 0.00001791
Iteration 437/1000 | Loss: 0.00001791
Iteration 438/1000 | Loss: 0.00001790
Iteration 439/1000 | Loss: 0.00001790
Iteration 440/1000 | Loss: 0.00001790
Iteration 441/1000 | Loss: 0.00001790
Iteration 442/1000 | Loss: 0.00001790
Iteration 443/1000 | Loss: 0.00001789
Iteration 444/1000 | Loss: 0.00001789
Iteration 445/1000 | Loss: 0.00001789
Iteration 446/1000 | Loss: 0.00001789
Iteration 447/1000 | Loss: 0.00001788
Iteration 448/1000 | Loss: 0.00001788
Iteration 449/1000 | Loss: 0.00001788
Iteration 450/1000 | Loss: 0.00001788
Iteration 451/1000 | Loss: 0.00001788
Iteration 452/1000 | Loss: 0.00001788
Iteration 453/1000 | Loss: 0.00001788
Iteration 454/1000 | Loss: 0.00001788
Iteration 455/1000 | Loss: 0.00001788
Iteration 456/1000 | Loss: 0.00001788
Iteration 457/1000 | Loss: 0.00001788
Iteration 458/1000 | Loss: 0.00002672
Iteration 459/1000 | Loss: 0.00001786
Iteration 460/1000 | Loss: 0.00001786
Iteration 461/1000 | Loss: 0.00001786
Iteration 462/1000 | Loss: 0.00001786
Iteration 463/1000 | Loss: 0.00001786
Iteration 464/1000 | Loss: 0.00001786
Iteration 465/1000 | Loss: 0.00001786
Iteration 466/1000 | Loss: 0.00001786
Iteration 467/1000 | Loss: 0.00001786
Iteration 468/1000 | Loss: 0.00001786
Iteration 469/1000 | Loss: 0.00001786
Iteration 470/1000 | Loss: 0.00001786
Iteration 471/1000 | Loss: 0.00001786
Iteration 472/1000 | Loss: 0.00001785
Iteration 473/1000 | Loss: 0.00001785
Iteration 474/1000 | Loss: 0.00001785
Iteration 475/1000 | Loss: 0.00001785
Iteration 476/1000 | Loss: 0.00001785
Iteration 477/1000 | Loss: 0.00001785
Iteration 478/1000 | Loss: 0.00001785
Iteration 479/1000 | Loss: 0.00001785
Iteration 480/1000 | Loss: 0.00001785
Iteration 481/1000 | Loss: 0.00001785
Iteration 482/1000 | Loss: 0.00001785
Iteration 483/1000 | Loss: 0.00001785
Iteration 484/1000 | Loss: 0.00001785
Iteration 485/1000 | Loss: 0.00001785
Iteration 486/1000 | Loss: 0.00001785
Iteration 487/1000 | Loss: 0.00001785
Iteration 488/1000 | Loss: 0.00001785
Iteration 489/1000 | Loss: 0.00001785
Iteration 490/1000 | Loss: 0.00001785
Iteration 491/1000 | Loss: 0.00001784
Iteration 492/1000 | Loss: 0.00001784
Iteration 493/1000 | Loss: 0.00001784
Iteration 494/1000 | Loss: 0.00001784
Iteration 495/1000 | Loss: 0.00001784
Iteration 496/1000 | Loss: 0.00001784
Iteration 497/1000 | Loss: 0.00001784
Iteration 498/1000 | Loss: 0.00001784
Iteration 499/1000 | Loss: 0.00001784
Iteration 500/1000 | Loss: 0.00001784
Iteration 501/1000 | Loss: 0.00001784
Iteration 502/1000 | Loss: 0.00001784
Iteration 503/1000 | Loss: 0.00005554
Iteration 504/1000 | Loss: 0.00002447
Iteration 505/1000 | Loss: 0.00001786
Iteration 506/1000 | Loss: 0.00001785
Iteration 507/1000 | Loss: 0.00001785
Iteration 508/1000 | Loss: 0.00001785
Iteration 509/1000 | Loss: 0.00001785
Iteration 510/1000 | Loss: 0.00001784
Iteration 511/1000 | Loss: 0.00001784
Iteration 512/1000 | Loss: 0.00001784
Iteration 513/1000 | Loss: 0.00001784
Iteration 514/1000 | Loss: 0.00001784
Iteration 515/1000 | Loss: 0.00001783
Iteration 516/1000 | Loss: 0.00006383
Iteration 517/1000 | Loss: 0.00003670
Iteration 518/1000 | Loss: 0.00001785
Iteration 519/1000 | Loss: 0.00001785
Iteration 520/1000 | Loss: 0.00006036
Iteration 521/1000 | Loss: 0.00006036
Iteration 522/1000 | Loss: 0.00109235
Iteration 523/1000 | Loss: 0.00003360
Iteration 524/1000 | Loss: 0.00001827
Iteration 525/1000 | Loss: 0.00009417
Iteration 526/1000 | Loss: 0.00002633
Iteration 527/1000 | Loss: 0.00003271
Iteration 528/1000 | Loss: 0.00010375
Iteration 529/1000 | Loss: 0.00002090
Iteration 530/1000 | Loss: 0.00006536
Iteration 531/1000 | Loss: 0.00003529
Iteration 532/1000 | Loss: 0.00002217
Iteration 533/1000 | Loss: 0.00002220
Iteration 534/1000 | Loss: 0.00001849
Iteration 535/1000 | Loss: 0.00012514
Iteration 536/1000 | Loss: 0.00002804
Iteration 537/1000 | Loss: 0.00003308
Iteration 538/1000 | Loss: 0.00003465
Iteration 539/1000 | Loss: 0.00006068
Iteration 540/1000 | Loss: 0.00002124
Iteration 541/1000 | Loss: 0.00004385
Iteration 542/1000 | Loss: 0.00001934
Iteration 543/1000 | Loss: 0.00003553
Iteration 544/1000 | Loss: 0.00005239
Iteration 545/1000 | Loss: 0.00004236
Iteration 546/1000 | Loss: 0.00002893
Iteration 547/1000 | Loss: 0.00003324
Iteration 548/1000 | Loss: 0.00003324
Iteration 549/1000 | Loss: 0.00004690
Iteration 550/1000 | Loss: 0.00008784
Iteration 551/1000 | Loss: 0.00004087
Iteration 552/1000 | Loss: 0.00019394
Iteration 553/1000 | Loss: 0.00010185
Iteration 554/1000 | Loss: 0.00009152
Iteration 555/1000 | Loss: 0.00006046
Iteration 556/1000 | Loss: 0.00001993
Iteration 557/1000 | Loss: 0.00001792
Iteration 558/1000 | Loss: 0.00001790
Iteration 559/1000 | Loss: 0.00001789
Iteration 560/1000 | Loss: 0.00001789
Iteration 561/1000 | Loss: 0.00001788
Iteration 562/1000 | Loss: 0.00001788
Iteration 563/1000 | Loss: 0.00001788
Iteration 564/1000 | Loss: 0.00001787
Iteration 565/1000 | Loss: 0.00001787
Iteration 566/1000 | Loss: 0.00001787
Iteration 567/1000 | Loss: 0.00001787
Iteration 568/1000 | Loss: 0.00001787
Iteration 569/1000 | Loss: 0.00001787
Iteration 570/1000 | Loss: 0.00001786
Iteration 571/1000 | Loss: 0.00001786
Iteration 572/1000 | Loss: 0.00003140
Iteration 573/1000 | Loss: 0.00004404
Iteration 574/1000 | Loss: 0.00001787
Iteration 575/1000 | Loss: 0.00002027
Iteration 576/1000 | Loss: 0.00005547
Iteration 577/1000 | Loss: 0.00022336
Iteration 578/1000 | Loss: 0.00002792
Iteration 579/1000 | Loss: 0.00003311
Iteration 580/1000 | Loss: 0.00002015
Iteration 581/1000 | Loss: 0.00001786
Iteration 582/1000 | Loss: 0.00001785
Iteration 583/1000 | Loss: 0.00001785
Iteration 584/1000 | Loss: 0.00001785
Iteration 585/1000 | Loss: 0.00001785
Iteration 586/1000 | Loss: 0.00001785
Iteration 587/1000 | Loss: 0.00001785
Iteration 588/1000 | Loss: 0.00001785
Iteration 589/1000 | Loss: 0.00001785
Iteration 590/1000 | Loss: 0.00001785
Iteration 591/1000 | Loss: 0.00001784
Iteration 592/1000 | Loss: 0.00001784
Iteration 593/1000 | Loss: 0.00001784
Iteration 594/1000 | Loss: 0.00001783
Iteration 595/1000 | Loss: 0.00001783
Iteration 596/1000 | Loss: 0.00001783
Iteration 597/1000 | Loss: 0.00001783
Iteration 598/1000 | Loss: 0.00001782
Iteration 599/1000 | Loss: 0.00001782
Iteration 600/1000 | Loss: 0.00005121
Iteration 601/1000 | Loss: 0.00002184
Iteration 602/1000 | Loss: 0.00003900
Iteration 603/1000 | Loss: 0.00001867
Iteration 604/1000 | Loss: 0.00002882
Iteration 605/1000 | Loss: 0.00002022
Iteration 606/1000 | Loss: 0.00001783
Iteration 607/1000 | Loss: 0.00001783
Iteration 608/1000 | Loss: 0.00001783
Iteration 609/1000 | Loss: 0.00001783
Iteration 610/1000 | Loss: 0.00001783
Iteration 611/1000 | Loss: 0.00001783
Iteration 612/1000 | Loss: 0.00001782
Iteration 613/1000 | Loss: 0.00001782
Iteration 614/1000 | Loss: 0.00001782
Iteration 615/1000 | Loss: 0.00001781
Iteration 616/1000 | Loss: 0.00001781
Iteration 617/1000 | Loss: 0.00001781
Iteration 618/1000 | Loss: 0.00001781
Iteration 619/1000 | Loss: 0.00001781
Iteration 620/1000 | Loss: 0.00001781
Iteration 621/1000 | Loss: 0.00001780
Iteration 622/1000 | Loss: 0.00001780
Iteration 623/1000 | Loss: 0.00001780
Iteration 624/1000 | Loss: 0.00001780
Iteration 625/1000 | Loss: 0.00001780
Iteration 626/1000 | Loss: 0.00001780
Iteration 627/1000 | Loss: 0.00003986
Iteration 628/1000 | Loss: 0.00002155
Iteration 629/1000 | Loss: 0.00001783
Iteration 630/1000 | Loss: 0.00001782
Iteration 631/1000 | Loss: 0.00001782
Iteration 632/1000 | Loss: 0.00001782
Iteration 633/1000 | Loss: 0.00001782
Iteration 634/1000 | Loss: 0.00001782
Iteration 635/1000 | Loss: 0.00001782
Iteration 636/1000 | Loss: 0.00001781
Iteration 637/1000 | Loss: 0.00001781
Iteration 638/1000 | Loss: 0.00001781
Iteration 639/1000 | Loss: 0.00001781
Iteration 640/1000 | Loss: 0.00001781
Iteration 641/1000 | Loss: 0.00001781
Iteration 642/1000 | Loss: 0.00001781
Iteration 643/1000 | Loss: 0.00001781
Iteration 644/1000 | Loss: 0.00001781
Iteration 645/1000 | Loss: 0.00001781
Iteration 646/1000 | Loss: 0.00001781
Iteration 647/1000 | Loss: 0.00001780
Iteration 648/1000 | Loss: 0.00001780
Iteration 649/1000 | Loss: 0.00001780
Iteration 650/1000 | Loss: 0.00001780
Iteration 651/1000 | Loss: 0.00001780
Iteration 652/1000 | Loss: 0.00001780
Iteration 653/1000 | Loss: 0.00001780
Iteration 654/1000 | Loss: 0.00001779
Iteration 655/1000 | Loss: 0.00001779
Iteration 656/1000 | Loss: 0.00001779
Iteration 657/1000 | Loss: 0.00001778
Iteration 658/1000 | Loss: 0.00001778
Iteration 659/1000 | Loss: 0.00001778
Iteration 660/1000 | Loss: 0.00001778
Iteration 661/1000 | Loss: 0.00001778
Iteration 662/1000 | Loss: 0.00001778
Iteration 663/1000 | Loss: 0.00001778
Iteration 664/1000 | Loss: 0.00001778
Iteration 665/1000 | Loss: 0.00001778
Iteration 666/1000 | Loss: 0.00001778
Iteration 667/1000 | Loss: 0.00001778
Iteration 668/1000 | Loss: 0.00001778
Iteration 669/1000 | Loss: 0.00001778
Iteration 670/1000 | Loss: 0.00001778
Iteration 671/1000 | Loss: 0.00001778
Iteration 672/1000 | Loss: 0.00001778
Iteration 673/1000 | Loss: 0.00001778
Iteration 674/1000 | Loss: 0.00001778
Iteration 675/1000 | Loss: 0.00001778
Iteration 676/1000 | Loss: 0.00001778
Iteration 677/1000 | Loss: 0.00001778
Iteration 678/1000 | Loss: 0.00001778
Iteration 679/1000 | Loss: 0.00001778
Iteration 680/1000 | Loss: 0.00001778
Iteration 681/1000 | Loss: 0.00001778
Iteration 682/1000 | Loss: 0.00001778
Iteration 683/1000 | Loss: 0.00001778
Iteration 684/1000 | Loss: 0.00001778
Iteration 685/1000 | Loss: 0.00001778
Iteration 686/1000 | Loss: 0.00001778
Iteration 687/1000 | Loss: 0.00001778
Iteration 688/1000 | Loss: 0.00001778
Iteration 689/1000 | Loss: 0.00001778
Iteration 690/1000 | Loss: 0.00001778
Iteration 691/1000 | Loss: 0.00001778
Iteration 692/1000 | Loss: 0.00001778
Iteration 693/1000 | Loss: 0.00001778
Iteration 694/1000 | Loss: 0.00001778
Iteration 695/1000 | Loss: 0.00001778
Iteration 696/1000 | Loss: 0.00001778
Iteration 697/1000 | Loss: 0.00001778
Iteration 698/1000 | Loss: 0.00001778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 698. Stopping optimization.
Last 5 losses: [1.7779937479645014e-05, 1.7779937479645014e-05, 1.7779937479645014e-05, 1.7779937479645014e-05, 1.7779937479645014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7779937479645014e-05

Optimization complete. Final v2v error: 3.6246888637542725 mm

Highest mean error: 3.930172920227051 mm for frame 2

Lowest mean error: 3.4004414081573486 mm for frame 145

Saving results

Total time: 495.439022064209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00572851
Iteration 2/25 | Loss: 0.00131850
Iteration 3/25 | Loss: 0.00125493
Iteration 4/25 | Loss: 0.00124619
Iteration 5/25 | Loss: 0.00124315
Iteration 6/25 | Loss: 0.00124292
Iteration 7/25 | Loss: 0.00124292
Iteration 8/25 | Loss: 0.00124292
Iteration 9/25 | Loss: 0.00124292
Iteration 10/25 | Loss: 0.00124292
Iteration 11/25 | Loss: 0.00124292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012429209891706705, 0.0012429209891706705, 0.0012429209891706705, 0.0012429209891706705, 0.0012429209891706705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012429209891706705

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.90191412
Iteration 2/25 | Loss: 0.00084589
Iteration 3/25 | Loss: 0.00084589
Iteration 4/25 | Loss: 0.00084589
Iteration 5/25 | Loss: 0.00084589
Iteration 6/25 | Loss: 0.00084589
Iteration 7/25 | Loss: 0.00084589
Iteration 8/25 | Loss: 0.00084589
Iteration 9/25 | Loss: 0.00084589
Iteration 10/25 | Loss: 0.00084589
Iteration 11/25 | Loss: 0.00084589
Iteration 12/25 | Loss: 0.00084589
Iteration 13/25 | Loss: 0.00084589
Iteration 14/25 | Loss: 0.00084589
Iteration 15/25 | Loss: 0.00084589
Iteration 16/25 | Loss: 0.00084589
Iteration 17/25 | Loss: 0.00084589
Iteration 18/25 | Loss: 0.00084589
Iteration 19/25 | Loss: 0.00084589
Iteration 20/25 | Loss: 0.00084589
Iteration 21/25 | Loss: 0.00084589
Iteration 22/25 | Loss: 0.00084589
Iteration 23/25 | Loss: 0.00084589
Iteration 24/25 | Loss: 0.00084589
Iteration 25/25 | Loss: 0.00084589

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084589
Iteration 2/1000 | Loss: 0.00002720
Iteration 3/1000 | Loss: 0.00001817
Iteration 4/1000 | Loss: 0.00001541
Iteration 5/1000 | Loss: 0.00001441
Iteration 6/1000 | Loss: 0.00001372
Iteration 7/1000 | Loss: 0.00001330
Iteration 8/1000 | Loss: 0.00001326
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00001315
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001269
Iteration 13/1000 | Loss: 0.00001269
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001261
Iteration 16/1000 | Loss: 0.00001260
Iteration 17/1000 | Loss: 0.00001255
Iteration 18/1000 | Loss: 0.00001252
Iteration 19/1000 | Loss: 0.00001246
Iteration 20/1000 | Loss: 0.00001244
Iteration 21/1000 | Loss: 0.00001243
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001234
Iteration 24/1000 | Loss: 0.00001229
Iteration 25/1000 | Loss: 0.00001218
Iteration 26/1000 | Loss: 0.00001218
Iteration 27/1000 | Loss: 0.00001218
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001217
Iteration 30/1000 | Loss: 0.00001216
Iteration 31/1000 | Loss: 0.00001216
Iteration 32/1000 | Loss: 0.00001216
Iteration 33/1000 | Loss: 0.00001212
Iteration 34/1000 | Loss: 0.00001212
Iteration 35/1000 | Loss: 0.00001211
Iteration 36/1000 | Loss: 0.00001211
Iteration 37/1000 | Loss: 0.00001211
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001208
Iteration 40/1000 | Loss: 0.00001208
Iteration 41/1000 | Loss: 0.00001203
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001200
Iteration 46/1000 | Loss: 0.00001199
Iteration 47/1000 | Loss: 0.00001199
Iteration 48/1000 | Loss: 0.00001198
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001198
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001197
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001196
Iteration 55/1000 | Loss: 0.00001195
Iteration 56/1000 | Loss: 0.00001195
Iteration 57/1000 | Loss: 0.00001195
Iteration 58/1000 | Loss: 0.00001194
Iteration 59/1000 | Loss: 0.00001194
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001194
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001192
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001189
Iteration 73/1000 | Loss: 0.00001189
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001188
Iteration 76/1000 | Loss: 0.00001188
Iteration 77/1000 | Loss: 0.00001188
Iteration 78/1000 | Loss: 0.00001187
Iteration 79/1000 | Loss: 0.00001187
Iteration 80/1000 | Loss: 0.00001187
Iteration 81/1000 | Loss: 0.00001186
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001185
Iteration 84/1000 | Loss: 0.00001185
Iteration 85/1000 | Loss: 0.00001185
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001184
Iteration 89/1000 | Loss: 0.00001184
Iteration 90/1000 | Loss: 0.00001183
Iteration 91/1000 | Loss: 0.00001183
Iteration 92/1000 | Loss: 0.00001183
Iteration 93/1000 | Loss: 0.00001183
Iteration 94/1000 | Loss: 0.00001183
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001182
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001182
Iteration 100/1000 | Loss: 0.00001181
Iteration 101/1000 | Loss: 0.00001181
Iteration 102/1000 | Loss: 0.00001181
Iteration 103/1000 | Loss: 0.00001181
Iteration 104/1000 | Loss: 0.00001181
Iteration 105/1000 | Loss: 0.00001181
Iteration 106/1000 | Loss: 0.00001181
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001181
Iteration 113/1000 | Loss: 0.00001180
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001180
Iteration 116/1000 | Loss: 0.00001180
Iteration 117/1000 | Loss: 0.00001180
Iteration 118/1000 | Loss: 0.00001180
Iteration 119/1000 | Loss: 0.00001180
Iteration 120/1000 | Loss: 0.00001180
Iteration 121/1000 | Loss: 0.00001180
Iteration 122/1000 | Loss: 0.00001180
Iteration 123/1000 | Loss: 0.00001180
Iteration 124/1000 | Loss: 0.00001180
Iteration 125/1000 | Loss: 0.00001180
Iteration 126/1000 | Loss: 0.00001180
Iteration 127/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.1796152648457792e-05, 1.1796152648457792e-05, 1.1796152648457792e-05, 1.1796152648457792e-05, 1.1796152648457792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1796152648457792e-05

Optimization complete. Final v2v error: 2.9583585262298584 mm

Highest mean error: 3.2597649097442627 mm for frame 108

Lowest mean error: 2.7960214614868164 mm for frame 169

Saving results

Total time: 37.077422857284546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760529
Iteration 2/25 | Loss: 0.00158496
Iteration 3/25 | Loss: 0.00143000
Iteration 4/25 | Loss: 0.00135464
Iteration 5/25 | Loss: 0.00132508
Iteration 6/25 | Loss: 0.00132062
Iteration 7/25 | Loss: 0.00131613
Iteration 8/25 | Loss: 0.00131334
Iteration 9/25 | Loss: 0.00131275
Iteration 10/25 | Loss: 0.00131263
Iteration 11/25 | Loss: 0.00131261
Iteration 12/25 | Loss: 0.00131260
Iteration 13/25 | Loss: 0.00131260
Iteration 14/25 | Loss: 0.00131260
Iteration 15/25 | Loss: 0.00131260
Iteration 16/25 | Loss: 0.00131260
Iteration 17/25 | Loss: 0.00131260
Iteration 18/25 | Loss: 0.00131260
Iteration 19/25 | Loss: 0.00131260
Iteration 20/25 | Loss: 0.00131260
Iteration 21/25 | Loss: 0.00131260
Iteration 22/25 | Loss: 0.00131260
Iteration 23/25 | Loss: 0.00131260
Iteration 24/25 | Loss: 0.00131260
Iteration 25/25 | Loss: 0.00131260

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87224901
Iteration 2/25 | Loss: 0.00097341
Iteration 3/25 | Loss: 0.00097341
Iteration 4/25 | Loss: 0.00097341
Iteration 5/25 | Loss: 0.00097341
Iteration 6/25 | Loss: 0.00097340
Iteration 7/25 | Loss: 0.00097340
Iteration 8/25 | Loss: 0.00097340
Iteration 9/25 | Loss: 0.00097340
Iteration 10/25 | Loss: 0.00097340
Iteration 11/25 | Loss: 0.00097340
Iteration 12/25 | Loss: 0.00097340
Iteration 13/25 | Loss: 0.00097340
Iteration 14/25 | Loss: 0.00097340
Iteration 15/25 | Loss: 0.00097340
Iteration 16/25 | Loss: 0.00097340
Iteration 17/25 | Loss: 0.00097340
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009734033374115825, 0.0009734033374115825, 0.0009734033374115825, 0.0009734033374115825, 0.0009734033374115825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009734033374115825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097340
Iteration 2/1000 | Loss: 0.00005411
Iteration 3/1000 | Loss: 0.00003464
Iteration 4/1000 | Loss: 0.00002951
Iteration 5/1000 | Loss: 0.00002838
Iteration 6/1000 | Loss: 0.00002728
Iteration 7/1000 | Loss: 0.00002677
Iteration 8/1000 | Loss: 0.00002627
Iteration 9/1000 | Loss: 0.00002586
Iteration 10/1000 | Loss: 0.00002555
Iteration 11/1000 | Loss: 0.00002528
Iteration 12/1000 | Loss: 0.00002498
Iteration 13/1000 | Loss: 0.00002487
Iteration 14/1000 | Loss: 0.00002468
Iteration 15/1000 | Loss: 0.00002468
Iteration 16/1000 | Loss: 0.00002454
Iteration 17/1000 | Loss: 0.00002448
Iteration 18/1000 | Loss: 0.00002446
Iteration 19/1000 | Loss: 0.00002442
Iteration 20/1000 | Loss: 0.00002438
Iteration 21/1000 | Loss: 0.00002438
Iteration 22/1000 | Loss: 0.00002438
Iteration 23/1000 | Loss: 0.00002437
Iteration 24/1000 | Loss: 0.00002437
Iteration 25/1000 | Loss: 0.00002437
Iteration 26/1000 | Loss: 0.00002436
Iteration 27/1000 | Loss: 0.00002435
Iteration 28/1000 | Loss: 0.00002435
Iteration 29/1000 | Loss: 0.00002435
Iteration 30/1000 | Loss: 0.00002435
Iteration 31/1000 | Loss: 0.00002435
Iteration 32/1000 | Loss: 0.00002434
Iteration 33/1000 | Loss: 0.00002434
Iteration 34/1000 | Loss: 0.00002434
Iteration 35/1000 | Loss: 0.00002433
Iteration 36/1000 | Loss: 0.00002433
Iteration 37/1000 | Loss: 0.00002433
Iteration 38/1000 | Loss: 0.00002433
Iteration 39/1000 | Loss: 0.00002433
Iteration 40/1000 | Loss: 0.00002433
Iteration 41/1000 | Loss: 0.00002433
Iteration 42/1000 | Loss: 0.00002432
Iteration 43/1000 | Loss: 0.00002432
Iteration 44/1000 | Loss: 0.00002431
Iteration 45/1000 | Loss: 0.00002431
Iteration 46/1000 | Loss: 0.00002431
Iteration 47/1000 | Loss: 0.00002431
Iteration 48/1000 | Loss: 0.00002430
Iteration 49/1000 | Loss: 0.00002430
Iteration 50/1000 | Loss: 0.00002430
Iteration 51/1000 | Loss: 0.00002430
Iteration 52/1000 | Loss: 0.00002430
Iteration 53/1000 | Loss: 0.00002430
Iteration 54/1000 | Loss: 0.00002429
Iteration 55/1000 | Loss: 0.00002429
Iteration 56/1000 | Loss: 0.00002429
Iteration 57/1000 | Loss: 0.00002428
Iteration 58/1000 | Loss: 0.00002428
Iteration 59/1000 | Loss: 0.00002428
Iteration 60/1000 | Loss: 0.00002428
Iteration 61/1000 | Loss: 0.00002428
Iteration 62/1000 | Loss: 0.00002428
Iteration 63/1000 | Loss: 0.00002427
Iteration 64/1000 | Loss: 0.00002427
Iteration 65/1000 | Loss: 0.00002427
Iteration 66/1000 | Loss: 0.00002426
Iteration 67/1000 | Loss: 0.00002426
Iteration 68/1000 | Loss: 0.00002426
Iteration 69/1000 | Loss: 0.00002426
Iteration 70/1000 | Loss: 0.00002425
Iteration 71/1000 | Loss: 0.00002425
Iteration 72/1000 | Loss: 0.00002425
Iteration 73/1000 | Loss: 0.00002424
Iteration 74/1000 | Loss: 0.00002424
Iteration 75/1000 | Loss: 0.00002424
Iteration 76/1000 | Loss: 0.00002424
Iteration 77/1000 | Loss: 0.00002424
Iteration 78/1000 | Loss: 0.00002424
Iteration 79/1000 | Loss: 0.00002424
Iteration 80/1000 | Loss: 0.00002424
Iteration 81/1000 | Loss: 0.00002424
Iteration 82/1000 | Loss: 0.00002423
Iteration 83/1000 | Loss: 0.00002423
Iteration 84/1000 | Loss: 0.00002423
Iteration 85/1000 | Loss: 0.00002423
Iteration 86/1000 | Loss: 0.00002422
Iteration 87/1000 | Loss: 0.00002422
Iteration 88/1000 | Loss: 0.00002422
Iteration 89/1000 | Loss: 0.00002422
Iteration 90/1000 | Loss: 0.00002421
Iteration 91/1000 | Loss: 0.00002421
Iteration 92/1000 | Loss: 0.00002421
Iteration 93/1000 | Loss: 0.00002421
Iteration 94/1000 | Loss: 0.00002421
Iteration 95/1000 | Loss: 0.00002421
Iteration 96/1000 | Loss: 0.00002421
Iteration 97/1000 | Loss: 0.00002420
Iteration 98/1000 | Loss: 0.00002420
Iteration 99/1000 | Loss: 0.00002420
Iteration 100/1000 | Loss: 0.00002420
Iteration 101/1000 | Loss: 0.00002420
Iteration 102/1000 | Loss: 0.00002420
Iteration 103/1000 | Loss: 0.00002420
Iteration 104/1000 | Loss: 0.00002419
Iteration 105/1000 | Loss: 0.00002419
Iteration 106/1000 | Loss: 0.00002419
Iteration 107/1000 | Loss: 0.00002419
Iteration 108/1000 | Loss: 0.00002419
Iteration 109/1000 | Loss: 0.00002419
Iteration 110/1000 | Loss: 0.00002418
Iteration 111/1000 | Loss: 0.00002418
Iteration 112/1000 | Loss: 0.00002418
Iteration 113/1000 | Loss: 0.00002418
Iteration 114/1000 | Loss: 0.00002418
Iteration 115/1000 | Loss: 0.00002417
Iteration 116/1000 | Loss: 0.00002417
Iteration 117/1000 | Loss: 0.00002417
Iteration 118/1000 | Loss: 0.00002417
Iteration 119/1000 | Loss: 0.00002417
Iteration 120/1000 | Loss: 0.00002417
Iteration 121/1000 | Loss: 0.00002416
Iteration 122/1000 | Loss: 0.00002416
Iteration 123/1000 | Loss: 0.00002416
Iteration 124/1000 | Loss: 0.00002416
Iteration 125/1000 | Loss: 0.00002415
Iteration 126/1000 | Loss: 0.00002415
Iteration 127/1000 | Loss: 0.00002415
Iteration 128/1000 | Loss: 0.00002415
Iteration 129/1000 | Loss: 0.00002415
Iteration 130/1000 | Loss: 0.00002415
Iteration 131/1000 | Loss: 0.00002414
Iteration 132/1000 | Loss: 0.00002414
Iteration 133/1000 | Loss: 0.00002414
Iteration 134/1000 | Loss: 0.00002414
Iteration 135/1000 | Loss: 0.00002414
Iteration 136/1000 | Loss: 0.00002414
Iteration 137/1000 | Loss: 0.00002414
Iteration 138/1000 | Loss: 0.00002414
Iteration 139/1000 | Loss: 0.00002414
Iteration 140/1000 | Loss: 0.00002414
Iteration 141/1000 | Loss: 0.00002414
Iteration 142/1000 | Loss: 0.00002414
Iteration 143/1000 | Loss: 0.00002414
Iteration 144/1000 | Loss: 0.00002414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.4144184862961993e-05, 2.4144184862961993e-05, 2.4144184862961993e-05, 2.4144184862961993e-05, 2.4144184862961993e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4144184862961993e-05

Optimization complete. Final v2v error: 4.130374431610107 mm

Highest mean error: 4.87680196762085 mm for frame 2

Lowest mean error: 3.473012685775757 mm for frame 131

Saving results

Total time: 47.64887547492981
