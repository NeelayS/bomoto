Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=265, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14840-14895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077336
Iteration 2/25 | Loss: 0.00215757
Iteration 3/25 | Loss: 0.00122457
Iteration 4/25 | Loss: 0.00104016
Iteration 5/25 | Loss: 0.00097575
Iteration 6/25 | Loss: 0.00095483
Iteration 7/25 | Loss: 0.00087131
Iteration 8/25 | Loss: 0.00081451
Iteration 9/25 | Loss: 0.00078108
Iteration 10/25 | Loss: 0.00075885
Iteration 11/25 | Loss: 0.00075128
Iteration 12/25 | Loss: 0.00074341
Iteration 13/25 | Loss: 0.00073636
Iteration 14/25 | Loss: 0.00073922
Iteration 15/25 | Loss: 0.00073175
Iteration 16/25 | Loss: 0.00072734
Iteration 17/25 | Loss: 0.00072767
Iteration 18/25 | Loss: 0.00072562
Iteration 19/25 | Loss: 0.00072481
Iteration 20/25 | Loss: 0.00072457
Iteration 21/25 | Loss: 0.00072417
Iteration 22/25 | Loss: 0.00072381
Iteration 23/25 | Loss: 0.00072354
Iteration 24/25 | Loss: 0.00072349
Iteration 25/25 | Loss: 0.00072349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45816827
Iteration 2/25 | Loss: 0.00070716
Iteration 3/25 | Loss: 0.00070715
Iteration 4/25 | Loss: 0.00070715
Iteration 5/25 | Loss: 0.00070715
Iteration 6/25 | Loss: 0.00070715
Iteration 7/25 | Loss: 0.00070715
Iteration 8/25 | Loss: 0.00070715
Iteration 9/25 | Loss: 0.00070715
Iteration 10/25 | Loss: 0.00070715
Iteration 11/25 | Loss: 0.00070715
Iteration 12/25 | Loss: 0.00070715
Iteration 13/25 | Loss: 0.00070715
Iteration 14/25 | Loss: 0.00070715
Iteration 15/25 | Loss: 0.00070715
Iteration 16/25 | Loss: 0.00070715
Iteration 17/25 | Loss: 0.00070715
Iteration 18/25 | Loss: 0.00070715
Iteration 19/25 | Loss: 0.00070715
Iteration 20/25 | Loss: 0.00070715
Iteration 21/25 | Loss: 0.00070715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007071523577906191, 0.0007071523577906191, 0.0007071523577906191, 0.0007071523577906191, 0.0007071523577906191]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007071523577906191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070715
Iteration 2/1000 | Loss: 0.00011411
Iteration 3/1000 | Loss: 0.00008293
Iteration 4/1000 | Loss: 0.00014283
Iteration 5/1000 | Loss: 0.00006569
Iteration 6/1000 | Loss: 0.00144157
Iteration 7/1000 | Loss: 0.00017307
Iteration 8/1000 | Loss: 0.00006817
Iteration 9/1000 | Loss: 0.00015864
Iteration 10/1000 | Loss: 0.00025229
Iteration 11/1000 | Loss: 0.00047919
Iteration 12/1000 | Loss: 0.00003369
Iteration 13/1000 | Loss: 0.00002869
Iteration 14/1000 | Loss: 0.00003213
Iteration 15/1000 | Loss: 0.00002411
Iteration 16/1000 | Loss: 0.00002216
Iteration 17/1000 | Loss: 0.00002033
Iteration 18/1000 | Loss: 0.00001951
Iteration 19/1000 | Loss: 0.00001888
Iteration 20/1000 | Loss: 0.00001831
Iteration 21/1000 | Loss: 0.00001797
Iteration 22/1000 | Loss: 0.00001766
Iteration 23/1000 | Loss: 0.00001748
Iteration 24/1000 | Loss: 0.00001738
Iteration 25/1000 | Loss: 0.00001737
Iteration 26/1000 | Loss: 0.00001736
Iteration 27/1000 | Loss: 0.00001736
Iteration 28/1000 | Loss: 0.00001735
Iteration 29/1000 | Loss: 0.00001734
Iteration 30/1000 | Loss: 0.00001729
Iteration 31/1000 | Loss: 0.00001727
Iteration 32/1000 | Loss: 0.00001726
Iteration 33/1000 | Loss: 0.00001726
Iteration 34/1000 | Loss: 0.00001725
Iteration 35/1000 | Loss: 0.00001725
Iteration 36/1000 | Loss: 0.00001724
Iteration 37/1000 | Loss: 0.00001724
Iteration 38/1000 | Loss: 0.00001723
Iteration 39/1000 | Loss: 0.00001723
Iteration 40/1000 | Loss: 0.00001722
Iteration 41/1000 | Loss: 0.00001722
Iteration 42/1000 | Loss: 0.00001722
Iteration 43/1000 | Loss: 0.00001721
Iteration 44/1000 | Loss: 0.00001720
Iteration 45/1000 | Loss: 0.00001720
Iteration 46/1000 | Loss: 0.00001719
Iteration 47/1000 | Loss: 0.00001719
Iteration 48/1000 | Loss: 0.00001719
Iteration 49/1000 | Loss: 0.00001719
Iteration 50/1000 | Loss: 0.00001718
Iteration 51/1000 | Loss: 0.00001718
Iteration 52/1000 | Loss: 0.00001718
Iteration 53/1000 | Loss: 0.00001718
Iteration 54/1000 | Loss: 0.00001718
Iteration 55/1000 | Loss: 0.00001717
Iteration 56/1000 | Loss: 0.00001717
Iteration 57/1000 | Loss: 0.00001717
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001716
Iteration 62/1000 | Loss: 0.00001716
Iteration 63/1000 | Loss: 0.00001716
Iteration 64/1000 | Loss: 0.00001715
Iteration 65/1000 | Loss: 0.00001715
Iteration 66/1000 | Loss: 0.00001715
Iteration 67/1000 | Loss: 0.00001715
Iteration 68/1000 | Loss: 0.00001715
Iteration 69/1000 | Loss: 0.00001714
Iteration 70/1000 | Loss: 0.00001714
Iteration 71/1000 | Loss: 0.00001714
Iteration 72/1000 | Loss: 0.00001714
Iteration 73/1000 | Loss: 0.00001714
Iteration 74/1000 | Loss: 0.00001714
Iteration 75/1000 | Loss: 0.00001714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.714168865873944e-05, 1.714168865873944e-05, 1.714168865873944e-05, 1.714168865873944e-05, 1.714168865873944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.714168865873944e-05

Optimization complete. Final v2v error: 3.4499404430389404 mm

Highest mean error: 4.022923469543457 mm for frame 164

Lowest mean error: 3.1363861560821533 mm for frame 223

Saving results

Total time: 91.92609858512878
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981682
Iteration 2/25 | Loss: 0.00159010
Iteration 3/25 | Loss: 0.00091909
Iteration 4/25 | Loss: 0.00085589
Iteration 5/25 | Loss: 0.00083526
Iteration 6/25 | Loss: 0.00082904
Iteration 7/25 | Loss: 0.00082776
Iteration 8/25 | Loss: 0.00082765
Iteration 9/25 | Loss: 0.00082765
Iteration 10/25 | Loss: 0.00082765
Iteration 11/25 | Loss: 0.00082765
Iteration 12/25 | Loss: 0.00082765
Iteration 13/25 | Loss: 0.00082765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008276498992927372, 0.0008276498992927372, 0.0008276498992927372, 0.0008276498992927372, 0.0008276498992927372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008276498992927372

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96360868
Iteration 2/25 | Loss: 0.00026114
Iteration 3/25 | Loss: 0.00026114
Iteration 4/25 | Loss: 0.00026114
Iteration 5/25 | Loss: 0.00026114
Iteration 6/25 | Loss: 0.00026114
Iteration 7/25 | Loss: 0.00026114
Iteration 8/25 | Loss: 0.00026114
Iteration 9/25 | Loss: 0.00026114
Iteration 10/25 | Loss: 0.00026114
Iteration 11/25 | Loss: 0.00026114
Iteration 12/25 | Loss: 0.00026114
Iteration 13/25 | Loss: 0.00026114
Iteration 14/25 | Loss: 0.00026114
Iteration 15/25 | Loss: 0.00026114
Iteration 16/25 | Loss: 0.00026114
Iteration 17/25 | Loss: 0.00026114
Iteration 18/25 | Loss: 0.00026114
Iteration 19/25 | Loss: 0.00026114
Iteration 20/25 | Loss: 0.00026114
Iteration 21/25 | Loss: 0.00026114
Iteration 22/25 | Loss: 0.00026114
Iteration 23/25 | Loss: 0.00026114
Iteration 24/25 | Loss: 0.00026114
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00026113586500287056, 0.00026113586500287056, 0.00026113586500287056, 0.00026113586500287056, 0.00026113586500287056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026113586500287056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026114
Iteration 2/1000 | Loss: 0.00006372
Iteration 3/1000 | Loss: 0.00004354
Iteration 4/1000 | Loss: 0.00004060
Iteration 5/1000 | Loss: 0.00003852
Iteration 6/1000 | Loss: 0.00003759
Iteration 7/1000 | Loss: 0.00003671
Iteration 8/1000 | Loss: 0.00003601
Iteration 9/1000 | Loss: 0.00003566
Iteration 10/1000 | Loss: 0.00003533
Iteration 11/1000 | Loss: 0.00003506
Iteration 12/1000 | Loss: 0.00003487
Iteration 13/1000 | Loss: 0.00003463
Iteration 14/1000 | Loss: 0.00003451
Iteration 15/1000 | Loss: 0.00003436
Iteration 16/1000 | Loss: 0.00003423
Iteration 17/1000 | Loss: 0.00003420
Iteration 18/1000 | Loss: 0.00003418
Iteration 19/1000 | Loss: 0.00003417
Iteration 20/1000 | Loss: 0.00003413
Iteration 21/1000 | Loss: 0.00003412
Iteration 22/1000 | Loss: 0.00003410
Iteration 23/1000 | Loss: 0.00003409
Iteration 24/1000 | Loss: 0.00003409
Iteration 25/1000 | Loss: 0.00003408
Iteration 26/1000 | Loss: 0.00003408
Iteration 27/1000 | Loss: 0.00003405
Iteration 28/1000 | Loss: 0.00003404
Iteration 29/1000 | Loss: 0.00003403
Iteration 30/1000 | Loss: 0.00003403
Iteration 31/1000 | Loss: 0.00003402
Iteration 32/1000 | Loss: 0.00003402
Iteration 33/1000 | Loss: 0.00003401
Iteration 34/1000 | Loss: 0.00003401
Iteration 35/1000 | Loss: 0.00003399
Iteration 36/1000 | Loss: 0.00003399
Iteration 37/1000 | Loss: 0.00003398
Iteration 38/1000 | Loss: 0.00003397
Iteration 39/1000 | Loss: 0.00003396
Iteration 40/1000 | Loss: 0.00003396
Iteration 41/1000 | Loss: 0.00003396
Iteration 42/1000 | Loss: 0.00003396
Iteration 43/1000 | Loss: 0.00003395
Iteration 44/1000 | Loss: 0.00003395
Iteration 45/1000 | Loss: 0.00003394
Iteration 46/1000 | Loss: 0.00003394
Iteration 47/1000 | Loss: 0.00003394
Iteration 48/1000 | Loss: 0.00003394
Iteration 49/1000 | Loss: 0.00003394
Iteration 50/1000 | Loss: 0.00003394
Iteration 51/1000 | Loss: 0.00003393
Iteration 52/1000 | Loss: 0.00003393
Iteration 53/1000 | Loss: 0.00003393
Iteration 54/1000 | Loss: 0.00003393
Iteration 55/1000 | Loss: 0.00003393
Iteration 56/1000 | Loss: 0.00003392
Iteration 57/1000 | Loss: 0.00003392
Iteration 58/1000 | Loss: 0.00003392
Iteration 59/1000 | Loss: 0.00003391
Iteration 60/1000 | Loss: 0.00003391
Iteration 61/1000 | Loss: 0.00003391
Iteration 62/1000 | Loss: 0.00003390
Iteration 63/1000 | Loss: 0.00003390
Iteration 64/1000 | Loss: 0.00003390
Iteration 65/1000 | Loss: 0.00003389
Iteration 66/1000 | Loss: 0.00003388
Iteration 67/1000 | Loss: 0.00003388
Iteration 68/1000 | Loss: 0.00003388
Iteration 69/1000 | Loss: 0.00003387
Iteration 70/1000 | Loss: 0.00003387
Iteration 71/1000 | Loss: 0.00003387
Iteration 72/1000 | Loss: 0.00003387
Iteration 73/1000 | Loss: 0.00003387
Iteration 74/1000 | Loss: 0.00003386
Iteration 75/1000 | Loss: 0.00003385
Iteration 76/1000 | Loss: 0.00003385
Iteration 77/1000 | Loss: 0.00003385
Iteration 78/1000 | Loss: 0.00003385
Iteration 79/1000 | Loss: 0.00003385
Iteration 80/1000 | Loss: 0.00003385
Iteration 81/1000 | Loss: 0.00003385
Iteration 82/1000 | Loss: 0.00003385
Iteration 83/1000 | Loss: 0.00003385
Iteration 84/1000 | Loss: 0.00003385
Iteration 85/1000 | Loss: 0.00003385
Iteration 86/1000 | Loss: 0.00003384
Iteration 87/1000 | Loss: 0.00003384
Iteration 88/1000 | Loss: 0.00003384
Iteration 89/1000 | Loss: 0.00003384
Iteration 90/1000 | Loss: 0.00003384
Iteration 91/1000 | Loss: 0.00003384
Iteration 92/1000 | Loss: 0.00003384
Iteration 93/1000 | Loss: 0.00003384
Iteration 94/1000 | Loss: 0.00003384
Iteration 95/1000 | Loss: 0.00003384
Iteration 96/1000 | Loss: 0.00003384
Iteration 97/1000 | Loss: 0.00003383
Iteration 98/1000 | Loss: 0.00003383
Iteration 99/1000 | Loss: 0.00003383
Iteration 100/1000 | Loss: 0.00003382
Iteration 101/1000 | Loss: 0.00003382
Iteration 102/1000 | Loss: 0.00003381
Iteration 103/1000 | Loss: 0.00003381
Iteration 104/1000 | Loss: 0.00003381
Iteration 105/1000 | Loss: 0.00003381
Iteration 106/1000 | Loss: 0.00003381
Iteration 107/1000 | Loss: 0.00003380
Iteration 108/1000 | Loss: 0.00003380
Iteration 109/1000 | Loss: 0.00003380
Iteration 110/1000 | Loss: 0.00003380
Iteration 111/1000 | Loss: 0.00003380
Iteration 112/1000 | Loss: 0.00003380
Iteration 113/1000 | Loss: 0.00003380
Iteration 114/1000 | Loss: 0.00003380
Iteration 115/1000 | Loss: 0.00003380
Iteration 116/1000 | Loss: 0.00003380
Iteration 117/1000 | Loss: 0.00003380
Iteration 118/1000 | Loss: 0.00003380
Iteration 119/1000 | Loss: 0.00003379
Iteration 120/1000 | Loss: 0.00003379
Iteration 121/1000 | Loss: 0.00003379
Iteration 122/1000 | Loss: 0.00003379
Iteration 123/1000 | Loss: 0.00003379
Iteration 124/1000 | Loss: 0.00003379
Iteration 125/1000 | Loss: 0.00003379
Iteration 126/1000 | Loss: 0.00003379
Iteration 127/1000 | Loss: 0.00003378
Iteration 128/1000 | Loss: 0.00003378
Iteration 129/1000 | Loss: 0.00003378
Iteration 130/1000 | Loss: 0.00003378
Iteration 131/1000 | Loss: 0.00003378
Iteration 132/1000 | Loss: 0.00003378
Iteration 133/1000 | Loss: 0.00003377
Iteration 134/1000 | Loss: 0.00003377
Iteration 135/1000 | Loss: 0.00003377
Iteration 136/1000 | Loss: 0.00003377
Iteration 137/1000 | Loss: 0.00003377
Iteration 138/1000 | Loss: 0.00003377
Iteration 139/1000 | Loss: 0.00003377
Iteration 140/1000 | Loss: 0.00003377
Iteration 141/1000 | Loss: 0.00003377
Iteration 142/1000 | Loss: 0.00003376
Iteration 143/1000 | Loss: 0.00003376
Iteration 144/1000 | Loss: 0.00003376
Iteration 145/1000 | Loss: 0.00003376
Iteration 146/1000 | Loss: 0.00003376
Iteration 147/1000 | Loss: 0.00003376
Iteration 148/1000 | Loss: 0.00003376
Iteration 149/1000 | Loss: 0.00003376
Iteration 150/1000 | Loss: 0.00003376
Iteration 151/1000 | Loss: 0.00003375
Iteration 152/1000 | Loss: 0.00003375
Iteration 153/1000 | Loss: 0.00003375
Iteration 154/1000 | Loss: 0.00003375
Iteration 155/1000 | Loss: 0.00003375
Iteration 156/1000 | Loss: 0.00003375
Iteration 157/1000 | Loss: 0.00003375
Iteration 158/1000 | Loss: 0.00003375
Iteration 159/1000 | Loss: 0.00003375
Iteration 160/1000 | Loss: 0.00003374
Iteration 161/1000 | Loss: 0.00003374
Iteration 162/1000 | Loss: 0.00003374
Iteration 163/1000 | Loss: 0.00003374
Iteration 164/1000 | Loss: 0.00003374
Iteration 165/1000 | Loss: 0.00003374
Iteration 166/1000 | Loss: 0.00003374
Iteration 167/1000 | Loss: 0.00003373
Iteration 168/1000 | Loss: 0.00003373
Iteration 169/1000 | Loss: 0.00003373
Iteration 170/1000 | Loss: 0.00003373
Iteration 171/1000 | Loss: 0.00003373
Iteration 172/1000 | Loss: 0.00003373
Iteration 173/1000 | Loss: 0.00003373
Iteration 174/1000 | Loss: 0.00003373
Iteration 175/1000 | Loss: 0.00003373
Iteration 176/1000 | Loss: 0.00003373
Iteration 177/1000 | Loss: 0.00003372
Iteration 178/1000 | Loss: 0.00003372
Iteration 179/1000 | Loss: 0.00003372
Iteration 180/1000 | Loss: 0.00003372
Iteration 181/1000 | Loss: 0.00003372
Iteration 182/1000 | Loss: 0.00003372
Iteration 183/1000 | Loss: 0.00003372
Iteration 184/1000 | Loss: 0.00003372
Iteration 185/1000 | Loss: 0.00003372
Iteration 186/1000 | Loss: 0.00003372
Iteration 187/1000 | Loss: 0.00003372
Iteration 188/1000 | Loss: 0.00003372
Iteration 189/1000 | Loss: 0.00003372
Iteration 190/1000 | Loss: 0.00003372
Iteration 191/1000 | Loss: 0.00003372
Iteration 192/1000 | Loss: 0.00003372
Iteration 193/1000 | Loss: 0.00003371
Iteration 194/1000 | Loss: 0.00003371
Iteration 195/1000 | Loss: 0.00003371
Iteration 196/1000 | Loss: 0.00003371
Iteration 197/1000 | Loss: 0.00003371
Iteration 198/1000 | Loss: 0.00003371
Iteration 199/1000 | Loss: 0.00003371
Iteration 200/1000 | Loss: 0.00003371
Iteration 201/1000 | Loss: 0.00003371
Iteration 202/1000 | Loss: 0.00003371
Iteration 203/1000 | Loss: 0.00003371
Iteration 204/1000 | Loss: 0.00003371
Iteration 205/1000 | Loss: 0.00003371
Iteration 206/1000 | Loss: 0.00003371
Iteration 207/1000 | Loss: 0.00003371
Iteration 208/1000 | Loss: 0.00003371
Iteration 209/1000 | Loss: 0.00003371
Iteration 210/1000 | Loss: 0.00003371
Iteration 211/1000 | Loss: 0.00003371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [3.3711236028466374e-05, 3.3711236028466374e-05, 3.3711236028466374e-05, 3.3711236028466374e-05, 3.3711236028466374e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3711236028466374e-05

Optimization complete. Final v2v error: 4.76021671295166 mm

Highest mean error: 6.16595458984375 mm for frame 119

Lowest mean error: 3.921067714691162 mm for frame 1

Saving results

Total time: 56.20426321029663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827114
Iteration 2/25 | Loss: 0.00099755
Iteration 3/25 | Loss: 0.00067253
Iteration 4/25 | Loss: 0.00061408
Iteration 5/25 | Loss: 0.00060241
Iteration 6/25 | Loss: 0.00060018
Iteration 7/25 | Loss: 0.00059990
Iteration 8/25 | Loss: 0.00059990
Iteration 9/25 | Loss: 0.00059990
Iteration 10/25 | Loss: 0.00059990
Iteration 11/25 | Loss: 0.00059990
Iteration 12/25 | Loss: 0.00059990
Iteration 13/25 | Loss: 0.00059990
Iteration 14/25 | Loss: 0.00059990
Iteration 15/25 | Loss: 0.00059990
Iteration 16/25 | Loss: 0.00059990
Iteration 17/25 | Loss: 0.00059990
Iteration 18/25 | Loss: 0.00059990
Iteration 19/25 | Loss: 0.00059990
Iteration 20/25 | Loss: 0.00059990
Iteration 21/25 | Loss: 0.00059990
Iteration 22/25 | Loss: 0.00059990
Iteration 23/25 | Loss: 0.00059990
Iteration 24/25 | Loss: 0.00059990
Iteration 25/25 | Loss: 0.00059990

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24940133
Iteration 2/25 | Loss: 0.00020130
Iteration 3/25 | Loss: 0.00020130
Iteration 4/25 | Loss: 0.00020130
Iteration 5/25 | Loss: 0.00020130
Iteration 6/25 | Loss: 0.00020130
Iteration 7/25 | Loss: 0.00020130
Iteration 8/25 | Loss: 0.00020130
Iteration 9/25 | Loss: 0.00020130
Iteration 10/25 | Loss: 0.00020130
Iteration 11/25 | Loss: 0.00020130
Iteration 12/25 | Loss: 0.00020130
Iteration 13/25 | Loss: 0.00020130
Iteration 14/25 | Loss: 0.00020130
Iteration 15/25 | Loss: 0.00020130
Iteration 16/25 | Loss: 0.00020130
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00020129734184592962, 0.00020129734184592962, 0.00020129734184592962, 0.00020129734184592962, 0.00020129734184592962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020129734184592962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020130
Iteration 2/1000 | Loss: 0.00003423
Iteration 3/1000 | Loss: 0.00002494
Iteration 4/1000 | Loss: 0.00002175
Iteration 5/1000 | Loss: 0.00002033
Iteration 6/1000 | Loss: 0.00001935
Iteration 7/1000 | Loss: 0.00001880
Iteration 8/1000 | Loss: 0.00001836
Iteration 9/1000 | Loss: 0.00001791
Iteration 10/1000 | Loss: 0.00001772
Iteration 11/1000 | Loss: 0.00001755
Iteration 12/1000 | Loss: 0.00001754
Iteration 13/1000 | Loss: 0.00001750
Iteration 14/1000 | Loss: 0.00001745
Iteration 15/1000 | Loss: 0.00001745
Iteration 16/1000 | Loss: 0.00001735
Iteration 17/1000 | Loss: 0.00001730
Iteration 18/1000 | Loss: 0.00001730
Iteration 19/1000 | Loss: 0.00001729
Iteration 20/1000 | Loss: 0.00001727
Iteration 21/1000 | Loss: 0.00001727
Iteration 22/1000 | Loss: 0.00001726
Iteration 23/1000 | Loss: 0.00001722
Iteration 24/1000 | Loss: 0.00001720
Iteration 25/1000 | Loss: 0.00001719
Iteration 26/1000 | Loss: 0.00001718
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001717
Iteration 29/1000 | Loss: 0.00001716
Iteration 30/1000 | Loss: 0.00001716
Iteration 31/1000 | Loss: 0.00001715
Iteration 32/1000 | Loss: 0.00001712
Iteration 33/1000 | Loss: 0.00001712
Iteration 34/1000 | Loss: 0.00001711
Iteration 35/1000 | Loss: 0.00001711
Iteration 36/1000 | Loss: 0.00001710
Iteration 37/1000 | Loss: 0.00001710
Iteration 38/1000 | Loss: 0.00001709
Iteration 39/1000 | Loss: 0.00001709
Iteration 40/1000 | Loss: 0.00001709
Iteration 41/1000 | Loss: 0.00001709
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001708
Iteration 44/1000 | Loss: 0.00001708
Iteration 45/1000 | Loss: 0.00001708
Iteration 46/1000 | Loss: 0.00001708
Iteration 47/1000 | Loss: 0.00001708
Iteration 48/1000 | Loss: 0.00001708
Iteration 49/1000 | Loss: 0.00001707
Iteration 50/1000 | Loss: 0.00001707
Iteration 51/1000 | Loss: 0.00001707
Iteration 52/1000 | Loss: 0.00001706
Iteration 53/1000 | Loss: 0.00001706
Iteration 54/1000 | Loss: 0.00001706
Iteration 55/1000 | Loss: 0.00001706
Iteration 56/1000 | Loss: 0.00001705
Iteration 57/1000 | Loss: 0.00001705
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001705
Iteration 60/1000 | Loss: 0.00001704
Iteration 61/1000 | Loss: 0.00001704
Iteration 62/1000 | Loss: 0.00001704
Iteration 63/1000 | Loss: 0.00001704
Iteration 64/1000 | Loss: 0.00001704
Iteration 65/1000 | Loss: 0.00001704
Iteration 66/1000 | Loss: 0.00001704
Iteration 67/1000 | Loss: 0.00001704
Iteration 68/1000 | Loss: 0.00001704
Iteration 69/1000 | Loss: 0.00001704
Iteration 70/1000 | Loss: 0.00001704
Iteration 71/1000 | Loss: 0.00001704
Iteration 72/1000 | Loss: 0.00001704
Iteration 73/1000 | Loss: 0.00001704
Iteration 74/1000 | Loss: 0.00001704
Iteration 75/1000 | Loss: 0.00001704
Iteration 76/1000 | Loss: 0.00001704
Iteration 77/1000 | Loss: 0.00001704
Iteration 78/1000 | Loss: 0.00001704
Iteration 79/1000 | Loss: 0.00001704
Iteration 80/1000 | Loss: 0.00001704
Iteration 81/1000 | Loss: 0.00001704
Iteration 82/1000 | Loss: 0.00001704
Iteration 83/1000 | Loss: 0.00001704
Iteration 84/1000 | Loss: 0.00001704
Iteration 85/1000 | Loss: 0.00001704
Iteration 86/1000 | Loss: 0.00001704
Iteration 87/1000 | Loss: 0.00001704
Iteration 88/1000 | Loss: 0.00001704
Iteration 89/1000 | Loss: 0.00001704
Iteration 90/1000 | Loss: 0.00001704
Iteration 91/1000 | Loss: 0.00001704
Iteration 92/1000 | Loss: 0.00001704
Iteration 93/1000 | Loss: 0.00001704
Iteration 94/1000 | Loss: 0.00001704
Iteration 95/1000 | Loss: 0.00001704
Iteration 96/1000 | Loss: 0.00001704
Iteration 97/1000 | Loss: 0.00001704
Iteration 98/1000 | Loss: 0.00001704
Iteration 99/1000 | Loss: 0.00001704
Iteration 100/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.7037549696397036e-05, 1.7037549696397036e-05, 1.7037549696397036e-05, 1.7037549696397036e-05, 1.7037549696397036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7037549696397036e-05

Optimization complete. Final v2v error: 3.4158108234405518 mm

Highest mean error: 5.045504093170166 mm for frame 69

Lowest mean error: 2.7231080532073975 mm for frame 101

Saving results

Total time: 34.93057155609131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068599
Iteration 2/25 | Loss: 0.00243260
Iteration 3/25 | Loss: 0.00129296
Iteration 4/25 | Loss: 0.00124870
Iteration 5/25 | Loss: 0.00110103
Iteration 6/25 | Loss: 0.00107255
Iteration 7/25 | Loss: 0.00102187
Iteration 8/25 | Loss: 0.00098769
Iteration 9/25 | Loss: 0.00097081
Iteration 10/25 | Loss: 0.00097248
Iteration 11/25 | Loss: 0.00096453
Iteration 12/25 | Loss: 0.00095874
Iteration 13/25 | Loss: 0.00094951
Iteration 14/25 | Loss: 0.00093417
Iteration 15/25 | Loss: 0.00092729
Iteration 16/25 | Loss: 0.00092146
Iteration 17/25 | Loss: 0.00090823
Iteration 18/25 | Loss: 0.00090320
Iteration 19/25 | Loss: 0.00090242
Iteration 20/25 | Loss: 0.00090279
Iteration 21/25 | Loss: 0.00089585
Iteration 22/25 | Loss: 0.00088247
Iteration 23/25 | Loss: 0.00087298
Iteration 24/25 | Loss: 0.00086945
Iteration 25/25 | Loss: 0.00086598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49742496
Iteration 2/25 | Loss: 0.00174083
Iteration 3/25 | Loss: 0.00174083
Iteration 4/25 | Loss: 0.00174083
Iteration 5/25 | Loss: 0.00174083
Iteration 6/25 | Loss: 0.00174083
Iteration 7/25 | Loss: 0.00174083
Iteration 8/25 | Loss: 0.00174083
Iteration 9/25 | Loss: 0.00174083
Iteration 10/25 | Loss: 0.00174083
Iteration 11/25 | Loss: 0.00174083
Iteration 12/25 | Loss: 0.00174083
Iteration 13/25 | Loss: 0.00174083
Iteration 14/25 | Loss: 0.00174083
Iteration 15/25 | Loss: 0.00174083
Iteration 16/25 | Loss: 0.00174083
Iteration 17/25 | Loss: 0.00174083
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017408276908099651, 0.0017408276908099651, 0.0017408276908099651, 0.0017408276908099651, 0.0017408276908099651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017408276908099651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174083
Iteration 2/1000 | Loss: 0.00188139
Iteration 3/1000 | Loss: 0.00060678
Iteration 4/1000 | Loss: 0.00041052
Iteration 5/1000 | Loss: 0.00037820
Iteration 6/1000 | Loss: 0.00019366
Iteration 7/1000 | Loss: 0.00017493
Iteration 8/1000 | Loss: 0.00036344
Iteration 9/1000 | Loss: 0.00111333
Iteration 10/1000 | Loss: 0.00591820
Iteration 11/1000 | Loss: 0.01165556
Iteration 12/1000 | Loss: 0.00222466
Iteration 13/1000 | Loss: 0.00178769
Iteration 14/1000 | Loss: 0.00171849
Iteration 15/1000 | Loss: 0.00090296
Iteration 16/1000 | Loss: 0.00061422
Iteration 17/1000 | Loss: 0.00043701
Iteration 18/1000 | Loss: 0.00023927
Iteration 19/1000 | Loss: 0.00098937
Iteration 20/1000 | Loss: 0.00008955
Iteration 21/1000 | Loss: 0.00023260
Iteration 22/1000 | Loss: 0.00004626
Iteration 23/1000 | Loss: 0.00003646
Iteration 24/1000 | Loss: 0.00002912
Iteration 25/1000 | Loss: 0.00002490
Iteration 26/1000 | Loss: 0.00047118
Iteration 27/1000 | Loss: 0.00005641
Iteration 28/1000 | Loss: 0.00002796
Iteration 29/1000 | Loss: 0.00002197
Iteration 30/1000 | Loss: 0.00069290
Iteration 31/1000 | Loss: 0.00038151
Iteration 32/1000 | Loss: 0.00095478
Iteration 33/1000 | Loss: 0.00002435
Iteration 34/1000 | Loss: 0.00024038
Iteration 35/1000 | Loss: 0.00003176
Iteration 36/1000 | Loss: 0.00004625
Iteration 37/1000 | Loss: 0.00010155
Iteration 38/1000 | Loss: 0.00062075
Iteration 39/1000 | Loss: 0.00003154
Iteration 40/1000 | Loss: 0.00002447
Iteration 41/1000 | Loss: 0.00002064
Iteration 42/1000 | Loss: 0.00001698
Iteration 43/1000 | Loss: 0.00001488
Iteration 44/1000 | Loss: 0.00001338
Iteration 45/1000 | Loss: 0.00001272
Iteration 46/1000 | Loss: 0.00001219
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001160
Iteration 49/1000 | Loss: 0.00001139
Iteration 50/1000 | Loss: 0.00001127
Iteration 51/1000 | Loss: 0.00001116
Iteration 52/1000 | Loss: 0.00001112
Iteration 53/1000 | Loss: 0.00001109
Iteration 54/1000 | Loss: 0.00001107
Iteration 55/1000 | Loss: 0.00001107
Iteration 56/1000 | Loss: 0.00001106
Iteration 57/1000 | Loss: 0.00001106
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001104
Iteration 61/1000 | Loss: 0.00001104
Iteration 62/1000 | Loss: 0.00001104
Iteration 63/1000 | Loss: 0.00001103
Iteration 64/1000 | Loss: 0.00001103
Iteration 65/1000 | Loss: 0.00001102
Iteration 66/1000 | Loss: 0.00001100
Iteration 67/1000 | Loss: 0.00001100
Iteration 68/1000 | Loss: 0.00001099
Iteration 69/1000 | Loss: 0.00001099
Iteration 70/1000 | Loss: 0.00001098
Iteration 71/1000 | Loss: 0.00001098
Iteration 72/1000 | Loss: 0.00001098
Iteration 73/1000 | Loss: 0.00001097
Iteration 74/1000 | Loss: 0.00001097
Iteration 75/1000 | Loss: 0.00001096
Iteration 76/1000 | Loss: 0.00001096
Iteration 77/1000 | Loss: 0.00001096
Iteration 78/1000 | Loss: 0.00001095
Iteration 79/1000 | Loss: 0.00001095
Iteration 80/1000 | Loss: 0.00001095
Iteration 81/1000 | Loss: 0.00001094
Iteration 82/1000 | Loss: 0.00001094
Iteration 83/1000 | Loss: 0.00001094
Iteration 84/1000 | Loss: 0.00001094
Iteration 85/1000 | Loss: 0.00001094
Iteration 86/1000 | Loss: 0.00001093
Iteration 87/1000 | Loss: 0.00001093
Iteration 88/1000 | Loss: 0.00001093
Iteration 89/1000 | Loss: 0.00001092
Iteration 90/1000 | Loss: 0.00001092
Iteration 91/1000 | Loss: 0.00001092
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001091
Iteration 94/1000 | Loss: 0.00001090
Iteration 95/1000 | Loss: 0.00001090
Iteration 96/1000 | Loss: 0.00001090
Iteration 97/1000 | Loss: 0.00001089
Iteration 98/1000 | Loss: 0.00001089
Iteration 99/1000 | Loss: 0.00001089
Iteration 100/1000 | Loss: 0.00001089
Iteration 101/1000 | Loss: 0.00001088
Iteration 102/1000 | Loss: 0.00001088
Iteration 103/1000 | Loss: 0.00001088
Iteration 104/1000 | Loss: 0.00001088
Iteration 105/1000 | Loss: 0.00001087
Iteration 106/1000 | Loss: 0.00001087
Iteration 107/1000 | Loss: 0.00001087
Iteration 108/1000 | Loss: 0.00001087
Iteration 109/1000 | Loss: 0.00001087
Iteration 110/1000 | Loss: 0.00001087
Iteration 111/1000 | Loss: 0.00001087
Iteration 112/1000 | Loss: 0.00001087
Iteration 113/1000 | Loss: 0.00001087
Iteration 114/1000 | Loss: 0.00001087
Iteration 115/1000 | Loss: 0.00001086
Iteration 116/1000 | Loss: 0.00001086
Iteration 117/1000 | Loss: 0.00001086
Iteration 118/1000 | Loss: 0.00001086
Iteration 119/1000 | Loss: 0.00001086
Iteration 120/1000 | Loss: 0.00001086
Iteration 121/1000 | Loss: 0.00001086
Iteration 122/1000 | Loss: 0.00001086
Iteration 123/1000 | Loss: 0.00001086
Iteration 124/1000 | Loss: 0.00001086
Iteration 125/1000 | Loss: 0.00001085
Iteration 126/1000 | Loss: 0.00001085
Iteration 127/1000 | Loss: 0.00001085
Iteration 128/1000 | Loss: 0.00001085
Iteration 129/1000 | Loss: 0.00001085
Iteration 130/1000 | Loss: 0.00001085
Iteration 131/1000 | Loss: 0.00001085
Iteration 132/1000 | Loss: 0.00001085
Iteration 133/1000 | Loss: 0.00001085
Iteration 134/1000 | Loss: 0.00001085
Iteration 135/1000 | Loss: 0.00001085
Iteration 136/1000 | Loss: 0.00001085
Iteration 137/1000 | Loss: 0.00001085
Iteration 138/1000 | Loss: 0.00001085
Iteration 139/1000 | Loss: 0.00001085
Iteration 140/1000 | Loss: 0.00001085
Iteration 141/1000 | Loss: 0.00001084
Iteration 142/1000 | Loss: 0.00001084
Iteration 143/1000 | Loss: 0.00001084
Iteration 144/1000 | Loss: 0.00001084
Iteration 145/1000 | Loss: 0.00001084
Iteration 146/1000 | Loss: 0.00001084
Iteration 147/1000 | Loss: 0.00001084
Iteration 148/1000 | Loss: 0.00001084
Iteration 149/1000 | Loss: 0.00001084
Iteration 150/1000 | Loss: 0.00001084
Iteration 151/1000 | Loss: 0.00001084
Iteration 152/1000 | Loss: 0.00001084
Iteration 153/1000 | Loss: 0.00001084
Iteration 154/1000 | Loss: 0.00001084
Iteration 155/1000 | Loss: 0.00001084
Iteration 156/1000 | Loss: 0.00001084
Iteration 157/1000 | Loss: 0.00001084
Iteration 158/1000 | Loss: 0.00001084
Iteration 159/1000 | Loss: 0.00001084
Iteration 160/1000 | Loss: 0.00001084
Iteration 161/1000 | Loss: 0.00001084
Iteration 162/1000 | Loss: 0.00001084
Iteration 163/1000 | Loss: 0.00001084
Iteration 164/1000 | Loss: 0.00001084
Iteration 165/1000 | Loss: 0.00001084
Iteration 166/1000 | Loss: 0.00001084
Iteration 167/1000 | Loss: 0.00001084
Iteration 168/1000 | Loss: 0.00001084
Iteration 169/1000 | Loss: 0.00001084
Iteration 170/1000 | Loss: 0.00001084
Iteration 171/1000 | Loss: 0.00001084
Iteration 172/1000 | Loss: 0.00001084
Iteration 173/1000 | Loss: 0.00001084
Iteration 174/1000 | Loss: 0.00001084
Iteration 175/1000 | Loss: 0.00001084
Iteration 176/1000 | Loss: 0.00001084
Iteration 177/1000 | Loss: 0.00001084
Iteration 178/1000 | Loss: 0.00001084
Iteration 179/1000 | Loss: 0.00001084
Iteration 180/1000 | Loss: 0.00001084
Iteration 181/1000 | Loss: 0.00001084
Iteration 182/1000 | Loss: 0.00001084
Iteration 183/1000 | Loss: 0.00001084
Iteration 184/1000 | Loss: 0.00001084
Iteration 185/1000 | Loss: 0.00001084
Iteration 186/1000 | Loss: 0.00001084
Iteration 187/1000 | Loss: 0.00001084
Iteration 188/1000 | Loss: 0.00001084
Iteration 189/1000 | Loss: 0.00001084
Iteration 190/1000 | Loss: 0.00001084
Iteration 191/1000 | Loss: 0.00001084
Iteration 192/1000 | Loss: 0.00001084
Iteration 193/1000 | Loss: 0.00001084
Iteration 194/1000 | Loss: 0.00001084
Iteration 195/1000 | Loss: 0.00001084
Iteration 196/1000 | Loss: 0.00001084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.0835550710908137e-05, 1.0835550710908137e-05, 1.0835550710908137e-05, 1.0835550710908137e-05, 1.0835550710908137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0835550710908137e-05

Optimization complete. Final v2v error: 2.786285877227783 mm

Highest mean error: 3.486090660095215 mm for frame 40

Lowest mean error: 2.370229959487915 mm for frame 6

Saving results

Total time: 133.6601893901825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790899
Iteration 2/25 | Loss: 0.00095539
Iteration 3/25 | Loss: 0.00073227
Iteration 4/25 | Loss: 0.00068225
Iteration 5/25 | Loss: 0.00066355
Iteration 6/25 | Loss: 0.00065967
Iteration 7/25 | Loss: 0.00065914
Iteration 8/25 | Loss: 0.00065914
Iteration 9/25 | Loss: 0.00065914
Iteration 10/25 | Loss: 0.00065914
Iteration 11/25 | Loss: 0.00065914
Iteration 12/25 | Loss: 0.00065914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006591440178453922, 0.0006591440178453922, 0.0006591440178453922, 0.0006591440178453922, 0.0006591440178453922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006591440178453922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53611124
Iteration 2/25 | Loss: 0.00024467
Iteration 3/25 | Loss: 0.00024467
Iteration 4/25 | Loss: 0.00024467
Iteration 5/25 | Loss: 0.00024467
Iteration 6/25 | Loss: 0.00024467
Iteration 7/25 | Loss: 0.00024467
Iteration 8/25 | Loss: 0.00024467
Iteration 9/25 | Loss: 0.00024467
Iteration 10/25 | Loss: 0.00024467
Iteration 11/25 | Loss: 0.00024467
Iteration 12/25 | Loss: 0.00024467
Iteration 13/25 | Loss: 0.00024467
Iteration 14/25 | Loss: 0.00024467
Iteration 15/25 | Loss: 0.00024467
Iteration 16/25 | Loss: 0.00024467
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002446676080580801, 0.0002446676080580801, 0.0002446676080580801, 0.0002446676080580801, 0.0002446676080580801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002446676080580801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024467
Iteration 2/1000 | Loss: 0.00003670
Iteration 3/1000 | Loss: 0.00002660
Iteration 4/1000 | Loss: 0.00002424
Iteration 5/1000 | Loss: 0.00002318
Iteration 6/1000 | Loss: 0.00002213
Iteration 7/1000 | Loss: 0.00002164
Iteration 8/1000 | Loss: 0.00002103
Iteration 9/1000 | Loss: 0.00002064
Iteration 10/1000 | Loss: 0.00002036
Iteration 11/1000 | Loss: 0.00002014
Iteration 12/1000 | Loss: 0.00002014
Iteration 13/1000 | Loss: 0.00002001
Iteration 14/1000 | Loss: 0.00001989
Iteration 15/1000 | Loss: 0.00001988
Iteration 16/1000 | Loss: 0.00001984
Iteration 17/1000 | Loss: 0.00001980
Iteration 18/1000 | Loss: 0.00001979
Iteration 19/1000 | Loss: 0.00001979
Iteration 20/1000 | Loss: 0.00001979
Iteration 21/1000 | Loss: 0.00001979
Iteration 22/1000 | Loss: 0.00001978
Iteration 23/1000 | Loss: 0.00001978
Iteration 24/1000 | Loss: 0.00001977
Iteration 25/1000 | Loss: 0.00001977
Iteration 26/1000 | Loss: 0.00001976
Iteration 27/1000 | Loss: 0.00001976
Iteration 28/1000 | Loss: 0.00001975
Iteration 29/1000 | Loss: 0.00001975
Iteration 30/1000 | Loss: 0.00001974
Iteration 31/1000 | Loss: 0.00001973
Iteration 32/1000 | Loss: 0.00001973
Iteration 33/1000 | Loss: 0.00001973
Iteration 34/1000 | Loss: 0.00001972
Iteration 35/1000 | Loss: 0.00001971
Iteration 36/1000 | Loss: 0.00001971
Iteration 37/1000 | Loss: 0.00001971
Iteration 38/1000 | Loss: 0.00001970
Iteration 39/1000 | Loss: 0.00001970
Iteration 40/1000 | Loss: 0.00001970
Iteration 41/1000 | Loss: 0.00001969
Iteration 42/1000 | Loss: 0.00001969
Iteration 43/1000 | Loss: 0.00001969
Iteration 44/1000 | Loss: 0.00001969
Iteration 45/1000 | Loss: 0.00001968
Iteration 46/1000 | Loss: 0.00001968
Iteration 47/1000 | Loss: 0.00001968
Iteration 48/1000 | Loss: 0.00001968
Iteration 49/1000 | Loss: 0.00001968
Iteration 50/1000 | Loss: 0.00001968
Iteration 51/1000 | Loss: 0.00001968
Iteration 52/1000 | Loss: 0.00001968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 52. Stopping optimization.
Last 5 losses: [1.967611569853034e-05, 1.967611569853034e-05, 1.967611569853034e-05, 1.967611569853034e-05, 1.967611569853034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.967611569853034e-05

Optimization complete. Final v2v error: 3.641721487045288 mm

Highest mean error: 4.108495235443115 mm for frame 26

Lowest mean error: 2.8992161750793457 mm for frame 173

Saving results

Total time: 34.93525981903076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446718
Iteration 2/25 | Loss: 0.00101021
Iteration 3/25 | Loss: 0.00068177
Iteration 4/25 | Loss: 0.00063801
Iteration 5/25 | Loss: 0.00063061
Iteration 6/25 | Loss: 0.00062848
Iteration 7/25 | Loss: 0.00062813
Iteration 8/25 | Loss: 0.00062813
Iteration 9/25 | Loss: 0.00062813
Iteration 10/25 | Loss: 0.00062813
Iteration 11/25 | Loss: 0.00062813
Iteration 12/25 | Loss: 0.00062813
Iteration 13/25 | Loss: 0.00062813
Iteration 14/25 | Loss: 0.00062813
Iteration 15/25 | Loss: 0.00062813
Iteration 16/25 | Loss: 0.00062813
Iteration 17/25 | Loss: 0.00062813
Iteration 18/25 | Loss: 0.00062813
Iteration 19/25 | Loss: 0.00062813
Iteration 20/25 | Loss: 0.00062813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006281291134655476, 0.0006281291134655476, 0.0006281291134655476, 0.0006281291134655476, 0.0006281291134655476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006281291134655476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.21518135
Iteration 2/25 | Loss: 0.00026119
Iteration 3/25 | Loss: 0.00026116
Iteration 4/25 | Loss: 0.00026116
Iteration 5/25 | Loss: 0.00026116
Iteration 6/25 | Loss: 0.00026116
Iteration 7/25 | Loss: 0.00026116
Iteration 8/25 | Loss: 0.00026116
Iteration 9/25 | Loss: 0.00026116
Iteration 10/25 | Loss: 0.00026116
Iteration 11/25 | Loss: 0.00026116
Iteration 12/25 | Loss: 0.00026116
Iteration 13/25 | Loss: 0.00026116
Iteration 14/25 | Loss: 0.00026116
Iteration 15/25 | Loss: 0.00026116
Iteration 16/25 | Loss: 0.00026116
Iteration 17/25 | Loss: 0.00026116
Iteration 18/25 | Loss: 0.00026116
Iteration 19/25 | Loss: 0.00026116
Iteration 20/25 | Loss: 0.00026116
Iteration 21/25 | Loss: 0.00026116
Iteration 22/25 | Loss: 0.00026116
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00026115647051483393, 0.00026115647051483393, 0.00026115647051483393, 0.00026115647051483393, 0.00026115647051483393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026115647051483393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026116
Iteration 2/1000 | Loss: 0.00002420
Iteration 3/1000 | Loss: 0.00002002
Iteration 4/1000 | Loss: 0.00001876
Iteration 5/1000 | Loss: 0.00001771
Iteration 6/1000 | Loss: 0.00001726
Iteration 7/1000 | Loss: 0.00001683
Iteration 8/1000 | Loss: 0.00001654
Iteration 9/1000 | Loss: 0.00001633
Iteration 10/1000 | Loss: 0.00001630
Iteration 11/1000 | Loss: 0.00001624
Iteration 12/1000 | Loss: 0.00001623
Iteration 13/1000 | Loss: 0.00001619
Iteration 14/1000 | Loss: 0.00001618
Iteration 15/1000 | Loss: 0.00001617
Iteration 16/1000 | Loss: 0.00001615
Iteration 17/1000 | Loss: 0.00001614
Iteration 18/1000 | Loss: 0.00001614
Iteration 19/1000 | Loss: 0.00001614
Iteration 20/1000 | Loss: 0.00001614
Iteration 21/1000 | Loss: 0.00001613
Iteration 22/1000 | Loss: 0.00001613
Iteration 23/1000 | Loss: 0.00001613
Iteration 24/1000 | Loss: 0.00001613
Iteration 25/1000 | Loss: 0.00001613
Iteration 26/1000 | Loss: 0.00001613
Iteration 27/1000 | Loss: 0.00001612
Iteration 28/1000 | Loss: 0.00001611
Iteration 29/1000 | Loss: 0.00001610
Iteration 30/1000 | Loss: 0.00001610
Iteration 31/1000 | Loss: 0.00001610
Iteration 32/1000 | Loss: 0.00001610
Iteration 33/1000 | Loss: 0.00001610
Iteration 34/1000 | Loss: 0.00001610
Iteration 35/1000 | Loss: 0.00001610
Iteration 36/1000 | Loss: 0.00001610
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001609
Iteration 39/1000 | Loss: 0.00001608
Iteration 40/1000 | Loss: 0.00001608
Iteration 41/1000 | Loss: 0.00001607
Iteration 42/1000 | Loss: 0.00001607
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001607
Iteration 45/1000 | Loss: 0.00001607
Iteration 46/1000 | Loss: 0.00001607
Iteration 47/1000 | Loss: 0.00001607
Iteration 48/1000 | Loss: 0.00001607
Iteration 49/1000 | Loss: 0.00001606
Iteration 50/1000 | Loss: 0.00001606
Iteration 51/1000 | Loss: 0.00001606
Iteration 52/1000 | Loss: 0.00001605
Iteration 53/1000 | Loss: 0.00001605
Iteration 54/1000 | Loss: 0.00001604
Iteration 55/1000 | Loss: 0.00001604
Iteration 56/1000 | Loss: 0.00001604
Iteration 57/1000 | Loss: 0.00001604
Iteration 58/1000 | Loss: 0.00001604
Iteration 59/1000 | Loss: 0.00001603
Iteration 60/1000 | Loss: 0.00001603
Iteration 61/1000 | Loss: 0.00001603
Iteration 62/1000 | Loss: 0.00001603
Iteration 63/1000 | Loss: 0.00001603
Iteration 64/1000 | Loss: 0.00001603
Iteration 65/1000 | Loss: 0.00001603
Iteration 66/1000 | Loss: 0.00001603
Iteration 67/1000 | Loss: 0.00001602
Iteration 68/1000 | Loss: 0.00001602
Iteration 69/1000 | Loss: 0.00001602
Iteration 70/1000 | Loss: 0.00001601
Iteration 71/1000 | Loss: 0.00001601
Iteration 72/1000 | Loss: 0.00001601
Iteration 73/1000 | Loss: 0.00001600
Iteration 74/1000 | Loss: 0.00001600
Iteration 75/1000 | Loss: 0.00001599
Iteration 76/1000 | Loss: 0.00001599
Iteration 77/1000 | Loss: 0.00001599
Iteration 78/1000 | Loss: 0.00001599
Iteration 79/1000 | Loss: 0.00001599
Iteration 80/1000 | Loss: 0.00001599
Iteration 81/1000 | Loss: 0.00001598
Iteration 82/1000 | Loss: 0.00001597
Iteration 83/1000 | Loss: 0.00001596
Iteration 84/1000 | Loss: 0.00001596
Iteration 85/1000 | Loss: 0.00001596
Iteration 86/1000 | Loss: 0.00001595
Iteration 87/1000 | Loss: 0.00001595
Iteration 88/1000 | Loss: 0.00001595
Iteration 89/1000 | Loss: 0.00001595
Iteration 90/1000 | Loss: 0.00001595
Iteration 91/1000 | Loss: 0.00001595
Iteration 92/1000 | Loss: 0.00001595
Iteration 93/1000 | Loss: 0.00001594
Iteration 94/1000 | Loss: 0.00001594
Iteration 95/1000 | Loss: 0.00001594
Iteration 96/1000 | Loss: 0.00001594
Iteration 97/1000 | Loss: 0.00001594
Iteration 98/1000 | Loss: 0.00001594
Iteration 99/1000 | Loss: 0.00001594
Iteration 100/1000 | Loss: 0.00001594
Iteration 101/1000 | Loss: 0.00001593
Iteration 102/1000 | Loss: 0.00001593
Iteration 103/1000 | Loss: 0.00001593
Iteration 104/1000 | Loss: 0.00001593
Iteration 105/1000 | Loss: 0.00001593
Iteration 106/1000 | Loss: 0.00001593
Iteration 107/1000 | Loss: 0.00001593
Iteration 108/1000 | Loss: 0.00001592
Iteration 109/1000 | Loss: 0.00001592
Iteration 110/1000 | Loss: 0.00001592
Iteration 111/1000 | Loss: 0.00001592
Iteration 112/1000 | Loss: 0.00001592
Iteration 113/1000 | Loss: 0.00001592
Iteration 114/1000 | Loss: 0.00001592
Iteration 115/1000 | Loss: 0.00001592
Iteration 116/1000 | Loss: 0.00001591
Iteration 117/1000 | Loss: 0.00001591
Iteration 118/1000 | Loss: 0.00001591
Iteration 119/1000 | Loss: 0.00001591
Iteration 120/1000 | Loss: 0.00001591
Iteration 121/1000 | Loss: 0.00001591
Iteration 122/1000 | Loss: 0.00001591
Iteration 123/1000 | Loss: 0.00001591
Iteration 124/1000 | Loss: 0.00001591
Iteration 125/1000 | Loss: 0.00001591
Iteration 126/1000 | Loss: 0.00001591
Iteration 127/1000 | Loss: 0.00001591
Iteration 128/1000 | Loss: 0.00001591
Iteration 129/1000 | Loss: 0.00001591
Iteration 130/1000 | Loss: 0.00001591
Iteration 131/1000 | Loss: 0.00001591
Iteration 132/1000 | Loss: 0.00001591
Iteration 133/1000 | Loss: 0.00001591
Iteration 134/1000 | Loss: 0.00001591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.5912673916318454e-05, 1.5912673916318454e-05, 1.5912673916318454e-05, 1.5912673916318454e-05, 1.5912673916318454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5912673916318454e-05

Optimization complete. Final v2v error: 3.403027057647705 mm

Highest mean error: 3.9177708625793457 mm for frame 62

Lowest mean error: 3.1580402851104736 mm for frame 0

Saving results

Total time: 32.65652346611023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871973
Iteration 2/25 | Loss: 0.00121957
Iteration 3/25 | Loss: 0.00075305
Iteration 4/25 | Loss: 0.00071396
Iteration 5/25 | Loss: 0.00064846
Iteration 6/25 | Loss: 0.00064352
Iteration 7/25 | Loss: 0.00065518
Iteration 8/25 | Loss: 0.00065027
Iteration 9/25 | Loss: 0.00063921
Iteration 10/25 | Loss: 0.00063094
Iteration 11/25 | Loss: 0.00062466
Iteration 12/25 | Loss: 0.00061715
Iteration 13/25 | Loss: 0.00061187
Iteration 14/25 | Loss: 0.00060885
Iteration 15/25 | Loss: 0.00060769
Iteration 16/25 | Loss: 0.00060696
Iteration 17/25 | Loss: 0.00060672
Iteration 18/25 | Loss: 0.00060662
Iteration 19/25 | Loss: 0.00060653
Iteration 20/25 | Loss: 0.00060653
Iteration 21/25 | Loss: 0.00060653
Iteration 22/25 | Loss: 0.00060652
Iteration 23/25 | Loss: 0.00060652
Iteration 24/25 | Loss: 0.00060652
Iteration 25/25 | Loss: 0.00060652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.99677658
Iteration 2/25 | Loss: 0.00024804
Iteration 3/25 | Loss: 0.00024803
Iteration 4/25 | Loss: 0.00024803
Iteration 5/25 | Loss: 0.00024803
Iteration 6/25 | Loss: 0.00024802
Iteration 7/25 | Loss: 0.00024802
Iteration 8/25 | Loss: 0.00024802
Iteration 9/25 | Loss: 0.00024802
Iteration 10/25 | Loss: 0.00024802
Iteration 11/25 | Loss: 0.00024802
Iteration 12/25 | Loss: 0.00024802
Iteration 13/25 | Loss: 0.00024802
Iteration 14/25 | Loss: 0.00024802
Iteration 15/25 | Loss: 0.00024802
Iteration 16/25 | Loss: 0.00024802
Iteration 17/25 | Loss: 0.00024802
Iteration 18/25 | Loss: 0.00024802
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00024802368716336787, 0.00024802368716336787, 0.00024802368716336787, 0.00024802368716336787, 0.00024802368716336787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024802368716336787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024802
Iteration 2/1000 | Loss: 0.00002877
Iteration 3/1000 | Loss: 0.00002366
Iteration 4/1000 | Loss: 0.00010966
Iteration 5/1000 | Loss: 0.00021531
Iteration 6/1000 | Loss: 0.00003183
Iteration 7/1000 | Loss: 0.00019561
Iteration 8/1000 | Loss: 0.00002591
Iteration 9/1000 | Loss: 0.00019190
Iteration 10/1000 | Loss: 0.00002129
Iteration 11/1000 | Loss: 0.00002569
Iteration 12/1000 | Loss: 0.00001972
Iteration 13/1000 | Loss: 0.00001859
Iteration 14/1000 | Loss: 0.00001826
Iteration 15/1000 | Loss: 0.00001817
Iteration 16/1000 | Loss: 0.00001810
Iteration 17/1000 | Loss: 0.00001806
Iteration 18/1000 | Loss: 0.00001797
Iteration 19/1000 | Loss: 0.00001795
Iteration 20/1000 | Loss: 0.00001794
Iteration 21/1000 | Loss: 0.00001792
Iteration 22/1000 | Loss: 0.00001792
Iteration 23/1000 | Loss: 0.00001792
Iteration 24/1000 | Loss: 0.00001792
Iteration 25/1000 | Loss: 0.00001792
Iteration 26/1000 | Loss: 0.00001791
Iteration 27/1000 | Loss: 0.00001791
Iteration 28/1000 | Loss: 0.00001791
Iteration 29/1000 | Loss: 0.00001791
Iteration 30/1000 | Loss: 0.00001791
Iteration 31/1000 | Loss: 0.00001791
Iteration 32/1000 | Loss: 0.00001791
Iteration 33/1000 | Loss: 0.00001791
Iteration 34/1000 | Loss: 0.00001791
Iteration 35/1000 | Loss: 0.00001791
Iteration 36/1000 | Loss: 0.00001791
Iteration 37/1000 | Loss: 0.00001791
Iteration 38/1000 | Loss: 0.00001790
Iteration 39/1000 | Loss: 0.00001788
Iteration 40/1000 | Loss: 0.00001786
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001780
Iteration 44/1000 | Loss: 0.00001780
Iteration 45/1000 | Loss: 0.00001775
Iteration 46/1000 | Loss: 0.00008334
Iteration 47/1000 | Loss: 0.00008334
Iteration 48/1000 | Loss: 0.00032869
Iteration 49/1000 | Loss: 0.00023433
Iteration 50/1000 | Loss: 0.00014159
Iteration 51/1000 | Loss: 0.00014659
Iteration 52/1000 | Loss: 0.00019918
Iteration 53/1000 | Loss: 0.00014678
Iteration 54/1000 | Loss: 0.00016401
Iteration 55/1000 | Loss: 0.00013182
Iteration 56/1000 | Loss: 0.00001849
Iteration 57/1000 | Loss: 0.00001787
Iteration 58/1000 | Loss: 0.00001772
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001771
Iteration 62/1000 | Loss: 0.00001768
Iteration 63/1000 | Loss: 0.00001768
Iteration 64/1000 | Loss: 0.00001767
Iteration 65/1000 | Loss: 0.00001767
Iteration 66/1000 | Loss: 0.00001766
Iteration 67/1000 | Loss: 0.00001765
Iteration 68/1000 | Loss: 0.00001763
Iteration 69/1000 | Loss: 0.00001763
Iteration 70/1000 | Loss: 0.00001762
Iteration 71/1000 | Loss: 0.00001762
Iteration 72/1000 | Loss: 0.00001762
Iteration 73/1000 | Loss: 0.00001762
Iteration 74/1000 | Loss: 0.00001761
Iteration 75/1000 | Loss: 0.00001759
Iteration 76/1000 | Loss: 0.00001758
Iteration 77/1000 | Loss: 0.00001758
Iteration 78/1000 | Loss: 0.00001758
Iteration 79/1000 | Loss: 0.00001758
Iteration 80/1000 | Loss: 0.00001757
Iteration 81/1000 | Loss: 0.00001757
Iteration 82/1000 | Loss: 0.00001757
Iteration 83/1000 | Loss: 0.00001757
Iteration 84/1000 | Loss: 0.00001757
Iteration 85/1000 | Loss: 0.00001756
Iteration 86/1000 | Loss: 0.00001756
Iteration 87/1000 | Loss: 0.00001756
Iteration 88/1000 | Loss: 0.00001756
Iteration 89/1000 | Loss: 0.00001755
Iteration 90/1000 | Loss: 0.00001755
Iteration 91/1000 | Loss: 0.00001755
Iteration 92/1000 | Loss: 0.00001754
Iteration 93/1000 | Loss: 0.00001754
Iteration 94/1000 | Loss: 0.00001754
Iteration 95/1000 | Loss: 0.00001753
Iteration 96/1000 | Loss: 0.00001753
Iteration 97/1000 | Loss: 0.00001752
Iteration 98/1000 | Loss: 0.00001751
Iteration 99/1000 | Loss: 0.00001751
Iteration 100/1000 | Loss: 0.00001751
Iteration 101/1000 | Loss: 0.00001750
Iteration 102/1000 | Loss: 0.00001750
Iteration 103/1000 | Loss: 0.00001749
Iteration 104/1000 | Loss: 0.00001749
Iteration 105/1000 | Loss: 0.00001749
Iteration 106/1000 | Loss: 0.00001748
Iteration 107/1000 | Loss: 0.00014580
Iteration 108/1000 | Loss: 0.00010210
Iteration 109/1000 | Loss: 0.00001758
Iteration 110/1000 | Loss: 0.00013213
Iteration 111/1000 | Loss: 0.00007104
Iteration 112/1000 | Loss: 0.00001828
Iteration 113/1000 | Loss: 0.00001771
Iteration 114/1000 | Loss: 0.00001766
Iteration 115/1000 | Loss: 0.00001763
Iteration 116/1000 | Loss: 0.00001763
Iteration 117/1000 | Loss: 0.00001763
Iteration 118/1000 | Loss: 0.00001762
Iteration 119/1000 | Loss: 0.00001761
Iteration 120/1000 | Loss: 0.00001756
Iteration 121/1000 | Loss: 0.00001756
Iteration 122/1000 | Loss: 0.00001755
Iteration 123/1000 | Loss: 0.00001754
Iteration 124/1000 | Loss: 0.00001754
Iteration 125/1000 | Loss: 0.00001753
Iteration 126/1000 | Loss: 0.00001753
Iteration 127/1000 | Loss: 0.00001753
Iteration 128/1000 | Loss: 0.00001752
Iteration 129/1000 | Loss: 0.00001752
Iteration 130/1000 | Loss: 0.00001752
Iteration 131/1000 | Loss: 0.00001751
Iteration 132/1000 | Loss: 0.00001751
Iteration 133/1000 | Loss: 0.00001751
Iteration 134/1000 | Loss: 0.00001750
Iteration 135/1000 | Loss: 0.00001750
Iteration 136/1000 | Loss: 0.00001750
Iteration 137/1000 | Loss: 0.00001750
Iteration 138/1000 | Loss: 0.00001749
Iteration 139/1000 | Loss: 0.00001749
Iteration 140/1000 | Loss: 0.00001747
Iteration 141/1000 | Loss: 0.00001747
Iteration 142/1000 | Loss: 0.00001747
Iteration 143/1000 | Loss: 0.00001747
Iteration 144/1000 | Loss: 0.00001747
Iteration 145/1000 | Loss: 0.00001746
Iteration 146/1000 | Loss: 0.00001746
Iteration 147/1000 | Loss: 0.00001746
Iteration 148/1000 | Loss: 0.00001746
Iteration 149/1000 | Loss: 0.00001746
Iteration 150/1000 | Loss: 0.00001746
Iteration 151/1000 | Loss: 0.00001746
Iteration 152/1000 | Loss: 0.00001745
Iteration 153/1000 | Loss: 0.00001745
Iteration 154/1000 | Loss: 0.00001745
Iteration 155/1000 | Loss: 0.00001745
Iteration 156/1000 | Loss: 0.00001744
Iteration 157/1000 | Loss: 0.00001744
Iteration 158/1000 | Loss: 0.00001743
Iteration 159/1000 | Loss: 0.00001743
Iteration 160/1000 | Loss: 0.00001743
Iteration 161/1000 | Loss: 0.00015153
Iteration 162/1000 | Loss: 0.00018254
Iteration 163/1000 | Loss: 0.00014283
Iteration 164/1000 | Loss: 0.00016518
Iteration 165/1000 | Loss: 0.00017639
Iteration 166/1000 | Loss: 0.00011087
Iteration 167/1000 | Loss: 0.00012666
Iteration 168/1000 | Loss: 0.00002261
Iteration 169/1000 | Loss: 0.00003843
Iteration 170/1000 | Loss: 0.00002013
Iteration 171/1000 | Loss: 0.00001825
Iteration 172/1000 | Loss: 0.00001767
Iteration 173/1000 | Loss: 0.00001754
Iteration 174/1000 | Loss: 0.00001751
Iteration 175/1000 | Loss: 0.00001751
Iteration 176/1000 | Loss: 0.00001750
Iteration 177/1000 | Loss: 0.00001749
Iteration 178/1000 | Loss: 0.00028065
Iteration 179/1000 | Loss: 0.00001991
Iteration 180/1000 | Loss: 0.00001780
Iteration 181/1000 | Loss: 0.00001692
Iteration 182/1000 | Loss: 0.00001618
Iteration 183/1000 | Loss: 0.00001579
Iteration 184/1000 | Loss: 0.00001569
Iteration 185/1000 | Loss: 0.00001569
Iteration 186/1000 | Loss: 0.00001564
Iteration 187/1000 | Loss: 0.00001561
Iteration 188/1000 | Loss: 0.00001561
Iteration 189/1000 | Loss: 0.00001561
Iteration 190/1000 | Loss: 0.00001557
Iteration 191/1000 | Loss: 0.00001556
Iteration 192/1000 | Loss: 0.00001556
Iteration 193/1000 | Loss: 0.00001556
Iteration 194/1000 | Loss: 0.00001556
Iteration 195/1000 | Loss: 0.00001556
Iteration 196/1000 | Loss: 0.00001556
Iteration 197/1000 | Loss: 0.00001555
Iteration 198/1000 | Loss: 0.00001555
Iteration 199/1000 | Loss: 0.00001554
Iteration 200/1000 | Loss: 0.00001554
Iteration 201/1000 | Loss: 0.00001554
Iteration 202/1000 | Loss: 0.00001554
Iteration 203/1000 | Loss: 0.00001554
Iteration 204/1000 | Loss: 0.00001554
Iteration 205/1000 | Loss: 0.00001554
Iteration 206/1000 | Loss: 0.00001554
Iteration 207/1000 | Loss: 0.00001554
Iteration 208/1000 | Loss: 0.00001554
Iteration 209/1000 | Loss: 0.00001554
Iteration 210/1000 | Loss: 0.00001554
Iteration 211/1000 | Loss: 0.00001553
Iteration 212/1000 | Loss: 0.00001553
Iteration 213/1000 | Loss: 0.00001553
Iteration 214/1000 | Loss: 0.00001553
Iteration 215/1000 | Loss: 0.00001552
Iteration 216/1000 | Loss: 0.00001552
Iteration 217/1000 | Loss: 0.00001552
Iteration 218/1000 | Loss: 0.00001551
Iteration 219/1000 | Loss: 0.00001551
Iteration 220/1000 | Loss: 0.00001551
Iteration 221/1000 | Loss: 0.00001551
Iteration 222/1000 | Loss: 0.00001551
Iteration 223/1000 | Loss: 0.00001551
Iteration 224/1000 | Loss: 0.00001551
Iteration 225/1000 | Loss: 0.00001551
Iteration 226/1000 | Loss: 0.00001551
Iteration 227/1000 | Loss: 0.00001551
Iteration 228/1000 | Loss: 0.00001551
Iteration 229/1000 | Loss: 0.00001551
Iteration 230/1000 | Loss: 0.00001550
Iteration 231/1000 | Loss: 0.00001550
Iteration 232/1000 | Loss: 0.00001550
Iteration 233/1000 | Loss: 0.00001550
Iteration 234/1000 | Loss: 0.00001549
Iteration 235/1000 | Loss: 0.00001549
Iteration 236/1000 | Loss: 0.00001549
Iteration 237/1000 | Loss: 0.00001549
Iteration 238/1000 | Loss: 0.00001549
Iteration 239/1000 | Loss: 0.00001549
Iteration 240/1000 | Loss: 0.00001549
Iteration 241/1000 | Loss: 0.00001549
Iteration 242/1000 | Loss: 0.00001549
Iteration 243/1000 | Loss: 0.00001548
Iteration 244/1000 | Loss: 0.00001548
Iteration 245/1000 | Loss: 0.00001548
Iteration 246/1000 | Loss: 0.00001548
Iteration 247/1000 | Loss: 0.00001548
Iteration 248/1000 | Loss: 0.00001548
Iteration 249/1000 | Loss: 0.00001548
Iteration 250/1000 | Loss: 0.00001547
Iteration 251/1000 | Loss: 0.00001547
Iteration 252/1000 | Loss: 0.00001547
Iteration 253/1000 | Loss: 0.00001547
Iteration 254/1000 | Loss: 0.00001547
Iteration 255/1000 | Loss: 0.00001547
Iteration 256/1000 | Loss: 0.00001547
Iteration 257/1000 | Loss: 0.00001547
Iteration 258/1000 | Loss: 0.00001547
Iteration 259/1000 | Loss: 0.00001547
Iteration 260/1000 | Loss: 0.00001547
Iteration 261/1000 | Loss: 0.00001547
Iteration 262/1000 | Loss: 0.00001547
Iteration 263/1000 | Loss: 0.00001547
Iteration 264/1000 | Loss: 0.00001547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.547287502035033e-05, 1.547287502035033e-05, 1.547287502035033e-05, 1.547287502035033e-05, 1.547287502035033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.547287502035033e-05

Optimization complete. Final v2v error: 3.332268476486206 mm

Highest mean error: 5.31343412399292 mm for frame 138

Lowest mean error: 2.980506181716919 mm for frame 32

Saving results

Total time: 145.00770115852356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386771
Iteration 2/25 | Loss: 0.00082641
Iteration 3/25 | Loss: 0.00062929
Iteration 4/25 | Loss: 0.00059419
Iteration 5/25 | Loss: 0.00058664
Iteration 6/25 | Loss: 0.00058441
Iteration 7/25 | Loss: 0.00058367
Iteration 8/25 | Loss: 0.00058356
Iteration 9/25 | Loss: 0.00058356
Iteration 10/25 | Loss: 0.00058356
Iteration 11/25 | Loss: 0.00058356
Iteration 12/25 | Loss: 0.00058356
Iteration 13/25 | Loss: 0.00058356
Iteration 14/25 | Loss: 0.00058356
Iteration 15/25 | Loss: 0.00058356
Iteration 16/25 | Loss: 0.00058356
Iteration 17/25 | Loss: 0.00058356
Iteration 18/25 | Loss: 0.00058356
Iteration 19/25 | Loss: 0.00058356
Iteration 20/25 | Loss: 0.00058356
Iteration 21/25 | Loss: 0.00058356
Iteration 22/25 | Loss: 0.00058356
Iteration 23/25 | Loss: 0.00058356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005835614283569157, 0.0005835614283569157, 0.0005835614283569157, 0.0005835614283569157, 0.0005835614283569157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005835614283569157

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47648811
Iteration 2/25 | Loss: 0.00027751
Iteration 3/25 | Loss: 0.00027751
Iteration 4/25 | Loss: 0.00027751
Iteration 5/25 | Loss: 0.00027751
Iteration 6/25 | Loss: 0.00027751
Iteration 7/25 | Loss: 0.00027751
Iteration 8/25 | Loss: 0.00027751
Iteration 9/25 | Loss: 0.00027751
Iteration 10/25 | Loss: 0.00027751
Iteration 11/25 | Loss: 0.00027751
Iteration 12/25 | Loss: 0.00027751
Iteration 13/25 | Loss: 0.00027751
Iteration 14/25 | Loss: 0.00027751
Iteration 15/25 | Loss: 0.00027751
Iteration 16/25 | Loss: 0.00027751
Iteration 17/25 | Loss: 0.00027751
Iteration 18/25 | Loss: 0.00027751
Iteration 19/25 | Loss: 0.00027751
Iteration 20/25 | Loss: 0.00027751
Iteration 21/25 | Loss: 0.00027751
Iteration 22/25 | Loss: 0.00027751
Iteration 23/25 | Loss: 0.00027751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002775062748696655, 0.0002775062748696655, 0.0002775062748696655, 0.0002775062748696655, 0.0002775062748696655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002775062748696655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027751
Iteration 2/1000 | Loss: 0.00002644
Iteration 3/1000 | Loss: 0.00001797
Iteration 4/1000 | Loss: 0.00001583
Iteration 5/1000 | Loss: 0.00001507
Iteration 6/1000 | Loss: 0.00001476
Iteration 7/1000 | Loss: 0.00001451
Iteration 8/1000 | Loss: 0.00001427
Iteration 9/1000 | Loss: 0.00001422
Iteration 10/1000 | Loss: 0.00001414
Iteration 11/1000 | Loss: 0.00001397
Iteration 12/1000 | Loss: 0.00001384
Iteration 13/1000 | Loss: 0.00001375
Iteration 14/1000 | Loss: 0.00001372
Iteration 15/1000 | Loss: 0.00001371
Iteration 16/1000 | Loss: 0.00001368
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001364
Iteration 19/1000 | Loss: 0.00001364
Iteration 20/1000 | Loss: 0.00001363
Iteration 21/1000 | Loss: 0.00001361
Iteration 22/1000 | Loss: 0.00001361
Iteration 23/1000 | Loss: 0.00001361
Iteration 24/1000 | Loss: 0.00001360
Iteration 25/1000 | Loss: 0.00001360
Iteration 26/1000 | Loss: 0.00001359
Iteration 27/1000 | Loss: 0.00001359
Iteration 28/1000 | Loss: 0.00001358
Iteration 29/1000 | Loss: 0.00001358
Iteration 30/1000 | Loss: 0.00001356
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001355
Iteration 33/1000 | Loss: 0.00001354
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001351
Iteration 41/1000 | Loss: 0.00001351
Iteration 42/1000 | Loss: 0.00001351
Iteration 43/1000 | Loss: 0.00001351
Iteration 44/1000 | Loss: 0.00001351
Iteration 45/1000 | Loss: 0.00001351
Iteration 46/1000 | Loss: 0.00001350
Iteration 47/1000 | Loss: 0.00001349
Iteration 48/1000 | Loss: 0.00001349
Iteration 49/1000 | Loss: 0.00001349
Iteration 50/1000 | Loss: 0.00001349
Iteration 51/1000 | Loss: 0.00001349
Iteration 52/1000 | Loss: 0.00001349
Iteration 53/1000 | Loss: 0.00001349
Iteration 54/1000 | Loss: 0.00001348
Iteration 55/1000 | Loss: 0.00001348
Iteration 56/1000 | Loss: 0.00001348
Iteration 57/1000 | Loss: 0.00001348
Iteration 58/1000 | Loss: 0.00001348
Iteration 59/1000 | Loss: 0.00001348
Iteration 60/1000 | Loss: 0.00001348
Iteration 61/1000 | Loss: 0.00001348
Iteration 62/1000 | Loss: 0.00001348
Iteration 63/1000 | Loss: 0.00001347
Iteration 64/1000 | Loss: 0.00001347
Iteration 65/1000 | Loss: 0.00001347
Iteration 66/1000 | Loss: 0.00001347
Iteration 67/1000 | Loss: 0.00001347
Iteration 68/1000 | Loss: 0.00001347
Iteration 69/1000 | Loss: 0.00001347
Iteration 70/1000 | Loss: 0.00001346
Iteration 71/1000 | Loss: 0.00001346
Iteration 72/1000 | Loss: 0.00001346
Iteration 73/1000 | Loss: 0.00001346
Iteration 74/1000 | Loss: 0.00001346
Iteration 75/1000 | Loss: 0.00001345
Iteration 76/1000 | Loss: 0.00001345
Iteration 77/1000 | Loss: 0.00001345
Iteration 78/1000 | Loss: 0.00001345
Iteration 79/1000 | Loss: 0.00001345
Iteration 80/1000 | Loss: 0.00001345
Iteration 81/1000 | Loss: 0.00001345
Iteration 82/1000 | Loss: 0.00001345
Iteration 83/1000 | Loss: 0.00001344
Iteration 84/1000 | Loss: 0.00001344
Iteration 85/1000 | Loss: 0.00001344
Iteration 86/1000 | Loss: 0.00001344
Iteration 87/1000 | Loss: 0.00001344
Iteration 88/1000 | Loss: 0.00001344
Iteration 89/1000 | Loss: 0.00001344
Iteration 90/1000 | Loss: 0.00001344
Iteration 91/1000 | Loss: 0.00001344
Iteration 92/1000 | Loss: 0.00001343
Iteration 93/1000 | Loss: 0.00001343
Iteration 94/1000 | Loss: 0.00001343
Iteration 95/1000 | Loss: 0.00001343
Iteration 96/1000 | Loss: 0.00001343
Iteration 97/1000 | Loss: 0.00001343
Iteration 98/1000 | Loss: 0.00001343
Iteration 99/1000 | Loss: 0.00001343
Iteration 100/1000 | Loss: 0.00001343
Iteration 101/1000 | Loss: 0.00001343
Iteration 102/1000 | Loss: 0.00001343
Iteration 103/1000 | Loss: 0.00001343
Iteration 104/1000 | Loss: 0.00001343
Iteration 105/1000 | Loss: 0.00001343
Iteration 106/1000 | Loss: 0.00001343
Iteration 107/1000 | Loss: 0.00001343
Iteration 108/1000 | Loss: 0.00001343
Iteration 109/1000 | Loss: 0.00001343
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001342
Iteration 112/1000 | Loss: 0.00001342
Iteration 113/1000 | Loss: 0.00001342
Iteration 114/1000 | Loss: 0.00001342
Iteration 115/1000 | Loss: 0.00001342
Iteration 116/1000 | Loss: 0.00001342
Iteration 117/1000 | Loss: 0.00001342
Iteration 118/1000 | Loss: 0.00001342
Iteration 119/1000 | Loss: 0.00001341
Iteration 120/1000 | Loss: 0.00001341
Iteration 121/1000 | Loss: 0.00001341
Iteration 122/1000 | Loss: 0.00001341
Iteration 123/1000 | Loss: 0.00001341
Iteration 124/1000 | Loss: 0.00001340
Iteration 125/1000 | Loss: 0.00001340
Iteration 126/1000 | Loss: 0.00001340
Iteration 127/1000 | Loss: 0.00001340
Iteration 128/1000 | Loss: 0.00001340
Iteration 129/1000 | Loss: 0.00001340
Iteration 130/1000 | Loss: 0.00001340
Iteration 131/1000 | Loss: 0.00001340
Iteration 132/1000 | Loss: 0.00001340
Iteration 133/1000 | Loss: 0.00001340
Iteration 134/1000 | Loss: 0.00001339
Iteration 135/1000 | Loss: 0.00001339
Iteration 136/1000 | Loss: 0.00001339
Iteration 137/1000 | Loss: 0.00001339
Iteration 138/1000 | Loss: 0.00001339
Iteration 139/1000 | Loss: 0.00001339
Iteration 140/1000 | Loss: 0.00001339
Iteration 141/1000 | Loss: 0.00001339
Iteration 142/1000 | Loss: 0.00001339
Iteration 143/1000 | Loss: 0.00001339
Iteration 144/1000 | Loss: 0.00001339
Iteration 145/1000 | Loss: 0.00001339
Iteration 146/1000 | Loss: 0.00001339
Iteration 147/1000 | Loss: 0.00001339
Iteration 148/1000 | Loss: 0.00001339
Iteration 149/1000 | Loss: 0.00001339
Iteration 150/1000 | Loss: 0.00001339
Iteration 151/1000 | Loss: 0.00001339
Iteration 152/1000 | Loss: 0.00001339
Iteration 153/1000 | Loss: 0.00001339
Iteration 154/1000 | Loss: 0.00001339
Iteration 155/1000 | Loss: 0.00001338
Iteration 156/1000 | Loss: 0.00001338
Iteration 157/1000 | Loss: 0.00001338
Iteration 158/1000 | Loss: 0.00001338
Iteration 159/1000 | Loss: 0.00001338
Iteration 160/1000 | Loss: 0.00001338
Iteration 161/1000 | Loss: 0.00001338
Iteration 162/1000 | Loss: 0.00001338
Iteration 163/1000 | Loss: 0.00001338
Iteration 164/1000 | Loss: 0.00001338
Iteration 165/1000 | Loss: 0.00001338
Iteration 166/1000 | Loss: 0.00001338
Iteration 167/1000 | Loss: 0.00001338
Iteration 168/1000 | Loss: 0.00001338
Iteration 169/1000 | Loss: 0.00001338
Iteration 170/1000 | Loss: 0.00001338
Iteration 171/1000 | Loss: 0.00001338
Iteration 172/1000 | Loss: 0.00001338
Iteration 173/1000 | Loss: 0.00001338
Iteration 174/1000 | Loss: 0.00001338
Iteration 175/1000 | Loss: 0.00001338
Iteration 176/1000 | Loss: 0.00001338
Iteration 177/1000 | Loss: 0.00001337
Iteration 178/1000 | Loss: 0.00001337
Iteration 179/1000 | Loss: 0.00001337
Iteration 180/1000 | Loss: 0.00001337
Iteration 181/1000 | Loss: 0.00001337
Iteration 182/1000 | Loss: 0.00001337
Iteration 183/1000 | Loss: 0.00001337
Iteration 184/1000 | Loss: 0.00001337
Iteration 185/1000 | Loss: 0.00001337
Iteration 186/1000 | Loss: 0.00001337
Iteration 187/1000 | Loss: 0.00001337
Iteration 188/1000 | Loss: 0.00001337
Iteration 189/1000 | Loss: 0.00001337
Iteration 190/1000 | Loss: 0.00001337
Iteration 191/1000 | Loss: 0.00001337
Iteration 192/1000 | Loss: 0.00001337
Iteration 193/1000 | Loss: 0.00001336
Iteration 194/1000 | Loss: 0.00001336
Iteration 195/1000 | Loss: 0.00001336
Iteration 196/1000 | Loss: 0.00001336
Iteration 197/1000 | Loss: 0.00001336
Iteration 198/1000 | Loss: 0.00001336
Iteration 199/1000 | Loss: 0.00001336
Iteration 200/1000 | Loss: 0.00001336
Iteration 201/1000 | Loss: 0.00001336
Iteration 202/1000 | Loss: 0.00001336
Iteration 203/1000 | Loss: 0.00001336
Iteration 204/1000 | Loss: 0.00001336
Iteration 205/1000 | Loss: 0.00001336
Iteration 206/1000 | Loss: 0.00001336
Iteration 207/1000 | Loss: 0.00001336
Iteration 208/1000 | Loss: 0.00001336
Iteration 209/1000 | Loss: 0.00001336
Iteration 210/1000 | Loss: 0.00001336
Iteration 211/1000 | Loss: 0.00001336
Iteration 212/1000 | Loss: 0.00001336
Iteration 213/1000 | Loss: 0.00001336
Iteration 214/1000 | Loss: 0.00001336
Iteration 215/1000 | Loss: 0.00001336
Iteration 216/1000 | Loss: 0.00001336
Iteration 217/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.3358705473365262e-05, 1.3358705473365262e-05, 1.3358705473365262e-05, 1.3358705473365262e-05, 1.3358705473365262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3358705473365262e-05

Optimization complete. Final v2v error: 3.0076193809509277 mm

Highest mean error: 3.574368715286255 mm for frame 21

Lowest mean error: 2.4653801918029785 mm for frame 36

Saving results

Total time: 39.94930052757263
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840252
Iteration 2/25 | Loss: 0.00070976
Iteration 3/25 | Loss: 0.00059829
Iteration 4/25 | Loss: 0.00058298
Iteration 5/25 | Loss: 0.00057940
Iteration 6/25 | Loss: 0.00057862
Iteration 7/25 | Loss: 0.00057853
Iteration 8/25 | Loss: 0.00057853
Iteration 9/25 | Loss: 0.00057853
Iteration 10/25 | Loss: 0.00057853
Iteration 11/25 | Loss: 0.00057853
Iteration 12/25 | Loss: 0.00057853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005785319954156876, 0.0005785319954156876, 0.0005785319954156876, 0.0005785319954156876, 0.0005785319954156876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005785319954156876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.19389343
Iteration 2/25 | Loss: 0.00025850
Iteration 3/25 | Loss: 0.00025848
Iteration 4/25 | Loss: 0.00025848
Iteration 5/25 | Loss: 0.00025848
Iteration 6/25 | Loss: 0.00025848
Iteration 7/25 | Loss: 0.00025848
Iteration 8/25 | Loss: 0.00025848
Iteration 9/25 | Loss: 0.00025848
Iteration 10/25 | Loss: 0.00025848
Iteration 11/25 | Loss: 0.00025848
Iteration 12/25 | Loss: 0.00025848
Iteration 13/25 | Loss: 0.00025848
Iteration 14/25 | Loss: 0.00025848
Iteration 15/25 | Loss: 0.00025848
Iteration 16/25 | Loss: 0.00025848
Iteration 17/25 | Loss: 0.00025848
Iteration 18/25 | Loss: 0.00025848
Iteration 19/25 | Loss: 0.00025848
Iteration 20/25 | Loss: 0.00025848
Iteration 21/25 | Loss: 0.00025848
Iteration 22/25 | Loss: 0.00025848
Iteration 23/25 | Loss: 0.00025848
Iteration 24/25 | Loss: 0.00025848
Iteration 25/25 | Loss: 0.00025848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025848
Iteration 2/1000 | Loss: 0.00002017
Iteration 3/1000 | Loss: 0.00001567
Iteration 4/1000 | Loss: 0.00001475
Iteration 5/1000 | Loss: 0.00001391
Iteration 6/1000 | Loss: 0.00001351
Iteration 7/1000 | Loss: 0.00001324
Iteration 8/1000 | Loss: 0.00001309
Iteration 9/1000 | Loss: 0.00001298
Iteration 10/1000 | Loss: 0.00001298
Iteration 11/1000 | Loss: 0.00001296
Iteration 12/1000 | Loss: 0.00001295
Iteration 13/1000 | Loss: 0.00001292
Iteration 14/1000 | Loss: 0.00001292
Iteration 15/1000 | Loss: 0.00001291
Iteration 16/1000 | Loss: 0.00001290
Iteration 17/1000 | Loss: 0.00001289
Iteration 18/1000 | Loss: 0.00001289
Iteration 19/1000 | Loss: 0.00001288
Iteration 20/1000 | Loss: 0.00001285
Iteration 21/1000 | Loss: 0.00001285
Iteration 22/1000 | Loss: 0.00001285
Iteration 23/1000 | Loss: 0.00001284
Iteration 24/1000 | Loss: 0.00001284
Iteration 25/1000 | Loss: 0.00001284
Iteration 26/1000 | Loss: 0.00001283
Iteration 27/1000 | Loss: 0.00001283
Iteration 28/1000 | Loss: 0.00001282
Iteration 29/1000 | Loss: 0.00001282
Iteration 30/1000 | Loss: 0.00001282
Iteration 31/1000 | Loss: 0.00001281
Iteration 32/1000 | Loss: 0.00001281
Iteration 33/1000 | Loss: 0.00001281
Iteration 34/1000 | Loss: 0.00001281
Iteration 35/1000 | Loss: 0.00001281
Iteration 36/1000 | Loss: 0.00001281
Iteration 37/1000 | Loss: 0.00001281
Iteration 38/1000 | Loss: 0.00001280
Iteration 39/1000 | Loss: 0.00001280
Iteration 40/1000 | Loss: 0.00001280
Iteration 41/1000 | Loss: 0.00001279
Iteration 42/1000 | Loss: 0.00001279
Iteration 43/1000 | Loss: 0.00001278
Iteration 44/1000 | Loss: 0.00001278
Iteration 45/1000 | Loss: 0.00001278
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001278
Iteration 48/1000 | Loss: 0.00001278
Iteration 49/1000 | Loss: 0.00001277
Iteration 50/1000 | Loss: 0.00001277
Iteration 51/1000 | Loss: 0.00001276
Iteration 52/1000 | Loss: 0.00001276
Iteration 53/1000 | Loss: 0.00001275
Iteration 54/1000 | Loss: 0.00001275
Iteration 55/1000 | Loss: 0.00001275
Iteration 56/1000 | Loss: 0.00001274
Iteration 57/1000 | Loss: 0.00001274
Iteration 58/1000 | Loss: 0.00001274
Iteration 59/1000 | Loss: 0.00001274
Iteration 60/1000 | Loss: 0.00001274
Iteration 61/1000 | Loss: 0.00001274
Iteration 62/1000 | Loss: 0.00001274
Iteration 63/1000 | Loss: 0.00001274
Iteration 64/1000 | Loss: 0.00001273
Iteration 65/1000 | Loss: 0.00001273
Iteration 66/1000 | Loss: 0.00001273
Iteration 67/1000 | Loss: 0.00001273
Iteration 68/1000 | Loss: 0.00001273
Iteration 69/1000 | Loss: 0.00001273
Iteration 70/1000 | Loss: 0.00001272
Iteration 71/1000 | Loss: 0.00001272
Iteration 72/1000 | Loss: 0.00001272
Iteration 73/1000 | Loss: 0.00001272
Iteration 74/1000 | Loss: 0.00001272
Iteration 75/1000 | Loss: 0.00001272
Iteration 76/1000 | Loss: 0.00001271
Iteration 77/1000 | Loss: 0.00001271
Iteration 78/1000 | Loss: 0.00001271
Iteration 79/1000 | Loss: 0.00001271
Iteration 80/1000 | Loss: 0.00001270
Iteration 81/1000 | Loss: 0.00001270
Iteration 82/1000 | Loss: 0.00001270
Iteration 83/1000 | Loss: 0.00001270
Iteration 84/1000 | Loss: 0.00001270
Iteration 85/1000 | Loss: 0.00001270
Iteration 86/1000 | Loss: 0.00001270
Iteration 87/1000 | Loss: 0.00001270
Iteration 88/1000 | Loss: 0.00001269
Iteration 89/1000 | Loss: 0.00001269
Iteration 90/1000 | Loss: 0.00001269
Iteration 91/1000 | Loss: 0.00001269
Iteration 92/1000 | Loss: 0.00001269
Iteration 93/1000 | Loss: 0.00001269
Iteration 94/1000 | Loss: 0.00001269
Iteration 95/1000 | Loss: 0.00001269
Iteration 96/1000 | Loss: 0.00001269
Iteration 97/1000 | Loss: 0.00001269
Iteration 98/1000 | Loss: 0.00001269
Iteration 99/1000 | Loss: 0.00001269
Iteration 100/1000 | Loss: 0.00001268
Iteration 101/1000 | Loss: 0.00001268
Iteration 102/1000 | Loss: 0.00001268
Iteration 103/1000 | Loss: 0.00001268
Iteration 104/1000 | Loss: 0.00001268
Iteration 105/1000 | Loss: 0.00001267
Iteration 106/1000 | Loss: 0.00001267
Iteration 107/1000 | Loss: 0.00001267
Iteration 108/1000 | Loss: 0.00001267
Iteration 109/1000 | Loss: 0.00001267
Iteration 110/1000 | Loss: 0.00001266
Iteration 111/1000 | Loss: 0.00001266
Iteration 112/1000 | Loss: 0.00001266
Iteration 113/1000 | Loss: 0.00001266
Iteration 114/1000 | Loss: 0.00001266
Iteration 115/1000 | Loss: 0.00001266
Iteration 116/1000 | Loss: 0.00001266
Iteration 117/1000 | Loss: 0.00001266
Iteration 118/1000 | Loss: 0.00001266
Iteration 119/1000 | Loss: 0.00001266
Iteration 120/1000 | Loss: 0.00001266
Iteration 121/1000 | Loss: 0.00001265
Iteration 122/1000 | Loss: 0.00001265
Iteration 123/1000 | Loss: 0.00001265
Iteration 124/1000 | Loss: 0.00001265
Iteration 125/1000 | Loss: 0.00001265
Iteration 126/1000 | Loss: 0.00001265
Iteration 127/1000 | Loss: 0.00001265
Iteration 128/1000 | Loss: 0.00001265
Iteration 129/1000 | Loss: 0.00001265
Iteration 130/1000 | Loss: 0.00001265
Iteration 131/1000 | Loss: 0.00001265
Iteration 132/1000 | Loss: 0.00001264
Iteration 133/1000 | Loss: 0.00001264
Iteration 134/1000 | Loss: 0.00001264
Iteration 135/1000 | Loss: 0.00001264
Iteration 136/1000 | Loss: 0.00001264
Iteration 137/1000 | Loss: 0.00001264
Iteration 138/1000 | Loss: 0.00001264
Iteration 139/1000 | Loss: 0.00001264
Iteration 140/1000 | Loss: 0.00001264
Iteration 141/1000 | Loss: 0.00001264
Iteration 142/1000 | Loss: 0.00001263
Iteration 143/1000 | Loss: 0.00001263
Iteration 144/1000 | Loss: 0.00001263
Iteration 145/1000 | Loss: 0.00001263
Iteration 146/1000 | Loss: 0.00001263
Iteration 147/1000 | Loss: 0.00001263
Iteration 148/1000 | Loss: 0.00001263
Iteration 149/1000 | Loss: 0.00001263
Iteration 150/1000 | Loss: 0.00001263
Iteration 151/1000 | Loss: 0.00001262
Iteration 152/1000 | Loss: 0.00001262
Iteration 153/1000 | Loss: 0.00001262
Iteration 154/1000 | Loss: 0.00001261
Iteration 155/1000 | Loss: 0.00001261
Iteration 156/1000 | Loss: 0.00001261
Iteration 157/1000 | Loss: 0.00001261
Iteration 158/1000 | Loss: 0.00001261
Iteration 159/1000 | Loss: 0.00001261
Iteration 160/1000 | Loss: 0.00001261
Iteration 161/1000 | Loss: 0.00001261
Iteration 162/1000 | Loss: 0.00001261
Iteration 163/1000 | Loss: 0.00001260
Iteration 164/1000 | Loss: 0.00001260
Iteration 165/1000 | Loss: 0.00001260
Iteration 166/1000 | Loss: 0.00001260
Iteration 167/1000 | Loss: 0.00001260
Iteration 168/1000 | Loss: 0.00001260
Iteration 169/1000 | Loss: 0.00001260
Iteration 170/1000 | Loss: 0.00001260
Iteration 171/1000 | Loss: 0.00001260
Iteration 172/1000 | Loss: 0.00001260
Iteration 173/1000 | Loss: 0.00001260
Iteration 174/1000 | Loss: 0.00001260
Iteration 175/1000 | Loss: 0.00001260
Iteration 176/1000 | Loss: 0.00001260
Iteration 177/1000 | Loss: 0.00001260
Iteration 178/1000 | Loss: 0.00001260
Iteration 179/1000 | Loss: 0.00001260
Iteration 180/1000 | Loss: 0.00001260
Iteration 181/1000 | Loss: 0.00001260
Iteration 182/1000 | Loss: 0.00001260
Iteration 183/1000 | Loss: 0.00001259
Iteration 184/1000 | Loss: 0.00001259
Iteration 185/1000 | Loss: 0.00001259
Iteration 186/1000 | Loss: 0.00001259
Iteration 187/1000 | Loss: 0.00001259
Iteration 188/1000 | Loss: 0.00001259
Iteration 189/1000 | Loss: 0.00001259
Iteration 190/1000 | Loss: 0.00001259
Iteration 191/1000 | Loss: 0.00001259
Iteration 192/1000 | Loss: 0.00001259
Iteration 193/1000 | Loss: 0.00001259
Iteration 194/1000 | Loss: 0.00001259
Iteration 195/1000 | Loss: 0.00001259
Iteration 196/1000 | Loss: 0.00001259
Iteration 197/1000 | Loss: 0.00001259
Iteration 198/1000 | Loss: 0.00001259
Iteration 199/1000 | Loss: 0.00001258
Iteration 200/1000 | Loss: 0.00001258
Iteration 201/1000 | Loss: 0.00001258
Iteration 202/1000 | Loss: 0.00001258
Iteration 203/1000 | Loss: 0.00001258
Iteration 204/1000 | Loss: 0.00001258
Iteration 205/1000 | Loss: 0.00001258
Iteration 206/1000 | Loss: 0.00001258
Iteration 207/1000 | Loss: 0.00001258
Iteration 208/1000 | Loss: 0.00001258
Iteration 209/1000 | Loss: 0.00001258
Iteration 210/1000 | Loss: 0.00001258
Iteration 211/1000 | Loss: 0.00001258
Iteration 212/1000 | Loss: 0.00001258
Iteration 213/1000 | Loss: 0.00001258
Iteration 214/1000 | Loss: 0.00001257
Iteration 215/1000 | Loss: 0.00001257
Iteration 216/1000 | Loss: 0.00001257
Iteration 217/1000 | Loss: 0.00001257
Iteration 218/1000 | Loss: 0.00001257
Iteration 219/1000 | Loss: 0.00001257
Iteration 220/1000 | Loss: 0.00001257
Iteration 221/1000 | Loss: 0.00001257
Iteration 222/1000 | Loss: 0.00001257
Iteration 223/1000 | Loss: 0.00001257
Iteration 224/1000 | Loss: 0.00001257
Iteration 225/1000 | Loss: 0.00001257
Iteration 226/1000 | Loss: 0.00001257
Iteration 227/1000 | Loss: 0.00001257
Iteration 228/1000 | Loss: 0.00001257
Iteration 229/1000 | Loss: 0.00001257
Iteration 230/1000 | Loss: 0.00001257
Iteration 231/1000 | Loss: 0.00001257
Iteration 232/1000 | Loss: 0.00001257
Iteration 233/1000 | Loss: 0.00001257
Iteration 234/1000 | Loss: 0.00001257
Iteration 235/1000 | Loss: 0.00001256
Iteration 236/1000 | Loss: 0.00001256
Iteration 237/1000 | Loss: 0.00001256
Iteration 238/1000 | Loss: 0.00001256
Iteration 239/1000 | Loss: 0.00001256
Iteration 240/1000 | Loss: 0.00001256
Iteration 241/1000 | Loss: 0.00001256
Iteration 242/1000 | Loss: 0.00001256
Iteration 243/1000 | Loss: 0.00001256
Iteration 244/1000 | Loss: 0.00001256
Iteration 245/1000 | Loss: 0.00001256
Iteration 246/1000 | Loss: 0.00001256
Iteration 247/1000 | Loss: 0.00001256
Iteration 248/1000 | Loss: 0.00001256
Iteration 249/1000 | Loss: 0.00001256
Iteration 250/1000 | Loss: 0.00001256
Iteration 251/1000 | Loss: 0.00001256
Iteration 252/1000 | Loss: 0.00001256
Iteration 253/1000 | Loss: 0.00001256
Iteration 254/1000 | Loss: 0.00001256
Iteration 255/1000 | Loss: 0.00001256
Iteration 256/1000 | Loss: 0.00001256
Iteration 257/1000 | Loss: 0.00001256
Iteration 258/1000 | Loss: 0.00001256
Iteration 259/1000 | Loss: 0.00001255
Iteration 260/1000 | Loss: 0.00001255
Iteration 261/1000 | Loss: 0.00001255
Iteration 262/1000 | Loss: 0.00001255
Iteration 263/1000 | Loss: 0.00001255
Iteration 264/1000 | Loss: 0.00001255
Iteration 265/1000 | Loss: 0.00001255
Iteration 266/1000 | Loss: 0.00001255
Iteration 267/1000 | Loss: 0.00001255
Iteration 268/1000 | Loss: 0.00001255
Iteration 269/1000 | Loss: 0.00001255
Iteration 270/1000 | Loss: 0.00001255
Iteration 271/1000 | Loss: 0.00001255
Iteration 272/1000 | Loss: 0.00001255
Iteration 273/1000 | Loss: 0.00001255
Iteration 274/1000 | Loss: 0.00001255
Iteration 275/1000 | Loss: 0.00001255
Iteration 276/1000 | Loss: 0.00001255
Iteration 277/1000 | Loss: 0.00001255
Iteration 278/1000 | Loss: 0.00001255
Iteration 279/1000 | Loss: 0.00001255
Iteration 280/1000 | Loss: 0.00001255
Iteration 281/1000 | Loss: 0.00001255
Iteration 282/1000 | Loss: 0.00001255
Iteration 283/1000 | Loss: 0.00001255
Iteration 284/1000 | Loss: 0.00001255
Iteration 285/1000 | Loss: 0.00001255
Iteration 286/1000 | Loss: 0.00001255
Iteration 287/1000 | Loss: 0.00001255
Iteration 288/1000 | Loss: 0.00001255
Iteration 289/1000 | Loss: 0.00001255
Iteration 290/1000 | Loss: 0.00001255
Iteration 291/1000 | Loss: 0.00001255
Iteration 292/1000 | Loss: 0.00001255
Iteration 293/1000 | Loss: 0.00001255
Iteration 294/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [1.2551301551866345e-05, 1.2551301551866345e-05, 1.2551301551866345e-05, 1.2551301551866345e-05, 1.2551301551866345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2551301551866345e-05

Optimization complete. Final v2v error: 3.0176048278808594 mm

Highest mean error: 3.1963417530059814 mm for frame 123

Lowest mean error: 2.7611935138702393 mm for frame 1

Saving results

Total time: 38.20625329017639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840819
Iteration 2/25 | Loss: 0.00151478
Iteration 3/25 | Loss: 0.00098963
Iteration 4/25 | Loss: 0.00086890
Iteration 5/25 | Loss: 0.00081817
Iteration 6/25 | Loss: 0.00080926
Iteration 7/25 | Loss: 0.00080765
Iteration 8/25 | Loss: 0.00080091
Iteration 9/25 | Loss: 0.00079486
Iteration 10/25 | Loss: 0.00079045
Iteration 11/25 | Loss: 0.00078695
Iteration 12/25 | Loss: 0.00077632
Iteration 13/25 | Loss: 0.00076543
Iteration 14/25 | Loss: 0.00076390
Iteration 15/25 | Loss: 0.00076378
Iteration 16/25 | Loss: 0.00076913
Iteration 17/25 | Loss: 0.00076774
Iteration 18/25 | Loss: 0.00076947
Iteration 19/25 | Loss: 0.00076514
Iteration 20/25 | Loss: 0.00076929
Iteration 21/25 | Loss: 0.00076884
Iteration 22/25 | Loss: 0.00076486
Iteration 23/25 | Loss: 0.00076119
Iteration 24/25 | Loss: 0.00076054
Iteration 25/25 | Loss: 0.00076004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.60853720
Iteration 2/25 | Loss: 0.00084273
Iteration 3/25 | Loss: 0.00084259
Iteration 4/25 | Loss: 0.00084259
Iteration 5/25 | Loss: 0.00084259
Iteration 6/25 | Loss: 0.00084259
Iteration 7/25 | Loss: 0.00084259
Iteration 8/25 | Loss: 0.00084259
Iteration 9/25 | Loss: 0.00084259
Iteration 10/25 | Loss: 0.00084259
Iteration 11/25 | Loss: 0.00084259
Iteration 12/25 | Loss: 0.00084259
Iteration 13/25 | Loss: 0.00084259
Iteration 14/25 | Loss: 0.00084259
Iteration 15/25 | Loss: 0.00084259
Iteration 16/25 | Loss: 0.00084259
Iteration 17/25 | Loss: 0.00084259
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008425853447988629, 0.0008425853447988629, 0.0008425853447988629, 0.0008425853447988629, 0.0008425853447988629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008425853447988629

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084259
Iteration 2/1000 | Loss: 0.00107340
Iteration 3/1000 | Loss: 0.00013605
Iteration 4/1000 | Loss: 0.00027479
Iteration 5/1000 | Loss: 0.00020287
Iteration 6/1000 | Loss: 0.00008737
Iteration 7/1000 | Loss: 0.00122880
Iteration 8/1000 | Loss: 0.00089152
Iteration 9/1000 | Loss: 0.00109574
Iteration 10/1000 | Loss: 0.00018969
Iteration 11/1000 | Loss: 0.00007908
Iteration 12/1000 | Loss: 0.00014256
Iteration 13/1000 | Loss: 0.00023175
Iteration 14/1000 | Loss: 0.00016720
Iteration 15/1000 | Loss: 0.00021934
Iteration 16/1000 | Loss: 0.00027278
Iteration 17/1000 | Loss: 0.00008028
Iteration 18/1000 | Loss: 0.00009816
Iteration 19/1000 | Loss: 0.00030088
Iteration 20/1000 | Loss: 0.00008121
Iteration 21/1000 | Loss: 0.00044097
Iteration 22/1000 | Loss: 0.00023982
Iteration 23/1000 | Loss: 0.00010476
Iteration 24/1000 | Loss: 0.00006223
Iteration 25/1000 | Loss: 0.00017994
Iteration 26/1000 | Loss: 0.00013363
Iteration 27/1000 | Loss: 0.00026647
Iteration 28/1000 | Loss: 0.00013874
Iteration 29/1000 | Loss: 0.00010667
Iteration 30/1000 | Loss: 0.00008609
Iteration 31/1000 | Loss: 0.00023972
Iteration 32/1000 | Loss: 0.00023596
Iteration 33/1000 | Loss: 0.00027344
Iteration 34/1000 | Loss: 0.00023646
Iteration 35/1000 | Loss: 0.00025093
Iteration 36/1000 | Loss: 0.00016228
Iteration 37/1000 | Loss: 0.00016432
Iteration 38/1000 | Loss: 0.00014644
Iteration 39/1000 | Loss: 0.00032203
Iteration 40/1000 | Loss: 0.00013752
Iteration 41/1000 | Loss: 0.00023941
Iteration 42/1000 | Loss: 0.00017097
Iteration 43/1000 | Loss: 0.00015704
Iteration 44/1000 | Loss: 0.00128584
Iteration 45/1000 | Loss: 0.00089001
Iteration 46/1000 | Loss: 0.00114390
Iteration 47/1000 | Loss: 0.00106609
Iteration 48/1000 | Loss: 0.00101391
Iteration 49/1000 | Loss: 0.00088280
Iteration 50/1000 | Loss: 0.00096012
Iteration 51/1000 | Loss: 0.00022868
Iteration 52/1000 | Loss: 0.00063573
Iteration 53/1000 | Loss: 0.00041530
Iteration 54/1000 | Loss: 0.00036198
Iteration 55/1000 | Loss: 0.00030069
Iteration 56/1000 | Loss: 0.00101667
Iteration 57/1000 | Loss: 0.00069786
Iteration 58/1000 | Loss: 0.00029404
Iteration 59/1000 | Loss: 0.00037453
Iteration 60/1000 | Loss: 0.00020298
Iteration 61/1000 | Loss: 0.00013998
Iteration 62/1000 | Loss: 0.00025256
Iteration 63/1000 | Loss: 0.00255354
Iteration 64/1000 | Loss: 0.00148202
Iteration 65/1000 | Loss: 0.00035965
Iteration 66/1000 | Loss: 0.00145929
Iteration 67/1000 | Loss: 0.00090801
Iteration 68/1000 | Loss: 0.00041397
Iteration 69/1000 | Loss: 0.00101190
Iteration 70/1000 | Loss: 0.00062237
Iteration 71/1000 | Loss: 0.00055981
Iteration 72/1000 | Loss: 0.00063303
Iteration 73/1000 | Loss: 0.00069487
Iteration 74/1000 | Loss: 0.00028077
Iteration 75/1000 | Loss: 0.00102757
Iteration 76/1000 | Loss: 0.00093319
Iteration 77/1000 | Loss: 0.00213179
Iteration 78/1000 | Loss: 0.00159437
Iteration 79/1000 | Loss: 0.00334786
Iteration 80/1000 | Loss: 0.00024108
Iteration 81/1000 | Loss: 0.00028040
Iteration 82/1000 | Loss: 0.00073743
Iteration 83/1000 | Loss: 0.00021634
Iteration 84/1000 | Loss: 0.00011609
Iteration 85/1000 | Loss: 0.00081008
Iteration 86/1000 | Loss: 0.00079901
Iteration 87/1000 | Loss: 0.00065922
Iteration 88/1000 | Loss: 0.00051657
Iteration 89/1000 | Loss: 0.00037355
Iteration 90/1000 | Loss: 0.00049969
Iteration 91/1000 | Loss: 0.00024740
Iteration 92/1000 | Loss: 0.00099652
Iteration 93/1000 | Loss: 0.00051120
Iteration 94/1000 | Loss: 0.00011795
Iteration 95/1000 | Loss: 0.00006419
Iteration 96/1000 | Loss: 0.00024620
Iteration 97/1000 | Loss: 0.00021059
Iteration 98/1000 | Loss: 0.00005440
Iteration 99/1000 | Loss: 0.00049472
Iteration 100/1000 | Loss: 0.00528588
Iteration 101/1000 | Loss: 0.00074213
Iteration 102/1000 | Loss: 0.00163123
Iteration 103/1000 | Loss: 0.00099622
Iteration 104/1000 | Loss: 0.00198920
Iteration 105/1000 | Loss: 0.00007574
Iteration 106/1000 | Loss: 0.00005718
Iteration 107/1000 | Loss: 0.00005221
Iteration 108/1000 | Loss: 0.00004988
Iteration 109/1000 | Loss: 0.00004777
Iteration 110/1000 | Loss: 0.00004621
Iteration 111/1000 | Loss: 0.00004410
Iteration 112/1000 | Loss: 0.00016364
Iteration 113/1000 | Loss: 0.00087607
Iteration 114/1000 | Loss: 0.00033327
Iteration 115/1000 | Loss: 0.00006774
Iteration 116/1000 | Loss: 0.00006163
Iteration 117/1000 | Loss: 0.00019172
Iteration 118/1000 | Loss: 0.00007598
Iteration 119/1000 | Loss: 0.00006615
Iteration 120/1000 | Loss: 0.00003869
Iteration 121/1000 | Loss: 0.00003665
Iteration 122/1000 | Loss: 0.00007966
Iteration 123/1000 | Loss: 0.00004272
Iteration 124/1000 | Loss: 0.00003858
Iteration 125/1000 | Loss: 0.00003392
Iteration 126/1000 | Loss: 0.00003273
Iteration 127/1000 | Loss: 0.00003235
Iteration 128/1000 | Loss: 0.00003196
Iteration 129/1000 | Loss: 0.00007133
Iteration 130/1000 | Loss: 0.00006130
Iteration 131/1000 | Loss: 0.00007074
Iteration 132/1000 | Loss: 0.00005902
Iteration 133/1000 | Loss: 0.00007170
Iteration 134/1000 | Loss: 0.00005998
Iteration 135/1000 | Loss: 0.00007018
Iteration 136/1000 | Loss: 0.00005715
Iteration 137/1000 | Loss: 0.00006975
Iteration 138/1000 | Loss: 0.00005251
Iteration 139/1000 | Loss: 0.00006947
Iteration 140/1000 | Loss: 0.00003929
Iteration 141/1000 | Loss: 0.00005101
Iteration 142/1000 | Loss: 0.00006809
Iteration 143/1000 | Loss: 0.00004223
Iteration 144/1000 | Loss: 0.00006796
Iteration 145/1000 | Loss: 0.00006356
Iteration 146/1000 | Loss: 0.00004617
Iteration 147/1000 | Loss: 0.00003848
Iteration 148/1000 | Loss: 0.00006960
Iteration 149/1000 | Loss: 0.00004454
Iteration 150/1000 | Loss: 0.00006645
Iteration 151/1000 | Loss: 0.00005922
Iteration 152/1000 | Loss: 0.00006506
Iteration 153/1000 | Loss: 0.00004999
Iteration 154/1000 | Loss: 0.00006432
Iteration 155/1000 | Loss: 0.00003612
Iteration 156/1000 | Loss: 0.00006359
Iteration 157/1000 | Loss: 0.00004053
Iteration 158/1000 | Loss: 0.00005933
Iteration 159/1000 | Loss: 0.00005150
Iteration 160/1000 | Loss: 0.00005949
Iteration 161/1000 | Loss: 0.00005463
Iteration 162/1000 | Loss: 0.00005793
Iteration 163/1000 | Loss: 0.00005439
Iteration 164/1000 | Loss: 0.00006109
Iteration 165/1000 | Loss: 0.00005347
Iteration 166/1000 | Loss: 0.00005297
Iteration 167/1000 | Loss: 0.00004929
Iteration 168/1000 | Loss: 0.00005123
Iteration 169/1000 | Loss: 0.00003165
Iteration 170/1000 | Loss: 0.00003127
Iteration 171/1000 | Loss: 0.00003106
Iteration 172/1000 | Loss: 0.00003089
Iteration 173/1000 | Loss: 0.00003077
Iteration 174/1000 | Loss: 0.00003077
Iteration 175/1000 | Loss: 0.00003067
Iteration 176/1000 | Loss: 0.00003067
Iteration 177/1000 | Loss: 0.00003066
Iteration 178/1000 | Loss: 0.00003053
Iteration 179/1000 | Loss: 0.00003050
Iteration 180/1000 | Loss: 0.00003044
Iteration 181/1000 | Loss: 0.00003042
Iteration 182/1000 | Loss: 0.00003042
Iteration 183/1000 | Loss: 0.00003042
Iteration 184/1000 | Loss: 0.00003042
Iteration 185/1000 | Loss: 0.00003042
Iteration 186/1000 | Loss: 0.00003042
Iteration 187/1000 | Loss: 0.00003042
Iteration 188/1000 | Loss: 0.00003041
Iteration 189/1000 | Loss: 0.00003041
Iteration 190/1000 | Loss: 0.00003041
Iteration 191/1000 | Loss: 0.00003039
Iteration 192/1000 | Loss: 0.00003039
Iteration 193/1000 | Loss: 0.00003038
Iteration 194/1000 | Loss: 0.00003038
Iteration 195/1000 | Loss: 0.00003038
Iteration 196/1000 | Loss: 0.00003038
Iteration 197/1000 | Loss: 0.00003038
Iteration 198/1000 | Loss: 0.00003038
Iteration 199/1000 | Loss: 0.00003038
Iteration 200/1000 | Loss: 0.00003037
Iteration 201/1000 | Loss: 0.00003037
Iteration 202/1000 | Loss: 0.00003037
Iteration 203/1000 | Loss: 0.00003037
Iteration 204/1000 | Loss: 0.00003037
Iteration 205/1000 | Loss: 0.00003037
Iteration 206/1000 | Loss: 0.00003037
Iteration 207/1000 | Loss: 0.00003037
Iteration 208/1000 | Loss: 0.00003037
Iteration 209/1000 | Loss: 0.00003037
Iteration 210/1000 | Loss: 0.00003037
Iteration 211/1000 | Loss: 0.00003036
Iteration 212/1000 | Loss: 0.00003036
Iteration 213/1000 | Loss: 0.00003036
Iteration 214/1000 | Loss: 0.00003036
Iteration 215/1000 | Loss: 0.00003036
Iteration 216/1000 | Loss: 0.00003036
Iteration 217/1000 | Loss: 0.00003036
Iteration 218/1000 | Loss: 0.00003036
Iteration 219/1000 | Loss: 0.00003035
Iteration 220/1000 | Loss: 0.00003035
Iteration 221/1000 | Loss: 0.00003034
Iteration 222/1000 | Loss: 0.00003034
Iteration 223/1000 | Loss: 0.00003033
Iteration 224/1000 | Loss: 0.00003033
Iteration 225/1000 | Loss: 0.00003032
Iteration 226/1000 | Loss: 0.00003032
Iteration 227/1000 | Loss: 0.00003032
Iteration 228/1000 | Loss: 0.00003031
Iteration 229/1000 | Loss: 0.00003031
Iteration 230/1000 | Loss: 0.00003031
Iteration 231/1000 | Loss: 0.00003030
Iteration 232/1000 | Loss: 0.00003030
Iteration 233/1000 | Loss: 0.00003029
Iteration 234/1000 | Loss: 0.00003029
Iteration 235/1000 | Loss: 0.00003028
Iteration 236/1000 | Loss: 0.00003027
Iteration 237/1000 | Loss: 0.00003026
Iteration 238/1000 | Loss: 0.00003026
Iteration 239/1000 | Loss: 0.00003026
Iteration 240/1000 | Loss: 0.00003025
Iteration 241/1000 | Loss: 0.00003025
Iteration 242/1000 | Loss: 0.00003024
Iteration 243/1000 | Loss: 0.00003024
Iteration 244/1000 | Loss: 0.00003024
Iteration 245/1000 | Loss: 0.00003024
Iteration 246/1000 | Loss: 0.00003023
Iteration 247/1000 | Loss: 0.00003023
Iteration 248/1000 | Loss: 0.00003022
Iteration 249/1000 | Loss: 0.00003022
Iteration 250/1000 | Loss: 0.00003022
Iteration 251/1000 | Loss: 0.00003022
Iteration 252/1000 | Loss: 0.00003022
Iteration 253/1000 | Loss: 0.00003021
Iteration 254/1000 | Loss: 0.00003021
Iteration 255/1000 | Loss: 0.00003021
Iteration 256/1000 | Loss: 0.00003020
Iteration 257/1000 | Loss: 0.00003020
Iteration 258/1000 | Loss: 0.00003020
Iteration 259/1000 | Loss: 0.00003020
Iteration 260/1000 | Loss: 0.00003020
Iteration 261/1000 | Loss: 0.00003019
Iteration 262/1000 | Loss: 0.00003019
Iteration 263/1000 | Loss: 0.00003019
Iteration 264/1000 | Loss: 0.00003019
Iteration 265/1000 | Loss: 0.00003019
Iteration 266/1000 | Loss: 0.00003019
Iteration 267/1000 | Loss: 0.00003019
Iteration 268/1000 | Loss: 0.00003018
Iteration 269/1000 | Loss: 0.00003018
Iteration 270/1000 | Loss: 0.00003018
Iteration 271/1000 | Loss: 0.00003018
Iteration 272/1000 | Loss: 0.00003018
Iteration 273/1000 | Loss: 0.00003018
Iteration 274/1000 | Loss: 0.00003017
Iteration 275/1000 | Loss: 0.00003017
Iteration 276/1000 | Loss: 0.00003017
Iteration 277/1000 | Loss: 0.00003017
Iteration 278/1000 | Loss: 0.00003016
Iteration 279/1000 | Loss: 0.00003016
Iteration 280/1000 | Loss: 0.00003016
Iteration 281/1000 | Loss: 0.00003015
Iteration 282/1000 | Loss: 0.00003015
Iteration 283/1000 | Loss: 0.00003015
Iteration 284/1000 | Loss: 0.00003014
Iteration 285/1000 | Loss: 0.00003014
Iteration 286/1000 | Loss: 0.00003014
Iteration 287/1000 | Loss: 0.00003013
Iteration 288/1000 | Loss: 0.00003013
Iteration 289/1000 | Loss: 0.00003013
Iteration 290/1000 | Loss: 0.00003013
Iteration 291/1000 | Loss: 0.00003013
Iteration 292/1000 | Loss: 0.00003012
Iteration 293/1000 | Loss: 0.00003012
Iteration 294/1000 | Loss: 0.00003012
Iteration 295/1000 | Loss: 0.00003012
Iteration 296/1000 | Loss: 0.00003011
Iteration 297/1000 | Loss: 0.00003011
Iteration 298/1000 | Loss: 0.00003011
Iteration 299/1000 | Loss: 0.00003011
Iteration 300/1000 | Loss: 0.00003011
Iteration 301/1000 | Loss: 0.00003011
Iteration 302/1000 | Loss: 0.00003010
Iteration 303/1000 | Loss: 0.00003010
Iteration 304/1000 | Loss: 0.00003010
Iteration 305/1000 | Loss: 0.00003010
Iteration 306/1000 | Loss: 0.00003010
Iteration 307/1000 | Loss: 0.00003010
Iteration 308/1000 | Loss: 0.00003010
Iteration 309/1000 | Loss: 0.00003010
Iteration 310/1000 | Loss: 0.00003010
Iteration 311/1000 | Loss: 0.00003010
Iteration 312/1000 | Loss: 0.00003009
Iteration 313/1000 | Loss: 0.00003009
Iteration 314/1000 | Loss: 0.00003009
Iteration 315/1000 | Loss: 0.00003009
Iteration 316/1000 | Loss: 0.00003009
Iteration 317/1000 | Loss: 0.00003009
Iteration 318/1000 | Loss: 0.00003009
Iteration 319/1000 | Loss: 0.00003009
Iteration 320/1000 | Loss: 0.00003009
Iteration 321/1000 | Loss: 0.00003009
Iteration 322/1000 | Loss: 0.00003009
Iteration 323/1000 | Loss: 0.00003009
Iteration 324/1000 | Loss: 0.00003009
Iteration 325/1000 | Loss: 0.00003009
Iteration 326/1000 | Loss: 0.00003009
Iteration 327/1000 | Loss: 0.00003009
Iteration 328/1000 | Loss: 0.00003009
Iteration 329/1000 | Loss: 0.00003009
Iteration 330/1000 | Loss: 0.00003009
Iteration 331/1000 | Loss: 0.00003008
Iteration 332/1000 | Loss: 0.00003008
Iteration 333/1000 | Loss: 0.00003008
Iteration 334/1000 | Loss: 0.00003008
Iteration 335/1000 | Loss: 0.00003008
Iteration 336/1000 | Loss: 0.00003008
Iteration 337/1000 | Loss: 0.00003008
Iteration 338/1000 | Loss: 0.00003008
Iteration 339/1000 | Loss: 0.00003008
Iteration 340/1000 | Loss: 0.00003008
Iteration 341/1000 | Loss: 0.00003008
Iteration 342/1000 | Loss: 0.00003008
Iteration 343/1000 | Loss: 0.00003008
Iteration 344/1000 | Loss: 0.00003008
Iteration 345/1000 | Loss: 0.00003008
Iteration 346/1000 | Loss: 0.00003008
Iteration 347/1000 | Loss: 0.00003007
Iteration 348/1000 | Loss: 0.00003007
Iteration 349/1000 | Loss: 0.00003007
Iteration 350/1000 | Loss: 0.00003007
Iteration 351/1000 | Loss: 0.00003007
Iteration 352/1000 | Loss: 0.00003007
Iteration 353/1000 | Loss: 0.00003007
Iteration 354/1000 | Loss: 0.00003007
Iteration 355/1000 | Loss: 0.00003007
Iteration 356/1000 | Loss: 0.00003007
Iteration 357/1000 | Loss: 0.00003007
Iteration 358/1000 | Loss: 0.00003007
Iteration 359/1000 | Loss: 0.00003007
Iteration 360/1000 | Loss: 0.00003007
Iteration 361/1000 | Loss: 0.00003007
Iteration 362/1000 | Loss: 0.00003007
Iteration 363/1000 | Loss: 0.00003007
Iteration 364/1000 | Loss: 0.00003007
Iteration 365/1000 | Loss: 0.00003007
Iteration 366/1000 | Loss: 0.00003007
Iteration 367/1000 | Loss: 0.00003007
Iteration 368/1000 | Loss: 0.00003007
Iteration 369/1000 | Loss: 0.00003007
Iteration 370/1000 | Loss: 0.00003007
Iteration 371/1000 | Loss: 0.00003007
Iteration 372/1000 | Loss: 0.00003007
Iteration 373/1000 | Loss: 0.00003007
Iteration 374/1000 | Loss: 0.00003007
Iteration 375/1000 | Loss: 0.00003007
Iteration 376/1000 | Loss: 0.00003007
Iteration 377/1000 | Loss: 0.00003007
Iteration 378/1000 | Loss: 0.00003007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 378. Stopping optimization.
Last 5 losses: [3.0072447771090083e-05, 3.0072447771090083e-05, 3.0072447771090083e-05, 3.0072447771090083e-05, 3.0072447771090083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0072447771090083e-05

Optimization complete. Final v2v error: 4.343902111053467 mm

Highest mean error: 6.933992385864258 mm for frame 60

Lowest mean error: 2.978956699371338 mm for frame 100

Saving results

Total time: 303.68233919143677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014871
Iteration 2/25 | Loss: 0.01014871
Iteration 3/25 | Loss: 0.01014871
Iteration 4/25 | Loss: 0.01014870
Iteration 5/25 | Loss: 0.00260371
Iteration 6/25 | Loss: 0.00200973
Iteration 7/25 | Loss: 0.00194549
Iteration 8/25 | Loss: 0.00187864
Iteration 9/25 | Loss: 0.00184050
Iteration 10/25 | Loss: 0.00179182
Iteration 11/25 | Loss: 0.00174507
Iteration 12/25 | Loss: 0.00173895
Iteration 13/25 | Loss: 0.00159683
Iteration 14/25 | Loss: 0.00154068
Iteration 15/25 | Loss: 0.00150942
Iteration 16/25 | Loss: 0.00149501
Iteration 17/25 | Loss: 0.00148500
Iteration 18/25 | Loss: 0.00149036
Iteration 19/25 | Loss: 0.00147638
Iteration 20/25 | Loss: 0.00147311
Iteration 21/25 | Loss: 0.00147666
Iteration 22/25 | Loss: 0.00147107
Iteration 23/25 | Loss: 0.00146291
Iteration 24/25 | Loss: 0.00145621
Iteration 25/25 | Loss: 0.00145048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15698838
Iteration 2/25 | Loss: 0.00938775
Iteration 3/25 | Loss: 0.00361926
Iteration 4/25 | Loss: 0.00361924
Iteration 5/25 | Loss: 0.00361924
Iteration 6/25 | Loss: 0.00361924
Iteration 7/25 | Loss: 0.00361924
Iteration 8/25 | Loss: 0.00361924
Iteration 9/25 | Loss: 0.00361924
Iteration 10/25 | Loss: 0.00361924
Iteration 11/25 | Loss: 0.00361924
Iteration 12/25 | Loss: 0.00361924
Iteration 13/25 | Loss: 0.00361924
Iteration 14/25 | Loss: 0.00361924
Iteration 15/25 | Loss: 0.00361924
Iteration 16/25 | Loss: 0.00361924
Iteration 17/25 | Loss: 0.00361924
Iteration 18/25 | Loss: 0.00361924
Iteration 19/25 | Loss: 0.00361924
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003619238268584013, 0.003619238268584013, 0.003619238268584013, 0.003619238268584013, 0.003619238268584013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003619238268584013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00361924
Iteration 2/1000 | Loss: 0.00390062
Iteration 3/1000 | Loss: 0.00613988
Iteration 4/1000 | Loss: 0.00852401
Iteration 5/1000 | Loss: 0.00088777
Iteration 6/1000 | Loss: 0.00058929
Iteration 7/1000 | Loss: 0.00125434
Iteration 8/1000 | Loss: 0.00273128
Iteration 9/1000 | Loss: 0.00175057
Iteration 10/1000 | Loss: 0.00213398
Iteration 11/1000 | Loss: 0.00177394
Iteration 12/1000 | Loss: 0.00973884
Iteration 13/1000 | Loss: 0.00096643
Iteration 14/1000 | Loss: 0.00293001
Iteration 15/1000 | Loss: 0.00134784
Iteration 16/1000 | Loss: 0.00199241
Iteration 17/1000 | Loss: 0.00088183
Iteration 18/1000 | Loss: 0.00047390
Iteration 19/1000 | Loss: 0.00104650
Iteration 20/1000 | Loss: 0.00091709
Iteration 21/1000 | Loss: 0.00084983
Iteration 22/1000 | Loss: 0.00049998
Iteration 23/1000 | Loss: 0.00047208
Iteration 24/1000 | Loss: 0.00021771
Iteration 25/1000 | Loss: 0.00046016
Iteration 26/1000 | Loss: 0.00029604
Iteration 27/1000 | Loss: 0.00015665
Iteration 28/1000 | Loss: 0.00117404
Iteration 29/1000 | Loss: 0.00011031
Iteration 30/1000 | Loss: 0.00096013
Iteration 31/1000 | Loss: 0.00041931
Iteration 32/1000 | Loss: 0.00020871
Iteration 33/1000 | Loss: 0.00008652
Iteration 34/1000 | Loss: 0.00004476
Iteration 35/1000 | Loss: 0.00019423
Iteration 36/1000 | Loss: 0.00004146
Iteration 37/1000 | Loss: 0.00003630
Iteration 38/1000 | Loss: 0.00010701
Iteration 39/1000 | Loss: 0.00004992
Iteration 40/1000 | Loss: 0.00003702
Iteration 41/1000 | Loss: 0.00003282
Iteration 42/1000 | Loss: 0.00010753
Iteration 43/1000 | Loss: 0.00019202
Iteration 44/1000 | Loss: 0.00003025
Iteration 45/1000 | Loss: 0.00024255
Iteration 46/1000 | Loss: 0.00003587
Iteration 47/1000 | Loss: 0.00002888
Iteration 48/1000 | Loss: 0.00002587
Iteration 49/1000 | Loss: 0.00002367
Iteration 50/1000 | Loss: 0.00002260
Iteration 51/1000 | Loss: 0.00002181
Iteration 52/1000 | Loss: 0.00002142
Iteration 53/1000 | Loss: 0.00002116
Iteration 54/1000 | Loss: 0.00002085
Iteration 55/1000 | Loss: 0.00002082
Iteration 56/1000 | Loss: 0.00002072
Iteration 57/1000 | Loss: 0.00002060
Iteration 58/1000 | Loss: 0.00002059
Iteration 59/1000 | Loss: 0.00002059
Iteration 60/1000 | Loss: 0.00002055
Iteration 61/1000 | Loss: 0.00002049
Iteration 62/1000 | Loss: 0.00002047
Iteration 63/1000 | Loss: 0.00002047
Iteration 64/1000 | Loss: 0.00002046
Iteration 65/1000 | Loss: 0.00002046
Iteration 66/1000 | Loss: 0.00002042
Iteration 67/1000 | Loss: 0.00002041
Iteration 68/1000 | Loss: 0.00002040
Iteration 69/1000 | Loss: 0.00002039
Iteration 70/1000 | Loss: 0.00002039
Iteration 71/1000 | Loss: 0.00002038
Iteration 72/1000 | Loss: 0.00002038
Iteration 73/1000 | Loss: 0.00002038
Iteration 74/1000 | Loss: 0.00002037
Iteration 75/1000 | Loss: 0.00002036
Iteration 76/1000 | Loss: 0.00002036
Iteration 77/1000 | Loss: 0.00002033
Iteration 78/1000 | Loss: 0.00002032
Iteration 79/1000 | Loss: 0.00002031
Iteration 80/1000 | Loss: 0.00002031
Iteration 81/1000 | Loss: 0.00002030
Iteration 82/1000 | Loss: 0.00002030
Iteration 83/1000 | Loss: 0.00002030
Iteration 84/1000 | Loss: 0.00002029
Iteration 85/1000 | Loss: 0.00002029
Iteration 86/1000 | Loss: 0.00002028
Iteration 87/1000 | Loss: 0.00002028
Iteration 88/1000 | Loss: 0.00002028
Iteration 89/1000 | Loss: 0.00002027
Iteration 90/1000 | Loss: 0.00002027
Iteration 91/1000 | Loss: 0.00002027
Iteration 92/1000 | Loss: 0.00002027
Iteration 93/1000 | Loss: 0.00002027
Iteration 94/1000 | Loss: 0.00002026
Iteration 95/1000 | Loss: 0.00002026
Iteration 96/1000 | Loss: 0.00002026
Iteration 97/1000 | Loss: 0.00002026
Iteration 98/1000 | Loss: 0.00002026
Iteration 99/1000 | Loss: 0.00002026
Iteration 100/1000 | Loss: 0.00002026
Iteration 101/1000 | Loss: 0.00002026
Iteration 102/1000 | Loss: 0.00002026
Iteration 103/1000 | Loss: 0.00002026
Iteration 104/1000 | Loss: 0.00002026
Iteration 105/1000 | Loss: 0.00002025
Iteration 106/1000 | Loss: 0.00002025
Iteration 107/1000 | Loss: 0.00002025
Iteration 108/1000 | Loss: 0.00002025
Iteration 109/1000 | Loss: 0.00002024
Iteration 110/1000 | Loss: 0.00002024
Iteration 111/1000 | Loss: 0.00002024
Iteration 112/1000 | Loss: 0.00002023
Iteration 113/1000 | Loss: 0.00002023
Iteration 114/1000 | Loss: 0.00002023
Iteration 115/1000 | Loss: 0.00002023
Iteration 116/1000 | Loss: 0.00002022
Iteration 117/1000 | Loss: 0.00002022
Iteration 118/1000 | Loss: 0.00002022
Iteration 119/1000 | Loss: 0.00002022
Iteration 120/1000 | Loss: 0.00002022
Iteration 121/1000 | Loss: 0.00002022
Iteration 122/1000 | Loss: 0.00002022
Iteration 123/1000 | Loss: 0.00002021
Iteration 124/1000 | Loss: 0.00002021
Iteration 125/1000 | Loss: 0.00002021
Iteration 126/1000 | Loss: 0.00002021
Iteration 127/1000 | Loss: 0.00002020
Iteration 128/1000 | Loss: 0.00002020
Iteration 129/1000 | Loss: 0.00002020
Iteration 130/1000 | Loss: 0.00002020
Iteration 131/1000 | Loss: 0.00002020
Iteration 132/1000 | Loss: 0.00002020
Iteration 133/1000 | Loss: 0.00002020
Iteration 134/1000 | Loss: 0.00002020
Iteration 135/1000 | Loss: 0.00002020
Iteration 136/1000 | Loss: 0.00002019
Iteration 137/1000 | Loss: 0.00002019
Iteration 138/1000 | Loss: 0.00002019
Iteration 139/1000 | Loss: 0.00002019
Iteration 140/1000 | Loss: 0.00002019
Iteration 141/1000 | Loss: 0.00002019
Iteration 142/1000 | Loss: 0.00002019
Iteration 143/1000 | Loss: 0.00002019
Iteration 144/1000 | Loss: 0.00010379
Iteration 145/1000 | Loss: 0.00002916
Iteration 146/1000 | Loss: 0.00002371
Iteration 147/1000 | Loss: 0.00002144
Iteration 148/1000 | Loss: 0.00002020
Iteration 149/1000 | Loss: 0.00001965
Iteration 150/1000 | Loss: 0.00001926
Iteration 151/1000 | Loss: 0.00001921
Iteration 152/1000 | Loss: 0.00001907
Iteration 153/1000 | Loss: 0.00001904
Iteration 154/1000 | Loss: 0.00001900
Iteration 155/1000 | Loss: 0.00001899
Iteration 156/1000 | Loss: 0.00001899
Iteration 157/1000 | Loss: 0.00001898
Iteration 158/1000 | Loss: 0.00001898
Iteration 159/1000 | Loss: 0.00001898
Iteration 160/1000 | Loss: 0.00001897
Iteration 161/1000 | Loss: 0.00001896
Iteration 162/1000 | Loss: 0.00001896
Iteration 163/1000 | Loss: 0.00001896
Iteration 164/1000 | Loss: 0.00001895
Iteration 165/1000 | Loss: 0.00001894
Iteration 166/1000 | Loss: 0.00001893
Iteration 167/1000 | Loss: 0.00001892
Iteration 168/1000 | Loss: 0.00001891
Iteration 169/1000 | Loss: 0.00001891
Iteration 170/1000 | Loss: 0.00001890
Iteration 171/1000 | Loss: 0.00001889
Iteration 172/1000 | Loss: 0.00001887
Iteration 173/1000 | Loss: 0.00001886
Iteration 174/1000 | Loss: 0.00001886
Iteration 175/1000 | Loss: 0.00001886
Iteration 176/1000 | Loss: 0.00001885
Iteration 177/1000 | Loss: 0.00001884
Iteration 178/1000 | Loss: 0.00001884
Iteration 179/1000 | Loss: 0.00001884
Iteration 180/1000 | Loss: 0.00001883
Iteration 181/1000 | Loss: 0.00001883
Iteration 182/1000 | Loss: 0.00001883
Iteration 183/1000 | Loss: 0.00001881
Iteration 184/1000 | Loss: 0.00001881
Iteration 185/1000 | Loss: 0.00001880
Iteration 186/1000 | Loss: 0.00001880
Iteration 187/1000 | Loss: 0.00001880
Iteration 188/1000 | Loss: 0.00001880
Iteration 189/1000 | Loss: 0.00001879
Iteration 190/1000 | Loss: 0.00001879
Iteration 191/1000 | Loss: 0.00001879
Iteration 192/1000 | Loss: 0.00001879
Iteration 193/1000 | Loss: 0.00001878
Iteration 194/1000 | Loss: 0.00001878
Iteration 195/1000 | Loss: 0.00001878
Iteration 196/1000 | Loss: 0.00001878
Iteration 197/1000 | Loss: 0.00001878
Iteration 198/1000 | Loss: 0.00001878
Iteration 199/1000 | Loss: 0.00001877
Iteration 200/1000 | Loss: 0.00001877
Iteration 201/1000 | Loss: 0.00001877
Iteration 202/1000 | Loss: 0.00001877
Iteration 203/1000 | Loss: 0.00001876
Iteration 204/1000 | Loss: 0.00001876
Iteration 205/1000 | Loss: 0.00001875
Iteration 206/1000 | Loss: 0.00001875
Iteration 207/1000 | Loss: 0.00001875
Iteration 208/1000 | Loss: 0.00001874
Iteration 209/1000 | Loss: 0.00001874
Iteration 210/1000 | Loss: 0.00001874
Iteration 211/1000 | Loss: 0.00001874
Iteration 212/1000 | Loss: 0.00001874
Iteration 213/1000 | Loss: 0.00001874
Iteration 214/1000 | Loss: 0.00001874
Iteration 215/1000 | Loss: 0.00001874
Iteration 216/1000 | Loss: 0.00001874
Iteration 217/1000 | Loss: 0.00001874
Iteration 218/1000 | Loss: 0.00001874
Iteration 219/1000 | Loss: 0.00001874
Iteration 220/1000 | Loss: 0.00001874
Iteration 221/1000 | Loss: 0.00001874
Iteration 222/1000 | Loss: 0.00001874
Iteration 223/1000 | Loss: 0.00001874
Iteration 224/1000 | Loss: 0.00001874
Iteration 225/1000 | Loss: 0.00001874
Iteration 226/1000 | Loss: 0.00001874
Iteration 227/1000 | Loss: 0.00001874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.8735874618869275e-05, 1.8735874618869275e-05, 1.8735874618869275e-05, 1.8735874618869275e-05, 1.8735874618869275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8735874618869275e-05

Optimization complete. Final v2v error: 3.091052293777466 mm

Highest mean error: 10.242265701293945 mm for frame 38

Lowest mean error: 2.579094648361206 mm for frame 60

Saving results

Total time: 167.6190824508667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487995
Iteration 2/25 | Loss: 0.00120117
Iteration 3/25 | Loss: 0.00110708
Iteration 4/25 | Loss: 0.00109274
Iteration 5/25 | Loss: 0.00108827
Iteration 6/25 | Loss: 0.00108668
Iteration 7/25 | Loss: 0.00108668
Iteration 8/25 | Loss: 0.00108668
Iteration 9/25 | Loss: 0.00108668
Iteration 10/25 | Loss: 0.00108668
Iteration 11/25 | Loss: 0.00108668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010866823140531778, 0.0010866823140531778, 0.0010866823140531778, 0.0010866823140531778, 0.0010866823140531778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010866823140531778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19842577
Iteration 2/25 | Loss: 0.00226705
Iteration 3/25 | Loss: 0.00226702
Iteration 4/25 | Loss: 0.00226702
Iteration 5/25 | Loss: 0.00226702
Iteration 6/25 | Loss: 0.00226702
Iteration 7/25 | Loss: 0.00226702
Iteration 8/25 | Loss: 0.00226702
Iteration 9/25 | Loss: 0.00226702
Iteration 10/25 | Loss: 0.00226702
Iteration 11/25 | Loss: 0.00226702
Iteration 12/25 | Loss: 0.00226702
Iteration 13/25 | Loss: 0.00226701
Iteration 14/25 | Loss: 0.00226701
Iteration 15/25 | Loss: 0.00226701
Iteration 16/25 | Loss: 0.00226701
Iteration 17/25 | Loss: 0.00226701
Iteration 18/25 | Loss: 0.00226701
Iteration 19/25 | Loss: 0.00226701
Iteration 20/25 | Loss: 0.00226701
Iteration 21/25 | Loss: 0.00226701
Iteration 22/25 | Loss: 0.00226701
Iteration 23/25 | Loss: 0.00226701
Iteration 24/25 | Loss: 0.00226701
Iteration 25/25 | Loss: 0.00226701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226701
Iteration 2/1000 | Loss: 0.00003131
Iteration 3/1000 | Loss: 0.00001727
Iteration 4/1000 | Loss: 0.00001485
Iteration 5/1000 | Loss: 0.00001316
Iteration 6/1000 | Loss: 0.00001252
Iteration 7/1000 | Loss: 0.00001221
Iteration 8/1000 | Loss: 0.00001195
Iteration 9/1000 | Loss: 0.00001195
Iteration 10/1000 | Loss: 0.00001194
Iteration 11/1000 | Loss: 0.00001185
Iteration 12/1000 | Loss: 0.00001173
Iteration 13/1000 | Loss: 0.00001168
Iteration 14/1000 | Loss: 0.00001151
Iteration 15/1000 | Loss: 0.00001143
Iteration 16/1000 | Loss: 0.00001141
Iteration 17/1000 | Loss: 0.00001141
Iteration 18/1000 | Loss: 0.00001134
Iteration 19/1000 | Loss: 0.00001133
Iteration 20/1000 | Loss: 0.00001133
Iteration 21/1000 | Loss: 0.00001128
Iteration 22/1000 | Loss: 0.00001127
Iteration 23/1000 | Loss: 0.00001127
Iteration 24/1000 | Loss: 0.00001124
Iteration 25/1000 | Loss: 0.00001123
Iteration 26/1000 | Loss: 0.00001119
Iteration 27/1000 | Loss: 0.00001116
Iteration 28/1000 | Loss: 0.00001115
Iteration 29/1000 | Loss: 0.00001115
Iteration 30/1000 | Loss: 0.00001115
Iteration 31/1000 | Loss: 0.00001114
Iteration 32/1000 | Loss: 0.00001114
Iteration 33/1000 | Loss: 0.00001111
Iteration 34/1000 | Loss: 0.00001110
Iteration 35/1000 | Loss: 0.00001110
Iteration 36/1000 | Loss: 0.00001110
Iteration 37/1000 | Loss: 0.00001110
Iteration 38/1000 | Loss: 0.00001109
Iteration 39/1000 | Loss: 0.00001107
Iteration 40/1000 | Loss: 0.00001106
Iteration 41/1000 | Loss: 0.00001106
Iteration 42/1000 | Loss: 0.00001106
Iteration 43/1000 | Loss: 0.00001106
Iteration 44/1000 | Loss: 0.00001105
Iteration 45/1000 | Loss: 0.00001105
Iteration 46/1000 | Loss: 0.00001105
Iteration 47/1000 | Loss: 0.00001105
Iteration 48/1000 | Loss: 0.00001105
Iteration 49/1000 | Loss: 0.00001104
Iteration 50/1000 | Loss: 0.00001104
Iteration 51/1000 | Loss: 0.00001104
Iteration 52/1000 | Loss: 0.00001103
Iteration 53/1000 | Loss: 0.00001102
Iteration 54/1000 | Loss: 0.00001101
Iteration 55/1000 | Loss: 0.00001101
Iteration 56/1000 | Loss: 0.00001101
Iteration 57/1000 | Loss: 0.00001100
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001099
Iteration 60/1000 | Loss: 0.00001098
Iteration 61/1000 | Loss: 0.00001097
Iteration 62/1000 | Loss: 0.00001096
Iteration 63/1000 | Loss: 0.00001096
Iteration 64/1000 | Loss: 0.00001095
Iteration 65/1000 | Loss: 0.00001095
Iteration 66/1000 | Loss: 0.00001094
Iteration 67/1000 | Loss: 0.00001093
Iteration 68/1000 | Loss: 0.00001093
Iteration 69/1000 | Loss: 0.00001093
Iteration 70/1000 | Loss: 0.00001093
Iteration 71/1000 | Loss: 0.00001092
Iteration 72/1000 | Loss: 0.00001092
Iteration 73/1000 | Loss: 0.00001092
Iteration 74/1000 | Loss: 0.00001092
Iteration 75/1000 | Loss: 0.00001091
Iteration 76/1000 | Loss: 0.00001091
Iteration 77/1000 | Loss: 0.00001091
Iteration 78/1000 | Loss: 0.00001091
Iteration 79/1000 | Loss: 0.00001090
Iteration 80/1000 | Loss: 0.00001090
Iteration 81/1000 | Loss: 0.00001090
Iteration 82/1000 | Loss: 0.00001089
Iteration 83/1000 | Loss: 0.00001089
Iteration 84/1000 | Loss: 0.00001089
Iteration 85/1000 | Loss: 0.00001089
Iteration 86/1000 | Loss: 0.00001088
Iteration 87/1000 | Loss: 0.00001088
Iteration 88/1000 | Loss: 0.00001088
Iteration 89/1000 | Loss: 0.00001088
Iteration 90/1000 | Loss: 0.00001087
Iteration 91/1000 | Loss: 0.00001087
Iteration 92/1000 | Loss: 0.00001087
Iteration 93/1000 | Loss: 0.00001087
Iteration 94/1000 | Loss: 0.00001086
Iteration 95/1000 | Loss: 0.00001086
Iteration 96/1000 | Loss: 0.00001086
Iteration 97/1000 | Loss: 0.00001085
Iteration 98/1000 | Loss: 0.00001085
Iteration 99/1000 | Loss: 0.00001085
Iteration 100/1000 | Loss: 0.00001085
Iteration 101/1000 | Loss: 0.00001085
Iteration 102/1000 | Loss: 0.00001084
Iteration 103/1000 | Loss: 0.00001084
Iteration 104/1000 | Loss: 0.00001084
Iteration 105/1000 | Loss: 0.00001084
Iteration 106/1000 | Loss: 0.00001084
Iteration 107/1000 | Loss: 0.00001084
Iteration 108/1000 | Loss: 0.00001084
Iteration 109/1000 | Loss: 0.00001084
Iteration 110/1000 | Loss: 0.00001083
Iteration 111/1000 | Loss: 0.00001083
Iteration 112/1000 | Loss: 0.00001083
Iteration 113/1000 | Loss: 0.00001083
Iteration 114/1000 | Loss: 0.00001082
Iteration 115/1000 | Loss: 0.00001082
Iteration 116/1000 | Loss: 0.00001082
Iteration 117/1000 | Loss: 0.00001081
Iteration 118/1000 | Loss: 0.00001081
Iteration 119/1000 | Loss: 0.00001081
Iteration 120/1000 | Loss: 0.00001080
Iteration 121/1000 | Loss: 0.00001080
Iteration 122/1000 | Loss: 0.00001080
Iteration 123/1000 | Loss: 0.00001080
Iteration 124/1000 | Loss: 0.00001080
Iteration 125/1000 | Loss: 0.00001080
Iteration 126/1000 | Loss: 0.00001080
Iteration 127/1000 | Loss: 0.00001079
Iteration 128/1000 | Loss: 0.00001079
Iteration 129/1000 | Loss: 0.00001079
Iteration 130/1000 | Loss: 0.00001078
Iteration 131/1000 | Loss: 0.00001078
Iteration 132/1000 | Loss: 0.00001078
Iteration 133/1000 | Loss: 0.00001078
Iteration 134/1000 | Loss: 0.00001078
Iteration 135/1000 | Loss: 0.00001078
Iteration 136/1000 | Loss: 0.00001078
Iteration 137/1000 | Loss: 0.00001077
Iteration 138/1000 | Loss: 0.00001077
Iteration 139/1000 | Loss: 0.00001077
Iteration 140/1000 | Loss: 0.00001077
Iteration 141/1000 | Loss: 0.00001077
Iteration 142/1000 | Loss: 0.00001077
Iteration 143/1000 | Loss: 0.00001077
Iteration 144/1000 | Loss: 0.00001077
Iteration 145/1000 | Loss: 0.00001077
Iteration 146/1000 | Loss: 0.00001077
Iteration 147/1000 | Loss: 0.00001076
Iteration 148/1000 | Loss: 0.00001076
Iteration 149/1000 | Loss: 0.00001076
Iteration 150/1000 | Loss: 0.00001075
Iteration 151/1000 | Loss: 0.00001075
Iteration 152/1000 | Loss: 0.00001075
Iteration 153/1000 | Loss: 0.00001075
Iteration 154/1000 | Loss: 0.00001075
Iteration 155/1000 | Loss: 0.00001075
Iteration 156/1000 | Loss: 0.00001075
Iteration 157/1000 | Loss: 0.00001075
Iteration 158/1000 | Loss: 0.00001075
Iteration 159/1000 | Loss: 0.00001075
Iteration 160/1000 | Loss: 0.00001074
Iteration 161/1000 | Loss: 0.00001074
Iteration 162/1000 | Loss: 0.00001074
Iteration 163/1000 | Loss: 0.00001074
Iteration 164/1000 | Loss: 0.00001074
Iteration 165/1000 | Loss: 0.00001074
Iteration 166/1000 | Loss: 0.00001074
Iteration 167/1000 | Loss: 0.00001073
Iteration 168/1000 | Loss: 0.00001073
Iteration 169/1000 | Loss: 0.00001073
Iteration 170/1000 | Loss: 0.00001073
Iteration 171/1000 | Loss: 0.00001073
Iteration 172/1000 | Loss: 0.00001073
Iteration 173/1000 | Loss: 0.00001073
Iteration 174/1000 | Loss: 0.00001073
Iteration 175/1000 | Loss: 0.00001073
Iteration 176/1000 | Loss: 0.00001073
Iteration 177/1000 | Loss: 0.00001073
Iteration 178/1000 | Loss: 0.00001072
Iteration 179/1000 | Loss: 0.00001072
Iteration 180/1000 | Loss: 0.00001072
Iteration 181/1000 | Loss: 0.00001072
Iteration 182/1000 | Loss: 0.00001072
Iteration 183/1000 | Loss: 0.00001072
Iteration 184/1000 | Loss: 0.00001072
Iteration 185/1000 | Loss: 0.00001072
Iteration 186/1000 | Loss: 0.00001071
Iteration 187/1000 | Loss: 0.00001071
Iteration 188/1000 | Loss: 0.00001071
Iteration 189/1000 | Loss: 0.00001071
Iteration 190/1000 | Loss: 0.00001071
Iteration 191/1000 | Loss: 0.00001071
Iteration 192/1000 | Loss: 0.00001071
Iteration 193/1000 | Loss: 0.00001071
Iteration 194/1000 | Loss: 0.00001071
Iteration 195/1000 | Loss: 0.00001071
Iteration 196/1000 | Loss: 0.00001071
Iteration 197/1000 | Loss: 0.00001071
Iteration 198/1000 | Loss: 0.00001071
Iteration 199/1000 | Loss: 0.00001071
Iteration 200/1000 | Loss: 0.00001070
Iteration 201/1000 | Loss: 0.00001070
Iteration 202/1000 | Loss: 0.00001070
Iteration 203/1000 | Loss: 0.00001070
Iteration 204/1000 | Loss: 0.00001070
Iteration 205/1000 | Loss: 0.00001070
Iteration 206/1000 | Loss: 0.00001070
Iteration 207/1000 | Loss: 0.00001070
Iteration 208/1000 | Loss: 0.00001070
Iteration 209/1000 | Loss: 0.00001070
Iteration 210/1000 | Loss: 0.00001070
Iteration 211/1000 | Loss: 0.00001070
Iteration 212/1000 | Loss: 0.00001070
Iteration 213/1000 | Loss: 0.00001070
Iteration 214/1000 | Loss: 0.00001070
Iteration 215/1000 | Loss: 0.00001070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.070076359610539e-05, 1.070076359610539e-05, 1.070076359610539e-05, 1.070076359610539e-05, 1.070076359610539e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.070076359610539e-05

Optimization complete. Final v2v error: 2.869051218032837 mm

Highest mean error: 3.1083030700683594 mm for frame 26

Lowest mean error: 2.6290812492370605 mm for frame 125

Saving results

Total time: 44.3552725315094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380772
Iteration 2/25 | Loss: 0.00118117
Iteration 3/25 | Loss: 0.00111054
Iteration 4/25 | Loss: 0.00109972
Iteration 5/25 | Loss: 0.00109593
Iteration 6/25 | Loss: 0.00109526
Iteration 7/25 | Loss: 0.00109526
Iteration 8/25 | Loss: 0.00109526
Iteration 9/25 | Loss: 0.00109526
Iteration 10/25 | Loss: 0.00109526
Iteration 11/25 | Loss: 0.00109526
Iteration 12/25 | Loss: 0.00109526
Iteration 13/25 | Loss: 0.00109526
Iteration 14/25 | Loss: 0.00109526
Iteration 15/25 | Loss: 0.00109526
Iteration 16/25 | Loss: 0.00109526
Iteration 17/25 | Loss: 0.00109526
Iteration 18/25 | Loss: 0.00109526
Iteration 19/25 | Loss: 0.00109526
Iteration 20/25 | Loss: 0.00109526
Iteration 21/25 | Loss: 0.00109526
Iteration 22/25 | Loss: 0.00109526
Iteration 23/25 | Loss: 0.00109526
Iteration 24/25 | Loss: 0.00109526
Iteration 25/25 | Loss: 0.00109526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20257771
Iteration 2/25 | Loss: 0.00226295
Iteration 3/25 | Loss: 0.00226295
Iteration 4/25 | Loss: 0.00226295
Iteration 5/25 | Loss: 0.00226295
Iteration 6/25 | Loss: 0.00226295
Iteration 7/25 | Loss: 0.00226295
Iteration 8/25 | Loss: 0.00226295
Iteration 9/25 | Loss: 0.00226295
Iteration 10/25 | Loss: 0.00226295
Iteration 11/25 | Loss: 0.00226295
Iteration 12/25 | Loss: 0.00226295
Iteration 13/25 | Loss: 0.00226295
Iteration 14/25 | Loss: 0.00226295
Iteration 15/25 | Loss: 0.00226295
Iteration 16/25 | Loss: 0.00226295
Iteration 17/25 | Loss: 0.00226295
Iteration 18/25 | Loss: 0.00226295
Iteration 19/25 | Loss: 0.00226295
Iteration 20/25 | Loss: 0.00226295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002262945519760251, 0.002262945519760251, 0.002262945519760251, 0.002262945519760251, 0.002262945519760251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002262945519760251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226295
Iteration 2/1000 | Loss: 0.00003505
Iteration 3/1000 | Loss: 0.00002027
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001590
Iteration 6/1000 | Loss: 0.00001511
Iteration 7/1000 | Loss: 0.00001445
Iteration 8/1000 | Loss: 0.00001411
Iteration 9/1000 | Loss: 0.00001372
Iteration 10/1000 | Loss: 0.00001335
Iteration 11/1000 | Loss: 0.00001311
Iteration 12/1000 | Loss: 0.00001307
Iteration 13/1000 | Loss: 0.00001300
Iteration 14/1000 | Loss: 0.00001298
Iteration 15/1000 | Loss: 0.00001298
Iteration 16/1000 | Loss: 0.00001297
Iteration 17/1000 | Loss: 0.00001297
Iteration 18/1000 | Loss: 0.00001296
Iteration 19/1000 | Loss: 0.00001291
Iteration 20/1000 | Loss: 0.00001290
Iteration 21/1000 | Loss: 0.00001290
Iteration 22/1000 | Loss: 0.00001290
Iteration 23/1000 | Loss: 0.00001290
Iteration 24/1000 | Loss: 0.00001289
Iteration 25/1000 | Loss: 0.00001287
Iteration 26/1000 | Loss: 0.00001285
Iteration 27/1000 | Loss: 0.00001282
Iteration 28/1000 | Loss: 0.00001281
Iteration 29/1000 | Loss: 0.00001281
Iteration 30/1000 | Loss: 0.00001280
Iteration 31/1000 | Loss: 0.00001279
Iteration 32/1000 | Loss: 0.00001275
Iteration 33/1000 | Loss: 0.00001274
Iteration 34/1000 | Loss: 0.00001271
Iteration 35/1000 | Loss: 0.00001270
Iteration 36/1000 | Loss: 0.00001258
Iteration 37/1000 | Loss: 0.00001257
Iteration 38/1000 | Loss: 0.00001255
Iteration 39/1000 | Loss: 0.00001254
Iteration 40/1000 | Loss: 0.00001254
Iteration 41/1000 | Loss: 0.00001254
Iteration 42/1000 | Loss: 0.00001253
Iteration 43/1000 | Loss: 0.00001253
Iteration 44/1000 | Loss: 0.00001252
Iteration 45/1000 | Loss: 0.00001252
Iteration 46/1000 | Loss: 0.00001252
Iteration 47/1000 | Loss: 0.00001251
Iteration 48/1000 | Loss: 0.00001251
Iteration 49/1000 | Loss: 0.00001251
Iteration 50/1000 | Loss: 0.00001250
Iteration 51/1000 | Loss: 0.00001250
Iteration 52/1000 | Loss: 0.00001250
Iteration 53/1000 | Loss: 0.00001250
Iteration 54/1000 | Loss: 0.00001250
Iteration 55/1000 | Loss: 0.00001250
Iteration 56/1000 | Loss: 0.00001249
Iteration 57/1000 | Loss: 0.00001249
Iteration 58/1000 | Loss: 0.00001248
Iteration 59/1000 | Loss: 0.00001248
Iteration 60/1000 | Loss: 0.00001248
Iteration 61/1000 | Loss: 0.00001248
Iteration 62/1000 | Loss: 0.00001247
Iteration 63/1000 | Loss: 0.00001247
Iteration 64/1000 | Loss: 0.00001247
Iteration 65/1000 | Loss: 0.00001247
Iteration 66/1000 | Loss: 0.00001246
Iteration 67/1000 | Loss: 0.00001246
Iteration 68/1000 | Loss: 0.00001246
Iteration 69/1000 | Loss: 0.00001246
Iteration 70/1000 | Loss: 0.00001246
Iteration 71/1000 | Loss: 0.00001246
Iteration 72/1000 | Loss: 0.00001246
Iteration 73/1000 | Loss: 0.00001246
Iteration 74/1000 | Loss: 0.00001246
Iteration 75/1000 | Loss: 0.00001246
Iteration 76/1000 | Loss: 0.00001246
Iteration 77/1000 | Loss: 0.00001245
Iteration 78/1000 | Loss: 0.00001245
Iteration 79/1000 | Loss: 0.00001245
Iteration 80/1000 | Loss: 0.00001245
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001245
Iteration 85/1000 | Loss: 0.00001245
Iteration 86/1000 | Loss: 0.00001245
Iteration 87/1000 | Loss: 0.00001245
Iteration 88/1000 | Loss: 0.00001245
Iteration 89/1000 | Loss: 0.00001244
Iteration 90/1000 | Loss: 0.00001244
Iteration 91/1000 | Loss: 0.00001244
Iteration 92/1000 | Loss: 0.00001244
Iteration 93/1000 | Loss: 0.00001244
Iteration 94/1000 | Loss: 0.00001244
Iteration 95/1000 | Loss: 0.00001244
Iteration 96/1000 | Loss: 0.00001244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.2444654203136452e-05, 1.2444654203136452e-05, 1.2444654203136452e-05, 1.2444654203136452e-05, 1.2444654203136452e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2444654203136452e-05

Optimization complete. Final v2v error: 3.0824246406555176 mm

Highest mean error: 3.4728314876556396 mm for frame 157

Lowest mean error: 2.801384449005127 mm for frame 17

Saving results

Total time: 35.392595052719116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384832
Iteration 2/25 | Loss: 0.00118879
Iteration 3/25 | Loss: 0.00109656
Iteration 4/25 | Loss: 0.00108487
Iteration 5/25 | Loss: 0.00108136
Iteration 6/25 | Loss: 0.00108136
Iteration 7/25 | Loss: 0.00108136
Iteration 8/25 | Loss: 0.00108136
Iteration 9/25 | Loss: 0.00108136
Iteration 10/25 | Loss: 0.00108136
Iteration 11/25 | Loss: 0.00108136
Iteration 12/25 | Loss: 0.00108136
Iteration 13/25 | Loss: 0.00108136
Iteration 14/25 | Loss: 0.00108136
Iteration 15/25 | Loss: 0.00108136
Iteration 16/25 | Loss: 0.00108136
Iteration 17/25 | Loss: 0.00108136
Iteration 18/25 | Loss: 0.00108136
Iteration 19/25 | Loss: 0.00108136
Iteration 20/25 | Loss: 0.00108136
Iteration 21/25 | Loss: 0.00108136
Iteration 22/25 | Loss: 0.00108136
Iteration 23/25 | Loss: 0.00108136
Iteration 24/25 | Loss: 0.00108136
Iteration 25/25 | Loss: 0.00108136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42557991
Iteration 2/25 | Loss: 0.00233249
Iteration 3/25 | Loss: 0.00233249
Iteration 4/25 | Loss: 0.00233249
Iteration 5/25 | Loss: 0.00233249
Iteration 6/25 | Loss: 0.00233249
Iteration 7/25 | Loss: 0.00233249
Iteration 8/25 | Loss: 0.00233249
Iteration 9/25 | Loss: 0.00233249
Iteration 10/25 | Loss: 0.00233249
Iteration 11/25 | Loss: 0.00233249
Iteration 12/25 | Loss: 0.00233249
Iteration 13/25 | Loss: 0.00233249
Iteration 14/25 | Loss: 0.00233249
Iteration 15/25 | Loss: 0.00233249
Iteration 16/25 | Loss: 0.00233249
Iteration 17/25 | Loss: 0.00233249
Iteration 18/25 | Loss: 0.00233249
Iteration 19/25 | Loss: 0.00233249
Iteration 20/25 | Loss: 0.00233249
Iteration 21/25 | Loss: 0.00233249
Iteration 22/25 | Loss: 0.00233249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0023324901703745127, 0.0023324901703745127, 0.0023324901703745127, 0.0023324901703745127, 0.0023324901703745127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023324901703745127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233249
Iteration 2/1000 | Loss: 0.00003797
Iteration 3/1000 | Loss: 0.00002363
Iteration 4/1000 | Loss: 0.00001874
Iteration 5/1000 | Loss: 0.00001608
Iteration 6/1000 | Loss: 0.00001468
Iteration 7/1000 | Loss: 0.00001402
Iteration 8/1000 | Loss: 0.00001356
Iteration 9/1000 | Loss: 0.00001322
Iteration 10/1000 | Loss: 0.00001284
Iteration 11/1000 | Loss: 0.00001273
Iteration 12/1000 | Loss: 0.00001254
Iteration 13/1000 | Loss: 0.00001233
Iteration 14/1000 | Loss: 0.00001226
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001217
Iteration 17/1000 | Loss: 0.00001206
Iteration 18/1000 | Loss: 0.00001204
Iteration 19/1000 | Loss: 0.00001201
Iteration 20/1000 | Loss: 0.00001199
Iteration 21/1000 | Loss: 0.00001194
Iteration 22/1000 | Loss: 0.00001193
Iteration 23/1000 | Loss: 0.00001179
Iteration 24/1000 | Loss: 0.00001172
Iteration 25/1000 | Loss: 0.00001168
Iteration 26/1000 | Loss: 0.00001168
Iteration 27/1000 | Loss: 0.00001168
Iteration 28/1000 | Loss: 0.00001167
Iteration 29/1000 | Loss: 0.00001165
Iteration 30/1000 | Loss: 0.00001165
Iteration 31/1000 | Loss: 0.00001165
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001163
Iteration 35/1000 | Loss: 0.00001162
Iteration 36/1000 | Loss: 0.00001162
Iteration 37/1000 | Loss: 0.00001161
Iteration 38/1000 | Loss: 0.00001160
Iteration 39/1000 | Loss: 0.00001160
Iteration 40/1000 | Loss: 0.00001159
Iteration 41/1000 | Loss: 0.00001158
Iteration 42/1000 | Loss: 0.00001158
Iteration 43/1000 | Loss: 0.00001158
Iteration 44/1000 | Loss: 0.00001158
Iteration 45/1000 | Loss: 0.00001158
Iteration 46/1000 | Loss: 0.00001158
Iteration 47/1000 | Loss: 0.00001157
Iteration 48/1000 | Loss: 0.00001157
Iteration 49/1000 | Loss: 0.00001157
Iteration 50/1000 | Loss: 0.00001157
Iteration 51/1000 | Loss: 0.00001157
Iteration 52/1000 | Loss: 0.00001156
Iteration 53/1000 | Loss: 0.00001155
Iteration 54/1000 | Loss: 0.00001155
Iteration 55/1000 | Loss: 0.00001155
Iteration 56/1000 | Loss: 0.00001155
Iteration 57/1000 | Loss: 0.00001155
Iteration 58/1000 | Loss: 0.00001154
Iteration 59/1000 | Loss: 0.00001154
Iteration 60/1000 | Loss: 0.00001154
Iteration 61/1000 | Loss: 0.00001154
Iteration 62/1000 | Loss: 0.00001154
Iteration 63/1000 | Loss: 0.00001153
Iteration 64/1000 | Loss: 0.00001153
Iteration 65/1000 | Loss: 0.00001153
Iteration 66/1000 | Loss: 0.00001153
Iteration 67/1000 | Loss: 0.00001152
Iteration 68/1000 | Loss: 0.00001151
Iteration 69/1000 | Loss: 0.00001151
Iteration 70/1000 | Loss: 0.00001151
Iteration 71/1000 | Loss: 0.00001150
Iteration 72/1000 | Loss: 0.00001149
Iteration 73/1000 | Loss: 0.00001149
Iteration 74/1000 | Loss: 0.00001149
Iteration 75/1000 | Loss: 0.00001149
Iteration 76/1000 | Loss: 0.00001149
Iteration 77/1000 | Loss: 0.00001149
Iteration 78/1000 | Loss: 0.00001149
Iteration 79/1000 | Loss: 0.00001149
Iteration 80/1000 | Loss: 0.00001149
Iteration 81/1000 | Loss: 0.00001149
Iteration 82/1000 | Loss: 0.00001149
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001149
Iteration 85/1000 | Loss: 0.00001149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.1491640179883689e-05, 1.1491640179883689e-05, 1.1491640179883689e-05, 1.1491640179883689e-05, 1.1491640179883689e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1491640179883689e-05

Optimization complete. Final v2v error: 2.9624359607696533 mm

Highest mean error: 3.351156234741211 mm for frame 52

Lowest mean error: 2.537304401397705 mm for frame 263

Saving results

Total time: 42.21917366981506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803171
Iteration 2/25 | Loss: 0.00131121
Iteration 3/25 | Loss: 0.00112342
Iteration 4/25 | Loss: 0.00109989
Iteration 5/25 | Loss: 0.00109623
Iteration 6/25 | Loss: 0.00109582
Iteration 7/25 | Loss: 0.00109582
Iteration 8/25 | Loss: 0.00109582
Iteration 9/25 | Loss: 0.00109582
Iteration 10/25 | Loss: 0.00109582
Iteration 11/25 | Loss: 0.00109582
Iteration 12/25 | Loss: 0.00109582
Iteration 13/25 | Loss: 0.00109582
Iteration 14/25 | Loss: 0.00109582
Iteration 15/25 | Loss: 0.00109582
Iteration 16/25 | Loss: 0.00109582
Iteration 17/25 | Loss: 0.00109582
Iteration 18/25 | Loss: 0.00109582
Iteration 19/25 | Loss: 0.00109582
Iteration 20/25 | Loss: 0.00109582
Iteration 21/25 | Loss: 0.00109582
Iteration 22/25 | Loss: 0.00109582
Iteration 23/25 | Loss: 0.00109582
Iteration 24/25 | Loss: 0.00109582
Iteration 25/25 | Loss: 0.00109582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15267718
Iteration 2/25 | Loss: 0.00211110
Iteration 3/25 | Loss: 0.00211109
Iteration 4/25 | Loss: 0.00211109
Iteration 5/25 | Loss: 0.00211109
Iteration 6/25 | Loss: 0.00211109
Iteration 7/25 | Loss: 0.00211109
Iteration 8/25 | Loss: 0.00211109
Iteration 9/25 | Loss: 0.00211109
Iteration 10/25 | Loss: 0.00211109
Iteration 11/25 | Loss: 0.00211109
Iteration 12/25 | Loss: 0.00211109
Iteration 13/25 | Loss: 0.00211109
Iteration 14/25 | Loss: 0.00211109
Iteration 15/25 | Loss: 0.00211109
Iteration 16/25 | Loss: 0.00211109
Iteration 17/25 | Loss: 0.00211109
Iteration 18/25 | Loss: 0.00211109
Iteration 19/25 | Loss: 0.00211109
Iteration 20/25 | Loss: 0.00211109
Iteration 21/25 | Loss: 0.00211109
Iteration 22/25 | Loss: 0.00211109
Iteration 23/25 | Loss: 0.00211109
Iteration 24/25 | Loss: 0.00211109
Iteration 25/25 | Loss: 0.00211109

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211109
Iteration 2/1000 | Loss: 0.00003197
Iteration 3/1000 | Loss: 0.00002167
Iteration 4/1000 | Loss: 0.00001902
Iteration 5/1000 | Loss: 0.00001773
Iteration 6/1000 | Loss: 0.00001680
Iteration 7/1000 | Loss: 0.00001632
Iteration 8/1000 | Loss: 0.00001600
Iteration 9/1000 | Loss: 0.00001572
Iteration 10/1000 | Loss: 0.00001548
Iteration 11/1000 | Loss: 0.00001529
Iteration 12/1000 | Loss: 0.00001521
Iteration 13/1000 | Loss: 0.00001517
Iteration 14/1000 | Loss: 0.00001516
Iteration 15/1000 | Loss: 0.00001507
Iteration 16/1000 | Loss: 0.00001506
Iteration 17/1000 | Loss: 0.00001503
Iteration 18/1000 | Loss: 0.00001501
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001500
Iteration 21/1000 | Loss: 0.00001499
Iteration 22/1000 | Loss: 0.00001493
Iteration 23/1000 | Loss: 0.00001490
Iteration 24/1000 | Loss: 0.00001489
Iteration 25/1000 | Loss: 0.00001489
Iteration 26/1000 | Loss: 0.00001488
Iteration 27/1000 | Loss: 0.00001487
Iteration 28/1000 | Loss: 0.00001487
Iteration 29/1000 | Loss: 0.00001487
Iteration 30/1000 | Loss: 0.00001484
Iteration 31/1000 | Loss: 0.00001484
Iteration 32/1000 | Loss: 0.00001483
Iteration 33/1000 | Loss: 0.00001477
Iteration 34/1000 | Loss: 0.00001477
Iteration 35/1000 | Loss: 0.00001475
Iteration 36/1000 | Loss: 0.00001474
Iteration 37/1000 | Loss: 0.00001474
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001473
Iteration 41/1000 | Loss: 0.00001473
Iteration 42/1000 | Loss: 0.00001472
Iteration 43/1000 | Loss: 0.00001472
Iteration 44/1000 | Loss: 0.00001472
Iteration 45/1000 | Loss: 0.00001471
Iteration 46/1000 | Loss: 0.00001471
Iteration 47/1000 | Loss: 0.00001471
Iteration 48/1000 | Loss: 0.00001471
Iteration 49/1000 | Loss: 0.00001471
Iteration 50/1000 | Loss: 0.00001471
Iteration 51/1000 | Loss: 0.00001471
Iteration 52/1000 | Loss: 0.00001470
Iteration 53/1000 | Loss: 0.00001470
Iteration 54/1000 | Loss: 0.00001470
Iteration 55/1000 | Loss: 0.00001470
Iteration 56/1000 | Loss: 0.00001470
Iteration 57/1000 | Loss: 0.00001470
Iteration 58/1000 | Loss: 0.00001469
Iteration 59/1000 | Loss: 0.00001469
Iteration 60/1000 | Loss: 0.00001468
Iteration 61/1000 | Loss: 0.00001467
Iteration 62/1000 | Loss: 0.00001467
Iteration 63/1000 | Loss: 0.00001467
Iteration 64/1000 | Loss: 0.00001466
Iteration 65/1000 | Loss: 0.00001466
Iteration 66/1000 | Loss: 0.00001466
Iteration 67/1000 | Loss: 0.00001466
Iteration 68/1000 | Loss: 0.00001465
Iteration 69/1000 | Loss: 0.00001465
Iteration 70/1000 | Loss: 0.00001465
Iteration 71/1000 | Loss: 0.00001465
Iteration 72/1000 | Loss: 0.00001464
Iteration 73/1000 | Loss: 0.00001464
Iteration 74/1000 | Loss: 0.00001464
Iteration 75/1000 | Loss: 0.00001464
Iteration 76/1000 | Loss: 0.00001463
Iteration 77/1000 | Loss: 0.00001463
Iteration 78/1000 | Loss: 0.00001463
Iteration 79/1000 | Loss: 0.00001463
Iteration 80/1000 | Loss: 0.00001463
Iteration 81/1000 | Loss: 0.00001462
Iteration 82/1000 | Loss: 0.00001462
Iteration 83/1000 | Loss: 0.00001462
Iteration 84/1000 | Loss: 0.00001462
Iteration 85/1000 | Loss: 0.00001462
Iteration 86/1000 | Loss: 0.00001461
Iteration 87/1000 | Loss: 0.00001461
Iteration 88/1000 | Loss: 0.00001461
Iteration 89/1000 | Loss: 0.00001461
Iteration 90/1000 | Loss: 0.00001461
Iteration 91/1000 | Loss: 0.00001461
Iteration 92/1000 | Loss: 0.00001461
Iteration 93/1000 | Loss: 0.00001461
Iteration 94/1000 | Loss: 0.00001460
Iteration 95/1000 | Loss: 0.00001460
Iteration 96/1000 | Loss: 0.00001460
Iteration 97/1000 | Loss: 0.00001459
Iteration 98/1000 | Loss: 0.00001459
Iteration 99/1000 | Loss: 0.00001459
Iteration 100/1000 | Loss: 0.00001459
Iteration 101/1000 | Loss: 0.00001459
Iteration 102/1000 | Loss: 0.00001459
Iteration 103/1000 | Loss: 0.00001459
Iteration 104/1000 | Loss: 0.00001459
Iteration 105/1000 | Loss: 0.00001458
Iteration 106/1000 | Loss: 0.00001458
Iteration 107/1000 | Loss: 0.00001458
Iteration 108/1000 | Loss: 0.00001458
Iteration 109/1000 | Loss: 0.00001458
Iteration 110/1000 | Loss: 0.00001458
Iteration 111/1000 | Loss: 0.00001458
Iteration 112/1000 | Loss: 0.00001458
Iteration 113/1000 | Loss: 0.00001458
Iteration 114/1000 | Loss: 0.00001458
Iteration 115/1000 | Loss: 0.00001458
Iteration 116/1000 | Loss: 0.00001458
Iteration 117/1000 | Loss: 0.00001458
Iteration 118/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.4580327842850238e-05, 1.4580327842850238e-05, 1.4580327842850238e-05, 1.4580327842850238e-05, 1.4580327842850238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4580327842850238e-05

Optimization complete. Final v2v error: 3.273702383041382 mm

Highest mean error: 4.723802089691162 mm for frame 177

Lowest mean error: 2.7667744159698486 mm for frame 136

Saving results

Total time: 40.92188358306885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00538578
Iteration 2/25 | Loss: 0.00131793
Iteration 3/25 | Loss: 0.00112059
Iteration 4/25 | Loss: 0.00109593
Iteration 5/25 | Loss: 0.00109022
Iteration 6/25 | Loss: 0.00108804
Iteration 7/25 | Loss: 0.00108470
Iteration 8/25 | Loss: 0.00108654
Iteration 9/25 | Loss: 0.00108372
Iteration 10/25 | Loss: 0.00108317
Iteration 11/25 | Loss: 0.00108288
Iteration 12/25 | Loss: 0.00108287
Iteration 13/25 | Loss: 0.00108286
Iteration 14/25 | Loss: 0.00108286
Iteration 15/25 | Loss: 0.00108286
Iteration 16/25 | Loss: 0.00108286
Iteration 17/25 | Loss: 0.00108286
Iteration 18/25 | Loss: 0.00108285
Iteration 19/25 | Loss: 0.00108285
Iteration 20/25 | Loss: 0.00108396
Iteration 21/25 | Loss: 0.00108306
Iteration 22/25 | Loss: 0.00108283
Iteration 23/25 | Loss: 0.00108283
Iteration 24/25 | Loss: 0.00108283
Iteration 25/25 | Loss: 0.00108282

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.72863293
Iteration 2/25 | Loss: 0.00211607
Iteration 3/25 | Loss: 0.00211433
Iteration 4/25 | Loss: 0.00211433
Iteration 5/25 | Loss: 0.00211433
Iteration 6/25 | Loss: 0.00211433
Iteration 7/25 | Loss: 0.00211432
Iteration 8/25 | Loss: 0.00211432
Iteration 9/25 | Loss: 0.00211432
Iteration 10/25 | Loss: 0.00211432
Iteration 11/25 | Loss: 0.00211432
Iteration 12/25 | Loss: 0.00211432
Iteration 13/25 | Loss: 0.00211432
Iteration 14/25 | Loss: 0.00211432
Iteration 15/25 | Loss: 0.00211432
Iteration 16/25 | Loss: 0.00211432
Iteration 17/25 | Loss: 0.00211432
Iteration 18/25 | Loss: 0.00211432
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021143232006579638, 0.0021143232006579638, 0.0021143232006579638, 0.0021143232006579638, 0.0021143232006579638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021143232006579638

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211432
Iteration 2/1000 | Loss: 0.00003672
Iteration 3/1000 | Loss: 0.00002677
Iteration 4/1000 | Loss: 0.00005078
Iteration 5/1000 | Loss: 0.00001424
Iteration 6/1000 | Loss: 0.00001355
Iteration 7/1000 | Loss: 0.00003626
Iteration 8/1000 | Loss: 0.00002190
Iteration 9/1000 | Loss: 0.00001269
Iteration 10/1000 | Loss: 0.00001247
Iteration 11/1000 | Loss: 0.00001224
Iteration 12/1000 | Loss: 0.00001212
Iteration 13/1000 | Loss: 0.00001198
Iteration 14/1000 | Loss: 0.00001196
Iteration 15/1000 | Loss: 0.00001194
Iteration 16/1000 | Loss: 0.00001194
Iteration 17/1000 | Loss: 0.00001188
Iteration 18/1000 | Loss: 0.00001187
Iteration 19/1000 | Loss: 0.00001186
Iteration 20/1000 | Loss: 0.00001183
Iteration 21/1000 | Loss: 0.00001183
Iteration 22/1000 | Loss: 0.00001183
Iteration 23/1000 | Loss: 0.00001182
Iteration 24/1000 | Loss: 0.00001182
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001181
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001179
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001177
Iteration 32/1000 | Loss: 0.00001176
Iteration 33/1000 | Loss: 0.00001176
Iteration 34/1000 | Loss: 0.00001175
Iteration 35/1000 | Loss: 0.00002239
Iteration 36/1000 | Loss: 0.00001169
Iteration 37/1000 | Loss: 0.00002330
Iteration 38/1000 | Loss: 0.00001162
Iteration 39/1000 | Loss: 0.00001158
Iteration 40/1000 | Loss: 0.00001157
Iteration 41/1000 | Loss: 0.00001157
Iteration 42/1000 | Loss: 0.00001157
Iteration 43/1000 | Loss: 0.00001156
Iteration 44/1000 | Loss: 0.00001156
Iteration 45/1000 | Loss: 0.00001156
Iteration 46/1000 | Loss: 0.00001156
Iteration 47/1000 | Loss: 0.00001156
Iteration 48/1000 | Loss: 0.00001156
Iteration 49/1000 | Loss: 0.00001155
Iteration 50/1000 | Loss: 0.00001155
Iteration 51/1000 | Loss: 0.00001155
Iteration 52/1000 | Loss: 0.00001155
Iteration 53/1000 | Loss: 0.00001154
Iteration 54/1000 | Loss: 0.00001154
Iteration 55/1000 | Loss: 0.00001153
Iteration 56/1000 | Loss: 0.00001153
Iteration 57/1000 | Loss: 0.00001153
Iteration 58/1000 | Loss: 0.00001153
Iteration 59/1000 | Loss: 0.00001152
Iteration 60/1000 | Loss: 0.00001152
Iteration 61/1000 | Loss: 0.00001152
Iteration 62/1000 | Loss: 0.00001152
Iteration 63/1000 | Loss: 0.00001152
Iteration 64/1000 | Loss: 0.00001152
Iteration 65/1000 | Loss: 0.00001152
Iteration 66/1000 | Loss: 0.00001152
Iteration 67/1000 | Loss: 0.00001152
Iteration 68/1000 | Loss: 0.00001152
Iteration 69/1000 | Loss: 0.00001151
Iteration 70/1000 | Loss: 0.00002057
Iteration 71/1000 | Loss: 0.00004521
Iteration 72/1000 | Loss: 0.00005325
Iteration 73/1000 | Loss: 0.00001788
Iteration 74/1000 | Loss: 0.00001171
Iteration 75/1000 | Loss: 0.00001147
Iteration 76/1000 | Loss: 0.00001146
Iteration 77/1000 | Loss: 0.00001143
Iteration 78/1000 | Loss: 0.00001143
Iteration 79/1000 | Loss: 0.00001142
Iteration 80/1000 | Loss: 0.00001142
Iteration 81/1000 | Loss: 0.00001142
Iteration 82/1000 | Loss: 0.00001142
Iteration 83/1000 | Loss: 0.00001142
Iteration 84/1000 | Loss: 0.00001142
Iteration 85/1000 | Loss: 0.00001142
Iteration 86/1000 | Loss: 0.00001142
Iteration 87/1000 | Loss: 0.00001142
Iteration 88/1000 | Loss: 0.00001142
Iteration 89/1000 | Loss: 0.00001141
Iteration 90/1000 | Loss: 0.00001141
Iteration 91/1000 | Loss: 0.00001141
Iteration 92/1000 | Loss: 0.00001141
Iteration 93/1000 | Loss: 0.00001141
Iteration 94/1000 | Loss: 0.00001141
Iteration 95/1000 | Loss: 0.00001141
Iteration 96/1000 | Loss: 0.00001141
Iteration 97/1000 | Loss: 0.00001141
Iteration 98/1000 | Loss: 0.00001141
Iteration 99/1000 | Loss: 0.00001141
Iteration 100/1000 | Loss: 0.00001140
Iteration 101/1000 | Loss: 0.00001140
Iteration 102/1000 | Loss: 0.00001140
Iteration 103/1000 | Loss: 0.00001140
Iteration 104/1000 | Loss: 0.00001140
Iteration 105/1000 | Loss: 0.00001140
Iteration 106/1000 | Loss: 0.00001140
Iteration 107/1000 | Loss: 0.00001139
Iteration 108/1000 | Loss: 0.00001139
Iteration 109/1000 | Loss: 0.00001139
Iteration 110/1000 | Loss: 0.00001139
Iteration 111/1000 | Loss: 0.00001139
Iteration 112/1000 | Loss: 0.00001139
Iteration 113/1000 | Loss: 0.00001139
Iteration 114/1000 | Loss: 0.00001139
Iteration 115/1000 | Loss: 0.00001139
Iteration 116/1000 | Loss: 0.00001139
Iteration 117/1000 | Loss: 0.00001139
Iteration 118/1000 | Loss: 0.00001139
Iteration 119/1000 | Loss: 0.00001139
Iteration 120/1000 | Loss: 0.00001139
Iteration 121/1000 | Loss: 0.00001139
Iteration 122/1000 | Loss: 0.00001139
Iteration 123/1000 | Loss: 0.00001139
Iteration 124/1000 | Loss: 0.00001139
Iteration 125/1000 | Loss: 0.00001139
Iteration 126/1000 | Loss: 0.00001139
Iteration 127/1000 | Loss: 0.00001139
Iteration 128/1000 | Loss: 0.00001139
Iteration 129/1000 | Loss: 0.00001139
Iteration 130/1000 | Loss: 0.00001139
Iteration 131/1000 | Loss: 0.00001139
Iteration 132/1000 | Loss: 0.00001139
Iteration 133/1000 | Loss: 0.00001139
Iteration 134/1000 | Loss: 0.00001139
Iteration 135/1000 | Loss: 0.00001139
Iteration 136/1000 | Loss: 0.00001139
Iteration 137/1000 | Loss: 0.00001139
Iteration 138/1000 | Loss: 0.00001139
Iteration 139/1000 | Loss: 0.00001139
Iteration 140/1000 | Loss: 0.00001139
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001139
Iteration 143/1000 | Loss: 0.00001139
Iteration 144/1000 | Loss: 0.00001139
Iteration 145/1000 | Loss: 0.00001139
Iteration 146/1000 | Loss: 0.00001139
Iteration 147/1000 | Loss: 0.00001139
Iteration 148/1000 | Loss: 0.00001139
Iteration 149/1000 | Loss: 0.00001139
Iteration 150/1000 | Loss: 0.00001139
Iteration 151/1000 | Loss: 0.00001139
Iteration 152/1000 | Loss: 0.00001139
Iteration 153/1000 | Loss: 0.00001139
Iteration 154/1000 | Loss: 0.00001139
Iteration 155/1000 | Loss: 0.00001139
Iteration 156/1000 | Loss: 0.00001139
Iteration 157/1000 | Loss: 0.00001139
Iteration 158/1000 | Loss: 0.00001139
Iteration 159/1000 | Loss: 0.00001139
Iteration 160/1000 | Loss: 0.00001139
Iteration 161/1000 | Loss: 0.00001139
Iteration 162/1000 | Loss: 0.00001139
Iteration 163/1000 | Loss: 0.00001139
Iteration 164/1000 | Loss: 0.00001139
Iteration 165/1000 | Loss: 0.00001139
Iteration 166/1000 | Loss: 0.00001139
Iteration 167/1000 | Loss: 0.00001139
Iteration 168/1000 | Loss: 0.00001139
Iteration 169/1000 | Loss: 0.00001139
Iteration 170/1000 | Loss: 0.00001139
Iteration 171/1000 | Loss: 0.00001139
Iteration 172/1000 | Loss: 0.00001139
Iteration 173/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.1387619451852515e-05, 1.1387619451852515e-05, 1.1387619451852515e-05, 1.1387619451852515e-05, 1.1387619451852515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1387619451852515e-05

Optimization complete. Final v2v error: 2.9428341388702393 mm

Highest mean error: 3.2856876850128174 mm for frame 227

Lowest mean error: 2.621324300765991 mm for frame 105

Saving results

Total time: 71.18523168563843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025992
Iteration 2/25 | Loss: 0.00254484
Iteration 3/25 | Loss: 0.00180321
Iteration 4/25 | Loss: 0.00174568
Iteration 5/25 | Loss: 0.00165956
Iteration 6/25 | Loss: 0.00156791
Iteration 7/25 | Loss: 0.00147945
Iteration 8/25 | Loss: 0.00141816
Iteration 9/25 | Loss: 0.00138386
Iteration 10/25 | Loss: 0.00136590
Iteration 11/25 | Loss: 0.00136023
Iteration 12/25 | Loss: 0.00135922
Iteration 13/25 | Loss: 0.00135509
Iteration 14/25 | Loss: 0.00134587
Iteration 15/25 | Loss: 0.00133993
Iteration 16/25 | Loss: 0.00133869
Iteration 17/25 | Loss: 0.00133417
Iteration 18/25 | Loss: 0.00133397
Iteration 19/25 | Loss: 0.00133095
Iteration 20/25 | Loss: 0.00133208
Iteration 21/25 | Loss: 0.00133152
Iteration 22/25 | Loss: 0.00133343
Iteration 23/25 | Loss: 0.00133253
Iteration 24/25 | Loss: 0.00133259
Iteration 25/25 | Loss: 0.00133305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21577823
Iteration 2/25 | Loss: 0.00380865
Iteration 3/25 | Loss: 0.00335660
Iteration 4/25 | Loss: 0.00335660
Iteration 5/25 | Loss: 0.00335659
Iteration 6/25 | Loss: 0.00335659
Iteration 7/25 | Loss: 0.00335659
Iteration 8/25 | Loss: 0.00335659
Iteration 9/25 | Loss: 0.00335659
Iteration 10/25 | Loss: 0.00335659
Iteration 11/25 | Loss: 0.00335659
Iteration 12/25 | Loss: 0.00335659
Iteration 13/25 | Loss: 0.00335659
Iteration 14/25 | Loss: 0.00335659
Iteration 15/25 | Loss: 0.00335659
Iteration 16/25 | Loss: 0.00335659
Iteration 17/25 | Loss: 0.00335659
Iteration 18/25 | Loss: 0.00335659
Iteration 19/25 | Loss: 0.00335659
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003356589935719967, 0.003356589935719967, 0.003356589935719967, 0.003356589935719967, 0.003356589935719967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003356589935719967

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00335659
Iteration 2/1000 | Loss: 0.00080367
Iteration 3/1000 | Loss: 0.00028267
Iteration 4/1000 | Loss: 0.00028609
Iteration 5/1000 | Loss: 0.00020476
Iteration 6/1000 | Loss: 0.00026621
Iteration 7/1000 | Loss: 0.00020837
Iteration 8/1000 | Loss: 0.00016367
Iteration 9/1000 | Loss: 0.00037973
Iteration 10/1000 | Loss: 0.00029155
Iteration 11/1000 | Loss: 0.00014802
Iteration 12/1000 | Loss: 0.00018616
Iteration 13/1000 | Loss: 0.00014564
Iteration 14/1000 | Loss: 0.00016096
Iteration 15/1000 | Loss: 0.00022454
Iteration 16/1000 | Loss: 0.00018108
Iteration 17/1000 | Loss: 0.00021410
Iteration 18/1000 | Loss: 0.00019676
Iteration 19/1000 | Loss: 0.00021098
Iteration 20/1000 | Loss: 0.00075297
Iteration 21/1000 | Loss: 0.00201779
Iteration 22/1000 | Loss: 0.00261976
Iteration 23/1000 | Loss: 0.00037948
Iteration 24/1000 | Loss: 0.00016576
Iteration 25/1000 | Loss: 0.00013475
Iteration 26/1000 | Loss: 0.00034380
Iteration 27/1000 | Loss: 0.00016558
Iteration 28/1000 | Loss: 0.00026469
Iteration 29/1000 | Loss: 0.00009913
Iteration 30/1000 | Loss: 0.00008613
Iteration 31/1000 | Loss: 0.00022530
Iteration 32/1000 | Loss: 0.00012749
Iteration 33/1000 | Loss: 0.00014995
Iteration 34/1000 | Loss: 0.00012788
Iteration 35/1000 | Loss: 0.00030115
Iteration 36/1000 | Loss: 0.00017118
Iteration 37/1000 | Loss: 0.00024780
Iteration 38/1000 | Loss: 0.00020195
Iteration 39/1000 | Loss: 0.00021949
Iteration 40/1000 | Loss: 0.00017215
Iteration 41/1000 | Loss: 0.00015038
Iteration 42/1000 | Loss: 0.00022836
Iteration 43/1000 | Loss: 0.00008512
Iteration 44/1000 | Loss: 0.00007999
Iteration 45/1000 | Loss: 0.00007710
Iteration 46/1000 | Loss: 0.00023889
Iteration 47/1000 | Loss: 0.00007468
Iteration 48/1000 | Loss: 0.00034927
Iteration 49/1000 | Loss: 0.00016298
Iteration 50/1000 | Loss: 0.00007159
Iteration 51/1000 | Loss: 0.00007023
Iteration 52/1000 | Loss: 0.00042931
Iteration 53/1000 | Loss: 0.00008615
Iteration 54/1000 | Loss: 0.00008057
Iteration 55/1000 | Loss: 0.00006983
Iteration 56/1000 | Loss: 0.00006691
Iteration 57/1000 | Loss: 0.00047499
Iteration 58/1000 | Loss: 0.00011578
Iteration 59/1000 | Loss: 0.00007166
Iteration 60/1000 | Loss: 0.00007223
Iteration 61/1000 | Loss: 0.00006447
Iteration 62/1000 | Loss: 0.00006114
Iteration 63/1000 | Loss: 0.00005977
Iteration 64/1000 | Loss: 0.00005870
Iteration 65/1000 | Loss: 0.00018328
Iteration 66/1000 | Loss: 0.00015490
Iteration 67/1000 | Loss: 0.00008367
Iteration 68/1000 | Loss: 0.00013549
Iteration 69/1000 | Loss: 0.00009616
Iteration 70/1000 | Loss: 0.00006195
Iteration 71/1000 | Loss: 0.00005872
Iteration 72/1000 | Loss: 0.00005674
Iteration 73/1000 | Loss: 0.00017082
Iteration 74/1000 | Loss: 0.00005627
Iteration 75/1000 | Loss: 0.00005481
Iteration 76/1000 | Loss: 0.00005424
Iteration 77/1000 | Loss: 0.00005381
Iteration 78/1000 | Loss: 0.00005339
Iteration 79/1000 | Loss: 0.00005279
Iteration 80/1000 | Loss: 0.00005244
Iteration 81/1000 | Loss: 0.00005231
Iteration 82/1000 | Loss: 0.00005213
Iteration 83/1000 | Loss: 0.00005198
Iteration 84/1000 | Loss: 0.00005194
Iteration 85/1000 | Loss: 0.00005180
Iteration 86/1000 | Loss: 0.00005174
Iteration 87/1000 | Loss: 0.00005168
Iteration 88/1000 | Loss: 0.00005157
Iteration 89/1000 | Loss: 0.00005151
Iteration 90/1000 | Loss: 0.00005151
Iteration 91/1000 | Loss: 0.00005143
Iteration 92/1000 | Loss: 0.00005140
Iteration 93/1000 | Loss: 0.00005139
Iteration 94/1000 | Loss: 0.00005137
Iteration 95/1000 | Loss: 0.00005136
Iteration 96/1000 | Loss: 0.00005134
Iteration 97/1000 | Loss: 0.00005125
Iteration 98/1000 | Loss: 0.00005124
Iteration 99/1000 | Loss: 0.00005117
Iteration 100/1000 | Loss: 0.00012878
Iteration 101/1000 | Loss: 0.00005399
Iteration 102/1000 | Loss: 0.00005203
Iteration 103/1000 | Loss: 0.00005092
Iteration 104/1000 | Loss: 0.00004991
Iteration 105/1000 | Loss: 0.00004946
Iteration 106/1000 | Loss: 0.00004909
Iteration 107/1000 | Loss: 0.00004889
Iteration 108/1000 | Loss: 0.00004876
Iteration 109/1000 | Loss: 0.00004871
Iteration 110/1000 | Loss: 0.00004865
Iteration 111/1000 | Loss: 0.00004864
Iteration 112/1000 | Loss: 0.00004864
Iteration 113/1000 | Loss: 0.00004863
Iteration 114/1000 | Loss: 0.00004862
Iteration 115/1000 | Loss: 0.00004862
Iteration 116/1000 | Loss: 0.00004862
Iteration 117/1000 | Loss: 0.00004859
Iteration 118/1000 | Loss: 0.00004851
Iteration 119/1000 | Loss: 0.00004851
Iteration 120/1000 | Loss: 0.00004848
Iteration 121/1000 | Loss: 0.00004848
Iteration 122/1000 | Loss: 0.00004848
Iteration 123/1000 | Loss: 0.00004847
Iteration 124/1000 | Loss: 0.00004847
Iteration 125/1000 | Loss: 0.00004847
Iteration 126/1000 | Loss: 0.00004847
Iteration 127/1000 | Loss: 0.00004847
Iteration 128/1000 | Loss: 0.00004847
Iteration 129/1000 | Loss: 0.00004846
Iteration 130/1000 | Loss: 0.00004844
Iteration 131/1000 | Loss: 0.00004843
Iteration 132/1000 | Loss: 0.00004843
Iteration 133/1000 | Loss: 0.00004843
Iteration 134/1000 | Loss: 0.00004842
Iteration 135/1000 | Loss: 0.00004842
Iteration 136/1000 | Loss: 0.00004842
Iteration 137/1000 | Loss: 0.00004841
Iteration 138/1000 | Loss: 0.00004841
Iteration 139/1000 | Loss: 0.00004841
Iteration 140/1000 | Loss: 0.00004841
Iteration 141/1000 | Loss: 0.00004841
Iteration 142/1000 | Loss: 0.00004841
Iteration 143/1000 | Loss: 0.00004841
Iteration 144/1000 | Loss: 0.00004841
Iteration 145/1000 | Loss: 0.00004841
Iteration 146/1000 | Loss: 0.00004841
Iteration 147/1000 | Loss: 0.00004840
Iteration 148/1000 | Loss: 0.00004840
Iteration 149/1000 | Loss: 0.00004840
Iteration 150/1000 | Loss: 0.00004840
Iteration 151/1000 | Loss: 0.00004840
Iteration 152/1000 | Loss: 0.00004840
Iteration 153/1000 | Loss: 0.00004840
Iteration 154/1000 | Loss: 0.00004840
Iteration 155/1000 | Loss: 0.00004840
Iteration 156/1000 | Loss: 0.00004840
Iteration 157/1000 | Loss: 0.00004840
Iteration 158/1000 | Loss: 0.00004840
Iteration 159/1000 | Loss: 0.00004839
Iteration 160/1000 | Loss: 0.00004839
Iteration 161/1000 | Loss: 0.00004839
Iteration 162/1000 | Loss: 0.00004839
Iteration 163/1000 | Loss: 0.00004839
Iteration 164/1000 | Loss: 0.00004839
Iteration 165/1000 | Loss: 0.00004839
Iteration 166/1000 | Loss: 0.00004839
Iteration 167/1000 | Loss: 0.00004839
Iteration 168/1000 | Loss: 0.00004839
Iteration 169/1000 | Loss: 0.00004839
Iteration 170/1000 | Loss: 0.00004839
Iteration 171/1000 | Loss: 0.00004839
Iteration 172/1000 | Loss: 0.00004839
Iteration 173/1000 | Loss: 0.00004839
Iteration 174/1000 | Loss: 0.00004839
Iteration 175/1000 | Loss: 0.00004839
Iteration 176/1000 | Loss: 0.00004839
Iteration 177/1000 | Loss: 0.00004839
Iteration 178/1000 | Loss: 0.00004839
Iteration 179/1000 | Loss: 0.00004839
Iteration 180/1000 | Loss: 0.00004839
Iteration 181/1000 | Loss: 0.00004839
Iteration 182/1000 | Loss: 0.00004839
Iteration 183/1000 | Loss: 0.00004839
Iteration 184/1000 | Loss: 0.00004839
Iteration 185/1000 | Loss: 0.00004839
Iteration 186/1000 | Loss: 0.00004839
Iteration 187/1000 | Loss: 0.00004839
Iteration 188/1000 | Loss: 0.00004839
Iteration 189/1000 | Loss: 0.00004839
Iteration 190/1000 | Loss: 0.00004839
Iteration 191/1000 | Loss: 0.00004839
Iteration 192/1000 | Loss: 0.00004839
Iteration 193/1000 | Loss: 0.00004839
Iteration 194/1000 | Loss: 0.00004839
Iteration 195/1000 | Loss: 0.00004839
Iteration 196/1000 | Loss: 0.00004839
Iteration 197/1000 | Loss: 0.00004839
Iteration 198/1000 | Loss: 0.00004839
Iteration 199/1000 | Loss: 0.00004839
Iteration 200/1000 | Loss: 0.00004839
Iteration 201/1000 | Loss: 0.00004839
Iteration 202/1000 | Loss: 0.00004839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [4.839375105802901e-05, 4.839375105802901e-05, 4.839375105802901e-05, 4.839375105802901e-05, 4.839375105802901e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.839375105802901e-05

Optimization complete. Final v2v error: 4.296549320220947 mm

Highest mean error: 23.2863826751709 mm for frame 94

Lowest mean error: 2.990628242492676 mm for frame 4

Saving results

Total time: 219.68680143356323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074456
Iteration 2/25 | Loss: 0.00241538
Iteration 3/25 | Loss: 0.00183470
Iteration 4/25 | Loss: 0.00211514
Iteration 5/25 | Loss: 0.00221058
Iteration 6/25 | Loss: 0.00197125
Iteration 7/25 | Loss: 0.00182856
Iteration 8/25 | Loss: 0.00176635
Iteration 9/25 | Loss: 0.00171638
Iteration 10/25 | Loss: 0.00168158
Iteration 11/25 | Loss: 0.00159462
Iteration 12/25 | Loss: 0.00152551
Iteration 13/25 | Loss: 0.00149877
Iteration 14/25 | Loss: 0.00143875
Iteration 15/25 | Loss: 0.00138835
Iteration 16/25 | Loss: 0.00138262
Iteration 17/25 | Loss: 0.00137946
Iteration 18/25 | Loss: 0.00137072
Iteration 19/25 | Loss: 0.00134731
Iteration 20/25 | Loss: 0.00133610
Iteration 21/25 | Loss: 0.00134377
Iteration 22/25 | Loss: 0.00132276
Iteration 23/25 | Loss: 0.00131270
Iteration 24/25 | Loss: 0.00131422
Iteration 25/25 | Loss: 0.00131120

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35238969
Iteration 2/25 | Loss: 0.00271578
Iteration 3/25 | Loss: 0.00244538
Iteration 4/25 | Loss: 0.00244538
Iteration 5/25 | Loss: 0.00244538
Iteration 6/25 | Loss: 0.00244538
Iteration 7/25 | Loss: 0.00244538
Iteration 8/25 | Loss: 0.00244538
Iteration 9/25 | Loss: 0.00244538
Iteration 10/25 | Loss: 0.00244538
Iteration 11/25 | Loss: 0.00244538
Iteration 12/25 | Loss: 0.00244538
Iteration 13/25 | Loss: 0.00244538
Iteration 14/25 | Loss: 0.00244538
Iteration 15/25 | Loss: 0.00244538
Iteration 16/25 | Loss: 0.00244538
Iteration 17/25 | Loss: 0.00244538
Iteration 18/25 | Loss: 0.00244538
Iteration 19/25 | Loss: 0.00244538
Iteration 20/25 | Loss: 0.00244538
Iteration 21/25 | Loss: 0.00244538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002445379039272666, 0.002445379039272666, 0.002445379039272666, 0.002445379039272666, 0.002445379039272666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002445379039272666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244538
Iteration 2/1000 | Loss: 0.00171783
Iteration 3/1000 | Loss: 0.00092643
Iteration 4/1000 | Loss: 0.00072329
Iteration 5/1000 | Loss: 0.00072030
Iteration 6/1000 | Loss: 0.00077973
Iteration 7/1000 | Loss: 0.00060906
Iteration 8/1000 | Loss: 0.00061625
Iteration 9/1000 | Loss: 0.00046872
Iteration 10/1000 | Loss: 0.00080554
Iteration 11/1000 | Loss: 0.00088609
Iteration 12/1000 | Loss: 0.00079512
Iteration 13/1000 | Loss: 0.00039246
Iteration 14/1000 | Loss: 0.00083674
Iteration 15/1000 | Loss: 0.00057839
Iteration 16/1000 | Loss: 0.00098946
Iteration 17/1000 | Loss: 0.00118391
Iteration 18/1000 | Loss: 0.00068284
Iteration 19/1000 | Loss: 0.00081216
Iteration 20/1000 | Loss: 0.00084881
Iteration 21/1000 | Loss: 0.00104194
Iteration 22/1000 | Loss: 0.00142561
Iteration 23/1000 | Loss: 0.00133755
Iteration 24/1000 | Loss: 0.00094861
Iteration 25/1000 | Loss: 0.00069210
Iteration 26/1000 | Loss: 0.00096696
Iteration 27/1000 | Loss: 0.00087335
Iteration 28/1000 | Loss: 0.00068866
Iteration 29/1000 | Loss: 0.00049045
Iteration 30/1000 | Loss: 0.00106101
Iteration 31/1000 | Loss: 0.00070964
Iteration 32/1000 | Loss: 0.00073057
Iteration 33/1000 | Loss: 0.00106720
Iteration 34/1000 | Loss: 0.00111631
Iteration 35/1000 | Loss: 0.00093349
Iteration 36/1000 | Loss: 0.00096272
Iteration 37/1000 | Loss: 0.00067419
Iteration 38/1000 | Loss: 0.00151522
Iteration 39/1000 | Loss: 0.00142354
Iteration 40/1000 | Loss: 0.00111711
Iteration 41/1000 | Loss: 0.00074655
Iteration 42/1000 | Loss: 0.00109276
Iteration 43/1000 | Loss: 0.00139325
Iteration 44/1000 | Loss: 0.00086986
Iteration 45/1000 | Loss: 0.00110859
Iteration 46/1000 | Loss: 0.00174721
Iteration 47/1000 | Loss: 0.00083150
Iteration 48/1000 | Loss: 0.00107149
Iteration 49/1000 | Loss: 0.00109744
Iteration 50/1000 | Loss: 0.00077052
Iteration 51/1000 | Loss: 0.00070879
Iteration 52/1000 | Loss: 0.00070464
Iteration 53/1000 | Loss: 0.00112781
Iteration 54/1000 | Loss: 0.00226615
Iteration 55/1000 | Loss: 0.00151700
Iteration 56/1000 | Loss: 0.00281881
Iteration 57/1000 | Loss: 0.00135711
Iteration 58/1000 | Loss: 0.00104783
Iteration 59/1000 | Loss: 0.00136102
Iteration 60/1000 | Loss: 0.00179605
Iteration 61/1000 | Loss: 0.00117696
Iteration 62/1000 | Loss: 0.00072784
Iteration 63/1000 | Loss: 0.00060008
Iteration 64/1000 | Loss: 0.00061845
Iteration 65/1000 | Loss: 0.00069875
Iteration 66/1000 | Loss: 0.00051442
Iteration 67/1000 | Loss: 0.00069380
Iteration 68/1000 | Loss: 0.00075363
Iteration 69/1000 | Loss: 0.00026814
Iteration 70/1000 | Loss: 0.00036399
Iteration 71/1000 | Loss: 0.00050457
Iteration 72/1000 | Loss: 0.00086337
Iteration 73/1000 | Loss: 0.00093602
Iteration 74/1000 | Loss: 0.00081872
Iteration 75/1000 | Loss: 0.00069671
Iteration 76/1000 | Loss: 0.00067537
Iteration 77/1000 | Loss: 0.00061409
Iteration 78/1000 | Loss: 0.00061240
Iteration 79/1000 | Loss: 0.00064882
Iteration 80/1000 | Loss: 0.00032338
Iteration 81/1000 | Loss: 0.00030366
Iteration 82/1000 | Loss: 0.00040958
Iteration 83/1000 | Loss: 0.00042301
Iteration 84/1000 | Loss: 0.00039939
Iteration 85/1000 | Loss: 0.00055475
Iteration 86/1000 | Loss: 0.00054268
Iteration 87/1000 | Loss: 0.00044838
Iteration 88/1000 | Loss: 0.00043341
Iteration 89/1000 | Loss: 0.00042607
Iteration 90/1000 | Loss: 0.00042484
Iteration 91/1000 | Loss: 0.00043641
Iteration 92/1000 | Loss: 0.00104683
Iteration 93/1000 | Loss: 0.00071835
Iteration 94/1000 | Loss: 0.00070093
Iteration 95/1000 | Loss: 0.00070105
Iteration 96/1000 | Loss: 0.00080213
Iteration 97/1000 | Loss: 0.00087251
Iteration 98/1000 | Loss: 0.00050069
Iteration 99/1000 | Loss: 0.00070554
Iteration 100/1000 | Loss: 0.00047824
Iteration 101/1000 | Loss: 0.00069778
Iteration 102/1000 | Loss: 0.00069854
Iteration 103/1000 | Loss: 0.00056648
Iteration 104/1000 | Loss: 0.00063158
Iteration 105/1000 | Loss: 0.00062439
Iteration 106/1000 | Loss: 0.00084939
Iteration 107/1000 | Loss: 0.00094970
Iteration 108/1000 | Loss: 0.00039396
Iteration 109/1000 | Loss: 0.00093134
Iteration 110/1000 | Loss: 0.00043475
Iteration 111/1000 | Loss: 0.00052146
Iteration 112/1000 | Loss: 0.00045301
Iteration 113/1000 | Loss: 0.00057189
Iteration 114/1000 | Loss: 0.00034905
Iteration 115/1000 | Loss: 0.00044506
Iteration 116/1000 | Loss: 0.00043586
Iteration 117/1000 | Loss: 0.00059356
Iteration 118/1000 | Loss: 0.00061906
Iteration 119/1000 | Loss: 0.00077362
Iteration 120/1000 | Loss: 0.00046070
Iteration 121/1000 | Loss: 0.00054176
Iteration 122/1000 | Loss: 0.00045790
Iteration 123/1000 | Loss: 0.00049749
Iteration 124/1000 | Loss: 0.00069792
Iteration 125/1000 | Loss: 0.00068718
Iteration 126/1000 | Loss: 0.00063340
Iteration 127/1000 | Loss: 0.00054087
Iteration 128/1000 | Loss: 0.00043053
Iteration 129/1000 | Loss: 0.00055252
Iteration 130/1000 | Loss: 0.00090917
Iteration 131/1000 | Loss: 0.00069153
Iteration 132/1000 | Loss: 0.00059842
Iteration 133/1000 | Loss: 0.00036724
Iteration 134/1000 | Loss: 0.00060998
Iteration 135/1000 | Loss: 0.00045182
Iteration 136/1000 | Loss: 0.00051267
Iteration 137/1000 | Loss: 0.00043890
Iteration 138/1000 | Loss: 0.00056818
Iteration 139/1000 | Loss: 0.00060159
Iteration 140/1000 | Loss: 0.00062419
Iteration 141/1000 | Loss: 0.00057113
Iteration 142/1000 | Loss: 0.00061214
Iteration 143/1000 | Loss: 0.00052194
Iteration 144/1000 | Loss: 0.00047058
Iteration 145/1000 | Loss: 0.00057541
Iteration 146/1000 | Loss: 0.00047589
Iteration 147/1000 | Loss: 0.00057286
Iteration 148/1000 | Loss: 0.00057293
Iteration 149/1000 | Loss: 0.00051871
Iteration 150/1000 | Loss: 0.00056694
Iteration 151/1000 | Loss: 0.00052464
Iteration 152/1000 | Loss: 0.00044256
Iteration 153/1000 | Loss: 0.00035199
Iteration 154/1000 | Loss: 0.00034909
Iteration 155/1000 | Loss: 0.00034891
Iteration 156/1000 | Loss: 0.00028559
Iteration 157/1000 | Loss: 0.00029303
Iteration 158/1000 | Loss: 0.00058528
Iteration 159/1000 | Loss: 0.00050363
Iteration 160/1000 | Loss: 0.00049740
Iteration 161/1000 | Loss: 0.00033122
Iteration 162/1000 | Loss: 0.00041544
Iteration 163/1000 | Loss: 0.00029495
Iteration 164/1000 | Loss: 0.00052273
Iteration 165/1000 | Loss: 0.00043694
Iteration 166/1000 | Loss: 0.00027077
Iteration 167/1000 | Loss: 0.00049607
Iteration 168/1000 | Loss: 0.00029097
Iteration 169/1000 | Loss: 0.00044317
Iteration 170/1000 | Loss: 0.00035749
Iteration 171/1000 | Loss: 0.00046129
Iteration 172/1000 | Loss: 0.00047126
Iteration 173/1000 | Loss: 0.00033448
Iteration 174/1000 | Loss: 0.00037403
Iteration 175/1000 | Loss: 0.00032338
Iteration 176/1000 | Loss: 0.00044591
Iteration 177/1000 | Loss: 0.00034482
Iteration 178/1000 | Loss: 0.00038323
Iteration 179/1000 | Loss: 0.00052253
Iteration 180/1000 | Loss: 0.00068875
Iteration 181/1000 | Loss: 0.00047395
Iteration 182/1000 | Loss: 0.00063166
Iteration 183/1000 | Loss: 0.00059655
Iteration 184/1000 | Loss: 0.00050103
Iteration 185/1000 | Loss: 0.00049466
Iteration 186/1000 | Loss: 0.00052497
Iteration 187/1000 | Loss: 0.00054868
Iteration 188/1000 | Loss: 0.00080278
Iteration 189/1000 | Loss: 0.00041265
Iteration 190/1000 | Loss: 0.00034357
Iteration 191/1000 | Loss: 0.00042081
Iteration 192/1000 | Loss: 0.00074884
Iteration 193/1000 | Loss: 0.00046130
Iteration 194/1000 | Loss: 0.00039210
Iteration 195/1000 | Loss: 0.00044112
Iteration 196/1000 | Loss: 0.00062694
Iteration 197/1000 | Loss: 0.00044406
Iteration 198/1000 | Loss: 0.00040539
Iteration 199/1000 | Loss: 0.00050726
Iteration 200/1000 | Loss: 0.00046431
Iteration 201/1000 | Loss: 0.00048101
Iteration 202/1000 | Loss: 0.00030170
Iteration 203/1000 | Loss: 0.00021059
Iteration 204/1000 | Loss: 0.00030244
Iteration 205/1000 | Loss: 0.00039355
Iteration 206/1000 | Loss: 0.00036196
Iteration 207/1000 | Loss: 0.00036391
Iteration 208/1000 | Loss: 0.00044714
Iteration 209/1000 | Loss: 0.00042447
Iteration 210/1000 | Loss: 0.00033598
Iteration 211/1000 | Loss: 0.00035430
Iteration 212/1000 | Loss: 0.00030858
Iteration 213/1000 | Loss: 0.00034317
Iteration 214/1000 | Loss: 0.00029359
Iteration 215/1000 | Loss: 0.00024707
Iteration 216/1000 | Loss: 0.00032125
Iteration 217/1000 | Loss: 0.00037721
Iteration 218/1000 | Loss: 0.00038134
Iteration 219/1000 | Loss: 0.00039758
Iteration 220/1000 | Loss: 0.00039439
Iteration 221/1000 | Loss: 0.00039995
Iteration 222/1000 | Loss: 0.00041454
Iteration 223/1000 | Loss: 0.00023396
Iteration 224/1000 | Loss: 0.00045366
Iteration 225/1000 | Loss: 0.00031262
Iteration 226/1000 | Loss: 0.00031942
Iteration 227/1000 | Loss: 0.00025353
Iteration 228/1000 | Loss: 0.00039891
Iteration 229/1000 | Loss: 0.00039950
Iteration 230/1000 | Loss: 0.00039427
Iteration 231/1000 | Loss: 0.00039359
Iteration 232/1000 | Loss: 0.00038266
Iteration 233/1000 | Loss: 0.00030944
Iteration 234/1000 | Loss: 0.00019132
Iteration 235/1000 | Loss: 0.00030563
Iteration 236/1000 | Loss: 0.00030843
Iteration 237/1000 | Loss: 0.00024119
Iteration 238/1000 | Loss: 0.00017411
Iteration 239/1000 | Loss: 0.00014299
Iteration 240/1000 | Loss: 0.00025053
Iteration 241/1000 | Loss: 0.00026260
Iteration 242/1000 | Loss: 0.00021475
Iteration 243/1000 | Loss: 0.00018894
Iteration 244/1000 | Loss: 0.00014742
Iteration 245/1000 | Loss: 0.00029880
Iteration 246/1000 | Loss: 0.00019039
Iteration 247/1000 | Loss: 0.00024934
Iteration 248/1000 | Loss: 0.00019829
Iteration 249/1000 | Loss: 0.00020109
Iteration 250/1000 | Loss: 0.00021329
Iteration 251/1000 | Loss: 0.00023336
Iteration 252/1000 | Loss: 0.00048775
Iteration 253/1000 | Loss: 0.00018374
Iteration 254/1000 | Loss: 0.00019162
Iteration 255/1000 | Loss: 0.00022932
Iteration 256/1000 | Loss: 0.00023203
Iteration 257/1000 | Loss: 0.00023387
Iteration 258/1000 | Loss: 0.00025361
Iteration 259/1000 | Loss: 0.00025704
Iteration 260/1000 | Loss: 0.00026543
Iteration 261/1000 | Loss: 0.00026089
Iteration 262/1000 | Loss: 0.00043827
Iteration 263/1000 | Loss: 0.00030343
Iteration 264/1000 | Loss: 0.00017794
Iteration 265/1000 | Loss: 0.00020943
Iteration 266/1000 | Loss: 0.00027493
Iteration 267/1000 | Loss: 0.00023114
Iteration 268/1000 | Loss: 0.00024157
Iteration 269/1000 | Loss: 0.00025008
Iteration 270/1000 | Loss: 0.00021303
Iteration 271/1000 | Loss: 0.00033691
Iteration 272/1000 | Loss: 0.00029110
Iteration 273/1000 | Loss: 0.00026338
Iteration 274/1000 | Loss: 0.00027562
Iteration 275/1000 | Loss: 0.00030480
Iteration 276/1000 | Loss: 0.00029307
Iteration 277/1000 | Loss: 0.00058090
Iteration 278/1000 | Loss: 0.00032081
Iteration 279/1000 | Loss: 0.00033020
Iteration 280/1000 | Loss: 0.00041336
Iteration 281/1000 | Loss: 0.00035264
Iteration 282/1000 | Loss: 0.00057436
Iteration 283/1000 | Loss: 0.00044675
Iteration 284/1000 | Loss: 0.00042836
Iteration 285/1000 | Loss: 0.00036615
Iteration 286/1000 | Loss: 0.00054231
Iteration 287/1000 | Loss: 0.00039019
Iteration 288/1000 | Loss: 0.00043106
Iteration 289/1000 | Loss: 0.00029674
Iteration 290/1000 | Loss: 0.00022818
Iteration 291/1000 | Loss: 0.00030924
Iteration 292/1000 | Loss: 0.00028810
Iteration 293/1000 | Loss: 0.00032360
Iteration 294/1000 | Loss: 0.00029113
Iteration 295/1000 | Loss: 0.00030545
Iteration 296/1000 | Loss: 0.00029667
Iteration 297/1000 | Loss: 0.00029412
Iteration 298/1000 | Loss: 0.00027021
Iteration 299/1000 | Loss: 0.00028185
Iteration 300/1000 | Loss: 0.00025841
Iteration 301/1000 | Loss: 0.00042857
Iteration 302/1000 | Loss: 0.00036713
Iteration 303/1000 | Loss: 0.00048327
Iteration 304/1000 | Loss: 0.00044634
Iteration 305/1000 | Loss: 0.00049992
Iteration 306/1000 | Loss: 0.00072533
Iteration 307/1000 | Loss: 0.00062329
Iteration 308/1000 | Loss: 0.00037611
Iteration 309/1000 | Loss: 0.00039848
Iteration 310/1000 | Loss: 0.00029902
Iteration 311/1000 | Loss: 0.00026192
Iteration 312/1000 | Loss: 0.00027724
Iteration 313/1000 | Loss: 0.00026038
Iteration 314/1000 | Loss: 0.00027822
Iteration 315/1000 | Loss: 0.00027195
Iteration 316/1000 | Loss: 0.00026710
Iteration 317/1000 | Loss: 0.00025839
Iteration 318/1000 | Loss: 0.00026039
Iteration 319/1000 | Loss: 0.00024944
Iteration 320/1000 | Loss: 0.00026493
Iteration 321/1000 | Loss: 0.00018192
Iteration 322/1000 | Loss: 0.00021952
Iteration 323/1000 | Loss: 0.00027564
Iteration 324/1000 | Loss: 0.00026038
Iteration 325/1000 | Loss: 0.00028376
Iteration 326/1000 | Loss: 0.00021300
Iteration 327/1000 | Loss: 0.00025096
Iteration 328/1000 | Loss: 0.00021363
Iteration 329/1000 | Loss: 0.00014591
Iteration 330/1000 | Loss: 0.00017093
Iteration 331/1000 | Loss: 0.00021830
Iteration 332/1000 | Loss: 0.00028990
Iteration 333/1000 | Loss: 0.00025113
Iteration 334/1000 | Loss: 0.00028051
Iteration 335/1000 | Loss: 0.00021521
Iteration 336/1000 | Loss: 0.00019755
Iteration 337/1000 | Loss: 0.00015903
Iteration 338/1000 | Loss: 0.00023715
Iteration 339/1000 | Loss: 0.00024685
Iteration 340/1000 | Loss: 0.00022195
Iteration 341/1000 | Loss: 0.00020308
Iteration 342/1000 | Loss: 0.00018519
Iteration 343/1000 | Loss: 0.00023001
Iteration 344/1000 | Loss: 0.00022768
Iteration 345/1000 | Loss: 0.00023175
Iteration 346/1000 | Loss: 0.00021785
Iteration 347/1000 | Loss: 0.00027198
Iteration 348/1000 | Loss: 0.00045508
Iteration 349/1000 | Loss: 0.00044378
Iteration 350/1000 | Loss: 0.00026232
Iteration 351/1000 | Loss: 0.00027320
Iteration 352/1000 | Loss: 0.00024573
Iteration 353/1000 | Loss: 0.00045087
Iteration 354/1000 | Loss: 0.00047676
Iteration 355/1000 | Loss: 0.00044193
Iteration 356/1000 | Loss: 0.00027735
Iteration 357/1000 | Loss: 0.00025719
Iteration 358/1000 | Loss: 0.00024544
Iteration 359/1000 | Loss: 0.00021978
Iteration 360/1000 | Loss: 0.00025274
Iteration 361/1000 | Loss: 0.00023837
Iteration 362/1000 | Loss: 0.00025092
Iteration 363/1000 | Loss: 0.00024396
Iteration 364/1000 | Loss: 0.00024040
Iteration 365/1000 | Loss: 0.00025552
Iteration 366/1000 | Loss: 0.00024593
Iteration 367/1000 | Loss: 0.00014371
Iteration 368/1000 | Loss: 0.00019367
Iteration 369/1000 | Loss: 0.00021549
Iteration 370/1000 | Loss: 0.00023561
Iteration 371/1000 | Loss: 0.00022753
Iteration 372/1000 | Loss: 0.00022448
Iteration 373/1000 | Loss: 0.00016154
Iteration 374/1000 | Loss: 0.00015078
Iteration 375/1000 | Loss: 0.00012940
Iteration 376/1000 | Loss: 0.00013633
Iteration 377/1000 | Loss: 0.00011785
Iteration 378/1000 | Loss: 0.00010007
Iteration 379/1000 | Loss: 0.00014598
Iteration 380/1000 | Loss: 0.00010354
Iteration 381/1000 | Loss: 0.00013644
Iteration 382/1000 | Loss: 0.00013019
Iteration 383/1000 | Loss: 0.00012238
Iteration 384/1000 | Loss: 0.00011229
Iteration 385/1000 | Loss: 0.00013469
Iteration 386/1000 | Loss: 0.00014146
Iteration 387/1000 | Loss: 0.00011796
Iteration 388/1000 | Loss: 0.00013369
Iteration 389/1000 | Loss: 0.00010775
Iteration 390/1000 | Loss: 0.00013268
Iteration 391/1000 | Loss: 0.00012521
Iteration 392/1000 | Loss: 0.00014435
Iteration 393/1000 | Loss: 0.00015956
Iteration 394/1000 | Loss: 0.00010451
Iteration 395/1000 | Loss: 0.00010676
Iteration 396/1000 | Loss: 0.00014496
Iteration 397/1000 | Loss: 0.00013525
Iteration 398/1000 | Loss: 0.00009479
Iteration 399/1000 | Loss: 0.00011748
Iteration 400/1000 | Loss: 0.00013874
Iteration 401/1000 | Loss: 0.00014142
Iteration 402/1000 | Loss: 0.00013197
Iteration 403/1000 | Loss: 0.00015347
Iteration 404/1000 | Loss: 0.00014295
Iteration 405/1000 | Loss: 0.00012041
Iteration 406/1000 | Loss: 0.00019167
Iteration 407/1000 | Loss: 0.00020453
Iteration 408/1000 | Loss: 0.00013069
Iteration 409/1000 | Loss: 0.00011203
Iteration 410/1000 | Loss: 0.00024829
Iteration 411/1000 | Loss: 0.00052395
Iteration 412/1000 | Loss: 0.00052534
Iteration 413/1000 | Loss: 0.00033644
Iteration 414/1000 | Loss: 0.00013554
Iteration 415/1000 | Loss: 0.00011582
Iteration 416/1000 | Loss: 0.00011844
Iteration 417/1000 | Loss: 0.00012047
Iteration 418/1000 | Loss: 0.00011338
Iteration 419/1000 | Loss: 0.00014447
Iteration 420/1000 | Loss: 0.00011675
Iteration 421/1000 | Loss: 0.00012616
Iteration 422/1000 | Loss: 0.00025350
Iteration 423/1000 | Loss: 0.00017266
Iteration 424/1000 | Loss: 0.00014851
Iteration 425/1000 | Loss: 0.00015637
Iteration 426/1000 | Loss: 0.00014453
Iteration 427/1000 | Loss: 0.00017075
Iteration 428/1000 | Loss: 0.00017098
Iteration 429/1000 | Loss: 0.00022925
Iteration 430/1000 | Loss: 0.00014988
Iteration 431/1000 | Loss: 0.00014143
Iteration 432/1000 | Loss: 0.00013638
Iteration 433/1000 | Loss: 0.00016206
Iteration 434/1000 | Loss: 0.00014370
Iteration 435/1000 | Loss: 0.00046040
Iteration 436/1000 | Loss: 0.00015914
Iteration 437/1000 | Loss: 0.00012622
Iteration 438/1000 | Loss: 0.00011860
Iteration 439/1000 | Loss: 0.00015015
Iteration 440/1000 | Loss: 0.00013692
Iteration 441/1000 | Loss: 0.00013743
Iteration 442/1000 | Loss: 0.00015920
Iteration 443/1000 | Loss: 0.00014363
Iteration 444/1000 | Loss: 0.00014929
Iteration 445/1000 | Loss: 0.00041185
Iteration 446/1000 | Loss: 0.00008837
Iteration 447/1000 | Loss: 0.00018495
Iteration 448/1000 | Loss: 0.00018276
Iteration 449/1000 | Loss: 0.00013922
Iteration 450/1000 | Loss: 0.00011309
Iteration 451/1000 | Loss: 0.00011702
Iteration 452/1000 | Loss: 0.00012240
Iteration 453/1000 | Loss: 0.00012028
Iteration 454/1000 | Loss: 0.00012051
Iteration 455/1000 | Loss: 0.00015698
Iteration 456/1000 | Loss: 0.00011004
Iteration 457/1000 | Loss: 0.00011690
Iteration 458/1000 | Loss: 0.00009237
Iteration 459/1000 | Loss: 0.00009096
Iteration 460/1000 | Loss: 0.00010534
Iteration 461/1000 | Loss: 0.00011037
Iteration 462/1000 | Loss: 0.00009783
Iteration 463/1000 | Loss: 0.00011952
Iteration 464/1000 | Loss: 0.00012132
Iteration 465/1000 | Loss: 0.00012067
Iteration 466/1000 | Loss: 0.00014296
Iteration 467/1000 | Loss: 0.00013105
Iteration 468/1000 | Loss: 0.00013837
Iteration 469/1000 | Loss: 0.00011057
Iteration 470/1000 | Loss: 0.00011337
Iteration 471/1000 | Loss: 0.00025175
Iteration 472/1000 | Loss: 0.00008136
Iteration 473/1000 | Loss: 0.00011661
Iteration 474/1000 | Loss: 0.00014114
Iteration 475/1000 | Loss: 0.00009812
Iteration 476/1000 | Loss: 0.00010817
Iteration 477/1000 | Loss: 0.00012992
Iteration 478/1000 | Loss: 0.00014269
Iteration 479/1000 | Loss: 0.00012740
Iteration 480/1000 | Loss: 0.00014157
Iteration 481/1000 | Loss: 0.00010942
Iteration 482/1000 | Loss: 0.00013712
Iteration 483/1000 | Loss: 0.00013349
Iteration 484/1000 | Loss: 0.00014389
Iteration 485/1000 | Loss: 0.00013228
Iteration 486/1000 | Loss: 0.00014089
Iteration 487/1000 | Loss: 0.00012986
Iteration 488/1000 | Loss: 0.00015755
Iteration 489/1000 | Loss: 0.00014943
Iteration 490/1000 | Loss: 0.00013329
Iteration 491/1000 | Loss: 0.00013918
Iteration 492/1000 | Loss: 0.00016195
Iteration 493/1000 | Loss: 0.00013866
Iteration 494/1000 | Loss: 0.00013465
Iteration 495/1000 | Loss: 0.00012169
Iteration 496/1000 | Loss: 0.00009492
Iteration 497/1000 | Loss: 0.00007241
Iteration 498/1000 | Loss: 0.00011235
Iteration 499/1000 | Loss: 0.00012443
Iteration 500/1000 | Loss: 0.00034296
Iteration 501/1000 | Loss: 0.00013278
Iteration 502/1000 | Loss: 0.00014270
Iteration 503/1000 | Loss: 0.00014742
Iteration 504/1000 | Loss: 0.00027461
Iteration 505/1000 | Loss: 0.00013644
Iteration 506/1000 | Loss: 0.00011836
Iteration 507/1000 | Loss: 0.00009529
Iteration 508/1000 | Loss: 0.00012535
Iteration 509/1000 | Loss: 0.00011126
Iteration 510/1000 | Loss: 0.00010374
Iteration 511/1000 | Loss: 0.00007706
Iteration 512/1000 | Loss: 0.00012400
Iteration 513/1000 | Loss: 0.00012772
Iteration 514/1000 | Loss: 0.00024367
Iteration 515/1000 | Loss: 0.00012719
Iteration 516/1000 | Loss: 0.00013036
Iteration 517/1000 | Loss: 0.00009077
Iteration 518/1000 | Loss: 0.00015060
Iteration 519/1000 | Loss: 0.00008136
Iteration 520/1000 | Loss: 0.00017246
Iteration 521/1000 | Loss: 0.00013198
Iteration 522/1000 | Loss: 0.00011486
Iteration 523/1000 | Loss: 0.00012294
Iteration 524/1000 | Loss: 0.00011063
Iteration 525/1000 | Loss: 0.00011058
Iteration 526/1000 | Loss: 0.00011190
Iteration 527/1000 | Loss: 0.00011941
Iteration 528/1000 | Loss: 0.00012227
Iteration 529/1000 | Loss: 0.00017054
Iteration 530/1000 | Loss: 0.00010086
Iteration 531/1000 | Loss: 0.00010260
Iteration 532/1000 | Loss: 0.00010407
Iteration 533/1000 | Loss: 0.00014629
Iteration 534/1000 | Loss: 0.00012045
Iteration 535/1000 | Loss: 0.00022000
Iteration 536/1000 | Loss: 0.00012129
Iteration 537/1000 | Loss: 0.00014239
Iteration 538/1000 | Loss: 0.00014252
Iteration 539/1000 | Loss: 0.00010236
Iteration 540/1000 | Loss: 0.00010173
Iteration 541/1000 | Loss: 0.00012070
Iteration 542/1000 | Loss: 0.00009578
Iteration 543/1000 | Loss: 0.00019140
Iteration 544/1000 | Loss: 0.00010306
Iteration 545/1000 | Loss: 0.00012843
Iteration 546/1000 | Loss: 0.00014181
Iteration 547/1000 | Loss: 0.00014811
Iteration 548/1000 | Loss: 0.00010784
Iteration 549/1000 | Loss: 0.00009572
Iteration 550/1000 | Loss: 0.00011920
Iteration 551/1000 | Loss: 0.00012705
Iteration 552/1000 | Loss: 0.00012820
Iteration 553/1000 | Loss: 0.00012643
Iteration 554/1000 | Loss: 0.00012499
Iteration 555/1000 | Loss: 0.00012738
Iteration 556/1000 | Loss: 0.00014872
Iteration 557/1000 | Loss: 0.00013846
Iteration 558/1000 | Loss: 0.00011713
Iteration 559/1000 | Loss: 0.00012900
Iteration 560/1000 | Loss: 0.00007046
Iteration 561/1000 | Loss: 0.00011250
Iteration 562/1000 | Loss: 0.00017138
Iteration 563/1000 | Loss: 0.00014250
Iteration 564/1000 | Loss: 0.00011755
Iteration 565/1000 | Loss: 0.00013083
Iteration 566/1000 | Loss: 0.00012131
Iteration 567/1000 | Loss: 0.00014476
Iteration 568/1000 | Loss: 0.00013380
Iteration 569/1000 | Loss: 0.00012025
Iteration 570/1000 | Loss: 0.00012767
Iteration 571/1000 | Loss: 0.00010894
Iteration 572/1000 | Loss: 0.00012092
Iteration 573/1000 | Loss: 0.00013811
Iteration 574/1000 | Loss: 0.00012364
Iteration 575/1000 | Loss: 0.00012744
Iteration 576/1000 | Loss: 0.00022888
Iteration 577/1000 | Loss: 0.00019659
Iteration 578/1000 | Loss: 0.00030202
Iteration 579/1000 | Loss: 0.00016673
Iteration 580/1000 | Loss: 0.00013022
Iteration 581/1000 | Loss: 0.00010926
Iteration 582/1000 | Loss: 0.00010477
Iteration 583/1000 | Loss: 0.00012672
Iteration 584/1000 | Loss: 0.00013391
Iteration 585/1000 | Loss: 0.00018350
Iteration 586/1000 | Loss: 0.00012671
Iteration 587/1000 | Loss: 0.00015037
Iteration 588/1000 | Loss: 0.00014127
Iteration 589/1000 | Loss: 0.00015070
Iteration 590/1000 | Loss: 0.00014810
Iteration 591/1000 | Loss: 0.00014936
Iteration 592/1000 | Loss: 0.00016745
Iteration 593/1000 | Loss: 0.00012887
Iteration 594/1000 | Loss: 0.00014410
Iteration 595/1000 | Loss: 0.00012139
Iteration 596/1000 | Loss: 0.00013683
Iteration 597/1000 | Loss: 0.00013170
Iteration 598/1000 | Loss: 0.00007682
Iteration 599/1000 | Loss: 0.00011523
Iteration 600/1000 | Loss: 0.00012024
Iteration 601/1000 | Loss: 0.00012764
Iteration 602/1000 | Loss: 0.00012402
Iteration 603/1000 | Loss: 0.00014034
Iteration 604/1000 | Loss: 0.00012280
Iteration 605/1000 | Loss: 0.00011373
Iteration 606/1000 | Loss: 0.00007659
Iteration 607/1000 | Loss: 0.00014005
Iteration 608/1000 | Loss: 0.00013900
Iteration 609/1000 | Loss: 0.00013659
Iteration 610/1000 | Loss: 0.00012566
Iteration 611/1000 | Loss: 0.00014145
Iteration 612/1000 | Loss: 0.00023955
Iteration 613/1000 | Loss: 0.00019888
Iteration 614/1000 | Loss: 0.00017497
Iteration 615/1000 | Loss: 0.00015728
Iteration 616/1000 | Loss: 0.00018786
Iteration 617/1000 | Loss: 0.00018473
Iteration 618/1000 | Loss: 0.00013690
Iteration 619/1000 | Loss: 0.00016590
Iteration 620/1000 | Loss: 0.00011101
Iteration 621/1000 | Loss: 0.00016950
Iteration 622/1000 | Loss: 0.00010110
Iteration 623/1000 | Loss: 0.00012251
Iteration 624/1000 | Loss: 0.00011211
Iteration 625/1000 | Loss: 0.00012516
Iteration 626/1000 | Loss: 0.00010547
Iteration 627/1000 | Loss: 0.00012155
Iteration 628/1000 | Loss: 0.00011633
Iteration 629/1000 | Loss: 0.00012814
Iteration 630/1000 | Loss: 0.00011667
Iteration 631/1000 | Loss: 0.00040025
Iteration 632/1000 | Loss: 0.00016664
Iteration 633/1000 | Loss: 0.00008048
Iteration 634/1000 | Loss: 0.00007570
Iteration 635/1000 | Loss: 0.00004768
Iteration 636/1000 | Loss: 0.00015189
Iteration 637/1000 | Loss: 0.00004568
Iteration 638/1000 | Loss: 0.00006920
Iteration 639/1000 | Loss: 0.00003266
Iteration 640/1000 | Loss: 0.00003777
Iteration 641/1000 | Loss: 0.00003338
Iteration 642/1000 | Loss: 0.00003358
Iteration 643/1000 | Loss: 0.00003743
Iteration 644/1000 | Loss: 0.00002768
Iteration 645/1000 | Loss: 0.00002610
Iteration 646/1000 | Loss: 0.00002424
Iteration 647/1000 | Loss: 0.00003577
Iteration 648/1000 | Loss: 0.00003982
Iteration 649/1000 | Loss: 0.00003633
Iteration 650/1000 | Loss: 0.00003687
Iteration 651/1000 | Loss: 0.00010876
Iteration 652/1000 | Loss: 0.00005540
Iteration 653/1000 | Loss: 0.00004525
Iteration 654/1000 | Loss: 0.00004200
Iteration 655/1000 | Loss: 0.00003515
Iteration 656/1000 | Loss: 0.00003614
Iteration 657/1000 | Loss: 0.00003607
Iteration 658/1000 | Loss: 0.00002603
Iteration 659/1000 | Loss: 0.00019244
Iteration 660/1000 | Loss: 0.00004338
Iteration 661/1000 | Loss: 0.00002478
Iteration 662/1000 | Loss: 0.00007421
Iteration 663/1000 | Loss: 0.00003042
Iteration 664/1000 | Loss: 0.00003240
Iteration 665/1000 | Loss: 0.00002111
Iteration 666/1000 | Loss: 0.00002072
Iteration 667/1000 | Loss: 0.00002020
Iteration 668/1000 | Loss: 0.00001991
Iteration 669/1000 | Loss: 0.00001973
Iteration 670/1000 | Loss: 0.00001957
Iteration 671/1000 | Loss: 0.00001949
Iteration 672/1000 | Loss: 0.00001948
Iteration 673/1000 | Loss: 0.00001947
Iteration 674/1000 | Loss: 0.00001945
Iteration 675/1000 | Loss: 0.00001944
Iteration 676/1000 | Loss: 0.00001943
Iteration 677/1000 | Loss: 0.00001942
Iteration 678/1000 | Loss: 0.00001942
Iteration 679/1000 | Loss: 0.00001941
Iteration 680/1000 | Loss: 0.00001940
Iteration 681/1000 | Loss: 0.00001939
Iteration 682/1000 | Loss: 0.00001937
Iteration 683/1000 | Loss: 0.00001937
Iteration 684/1000 | Loss: 0.00001935
Iteration 685/1000 | Loss: 0.00001935
Iteration 686/1000 | Loss: 0.00001934
Iteration 687/1000 | Loss: 0.00001934
Iteration 688/1000 | Loss: 0.00001934
Iteration 689/1000 | Loss: 0.00001933
Iteration 690/1000 | Loss: 0.00001933
Iteration 691/1000 | Loss: 0.00001933
Iteration 692/1000 | Loss: 0.00001932
Iteration 693/1000 | Loss: 0.00001932
Iteration 694/1000 | Loss: 0.00001932
Iteration 695/1000 | Loss: 0.00001932
Iteration 696/1000 | Loss: 0.00001932
Iteration 697/1000 | Loss: 0.00001931
Iteration 698/1000 | Loss: 0.00001931
Iteration 699/1000 | Loss: 0.00001930
Iteration 700/1000 | Loss: 0.00001930
Iteration 701/1000 | Loss: 0.00001930
Iteration 702/1000 | Loss: 0.00001929
Iteration 703/1000 | Loss: 0.00001929
Iteration 704/1000 | Loss: 0.00001928
Iteration 705/1000 | Loss: 0.00001928
Iteration 706/1000 | Loss: 0.00001928
Iteration 707/1000 | Loss: 0.00001928
Iteration 708/1000 | Loss: 0.00001928
Iteration 709/1000 | Loss: 0.00001928
Iteration 710/1000 | Loss: 0.00001927
Iteration 711/1000 | Loss: 0.00001927
Iteration 712/1000 | Loss: 0.00001927
Iteration 713/1000 | Loss: 0.00001927
Iteration 714/1000 | Loss: 0.00001927
Iteration 715/1000 | Loss: 0.00001927
Iteration 716/1000 | Loss: 0.00001927
Iteration 717/1000 | Loss: 0.00001927
Iteration 718/1000 | Loss: 0.00001927
Iteration 719/1000 | Loss: 0.00001927
Iteration 720/1000 | Loss: 0.00001926
Iteration 721/1000 | Loss: 0.00001926
Iteration 722/1000 | Loss: 0.00001926
Iteration 723/1000 | Loss: 0.00001926
Iteration 724/1000 | Loss: 0.00001926
Iteration 725/1000 | Loss: 0.00001926
Iteration 726/1000 | Loss: 0.00001926
Iteration 727/1000 | Loss: 0.00001926
Iteration 728/1000 | Loss: 0.00001926
Iteration 729/1000 | Loss: 0.00001926
Iteration 730/1000 | Loss: 0.00001926
Iteration 731/1000 | Loss: 0.00001926
Iteration 732/1000 | Loss: 0.00001926
Iteration 733/1000 | Loss: 0.00001926
Iteration 734/1000 | Loss: 0.00001926
Iteration 735/1000 | Loss: 0.00001925
Iteration 736/1000 | Loss: 0.00001925
Iteration 737/1000 | Loss: 0.00001925
Iteration 738/1000 | Loss: 0.00001925
Iteration 739/1000 | Loss: 0.00001925
Iteration 740/1000 | Loss: 0.00001925
Iteration 741/1000 | Loss: 0.00001925
Iteration 742/1000 | Loss: 0.00001925
Iteration 743/1000 | Loss: 0.00001925
Iteration 744/1000 | Loss: 0.00001925
Iteration 745/1000 | Loss: 0.00001925
Iteration 746/1000 | Loss: 0.00001925
Iteration 747/1000 | Loss: 0.00001925
Iteration 748/1000 | Loss: 0.00001925
Iteration 749/1000 | Loss: 0.00001925
Iteration 750/1000 | Loss: 0.00001925
Iteration 751/1000 | Loss: 0.00001925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 751. Stopping optimization.
Last 5 losses: [1.925260767166037e-05, 1.925260767166037e-05, 1.925260767166037e-05, 1.925260767166037e-05, 1.925260767166037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.925260767166037e-05

Optimization complete. Final v2v error: 3.68339467048645 mm

Highest mean error: 4.394871234893799 mm for frame 76

Lowest mean error: 3.2986505031585693 mm for frame 147

Saving results

Total time: 984.0645890235901
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425446
Iteration 2/25 | Loss: 0.00117685
Iteration 3/25 | Loss: 0.00109789
Iteration 4/25 | Loss: 0.00109099
Iteration 5/25 | Loss: 0.00108921
Iteration 6/25 | Loss: 0.00108921
Iteration 7/25 | Loss: 0.00108921
Iteration 8/25 | Loss: 0.00108921
Iteration 9/25 | Loss: 0.00108921
Iteration 10/25 | Loss: 0.00108921
Iteration 11/25 | Loss: 0.00108921
Iteration 12/25 | Loss: 0.00108921
Iteration 13/25 | Loss: 0.00108921
Iteration 14/25 | Loss: 0.00108921
Iteration 15/25 | Loss: 0.00108921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010892085265368223, 0.0010892085265368223, 0.0010892085265368223, 0.0010892085265368223, 0.0010892085265368223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010892085265368223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19897783
Iteration 2/25 | Loss: 0.00204133
Iteration 3/25 | Loss: 0.00204133
Iteration 4/25 | Loss: 0.00204133
Iteration 5/25 | Loss: 0.00204133
Iteration 6/25 | Loss: 0.00204133
Iteration 7/25 | Loss: 0.00204133
Iteration 8/25 | Loss: 0.00204133
Iteration 9/25 | Loss: 0.00204133
Iteration 10/25 | Loss: 0.00204133
Iteration 11/25 | Loss: 0.00204133
Iteration 12/25 | Loss: 0.00204133
Iteration 13/25 | Loss: 0.00204133
Iteration 14/25 | Loss: 0.00204133
Iteration 15/25 | Loss: 0.00204133
Iteration 16/25 | Loss: 0.00204133
Iteration 17/25 | Loss: 0.00204133
Iteration 18/25 | Loss: 0.00204133
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0020413261372596025, 0.0020413261372596025, 0.0020413261372596025, 0.0020413261372596025, 0.0020413261372596025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020413261372596025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204133
Iteration 2/1000 | Loss: 0.00001946
Iteration 3/1000 | Loss: 0.00001489
Iteration 4/1000 | Loss: 0.00001279
Iteration 5/1000 | Loss: 0.00001209
Iteration 6/1000 | Loss: 0.00001152
Iteration 7/1000 | Loss: 0.00001123
Iteration 8/1000 | Loss: 0.00001091
Iteration 9/1000 | Loss: 0.00001069
Iteration 10/1000 | Loss: 0.00001047
Iteration 11/1000 | Loss: 0.00001045
Iteration 12/1000 | Loss: 0.00001033
Iteration 13/1000 | Loss: 0.00001030
Iteration 14/1000 | Loss: 0.00001029
Iteration 15/1000 | Loss: 0.00001029
Iteration 16/1000 | Loss: 0.00001023
Iteration 17/1000 | Loss: 0.00001014
Iteration 18/1000 | Loss: 0.00001013
Iteration 19/1000 | Loss: 0.00001012
Iteration 20/1000 | Loss: 0.00001009
Iteration 21/1000 | Loss: 0.00001008
Iteration 22/1000 | Loss: 0.00001007
Iteration 23/1000 | Loss: 0.00000998
Iteration 24/1000 | Loss: 0.00000995
Iteration 25/1000 | Loss: 0.00000995
Iteration 26/1000 | Loss: 0.00000994
Iteration 27/1000 | Loss: 0.00000993
Iteration 28/1000 | Loss: 0.00000990
Iteration 29/1000 | Loss: 0.00000989
Iteration 30/1000 | Loss: 0.00000987
Iteration 31/1000 | Loss: 0.00000987
Iteration 32/1000 | Loss: 0.00000987
Iteration 33/1000 | Loss: 0.00000986
Iteration 34/1000 | Loss: 0.00000985
Iteration 35/1000 | Loss: 0.00000985
Iteration 36/1000 | Loss: 0.00000983
Iteration 37/1000 | Loss: 0.00000982
Iteration 38/1000 | Loss: 0.00000982
Iteration 39/1000 | Loss: 0.00000981
Iteration 40/1000 | Loss: 0.00000981
Iteration 41/1000 | Loss: 0.00000980
Iteration 42/1000 | Loss: 0.00000980
Iteration 43/1000 | Loss: 0.00000980
Iteration 44/1000 | Loss: 0.00000980
Iteration 45/1000 | Loss: 0.00000980
Iteration 46/1000 | Loss: 0.00000979
Iteration 47/1000 | Loss: 0.00000979
Iteration 48/1000 | Loss: 0.00000979
Iteration 49/1000 | Loss: 0.00000978
Iteration 50/1000 | Loss: 0.00000978
Iteration 51/1000 | Loss: 0.00000978
Iteration 52/1000 | Loss: 0.00000978
Iteration 53/1000 | Loss: 0.00000978
Iteration 54/1000 | Loss: 0.00000978
Iteration 55/1000 | Loss: 0.00000978
Iteration 56/1000 | Loss: 0.00000978
Iteration 57/1000 | Loss: 0.00000978
Iteration 58/1000 | Loss: 0.00000978
Iteration 59/1000 | Loss: 0.00000977
Iteration 60/1000 | Loss: 0.00000977
Iteration 61/1000 | Loss: 0.00000977
Iteration 62/1000 | Loss: 0.00000977
Iteration 63/1000 | Loss: 0.00000977
Iteration 64/1000 | Loss: 0.00000977
Iteration 65/1000 | Loss: 0.00000977
Iteration 66/1000 | Loss: 0.00000977
Iteration 67/1000 | Loss: 0.00000977
Iteration 68/1000 | Loss: 0.00000977
Iteration 69/1000 | Loss: 0.00000977
Iteration 70/1000 | Loss: 0.00000977
Iteration 71/1000 | Loss: 0.00000977
Iteration 72/1000 | Loss: 0.00000976
Iteration 73/1000 | Loss: 0.00000976
Iteration 74/1000 | Loss: 0.00000976
Iteration 75/1000 | Loss: 0.00000976
Iteration 76/1000 | Loss: 0.00000976
Iteration 77/1000 | Loss: 0.00000976
Iteration 78/1000 | Loss: 0.00000976
Iteration 79/1000 | Loss: 0.00000976
Iteration 80/1000 | Loss: 0.00000976
Iteration 81/1000 | Loss: 0.00000976
Iteration 82/1000 | Loss: 0.00000975
Iteration 83/1000 | Loss: 0.00000975
Iteration 84/1000 | Loss: 0.00000975
Iteration 85/1000 | Loss: 0.00000975
Iteration 86/1000 | Loss: 0.00000975
Iteration 87/1000 | Loss: 0.00000975
Iteration 88/1000 | Loss: 0.00000975
Iteration 89/1000 | Loss: 0.00000975
Iteration 90/1000 | Loss: 0.00000974
Iteration 91/1000 | Loss: 0.00000974
Iteration 92/1000 | Loss: 0.00000974
Iteration 93/1000 | Loss: 0.00000974
Iteration 94/1000 | Loss: 0.00000974
Iteration 95/1000 | Loss: 0.00000973
Iteration 96/1000 | Loss: 0.00000973
Iteration 97/1000 | Loss: 0.00000972
Iteration 98/1000 | Loss: 0.00000972
Iteration 99/1000 | Loss: 0.00000972
Iteration 100/1000 | Loss: 0.00000972
Iteration 101/1000 | Loss: 0.00000972
Iteration 102/1000 | Loss: 0.00000972
Iteration 103/1000 | Loss: 0.00000971
Iteration 104/1000 | Loss: 0.00000971
Iteration 105/1000 | Loss: 0.00000971
Iteration 106/1000 | Loss: 0.00000971
Iteration 107/1000 | Loss: 0.00000971
Iteration 108/1000 | Loss: 0.00000971
Iteration 109/1000 | Loss: 0.00000971
Iteration 110/1000 | Loss: 0.00000971
Iteration 111/1000 | Loss: 0.00000971
Iteration 112/1000 | Loss: 0.00000971
Iteration 113/1000 | Loss: 0.00000970
Iteration 114/1000 | Loss: 0.00000970
Iteration 115/1000 | Loss: 0.00000970
Iteration 116/1000 | Loss: 0.00000970
Iteration 117/1000 | Loss: 0.00000969
Iteration 118/1000 | Loss: 0.00000969
Iteration 119/1000 | Loss: 0.00000969
Iteration 120/1000 | Loss: 0.00000969
Iteration 121/1000 | Loss: 0.00000969
Iteration 122/1000 | Loss: 0.00000969
Iteration 123/1000 | Loss: 0.00000969
Iteration 124/1000 | Loss: 0.00000969
Iteration 125/1000 | Loss: 0.00000969
Iteration 126/1000 | Loss: 0.00000968
Iteration 127/1000 | Loss: 0.00000968
Iteration 128/1000 | Loss: 0.00000968
Iteration 129/1000 | Loss: 0.00000968
Iteration 130/1000 | Loss: 0.00000968
Iteration 131/1000 | Loss: 0.00000968
Iteration 132/1000 | Loss: 0.00000968
Iteration 133/1000 | Loss: 0.00000968
Iteration 134/1000 | Loss: 0.00000968
Iteration 135/1000 | Loss: 0.00000968
Iteration 136/1000 | Loss: 0.00000968
Iteration 137/1000 | Loss: 0.00000968
Iteration 138/1000 | Loss: 0.00000968
Iteration 139/1000 | Loss: 0.00000968
Iteration 140/1000 | Loss: 0.00000968
Iteration 141/1000 | Loss: 0.00000968
Iteration 142/1000 | Loss: 0.00000967
Iteration 143/1000 | Loss: 0.00000967
Iteration 144/1000 | Loss: 0.00000967
Iteration 145/1000 | Loss: 0.00000967
Iteration 146/1000 | Loss: 0.00000967
Iteration 147/1000 | Loss: 0.00000967
Iteration 148/1000 | Loss: 0.00000967
Iteration 149/1000 | Loss: 0.00000967
Iteration 150/1000 | Loss: 0.00000967
Iteration 151/1000 | Loss: 0.00000967
Iteration 152/1000 | Loss: 0.00000967
Iteration 153/1000 | Loss: 0.00000966
Iteration 154/1000 | Loss: 0.00000966
Iteration 155/1000 | Loss: 0.00000966
Iteration 156/1000 | Loss: 0.00000966
Iteration 157/1000 | Loss: 0.00000966
Iteration 158/1000 | Loss: 0.00000966
Iteration 159/1000 | Loss: 0.00000966
Iteration 160/1000 | Loss: 0.00000965
Iteration 161/1000 | Loss: 0.00000965
Iteration 162/1000 | Loss: 0.00000965
Iteration 163/1000 | Loss: 0.00000965
Iteration 164/1000 | Loss: 0.00000965
Iteration 165/1000 | Loss: 0.00000965
Iteration 166/1000 | Loss: 0.00000965
Iteration 167/1000 | Loss: 0.00000965
Iteration 168/1000 | Loss: 0.00000965
Iteration 169/1000 | Loss: 0.00000965
Iteration 170/1000 | Loss: 0.00000965
Iteration 171/1000 | Loss: 0.00000965
Iteration 172/1000 | Loss: 0.00000965
Iteration 173/1000 | Loss: 0.00000965
Iteration 174/1000 | Loss: 0.00000965
Iteration 175/1000 | Loss: 0.00000965
Iteration 176/1000 | Loss: 0.00000965
Iteration 177/1000 | Loss: 0.00000965
Iteration 178/1000 | Loss: 0.00000965
Iteration 179/1000 | Loss: 0.00000965
Iteration 180/1000 | Loss: 0.00000965
Iteration 181/1000 | Loss: 0.00000965
Iteration 182/1000 | Loss: 0.00000965
Iteration 183/1000 | Loss: 0.00000965
Iteration 184/1000 | Loss: 0.00000965
Iteration 185/1000 | Loss: 0.00000965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [9.64744140219409e-06, 9.64744140219409e-06, 9.64744140219409e-06, 9.64744140219409e-06, 9.64744140219409e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.64744140219409e-06

Optimization complete. Final v2v error: 2.706683874130249 mm

Highest mean error: 2.912017583847046 mm for frame 38

Lowest mean error: 2.43304443359375 mm for frame 4

Saving results

Total time: 41.39100408554077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00339553
Iteration 2/25 | Loss: 0.00125593
Iteration 3/25 | Loss: 0.00110946
Iteration 4/25 | Loss: 0.00109424
Iteration 5/25 | Loss: 0.00108959
Iteration 6/25 | Loss: 0.00108810
Iteration 7/25 | Loss: 0.00108746
Iteration 8/25 | Loss: 0.00108678
Iteration 9/25 | Loss: 0.00108670
Iteration 10/25 | Loss: 0.00108670
Iteration 11/25 | Loss: 0.00108670
Iteration 12/25 | Loss: 0.00108670
Iteration 13/25 | Loss: 0.00108670
Iteration 14/25 | Loss: 0.00108670
Iteration 15/25 | Loss: 0.00108670
Iteration 16/25 | Loss: 0.00108670
Iteration 17/25 | Loss: 0.00108670
Iteration 18/25 | Loss: 0.00108670
Iteration 19/25 | Loss: 0.00108670
Iteration 20/25 | Loss: 0.00108670
Iteration 21/25 | Loss: 0.00108670
Iteration 22/25 | Loss: 0.00108670
Iteration 23/25 | Loss: 0.00108670
Iteration 24/25 | Loss: 0.00108670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010867042001336813, 0.0010867042001336813, 0.0010867042001336813, 0.0010867042001336813, 0.0010867042001336813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010867042001336813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22416234
Iteration 2/25 | Loss: 0.00340921
Iteration 3/25 | Loss: 0.00340920
Iteration 4/25 | Loss: 0.00340920
Iteration 5/25 | Loss: 0.00340920
Iteration 6/25 | Loss: 0.00340920
Iteration 7/25 | Loss: 0.00340920
Iteration 8/25 | Loss: 0.00340920
Iteration 9/25 | Loss: 0.00340920
Iteration 10/25 | Loss: 0.00340920
Iteration 11/25 | Loss: 0.00340920
Iteration 12/25 | Loss: 0.00340920
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0034092010464519262, 0.0034092010464519262, 0.0034092010464519262, 0.0034092010464519262, 0.0034092010464519262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034092010464519262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00340920
Iteration 2/1000 | Loss: 0.00004919
Iteration 3/1000 | Loss: 0.00002651
Iteration 4/1000 | Loss: 0.00002045
Iteration 5/1000 | Loss: 0.00001752
Iteration 6/1000 | Loss: 0.00001622
Iteration 7/1000 | Loss: 0.00001503
Iteration 8/1000 | Loss: 0.00001446
Iteration 9/1000 | Loss: 0.00001399
Iteration 10/1000 | Loss: 0.00001367
Iteration 11/1000 | Loss: 0.00001340
Iteration 12/1000 | Loss: 0.00001328
Iteration 13/1000 | Loss: 0.00001322
Iteration 14/1000 | Loss: 0.00001303
Iteration 15/1000 | Loss: 0.00001296
Iteration 16/1000 | Loss: 0.00001286
Iteration 17/1000 | Loss: 0.00001284
Iteration 18/1000 | Loss: 0.00001283
Iteration 19/1000 | Loss: 0.00001282
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001281
Iteration 22/1000 | Loss: 0.00001281
Iteration 23/1000 | Loss: 0.00001280
Iteration 24/1000 | Loss: 0.00001279
Iteration 25/1000 | Loss: 0.00001278
Iteration 26/1000 | Loss: 0.00001277
Iteration 27/1000 | Loss: 0.00001277
Iteration 28/1000 | Loss: 0.00001277
Iteration 29/1000 | Loss: 0.00001276
Iteration 30/1000 | Loss: 0.00001276
Iteration 31/1000 | Loss: 0.00001276
Iteration 32/1000 | Loss: 0.00001275
Iteration 33/1000 | Loss: 0.00001275
Iteration 34/1000 | Loss: 0.00001275
Iteration 35/1000 | Loss: 0.00001274
Iteration 36/1000 | Loss: 0.00001273
Iteration 37/1000 | Loss: 0.00001273
Iteration 38/1000 | Loss: 0.00001273
Iteration 39/1000 | Loss: 0.00001272
Iteration 40/1000 | Loss: 0.00001272
Iteration 41/1000 | Loss: 0.00001272
Iteration 42/1000 | Loss: 0.00001271
Iteration 43/1000 | Loss: 0.00001271
Iteration 44/1000 | Loss: 0.00001270
Iteration 45/1000 | Loss: 0.00001270
Iteration 46/1000 | Loss: 0.00001270
Iteration 47/1000 | Loss: 0.00001269
Iteration 48/1000 | Loss: 0.00001269
Iteration 49/1000 | Loss: 0.00001269
Iteration 50/1000 | Loss: 0.00001268
Iteration 51/1000 | Loss: 0.00001268
Iteration 52/1000 | Loss: 0.00001267
Iteration 53/1000 | Loss: 0.00001267
Iteration 54/1000 | Loss: 0.00001267
Iteration 55/1000 | Loss: 0.00001267
Iteration 56/1000 | Loss: 0.00001266
Iteration 57/1000 | Loss: 0.00001266
Iteration 58/1000 | Loss: 0.00001265
Iteration 59/1000 | Loss: 0.00001265
Iteration 60/1000 | Loss: 0.00001265
Iteration 61/1000 | Loss: 0.00001264
Iteration 62/1000 | Loss: 0.00001264
Iteration 63/1000 | Loss: 0.00001264
Iteration 64/1000 | Loss: 0.00001263
Iteration 65/1000 | Loss: 0.00001263
Iteration 66/1000 | Loss: 0.00001263
Iteration 67/1000 | Loss: 0.00001263
Iteration 68/1000 | Loss: 0.00001263
Iteration 69/1000 | Loss: 0.00001262
Iteration 70/1000 | Loss: 0.00001262
Iteration 71/1000 | Loss: 0.00001262
Iteration 72/1000 | Loss: 0.00001262
Iteration 73/1000 | Loss: 0.00001262
Iteration 74/1000 | Loss: 0.00001262
Iteration 75/1000 | Loss: 0.00001262
Iteration 76/1000 | Loss: 0.00001261
Iteration 77/1000 | Loss: 0.00001261
Iteration 78/1000 | Loss: 0.00001261
Iteration 79/1000 | Loss: 0.00001261
Iteration 80/1000 | Loss: 0.00001261
Iteration 81/1000 | Loss: 0.00001260
Iteration 82/1000 | Loss: 0.00001260
Iteration 83/1000 | Loss: 0.00001260
Iteration 84/1000 | Loss: 0.00001259
Iteration 85/1000 | Loss: 0.00001259
Iteration 86/1000 | Loss: 0.00001259
Iteration 87/1000 | Loss: 0.00001258
Iteration 88/1000 | Loss: 0.00001258
Iteration 89/1000 | Loss: 0.00001258
Iteration 90/1000 | Loss: 0.00001258
Iteration 91/1000 | Loss: 0.00001258
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001257
Iteration 94/1000 | Loss: 0.00001257
Iteration 95/1000 | Loss: 0.00001256
Iteration 96/1000 | Loss: 0.00001256
Iteration 97/1000 | Loss: 0.00001256
Iteration 98/1000 | Loss: 0.00001256
Iteration 99/1000 | Loss: 0.00001256
Iteration 100/1000 | Loss: 0.00001256
Iteration 101/1000 | Loss: 0.00001255
Iteration 102/1000 | Loss: 0.00001255
Iteration 103/1000 | Loss: 0.00001255
Iteration 104/1000 | Loss: 0.00001255
Iteration 105/1000 | Loss: 0.00001255
Iteration 106/1000 | Loss: 0.00001255
Iteration 107/1000 | Loss: 0.00001254
Iteration 108/1000 | Loss: 0.00001254
Iteration 109/1000 | Loss: 0.00001254
Iteration 110/1000 | Loss: 0.00001254
Iteration 111/1000 | Loss: 0.00001254
Iteration 112/1000 | Loss: 0.00001254
Iteration 113/1000 | Loss: 0.00001254
Iteration 114/1000 | Loss: 0.00001254
Iteration 115/1000 | Loss: 0.00001254
Iteration 116/1000 | Loss: 0.00001253
Iteration 117/1000 | Loss: 0.00001253
Iteration 118/1000 | Loss: 0.00001253
Iteration 119/1000 | Loss: 0.00001252
Iteration 120/1000 | Loss: 0.00001252
Iteration 121/1000 | Loss: 0.00001252
Iteration 122/1000 | Loss: 0.00001252
Iteration 123/1000 | Loss: 0.00001251
Iteration 124/1000 | Loss: 0.00001251
Iteration 125/1000 | Loss: 0.00001251
Iteration 126/1000 | Loss: 0.00001251
Iteration 127/1000 | Loss: 0.00001251
Iteration 128/1000 | Loss: 0.00001250
Iteration 129/1000 | Loss: 0.00001250
Iteration 130/1000 | Loss: 0.00001250
Iteration 131/1000 | Loss: 0.00001250
Iteration 132/1000 | Loss: 0.00001250
Iteration 133/1000 | Loss: 0.00001250
Iteration 134/1000 | Loss: 0.00001250
Iteration 135/1000 | Loss: 0.00001249
Iteration 136/1000 | Loss: 0.00001249
Iteration 137/1000 | Loss: 0.00001249
Iteration 138/1000 | Loss: 0.00001249
Iteration 139/1000 | Loss: 0.00001248
Iteration 140/1000 | Loss: 0.00001248
Iteration 141/1000 | Loss: 0.00001248
Iteration 142/1000 | Loss: 0.00001248
Iteration 143/1000 | Loss: 0.00001248
Iteration 144/1000 | Loss: 0.00001248
Iteration 145/1000 | Loss: 0.00001248
Iteration 146/1000 | Loss: 0.00001248
Iteration 147/1000 | Loss: 0.00001248
Iteration 148/1000 | Loss: 0.00001247
Iteration 149/1000 | Loss: 0.00001247
Iteration 150/1000 | Loss: 0.00001247
Iteration 151/1000 | Loss: 0.00001247
Iteration 152/1000 | Loss: 0.00001247
Iteration 153/1000 | Loss: 0.00001247
Iteration 154/1000 | Loss: 0.00001247
Iteration 155/1000 | Loss: 0.00001247
Iteration 156/1000 | Loss: 0.00001247
Iteration 157/1000 | Loss: 0.00001247
Iteration 158/1000 | Loss: 0.00001246
Iteration 159/1000 | Loss: 0.00001246
Iteration 160/1000 | Loss: 0.00001246
Iteration 161/1000 | Loss: 0.00001246
Iteration 162/1000 | Loss: 0.00001245
Iteration 163/1000 | Loss: 0.00001245
Iteration 164/1000 | Loss: 0.00001245
Iteration 165/1000 | Loss: 0.00001245
Iteration 166/1000 | Loss: 0.00001245
Iteration 167/1000 | Loss: 0.00001244
Iteration 168/1000 | Loss: 0.00001244
Iteration 169/1000 | Loss: 0.00001244
Iteration 170/1000 | Loss: 0.00001243
Iteration 171/1000 | Loss: 0.00001243
Iteration 172/1000 | Loss: 0.00001243
Iteration 173/1000 | Loss: 0.00001243
Iteration 174/1000 | Loss: 0.00001243
Iteration 175/1000 | Loss: 0.00001243
Iteration 176/1000 | Loss: 0.00001242
Iteration 177/1000 | Loss: 0.00001242
Iteration 178/1000 | Loss: 0.00001242
Iteration 179/1000 | Loss: 0.00001242
Iteration 180/1000 | Loss: 0.00001242
Iteration 181/1000 | Loss: 0.00001242
Iteration 182/1000 | Loss: 0.00001242
Iteration 183/1000 | Loss: 0.00001242
Iteration 184/1000 | Loss: 0.00001242
Iteration 185/1000 | Loss: 0.00001242
Iteration 186/1000 | Loss: 0.00001242
Iteration 187/1000 | Loss: 0.00001242
Iteration 188/1000 | Loss: 0.00001241
Iteration 189/1000 | Loss: 0.00001241
Iteration 190/1000 | Loss: 0.00001241
Iteration 191/1000 | Loss: 0.00001241
Iteration 192/1000 | Loss: 0.00001241
Iteration 193/1000 | Loss: 0.00001241
Iteration 194/1000 | Loss: 0.00001241
Iteration 195/1000 | Loss: 0.00001241
Iteration 196/1000 | Loss: 0.00001241
Iteration 197/1000 | Loss: 0.00001240
Iteration 198/1000 | Loss: 0.00001240
Iteration 199/1000 | Loss: 0.00001240
Iteration 200/1000 | Loss: 0.00001240
Iteration 201/1000 | Loss: 0.00001240
Iteration 202/1000 | Loss: 0.00001240
Iteration 203/1000 | Loss: 0.00001240
Iteration 204/1000 | Loss: 0.00001240
Iteration 205/1000 | Loss: 0.00001240
Iteration 206/1000 | Loss: 0.00001240
Iteration 207/1000 | Loss: 0.00001240
Iteration 208/1000 | Loss: 0.00001240
Iteration 209/1000 | Loss: 0.00001240
Iteration 210/1000 | Loss: 0.00001240
Iteration 211/1000 | Loss: 0.00001240
Iteration 212/1000 | Loss: 0.00001240
Iteration 213/1000 | Loss: 0.00001240
Iteration 214/1000 | Loss: 0.00001240
Iteration 215/1000 | Loss: 0.00001239
Iteration 216/1000 | Loss: 0.00001239
Iteration 217/1000 | Loss: 0.00001239
Iteration 218/1000 | Loss: 0.00001239
Iteration 219/1000 | Loss: 0.00001239
Iteration 220/1000 | Loss: 0.00001239
Iteration 221/1000 | Loss: 0.00001239
Iteration 222/1000 | Loss: 0.00001239
Iteration 223/1000 | Loss: 0.00001239
Iteration 224/1000 | Loss: 0.00001239
Iteration 225/1000 | Loss: 0.00001239
Iteration 226/1000 | Loss: 0.00001239
Iteration 227/1000 | Loss: 0.00001239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.2391913514875341e-05, 1.2391913514875341e-05, 1.2391913514875341e-05, 1.2391913514875341e-05, 1.2391913514875341e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2391913514875341e-05

Optimization complete. Final v2v error: 3.0824193954467773 mm

Highest mean error: 3.417222023010254 mm for frame 33

Lowest mean error: 2.897815465927124 mm for frame 52

Saving results

Total time: 47.29381275177002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00529964
Iteration 2/25 | Loss: 0.00139161
Iteration 3/25 | Loss: 0.00120586
Iteration 4/25 | Loss: 0.00118297
Iteration 5/25 | Loss: 0.00117782
Iteration 6/25 | Loss: 0.00116436
Iteration 7/25 | Loss: 0.00116314
Iteration 8/25 | Loss: 0.00116068
Iteration 9/25 | Loss: 0.00115476
Iteration 10/25 | Loss: 0.00114783
Iteration 11/25 | Loss: 0.00114507
Iteration 12/25 | Loss: 0.00114198
Iteration 13/25 | Loss: 0.00113955
Iteration 14/25 | Loss: 0.00113613
Iteration 15/25 | Loss: 0.00113493
Iteration 16/25 | Loss: 0.00113842
Iteration 17/25 | Loss: 0.00113637
Iteration 18/25 | Loss: 0.00113831
Iteration 19/25 | Loss: 0.00113478
Iteration 20/25 | Loss: 0.00113328
Iteration 21/25 | Loss: 0.00113185
Iteration 22/25 | Loss: 0.00113155
Iteration 23/25 | Loss: 0.00113126
Iteration 24/25 | Loss: 0.00113110
Iteration 25/25 | Loss: 0.00113086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25029266
Iteration 2/25 | Loss: 0.00247802
Iteration 3/25 | Loss: 0.00247802
Iteration 4/25 | Loss: 0.00247802
Iteration 5/25 | Loss: 0.00247801
Iteration 6/25 | Loss: 0.00247801
Iteration 7/25 | Loss: 0.00247801
Iteration 8/25 | Loss: 0.00247801
Iteration 9/25 | Loss: 0.00247801
Iteration 10/25 | Loss: 0.00247801
Iteration 11/25 | Loss: 0.00247801
Iteration 12/25 | Loss: 0.00247801
Iteration 13/25 | Loss: 0.00247801
Iteration 14/25 | Loss: 0.00247801
Iteration 15/25 | Loss: 0.00247801
Iteration 16/25 | Loss: 0.00247801
Iteration 17/25 | Loss: 0.00247801
Iteration 18/25 | Loss: 0.00247801
Iteration 19/25 | Loss: 0.00247801
Iteration 20/25 | Loss: 0.00247801
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0024780111853033304, 0.0024780111853033304, 0.0024780111853033304, 0.0024780111853033304, 0.0024780111853033304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024780111853033304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00247801
Iteration 2/1000 | Loss: 0.00004055
Iteration 3/1000 | Loss: 0.00002306
Iteration 4/1000 | Loss: 0.00001991
Iteration 5/1000 | Loss: 0.00001817
Iteration 6/1000 | Loss: 0.00001714
Iteration 7/1000 | Loss: 0.00001642
Iteration 8/1000 | Loss: 0.00001592
Iteration 9/1000 | Loss: 0.00001561
Iteration 10/1000 | Loss: 0.00001532
Iteration 11/1000 | Loss: 0.00001494
Iteration 12/1000 | Loss: 0.00001466
Iteration 13/1000 | Loss: 0.00001458
Iteration 14/1000 | Loss: 0.00001447
Iteration 15/1000 | Loss: 0.00001432
Iteration 16/1000 | Loss: 0.00001417
Iteration 17/1000 | Loss: 0.00001407
Iteration 18/1000 | Loss: 0.00001406
Iteration 19/1000 | Loss: 0.00001405
Iteration 20/1000 | Loss: 0.00001398
Iteration 21/1000 | Loss: 0.00001398
Iteration 22/1000 | Loss: 0.00001397
Iteration 23/1000 | Loss: 0.00001396
Iteration 24/1000 | Loss: 0.00001396
Iteration 25/1000 | Loss: 0.00001395
Iteration 26/1000 | Loss: 0.00001394
Iteration 27/1000 | Loss: 0.00001394
Iteration 28/1000 | Loss: 0.00001394
Iteration 29/1000 | Loss: 0.00001393
Iteration 30/1000 | Loss: 0.00001393
Iteration 31/1000 | Loss: 0.00001393
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001393
Iteration 34/1000 | Loss: 0.00001393
Iteration 35/1000 | Loss: 0.00001393
Iteration 36/1000 | Loss: 0.00001393
Iteration 37/1000 | Loss: 0.00001393
Iteration 38/1000 | Loss: 0.00001393
Iteration 39/1000 | Loss: 0.00001392
Iteration 40/1000 | Loss: 0.00001392
Iteration 41/1000 | Loss: 0.00001392
Iteration 42/1000 | Loss: 0.00001392
Iteration 43/1000 | Loss: 0.00001392
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001391
Iteration 46/1000 | Loss: 0.00001391
Iteration 47/1000 | Loss: 0.00001391
Iteration 48/1000 | Loss: 0.00001391
Iteration 49/1000 | Loss: 0.00001391
Iteration 50/1000 | Loss: 0.00001390
Iteration 51/1000 | Loss: 0.00001390
Iteration 52/1000 | Loss: 0.00001390
Iteration 53/1000 | Loss: 0.00001390
Iteration 54/1000 | Loss: 0.00001390
Iteration 55/1000 | Loss: 0.00001390
Iteration 56/1000 | Loss: 0.00001390
Iteration 57/1000 | Loss: 0.00001390
Iteration 58/1000 | Loss: 0.00001390
Iteration 59/1000 | Loss: 0.00001390
Iteration 60/1000 | Loss: 0.00001390
Iteration 61/1000 | Loss: 0.00001390
Iteration 62/1000 | Loss: 0.00001390
Iteration 63/1000 | Loss: 0.00001390
Iteration 64/1000 | Loss: 0.00001390
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001389
Iteration 67/1000 | Loss: 0.00001389
Iteration 68/1000 | Loss: 0.00001389
Iteration 69/1000 | Loss: 0.00001389
Iteration 70/1000 | Loss: 0.00001389
Iteration 71/1000 | Loss: 0.00001389
Iteration 72/1000 | Loss: 0.00001389
Iteration 73/1000 | Loss: 0.00001389
Iteration 74/1000 | Loss: 0.00001389
Iteration 75/1000 | Loss: 0.00001389
Iteration 76/1000 | Loss: 0.00001388
Iteration 77/1000 | Loss: 0.00001388
Iteration 78/1000 | Loss: 0.00001388
Iteration 79/1000 | Loss: 0.00001388
Iteration 80/1000 | Loss: 0.00001388
Iteration 81/1000 | Loss: 0.00001388
Iteration 82/1000 | Loss: 0.00001388
Iteration 83/1000 | Loss: 0.00001388
Iteration 84/1000 | Loss: 0.00001388
Iteration 85/1000 | Loss: 0.00001388
Iteration 86/1000 | Loss: 0.00001388
Iteration 87/1000 | Loss: 0.00001388
Iteration 88/1000 | Loss: 0.00001388
Iteration 89/1000 | Loss: 0.00001388
Iteration 90/1000 | Loss: 0.00001388
Iteration 91/1000 | Loss: 0.00001388
Iteration 92/1000 | Loss: 0.00001388
Iteration 93/1000 | Loss: 0.00001388
Iteration 94/1000 | Loss: 0.00001387
Iteration 95/1000 | Loss: 0.00001387
Iteration 96/1000 | Loss: 0.00001387
Iteration 97/1000 | Loss: 0.00001387
Iteration 98/1000 | Loss: 0.00001387
Iteration 99/1000 | Loss: 0.00001387
Iteration 100/1000 | Loss: 0.00001387
Iteration 101/1000 | Loss: 0.00001387
Iteration 102/1000 | Loss: 0.00001387
Iteration 103/1000 | Loss: 0.00001387
Iteration 104/1000 | Loss: 0.00001387
Iteration 105/1000 | Loss: 0.00001387
Iteration 106/1000 | Loss: 0.00001387
Iteration 107/1000 | Loss: 0.00001387
Iteration 108/1000 | Loss: 0.00001387
Iteration 109/1000 | Loss: 0.00001386
Iteration 110/1000 | Loss: 0.00001386
Iteration 111/1000 | Loss: 0.00001386
Iteration 112/1000 | Loss: 0.00001386
Iteration 113/1000 | Loss: 0.00001386
Iteration 114/1000 | Loss: 0.00001385
Iteration 115/1000 | Loss: 0.00001385
Iteration 116/1000 | Loss: 0.00001385
Iteration 117/1000 | Loss: 0.00001385
Iteration 118/1000 | Loss: 0.00001385
Iteration 119/1000 | Loss: 0.00001385
Iteration 120/1000 | Loss: 0.00001385
Iteration 121/1000 | Loss: 0.00001384
Iteration 122/1000 | Loss: 0.00001384
Iteration 123/1000 | Loss: 0.00001384
Iteration 124/1000 | Loss: 0.00001384
Iteration 125/1000 | Loss: 0.00001383
Iteration 126/1000 | Loss: 0.00001383
Iteration 127/1000 | Loss: 0.00001383
Iteration 128/1000 | Loss: 0.00001383
Iteration 129/1000 | Loss: 0.00001383
Iteration 130/1000 | Loss: 0.00001383
Iteration 131/1000 | Loss: 0.00001382
Iteration 132/1000 | Loss: 0.00001382
Iteration 133/1000 | Loss: 0.00001382
Iteration 134/1000 | Loss: 0.00001382
Iteration 135/1000 | Loss: 0.00001382
Iteration 136/1000 | Loss: 0.00001382
Iteration 137/1000 | Loss: 0.00001382
Iteration 138/1000 | Loss: 0.00001382
Iteration 139/1000 | Loss: 0.00001381
Iteration 140/1000 | Loss: 0.00001381
Iteration 141/1000 | Loss: 0.00001381
Iteration 142/1000 | Loss: 0.00001381
Iteration 143/1000 | Loss: 0.00001381
Iteration 144/1000 | Loss: 0.00001381
Iteration 145/1000 | Loss: 0.00001380
Iteration 146/1000 | Loss: 0.00001380
Iteration 147/1000 | Loss: 0.00001380
Iteration 148/1000 | Loss: 0.00001380
Iteration 149/1000 | Loss: 0.00001380
Iteration 150/1000 | Loss: 0.00001380
Iteration 151/1000 | Loss: 0.00001379
Iteration 152/1000 | Loss: 0.00001379
Iteration 153/1000 | Loss: 0.00001379
Iteration 154/1000 | Loss: 0.00001379
Iteration 155/1000 | Loss: 0.00001378
Iteration 156/1000 | Loss: 0.00001378
Iteration 157/1000 | Loss: 0.00001378
Iteration 158/1000 | Loss: 0.00001378
Iteration 159/1000 | Loss: 0.00001378
Iteration 160/1000 | Loss: 0.00001378
Iteration 161/1000 | Loss: 0.00001378
Iteration 162/1000 | Loss: 0.00001378
Iteration 163/1000 | Loss: 0.00001378
Iteration 164/1000 | Loss: 0.00001378
Iteration 165/1000 | Loss: 0.00001378
Iteration 166/1000 | Loss: 0.00001378
Iteration 167/1000 | Loss: 0.00001378
Iteration 168/1000 | Loss: 0.00001378
Iteration 169/1000 | Loss: 0.00001378
Iteration 170/1000 | Loss: 0.00001378
Iteration 171/1000 | Loss: 0.00001378
Iteration 172/1000 | Loss: 0.00001378
Iteration 173/1000 | Loss: 0.00001378
Iteration 174/1000 | Loss: 0.00001378
Iteration 175/1000 | Loss: 0.00001378
Iteration 176/1000 | Loss: 0.00001378
Iteration 177/1000 | Loss: 0.00001378
Iteration 178/1000 | Loss: 0.00001378
Iteration 179/1000 | Loss: 0.00001378
Iteration 180/1000 | Loss: 0.00001378
Iteration 181/1000 | Loss: 0.00001378
Iteration 182/1000 | Loss: 0.00001378
Iteration 183/1000 | Loss: 0.00001378
Iteration 184/1000 | Loss: 0.00001378
Iteration 185/1000 | Loss: 0.00001378
Iteration 186/1000 | Loss: 0.00001378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.377581156702945e-05, 1.377581156702945e-05, 1.377581156702945e-05, 1.377581156702945e-05, 1.377581156702945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.377581156702945e-05

Optimization complete. Final v2v error: 3.252058982849121 mm

Highest mean error: 3.788888692855835 mm for frame 40

Lowest mean error: 2.637725591659546 mm for frame 0

Saving results

Total time: 79.45412826538086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848505
Iteration 2/25 | Loss: 0.00129352
Iteration 3/25 | Loss: 0.00116530
Iteration 4/25 | Loss: 0.00114571
Iteration 5/25 | Loss: 0.00114020
Iteration 6/25 | Loss: 0.00113861
Iteration 7/25 | Loss: 0.00113795
Iteration 8/25 | Loss: 0.00113736
Iteration 9/25 | Loss: 0.00113907
Iteration 10/25 | Loss: 0.00113983
Iteration 11/25 | Loss: 0.00113872
Iteration 12/25 | Loss: 0.00113892
Iteration 13/25 | Loss: 0.00113969
Iteration 14/25 | Loss: 0.00113939
Iteration 15/25 | Loss: 0.00113932
Iteration 16/25 | Loss: 0.00113958
Iteration 17/25 | Loss: 0.00113869
Iteration 18/25 | Loss: 0.00113870
Iteration 19/25 | Loss: 0.00113985
Iteration 20/25 | Loss: 0.00113873
Iteration 21/25 | Loss: 0.00113690
Iteration 22/25 | Loss: 0.00113934
Iteration 23/25 | Loss: 0.00114014
Iteration 24/25 | Loss: 0.00113864
Iteration 25/25 | Loss: 0.00113887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18902063
Iteration 2/25 | Loss: 0.00343349
Iteration 3/25 | Loss: 0.00343348
Iteration 4/25 | Loss: 0.00343348
Iteration 5/25 | Loss: 0.00343348
Iteration 6/25 | Loss: 0.00343348
Iteration 7/25 | Loss: 0.00343348
Iteration 8/25 | Loss: 0.00343348
Iteration 9/25 | Loss: 0.00343348
Iteration 10/25 | Loss: 0.00343348
Iteration 11/25 | Loss: 0.00343348
Iteration 12/25 | Loss: 0.00343348
Iteration 13/25 | Loss: 0.00343348
Iteration 14/25 | Loss: 0.00343348
Iteration 15/25 | Loss: 0.00343348
Iteration 16/25 | Loss: 0.00343348
Iteration 17/25 | Loss: 0.00343348
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0034334808588027954, 0.0034334808588027954, 0.0034334808588027954, 0.0034334808588027954, 0.0034334808588027954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034334808588027954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00343348
Iteration 2/1000 | Loss: 0.00021441
Iteration 3/1000 | Loss: 0.00014815
Iteration 4/1000 | Loss: 0.00008055
Iteration 5/1000 | Loss: 0.00011157
Iteration 6/1000 | Loss: 0.00010438
Iteration 7/1000 | Loss: 0.00008714
Iteration 8/1000 | Loss: 0.00006934
Iteration 9/1000 | Loss: 0.00010139
Iteration 10/1000 | Loss: 0.00011405
Iteration 11/1000 | Loss: 0.00006526
Iteration 12/1000 | Loss: 0.00008882
Iteration 13/1000 | Loss: 0.00011473
Iteration 14/1000 | Loss: 0.00011455
Iteration 15/1000 | Loss: 0.00015155
Iteration 16/1000 | Loss: 0.00006372
Iteration 17/1000 | Loss: 0.00005778
Iteration 18/1000 | Loss: 0.00015839
Iteration 19/1000 | Loss: 0.00011526
Iteration 20/1000 | Loss: 0.00012385
Iteration 21/1000 | Loss: 0.00012873
Iteration 22/1000 | Loss: 0.00007172
Iteration 23/1000 | Loss: 0.00011548
Iteration 24/1000 | Loss: 0.00006403
Iteration 25/1000 | Loss: 0.00018616
Iteration 26/1000 | Loss: 0.00013328
Iteration 27/1000 | Loss: 0.00006633
Iteration 28/1000 | Loss: 0.00017194
Iteration 29/1000 | Loss: 0.00005655
Iteration 30/1000 | Loss: 0.00005165
Iteration 31/1000 | Loss: 0.00005023
Iteration 32/1000 | Loss: 0.00004909
Iteration 33/1000 | Loss: 0.00004848
Iteration 34/1000 | Loss: 0.00004815
Iteration 35/1000 | Loss: 0.00004776
Iteration 36/1000 | Loss: 0.00004746
Iteration 37/1000 | Loss: 0.00004724
Iteration 38/1000 | Loss: 0.00004700
Iteration 39/1000 | Loss: 0.00004680
Iteration 40/1000 | Loss: 0.00004655
Iteration 41/1000 | Loss: 0.00004624
Iteration 42/1000 | Loss: 0.00004589
Iteration 43/1000 | Loss: 0.00004561
Iteration 44/1000 | Loss: 0.00004529
Iteration 45/1000 | Loss: 0.00013252
Iteration 46/1000 | Loss: 0.00006629
Iteration 47/1000 | Loss: 0.00004513
Iteration 48/1000 | Loss: 0.00004464
Iteration 49/1000 | Loss: 0.00004429
Iteration 50/1000 | Loss: 0.00004394
Iteration 51/1000 | Loss: 0.00039928
Iteration 52/1000 | Loss: 0.00184889
Iteration 53/1000 | Loss: 0.00160939
Iteration 54/1000 | Loss: 0.00101712
Iteration 55/1000 | Loss: 0.00107848
Iteration 56/1000 | Loss: 0.00137384
Iteration 57/1000 | Loss: 0.00179707
Iteration 58/1000 | Loss: 0.00099162
Iteration 59/1000 | Loss: 0.00034285
Iteration 60/1000 | Loss: 0.00050830
Iteration 61/1000 | Loss: 0.00015588
Iteration 62/1000 | Loss: 0.00005331
Iteration 63/1000 | Loss: 0.00014441
Iteration 64/1000 | Loss: 0.00004444
Iteration 65/1000 | Loss: 0.00023619
Iteration 66/1000 | Loss: 0.00022806
Iteration 67/1000 | Loss: 0.00018540
Iteration 68/1000 | Loss: 0.00004437
Iteration 69/1000 | Loss: 0.00003778
Iteration 70/1000 | Loss: 0.00003538
Iteration 71/1000 | Loss: 0.00003416
Iteration 72/1000 | Loss: 0.00013917
Iteration 73/1000 | Loss: 0.00010485
Iteration 74/1000 | Loss: 0.00004393
Iteration 75/1000 | Loss: 0.00003753
Iteration 76/1000 | Loss: 0.00003352
Iteration 77/1000 | Loss: 0.00003260
Iteration 78/1000 | Loss: 0.00003203
Iteration 79/1000 | Loss: 0.00011878
Iteration 80/1000 | Loss: 0.00003539
Iteration 81/1000 | Loss: 0.00003200
Iteration 82/1000 | Loss: 0.00003124
Iteration 83/1000 | Loss: 0.00003071
Iteration 84/1000 | Loss: 0.00003021
Iteration 85/1000 | Loss: 0.00002994
Iteration 86/1000 | Loss: 0.00002967
Iteration 87/1000 | Loss: 0.00002957
Iteration 88/1000 | Loss: 0.00002922
Iteration 89/1000 | Loss: 0.00002906
Iteration 90/1000 | Loss: 0.00002894
Iteration 91/1000 | Loss: 0.00002893
Iteration 92/1000 | Loss: 0.00002877
Iteration 93/1000 | Loss: 0.00002867
Iteration 94/1000 | Loss: 0.00002866
Iteration 95/1000 | Loss: 0.00002866
Iteration 96/1000 | Loss: 0.00002864
Iteration 97/1000 | Loss: 0.00002863
Iteration 98/1000 | Loss: 0.00002861
Iteration 99/1000 | Loss: 0.00002861
Iteration 100/1000 | Loss: 0.00002860
Iteration 101/1000 | Loss: 0.00002858
Iteration 102/1000 | Loss: 0.00002857
Iteration 103/1000 | Loss: 0.00002856
Iteration 104/1000 | Loss: 0.00002856
Iteration 105/1000 | Loss: 0.00002855
Iteration 106/1000 | Loss: 0.00002855
Iteration 107/1000 | Loss: 0.00002854
Iteration 108/1000 | Loss: 0.00002854
Iteration 109/1000 | Loss: 0.00002853
Iteration 110/1000 | Loss: 0.00002852
Iteration 111/1000 | Loss: 0.00002852
Iteration 112/1000 | Loss: 0.00002851
Iteration 113/1000 | Loss: 0.00002850
Iteration 114/1000 | Loss: 0.00002846
Iteration 115/1000 | Loss: 0.00002846
Iteration 116/1000 | Loss: 0.00002844
Iteration 117/1000 | Loss: 0.00002843
Iteration 118/1000 | Loss: 0.00002843
Iteration 119/1000 | Loss: 0.00002843
Iteration 120/1000 | Loss: 0.00002842
Iteration 121/1000 | Loss: 0.00002841
Iteration 122/1000 | Loss: 0.00002841
Iteration 123/1000 | Loss: 0.00002841
Iteration 124/1000 | Loss: 0.00002841
Iteration 125/1000 | Loss: 0.00002840
Iteration 126/1000 | Loss: 0.00002840
Iteration 127/1000 | Loss: 0.00002840
Iteration 128/1000 | Loss: 0.00002840
Iteration 129/1000 | Loss: 0.00002839
Iteration 130/1000 | Loss: 0.00002839
Iteration 131/1000 | Loss: 0.00002839
Iteration 132/1000 | Loss: 0.00002839
Iteration 133/1000 | Loss: 0.00002839
Iteration 134/1000 | Loss: 0.00002839
Iteration 135/1000 | Loss: 0.00002839
Iteration 136/1000 | Loss: 0.00002839
Iteration 137/1000 | Loss: 0.00002839
Iteration 138/1000 | Loss: 0.00002839
Iteration 139/1000 | Loss: 0.00002839
Iteration 140/1000 | Loss: 0.00002838
Iteration 141/1000 | Loss: 0.00002838
Iteration 142/1000 | Loss: 0.00002838
Iteration 143/1000 | Loss: 0.00002838
Iteration 144/1000 | Loss: 0.00002838
Iteration 145/1000 | Loss: 0.00002838
Iteration 146/1000 | Loss: 0.00002837
Iteration 147/1000 | Loss: 0.00002837
Iteration 148/1000 | Loss: 0.00002837
Iteration 149/1000 | Loss: 0.00002836
Iteration 150/1000 | Loss: 0.00002836
Iteration 151/1000 | Loss: 0.00002836
Iteration 152/1000 | Loss: 0.00002836
Iteration 153/1000 | Loss: 0.00002836
Iteration 154/1000 | Loss: 0.00002835
Iteration 155/1000 | Loss: 0.00002835
Iteration 156/1000 | Loss: 0.00002835
Iteration 157/1000 | Loss: 0.00002835
Iteration 158/1000 | Loss: 0.00002835
Iteration 159/1000 | Loss: 0.00002835
Iteration 160/1000 | Loss: 0.00002835
Iteration 161/1000 | Loss: 0.00002835
Iteration 162/1000 | Loss: 0.00002835
Iteration 163/1000 | Loss: 0.00002835
Iteration 164/1000 | Loss: 0.00002835
Iteration 165/1000 | Loss: 0.00002835
Iteration 166/1000 | Loss: 0.00002835
Iteration 167/1000 | Loss: 0.00002835
Iteration 168/1000 | Loss: 0.00002835
Iteration 169/1000 | Loss: 0.00002835
Iteration 170/1000 | Loss: 0.00002835
Iteration 171/1000 | Loss: 0.00002835
Iteration 172/1000 | Loss: 0.00002835
Iteration 173/1000 | Loss: 0.00002835
Iteration 174/1000 | Loss: 0.00002835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.834503357007634e-05, 2.834503357007634e-05, 2.834503357007634e-05, 2.834503357007634e-05, 2.834503357007634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.834503357007634e-05

Optimization complete. Final v2v error: 2.9666073322296143 mm

Highest mean error: 13.989572525024414 mm for frame 115

Lowest mean error: 2.275132179260254 mm for frame 46

Saving results

Total time: 207.70949745178223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023623
Iteration 2/25 | Loss: 0.00192904
Iteration 3/25 | Loss: 0.00134659
Iteration 4/25 | Loss: 0.00132222
Iteration 5/25 | Loss: 0.00124957
Iteration 6/25 | Loss: 0.00120218
Iteration 7/25 | Loss: 0.00119865
Iteration 8/25 | Loss: 0.00119537
Iteration 9/25 | Loss: 0.00119654
Iteration 10/25 | Loss: 0.00119422
Iteration 11/25 | Loss: 0.00118530
Iteration 12/25 | Loss: 0.00118566
Iteration 13/25 | Loss: 0.00118853
Iteration 14/25 | Loss: 0.00118836
Iteration 15/25 | Loss: 0.00118665
Iteration 16/25 | Loss: 0.00118729
Iteration 17/25 | Loss: 0.00118520
Iteration 18/25 | Loss: 0.00118520
Iteration 19/25 | Loss: 0.00118824
Iteration 20/25 | Loss: 0.00118542
Iteration 21/25 | Loss: 0.00118651
Iteration 22/25 | Loss: 0.00118473
Iteration 23/25 | Loss: 0.00118667
Iteration 24/25 | Loss: 0.00118610
Iteration 25/25 | Loss: 0.00118238

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.76236916
Iteration 2/25 | Loss: 0.00285890
Iteration 3/25 | Loss: 0.00285726
Iteration 4/25 | Loss: 0.00285726
Iteration 5/25 | Loss: 0.00285726
Iteration 6/25 | Loss: 0.00285726
Iteration 7/25 | Loss: 0.00285725
Iteration 8/25 | Loss: 0.00285725
Iteration 9/25 | Loss: 0.00285725
Iteration 10/25 | Loss: 0.00285725
Iteration 11/25 | Loss: 0.00285725
Iteration 12/25 | Loss: 0.00285725
Iteration 13/25 | Loss: 0.00285725
Iteration 14/25 | Loss: 0.00285725
Iteration 15/25 | Loss: 0.00285725
Iteration 16/25 | Loss: 0.00285725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002857254119589925, 0.002857254119589925, 0.002857254119589925, 0.002857254119589925, 0.002857254119589925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002857254119589925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285725
Iteration 2/1000 | Loss: 0.00037109
Iteration 3/1000 | Loss: 0.00055125
Iteration 4/1000 | Loss: 0.00036467
Iteration 5/1000 | Loss: 0.00049038
Iteration 6/1000 | Loss: 0.00027265
Iteration 7/1000 | Loss: 0.00053270
Iteration 8/1000 | Loss: 0.00055909
Iteration 9/1000 | Loss: 0.00064074
Iteration 10/1000 | Loss: 0.00040781
Iteration 11/1000 | Loss: 0.00042547
Iteration 12/1000 | Loss: 0.00037033
Iteration 13/1000 | Loss: 0.00054354
Iteration 14/1000 | Loss: 0.00038786
Iteration 15/1000 | Loss: 0.00054664
Iteration 16/1000 | Loss: 0.00037861
Iteration 17/1000 | Loss: 0.00041582
Iteration 18/1000 | Loss: 0.00753377
Iteration 19/1000 | Loss: 0.00638626
Iteration 20/1000 | Loss: 0.00090796
Iteration 21/1000 | Loss: 0.00020242
Iteration 22/1000 | Loss: 0.00074846
Iteration 23/1000 | Loss: 0.00126568
Iteration 24/1000 | Loss: 0.00076999
Iteration 25/1000 | Loss: 0.00334992
Iteration 26/1000 | Loss: 0.00015459
Iteration 27/1000 | Loss: 0.00008778
Iteration 28/1000 | Loss: 0.00006048
Iteration 29/1000 | Loss: 0.00004605
Iteration 30/1000 | Loss: 0.00004223
Iteration 31/1000 | Loss: 0.00003665
Iteration 32/1000 | Loss: 0.00003396
Iteration 33/1000 | Loss: 0.00003253
Iteration 34/1000 | Loss: 0.00003140
Iteration 35/1000 | Loss: 0.00003064
Iteration 36/1000 | Loss: 0.00053992
Iteration 37/1000 | Loss: 0.00003322
Iteration 38/1000 | Loss: 0.00002990
Iteration 39/1000 | Loss: 0.00002887
Iteration 40/1000 | Loss: 0.00002794
Iteration 41/1000 | Loss: 0.00002723
Iteration 42/1000 | Loss: 0.00002690
Iteration 43/1000 | Loss: 0.00002666
Iteration 44/1000 | Loss: 0.00002642
Iteration 45/1000 | Loss: 0.00002624
Iteration 46/1000 | Loss: 0.00002624
Iteration 47/1000 | Loss: 0.00002609
Iteration 48/1000 | Loss: 0.00002597
Iteration 49/1000 | Loss: 0.00002596
Iteration 50/1000 | Loss: 0.00002592
Iteration 51/1000 | Loss: 0.00002591
Iteration 52/1000 | Loss: 0.00002591
Iteration 53/1000 | Loss: 0.00002590
Iteration 54/1000 | Loss: 0.00002590
Iteration 55/1000 | Loss: 0.00002589
Iteration 56/1000 | Loss: 0.00002589
Iteration 57/1000 | Loss: 0.00002588
Iteration 58/1000 | Loss: 0.00002586
Iteration 59/1000 | Loss: 0.00002585
Iteration 60/1000 | Loss: 0.00002585
Iteration 61/1000 | Loss: 0.00002583
Iteration 62/1000 | Loss: 0.00002580
Iteration 63/1000 | Loss: 0.00002577
Iteration 64/1000 | Loss: 0.00002577
Iteration 65/1000 | Loss: 0.00002576
Iteration 66/1000 | Loss: 0.00002576
Iteration 67/1000 | Loss: 0.00002576
Iteration 68/1000 | Loss: 0.00002576
Iteration 69/1000 | Loss: 0.00002576
Iteration 70/1000 | Loss: 0.00002576
Iteration 71/1000 | Loss: 0.00002576
Iteration 72/1000 | Loss: 0.00002576
Iteration 73/1000 | Loss: 0.00002576
Iteration 74/1000 | Loss: 0.00002576
Iteration 75/1000 | Loss: 0.00002576
Iteration 76/1000 | Loss: 0.00002576
Iteration 77/1000 | Loss: 0.00002576
Iteration 78/1000 | Loss: 0.00002576
Iteration 79/1000 | Loss: 0.00002575
Iteration 80/1000 | Loss: 0.00002575
Iteration 81/1000 | Loss: 0.00002575
Iteration 82/1000 | Loss: 0.00002574
Iteration 83/1000 | Loss: 0.00002574
Iteration 84/1000 | Loss: 0.00002574
Iteration 85/1000 | Loss: 0.00002574
Iteration 86/1000 | Loss: 0.00002573
Iteration 87/1000 | Loss: 0.00002573
Iteration 88/1000 | Loss: 0.00002573
Iteration 89/1000 | Loss: 0.00002573
Iteration 90/1000 | Loss: 0.00002573
Iteration 91/1000 | Loss: 0.00002572
Iteration 92/1000 | Loss: 0.00002572
Iteration 93/1000 | Loss: 0.00002572
Iteration 94/1000 | Loss: 0.00002572
Iteration 95/1000 | Loss: 0.00002571
Iteration 96/1000 | Loss: 0.00002571
Iteration 97/1000 | Loss: 0.00002571
Iteration 98/1000 | Loss: 0.00002570
Iteration 99/1000 | Loss: 0.00002570
Iteration 100/1000 | Loss: 0.00002570
Iteration 101/1000 | Loss: 0.00002570
Iteration 102/1000 | Loss: 0.00002569
Iteration 103/1000 | Loss: 0.00002569
Iteration 104/1000 | Loss: 0.00002568
Iteration 105/1000 | Loss: 0.00002568
Iteration 106/1000 | Loss: 0.00002568
Iteration 107/1000 | Loss: 0.00002568
Iteration 108/1000 | Loss: 0.00002567
Iteration 109/1000 | Loss: 0.00002567
Iteration 110/1000 | Loss: 0.00002567
Iteration 111/1000 | Loss: 0.00002566
Iteration 112/1000 | Loss: 0.00002566
Iteration 113/1000 | Loss: 0.00002566
Iteration 114/1000 | Loss: 0.00002566
Iteration 115/1000 | Loss: 0.00002566
Iteration 116/1000 | Loss: 0.00002566
Iteration 117/1000 | Loss: 0.00002566
Iteration 118/1000 | Loss: 0.00002566
Iteration 119/1000 | Loss: 0.00002565
Iteration 120/1000 | Loss: 0.00002565
Iteration 121/1000 | Loss: 0.00002565
Iteration 122/1000 | Loss: 0.00002565
Iteration 123/1000 | Loss: 0.00002565
Iteration 124/1000 | Loss: 0.00002564
Iteration 125/1000 | Loss: 0.00002564
Iteration 126/1000 | Loss: 0.00002564
Iteration 127/1000 | Loss: 0.00002564
Iteration 128/1000 | Loss: 0.00002564
Iteration 129/1000 | Loss: 0.00002564
Iteration 130/1000 | Loss: 0.00002564
Iteration 131/1000 | Loss: 0.00002563
Iteration 132/1000 | Loss: 0.00002563
Iteration 133/1000 | Loss: 0.00002563
Iteration 134/1000 | Loss: 0.00002563
Iteration 135/1000 | Loss: 0.00002563
Iteration 136/1000 | Loss: 0.00002563
Iteration 137/1000 | Loss: 0.00002562
Iteration 138/1000 | Loss: 0.00002562
Iteration 139/1000 | Loss: 0.00002562
Iteration 140/1000 | Loss: 0.00002562
Iteration 141/1000 | Loss: 0.00002561
Iteration 142/1000 | Loss: 0.00002561
Iteration 143/1000 | Loss: 0.00002561
Iteration 144/1000 | Loss: 0.00002561
Iteration 145/1000 | Loss: 0.00002561
Iteration 146/1000 | Loss: 0.00002561
Iteration 147/1000 | Loss: 0.00002561
Iteration 148/1000 | Loss: 0.00002561
Iteration 149/1000 | Loss: 0.00002560
Iteration 150/1000 | Loss: 0.00002560
Iteration 151/1000 | Loss: 0.00002560
Iteration 152/1000 | Loss: 0.00002560
Iteration 153/1000 | Loss: 0.00002560
Iteration 154/1000 | Loss: 0.00002560
Iteration 155/1000 | Loss: 0.00002559
Iteration 156/1000 | Loss: 0.00002559
Iteration 157/1000 | Loss: 0.00002559
Iteration 158/1000 | Loss: 0.00002559
Iteration 159/1000 | Loss: 0.00002558
Iteration 160/1000 | Loss: 0.00002558
Iteration 161/1000 | Loss: 0.00002558
Iteration 162/1000 | Loss: 0.00002557
Iteration 163/1000 | Loss: 0.00002557
Iteration 164/1000 | Loss: 0.00002557
Iteration 165/1000 | Loss: 0.00002556
Iteration 166/1000 | Loss: 0.00002556
Iteration 167/1000 | Loss: 0.00002555
Iteration 168/1000 | Loss: 0.00002555
Iteration 169/1000 | Loss: 0.00002555
Iteration 170/1000 | Loss: 0.00002554
Iteration 171/1000 | Loss: 0.00002554
Iteration 172/1000 | Loss: 0.00002554
Iteration 173/1000 | Loss: 0.00002553
Iteration 174/1000 | Loss: 0.00002553
Iteration 175/1000 | Loss: 0.00002553
Iteration 176/1000 | Loss: 0.00002553
Iteration 177/1000 | Loss: 0.00002553
Iteration 178/1000 | Loss: 0.00002552
Iteration 179/1000 | Loss: 0.00002552
Iteration 180/1000 | Loss: 0.00002552
Iteration 181/1000 | Loss: 0.00002552
Iteration 182/1000 | Loss: 0.00002551
Iteration 183/1000 | Loss: 0.00002551
Iteration 184/1000 | Loss: 0.00002551
Iteration 185/1000 | Loss: 0.00002551
Iteration 186/1000 | Loss: 0.00002551
Iteration 187/1000 | Loss: 0.00002550
Iteration 188/1000 | Loss: 0.00002550
Iteration 189/1000 | Loss: 0.00002550
Iteration 190/1000 | Loss: 0.00002550
Iteration 191/1000 | Loss: 0.00002550
Iteration 192/1000 | Loss: 0.00002549
Iteration 193/1000 | Loss: 0.00002549
Iteration 194/1000 | Loss: 0.00002549
Iteration 195/1000 | Loss: 0.00002549
Iteration 196/1000 | Loss: 0.00002548
Iteration 197/1000 | Loss: 0.00002548
Iteration 198/1000 | Loss: 0.00002548
Iteration 199/1000 | Loss: 0.00002547
Iteration 200/1000 | Loss: 0.00002547
Iteration 201/1000 | Loss: 0.00002547
Iteration 202/1000 | Loss: 0.00002547
Iteration 203/1000 | Loss: 0.00002547
Iteration 204/1000 | Loss: 0.00002547
Iteration 205/1000 | Loss: 0.00002546
Iteration 206/1000 | Loss: 0.00002546
Iteration 207/1000 | Loss: 0.00002546
Iteration 208/1000 | Loss: 0.00002546
Iteration 209/1000 | Loss: 0.00002546
Iteration 210/1000 | Loss: 0.00002546
Iteration 211/1000 | Loss: 0.00002546
Iteration 212/1000 | Loss: 0.00002546
Iteration 213/1000 | Loss: 0.00002545
Iteration 214/1000 | Loss: 0.00002545
Iteration 215/1000 | Loss: 0.00002544
Iteration 216/1000 | Loss: 0.00002544
Iteration 217/1000 | Loss: 0.00002544
Iteration 218/1000 | Loss: 0.00002543
Iteration 219/1000 | Loss: 0.00002543
Iteration 220/1000 | Loss: 0.00002543
Iteration 221/1000 | Loss: 0.00002543
Iteration 222/1000 | Loss: 0.00002543
Iteration 223/1000 | Loss: 0.00002543
Iteration 224/1000 | Loss: 0.00002543
Iteration 225/1000 | Loss: 0.00002543
Iteration 226/1000 | Loss: 0.00002542
Iteration 227/1000 | Loss: 0.00002542
Iteration 228/1000 | Loss: 0.00002542
Iteration 229/1000 | Loss: 0.00002542
Iteration 230/1000 | Loss: 0.00002542
Iteration 231/1000 | Loss: 0.00002541
Iteration 232/1000 | Loss: 0.00002541
Iteration 233/1000 | Loss: 0.00002541
Iteration 234/1000 | Loss: 0.00002541
Iteration 235/1000 | Loss: 0.00002541
Iteration 236/1000 | Loss: 0.00002540
Iteration 237/1000 | Loss: 0.00002540
Iteration 238/1000 | Loss: 0.00002540
Iteration 239/1000 | Loss: 0.00002540
Iteration 240/1000 | Loss: 0.00002540
Iteration 241/1000 | Loss: 0.00002540
Iteration 242/1000 | Loss: 0.00002540
Iteration 243/1000 | Loss: 0.00002539
Iteration 244/1000 | Loss: 0.00002539
Iteration 245/1000 | Loss: 0.00002539
Iteration 246/1000 | Loss: 0.00002539
Iteration 247/1000 | Loss: 0.00002539
Iteration 248/1000 | Loss: 0.00002539
Iteration 249/1000 | Loss: 0.00002539
Iteration 250/1000 | Loss: 0.00002539
Iteration 251/1000 | Loss: 0.00002538
Iteration 252/1000 | Loss: 0.00002538
Iteration 253/1000 | Loss: 0.00002538
Iteration 254/1000 | Loss: 0.00002538
Iteration 255/1000 | Loss: 0.00002538
Iteration 256/1000 | Loss: 0.00002538
Iteration 257/1000 | Loss: 0.00002538
Iteration 258/1000 | Loss: 0.00002538
Iteration 259/1000 | Loss: 0.00002538
Iteration 260/1000 | Loss: 0.00002538
Iteration 261/1000 | Loss: 0.00002538
Iteration 262/1000 | Loss: 0.00002538
Iteration 263/1000 | Loss: 0.00002538
Iteration 264/1000 | Loss: 0.00002537
Iteration 265/1000 | Loss: 0.00002537
Iteration 266/1000 | Loss: 0.00002537
Iteration 267/1000 | Loss: 0.00002537
Iteration 268/1000 | Loss: 0.00002537
Iteration 269/1000 | Loss: 0.00002537
Iteration 270/1000 | Loss: 0.00002537
Iteration 271/1000 | Loss: 0.00002537
Iteration 272/1000 | Loss: 0.00002537
Iteration 273/1000 | Loss: 0.00002537
Iteration 274/1000 | Loss: 0.00002537
Iteration 275/1000 | Loss: 0.00002537
Iteration 276/1000 | Loss: 0.00002537
Iteration 277/1000 | Loss: 0.00002537
Iteration 278/1000 | Loss: 0.00002537
Iteration 279/1000 | Loss: 0.00002537
Iteration 280/1000 | Loss: 0.00002537
Iteration 281/1000 | Loss: 0.00002536
Iteration 282/1000 | Loss: 0.00002536
Iteration 283/1000 | Loss: 0.00002536
Iteration 284/1000 | Loss: 0.00002536
Iteration 285/1000 | Loss: 0.00002536
Iteration 286/1000 | Loss: 0.00002536
Iteration 287/1000 | Loss: 0.00002536
Iteration 288/1000 | Loss: 0.00002536
Iteration 289/1000 | Loss: 0.00002536
Iteration 290/1000 | Loss: 0.00002536
Iteration 291/1000 | Loss: 0.00002536
Iteration 292/1000 | Loss: 0.00002536
Iteration 293/1000 | Loss: 0.00002536
Iteration 294/1000 | Loss: 0.00002536
Iteration 295/1000 | Loss: 0.00002536
Iteration 296/1000 | Loss: 0.00002536
Iteration 297/1000 | Loss: 0.00002536
Iteration 298/1000 | Loss: 0.00002536
Iteration 299/1000 | Loss: 0.00002536
Iteration 300/1000 | Loss: 0.00002536
Iteration 301/1000 | Loss: 0.00002536
Iteration 302/1000 | Loss: 0.00002536
Iteration 303/1000 | Loss: 0.00002536
Iteration 304/1000 | Loss: 0.00002536
Iteration 305/1000 | Loss: 0.00002536
Iteration 306/1000 | Loss: 0.00002536
Iteration 307/1000 | Loss: 0.00002536
Iteration 308/1000 | Loss: 0.00002536
Iteration 309/1000 | Loss: 0.00002536
Iteration 310/1000 | Loss: 0.00002536
Iteration 311/1000 | Loss: 0.00002536
Iteration 312/1000 | Loss: 0.00002536
Iteration 313/1000 | Loss: 0.00002536
Iteration 314/1000 | Loss: 0.00002536
Iteration 315/1000 | Loss: 0.00002536
Iteration 316/1000 | Loss: 0.00002536
Iteration 317/1000 | Loss: 0.00002536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [2.5355740945087746e-05, 2.5355740945087746e-05, 2.5355740945087746e-05, 2.5355740945087746e-05, 2.5355740945087746e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5355740945087746e-05

Optimization complete. Final v2v error: 4.183643341064453 mm

Highest mean error: 6.090653896331787 mm for frame 55

Lowest mean error: 2.9925026893615723 mm for frame 0

Saving results

Total time: 135.3159110546112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00750461
Iteration 2/25 | Loss: 0.00124162
Iteration 3/25 | Loss: 0.00109830
Iteration 4/25 | Loss: 0.00108173
Iteration 5/25 | Loss: 0.00107776
Iteration 6/25 | Loss: 0.00107749
Iteration 7/25 | Loss: 0.00107749
Iteration 8/25 | Loss: 0.00107749
Iteration 9/25 | Loss: 0.00107749
Iteration 10/25 | Loss: 0.00107749
Iteration 11/25 | Loss: 0.00107749
Iteration 12/25 | Loss: 0.00107749
Iteration 13/25 | Loss: 0.00107749
Iteration 14/25 | Loss: 0.00107749
Iteration 15/25 | Loss: 0.00107749
Iteration 16/25 | Loss: 0.00107749
Iteration 17/25 | Loss: 0.00107749
Iteration 18/25 | Loss: 0.00107749
Iteration 19/25 | Loss: 0.00107749
Iteration 20/25 | Loss: 0.00107749
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010774852707982063, 0.0010774852707982063, 0.0010774852707982063, 0.0010774852707982063, 0.0010774852707982063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010774852707982063

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16048610
Iteration 2/25 | Loss: 0.00217282
Iteration 3/25 | Loss: 0.00217282
Iteration 4/25 | Loss: 0.00217282
Iteration 5/25 | Loss: 0.00217282
Iteration 6/25 | Loss: 0.00217282
Iteration 7/25 | Loss: 0.00217282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 7. Stopping optimization.
Last 5 losses: [0.0021728186402469873, 0.0021728186402469873, 0.0021728186402469873, 0.0021728186402469873, 0.0021728186402469873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021728186402469873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217282
Iteration 2/1000 | Loss: 0.00003672
Iteration 3/1000 | Loss: 0.00002785
Iteration 4/1000 | Loss: 0.00002467
Iteration 5/1000 | Loss: 0.00002293
Iteration 6/1000 | Loss: 0.00002076
Iteration 7/1000 | Loss: 0.00001979
Iteration 8/1000 | Loss: 0.00001914
Iteration 9/1000 | Loss: 0.00001861
Iteration 10/1000 | Loss: 0.00001819
Iteration 11/1000 | Loss: 0.00001792
Iteration 12/1000 | Loss: 0.00001771
Iteration 13/1000 | Loss: 0.00001768
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001756
Iteration 16/1000 | Loss: 0.00001756
Iteration 17/1000 | Loss: 0.00001756
Iteration 18/1000 | Loss: 0.00001755
Iteration 19/1000 | Loss: 0.00001755
Iteration 20/1000 | Loss: 0.00001753
Iteration 21/1000 | Loss: 0.00001753
Iteration 22/1000 | Loss: 0.00001753
Iteration 23/1000 | Loss: 0.00001753
Iteration 24/1000 | Loss: 0.00001752
Iteration 25/1000 | Loss: 0.00001751
Iteration 26/1000 | Loss: 0.00001748
Iteration 27/1000 | Loss: 0.00001743
Iteration 28/1000 | Loss: 0.00001741
Iteration 29/1000 | Loss: 0.00001740
Iteration 30/1000 | Loss: 0.00001740
Iteration 31/1000 | Loss: 0.00001739
Iteration 32/1000 | Loss: 0.00001739
Iteration 33/1000 | Loss: 0.00001739
Iteration 34/1000 | Loss: 0.00001738
Iteration 35/1000 | Loss: 0.00001738
Iteration 36/1000 | Loss: 0.00001737
Iteration 37/1000 | Loss: 0.00001734
Iteration 38/1000 | Loss: 0.00001732
Iteration 39/1000 | Loss: 0.00001725
Iteration 40/1000 | Loss: 0.00001725
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001724
Iteration 43/1000 | Loss: 0.00001723
Iteration 44/1000 | Loss: 0.00001723
Iteration 45/1000 | Loss: 0.00001722
Iteration 46/1000 | Loss: 0.00001722
Iteration 47/1000 | Loss: 0.00001722
Iteration 48/1000 | Loss: 0.00001721
Iteration 49/1000 | Loss: 0.00001721
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001721
Iteration 53/1000 | Loss: 0.00001721
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001719
Iteration 56/1000 | Loss: 0.00001719
Iteration 57/1000 | Loss: 0.00001717
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001715
Iteration 61/1000 | Loss: 0.00001715
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001713
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001711
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001711
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001710
Iteration 79/1000 | Loss: 0.00001710
Iteration 80/1000 | Loss: 0.00001709
Iteration 81/1000 | Loss: 0.00001709
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001707
Iteration 91/1000 | Loss: 0.00001707
Iteration 92/1000 | Loss: 0.00001707
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001706
Iteration 95/1000 | Loss: 0.00001706
Iteration 96/1000 | Loss: 0.00001706
Iteration 97/1000 | Loss: 0.00001705
Iteration 98/1000 | Loss: 0.00001705
Iteration 99/1000 | Loss: 0.00001705
Iteration 100/1000 | Loss: 0.00001705
Iteration 101/1000 | Loss: 0.00001705
Iteration 102/1000 | Loss: 0.00001705
Iteration 103/1000 | Loss: 0.00001705
Iteration 104/1000 | Loss: 0.00001705
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001704
Iteration 107/1000 | Loss: 0.00001704
Iteration 108/1000 | Loss: 0.00001704
Iteration 109/1000 | Loss: 0.00001703
Iteration 110/1000 | Loss: 0.00001703
Iteration 111/1000 | Loss: 0.00001703
Iteration 112/1000 | Loss: 0.00001703
Iteration 113/1000 | Loss: 0.00001702
Iteration 114/1000 | Loss: 0.00001702
Iteration 115/1000 | Loss: 0.00001702
Iteration 116/1000 | Loss: 0.00001702
Iteration 117/1000 | Loss: 0.00001702
Iteration 118/1000 | Loss: 0.00001702
Iteration 119/1000 | Loss: 0.00001702
Iteration 120/1000 | Loss: 0.00001702
Iteration 121/1000 | Loss: 0.00001702
Iteration 122/1000 | Loss: 0.00001701
Iteration 123/1000 | Loss: 0.00001701
Iteration 124/1000 | Loss: 0.00001701
Iteration 125/1000 | Loss: 0.00001701
Iteration 126/1000 | Loss: 0.00001701
Iteration 127/1000 | Loss: 0.00001701
Iteration 128/1000 | Loss: 0.00001701
Iteration 129/1000 | Loss: 0.00001701
Iteration 130/1000 | Loss: 0.00001701
Iteration 131/1000 | Loss: 0.00001700
Iteration 132/1000 | Loss: 0.00001700
Iteration 133/1000 | Loss: 0.00001700
Iteration 134/1000 | Loss: 0.00001700
Iteration 135/1000 | Loss: 0.00001700
Iteration 136/1000 | Loss: 0.00001699
Iteration 137/1000 | Loss: 0.00001699
Iteration 138/1000 | Loss: 0.00001699
Iteration 139/1000 | Loss: 0.00001699
Iteration 140/1000 | Loss: 0.00001699
Iteration 141/1000 | Loss: 0.00001699
Iteration 142/1000 | Loss: 0.00001699
Iteration 143/1000 | Loss: 0.00001699
Iteration 144/1000 | Loss: 0.00001699
Iteration 145/1000 | Loss: 0.00001699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.6991634765872732e-05, 1.6991634765872732e-05, 1.6991634765872732e-05, 1.6991634765872732e-05, 1.6991634765872732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6991634765872732e-05

Optimization complete. Final v2v error: 3.496673583984375 mm

Highest mean error: 4.696940898895264 mm for frame 95

Lowest mean error: 2.710951328277588 mm for frame 46

Saving results

Total time: 43.95245671272278
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01102336
Iteration 2/25 | Loss: 0.01102335
Iteration 3/25 | Loss: 0.01102334
Iteration 4/25 | Loss: 0.00216685
Iteration 5/25 | Loss: 0.00140884
Iteration 6/25 | Loss: 0.00121246
Iteration 7/25 | Loss: 0.00122650
Iteration 8/25 | Loss: 0.00118624
Iteration 9/25 | Loss: 0.00116244
Iteration 10/25 | Loss: 0.00113739
Iteration 11/25 | Loss: 0.00113672
Iteration 12/25 | Loss: 0.00113665
Iteration 13/25 | Loss: 0.00113665
Iteration 14/25 | Loss: 0.00113665
Iteration 15/25 | Loss: 0.00113665
Iteration 16/25 | Loss: 0.00113664
Iteration 17/25 | Loss: 0.00113664
Iteration 18/25 | Loss: 0.00113664
Iteration 19/25 | Loss: 0.00113664
Iteration 20/25 | Loss: 0.00113664
Iteration 21/25 | Loss: 0.00113664
Iteration 22/25 | Loss: 0.00113664
Iteration 23/25 | Loss: 0.00113664
Iteration 24/25 | Loss: 0.00113664
Iteration 25/25 | Loss: 0.00113663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23080301
Iteration 2/25 | Loss: 0.00230106
Iteration 3/25 | Loss: 0.00230106
Iteration 4/25 | Loss: 0.00230106
Iteration 5/25 | Loss: 0.00230106
Iteration 6/25 | Loss: 0.00230106
Iteration 7/25 | Loss: 0.00230106
Iteration 8/25 | Loss: 0.00230106
Iteration 9/25 | Loss: 0.00230106
Iteration 10/25 | Loss: 0.00230106
Iteration 11/25 | Loss: 0.00230106
Iteration 12/25 | Loss: 0.00230106
Iteration 13/25 | Loss: 0.00230106
Iteration 14/25 | Loss: 0.00230106
Iteration 15/25 | Loss: 0.00230106
Iteration 16/25 | Loss: 0.00230106
Iteration 17/25 | Loss: 0.00230106
Iteration 18/25 | Loss: 0.00230106
Iteration 19/25 | Loss: 0.00230106
Iteration 20/25 | Loss: 0.00230106
Iteration 21/25 | Loss: 0.00230106
Iteration 22/25 | Loss: 0.00230106
Iteration 23/25 | Loss: 0.00230106
Iteration 24/25 | Loss: 0.00230106
Iteration 25/25 | Loss: 0.00230106

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230106
Iteration 2/1000 | Loss: 0.00005375
Iteration 3/1000 | Loss: 0.00003117
Iteration 4/1000 | Loss: 0.00002371
Iteration 5/1000 | Loss: 0.00002161
Iteration 6/1000 | Loss: 0.00002374
Iteration 7/1000 | Loss: 0.00002167
Iteration 8/1000 | Loss: 0.00002540
Iteration 9/1000 | Loss: 0.00001962
Iteration 10/1000 | Loss: 0.00001831
Iteration 11/1000 | Loss: 0.00001796
Iteration 12/1000 | Loss: 0.00001763
Iteration 13/1000 | Loss: 0.00003129
Iteration 14/1000 | Loss: 0.00001732
Iteration 15/1000 | Loss: 0.00001719
Iteration 16/1000 | Loss: 0.00001708
Iteration 17/1000 | Loss: 0.00001699
Iteration 18/1000 | Loss: 0.00003092
Iteration 19/1000 | Loss: 0.00001782
Iteration 20/1000 | Loss: 0.00001685
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001675
Iteration 23/1000 | Loss: 0.00001675
Iteration 24/1000 | Loss: 0.00001675
Iteration 25/1000 | Loss: 0.00001674
Iteration 26/1000 | Loss: 0.00001674
Iteration 27/1000 | Loss: 0.00001674
Iteration 28/1000 | Loss: 0.00001674
Iteration 29/1000 | Loss: 0.00001674
Iteration 30/1000 | Loss: 0.00001674
Iteration 31/1000 | Loss: 0.00001673
Iteration 32/1000 | Loss: 0.00001673
Iteration 33/1000 | Loss: 0.00001673
Iteration 34/1000 | Loss: 0.00001672
Iteration 35/1000 | Loss: 0.00001672
Iteration 36/1000 | Loss: 0.00001672
Iteration 37/1000 | Loss: 0.00001672
Iteration 38/1000 | Loss: 0.00001672
Iteration 39/1000 | Loss: 0.00001671
Iteration 40/1000 | Loss: 0.00002570
Iteration 41/1000 | Loss: 0.00001672
Iteration 42/1000 | Loss: 0.00001672
Iteration 43/1000 | Loss: 0.00001672
Iteration 44/1000 | Loss: 0.00001671
Iteration 45/1000 | Loss: 0.00001671
Iteration 46/1000 | Loss: 0.00001670
Iteration 47/1000 | Loss: 0.00001670
Iteration 48/1000 | Loss: 0.00001669
Iteration 49/1000 | Loss: 0.00001669
Iteration 50/1000 | Loss: 0.00001668
Iteration 51/1000 | Loss: 0.00001668
Iteration 52/1000 | Loss: 0.00001668
Iteration 53/1000 | Loss: 0.00001668
Iteration 54/1000 | Loss: 0.00001668
Iteration 55/1000 | Loss: 0.00001668
Iteration 56/1000 | Loss: 0.00001668
Iteration 57/1000 | Loss: 0.00001668
Iteration 58/1000 | Loss: 0.00001668
Iteration 59/1000 | Loss: 0.00001667
Iteration 60/1000 | Loss: 0.00001666
Iteration 61/1000 | Loss: 0.00001663
Iteration 62/1000 | Loss: 0.00001661
Iteration 63/1000 | Loss: 0.00001661
Iteration 64/1000 | Loss: 0.00001661
Iteration 65/1000 | Loss: 0.00001661
Iteration 66/1000 | Loss: 0.00001661
Iteration 67/1000 | Loss: 0.00001661
Iteration 68/1000 | Loss: 0.00001661
Iteration 69/1000 | Loss: 0.00001660
Iteration 70/1000 | Loss: 0.00001660
Iteration 71/1000 | Loss: 0.00001660
Iteration 72/1000 | Loss: 0.00001660
Iteration 73/1000 | Loss: 0.00001660
Iteration 74/1000 | Loss: 0.00001659
Iteration 75/1000 | Loss: 0.00001659
Iteration 76/1000 | Loss: 0.00001659
Iteration 77/1000 | Loss: 0.00001659
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001658
Iteration 80/1000 | Loss: 0.00001658
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001658
Iteration 84/1000 | Loss: 0.00001658
Iteration 85/1000 | Loss: 0.00001658
Iteration 86/1000 | Loss: 0.00001658
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001657
Iteration 92/1000 | Loss: 0.00001657
Iteration 93/1000 | Loss: 0.00001657
Iteration 94/1000 | Loss: 0.00001657
Iteration 95/1000 | Loss: 0.00001657
Iteration 96/1000 | Loss: 0.00001657
Iteration 97/1000 | Loss: 0.00001657
Iteration 98/1000 | Loss: 0.00001657
Iteration 99/1000 | Loss: 0.00001657
Iteration 100/1000 | Loss: 0.00001657
Iteration 101/1000 | Loss: 0.00001657
Iteration 102/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.6574515029788017e-05, 1.6574515029788017e-05, 1.6574515029788017e-05, 1.6574515029788017e-05, 1.6574515029788017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6574515029788017e-05

Optimization complete. Final v2v error: 3.530059337615967 mm

Highest mean error: 3.985106945037842 mm for frame 117

Lowest mean error: 3.1184580326080322 mm for frame 90

Saving results

Total time: 48.45900821685791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486731
Iteration 2/25 | Loss: 0.00120925
Iteration 3/25 | Loss: 0.00112784
Iteration 4/25 | Loss: 0.00111978
Iteration 5/25 | Loss: 0.00111746
Iteration 6/25 | Loss: 0.00111746
Iteration 7/25 | Loss: 0.00111746
Iteration 8/25 | Loss: 0.00111746
Iteration 9/25 | Loss: 0.00111746
Iteration 10/25 | Loss: 0.00111746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011174647370353341, 0.0011174647370353341, 0.0011174647370353341, 0.0011174647370353341, 0.0011174647370353341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011174647370353341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37140751
Iteration 2/25 | Loss: 0.00229647
Iteration 3/25 | Loss: 0.00229646
Iteration 4/25 | Loss: 0.00229646
Iteration 5/25 | Loss: 0.00229645
Iteration 6/25 | Loss: 0.00229645
Iteration 7/25 | Loss: 0.00229645
Iteration 8/25 | Loss: 0.00229645
Iteration 9/25 | Loss: 0.00229645
Iteration 10/25 | Loss: 0.00229645
Iteration 11/25 | Loss: 0.00229645
Iteration 12/25 | Loss: 0.00229645
Iteration 13/25 | Loss: 0.00229645
Iteration 14/25 | Loss: 0.00229645
Iteration 15/25 | Loss: 0.00229645
Iteration 16/25 | Loss: 0.00229645
Iteration 17/25 | Loss: 0.00229645
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0022964526433497667, 0.0022964526433497667, 0.0022964526433497667, 0.0022964526433497667, 0.0022964526433497667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022964526433497667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229645
Iteration 2/1000 | Loss: 0.00004458
Iteration 3/1000 | Loss: 0.00002645
Iteration 4/1000 | Loss: 0.00002286
Iteration 5/1000 | Loss: 0.00002025
Iteration 6/1000 | Loss: 0.00001865
Iteration 7/1000 | Loss: 0.00001796
Iteration 8/1000 | Loss: 0.00001745
Iteration 9/1000 | Loss: 0.00001707
Iteration 10/1000 | Loss: 0.00001675
Iteration 11/1000 | Loss: 0.00001649
Iteration 12/1000 | Loss: 0.00001638
Iteration 13/1000 | Loss: 0.00001625
Iteration 14/1000 | Loss: 0.00001620
Iteration 15/1000 | Loss: 0.00001603
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001592
Iteration 18/1000 | Loss: 0.00001592
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001579
Iteration 21/1000 | Loss: 0.00001575
Iteration 22/1000 | Loss: 0.00001574
Iteration 23/1000 | Loss: 0.00001572
Iteration 24/1000 | Loss: 0.00001570
Iteration 25/1000 | Loss: 0.00001569
Iteration 26/1000 | Loss: 0.00001567
Iteration 27/1000 | Loss: 0.00001566
Iteration 28/1000 | Loss: 0.00001563
Iteration 29/1000 | Loss: 0.00001562
Iteration 30/1000 | Loss: 0.00001562
Iteration 31/1000 | Loss: 0.00001562
Iteration 32/1000 | Loss: 0.00001559
Iteration 33/1000 | Loss: 0.00001557
Iteration 34/1000 | Loss: 0.00001556
Iteration 35/1000 | Loss: 0.00001556
Iteration 36/1000 | Loss: 0.00001556
Iteration 37/1000 | Loss: 0.00001556
Iteration 38/1000 | Loss: 0.00001556
Iteration 39/1000 | Loss: 0.00001555
Iteration 40/1000 | Loss: 0.00001554
Iteration 41/1000 | Loss: 0.00001554
Iteration 42/1000 | Loss: 0.00001554
Iteration 43/1000 | Loss: 0.00001554
Iteration 44/1000 | Loss: 0.00001553
Iteration 45/1000 | Loss: 0.00001553
Iteration 46/1000 | Loss: 0.00001553
Iteration 47/1000 | Loss: 0.00001553
Iteration 48/1000 | Loss: 0.00001553
Iteration 49/1000 | Loss: 0.00001553
Iteration 50/1000 | Loss: 0.00001553
Iteration 51/1000 | Loss: 0.00001552
Iteration 52/1000 | Loss: 0.00001552
Iteration 53/1000 | Loss: 0.00001552
Iteration 54/1000 | Loss: 0.00001552
Iteration 55/1000 | Loss: 0.00001552
Iteration 56/1000 | Loss: 0.00001551
Iteration 57/1000 | Loss: 0.00001550
Iteration 58/1000 | Loss: 0.00001550
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001549
Iteration 61/1000 | Loss: 0.00001549
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001549
Iteration 64/1000 | Loss: 0.00001549
Iteration 65/1000 | Loss: 0.00001548
Iteration 66/1000 | Loss: 0.00001548
Iteration 67/1000 | Loss: 0.00001548
Iteration 68/1000 | Loss: 0.00001548
Iteration 69/1000 | Loss: 0.00001548
Iteration 70/1000 | Loss: 0.00001548
Iteration 71/1000 | Loss: 0.00001547
Iteration 72/1000 | Loss: 0.00001547
Iteration 73/1000 | Loss: 0.00001547
Iteration 74/1000 | Loss: 0.00001547
Iteration 75/1000 | Loss: 0.00001547
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001546
Iteration 78/1000 | Loss: 0.00001546
Iteration 79/1000 | Loss: 0.00001546
Iteration 80/1000 | Loss: 0.00001545
Iteration 81/1000 | Loss: 0.00001545
Iteration 82/1000 | Loss: 0.00001545
Iteration 83/1000 | Loss: 0.00001545
Iteration 84/1000 | Loss: 0.00001545
Iteration 85/1000 | Loss: 0.00001544
Iteration 86/1000 | Loss: 0.00001544
Iteration 87/1000 | Loss: 0.00001544
Iteration 88/1000 | Loss: 0.00001543
Iteration 89/1000 | Loss: 0.00001543
Iteration 90/1000 | Loss: 0.00001543
Iteration 91/1000 | Loss: 0.00001543
Iteration 92/1000 | Loss: 0.00001543
Iteration 93/1000 | Loss: 0.00001543
Iteration 94/1000 | Loss: 0.00001543
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001542
Iteration 98/1000 | Loss: 0.00001542
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001542
Iteration 101/1000 | Loss: 0.00001542
Iteration 102/1000 | Loss: 0.00001542
Iteration 103/1000 | Loss: 0.00001542
Iteration 104/1000 | Loss: 0.00001542
Iteration 105/1000 | Loss: 0.00001542
Iteration 106/1000 | Loss: 0.00001541
Iteration 107/1000 | Loss: 0.00001541
Iteration 108/1000 | Loss: 0.00001541
Iteration 109/1000 | Loss: 0.00001541
Iteration 110/1000 | Loss: 0.00001541
Iteration 111/1000 | Loss: 0.00001541
Iteration 112/1000 | Loss: 0.00001541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.541420897410717e-05, 1.541420897410717e-05, 1.541420897410717e-05, 1.541420897410717e-05, 1.541420897410717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.541420897410717e-05

Optimization complete. Final v2v error: 3.45660662651062 mm

Highest mean error: 4.148879528045654 mm for frame 218

Lowest mean error: 2.9975132942199707 mm for frame 17

Saving results

Total time: 43.37808108329773
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817016
Iteration 2/25 | Loss: 0.00114711
Iteration 3/25 | Loss: 0.00105043
Iteration 4/25 | Loss: 0.00104020
Iteration 5/25 | Loss: 0.00103810
Iteration 6/25 | Loss: 0.00103779
Iteration 7/25 | Loss: 0.00103779
Iteration 8/25 | Loss: 0.00103779
Iteration 9/25 | Loss: 0.00103779
Iteration 10/25 | Loss: 0.00103779
Iteration 11/25 | Loss: 0.00103779
Iteration 12/25 | Loss: 0.00103779
Iteration 13/25 | Loss: 0.00103779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001037789974361658, 0.001037789974361658, 0.001037789974361658, 0.001037789974361658, 0.001037789974361658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001037789974361658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19177520
Iteration 2/25 | Loss: 0.00226942
Iteration 3/25 | Loss: 0.00226942
Iteration 4/25 | Loss: 0.00226942
Iteration 5/25 | Loss: 0.00226941
Iteration 6/25 | Loss: 0.00226941
Iteration 7/25 | Loss: 0.00226941
Iteration 8/25 | Loss: 0.00226941
Iteration 9/25 | Loss: 0.00226941
Iteration 10/25 | Loss: 0.00226941
Iteration 11/25 | Loss: 0.00226941
Iteration 12/25 | Loss: 0.00226941
Iteration 13/25 | Loss: 0.00226941
Iteration 14/25 | Loss: 0.00226941
Iteration 15/25 | Loss: 0.00226941
Iteration 16/25 | Loss: 0.00226941
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022694135550409555, 0.0022694135550409555, 0.0022694135550409555, 0.0022694135550409555, 0.0022694135550409555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022694135550409555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226941
Iteration 2/1000 | Loss: 0.00003086
Iteration 3/1000 | Loss: 0.00001759
Iteration 4/1000 | Loss: 0.00001474
Iteration 5/1000 | Loss: 0.00001344
Iteration 6/1000 | Loss: 0.00001203
Iteration 7/1000 | Loss: 0.00001140
Iteration 8/1000 | Loss: 0.00001076
Iteration 9/1000 | Loss: 0.00001047
Iteration 10/1000 | Loss: 0.00001016
Iteration 11/1000 | Loss: 0.00000989
Iteration 12/1000 | Loss: 0.00000973
Iteration 13/1000 | Loss: 0.00000973
Iteration 14/1000 | Loss: 0.00000973
Iteration 15/1000 | Loss: 0.00000964
Iteration 16/1000 | Loss: 0.00000962
Iteration 17/1000 | Loss: 0.00000960
Iteration 18/1000 | Loss: 0.00000960
Iteration 19/1000 | Loss: 0.00000959
Iteration 20/1000 | Loss: 0.00000959
Iteration 21/1000 | Loss: 0.00000958
Iteration 22/1000 | Loss: 0.00000958
Iteration 23/1000 | Loss: 0.00000957
Iteration 24/1000 | Loss: 0.00000956
Iteration 25/1000 | Loss: 0.00000951
Iteration 26/1000 | Loss: 0.00000950
Iteration 27/1000 | Loss: 0.00000949
Iteration 28/1000 | Loss: 0.00000944
Iteration 29/1000 | Loss: 0.00000940
Iteration 30/1000 | Loss: 0.00000937
Iteration 31/1000 | Loss: 0.00000935
Iteration 32/1000 | Loss: 0.00000934
Iteration 33/1000 | Loss: 0.00000926
Iteration 34/1000 | Loss: 0.00000921
Iteration 35/1000 | Loss: 0.00000920
Iteration 36/1000 | Loss: 0.00000915
Iteration 37/1000 | Loss: 0.00000914
Iteration 38/1000 | Loss: 0.00000913
Iteration 39/1000 | Loss: 0.00000913
Iteration 40/1000 | Loss: 0.00000913
Iteration 41/1000 | Loss: 0.00000913
Iteration 42/1000 | Loss: 0.00000913
Iteration 43/1000 | Loss: 0.00000912
Iteration 44/1000 | Loss: 0.00000912
Iteration 45/1000 | Loss: 0.00000912
Iteration 46/1000 | Loss: 0.00000911
Iteration 47/1000 | Loss: 0.00000911
Iteration 48/1000 | Loss: 0.00000910
Iteration 49/1000 | Loss: 0.00000910
Iteration 50/1000 | Loss: 0.00000910
Iteration 51/1000 | Loss: 0.00000910
Iteration 52/1000 | Loss: 0.00000910
Iteration 53/1000 | Loss: 0.00000909
Iteration 54/1000 | Loss: 0.00000909
Iteration 55/1000 | Loss: 0.00000909
Iteration 56/1000 | Loss: 0.00000909
Iteration 57/1000 | Loss: 0.00000908
Iteration 58/1000 | Loss: 0.00000908
Iteration 59/1000 | Loss: 0.00000908
Iteration 60/1000 | Loss: 0.00000908
Iteration 61/1000 | Loss: 0.00000907
Iteration 62/1000 | Loss: 0.00000907
Iteration 63/1000 | Loss: 0.00000907
Iteration 64/1000 | Loss: 0.00000907
Iteration 65/1000 | Loss: 0.00000907
Iteration 66/1000 | Loss: 0.00000907
Iteration 67/1000 | Loss: 0.00000906
Iteration 68/1000 | Loss: 0.00000906
Iteration 69/1000 | Loss: 0.00000906
Iteration 70/1000 | Loss: 0.00000906
Iteration 71/1000 | Loss: 0.00000906
Iteration 72/1000 | Loss: 0.00000906
Iteration 73/1000 | Loss: 0.00000906
Iteration 74/1000 | Loss: 0.00000906
Iteration 75/1000 | Loss: 0.00000906
Iteration 76/1000 | Loss: 0.00000905
Iteration 77/1000 | Loss: 0.00000905
Iteration 78/1000 | Loss: 0.00000905
Iteration 79/1000 | Loss: 0.00000905
Iteration 80/1000 | Loss: 0.00000905
Iteration 81/1000 | Loss: 0.00000905
Iteration 82/1000 | Loss: 0.00000905
Iteration 83/1000 | Loss: 0.00000905
Iteration 84/1000 | Loss: 0.00000905
Iteration 85/1000 | Loss: 0.00000905
Iteration 86/1000 | Loss: 0.00000905
Iteration 87/1000 | Loss: 0.00000905
Iteration 88/1000 | Loss: 0.00000905
Iteration 89/1000 | Loss: 0.00000905
Iteration 90/1000 | Loss: 0.00000905
Iteration 91/1000 | Loss: 0.00000905
Iteration 92/1000 | Loss: 0.00000904
Iteration 93/1000 | Loss: 0.00000904
Iteration 94/1000 | Loss: 0.00000904
Iteration 95/1000 | Loss: 0.00000904
Iteration 96/1000 | Loss: 0.00000904
Iteration 97/1000 | Loss: 0.00000904
Iteration 98/1000 | Loss: 0.00000904
Iteration 99/1000 | Loss: 0.00000903
Iteration 100/1000 | Loss: 0.00000903
Iteration 101/1000 | Loss: 0.00000903
Iteration 102/1000 | Loss: 0.00000903
Iteration 103/1000 | Loss: 0.00000903
Iteration 104/1000 | Loss: 0.00000902
Iteration 105/1000 | Loss: 0.00000902
Iteration 106/1000 | Loss: 0.00000902
Iteration 107/1000 | Loss: 0.00000902
Iteration 108/1000 | Loss: 0.00000902
Iteration 109/1000 | Loss: 0.00000902
Iteration 110/1000 | Loss: 0.00000902
Iteration 111/1000 | Loss: 0.00000901
Iteration 112/1000 | Loss: 0.00000901
Iteration 113/1000 | Loss: 0.00000901
Iteration 114/1000 | Loss: 0.00000901
Iteration 115/1000 | Loss: 0.00000900
Iteration 116/1000 | Loss: 0.00000900
Iteration 117/1000 | Loss: 0.00000900
Iteration 118/1000 | Loss: 0.00000900
Iteration 119/1000 | Loss: 0.00000900
Iteration 120/1000 | Loss: 0.00000900
Iteration 121/1000 | Loss: 0.00000900
Iteration 122/1000 | Loss: 0.00000900
Iteration 123/1000 | Loss: 0.00000900
Iteration 124/1000 | Loss: 0.00000900
Iteration 125/1000 | Loss: 0.00000900
Iteration 126/1000 | Loss: 0.00000900
Iteration 127/1000 | Loss: 0.00000900
Iteration 128/1000 | Loss: 0.00000900
Iteration 129/1000 | Loss: 0.00000899
Iteration 130/1000 | Loss: 0.00000899
Iteration 131/1000 | Loss: 0.00000899
Iteration 132/1000 | Loss: 0.00000899
Iteration 133/1000 | Loss: 0.00000899
Iteration 134/1000 | Loss: 0.00000899
Iteration 135/1000 | Loss: 0.00000899
Iteration 136/1000 | Loss: 0.00000899
Iteration 137/1000 | Loss: 0.00000899
Iteration 138/1000 | Loss: 0.00000899
Iteration 139/1000 | Loss: 0.00000898
Iteration 140/1000 | Loss: 0.00000898
Iteration 141/1000 | Loss: 0.00000898
Iteration 142/1000 | Loss: 0.00000898
Iteration 143/1000 | Loss: 0.00000898
Iteration 144/1000 | Loss: 0.00000898
Iteration 145/1000 | Loss: 0.00000898
Iteration 146/1000 | Loss: 0.00000898
Iteration 147/1000 | Loss: 0.00000898
Iteration 148/1000 | Loss: 0.00000898
Iteration 149/1000 | Loss: 0.00000898
Iteration 150/1000 | Loss: 0.00000898
Iteration 151/1000 | Loss: 0.00000898
Iteration 152/1000 | Loss: 0.00000897
Iteration 153/1000 | Loss: 0.00000897
Iteration 154/1000 | Loss: 0.00000897
Iteration 155/1000 | Loss: 0.00000897
Iteration 156/1000 | Loss: 0.00000897
Iteration 157/1000 | Loss: 0.00000896
Iteration 158/1000 | Loss: 0.00000896
Iteration 159/1000 | Loss: 0.00000896
Iteration 160/1000 | Loss: 0.00000896
Iteration 161/1000 | Loss: 0.00000896
Iteration 162/1000 | Loss: 0.00000896
Iteration 163/1000 | Loss: 0.00000896
Iteration 164/1000 | Loss: 0.00000896
Iteration 165/1000 | Loss: 0.00000896
Iteration 166/1000 | Loss: 0.00000896
Iteration 167/1000 | Loss: 0.00000896
Iteration 168/1000 | Loss: 0.00000896
Iteration 169/1000 | Loss: 0.00000896
Iteration 170/1000 | Loss: 0.00000896
Iteration 171/1000 | Loss: 0.00000896
Iteration 172/1000 | Loss: 0.00000896
Iteration 173/1000 | Loss: 0.00000896
Iteration 174/1000 | Loss: 0.00000896
Iteration 175/1000 | Loss: 0.00000896
Iteration 176/1000 | Loss: 0.00000896
Iteration 177/1000 | Loss: 0.00000896
Iteration 178/1000 | Loss: 0.00000896
Iteration 179/1000 | Loss: 0.00000896
Iteration 180/1000 | Loss: 0.00000896
Iteration 181/1000 | Loss: 0.00000896
Iteration 182/1000 | Loss: 0.00000896
Iteration 183/1000 | Loss: 0.00000896
Iteration 184/1000 | Loss: 0.00000896
Iteration 185/1000 | Loss: 0.00000896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [8.962695574155077e-06, 8.962695574155077e-06, 8.962695574155077e-06, 8.962695574155077e-06, 8.962695574155077e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.962695574155077e-06

Optimization complete. Final v2v error: 2.5964226722717285 mm

Highest mean error: 2.8523058891296387 mm for frame 54

Lowest mean error: 2.3516337871551514 mm for frame 0

Saving results

Total time: 38.76669192314148
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00518914
Iteration 2/25 | Loss: 0.00119269
Iteration 3/25 | Loss: 0.00112280
Iteration 4/25 | Loss: 0.00110911
Iteration 5/25 | Loss: 0.00110400
Iteration 6/25 | Loss: 0.00110237
Iteration 7/25 | Loss: 0.00110227
Iteration 8/25 | Loss: 0.00110227
Iteration 9/25 | Loss: 0.00110227
Iteration 10/25 | Loss: 0.00110227
Iteration 11/25 | Loss: 0.00110227
Iteration 12/25 | Loss: 0.00110227
Iteration 13/25 | Loss: 0.00110227
Iteration 14/25 | Loss: 0.00110227
Iteration 15/25 | Loss: 0.00110227
Iteration 16/25 | Loss: 0.00110227
Iteration 17/25 | Loss: 0.00110227
Iteration 18/25 | Loss: 0.00110227
Iteration 19/25 | Loss: 0.00110227
Iteration 20/25 | Loss: 0.00110227
Iteration 21/25 | Loss: 0.00110227
Iteration 22/25 | Loss: 0.00110227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011022697435691953, 0.0011022697435691953, 0.0011022697435691953, 0.0011022697435691953, 0.0011022697435691953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011022697435691953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03448057
Iteration 2/25 | Loss: 0.00232565
Iteration 3/25 | Loss: 0.00232564
Iteration 4/25 | Loss: 0.00232564
Iteration 5/25 | Loss: 0.00232564
Iteration 6/25 | Loss: 0.00232564
Iteration 7/25 | Loss: 0.00232564
Iteration 8/25 | Loss: 0.00232564
Iteration 9/25 | Loss: 0.00232564
Iteration 10/25 | Loss: 0.00232564
Iteration 11/25 | Loss: 0.00232564
Iteration 12/25 | Loss: 0.00232564
Iteration 13/25 | Loss: 0.00232564
Iteration 14/25 | Loss: 0.00232564
Iteration 15/25 | Loss: 0.00232564
Iteration 16/25 | Loss: 0.00232564
Iteration 17/25 | Loss: 0.00232564
Iteration 18/25 | Loss: 0.00232564
Iteration 19/25 | Loss: 0.00232564
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0023256377317011356, 0.0023256377317011356, 0.0023256377317011356, 0.0023256377317011356, 0.0023256377317011356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023256377317011356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232564
Iteration 2/1000 | Loss: 0.00003427
Iteration 3/1000 | Loss: 0.00002023
Iteration 4/1000 | Loss: 0.00001633
Iteration 5/1000 | Loss: 0.00001456
Iteration 6/1000 | Loss: 0.00001363
Iteration 7/1000 | Loss: 0.00001324
Iteration 8/1000 | Loss: 0.00001270
Iteration 9/1000 | Loss: 0.00001244
Iteration 10/1000 | Loss: 0.00001236
Iteration 11/1000 | Loss: 0.00001204
Iteration 12/1000 | Loss: 0.00001178
Iteration 13/1000 | Loss: 0.00001155
Iteration 14/1000 | Loss: 0.00001145
Iteration 15/1000 | Loss: 0.00001142
Iteration 16/1000 | Loss: 0.00001137
Iteration 17/1000 | Loss: 0.00001133
Iteration 18/1000 | Loss: 0.00001130
Iteration 19/1000 | Loss: 0.00001130
Iteration 20/1000 | Loss: 0.00001129
Iteration 21/1000 | Loss: 0.00001122
Iteration 22/1000 | Loss: 0.00001121
Iteration 23/1000 | Loss: 0.00001116
Iteration 24/1000 | Loss: 0.00001115
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001114
Iteration 28/1000 | Loss: 0.00001112
Iteration 29/1000 | Loss: 0.00001112
Iteration 30/1000 | Loss: 0.00001112
Iteration 31/1000 | Loss: 0.00001111
Iteration 32/1000 | Loss: 0.00001110
Iteration 33/1000 | Loss: 0.00001110
Iteration 34/1000 | Loss: 0.00001110
Iteration 35/1000 | Loss: 0.00001110
Iteration 36/1000 | Loss: 0.00001110
Iteration 37/1000 | Loss: 0.00001110
Iteration 38/1000 | Loss: 0.00001110
Iteration 39/1000 | Loss: 0.00001110
Iteration 40/1000 | Loss: 0.00001110
Iteration 41/1000 | Loss: 0.00001109
Iteration 42/1000 | Loss: 0.00001109
Iteration 43/1000 | Loss: 0.00001109
Iteration 44/1000 | Loss: 0.00001109
Iteration 45/1000 | Loss: 0.00001109
Iteration 46/1000 | Loss: 0.00001109
Iteration 47/1000 | Loss: 0.00001109
Iteration 48/1000 | Loss: 0.00001109
Iteration 49/1000 | Loss: 0.00001108
Iteration 50/1000 | Loss: 0.00001108
Iteration 51/1000 | Loss: 0.00001108
Iteration 52/1000 | Loss: 0.00001107
Iteration 53/1000 | Loss: 0.00001107
Iteration 54/1000 | Loss: 0.00001107
Iteration 55/1000 | Loss: 0.00001107
Iteration 56/1000 | Loss: 0.00001107
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001107
Iteration 59/1000 | Loss: 0.00001106
Iteration 60/1000 | Loss: 0.00001106
Iteration 61/1000 | Loss: 0.00001106
Iteration 62/1000 | Loss: 0.00001106
Iteration 63/1000 | Loss: 0.00001106
Iteration 64/1000 | Loss: 0.00001106
Iteration 65/1000 | Loss: 0.00001106
Iteration 66/1000 | Loss: 0.00001106
Iteration 67/1000 | Loss: 0.00001106
Iteration 68/1000 | Loss: 0.00001106
Iteration 69/1000 | Loss: 0.00001105
Iteration 70/1000 | Loss: 0.00001105
Iteration 71/1000 | Loss: 0.00001105
Iteration 72/1000 | Loss: 0.00001105
Iteration 73/1000 | Loss: 0.00001104
Iteration 74/1000 | Loss: 0.00001104
Iteration 75/1000 | Loss: 0.00001104
Iteration 76/1000 | Loss: 0.00001104
Iteration 77/1000 | Loss: 0.00001104
Iteration 78/1000 | Loss: 0.00001104
Iteration 79/1000 | Loss: 0.00001103
Iteration 80/1000 | Loss: 0.00001103
Iteration 81/1000 | Loss: 0.00001103
Iteration 82/1000 | Loss: 0.00001103
Iteration 83/1000 | Loss: 0.00001103
Iteration 84/1000 | Loss: 0.00001103
Iteration 85/1000 | Loss: 0.00001103
Iteration 86/1000 | Loss: 0.00001103
Iteration 87/1000 | Loss: 0.00001103
Iteration 88/1000 | Loss: 0.00001103
Iteration 89/1000 | Loss: 0.00001103
Iteration 90/1000 | Loss: 0.00001103
Iteration 91/1000 | Loss: 0.00001103
Iteration 92/1000 | Loss: 0.00001103
Iteration 93/1000 | Loss: 0.00001103
Iteration 94/1000 | Loss: 0.00001103
Iteration 95/1000 | Loss: 0.00001103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.1030135283363052e-05, 1.1030135283363052e-05, 1.1030135283363052e-05, 1.1030135283363052e-05, 1.1030135283363052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1030135283363052e-05

Optimization complete. Final v2v error: 2.8498177528381348 mm

Highest mean error: 3.2699663639068604 mm for frame 142

Lowest mean error: 2.6676523685455322 mm for frame 46

Saving results

Total time: 35.47523212432861
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835630
Iteration 2/25 | Loss: 0.00150029
Iteration 3/25 | Loss: 0.00127268
Iteration 4/25 | Loss: 0.00125704
Iteration 5/25 | Loss: 0.00125461
Iteration 6/25 | Loss: 0.00125410
Iteration 7/25 | Loss: 0.00125410
Iteration 8/25 | Loss: 0.00125410
Iteration 9/25 | Loss: 0.00125410
Iteration 10/25 | Loss: 0.00125410
Iteration 11/25 | Loss: 0.00125410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012541045434772968, 0.0012541045434772968, 0.0012541045434772968, 0.0012541045434772968, 0.0012541045434772968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012541045434772968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11145306
Iteration 2/25 | Loss: 0.00184727
Iteration 3/25 | Loss: 0.00184722
Iteration 4/25 | Loss: 0.00184722
Iteration 5/25 | Loss: 0.00184721
Iteration 6/25 | Loss: 0.00184721
Iteration 7/25 | Loss: 0.00184721
Iteration 8/25 | Loss: 0.00184721
Iteration 9/25 | Loss: 0.00184721
Iteration 10/25 | Loss: 0.00184721
Iteration 11/25 | Loss: 0.00184721
Iteration 12/25 | Loss: 0.00184721
Iteration 13/25 | Loss: 0.00184721
Iteration 14/25 | Loss: 0.00184721
Iteration 15/25 | Loss: 0.00184721
Iteration 16/25 | Loss: 0.00184721
Iteration 17/25 | Loss: 0.00184721
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018472131341695786, 0.0018472131341695786, 0.0018472131341695786, 0.0018472131341695786, 0.0018472131341695786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018472131341695786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184721
Iteration 2/1000 | Loss: 0.00004921
Iteration 3/1000 | Loss: 0.00003442
Iteration 4/1000 | Loss: 0.00003024
Iteration 5/1000 | Loss: 0.00002791
Iteration 6/1000 | Loss: 0.00002685
Iteration 7/1000 | Loss: 0.00002618
Iteration 8/1000 | Loss: 0.00002545
Iteration 9/1000 | Loss: 0.00002508
Iteration 10/1000 | Loss: 0.00002454
Iteration 11/1000 | Loss: 0.00002414
Iteration 12/1000 | Loss: 0.00002378
Iteration 13/1000 | Loss: 0.00002352
Iteration 14/1000 | Loss: 0.00002321
Iteration 15/1000 | Loss: 0.00002296
Iteration 16/1000 | Loss: 0.00002279
Iteration 17/1000 | Loss: 0.00002262
Iteration 18/1000 | Loss: 0.00002260
Iteration 19/1000 | Loss: 0.00002253
Iteration 20/1000 | Loss: 0.00002251
Iteration 21/1000 | Loss: 0.00002251
Iteration 22/1000 | Loss: 0.00002250
Iteration 23/1000 | Loss: 0.00002250
Iteration 24/1000 | Loss: 0.00002247
Iteration 25/1000 | Loss: 0.00002245
Iteration 26/1000 | Loss: 0.00002234
Iteration 27/1000 | Loss: 0.00002232
Iteration 28/1000 | Loss: 0.00002232
Iteration 29/1000 | Loss: 0.00002231
Iteration 30/1000 | Loss: 0.00002231
Iteration 31/1000 | Loss: 0.00002231
Iteration 32/1000 | Loss: 0.00002231
Iteration 33/1000 | Loss: 0.00002231
Iteration 34/1000 | Loss: 0.00002231
Iteration 35/1000 | Loss: 0.00002231
Iteration 36/1000 | Loss: 0.00002231
Iteration 37/1000 | Loss: 0.00002231
Iteration 38/1000 | Loss: 0.00002230
Iteration 39/1000 | Loss: 0.00002230
Iteration 40/1000 | Loss: 0.00002230
Iteration 41/1000 | Loss: 0.00002229
Iteration 42/1000 | Loss: 0.00002228
Iteration 43/1000 | Loss: 0.00002228
Iteration 44/1000 | Loss: 0.00002228
Iteration 45/1000 | Loss: 0.00002228
Iteration 46/1000 | Loss: 0.00002225
Iteration 47/1000 | Loss: 0.00002225
Iteration 48/1000 | Loss: 0.00002224
Iteration 49/1000 | Loss: 0.00002224
Iteration 50/1000 | Loss: 0.00002223
Iteration 51/1000 | Loss: 0.00002223
Iteration 52/1000 | Loss: 0.00002222
Iteration 53/1000 | Loss: 0.00002221
Iteration 54/1000 | Loss: 0.00002220
Iteration 55/1000 | Loss: 0.00002219
Iteration 56/1000 | Loss: 0.00002219
Iteration 57/1000 | Loss: 0.00002218
Iteration 58/1000 | Loss: 0.00002218
Iteration 59/1000 | Loss: 0.00002217
Iteration 60/1000 | Loss: 0.00002214
Iteration 61/1000 | Loss: 0.00002214
Iteration 62/1000 | Loss: 0.00002214
Iteration 63/1000 | Loss: 0.00002214
Iteration 64/1000 | Loss: 0.00002213
Iteration 65/1000 | Loss: 0.00002213
Iteration 66/1000 | Loss: 0.00002213
Iteration 67/1000 | Loss: 0.00002213
Iteration 68/1000 | Loss: 0.00002212
Iteration 69/1000 | Loss: 0.00002212
Iteration 70/1000 | Loss: 0.00002211
Iteration 71/1000 | Loss: 0.00002211
Iteration 72/1000 | Loss: 0.00002211
Iteration 73/1000 | Loss: 0.00002211
Iteration 74/1000 | Loss: 0.00002211
Iteration 75/1000 | Loss: 0.00002211
Iteration 76/1000 | Loss: 0.00002211
Iteration 77/1000 | Loss: 0.00002211
Iteration 78/1000 | Loss: 0.00002211
Iteration 79/1000 | Loss: 0.00002211
Iteration 80/1000 | Loss: 0.00002211
Iteration 81/1000 | Loss: 0.00002211
Iteration 82/1000 | Loss: 0.00002210
Iteration 83/1000 | Loss: 0.00002210
Iteration 84/1000 | Loss: 0.00002210
Iteration 85/1000 | Loss: 0.00002210
Iteration 86/1000 | Loss: 0.00002209
Iteration 87/1000 | Loss: 0.00002209
Iteration 88/1000 | Loss: 0.00002209
Iteration 89/1000 | Loss: 0.00002209
Iteration 90/1000 | Loss: 0.00002209
Iteration 91/1000 | Loss: 0.00002209
Iteration 92/1000 | Loss: 0.00002208
Iteration 93/1000 | Loss: 0.00002208
Iteration 94/1000 | Loss: 0.00002208
Iteration 95/1000 | Loss: 0.00002208
Iteration 96/1000 | Loss: 0.00002208
Iteration 97/1000 | Loss: 0.00002207
Iteration 98/1000 | Loss: 0.00002207
Iteration 99/1000 | Loss: 0.00002206
Iteration 100/1000 | Loss: 0.00002206
Iteration 101/1000 | Loss: 0.00002206
Iteration 102/1000 | Loss: 0.00002206
Iteration 103/1000 | Loss: 0.00002205
Iteration 104/1000 | Loss: 0.00002205
Iteration 105/1000 | Loss: 0.00002205
Iteration 106/1000 | Loss: 0.00002205
Iteration 107/1000 | Loss: 0.00002204
Iteration 108/1000 | Loss: 0.00002204
Iteration 109/1000 | Loss: 0.00002204
Iteration 110/1000 | Loss: 0.00002204
Iteration 111/1000 | Loss: 0.00002204
Iteration 112/1000 | Loss: 0.00002203
Iteration 113/1000 | Loss: 0.00002203
Iteration 114/1000 | Loss: 0.00002203
Iteration 115/1000 | Loss: 0.00002203
Iteration 116/1000 | Loss: 0.00002203
Iteration 117/1000 | Loss: 0.00002203
Iteration 118/1000 | Loss: 0.00002203
Iteration 119/1000 | Loss: 0.00002203
Iteration 120/1000 | Loss: 0.00002203
Iteration 121/1000 | Loss: 0.00002203
Iteration 122/1000 | Loss: 0.00002203
Iteration 123/1000 | Loss: 0.00002203
Iteration 124/1000 | Loss: 0.00002203
Iteration 125/1000 | Loss: 0.00002203
Iteration 126/1000 | Loss: 0.00002203
Iteration 127/1000 | Loss: 0.00002203
Iteration 128/1000 | Loss: 0.00002203
Iteration 129/1000 | Loss: 0.00002203
Iteration 130/1000 | Loss: 0.00002203
Iteration 131/1000 | Loss: 0.00002203
Iteration 132/1000 | Loss: 0.00002203
Iteration 133/1000 | Loss: 0.00002203
Iteration 134/1000 | Loss: 0.00002203
Iteration 135/1000 | Loss: 0.00002203
Iteration 136/1000 | Loss: 0.00002203
Iteration 137/1000 | Loss: 0.00002203
Iteration 138/1000 | Loss: 0.00002203
Iteration 139/1000 | Loss: 0.00002203
Iteration 140/1000 | Loss: 0.00002203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.2028702005627565e-05, 2.2028702005627565e-05, 2.2028702005627565e-05, 2.2028702005627565e-05, 2.2028702005627565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2028702005627565e-05

Optimization complete. Final v2v error: 4.13307523727417 mm

Highest mean error: 4.4316487312316895 mm for frame 82

Lowest mean error: 3.7860260009765625 mm for frame 2

Saving results

Total time: 42.07896971702576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433199
Iteration 2/25 | Loss: 0.00118661
Iteration 3/25 | Loss: 0.00109347
Iteration 4/25 | Loss: 0.00107592
Iteration 5/25 | Loss: 0.00107021
Iteration 6/25 | Loss: 0.00106851
Iteration 7/25 | Loss: 0.00106851
Iteration 8/25 | Loss: 0.00106851
Iteration 9/25 | Loss: 0.00106851
Iteration 10/25 | Loss: 0.00106851
Iteration 11/25 | Loss: 0.00106851
Iteration 12/25 | Loss: 0.00106851
Iteration 13/25 | Loss: 0.00106851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001068514073267579, 0.001068514073267579, 0.001068514073267579, 0.001068514073267579, 0.001068514073267579]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001068514073267579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.40849066
Iteration 2/25 | Loss: 0.00234047
Iteration 3/25 | Loss: 0.00234046
Iteration 4/25 | Loss: 0.00234046
Iteration 5/25 | Loss: 0.00234046
Iteration 6/25 | Loss: 0.00234046
Iteration 7/25 | Loss: 0.00234046
Iteration 8/25 | Loss: 0.00234046
Iteration 9/25 | Loss: 0.00234046
Iteration 10/25 | Loss: 0.00234046
Iteration 11/25 | Loss: 0.00234046
Iteration 12/25 | Loss: 0.00234046
Iteration 13/25 | Loss: 0.00234046
Iteration 14/25 | Loss: 0.00234046
Iteration 15/25 | Loss: 0.00234046
Iteration 16/25 | Loss: 0.00234046
Iteration 17/25 | Loss: 0.00234046
Iteration 18/25 | Loss: 0.00234046
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0023404585663229227, 0.0023404585663229227, 0.0023404585663229227, 0.0023404585663229227, 0.0023404585663229227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023404585663229227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234046
Iteration 2/1000 | Loss: 0.00003264
Iteration 3/1000 | Loss: 0.00002091
Iteration 4/1000 | Loss: 0.00001819
Iteration 5/1000 | Loss: 0.00001602
Iteration 6/1000 | Loss: 0.00001476
Iteration 7/1000 | Loss: 0.00001405
Iteration 8/1000 | Loss: 0.00001352
Iteration 9/1000 | Loss: 0.00001314
Iteration 10/1000 | Loss: 0.00001285
Iteration 11/1000 | Loss: 0.00001262
Iteration 12/1000 | Loss: 0.00001242
Iteration 13/1000 | Loss: 0.00001240
Iteration 14/1000 | Loss: 0.00001232
Iteration 15/1000 | Loss: 0.00001232
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001229
Iteration 18/1000 | Loss: 0.00001228
Iteration 19/1000 | Loss: 0.00001227
Iteration 20/1000 | Loss: 0.00001222
Iteration 21/1000 | Loss: 0.00001222
Iteration 22/1000 | Loss: 0.00001221
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001209
Iteration 25/1000 | Loss: 0.00001208
Iteration 26/1000 | Loss: 0.00001202
Iteration 27/1000 | Loss: 0.00001196
Iteration 28/1000 | Loss: 0.00001195
Iteration 29/1000 | Loss: 0.00001188
Iteration 30/1000 | Loss: 0.00001181
Iteration 31/1000 | Loss: 0.00001181
Iteration 32/1000 | Loss: 0.00001180
Iteration 33/1000 | Loss: 0.00001180
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001179
Iteration 37/1000 | Loss: 0.00001179
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001178
Iteration 41/1000 | Loss: 0.00001178
Iteration 42/1000 | Loss: 0.00001177
Iteration 43/1000 | Loss: 0.00001177
Iteration 44/1000 | Loss: 0.00001176
Iteration 45/1000 | Loss: 0.00001176
Iteration 46/1000 | Loss: 0.00001176
Iteration 47/1000 | Loss: 0.00001176
Iteration 48/1000 | Loss: 0.00001175
Iteration 49/1000 | Loss: 0.00001175
Iteration 50/1000 | Loss: 0.00001175
Iteration 51/1000 | Loss: 0.00001175
Iteration 52/1000 | Loss: 0.00001175
Iteration 53/1000 | Loss: 0.00001175
Iteration 54/1000 | Loss: 0.00001175
Iteration 55/1000 | Loss: 0.00001174
Iteration 56/1000 | Loss: 0.00001174
Iteration 57/1000 | Loss: 0.00001174
Iteration 58/1000 | Loss: 0.00001174
Iteration 59/1000 | Loss: 0.00001174
Iteration 60/1000 | Loss: 0.00001174
Iteration 61/1000 | Loss: 0.00001173
Iteration 62/1000 | Loss: 0.00001173
Iteration 63/1000 | Loss: 0.00001173
Iteration 64/1000 | Loss: 0.00001173
Iteration 65/1000 | Loss: 0.00001172
Iteration 66/1000 | Loss: 0.00001172
Iteration 67/1000 | Loss: 0.00001172
Iteration 68/1000 | Loss: 0.00001172
Iteration 69/1000 | Loss: 0.00001171
Iteration 70/1000 | Loss: 0.00001171
Iteration 71/1000 | Loss: 0.00001171
Iteration 72/1000 | Loss: 0.00001171
Iteration 73/1000 | Loss: 0.00001171
Iteration 74/1000 | Loss: 0.00001171
Iteration 75/1000 | Loss: 0.00001171
Iteration 76/1000 | Loss: 0.00001170
Iteration 77/1000 | Loss: 0.00001170
Iteration 78/1000 | Loss: 0.00001170
Iteration 79/1000 | Loss: 0.00001170
Iteration 80/1000 | Loss: 0.00001170
Iteration 81/1000 | Loss: 0.00001170
Iteration 82/1000 | Loss: 0.00001170
Iteration 83/1000 | Loss: 0.00001169
Iteration 84/1000 | Loss: 0.00001169
Iteration 85/1000 | Loss: 0.00001169
Iteration 86/1000 | Loss: 0.00001169
Iteration 87/1000 | Loss: 0.00001168
Iteration 88/1000 | Loss: 0.00001168
Iteration 89/1000 | Loss: 0.00001168
Iteration 90/1000 | Loss: 0.00001168
Iteration 91/1000 | Loss: 0.00001167
Iteration 92/1000 | Loss: 0.00001167
Iteration 93/1000 | Loss: 0.00001167
Iteration 94/1000 | Loss: 0.00001166
Iteration 95/1000 | Loss: 0.00001166
Iteration 96/1000 | Loss: 0.00001165
Iteration 97/1000 | Loss: 0.00001165
Iteration 98/1000 | Loss: 0.00001164
Iteration 99/1000 | Loss: 0.00001164
Iteration 100/1000 | Loss: 0.00001163
Iteration 101/1000 | Loss: 0.00001163
Iteration 102/1000 | Loss: 0.00001163
Iteration 103/1000 | Loss: 0.00001163
Iteration 104/1000 | Loss: 0.00001163
Iteration 105/1000 | Loss: 0.00001163
Iteration 106/1000 | Loss: 0.00001162
Iteration 107/1000 | Loss: 0.00001162
Iteration 108/1000 | Loss: 0.00001162
Iteration 109/1000 | Loss: 0.00001162
Iteration 110/1000 | Loss: 0.00001161
Iteration 111/1000 | Loss: 0.00001161
Iteration 112/1000 | Loss: 0.00001161
Iteration 113/1000 | Loss: 0.00001160
Iteration 114/1000 | Loss: 0.00001160
Iteration 115/1000 | Loss: 0.00001160
Iteration 116/1000 | Loss: 0.00001160
Iteration 117/1000 | Loss: 0.00001159
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001159
Iteration 120/1000 | Loss: 0.00001159
Iteration 121/1000 | Loss: 0.00001159
Iteration 122/1000 | Loss: 0.00001159
Iteration 123/1000 | Loss: 0.00001158
Iteration 124/1000 | Loss: 0.00001158
Iteration 125/1000 | Loss: 0.00001158
Iteration 126/1000 | Loss: 0.00001158
Iteration 127/1000 | Loss: 0.00001158
Iteration 128/1000 | Loss: 0.00001158
Iteration 129/1000 | Loss: 0.00001158
Iteration 130/1000 | Loss: 0.00001158
Iteration 131/1000 | Loss: 0.00001158
Iteration 132/1000 | Loss: 0.00001158
Iteration 133/1000 | Loss: 0.00001158
Iteration 134/1000 | Loss: 0.00001158
Iteration 135/1000 | Loss: 0.00001157
Iteration 136/1000 | Loss: 0.00001157
Iteration 137/1000 | Loss: 0.00001157
Iteration 138/1000 | Loss: 0.00001157
Iteration 139/1000 | Loss: 0.00001157
Iteration 140/1000 | Loss: 0.00001157
Iteration 141/1000 | Loss: 0.00001157
Iteration 142/1000 | Loss: 0.00001157
Iteration 143/1000 | Loss: 0.00001157
Iteration 144/1000 | Loss: 0.00001157
Iteration 145/1000 | Loss: 0.00001157
Iteration 146/1000 | Loss: 0.00001157
Iteration 147/1000 | Loss: 0.00001157
Iteration 148/1000 | Loss: 0.00001157
Iteration 149/1000 | Loss: 0.00001157
Iteration 150/1000 | Loss: 0.00001156
Iteration 151/1000 | Loss: 0.00001156
Iteration 152/1000 | Loss: 0.00001156
Iteration 153/1000 | Loss: 0.00001156
Iteration 154/1000 | Loss: 0.00001156
Iteration 155/1000 | Loss: 0.00001156
Iteration 156/1000 | Loss: 0.00001156
Iteration 157/1000 | Loss: 0.00001156
Iteration 158/1000 | Loss: 0.00001156
Iteration 159/1000 | Loss: 0.00001156
Iteration 160/1000 | Loss: 0.00001156
Iteration 161/1000 | Loss: 0.00001156
Iteration 162/1000 | Loss: 0.00001156
Iteration 163/1000 | Loss: 0.00001156
Iteration 164/1000 | Loss: 0.00001156
Iteration 165/1000 | Loss: 0.00001155
Iteration 166/1000 | Loss: 0.00001155
Iteration 167/1000 | Loss: 0.00001155
Iteration 168/1000 | Loss: 0.00001155
Iteration 169/1000 | Loss: 0.00001155
Iteration 170/1000 | Loss: 0.00001155
Iteration 171/1000 | Loss: 0.00001155
Iteration 172/1000 | Loss: 0.00001155
Iteration 173/1000 | Loss: 0.00001155
Iteration 174/1000 | Loss: 0.00001155
Iteration 175/1000 | Loss: 0.00001155
Iteration 176/1000 | Loss: 0.00001155
Iteration 177/1000 | Loss: 0.00001155
Iteration 178/1000 | Loss: 0.00001155
Iteration 179/1000 | Loss: 0.00001155
Iteration 180/1000 | Loss: 0.00001155
Iteration 181/1000 | Loss: 0.00001155
Iteration 182/1000 | Loss: 0.00001155
Iteration 183/1000 | Loss: 0.00001155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.1549763257789891e-05, 1.1549763257789891e-05, 1.1549763257789891e-05, 1.1549763257789891e-05, 1.1549763257789891e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1549763257789891e-05

Optimization complete. Final v2v error: 2.956573724746704 mm

Highest mean error: 3.4113965034484863 mm for frame 92

Lowest mean error: 2.688478708267212 mm for frame 0

Saving results

Total time: 47.45071864128113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785556
Iteration 2/25 | Loss: 0.00126850
Iteration 3/25 | Loss: 0.00113602
Iteration 4/25 | Loss: 0.00111780
Iteration 5/25 | Loss: 0.00111963
Iteration 6/25 | Loss: 0.00110357
Iteration 7/25 | Loss: 0.00108455
Iteration 8/25 | Loss: 0.00108098
Iteration 9/25 | Loss: 0.00108009
Iteration 10/25 | Loss: 0.00107976
Iteration 11/25 | Loss: 0.00107964
Iteration 12/25 | Loss: 0.00107963
Iteration 13/25 | Loss: 0.00107963
Iteration 14/25 | Loss: 0.00107963
Iteration 15/25 | Loss: 0.00107963
Iteration 16/25 | Loss: 0.00107963
Iteration 17/25 | Loss: 0.00107962
Iteration 18/25 | Loss: 0.00107962
Iteration 19/25 | Loss: 0.00107962
Iteration 20/25 | Loss: 0.00107962
Iteration 21/25 | Loss: 0.00107962
Iteration 22/25 | Loss: 0.00107962
Iteration 23/25 | Loss: 0.00107962
Iteration 24/25 | Loss: 0.00107962
Iteration 25/25 | Loss: 0.00107962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14888513
Iteration 2/25 | Loss: 0.00275144
Iteration 3/25 | Loss: 0.00275144
Iteration 4/25 | Loss: 0.00275144
Iteration 5/25 | Loss: 0.00275144
Iteration 6/25 | Loss: 0.00275144
Iteration 7/25 | Loss: 0.00275144
Iteration 8/25 | Loss: 0.00275143
Iteration 9/25 | Loss: 0.00275143
Iteration 10/25 | Loss: 0.00275143
Iteration 11/25 | Loss: 0.00275143
Iteration 12/25 | Loss: 0.00275143
Iteration 13/25 | Loss: 0.00275143
Iteration 14/25 | Loss: 0.00275143
Iteration 15/25 | Loss: 0.00275143
Iteration 16/25 | Loss: 0.00275143
Iteration 17/25 | Loss: 0.00275143
Iteration 18/25 | Loss: 0.00275143
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0027514335233718157, 0.0027514335233718157, 0.0027514335233718157, 0.0027514335233718157, 0.0027514335233718157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027514335233718157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275143
Iteration 2/1000 | Loss: 0.00005593
Iteration 3/1000 | Loss: 0.00003951
Iteration 4/1000 | Loss: 0.00003324
Iteration 5/1000 | Loss: 0.00003038
Iteration 6/1000 | Loss: 0.00002877
Iteration 7/1000 | Loss: 0.00002739
Iteration 8/1000 | Loss: 0.00002675
Iteration 9/1000 | Loss: 0.00002617
Iteration 10/1000 | Loss: 0.00002574
Iteration 11/1000 | Loss: 0.00002544
Iteration 12/1000 | Loss: 0.00002520
Iteration 13/1000 | Loss: 0.00002496
Iteration 14/1000 | Loss: 0.00002472
Iteration 15/1000 | Loss: 0.00002453
Iteration 16/1000 | Loss: 0.00002436
Iteration 17/1000 | Loss: 0.00002433
Iteration 18/1000 | Loss: 0.00002432
Iteration 19/1000 | Loss: 0.00002428
Iteration 20/1000 | Loss: 0.00002427
Iteration 21/1000 | Loss: 0.00002427
Iteration 22/1000 | Loss: 0.00002426
Iteration 23/1000 | Loss: 0.00002423
Iteration 24/1000 | Loss: 0.00002422
Iteration 25/1000 | Loss: 0.00002421
Iteration 26/1000 | Loss: 0.00002417
Iteration 27/1000 | Loss: 0.00002416
Iteration 28/1000 | Loss: 0.00002416
Iteration 29/1000 | Loss: 0.00002416
Iteration 30/1000 | Loss: 0.00002416
Iteration 31/1000 | Loss: 0.00002416
Iteration 32/1000 | Loss: 0.00002415
Iteration 33/1000 | Loss: 0.00002415
Iteration 34/1000 | Loss: 0.00002415
Iteration 35/1000 | Loss: 0.00002414
Iteration 36/1000 | Loss: 0.00002414
Iteration 37/1000 | Loss: 0.00002413
Iteration 38/1000 | Loss: 0.00002413
Iteration 39/1000 | Loss: 0.00002413
Iteration 40/1000 | Loss: 0.00002412
Iteration 41/1000 | Loss: 0.00002412
Iteration 42/1000 | Loss: 0.00002412
Iteration 43/1000 | Loss: 0.00002411
Iteration 44/1000 | Loss: 0.00002411
Iteration 45/1000 | Loss: 0.00002411
Iteration 46/1000 | Loss: 0.00002411
Iteration 47/1000 | Loss: 0.00002411
Iteration 48/1000 | Loss: 0.00002410
Iteration 49/1000 | Loss: 0.00002410
Iteration 50/1000 | Loss: 0.00002410
Iteration 51/1000 | Loss: 0.00002410
Iteration 52/1000 | Loss: 0.00002409
Iteration 53/1000 | Loss: 0.00002409
Iteration 54/1000 | Loss: 0.00002409
Iteration 55/1000 | Loss: 0.00002409
Iteration 56/1000 | Loss: 0.00002408
Iteration 57/1000 | Loss: 0.00002408
Iteration 58/1000 | Loss: 0.00002408
Iteration 59/1000 | Loss: 0.00002407
Iteration 60/1000 | Loss: 0.00002407
Iteration 61/1000 | Loss: 0.00002407
Iteration 62/1000 | Loss: 0.00002407
Iteration 63/1000 | Loss: 0.00002407
Iteration 64/1000 | Loss: 0.00002407
Iteration 65/1000 | Loss: 0.00002407
Iteration 66/1000 | Loss: 0.00002407
Iteration 67/1000 | Loss: 0.00002407
Iteration 68/1000 | Loss: 0.00002407
Iteration 69/1000 | Loss: 0.00002406
Iteration 70/1000 | Loss: 0.00002406
Iteration 71/1000 | Loss: 0.00002406
Iteration 72/1000 | Loss: 0.00002406
Iteration 73/1000 | Loss: 0.00002406
Iteration 74/1000 | Loss: 0.00002406
Iteration 75/1000 | Loss: 0.00002406
Iteration 76/1000 | Loss: 0.00002406
Iteration 77/1000 | Loss: 0.00002405
Iteration 78/1000 | Loss: 0.00002405
Iteration 79/1000 | Loss: 0.00002405
Iteration 80/1000 | Loss: 0.00002405
Iteration 81/1000 | Loss: 0.00002405
Iteration 82/1000 | Loss: 0.00002405
Iteration 83/1000 | Loss: 0.00002405
Iteration 84/1000 | Loss: 0.00002405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.4052740627666935e-05, 2.4052740627666935e-05, 2.4052740627666935e-05, 2.4052740627666935e-05, 2.4052740627666935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4052740627666935e-05

Optimization complete. Final v2v error: 4.201239109039307 mm

Highest mean error: 4.668580055236816 mm for frame 58

Lowest mean error: 3.4147584438323975 mm for frame 0

Saving results

Total time: 54.50012159347534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447109
Iteration 2/25 | Loss: 0.00123051
Iteration 3/25 | Loss: 0.00114327
Iteration 4/25 | Loss: 0.00112822
Iteration 5/25 | Loss: 0.00112406
Iteration 6/25 | Loss: 0.00112283
Iteration 7/25 | Loss: 0.00112275
Iteration 8/25 | Loss: 0.00112275
Iteration 9/25 | Loss: 0.00112275
Iteration 10/25 | Loss: 0.00112275
Iteration 11/25 | Loss: 0.00112275
Iteration 12/25 | Loss: 0.00112275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011227517388761044, 0.0011227517388761044, 0.0011227517388761044, 0.0011227517388761044, 0.0011227517388761044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011227517388761044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24422026
Iteration 2/25 | Loss: 0.00233216
Iteration 3/25 | Loss: 0.00233215
Iteration 4/25 | Loss: 0.00233215
Iteration 5/25 | Loss: 0.00233215
Iteration 6/25 | Loss: 0.00233215
Iteration 7/25 | Loss: 0.00233215
Iteration 8/25 | Loss: 0.00233215
Iteration 9/25 | Loss: 0.00233215
Iteration 10/25 | Loss: 0.00233215
Iteration 11/25 | Loss: 0.00233215
Iteration 12/25 | Loss: 0.00233215
Iteration 13/25 | Loss: 0.00233215
Iteration 14/25 | Loss: 0.00233215
Iteration 15/25 | Loss: 0.00233215
Iteration 16/25 | Loss: 0.00233215
Iteration 17/25 | Loss: 0.00233215
Iteration 18/25 | Loss: 0.00233215
Iteration 19/25 | Loss: 0.00233215
Iteration 20/25 | Loss: 0.00233215
Iteration 21/25 | Loss: 0.00233215
Iteration 22/25 | Loss: 0.00233215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0023321500048041344, 0.0023321500048041344, 0.0023321500048041344, 0.0023321500048041344, 0.0023321500048041344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023321500048041344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233215
Iteration 2/1000 | Loss: 0.00003381
Iteration 3/1000 | Loss: 0.00002552
Iteration 4/1000 | Loss: 0.00002267
Iteration 5/1000 | Loss: 0.00002063
Iteration 6/1000 | Loss: 0.00001985
Iteration 7/1000 | Loss: 0.00001907
Iteration 8/1000 | Loss: 0.00001872
Iteration 9/1000 | Loss: 0.00001840
Iteration 10/1000 | Loss: 0.00001806
Iteration 11/1000 | Loss: 0.00001780
Iteration 12/1000 | Loss: 0.00001773
Iteration 13/1000 | Loss: 0.00001768
Iteration 14/1000 | Loss: 0.00001764
Iteration 15/1000 | Loss: 0.00001759
Iteration 16/1000 | Loss: 0.00001754
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001727
Iteration 19/1000 | Loss: 0.00001722
Iteration 20/1000 | Loss: 0.00001715
Iteration 21/1000 | Loss: 0.00001714
Iteration 22/1000 | Loss: 0.00001714
Iteration 23/1000 | Loss: 0.00001710
Iteration 24/1000 | Loss: 0.00001700
Iteration 25/1000 | Loss: 0.00001698
Iteration 26/1000 | Loss: 0.00001697
Iteration 27/1000 | Loss: 0.00001697
Iteration 28/1000 | Loss: 0.00001696
Iteration 29/1000 | Loss: 0.00001694
Iteration 30/1000 | Loss: 0.00001694
Iteration 31/1000 | Loss: 0.00001693
Iteration 32/1000 | Loss: 0.00001692
Iteration 33/1000 | Loss: 0.00001692
Iteration 34/1000 | Loss: 0.00001692
Iteration 35/1000 | Loss: 0.00001691
Iteration 36/1000 | Loss: 0.00001691
Iteration 37/1000 | Loss: 0.00001691
Iteration 38/1000 | Loss: 0.00001688
Iteration 39/1000 | Loss: 0.00001688
Iteration 40/1000 | Loss: 0.00001688
Iteration 41/1000 | Loss: 0.00001688
Iteration 42/1000 | Loss: 0.00001688
Iteration 43/1000 | Loss: 0.00001688
Iteration 44/1000 | Loss: 0.00001688
Iteration 45/1000 | Loss: 0.00001688
Iteration 46/1000 | Loss: 0.00001688
Iteration 47/1000 | Loss: 0.00001688
Iteration 48/1000 | Loss: 0.00001687
Iteration 49/1000 | Loss: 0.00001686
Iteration 50/1000 | Loss: 0.00001685
Iteration 51/1000 | Loss: 0.00001685
Iteration 52/1000 | Loss: 0.00001684
Iteration 53/1000 | Loss: 0.00001684
Iteration 54/1000 | Loss: 0.00001684
Iteration 55/1000 | Loss: 0.00001684
Iteration 56/1000 | Loss: 0.00001684
Iteration 57/1000 | Loss: 0.00001684
Iteration 58/1000 | Loss: 0.00001683
Iteration 59/1000 | Loss: 0.00001683
Iteration 60/1000 | Loss: 0.00001683
Iteration 61/1000 | Loss: 0.00001683
Iteration 62/1000 | Loss: 0.00001682
Iteration 63/1000 | Loss: 0.00001682
Iteration 64/1000 | Loss: 0.00001682
Iteration 65/1000 | Loss: 0.00001681
Iteration 66/1000 | Loss: 0.00001680
Iteration 67/1000 | Loss: 0.00001680
Iteration 68/1000 | Loss: 0.00001680
Iteration 69/1000 | Loss: 0.00001679
Iteration 70/1000 | Loss: 0.00001679
Iteration 71/1000 | Loss: 0.00001679
Iteration 72/1000 | Loss: 0.00001679
Iteration 73/1000 | Loss: 0.00001679
Iteration 74/1000 | Loss: 0.00001678
Iteration 75/1000 | Loss: 0.00001678
Iteration 76/1000 | Loss: 0.00001677
Iteration 77/1000 | Loss: 0.00001677
Iteration 78/1000 | Loss: 0.00001676
Iteration 79/1000 | Loss: 0.00001676
Iteration 80/1000 | Loss: 0.00001676
Iteration 81/1000 | Loss: 0.00001676
Iteration 82/1000 | Loss: 0.00001675
Iteration 83/1000 | Loss: 0.00001675
Iteration 84/1000 | Loss: 0.00001675
Iteration 85/1000 | Loss: 0.00001675
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001674
Iteration 88/1000 | Loss: 0.00001674
Iteration 89/1000 | Loss: 0.00001673
Iteration 90/1000 | Loss: 0.00001673
Iteration 91/1000 | Loss: 0.00001673
Iteration 92/1000 | Loss: 0.00001673
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001672
Iteration 95/1000 | Loss: 0.00001672
Iteration 96/1000 | Loss: 0.00001672
Iteration 97/1000 | Loss: 0.00001671
Iteration 98/1000 | Loss: 0.00001671
Iteration 99/1000 | Loss: 0.00001671
Iteration 100/1000 | Loss: 0.00001670
Iteration 101/1000 | Loss: 0.00001670
Iteration 102/1000 | Loss: 0.00001670
Iteration 103/1000 | Loss: 0.00001670
Iteration 104/1000 | Loss: 0.00001670
Iteration 105/1000 | Loss: 0.00001670
Iteration 106/1000 | Loss: 0.00001669
Iteration 107/1000 | Loss: 0.00001669
Iteration 108/1000 | Loss: 0.00001669
Iteration 109/1000 | Loss: 0.00001669
Iteration 110/1000 | Loss: 0.00001669
Iteration 111/1000 | Loss: 0.00001669
Iteration 112/1000 | Loss: 0.00001669
Iteration 113/1000 | Loss: 0.00001669
Iteration 114/1000 | Loss: 0.00001669
Iteration 115/1000 | Loss: 0.00001669
Iteration 116/1000 | Loss: 0.00001668
Iteration 117/1000 | Loss: 0.00001668
Iteration 118/1000 | Loss: 0.00001668
Iteration 119/1000 | Loss: 0.00001667
Iteration 120/1000 | Loss: 0.00001667
Iteration 121/1000 | Loss: 0.00001667
Iteration 122/1000 | Loss: 0.00001667
Iteration 123/1000 | Loss: 0.00001667
Iteration 124/1000 | Loss: 0.00001666
Iteration 125/1000 | Loss: 0.00001666
Iteration 126/1000 | Loss: 0.00001666
Iteration 127/1000 | Loss: 0.00001666
Iteration 128/1000 | Loss: 0.00001666
Iteration 129/1000 | Loss: 0.00001666
Iteration 130/1000 | Loss: 0.00001666
Iteration 131/1000 | Loss: 0.00001666
Iteration 132/1000 | Loss: 0.00001665
Iteration 133/1000 | Loss: 0.00001665
Iteration 134/1000 | Loss: 0.00001665
Iteration 135/1000 | Loss: 0.00001665
Iteration 136/1000 | Loss: 0.00001665
Iteration 137/1000 | Loss: 0.00001665
Iteration 138/1000 | Loss: 0.00001665
Iteration 139/1000 | Loss: 0.00001665
Iteration 140/1000 | Loss: 0.00001665
Iteration 141/1000 | Loss: 0.00001665
Iteration 142/1000 | Loss: 0.00001664
Iteration 143/1000 | Loss: 0.00001664
Iteration 144/1000 | Loss: 0.00001664
Iteration 145/1000 | Loss: 0.00001664
Iteration 146/1000 | Loss: 0.00001664
Iteration 147/1000 | Loss: 0.00001664
Iteration 148/1000 | Loss: 0.00001664
Iteration 149/1000 | Loss: 0.00001664
Iteration 150/1000 | Loss: 0.00001664
Iteration 151/1000 | Loss: 0.00001664
Iteration 152/1000 | Loss: 0.00001664
Iteration 153/1000 | Loss: 0.00001664
Iteration 154/1000 | Loss: 0.00001664
Iteration 155/1000 | Loss: 0.00001664
Iteration 156/1000 | Loss: 0.00001664
Iteration 157/1000 | Loss: 0.00001663
Iteration 158/1000 | Loss: 0.00001663
Iteration 159/1000 | Loss: 0.00001663
Iteration 160/1000 | Loss: 0.00001663
Iteration 161/1000 | Loss: 0.00001663
Iteration 162/1000 | Loss: 0.00001663
Iteration 163/1000 | Loss: 0.00001663
Iteration 164/1000 | Loss: 0.00001663
Iteration 165/1000 | Loss: 0.00001663
Iteration 166/1000 | Loss: 0.00001663
Iteration 167/1000 | Loss: 0.00001663
Iteration 168/1000 | Loss: 0.00001663
Iteration 169/1000 | Loss: 0.00001663
Iteration 170/1000 | Loss: 0.00001663
Iteration 171/1000 | Loss: 0.00001663
Iteration 172/1000 | Loss: 0.00001663
Iteration 173/1000 | Loss: 0.00001663
Iteration 174/1000 | Loss: 0.00001663
Iteration 175/1000 | Loss: 0.00001663
Iteration 176/1000 | Loss: 0.00001663
Iteration 177/1000 | Loss: 0.00001663
Iteration 178/1000 | Loss: 0.00001663
Iteration 179/1000 | Loss: 0.00001663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.663229886617046e-05, 1.663229886617046e-05, 1.663229886617046e-05, 1.663229886617046e-05, 1.663229886617046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.663229886617046e-05

Optimization complete. Final v2v error: 3.51625657081604 mm

Highest mean error: 4.1492438316345215 mm for frame 71

Lowest mean error: 2.946962356567383 mm for frame 34

Saving results

Total time: 42.25427007675171
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426412
Iteration 2/25 | Loss: 0.00129281
Iteration 3/25 | Loss: 0.00120042
Iteration 4/25 | Loss: 0.00118351
Iteration 5/25 | Loss: 0.00117628
Iteration 6/25 | Loss: 0.00117389
Iteration 7/25 | Loss: 0.00117387
Iteration 8/25 | Loss: 0.00117387
Iteration 9/25 | Loss: 0.00117387
Iteration 10/25 | Loss: 0.00117387
Iteration 11/25 | Loss: 0.00117387
Iteration 12/25 | Loss: 0.00117387
Iteration 13/25 | Loss: 0.00117387
Iteration 14/25 | Loss: 0.00117387
Iteration 15/25 | Loss: 0.00117387
Iteration 16/25 | Loss: 0.00117387
Iteration 17/25 | Loss: 0.00117387
Iteration 18/25 | Loss: 0.00117387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011738709872588515, 0.0011738709872588515, 0.0011738709872588515, 0.0011738709872588515, 0.0011738709872588515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011738709872588515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60528135
Iteration 2/25 | Loss: 0.00236011
Iteration 3/25 | Loss: 0.00236011
Iteration 4/25 | Loss: 0.00236011
Iteration 5/25 | Loss: 0.00236011
Iteration 6/25 | Loss: 0.00236010
Iteration 7/25 | Loss: 0.00236010
Iteration 8/25 | Loss: 0.00236010
Iteration 9/25 | Loss: 0.00236010
Iteration 10/25 | Loss: 0.00236010
Iteration 11/25 | Loss: 0.00236010
Iteration 12/25 | Loss: 0.00236010
Iteration 13/25 | Loss: 0.00236010
Iteration 14/25 | Loss: 0.00236010
Iteration 15/25 | Loss: 0.00236010
Iteration 16/25 | Loss: 0.00236010
Iteration 17/25 | Loss: 0.00236010
Iteration 18/25 | Loss: 0.00236010
Iteration 19/25 | Loss: 0.00236010
Iteration 20/25 | Loss: 0.00236010
Iteration 21/25 | Loss: 0.00236010
Iteration 22/25 | Loss: 0.00236010
Iteration 23/25 | Loss: 0.00236010
Iteration 24/25 | Loss: 0.00236010
Iteration 25/25 | Loss: 0.00236010

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00236010
Iteration 2/1000 | Loss: 0.00003081
Iteration 3/1000 | Loss: 0.00002517
Iteration 4/1000 | Loss: 0.00002307
Iteration 5/1000 | Loss: 0.00002226
Iteration 6/1000 | Loss: 0.00002165
Iteration 7/1000 | Loss: 0.00002114
Iteration 8/1000 | Loss: 0.00002078
Iteration 9/1000 | Loss: 0.00002051
Iteration 10/1000 | Loss: 0.00002023
Iteration 11/1000 | Loss: 0.00002004
Iteration 12/1000 | Loss: 0.00001998
Iteration 13/1000 | Loss: 0.00001989
Iteration 14/1000 | Loss: 0.00001986
Iteration 15/1000 | Loss: 0.00001984
Iteration 16/1000 | Loss: 0.00001983
Iteration 17/1000 | Loss: 0.00001981
Iteration 18/1000 | Loss: 0.00001979
Iteration 19/1000 | Loss: 0.00001972
Iteration 20/1000 | Loss: 0.00001972
Iteration 21/1000 | Loss: 0.00001971
Iteration 22/1000 | Loss: 0.00001961
Iteration 23/1000 | Loss: 0.00001960
Iteration 24/1000 | Loss: 0.00001959
Iteration 25/1000 | Loss: 0.00001957
Iteration 26/1000 | Loss: 0.00001957
Iteration 27/1000 | Loss: 0.00001956
Iteration 28/1000 | Loss: 0.00001955
Iteration 29/1000 | Loss: 0.00001954
Iteration 30/1000 | Loss: 0.00001953
Iteration 31/1000 | Loss: 0.00001951
Iteration 32/1000 | Loss: 0.00001949
Iteration 33/1000 | Loss: 0.00001949
Iteration 34/1000 | Loss: 0.00001949
Iteration 35/1000 | Loss: 0.00001948
Iteration 36/1000 | Loss: 0.00001948
Iteration 37/1000 | Loss: 0.00001948
Iteration 38/1000 | Loss: 0.00001948
Iteration 39/1000 | Loss: 0.00001947
Iteration 40/1000 | Loss: 0.00001946
Iteration 41/1000 | Loss: 0.00001946
Iteration 42/1000 | Loss: 0.00001945
Iteration 43/1000 | Loss: 0.00001945
Iteration 44/1000 | Loss: 0.00001945
Iteration 45/1000 | Loss: 0.00001945
Iteration 46/1000 | Loss: 0.00001945
Iteration 47/1000 | Loss: 0.00001945
Iteration 48/1000 | Loss: 0.00001945
Iteration 49/1000 | Loss: 0.00001945
Iteration 50/1000 | Loss: 0.00001945
Iteration 51/1000 | Loss: 0.00001945
Iteration 52/1000 | Loss: 0.00001944
Iteration 53/1000 | Loss: 0.00001944
Iteration 54/1000 | Loss: 0.00001944
Iteration 55/1000 | Loss: 0.00001944
Iteration 56/1000 | Loss: 0.00001944
Iteration 57/1000 | Loss: 0.00001944
Iteration 58/1000 | Loss: 0.00001944
Iteration 59/1000 | Loss: 0.00001943
Iteration 60/1000 | Loss: 0.00001943
Iteration 61/1000 | Loss: 0.00001943
Iteration 62/1000 | Loss: 0.00001943
Iteration 63/1000 | Loss: 0.00001943
Iteration 64/1000 | Loss: 0.00001943
Iteration 65/1000 | Loss: 0.00001943
Iteration 66/1000 | Loss: 0.00001943
Iteration 67/1000 | Loss: 0.00001942
Iteration 68/1000 | Loss: 0.00001942
Iteration 69/1000 | Loss: 0.00001942
Iteration 70/1000 | Loss: 0.00001942
Iteration 71/1000 | Loss: 0.00001942
Iteration 72/1000 | Loss: 0.00001942
Iteration 73/1000 | Loss: 0.00001942
Iteration 74/1000 | Loss: 0.00001942
Iteration 75/1000 | Loss: 0.00001942
Iteration 76/1000 | Loss: 0.00001942
Iteration 77/1000 | Loss: 0.00001942
Iteration 78/1000 | Loss: 0.00001942
Iteration 79/1000 | Loss: 0.00001942
Iteration 80/1000 | Loss: 0.00001942
Iteration 81/1000 | Loss: 0.00001942
Iteration 82/1000 | Loss: 0.00001942
Iteration 83/1000 | Loss: 0.00001942
Iteration 84/1000 | Loss: 0.00001942
Iteration 85/1000 | Loss: 0.00001942
Iteration 86/1000 | Loss: 0.00001942
Iteration 87/1000 | Loss: 0.00001942
Iteration 88/1000 | Loss: 0.00001942
Iteration 89/1000 | Loss: 0.00001942
Iteration 90/1000 | Loss: 0.00001942
Iteration 91/1000 | Loss: 0.00001942
Iteration 92/1000 | Loss: 0.00001942
Iteration 93/1000 | Loss: 0.00001942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.9421177057665773e-05, 1.9421177057665773e-05, 1.9421177057665773e-05, 1.9421177057665773e-05, 1.9421177057665773e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9421177057665773e-05

Optimization complete. Final v2v error: 3.8348875045776367 mm

Highest mean error: 4.073894500732422 mm for frame 158

Lowest mean error: 3.5974314212799072 mm for frame 0

Saving results

Total time: 37.57849097251892
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00654254
Iteration 2/25 | Loss: 0.00124184
Iteration 3/25 | Loss: 0.00113265
Iteration 4/25 | Loss: 0.00111925
Iteration 5/25 | Loss: 0.00111634
Iteration 6/25 | Loss: 0.00111634
Iteration 7/25 | Loss: 0.00111634
Iteration 8/25 | Loss: 0.00111634
Iteration 9/25 | Loss: 0.00111634
Iteration 10/25 | Loss: 0.00111634
Iteration 11/25 | Loss: 0.00111634
Iteration 12/25 | Loss: 0.00111634
Iteration 13/25 | Loss: 0.00111634
Iteration 14/25 | Loss: 0.00111634
Iteration 15/25 | Loss: 0.00111634
Iteration 16/25 | Loss: 0.00111634
Iteration 17/25 | Loss: 0.00111634
Iteration 18/25 | Loss: 0.00111634
Iteration 19/25 | Loss: 0.00111634
Iteration 20/25 | Loss: 0.00111634
Iteration 21/25 | Loss: 0.00111634
Iteration 22/25 | Loss: 0.00111634
Iteration 23/25 | Loss: 0.00111634
Iteration 24/25 | Loss: 0.00111634
Iteration 25/25 | Loss: 0.00111634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.87290716
Iteration 2/25 | Loss: 0.00231424
Iteration 3/25 | Loss: 0.00231423
Iteration 4/25 | Loss: 0.00231423
Iteration 5/25 | Loss: 0.00231423
Iteration 6/25 | Loss: 0.00231423
Iteration 7/25 | Loss: 0.00231423
Iteration 8/25 | Loss: 0.00231423
Iteration 9/25 | Loss: 0.00231423
Iteration 10/25 | Loss: 0.00231423
Iteration 11/25 | Loss: 0.00231423
Iteration 12/25 | Loss: 0.00231423
Iteration 13/25 | Loss: 0.00231423
Iteration 14/25 | Loss: 0.00231423
Iteration 15/25 | Loss: 0.00231423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0023142294958233833, 0.0023142294958233833, 0.0023142294958233833, 0.0023142294958233833, 0.0023142294958233833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023142294958233833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231423
Iteration 2/1000 | Loss: 0.00004038
Iteration 3/1000 | Loss: 0.00002676
Iteration 4/1000 | Loss: 0.00002186
Iteration 5/1000 | Loss: 0.00001947
Iteration 6/1000 | Loss: 0.00001804
Iteration 7/1000 | Loss: 0.00001703
Iteration 8/1000 | Loss: 0.00001648
Iteration 9/1000 | Loss: 0.00001604
Iteration 10/1000 | Loss: 0.00001568
Iteration 11/1000 | Loss: 0.00001531
Iteration 12/1000 | Loss: 0.00001506
Iteration 13/1000 | Loss: 0.00001493
Iteration 14/1000 | Loss: 0.00001488
Iteration 15/1000 | Loss: 0.00001477
Iteration 16/1000 | Loss: 0.00001474
Iteration 17/1000 | Loss: 0.00001471
Iteration 18/1000 | Loss: 0.00001471
Iteration 19/1000 | Loss: 0.00001462
Iteration 20/1000 | Loss: 0.00001459
Iteration 21/1000 | Loss: 0.00001453
Iteration 22/1000 | Loss: 0.00001445
Iteration 23/1000 | Loss: 0.00001442
Iteration 24/1000 | Loss: 0.00001442
Iteration 25/1000 | Loss: 0.00001441
Iteration 26/1000 | Loss: 0.00001441
Iteration 27/1000 | Loss: 0.00001440
Iteration 28/1000 | Loss: 0.00001440
Iteration 29/1000 | Loss: 0.00001439
Iteration 30/1000 | Loss: 0.00001437
Iteration 31/1000 | Loss: 0.00001435
Iteration 32/1000 | Loss: 0.00001434
Iteration 33/1000 | Loss: 0.00001433
Iteration 34/1000 | Loss: 0.00001431
Iteration 35/1000 | Loss: 0.00001430
Iteration 36/1000 | Loss: 0.00001429
Iteration 37/1000 | Loss: 0.00001429
Iteration 38/1000 | Loss: 0.00001427
Iteration 39/1000 | Loss: 0.00001427
Iteration 40/1000 | Loss: 0.00001426
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00001425
Iteration 43/1000 | Loss: 0.00001425
Iteration 44/1000 | Loss: 0.00001425
Iteration 45/1000 | Loss: 0.00001423
Iteration 46/1000 | Loss: 0.00001423
Iteration 47/1000 | Loss: 0.00001422
Iteration 48/1000 | Loss: 0.00001422
Iteration 49/1000 | Loss: 0.00001421
Iteration 50/1000 | Loss: 0.00001420
Iteration 51/1000 | Loss: 0.00001420
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001419
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001418
Iteration 56/1000 | Loss: 0.00001417
Iteration 57/1000 | Loss: 0.00001417
Iteration 58/1000 | Loss: 0.00001416
Iteration 59/1000 | Loss: 0.00001416
Iteration 60/1000 | Loss: 0.00001416
Iteration 61/1000 | Loss: 0.00001415
Iteration 62/1000 | Loss: 0.00001415
Iteration 63/1000 | Loss: 0.00001415
Iteration 64/1000 | Loss: 0.00001415
Iteration 65/1000 | Loss: 0.00001415
Iteration 66/1000 | Loss: 0.00001414
Iteration 67/1000 | Loss: 0.00001414
Iteration 68/1000 | Loss: 0.00001413
Iteration 69/1000 | Loss: 0.00001413
Iteration 70/1000 | Loss: 0.00001412
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001412
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001411
Iteration 75/1000 | Loss: 0.00001411
Iteration 76/1000 | Loss: 0.00001410
Iteration 77/1000 | Loss: 0.00001410
Iteration 78/1000 | Loss: 0.00001410
Iteration 79/1000 | Loss: 0.00001410
Iteration 80/1000 | Loss: 0.00001410
Iteration 81/1000 | Loss: 0.00001410
Iteration 82/1000 | Loss: 0.00001410
Iteration 83/1000 | Loss: 0.00001409
Iteration 84/1000 | Loss: 0.00001409
Iteration 85/1000 | Loss: 0.00001409
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001408
Iteration 89/1000 | Loss: 0.00001408
Iteration 90/1000 | Loss: 0.00001408
Iteration 91/1000 | Loss: 0.00001407
Iteration 92/1000 | Loss: 0.00001407
Iteration 93/1000 | Loss: 0.00001407
Iteration 94/1000 | Loss: 0.00001407
Iteration 95/1000 | Loss: 0.00001407
Iteration 96/1000 | Loss: 0.00001406
Iteration 97/1000 | Loss: 0.00001406
Iteration 98/1000 | Loss: 0.00001406
Iteration 99/1000 | Loss: 0.00001405
Iteration 100/1000 | Loss: 0.00001405
Iteration 101/1000 | Loss: 0.00001404
Iteration 102/1000 | Loss: 0.00001404
Iteration 103/1000 | Loss: 0.00001403
Iteration 104/1000 | Loss: 0.00001403
Iteration 105/1000 | Loss: 0.00001402
Iteration 106/1000 | Loss: 0.00001401
Iteration 107/1000 | Loss: 0.00001401
Iteration 108/1000 | Loss: 0.00001401
Iteration 109/1000 | Loss: 0.00001401
Iteration 110/1000 | Loss: 0.00001401
Iteration 111/1000 | Loss: 0.00001401
Iteration 112/1000 | Loss: 0.00001401
Iteration 113/1000 | Loss: 0.00001400
Iteration 114/1000 | Loss: 0.00001400
Iteration 115/1000 | Loss: 0.00001400
Iteration 116/1000 | Loss: 0.00001399
Iteration 117/1000 | Loss: 0.00001399
Iteration 118/1000 | Loss: 0.00001398
Iteration 119/1000 | Loss: 0.00001398
Iteration 120/1000 | Loss: 0.00001398
Iteration 121/1000 | Loss: 0.00001398
Iteration 122/1000 | Loss: 0.00001398
Iteration 123/1000 | Loss: 0.00001398
Iteration 124/1000 | Loss: 0.00001397
Iteration 125/1000 | Loss: 0.00001397
Iteration 126/1000 | Loss: 0.00001397
Iteration 127/1000 | Loss: 0.00001397
Iteration 128/1000 | Loss: 0.00001397
Iteration 129/1000 | Loss: 0.00001397
Iteration 130/1000 | Loss: 0.00001396
Iteration 131/1000 | Loss: 0.00001396
Iteration 132/1000 | Loss: 0.00001396
Iteration 133/1000 | Loss: 0.00001396
Iteration 134/1000 | Loss: 0.00001395
Iteration 135/1000 | Loss: 0.00001395
Iteration 136/1000 | Loss: 0.00001395
Iteration 137/1000 | Loss: 0.00001394
Iteration 138/1000 | Loss: 0.00001394
Iteration 139/1000 | Loss: 0.00001393
Iteration 140/1000 | Loss: 0.00001393
Iteration 141/1000 | Loss: 0.00001393
Iteration 142/1000 | Loss: 0.00001393
Iteration 143/1000 | Loss: 0.00001393
Iteration 144/1000 | Loss: 0.00001393
Iteration 145/1000 | Loss: 0.00001392
Iteration 146/1000 | Loss: 0.00001392
Iteration 147/1000 | Loss: 0.00001392
Iteration 148/1000 | Loss: 0.00001392
Iteration 149/1000 | Loss: 0.00001392
Iteration 150/1000 | Loss: 0.00001391
Iteration 151/1000 | Loss: 0.00001391
Iteration 152/1000 | Loss: 0.00001391
Iteration 153/1000 | Loss: 0.00001391
Iteration 154/1000 | Loss: 0.00001391
Iteration 155/1000 | Loss: 0.00001391
Iteration 156/1000 | Loss: 0.00001391
Iteration 157/1000 | Loss: 0.00001391
Iteration 158/1000 | Loss: 0.00001391
Iteration 159/1000 | Loss: 0.00001391
Iteration 160/1000 | Loss: 0.00001391
Iteration 161/1000 | Loss: 0.00001391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.3910500456404407e-05, 1.3910500456404407e-05, 1.3910500456404407e-05, 1.3910500456404407e-05, 1.3910500456404407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3910500456404407e-05

Optimization complete. Final v2v error: 3.2130966186523438 mm

Highest mean error: 3.7607598304748535 mm for frame 197

Lowest mean error: 2.762859344482422 mm for frame 91

Saving results

Total time: 48.58433485031128
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_0231/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_0231/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935719
Iteration 2/25 | Loss: 0.00138380
Iteration 3/25 | Loss: 0.00117820
Iteration 4/25 | Loss: 0.00115604
Iteration 5/25 | Loss: 0.00115187
Iteration 6/25 | Loss: 0.00115117
Iteration 7/25 | Loss: 0.00115117
Iteration 8/25 | Loss: 0.00115117
Iteration 9/25 | Loss: 0.00115117
Iteration 10/25 | Loss: 0.00115117
Iteration 11/25 | Loss: 0.00115117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001151166739873588, 0.001151166739873588, 0.001151166739873588, 0.001151166739873588, 0.001151166739873588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001151166739873588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29805195
Iteration 2/25 | Loss: 0.00314642
Iteration 3/25 | Loss: 0.00314642
Iteration 4/25 | Loss: 0.00314642
Iteration 5/25 | Loss: 0.00314642
Iteration 6/25 | Loss: 0.00314642
Iteration 7/25 | Loss: 0.00314642
Iteration 8/25 | Loss: 0.00314642
Iteration 9/25 | Loss: 0.00314642
Iteration 10/25 | Loss: 0.00314642
Iteration 11/25 | Loss: 0.00314642
Iteration 12/25 | Loss: 0.00314642
Iteration 13/25 | Loss: 0.00314642
Iteration 14/25 | Loss: 0.00314642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0031464167404919863, 0.0031464167404919863, 0.0031464167404919863, 0.0031464167404919863, 0.0031464167404919863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031464167404919863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00314642
Iteration 2/1000 | Loss: 0.00005439
Iteration 3/1000 | Loss: 0.00002723
Iteration 4/1000 | Loss: 0.00002253
Iteration 5/1000 | Loss: 0.00002057
Iteration 6/1000 | Loss: 0.00001967
Iteration 7/1000 | Loss: 0.00001883
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001799
Iteration 10/1000 | Loss: 0.00001764
Iteration 11/1000 | Loss: 0.00001740
Iteration 12/1000 | Loss: 0.00001719
Iteration 13/1000 | Loss: 0.00001697
Iteration 14/1000 | Loss: 0.00001685
Iteration 15/1000 | Loss: 0.00001681
Iteration 16/1000 | Loss: 0.00001671
Iteration 17/1000 | Loss: 0.00001666
Iteration 18/1000 | Loss: 0.00001665
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001663
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001652
Iteration 23/1000 | Loss: 0.00001647
Iteration 24/1000 | Loss: 0.00001642
Iteration 25/1000 | Loss: 0.00001642
Iteration 26/1000 | Loss: 0.00001641
Iteration 27/1000 | Loss: 0.00001637
Iteration 28/1000 | Loss: 0.00001636
Iteration 29/1000 | Loss: 0.00001634
Iteration 30/1000 | Loss: 0.00001634
Iteration 31/1000 | Loss: 0.00001634
Iteration 32/1000 | Loss: 0.00001634
Iteration 33/1000 | Loss: 0.00001634
Iteration 34/1000 | Loss: 0.00001634
Iteration 35/1000 | Loss: 0.00001634
Iteration 36/1000 | Loss: 0.00001634
Iteration 37/1000 | Loss: 0.00001634
Iteration 38/1000 | Loss: 0.00001634
Iteration 39/1000 | Loss: 0.00001633
Iteration 40/1000 | Loss: 0.00001633
Iteration 41/1000 | Loss: 0.00001633
Iteration 42/1000 | Loss: 0.00001633
Iteration 43/1000 | Loss: 0.00001633
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001633
Iteration 46/1000 | Loss: 0.00001633
Iteration 47/1000 | Loss: 0.00001633
Iteration 48/1000 | Loss: 0.00001633
Iteration 49/1000 | Loss: 0.00001632
Iteration 50/1000 | Loss: 0.00001632
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001628
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001624
Iteration 57/1000 | Loss: 0.00001624
Iteration 58/1000 | Loss: 0.00001623
Iteration 59/1000 | Loss: 0.00001623
Iteration 60/1000 | Loss: 0.00001623
Iteration 61/1000 | Loss: 0.00001622
Iteration 62/1000 | Loss: 0.00001622
Iteration 63/1000 | Loss: 0.00001622
Iteration 64/1000 | Loss: 0.00001621
Iteration 65/1000 | Loss: 0.00001621
Iteration 66/1000 | Loss: 0.00001621
Iteration 67/1000 | Loss: 0.00001620
Iteration 68/1000 | Loss: 0.00001620
Iteration 69/1000 | Loss: 0.00001620
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001617
Iteration 72/1000 | Loss: 0.00001617
Iteration 73/1000 | Loss: 0.00001617
Iteration 74/1000 | Loss: 0.00001617
Iteration 75/1000 | Loss: 0.00001617
Iteration 76/1000 | Loss: 0.00001616
Iteration 77/1000 | Loss: 0.00001616
Iteration 78/1000 | Loss: 0.00001616
Iteration 79/1000 | Loss: 0.00001616
Iteration 80/1000 | Loss: 0.00001616
Iteration 81/1000 | Loss: 0.00001616
Iteration 82/1000 | Loss: 0.00001616
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001616
Iteration 86/1000 | Loss: 0.00001616
Iteration 87/1000 | Loss: 0.00001615
Iteration 88/1000 | Loss: 0.00001615
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00001615
Iteration 91/1000 | Loss: 0.00001615
Iteration 92/1000 | Loss: 0.00001614
Iteration 93/1000 | Loss: 0.00001613
Iteration 94/1000 | Loss: 0.00001613
Iteration 95/1000 | Loss: 0.00001612
Iteration 96/1000 | Loss: 0.00001611
Iteration 97/1000 | Loss: 0.00001611
Iteration 98/1000 | Loss: 0.00001611
Iteration 99/1000 | Loss: 0.00001607
Iteration 100/1000 | Loss: 0.00001607
Iteration 101/1000 | Loss: 0.00001607
Iteration 102/1000 | Loss: 0.00001607
Iteration 103/1000 | Loss: 0.00001607
Iteration 104/1000 | Loss: 0.00001607
Iteration 105/1000 | Loss: 0.00001606
Iteration 106/1000 | Loss: 0.00001605
Iteration 107/1000 | Loss: 0.00001604
Iteration 108/1000 | Loss: 0.00001603
Iteration 109/1000 | Loss: 0.00001603
Iteration 110/1000 | Loss: 0.00001603
Iteration 111/1000 | Loss: 0.00001602
Iteration 112/1000 | Loss: 0.00001602
Iteration 113/1000 | Loss: 0.00001602
Iteration 114/1000 | Loss: 0.00001602
Iteration 115/1000 | Loss: 0.00001602
Iteration 116/1000 | Loss: 0.00001601
Iteration 117/1000 | Loss: 0.00001601
Iteration 118/1000 | Loss: 0.00001601
Iteration 119/1000 | Loss: 0.00001601
Iteration 120/1000 | Loss: 0.00001600
Iteration 121/1000 | Loss: 0.00001600
Iteration 122/1000 | Loss: 0.00001599
Iteration 123/1000 | Loss: 0.00001599
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001597
Iteration 126/1000 | Loss: 0.00001597
Iteration 127/1000 | Loss: 0.00001597
Iteration 128/1000 | Loss: 0.00001596
Iteration 129/1000 | Loss: 0.00001596
Iteration 130/1000 | Loss: 0.00001596
Iteration 131/1000 | Loss: 0.00001596
Iteration 132/1000 | Loss: 0.00001596
Iteration 133/1000 | Loss: 0.00001596
Iteration 134/1000 | Loss: 0.00001595
Iteration 135/1000 | Loss: 0.00001595
Iteration 136/1000 | Loss: 0.00001595
Iteration 137/1000 | Loss: 0.00001595
Iteration 138/1000 | Loss: 0.00001595
Iteration 139/1000 | Loss: 0.00001594
Iteration 140/1000 | Loss: 0.00001594
Iteration 141/1000 | Loss: 0.00001594
Iteration 142/1000 | Loss: 0.00001594
Iteration 143/1000 | Loss: 0.00001593
Iteration 144/1000 | Loss: 0.00001593
Iteration 145/1000 | Loss: 0.00001593
Iteration 146/1000 | Loss: 0.00001593
Iteration 147/1000 | Loss: 0.00001593
Iteration 148/1000 | Loss: 0.00001593
Iteration 149/1000 | Loss: 0.00001593
Iteration 150/1000 | Loss: 0.00001593
Iteration 151/1000 | Loss: 0.00001593
Iteration 152/1000 | Loss: 0.00001593
Iteration 153/1000 | Loss: 0.00001593
Iteration 154/1000 | Loss: 0.00001593
Iteration 155/1000 | Loss: 0.00001592
Iteration 156/1000 | Loss: 0.00001592
Iteration 157/1000 | Loss: 0.00001592
Iteration 158/1000 | Loss: 0.00001592
Iteration 159/1000 | Loss: 0.00001592
Iteration 160/1000 | Loss: 0.00001591
Iteration 161/1000 | Loss: 0.00001591
Iteration 162/1000 | Loss: 0.00001591
Iteration 163/1000 | Loss: 0.00001591
Iteration 164/1000 | Loss: 0.00001590
Iteration 165/1000 | Loss: 0.00001590
Iteration 166/1000 | Loss: 0.00001590
Iteration 167/1000 | Loss: 0.00001589
Iteration 168/1000 | Loss: 0.00001589
Iteration 169/1000 | Loss: 0.00001589
Iteration 170/1000 | Loss: 0.00001589
Iteration 171/1000 | Loss: 0.00001589
Iteration 172/1000 | Loss: 0.00001589
Iteration 173/1000 | Loss: 0.00001589
Iteration 174/1000 | Loss: 0.00001589
Iteration 175/1000 | Loss: 0.00001589
Iteration 176/1000 | Loss: 0.00001588
Iteration 177/1000 | Loss: 0.00001588
Iteration 178/1000 | Loss: 0.00001588
Iteration 179/1000 | Loss: 0.00001588
Iteration 180/1000 | Loss: 0.00001588
Iteration 181/1000 | Loss: 0.00001588
Iteration 182/1000 | Loss: 0.00001587
Iteration 183/1000 | Loss: 0.00001587
Iteration 184/1000 | Loss: 0.00001587
Iteration 185/1000 | Loss: 0.00001587
Iteration 186/1000 | Loss: 0.00001586
Iteration 187/1000 | Loss: 0.00001586
Iteration 188/1000 | Loss: 0.00001586
Iteration 189/1000 | Loss: 0.00001586
Iteration 190/1000 | Loss: 0.00001586
Iteration 191/1000 | Loss: 0.00001586
Iteration 192/1000 | Loss: 0.00001585
Iteration 193/1000 | Loss: 0.00001585
Iteration 194/1000 | Loss: 0.00001585
Iteration 195/1000 | Loss: 0.00001585
Iteration 196/1000 | Loss: 0.00001585
Iteration 197/1000 | Loss: 0.00001585
Iteration 198/1000 | Loss: 0.00001585
Iteration 199/1000 | Loss: 0.00001585
Iteration 200/1000 | Loss: 0.00001585
Iteration 201/1000 | Loss: 0.00001585
Iteration 202/1000 | Loss: 0.00001585
Iteration 203/1000 | Loss: 0.00001584
Iteration 204/1000 | Loss: 0.00001584
Iteration 205/1000 | Loss: 0.00001584
Iteration 206/1000 | Loss: 0.00001584
Iteration 207/1000 | Loss: 0.00001584
Iteration 208/1000 | Loss: 0.00001584
Iteration 209/1000 | Loss: 0.00001584
Iteration 210/1000 | Loss: 0.00001584
Iteration 211/1000 | Loss: 0.00001584
Iteration 212/1000 | Loss: 0.00001584
Iteration 213/1000 | Loss: 0.00001584
Iteration 214/1000 | Loss: 0.00001584
Iteration 215/1000 | Loss: 0.00001584
Iteration 216/1000 | Loss: 0.00001584
Iteration 217/1000 | Loss: 0.00001584
Iteration 218/1000 | Loss: 0.00001584
Iteration 219/1000 | Loss: 0.00001584
Iteration 220/1000 | Loss: 0.00001584
Iteration 221/1000 | Loss: 0.00001584
Iteration 222/1000 | Loss: 0.00001584
Iteration 223/1000 | Loss: 0.00001584
Iteration 224/1000 | Loss: 0.00001584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.583801167726051e-05, 1.583801167726051e-05, 1.583801167726051e-05, 1.583801167726051e-05, 1.583801167726051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.583801167726051e-05

Optimization complete. Final v2v error: 3.4701709747314453 mm

Highest mean error: 3.75026535987854 mm for frame 154

Lowest mean error: 3.2331383228302 mm for frame 0

Saving results

Total time: 48.61433005332947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00612345
Iteration 2/25 | Loss: 0.00168116
Iteration 3/25 | Loss: 0.00155321
Iteration 4/25 | Loss: 0.00153053
Iteration 5/25 | Loss: 0.00152493
Iteration 6/25 | Loss: 0.00152474
Iteration 7/25 | Loss: 0.00152474
Iteration 8/25 | Loss: 0.00152474
Iteration 9/25 | Loss: 0.00152474
Iteration 10/25 | Loss: 0.00152474
Iteration 11/25 | Loss: 0.00152467
Iteration 12/25 | Loss: 0.00152467
Iteration 13/25 | Loss: 0.00152467
Iteration 14/25 | Loss: 0.00152467
Iteration 15/25 | Loss: 0.00152467
Iteration 16/25 | Loss: 0.00152467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015246743569150567, 0.0015246743569150567, 0.0015246743569150567, 0.0015246743569150567, 0.0015246743569150567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015246743569150567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55522466
Iteration 2/25 | Loss: 0.00226389
Iteration 3/25 | Loss: 0.00226389
Iteration 4/25 | Loss: 0.00226389
Iteration 5/25 | Loss: 0.00226389
Iteration 6/25 | Loss: 0.00226389
Iteration 7/25 | Loss: 0.00226389
Iteration 8/25 | Loss: 0.00226389
Iteration 9/25 | Loss: 0.00226388
Iteration 10/25 | Loss: 0.00226388
Iteration 11/25 | Loss: 0.00226388
Iteration 12/25 | Loss: 0.00226388
Iteration 13/25 | Loss: 0.00226388
Iteration 14/25 | Loss: 0.00226388
Iteration 15/25 | Loss: 0.00226388
Iteration 16/25 | Loss: 0.00226388
Iteration 17/25 | Loss: 0.00226388
Iteration 18/25 | Loss: 0.00226388
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0022638847585767508, 0.0022638847585767508, 0.0022638847585767508, 0.0022638847585767508, 0.0022638847585767508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022638847585767508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226388
Iteration 2/1000 | Loss: 0.00007375
Iteration 3/1000 | Loss: 0.00005613
Iteration 4/1000 | Loss: 0.00004803
Iteration 5/1000 | Loss: 0.00004475
Iteration 6/1000 | Loss: 0.00004331
Iteration 7/1000 | Loss: 0.00004229
Iteration 8/1000 | Loss: 0.00004166
Iteration 9/1000 | Loss: 0.00004123
Iteration 10/1000 | Loss: 0.00004092
Iteration 11/1000 | Loss: 0.00004068
Iteration 12/1000 | Loss: 0.00004054
Iteration 13/1000 | Loss: 0.00004053
Iteration 14/1000 | Loss: 0.00004046
Iteration 15/1000 | Loss: 0.00004041
Iteration 16/1000 | Loss: 0.00004041
Iteration 17/1000 | Loss: 0.00004040
Iteration 18/1000 | Loss: 0.00004030
Iteration 19/1000 | Loss: 0.00004023
Iteration 20/1000 | Loss: 0.00004019
Iteration 21/1000 | Loss: 0.00004018
Iteration 22/1000 | Loss: 0.00004018
Iteration 23/1000 | Loss: 0.00004012
Iteration 24/1000 | Loss: 0.00004012
Iteration 25/1000 | Loss: 0.00004011
Iteration 26/1000 | Loss: 0.00004011
Iteration 27/1000 | Loss: 0.00004011
Iteration 28/1000 | Loss: 0.00004010
Iteration 29/1000 | Loss: 0.00004010
Iteration 30/1000 | Loss: 0.00004010
Iteration 31/1000 | Loss: 0.00004010
Iteration 32/1000 | Loss: 0.00004010
Iteration 33/1000 | Loss: 0.00004010
Iteration 34/1000 | Loss: 0.00004009
Iteration 35/1000 | Loss: 0.00004009
Iteration 36/1000 | Loss: 0.00004009
Iteration 37/1000 | Loss: 0.00004008
Iteration 38/1000 | Loss: 0.00004008
Iteration 39/1000 | Loss: 0.00004008
Iteration 40/1000 | Loss: 0.00004008
Iteration 41/1000 | Loss: 0.00004008
Iteration 42/1000 | Loss: 0.00004008
Iteration 43/1000 | Loss: 0.00004008
Iteration 44/1000 | Loss: 0.00004008
Iteration 45/1000 | Loss: 0.00004008
Iteration 46/1000 | Loss: 0.00004008
Iteration 47/1000 | Loss: 0.00004008
Iteration 48/1000 | Loss: 0.00004008
Iteration 49/1000 | Loss: 0.00004006
Iteration 50/1000 | Loss: 0.00004006
Iteration 51/1000 | Loss: 0.00004005
Iteration 52/1000 | Loss: 0.00004005
Iteration 53/1000 | Loss: 0.00004005
Iteration 54/1000 | Loss: 0.00004005
Iteration 55/1000 | Loss: 0.00004004
Iteration 56/1000 | Loss: 0.00004003
Iteration 57/1000 | Loss: 0.00004003
Iteration 58/1000 | Loss: 0.00004003
Iteration 59/1000 | Loss: 0.00004003
Iteration 60/1000 | Loss: 0.00004003
Iteration 61/1000 | Loss: 0.00004003
Iteration 62/1000 | Loss: 0.00004002
Iteration 63/1000 | Loss: 0.00004002
Iteration 64/1000 | Loss: 0.00004002
Iteration 65/1000 | Loss: 0.00004002
Iteration 66/1000 | Loss: 0.00004002
Iteration 67/1000 | Loss: 0.00004001
Iteration 68/1000 | Loss: 0.00004001
Iteration 69/1000 | Loss: 0.00004001
Iteration 70/1000 | Loss: 0.00004001
Iteration 71/1000 | Loss: 0.00004000
Iteration 72/1000 | Loss: 0.00004000
Iteration 73/1000 | Loss: 0.00004000
Iteration 74/1000 | Loss: 0.00004000
Iteration 75/1000 | Loss: 0.00004000
Iteration 76/1000 | Loss: 0.00003999
Iteration 77/1000 | Loss: 0.00003999
Iteration 78/1000 | Loss: 0.00003999
Iteration 79/1000 | Loss: 0.00003999
Iteration 80/1000 | Loss: 0.00003999
Iteration 81/1000 | Loss: 0.00003999
Iteration 82/1000 | Loss: 0.00003999
Iteration 83/1000 | Loss: 0.00003999
Iteration 84/1000 | Loss: 0.00003998
Iteration 85/1000 | Loss: 0.00003997
Iteration 86/1000 | Loss: 0.00003997
Iteration 87/1000 | Loss: 0.00003997
Iteration 88/1000 | Loss: 0.00003997
Iteration 89/1000 | Loss: 0.00003996
Iteration 90/1000 | Loss: 0.00003996
Iteration 91/1000 | Loss: 0.00003996
Iteration 92/1000 | Loss: 0.00003995
Iteration 93/1000 | Loss: 0.00003995
Iteration 94/1000 | Loss: 0.00003995
Iteration 95/1000 | Loss: 0.00003995
Iteration 96/1000 | Loss: 0.00003995
Iteration 97/1000 | Loss: 0.00003995
Iteration 98/1000 | Loss: 0.00003995
Iteration 99/1000 | Loss: 0.00003995
Iteration 100/1000 | Loss: 0.00003994
Iteration 101/1000 | Loss: 0.00003994
Iteration 102/1000 | Loss: 0.00003994
Iteration 103/1000 | Loss: 0.00003994
Iteration 104/1000 | Loss: 0.00003994
Iteration 105/1000 | Loss: 0.00003993
Iteration 106/1000 | Loss: 0.00003993
Iteration 107/1000 | Loss: 0.00003993
Iteration 108/1000 | Loss: 0.00003993
Iteration 109/1000 | Loss: 0.00003993
Iteration 110/1000 | Loss: 0.00003993
Iteration 111/1000 | Loss: 0.00003992
Iteration 112/1000 | Loss: 0.00003992
Iteration 113/1000 | Loss: 0.00003992
Iteration 114/1000 | Loss: 0.00003992
Iteration 115/1000 | Loss: 0.00003990
Iteration 116/1000 | Loss: 0.00003990
Iteration 117/1000 | Loss: 0.00003990
Iteration 118/1000 | Loss: 0.00003990
Iteration 119/1000 | Loss: 0.00003989
Iteration 120/1000 | Loss: 0.00003989
Iteration 121/1000 | Loss: 0.00003989
Iteration 122/1000 | Loss: 0.00003989
Iteration 123/1000 | Loss: 0.00003988
Iteration 124/1000 | Loss: 0.00003988
Iteration 125/1000 | Loss: 0.00003988
Iteration 126/1000 | Loss: 0.00003988
Iteration 127/1000 | Loss: 0.00003988
Iteration 128/1000 | Loss: 0.00003988
Iteration 129/1000 | Loss: 0.00003987
Iteration 130/1000 | Loss: 0.00003987
Iteration 131/1000 | Loss: 0.00003987
Iteration 132/1000 | Loss: 0.00003987
Iteration 133/1000 | Loss: 0.00003987
Iteration 134/1000 | Loss: 0.00003987
Iteration 135/1000 | Loss: 0.00003987
Iteration 136/1000 | Loss: 0.00003987
Iteration 137/1000 | Loss: 0.00003987
Iteration 138/1000 | Loss: 0.00003987
Iteration 139/1000 | Loss: 0.00003986
Iteration 140/1000 | Loss: 0.00003986
Iteration 141/1000 | Loss: 0.00003986
Iteration 142/1000 | Loss: 0.00003986
Iteration 143/1000 | Loss: 0.00003986
Iteration 144/1000 | Loss: 0.00003986
Iteration 145/1000 | Loss: 0.00003986
Iteration 146/1000 | Loss: 0.00003986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [3.9862428820924833e-05, 3.9862428820924833e-05, 3.9862428820924833e-05, 3.9862428820924833e-05, 3.9862428820924833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9862428820924833e-05

Optimization complete. Final v2v error: 5.419898509979248 mm

Highest mean error: 5.910815715789795 mm for frame 0

Lowest mean error: 5.147766590118408 mm for frame 119

Saving results

Total time: 43.37349557876587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900414
Iteration 2/25 | Loss: 0.00183956
Iteration 3/25 | Loss: 0.00153725
Iteration 4/25 | Loss: 0.00151412
Iteration 5/25 | Loss: 0.00150813
Iteration 6/25 | Loss: 0.00150730
Iteration 7/25 | Loss: 0.00150730
Iteration 8/25 | Loss: 0.00150730
Iteration 9/25 | Loss: 0.00150730
Iteration 10/25 | Loss: 0.00150730
Iteration 11/25 | Loss: 0.00150730
Iteration 12/25 | Loss: 0.00150730
Iteration 13/25 | Loss: 0.00150730
Iteration 14/25 | Loss: 0.00150730
Iteration 15/25 | Loss: 0.00150730
Iteration 16/25 | Loss: 0.00150730
Iteration 17/25 | Loss: 0.00150730
Iteration 18/25 | Loss: 0.00150730
Iteration 19/25 | Loss: 0.00150730
Iteration 20/25 | Loss: 0.00150730
Iteration 21/25 | Loss: 0.00150730
Iteration 22/25 | Loss: 0.00150730
Iteration 23/25 | Loss: 0.00150730
Iteration 24/25 | Loss: 0.00150730
Iteration 25/25 | Loss: 0.00150730

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55165112
Iteration 2/25 | Loss: 0.00181338
Iteration 3/25 | Loss: 0.00181338
Iteration 4/25 | Loss: 0.00181338
Iteration 5/25 | Loss: 0.00181338
Iteration 6/25 | Loss: 0.00181338
Iteration 7/25 | Loss: 0.00181338
Iteration 8/25 | Loss: 0.00181338
Iteration 9/25 | Loss: 0.00181338
Iteration 10/25 | Loss: 0.00181338
Iteration 11/25 | Loss: 0.00181338
Iteration 12/25 | Loss: 0.00181337
Iteration 13/25 | Loss: 0.00181337
Iteration 14/25 | Loss: 0.00181338
Iteration 15/25 | Loss: 0.00181338
Iteration 16/25 | Loss: 0.00181338
Iteration 17/25 | Loss: 0.00181338
Iteration 18/25 | Loss: 0.00181338
Iteration 19/25 | Loss: 0.00181338
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018133751582354307, 0.0018133751582354307, 0.0018133751582354307, 0.0018133751582354307, 0.0018133751582354307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018133751582354307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181338
Iteration 2/1000 | Loss: 0.00006314
Iteration 3/1000 | Loss: 0.00004961
Iteration 4/1000 | Loss: 0.00004414
Iteration 5/1000 | Loss: 0.00004136
Iteration 6/1000 | Loss: 0.00003995
Iteration 7/1000 | Loss: 0.00003935
Iteration 8/1000 | Loss: 0.00003901
Iteration 9/1000 | Loss: 0.00003869
Iteration 10/1000 | Loss: 0.00003819
Iteration 11/1000 | Loss: 0.00003791
Iteration 12/1000 | Loss: 0.00003784
Iteration 13/1000 | Loss: 0.00003782
Iteration 14/1000 | Loss: 0.00003782
Iteration 15/1000 | Loss: 0.00003782
Iteration 16/1000 | Loss: 0.00003781
Iteration 17/1000 | Loss: 0.00003780
Iteration 18/1000 | Loss: 0.00003770
Iteration 19/1000 | Loss: 0.00003768
Iteration 20/1000 | Loss: 0.00003767
Iteration 21/1000 | Loss: 0.00003762
Iteration 22/1000 | Loss: 0.00003760
Iteration 23/1000 | Loss: 0.00003760
Iteration 24/1000 | Loss: 0.00003759
Iteration 25/1000 | Loss: 0.00003759
Iteration 26/1000 | Loss: 0.00003759
Iteration 27/1000 | Loss: 0.00003759
Iteration 28/1000 | Loss: 0.00003759
Iteration 29/1000 | Loss: 0.00003759
Iteration 30/1000 | Loss: 0.00003758
Iteration 31/1000 | Loss: 0.00003753
Iteration 32/1000 | Loss: 0.00003753
Iteration 33/1000 | Loss: 0.00003752
Iteration 34/1000 | Loss: 0.00003750
Iteration 35/1000 | Loss: 0.00003750
Iteration 36/1000 | Loss: 0.00003750
Iteration 37/1000 | Loss: 0.00003750
Iteration 38/1000 | Loss: 0.00003750
Iteration 39/1000 | Loss: 0.00003750
Iteration 40/1000 | Loss: 0.00003750
Iteration 41/1000 | Loss: 0.00003750
Iteration 42/1000 | Loss: 0.00003750
Iteration 43/1000 | Loss: 0.00003750
Iteration 44/1000 | Loss: 0.00003750
Iteration 45/1000 | Loss: 0.00003749
Iteration 46/1000 | Loss: 0.00003749
Iteration 47/1000 | Loss: 0.00003749
Iteration 48/1000 | Loss: 0.00003749
Iteration 49/1000 | Loss: 0.00003746
Iteration 50/1000 | Loss: 0.00003746
Iteration 51/1000 | Loss: 0.00003746
Iteration 52/1000 | Loss: 0.00003746
Iteration 53/1000 | Loss: 0.00003746
Iteration 54/1000 | Loss: 0.00003746
Iteration 55/1000 | Loss: 0.00003746
Iteration 56/1000 | Loss: 0.00003746
Iteration 57/1000 | Loss: 0.00003746
Iteration 58/1000 | Loss: 0.00003746
Iteration 59/1000 | Loss: 0.00003745
Iteration 60/1000 | Loss: 0.00003745
Iteration 61/1000 | Loss: 0.00003744
Iteration 62/1000 | Loss: 0.00003744
Iteration 63/1000 | Loss: 0.00003744
Iteration 64/1000 | Loss: 0.00003744
Iteration 65/1000 | Loss: 0.00003743
Iteration 66/1000 | Loss: 0.00003743
Iteration 67/1000 | Loss: 0.00003743
Iteration 68/1000 | Loss: 0.00003743
Iteration 69/1000 | Loss: 0.00003743
Iteration 70/1000 | Loss: 0.00003743
Iteration 71/1000 | Loss: 0.00003743
Iteration 72/1000 | Loss: 0.00003743
Iteration 73/1000 | Loss: 0.00003743
Iteration 74/1000 | Loss: 0.00003743
Iteration 75/1000 | Loss: 0.00003743
Iteration 76/1000 | Loss: 0.00003742
Iteration 77/1000 | Loss: 0.00003742
Iteration 78/1000 | Loss: 0.00003742
Iteration 79/1000 | Loss: 0.00003742
Iteration 80/1000 | Loss: 0.00003741
Iteration 81/1000 | Loss: 0.00003741
Iteration 82/1000 | Loss: 0.00003741
Iteration 83/1000 | Loss: 0.00003741
Iteration 84/1000 | Loss: 0.00003741
Iteration 85/1000 | Loss: 0.00003741
Iteration 86/1000 | Loss: 0.00003741
Iteration 87/1000 | Loss: 0.00003741
Iteration 88/1000 | Loss: 0.00003741
Iteration 89/1000 | Loss: 0.00003741
Iteration 90/1000 | Loss: 0.00003740
Iteration 91/1000 | Loss: 0.00003740
Iteration 92/1000 | Loss: 0.00003740
Iteration 93/1000 | Loss: 0.00003740
Iteration 94/1000 | Loss: 0.00003740
Iteration 95/1000 | Loss: 0.00003740
Iteration 96/1000 | Loss: 0.00003740
Iteration 97/1000 | Loss: 0.00003740
Iteration 98/1000 | Loss: 0.00003740
Iteration 99/1000 | Loss: 0.00003740
Iteration 100/1000 | Loss: 0.00003740
Iteration 101/1000 | Loss: 0.00003740
Iteration 102/1000 | Loss: 0.00003740
Iteration 103/1000 | Loss: 0.00003740
Iteration 104/1000 | Loss: 0.00003740
Iteration 105/1000 | Loss: 0.00003740
Iteration 106/1000 | Loss: 0.00003740
Iteration 107/1000 | Loss: 0.00003740
Iteration 108/1000 | Loss: 0.00003740
Iteration 109/1000 | Loss: 0.00003740
Iteration 110/1000 | Loss: 0.00003740
Iteration 111/1000 | Loss: 0.00003740
Iteration 112/1000 | Loss: 0.00003740
Iteration 113/1000 | Loss: 0.00003740
Iteration 114/1000 | Loss: 0.00003740
Iteration 115/1000 | Loss: 0.00003740
Iteration 116/1000 | Loss: 0.00003740
Iteration 117/1000 | Loss: 0.00003740
Iteration 118/1000 | Loss: 0.00003740
Iteration 119/1000 | Loss: 0.00003740
Iteration 120/1000 | Loss: 0.00003740
Iteration 121/1000 | Loss: 0.00003740
Iteration 122/1000 | Loss: 0.00003740
Iteration 123/1000 | Loss: 0.00003740
Iteration 124/1000 | Loss: 0.00003740
Iteration 125/1000 | Loss: 0.00003740
Iteration 126/1000 | Loss: 0.00003740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [3.739934618351981e-05, 3.739934618351981e-05, 3.739934618351981e-05, 3.739934618351981e-05, 3.739934618351981e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.739934618351981e-05

Optimization complete. Final v2v error: 5.302933692932129 mm

Highest mean error: 5.660684108734131 mm for frame 219

Lowest mean error: 4.943196773529053 mm for frame 111

Saving results

Total time: 38.3957793712616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462990
Iteration 2/25 | Loss: 0.00159081
Iteration 3/25 | Loss: 0.00150607
Iteration 4/25 | Loss: 0.00149703
Iteration 5/25 | Loss: 0.00148903
Iteration 6/25 | Loss: 0.00148803
Iteration 7/25 | Loss: 0.00148803
Iteration 8/25 | Loss: 0.00148803
Iteration 9/25 | Loss: 0.00148803
Iteration 10/25 | Loss: 0.00148803
Iteration 11/25 | Loss: 0.00148803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014880303060635924, 0.0014880303060635924, 0.0014880303060635924, 0.0014880303060635924, 0.0014880303060635924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014880303060635924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80866754
Iteration 2/25 | Loss: 0.00210150
Iteration 3/25 | Loss: 0.00210145
Iteration 4/25 | Loss: 0.00210145
Iteration 5/25 | Loss: 0.00210145
Iteration 6/25 | Loss: 0.00210145
Iteration 7/25 | Loss: 0.00210145
Iteration 8/25 | Loss: 0.00210145
Iteration 9/25 | Loss: 0.00210145
Iteration 10/25 | Loss: 0.00210145
Iteration 11/25 | Loss: 0.00210145
Iteration 12/25 | Loss: 0.00210145
Iteration 13/25 | Loss: 0.00210145
Iteration 14/25 | Loss: 0.00210145
Iteration 15/25 | Loss: 0.00210145
Iteration 16/25 | Loss: 0.00210145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021014511585235596, 0.0021014511585235596, 0.0021014511585235596, 0.0021014511585235596, 0.0021014511585235596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021014511585235596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210145
Iteration 2/1000 | Loss: 0.00005772
Iteration 3/1000 | Loss: 0.00004096
Iteration 4/1000 | Loss: 0.00003360
Iteration 5/1000 | Loss: 0.00003075
Iteration 6/1000 | Loss: 0.00002937
Iteration 7/1000 | Loss: 0.00002866
Iteration 8/1000 | Loss: 0.00002812
Iteration 9/1000 | Loss: 0.00002765
Iteration 10/1000 | Loss: 0.00002726
Iteration 11/1000 | Loss: 0.00002693
Iteration 12/1000 | Loss: 0.00002677
Iteration 13/1000 | Loss: 0.00002657
Iteration 14/1000 | Loss: 0.00002648
Iteration 15/1000 | Loss: 0.00002645
Iteration 16/1000 | Loss: 0.00002641
Iteration 17/1000 | Loss: 0.00002636
Iteration 18/1000 | Loss: 0.00002636
Iteration 19/1000 | Loss: 0.00002636
Iteration 20/1000 | Loss: 0.00002636
Iteration 21/1000 | Loss: 0.00002636
Iteration 22/1000 | Loss: 0.00002635
Iteration 23/1000 | Loss: 0.00002635
Iteration 24/1000 | Loss: 0.00002634
Iteration 25/1000 | Loss: 0.00002633
Iteration 26/1000 | Loss: 0.00002633
Iteration 27/1000 | Loss: 0.00002633
Iteration 28/1000 | Loss: 0.00002633
Iteration 29/1000 | Loss: 0.00002633
Iteration 30/1000 | Loss: 0.00002633
Iteration 31/1000 | Loss: 0.00002632
Iteration 32/1000 | Loss: 0.00002632
Iteration 33/1000 | Loss: 0.00002632
Iteration 34/1000 | Loss: 0.00002632
Iteration 35/1000 | Loss: 0.00002631
Iteration 36/1000 | Loss: 0.00002631
Iteration 37/1000 | Loss: 0.00002631
Iteration 38/1000 | Loss: 0.00002631
Iteration 39/1000 | Loss: 0.00002631
Iteration 40/1000 | Loss: 0.00002630
Iteration 41/1000 | Loss: 0.00002630
Iteration 42/1000 | Loss: 0.00002630
Iteration 43/1000 | Loss: 0.00002630
Iteration 44/1000 | Loss: 0.00002630
Iteration 45/1000 | Loss: 0.00002630
Iteration 46/1000 | Loss: 0.00002630
Iteration 47/1000 | Loss: 0.00002630
Iteration 48/1000 | Loss: 0.00002630
Iteration 49/1000 | Loss: 0.00002630
Iteration 50/1000 | Loss: 0.00002629
Iteration 51/1000 | Loss: 0.00002629
Iteration 52/1000 | Loss: 0.00002629
Iteration 53/1000 | Loss: 0.00002629
Iteration 54/1000 | Loss: 0.00002629
Iteration 55/1000 | Loss: 0.00002629
Iteration 56/1000 | Loss: 0.00002629
Iteration 57/1000 | Loss: 0.00002629
Iteration 58/1000 | Loss: 0.00002629
Iteration 59/1000 | Loss: 0.00002629
Iteration 60/1000 | Loss: 0.00002628
Iteration 61/1000 | Loss: 0.00002628
Iteration 62/1000 | Loss: 0.00002628
Iteration 63/1000 | Loss: 0.00002627
Iteration 64/1000 | Loss: 0.00002627
Iteration 65/1000 | Loss: 0.00002627
Iteration 66/1000 | Loss: 0.00002627
Iteration 67/1000 | Loss: 0.00002627
Iteration 68/1000 | Loss: 0.00002627
Iteration 69/1000 | Loss: 0.00002626
Iteration 70/1000 | Loss: 0.00002626
Iteration 71/1000 | Loss: 0.00002626
Iteration 72/1000 | Loss: 0.00002626
Iteration 73/1000 | Loss: 0.00002626
Iteration 74/1000 | Loss: 0.00002626
Iteration 75/1000 | Loss: 0.00002626
Iteration 76/1000 | Loss: 0.00002626
Iteration 77/1000 | Loss: 0.00002626
Iteration 78/1000 | Loss: 0.00002626
Iteration 79/1000 | Loss: 0.00002626
Iteration 80/1000 | Loss: 0.00002626
Iteration 81/1000 | Loss: 0.00002626
Iteration 82/1000 | Loss: 0.00002626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [2.6263003746862523e-05, 2.6263003746862523e-05, 2.6263003746862523e-05, 2.6263003746862523e-05, 2.6263003746862523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6263003746862523e-05

Optimization complete. Final v2v error: 4.604795932769775 mm

Highest mean error: 4.884835243225098 mm for frame 3

Lowest mean error: 4.421388149261475 mm for frame 196

Saving results

Total time: 37.38858604431152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478207
Iteration 2/25 | Loss: 0.00169037
Iteration 3/25 | Loss: 0.00144612
Iteration 4/25 | Loss: 0.00143027
Iteration 5/25 | Loss: 0.00142448
Iteration 6/25 | Loss: 0.00142298
Iteration 7/25 | Loss: 0.00142267
Iteration 8/25 | Loss: 0.00142267
Iteration 9/25 | Loss: 0.00142267
Iteration 10/25 | Loss: 0.00142267
Iteration 11/25 | Loss: 0.00142267
Iteration 12/25 | Loss: 0.00142267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014226670609787107, 0.0014226670609787107, 0.0014226670609787107, 0.0014226670609787107, 0.0014226670609787107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014226670609787107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60834873
Iteration 2/25 | Loss: 0.00197657
Iteration 3/25 | Loss: 0.00197657
Iteration 4/25 | Loss: 0.00197657
Iteration 5/25 | Loss: 0.00197657
Iteration 6/25 | Loss: 0.00197657
Iteration 7/25 | Loss: 0.00197657
Iteration 8/25 | Loss: 0.00197657
Iteration 9/25 | Loss: 0.00197657
Iteration 10/25 | Loss: 0.00197657
Iteration 11/25 | Loss: 0.00197657
Iteration 12/25 | Loss: 0.00197657
Iteration 13/25 | Loss: 0.00197657
Iteration 14/25 | Loss: 0.00197657
Iteration 15/25 | Loss: 0.00197657
Iteration 16/25 | Loss: 0.00197657
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00197657011449337, 0.00197657011449337, 0.00197657011449337, 0.00197657011449337, 0.00197657011449337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00197657011449337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197657
Iteration 2/1000 | Loss: 0.00005880
Iteration 3/1000 | Loss: 0.00004251
Iteration 4/1000 | Loss: 0.00003408
Iteration 5/1000 | Loss: 0.00003071
Iteration 6/1000 | Loss: 0.00002874
Iteration 7/1000 | Loss: 0.00002779
Iteration 8/1000 | Loss: 0.00002745
Iteration 9/1000 | Loss: 0.00002714
Iteration 10/1000 | Loss: 0.00002690
Iteration 11/1000 | Loss: 0.00002670
Iteration 12/1000 | Loss: 0.00002667
Iteration 13/1000 | Loss: 0.00002664
Iteration 14/1000 | Loss: 0.00002664
Iteration 15/1000 | Loss: 0.00002658
Iteration 16/1000 | Loss: 0.00002658
Iteration 17/1000 | Loss: 0.00002658
Iteration 18/1000 | Loss: 0.00002657
Iteration 19/1000 | Loss: 0.00002656
Iteration 20/1000 | Loss: 0.00002654
Iteration 21/1000 | Loss: 0.00002654
Iteration 22/1000 | Loss: 0.00002654
Iteration 23/1000 | Loss: 0.00002653
Iteration 24/1000 | Loss: 0.00002652
Iteration 25/1000 | Loss: 0.00002650
Iteration 26/1000 | Loss: 0.00002650
Iteration 27/1000 | Loss: 0.00002649
Iteration 28/1000 | Loss: 0.00002649
Iteration 29/1000 | Loss: 0.00002649
Iteration 30/1000 | Loss: 0.00002648
Iteration 31/1000 | Loss: 0.00002648
Iteration 32/1000 | Loss: 0.00002647
Iteration 33/1000 | Loss: 0.00002646
Iteration 34/1000 | Loss: 0.00002645
Iteration 35/1000 | Loss: 0.00002645
Iteration 36/1000 | Loss: 0.00002645
Iteration 37/1000 | Loss: 0.00002644
Iteration 38/1000 | Loss: 0.00002644
Iteration 39/1000 | Loss: 0.00002644
Iteration 40/1000 | Loss: 0.00002644
Iteration 41/1000 | Loss: 0.00002644
Iteration 42/1000 | Loss: 0.00002644
Iteration 43/1000 | Loss: 0.00002643
Iteration 44/1000 | Loss: 0.00002643
Iteration 45/1000 | Loss: 0.00002642
Iteration 46/1000 | Loss: 0.00002642
Iteration 47/1000 | Loss: 0.00002642
Iteration 48/1000 | Loss: 0.00002641
Iteration 49/1000 | Loss: 0.00002641
Iteration 50/1000 | Loss: 0.00002641
Iteration 51/1000 | Loss: 0.00002641
Iteration 52/1000 | Loss: 0.00002641
Iteration 53/1000 | Loss: 0.00002641
Iteration 54/1000 | Loss: 0.00002641
Iteration 55/1000 | Loss: 0.00002640
Iteration 56/1000 | Loss: 0.00002640
Iteration 57/1000 | Loss: 0.00002640
Iteration 58/1000 | Loss: 0.00002640
Iteration 59/1000 | Loss: 0.00002639
Iteration 60/1000 | Loss: 0.00002639
Iteration 61/1000 | Loss: 0.00002638
Iteration 62/1000 | Loss: 0.00002638
Iteration 63/1000 | Loss: 0.00002637
Iteration 64/1000 | Loss: 0.00002637
Iteration 65/1000 | Loss: 0.00002636
Iteration 66/1000 | Loss: 0.00002636
Iteration 67/1000 | Loss: 0.00002636
Iteration 68/1000 | Loss: 0.00002635
Iteration 69/1000 | Loss: 0.00002635
Iteration 70/1000 | Loss: 0.00002635
Iteration 71/1000 | Loss: 0.00002635
Iteration 72/1000 | Loss: 0.00002635
Iteration 73/1000 | Loss: 0.00002635
Iteration 74/1000 | Loss: 0.00002635
Iteration 75/1000 | Loss: 0.00002635
Iteration 76/1000 | Loss: 0.00002635
Iteration 77/1000 | Loss: 0.00002634
Iteration 78/1000 | Loss: 0.00002634
Iteration 79/1000 | Loss: 0.00002633
Iteration 80/1000 | Loss: 0.00002633
Iteration 81/1000 | Loss: 0.00002633
Iteration 82/1000 | Loss: 0.00002632
Iteration 83/1000 | Loss: 0.00002632
Iteration 84/1000 | Loss: 0.00002632
Iteration 85/1000 | Loss: 0.00002632
Iteration 86/1000 | Loss: 0.00002632
Iteration 87/1000 | Loss: 0.00002632
Iteration 88/1000 | Loss: 0.00002632
Iteration 89/1000 | Loss: 0.00002632
Iteration 90/1000 | Loss: 0.00002631
Iteration 91/1000 | Loss: 0.00002631
Iteration 92/1000 | Loss: 0.00002631
Iteration 93/1000 | Loss: 0.00002630
Iteration 94/1000 | Loss: 0.00002630
Iteration 95/1000 | Loss: 0.00002630
Iteration 96/1000 | Loss: 0.00002630
Iteration 97/1000 | Loss: 0.00002630
Iteration 98/1000 | Loss: 0.00002630
Iteration 99/1000 | Loss: 0.00002630
Iteration 100/1000 | Loss: 0.00002630
Iteration 101/1000 | Loss: 0.00002629
Iteration 102/1000 | Loss: 0.00002629
Iteration 103/1000 | Loss: 0.00002629
Iteration 104/1000 | Loss: 0.00002629
Iteration 105/1000 | Loss: 0.00002629
Iteration 106/1000 | Loss: 0.00002629
Iteration 107/1000 | Loss: 0.00002628
Iteration 108/1000 | Loss: 0.00002628
Iteration 109/1000 | Loss: 0.00002628
Iteration 110/1000 | Loss: 0.00002628
Iteration 111/1000 | Loss: 0.00002628
Iteration 112/1000 | Loss: 0.00002628
Iteration 113/1000 | Loss: 0.00002627
Iteration 114/1000 | Loss: 0.00002627
Iteration 115/1000 | Loss: 0.00002627
Iteration 116/1000 | Loss: 0.00002627
Iteration 117/1000 | Loss: 0.00002627
Iteration 118/1000 | Loss: 0.00002627
Iteration 119/1000 | Loss: 0.00002627
Iteration 120/1000 | Loss: 0.00002627
Iteration 121/1000 | Loss: 0.00002627
Iteration 122/1000 | Loss: 0.00002627
Iteration 123/1000 | Loss: 0.00002627
Iteration 124/1000 | Loss: 0.00002627
Iteration 125/1000 | Loss: 0.00002627
Iteration 126/1000 | Loss: 0.00002627
Iteration 127/1000 | Loss: 0.00002627
Iteration 128/1000 | Loss: 0.00002627
Iteration 129/1000 | Loss: 0.00002627
Iteration 130/1000 | Loss: 0.00002627
Iteration 131/1000 | Loss: 0.00002627
Iteration 132/1000 | Loss: 0.00002626
Iteration 133/1000 | Loss: 0.00002626
Iteration 134/1000 | Loss: 0.00002626
Iteration 135/1000 | Loss: 0.00002626
Iteration 136/1000 | Loss: 0.00002626
Iteration 137/1000 | Loss: 0.00002626
Iteration 138/1000 | Loss: 0.00002626
Iteration 139/1000 | Loss: 0.00002626
Iteration 140/1000 | Loss: 0.00002626
Iteration 141/1000 | Loss: 0.00002626
Iteration 142/1000 | Loss: 0.00002626
Iteration 143/1000 | Loss: 0.00002626
Iteration 144/1000 | Loss: 0.00002626
Iteration 145/1000 | Loss: 0.00002626
Iteration 146/1000 | Loss: 0.00002626
Iteration 147/1000 | Loss: 0.00002626
Iteration 148/1000 | Loss: 0.00002626
Iteration 149/1000 | Loss: 0.00002626
Iteration 150/1000 | Loss: 0.00002626
Iteration 151/1000 | Loss: 0.00002626
Iteration 152/1000 | Loss: 0.00002626
Iteration 153/1000 | Loss: 0.00002626
Iteration 154/1000 | Loss: 0.00002626
Iteration 155/1000 | Loss: 0.00002626
Iteration 156/1000 | Loss: 0.00002626
Iteration 157/1000 | Loss: 0.00002626
Iteration 158/1000 | Loss: 0.00002626
Iteration 159/1000 | Loss: 0.00002626
Iteration 160/1000 | Loss: 0.00002626
Iteration 161/1000 | Loss: 0.00002626
Iteration 162/1000 | Loss: 0.00002626
Iteration 163/1000 | Loss: 0.00002626
Iteration 164/1000 | Loss: 0.00002626
Iteration 165/1000 | Loss: 0.00002626
Iteration 166/1000 | Loss: 0.00002626
Iteration 167/1000 | Loss: 0.00002626
Iteration 168/1000 | Loss: 0.00002626
Iteration 169/1000 | Loss: 0.00002626
Iteration 170/1000 | Loss: 0.00002626
Iteration 171/1000 | Loss: 0.00002626
Iteration 172/1000 | Loss: 0.00002626
Iteration 173/1000 | Loss: 0.00002626
Iteration 174/1000 | Loss: 0.00002626
Iteration 175/1000 | Loss: 0.00002626
Iteration 176/1000 | Loss: 0.00002626
Iteration 177/1000 | Loss: 0.00002626
Iteration 178/1000 | Loss: 0.00002626
Iteration 179/1000 | Loss: 0.00002626
Iteration 180/1000 | Loss: 0.00002626
Iteration 181/1000 | Loss: 0.00002626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.6257011995767243e-05, 2.6257011995767243e-05, 2.6257011995767243e-05, 2.6257011995767243e-05, 2.6257011995767243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6257011995767243e-05

Optimization complete. Final v2v error: 4.451979637145996 mm

Highest mean error: 5.173655033111572 mm for frame 26

Lowest mean error: 3.824486494064331 mm for frame 84

Saving results

Total time: 36.2320613861084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464839
Iteration 2/25 | Loss: 0.00162132
Iteration 3/25 | Loss: 0.00153570
Iteration 4/25 | Loss: 0.00151567
Iteration 5/25 | Loss: 0.00150699
Iteration 6/25 | Loss: 0.00150451
Iteration 7/25 | Loss: 0.00150379
Iteration 8/25 | Loss: 0.00150376
Iteration 9/25 | Loss: 0.00150376
Iteration 10/25 | Loss: 0.00150376
Iteration 11/25 | Loss: 0.00150376
Iteration 12/25 | Loss: 0.00150376
Iteration 13/25 | Loss: 0.00150376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0015037647681310773, 0.0015037647681310773, 0.0015037647681310773, 0.0015037647681310773, 0.0015037647681310773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015037647681310773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.12155962
Iteration 2/25 | Loss: 0.00197465
Iteration 3/25 | Loss: 0.00197464
Iteration 4/25 | Loss: 0.00197464
Iteration 5/25 | Loss: 0.00197464
Iteration 6/25 | Loss: 0.00197464
Iteration 7/25 | Loss: 0.00197464
Iteration 8/25 | Loss: 0.00197464
Iteration 9/25 | Loss: 0.00197464
Iteration 10/25 | Loss: 0.00197464
Iteration 11/25 | Loss: 0.00197464
Iteration 12/25 | Loss: 0.00197464
Iteration 13/25 | Loss: 0.00197464
Iteration 14/25 | Loss: 0.00197464
Iteration 15/25 | Loss: 0.00197464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001974639482796192, 0.001974639482796192, 0.001974639482796192, 0.001974639482796192, 0.001974639482796192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001974639482796192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197464
Iteration 2/1000 | Loss: 0.00007229
Iteration 3/1000 | Loss: 0.00005035
Iteration 4/1000 | Loss: 0.00004445
Iteration 5/1000 | Loss: 0.00004011
Iteration 6/1000 | Loss: 0.00003848
Iteration 7/1000 | Loss: 0.00003710
Iteration 8/1000 | Loss: 0.00003615
Iteration 9/1000 | Loss: 0.00003563
Iteration 10/1000 | Loss: 0.00003532
Iteration 11/1000 | Loss: 0.00003495
Iteration 12/1000 | Loss: 0.00003476
Iteration 13/1000 | Loss: 0.00003468
Iteration 14/1000 | Loss: 0.00003463
Iteration 15/1000 | Loss: 0.00003463
Iteration 16/1000 | Loss: 0.00003461
Iteration 17/1000 | Loss: 0.00003461
Iteration 18/1000 | Loss: 0.00003460
Iteration 19/1000 | Loss: 0.00003460
Iteration 20/1000 | Loss: 0.00003459
Iteration 21/1000 | Loss: 0.00003455
Iteration 22/1000 | Loss: 0.00003455
Iteration 23/1000 | Loss: 0.00003453
Iteration 24/1000 | Loss: 0.00003453
Iteration 25/1000 | Loss: 0.00003453
Iteration 26/1000 | Loss: 0.00003452
Iteration 27/1000 | Loss: 0.00003452
Iteration 28/1000 | Loss: 0.00003451
Iteration 29/1000 | Loss: 0.00003451
Iteration 30/1000 | Loss: 0.00003450
Iteration 31/1000 | Loss: 0.00003449
Iteration 32/1000 | Loss: 0.00003449
Iteration 33/1000 | Loss: 0.00003449
Iteration 34/1000 | Loss: 0.00003448
Iteration 35/1000 | Loss: 0.00003448
Iteration 36/1000 | Loss: 0.00003448
Iteration 37/1000 | Loss: 0.00003448
Iteration 38/1000 | Loss: 0.00003448
Iteration 39/1000 | Loss: 0.00003448
Iteration 40/1000 | Loss: 0.00003448
Iteration 41/1000 | Loss: 0.00003448
Iteration 42/1000 | Loss: 0.00003448
Iteration 43/1000 | Loss: 0.00003448
Iteration 44/1000 | Loss: 0.00003447
Iteration 45/1000 | Loss: 0.00003447
Iteration 46/1000 | Loss: 0.00003447
Iteration 47/1000 | Loss: 0.00003447
Iteration 48/1000 | Loss: 0.00003447
Iteration 49/1000 | Loss: 0.00003446
Iteration 50/1000 | Loss: 0.00003446
Iteration 51/1000 | Loss: 0.00003446
Iteration 52/1000 | Loss: 0.00003446
Iteration 53/1000 | Loss: 0.00003445
Iteration 54/1000 | Loss: 0.00003445
Iteration 55/1000 | Loss: 0.00003445
Iteration 56/1000 | Loss: 0.00003445
Iteration 57/1000 | Loss: 0.00003444
Iteration 58/1000 | Loss: 0.00003444
Iteration 59/1000 | Loss: 0.00003444
Iteration 60/1000 | Loss: 0.00003444
Iteration 61/1000 | Loss: 0.00003443
Iteration 62/1000 | Loss: 0.00003443
Iteration 63/1000 | Loss: 0.00003443
Iteration 64/1000 | Loss: 0.00003443
Iteration 65/1000 | Loss: 0.00003443
Iteration 66/1000 | Loss: 0.00003443
Iteration 67/1000 | Loss: 0.00003443
Iteration 68/1000 | Loss: 0.00003443
Iteration 69/1000 | Loss: 0.00003443
Iteration 70/1000 | Loss: 0.00003443
Iteration 71/1000 | Loss: 0.00003443
Iteration 72/1000 | Loss: 0.00003443
Iteration 73/1000 | Loss: 0.00003443
Iteration 74/1000 | Loss: 0.00003443
Iteration 75/1000 | Loss: 0.00003443
Iteration 76/1000 | Loss: 0.00003443
Iteration 77/1000 | Loss: 0.00003443
Iteration 78/1000 | Loss: 0.00003443
Iteration 79/1000 | Loss: 0.00003443
Iteration 80/1000 | Loss: 0.00003443
Iteration 81/1000 | Loss: 0.00003443
Iteration 82/1000 | Loss: 0.00003443
Iteration 83/1000 | Loss: 0.00003443
Iteration 84/1000 | Loss: 0.00003443
Iteration 85/1000 | Loss: 0.00003443
Iteration 86/1000 | Loss: 0.00003443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [3.442528395680711e-05, 3.442528395680711e-05, 3.442528395680711e-05, 3.442528395680711e-05, 3.442528395680711e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.442528395680711e-05

Optimization complete. Final v2v error: 5.05321741104126 mm

Highest mean error: 5.548473834991455 mm for frame 62

Lowest mean error: 4.747639179229736 mm for frame 117

Saving results

Total time: 33.28565716743469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489767
Iteration 2/25 | Loss: 0.00153541
Iteration 3/25 | Loss: 0.00145734
Iteration 4/25 | Loss: 0.00144687
Iteration 5/25 | Loss: 0.00144346
Iteration 6/25 | Loss: 0.00144327
Iteration 7/25 | Loss: 0.00144327
Iteration 8/25 | Loss: 0.00144327
Iteration 9/25 | Loss: 0.00144327
Iteration 10/25 | Loss: 0.00144327
Iteration 11/25 | Loss: 0.00144327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014432738535106182, 0.0014432738535106182, 0.0014432738535106182, 0.0014432738535106182, 0.0014432738535106182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014432738535106182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52236247
Iteration 2/25 | Loss: 0.00184988
Iteration 3/25 | Loss: 0.00184988
Iteration 4/25 | Loss: 0.00184988
Iteration 5/25 | Loss: 0.00184988
Iteration 6/25 | Loss: 0.00184988
Iteration 7/25 | Loss: 0.00184988
Iteration 8/25 | Loss: 0.00184988
Iteration 9/25 | Loss: 0.00184988
Iteration 10/25 | Loss: 0.00184988
Iteration 11/25 | Loss: 0.00184988
Iteration 12/25 | Loss: 0.00184988
Iteration 13/25 | Loss: 0.00184988
Iteration 14/25 | Loss: 0.00184988
Iteration 15/25 | Loss: 0.00184988
Iteration 16/25 | Loss: 0.00184988
Iteration 17/25 | Loss: 0.00184988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018498816061764956, 0.0018498816061764956, 0.0018498816061764956, 0.0018498816061764956, 0.0018498816061764956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018498816061764956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184988
Iteration 2/1000 | Loss: 0.00006047
Iteration 3/1000 | Loss: 0.00004552
Iteration 4/1000 | Loss: 0.00003903
Iteration 5/1000 | Loss: 0.00003425
Iteration 6/1000 | Loss: 0.00003253
Iteration 7/1000 | Loss: 0.00003161
Iteration 8/1000 | Loss: 0.00003126
Iteration 9/1000 | Loss: 0.00003098
Iteration 10/1000 | Loss: 0.00003077
Iteration 11/1000 | Loss: 0.00003063
Iteration 12/1000 | Loss: 0.00003058
Iteration 13/1000 | Loss: 0.00003058
Iteration 14/1000 | Loss: 0.00003058
Iteration 15/1000 | Loss: 0.00003057
Iteration 16/1000 | Loss: 0.00003057
Iteration 17/1000 | Loss: 0.00003054
Iteration 18/1000 | Loss: 0.00003054
Iteration 19/1000 | Loss: 0.00003054
Iteration 20/1000 | Loss: 0.00003053
Iteration 21/1000 | Loss: 0.00003053
Iteration 22/1000 | Loss: 0.00003052
Iteration 23/1000 | Loss: 0.00003051
Iteration 24/1000 | Loss: 0.00003050
Iteration 25/1000 | Loss: 0.00003050
Iteration 26/1000 | Loss: 0.00003050
Iteration 27/1000 | Loss: 0.00003049
Iteration 28/1000 | Loss: 0.00003049
Iteration 29/1000 | Loss: 0.00003048
Iteration 30/1000 | Loss: 0.00003048
Iteration 31/1000 | Loss: 0.00003047
Iteration 32/1000 | Loss: 0.00003046
Iteration 33/1000 | Loss: 0.00003046
Iteration 34/1000 | Loss: 0.00003046
Iteration 35/1000 | Loss: 0.00003046
Iteration 36/1000 | Loss: 0.00003045
Iteration 37/1000 | Loss: 0.00003045
Iteration 38/1000 | Loss: 0.00003045
Iteration 39/1000 | Loss: 0.00003045
Iteration 40/1000 | Loss: 0.00003044
Iteration 41/1000 | Loss: 0.00003043
Iteration 42/1000 | Loss: 0.00003043
Iteration 43/1000 | Loss: 0.00003042
Iteration 44/1000 | Loss: 0.00003042
Iteration 45/1000 | Loss: 0.00003042
Iteration 46/1000 | Loss: 0.00003042
Iteration 47/1000 | Loss: 0.00003041
Iteration 48/1000 | Loss: 0.00003041
Iteration 49/1000 | Loss: 0.00003040
Iteration 50/1000 | Loss: 0.00003040
Iteration 51/1000 | Loss: 0.00003039
Iteration 52/1000 | Loss: 0.00003039
Iteration 53/1000 | Loss: 0.00003039
Iteration 54/1000 | Loss: 0.00003039
Iteration 55/1000 | Loss: 0.00003039
Iteration 56/1000 | Loss: 0.00003039
Iteration 57/1000 | Loss: 0.00003039
Iteration 58/1000 | Loss: 0.00003039
Iteration 59/1000 | Loss: 0.00003039
Iteration 60/1000 | Loss: 0.00003038
Iteration 61/1000 | Loss: 0.00003038
Iteration 62/1000 | Loss: 0.00003038
Iteration 63/1000 | Loss: 0.00003037
Iteration 64/1000 | Loss: 0.00003037
Iteration 65/1000 | Loss: 0.00003037
Iteration 66/1000 | Loss: 0.00003037
Iteration 67/1000 | Loss: 0.00003037
Iteration 68/1000 | Loss: 0.00003037
Iteration 69/1000 | Loss: 0.00003037
Iteration 70/1000 | Loss: 0.00003037
Iteration 71/1000 | Loss: 0.00003036
Iteration 72/1000 | Loss: 0.00003036
Iteration 73/1000 | Loss: 0.00003035
Iteration 74/1000 | Loss: 0.00003035
Iteration 75/1000 | Loss: 0.00003034
Iteration 76/1000 | Loss: 0.00003033
Iteration 77/1000 | Loss: 0.00003033
Iteration 78/1000 | Loss: 0.00003033
Iteration 79/1000 | Loss: 0.00003033
Iteration 80/1000 | Loss: 0.00003033
Iteration 81/1000 | Loss: 0.00003033
Iteration 82/1000 | Loss: 0.00003033
Iteration 83/1000 | Loss: 0.00003033
Iteration 84/1000 | Loss: 0.00003032
Iteration 85/1000 | Loss: 0.00003032
Iteration 86/1000 | Loss: 0.00003032
Iteration 87/1000 | Loss: 0.00003032
Iteration 88/1000 | Loss: 0.00003031
Iteration 89/1000 | Loss: 0.00003031
Iteration 90/1000 | Loss: 0.00003031
Iteration 91/1000 | Loss: 0.00003030
Iteration 92/1000 | Loss: 0.00003030
Iteration 93/1000 | Loss: 0.00003030
Iteration 94/1000 | Loss: 0.00003030
Iteration 95/1000 | Loss: 0.00003030
Iteration 96/1000 | Loss: 0.00003030
Iteration 97/1000 | Loss: 0.00003029
Iteration 98/1000 | Loss: 0.00003029
Iteration 99/1000 | Loss: 0.00003029
Iteration 100/1000 | Loss: 0.00003029
Iteration 101/1000 | Loss: 0.00003029
Iteration 102/1000 | Loss: 0.00003029
Iteration 103/1000 | Loss: 0.00003029
Iteration 104/1000 | Loss: 0.00003028
Iteration 105/1000 | Loss: 0.00003028
Iteration 106/1000 | Loss: 0.00003028
Iteration 107/1000 | Loss: 0.00003028
Iteration 108/1000 | Loss: 0.00003028
Iteration 109/1000 | Loss: 0.00003028
Iteration 110/1000 | Loss: 0.00003028
Iteration 111/1000 | Loss: 0.00003027
Iteration 112/1000 | Loss: 0.00003027
Iteration 113/1000 | Loss: 0.00003027
Iteration 114/1000 | Loss: 0.00003027
Iteration 115/1000 | Loss: 0.00003027
Iteration 116/1000 | Loss: 0.00003027
Iteration 117/1000 | Loss: 0.00003027
Iteration 118/1000 | Loss: 0.00003027
Iteration 119/1000 | Loss: 0.00003027
Iteration 120/1000 | Loss: 0.00003027
Iteration 121/1000 | Loss: 0.00003027
Iteration 122/1000 | Loss: 0.00003027
Iteration 123/1000 | Loss: 0.00003027
Iteration 124/1000 | Loss: 0.00003026
Iteration 125/1000 | Loss: 0.00003026
Iteration 126/1000 | Loss: 0.00003026
Iteration 127/1000 | Loss: 0.00003026
Iteration 128/1000 | Loss: 0.00003026
Iteration 129/1000 | Loss: 0.00003026
Iteration 130/1000 | Loss: 0.00003025
Iteration 131/1000 | Loss: 0.00003025
Iteration 132/1000 | Loss: 0.00003025
Iteration 133/1000 | Loss: 0.00003025
Iteration 134/1000 | Loss: 0.00003025
Iteration 135/1000 | Loss: 0.00003025
Iteration 136/1000 | Loss: 0.00003025
Iteration 137/1000 | Loss: 0.00003024
Iteration 138/1000 | Loss: 0.00003024
Iteration 139/1000 | Loss: 0.00003024
Iteration 140/1000 | Loss: 0.00003024
Iteration 141/1000 | Loss: 0.00003024
Iteration 142/1000 | Loss: 0.00003024
Iteration 143/1000 | Loss: 0.00003024
Iteration 144/1000 | Loss: 0.00003024
Iteration 145/1000 | Loss: 0.00003024
Iteration 146/1000 | Loss: 0.00003024
Iteration 147/1000 | Loss: 0.00003024
Iteration 148/1000 | Loss: 0.00003024
Iteration 149/1000 | Loss: 0.00003023
Iteration 150/1000 | Loss: 0.00003023
Iteration 151/1000 | Loss: 0.00003023
Iteration 152/1000 | Loss: 0.00003023
Iteration 153/1000 | Loss: 0.00003023
Iteration 154/1000 | Loss: 0.00003023
Iteration 155/1000 | Loss: 0.00003023
Iteration 156/1000 | Loss: 0.00003023
Iteration 157/1000 | Loss: 0.00003023
Iteration 158/1000 | Loss: 0.00003023
Iteration 159/1000 | Loss: 0.00003022
Iteration 160/1000 | Loss: 0.00003022
Iteration 161/1000 | Loss: 0.00003022
Iteration 162/1000 | Loss: 0.00003022
Iteration 163/1000 | Loss: 0.00003021
Iteration 164/1000 | Loss: 0.00003021
Iteration 165/1000 | Loss: 0.00003020
Iteration 166/1000 | Loss: 0.00003020
Iteration 167/1000 | Loss: 0.00003020
Iteration 168/1000 | Loss: 0.00003020
Iteration 169/1000 | Loss: 0.00003020
Iteration 170/1000 | Loss: 0.00003020
Iteration 171/1000 | Loss: 0.00003020
Iteration 172/1000 | Loss: 0.00003020
Iteration 173/1000 | Loss: 0.00003020
Iteration 174/1000 | Loss: 0.00003020
Iteration 175/1000 | Loss: 0.00003019
Iteration 176/1000 | Loss: 0.00003019
Iteration 177/1000 | Loss: 0.00003019
Iteration 178/1000 | Loss: 0.00003019
Iteration 179/1000 | Loss: 0.00003019
Iteration 180/1000 | Loss: 0.00003019
Iteration 181/1000 | Loss: 0.00003019
Iteration 182/1000 | Loss: 0.00003018
Iteration 183/1000 | Loss: 0.00003018
Iteration 184/1000 | Loss: 0.00003018
Iteration 185/1000 | Loss: 0.00003018
Iteration 186/1000 | Loss: 0.00003018
Iteration 187/1000 | Loss: 0.00003018
Iteration 188/1000 | Loss: 0.00003018
Iteration 189/1000 | Loss: 0.00003017
Iteration 190/1000 | Loss: 0.00003017
Iteration 191/1000 | Loss: 0.00003017
Iteration 192/1000 | Loss: 0.00003017
Iteration 193/1000 | Loss: 0.00003017
Iteration 194/1000 | Loss: 0.00003017
Iteration 195/1000 | Loss: 0.00003017
Iteration 196/1000 | Loss: 0.00003017
Iteration 197/1000 | Loss: 0.00003017
Iteration 198/1000 | Loss: 0.00003017
Iteration 199/1000 | Loss: 0.00003017
Iteration 200/1000 | Loss: 0.00003017
Iteration 201/1000 | Loss: 0.00003017
Iteration 202/1000 | Loss: 0.00003017
Iteration 203/1000 | Loss: 0.00003017
Iteration 204/1000 | Loss: 0.00003017
Iteration 205/1000 | Loss: 0.00003017
Iteration 206/1000 | Loss: 0.00003017
Iteration 207/1000 | Loss: 0.00003017
Iteration 208/1000 | Loss: 0.00003017
Iteration 209/1000 | Loss: 0.00003017
Iteration 210/1000 | Loss: 0.00003017
Iteration 211/1000 | Loss: 0.00003017
Iteration 212/1000 | Loss: 0.00003017
Iteration 213/1000 | Loss: 0.00003017
Iteration 214/1000 | Loss: 0.00003017
Iteration 215/1000 | Loss: 0.00003017
Iteration 216/1000 | Loss: 0.00003017
Iteration 217/1000 | Loss: 0.00003017
Iteration 218/1000 | Loss: 0.00003017
Iteration 219/1000 | Loss: 0.00003017
Iteration 220/1000 | Loss: 0.00003017
Iteration 221/1000 | Loss: 0.00003017
Iteration 222/1000 | Loss: 0.00003017
Iteration 223/1000 | Loss: 0.00003017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [3.017286871909164e-05, 3.017286871909164e-05, 3.017286871909164e-05, 3.017286871909164e-05, 3.017286871909164e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.017286871909164e-05

Optimization complete. Final v2v error: 4.814665794372559 mm

Highest mean error: 5.152966499328613 mm for frame 77

Lowest mean error: 4.471958637237549 mm for frame 154

Saving results

Total time: 38.42776703834534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01091052
Iteration 2/25 | Loss: 0.00214475
Iteration 3/25 | Loss: 0.00168385
Iteration 4/25 | Loss: 0.00165548
Iteration 5/25 | Loss: 0.00164361
Iteration 6/25 | Loss: 0.00164004
Iteration 7/25 | Loss: 0.00163887
Iteration 8/25 | Loss: 0.00163887
Iteration 9/25 | Loss: 0.00163887
Iteration 10/25 | Loss: 0.00163887
Iteration 11/25 | Loss: 0.00163887
Iteration 12/25 | Loss: 0.00163887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0016388733638450503, 0.0016388733638450503, 0.0016388733638450503, 0.0016388733638450503, 0.0016388733638450503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016388733638450503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87549233
Iteration 2/25 | Loss: 0.00182171
Iteration 3/25 | Loss: 0.00182171
Iteration 4/25 | Loss: 0.00182171
Iteration 5/25 | Loss: 0.00182171
Iteration 6/25 | Loss: 0.00182171
Iteration 7/25 | Loss: 0.00182171
Iteration 8/25 | Loss: 0.00182170
Iteration 9/25 | Loss: 0.00182170
Iteration 10/25 | Loss: 0.00182170
Iteration 11/25 | Loss: 0.00182170
Iteration 12/25 | Loss: 0.00182170
Iteration 13/25 | Loss: 0.00182170
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0018217049073427916, 0.0018217049073427916, 0.0018217049073427916, 0.0018217049073427916, 0.0018217049073427916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018217049073427916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182170
Iteration 2/1000 | Loss: 0.00010562
Iteration 3/1000 | Loss: 0.00007801
Iteration 4/1000 | Loss: 0.00006888
Iteration 5/1000 | Loss: 0.00006074
Iteration 6/1000 | Loss: 0.00005762
Iteration 7/1000 | Loss: 0.00005562
Iteration 8/1000 | Loss: 0.00005476
Iteration 9/1000 | Loss: 0.00005416
Iteration 10/1000 | Loss: 0.00005365
Iteration 11/1000 | Loss: 0.00005318
Iteration 12/1000 | Loss: 0.00005289
Iteration 13/1000 | Loss: 0.00005277
Iteration 14/1000 | Loss: 0.00005269
Iteration 15/1000 | Loss: 0.00005267
Iteration 16/1000 | Loss: 0.00005266
Iteration 17/1000 | Loss: 0.00005266
Iteration 18/1000 | Loss: 0.00005265
Iteration 19/1000 | Loss: 0.00005265
Iteration 20/1000 | Loss: 0.00005263
Iteration 21/1000 | Loss: 0.00005262
Iteration 22/1000 | Loss: 0.00005260
Iteration 23/1000 | Loss: 0.00005257
Iteration 24/1000 | Loss: 0.00005252
Iteration 25/1000 | Loss: 0.00005251
Iteration 26/1000 | Loss: 0.00005251
Iteration 27/1000 | Loss: 0.00005248
Iteration 28/1000 | Loss: 0.00005248
Iteration 29/1000 | Loss: 0.00005248
Iteration 30/1000 | Loss: 0.00005247
Iteration 31/1000 | Loss: 0.00005247
Iteration 32/1000 | Loss: 0.00005246
Iteration 33/1000 | Loss: 0.00005245
Iteration 34/1000 | Loss: 0.00005245
Iteration 35/1000 | Loss: 0.00005245
Iteration 36/1000 | Loss: 0.00005245
Iteration 37/1000 | Loss: 0.00005245
Iteration 38/1000 | Loss: 0.00005244
Iteration 39/1000 | Loss: 0.00005244
Iteration 40/1000 | Loss: 0.00005243
Iteration 41/1000 | Loss: 0.00005243
Iteration 42/1000 | Loss: 0.00005242
Iteration 43/1000 | Loss: 0.00005242
Iteration 44/1000 | Loss: 0.00005241
Iteration 45/1000 | Loss: 0.00005241
Iteration 46/1000 | Loss: 0.00005241
Iteration 47/1000 | Loss: 0.00005241
Iteration 48/1000 | Loss: 0.00005241
Iteration 49/1000 | Loss: 0.00005240
Iteration 50/1000 | Loss: 0.00005240
Iteration 51/1000 | Loss: 0.00005240
Iteration 52/1000 | Loss: 0.00005239
Iteration 53/1000 | Loss: 0.00005239
Iteration 54/1000 | Loss: 0.00005238
Iteration 55/1000 | Loss: 0.00005238
Iteration 56/1000 | Loss: 0.00005238
Iteration 57/1000 | Loss: 0.00005238
Iteration 58/1000 | Loss: 0.00005237
Iteration 59/1000 | Loss: 0.00005237
Iteration 60/1000 | Loss: 0.00005237
Iteration 61/1000 | Loss: 0.00005237
Iteration 62/1000 | Loss: 0.00005237
Iteration 63/1000 | Loss: 0.00005237
Iteration 64/1000 | Loss: 0.00005237
Iteration 65/1000 | Loss: 0.00005236
Iteration 66/1000 | Loss: 0.00005236
Iteration 67/1000 | Loss: 0.00005235
Iteration 68/1000 | Loss: 0.00005235
Iteration 69/1000 | Loss: 0.00005235
Iteration 70/1000 | Loss: 0.00005234
Iteration 71/1000 | Loss: 0.00005234
Iteration 72/1000 | Loss: 0.00005234
Iteration 73/1000 | Loss: 0.00005234
Iteration 74/1000 | Loss: 0.00005233
Iteration 75/1000 | Loss: 0.00005233
Iteration 76/1000 | Loss: 0.00005233
Iteration 77/1000 | Loss: 0.00005233
Iteration 78/1000 | Loss: 0.00005233
Iteration 79/1000 | Loss: 0.00005233
Iteration 80/1000 | Loss: 0.00005232
Iteration 81/1000 | Loss: 0.00005232
Iteration 82/1000 | Loss: 0.00005232
Iteration 83/1000 | Loss: 0.00005232
Iteration 84/1000 | Loss: 0.00005232
Iteration 85/1000 | Loss: 0.00005232
Iteration 86/1000 | Loss: 0.00005232
Iteration 87/1000 | Loss: 0.00005232
Iteration 88/1000 | Loss: 0.00005231
Iteration 89/1000 | Loss: 0.00005231
Iteration 90/1000 | Loss: 0.00005231
Iteration 91/1000 | Loss: 0.00005231
Iteration 92/1000 | Loss: 0.00005231
Iteration 93/1000 | Loss: 0.00005231
Iteration 94/1000 | Loss: 0.00005230
Iteration 95/1000 | Loss: 0.00005230
Iteration 96/1000 | Loss: 0.00005230
Iteration 97/1000 | Loss: 0.00005230
Iteration 98/1000 | Loss: 0.00005230
Iteration 99/1000 | Loss: 0.00005230
Iteration 100/1000 | Loss: 0.00005230
Iteration 101/1000 | Loss: 0.00005230
Iteration 102/1000 | Loss: 0.00005230
Iteration 103/1000 | Loss: 0.00005230
Iteration 104/1000 | Loss: 0.00005230
Iteration 105/1000 | Loss: 0.00005230
Iteration 106/1000 | Loss: 0.00005230
Iteration 107/1000 | Loss: 0.00005230
Iteration 108/1000 | Loss: 0.00005229
Iteration 109/1000 | Loss: 0.00005229
Iteration 110/1000 | Loss: 0.00005229
Iteration 111/1000 | Loss: 0.00005229
Iteration 112/1000 | Loss: 0.00005229
Iteration 113/1000 | Loss: 0.00005229
Iteration 114/1000 | Loss: 0.00005229
Iteration 115/1000 | Loss: 0.00005229
Iteration 116/1000 | Loss: 0.00005229
Iteration 117/1000 | Loss: 0.00005229
Iteration 118/1000 | Loss: 0.00005229
Iteration 119/1000 | Loss: 0.00005229
Iteration 120/1000 | Loss: 0.00005229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [5.229328598943539e-05, 5.229328598943539e-05, 5.229328598943539e-05, 5.229328598943539e-05, 5.229328598943539e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.229328598943539e-05

Optimization complete. Final v2v error: 6.179396629333496 mm

Highest mean error: 6.775089740753174 mm for frame 11

Lowest mean error: 5.543413162231445 mm for frame 49

Saving results

Total time: 41.63841652870178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864078
Iteration 2/25 | Loss: 0.00194427
Iteration 3/25 | Loss: 0.00154202
Iteration 4/25 | Loss: 0.00151141
Iteration 5/25 | Loss: 0.00150698
Iteration 6/25 | Loss: 0.00149759
Iteration 7/25 | Loss: 0.00149604
Iteration 8/25 | Loss: 0.00148634
Iteration 9/25 | Loss: 0.00148129
Iteration 10/25 | Loss: 0.00148061
Iteration 11/25 | Loss: 0.00148037
Iteration 12/25 | Loss: 0.00148037
Iteration 13/25 | Loss: 0.00148037
Iteration 14/25 | Loss: 0.00148037
Iteration 15/25 | Loss: 0.00148037
Iteration 16/25 | Loss: 0.00148037
Iteration 17/25 | Loss: 0.00148037
Iteration 18/25 | Loss: 0.00148036
Iteration 19/25 | Loss: 0.00148036
Iteration 20/25 | Loss: 0.00148036
Iteration 21/25 | Loss: 0.00148036
Iteration 22/25 | Loss: 0.00148036
Iteration 23/25 | Loss: 0.00148036
Iteration 24/25 | Loss: 0.00148036
Iteration 25/25 | Loss: 0.00148036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 15.61757851
Iteration 2/25 | Loss: 0.00166311
Iteration 3/25 | Loss: 0.00166278
Iteration 4/25 | Loss: 0.00166278
Iteration 5/25 | Loss: 0.00166278
Iteration 6/25 | Loss: 0.00166278
Iteration 7/25 | Loss: 0.00166278
Iteration 8/25 | Loss: 0.00166278
Iteration 9/25 | Loss: 0.00166278
Iteration 10/25 | Loss: 0.00166278
Iteration 11/25 | Loss: 0.00166278
Iteration 12/25 | Loss: 0.00166278
Iteration 13/25 | Loss: 0.00166278
Iteration 14/25 | Loss: 0.00166278
Iteration 15/25 | Loss: 0.00166278
Iteration 16/25 | Loss: 0.00166278
Iteration 17/25 | Loss: 0.00166278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016627783188596368, 0.0016627783188596368, 0.0016627783188596368, 0.0016627783188596368, 0.0016627783188596368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016627783188596368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166278
Iteration 2/1000 | Loss: 0.00005780
Iteration 3/1000 | Loss: 0.00004488
Iteration 4/1000 | Loss: 0.00003802
Iteration 5/1000 | Loss: 0.00003643
Iteration 6/1000 | Loss: 0.00003522
Iteration 7/1000 | Loss: 0.00003460
Iteration 8/1000 | Loss: 0.00003423
Iteration 9/1000 | Loss: 0.00003380
Iteration 10/1000 | Loss: 0.00003354
Iteration 11/1000 | Loss: 0.00003333
Iteration 12/1000 | Loss: 0.00003331
Iteration 13/1000 | Loss: 0.00003323
Iteration 14/1000 | Loss: 0.00003321
Iteration 15/1000 | Loss: 0.00003321
Iteration 16/1000 | Loss: 0.00003321
Iteration 17/1000 | Loss: 0.00003320
Iteration 18/1000 | Loss: 0.00003320
Iteration 19/1000 | Loss: 0.00003320
Iteration 20/1000 | Loss: 0.00003319
Iteration 21/1000 | Loss: 0.00003317
Iteration 22/1000 | Loss: 0.00003313
Iteration 23/1000 | Loss: 0.00003312
Iteration 24/1000 | Loss: 0.00003311
Iteration 25/1000 | Loss: 0.00003311
Iteration 26/1000 | Loss: 0.00003309
Iteration 27/1000 | Loss: 0.00003309
Iteration 28/1000 | Loss: 0.00003308
Iteration 29/1000 | Loss: 0.00003308
Iteration 30/1000 | Loss: 0.00003307
Iteration 31/1000 | Loss: 0.00003305
Iteration 32/1000 | Loss: 0.00003305
Iteration 33/1000 | Loss: 0.00003305
Iteration 34/1000 | Loss: 0.00003305
Iteration 35/1000 | Loss: 0.00003304
Iteration 36/1000 | Loss: 0.00003304
Iteration 37/1000 | Loss: 0.00003304
Iteration 38/1000 | Loss: 0.00003304
Iteration 39/1000 | Loss: 0.00003304
Iteration 40/1000 | Loss: 0.00003304
Iteration 41/1000 | Loss: 0.00003304
Iteration 42/1000 | Loss: 0.00003302
Iteration 43/1000 | Loss: 0.00003301
Iteration 44/1000 | Loss: 0.00003301
Iteration 45/1000 | Loss: 0.00003300
Iteration 46/1000 | Loss: 0.00003298
Iteration 47/1000 | Loss: 0.00003298
Iteration 48/1000 | Loss: 0.00003298
Iteration 49/1000 | Loss: 0.00003298
Iteration 50/1000 | Loss: 0.00003298
Iteration 51/1000 | Loss: 0.00003298
Iteration 52/1000 | Loss: 0.00003297
Iteration 53/1000 | Loss: 0.00003297
Iteration 54/1000 | Loss: 0.00003297
Iteration 55/1000 | Loss: 0.00003297
Iteration 56/1000 | Loss: 0.00003297
Iteration 57/1000 | Loss: 0.00003297
Iteration 58/1000 | Loss: 0.00003297
Iteration 59/1000 | Loss: 0.00003297
Iteration 60/1000 | Loss: 0.00003297
Iteration 61/1000 | Loss: 0.00003297
Iteration 62/1000 | Loss: 0.00003297
Iteration 63/1000 | Loss: 0.00003297
Iteration 64/1000 | Loss: 0.00003296
Iteration 65/1000 | Loss: 0.00003296
Iteration 66/1000 | Loss: 0.00003296
Iteration 67/1000 | Loss: 0.00003295
Iteration 68/1000 | Loss: 0.00003294
Iteration 69/1000 | Loss: 0.00003294
Iteration 70/1000 | Loss: 0.00003294
Iteration 71/1000 | Loss: 0.00003294
Iteration 72/1000 | Loss: 0.00003294
Iteration 73/1000 | Loss: 0.00003294
Iteration 74/1000 | Loss: 0.00003294
Iteration 75/1000 | Loss: 0.00003294
Iteration 76/1000 | Loss: 0.00003294
Iteration 77/1000 | Loss: 0.00003293
Iteration 78/1000 | Loss: 0.00003293
Iteration 79/1000 | Loss: 0.00003293
Iteration 80/1000 | Loss: 0.00003293
Iteration 81/1000 | Loss: 0.00003293
Iteration 82/1000 | Loss: 0.00003293
Iteration 83/1000 | Loss: 0.00003293
Iteration 84/1000 | Loss: 0.00003293
Iteration 85/1000 | Loss: 0.00003293
Iteration 86/1000 | Loss: 0.00003293
Iteration 87/1000 | Loss: 0.00003293
Iteration 88/1000 | Loss: 0.00003293
Iteration 89/1000 | Loss: 0.00003293
Iteration 90/1000 | Loss: 0.00003292
Iteration 91/1000 | Loss: 0.00003292
Iteration 92/1000 | Loss: 0.00003292
Iteration 93/1000 | Loss: 0.00003292
Iteration 94/1000 | Loss: 0.00003292
Iteration 95/1000 | Loss: 0.00003292
Iteration 96/1000 | Loss: 0.00003292
Iteration 97/1000 | Loss: 0.00003292
Iteration 98/1000 | Loss: 0.00003292
Iteration 99/1000 | Loss: 0.00003292
Iteration 100/1000 | Loss: 0.00003292
Iteration 101/1000 | Loss: 0.00003292
Iteration 102/1000 | Loss: 0.00003291
Iteration 103/1000 | Loss: 0.00003291
Iteration 104/1000 | Loss: 0.00003291
Iteration 105/1000 | Loss: 0.00003291
Iteration 106/1000 | Loss: 0.00003291
Iteration 107/1000 | Loss: 0.00003291
Iteration 108/1000 | Loss: 0.00003291
Iteration 109/1000 | Loss: 0.00003291
Iteration 110/1000 | Loss: 0.00003291
Iteration 111/1000 | Loss: 0.00003291
Iteration 112/1000 | Loss: 0.00003291
Iteration 113/1000 | Loss: 0.00003291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [3.2914835173869506e-05, 3.2914835173869506e-05, 3.2914835173869506e-05, 3.2914835173869506e-05, 3.2914835173869506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2914835173869506e-05

Optimization complete. Final v2v error: 5.01171350479126 mm

Highest mean error: 5.292648792266846 mm for frame 28

Lowest mean error: 4.695811748504639 mm for frame 67

Saving results

Total time: 48.78644275665283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00940318
Iteration 2/25 | Loss: 0.00154562
Iteration 3/25 | Loss: 0.00144028
Iteration 4/25 | Loss: 0.00142950
Iteration 5/25 | Loss: 0.00142617
Iteration 6/25 | Loss: 0.00142570
Iteration 7/25 | Loss: 0.00142570
Iteration 8/25 | Loss: 0.00142570
Iteration 9/25 | Loss: 0.00142570
Iteration 10/25 | Loss: 0.00142570
Iteration 11/25 | Loss: 0.00142570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014256980502977967, 0.0014256980502977967, 0.0014256980502977967, 0.0014256980502977967, 0.0014256980502977967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014256980502977967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53102922
Iteration 2/25 | Loss: 0.00181702
Iteration 3/25 | Loss: 0.00181701
Iteration 4/25 | Loss: 0.00181701
Iteration 5/25 | Loss: 0.00181701
Iteration 6/25 | Loss: 0.00181701
Iteration 7/25 | Loss: 0.00181701
Iteration 8/25 | Loss: 0.00181701
Iteration 9/25 | Loss: 0.00181701
Iteration 10/25 | Loss: 0.00181701
Iteration 11/25 | Loss: 0.00181701
Iteration 12/25 | Loss: 0.00181701
Iteration 13/25 | Loss: 0.00181701
Iteration 14/25 | Loss: 0.00181701
Iteration 15/25 | Loss: 0.00181701
Iteration 16/25 | Loss: 0.00181701
Iteration 17/25 | Loss: 0.00181701
Iteration 18/25 | Loss: 0.00181701
Iteration 19/25 | Loss: 0.00181701
Iteration 20/25 | Loss: 0.00181701
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018170136027038097, 0.0018170136027038097, 0.0018170136027038097, 0.0018170136027038097, 0.0018170136027038097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018170136027038097

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181701
Iteration 2/1000 | Loss: 0.00005349
Iteration 3/1000 | Loss: 0.00003950
Iteration 4/1000 | Loss: 0.00003392
Iteration 5/1000 | Loss: 0.00003179
Iteration 6/1000 | Loss: 0.00003060
Iteration 7/1000 | Loss: 0.00002998
Iteration 8/1000 | Loss: 0.00002978
Iteration 9/1000 | Loss: 0.00002947
Iteration 10/1000 | Loss: 0.00002915
Iteration 11/1000 | Loss: 0.00002897
Iteration 12/1000 | Loss: 0.00002894
Iteration 13/1000 | Loss: 0.00002894
Iteration 14/1000 | Loss: 0.00002888
Iteration 15/1000 | Loss: 0.00002888
Iteration 16/1000 | Loss: 0.00002884
Iteration 17/1000 | Loss: 0.00002883
Iteration 18/1000 | Loss: 0.00002883
Iteration 19/1000 | Loss: 0.00002882
Iteration 20/1000 | Loss: 0.00002882
Iteration 21/1000 | Loss: 0.00002881
Iteration 22/1000 | Loss: 0.00002880
Iteration 23/1000 | Loss: 0.00002880
Iteration 24/1000 | Loss: 0.00002878
Iteration 25/1000 | Loss: 0.00002878
Iteration 26/1000 | Loss: 0.00002877
Iteration 27/1000 | Loss: 0.00002877
Iteration 28/1000 | Loss: 0.00002877
Iteration 29/1000 | Loss: 0.00002876
Iteration 30/1000 | Loss: 0.00002876
Iteration 31/1000 | Loss: 0.00002875
Iteration 32/1000 | Loss: 0.00002873
Iteration 33/1000 | Loss: 0.00002873
Iteration 34/1000 | Loss: 0.00002873
Iteration 35/1000 | Loss: 0.00002873
Iteration 36/1000 | Loss: 0.00002873
Iteration 37/1000 | Loss: 0.00002873
Iteration 38/1000 | Loss: 0.00002873
Iteration 39/1000 | Loss: 0.00002873
Iteration 40/1000 | Loss: 0.00002873
Iteration 41/1000 | Loss: 0.00002873
Iteration 42/1000 | Loss: 0.00002873
Iteration 43/1000 | Loss: 0.00002872
Iteration 44/1000 | Loss: 0.00002872
Iteration 45/1000 | Loss: 0.00002872
Iteration 46/1000 | Loss: 0.00002871
Iteration 47/1000 | Loss: 0.00002871
Iteration 48/1000 | Loss: 0.00002871
Iteration 49/1000 | Loss: 0.00002870
Iteration 50/1000 | Loss: 0.00002870
Iteration 51/1000 | Loss: 0.00002870
Iteration 52/1000 | Loss: 0.00002869
Iteration 53/1000 | Loss: 0.00002869
Iteration 54/1000 | Loss: 0.00002869
Iteration 55/1000 | Loss: 0.00002869
Iteration 56/1000 | Loss: 0.00002869
Iteration 57/1000 | Loss: 0.00002868
Iteration 58/1000 | Loss: 0.00002868
Iteration 59/1000 | Loss: 0.00002868
Iteration 60/1000 | Loss: 0.00002868
Iteration 61/1000 | Loss: 0.00002868
Iteration 62/1000 | Loss: 0.00002868
Iteration 63/1000 | Loss: 0.00002868
Iteration 64/1000 | Loss: 0.00002868
Iteration 65/1000 | Loss: 0.00002868
Iteration 66/1000 | Loss: 0.00002868
Iteration 67/1000 | Loss: 0.00002868
Iteration 68/1000 | Loss: 0.00002868
Iteration 69/1000 | Loss: 0.00002867
Iteration 70/1000 | Loss: 0.00002867
Iteration 71/1000 | Loss: 0.00002867
Iteration 72/1000 | Loss: 0.00002866
Iteration 73/1000 | Loss: 0.00002866
Iteration 74/1000 | Loss: 0.00002865
Iteration 75/1000 | Loss: 0.00002865
Iteration 76/1000 | Loss: 0.00002865
Iteration 77/1000 | Loss: 0.00002865
Iteration 78/1000 | Loss: 0.00002865
Iteration 79/1000 | Loss: 0.00002865
Iteration 80/1000 | Loss: 0.00002864
Iteration 81/1000 | Loss: 0.00002864
Iteration 82/1000 | Loss: 0.00002864
Iteration 83/1000 | Loss: 0.00002863
Iteration 84/1000 | Loss: 0.00002863
Iteration 85/1000 | Loss: 0.00002863
Iteration 86/1000 | Loss: 0.00002862
Iteration 87/1000 | Loss: 0.00002862
Iteration 88/1000 | Loss: 0.00002862
Iteration 89/1000 | Loss: 0.00002862
Iteration 90/1000 | Loss: 0.00002862
Iteration 91/1000 | Loss: 0.00002861
Iteration 92/1000 | Loss: 0.00002861
Iteration 93/1000 | Loss: 0.00002861
Iteration 94/1000 | Loss: 0.00002860
Iteration 95/1000 | Loss: 0.00002860
Iteration 96/1000 | Loss: 0.00002860
Iteration 97/1000 | Loss: 0.00002859
Iteration 98/1000 | Loss: 0.00002859
Iteration 99/1000 | Loss: 0.00002859
Iteration 100/1000 | Loss: 0.00002859
Iteration 101/1000 | Loss: 0.00002859
Iteration 102/1000 | Loss: 0.00002859
Iteration 103/1000 | Loss: 0.00002859
Iteration 104/1000 | Loss: 0.00002858
Iteration 105/1000 | Loss: 0.00002858
Iteration 106/1000 | Loss: 0.00002858
Iteration 107/1000 | Loss: 0.00002858
Iteration 108/1000 | Loss: 0.00002858
Iteration 109/1000 | Loss: 0.00002858
Iteration 110/1000 | Loss: 0.00002858
Iteration 111/1000 | Loss: 0.00002858
Iteration 112/1000 | Loss: 0.00002858
Iteration 113/1000 | Loss: 0.00002858
Iteration 114/1000 | Loss: 0.00002858
Iteration 115/1000 | Loss: 0.00002858
Iteration 116/1000 | Loss: 0.00002858
Iteration 117/1000 | Loss: 0.00002858
Iteration 118/1000 | Loss: 0.00002857
Iteration 119/1000 | Loss: 0.00002857
Iteration 120/1000 | Loss: 0.00002857
Iteration 121/1000 | Loss: 0.00002857
Iteration 122/1000 | Loss: 0.00002857
Iteration 123/1000 | Loss: 0.00002857
Iteration 124/1000 | Loss: 0.00002857
Iteration 125/1000 | Loss: 0.00002857
Iteration 126/1000 | Loss: 0.00002857
Iteration 127/1000 | Loss: 0.00002857
Iteration 128/1000 | Loss: 0.00002857
Iteration 129/1000 | Loss: 0.00002857
Iteration 130/1000 | Loss: 0.00002857
Iteration 131/1000 | Loss: 0.00002857
Iteration 132/1000 | Loss: 0.00002857
Iteration 133/1000 | Loss: 0.00002857
Iteration 134/1000 | Loss: 0.00002857
Iteration 135/1000 | Loss: 0.00002857
Iteration 136/1000 | Loss: 0.00002857
Iteration 137/1000 | Loss: 0.00002857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.8573336749104783e-05, 2.8573336749104783e-05, 2.8573336749104783e-05, 2.8573336749104783e-05, 2.8573336749104783e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8573336749104783e-05

Optimization complete. Final v2v error: 4.707759857177734 mm

Highest mean error: 4.996248245239258 mm for frame 164

Lowest mean error: 4.443374156951904 mm for frame 16

Saving results

Total time: 34.96620583534241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01125295
Iteration 2/25 | Loss: 0.00280840
Iteration 3/25 | Loss: 0.00150651
Iteration 4/25 | Loss: 0.00143754
Iteration 5/25 | Loss: 0.00143075
Iteration 6/25 | Loss: 0.00142984
Iteration 7/25 | Loss: 0.00142964
Iteration 8/25 | Loss: 0.00142964
Iteration 9/25 | Loss: 0.00142964
Iteration 10/25 | Loss: 0.00142964
Iteration 11/25 | Loss: 0.00142964
Iteration 12/25 | Loss: 0.00142964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014296374283730984, 0.0014296374283730984, 0.0014296374283730984, 0.0014296374283730984, 0.0014296374283730984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014296374283730984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53085053
Iteration 2/25 | Loss: 0.00194281
Iteration 3/25 | Loss: 0.00194281
Iteration 4/25 | Loss: 0.00194280
Iteration 5/25 | Loss: 0.00194280
Iteration 6/25 | Loss: 0.00194280
Iteration 7/25 | Loss: 0.00194280
Iteration 8/25 | Loss: 0.00194280
Iteration 9/25 | Loss: 0.00194280
Iteration 10/25 | Loss: 0.00194280
Iteration 11/25 | Loss: 0.00194280
Iteration 12/25 | Loss: 0.00194280
Iteration 13/25 | Loss: 0.00194280
Iteration 14/25 | Loss: 0.00194280
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0019428024534136057, 0.0019428024534136057, 0.0019428024534136057, 0.0019428024534136057, 0.0019428024534136057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019428024534136057

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194280
Iteration 2/1000 | Loss: 0.00006234
Iteration 3/1000 | Loss: 0.00004697
Iteration 4/1000 | Loss: 0.00003899
Iteration 5/1000 | Loss: 0.00003590
Iteration 6/1000 | Loss: 0.00003377
Iteration 7/1000 | Loss: 0.00003278
Iteration 8/1000 | Loss: 0.00003199
Iteration 9/1000 | Loss: 0.00003149
Iteration 10/1000 | Loss: 0.00003092
Iteration 11/1000 | Loss: 0.00003063
Iteration 12/1000 | Loss: 0.00003043
Iteration 13/1000 | Loss: 0.00003043
Iteration 14/1000 | Loss: 0.00003042
Iteration 15/1000 | Loss: 0.00003042
Iteration 16/1000 | Loss: 0.00003032
Iteration 17/1000 | Loss: 0.00003031
Iteration 18/1000 | Loss: 0.00003031
Iteration 19/1000 | Loss: 0.00003030
Iteration 20/1000 | Loss: 0.00003030
Iteration 21/1000 | Loss: 0.00003029
Iteration 22/1000 | Loss: 0.00003029
Iteration 23/1000 | Loss: 0.00003029
Iteration 24/1000 | Loss: 0.00003029
Iteration 25/1000 | Loss: 0.00003029
Iteration 26/1000 | Loss: 0.00003029
Iteration 27/1000 | Loss: 0.00003029
Iteration 28/1000 | Loss: 0.00003028
Iteration 29/1000 | Loss: 0.00003028
Iteration 30/1000 | Loss: 0.00003028
Iteration 31/1000 | Loss: 0.00003028
Iteration 32/1000 | Loss: 0.00003028
Iteration 33/1000 | Loss: 0.00003028
Iteration 34/1000 | Loss: 0.00003028
Iteration 35/1000 | Loss: 0.00003028
Iteration 36/1000 | Loss: 0.00003027
Iteration 37/1000 | Loss: 0.00003027
Iteration 38/1000 | Loss: 0.00003026
Iteration 39/1000 | Loss: 0.00003026
Iteration 40/1000 | Loss: 0.00003025
Iteration 41/1000 | Loss: 0.00003025
Iteration 42/1000 | Loss: 0.00003024
Iteration 43/1000 | Loss: 0.00003023
Iteration 44/1000 | Loss: 0.00003023
Iteration 45/1000 | Loss: 0.00003023
Iteration 46/1000 | Loss: 0.00003023
Iteration 47/1000 | Loss: 0.00003023
Iteration 48/1000 | Loss: 0.00003022
Iteration 49/1000 | Loss: 0.00003022
Iteration 50/1000 | Loss: 0.00003021
Iteration 51/1000 | Loss: 0.00003021
Iteration 52/1000 | Loss: 0.00003020
Iteration 53/1000 | Loss: 0.00003020
Iteration 54/1000 | Loss: 0.00003020
Iteration 55/1000 | Loss: 0.00003020
Iteration 56/1000 | Loss: 0.00003019
Iteration 57/1000 | Loss: 0.00003019
Iteration 58/1000 | Loss: 0.00003017
Iteration 59/1000 | Loss: 0.00003017
Iteration 60/1000 | Loss: 0.00003016
Iteration 61/1000 | Loss: 0.00003016
Iteration 62/1000 | Loss: 0.00003016
Iteration 63/1000 | Loss: 0.00003016
Iteration 64/1000 | Loss: 0.00003016
Iteration 65/1000 | Loss: 0.00003016
Iteration 66/1000 | Loss: 0.00003016
Iteration 67/1000 | Loss: 0.00003015
Iteration 68/1000 | Loss: 0.00003015
Iteration 69/1000 | Loss: 0.00003015
Iteration 70/1000 | Loss: 0.00003015
Iteration 71/1000 | Loss: 0.00003015
Iteration 72/1000 | Loss: 0.00003015
Iteration 73/1000 | Loss: 0.00003015
Iteration 74/1000 | Loss: 0.00003015
Iteration 75/1000 | Loss: 0.00003015
Iteration 76/1000 | Loss: 0.00003015
Iteration 77/1000 | Loss: 0.00003015
Iteration 78/1000 | Loss: 0.00003015
Iteration 79/1000 | Loss: 0.00003015
Iteration 80/1000 | Loss: 0.00003014
Iteration 81/1000 | Loss: 0.00003014
Iteration 82/1000 | Loss: 0.00003014
Iteration 83/1000 | Loss: 0.00003014
Iteration 84/1000 | Loss: 0.00003014
Iteration 85/1000 | Loss: 0.00003014
Iteration 86/1000 | Loss: 0.00003014
Iteration 87/1000 | Loss: 0.00003014
Iteration 88/1000 | Loss: 0.00003014
Iteration 89/1000 | Loss: 0.00003014
Iteration 90/1000 | Loss: 0.00003014
Iteration 91/1000 | Loss: 0.00003014
Iteration 92/1000 | Loss: 0.00003013
Iteration 93/1000 | Loss: 0.00003013
Iteration 94/1000 | Loss: 0.00003013
Iteration 95/1000 | Loss: 0.00003013
Iteration 96/1000 | Loss: 0.00003013
Iteration 97/1000 | Loss: 0.00003013
Iteration 98/1000 | Loss: 0.00003013
Iteration 99/1000 | Loss: 0.00003013
Iteration 100/1000 | Loss: 0.00003012
Iteration 101/1000 | Loss: 0.00003012
Iteration 102/1000 | Loss: 0.00003012
Iteration 103/1000 | Loss: 0.00003011
Iteration 104/1000 | Loss: 0.00003011
Iteration 105/1000 | Loss: 0.00003011
Iteration 106/1000 | Loss: 0.00003010
Iteration 107/1000 | Loss: 0.00003010
Iteration 108/1000 | Loss: 0.00003010
Iteration 109/1000 | Loss: 0.00003010
Iteration 110/1000 | Loss: 0.00003010
Iteration 111/1000 | Loss: 0.00003010
Iteration 112/1000 | Loss: 0.00003010
Iteration 113/1000 | Loss: 0.00003010
Iteration 114/1000 | Loss: 0.00003010
Iteration 115/1000 | Loss: 0.00003010
Iteration 116/1000 | Loss: 0.00003010
Iteration 117/1000 | Loss: 0.00003010
Iteration 118/1000 | Loss: 0.00003010
Iteration 119/1000 | Loss: 0.00003010
Iteration 120/1000 | Loss: 0.00003010
Iteration 121/1000 | Loss: 0.00003010
Iteration 122/1000 | Loss: 0.00003010
Iteration 123/1000 | Loss: 0.00003009
Iteration 124/1000 | Loss: 0.00003009
Iteration 125/1000 | Loss: 0.00003009
Iteration 126/1000 | Loss: 0.00003009
Iteration 127/1000 | Loss: 0.00003009
Iteration 128/1000 | Loss: 0.00003009
Iteration 129/1000 | Loss: 0.00003009
Iteration 130/1000 | Loss: 0.00003009
Iteration 131/1000 | Loss: 0.00003009
Iteration 132/1000 | Loss: 0.00003009
Iteration 133/1000 | Loss: 0.00003009
Iteration 134/1000 | Loss: 0.00003009
Iteration 135/1000 | Loss: 0.00003009
Iteration 136/1000 | Loss: 0.00003009
Iteration 137/1000 | Loss: 0.00003009
Iteration 138/1000 | Loss: 0.00003009
Iteration 139/1000 | Loss: 0.00003009
Iteration 140/1000 | Loss: 0.00003009
Iteration 141/1000 | Loss: 0.00003009
Iteration 142/1000 | Loss: 0.00003009
Iteration 143/1000 | Loss: 0.00003009
Iteration 144/1000 | Loss: 0.00003009
Iteration 145/1000 | Loss: 0.00003009
Iteration 146/1000 | Loss: 0.00003009
Iteration 147/1000 | Loss: 0.00003009
Iteration 148/1000 | Loss: 0.00003009
Iteration 149/1000 | Loss: 0.00003009
Iteration 150/1000 | Loss: 0.00003009
Iteration 151/1000 | Loss: 0.00003009
Iteration 152/1000 | Loss: 0.00003009
Iteration 153/1000 | Loss: 0.00003009
Iteration 154/1000 | Loss: 0.00003009
Iteration 155/1000 | Loss: 0.00003009
Iteration 156/1000 | Loss: 0.00003009
Iteration 157/1000 | Loss: 0.00003009
Iteration 158/1000 | Loss: 0.00003009
Iteration 159/1000 | Loss: 0.00003009
Iteration 160/1000 | Loss: 0.00003009
Iteration 161/1000 | Loss: 0.00003009
Iteration 162/1000 | Loss: 0.00003009
Iteration 163/1000 | Loss: 0.00003009
Iteration 164/1000 | Loss: 0.00003009
Iteration 165/1000 | Loss: 0.00003009
Iteration 166/1000 | Loss: 0.00003009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [3.0093764507910237e-05, 3.0093764507910237e-05, 3.0093764507910237e-05, 3.0093764507910237e-05, 3.0093764507910237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0093764507910237e-05

Optimization complete. Final v2v error: 4.878425598144531 mm

Highest mean error: 5.701508522033691 mm for frame 237

Lowest mean error: 4.489108562469482 mm for frame 0

Saving results

Total time: 41.92846369743347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496463
Iteration 2/25 | Loss: 0.00159705
Iteration 3/25 | Loss: 0.00150358
Iteration 4/25 | Loss: 0.00148737
Iteration 5/25 | Loss: 0.00148101
Iteration 6/25 | Loss: 0.00147944
Iteration 7/25 | Loss: 0.00147944
Iteration 8/25 | Loss: 0.00147944
Iteration 9/25 | Loss: 0.00147944
Iteration 10/25 | Loss: 0.00147944
Iteration 11/25 | Loss: 0.00147944
Iteration 12/25 | Loss: 0.00147944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014794430462643504, 0.0014794430462643504, 0.0014794430462643504, 0.0014794430462643504, 0.0014794430462643504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014794430462643504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90569413
Iteration 2/25 | Loss: 0.00172485
Iteration 3/25 | Loss: 0.00172485
Iteration 4/25 | Loss: 0.00172485
Iteration 5/25 | Loss: 0.00172485
Iteration 6/25 | Loss: 0.00172485
Iteration 7/25 | Loss: 0.00172485
Iteration 8/25 | Loss: 0.00172484
Iteration 9/25 | Loss: 0.00172484
Iteration 10/25 | Loss: 0.00172484
Iteration 11/25 | Loss: 0.00172484
Iteration 12/25 | Loss: 0.00172484
Iteration 13/25 | Loss: 0.00172484
Iteration 14/25 | Loss: 0.00172484
Iteration 15/25 | Loss: 0.00172484
Iteration 16/25 | Loss: 0.00172484
Iteration 17/25 | Loss: 0.00172484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001724844565615058, 0.001724844565615058, 0.001724844565615058, 0.001724844565615058, 0.001724844565615058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001724844565615058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172484
Iteration 2/1000 | Loss: 0.00006381
Iteration 3/1000 | Loss: 0.00004541
Iteration 4/1000 | Loss: 0.00004018
Iteration 5/1000 | Loss: 0.00003701
Iteration 6/1000 | Loss: 0.00003564
Iteration 7/1000 | Loss: 0.00003505
Iteration 8/1000 | Loss: 0.00003460
Iteration 9/1000 | Loss: 0.00003409
Iteration 10/1000 | Loss: 0.00003375
Iteration 11/1000 | Loss: 0.00003359
Iteration 12/1000 | Loss: 0.00003358
Iteration 13/1000 | Loss: 0.00003357
Iteration 14/1000 | Loss: 0.00003357
Iteration 15/1000 | Loss: 0.00003355
Iteration 16/1000 | Loss: 0.00003351
Iteration 17/1000 | Loss: 0.00003350
Iteration 18/1000 | Loss: 0.00003349
Iteration 19/1000 | Loss: 0.00003348
Iteration 20/1000 | Loss: 0.00003348
Iteration 21/1000 | Loss: 0.00003348
Iteration 22/1000 | Loss: 0.00003348
Iteration 23/1000 | Loss: 0.00003348
Iteration 24/1000 | Loss: 0.00003347
Iteration 25/1000 | Loss: 0.00003347
Iteration 26/1000 | Loss: 0.00003344
Iteration 27/1000 | Loss: 0.00003344
Iteration 28/1000 | Loss: 0.00003344
Iteration 29/1000 | Loss: 0.00003344
Iteration 30/1000 | Loss: 0.00003344
Iteration 31/1000 | Loss: 0.00003344
Iteration 32/1000 | Loss: 0.00003344
Iteration 33/1000 | Loss: 0.00003344
Iteration 34/1000 | Loss: 0.00003344
Iteration 35/1000 | Loss: 0.00003344
Iteration 36/1000 | Loss: 0.00003344
Iteration 37/1000 | Loss: 0.00003344
Iteration 38/1000 | Loss: 0.00003344
Iteration 39/1000 | Loss: 0.00003344
Iteration 40/1000 | Loss: 0.00003344
Iteration 41/1000 | Loss: 0.00003344
Iteration 42/1000 | Loss: 0.00003344
Iteration 43/1000 | Loss: 0.00003344
Iteration 44/1000 | Loss: 0.00003344
Iteration 45/1000 | Loss: 0.00003343
Iteration 46/1000 | Loss: 0.00003343
Iteration 47/1000 | Loss: 0.00003343
Iteration 48/1000 | Loss: 0.00003343
Iteration 49/1000 | Loss: 0.00003343
Iteration 50/1000 | Loss: 0.00003343
Iteration 51/1000 | Loss: 0.00003343
Iteration 52/1000 | Loss: 0.00003343
Iteration 53/1000 | Loss: 0.00003343
Iteration 54/1000 | Loss: 0.00003343
Iteration 55/1000 | Loss: 0.00003343
Iteration 56/1000 | Loss: 0.00003343
Iteration 57/1000 | Loss: 0.00003343
Iteration 58/1000 | Loss: 0.00003343
Iteration 59/1000 | Loss: 0.00003343
Iteration 60/1000 | Loss: 0.00003343
Iteration 61/1000 | Loss: 0.00003343
Iteration 62/1000 | Loss: 0.00003343
Iteration 63/1000 | Loss: 0.00003343
Iteration 64/1000 | Loss: 0.00003343
Iteration 65/1000 | Loss: 0.00003343
Iteration 66/1000 | Loss: 0.00003343
Iteration 67/1000 | Loss: 0.00003343
Iteration 68/1000 | Loss: 0.00003343
Iteration 69/1000 | Loss: 0.00003343
Iteration 70/1000 | Loss: 0.00003343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [3.343243588460609e-05, 3.343243588460609e-05, 3.343243588460609e-05, 3.343243588460609e-05, 3.343243588460609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.343243588460609e-05

Optimization complete. Final v2v error: 5.018579483032227 mm

Highest mean error: 5.360116481781006 mm for frame 99

Lowest mean error: 4.746970176696777 mm for frame 227

Saving results

Total time: 32.115928411483765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450457
Iteration 2/25 | Loss: 0.00150548
Iteration 3/25 | Loss: 0.00145409
Iteration 4/25 | Loss: 0.00144492
Iteration 5/25 | Loss: 0.00144218
Iteration 6/25 | Loss: 0.00144134
Iteration 7/25 | Loss: 0.00144132
Iteration 8/25 | Loss: 0.00144132
Iteration 9/25 | Loss: 0.00144132
Iteration 10/25 | Loss: 0.00144132
Iteration 11/25 | Loss: 0.00144132
Iteration 12/25 | Loss: 0.00144132
Iteration 13/25 | Loss: 0.00144132
Iteration 14/25 | Loss: 0.00144132
Iteration 15/25 | Loss: 0.00144132
Iteration 16/25 | Loss: 0.00144132
Iteration 17/25 | Loss: 0.00144132
Iteration 18/25 | Loss: 0.00144132
Iteration 19/25 | Loss: 0.00144132
Iteration 20/25 | Loss: 0.00144132
Iteration 21/25 | Loss: 0.00144132
Iteration 22/25 | Loss: 0.00144132
Iteration 23/25 | Loss: 0.00144132
Iteration 24/25 | Loss: 0.00144132
Iteration 25/25 | Loss: 0.00144132

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.73233056
Iteration 2/25 | Loss: 0.00179066
Iteration 3/25 | Loss: 0.00179065
Iteration 4/25 | Loss: 0.00179065
Iteration 5/25 | Loss: 0.00179065
Iteration 6/25 | Loss: 0.00179065
Iteration 7/25 | Loss: 0.00179065
Iteration 8/25 | Loss: 0.00179065
Iteration 9/25 | Loss: 0.00179065
Iteration 10/25 | Loss: 0.00179065
Iteration 11/25 | Loss: 0.00179065
Iteration 12/25 | Loss: 0.00179065
Iteration 13/25 | Loss: 0.00179065
Iteration 14/25 | Loss: 0.00179065
Iteration 15/25 | Loss: 0.00179065
Iteration 16/25 | Loss: 0.00179065
Iteration 17/25 | Loss: 0.00179065
Iteration 18/25 | Loss: 0.00179065
Iteration 19/25 | Loss: 0.00179065
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0017906494904309511, 0.0017906494904309511, 0.0017906494904309511, 0.0017906494904309511, 0.0017906494904309511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017906494904309511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179065
Iteration 2/1000 | Loss: 0.00005727
Iteration 3/1000 | Loss: 0.00003841
Iteration 4/1000 | Loss: 0.00003142
Iteration 5/1000 | Loss: 0.00002819
Iteration 6/1000 | Loss: 0.00002670
Iteration 7/1000 | Loss: 0.00002577
Iteration 8/1000 | Loss: 0.00002531
Iteration 9/1000 | Loss: 0.00002505
Iteration 10/1000 | Loss: 0.00002483
Iteration 11/1000 | Loss: 0.00002461
Iteration 12/1000 | Loss: 0.00002451
Iteration 13/1000 | Loss: 0.00002443
Iteration 14/1000 | Loss: 0.00002443
Iteration 15/1000 | Loss: 0.00002442
Iteration 16/1000 | Loss: 0.00002441
Iteration 17/1000 | Loss: 0.00002439
Iteration 18/1000 | Loss: 0.00002439
Iteration 19/1000 | Loss: 0.00002438
Iteration 20/1000 | Loss: 0.00002438
Iteration 21/1000 | Loss: 0.00002438
Iteration 22/1000 | Loss: 0.00002438
Iteration 23/1000 | Loss: 0.00002437
Iteration 24/1000 | Loss: 0.00002436
Iteration 25/1000 | Loss: 0.00002436
Iteration 26/1000 | Loss: 0.00002436
Iteration 27/1000 | Loss: 0.00002435
Iteration 28/1000 | Loss: 0.00002435
Iteration 29/1000 | Loss: 0.00002435
Iteration 30/1000 | Loss: 0.00002435
Iteration 31/1000 | Loss: 0.00002435
Iteration 32/1000 | Loss: 0.00002435
Iteration 33/1000 | Loss: 0.00002435
Iteration 34/1000 | Loss: 0.00002434
Iteration 35/1000 | Loss: 0.00002433
Iteration 36/1000 | Loss: 0.00002432
Iteration 37/1000 | Loss: 0.00002432
Iteration 38/1000 | Loss: 0.00002432
Iteration 39/1000 | Loss: 0.00002432
Iteration 40/1000 | Loss: 0.00002432
Iteration 41/1000 | Loss: 0.00002432
Iteration 42/1000 | Loss: 0.00002432
Iteration 43/1000 | Loss: 0.00002432
Iteration 44/1000 | Loss: 0.00002432
Iteration 45/1000 | Loss: 0.00002432
Iteration 46/1000 | Loss: 0.00002431
Iteration 47/1000 | Loss: 0.00002431
Iteration 48/1000 | Loss: 0.00002431
Iteration 49/1000 | Loss: 0.00002430
Iteration 50/1000 | Loss: 0.00002430
Iteration 51/1000 | Loss: 0.00002430
Iteration 52/1000 | Loss: 0.00002430
Iteration 53/1000 | Loss: 0.00002429
Iteration 54/1000 | Loss: 0.00002429
Iteration 55/1000 | Loss: 0.00002429
Iteration 56/1000 | Loss: 0.00002429
Iteration 57/1000 | Loss: 0.00002429
Iteration 58/1000 | Loss: 0.00002428
Iteration 59/1000 | Loss: 0.00002428
Iteration 60/1000 | Loss: 0.00002428
Iteration 61/1000 | Loss: 0.00002428
Iteration 62/1000 | Loss: 0.00002428
Iteration 63/1000 | Loss: 0.00002428
Iteration 64/1000 | Loss: 0.00002428
Iteration 65/1000 | Loss: 0.00002428
Iteration 66/1000 | Loss: 0.00002428
Iteration 67/1000 | Loss: 0.00002428
Iteration 68/1000 | Loss: 0.00002428
Iteration 69/1000 | Loss: 0.00002428
Iteration 70/1000 | Loss: 0.00002428
Iteration 71/1000 | Loss: 0.00002428
Iteration 72/1000 | Loss: 0.00002427
Iteration 73/1000 | Loss: 0.00002427
Iteration 74/1000 | Loss: 0.00002427
Iteration 75/1000 | Loss: 0.00002427
Iteration 76/1000 | Loss: 0.00002427
Iteration 77/1000 | Loss: 0.00002427
Iteration 78/1000 | Loss: 0.00002426
Iteration 79/1000 | Loss: 0.00002426
Iteration 80/1000 | Loss: 0.00002426
Iteration 81/1000 | Loss: 0.00002426
Iteration 82/1000 | Loss: 0.00002426
Iteration 83/1000 | Loss: 0.00002426
Iteration 84/1000 | Loss: 0.00002426
Iteration 85/1000 | Loss: 0.00002426
Iteration 86/1000 | Loss: 0.00002425
Iteration 87/1000 | Loss: 0.00002425
Iteration 88/1000 | Loss: 0.00002425
Iteration 89/1000 | Loss: 0.00002425
Iteration 90/1000 | Loss: 0.00002425
Iteration 91/1000 | Loss: 0.00002425
Iteration 92/1000 | Loss: 0.00002425
Iteration 93/1000 | Loss: 0.00002425
Iteration 94/1000 | Loss: 0.00002425
Iteration 95/1000 | Loss: 0.00002425
Iteration 96/1000 | Loss: 0.00002425
Iteration 97/1000 | Loss: 0.00002424
Iteration 98/1000 | Loss: 0.00002424
Iteration 99/1000 | Loss: 0.00002424
Iteration 100/1000 | Loss: 0.00002424
Iteration 101/1000 | Loss: 0.00002424
Iteration 102/1000 | Loss: 0.00002424
Iteration 103/1000 | Loss: 0.00002424
Iteration 104/1000 | Loss: 0.00002424
Iteration 105/1000 | Loss: 0.00002424
Iteration 106/1000 | Loss: 0.00002424
Iteration 107/1000 | Loss: 0.00002424
Iteration 108/1000 | Loss: 0.00002424
Iteration 109/1000 | Loss: 0.00002424
Iteration 110/1000 | Loss: 0.00002424
Iteration 111/1000 | Loss: 0.00002424
Iteration 112/1000 | Loss: 0.00002424
Iteration 113/1000 | Loss: 0.00002424
Iteration 114/1000 | Loss: 0.00002424
Iteration 115/1000 | Loss: 0.00002424
Iteration 116/1000 | Loss: 0.00002424
Iteration 117/1000 | Loss: 0.00002424
Iteration 118/1000 | Loss: 0.00002424
Iteration 119/1000 | Loss: 0.00002424
Iteration 120/1000 | Loss: 0.00002424
Iteration 121/1000 | Loss: 0.00002424
Iteration 122/1000 | Loss: 0.00002424
Iteration 123/1000 | Loss: 0.00002424
Iteration 124/1000 | Loss: 0.00002424
Iteration 125/1000 | Loss: 0.00002424
Iteration 126/1000 | Loss: 0.00002424
Iteration 127/1000 | Loss: 0.00002424
Iteration 128/1000 | Loss: 0.00002424
Iteration 129/1000 | Loss: 0.00002424
Iteration 130/1000 | Loss: 0.00002424
Iteration 131/1000 | Loss: 0.00002424
Iteration 132/1000 | Loss: 0.00002424
Iteration 133/1000 | Loss: 0.00002424
Iteration 134/1000 | Loss: 0.00002424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.4242935978691094e-05, 2.4242935978691094e-05, 2.4242935978691094e-05, 2.4242935978691094e-05, 2.4242935978691094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4242935978691094e-05

Optimization complete. Final v2v error: 4.2563371658325195 mm

Highest mean error: 4.604623794555664 mm for frame 19

Lowest mean error: 3.8219003677368164 mm for frame 49

Saving results

Total time: 32.78217101097107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515571
Iteration 2/25 | Loss: 0.00154970
Iteration 3/25 | Loss: 0.00148446
Iteration 4/25 | Loss: 0.00147104
Iteration 5/25 | Loss: 0.00146718
Iteration 6/25 | Loss: 0.00146598
Iteration 7/25 | Loss: 0.00146587
Iteration 8/25 | Loss: 0.00146587
Iteration 9/25 | Loss: 0.00146587
Iteration 10/25 | Loss: 0.00146587
Iteration 11/25 | Loss: 0.00146587
Iteration 12/25 | Loss: 0.00146587
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014658732106909156, 0.0014658732106909156, 0.0014658732106909156, 0.0014658732106909156, 0.0014658732106909156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014658732106909156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65409493
Iteration 2/25 | Loss: 0.00183159
Iteration 3/25 | Loss: 0.00183158
Iteration 4/25 | Loss: 0.00183158
Iteration 5/25 | Loss: 0.00183158
Iteration 6/25 | Loss: 0.00183158
Iteration 7/25 | Loss: 0.00183158
Iteration 8/25 | Loss: 0.00183158
Iteration 9/25 | Loss: 0.00183158
Iteration 10/25 | Loss: 0.00183158
Iteration 11/25 | Loss: 0.00183158
Iteration 12/25 | Loss: 0.00183158
Iteration 13/25 | Loss: 0.00183158
Iteration 14/25 | Loss: 0.00183158
Iteration 15/25 | Loss: 0.00183158
Iteration 16/25 | Loss: 0.00183158
Iteration 17/25 | Loss: 0.00183158
Iteration 18/25 | Loss: 0.00183158
Iteration 19/25 | Loss: 0.00183158
Iteration 20/25 | Loss: 0.00183158
Iteration 21/25 | Loss: 0.00183158
Iteration 22/25 | Loss: 0.00183158
Iteration 23/25 | Loss: 0.00183158
Iteration 24/25 | Loss: 0.00183158
Iteration 25/25 | Loss: 0.00183158
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001831580768339336, 0.001831580768339336, 0.001831580768339336, 0.001831580768339336, 0.001831580768339336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001831580768339336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183158
Iteration 2/1000 | Loss: 0.00005422
Iteration 3/1000 | Loss: 0.00003630
Iteration 4/1000 | Loss: 0.00003157
Iteration 5/1000 | Loss: 0.00002847
Iteration 6/1000 | Loss: 0.00002714
Iteration 7/1000 | Loss: 0.00002662
Iteration 8/1000 | Loss: 0.00002635
Iteration 9/1000 | Loss: 0.00002616
Iteration 10/1000 | Loss: 0.00002615
Iteration 11/1000 | Loss: 0.00002607
Iteration 12/1000 | Loss: 0.00002590
Iteration 13/1000 | Loss: 0.00002589
Iteration 14/1000 | Loss: 0.00002589
Iteration 15/1000 | Loss: 0.00002589
Iteration 16/1000 | Loss: 0.00002589
Iteration 17/1000 | Loss: 0.00002588
Iteration 18/1000 | Loss: 0.00002587
Iteration 19/1000 | Loss: 0.00002585
Iteration 20/1000 | Loss: 0.00002584
Iteration 21/1000 | Loss: 0.00002584
Iteration 22/1000 | Loss: 0.00002583
Iteration 23/1000 | Loss: 0.00002583
Iteration 24/1000 | Loss: 0.00002582
Iteration 25/1000 | Loss: 0.00002582
Iteration 26/1000 | Loss: 0.00002581
Iteration 27/1000 | Loss: 0.00002581
Iteration 28/1000 | Loss: 0.00002581
Iteration 29/1000 | Loss: 0.00002580
Iteration 30/1000 | Loss: 0.00002580
Iteration 31/1000 | Loss: 0.00002579
Iteration 32/1000 | Loss: 0.00002579
Iteration 33/1000 | Loss: 0.00002579
Iteration 34/1000 | Loss: 0.00002578
Iteration 35/1000 | Loss: 0.00002578
Iteration 36/1000 | Loss: 0.00002578
Iteration 37/1000 | Loss: 0.00002577
Iteration 38/1000 | Loss: 0.00002577
Iteration 39/1000 | Loss: 0.00002577
Iteration 40/1000 | Loss: 0.00002577
Iteration 41/1000 | Loss: 0.00002576
Iteration 42/1000 | Loss: 0.00002576
Iteration 43/1000 | Loss: 0.00002576
Iteration 44/1000 | Loss: 0.00002576
Iteration 45/1000 | Loss: 0.00002576
Iteration 46/1000 | Loss: 0.00002576
Iteration 47/1000 | Loss: 0.00002576
Iteration 48/1000 | Loss: 0.00002576
Iteration 49/1000 | Loss: 0.00002576
Iteration 50/1000 | Loss: 0.00002576
Iteration 51/1000 | Loss: 0.00002576
Iteration 52/1000 | Loss: 0.00002575
Iteration 53/1000 | Loss: 0.00002575
Iteration 54/1000 | Loss: 0.00002575
Iteration 55/1000 | Loss: 0.00002575
Iteration 56/1000 | Loss: 0.00002575
Iteration 57/1000 | Loss: 0.00002574
Iteration 58/1000 | Loss: 0.00002574
Iteration 59/1000 | Loss: 0.00002574
Iteration 60/1000 | Loss: 0.00002573
Iteration 61/1000 | Loss: 0.00002573
Iteration 62/1000 | Loss: 0.00002573
Iteration 63/1000 | Loss: 0.00002573
Iteration 64/1000 | Loss: 0.00002573
Iteration 65/1000 | Loss: 0.00002572
Iteration 66/1000 | Loss: 0.00002572
Iteration 67/1000 | Loss: 0.00002572
Iteration 68/1000 | Loss: 0.00002572
Iteration 69/1000 | Loss: 0.00002572
Iteration 70/1000 | Loss: 0.00002572
Iteration 71/1000 | Loss: 0.00002571
Iteration 72/1000 | Loss: 0.00002571
Iteration 73/1000 | Loss: 0.00002571
Iteration 74/1000 | Loss: 0.00002570
Iteration 75/1000 | Loss: 0.00002569
Iteration 76/1000 | Loss: 0.00002569
Iteration 77/1000 | Loss: 0.00002569
Iteration 78/1000 | Loss: 0.00002569
Iteration 79/1000 | Loss: 0.00002569
Iteration 80/1000 | Loss: 0.00002569
Iteration 81/1000 | Loss: 0.00002569
Iteration 82/1000 | Loss: 0.00002569
Iteration 83/1000 | Loss: 0.00002569
Iteration 84/1000 | Loss: 0.00002569
Iteration 85/1000 | Loss: 0.00002568
Iteration 86/1000 | Loss: 0.00002568
Iteration 87/1000 | Loss: 0.00002568
Iteration 88/1000 | Loss: 0.00002568
Iteration 89/1000 | Loss: 0.00002567
Iteration 90/1000 | Loss: 0.00002567
Iteration 91/1000 | Loss: 0.00002567
Iteration 92/1000 | Loss: 0.00002567
Iteration 93/1000 | Loss: 0.00002567
Iteration 94/1000 | Loss: 0.00002567
Iteration 95/1000 | Loss: 0.00002567
Iteration 96/1000 | Loss: 0.00002567
Iteration 97/1000 | Loss: 0.00002567
Iteration 98/1000 | Loss: 0.00002567
Iteration 99/1000 | Loss: 0.00002567
Iteration 100/1000 | Loss: 0.00002566
Iteration 101/1000 | Loss: 0.00002566
Iteration 102/1000 | Loss: 0.00002566
Iteration 103/1000 | Loss: 0.00002566
Iteration 104/1000 | Loss: 0.00002566
Iteration 105/1000 | Loss: 0.00002565
Iteration 106/1000 | Loss: 0.00002565
Iteration 107/1000 | Loss: 0.00002565
Iteration 108/1000 | Loss: 0.00002565
Iteration 109/1000 | Loss: 0.00002565
Iteration 110/1000 | Loss: 0.00002565
Iteration 111/1000 | Loss: 0.00002565
Iteration 112/1000 | Loss: 0.00002565
Iteration 113/1000 | Loss: 0.00002564
Iteration 114/1000 | Loss: 0.00002564
Iteration 115/1000 | Loss: 0.00002564
Iteration 116/1000 | Loss: 0.00002564
Iteration 117/1000 | Loss: 0.00002564
Iteration 118/1000 | Loss: 0.00002563
Iteration 119/1000 | Loss: 0.00002563
Iteration 120/1000 | Loss: 0.00002563
Iteration 121/1000 | Loss: 0.00002563
Iteration 122/1000 | Loss: 0.00002563
Iteration 123/1000 | Loss: 0.00002563
Iteration 124/1000 | Loss: 0.00002563
Iteration 125/1000 | Loss: 0.00002563
Iteration 126/1000 | Loss: 0.00002562
Iteration 127/1000 | Loss: 0.00002562
Iteration 128/1000 | Loss: 0.00002562
Iteration 129/1000 | Loss: 0.00002562
Iteration 130/1000 | Loss: 0.00002562
Iteration 131/1000 | Loss: 0.00002561
Iteration 132/1000 | Loss: 0.00002561
Iteration 133/1000 | Loss: 0.00002561
Iteration 134/1000 | Loss: 0.00002561
Iteration 135/1000 | Loss: 0.00002561
Iteration 136/1000 | Loss: 0.00002561
Iteration 137/1000 | Loss: 0.00002561
Iteration 138/1000 | Loss: 0.00002561
Iteration 139/1000 | Loss: 0.00002561
Iteration 140/1000 | Loss: 0.00002561
Iteration 141/1000 | Loss: 0.00002561
Iteration 142/1000 | Loss: 0.00002560
Iteration 143/1000 | Loss: 0.00002560
Iteration 144/1000 | Loss: 0.00002560
Iteration 145/1000 | Loss: 0.00002560
Iteration 146/1000 | Loss: 0.00002560
Iteration 147/1000 | Loss: 0.00002560
Iteration 148/1000 | Loss: 0.00002560
Iteration 149/1000 | Loss: 0.00002560
Iteration 150/1000 | Loss: 0.00002560
Iteration 151/1000 | Loss: 0.00002560
Iteration 152/1000 | Loss: 0.00002560
Iteration 153/1000 | Loss: 0.00002559
Iteration 154/1000 | Loss: 0.00002559
Iteration 155/1000 | Loss: 0.00002559
Iteration 156/1000 | Loss: 0.00002558
Iteration 157/1000 | Loss: 0.00002558
Iteration 158/1000 | Loss: 0.00002558
Iteration 159/1000 | Loss: 0.00002558
Iteration 160/1000 | Loss: 0.00002558
Iteration 161/1000 | Loss: 0.00002558
Iteration 162/1000 | Loss: 0.00002558
Iteration 163/1000 | Loss: 0.00002558
Iteration 164/1000 | Loss: 0.00002558
Iteration 165/1000 | Loss: 0.00002558
Iteration 166/1000 | Loss: 0.00002558
Iteration 167/1000 | Loss: 0.00002558
Iteration 168/1000 | Loss: 0.00002558
Iteration 169/1000 | Loss: 0.00002558
Iteration 170/1000 | Loss: 0.00002558
Iteration 171/1000 | Loss: 0.00002557
Iteration 172/1000 | Loss: 0.00002557
Iteration 173/1000 | Loss: 0.00002557
Iteration 174/1000 | Loss: 0.00002557
Iteration 175/1000 | Loss: 0.00002557
Iteration 176/1000 | Loss: 0.00002557
Iteration 177/1000 | Loss: 0.00002557
Iteration 178/1000 | Loss: 0.00002557
Iteration 179/1000 | Loss: 0.00002557
Iteration 180/1000 | Loss: 0.00002557
Iteration 181/1000 | Loss: 0.00002557
Iteration 182/1000 | Loss: 0.00002557
Iteration 183/1000 | Loss: 0.00002557
Iteration 184/1000 | Loss: 0.00002557
Iteration 185/1000 | Loss: 0.00002557
Iteration 186/1000 | Loss: 0.00002557
Iteration 187/1000 | Loss: 0.00002557
Iteration 188/1000 | Loss: 0.00002557
Iteration 189/1000 | Loss: 0.00002557
Iteration 190/1000 | Loss: 0.00002557
Iteration 191/1000 | Loss: 0.00002557
Iteration 192/1000 | Loss: 0.00002557
Iteration 193/1000 | Loss: 0.00002557
Iteration 194/1000 | Loss: 0.00002557
Iteration 195/1000 | Loss: 0.00002557
Iteration 196/1000 | Loss: 0.00002557
Iteration 197/1000 | Loss: 0.00002557
Iteration 198/1000 | Loss: 0.00002557
Iteration 199/1000 | Loss: 0.00002557
Iteration 200/1000 | Loss: 0.00002557
Iteration 201/1000 | Loss: 0.00002557
Iteration 202/1000 | Loss: 0.00002557
Iteration 203/1000 | Loss: 0.00002557
Iteration 204/1000 | Loss: 0.00002557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.5565148462192155e-05, 2.5565148462192155e-05, 2.5565148462192155e-05, 2.5565148462192155e-05, 2.5565148462192155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5565148462192155e-05

Optimization complete. Final v2v error: 4.372156143188477 mm

Highest mean error: 4.565000057220459 mm for frame 96

Lowest mean error: 4.197875022888184 mm for frame 14

Saving results

Total time: 34.857362270355225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01239283
Iteration 2/25 | Loss: 0.00217739
Iteration 3/25 | Loss: 0.00169762
Iteration 4/25 | Loss: 0.00164977
Iteration 5/25 | Loss: 0.00164549
Iteration 6/25 | Loss: 0.00164209
Iteration 7/25 | Loss: 0.00165105
Iteration 8/25 | Loss: 0.00164948
Iteration 9/25 | Loss: 0.00164273
Iteration 10/25 | Loss: 0.00163893
Iteration 11/25 | Loss: 0.00163398
Iteration 12/25 | Loss: 0.00163228
Iteration 13/25 | Loss: 0.00162946
Iteration 14/25 | Loss: 0.00162813
Iteration 15/25 | Loss: 0.00162872
Iteration 16/25 | Loss: 0.00162868
Iteration 17/25 | Loss: 0.00163111
Iteration 18/25 | Loss: 0.00162973
Iteration 19/25 | Loss: 0.00162809
Iteration 20/25 | Loss: 0.00162775
Iteration 21/25 | Loss: 0.00163111
Iteration 22/25 | Loss: 0.00162964
Iteration 23/25 | Loss: 0.00162735
Iteration 24/25 | Loss: 0.00163178
Iteration 25/25 | Loss: 0.00162864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10637045
Iteration 2/25 | Loss: 0.00192966
Iteration 3/25 | Loss: 0.00192965
Iteration 4/25 | Loss: 0.00192965
Iteration 5/25 | Loss: 0.00192965
Iteration 6/25 | Loss: 0.00192965
Iteration 7/25 | Loss: 0.00192965
Iteration 8/25 | Loss: 0.00192965
Iteration 9/25 | Loss: 0.00192965
Iteration 10/25 | Loss: 0.00192965
Iteration 11/25 | Loss: 0.00192965
Iteration 12/25 | Loss: 0.00192965
Iteration 13/25 | Loss: 0.00192965
Iteration 14/25 | Loss: 0.00192965
Iteration 15/25 | Loss: 0.00192965
Iteration 16/25 | Loss: 0.00192965
Iteration 17/25 | Loss: 0.00192965
Iteration 18/25 | Loss: 0.00192965
Iteration 19/25 | Loss: 0.00192965
Iteration 20/25 | Loss: 0.00192965
Iteration 21/25 | Loss: 0.00192965
Iteration 22/25 | Loss: 0.00192965
Iteration 23/25 | Loss: 0.00192965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0019296512473374605, 0.0019296512473374605, 0.0019296512473374605, 0.0019296512473374605, 0.0019296512473374605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019296512473374605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192965
Iteration 2/1000 | Loss: 0.00035603
Iteration 3/1000 | Loss: 0.00038044
Iteration 4/1000 | Loss: 0.00024297
Iteration 5/1000 | Loss: 0.00018066
Iteration 6/1000 | Loss: 0.00037749
Iteration 7/1000 | Loss: 0.00035320
Iteration 8/1000 | Loss: 0.00029485
Iteration 9/1000 | Loss: 0.00032408
Iteration 10/1000 | Loss: 0.00043662
Iteration 11/1000 | Loss: 0.00039325
Iteration 12/1000 | Loss: 0.00046785
Iteration 13/1000 | Loss: 0.00021010
Iteration 14/1000 | Loss: 0.00040313
Iteration 15/1000 | Loss: 0.00027427
Iteration 16/1000 | Loss: 0.00018701
Iteration 17/1000 | Loss: 0.00031391
Iteration 18/1000 | Loss: 0.00045660
Iteration 19/1000 | Loss: 0.00063312
Iteration 20/1000 | Loss: 0.00045061
Iteration 21/1000 | Loss: 0.00044346
Iteration 22/1000 | Loss: 0.00055933
Iteration 23/1000 | Loss: 0.00047585
Iteration 24/1000 | Loss: 0.00057642
Iteration 25/1000 | Loss: 0.00030756
Iteration 26/1000 | Loss: 0.00025130
Iteration 27/1000 | Loss: 0.00025187
Iteration 28/1000 | Loss: 0.00022457
Iteration 29/1000 | Loss: 0.00033652
Iteration 30/1000 | Loss: 0.00022476
Iteration 31/1000 | Loss: 0.00023943
Iteration 32/1000 | Loss: 0.00024428
Iteration 33/1000 | Loss: 0.00017732
Iteration 34/1000 | Loss: 0.00016379
Iteration 35/1000 | Loss: 0.00020023
Iteration 36/1000 | Loss: 0.00023067
Iteration 37/1000 | Loss: 0.00049128
Iteration 38/1000 | Loss: 0.00023097
Iteration 39/1000 | Loss: 0.00013765
Iteration 40/1000 | Loss: 0.00040657
Iteration 41/1000 | Loss: 0.00013446
Iteration 42/1000 | Loss: 0.00016763
Iteration 43/1000 | Loss: 0.00015504
Iteration 44/1000 | Loss: 0.00016228
Iteration 45/1000 | Loss: 0.00020343
Iteration 46/1000 | Loss: 0.00021434
Iteration 47/1000 | Loss: 0.00020368
Iteration 48/1000 | Loss: 0.00020048
Iteration 49/1000 | Loss: 0.00041847
Iteration 50/1000 | Loss: 0.00015819
Iteration 51/1000 | Loss: 0.00008361
Iteration 52/1000 | Loss: 0.00019169
Iteration 53/1000 | Loss: 0.00021086
Iteration 54/1000 | Loss: 0.00016913
Iteration 55/1000 | Loss: 0.00018996
Iteration 56/1000 | Loss: 0.00014075
Iteration 57/1000 | Loss: 0.00012772
Iteration 58/1000 | Loss: 0.00012278
Iteration 59/1000 | Loss: 0.00016842
Iteration 60/1000 | Loss: 0.00024567
Iteration 61/1000 | Loss: 0.00025975
Iteration 62/1000 | Loss: 0.00017538
Iteration 63/1000 | Loss: 0.00017141
Iteration 64/1000 | Loss: 0.00021379
Iteration 65/1000 | Loss: 0.00019115
Iteration 66/1000 | Loss: 0.00029639
Iteration 67/1000 | Loss: 0.00013150
Iteration 68/1000 | Loss: 0.00021321
Iteration 69/1000 | Loss: 0.00010947
Iteration 70/1000 | Loss: 0.00006693
Iteration 71/1000 | Loss: 0.00006387
Iteration 72/1000 | Loss: 0.00017933
Iteration 73/1000 | Loss: 0.00010482
Iteration 74/1000 | Loss: 0.00018163
Iteration 75/1000 | Loss: 0.00021704
Iteration 76/1000 | Loss: 0.00014449
Iteration 77/1000 | Loss: 0.00033046
Iteration 78/1000 | Loss: 0.00007119
Iteration 79/1000 | Loss: 0.00031555
Iteration 80/1000 | Loss: 0.00037088
Iteration 81/1000 | Loss: 0.00066727
Iteration 82/1000 | Loss: 0.00018490
Iteration 83/1000 | Loss: 0.00029735
Iteration 84/1000 | Loss: 0.00012033
Iteration 85/1000 | Loss: 0.00021324
Iteration 86/1000 | Loss: 0.00006963
Iteration 87/1000 | Loss: 0.00006417
Iteration 88/1000 | Loss: 0.00007208
Iteration 89/1000 | Loss: 0.00006640
Iteration 90/1000 | Loss: 0.00006483
Iteration 91/1000 | Loss: 0.00006733
Iteration 92/1000 | Loss: 0.00006721
Iteration 93/1000 | Loss: 0.00011923
Iteration 94/1000 | Loss: 0.00007219
Iteration 95/1000 | Loss: 0.00007166
Iteration 96/1000 | Loss: 0.00006921
Iteration 97/1000 | Loss: 0.00008608
Iteration 98/1000 | Loss: 0.00007277
Iteration 99/1000 | Loss: 0.00009299
Iteration 100/1000 | Loss: 0.00007645
Iteration 101/1000 | Loss: 0.00007780
Iteration 102/1000 | Loss: 0.00006217
Iteration 103/1000 | Loss: 0.00008350
Iteration 104/1000 | Loss: 0.00010227
Iteration 105/1000 | Loss: 0.00007585
Iteration 106/1000 | Loss: 0.00005755
Iteration 107/1000 | Loss: 0.00005391
Iteration 108/1000 | Loss: 0.00005204
Iteration 109/1000 | Loss: 0.00005070
Iteration 110/1000 | Loss: 0.00005002
Iteration 111/1000 | Loss: 0.00004949
Iteration 112/1000 | Loss: 0.00004897
Iteration 113/1000 | Loss: 0.00004873
Iteration 114/1000 | Loss: 0.00004856
Iteration 115/1000 | Loss: 0.00004852
Iteration 116/1000 | Loss: 0.00004852
Iteration 117/1000 | Loss: 0.00004851
Iteration 118/1000 | Loss: 0.00004850
Iteration 119/1000 | Loss: 0.00004839
Iteration 120/1000 | Loss: 0.00004837
Iteration 121/1000 | Loss: 0.00004824
Iteration 122/1000 | Loss: 0.00004822
Iteration 123/1000 | Loss: 0.00004811
Iteration 124/1000 | Loss: 0.00004810
Iteration 125/1000 | Loss: 0.00004810
Iteration 126/1000 | Loss: 0.00004810
Iteration 127/1000 | Loss: 0.00004810
Iteration 128/1000 | Loss: 0.00004810
Iteration 129/1000 | Loss: 0.00004810
Iteration 130/1000 | Loss: 0.00004809
Iteration 131/1000 | Loss: 0.00004809
Iteration 132/1000 | Loss: 0.00004809
Iteration 133/1000 | Loss: 0.00004809
Iteration 134/1000 | Loss: 0.00004808
Iteration 135/1000 | Loss: 0.00004808
Iteration 136/1000 | Loss: 0.00004808
Iteration 137/1000 | Loss: 0.00004808
Iteration 138/1000 | Loss: 0.00004808
Iteration 139/1000 | Loss: 0.00004808
Iteration 140/1000 | Loss: 0.00004808
Iteration 141/1000 | Loss: 0.00004808
Iteration 142/1000 | Loss: 0.00004807
Iteration 143/1000 | Loss: 0.00004807
Iteration 144/1000 | Loss: 0.00004807
Iteration 145/1000 | Loss: 0.00004807
Iteration 146/1000 | Loss: 0.00004807
Iteration 147/1000 | Loss: 0.00004807
Iteration 148/1000 | Loss: 0.00004807
Iteration 149/1000 | Loss: 0.00004807
Iteration 150/1000 | Loss: 0.00004806
Iteration 151/1000 | Loss: 0.00004806
Iteration 152/1000 | Loss: 0.00004806
Iteration 153/1000 | Loss: 0.00004806
Iteration 154/1000 | Loss: 0.00004806
Iteration 155/1000 | Loss: 0.00004806
Iteration 156/1000 | Loss: 0.00004806
Iteration 157/1000 | Loss: 0.00004806
Iteration 158/1000 | Loss: 0.00004805
Iteration 159/1000 | Loss: 0.00004805
Iteration 160/1000 | Loss: 0.00004805
Iteration 161/1000 | Loss: 0.00004805
Iteration 162/1000 | Loss: 0.00004805
Iteration 163/1000 | Loss: 0.00004805
Iteration 164/1000 | Loss: 0.00004805
Iteration 165/1000 | Loss: 0.00004805
Iteration 166/1000 | Loss: 0.00004805
Iteration 167/1000 | Loss: 0.00004805
Iteration 168/1000 | Loss: 0.00004805
Iteration 169/1000 | Loss: 0.00004805
Iteration 170/1000 | Loss: 0.00004805
Iteration 171/1000 | Loss: 0.00004805
Iteration 172/1000 | Loss: 0.00004805
Iteration 173/1000 | Loss: 0.00004805
Iteration 174/1000 | Loss: 0.00004805
Iteration 175/1000 | Loss: 0.00004805
Iteration 176/1000 | Loss: 0.00004804
Iteration 177/1000 | Loss: 0.00004804
Iteration 178/1000 | Loss: 0.00004804
Iteration 179/1000 | Loss: 0.00004804
Iteration 180/1000 | Loss: 0.00004804
Iteration 181/1000 | Loss: 0.00004804
Iteration 182/1000 | Loss: 0.00004804
Iteration 183/1000 | Loss: 0.00004804
Iteration 184/1000 | Loss: 0.00004804
Iteration 185/1000 | Loss: 0.00004804
Iteration 186/1000 | Loss: 0.00004804
Iteration 187/1000 | Loss: 0.00004804
Iteration 188/1000 | Loss: 0.00004803
Iteration 189/1000 | Loss: 0.00004803
Iteration 190/1000 | Loss: 0.00004803
Iteration 191/1000 | Loss: 0.00004803
Iteration 192/1000 | Loss: 0.00004803
Iteration 193/1000 | Loss: 0.00004802
Iteration 194/1000 | Loss: 0.00004802
Iteration 195/1000 | Loss: 0.00004802
Iteration 196/1000 | Loss: 0.00004802
Iteration 197/1000 | Loss: 0.00004802
Iteration 198/1000 | Loss: 0.00004802
Iteration 199/1000 | Loss: 0.00004802
Iteration 200/1000 | Loss: 0.00004802
Iteration 201/1000 | Loss: 0.00004802
Iteration 202/1000 | Loss: 0.00004802
Iteration 203/1000 | Loss: 0.00004802
Iteration 204/1000 | Loss: 0.00004802
Iteration 205/1000 | Loss: 0.00004801
Iteration 206/1000 | Loss: 0.00004801
Iteration 207/1000 | Loss: 0.00004801
Iteration 208/1000 | Loss: 0.00004801
Iteration 209/1000 | Loss: 0.00004801
Iteration 210/1000 | Loss: 0.00004801
Iteration 211/1000 | Loss: 0.00004801
Iteration 212/1000 | Loss: 0.00004801
Iteration 213/1000 | Loss: 0.00004801
Iteration 214/1000 | Loss: 0.00004801
Iteration 215/1000 | Loss: 0.00004801
Iteration 216/1000 | Loss: 0.00004801
Iteration 217/1000 | Loss: 0.00004801
Iteration 218/1000 | Loss: 0.00004801
Iteration 219/1000 | Loss: 0.00004801
Iteration 220/1000 | Loss: 0.00004801
Iteration 221/1000 | Loss: 0.00004801
Iteration 222/1000 | Loss: 0.00004801
Iteration 223/1000 | Loss: 0.00004801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [4.800601891474798e-05, 4.800601891474798e-05, 4.800601891474798e-05, 4.800601891474798e-05, 4.800601891474798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.800601891474798e-05

Optimization complete. Final v2v error: 5.796255111694336 mm

Highest mean error: 6.512442111968994 mm for frame 81

Lowest mean error: 5.39446496963501 mm for frame 201

Saving results

Total time: 228.21204376220703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00630310
Iteration 2/25 | Loss: 0.00170551
Iteration 3/25 | Loss: 0.00151616
Iteration 4/25 | Loss: 0.00145476
Iteration 5/25 | Loss: 0.00144773
Iteration 6/25 | Loss: 0.00143340
Iteration 7/25 | Loss: 0.00142965
Iteration 8/25 | Loss: 0.00142708
Iteration 9/25 | Loss: 0.00142419
Iteration 10/25 | Loss: 0.00142331
Iteration 11/25 | Loss: 0.00142444
Iteration 12/25 | Loss: 0.00142252
Iteration 13/25 | Loss: 0.00142233
Iteration 14/25 | Loss: 0.00142222
Iteration 15/25 | Loss: 0.00142210
Iteration 16/25 | Loss: 0.00142202
Iteration 17/25 | Loss: 0.00142433
Iteration 18/25 | Loss: 0.00142186
Iteration 19/25 | Loss: 0.00142175
Iteration 20/25 | Loss: 0.00142175
Iteration 21/25 | Loss: 0.00142174
Iteration 22/25 | Loss: 0.00142174
Iteration 23/25 | Loss: 0.00142174
Iteration 24/25 | Loss: 0.00142174
Iteration 25/25 | Loss: 0.00142174

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77204883
Iteration 2/25 | Loss: 0.00174036
Iteration 3/25 | Loss: 0.00169359
Iteration 4/25 | Loss: 0.00169359
Iteration 5/25 | Loss: 0.00169359
Iteration 6/25 | Loss: 0.00169359
Iteration 7/25 | Loss: 0.00169359
Iteration 8/25 | Loss: 0.00169359
Iteration 9/25 | Loss: 0.00169359
Iteration 10/25 | Loss: 0.00169359
Iteration 11/25 | Loss: 0.00169359
Iteration 12/25 | Loss: 0.00169359
Iteration 13/25 | Loss: 0.00169359
Iteration 14/25 | Loss: 0.00169359
Iteration 15/25 | Loss: 0.00169359
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016935855383053422, 0.0016935855383053422, 0.0016935855383053422, 0.0016935855383053422, 0.0016935855383053422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016935855383053422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169359
Iteration 2/1000 | Loss: 0.00005968
Iteration 3/1000 | Loss: 0.00004383
Iteration 4/1000 | Loss: 0.00003589
Iteration 5/1000 | Loss: 0.00003237
Iteration 6/1000 | Loss: 0.00003086
Iteration 7/1000 | Loss: 0.00003025
Iteration 8/1000 | Loss: 0.00002984
Iteration 9/1000 | Loss: 0.00002935
Iteration 10/1000 | Loss: 0.00002898
Iteration 11/1000 | Loss: 0.00003059
Iteration 12/1000 | Loss: 0.00002916
Iteration 13/1000 | Loss: 0.00002876
Iteration 14/1000 | Loss: 0.00002871
Iteration 15/1000 | Loss: 0.00002871
Iteration 16/1000 | Loss: 0.00002871
Iteration 17/1000 | Loss: 0.00002870
Iteration 18/1000 | Loss: 0.00002870
Iteration 19/1000 | Loss: 0.00002870
Iteration 20/1000 | Loss: 0.00002870
Iteration 21/1000 | Loss: 0.00002870
Iteration 22/1000 | Loss: 0.00002869
Iteration 23/1000 | Loss: 0.00002869
Iteration 24/1000 | Loss: 0.00002869
Iteration 25/1000 | Loss: 0.00002869
Iteration 26/1000 | Loss: 0.00002869
Iteration 27/1000 | Loss: 0.00002869
Iteration 28/1000 | Loss: 0.00002869
Iteration 29/1000 | Loss: 0.00002869
Iteration 30/1000 | Loss: 0.00002869
Iteration 31/1000 | Loss: 0.00002979
Iteration 32/1000 | Loss: 0.00002895
Iteration 33/1000 | Loss: 0.00002936
Iteration 34/1000 | Loss: 0.00002915
Iteration 35/1000 | Loss: 0.00002915
Iteration 36/1000 | Loss: 0.00002941
Iteration 37/1000 | Loss: 0.00002894
Iteration 38/1000 | Loss: 0.00002905
Iteration 39/1000 | Loss: 0.00002913
Iteration 40/1000 | Loss: 0.00002946
Iteration 41/1000 | Loss: 0.00002927
Iteration 42/1000 | Loss: 0.00002940
Iteration 43/1000 | Loss: 0.00002921
Iteration 44/1000 | Loss: 0.00002908
Iteration 45/1000 | Loss: 0.00002918
Iteration 46/1000 | Loss: 0.00002936
Iteration 47/1000 | Loss: 0.00002935
Iteration 48/1000 | Loss: 0.00002865
Iteration 49/1000 | Loss: 0.00005466
Iteration 50/1000 | Loss: 0.00002945
Iteration 51/1000 | Loss: 0.00002937
Iteration 52/1000 | Loss: 0.00002927
Iteration 53/1000 | Loss: 0.00002850
Iteration 54/1000 | Loss: 0.00002850
Iteration 55/1000 | Loss: 0.00002850
Iteration 56/1000 | Loss: 0.00002849
Iteration 57/1000 | Loss: 0.00002849
Iteration 58/1000 | Loss: 0.00002849
Iteration 59/1000 | Loss: 0.00002849
Iteration 60/1000 | Loss: 0.00002849
Iteration 61/1000 | Loss: 0.00002849
Iteration 62/1000 | Loss: 0.00002849
Iteration 63/1000 | Loss: 0.00002848
Iteration 64/1000 | Loss: 0.00002848
Iteration 65/1000 | Loss: 0.00002848
Iteration 66/1000 | Loss: 0.00002848
Iteration 67/1000 | Loss: 0.00002848
Iteration 68/1000 | Loss: 0.00002848
Iteration 69/1000 | Loss: 0.00002848
Iteration 70/1000 | Loss: 0.00002848
Iteration 71/1000 | Loss: 0.00002848
Iteration 72/1000 | Loss: 0.00002848
Iteration 73/1000 | Loss: 0.00002848
Iteration 74/1000 | Loss: 0.00002848
Iteration 75/1000 | Loss: 0.00002848
Iteration 76/1000 | Loss: 0.00002848
Iteration 77/1000 | Loss: 0.00002848
Iteration 78/1000 | Loss: 0.00002848
Iteration 79/1000 | Loss: 0.00002848
Iteration 80/1000 | Loss: 0.00002848
Iteration 81/1000 | Loss: 0.00002848
Iteration 82/1000 | Loss: 0.00002848
Iteration 83/1000 | Loss: 0.00002848
Iteration 84/1000 | Loss: 0.00002848
Iteration 85/1000 | Loss: 0.00002848
Iteration 86/1000 | Loss: 0.00002848
Iteration 87/1000 | Loss: 0.00002848
Iteration 88/1000 | Loss: 0.00002848
Iteration 89/1000 | Loss: 0.00002848
Iteration 90/1000 | Loss: 0.00002848
Iteration 91/1000 | Loss: 0.00002848
Iteration 92/1000 | Loss: 0.00002848
Iteration 93/1000 | Loss: 0.00002848
Iteration 94/1000 | Loss: 0.00002848
Iteration 95/1000 | Loss: 0.00002848
Iteration 96/1000 | Loss: 0.00002848
Iteration 97/1000 | Loss: 0.00002848
Iteration 98/1000 | Loss: 0.00002848
Iteration 99/1000 | Loss: 0.00002848
Iteration 100/1000 | Loss: 0.00002848
Iteration 101/1000 | Loss: 0.00002848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.848425901902374e-05, 2.848425901902374e-05, 2.848425901902374e-05, 2.848425901902374e-05, 2.848425901902374e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.848425901902374e-05

Optimization complete. Final v2v error: 4.514797687530518 mm

Highest mean error: 10.772525787353516 mm for frame 238

Lowest mean error: 4.014684200286865 mm for frame 63

Saving results

Total time: 87.9552571773529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986109
Iteration 2/25 | Loss: 0.00182450
Iteration 3/25 | Loss: 0.00153656
Iteration 4/25 | Loss: 0.00150499
Iteration 5/25 | Loss: 0.00149800
Iteration 6/25 | Loss: 0.00149670
Iteration 7/25 | Loss: 0.00149670
Iteration 8/25 | Loss: 0.00149670
Iteration 9/25 | Loss: 0.00149670
Iteration 10/25 | Loss: 0.00149670
Iteration 11/25 | Loss: 0.00149670
Iteration 12/25 | Loss: 0.00149670
Iteration 13/25 | Loss: 0.00149670
Iteration 14/25 | Loss: 0.00149670
Iteration 15/25 | Loss: 0.00149670
Iteration 16/25 | Loss: 0.00149670
Iteration 17/25 | Loss: 0.00149670
Iteration 18/25 | Loss: 0.00149670
Iteration 19/25 | Loss: 0.00149670
Iteration 20/25 | Loss: 0.00149670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014966956805437803, 0.0014966956805437803, 0.0014966956805437803, 0.0014966956805437803, 0.0014966956805437803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014966956805437803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.43807268
Iteration 2/25 | Loss: 0.00200647
Iteration 3/25 | Loss: 0.00200645
Iteration 4/25 | Loss: 0.00200645
Iteration 5/25 | Loss: 0.00200645
Iteration 6/25 | Loss: 0.00200645
Iteration 7/25 | Loss: 0.00200645
Iteration 8/25 | Loss: 0.00200645
Iteration 9/25 | Loss: 0.00200645
Iteration 10/25 | Loss: 0.00200645
Iteration 11/25 | Loss: 0.00200645
Iteration 12/25 | Loss: 0.00200645
Iteration 13/25 | Loss: 0.00200645
Iteration 14/25 | Loss: 0.00200645
Iteration 15/25 | Loss: 0.00200645
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0020064502023160458, 0.0020064502023160458, 0.0020064502023160458, 0.0020064502023160458, 0.0020064502023160458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020064502023160458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200645
Iteration 2/1000 | Loss: 0.00006527
Iteration 3/1000 | Loss: 0.00004684
Iteration 4/1000 | Loss: 0.00003828
Iteration 5/1000 | Loss: 0.00003307
Iteration 6/1000 | Loss: 0.00003026
Iteration 7/1000 | Loss: 0.00002895
Iteration 8/1000 | Loss: 0.00002828
Iteration 9/1000 | Loss: 0.00002763
Iteration 10/1000 | Loss: 0.00002722
Iteration 11/1000 | Loss: 0.00002691
Iteration 12/1000 | Loss: 0.00002670
Iteration 13/1000 | Loss: 0.00002664
Iteration 14/1000 | Loss: 0.00002659
Iteration 15/1000 | Loss: 0.00002652
Iteration 16/1000 | Loss: 0.00002652
Iteration 17/1000 | Loss: 0.00002650
Iteration 18/1000 | Loss: 0.00002650
Iteration 19/1000 | Loss: 0.00002649
Iteration 20/1000 | Loss: 0.00002649
Iteration 21/1000 | Loss: 0.00002649
Iteration 22/1000 | Loss: 0.00002649
Iteration 23/1000 | Loss: 0.00002649
Iteration 24/1000 | Loss: 0.00002648
Iteration 25/1000 | Loss: 0.00002648
Iteration 26/1000 | Loss: 0.00002647
Iteration 27/1000 | Loss: 0.00002647
Iteration 28/1000 | Loss: 0.00002647
Iteration 29/1000 | Loss: 0.00002647
Iteration 30/1000 | Loss: 0.00002646
Iteration 31/1000 | Loss: 0.00002646
Iteration 32/1000 | Loss: 0.00002646
Iteration 33/1000 | Loss: 0.00002645
Iteration 34/1000 | Loss: 0.00002643
Iteration 35/1000 | Loss: 0.00002642
Iteration 36/1000 | Loss: 0.00002642
Iteration 37/1000 | Loss: 0.00002642
Iteration 38/1000 | Loss: 0.00002641
Iteration 39/1000 | Loss: 0.00002641
Iteration 40/1000 | Loss: 0.00002640
Iteration 41/1000 | Loss: 0.00002640
Iteration 42/1000 | Loss: 0.00002640
Iteration 43/1000 | Loss: 0.00002640
Iteration 44/1000 | Loss: 0.00002640
Iteration 45/1000 | Loss: 0.00002640
Iteration 46/1000 | Loss: 0.00002639
Iteration 47/1000 | Loss: 0.00002639
Iteration 48/1000 | Loss: 0.00002639
Iteration 49/1000 | Loss: 0.00002639
Iteration 50/1000 | Loss: 0.00002639
Iteration 51/1000 | Loss: 0.00002639
Iteration 52/1000 | Loss: 0.00002639
Iteration 53/1000 | Loss: 0.00002639
Iteration 54/1000 | Loss: 0.00002639
Iteration 55/1000 | Loss: 0.00002639
Iteration 56/1000 | Loss: 0.00002639
Iteration 57/1000 | Loss: 0.00002639
Iteration 58/1000 | Loss: 0.00002639
Iteration 59/1000 | Loss: 0.00002639
Iteration 60/1000 | Loss: 0.00002639
Iteration 61/1000 | Loss: 0.00002639
Iteration 62/1000 | Loss: 0.00002639
Iteration 63/1000 | Loss: 0.00002639
Iteration 64/1000 | Loss: 0.00002639
Iteration 65/1000 | Loss: 0.00002639
Iteration 66/1000 | Loss: 0.00002639
Iteration 67/1000 | Loss: 0.00002639
Iteration 68/1000 | Loss: 0.00002639
Iteration 69/1000 | Loss: 0.00002639
Iteration 70/1000 | Loss: 0.00002639
Iteration 71/1000 | Loss: 0.00002639
Iteration 72/1000 | Loss: 0.00002639
Iteration 73/1000 | Loss: 0.00002639
Iteration 74/1000 | Loss: 0.00002639
Iteration 75/1000 | Loss: 0.00002639
Iteration 76/1000 | Loss: 0.00002639
Iteration 77/1000 | Loss: 0.00002639
Iteration 78/1000 | Loss: 0.00002639
Iteration 79/1000 | Loss: 0.00002639
Iteration 80/1000 | Loss: 0.00002639
Iteration 81/1000 | Loss: 0.00002639
Iteration 82/1000 | Loss: 0.00002639
Iteration 83/1000 | Loss: 0.00002639
Iteration 84/1000 | Loss: 0.00002639
Iteration 85/1000 | Loss: 0.00002639
Iteration 86/1000 | Loss: 0.00002639
Iteration 87/1000 | Loss: 0.00002639
Iteration 88/1000 | Loss: 0.00002639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [2.638757723616436e-05, 2.638757723616436e-05, 2.638757723616436e-05, 2.638757723616436e-05, 2.638757723616436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.638757723616436e-05

Optimization complete. Final v2v error: 4.4658427238464355 mm

Highest mean error: 4.768063545227051 mm for frame 30

Lowest mean error: 4.185051918029785 mm for frame 203

Saving results

Total time: 36.64061880111694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500208
Iteration 2/25 | Loss: 0.00157539
Iteration 3/25 | Loss: 0.00145752
Iteration 4/25 | Loss: 0.00145059
Iteration 5/25 | Loss: 0.00145001
Iteration 6/25 | Loss: 0.00145001
Iteration 7/25 | Loss: 0.00145001
Iteration 8/25 | Loss: 0.00145001
Iteration 9/25 | Loss: 0.00145001
Iteration 10/25 | Loss: 0.00145001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001450014184229076, 0.001450014184229076, 0.001450014184229076, 0.001450014184229076, 0.001450014184229076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001450014184229076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56291640
Iteration 2/25 | Loss: 0.00208643
Iteration 3/25 | Loss: 0.00208643
Iteration 4/25 | Loss: 0.00208643
Iteration 5/25 | Loss: 0.00208643
Iteration 6/25 | Loss: 0.00208642
Iteration 7/25 | Loss: 0.00208642
Iteration 8/25 | Loss: 0.00208642
Iteration 9/25 | Loss: 0.00208642
Iteration 10/25 | Loss: 0.00208642
Iteration 11/25 | Loss: 0.00208642
Iteration 12/25 | Loss: 0.00208642
Iteration 13/25 | Loss: 0.00208642
Iteration 14/25 | Loss: 0.00208642
Iteration 15/25 | Loss: 0.00208642
Iteration 16/25 | Loss: 0.00208642
Iteration 17/25 | Loss: 0.00208642
Iteration 18/25 | Loss: 0.00208642
Iteration 19/25 | Loss: 0.00208642
Iteration 20/25 | Loss: 0.00208642
Iteration 21/25 | Loss: 0.00208642
Iteration 22/25 | Loss: 0.00208642
Iteration 23/25 | Loss: 0.00208642
Iteration 24/25 | Loss: 0.00208642
Iteration 25/25 | Loss: 0.00208642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208642
Iteration 2/1000 | Loss: 0.00006172
Iteration 3/1000 | Loss: 0.00004344
Iteration 4/1000 | Loss: 0.00003642
Iteration 5/1000 | Loss: 0.00003363
Iteration 6/1000 | Loss: 0.00003230
Iteration 7/1000 | Loss: 0.00003166
Iteration 8/1000 | Loss: 0.00003131
Iteration 9/1000 | Loss: 0.00003107
Iteration 10/1000 | Loss: 0.00003096
Iteration 11/1000 | Loss: 0.00003084
Iteration 12/1000 | Loss: 0.00003084
Iteration 13/1000 | Loss: 0.00003077
Iteration 14/1000 | Loss: 0.00003075
Iteration 15/1000 | Loss: 0.00003075
Iteration 16/1000 | Loss: 0.00003074
Iteration 17/1000 | Loss: 0.00003074
Iteration 18/1000 | Loss: 0.00003074
Iteration 19/1000 | Loss: 0.00003072
Iteration 20/1000 | Loss: 0.00003072
Iteration 21/1000 | Loss: 0.00003069
Iteration 22/1000 | Loss: 0.00003067
Iteration 23/1000 | Loss: 0.00003067
Iteration 24/1000 | Loss: 0.00003066
Iteration 25/1000 | Loss: 0.00003065
Iteration 26/1000 | Loss: 0.00003065
Iteration 27/1000 | Loss: 0.00003062
Iteration 28/1000 | Loss: 0.00003061
Iteration 29/1000 | Loss: 0.00003061
Iteration 30/1000 | Loss: 0.00003059
Iteration 31/1000 | Loss: 0.00003059
Iteration 32/1000 | Loss: 0.00003059
Iteration 33/1000 | Loss: 0.00003058
Iteration 34/1000 | Loss: 0.00003058
Iteration 35/1000 | Loss: 0.00003058
Iteration 36/1000 | Loss: 0.00003058
Iteration 37/1000 | Loss: 0.00003058
Iteration 38/1000 | Loss: 0.00003058
Iteration 39/1000 | Loss: 0.00003058
Iteration 40/1000 | Loss: 0.00003057
Iteration 41/1000 | Loss: 0.00003057
Iteration 42/1000 | Loss: 0.00003057
Iteration 43/1000 | Loss: 0.00003057
Iteration 44/1000 | Loss: 0.00003056
Iteration 45/1000 | Loss: 0.00003056
Iteration 46/1000 | Loss: 0.00003056
Iteration 47/1000 | Loss: 0.00003056
Iteration 48/1000 | Loss: 0.00003055
Iteration 49/1000 | Loss: 0.00003055
Iteration 50/1000 | Loss: 0.00003055
Iteration 51/1000 | Loss: 0.00003055
Iteration 52/1000 | Loss: 0.00003053
Iteration 53/1000 | Loss: 0.00003053
Iteration 54/1000 | Loss: 0.00003053
Iteration 55/1000 | Loss: 0.00003052
Iteration 56/1000 | Loss: 0.00003051
Iteration 57/1000 | Loss: 0.00003050
Iteration 58/1000 | Loss: 0.00003050
Iteration 59/1000 | Loss: 0.00003050
Iteration 60/1000 | Loss: 0.00003049
Iteration 61/1000 | Loss: 0.00003049
Iteration 62/1000 | Loss: 0.00003049
Iteration 63/1000 | Loss: 0.00003048
Iteration 64/1000 | Loss: 0.00003048
Iteration 65/1000 | Loss: 0.00003048
Iteration 66/1000 | Loss: 0.00003048
Iteration 67/1000 | Loss: 0.00003048
Iteration 68/1000 | Loss: 0.00003048
Iteration 69/1000 | Loss: 0.00003048
Iteration 70/1000 | Loss: 0.00003047
Iteration 71/1000 | Loss: 0.00003047
Iteration 72/1000 | Loss: 0.00003047
Iteration 73/1000 | Loss: 0.00003047
Iteration 74/1000 | Loss: 0.00003047
Iteration 75/1000 | Loss: 0.00003047
Iteration 76/1000 | Loss: 0.00003047
Iteration 77/1000 | Loss: 0.00003047
Iteration 78/1000 | Loss: 0.00003047
Iteration 79/1000 | Loss: 0.00003047
Iteration 80/1000 | Loss: 0.00003047
Iteration 81/1000 | Loss: 0.00003047
Iteration 82/1000 | Loss: 0.00003047
Iteration 83/1000 | Loss: 0.00003047
Iteration 84/1000 | Loss: 0.00003047
Iteration 85/1000 | Loss: 0.00003047
Iteration 86/1000 | Loss: 0.00003047
Iteration 87/1000 | Loss: 0.00003047
Iteration 88/1000 | Loss: 0.00003047
Iteration 89/1000 | Loss: 0.00003047
Iteration 90/1000 | Loss: 0.00003047
Iteration 91/1000 | Loss: 0.00003047
Iteration 92/1000 | Loss: 0.00003047
Iteration 93/1000 | Loss: 0.00003047
Iteration 94/1000 | Loss: 0.00003047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [3.0471745048998855e-05, 3.0471745048998855e-05, 3.0471745048998855e-05, 3.0471745048998855e-05, 3.0471745048998855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0471745048998855e-05

Optimization complete. Final v2v error: 4.81254768371582 mm

Highest mean error: 5.333166599273682 mm for frame 52

Lowest mean error: 4.5296244621276855 mm for frame 150

Saving results

Total time: 32.27537202835083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01249220
Iteration 2/25 | Loss: 0.00188367
Iteration 3/25 | Loss: 0.00164612
Iteration 4/25 | Loss: 0.00161263
Iteration 5/25 | Loss: 0.00160626
Iteration 6/25 | Loss: 0.00160813
Iteration 7/25 | Loss: 0.00159591
Iteration 8/25 | Loss: 0.00159561
Iteration 9/25 | Loss: 0.00159559
Iteration 10/25 | Loss: 0.00159559
Iteration 11/25 | Loss: 0.00159559
Iteration 12/25 | Loss: 0.00159558
Iteration 13/25 | Loss: 0.00159558
Iteration 14/25 | Loss: 0.00159558
Iteration 15/25 | Loss: 0.00159558
Iteration 16/25 | Loss: 0.00159558
Iteration 17/25 | Loss: 0.00159558
Iteration 18/25 | Loss: 0.00159558
Iteration 19/25 | Loss: 0.00159558
Iteration 20/25 | Loss: 0.00159558
Iteration 21/25 | Loss: 0.00159558
Iteration 22/25 | Loss: 0.00159558
Iteration 23/25 | Loss: 0.00159558
Iteration 24/25 | Loss: 0.00159558
Iteration 25/25 | Loss: 0.00159558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91194892
Iteration 2/25 | Loss: 0.00202757
Iteration 3/25 | Loss: 0.00199059
Iteration 4/25 | Loss: 0.00199059
Iteration 5/25 | Loss: 0.00199059
Iteration 6/25 | Loss: 0.00199059
Iteration 7/25 | Loss: 0.00199059
Iteration 8/25 | Loss: 0.00199059
Iteration 9/25 | Loss: 0.00199059
Iteration 10/25 | Loss: 0.00199059
Iteration 11/25 | Loss: 0.00199059
Iteration 12/25 | Loss: 0.00199059
Iteration 13/25 | Loss: 0.00199059
Iteration 14/25 | Loss: 0.00199059
Iteration 15/25 | Loss: 0.00199059
Iteration 16/25 | Loss: 0.00199059
Iteration 17/25 | Loss: 0.00199059
Iteration 18/25 | Loss: 0.00199059
Iteration 19/25 | Loss: 0.00199059
Iteration 20/25 | Loss: 0.00199059
Iteration 21/25 | Loss: 0.00199059
Iteration 22/25 | Loss: 0.00199059
Iteration 23/25 | Loss: 0.00199059
Iteration 24/25 | Loss: 0.00199059
Iteration 25/25 | Loss: 0.00199059

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199059
Iteration 2/1000 | Loss: 0.00011265
Iteration 3/1000 | Loss: 0.00005255
Iteration 4/1000 | Loss: 0.00004555
Iteration 5/1000 | Loss: 0.00004287
Iteration 6/1000 | Loss: 0.00004148
Iteration 7/1000 | Loss: 0.00004082
Iteration 8/1000 | Loss: 0.00004044
Iteration 9/1000 | Loss: 0.00004012
Iteration 10/1000 | Loss: 0.00003988
Iteration 11/1000 | Loss: 0.00003970
Iteration 12/1000 | Loss: 0.00003963
Iteration 13/1000 | Loss: 0.00003960
Iteration 14/1000 | Loss: 0.00003959
Iteration 15/1000 | Loss: 0.00003959
Iteration 16/1000 | Loss: 0.00003959
Iteration 17/1000 | Loss: 0.00003958
Iteration 18/1000 | Loss: 0.00003956
Iteration 19/1000 | Loss: 0.00003956
Iteration 20/1000 | Loss: 0.00003956
Iteration 21/1000 | Loss: 0.00003956
Iteration 22/1000 | Loss: 0.00003956
Iteration 23/1000 | Loss: 0.00003956
Iteration 24/1000 | Loss: 0.00003956
Iteration 25/1000 | Loss: 0.00003956
Iteration 26/1000 | Loss: 0.00003956
Iteration 27/1000 | Loss: 0.00003956
Iteration 28/1000 | Loss: 0.00003955
Iteration 29/1000 | Loss: 0.00003953
Iteration 30/1000 | Loss: 0.00003953
Iteration 31/1000 | Loss: 0.00003953
Iteration 32/1000 | Loss: 0.00003952
Iteration 33/1000 | Loss: 0.00003952
Iteration 34/1000 | Loss: 0.00003952
Iteration 35/1000 | Loss: 0.00003952
Iteration 36/1000 | Loss: 0.00003952
Iteration 37/1000 | Loss: 0.00003952
Iteration 38/1000 | Loss: 0.00003952
Iteration 39/1000 | Loss: 0.00003952
Iteration 40/1000 | Loss: 0.00003952
Iteration 41/1000 | Loss: 0.00003952
Iteration 42/1000 | Loss: 0.00003951
Iteration 43/1000 | Loss: 0.00003950
Iteration 44/1000 | Loss: 0.00003950
Iteration 45/1000 | Loss: 0.00003950
Iteration 46/1000 | Loss: 0.00003950
Iteration 47/1000 | Loss: 0.00003950
Iteration 48/1000 | Loss: 0.00003950
Iteration 49/1000 | Loss: 0.00003950
Iteration 50/1000 | Loss: 0.00003950
Iteration 51/1000 | Loss: 0.00003950
Iteration 52/1000 | Loss: 0.00003950
Iteration 53/1000 | Loss: 0.00003949
Iteration 54/1000 | Loss: 0.00003949
Iteration 55/1000 | Loss: 0.00003949
Iteration 56/1000 | Loss: 0.00003949
Iteration 57/1000 | Loss: 0.00003949
Iteration 58/1000 | Loss: 0.00003949
Iteration 59/1000 | Loss: 0.00003949
Iteration 60/1000 | Loss: 0.00003949
Iteration 61/1000 | Loss: 0.00003949
Iteration 62/1000 | Loss: 0.00003949
Iteration 63/1000 | Loss: 0.00003949
Iteration 64/1000 | Loss: 0.00003949
Iteration 65/1000 | Loss: 0.00003949
Iteration 66/1000 | Loss: 0.00003949
Iteration 67/1000 | Loss: 0.00003949
Iteration 68/1000 | Loss: 0.00003949
Iteration 69/1000 | Loss: 0.00003949
Iteration 70/1000 | Loss: 0.00003949
Iteration 71/1000 | Loss: 0.00003949
Iteration 72/1000 | Loss: 0.00003949
Iteration 73/1000 | Loss: 0.00003948
Iteration 74/1000 | Loss: 0.00003948
Iteration 75/1000 | Loss: 0.00003948
Iteration 76/1000 | Loss: 0.00003948
Iteration 77/1000 | Loss: 0.00003948
Iteration 78/1000 | Loss: 0.00003948
Iteration 79/1000 | Loss: 0.00003948
Iteration 80/1000 | Loss: 0.00003948
Iteration 81/1000 | Loss: 0.00003948
Iteration 82/1000 | Loss: 0.00003948
Iteration 83/1000 | Loss: 0.00003948
Iteration 84/1000 | Loss: 0.00003948
Iteration 85/1000 | Loss: 0.00003948
Iteration 86/1000 | Loss: 0.00003948
Iteration 87/1000 | Loss: 0.00003948
Iteration 88/1000 | Loss: 0.00003948
Iteration 89/1000 | Loss: 0.00003948
Iteration 90/1000 | Loss: 0.00003948
Iteration 91/1000 | Loss: 0.00003948
Iteration 92/1000 | Loss: 0.00003948
Iteration 93/1000 | Loss: 0.00003948
Iteration 94/1000 | Loss: 0.00003948
Iteration 95/1000 | Loss: 0.00003948
Iteration 96/1000 | Loss: 0.00003948
Iteration 97/1000 | Loss: 0.00003948
Iteration 98/1000 | Loss: 0.00003948
Iteration 99/1000 | Loss: 0.00003948
Iteration 100/1000 | Loss: 0.00003948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [3.948322409996763e-05, 3.948322409996763e-05, 3.948322409996763e-05, 3.948322409996763e-05, 3.948322409996763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.948322409996763e-05

Optimization complete. Final v2v error: 5.4411797523498535 mm

Highest mean error: 5.895879745483398 mm for frame 103

Lowest mean error: 4.788609504699707 mm for frame 1

Saving results

Total time: 35.453593730926514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01184750
Iteration 2/25 | Loss: 0.01184750
Iteration 3/25 | Loss: 0.01184750
Iteration 4/25 | Loss: 0.00513680
Iteration 5/25 | Loss: 0.00366966
Iteration 6/25 | Loss: 0.00299710
Iteration 7/25 | Loss: 0.00227263
Iteration 8/25 | Loss: 0.00213290
Iteration 9/25 | Loss: 0.00166613
Iteration 10/25 | Loss: 0.00150179
Iteration 11/25 | Loss: 0.00144792
Iteration 12/25 | Loss: 0.00137324
Iteration 13/25 | Loss: 0.00134777
Iteration 14/25 | Loss: 0.00133672
Iteration 15/25 | Loss: 0.00133120
Iteration 16/25 | Loss: 0.00132914
Iteration 17/25 | Loss: 0.00132763
Iteration 18/25 | Loss: 0.00132707
Iteration 19/25 | Loss: 0.00132681
Iteration 20/25 | Loss: 0.00132664
Iteration 21/25 | Loss: 0.00132566
Iteration 22/25 | Loss: 0.00132532
Iteration 23/25 | Loss: 0.00132524
Iteration 24/25 | Loss: 0.00132523
Iteration 25/25 | Loss: 0.00132522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.59321588
Iteration 2/25 | Loss: 0.00132077
Iteration 3/25 | Loss: 0.00132077
Iteration 4/25 | Loss: 0.00132077
Iteration 5/25 | Loss: 0.00132077
Iteration 6/25 | Loss: 0.00132077
Iteration 7/25 | Loss: 0.00132077
Iteration 8/25 | Loss: 0.00132077
Iteration 9/25 | Loss: 0.00132077
Iteration 10/25 | Loss: 0.00132077
Iteration 11/25 | Loss: 0.00132077
Iteration 12/25 | Loss: 0.00132077
Iteration 13/25 | Loss: 0.00132077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013207689626142383, 0.0013207689626142383, 0.0013207689626142383, 0.0013207689626142383, 0.0013207689626142383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013207689626142383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132077
Iteration 2/1000 | Loss: 0.00007785
Iteration 3/1000 | Loss: 0.00005862
Iteration 4/1000 | Loss: 0.00006168
Iteration 5/1000 | Loss: 0.00005037
Iteration 6/1000 | Loss: 0.00004467
Iteration 7/1000 | Loss: 0.00004351
Iteration 8/1000 | Loss: 0.00004298
Iteration 9/1000 | Loss: 0.00004277
Iteration 10/1000 | Loss: 0.00004246
Iteration 11/1000 | Loss: 0.00004224
Iteration 12/1000 | Loss: 0.00004215
Iteration 13/1000 | Loss: 0.00004214
Iteration 14/1000 | Loss: 0.00004213
Iteration 15/1000 | Loss: 0.00004212
Iteration 16/1000 | Loss: 0.00004211
Iteration 17/1000 | Loss: 0.00004210
Iteration 18/1000 | Loss: 0.00004210
Iteration 19/1000 | Loss: 0.00004210
Iteration 20/1000 | Loss: 0.00004209
Iteration 21/1000 | Loss: 0.00004208
Iteration 22/1000 | Loss: 0.00004208
Iteration 23/1000 | Loss: 0.00004208
Iteration 24/1000 | Loss: 0.00004208
Iteration 25/1000 | Loss: 0.00004207
Iteration 26/1000 | Loss: 0.00004206
Iteration 27/1000 | Loss: 0.00004205
Iteration 28/1000 | Loss: 0.00004205
Iteration 29/1000 | Loss: 0.00004205
Iteration 30/1000 | Loss: 0.00004204
Iteration 31/1000 | Loss: 0.00004203
Iteration 32/1000 | Loss: 0.00004203
Iteration 33/1000 | Loss: 0.00004203
Iteration 34/1000 | Loss: 0.00004202
Iteration 35/1000 | Loss: 0.00004202
Iteration 36/1000 | Loss: 0.00004202
Iteration 37/1000 | Loss: 0.00004202
Iteration 38/1000 | Loss: 0.00004201
Iteration 39/1000 | Loss: 0.00004198
Iteration 40/1000 | Loss: 0.00004198
Iteration 41/1000 | Loss: 0.00004197
Iteration 42/1000 | Loss: 0.00004197
Iteration 43/1000 | Loss: 0.00004197
Iteration 44/1000 | Loss: 0.00004197
Iteration 45/1000 | Loss: 0.00004197
Iteration 46/1000 | Loss: 0.00004197
Iteration 47/1000 | Loss: 0.00004197
Iteration 48/1000 | Loss: 0.00004197
Iteration 49/1000 | Loss: 0.00004197
Iteration 50/1000 | Loss: 0.00004197
Iteration 51/1000 | Loss: 0.00004197
Iteration 52/1000 | Loss: 0.00004197
Iteration 53/1000 | Loss: 0.00004196
Iteration 54/1000 | Loss: 0.00004196
Iteration 55/1000 | Loss: 0.00004194
Iteration 56/1000 | Loss: 0.00004193
Iteration 57/1000 | Loss: 0.00004193
Iteration 58/1000 | Loss: 0.00004193
Iteration 59/1000 | Loss: 0.00004192
Iteration 60/1000 | Loss: 0.00004192
Iteration 61/1000 | Loss: 0.00004192
Iteration 62/1000 | Loss: 0.00004191
Iteration 63/1000 | Loss: 0.00004191
Iteration 64/1000 | Loss: 0.00004191
Iteration 65/1000 | Loss: 0.00004191
Iteration 66/1000 | Loss: 0.00004191
Iteration 67/1000 | Loss: 0.00004191
Iteration 68/1000 | Loss: 0.00004191
Iteration 69/1000 | Loss: 0.00004191
Iteration 70/1000 | Loss: 0.00004191
Iteration 71/1000 | Loss: 0.00004191
Iteration 72/1000 | Loss: 0.00004190
Iteration 73/1000 | Loss: 0.00004190
Iteration 74/1000 | Loss: 0.00004190
Iteration 75/1000 | Loss: 0.00004189
Iteration 76/1000 | Loss: 0.00004189
Iteration 77/1000 | Loss: 0.00004189
Iteration 78/1000 | Loss: 0.00004188
Iteration 79/1000 | Loss: 0.00004188
Iteration 80/1000 | Loss: 0.00004187
Iteration 81/1000 | Loss: 0.00004187
Iteration 82/1000 | Loss: 0.00004187
Iteration 83/1000 | Loss: 0.00004187
Iteration 84/1000 | Loss: 0.00004186
Iteration 85/1000 | Loss: 0.00004186
Iteration 86/1000 | Loss: 0.00004186
Iteration 87/1000 | Loss: 0.00004186
Iteration 88/1000 | Loss: 0.00004186
Iteration 89/1000 | Loss: 0.00004186
Iteration 90/1000 | Loss: 0.00004186
Iteration 91/1000 | Loss: 0.00004186
Iteration 92/1000 | Loss: 0.00004186
Iteration 93/1000 | Loss: 0.00004185
Iteration 94/1000 | Loss: 0.00004185
Iteration 95/1000 | Loss: 0.00004185
Iteration 96/1000 | Loss: 0.00004185
Iteration 97/1000 | Loss: 0.00004185
Iteration 98/1000 | Loss: 0.00004185
Iteration 99/1000 | Loss: 0.00004185
Iteration 100/1000 | Loss: 0.00004185
Iteration 101/1000 | Loss: 0.00004185
Iteration 102/1000 | Loss: 0.00004185
Iteration 103/1000 | Loss: 0.00004185
Iteration 104/1000 | Loss: 0.00004185
Iteration 105/1000 | Loss: 0.00004185
Iteration 106/1000 | Loss: 0.00004185
Iteration 107/1000 | Loss: 0.00004185
Iteration 108/1000 | Loss: 0.00004185
Iteration 109/1000 | Loss: 0.00004185
Iteration 110/1000 | Loss: 0.00004185
Iteration 111/1000 | Loss: 0.00004185
Iteration 112/1000 | Loss: 0.00004185
Iteration 113/1000 | Loss: 0.00004185
Iteration 114/1000 | Loss: 0.00004185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [4.185270154266618e-05, 4.185270154266618e-05, 4.185270154266618e-05, 4.185270154266618e-05, 4.185270154266618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.185270154266618e-05

Optimization complete. Final v2v error: 5.149160861968994 mm

Highest mean error: 23.41404151916504 mm for frame 114

Lowest mean error: 4.668911933898926 mm for frame 236

Saving results

Total time: 68.25957798957825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01115338
Iteration 2/25 | Loss: 0.00446704
Iteration 3/25 | Loss: 0.00346024
Iteration 4/25 | Loss: 0.00220915
Iteration 5/25 | Loss: 0.00188332
Iteration 6/25 | Loss: 0.00166932
Iteration 7/25 | Loss: 0.00141894
Iteration 8/25 | Loss: 0.00129572
Iteration 9/25 | Loss: 0.00123437
Iteration 10/25 | Loss: 0.00122992
Iteration 11/25 | Loss: 0.00120606
Iteration 12/25 | Loss: 0.00119481
Iteration 13/25 | Loss: 0.00118800
Iteration 14/25 | Loss: 0.00118881
Iteration 15/25 | Loss: 0.00117868
Iteration 16/25 | Loss: 0.00118097
Iteration 17/25 | Loss: 0.00117708
Iteration 18/25 | Loss: 0.00118007
Iteration 19/25 | Loss: 0.00117583
Iteration 20/25 | Loss: 0.00117572
Iteration 21/25 | Loss: 0.00117551
Iteration 22/25 | Loss: 0.00117626
Iteration 23/25 | Loss: 0.00117584
Iteration 24/25 | Loss: 0.00117573
Iteration 25/25 | Loss: 0.00117545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51849866
Iteration 2/25 | Loss: 0.00124560
Iteration 3/25 | Loss: 0.00124560
Iteration 4/25 | Loss: 0.00124560
Iteration 5/25 | Loss: 0.00124560
Iteration 6/25 | Loss: 0.00124560
Iteration 7/25 | Loss: 0.00124560
Iteration 8/25 | Loss: 0.00124560
Iteration 9/25 | Loss: 0.00124560
Iteration 10/25 | Loss: 0.00124560
Iteration 11/25 | Loss: 0.00124560
Iteration 12/25 | Loss: 0.00124560
Iteration 13/25 | Loss: 0.00124560
Iteration 14/25 | Loss: 0.00124560
Iteration 15/25 | Loss: 0.00124560
Iteration 16/25 | Loss: 0.00124560
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012456016847863793, 0.0012456016847863793, 0.0012456016847863793, 0.0012456016847863793, 0.0012456016847863793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012456016847863793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124560
Iteration 2/1000 | Loss: 0.00010423
Iteration 3/1000 | Loss: 0.00006070
Iteration 4/1000 | Loss: 0.00020459
Iteration 5/1000 | Loss: 0.00005164
Iteration 6/1000 | Loss: 0.00004755
Iteration 7/1000 | Loss: 0.00010450
Iteration 8/1000 | Loss: 0.00008114
Iteration 9/1000 | Loss: 0.00004351
Iteration 10/1000 | Loss: 0.00008365
Iteration 11/1000 | Loss: 0.00004282
Iteration 12/1000 | Loss: 0.00004243
Iteration 13/1000 | Loss: 0.00004207
Iteration 14/1000 | Loss: 0.00010158
Iteration 15/1000 | Loss: 0.00004232
Iteration 16/1000 | Loss: 0.00017484
Iteration 17/1000 | Loss: 0.00004699
Iteration 18/1000 | Loss: 0.00004191
Iteration 19/1000 | Loss: 0.00006348
Iteration 20/1000 | Loss: 0.00004186
Iteration 21/1000 | Loss: 0.00004167
Iteration 22/1000 | Loss: 0.00004164
Iteration 23/1000 | Loss: 0.00004161
Iteration 24/1000 | Loss: 0.00004160
Iteration 25/1000 | Loss: 0.00004158
Iteration 26/1000 | Loss: 0.00004158
Iteration 27/1000 | Loss: 0.00004158
Iteration 28/1000 | Loss: 0.00004158
Iteration 29/1000 | Loss: 0.00004158
Iteration 30/1000 | Loss: 0.00004158
Iteration 31/1000 | Loss: 0.00004158
Iteration 32/1000 | Loss: 0.00004157
Iteration 33/1000 | Loss: 0.00004157
Iteration 34/1000 | Loss: 0.00004152
Iteration 35/1000 | Loss: 0.00004152
Iteration 36/1000 | Loss: 0.00004151
Iteration 37/1000 | Loss: 0.00004151
Iteration 38/1000 | Loss: 0.00004150
Iteration 39/1000 | Loss: 0.00004150
Iteration 40/1000 | Loss: 0.00004149
Iteration 41/1000 | Loss: 0.00004149
Iteration 42/1000 | Loss: 0.00004149
Iteration 43/1000 | Loss: 0.00004149
Iteration 44/1000 | Loss: 0.00004149
Iteration 45/1000 | Loss: 0.00004149
Iteration 46/1000 | Loss: 0.00004149
Iteration 47/1000 | Loss: 0.00004148
Iteration 48/1000 | Loss: 0.00004148
Iteration 49/1000 | Loss: 0.00004148
Iteration 50/1000 | Loss: 0.00004148
Iteration 51/1000 | Loss: 0.00004148
Iteration 52/1000 | Loss: 0.00004148
Iteration 53/1000 | Loss: 0.00004148
Iteration 54/1000 | Loss: 0.00004147
Iteration 55/1000 | Loss: 0.00004147
Iteration 56/1000 | Loss: 0.00004147
Iteration 57/1000 | Loss: 0.00004147
Iteration 58/1000 | Loss: 0.00004147
Iteration 59/1000 | Loss: 0.00004147
Iteration 60/1000 | Loss: 0.00004147
Iteration 61/1000 | Loss: 0.00004147
Iteration 62/1000 | Loss: 0.00004147
Iteration 63/1000 | Loss: 0.00004147
Iteration 64/1000 | Loss: 0.00004147
Iteration 65/1000 | Loss: 0.00004147
Iteration 66/1000 | Loss: 0.00004147
Iteration 67/1000 | Loss: 0.00004147
Iteration 68/1000 | Loss: 0.00004147
Iteration 69/1000 | Loss: 0.00004146
Iteration 70/1000 | Loss: 0.00004146
Iteration 71/1000 | Loss: 0.00004146
Iteration 72/1000 | Loss: 0.00004146
Iteration 73/1000 | Loss: 0.00004146
Iteration 74/1000 | Loss: 0.00004146
Iteration 75/1000 | Loss: 0.00004146
Iteration 76/1000 | Loss: 0.00004145
Iteration 77/1000 | Loss: 0.00004145
Iteration 78/1000 | Loss: 0.00004145
Iteration 79/1000 | Loss: 0.00004145
Iteration 80/1000 | Loss: 0.00004145
Iteration 81/1000 | Loss: 0.00004145
Iteration 82/1000 | Loss: 0.00004145
Iteration 83/1000 | Loss: 0.00004145
Iteration 84/1000 | Loss: 0.00004145
Iteration 85/1000 | Loss: 0.00004145
Iteration 86/1000 | Loss: 0.00004145
Iteration 87/1000 | Loss: 0.00004144
Iteration 88/1000 | Loss: 0.00004144
Iteration 89/1000 | Loss: 0.00004143
Iteration 90/1000 | Loss: 0.00004143
Iteration 91/1000 | Loss: 0.00004143
Iteration 92/1000 | Loss: 0.00004143
Iteration 93/1000 | Loss: 0.00004143
Iteration 94/1000 | Loss: 0.00004143
Iteration 95/1000 | Loss: 0.00004143
Iteration 96/1000 | Loss: 0.00004143
Iteration 97/1000 | Loss: 0.00004143
Iteration 98/1000 | Loss: 0.00004142
Iteration 99/1000 | Loss: 0.00004142
Iteration 100/1000 | Loss: 0.00004142
Iteration 101/1000 | Loss: 0.00004142
Iteration 102/1000 | Loss: 0.00004142
Iteration 103/1000 | Loss: 0.00004142
Iteration 104/1000 | Loss: 0.00004142
Iteration 105/1000 | Loss: 0.00004142
Iteration 106/1000 | Loss: 0.00004141
Iteration 107/1000 | Loss: 0.00004141
Iteration 108/1000 | Loss: 0.00004141
Iteration 109/1000 | Loss: 0.00004141
Iteration 110/1000 | Loss: 0.00004140
Iteration 111/1000 | Loss: 0.00004140
Iteration 112/1000 | Loss: 0.00004140
Iteration 113/1000 | Loss: 0.00004140
Iteration 114/1000 | Loss: 0.00004140
Iteration 115/1000 | Loss: 0.00004139
Iteration 116/1000 | Loss: 0.00004138
Iteration 117/1000 | Loss: 0.00004138
Iteration 118/1000 | Loss: 0.00004138
Iteration 119/1000 | Loss: 0.00004137
Iteration 120/1000 | Loss: 0.00004137
Iteration 121/1000 | Loss: 0.00004137
Iteration 122/1000 | Loss: 0.00004137
Iteration 123/1000 | Loss: 0.00004136
Iteration 124/1000 | Loss: 0.00004136
Iteration 125/1000 | Loss: 0.00004136
Iteration 126/1000 | Loss: 0.00004136
Iteration 127/1000 | Loss: 0.00004136
Iteration 128/1000 | Loss: 0.00004136
Iteration 129/1000 | Loss: 0.00004136
Iteration 130/1000 | Loss: 0.00004136
Iteration 131/1000 | Loss: 0.00004136
Iteration 132/1000 | Loss: 0.00004136
Iteration 133/1000 | Loss: 0.00004135
Iteration 134/1000 | Loss: 0.00004135
Iteration 135/1000 | Loss: 0.00004135
Iteration 136/1000 | Loss: 0.00004135
Iteration 137/1000 | Loss: 0.00004135
Iteration 138/1000 | Loss: 0.00004135
Iteration 139/1000 | Loss: 0.00004135
Iteration 140/1000 | Loss: 0.00004135
Iteration 141/1000 | Loss: 0.00004135
Iteration 142/1000 | Loss: 0.00004135
Iteration 143/1000 | Loss: 0.00004135
Iteration 144/1000 | Loss: 0.00004134
Iteration 145/1000 | Loss: 0.00004134
Iteration 146/1000 | Loss: 0.00004134
Iteration 147/1000 | Loss: 0.00004134
Iteration 148/1000 | Loss: 0.00004134
Iteration 149/1000 | Loss: 0.00004134
Iteration 150/1000 | Loss: 0.00004134
Iteration 151/1000 | Loss: 0.00004134
Iteration 152/1000 | Loss: 0.00004134
Iteration 153/1000 | Loss: 0.00004134
Iteration 154/1000 | Loss: 0.00004134
Iteration 155/1000 | Loss: 0.00004134
Iteration 156/1000 | Loss: 0.00004134
Iteration 157/1000 | Loss: 0.00004134
Iteration 158/1000 | Loss: 0.00004134
Iteration 159/1000 | Loss: 0.00004134
Iteration 160/1000 | Loss: 0.00004134
Iteration 161/1000 | Loss: 0.00004134
Iteration 162/1000 | Loss: 0.00004133
Iteration 163/1000 | Loss: 0.00004133
Iteration 164/1000 | Loss: 0.00004133
Iteration 165/1000 | Loss: 0.00004133
Iteration 166/1000 | Loss: 0.00004133
Iteration 167/1000 | Loss: 0.00004133
Iteration 168/1000 | Loss: 0.00004133
Iteration 169/1000 | Loss: 0.00004132
Iteration 170/1000 | Loss: 0.00004132
Iteration 171/1000 | Loss: 0.00004132
Iteration 172/1000 | Loss: 0.00004132
Iteration 173/1000 | Loss: 0.00004132
Iteration 174/1000 | Loss: 0.00004132
Iteration 175/1000 | Loss: 0.00004132
Iteration 176/1000 | Loss: 0.00004132
Iteration 177/1000 | Loss: 0.00004132
Iteration 178/1000 | Loss: 0.00004132
Iteration 179/1000 | Loss: 0.00004132
Iteration 180/1000 | Loss: 0.00004132
Iteration 181/1000 | Loss: 0.00004132
Iteration 182/1000 | Loss: 0.00004132
Iteration 183/1000 | Loss: 0.00004132
Iteration 184/1000 | Loss: 0.00004132
Iteration 185/1000 | Loss: 0.00004132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [4.1318300645798445e-05, 4.1318300645798445e-05, 4.1318300645798445e-05, 4.1318300645798445e-05, 4.1318300645798445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.1318300645798445e-05

Optimization complete. Final v2v error: 5.256841659545898 mm

Highest mean error: 10.880918502807617 mm for frame 217

Lowest mean error: 4.663180351257324 mm for frame 231

Saving results

Total time: 96.31199336051941
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_0122/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_0122/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01132606
Iteration 2/25 | Loss: 0.00300727
Iteration 3/25 | Loss: 0.00205405
Iteration 4/25 | Loss: 0.00177906
Iteration 5/25 | Loss: 0.00165578
Iteration 6/25 | Loss: 0.00155660
Iteration 7/25 | Loss: 0.00153342
Iteration 8/25 | Loss: 0.00151978
Iteration 9/25 | Loss: 0.00148606
Iteration 10/25 | Loss: 0.00149752
Iteration 11/25 | Loss: 0.00148661
Iteration 12/25 | Loss: 0.00148673
Iteration 13/25 | Loss: 0.00147193
Iteration 14/25 | Loss: 0.00146567
Iteration 15/25 | Loss: 0.00146582
Iteration 16/25 | Loss: 0.00145880
Iteration 17/25 | Loss: 0.00146923
Iteration 18/25 | Loss: 0.00147655
Iteration 19/25 | Loss: 0.00143482
Iteration 20/25 | Loss: 0.00142588
Iteration 21/25 | Loss: 0.00142878
Iteration 22/25 | Loss: 0.00142456
Iteration 23/25 | Loss: 0.00143989
Iteration 24/25 | Loss: 0.00141999
Iteration 25/25 | Loss: 0.00141925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57744157
Iteration 2/25 | Loss: 0.00238073
Iteration 3/25 | Loss: 0.00226806
Iteration 4/25 | Loss: 0.00226806
Iteration 5/25 | Loss: 0.00226806
Iteration 6/25 | Loss: 0.00226806
Iteration 7/25 | Loss: 0.00226806
Iteration 8/25 | Loss: 0.00226806
Iteration 9/25 | Loss: 0.00226806
Iteration 10/25 | Loss: 0.00226806
Iteration 11/25 | Loss: 0.00226806
Iteration 12/25 | Loss: 0.00226806
Iteration 13/25 | Loss: 0.00226806
Iteration 14/25 | Loss: 0.00226806
Iteration 15/25 | Loss: 0.00226806
Iteration 16/25 | Loss: 0.00226806
Iteration 17/25 | Loss: 0.00226806
Iteration 18/25 | Loss: 0.00226806
Iteration 19/25 | Loss: 0.00226806
Iteration 20/25 | Loss: 0.00226806
Iteration 21/25 | Loss: 0.00226806
Iteration 22/25 | Loss: 0.00226806
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002268060576170683, 0.002268060576170683, 0.002268060576170683, 0.002268060576170683, 0.002268060576170683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002268060576170683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226806
Iteration 2/1000 | Loss: 0.00027142
Iteration 3/1000 | Loss: 0.00023616
Iteration 4/1000 | Loss: 0.00019587
Iteration 5/1000 | Loss: 0.00022794
Iteration 6/1000 | Loss: 0.00013482
Iteration 7/1000 | Loss: 0.00018846
Iteration 8/1000 | Loss: 0.00009347
Iteration 9/1000 | Loss: 0.00015535
Iteration 10/1000 | Loss: 0.00039963
Iteration 11/1000 | Loss: 0.00041665
Iteration 12/1000 | Loss: 0.00429266
Iteration 13/1000 | Loss: 0.00584429
Iteration 14/1000 | Loss: 0.00141157
Iteration 15/1000 | Loss: 0.00164111
Iteration 16/1000 | Loss: 0.00097202
Iteration 17/1000 | Loss: 0.00060700
Iteration 18/1000 | Loss: 0.00045264
Iteration 19/1000 | Loss: 0.00043881
Iteration 20/1000 | Loss: 0.00060699
Iteration 21/1000 | Loss: 0.00029076
Iteration 22/1000 | Loss: 0.00042254
Iteration 23/1000 | Loss: 0.00036460
Iteration 24/1000 | Loss: 0.00024004
Iteration 25/1000 | Loss: 0.00048651
Iteration 26/1000 | Loss: 0.00015785
Iteration 27/1000 | Loss: 0.00013581
Iteration 28/1000 | Loss: 0.00013102
Iteration 29/1000 | Loss: 0.00069299
Iteration 30/1000 | Loss: 0.00082390
Iteration 31/1000 | Loss: 0.00082751
Iteration 32/1000 | Loss: 0.00009046
Iteration 33/1000 | Loss: 0.00044312
Iteration 34/1000 | Loss: 0.00066615
Iteration 35/1000 | Loss: 0.00093629
Iteration 36/1000 | Loss: 0.00008088
Iteration 37/1000 | Loss: 0.00078474
Iteration 38/1000 | Loss: 0.00056082
Iteration 39/1000 | Loss: 0.00011819
Iteration 40/1000 | Loss: 0.00037230
Iteration 41/1000 | Loss: 0.00005867
Iteration 42/1000 | Loss: 0.00048491
Iteration 43/1000 | Loss: 0.00012444
Iteration 44/1000 | Loss: 0.00008302
Iteration 45/1000 | Loss: 0.00006626
Iteration 46/1000 | Loss: 0.00006169
Iteration 47/1000 | Loss: 0.00006987
Iteration 48/1000 | Loss: 0.00006808
Iteration 49/1000 | Loss: 0.00004167
Iteration 50/1000 | Loss: 0.00004481
Iteration 51/1000 | Loss: 0.00006308
Iteration 52/1000 | Loss: 0.00006336
Iteration 53/1000 | Loss: 0.00010805
Iteration 54/1000 | Loss: 0.00020204
Iteration 55/1000 | Loss: 0.00023778
Iteration 56/1000 | Loss: 0.00003820
Iteration 57/1000 | Loss: 0.00009415
Iteration 58/1000 | Loss: 0.00030234
Iteration 59/1000 | Loss: 0.00005385
Iteration 60/1000 | Loss: 0.00008266
Iteration 61/1000 | Loss: 0.00004025
Iteration 62/1000 | Loss: 0.00006383
Iteration 63/1000 | Loss: 0.00007080
Iteration 64/1000 | Loss: 0.00004745
Iteration 65/1000 | Loss: 0.00005089
Iteration 66/1000 | Loss: 0.00003726
Iteration 67/1000 | Loss: 0.00004583
Iteration 68/1000 | Loss: 0.00007089
Iteration 69/1000 | Loss: 0.00003722
Iteration 70/1000 | Loss: 0.00005415
Iteration 71/1000 | Loss: 0.00006419
Iteration 72/1000 | Loss: 0.00005097
Iteration 73/1000 | Loss: 0.00004278
Iteration 74/1000 | Loss: 0.00003689
Iteration 75/1000 | Loss: 0.00003689
Iteration 76/1000 | Loss: 0.00003688
Iteration 77/1000 | Loss: 0.00003685
Iteration 78/1000 | Loss: 0.00003682
Iteration 79/1000 | Loss: 0.00003682
Iteration 80/1000 | Loss: 0.00003672
Iteration 81/1000 | Loss: 0.00003669
Iteration 82/1000 | Loss: 0.00003669
Iteration 83/1000 | Loss: 0.00003669
Iteration 84/1000 | Loss: 0.00003668
Iteration 85/1000 | Loss: 0.00003668
Iteration 86/1000 | Loss: 0.00003668
Iteration 87/1000 | Loss: 0.00003668
Iteration 88/1000 | Loss: 0.00003667
Iteration 89/1000 | Loss: 0.00003667
Iteration 90/1000 | Loss: 0.00003667
Iteration 91/1000 | Loss: 0.00003666
Iteration 92/1000 | Loss: 0.00003666
Iteration 93/1000 | Loss: 0.00003666
Iteration 94/1000 | Loss: 0.00003666
Iteration 95/1000 | Loss: 0.00003665
Iteration 96/1000 | Loss: 0.00003665
Iteration 97/1000 | Loss: 0.00003665
Iteration 98/1000 | Loss: 0.00003665
Iteration 99/1000 | Loss: 0.00003664
Iteration 100/1000 | Loss: 0.00003664
Iteration 101/1000 | Loss: 0.00003664
Iteration 102/1000 | Loss: 0.00003663
Iteration 103/1000 | Loss: 0.00003663
Iteration 104/1000 | Loss: 0.00003663
Iteration 105/1000 | Loss: 0.00003663
Iteration 106/1000 | Loss: 0.00003663
Iteration 107/1000 | Loss: 0.00003663
Iteration 108/1000 | Loss: 0.00003663
Iteration 109/1000 | Loss: 0.00003662
Iteration 110/1000 | Loss: 0.00003662
Iteration 111/1000 | Loss: 0.00003662
Iteration 112/1000 | Loss: 0.00003662
Iteration 113/1000 | Loss: 0.00003662
Iteration 114/1000 | Loss: 0.00003662
Iteration 115/1000 | Loss: 0.00003662
Iteration 116/1000 | Loss: 0.00003662
Iteration 117/1000 | Loss: 0.00003662
Iteration 118/1000 | Loss: 0.00003662
Iteration 119/1000 | Loss: 0.00003662
Iteration 120/1000 | Loss: 0.00003662
Iteration 121/1000 | Loss: 0.00003662
Iteration 122/1000 | Loss: 0.00003662
Iteration 123/1000 | Loss: 0.00003661
Iteration 124/1000 | Loss: 0.00003661
Iteration 125/1000 | Loss: 0.00003661
Iteration 126/1000 | Loss: 0.00003661
Iteration 127/1000 | Loss: 0.00003661
Iteration 128/1000 | Loss: 0.00003661
Iteration 129/1000 | Loss: 0.00003661
Iteration 130/1000 | Loss: 0.00003661
Iteration 131/1000 | Loss: 0.00003661
Iteration 132/1000 | Loss: 0.00003661
Iteration 133/1000 | Loss: 0.00003661
Iteration 134/1000 | Loss: 0.00003660
Iteration 135/1000 | Loss: 0.00003660
Iteration 136/1000 | Loss: 0.00003660
Iteration 137/1000 | Loss: 0.00003660
Iteration 138/1000 | Loss: 0.00003660
Iteration 139/1000 | Loss: 0.00003660
Iteration 140/1000 | Loss: 0.00003660
Iteration 141/1000 | Loss: 0.00003660
Iteration 142/1000 | Loss: 0.00003660
Iteration 143/1000 | Loss: 0.00003660
Iteration 144/1000 | Loss: 0.00003660
Iteration 145/1000 | Loss: 0.00003660
Iteration 146/1000 | Loss: 0.00003660
Iteration 147/1000 | Loss: 0.00003660
Iteration 148/1000 | Loss: 0.00003659
Iteration 149/1000 | Loss: 0.00003659
Iteration 150/1000 | Loss: 0.00003659
Iteration 151/1000 | Loss: 0.00003659
Iteration 152/1000 | Loss: 0.00003659
Iteration 153/1000 | Loss: 0.00003659
Iteration 154/1000 | Loss: 0.00003659
Iteration 155/1000 | Loss: 0.00003659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [3.6594610719475895e-05, 3.6594610719475895e-05, 3.6594610719475895e-05, 3.6594610719475895e-05, 3.6594610719475895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6594610719475895e-05

Optimization complete. Final v2v error: 5.10902214050293 mm

Highest mean error: 11.244440078735352 mm for frame 16

Lowest mean error: 4.640061378479004 mm for frame 17

Saving results

Total time: 151.52190470695496
