Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=160, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8960-9015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391524
Iteration 2/25 | Loss: 0.00124330
Iteration 3/25 | Loss: 0.00106310
Iteration 4/25 | Loss: 0.00104622
Iteration 5/25 | Loss: 0.00104169
Iteration 6/25 | Loss: 0.00104069
Iteration 7/25 | Loss: 0.00104064
Iteration 8/25 | Loss: 0.00104064
Iteration 9/25 | Loss: 0.00104064
Iteration 10/25 | Loss: 0.00104064
Iteration 11/25 | Loss: 0.00104064
Iteration 12/25 | Loss: 0.00104064
Iteration 13/25 | Loss: 0.00104064
Iteration 14/25 | Loss: 0.00104064
Iteration 15/25 | Loss: 0.00104064
Iteration 16/25 | Loss: 0.00104064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010406402871012688, 0.0010406402871012688, 0.0010406402871012688, 0.0010406402871012688, 0.0010406402871012688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010406402871012688

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43158209
Iteration 2/25 | Loss: 0.00055864
Iteration 3/25 | Loss: 0.00055863
Iteration 4/25 | Loss: 0.00055863
Iteration 5/25 | Loss: 0.00055863
Iteration 6/25 | Loss: 0.00055863
Iteration 7/25 | Loss: 0.00055863
Iteration 8/25 | Loss: 0.00055863
Iteration 9/25 | Loss: 0.00055863
Iteration 10/25 | Loss: 0.00055863
Iteration 11/25 | Loss: 0.00055863
Iteration 12/25 | Loss: 0.00055863
Iteration 13/25 | Loss: 0.00055863
Iteration 14/25 | Loss: 0.00055863
Iteration 15/25 | Loss: 0.00055863
Iteration 16/25 | Loss: 0.00055863
Iteration 17/25 | Loss: 0.00055863
Iteration 18/25 | Loss: 0.00055863
Iteration 19/25 | Loss: 0.00055863
Iteration 20/25 | Loss: 0.00055863
Iteration 21/25 | Loss: 0.00055863
Iteration 22/25 | Loss: 0.00055863
Iteration 23/25 | Loss: 0.00055863
Iteration 24/25 | Loss: 0.00055863
Iteration 25/25 | Loss: 0.00055863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055863
Iteration 2/1000 | Loss: 0.00005292
Iteration 3/1000 | Loss: 0.00002929
Iteration 4/1000 | Loss: 0.00002521
Iteration 5/1000 | Loss: 0.00002382
Iteration 6/1000 | Loss: 0.00002312
Iteration 7/1000 | Loss: 0.00002274
Iteration 8/1000 | Loss: 0.00002225
Iteration 9/1000 | Loss: 0.00002206
Iteration 10/1000 | Loss: 0.00002180
Iteration 11/1000 | Loss: 0.00002175
Iteration 12/1000 | Loss: 0.00002166
Iteration 13/1000 | Loss: 0.00002162
Iteration 14/1000 | Loss: 0.00002158
Iteration 15/1000 | Loss: 0.00002143
Iteration 16/1000 | Loss: 0.00002139
Iteration 17/1000 | Loss: 0.00002127
Iteration 18/1000 | Loss: 0.00002125
Iteration 19/1000 | Loss: 0.00002117
Iteration 20/1000 | Loss: 0.00002107
Iteration 21/1000 | Loss: 0.00002106
Iteration 22/1000 | Loss: 0.00002099
Iteration 23/1000 | Loss: 0.00002095
Iteration 24/1000 | Loss: 0.00002095
Iteration 25/1000 | Loss: 0.00002095
Iteration 26/1000 | Loss: 0.00002095
Iteration 27/1000 | Loss: 0.00002095
Iteration 28/1000 | Loss: 0.00002095
Iteration 29/1000 | Loss: 0.00002095
Iteration 30/1000 | Loss: 0.00002095
Iteration 31/1000 | Loss: 0.00002095
Iteration 32/1000 | Loss: 0.00002093
Iteration 33/1000 | Loss: 0.00002091
Iteration 34/1000 | Loss: 0.00002091
Iteration 35/1000 | Loss: 0.00002090
Iteration 36/1000 | Loss: 0.00002090
Iteration 37/1000 | Loss: 0.00002089
Iteration 38/1000 | Loss: 0.00002087
Iteration 39/1000 | Loss: 0.00002084
Iteration 40/1000 | Loss: 0.00002084
Iteration 41/1000 | Loss: 0.00002081
Iteration 42/1000 | Loss: 0.00002080
Iteration 43/1000 | Loss: 0.00002080
Iteration 44/1000 | Loss: 0.00002080
Iteration 45/1000 | Loss: 0.00002080
Iteration 46/1000 | Loss: 0.00002079
Iteration 47/1000 | Loss: 0.00002079
Iteration 48/1000 | Loss: 0.00002079
Iteration 49/1000 | Loss: 0.00002078
Iteration 50/1000 | Loss: 0.00002077
Iteration 51/1000 | Loss: 0.00002077
Iteration 52/1000 | Loss: 0.00002077
Iteration 53/1000 | Loss: 0.00002077
Iteration 54/1000 | Loss: 0.00002077
Iteration 55/1000 | Loss: 0.00002076
Iteration 56/1000 | Loss: 0.00002076
Iteration 57/1000 | Loss: 0.00002076
Iteration 58/1000 | Loss: 0.00002076
Iteration 59/1000 | Loss: 0.00002076
Iteration 60/1000 | Loss: 0.00002076
Iteration 61/1000 | Loss: 0.00002076
Iteration 62/1000 | Loss: 0.00002076
Iteration 63/1000 | Loss: 0.00002076
Iteration 64/1000 | Loss: 0.00002076
Iteration 65/1000 | Loss: 0.00002075
Iteration 66/1000 | Loss: 0.00002075
Iteration 67/1000 | Loss: 0.00002075
Iteration 68/1000 | Loss: 0.00002074
Iteration 69/1000 | Loss: 0.00002074
Iteration 70/1000 | Loss: 0.00002074
Iteration 71/1000 | Loss: 0.00002074
Iteration 72/1000 | Loss: 0.00002074
Iteration 73/1000 | Loss: 0.00002074
Iteration 74/1000 | Loss: 0.00002074
Iteration 75/1000 | Loss: 0.00002074
Iteration 76/1000 | Loss: 0.00002073
Iteration 77/1000 | Loss: 0.00002073
Iteration 78/1000 | Loss: 0.00002073
Iteration 79/1000 | Loss: 0.00002073
Iteration 80/1000 | Loss: 0.00002073
Iteration 81/1000 | Loss: 0.00002073
Iteration 82/1000 | Loss: 0.00002073
Iteration 83/1000 | Loss: 0.00002073
Iteration 84/1000 | Loss: 0.00002072
Iteration 85/1000 | Loss: 0.00002072
Iteration 86/1000 | Loss: 0.00002072
Iteration 87/1000 | Loss: 0.00002072
Iteration 88/1000 | Loss: 0.00002072
Iteration 89/1000 | Loss: 0.00002070
Iteration 90/1000 | Loss: 0.00002070
Iteration 91/1000 | Loss: 0.00002070
Iteration 92/1000 | Loss: 0.00002070
Iteration 93/1000 | Loss: 0.00002070
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002069
Iteration 97/1000 | Loss: 0.00002069
Iteration 98/1000 | Loss: 0.00002068
Iteration 99/1000 | Loss: 0.00002068
Iteration 100/1000 | Loss: 0.00002068
Iteration 101/1000 | Loss: 0.00002068
Iteration 102/1000 | Loss: 0.00002068
Iteration 103/1000 | Loss: 0.00002068
Iteration 104/1000 | Loss: 0.00002067
Iteration 105/1000 | Loss: 0.00002067
Iteration 106/1000 | Loss: 0.00002067
Iteration 107/1000 | Loss: 0.00002067
Iteration 108/1000 | Loss: 0.00002067
Iteration 109/1000 | Loss: 0.00002067
Iteration 110/1000 | Loss: 0.00002067
Iteration 111/1000 | Loss: 0.00002067
Iteration 112/1000 | Loss: 0.00002067
Iteration 113/1000 | Loss: 0.00002066
Iteration 114/1000 | Loss: 0.00002066
Iteration 115/1000 | Loss: 0.00002066
Iteration 116/1000 | Loss: 0.00002066
Iteration 117/1000 | Loss: 0.00002065
Iteration 118/1000 | Loss: 0.00002065
Iteration 119/1000 | Loss: 0.00002065
Iteration 120/1000 | Loss: 0.00002065
Iteration 121/1000 | Loss: 0.00002065
Iteration 122/1000 | Loss: 0.00002064
Iteration 123/1000 | Loss: 0.00002064
Iteration 124/1000 | Loss: 0.00002064
Iteration 125/1000 | Loss: 0.00002064
Iteration 126/1000 | Loss: 0.00002064
Iteration 127/1000 | Loss: 0.00002063
Iteration 128/1000 | Loss: 0.00002063
Iteration 129/1000 | Loss: 0.00002063
Iteration 130/1000 | Loss: 0.00002063
Iteration 131/1000 | Loss: 0.00002063
Iteration 132/1000 | Loss: 0.00002063
Iteration 133/1000 | Loss: 0.00002062
Iteration 134/1000 | Loss: 0.00002062
Iteration 135/1000 | Loss: 0.00002062
Iteration 136/1000 | Loss: 0.00002062
Iteration 137/1000 | Loss: 0.00002062
Iteration 138/1000 | Loss: 0.00002062
Iteration 139/1000 | Loss: 0.00002062
Iteration 140/1000 | Loss: 0.00002061
Iteration 141/1000 | Loss: 0.00002061
Iteration 142/1000 | Loss: 0.00002061
Iteration 143/1000 | Loss: 0.00002061
Iteration 144/1000 | Loss: 0.00002061
Iteration 145/1000 | Loss: 0.00002061
Iteration 146/1000 | Loss: 0.00002061
Iteration 147/1000 | Loss: 0.00002061
Iteration 148/1000 | Loss: 0.00002061
Iteration 149/1000 | Loss: 0.00002061
Iteration 150/1000 | Loss: 0.00002061
Iteration 151/1000 | Loss: 0.00002061
Iteration 152/1000 | Loss: 0.00002060
Iteration 153/1000 | Loss: 0.00002060
Iteration 154/1000 | Loss: 0.00002060
Iteration 155/1000 | Loss: 0.00002060
Iteration 156/1000 | Loss: 0.00002060
Iteration 157/1000 | Loss: 0.00002060
Iteration 158/1000 | Loss: 0.00002059
Iteration 159/1000 | Loss: 0.00002059
Iteration 160/1000 | Loss: 0.00002059
Iteration 161/1000 | Loss: 0.00002059
Iteration 162/1000 | Loss: 0.00002059
Iteration 163/1000 | Loss: 0.00002059
Iteration 164/1000 | Loss: 0.00002059
Iteration 165/1000 | Loss: 0.00002059
Iteration 166/1000 | Loss: 0.00002059
Iteration 167/1000 | Loss: 0.00002059
Iteration 168/1000 | Loss: 0.00002059
Iteration 169/1000 | Loss: 0.00002059
Iteration 170/1000 | Loss: 0.00002059
Iteration 171/1000 | Loss: 0.00002058
Iteration 172/1000 | Loss: 0.00002058
Iteration 173/1000 | Loss: 0.00002058
Iteration 174/1000 | Loss: 0.00002058
Iteration 175/1000 | Loss: 0.00002058
Iteration 176/1000 | Loss: 0.00002058
Iteration 177/1000 | Loss: 0.00002058
Iteration 178/1000 | Loss: 0.00002058
Iteration 179/1000 | Loss: 0.00002058
Iteration 180/1000 | Loss: 0.00002058
Iteration 181/1000 | Loss: 0.00002058
Iteration 182/1000 | Loss: 0.00002058
Iteration 183/1000 | Loss: 0.00002058
Iteration 184/1000 | Loss: 0.00002058
Iteration 185/1000 | Loss: 0.00002058
Iteration 186/1000 | Loss: 0.00002058
Iteration 187/1000 | Loss: 0.00002058
Iteration 188/1000 | Loss: 0.00002058
Iteration 189/1000 | Loss: 0.00002058
Iteration 190/1000 | Loss: 0.00002058
Iteration 191/1000 | Loss: 0.00002058
Iteration 192/1000 | Loss: 0.00002058
Iteration 193/1000 | Loss: 0.00002058
Iteration 194/1000 | Loss: 0.00002058
Iteration 195/1000 | Loss: 0.00002058
Iteration 196/1000 | Loss: 0.00002058
Iteration 197/1000 | Loss: 0.00002058
Iteration 198/1000 | Loss: 0.00002058
Iteration 199/1000 | Loss: 0.00002058
Iteration 200/1000 | Loss: 0.00002058
Iteration 201/1000 | Loss: 0.00002058
Iteration 202/1000 | Loss: 0.00002058
Iteration 203/1000 | Loss: 0.00002058
Iteration 204/1000 | Loss: 0.00002058
Iteration 205/1000 | Loss: 0.00002058
Iteration 206/1000 | Loss: 0.00002058
Iteration 207/1000 | Loss: 0.00002058
Iteration 208/1000 | Loss: 0.00002058
Iteration 209/1000 | Loss: 0.00002058
Iteration 210/1000 | Loss: 0.00002058
Iteration 211/1000 | Loss: 0.00002058
Iteration 212/1000 | Loss: 0.00002058
Iteration 213/1000 | Loss: 0.00002058
Iteration 214/1000 | Loss: 0.00002058
Iteration 215/1000 | Loss: 0.00002058
Iteration 216/1000 | Loss: 0.00002058
Iteration 217/1000 | Loss: 0.00002058
Iteration 218/1000 | Loss: 0.00002058
Iteration 219/1000 | Loss: 0.00002058
Iteration 220/1000 | Loss: 0.00002058
Iteration 221/1000 | Loss: 0.00002058
Iteration 222/1000 | Loss: 0.00002058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [2.057549136225134e-05, 2.057549136225134e-05, 2.057549136225134e-05, 2.057549136225134e-05, 2.057549136225134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.057549136225134e-05

Optimization complete. Final v2v error: 3.7325477600097656 mm

Highest mean error: 4.118274211883545 mm for frame 38

Lowest mean error: 3.0113210678100586 mm for frame 145

Saving results

Total time: 49.102139949798584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431568
Iteration 2/25 | Loss: 0.00120785
Iteration 3/25 | Loss: 0.00105464
Iteration 4/25 | Loss: 0.00103265
Iteration 5/25 | Loss: 0.00102549
Iteration 6/25 | Loss: 0.00102370
Iteration 7/25 | Loss: 0.00102320
Iteration 8/25 | Loss: 0.00102312
Iteration 9/25 | Loss: 0.00102312
Iteration 10/25 | Loss: 0.00102312
Iteration 11/25 | Loss: 0.00102312
Iteration 12/25 | Loss: 0.00102312
Iteration 13/25 | Loss: 0.00102311
Iteration 14/25 | Loss: 0.00102311
Iteration 15/25 | Loss: 0.00102311
Iteration 16/25 | Loss: 0.00102311
Iteration 17/25 | Loss: 0.00102311
Iteration 18/25 | Loss: 0.00102311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010231105843558908, 0.0010231105843558908, 0.0010231105843558908, 0.0010231105843558908, 0.0010231105843558908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010231105843558908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45344675
Iteration 2/25 | Loss: 0.00046392
Iteration 3/25 | Loss: 0.00046392
Iteration 4/25 | Loss: 0.00046392
Iteration 5/25 | Loss: 0.00046392
Iteration 6/25 | Loss: 0.00046392
Iteration 7/25 | Loss: 0.00046392
Iteration 8/25 | Loss: 0.00046392
Iteration 9/25 | Loss: 0.00046392
Iteration 10/25 | Loss: 0.00046392
Iteration 11/25 | Loss: 0.00046392
Iteration 12/25 | Loss: 0.00046392
Iteration 13/25 | Loss: 0.00046392
Iteration 14/25 | Loss: 0.00046392
Iteration 15/25 | Loss: 0.00046392
Iteration 16/25 | Loss: 0.00046392
Iteration 17/25 | Loss: 0.00046392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00046391505748033524, 0.00046391505748033524, 0.00046391505748033524, 0.00046391505748033524, 0.00046391505748033524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046391505748033524

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046392
Iteration 2/1000 | Loss: 0.00003627
Iteration 3/1000 | Loss: 0.00002311
Iteration 4/1000 | Loss: 0.00001987
Iteration 5/1000 | Loss: 0.00001869
Iteration 6/1000 | Loss: 0.00001800
Iteration 7/1000 | Loss: 0.00001748
Iteration 8/1000 | Loss: 0.00001709
Iteration 9/1000 | Loss: 0.00001676
Iteration 10/1000 | Loss: 0.00001657
Iteration 11/1000 | Loss: 0.00001646
Iteration 12/1000 | Loss: 0.00001637
Iteration 13/1000 | Loss: 0.00001631
Iteration 14/1000 | Loss: 0.00001619
Iteration 15/1000 | Loss: 0.00001616
Iteration 16/1000 | Loss: 0.00001612
Iteration 17/1000 | Loss: 0.00001606
Iteration 18/1000 | Loss: 0.00001603
Iteration 19/1000 | Loss: 0.00001601
Iteration 20/1000 | Loss: 0.00001597
Iteration 21/1000 | Loss: 0.00001597
Iteration 22/1000 | Loss: 0.00001596
Iteration 23/1000 | Loss: 0.00001596
Iteration 24/1000 | Loss: 0.00001594
Iteration 25/1000 | Loss: 0.00001594
Iteration 26/1000 | Loss: 0.00001594
Iteration 27/1000 | Loss: 0.00001593
Iteration 28/1000 | Loss: 0.00001593
Iteration 29/1000 | Loss: 0.00001593
Iteration 30/1000 | Loss: 0.00001593
Iteration 31/1000 | Loss: 0.00001592
Iteration 32/1000 | Loss: 0.00001592
Iteration 33/1000 | Loss: 0.00001592
Iteration 34/1000 | Loss: 0.00001591
Iteration 35/1000 | Loss: 0.00001591
Iteration 36/1000 | Loss: 0.00001591
Iteration 37/1000 | Loss: 0.00001591
Iteration 38/1000 | Loss: 0.00001591
Iteration 39/1000 | Loss: 0.00001591
Iteration 40/1000 | Loss: 0.00001591
Iteration 41/1000 | Loss: 0.00001590
Iteration 42/1000 | Loss: 0.00001590
Iteration 43/1000 | Loss: 0.00001590
Iteration 44/1000 | Loss: 0.00001590
Iteration 45/1000 | Loss: 0.00001590
Iteration 46/1000 | Loss: 0.00001590
Iteration 47/1000 | Loss: 0.00001590
Iteration 48/1000 | Loss: 0.00001590
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001589
Iteration 51/1000 | Loss: 0.00001589
Iteration 52/1000 | Loss: 0.00001589
Iteration 53/1000 | Loss: 0.00001589
Iteration 54/1000 | Loss: 0.00001589
Iteration 55/1000 | Loss: 0.00001589
Iteration 56/1000 | Loss: 0.00001589
Iteration 57/1000 | Loss: 0.00001588
Iteration 58/1000 | Loss: 0.00001588
Iteration 59/1000 | Loss: 0.00001588
Iteration 60/1000 | Loss: 0.00001588
Iteration 61/1000 | Loss: 0.00001588
Iteration 62/1000 | Loss: 0.00001588
Iteration 63/1000 | Loss: 0.00001588
Iteration 64/1000 | Loss: 0.00001588
Iteration 65/1000 | Loss: 0.00001587
Iteration 66/1000 | Loss: 0.00001587
Iteration 67/1000 | Loss: 0.00001587
Iteration 68/1000 | Loss: 0.00001587
Iteration 69/1000 | Loss: 0.00001587
Iteration 70/1000 | Loss: 0.00001587
Iteration 71/1000 | Loss: 0.00001587
Iteration 72/1000 | Loss: 0.00001587
Iteration 73/1000 | Loss: 0.00001587
Iteration 74/1000 | Loss: 0.00001586
Iteration 75/1000 | Loss: 0.00001586
Iteration 76/1000 | Loss: 0.00001586
Iteration 77/1000 | Loss: 0.00001585
Iteration 78/1000 | Loss: 0.00001585
Iteration 79/1000 | Loss: 0.00001585
Iteration 80/1000 | Loss: 0.00001585
Iteration 81/1000 | Loss: 0.00001584
Iteration 82/1000 | Loss: 0.00001584
Iteration 83/1000 | Loss: 0.00001584
Iteration 84/1000 | Loss: 0.00001584
Iteration 85/1000 | Loss: 0.00001583
Iteration 86/1000 | Loss: 0.00001583
Iteration 87/1000 | Loss: 0.00001583
Iteration 88/1000 | Loss: 0.00001583
Iteration 89/1000 | Loss: 0.00001582
Iteration 90/1000 | Loss: 0.00001582
Iteration 91/1000 | Loss: 0.00001582
Iteration 92/1000 | Loss: 0.00001582
Iteration 93/1000 | Loss: 0.00001581
Iteration 94/1000 | Loss: 0.00001581
Iteration 95/1000 | Loss: 0.00001581
Iteration 96/1000 | Loss: 0.00001581
Iteration 97/1000 | Loss: 0.00001581
Iteration 98/1000 | Loss: 0.00001581
Iteration 99/1000 | Loss: 0.00001581
Iteration 100/1000 | Loss: 0.00001581
Iteration 101/1000 | Loss: 0.00001581
Iteration 102/1000 | Loss: 0.00001581
Iteration 103/1000 | Loss: 0.00001581
Iteration 104/1000 | Loss: 0.00001580
Iteration 105/1000 | Loss: 0.00001580
Iteration 106/1000 | Loss: 0.00001580
Iteration 107/1000 | Loss: 0.00001580
Iteration 108/1000 | Loss: 0.00001580
Iteration 109/1000 | Loss: 0.00001580
Iteration 110/1000 | Loss: 0.00001580
Iteration 111/1000 | Loss: 0.00001580
Iteration 112/1000 | Loss: 0.00001580
Iteration 113/1000 | Loss: 0.00001580
Iteration 114/1000 | Loss: 0.00001580
Iteration 115/1000 | Loss: 0.00001580
Iteration 116/1000 | Loss: 0.00001580
Iteration 117/1000 | Loss: 0.00001580
Iteration 118/1000 | Loss: 0.00001580
Iteration 119/1000 | Loss: 0.00001580
Iteration 120/1000 | Loss: 0.00001579
Iteration 121/1000 | Loss: 0.00001579
Iteration 122/1000 | Loss: 0.00001579
Iteration 123/1000 | Loss: 0.00001579
Iteration 124/1000 | Loss: 0.00001579
Iteration 125/1000 | Loss: 0.00001579
Iteration 126/1000 | Loss: 0.00001579
Iteration 127/1000 | Loss: 0.00001579
Iteration 128/1000 | Loss: 0.00001579
Iteration 129/1000 | Loss: 0.00001579
Iteration 130/1000 | Loss: 0.00001579
Iteration 131/1000 | Loss: 0.00001579
Iteration 132/1000 | Loss: 0.00001579
Iteration 133/1000 | Loss: 0.00001579
Iteration 134/1000 | Loss: 0.00001579
Iteration 135/1000 | Loss: 0.00001579
Iteration 136/1000 | Loss: 0.00001579
Iteration 137/1000 | Loss: 0.00001579
Iteration 138/1000 | Loss: 0.00001578
Iteration 139/1000 | Loss: 0.00001578
Iteration 140/1000 | Loss: 0.00001578
Iteration 141/1000 | Loss: 0.00001578
Iteration 142/1000 | Loss: 0.00001578
Iteration 143/1000 | Loss: 0.00001578
Iteration 144/1000 | Loss: 0.00001578
Iteration 145/1000 | Loss: 0.00001578
Iteration 146/1000 | Loss: 0.00001578
Iteration 147/1000 | Loss: 0.00001578
Iteration 148/1000 | Loss: 0.00001578
Iteration 149/1000 | Loss: 0.00001578
Iteration 150/1000 | Loss: 0.00001578
Iteration 151/1000 | Loss: 0.00001578
Iteration 152/1000 | Loss: 0.00001578
Iteration 153/1000 | Loss: 0.00001578
Iteration 154/1000 | Loss: 0.00001578
Iteration 155/1000 | Loss: 0.00001578
Iteration 156/1000 | Loss: 0.00001577
Iteration 157/1000 | Loss: 0.00001577
Iteration 158/1000 | Loss: 0.00001577
Iteration 159/1000 | Loss: 0.00001577
Iteration 160/1000 | Loss: 0.00001577
Iteration 161/1000 | Loss: 0.00001577
Iteration 162/1000 | Loss: 0.00001577
Iteration 163/1000 | Loss: 0.00001577
Iteration 164/1000 | Loss: 0.00001577
Iteration 165/1000 | Loss: 0.00001577
Iteration 166/1000 | Loss: 0.00001577
Iteration 167/1000 | Loss: 0.00001577
Iteration 168/1000 | Loss: 0.00001577
Iteration 169/1000 | Loss: 0.00001577
Iteration 170/1000 | Loss: 0.00001577
Iteration 171/1000 | Loss: 0.00001577
Iteration 172/1000 | Loss: 0.00001577
Iteration 173/1000 | Loss: 0.00001577
Iteration 174/1000 | Loss: 0.00001577
Iteration 175/1000 | Loss: 0.00001577
Iteration 176/1000 | Loss: 0.00001577
Iteration 177/1000 | Loss: 0.00001577
Iteration 178/1000 | Loss: 0.00001577
Iteration 179/1000 | Loss: 0.00001577
Iteration 180/1000 | Loss: 0.00001576
Iteration 181/1000 | Loss: 0.00001576
Iteration 182/1000 | Loss: 0.00001576
Iteration 183/1000 | Loss: 0.00001576
Iteration 184/1000 | Loss: 0.00001576
Iteration 185/1000 | Loss: 0.00001576
Iteration 186/1000 | Loss: 0.00001576
Iteration 187/1000 | Loss: 0.00001576
Iteration 188/1000 | Loss: 0.00001576
Iteration 189/1000 | Loss: 0.00001576
Iteration 190/1000 | Loss: 0.00001576
Iteration 191/1000 | Loss: 0.00001576
Iteration 192/1000 | Loss: 0.00001576
Iteration 193/1000 | Loss: 0.00001576
Iteration 194/1000 | Loss: 0.00001576
Iteration 195/1000 | Loss: 0.00001576
Iteration 196/1000 | Loss: 0.00001576
Iteration 197/1000 | Loss: 0.00001576
Iteration 198/1000 | Loss: 0.00001576
Iteration 199/1000 | Loss: 0.00001576
Iteration 200/1000 | Loss: 0.00001576
Iteration 201/1000 | Loss: 0.00001576
Iteration 202/1000 | Loss: 0.00001576
Iteration 203/1000 | Loss: 0.00001576
Iteration 204/1000 | Loss: 0.00001576
Iteration 205/1000 | Loss: 0.00001576
Iteration 206/1000 | Loss: 0.00001576
Iteration 207/1000 | Loss: 0.00001576
Iteration 208/1000 | Loss: 0.00001576
Iteration 209/1000 | Loss: 0.00001576
Iteration 210/1000 | Loss: 0.00001576
Iteration 211/1000 | Loss: 0.00001576
Iteration 212/1000 | Loss: 0.00001576
Iteration 213/1000 | Loss: 0.00001576
Iteration 214/1000 | Loss: 0.00001576
Iteration 215/1000 | Loss: 0.00001576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.575647183926776e-05, 1.575647183926776e-05, 1.575647183926776e-05, 1.575647183926776e-05, 1.575647183926776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.575647183926776e-05

Optimization complete. Final v2v error: 3.48089337348938 mm

Highest mean error: 4.334433078765869 mm for frame 30

Lowest mean error: 2.8869071006774902 mm for frame 0

Saving results

Total time: 43.82029056549072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864610
Iteration 2/25 | Loss: 0.00161902
Iteration 3/25 | Loss: 0.00118931
Iteration 4/25 | Loss: 0.00112623
Iteration 5/25 | Loss: 0.00111454
Iteration 6/25 | Loss: 0.00108827
Iteration 7/25 | Loss: 0.00107644
Iteration 8/25 | Loss: 0.00107193
Iteration 9/25 | Loss: 0.00106914
Iteration 10/25 | Loss: 0.00106424
Iteration 11/25 | Loss: 0.00106133
Iteration 12/25 | Loss: 0.00106538
Iteration 13/25 | Loss: 0.00106130
Iteration 14/25 | Loss: 0.00106001
Iteration 15/25 | Loss: 0.00105996
Iteration 16/25 | Loss: 0.00105978
Iteration 17/25 | Loss: 0.00105972
Iteration 18/25 | Loss: 0.00105972
Iteration 19/25 | Loss: 0.00105972
Iteration 20/25 | Loss: 0.00105971
Iteration 21/25 | Loss: 0.00105971
Iteration 22/25 | Loss: 0.00105971
Iteration 23/25 | Loss: 0.00105971
Iteration 24/25 | Loss: 0.00105971
Iteration 25/25 | Loss: 0.00105971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.58768988
Iteration 2/25 | Loss: 0.00036114
Iteration 3/25 | Loss: 0.00036112
Iteration 4/25 | Loss: 0.00036112
Iteration 5/25 | Loss: 0.00036112
Iteration 6/25 | Loss: 0.00036112
Iteration 7/25 | Loss: 0.00036112
Iteration 8/25 | Loss: 0.00036112
Iteration 9/25 | Loss: 0.00036112
Iteration 10/25 | Loss: 0.00036112
Iteration 11/25 | Loss: 0.00036112
Iteration 12/25 | Loss: 0.00036112
Iteration 13/25 | Loss: 0.00036112
Iteration 14/25 | Loss: 0.00036112
Iteration 15/25 | Loss: 0.00036112
Iteration 16/25 | Loss: 0.00036112
Iteration 17/25 | Loss: 0.00036112
Iteration 18/25 | Loss: 0.00036112
Iteration 19/25 | Loss: 0.00036112
Iteration 20/25 | Loss: 0.00036112
Iteration 21/25 | Loss: 0.00036112
Iteration 22/25 | Loss: 0.00036112
Iteration 23/25 | Loss: 0.00036112
Iteration 24/25 | Loss: 0.00036112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003611158754210919, 0.0003611158754210919, 0.0003611158754210919, 0.0003611158754210919, 0.0003611158754210919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003611158754210919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036112
Iteration 2/1000 | Loss: 0.00003347
Iteration 3/1000 | Loss: 0.00002470
Iteration 4/1000 | Loss: 0.00002221
Iteration 5/1000 | Loss: 0.00002123
Iteration 6/1000 | Loss: 0.00002076
Iteration 7/1000 | Loss: 0.00002040
Iteration 8/1000 | Loss: 0.00001995
Iteration 9/1000 | Loss: 0.00001961
Iteration 10/1000 | Loss: 0.00001955
Iteration 11/1000 | Loss: 0.00001937
Iteration 12/1000 | Loss: 0.00001924
Iteration 13/1000 | Loss: 0.00001910
Iteration 14/1000 | Loss: 0.00001910
Iteration 15/1000 | Loss: 0.00001909
Iteration 16/1000 | Loss: 0.00001904
Iteration 17/1000 | Loss: 0.00001901
Iteration 18/1000 | Loss: 0.00001900
Iteration 19/1000 | Loss: 0.00001900
Iteration 20/1000 | Loss: 0.00001900
Iteration 21/1000 | Loss: 0.00001899
Iteration 22/1000 | Loss: 0.00001898
Iteration 23/1000 | Loss: 0.00001898
Iteration 24/1000 | Loss: 0.00001898
Iteration 25/1000 | Loss: 0.00001898
Iteration 26/1000 | Loss: 0.00001897
Iteration 27/1000 | Loss: 0.00001897
Iteration 28/1000 | Loss: 0.00001897
Iteration 29/1000 | Loss: 0.00001896
Iteration 30/1000 | Loss: 0.00001896
Iteration 31/1000 | Loss: 0.00001895
Iteration 32/1000 | Loss: 0.00001895
Iteration 33/1000 | Loss: 0.00001895
Iteration 34/1000 | Loss: 0.00001894
Iteration 35/1000 | Loss: 0.00001894
Iteration 36/1000 | Loss: 0.00001894
Iteration 37/1000 | Loss: 0.00001893
Iteration 38/1000 | Loss: 0.00001893
Iteration 39/1000 | Loss: 0.00001893
Iteration 40/1000 | Loss: 0.00001893
Iteration 41/1000 | Loss: 0.00001893
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001892
Iteration 44/1000 | Loss: 0.00001896
Iteration 45/1000 | Loss: 0.00001894
Iteration 46/1000 | Loss: 0.00001893
Iteration 47/1000 | Loss: 0.00001893
Iteration 48/1000 | Loss: 0.00001893
Iteration 49/1000 | Loss: 0.00001891
Iteration 50/1000 | Loss: 0.00001891
Iteration 51/1000 | Loss: 0.00001890
Iteration 52/1000 | Loss: 0.00001890
Iteration 53/1000 | Loss: 0.00001889
Iteration 54/1000 | Loss: 0.00001889
Iteration 55/1000 | Loss: 0.00001889
Iteration 56/1000 | Loss: 0.00001889
Iteration 57/1000 | Loss: 0.00001886
Iteration 58/1000 | Loss: 0.00001886
Iteration 59/1000 | Loss: 0.00001886
Iteration 60/1000 | Loss: 0.00001886
Iteration 61/1000 | Loss: 0.00001886
Iteration 62/1000 | Loss: 0.00001886
Iteration 63/1000 | Loss: 0.00001886
Iteration 64/1000 | Loss: 0.00001885
Iteration 65/1000 | Loss: 0.00001885
Iteration 66/1000 | Loss: 0.00001885
Iteration 67/1000 | Loss: 0.00001885
Iteration 68/1000 | Loss: 0.00001885
Iteration 69/1000 | Loss: 0.00001885
Iteration 70/1000 | Loss: 0.00001885
Iteration 71/1000 | Loss: 0.00001884
Iteration 72/1000 | Loss: 0.00001884
Iteration 73/1000 | Loss: 0.00001884
Iteration 74/1000 | Loss: 0.00001888
Iteration 75/1000 | Loss: 0.00001887
Iteration 76/1000 | Loss: 0.00001887
Iteration 77/1000 | Loss: 0.00001887
Iteration 78/1000 | Loss: 0.00001887
Iteration 79/1000 | Loss: 0.00001887
Iteration 80/1000 | Loss: 0.00001887
Iteration 81/1000 | Loss: 0.00001886
Iteration 82/1000 | Loss: 0.00001885
Iteration 83/1000 | Loss: 0.00001885
Iteration 84/1000 | Loss: 0.00001885
Iteration 85/1000 | Loss: 0.00001885
Iteration 86/1000 | Loss: 0.00001885
Iteration 87/1000 | Loss: 0.00001885
Iteration 88/1000 | Loss: 0.00001881
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001881
Iteration 91/1000 | Loss: 0.00001881
Iteration 92/1000 | Loss: 0.00001881
Iteration 93/1000 | Loss: 0.00001881
Iteration 94/1000 | Loss: 0.00001881
Iteration 95/1000 | Loss: 0.00001881
Iteration 96/1000 | Loss: 0.00001881
Iteration 97/1000 | Loss: 0.00001881
Iteration 98/1000 | Loss: 0.00001881
Iteration 99/1000 | Loss: 0.00001881
Iteration 100/1000 | Loss: 0.00001881
Iteration 101/1000 | Loss: 0.00001880
Iteration 102/1000 | Loss: 0.00001880
Iteration 103/1000 | Loss: 0.00001880
Iteration 104/1000 | Loss: 0.00001880
Iteration 105/1000 | Loss: 0.00001882
Iteration 106/1000 | Loss: 0.00001882
Iteration 107/1000 | Loss: 0.00001882
Iteration 108/1000 | Loss: 0.00001882
Iteration 109/1000 | Loss: 0.00001881
Iteration 110/1000 | Loss: 0.00001881
Iteration 111/1000 | Loss: 0.00001881
Iteration 112/1000 | Loss: 0.00001881
Iteration 113/1000 | Loss: 0.00001881
Iteration 114/1000 | Loss: 0.00001881
Iteration 115/1000 | Loss: 0.00001881
Iteration 116/1000 | Loss: 0.00001881
Iteration 117/1000 | Loss: 0.00001881
Iteration 118/1000 | Loss: 0.00001881
Iteration 119/1000 | Loss: 0.00001880
Iteration 120/1000 | Loss: 0.00001880
Iteration 121/1000 | Loss: 0.00001880
Iteration 122/1000 | Loss: 0.00001880
Iteration 123/1000 | Loss: 0.00001880
Iteration 124/1000 | Loss: 0.00001880
Iteration 125/1000 | Loss: 0.00001880
Iteration 126/1000 | Loss: 0.00001880
Iteration 127/1000 | Loss: 0.00001880
Iteration 128/1000 | Loss: 0.00001880
Iteration 129/1000 | Loss: 0.00001880
Iteration 130/1000 | Loss: 0.00001880
Iteration 131/1000 | Loss: 0.00001880
Iteration 132/1000 | Loss: 0.00001880
Iteration 133/1000 | Loss: 0.00001880
Iteration 134/1000 | Loss: 0.00001880
Iteration 135/1000 | Loss: 0.00001880
Iteration 136/1000 | Loss: 0.00001879
Iteration 137/1000 | Loss: 0.00001879
Iteration 138/1000 | Loss: 0.00001879
Iteration 139/1000 | Loss: 0.00001879
Iteration 140/1000 | Loss: 0.00001878
Iteration 141/1000 | Loss: 0.00001878
Iteration 142/1000 | Loss: 0.00001877
Iteration 143/1000 | Loss: 0.00001877
Iteration 144/1000 | Loss: 0.00001877
Iteration 145/1000 | Loss: 0.00001877
Iteration 146/1000 | Loss: 0.00001877
Iteration 147/1000 | Loss: 0.00001877
Iteration 148/1000 | Loss: 0.00001877
Iteration 149/1000 | Loss: 0.00001877
Iteration 150/1000 | Loss: 0.00001877
Iteration 151/1000 | Loss: 0.00001877
Iteration 152/1000 | Loss: 0.00001877
Iteration 153/1000 | Loss: 0.00001877
Iteration 154/1000 | Loss: 0.00001877
Iteration 155/1000 | Loss: 0.00001876
Iteration 156/1000 | Loss: 0.00001876
Iteration 157/1000 | Loss: 0.00001876
Iteration 158/1000 | Loss: 0.00001876
Iteration 159/1000 | Loss: 0.00001876
Iteration 160/1000 | Loss: 0.00001876
Iteration 161/1000 | Loss: 0.00001876
Iteration 162/1000 | Loss: 0.00001876
Iteration 163/1000 | Loss: 0.00001876
Iteration 164/1000 | Loss: 0.00001875
Iteration 165/1000 | Loss: 0.00001875
Iteration 166/1000 | Loss: 0.00001875
Iteration 167/1000 | Loss: 0.00001875
Iteration 168/1000 | Loss: 0.00001875
Iteration 169/1000 | Loss: 0.00001875
Iteration 170/1000 | Loss: 0.00001875
Iteration 171/1000 | Loss: 0.00001875
Iteration 172/1000 | Loss: 0.00001875
Iteration 173/1000 | Loss: 0.00001875
Iteration 174/1000 | Loss: 0.00001875
Iteration 175/1000 | Loss: 0.00001875
Iteration 176/1000 | Loss: 0.00001875
Iteration 177/1000 | Loss: 0.00001875
Iteration 178/1000 | Loss: 0.00001875
Iteration 179/1000 | Loss: 0.00001875
Iteration 180/1000 | Loss: 0.00001875
Iteration 181/1000 | Loss: 0.00001875
Iteration 182/1000 | Loss: 0.00001875
Iteration 183/1000 | Loss: 0.00001875
Iteration 184/1000 | Loss: 0.00001875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.875162161013577e-05, 1.875162161013577e-05, 1.875162161013577e-05, 1.875162161013577e-05, 1.875162161013577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.875162161013577e-05

Optimization complete. Final v2v error: 3.7339141368865967 mm

Highest mean error: 10.186882972717285 mm for frame 203

Lowest mean error: 3.400350332260132 mm for frame 118

Saving results

Total time: 70.03047370910645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817030
Iteration 2/25 | Loss: 0.00176430
Iteration 3/25 | Loss: 0.00106359
Iteration 4/25 | Loss: 0.00094689
Iteration 5/25 | Loss: 0.00093543
Iteration 6/25 | Loss: 0.00093271
Iteration 7/25 | Loss: 0.00093058
Iteration 8/25 | Loss: 0.00092706
Iteration 9/25 | Loss: 0.00092170
Iteration 10/25 | Loss: 0.00091946
Iteration 11/25 | Loss: 0.00091735
Iteration 12/25 | Loss: 0.00091673
Iteration 13/25 | Loss: 0.00091657
Iteration 14/25 | Loss: 0.00091656
Iteration 15/25 | Loss: 0.00091656
Iteration 16/25 | Loss: 0.00091656
Iteration 17/25 | Loss: 0.00091655
Iteration 18/25 | Loss: 0.00091655
Iteration 19/25 | Loss: 0.00091655
Iteration 20/25 | Loss: 0.00091655
Iteration 21/25 | Loss: 0.00091655
Iteration 22/25 | Loss: 0.00091655
Iteration 23/25 | Loss: 0.00091655
Iteration 24/25 | Loss: 0.00091655
Iteration 25/25 | Loss: 0.00091654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86205375
Iteration 2/25 | Loss: 0.00046171
Iteration 3/25 | Loss: 0.00046171
Iteration 4/25 | Loss: 0.00046171
Iteration 5/25 | Loss: 0.00046170
Iteration 6/25 | Loss: 0.00046170
Iteration 7/25 | Loss: 0.00046170
Iteration 8/25 | Loss: 0.00046170
Iteration 9/25 | Loss: 0.00046170
Iteration 10/25 | Loss: 0.00046170
Iteration 11/25 | Loss: 0.00046170
Iteration 12/25 | Loss: 0.00046170
Iteration 13/25 | Loss: 0.00046170
Iteration 14/25 | Loss: 0.00046170
Iteration 15/25 | Loss: 0.00046170
Iteration 16/25 | Loss: 0.00046170
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00046170319546945393, 0.00046170319546945393, 0.00046170319546945393, 0.00046170319546945393, 0.00046170319546945393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046170319546945393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046170
Iteration 2/1000 | Loss: 0.00002658
Iteration 3/1000 | Loss: 0.00002078
Iteration 4/1000 | Loss: 0.00001837
Iteration 5/1000 | Loss: 0.00001675
Iteration 6/1000 | Loss: 0.00001571
Iteration 7/1000 | Loss: 0.00001519
Iteration 8/1000 | Loss: 0.00001737
Iteration 9/1000 | Loss: 0.00001566
Iteration 10/1000 | Loss: 0.00001491
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001471
Iteration 13/1000 | Loss: 0.00001471
Iteration 14/1000 | Loss: 0.00001470
Iteration 15/1000 | Loss: 0.00001464
Iteration 16/1000 | Loss: 0.00001464
Iteration 17/1000 | Loss: 0.00001461
Iteration 18/1000 | Loss: 0.00001461
Iteration 19/1000 | Loss: 0.00001460
Iteration 20/1000 | Loss: 0.00001460
Iteration 21/1000 | Loss: 0.00001460
Iteration 22/1000 | Loss: 0.00001460
Iteration 23/1000 | Loss: 0.00001459
Iteration 24/1000 | Loss: 0.00001458
Iteration 25/1000 | Loss: 0.00001458
Iteration 26/1000 | Loss: 0.00001457
Iteration 27/1000 | Loss: 0.00001456
Iteration 28/1000 | Loss: 0.00001456
Iteration 29/1000 | Loss: 0.00001456
Iteration 30/1000 | Loss: 0.00001456
Iteration 31/1000 | Loss: 0.00001456
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00001455
Iteration 34/1000 | Loss: 0.00001454
Iteration 35/1000 | Loss: 0.00001453
Iteration 36/1000 | Loss: 0.00001489
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001453
Iteration 39/1000 | Loss: 0.00001477
Iteration 40/1000 | Loss: 0.00001455
Iteration 41/1000 | Loss: 0.00001454
Iteration 42/1000 | Loss: 0.00001453
Iteration 43/1000 | Loss: 0.00001453
Iteration 44/1000 | Loss: 0.00001452
Iteration 45/1000 | Loss: 0.00001452
Iteration 46/1000 | Loss: 0.00001451
Iteration 47/1000 | Loss: 0.00001451
Iteration 48/1000 | Loss: 0.00001451
Iteration 49/1000 | Loss: 0.00001487
Iteration 50/1000 | Loss: 0.00001453
Iteration 51/1000 | Loss: 0.00001453
Iteration 52/1000 | Loss: 0.00001452
Iteration 53/1000 | Loss: 0.00001451
Iteration 54/1000 | Loss: 0.00001450
Iteration 55/1000 | Loss: 0.00001478
Iteration 56/1000 | Loss: 0.00001453
Iteration 57/1000 | Loss: 0.00001453
Iteration 58/1000 | Loss: 0.00001452
Iteration 59/1000 | Loss: 0.00001478
Iteration 60/1000 | Loss: 0.00001453
Iteration 61/1000 | Loss: 0.00001469
Iteration 62/1000 | Loss: 0.00001449
Iteration 63/1000 | Loss: 0.00001449
Iteration 64/1000 | Loss: 0.00001452
Iteration 65/1000 | Loss: 0.00001452
Iteration 66/1000 | Loss: 0.00001452
Iteration 67/1000 | Loss: 0.00001452
Iteration 68/1000 | Loss: 0.00001451
Iteration 69/1000 | Loss: 0.00001451
Iteration 70/1000 | Loss: 0.00001451
Iteration 71/1000 | Loss: 0.00001451
Iteration 72/1000 | Loss: 0.00001451
Iteration 73/1000 | Loss: 0.00001451
Iteration 74/1000 | Loss: 0.00001451
Iteration 75/1000 | Loss: 0.00001450
Iteration 76/1000 | Loss: 0.00001450
Iteration 77/1000 | Loss: 0.00001446
Iteration 78/1000 | Loss: 0.00001446
Iteration 79/1000 | Loss: 0.00001446
Iteration 80/1000 | Loss: 0.00001446
Iteration 81/1000 | Loss: 0.00001446
Iteration 82/1000 | Loss: 0.00001446
Iteration 83/1000 | Loss: 0.00001446
Iteration 84/1000 | Loss: 0.00001446
Iteration 85/1000 | Loss: 0.00001446
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00001446
Iteration 91/1000 | Loss: 0.00001446
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.4457568795478437e-05, 1.4457568795478437e-05, 1.4457568795478437e-05, 1.4457568795478437e-05, 1.4457568795478437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4457568795478437e-05

Optimization complete. Final v2v error: 3.169799566268921 mm

Highest mean error: 10.086424827575684 mm for frame 181

Lowest mean error: 2.744809865951538 mm for frame 157

Saving results

Total time: 55.314924240112305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013745
Iteration 2/25 | Loss: 0.01013745
Iteration 3/25 | Loss: 0.01013745
Iteration 4/25 | Loss: 0.01013745
Iteration 5/25 | Loss: 0.00271711
Iteration 6/25 | Loss: 0.00195083
Iteration 7/25 | Loss: 0.00167220
Iteration 8/25 | Loss: 0.00148580
Iteration 9/25 | Loss: 0.00129069
Iteration 10/25 | Loss: 0.00126082
Iteration 11/25 | Loss: 0.00122289
Iteration 12/25 | Loss: 0.00120414
Iteration 13/25 | Loss: 0.00118587
Iteration 14/25 | Loss: 0.00118938
Iteration 15/25 | Loss: 0.00118446
Iteration 16/25 | Loss: 0.00117545
Iteration 17/25 | Loss: 0.00117393
Iteration 18/25 | Loss: 0.00117333
Iteration 19/25 | Loss: 0.00117981
Iteration 20/25 | Loss: 0.00117238
Iteration 21/25 | Loss: 0.00117186
Iteration 22/25 | Loss: 0.00117153
Iteration 23/25 | Loss: 0.00117143
Iteration 24/25 | Loss: 0.00117395
Iteration 25/25 | Loss: 0.00117131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32786465
Iteration 2/25 | Loss: 0.00253441
Iteration 3/25 | Loss: 0.00199819
Iteration 4/25 | Loss: 0.00199819
Iteration 5/25 | Loss: 0.00199819
Iteration 6/25 | Loss: 0.00199819
Iteration 7/25 | Loss: 0.00199819
Iteration 8/25 | Loss: 0.00199819
Iteration 9/25 | Loss: 0.00199819
Iteration 10/25 | Loss: 0.00199819
Iteration 11/25 | Loss: 0.00199819
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0019981900695711374, 0.0019981900695711374, 0.0019981900695711374, 0.0019981900695711374, 0.0019981900695711374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019981900695711374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199819
Iteration 2/1000 | Loss: 0.00069244
Iteration 3/1000 | Loss: 0.00042369
Iteration 4/1000 | Loss: 0.00025606
Iteration 5/1000 | Loss: 0.00105012
Iteration 6/1000 | Loss: 0.00036632
Iteration 7/1000 | Loss: 0.00062124
Iteration 8/1000 | Loss: 0.00065566
Iteration 9/1000 | Loss: 0.00064686
Iteration 10/1000 | Loss: 0.00094543
Iteration 11/1000 | Loss: 0.00088328
Iteration 12/1000 | Loss: 0.00066889
Iteration 13/1000 | Loss: 0.00074496
Iteration 14/1000 | Loss: 0.00019193
Iteration 15/1000 | Loss: 0.00043824
Iteration 16/1000 | Loss: 0.00032042
Iteration 17/1000 | Loss: 0.00017887
Iteration 18/1000 | Loss: 0.00035071
Iteration 19/1000 | Loss: 0.00009839
Iteration 20/1000 | Loss: 0.00022992
Iteration 21/1000 | Loss: 0.00008912
Iteration 22/1000 | Loss: 0.00011687
Iteration 23/1000 | Loss: 0.00060326
Iteration 24/1000 | Loss: 0.00052813
Iteration 25/1000 | Loss: 0.00019465
Iteration 26/1000 | Loss: 0.00007469
Iteration 27/1000 | Loss: 0.00007280
Iteration 28/1000 | Loss: 0.00064183
Iteration 29/1000 | Loss: 0.00041372
Iteration 30/1000 | Loss: 0.00091005
Iteration 31/1000 | Loss: 0.00008123
Iteration 32/1000 | Loss: 0.00033172
Iteration 33/1000 | Loss: 0.00010118
Iteration 34/1000 | Loss: 0.00013711
Iteration 35/1000 | Loss: 0.00073384
Iteration 36/1000 | Loss: 0.00021992
Iteration 37/1000 | Loss: 0.00066076
Iteration 38/1000 | Loss: 0.00163755
Iteration 39/1000 | Loss: 0.00082737
Iteration 40/1000 | Loss: 0.00057579
Iteration 41/1000 | Loss: 0.00136678
Iteration 42/1000 | Loss: 0.00151444
Iteration 43/1000 | Loss: 0.00011160
Iteration 44/1000 | Loss: 0.00011475
Iteration 45/1000 | Loss: 0.00007291
Iteration 46/1000 | Loss: 0.00006450
Iteration 47/1000 | Loss: 0.00034958
Iteration 48/1000 | Loss: 0.00041304
Iteration 49/1000 | Loss: 0.00008663
Iteration 50/1000 | Loss: 0.00008864
Iteration 51/1000 | Loss: 0.00020824
Iteration 52/1000 | Loss: 0.00020208
Iteration 53/1000 | Loss: 0.00020367
Iteration 54/1000 | Loss: 0.00065919
Iteration 55/1000 | Loss: 0.00012830
Iteration 56/1000 | Loss: 0.00016693
Iteration 57/1000 | Loss: 0.00016620
Iteration 58/1000 | Loss: 0.00073760
Iteration 59/1000 | Loss: 0.00007096
Iteration 60/1000 | Loss: 0.00043927
Iteration 61/1000 | Loss: 0.00068092
Iteration 62/1000 | Loss: 0.00044678
Iteration 63/1000 | Loss: 0.00023705
Iteration 64/1000 | Loss: 0.00107402
Iteration 65/1000 | Loss: 0.00023920
Iteration 66/1000 | Loss: 0.00016364
Iteration 67/1000 | Loss: 0.00005562
Iteration 68/1000 | Loss: 0.00039744
Iteration 69/1000 | Loss: 0.00005839
Iteration 70/1000 | Loss: 0.00005512
Iteration 71/1000 | Loss: 0.00005402
Iteration 72/1000 | Loss: 0.00005244
Iteration 73/1000 | Loss: 0.00020142
Iteration 74/1000 | Loss: 0.00027098
Iteration 75/1000 | Loss: 0.00018917
Iteration 76/1000 | Loss: 0.00020304
Iteration 77/1000 | Loss: 0.00071015
Iteration 78/1000 | Loss: 0.00006349
Iteration 79/1000 | Loss: 0.00012124
Iteration 80/1000 | Loss: 0.00060351
Iteration 81/1000 | Loss: 0.00033536
Iteration 82/1000 | Loss: 0.00007835
Iteration 83/1000 | Loss: 0.00005190
Iteration 84/1000 | Loss: 0.00010900
Iteration 85/1000 | Loss: 0.00019999
Iteration 86/1000 | Loss: 0.00019229
Iteration 87/1000 | Loss: 0.00037957
Iteration 88/1000 | Loss: 0.00021924
Iteration 89/1000 | Loss: 0.00020336
Iteration 90/1000 | Loss: 0.00016090
Iteration 91/1000 | Loss: 0.00018857
Iteration 92/1000 | Loss: 0.00022877
Iteration 93/1000 | Loss: 0.00022179
Iteration 94/1000 | Loss: 0.00006265
Iteration 95/1000 | Loss: 0.00028547
Iteration 96/1000 | Loss: 0.00005587
Iteration 97/1000 | Loss: 0.00013309
Iteration 98/1000 | Loss: 0.00005281
Iteration 99/1000 | Loss: 0.00024086
Iteration 100/1000 | Loss: 0.00006349
Iteration 101/1000 | Loss: 0.00011190
Iteration 102/1000 | Loss: 0.00013976
Iteration 103/1000 | Loss: 0.00005961
Iteration 104/1000 | Loss: 0.00004939
Iteration 105/1000 | Loss: 0.00004905
Iteration 106/1000 | Loss: 0.00020183
Iteration 107/1000 | Loss: 0.00004920
Iteration 108/1000 | Loss: 0.00004864
Iteration 109/1000 | Loss: 0.00043656
Iteration 110/1000 | Loss: 0.00039511
Iteration 111/1000 | Loss: 0.00070854
Iteration 112/1000 | Loss: 0.00105444
Iteration 113/1000 | Loss: 0.00007319
Iteration 114/1000 | Loss: 0.00005384
Iteration 115/1000 | Loss: 0.00060183
Iteration 116/1000 | Loss: 0.00004848
Iteration 117/1000 | Loss: 0.00004746
Iteration 118/1000 | Loss: 0.00041050
Iteration 119/1000 | Loss: 0.00010887
Iteration 120/1000 | Loss: 0.00007787
Iteration 121/1000 | Loss: 0.00004920
Iteration 122/1000 | Loss: 0.00004692
Iteration 123/1000 | Loss: 0.00031310
Iteration 124/1000 | Loss: 0.00004524
Iteration 125/1000 | Loss: 0.00014760
Iteration 126/1000 | Loss: 0.00004420
Iteration 127/1000 | Loss: 0.00069114
Iteration 128/1000 | Loss: 0.00005151
Iteration 129/1000 | Loss: 0.00004441
Iteration 130/1000 | Loss: 0.00004261
Iteration 131/1000 | Loss: 0.00007124
Iteration 132/1000 | Loss: 0.00004029
Iteration 133/1000 | Loss: 0.00006412
Iteration 134/1000 | Loss: 0.00003912
Iteration 135/1000 | Loss: 0.00003878
Iteration 136/1000 | Loss: 0.00003876
Iteration 137/1000 | Loss: 0.00003852
Iteration 138/1000 | Loss: 0.00003851
Iteration 139/1000 | Loss: 0.00003832
Iteration 140/1000 | Loss: 0.00003829
Iteration 141/1000 | Loss: 0.00003826
Iteration 142/1000 | Loss: 0.00003814
Iteration 143/1000 | Loss: 0.00003809
Iteration 144/1000 | Loss: 0.00003801
Iteration 145/1000 | Loss: 0.00003798
Iteration 146/1000 | Loss: 0.00003796
Iteration 147/1000 | Loss: 0.00003795
Iteration 148/1000 | Loss: 0.00003795
Iteration 149/1000 | Loss: 0.00034528
Iteration 150/1000 | Loss: 0.00072236
Iteration 151/1000 | Loss: 0.00043680
Iteration 152/1000 | Loss: 0.00026443
Iteration 153/1000 | Loss: 0.00018181
Iteration 154/1000 | Loss: 0.00009820
Iteration 155/1000 | Loss: 0.00004908
Iteration 156/1000 | Loss: 0.00008549
Iteration 157/1000 | Loss: 0.00003843
Iteration 158/1000 | Loss: 0.00021117
Iteration 159/1000 | Loss: 0.00004650
Iteration 160/1000 | Loss: 0.00007749
Iteration 161/1000 | Loss: 0.00003072
Iteration 162/1000 | Loss: 0.00007505
Iteration 163/1000 | Loss: 0.00007524
Iteration 164/1000 | Loss: 0.00003411
Iteration 165/1000 | Loss: 0.00003078
Iteration 166/1000 | Loss: 0.00002896
Iteration 167/1000 | Loss: 0.00005132
Iteration 168/1000 | Loss: 0.00002830
Iteration 169/1000 | Loss: 0.00002952
Iteration 170/1000 | Loss: 0.00002782
Iteration 171/1000 | Loss: 0.00002756
Iteration 172/1000 | Loss: 0.00002755
Iteration 173/1000 | Loss: 0.00002744
Iteration 174/1000 | Loss: 0.00002743
Iteration 175/1000 | Loss: 0.00002743
Iteration 176/1000 | Loss: 0.00002743
Iteration 177/1000 | Loss: 0.00002743
Iteration 178/1000 | Loss: 0.00002743
Iteration 179/1000 | Loss: 0.00002743
Iteration 180/1000 | Loss: 0.00002743
Iteration 181/1000 | Loss: 0.00002743
Iteration 182/1000 | Loss: 0.00002743
Iteration 183/1000 | Loss: 0.00002743
Iteration 184/1000 | Loss: 0.00002743
Iteration 185/1000 | Loss: 0.00002743
Iteration 186/1000 | Loss: 0.00002742
Iteration 187/1000 | Loss: 0.00002742
Iteration 188/1000 | Loss: 0.00002742
Iteration 189/1000 | Loss: 0.00002742
Iteration 190/1000 | Loss: 0.00002737
Iteration 191/1000 | Loss: 0.00002734
Iteration 192/1000 | Loss: 0.00002734
Iteration 193/1000 | Loss: 0.00002733
Iteration 194/1000 | Loss: 0.00002733
Iteration 195/1000 | Loss: 0.00002733
Iteration 196/1000 | Loss: 0.00002732
Iteration 197/1000 | Loss: 0.00002732
Iteration 198/1000 | Loss: 0.00002732
Iteration 199/1000 | Loss: 0.00002731
Iteration 200/1000 | Loss: 0.00002731
Iteration 201/1000 | Loss: 0.00002731
Iteration 202/1000 | Loss: 0.00002731
Iteration 203/1000 | Loss: 0.00007150
Iteration 204/1000 | Loss: 0.00002761
Iteration 205/1000 | Loss: 0.00002733
Iteration 206/1000 | Loss: 0.00002731
Iteration 207/1000 | Loss: 0.00002727
Iteration 208/1000 | Loss: 0.00002727
Iteration 209/1000 | Loss: 0.00002727
Iteration 210/1000 | Loss: 0.00002727
Iteration 211/1000 | Loss: 0.00002726
Iteration 212/1000 | Loss: 0.00002726
Iteration 213/1000 | Loss: 0.00002726
Iteration 214/1000 | Loss: 0.00002726
Iteration 215/1000 | Loss: 0.00002726
Iteration 216/1000 | Loss: 0.00002726
Iteration 217/1000 | Loss: 0.00002726
Iteration 218/1000 | Loss: 0.00002725
Iteration 219/1000 | Loss: 0.00002725
Iteration 220/1000 | Loss: 0.00002725
Iteration 221/1000 | Loss: 0.00002725
Iteration 222/1000 | Loss: 0.00002725
Iteration 223/1000 | Loss: 0.00002725
Iteration 224/1000 | Loss: 0.00002725
Iteration 225/1000 | Loss: 0.00002725
Iteration 226/1000 | Loss: 0.00002724
Iteration 227/1000 | Loss: 0.00002724
Iteration 228/1000 | Loss: 0.00002724
Iteration 229/1000 | Loss: 0.00002724
Iteration 230/1000 | Loss: 0.00002724
Iteration 231/1000 | Loss: 0.00002724
Iteration 232/1000 | Loss: 0.00002724
Iteration 233/1000 | Loss: 0.00002723
Iteration 234/1000 | Loss: 0.00002723
Iteration 235/1000 | Loss: 0.00002723
Iteration 236/1000 | Loss: 0.00002723
Iteration 237/1000 | Loss: 0.00002723
Iteration 238/1000 | Loss: 0.00002723
Iteration 239/1000 | Loss: 0.00002723
Iteration 240/1000 | Loss: 0.00002723
Iteration 241/1000 | Loss: 0.00002723
Iteration 242/1000 | Loss: 0.00002723
Iteration 243/1000 | Loss: 0.00002723
Iteration 244/1000 | Loss: 0.00002723
Iteration 245/1000 | Loss: 0.00002723
Iteration 246/1000 | Loss: 0.00002723
Iteration 247/1000 | Loss: 0.00002723
Iteration 248/1000 | Loss: 0.00002723
Iteration 249/1000 | Loss: 0.00002723
Iteration 250/1000 | Loss: 0.00002723
Iteration 251/1000 | Loss: 0.00002723
Iteration 252/1000 | Loss: 0.00002722
Iteration 253/1000 | Loss: 0.00002722
Iteration 254/1000 | Loss: 0.00002722
Iteration 255/1000 | Loss: 0.00002722
Iteration 256/1000 | Loss: 0.00002722
Iteration 257/1000 | Loss: 0.00002722
Iteration 258/1000 | Loss: 0.00002722
Iteration 259/1000 | Loss: 0.00002722
Iteration 260/1000 | Loss: 0.00002722
Iteration 261/1000 | Loss: 0.00002722
Iteration 262/1000 | Loss: 0.00002722
Iteration 263/1000 | Loss: 0.00002722
Iteration 264/1000 | Loss: 0.00002722
Iteration 265/1000 | Loss: 0.00002722
Iteration 266/1000 | Loss: 0.00002722
Iteration 267/1000 | Loss: 0.00002722
Iteration 268/1000 | Loss: 0.00002722
Iteration 269/1000 | Loss: 0.00002722
Iteration 270/1000 | Loss: 0.00002722
Iteration 271/1000 | Loss: 0.00002722
Iteration 272/1000 | Loss: 0.00002722
Iteration 273/1000 | Loss: 0.00002722
Iteration 274/1000 | Loss: 0.00002722
Iteration 275/1000 | Loss: 0.00002722
Iteration 276/1000 | Loss: 0.00002722
Iteration 277/1000 | Loss: 0.00002722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [2.7223610231885687e-05, 2.7223610231885687e-05, 2.7223610231885687e-05, 2.7223610231885687e-05, 2.7223610231885687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7223610231885687e-05

Optimization complete. Final v2v error: 3.5245285034179688 mm

Highest mean error: 12.971166610717773 mm for frame 126

Lowest mean error: 2.769961357116699 mm for frame 234

Saving results

Total time: 320.8822386264801
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01095511
Iteration 2/25 | Loss: 0.01095511
Iteration 3/25 | Loss: 0.01095511
Iteration 4/25 | Loss: 0.01095511
Iteration 5/25 | Loss: 0.01095511
Iteration 6/25 | Loss: 0.01095511
Iteration 7/25 | Loss: 0.01095510
Iteration 8/25 | Loss: 0.01095510
Iteration 9/25 | Loss: 0.01095510
Iteration 10/25 | Loss: 0.01095510
Iteration 11/25 | Loss: 0.01095510
Iteration 12/25 | Loss: 0.01095510
Iteration 13/25 | Loss: 0.01095510
Iteration 14/25 | Loss: 0.01095510
Iteration 15/25 | Loss: 0.01095510
Iteration 16/25 | Loss: 0.01095510
Iteration 17/25 | Loss: 0.01095510
Iteration 18/25 | Loss: 0.01095510
Iteration 19/25 | Loss: 0.01095509
Iteration 20/25 | Loss: 0.01095509
Iteration 21/25 | Loss: 0.01095509
Iteration 22/25 | Loss: 0.01095509
Iteration 23/25 | Loss: 0.01095509
Iteration 24/25 | Loss: 0.01095509
Iteration 25/25 | Loss: 0.01095509

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54980755
Iteration 2/25 | Loss: 0.07045493
Iteration 3/25 | Loss: 0.07044292
Iteration 4/25 | Loss: 0.07044782
Iteration 5/25 | Loss: 0.07044782
Iteration 6/25 | Loss: 0.07041906
Iteration 7/25 | Loss: 0.07041904
Iteration 8/25 | Loss: 0.07041904
Iteration 9/25 | Loss: 0.07041904
Iteration 10/25 | Loss: 0.07041904
Iteration 11/25 | Loss: 0.07041904
Iteration 12/25 | Loss: 0.07041904
Iteration 13/25 | Loss: 0.07041904
Iteration 14/25 | Loss: 0.07041903
Iteration 15/25 | Loss: 0.07041903
Iteration 16/25 | Loss: 0.07041903
Iteration 17/25 | Loss: 0.07041903
Iteration 18/25 | Loss: 0.07041903
Iteration 19/25 | Loss: 0.07041903
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.07041902840137482, 0.07041902840137482, 0.07041902840137482, 0.07041902840137482, 0.07041902840137482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07041902840137482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07041903
Iteration 2/1000 | Loss: 0.00527460
Iteration 3/1000 | Loss: 0.00153303
Iteration 4/1000 | Loss: 0.00069019
Iteration 5/1000 | Loss: 0.00089877
Iteration 6/1000 | Loss: 0.00047829
Iteration 7/1000 | Loss: 0.00032386
Iteration 8/1000 | Loss: 0.00102619
Iteration 9/1000 | Loss: 0.00014447
Iteration 10/1000 | Loss: 0.00036916
Iteration 11/1000 | Loss: 0.00007322
Iteration 12/1000 | Loss: 0.00038327
Iteration 13/1000 | Loss: 0.00025500
Iteration 14/1000 | Loss: 0.00006086
Iteration 15/1000 | Loss: 0.00005622
Iteration 16/1000 | Loss: 0.00005201
Iteration 17/1000 | Loss: 0.00015692
Iteration 18/1000 | Loss: 0.00004639
Iteration 19/1000 | Loss: 0.00019850
Iteration 20/1000 | Loss: 0.00078029
Iteration 21/1000 | Loss: 0.00020791
Iteration 22/1000 | Loss: 0.00005503
Iteration 23/1000 | Loss: 0.00022091
Iteration 24/1000 | Loss: 0.00018281
Iteration 25/1000 | Loss: 0.00005265
Iteration 26/1000 | Loss: 0.00005625
Iteration 27/1000 | Loss: 0.00004477
Iteration 28/1000 | Loss: 0.00004536
Iteration 29/1000 | Loss: 0.00028226
Iteration 30/1000 | Loss: 0.00012745
Iteration 31/1000 | Loss: 0.00020007
Iteration 32/1000 | Loss: 0.00004848
Iteration 33/1000 | Loss: 0.00003380
Iteration 34/1000 | Loss: 0.00007081
Iteration 35/1000 | Loss: 0.00003199
Iteration 36/1000 | Loss: 0.00003058
Iteration 37/1000 | Loss: 0.00002930
Iteration 38/1000 | Loss: 0.00002809
Iteration 39/1000 | Loss: 0.00002704
Iteration 40/1000 | Loss: 0.00002610
Iteration 41/1000 | Loss: 0.00002576
Iteration 42/1000 | Loss: 0.00002484
Iteration 43/1000 | Loss: 0.00006105
Iteration 44/1000 | Loss: 0.00004456
Iteration 45/1000 | Loss: 0.00005944
Iteration 46/1000 | Loss: 0.00003654
Iteration 47/1000 | Loss: 0.00005908
Iteration 48/1000 | Loss: 0.00003529
Iteration 49/1000 | Loss: 0.00007530
Iteration 50/1000 | Loss: 0.00004108
Iteration 51/1000 | Loss: 0.00030869
Iteration 52/1000 | Loss: 0.00015271
Iteration 53/1000 | Loss: 0.00016788
Iteration 54/1000 | Loss: 0.00006893
Iteration 55/1000 | Loss: 0.00005445
Iteration 56/1000 | Loss: 0.00003091
Iteration 57/1000 | Loss: 0.00019543
Iteration 58/1000 | Loss: 0.00008556
Iteration 59/1000 | Loss: 0.00003120
Iteration 60/1000 | Loss: 0.00003385
Iteration 61/1000 | Loss: 0.00002690
Iteration 62/1000 | Loss: 0.00009910
Iteration 63/1000 | Loss: 0.00004560
Iteration 64/1000 | Loss: 0.00017454
Iteration 65/1000 | Loss: 0.00054543
Iteration 66/1000 | Loss: 0.00036953
Iteration 67/1000 | Loss: 0.00024577
Iteration 68/1000 | Loss: 0.00016775
Iteration 69/1000 | Loss: 0.00017691
Iteration 70/1000 | Loss: 0.00012890
Iteration 71/1000 | Loss: 0.00012451
Iteration 72/1000 | Loss: 0.00015060
Iteration 73/1000 | Loss: 0.00014690
Iteration 74/1000 | Loss: 0.00016428
Iteration 75/1000 | Loss: 0.00013427
Iteration 76/1000 | Loss: 0.00016036
Iteration 77/1000 | Loss: 0.00045313
Iteration 78/1000 | Loss: 0.00042638
Iteration 79/1000 | Loss: 0.00025879
Iteration 80/1000 | Loss: 0.00017737
Iteration 81/1000 | Loss: 0.00016288
Iteration 82/1000 | Loss: 0.00013320
Iteration 83/1000 | Loss: 0.00015799
Iteration 84/1000 | Loss: 0.00014684
Iteration 85/1000 | Loss: 0.00011749
Iteration 86/1000 | Loss: 0.00012773
Iteration 87/1000 | Loss: 0.00012729
Iteration 88/1000 | Loss: 0.00012915
Iteration 89/1000 | Loss: 0.00015636
Iteration 90/1000 | Loss: 0.00012881
Iteration 91/1000 | Loss: 0.00018832
Iteration 92/1000 | Loss: 0.00018630
Iteration 93/1000 | Loss: 0.00011055
Iteration 94/1000 | Loss: 0.00014320
Iteration 95/1000 | Loss: 0.00016281
Iteration 96/1000 | Loss: 0.00013886
Iteration 97/1000 | Loss: 0.00014589
Iteration 98/1000 | Loss: 0.00013672
Iteration 99/1000 | Loss: 0.00014032
Iteration 100/1000 | Loss: 0.00015537
Iteration 101/1000 | Loss: 0.00014473
Iteration 102/1000 | Loss: 0.00016248
Iteration 103/1000 | Loss: 0.00014289
Iteration 104/1000 | Loss: 0.00021477
Iteration 105/1000 | Loss: 0.00017988
Iteration 106/1000 | Loss: 0.00017657
Iteration 107/1000 | Loss: 0.00017856
Iteration 108/1000 | Loss: 0.00015917
Iteration 109/1000 | Loss: 0.00014783
Iteration 110/1000 | Loss: 0.00016303
Iteration 111/1000 | Loss: 0.00015483
Iteration 112/1000 | Loss: 0.00015500
Iteration 113/1000 | Loss: 0.00022457
Iteration 114/1000 | Loss: 0.00015513
Iteration 115/1000 | Loss: 0.00017515
Iteration 116/1000 | Loss: 0.00016622
Iteration 117/1000 | Loss: 0.00017362
Iteration 118/1000 | Loss: 0.00016668
Iteration 119/1000 | Loss: 0.00017184
Iteration 120/1000 | Loss: 0.00015822
Iteration 121/1000 | Loss: 0.00021422
Iteration 122/1000 | Loss: 0.00015585
Iteration 123/1000 | Loss: 0.00018540
Iteration 124/1000 | Loss: 0.00015461
Iteration 125/1000 | Loss: 0.00021088
Iteration 126/1000 | Loss: 0.00014905
Iteration 127/1000 | Loss: 0.00020581
Iteration 128/1000 | Loss: 0.00018263
Iteration 129/1000 | Loss: 0.00017587
Iteration 130/1000 | Loss: 0.00016551
Iteration 131/1000 | Loss: 0.00016923
Iteration 132/1000 | Loss: 0.00015741
Iteration 133/1000 | Loss: 0.00015547
Iteration 134/1000 | Loss: 0.00015006
Iteration 135/1000 | Loss: 0.00014909
Iteration 136/1000 | Loss: 0.00018886
Iteration 137/1000 | Loss: 0.00015122
Iteration 138/1000 | Loss: 0.00020922
Iteration 139/1000 | Loss: 0.00015660
Iteration 140/1000 | Loss: 0.00017714
Iteration 141/1000 | Loss: 0.00016540
Iteration 142/1000 | Loss: 0.00016985
Iteration 143/1000 | Loss: 0.00014672
Iteration 144/1000 | Loss: 0.00019019
Iteration 145/1000 | Loss: 0.00014115
Iteration 146/1000 | Loss: 0.00019084
Iteration 147/1000 | Loss: 0.00015082
Iteration 148/1000 | Loss: 0.00021863
Iteration 149/1000 | Loss: 0.00012618
Iteration 150/1000 | Loss: 0.00011811
Iteration 151/1000 | Loss: 0.00014233
Iteration 152/1000 | Loss: 0.00013822
Iteration 153/1000 | Loss: 0.00013573
Iteration 154/1000 | Loss: 0.00014568
Iteration 155/1000 | Loss: 0.00014043
Iteration 156/1000 | Loss: 0.00019674
Iteration 157/1000 | Loss: 0.00017272
Iteration 158/1000 | Loss: 0.00013191
Iteration 159/1000 | Loss: 0.00016647
Iteration 160/1000 | Loss: 0.00014709
Iteration 161/1000 | Loss: 0.00014944
Iteration 162/1000 | Loss: 0.00015286
Iteration 163/1000 | Loss: 0.00014953
Iteration 164/1000 | Loss: 0.00022097
Iteration 165/1000 | Loss: 0.00014425
Iteration 166/1000 | Loss: 0.00015280
Iteration 167/1000 | Loss: 0.00013997
Iteration 168/1000 | Loss: 0.00015765
Iteration 169/1000 | Loss: 0.00016136
Iteration 170/1000 | Loss: 0.00016555
Iteration 171/1000 | Loss: 0.00019254
Iteration 172/1000 | Loss: 0.00015165
Iteration 173/1000 | Loss: 0.00014009
Iteration 174/1000 | Loss: 0.00013843
Iteration 175/1000 | Loss: 0.00014033
Iteration 176/1000 | Loss: 0.00016484
Iteration 177/1000 | Loss: 0.00013658
Iteration 178/1000 | Loss: 0.00019602
Iteration 179/1000 | Loss: 0.00016177
Iteration 180/1000 | Loss: 0.00016350
Iteration 181/1000 | Loss: 0.00014207
Iteration 182/1000 | Loss: 0.00014759
Iteration 183/1000 | Loss: 0.00015108
Iteration 184/1000 | Loss: 0.00013926
Iteration 185/1000 | Loss: 0.00017817
Iteration 186/1000 | Loss: 0.00014746
Iteration 187/1000 | Loss: 0.00013840
Iteration 188/1000 | Loss: 0.00013876
Iteration 189/1000 | Loss: 0.00012531
Iteration 190/1000 | Loss: 0.00013145
Iteration 191/1000 | Loss: 0.00012345
Iteration 192/1000 | Loss: 0.00013778
Iteration 193/1000 | Loss: 0.00012839
Iteration 194/1000 | Loss: 0.00016572
Iteration 195/1000 | Loss: 0.00012982
Iteration 196/1000 | Loss: 0.00020067
Iteration 197/1000 | Loss: 0.00016865
Iteration 198/1000 | Loss: 0.00016161
Iteration 199/1000 | Loss: 0.00016737
Iteration 200/1000 | Loss: 0.00014636
Iteration 201/1000 | Loss: 0.00016110
Iteration 202/1000 | Loss: 0.00016648
Iteration 203/1000 | Loss: 0.00014569
Iteration 204/1000 | Loss: 0.00015581
Iteration 205/1000 | Loss: 0.00015724
Iteration 206/1000 | Loss: 0.00014765
Iteration 207/1000 | Loss: 0.00014003
Iteration 208/1000 | Loss: 0.00015845
Iteration 209/1000 | Loss: 0.00015018
Iteration 210/1000 | Loss: 0.00016522
Iteration 211/1000 | Loss: 0.00021275
Iteration 212/1000 | Loss: 0.00018250
Iteration 213/1000 | Loss: 0.00015215
Iteration 214/1000 | Loss: 0.00015920
Iteration 215/1000 | Loss: 0.00015599
Iteration 216/1000 | Loss: 0.00018636
Iteration 217/1000 | Loss: 0.00015560
Iteration 218/1000 | Loss: 0.00013708
Iteration 219/1000 | Loss: 0.00016619
Iteration 220/1000 | Loss: 0.00013317
Iteration 221/1000 | Loss: 0.00015220
Iteration 222/1000 | Loss: 0.00015088
Iteration 223/1000 | Loss: 0.00014290
Iteration 224/1000 | Loss: 0.00013667
Iteration 225/1000 | Loss: 0.00021643
Iteration 226/1000 | Loss: 0.00017247
Iteration 227/1000 | Loss: 0.00015977
Iteration 228/1000 | Loss: 0.00018233
Iteration 229/1000 | Loss: 0.00020039
Iteration 230/1000 | Loss: 0.00016149
Iteration 231/1000 | Loss: 0.00019519
Iteration 232/1000 | Loss: 0.00017987
Iteration 233/1000 | Loss: 0.00013391
Iteration 234/1000 | Loss: 0.00015006
Iteration 235/1000 | Loss: 0.00017079
Iteration 236/1000 | Loss: 0.00015629
Iteration 237/1000 | Loss: 0.00014404
Iteration 238/1000 | Loss: 0.00014619
Iteration 239/1000 | Loss: 0.00014682
Iteration 240/1000 | Loss: 0.00014948
Iteration 241/1000 | Loss: 0.00013181
Iteration 242/1000 | Loss: 0.00013308
Iteration 243/1000 | Loss: 0.00012848
Iteration 244/1000 | Loss: 0.00014882
Iteration 245/1000 | Loss: 0.00013729
Iteration 246/1000 | Loss: 0.00013484
Iteration 247/1000 | Loss: 0.00013554
Iteration 248/1000 | Loss: 0.00016417
Iteration 249/1000 | Loss: 0.00013590
Iteration 250/1000 | Loss: 0.00015748
Iteration 251/1000 | Loss: 0.00013133
Iteration 252/1000 | Loss: 0.00013850
Iteration 253/1000 | Loss: 0.00013534
Iteration 254/1000 | Loss: 0.00013625
Iteration 255/1000 | Loss: 0.00013586
Iteration 256/1000 | Loss: 0.00014130
Iteration 257/1000 | Loss: 0.00014698
Iteration 258/1000 | Loss: 0.00013697
Iteration 259/1000 | Loss: 0.00016832
Iteration 260/1000 | Loss: 0.00018378
Iteration 261/1000 | Loss: 0.00014219
Iteration 262/1000 | Loss: 0.00013837
Iteration 263/1000 | Loss: 0.00019248
Iteration 264/1000 | Loss: 0.00016049
Iteration 265/1000 | Loss: 0.00014077
Iteration 266/1000 | Loss: 0.00013818
Iteration 267/1000 | Loss: 0.00016941
Iteration 268/1000 | Loss: 0.00016152
Iteration 269/1000 | Loss: 0.00013553
Iteration 270/1000 | Loss: 0.00015195
Iteration 271/1000 | Loss: 0.00012857
Iteration 272/1000 | Loss: 0.00019125
Iteration 273/1000 | Loss: 0.00017745
Iteration 274/1000 | Loss: 0.00014889
Iteration 275/1000 | Loss: 0.00015037
Iteration 276/1000 | Loss: 0.00014441
Iteration 277/1000 | Loss: 0.00013722
Iteration 278/1000 | Loss: 0.00015274
Iteration 279/1000 | Loss: 0.00017082
Iteration 280/1000 | Loss: 0.00016265
Iteration 281/1000 | Loss: 0.00017609
Iteration 282/1000 | Loss: 0.00014915
Iteration 283/1000 | Loss: 0.00022775
Iteration 284/1000 | Loss: 0.00015387
Iteration 285/1000 | Loss: 0.00014391
Iteration 286/1000 | Loss: 0.00016982
Iteration 287/1000 | Loss: 0.00013918
Iteration 288/1000 | Loss: 0.00014142
Iteration 289/1000 | Loss: 0.00018838
Iteration 290/1000 | Loss: 0.00017913
Iteration 291/1000 | Loss: 0.00015049
Iteration 292/1000 | Loss: 0.00014946
Iteration 293/1000 | Loss: 0.00013400
Iteration 294/1000 | Loss: 0.00012688
Iteration 295/1000 | Loss: 0.00017056
Iteration 296/1000 | Loss: 0.00013287
Iteration 297/1000 | Loss: 0.00013259
Iteration 298/1000 | Loss: 0.00013422
Iteration 299/1000 | Loss: 0.00019707
Iteration 300/1000 | Loss: 0.00016207
Iteration 301/1000 | Loss: 0.00013829
Iteration 302/1000 | Loss: 0.00017978
Iteration 303/1000 | Loss: 0.00014751
Iteration 304/1000 | Loss: 0.00018555
Iteration 305/1000 | Loss: 0.00014772
Iteration 306/1000 | Loss: 0.00014250
Iteration 307/1000 | Loss: 0.00014271
Iteration 308/1000 | Loss: 0.00014120
Iteration 309/1000 | Loss: 0.00014394
Iteration 310/1000 | Loss: 0.00027588
Iteration 311/1000 | Loss: 0.00018346
Iteration 312/1000 | Loss: 0.00019197
Iteration 313/1000 | Loss: 0.00014677
Iteration 314/1000 | Loss: 0.00014719
Iteration 315/1000 | Loss: 0.00014179
Iteration 316/1000 | Loss: 0.00015609
Iteration 317/1000 | Loss: 0.00014706
Iteration 318/1000 | Loss: 0.00020105
Iteration 319/1000 | Loss: 0.00016683
Iteration 320/1000 | Loss: 0.00014585
Iteration 321/1000 | Loss: 0.00014786
Iteration 322/1000 | Loss: 0.00014829
Iteration 323/1000 | Loss: 0.00013986
Iteration 324/1000 | Loss: 0.00014605
Iteration 325/1000 | Loss: 0.00021376
Iteration 326/1000 | Loss: 0.00016410
Iteration 327/1000 | Loss: 0.00017955
Iteration 328/1000 | Loss: 0.00017403
Iteration 329/1000 | Loss: 0.00013646
Iteration 330/1000 | Loss: 0.00014271
Iteration 331/1000 | Loss: 0.00013925
Iteration 332/1000 | Loss: 0.00015820
Iteration 333/1000 | Loss: 0.00016548
Iteration 334/1000 | Loss: 0.00014019
Iteration 335/1000 | Loss: 0.00014210
Iteration 336/1000 | Loss: 0.00013764
Iteration 337/1000 | Loss: 0.00014456
Iteration 338/1000 | Loss: 0.00016250
Iteration 339/1000 | Loss: 0.00016771
Iteration 340/1000 | Loss: 0.00016290
Iteration 341/1000 | Loss: 0.00015920
Iteration 342/1000 | Loss: 0.00015409
Iteration 343/1000 | Loss: 0.00018111
Iteration 344/1000 | Loss: 0.00015149
Iteration 345/1000 | Loss: 0.00015619
Iteration 346/1000 | Loss: 0.00016063
Iteration 347/1000 | Loss: 0.00017171
Iteration 348/1000 | Loss: 0.00018236
Iteration 349/1000 | Loss: 0.00016829
Iteration 350/1000 | Loss: 0.00015636
Iteration 351/1000 | Loss: 0.00016778
Iteration 352/1000 | Loss: 0.00018068
Iteration 353/1000 | Loss: 0.00012499
Iteration 354/1000 | Loss: 0.00014889
Iteration 355/1000 | Loss: 0.00011567
Iteration 356/1000 | Loss: 0.00013584
Iteration 357/1000 | Loss: 0.00012068
Iteration 358/1000 | Loss: 0.00015396
Iteration 359/1000 | Loss: 0.00012783
Iteration 360/1000 | Loss: 0.00014463
Iteration 361/1000 | Loss: 0.00014187
Iteration 362/1000 | Loss: 0.00010828
Iteration 363/1000 | Loss: 0.00011093
Iteration 364/1000 | Loss: 0.00016568
Iteration 365/1000 | Loss: 0.00014489
Iteration 366/1000 | Loss: 0.00013556
Iteration 367/1000 | Loss: 0.00013087
Iteration 368/1000 | Loss: 0.00014920
Iteration 369/1000 | Loss: 0.00011925
Iteration 370/1000 | Loss: 0.00014120
Iteration 371/1000 | Loss: 0.00018061
Iteration 372/1000 | Loss: 0.00014340
Iteration 373/1000 | Loss: 0.00015067
Iteration 374/1000 | Loss: 0.00013341
Iteration 375/1000 | Loss: 0.00017880
Iteration 376/1000 | Loss: 0.00012823
Iteration 377/1000 | Loss: 0.00018082
Iteration 378/1000 | Loss: 0.00012925
Iteration 379/1000 | Loss: 0.00018718
Iteration 380/1000 | Loss: 0.00014128
Iteration 381/1000 | Loss: 0.00019166
Iteration 382/1000 | Loss: 0.00012799
Iteration 383/1000 | Loss: 0.00015911
Iteration 384/1000 | Loss: 0.00013403
Iteration 385/1000 | Loss: 0.00019761
Iteration 386/1000 | Loss: 0.00015028
Iteration 387/1000 | Loss: 0.00016245
Iteration 388/1000 | Loss: 0.00018352
Iteration 389/1000 | Loss: 0.00019078
Iteration 390/1000 | Loss: 0.00017112
Iteration 391/1000 | Loss: 0.00019152
Iteration 392/1000 | Loss: 0.00016076
Iteration 393/1000 | Loss: 0.00017831
Iteration 394/1000 | Loss: 0.00014343
Iteration 395/1000 | Loss: 0.00012023
Iteration 396/1000 | Loss: 0.00016041
Iteration 397/1000 | Loss: 0.00016757
Iteration 398/1000 | Loss: 0.00018643
Iteration 399/1000 | Loss: 0.00025635
Iteration 400/1000 | Loss: 0.00021967
Iteration 401/1000 | Loss: 0.00016581
Iteration 402/1000 | Loss: 0.00014884
Iteration 403/1000 | Loss: 0.00016701
Iteration 404/1000 | Loss: 0.00016074
Iteration 405/1000 | Loss: 0.00014183
Iteration 406/1000 | Loss: 0.00018583
Iteration 407/1000 | Loss: 0.00014902
Iteration 408/1000 | Loss: 0.00013775
Iteration 409/1000 | Loss: 0.00012223
Iteration 410/1000 | Loss: 0.00014498
Iteration 411/1000 | Loss: 0.00020722
Iteration 412/1000 | Loss: 0.00017142
Iteration 413/1000 | Loss: 0.00019207
Iteration 414/1000 | Loss: 0.00016207
Iteration 415/1000 | Loss: 0.00015893
Iteration 416/1000 | Loss: 0.00015884
Iteration 417/1000 | Loss: 0.00015484
Iteration 418/1000 | Loss: 0.00014292
Iteration 419/1000 | Loss: 0.00014629
Iteration 420/1000 | Loss: 0.00012798
Iteration 421/1000 | Loss: 0.00014885
Iteration 422/1000 | Loss: 0.00013122
Iteration 423/1000 | Loss: 0.00016970
Iteration 424/1000 | Loss: 0.00019367
Iteration 425/1000 | Loss: 0.00016414
Iteration 426/1000 | Loss: 0.00015422
Iteration 427/1000 | Loss: 0.00018620
Iteration 428/1000 | Loss: 0.00017099
Iteration 429/1000 | Loss: 0.00013740
Iteration 430/1000 | Loss: 0.00013594
Iteration 431/1000 | Loss: 0.00013963
Iteration 432/1000 | Loss: 0.00015247
Iteration 433/1000 | Loss: 0.00013765
Iteration 434/1000 | Loss: 0.00014460
Iteration 435/1000 | Loss: 0.00019229
Iteration 436/1000 | Loss: 0.00016822
Iteration 437/1000 | Loss: 0.00012360
Iteration 438/1000 | Loss: 0.00015577
Iteration 439/1000 | Loss: 0.00013534
Iteration 440/1000 | Loss: 0.00015950
Iteration 441/1000 | Loss: 0.00012291
Iteration 442/1000 | Loss: 0.00017174
Iteration 443/1000 | Loss: 0.00012285
Iteration 444/1000 | Loss: 0.00011733
Iteration 445/1000 | Loss: 0.00012466
Iteration 446/1000 | Loss: 0.00013277
Iteration 447/1000 | Loss: 0.00011283
Iteration 448/1000 | Loss: 0.00014720
Iteration 449/1000 | Loss: 0.00012596
Iteration 450/1000 | Loss: 0.00014799
Iteration 451/1000 | Loss: 0.00013663
Iteration 452/1000 | Loss: 0.00011580
Iteration 453/1000 | Loss: 0.00012429
Iteration 454/1000 | Loss: 0.00011725
Iteration 455/1000 | Loss: 0.00012404
Iteration 456/1000 | Loss: 0.00012706
Iteration 457/1000 | Loss: 0.00012269
Iteration 458/1000 | Loss: 0.00013885
Iteration 459/1000 | Loss: 0.00013584
Iteration 460/1000 | Loss: 0.00014745
Iteration 461/1000 | Loss: 0.00014534
Iteration 462/1000 | Loss: 0.00015183
Iteration 463/1000 | Loss: 0.00024598
Iteration 464/1000 | Loss: 0.00017468
Iteration 465/1000 | Loss: 0.00013033
Iteration 466/1000 | Loss: 0.00015142
Iteration 467/1000 | Loss: 0.00014507
Iteration 468/1000 | Loss: 0.00013852
Iteration 469/1000 | Loss: 0.00012439
Iteration 470/1000 | Loss: 0.00013197
Iteration 471/1000 | Loss: 0.00015753
Iteration 472/1000 | Loss: 0.00015491
Iteration 473/1000 | Loss: 0.00013339
Iteration 474/1000 | Loss: 0.00014299
Iteration 475/1000 | Loss: 0.00012668
Iteration 476/1000 | Loss: 0.00011091
Iteration 477/1000 | Loss: 0.00014137
Iteration 478/1000 | Loss: 0.00011907
Iteration 479/1000 | Loss: 0.00013392
Iteration 480/1000 | Loss: 0.00012960
Iteration 481/1000 | Loss: 0.00012734
Iteration 482/1000 | Loss: 0.00013264
Iteration 483/1000 | Loss: 0.00016108
Iteration 484/1000 | Loss: 0.00012553
Iteration 485/1000 | Loss: 0.00013247
Iteration 486/1000 | Loss: 0.00011753
Iteration 487/1000 | Loss: 0.00011790
Iteration 488/1000 | Loss: 0.00011261
Iteration 489/1000 | Loss: 0.00011474
Iteration 490/1000 | Loss: 0.00012772
Iteration 491/1000 | Loss: 0.00012898
Iteration 492/1000 | Loss: 0.00010100
Iteration 493/1000 | Loss: 0.00012904
Iteration 494/1000 | Loss: 0.00011572
Iteration 495/1000 | Loss: 0.00012831
Iteration 496/1000 | Loss: 0.00012988
Iteration 497/1000 | Loss: 0.00012439
Iteration 498/1000 | Loss: 0.00012284
Iteration 499/1000 | Loss: 0.00012412
Iteration 500/1000 | Loss: 0.00015725
Iteration 501/1000 | Loss: 0.00013102
Iteration 502/1000 | Loss: 0.00012529
Iteration 503/1000 | Loss: 0.00012092
Iteration 504/1000 | Loss: 0.00014851
Iteration 505/1000 | Loss: 0.00017206
Iteration 506/1000 | Loss: 0.00014357
Iteration 507/1000 | Loss: 0.00018806
Iteration 508/1000 | Loss: 0.00014710
Iteration 509/1000 | Loss: 0.00012837
Iteration 510/1000 | Loss: 0.00011899
Iteration 511/1000 | Loss: 0.00012574
Iteration 512/1000 | Loss: 0.00014638
Iteration 513/1000 | Loss: 0.00011020
Iteration 514/1000 | Loss: 0.00015410
Iteration 515/1000 | Loss: 0.00014263
Iteration 516/1000 | Loss: 0.00013163
Iteration 517/1000 | Loss: 0.00013808
Iteration 518/1000 | Loss: 0.00023592
Iteration 519/1000 | Loss: 0.00016122
Iteration 520/1000 | Loss: 0.00014662
Iteration 521/1000 | Loss: 0.00013550
Iteration 522/1000 | Loss: 0.00013248
Iteration 523/1000 | Loss: 0.00014877
Iteration 524/1000 | Loss: 0.00011748
Iteration 525/1000 | Loss: 0.00012371
Iteration 526/1000 | Loss: 0.00012220
Iteration 527/1000 | Loss: 0.00012204
Iteration 528/1000 | Loss: 0.00011823
Iteration 529/1000 | Loss: 0.00017376
Iteration 530/1000 | Loss: 0.00012831
Iteration 531/1000 | Loss: 0.00016333
Iteration 532/1000 | Loss: 0.00012515
Iteration 533/1000 | Loss: 0.00014041
Iteration 534/1000 | Loss: 0.00013794
Iteration 535/1000 | Loss: 0.00012602
Iteration 536/1000 | Loss: 0.00013435
Iteration 537/1000 | Loss: 0.00015753
Iteration 538/1000 | Loss: 0.00018762
Iteration 539/1000 | Loss: 0.00016642
Iteration 540/1000 | Loss: 0.00019100
Iteration 541/1000 | Loss: 0.00016571
Iteration 542/1000 | Loss: 0.00021174
Iteration 543/1000 | Loss: 0.00022057
Iteration 544/1000 | Loss: 0.00015861
Iteration 545/1000 | Loss: 0.00018555
Iteration 546/1000 | Loss: 0.00017309
Iteration 547/1000 | Loss: 0.00019921
Iteration 548/1000 | Loss: 0.00015029
Iteration 549/1000 | Loss: 0.00018449
Iteration 550/1000 | Loss: 0.00012784
Iteration 551/1000 | Loss: 0.00016037
Iteration 552/1000 | Loss: 0.00012467
Iteration 553/1000 | Loss: 0.00017888
Iteration 554/1000 | Loss: 0.00014256
Iteration 555/1000 | Loss: 0.00014466
Iteration 556/1000 | Loss: 0.00014160
Iteration 557/1000 | Loss: 0.00018478
Iteration 558/1000 | Loss: 0.00013806
Iteration 559/1000 | Loss: 0.00017632
Iteration 560/1000 | Loss: 0.00017743
Iteration 561/1000 | Loss: 0.00016649
Iteration 562/1000 | Loss: 0.00012604
Iteration 563/1000 | Loss: 0.00012829
Iteration 564/1000 | Loss: 0.00012256
Iteration 565/1000 | Loss: 0.00013208
Iteration 566/1000 | Loss: 0.00012206
Iteration 567/1000 | Loss: 0.00013650
Iteration 568/1000 | Loss: 0.00015686
Iteration 569/1000 | Loss: 0.00013827
Iteration 570/1000 | Loss: 0.00012686
Iteration 571/1000 | Loss: 0.00013044
Iteration 572/1000 | Loss: 0.00014898
Iteration 573/1000 | Loss: 0.00012402
Iteration 574/1000 | Loss: 0.00015405
Iteration 575/1000 | Loss: 0.00013104
Iteration 576/1000 | Loss: 0.00015655
Iteration 577/1000 | Loss: 0.00016679
Iteration 578/1000 | Loss: 0.00012936
Iteration 579/1000 | Loss: 0.00012202
Iteration 580/1000 | Loss: 0.00012756
Iteration 581/1000 | Loss: 0.00014288
Iteration 582/1000 | Loss: 0.00013546
Iteration 583/1000 | Loss: 0.00012447
Iteration 584/1000 | Loss: 0.00014586
Iteration 585/1000 | Loss: 0.00012524
Iteration 586/1000 | Loss: 0.00012544
Iteration 587/1000 | Loss: 0.00013744
Iteration 588/1000 | Loss: 0.00013313
Iteration 589/1000 | Loss: 0.00013172
Iteration 590/1000 | Loss: 0.00013027
Iteration 591/1000 | Loss: 0.00013249
Iteration 592/1000 | Loss: 0.00015182
Iteration 593/1000 | Loss: 0.00013336
Iteration 594/1000 | Loss: 0.00012983
Iteration 595/1000 | Loss: 0.00014471
Iteration 596/1000 | Loss: 0.00019804
Iteration 597/1000 | Loss: 0.00014511
Iteration 598/1000 | Loss: 0.00012776
Iteration 599/1000 | Loss: 0.00014930
Iteration 600/1000 | Loss: 0.00012394
Iteration 601/1000 | Loss: 0.00014030
Iteration 602/1000 | Loss: 0.00016555
Iteration 603/1000 | Loss: 0.00016219
Iteration 604/1000 | Loss: 0.00018405
Iteration 605/1000 | Loss: 0.00015236
Iteration 606/1000 | Loss: 0.00017264
Iteration 607/1000 | Loss: 0.00014751
Iteration 608/1000 | Loss: 0.00015472
Iteration 609/1000 | Loss: 0.00014844
Iteration 610/1000 | Loss: 0.00013298
Iteration 611/1000 | Loss: 0.00015353
Iteration 612/1000 | Loss: 0.00013902
Iteration 613/1000 | Loss: 0.00014521
Iteration 614/1000 | Loss: 0.00014740
Iteration 615/1000 | Loss: 0.00015606
Iteration 616/1000 | Loss: 0.00018799
Iteration 617/1000 | Loss: 0.00012196
Iteration 618/1000 | Loss: 0.00012439
Iteration 619/1000 | Loss: 0.00012047
Iteration 620/1000 | Loss: 0.00012162
Iteration 621/1000 | Loss: 0.00009093
Iteration 622/1000 | Loss: 0.00011346
Iteration 623/1000 | Loss: 0.00010224
Iteration 624/1000 | Loss: 0.00009804
Iteration 625/1000 | Loss: 0.00009603
Iteration 626/1000 | Loss: 0.00010461
Iteration 627/1000 | Loss: 0.00009508
Iteration 628/1000 | Loss: 0.00009478
Iteration 629/1000 | Loss: 0.00009868
Iteration 630/1000 | Loss: 0.00010007
Iteration 631/1000 | Loss: 0.00009692
Iteration 632/1000 | Loss: 0.00012384
Iteration 633/1000 | Loss: 0.00010396
Iteration 634/1000 | Loss: 0.00014386
Iteration 635/1000 | Loss: 0.00010060
Iteration 636/1000 | Loss: 0.00015836
Iteration 637/1000 | Loss: 0.00009747
Iteration 638/1000 | Loss: 0.00015933
Iteration 639/1000 | Loss: 0.00010081
Iteration 640/1000 | Loss: 0.00010788
Iteration 641/1000 | Loss: 0.00009824
Iteration 642/1000 | Loss: 0.00009509
Iteration 643/1000 | Loss: 0.00009766
Iteration 644/1000 | Loss: 0.00010238
Iteration 645/1000 | Loss: 0.00010939
Iteration 646/1000 | Loss: 0.00014619
Iteration 647/1000 | Loss: 0.00010598
Iteration 648/1000 | Loss: 0.00010350
Iteration 649/1000 | Loss: 0.00009359
Iteration 650/1000 | Loss: 0.00010365
Iteration 651/1000 | Loss: 0.00009649
Iteration 652/1000 | Loss: 0.00009712
Iteration 653/1000 | Loss: 0.00010233
Iteration 654/1000 | Loss: 0.00010614
Iteration 655/1000 | Loss: 0.00010034
Iteration 656/1000 | Loss: 0.00010988
Iteration 657/1000 | Loss: 0.00009877
Iteration 658/1000 | Loss: 0.00011542
Iteration 659/1000 | Loss: 0.00009568
Iteration 660/1000 | Loss: 0.00012437
Iteration 661/1000 | Loss: 0.00009143
Iteration 662/1000 | Loss: 0.00011174
Iteration 663/1000 | Loss: 0.00010675
Iteration 664/1000 | Loss: 0.00012194
Iteration 665/1000 | Loss: 0.00012412
Iteration 666/1000 | Loss: 0.00011460
Iteration 667/1000 | Loss: 0.00010425
Iteration 668/1000 | Loss: 0.00008473
Iteration 669/1000 | Loss: 0.00014187
Iteration 670/1000 | Loss: 0.00008939
Iteration 671/1000 | Loss: 0.00010203
Iteration 672/1000 | Loss: 0.00009473
Iteration 673/1000 | Loss: 0.00009405
Iteration 674/1000 | Loss: 0.00010594
Iteration 675/1000 | Loss: 0.00010136
Iteration 676/1000 | Loss: 0.00010810
Iteration 677/1000 | Loss: 0.00009876
Iteration 678/1000 | Loss: 0.00010482
Iteration 679/1000 | Loss: 0.00010098
Iteration 680/1000 | Loss: 0.00012942
Iteration 681/1000 | Loss: 0.00010310
Iteration 682/1000 | Loss: 0.00010745
Iteration 683/1000 | Loss: 0.00009865
Iteration 684/1000 | Loss: 0.00010836
Iteration 685/1000 | Loss: 0.00010031
Iteration 686/1000 | Loss: 0.00012275
Iteration 687/1000 | Loss: 0.00010829
Iteration 688/1000 | Loss: 0.00011243
Iteration 689/1000 | Loss: 0.00011534
Iteration 690/1000 | Loss: 0.00010876
Iteration 691/1000 | Loss: 0.00010990
Iteration 692/1000 | Loss: 0.00012275
Iteration 693/1000 | Loss: 0.00010827
Iteration 694/1000 | Loss: 0.00011961
Iteration 695/1000 | Loss: 0.00012508
Iteration 696/1000 | Loss: 0.00009700
Iteration 697/1000 | Loss: 0.00010952
Iteration 698/1000 | Loss: 0.00009626
Iteration 699/1000 | Loss: 0.00010301
Iteration 700/1000 | Loss: 0.00009749
Iteration 701/1000 | Loss: 0.00008949
Iteration 702/1000 | Loss: 0.00008380
Iteration 703/1000 | Loss: 0.00009109
Iteration 704/1000 | Loss: 0.00008409
Iteration 705/1000 | Loss: 0.00009296
Iteration 706/1000 | Loss: 0.00009931
Iteration 707/1000 | Loss: 0.00008023
Iteration 708/1000 | Loss: 0.00008385
Iteration 709/1000 | Loss: 0.00008070
Iteration 710/1000 | Loss: 0.00009641
Iteration 711/1000 | Loss: 0.00009041
Iteration 712/1000 | Loss: 0.00009350
Iteration 713/1000 | Loss: 0.00009258
Iteration 714/1000 | Loss: 0.00009433
Iteration 715/1000 | Loss: 0.00009019
Iteration 716/1000 | Loss: 0.00009768
Iteration 717/1000 | Loss: 0.00009289
Iteration 718/1000 | Loss: 0.00008785
Iteration 719/1000 | Loss: 0.00009301
Iteration 720/1000 | Loss: 0.00008833
Iteration 721/1000 | Loss: 0.00008409
Iteration 722/1000 | Loss: 0.00009901
Iteration 723/1000 | Loss: 0.00006657
Iteration 724/1000 | Loss: 0.00006914
Iteration 725/1000 | Loss: 0.00008446
Iteration 726/1000 | Loss: 0.00007876
Iteration 727/1000 | Loss: 0.00007640
Iteration 728/1000 | Loss: 0.00007291
Iteration 729/1000 | Loss: 0.00007436
Iteration 730/1000 | Loss: 0.00007845
Iteration 731/1000 | Loss: 0.00007409
Iteration 732/1000 | Loss: 0.00007514
Iteration 733/1000 | Loss: 0.00007114
Iteration 734/1000 | Loss: 0.00007382
Iteration 735/1000 | Loss: 0.00006939
Iteration 736/1000 | Loss: 0.00007433
Iteration 737/1000 | Loss: 0.00006871
Iteration 738/1000 | Loss: 0.00007578
Iteration 739/1000 | Loss: 0.00007607
Iteration 740/1000 | Loss: 0.00007715
Iteration 741/1000 | Loss: 0.00007604
Iteration 742/1000 | Loss: 0.00007784
Iteration 743/1000 | Loss: 0.00008246
Iteration 744/1000 | Loss: 0.00006317
Iteration 745/1000 | Loss: 0.00006194
Iteration 746/1000 | Loss: 0.00007769
Iteration 747/1000 | Loss: 0.00006030
Iteration 748/1000 | Loss: 0.00007690
Iteration 749/1000 | Loss: 0.00007719
Iteration 750/1000 | Loss: 0.00006910
Iteration 751/1000 | Loss: 0.00008963
Iteration 752/1000 | Loss: 0.00009342
Iteration 753/1000 | Loss: 0.00007478
Iteration 754/1000 | Loss: 0.00011213
Iteration 755/1000 | Loss: 0.00010845
Iteration 756/1000 | Loss: 0.00006348
Iteration 757/1000 | Loss: 0.00005864
Iteration 758/1000 | Loss: 0.00006482
Iteration 759/1000 | Loss: 0.00005387
Iteration 760/1000 | Loss: 0.00005941
Iteration 761/1000 | Loss: 0.00005471
Iteration 762/1000 | Loss: 0.00005860
Iteration 763/1000 | Loss: 0.00007254
Iteration 764/1000 | Loss: 0.00005986
Iteration 765/1000 | Loss: 0.00005916
Iteration 766/1000 | Loss: 0.00005857
Iteration 767/1000 | Loss: 0.00006916
Iteration 768/1000 | Loss: 0.00006228
Iteration 769/1000 | Loss: 0.00006844
Iteration 770/1000 | Loss: 0.00006338
Iteration 771/1000 | Loss: 0.00007998
Iteration 772/1000 | Loss: 0.00006165
Iteration 773/1000 | Loss: 0.00007209
Iteration 774/1000 | Loss: 0.00006486
Iteration 775/1000 | Loss: 0.00006884
Iteration 776/1000 | Loss: 0.00006831
Iteration 777/1000 | Loss: 0.00005892
Iteration 778/1000 | Loss: 0.00006283
Iteration 779/1000 | Loss: 0.00006579
Iteration 780/1000 | Loss: 0.00006622
Iteration 781/1000 | Loss: 0.00007033
Iteration 782/1000 | Loss: 0.00006563
Iteration 783/1000 | Loss: 0.00006879
Iteration 784/1000 | Loss: 0.00006491
Iteration 785/1000 | Loss: 0.00007355
Iteration 786/1000 | Loss: 0.00006355
Iteration 787/1000 | Loss: 0.00006548
Iteration 788/1000 | Loss: 0.00006200
Iteration 789/1000 | Loss: 0.00006429
Iteration 790/1000 | Loss: 0.00006699
Iteration 791/1000 | Loss: 0.00006773
Iteration 792/1000 | Loss: 0.00008038
Iteration 793/1000 | Loss: 0.00007637
Iteration 794/1000 | Loss: 0.00006797
Iteration 795/1000 | Loss: 0.00007255
Iteration 796/1000 | Loss: 0.00007096
Iteration 797/1000 | Loss: 0.00006349
Iteration 798/1000 | Loss: 0.00006591
Iteration 799/1000 | Loss: 0.00008422
Iteration 800/1000 | Loss: 0.00006842
Iteration 801/1000 | Loss: 0.00006733
Iteration 802/1000 | Loss: 0.00006644
Iteration 803/1000 | Loss: 0.00008713
Iteration 804/1000 | Loss: 0.00008238
Iteration 805/1000 | Loss: 0.00007405
Iteration 806/1000 | Loss: 0.00007127
Iteration 807/1000 | Loss: 0.00006010
Iteration 808/1000 | Loss: 0.00007037
Iteration 809/1000 | Loss: 0.00006845
Iteration 810/1000 | Loss: 0.00006903
Iteration 811/1000 | Loss: 0.00007655
Iteration 812/1000 | Loss: 0.00006112
Iteration 813/1000 | Loss: 0.00007252
Iteration 814/1000 | Loss: 0.00006046
Iteration 815/1000 | Loss: 0.00007597
Iteration 816/1000 | Loss: 0.00006286
Iteration 817/1000 | Loss: 0.00007362
Iteration 818/1000 | Loss: 0.00006887
Iteration 819/1000 | Loss: 0.00007643
Iteration 820/1000 | Loss: 0.00006532
Iteration 821/1000 | Loss: 0.00007211
Iteration 822/1000 | Loss: 0.00006246
Iteration 823/1000 | Loss: 0.00007839
Iteration 824/1000 | Loss: 0.00006178
Iteration 825/1000 | Loss: 0.00006218
Iteration 826/1000 | Loss: 0.00006363
Iteration 827/1000 | Loss: 0.00008229
Iteration 828/1000 | Loss: 0.00006683
Iteration 829/1000 | Loss: 0.00006993
Iteration 830/1000 | Loss: 0.00007591
Iteration 831/1000 | Loss: 0.00007032
Iteration 832/1000 | Loss: 0.00005756
Iteration 833/1000 | Loss: 0.00006620
Iteration 834/1000 | Loss: 0.00006434
Iteration 835/1000 | Loss: 0.00007212
Iteration 836/1000 | Loss: 0.00006538
Iteration 837/1000 | Loss: 0.00006292
Iteration 838/1000 | Loss: 0.00009688
Iteration 839/1000 | Loss: 0.00006238
Iteration 840/1000 | Loss: 0.00006709
Iteration 841/1000 | Loss: 0.00006965
Iteration 842/1000 | Loss: 0.00006580
Iteration 843/1000 | Loss: 0.00006403
Iteration 844/1000 | Loss: 0.00006747
Iteration 845/1000 | Loss: 0.00005395
Iteration 846/1000 | Loss: 0.00006853
Iteration 847/1000 | Loss: 0.00005741
Iteration 848/1000 | Loss: 0.00007429
Iteration 849/1000 | Loss: 0.00006814
Iteration 850/1000 | Loss: 0.00006196
Iteration 851/1000 | Loss: 0.00006710
Iteration 852/1000 | Loss: 0.00006608
Iteration 853/1000 | Loss: 0.00006430
Iteration 854/1000 | Loss: 0.00006223
Iteration 855/1000 | Loss: 0.00006271
Iteration 856/1000 | Loss: 0.00010732
Iteration 857/1000 | Loss: 0.00006520
Iteration 858/1000 | Loss: 0.00006717
Iteration 859/1000 | Loss: 0.00005955
Iteration 860/1000 | Loss: 0.00006489
Iteration 861/1000 | Loss: 0.00006687
Iteration 862/1000 | Loss: 0.00006491
Iteration 863/1000 | Loss: 0.00004878
Iteration 864/1000 | Loss: 0.00006828
Iteration 865/1000 | Loss: 0.00008027
Iteration 866/1000 | Loss: 0.00006106
Iteration 867/1000 | Loss: 0.00006719
Iteration 868/1000 | Loss: 0.00006288
Iteration 869/1000 | Loss: 0.00006218
Iteration 870/1000 | Loss: 0.00006368
Iteration 871/1000 | Loss: 0.00006494
Iteration 872/1000 | Loss: 0.00006344
Iteration 873/1000 | Loss: 0.00006390
Iteration 874/1000 | Loss: 0.00011413
Iteration 875/1000 | Loss: 0.00006528
Iteration 876/1000 | Loss: 0.00009445
Iteration 877/1000 | Loss: 0.00006425
Iteration 878/1000 | Loss: 0.00007758
Iteration 879/1000 | Loss: 0.00006424
Iteration 880/1000 | Loss: 0.00007893
Iteration 881/1000 | Loss: 0.00008463
Iteration 882/1000 | Loss: 0.00007578
Iteration 883/1000 | Loss: 0.00007515
Iteration 884/1000 | Loss: 0.00005549
Iteration 885/1000 | Loss: 0.00008708
Iteration 886/1000 | Loss: 0.00007188
Iteration 887/1000 | Loss: 0.00009714
Iteration 888/1000 | Loss: 0.00007666
Iteration 889/1000 | Loss: 0.00006307
Iteration 890/1000 | Loss: 0.00005583
Iteration 891/1000 | Loss: 0.00008440
Iteration 892/1000 | Loss: 0.00006028
Iteration 893/1000 | Loss: 0.00006732
Iteration 894/1000 | Loss: 0.00006282
Iteration 895/1000 | Loss: 0.00006711
Iteration 896/1000 | Loss: 0.00006453
Iteration 897/1000 | Loss: 0.00006301
Iteration 898/1000 | Loss: 0.00006828
Iteration 899/1000 | Loss: 0.00006366
Iteration 900/1000 | Loss: 0.00006161
Iteration 901/1000 | Loss: 0.00007186
Iteration 902/1000 | Loss: 0.00006427
Iteration 903/1000 | Loss: 0.00007277
Iteration 904/1000 | Loss: 0.00006252
Iteration 905/1000 | Loss: 0.00007148
Iteration 906/1000 | Loss: 0.00006066
Iteration 907/1000 | Loss: 0.00005626
Iteration 908/1000 | Loss: 0.00006413
Iteration 909/1000 | Loss: 0.00006998
Iteration 910/1000 | Loss: 0.00008121
Iteration 911/1000 | Loss: 0.00006595
Iteration 912/1000 | Loss: 0.00006377
Iteration 913/1000 | Loss: 0.00006383
Iteration 914/1000 | Loss: 0.00006859
Iteration 915/1000 | Loss: 0.00005928
Iteration 916/1000 | Loss: 0.00007139
Iteration 917/1000 | Loss: 0.00007718
Iteration 918/1000 | Loss: 0.00006614
Iteration 919/1000 | Loss: 0.00007069
Iteration 920/1000 | Loss: 0.00006667
Iteration 921/1000 | Loss: 0.00007212
Iteration 922/1000 | Loss: 0.00007233
Iteration 923/1000 | Loss: 0.00008129
Iteration 924/1000 | Loss: 0.00006850
Iteration 925/1000 | Loss: 0.00008256
Iteration 926/1000 | Loss: 0.00006030
Iteration 927/1000 | Loss: 0.00009852
Iteration 928/1000 | Loss: 0.00005926
Iteration 929/1000 | Loss: 0.00007653
Iteration 930/1000 | Loss: 0.00007866
Iteration 931/1000 | Loss: 0.00007451
Iteration 932/1000 | Loss: 0.00006661
Iteration 933/1000 | Loss: 0.00007629
Iteration 934/1000 | Loss: 0.00006439
Iteration 935/1000 | Loss: 0.00006903
Iteration 936/1000 | Loss: 0.00006750
Iteration 937/1000 | Loss: 0.00009361
Iteration 938/1000 | Loss: 0.00010048
Iteration 939/1000 | Loss: 0.00005154
Iteration 940/1000 | Loss: 0.00004389
Iteration 941/1000 | Loss: 0.00004739
Iteration 942/1000 | Loss: 0.00006223
Iteration 943/1000 | Loss: 0.00004424
Iteration 944/1000 | Loss: 0.00005665
Iteration 945/1000 | Loss: 0.00004974
Iteration 946/1000 | Loss: 0.00004969
Iteration 947/1000 | Loss: 0.00005104
Iteration 948/1000 | Loss: 0.00006531
Iteration 949/1000 | Loss: 0.00004704
Iteration 950/1000 | Loss: 0.00004708
Iteration 951/1000 | Loss: 0.00004715
Iteration 952/1000 | Loss: 0.00005577
Iteration 953/1000 | Loss: 0.00004695
Iteration 954/1000 | Loss: 0.00004697
Iteration 955/1000 | Loss: 0.00004367
Iteration 956/1000 | Loss: 0.00004549
Iteration 957/1000 | Loss: 0.00004652
Iteration 958/1000 | Loss: 0.00005312
Iteration 959/1000 | Loss: 0.00004677
Iteration 960/1000 | Loss: 0.00004592
Iteration 961/1000 | Loss: 0.00004705
Iteration 962/1000 | Loss: 0.00004505
Iteration 963/1000 | Loss: 0.00006625
Iteration 964/1000 | Loss: 0.00004780
Iteration 965/1000 | Loss: 0.00004555
Iteration 966/1000 | Loss: 0.00004640
Iteration 967/1000 | Loss: 0.00004451
Iteration 968/1000 | Loss: 0.00006358
Iteration 969/1000 | Loss: 0.00006092
Iteration 970/1000 | Loss: 0.00006515
Iteration 971/1000 | Loss: 0.00004578
Iteration 972/1000 | Loss: 0.00004562
Iteration 973/1000 | Loss: 0.00004491
Iteration 974/1000 | Loss: 0.00004875
Iteration 975/1000 | Loss: 0.00004449
Iteration 976/1000 | Loss: 0.00004779
Iteration 977/1000 | Loss: 0.00004722
Iteration 978/1000 | Loss: 0.00005045
Iteration 979/1000 | Loss: 0.00005500
Iteration 980/1000 | Loss: 0.00004725
Iteration 981/1000 | Loss: 0.00004519
Iteration 982/1000 | Loss: 0.00004737
Iteration 983/1000 | Loss: 0.00004633
Iteration 984/1000 | Loss: 0.00005407
Iteration 985/1000 | Loss: 0.00004153
Iteration 986/1000 | Loss: 0.00005232
Iteration 987/1000 | Loss: 0.00004094
Iteration 988/1000 | Loss: 0.00004683
Iteration 989/1000 | Loss: 0.00004151
Iteration 990/1000 | Loss: 0.00004118
Iteration 991/1000 | Loss: 0.00005167
Iteration 992/1000 | Loss: 0.00004185
Iteration 993/1000 | Loss: 0.00004235
Iteration 994/1000 | Loss: 0.00004065
Iteration 995/1000 | Loss: 0.00004846
Iteration 996/1000 | Loss: 0.00004597
Iteration 997/1000 | Loss: 0.00004956
Iteration 998/1000 | Loss: 0.00004307
Iteration 999/1000 | Loss: 0.00004710
Iteration 1000/1000 | Loss: 0.00004547

Optimization complete. Final v2v error: 3.9197514057159424 mm

Highest mean error: 40.038429260253906 mm for frame 230

Lowest mean error: 2.8266384601593018 mm for frame 131

Saving results

Total time: 1619.8081555366516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964867
Iteration 2/25 | Loss: 0.00138659
Iteration 3/25 | Loss: 0.00109466
Iteration 4/25 | Loss: 0.00106829
Iteration 5/25 | Loss: 0.00106030
Iteration 6/25 | Loss: 0.00105956
Iteration 7/25 | Loss: 0.00105956
Iteration 8/25 | Loss: 0.00105956
Iteration 9/25 | Loss: 0.00105956
Iteration 10/25 | Loss: 0.00105956
Iteration 11/25 | Loss: 0.00105956
Iteration 12/25 | Loss: 0.00105956
Iteration 13/25 | Loss: 0.00105956
Iteration 14/25 | Loss: 0.00105956
Iteration 15/25 | Loss: 0.00105956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010595563799142838, 0.0010595563799142838, 0.0010595563799142838, 0.0010595563799142838, 0.0010595563799142838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010595563799142838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.71137112
Iteration 2/25 | Loss: 0.00056441
Iteration 3/25 | Loss: 0.00056441
Iteration 4/25 | Loss: 0.00056441
Iteration 5/25 | Loss: 0.00056441
Iteration 6/25 | Loss: 0.00056441
Iteration 7/25 | Loss: 0.00056441
Iteration 8/25 | Loss: 0.00056441
Iteration 9/25 | Loss: 0.00056441
Iteration 10/25 | Loss: 0.00056441
Iteration 11/25 | Loss: 0.00056441
Iteration 12/25 | Loss: 0.00056441
Iteration 13/25 | Loss: 0.00056441
Iteration 14/25 | Loss: 0.00056441
Iteration 15/25 | Loss: 0.00056441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005644109332934022, 0.0005644109332934022, 0.0005644109332934022, 0.0005644109332934022, 0.0005644109332934022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005644109332934022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056441
Iteration 2/1000 | Loss: 0.00005857
Iteration 3/1000 | Loss: 0.00004398
Iteration 4/1000 | Loss: 0.00003765
Iteration 5/1000 | Loss: 0.00003616
Iteration 6/1000 | Loss: 0.00003537
Iteration 7/1000 | Loss: 0.00003474
Iteration 8/1000 | Loss: 0.00003416
Iteration 9/1000 | Loss: 0.00003380
Iteration 10/1000 | Loss: 0.00003350
Iteration 11/1000 | Loss: 0.00003317
Iteration 12/1000 | Loss: 0.00003263
Iteration 13/1000 | Loss: 0.00003204
Iteration 14/1000 | Loss: 0.00003169
Iteration 15/1000 | Loss: 0.00003145
Iteration 16/1000 | Loss: 0.00003126
Iteration 17/1000 | Loss: 0.00003087
Iteration 18/1000 | Loss: 0.00003064
Iteration 19/1000 | Loss: 0.00003040
Iteration 20/1000 | Loss: 0.00003023
Iteration 21/1000 | Loss: 0.00003021
Iteration 22/1000 | Loss: 0.00003006
Iteration 23/1000 | Loss: 0.00002995
Iteration 24/1000 | Loss: 0.00002991
Iteration 25/1000 | Loss: 0.00002984
Iteration 26/1000 | Loss: 0.00002982
Iteration 27/1000 | Loss: 0.00002982
Iteration 28/1000 | Loss: 0.00002979
Iteration 29/1000 | Loss: 0.00002979
Iteration 30/1000 | Loss: 0.00002979
Iteration 31/1000 | Loss: 0.00002979
Iteration 32/1000 | Loss: 0.00002978
Iteration 33/1000 | Loss: 0.00002978
Iteration 34/1000 | Loss: 0.00002978
Iteration 35/1000 | Loss: 0.00002978
Iteration 36/1000 | Loss: 0.00002978
Iteration 37/1000 | Loss: 0.00002978
Iteration 38/1000 | Loss: 0.00002978
Iteration 39/1000 | Loss: 0.00002978
Iteration 40/1000 | Loss: 0.00002978
Iteration 41/1000 | Loss: 0.00002978
Iteration 42/1000 | Loss: 0.00002978
Iteration 43/1000 | Loss: 0.00002977
Iteration 44/1000 | Loss: 0.00002977
Iteration 45/1000 | Loss: 0.00002976
Iteration 46/1000 | Loss: 0.00002975
Iteration 47/1000 | Loss: 0.00002975
Iteration 48/1000 | Loss: 0.00002975
Iteration 49/1000 | Loss: 0.00002974
Iteration 50/1000 | Loss: 0.00002974
Iteration 51/1000 | Loss: 0.00002974
Iteration 52/1000 | Loss: 0.00002974
Iteration 53/1000 | Loss: 0.00002972
Iteration 54/1000 | Loss: 0.00002972
Iteration 55/1000 | Loss: 0.00002971
Iteration 56/1000 | Loss: 0.00002971
Iteration 57/1000 | Loss: 0.00002971
Iteration 58/1000 | Loss: 0.00002971
Iteration 59/1000 | Loss: 0.00002971
Iteration 60/1000 | Loss: 0.00002971
Iteration 61/1000 | Loss: 0.00002971
Iteration 62/1000 | Loss: 0.00002971
Iteration 63/1000 | Loss: 0.00002971
Iteration 64/1000 | Loss: 0.00002971
Iteration 65/1000 | Loss: 0.00002970
Iteration 66/1000 | Loss: 0.00002969
Iteration 67/1000 | Loss: 0.00002969
Iteration 68/1000 | Loss: 0.00002969
Iteration 69/1000 | Loss: 0.00002969
Iteration 70/1000 | Loss: 0.00002968
Iteration 71/1000 | Loss: 0.00002968
Iteration 72/1000 | Loss: 0.00002968
Iteration 73/1000 | Loss: 0.00002968
Iteration 74/1000 | Loss: 0.00002968
Iteration 75/1000 | Loss: 0.00002968
Iteration 76/1000 | Loss: 0.00002968
Iteration 77/1000 | Loss: 0.00002968
Iteration 78/1000 | Loss: 0.00002968
Iteration 79/1000 | Loss: 0.00002968
Iteration 80/1000 | Loss: 0.00002968
Iteration 81/1000 | Loss: 0.00002968
Iteration 82/1000 | Loss: 0.00002968
Iteration 83/1000 | Loss: 0.00002968
Iteration 84/1000 | Loss: 0.00002968
Iteration 85/1000 | Loss: 0.00002967
Iteration 86/1000 | Loss: 0.00002967
Iteration 87/1000 | Loss: 0.00002967
Iteration 88/1000 | Loss: 0.00002967
Iteration 89/1000 | Loss: 0.00002966
Iteration 90/1000 | Loss: 0.00002966
Iteration 91/1000 | Loss: 0.00002966
Iteration 92/1000 | Loss: 0.00002965
Iteration 93/1000 | Loss: 0.00002962
Iteration 94/1000 | Loss: 0.00002962
Iteration 95/1000 | Loss: 0.00002962
Iteration 96/1000 | Loss: 0.00002961
Iteration 97/1000 | Loss: 0.00002961
Iteration 98/1000 | Loss: 0.00002961
Iteration 99/1000 | Loss: 0.00002961
Iteration 100/1000 | Loss: 0.00002961
Iteration 101/1000 | Loss: 0.00002961
Iteration 102/1000 | Loss: 0.00002961
Iteration 103/1000 | Loss: 0.00002961
Iteration 104/1000 | Loss: 0.00002961
Iteration 105/1000 | Loss: 0.00002961
Iteration 106/1000 | Loss: 0.00002961
Iteration 107/1000 | Loss: 0.00002960
Iteration 108/1000 | Loss: 0.00002960
Iteration 109/1000 | Loss: 0.00002960
Iteration 110/1000 | Loss: 0.00002960
Iteration 111/1000 | Loss: 0.00002960
Iteration 112/1000 | Loss: 0.00002960
Iteration 113/1000 | Loss: 0.00002960
Iteration 114/1000 | Loss: 0.00002959
Iteration 115/1000 | Loss: 0.00002959
Iteration 116/1000 | Loss: 0.00002959
Iteration 117/1000 | Loss: 0.00002959
Iteration 118/1000 | Loss: 0.00002959
Iteration 119/1000 | Loss: 0.00002959
Iteration 120/1000 | Loss: 0.00002959
Iteration 121/1000 | Loss: 0.00002959
Iteration 122/1000 | Loss: 0.00002959
Iteration 123/1000 | Loss: 0.00002958
Iteration 124/1000 | Loss: 0.00002958
Iteration 125/1000 | Loss: 0.00002958
Iteration 126/1000 | Loss: 0.00002958
Iteration 127/1000 | Loss: 0.00002958
Iteration 128/1000 | Loss: 0.00002958
Iteration 129/1000 | Loss: 0.00002958
Iteration 130/1000 | Loss: 0.00002958
Iteration 131/1000 | Loss: 0.00002958
Iteration 132/1000 | Loss: 0.00002958
Iteration 133/1000 | Loss: 0.00002958
Iteration 134/1000 | Loss: 0.00002957
Iteration 135/1000 | Loss: 0.00002957
Iteration 136/1000 | Loss: 0.00002957
Iteration 137/1000 | Loss: 0.00002957
Iteration 138/1000 | Loss: 0.00002957
Iteration 139/1000 | Loss: 0.00002957
Iteration 140/1000 | Loss: 0.00002957
Iteration 141/1000 | Loss: 0.00002957
Iteration 142/1000 | Loss: 0.00002957
Iteration 143/1000 | Loss: 0.00002957
Iteration 144/1000 | Loss: 0.00002957
Iteration 145/1000 | Loss: 0.00002957
Iteration 146/1000 | Loss: 0.00002957
Iteration 147/1000 | Loss: 0.00002957
Iteration 148/1000 | Loss: 0.00002956
Iteration 149/1000 | Loss: 0.00002956
Iteration 150/1000 | Loss: 0.00002956
Iteration 151/1000 | Loss: 0.00002956
Iteration 152/1000 | Loss: 0.00002956
Iteration 153/1000 | Loss: 0.00002956
Iteration 154/1000 | Loss: 0.00002956
Iteration 155/1000 | Loss: 0.00002956
Iteration 156/1000 | Loss: 0.00002956
Iteration 157/1000 | Loss: 0.00002956
Iteration 158/1000 | Loss: 0.00002956
Iteration 159/1000 | Loss: 0.00002956
Iteration 160/1000 | Loss: 0.00002956
Iteration 161/1000 | Loss: 0.00002956
Iteration 162/1000 | Loss: 0.00002956
Iteration 163/1000 | Loss: 0.00002956
Iteration 164/1000 | Loss: 0.00002956
Iteration 165/1000 | Loss: 0.00002956
Iteration 166/1000 | Loss: 0.00002956
Iteration 167/1000 | Loss: 0.00002956
Iteration 168/1000 | Loss: 0.00002956
Iteration 169/1000 | Loss: 0.00002956
Iteration 170/1000 | Loss: 0.00002955
Iteration 171/1000 | Loss: 0.00002955
Iteration 172/1000 | Loss: 0.00002955
Iteration 173/1000 | Loss: 0.00002955
Iteration 174/1000 | Loss: 0.00002955
Iteration 175/1000 | Loss: 0.00002955
Iteration 176/1000 | Loss: 0.00002955
Iteration 177/1000 | Loss: 0.00002955
Iteration 178/1000 | Loss: 0.00002955
Iteration 179/1000 | Loss: 0.00002955
Iteration 180/1000 | Loss: 0.00002955
Iteration 181/1000 | Loss: 0.00002955
Iteration 182/1000 | Loss: 0.00002955
Iteration 183/1000 | Loss: 0.00002955
Iteration 184/1000 | Loss: 0.00002955
Iteration 185/1000 | Loss: 0.00002955
Iteration 186/1000 | Loss: 0.00002955
Iteration 187/1000 | Loss: 0.00002955
Iteration 188/1000 | Loss: 0.00002955
Iteration 189/1000 | Loss: 0.00002955
Iteration 190/1000 | Loss: 0.00002955
Iteration 191/1000 | Loss: 0.00002955
Iteration 192/1000 | Loss: 0.00002955
Iteration 193/1000 | Loss: 0.00002955
Iteration 194/1000 | Loss: 0.00002955
Iteration 195/1000 | Loss: 0.00002955
Iteration 196/1000 | Loss: 0.00002955
Iteration 197/1000 | Loss: 0.00002955
Iteration 198/1000 | Loss: 0.00002955
Iteration 199/1000 | Loss: 0.00002955
Iteration 200/1000 | Loss: 0.00002955
Iteration 201/1000 | Loss: 0.00002955
Iteration 202/1000 | Loss: 0.00002955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [2.9550805265898816e-05, 2.9550805265898816e-05, 2.9550805265898816e-05, 2.9550805265898816e-05, 2.9550805265898816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9550805265898816e-05

Optimization complete. Final v2v error: 4.4060845375061035 mm

Highest mean error: 4.692862510681152 mm for frame 61

Lowest mean error: 3.9241323471069336 mm for frame 165

Saving results

Total time: 54.44847655296326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460759
Iteration 2/25 | Loss: 0.00119755
Iteration 3/25 | Loss: 0.00101039
Iteration 4/25 | Loss: 0.00098961
Iteration 5/25 | Loss: 0.00098586
Iteration 6/25 | Loss: 0.00098489
Iteration 7/25 | Loss: 0.00098483
Iteration 8/25 | Loss: 0.00098483
Iteration 9/25 | Loss: 0.00098483
Iteration 10/25 | Loss: 0.00098483
Iteration 11/25 | Loss: 0.00098483
Iteration 12/25 | Loss: 0.00098483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009848345071077347, 0.0009848345071077347, 0.0009848345071077347, 0.0009848345071077347, 0.0009848345071077347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009848345071077347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38895667
Iteration 2/25 | Loss: 0.00042651
Iteration 3/25 | Loss: 0.00042650
Iteration 4/25 | Loss: 0.00042650
Iteration 5/25 | Loss: 0.00042650
Iteration 6/25 | Loss: 0.00042650
Iteration 7/25 | Loss: 0.00042650
Iteration 8/25 | Loss: 0.00042650
Iteration 9/25 | Loss: 0.00042650
Iteration 10/25 | Loss: 0.00042650
Iteration 11/25 | Loss: 0.00042650
Iteration 12/25 | Loss: 0.00042650
Iteration 13/25 | Loss: 0.00042650
Iteration 14/25 | Loss: 0.00042650
Iteration 15/25 | Loss: 0.00042650
Iteration 16/25 | Loss: 0.00042650
Iteration 17/25 | Loss: 0.00042650
Iteration 18/25 | Loss: 0.00042650
Iteration 19/25 | Loss: 0.00042650
Iteration 20/25 | Loss: 0.00042650
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000426501443143934, 0.000426501443143934, 0.000426501443143934, 0.000426501443143934, 0.000426501443143934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000426501443143934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042650
Iteration 2/1000 | Loss: 0.00002936
Iteration 3/1000 | Loss: 0.00002007
Iteration 4/1000 | Loss: 0.00001802
Iteration 5/1000 | Loss: 0.00001717
Iteration 6/1000 | Loss: 0.00001675
Iteration 7/1000 | Loss: 0.00001639
Iteration 8/1000 | Loss: 0.00001617
Iteration 9/1000 | Loss: 0.00001612
Iteration 10/1000 | Loss: 0.00001607
Iteration 11/1000 | Loss: 0.00001602
Iteration 12/1000 | Loss: 0.00001601
Iteration 13/1000 | Loss: 0.00001596
Iteration 14/1000 | Loss: 0.00001592
Iteration 15/1000 | Loss: 0.00001587
Iteration 16/1000 | Loss: 0.00001585
Iteration 17/1000 | Loss: 0.00001580
Iteration 18/1000 | Loss: 0.00001580
Iteration 19/1000 | Loss: 0.00001580
Iteration 20/1000 | Loss: 0.00001580
Iteration 21/1000 | Loss: 0.00001580
Iteration 22/1000 | Loss: 0.00001580
Iteration 23/1000 | Loss: 0.00001579
Iteration 24/1000 | Loss: 0.00001579
Iteration 25/1000 | Loss: 0.00001579
Iteration 26/1000 | Loss: 0.00001579
Iteration 27/1000 | Loss: 0.00001574
Iteration 28/1000 | Loss: 0.00001572
Iteration 29/1000 | Loss: 0.00001570
Iteration 30/1000 | Loss: 0.00001570
Iteration 31/1000 | Loss: 0.00001570
Iteration 32/1000 | Loss: 0.00001568
Iteration 33/1000 | Loss: 0.00001563
Iteration 34/1000 | Loss: 0.00001562
Iteration 35/1000 | Loss: 0.00001561
Iteration 36/1000 | Loss: 0.00001561
Iteration 37/1000 | Loss: 0.00001560
Iteration 38/1000 | Loss: 0.00001560
Iteration 39/1000 | Loss: 0.00001560
Iteration 40/1000 | Loss: 0.00001559
Iteration 41/1000 | Loss: 0.00001559
Iteration 42/1000 | Loss: 0.00001559
Iteration 43/1000 | Loss: 0.00001559
Iteration 44/1000 | Loss: 0.00001559
Iteration 45/1000 | Loss: 0.00001559
Iteration 46/1000 | Loss: 0.00001558
Iteration 47/1000 | Loss: 0.00001557
Iteration 48/1000 | Loss: 0.00001557
Iteration 49/1000 | Loss: 0.00001556
Iteration 50/1000 | Loss: 0.00001556
Iteration 51/1000 | Loss: 0.00001556
Iteration 52/1000 | Loss: 0.00001556
Iteration 53/1000 | Loss: 0.00001555
Iteration 54/1000 | Loss: 0.00001554
Iteration 55/1000 | Loss: 0.00001554
Iteration 56/1000 | Loss: 0.00001554
Iteration 57/1000 | Loss: 0.00001553
Iteration 58/1000 | Loss: 0.00001553
Iteration 59/1000 | Loss: 0.00001553
Iteration 60/1000 | Loss: 0.00001553
Iteration 61/1000 | Loss: 0.00001553
Iteration 62/1000 | Loss: 0.00001553
Iteration 63/1000 | Loss: 0.00001553
Iteration 64/1000 | Loss: 0.00001553
Iteration 65/1000 | Loss: 0.00001553
Iteration 66/1000 | Loss: 0.00001552
Iteration 67/1000 | Loss: 0.00001552
Iteration 68/1000 | Loss: 0.00001552
Iteration 69/1000 | Loss: 0.00001551
Iteration 70/1000 | Loss: 0.00001551
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001551
Iteration 73/1000 | Loss: 0.00001551
Iteration 74/1000 | Loss: 0.00001551
Iteration 75/1000 | Loss: 0.00001551
Iteration 76/1000 | Loss: 0.00001551
Iteration 77/1000 | Loss: 0.00001550
Iteration 78/1000 | Loss: 0.00001550
Iteration 79/1000 | Loss: 0.00001550
Iteration 80/1000 | Loss: 0.00001550
Iteration 81/1000 | Loss: 0.00001550
Iteration 82/1000 | Loss: 0.00001549
Iteration 83/1000 | Loss: 0.00001549
Iteration 84/1000 | Loss: 0.00001549
Iteration 85/1000 | Loss: 0.00001549
Iteration 86/1000 | Loss: 0.00001549
Iteration 87/1000 | Loss: 0.00001549
Iteration 88/1000 | Loss: 0.00001549
Iteration 89/1000 | Loss: 0.00001548
Iteration 90/1000 | Loss: 0.00001548
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001548
Iteration 94/1000 | Loss: 0.00001547
Iteration 95/1000 | Loss: 0.00001547
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001546
Iteration 98/1000 | Loss: 0.00001546
Iteration 99/1000 | Loss: 0.00001546
Iteration 100/1000 | Loss: 0.00001546
Iteration 101/1000 | Loss: 0.00001546
Iteration 102/1000 | Loss: 0.00001546
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001545
Iteration 105/1000 | Loss: 0.00001545
Iteration 106/1000 | Loss: 0.00001545
Iteration 107/1000 | Loss: 0.00001545
Iteration 108/1000 | Loss: 0.00001545
Iteration 109/1000 | Loss: 0.00001545
Iteration 110/1000 | Loss: 0.00001544
Iteration 111/1000 | Loss: 0.00001544
Iteration 112/1000 | Loss: 0.00001544
Iteration 113/1000 | Loss: 0.00001544
Iteration 114/1000 | Loss: 0.00001544
Iteration 115/1000 | Loss: 0.00001543
Iteration 116/1000 | Loss: 0.00001543
Iteration 117/1000 | Loss: 0.00001543
Iteration 118/1000 | Loss: 0.00001542
Iteration 119/1000 | Loss: 0.00001542
Iteration 120/1000 | Loss: 0.00001542
Iteration 121/1000 | Loss: 0.00001542
Iteration 122/1000 | Loss: 0.00001542
Iteration 123/1000 | Loss: 0.00001542
Iteration 124/1000 | Loss: 0.00001542
Iteration 125/1000 | Loss: 0.00001542
Iteration 126/1000 | Loss: 0.00001541
Iteration 127/1000 | Loss: 0.00001541
Iteration 128/1000 | Loss: 0.00001541
Iteration 129/1000 | Loss: 0.00001541
Iteration 130/1000 | Loss: 0.00001541
Iteration 131/1000 | Loss: 0.00001541
Iteration 132/1000 | Loss: 0.00001541
Iteration 133/1000 | Loss: 0.00001541
Iteration 134/1000 | Loss: 0.00001540
Iteration 135/1000 | Loss: 0.00001540
Iteration 136/1000 | Loss: 0.00001540
Iteration 137/1000 | Loss: 0.00001540
Iteration 138/1000 | Loss: 0.00001540
Iteration 139/1000 | Loss: 0.00001540
Iteration 140/1000 | Loss: 0.00001540
Iteration 141/1000 | Loss: 0.00001540
Iteration 142/1000 | Loss: 0.00001540
Iteration 143/1000 | Loss: 0.00001540
Iteration 144/1000 | Loss: 0.00001539
Iteration 145/1000 | Loss: 0.00001539
Iteration 146/1000 | Loss: 0.00001539
Iteration 147/1000 | Loss: 0.00001539
Iteration 148/1000 | Loss: 0.00001539
Iteration 149/1000 | Loss: 0.00001539
Iteration 150/1000 | Loss: 0.00001539
Iteration 151/1000 | Loss: 0.00001538
Iteration 152/1000 | Loss: 0.00001538
Iteration 153/1000 | Loss: 0.00001538
Iteration 154/1000 | Loss: 0.00001538
Iteration 155/1000 | Loss: 0.00001538
Iteration 156/1000 | Loss: 0.00001538
Iteration 157/1000 | Loss: 0.00001538
Iteration 158/1000 | Loss: 0.00001538
Iteration 159/1000 | Loss: 0.00001538
Iteration 160/1000 | Loss: 0.00001538
Iteration 161/1000 | Loss: 0.00001538
Iteration 162/1000 | Loss: 0.00001538
Iteration 163/1000 | Loss: 0.00001538
Iteration 164/1000 | Loss: 0.00001538
Iteration 165/1000 | Loss: 0.00001538
Iteration 166/1000 | Loss: 0.00001538
Iteration 167/1000 | Loss: 0.00001538
Iteration 168/1000 | Loss: 0.00001537
Iteration 169/1000 | Loss: 0.00001537
Iteration 170/1000 | Loss: 0.00001537
Iteration 171/1000 | Loss: 0.00001537
Iteration 172/1000 | Loss: 0.00001537
Iteration 173/1000 | Loss: 0.00001537
Iteration 174/1000 | Loss: 0.00001537
Iteration 175/1000 | Loss: 0.00001537
Iteration 176/1000 | Loss: 0.00001537
Iteration 177/1000 | Loss: 0.00001537
Iteration 178/1000 | Loss: 0.00001537
Iteration 179/1000 | Loss: 0.00001537
Iteration 180/1000 | Loss: 0.00001537
Iteration 181/1000 | Loss: 0.00001537
Iteration 182/1000 | Loss: 0.00001536
Iteration 183/1000 | Loss: 0.00001536
Iteration 184/1000 | Loss: 0.00001536
Iteration 185/1000 | Loss: 0.00001536
Iteration 186/1000 | Loss: 0.00001536
Iteration 187/1000 | Loss: 0.00001536
Iteration 188/1000 | Loss: 0.00001536
Iteration 189/1000 | Loss: 0.00001536
Iteration 190/1000 | Loss: 0.00001536
Iteration 191/1000 | Loss: 0.00001536
Iteration 192/1000 | Loss: 0.00001536
Iteration 193/1000 | Loss: 0.00001535
Iteration 194/1000 | Loss: 0.00001535
Iteration 195/1000 | Loss: 0.00001535
Iteration 196/1000 | Loss: 0.00001535
Iteration 197/1000 | Loss: 0.00001535
Iteration 198/1000 | Loss: 0.00001535
Iteration 199/1000 | Loss: 0.00001535
Iteration 200/1000 | Loss: 0.00001535
Iteration 201/1000 | Loss: 0.00001535
Iteration 202/1000 | Loss: 0.00001535
Iteration 203/1000 | Loss: 0.00001535
Iteration 204/1000 | Loss: 0.00001535
Iteration 205/1000 | Loss: 0.00001535
Iteration 206/1000 | Loss: 0.00001535
Iteration 207/1000 | Loss: 0.00001535
Iteration 208/1000 | Loss: 0.00001534
Iteration 209/1000 | Loss: 0.00001534
Iteration 210/1000 | Loss: 0.00001534
Iteration 211/1000 | Loss: 0.00001534
Iteration 212/1000 | Loss: 0.00001534
Iteration 213/1000 | Loss: 0.00001534
Iteration 214/1000 | Loss: 0.00001534
Iteration 215/1000 | Loss: 0.00001534
Iteration 216/1000 | Loss: 0.00001534
Iteration 217/1000 | Loss: 0.00001534
Iteration 218/1000 | Loss: 0.00001534
Iteration 219/1000 | Loss: 0.00001534
Iteration 220/1000 | Loss: 0.00001534
Iteration 221/1000 | Loss: 0.00001534
Iteration 222/1000 | Loss: 0.00001534
Iteration 223/1000 | Loss: 0.00001534
Iteration 224/1000 | Loss: 0.00001534
Iteration 225/1000 | Loss: 0.00001534
Iteration 226/1000 | Loss: 0.00001534
Iteration 227/1000 | Loss: 0.00001534
Iteration 228/1000 | Loss: 0.00001534
Iteration 229/1000 | Loss: 0.00001534
Iteration 230/1000 | Loss: 0.00001534
Iteration 231/1000 | Loss: 0.00001534
Iteration 232/1000 | Loss: 0.00001534
Iteration 233/1000 | Loss: 0.00001534
Iteration 234/1000 | Loss: 0.00001534
Iteration 235/1000 | Loss: 0.00001534
Iteration 236/1000 | Loss: 0.00001534
Iteration 237/1000 | Loss: 0.00001534
Iteration 238/1000 | Loss: 0.00001534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.5337669537984766e-05, 1.5337669537984766e-05, 1.5337669537984766e-05, 1.5337669537984766e-05, 1.5337669537984766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5337669537984766e-05

Optimization complete. Final v2v error: 3.2201437950134277 mm

Highest mean error: 3.6450705528259277 mm for frame 36

Lowest mean error: 2.6721363067626953 mm for frame 7

Saving results

Total time: 40.28837728500366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891054
Iteration 2/25 | Loss: 0.00130596
Iteration 3/25 | Loss: 0.00107812
Iteration 4/25 | Loss: 0.00104995
Iteration 5/25 | Loss: 0.00105000
Iteration 6/25 | Loss: 0.00105131
Iteration 7/25 | Loss: 0.00104315
Iteration 8/25 | Loss: 0.00104098
Iteration 9/25 | Loss: 0.00103913
Iteration 10/25 | Loss: 0.00103737
Iteration 11/25 | Loss: 0.00103645
Iteration 12/25 | Loss: 0.00103597
Iteration 13/25 | Loss: 0.00103582
Iteration 14/25 | Loss: 0.00103582
Iteration 15/25 | Loss: 0.00103581
Iteration 16/25 | Loss: 0.00103581
Iteration 17/25 | Loss: 0.00103581
Iteration 18/25 | Loss: 0.00103581
Iteration 19/25 | Loss: 0.00103581
Iteration 20/25 | Loss: 0.00103581
Iteration 21/25 | Loss: 0.00103580
Iteration 22/25 | Loss: 0.00103580
Iteration 23/25 | Loss: 0.00103580
Iteration 24/25 | Loss: 0.00103580
Iteration 25/25 | Loss: 0.00103579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.66212797
Iteration 2/25 | Loss: 0.00051245
Iteration 3/25 | Loss: 0.00051242
Iteration 4/25 | Loss: 0.00051242
Iteration 5/25 | Loss: 0.00051241
Iteration 6/25 | Loss: 0.00051241
Iteration 7/25 | Loss: 0.00051241
Iteration 8/25 | Loss: 0.00051241
Iteration 9/25 | Loss: 0.00051241
Iteration 10/25 | Loss: 0.00051241
Iteration 11/25 | Loss: 0.00051241
Iteration 12/25 | Loss: 0.00051241
Iteration 13/25 | Loss: 0.00051241
Iteration 14/25 | Loss: 0.00051241
Iteration 15/25 | Loss: 0.00051241
Iteration 16/25 | Loss: 0.00051241
Iteration 17/25 | Loss: 0.00051241
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000512413214892149, 0.000512413214892149, 0.000512413214892149, 0.000512413214892149, 0.000512413214892149]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000512413214892149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051241
Iteration 2/1000 | Loss: 0.00003586
Iteration 3/1000 | Loss: 0.00002402
Iteration 4/1000 | Loss: 0.00002103
Iteration 5/1000 | Loss: 0.00001995
Iteration 6/1000 | Loss: 0.00001915
Iteration 7/1000 | Loss: 0.00001862
Iteration 8/1000 | Loss: 0.00001824
Iteration 9/1000 | Loss: 0.00038292
Iteration 10/1000 | Loss: 0.00002043
Iteration 11/1000 | Loss: 0.00001803
Iteration 12/1000 | Loss: 0.00001722
Iteration 13/1000 | Loss: 0.00001673
Iteration 14/1000 | Loss: 0.00001634
Iteration 15/1000 | Loss: 0.00001613
Iteration 16/1000 | Loss: 0.00001602
Iteration 17/1000 | Loss: 0.00001601
Iteration 18/1000 | Loss: 0.00001601
Iteration 19/1000 | Loss: 0.00001599
Iteration 20/1000 | Loss: 0.00001598
Iteration 21/1000 | Loss: 0.00001597
Iteration 22/1000 | Loss: 0.00001597
Iteration 23/1000 | Loss: 0.00001597
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001597
Iteration 26/1000 | Loss: 0.00001596
Iteration 27/1000 | Loss: 0.00001596
Iteration 28/1000 | Loss: 0.00001595
Iteration 29/1000 | Loss: 0.00001595
Iteration 30/1000 | Loss: 0.00001594
Iteration 31/1000 | Loss: 0.00001592
Iteration 32/1000 | Loss: 0.00001592
Iteration 33/1000 | Loss: 0.00001591
Iteration 34/1000 | Loss: 0.00001591
Iteration 35/1000 | Loss: 0.00001591
Iteration 36/1000 | Loss: 0.00001591
Iteration 37/1000 | Loss: 0.00001590
Iteration 38/1000 | Loss: 0.00001590
Iteration 39/1000 | Loss: 0.00001589
Iteration 40/1000 | Loss: 0.00001589
Iteration 41/1000 | Loss: 0.00001589
Iteration 42/1000 | Loss: 0.00001589
Iteration 43/1000 | Loss: 0.00001589
Iteration 44/1000 | Loss: 0.00001589
Iteration 45/1000 | Loss: 0.00001588
Iteration 46/1000 | Loss: 0.00001588
Iteration 47/1000 | Loss: 0.00001588
Iteration 48/1000 | Loss: 0.00001588
Iteration 49/1000 | Loss: 0.00001588
Iteration 50/1000 | Loss: 0.00001587
Iteration 51/1000 | Loss: 0.00001587
Iteration 52/1000 | Loss: 0.00001587
Iteration 53/1000 | Loss: 0.00001587
Iteration 54/1000 | Loss: 0.00001587
Iteration 55/1000 | Loss: 0.00001586
Iteration 56/1000 | Loss: 0.00001586
Iteration 57/1000 | Loss: 0.00001586
Iteration 58/1000 | Loss: 0.00001585
Iteration 59/1000 | Loss: 0.00001585
Iteration 60/1000 | Loss: 0.00001585
Iteration 61/1000 | Loss: 0.00001584
Iteration 62/1000 | Loss: 0.00001584
Iteration 63/1000 | Loss: 0.00001584
Iteration 64/1000 | Loss: 0.00001584
Iteration 65/1000 | Loss: 0.00001584
Iteration 66/1000 | Loss: 0.00001584
Iteration 67/1000 | Loss: 0.00001583
Iteration 68/1000 | Loss: 0.00001583
Iteration 69/1000 | Loss: 0.00001582
Iteration 70/1000 | Loss: 0.00001581
Iteration 71/1000 | Loss: 0.00001581
Iteration 72/1000 | Loss: 0.00001580
Iteration 73/1000 | Loss: 0.00001580
Iteration 74/1000 | Loss: 0.00001576
Iteration 75/1000 | Loss: 0.00001576
Iteration 76/1000 | Loss: 0.00001575
Iteration 77/1000 | Loss: 0.00001575
Iteration 78/1000 | Loss: 0.00001574
Iteration 79/1000 | Loss: 0.00001573
Iteration 80/1000 | Loss: 0.00001573
Iteration 81/1000 | Loss: 0.00001572
Iteration 82/1000 | Loss: 0.00001572
Iteration 83/1000 | Loss: 0.00001572
Iteration 84/1000 | Loss: 0.00001572
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001571
Iteration 87/1000 | Loss: 0.00001571
Iteration 88/1000 | Loss: 0.00001571
Iteration 89/1000 | Loss: 0.00001570
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001569
Iteration 92/1000 | Loss: 0.00001568
Iteration 93/1000 | Loss: 0.00001567
Iteration 94/1000 | Loss: 0.00001567
Iteration 95/1000 | Loss: 0.00001566
Iteration 96/1000 | Loss: 0.00001566
Iteration 97/1000 | Loss: 0.00001565
Iteration 98/1000 | Loss: 0.00001565
Iteration 99/1000 | Loss: 0.00001564
Iteration 100/1000 | Loss: 0.00001564
Iteration 101/1000 | Loss: 0.00001563
Iteration 102/1000 | Loss: 0.00001563
Iteration 103/1000 | Loss: 0.00001563
Iteration 104/1000 | Loss: 0.00001563
Iteration 105/1000 | Loss: 0.00001562
Iteration 106/1000 | Loss: 0.00001562
Iteration 107/1000 | Loss: 0.00001561
Iteration 108/1000 | Loss: 0.00001561
Iteration 109/1000 | Loss: 0.00001561
Iteration 110/1000 | Loss: 0.00001560
Iteration 111/1000 | Loss: 0.00001560
Iteration 112/1000 | Loss: 0.00001560
Iteration 113/1000 | Loss: 0.00001559
Iteration 114/1000 | Loss: 0.00001559
Iteration 115/1000 | Loss: 0.00001559
Iteration 116/1000 | Loss: 0.00001559
Iteration 117/1000 | Loss: 0.00001558
Iteration 118/1000 | Loss: 0.00001558
Iteration 119/1000 | Loss: 0.00001558
Iteration 120/1000 | Loss: 0.00001558
Iteration 121/1000 | Loss: 0.00001558
Iteration 122/1000 | Loss: 0.00001558
Iteration 123/1000 | Loss: 0.00001557
Iteration 124/1000 | Loss: 0.00001557
Iteration 125/1000 | Loss: 0.00001557
Iteration 126/1000 | Loss: 0.00001557
Iteration 127/1000 | Loss: 0.00001557
Iteration 128/1000 | Loss: 0.00001557
Iteration 129/1000 | Loss: 0.00001556
Iteration 130/1000 | Loss: 0.00001556
Iteration 131/1000 | Loss: 0.00001556
Iteration 132/1000 | Loss: 0.00001555
Iteration 133/1000 | Loss: 0.00001555
Iteration 134/1000 | Loss: 0.00001555
Iteration 135/1000 | Loss: 0.00001555
Iteration 136/1000 | Loss: 0.00001555
Iteration 137/1000 | Loss: 0.00001555
Iteration 138/1000 | Loss: 0.00001555
Iteration 139/1000 | Loss: 0.00001555
Iteration 140/1000 | Loss: 0.00001555
Iteration 141/1000 | Loss: 0.00001554
Iteration 142/1000 | Loss: 0.00001554
Iteration 143/1000 | Loss: 0.00001554
Iteration 144/1000 | Loss: 0.00001554
Iteration 145/1000 | Loss: 0.00001554
Iteration 146/1000 | Loss: 0.00001554
Iteration 147/1000 | Loss: 0.00001553
Iteration 148/1000 | Loss: 0.00001553
Iteration 149/1000 | Loss: 0.00001553
Iteration 150/1000 | Loss: 0.00001553
Iteration 151/1000 | Loss: 0.00001553
Iteration 152/1000 | Loss: 0.00001553
Iteration 153/1000 | Loss: 0.00001553
Iteration 154/1000 | Loss: 0.00001553
Iteration 155/1000 | Loss: 0.00001553
Iteration 156/1000 | Loss: 0.00001553
Iteration 157/1000 | Loss: 0.00001553
Iteration 158/1000 | Loss: 0.00001552
Iteration 159/1000 | Loss: 0.00001552
Iteration 160/1000 | Loss: 0.00001552
Iteration 161/1000 | Loss: 0.00001552
Iteration 162/1000 | Loss: 0.00001552
Iteration 163/1000 | Loss: 0.00001552
Iteration 164/1000 | Loss: 0.00001552
Iteration 165/1000 | Loss: 0.00001552
Iteration 166/1000 | Loss: 0.00001551
Iteration 167/1000 | Loss: 0.00001551
Iteration 168/1000 | Loss: 0.00001551
Iteration 169/1000 | Loss: 0.00001551
Iteration 170/1000 | Loss: 0.00001551
Iteration 171/1000 | Loss: 0.00001551
Iteration 172/1000 | Loss: 0.00001551
Iteration 173/1000 | Loss: 0.00001551
Iteration 174/1000 | Loss: 0.00001551
Iteration 175/1000 | Loss: 0.00001551
Iteration 176/1000 | Loss: 0.00001550
Iteration 177/1000 | Loss: 0.00001550
Iteration 178/1000 | Loss: 0.00001550
Iteration 179/1000 | Loss: 0.00001550
Iteration 180/1000 | Loss: 0.00001550
Iteration 181/1000 | Loss: 0.00001550
Iteration 182/1000 | Loss: 0.00001550
Iteration 183/1000 | Loss: 0.00001550
Iteration 184/1000 | Loss: 0.00001550
Iteration 185/1000 | Loss: 0.00001550
Iteration 186/1000 | Loss: 0.00001550
Iteration 187/1000 | Loss: 0.00001550
Iteration 188/1000 | Loss: 0.00001550
Iteration 189/1000 | Loss: 0.00001550
Iteration 190/1000 | Loss: 0.00001550
Iteration 191/1000 | Loss: 0.00001550
Iteration 192/1000 | Loss: 0.00001550
Iteration 193/1000 | Loss: 0.00001550
Iteration 194/1000 | Loss: 0.00001550
Iteration 195/1000 | Loss: 0.00001550
Iteration 196/1000 | Loss: 0.00001550
Iteration 197/1000 | Loss: 0.00001550
Iteration 198/1000 | Loss: 0.00001550
Iteration 199/1000 | Loss: 0.00001550
Iteration 200/1000 | Loss: 0.00001550
Iteration 201/1000 | Loss: 0.00001550
Iteration 202/1000 | Loss: 0.00001550
Iteration 203/1000 | Loss: 0.00001550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.5503153917961754e-05, 1.5503153917961754e-05, 1.5503153917961754e-05, 1.5503153917961754e-05, 1.5503153917961754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5503153917961754e-05

Optimization complete. Final v2v error: 3.3426661491394043 mm

Highest mean error: 4.251217365264893 mm for frame 79

Lowest mean error: 2.9128143787384033 mm for frame 236

Saving results

Total time: 68.32281184196472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763124
Iteration 2/25 | Loss: 0.00123090
Iteration 3/25 | Loss: 0.00107477
Iteration 4/25 | Loss: 0.00105357
Iteration 5/25 | Loss: 0.00104990
Iteration 6/25 | Loss: 0.00104890
Iteration 7/25 | Loss: 0.00104880
Iteration 8/25 | Loss: 0.00104880
Iteration 9/25 | Loss: 0.00104880
Iteration 10/25 | Loss: 0.00104880
Iteration 11/25 | Loss: 0.00104880
Iteration 12/25 | Loss: 0.00104880
Iteration 13/25 | Loss: 0.00104880
Iteration 14/25 | Loss: 0.00104880
Iteration 15/25 | Loss: 0.00104880
Iteration 16/25 | Loss: 0.00104880
Iteration 17/25 | Loss: 0.00104880
Iteration 18/25 | Loss: 0.00104880
Iteration 19/25 | Loss: 0.00104880
Iteration 20/25 | Loss: 0.00104880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010487954132258892, 0.0010487954132258892, 0.0010487954132258892, 0.0010487954132258892, 0.0010487954132258892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010487954132258892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25702357
Iteration 2/25 | Loss: 0.00052892
Iteration 3/25 | Loss: 0.00052891
Iteration 4/25 | Loss: 0.00052891
Iteration 5/25 | Loss: 0.00052891
Iteration 6/25 | Loss: 0.00052891
Iteration 7/25 | Loss: 0.00052891
Iteration 8/25 | Loss: 0.00052891
Iteration 9/25 | Loss: 0.00052891
Iteration 10/25 | Loss: 0.00052891
Iteration 11/25 | Loss: 0.00052891
Iteration 12/25 | Loss: 0.00052891
Iteration 13/25 | Loss: 0.00052891
Iteration 14/25 | Loss: 0.00052891
Iteration 15/25 | Loss: 0.00052891
Iteration 16/25 | Loss: 0.00052891
Iteration 17/25 | Loss: 0.00052891
Iteration 18/25 | Loss: 0.00052891
Iteration 19/25 | Loss: 0.00052891
Iteration 20/25 | Loss: 0.00052891
Iteration 21/25 | Loss: 0.00052891
Iteration 22/25 | Loss: 0.00052891
Iteration 23/25 | Loss: 0.00052891
Iteration 24/25 | Loss: 0.00052891
Iteration 25/25 | Loss: 0.00052891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052891
Iteration 2/1000 | Loss: 0.00003324
Iteration 3/1000 | Loss: 0.00002497
Iteration 4/1000 | Loss: 0.00002220
Iteration 5/1000 | Loss: 0.00002070
Iteration 6/1000 | Loss: 0.00001975
Iteration 7/1000 | Loss: 0.00001931
Iteration 8/1000 | Loss: 0.00001899
Iteration 9/1000 | Loss: 0.00001877
Iteration 10/1000 | Loss: 0.00001872
Iteration 11/1000 | Loss: 0.00001869
Iteration 12/1000 | Loss: 0.00001861
Iteration 13/1000 | Loss: 0.00001846
Iteration 14/1000 | Loss: 0.00001840
Iteration 15/1000 | Loss: 0.00001838
Iteration 16/1000 | Loss: 0.00001834
Iteration 17/1000 | Loss: 0.00001833
Iteration 18/1000 | Loss: 0.00001832
Iteration 19/1000 | Loss: 0.00001830
Iteration 20/1000 | Loss: 0.00001830
Iteration 21/1000 | Loss: 0.00001830
Iteration 22/1000 | Loss: 0.00001830
Iteration 23/1000 | Loss: 0.00001830
Iteration 24/1000 | Loss: 0.00001830
Iteration 25/1000 | Loss: 0.00001830
Iteration 26/1000 | Loss: 0.00001830
Iteration 27/1000 | Loss: 0.00001830
Iteration 28/1000 | Loss: 0.00001830
Iteration 29/1000 | Loss: 0.00001830
Iteration 30/1000 | Loss: 0.00001829
Iteration 31/1000 | Loss: 0.00001829
Iteration 32/1000 | Loss: 0.00001828
Iteration 33/1000 | Loss: 0.00001827
Iteration 34/1000 | Loss: 0.00001827
Iteration 35/1000 | Loss: 0.00001827
Iteration 36/1000 | Loss: 0.00001826
Iteration 37/1000 | Loss: 0.00001826
Iteration 38/1000 | Loss: 0.00001826
Iteration 39/1000 | Loss: 0.00001825
Iteration 40/1000 | Loss: 0.00001825
Iteration 41/1000 | Loss: 0.00001825
Iteration 42/1000 | Loss: 0.00001824
Iteration 43/1000 | Loss: 0.00001824
Iteration 44/1000 | Loss: 0.00001823
Iteration 45/1000 | Loss: 0.00001823
Iteration 46/1000 | Loss: 0.00001822
Iteration 47/1000 | Loss: 0.00001822
Iteration 48/1000 | Loss: 0.00001822
Iteration 49/1000 | Loss: 0.00001822
Iteration 50/1000 | Loss: 0.00001822
Iteration 51/1000 | Loss: 0.00001821
Iteration 52/1000 | Loss: 0.00001821
Iteration 53/1000 | Loss: 0.00001820
Iteration 54/1000 | Loss: 0.00001820
Iteration 55/1000 | Loss: 0.00001820
Iteration 56/1000 | Loss: 0.00001820
Iteration 57/1000 | Loss: 0.00001820
Iteration 58/1000 | Loss: 0.00001820
Iteration 59/1000 | Loss: 0.00001820
Iteration 60/1000 | Loss: 0.00001820
Iteration 61/1000 | Loss: 0.00001819
Iteration 62/1000 | Loss: 0.00001819
Iteration 63/1000 | Loss: 0.00001819
Iteration 64/1000 | Loss: 0.00001819
Iteration 65/1000 | Loss: 0.00001819
Iteration 66/1000 | Loss: 0.00001819
Iteration 67/1000 | Loss: 0.00001818
Iteration 68/1000 | Loss: 0.00001818
Iteration 69/1000 | Loss: 0.00001818
Iteration 70/1000 | Loss: 0.00001818
Iteration 71/1000 | Loss: 0.00001817
Iteration 72/1000 | Loss: 0.00001817
Iteration 73/1000 | Loss: 0.00001817
Iteration 74/1000 | Loss: 0.00001817
Iteration 75/1000 | Loss: 0.00001817
Iteration 76/1000 | Loss: 0.00001817
Iteration 77/1000 | Loss: 0.00001817
Iteration 78/1000 | Loss: 0.00001817
Iteration 79/1000 | Loss: 0.00001817
Iteration 80/1000 | Loss: 0.00001817
Iteration 81/1000 | Loss: 0.00001817
Iteration 82/1000 | Loss: 0.00001817
Iteration 83/1000 | Loss: 0.00001817
Iteration 84/1000 | Loss: 0.00001817
Iteration 85/1000 | Loss: 0.00001817
Iteration 86/1000 | Loss: 0.00001817
Iteration 87/1000 | Loss: 0.00001817
Iteration 88/1000 | Loss: 0.00001817
Iteration 89/1000 | Loss: 0.00001817
Iteration 90/1000 | Loss: 0.00001817
Iteration 91/1000 | Loss: 0.00001816
Iteration 92/1000 | Loss: 0.00001816
Iteration 93/1000 | Loss: 0.00001816
Iteration 94/1000 | Loss: 0.00001816
Iteration 95/1000 | Loss: 0.00001816
Iteration 96/1000 | Loss: 0.00001816
Iteration 97/1000 | Loss: 0.00001816
Iteration 98/1000 | Loss: 0.00001816
Iteration 99/1000 | Loss: 0.00001816
Iteration 100/1000 | Loss: 0.00001815
Iteration 101/1000 | Loss: 0.00001815
Iteration 102/1000 | Loss: 0.00001815
Iteration 103/1000 | Loss: 0.00001815
Iteration 104/1000 | Loss: 0.00001815
Iteration 105/1000 | Loss: 0.00001815
Iteration 106/1000 | Loss: 0.00001815
Iteration 107/1000 | Loss: 0.00001815
Iteration 108/1000 | Loss: 0.00001815
Iteration 109/1000 | Loss: 0.00001815
Iteration 110/1000 | Loss: 0.00001815
Iteration 111/1000 | Loss: 0.00001814
Iteration 112/1000 | Loss: 0.00001814
Iteration 113/1000 | Loss: 0.00001814
Iteration 114/1000 | Loss: 0.00001814
Iteration 115/1000 | Loss: 0.00001814
Iteration 116/1000 | Loss: 0.00001814
Iteration 117/1000 | Loss: 0.00001814
Iteration 118/1000 | Loss: 0.00001814
Iteration 119/1000 | Loss: 0.00001814
Iteration 120/1000 | Loss: 0.00001814
Iteration 121/1000 | Loss: 0.00001814
Iteration 122/1000 | Loss: 0.00001813
Iteration 123/1000 | Loss: 0.00001813
Iteration 124/1000 | Loss: 0.00001813
Iteration 125/1000 | Loss: 0.00001813
Iteration 126/1000 | Loss: 0.00001813
Iteration 127/1000 | Loss: 0.00001813
Iteration 128/1000 | Loss: 0.00001813
Iteration 129/1000 | Loss: 0.00001813
Iteration 130/1000 | Loss: 0.00001813
Iteration 131/1000 | Loss: 0.00001813
Iteration 132/1000 | Loss: 0.00001813
Iteration 133/1000 | Loss: 0.00001812
Iteration 134/1000 | Loss: 0.00001812
Iteration 135/1000 | Loss: 0.00001812
Iteration 136/1000 | Loss: 0.00001812
Iteration 137/1000 | Loss: 0.00001812
Iteration 138/1000 | Loss: 0.00001812
Iteration 139/1000 | Loss: 0.00001812
Iteration 140/1000 | Loss: 0.00001812
Iteration 141/1000 | Loss: 0.00001812
Iteration 142/1000 | Loss: 0.00001812
Iteration 143/1000 | Loss: 0.00001812
Iteration 144/1000 | Loss: 0.00001811
Iteration 145/1000 | Loss: 0.00001811
Iteration 146/1000 | Loss: 0.00001811
Iteration 147/1000 | Loss: 0.00001811
Iteration 148/1000 | Loss: 0.00001811
Iteration 149/1000 | Loss: 0.00001810
Iteration 150/1000 | Loss: 0.00001810
Iteration 151/1000 | Loss: 0.00001810
Iteration 152/1000 | Loss: 0.00001810
Iteration 153/1000 | Loss: 0.00001810
Iteration 154/1000 | Loss: 0.00001810
Iteration 155/1000 | Loss: 0.00001810
Iteration 156/1000 | Loss: 0.00001810
Iteration 157/1000 | Loss: 0.00001810
Iteration 158/1000 | Loss: 0.00001810
Iteration 159/1000 | Loss: 0.00001810
Iteration 160/1000 | Loss: 0.00001810
Iteration 161/1000 | Loss: 0.00001810
Iteration 162/1000 | Loss: 0.00001809
Iteration 163/1000 | Loss: 0.00001809
Iteration 164/1000 | Loss: 0.00001809
Iteration 165/1000 | Loss: 0.00001809
Iteration 166/1000 | Loss: 0.00001808
Iteration 167/1000 | Loss: 0.00001808
Iteration 168/1000 | Loss: 0.00001808
Iteration 169/1000 | Loss: 0.00001808
Iteration 170/1000 | Loss: 0.00001808
Iteration 171/1000 | Loss: 0.00001808
Iteration 172/1000 | Loss: 0.00001808
Iteration 173/1000 | Loss: 0.00001808
Iteration 174/1000 | Loss: 0.00001808
Iteration 175/1000 | Loss: 0.00001808
Iteration 176/1000 | Loss: 0.00001808
Iteration 177/1000 | Loss: 0.00001808
Iteration 178/1000 | Loss: 0.00001808
Iteration 179/1000 | Loss: 0.00001808
Iteration 180/1000 | Loss: 0.00001807
Iteration 181/1000 | Loss: 0.00001807
Iteration 182/1000 | Loss: 0.00001807
Iteration 183/1000 | Loss: 0.00001807
Iteration 184/1000 | Loss: 0.00001807
Iteration 185/1000 | Loss: 0.00001807
Iteration 186/1000 | Loss: 0.00001807
Iteration 187/1000 | Loss: 0.00001807
Iteration 188/1000 | Loss: 0.00001807
Iteration 189/1000 | Loss: 0.00001807
Iteration 190/1000 | Loss: 0.00001807
Iteration 191/1000 | Loss: 0.00001807
Iteration 192/1000 | Loss: 0.00001807
Iteration 193/1000 | Loss: 0.00001807
Iteration 194/1000 | Loss: 0.00001807
Iteration 195/1000 | Loss: 0.00001807
Iteration 196/1000 | Loss: 0.00001807
Iteration 197/1000 | Loss: 0.00001807
Iteration 198/1000 | Loss: 0.00001807
Iteration 199/1000 | Loss: 0.00001807
Iteration 200/1000 | Loss: 0.00001807
Iteration 201/1000 | Loss: 0.00001807
Iteration 202/1000 | Loss: 0.00001807
Iteration 203/1000 | Loss: 0.00001807
Iteration 204/1000 | Loss: 0.00001807
Iteration 205/1000 | Loss: 0.00001807
Iteration 206/1000 | Loss: 0.00001807
Iteration 207/1000 | Loss: 0.00001807
Iteration 208/1000 | Loss: 0.00001807
Iteration 209/1000 | Loss: 0.00001807
Iteration 210/1000 | Loss: 0.00001807
Iteration 211/1000 | Loss: 0.00001807
Iteration 212/1000 | Loss: 0.00001807
Iteration 213/1000 | Loss: 0.00001807
Iteration 214/1000 | Loss: 0.00001807
Iteration 215/1000 | Loss: 0.00001807
Iteration 216/1000 | Loss: 0.00001807
Iteration 217/1000 | Loss: 0.00001807
Iteration 218/1000 | Loss: 0.00001807
Iteration 219/1000 | Loss: 0.00001807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.8065098629449494e-05, 1.8065098629449494e-05, 1.8065098629449494e-05, 1.8065098629449494e-05, 1.8065098629449494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8065098629449494e-05

Optimization complete. Final v2v error: 3.626465082168579 mm

Highest mean error: 4.081441879272461 mm for frame 1

Lowest mean error: 3.1169352531433105 mm for frame 77

Saving results

Total time: 38.05947017669678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888349
Iteration 2/25 | Loss: 0.00113681
Iteration 3/25 | Loss: 0.00100260
Iteration 4/25 | Loss: 0.00099067
Iteration 5/25 | Loss: 0.00098646
Iteration 6/25 | Loss: 0.00098550
Iteration 7/25 | Loss: 0.00098530
Iteration 8/25 | Loss: 0.00098530
Iteration 9/25 | Loss: 0.00098530
Iteration 10/25 | Loss: 0.00098530
Iteration 11/25 | Loss: 0.00098530
Iteration 12/25 | Loss: 0.00098530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009853000519797206, 0.0009853000519797206, 0.0009853000519797206, 0.0009853000519797206, 0.0009853000519797206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009853000519797206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.25152969
Iteration 2/25 | Loss: 0.00049212
Iteration 3/25 | Loss: 0.00049212
Iteration 4/25 | Loss: 0.00049212
Iteration 5/25 | Loss: 0.00049212
Iteration 6/25 | Loss: 0.00049212
Iteration 7/25 | Loss: 0.00049212
Iteration 8/25 | Loss: 0.00049212
Iteration 9/25 | Loss: 0.00049212
Iteration 10/25 | Loss: 0.00049211
Iteration 11/25 | Loss: 0.00049211
Iteration 12/25 | Loss: 0.00049211
Iteration 13/25 | Loss: 0.00049211
Iteration 14/25 | Loss: 0.00049211
Iteration 15/25 | Loss: 0.00049211
Iteration 16/25 | Loss: 0.00049211
Iteration 17/25 | Loss: 0.00049211
Iteration 18/25 | Loss: 0.00049211
Iteration 19/25 | Loss: 0.00049211
Iteration 20/25 | Loss: 0.00049211
Iteration 21/25 | Loss: 0.00049211
Iteration 22/25 | Loss: 0.00049211
Iteration 23/25 | Loss: 0.00049211
Iteration 24/25 | Loss: 0.00049211
Iteration 25/25 | Loss: 0.00049211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049211
Iteration 2/1000 | Loss: 0.00002605
Iteration 3/1000 | Loss: 0.00001823
Iteration 4/1000 | Loss: 0.00001620
Iteration 5/1000 | Loss: 0.00001519
Iteration 6/1000 | Loss: 0.00001459
Iteration 7/1000 | Loss: 0.00001423
Iteration 8/1000 | Loss: 0.00001398
Iteration 9/1000 | Loss: 0.00001383
Iteration 10/1000 | Loss: 0.00001382
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001371
Iteration 14/1000 | Loss: 0.00001369
Iteration 15/1000 | Loss: 0.00001368
Iteration 16/1000 | Loss: 0.00001368
Iteration 17/1000 | Loss: 0.00001367
Iteration 18/1000 | Loss: 0.00001367
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001359
Iteration 21/1000 | Loss: 0.00001354
Iteration 22/1000 | Loss: 0.00001351
Iteration 23/1000 | Loss: 0.00001350
Iteration 24/1000 | Loss: 0.00001350
Iteration 25/1000 | Loss: 0.00001346
Iteration 26/1000 | Loss: 0.00001345
Iteration 27/1000 | Loss: 0.00001342
Iteration 28/1000 | Loss: 0.00001342
Iteration 29/1000 | Loss: 0.00001341
Iteration 30/1000 | Loss: 0.00001340
Iteration 31/1000 | Loss: 0.00001340
Iteration 32/1000 | Loss: 0.00001339
Iteration 33/1000 | Loss: 0.00001339
Iteration 34/1000 | Loss: 0.00001338
Iteration 35/1000 | Loss: 0.00001338
Iteration 36/1000 | Loss: 0.00001336
Iteration 37/1000 | Loss: 0.00001335
Iteration 38/1000 | Loss: 0.00001335
Iteration 39/1000 | Loss: 0.00001335
Iteration 40/1000 | Loss: 0.00001335
Iteration 41/1000 | Loss: 0.00001334
Iteration 42/1000 | Loss: 0.00001334
Iteration 43/1000 | Loss: 0.00001334
Iteration 44/1000 | Loss: 0.00001333
Iteration 45/1000 | Loss: 0.00001332
Iteration 46/1000 | Loss: 0.00001332
Iteration 47/1000 | Loss: 0.00001332
Iteration 48/1000 | Loss: 0.00001331
Iteration 49/1000 | Loss: 0.00001331
Iteration 50/1000 | Loss: 0.00001331
Iteration 51/1000 | Loss: 0.00001331
Iteration 52/1000 | Loss: 0.00001331
Iteration 53/1000 | Loss: 0.00001331
Iteration 54/1000 | Loss: 0.00001331
Iteration 55/1000 | Loss: 0.00001331
Iteration 56/1000 | Loss: 0.00001330
Iteration 57/1000 | Loss: 0.00001330
Iteration 58/1000 | Loss: 0.00001330
Iteration 59/1000 | Loss: 0.00001330
Iteration 60/1000 | Loss: 0.00001330
Iteration 61/1000 | Loss: 0.00001330
Iteration 62/1000 | Loss: 0.00001330
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001329
Iteration 66/1000 | Loss: 0.00001329
Iteration 67/1000 | Loss: 0.00001329
Iteration 68/1000 | Loss: 0.00001329
Iteration 69/1000 | Loss: 0.00001329
Iteration 70/1000 | Loss: 0.00001328
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001328
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001327
Iteration 79/1000 | Loss: 0.00001327
Iteration 80/1000 | Loss: 0.00001327
Iteration 81/1000 | Loss: 0.00001326
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001326
Iteration 84/1000 | Loss: 0.00001326
Iteration 85/1000 | Loss: 0.00001326
Iteration 86/1000 | Loss: 0.00001326
Iteration 87/1000 | Loss: 0.00001326
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001325
Iteration 91/1000 | Loss: 0.00001325
Iteration 92/1000 | Loss: 0.00001325
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001325
Iteration 95/1000 | Loss: 0.00001325
Iteration 96/1000 | Loss: 0.00001325
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001324
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001324
Iteration 102/1000 | Loss: 0.00001324
Iteration 103/1000 | Loss: 0.00001324
Iteration 104/1000 | Loss: 0.00001323
Iteration 105/1000 | Loss: 0.00001323
Iteration 106/1000 | Loss: 0.00001323
Iteration 107/1000 | Loss: 0.00001323
Iteration 108/1000 | Loss: 0.00001323
Iteration 109/1000 | Loss: 0.00001322
Iteration 110/1000 | Loss: 0.00001322
Iteration 111/1000 | Loss: 0.00001322
Iteration 112/1000 | Loss: 0.00001322
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001322
Iteration 117/1000 | Loss: 0.00001322
Iteration 118/1000 | Loss: 0.00001322
Iteration 119/1000 | Loss: 0.00001322
Iteration 120/1000 | Loss: 0.00001322
Iteration 121/1000 | Loss: 0.00001322
Iteration 122/1000 | Loss: 0.00001322
Iteration 123/1000 | Loss: 0.00001321
Iteration 124/1000 | Loss: 0.00001321
Iteration 125/1000 | Loss: 0.00001321
Iteration 126/1000 | Loss: 0.00001321
Iteration 127/1000 | Loss: 0.00001321
Iteration 128/1000 | Loss: 0.00001321
Iteration 129/1000 | Loss: 0.00001321
Iteration 130/1000 | Loss: 0.00001321
Iteration 131/1000 | Loss: 0.00001321
Iteration 132/1000 | Loss: 0.00001321
Iteration 133/1000 | Loss: 0.00001321
Iteration 134/1000 | Loss: 0.00001321
Iteration 135/1000 | Loss: 0.00001321
Iteration 136/1000 | Loss: 0.00001321
Iteration 137/1000 | Loss: 0.00001321
Iteration 138/1000 | Loss: 0.00001321
Iteration 139/1000 | Loss: 0.00001321
Iteration 140/1000 | Loss: 0.00001320
Iteration 141/1000 | Loss: 0.00001320
Iteration 142/1000 | Loss: 0.00001320
Iteration 143/1000 | Loss: 0.00001320
Iteration 144/1000 | Loss: 0.00001320
Iteration 145/1000 | Loss: 0.00001320
Iteration 146/1000 | Loss: 0.00001320
Iteration 147/1000 | Loss: 0.00001320
Iteration 148/1000 | Loss: 0.00001320
Iteration 149/1000 | Loss: 0.00001320
Iteration 150/1000 | Loss: 0.00001320
Iteration 151/1000 | Loss: 0.00001320
Iteration 152/1000 | Loss: 0.00001320
Iteration 153/1000 | Loss: 0.00001320
Iteration 154/1000 | Loss: 0.00001320
Iteration 155/1000 | Loss: 0.00001320
Iteration 156/1000 | Loss: 0.00001320
Iteration 157/1000 | Loss: 0.00001320
Iteration 158/1000 | Loss: 0.00001320
Iteration 159/1000 | Loss: 0.00001320
Iteration 160/1000 | Loss: 0.00001320
Iteration 161/1000 | Loss: 0.00001320
Iteration 162/1000 | Loss: 0.00001320
Iteration 163/1000 | Loss: 0.00001320
Iteration 164/1000 | Loss: 0.00001320
Iteration 165/1000 | Loss: 0.00001320
Iteration 166/1000 | Loss: 0.00001320
Iteration 167/1000 | Loss: 0.00001320
Iteration 168/1000 | Loss: 0.00001320
Iteration 169/1000 | Loss: 0.00001320
Iteration 170/1000 | Loss: 0.00001320
Iteration 171/1000 | Loss: 0.00001320
Iteration 172/1000 | Loss: 0.00001320
Iteration 173/1000 | Loss: 0.00001320
Iteration 174/1000 | Loss: 0.00001320
Iteration 175/1000 | Loss: 0.00001320
Iteration 176/1000 | Loss: 0.00001320
Iteration 177/1000 | Loss: 0.00001320
Iteration 178/1000 | Loss: 0.00001320
Iteration 179/1000 | Loss: 0.00001320
Iteration 180/1000 | Loss: 0.00001320
Iteration 181/1000 | Loss: 0.00001320
Iteration 182/1000 | Loss: 0.00001320
Iteration 183/1000 | Loss: 0.00001320
Iteration 184/1000 | Loss: 0.00001320
Iteration 185/1000 | Loss: 0.00001320
Iteration 186/1000 | Loss: 0.00001320
Iteration 187/1000 | Loss: 0.00001320
Iteration 188/1000 | Loss: 0.00001320
Iteration 189/1000 | Loss: 0.00001320
Iteration 190/1000 | Loss: 0.00001320
Iteration 191/1000 | Loss: 0.00001320
Iteration 192/1000 | Loss: 0.00001320
Iteration 193/1000 | Loss: 0.00001320
Iteration 194/1000 | Loss: 0.00001320
Iteration 195/1000 | Loss: 0.00001320
Iteration 196/1000 | Loss: 0.00001320
Iteration 197/1000 | Loss: 0.00001320
Iteration 198/1000 | Loss: 0.00001320
Iteration 199/1000 | Loss: 0.00001320
Iteration 200/1000 | Loss: 0.00001320
Iteration 201/1000 | Loss: 0.00001320
Iteration 202/1000 | Loss: 0.00001320
Iteration 203/1000 | Loss: 0.00001320
Iteration 204/1000 | Loss: 0.00001320
Iteration 205/1000 | Loss: 0.00001320
Iteration 206/1000 | Loss: 0.00001320
Iteration 207/1000 | Loss: 0.00001320
Iteration 208/1000 | Loss: 0.00001320
Iteration 209/1000 | Loss: 0.00001320
Iteration 210/1000 | Loss: 0.00001320
Iteration 211/1000 | Loss: 0.00001320
Iteration 212/1000 | Loss: 0.00001320
Iteration 213/1000 | Loss: 0.00001320
Iteration 214/1000 | Loss: 0.00001320
Iteration 215/1000 | Loss: 0.00001320
Iteration 216/1000 | Loss: 0.00001320
Iteration 217/1000 | Loss: 0.00001320
Iteration 218/1000 | Loss: 0.00001320
Iteration 219/1000 | Loss: 0.00001320
Iteration 220/1000 | Loss: 0.00001320
Iteration 221/1000 | Loss: 0.00001320
Iteration 222/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.3204462447902188e-05, 1.3204462447902188e-05, 1.3204462447902188e-05, 1.3204462447902188e-05, 1.3204462447902188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3204462447902188e-05

Optimization complete. Final v2v error: 3.089148998260498 mm

Highest mean error: 4.28093957901001 mm for frame 44

Lowest mean error: 2.665781259536743 mm for frame 106

Saving results

Total time: 37.25418519973755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01134812
Iteration 2/25 | Loss: 0.00223136
Iteration 3/25 | Loss: 0.00142138
Iteration 4/25 | Loss: 0.00151552
Iteration 5/25 | Loss: 0.00140447
Iteration 6/25 | Loss: 0.00132866
Iteration 7/25 | Loss: 0.00137419
Iteration 8/25 | Loss: 0.00145772
Iteration 9/25 | Loss: 0.00122525
Iteration 10/25 | Loss: 0.00127664
Iteration 11/25 | Loss: 0.00107027
Iteration 12/25 | Loss: 0.00106007
Iteration 13/25 | Loss: 0.00105247
Iteration 14/25 | Loss: 0.00104876
Iteration 15/25 | Loss: 0.00105456
Iteration 16/25 | Loss: 0.00105599
Iteration 17/25 | Loss: 0.00105149
Iteration 18/25 | Loss: 0.00105105
Iteration 19/25 | Loss: 0.00104118
Iteration 20/25 | Loss: 0.00103600
Iteration 21/25 | Loss: 0.00104778
Iteration 22/25 | Loss: 0.00104543
Iteration 23/25 | Loss: 0.00104660
Iteration 24/25 | Loss: 0.00103291
Iteration 25/25 | Loss: 0.00104179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43648207
Iteration 2/25 | Loss: 0.00124529
Iteration 3/25 | Loss: 0.00120668
Iteration 4/25 | Loss: 0.00120668
Iteration 5/25 | Loss: 0.00120668
Iteration 6/25 | Loss: 0.00120668
Iteration 7/25 | Loss: 0.00120668
Iteration 8/25 | Loss: 0.00120668
Iteration 9/25 | Loss: 0.00120668
Iteration 10/25 | Loss: 0.00120668
Iteration 11/25 | Loss: 0.00120668
Iteration 12/25 | Loss: 0.00120668
Iteration 13/25 | Loss: 0.00120668
Iteration 14/25 | Loss: 0.00120668
Iteration 15/25 | Loss: 0.00120668
Iteration 16/25 | Loss: 0.00120668
Iteration 17/25 | Loss: 0.00120668
Iteration 18/25 | Loss: 0.00120668
Iteration 19/25 | Loss: 0.00120668
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012066756607964635, 0.0012066756607964635, 0.0012066756607964635, 0.0012066756607964635, 0.0012066756607964635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012066756607964635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120668
Iteration 2/1000 | Loss: 0.00083345
Iteration 3/1000 | Loss: 0.00080721
Iteration 4/1000 | Loss: 0.00090578
Iteration 5/1000 | Loss: 0.00096677
Iteration 6/1000 | Loss: 0.00105686
Iteration 7/1000 | Loss: 0.00050733
Iteration 8/1000 | Loss: 0.00058301
Iteration 9/1000 | Loss: 0.00101838
Iteration 10/1000 | Loss: 0.00084816
Iteration 11/1000 | Loss: 0.00071676
Iteration 12/1000 | Loss: 0.00089510
Iteration 13/1000 | Loss: 0.00063432
Iteration 14/1000 | Loss: 0.00097538
Iteration 15/1000 | Loss: 0.00083919
Iteration 16/1000 | Loss: 0.00088338
Iteration 17/1000 | Loss: 0.00093563
Iteration 18/1000 | Loss: 0.00075925
Iteration 19/1000 | Loss: 0.00109482
Iteration 20/1000 | Loss: 0.00084069
Iteration 21/1000 | Loss: 0.00075800
Iteration 22/1000 | Loss: 0.00117197
Iteration 23/1000 | Loss: 0.00106589
Iteration 24/1000 | Loss: 0.00109450
Iteration 25/1000 | Loss: 0.00090314
Iteration 26/1000 | Loss: 0.00073130
Iteration 27/1000 | Loss: 0.00093174
Iteration 28/1000 | Loss: 0.00084986
Iteration 29/1000 | Loss: 0.00074852
Iteration 30/1000 | Loss: 0.00086029
Iteration 31/1000 | Loss: 0.00102659
Iteration 32/1000 | Loss: 0.00063929
Iteration 33/1000 | Loss: 0.00049418
Iteration 34/1000 | Loss: 0.00086669
Iteration 35/1000 | Loss: 0.00097785
Iteration 36/1000 | Loss: 0.00104670
Iteration 37/1000 | Loss: 0.00096941
Iteration 38/1000 | Loss: 0.00090324
Iteration 39/1000 | Loss: 0.00087963
Iteration 40/1000 | Loss: 0.00120339
Iteration 41/1000 | Loss: 0.00114366
Iteration 42/1000 | Loss: 0.00078349
Iteration 43/1000 | Loss: 0.00097007
Iteration 44/1000 | Loss: 0.00078012
Iteration 45/1000 | Loss: 0.00083894
Iteration 46/1000 | Loss: 0.00099479
Iteration 47/1000 | Loss: 0.00127638
Iteration 48/1000 | Loss: 0.00088361
Iteration 49/1000 | Loss: 0.00061717
Iteration 50/1000 | Loss: 0.00088564
Iteration 51/1000 | Loss: 0.00102300
Iteration 52/1000 | Loss: 0.00084841
Iteration 53/1000 | Loss: 0.00097630
Iteration 54/1000 | Loss: 0.00089956
Iteration 55/1000 | Loss: 0.00102843
Iteration 56/1000 | Loss: 0.00103874
Iteration 57/1000 | Loss: 0.00061920
Iteration 58/1000 | Loss: 0.00056903
Iteration 59/1000 | Loss: 0.00128254
Iteration 60/1000 | Loss: 0.00093957
Iteration 61/1000 | Loss: 0.00102700
Iteration 62/1000 | Loss: 0.00101634
Iteration 63/1000 | Loss: 0.00070061
Iteration 64/1000 | Loss: 0.00063936
Iteration 65/1000 | Loss: 0.00099556
Iteration 66/1000 | Loss: 0.00138633
Iteration 67/1000 | Loss: 0.00197209
Iteration 68/1000 | Loss: 0.00106140
Iteration 69/1000 | Loss: 0.00062265
Iteration 70/1000 | Loss: 0.00100105
Iteration 71/1000 | Loss: 0.00117748
Iteration 72/1000 | Loss: 0.00098276
Iteration 73/1000 | Loss: 0.00103981
Iteration 74/1000 | Loss: 0.00101809
Iteration 75/1000 | Loss: 0.00097835
Iteration 76/1000 | Loss: 0.00126153
Iteration 77/1000 | Loss: 0.00097117
Iteration 78/1000 | Loss: 0.00157385
Iteration 79/1000 | Loss: 0.00111886
Iteration 80/1000 | Loss: 0.00053447
Iteration 81/1000 | Loss: 0.00052228
Iteration 82/1000 | Loss: 0.00050368
Iteration 83/1000 | Loss: 0.00058301
Iteration 84/1000 | Loss: 0.00062333
Iteration 85/1000 | Loss: 0.00065525
Iteration 86/1000 | Loss: 0.00056613
Iteration 87/1000 | Loss: 0.00070516
Iteration 88/1000 | Loss: 0.00062106
Iteration 89/1000 | Loss: 0.00067397
Iteration 90/1000 | Loss: 0.00065352
Iteration 91/1000 | Loss: 0.00049429
Iteration 92/1000 | Loss: 0.00032762
Iteration 93/1000 | Loss: 0.00036209
Iteration 94/1000 | Loss: 0.00042700
Iteration 95/1000 | Loss: 0.00065142
Iteration 96/1000 | Loss: 0.00030715
Iteration 97/1000 | Loss: 0.00037055
Iteration 98/1000 | Loss: 0.00052546
Iteration 99/1000 | Loss: 0.00045719
Iteration 100/1000 | Loss: 0.00052104
Iteration 101/1000 | Loss: 0.00080490
Iteration 102/1000 | Loss: 0.00074511
Iteration 103/1000 | Loss: 0.00074170
Iteration 104/1000 | Loss: 0.00067528
Iteration 105/1000 | Loss: 0.00055988
Iteration 106/1000 | Loss: 0.00042305
Iteration 107/1000 | Loss: 0.00076645
Iteration 108/1000 | Loss: 0.00042809
Iteration 109/1000 | Loss: 0.00046788
Iteration 110/1000 | Loss: 0.00060340
Iteration 111/1000 | Loss: 0.00054552
Iteration 112/1000 | Loss: 0.00131991
Iteration 113/1000 | Loss: 0.00130154
Iteration 114/1000 | Loss: 0.00093750
Iteration 115/1000 | Loss: 0.00053416
Iteration 116/1000 | Loss: 0.00055809
Iteration 117/1000 | Loss: 0.00048858
Iteration 118/1000 | Loss: 0.00032033
Iteration 119/1000 | Loss: 0.00037564
Iteration 120/1000 | Loss: 0.00070002
Iteration 121/1000 | Loss: 0.00043842
Iteration 122/1000 | Loss: 0.00020320
Iteration 123/1000 | Loss: 0.00020941
Iteration 124/1000 | Loss: 0.00018811
Iteration 125/1000 | Loss: 0.00013172
Iteration 126/1000 | Loss: 0.00012361
Iteration 127/1000 | Loss: 0.00031175
Iteration 128/1000 | Loss: 0.00026688
Iteration 129/1000 | Loss: 0.00006913
Iteration 130/1000 | Loss: 0.00037432
Iteration 131/1000 | Loss: 0.00041796
Iteration 132/1000 | Loss: 0.00067035
Iteration 133/1000 | Loss: 0.00061506
Iteration 134/1000 | Loss: 0.00029843
Iteration 135/1000 | Loss: 0.00069190
Iteration 136/1000 | Loss: 0.00043187
Iteration 137/1000 | Loss: 0.00068561
Iteration 138/1000 | Loss: 0.00040820
Iteration 139/1000 | Loss: 0.00013826
Iteration 140/1000 | Loss: 0.00016575
Iteration 141/1000 | Loss: 0.00016227
Iteration 142/1000 | Loss: 0.00018918
Iteration 143/1000 | Loss: 0.00033606
Iteration 144/1000 | Loss: 0.00066351
Iteration 145/1000 | Loss: 0.00039377
Iteration 146/1000 | Loss: 0.00075082
Iteration 147/1000 | Loss: 0.00064072
Iteration 148/1000 | Loss: 0.00085047
Iteration 149/1000 | Loss: 0.00063533
Iteration 150/1000 | Loss: 0.00088493
Iteration 151/1000 | Loss: 0.00030601
Iteration 152/1000 | Loss: 0.00057290
Iteration 153/1000 | Loss: 0.00035704
Iteration 154/1000 | Loss: 0.00030918
Iteration 155/1000 | Loss: 0.00012979
Iteration 156/1000 | Loss: 0.00038910
Iteration 157/1000 | Loss: 0.00016307
Iteration 158/1000 | Loss: 0.00007799
Iteration 159/1000 | Loss: 0.00031878
Iteration 160/1000 | Loss: 0.00018605
Iteration 161/1000 | Loss: 0.00020364
Iteration 162/1000 | Loss: 0.00046406
Iteration 163/1000 | Loss: 0.00180362
Iteration 164/1000 | Loss: 0.00083325
Iteration 165/1000 | Loss: 0.00022665
Iteration 166/1000 | Loss: 0.00013965
Iteration 167/1000 | Loss: 0.00004323
Iteration 168/1000 | Loss: 0.00018546
Iteration 169/1000 | Loss: 0.00042376
Iteration 170/1000 | Loss: 0.00061673
Iteration 171/1000 | Loss: 0.00058555
Iteration 172/1000 | Loss: 0.00026570
Iteration 173/1000 | Loss: 0.00026576
Iteration 174/1000 | Loss: 0.00025556
Iteration 175/1000 | Loss: 0.00024510
Iteration 176/1000 | Loss: 0.00026894
Iteration 177/1000 | Loss: 0.00003262
Iteration 178/1000 | Loss: 0.00002495
Iteration 179/1000 | Loss: 0.00002216
Iteration 180/1000 | Loss: 0.00020312
Iteration 181/1000 | Loss: 0.00025922
Iteration 182/1000 | Loss: 0.00024082
Iteration 183/1000 | Loss: 0.00025774
Iteration 184/1000 | Loss: 0.00022020
Iteration 185/1000 | Loss: 0.00027407
Iteration 186/1000 | Loss: 0.00003285
Iteration 187/1000 | Loss: 0.00002294
Iteration 188/1000 | Loss: 0.00001899
Iteration 189/1000 | Loss: 0.00027896
Iteration 190/1000 | Loss: 0.00004139
Iteration 191/1000 | Loss: 0.00002446
Iteration 192/1000 | Loss: 0.00002177
Iteration 193/1000 | Loss: 0.00001769
Iteration 194/1000 | Loss: 0.00023451
Iteration 195/1000 | Loss: 0.00002530
Iteration 196/1000 | Loss: 0.00001863
Iteration 197/1000 | Loss: 0.00021600
Iteration 198/1000 | Loss: 0.00002506
Iteration 199/1000 | Loss: 0.00003306
Iteration 200/1000 | Loss: 0.00002268
Iteration 201/1000 | Loss: 0.00001791
Iteration 202/1000 | Loss: 0.00001722
Iteration 203/1000 | Loss: 0.00001670
Iteration 204/1000 | Loss: 0.00001612
Iteration 205/1000 | Loss: 0.00009317
Iteration 206/1000 | Loss: 0.00002130
Iteration 207/1000 | Loss: 0.00002310
Iteration 208/1000 | Loss: 0.00001684
Iteration 209/1000 | Loss: 0.00001533
Iteration 210/1000 | Loss: 0.00001468
Iteration 211/1000 | Loss: 0.00001436
Iteration 212/1000 | Loss: 0.00001411
Iteration 213/1000 | Loss: 0.00001385
Iteration 214/1000 | Loss: 0.00001383
Iteration 215/1000 | Loss: 0.00001370
Iteration 216/1000 | Loss: 0.00001368
Iteration 217/1000 | Loss: 0.00001367
Iteration 218/1000 | Loss: 0.00001366
Iteration 219/1000 | Loss: 0.00001366
Iteration 220/1000 | Loss: 0.00001366
Iteration 221/1000 | Loss: 0.00001365
Iteration 222/1000 | Loss: 0.00001365
Iteration 223/1000 | Loss: 0.00001365
Iteration 224/1000 | Loss: 0.00001361
Iteration 225/1000 | Loss: 0.00001360
Iteration 226/1000 | Loss: 0.00001359
Iteration 227/1000 | Loss: 0.00001358
Iteration 228/1000 | Loss: 0.00001358
Iteration 229/1000 | Loss: 0.00001358
Iteration 230/1000 | Loss: 0.00001357
Iteration 231/1000 | Loss: 0.00001357
Iteration 232/1000 | Loss: 0.00001357
Iteration 233/1000 | Loss: 0.00001357
Iteration 234/1000 | Loss: 0.00001356
Iteration 235/1000 | Loss: 0.00001356
Iteration 236/1000 | Loss: 0.00001356
Iteration 237/1000 | Loss: 0.00001356
Iteration 238/1000 | Loss: 0.00001356
Iteration 239/1000 | Loss: 0.00001356
Iteration 240/1000 | Loss: 0.00001355
Iteration 241/1000 | Loss: 0.00001355
Iteration 242/1000 | Loss: 0.00001355
Iteration 243/1000 | Loss: 0.00001355
Iteration 244/1000 | Loss: 0.00001355
Iteration 245/1000 | Loss: 0.00001354
Iteration 246/1000 | Loss: 0.00001354
Iteration 247/1000 | Loss: 0.00001354
Iteration 248/1000 | Loss: 0.00001354
Iteration 249/1000 | Loss: 0.00001354
Iteration 250/1000 | Loss: 0.00001354
Iteration 251/1000 | Loss: 0.00001354
Iteration 252/1000 | Loss: 0.00001354
Iteration 253/1000 | Loss: 0.00001354
Iteration 254/1000 | Loss: 0.00001353
Iteration 255/1000 | Loss: 0.00001353
Iteration 256/1000 | Loss: 0.00001350
Iteration 257/1000 | Loss: 0.00001350
Iteration 258/1000 | Loss: 0.00001350
Iteration 259/1000 | Loss: 0.00001350
Iteration 260/1000 | Loss: 0.00001350
Iteration 261/1000 | Loss: 0.00001350
Iteration 262/1000 | Loss: 0.00001350
Iteration 263/1000 | Loss: 0.00001349
Iteration 264/1000 | Loss: 0.00001349
Iteration 265/1000 | Loss: 0.00001349
Iteration 266/1000 | Loss: 0.00001348
Iteration 267/1000 | Loss: 0.00001347
Iteration 268/1000 | Loss: 0.00001347
Iteration 269/1000 | Loss: 0.00001346
Iteration 270/1000 | Loss: 0.00001346
Iteration 271/1000 | Loss: 0.00001346
Iteration 272/1000 | Loss: 0.00001346
Iteration 273/1000 | Loss: 0.00001346
Iteration 274/1000 | Loss: 0.00001346
Iteration 275/1000 | Loss: 0.00001346
Iteration 276/1000 | Loss: 0.00001346
Iteration 277/1000 | Loss: 0.00001345
Iteration 278/1000 | Loss: 0.00001345
Iteration 279/1000 | Loss: 0.00001345
Iteration 280/1000 | Loss: 0.00001345
Iteration 281/1000 | Loss: 0.00001344
Iteration 282/1000 | Loss: 0.00001344
Iteration 283/1000 | Loss: 0.00001344
Iteration 284/1000 | Loss: 0.00001344
Iteration 285/1000 | Loss: 0.00001344
Iteration 286/1000 | Loss: 0.00001344
Iteration 287/1000 | Loss: 0.00001343
Iteration 288/1000 | Loss: 0.00001343
Iteration 289/1000 | Loss: 0.00001343
Iteration 290/1000 | Loss: 0.00001343
Iteration 291/1000 | Loss: 0.00001343
Iteration 292/1000 | Loss: 0.00001343
Iteration 293/1000 | Loss: 0.00001343
Iteration 294/1000 | Loss: 0.00001342
Iteration 295/1000 | Loss: 0.00001342
Iteration 296/1000 | Loss: 0.00001342
Iteration 297/1000 | Loss: 0.00001342
Iteration 298/1000 | Loss: 0.00001342
Iteration 299/1000 | Loss: 0.00001342
Iteration 300/1000 | Loss: 0.00001342
Iteration 301/1000 | Loss: 0.00001342
Iteration 302/1000 | Loss: 0.00001341
Iteration 303/1000 | Loss: 0.00001341
Iteration 304/1000 | Loss: 0.00001341
Iteration 305/1000 | Loss: 0.00001341
Iteration 306/1000 | Loss: 0.00001341
Iteration 307/1000 | Loss: 0.00001341
Iteration 308/1000 | Loss: 0.00001341
Iteration 309/1000 | Loss: 0.00001341
Iteration 310/1000 | Loss: 0.00001341
Iteration 311/1000 | Loss: 0.00001341
Iteration 312/1000 | Loss: 0.00001341
Iteration 313/1000 | Loss: 0.00001341
Iteration 314/1000 | Loss: 0.00001341
Iteration 315/1000 | Loss: 0.00001341
Iteration 316/1000 | Loss: 0.00001341
Iteration 317/1000 | Loss: 0.00001340
Iteration 318/1000 | Loss: 0.00001340
Iteration 319/1000 | Loss: 0.00001340
Iteration 320/1000 | Loss: 0.00001340
Iteration 321/1000 | Loss: 0.00001340
Iteration 322/1000 | Loss: 0.00001340
Iteration 323/1000 | Loss: 0.00001340
Iteration 324/1000 | Loss: 0.00001340
Iteration 325/1000 | Loss: 0.00001340
Iteration 326/1000 | Loss: 0.00001339
Iteration 327/1000 | Loss: 0.00001339
Iteration 328/1000 | Loss: 0.00001339
Iteration 329/1000 | Loss: 0.00001339
Iteration 330/1000 | Loss: 0.00001339
Iteration 331/1000 | Loss: 0.00001339
Iteration 332/1000 | Loss: 0.00001339
Iteration 333/1000 | Loss: 0.00001339
Iteration 334/1000 | Loss: 0.00001339
Iteration 335/1000 | Loss: 0.00001339
Iteration 336/1000 | Loss: 0.00001339
Iteration 337/1000 | Loss: 0.00001339
Iteration 338/1000 | Loss: 0.00001339
Iteration 339/1000 | Loss: 0.00001338
Iteration 340/1000 | Loss: 0.00001338
Iteration 341/1000 | Loss: 0.00001338
Iteration 342/1000 | Loss: 0.00001338
Iteration 343/1000 | Loss: 0.00001338
Iteration 344/1000 | Loss: 0.00001338
Iteration 345/1000 | Loss: 0.00001338
Iteration 346/1000 | Loss: 0.00001338
Iteration 347/1000 | Loss: 0.00001338
Iteration 348/1000 | Loss: 0.00001338
Iteration 349/1000 | Loss: 0.00001338
Iteration 350/1000 | Loss: 0.00001338
Iteration 351/1000 | Loss: 0.00001338
Iteration 352/1000 | Loss: 0.00001338
Iteration 353/1000 | Loss: 0.00001338
Iteration 354/1000 | Loss: 0.00001338
Iteration 355/1000 | Loss: 0.00001338
Iteration 356/1000 | Loss: 0.00001338
Iteration 357/1000 | Loss: 0.00001338
Iteration 358/1000 | Loss: 0.00001338
Iteration 359/1000 | Loss: 0.00001338
Iteration 360/1000 | Loss: 0.00001338
Iteration 361/1000 | Loss: 0.00001338
Iteration 362/1000 | Loss: 0.00001338
Iteration 363/1000 | Loss: 0.00001338
Iteration 364/1000 | Loss: 0.00001338
Iteration 365/1000 | Loss: 0.00001338
Iteration 366/1000 | Loss: 0.00001338
Iteration 367/1000 | Loss: 0.00001338
Iteration 368/1000 | Loss: 0.00001338
Iteration 369/1000 | Loss: 0.00001338
Iteration 370/1000 | Loss: 0.00001338
Iteration 371/1000 | Loss: 0.00001338
Iteration 372/1000 | Loss: 0.00001338
Iteration 373/1000 | Loss: 0.00001338
Iteration 374/1000 | Loss: 0.00001338
Iteration 375/1000 | Loss: 0.00001338
Iteration 376/1000 | Loss: 0.00001338
Iteration 377/1000 | Loss: 0.00001338
Iteration 378/1000 | Loss: 0.00001338
Iteration 379/1000 | Loss: 0.00001338
Iteration 380/1000 | Loss: 0.00001338
Iteration 381/1000 | Loss: 0.00001338
Iteration 382/1000 | Loss: 0.00001338
Iteration 383/1000 | Loss: 0.00001338
Iteration 384/1000 | Loss: 0.00001338
Iteration 385/1000 | Loss: 0.00001338
Iteration 386/1000 | Loss: 0.00001338
Iteration 387/1000 | Loss: 0.00001338
Iteration 388/1000 | Loss: 0.00001338
Iteration 389/1000 | Loss: 0.00001338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 389. Stopping optimization.
Last 5 losses: [1.3378347830439452e-05, 1.3378347830439452e-05, 1.3378347830439452e-05, 1.3378347830439452e-05, 1.3378347830439452e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3378347830439452e-05

Optimization complete. Final v2v error: 3.2064616680145264 mm

Highest mean error: 4.422083854675293 mm for frame 57

Lowest mean error: 2.6077868938446045 mm for frame 124

Saving results

Total time: 352.2747130393982
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090347
Iteration 2/25 | Loss: 0.01090347
Iteration 3/25 | Loss: 0.01090347
Iteration 4/25 | Loss: 0.01090347
Iteration 5/25 | Loss: 0.01090347
Iteration 6/25 | Loss: 0.01090347
Iteration 7/25 | Loss: 0.01090347
Iteration 8/25 | Loss: 0.01090347
Iteration 9/25 | Loss: 0.01090347
Iteration 10/25 | Loss: 0.01090346
Iteration 11/25 | Loss: 0.01090346
Iteration 12/25 | Loss: 0.01090346
Iteration 13/25 | Loss: 0.01090346
Iteration 14/25 | Loss: 0.01090346
Iteration 15/25 | Loss: 0.01090346
Iteration 16/25 | Loss: 0.01090346
Iteration 17/25 | Loss: 0.01090346
Iteration 18/25 | Loss: 0.01090346
Iteration 19/25 | Loss: 0.01090346
Iteration 20/25 | Loss: 0.01090346
Iteration 21/25 | Loss: 0.01090346
Iteration 22/25 | Loss: 0.01090346
Iteration 23/25 | Loss: 0.01090345
Iteration 24/25 | Loss: 0.01090345
Iteration 25/25 | Loss: 0.01090345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58899093
Iteration 2/25 | Loss: 0.10860195
Iteration 3/25 | Loss: 0.10850511
Iteration 4/25 | Loss: 0.10850510
Iteration 5/25 | Loss: 0.10850510
Iteration 6/25 | Loss: 0.10850510
Iteration 7/25 | Loss: 0.10850510
Iteration 8/25 | Loss: 0.10850510
Iteration 9/25 | Loss: 0.10850510
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.10850510001182556, 0.10850510001182556, 0.10850510001182556, 0.10850510001182556, 0.10850510001182556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10850510001182556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10850510
Iteration 2/1000 | Loss: 0.00327624
Iteration 3/1000 | Loss: 0.00264749
Iteration 4/1000 | Loss: 0.00190475
Iteration 5/1000 | Loss: 0.00040174
Iteration 6/1000 | Loss: 0.00069951
Iteration 7/1000 | Loss: 0.00159209
Iteration 8/1000 | Loss: 0.00023831
Iteration 9/1000 | Loss: 0.00068217
Iteration 10/1000 | Loss: 0.00113690
Iteration 11/1000 | Loss: 0.00008506
Iteration 12/1000 | Loss: 0.00026507
Iteration 13/1000 | Loss: 0.00014698
Iteration 14/1000 | Loss: 0.00016357
Iteration 15/1000 | Loss: 0.00129328
Iteration 16/1000 | Loss: 0.00011020
Iteration 17/1000 | Loss: 0.00018162
Iteration 18/1000 | Loss: 0.00006451
Iteration 19/1000 | Loss: 0.00027181
Iteration 20/1000 | Loss: 0.00004429
Iteration 21/1000 | Loss: 0.00013308
Iteration 22/1000 | Loss: 0.00004085
Iteration 23/1000 | Loss: 0.00008237
Iteration 24/1000 | Loss: 0.00008748
Iteration 25/1000 | Loss: 0.00001867
Iteration 26/1000 | Loss: 0.00014077
Iteration 27/1000 | Loss: 0.00003912
Iteration 28/1000 | Loss: 0.00006616
Iteration 29/1000 | Loss: 0.00004561
Iteration 30/1000 | Loss: 0.00003280
Iteration 31/1000 | Loss: 0.00007466
Iteration 32/1000 | Loss: 0.00002124
Iteration 33/1000 | Loss: 0.00001671
Iteration 34/1000 | Loss: 0.00003830
Iteration 35/1000 | Loss: 0.00002523
Iteration 36/1000 | Loss: 0.00002469
Iteration 37/1000 | Loss: 0.00002317
Iteration 38/1000 | Loss: 0.00002993
Iteration 39/1000 | Loss: 0.00001861
Iteration 40/1000 | Loss: 0.00003217
Iteration 41/1000 | Loss: 0.00012972
Iteration 42/1000 | Loss: 0.00004809
Iteration 43/1000 | Loss: 0.00004786
Iteration 44/1000 | Loss: 0.00002736
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00002295
Iteration 47/1000 | Loss: 0.00001381
Iteration 48/1000 | Loss: 0.00005680
Iteration 49/1000 | Loss: 0.00003189
Iteration 50/1000 | Loss: 0.00002118
Iteration 51/1000 | Loss: 0.00009710
Iteration 52/1000 | Loss: 0.00023257
Iteration 53/1000 | Loss: 0.00014743
Iteration 54/1000 | Loss: 0.00007640
Iteration 55/1000 | Loss: 0.00002513
Iteration 56/1000 | Loss: 0.00002044
Iteration 57/1000 | Loss: 0.00013145
Iteration 58/1000 | Loss: 0.00001607
Iteration 59/1000 | Loss: 0.00003228
Iteration 60/1000 | Loss: 0.00009165
Iteration 61/1000 | Loss: 0.00002207
Iteration 62/1000 | Loss: 0.00002571
Iteration 63/1000 | Loss: 0.00002292
Iteration 64/1000 | Loss: 0.00026809
Iteration 65/1000 | Loss: 0.00003980
Iteration 66/1000 | Loss: 0.00001528
Iteration 67/1000 | Loss: 0.00002200
Iteration 68/1000 | Loss: 0.00001446
Iteration 69/1000 | Loss: 0.00002041
Iteration 70/1000 | Loss: 0.00001457
Iteration 71/1000 | Loss: 0.00001356
Iteration 72/1000 | Loss: 0.00001686
Iteration 73/1000 | Loss: 0.00001413
Iteration 74/1000 | Loss: 0.00001329
Iteration 75/1000 | Loss: 0.00001329
Iteration 76/1000 | Loss: 0.00001329
Iteration 77/1000 | Loss: 0.00001329
Iteration 78/1000 | Loss: 0.00001329
Iteration 79/1000 | Loss: 0.00001329
Iteration 80/1000 | Loss: 0.00001329
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001328
Iteration 84/1000 | Loss: 0.00001328
Iteration 85/1000 | Loss: 0.00001328
Iteration 86/1000 | Loss: 0.00001327
Iteration 87/1000 | Loss: 0.00001327
Iteration 88/1000 | Loss: 0.00001327
Iteration 89/1000 | Loss: 0.00001327
Iteration 90/1000 | Loss: 0.00001327
Iteration 91/1000 | Loss: 0.00001327
Iteration 92/1000 | Loss: 0.00001327
Iteration 93/1000 | Loss: 0.00001327
Iteration 94/1000 | Loss: 0.00001327
Iteration 95/1000 | Loss: 0.00001327
Iteration 96/1000 | Loss: 0.00001327
Iteration 97/1000 | Loss: 0.00001327
Iteration 98/1000 | Loss: 0.00001327
Iteration 99/1000 | Loss: 0.00001327
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001326
Iteration 102/1000 | Loss: 0.00001326
Iteration 103/1000 | Loss: 0.00001326
Iteration 104/1000 | Loss: 0.00001326
Iteration 105/1000 | Loss: 0.00001326
Iteration 106/1000 | Loss: 0.00001326
Iteration 107/1000 | Loss: 0.00001326
Iteration 108/1000 | Loss: 0.00001326
Iteration 109/1000 | Loss: 0.00001326
Iteration 110/1000 | Loss: 0.00001326
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001325
Iteration 114/1000 | Loss: 0.00001325
Iteration 115/1000 | Loss: 0.00001325
Iteration 116/1000 | Loss: 0.00001325
Iteration 117/1000 | Loss: 0.00001325
Iteration 118/1000 | Loss: 0.00001325
Iteration 119/1000 | Loss: 0.00001325
Iteration 120/1000 | Loss: 0.00001325
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001325
Iteration 125/1000 | Loss: 0.00001324
Iteration 126/1000 | Loss: 0.00001324
Iteration 127/1000 | Loss: 0.00001324
Iteration 128/1000 | Loss: 0.00001324
Iteration 129/1000 | Loss: 0.00001324
Iteration 130/1000 | Loss: 0.00001324
Iteration 131/1000 | Loss: 0.00001324
Iteration 132/1000 | Loss: 0.00001324
Iteration 133/1000 | Loss: 0.00001324
Iteration 134/1000 | Loss: 0.00001324
Iteration 135/1000 | Loss: 0.00001324
Iteration 136/1000 | Loss: 0.00001324
Iteration 137/1000 | Loss: 0.00001324
Iteration 138/1000 | Loss: 0.00001324
Iteration 139/1000 | Loss: 0.00001324
Iteration 140/1000 | Loss: 0.00001324
Iteration 141/1000 | Loss: 0.00001324
Iteration 142/1000 | Loss: 0.00001324
Iteration 143/1000 | Loss: 0.00001813
Iteration 144/1000 | Loss: 0.00001324
Iteration 145/1000 | Loss: 0.00001324
Iteration 146/1000 | Loss: 0.00001324
Iteration 147/1000 | Loss: 0.00001324
Iteration 148/1000 | Loss: 0.00001324
Iteration 149/1000 | Loss: 0.00001324
Iteration 150/1000 | Loss: 0.00001324
Iteration 151/1000 | Loss: 0.00001324
Iteration 152/1000 | Loss: 0.00002082
Iteration 153/1000 | Loss: 0.00001360
Iteration 154/1000 | Loss: 0.00001353
Iteration 155/1000 | Loss: 0.00001376
Iteration 156/1000 | Loss: 0.00001331
Iteration 157/1000 | Loss: 0.00001325
Iteration 158/1000 | Loss: 0.00001324
Iteration 159/1000 | Loss: 0.00001324
Iteration 160/1000 | Loss: 0.00001324
Iteration 161/1000 | Loss: 0.00001324
Iteration 162/1000 | Loss: 0.00001324
Iteration 163/1000 | Loss: 0.00001324
Iteration 164/1000 | Loss: 0.00001324
Iteration 165/1000 | Loss: 0.00001324
Iteration 166/1000 | Loss: 0.00001324
Iteration 167/1000 | Loss: 0.00001324
Iteration 168/1000 | Loss: 0.00001324
Iteration 169/1000 | Loss: 0.00001324
Iteration 170/1000 | Loss: 0.00001324
Iteration 171/1000 | Loss: 0.00001324
Iteration 172/1000 | Loss: 0.00001324
Iteration 173/1000 | Loss: 0.00002206
Iteration 174/1000 | Loss: 0.00001326
Iteration 175/1000 | Loss: 0.00001326
Iteration 176/1000 | Loss: 0.00001326
Iteration 177/1000 | Loss: 0.00001326
Iteration 178/1000 | Loss: 0.00001325
Iteration 179/1000 | Loss: 0.00002416
Iteration 180/1000 | Loss: 0.00001657
Iteration 181/1000 | Loss: 0.00001564
Iteration 182/1000 | Loss: 0.00001324
Iteration 183/1000 | Loss: 0.00001324
Iteration 184/1000 | Loss: 0.00001323
Iteration 185/1000 | Loss: 0.00001323
Iteration 186/1000 | Loss: 0.00001323
Iteration 187/1000 | Loss: 0.00001323
Iteration 188/1000 | Loss: 0.00001323
Iteration 189/1000 | Loss: 0.00001323
Iteration 190/1000 | Loss: 0.00001323
Iteration 191/1000 | Loss: 0.00001323
Iteration 192/1000 | Loss: 0.00001323
Iteration 193/1000 | Loss: 0.00001323
Iteration 194/1000 | Loss: 0.00001322
Iteration 195/1000 | Loss: 0.00001322
Iteration 196/1000 | Loss: 0.00001322
Iteration 197/1000 | Loss: 0.00001322
Iteration 198/1000 | Loss: 0.00001322
Iteration 199/1000 | Loss: 0.00001322
Iteration 200/1000 | Loss: 0.00001322
Iteration 201/1000 | Loss: 0.00001322
Iteration 202/1000 | Loss: 0.00001322
Iteration 203/1000 | Loss: 0.00001322
Iteration 204/1000 | Loss: 0.00001322
Iteration 205/1000 | Loss: 0.00001322
Iteration 206/1000 | Loss: 0.00001322
Iteration 207/1000 | Loss: 0.00001322
Iteration 208/1000 | Loss: 0.00001322
Iteration 209/1000 | Loss: 0.00001322
Iteration 210/1000 | Loss: 0.00001322
Iteration 211/1000 | Loss: 0.00001322
Iteration 212/1000 | Loss: 0.00001322
Iteration 213/1000 | Loss: 0.00001322
Iteration 214/1000 | Loss: 0.00001322
Iteration 215/1000 | Loss: 0.00001322
Iteration 216/1000 | Loss: 0.00001322
Iteration 217/1000 | Loss: 0.00001322
Iteration 218/1000 | Loss: 0.00001322
Iteration 219/1000 | Loss: 0.00001322
Iteration 220/1000 | Loss: 0.00001322
Iteration 221/1000 | Loss: 0.00001322
Iteration 222/1000 | Loss: 0.00001322
Iteration 223/1000 | Loss: 0.00001322
Iteration 224/1000 | Loss: 0.00001322
Iteration 225/1000 | Loss: 0.00001322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.322470143350074e-05, 1.322470143350074e-05, 1.322470143350074e-05, 1.322470143350074e-05, 1.322470143350074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.322470143350074e-05

Optimization complete. Final v2v error: 3.0608720779418945 mm

Highest mean error: 3.2634661197662354 mm for frame 53

Lowest mean error: 2.820995330810547 mm for frame 223

Saving results

Total time: 138.24647045135498
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896046
Iteration 2/25 | Loss: 0.00106130
Iteration 3/25 | Loss: 0.00098581
Iteration 4/25 | Loss: 0.00097438
Iteration 5/25 | Loss: 0.00097056
Iteration 6/25 | Loss: 0.00096957
Iteration 7/25 | Loss: 0.00096953
Iteration 8/25 | Loss: 0.00096953
Iteration 9/25 | Loss: 0.00096953
Iteration 10/25 | Loss: 0.00096953
Iteration 11/25 | Loss: 0.00096953
Iteration 12/25 | Loss: 0.00096953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009695346234366298, 0.0009695346234366298, 0.0009695346234366298, 0.0009695346234366298, 0.0009695346234366298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009695346234366298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.39920282
Iteration 2/25 | Loss: 0.00046282
Iteration 3/25 | Loss: 0.00046282
Iteration 4/25 | Loss: 0.00046282
Iteration 5/25 | Loss: 0.00046282
Iteration 6/25 | Loss: 0.00046282
Iteration 7/25 | Loss: 0.00046282
Iteration 8/25 | Loss: 0.00046282
Iteration 9/25 | Loss: 0.00046282
Iteration 10/25 | Loss: 0.00046282
Iteration 11/25 | Loss: 0.00046282
Iteration 12/25 | Loss: 0.00046282
Iteration 13/25 | Loss: 0.00046282
Iteration 14/25 | Loss: 0.00046282
Iteration 15/25 | Loss: 0.00046282
Iteration 16/25 | Loss: 0.00046282
Iteration 17/25 | Loss: 0.00046282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00046281571849249303, 0.00046281571849249303, 0.00046281571849249303, 0.00046281571849249303, 0.00046281571849249303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046281571849249303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046282
Iteration 2/1000 | Loss: 0.00002504
Iteration 3/1000 | Loss: 0.00001804
Iteration 4/1000 | Loss: 0.00001630
Iteration 5/1000 | Loss: 0.00001559
Iteration 6/1000 | Loss: 0.00001501
Iteration 7/1000 | Loss: 0.00001454
Iteration 8/1000 | Loss: 0.00001430
Iteration 9/1000 | Loss: 0.00001420
Iteration 10/1000 | Loss: 0.00001417
Iteration 11/1000 | Loss: 0.00001416
Iteration 12/1000 | Loss: 0.00001413
Iteration 13/1000 | Loss: 0.00001413
Iteration 14/1000 | Loss: 0.00001411
Iteration 15/1000 | Loss: 0.00001404
Iteration 16/1000 | Loss: 0.00001402
Iteration 17/1000 | Loss: 0.00001401
Iteration 18/1000 | Loss: 0.00001401
Iteration 19/1000 | Loss: 0.00001400
Iteration 20/1000 | Loss: 0.00001400
Iteration 21/1000 | Loss: 0.00001399
Iteration 22/1000 | Loss: 0.00001398
Iteration 23/1000 | Loss: 0.00001397
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001395
Iteration 26/1000 | Loss: 0.00001394
Iteration 27/1000 | Loss: 0.00001394
Iteration 28/1000 | Loss: 0.00001393
Iteration 29/1000 | Loss: 0.00001391
Iteration 30/1000 | Loss: 0.00001390
Iteration 31/1000 | Loss: 0.00001389
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001389
Iteration 34/1000 | Loss: 0.00001389
Iteration 35/1000 | Loss: 0.00001389
Iteration 36/1000 | Loss: 0.00001389
Iteration 37/1000 | Loss: 0.00001389
Iteration 38/1000 | Loss: 0.00001388
Iteration 39/1000 | Loss: 0.00001388
Iteration 40/1000 | Loss: 0.00001388
Iteration 41/1000 | Loss: 0.00001388
Iteration 42/1000 | Loss: 0.00001387
Iteration 43/1000 | Loss: 0.00001386
Iteration 44/1000 | Loss: 0.00001385
Iteration 45/1000 | Loss: 0.00001384
Iteration 46/1000 | Loss: 0.00001384
Iteration 47/1000 | Loss: 0.00001384
Iteration 48/1000 | Loss: 0.00001383
Iteration 49/1000 | Loss: 0.00001383
Iteration 50/1000 | Loss: 0.00001380
Iteration 51/1000 | Loss: 0.00001380
Iteration 52/1000 | Loss: 0.00001380
Iteration 53/1000 | Loss: 0.00001380
Iteration 54/1000 | Loss: 0.00001380
Iteration 55/1000 | Loss: 0.00001379
Iteration 56/1000 | Loss: 0.00001379
Iteration 57/1000 | Loss: 0.00001379
Iteration 58/1000 | Loss: 0.00001379
Iteration 59/1000 | Loss: 0.00001379
Iteration 60/1000 | Loss: 0.00001379
Iteration 61/1000 | Loss: 0.00001379
Iteration 62/1000 | Loss: 0.00001378
Iteration 63/1000 | Loss: 0.00001378
Iteration 64/1000 | Loss: 0.00001378
Iteration 65/1000 | Loss: 0.00001378
Iteration 66/1000 | Loss: 0.00001377
Iteration 67/1000 | Loss: 0.00001377
Iteration 68/1000 | Loss: 0.00001377
Iteration 69/1000 | Loss: 0.00001377
Iteration 70/1000 | Loss: 0.00001377
Iteration 71/1000 | Loss: 0.00001377
Iteration 72/1000 | Loss: 0.00001377
Iteration 73/1000 | Loss: 0.00001377
Iteration 74/1000 | Loss: 0.00001377
Iteration 75/1000 | Loss: 0.00001377
Iteration 76/1000 | Loss: 0.00001377
Iteration 77/1000 | Loss: 0.00001377
Iteration 78/1000 | Loss: 0.00001377
Iteration 79/1000 | Loss: 0.00001377
Iteration 80/1000 | Loss: 0.00001377
Iteration 81/1000 | Loss: 0.00001376
Iteration 82/1000 | Loss: 0.00001376
Iteration 83/1000 | Loss: 0.00001376
Iteration 84/1000 | Loss: 0.00001376
Iteration 85/1000 | Loss: 0.00001376
Iteration 86/1000 | Loss: 0.00001376
Iteration 87/1000 | Loss: 0.00001375
Iteration 88/1000 | Loss: 0.00001375
Iteration 89/1000 | Loss: 0.00001375
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001375
Iteration 92/1000 | Loss: 0.00001375
Iteration 93/1000 | Loss: 0.00001375
Iteration 94/1000 | Loss: 0.00001375
Iteration 95/1000 | Loss: 0.00001375
Iteration 96/1000 | Loss: 0.00001375
Iteration 97/1000 | Loss: 0.00001375
Iteration 98/1000 | Loss: 0.00001375
Iteration 99/1000 | Loss: 0.00001375
Iteration 100/1000 | Loss: 0.00001375
Iteration 101/1000 | Loss: 0.00001374
Iteration 102/1000 | Loss: 0.00001374
Iteration 103/1000 | Loss: 0.00001374
Iteration 104/1000 | Loss: 0.00001374
Iteration 105/1000 | Loss: 0.00001374
Iteration 106/1000 | Loss: 0.00001374
Iteration 107/1000 | Loss: 0.00001374
Iteration 108/1000 | Loss: 0.00001374
Iteration 109/1000 | Loss: 0.00001374
Iteration 110/1000 | Loss: 0.00001374
Iteration 111/1000 | Loss: 0.00001374
Iteration 112/1000 | Loss: 0.00001374
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001374
Iteration 116/1000 | Loss: 0.00001374
Iteration 117/1000 | Loss: 0.00001374
Iteration 118/1000 | Loss: 0.00001374
Iteration 119/1000 | Loss: 0.00001374
Iteration 120/1000 | Loss: 0.00001374
Iteration 121/1000 | Loss: 0.00001374
Iteration 122/1000 | Loss: 0.00001374
Iteration 123/1000 | Loss: 0.00001374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.3744224816036876e-05, 1.3744224816036876e-05, 1.3744224816036876e-05, 1.3744224816036876e-05, 1.3744224816036876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3744224816036876e-05

Optimization complete. Final v2v error: 3.205481767654419 mm

Highest mean error: 3.764187812805176 mm for frame 105

Lowest mean error: 2.814279079437256 mm for frame 46

Saving results

Total time: 30.481004238128662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01098238
Iteration 2/25 | Loss: 0.00277107
Iteration 3/25 | Loss: 0.00192395
Iteration 4/25 | Loss: 0.00180871
Iteration 5/25 | Loss: 0.00160283
Iteration 6/25 | Loss: 0.00140342
Iteration 7/25 | Loss: 0.00136529
Iteration 8/25 | Loss: 0.00136503
Iteration 9/25 | Loss: 0.00135429
Iteration 10/25 | Loss: 0.00132190
Iteration 11/25 | Loss: 0.00132042
Iteration 12/25 | Loss: 0.00130297
Iteration 13/25 | Loss: 0.00131047
Iteration 14/25 | Loss: 0.00130498
Iteration 15/25 | Loss: 0.00130081
Iteration 16/25 | Loss: 0.00128309
Iteration 17/25 | Loss: 0.00128117
Iteration 18/25 | Loss: 0.00128363
Iteration 19/25 | Loss: 0.00127009
Iteration 20/25 | Loss: 0.00126472
Iteration 21/25 | Loss: 0.00126096
Iteration 22/25 | Loss: 0.00125523
Iteration 23/25 | Loss: 0.00125888
Iteration 24/25 | Loss: 0.00125465
Iteration 25/25 | Loss: 0.00125192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.76721168
Iteration 2/25 | Loss: 0.00245163
Iteration 3/25 | Loss: 0.00245163
Iteration 4/25 | Loss: 0.00245163
Iteration 5/25 | Loss: 0.00245163
Iteration 6/25 | Loss: 0.00245163
Iteration 7/25 | Loss: 0.00245163
Iteration 8/25 | Loss: 0.00245163
Iteration 9/25 | Loss: 0.00245162
Iteration 10/25 | Loss: 0.00245162
Iteration 11/25 | Loss: 0.00245162
Iteration 12/25 | Loss: 0.00245162
Iteration 13/25 | Loss: 0.00245162
Iteration 14/25 | Loss: 0.00245162
Iteration 15/25 | Loss: 0.00245162
Iteration 16/25 | Loss: 0.00245162
Iteration 17/25 | Loss: 0.00245162
Iteration 18/25 | Loss: 0.00245162
Iteration 19/25 | Loss: 0.00245162
Iteration 20/25 | Loss: 0.00245162
Iteration 21/25 | Loss: 0.00245162
Iteration 22/25 | Loss: 0.00245162
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0024516244884580374, 0.0024516244884580374, 0.0024516244884580374, 0.0024516244884580374, 0.0024516244884580374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024516244884580374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245162
Iteration 2/1000 | Loss: 0.00032411
Iteration 3/1000 | Loss: 0.00026789
Iteration 4/1000 | Loss: 0.00024549
Iteration 5/1000 | Loss: 0.00022107
Iteration 6/1000 | Loss: 0.00177974
Iteration 7/1000 | Loss: 0.00055826
Iteration 8/1000 | Loss: 0.00211978
Iteration 9/1000 | Loss: 0.00095882
Iteration 10/1000 | Loss: 0.00025801
Iteration 11/1000 | Loss: 0.00022670
Iteration 12/1000 | Loss: 0.00019476
Iteration 13/1000 | Loss: 0.00017921
Iteration 14/1000 | Loss: 0.00018387
Iteration 15/1000 | Loss: 0.00017565
Iteration 16/1000 | Loss: 0.00017953
Iteration 17/1000 | Loss: 0.00017703
Iteration 18/1000 | Loss: 0.00014523
Iteration 19/1000 | Loss: 0.00017038
Iteration 20/1000 | Loss: 0.00014377
Iteration 21/1000 | Loss: 0.00013975
Iteration 22/1000 | Loss: 0.00016075
Iteration 23/1000 | Loss: 0.00016917
Iteration 24/1000 | Loss: 0.00015840
Iteration 25/1000 | Loss: 0.00015812
Iteration 26/1000 | Loss: 0.00014953
Iteration 27/1000 | Loss: 0.00013860
Iteration 28/1000 | Loss: 0.00131726
Iteration 29/1000 | Loss: 0.00097509
Iteration 30/1000 | Loss: 0.00062942
Iteration 31/1000 | Loss: 0.00077268
Iteration 32/1000 | Loss: 0.00017280
Iteration 33/1000 | Loss: 0.00014704
Iteration 34/1000 | Loss: 0.00040311
Iteration 35/1000 | Loss: 0.00033481
Iteration 36/1000 | Loss: 0.00016338
Iteration 37/1000 | Loss: 0.00014026
Iteration 38/1000 | Loss: 0.00052416
Iteration 39/1000 | Loss: 0.00057806
Iteration 40/1000 | Loss: 0.00089308
Iteration 41/1000 | Loss: 0.00027051
Iteration 42/1000 | Loss: 0.00035573
Iteration 43/1000 | Loss: 0.00014039
Iteration 44/1000 | Loss: 0.00012222
Iteration 45/1000 | Loss: 0.00011594
Iteration 46/1000 | Loss: 0.00011234
Iteration 47/1000 | Loss: 0.00011033
Iteration 48/1000 | Loss: 0.00010869
Iteration 49/1000 | Loss: 0.00069693
Iteration 50/1000 | Loss: 0.00015389
Iteration 51/1000 | Loss: 0.00012145
Iteration 52/1000 | Loss: 0.00026651
Iteration 53/1000 | Loss: 0.00013002
Iteration 54/1000 | Loss: 0.00011305
Iteration 55/1000 | Loss: 0.00011113
Iteration 56/1000 | Loss: 0.00065411
Iteration 57/1000 | Loss: 0.00025632
Iteration 58/1000 | Loss: 0.00081858
Iteration 59/1000 | Loss: 0.00045536
Iteration 60/1000 | Loss: 0.00011544
Iteration 61/1000 | Loss: 0.00044489
Iteration 62/1000 | Loss: 0.00042553
Iteration 63/1000 | Loss: 0.00063206
Iteration 64/1000 | Loss: 0.00045618
Iteration 65/1000 | Loss: 0.00047585
Iteration 66/1000 | Loss: 0.00042215
Iteration 67/1000 | Loss: 0.00040025
Iteration 68/1000 | Loss: 0.00038654
Iteration 69/1000 | Loss: 0.00039888
Iteration 70/1000 | Loss: 0.00042204
Iteration 71/1000 | Loss: 0.00041421
Iteration 72/1000 | Loss: 0.00036844
Iteration 73/1000 | Loss: 0.00083539
Iteration 74/1000 | Loss: 0.00018221
Iteration 75/1000 | Loss: 0.00034718
Iteration 76/1000 | Loss: 0.00059379
Iteration 77/1000 | Loss: 0.00018643
Iteration 78/1000 | Loss: 0.00011813
Iteration 79/1000 | Loss: 0.00010997
Iteration 80/1000 | Loss: 0.00010808
Iteration 81/1000 | Loss: 0.00022589
Iteration 82/1000 | Loss: 0.00012013
Iteration 83/1000 | Loss: 0.00010652
Iteration 84/1000 | Loss: 0.00010132
Iteration 85/1000 | Loss: 0.00009696
Iteration 86/1000 | Loss: 0.00009332
Iteration 87/1000 | Loss: 0.00009065
Iteration 88/1000 | Loss: 0.00048609
Iteration 89/1000 | Loss: 0.00016254
Iteration 90/1000 | Loss: 0.00011521
Iteration 91/1000 | Loss: 0.00014245
Iteration 92/1000 | Loss: 0.00009924
Iteration 93/1000 | Loss: 0.00009343
Iteration 94/1000 | Loss: 0.00009169
Iteration 95/1000 | Loss: 0.00049979
Iteration 96/1000 | Loss: 0.00016890
Iteration 97/1000 | Loss: 0.00010838
Iteration 98/1000 | Loss: 0.00009120
Iteration 99/1000 | Loss: 0.00008883
Iteration 100/1000 | Loss: 0.00008795
Iteration 101/1000 | Loss: 0.00008732
Iteration 102/1000 | Loss: 0.00008690
Iteration 103/1000 | Loss: 0.00008640
Iteration 104/1000 | Loss: 0.00008614
Iteration 105/1000 | Loss: 0.00008594
Iteration 106/1000 | Loss: 0.00008587
Iteration 107/1000 | Loss: 0.00008579
Iteration 108/1000 | Loss: 0.00008578
Iteration 109/1000 | Loss: 0.00008578
Iteration 110/1000 | Loss: 0.00008577
Iteration 111/1000 | Loss: 0.00008577
Iteration 112/1000 | Loss: 0.00008567
Iteration 113/1000 | Loss: 0.00008566
Iteration 114/1000 | Loss: 0.00008565
Iteration 115/1000 | Loss: 0.00008565
Iteration 116/1000 | Loss: 0.00008565
Iteration 117/1000 | Loss: 0.00008565
Iteration 118/1000 | Loss: 0.00008565
Iteration 119/1000 | Loss: 0.00008565
Iteration 120/1000 | Loss: 0.00008564
Iteration 121/1000 | Loss: 0.00008564
Iteration 122/1000 | Loss: 0.00008564
Iteration 123/1000 | Loss: 0.00008564
Iteration 124/1000 | Loss: 0.00008564
Iteration 125/1000 | Loss: 0.00008564
Iteration 126/1000 | Loss: 0.00008564
Iteration 127/1000 | Loss: 0.00008564
Iteration 128/1000 | Loss: 0.00008564
Iteration 129/1000 | Loss: 0.00008564
Iteration 130/1000 | Loss: 0.00008564
Iteration 131/1000 | Loss: 0.00008564
Iteration 132/1000 | Loss: 0.00008563
Iteration 133/1000 | Loss: 0.00008563
Iteration 134/1000 | Loss: 0.00008563
Iteration 135/1000 | Loss: 0.00008563
Iteration 136/1000 | Loss: 0.00008563
Iteration 137/1000 | Loss: 0.00008563
Iteration 138/1000 | Loss: 0.00008563
Iteration 139/1000 | Loss: 0.00008562
Iteration 140/1000 | Loss: 0.00008562
Iteration 141/1000 | Loss: 0.00008562
Iteration 142/1000 | Loss: 0.00008562
Iteration 143/1000 | Loss: 0.00008562
Iteration 144/1000 | Loss: 0.00008561
Iteration 145/1000 | Loss: 0.00008561
Iteration 146/1000 | Loss: 0.00008561
Iteration 147/1000 | Loss: 0.00008560
Iteration 148/1000 | Loss: 0.00008559
Iteration 149/1000 | Loss: 0.00008559
Iteration 150/1000 | Loss: 0.00008559
Iteration 151/1000 | Loss: 0.00008559
Iteration 152/1000 | Loss: 0.00008558
Iteration 153/1000 | Loss: 0.00008558
Iteration 154/1000 | Loss: 0.00008558
Iteration 155/1000 | Loss: 0.00008558
Iteration 156/1000 | Loss: 0.00008558
Iteration 157/1000 | Loss: 0.00008558
Iteration 158/1000 | Loss: 0.00008557
Iteration 159/1000 | Loss: 0.00008557
Iteration 160/1000 | Loss: 0.00008557
Iteration 161/1000 | Loss: 0.00008557
Iteration 162/1000 | Loss: 0.00008557
Iteration 163/1000 | Loss: 0.00008557
Iteration 164/1000 | Loss: 0.00008557
Iteration 165/1000 | Loss: 0.00008557
Iteration 166/1000 | Loss: 0.00008557
Iteration 167/1000 | Loss: 0.00008556
Iteration 168/1000 | Loss: 0.00008556
Iteration 169/1000 | Loss: 0.00008556
Iteration 170/1000 | Loss: 0.00008556
Iteration 171/1000 | Loss: 0.00008556
Iteration 172/1000 | Loss: 0.00008556
Iteration 173/1000 | Loss: 0.00008556
Iteration 174/1000 | Loss: 0.00008556
Iteration 175/1000 | Loss: 0.00008556
Iteration 176/1000 | Loss: 0.00008556
Iteration 177/1000 | Loss: 0.00008556
Iteration 178/1000 | Loss: 0.00008555
Iteration 179/1000 | Loss: 0.00008555
Iteration 180/1000 | Loss: 0.00008555
Iteration 181/1000 | Loss: 0.00008555
Iteration 182/1000 | Loss: 0.00008555
Iteration 183/1000 | Loss: 0.00008555
Iteration 184/1000 | Loss: 0.00008555
Iteration 185/1000 | Loss: 0.00008555
Iteration 186/1000 | Loss: 0.00008555
Iteration 187/1000 | Loss: 0.00008555
Iteration 188/1000 | Loss: 0.00008555
Iteration 189/1000 | Loss: 0.00008555
Iteration 190/1000 | Loss: 0.00008554
Iteration 191/1000 | Loss: 0.00008554
Iteration 192/1000 | Loss: 0.00008554
Iteration 193/1000 | Loss: 0.00008554
Iteration 194/1000 | Loss: 0.00008554
Iteration 195/1000 | Loss: 0.00008554
Iteration 196/1000 | Loss: 0.00008554
Iteration 197/1000 | Loss: 0.00008554
Iteration 198/1000 | Loss: 0.00008554
Iteration 199/1000 | Loss: 0.00008554
Iteration 200/1000 | Loss: 0.00008554
Iteration 201/1000 | Loss: 0.00008554
Iteration 202/1000 | Loss: 0.00008554
Iteration 203/1000 | Loss: 0.00008554
Iteration 204/1000 | Loss: 0.00008553
Iteration 205/1000 | Loss: 0.00008553
Iteration 206/1000 | Loss: 0.00008553
Iteration 207/1000 | Loss: 0.00008553
Iteration 208/1000 | Loss: 0.00008553
Iteration 209/1000 | Loss: 0.00008553
Iteration 210/1000 | Loss: 0.00008553
Iteration 211/1000 | Loss: 0.00008553
Iteration 212/1000 | Loss: 0.00008553
Iteration 213/1000 | Loss: 0.00008553
Iteration 214/1000 | Loss: 0.00008553
Iteration 215/1000 | Loss: 0.00008553
Iteration 216/1000 | Loss: 0.00008553
Iteration 217/1000 | Loss: 0.00008553
Iteration 218/1000 | Loss: 0.00008552
Iteration 219/1000 | Loss: 0.00008552
Iteration 220/1000 | Loss: 0.00008552
Iteration 221/1000 | Loss: 0.00008552
Iteration 222/1000 | Loss: 0.00008552
Iteration 223/1000 | Loss: 0.00008552
Iteration 224/1000 | Loss: 0.00008552
Iteration 225/1000 | Loss: 0.00008552
Iteration 226/1000 | Loss: 0.00008552
Iteration 227/1000 | Loss: 0.00008552
Iteration 228/1000 | Loss: 0.00008552
Iteration 229/1000 | Loss: 0.00008552
Iteration 230/1000 | Loss: 0.00008552
Iteration 231/1000 | Loss: 0.00008552
Iteration 232/1000 | Loss: 0.00008552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [8.552218059776351e-05, 8.552218059776351e-05, 8.552218059776351e-05, 8.552218059776351e-05, 8.552218059776351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.552218059776351e-05

Optimization complete. Final v2v error: 4.818210124969482 mm

Highest mean error: 12.17051887512207 mm for frame 52

Lowest mean error: 2.9230399131774902 mm for frame 2

Saving results

Total time: 201.05252599716187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00173292
Iteration 2/25 | Loss: 0.00101477
Iteration 3/25 | Loss: 0.00094060
Iteration 4/25 | Loss: 0.00092503
Iteration 5/25 | Loss: 0.00091904
Iteration 6/25 | Loss: 0.00091784
Iteration 7/25 | Loss: 0.00091784
Iteration 8/25 | Loss: 0.00091784
Iteration 9/25 | Loss: 0.00091784
Iteration 10/25 | Loss: 0.00091784
Iteration 11/25 | Loss: 0.00091784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009178392356261611, 0.0009178392356261611, 0.0009178392356261611, 0.0009178392356261611, 0.0009178392356261611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009178392356261611

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29970336
Iteration 2/25 | Loss: 0.00132333
Iteration 3/25 | Loss: 0.00132333
Iteration 4/25 | Loss: 0.00132333
Iteration 5/25 | Loss: 0.00132332
Iteration 6/25 | Loss: 0.00132332
Iteration 7/25 | Loss: 0.00132332
Iteration 8/25 | Loss: 0.00132332
Iteration 9/25 | Loss: 0.00132332
Iteration 10/25 | Loss: 0.00132332
Iteration 11/25 | Loss: 0.00132332
Iteration 12/25 | Loss: 0.00132332
Iteration 13/25 | Loss: 0.00132332
Iteration 14/25 | Loss: 0.00132332
Iteration 15/25 | Loss: 0.00132332
Iteration 16/25 | Loss: 0.00132332
Iteration 17/25 | Loss: 0.00132332
Iteration 18/25 | Loss: 0.00132332
Iteration 19/25 | Loss: 0.00132332
Iteration 20/25 | Loss: 0.00132332
Iteration 21/25 | Loss: 0.00132332
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001323323929682374, 0.001323323929682374, 0.001323323929682374, 0.001323323929682374, 0.001323323929682374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001323323929682374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132332
Iteration 2/1000 | Loss: 0.00004486
Iteration 3/1000 | Loss: 0.00002145
Iteration 4/1000 | Loss: 0.00001785
Iteration 5/1000 | Loss: 0.00001583
Iteration 6/1000 | Loss: 0.00001503
Iteration 7/1000 | Loss: 0.00001451
Iteration 8/1000 | Loss: 0.00001419
Iteration 9/1000 | Loss: 0.00001414
Iteration 10/1000 | Loss: 0.00001386
Iteration 11/1000 | Loss: 0.00001373
Iteration 12/1000 | Loss: 0.00001369
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001368
Iteration 15/1000 | Loss: 0.00001366
Iteration 16/1000 | Loss: 0.00001357
Iteration 17/1000 | Loss: 0.00001342
Iteration 18/1000 | Loss: 0.00001338
Iteration 19/1000 | Loss: 0.00001337
Iteration 20/1000 | Loss: 0.00001337
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001336
Iteration 23/1000 | Loss: 0.00001335
Iteration 24/1000 | Loss: 0.00001334
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001333
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001333
Iteration 30/1000 | Loss: 0.00001333
Iteration 31/1000 | Loss: 0.00001332
Iteration 32/1000 | Loss: 0.00001331
Iteration 33/1000 | Loss: 0.00001331
Iteration 34/1000 | Loss: 0.00001331
Iteration 35/1000 | Loss: 0.00001331
Iteration 36/1000 | Loss: 0.00001331
Iteration 37/1000 | Loss: 0.00001331
Iteration 38/1000 | Loss: 0.00001331
Iteration 39/1000 | Loss: 0.00001331
Iteration 40/1000 | Loss: 0.00001331
Iteration 41/1000 | Loss: 0.00001331
Iteration 42/1000 | Loss: 0.00001331
Iteration 43/1000 | Loss: 0.00001331
Iteration 44/1000 | Loss: 0.00001331
Iteration 45/1000 | Loss: 0.00001330
Iteration 46/1000 | Loss: 0.00001330
Iteration 47/1000 | Loss: 0.00001330
Iteration 48/1000 | Loss: 0.00001329
Iteration 49/1000 | Loss: 0.00001329
Iteration 50/1000 | Loss: 0.00001329
Iteration 51/1000 | Loss: 0.00001329
Iteration 52/1000 | Loss: 0.00001329
Iteration 53/1000 | Loss: 0.00001329
Iteration 54/1000 | Loss: 0.00001329
Iteration 55/1000 | Loss: 0.00001329
Iteration 56/1000 | Loss: 0.00001329
Iteration 57/1000 | Loss: 0.00001329
Iteration 58/1000 | Loss: 0.00001328
Iteration 59/1000 | Loss: 0.00001328
Iteration 60/1000 | Loss: 0.00001328
Iteration 61/1000 | Loss: 0.00001328
Iteration 62/1000 | Loss: 0.00001327
Iteration 63/1000 | Loss: 0.00001327
Iteration 64/1000 | Loss: 0.00001327
Iteration 65/1000 | Loss: 0.00001327
Iteration 66/1000 | Loss: 0.00001327
Iteration 67/1000 | Loss: 0.00001327
Iteration 68/1000 | Loss: 0.00001326
Iteration 69/1000 | Loss: 0.00001326
Iteration 70/1000 | Loss: 0.00001326
Iteration 71/1000 | Loss: 0.00001326
Iteration 72/1000 | Loss: 0.00001326
Iteration 73/1000 | Loss: 0.00001326
Iteration 74/1000 | Loss: 0.00001325
Iteration 75/1000 | Loss: 0.00001325
Iteration 76/1000 | Loss: 0.00001325
Iteration 77/1000 | Loss: 0.00001325
Iteration 78/1000 | Loss: 0.00001325
Iteration 79/1000 | Loss: 0.00001325
Iteration 80/1000 | Loss: 0.00001324
Iteration 81/1000 | Loss: 0.00001324
Iteration 82/1000 | Loss: 0.00001324
Iteration 83/1000 | Loss: 0.00001324
Iteration 84/1000 | Loss: 0.00001324
Iteration 85/1000 | Loss: 0.00001324
Iteration 86/1000 | Loss: 0.00001324
Iteration 87/1000 | Loss: 0.00001324
Iteration 88/1000 | Loss: 0.00001324
Iteration 89/1000 | Loss: 0.00001324
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00001323
Iteration 96/1000 | Loss: 0.00001323
Iteration 97/1000 | Loss: 0.00001323
Iteration 98/1000 | Loss: 0.00001323
Iteration 99/1000 | Loss: 0.00001323
Iteration 100/1000 | Loss: 0.00001323
Iteration 101/1000 | Loss: 0.00001323
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001322
Iteration 105/1000 | Loss: 0.00001321
Iteration 106/1000 | Loss: 0.00001321
Iteration 107/1000 | Loss: 0.00001321
Iteration 108/1000 | Loss: 0.00001321
Iteration 109/1000 | Loss: 0.00001320
Iteration 110/1000 | Loss: 0.00001320
Iteration 111/1000 | Loss: 0.00001320
Iteration 112/1000 | Loss: 0.00001319
Iteration 113/1000 | Loss: 0.00001319
Iteration 114/1000 | Loss: 0.00001319
Iteration 115/1000 | Loss: 0.00001319
Iteration 116/1000 | Loss: 0.00001319
Iteration 117/1000 | Loss: 0.00001318
Iteration 118/1000 | Loss: 0.00001318
Iteration 119/1000 | Loss: 0.00001318
Iteration 120/1000 | Loss: 0.00001318
Iteration 121/1000 | Loss: 0.00001317
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001317
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001316
Iteration 129/1000 | Loss: 0.00001316
Iteration 130/1000 | Loss: 0.00001316
Iteration 131/1000 | Loss: 0.00001315
Iteration 132/1000 | Loss: 0.00001315
Iteration 133/1000 | Loss: 0.00001315
Iteration 134/1000 | Loss: 0.00001315
Iteration 135/1000 | Loss: 0.00001315
Iteration 136/1000 | Loss: 0.00001315
Iteration 137/1000 | Loss: 0.00001315
Iteration 138/1000 | Loss: 0.00001315
Iteration 139/1000 | Loss: 0.00001314
Iteration 140/1000 | Loss: 0.00001314
Iteration 141/1000 | Loss: 0.00001314
Iteration 142/1000 | Loss: 0.00001314
Iteration 143/1000 | Loss: 0.00001314
Iteration 144/1000 | Loss: 0.00001314
Iteration 145/1000 | Loss: 0.00001314
Iteration 146/1000 | Loss: 0.00001314
Iteration 147/1000 | Loss: 0.00001314
Iteration 148/1000 | Loss: 0.00001314
Iteration 149/1000 | Loss: 0.00001313
Iteration 150/1000 | Loss: 0.00001313
Iteration 151/1000 | Loss: 0.00001313
Iteration 152/1000 | Loss: 0.00001313
Iteration 153/1000 | Loss: 0.00001313
Iteration 154/1000 | Loss: 0.00001313
Iteration 155/1000 | Loss: 0.00001313
Iteration 156/1000 | Loss: 0.00001313
Iteration 157/1000 | Loss: 0.00001313
Iteration 158/1000 | Loss: 0.00001313
Iteration 159/1000 | Loss: 0.00001312
Iteration 160/1000 | Loss: 0.00001312
Iteration 161/1000 | Loss: 0.00001312
Iteration 162/1000 | Loss: 0.00001312
Iteration 163/1000 | Loss: 0.00001311
Iteration 164/1000 | Loss: 0.00001311
Iteration 165/1000 | Loss: 0.00001311
Iteration 166/1000 | Loss: 0.00001311
Iteration 167/1000 | Loss: 0.00001311
Iteration 168/1000 | Loss: 0.00001311
Iteration 169/1000 | Loss: 0.00001311
Iteration 170/1000 | Loss: 0.00001310
Iteration 171/1000 | Loss: 0.00001310
Iteration 172/1000 | Loss: 0.00001310
Iteration 173/1000 | Loss: 0.00001310
Iteration 174/1000 | Loss: 0.00001310
Iteration 175/1000 | Loss: 0.00001309
Iteration 176/1000 | Loss: 0.00001309
Iteration 177/1000 | Loss: 0.00001309
Iteration 178/1000 | Loss: 0.00001309
Iteration 179/1000 | Loss: 0.00001309
Iteration 180/1000 | Loss: 0.00001309
Iteration 181/1000 | Loss: 0.00001309
Iteration 182/1000 | Loss: 0.00001309
Iteration 183/1000 | Loss: 0.00001309
Iteration 184/1000 | Loss: 0.00001309
Iteration 185/1000 | Loss: 0.00001309
Iteration 186/1000 | Loss: 0.00001309
Iteration 187/1000 | Loss: 0.00001309
Iteration 188/1000 | Loss: 0.00001309
Iteration 189/1000 | Loss: 0.00001309
Iteration 190/1000 | Loss: 0.00001309
Iteration 191/1000 | Loss: 0.00001309
Iteration 192/1000 | Loss: 0.00001309
Iteration 193/1000 | Loss: 0.00001309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.3092557310301345e-05, 1.3092557310301345e-05, 1.3092557310301345e-05, 1.3092557310301345e-05, 1.3092557310301345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3092557310301345e-05

Optimization complete. Final v2v error: 3.050713062286377 mm

Highest mean error: 3.483532190322876 mm for frame 177

Lowest mean error: 2.5123417377471924 mm for frame 7

Saving results

Total time: 43.266682863235474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898669
Iteration 2/25 | Loss: 0.00137308
Iteration 3/25 | Loss: 0.00105834
Iteration 4/25 | Loss: 0.00102317
Iteration 5/25 | Loss: 0.00101512
Iteration 6/25 | Loss: 0.00101422
Iteration 7/25 | Loss: 0.00101422
Iteration 8/25 | Loss: 0.00101422
Iteration 9/25 | Loss: 0.00101422
Iteration 10/25 | Loss: 0.00101422
Iteration 11/25 | Loss: 0.00101422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001014217035844922, 0.001014217035844922, 0.001014217035844922, 0.001014217035844922, 0.001014217035844922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001014217035844922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86132938
Iteration 2/25 | Loss: 0.00051322
Iteration 3/25 | Loss: 0.00051322
Iteration 4/25 | Loss: 0.00051322
Iteration 5/25 | Loss: 0.00051322
Iteration 6/25 | Loss: 0.00051322
Iteration 7/25 | Loss: 0.00051322
Iteration 8/25 | Loss: 0.00051322
Iteration 9/25 | Loss: 0.00051322
Iteration 10/25 | Loss: 0.00051322
Iteration 11/25 | Loss: 0.00051322
Iteration 12/25 | Loss: 0.00051322
Iteration 13/25 | Loss: 0.00051322
Iteration 14/25 | Loss: 0.00051322
Iteration 15/25 | Loss: 0.00051322
Iteration 16/25 | Loss: 0.00051322
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005132186342962086, 0.0005132186342962086, 0.0005132186342962086, 0.0005132186342962086, 0.0005132186342962086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005132186342962086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051322
Iteration 2/1000 | Loss: 0.00006590
Iteration 3/1000 | Loss: 0.00005061
Iteration 4/1000 | Loss: 0.00003986
Iteration 5/1000 | Loss: 0.00003632
Iteration 6/1000 | Loss: 0.00003406
Iteration 7/1000 | Loss: 0.00003220
Iteration 8/1000 | Loss: 0.00003106
Iteration 9/1000 | Loss: 0.00003049
Iteration 10/1000 | Loss: 0.00002999
Iteration 11/1000 | Loss: 0.00002966
Iteration 12/1000 | Loss: 0.00002942
Iteration 13/1000 | Loss: 0.00002918
Iteration 14/1000 | Loss: 0.00002905
Iteration 15/1000 | Loss: 0.00002903
Iteration 16/1000 | Loss: 0.00002900
Iteration 17/1000 | Loss: 0.00002894
Iteration 18/1000 | Loss: 0.00002889
Iteration 19/1000 | Loss: 0.00002889
Iteration 20/1000 | Loss: 0.00002889
Iteration 21/1000 | Loss: 0.00002889
Iteration 22/1000 | Loss: 0.00002889
Iteration 23/1000 | Loss: 0.00002889
Iteration 24/1000 | Loss: 0.00002888
Iteration 25/1000 | Loss: 0.00002888
Iteration 26/1000 | Loss: 0.00002888
Iteration 27/1000 | Loss: 0.00002888
Iteration 28/1000 | Loss: 0.00002888
Iteration 29/1000 | Loss: 0.00002888
Iteration 30/1000 | Loss: 0.00002887
Iteration 31/1000 | Loss: 0.00002887
Iteration 32/1000 | Loss: 0.00002887
Iteration 33/1000 | Loss: 0.00002887
Iteration 34/1000 | Loss: 0.00002887
Iteration 35/1000 | Loss: 0.00002887
Iteration 36/1000 | Loss: 0.00002887
Iteration 37/1000 | Loss: 0.00002887
Iteration 38/1000 | Loss: 0.00002887
Iteration 39/1000 | Loss: 0.00002887
Iteration 40/1000 | Loss: 0.00002887
Iteration 41/1000 | Loss: 0.00002886
Iteration 42/1000 | Loss: 0.00002886
Iteration 43/1000 | Loss: 0.00002886
Iteration 44/1000 | Loss: 0.00002886
Iteration 45/1000 | Loss: 0.00002886
Iteration 46/1000 | Loss: 0.00002886
Iteration 47/1000 | Loss: 0.00002886
Iteration 48/1000 | Loss: 0.00002886
Iteration 49/1000 | Loss: 0.00002886
Iteration 50/1000 | Loss: 0.00002886
Iteration 51/1000 | Loss: 0.00002886
Iteration 52/1000 | Loss: 0.00002886
Iteration 53/1000 | Loss: 0.00002886
Iteration 54/1000 | Loss: 0.00002886
Iteration 55/1000 | Loss: 0.00002886
Iteration 56/1000 | Loss: 0.00002886
Iteration 57/1000 | Loss: 0.00002886
Iteration 58/1000 | Loss: 0.00002886
Iteration 59/1000 | Loss: 0.00002886
Iteration 60/1000 | Loss: 0.00002886
Iteration 61/1000 | Loss: 0.00002886
Iteration 62/1000 | Loss: 0.00002886
Iteration 63/1000 | Loss: 0.00002886
Iteration 64/1000 | Loss: 0.00002886
Iteration 65/1000 | Loss: 0.00002886
Iteration 66/1000 | Loss: 0.00002886
Iteration 67/1000 | Loss: 0.00002886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [2.886162474169396e-05, 2.886162474169396e-05, 2.886162474169396e-05, 2.886162474169396e-05, 2.886162474169396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.886162474169396e-05

Optimization complete. Final v2v error: 4.564409255981445 mm

Highest mean error: 4.968208312988281 mm for frame 27

Lowest mean error: 4.243743419647217 mm for frame 5

Saving results

Total time: 31.83549213409424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00910781
Iteration 2/25 | Loss: 0.00215866
Iteration 3/25 | Loss: 0.00147457
Iteration 4/25 | Loss: 0.00124135
Iteration 5/25 | Loss: 0.00116498
Iteration 6/25 | Loss: 0.00114781
Iteration 7/25 | Loss: 0.00116261
Iteration 8/25 | Loss: 0.00118758
Iteration 9/25 | Loss: 0.00118011
Iteration 10/25 | Loss: 0.00117683
Iteration 11/25 | Loss: 0.00118031
Iteration 12/25 | Loss: 0.00117216
Iteration 13/25 | Loss: 0.00116449
Iteration 14/25 | Loss: 0.00116452
Iteration 15/25 | Loss: 0.00116093
Iteration 16/25 | Loss: 0.00116094
Iteration 17/25 | Loss: 0.00116240
Iteration 18/25 | Loss: 0.00115746
Iteration 19/25 | Loss: 0.00115978
Iteration 20/25 | Loss: 0.00115678
Iteration 21/25 | Loss: 0.00115539
Iteration 22/25 | Loss: 0.00115618
Iteration 23/25 | Loss: 0.00115041
Iteration 24/25 | Loss: 0.00115173
Iteration 25/25 | Loss: 0.00115468

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07884192
Iteration 2/25 | Loss: 0.00084207
Iteration 3/25 | Loss: 0.00084206
Iteration 4/25 | Loss: 0.00084206
Iteration 5/25 | Loss: 0.00084206
Iteration 6/25 | Loss: 0.00084206
Iteration 7/25 | Loss: 0.00084206
Iteration 8/25 | Loss: 0.00084206
Iteration 9/25 | Loss: 0.00084206
Iteration 10/25 | Loss: 0.00084206
Iteration 11/25 | Loss: 0.00084206
Iteration 12/25 | Loss: 0.00084206
Iteration 13/25 | Loss: 0.00084206
Iteration 14/25 | Loss: 0.00084206
Iteration 15/25 | Loss: 0.00084206
Iteration 16/25 | Loss: 0.00084206
Iteration 17/25 | Loss: 0.00084206
Iteration 18/25 | Loss: 0.00084206
Iteration 19/25 | Loss: 0.00084206
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000842059962451458, 0.000842059962451458, 0.000842059962451458, 0.000842059962451458, 0.000842059962451458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000842059962451458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084206
Iteration 2/1000 | Loss: 0.00020918
Iteration 3/1000 | Loss: 0.00038754
Iteration 4/1000 | Loss: 0.00026023
Iteration 5/1000 | Loss: 0.00025686
Iteration 6/1000 | Loss: 0.00028875
Iteration 7/1000 | Loss: 0.00034151
Iteration 8/1000 | Loss: 0.00032190
Iteration 9/1000 | Loss: 0.00027924
Iteration 10/1000 | Loss: 0.00040237
Iteration 11/1000 | Loss: 0.00033021
Iteration 12/1000 | Loss: 0.00030929
Iteration 13/1000 | Loss: 0.00030106
Iteration 14/1000 | Loss: 0.00046776
Iteration 15/1000 | Loss: 0.00034995
Iteration 16/1000 | Loss: 0.00027522
Iteration 17/1000 | Loss: 0.00041851
Iteration 18/1000 | Loss: 0.00036903
Iteration 19/1000 | Loss: 0.00030216
Iteration 20/1000 | Loss: 0.00027625
Iteration 21/1000 | Loss: 0.00042788
Iteration 22/1000 | Loss: 0.00034710
Iteration 23/1000 | Loss: 0.00039897
Iteration 24/1000 | Loss: 0.00030798
Iteration 25/1000 | Loss: 0.00043193
Iteration 26/1000 | Loss: 0.00034166
Iteration 27/1000 | Loss: 0.00053271
Iteration 28/1000 | Loss: 0.00043041
Iteration 29/1000 | Loss: 0.00032908
Iteration 30/1000 | Loss: 0.00031916
Iteration 31/1000 | Loss: 0.00029462
Iteration 32/1000 | Loss: 0.00035363
Iteration 33/1000 | Loss: 0.00034847
Iteration 34/1000 | Loss: 0.00036325
Iteration 35/1000 | Loss: 0.00042402
Iteration 36/1000 | Loss: 0.00031217
Iteration 37/1000 | Loss: 0.00033340
Iteration 38/1000 | Loss: 0.00037642
Iteration 39/1000 | Loss: 0.00042643
Iteration 40/1000 | Loss: 0.00044776
Iteration 41/1000 | Loss: 0.00035123
Iteration 42/1000 | Loss: 0.00036867
Iteration 43/1000 | Loss: 0.00042871
Iteration 44/1000 | Loss: 0.00051081
Iteration 45/1000 | Loss: 0.00037152
Iteration 46/1000 | Loss: 0.00024489
Iteration 47/1000 | Loss: 0.00028325
Iteration 48/1000 | Loss: 0.00033572
Iteration 49/1000 | Loss: 0.00044409
Iteration 50/1000 | Loss: 0.00041784
Iteration 51/1000 | Loss: 0.00029284
Iteration 52/1000 | Loss: 0.00032940
Iteration 53/1000 | Loss: 0.00033094
Iteration 54/1000 | Loss: 0.00031664
Iteration 55/1000 | Loss: 0.00029351
Iteration 56/1000 | Loss: 0.00031947
Iteration 57/1000 | Loss: 0.00029770
Iteration 58/1000 | Loss: 0.00028238
Iteration 59/1000 | Loss: 0.00033959
Iteration 60/1000 | Loss: 0.00023312
Iteration 61/1000 | Loss: 0.00028531
Iteration 62/1000 | Loss: 0.00036550
Iteration 63/1000 | Loss: 0.00034181
Iteration 64/1000 | Loss: 0.00027686
Iteration 65/1000 | Loss: 0.00026611
Iteration 66/1000 | Loss: 0.00030284
Iteration 67/1000 | Loss: 0.00029442
Iteration 68/1000 | Loss: 0.00028523
Iteration 69/1000 | Loss: 0.00032844
Iteration 70/1000 | Loss: 0.00025805
Iteration 71/1000 | Loss: 0.00030283
Iteration 72/1000 | Loss: 0.00029136
Iteration 73/1000 | Loss: 0.00031089
Iteration 74/1000 | Loss: 0.00035554
Iteration 75/1000 | Loss: 0.00017693
Iteration 76/1000 | Loss: 0.00026334
Iteration 77/1000 | Loss: 0.00025054
Iteration 78/1000 | Loss: 0.00027927
Iteration 79/1000 | Loss: 0.00029356
Iteration 80/1000 | Loss: 0.00029070
Iteration 81/1000 | Loss: 0.00031493
Iteration 82/1000 | Loss: 0.00022660
Iteration 83/1000 | Loss: 0.00021383
Iteration 84/1000 | Loss: 0.00029268
Iteration 85/1000 | Loss: 0.00028089
Iteration 86/1000 | Loss: 0.00020717
Iteration 87/1000 | Loss: 0.00015015
Iteration 88/1000 | Loss: 0.00017665
Iteration 89/1000 | Loss: 0.00022220
Iteration 90/1000 | Loss: 0.00025497
Iteration 91/1000 | Loss: 0.00028925
Iteration 92/1000 | Loss: 0.00025409
Iteration 93/1000 | Loss: 0.00034613
Iteration 94/1000 | Loss: 0.00026115
Iteration 95/1000 | Loss: 0.00025268
Iteration 96/1000 | Loss: 0.00034148
Iteration 97/1000 | Loss: 0.00024103
Iteration 98/1000 | Loss: 0.00023012
Iteration 99/1000 | Loss: 0.00025649
Iteration 100/1000 | Loss: 0.00024768
Iteration 101/1000 | Loss: 0.00020192
Iteration 102/1000 | Loss: 0.00024800
Iteration 103/1000 | Loss: 0.00028912
Iteration 104/1000 | Loss: 0.00023334
Iteration 105/1000 | Loss: 0.00026638
Iteration 106/1000 | Loss: 0.00033658
Iteration 107/1000 | Loss: 0.00023539
Iteration 108/1000 | Loss: 0.00029409
Iteration 109/1000 | Loss: 0.00027802
Iteration 110/1000 | Loss: 0.00036147
Iteration 111/1000 | Loss: 0.00026362
Iteration 112/1000 | Loss: 0.00026704
Iteration 113/1000 | Loss: 0.00032993
Iteration 114/1000 | Loss: 0.00015306
Iteration 115/1000 | Loss: 0.00022985
Iteration 116/1000 | Loss: 0.00022196
Iteration 117/1000 | Loss: 0.00020386
Iteration 118/1000 | Loss: 0.00024988
Iteration 119/1000 | Loss: 0.00029227
Iteration 120/1000 | Loss: 0.00020702
Iteration 121/1000 | Loss: 0.00031772
Iteration 122/1000 | Loss: 0.00029198
Iteration 123/1000 | Loss: 0.00028331
Iteration 124/1000 | Loss: 0.00020215
Iteration 125/1000 | Loss: 0.00027721
Iteration 126/1000 | Loss: 0.00019628
Iteration 127/1000 | Loss: 0.00027463
Iteration 128/1000 | Loss: 0.00016598
Iteration 129/1000 | Loss: 0.00021610
Iteration 130/1000 | Loss: 0.00022940
Iteration 131/1000 | Loss: 0.00020510
Iteration 132/1000 | Loss: 0.00019112
Iteration 133/1000 | Loss: 0.00022743
Iteration 134/1000 | Loss: 0.00031178
Iteration 135/1000 | Loss: 0.00023009
Iteration 136/1000 | Loss: 0.00025844
Iteration 137/1000 | Loss: 0.00023563
Iteration 138/1000 | Loss: 0.00023872
Iteration 139/1000 | Loss: 0.00022533
Iteration 140/1000 | Loss: 0.00018556
Iteration 141/1000 | Loss: 0.00023897
Iteration 142/1000 | Loss: 0.00027456
Iteration 143/1000 | Loss: 0.00022996
Iteration 144/1000 | Loss: 0.00022960
Iteration 145/1000 | Loss: 0.00028194
Iteration 146/1000 | Loss: 0.00026504
Iteration 147/1000 | Loss: 0.00029420
Iteration 148/1000 | Loss: 0.00024856
Iteration 149/1000 | Loss: 0.00023140
Iteration 150/1000 | Loss: 0.00026266
Iteration 151/1000 | Loss: 0.00029220
Iteration 152/1000 | Loss: 0.00025890
Iteration 153/1000 | Loss: 0.00033895
Iteration 154/1000 | Loss: 0.00031210
Iteration 155/1000 | Loss: 0.00020183
Iteration 156/1000 | Loss: 0.00014200
Iteration 157/1000 | Loss: 0.00015549
Iteration 158/1000 | Loss: 0.00013284
Iteration 159/1000 | Loss: 0.00018104
Iteration 160/1000 | Loss: 0.00013017
Iteration 161/1000 | Loss: 0.00019014
Iteration 162/1000 | Loss: 0.00029102
Iteration 163/1000 | Loss: 0.00016650
Iteration 164/1000 | Loss: 0.00016021
Iteration 165/1000 | Loss: 0.00013852
Iteration 166/1000 | Loss: 0.00014859
Iteration 167/1000 | Loss: 0.00016934
Iteration 168/1000 | Loss: 0.00022834
Iteration 169/1000 | Loss: 0.00018116
Iteration 170/1000 | Loss: 0.00011383
Iteration 171/1000 | Loss: 0.00016154
Iteration 172/1000 | Loss: 0.00018653
Iteration 173/1000 | Loss: 0.00012730
Iteration 174/1000 | Loss: 0.00019950
Iteration 175/1000 | Loss: 0.00019792
Iteration 176/1000 | Loss: 0.00019209
Iteration 177/1000 | Loss: 0.00014379
Iteration 178/1000 | Loss: 0.00015369
Iteration 179/1000 | Loss: 0.00018278
Iteration 180/1000 | Loss: 0.00016839
Iteration 181/1000 | Loss: 0.00020277
Iteration 182/1000 | Loss: 0.00011871
Iteration 183/1000 | Loss: 0.00016676
Iteration 184/1000 | Loss: 0.00013311
Iteration 185/1000 | Loss: 0.00016280
Iteration 186/1000 | Loss: 0.00016803
Iteration 187/1000 | Loss: 0.00018335
Iteration 188/1000 | Loss: 0.00018591
Iteration 189/1000 | Loss: 0.00015461
Iteration 190/1000 | Loss: 0.00020530
Iteration 191/1000 | Loss: 0.00019032
Iteration 192/1000 | Loss: 0.00014921
Iteration 193/1000 | Loss: 0.00015323
Iteration 194/1000 | Loss: 0.00018611
Iteration 195/1000 | Loss: 0.00016588
Iteration 196/1000 | Loss: 0.00017294
Iteration 197/1000 | Loss: 0.00018915
Iteration 198/1000 | Loss: 0.00020291
Iteration 199/1000 | Loss: 0.00021132
Iteration 200/1000 | Loss: 0.00024099
Iteration 201/1000 | Loss: 0.00025875
Iteration 202/1000 | Loss: 0.00024057
Iteration 203/1000 | Loss: 0.00014398
Iteration 204/1000 | Loss: 0.00013041
Iteration 205/1000 | Loss: 0.00013494
Iteration 206/1000 | Loss: 0.00014671
Iteration 207/1000 | Loss: 0.00011144
Iteration 208/1000 | Loss: 0.00027869
Iteration 209/1000 | Loss: 0.00015908
Iteration 210/1000 | Loss: 0.00014692
Iteration 211/1000 | Loss: 0.00013716
Iteration 212/1000 | Loss: 0.00016338
Iteration 213/1000 | Loss: 0.00016056
Iteration 214/1000 | Loss: 0.00012105
Iteration 215/1000 | Loss: 0.00013391
Iteration 216/1000 | Loss: 0.00012292
Iteration 217/1000 | Loss: 0.00013790
Iteration 218/1000 | Loss: 0.00014044
Iteration 219/1000 | Loss: 0.00013931
Iteration 220/1000 | Loss: 0.00012172
Iteration 221/1000 | Loss: 0.00012697
Iteration 222/1000 | Loss: 0.00015000
Iteration 223/1000 | Loss: 0.00017140
Iteration 224/1000 | Loss: 0.00018011
Iteration 225/1000 | Loss: 0.00012679
Iteration 226/1000 | Loss: 0.00013798
Iteration 227/1000 | Loss: 0.00010698
Iteration 228/1000 | Loss: 0.00012611
Iteration 229/1000 | Loss: 0.00015965
Iteration 230/1000 | Loss: 0.00015514
Iteration 231/1000 | Loss: 0.00010195
Iteration 232/1000 | Loss: 0.00008943
Iteration 233/1000 | Loss: 0.00019260
Iteration 234/1000 | Loss: 0.00015889
Iteration 235/1000 | Loss: 0.00008778
Iteration 236/1000 | Loss: 0.00011938
Iteration 237/1000 | Loss: 0.00010997
Iteration 238/1000 | Loss: 0.00013027
Iteration 239/1000 | Loss: 0.00011347
Iteration 240/1000 | Loss: 0.00008220
Iteration 241/1000 | Loss: 0.00009870
Iteration 242/1000 | Loss: 0.00008289
Iteration 243/1000 | Loss: 0.00008901
Iteration 244/1000 | Loss: 0.00008521
Iteration 245/1000 | Loss: 0.00007810
Iteration 246/1000 | Loss: 0.00007361
Iteration 247/1000 | Loss: 0.00007558
Iteration 248/1000 | Loss: 0.00010635
Iteration 249/1000 | Loss: 0.00007988
Iteration 250/1000 | Loss: 0.00008706
Iteration 251/1000 | Loss: 0.00009952
Iteration 252/1000 | Loss: 0.00010231
Iteration 253/1000 | Loss: 0.00012918
Iteration 254/1000 | Loss: 0.00009426
Iteration 255/1000 | Loss: 0.00009006
Iteration 256/1000 | Loss: 0.00007333
Iteration 257/1000 | Loss: 0.00006917
Iteration 258/1000 | Loss: 0.00008195
Iteration 259/1000 | Loss: 0.00008005
Iteration 260/1000 | Loss: 0.00008178
Iteration 261/1000 | Loss: 0.00007678
Iteration 262/1000 | Loss: 0.00008111
Iteration 263/1000 | Loss: 0.00018148
Iteration 264/1000 | Loss: 0.00007803
Iteration 265/1000 | Loss: 0.00008852
Iteration 266/1000 | Loss: 0.00011443
Iteration 267/1000 | Loss: 0.00009539
Iteration 268/1000 | Loss: 0.00007104
Iteration 269/1000 | Loss: 0.00005103
Iteration 270/1000 | Loss: 0.00012515
Iteration 271/1000 | Loss: 0.00004465
Iteration 272/1000 | Loss: 0.00008104
Iteration 273/1000 | Loss: 0.00004287
Iteration 274/1000 | Loss: 0.00005585
Iteration 275/1000 | Loss: 0.00004345
Iteration 276/1000 | Loss: 0.00006363
Iteration 277/1000 | Loss: 0.00004661
Iteration 278/1000 | Loss: 0.00002924
Iteration 279/1000 | Loss: 0.00003322
Iteration 280/1000 | Loss: 0.00002838
Iteration 281/1000 | Loss: 0.00004298
Iteration 282/1000 | Loss: 0.00003120
Iteration 283/1000 | Loss: 0.00005330
Iteration 284/1000 | Loss: 0.00005633
Iteration 285/1000 | Loss: 0.00004935
Iteration 286/1000 | Loss: 0.00003668
Iteration 287/1000 | Loss: 0.00004573
Iteration 288/1000 | Loss: 0.00004021
Iteration 289/1000 | Loss: 0.00004052
Iteration 290/1000 | Loss: 0.00005065
Iteration 291/1000 | Loss: 0.00005309
Iteration 292/1000 | Loss: 0.00004794
Iteration 293/1000 | Loss: 0.00005688
Iteration 294/1000 | Loss: 0.00003467
Iteration 295/1000 | Loss: 0.00003541
Iteration 296/1000 | Loss: 0.00003593
Iteration 297/1000 | Loss: 0.00003697
Iteration 298/1000 | Loss: 0.00002622
Iteration 299/1000 | Loss: 0.00002843
Iteration 300/1000 | Loss: 0.00004595
Iteration 301/1000 | Loss: 0.00003200
Iteration 302/1000 | Loss: 0.00003865
Iteration 303/1000 | Loss: 0.00004477
Iteration 304/1000 | Loss: 0.00007117
Iteration 305/1000 | Loss: 0.00005343
Iteration 306/1000 | Loss: 0.00004837
Iteration 307/1000 | Loss: 0.00004554
Iteration 308/1000 | Loss: 0.00004371
Iteration 309/1000 | Loss: 0.00003421
Iteration 310/1000 | Loss: 0.00004434
Iteration 311/1000 | Loss: 0.00004392
Iteration 312/1000 | Loss: 0.00002726
Iteration 313/1000 | Loss: 0.00003651
Iteration 314/1000 | Loss: 0.00002955
Iteration 315/1000 | Loss: 0.00002764
Iteration 316/1000 | Loss: 0.00002862
Iteration 317/1000 | Loss: 0.00002670
Iteration 318/1000 | Loss: 0.00002771
Iteration 319/1000 | Loss: 0.00002938
Iteration 320/1000 | Loss: 0.00002700
Iteration 321/1000 | Loss: 0.00002936
Iteration 322/1000 | Loss: 0.00002671
Iteration 323/1000 | Loss: 0.00002923
Iteration 324/1000 | Loss: 0.00003301
Iteration 325/1000 | Loss: 0.00003025
Iteration 326/1000 | Loss: 0.00004958
Iteration 327/1000 | Loss: 0.00002837
Iteration 328/1000 | Loss: 0.00003139
Iteration 329/1000 | Loss: 0.00002358
Iteration 330/1000 | Loss: 0.00002287
Iteration 331/1000 | Loss: 0.00002241
Iteration 332/1000 | Loss: 0.00002216
Iteration 333/1000 | Loss: 0.00002200
Iteration 334/1000 | Loss: 0.00002189
Iteration 335/1000 | Loss: 0.00002188
Iteration 336/1000 | Loss: 0.00002177
Iteration 337/1000 | Loss: 0.00002171
Iteration 338/1000 | Loss: 0.00002166
Iteration 339/1000 | Loss: 0.00002823
Iteration 340/1000 | Loss: 0.00004289
Iteration 341/1000 | Loss: 0.00003896
Iteration 342/1000 | Loss: 0.00002165
Iteration 343/1000 | Loss: 0.00002787
Iteration 344/1000 | Loss: 0.00004131
Iteration 345/1000 | Loss: 0.00003247
Iteration 346/1000 | Loss: 0.00003848
Iteration 347/1000 | Loss: 0.00002730
Iteration 348/1000 | Loss: 0.00003850
Iteration 349/1000 | Loss: 0.00003983
Iteration 350/1000 | Loss: 0.00003389
Iteration 351/1000 | Loss: 0.00004079
Iteration 352/1000 | Loss: 0.00003007
Iteration 353/1000 | Loss: 0.00004101
Iteration 354/1000 | Loss: 0.00003332
Iteration 355/1000 | Loss: 0.00004003
Iteration 356/1000 | Loss: 0.00002903
Iteration 357/1000 | Loss: 0.00003950
Iteration 358/1000 | Loss: 0.00002842
Iteration 359/1000 | Loss: 0.00003822
Iteration 360/1000 | Loss: 0.00003044
Iteration 361/1000 | Loss: 0.00003650
Iteration 362/1000 | Loss: 0.00003065
Iteration 363/1000 | Loss: 0.00003630
Iteration 364/1000 | Loss: 0.00003629
Iteration 365/1000 | Loss: 0.00003629
Iteration 366/1000 | Loss: 0.00003188
Iteration 367/1000 | Loss: 0.00003623
Iteration 368/1000 | Loss: 0.00002892
Iteration 369/1000 | Loss: 0.00003516
Iteration 370/1000 | Loss: 0.00003007
Iteration 371/1000 | Loss: 0.00003485
Iteration 372/1000 | Loss: 0.00002802
Iteration 373/1000 | Loss: 0.00003450
Iteration 374/1000 | Loss: 0.00002136
Iteration 375/1000 | Loss: 0.00002071
Iteration 376/1000 | Loss: 0.00002005
Iteration 377/1000 | Loss: 0.00001982
Iteration 378/1000 | Loss: 0.00001963
Iteration 379/1000 | Loss: 0.00001945
Iteration 380/1000 | Loss: 0.00001941
Iteration 381/1000 | Loss: 0.00001940
Iteration 382/1000 | Loss: 0.00001939
Iteration 383/1000 | Loss: 0.00001938
Iteration 384/1000 | Loss: 0.00001938
Iteration 385/1000 | Loss: 0.00001937
Iteration 386/1000 | Loss: 0.00001937
Iteration 387/1000 | Loss: 0.00001937
Iteration 388/1000 | Loss: 0.00001936
Iteration 389/1000 | Loss: 0.00001936
Iteration 390/1000 | Loss: 0.00001935
Iteration 391/1000 | Loss: 0.00001935
Iteration 392/1000 | Loss: 0.00001935
Iteration 393/1000 | Loss: 0.00001935
Iteration 394/1000 | Loss: 0.00001934
Iteration 395/1000 | Loss: 0.00001934
Iteration 396/1000 | Loss: 0.00001934
Iteration 397/1000 | Loss: 0.00001934
Iteration 398/1000 | Loss: 0.00001933
Iteration 399/1000 | Loss: 0.00001933
Iteration 400/1000 | Loss: 0.00001933
Iteration 401/1000 | Loss: 0.00001933
Iteration 402/1000 | Loss: 0.00001932
Iteration 403/1000 | Loss: 0.00001932
Iteration 404/1000 | Loss: 0.00001932
Iteration 405/1000 | Loss: 0.00001932
Iteration 406/1000 | Loss: 0.00001932
Iteration 407/1000 | Loss: 0.00001932
Iteration 408/1000 | Loss: 0.00001932
Iteration 409/1000 | Loss: 0.00001932
Iteration 410/1000 | Loss: 0.00001932
Iteration 411/1000 | Loss: 0.00001932
Iteration 412/1000 | Loss: 0.00001932
Iteration 413/1000 | Loss: 0.00001932
Iteration 414/1000 | Loss: 0.00001931
Iteration 415/1000 | Loss: 0.00001931
Iteration 416/1000 | Loss: 0.00001931
Iteration 417/1000 | Loss: 0.00001931
Iteration 418/1000 | Loss: 0.00001931
Iteration 419/1000 | Loss: 0.00001931
Iteration 420/1000 | Loss: 0.00001931
Iteration 421/1000 | Loss: 0.00001931
Iteration 422/1000 | Loss: 0.00001930
Iteration 423/1000 | Loss: 0.00001930
Iteration 424/1000 | Loss: 0.00001930
Iteration 425/1000 | Loss: 0.00001930
Iteration 426/1000 | Loss: 0.00001929
Iteration 427/1000 | Loss: 0.00001929
Iteration 428/1000 | Loss: 0.00001929
Iteration 429/1000 | Loss: 0.00001929
Iteration 430/1000 | Loss: 0.00001928
Iteration 431/1000 | Loss: 0.00001928
Iteration 432/1000 | Loss: 0.00001928
Iteration 433/1000 | Loss: 0.00001928
Iteration 434/1000 | Loss: 0.00001928
Iteration 435/1000 | Loss: 0.00001928
Iteration 436/1000 | Loss: 0.00001928
Iteration 437/1000 | Loss: 0.00001928
Iteration 438/1000 | Loss: 0.00001928
Iteration 439/1000 | Loss: 0.00001928
Iteration 440/1000 | Loss: 0.00001927
Iteration 441/1000 | Loss: 0.00001927
Iteration 442/1000 | Loss: 0.00001927
Iteration 443/1000 | Loss: 0.00001927
Iteration 444/1000 | Loss: 0.00001927
Iteration 445/1000 | Loss: 0.00001927
Iteration 446/1000 | Loss: 0.00001927
Iteration 447/1000 | Loss: 0.00001927
Iteration 448/1000 | Loss: 0.00001927
Iteration 449/1000 | Loss: 0.00001927
Iteration 450/1000 | Loss: 0.00001927
Iteration 451/1000 | Loss: 0.00001927
Iteration 452/1000 | Loss: 0.00001927
Iteration 453/1000 | Loss: 0.00001926
Iteration 454/1000 | Loss: 0.00001926
Iteration 455/1000 | Loss: 0.00001926
Iteration 456/1000 | Loss: 0.00001926
Iteration 457/1000 | Loss: 0.00001926
Iteration 458/1000 | Loss: 0.00001926
Iteration 459/1000 | Loss: 0.00001926
Iteration 460/1000 | Loss: 0.00001926
Iteration 461/1000 | Loss: 0.00001926
Iteration 462/1000 | Loss: 0.00001926
Iteration 463/1000 | Loss: 0.00001926
Iteration 464/1000 | Loss: 0.00001926
Iteration 465/1000 | Loss: 0.00001926
Iteration 466/1000 | Loss: 0.00001926
Iteration 467/1000 | Loss: 0.00001926
Iteration 468/1000 | Loss: 0.00001926
Iteration 469/1000 | Loss: 0.00001926
Iteration 470/1000 | Loss: 0.00001926
Iteration 471/1000 | Loss: 0.00001926
Iteration 472/1000 | Loss: 0.00001925
Iteration 473/1000 | Loss: 0.00001925
Iteration 474/1000 | Loss: 0.00001925
Iteration 475/1000 | Loss: 0.00001925
Iteration 476/1000 | Loss: 0.00001925
Iteration 477/1000 | Loss: 0.00001925
Iteration 478/1000 | Loss: 0.00001925
Iteration 479/1000 | Loss: 0.00001925
Iteration 480/1000 | Loss: 0.00001925
Iteration 481/1000 | Loss: 0.00001925
Iteration 482/1000 | Loss: 0.00001925
Iteration 483/1000 | Loss: 0.00001925
Iteration 484/1000 | Loss: 0.00001925
Iteration 485/1000 | Loss: 0.00001925
Iteration 486/1000 | Loss: 0.00001924
Iteration 487/1000 | Loss: 0.00001924
Iteration 488/1000 | Loss: 0.00001924
Iteration 489/1000 | Loss: 0.00001924
Iteration 490/1000 | Loss: 0.00001924
Iteration 491/1000 | Loss: 0.00001924
Iteration 492/1000 | Loss: 0.00001924
Iteration 493/1000 | Loss: 0.00001924
Iteration 494/1000 | Loss: 0.00001924
Iteration 495/1000 | Loss: 0.00001924
Iteration 496/1000 | Loss: 0.00001924
Iteration 497/1000 | Loss: 0.00001924
Iteration 498/1000 | Loss: 0.00001924
Iteration 499/1000 | Loss: 0.00001924
Iteration 500/1000 | Loss: 0.00001924
Iteration 501/1000 | Loss: 0.00001924
Iteration 502/1000 | Loss: 0.00001924
Iteration 503/1000 | Loss: 0.00001924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 503. Stopping optimization.
Last 5 losses: [1.924248135765083e-05, 1.924248135765083e-05, 1.924248135765083e-05, 1.924248135765083e-05, 1.924248135765083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.924248135765083e-05

Optimization complete. Final v2v error: 3.6740071773529053 mm

Highest mean error: 4.249988079071045 mm for frame 158

Lowest mean error: 3.464298725128174 mm for frame 67

Saving results

Total time: 665.5940997600555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386676
Iteration 2/25 | Loss: 0.00093419
Iteration 3/25 | Loss: 0.00086328
Iteration 4/25 | Loss: 0.00085700
Iteration 5/25 | Loss: 0.00085529
Iteration 6/25 | Loss: 0.00085475
Iteration 7/25 | Loss: 0.00085475
Iteration 8/25 | Loss: 0.00085475
Iteration 9/25 | Loss: 0.00085475
Iteration 10/25 | Loss: 0.00085475
Iteration 11/25 | Loss: 0.00085475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00085474643856287, 0.00085474643856287, 0.00085474643856287, 0.00085474643856287, 0.00085474643856287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00085474643856287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.98147750
Iteration 2/25 | Loss: 0.00078287
Iteration 3/25 | Loss: 0.00078287
Iteration 4/25 | Loss: 0.00078287
Iteration 5/25 | Loss: 0.00078286
Iteration 6/25 | Loss: 0.00078286
Iteration 7/25 | Loss: 0.00078286
Iteration 8/25 | Loss: 0.00078286
Iteration 9/25 | Loss: 0.00078286
Iteration 10/25 | Loss: 0.00078286
Iteration 11/25 | Loss: 0.00078286
Iteration 12/25 | Loss: 0.00078286
Iteration 13/25 | Loss: 0.00078286
Iteration 14/25 | Loss: 0.00078286
Iteration 15/25 | Loss: 0.00078286
Iteration 16/25 | Loss: 0.00078286
Iteration 17/25 | Loss: 0.00078286
Iteration 18/25 | Loss: 0.00078286
Iteration 19/25 | Loss: 0.00078286
Iteration 20/25 | Loss: 0.00078286
Iteration 21/25 | Loss: 0.00078286
Iteration 22/25 | Loss: 0.00078286
Iteration 23/25 | Loss: 0.00078286
Iteration 24/25 | Loss: 0.00078286
Iteration 25/25 | Loss: 0.00078286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078286
Iteration 2/1000 | Loss: 0.00002505
Iteration 3/1000 | Loss: 0.00001573
Iteration 4/1000 | Loss: 0.00001289
Iteration 5/1000 | Loss: 0.00001194
Iteration 6/1000 | Loss: 0.00001125
Iteration 7/1000 | Loss: 0.00001094
Iteration 8/1000 | Loss: 0.00001073
Iteration 9/1000 | Loss: 0.00001070
Iteration 10/1000 | Loss: 0.00001060
Iteration 11/1000 | Loss: 0.00001059
Iteration 12/1000 | Loss: 0.00001059
Iteration 13/1000 | Loss: 0.00001058
Iteration 14/1000 | Loss: 0.00001051
Iteration 15/1000 | Loss: 0.00001050
Iteration 16/1000 | Loss: 0.00001039
Iteration 17/1000 | Loss: 0.00001038
Iteration 18/1000 | Loss: 0.00001038
Iteration 19/1000 | Loss: 0.00001036
Iteration 20/1000 | Loss: 0.00001035
Iteration 21/1000 | Loss: 0.00001034
Iteration 22/1000 | Loss: 0.00001034
Iteration 23/1000 | Loss: 0.00001034
Iteration 24/1000 | Loss: 0.00001033
Iteration 25/1000 | Loss: 0.00001033
Iteration 26/1000 | Loss: 0.00001033
Iteration 27/1000 | Loss: 0.00001032
Iteration 28/1000 | Loss: 0.00001031
Iteration 29/1000 | Loss: 0.00001030
Iteration 30/1000 | Loss: 0.00001030
Iteration 31/1000 | Loss: 0.00001030
Iteration 32/1000 | Loss: 0.00001030
Iteration 33/1000 | Loss: 0.00001030
Iteration 34/1000 | Loss: 0.00001030
Iteration 35/1000 | Loss: 0.00001029
Iteration 36/1000 | Loss: 0.00001029
Iteration 37/1000 | Loss: 0.00001029
Iteration 38/1000 | Loss: 0.00001029
Iteration 39/1000 | Loss: 0.00001029
Iteration 40/1000 | Loss: 0.00001029
Iteration 41/1000 | Loss: 0.00001028
Iteration 42/1000 | Loss: 0.00001028
Iteration 43/1000 | Loss: 0.00001028
Iteration 44/1000 | Loss: 0.00001028
Iteration 45/1000 | Loss: 0.00001027
Iteration 46/1000 | Loss: 0.00001027
Iteration 47/1000 | Loss: 0.00001026
Iteration 48/1000 | Loss: 0.00001025
Iteration 49/1000 | Loss: 0.00001025
Iteration 50/1000 | Loss: 0.00001024
Iteration 51/1000 | Loss: 0.00001024
Iteration 52/1000 | Loss: 0.00001024
Iteration 53/1000 | Loss: 0.00001024
Iteration 54/1000 | Loss: 0.00001023
Iteration 55/1000 | Loss: 0.00001023
Iteration 56/1000 | Loss: 0.00001023
Iteration 57/1000 | Loss: 0.00001023
Iteration 58/1000 | Loss: 0.00001023
Iteration 59/1000 | Loss: 0.00001023
Iteration 60/1000 | Loss: 0.00001023
Iteration 61/1000 | Loss: 0.00001023
Iteration 62/1000 | Loss: 0.00001023
Iteration 63/1000 | Loss: 0.00001023
Iteration 64/1000 | Loss: 0.00001023
Iteration 65/1000 | Loss: 0.00001023
Iteration 66/1000 | Loss: 0.00001023
Iteration 67/1000 | Loss: 0.00001023
Iteration 68/1000 | Loss: 0.00001022
Iteration 69/1000 | Loss: 0.00001022
Iteration 70/1000 | Loss: 0.00001022
Iteration 71/1000 | Loss: 0.00001022
Iteration 72/1000 | Loss: 0.00001022
Iteration 73/1000 | Loss: 0.00001022
Iteration 74/1000 | Loss: 0.00001022
Iteration 75/1000 | Loss: 0.00001022
Iteration 76/1000 | Loss: 0.00001021
Iteration 77/1000 | Loss: 0.00001021
Iteration 78/1000 | Loss: 0.00001021
Iteration 79/1000 | Loss: 0.00001021
Iteration 80/1000 | Loss: 0.00001020
Iteration 81/1000 | Loss: 0.00001020
Iteration 82/1000 | Loss: 0.00001019
Iteration 83/1000 | Loss: 0.00001019
Iteration 84/1000 | Loss: 0.00001019
Iteration 85/1000 | Loss: 0.00001018
Iteration 86/1000 | Loss: 0.00001018
Iteration 87/1000 | Loss: 0.00001017
Iteration 88/1000 | Loss: 0.00001017
Iteration 89/1000 | Loss: 0.00001017
Iteration 90/1000 | Loss: 0.00001017
Iteration 91/1000 | Loss: 0.00001016
Iteration 92/1000 | Loss: 0.00001016
Iteration 93/1000 | Loss: 0.00001015
Iteration 94/1000 | Loss: 0.00001014
Iteration 95/1000 | Loss: 0.00001014
Iteration 96/1000 | Loss: 0.00001014
Iteration 97/1000 | Loss: 0.00001012
Iteration 98/1000 | Loss: 0.00001012
Iteration 99/1000 | Loss: 0.00001012
Iteration 100/1000 | Loss: 0.00001012
Iteration 101/1000 | Loss: 0.00001012
Iteration 102/1000 | Loss: 0.00001012
Iteration 103/1000 | Loss: 0.00001012
Iteration 104/1000 | Loss: 0.00001012
Iteration 105/1000 | Loss: 0.00001012
Iteration 106/1000 | Loss: 0.00001011
Iteration 107/1000 | Loss: 0.00001011
Iteration 108/1000 | Loss: 0.00001011
Iteration 109/1000 | Loss: 0.00001010
Iteration 110/1000 | Loss: 0.00001009
Iteration 111/1000 | Loss: 0.00001009
Iteration 112/1000 | Loss: 0.00001009
Iteration 113/1000 | Loss: 0.00001008
Iteration 114/1000 | Loss: 0.00001008
Iteration 115/1000 | Loss: 0.00001008
Iteration 116/1000 | Loss: 0.00001008
Iteration 117/1000 | Loss: 0.00001008
Iteration 118/1000 | Loss: 0.00001007
Iteration 119/1000 | Loss: 0.00001007
Iteration 120/1000 | Loss: 0.00001007
Iteration 121/1000 | Loss: 0.00001007
Iteration 122/1000 | Loss: 0.00001007
Iteration 123/1000 | Loss: 0.00001007
Iteration 124/1000 | Loss: 0.00001007
Iteration 125/1000 | Loss: 0.00001007
Iteration 126/1000 | Loss: 0.00001007
Iteration 127/1000 | Loss: 0.00001007
Iteration 128/1000 | Loss: 0.00001006
Iteration 129/1000 | Loss: 0.00001006
Iteration 130/1000 | Loss: 0.00001006
Iteration 131/1000 | Loss: 0.00001006
Iteration 132/1000 | Loss: 0.00001006
Iteration 133/1000 | Loss: 0.00001006
Iteration 134/1000 | Loss: 0.00001006
Iteration 135/1000 | Loss: 0.00001006
Iteration 136/1000 | Loss: 0.00001006
Iteration 137/1000 | Loss: 0.00001006
Iteration 138/1000 | Loss: 0.00001006
Iteration 139/1000 | Loss: 0.00001005
Iteration 140/1000 | Loss: 0.00001005
Iteration 141/1000 | Loss: 0.00001005
Iteration 142/1000 | Loss: 0.00001005
Iteration 143/1000 | Loss: 0.00001005
Iteration 144/1000 | Loss: 0.00001005
Iteration 145/1000 | Loss: 0.00001005
Iteration 146/1000 | Loss: 0.00001005
Iteration 147/1000 | Loss: 0.00001005
Iteration 148/1000 | Loss: 0.00001005
Iteration 149/1000 | Loss: 0.00001005
Iteration 150/1000 | Loss: 0.00001005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.0053434380097315e-05, 1.0053434380097315e-05, 1.0053434380097315e-05, 1.0053434380097315e-05, 1.0053434380097315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0053434380097315e-05

Optimization complete. Final v2v error: 2.7271432876586914 mm

Highest mean error: 2.906252384185791 mm for frame 62

Lowest mean error: 2.38747501373291 mm for frame 147

Saving results

Total time: 35.057865619659424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475467
Iteration 2/25 | Loss: 0.00107729
Iteration 3/25 | Loss: 0.00095427
Iteration 4/25 | Loss: 0.00094508
Iteration 5/25 | Loss: 0.00094264
Iteration 6/25 | Loss: 0.00094197
Iteration 7/25 | Loss: 0.00094197
Iteration 8/25 | Loss: 0.00094197
Iteration 9/25 | Loss: 0.00094197
Iteration 10/25 | Loss: 0.00094197
Iteration 11/25 | Loss: 0.00094197
Iteration 12/25 | Loss: 0.00094197
Iteration 13/25 | Loss: 0.00094197
Iteration 14/25 | Loss: 0.00094197
Iteration 15/25 | Loss: 0.00094197
Iteration 16/25 | Loss: 0.00094197
Iteration 17/25 | Loss: 0.00094197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009419708512723446, 0.0009419708512723446, 0.0009419708512723446, 0.0009419708512723446, 0.0009419708512723446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009419708512723446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31183445
Iteration 2/25 | Loss: 0.00084552
Iteration 3/25 | Loss: 0.00084552
Iteration 4/25 | Loss: 0.00084552
Iteration 5/25 | Loss: 0.00084552
Iteration 6/25 | Loss: 0.00084552
Iteration 7/25 | Loss: 0.00084551
Iteration 8/25 | Loss: 0.00084551
Iteration 9/25 | Loss: 0.00084551
Iteration 10/25 | Loss: 0.00084551
Iteration 11/25 | Loss: 0.00084551
Iteration 12/25 | Loss: 0.00084551
Iteration 13/25 | Loss: 0.00084551
Iteration 14/25 | Loss: 0.00084551
Iteration 15/25 | Loss: 0.00084551
Iteration 16/25 | Loss: 0.00084551
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000845514063257724, 0.000845514063257724, 0.000845514063257724, 0.000845514063257724, 0.000845514063257724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000845514063257724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084551
Iteration 2/1000 | Loss: 0.00003278
Iteration 3/1000 | Loss: 0.00002255
Iteration 4/1000 | Loss: 0.00001908
Iteration 5/1000 | Loss: 0.00001803
Iteration 6/1000 | Loss: 0.00001725
Iteration 7/1000 | Loss: 0.00001682
Iteration 8/1000 | Loss: 0.00001654
Iteration 9/1000 | Loss: 0.00001628
Iteration 10/1000 | Loss: 0.00001614
Iteration 11/1000 | Loss: 0.00001614
Iteration 12/1000 | Loss: 0.00001612
Iteration 13/1000 | Loss: 0.00001611
Iteration 14/1000 | Loss: 0.00001600
Iteration 15/1000 | Loss: 0.00001599
Iteration 16/1000 | Loss: 0.00001595
Iteration 17/1000 | Loss: 0.00001595
Iteration 18/1000 | Loss: 0.00001593
Iteration 19/1000 | Loss: 0.00001589
Iteration 20/1000 | Loss: 0.00001588
Iteration 21/1000 | Loss: 0.00001584
Iteration 22/1000 | Loss: 0.00001583
Iteration 23/1000 | Loss: 0.00001581
Iteration 24/1000 | Loss: 0.00001581
Iteration 25/1000 | Loss: 0.00001581
Iteration 26/1000 | Loss: 0.00001581
Iteration 27/1000 | Loss: 0.00001581
Iteration 28/1000 | Loss: 0.00001580
Iteration 29/1000 | Loss: 0.00001579
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001566
Iteration 32/1000 | Loss: 0.00001560
Iteration 33/1000 | Loss: 0.00001554
Iteration 34/1000 | Loss: 0.00001553
Iteration 35/1000 | Loss: 0.00001552
Iteration 36/1000 | Loss: 0.00001551
Iteration 37/1000 | Loss: 0.00001551
Iteration 38/1000 | Loss: 0.00001551
Iteration 39/1000 | Loss: 0.00001551
Iteration 40/1000 | Loss: 0.00001551
Iteration 41/1000 | Loss: 0.00001551
Iteration 42/1000 | Loss: 0.00001551
Iteration 43/1000 | Loss: 0.00001551
Iteration 44/1000 | Loss: 0.00001551
Iteration 45/1000 | Loss: 0.00001550
Iteration 46/1000 | Loss: 0.00001550
Iteration 47/1000 | Loss: 0.00001550
Iteration 48/1000 | Loss: 0.00001550
Iteration 49/1000 | Loss: 0.00001549
Iteration 50/1000 | Loss: 0.00001547
Iteration 51/1000 | Loss: 0.00001547
Iteration 52/1000 | Loss: 0.00001546
Iteration 53/1000 | Loss: 0.00001546
Iteration 54/1000 | Loss: 0.00001546
Iteration 55/1000 | Loss: 0.00001546
Iteration 56/1000 | Loss: 0.00001546
Iteration 57/1000 | Loss: 0.00001546
Iteration 58/1000 | Loss: 0.00001546
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001546
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001546
Iteration 63/1000 | Loss: 0.00001545
Iteration 64/1000 | Loss: 0.00001545
Iteration 65/1000 | Loss: 0.00001545
Iteration 66/1000 | Loss: 0.00001544
Iteration 67/1000 | Loss: 0.00001544
Iteration 68/1000 | Loss: 0.00001544
Iteration 69/1000 | Loss: 0.00001543
Iteration 70/1000 | Loss: 0.00001543
Iteration 71/1000 | Loss: 0.00001543
Iteration 72/1000 | Loss: 0.00001543
Iteration 73/1000 | Loss: 0.00001542
Iteration 74/1000 | Loss: 0.00001542
Iteration 75/1000 | Loss: 0.00001542
Iteration 76/1000 | Loss: 0.00001542
Iteration 77/1000 | Loss: 0.00001542
Iteration 78/1000 | Loss: 0.00001542
Iteration 79/1000 | Loss: 0.00001542
Iteration 80/1000 | Loss: 0.00001542
Iteration 81/1000 | Loss: 0.00001542
Iteration 82/1000 | Loss: 0.00001542
Iteration 83/1000 | Loss: 0.00001541
Iteration 84/1000 | Loss: 0.00001541
Iteration 85/1000 | Loss: 0.00001541
Iteration 86/1000 | Loss: 0.00001541
Iteration 87/1000 | Loss: 0.00001541
Iteration 88/1000 | Loss: 0.00001541
Iteration 89/1000 | Loss: 0.00001541
Iteration 90/1000 | Loss: 0.00001541
Iteration 91/1000 | Loss: 0.00001541
Iteration 92/1000 | Loss: 0.00001541
Iteration 93/1000 | Loss: 0.00001541
Iteration 94/1000 | Loss: 0.00001541
Iteration 95/1000 | Loss: 0.00001541
Iteration 96/1000 | Loss: 0.00001541
Iteration 97/1000 | Loss: 0.00001541
Iteration 98/1000 | Loss: 0.00001541
Iteration 99/1000 | Loss: 0.00001541
Iteration 100/1000 | Loss: 0.00001541
Iteration 101/1000 | Loss: 0.00001541
Iteration 102/1000 | Loss: 0.00001541
Iteration 103/1000 | Loss: 0.00001541
Iteration 104/1000 | Loss: 0.00001541
Iteration 105/1000 | Loss: 0.00001541
Iteration 106/1000 | Loss: 0.00001541
Iteration 107/1000 | Loss: 0.00001541
Iteration 108/1000 | Loss: 0.00001541
Iteration 109/1000 | Loss: 0.00001541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.5409425031975843e-05, 1.5409425031975843e-05, 1.5409425031975843e-05, 1.5409425031975843e-05, 1.5409425031975843e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5409425031975843e-05

Optimization complete. Final v2v error: 3.2574520111083984 mm

Highest mean error: 3.607320547103882 mm for frame 44

Lowest mean error: 3.0066819190979004 mm for frame 97

Saving results

Total time: 36.95898509025574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01164193
Iteration 2/25 | Loss: 0.00178630
Iteration 3/25 | Loss: 0.00108039
Iteration 4/25 | Loss: 0.00103623
Iteration 5/25 | Loss: 0.00105830
Iteration 6/25 | Loss: 0.00103176
Iteration 7/25 | Loss: 0.00102731
Iteration 8/25 | Loss: 0.00102317
Iteration 9/25 | Loss: 0.00101843
Iteration 10/25 | Loss: 0.00101584
Iteration 11/25 | Loss: 0.00101555
Iteration 12/25 | Loss: 0.00101548
Iteration 13/25 | Loss: 0.00101548
Iteration 14/25 | Loss: 0.00101548
Iteration 15/25 | Loss: 0.00101548
Iteration 16/25 | Loss: 0.00101547
Iteration 17/25 | Loss: 0.00101547
Iteration 18/25 | Loss: 0.00101547
Iteration 19/25 | Loss: 0.00101547
Iteration 20/25 | Loss: 0.00101547
Iteration 21/25 | Loss: 0.00101547
Iteration 22/25 | Loss: 0.00101547
Iteration 23/25 | Loss: 0.00101547
Iteration 24/25 | Loss: 0.00101547
Iteration 25/25 | Loss: 0.00101547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31308663
Iteration 2/25 | Loss: 0.00060376
Iteration 3/25 | Loss: 0.00060375
Iteration 4/25 | Loss: 0.00060375
Iteration 5/25 | Loss: 0.00060375
Iteration 6/25 | Loss: 0.00060375
Iteration 7/25 | Loss: 0.00060375
Iteration 8/25 | Loss: 0.00060375
Iteration 9/25 | Loss: 0.00060375
Iteration 10/25 | Loss: 0.00060375
Iteration 11/25 | Loss: 0.00060375
Iteration 12/25 | Loss: 0.00060375
Iteration 13/25 | Loss: 0.00060375
Iteration 14/25 | Loss: 0.00060375
Iteration 15/25 | Loss: 0.00060375
Iteration 16/25 | Loss: 0.00060375
Iteration 17/25 | Loss: 0.00060375
Iteration 18/25 | Loss: 0.00060375
Iteration 19/25 | Loss: 0.00060375
Iteration 20/25 | Loss: 0.00060375
Iteration 21/25 | Loss: 0.00060375
Iteration 22/25 | Loss: 0.00060375
Iteration 23/25 | Loss: 0.00060375
Iteration 24/25 | Loss: 0.00060375
Iteration 25/25 | Loss: 0.00060375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060375
Iteration 2/1000 | Loss: 0.00004744
Iteration 3/1000 | Loss: 0.00003433
Iteration 4/1000 | Loss: 0.00003015
Iteration 5/1000 | Loss: 0.00002880
Iteration 6/1000 | Loss: 0.00002786
Iteration 7/1000 | Loss: 0.00002740
Iteration 8/1000 | Loss: 0.00002707
Iteration 9/1000 | Loss: 0.00002681
Iteration 10/1000 | Loss: 0.00002660
Iteration 11/1000 | Loss: 0.00002656
Iteration 12/1000 | Loss: 0.00002646
Iteration 13/1000 | Loss: 0.00002639
Iteration 14/1000 | Loss: 0.00002630
Iteration 15/1000 | Loss: 0.00002630
Iteration 16/1000 | Loss: 0.00002629
Iteration 17/1000 | Loss: 0.00002628
Iteration 18/1000 | Loss: 0.00002628
Iteration 19/1000 | Loss: 0.00002628
Iteration 20/1000 | Loss: 0.00002627
Iteration 21/1000 | Loss: 0.00002625
Iteration 22/1000 | Loss: 0.00002624
Iteration 23/1000 | Loss: 0.00002624
Iteration 24/1000 | Loss: 0.00002624
Iteration 25/1000 | Loss: 0.00002624
Iteration 26/1000 | Loss: 0.00002623
Iteration 27/1000 | Loss: 0.00002622
Iteration 28/1000 | Loss: 0.00002622
Iteration 29/1000 | Loss: 0.00002622
Iteration 30/1000 | Loss: 0.00002621
Iteration 31/1000 | Loss: 0.00002621
Iteration 32/1000 | Loss: 0.00002620
Iteration 33/1000 | Loss: 0.00002620
Iteration 34/1000 | Loss: 0.00002620
Iteration 35/1000 | Loss: 0.00002620
Iteration 36/1000 | Loss: 0.00002618
Iteration 37/1000 | Loss: 0.00002617
Iteration 38/1000 | Loss: 0.00002617
Iteration 39/1000 | Loss: 0.00002616
Iteration 40/1000 | Loss: 0.00002615
Iteration 41/1000 | Loss: 0.00002615
Iteration 42/1000 | Loss: 0.00002614
Iteration 43/1000 | Loss: 0.00002613
Iteration 44/1000 | Loss: 0.00002613
Iteration 45/1000 | Loss: 0.00002613
Iteration 46/1000 | Loss: 0.00002613
Iteration 47/1000 | Loss: 0.00002613
Iteration 48/1000 | Loss: 0.00002613
Iteration 49/1000 | Loss: 0.00002613
Iteration 50/1000 | Loss: 0.00002613
Iteration 51/1000 | Loss: 0.00002612
Iteration 52/1000 | Loss: 0.00002612
Iteration 53/1000 | Loss: 0.00002612
Iteration 54/1000 | Loss: 0.00002612
Iteration 55/1000 | Loss: 0.00002612
Iteration 56/1000 | Loss: 0.00002612
Iteration 57/1000 | Loss: 0.00002612
Iteration 58/1000 | Loss: 0.00002611
Iteration 59/1000 | Loss: 0.00002611
Iteration 60/1000 | Loss: 0.00002610
Iteration 61/1000 | Loss: 0.00002610
Iteration 62/1000 | Loss: 0.00002610
Iteration 63/1000 | Loss: 0.00002610
Iteration 64/1000 | Loss: 0.00002610
Iteration 65/1000 | Loss: 0.00002610
Iteration 66/1000 | Loss: 0.00002610
Iteration 67/1000 | Loss: 0.00002610
Iteration 68/1000 | Loss: 0.00002610
Iteration 69/1000 | Loss: 0.00002610
Iteration 70/1000 | Loss: 0.00002610
Iteration 71/1000 | Loss: 0.00002610
Iteration 72/1000 | Loss: 0.00002610
Iteration 73/1000 | Loss: 0.00002610
Iteration 74/1000 | Loss: 0.00002610
Iteration 75/1000 | Loss: 0.00002610
Iteration 76/1000 | Loss: 0.00002610
Iteration 77/1000 | Loss: 0.00002610
Iteration 78/1000 | Loss: 0.00002610
Iteration 79/1000 | Loss: 0.00002610
Iteration 80/1000 | Loss: 0.00002610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.6096462534042075e-05, 2.6096462534042075e-05, 2.6096462534042075e-05, 2.6096462534042075e-05, 2.6096462534042075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6096462534042075e-05

Optimization complete. Final v2v error: 3.8283145427703857 mm

Highest mean error: 20.269804000854492 mm for frame 204

Lowest mean error: 3.0769667625427246 mm for frame 0

Saving results

Total time: 51.144652128219604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00351497
Iteration 2/25 | Loss: 0.00098589
Iteration 3/25 | Loss: 0.00090619
Iteration 4/25 | Loss: 0.00088687
Iteration 5/25 | Loss: 0.00088258
Iteration 6/25 | Loss: 0.00088163
Iteration 7/25 | Loss: 0.00088122
Iteration 8/25 | Loss: 0.00088122
Iteration 9/25 | Loss: 0.00088122
Iteration 10/25 | Loss: 0.00088122
Iteration 11/25 | Loss: 0.00088122
Iteration 12/25 | Loss: 0.00088122
Iteration 13/25 | Loss: 0.00088122
Iteration 14/25 | Loss: 0.00088122
Iteration 15/25 | Loss: 0.00088122
Iteration 16/25 | Loss: 0.00088122
Iteration 17/25 | Loss: 0.00088122
Iteration 18/25 | Loss: 0.00088122
Iteration 19/25 | Loss: 0.00088122
Iteration 20/25 | Loss: 0.00088122
Iteration 21/25 | Loss: 0.00088122
Iteration 22/25 | Loss: 0.00088122
Iteration 23/25 | Loss: 0.00088122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008812216692604125, 0.0008812216692604125, 0.0008812216692604125, 0.0008812216692604125, 0.0008812216692604125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008812216692604125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34705019
Iteration 2/25 | Loss: 0.00090081
Iteration 3/25 | Loss: 0.00090081
Iteration 4/25 | Loss: 0.00090081
Iteration 5/25 | Loss: 0.00090081
Iteration 6/25 | Loss: 0.00090081
Iteration 7/25 | Loss: 0.00090081
Iteration 8/25 | Loss: 0.00090081
Iteration 9/25 | Loss: 0.00090081
Iteration 10/25 | Loss: 0.00090081
Iteration 11/25 | Loss: 0.00090081
Iteration 12/25 | Loss: 0.00090081
Iteration 13/25 | Loss: 0.00090081
Iteration 14/25 | Loss: 0.00090081
Iteration 15/25 | Loss: 0.00090081
Iteration 16/25 | Loss: 0.00090081
Iteration 17/25 | Loss: 0.00090081
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009008101769722998, 0.0009008101769722998, 0.0009008101769722998, 0.0009008101769722998, 0.0009008101769722998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009008101769722998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090081
Iteration 2/1000 | Loss: 0.00003385
Iteration 3/1000 | Loss: 0.00001949
Iteration 4/1000 | Loss: 0.00001549
Iteration 5/1000 | Loss: 0.00001429
Iteration 6/1000 | Loss: 0.00001351
Iteration 7/1000 | Loss: 0.00001312
Iteration 8/1000 | Loss: 0.00001279
Iteration 9/1000 | Loss: 0.00001276
Iteration 10/1000 | Loss: 0.00001269
Iteration 11/1000 | Loss: 0.00001268
Iteration 12/1000 | Loss: 0.00001268
Iteration 13/1000 | Loss: 0.00001265
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001253
Iteration 16/1000 | Loss: 0.00001253
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001251
Iteration 19/1000 | Loss: 0.00001251
Iteration 20/1000 | Loss: 0.00001250
Iteration 21/1000 | Loss: 0.00001248
Iteration 22/1000 | Loss: 0.00001248
Iteration 23/1000 | Loss: 0.00001247
Iteration 24/1000 | Loss: 0.00001247
Iteration 25/1000 | Loss: 0.00001246
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001242
Iteration 28/1000 | Loss: 0.00001242
Iteration 29/1000 | Loss: 0.00001241
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001241
Iteration 32/1000 | Loss: 0.00001241
Iteration 33/1000 | Loss: 0.00001241
Iteration 34/1000 | Loss: 0.00001241
Iteration 35/1000 | Loss: 0.00001241
Iteration 36/1000 | Loss: 0.00001241
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001239
Iteration 39/1000 | Loss: 0.00001238
Iteration 40/1000 | Loss: 0.00001238
Iteration 41/1000 | Loss: 0.00001238
Iteration 42/1000 | Loss: 0.00001238
Iteration 43/1000 | Loss: 0.00001238
Iteration 44/1000 | Loss: 0.00001237
Iteration 45/1000 | Loss: 0.00001237
Iteration 46/1000 | Loss: 0.00001237
Iteration 47/1000 | Loss: 0.00001237
Iteration 48/1000 | Loss: 0.00001237
Iteration 49/1000 | Loss: 0.00001236
Iteration 50/1000 | Loss: 0.00001236
Iteration 51/1000 | Loss: 0.00001236
Iteration 52/1000 | Loss: 0.00001235
Iteration 53/1000 | Loss: 0.00001235
Iteration 54/1000 | Loss: 0.00001235
Iteration 55/1000 | Loss: 0.00001235
Iteration 56/1000 | Loss: 0.00001235
Iteration 57/1000 | Loss: 0.00001235
Iteration 58/1000 | Loss: 0.00001234
Iteration 59/1000 | Loss: 0.00001234
Iteration 60/1000 | Loss: 0.00001234
Iteration 61/1000 | Loss: 0.00001234
Iteration 62/1000 | Loss: 0.00001234
Iteration 63/1000 | Loss: 0.00001234
Iteration 64/1000 | Loss: 0.00001234
Iteration 65/1000 | Loss: 0.00001234
Iteration 66/1000 | Loss: 0.00001234
Iteration 67/1000 | Loss: 0.00001234
Iteration 68/1000 | Loss: 0.00001233
Iteration 69/1000 | Loss: 0.00001233
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001233
Iteration 72/1000 | Loss: 0.00001232
Iteration 73/1000 | Loss: 0.00001232
Iteration 74/1000 | Loss: 0.00001232
Iteration 75/1000 | Loss: 0.00001231
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001231
Iteration 78/1000 | Loss: 0.00001231
Iteration 79/1000 | Loss: 0.00001231
Iteration 80/1000 | Loss: 0.00001231
Iteration 81/1000 | Loss: 0.00001231
Iteration 82/1000 | Loss: 0.00001231
Iteration 83/1000 | Loss: 0.00001231
Iteration 84/1000 | Loss: 0.00001231
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001231
Iteration 87/1000 | Loss: 0.00001231
Iteration 88/1000 | Loss: 0.00001230
Iteration 89/1000 | Loss: 0.00001230
Iteration 90/1000 | Loss: 0.00001230
Iteration 91/1000 | Loss: 0.00001230
Iteration 92/1000 | Loss: 0.00001230
Iteration 93/1000 | Loss: 0.00001230
Iteration 94/1000 | Loss: 0.00001230
Iteration 95/1000 | Loss: 0.00001230
Iteration 96/1000 | Loss: 0.00001229
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001228
Iteration 101/1000 | Loss: 0.00001228
Iteration 102/1000 | Loss: 0.00001227
Iteration 103/1000 | Loss: 0.00001227
Iteration 104/1000 | Loss: 0.00001227
Iteration 105/1000 | Loss: 0.00001227
Iteration 106/1000 | Loss: 0.00001227
Iteration 107/1000 | Loss: 0.00001227
Iteration 108/1000 | Loss: 0.00001227
Iteration 109/1000 | Loss: 0.00001227
Iteration 110/1000 | Loss: 0.00001227
Iteration 111/1000 | Loss: 0.00001227
Iteration 112/1000 | Loss: 0.00001226
Iteration 113/1000 | Loss: 0.00001226
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001226
Iteration 116/1000 | Loss: 0.00001225
Iteration 117/1000 | Loss: 0.00001225
Iteration 118/1000 | Loss: 0.00001225
Iteration 119/1000 | Loss: 0.00001225
Iteration 120/1000 | Loss: 0.00001225
Iteration 121/1000 | Loss: 0.00001224
Iteration 122/1000 | Loss: 0.00001224
Iteration 123/1000 | Loss: 0.00001224
Iteration 124/1000 | Loss: 0.00001224
Iteration 125/1000 | Loss: 0.00001224
Iteration 126/1000 | Loss: 0.00001224
Iteration 127/1000 | Loss: 0.00001224
Iteration 128/1000 | Loss: 0.00001224
Iteration 129/1000 | Loss: 0.00001224
Iteration 130/1000 | Loss: 0.00001223
Iteration 131/1000 | Loss: 0.00001223
Iteration 132/1000 | Loss: 0.00001223
Iteration 133/1000 | Loss: 0.00001223
Iteration 134/1000 | Loss: 0.00001222
Iteration 135/1000 | Loss: 0.00001222
Iteration 136/1000 | Loss: 0.00001222
Iteration 137/1000 | Loss: 0.00001221
Iteration 138/1000 | Loss: 0.00001221
Iteration 139/1000 | Loss: 0.00001221
Iteration 140/1000 | Loss: 0.00001221
Iteration 141/1000 | Loss: 0.00001221
Iteration 142/1000 | Loss: 0.00001221
Iteration 143/1000 | Loss: 0.00001221
Iteration 144/1000 | Loss: 0.00001220
Iteration 145/1000 | Loss: 0.00001220
Iteration 146/1000 | Loss: 0.00001220
Iteration 147/1000 | Loss: 0.00001220
Iteration 148/1000 | Loss: 0.00001220
Iteration 149/1000 | Loss: 0.00001220
Iteration 150/1000 | Loss: 0.00001220
Iteration 151/1000 | Loss: 0.00001220
Iteration 152/1000 | Loss: 0.00001220
Iteration 153/1000 | Loss: 0.00001220
Iteration 154/1000 | Loss: 0.00001220
Iteration 155/1000 | Loss: 0.00001220
Iteration 156/1000 | Loss: 0.00001220
Iteration 157/1000 | Loss: 0.00001220
Iteration 158/1000 | Loss: 0.00001220
Iteration 159/1000 | Loss: 0.00001219
Iteration 160/1000 | Loss: 0.00001219
Iteration 161/1000 | Loss: 0.00001219
Iteration 162/1000 | Loss: 0.00001219
Iteration 163/1000 | Loss: 0.00001219
Iteration 164/1000 | Loss: 0.00001219
Iteration 165/1000 | Loss: 0.00001219
Iteration 166/1000 | Loss: 0.00001219
Iteration 167/1000 | Loss: 0.00001219
Iteration 168/1000 | Loss: 0.00001219
Iteration 169/1000 | Loss: 0.00001219
Iteration 170/1000 | Loss: 0.00001219
Iteration 171/1000 | Loss: 0.00001219
Iteration 172/1000 | Loss: 0.00001219
Iteration 173/1000 | Loss: 0.00001219
Iteration 174/1000 | Loss: 0.00001219
Iteration 175/1000 | Loss: 0.00001219
Iteration 176/1000 | Loss: 0.00001219
Iteration 177/1000 | Loss: 0.00001219
Iteration 178/1000 | Loss: 0.00001219
Iteration 179/1000 | Loss: 0.00001219
Iteration 180/1000 | Loss: 0.00001219
Iteration 181/1000 | Loss: 0.00001219
Iteration 182/1000 | Loss: 0.00001219
Iteration 183/1000 | Loss: 0.00001219
Iteration 184/1000 | Loss: 0.00001219
Iteration 185/1000 | Loss: 0.00001219
Iteration 186/1000 | Loss: 0.00001219
Iteration 187/1000 | Loss: 0.00001219
Iteration 188/1000 | Loss: 0.00001219
Iteration 189/1000 | Loss: 0.00001219
Iteration 190/1000 | Loss: 0.00001219
Iteration 191/1000 | Loss: 0.00001219
Iteration 192/1000 | Loss: 0.00001219
Iteration 193/1000 | Loss: 0.00001219
Iteration 194/1000 | Loss: 0.00001219
Iteration 195/1000 | Loss: 0.00001219
Iteration 196/1000 | Loss: 0.00001219
Iteration 197/1000 | Loss: 0.00001219
Iteration 198/1000 | Loss: 0.00001219
Iteration 199/1000 | Loss: 0.00001219
Iteration 200/1000 | Loss: 0.00001219
Iteration 201/1000 | Loss: 0.00001219
Iteration 202/1000 | Loss: 0.00001219
Iteration 203/1000 | Loss: 0.00001219
Iteration 204/1000 | Loss: 0.00001219
Iteration 205/1000 | Loss: 0.00001219
Iteration 206/1000 | Loss: 0.00001219
Iteration 207/1000 | Loss: 0.00001219
Iteration 208/1000 | Loss: 0.00001219
Iteration 209/1000 | Loss: 0.00001219
Iteration 210/1000 | Loss: 0.00001219
Iteration 211/1000 | Loss: 0.00001219
Iteration 212/1000 | Loss: 0.00001219
Iteration 213/1000 | Loss: 0.00001219
Iteration 214/1000 | Loss: 0.00001219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.2187433640065137e-05, 1.2187433640065137e-05, 1.2187433640065137e-05, 1.2187433640065137e-05, 1.2187433640065137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2187433640065137e-05

Optimization complete. Final v2v error: 2.9944117069244385 mm

Highest mean error: 3.2650978565216064 mm for frame 72

Lowest mean error: 2.546201229095459 mm for frame 13

Saving results

Total time: 37.97250843048096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01135252
Iteration 2/25 | Loss: 0.00250157
Iteration 3/25 | Loss: 0.00168894
Iteration 4/25 | Loss: 0.00157266
Iteration 5/25 | Loss: 0.00141426
Iteration 6/25 | Loss: 0.00143988
Iteration 7/25 | Loss: 0.00138303
Iteration 8/25 | Loss: 0.00136123
Iteration 9/25 | Loss: 0.00123820
Iteration 10/25 | Loss: 0.00122069
Iteration 11/25 | Loss: 0.00118677
Iteration 12/25 | Loss: 0.00117596
Iteration 13/25 | Loss: 0.00114327
Iteration 14/25 | Loss: 0.00111208
Iteration 15/25 | Loss: 0.00110850
Iteration 16/25 | Loss: 0.00110646
Iteration 17/25 | Loss: 0.00110372
Iteration 18/25 | Loss: 0.00108162
Iteration 19/25 | Loss: 0.00107150
Iteration 20/25 | Loss: 0.00106631
Iteration 21/25 | Loss: 0.00106174
Iteration 22/25 | Loss: 0.00105914
Iteration 23/25 | Loss: 0.00106141
Iteration 24/25 | Loss: 0.00105352
Iteration 25/25 | Loss: 0.00105687

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05497432
Iteration 2/25 | Loss: 0.00109275
Iteration 3/25 | Loss: 0.00109275
Iteration 4/25 | Loss: 0.00109275
Iteration 5/25 | Loss: 0.00109275
Iteration 6/25 | Loss: 0.00109275
Iteration 7/25 | Loss: 0.00109275
Iteration 8/25 | Loss: 0.00109275
Iteration 9/25 | Loss: 0.00109275
Iteration 10/25 | Loss: 0.00109275
Iteration 11/25 | Loss: 0.00109275
Iteration 12/25 | Loss: 0.00109275
Iteration 13/25 | Loss: 0.00109275
Iteration 14/25 | Loss: 0.00109275
Iteration 15/25 | Loss: 0.00109275
Iteration 16/25 | Loss: 0.00109275
Iteration 17/25 | Loss: 0.00109275
Iteration 18/25 | Loss: 0.00109275
Iteration 19/25 | Loss: 0.00109275
Iteration 20/25 | Loss: 0.00109275
Iteration 21/25 | Loss: 0.00109275
Iteration 22/25 | Loss: 0.00109275
Iteration 23/25 | Loss: 0.00109275
Iteration 24/25 | Loss: 0.00109275
Iteration 25/25 | Loss: 0.00109275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109275
Iteration 2/1000 | Loss: 0.00063883
Iteration 3/1000 | Loss: 0.00019410
Iteration 4/1000 | Loss: 0.00012825
Iteration 5/1000 | Loss: 0.00012049
Iteration 6/1000 | Loss: 0.00012257
Iteration 7/1000 | Loss: 0.00009516
Iteration 8/1000 | Loss: 0.00009975
Iteration 9/1000 | Loss: 0.00020759
Iteration 10/1000 | Loss: 0.00010704
Iteration 11/1000 | Loss: 0.00007703
Iteration 12/1000 | Loss: 0.00011535
Iteration 13/1000 | Loss: 0.00012185
Iteration 14/1000 | Loss: 0.00017312
Iteration 15/1000 | Loss: 0.00017731
Iteration 16/1000 | Loss: 0.00010420
Iteration 17/1000 | Loss: 0.00013854
Iteration 18/1000 | Loss: 0.00018576
Iteration 19/1000 | Loss: 0.00016660
Iteration 20/1000 | Loss: 0.00018259
Iteration 21/1000 | Loss: 0.00031231
Iteration 22/1000 | Loss: 0.00021762
Iteration 23/1000 | Loss: 0.00013009
Iteration 24/1000 | Loss: 0.00016230
Iteration 25/1000 | Loss: 0.00010520
Iteration 26/1000 | Loss: 0.00014202
Iteration 27/1000 | Loss: 0.00014119
Iteration 28/1000 | Loss: 0.00027999
Iteration 29/1000 | Loss: 0.00015075
Iteration 30/1000 | Loss: 0.00059388
Iteration 31/1000 | Loss: 0.00023564
Iteration 32/1000 | Loss: 0.00009815
Iteration 33/1000 | Loss: 0.00012480
Iteration 34/1000 | Loss: 0.00015593
Iteration 35/1000 | Loss: 0.00013126
Iteration 36/1000 | Loss: 0.00012301
Iteration 37/1000 | Loss: 0.00011893
Iteration 38/1000 | Loss: 0.00012027
Iteration 39/1000 | Loss: 0.00012244
Iteration 40/1000 | Loss: 0.00012477
Iteration 41/1000 | Loss: 0.00025827
Iteration 42/1000 | Loss: 0.00019025
Iteration 43/1000 | Loss: 0.00051095
Iteration 44/1000 | Loss: 0.00032702
Iteration 45/1000 | Loss: 0.00037631
Iteration 46/1000 | Loss: 0.00021366
Iteration 47/1000 | Loss: 0.00009196
Iteration 48/1000 | Loss: 0.00010489
Iteration 49/1000 | Loss: 0.00008082
Iteration 50/1000 | Loss: 0.00010081
Iteration 51/1000 | Loss: 0.00039818
Iteration 52/1000 | Loss: 0.00020017
Iteration 53/1000 | Loss: 0.00025817
Iteration 54/1000 | Loss: 0.00014176
Iteration 55/1000 | Loss: 0.00023872
Iteration 56/1000 | Loss: 0.00011869
Iteration 57/1000 | Loss: 0.00021074
Iteration 58/1000 | Loss: 0.00009368
Iteration 59/1000 | Loss: 0.00006817
Iteration 60/1000 | Loss: 0.00004537
Iteration 61/1000 | Loss: 0.00005593
Iteration 62/1000 | Loss: 0.00005462
Iteration 63/1000 | Loss: 0.00003650
Iteration 64/1000 | Loss: 0.00005536
Iteration 65/1000 | Loss: 0.00005684
Iteration 66/1000 | Loss: 0.00004001
Iteration 67/1000 | Loss: 0.00004359
Iteration 68/1000 | Loss: 0.00005570
Iteration 69/1000 | Loss: 0.00005631
Iteration 70/1000 | Loss: 0.00005554
Iteration 71/1000 | Loss: 0.00004939
Iteration 72/1000 | Loss: 0.00005333
Iteration 73/1000 | Loss: 0.00005457
Iteration 74/1000 | Loss: 0.00004022
Iteration 75/1000 | Loss: 0.00003559
Iteration 76/1000 | Loss: 0.00004446
Iteration 77/1000 | Loss: 0.00007552
Iteration 78/1000 | Loss: 0.00004383
Iteration 79/1000 | Loss: 0.00007628
Iteration 80/1000 | Loss: 0.00004883
Iteration 81/1000 | Loss: 0.00005225
Iteration 82/1000 | Loss: 0.00005307
Iteration 83/1000 | Loss: 0.00006789
Iteration 84/1000 | Loss: 0.00004049
Iteration 85/1000 | Loss: 0.00003452
Iteration 86/1000 | Loss: 0.00003102
Iteration 87/1000 | Loss: 0.00002949
Iteration 88/1000 | Loss: 0.00002887
Iteration 89/1000 | Loss: 0.00002827
Iteration 90/1000 | Loss: 0.00018441
Iteration 91/1000 | Loss: 0.00009006
Iteration 92/1000 | Loss: 0.00015238
Iteration 93/1000 | Loss: 0.00004410
Iteration 94/1000 | Loss: 0.00002760
Iteration 95/1000 | Loss: 0.00002754
Iteration 96/1000 | Loss: 0.00002753
Iteration 97/1000 | Loss: 0.00002748
Iteration 98/1000 | Loss: 0.00002746
Iteration 99/1000 | Loss: 0.00002745
Iteration 100/1000 | Loss: 0.00002745
Iteration 101/1000 | Loss: 0.00002745
Iteration 102/1000 | Loss: 0.00002744
Iteration 103/1000 | Loss: 0.00002744
Iteration 104/1000 | Loss: 0.00002744
Iteration 105/1000 | Loss: 0.00002744
Iteration 106/1000 | Loss: 0.00002743
Iteration 107/1000 | Loss: 0.00002742
Iteration 108/1000 | Loss: 0.00002742
Iteration 109/1000 | Loss: 0.00002741
Iteration 110/1000 | Loss: 0.00002740
Iteration 111/1000 | Loss: 0.00002738
Iteration 112/1000 | Loss: 0.00002738
Iteration 113/1000 | Loss: 0.00002738
Iteration 114/1000 | Loss: 0.00002738
Iteration 115/1000 | Loss: 0.00002738
Iteration 116/1000 | Loss: 0.00002737
Iteration 117/1000 | Loss: 0.00002737
Iteration 118/1000 | Loss: 0.00002737
Iteration 119/1000 | Loss: 0.00002728
Iteration 120/1000 | Loss: 0.00002728
Iteration 121/1000 | Loss: 0.00002727
Iteration 122/1000 | Loss: 0.00002727
Iteration 123/1000 | Loss: 0.00002727
Iteration 124/1000 | Loss: 0.00002726
Iteration 125/1000 | Loss: 0.00002723
Iteration 126/1000 | Loss: 0.00002716
Iteration 127/1000 | Loss: 0.00002708
Iteration 128/1000 | Loss: 0.00002708
Iteration 129/1000 | Loss: 0.00002708
Iteration 130/1000 | Loss: 0.00002708
Iteration 131/1000 | Loss: 0.00002707
Iteration 132/1000 | Loss: 0.00002707
Iteration 133/1000 | Loss: 0.00002707
Iteration 134/1000 | Loss: 0.00002707
Iteration 135/1000 | Loss: 0.00002706
Iteration 136/1000 | Loss: 0.00002706
Iteration 137/1000 | Loss: 0.00002704
Iteration 138/1000 | Loss: 0.00002703
Iteration 139/1000 | Loss: 0.00002703
Iteration 140/1000 | Loss: 0.00002702
Iteration 141/1000 | Loss: 0.00002702
Iteration 142/1000 | Loss: 0.00011406
Iteration 143/1000 | Loss: 0.00004953
Iteration 144/1000 | Loss: 0.00011413
Iteration 145/1000 | Loss: 0.00003382
Iteration 146/1000 | Loss: 0.00011211
Iteration 147/1000 | Loss: 0.00007700
Iteration 148/1000 | Loss: 0.00003359
Iteration 149/1000 | Loss: 0.00002799
Iteration 150/1000 | Loss: 0.00015380
Iteration 151/1000 | Loss: 0.00003711
Iteration 152/1000 | Loss: 0.00013414
Iteration 153/1000 | Loss: 0.00011703
Iteration 154/1000 | Loss: 0.00004813
Iteration 155/1000 | Loss: 0.00003207
Iteration 156/1000 | Loss: 0.00011914
Iteration 157/1000 | Loss: 0.00002849
Iteration 158/1000 | Loss: 0.00002744
Iteration 159/1000 | Loss: 0.00010636
Iteration 160/1000 | Loss: 0.00004306
Iteration 161/1000 | Loss: 0.00005935
Iteration 162/1000 | Loss: 0.00007536
Iteration 163/1000 | Loss: 0.00005777
Iteration 164/1000 | Loss: 0.00007195
Iteration 165/1000 | Loss: 0.00005993
Iteration 166/1000 | Loss: 0.00010324
Iteration 167/1000 | Loss: 0.00006029
Iteration 168/1000 | Loss: 0.00005447
Iteration 169/1000 | Loss: 0.00005794
Iteration 170/1000 | Loss: 0.00002760
Iteration 171/1000 | Loss: 0.00010397
Iteration 172/1000 | Loss: 0.00015770
Iteration 173/1000 | Loss: 0.00009709
Iteration 174/1000 | Loss: 0.00002862
Iteration 175/1000 | Loss: 0.00002812
Iteration 176/1000 | Loss: 0.00002785
Iteration 177/1000 | Loss: 0.00002761
Iteration 178/1000 | Loss: 0.00002751
Iteration 179/1000 | Loss: 0.00002745
Iteration 180/1000 | Loss: 0.00002743
Iteration 181/1000 | Loss: 0.00002742
Iteration 182/1000 | Loss: 0.00002741
Iteration 183/1000 | Loss: 0.00002736
Iteration 184/1000 | Loss: 0.00002732
Iteration 185/1000 | Loss: 0.00002730
Iteration 186/1000 | Loss: 0.00002729
Iteration 187/1000 | Loss: 0.00002728
Iteration 188/1000 | Loss: 0.00002728
Iteration 189/1000 | Loss: 0.00002728
Iteration 190/1000 | Loss: 0.00002728
Iteration 191/1000 | Loss: 0.00002728
Iteration 192/1000 | Loss: 0.00002727
Iteration 193/1000 | Loss: 0.00002727
Iteration 194/1000 | Loss: 0.00002726
Iteration 195/1000 | Loss: 0.00002724
Iteration 196/1000 | Loss: 0.00002722
Iteration 197/1000 | Loss: 0.00002716
Iteration 198/1000 | Loss: 0.00002716
Iteration 199/1000 | Loss: 0.00002711
Iteration 200/1000 | Loss: 0.00002708
Iteration 201/1000 | Loss: 0.00002695
Iteration 202/1000 | Loss: 0.00002694
Iteration 203/1000 | Loss: 0.00002693
Iteration 204/1000 | Loss: 0.00002693
Iteration 205/1000 | Loss: 0.00002693
Iteration 206/1000 | Loss: 0.00002693
Iteration 207/1000 | Loss: 0.00002693
Iteration 208/1000 | Loss: 0.00002693
Iteration 209/1000 | Loss: 0.00002692
Iteration 210/1000 | Loss: 0.00002692
Iteration 211/1000 | Loss: 0.00002691
Iteration 212/1000 | Loss: 0.00002689
Iteration 213/1000 | Loss: 0.00002689
Iteration 214/1000 | Loss: 0.00002689
Iteration 215/1000 | Loss: 0.00002689
Iteration 216/1000 | Loss: 0.00002689
Iteration 217/1000 | Loss: 0.00002688
Iteration 218/1000 | Loss: 0.00002688
Iteration 219/1000 | Loss: 0.00002688
Iteration 220/1000 | Loss: 0.00002688
Iteration 221/1000 | Loss: 0.00002688
Iteration 222/1000 | Loss: 0.00002687
Iteration 223/1000 | Loss: 0.00002687
Iteration 224/1000 | Loss: 0.00002687
Iteration 225/1000 | Loss: 0.00002686
Iteration 226/1000 | Loss: 0.00002686
Iteration 227/1000 | Loss: 0.00002686
Iteration 228/1000 | Loss: 0.00002686
Iteration 229/1000 | Loss: 0.00002686
Iteration 230/1000 | Loss: 0.00002686
Iteration 231/1000 | Loss: 0.00002686
Iteration 232/1000 | Loss: 0.00002685
Iteration 233/1000 | Loss: 0.00002685
Iteration 234/1000 | Loss: 0.00002685
Iteration 235/1000 | Loss: 0.00002685
Iteration 236/1000 | Loss: 0.00002685
Iteration 237/1000 | Loss: 0.00002684
Iteration 238/1000 | Loss: 0.00002684
Iteration 239/1000 | Loss: 0.00002684
Iteration 240/1000 | Loss: 0.00002684
Iteration 241/1000 | Loss: 0.00002684
Iteration 242/1000 | Loss: 0.00002683
Iteration 243/1000 | Loss: 0.00002683
Iteration 244/1000 | Loss: 0.00002683
Iteration 245/1000 | Loss: 0.00002683
Iteration 246/1000 | Loss: 0.00002683
Iteration 247/1000 | Loss: 0.00002683
Iteration 248/1000 | Loss: 0.00002683
Iteration 249/1000 | Loss: 0.00002683
Iteration 250/1000 | Loss: 0.00002682
Iteration 251/1000 | Loss: 0.00002682
Iteration 252/1000 | Loss: 0.00002682
Iteration 253/1000 | Loss: 0.00002682
Iteration 254/1000 | Loss: 0.00002682
Iteration 255/1000 | Loss: 0.00002682
Iteration 256/1000 | Loss: 0.00002682
Iteration 257/1000 | Loss: 0.00002682
Iteration 258/1000 | Loss: 0.00002682
Iteration 259/1000 | Loss: 0.00002682
Iteration 260/1000 | Loss: 0.00002682
Iteration 261/1000 | Loss: 0.00002681
Iteration 262/1000 | Loss: 0.00002681
Iteration 263/1000 | Loss: 0.00002681
Iteration 264/1000 | Loss: 0.00002681
Iteration 265/1000 | Loss: 0.00002681
Iteration 266/1000 | Loss: 0.00002680
Iteration 267/1000 | Loss: 0.00002680
Iteration 268/1000 | Loss: 0.00002680
Iteration 269/1000 | Loss: 0.00002680
Iteration 270/1000 | Loss: 0.00002680
Iteration 271/1000 | Loss: 0.00002680
Iteration 272/1000 | Loss: 0.00002680
Iteration 273/1000 | Loss: 0.00002680
Iteration 274/1000 | Loss: 0.00002680
Iteration 275/1000 | Loss: 0.00002680
Iteration 276/1000 | Loss: 0.00002680
Iteration 277/1000 | Loss: 0.00002680
Iteration 278/1000 | Loss: 0.00002680
Iteration 279/1000 | Loss: 0.00002679
Iteration 280/1000 | Loss: 0.00002679
Iteration 281/1000 | Loss: 0.00002679
Iteration 282/1000 | Loss: 0.00002679
Iteration 283/1000 | Loss: 0.00002679
Iteration 284/1000 | Loss: 0.00002679
Iteration 285/1000 | Loss: 0.00002679
Iteration 286/1000 | Loss: 0.00002679
Iteration 287/1000 | Loss: 0.00002679
Iteration 288/1000 | Loss: 0.00002679
Iteration 289/1000 | Loss: 0.00002679
Iteration 290/1000 | Loss: 0.00002679
Iteration 291/1000 | Loss: 0.00002679
Iteration 292/1000 | Loss: 0.00002678
Iteration 293/1000 | Loss: 0.00002678
Iteration 294/1000 | Loss: 0.00002678
Iteration 295/1000 | Loss: 0.00002678
Iteration 296/1000 | Loss: 0.00002678
Iteration 297/1000 | Loss: 0.00002678
Iteration 298/1000 | Loss: 0.00002678
Iteration 299/1000 | Loss: 0.00002678
Iteration 300/1000 | Loss: 0.00002678
Iteration 301/1000 | Loss: 0.00002678
Iteration 302/1000 | Loss: 0.00002678
Iteration 303/1000 | Loss: 0.00002677
Iteration 304/1000 | Loss: 0.00002677
Iteration 305/1000 | Loss: 0.00002677
Iteration 306/1000 | Loss: 0.00002677
Iteration 307/1000 | Loss: 0.00002677
Iteration 308/1000 | Loss: 0.00002677
Iteration 309/1000 | Loss: 0.00002677
Iteration 310/1000 | Loss: 0.00002677
Iteration 311/1000 | Loss: 0.00002677
Iteration 312/1000 | Loss: 0.00002677
Iteration 313/1000 | Loss: 0.00002677
Iteration 314/1000 | Loss: 0.00002677
Iteration 315/1000 | Loss: 0.00002677
Iteration 316/1000 | Loss: 0.00002677
Iteration 317/1000 | Loss: 0.00002677
Iteration 318/1000 | Loss: 0.00002677
Iteration 319/1000 | Loss: 0.00002677
Iteration 320/1000 | Loss: 0.00002677
Iteration 321/1000 | Loss: 0.00002677
Iteration 322/1000 | Loss: 0.00002677
Iteration 323/1000 | Loss: 0.00002677
Iteration 324/1000 | Loss: 0.00002677
Iteration 325/1000 | Loss: 0.00002677
Iteration 326/1000 | Loss: 0.00002677
Iteration 327/1000 | Loss: 0.00002677
Iteration 328/1000 | Loss: 0.00002677
Iteration 329/1000 | Loss: 0.00002677
Iteration 330/1000 | Loss: 0.00002677
Iteration 331/1000 | Loss: 0.00002677
Iteration 332/1000 | Loss: 0.00002677
Iteration 333/1000 | Loss: 0.00002677
Iteration 334/1000 | Loss: 0.00002677
Iteration 335/1000 | Loss: 0.00002677
Iteration 336/1000 | Loss: 0.00002677
Iteration 337/1000 | Loss: 0.00002677
Iteration 338/1000 | Loss: 0.00002677
Iteration 339/1000 | Loss: 0.00002677
Iteration 340/1000 | Loss: 0.00002677
Iteration 341/1000 | Loss: 0.00002677
Iteration 342/1000 | Loss: 0.00002677
Iteration 343/1000 | Loss: 0.00002677
Iteration 344/1000 | Loss: 0.00002677
Iteration 345/1000 | Loss: 0.00002677
Iteration 346/1000 | Loss: 0.00002677
Iteration 347/1000 | Loss: 0.00002677
Iteration 348/1000 | Loss: 0.00002677
Iteration 349/1000 | Loss: 0.00002677
Iteration 350/1000 | Loss: 0.00002677
Iteration 351/1000 | Loss: 0.00002677
Iteration 352/1000 | Loss: 0.00002677
Iteration 353/1000 | Loss: 0.00002677
Iteration 354/1000 | Loss: 0.00002677
Iteration 355/1000 | Loss: 0.00002677
Iteration 356/1000 | Loss: 0.00002677
Iteration 357/1000 | Loss: 0.00002677
Iteration 358/1000 | Loss: 0.00002677
Iteration 359/1000 | Loss: 0.00002677
Iteration 360/1000 | Loss: 0.00002677
Iteration 361/1000 | Loss: 0.00002677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 361. Stopping optimization.
Last 5 losses: [2.677243719517719e-05, 2.677243719517719e-05, 2.677243719517719e-05, 2.677243719517719e-05, 2.677243719517719e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.677243719517719e-05

Optimization complete. Final v2v error: 4.35027551651001 mm

Highest mean error: 9.715337753295898 mm for frame 90

Lowest mean error: 3.841003656387329 mm for frame 128

Saving results

Total time: 255.1657600402832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00673035
Iteration 2/25 | Loss: 0.00124672
Iteration 3/25 | Loss: 0.00102465
Iteration 4/25 | Loss: 0.00099964
Iteration 5/25 | Loss: 0.00099631
Iteration 6/25 | Loss: 0.00099549
Iteration 7/25 | Loss: 0.00099549
Iteration 8/25 | Loss: 0.00099549
Iteration 9/25 | Loss: 0.00099549
Iteration 10/25 | Loss: 0.00099549
Iteration 11/25 | Loss: 0.00099549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009954894194379449, 0.0009954894194379449, 0.0009954894194379449, 0.0009954894194379449, 0.0009954894194379449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009954894194379449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.90479374
Iteration 2/25 | Loss: 0.00059881
Iteration 3/25 | Loss: 0.00059868
Iteration 4/25 | Loss: 0.00059868
Iteration 5/25 | Loss: 0.00059868
Iteration 6/25 | Loss: 0.00059868
Iteration 7/25 | Loss: 0.00059868
Iteration 8/25 | Loss: 0.00059868
Iteration 9/25 | Loss: 0.00059868
Iteration 10/25 | Loss: 0.00059868
Iteration 11/25 | Loss: 0.00059868
Iteration 12/25 | Loss: 0.00059868
Iteration 13/25 | Loss: 0.00059868
Iteration 14/25 | Loss: 0.00059868
Iteration 15/25 | Loss: 0.00059868
Iteration 16/25 | Loss: 0.00059868
Iteration 17/25 | Loss: 0.00059868
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000598678772803396, 0.000598678772803396, 0.000598678772803396, 0.000598678772803396, 0.000598678772803396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000598678772803396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059868
Iteration 2/1000 | Loss: 0.00005573
Iteration 3/1000 | Loss: 0.00003445
Iteration 4/1000 | Loss: 0.00002677
Iteration 5/1000 | Loss: 0.00002306
Iteration 6/1000 | Loss: 0.00002188
Iteration 7/1000 | Loss: 0.00002127
Iteration 8/1000 | Loss: 0.00002078
Iteration 9/1000 | Loss: 0.00002040
Iteration 10/1000 | Loss: 0.00002006
Iteration 11/1000 | Loss: 0.00001982
Iteration 12/1000 | Loss: 0.00001964
Iteration 13/1000 | Loss: 0.00001958
Iteration 14/1000 | Loss: 0.00001957
Iteration 15/1000 | Loss: 0.00001954
Iteration 16/1000 | Loss: 0.00001954
Iteration 17/1000 | Loss: 0.00001953
Iteration 18/1000 | Loss: 0.00001952
Iteration 19/1000 | Loss: 0.00001951
Iteration 20/1000 | Loss: 0.00001946
Iteration 21/1000 | Loss: 0.00001941
Iteration 22/1000 | Loss: 0.00001940
Iteration 23/1000 | Loss: 0.00001933
Iteration 24/1000 | Loss: 0.00001929
Iteration 25/1000 | Loss: 0.00001927
Iteration 26/1000 | Loss: 0.00001927
Iteration 27/1000 | Loss: 0.00001926
Iteration 28/1000 | Loss: 0.00001926
Iteration 29/1000 | Loss: 0.00001924
Iteration 30/1000 | Loss: 0.00001923
Iteration 31/1000 | Loss: 0.00001923
Iteration 32/1000 | Loss: 0.00001923
Iteration 33/1000 | Loss: 0.00001923
Iteration 34/1000 | Loss: 0.00001923
Iteration 35/1000 | Loss: 0.00001923
Iteration 36/1000 | Loss: 0.00001923
Iteration 37/1000 | Loss: 0.00001922
Iteration 38/1000 | Loss: 0.00001922
Iteration 39/1000 | Loss: 0.00001922
Iteration 40/1000 | Loss: 0.00001922
Iteration 41/1000 | Loss: 0.00001921
Iteration 42/1000 | Loss: 0.00001920
Iteration 43/1000 | Loss: 0.00001919
Iteration 44/1000 | Loss: 0.00001919
Iteration 45/1000 | Loss: 0.00001918
Iteration 46/1000 | Loss: 0.00001918
Iteration 47/1000 | Loss: 0.00001918
Iteration 48/1000 | Loss: 0.00001917
Iteration 49/1000 | Loss: 0.00001917
Iteration 50/1000 | Loss: 0.00001916
Iteration 51/1000 | Loss: 0.00001915
Iteration 52/1000 | Loss: 0.00001915
Iteration 53/1000 | Loss: 0.00001915
Iteration 54/1000 | Loss: 0.00001914
Iteration 55/1000 | Loss: 0.00001914
Iteration 56/1000 | Loss: 0.00001914
Iteration 57/1000 | Loss: 0.00001913
Iteration 58/1000 | Loss: 0.00001912
Iteration 59/1000 | Loss: 0.00001912
Iteration 60/1000 | Loss: 0.00001912
Iteration 61/1000 | Loss: 0.00001912
Iteration 62/1000 | Loss: 0.00001912
Iteration 63/1000 | Loss: 0.00001912
Iteration 64/1000 | Loss: 0.00001912
Iteration 65/1000 | Loss: 0.00001912
Iteration 66/1000 | Loss: 0.00001912
Iteration 67/1000 | Loss: 0.00001911
Iteration 68/1000 | Loss: 0.00001911
Iteration 69/1000 | Loss: 0.00001911
Iteration 70/1000 | Loss: 0.00001911
Iteration 71/1000 | Loss: 0.00001911
Iteration 72/1000 | Loss: 0.00001911
Iteration 73/1000 | Loss: 0.00001910
Iteration 74/1000 | Loss: 0.00001910
Iteration 75/1000 | Loss: 0.00001909
Iteration 76/1000 | Loss: 0.00001909
Iteration 77/1000 | Loss: 0.00001909
Iteration 78/1000 | Loss: 0.00001909
Iteration 79/1000 | Loss: 0.00001908
Iteration 80/1000 | Loss: 0.00001908
Iteration 81/1000 | Loss: 0.00001908
Iteration 82/1000 | Loss: 0.00001908
Iteration 83/1000 | Loss: 0.00001908
Iteration 84/1000 | Loss: 0.00001908
Iteration 85/1000 | Loss: 0.00001908
Iteration 86/1000 | Loss: 0.00001907
Iteration 87/1000 | Loss: 0.00001907
Iteration 88/1000 | Loss: 0.00001907
Iteration 89/1000 | Loss: 0.00001907
Iteration 90/1000 | Loss: 0.00001907
Iteration 91/1000 | Loss: 0.00001907
Iteration 92/1000 | Loss: 0.00001907
Iteration 93/1000 | Loss: 0.00001907
Iteration 94/1000 | Loss: 0.00001906
Iteration 95/1000 | Loss: 0.00001906
Iteration 96/1000 | Loss: 0.00001906
Iteration 97/1000 | Loss: 0.00001906
Iteration 98/1000 | Loss: 0.00001906
Iteration 99/1000 | Loss: 0.00001905
Iteration 100/1000 | Loss: 0.00001905
Iteration 101/1000 | Loss: 0.00001905
Iteration 102/1000 | Loss: 0.00001905
Iteration 103/1000 | Loss: 0.00001904
Iteration 104/1000 | Loss: 0.00001903
Iteration 105/1000 | Loss: 0.00001903
Iteration 106/1000 | Loss: 0.00001903
Iteration 107/1000 | Loss: 0.00001903
Iteration 108/1000 | Loss: 0.00001903
Iteration 109/1000 | Loss: 0.00001902
Iteration 110/1000 | Loss: 0.00001902
Iteration 111/1000 | Loss: 0.00001902
Iteration 112/1000 | Loss: 0.00001901
Iteration 113/1000 | Loss: 0.00001901
Iteration 114/1000 | Loss: 0.00001900
Iteration 115/1000 | Loss: 0.00001899
Iteration 116/1000 | Loss: 0.00001899
Iteration 117/1000 | Loss: 0.00001899
Iteration 118/1000 | Loss: 0.00001899
Iteration 119/1000 | Loss: 0.00001899
Iteration 120/1000 | Loss: 0.00001899
Iteration 121/1000 | Loss: 0.00001899
Iteration 122/1000 | Loss: 0.00001899
Iteration 123/1000 | Loss: 0.00001899
Iteration 124/1000 | Loss: 0.00001899
Iteration 125/1000 | Loss: 0.00001899
Iteration 126/1000 | Loss: 0.00001899
Iteration 127/1000 | Loss: 0.00001899
Iteration 128/1000 | Loss: 0.00001899
Iteration 129/1000 | Loss: 0.00001899
Iteration 130/1000 | Loss: 0.00001899
Iteration 131/1000 | Loss: 0.00001899
Iteration 132/1000 | Loss: 0.00001899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.8986145732924342e-05, 1.8986145732924342e-05, 1.8986145732924342e-05, 1.8986145732924342e-05, 1.8986145732924342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8986145732924342e-05

Optimization complete. Final v2v error: 3.622695207595825 mm

Highest mean error: 4.380743980407715 mm for frame 128

Lowest mean error: 2.7259521484375 mm for frame 19

Saving results

Total time: 37.603883028030396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876843
Iteration 2/25 | Loss: 0.00125357
Iteration 3/25 | Loss: 0.00096978
Iteration 4/25 | Loss: 0.00092320
Iteration 5/25 | Loss: 0.00091708
Iteration 6/25 | Loss: 0.00091534
Iteration 7/25 | Loss: 0.00091534
Iteration 8/25 | Loss: 0.00091534
Iteration 9/25 | Loss: 0.00091534
Iteration 10/25 | Loss: 0.00091534
Iteration 11/25 | Loss: 0.00091534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009153417777270079, 0.0009153417777270079, 0.0009153417777270079, 0.0009153417777270079, 0.0009153417777270079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009153417777270079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30969429
Iteration 2/25 | Loss: 0.00079186
Iteration 3/25 | Loss: 0.00079186
Iteration 4/25 | Loss: 0.00079186
Iteration 5/25 | Loss: 0.00079186
Iteration 6/25 | Loss: 0.00079186
Iteration 7/25 | Loss: 0.00079186
Iteration 8/25 | Loss: 0.00079186
Iteration 9/25 | Loss: 0.00079186
Iteration 10/25 | Loss: 0.00079186
Iteration 11/25 | Loss: 0.00079186
Iteration 12/25 | Loss: 0.00079186
Iteration 13/25 | Loss: 0.00079186
Iteration 14/25 | Loss: 0.00079186
Iteration 15/25 | Loss: 0.00079186
Iteration 16/25 | Loss: 0.00079185
Iteration 17/25 | Loss: 0.00079185
Iteration 18/25 | Loss: 0.00079185
Iteration 19/25 | Loss: 0.00079185
Iteration 20/25 | Loss: 0.00079185
Iteration 21/25 | Loss: 0.00079185
Iteration 22/25 | Loss: 0.00079185
Iteration 23/25 | Loss: 0.00079185
Iteration 24/25 | Loss: 0.00079185
Iteration 25/25 | Loss: 0.00079185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079185
Iteration 2/1000 | Loss: 0.00003774
Iteration 3/1000 | Loss: 0.00002443
Iteration 4/1000 | Loss: 0.00001807
Iteration 5/1000 | Loss: 0.00001519
Iteration 6/1000 | Loss: 0.00001439
Iteration 7/1000 | Loss: 0.00001338
Iteration 8/1000 | Loss: 0.00001296
Iteration 9/1000 | Loss: 0.00001266
Iteration 10/1000 | Loss: 0.00001265
Iteration 11/1000 | Loss: 0.00001241
Iteration 12/1000 | Loss: 0.00001220
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001197
Iteration 15/1000 | Loss: 0.00001196
Iteration 16/1000 | Loss: 0.00001189
Iteration 17/1000 | Loss: 0.00001189
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001188
Iteration 20/1000 | Loss: 0.00001186
Iteration 21/1000 | Loss: 0.00001186
Iteration 22/1000 | Loss: 0.00001185
Iteration 23/1000 | Loss: 0.00001185
Iteration 24/1000 | Loss: 0.00001185
Iteration 25/1000 | Loss: 0.00001184
Iteration 26/1000 | Loss: 0.00001181
Iteration 27/1000 | Loss: 0.00001178
Iteration 28/1000 | Loss: 0.00001178
Iteration 29/1000 | Loss: 0.00001178
Iteration 30/1000 | Loss: 0.00001176
Iteration 31/1000 | Loss: 0.00001175
Iteration 32/1000 | Loss: 0.00001175
Iteration 33/1000 | Loss: 0.00001173
Iteration 34/1000 | Loss: 0.00001173
Iteration 35/1000 | Loss: 0.00001173
Iteration 36/1000 | Loss: 0.00001172
Iteration 37/1000 | Loss: 0.00001172
Iteration 38/1000 | Loss: 0.00001172
Iteration 39/1000 | Loss: 0.00001171
Iteration 40/1000 | Loss: 0.00001170
Iteration 41/1000 | Loss: 0.00001168
Iteration 42/1000 | Loss: 0.00001168
Iteration 43/1000 | Loss: 0.00001168
Iteration 44/1000 | Loss: 0.00001167
Iteration 45/1000 | Loss: 0.00001167
Iteration 46/1000 | Loss: 0.00001162
Iteration 47/1000 | Loss: 0.00001161
Iteration 48/1000 | Loss: 0.00001160
Iteration 49/1000 | Loss: 0.00001159
Iteration 50/1000 | Loss: 0.00001159
Iteration 51/1000 | Loss: 0.00001155
Iteration 52/1000 | Loss: 0.00001155
Iteration 53/1000 | Loss: 0.00001155
Iteration 54/1000 | Loss: 0.00001155
Iteration 55/1000 | Loss: 0.00001154
Iteration 56/1000 | Loss: 0.00001154
Iteration 57/1000 | Loss: 0.00001152
Iteration 58/1000 | Loss: 0.00001152
Iteration 59/1000 | Loss: 0.00001151
Iteration 60/1000 | Loss: 0.00001150
Iteration 61/1000 | Loss: 0.00001149
Iteration 62/1000 | Loss: 0.00001149
Iteration 63/1000 | Loss: 0.00001148
Iteration 64/1000 | Loss: 0.00001148
Iteration 65/1000 | Loss: 0.00001148
Iteration 66/1000 | Loss: 0.00001147
Iteration 67/1000 | Loss: 0.00001147
Iteration 68/1000 | Loss: 0.00001146
Iteration 69/1000 | Loss: 0.00001145
Iteration 70/1000 | Loss: 0.00001145
Iteration 71/1000 | Loss: 0.00001144
Iteration 72/1000 | Loss: 0.00001143
Iteration 73/1000 | Loss: 0.00001143
Iteration 74/1000 | Loss: 0.00001142
Iteration 75/1000 | Loss: 0.00001141
Iteration 76/1000 | Loss: 0.00001141
Iteration 77/1000 | Loss: 0.00001140
Iteration 78/1000 | Loss: 0.00001140
Iteration 79/1000 | Loss: 0.00001140
Iteration 80/1000 | Loss: 0.00001140
Iteration 81/1000 | Loss: 0.00001139
Iteration 82/1000 | Loss: 0.00001139
Iteration 83/1000 | Loss: 0.00001139
Iteration 84/1000 | Loss: 0.00001139
Iteration 85/1000 | Loss: 0.00001139
Iteration 86/1000 | Loss: 0.00001139
Iteration 87/1000 | Loss: 0.00001139
Iteration 88/1000 | Loss: 0.00001138
Iteration 89/1000 | Loss: 0.00001138
Iteration 90/1000 | Loss: 0.00001137
Iteration 91/1000 | Loss: 0.00001137
Iteration 92/1000 | Loss: 0.00001137
Iteration 93/1000 | Loss: 0.00001137
Iteration 94/1000 | Loss: 0.00001136
Iteration 95/1000 | Loss: 0.00001136
Iteration 96/1000 | Loss: 0.00001135
Iteration 97/1000 | Loss: 0.00001135
Iteration 98/1000 | Loss: 0.00001134
Iteration 99/1000 | Loss: 0.00001134
Iteration 100/1000 | Loss: 0.00001134
Iteration 101/1000 | Loss: 0.00001134
Iteration 102/1000 | Loss: 0.00001134
Iteration 103/1000 | Loss: 0.00001134
Iteration 104/1000 | Loss: 0.00001134
Iteration 105/1000 | Loss: 0.00001133
Iteration 106/1000 | Loss: 0.00001133
Iteration 107/1000 | Loss: 0.00001133
Iteration 108/1000 | Loss: 0.00001133
Iteration 109/1000 | Loss: 0.00001133
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001132
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001131
Iteration 117/1000 | Loss: 0.00001131
Iteration 118/1000 | Loss: 0.00001131
Iteration 119/1000 | Loss: 0.00001131
Iteration 120/1000 | Loss: 0.00001131
Iteration 121/1000 | Loss: 0.00001130
Iteration 122/1000 | Loss: 0.00001130
Iteration 123/1000 | Loss: 0.00001130
Iteration 124/1000 | Loss: 0.00001130
Iteration 125/1000 | Loss: 0.00001130
Iteration 126/1000 | Loss: 0.00001130
Iteration 127/1000 | Loss: 0.00001130
Iteration 128/1000 | Loss: 0.00001130
Iteration 129/1000 | Loss: 0.00001130
Iteration 130/1000 | Loss: 0.00001130
Iteration 131/1000 | Loss: 0.00001130
Iteration 132/1000 | Loss: 0.00001130
Iteration 133/1000 | Loss: 0.00001129
Iteration 134/1000 | Loss: 0.00001129
Iteration 135/1000 | Loss: 0.00001129
Iteration 136/1000 | Loss: 0.00001129
Iteration 137/1000 | Loss: 0.00001128
Iteration 138/1000 | Loss: 0.00001128
Iteration 139/1000 | Loss: 0.00001128
Iteration 140/1000 | Loss: 0.00001128
Iteration 141/1000 | Loss: 0.00001128
Iteration 142/1000 | Loss: 0.00001128
Iteration 143/1000 | Loss: 0.00001128
Iteration 144/1000 | Loss: 0.00001128
Iteration 145/1000 | Loss: 0.00001128
Iteration 146/1000 | Loss: 0.00001128
Iteration 147/1000 | Loss: 0.00001128
Iteration 148/1000 | Loss: 0.00001128
Iteration 149/1000 | Loss: 0.00001128
Iteration 150/1000 | Loss: 0.00001128
Iteration 151/1000 | Loss: 0.00001128
Iteration 152/1000 | Loss: 0.00001128
Iteration 153/1000 | Loss: 0.00001128
Iteration 154/1000 | Loss: 0.00001128
Iteration 155/1000 | Loss: 0.00001128
Iteration 156/1000 | Loss: 0.00001128
Iteration 157/1000 | Loss: 0.00001128
Iteration 158/1000 | Loss: 0.00001128
Iteration 159/1000 | Loss: 0.00001128
Iteration 160/1000 | Loss: 0.00001128
Iteration 161/1000 | Loss: 0.00001128
Iteration 162/1000 | Loss: 0.00001128
Iteration 163/1000 | Loss: 0.00001128
Iteration 164/1000 | Loss: 0.00001128
Iteration 165/1000 | Loss: 0.00001128
Iteration 166/1000 | Loss: 0.00001128
Iteration 167/1000 | Loss: 0.00001128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.1276231816736981e-05, 1.1276231816736981e-05, 1.1276231816736981e-05, 1.1276231816736981e-05, 1.1276231816736981e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1276231816736981e-05

Optimization complete. Final v2v error: 2.8376410007476807 mm

Highest mean error: 3.3236653804779053 mm for frame 164

Lowest mean error: 2.398341417312622 mm for frame 60

Saving results

Total time: 46.480079650878906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513231
Iteration 2/25 | Loss: 0.00115391
Iteration 3/25 | Loss: 0.00094835
Iteration 4/25 | Loss: 0.00093362
Iteration 5/25 | Loss: 0.00093092
Iteration 6/25 | Loss: 0.00093056
Iteration 7/25 | Loss: 0.00093056
Iteration 8/25 | Loss: 0.00093056
Iteration 9/25 | Loss: 0.00093056
Iteration 10/25 | Loss: 0.00093056
Iteration 11/25 | Loss: 0.00093056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009305600542575121, 0.0009305600542575121, 0.0009305600542575121, 0.0009305600542575121, 0.0009305600542575121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009305600542575121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32761610
Iteration 2/25 | Loss: 0.00072085
Iteration 3/25 | Loss: 0.00072084
Iteration 4/25 | Loss: 0.00072084
Iteration 5/25 | Loss: 0.00072084
Iteration 6/25 | Loss: 0.00072084
Iteration 7/25 | Loss: 0.00072084
Iteration 8/25 | Loss: 0.00072084
Iteration 9/25 | Loss: 0.00072084
Iteration 10/25 | Loss: 0.00072084
Iteration 11/25 | Loss: 0.00072084
Iteration 12/25 | Loss: 0.00072084
Iteration 13/25 | Loss: 0.00072084
Iteration 14/25 | Loss: 0.00072084
Iteration 15/25 | Loss: 0.00072084
Iteration 16/25 | Loss: 0.00072084
Iteration 17/25 | Loss: 0.00072084
Iteration 18/25 | Loss: 0.00072084
Iteration 19/25 | Loss: 0.00072084
Iteration 20/25 | Loss: 0.00072084
Iteration 21/25 | Loss: 0.00072084
Iteration 22/25 | Loss: 0.00072084
Iteration 23/25 | Loss: 0.00072084
Iteration 24/25 | Loss: 0.00072084
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007208353490568697, 0.0007208353490568697, 0.0007208353490568697, 0.0007208353490568697, 0.0007208353490568697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007208353490568697

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072084
Iteration 2/1000 | Loss: 0.00003287
Iteration 3/1000 | Loss: 0.00002381
Iteration 4/1000 | Loss: 0.00002130
Iteration 5/1000 | Loss: 0.00002031
Iteration 6/1000 | Loss: 0.00001968
Iteration 7/1000 | Loss: 0.00001924
Iteration 8/1000 | Loss: 0.00001894
Iteration 9/1000 | Loss: 0.00001873
Iteration 10/1000 | Loss: 0.00001854
Iteration 11/1000 | Loss: 0.00001852
Iteration 12/1000 | Loss: 0.00001838
Iteration 13/1000 | Loss: 0.00001834
Iteration 14/1000 | Loss: 0.00001833
Iteration 15/1000 | Loss: 0.00001829
Iteration 16/1000 | Loss: 0.00001828
Iteration 17/1000 | Loss: 0.00001826
Iteration 18/1000 | Loss: 0.00001826
Iteration 19/1000 | Loss: 0.00001824
Iteration 20/1000 | Loss: 0.00001823
Iteration 21/1000 | Loss: 0.00001823
Iteration 22/1000 | Loss: 0.00001823
Iteration 23/1000 | Loss: 0.00001822
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001822
Iteration 26/1000 | Loss: 0.00001822
Iteration 27/1000 | Loss: 0.00001821
Iteration 28/1000 | Loss: 0.00001821
Iteration 29/1000 | Loss: 0.00001821
Iteration 30/1000 | Loss: 0.00001820
Iteration 31/1000 | Loss: 0.00001820
Iteration 32/1000 | Loss: 0.00001819
Iteration 33/1000 | Loss: 0.00001819
Iteration 34/1000 | Loss: 0.00001818
Iteration 35/1000 | Loss: 0.00001816
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001811
Iteration 38/1000 | Loss: 0.00001808
Iteration 39/1000 | Loss: 0.00001805
Iteration 40/1000 | Loss: 0.00001804
Iteration 41/1000 | Loss: 0.00001803
Iteration 42/1000 | Loss: 0.00001802
Iteration 43/1000 | Loss: 0.00001799
Iteration 44/1000 | Loss: 0.00001798
Iteration 45/1000 | Loss: 0.00001797
Iteration 46/1000 | Loss: 0.00001796
Iteration 47/1000 | Loss: 0.00001796
Iteration 48/1000 | Loss: 0.00001796
Iteration 49/1000 | Loss: 0.00001795
Iteration 50/1000 | Loss: 0.00001794
Iteration 51/1000 | Loss: 0.00001793
Iteration 52/1000 | Loss: 0.00001792
Iteration 53/1000 | Loss: 0.00001791
Iteration 54/1000 | Loss: 0.00001791
Iteration 55/1000 | Loss: 0.00001791
Iteration 56/1000 | Loss: 0.00001790
Iteration 57/1000 | Loss: 0.00001790
Iteration 58/1000 | Loss: 0.00001788
Iteration 59/1000 | Loss: 0.00001787
Iteration 60/1000 | Loss: 0.00001787
Iteration 61/1000 | Loss: 0.00001785
Iteration 62/1000 | Loss: 0.00001784
Iteration 63/1000 | Loss: 0.00001784
Iteration 64/1000 | Loss: 0.00001784
Iteration 65/1000 | Loss: 0.00001784
Iteration 66/1000 | Loss: 0.00001783
Iteration 67/1000 | Loss: 0.00001783
Iteration 68/1000 | Loss: 0.00001783
Iteration 69/1000 | Loss: 0.00001783
Iteration 70/1000 | Loss: 0.00001783
Iteration 71/1000 | Loss: 0.00001783
Iteration 72/1000 | Loss: 0.00001783
Iteration 73/1000 | Loss: 0.00001783
Iteration 74/1000 | Loss: 0.00001783
Iteration 75/1000 | Loss: 0.00001783
Iteration 76/1000 | Loss: 0.00001782
Iteration 77/1000 | Loss: 0.00001782
Iteration 78/1000 | Loss: 0.00001782
Iteration 79/1000 | Loss: 0.00001782
Iteration 80/1000 | Loss: 0.00001782
Iteration 81/1000 | Loss: 0.00001782
Iteration 82/1000 | Loss: 0.00001780
Iteration 83/1000 | Loss: 0.00001780
Iteration 84/1000 | Loss: 0.00001780
Iteration 85/1000 | Loss: 0.00001780
Iteration 86/1000 | Loss: 0.00001780
Iteration 87/1000 | Loss: 0.00001780
Iteration 88/1000 | Loss: 0.00001780
Iteration 89/1000 | Loss: 0.00001780
Iteration 90/1000 | Loss: 0.00001779
Iteration 91/1000 | Loss: 0.00001779
Iteration 92/1000 | Loss: 0.00001779
Iteration 93/1000 | Loss: 0.00001779
Iteration 94/1000 | Loss: 0.00001779
Iteration 95/1000 | Loss: 0.00001779
Iteration 96/1000 | Loss: 0.00001779
Iteration 97/1000 | Loss: 0.00001779
Iteration 98/1000 | Loss: 0.00001777
Iteration 99/1000 | Loss: 0.00001777
Iteration 100/1000 | Loss: 0.00001777
Iteration 101/1000 | Loss: 0.00001777
Iteration 102/1000 | Loss: 0.00001777
Iteration 103/1000 | Loss: 0.00001777
Iteration 104/1000 | Loss: 0.00001776
Iteration 105/1000 | Loss: 0.00001776
Iteration 106/1000 | Loss: 0.00001776
Iteration 107/1000 | Loss: 0.00001776
Iteration 108/1000 | Loss: 0.00001776
Iteration 109/1000 | Loss: 0.00001776
Iteration 110/1000 | Loss: 0.00001776
Iteration 111/1000 | Loss: 0.00001776
Iteration 112/1000 | Loss: 0.00001776
Iteration 113/1000 | Loss: 0.00001776
Iteration 114/1000 | Loss: 0.00001775
Iteration 115/1000 | Loss: 0.00001775
Iteration 116/1000 | Loss: 0.00001774
Iteration 117/1000 | Loss: 0.00001774
Iteration 118/1000 | Loss: 0.00001773
Iteration 119/1000 | Loss: 0.00001772
Iteration 120/1000 | Loss: 0.00001772
Iteration 121/1000 | Loss: 0.00001772
Iteration 122/1000 | Loss: 0.00001772
Iteration 123/1000 | Loss: 0.00001772
Iteration 124/1000 | Loss: 0.00001771
Iteration 125/1000 | Loss: 0.00001771
Iteration 126/1000 | Loss: 0.00001770
Iteration 127/1000 | Loss: 0.00001770
Iteration 128/1000 | Loss: 0.00001770
Iteration 129/1000 | Loss: 0.00001770
Iteration 130/1000 | Loss: 0.00001770
Iteration 131/1000 | Loss: 0.00001770
Iteration 132/1000 | Loss: 0.00001769
Iteration 133/1000 | Loss: 0.00001769
Iteration 134/1000 | Loss: 0.00001769
Iteration 135/1000 | Loss: 0.00001768
Iteration 136/1000 | Loss: 0.00001768
Iteration 137/1000 | Loss: 0.00001768
Iteration 138/1000 | Loss: 0.00001768
Iteration 139/1000 | Loss: 0.00001767
Iteration 140/1000 | Loss: 0.00001767
Iteration 141/1000 | Loss: 0.00001767
Iteration 142/1000 | Loss: 0.00001767
Iteration 143/1000 | Loss: 0.00001767
Iteration 144/1000 | Loss: 0.00001767
Iteration 145/1000 | Loss: 0.00001766
Iteration 146/1000 | Loss: 0.00001766
Iteration 147/1000 | Loss: 0.00001766
Iteration 148/1000 | Loss: 0.00001766
Iteration 149/1000 | Loss: 0.00001765
Iteration 150/1000 | Loss: 0.00001765
Iteration 151/1000 | Loss: 0.00001765
Iteration 152/1000 | Loss: 0.00001765
Iteration 153/1000 | Loss: 0.00001765
Iteration 154/1000 | Loss: 0.00001765
Iteration 155/1000 | Loss: 0.00001765
Iteration 156/1000 | Loss: 0.00001765
Iteration 157/1000 | Loss: 0.00001764
Iteration 158/1000 | Loss: 0.00001764
Iteration 159/1000 | Loss: 0.00001764
Iteration 160/1000 | Loss: 0.00001764
Iteration 161/1000 | Loss: 0.00001764
Iteration 162/1000 | Loss: 0.00001763
Iteration 163/1000 | Loss: 0.00001763
Iteration 164/1000 | Loss: 0.00001763
Iteration 165/1000 | Loss: 0.00001763
Iteration 166/1000 | Loss: 0.00001763
Iteration 167/1000 | Loss: 0.00001763
Iteration 168/1000 | Loss: 0.00001763
Iteration 169/1000 | Loss: 0.00001763
Iteration 170/1000 | Loss: 0.00001763
Iteration 171/1000 | Loss: 0.00001763
Iteration 172/1000 | Loss: 0.00001763
Iteration 173/1000 | Loss: 0.00001763
Iteration 174/1000 | Loss: 0.00001763
Iteration 175/1000 | Loss: 0.00001762
Iteration 176/1000 | Loss: 0.00001762
Iteration 177/1000 | Loss: 0.00001762
Iteration 178/1000 | Loss: 0.00001762
Iteration 179/1000 | Loss: 0.00001762
Iteration 180/1000 | Loss: 0.00001761
Iteration 181/1000 | Loss: 0.00001761
Iteration 182/1000 | Loss: 0.00001761
Iteration 183/1000 | Loss: 0.00001761
Iteration 184/1000 | Loss: 0.00001760
Iteration 185/1000 | Loss: 0.00001760
Iteration 186/1000 | Loss: 0.00001760
Iteration 187/1000 | Loss: 0.00001760
Iteration 188/1000 | Loss: 0.00001759
Iteration 189/1000 | Loss: 0.00001759
Iteration 190/1000 | Loss: 0.00001759
Iteration 191/1000 | Loss: 0.00001758
Iteration 192/1000 | Loss: 0.00001758
Iteration 193/1000 | Loss: 0.00001758
Iteration 194/1000 | Loss: 0.00001758
Iteration 195/1000 | Loss: 0.00001758
Iteration 196/1000 | Loss: 0.00001758
Iteration 197/1000 | Loss: 0.00001758
Iteration 198/1000 | Loss: 0.00001758
Iteration 199/1000 | Loss: 0.00001757
Iteration 200/1000 | Loss: 0.00001757
Iteration 201/1000 | Loss: 0.00001757
Iteration 202/1000 | Loss: 0.00001757
Iteration 203/1000 | Loss: 0.00001757
Iteration 204/1000 | Loss: 0.00001757
Iteration 205/1000 | Loss: 0.00001757
Iteration 206/1000 | Loss: 0.00001757
Iteration 207/1000 | Loss: 0.00001757
Iteration 208/1000 | Loss: 0.00001757
Iteration 209/1000 | Loss: 0.00001757
Iteration 210/1000 | Loss: 0.00001757
Iteration 211/1000 | Loss: 0.00001757
Iteration 212/1000 | Loss: 0.00001757
Iteration 213/1000 | Loss: 0.00001757
Iteration 214/1000 | Loss: 0.00001756
Iteration 215/1000 | Loss: 0.00001756
Iteration 216/1000 | Loss: 0.00001756
Iteration 217/1000 | Loss: 0.00001756
Iteration 218/1000 | Loss: 0.00001756
Iteration 219/1000 | Loss: 0.00001756
Iteration 220/1000 | Loss: 0.00001756
Iteration 221/1000 | Loss: 0.00001756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.7564518202561885e-05, 1.7564518202561885e-05, 1.7564518202561885e-05, 1.7564518202561885e-05, 1.7564518202561885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7564518202561885e-05

Optimization complete. Final v2v error: 3.3633856773376465 mm

Highest mean error: 3.9944043159484863 mm for frame 197

Lowest mean error: 2.7876064777374268 mm for frame 14

Saving results

Total time: 50.02375769615173
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824707
Iteration 2/25 | Loss: 0.00113657
Iteration 3/25 | Loss: 0.00099431
Iteration 4/25 | Loss: 0.00096939
Iteration 5/25 | Loss: 0.00096046
Iteration 6/25 | Loss: 0.00095831
Iteration 7/25 | Loss: 0.00095780
Iteration 8/25 | Loss: 0.00095780
Iteration 9/25 | Loss: 0.00095780
Iteration 10/25 | Loss: 0.00095780
Iteration 11/25 | Loss: 0.00095780
Iteration 12/25 | Loss: 0.00095780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009578010067343712, 0.0009578010067343712, 0.0009578010067343712, 0.0009578010067343712, 0.0009578010067343712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009578010067343712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.10826397
Iteration 2/25 | Loss: 0.00107663
Iteration 3/25 | Loss: 0.00107663
Iteration 4/25 | Loss: 0.00107663
Iteration 5/25 | Loss: 0.00107663
Iteration 6/25 | Loss: 0.00107663
Iteration 7/25 | Loss: 0.00107663
Iteration 8/25 | Loss: 0.00107663
Iteration 9/25 | Loss: 0.00107663
Iteration 10/25 | Loss: 0.00107663
Iteration 11/25 | Loss: 0.00107663
Iteration 12/25 | Loss: 0.00107663
Iteration 13/25 | Loss: 0.00107663
Iteration 14/25 | Loss: 0.00107663
Iteration 15/25 | Loss: 0.00107663
Iteration 16/25 | Loss: 0.00107663
Iteration 17/25 | Loss: 0.00107663
Iteration 18/25 | Loss: 0.00107663
Iteration 19/25 | Loss: 0.00107663
Iteration 20/25 | Loss: 0.00107663
Iteration 21/25 | Loss: 0.00107663
Iteration 22/25 | Loss: 0.00107663
Iteration 23/25 | Loss: 0.00107663
Iteration 24/25 | Loss: 0.00107663
Iteration 25/25 | Loss: 0.00107663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107663
Iteration 2/1000 | Loss: 0.00005860
Iteration 3/1000 | Loss: 0.00003438
Iteration 4/1000 | Loss: 0.00002475
Iteration 5/1000 | Loss: 0.00002103
Iteration 6/1000 | Loss: 0.00001990
Iteration 7/1000 | Loss: 0.00001908
Iteration 8/1000 | Loss: 0.00001847
Iteration 9/1000 | Loss: 0.00001800
Iteration 10/1000 | Loss: 0.00001763
Iteration 11/1000 | Loss: 0.00001744
Iteration 12/1000 | Loss: 0.00001739
Iteration 13/1000 | Loss: 0.00001737
Iteration 14/1000 | Loss: 0.00001730
Iteration 15/1000 | Loss: 0.00001728
Iteration 16/1000 | Loss: 0.00001725
Iteration 17/1000 | Loss: 0.00001721
Iteration 18/1000 | Loss: 0.00001720
Iteration 19/1000 | Loss: 0.00001720
Iteration 20/1000 | Loss: 0.00001719
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001718
Iteration 23/1000 | Loss: 0.00001714
Iteration 24/1000 | Loss: 0.00001708
Iteration 25/1000 | Loss: 0.00001705
Iteration 26/1000 | Loss: 0.00001703
Iteration 27/1000 | Loss: 0.00001697
Iteration 28/1000 | Loss: 0.00001697
Iteration 29/1000 | Loss: 0.00001690
Iteration 30/1000 | Loss: 0.00001687
Iteration 31/1000 | Loss: 0.00001687
Iteration 32/1000 | Loss: 0.00001686
Iteration 33/1000 | Loss: 0.00001686
Iteration 34/1000 | Loss: 0.00001686
Iteration 35/1000 | Loss: 0.00001685
Iteration 36/1000 | Loss: 0.00001685
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001684
Iteration 39/1000 | Loss: 0.00001684
Iteration 40/1000 | Loss: 0.00001683
Iteration 41/1000 | Loss: 0.00001683
Iteration 42/1000 | Loss: 0.00001683
Iteration 43/1000 | Loss: 0.00001683
Iteration 44/1000 | Loss: 0.00001682
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00001682
Iteration 47/1000 | Loss: 0.00001682
Iteration 48/1000 | Loss: 0.00001682
Iteration 49/1000 | Loss: 0.00001682
Iteration 50/1000 | Loss: 0.00001682
Iteration 51/1000 | Loss: 0.00001682
Iteration 52/1000 | Loss: 0.00001682
Iteration 53/1000 | Loss: 0.00001681
Iteration 54/1000 | Loss: 0.00001681
Iteration 55/1000 | Loss: 0.00001681
Iteration 56/1000 | Loss: 0.00001680
Iteration 57/1000 | Loss: 0.00001680
Iteration 58/1000 | Loss: 0.00001680
Iteration 59/1000 | Loss: 0.00001680
Iteration 60/1000 | Loss: 0.00001680
Iteration 61/1000 | Loss: 0.00001679
Iteration 62/1000 | Loss: 0.00001679
Iteration 63/1000 | Loss: 0.00001679
Iteration 64/1000 | Loss: 0.00001679
Iteration 65/1000 | Loss: 0.00001679
Iteration 66/1000 | Loss: 0.00001678
Iteration 67/1000 | Loss: 0.00001678
Iteration 68/1000 | Loss: 0.00001678
Iteration 69/1000 | Loss: 0.00001677
Iteration 70/1000 | Loss: 0.00001677
Iteration 71/1000 | Loss: 0.00001677
Iteration 72/1000 | Loss: 0.00001677
Iteration 73/1000 | Loss: 0.00001676
Iteration 74/1000 | Loss: 0.00001676
Iteration 75/1000 | Loss: 0.00001676
Iteration 76/1000 | Loss: 0.00001676
Iteration 77/1000 | Loss: 0.00001676
Iteration 78/1000 | Loss: 0.00001676
Iteration 79/1000 | Loss: 0.00001675
Iteration 80/1000 | Loss: 0.00001675
Iteration 81/1000 | Loss: 0.00001675
Iteration 82/1000 | Loss: 0.00001675
Iteration 83/1000 | Loss: 0.00001675
Iteration 84/1000 | Loss: 0.00001674
Iteration 85/1000 | Loss: 0.00001674
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001674
Iteration 88/1000 | Loss: 0.00001674
Iteration 89/1000 | Loss: 0.00001674
Iteration 90/1000 | Loss: 0.00001673
Iteration 91/1000 | Loss: 0.00001673
Iteration 92/1000 | Loss: 0.00001673
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001672
Iteration 95/1000 | Loss: 0.00001672
Iteration 96/1000 | Loss: 0.00001672
Iteration 97/1000 | Loss: 0.00001672
Iteration 98/1000 | Loss: 0.00001672
Iteration 99/1000 | Loss: 0.00001672
Iteration 100/1000 | Loss: 0.00001672
Iteration 101/1000 | Loss: 0.00001672
Iteration 102/1000 | Loss: 0.00001671
Iteration 103/1000 | Loss: 0.00001671
Iteration 104/1000 | Loss: 0.00001671
Iteration 105/1000 | Loss: 0.00001671
Iteration 106/1000 | Loss: 0.00001671
Iteration 107/1000 | Loss: 0.00001671
Iteration 108/1000 | Loss: 0.00001671
Iteration 109/1000 | Loss: 0.00001670
Iteration 110/1000 | Loss: 0.00001670
Iteration 111/1000 | Loss: 0.00001670
Iteration 112/1000 | Loss: 0.00001670
Iteration 113/1000 | Loss: 0.00001670
Iteration 114/1000 | Loss: 0.00001669
Iteration 115/1000 | Loss: 0.00001669
Iteration 116/1000 | Loss: 0.00001669
Iteration 117/1000 | Loss: 0.00001669
Iteration 118/1000 | Loss: 0.00001668
Iteration 119/1000 | Loss: 0.00001668
Iteration 120/1000 | Loss: 0.00001668
Iteration 121/1000 | Loss: 0.00001667
Iteration 122/1000 | Loss: 0.00001667
Iteration 123/1000 | Loss: 0.00001667
Iteration 124/1000 | Loss: 0.00001666
Iteration 125/1000 | Loss: 0.00001666
Iteration 126/1000 | Loss: 0.00001666
Iteration 127/1000 | Loss: 0.00001665
Iteration 128/1000 | Loss: 0.00001665
Iteration 129/1000 | Loss: 0.00001664
Iteration 130/1000 | Loss: 0.00001664
Iteration 131/1000 | Loss: 0.00001664
Iteration 132/1000 | Loss: 0.00001664
Iteration 133/1000 | Loss: 0.00001664
Iteration 134/1000 | Loss: 0.00001664
Iteration 135/1000 | Loss: 0.00001663
Iteration 136/1000 | Loss: 0.00001663
Iteration 137/1000 | Loss: 0.00001663
Iteration 138/1000 | Loss: 0.00001663
Iteration 139/1000 | Loss: 0.00001663
Iteration 140/1000 | Loss: 0.00001663
Iteration 141/1000 | Loss: 0.00001662
Iteration 142/1000 | Loss: 0.00001662
Iteration 143/1000 | Loss: 0.00001662
Iteration 144/1000 | Loss: 0.00001662
Iteration 145/1000 | Loss: 0.00001662
Iteration 146/1000 | Loss: 0.00001661
Iteration 147/1000 | Loss: 0.00001661
Iteration 148/1000 | Loss: 0.00001661
Iteration 149/1000 | Loss: 0.00001660
Iteration 150/1000 | Loss: 0.00001660
Iteration 151/1000 | Loss: 0.00001660
Iteration 152/1000 | Loss: 0.00001660
Iteration 153/1000 | Loss: 0.00001659
Iteration 154/1000 | Loss: 0.00001659
Iteration 155/1000 | Loss: 0.00001659
Iteration 156/1000 | Loss: 0.00001659
Iteration 157/1000 | Loss: 0.00001659
Iteration 158/1000 | Loss: 0.00001659
Iteration 159/1000 | Loss: 0.00001659
Iteration 160/1000 | Loss: 0.00001659
Iteration 161/1000 | Loss: 0.00001658
Iteration 162/1000 | Loss: 0.00001658
Iteration 163/1000 | Loss: 0.00001658
Iteration 164/1000 | Loss: 0.00001658
Iteration 165/1000 | Loss: 0.00001657
Iteration 166/1000 | Loss: 0.00001657
Iteration 167/1000 | Loss: 0.00001657
Iteration 168/1000 | Loss: 0.00001657
Iteration 169/1000 | Loss: 0.00001656
Iteration 170/1000 | Loss: 0.00001656
Iteration 171/1000 | Loss: 0.00001656
Iteration 172/1000 | Loss: 0.00001656
Iteration 173/1000 | Loss: 0.00001656
Iteration 174/1000 | Loss: 0.00001656
Iteration 175/1000 | Loss: 0.00001656
Iteration 176/1000 | Loss: 0.00001655
Iteration 177/1000 | Loss: 0.00001655
Iteration 178/1000 | Loss: 0.00001655
Iteration 179/1000 | Loss: 0.00001655
Iteration 180/1000 | Loss: 0.00001655
Iteration 181/1000 | Loss: 0.00001655
Iteration 182/1000 | Loss: 0.00001655
Iteration 183/1000 | Loss: 0.00001655
Iteration 184/1000 | Loss: 0.00001655
Iteration 185/1000 | Loss: 0.00001655
Iteration 186/1000 | Loss: 0.00001655
Iteration 187/1000 | Loss: 0.00001654
Iteration 188/1000 | Loss: 0.00001654
Iteration 189/1000 | Loss: 0.00001654
Iteration 190/1000 | Loss: 0.00001654
Iteration 191/1000 | Loss: 0.00001654
Iteration 192/1000 | Loss: 0.00001653
Iteration 193/1000 | Loss: 0.00001653
Iteration 194/1000 | Loss: 0.00001653
Iteration 195/1000 | Loss: 0.00001653
Iteration 196/1000 | Loss: 0.00001653
Iteration 197/1000 | Loss: 0.00001653
Iteration 198/1000 | Loss: 0.00001653
Iteration 199/1000 | Loss: 0.00001653
Iteration 200/1000 | Loss: 0.00001653
Iteration 201/1000 | Loss: 0.00001653
Iteration 202/1000 | Loss: 0.00001653
Iteration 203/1000 | Loss: 0.00001653
Iteration 204/1000 | Loss: 0.00001653
Iteration 205/1000 | Loss: 0.00001653
Iteration 206/1000 | Loss: 0.00001652
Iteration 207/1000 | Loss: 0.00001652
Iteration 208/1000 | Loss: 0.00001652
Iteration 209/1000 | Loss: 0.00001652
Iteration 210/1000 | Loss: 0.00001652
Iteration 211/1000 | Loss: 0.00001652
Iteration 212/1000 | Loss: 0.00001652
Iteration 213/1000 | Loss: 0.00001651
Iteration 214/1000 | Loss: 0.00001651
Iteration 215/1000 | Loss: 0.00001651
Iteration 216/1000 | Loss: 0.00001651
Iteration 217/1000 | Loss: 0.00001651
Iteration 218/1000 | Loss: 0.00001651
Iteration 219/1000 | Loss: 0.00001651
Iteration 220/1000 | Loss: 0.00001651
Iteration 221/1000 | Loss: 0.00001651
Iteration 222/1000 | Loss: 0.00001651
Iteration 223/1000 | Loss: 0.00001651
Iteration 224/1000 | Loss: 0.00001651
Iteration 225/1000 | Loss: 0.00001651
Iteration 226/1000 | Loss: 0.00001650
Iteration 227/1000 | Loss: 0.00001650
Iteration 228/1000 | Loss: 0.00001650
Iteration 229/1000 | Loss: 0.00001650
Iteration 230/1000 | Loss: 0.00001650
Iteration 231/1000 | Loss: 0.00001650
Iteration 232/1000 | Loss: 0.00001650
Iteration 233/1000 | Loss: 0.00001650
Iteration 234/1000 | Loss: 0.00001650
Iteration 235/1000 | Loss: 0.00001650
Iteration 236/1000 | Loss: 0.00001650
Iteration 237/1000 | Loss: 0.00001649
Iteration 238/1000 | Loss: 0.00001649
Iteration 239/1000 | Loss: 0.00001649
Iteration 240/1000 | Loss: 0.00001649
Iteration 241/1000 | Loss: 0.00001649
Iteration 242/1000 | Loss: 0.00001649
Iteration 243/1000 | Loss: 0.00001649
Iteration 244/1000 | Loss: 0.00001649
Iteration 245/1000 | Loss: 0.00001649
Iteration 246/1000 | Loss: 0.00001649
Iteration 247/1000 | Loss: 0.00001649
Iteration 248/1000 | Loss: 0.00001649
Iteration 249/1000 | Loss: 0.00001649
Iteration 250/1000 | Loss: 0.00001649
Iteration 251/1000 | Loss: 0.00001648
Iteration 252/1000 | Loss: 0.00001648
Iteration 253/1000 | Loss: 0.00001648
Iteration 254/1000 | Loss: 0.00001648
Iteration 255/1000 | Loss: 0.00001648
Iteration 256/1000 | Loss: 0.00001648
Iteration 257/1000 | Loss: 0.00001648
Iteration 258/1000 | Loss: 0.00001648
Iteration 259/1000 | Loss: 0.00001648
Iteration 260/1000 | Loss: 0.00001648
Iteration 261/1000 | Loss: 0.00001648
Iteration 262/1000 | Loss: 0.00001648
Iteration 263/1000 | Loss: 0.00001648
Iteration 264/1000 | Loss: 0.00001648
Iteration 265/1000 | Loss: 0.00001648
Iteration 266/1000 | Loss: 0.00001648
Iteration 267/1000 | Loss: 0.00001648
Iteration 268/1000 | Loss: 0.00001647
Iteration 269/1000 | Loss: 0.00001647
Iteration 270/1000 | Loss: 0.00001647
Iteration 271/1000 | Loss: 0.00001647
Iteration 272/1000 | Loss: 0.00001647
Iteration 273/1000 | Loss: 0.00001647
Iteration 274/1000 | Loss: 0.00001647
Iteration 275/1000 | Loss: 0.00001647
Iteration 276/1000 | Loss: 0.00001647
Iteration 277/1000 | Loss: 0.00001647
Iteration 278/1000 | Loss: 0.00001647
Iteration 279/1000 | Loss: 0.00001647
Iteration 280/1000 | Loss: 0.00001647
Iteration 281/1000 | Loss: 0.00001646
Iteration 282/1000 | Loss: 0.00001646
Iteration 283/1000 | Loss: 0.00001646
Iteration 284/1000 | Loss: 0.00001646
Iteration 285/1000 | Loss: 0.00001646
Iteration 286/1000 | Loss: 0.00001646
Iteration 287/1000 | Loss: 0.00001646
Iteration 288/1000 | Loss: 0.00001646
Iteration 289/1000 | Loss: 0.00001646
Iteration 290/1000 | Loss: 0.00001646
Iteration 291/1000 | Loss: 0.00001646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [1.646341297600884e-05, 1.646341297600884e-05, 1.646341297600884e-05, 1.646341297600884e-05, 1.646341297600884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.646341297600884e-05

Optimization complete. Final v2v error: 3.410851001739502 mm

Highest mean error: 4.621129512786865 mm for frame 55

Lowest mean error: 2.838027000427246 mm for frame 40

Saving results

Total time: 50.08197498321533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013099
Iteration 2/25 | Loss: 0.01013098
Iteration 3/25 | Loss: 0.01013098
Iteration 4/25 | Loss: 0.01013098
Iteration 5/25 | Loss: 0.01013098
Iteration 6/25 | Loss: 0.01013098
Iteration 7/25 | Loss: 0.01013098
Iteration 8/25 | Loss: 0.01013098
Iteration 9/25 | Loss: 0.01013098
Iteration 10/25 | Loss: 0.01013097
Iteration 11/25 | Loss: 0.01013097
Iteration 12/25 | Loss: 0.01013097
Iteration 13/25 | Loss: 0.01013097
Iteration 14/25 | Loss: 0.01013097
Iteration 15/25 | Loss: 0.01013097
Iteration 16/25 | Loss: 0.01013097
Iteration 17/25 | Loss: 0.01013097
Iteration 18/25 | Loss: 0.01013096
Iteration 19/25 | Loss: 0.01013096
Iteration 20/25 | Loss: 0.01013096
Iteration 21/25 | Loss: 0.01013096
Iteration 22/25 | Loss: 0.01013096
Iteration 23/25 | Loss: 0.01013096
Iteration 24/25 | Loss: 0.01013096
Iteration 25/25 | Loss: 0.01013096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69696045
Iteration 2/25 | Loss: 0.17581837
Iteration 3/25 | Loss: 0.17455658
Iteration 4/25 | Loss: 0.17293821
Iteration 5/25 | Loss: 0.17293818
Iteration 6/25 | Loss: 0.17293817
Iteration 7/25 | Loss: 0.17293817
Iteration 8/25 | Loss: 0.17293815
Iteration 9/25 | Loss: 0.17293814
Iteration 10/25 | Loss: 0.17293814
Iteration 11/25 | Loss: 0.17293814
Iteration 12/25 | Loss: 0.17293814
Iteration 13/25 | Loss: 0.17293814
Iteration 14/25 | Loss: 0.17293814
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.17293813824653625, 0.17293813824653625, 0.17293813824653625, 0.17293813824653625, 0.17293813824653625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17293813824653625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17293814
Iteration 2/1000 | Loss: 0.00804049
Iteration 3/1000 | Loss: 0.00679949
Iteration 4/1000 | Loss: 0.00177749
Iteration 5/1000 | Loss: 0.00072193
Iteration 6/1000 | Loss: 0.00036746
Iteration 7/1000 | Loss: 0.00022286
Iteration 8/1000 | Loss: 0.00016634
Iteration 9/1000 | Loss: 0.00014088
Iteration 10/1000 | Loss: 0.00011928
Iteration 11/1000 | Loss: 0.00010366
Iteration 12/1000 | Loss: 0.00008866
Iteration 13/1000 | Loss: 0.00007811
Iteration 14/1000 | Loss: 0.00006828
Iteration 15/1000 | Loss: 0.00005932
Iteration 16/1000 | Loss: 0.00004989
Iteration 17/1000 | Loss: 0.00004728
Iteration 18/1000 | Loss: 0.00004207
Iteration 19/1000 | Loss: 0.00003906
Iteration 20/1000 | Loss: 0.00003739
Iteration 21/1000 | Loss: 0.00003552
Iteration 22/1000 | Loss: 0.00003409
Iteration 23/1000 | Loss: 0.00003310
Iteration 24/1000 | Loss: 0.00003237
Iteration 25/1000 | Loss: 0.00003172
Iteration 26/1000 | Loss: 0.00003122
Iteration 27/1000 | Loss: 0.00003086
Iteration 28/1000 | Loss: 0.00003094
Iteration 29/1000 | Loss: 0.00003054
Iteration 30/1000 | Loss: 0.00003029
Iteration 31/1000 | Loss: 0.00003018
Iteration 32/1000 | Loss: 0.00003017
Iteration 33/1000 | Loss: 0.00003008
Iteration 34/1000 | Loss: 0.00003004
Iteration 35/1000 | Loss: 0.00002991
Iteration 36/1000 | Loss: 0.00002991
Iteration 37/1000 | Loss: 0.00002987
Iteration 38/1000 | Loss: 0.00002986
Iteration 39/1000 | Loss: 0.00002986
Iteration 40/1000 | Loss: 0.00002981
Iteration 41/1000 | Loss: 0.00002981
Iteration 42/1000 | Loss: 0.00002977
Iteration 43/1000 | Loss: 0.00002977
Iteration 44/1000 | Loss: 0.00002976
Iteration 45/1000 | Loss: 0.00002976
Iteration 46/1000 | Loss: 0.00002976
Iteration 47/1000 | Loss: 0.00002976
Iteration 48/1000 | Loss: 0.00002976
Iteration 49/1000 | Loss: 0.00002976
Iteration 50/1000 | Loss: 0.00002976
Iteration 51/1000 | Loss: 0.00002976
Iteration 52/1000 | Loss: 0.00002976
Iteration 53/1000 | Loss: 0.00002976
Iteration 54/1000 | Loss: 0.00002976
Iteration 55/1000 | Loss: 0.00002975
Iteration 56/1000 | Loss: 0.00002975
Iteration 57/1000 | Loss: 0.00002975
Iteration 58/1000 | Loss: 0.00002974
Iteration 59/1000 | Loss: 0.00002974
Iteration 60/1000 | Loss: 0.00002973
Iteration 61/1000 | Loss: 0.00002973
Iteration 62/1000 | Loss: 0.00002972
Iteration 63/1000 | Loss: 0.00002971
Iteration 64/1000 | Loss: 0.00002970
Iteration 65/1000 | Loss: 0.00002970
Iteration 66/1000 | Loss: 0.00003010
Iteration 67/1000 | Loss: 0.00002972
Iteration 68/1000 | Loss: 0.00002972
Iteration 69/1000 | Loss: 0.00002968
Iteration 70/1000 | Loss: 0.00002967
Iteration 71/1000 | Loss: 0.00002966
Iteration 72/1000 | Loss: 0.00002965
Iteration 73/1000 | Loss: 0.00002964
Iteration 74/1000 | Loss: 0.00002957
Iteration 75/1000 | Loss: 0.00002957
Iteration 76/1000 | Loss: 0.00002957
Iteration 77/1000 | Loss: 0.00002957
Iteration 78/1000 | Loss: 0.00002957
Iteration 79/1000 | Loss: 0.00002957
Iteration 80/1000 | Loss: 0.00002957
Iteration 81/1000 | Loss: 0.00002957
Iteration 82/1000 | Loss: 0.00002957
Iteration 83/1000 | Loss: 0.00002957
Iteration 84/1000 | Loss: 0.00002956
Iteration 85/1000 | Loss: 0.00002956
Iteration 86/1000 | Loss: 0.00002956
Iteration 87/1000 | Loss: 0.00002956
Iteration 88/1000 | Loss: 0.00002956
Iteration 89/1000 | Loss: 0.00002956
Iteration 90/1000 | Loss: 0.00002956
Iteration 91/1000 | Loss: 0.00002956
Iteration 92/1000 | Loss: 0.00002956
Iteration 93/1000 | Loss: 0.00002956
Iteration 94/1000 | Loss: 0.00002955
Iteration 95/1000 | Loss: 0.00002955
Iteration 96/1000 | Loss: 0.00002955
Iteration 97/1000 | Loss: 0.00002954
Iteration 98/1000 | Loss: 0.00002954
Iteration 99/1000 | Loss: 0.00002954
Iteration 100/1000 | Loss: 0.00002953
Iteration 101/1000 | Loss: 0.00002953
Iteration 102/1000 | Loss: 0.00002953
Iteration 103/1000 | Loss: 0.00002953
Iteration 104/1000 | Loss: 0.00002953
Iteration 105/1000 | Loss: 0.00002953
Iteration 106/1000 | Loss: 0.00002953
Iteration 107/1000 | Loss: 0.00002953
Iteration 108/1000 | Loss: 0.00002953
Iteration 109/1000 | Loss: 0.00002953
Iteration 110/1000 | Loss: 0.00002953
Iteration 111/1000 | Loss: 0.00002953
Iteration 112/1000 | Loss: 0.00002953
Iteration 113/1000 | Loss: 0.00002953
Iteration 114/1000 | Loss: 0.00002953
Iteration 115/1000 | Loss: 0.00002953
Iteration 116/1000 | Loss: 0.00002953
Iteration 117/1000 | Loss: 0.00002952
Iteration 118/1000 | Loss: 0.00002952
Iteration 119/1000 | Loss: 0.00002952
Iteration 120/1000 | Loss: 0.00002952
Iteration 121/1000 | Loss: 0.00002952
Iteration 122/1000 | Loss: 0.00002952
Iteration 123/1000 | Loss: 0.00002952
Iteration 124/1000 | Loss: 0.00002952
Iteration 125/1000 | Loss: 0.00002952
Iteration 126/1000 | Loss: 0.00002951
Iteration 127/1000 | Loss: 0.00002951
Iteration 128/1000 | Loss: 0.00002951
Iteration 129/1000 | Loss: 0.00002951
Iteration 130/1000 | Loss: 0.00002951
Iteration 131/1000 | Loss: 0.00002951
Iteration 132/1000 | Loss: 0.00002951
Iteration 133/1000 | Loss: 0.00002951
Iteration 134/1000 | Loss: 0.00002951
Iteration 135/1000 | Loss: 0.00002951
Iteration 136/1000 | Loss: 0.00002951
Iteration 137/1000 | Loss: 0.00002950
Iteration 138/1000 | Loss: 0.00002950
Iteration 139/1000 | Loss: 0.00002950
Iteration 140/1000 | Loss: 0.00002949
Iteration 141/1000 | Loss: 0.00002949
Iteration 142/1000 | Loss: 0.00002949
Iteration 143/1000 | Loss: 0.00002949
Iteration 144/1000 | Loss: 0.00002949
Iteration 145/1000 | Loss: 0.00002949
Iteration 146/1000 | Loss: 0.00002949
Iteration 147/1000 | Loss: 0.00002949
Iteration 148/1000 | Loss: 0.00002949
Iteration 149/1000 | Loss: 0.00002949
Iteration 150/1000 | Loss: 0.00002949
Iteration 151/1000 | Loss: 0.00002948
Iteration 152/1000 | Loss: 0.00002948
Iteration 153/1000 | Loss: 0.00002948
Iteration 154/1000 | Loss: 0.00002948
Iteration 155/1000 | Loss: 0.00002948
Iteration 156/1000 | Loss: 0.00002948
Iteration 157/1000 | Loss: 0.00002948
Iteration 158/1000 | Loss: 0.00002948
Iteration 159/1000 | Loss: 0.00002948
Iteration 160/1000 | Loss: 0.00002948
Iteration 161/1000 | Loss: 0.00002948
Iteration 162/1000 | Loss: 0.00002948
Iteration 163/1000 | Loss: 0.00002947
Iteration 164/1000 | Loss: 0.00002947
Iteration 165/1000 | Loss: 0.00002947
Iteration 166/1000 | Loss: 0.00002947
Iteration 167/1000 | Loss: 0.00002947
Iteration 168/1000 | Loss: 0.00002947
Iteration 169/1000 | Loss: 0.00002947
Iteration 170/1000 | Loss: 0.00002947
Iteration 171/1000 | Loss: 0.00002946
Iteration 172/1000 | Loss: 0.00002946
Iteration 173/1000 | Loss: 0.00002946
Iteration 174/1000 | Loss: 0.00002945
Iteration 175/1000 | Loss: 0.00002945
Iteration 176/1000 | Loss: 0.00002945
Iteration 177/1000 | Loss: 0.00002944
Iteration 178/1000 | Loss: 0.00002944
Iteration 179/1000 | Loss: 0.00002944
Iteration 180/1000 | Loss: 0.00002944
Iteration 181/1000 | Loss: 0.00002944
Iteration 182/1000 | Loss: 0.00002944
Iteration 183/1000 | Loss: 0.00002944
Iteration 184/1000 | Loss: 0.00002943
Iteration 185/1000 | Loss: 0.00002943
Iteration 186/1000 | Loss: 0.00002943
Iteration 187/1000 | Loss: 0.00002943
Iteration 188/1000 | Loss: 0.00002943
Iteration 189/1000 | Loss: 0.00002943
Iteration 190/1000 | Loss: 0.00002943
Iteration 191/1000 | Loss: 0.00002943
Iteration 192/1000 | Loss: 0.00002943
Iteration 193/1000 | Loss: 0.00002943
Iteration 194/1000 | Loss: 0.00002943
Iteration 195/1000 | Loss: 0.00002943
Iteration 196/1000 | Loss: 0.00002943
Iteration 197/1000 | Loss: 0.00002943
Iteration 198/1000 | Loss: 0.00002943
Iteration 199/1000 | Loss: 0.00002943
Iteration 200/1000 | Loss: 0.00002943
Iteration 201/1000 | Loss: 0.00002943
Iteration 202/1000 | Loss: 0.00002943
Iteration 203/1000 | Loss: 0.00002943
Iteration 204/1000 | Loss: 0.00002943
Iteration 205/1000 | Loss: 0.00002943
Iteration 206/1000 | Loss: 0.00002943
Iteration 207/1000 | Loss: 0.00002943
Iteration 208/1000 | Loss: 0.00002943
Iteration 209/1000 | Loss: 0.00002943
Iteration 210/1000 | Loss: 0.00002943
Iteration 211/1000 | Loss: 0.00002943
Iteration 212/1000 | Loss: 0.00002943
Iteration 213/1000 | Loss: 0.00002943
Iteration 214/1000 | Loss: 0.00002943
Iteration 215/1000 | Loss: 0.00002943
Iteration 216/1000 | Loss: 0.00002943
Iteration 217/1000 | Loss: 0.00002943
Iteration 218/1000 | Loss: 0.00002943
Iteration 219/1000 | Loss: 0.00002943
Iteration 220/1000 | Loss: 0.00002943
Iteration 221/1000 | Loss: 0.00002943
Iteration 222/1000 | Loss: 0.00002943
Iteration 223/1000 | Loss: 0.00002943
Iteration 224/1000 | Loss: 0.00002943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [2.9427670597215183e-05, 2.9427670597215183e-05, 2.9427670597215183e-05, 2.9427670597215183e-05, 2.9427670597215183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9427670597215183e-05

Optimization complete. Final v2v error: 4.093668460845947 mm

Highest mean error: 20.58493423461914 mm for frame 174

Lowest mean error: 3.494387149810791 mm for frame 39

Saving results

Total time: 77.43383574485779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084494
Iteration 2/25 | Loss: 0.00261479
Iteration 3/25 | Loss: 0.00154601
Iteration 4/25 | Loss: 0.00145546
Iteration 5/25 | Loss: 0.00138297
Iteration 6/25 | Loss: 0.00137703
Iteration 7/25 | Loss: 0.00133388
Iteration 8/25 | Loss: 0.00127067
Iteration 9/25 | Loss: 0.00121521
Iteration 10/25 | Loss: 0.00123383
Iteration 11/25 | Loss: 0.00115999
Iteration 12/25 | Loss: 0.00109121
Iteration 13/25 | Loss: 0.00113113
Iteration 14/25 | Loss: 0.00116278
Iteration 15/25 | Loss: 0.00108328
Iteration 16/25 | Loss: 0.00110333
Iteration 17/25 | Loss: 0.00104301
Iteration 18/25 | Loss: 0.00103598
Iteration 19/25 | Loss: 0.00106459
Iteration 20/25 | Loss: 0.00102867
Iteration 21/25 | Loss: 0.00102469
Iteration 22/25 | Loss: 0.00103249
Iteration 23/25 | Loss: 0.00102935
Iteration 24/25 | Loss: 0.00102823
Iteration 25/25 | Loss: 0.00102963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33291614
Iteration 2/25 | Loss: 0.00185840
Iteration 3/25 | Loss: 0.00185840
Iteration 4/25 | Loss: 0.00185840
Iteration 5/25 | Loss: 0.00185840
Iteration 6/25 | Loss: 0.00185840
Iteration 7/25 | Loss: 0.00185840
Iteration 8/25 | Loss: 0.00185840
Iteration 9/25 | Loss: 0.00185840
Iteration 10/25 | Loss: 0.00185840
Iteration 11/25 | Loss: 0.00185840
Iteration 12/25 | Loss: 0.00185840
Iteration 13/25 | Loss: 0.00185840
Iteration 14/25 | Loss: 0.00185840
Iteration 15/25 | Loss: 0.00185840
Iteration 16/25 | Loss: 0.00185840
Iteration 17/25 | Loss: 0.00185840
Iteration 18/25 | Loss: 0.00185840
Iteration 19/25 | Loss: 0.00185840
Iteration 20/25 | Loss: 0.00185840
Iteration 21/25 | Loss: 0.00185840
Iteration 22/25 | Loss: 0.00185840
Iteration 23/25 | Loss: 0.00185840
Iteration 24/25 | Loss: 0.00185840
Iteration 25/25 | Loss: 0.00185840

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185840
Iteration 2/1000 | Loss: 0.00028728
Iteration 3/1000 | Loss: 0.00032233
Iteration 4/1000 | Loss: 0.00039246
Iteration 5/1000 | Loss: 0.00145647
Iteration 6/1000 | Loss: 0.00100021
Iteration 7/1000 | Loss: 0.00070580
Iteration 8/1000 | Loss: 0.00048963
Iteration 9/1000 | Loss: 0.00110379
Iteration 10/1000 | Loss: 0.00091031
Iteration 11/1000 | Loss: 0.00046766
Iteration 12/1000 | Loss: 0.00035675
Iteration 13/1000 | Loss: 0.00040697
Iteration 14/1000 | Loss: 0.00057371
Iteration 15/1000 | Loss: 0.00034677
Iteration 16/1000 | Loss: 0.00033714
Iteration 17/1000 | Loss: 0.00041395
Iteration 18/1000 | Loss: 0.00028141
Iteration 19/1000 | Loss: 0.00025846
Iteration 20/1000 | Loss: 0.00027523
Iteration 21/1000 | Loss: 0.00080104
Iteration 22/1000 | Loss: 0.00053314
Iteration 23/1000 | Loss: 0.00037964
Iteration 24/1000 | Loss: 0.00049049
Iteration 25/1000 | Loss: 0.00030150
Iteration 26/1000 | Loss: 0.00027288
Iteration 27/1000 | Loss: 0.00151572
Iteration 28/1000 | Loss: 0.00040083
Iteration 29/1000 | Loss: 0.00051663
Iteration 30/1000 | Loss: 0.00039204
Iteration 31/1000 | Loss: 0.00033872
Iteration 32/1000 | Loss: 0.00035486
Iteration 33/1000 | Loss: 0.00047362
Iteration 34/1000 | Loss: 0.00023179
Iteration 35/1000 | Loss: 0.00037757
Iteration 36/1000 | Loss: 0.00040998
Iteration 37/1000 | Loss: 0.00044773
Iteration 38/1000 | Loss: 0.00036555
Iteration 39/1000 | Loss: 0.00032437
Iteration 40/1000 | Loss: 0.00033505
Iteration 41/1000 | Loss: 0.00034870
Iteration 42/1000 | Loss: 0.00035236
Iteration 43/1000 | Loss: 0.00032488
Iteration 44/1000 | Loss: 0.00028735
Iteration 45/1000 | Loss: 0.00025736
Iteration 46/1000 | Loss: 0.00028262
Iteration 47/1000 | Loss: 0.00040021
Iteration 48/1000 | Loss: 0.00041972
Iteration 49/1000 | Loss: 0.00045392
Iteration 50/1000 | Loss: 0.00058332
Iteration 51/1000 | Loss: 0.00092787
Iteration 52/1000 | Loss: 0.00048938
Iteration 53/1000 | Loss: 0.00032285
Iteration 54/1000 | Loss: 0.00047287
Iteration 55/1000 | Loss: 0.00032160
Iteration 56/1000 | Loss: 0.00043058
Iteration 57/1000 | Loss: 0.00041384
Iteration 58/1000 | Loss: 0.00048551
Iteration 59/1000 | Loss: 0.00044624
Iteration 60/1000 | Loss: 0.00055773
Iteration 61/1000 | Loss: 0.00034646
Iteration 62/1000 | Loss: 0.00035407
Iteration 63/1000 | Loss: 0.00034042
Iteration 64/1000 | Loss: 0.00042027
Iteration 65/1000 | Loss: 0.00024429
Iteration 66/1000 | Loss: 0.00013567
Iteration 67/1000 | Loss: 0.00040968
Iteration 68/1000 | Loss: 0.00020797
Iteration 69/1000 | Loss: 0.00016828
Iteration 70/1000 | Loss: 0.00019152
Iteration 71/1000 | Loss: 0.00013760
Iteration 72/1000 | Loss: 0.00063103
Iteration 73/1000 | Loss: 0.00011799
Iteration 74/1000 | Loss: 0.00073748
Iteration 75/1000 | Loss: 0.00068141
Iteration 76/1000 | Loss: 0.00080875
Iteration 77/1000 | Loss: 0.00088472
Iteration 78/1000 | Loss: 0.00048631
Iteration 79/1000 | Loss: 0.00048569
Iteration 80/1000 | Loss: 0.00147347
Iteration 81/1000 | Loss: 0.00141604
Iteration 82/1000 | Loss: 0.00117273
Iteration 83/1000 | Loss: 0.00171449
Iteration 84/1000 | Loss: 0.00109570
Iteration 85/1000 | Loss: 0.00097974
Iteration 86/1000 | Loss: 0.00064134
Iteration 87/1000 | Loss: 0.00038016
Iteration 88/1000 | Loss: 0.00023286
Iteration 89/1000 | Loss: 0.00023444
Iteration 90/1000 | Loss: 0.00028772
Iteration 91/1000 | Loss: 0.00024779
Iteration 92/1000 | Loss: 0.00039025
Iteration 93/1000 | Loss: 0.00024895
Iteration 94/1000 | Loss: 0.00023607
Iteration 95/1000 | Loss: 0.00018939
Iteration 96/1000 | Loss: 0.00014268
Iteration 97/1000 | Loss: 0.00028995
Iteration 98/1000 | Loss: 0.00016064
Iteration 99/1000 | Loss: 0.00025379
Iteration 100/1000 | Loss: 0.00100923
Iteration 101/1000 | Loss: 0.00065747
Iteration 102/1000 | Loss: 0.00127463
Iteration 103/1000 | Loss: 0.00078103
Iteration 104/1000 | Loss: 0.00045371
Iteration 105/1000 | Loss: 0.00079081
Iteration 106/1000 | Loss: 0.00074775
Iteration 107/1000 | Loss: 0.00071350
Iteration 108/1000 | Loss: 0.00048394
Iteration 109/1000 | Loss: 0.00006012
Iteration 110/1000 | Loss: 0.00007630
Iteration 111/1000 | Loss: 0.00005808
Iteration 112/1000 | Loss: 0.00004863
Iteration 113/1000 | Loss: 0.00005811
Iteration 114/1000 | Loss: 0.00004295
Iteration 115/1000 | Loss: 0.00003266
Iteration 116/1000 | Loss: 0.00003873
Iteration 117/1000 | Loss: 0.00004899
Iteration 118/1000 | Loss: 0.00017729
Iteration 119/1000 | Loss: 0.00003400
Iteration 120/1000 | Loss: 0.00025677
Iteration 121/1000 | Loss: 0.00003402
Iteration 122/1000 | Loss: 0.00057492
Iteration 123/1000 | Loss: 0.00033649
Iteration 124/1000 | Loss: 0.00030286
Iteration 125/1000 | Loss: 0.00051259
Iteration 126/1000 | Loss: 0.00037602
Iteration 127/1000 | Loss: 0.00053778
Iteration 128/1000 | Loss: 0.00049654
Iteration 129/1000 | Loss: 0.00052169
Iteration 130/1000 | Loss: 0.00025448
Iteration 131/1000 | Loss: 0.00004015
Iteration 132/1000 | Loss: 0.00003819
Iteration 133/1000 | Loss: 0.00004378
Iteration 134/1000 | Loss: 0.00031583
Iteration 135/1000 | Loss: 0.00064053
Iteration 136/1000 | Loss: 0.00040213
Iteration 137/1000 | Loss: 0.00048698
Iteration 138/1000 | Loss: 0.00013855
Iteration 139/1000 | Loss: 0.00006038
Iteration 140/1000 | Loss: 0.00006486
Iteration 141/1000 | Loss: 0.00007644
Iteration 142/1000 | Loss: 0.00006171
Iteration 143/1000 | Loss: 0.00025273
Iteration 144/1000 | Loss: 0.00009578
Iteration 145/1000 | Loss: 0.00006727
Iteration 146/1000 | Loss: 0.00007534
Iteration 147/1000 | Loss: 0.00013489
Iteration 148/1000 | Loss: 0.00012228
Iteration 149/1000 | Loss: 0.00006952
Iteration 150/1000 | Loss: 0.00015057
Iteration 151/1000 | Loss: 0.00025269
Iteration 152/1000 | Loss: 0.00014150
Iteration 153/1000 | Loss: 0.00022379
Iteration 154/1000 | Loss: 0.00016694
Iteration 155/1000 | Loss: 0.00022368
Iteration 156/1000 | Loss: 0.00023210
Iteration 157/1000 | Loss: 0.00038515
Iteration 158/1000 | Loss: 0.00038926
Iteration 159/1000 | Loss: 0.00003446
Iteration 160/1000 | Loss: 0.00007237
Iteration 161/1000 | Loss: 0.00020932
Iteration 162/1000 | Loss: 0.00008430
Iteration 163/1000 | Loss: 0.00018734
Iteration 164/1000 | Loss: 0.00016814
Iteration 165/1000 | Loss: 0.00015437
Iteration 166/1000 | Loss: 0.00009395
Iteration 167/1000 | Loss: 0.00007343
Iteration 168/1000 | Loss: 0.00009689
Iteration 169/1000 | Loss: 0.00005047
Iteration 170/1000 | Loss: 0.00003736
Iteration 171/1000 | Loss: 0.00006956
Iteration 172/1000 | Loss: 0.00005161
Iteration 173/1000 | Loss: 0.00002947
Iteration 174/1000 | Loss: 0.00003458
Iteration 175/1000 | Loss: 0.00004367
Iteration 176/1000 | Loss: 0.00003840
Iteration 177/1000 | Loss: 0.00004092
Iteration 178/1000 | Loss: 0.00003717
Iteration 179/1000 | Loss: 0.00003458
Iteration 180/1000 | Loss: 0.00002582
Iteration 181/1000 | Loss: 0.00002969
Iteration 182/1000 | Loss: 0.00002588
Iteration 183/1000 | Loss: 0.00002392
Iteration 184/1000 | Loss: 0.00003529
Iteration 185/1000 | Loss: 0.00003204
Iteration 186/1000 | Loss: 0.00003747
Iteration 187/1000 | Loss: 0.00003479
Iteration 188/1000 | Loss: 0.00003461
Iteration 189/1000 | Loss: 0.00003436
Iteration 190/1000 | Loss: 0.00002831
Iteration 191/1000 | Loss: 0.00003422
Iteration 192/1000 | Loss: 0.00003739
Iteration 193/1000 | Loss: 0.00003426
Iteration 194/1000 | Loss: 0.00003823
Iteration 195/1000 | Loss: 0.00003299
Iteration 196/1000 | Loss: 0.00003683
Iteration 197/1000 | Loss: 0.00003283
Iteration 198/1000 | Loss: 0.00002396
Iteration 199/1000 | Loss: 0.00003919
Iteration 200/1000 | Loss: 0.00004047
Iteration 201/1000 | Loss: 0.00003359
Iteration 202/1000 | Loss: 0.00002825
Iteration 203/1000 | Loss: 0.00003302
Iteration 204/1000 | Loss: 0.00004117
Iteration 205/1000 | Loss: 0.00004852
Iteration 206/1000 | Loss: 0.00003501
Iteration 207/1000 | Loss: 0.00003489
Iteration 208/1000 | Loss: 0.00003317
Iteration 209/1000 | Loss: 0.00003499
Iteration 210/1000 | Loss: 0.00003437
Iteration 211/1000 | Loss: 0.00005098
Iteration 212/1000 | Loss: 0.00002018
Iteration 213/1000 | Loss: 0.00003307
Iteration 214/1000 | Loss: 0.00003500
Iteration 215/1000 | Loss: 0.00003493
Iteration 216/1000 | Loss: 0.00003558
Iteration 217/1000 | Loss: 0.00004506
Iteration 218/1000 | Loss: 0.00004152
Iteration 219/1000 | Loss: 0.00002276
Iteration 220/1000 | Loss: 0.00001764
Iteration 221/1000 | Loss: 0.00001617
Iteration 222/1000 | Loss: 0.00001549
Iteration 223/1000 | Loss: 0.00001501
Iteration 224/1000 | Loss: 0.00001499
Iteration 225/1000 | Loss: 0.00001496
Iteration 226/1000 | Loss: 0.00001494
Iteration 227/1000 | Loss: 0.00001479
Iteration 228/1000 | Loss: 0.00001473
Iteration 229/1000 | Loss: 0.00001473
Iteration 230/1000 | Loss: 0.00001473
Iteration 231/1000 | Loss: 0.00001472
Iteration 232/1000 | Loss: 0.00001471
Iteration 233/1000 | Loss: 0.00001470
Iteration 234/1000 | Loss: 0.00001469
Iteration 235/1000 | Loss: 0.00001469
Iteration 236/1000 | Loss: 0.00001468
Iteration 237/1000 | Loss: 0.00001467
Iteration 238/1000 | Loss: 0.00001467
Iteration 239/1000 | Loss: 0.00001466
Iteration 240/1000 | Loss: 0.00001466
Iteration 241/1000 | Loss: 0.00001466
Iteration 242/1000 | Loss: 0.00001465
Iteration 243/1000 | Loss: 0.00001464
Iteration 244/1000 | Loss: 0.00001464
Iteration 245/1000 | Loss: 0.00001463
Iteration 246/1000 | Loss: 0.00001456
Iteration 247/1000 | Loss: 0.00001456
Iteration 248/1000 | Loss: 0.00001454
Iteration 249/1000 | Loss: 0.00001454
Iteration 250/1000 | Loss: 0.00001454
Iteration 251/1000 | Loss: 0.00001453
Iteration 252/1000 | Loss: 0.00001453
Iteration 253/1000 | Loss: 0.00001453
Iteration 254/1000 | Loss: 0.00001453
Iteration 255/1000 | Loss: 0.00001453
Iteration 256/1000 | Loss: 0.00001452
Iteration 257/1000 | Loss: 0.00001452
Iteration 258/1000 | Loss: 0.00001452
Iteration 259/1000 | Loss: 0.00001452
Iteration 260/1000 | Loss: 0.00001452
Iteration 261/1000 | Loss: 0.00001452
Iteration 262/1000 | Loss: 0.00001452
Iteration 263/1000 | Loss: 0.00001452
Iteration 264/1000 | Loss: 0.00001451
Iteration 265/1000 | Loss: 0.00001451
Iteration 266/1000 | Loss: 0.00001450
Iteration 267/1000 | Loss: 0.00001450
Iteration 268/1000 | Loss: 0.00001450
Iteration 269/1000 | Loss: 0.00001449
Iteration 270/1000 | Loss: 0.00001449
Iteration 271/1000 | Loss: 0.00001449
Iteration 272/1000 | Loss: 0.00001449
Iteration 273/1000 | Loss: 0.00001449
Iteration 274/1000 | Loss: 0.00001449
Iteration 275/1000 | Loss: 0.00001449
Iteration 276/1000 | Loss: 0.00001449
Iteration 277/1000 | Loss: 0.00001448
Iteration 278/1000 | Loss: 0.00001448
Iteration 279/1000 | Loss: 0.00001448
Iteration 280/1000 | Loss: 0.00001448
Iteration 281/1000 | Loss: 0.00001448
Iteration 282/1000 | Loss: 0.00001447
Iteration 283/1000 | Loss: 0.00001447
Iteration 284/1000 | Loss: 0.00001447
Iteration 285/1000 | Loss: 0.00001446
Iteration 286/1000 | Loss: 0.00001446
Iteration 287/1000 | Loss: 0.00001446
Iteration 288/1000 | Loss: 0.00001445
Iteration 289/1000 | Loss: 0.00001445
Iteration 290/1000 | Loss: 0.00001445
Iteration 291/1000 | Loss: 0.00001445
Iteration 292/1000 | Loss: 0.00001445
Iteration 293/1000 | Loss: 0.00001445
Iteration 294/1000 | Loss: 0.00001445
Iteration 295/1000 | Loss: 0.00001445
Iteration 296/1000 | Loss: 0.00001445
Iteration 297/1000 | Loss: 0.00001445
Iteration 298/1000 | Loss: 0.00001445
Iteration 299/1000 | Loss: 0.00001445
Iteration 300/1000 | Loss: 0.00001445
Iteration 301/1000 | Loss: 0.00001444
Iteration 302/1000 | Loss: 0.00001444
Iteration 303/1000 | Loss: 0.00001444
Iteration 304/1000 | Loss: 0.00001444
Iteration 305/1000 | Loss: 0.00001444
Iteration 306/1000 | Loss: 0.00001444
Iteration 307/1000 | Loss: 0.00001444
Iteration 308/1000 | Loss: 0.00001444
Iteration 309/1000 | Loss: 0.00001444
Iteration 310/1000 | Loss: 0.00001444
Iteration 311/1000 | Loss: 0.00001444
Iteration 312/1000 | Loss: 0.00001444
Iteration 313/1000 | Loss: 0.00001444
Iteration 314/1000 | Loss: 0.00001444
Iteration 315/1000 | Loss: 0.00001444
Iteration 316/1000 | Loss: 0.00001443
Iteration 317/1000 | Loss: 0.00001443
Iteration 318/1000 | Loss: 0.00001443
Iteration 319/1000 | Loss: 0.00001443
Iteration 320/1000 | Loss: 0.00001443
Iteration 321/1000 | Loss: 0.00001443
Iteration 322/1000 | Loss: 0.00001443
Iteration 323/1000 | Loss: 0.00001443
Iteration 324/1000 | Loss: 0.00001443
Iteration 325/1000 | Loss: 0.00001443
Iteration 326/1000 | Loss: 0.00001443
Iteration 327/1000 | Loss: 0.00001442
Iteration 328/1000 | Loss: 0.00001442
Iteration 329/1000 | Loss: 0.00001442
Iteration 330/1000 | Loss: 0.00001442
Iteration 331/1000 | Loss: 0.00001442
Iteration 332/1000 | Loss: 0.00001442
Iteration 333/1000 | Loss: 0.00001442
Iteration 334/1000 | Loss: 0.00001442
Iteration 335/1000 | Loss: 0.00001442
Iteration 336/1000 | Loss: 0.00001442
Iteration 337/1000 | Loss: 0.00001442
Iteration 338/1000 | Loss: 0.00001442
Iteration 339/1000 | Loss: 0.00001442
Iteration 340/1000 | Loss: 0.00001442
Iteration 341/1000 | Loss: 0.00001442
Iteration 342/1000 | Loss: 0.00001442
Iteration 343/1000 | Loss: 0.00001442
Iteration 344/1000 | Loss: 0.00001442
Iteration 345/1000 | Loss: 0.00001442
Iteration 346/1000 | Loss: 0.00001442
Iteration 347/1000 | Loss: 0.00001442
Iteration 348/1000 | Loss: 0.00001442
Iteration 349/1000 | Loss: 0.00001442
Iteration 350/1000 | Loss: 0.00001442
Iteration 351/1000 | Loss: 0.00001442
Iteration 352/1000 | Loss: 0.00001442
Iteration 353/1000 | Loss: 0.00001442
Iteration 354/1000 | Loss: 0.00001442
Iteration 355/1000 | Loss: 0.00001442
Iteration 356/1000 | Loss: 0.00001442
Iteration 357/1000 | Loss: 0.00001442
Iteration 358/1000 | Loss: 0.00001442
Iteration 359/1000 | Loss: 0.00001442
Iteration 360/1000 | Loss: 0.00001442
Iteration 361/1000 | Loss: 0.00001442
Iteration 362/1000 | Loss: 0.00001442
Iteration 363/1000 | Loss: 0.00001442
Iteration 364/1000 | Loss: 0.00001442
Iteration 365/1000 | Loss: 0.00001442
Iteration 366/1000 | Loss: 0.00001442
Iteration 367/1000 | Loss: 0.00001442
Iteration 368/1000 | Loss: 0.00001442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 368. Stopping optimization.
Last 5 losses: [1.4416794329008553e-05, 1.4416794329008553e-05, 1.4416794329008553e-05, 1.4416794329008553e-05, 1.4416794329008553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4416794329008553e-05

Optimization complete. Final v2v error: 3.2082982063293457 mm

Highest mean error: 4.844391345977783 mm for frame 78

Lowest mean error: 2.532824754714966 mm for frame 107

Saving results

Total time: 370.37871861457825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071802
Iteration 2/25 | Loss: 0.00200083
Iteration 3/25 | Loss: 0.00126889
Iteration 4/25 | Loss: 0.00113594
Iteration 5/25 | Loss: 0.00117882
Iteration 6/25 | Loss: 0.00114814
Iteration 7/25 | Loss: 0.00112865
Iteration 8/25 | Loss: 0.00107577
Iteration 9/25 | Loss: 0.00096567
Iteration 10/25 | Loss: 0.00094567
Iteration 11/25 | Loss: 0.00093578
Iteration 12/25 | Loss: 0.00093503
Iteration 13/25 | Loss: 0.00093075
Iteration 14/25 | Loss: 0.00092638
Iteration 15/25 | Loss: 0.00092127
Iteration 16/25 | Loss: 0.00092442
Iteration 17/25 | Loss: 0.00092971
Iteration 18/25 | Loss: 0.00091323
Iteration 19/25 | Loss: 0.00090723
Iteration 20/25 | Loss: 0.00090577
Iteration 21/25 | Loss: 0.00090527
Iteration 22/25 | Loss: 0.00090510
Iteration 23/25 | Loss: 0.00090509
Iteration 24/25 | Loss: 0.00090509
Iteration 25/25 | Loss: 0.00090508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35357666
Iteration 2/25 | Loss: 0.00082780
Iteration 3/25 | Loss: 0.00082780
Iteration 4/25 | Loss: 0.00082780
Iteration 5/25 | Loss: 0.00082780
Iteration 6/25 | Loss: 0.00082780
Iteration 7/25 | Loss: 0.00082780
Iteration 8/25 | Loss: 0.00082780
Iteration 9/25 | Loss: 0.00082780
Iteration 10/25 | Loss: 0.00082780
Iteration 11/25 | Loss: 0.00082780
Iteration 12/25 | Loss: 0.00082779
Iteration 13/25 | Loss: 0.00082779
Iteration 14/25 | Loss: 0.00082779
Iteration 15/25 | Loss: 0.00082779
Iteration 16/25 | Loss: 0.00082779
Iteration 17/25 | Loss: 0.00082779
Iteration 18/25 | Loss: 0.00082779
Iteration 19/25 | Loss: 0.00082779
Iteration 20/25 | Loss: 0.00082779
Iteration 21/25 | Loss: 0.00082779
Iteration 22/25 | Loss: 0.00082779
Iteration 23/25 | Loss: 0.00082779
Iteration 24/25 | Loss: 0.00082779
Iteration 25/25 | Loss: 0.00082779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082779
Iteration 2/1000 | Loss: 0.00003816
Iteration 3/1000 | Loss: 0.00007290
Iteration 4/1000 | Loss: 0.00001704
Iteration 5/1000 | Loss: 0.00005932
Iteration 6/1000 | Loss: 0.00001476
Iteration 7/1000 | Loss: 0.00001396
Iteration 8/1000 | Loss: 0.00007901
Iteration 9/1000 | Loss: 0.00001372
Iteration 10/1000 | Loss: 0.00001311
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001309
Iteration 13/1000 | Loss: 0.00001308
Iteration 14/1000 | Loss: 0.00001306
Iteration 15/1000 | Loss: 0.00001301
Iteration 16/1000 | Loss: 0.00001298
Iteration 17/1000 | Loss: 0.00001297
Iteration 18/1000 | Loss: 0.00001293
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001291
Iteration 21/1000 | Loss: 0.00001291
Iteration 22/1000 | Loss: 0.00001291
Iteration 23/1000 | Loss: 0.00001291
Iteration 24/1000 | Loss: 0.00001290
Iteration 25/1000 | Loss: 0.00001290
Iteration 26/1000 | Loss: 0.00001289
Iteration 27/1000 | Loss: 0.00001288
Iteration 28/1000 | Loss: 0.00001287
Iteration 29/1000 | Loss: 0.00001286
Iteration 30/1000 | Loss: 0.00001286
Iteration 31/1000 | Loss: 0.00001286
Iteration 32/1000 | Loss: 0.00001286
Iteration 33/1000 | Loss: 0.00001285
Iteration 34/1000 | Loss: 0.00001285
Iteration 35/1000 | Loss: 0.00001284
Iteration 36/1000 | Loss: 0.00001284
Iteration 37/1000 | Loss: 0.00001284
Iteration 38/1000 | Loss: 0.00001283
Iteration 39/1000 | Loss: 0.00001283
Iteration 40/1000 | Loss: 0.00001283
Iteration 41/1000 | Loss: 0.00001283
Iteration 42/1000 | Loss: 0.00001283
Iteration 43/1000 | Loss: 0.00001283
Iteration 44/1000 | Loss: 0.00001282
Iteration 45/1000 | Loss: 0.00001282
Iteration 46/1000 | Loss: 0.00001282
Iteration 47/1000 | Loss: 0.00001282
Iteration 48/1000 | Loss: 0.00001282
Iteration 49/1000 | Loss: 0.00001281
Iteration 50/1000 | Loss: 0.00001281
Iteration 51/1000 | Loss: 0.00001281
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001280
Iteration 54/1000 | Loss: 0.00001280
Iteration 55/1000 | Loss: 0.00001279
Iteration 56/1000 | Loss: 0.00001279
Iteration 57/1000 | Loss: 0.00001279
Iteration 58/1000 | Loss: 0.00001278
Iteration 59/1000 | Loss: 0.00001278
Iteration 60/1000 | Loss: 0.00001277
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00001275
Iteration 63/1000 | Loss: 0.00001275
Iteration 64/1000 | Loss: 0.00001275
Iteration 65/1000 | Loss: 0.00001275
Iteration 66/1000 | Loss: 0.00001275
Iteration 67/1000 | Loss: 0.00001275
Iteration 68/1000 | Loss: 0.00001275
Iteration 69/1000 | Loss: 0.00001275
Iteration 70/1000 | Loss: 0.00001274
Iteration 71/1000 | Loss: 0.00001274
Iteration 72/1000 | Loss: 0.00001273
Iteration 73/1000 | Loss: 0.00001272
Iteration 74/1000 | Loss: 0.00001272
Iteration 75/1000 | Loss: 0.00001271
Iteration 76/1000 | Loss: 0.00001271
Iteration 77/1000 | Loss: 0.00001271
Iteration 78/1000 | Loss: 0.00001271
Iteration 79/1000 | Loss: 0.00001270
Iteration 80/1000 | Loss: 0.00001270
Iteration 81/1000 | Loss: 0.00001270
Iteration 82/1000 | Loss: 0.00001270
Iteration 83/1000 | Loss: 0.00001270
Iteration 84/1000 | Loss: 0.00001270
Iteration 85/1000 | Loss: 0.00001270
Iteration 86/1000 | Loss: 0.00001270
Iteration 87/1000 | Loss: 0.00001270
Iteration 88/1000 | Loss: 0.00001270
Iteration 89/1000 | Loss: 0.00001270
Iteration 90/1000 | Loss: 0.00001269
Iteration 91/1000 | Loss: 0.00001269
Iteration 92/1000 | Loss: 0.00001269
Iteration 93/1000 | Loss: 0.00001269
Iteration 94/1000 | Loss: 0.00001269
Iteration 95/1000 | Loss: 0.00001269
Iteration 96/1000 | Loss: 0.00001268
Iteration 97/1000 | Loss: 0.00001268
Iteration 98/1000 | Loss: 0.00001268
Iteration 99/1000 | Loss: 0.00001268
Iteration 100/1000 | Loss: 0.00001268
Iteration 101/1000 | Loss: 0.00001268
Iteration 102/1000 | Loss: 0.00001268
Iteration 103/1000 | Loss: 0.00001268
Iteration 104/1000 | Loss: 0.00001267
Iteration 105/1000 | Loss: 0.00001267
Iteration 106/1000 | Loss: 0.00001267
Iteration 107/1000 | Loss: 0.00001267
Iteration 108/1000 | Loss: 0.00001267
Iteration 109/1000 | Loss: 0.00001267
Iteration 110/1000 | Loss: 0.00001267
Iteration 111/1000 | Loss: 0.00001267
Iteration 112/1000 | Loss: 0.00001267
Iteration 113/1000 | Loss: 0.00001267
Iteration 114/1000 | Loss: 0.00001267
Iteration 115/1000 | Loss: 0.00001267
Iteration 116/1000 | Loss: 0.00001267
Iteration 117/1000 | Loss: 0.00001266
Iteration 118/1000 | Loss: 0.00001266
Iteration 119/1000 | Loss: 0.00001266
Iteration 120/1000 | Loss: 0.00001266
Iteration 121/1000 | Loss: 0.00001266
Iteration 122/1000 | Loss: 0.00001266
Iteration 123/1000 | Loss: 0.00001266
Iteration 124/1000 | Loss: 0.00001266
Iteration 125/1000 | Loss: 0.00001266
Iteration 126/1000 | Loss: 0.00001266
Iteration 127/1000 | Loss: 0.00001265
Iteration 128/1000 | Loss: 0.00001265
Iteration 129/1000 | Loss: 0.00001265
Iteration 130/1000 | Loss: 0.00001265
Iteration 131/1000 | Loss: 0.00001265
Iteration 132/1000 | Loss: 0.00001265
Iteration 133/1000 | Loss: 0.00001265
Iteration 134/1000 | Loss: 0.00001264
Iteration 135/1000 | Loss: 0.00001264
Iteration 136/1000 | Loss: 0.00001264
Iteration 137/1000 | Loss: 0.00001264
Iteration 138/1000 | Loss: 0.00001264
Iteration 139/1000 | Loss: 0.00001264
Iteration 140/1000 | Loss: 0.00001264
Iteration 141/1000 | Loss: 0.00001264
Iteration 142/1000 | Loss: 0.00001264
Iteration 143/1000 | Loss: 0.00001264
Iteration 144/1000 | Loss: 0.00001264
Iteration 145/1000 | Loss: 0.00001264
Iteration 146/1000 | Loss: 0.00001264
Iteration 147/1000 | Loss: 0.00001264
Iteration 148/1000 | Loss: 0.00001264
Iteration 149/1000 | Loss: 0.00001264
Iteration 150/1000 | Loss: 0.00001264
Iteration 151/1000 | Loss: 0.00001264
Iteration 152/1000 | Loss: 0.00001264
Iteration 153/1000 | Loss: 0.00001264
Iteration 154/1000 | Loss: 0.00001264
Iteration 155/1000 | Loss: 0.00001264
Iteration 156/1000 | Loss: 0.00001264
Iteration 157/1000 | Loss: 0.00001264
Iteration 158/1000 | Loss: 0.00001264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.2635904568014666e-05, 1.2635904568014666e-05, 1.2635904568014666e-05, 1.2635904568014666e-05, 1.2635904568014666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2635904568014666e-05

Optimization complete. Final v2v error: 2.9841952323913574 mm

Highest mean error: 4.082851409912109 mm for frame 64

Lowest mean error: 2.475008249282837 mm for frame 147

Saving results

Total time: 65.23408603668213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00672050
Iteration 2/25 | Loss: 0.00112327
Iteration 3/25 | Loss: 0.00097137
Iteration 4/25 | Loss: 0.00095843
Iteration 5/25 | Loss: 0.00095479
Iteration 6/25 | Loss: 0.00095432
Iteration 7/25 | Loss: 0.00095432
Iteration 8/25 | Loss: 0.00095432
Iteration 9/25 | Loss: 0.00095432
Iteration 10/25 | Loss: 0.00095432
Iteration 11/25 | Loss: 0.00095432
Iteration 12/25 | Loss: 0.00095432
Iteration 13/25 | Loss: 0.00095432
Iteration 14/25 | Loss: 0.00095432
Iteration 15/25 | Loss: 0.00095432
Iteration 16/25 | Loss: 0.00095432
Iteration 17/25 | Loss: 0.00095432
Iteration 18/25 | Loss: 0.00095432
Iteration 19/25 | Loss: 0.00095432
Iteration 20/25 | Loss: 0.00095432
Iteration 21/25 | Loss: 0.00095432
Iteration 22/25 | Loss: 0.00095432
Iteration 23/25 | Loss: 0.00095432
Iteration 24/25 | Loss: 0.00095432
Iteration 25/25 | Loss: 0.00095432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.01984167
Iteration 2/25 | Loss: 0.00074630
Iteration 3/25 | Loss: 0.00074629
Iteration 4/25 | Loss: 0.00074629
Iteration 5/25 | Loss: 0.00074629
Iteration 6/25 | Loss: 0.00074629
Iteration 7/25 | Loss: 0.00074629
Iteration 8/25 | Loss: 0.00074629
Iteration 9/25 | Loss: 0.00074629
Iteration 10/25 | Loss: 0.00074629
Iteration 11/25 | Loss: 0.00074629
Iteration 12/25 | Loss: 0.00074629
Iteration 13/25 | Loss: 0.00074629
Iteration 14/25 | Loss: 0.00074629
Iteration 15/25 | Loss: 0.00074629
Iteration 16/25 | Loss: 0.00074629
Iteration 17/25 | Loss: 0.00074629
Iteration 18/25 | Loss: 0.00074629
Iteration 19/25 | Loss: 0.00074629
Iteration 20/25 | Loss: 0.00074629
Iteration 21/25 | Loss: 0.00074629
Iteration 22/25 | Loss: 0.00074629
Iteration 23/25 | Loss: 0.00074629
Iteration 24/25 | Loss: 0.00074629
Iteration 25/25 | Loss: 0.00074629

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074629
Iteration 2/1000 | Loss: 0.00004624
Iteration 3/1000 | Loss: 0.00003188
Iteration 4/1000 | Loss: 0.00002397
Iteration 5/1000 | Loss: 0.00002092
Iteration 6/1000 | Loss: 0.00001970
Iteration 7/1000 | Loss: 0.00001877
Iteration 8/1000 | Loss: 0.00001833
Iteration 9/1000 | Loss: 0.00001797
Iteration 10/1000 | Loss: 0.00001776
Iteration 11/1000 | Loss: 0.00001748
Iteration 12/1000 | Loss: 0.00001727
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001708
Iteration 15/1000 | Loss: 0.00001708
Iteration 16/1000 | Loss: 0.00001707
Iteration 17/1000 | Loss: 0.00001707
Iteration 18/1000 | Loss: 0.00001706
Iteration 19/1000 | Loss: 0.00001706
Iteration 20/1000 | Loss: 0.00001705
Iteration 21/1000 | Loss: 0.00001704
Iteration 22/1000 | Loss: 0.00001704
Iteration 23/1000 | Loss: 0.00001703
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001702
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001699
Iteration 31/1000 | Loss: 0.00001699
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001698
Iteration 34/1000 | Loss: 0.00001697
Iteration 35/1000 | Loss: 0.00001695
Iteration 36/1000 | Loss: 0.00001694
Iteration 37/1000 | Loss: 0.00001693
Iteration 38/1000 | Loss: 0.00001693
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001691
Iteration 42/1000 | Loss: 0.00001688
Iteration 43/1000 | Loss: 0.00001687
Iteration 44/1000 | Loss: 0.00001686
Iteration 45/1000 | Loss: 0.00001686
Iteration 46/1000 | Loss: 0.00001686
Iteration 47/1000 | Loss: 0.00001685
Iteration 48/1000 | Loss: 0.00001684
Iteration 49/1000 | Loss: 0.00001684
Iteration 50/1000 | Loss: 0.00001683
Iteration 51/1000 | Loss: 0.00001683
Iteration 52/1000 | Loss: 0.00001683
Iteration 53/1000 | Loss: 0.00001683
Iteration 54/1000 | Loss: 0.00001683
Iteration 55/1000 | Loss: 0.00001682
Iteration 56/1000 | Loss: 0.00001682
Iteration 57/1000 | Loss: 0.00001682
Iteration 58/1000 | Loss: 0.00001682
Iteration 59/1000 | Loss: 0.00001682
Iteration 60/1000 | Loss: 0.00001682
Iteration 61/1000 | Loss: 0.00001681
Iteration 62/1000 | Loss: 0.00001678
Iteration 63/1000 | Loss: 0.00001678
Iteration 64/1000 | Loss: 0.00001676
Iteration 65/1000 | Loss: 0.00001676
Iteration 66/1000 | Loss: 0.00001676
Iteration 67/1000 | Loss: 0.00001676
Iteration 68/1000 | Loss: 0.00001676
Iteration 69/1000 | Loss: 0.00001675
Iteration 70/1000 | Loss: 0.00001675
Iteration 71/1000 | Loss: 0.00001674
Iteration 72/1000 | Loss: 0.00001673
Iteration 73/1000 | Loss: 0.00001673
Iteration 74/1000 | Loss: 0.00001673
Iteration 75/1000 | Loss: 0.00001672
Iteration 76/1000 | Loss: 0.00001672
Iteration 77/1000 | Loss: 0.00001671
Iteration 78/1000 | Loss: 0.00001671
Iteration 79/1000 | Loss: 0.00001670
Iteration 80/1000 | Loss: 0.00001670
Iteration 81/1000 | Loss: 0.00001670
Iteration 82/1000 | Loss: 0.00001670
Iteration 83/1000 | Loss: 0.00001670
Iteration 84/1000 | Loss: 0.00001669
Iteration 85/1000 | Loss: 0.00001668
Iteration 86/1000 | Loss: 0.00001668
Iteration 87/1000 | Loss: 0.00001667
Iteration 88/1000 | Loss: 0.00001667
Iteration 89/1000 | Loss: 0.00001667
Iteration 90/1000 | Loss: 0.00001666
Iteration 91/1000 | Loss: 0.00001666
Iteration 92/1000 | Loss: 0.00001666
Iteration 93/1000 | Loss: 0.00001665
Iteration 94/1000 | Loss: 0.00001665
Iteration 95/1000 | Loss: 0.00001665
Iteration 96/1000 | Loss: 0.00001664
Iteration 97/1000 | Loss: 0.00001663
Iteration 98/1000 | Loss: 0.00001662
Iteration 99/1000 | Loss: 0.00001662
Iteration 100/1000 | Loss: 0.00001662
Iteration 101/1000 | Loss: 0.00001662
Iteration 102/1000 | Loss: 0.00001662
Iteration 103/1000 | Loss: 0.00001662
Iteration 104/1000 | Loss: 0.00001662
Iteration 105/1000 | Loss: 0.00001661
Iteration 106/1000 | Loss: 0.00001661
Iteration 107/1000 | Loss: 0.00001661
Iteration 108/1000 | Loss: 0.00001661
Iteration 109/1000 | Loss: 0.00001661
Iteration 110/1000 | Loss: 0.00001661
Iteration 111/1000 | Loss: 0.00001661
Iteration 112/1000 | Loss: 0.00001661
Iteration 113/1000 | Loss: 0.00001661
Iteration 114/1000 | Loss: 0.00001661
Iteration 115/1000 | Loss: 0.00001660
Iteration 116/1000 | Loss: 0.00001660
Iteration 117/1000 | Loss: 0.00001660
Iteration 118/1000 | Loss: 0.00001660
Iteration 119/1000 | Loss: 0.00001660
Iteration 120/1000 | Loss: 0.00001660
Iteration 121/1000 | Loss: 0.00001660
Iteration 122/1000 | Loss: 0.00001660
Iteration 123/1000 | Loss: 0.00001659
Iteration 124/1000 | Loss: 0.00001659
Iteration 125/1000 | Loss: 0.00001659
Iteration 126/1000 | Loss: 0.00001659
Iteration 127/1000 | Loss: 0.00001659
Iteration 128/1000 | Loss: 0.00001659
Iteration 129/1000 | Loss: 0.00001658
Iteration 130/1000 | Loss: 0.00001658
Iteration 131/1000 | Loss: 0.00001658
Iteration 132/1000 | Loss: 0.00001658
Iteration 133/1000 | Loss: 0.00001658
Iteration 134/1000 | Loss: 0.00001657
Iteration 135/1000 | Loss: 0.00001657
Iteration 136/1000 | Loss: 0.00001657
Iteration 137/1000 | Loss: 0.00001657
Iteration 138/1000 | Loss: 0.00001657
Iteration 139/1000 | Loss: 0.00001657
Iteration 140/1000 | Loss: 0.00001657
Iteration 141/1000 | Loss: 0.00001656
Iteration 142/1000 | Loss: 0.00001656
Iteration 143/1000 | Loss: 0.00001656
Iteration 144/1000 | Loss: 0.00001656
Iteration 145/1000 | Loss: 0.00001656
Iteration 146/1000 | Loss: 0.00001656
Iteration 147/1000 | Loss: 0.00001656
Iteration 148/1000 | Loss: 0.00001656
Iteration 149/1000 | Loss: 0.00001656
Iteration 150/1000 | Loss: 0.00001656
Iteration 151/1000 | Loss: 0.00001656
Iteration 152/1000 | Loss: 0.00001655
Iteration 153/1000 | Loss: 0.00001655
Iteration 154/1000 | Loss: 0.00001655
Iteration 155/1000 | Loss: 0.00001655
Iteration 156/1000 | Loss: 0.00001655
Iteration 157/1000 | Loss: 0.00001655
Iteration 158/1000 | Loss: 0.00001655
Iteration 159/1000 | Loss: 0.00001655
Iteration 160/1000 | Loss: 0.00001655
Iteration 161/1000 | Loss: 0.00001654
Iteration 162/1000 | Loss: 0.00001654
Iteration 163/1000 | Loss: 0.00001654
Iteration 164/1000 | Loss: 0.00001654
Iteration 165/1000 | Loss: 0.00001654
Iteration 166/1000 | Loss: 0.00001653
Iteration 167/1000 | Loss: 0.00001653
Iteration 168/1000 | Loss: 0.00001653
Iteration 169/1000 | Loss: 0.00001652
Iteration 170/1000 | Loss: 0.00001652
Iteration 171/1000 | Loss: 0.00001652
Iteration 172/1000 | Loss: 0.00001652
Iteration 173/1000 | Loss: 0.00001652
Iteration 174/1000 | Loss: 0.00001652
Iteration 175/1000 | Loss: 0.00001652
Iteration 176/1000 | Loss: 0.00001652
Iteration 177/1000 | Loss: 0.00001651
Iteration 178/1000 | Loss: 0.00001651
Iteration 179/1000 | Loss: 0.00001650
Iteration 180/1000 | Loss: 0.00001650
Iteration 181/1000 | Loss: 0.00001650
Iteration 182/1000 | Loss: 0.00001650
Iteration 183/1000 | Loss: 0.00001649
Iteration 184/1000 | Loss: 0.00001649
Iteration 185/1000 | Loss: 0.00001649
Iteration 186/1000 | Loss: 0.00001649
Iteration 187/1000 | Loss: 0.00001649
Iteration 188/1000 | Loss: 0.00001649
Iteration 189/1000 | Loss: 0.00001649
Iteration 190/1000 | Loss: 0.00001648
Iteration 191/1000 | Loss: 0.00001648
Iteration 192/1000 | Loss: 0.00001648
Iteration 193/1000 | Loss: 0.00001648
Iteration 194/1000 | Loss: 0.00001648
Iteration 195/1000 | Loss: 0.00001648
Iteration 196/1000 | Loss: 0.00001647
Iteration 197/1000 | Loss: 0.00001647
Iteration 198/1000 | Loss: 0.00001647
Iteration 199/1000 | Loss: 0.00001647
Iteration 200/1000 | Loss: 0.00001647
Iteration 201/1000 | Loss: 0.00001646
Iteration 202/1000 | Loss: 0.00001646
Iteration 203/1000 | Loss: 0.00001646
Iteration 204/1000 | Loss: 0.00001646
Iteration 205/1000 | Loss: 0.00001646
Iteration 206/1000 | Loss: 0.00001646
Iteration 207/1000 | Loss: 0.00001646
Iteration 208/1000 | Loss: 0.00001645
Iteration 209/1000 | Loss: 0.00001645
Iteration 210/1000 | Loss: 0.00001645
Iteration 211/1000 | Loss: 0.00001645
Iteration 212/1000 | Loss: 0.00001645
Iteration 213/1000 | Loss: 0.00001645
Iteration 214/1000 | Loss: 0.00001644
Iteration 215/1000 | Loss: 0.00001644
Iteration 216/1000 | Loss: 0.00001644
Iteration 217/1000 | Loss: 0.00001644
Iteration 218/1000 | Loss: 0.00001644
Iteration 219/1000 | Loss: 0.00001644
Iteration 220/1000 | Loss: 0.00001643
Iteration 221/1000 | Loss: 0.00001643
Iteration 222/1000 | Loss: 0.00001643
Iteration 223/1000 | Loss: 0.00001643
Iteration 224/1000 | Loss: 0.00001643
Iteration 225/1000 | Loss: 0.00001643
Iteration 226/1000 | Loss: 0.00001643
Iteration 227/1000 | Loss: 0.00001643
Iteration 228/1000 | Loss: 0.00001643
Iteration 229/1000 | Loss: 0.00001643
Iteration 230/1000 | Loss: 0.00001643
Iteration 231/1000 | Loss: 0.00001643
Iteration 232/1000 | Loss: 0.00001643
Iteration 233/1000 | Loss: 0.00001643
Iteration 234/1000 | Loss: 0.00001643
Iteration 235/1000 | Loss: 0.00001643
Iteration 236/1000 | Loss: 0.00001642
Iteration 237/1000 | Loss: 0.00001642
Iteration 238/1000 | Loss: 0.00001642
Iteration 239/1000 | Loss: 0.00001642
Iteration 240/1000 | Loss: 0.00001642
Iteration 241/1000 | Loss: 0.00001642
Iteration 242/1000 | Loss: 0.00001642
Iteration 243/1000 | Loss: 0.00001642
Iteration 244/1000 | Loss: 0.00001642
Iteration 245/1000 | Loss: 0.00001642
Iteration 246/1000 | Loss: 0.00001642
Iteration 247/1000 | Loss: 0.00001642
Iteration 248/1000 | Loss: 0.00001642
Iteration 249/1000 | Loss: 0.00001641
Iteration 250/1000 | Loss: 0.00001641
Iteration 251/1000 | Loss: 0.00001641
Iteration 252/1000 | Loss: 0.00001641
Iteration 253/1000 | Loss: 0.00001641
Iteration 254/1000 | Loss: 0.00001641
Iteration 255/1000 | Loss: 0.00001641
Iteration 256/1000 | Loss: 0.00001641
Iteration 257/1000 | Loss: 0.00001641
Iteration 258/1000 | Loss: 0.00001641
Iteration 259/1000 | Loss: 0.00001641
Iteration 260/1000 | Loss: 0.00001641
Iteration 261/1000 | Loss: 0.00001641
Iteration 262/1000 | Loss: 0.00001641
Iteration 263/1000 | Loss: 0.00001641
Iteration 264/1000 | Loss: 0.00001641
Iteration 265/1000 | Loss: 0.00001641
Iteration 266/1000 | Loss: 0.00001641
Iteration 267/1000 | Loss: 0.00001641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [1.640804657654371e-05, 1.640804657654371e-05, 1.640804657654371e-05, 1.640804657654371e-05, 1.640804657654371e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.640804657654371e-05

Optimization complete. Final v2v error: 3.4243409633636475 mm

Highest mean error: 3.9829583168029785 mm for frame 140

Lowest mean error: 2.85211181640625 mm for frame 51

Saving results

Total time: 54.43152856826782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851938
Iteration 2/25 | Loss: 0.00113852
Iteration 3/25 | Loss: 0.00090284
Iteration 4/25 | Loss: 0.00088722
Iteration 5/25 | Loss: 0.00088356
Iteration 6/25 | Loss: 0.00088222
Iteration 7/25 | Loss: 0.00088218
Iteration 8/25 | Loss: 0.00088218
Iteration 9/25 | Loss: 0.00088218
Iteration 10/25 | Loss: 0.00088218
Iteration 11/25 | Loss: 0.00088218
Iteration 12/25 | Loss: 0.00088218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008821820956654847, 0.0008821820956654847, 0.0008821820956654847, 0.0008821820956654847, 0.0008821820956654847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008821820956654847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.29075718
Iteration 2/25 | Loss: 0.00069297
Iteration 3/25 | Loss: 0.00069292
Iteration 4/25 | Loss: 0.00069292
Iteration 5/25 | Loss: 0.00069292
Iteration 6/25 | Loss: 0.00069292
Iteration 7/25 | Loss: 0.00069292
Iteration 8/25 | Loss: 0.00069292
Iteration 9/25 | Loss: 0.00069292
Iteration 10/25 | Loss: 0.00069292
Iteration 11/25 | Loss: 0.00069292
Iteration 12/25 | Loss: 0.00069292
Iteration 13/25 | Loss: 0.00069292
Iteration 14/25 | Loss: 0.00069292
Iteration 15/25 | Loss: 0.00069292
Iteration 16/25 | Loss: 0.00069292
Iteration 17/25 | Loss: 0.00069292
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006929156952537596, 0.0006929156952537596, 0.0006929156952537596, 0.0006929156952537596, 0.0006929156952537596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006929156952537596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069292
Iteration 2/1000 | Loss: 0.00003257
Iteration 3/1000 | Loss: 0.00002303
Iteration 4/1000 | Loss: 0.00002027
Iteration 5/1000 | Loss: 0.00001886
Iteration 6/1000 | Loss: 0.00001790
Iteration 7/1000 | Loss: 0.00001724
Iteration 8/1000 | Loss: 0.00001686
Iteration 9/1000 | Loss: 0.00001654
Iteration 10/1000 | Loss: 0.00001632
Iteration 11/1000 | Loss: 0.00001630
Iteration 12/1000 | Loss: 0.00001619
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001607
Iteration 15/1000 | Loss: 0.00001606
Iteration 16/1000 | Loss: 0.00001604
Iteration 17/1000 | Loss: 0.00001604
Iteration 18/1000 | Loss: 0.00001603
Iteration 19/1000 | Loss: 0.00001603
Iteration 20/1000 | Loss: 0.00001602
Iteration 21/1000 | Loss: 0.00001601
Iteration 22/1000 | Loss: 0.00001601
Iteration 23/1000 | Loss: 0.00001600
Iteration 24/1000 | Loss: 0.00001600
Iteration 25/1000 | Loss: 0.00001599
Iteration 26/1000 | Loss: 0.00001598
Iteration 27/1000 | Loss: 0.00001598
Iteration 28/1000 | Loss: 0.00001598
Iteration 29/1000 | Loss: 0.00001597
Iteration 30/1000 | Loss: 0.00001597
Iteration 31/1000 | Loss: 0.00001597
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001597
Iteration 34/1000 | Loss: 0.00001596
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001596
Iteration 37/1000 | Loss: 0.00001595
Iteration 38/1000 | Loss: 0.00001594
Iteration 39/1000 | Loss: 0.00001593
Iteration 40/1000 | Loss: 0.00001592
Iteration 41/1000 | Loss: 0.00001592
Iteration 42/1000 | Loss: 0.00001591
Iteration 43/1000 | Loss: 0.00001591
Iteration 44/1000 | Loss: 0.00001590
Iteration 45/1000 | Loss: 0.00001590
Iteration 46/1000 | Loss: 0.00001590
Iteration 47/1000 | Loss: 0.00001588
Iteration 48/1000 | Loss: 0.00001587
Iteration 49/1000 | Loss: 0.00001587
Iteration 50/1000 | Loss: 0.00001587
Iteration 51/1000 | Loss: 0.00001586
Iteration 52/1000 | Loss: 0.00001586
Iteration 53/1000 | Loss: 0.00001586
Iteration 54/1000 | Loss: 0.00001585
Iteration 55/1000 | Loss: 0.00001585
Iteration 56/1000 | Loss: 0.00001584
Iteration 57/1000 | Loss: 0.00001584
Iteration 58/1000 | Loss: 0.00001583
Iteration 59/1000 | Loss: 0.00001583
Iteration 60/1000 | Loss: 0.00001582
Iteration 61/1000 | Loss: 0.00001582
Iteration 62/1000 | Loss: 0.00001581
Iteration 63/1000 | Loss: 0.00001581
Iteration 64/1000 | Loss: 0.00001581
Iteration 65/1000 | Loss: 0.00001581
Iteration 66/1000 | Loss: 0.00001581
Iteration 67/1000 | Loss: 0.00001581
Iteration 68/1000 | Loss: 0.00001581
Iteration 69/1000 | Loss: 0.00001581
Iteration 70/1000 | Loss: 0.00001580
Iteration 71/1000 | Loss: 0.00001580
Iteration 72/1000 | Loss: 0.00001580
Iteration 73/1000 | Loss: 0.00001580
Iteration 74/1000 | Loss: 0.00001580
Iteration 75/1000 | Loss: 0.00001580
Iteration 76/1000 | Loss: 0.00001580
Iteration 77/1000 | Loss: 0.00001579
Iteration 78/1000 | Loss: 0.00001579
Iteration 79/1000 | Loss: 0.00001579
Iteration 80/1000 | Loss: 0.00001579
Iteration 81/1000 | Loss: 0.00001578
Iteration 82/1000 | Loss: 0.00001578
Iteration 83/1000 | Loss: 0.00001578
Iteration 84/1000 | Loss: 0.00001578
Iteration 85/1000 | Loss: 0.00001578
Iteration 86/1000 | Loss: 0.00001577
Iteration 87/1000 | Loss: 0.00001577
Iteration 88/1000 | Loss: 0.00001577
Iteration 89/1000 | Loss: 0.00001576
Iteration 90/1000 | Loss: 0.00001576
Iteration 91/1000 | Loss: 0.00001576
Iteration 92/1000 | Loss: 0.00001576
Iteration 93/1000 | Loss: 0.00001576
Iteration 94/1000 | Loss: 0.00001576
Iteration 95/1000 | Loss: 0.00001576
Iteration 96/1000 | Loss: 0.00001575
Iteration 97/1000 | Loss: 0.00001575
Iteration 98/1000 | Loss: 0.00001575
Iteration 99/1000 | Loss: 0.00001574
Iteration 100/1000 | Loss: 0.00001574
Iteration 101/1000 | Loss: 0.00001574
Iteration 102/1000 | Loss: 0.00001574
Iteration 103/1000 | Loss: 0.00001574
Iteration 104/1000 | Loss: 0.00001574
Iteration 105/1000 | Loss: 0.00001574
Iteration 106/1000 | Loss: 0.00001574
Iteration 107/1000 | Loss: 0.00001574
Iteration 108/1000 | Loss: 0.00001574
Iteration 109/1000 | Loss: 0.00001574
Iteration 110/1000 | Loss: 0.00001574
Iteration 111/1000 | Loss: 0.00001574
Iteration 112/1000 | Loss: 0.00001573
Iteration 113/1000 | Loss: 0.00001573
Iteration 114/1000 | Loss: 0.00001573
Iteration 115/1000 | Loss: 0.00001573
Iteration 116/1000 | Loss: 0.00001573
Iteration 117/1000 | Loss: 0.00001572
Iteration 118/1000 | Loss: 0.00001572
Iteration 119/1000 | Loss: 0.00001571
Iteration 120/1000 | Loss: 0.00001571
Iteration 121/1000 | Loss: 0.00001571
Iteration 122/1000 | Loss: 0.00001571
Iteration 123/1000 | Loss: 0.00001570
Iteration 124/1000 | Loss: 0.00001570
Iteration 125/1000 | Loss: 0.00001569
Iteration 126/1000 | Loss: 0.00001569
Iteration 127/1000 | Loss: 0.00001569
Iteration 128/1000 | Loss: 0.00001569
Iteration 129/1000 | Loss: 0.00001568
Iteration 130/1000 | Loss: 0.00001568
Iteration 131/1000 | Loss: 0.00001568
Iteration 132/1000 | Loss: 0.00001568
Iteration 133/1000 | Loss: 0.00001568
Iteration 134/1000 | Loss: 0.00001568
Iteration 135/1000 | Loss: 0.00001568
Iteration 136/1000 | Loss: 0.00001568
Iteration 137/1000 | Loss: 0.00001568
Iteration 138/1000 | Loss: 0.00001567
Iteration 139/1000 | Loss: 0.00001567
Iteration 140/1000 | Loss: 0.00001567
Iteration 141/1000 | Loss: 0.00001567
Iteration 142/1000 | Loss: 0.00001567
Iteration 143/1000 | Loss: 0.00001567
Iteration 144/1000 | Loss: 0.00001566
Iteration 145/1000 | Loss: 0.00001566
Iteration 146/1000 | Loss: 0.00001566
Iteration 147/1000 | Loss: 0.00001566
Iteration 148/1000 | Loss: 0.00001566
Iteration 149/1000 | Loss: 0.00001566
Iteration 150/1000 | Loss: 0.00001566
Iteration 151/1000 | Loss: 0.00001566
Iteration 152/1000 | Loss: 0.00001566
Iteration 153/1000 | Loss: 0.00001566
Iteration 154/1000 | Loss: 0.00001566
Iteration 155/1000 | Loss: 0.00001566
Iteration 156/1000 | Loss: 0.00001566
Iteration 157/1000 | Loss: 0.00001566
Iteration 158/1000 | Loss: 0.00001566
Iteration 159/1000 | Loss: 0.00001566
Iteration 160/1000 | Loss: 0.00001566
Iteration 161/1000 | Loss: 0.00001566
Iteration 162/1000 | Loss: 0.00001566
Iteration 163/1000 | Loss: 0.00001566
Iteration 164/1000 | Loss: 0.00001566
Iteration 165/1000 | Loss: 0.00001566
Iteration 166/1000 | Loss: 0.00001566
Iteration 167/1000 | Loss: 0.00001566
Iteration 168/1000 | Loss: 0.00001566
Iteration 169/1000 | Loss: 0.00001566
Iteration 170/1000 | Loss: 0.00001566
Iteration 171/1000 | Loss: 0.00001566
Iteration 172/1000 | Loss: 0.00001566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.5659394193789922e-05, 1.5659394193789922e-05, 1.5659394193789922e-05, 1.5659394193789922e-05, 1.5659394193789922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5659394193789922e-05

Optimization complete. Final v2v error: 3.2810611724853516 mm

Highest mean error: 4.453640937805176 mm for frame 30

Lowest mean error: 2.5826756954193115 mm for frame 233

Saving results

Total time: 43.63500666618347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404653
Iteration 2/25 | Loss: 0.00103348
Iteration 3/25 | Loss: 0.00091487
Iteration 4/25 | Loss: 0.00090467
Iteration 5/25 | Loss: 0.00090090
Iteration 6/25 | Loss: 0.00089988
Iteration 7/25 | Loss: 0.00089988
Iteration 8/25 | Loss: 0.00089988
Iteration 9/25 | Loss: 0.00089988
Iteration 10/25 | Loss: 0.00089988
Iteration 11/25 | Loss: 0.00089988
Iteration 12/25 | Loss: 0.00089988
Iteration 13/25 | Loss: 0.00089988
Iteration 14/25 | Loss: 0.00089984
Iteration 15/25 | Loss: 0.00089984
Iteration 16/25 | Loss: 0.00089984
Iteration 17/25 | Loss: 0.00089984
Iteration 18/25 | Loss: 0.00089984
Iteration 19/25 | Loss: 0.00089984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000899840029887855, 0.000899840029887855, 0.000899840029887855, 0.000899840029887855, 0.000899840029887855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000899840029887855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31474376
Iteration 2/25 | Loss: 0.00069430
Iteration 3/25 | Loss: 0.00069430
Iteration 4/25 | Loss: 0.00069430
Iteration 5/25 | Loss: 0.00069430
Iteration 6/25 | Loss: 0.00069430
Iteration 7/25 | Loss: 0.00069430
Iteration 8/25 | Loss: 0.00069430
Iteration 9/25 | Loss: 0.00069430
Iteration 10/25 | Loss: 0.00069430
Iteration 11/25 | Loss: 0.00069430
Iteration 12/25 | Loss: 0.00069430
Iteration 13/25 | Loss: 0.00069430
Iteration 14/25 | Loss: 0.00069430
Iteration 15/25 | Loss: 0.00069430
Iteration 16/25 | Loss: 0.00069430
Iteration 17/25 | Loss: 0.00069430
Iteration 18/25 | Loss: 0.00069430
Iteration 19/25 | Loss: 0.00069430
Iteration 20/25 | Loss: 0.00069430
Iteration 21/25 | Loss: 0.00069430
Iteration 22/25 | Loss: 0.00069430
Iteration 23/25 | Loss: 0.00069430
Iteration 24/25 | Loss: 0.00069430
Iteration 25/25 | Loss: 0.00069430

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069430
Iteration 2/1000 | Loss: 0.00003218
Iteration 3/1000 | Loss: 0.00001834
Iteration 4/1000 | Loss: 0.00001433
Iteration 5/1000 | Loss: 0.00001309
Iteration 6/1000 | Loss: 0.00001236
Iteration 7/1000 | Loss: 0.00001189
Iteration 8/1000 | Loss: 0.00001163
Iteration 9/1000 | Loss: 0.00001160
Iteration 10/1000 | Loss: 0.00001133
Iteration 11/1000 | Loss: 0.00001103
Iteration 12/1000 | Loss: 0.00001097
Iteration 13/1000 | Loss: 0.00001087
Iteration 14/1000 | Loss: 0.00001070
Iteration 15/1000 | Loss: 0.00001066
Iteration 16/1000 | Loss: 0.00001063
Iteration 17/1000 | Loss: 0.00001063
Iteration 18/1000 | Loss: 0.00001062
Iteration 19/1000 | Loss: 0.00001062
Iteration 20/1000 | Loss: 0.00001061
Iteration 21/1000 | Loss: 0.00001059
Iteration 22/1000 | Loss: 0.00001058
Iteration 23/1000 | Loss: 0.00001058
Iteration 24/1000 | Loss: 0.00001057
Iteration 25/1000 | Loss: 0.00001057
Iteration 26/1000 | Loss: 0.00001056
Iteration 27/1000 | Loss: 0.00001056
Iteration 28/1000 | Loss: 0.00001055
Iteration 29/1000 | Loss: 0.00001055
Iteration 30/1000 | Loss: 0.00001054
Iteration 31/1000 | Loss: 0.00001053
Iteration 32/1000 | Loss: 0.00001053
Iteration 33/1000 | Loss: 0.00001053
Iteration 34/1000 | Loss: 0.00001052
Iteration 35/1000 | Loss: 0.00001052
Iteration 36/1000 | Loss: 0.00001052
Iteration 37/1000 | Loss: 0.00001051
Iteration 38/1000 | Loss: 0.00001051
Iteration 39/1000 | Loss: 0.00001051
Iteration 40/1000 | Loss: 0.00001051
Iteration 41/1000 | Loss: 0.00001050
Iteration 42/1000 | Loss: 0.00001050
Iteration 43/1000 | Loss: 0.00001050
Iteration 44/1000 | Loss: 0.00001050
Iteration 45/1000 | Loss: 0.00001050
Iteration 46/1000 | Loss: 0.00001050
Iteration 47/1000 | Loss: 0.00001049
Iteration 48/1000 | Loss: 0.00001049
Iteration 49/1000 | Loss: 0.00001049
Iteration 50/1000 | Loss: 0.00001049
Iteration 51/1000 | Loss: 0.00001048
Iteration 52/1000 | Loss: 0.00001048
Iteration 53/1000 | Loss: 0.00001048
Iteration 54/1000 | Loss: 0.00001048
Iteration 55/1000 | Loss: 0.00001048
Iteration 56/1000 | Loss: 0.00001048
Iteration 57/1000 | Loss: 0.00001048
Iteration 58/1000 | Loss: 0.00001047
Iteration 59/1000 | Loss: 0.00001047
Iteration 60/1000 | Loss: 0.00001047
Iteration 61/1000 | Loss: 0.00001047
Iteration 62/1000 | Loss: 0.00001046
Iteration 63/1000 | Loss: 0.00001046
Iteration 64/1000 | Loss: 0.00001046
Iteration 65/1000 | Loss: 0.00001046
Iteration 66/1000 | Loss: 0.00001046
Iteration 67/1000 | Loss: 0.00001046
Iteration 68/1000 | Loss: 0.00001046
Iteration 69/1000 | Loss: 0.00001046
Iteration 70/1000 | Loss: 0.00001046
Iteration 71/1000 | Loss: 0.00001046
Iteration 72/1000 | Loss: 0.00001045
Iteration 73/1000 | Loss: 0.00001045
Iteration 74/1000 | Loss: 0.00001045
Iteration 75/1000 | Loss: 0.00001045
Iteration 76/1000 | Loss: 0.00001045
Iteration 77/1000 | Loss: 0.00001045
Iteration 78/1000 | Loss: 0.00001045
Iteration 79/1000 | Loss: 0.00001045
Iteration 80/1000 | Loss: 0.00001045
Iteration 81/1000 | Loss: 0.00001045
Iteration 82/1000 | Loss: 0.00001045
Iteration 83/1000 | Loss: 0.00001045
Iteration 84/1000 | Loss: 0.00001045
Iteration 85/1000 | Loss: 0.00001044
Iteration 86/1000 | Loss: 0.00001044
Iteration 87/1000 | Loss: 0.00001044
Iteration 88/1000 | Loss: 0.00001044
Iteration 89/1000 | Loss: 0.00001044
Iteration 90/1000 | Loss: 0.00001044
Iteration 91/1000 | Loss: 0.00001044
Iteration 92/1000 | Loss: 0.00001044
Iteration 93/1000 | Loss: 0.00001044
Iteration 94/1000 | Loss: 0.00001044
Iteration 95/1000 | Loss: 0.00001044
Iteration 96/1000 | Loss: 0.00001044
Iteration 97/1000 | Loss: 0.00001044
Iteration 98/1000 | Loss: 0.00001044
Iteration 99/1000 | Loss: 0.00001044
Iteration 100/1000 | Loss: 0.00001044
Iteration 101/1000 | Loss: 0.00001044
Iteration 102/1000 | Loss: 0.00001044
Iteration 103/1000 | Loss: 0.00001044
Iteration 104/1000 | Loss: 0.00001044
Iteration 105/1000 | Loss: 0.00001044
Iteration 106/1000 | Loss: 0.00001044
Iteration 107/1000 | Loss: 0.00001043
Iteration 108/1000 | Loss: 0.00001043
Iteration 109/1000 | Loss: 0.00001043
Iteration 110/1000 | Loss: 0.00001043
Iteration 111/1000 | Loss: 0.00001043
Iteration 112/1000 | Loss: 0.00001043
Iteration 113/1000 | Loss: 0.00001043
Iteration 114/1000 | Loss: 0.00001043
Iteration 115/1000 | Loss: 0.00001043
Iteration 116/1000 | Loss: 0.00001043
Iteration 117/1000 | Loss: 0.00001043
Iteration 118/1000 | Loss: 0.00001043
Iteration 119/1000 | Loss: 0.00001043
Iteration 120/1000 | Loss: 0.00001043
Iteration 121/1000 | Loss: 0.00001043
Iteration 122/1000 | Loss: 0.00001043
Iteration 123/1000 | Loss: 0.00001042
Iteration 124/1000 | Loss: 0.00001042
Iteration 125/1000 | Loss: 0.00001042
Iteration 126/1000 | Loss: 0.00001042
Iteration 127/1000 | Loss: 0.00001042
Iteration 128/1000 | Loss: 0.00001042
Iteration 129/1000 | Loss: 0.00001041
Iteration 130/1000 | Loss: 0.00001041
Iteration 131/1000 | Loss: 0.00001041
Iteration 132/1000 | Loss: 0.00001041
Iteration 133/1000 | Loss: 0.00001041
Iteration 134/1000 | Loss: 0.00001041
Iteration 135/1000 | Loss: 0.00001041
Iteration 136/1000 | Loss: 0.00001041
Iteration 137/1000 | Loss: 0.00001041
Iteration 138/1000 | Loss: 0.00001041
Iteration 139/1000 | Loss: 0.00001041
Iteration 140/1000 | Loss: 0.00001040
Iteration 141/1000 | Loss: 0.00001040
Iteration 142/1000 | Loss: 0.00001040
Iteration 143/1000 | Loss: 0.00001040
Iteration 144/1000 | Loss: 0.00001040
Iteration 145/1000 | Loss: 0.00001040
Iteration 146/1000 | Loss: 0.00001040
Iteration 147/1000 | Loss: 0.00001040
Iteration 148/1000 | Loss: 0.00001040
Iteration 149/1000 | Loss: 0.00001040
Iteration 150/1000 | Loss: 0.00001040
Iteration 151/1000 | Loss: 0.00001040
Iteration 152/1000 | Loss: 0.00001040
Iteration 153/1000 | Loss: 0.00001040
Iteration 154/1000 | Loss: 0.00001040
Iteration 155/1000 | Loss: 0.00001040
Iteration 156/1000 | Loss: 0.00001040
Iteration 157/1000 | Loss: 0.00001039
Iteration 158/1000 | Loss: 0.00001039
Iteration 159/1000 | Loss: 0.00001039
Iteration 160/1000 | Loss: 0.00001039
Iteration 161/1000 | Loss: 0.00001039
Iteration 162/1000 | Loss: 0.00001039
Iteration 163/1000 | Loss: 0.00001039
Iteration 164/1000 | Loss: 0.00001039
Iteration 165/1000 | Loss: 0.00001039
Iteration 166/1000 | Loss: 0.00001039
Iteration 167/1000 | Loss: 0.00001039
Iteration 168/1000 | Loss: 0.00001039
Iteration 169/1000 | Loss: 0.00001039
Iteration 170/1000 | Loss: 0.00001039
Iteration 171/1000 | Loss: 0.00001039
Iteration 172/1000 | Loss: 0.00001039
Iteration 173/1000 | Loss: 0.00001039
Iteration 174/1000 | Loss: 0.00001039
Iteration 175/1000 | Loss: 0.00001039
Iteration 176/1000 | Loss: 0.00001039
Iteration 177/1000 | Loss: 0.00001039
Iteration 178/1000 | Loss: 0.00001038
Iteration 179/1000 | Loss: 0.00001038
Iteration 180/1000 | Loss: 0.00001038
Iteration 181/1000 | Loss: 0.00001038
Iteration 182/1000 | Loss: 0.00001038
Iteration 183/1000 | Loss: 0.00001038
Iteration 184/1000 | Loss: 0.00001038
Iteration 185/1000 | Loss: 0.00001038
Iteration 186/1000 | Loss: 0.00001038
Iteration 187/1000 | Loss: 0.00001038
Iteration 188/1000 | Loss: 0.00001038
Iteration 189/1000 | Loss: 0.00001038
Iteration 190/1000 | Loss: 0.00001038
Iteration 191/1000 | Loss: 0.00001038
Iteration 192/1000 | Loss: 0.00001038
Iteration 193/1000 | Loss: 0.00001038
Iteration 194/1000 | Loss: 0.00001038
Iteration 195/1000 | Loss: 0.00001038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.0384339475422166e-05, 1.0384339475422166e-05, 1.0384339475422166e-05, 1.0384339475422166e-05, 1.0384339475422166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0384339475422166e-05

Optimization complete. Final v2v error: 2.8042256832122803 mm

Highest mean error: 3.030367374420166 mm for frame 71

Lowest mean error: 2.530371904373169 mm for frame 8

Saving results

Total time: 37.20755076408386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813098
Iteration 2/25 | Loss: 0.00128108
Iteration 3/25 | Loss: 0.00099840
Iteration 4/25 | Loss: 0.00098215
Iteration 5/25 | Loss: 0.00097949
Iteration 6/25 | Loss: 0.00097949
Iteration 7/25 | Loss: 0.00097949
Iteration 8/25 | Loss: 0.00097949
Iteration 9/25 | Loss: 0.00097949
Iteration 10/25 | Loss: 0.00097949
Iteration 11/25 | Loss: 0.00097949
Iteration 12/25 | Loss: 0.00097949
Iteration 13/25 | Loss: 0.00097949
Iteration 14/25 | Loss: 0.00097949
Iteration 15/25 | Loss: 0.00097949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000979493255726993, 0.000979493255726993, 0.000979493255726993, 0.000979493255726993, 0.000979493255726993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000979493255726993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28555799
Iteration 2/25 | Loss: 0.00045162
Iteration 3/25 | Loss: 0.00045162
Iteration 4/25 | Loss: 0.00045162
Iteration 5/25 | Loss: 0.00045162
Iteration 6/25 | Loss: 0.00045162
Iteration 7/25 | Loss: 0.00045162
Iteration 8/25 | Loss: 0.00045162
Iteration 9/25 | Loss: 0.00045161
Iteration 10/25 | Loss: 0.00045161
Iteration 11/25 | Loss: 0.00045161
Iteration 12/25 | Loss: 0.00045161
Iteration 13/25 | Loss: 0.00045161
Iteration 14/25 | Loss: 0.00045161
Iteration 15/25 | Loss: 0.00045161
Iteration 16/25 | Loss: 0.00045161
Iteration 17/25 | Loss: 0.00045161
Iteration 18/25 | Loss: 0.00045161
Iteration 19/25 | Loss: 0.00045161
Iteration 20/25 | Loss: 0.00045161
Iteration 21/25 | Loss: 0.00045161
Iteration 22/25 | Loss: 0.00045161
Iteration 23/25 | Loss: 0.00045161
Iteration 24/25 | Loss: 0.00045161
Iteration 25/25 | Loss: 0.00045161

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045161
Iteration 2/1000 | Loss: 0.00003105
Iteration 3/1000 | Loss: 0.00002178
Iteration 4/1000 | Loss: 0.00002023
Iteration 5/1000 | Loss: 0.00001949
Iteration 6/1000 | Loss: 0.00001894
Iteration 7/1000 | Loss: 0.00001857
Iteration 8/1000 | Loss: 0.00001811
Iteration 9/1000 | Loss: 0.00001783
Iteration 10/1000 | Loss: 0.00001766
Iteration 11/1000 | Loss: 0.00001758
Iteration 12/1000 | Loss: 0.00001756
Iteration 13/1000 | Loss: 0.00001755
Iteration 14/1000 | Loss: 0.00001755
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001753
Iteration 17/1000 | Loss: 0.00001753
Iteration 18/1000 | Loss: 0.00001752
Iteration 19/1000 | Loss: 0.00001750
Iteration 20/1000 | Loss: 0.00001749
Iteration 21/1000 | Loss: 0.00001748
Iteration 22/1000 | Loss: 0.00001748
Iteration 23/1000 | Loss: 0.00001748
Iteration 24/1000 | Loss: 0.00001748
Iteration 25/1000 | Loss: 0.00001747
Iteration 26/1000 | Loss: 0.00001747
Iteration 27/1000 | Loss: 0.00001746
Iteration 28/1000 | Loss: 0.00001746
Iteration 29/1000 | Loss: 0.00001745
Iteration 30/1000 | Loss: 0.00001745
Iteration 31/1000 | Loss: 0.00001745
Iteration 32/1000 | Loss: 0.00001745
Iteration 33/1000 | Loss: 0.00001745
Iteration 34/1000 | Loss: 0.00001744
Iteration 35/1000 | Loss: 0.00001744
Iteration 36/1000 | Loss: 0.00001744
Iteration 37/1000 | Loss: 0.00001743
Iteration 38/1000 | Loss: 0.00001742
Iteration 39/1000 | Loss: 0.00001741
Iteration 40/1000 | Loss: 0.00001741
Iteration 41/1000 | Loss: 0.00001741
Iteration 42/1000 | Loss: 0.00001740
Iteration 43/1000 | Loss: 0.00001740
Iteration 44/1000 | Loss: 0.00001740
Iteration 45/1000 | Loss: 0.00001739
Iteration 46/1000 | Loss: 0.00001739
Iteration 47/1000 | Loss: 0.00001739
Iteration 48/1000 | Loss: 0.00001739
Iteration 49/1000 | Loss: 0.00001738
Iteration 50/1000 | Loss: 0.00001738
Iteration 51/1000 | Loss: 0.00001738
Iteration 52/1000 | Loss: 0.00001738
Iteration 53/1000 | Loss: 0.00001738
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001737
Iteration 56/1000 | Loss: 0.00001737
Iteration 57/1000 | Loss: 0.00001736
Iteration 58/1000 | Loss: 0.00001735
Iteration 59/1000 | Loss: 0.00001734
Iteration 60/1000 | Loss: 0.00001733
Iteration 61/1000 | Loss: 0.00001733
Iteration 62/1000 | Loss: 0.00001733
Iteration 63/1000 | Loss: 0.00001732
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001732
Iteration 66/1000 | Loss: 0.00001731
Iteration 67/1000 | Loss: 0.00001731
Iteration 68/1000 | Loss: 0.00001731
Iteration 69/1000 | Loss: 0.00001731
Iteration 70/1000 | Loss: 0.00001731
Iteration 71/1000 | Loss: 0.00001730
Iteration 72/1000 | Loss: 0.00001730
Iteration 73/1000 | Loss: 0.00001730
Iteration 74/1000 | Loss: 0.00001730
Iteration 75/1000 | Loss: 0.00001730
Iteration 76/1000 | Loss: 0.00001730
Iteration 77/1000 | Loss: 0.00001730
Iteration 78/1000 | Loss: 0.00001729
Iteration 79/1000 | Loss: 0.00001729
Iteration 80/1000 | Loss: 0.00001729
Iteration 81/1000 | Loss: 0.00001729
Iteration 82/1000 | Loss: 0.00001729
Iteration 83/1000 | Loss: 0.00001729
Iteration 84/1000 | Loss: 0.00001729
Iteration 85/1000 | Loss: 0.00001729
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001729
Iteration 89/1000 | Loss: 0.00001729
Iteration 90/1000 | Loss: 0.00001729
Iteration 91/1000 | Loss: 0.00001729
Iteration 92/1000 | Loss: 0.00001729
Iteration 93/1000 | Loss: 0.00001729
Iteration 94/1000 | Loss: 0.00001729
Iteration 95/1000 | Loss: 0.00001729
Iteration 96/1000 | Loss: 0.00001729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.728881943563465e-05, 1.728881943563465e-05, 1.728881943563465e-05, 1.728881943563465e-05, 1.728881943563465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.728881943563465e-05

Optimization complete. Final v2v error: 3.5308244228363037 mm

Highest mean error: 3.805720329284668 mm for frame 129

Lowest mean error: 3.1800761222839355 mm for frame 239

Saving results

Total time: 34.718536376953125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105894
Iteration 2/25 | Loss: 0.01105894
Iteration 3/25 | Loss: 0.01105894
Iteration 4/25 | Loss: 0.01105894
Iteration 5/25 | Loss: 0.01105894
Iteration 6/25 | Loss: 0.01105894
Iteration 7/25 | Loss: 0.01105894
Iteration 8/25 | Loss: 0.01105894
Iteration 9/25 | Loss: 0.01105894
Iteration 10/25 | Loss: 0.01105894
Iteration 11/25 | Loss: 0.01105894
Iteration 12/25 | Loss: 0.01105894
Iteration 13/25 | Loss: 0.01105893
Iteration 14/25 | Loss: 0.01105893
Iteration 15/25 | Loss: 0.01105893
Iteration 16/25 | Loss: 0.01105893
Iteration 17/25 | Loss: 0.01105893
Iteration 18/25 | Loss: 0.01105893
Iteration 19/25 | Loss: 0.01105893
Iteration 20/25 | Loss: 0.01105893
Iteration 21/25 | Loss: 0.01105893
Iteration 22/25 | Loss: 0.01105893
Iteration 23/25 | Loss: 0.01105892
Iteration 24/25 | Loss: 0.01105892
Iteration 25/25 | Loss: 0.01105892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68028045
Iteration 2/25 | Loss: 0.05772612
Iteration 3/25 | Loss: 0.05770713
Iteration 4/25 | Loss: 0.05770713
Iteration 5/25 | Loss: 0.05770713
Iteration 6/25 | Loss: 0.05770712
Iteration 7/25 | Loss: 0.05770712
Iteration 8/25 | Loss: 0.05770712
Iteration 9/25 | Loss: 0.05770711
Iteration 10/25 | Loss: 0.05770711
Iteration 11/25 | Loss: 0.05770711
Iteration 12/25 | Loss: 0.05770711
Iteration 13/25 | Loss: 0.05770711
Iteration 14/25 | Loss: 0.05770710
Iteration 15/25 | Loss: 0.05770710
Iteration 16/25 | Loss: 0.05770710
Iteration 17/25 | Loss: 0.05770710
Iteration 18/25 | Loss: 0.05770710
Iteration 19/25 | Loss: 0.05770711
Iteration 20/25 | Loss: 0.05770710
Iteration 21/25 | Loss: 0.05770710
Iteration 22/25 | Loss: 0.05770710
Iteration 23/25 | Loss: 0.05770710
Iteration 24/25 | Loss: 0.05770710
Iteration 25/25 | Loss: 0.05770711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05770711
Iteration 2/1000 | Loss: 0.00404473
Iteration 3/1000 | Loss: 0.00114465
Iteration 4/1000 | Loss: 0.00278523
Iteration 5/1000 | Loss: 0.00020016
Iteration 6/1000 | Loss: 0.00011083
Iteration 7/1000 | Loss: 0.00018447
Iteration 8/1000 | Loss: 0.00014300
Iteration 9/1000 | Loss: 0.00025924
Iteration 10/1000 | Loss: 0.00006698
Iteration 11/1000 | Loss: 0.00010626
Iteration 12/1000 | Loss: 0.00003809
Iteration 13/1000 | Loss: 0.00021264
Iteration 14/1000 | Loss: 0.00010988
Iteration 15/1000 | Loss: 0.00029194
Iteration 16/1000 | Loss: 0.00062300
Iteration 17/1000 | Loss: 0.00004596
Iteration 18/1000 | Loss: 0.00024585
Iteration 19/1000 | Loss: 0.00010061
Iteration 20/1000 | Loss: 0.00002773
Iteration 21/1000 | Loss: 0.00002599
Iteration 22/1000 | Loss: 0.00009174
Iteration 23/1000 | Loss: 0.00002436
Iteration 24/1000 | Loss: 0.00008606
Iteration 25/1000 | Loss: 0.00020053
Iteration 26/1000 | Loss: 0.00050592
Iteration 27/1000 | Loss: 0.00164829
Iteration 28/1000 | Loss: 0.00068232
Iteration 29/1000 | Loss: 0.00034810
Iteration 30/1000 | Loss: 0.00018880
Iteration 31/1000 | Loss: 0.00004199
Iteration 32/1000 | Loss: 0.00004761
Iteration 33/1000 | Loss: 0.00002206
Iteration 34/1000 | Loss: 0.00002210
Iteration 35/1000 | Loss: 0.00017552
Iteration 36/1000 | Loss: 0.00002850
Iteration 37/1000 | Loss: 0.00004254
Iteration 38/1000 | Loss: 0.00002044
Iteration 39/1000 | Loss: 0.00004949
Iteration 40/1000 | Loss: 0.00004113
Iteration 41/1000 | Loss: 0.00023433
Iteration 42/1000 | Loss: 0.00003671
Iteration 43/1000 | Loss: 0.00001905
Iteration 44/1000 | Loss: 0.00001881
Iteration 45/1000 | Loss: 0.00005231
Iteration 46/1000 | Loss: 0.00001837
Iteration 47/1000 | Loss: 0.00038248
Iteration 48/1000 | Loss: 0.00008480
Iteration 49/1000 | Loss: 0.00005080
Iteration 50/1000 | Loss: 0.00009574
Iteration 51/1000 | Loss: 0.00002170
Iteration 52/1000 | Loss: 0.00001792
Iteration 53/1000 | Loss: 0.00001767
Iteration 54/1000 | Loss: 0.00001757
Iteration 55/1000 | Loss: 0.00001756
Iteration 56/1000 | Loss: 0.00001736
Iteration 57/1000 | Loss: 0.00001725
Iteration 58/1000 | Loss: 0.00001724
Iteration 59/1000 | Loss: 0.00001712
Iteration 60/1000 | Loss: 0.00001741
Iteration 61/1000 | Loss: 0.00004853
Iteration 62/1000 | Loss: 0.00015766
Iteration 63/1000 | Loss: 0.00003220
Iteration 64/1000 | Loss: 0.00004846
Iteration 65/1000 | Loss: 0.00001719
Iteration 66/1000 | Loss: 0.00001707
Iteration 67/1000 | Loss: 0.00004093
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001703
Iteration 70/1000 | Loss: 0.00001701
Iteration 71/1000 | Loss: 0.00001700
Iteration 72/1000 | Loss: 0.00001700
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001699
Iteration 75/1000 | Loss: 0.00001699
Iteration 76/1000 | Loss: 0.00001728
Iteration 77/1000 | Loss: 0.00001728
Iteration 78/1000 | Loss: 0.00001706
Iteration 79/1000 | Loss: 0.00001704
Iteration 80/1000 | Loss: 0.00001703
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001698
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001697
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001697
Iteration 90/1000 | Loss: 0.00001697
Iteration 91/1000 | Loss: 0.00001697
Iteration 92/1000 | Loss: 0.00001697
Iteration 93/1000 | Loss: 0.00001696
Iteration 94/1000 | Loss: 0.00001696
Iteration 95/1000 | Loss: 0.00001696
Iteration 96/1000 | Loss: 0.00001696
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001696
Iteration 99/1000 | Loss: 0.00001696
Iteration 100/1000 | Loss: 0.00001695
Iteration 101/1000 | Loss: 0.00001695
Iteration 102/1000 | Loss: 0.00001695
Iteration 103/1000 | Loss: 0.00001695
Iteration 104/1000 | Loss: 0.00001695
Iteration 105/1000 | Loss: 0.00001695
Iteration 106/1000 | Loss: 0.00001695
Iteration 107/1000 | Loss: 0.00001695
Iteration 108/1000 | Loss: 0.00001695
Iteration 109/1000 | Loss: 0.00001695
Iteration 110/1000 | Loss: 0.00001695
Iteration 111/1000 | Loss: 0.00001695
Iteration 112/1000 | Loss: 0.00001695
Iteration 113/1000 | Loss: 0.00001695
Iteration 114/1000 | Loss: 0.00001694
Iteration 115/1000 | Loss: 0.00001722
Iteration 116/1000 | Loss: 0.00001721
Iteration 117/1000 | Loss: 0.00001721
Iteration 118/1000 | Loss: 0.00001699
Iteration 119/1000 | Loss: 0.00001697
Iteration 120/1000 | Loss: 0.00001696
Iteration 121/1000 | Loss: 0.00001695
Iteration 122/1000 | Loss: 0.00001694
Iteration 123/1000 | Loss: 0.00001694
Iteration 124/1000 | Loss: 0.00001694
Iteration 125/1000 | Loss: 0.00001694
Iteration 126/1000 | Loss: 0.00001694
Iteration 127/1000 | Loss: 0.00001693
Iteration 128/1000 | Loss: 0.00001693
Iteration 129/1000 | Loss: 0.00001717
Iteration 130/1000 | Loss: 0.00001717
Iteration 131/1000 | Loss: 0.00004995
Iteration 132/1000 | Loss: 0.00012757
Iteration 133/1000 | Loss: 0.00001974
Iteration 134/1000 | Loss: 0.00003640
Iteration 135/1000 | Loss: 0.00001704
Iteration 136/1000 | Loss: 0.00007607
Iteration 137/1000 | Loss: 0.00001730
Iteration 138/1000 | Loss: 0.00008617
Iteration 139/1000 | Loss: 0.00003190
Iteration 140/1000 | Loss: 0.00002463
Iteration 141/1000 | Loss: 0.00001724
Iteration 142/1000 | Loss: 0.00001707
Iteration 143/1000 | Loss: 0.00001700
Iteration 144/1000 | Loss: 0.00001699
Iteration 145/1000 | Loss: 0.00001699
Iteration 146/1000 | Loss: 0.00001698
Iteration 147/1000 | Loss: 0.00001698
Iteration 148/1000 | Loss: 0.00001697
Iteration 149/1000 | Loss: 0.00001718
Iteration 150/1000 | Loss: 0.00001717
Iteration 151/1000 | Loss: 0.00001705
Iteration 152/1000 | Loss: 0.00001704
Iteration 153/1000 | Loss: 0.00001695
Iteration 154/1000 | Loss: 0.00001695
Iteration 155/1000 | Loss: 0.00001695
Iteration 156/1000 | Loss: 0.00001695
Iteration 157/1000 | Loss: 0.00001695
Iteration 158/1000 | Loss: 0.00001695
Iteration 159/1000 | Loss: 0.00001694
Iteration 160/1000 | Loss: 0.00001694
Iteration 161/1000 | Loss: 0.00001694
Iteration 162/1000 | Loss: 0.00001711
Iteration 163/1000 | Loss: 0.00001702
Iteration 164/1000 | Loss: 0.00001702
Iteration 165/1000 | Loss: 0.00001695
Iteration 166/1000 | Loss: 0.00001695
Iteration 167/1000 | Loss: 0.00001695
Iteration 168/1000 | Loss: 0.00001694
Iteration 169/1000 | Loss: 0.00001694
Iteration 170/1000 | Loss: 0.00001693
Iteration 171/1000 | Loss: 0.00001708
Iteration 172/1000 | Loss: 0.00001701
Iteration 173/1000 | Loss: 0.00001701
Iteration 174/1000 | Loss: 0.00001693
Iteration 175/1000 | Loss: 0.00001709
Iteration 176/1000 | Loss: 0.00001709
Iteration 177/1000 | Loss: 0.00001701
Iteration 178/1000 | Loss: 0.00001692
Iteration 179/1000 | Loss: 0.00001692
Iteration 180/1000 | Loss: 0.00001692
Iteration 181/1000 | Loss: 0.00001692
Iteration 182/1000 | Loss: 0.00001692
Iteration 183/1000 | Loss: 0.00001708
Iteration 184/1000 | Loss: 0.00001700
Iteration 185/1000 | Loss: 0.00001700
Iteration 186/1000 | Loss: 0.00001699
Iteration 187/1000 | Loss: 0.00001693
Iteration 188/1000 | Loss: 0.00001692
Iteration 189/1000 | Loss: 0.00001708
Iteration 190/1000 | Loss: 0.00001708
Iteration 191/1000 | Loss: 0.00001694
Iteration 192/1000 | Loss: 0.00001692
Iteration 193/1000 | Loss: 0.00001692
Iteration 194/1000 | Loss: 0.00001692
Iteration 195/1000 | Loss: 0.00001692
Iteration 196/1000 | Loss: 0.00001692
Iteration 197/1000 | Loss: 0.00001692
Iteration 198/1000 | Loss: 0.00001692
Iteration 199/1000 | Loss: 0.00001692
Iteration 200/1000 | Loss: 0.00001692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.6920188500080258e-05, 1.6920188500080258e-05, 1.6920188500080258e-05, 1.6920188500080258e-05, 1.6920188500080258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6920188500080258e-05

Optimization complete. Final v2v error: 3.077444553375244 mm

Highest mean error: 21.357213973999023 mm for frame 223

Lowest mean error: 2.642519235610962 mm for frame 251

Saving results

Total time: 154.52897691726685
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846191
Iteration 2/25 | Loss: 0.00120739
Iteration 3/25 | Loss: 0.00102916
Iteration 4/25 | Loss: 0.00101859
Iteration 5/25 | Loss: 0.00101031
Iteration 6/25 | Loss: 0.00100895
Iteration 7/25 | Loss: 0.00101623
Iteration 8/25 | Loss: 0.00100810
Iteration 9/25 | Loss: 0.00100628
Iteration 10/25 | Loss: 0.00100593
Iteration 11/25 | Loss: 0.00100592
Iteration 12/25 | Loss: 0.00100592
Iteration 13/25 | Loss: 0.00100592
Iteration 14/25 | Loss: 0.00100592
Iteration 15/25 | Loss: 0.00100592
Iteration 16/25 | Loss: 0.00100591
Iteration 17/25 | Loss: 0.00100591
Iteration 18/25 | Loss: 0.00100591
Iteration 19/25 | Loss: 0.00100591
Iteration 20/25 | Loss: 0.00100591
Iteration 21/25 | Loss: 0.00100591
Iteration 22/25 | Loss: 0.00100591
Iteration 23/25 | Loss: 0.00100591
Iteration 24/25 | Loss: 0.00100591
Iteration 25/25 | Loss: 0.00100591

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.29837656
Iteration 2/25 | Loss: 0.00057394
Iteration 3/25 | Loss: 0.00057390
Iteration 4/25 | Loss: 0.00057390
Iteration 5/25 | Loss: 0.00057390
Iteration 6/25 | Loss: 0.00057390
Iteration 7/25 | Loss: 0.00057390
Iteration 8/25 | Loss: 0.00057390
Iteration 9/25 | Loss: 0.00057390
Iteration 10/25 | Loss: 0.00057390
Iteration 11/25 | Loss: 0.00057390
Iteration 12/25 | Loss: 0.00057390
Iteration 13/25 | Loss: 0.00057390
Iteration 14/25 | Loss: 0.00057390
Iteration 15/25 | Loss: 0.00057390
Iteration 16/25 | Loss: 0.00057390
Iteration 17/25 | Loss: 0.00057390
Iteration 18/25 | Loss: 0.00057390
Iteration 19/25 | Loss: 0.00057390
Iteration 20/25 | Loss: 0.00057390
Iteration 21/25 | Loss: 0.00057390
Iteration 22/25 | Loss: 0.00057390
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000573899713344872, 0.000573899713344872, 0.000573899713344872, 0.000573899713344872, 0.000573899713344872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000573899713344872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057390
Iteration 2/1000 | Loss: 0.00005762
Iteration 3/1000 | Loss: 0.00003247
Iteration 4/1000 | Loss: 0.00002751
Iteration 5/1000 | Loss: 0.00002458
Iteration 6/1000 | Loss: 0.00002356
Iteration 7/1000 | Loss: 0.00002296
Iteration 8/1000 | Loss: 0.00002245
Iteration 9/1000 | Loss: 0.00002209
Iteration 10/1000 | Loss: 0.00002185
Iteration 11/1000 | Loss: 0.00002167
Iteration 12/1000 | Loss: 0.00002155
Iteration 13/1000 | Loss: 0.00002148
Iteration 14/1000 | Loss: 0.00002148
Iteration 15/1000 | Loss: 0.00002145
Iteration 16/1000 | Loss: 0.00002140
Iteration 17/1000 | Loss: 0.00002129
Iteration 18/1000 | Loss: 0.00002126
Iteration 19/1000 | Loss: 0.00002122
Iteration 20/1000 | Loss: 0.00002120
Iteration 21/1000 | Loss: 0.00002119
Iteration 22/1000 | Loss: 0.00002119
Iteration 23/1000 | Loss: 0.00002119
Iteration 24/1000 | Loss: 0.00002119
Iteration 25/1000 | Loss: 0.00002118
Iteration 26/1000 | Loss: 0.00002117
Iteration 27/1000 | Loss: 0.00002117
Iteration 28/1000 | Loss: 0.00002117
Iteration 29/1000 | Loss: 0.00002116
Iteration 30/1000 | Loss: 0.00002116
Iteration 31/1000 | Loss: 0.00002116
Iteration 32/1000 | Loss: 0.00002115
Iteration 33/1000 | Loss: 0.00002115
Iteration 34/1000 | Loss: 0.00002115
Iteration 35/1000 | Loss: 0.00002115
Iteration 36/1000 | Loss: 0.00002114
Iteration 37/1000 | Loss: 0.00002114
Iteration 38/1000 | Loss: 0.00002113
Iteration 39/1000 | Loss: 0.00002113
Iteration 40/1000 | Loss: 0.00002113
Iteration 41/1000 | Loss: 0.00002112
Iteration 42/1000 | Loss: 0.00002112
Iteration 43/1000 | Loss: 0.00002112
Iteration 44/1000 | Loss: 0.00002111
Iteration 45/1000 | Loss: 0.00002110
Iteration 46/1000 | Loss: 0.00002110
Iteration 47/1000 | Loss: 0.00002109
Iteration 48/1000 | Loss: 0.00002109
Iteration 49/1000 | Loss: 0.00002109
Iteration 50/1000 | Loss: 0.00002109
Iteration 51/1000 | Loss: 0.00002109
Iteration 52/1000 | Loss: 0.00002109
Iteration 53/1000 | Loss: 0.00002109
Iteration 54/1000 | Loss: 0.00002109
Iteration 55/1000 | Loss: 0.00002109
Iteration 56/1000 | Loss: 0.00002108
Iteration 57/1000 | Loss: 0.00002108
Iteration 58/1000 | Loss: 0.00002108
Iteration 59/1000 | Loss: 0.00002108
Iteration 60/1000 | Loss: 0.00002108
Iteration 61/1000 | Loss: 0.00002108
Iteration 62/1000 | Loss: 0.00002108
Iteration 63/1000 | Loss: 0.00002108
Iteration 64/1000 | Loss: 0.00002108
Iteration 65/1000 | Loss: 0.00002107
Iteration 66/1000 | Loss: 0.00002107
Iteration 67/1000 | Loss: 0.00002107
Iteration 68/1000 | Loss: 0.00002107
Iteration 69/1000 | Loss: 0.00002107
Iteration 70/1000 | Loss: 0.00002106
Iteration 71/1000 | Loss: 0.00002106
Iteration 72/1000 | Loss: 0.00002106
Iteration 73/1000 | Loss: 0.00002105
Iteration 74/1000 | Loss: 0.00002104
Iteration 75/1000 | Loss: 0.00002104
Iteration 76/1000 | Loss: 0.00002103
Iteration 77/1000 | Loss: 0.00002102
Iteration 78/1000 | Loss: 0.00002102
Iteration 79/1000 | Loss: 0.00002101
Iteration 80/1000 | Loss: 0.00002101
Iteration 81/1000 | Loss: 0.00002101
Iteration 82/1000 | Loss: 0.00002101
Iteration 83/1000 | Loss: 0.00002101
Iteration 84/1000 | Loss: 0.00002101
Iteration 85/1000 | Loss: 0.00002101
Iteration 86/1000 | Loss: 0.00002101
Iteration 87/1000 | Loss: 0.00002100
Iteration 88/1000 | Loss: 0.00002100
Iteration 89/1000 | Loss: 0.00002100
Iteration 90/1000 | Loss: 0.00002099
Iteration 91/1000 | Loss: 0.00002099
Iteration 92/1000 | Loss: 0.00002099
Iteration 93/1000 | Loss: 0.00002098
Iteration 94/1000 | Loss: 0.00002098
Iteration 95/1000 | Loss: 0.00002098
Iteration 96/1000 | Loss: 0.00002098
Iteration 97/1000 | Loss: 0.00002098
Iteration 98/1000 | Loss: 0.00002098
Iteration 99/1000 | Loss: 0.00002098
Iteration 100/1000 | Loss: 0.00002098
Iteration 101/1000 | Loss: 0.00002097
Iteration 102/1000 | Loss: 0.00002097
Iteration 103/1000 | Loss: 0.00002097
Iteration 104/1000 | Loss: 0.00002097
Iteration 105/1000 | Loss: 0.00002097
Iteration 106/1000 | Loss: 0.00002097
Iteration 107/1000 | Loss: 0.00002097
Iteration 108/1000 | Loss: 0.00002097
Iteration 109/1000 | Loss: 0.00002096
Iteration 110/1000 | Loss: 0.00002096
Iteration 111/1000 | Loss: 0.00002095
Iteration 112/1000 | Loss: 0.00002095
Iteration 113/1000 | Loss: 0.00002095
Iteration 114/1000 | Loss: 0.00002095
Iteration 115/1000 | Loss: 0.00002095
Iteration 116/1000 | Loss: 0.00002095
Iteration 117/1000 | Loss: 0.00002095
Iteration 118/1000 | Loss: 0.00002094
Iteration 119/1000 | Loss: 0.00002094
Iteration 120/1000 | Loss: 0.00002094
Iteration 121/1000 | Loss: 0.00002094
Iteration 122/1000 | Loss: 0.00002094
Iteration 123/1000 | Loss: 0.00002094
Iteration 124/1000 | Loss: 0.00002094
Iteration 125/1000 | Loss: 0.00002094
Iteration 126/1000 | Loss: 0.00002094
Iteration 127/1000 | Loss: 0.00002094
Iteration 128/1000 | Loss: 0.00002094
Iteration 129/1000 | Loss: 0.00002094
Iteration 130/1000 | Loss: 0.00002093
Iteration 131/1000 | Loss: 0.00002093
Iteration 132/1000 | Loss: 0.00002090
Iteration 133/1000 | Loss: 0.00002090
Iteration 134/1000 | Loss: 0.00002089
Iteration 135/1000 | Loss: 0.00002089
Iteration 136/1000 | Loss: 0.00002088
Iteration 137/1000 | Loss: 0.00002088
Iteration 138/1000 | Loss: 0.00002088
Iteration 139/1000 | Loss: 0.00002087
Iteration 140/1000 | Loss: 0.00002087
Iteration 141/1000 | Loss: 0.00002087
Iteration 142/1000 | Loss: 0.00002087
Iteration 143/1000 | Loss: 0.00002086
Iteration 144/1000 | Loss: 0.00002086
Iteration 145/1000 | Loss: 0.00002086
Iteration 146/1000 | Loss: 0.00002086
Iteration 147/1000 | Loss: 0.00002086
Iteration 148/1000 | Loss: 0.00002086
Iteration 149/1000 | Loss: 0.00002086
Iteration 150/1000 | Loss: 0.00002086
Iteration 151/1000 | Loss: 0.00002086
Iteration 152/1000 | Loss: 0.00002085
Iteration 153/1000 | Loss: 0.00002085
Iteration 154/1000 | Loss: 0.00002085
Iteration 155/1000 | Loss: 0.00002085
Iteration 156/1000 | Loss: 0.00002085
Iteration 157/1000 | Loss: 0.00002085
Iteration 158/1000 | Loss: 0.00002084
Iteration 159/1000 | Loss: 0.00002084
Iteration 160/1000 | Loss: 0.00002083
Iteration 161/1000 | Loss: 0.00002083
Iteration 162/1000 | Loss: 0.00002083
Iteration 163/1000 | Loss: 0.00002082
Iteration 164/1000 | Loss: 0.00002082
Iteration 165/1000 | Loss: 0.00002081
Iteration 166/1000 | Loss: 0.00002081
Iteration 167/1000 | Loss: 0.00002081
Iteration 168/1000 | Loss: 0.00002081
Iteration 169/1000 | Loss: 0.00002081
Iteration 170/1000 | Loss: 0.00002081
Iteration 171/1000 | Loss: 0.00002081
Iteration 172/1000 | Loss: 0.00002081
Iteration 173/1000 | Loss: 0.00002081
Iteration 174/1000 | Loss: 0.00002080
Iteration 175/1000 | Loss: 0.00002080
Iteration 176/1000 | Loss: 0.00002080
Iteration 177/1000 | Loss: 0.00002080
Iteration 178/1000 | Loss: 0.00002080
Iteration 179/1000 | Loss: 0.00002080
Iteration 180/1000 | Loss: 0.00002080
Iteration 181/1000 | Loss: 0.00002079
Iteration 182/1000 | Loss: 0.00002079
Iteration 183/1000 | Loss: 0.00002079
Iteration 184/1000 | Loss: 0.00002079
Iteration 185/1000 | Loss: 0.00002079
Iteration 186/1000 | Loss: 0.00002079
Iteration 187/1000 | Loss: 0.00002079
Iteration 188/1000 | Loss: 0.00002079
Iteration 189/1000 | Loss: 0.00002079
Iteration 190/1000 | Loss: 0.00002078
Iteration 191/1000 | Loss: 0.00002078
Iteration 192/1000 | Loss: 0.00002078
Iteration 193/1000 | Loss: 0.00002078
Iteration 194/1000 | Loss: 0.00002078
Iteration 195/1000 | Loss: 0.00002078
Iteration 196/1000 | Loss: 0.00002078
Iteration 197/1000 | Loss: 0.00002078
Iteration 198/1000 | Loss: 0.00002077
Iteration 199/1000 | Loss: 0.00002077
Iteration 200/1000 | Loss: 0.00002077
Iteration 201/1000 | Loss: 0.00002077
Iteration 202/1000 | Loss: 0.00002077
Iteration 203/1000 | Loss: 0.00002077
Iteration 204/1000 | Loss: 0.00002076
Iteration 205/1000 | Loss: 0.00002076
Iteration 206/1000 | Loss: 0.00002076
Iteration 207/1000 | Loss: 0.00002076
Iteration 208/1000 | Loss: 0.00002076
Iteration 209/1000 | Loss: 0.00002076
Iteration 210/1000 | Loss: 0.00002076
Iteration 211/1000 | Loss: 0.00002075
Iteration 212/1000 | Loss: 0.00002075
Iteration 213/1000 | Loss: 0.00002075
Iteration 214/1000 | Loss: 0.00002075
Iteration 215/1000 | Loss: 0.00002075
Iteration 216/1000 | Loss: 0.00002075
Iteration 217/1000 | Loss: 0.00002075
Iteration 218/1000 | Loss: 0.00002075
Iteration 219/1000 | Loss: 0.00002075
Iteration 220/1000 | Loss: 0.00002075
Iteration 221/1000 | Loss: 0.00002075
Iteration 222/1000 | Loss: 0.00002075
Iteration 223/1000 | Loss: 0.00002075
Iteration 224/1000 | Loss: 0.00002075
Iteration 225/1000 | Loss: 0.00002074
Iteration 226/1000 | Loss: 0.00002074
Iteration 227/1000 | Loss: 0.00002074
Iteration 228/1000 | Loss: 0.00002074
Iteration 229/1000 | Loss: 0.00002074
Iteration 230/1000 | Loss: 0.00002074
Iteration 231/1000 | Loss: 0.00002074
Iteration 232/1000 | Loss: 0.00002074
Iteration 233/1000 | Loss: 0.00002074
Iteration 234/1000 | Loss: 0.00002073
Iteration 235/1000 | Loss: 0.00002073
Iteration 236/1000 | Loss: 0.00002073
Iteration 237/1000 | Loss: 0.00002073
Iteration 238/1000 | Loss: 0.00002073
Iteration 239/1000 | Loss: 0.00002073
Iteration 240/1000 | Loss: 0.00002073
Iteration 241/1000 | Loss: 0.00002073
Iteration 242/1000 | Loss: 0.00002073
Iteration 243/1000 | Loss: 0.00002073
Iteration 244/1000 | Loss: 0.00002073
Iteration 245/1000 | Loss: 0.00002073
Iteration 246/1000 | Loss: 0.00002073
Iteration 247/1000 | Loss: 0.00002073
Iteration 248/1000 | Loss: 0.00002073
Iteration 249/1000 | Loss: 0.00002073
Iteration 250/1000 | Loss: 0.00002073
Iteration 251/1000 | Loss: 0.00002073
Iteration 252/1000 | Loss: 0.00002073
Iteration 253/1000 | Loss: 0.00002073
Iteration 254/1000 | Loss: 0.00002073
Iteration 255/1000 | Loss: 0.00002073
Iteration 256/1000 | Loss: 0.00002073
Iteration 257/1000 | Loss: 0.00002073
Iteration 258/1000 | Loss: 0.00002073
Iteration 259/1000 | Loss: 0.00002073
Iteration 260/1000 | Loss: 0.00002073
Iteration 261/1000 | Loss: 0.00002073
Iteration 262/1000 | Loss: 0.00002073
Iteration 263/1000 | Loss: 0.00002073
Iteration 264/1000 | Loss: 0.00002073
Iteration 265/1000 | Loss: 0.00002073
Iteration 266/1000 | Loss: 0.00002073
Iteration 267/1000 | Loss: 0.00002073
Iteration 268/1000 | Loss: 0.00002073
Iteration 269/1000 | Loss: 0.00002073
Iteration 270/1000 | Loss: 0.00002073
Iteration 271/1000 | Loss: 0.00002073
Iteration 272/1000 | Loss: 0.00002073
Iteration 273/1000 | Loss: 0.00002073
Iteration 274/1000 | Loss: 0.00002073
Iteration 275/1000 | Loss: 0.00002073
Iteration 276/1000 | Loss: 0.00002073
Iteration 277/1000 | Loss: 0.00002073
Iteration 278/1000 | Loss: 0.00002073
Iteration 279/1000 | Loss: 0.00002073
Iteration 280/1000 | Loss: 0.00002073
Iteration 281/1000 | Loss: 0.00002073
Iteration 282/1000 | Loss: 0.00002073
Iteration 283/1000 | Loss: 0.00002073
Iteration 284/1000 | Loss: 0.00002073
Iteration 285/1000 | Loss: 0.00002073
Iteration 286/1000 | Loss: 0.00002073
Iteration 287/1000 | Loss: 0.00002073
Iteration 288/1000 | Loss: 0.00002073
Iteration 289/1000 | Loss: 0.00002073
Iteration 290/1000 | Loss: 0.00002073
Iteration 291/1000 | Loss: 0.00002073
Iteration 292/1000 | Loss: 0.00002073
Iteration 293/1000 | Loss: 0.00002073
Iteration 294/1000 | Loss: 0.00002073
Iteration 295/1000 | Loss: 0.00002073
Iteration 296/1000 | Loss: 0.00002073
Iteration 297/1000 | Loss: 0.00002073
Iteration 298/1000 | Loss: 0.00002073
Iteration 299/1000 | Loss: 0.00002073
Iteration 300/1000 | Loss: 0.00002073
Iteration 301/1000 | Loss: 0.00002073
Iteration 302/1000 | Loss: 0.00002073
Iteration 303/1000 | Loss: 0.00002073
Iteration 304/1000 | Loss: 0.00002073
Iteration 305/1000 | Loss: 0.00002073
Iteration 306/1000 | Loss: 0.00002073
Iteration 307/1000 | Loss: 0.00002073
Iteration 308/1000 | Loss: 0.00002073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 308. Stopping optimization.
Last 5 losses: [2.0725296053569764e-05, 2.0725296053569764e-05, 2.0725296053569764e-05, 2.0725296053569764e-05, 2.0725296053569764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0725296053569764e-05

Optimization complete. Final v2v error: 3.868992805480957 mm

Highest mean error: 4.479585647583008 mm for frame 94

Lowest mean error: 3.2953741550445557 mm for frame 4

Saving results

Total time: 56.31353449821472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746487
Iteration 2/25 | Loss: 0.00122669
Iteration 3/25 | Loss: 0.00103582
Iteration 4/25 | Loss: 0.00102345
Iteration 5/25 | Loss: 0.00101888
Iteration 6/25 | Loss: 0.00101765
Iteration 7/25 | Loss: 0.00101765
Iteration 8/25 | Loss: 0.00101765
Iteration 9/25 | Loss: 0.00101765
Iteration 10/25 | Loss: 0.00101765
Iteration 11/25 | Loss: 0.00101765
Iteration 12/25 | Loss: 0.00101765
Iteration 13/25 | Loss: 0.00101765
Iteration 14/25 | Loss: 0.00101765
Iteration 15/25 | Loss: 0.00101765
Iteration 16/25 | Loss: 0.00101765
Iteration 17/25 | Loss: 0.00101765
Iteration 18/25 | Loss: 0.00101765
Iteration 19/25 | Loss: 0.00101765
Iteration 20/25 | Loss: 0.00101765
Iteration 21/25 | Loss: 0.00101765
Iteration 22/25 | Loss: 0.00101765
Iteration 23/25 | Loss: 0.00101765
Iteration 24/25 | Loss: 0.00101765
Iteration 25/25 | Loss: 0.00101765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03129339
Iteration 2/25 | Loss: 0.00093829
Iteration 3/25 | Loss: 0.00093829
Iteration 4/25 | Loss: 0.00093829
Iteration 5/25 | Loss: 0.00093829
Iteration 6/25 | Loss: 0.00093829
Iteration 7/25 | Loss: 0.00093829
Iteration 8/25 | Loss: 0.00093829
Iteration 9/25 | Loss: 0.00093828
Iteration 10/25 | Loss: 0.00093828
Iteration 11/25 | Loss: 0.00093828
Iteration 12/25 | Loss: 0.00093828
Iteration 13/25 | Loss: 0.00093828
Iteration 14/25 | Loss: 0.00093828
Iteration 15/25 | Loss: 0.00093828
Iteration 16/25 | Loss: 0.00093828
Iteration 17/25 | Loss: 0.00093828
Iteration 18/25 | Loss: 0.00093828
Iteration 19/25 | Loss: 0.00093828
Iteration 20/25 | Loss: 0.00093828
Iteration 21/25 | Loss: 0.00093828
Iteration 22/25 | Loss: 0.00093828
Iteration 23/25 | Loss: 0.00093828
Iteration 24/25 | Loss: 0.00093828
Iteration 25/25 | Loss: 0.00093828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093828
Iteration 2/1000 | Loss: 0.00005553
Iteration 3/1000 | Loss: 0.00003875
Iteration 4/1000 | Loss: 0.00002870
Iteration 5/1000 | Loss: 0.00002601
Iteration 6/1000 | Loss: 0.00002461
Iteration 7/1000 | Loss: 0.00002370
Iteration 8/1000 | Loss: 0.00002302
Iteration 9/1000 | Loss: 0.00002256
Iteration 10/1000 | Loss: 0.00002228
Iteration 11/1000 | Loss: 0.00002205
Iteration 12/1000 | Loss: 0.00002189
Iteration 13/1000 | Loss: 0.00002178
Iteration 14/1000 | Loss: 0.00002176
Iteration 15/1000 | Loss: 0.00002168
Iteration 16/1000 | Loss: 0.00002165
Iteration 17/1000 | Loss: 0.00002165
Iteration 18/1000 | Loss: 0.00002162
Iteration 19/1000 | Loss: 0.00002160
Iteration 20/1000 | Loss: 0.00002159
Iteration 21/1000 | Loss: 0.00002159
Iteration 22/1000 | Loss: 0.00002158
Iteration 23/1000 | Loss: 0.00002157
Iteration 24/1000 | Loss: 0.00002157
Iteration 25/1000 | Loss: 0.00002154
Iteration 26/1000 | Loss: 0.00002154
Iteration 27/1000 | Loss: 0.00002154
Iteration 28/1000 | Loss: 0.00002154
Iteration 29/1000 | Loss: 0.00002154
Iteration 30/1000 | Loss: 0.00002154
Iteration 31/1000 | Loss: 0.00002154
Iteration 32/1000 | Loss: 0.00002153
Iteration 33/1000 | Loss: 0.00002153
Iteration 34/1000 | Loss: 0.00002153
Iteration 35/1000 | Loss: 0.00002153
Iteration 36/1000 | Loss: 0.00002153
Iteration 37/1000 | Loss: 0.00002153
Iteration 38/1000 | Loss: 0.00002150
Iteration 39/1000 | Loss: 0.00002150
Iteration 40/1000 | Loss: 0.00002150
Iteration 41/1000 | Loss: 0.00002150
Iteration 42/1000 | Loss: 0.00002150
Iteration 43/1000 | Loss: 0.00002149
Iteration 44/1000 | Loss: 0.00002149
Iteration 45/1000 | Loss: 0.00002149
Iteration 46/1000 | Loss: 0.00002148
Iteration 47/1000 | Loss: 0.00002148
Iteration 48/1000 | Loss: 0.00002147
Iteration 49/1000 | Loss: 0.00002147
Iteration 50/1000 | Loss: 0.00002147
Iteration 51/1000 | Loss: 0.00002147
Iteration 52/1000 | Loss: 0.00002147
Iteration 53/1000 | Loss: 0.00002146
Iteration 54/1000 | Loss: 0.00002146
Iteration 55/1000 | Loss: 0.00002146
Iteration 56/1000 | Loss: 0.00002145
Iteration 57/1000 | Loss: 0.00002145
Iteration 58/1000 | Loss: 0.00002145
Iteration 59/1000 | Loss: 0.00002144
Iteration 60/1000 | Loss: 0.00002144
Iteration 61/1000 | Loss: 0.00002144
Iteration 62/1000 | Loss: 0.00002143
Iteration 63/1000 | Loss: 0.00002143
Iteration 64/1000 | Loss: 0.00002142
Iteration 65/1000 | Loss: 0.00002142
Iteration 66/1000 | Loss: 0.00002141
Iteration 67/1000 | Loss: 0.00002141
Iteration 68/1000 | Loss: 0.00002140
Iteration 69/1000 | Loss: 0.00002140
Iteration 70/1000 | Loss: 0.00002140
Iteration 71/1000 | Loss: 0.00002139
Iteration 72/1000 | Loss: 0.00002139
Iteration 73/1000 | Loss: 0.00002138
Iteration 74/1000 | Loss: 0.00002138
Iteration 75/1000 | Loss: 0.00002137
Iteration 76/1000 | Loss: 0.00002137
Iteration 77/1000 | Loss: 0.00002137
Iteration 78/1000 | Loss: 0.00002137
Iteration 79/1000 | Loss: 0.00002137
Iteration 80/1000 | Loss: 0.00002137
Iteration 81/1000 | Loss: 0.00002136
Iteration 82/1000 | Loss: 0.00002136
Iteration 83/1000 | Loss: 0.00002136
Iteration 84/1000 | Loss: 0.00002136
Iteration 85/1000 | Loss: 0.00002136
Iteration 86/1000 | Loss: 0.00002136
Iteration 87/1000 | Loss: 0.00002135
Iteration 88/1000 | Loss: 0.00002135
Iteration 89/1000 | Loss: 0.00002135
Iteration 90/1000 | Loss: 0.00002135
Iteration 91/1000 | Loss: 0.00002135
Iteration 92/1000 | Loss: 0.00002135
Iteration 93/1000 | Loss: 0.00002134
Iteration 94/1000 | Loss: 0.00002134
Iteration 95/1000 | Loss: 0.00002134
Iteration 96/1000 | Loss: 0.00002134
Iteration 97/1000 | Loss: 0.00002134
Iteration 98/1000 | Loss: 0.00002134
Iteration 99/1000 | Loss: 0.00002134
Iteration 100/1000 | Loss: 0.00002134
Iteration 101/1000 | Loss: 0.00002134
Iteration 102/1000 | Loss: 0.00002133
Iteration 103/1000 | Loss: 0.00002133
Iteration 104/1000 | Loss: 0.00002133
Iteration 105/1000 | Loss: 0.00002133
Iteration 106/1000 | Loss: 0.00002133
Iteration 107/1000 | Loss: 0.00002133
Iteration 108/1000 | Loss: 0.00002133
Iteration 109/1000 | Loss: 0.00002133
Iteration 110/1000 | Loss: 0.00002133
Iteration 111/1000 | Loss: 0.00002133
Iteration 112/1000 | Loss: 0.00002133
Iteration 113/1000 | Loss: 0.00002133
Iteration 114/1000 | Loss: 0.00002133
Iteration 115/1000 | Loss: 0.00002133
Iteration 116/1000 | Loss: 0.00002133
Iteration 117/1000 | Loss: 0.00002132
Iteration 118/1000 | Loss: 0.00002132
Iteration 119/1000 | Loss: 0.00002132
Iteration 120/1000 | Loss: 0.00002132
Iteration 121/1000 | Loss: 0.00002132
Iteration 122/1000 | Loss: 0.00002132
Iteration 123/1000 | Loss: 0.00002132
Iteration 124/1000 | Loss: 0.00002132
Iteration 125/1000 | Loss: 0.00002132
Iteration 126/1000 | Loss: 0.00002132
Iteration 127/1000 | Loss: 0.00002132
Iteration 128/1000 | Loss: 0.00002132
Iteration 129/1000 | Loss: 0.00002131
Iteration 130/1000 | Loss: 0.00002131
Iteration 131/1000 | Loss: 0.00002131
Iteration 132/1000 | Loss: 0.00002131
Iteration 133/1000 | Loss: 0.00002131
Iteration 134/1000 | Loss: 0.00002131
Iteration 135/1000 | Loss: 0.00002131
Iteration 136/1000 | Loss: 0.00002131
Iteration 137/1000 | Loss: 0.00002131
Iteration 138/1000 | Loss: 0.00002131
Iteration 139/1000 | Loss: 0.00002131
Iteration 140/1000 | Loss: 0.00002131
Iteration 141/1000 | Loss: 0.00002131
Iteration 142/1000 | Loss: 0.00002131
Iteration 143/1000 | Loss: 0.00002131
Iteration 144/1000 | Loss: 0.00002131
Iteration 145/1000 | Loss: 0.00002131
Iteration 146/1000 | Loss: 0.00002131
Iteration 147/1000 | Loss: 0.00002131
Iteration 148/1000 | Loss: 0.00002131
Iteration 149/1000 | Loss: 0.00002131
Iteration 150/1000 | Loss: 0.00002131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.1308469513314776e-05, 2.1308469513314776e-05, 2.1308469513314776e-05, 2.1308469513314776e-05, 2.1308469513314776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1308469513314776e-05

Optimization complete. Final v2v error: 3.9485957622528076 mm

Highest mean error: 5.5262675285339355 mm for frame 98

Lowest mean error: 3.087364912033081 mm for frame 16

Saving results

Total time: 38.91598200798035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433045
Iteration 2/25 | Loss: 0.00101566
Iteration 3/25 | Loss: 0.00092747
Iteration 4/25 | Loss: 0.00091247
Iteration 5/25 | Loss: 0.00090756
Iteration 6/25 | Loss: 0.00090659
Iteration 7/25 | Loss: 0.00090659
Iteration 8/25 | Loss: 0.00090659
Iteration 9/25 | Loss: 0.00090659
Iteration 10/25 | Loss: 0.00090659
Iteration 11/25 | Loss: 0.00090659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000906594330444932, 0.000906594330444932, 0.000906594330444932, 0.000906594330444932, 0.000906594330444932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000906594330444932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32740760
Iteration 2/25 | Loss: 0.00076985
Iteration 3/25 | Loss: 0.00076985
Iteration 4/25 | Loss: 0.00076985
Iteration 5/25 | Loss: 0.00076985
Iteration 6/25 | Loss: 0.00076985
Iteration 7/25 | Loss: 0.00076985
Iteration 8/25 | Loss: 0.00076985
Iteration 9/25 | Loss: 0.00076985
Iteration 10/25 | Loss: 0.00076985
Iteration 11/25 | Loss: 0.00076985
Iteration 12/25 | Loss: 0.00076985
Iteration 13/25 | Loss: 0.00076985
Iteration 14/25 | Loss: 0.00076985
Iteration 15/25 | Loss: 0.00076985
Iteration 16/25 | Loss: 0.00076985
Iteration 17/25 | Loss: 0.00076985
Iteration 18/25 | Loss: 0.00076985
Iteration 19/25 | Loss: 0.00076985
Iteration 20/25 | Loss: 0.00076985
Iteration 21/25 | Loss: 0.00076985
Iteration 22/25 | Loss: 0.00076985
Iteration 23/25 | Loss: 0.00076985
Iteration 24/25 | Loss: 0.00076985
Iteration 25/25 | Loss: 0.00076985
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007698490517213941, 0.0007698490517213941, 0.0007698490517213941, 0.0007698490517213941, 0.0007698490517213941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007698490517213941

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076985
Iteration 2/1000 | Loss: 0.00002547
Iteration 3/1000 | Loss: 0.00001686
Iteration 4/1000 | Loss: 0.00001429
Iteration 5/1000 | Loss: 0.00001321
Iteration 6/1000 | Loss: 0.00001260
Iteration 7/1000 | Loss: 0.00001209
Iteration 8/1000 | Loss: 0.00001179
Iteration 9/1000 | Loss: 0.00001154
Iteration 10/1000 | Loss: 0.00001141
Iteration 11/1000 | Loss: 0.00001132
Iteration 12/1000 | Loss: 0.00001129
Iteration 13/1000 | Loss: 0.00001126
Iteration 14/1000 | Loss: 0.00001126
Iteration 15/1000 | Loss: 0.00001125
Iteration 16/1000 | Loss: 0.00001125
Iteration 17/1000 | Loss: 0.00001124
Iteration 18/1000 | Loss: 0.00001124
Iteration 19/1000 | Loss: 0.00001124
Iteration 20/1000 | Loss: 0.00001123
Iteration 21/1000 | Loss: 0.00001120
Iteration 22/1000 | Loss: 0.00001119
Iteration 23/1000 | Loss: 0.00001118
Iteration 24/1000 | Loss: 0.00001118
Iteration 25/1000 | Loss: 0.00001117
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001110
Iteration 28/1000 | Loss: 0.00001099
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001095
Iteration 31/1000 | Loss: 0.00001093
Iteration 32/1000 | Loss: 0.00001093
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001091
Iteration 35/1000 | Loss: 0.00001090
Iteration 36/1000 | Loss: 0.00001090
Iteration 37/1000 | Loss: 0.00001090
Iteration 38/1000 | Loss: 0.00001089
Iteration 39/1000 | Loss: 0.00001089
Iteration 40/1000 | Loss: 0.00001089
Iteration 41/1000 | Loss: 0.00001089
Iteration 42/1000 | Loss: 0.00001089
Iteration 43/1000 | Loss: 0.00001088
Iteration 44/1000 | Loss: 0.00001088
Iteration 45/1000 | Loss: 0.00001087
Iteration 46/1000 | Loss: 0.00001087
Iteration 47/1000 | Loss: 0.00001087
Iteration 48/1000 | Loss: 0.00001087
Iteration 49/1000 | Loss: 0.00001087
Iteration 50/1000 | Loss: 0.00001087
Iteration 51/1000 | Loss: 0.00001087
Iteration 52/1000 | Loss: 0.00001087
Iteration 53/1000 | Loss: 0.00001087
Iteration 54/1000 | Loss: 0.00001087
Iteration 55/1000 | Loss: 0.00001087
Iteration 56/1000 | Loss: 0.00001086
Iteration 57/1000 | Loss: 0.00001086
Iteration 58/1000 | Loss: 0.00001086
Iteration 59/1000 | Loss: 0.00001086
Iteration 60/1000 | Loss: 0.00001086
Iteration 61/1000 | Loss: 0.00001086
Iteration 62/1000 | Loss: 0.00001086
Iteration 63/1000 | Loss: 0.00001086
Iteration 64/1000 | Loss: 0.00001085
Iteration 65/1000 | Loss: 0.00001085
Iteration 66/1000 | Loss: 0.00001085
Iteration 67/1000 | Loss: 0.00001085
Iteration 68/1000 | Loss: 0.00001085
Iteration 69/1000 | Loss: 0.00001085
Iteration 70/1000 | Loss: 0.00001085
Iteration 71/1000 | Loss: 0.00001085
Iteration 72/1000 | Loss: 0.00001085
Iteration 73/1000 | Loss: 0.00001085
Iteration 74/1000 | Loss: 0.00001085
Iteration 75/1000 | Loss: 0.00001085
Iteration 76/1000 | Loss: 0.00001085
Iteration 77/1000 | Loss: 0.00001084
Iteration 78/1000 | Loss: 0.00001084
Iteration 79/1000 | Loss: 0.00001084
Iteration 80/1000 | Loss: 0.00001084
Iteration 81/1000 | Loss: 0.00001084
Iteration 82/1000 | Loss: 0.00001084
Iteration 83/1000 | Loss: 0.00001084
Iteration 84/1000 | Loss: 0.00001083
Iteration 85/1000 | Loss: 0.00001083
Iteration 86/1000 | Loss: 0.00001083
Iteration 87/1000 | Loss: 0.00001083
Iteration 88/1000 | Loss: 0.00001083
Iteration 89/1000 | Loss: 0.00001083
Iteration 90/1000 | Loss: 0.00001083
Iteration 91/1000 | Loss: 0.00001083
Iteration 92/1000 | Loss: 0.00001083
Iteration 93/1000 | Loss: 0.00001083
Iteration 94/1000 | Loss: 0.00001082
Iteration 95/1000 | Loss: 0.00001082
Iteration 96/1000 | Loss: 0.00001082
Iteration 97/1000 | Loss: 0.00001082
Iteration 98/1000 | Loss: 0.00001082
Iteration 99/1000 | Loss: 0.00001082
Iteration 100/1000 | Loss: 0.00001082
Iteration 101/1000 | Loss: 0.00001082
Iteration 102/1000 | Loss: 0.00001082
Iteration 103/1000 | Loss: 0.00001082
Iteration 104/1000 | Loss: 0.00001082
Iteration 105/1000 | Loss: 0.00001082
Iteration 106/1000 | Loss: 0.00001082
Iteration 107/1000 | Loss: 0.00001081
Iteration 108/1000 | Loss: 0.00001081
Iteration 109/1000 | Loss: 0.00001081
Iteration 110/1000 | Loss: 0.00001081
Iteration 111/1000 | Loss: 0.00001081
Iteration 112/1000 | Loss: 0.00001081
Iteration 113/1000 | Loss: 0.00001081
Iteration 114/1000 | Loss: 0.00001081
Iteration 115/1000 | Loss: 0.00001080
Iteration 116/1000 | Loss: 0.00001080
Iteration 117/1000 | Loss: 0.00001080
Iteration 118/1000 | Loss: 0.00001080
Iteration 119/1000 | Loss: 0.00001080
Iteration 120/1000 | Loss: 0.00001080
Iteration 121/1000 | Loss: 0.00001080
Iteration 122/1000 | Loss: 0.00001080
Iteration 123/1000 | Loss: 0.00001080
Iteration 124/1000 | Loss: 0.00001080
Iteration 125/1000 | Loss: 0.00001080
Iteration 126/1000 | Loss: 0.00001080
Iteration 127/1000 | Loss: 0.00001080
Iteration 128/1000 | Loss: 0.00001080
Iteration 129/1000 | Loss: 0.00001080
Iteration 130/1000 | Loss: 0.00001080
Iteration 131/1000 | Loss: 0.00001080
Iteration 132/1000 | Loss: 0.00001080
Iteration 133/1000 | Loss: 0.00001080
Iteration 134/1000 | Loss: 0.00001080
Iteration 135/1000 | Loss: 0.00001080
Iteration 136/1000 | Loss: 0.00001080
Iteration 137/1000 | Loss: 0.00001080
Iteration 138/1000 | Loss: 0.00001080
Iteration 139/1000 | Loss: 0.00001080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.0798538824019488e-05, 1.0798538824019488e-05, 1.0798538824019488e-05, 1.0798538824019488e-05, 1.0798538824019488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0798538824019488e-05

Optimization complete. Final v2v error: 2.845623254776001 mm

Highest mean error: 3.188936233520508 mm for frame 212

Lowest mean error: 2.461061477661133 mm for frame 11

Saving results

Total time: 40.69302988052368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00738928
Iteration 2/25 | Loss: 0.00111869
Iteration 3/25 | Loss: 0.00093475
Iteration 4/25 | Loss: 0.00090191
Iteration 5/25 | Loss: 0.00088494
Iteration 6/25 | Loss: 0.00088536
Iteration 7/25 | Loss: 0.00088383
Iteration 8/25 | Loss: 0.00088240
Iteration 9/25 | Loss: 0.00088185
Iteration 10/25 | Loss: 0.00088147
Iteration 11/25 | Loss: 0.00088120
Iteration 12/25 | Loss: 0.00088111
Iteration 13/25 | Loss: 0.00088104
Iteration 14/25 | Loss: 0.00088104
Iteration 15/25 | Loss: 0.00088104
Iteration 16/25 | Loss: 0.00088103
Iteration 17/25 | Loss: 0.00088103
Iteration 18/25 | Loss: 0.00088103
Iteration 19/25 | Loss: 0.00088103
Iteration 20/25 | Loss: 0.00088103
Iteration 21/25 | Loss: 0.00088103
Iteration 22/25 | Loss: 0.00088103
Iteration 23/25 | Loss: 0.00088103
Iteration 24/25 | Loss: 0.00088102
Iteration 25/25 | Loss: 0.00088102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.77504730
Iteration 2/25 | Loss: 0.00085105
Iteration 3/25 | Loss: 0.00085105
Iteration 4/25 | Loss: 0.00085105
Iteration 5/25 | Loss: 0.00085105
Iteration 6/25 | Loss: 0.00085105
Iteration 7/25 | Loss: 0.00085105
Iteration 8/25 | Loss: 0.00085105
Iteration 9/25 | Loss: 0.00085105
Iteration 10/25 | Loss: 0.00085105
Iteration 11/25 | Loss: 0.00085105
Iteration 12/25 | Loss: 0.00085105
Iteration 13/25 | Loss: 0.00085105
Iteration 14/25 | Loss: 0.00085105
Iteration 15/25 | Loss: 0.00085105
Iteration 16/25 | Loss: 0.00085105
Iteration 17/25 | Loss: 0.00085105
Iteration 18/25 | Loss: 0.00085105
Iteration 19/25 | Loss: 0.00085105
Iteration 20/25 | Loss: 0.00085105
Iteration 21/25 | Loss: 0.00085105
Iteration 22/25 | Loss: 0.00085105
Iteration 23/25 | Loss: 0.00085105
Iteration 24/25 | Loss: 0.00085105
Iteration 25/25 | Loss: 0.00085105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085105
Iteration 2/1000 | Loss: 0.00002759
Iteration 3/1000 | Loss: 0.00001822
Iteration 4/1000 | Loss: 0.00001565
Iteration 5/1000 | Loss: 0.00005815
Iteration 6/1000 | Loss: 0.00001359
Iteration 7/1000 | Loss: 0.00001304
Iteration 8/1000 | Loss: 0.00001278
Iteration 9/1000 | Loss: 0.00001265
Iteration 10/1000 | Loss: 0.00001253
Iteration 11/1000 | Loss: 0.00001245
Iteration 12/1000 | Loss: 0.00005899
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001238
Iteration 15/1000 | Loss: 0.00001235
Iteration 16/1000 | Loss: 0.00001235
Iteration 17/1000 | Loss: 0.00001235
Iteration 18/1000 | Loss: 0.00001235
Iteration 19/1000 | Loss: 0.00001235
Iteration 20/1000 | Loss: 0.00001235
Iteration 21/1000 | Loss: 0.00001235
Iteration 22/1000 | Loss: 0.00001235
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001235
Iteration 25/1000 | Loss: 0.00001235
Iteration 26/1000 | Loss: 0.00001234
Iteration 27/1000 | Loss: 0.00001234
Iteration 28/1000 | Loss: 0.00001234
Iteration 29/1000 | Loss: 0.00001234
Iteration 30/1000 | Loss: 0.00001234
Iteration 31/1000 | Loss: 0.00001234
Iteration 32/1000 | Loss: 0.00001234
Iteration 33/1000 | Loss: 0.00001233
Iteration 34/1000 | Loss: 0.00001233
Iteration 35/1000 | Loss: 0.00001232
Iteration 36/1000 | Loss: 0.00001232
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001231
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001231
Iteration 41/1000 | Loss: 0.00001231
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001230
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001229
Iteration 46/1000 | Loss: 0.00001228
Iteration 47/1000 | Loss: 0.00001228
Iteration 48/1000 | Loss: 0.00001228
Iteration 49/1000 | Loss: 0.00001227
Iteration 50/1000 | Loss: 0.00001227
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00006204
Iteration 55/1000 | Loss: 0.00001249
Iteration 56/1000 | Loss: 0.00001225
Iteration 57/1000 | Loss: 0.00001224
Iteration 58/1000 | Loss: 0.00001224
Iteration 59/1000 | Loss: 0.00001221
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001219
Iteration 63/1000 | Loss: 0.00001218
Iteration 64/1000 | Loss: 0.00001218
Iteration 65/1000 | Loss: 0.00001218
Iteration 66/1000 | Loss: 0.00001218
Iteration 67/1000 | Loss: 0.00001218
Iteration 68/1000 | Loss: 0.00001217
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001215
Iteration 73/1000 | Loss: 0.00001215
Iteration 74/1000 | Loss: 0.00001215
Iteration 75/1000 | Loss: 0.00001215
Iteration 76/1000 | Loss: 0.00001215
Iteration 77/1000 | Loss: 0.00001215
Iteration 78/1000 | Loss: 0.00001215
Iteration 79/1000 | Loss: 0.00001214
Iteration 80/1000 | Loss: 0.00001214
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001213
Iteration 83/1000 | Loss: 0.00001213
Iteration 84/1000 | Loss: 0.00001213
Iteration 85/1000 | Loss: 0.00001212
Iteration 86/1000 | Loss: 0.00001211
Iteration 87/1000 | Loss: 0.00005791
Iteration 88/1000 | Loss: 0.00001211
Iteration 89/1000 | Loss: 0.00001211
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001207
Iteration 92/1000 | Loss: 0.00001207
Iteration 93/1000 | Loss: 0.00001207
Iteration 94/1000 | Loss: 0.00001207
Iteration 95/1000 | Loss: 0.00001206
Iteration 96/1000 | Loss: 0.00001206
Iteration 97/1000 | Loss: 0.00001206
Iteration 98/1000 | Loss: 0.00001206
Iteration 99/1000 | Loss: 0.00001206
Iteration 100/1000 | Loss: 0.00001206
Iteration 101/1000 | Loss: 0.00001206
Iteration 102/1000 | Loss: 0.00001206
Iteration 103/1000 | Loss: 0.00001206
Iteration 104/1000 | Loss: 0.00001206
Iteration 105/1000 | Loss: 0.00001206
Iteration 106/1000 | Loss: 0.00001206
Iteration 107/1000 | Loss: 0.00001206
Iteration 108/1000 | Loss: 0.00001206
Iteration 109/1000 | Loss: 0.00001205
Iteration 110/1000 | Loss: 0.00001205
Iteration 111/1000 | Loss: 0.00001205
Iteration 112/1000 | Loss: 0.00001205
Iteration 113/1000 | Loss: 0.00001205
Iteration 114/1000 | Loss: 0.00001205
Iteration 115/1000 | Loss: 0.00001205
Iteration 116/1000 | Loss: 0.00001205
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001205
Iteration 122/1000 | Loss: 0.00001205
Iteration 123/1000 | Loss: 0.00001205
Iteration 124/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.2054085345880594e-05, 1.2054085345880594e-05, 1.2054085345880594e-05, 1.2054085345880594e-05, 1.2054085345880594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2054085345880594e-05

Optimization complete. Final v2v error: 2.9558591842651367 mm

Highest mean error: 3.4114327430725098 mm for frame 145

Lowest mean error: 2.5912058353424072 mm for frame 243

Saving results

Total time: 58.78330421447754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5690/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5690/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537285
Iteration 2/25 | Loss: 0.00114884
Iteration 3/25 | Loss: 0.00092335
Iteration 4/25 | Loss: 0.00090536
Iteration 5/25 | Loss: 0.00090132
Iteration 6/25 | Loss: 0.00090059
Iteration 7/25 | Loss: 0.00090059
Iteration 8/25 | Loss: 0.00090059
Iteration 9/25 | Loss: 0.00090059
Iteration 10/25 | Loss: 0.00090059
Iteration 11/25 | Loss: 0.00090059
Iteration 12/25 | Loss: 0.00090059
Iteration 13/25 | Loss: 0.00090059
Iteration 14/25 | Loss: 0.00090059
Iteration 15/25 | Loss: 0.00090059
Iteration 16/25 | Loss: 0.00090059
Iteration 17/25 | Loss: 0.00090059
Iteration 18/25 | Loss: 0.00090059
Iteration 19/25 | Loss: 0.00090059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009005879983305931, 0.0009005879983305931, 0.0009005879983305931, 0.0009005879983305931, 0.0009005879983305931]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009005879983305931

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50143778
Iteration 2/25 | Loss: 0.00064886
Iteration 3/25 | Loss: 0.00064885
Iteration 4/25 | Loss: 0.00064885
Iteration 5/25 | Loss: 0.00064885
Iteration 6/25 | Loss: 0.00064885
Iteration 7/25 | Loss: 0.00064885
Iteration 8/25 | Loss: 0.00064885
Iteration 9/25 | Loss: 0.00064885
Iteration 10/25 | Loss: 0.00064885
Iteration 11/25 | Loss: 0.00064885
Iteration 12/25 | Loss: 0.00064885
Iteration 13/25 | Loss: 0.00064885
Iteration 14/25 | Loss: 0.00064885
Iteration 15/25 | Loss: 0.00064885
Iteration 16/25 | Loss: 0.00064885
Iteration 17/25 | Loss: 0.00064885
Iteration 18/25 | Loss: 0.00064885
Iteration 19/25 | Loss: 0.00064885
Iteration 20/25 | Loss: 0.00064885
Iteration 21/25 | Loss: 0.00064885
Iteration 22/25 | Loss: 0.00064885
Iteration 23/25 | Loss: 0.00064885
Iteration 24/25 | Loss: 0.00064885
Iteration 25/25 | Loss: 0.00064885

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064885
Iteration 2/1000 | Loss: 0.00002726
Iteration 3/1000 | Loss: 0.00001917
Iteration 4/1000 | Loss: 0.00001549
Iteration 5/1000 | Loss: 0.00001396
Iteration 6/1000 | Loss: 0.00001316
Iteration 7/1000 | Loss: 0.00001266
Iteration 8/1000 | Loss: 0.00001223
Iteration 9/1000 | Loss: 0.00001202
Iteration 10/1000 | Loss: 0.00001202
Iteration 11/1000 | Loss: 0.00001181
Iteration 12/1000 | Loss: 0.00001181
Iteration 13/1000 | Loss: 0.00001180
Iteration 14/1000 | Loss: 0.00001172
Iteration 15/1000 | Loss: 0.00001166
Iteration 16/1000 | Loss: 0.00001166
Iteration 17/1000 | Loss: 0.00001162
Iteration 18/1000 | Loss: 0.00001161
Iteration 19/1000 | Loss: 0.00001161
Iteration 20/1000 | Loss: 0.00001160
Iteration 21/1000 | Loss: 0.00001160
Iteration 22/1000 | Loss: 0.00001159
Iteration 23/1000 | Loss: 0.00001159
Iteration 24/1000 | Loss: 0.00001158
Iteration 25/1000 | Loss: 0.00001157
Iteration 26/1000 | Loss: 0.00001157
Iteration 27/1000 | Loss: 0.00001157
Iteration 28/1000 | Loss: 0.00001156
Iteration 29/1000 | Loss: 0.00001156
Iteration 30/1000 | Loss: 0.00001156
Iteration 31/1000 | Loss: 0.00001156
Iteration 32/1000 | Loss: 0.00001155
Iteration 33/1000 | Loss: 0.00001155
Iteration 34/1000 | Loss: 0.00001155
Iteration 35/1000 | Loss: 0.00001155
Iteration 36/1000 | Loss: 0.00001155
Iteration 37/1000 | Loss: 0.00001155
Iteration 38/1000 | Loss: 0.00001155
Iteration 39/1000 | Loss: 0.00001154
Iteration 40/1000 | Loss: 0.00001154
Iteration 41/1000 | Loss: 0.00001154
Iteration 42/1000 | Loss: 0.00001154
Iteration 43/1000 | Loss: 0.00001154
Iteration 44/1000 | Loss: 0.00001154
Iteration 45/1000 | Loss: 0.00001154
Iteration 46/1000 | Loss: 0.00001154
Iteration 47/1000 | Loss: 0.00001154
Iteration 48/1000 | Loss: 0.00001154
Iteration 49/1000 | Loss: 0.00001154
Iteration 50/1000 | Loss: 0.00001154
Iteration 51/1000 | Loss: 0.00001153
Iteration 52/1000 | Loss: 0.00001153
Iteration 53/1000 | Loss: 0.00001153
Iteration 54/1000 | Loss: 0.00001153
Iteration 55/1000 | Loss: 0.00001152
Iteration 56/1000 | Loss: 0.00001152
Iteration 57/1000 | Loss: 0.00001152
Iteration 58/1000 | Loss: 0.00001152
Iteration 59/1000 | Loss: 0.00001152
Iteration 60/1000 | Loss: 0.00001152
Iteration 61/1000 | Loss: 0.00001151
Iteration 62/1000 | Loss: 0.00001151
Iteration 63/1000 | Loss: 0.00001151
Iteration 64/1000 | Loss: 0.00001151
Iteration 65/1000 | Loss: 0.00001151
Iteration 66/1000 | Loss: 0.00001151
Iteration 67/1000 | Loss: 0.00001150
Iteration 68/1000 | Loss: 0.00001150
Iteration 69/1000 | Loss: 0.00001150
Iteration 70/1000 | Loss: 0.00001149
Iteration 71/1000 | Loss: 0.00001149
Iteration 72/1000 | Loss: 0.00001149
Iteration 73/1000 | Loss: 0.00001148
Iteration 74/1000 | Loss: 0.00001148
Iteration 75/1000 | Loss: 0.00001148
Iteration 76/1000 | Loss: 0.00001148
Iteration 77/1000 | Loss: 0.00001147
Iteration 78/1000 | Loss: 0.00001147
Iteration 79/1000 | Loss: 0.00001147
Iteration 80/1000 | Loss: 0.00001147
Iteration 81/1000 | Loss: 0.00001146
Iteration 82/1000 | Loss: 0.00001146
Iteration 83/1000 | Loss: 0.00001146
Iteration 84/1000 | Loss: 0.00001146
Iteration 85/1000 | Loss: 0.00001145
Iteration 86/1000 | Loss: 0.00001145
Iteration 87/1000 | Loss: 0.00001145
Iteration 88/1000 | Loss: 0.00001145
Iteration 89/1000 | Loss: 0.00001144
Iteration 90/1000 | Loss: 0.00001144
Iteration 91/1000 | Loss: 0.00001144
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001142
Iteration 94/1000 | Loss: 0.00001142
Iteration 95/1000 | Loss: 0.00001142
Iteration 96/1000 | Loss: 0.00001142
Iteration 97/1000 | Loss: 0.00001141
Iteration 98/1000 | Loss: 0.00001141
Iteration 99/1000 | Loss: 0.00001140
Iteration 100/1000 | Loss: 0.00001140
Iteration 101/1000 | Loss: 0.00001140
Iteration 102/1000 | Loss: 0.00001140
Iteration 103/1000 | Loss: 0.00001139
Iteration 104/1000 | Loss: 0.00001139
Iteration 105/1000 | Loss: 0.00001139
Iteration 106/1000 | Loss: 0.00001139
Iteration 107/1000 | Loss: 0.00001139
Iteration 108/1000 | Loss: 0.00001139
Iteration 109/1000 | Loss: 0.00001139
Iteration 110/1000 | Loss: 0.00001139
Iteration 111/1000 | Loss: 0.00001138
Iteration 112/1000 | Loss: 0.00001138
Iteration 113/1000 | Loss: 0.00001137
Iteration 114/1000 | Loss: 0.00001137
Iteration 115/1000 | Loss: 0.00001137
Iteration 116/1000 | Loss: 0.00001137
Iteration 117/1000 | Loss: 0.00001137
Iteration 118/1000 | Loss: 0.00001137
Iteration 119/1000 | Loss: 0.00001137
Iteration 120/1000 | Loss: 0.00001136
Iteration 121/1000 | Loss: 0.00001136
Iteration 122/1000 | Loss: 0.00001136
Iteration 123/1000 | Loss: 0.00001136
Iteration 124/1000 | Loss: 0.00001136
Iteration 125/1000 | Loss: 0.00001136
Iteration 126/1000 | Loss: 0.00001136
Iteration 127/1000 | Loss: 0.00001136
Iteration 128/1000 | Loss: 0.00001136
Iteration 129/1000 | Loss: 0.00001136
Iteration 130/1000 | Loss: 0.00001136
Iteration 131/1000 | Loss: 0.00001136
Iteration 132/1000 | Loss: 0.00001136
Iteration 133/1000 | Loss: 0.00001136
Iteration 134/1000 | Loss: 0.00001136
Iteration 135/1000 | Loss: 0.00001136
Iteration 136/1000 | Loss: 0.00001136
Iteration 137/1000 | Loss: 0.00001136
Iteration 138/1000 | Loss: 0.00001136
Iteration 139/1000 | Loss: 0.00001136
Iteration 140/1000 | Loss: 0.00001136
Iteration 141/1000 | Loss: 0.00001136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.1363523299223743e-05, 1.1363523299223743e-05, 1.1363523299223743e-05, 1.1363523299223743e-05, 1.1363523299223743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1363523299223743e-05

Optimization complete. Final v2v error: 2.859241008758545 mm

Highest mean error: 3.589916229248047 mm for frame 0

Lowest mean error: 2.498368740081787 mm for frame 119

Saving results

Total time: 38.42906856536865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948164
Iteration 2/25 | Loss: 0.00240441
Iteration 3/25 | Loss: 0.00218143
Iteration 4/25 | Loss: 0.00216970
Iteration 5/25 | Loss: 0.00216870
Iteration 6/25 | Loss: 0.00216870
Iteration 7/25 | Loss: 0.00216870
Iteration 8/25 | Loss: 0.00216870
Iteration 9/25 | Loss: 0.00216870
Iteration 10/25 | Loss: 0.00216870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00216870428994298, 0.00216870428994298, 0.00216870428994298, 0.00216870428994298, 0.00216870428994298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00216870428994298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87795401
Iteration 2/25 | Loss: 0.00179547
Iteration 3/25 | Loss: 0.00179547
Iteration 4/25 | Loss: 0.00179547
Iteration 5/25 | Loss: 0.00179547
Iteration 6/25 | Loss: 0.00179547
Iteration 7/25 | Loss: 0.00179547
Iteration 8/25 | Loss: 0.00179547
Iteration 9/25 | Loss: 0.00179547
Iteration 10/25 | Loss: 0.00179547
Iteration 11/25 | Loss: 0.00179547
Iteration 12/25 | Loss: 0.00179547
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001795465825125575, 0.001795465825125575, 0.001795465825125575, 0.001795465825125575, 0.001795465825125575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001795465825125575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179547
Iteration 2/1000 | Loss: 0.00007124
Iteration 3/1000 | Loss: 0.00005592
Iteration 4/1000 | Loss: 0.00005113
Iteration 5/1000 | Loss: 0.00004853
Iteration 6/1000 | Loss: 0.00004681
Iteration 7/1000 | Loss: 0.00004573
Iteration 8/1000 | Loss: 0.00004500
Iteration 9/1000 | Loss: 0.00004464
Iteration 10/1000 | Loss: 0.00004439
Iteration 11/1000 | Loss: 0.00004428
Iteration 12/1000 | Loss: 0.00004411
Iteration 13/1000 | Loss: 0.00004410
Iteration 14/1000 | Loss: 0.00004398
Iteration 15/1000 | Loss: 0.00004396
Iteration 16/1000 | Loss: 0.00004394
Iteration 17/1000 | Loss: 0.00004394
Iteration 18/1000 | Loss: 0.00004394
Iteration 19/1000 | Loss: 0.00004393
Iteration 20/1000 | Loss: 0.00004393
Iteration 21/1000 | Loss: 0.00004392
Iteration 22/1000 | Loss: 0.00004392
Iteration 23/1000 | Loss: 0.00004392
Iteration 24/1000 | Loss: 0.00004392
Iteration 25/1000 | Loss: 0.00004392
Iteration 26/1000 | Loss: 0.00004391
Iteration 27/1000 | Loss: 0.00004391
Iteration 28/1000 | Loss: 0.00004382
Iteration 29/1000 | Loss: 0.00004381
Iteration 30/1000 | Loss: 0.00004379
Iteration 31/1000 | Loss: 0.00004379
Iteration 32/1000 | Loss: 0.00004379
Iteration 33/1000 | Loss: 0.00004379
Iteration 34/1000 | Loss: 0.00004379
Iteration 35/1000 | Loss: 0.00004379
Iteration 36/1000 | Loss: 0.00004379
Iteration 37/1000 | Loss: 0.00004379
Iteration 38/1000 | Loss: 0.00004378
Iteration 39/1000 | Loss: 0.00004378
Iteration 40/1000 | Loss: 0.00004378
Iteration 41/1000 | Loss: 0.00004378
Iteration 42/1000 | Loss: 0.00004378
Iteration 43/1000 | Loss: 0.00004378
Iteration 44/1000 | Loss: 0.00004378
Iteration 45/1000 | Loss: 0.00004378
Iteration 46/1000 | Loss: 0.00004378
Iteration 47/1000 | Loss: 0.00004378
Iteration 48/1000 | Loss: 0.00004378
Iteration 49/1000 | Loss: 0.00004378
Iteration 50/1000 | Loss: 0.00004378
Iteration 51/1000 | Loss: 0.00004377
Iteration 52/1000 | Loss: 0.00004376
Iteration 53/1000 | Loss: 0.00004375
Iteration 54/1000 | Loss: 0.00004375
Iteration 55/1000 | Loss: 0.00004375
Iteration 56/1000 | Loss: 0.00004375
Iteration 57/1000 | Loss: 0.00004375
Iteration 58/1000 | Loss: 0.00004374
Iteration 59/1000 | Loss: 0.00004374
Iteration 60/1000 | Loss: 0.00004374
Iteration 61/1000 | Loss: 0.00004373
Iteration 62/1000 | Loss: 0.00004372
Iteration 63/1000 | Loss: 0.00004372
Iteration 64/1000 | Loss: 0.00004371
Iteration 65/1000 | Loss: 0.00004371
Iteration 66/1000 | Loss: 0.00004371
Iteration 67/1000 | Loss: 0.00004371
Iteration 68/1000 | Loss: 0.00004371
Iteration 69/1000 | Loss: 0.00004371
Iteration 70/1000 | Loss: 0.00004370
Iteration 71/1000 | Loss: 0.00004370
Iteration 72/1000 | Loss: 0.00004370
Iteration 73/1000 | Loss: 0.00004370
Iteration 74/1000 | Loss: 0.00004370
Iteration 75/1000 | Loss: 0.00004370
Iteration 76/1000 | Loss: 0.00004370
Iteration 77/1000 | Loss: 0.00004370
Iteration 78/1000 | Loss: 0.00004370
Iteration 79/1000 | Loss: 0.00004370
Iteration 80/1000 | Loss: 0.00004370
Iteration 81/1000 | Loss: 0.00004370
Iteration 82/1000 | Loss: 0.00004370
Iteration 83/1000 | Loss: 0.00004370
Iteration 84/1000 | Loss: 0.00004370
Iteration 85/1000 | Loss: 0.00004370
Iteration 86/1000 | Loss: 0.00004370
Iteration 87/1000 | Loss: 0.00004369
Iteration 88/1000 | Loss: 0.00004369
Iteration 89/1000 | Loss: 0.00004369
Iteration 90/1000 | Loss: 0.00004369
Iteration 91/1000 | Loss: 0.00004369
Iteration 92/1000 | Loss: 0.00004369
Iteration 93/1000 | Loss: 0.00004369
Iteration 94/1000 | Loss: 0.00004369
Iteration 95/1000 | Loss: 0.00004369
Iteration 96/1000 | Loss: 0.00004369
Iteration 97/1000 | Loss: 0.00004368
Iteration 98/1000 | Loss: 0.00004368
Iteration 99/1000 | Loss: 0.00004368
Iteration 100/1000 | Loss: 0.00004368
Iteration 101/1000 | Loss: 0.00004368
Iteration 102/1000 | Loss: 0.00004368
Iteration 103/1000 | Loss: 0.00004368
Iteration 104/1000 | Loss: 0.00004368
Iteration 105/1000 | Loss: 0.00004367
Iteration 106/1000 | Loss: 0.00004366
Iteration 107/1000 | Loss: 0.00004366
Iteration 108/1000 | Loss: 0.00004366
Iteration 109/1000 | Loss: 0.00004365
Iteration 110/1000 | Loss: 0.00004365
Iteration 111/1000 | Loss: 0.00004365
Iteration 112/1000 | Loss: 0.00004365
Iteration 113/1000 | Loss: 0.00004365
Iteration 114/1000 | Loss: 0.00004365
Iteration 115/1000 | Loss: 0.00004365
Iteration 116/1000 | Loss: 0.00004364
Iteration 117/1000 | Loss: 0.00004363
Iteration 118/1000 | Loss: 0.00004363
Iteration 119/1000 | Loss: 0.00004363
Iteration 120/1000 | Loss: 0.00004363
Iteration 121/1000 | Loss: 0.00004363
Iteration 122/1000 | Loss: 0.00004363
Iteration 123/1000 | Loss: 0.00004363
Iteration 124/1000 | Loss: 0.00004362
Iteration 125/1000 | Loss: 0.00004362
Iteration 126/1000 | Loss: 0.00004362
Iteration 127/1000 | Loss: 0.00004362
Iteration 128/1000 | Loss: 0.00004361
Iteration 129/1000 | Loss: 0.00004360
Iteration 130/1000 | Loss: 0.00004360
Iteration 131/1000 | Loss: 0.00004360
Iteration 132/1000 | Loss: 0.00004360
Iteration 133/1000 | Loss: 0.00004360
Iteration 134/1000 | Loss: 0.00004360
Iteration 135/1000 | Loss: 0.00004360
Iteration 136/1000 | Loss: 0.00004360
Iteration 137/1000 | Loss: 0.00004359
Iteration 138/1000 | Loss: 0.00004359
Iteration 139/1000 | Loss: 0.00004359
Iteration 140/1000 | Loss: 0.00004359
Iteration 141/1000 | Loss: 0.00004359
Iteration 142/1000 | Loss: 0.00004359
Iteration 143/1000 | Loss: 0.00004359
Iteration 144/1000 | Loss: 0.00004359
Iteration 145/1000 | Loss: 0.00004359
Iteration 146/1000 | Loss: 0.00004359
Iteration 147/1000 | Loss: 0.00004359
Iteration 148/1000 | Loss: 0.00004359
Iteration 149/1000 | Loss: 0.00004359
Iteration 150/1000 | Loss: 0.00004359
Iteration 151/1000 | Loss: 0.00004358
Iteration 152/1000 | Loss: 0.00004358
Iteration 153/1000 | Loss: 0.00004358
Iteration 154/1000 | Loss: 0.00004358
Iteration 155/1000 | Loss: 0.00004358
Iteration 156/1000 | Loss: 0.00004358
Iteration 157/1000 | Loss: 0.00004358
Iteration 158/1000 | Loss: 0.00004358
Iteration 159/1000 | Loss: 0.00004358
Iteration 160/1000 | Loss: 0.00004358
Iteration 161/1000 | Loss: 0.00004358
Iteration 162/1000 | Loss: 0.00004358
Iteration 163/1000 | Loss: 0.00004358
Iteration 164/1000 | Loss: 0.00004358
Iteration 165/1000 | Loss: 0.00004358
Iteration 166/1000 | Loss: 0.00004358
Iteration 167/1000 | Loss: 0.00004358
Iteration 168/1000 | Loss: 0.00004358
Iteration 169/1000 | Loss: 0.00004357
Iteration 170/1000 | Loss: 0.00004357
Iteration 171/1000 | Loss: 0.00004357
Iteration 172/1000 | Loss: 0.00004357
Iteration 173/1000 | Loss: 0.00004357
Iteration 174/1000 | Loss: 0.00004357
Iteration 175/1000 | Loss: 0.00004357
Iteration 176/1000 | Loss: 0.00004357
Iteration 177/1000 | Loss: 0.00004357
Iteration 178/1000 | Loss: 0.00004357
Iteration 179/1000 | Loss: 0.00004357
Iteration 180/1000 | Loss: 0.00004357
Iteration 181/1000 | Loss: 0.00004357
Iteration 182/1000 | Loss: 0.00004357
Iteration 183/1000 | Loss: 0.00004357
Iteration 184/1000 | Loss: 0.00004357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [4.3566767999436706e-05, 4.3566767999436706e-05, 4.3566767999436706e-05, 4.3566767999436706e-05, 4.3566767999436706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.3566767999436706e-05

Optimization complete. Final v2v error: 5.650496006011963 mm

Highest mean error: 5.766411304473877 mm for frame 5

Lowest mean error: 5.577890396118164 mm for frame 138

Saving results

Total time: 43.064067363739014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01100623
Iteration 2/25 | Loss: 0.00319123
Iteration 3/25 | Loss: 0.00241296
Iteration 4/25 | Loss: 0.00224967
Iteration 5/25 | Loss: 0.00242434
Iteration 6/25 | Loss: 0.00218758
Iteration 7/25 | Loss: 0.00203415
Iteration 8/25 | Loss: 0.00193186
Iteration 9/25 | Loss: 0.00189414
Iteration 10/25 | Loss: 0.00188064
Iteration 11/25 | Loss: 0.00185590
Iteration 12/25 | Loss: 0.00184493
Iteration 13/25 | Loss: 0.00184947
Iteration 14/25 | Loss: 0.00185273
Iteration 15/25 | Loss: 0.00181674
Iteration 16/25 | Loss: 0.00179881
Iteration 17/25 | Loss: 0.00179386
Iteration 18/25 | Loss: 0.00178957
Iteration 19/25 | Loss: 0.00176371
Iteration 20/25 | Loss: 0.00175314
Iteration 21/25 | Loss: 0.00173362
Iteration 22/25 | Loss: 0.00172986
Iteration 23/25 | Loss: 0.00173065
Iteration 24/25 | Loss: 0.00173003
Iteration 25/25 | Loss: 0.00171437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35141921
Iteration 2/25 | Loss: 0.00235801
Iteration 3/25 | Loss: 0.00228825
Iteration 4/25 | Loss: 0.00228824
Iteration 5/25 | Loss: 0.00228824
Iteration 6/25 | Loss: 0.00228824
Iteration 7/25 | Loss: 0.00228824
Iteration 8/25 | Loss: 0.00228824
Iteration 9/25 | Loss: 0.00228824
Iteration 10/25 | Loss: 0.00228824
Iteration 11/25 | Loss: 0.00228824
Iteration 12/25 | Loss: 0.00228824
Iteration 13/25 | Loss: 0.00228824
Iteration 14/25 | Loss: 0.00228824
Iteration 15/25 | Loss: 0.00228824
Iteration 16/25 | Loss: 0.00228824
Iteration 17/25 | Loss: 0.00228824
Iteration 18/25 | Loss: 0.00228824
Iteration 19/25 | Loss: 0.00228824
Iteration 20/25 | Loss: 0.00228824
Iteration 21/25 | Loss: 0.00228824
Iteration 22/25 | Loss: 0.00228824
Iteration 23/25 | Loss: 0.00228824
Iteration 24/25 | Loss: 0.00228824
Iteration 25/25 | Loss: 0.00228824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228824
Iteration 2/1000 | Loss: 0.00066949
Iteration 3/1000 | Loss: 0.00031336
Iteration 4/1000 | Loss: 0.00066312
Iteration 5/1000 | Loss: 0.00067275
Iteration 6/1000 | Loss: 0.00032315
Iteration 7/1000 | Loss: 0.00032303
Iteration 8/1000 | Loss: 0.00030799
Iteration 9/1000 | Loss: 0.00028441
Iteration 10/1000 | Loss: 0.00056617
Iteration 11/1000 | Loss: 0.00038920
Iteration 12/1000 | Loss: 0.00032482
Iteration 13/1000 | Loss: 0.00032018
Iteration 14/1000 | Loss: 0.00040099
Iteration 15/1000 | Loss: 0.00038508
Iteration 16/1000 | Loss: 0.00078044
Iteration 17/1000 | Loss: 0.00061649
Iteration 18/1000 | Loss: 0.00030154
Iteration 19/1000 | Loss: 0.00026143
Iteration 20/1000 | Loss: 0.00021236
Iteration 21/1000 | Loss: 0.00019123
Iteration 22/1000 | Loss: 0.00018613
Iteration 23/1000 | Loss: 0.00020307
Iteration 24/1000 | Loss: 0.00018634
Iteration 25/1000 | Loss: 0.00016971
Iteration 26/1000 | Loss: 0.00016894
Iteration 27/1000 | Loss: 0.00015739
Iteration 28/1000 | Loss: 0.00022301
Iteration 29/1000 | Loss: 0.00016907
Iteration 30/1000 | Loss: 0.00017356
Iteration 31/1000 | Loss: 0.00017538
Iteration 32/1000 | Loss: 0.00017553
Iteration 33/1000 | Loss: 0.00016278
Iteration 34/1000 | Loss: 0.00008158
Iteration 35/1000 | Loss: 0.00007436
Iteration 36/1000 | Loss: 0.00015980
Iteration 37/1000 | Loss: 0.00027476
Iteration 38/1000 | Loss: 0.00044772
Iteration 39/1000 | Loss: 0.00015657
Iteration 40/1000 | Loss: 0.00034786
Iteration 41/1000 | Loss: 0.00037298
Iteration 42/1000 | Loss: 0.00016815
Iteration 43/1000 | Loss: 0.00020689
Iteration 44/1000 | Loss: 0.00016778
Iteration 45/1000 | Loss: 0.00018571
Iteration 46/1000 | Loss: 0.00030621
Iteration 47/1000 | Loss: 0.00027496
Iteration 48/1000 | Loss: 0.00020900
Iteration 49/1000 | Loss: 0.00027678
Iteration 50/1000 | Loss: 0.00025412
Iteration 51/1000 | Loss: 0.00031697
Iteration 52/1000 | Loss: 0.00016244
Iteration 53/1000 | Loss: 0.00019446
Iteration 54/1000 | Loss: 0.00016998
Iteration 55/1000 | Loss: 0.00022864
Iteration 56/1000 | Loss: 0.00024344
Iteration 57/1000 | Loss: 0.00065433
Iteration 58/1000 | Loss: 0.00044993
Iteration 59/1000 | Loss: 0.00068152
Iteration 60/1000 | Loss: 0.00059809
Iteration 61/1000 | Loss: 0.00028034
Iteration 62/1000 | Loss: 0.00028815
Iteration 63/1000 | Loss: 0.00020609
Iteration 64/1000 | Loss: 0.00019503
Iteration 65/1000 | Loss: 0.00016000
Iteration 66/1000 | Loss: 0.00015428
Iteration 67/1000 | Loss: 0.00013738
Iteration 68/1000 | Loss: 0.00023213
Iteration 69/1000 | Loss: 0.00026572
Iteration 70/1000 | Loss: 0.00024676
Iteration 71/1000 | Loss: 0.00030703
Iteration 72/1000 | Loss: 0.00023692
Iteration 73/1000 | Loss: 0.00020551
Iteration 74/1000 | Loss: 0.00065777
Iteration 75/1000 | Loss: 0.00023072
Iteration 76/1000 | Loss: 0.00020799
Iteration 77/1000 | Loss: 0.00011856
Iteration 78/1000 | Loss: 0.00012594
Iteration 79/1000 | Loss: 0.00010111
Iteration 80/1000 | Loss: 0.00012832
Iteration 81/1000 | Loss: 0.00023939
Iteration 82/1000 | Loss: 0.00018345
Iteration 83/1000 | Loss: 0.00012258
Iteration 84/1000 | Loss: 0.00020650
Iteration 85/1000 | Loss: 0.00019688
Iteration 86/1000 | Loss: 0.00021536
Iteration 87/1000 | Loss: 0.00021744
Iteration 88/1000 | Loss: 0.00023920
Iteration 89/1000 | Loss: 0.00018495
Iteration 90/1000 | Loss: 0.00015435
Iteration 91/1000 | Loss: 0.00012241
Iteration 92/1000 | Loss: 0.00048368
Iteration 93/1000 | Loss: 0.00044403
Iteration 94/1000 | Loss: 0.00067400
Iteration 95/1000 | Loss: 0.00047891
Iteration 96/1000 | Loss: 0.00072162
Iteration 97/1000 | Loss: 0.00062351
Iteration 98/1000 | Loss: 0.00013346
Iteration 99/1000 | Loss: 0.00009991
Iteration 100/1000 | Loss: 0.00008465
Iteration 101/1000 | Loss: 0.00007205
Iteration 102/1000 | Loss: 0.00018442
Iteration 103/1000 | Loss: 0.00031786
Iteration 104/1000 | Loss: 0.00013948
Iteration 105/1000 | Loss: 0.00015445
Iteration 106/1000 | Loss: 0.00012352
Iteration 107/1000 | Loss: 0.00007907
Iteration 108/1000 | Loss: 0.00010354
Iteration 109/1000 | Loss: 0.00036346
Iteration 110/1000 | Loss: 0.00010422
Iteration 111/1000 | Loss: 0.00006638
Iteration 112/1000 | Loss: 0.00006633
Iteration 113/1000 | Loss: 0.00006177
Iteration 114/1000 | Loss: 0.00005873
Iteration 115/1000 | Loss: 0.00024518
Iteration 116/1000 | Loss: 0.00012776
Iteration 117/1000 | Loss: 0.00006571
Iteration 118/1000 | Loss: 0.00007300
Iteration 119/1000 | Loss: 0.00006760
Iteration 120/1000 | Loss: 0.00004939
Iteration 121/1000 | Loss: 0.00005963
Iteration 122/1000 | Loss: 0.00006819
Iteration 123/1000 | Loss: 0.00006531
Iteration 124/1000 | Loss: 0.00005178
Iteration 125/1000 | Loss: 0.00005888
Iteration 126/1000 | Loss: 0.00006321
Iteration 127/1000 | Loss: 0.00006307
Iteration 128/1000 | Loss: 0.00007064
Iteration 129/1000 | Loss: 0.00005977
Iteration 130/1000 | Loss: 0.00004226
Iteration 131/1000 | Loss: 0.00011669
Iteration 132/1000 | Loss: 0.00017414
Iteration 133/1000 | Loss: 0.00006362
Iteration 134/1000 | Loss: 0.00005894
Iteration 135/1000 | Loss: 0.00007046
Iteration 136/1000 | Loss: 0.00006241
Iteration 137/1000 | Loss: 0.00006162
Iteration 138/1000 | Loss: 0.00006670
Iteration 139/1000 | Loss: 0.00005604
Iteration 140/1000 | Loss: 0.00006121
Iteration 141/1000 | Loss: 0.00005686
Iteration 142/1000 | Loss: 0.00006040
Iteration 143/1000 | Loss: 0.00015013
Iteration 144/1000 | Loss: 0.00007111
Iteration 145/1000 | Loss: 0.00027115
Iteration 146/1000 | Loss: 0.00027844
Iteration 147/1000 | Loss: 0.00021331
Iteration 148/1000 | Loss: 0.00004967
Iteration 149/1000 | Loss: 0.00004457
Iteration 150/1000 | Loss: 0.00004170
Iteration 151/1000 | Loss: 0.00003984
Iteration 152/1000 | Loss: 0.00003857
Iteration 153/1000 | Loss: 0.00027582
Iteration 154/1000 | Loss: 0.00004562
Iteration 155/1000 | Loss: 0.00004243
Iteration 156/1000 | Loss: 0.00027049
Iteration 157/1000 | Loss: 0.00020779
Iteration 158/1000 | Loss: 0.00018890
Iteration 159/1000 | Loss: 0.00019096
Iteration 160/1000 | Loss: 0.00016286
Iteration 161/1000 | Loss: 0.00014915
Iteration 162/1000 | Loss: 0.00004784
Iteration 163/1000 | Loss: 0.00036728
Iteration 164/1000 | Loss: 0.00008388
Iteration 165/1000 | Loss: 0.00005798
Iteration 166/1000 | Loss: 0.00004414
Iteration 167/1000 | Loss: 0.00004064
Iteration 168/1000 | Loss: 0.00003972
Iteration 169/1000 | Loss: 0.00003870
Iteration 170/1000 | Loss: 0.00003785
Iteration 171/1000 | Loss: 0.00003744
Iteration 172/1000 | Loss: 0.00008255
Iteration 173/1000 | Loss: 0.00008283
Iteration 174/1000 | Loss: 0.00003983
Iteration 175/1000 | Loss: 0.00014839
Iteration 176/1000 | Loss: 0.00017175
Iteration 177/1000 | Loss: 0.00016194
Iteration 178/1000 | Loss: 0.00013700
Iteration 179/1000 | Loss: 0.00018182
Iteration 180/1000 | Loss: 0.00012931
Iteration 181/1000 | Loss: 0.00014360
Iteration 182/1000 | Loss: 0.00004221
Iteration 183/1000 | Loss: 0.00003741
Iteration 184/1000 | Loss: 0.00003626
Iteration 185/1000 | Loss: 0.00003536
Iteration 186/1000 | Loss: 0.00003480
Iteration 187/1000 | Loss: 0.00004656
Iteration 188/1000 | Loss: 0.00003810
Iteration 189/1000 | Loss: 0.00003571
Iteration 190/1000 | Loss: 0.00003434
Iteration 191/1000 | Loss: 0.00003353
Iteration 192/1000 | Loss: 0.00003292
Iteration 193/1000 | Loss: 0.00003235
Iteration 194/1000 | Loss: 0.00003207
Iteration 195/1000 | Loss: 0.00003205
Iteration 196/1000 | Loss: 0.00026256
Iteration 197/1000 | Loss: 0.00004450
Iteration 198/1000 | Loss: 0.00003926
Iteration 199/1000 | Loss: 0.00003632
Iteration 200/1000 | Loss: 0.00003536
Iteration 201/1000 | Loss: 0.00003464
Iteration 202/1000 | Loss: 0.00003406
Iteration 203/1000 | Loss: 0.00003367
Iteration 204/1000 | Loss: 0.00003375
Iteration 205/1000 | Loss: 0.00003393
Iteration 206/1000 | Loss: 0.00003355
Iteration 207/1000 | Loss: 0.00003351
Iteration 208/1000 | Loss: 0.00003319
Iteration 209/1000 | Loss: 0.00003288
Iteration 210/1000 | Loss: 0.00003261
Iteration 211/1000 | Loss: 0.00003258
Iteration 212/1000 | Loss: 0.00003244
Iteration 213/1000 | Loss: 0.00003265
Iteration 214/1000 | Loss: 0.00003244
Iteration 215/1000 | Loss: 0.00003233
Iteration 216/1000 | Loss: 0.00003222
Iteration 217/1000 | Loss: 0.00003220
Iteration 218/1000 | Loss: 0.00003220
Iteration 219/1000 | Loss: 0.00003220
Iteration 220/1000 | Loss: 0.00003207
Iteration 221/1000 | Loss: 0.00003193
Iteration 222/1000 | Loss: 0.00003192
Iteration 223/1000 | Loss: 0.00003192
Iteration 224/1000 | Loss: 0.00003191
Iteration 225/1000 | Loss: 0.00003190
Iteration 226/1000 | Loss: 0.00003190
Iteration 227/1000 | Loss: 0.00003189
Iteration 228/1000 | Loss: 0.00003189
Iteration 229/1000 | Loss: 0.00003187
Iteration 230/1000 | Loss: 0.00003187
Iteration 231/1000 | Loss: 0.00003187
Iteration 232/1000 | Loss: 0.00003187
Iteration 233/1000 | Loss: 0.00003187
Iteration 234/1000 | Loss: 0.00003186
Iteration 235/1000 | Loss: 0.00003186
Iteration 236/1000 | Loss: 0.00003186
Iteration 237/1000 | Loss: 0.00003186
Iteration 238/1000 | Loss: 0.00003186
Iteration 239/1000 | Loss: 0.00003184
Iteration 240/1000 | Loss: 0.00003184
Iteration 241/1000 | Loss: 0.00003184
Iteration 242/1000 | Loss: 0.00003183
Iteration 243/1000 | Loss: 0.00003183
Iteration 244/1000 | Loss: 0.00003183
Iteration 245/1000 | Loss: 0.00003183
Iteration 246/1000 | Loss: 0.00003183
Iteration 247/1000 | Loss: 0.00025741
Iteration 248/1000 | Loss: 0.00032072
Iteration 249/1000 | Loss: 0.00038727
Iteration 250/1000 | Loss: 0.00007205
Iteration 251/1000 | Loss: 0.00005006
Iteration 252/1000 | Loss: 0.00020936
Iteration 253/1000 | Loss: 0.00022420
Iteration 254/1000 | Loss: 0.00021008
Iteration 255/1000 | Loss: 0.00005414
Iteration 256/1000 | Loss: 0.00004747
Iteration 257/1000 | Loss: 0.00004267
Iteration 258/1000 | Loss: 0.00003868
Iteration 259/1000 | Loss: 0.00003667
Iteration 260/1000 | Loss: 0.00003514
Iteration 261/1000 | Loss: 0.00012410
Iteration 262/1000 | Loss: 0.00011959
Iteration 263/1000 | Loss: 0.00014187
Iteration 264/1000 | Loss: 0.00008628
Iteration 265/1000 | Loss: 0.00011751
Iteration 266/1000 | Loss: 0.00009622
Iteration 267/1000 | Loss: 0.00004274
Iteration 268/1000 | Loss: 0.00010575
Iteration 269/1000 | Loss: 0.00027413
Iteration 270/1000 | Loss: 0.00010574
Iteration 271/1000 | Loss: 0.00010256
Iteration 272/1000 | Loss: 0.00011000
Iteration 273/1000 | Loss: 0.00013715
Iteration 274/1000 | Loss: 0.00024101
Iteration 275/1000 | Loss: 0.00021649
Iteration 276/1000 | Loss: 0.00018355
Iteration 277/1000 | Loss: 0.00014020
Iteration 278/1000 | Loss: 0.00012999
Iteration 279/1000 | Loss: 0.00026080
Iteration 280/1000 | Loss: 0.00013553
Iteration 281/1000 | Loss: 0.00024181
Iteration 282/1000 | Loss: 0.00021253
Iteration 283/1000 | Loss: 0.00012831
Iteration 284/1000 | Loss: 0.00013078
Iteration 285/1000 | Loss: 0.00027616
Iteration 286/1000 | Loss: 0.00003744
Iteration 287/1000 | Loss: 0.00003498
Iteration 288/1000 | Loss: 0.00003313
Iteration 289/1000 | Loss: 0.00003245
Iteration 290/1000 | Loss: 0.00003199
Iteration 291/1000 | Loss: 0.00003176
Iteration 292/1000 | Loss: 0.00003158
Iteration 293/1000 | Loss: 0.00003154
Iteration 294/1000 | Loss: 0.00003144
Iteration 295/1000 | Loss: 0.00003144
Iteration 296/1000 | Loss: 0.00003144
Iteration 297/1000 | Loss: 0.00003144
Iteration 298/1000 | Loss: 0.00003143
Iteration 299/1000 | Loss: 0.00003143
Iteration 300/1000 | Loss: 0.00003143
Iteration 301/1000 | Loss: 0.00003143
Iteration 302/1000 | Loss: 0.00003143
Iteration 303/1000 | Loss: 0.00003141
Iteration 304/1000 | Loss: 0.00003141
Iteration 305/1000 | Loss: 0.00003141
Iteration 306/1000 | Loss: 0.00003137
Iteration 307/1000 | Loss: 0.00003137
Iteration 308/1000 | Loss: 0.00003135
Iteration 309/1000 | Loss: 0.00003135
Iteration 310/1000 | Loss: 0.00003135
Iteration 311/1000 | Loss: 0.00003134
Iteration 312/1000 | Loss: 0.00003134
Iteration 313/1000 | Loss: 0.00003133
Iteration 314/1000 | Loss: 0.00003133
Iteration 315/1000 | Loss: 0.00003132
Iteration 316/1000 | Loss: 0.00003132
Iteration 317/1000 | Loss: 0.00003132
Iteration 318/1000 | Loss: 0.00003131
Iteration 319/1000 | Loss: 0.00003131
Iteration 320/1000 | Loss: 0.00003130
Iteration 321/1000 | Loss: 0.00003130
Iteration 322/1000 | Loss: 0.00003130
Iteration 323/1000 | Loss: 0.00003129
Iteration 324/1000 | Loss: 0.00003129
Iteration 325/1000 | Loss: 0.00003129
Iteration 326/1000 | Loss: 0.00003128
Iteration 327/1000 | Loss: 0.00003128
Iteration 328/1000 | Loss: 0.00003128
Iteration 329/1000 | Loss: 0.00003128
Iteration 330/1000 | Loss: 0.00003128
Iteration 331/1000 | Loss: 0.00003127
Iteration 332/1000 | Loss: 0.00003127
Iteration 333/1000 | Loss: 0.00003127
Iteration 334/1000 | Loss: 0.00003127
Iteration 335/1000 | Loss: 0.00003127
Iteration 336/1000 | Loss: 0.00003127
Iteration 337/1000 | Loss: 0.00003127
Iteration 338/1000 | Loss: 0.00003127
Iteration 339/1000 | Loss: 0.00003127
Iteration 340/1000 | Loss: 0.00003127
Iteration 341/1000 | Loss: 0.00003127
Iteration 342/1000 | Loss: 0.00003127
Iteration 343/1000 | Loss: 0.00003127
Iteration 344/1000 | Loss: 0.00003127
Iteration 345/1000 | Loss: 0.00003126
Iteration 346/1000 | Loss: 0.00003126
Iteration 347/1000 | Loss: 0.00003126
Iteration 348/1000 | Loss: 0.00003126
Iteration 349/1000 | Loss: 0.00003126
Iteration 350/1000 | Loss: 0.00003126
Iteration 351/1000 | Loss: 0.00003126
Iteration 352/1000 | Loss: 0.00003126
Iteration 353/1000 | Loss: 0.00003126
Iteration 354/1000 | Loss: 0.00003126
Iteration 355/1000 | Loss: 0.00003126
Iteration 356/1000 | Loss: 0.00003126
Iteration 357/1000 | Loss: 0.00003126
Iteration 358/1000 | Loss: 0.00003126
Iteration 359/1000 | Loss: 0.00003125
Iteration 360/1000 | Loss: 0.00003125
Iteration 361/1000 | Loss: 0.00003125
Iteration 362/1000 | Loss: 0.00003125
Iteration 363/1000 | Loss: 0.00003124
Iteration 364/1000 | Loss: 0.00003124
Iteration 365/1000 | Loss: 0.00003124
Iteration 366/1000 | Loss: 0.00003124
Iteration 367/1000 | Loss: 0.00003124
Iteration 368/1000 | Loss: 0.00003124
Iteration 369/1000 | Loss: 0.00003124
Iteration 370/1000 | Loss: 0.00003124
Iteration 371/1000 | Loss: 0.00003124
Iteration 372/1000 | Loss: 0.00003124
Iteration 373/1000 | Loss: 0.00003124
Iteration 374/1000 | Loss: 0.00003124
Iteration 375/1000 | Loss: 0.00003124
Iteration 376/1000 | Loss: 0.00003123
Iteration 377/1000 | Loss: 0.00003123
Iteration 378/1000 | Loss: 0.00003123
Iteration 379/1000 | Loss: 0.00003123
Iteration 380/1000 | Loss: 0.00003123
Iteration 381/1000 | Loss: 0.00003123
Iteration 382/1000 | Loss: 0.00003123
Iteration 383/1000 | Loss: 0.00003123
Iteration 384/1000 | Loss: 0.00003123
Iteration 385/1000 | Loss: 0.00003123
Iteration 386/1000 | Loss: 0.00003123
Iteration 387/1000 | Loss: 0.00003123
Iteration 388/1000 | Loss: 0.00003123
Iteration 389/1000 | Loss: 0.00003123
Iteration 390/1000 | Loss: 0.00003123
Iteration 391/1000 | Loss: 0.00003123
Iteration 392/1000 | Loss: 0.00003123
Iteration 393/1000 | Loss: 0.00003123
Iteration 394/1000 | Loss: 0.00003123
Iteration 395/1000 | Loss: 0.00003123
Iteration 396/1000 | Loss: 0.00003123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 396. Stopping optimization.
Last 5 losses: [3.123307760688476e-05, 3.123307760688476e-05, 3.123307760688476e-05, 3.123307760688476e-05, 3.123307760688476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.123307760688476e-05

Optimization complete. Final v2v error: 4.564261436462402 mm

Highest mean error: 11.293519020080566 mm for frame 80

Lowest mean error: 3.9516305923461914 mm for frame 66

Saving results

Total time: 488.8251898288727
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00559823
Iteration 2/25 | Loss: 0.00230467
Iteration 3/25 | Loss: 0.00214800
Iteration 4/25 | Loss: 0.00213593
Iteration 5/25 | Loss: 0.00212966
Iteration 6/25 | Loss: 0.00212831
Iteration 7/25 | Loss: 0.00212831
Iteration 8/25 | Loss: 0.00212831
Iteration 9/25 | Loss: 0.00212831
Iteration 10/25 | Loss: 0.00212831
Iteration 11/25 | Loss: 0.00212831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0021283146925270557, 0.0021283146925270557, 0.0021283146925270557, 0.0021283146925270557, 0.0021283146925270557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021283146925270557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80166441
Iteration 2/25 | Loss: 0.00220624
Iteration 3/25 | Loss: 0.00220624
Iteration 4/25 | Loss: 0.00220624
Iteration 5/25 | Loss: 0.00220624
Iteration 6/25 | Loss: 0.00220624
Iteration 7/25 | Loss: 0.00220624
Iteration 8/25 | Loss: 0.00220624
Iteration 9/25 | Loss: 0.00220624
Iteration 10/25 | Loss: 0.00220624
Iteration 11/25 | Loss: 0.00220624
Iteration 12/25 | Loss: 0.00220624
Iteration 13/25 | Loss: 0.00220624
Iteration 14/25 | Loss: 0.00220624
Iteration 15/25 | Loss: 0.00220624
Iteration 16/25 | Loss: 0.00220624
Iteration 17/25 | Loss: 0.00220624
Iteration 18/25 | Loss: 0.00220624
Iteration 19/25 | Loss: 0.00220624
Iteration 20/25 | Loss: 0.00220624
Iteration 21/25 | Loss: 0.00220624
Iteration 22/25 | Loss: 0.00220624
Iteration 23/25 | Loss: 0.00220624
Iteration 24/25 | Loss: 0.00220624
Iteration 25/25 | Loss: 0.00220624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220624
Iteration 2/1000 | Loss: 0.00010696
Iteration 3/1000 | Loss: 0.00008386
Iteration 4/1000 | Loss: 0.00007048
Iteration 5/1000 | Loss: 0.00006777
Iteration 6/1000 | Loss: 0.00006575
Iteration 7/1000 | Loss: 0.00006412
Iteration 8/1000 | Loss: 0.00006301
Iteration 9/1000 | Loss: 0.00006191
Iteration 10/1000 | Loss: 0.00006081
Iteration 11/1000 | Loss: 0.00006018
Iteration 12/1000 | Loss: 0.00005970
Iteration 13/1000 | Loss: 0.00005914
Iteration 14/1000 | Loss: 0.00005851
Iteration 15/1000 | Loss: 0.00005813
Iteration 16/1000 | Loss: 0.00005777
Iteration 17/1000 | Loss: 0.00005751
Iteration 18/1000 | Loss: 0.00005728
Iteration 19/1000 | Loss: 0.00005708
Iteration 20/1000 | Loss: 0.00005688
Iteration 21/1000 | Loss: 0.00005671
Iteration 22/1000 | Loss: 0.00005661
Iteration 23/1000 | Loss: 0.00005660
Iteration 24/1000 | Loss: 0.00005651
Iteration 25/1000 | Loss: 0.00005646
Iteration 26/1000 | Loss: 0.00005644
Iteration 27/1000 | Loss: 0.00005644
Iteration 28/1000 | Loss: 0.00005644
Iteration 29/1000 | Loss: 0.00005643
Iteration 30/1000 | Loss: 0.00005642
Iteration 31/1000 | Loss: 0.00005635
Iteration 32/1000 | Loss: 0.00005634
Iteration 33/1000 | Loss: 0.00005633
Iteration 34/1000 | Loss: 0.00005632
Iteration 35/1000 | Loss: 0.00005629
Iteration 36/1000 | Loss: 0.00005629
Iteration 37/1000 | Loss: 0.00005626
Iteration 38/1000 | Loss: 0.00005613
Iteration 39/1000 | Loss: 0.00005611
Iteration 40/1000 | Loss: 0.00005611
Iteration 41/1000 | Loss: 0.00005611
Iteration 42/1000 | Loss: 0.00005611
Iteration 43/1000 | Loss: 0.00005611
Iteration 44/1000 | Loss: 0.00005611
Iteration 45/1000 | Loss: 0.00005611
Iteration 46/1000 | Loss: 0.00005611
Iteration 47/1000 | Loss: 0.00005611
Iteration 48/1000 | Loss: 0.00005610
Iteration 49/1000 | Loss: 0.00005610
Iteration 50/1000 | Loss: 0.00005610
Iteration 51/1000 | Loss: 0.00005610
Iteration 52/1000 | Loss: 0.00005610
Iteration 53/1000 | Loss: 0.00005610
Iteration 54/1000 | Loss: 0.00005609
Iteration 55/1000 | Loss: 0.00005609
Iteration 56/1000 | Loss: 0.00005609
Iteration 57/1000 | Loss: 0.00005609
Iteration 58/1000 | Loss: 0.00005608
Iteration 59/1000 | Loss: 0.00005607
Iteration 60/1000 | Loss: 0.00005607
Iteration 61/1000 | Loss: 0.00005607
Iteration 62/1000 | Loss: 0.00005607
Iteration 63/1000 | Loss: 0.00005607
Iteration 64/1000 | Loss: 0.00005607
Iteration 65/1000 | Loss: 0.00005607
Iteration 66/1000 | Loss: 0.00005607
Iteration 67/1000 | Loss: 0.00005606
Iteration 68/1000 | Loss: 0.00005605
Iteration 69/1000 | Loss: 0.00005605
Iteration 70/1000 | Loss: 0.00005604
Iteration 71/1000 | Loss: 0.00005604
Iteration 72/1000 | Loss: 0.00005604
Iteration 73/1000 | Loss: 0.00005604
Iteration 74/1000 | Loss: 0.00005604
Iteration 75/1000 | Loss: 0.00005604
Iteration 76/1000 | Loss: 0.00005604
Iteration 77/1000 | Loss: 0.00005604
Iteration 78/1000 | Loss: 0.00005604
Iteration 79/1000 | Loss: 0.00005604
Iteration 80/1000 | Loss: 0.00005603
Iteration 81/1000 | Loss: 0.00005603
Iteration 82/1000 | Loss: 0.00005603
Iteration 83/1000 | Loss: 0.00005603
Iteration 84/1000 | Loss: 0.00005603
Iteration 85/1000 | Loss: 0.00005603
Iteration 86/1000 | Loss: 0.00005603
Iteration 87/1000 | Loss: 0.00005602
Iteration 88/1000 | Loss: 0.00005602
Iteration 89/1000 | Loss: 0.00005602
Iteration 90/1000 | Loss: 0.00005602
Iteration 91/1000 | Loss: 0.00005602
Iteration 92/1000 | Loss: 0.00005602
Iteration 93/1000 | Loss: 0.00005602
Iteration 94/1000 | Loss: 0.00005602
Iteration 95/1000 | Loss: 0.00005602
Iteration 96/1000 | Loss: 0.00005602
Iteration 97/1000 | Loss: 0.00005602
Iteration 98/1000 | Loss: 0.00005602
Iteration 99/1000 | Loss: 0.00005602
Iteration 100/1000 | Loss: 0.00005601
Iteration 101/1000 | Loss: 0.00005601
Iteration 102/1000 | Loss: 0.00005601
Iteration 103/1000 | Loss: 0.00005601
Iteration 104/1000 | Loss: 0.00005601
Iteration 105/1000 | Loss: 0.00005601
Iteration 106/1000 | Loss: 0.00005601
Iteration 107/1000 | Loss: 0.00005601
Iteration 108/1000 | Loss: 0.00005601
Iteration 109/1000 | Loss: 0.00005601
Iteration 110/1000 | Loss: 0.00005601
Iteration 111/1000 | Loss: 0.00005601
Iteration 112/1000 | Loss: 0.00005601
Iteration 113/1000 | Loss: 0.00005601
Iteration 114/1000 | Loss: 0.00005601
Iteration 115/1000 | Loss: 0.00005601
Iteration 116/1000 | Loss: 0.00005601
Iteration 117/1000 | Loss: 0.00005601
Iteration 118/1000 | Loss: 0.00005601
Iteration 119/1000 | Loss: 0.00005601
Iteration 120/1000 | Loss: 0.00005601
Iteration 121/1000 | Loss: 0.00005601
Iteration 122/1000 | Loss: 0.00005601
Iteration 123/1000 | Loss: 0.00005601
Iteration 124/1000 | Loss: 0.00005601
Iteration 125/1000 | Loss: 0.00005601
Iteration 126/1000 | Loss: 0.00005601
Iteration 127/1000 | Loss: 0.00005601
Iteration 128/1000 | Loss: 0.00005601
Iteration 129/1000 | Loss: 0.00005601
Iteration 130/1000 | Loss: 0.00005601
Iteration 131/1000 | Loss: 0.00005601
Iteration 132/1000 | Loss: 0.00005601
Iteration 133/1000 | Loss: 0.00005601
Iteration 134/1000 | Loss: 0.00005601
Iteration 135/1000 | Loss: 0.00005601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [5.600722215604037e-05, 5.600722215604037e-05, 5.600722215604037e-05, 5.600722215604037e-05, 5.600722215604037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.600722215604037e-05

Optimization complete. Final v2v error: 6.290248394012451 mm

Highest mean error: 6.408276081085205 mm for frame 92

Lowest mean error: 6.176248073577881 mm for frame 4

Saving results

Total time: 60.02356004714966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080615
Iteration 2/25 | Loss: 0.00254506
Iteration 3/25 | Loss: 0.00220383
Iteration 4/25 | Loss: 0.00218327
Iteration 5/25 | Loss: 0.00217729
Iteration 6/25 | Loss: 0.00217551
Iteration 7/25 | Loss: 0.00217551
Iteration 8/25 | Loss: 0.00217551
Iteration 9/25 | Loss: 0.00217551
Iteration 10/25 | Loss: 0.00217551
Iteration 11/25 | Loss: 0.00217551
Iteration 12/25 | Loss: 0.00217551
Iteration 13/25 | Loss: 0.00217551
Iteration 14/25 | Loss: 0.00217551
Iteration 15/25 | Loss: 0.00217551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0021755057387053967, 0.0021755057387053967, 0.0021755057387053967, 0.0021755057387053967, 0.0021755057387053967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021755057387053967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00590217
Iteration 2/25 | Loss: 0.00192228
Iteration 3/25 | Loss: 0.00192227
Iteration 4/25 | Loss: 0.00192227
Iteration 5/25 | Loss: 0.00192227
Iteration 6/25 | Loss: 0.00192227
Iteration 7/25 | Loss: 0.00192227
Iteration 8/25 | Loss: 0.00192227
Iteration 9/25 | Loss: 0.00192227
Iteration 10/25 | Loss: 0.00192227
Iteration 11/25 | Loss: 0.00192227
Iteration 12/25 | Loss: 0.00192227
Iteration 13/25 | Loss: 0.00192227
Iteration 14/25 | Loss: 0.00192227
Iteration 15/25 | Loss: 0.00192227
Iteration 16/25 | Loss: 0.00192227
Iteration 17/25 | Loss: 0.00192227
Iteration 18/25 | Loss: 0.00192227
Iteration 19/25 | Loss: 0.00192227
Iteration 20/25 | Loss: 0.00192227
Iteration 21/25 | Loss: 0.00192227
Iteration 22/25 | Loss: 0.00192227
Iteration 23/25 | Loss: 0.00192227
Iteration 24/25 | Loss: 0.00192227
Iteration 25/25 | Loss: 0.00192227
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001922266324982047, 0.001922266324982047, 0.001922266324982047, 0.001922266324982047, 0.001922266324982047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001922266324982047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192227
Iteration 2/1000 | Loss: 0.00010889
Iteration 3/1000 | Loss: 0.00007754
Iteration 4/1000 | Loss: 0.00006832
Iteration 5/1000 | Loss: 0.00006197
Iteration 6/1000 | Loss: 0.00005951
Iteration 7/1000 | Loss: 0.00005748
Iteration 8/1000 | Loss: 0.00005617
Iteration 9/1000 | Loss: 0.00005513
Iteration 10/1000 | Loss: 0.00005432
Iteration 11/1000 | Loss: 0.00005384
Iteration 12/1000 | Loss: 0.00005338
Iteration 13/1000 | Loss: 0.00005299
Iteration 14/1000 | Loss: 0.00005271
Iteration 15/1000 | Loss: 0.00005248
Iteration 16/1000 | Loss: 0.00005222
Iteration 17/1000 | Loss: 0.00005201
Iteration 18/1000 | Loss: 0.00005187
Iteration 19/1000 | Loss: 0.00005177
Iteration 20/1000 | Loss: 0.00005174
Iteration 21/1000 | Loss: 0.00005173
Iteration 22/1000 | Loss: 0.00005173
Iteration 23/1000 | Loss: 0.00005171
Iteration 24/1000 | Loss: 0.00005170
Iteration 25/1000 | Loss: 0.00005170
Iteration 26/1000 | Loss: 0.00005169
Iteration 27/1000 | Loss: 0.00005169
Iteration 28/1000 | Loss: 0.00005165
Iteration 29/1000 | Loss: 0.00005165
Iteration 30/1000 | Loss: 0.00005164
Iteration 31/1000 | Loss: 0.00005163
Iteration 32/1000 | Loss: 0.00005163
Iteration 33/1000 | Loss: 0.00005162
Iteration 34/1000 | Loss: 0.00005160
Iteration 35/1000 | Loss: 0.00005160
Iteration 36/1000 | Loss: 0.00005160
Iteration 37/1000 | Loss: 0.00005160
Iteration 38/1000 | Loss: 0.00005160
Iteration 39/1000 | Loss: 0.00005160
Iteration 40/1000 | Loss: 0.00005160
Iteration 41/1000 | Loss: 0.00005159
Iteration 42/1000 | Loss: 0.00005159
Iteration 43/1000 | Loss: 0.00005159
Iteration 44/1000 | Loss: 0.00005159
Iteration 45/1000 | Loss: 0.00005159
Iteration 46/1000 | Loss: 0.00005159
Iteration 47/1000 | Loss: 0.00005159
Iteration 48/1000 | Loss: 0.00005158
Iteration 49/1000 | Loss: 0.00005157
Iteration 50/1000 | Loss: 0.00005157
Iteration 51/1000 | Loss: 0.00005156
Iteration 52/1000 | Loss: 0.00005156
Iteration 53/1000 | Loss: 0.00005155
Iteration 54/1000 | Loss: 0.00005153
Iteration 55/1000 | Loss: 0.00005151
Iteration 56/1000 | Loss: 0.00005151
Iteration 57/1000 | Loss: 0.00005150
Iteration 58/1000 | Loss: 0.00005150
Iteration 59/1000 | Loss: 0.00005149
Iteration 60/1000 | Loss: 0.00005149
Iteration 61/1000 | Loss: 0.00005149
Iteration 62/1000 | Loss: 0.00005149
Iteration 63/1000 | Loss: 0.00005148
Iteration 64/1000 | Loss: 0.00005148
Iteration 65/1000 | Loss: 0.00005148
Iteration 66/1000 | Loss: 0.00005148
Iteration 67/1000 | Loss: 0.00005146
Iteration 68/1000 | Loss: 0.00005146
Iteration 69/1000 | Loss: 0.00005146
Iteration 70/1000 | Loss: 0.00005146
Iteration 71/1000 | Loss: 0.00005146
Iteration 72/1000 | Loss: 0.00005146
Iteration 73/1000 | Loss: 0.00005146
Iteration 74/1000 | Loss: 0.00005146
Iteration 75/1000 | Loss: 0.00005146
Iteration 76/1000 | Loss: 0.00005146
Iteration 77/1000 | Loss: 0.00005146
Iteration 78/1000 | Loss: 0.00005145
Iteration 79/1000 | Loss: 0.00005145
Iteration 80/1000 | Loss: 0.00005145
Iteration 81/1000 | Loss: 0.00005145
Iteration 82/1000 | Loss: 0.00005145
Iteration 83/1000 | Loss: 0.00005145
Iteration 84/1000 | Loss: 0.00005145
Iteration 85/1000 | Loss: 0.00005145
Iteration 86/1000 | Loss: 0.00005145
Iteration 87/1000 | Loss: 0.00005144
Iteration 88/1000 | Loss: 0.00005144
Iteration 89/1000 | Loss: 0.00005144
Iteration 90/1000 | Loss: 0.00005143
Iteration 91/1000 | Loss: 0.00005143
Iteration 92/1000 | Loss: 0.00005142
Iteration 93/1000 | Loss: 0.00005142
Iteration 94/1000 | Loss: 0.00005142
Iteration 95/1000 | Loss: 0.00005142
Iteration 96/1000 | Loss: 0.00005142
Iteration 97/1000 | Loss: 0.00005142
Iteration 98/1000 | Loss: 0.00005142
Iteration 99/1000 | Loss: 0.00005142
Iteration 100/1000 | Loss: 0.00005141
Iteration 101/1000 | Loss: 0.00005141
Iteration 102/1000 | Loss: 0.00005141
Iteration 103/1000 | Loss: 0.00005141
Iteration 104/1000 | Loss: 0.00005141
Iteration 105/1000 | Loss: 0.00005141
Iteration 106/1000 | Loss: 0.00005141
Iteration 107/1000 | Loss: 0.00005140
Iteration 108/1000 | Loss: 0.00005140
Iteration 109/1000 | Loss: 0.00005140
Iteration 110/1000 | Loss: 0.00005139
Iteration 111/1000 | Loss: 0.00005139
Iteration 112/1000 | Loss: 0.00005139
Iteration 113/1000 | Loss: 0.00005139
Iteration 114/1000 | Loss: 0.00005139
Iteration 115/1000 | Loss: 0.00005139
Iteration 116/1000 | Loss: 0.00005139
Iteration 117/1000 | Loss: 0.00005138
Iteration 118/1000 | Loss: 0.00005138
Iteration 119/1000 | Loss: 0.00005138
Iteration 120/1000 | Loss: 0.00005138
Iteration 121/1000 | Loss: 0.00005138
Iteration 122/1000 | Loss: 0.00005138
Iteration 123/1000 | Loss: 0.00005137
Iteration 124/1000 | Loss: 0.00005137
Iteration 125/1000 | Loss: 0.00005137
Iteration 126/1000 | Loss: 0.00005137
Iteration 127/1000 | Loss: 0.00005137
Iteration 128/1000 | Loss: 0.00005137
Iteration 129/1000 | Loss: 0.00005136
Iteration 130/1000 | Loss: 0.00005136
Iteration 131/1000 | Loss: 0.00005136
Iteration 132/1000 | Loss: 0.00005136
Iteration 133/1000 | Loss: 0.00005136
Iteration 134/1000 | Loss: 0.00005136
Iteration 135/1000 | Loss: 0.00005136
Iteration 136/1000 | Loss: 0.00005136
Iteration 137/1000 | Loss: 0.00005136
Iteration 138/1000 | Loss: 0.00005136
Iteration 139/1000 | Loss: 0.00005136
Iteration 140/1000 | Loss: 0.00005135
Iteration 141/1000 | Loss: 0.00005135
Iteration 142/1000 | Loss: 0.00005135
Iteration 143/1000 | Loss: 0.00005135
Iteration 144/1000 | Loss: 0.00005135
Iteration 145/1000 | Loss: 0.00005135
Iteration 146/1000 | Loss: 0.00005135
Iteration 147/1000 | Loss: 0.00005135
Iteration 148/1000 | Loss: 0.00005134
Iteration 149/1000 | Loss: 0.00005134
Iteration 150/1000 | Loss: 0.00005134
Iteration 151/1000 | Loss: 0.00005134
Iteration 152/1000 | Loss: 0.00005134
Iteration 153/1000 | Loss: 0.00005134
Iteration 154/1000 | Loss: 0.00005133
Iteration 155/1000 | Loss: 0.00005133
Iteration 156/1000 | Loss: 0.00005133
Iteration 157/1000 | Loss: 0.00005133
Iteration 158/1000 | Loss: 0.00005133
Iteration 159/1000 | Loss: 0.00005133
Iteration 160/1000 | Loss: 0.00005133
Iteration 161/1000 | Loss: 0.00005133
Iteration 162/1000 | Loss: 0.00005133
Iteration 163/1000 | Loss: 0.00005132
Iteration 164/1000 | Loss: 0.00005132
Iteration 165/1000 | Loss: 0.00005132
Iteration 166/1000 | Loss: 0.00005132
Iteration 167/1000 | Loss: 0.00005132
Iteration 168/1000 | Loss: 0.00005132
Iteration 169/1000 | Loss: 0.00005131
Iteration 170/1000 | Loss: 0.00005131
Iteration 171/1000 | Loss: 0.00005131
Iteration 172/1000 | Loss: 0.00005131
Iteration 173/1000 | Loss: 0.00005131
Iteration 174/1000 | Loss: 0.00005131
Iteration 175/1000 | Loss: 0.00005131
Iteration 176/1000 | Loss: 0.00005131
Iteration 177/1000 | Loss: 0.00005131
Iteration 178/1000 | Loss: 0.00005131
Iteration 179/1000 | Loss: 0.00005131
Iteration 180/1000 | Loss: 0.00005131
Iteration 181/1000 | Loss: 0.00005131
Iteration 182/1000 | Loss: 0.00005131
Iteration 183/1000 | Loss: 0.00005131
Iteration 184/1000 | Loss: 0.00005131
Iteration 185/1000 | Loss: 0.00005131
Iteration 186/1000 | Loss: 0.00005130
Iteration 187/1000 | Loss: 0.00005130
Iteration 188/1000 | Loss: 0.00005130
Iteration 189/1000 | Loss: 0.00005130
Iteration 190/1000 | Loss: 0.00005130
Iteration 191/1000 | Loss: 0.00005130
Iteration 192/1000 | Loss: 0.00005130
Iteration 193/1000 | Loss: 0.00005130
Iteration 194/1000 | Loss: 0.00005130
Iteration 195/1000 | Loss: 0.00005130
Iteration 196/1000 | Loss: 0.00005130
Iteration 197/1000 | Loss: 0.00005129
Iteration 198/1000 | Loss: 0.00005129
Iteration 199/1000 | Loss: 0.00005129
Iteration 200/1000 | Loss: 0.00005129
Iteration 201/1000 | Loss: 0.00005129
Iteration 202/1000 | Loss: 0.00005129
Iteration 203/1000 | Loss: 0.00005129
Iteration 204/1000 | Loss: 0.00005129
Iteration 205/1000 | Loss: 0.00005129
Iteration 206/1000 | Loss: 0.00005129
Iteration 207/1000 | Loss: 0.00005129
Iteration 208/1000 | Loss: 0.00005129
Iteration 209/1000 | Loss: 0.00005129
Iteration 210/1000 | Loss: 0.00005129
Iteration 211/1000 | Loss: 0.00005129
Iteration 212/1000 | Loss: 0.00005129
Iteration 213/1000 | Loss: 0.00005129
Iteration 214/1000 | Loss: 0.00005129
Iteration 215/1000 | Loss: 0.00005129
Iteration 216/1000 | Loss: 0.00005129
Iteration 217/1000 | Loss: 0.00005129
Iteration 218/1000 | Loss: 0.00005129
Iteration 219/1000 | Loss: 0.00005129
Iteration 220/1000 | Loss: 0.00005129
Iteration 221/1000 | Loss: 0.00005129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [5.128643897478469e-05, 5.128643897478469e-05, 5.128643897478469e-05, 5.128643897478469e-05, 5.128643897478469e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.128643897478469e-05

Optimization complete. Final v2v error: 5.994804859161377 mm

Highest mean error: 6.592369079589844 mm for frame 90

Lowest mean error: 5.522254943847656 mm for frame 0

Saving results

Total time: 50.78649377822876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997673
Iteration 2/25 | Loss: 0.00241768
Iteration 3/25 | Loss: 0.00217277
Iteration 4/25 | Loss: 0.00214699
Iteration 5/25 | Loss: 0.00213470
Iteration 6/25 | Loss: 0.00212964
Iteration 7/25 | Loss: 0.00212781
Iteration 8/25 | Loss: 0.00212734
Iteration 9/25 | Loss: 0.00212734
Iteration 10/25 | Loss: 0.00212734
Iteration 11/25 | Loss: 0.00212734
Iteration 12/25 | Loss: 0.00212734
Iteration 13/25 | Loss: 0.00212734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.002127339830622077, 0.002127339830622077, 0.002127339830622077, 0.002127339830622077, 0.002127339830622077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002127339830622077

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38516021
Iteration 2/25 | Loss: 0.00202041
Iteration 3/25 | Loss: 0.00202041
Iteration 4/25 | Loss: 0.00202041
Iteration 5/25 | Loss: 0.00202041
Iteration 6/25 | Loss: 0.00202041
Iteration 7/25 | Loss: 0.00202041
Iteration 8/25 | Loss: 0.00202041
Iteration 9/25 | Loss: 0.00202041
Iteration 10/25 | Loss: 0.00202041
Iteration 11/25 | Loss: 0.00202041
Iteration 12/25 | Loss: 0.00202041
Iteration 13/25 | Loss: 0.00202041
Iteration 14/25 | Loss: 0.00202041
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0020204074680805206, 0.0020204074680805206, 0.0020204074680805206, 0.0020204074680805206, 0.0020204074680805206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020204074680805206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202041
Iteration 2/1000 | Loss: 0.00014952
Iteration 3/1000 | Loss: 0.00007760
Iteration 4/1000 | Loss: 0.00006624
Iteration 5/1000 | Loss: 0.00006132
Iteration 6/1000 | Loss: 0.00005716
Iteration 7/1000 | Loss: 0.00005543
Iteration 8/1000 | Loss: 0.00005410
Iteration 9/1000 | Loss: 0.00005310
Iteration 10/1000 | Loss: 0.00005206
Iteration 11/1000 | Loss: 0.00005133
Iteration 12/1000 | Loss: 0.00005070
Iteration 13/1000 | Loss: 0.00004993
Iteration 14/1000 | Loss: 0.00004933
Iteration 15/1000 | Loss: 0.00004894
Iteration 16/1000 | Loss: 0.00004863
Iteration 17/1000 | Loss: 0.00004838
Iteration 18/1000 | Loss: 0.00004830
Iteration 19/1000 | Loss: 0.00004815
Iteration 20/1000 | Loss: 0.00004805
Iteration 21/1000 | Loss: 0.00004799
Iteration 22/1000 | Loss: 0.00004785
Iteration 23/1000 | Loss: 0.00004782
Iteration 24/1000 | Loss: 0.00004782
Iteration 25/1000 | Loss: 0.00004781
Iteration 26/1000 | Loss: 0.00004776
Iteration 27/1000 | Loss: 0.00004776
Iteration 28/1000 | Loss: 0.00004776
Iteration 29/1000 | Loss: 0.00004776
Iteration 30/1000 | Loss: 0.00004776
Iteration 31/1000 | Loss: 0.00004776
Iteration 32/1000 | Loss: 0.00004776
Iteration 33/1000 | Loss: 0.00004776
Iteration 34/1000 | Loss: 0.00004776
Iteration 35/1000 | Loss: 0.00004775
Iteration 36/1000 | Loss: 0.00004775
Iteration 37/1000 | Loss: 0.00004775
Iteration 38/1000 | Loss: 0.00004775
Iteration 39/1000 | Loss: 0.00004774
Iteration 40/1000 | Loss: 0.00004773
Iteration 41/1000 | Loss: 0.00004772
Iteration 42/1000 | Loss: 0.00004772
Iteration 43/1000 | Loss: 0.00004772
Iteration 44/1000 | Loss: 0.00004771
Iteration 45/1000 | Loss: 0.00004771
Iteration 46/1000 | Loss: 0.00004771
Iteration 47/1000 | Loss: 0.00004771
Iteration 48/1000 | Loss: 0.00004771
Iteration 49/1000 | Loss: 0.00004771
Iteration 50/1000 | Loss: 0.00004770
Iteration 51/1000 | Loss: 0.00004770
Iteration 52/1000 | Loss: 0.00004770
Iteration 53/1000 | Loss: 0.00004770
Iteration 54/1000 | Loss: 0.00004770
Iteration 55/1000 | Loss: 0.00004769
Iteration 56/1000 | Loss: 0.00004769
Iteration 57/1000 | Loss: 0.00004769
Iteration 58/1000 | Loss: 0.00004769
Iteration 59/1000 | Loss: 0.00004769
Iteration 60/1000 | Loss: 0.00004768
Iteration 61/1000 | Loss: 0.00004768
Iteration 62/1000 | Loss: 0.00004768
Iteration 63/1000 | Loss: 0.00004768
Iteration 64/1000 | Loss: 0.00004768
Iteration 65/1000 | Loss: 0.00004768
Iteration 66/1000 | Loss: 0.00004768
Iteration 67/1000 | Loss: 0.00004768
Iteration 68/1000 | Loss: 0.00004768
Iteration 69/1000 | Loss: 0.00004768
Iteration 70/1000 | Loss: 0.00004768
Iteration 71/1000 | Loss: 0.00004768
Iteration 72/1000 | Loss: 0.00004768
Iteration 73/1000 | Loss: 0.00004767
Iteration 74/1000 | Loss: 0.00004767
Iteration 75/1000 | Loss: 0.00004767
Iteration 76/1000 | Loss: 0.00004767
Iteration 77/1000 | Loss: 0.00004767
Iteration 78/1000 | Loss: 0.00004767
Iteration 79/1000 | Loss: 0.00004766
Iteration 80/1000 | Loss: 0.00004766
Iteration 81/1000 | Loss: 0.00004766
Iteration 82/1000 | Loss: 0.00004765
Iteration 83/1000 | Loss: 0.00004765
Iteration 84/1000 | Loss: 0.00004765
Iteration 85/1000 | Loss: 0.00004764
Iteration 86/1000 | Loss: 0.00004764
Iteration 87/1000 | Loss: 0.00004764
Iteration 88/1000 | Loss: 0.00004764
Iteration 89/1000 | Loss: 0.00004763
Iteration 90/1000 | Loss: 0.00004763
Iteration 91/1000 | Loss: 0.00004763
Iteration 92/1000 | Loss: 0.00004763
Iteration 93/1000 | Loss: 0.00004763
Iteration 94/1000 | Loss: 0.00004763
Iteration 95/1000 | Loss: 0.00004763
Iteration 96/1000 | Loss: 0.00004762
Iteration 97/1000 | Loss: 0.00004762
Iteration 98/1000 | Loss: 0.00004762
Iteration 99/1000 | Loss: 0.00004762
Iteration 100/1000 | Loss: 0.00004761
Iteration 101/1000 | Loss: 0.00004761
Iteration 102/1000 | Loss: 0.00004761
Iteration 103/1000 | Loss: 0.00004761
Iteration 104/1000 | Loss: 0.00004761
Iteration 105/1000 | Loss: 0.00004761
Iteration 106/1000 | Loss: 0.00004761
Iteration 107/1000 | Loss: 0.00004761
Iteration 108/1000 | Loss: 0.00004760
Iteration 109/1000 | Loss: 0.00004760
Iteration 110/1000 | Loss: 0.00004760
Iteration 111/1000 | Loss: 0.00004760
Iteration 112/1000 | Loss: 0.00004759
Iteration 113/1000 | Loss: 0.00004759
Iteration 114/1000 | Loss: 0.00004759
Iteration 115/1000 | Loss: 0.00004759
Iteration 116/1000 | Loss: 0.00004759
Iteration 117/1000 | Loss: 0.00004759
Iteration 118/1000 | Loss: 0.00004758
Iteration 119/1000 | Loss: 0.00004758
Iteration 120/1000 | Loss: 0.00004758
Iteration 121/1000 | Loss: 0.00004758
Iteration 122/1000 | Loss: 0.00004758
Iteration 123/1000 | Loss: 0.00004758
Iteration 124/1000 | Loss: 0.00004758
Iteration 125/1000 | Loss: 0.00004758
Iteration 126/1000 | Loss: 0.00004758
Iteration 127/1000 | Loss: 0.00004758
Iteration 128/1000 | Loss: 0.00004758
Iteration 129/1000 | Loss: 0.00004758
Iteration 130/1000 | Loss: 0.00004758
Iteration 131/1000 | Loss: 0.00004757
Iteration 132/1000 | Loss: 0.00004757
Iteration 133/1000 | Loss: 0.00004757
Iteration 134/1000 | Loss: 0.00004757
Iteration 135/1000 | Loss: 0.00004757
Iteration 136/1000 | Loss: 0.00004757
Iteration 137/1000 | Loss: 0.00004756
Iteration 138/1000 | Loss: 0.00004756
Iteration 139/1000 | Loss: 0.00004756
Iteration 140/1000 | Loss: 0.00004756
Iteration 141/1000 | Loss: 0.00004756
Iteration 142/1000 | Loss: 0.00004756
Iteration 143/1000 | Loss: 0.00004756
Iteration 144/1000 | Loss: 0.00004756
Iteration 145/1000 | Loss: 0.00004755
Iteration 146/1000 | Loss: 0.00004755
Iteration 147/1000 | Loss: 0.00004755
Iteration 148/1000 | Loss: 0.00004755
Iteration 149/1000 | Loss: 0.00004755
Iteration 150/1000 | Loss: 0.00004755
Iteration 151/1000 | Loss: 0.00004755
Iteration 152/1000 | Loss: 0.00004755
Iteration 153/1000 | Loss: 0.00004755
Iteration 154/1000 | Loss: 0.00004755
Iteration 155/1000 | Loss: 0.00004755
Iteration 156/1000 | Loss: 0.00004755
Iteration 157/1000 | Loss: 0.00004755
Iteration 158/1000 | Loss: 0.00004755
Iteration 159/1000 | Loss: 0.00004755
Iteration 160/1000 | Loss: 0.00004755
Iteration 161/1000 | Loss: 0.00004755
Iteration 162/1000 | Loss: 0.00004755
Iteration 163/1000 | Loss: 0.00004755
Iteration 164/1000 | Loss: 0.00004755
Iteration 165/1000 | Loss: 0.00004755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [4.754889232572168e-05, 4.754889232572168e-05, 4.754889232572168e-05, 4.754889232572168e-05, 4.754889232572168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.754889232572168e-05

Optimization complete. Final v2v error: 5.7784810066223145 mm

Highest mean error: 7.124820232391357 mm for frame 96

Lowest mean error: 5.008388042449951 mm for frame 134

Saving results

Total time: 49.72621464729309
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894976
Iteration 2/25 | Loss: 0.00249661
Iteration 3/25 | Loss: 0.00219393
Iteration 4/25 | Loss: 0.00216627
Iteration 5/25 | Loss: 0.00219974
Iteration 6/25 | Loss: 0.00218275
Iteration 7/25 | Loss: 0.00215387
Iteration 8/25 | Loss: 0.00215057
Iteration 9/25 | Loss: 0.00214999
Iteration 10/25 | Loss: 0.00214943
Iteration 11/25 | Loss: 0.00214928
Iteration 12/25 | Loss: 0.00214927
Iteration 13/25 | Loss: 0.00214927
Iteration 14/25 | Loss: 0.00214926
Iteration 15/25 | Loss: 0.00214926
Iteration 16/25 | Loss: 0.00214926
Iteration 17/25 | Loss: 0.00214926
Iteration 18/25 | Loss: 0.00214926
Iteration 19/25 | Loss: 0.00214926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0021492631640285254, 0.0021492631640285254, 0.0021492631640285254, 0.0021492631640285254, 0.0021492631640285254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021492631640285254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.60918045
Iteration 2/25 | Loss: 0.00201623
Iteration 3/25 | Loss: 0.00201621
Iteration 4/25 | Loss: 0.00201621
Iteration 5/25 | Loss: 0.00201621
Iteration 6/25 | Loss: 0.00201621
Iteration 7/25 | Loss: 0.00201620
Iteration 8/25 | Loss: 0.00201620
Iteration 9/25 | Loss: 0.00201620
Iteration 10/25 | Loss: 0.00201620
Iteration 11/25 | Loss: 0.00201620
Iteration 12/25 | Loss: 0.00201620
Iteration 13/25 | Loss: 0.00201620
Iteration 14/25 | Loss: 0.00201620
Iteration 15/25 | Loss: 0.00201620
Iteration 16/25 | Loss: 0.00201620
Iteration 17/25 | Loss: 0.00201620
Iteration 18/25 | Loss: 0.00201620
Iteration 19/25 | Loss: 0.00201620
Iteration 20/25 | Loss: 0.00201620
Iteration 21/25 | Loss: 0.00201620
Iteration 22/25 | Loss: 0.00201620
Iteration 23/25 | Loss: 0.00201620
Iteration 24/25 | Loss: 0.00201620
Iteration 25/25 | Loss: 0.00201620

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201620
Iteration 2/1000 | Loss: 0.00008014
Iteration 3/1000 | Loss: 0.00005932
Iteration 4/1000 | Loss: 0.00005387
Iteration 5/1000 | Loss: 0.00005119
Iteration 6/1000 | Loss: 0.00004961
Iteration 7/1000 | Loss: 0.00004875
Iteration 8/1000 | Loss: 0.00004808
Iteration 9/1000 | Loss: 0.00004756
Iteration 10/1000 | Loss: 0.00004717
Iteration 11/1000 | Loss: 0.00004685
Iteration 12/1000 | Loss: 0.00004666
Iteration 13/1000 | Loss: 0.00004659
Iteration 14/1000 | Loss: 0.00004645
Iteration 15/1000 | Loss: 0.00004636
Iteration 16/1000 | Loss: 0.00004636
Iteration 17/1000 | Loss: 0.00004633
Iteration 18/1000 | Loss: 0.00004633
Iteration 19/1000 | Loss: 0.00004632
Iteration 20/1000 | Loss: 0.00004632
Iteration 21/1000 | Loss: 0.00004631
Iteration 22/1000 | Loss: 0.00004629
Iteration 23/1000 | Loss: 0.00004629
Iteration 24/1000 | Loss: 0.00004629
Iteration 25/1000 | Loss: 0.00004629
Iteration 26/1000 | Loss: 0.00004629
Iteration 27/1000 | Loss: 0.00004628
Iteration 28/1000 | Loss: 0.00004628
Iteration 29/1000 | Loss: 0.00004628
Iteration 30/1000 | Loss: 0.00004628
Iteration 31/1000 | Loss: 0.00004626
Iteration 32/1000 | Loss: 0.00004626
Iteration 33/1000 | Loss: 0.00004626
Iteration 34/1000 | Loss: 0.00004626
Iteration 35/1000 | Loss: 0.00004625
Iteration 36/1000 | Loss: 0.00004625
Iteration 37/1000 | Loss: 0.00004625
Iteration 38/1000 | Loss: 0.00004625
Iteration 39/1000 | Loss: 0.00004623
Iteration 40/1000 | Loss: 0.00004623
Iteration 41/1000 | Loss: 0.00004622
Iteration 42/1000 | Loss: 0.00004622
Iteration 43/1000 | Loss: 0.00004622
Iteration 44/1000 | Loss: 0.00004621
Iteration 45/1000 | Loss: 0.00004621
Iteration 46/1000 | Loss: 0.00004621
Iteration 47/1000 | Loss: 0.00004621
Iteration 48/1000 | Loss: 0.00004620
Iteration 49/1000 | Loss: 0.00004620
Iteration 50/1000 | Loss: 0.00004620
Iteration 51/1000 | Loss: 0.00004620
Iteration 52/1000 | Loss: 0.00004619
Iteration 53/1000 | Loss: 0.00004619
Iteration 54/1000 | Loss: 0.00004618
Iteration 55/1000 | Loss: 0.00004618
Iteration 56/1000 | Loss: 0.00004618
Iteration 57/1000 | Loss: 0.00004618
Iteration 58/1000 | Loss: 0.00004617
Iteration 59/1000 | Loss: 0.00004617
Iteration 60/1000 | Loss: 0.00004617
Iteration 61/1000 | Loss: 0.00004617
Iteration 62/1000 | Loss: 0.00004616
Iteration 63/1000 | Loss: 0.00004616
Iteration 64/1000 | Loss: 0.00004616
Iteration 65/1000 | Loss: 0.00004616
Iteration 66/1000 | Loss: 0.00004616
Iteration 67/1000 | Loss: 0.00004616
Iteration 68/1000 | Loss: 0.00004615
Iteration 69/1000 | Loss: 0.00004615
Iteration 70/1000 | Loss: 0.00004615
Iteration 71/1000 | Loss: 0.00004615
Iteration 72/1000 | Loss: 0.00004614
Iteration 73/1000 | Loss: 0.00004614
Iteration 74/1000 | Loss: 0.00004614
Iteration 75/1000 | Loss: 0.00004614
Iteration 76/1000 | Loss: 0.00004614
Iteration 77/1000 | Loss: 0.00004614
Iteration 78/1000 | Loss: 0.00004613
Iteration 79/1000 | Loss: 0.00004613
Iteration 80/1000 | Loss: 0.00004613
Iteration 81/1000 | Loss: 0.00004613
Iteration 82/1000 | Loss: 0.00004613
Iteration 83/1000 | Loss: 0.00004612
Iteration 84/1000 | Loss: 0.00004612
Iteration 85/1000 | Loss: 0.00004612
Iteration 86/1000 | Loss: 0.00004612
Iteration 87/1000 | Loss: 0.00004611
Iteration 88/1000 | Loss: 0.00004611
Iteration 89/1000 | Loss: 0.00004611
Iteration 90/1000 | Loss: 0.00004611
Iteration 91/1000 | Loss: 0.00004611
Iteration 92/1000 | Loss: 0.00004611
Iteration 93/1000 | Loss: 0.00004611
Iteration 94/1000 | Loss: 0.00004611
Iteration 95/1000 | Loss: 0.00004611
Iteration 96/1000 | Loss: 0.00004611
Iteration 97/1000 | Loss: 0.00004610
Iteration 98/1000 | Loss: 0.00004610
Iteration 99/1000 | Loss: 0.00004610
Iteration 100/1000 | Loss: 0.00004610
Iteration 101/1000 | Loss: 0.00004610
Iteration 102/1000 | Loss: 0.00004610
Iteration 103/1000 | Loss: 0.00004610
Iteration 104/1000 | Loss: 0.00004610
Iteration 105/1000 | Loss: 0.00004610
Iteration 106/1000 | Loss: 0.00004610
Iteration 107/1000 | Loss: 0.00004610
Iteration 108/1000 | Loss: 0.00004610
Iteration 109/1000 | Loss: 0.00004610
Iteration 110/1000 | Loss: 0.00004609
Iteration 111/1000 | Loss: 0.00004609
Iteration 112/1000 | Loss: 0.00004609
Iteration 113/1000 | Loss: 0.00004609
Iteration 114/1000 | Loss: 0.00004609
Iteration 115/1000 | Loss: 0.00004609
Iteration 116/1000 | Loss: 0.00004609
Iteration 117/1000 | Loss: 0.00004609
Iteration 118/1000 | Loss: 0.00004609
Iteration 119/1000 | Loss: 0.00004609
Iteration 120/1000 | Loss: 0.00004609
Iteration 121/1000 | Loss: 0.00004609
Iteration 122/1000 | Loss: 0.00004609
Iteration 123/1000 | Loss: 0.00004609
Iteration 124/1000 | Loss: 0.00004609
Iteration 125/1000 | Loss: 0.00004609
Iteration 126/1000 | Loss: 0.00004608
Iteration 127/1000 | Loss: 0.00004608
Iteration 128/1000 | Loss: 0.00004608
Iteration 129/1000 | Loss: 0.00004608
Iteration 130/1000 | Loss: 0.00004608
Iteration 131/1000 | Loss: 0.00004608
Iteration 132/1000 | Loss: 0.00004608
Iteration 133/1000 | Loss: 0.00004608
Iteration 134/1000 | Loss: 0.00004608
Iteration 135/1000 | Loss: 0.00004608
Iteration 136/1000 | Loss: 0.00004608
Iteration 137/1000 | Loss: 0.00004608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [4.607849768945016e-05, 4.607849768945016e-05, 4.607849768945016e-05, 4.607849768945016e-05, 4.607849768945016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.607849768945016e-05

Optimization complete. Final v2v error: 5.789687156677246 mm

Highest mean error: 6.493749618530273 mm for frame 232

Lowest mean error: 5.300193786621094 mm for frame 101

Saving results

Total time: 55.230913400650024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025003
Iteration 2/25 | Loss: 0.00246900
Iteration 3/25 | Loss: 0.00232277
Iteration 4/25 | Loss: 0.00228802
Iteration 5/25 | Loss: 0.00227831
Iteration 6/25 | Loss: 0.00227559
Iteration 7/25 | Loss: 0.00227400
Iteration 8/25 | Loss: 0.00227388
Iteration 9/25 | Loss: 0.00227388
Iteration 10/25 | Loss: 0.00227388
Iteration 11/25 | Loss: 0.00227388
Iteration 12/25 | Loss: 0.00227388
Iteration 13/25 | Loss: 0.00227388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0022738762199878693, 0.0022738762199878693, 0.0022738762199878693, 0.0022738762199878693, 0.0022738762199878693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022738762199878693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34215856
Iteration 2/25 | Loss: 0.00211936
Iteration 3/25 | Loss: 0.00211936
Iteration 4/25 | Loss: 0.00211936
Iteration 5/25 | Loss: 0.00211936
Iteration 6/25 | Loss: 0.00211936
Iteration 7/25 | Loss: 0.00211936
Iteration 8/25 | Loss: 0.00211936
Iteration 9/25 | Loss: 0.00211936
Iteration 10/25 | Loss: 0.00211936
Iteration 11/25 | Loss: 0.00211936
Iteration 12/25 | Loss: 0.00211936
Iteration 13/25 | Loss: 0.00211936
Iteration 14/25 | Loss: 0.00211936
Iteration 15/25 | Loss: 0.00211936
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002119362121447921, 0.002119362121447921, 0.002119362121447921, 0.002119362121447921, 0.002119362121447921]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002119362121447921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211936
Iteration 2/1000 | Loss: 0.00015300
Iteration 3/1000 | Loss: 0.00008129
Iteration 4/1000 | Loss: 0.00006919
Iteration 5/1000 | Loss: 0.00006295
Iteration 6/1000 | Loss: 0.00005929
Iteration 7/1000 | Loss: 0.00005630
Iteration 8/1000 | Loss: 0.00005324
Iteration 9/1000 | Loss: 0.00005096
Iteration 10/1000 | Loss: 0.00004968
Iteration 11/1000 | Loss: 0.00004891
Iteration 12/1000 | Loss: 0.00004814
Iteration 13/1000 | Loss: 0.00004732
Iteration 14/1000 | Loss: 0.00004683
Iteration 15/1000 | Loss: 0.00004649
Iteration 16/1000 | Loss: 0.00004636
Iteration 17/1000 | Loss: 0.00004616
Iteration 18/1000 | Loss: 0.00004610
Iteration 19/1000 | Loss: 0.00004606
Iteration 20/1000 | Loss: 0.00004597
Iteration 21/1000 | Loss: 0.00004594
Iteration 22/1000 | Loss: 0.00004593
Iteration 23/1000 | Loss: 0.00004593
Iteration 24/1000 | Loss: 0.00004593
Iteration 25/1000 | Loss: 0.00004593
Iteration 26/1000 | Loss: 0.00004592
Iteration 27/1000 | Loss: 0.00004592
Iteration 28/1000 | Loss: 0.00004588
Iteration 29/1000 | Loss: 0.00004588
Iteration 30/1000 | Loss: 0.00004588
Iteration 31/1000 | Loss: 0.00004587
Iteration 32/1000 | Loss: 0.00004587
Iteration 33/1000 | Loss: 0.00004586
Iteration 34/1000 | Loss: 0.00004585
Iteration 35/1000 | Loss: 0.00004585
Iteration 36/1000 | Loss: 0.00004585
Iteration 37/1000 | Loss: 0.00004584
Iteration 38/1000 | Loss: 0.00004584
Iteration 39/1000 | Loss: 0.00004584
Iteration 40/1000 | Loss: 0.00004583
Iteration 41/1000 | Loss: 0.00004583
Iteration 42/1000 | Loss: 0.00004583
Iteration 43/1000 | Loss: 0.00004582
Iteration 44/1000 | Loss: 0.00004582
Iteration 45/1000 | Loss: 0.00004582
Iteration 46/1000 | Loss: 0.00004582
Iteration 47/1000 | Loss: 0.00004581
Iteration 48/1000 | Loss: 0.00004581
Iteration 49/1000 | Loss: 0.00004579
Iteration 50/1000 | Loss: 0.00004579
Iteration 51/1000 | Loss: 0.00004579
Iteration 52/1000 | Loss: 0.00004579
Iteration 53/1000 | Loss: 0.00004579
Iteration 54/1000 | Loss: 0.00004579
Iteration 55/1000 | Loss: 0.00004579
Iteration 56/1000 | Loss: 0.00004579
Iteration 57/1000 | Loss: 0.00004578
Iteration 58/1000 | Loss: 0.00004578
Iteration 59/1000 | Loss: 0.00004578
Iteration 60/1000 | Loss: 0.00004578
Iteration 61/1000 | Loss: 0.00004578
Iteration 62/1000 | Loss: 0.00004578
Iteration 63/1000 | Loss: 0.00004578
Iteration 64/1000 | Loss: 0.00004578
Iteration 65/1000 | Loss: 0.00004578
Iteration 66/1000 | Loss: 0.00004577
Iteration 67/1000 | Loss: 0.00004577
Iteration 68/1000 | Loss: 0.00004576
Iteration 69/1000 | Loss: 0.00004576
Iteration 70/1000 | Loss: 0.00004576
Iteration 71/1000 | Loss: 0.00004575
Iteration 72/1000 | Loss: 0.00004575
Iteration 73/1000 | Loss: 0.00004575
Iteration 74/1000 | Loss: 0.00004574
Iteration 75/1000 | Loss: 0.00004574
Iteration 76/1000 | Loss: 0.00004573
Iteration 77/1000 | Loss: 0.00004573
Iteration 78/1000 | Loss: 0.00004573
Iteration 79/1000 | Loss: 0.00004572
Iteration 80/1000 | Loss: 0.00004571
Iteration 81/1000 | Loss: 0.00004570
Iteration 82/1000 | Loss: 0.00004570
Iteration 83/1000 | Loss: 0.00004570
Iteration 84/1000 | Loss: 0.00004570
Iteration 85/1000 | Loss: 0.00004570
Iteration 86/1000 | Loss: 0.00004570
Iteration 87/1000 | Loss: 0.00004570
Iteration 88/1000 | Loss: 0.00004569
Iteration 89/1000 | Loss: 0.00004569
Iteration 90/1000 | Loss: 0.00004569
Iteration 91/1000 | Loss: 0.00004569
Iteration 92/1000 | Loss: 0.00004569
Iteration 93/1000 | Loss: 0.00004569
Iteration 94/1000 | Loss: 0.00004569
Iteration 95/1000 | Loss: 0.00004568
Iteration 96/1000 | Loss: 0.00004568
Iteration 97/1000 | Loss: 0.00004568
Iteration 98/1000 | Loss: 0.00004568
Iteration 99/1000 | Loss: 0.00004568
Iteration 100/1000 | Loss: 0.00004568
Iteration 101/1000 | Loss: 0.00004568
Iteration 102/1000 | Loss: 0.00004568
Iteration 103/1000 | Loss: 0.00004568
Iteration 104/1000 | Loss: 0.00004567
Iteration 105/1000 | Loss: 0.00004567
Iteration 106/1000 | Loss: 0.00004567
Iteration 107/1000 | Loss: 0.00004567
Iteration 108/1000 | Loss: 0.00004567
Iteration 109/1000 | Loss: 0.00004567
Iteration 110/1000 | Loss: 0.00004567
Iteration 111/1000 | Loss: 0.00004567
Iteration 112/1000 | Loss: 0.00004567
Iteration 113/1000 | Loss: 0.00004567
Iteration 114/1000 | Loss: 0.00004566
Iteration 115/1000 | Loss: 0.00004566
Iteration 116/1000 | Loss: 0.00004566
Iteration 117/1000 | Loss: 0.00004566
Iteration 118/1000 | Loss: 0.00004566
Iteration 119/1000 | Loss: 0.00004566
Iteration 120/1000 | Loss: 0.00004565
Iteration 121/1000 | Loss: 0.00004565
Iteration 122/1000 | Loss: 0.00004565
Iteration 123/1000 | Loss: 0.00004564
Iteration 124/1000 | Loss: 0.00004564
Iteration 125/1000 | Loss: 0.00004564
Iteration 126/1000 | Loss: 0.00004564
Iteration 127/1000 | Loss: 0.00004564
Iteration 128/1000 | Loss: 0.00004564
Iteration 129/1000 | Loss: 0.00004563
Iteration 130/1000 | Loss: 0.00004563
Iteration 131/1000 | Loss: 0.00004563
Iteration 132/1000 | Loss: 0.00004563
Iteration 133/1000 | Loss: 0.00004563
Iteration 134/1000 | Loss: 0.00004563
Iteration 135/1000 | Loss: 0.00004563
Iteration 136/1000 | Loss: 0.00004563
Iteration 137/1000 | Loss: 0.00004562
Iteration 138/1000 | Loss: 0.00004562
Iteration 139/1000 | Loss: 0.00004562
Iteration 140/1000 | Loss: 0.00004562
Iteration 141/1000 | Loss: 0.00004561
Iteration 142/1000 | Loss: 0.00004561
Iteration 143/1000 | Loss: 0.00004561
Iteration 144/1000 | Loss: 0.00004560
Iteration 145/1000 | Loss: 0.00004560
Iteration 146/1000 | Loss: 0.00004560
Iteration 147/1000 | Loss: 0.00004559
Iteration 148/1000 | Loss: 0.00004559
Iteration 149/1000 | Loss: 0.00004559
Iteration 150/1000 | Loss: 0.00004558
Iteration 151/1000 | Loss: 0.00004558
Iteration 152/1000 | Loss: 0.00004558
Iteration 153/1000 | Loss: 0.00004558
Iteration 154/1000 | Loss: 0.00004558
Iteration 155/1000 | Loss: 0.00004558
Iteration 156/1000 | Loss: 0.00004557
Iteration 157/1000 | Loss: 0.00004557
Iteration 158/1000 | Loss: 0.00004557
Iteration 159/1000 | Loss: 0.00004557
Iteration 160/1000 | Loss: 0.00004557
Iteration 161/1000 | Loss: 0.00004557
Iteration 162/1000 | Loss: 0.00004557
Iteration 163/1000 | Loss: 0.00004557
Iteration 164/1000 | Loss: 0.00004556
Iteration 165/1000 | Loss: 0.00004556
Iteration 166/1000 | Loss: 0.00004556
Iteration 167/1000 | Loss: 0.00004556
Iteration 168/1000 | Loss: 0.00004556
Iteration 169/1000 | Loss: 0.00004556
Iteration 170/1000 | Loss: 0.00004556
Iteration 171/1000 | Loss: 0.00004556
Iteration 172/1000 | Loss: 0.00004555
Iteration 173/1000 | Loss: 0.00004555
Iteration 174/1000 | Loss: 0.00004555
Iteration 175/1000 | Loss: 0.00004555
Iteration 176/1000 | Loss: 0.00004555
Iteration 177/1000 | Loss: 0.00004554
Iteration 178/1000 | Loss: 0.00004554
Iteration 179/1000 | Loss: 0.00004554
Iteration 180/1000 | Loss: 0.00004554
Iteration 181/1000 | Loss: 0.00004554
Iteration 182/1000 | Loss: 0.00004554
Iteration 183/1000 | Loss: 0.00004554
Iteration 184/1000 | Loss: 0.00004554
Iteration 185/1000 | Loss: 0.00004554
Iteration 186/1000 | Loss: 0.00004554
Iteration 187/1000 | Loss: 0.00004554
Iteration 188/1000 | Loss: 0.00004553
Iteration 189/1000 | Loss: 0.00004553
Iteration 190/1000 | Loss: 0.00004553
Iteration 191/1000 | Loss: 0.00004553
Iteration 192/1000 | Loss: 0.00004552
Iteration 193/1000 | Loss: 0.00004552
Iteration 194/1000 | Loss: 0.00004552
Iteration 195/1000 | Loss: 0.00004552
Iteration 196/1000 | Loss: 0.00004552
Iteration 197/1000 | Loss: 0.00004551
Iteration 198/1000 | Loss: 0.00004551
Iteration 199/1000 | Loss: 0.00004551
Iteration 200/1000 | Loss: 0.00004551
Iteration 201/1000 | Loss: 0.00004551
Iteration 202/1000 | Loss: 0.00004551
Iteration 203/1000 | Loss: 0.00004551
Iteration 204/1000 | Loss: 0.00004551
Iteration 205/1000 | Loss: 0.00004551
Iteration 206/1000 | Loss: 0.00004551
Iteration 207/1000 | Loss: 0.00004551
Iteration 208/1000 | Loss: 0.00004551
Iteration 209/1000 | Loss: 0.00004551
Iteration 210/1000 | Loss: 0.00004551
Iteration 211/1000 | Loss: 0.00004550
Iteration 212/1000 | Loss: 0.00004550
Iteration 213/1000 | Loss: 0.00004550
Iteration 214/1000 | Loss: 0.00004550
Iteration 215/1000 | Loss: 0.00004550
Iteration 216/1000 | Loss: 0.00004550
Iteration 217/1000 | Loss: 0.00004550
Iteration 218/1000 | Loss: 0.00004550
Iteration 219/1000 | Loss: 0.00004550
Iteration 220/1000 | Loss: 0.00004550
Iteration 221/1000 | Loss: 0.00004550
Iteration 222/1000 | Loss: 0.00004550
Iteration 223/1000 | Loss: 0.00004550
Iteration 224/1000 | Loss: 0.00004550
Iteration 225/1000 | Loss: 0.00004550
Iteration 226/1000 | Loss: 0.00004550
Iteration 227/1000 | Loss: 0.00004550
Iteration 228/1000 | Loss: 0.00004550
Iteration 229/1000 | Loss: 0.00004549
Iteration 230/1000 | Loss: 0.00004549
Iteration 231/1000 | Loss: 0.00004549
Iteration 232/1000 | Loss: 0.00004549
Iteration 233/1000 | Loss: 0.00004549
Iteration 234/1000 | Loss: 0.00004549
Iteration 235/1000 | Loss: 0.00004549
Iteration 236/1000 | Loss: 0.00004549
Iteration 237/1000 | Loss: 0.00004549
Iteration 238/1000 | Loss: 0.00004548
Iteration 239/1000 | Loss: 0.00004548
Iteration 240/1000 | Loss: 0.00004548
Iteration 241/1000 | Loss: 0.00004548
Iteration 242/1000 | Loss: 0.00004548
Iteration 243/1000 | Loss: 0.00004548
Iteration 244/1000 | Loss: 0.00004548
Iteration 245/1000 | Loss: 0.00004548
Iteration 246/1000 | Loss: 0.00004548
Iteration 247/1000 | Loss: 0.00004548
Iteration 248/1000 | Loss: 0.00004548
Iteration 249/1000 | Loss: 0.00004548
Iteration 250/1000 | Loss: 0.00004548
Iteration 251/1000 | Loss: 0.00004547
Iteration 252/1000 | Loss: 0.00004547
Iteration 253/1000 | Loss: 0.00004547
Iteration 254/1000 | Loss: 0.00004547
Iteration 255/1000 | Loss: 0.00004547
Iteration 256/1000 | Loss: 0.00004547
Iteration 257/1000 | Loss: 0.00004547
Iteration 258/1000 | Loss: 0.00004547
Iteration 259/1000 | Loss: 0.00004547
Iteration 260/1000 | Loss: 0.00004547
Iteration 261/1000 | Loss: 0.00004547
Iteration 262/1000 | Loss: 0.00004547
Iteration 263/1000 | Loss: 0.00004547
Iteration 264/1000 | Loss: 0.00004546
Iteration 265/1000 | Loss: 0.00004546
Iteration 266/1000 | Loss: 0.00004546
Iteration 267/1000 | Loss: 0.00004546
Iteration 268/1000 | Loss: 0.00004546
Iteration 269/1000 | Loss: 0.00004546
Iteration 270/1000 | Loss: 0.00004546
Iteration 271/1000 | Loss: 0.00004546
Iteration 272/1000 | Loss: 0.00004546
Iteration 273/1000 | Loss: 0.00004546
Iteration 274/1000 | Loss: 0.00004546
Iteration 275/1000 | Loss: 0.00004545
Iteration 276/1000 | Loss: 0.00004545
Iteration 277/1000 | Loss: 0.00004545
Iteration 278/1000 | Loss: 0.00004545
Iteration 279/1000 | Loss: 0.00004545
Iteration 280/1000 | Loss: 0.00004545
Iteration 281/1000 | Loss: 0.00004545
Iteration 282/1000 | Loss: 0.00004545
Iteration 283/1000 | Loss: 0.00004545
Iteration 284/1000 | Loss: 0.00004545
Iteration 285/1000 | Loss: 0.00004545
Iteration 286/1000 | Loss: 0.00004545
Iteration 287/1000 | Loss: 0.00004545
Iteration 288/1000 | Loss: 0.00004545
Iteration 289/1000 | Loss: 0.00004545
Iteration 290/1000 | Loss: 0.00004545
Iteration 291/1000 | Loss: 0.00004545
Iteration 292/1000 | Loss: 0.00004545
Iteration 293/1000 | Loss: 0.00004545
Iteration 294/1000 | Loss: 0.00004545
Iteration 295/1000 | Loss: 0.00004545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [4.545399860944599e-05, 4.545399860944599e-05, 4.545399860944599e-05, 4.545399860944599e-05, 4.545399860944599e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.545399860944599e-05

Optimization complete. Final v2v error: 5.849918842315674 mm

Highest mean error: 6.033542633056641 mm for frame 62

Lowest mean error: 5.590404510498047 mm for frame 13

Saving results

Total time: 53.90815329551697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569423
Iteration 2/25 | Loss: 0.00231858
Iteration 3/25 | Loss: 0.00216886
Iteration 4/25 | Loss: 0.00214636
Iteration 5/25 | Loss: 0.00214196
Iteration 6/25 | Loss: 0.00214166
Iteration 7/25 | Loss: 0.00214166
Iteration 8/25 | Loss: 0.00214166
Iteration 9/25 | Loss: 0.00214166
Iteration 10/25 | Loss: 0.00214166
Iteration 11/25 | Loss: 0.00214166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002141661709174514, 0.002141661709174514, 0.002141661709174514, 0.002141661709174514, 0.002141661709174514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002141661709174514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39208794
Iteration 2/25 | Loss: 0.00200048
Iteration 3/25 | Loss: 0.00200045
Iteration 4/25 | Loss: 0.00200045
Iteration 5/25 | Loss: 0.00200045
Iteration 6/25 | Loss: 0.00200045
Iteration 7/25 | Loss: 0.00200045
Iteration 8/25 | Loss: 0.00200045
Iteration 9/25 | Loss: 0.00200045
Iteration 10/25 | Loss: 0.00200045
Iteration 11/25 | Loss: 0.00200045
Iteration 12/25 | Loss: 0.00200045
Iteration 13/25 | Loss: 0.00200045
Iteration 14/25 | Loss: 0.00200045
Iteration 15/25 | Loss: 0.00200045
Iteration 16/25 | Loss: 0.00200045
Iteration 17/25 | Loss: 0.00200045
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002000445267185569, 0.002000445267185569, 0.002000445267185569, 0.002000445267185569, 0.002000445267185569]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002000445267185569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200045
Iteration 2/1000 | Loss: 0.00006151
Iteration 3/1000 | Loss: 0.00004759
Iteration 4/1000 | Loss: 0.00004449
Iteration 5/1000 | Loss: 0.00004260
Iteration 6/1000 | Loss: 0.00004109
Iteration 7/1000 | Loss: 0.00004022
Iteration 8/1000 | Loss: 0.00003913
Iteration 9/1000 | Loss: 0.00003860
Iteration 10/1000 | Loss: 0.00003832
Iteration 11/1000 | Loss: 0.00003808
Iteration 12/1000 | Loss: 0.00003784
Iteration 13/1000 | Loss: 0.00003783
Iteration 14/1000 | Loss: 0.00003769
Iteration 15/1000 | Loss: 0.00003768
Iteration 16/1000 | Loss: 0.00003760
Iteration 17/1000 | Loss: 0.00003755
Iteration 18/1000 | Loss: 0.00003755
Iteration 19/1000 | Loss: 0.00003754
Iteration 20/1000 | Loss: 0.00003749
Iteration 21/1000 | Loss: 0.00003748
Iteration 22/1000 | Loss: 0.00003748
Iteration 23/1000 | Loss: 0.00003747
Iteration 24/1000 | Loss: 0.00003747
Iteration 25/1000 | Loss: 0.00003747
Iteration 26/1000 | Loss: 0.00003746
Iteration 27/1000 | Loss: 0.00003746
Iteration 28/1000 | Loss: 0.00003745
Iteration 29/1000 | Loss: 0.00003745
Iteration 30/1000 | Loss: 0.00003744
Iteration 31/1000 | Loss: 0.00003744
Iteration 32/1000 | Loss: 0.00003744
Iteration 33/1000 | Loss: 0.00003744
Iteration 34/1000 | Loss: 0.00003743
Iteration 35/1000 | Loss: 0.00003743
Iteration 36/1000 | Loss: 0.00003743
Iteration 37/1000 | Loss: 0.00003743
Iteration 38/1000 | Loss: 0.00003743
Iteration 39/1000 | Loss: 0.00003743
Iteration 40/1000 | Loss: 0.00003743
Iteration 41/1000 | Loss: 0.00003743
Iteration 42/1000 | Loss: 0.00003742
Iteration 43/1000 | Loss: 0.00003742
Iteration 44/1000 | Loss: 0.00003742
Iteration 45/1000 | Loss: 0.00003741
Iteration 46/1000 | Loss: 0.00003741
Iteration 47/1000 | Loss: 0.00003740
Iteration 48/1000 | Loss: 0.00003740
Iteration 49/1000 | Loss: 0.00003740
Iteration 50/1000 | Loss: 0.00003740
Iteration 51/1000 | Loss: 0.00003739
Iteration 52/1000 | Loss: 0.00003739
Iteration 53/1000 | Loss: 0.00003738
Iteration 54/1000 | Loss: 0.00003738
Iteration 55/1000 | Loss: 0.00003738
Iteration 56/1000 | Loss: 0.00003738
Iteration 57/1000 | Loss: 0.00003738
Iteration 58/1000 | Loss: 0.00003738
Iteration 59/1000 | Loss: 0.00003738
Iteration 60/1000 | Loss: 0.00003738
Iteration 61/1000 | Loss: 0.00003738
Iteration 62/1000 | Loss: 0.00003737
Iteration 63/1000 | Loss: 0.00003737
Iteration 64/1000 | Loss: 0.00003737
Iteration 65/1000 | Loss: 0.00003737
Iteration 66/1000 | Loss: 0.00003737
Iteration 67/1000 | Loss: 0.00003737
Iteration 68/1000 | Loss: 0.00003737
Iteration 69/1000 | Loss: 0.00003737
Iteration 70/1000 | Loss: 0.00003736
Iteration 71/1000 | Loss: 0.00003736
Iteration 72/1000 | Loss: 0.00003736
Iteration 73/1000 | Loss: 0.00003735
Iteration 74/1000 | Loss: 0.00003735
Iteration 75/1000 | Loss: 0.00003735
Iteration 76/1000 | Loss: 0.00003735
Iteration 77/1000 | Loss: 0.00003735
Iteration 78/1000 | Loss: 0.00003735
Iteration 79/1000 | Loss: 0.00003735
Iteration 80/1000 | Loss: 0.00003735
Iteration 81/1000 | Loss: 0.00003735
Iteration 82/1000 | Loss: 0.00003735
Iteration 83/1000 | Loss: 0.00003735
Iteration 84/1000 | Loss: 0.00003735
Iteration 85/1000 | Loss: 0.00003735
Iteration 86/1000 | Loss: 0.00003734
Iteration 87/1000 | Loss: 0.00003734
Iteration 88/1000 | Loss: 0.00003734
Iteration 89/1000 | Loss: 0.00003734
Iteration 90/1000 | Loss: 0.00003734
Iteration 91/1000 | Loss: 0.00003734
Iteration 92/1000 | Loss: 0.00003734
Iteration 93/1000 | Loss: 0.00003734
Iteration 94/1000 | Loss: 0.00003733
Iteration 95/1000 | Loss: 0.00003733
Iteration 96/1000 | Loss: 0.00003733
Iteration 97/1000 | Loss: 0.00003733
Iteration 98/1000 | Loss: 0.00003733
Iteration 99/1000 | Loss: 0.00003733
Iteration 100/1000 | Loss: 0.00003733
Iteration 101/1000 | Loss: 0.00003733
Iteration 102/1000 | Loss: 0.00003733
Iteration 103/1000 | Loss: 0.00003733
Iteration 104/1000 | Loss: 0.00003733
Iteration 105/1000 | Loss: 0.00003733
Iteration 106/1000 | Loss: 0.00003733
Iteration 107/1000 | Loss: 0.00003733
Iteration 108/1000 | Loss: 0.00003733
Iteration 109/1000 | Loss: 0.00003733
Iteration 110/1000 | Loss: 0.00003733
Iteration 111/1000 | Loss: 0.00003733
Iteration 112/1000 | Loss: 0.00003733
Iteration 113/1000 | Loss: 0.00003733
Iteration 114/1000 | Loss: 0.00003733
Iteration 115/1000 | Loss: 0.00003733
Iteration 116/1000 | Loss: 0.00003733
Iteration 117/1000 | Loss: 0.00003733
Iteration 118/1000 | Loss: 0.00003733
Iteration 119/1000 | Loss: 0.00003733
Iteration 120/1000 | Loss: 0.00003733
Iteration 121/1000 | Loss: 0.00003733
Iteration 122/1000 | Loss: 0.00003733
Iteration 123/1000 | Loss: 0.00003733
Iteration 124/1000 | Loss: 0.00003733
Iteration 125/1000 | Loss: 0.00003733
Iteration 126/1000 | Loss: 0.00003733
Iteration 127/1000 | Loss: 0.00003733
Iteration 128/1000 | Loss: 0.00003733
Iteration 129/1000 | Loss: 0.00003733
Iteration 130/1000 | Loss: 0.00003733
Iteration 131/1000 | Loss: 0.00003733
Iteration 132/1000 | Loss: 0.00003733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [3.7326175515772775e-05, 3.7326175515772775e-05, 3.7326175515772775e-05, 3.7326175515772775e-05, 3.7326175515772775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7326175515772775e-05

Optimization complete. Final v2v error: 5.2569990158081055 mm

Highest mean error: 5.434151649475098 mm for frame 221

Lowest mean error: 5.1090850830078125 mm for frame 97

Saving results

Total time: 40.07852244377136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01130989
Iteration 2/25 | Loss: 0.00240679
Iteration 3/25 | Loss: 0.00202385
Iteration 4/25 | Loss: 0.00198401
Iteration 5/25 | Loss: 0.00196846
Iteration 6/25 | Loss: 0.00194715
Iteration 7/25 | Loss: 0.00194095
Iteration 8/25 | Loss: 0.00193172
Iteration 9/25 | Loss: 0.00192107
Iteration 10/25 | Loss: 0.00191973
Iteration 11/25 | Loss: 0.00191926
Iteration 12/25 | Loss: 0.00192052
Iteration 13/25 | Loss: 0.00191950
Iteration 14/25 | Loss: 0.00191915
Iteration 15/25 | Loss: 0.00191812
Iteration 16/25 | Loss: 0.00191706
Iteration 17/25 | Loss: 0.00191644
Iteration 18/25 | Loss: 0.00191608
Iteration 19/25 | Loss: 0.00191610
Iteration 20/25 | Loss: 0.00191588
Iteration 21/25 | Loss: 0.00191604
Iteration 22/25 | Loss: 0.00191592
Iteration 23/25 | Loss: 0.00191615
Iteration 24/25 | Loss: 0.00191601
Iteration 25/25 | Loss: 0.00191613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91323578
Iteration 2/25 | Loss: 0.00165962
Iteration 3/25 | Loss: 0.00155256
Iteration 4/25 | Loss: 0.00155255
Iteration 5/25 | Loss: 0.00155255
Iteration 6/25 | Loss: 0.00155255
Iteration 7/25 | Loss: 0.00155255
Iteration 8/25 | Loss: 0.00155255
Iteration 9/25 | Loss: 0.00155255
Iteration 10/25 | Loss: 0.00155255
Iteration 11/25 | Loss: 0.00155255
Iteration 12/25 | Loss: 0.00155255
Iteration 13/25 | Loss: 0.00155255
Iteration 14/25 | Loss: 0.00155255
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0015525453491136432, 0.0015525453491136432, 0.0015525453491136432, 0.0015525453491136432, 0.0015525453491136432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015525453491136432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155255
Iteration 2/1000 | Loss: 0.00007899
Iteration 3/1000 | Loss: 0.00005214
Iteration 4/1000 | Loss: 0.00006321
Iteration 5/1000 | Loss: 0.00015052
Iteration 6/1000 | Loss: 0.00004381
Iteration 7/1000 | Loss: 0.00005750
Iteration 8/1000 | Loss: 0.00004987
Iteration 9/1000 | Loss: 0.00004235
Iteration 10/1000 | Loss: 0.00004189
Iteration 11/1000 | Loss: 0.00005793
Iteration 12/1000 | Loss: 0.00004058
Iteration 13/1000 | Loss: 0.00005523
Iteration 14/1000 | Loss: 0.00005097
Iteration 15/1000 | Loss: 0.00003912
Iteration 16/1000 | Loss: 0.00003898
Iteration 17/1000 | Loss: 0.00003863
Iteration 18/1000 | Loss: 0.00003836
Iteration 19/1000 | Loss: 0.00004923
Iteration 20/1000 | Loss: 0.00012975
Iteration 21/1000 | Loss: 0.00005838
Iteration 22/1000 | Loss: 0.00005159
Iteration 23/1000 | Loss: 0.00007571
Iteration 24/1000 | Loss: 0.00004488
Iteration 25/1000 | Loss: 0.00003873
Iteration 26/1000 | Loss: 0.00004891
Iteration 27/1000 | Loss: 0.00003826
Iteration 28/1000 | Loss: 0.00003809
Iteration 29/1000 | Loss: 0.00003808
Iteration 30/1000 | Loss: 0.00003806
Iteration 31/1000 | Loss: 0.00003806
Iteration 32/1000 | Loss: 0.00003805
Iteration 33/1000 | Loss: 0.00003805
Iteration 34/1000 | Loss: 0.00003804
Iteration 35/1000 | Loss: 0.00003804
Iteration 36/1000 | Loss: 0.00003804
Iteration 37/1000 | Loss: 0.00003804
Iteration 38/1000 | Loss: 0.00003804
Iteration 39/1000 | Loss: 0.00003804
Iteration 40/1000 | Loss: 0.00003804
Iteration 41/1000 | Loss: 0.00003804
Iteration 42/1000 | Loss: 0.00003804
Iteration 43/1000 | Loss: 0.00003804
Iteration 44/1000 | Loss: 0.00003804
Iteration 45/1000 | Loss: 0.00003803
Iteration 46/1000 | Loss: 0.00003803
Iteration 47/1000 | Loss: 0.00003803
Iteration 48/1000 | Loss: 0.00003803
Iteration 49/1000 | Loss: 0.00003802
Iteration 50/1000 | Loss: 0.00003802
Iteration 51/1000 | Loss: 0.00003801
Iteration 52/1000 | Loss: 0.00003801
Iteration 53/1000 | Loss: 0.00003801
Iteration 54/1000 | Loss: 0.00003800
Iteration 55/1000 | Loss: 0.00003800
Iteration 56/1000 | Loss: 0.00003800
Iteration 57/1000 | Loss: 0.00003800
Iteration 58/1000 | Loss: 0.00003799
Iteration 59/1000 | Loss: 0.00003799
Iteration 60/1000 | Loss: 0.00003799
Iteration 61/1000 | Loss: 0.00003798
Iteration 62/1000 | Loss: 0.00003798
Iteration 63/1000 | Loss: 0.00003798
Iteration 64/1000 | Loss: 0.00003798
Iteration 65/1000 | Loss: 0.00003798
Iteration 66/1000 | Loss: 0.00003798
Iteration 67/1000 | Loss: 0.00003798
Iteration 68/1000 | Loss: 0.00003798
Iteration 69/1000 | Loss: 0.00003798
Iteration 70/1000 | Loss: 0.00003798
Iteration 71/1000 | Loss: 0.00003798
Iteration 72/1000 | Loss: 0.00003797
Iteration 73/1000 | Loss: 0.00003797
Iteration 74/1000 | Loss: 0.00003797
Iteration 75/1000 | Loss: 0.00003797
Iteration 76/1000 | Loss: 0.00003797
Iteration 77/1000 | Loss: 0.00003797
Iteration 78/1000 | Loss: 0.00003797
Iteration 79/1000 | Loss: 0.00003796
Iteration 80/1000 | Loss: 0.00003796
Iteration 81/1000 | Loss: 0.00003796
Iteration 82/1000 | Loss: 0.00003795
Iteration 83/1000 | Loss: 0.00003795
Iteration 84/1000 | Loss: 0.00003795
Iteration 85/1000 | Loss: 0.00003795
Iteration 86/1000 | Loss: 0.00003795
Iteration 87/1000 | Loss: 0.00003795
Iteration 88/1000 | Loss: 0.00003795
Iteration 89/1000 | Loss: 0.00003795
Iteration 90/1000 | Loss: 0.00003795
Iteration 91/1000 | Loss: 0.00003795
Iteration 92/1000 | Loss: 0.00003794
Iteration 93/1000 | Loss: 0.00003916
Iteration 94/1000 | Loss: 0.00004925
Iteration 95/1000 | Loss: 0.00004375
Iteration 96/1000 | Loss: 0.00005147
Iteration 97/1000 | Loss: 0.00003801
Iteration 98/1000 | Loss: 0.00003801
Iteration 99/1000 | Loss: 0.00003800
Iteration 100/1000 | Loss: 0.00003800
Iteration 101/1000 | Loss: 0.00003800
Iteration 102/1000 | Loss: 0.00003800
Iteration 103/1000 | Loss: 0.00003800
Iteration 104/1000 | Loss: 0.00003800
Iteration 105/1000 | Loss: 0.00003800
Iteration 106/1000 | Loss: 0.00003800
Iteration 107/1000 | Loss: 0.00003800
Iteration 108/1000 | Loss: 0.00003800
Iteration 109/1000 | Loss: 0.00003800
Iteration 110/1000 | Loss: 0.00003800
Iteration 111/1000 | Loss: 0.00003800
Iteration 112/1000 | Loss: 0.00003800
Iteration 113/1000 | Loss: 0.00003799
Iteration 114/1000 | Loss: 0.00003799
Iteration 115/1000 | Loss: 0.00003799
Iteration 116/1000 | Loss: 0.00003799
Iteration 117/1000 | Loss: 0.00003799
Iteration 118/1000 | Loss: 0.00003799
Iteration 119/1000 | Loss: 0.00003799
Iteration 120/1000 | Loss: 0.00004317
Iteration 121/1000 | Loss: 0.00003802
Iteration 122/1000 | Loss: 0.00003802
Iteration 123/1000 | Loss: 0.00003802
Iteration 124/1000 | Loss: 0.00003801
Iteration 125/1000 | Loss: 0.00003801
Iteration 126/1000 | Loss: 0.00003825
Iteration 127/1000 | Loss: 0.00003825
Iteration 128/1000 | Loss: 0.00003800
Iteration 129/1000 | Loss: 0.00003800
Iteration 130/1000 | Loss: 0.00003800
Iteration 131/1000 | Loss: 0.00003800
Iteration 132/1000 | Loss: 0.00003800
Iteration 133/1000 | Loss: 0.00003800
Iteration 134/1000 | Loss: 0.00003800
Iteration 135/1000 | Loss: 0.00003800
Iteration 136/1000 | Loss: 0.00003800
Iteration 137/1000 | Loss: 0.00003800
Iteration 138/1000 | Loss: 0.00003799
Iteration 139/1000 | Loss: 0.00003799
Iteration 140/1000 | Loss: 0.00003799
Iteration 141/1000 | Loss: 0.00003799
Iteration 142/1000 | Loss: 0.00003799
Iteration 143/1000 | Loss: 0.00003798
Iteration 144/1000 | Loss: 0.00003798
Iteration 145/1000 | Loss: 0.00003798
Iteration 146/1000 | Loss: 0.00003798
Iteration 147/1000 | Loss: 0.00003798
Iteration 148/1000 | Loss: 0.00003798
Iteration 149/1000 | Loss: 0.00003798
Iteration 150/1000 | Loss: 0.00003798
Iteration 151/1000 | Loss: 0.00003797
Iteration 152/1000 | Loss: 0.00003797
Iteration 153/1000 | Loss: 0.00003797
Iteration 154/1000 | Loss: 0.00003797
Iteration 155/1000 | Loss: 0.00003797
Iteration 156/1000 | Loss: 0.00003797
Iteration 157/1000 | Loss: 0.00003797
Iteration 158/1000 | Loss: 0.00003797
Iteration 159/1000 | Loss: 0.00003797
Iteration 160/1000 | Loss: 0.00003797
Iteration 161/1000 | Loss: 0.00003796
Iteration 162/1000 | Loss: 0.00003796
Iteration 163/1000 | Loss: 0.00003796
Iteration 164/1000 | Loss: 0.00003796
Iteration 165/1000 | Loss: 0.00003796
Iteration 166/1000 | Loss: 0.00003796
Iteration 167/1000 | Loss: 0.00003796
Iteration 168/1000 | Loss: 0.00003796
Iteration 169/1000 | Loss: 0.00003796
Iteration 170/1000 | Loss: 0.00003796
Iteration 171/1000 | Loss: 0.00003795
Iteration 172/1000 | Loss: 0.00003795
Iteration 173/1000 | Loss: 0.00003795
Iteration 174/1000 | Loss: 0.00003795
Iteration 175/1000 | Loss: 0.00003795
Iteration 176/1000 | Loss: 0.00003795
Iteration 177/1000 | Loss: 0.00003795
Iteration 178/1000 | Loss: 0.00003795
Iteration 179/1000 | Loss: 0.00003795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [3.795377415372059e-05, 3.795377415372059e-05, 3.795377415372059e-05, 3.795377415372059e-05, 3.795377415372059e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.795377415372059e-05

Optimization complete. Final v2v error: 5.2425689697265625 mm

Highest mean error: 10.939058303833008 mm for frame 226

Lowest mean error: 4.848047256469727 mm for frame 220

Saving results

Total time: 115.76743793487549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01182847
Iteration 2/25 | Loss: 0.00235482
Iteration 3/25 | Loss: 0.00219920
Iteration 4/25 | Loss: 0.00217609
Iteration 5/25 | Loss: 0.00218488
Iteration 6/25 | Loss: 0.00216852
Iteration 7/25 | Loss: 0.00214901
Iteration 8/25 | Loss: 0.00213649
Iteration 9/25 | Loss: 0.00214033
Iteration 10/25 | Loss: 0.00213835
Iteration 11/25 | Loss: 0.00213324
Iteration 12/25 | Loss: 0.00213156
Iteration 13/25 | Loss: 0.00213135
Iteration 14/25 | Loss: 0.00213135
Iteration 15/25 | Loss: 0.00213135
Iteration 16/25 | Loss: 0.00213135
Iteration 17/25 | Loss: 0.00213135
Iteration 18/25 | Loss: 0.00213135
Iteration 19/25 | Loss: 0.00213135
Iteration 20/25 | Loss: 0.00213134
Iteration 21/25 | Loss: 0.00213134
Iteration 22/25 | Loss: 0.00213134
Iteration 23/25 | Loss: 0.00213134
Iteration 24/25 | Loss: 0.00213134
Iteration 25/25 | Loss: 0.00213134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82517767
Iteration 2/25 | Loss: 0.00192946
Iteration 3/25 | Loss: 0.00192946
Iteration 4/25 | Loss: 0.00192946
Iteration 5/25 | Loss: 0.00192946
Iteration 6/25 | Loss: 0.00192946
Iteration 7/25 | Loss: 0.00192946
Iteration 8/25 | Loss: 0.00192946
Iteration 9/25 | Loss: 0.00192946
Iteration 10/25 | Loss: 0.00192946
Iteration 11/25 | Loss: 0.00192946
Iteration 12/25 | Loss: 0.00192946
Iteration 13/25 | Loss: 0.00192946
Iteration 14/25 | Loss: 0.00192946
Iteration 15/25 | Loss: 0.00192946
Iteration 16/25 | Loss: 0.00192946
Iteration 17/25 | Loss: 0.00192946
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019294620724394917, 0.0019294620724394917, 0.0019294620724394917, 0.0019294620724394917, 0.0019294620724394917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019294620724394917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192946
Iteration 2/1000 | Loss: 0.00007944
Iteration 3/1000 | Loss: 0.00005702
Iteration 4/1000 | Loss: 0.00005074
Iteration 5/1000 | Loss: 0.00004835
Iteration 6/1000 | Loss: 0.00004689
Iteration 7/1000 | Loss: 0.00004535
Iteration 8/1000 | Loss: 0.00004444
Iteration 9/1000 | Loss: 0.00004353
Iteration 10/1000 | Loss: 0.00004285
Iteration 11/1000 | Loss: 0.00004243
Iteration 12/1000 | Loss: 0.00004206
Iteration 13/1000 | Loss: 0.00004186
Iteration 14/1000 | Loss: 0.00004172
Iteration 15/1000 | Loss: 0.00004156
Iteration 16/1000 | Loss: 0.00004154
Iteration 17/1000 | Loss: 0.00004150
Iteration 18/1000 | Loss: 0.00004149
Iteration 19/1000 | Loss: 0.00004149
Iteration 20/1000 | Loss: 0.00004149
Iteration 21/1000 | Loss: 0.00004148
Iteration 22/1000 | Loss: 0.00004148
Iteration 23/1000 | Loss: 0.00004147
Iteration 24/1000 | Loss: 0.00004147
Iteration 25/1000 | Loss: 0.00004147
Iteration 26/1000 | Loss: 0.00004147
Iteration 27/1000 | Loss: 0.00004147
Iteration 28/1000 | Loss: 0.00004147
Iteration 29/1000 | Loss: 0.00004147
Iteration 30/1000 | Loss: 0.00004147
Iteration 31/1000 | Loss: 0.00004147
Iteration 32/1000 | Loss: 0.00004147
Iteration 33/1000 | Loss: 0.00004146
Iteration 34/1000 | Loss: 0.00004145
Iteration 35/1000 | Loss: 0.00004144
Iteration 36/1000 | Loss: 0.00004144
Iteration 37/1000 | Loss: 0.00004144
Iteration 38/1000 | Loss: 0.00004144
Iteration 39/1000 | Loss: 0.00004144
Iteration 40/1000 | Loss: 0.00004144
Iteration 41/1000 | Loss: 0.00004143
Iteration 42/1000 | Loss: 0.00004143
Iteration 43/1000 | Loss: 0.00004143
Iteration 44/1000 | Loss: 0.00004143
Iteration 45/1000 | Loss: 0.00004143
Iteration 46/1000 | Loss: 0.00004143
Iteration 47/1000 | Loss: 0.00004143
Iteration 48/1000 | Loss: 0.00004142
Iteration 49/1000 | Loss: 0.00004142
Iteration 50/1000 | Loss: 0.00004142
Iteration 51/1000 | Loss: 0.00004141
Iteration 52/1000 | Loss: 0.00004141
Iteration 53/1000 | Loss: 0.00004140
Iteration 54/1000 | Loss: 0.00004140
Iteration 55/1000 | Loss: 0.00004140
Iteration 56/1000 | Loss: 0.00004140
Iteration 57/1000 | Loss: 0.00004140
Iteration 58/1000 | Loss: 0.00004140
Iteration 59/1000 | Loss: 0.00004140
Iteration 60/1000 | Loss: 0.00004140
Iteration 61/1000 | Loss: 0.00004140
Iteration 62/1000 | Loss: 0.00004140
Iteration 63/1000 | Loss: 0.00004139
Iteration 64/1000 | Loss: 0.00004139
Iteration 65/1000 | Loss: 0.00004139
Iteration 66/1000 | Loss: 0.00004139
Iteration 67/1000 | Loss: 0.00004138
Iteration 68/1000 | Loss: 0.00004138
Iteration 69/1000 | Loss: 0.00004138
Iteration 70/1000 | Loss: 0.00004138
Iteration 71/1000 | Loss: 0.00004138
Iteration 72/1000 | Loss: 0.00004138
Iteration 73/1000 | Loss: 0.00004137
Iteration 74/1000 | Loss: 0.00004137
Iteration 75/1000 | Loss: 0.00004137
Iteration 76/1000 | Loss: 0.00004137
Iteration 77/1000 | Loss: 0.00004137
Iteration 78/1000 | Loss: 0.00004136
Iteration 79/1000 | Loss: 0.00004136
Iteration 80/1000 | Loss: 0.00004136
Iteration 81/1000 | Loss: 0.00004136
Iteration 82/1000 | Loss: 0.00004135
Iteration 83/1000 | Loss: 0.00004135
Iteration 84/1000 | Loss: 0.00004135
Iteration 85/1000 | Loss: 0.00004135
Iteration 86/1000 | Loss: 0.00004135
Iteration 87/1000 | Loss: 0.00004134
Iteration 88/1000 | Loss: 0.00004134
Iteration 89/1000 | Loss: 0.00004134
Iteration 90/1000 | Loss: 0.00004134
Iteration 91/1000 | Loss: 0.00004134
Iteration 92/1000 | Loss: 0.00004134
Iteration 93/1000 | Loss: 0.00004134
Iteration 94/1000 | Loss: 0.00004134
Iteration 95/1000 | Loss: 0.00004134
Iteration 96/1000 | Loss: 0.00004134
Iteration 97/1000 | Loss: 0.00004133
Iteration 98/1000 | Loss: 0.00004133
Iteration 99/1000 | Loss: 0.00004133
Iteration 100/1000 | Loss: 0.00004133
Iteration 101/1000 | Loss: 0.00004133
Iteration 102/1000 | Loss: 0.00004133
Iteration 103/1000 | Loss: 0.00004132
Iteration 104/1000 | Loss: 0.00004132
Iteration 105/1000 | Loss: 0.00004132
Iteration 106/1000 | Loss: 0.00004132
Iteration 107/1000 | Loss: 0.00004132
Iteration 108/1000 | Loss: 0.00004132
Iteration 109/1000 | Loss: 0.00004132
Iteration 110/1000 | Loss: 0.00004132
Iteration 111/1000 | Loss: 0.00004132
Iteration 112/1000 | Loss: 0.00004132
Iteration 113/1000 | Loss: 0.00004132
Iteration 114/1000 | Loss: 0.00004132
Iteration 115/1000 | Loss: 0.00004132
Iteration 116/1000 | Loss: 0.00004132
Iteration 117/1000 | Loss: 0.00004132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [4.1319501178804785e-05, 4.1319501178804785e-05, 4.1319501178804785e-05, 4.1319501178804785e-05, 4.1319501178804785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.1319501178804785e-05

Optimization complete. Final v2v error: 5.584754467010498 mm

Highest mean error: 5.916832447052002 mm for frame 150

Lowest mean error: 5.187417507171631 mm for frame 41

Saving results

Total time: 50.44238519668579
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989180
Iteration 2/25 | Loss: 0.00244069
Iteration 3/25 | Loss: 0.00228413
Iteration 4/25 | Loss: 0.00225494
Iteration 5/25 | Loss: 0.00224276
Iteration 6/25 | Loss: 0.00223933
Iteration 7/25 | Loss: 0.00223850
Iteration 8/25 | Loss: 0.00223850
Iteration 9/25 | Loss: 0.00223850
Iteration 10/25 | Loss: 0.00223850
Iteration 11/25 | Loss: 0.00223850
Iteration 12/25 | Loss: 0.00223850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0022384990006685257, 0.0022384990006685257, 0.0022384990006685257, 0.0022384990006685257, 0.0022384990006685257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022384990006685257

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31062329
Iteration 2/25 | Loss: 0.00204649
Iteration 3/25 | Loss: 0.00204642
Iteration 4/25 | Loss: 0.00204642
Iteration 5/25 | Loss: 0.00204642
Iteration 6/25 | Loss: 0.00204642
Iteration 7/25 | Loss: 0.00204642
Iteration 8/25 | Loss: 0.00204642
Iteration 9/25 | Loss: 0.00204642
Iteration 10/25 | Loss: 0.00204642
Iteration 11/25 | Loss: 0.00204642
Iteration 12/25 | Loss: 0.00204642
Iteration 13/25 | Loss: 0.00204642
Iteration 14/25 | Loss: 0.00204642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002046421868726611, 0.002046421868726611, 0.002046421868726611, 0.002046421868726611, 0.002046421868726611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002046421868726611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204642
Iteration 2/1000 | Loss: 0.00010368
Iteration 3/1000 | Loss: 0.00007792
Iteration 4/1000 | Loss: 0.00006958
Iteration 5/1000 | Loss: 0.00006575
Iteration 6/1000 | Loss: 0.00006306
Iteration 7/1000 | Loss: 0.00006032
Iteration 8/1000 | Loss: 0.00005777
Iteration 9/1000 | Loss: 0.00005679
Iteration 10/1000 | Loss: 0.00005551
Iteration 11/1000 | Loss: 0.00005452
Iteration 12/1000 | Loss: 0.00005371
Iteration 13/1000 | Loss: 0.00005335
Iteration 14/1000 | Loss: 0.00005305
Iteration 15/1000 | Loss: 0.00005280
Iteration 16/1000 | Loss: 0.00005264
Iteration 17/1000 | Loss: 0.00005262
Iteration 18/1000 | Loss: 0.00005256
Iteration 19/1000 | Loss: 0.00005249
Iteration 20/1000 | Loss: 0.00005249
Iteration 21/1000 | Loss: 0.00005246
Iteration 22/1000 | Loss: 0.00005245
Iteration 23/1000 | Loss: 0.00005245
Iteration 24/1000 | Loss: 0.00005245
Iteration 25/1000 | Loss: 0.00005244
Iteration 26/1000 | Loss: 0.00005244
Iteration 27/1000 | Loss: 0.00005242
Iteration 28/1000 | Loss: 0.00005242
Iteration 29/1000 | Loss: 0.00005242
Iteration 30/1000 | Loss: 0.00005242
Iteration 31/1000 | Loss: 0.00005242
Iteration 32/1000 | Loss: 0.00005242
Iteration 33/1000 | Loss: 0.00005242
Iteration 34/1000 | Loss: 0.00005242
Iteration 35/1000 | Loss: 0.00005242
Iteration 36/1000 | Loss: 0.00005242
Iteration 37/1000 | Loss: 0.00005241
Iteration 38/1000 | Loss: 0.00005241
Iteration 39/1000 | Loss: 0.00005241
Iteration 40/1000 | Loss: 0.00005241
Iteration 41/1000 | Loss: 0.00005241
Iteration 42/1000 | Loss: 0.00005241
Iteration 43/1000 | Loss: 0.00005240
Iteration 44/1000 | Loss: 0.00005240
Iteration 45/1000 | Loss: 0.00005240
Iteration 46/1000 | Loss: 0.00005240
Iteration 47/1000 | Loss: 0.00005240
Iteration 48/1000 | Loss: 0.00005239
Iteration 49/1000 | Loss: 0.00005239
Iteration 50/1000 | Loss: 0.00005239
Iteration 51/1000 | Loss: 0.00005239
Iteration 52/1000 | Loss: 0.00005239
Iteration 53/1000 | Loss: 0.00005239
Iteration 54/1000 | Loss: 0.00005239
Iteration 55/1000 | Loss: 0.00005239
Iteration 56/1000 | Loss: 0.00005239
Iteration 57/1000 | Loss: 0.00005238
Iteration 58/1000 | Loss: 0.00005238
Iteration 59/1000 | Loss: 0.00005238
Iteration 60/1000 | Loss: 0.00005238
Iteration 61/1000 | Loss: 0.00005237
Iteration 62/1000 | Loss: 0.00005237
Iteration 63/1000 | Loss: 0.00005237
Iteration 64/1000 | Loss: 0.00005237
Iteration 65/1000 | Loss: 0.00005237
Iteration 66/1000 | Loss: 0.00005237
Iteration 67/1000 | Loss: 0.00005237
Iteration 68/1000 | Loss: 0.00005237
Iteration 69/1000 | Loss: 0.00005237
Iteration 70/1000 | Loss: 0.00005237
Iteration 71/1000 | Loss: 0.00005237
Iteration 72/1000 | Loss: 0.00005237
Iteration 73/1000 | Loss: 0.00005237
Iteration 74/1000 | Loss: 0.00005237
Iteration 75/1000 | Loss: 0.00005237
Iteration 76/1000 | Loss: 0.00005237
Iteration 77/1000 | Loss: 0.00005237
Iteration 78/1000 | Loss: 0.00005236
Iteration 79/1000 | Loss: 0.00005236
Iteration 80/1000 | Loss: 0.00005236
Iteration 81/1000 | Loss: 0.00005236
Iteration 82/1000 | Loss: 0.00005236
Iteration 83/1000 | Loss: 0.00005236
Iteration 84/1000 | Loss: 0.00005236
Iteration 85/1000 | Loss: 0.00005236
Iteration 86/1000 | Loss: 0.00005236
Iteration 87/1000 | Loss: 0.00005236
Iteration 88/1000 | Loss: 0.00005236
Iteration 89/1000 | Loss: 0.00005236
Iteration 90/1000 | Loss: 0.00005236
Iteration 91/1000 | Loss: 0.00005236
Iteration 92/1000 | Loss: 0.00005236
Iteration 93/1000 | Loss: 0.00005236
Iteration 94/1000 | Loss: 0.00005236
Iteration 95/1000 | Loss: 0.00005236
Iteration 96/1000 | Loss: 0.00005236
Iteration 97/1000 | Loss: 0.00005236
Iteration 98/1000 | Loss: 0.00005236
Iteration 99/1000 | Loss: 0.00005236
Iteration 100/1000 | Loss: 0.00005236
Iteration 101/1000 | Loss: 0.00005236
Iteration 102/1000 | Loss: 0.00005236
Iteration 103/1000 | Loss: 0.00005236
Iteration 104/1000 | Loss: 0.00005236
Iteration 105/1000 | Loss: 0.00005236
Iteration 106/1000 | Loss: 0.00005236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [5.23584931215737e-05, 5.23584931215737e-05, 5.23584931215737e-05, 5.23584931215737e-05, 5.23584931215737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.23584931215737e-05

Optimization complete. Final v2v error: 6.167106628417969 mm

Highest mean error: 6.4159417152404785 mm for frame 108

Lowest mean error: 5.8794708251953125 mm for frame 35

Saving results

Total time: 40.1037700176239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121473
Iteration 2/25 | Loss: 0.01121473
Iteration 3/25 | Loss: 0.01121473
Iteration 4/25 | Loss: 0.01121473
Iteration 5/25 | Loss: 0.01121473
Iteration 6/25 | Loss: 0.01121473
Iteration 7/25 | Loss: 0.01121472
Iteration 8/25 | Loss: 0.01121472
Iteration 9/25 | Loss: 0.01121472
Iteration 10/25 | Loss: 0.01121472
Iteration 11/25 | Loss: 0.01121472
Iteration 12/25 | Loss: 0.01121471
Iteration 13/25 | Loss: 0.00309376
Iteration 14/25 | Loss: 0.00190821
Iteration 15/25 | Loss: 0.00228526
Iteration 16/25 | Loss: 0.00211295
Iteration 17/25 | Loss: 0.00166813
Iteration 18/25 | Loss: 0.00156425
Iteration 19/25 | Loss: 0.00132142
Iteration 20/25 | Loss: 0.00127763
Iteration 21/25 | Loss: 0.00125513
Iteration 22/25 | Loss: 0.00124792
Iteration 23/25 | Loss: 0.00124865
Iteration 24/25 | Loss: 0.00123384
Iteration 25/25 | Loss: 0.00123798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58831692
Iteration 2/25 | Loss: 0.00125416
Iteration 3/25 | Loss: 0.00108748
Iteration 4/25 | Loss: 0.00108747
Iteration 5/25 | Loss: 0.00108747
Iteration 6/25 | Loss: 0.00108747
Iteration 7/25 | Loss: 0.00108747
Iteration 8/25 | Loss: 0.00108747
Iteration 9/25 | Loss: 0.00108747
Iteration 10/25 | Loss: 0.00108747
Iteration 11/25 | Loss: 0.00108747
Iteration 12/25 | Loss: 0.00108747
Iteration 13/25 | Loss: 0.00108747
Iteration 14/25 | Loss: 0.00108747
Iteration 15/25 | Loss: 0.00108747
Iteration 16/25 | Loss: 0.00108747
Iteration 17/25 | Loss: 0.00108747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010874724248424172, 0.0010874724248424172, 0.0010874724248424172, 0.0010874724248424172, 0.0010874724248424172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010874724248424172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108747
Iteration 2/1000 | Loss: 0.00013332
Iteration 3/1000 | Loss: 0.00014605
Iteration 4/1000 | Loss: 0.00021333
Iteration 5/1000 | Loss: 0.00107298
Iteration 6/1000 | Loss: 0.00013859
Iteration 7/1000 | Loss: 0.00008589
Iteration 8/1000 | Loss: 0.00008967
Iteration 9/1000 | Loss: 0.00020051
Iteration 10/1000 | Loss: 0.00007659
Iteration 11/1000 | Loss: 0.00006212
Iteration 12/1000 | Loss: 0.00006611
Iteration 13/1000 | Loss: 0.00007830
Iteration 14/1000 | Loss: 0.00007082
Iteration 15/1000 | Loss: 0.00005063
Iteration 16/1000 | Loss: 0.00004504
Iteration 17/1000 | Loss: 0.00004550
Iteration 18/1000 | Loss: 0.00004330
Iteration 19/1000 | Loss: 0.00005205
Iteration 20/1000 | Loss: 0.00004553
Iteration 21/1000 | Loss: 0.00004317
Iteration 22/1000 | Loss: 0.00004014
Iteration 23/1000 | Loss: 0.00004549
Iteration 24/1000 | Loss: 0.00006942
Iteration 25/1000 | Loss: 0.00003945
Iteration 26/1000 | Loss: 0.00004261
Iteration 27/1000 | Loss: 0.00004860
Iteration 28/1000 | Loss: 0.00005100
Iteration 29/1000 | Loss: 0.00006713
Iteration 30/1000 | Loss: 0.00004625
Iteration 31/1000 | Loss: 0.00004683
Iteration 32/1000 | Loss: 0.00021383
Iteration 33/1000 | Loss: 0.00019446
Iteration 34/1000 | Loss: 0.00006650
Iteration 35/1000 | Loss: 0.00006782
Iteration 36/1000 | Loss: 0.00004845
Iteration 37/1000 | Loss: 0.00005937
Iteration 38/1000 | Loss: 0.00004760
Iteration 39/1000 | Loss: 0.00003940
Iteration 40/1000 | Loss: 0.00004318
Iteration 41/1000 | Loss: 0.00004028
Iteration 42/1000 | Loss: 0.00004066
Iteration 43/1000 | Loss: 0.00004800
Iteration 44/1000 | Loss: 0.00004413
Iteration 45/1000 | Loss: 0.00005085
Iteration 46/1000 | Loss: 0.00005697
Iteration 47/1000 | Loss: 0.00004316
Iteration 48/1000 | Loss: 0.00003959
Iteration 49/1000 | Loss: 0.00004910
Iteration 50/1000 | Loss: 0.00005593
Iteration 51/1000 | Loss: 0.00004141
Iteration 52/1000 | Loss: 0.00004073
Iteration 53/1000 | Loss: 0.00003954
Iteration 54/1000 | Loss: 0.00004612
Iteration 55/1000 | Loss: 0.00007209
Iteration 56/1000 | Loss: 0.00004522
Iteration 57/1000 | Loss: 0.00004868
Iteration 58/1000 | Loss: 0.00007221
Iteration 59/1000 | Loss: 0.00004931
Iteration 60/1000 | Loss: 0.00003883
Iteration 61/1000 | Loss: 0.00005269
Iteration 62/1000 | Loss: 0.00004701
Iteration 63/1000 | Loss: 0.00005334
Iteration 64/1000 | Loss: 0.00004290
Iteration 65/1000 | Loss: 0.00004238
Iteration 66/1000 | Loss: 0.00005745
Iteration 67/1000 | Loss: 0.00004264
Iteration 68/1000 | Loss: 0.00004264
Iteration 69/1000 | Loss: 0.00005537
Iteration 70/1000 | Loss: 0.00004355
Iteration 71/1000 | Loss: 0.00004375
Iteration 72/1000 | Loss: 0.00004648
Iteration 73/1000 | Loss: 0.00004140
Iteration 74/1000 | Loss: 0.00004600
Iteration 75/1000 | Loss: 0.00004430
Iteration 76/1000 | Loss: 0.00004066
Iteration 77/1000 | Loss: 0.00004393
Iteration 78/1000 | Loss: 0.00004421
Iteration 79/1000 | Loss: 0.00004237
Iteration 80/1000 | Loss: 0.00004061
Iteration 81/1000 | Loss: 0.00004110
Iteration 82/1000 | Loss: 0.00004110
Iteration 83/1000 | Loss: 0.00007857
Iteration 84/1000 | Loss: 0.00005214
Iteration 85/1000 | Loss: 0.00005153
Iteration 86/1000 | Loss: 0.00003983
Iteration 87/1000 | Loss: 0.00005643
Iteration 88/1000 | Loss: 0.00005943
Iteration 89/1000 | Loss: 0.00005678
Iteration 90/1000 | Loss: 0.00004031
Iteration 91/1000 | Loss: 0.00004674
Iteration 92/1000 | Loss: 0.00005846
Iteration 93/1000 | Loss: 0.00005394
Iteration 94/1000 | Loss: 0.00004719
Iteration 95/1000 | Loss: 0.00004401
Iteration 96/1000 | Loss: 0.00006686
Iteration 97/1000 | Loss: 0.00004196
Iteration 98/1000 | Loss: 0.00003965
Iteration 99/1000 | Loss: 0.00004114
Iteration 100/1000 | Loss: 0.00003943
Iteration 101/1000 | Loss: 0.00004617
Iteration 102/1000 | Loss: 0.00004082
Iteration 103/1000 | Loss: 0.00005118
Iteration 104/1000 | Loss: 0.00004220
Iteration 105/1000 | Loss: 0.00004203
Iteration 106/1000 | Loss: 0.00004646
Iteration 107/1000 | Loss: 0.00004117
Iteration 108/1000 | Loss: 0.00004559
Iteration 109/1000 | Loss: 0.00004074
Iteration 110/1000 | Loss: 0.00004300
Iteration 111/1000 | Loss: 0.00004270
Iteration 112/1000 | Loss: 0.00004279
Iteration 113/1000 | Loss: 0.00004439
Iteration 114/1000 | Loss: 0.00004662
Iteration 115/1000 | Loss: 0.00004253
Iteration 116/1000 | Loss: 0.00004043
Iteration 117/1000 | Loss: 0.00004043
Iteration 118/1000 | Loss: 0.00004132
Iteration 119/1000 | Loss: 0.00004154
Iteration 120/1000 | Loss: 0.00004268
Iteration 121/1000 | Loss: 0.00004268
Iteration 122/1000 | Loss: 0.00007292
Iteration 123/1000 | Loss: 0.00004767
Iteration 124/1000 | Loss: 0.00004552
Iteration 125/1000 | Loss: 0.00004438
Iteration 126/1000 | Loss: 0.00004385
Iteration 127/1000 | Loss: 0.00003982
Iteration 128/1000 | Loss: 0.00005561
Iteration 129/1000 | Loss: 0.00004670
Iteration 130/1000 | Loss: 0.00004070
Iteration 131/1000 | Loss: 0.00004936
Iteration 132/1000 | Loss: 0.00004439
Iteration 133/1000 | Loss: 0.00004531
Iteration 134/1000 | Loss: 0.00004309
Iteration 135/1000 | Loss: 0.00005016
Iteration 136/1000 | Loss: 0.00006514
Iteration 137/1000 | Loss: 0.00004300
Iteration 138/1000 | Loss: 0.00004509
Iteration 139/1000 | Loss: 0.00004730
Iteration 140/1000 | Loss: 0.00005691
Iteration 141/1000 | Loss: 0.00007362
Iteration 142/1000 | Loss: 0.00007235
Iteration 143/1000 | Loss: 0.00079359
Iteration 144/1000 | Loss: 0.00013933
Iteration 145/1000 | Loss: 0.00050821
Iteration 146/1000 | Loss: 0.00005455
Iteration 147/1000 | Loss: 0.00004057
Iteration 148/1000 | Loss: 0.00004828
Iteration 149/1000 | Loss: 0.00004316
Iteration 150/1000 | Loss: 0.00003886
Iteration 151/1000 | Loss: 0.00005550
Iteration 152/1000 | Loss: 0.00004364
Iteration 153/1000 | Loss: 0.00004085
Iteration 154/1000 | Loss: 0.00003751
Iteration 155/1000 | Loss: 0.00003751
Iteration 156/1000 | Loss: 0.00003751
Iteration 157/1000 | Loss: 0.00003751
Iteration 158/1000 | Loss: 0.00003751
Iteration 159/1000 | Loss: 0.00003750
Iteration 160/1000 | Loss: 0.00003750
Iteration 161/1000 | Loss: 0.00003750
Iteration 162/1000 | Loss: 0.00003750
Iteration 163/1000 | Loss: 0.00003750
Iteration 164/1000 | Loss: 0.00003750
Iteration 165/1000 | Loss: 0.00003750
Iteration 166/1000 | Loss: 0.00003750
Iteration 167/1000 | Loss: 0.00003750
Iteration 168/1000 | Loss: 0.00003750
Iteration 169/1000 | Loss: 0.00003750
Iteration 170/1000 | Loss: 0.00003749
Iteration 171/1000 | Loss: 0.00003749
Iteration 172/1000 | Loss: 0.00003749
Iteration 173/1000 | Loss: 0.00003749
Iteration 174/1000 | Loss: 0.00003749
Iteration 175/1000 | Loss: 0.00003749
Iteration 176/1000 | Loss: 0.00003749
Iteration 177/1000 | Loss: 0.00003749
Iteration 178/1000 | Loss: 0.00003749
Iteration 179/1000 | Loss: 0.00003772
Iteration 180/1000 | Loss: 0.00003772
Iteration 181/1000 | Loss: 0.00003772
Iteration 182/1000 | Loss: 0.00003772
Iteration 183/1000 | Loss: 0.00003772
Iteration 184/1000 | Loss: 0.00003809
Iteration 185/1000 | Loss: 0.00003790
Iteration 186/1000 | Loss: 0.00003790
Iteration 187/1000 | Loss: 0.00003790
Iteration 188/1000 | Loss: 0.00003789
Iteration 189/1000 | Loss: 0.00004049
Iteration 190/1000 | Loss: 0.00004565
Iteration 191/1000 | Loss: 0.00004196
Iteration 192/1000 | Loss: 0.00006041
Iteration 193/1000 | Loss: 0.00004018
Iteration 194/1000 | Loss: 0.00003861
Iteration 195/1000 | Loss: 0.00003825
Iteration 196/1000 | Loss: 0.00003953
Iteration 197/1000 | Loss: 0.00003849
Iteration 198/1000 | Loss: 0.00004714
Iteration 199/1000 | Loss: 0.00004488
Iteration 200/1000 | Loss: 0.00005677
Iteration 201/1000 | Loss: 0.00004016
Iteration 202/1000 | Loss: 0.00003819
Iteration 203/1000 | Loss: 0.00004535
Iteration 204/1000 | Loss: 0.00003965
Iteration 205/1000 | Loss: 0.00004043
Iteration 206/1000 | Loss: 0.00004055
Iteration 207/1000 | Loss: 0.00004117
Iteration 208/1000 | Loss: 0.00004304
Iteration 209/1000 | Loss: 0.00004533
Iteration 210/1000 | Loss: 0.00005296
Iteration 211/1000 | Loss: 0.00004368
Iteration 212/1000 | Loss: 0.00004286
Iteration 213/1000 | Loss: 0.00004213
Iteration 214/1000 | Loss: 0.00004787
Iteration 215/1000 | Loss: 0.00006774
Iteration 216/1000 | Loss: 0.00004340
Iteration 217/1000 | Loss: 0.00006524
Iteration 218/1000 | Loss: 0.00004268
Iteration 219/1000 | Loss: 0.00003929
Iteration 220/1000 | Loss: 0.00004036
Iteration 221/1000 | Loss: 0.00004069
Iteration 222/1000 | Loss: 0.00005134
Iteration 223/1000 | Loss: 0.00004110
Iteration 224/1000 | Loss: 0.00004159
Iteration 225/1000 | Loss: 0.00004041
Iteration 226/1000 | Loss: 0.00003981
Iteration 227/1000 | Loss: 0.00005219
Iteration 228/1000 | Loss: 0.00007346
Iteration 229/1000 | Loss: 0.00043481
Iteration 230/1000 | Loss: 0.00008935
Iteration 231/1000 | Loss: 0.00013062
Iteration 232/1000 | Loss: 0.00004693
Iteration 233/1000 | Loss: 0.00005494
Iteration 234/1000 | Loss: 0.00019263
Iteration 235/1000 | Loss: 0.00005712
Iteration 236/1000 | Loss: 0.00008385
Iteration 237/1000 | Loss: 0.00004490
Iteration 238/1000 | Loss: 0.00004653
Iteration 239/1000 | Loss: 0.00004259
Iteration 240/1000 | Loss: 0.00004112
Iteration 241/1000 | Loss: 0.00004091
Iteration 242/1000 | Loss: 0.00005920
Iteration 243/1000 | Loss: 0.00004613
Iteration 244/1000 | Loss: 0.00004893
Iteration 245/1000 | Loss: 0.00009897
Iteration 246/1000 | Loss: 0.00004339
Iteration 247/1000 | Loss: 0.00004313
Iteration 248/1000 | Loss: 0.00004368
Iteration 249/1000 | Loss: 0.00008464
Iteration 250/1000 | Loss: 0.00011936
Iteration 251/1000 | Loss: 0.00005759
Iteration 252/1000 | Loss: 0.00008209
Iteration 253/1000 | Loss: 0.00004200
Iteration 254/1000 | Loss: 0.00004278
Iteration 255/1000 | Loss: 0.00004724
Iteration 256/1000 | Loss: 0.00005125
Iteration 257/1000 | Loss: 0.00004534
Iteration 258/1000 | Loss: 0.00004988
Iteration 259/1000 | Loss: 0.00008055
Iteration 260/1000 | Loss: 0.00010013
Iteration 261/1000 | Loss: 0.00006550
Iteration 262/1000 | Loss: 0.00005152
Iteration 263/1000 | Loss: 0.00005210
Iteration 264/1000 | Loss: 0.00017103
Iteration 265/1000 | Loss: 0.00007009
Iteration 266/1000 | Loss: 0.00005154
Iteration 267/1000 | Loss: 0.00013942
Iteration 268/1000 | Loss: 0.00005906
Iteration 269/1000 | Loss: 0.00005047
Iteration 270/1000 | Loss: 0.00004453
Iteration 271/1000 | Loss: 0.00004359
Iteration 272/1000 | Loss: 0.00010785
Iteration 273/1000 | Loss: 0.00004265
Iteration 274/1000 | Loss: 0.00003955
Iteration 275/1000 | Loss: 0.00004859
Iteration 276/1000 | Loss: 0.00004498
Iteration 277/1000 | Loss: 0.00004607
Iteration 278/1000 | Loss: 0.00006964
Iteration 279/1000 | Loss: 0.00004918
Iteration 280/1000 | Loss: 0.00004191
Iteration 281/1000 | Loss: 0.00004626
Iteration 282/1000 | Loss: 0.00005287
Iteration 283/1000 | Loss: 0.00004225
Iteration 284/1000 | Loss: 0.00006904
Iteration 285/1000 | Loss: 0.00004387
Iteration 286/1000 | Loss: 0.00006478
Iteration 287/1000 | Loss: 0.00004153
Iteration 288/1000 | Loss: 0.00004271
Iteration 289/1000 | Loss: 0.00004083
Iteration 290/1000 | Loss: 0.00004436
Iteration 291/1000 | Loss: 0.00004364
Iteration 292/1000 | Loss: 0.00009047
Iteration 293/1000 | Loss: 0.00004469
Iteration 294/1000 | Loss: 0.00004776
Iteration 295/1000 | Loss: 0.00005370
Iteration 296/1000 | Loss: 0.00003936
Iteration 297/1000 | Loss: 0.00006204
Iteration 298/1000 | Loss: 0.00004388
Iteration 299/1000 | Loss: 0.00004515
Iteration 300/1000 | Loss: 0.00004717
Iteration 301/1000 | Loss: 0.00017991
Iteration 302/1000 | Loss: 0.00004556
Iteration 303/1000 | Loss: 0.00004154
Iteration 304/1000 | Loss: 0.00004418
Iteration 305/1000 | Loss: 0.00003946
Iteration 306/1000 | Loss: 0.00005160
Iteration 307/1000 | Loss: 0.00004213
Iteration 308/1000 | Loss: 0.00003774
Iteration 309/1000 | Loss: 0.00003972
Iteration 310/1000 | Loss: 0.00004021
Iteration 311/1000 | Loss: 0.00004879
Iteration 312/1000 | Loss: 0.00003881
Iteration 313/1000 | Loss: 0.00004350
Iteration 314/1000 | Loss: 0.00004607
Iteration 315/1000 | Loss: 0.00005076
Iteration 316/1000 | Loss: 0.00004462
Iteration 317/1000 | Loss: 0.00003911
Iteration 318/1000 | Loss: 0.00003994
Iteration 319/1000 | Loss: 0.00004230
Iteration 320/1000 | Loss: 0.00006024
Iteration 321/1000 | Loss: 0.00004492
Iteration 322/1000 | Loss: 0.00004082
Iteration 323/1000 | Loss: 0.00004190
Iteration 324/1000 | Loss: 0.00004405
Iteration 325/1000 | Loss: 0.00004488
Iteration 326/1000 | Loss: 0.00003880
Iteration 327/1000 | Loss: 0.00003786
Iteration 328/1000 | Loss: 0.00004414
Iteration 329/1000 | Loss: 0.00004544
Iteration 330/1000 | Loss: 0.00003840
Iteration 331/1000 | Loss: 0.00004283
Iteration 332/1000 | Loss: 0.00004784
Iteration 333/1000 | Loss: 0.00003910
Iteration 334/1000 | Loss: 0.00004243
Iteration 335/1000 | Loss: 0.00005226
Iteration 336/1000 | Loss: 0.00003808
Iteration 337/1000 | Loss: 0.00004107
Iteration 338/1000 | Loss: 0.00003846
Iteration 339/1000 | Loss: 0.00003769
Iteration 340/1000 | Loss: 0.00004564
Iteration 341/1000 | Loss: 0.00004072
Iteration 342/1000 | Loss: 0.00003773
Iteration 343/1000 | Loss: 0.00004128
Iteration 344/1000 | Loss: 0.00004491
Iteration 345/1000 | Loss: 0.00003861
Iteration 346/1000 | Loss: 0.00004406
Iteration 347/1000 | Loss: 0.00004854
Iteration 348/1000 | Loss: 0.00004844
Iteration 349/1000 | Loss: 0.00004381
Iteration 350/1000 | Loss: 0.00005029
Iteration 351/1000 | Loss: 0.00004541
Iteration 352/1000 | Loss: 0.00004804
Iteration 353/1000 | Loss: 0.00003810
Iteration 354/1000 | Loss: 0.00004221
Iteration 355/1000 | Loss: 0.00004133
Iteration 356/1000 | Loss: 0.00003790
Iteration 357/1000 | Loss: 0.00004588
Iteration 358/1000 | Loss: 0.00003887
Iteration 359/1000 | Loss: 0.00003798
Iteration 360/1000 | Loss: 0.00004432
Iteration 361/1000 | Loss: 0.00004007
Iteration 362/1000 | Loss: 0.00009508
Iteration 363/1000 | Loss: 0.00004610
Iteration 364/1000 | Loss: 0.00004272
Iteration 365/1000 | Loss: 0.00004245
Iteration 366/1000 | Loss: 0.00004508
Iteration 367/1000 | Loss: 0.00005034
Iteration 368/1000 | Loss: 0.00004119
Iteration 369/1000 | Loss: 0.00005025
Iteration 370/1000 | Loss: 0.00008037
Iteration 371/1000 | Loss: 0.00004011
Iteration 372/1000 | Loss: 0.00005655
Iteration 373/1000 | Loss: 0.00004200
Iteration 374/1000 | Loss: 0.00004091
Iteration 375/1000 | Loss: 0.00004719
Iteration 376/1000 | Loss: 0.00005267
Iteration 377/1000 | Loss: 0.00004087
Iteration 378/1000 | Loss: 0.00004686
Iteration 379/1000 | Loss: 0.00004290
Iteration 380/1000 | Loss: 0.00003998
Iteration 381/1000 | Loss: 0.00004382
Iteration 382/1000 | Loss: 0.00004218
Iteration 383/1000 | Loss: 0.00004465
Iteration 384/1000 | Loss: 0.00004221
Iteration 385/1000 | Loss: 0.00004511
Iteration 386/1000 | Loss: 0.00004399
Iteration 387/1000 | Loss: 0.00004255
Iteration 388/1000 | Loss: 0.00003886
Iteration 389/1000 | Loss: 0.00004074
Iteration 390/1000 | Loss: 0.00004292
Iteration 391/1000 | Loss: 0.00003847
Iteration 392/1000 | Loss: 0.00004300
Iteration 393/1000 | Loss: 0.00006590
Iteration 394/1000 | Loss: 0.00003938
Iteration 395/1000 | Loss: 0.00004132
Iteration 396/1000 | Loss: 0.00005265
Iteration 397/1000 | Loss: 0.00003870
Iteration 398/1000 | Loss: 0.00004552
Iteration 399/1000 | Loss: 0.00007139
Iteration 400/1000 | Loss: 0.00003887
Iteration 401/1000 | Loss: 0.00004565
Iteration 402/1000 | Loss: 0.00004565
Iteration 403/1000 | Loss: 0.00007023
Iteration 404/1000 | Loss: 0.00004078
Iteration 405/1000 | Loss: 0.00004617
Iteration 406/1000 | Loss: 0.00003886
Iteration 407/1000 | Loss: 0.00003852
Iteration 408/1000 | Loss: 0.00004407
Iteration 409/1000 | Loss: 0.00006886
Iteration 410/1000 | Loss: 0.00003959
Iteration 411/1000 | Loss: 0.00004341
Iteration 412/1000 | Loss: 0.00004658
Iteration 413/1000 | Loss: 0.00003778
Iteration 414/1000 | Loss: 0.00004131
Iteration 415/1000 | Loss: 0.00004053
Iteration 416/1000 | Loss: 0.00003750
Iteration 417/1000 | Loss: 0.00004077
Iteration 418/1000 | Loss: 0.00003885
Iteration 419/1000 | Loss: 0.00003796
Iteration 420/1000 | Loss: 0.00003921
Iteration 421/1000 | Loss: 0.00004556
Iteration 422/1000 | Loss: 0.00004185
Iteration 423/1000 | Loss: 0.00003768
Iteration 424/1000 | Loss: 0.00003919
Iteration 425/1000 | Loss: 0.00004097
Iteration 426/1000 | Loss: 0.00003908
Iteration 427/1000 | Loss: 0.00005640
Iteration 428/1000 | Loss: 0.00003768
Iteration 429/1000 | Loss: 0.00003966
Iteration 430/1000 | Loss: 0.00004047
Iteration 431/1000 | Loss: 0.00004446
Iteration 432/1000 | Loss: 0.00004239
Iteration 433/1000 | Loss: 0.00004091
Iteration 434/1000 | Loss: 0.00003851
Iteration 435/1000 | Loss: 0.00004718
Iteration 436/1000 | Loss: 0.00004152
Iteration 437/1000 | Loss: 0.00004735
Iteration 438/1000 | Loss: 0.00004222
Iteration 439/1000 | Loss: 0.00004227
Iteration 440/1000 | Loss: 0.00004135
Iteration 441/1000 | Loss: 0.00003974
Iteration 442/1000 | Loss: 0.00004405
Iteration 443/1000 | Loss: 0.00005136
Iteration 444/1000 | Loss: 0.00004046
Iteration 445/1000 | Loss: 0.00004084
Iteration 446/1000 | Loss: 0.00004798
Iteration 447/1000 | Loss: 0.00004472
Iteration 448/1000 | Loss: 0.00004000
Iteration 449/1000 | Loss: 0.00004137
Iteration 450/1000 | Loss: 0.00004171
Iteration 451/1000 | Loss: 0.00005705
Iteration 452/1000 | Loss: 0.00003958
Iteration 453/1000 | Loss: 0.00003785
Iteration 454/1000 | Loss: 0.00003942
Iteration 455/1000 | Loss: 0.00004125
Iteration 456/1000 | Loss: 0.00003934
Iteration 457/1000 | Loss: 0.00003974
Iteration 458/1000 | Loss: 0.00003974
Iteration 459/1000 | Loss: 0.00003937
Iteration 460/1000 | Loss: 0.00004125
Iteration 461/1000 | Loss: 0.00005217
Iteration 462/1000 | Loss: 0.00003849
Iteration 463/1000 | Loss: 0.00003776
Iteration 464/1000 | Loss: 0.00004137
Iteration 465/1000 | Loss: 0.00004266
Iteration 466/1000 | Loss: 0.00003881
Iteration 467/1000 | Loss: 0.00003788
Iteration 468/1000 | Loss: 0.00004378
Iteration 469/1000 | Loss: 0.00004007
Iteration 470/1000 | Loss: 0.00003806
Iteration 471/1000 | Loss: 0.00004052
Iteration 472/1000 | Loss: 0.00004050
Iteration 473/1000 | Loss: 0.00006048
Iteration 474/1000 | Loss: 0.00003842
Iteration 475/1000 | Loss: 0.00004557
Iteration 476/1000 | Loss: 0.00003940
Iteration 477/1000 | Loss: 0.00004424
Iteration 478/1000 | Loss: 0.00003941
Iteration 479/1000 | Loss: 0.00004329
Iteration 480/1000 | Loss: 0.00004027
Iteration 481/1000 | Loss: 0.00006277
Iteration 482/1000 | Loss: 0.00003823
Iteration 483/1000 | Loss: 0.00004301
Iteration 484/1000 | Loss: 0.00003877
Iteration 485/1000 | Loss: 0.00004125
Iteration 486/1000 | Loss: 0.00004125
Iteration 487/1000 | Loss: 0.00005025
Iteration 488/1000 | Loss: 0.00004156
Iteration 489/1000 | Loss: 0.00004017
Iteration 490/1000 | Loss: 0.00003950
Iteration 491/1000 | Loss: 0.00004351
Iteration 492/1000 | Loss: 0.00004462
Iteration 493/1000 | Loss: 0.00003832
Iteration 494/1000 | Loss: 0.00004166
Iteration 495/1000 | Loss: 0.00006276
Iteration 496/1000 | Loss: 0.00003840
Iteration 497/1000 | Loss: 0.00004044
Iteration 498/1000 | Loss: 0.00004095
Iteration 499/1000 | Loss: 0.00004036
Iteration 500/1000 | Loss: 0.00003763
Iteration 501/1000 | Loss: 0.00003917
Iteration 502/1000 | Loss: 0.00004181
Iteration 503/1000 | Loss: 0.00004710
Iteration 504/1000 | Loss: 0.00003743
Iteration 505/1000 | Loss: 0.00004199
Iteration 506/1000 | Loss: 0.00005363
Iteration 507/1000 | Loss: 0.00004202
Iteration 508/1000 | Loss: 0.00004237
Iteration 509/1000 | Loss: 0.00004765
Iteration 510/1000 | Loss: 0.00004366
Iteration 511/1000 | Loss: 0.00004491
Iteration 512/1000 | Loss: 0.00003985
Iteration 513/1000 | Loss: 0.00004172
Iteration 514/1000 | Loss: 0.00005331
Iteration 515/1000 | Loss: 0.00004085
Iteration 516/1000 | Loss: 0.00003989
Iteration 517/1000 | Loss: 0.00004389
Iteration 518/1000 | Loss: 0.00004051
Iteration 519/1000 | Loss: 0.00004370
Iteration 520/1000 | Loss: 0.00006825
Iteration 521/1000 | Loss: 0.00004347
Iteration 522/1000 | Loss: 0.00006858
Iteration 523/1000 | Loss: 0.00004272
Iteration 524/1000 | Loss: 0.00005078
Iteration 525/1000 | Loss: 0.00004820
Iteration 526/1000 | Loss: 0.00004441
Iteration 527/1000 | Loss: 0.00004168
Iteration 528/1000 | Loss: 0.00004696
Iteration 529/1000 | Loss: 0.00004025
Iteration 530/1000 | Loss: 0.00005154
Iteration 531/1000 | Loss: 0.00003986
Iteration 532/1000 | Loss: 0.00003740
Iteration 533/1000 | Loss: 0.00003779
Iteration 534/1000 | Loss: 0.00004065
Iteration 535/1000 | Loss: 0.00004244
Iteration 536/1000 | Loss: 0.00004167
Iteration 537/1000 | Loss: 0.00004166
Iteration 538/1000 | Loss: 0.00006118
Iteration 539/1000 | Loss: 0.00004695
Iteration 540/1000 | Loss: 0.00005111
Iteration 541/1000 | Loss: 0.00003970
Iteration 542/1000 | Loss: 0.00003927
Iteration 543/1000 | Loss: 0.00003965
Iteration 544/1000 | Loss: 0.00003772
Iteration 545/1000 | Loss: 0.00004547
Iteration 546/1000 | Loss: 0.00004195
Iteration 547/1000 | Loss: 0.00005237
Iteration 548/1000 | Loss: 0.00004141
Iteration 549/1000 | Loss: 0.00010701
Iteration 550/1000 | Loss: 0.00006010
Iteration 551/1000 | Loss: 0.00005582
Iteration 552/1000 | Loss: 0.00003734
Iteration 553/1000 | Loss: 0.00004108
Iteration 554/1000 | Loss: 0.00003929
Iteration 555/1000 | Loss: 0.00003787
Iteration 556/1000 | Loss: 0.00004060
Iteration 557/1000 | Loss: 0.00003919
Iteration 558/1000 | Loss: 0.00004233
Iteration 559/1000 | Loss: 0.00003932
Iteration 560/1000 | Loss: 0.00003928
Iteration 561/1000 | Loss: 0.00004709
Iteration 562/1000 | Loss: 0.00003808
Iteration 563/1000 | Loss: 0.00004444
Iteration 564/1000 | Loss: 0.00003934
Iteration 565/1000 | Loss: 0.00003808
Iteration 566/1000 | Loss: 0.00004379
Iteration 567/1000 | Loss: 0.00003890
Iteration 568/1000 | Loss: 0.00003739
Iteration 569/1000 | Loss: 0.00003787
Iteration 570/1000 | Loss: 0.00004239
Iteration 571/1000 | Loss: 0.00004145
Iteration 572/1000 | Loss: 0.00003951
Iteration 573/1000 | Loss: 0.00003869
Iteration 574/1000 | Loss: 0.00004808
Iteration 575/1000 | Loss: 0.00004482
Iteration 576/1000 | Loss: 0.00003798
Iteration 577/1000 | Loss: 0.00004289
Iteration 578/1000 | Loss: 0.00004064
Iteration 579/1000 | Loss: 0.00003768
Iteration 580/1000 | Loss: 0.00003942
Iteration 581/1000 | Loss: 0.00004102
Iteration 582/1000 | Loss: 0.00004248
Iteration 583/1000 | Loss: 0.00003793
Iteration 584/1000 | Loss: 0.00004230
Iteration 585/1000 | Loss: 0.00004635
Iteration 586/1000 | Loss: 0.00003886
Iteration 587/1000 | Loss: 0.00004208
Iteration 588/1000 | Loss: 0.00004393
Iteration 589/1000 | Loss: 0.00003818
Iteration 590/1000 | Loss: 0.00004099
Iteration 591/1000 | Loss: 0.00003930
Iteration 592/1000 | Loss: 0.00003930
Iteration 593/1000 | Loss: 0.00007241
Iteration 594/1000 | Loss: 0.00003794
Iteration 595/1000 | Loss: 0.00005041
Iteration 596/1000 | Loss: 0.00003986
Iteration 597/1000 | Loss: 0.00004095
Iteration 598/1000 | Loss: 0.00004144
Iteration 599/1000 | Loss: 0.00004071
Iteration 600/1000 | Loss: 0.00006354
Iteration 601/1000 | Loss: 0.00003779
Iteration 602/1000 | Loss: 0.00004615
Iteration 603/1000 | Loss: 0.00004082
Iteration 604/1000 | Loss: 0.00003972
Iteration 605/1000 | Loss: 0.00003969
Iteration 606/1000 | Loss: 0.00003932
Iteration 607/1000 | Loss: 0.00004019
Iteration 608/1000 | Loss: 0.00004132
Iteration 609/1000 | Loss: 0.00003973
Iteration 610/1000 | Loss: 0.00003977
Iteration 611/1000 | Loss: 0.00004178
Iteration 612/1000 | Loss: 0.00003900
Iteration 613/1000 | Loss: 0.00003943
Iteration 614/1000 | Loss: 0.00004527
Iteration 615/1000 | Loss: 0.00003865
Iteration 616/1000 | Loss: 0.00004236
Iteration 617/1000 | Loss: 0.00004995
Iteration 618/1000 | Loss: 0.00003901
Iteration 619/1000 | Loss: 0.00003964
Iteration 620/1000 | Loss: 0.00004117
Iteration 621/1000 | Loss: 0.00003981
Iteration 622/1000 | Loss: 0.00003971
Iteration 623/1000 | Loss: 0.00004113
Iteration 624/1000 | Loss: 0.00004043
Iteration 625/1000 | Loss: 0.00006584
Iteration 626/1000 | Loss: 0.00004160
Iteration 627/1000 | Loss: 0.00004525
Iteration 628/1000 | Loss: 0.00004036
Iteration 629/1000 | Loss: 0.00003877
Iteration 630/1000 | Loss: 0.00003795
Iteration 631/1000 | Loss: 0.00003921
Iteration 632/1000 | Loss: 0.00005464
Iteration 633/1000 | Loss: 0.00004021
Iteration 634/1000 | Loss: 0.00003933
Iteration 635/1000 | Loss: 0.00004472
Iteration 636/1000 | Loss: 0.00003979
Iteration 637/1000 | Loss: 0.00003971
Iteration 638/1000 | Loss: 0.00004219
Iteration 639/1000 | Loss: 0.00022145
Iteration 640/1000 | Loss: 0.00004066
Iteration 641/1000 | Loss: 0.00005786
Iteration 642/1000 | Loss: 0.00003949
Iteration 643/1000 | Loss: 0.00004027
Iteration 644/1000 | Loss: 0.00005508
Iteration 645/1000 | Loss: 0.00007635
Iteration 646/1000 | Loss: 0.00005064
Iteration 647/1000 | Loss: 0.00005151
Iteration 648/1000 | Loss: 0.00004182
Iteration 649/1000 | Loss: 0.00004181
Iteration 650/1000 | Loss: 0.00004136
Iteration 651/1000 | Loss: 0.00004333
Iteration 652/1000 | Loss: 0.00007717
Iteration 653/1000 | Loss: 0.00005021
Iteration 654/1000 | Loss: 0.00005669
Iteration 655/1000 | Loss: 0.00007353
Iteration 656/1000 | Loss: 0.00003830
Iteration 657/1000 | Loss: 0.00004708
Iteration 658/1000 | Loss: 0.00004417
Iteration 659/1000 | Loss: 0.00003859
Iteration 660/1000 | Loss: 0.00005383
Iteration 661/1000 | Loss: 0.00003938
Iteration 662/1000 | Loss: 0.00004197
Iteration 663/1000 | Loss: 0.00004378
Iteration 664/1000 | Loss: 0.00004388
Iteration 665/1000 | Loss: 0.00004539
Iteration 666/1000 | Loss: 0.00004086
Iteration 667/1000 | Loss: 0.00004162
Iteration 668/1000 | Loss: 0.00004042
Iteration 669/1000 | Loss: 0.00009464
Iteration 670/1000 | Loss: 0.00005491
Iteration 671/1000 | Loss: 0.00004072
Iteration 672/1000 | Loss: 0.00004102
Iteration 673/1000 | Loss: 0.00004017
Iteration 674/1000 | Loss: 0.00009102
Iteration 675/1000 | Loss: 0.00004576
Iteration 676/1000 | Loss: 0.00004801
Iteration 677/1000 | Loss: 0.00005204
Iteration 678/1000 | Loss: 0.00004160
Iteration 679/1000 | Loss: 0.00004041
Iteration 680/1000 | Loss: 0.00004690
Iteration 681/1000 | Loss: 0.00004117
Iteration 682/1000 | Loss: 0.00004290
Iteration 683/1000 | Loss: 0.00006246
Iteration 684/1000 | Loss: 0.00004044
Iteration 685/1000 | Loss: 0.00004035
Iteration 686/1000 | Loss: 0.00004770
Iteration 687/1000 | Loss: 0.00007289
Iteration 688/1000 | Loss: 0.00004282
Iteration 689/1000 | Loss: 0.00003855
Iteration 690/1000 | Loss: 0.00004412
Iteration 691/1000 | Loss: 0.00004128
Iteration 692/1000 | Loss: 0.00004399
Iteration 693/1000 | Loss: 0.00004515
Iteration 694/1000 | Loss: 0.00014186
Iteration 695/1000 | Loss: 0.00004217
Iteration 696/1000 | Loss: 0.00005224
Iteration 697/1000 | Loss: 0.00004076
Iteration 698/1000 | Loss: 0.00003850
Iteration 699/1000 | Loss: 0.00003850
Iteration 700/1000 | Loss: 0.00004587
Iteration 701/1000 | Loss: 0.00004174
Iteration 702/1000 | Loss: 0.00005121
Iteration 703/1000 | Loss: 0.00004137
Iteration 704/1000 | Loss: 0.00003772
Iteration 705/1000 | Loss: 0.00003847
Iteration 706/1000 | Loss: 0.00004712
Iteration 707/1000 | Loss: 0.00004200
Iteration 708/1000 | Loss: 0.00003886
Iteration 709/1000 | Loss: 0.00003913
Iteration 710/1000 | Loss: 0.00003921
Iteration 711/1000 | Loss: 0.00004338
Iteration 712/1000 | Loss: 0.00003942
Iteration 713/1000 | Loss: 0.00004088
Iteration 714/1000 | Loss: 0.00003870
Iteration 715/1000 | Loss: 0.00003891
Iteration 716/1000 | Loss: 0.00004123
Iteration 717/1000 | Loss: 0.00003901
Iteration 718/1000 | Loss: 0.00004434
Iteration 719/1000 | Loss: 0.00004172
Iteration 720/1000 | Loss: 0.00003890
Iteration 721/1000 | Loss: 0.00004627
Iteration 722/1000 | Loss: 0.00005147
Iteration 723/1000 | Loss: 0.00003941
Iteration 724/1000 | Loss: 0.00003846
Iteration 725/1000 | Loss: 0.00004261
Iteration 726/1000 | Loss: 0.00004901
Iteration 727/1000 | Loss: 0.00004088
Iteration 728/1000 | Loss: 0.00004334
Iteration 729/1000 | Loss: 0.00004334
Iteration 730/1000 | Loss: 0.00008042
Iteration 731/1000 | Loss: 0.00004718
Iteration 732/1000 | Loss: 0.00006167
Iteration 733/1000 | Loss: 0.00004423
Iteration 734/1000 | Loss: 0.00005583
Iteration 735/1000 | Loss: 0.00004823
Iteration 736/1000 | Loss: 0.00004783
Iteration 737/1000 | Loss: 0.00004733
Iteration 738/1000 | Loss: 0.00003975
Iteration 739/1000 | Loss: 0.00004174
Iteration 740/1000 | Loss: 0.00006428
Iteration 741/1000 | Loss: 0.00006791
Iteration 742/1000 | Loss: 0.00004487
Iteration 743/1000 | Loss: 0.00004762
Iteration 744/1000 | Loss: 0.00004099
Iteration 745/1000 | Loss: 0.00003841
Iteration 746/1000 | Loss: 0.00005691
Iteration 747/1000 | Loss: 0.00005726
Iteration 748/1000 | Loss: 0.00004212
Iteration 749/1000 | Loss: 0.00004429
Iteration 750/1000 | Loss: 0.00005165
Iteration 751/1000 | Loss: 0.00006209
Iteration 752/1000 | Loss: 0.00003976
Iteration 753/1000 | Loss: 0.00004775
Iteration 754/1000 | Loss: 0.00006639
Iteration 755/1000 | Loss: 0.00004313
Iteration 756/1000 | Loss: 0.00004205
Iteration 757/1000 | Loss: 0.00004636
Iteration 758/1000 | Loss: 0.00004306
Iteration 759/1000 | Loss: 0.00003878
Iteration 760/1000 | Loss: 0.00004425
Iteration 761/1000 | Loss: 0.00003916
Iteration 762/1000 | Loss: 0.00003888
Iteration 763/1000 | Loss: 0.00004770
Iteration 764/1000 | Loss: 0.00004203
Iteration 765/1000 | Loss: 0.00005298
Iteration 766/1000 | Loss: 0.00004781
Iteration 767/1000 | Loss: 0.00006615
Iteration 768/1000 | Loss: 0.00004588
Iteration 769/1000 | Loss: 0.00007112
Iteration 770/1000 | Loss: 0.00004130
Iteration 771/1000 | Loss: 0.00005017
Iteration 772/1000 | Loss: 0.00010421
Iteration 773/1000 | Loss: 0.00004006
Iteration 774/1000 | Loss: 0.00003940
Iteration 775/1000 | Loss: 0.00004132
Iteration 776/1000 | Loss: 0.00004055
Iteration 777/1000 | Loss: 0.00003923
Iteration 778/1000 | Loss: 0.00003985
Iteration 779/1000 | Loss: 0.00004145
Iteration 780/1000 | Loss: 0.00005344
Iteration 781/1000 | Loss: 0.00004097
Iteration 782/1000 | Loss: 0.00004034
Iteration 783/1000 | Loss: 0.00005819
Iteration 784/1000 | Loss: 0.00004031
Iteration 785/1000 | Loss: 0.00004052
Iteration 786/1000 | Loss: 0.00004867
Iteration 787/1000 | Loss: 0.00004076
Iteration 788/1000 | Loss: 0.00003825
Iteration 789/1000 | Loss: 0.00004407
Iteration 790/1000 | Loss: 0.00004341
Iteration 791/1000 | Loss: 0.00003907
Iteration 792/1000 | Loss: 0.00004098
Iteration 793/1000 | Loss: 0.00004345
Iteration 794/1000 | Loss: 0.00003871
Iteration 795/1000 | Loss: 0.00004091
Iteration 796/1000 | Loss: 0.00004119
Iteration 797/1000 | Loss: 0.00005536
Iteration 798/1000 | Loss: 0.00004268
Iteration 799/1000 | Loss: 0.00004198
Iteration 800/1000 | Loss: 0.00003854
Iteration 801/1000 | Loss: 0.00003997
Iteration 802/1000 | Loss: 0.00004022
Iteration 803/1000 | Loss: 0.00004137
Iteration 804/1000 | Loss: 0.00004525
Iteration 805/1000 | Loss: 0.00004345
Iteration 806/1000 | Loss: 0.00004090
Iteration 807/1000 | Loss: 0.00013776
Iteration 808/1000 | Loss: 0.00004198
Iteration 809/1000 | Loss: 0.00005491
Iteration 810/1000 | Loss: 0.00004004
Iteration 811/1000 | Loss: 0.00004339
Iteration 812/1000 | Loss: 0.00004059
Iteration 813/1000 | Loss: 0.00004059
Iteration 814/1000 | Loss: 0.00004616
Iteration 815/1000 | Loss: 0.00004119
Iteration 816/1000 | Loss: 0.00003940
Iteration 817/1000 | Loss: 0.00004430
Iteration 818/1000 | Loss: 0.00004112
Iteration 819/1000 | Loss: 0.00004019
Iteration 820/1000 | Loss: 0.00003952
Iteration 821/1000 | Loss: 0.00004386
Iteration 822/1000 | Loss: 0.00004206
Iteration 823/1000 | Loss: 0.00003954
Iteration 824/1000 | Loss: 0.00003917
Iteration 825/1000 | Loss: 0.00004143
Iteration 826/1000 | Loss: 0.00004250
Iteration 827/1000 | Loss: 0.00004055
Iteration 828/1000 | Loss: 0.00004023
Iteration 829/1000 | Loss: 0.00003913
Iteration 830/1000 | Loss: 0.00004595
Iteration 831/1000 | Loss: 0.00004674
Iteration 832/1000 | Loss: 0.00004063
Iteration 833/1000 | Loss: 0.00004572
Iteration 834/1000 | Loss: 0.00004310
Iteration 835/1000 | Loss: 0.00004376
Iteration 836/1000 | Loss: 0.00003867
Iteration 837/1000 | Loss: 0.00003918
Iteration 838/1000 | Loss: 0.00004733
Iteration 839/1000 | Loss: 0.00006034
Iteration 840/1000 | Loss: 0.00006347
Iteration 841/1000 | Loss: 0.00004900
Iteration 842/1000 | Loss: 0.00007049
Iteration 843/1000 | Loss: 0.00005481
Iteration 844/1000 | Loss: 0.00005253
Iteration 845/1000 | Loss: 0.00005020
Iteration 846/1000 | Loss: 0.00005083
Iteration 847/1000 | Loss: 0.00004906
Iteration 848/1000 | Loss: 0.00006059
Iteration 849/1000 | Loss: 0.00019726
Iteration 850/1000 | Loss: 0.00004523
Iteration 851/1000 | Loss: 0.00004738
Iteration 852/1000 | Loss: 0.00005351
Iteration 853/1000 | Loss: 0.00004541
Iteration 854/1000 | Loss: 0.00005753
Iteration 855/1000 | Loss: 0.00004981
Iteration 856/1000 | Loss: 0.00004543
Iteration 857/1000 | Loss: 0.00004744
Iteration 858/1000 | Loss: 0.00004535
Iteration 859/1000 | Loss: 0.00006312
Iteration 860/1000 | Loss: 0.00005499
Iteration 861/1000 | Loss: 0.00008699
Iteration 862/1000 | Loss: 0.00004913
Iteration 863/1000 | Loss: 0.00005282
Iteration 864/1000 | Loss: 0.00004954
Iteration 865/1000 | Loss: 0.00004495
Iteration 866/1000 | Loss: 0.00004996
Iteration 867/1000 | Loss: 0.00004222
Iteration 868/1000 | Loss: 0.00005375
Iteration 869/1000 | Loss: 0.00008223
Iteration 870/1000 | Loss: 0.00004456
Iteration 871/1000 | Loss: 0.00004232
Iteration 872/1000 | Loss: 0.00003835
Iteration 873/1000 | Loss: 0.00003834
Iteration 874/1000 | Loss: 0.00005503
Iteration 875/1000 | Loss: 0.00004478
Iteration 876/1000 | Loss: 0.00023752
Iteration 877/1000 | Loss: 0.00005238
Iteration 878/1000 | Loss: 0.00003744
Iteration 879/1000 | Loss: 0.00003829
Iteration 880/1000 | Loss: 0.00003748
Iteration 881/1000 | Loss: 0.00004994
Iteration 882/1000 | Loss: 0.00004072
Iteration 883/1000 | Loss: 0.00004216
Iteration 884/1000 | Loss: 0.00003852
Iteration 885/1000 | Loss: 0.00004822
Iteration 886/1000 | Loss: 0.00004019
Iteration 887/1000 | Loss: 0.00004286
Iteration 888/1000 | Loss: 0.00006346
Iteration 889/1000 | Loss: 0.00004046
Iteration 890/1000 | Loss: 0.00003850
Iteration 891/1000 | Loss: 0.00003718
Iteration 892/1000 | Loss: 0.00003656
Iteration 893/1000 | Loss: 0.00003688
Iteration 894/1000 | Loss: 0.00003630
Iteration 895/1000 | Loss: 0.00003630
Iteration 896/1000 | Loss: 0.00003630
Iteration 897/1000 | Loss: 0.00003630
Iteration 898/1000 | Loss: 0.00003630
Iteration 899/1000 | Loss: 0.00003630
Iteration 900/1000 | Loss: 0.00003630
Iteration 901/1000 | Loss: 0.00003714
Iteration 902/1000 | Loss: 0.00003676
Iteration 903/1000 | Loss: 0.00003633
Iteration 904/1000 | Loss: 0.00003672
Iteration 905/1000 | Loss: 0.00005161
Iteration 906/1000 | Loss: 0.00004014
Iteration 907/1000 | Loss: 0.00003690
Iteration 908/1000 | Loss: 0.00003733
Iteration 909/1000 | Loss: 0.00005128
Iteration 910/1000 | Loss: 0.00005248
Iteration 911/1000 | Loss: 0.00010037
Iteration 912/1000 | Loss: 0.00006041
Iteration 913/1000 | Loss: 0.00009692
Iteration 914/1000 | Loss: 0.00005729
Iteration 915/1000 | Loss: 0.00004529
Iteration 916/1000 | Loss: 0.00003910
Iteration 917/1000 | Loss: 0.00003885
Iteration 918/1000 | Loss: 0.00003908
Iteration 919/1000 | Loss: 0.00003617
Iteration 920/1000 | Loss: 0.00003617
Iteration 921/1000 | Loss: 0.00003617
Iteration 922/1000 | Loss: 0.00003616
Iteration 923/1000 | Loss: 0.00003699
Iteration 924/1000 | Loss: 0.00003614
Iteration 925/1000 | Loss: 0.00003614
Iteration 926/1000 | Loss: 0.00003614
Iteration 927/1000 | Loss: 0.00003614
Iteration 928/1000 | Loss: 0.00003614
Iteration 929/1000 | Loss: 0.00003614
Iteration 930/1000 | Loss: 0.00003614
Iteration 931/1000 | Loss: 0.00003613
Iteration 932/1000 | Loss: 0.00003613
Iteration 933/1000 | Loss: 0.00003613
Iteration 934/1000 | Loss: 0.00003613
Iteration 935/1000 | Loss: 0.00003613
Iteration 936/1000 | Loss: 0.00003844
Iteration 937/1000 | Loss: 0.00003918
Iteration 938/1000 | Loss: 0.00004203
Iteration 939/1000 | Loss: 0.00009106
Iteration 940/1000 | Loss: 0.00006118
Iteration 941/1000 | Loss: 0.00005804
Iteration 942/1000 | Loss: 0.00004725
Iteration 943/1000 | Loss: 0.00003778
Iteration 944/1000 | Loss: 0.00003781
Iteration 945/1000 | Loss: 0.00004680
Iteration 946/1000 | Loss: 0.00004394
Iteration 947/1000 | Loss: 0.00003672
Iteration 948/1000 | Loss: 0.00003611
Iteration 949/1000 | Loss: 0.00003610
Iteration 950/1000 | Loss: 0.00003610
Iteration 951/1000 | Loss: 0.00003610
Iteration 952/1000 | Loss: 0.00003610
Iteration 953/1000 | Loss: 0.00003610
Iteration 954/1000 | Loss: 0.00003610
Iteration 955/1000 | Loss: 0.00003610
Iteration 956/1000 | Loss: 0.00003610
Iteration 957/1000 | Loss: 0.00003610
Iteration 958/1000 | Loss: 0.00003610
Iteration 959/1000 | Loss: 0.00003610
Iteration 960/1000 | Loss: 0.00003610
Iteration 961/1000 | Loss: 0.00003610
Iteration 962/1000 | Loss: 0.00003610
Iteration 963/1000 | Loss: 0.00003610
Iteration 964/1000 | Loss: 0.00003609
Iteration 965/1000 | Loss: 0.00003609
Iteration 966/1000 | Loss: 0.00003609
Iteration 967/1000 | Loss: 0.00003609
Iteration 968/1000 | Loss: 0.00003609
Iteration 969/1000 | Loss: 0.00003609
Iteration 970/1000 | Loss: 0.00003609
Iteration 971/1000 | Loss: 0.00003609
Iteration 972/1000 | Loss: 0.00003609
Iteration 973/1000 | Loss: 0.00003609
Iteration 974/1000 | Loss: 0.00003609
Iteration 975/1000 | Loss: 0.00003609
Iteration 976/1000 | Loss: 0.00003608
Iteration 977/1000 | Loss: 0.00003608
Iteration 978/1000 | Loss: 0.00004026
Iteration 979/1000 | Loss: 0.00003820
Iteration 980/1000 | Loss: 0.00004136
Iteration 981/1000 | Loss: 0.00004046
Iteration 982/1000 | Loss: 0.00004601
Iteration 983/1000 | Loss: 0.00003898
Iteration 984/1000 | Loss: 0.00003695
Iteration 985/1000 | Loss: 0.00003595
Iteration 986/1000 | Loss: 0.00003595
Iteration 987/1000 | Loss: 0.00003595
Iteration 988/1000 | Loss: 0.00003595
Iteration 989/1000 | Loss: 0.00003595
Iteration 990/1000 | Loss: 0.00003595
Iteration 991/1000 | Loss: 0.00003595
Iteration 992/1000 | Loss: 0.00003595
Iteration 993/1000 | Loss: 0.00003595
Iteration 994/1000 | Loss: 0.00003595
Iteration 995/1000 | Loss: 0.00003595
Iteration 996/1000 | Loss: 0.00003595
Iteration 997/1000 | Loss: 0.00003595
Iteration 998/1000 | Loss: 0.00003595
Iteration 999/1000 | Loss: 0.00003595
Iteration 1000/1000 | Loss: 0.00003594

Optimization complete. Final v2v error: 4.64274263381958 mm

Highest mean error: 10.730022430419922 mm for frame 99

Lowest mean error: 3.8649682998657227 mm for frame 229

Saving results

Total time: 1378.9019541740417
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944860
Iteration 2/25 | Loss: 0.00335134
Iteration 3/25 | Loss: 0.00231320
Iteration 4/25 | Loss: 0.00201124
Iteration 5/25 | Loss: 0.00199377
Iteration 6/25 | Loss: 0.00199742
Iteration 7/25 | Loss: 0.00194334
Iteration 8/25 | Loss: 0.00190001
Iteration 9/25 | Loss: 0.00188590
Iteration 10/25 | Loss: 0.00186487
Iteration 11/25 | Loss: 0.00186048
Iteration 12/25 | Loss: 0.00185910
Iteration 13/25 | Loss: 0.00186010
Iteration 14/25 | Loss: 0.00185631
Iteration 15/25 | Loss: 0.00185644
Iteration 16/25 | Loss: 0.00185083
Iteration 17/25 | Loss: 0.00184836
Iteration 18/25 | Loss: 0.00185164
Iteration 19/25 | Loss: 0.00184457
Iteration 20/25 | Loss: 0.00184422
Iteration 21/25 | Loss: 0.00184641
Iteration 22/25 | Loss: 0.00184410
Iteration 23/25 | Loss: 0.00184350
Iteration 24/25 | Loss: 0.00184330
Iteration 25/25 | Loss: 0.00184327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.06761312
Iteration 2/25 | Loss: 0.00436074
Iteration 3/25 | Loss: 0.00353149
Iteration 4/25 | Loss: 0.00353149
Iteration 5/25 | Loss: 0.00353149
Iteration 6/25 | Loss: 0.00353149
Iteration 7/25 | Loss: 0.00353149
Iteration 8/25 | Loss: 0.00353149
Iteration 9/25 | Loss: 0.00353149
Iteration 10/25 | Loss: 0.00353149
Iteration 11/25 | Loss: 0.00353149
Iteration 12/25 | Loss: 0.00353149
Iteration 13/25 | Loss: 0.00353149
Iteration 14/25 | Loss: 0.00353149
Iteration 15/25 | Loss: 0.00353149
Iteration 16/25 | Loss: 0.00353149
Iteration 17/25 | Loss: 0.00353149
Iteration 18/25 | Loss: 0.00353149
Iteration 19/25 | Loss: 0.00353149
Iteration 20/25 | Loss: 0.00353149
Iteration 21/25 | Loss: 0.00353149
Iteration 22/25 | Loss: 0.00353149
Iteration 23/25 | Loss: 0.00353149
Iteration 24/25 | Loss: 0.00353149
Iteration 25/25 | Loss: 0.00353149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00353149
Iteration 2/1000 | Loss: 0.00066260
Iteration 3/1000 | Loss: 0.00074688
Iteration 4/1000 | Loss: 0.00107455
Iteration 5/1000 | Loss: 0.00105358
Iteration 6/1000 | Loss: 0.00315210
Iteration 7/1000 | Loss: 0.01046123
Iteration 8/1000 | Loss: 0.00413387
Iteration 9/1000 | Loss: 0.00142304
Iteration 10/1000 | Loss: 0.00038820
Iteration 11/1000 | Loss: 0.00840526
Iteration 12/1000 | Loss: 0.00076764
Iteration 13/1000 | Loss: 0.00215375
Iteration 14/1000 | Loss: 0.00089603
Iteration 15/1000 | Loss: 0.00132829
Iteration 16/1000 | Loss: 0.00164624
Iteration 17/1000 | Loss: 0.00033884
Iteration 18/1000 | Loss: 0.00115758
Iteration 19/1000 | Loss: 0.00089130
Iteration 20/1000 | Loss: 0.00025794
Iteration 21/1000 | Loss: 0.00177483
Iteration 22/1000 | Loss: 0.00035017
Iteration 23/1000 | Loss: 0.00085484
Iteration 24/1000 | Loss: 0.00034023
Iteration 25/1000 | Loss: 0.00211373
Iteration 26/1000 | Loss: 0.00277336
Iteration 27/1000 | Loss: 0.00401983
Iteration 28/1000 | Loss: 0.00172171
Iteration 29/1000 | Loss: 0.00128918
Iteration 30/1000 | Loss: 0.00152026
Iteration 31/1000 | Loss: 0.00071190
Iteration 32/1000 | Loss: 0.00055726
Iteration 33/1000 | Loss: 0.00054257
Iteration 34/1000 | Loss: 0.00121256
Iteration 35/1000 | Loss: 0.00026614
Iteration 36/1000 | Loss: 0.00067162
Iteration 37/1000 | Loss: 0.00049020
Iteration 38/1000 | Loss: 0.00043313
Iteration 39/1000 | Loss: 0.00119790
Iteration 40/1000 | Loss: 0.00019895
Iteration 41/1000 | Loss: 0.00057447
Iteration 42/1000 | Loss: 0.00018278
Iteration 43/1000 | Loss: 0.00017708
Iteration 44/1000 | Loss: 0.00045877
Iteration 45/1000 | Loss: 0.00050308
Iteration 46/1000 | Loss: 0.00355848
Iteration 47/1000 | Loss: 0.00171154
Iteration 48/1000 | Loss: 0.00129249
Iteration 49/1000 | Loss: 0.00074796
Iteration 50/1000 | Loss: 0.00184654
Iteration 51/1000 | Loss: 0.00539548
Iteration 52/1000 | Loss: 0.00453675
Iteration 53/1000 | Loss: 0.00181406
Iteration 54/1000 | Loss: 0.00460402
Iteration 55/1000 | Loss: 0.00326011
Iteration 56/1000 | Loss: 0.00186228
Iteration 57/1000 | Loss: 0.00646742
Iteration 58/1000 | Loss: 0.00356594
Iteration 59/1000 | Loss: 0.00665946
Iteration 60/1000 | Loss: 0.00350484
Iteration 61/1000 | Loss: 0.00341641
Iteration 62/1000 | Loss: 0.00216012
Iteration 63/1000 | Loss: 0.00325077
Iteration 64/1000 | Loss: 0.00585367
Iteration 65/1000 | Loss: 0.00222552
Iteration 66/1000 | Loss: 0.00285264
Iteration 67/1000 | Loss: 0.00145048
Iteration 68/1000 | Loss: 0.00164808
Iteration 69/1000 | Loss: 0.00107936
Iteration 70/1000 | Loss: 0.00100206
Iteration 71/1000 | Loss: 0.00238882
Iteration 72/1000 | Loss: 0.00096644
Iteration 73/1000 | Loss: 0.00044053
Iteration 74/1000 | Loss: 0.00180979
Iteration 75/1000 | Loss: 0.00241092
Iteration 76/1000 | Loss: 0.00034923
Iteration 77/1000 | Loss: 0.00056795
Iteration 78/1000 | Loss: 0.00152561
Iteration 79/1000 | Loss: 0.00659263
Iteration 80/1000 | Loss: 0.00221673
Iteration 81/1000 | Loss: 0.00296215
Iteration 82/1000 | Loss: 0.00214378
Iteration 83/1000 | Loss: 0.00061806
Iteration 84/1000 | Loss: 0.00172714
Iteration 85/1000 | Loss: 0.00213536
Iteration 86/1000 | Loss: 0.00190116
Iteration 87/1000 | Loss: 0.00089349
Iteration 88/1000 | Loss: 0.00071123
Iteration 89/1000 | Loss: 0.00111750
Iteration 90/1000 | Loss: 0.00039709
Iteration 91/1000 | Loss: 0.00094880
Iteration 92/1000 | Loss: 0.00058018
Iteration 93/1000 | Loss: 0.00086792
Iteration 94/1000 | Loss: 0.00150529
Iteration 95/1000 | Loss: 0.00105872
Iteration 96/1000 | Loss: 0.00133157
Iteration 97/1000 | Loss: 0.00094532
Iteration 98/1000 | Loss: 0.00102136
Iteration 99/1000 | Loss: 0.00054395
Iteration 100/1000 | Loss: 0.00078693
Iteration 101/1000 | Loss: 0.00102380
Iteration 102/1000 | Loss: 0.00137055
Iteration 103/1000 | Loss: 0.00120144
Iteration 104/1000 | Loss: 0.00073154
Iteration 105/1000 | Loss: 0.00132361
Iteration 106/1000 | Loss: 0.00207261
Iteration 107/1000 | Loss: 0.00172003
Iteration 108/1000 | Loss: 0.00164577
Iteration 109/1000 | Loss: 0.00063136
Iteration 110/1000 | Loss: 0.00134730
Iteration 111/1000 | Loss: 0.00119725
Iteration 112/1000 | Loss: 0.00091031
Iteration 113/1000 | Loss: 0.00117592
Iteration 114/1000 | Loss: 0.00089068
Iteration 115/1000 | Loss: 0.00071033
Iteration 116/1000 | Loss: 0.00089063
Iteration 117/1000 | Loss: 0.00039990
Iteration 118/1000 | Loss: 0.00045911
Iteration 119/1000 | Loss: 0.00128437
Iteration 120/1000 | Loss: 0.00090745
Iteration 121/1000 | Loss: 0.00153541
Iteration 122/1000 | Loss: 0.00123641
Iteration 123/1000 | Loss: 0.00117978
Iteration 124/1000 | Loss: 0.00127635
Iteration 125/1000 | Loss: 0.00080037
Iteration 126/1000 | Loss: 0.00066789
Iteration 127/1000 | Loss: 0.00121306
Iteration 128/1000 | Loss: 0.00089994
Iteration 129/1000 | Loss: 0.00084997
Iteration 130/1000 | Loss: 0.00066652
Iteration 131/1000 | Loss: 0.00037265
Iteration 132/1000 | Loss: 0.00094559
Iteration 133/1000 | Loss: 0.00121987
Iteration 134/1000 | Loss: 0.00054098
Iteration 135/1000 | Loss: 0.00034166
Iteration 136/1000 | Loss: 0.00131902
Iteration 137/1000 | Loss: 0.00056429
Iteration 138/1000 | Loss: 0.00064138
Iteration 139/1000 | Loss: 0.00060288
Iteration 140/1000 | Loss: 0.00106488
Iteration 141/1000 | Loss: 0.00132806
Iteration 142/1000 | Loss: 0.00075593
Iteration 143/1000 | Loss: 0.00123647
Iteration 144/1000 | Loss: 0.00023694
Iteration 145/1000 | Loss: 0.00045475
Iteration 146/1000 | Loss: 0.00034969
Iteration 147/1000 | Loss: 0.00042393
Iteration 148/1000 | Loss: 0.00092021
Iteration 149/1000 | Loss: 0.00088351
Iteration 150/1000 | Loss: 0.00085512
Iteration 151/1000 | Loss: 0.00114999
Iteration 152/1000 | Loss: 0.00053447
Iteration 153/1000 | Loss: 0.00086475
Iteration 154/1000 | Loss: 0.00041321
Iteration 155/1000 | Loss: 0.00018077
Iteration 156/1000 | Loss: 0.00015159
Iteration 157/1000 | Loss: 0.00013066
Iteration 158/1000 | Loss: 0.00034335
Iteration 159/1000 | Loss: 0.00028243
Iteration 160/1000 | Loss: 0.00008082
Iteration 161/1000 | Loss: 0.00065072
Iteration 162/1000 | Loss: 0.00040078
Iteration 163/1000 | Loss: 0.00100702
Iteration 164/1000 | Loss: 0.00039972
Iteration 165/1000 | Loss: 0.00059035
Iteration 166/1000 | Loss: 0.00010752
Iteration 167/1000 | Loss: 0.00007577
Iteration 168/1000 | Loss: 0.00007035
Iteration 169/1000 | Loss: 0.00006541
Iteration 170/1000 | Loss: 0.00007934
Iteration 171/1000 | Loss: 0.00006251
Iteration 172/1000 | Loss: 0.00007517
Iteration 173/1000 | Loss: 0.00005934
Iteration 174/1000 | Loss: 0.00053797
Iteration 175/1000 | Loss: 0.00024731
Iteration 176/1000 | Loss: 0.00006045
Iteration 177/1000 | Loss: 0.00041461
Iteration 178/1000 | Loss: 0.00015044
Iteration 179/1000 | Loss: 0.00006187
Iteration 180/1000 | Loss: 0.00006141
Iteration 181/1000 | Loss: 0.00005786
Iteration 182/1000 | Loss: 0.00021439
Iteration 183/1000 | Loss: 0.00006922
Iteration 184/1000 | Loss: 0.00036670
Iteration 185/1000 | Loss: 0.00073950
Iteration 186/1000 | Loss: 0.00023191
Iteration 187/1000 | Loss: 0.00017820
Iteration 188/1000 | Loss: 0.00061932
Iteration 189/1000 | Loss: 0.00047100
Iteration 190/1000 | Loss: 0.00022443
Iteration 191/1000 | Loss: 0.00046948
Iteration 192/1000 | Loss: 0.00051672
Iteration 193/1000 | Loss: 0.00016474
Iteration 194/1000 | Loss: 0.00038231
Iteration 195/1000 | Loss: 0.00048881
Iteration 196/1000 | Loss: 0.00008116
Iteration 197/1000 | Loss: 0.00047418
Iteration 198/1000 | Loss: 0.00006184
Iteration 199/1000 | Loss: 0.00005915
Iteration 200/1000 | Loss: 0.00005779
Iteration 201/1000 | Loss: 0.00008290
Iteration 202/1000 | Loss: 0.00005605
Iteration 203/1000 | Loss: 0.00007760
Iteration 204/1000 | Loss: 0.00005518
Iteration 205/1000 | Loss: 0.00009649
Iteration 206/1000 | Loss: 0.00005471
Iteration 207/1000 | Loss: 0.00051075
Iteration 208/1000 | Loss: 0.00019786
Iteration 209/1000 | Loss: 0.00005490
Iteration 210/1000 | Loss: 0.00005434
Iteration 211/1000 | Loss: 0.00047425
Iteration 212/1000 | Loss: 0.00016005
Iteration 213/1000 | Loss: 0.00005532
Iteration 214/1000 | Loss: 0.00005427
Iteration 215/1000 | Loss: 0.00046218
Iteration 216/1000 | Loss: 0.00007561
Iteration 217/1000 | Loss: 0.00046071
Iteration 218/1000 | Loss: 0.00039345
Iteration 219/1000 | Loss: 0.00022229
Iteration 220/1000 | Loss: 0.00005628
Iteration 221/1000 | Loss: 0.00006690
Iteration 222/1000 | Loss: 0.00039041
Iteration 223/1000 | Loss: 0.00020293
Iteration 224/1000 | Loss: 0.00030194
Iteration 225/1000 | Loss: 0.00019550
Iteration 226/1000 | Loss: 0.00006879
Iteration 227/1000 | Loss: 0.00005486
Iteration 228/1000 | Loss: 0.00005428
Iteration 229/1000 | Loss: 0.00005410
Iteration 230/1000 | Loss: 0.00005407
Iteration 231/1000 | Loss: 0.00005405
Iteration 232/1000 | Loss: 0.00005404
Iteration 233/1000 | Loss: 0.00005404
Iteration 234/1000 | Loss: 0.00005404
Iteration 235/1000 | Loss: 0.00005403
Iteration 236/1000 | Loss: 0.00038558
Iteration 237/1000 | Loss: 0.00035409
Iteration 238/1000 | Loss: 0.00013766
Iteration 239/1000 | Loss: 0.00043635
Iteration 240/1000 | Loss: 0.00017064
Iteration 241/1000 | Loss: 0.00012539
Iteration 242/1000 | Loss: 0.00036439
Iteration 243/1000 | Loss: 0.00037229
Iteration 244/1000 | Loss: 0.00045186
Iteration 245/1000 | Loss: 0.00008523
Iteration 246/1000 | Loss: 0.00008512
Iteration 247/1000 | Loss: 0.00005893
Iteration 248/1000 | Loss: 0.00005705
Iteration 249/1000 | Loss: 0.00014843
Iteration 250/1000 | Loss: 0.00005565
Iteration 251/1000 | Loss: 0.00005473
Iteration 252/1000 | Loss: 0.00012979
Iteration 253/1000 | Loss: 0.00005429
Iteration 254/1000 | Loss: 0.00005413
Iteration 255/1000 | Loss: 0.00005413
Iteration 256/1000 | Loss: 0.00005412
Iteration 257/1000 | Loss: 0.00005412
Iteration 258/1000 | Loss: 0.00005412
Iteration 259/1000 | Loss: 0.00005412
Iteration 260/1000 | Loss: 0.00005412
Iteration 261/1000 | Loss: 0.00005412
Iteration 262/1000 | Loss: 0.00005412
Iteration 263/1000 | Loss: 0.00005412
Iteration 264/1000 | Loss: 0.00005412
Iteration 265/1000 | Loss: 0.00005412
Iteration 266/1000 | Loss: 0.00005412
Iteration 267/1000 | Loss: 0.00005412
Iteration 268/1000 | Loss: 0.00005411
Iteration 269/1000 | Loss: 0.00005411
Iteration 270/1000 | Loss: 0.00005411
Iteration 271/1000 | Loss: 0.00005411
Iteration 272/1000 | Loss: 0.00005411
Iteration 273/1000 | Loss: 0.00005411
Iteration 274/1000 | Loss: 0.00005411
Iteration 275/1000 | Loss: 0.00005411
Iteration 276/1000 | Loss: 0.00005411
Iteration 277/1000 | Loss: 0.00005410
Iteration 278/1000 | Loss: 0.00005410
Iteration 279/1000 | Loss: 0.00005409
Iteration 280/1000 | Loss: 0.00005408
Iteration 281/1000 | Loss: 0.00005408
Iteration 282/1000 | Loss: 0.00005408
Iteration 283/1000 | Loss: 0.00005408
Iteration 284/1000 | Loss: 0.00005407
Iteration 285/1000 | Loss: 0.00005407
Iteration 286/1000 | Loss: 0.00005407
Iteration 287/1000 | Loss: 0.00005407
Iteration 288/1000 | Loss: 0.00005407
Iteration 289/1000 | Loss: 0.00005406
Iteration 290/1000 | Loss: 0.00005406
Iteration 291/1000 | Loss: 0.00013378
Iteration 292/1000 | Loss: 0.00013378
Iteration 293/1000 | Loss: 0.00015719
Iteration 294/1000 | Loss: 0.00005423
Iteration 295/1000 | Loss: 0.00005406
Iteration 296/1000 | Loss: 0.00027146
Iteration 297/1000 | Loss: 0.00007226
Iteration 298/1000 | Loss: 0.00026916
Iteration 299/1000 | Loss: 0.00026846
Iteration 300/1000 | Loss: 0.00023272
Iteration 301/1000 | Loss: 0.00007195
Iteration 302/1000 | Loss: 0.00008073
Iteration 303/1000 | Loss: 0.00029103
Iteration 304/1000 | Loss: 0.00006880
Iteration 305/1000 | Loss: 0.00019484
Iteration 306/1000 | Loss: 0.00005520
Iteration 307/1000 | Loss: 0.00044607
Iteration 308/1000 | Loss: 0.00006848
Iteration 309/1000 | Loss: 0.00053991
Iteration 310/1000 | Loss: 0.00005697
Iteration 311/1000 | Loss: 0.00005446
Iteration 312/1000 | Loss: 0.00007660
Iteration 313/1000 | Loss: 0.00005419
Iteration 314/1000 | Loss: 0.00005410
Iteration 315/1000 | Loss: 0.00005407
Iteration 316/1000 | Loss: 0.00005407
Iteration 317/1000 | Loss: 0.00005407
Iteration 318/1000 | Loss: 0.00005406
Iteration 319/1000 | Loss: 0.00005406
Iteration 320/1000 | Loss: 0.00005406
Iteration 321/1000 | Loss: 0.00005405
Iteration 322/1000 | Loss: 0.00005405
Iteration 323/1000 | Loss: 0.00005405
Iteration 324/1000 | Loss: 0.00005404
Iteration 325/1000 | Loss: 0.00005404
Iteration 326/1000 | Loss: 0.00005404
Iteration 327/1000 | Loss: 0.00005403
Iteration 328/1000 | Loss: 0.00005403
Iteration 329/1000 | Loss: 0.00005403
Iteration 330/1000 | Loss: 0.00005403
Iteration 331/1000 | Loss: 0.00005403
Iteration 332/1000 | Loss: 0.00005403
Iteration 333/1000 | Loss: 0.00005403
Iteration 334/1000 | Loss: 0.00005402
Iteration 335/1000 | Loss: 0.00005402
Iteration 336/1000 | Loss: 0.00005402
Iteration 337/1000 | Loss: 0.00005401
Iteration 338/1000 | Loss: 0.00005401
Iteration 339/1000 | Loss: 0.00005401
Iteration 340/1000 | Loss: 0.00005400
Iteration 341/1000 | Loss: 0.00005400
Iteration 342/1000 | Loss: 0.00005400
Iteration 343/1000 | Loss: 0.00005400
Iteration 344/1000 | Loss: 0.00005399
Iteration 345/1000 | Loss: 0.00005399
Iteration 346/1000 | Loss: 0.00005399
Iteration 347/1000 | Loss: 0.00005399
Iteration 348/1000 | Loss: 0.00005399
Iteration 349/1000 | Loss: 0.00005398
Iteration 350/1000 | Loss: 0.00005398
Iteration 351/1000 | Loss: 0.00005398
Iteration 352/1000 | Loss: 0.00005397
Iteration 353/1000 | Loss: 0.00005397
Iteration 354/1000 | Loss: 0.00005397
Iteration 355/1000 | Loss: 0.00005395
Iteration 356/1000 | Loss: 0.00005394
Iteration 357/1000 | Loss: 0.00005394
Iteration 358/1000 | Loss: 0.00005393
Iteration 359/1000 | Loss: 0.00005393
Iteration 360/1000 | Loss: 0.00005392
Iteration 361/1000 | Loss: 0.00005392
Iteration 362/1000 | Loss: 0.00005391
Iteration 363/1000 | Loss: 0.00005391
Iteration 364/1000 | Loss: 0.00005390
Iteration 365/1000 | Loss: 0.00005390
Iteration 366/1000 | Loss: 0.00005390
Iteration 367/1000 | Loss: 0.00005389
Iteration 368/1000 | Loss: 0.00005388
Iteration 369/1000 | Loss: 0.00005388
Iteration 370/1000 | Loss: 0.00005387
Iteration 371/1000 | Loss: 0.00005384
Iteration 372/1000 | Loss: 0.00005384
Iteration 373/1000 | Loss: 0.00005383
Iteration 374/1000 | Loss: 0.00005383
Iteration 375/1000 | Loss: 0.00005383
Iteration 376/1000 | Loss: 0.00005382
Iteration 377/1000 | Loss: 0.00005379
Iteration 378/1000 | Loss: 0.00005378
Iteration 379/1000 | Loss: 0.00005378
Iteration 380/1000 | Loss: 0.00005377
Iteration 381/1000 | Loss: 0.00005377
Iteration 382/1000 | Loss: 0.00005376
Iteration 383/1000 | Loss: 0.00005376
Iteration 384/1000 | Loss: 0.00005375
Iteration 385/1000 | Loss: 0.00005375
Iteration 386/1000 | Loss: 0.00005374
Iteration 387/1000 | Loss: 0.00005374
Iteration 388/1000 | Loss: 0.00050604
Iteration 389/1000 | Loss: 0.00061559
Iteration 390/1000 | Loss: 0.00036349
Iteration 391/1000 | Loss: 0.00037124
Iteration 392/1000 | Loss: 0.00025605
Iteration 393/1000 | Loss: 0.00040660
Iteration 394/1000 | Loss: 0.00036662
Iteration 395/1000 | Loss: 0.00045125
Iteration 396/1000 | Loss: 0.00024244
Iteration 397/1000 | Loss: 0.00037097
Iteration 398/1000 | Loss: 0.00040642
Iteration 399/1000 | Loss: 0.00033384
Iteration 400/1000 | Loss: 0.00040462
Iteration 401/1000 | Loss: 0.00036257
Iteration 402/1000 | Loss: 0.00018094
Iteration 403/1000 | Loss: 0.00020736
Iteration 404/1000 | Loss: 0.00016178
Iteration 405/1000 | Loss: 0.00039170
Iteration 406/1000 | Loss: 0.00018271
Iteration 407/1000 | Loss: 0.00009523
Iteration 408/1000 | Loss: 0.00005477
Iteration 409/1000 | Loss: 0.00021255
Iteration 410/1000 | Loss: 0.00027201
Iteration 411/1000 | Loss: 0.00014639
Iteration 412/1000 | Loss: 0.00035617
Iteration 413/1000 | Loss: 0.00035997
Iteration 414/1000 | Loss: 0.00008474
Iteration 415/1000 | Loss: 0.00006756
Iteration 416/1000 | Loss: 0.00013289
Iteration 417/1000 | Loss: 0.00006342
Iteration 418/1000 | Loss: 0.00005829
Iteration 419/1000 | Loss: 0.00011756
Iteration 420/1000 | Loss: 0.00019728
Iteration 421/1000 | Loss: 0.00010820
Iteration 422/1000 | Loss: 0.00005342
Iteration 423/1000 | Loss: 0.00005221
Iteration 424/1000 | Loss: 0.00005187
Iteration 425/1000 | Loss: 0.00005161
Iteration 426/1000 | Loss: 0.00007422
Iteration 427/1000 | Loss: 0.00005131
Iteration 428/1000 | Loss: 0.00006312
Iteration 429/1000 | Loss: 0.00005188
Iteration 430/1000 | Loss: 0.00005123
Iteration 431/1000 | Loss: 0.00006842
Iteration 432/1000 | Loss: 0.00005455
Iteration 433/1000 | Loss: 0.00005774
Iteration 434/1000 | Loss: 0.00005518
Iteration 435/1000 | Loss: 0.00005614
Iteration 436/1000 | Loss: 0.00005613
Iteration 437/1000 | Loss: 0.00006487
Iteration 438/1000 | Loss: 0.00005117
Iteration 439/1000 | Loss: 0.00005117
Iteration 440/1000 | Loss: 0.00005117
Iteration 441/1000 | Loss: 0.00005117
Iteration 442/1000 | Loss: 0.00005117
Iteration 443/1000 | Loss: 0.00005117
Iteration 444/1000 | Loss: 0.00005117
Iteration 445/1000 | Loss: 0.00005117
Iteration 446/1000 | Loss: 0.00005117
Iteration 447/1000 | Loss: 0.00005117
Iteration 448/1000 | Loss: 0.00005117
Iteration 449/1000 | Loss: 0.00005117
Iteration 450/1000 | Loss: 0.00005116
Iteration 451/1000 | Loss: 0.00005116
Iteration 452/1000 | Loss: 0.00005116
Iteration 453/1000 | Loss: 0.00005116
Iteration 454/1000 | Loss: 0.00005116
Iteration 455/1000 | Loss: 0.00005116
Iteration 456/1000 | Loss: 0.00005116
Iteration 457/1000 | Loss: 0.00005115
Iteration 458/1000 | Loss: 0.00005115
Iteration 459/1000 | Loss: 0.00005115
Iteration 460/1000 | Loss: 0.00005115
Iteration 461/1000 | Loss: 0.00005115
Iteration 462/1000 | Loss: 0.00005115
Iteration 463/1000 | Loss: 0.00005115
Iteration 464/1000 | Loss: 0.00005115
Iteration 465/1000 | Loss: 0.00005113
Iteration 466/1000 | Loss: 0.00005113
Iteration 467/1000 | Loss: 0.00005113
Iteration 468/1000 | Loss: 0.00005113
Iteration 469/1000 | Loss: 0.00005113
Iteration 470/1000 | Loss: 0.00005112
Iteration 471/1000 | Loss: 0.00005112
Iteration 472/1000 | Loss: 0.00005112
Iteration 473/1000 | Loss: 0.00005112
Iteration 474/1000 | Loss: 0.00005112
Iteration 475/1000 | Loss: 0.00005112
Iteration 476/1000 | Loss: 0.00005112
Iteration 477/1000 | Loss: 0.00005112
Iteration 478/1000 | Loss: 0.00005112
Iteration 479/1000 | Loss: 0.00005111
Iteration 480/1000 | Loss: 0.00005111
Iteration 481/1000 | Loss: 0.00005111
Iteration 482/1000 | Loss: 0.00005111
Iteration 483/1000 | Loss: 0.00005111
Iteration 484/1000 | Loss: 0.00005111
Iteration 485/1000 | Loss: 0.00005111
Iteration 486/1000 | Loss: 0.00005111
Iteration 487/1000 | Loss: 0.00005111
Iteration 488/1000 | Loss: 0.00005111
Iteration 489/1000 | Loss: 0.00005110
Iteration 490/1000 | Loss: 0.00050179
Iteration 491/1000 | Loss: 0.00017212
Iteration 492/1000 | Loss: 0.00008003
Iteration 493/1000 | Loss: 0.00008725
Iteration 494/1000 | Loss: 0.00007263
Iteration 495/1000 | Loss: 0.00048498
Iteration 496/1000 | Loss: 0.00073562
Iteration 497/1000 | Loss: 0.00105550
Iteration 498/1000 | Loss: 0.00043235
Iteration 499/1000 | Loss: 0.00032856
Iteration 500/1000 | Loss: 0.00008784
Iteration 501/1000 | Loss: 0.00037928
Iteration 502/1000 | Loss: 0.00065655
Iteration 503/1000 | Loss: 0.00044158
Iteration 504/1000 | Loss: 0.00018937
Iteration 505/1000 | Loss: 0.00006682
Iteration 506/1000 | Loss: 0.00012618
Iteration 507/1000 | Loss: 0.00006378
Iteration 508/1000 | Loss: 0.00009667
Iteration 509/1000 | Loss: 0.00005894
Iteration 510/1000 | Loss: 0.00006716
Iteration 511/1000 | Loss: 0.00007124
Iteration 512/1000 | Loss: 0.00005478
Iteration 513/1000 | Loss: 0.00005419
Iteration 514/1000 | Loss: 0.00005078
Iteration 515/1000 | Loss: 0.00005034
Iteration 516/1000 | Loss: 0.00004979
Iteration 517/1000 | Loss: 0.00005564
Iteration 518/1000 | Loss: 0.00005215
Iteration 519/1000 | Loss: 0.00004917
Iteration 520/1000 | Loss: 0.00004905
Iteration 521/1000 | Loss: 0.00005702
Iteration 522/1000 | Loss: 0.00005044
Iteration 523/1000 | Loss: 0.00004983
Iteration 524/1000 | Loss: 0.00005213
Iteration 525/1000 | Loss: 0.00005281
Iteration 526/1000 | Loss: 0.00004894
Iteration 527/1000 | Loss: 0.00005698
Iteration 528/1000 | Loss: 0.00004887
Iteration 529/1000 | Loss: 0.00004882
Iteration 530/1000 | Loss: 0.00004882
Iteration 531/1000 | Loss: 0.00004881
Iteration 532/1000 | Loss: 0.00004876
Iteration 533/1000 | Loss: 0.00004876
Iteration 534/1000 | Loss: 0.00004875
Iteration 535/1000 | Loss: 0.00004875
Iteration 536/1000 | Loss: 0.00004875
Iteration 537/1000 | Loss: 0.00004874
Iteration 538/1000 | Loss: 0.00004874
Iteration 539/1000 | Loss: 0.00004874
Iteration 540/1000 | Loss: 0.00004873
Iteration 541/1000 | Loss: 0.00004873
Iteration 542/1000 | Loss: 0.00004873
Iteration 543/1000 | Loss: 0.00004872
Iteration 544/1000 | Loss: 0.00004872
Iteration 545/1000 | Loss: 0.00004872
Iteration 546/1000 | Loss: 0.00004871
Iteration 547/1000 | Loss: 0.00004869
Iteration 548/1000 | Loss: 0.00004869
Iteration 549/1000 | Loss: 0.00004868
Iteration 550/1000 | Loss: 0.00004868
Iteration 551/1000 | Loss: 0.00004867
Iteration 552/1000 | Loss: 0.00004867
Iteration 553/1000 | Loss: 0.00004866
Iteration 554/1000 | Loss: 0.00004866
Iteration 555/1000 | Loss: 0.00004866
Iteration 556/1000 | Loss: 0.00004866
Iteration 557/1000 | Loss: 0.00004866
Iteration 558/1000 | Loss: 0.00004866
Iteration 559/1000 | Loss: 0.00004866
Iteration 560/1000 | Loss: 0.00004866
Iteration 561/1000 | Loss: 0.00004866
Iteration 562/1000 | Loss: 0.00004866
Iteration 563/1000 | Loss: 0.00004865
Iteration 564/1000 | Loss: 0.00004865
Iteration 565/1000 | Loss: 0.00004865
Iteration 566/1000 | Loss: 0.00004864
Iteration 567/1000 | Loss: 0.00004864
Iteration 568/1000 | Loss: 0.00004864
Iteration 569/1000 | Loss: 0.00004864
Iteration 570/1000 | Loss: 0.00004863
Iteration 571/1000 | Loss: 0.00004863
Iteration 572/1000 | Loss: 0.00004863
Iteration 573/1000 | Loss: 0.00004863
Iteration 574/1000 | Loss: 0.00004862
Iteration 575/1000 | Loss: 0.00004862
Iteration 576/1000 | Loss: 0.00004862
Iteration 577/1000 | Loss: 0.00004862
Iteration 578/1000 | Loss: 0.00004862
Iteration 579/1000 | Loss: 0.00004862
Iteration 580/1000 | Loss: 0.00004862
Iteration 581/1000 | Loss: 0.00004862
Iteration 582/1000 | Loss: 0.00004862
Iteration 583/1000 | Loss: 0.00004862
Iteration 584/1000 | Loss: 0.00004862
Iteration 585/1000 | Loss: 0.00004861
Iteration 586/1000 | Loss: 0.00004861
Iteration 587/1000 | Loss: 0.00004861
Iteration 588/1000 | Loss: 0.00004861
Iteration 589/1000 | Loss: 0.00004861
Iteration 590/1000 | Loss: 0.00004861
Iteration 591/1000 | Loss: 0.00004861
Iteration 592/1000 | Loss: 0.00004861
Iteration 593/1000 | Loss: 0.00004861
Iteration 594/1000 | Loss: 0.00004861
Iteration 595/1000 | Loss: 0.00004861
Iteration 596/1000 | Loss: 0.00004861
Iteration 597/1000 | Loss: 0.00004861
Iteration 598/1000 | Loss: 0.00004860
Iteration 599/1000 | Loss: 0.00004860
Iteration 600/1000 | Loss: 0.00004860
Iteration 601/1000 | Loss: 0.00004860
Iteration 602/1000 | Loss: 0.00004860
Iteration 603/1000 | Loss: 0.00004860
Iteration 604/1000 | Loss: 0.00004860
Iteration 605/1000 | Loss: 0.00004860
Iteration 606/1000 | Loss: 0.00004860
Iteration 607/1000 | Loss: 0.00004860
Iteration 608/1000 | Loss: 0.00004860
Iteration 609/1000 | Loss: 0.00004860
Iteration 610/1000 | Loss: 0.00004860
Iteration 611/1000 | Loss: 0.00004860
Iteration 612/1000 | Loss: 0.00004860
Iteration 613/1000 | Loss: 0.00004860
Iteration 614/1000 | Loss: 0.00004860
Iteration 615/1000 | Loss: 0.00004860
Iteration 616/1000 | Loss: 0.00004860
Iteration 617/1000 | Loss: 0.00004860
Iteration 618/1000 | Loss: 0.00004860
Iteration 619/1000 | Loss: 0.00004860
Iteration 620/1000 | Loss: 0.00004860
Iteration 621/1000 | Loss: 0.00004860
Iteration 622/1000 | Loss: 0.00004860
Iteration 623/1000 | Loss: 0.00004860
Iteration 624/1000 | Loss: 0.00004860
Iteration 625/1000 | Loss: 0.00004860
Iteration 626/1000 | Loss: 0.00004860
Iteration 627/1000 | Loss: 0.00004860
Iteration 628/1000 | Loss: 0.00004860
Iteration 629/1000 | Loss: 0.00004860
Iteration 630/1000 | Loss: 0.00004860
Iteration 631/1000 | Loss: 0.00004860
Iteration 632/1000 | Loss: 0.00004860
Iteration 633/1000 | Loss: 0.00004860
Iteration 634/1000 | Loss: 0.00004860
Iteration 635/1000 | Loss: 0.00004860
Iteration 636/1000 | Loss: 0.00004860
Iteration 637/1000 | Loss: 0.00004860
Iteration 638/1000 | Loss: 0.00004860
Iteration 639/1000 | Loss: 0.00004860
Iteration 640/1000 | Loss: 0.00004860
Iteration 641/1000 | Loss: 0.00004860
Iteration 642/1000 | Loss: 0.00004860
Iteration 643/1000 | Loss: 0.00004860
Iteration 644/1000 | Loss: 0.00004860
Iteration 645/1000 | Loss: 0.00004860
Iteration 646/1000 | Loss: 0.00004860
Iteration 647/1000 | Loss: 0.00004860
Iteration 648/1000 | Loss: 0.00004860
Iteration 649/1000 | Loss: 0.00004860
Iteration 650/1000 | Loss: 0.00004860
Iteration 651/1000 | Loss: 0.00004860
Iteration 652/1000 | Loss: 0.00004860
Iteration 653/1000 | Loss: 0.00004860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 653. Stopping optimization.
Last 5 losses: [4.8596466513117775e-05, 4.8596466513117775e-05, 4.8596466513117775e-05, 4.8596466513117775e-05, 4.8596466513117775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.8596466513117775e-05

Optimization complete. Final v2v error: 5.009650230407715 mm

Highest mean error: 14.151028633117676 mm for frame 74

Lowest mean error: 3.8939051628112793 mm for frame 129

Saving results

Total time: 628.6584689617157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01177405
Iteration 2/25 | Loss: 0.00254296
Iteration 3/25 | Loss: 0.00226642
Iteration 4/25 | Loss: 0.00224649
Iteration 5/25 | Loss: 0.00223673
Iteration 6/25 | Loss: 0.00223478
Iteration 7/25 | Loss: 0.00223362
Iteration 8/25 | Loss: 0.00223362
Iteration 9/25 | Loss: 0.00223362
Iteration 10/25 | Loss: 0.00223362
Iteration 11/25 | Loss: 0.00223362
Iteration 12/25 | Loss: 0.00223362
Iteration 13/25 | Loss: 0.00223362
Iteration 14/25 | Loss: 0.00223362
Iteration 15/25 | Loss: 0.00223362
Iteration 16/25 | Loss: 0.00223362
Iteration 17/25 | Loss: 0.00223362
Iteration 18/25 | Loss: 0.00223362
Iteration 19/25 | Loss: 0.00223362
Iteration 20/25 | Loss: 0.00223362
Iteration 21/25 | Loss: 0.00223362
Iteration 22/25 | Loss: 0.00223362
Iteration 23/25 | Loss: 0.00223362
Iteration 24/25 | Loss: 0.00223362
Iteration 25/25 | Loss: 0.00223362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002233616542071104, 0.002233616542071104, 0.002233616542071104, 0.002233616542071104, 0.002233616542071104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002233616542071104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03396165
Iteration 2/25 | Loss: 0.00187607
Iteration 3/25 | Loss: 0.00187605
Iteration 4/25 | Loss: 0.00187605
Iteration 5/25 | Loss: 0.00187605
Iteration 6/25 | Loss: 0.00187605
Iteration 7/25 | Loss: 0.00187605
Iteration 8/25 | Loss: 0.00187605
Iteration 9/25 | Loss: 0.00187605
Iteration 10/25 | Loss: 0.00187605
Iteration 11/25 | Loss: 0.00187605
Iteration 12/25 | Loss: 0.00187605
Iteration 13/25 | Loss: 0.00187605
Iteration 14/25 | Loss: 0.00187605
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0018760522361844778, 0.0018760522361844778, 0.0018760522361844778, 0.0018760522361844778, 0.0018760522361844778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018760522361844778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187605
Iteration 2/1000 | Loss: 0.00013822
Iteration 3/1000 | Loss: 0.00010690
Iteration 4/1000 | Loss: 0.00009112
Iteration 5/1000 | Loss: 0.00008670
Iteration 6/1000 | Loss: 0.00008340
Iteration 7/1000 | Loss: 0.00008086
Iteration 8/1000 | Loss: 0.00007936
Iteration 9/1000 | Loss: 0.00007792
Iteration 10/1000 | Loss: 0.00007698
Iteration 11/1000 | Loss: 0.00007628
Iteration 12/1000 | Loss: 0.00007553
Iteration 13/1000 | Loss: 0.00007488
Iteration 14/1000 | Loss: 0.00007433
Iteration 15/1000 | Loss: 0.00007390
Iteration 16/1000 | Loss: 0.00007355
Iteration 17/1000 | Loss: 0.00007321
Iteration 18/1000 | Loss: 0.00007300
Iteration 19/1000 | Loss: 0.00007278
Iteration 20/1000 | Loss: 0.00007256
Iteration 21/1000 | Loss: 0.00007240
Iteration 22/1000 | Loss: 0.00007227
Iteration 23/1000 | Loss: 0.00007224
Iteration 24/1000 | Loss: 0.00007218
Iteration 25/1000 | Loss: 0.00007209
Iteration 26/1000 | Loss: 0.00007209
Iteration 27/1000 | Loss: 0.00007204
Iteration 28/1000 | Loss: 0.00007203
Iteration 29/1000 | Loss: 0.00007202
Iteration 30/1000 | Loss: 0.00007201
Iteration 31/1000 | Loss: 0.00007199
Iteration 32/1000 | Loss: 0.00007197
Iteration 33/1000 | Loss: 0.00007197
Iteration 34/1000 | Loss: 0.00007196
Iteration 35/1000 | Loss: 0.00007196
Iteration 36/1000 | Loss: 0.00007196
Iteration 37/1000 | Loss: 0.00007196
Iteration 38/1000 | Loss: 0.00007195
Iteration 39/1000 | Loss: 0.00007194
Iteration 40/1000 | Loss: 0.00007193
Iteration 41/1000 | Loss: 0.00007193
Iteration 42/1000 | Loss: 0.00007193
Iteration 43/1000 | Loss: 0.00007193
Iteration 44/1000 | Loss: 0.00007192
Iteration 45/1000 | Loss: 0.00007192
Iteration 46/1000 | Loss: 0.00007192
Iteration 47/1000 | Loss: 0.00007192
Iteration 48/1000 | Loss: 0.00007192
Iteration 49/1000 | Loss: 0.00007191
Iteration 50/1000 | Loss: 0.00007190
Iteration 51/1000 | Loss: 0.00007190
Iteration 52/1000 | Loss: 0.00007189
Iteration 53/1000 | Loss: 0.00007189
Iteration 54/1000 | Loss: 0.00007187
Iteration 55/1000 | Loss: 0.00007187
Iteration 56/1000 | Loss: 0.00007186
Iteration 57/1000 | Loss: 0.00007184
Iteration 58/1000 | Loss: 0.00007184
Iteration 59/1000 | Loss: 0.00007183
Iteration 60/1000 | Loss: 0.00007183
Iteration 61/1000 | Loss: 0.00007183
Iteration 62/1000 | Loss: 0.00007183
Iteration 63/1000 | Loss: 0.00007183
Iteration 64/1000 | Loss: 0.00007183
Iteration 65/1000 | Loss: 0.00007183
Iteration 66/1000 | Loss: 0.00007183
Iteration 67/1000 | Loss: 0.00007183
Iteration 68/1000 | Loss: 0.00007183
Iteration 69/1000 | Loss: 0.00007183
Iteration 70/1000 | Loss: 0.00007182
Iteration 71/1000 | Loss: 0.00007181
Iteration 72/1000 | Loss: 0.00007181
Iteration 73/1000 | Loss: 0.00007180
Iteration 74/1000 | Loss: 0.00007180
Iteration 75/1000 | Loss: 0.00007180
Iteration 76/1000 | Loss: 0.00007180
Iteration 77/1000 | Loss: 0.00007180
Iteration 78/1000 | Loss: 0.00007180
Iteration 79/1000 | Loss: 0.00007180
Iteration 80/1000 | Loss: 0.00007180
Iteration 81/1000 | Loss: 0.00007180
Iteration 82/1000 | Loss: 0.00007180
Iteration 83/1000 | Loss: 0.00007180
Iteration 84/1000 | Loss: 0.00007180
Iteration 85/1000 | Loss: 0.00007179
Iteration 86/1000 | Loss: 0.00007179
Iteration 87/1000 | Loss: 0.00007179
Iteration 88/1000 | Loss: 0.00007179
Iteration 89/1000 | Loss: 0.00007178
Iteration 90/1000 | Loss: 0.00007178
Iteration 91/1000 | Loss: 0.00007178
Iteration 92/1000 | Loss: 0.00007178
Iteration 93/1000 | Loss: 0.00007178
Iteration 94/1000 | Loss: 0.00007178
Iteration 95/1000 | Loss: 0.00007178
Iteration 96/1000 | Loss: 0.00007178
Iteration 97/1000 | Loss: 0.00007178
Iteration 98/1000 | Loss: 0.00007178
Iteration 99/1000 | Loss: 0.00007178
Iteration 100/1000 | Loss: 0.00007178
Iteration 101/1000 | Loss: 0.00007178
Iteration 102/1000 | Loss: 0.00007177
Iteration 103/1000 | Loss: 0.00007177
Iteration 104/1000 | Loss: 0.00007177
Iteration 105/1000 | Loss: 0.00007177
Iteration 106/1000 | Loss: 0.00007177
Iteration 107/1000 | Loss: 0.00007177
Iteration 108/1000 | Loss: 0.00007177
Iteration 109/1000 | Loss: 0.00007177
Iteration 110/1000 | Loss: 0.00007176
Iteration 111/1000 | Loss: 0.00007176
Iteration 112/1000 | Loss: 0.00007176
Iteration 113/1000 | Loss: 0.00007176
Iteration 114/1000 | Loss: 0.00007176
Iteration 115/1000 | Loss: 0.00007176
Iteration 116/1000 | Loss: 0.00007176
Iteration 117/1000 | Loss: 0.00007176
Iteration 118/1000 | Loss: 0.00007176
Iteration 119/1000 | Loss: 0.00007176
Iteration 120/1000 | Loss: 0.00007176
Iteration 121/1000 | Loss: 0.00007176
Iteration 122/1000 | Loss: 0.00007175
Iteration 123/1000 | Loss: 0.00007175
Iteration 124/1000 | Loss: 0.00007175
Iteration 125/1000 | Loss: 0.00007175
Iteration 126/1000 | Loss: 0.00007175
Iteration 127/1000 | Loss: 0.00007175
Iteration 128/1000 | Loss: 0.00007175
Iteration 129/1000 | Loss: 0.00007175
Iteration 130/1000 | Loss: 0.00007175
Iteration 131/1000 | Loss: 0.00007175
Iteration 132/1000 | Loss: 0.00007175
Iteration 133/1000 | Loss: 0.00007175
Iteration 134/1000 | Loss: 0.00007175
Iteration 135/1000 | Loss: 0.00007175
Iteration 136/1000 | Loss: 0.00007175
Iteration 137/1000 | Loss: 0.00007175
Iteration 138/1000 | Loss: 0.00007175
Iteration 139/1000 | Loss: 0.00007175
Iteration 140/1000 | Loss: 0.00007174
Iteration 141/1000 | Loss: 0.00007174
Iteration 142/1000 | Loss: 0.00007174
Iteration 143/1000 | Loss: 0.00007174
Iteration 144/1000 | Loss: 0.00007174
Iteration 145/1000 | Loss: 0.00007174
Iteration 146/1000 | Loss: 0.00007174
Iteration 147/1000 | Loss: 0.00007174
Iteration 148/1000 | Loss: 0.00007174
Iteration 149/1000 | Loss: 0.00007174
Iteration 150/1000 | Loss: 0.00007174
Iteration 151/1000 | Loss: 0.00007174
Iteration 152/1000 | Loss: 0.00007174
Iteration 153/1000 | Loss: 0.00007174
Iteration 154/1000 | Loss: 0.00007174
Iteration 155/1000 | Loss: 0.00007174
Iteration 156/1000 | Loss: 0.00007174
Iteration 157/1000 | Loss: 0.00007174
Iteration 158/1000 | Loss: 0.00007174
Iteration 159/1000 | Loss: 0.00007174
Iteration 160/1000 | Loss: 0.00007174
Iteration 161/1000 | Loss: 0.00007174
Iteration 162/1000 | Loss: 0.00007173
Iteration 163/1000 | Loss: 0.00007173
Iteration 164/1000 | Loss: 0.00007173
Iteration 165/1000 | Loss: 0.00007173
Iteration 166/1000 | Loss: 0.00007173
Iteration 167/1000 | Loss: 0.00007173
Iteration 168/1000 | Loss: 0.00007173
Iteration 169/1000 | Loss: 0.00007173
Iteration 170/1000 | Loss: 0.00007173
Iteration 171/1000 | Loss: 0.00007173
Iteration 172/1000 | Loss: 0.00007173
Iteration 173/1000 | Loss: 0.00007173
Iteration 174/1000 | Loss: 0.00007173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [7.173414633143693e-05, 7.173414633143693e-05, 7.173414633143693e-05, 7.173414633143693e-05, 7.173414633143693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.173414633143693e-05

Optimization complete. Final v2v error: 6.993372440338135 mm

Highest mean error: 8.46917724609375 mm for frame 79

Lowest mean error: 5.9713006019592285 mm for frame 26

Saving results

Total time: 56.891239643096924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960235
Iteration 2/25 | Loss: 0.00228848
Iteration 3/25 | Loss: 0.00208147
Iteration 4/25 | Loss: 0.00205073
Iteration 5/25 | Loss: 0.00204510
Iteration 6/25 | Loss: 0.00204270
Iteration 7/25 | Loss: 0.00204198
Iteration 8/25 | Loss: 0.00204165
Iteration 9/25 | Loss: 0.00204149
Iteration 10/25 | Loss: 0.00204114
Iteration 11/25 | Loss: 0.00204122
Iteration 12/25 | Loss: 0.00204057
Iteration 13/25 | Loss: 0.00204061
Iteration 14/25 | Loss: 0.00204016
Iteration 15/25 | Loss: 0.00204050
Iteration 16/25 | Loss: 0.00204092
Iteration 17/25 | Loss: 0.00204064
Iteration 18/25 | Loss: 0.00204118
Iteration 19/25 | Loss: 0.00204052
Iteration 20/25 | Loss: 0.00204127
Iteration 21/25 | Loss: 0.00204089
Iteration 22/25 | Loss: 0.00204107
Iteration 23/25 | Loss: 0.00204088
Iteration 24/25 | Loss: 0.00204035
Iteration 25/25 | Loss: 0.00204033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56274390
Iteration 2/25 | Loss: 0.00177922
Iteration 3/25 | Loss: 0.00177921
Iteration 4/25 | Loss: 0.00177921
Iteration 5/25 | Loss: 0.00177921
Iteration 6/25 | Loss: 0.00177921
Iteration 7/25 | Loss: 0.00177921
Iteration 8/25 | Loss: 0.00177921
Iteration 9/25 | Loss: 0.00177921
Iteration 10/25 | Loss: 0.00177921
Iteration 11/25 | Loss: 0.00177921
Iteration 12/25 | Loss: 0.00177921
Iteration 13/25 | Loss: 0.00177921
Iteration 14/25 | Loss: 0.00177921
Iteration 15/25 | Loss: 0.00177921
Iteration 16/25 | Loss: 0.00177921
Iteration 17/25 | Loss: 0.00177921
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017792071448639035, 0.0017792071448639035, 0.0017792071448639035, 0.0017792071448639035, 0.0017792071448639035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017792071448639035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177921
Iteration 2/1000 | Loss: 0.00008033
Iteration 3/1000 | Loss: 0.00005154
Iteration 4/1000 | Loss: 0.00004574
Iteration 5/1000 | Loss: 0.00004351
Iteration 6/1000 | Loss: 0.00004200
Iteration 7/1000 | Loss: 0.00004105
Iteration 8/1000 | Loss: 0.00004031
Iteration 9/1000 | Loss: 0.00003983
Iteration 10/1000 | Loss: 0.00003940
Iteration 11/1000 | Loss: 0.00003896
Iteration 12/1000 | Loss: 0.00003867
Iteration 13/1000 | Loss: 0.00003843
Iteration 14/1000 | Loss: 0.00003835
Iteration 15/1000 | Loss: 0.00003832
Iteration 16/1000 | Loss: 0.00003822
Iteration 17/1000 | Loss: 0.00003820
Iteration 18/1000 | Loss: 0.00003819
Iteration 19/1000 | Loss: 0.00003818
Iteration 20/1000 | Loss: 0.00003818
Iteration 21/1000 | Loss: 0.00003818
Iteration 22/1000 | Loss: 0.00003817
Iteration 23/1000 | Loss: 0.00003817
Iteration 24/1000 | Loss: 0.00003817
Iteration 25/1000 | Loss: 0.00003816
Iteration 26/1000 | Loss: 0.00003816
Iteration 27/1000 | Loss: 0.00003816
Iteration 28/1000 | Loss: 0.00003816
Iteration 29/1000 | Loss: 0.00003816
Iteration 30/1000 | Loss: 0.00003805
Iteration 31/1000 | Loss: 0.00003796
Iteration 32/1000 | Loss: 0.00003793
Iteration 33/1000 | Loss: 0.00003789
Iteration 34/1000 | Loss: 0.00003785
Iteration 35/1000 | Loss: 0.00003785
Iteration 36/1000 | Loss: 0.00003785
Iteration 37/1000 | Loss: 0.00003785
Iteration 38/1000 | Loss: 0.00003785
Iteration 39/1000 | Loss: 0.00003784
Iteration 40/1000 | Loss: 0.00003784
Iteration 41/1000 | Loss: 0.00003784
Iteration 42/1000 | Loss: 0.00003784
Iteration 43/1000 | Loss: 0.00003784
Iteration 44/1000 | Loss: 0.00003784
Iteration 45/1000 | Loss: 0.00003784
Iteration 46/1000 | Loss: 0.00003784
Iteration 47/1000 | Loss: 0.00003784
Iteration 48/1000 | Loss: 0.00003783
Iteration 49/1000 | Loss: 0.00003783
Iteration 50/1000 | Loss: 0.00003783
Iteration 51/1000 | Loss: 0.00003783
Iteration 52/1000 | Loss: 0.00003783
Iteration 53/1000 | Loss: 0.00003783
Iteration 54/1000 | Loss: 0.00003783
Iteration 55/1000 | Loss: 0.00003782
Iteration 56/1000 | Loss: 0.00003782
Iteration 57/1000 | Loss: 0.00003782
Iteration 58/1000 | Loss: 0.00003782
Iteration 59/1000 | Loss: 0.00003782
Iteration 60/1000 | Loss: 0.00003781
Iteration 61/1000 | Loss: 0.00003781
Iteration 62/1000 | Loss: 0.00003781
Iteration 63/1000 | Loss: 0.00003781
Iteration 64/1000 | Loss: 0.00003781
Iteration 65/1000 | Loss: 0.00003781
Iteration 66/1000 | Loss: 0.00003781
Iteration 67/1000 | Loss: 0.00003780
Iteration 68/1000 | Loss: 0.00003780
Iteration 69/1000 | Loss: 0.00003780
Iteration 70/1000 | Loss: 0.00003780
Iteration 71/1000 | Loss: 0.00003780
Iteration 72/1000 | Loss: 0.00003780
Iteration 73/1000 | Loss: 0.00003780
Iteration 74/1000 | Loss: 0.00003780
Iteration 75/1000 | Loss: 0.00003779
Iteration 76/1000 | Loss: 0.00003779
Iteration 77/1000 | Loss: 0.00003779
Iteration 78/1000 | Loss: 0.00003779
Iteration 79/1000 | Loss: 0.00003815
Iteration 80/1000 | Loss: 0.00003815
Iteration 81/1000 | Loss: 0.00003814
Iteration 82/1000 | Loss: 0.00003813
Iteration 83/1000 | Loss: 0.00003806
Iteration 84/1000 | Loss: 0.00003804
Iteration 85/1000 | Loss: 0.00003803
Iteration 86/1000 | Loss: 0.00003803
Iteration 87/1000 | Loss: 0.00003803
Iteration 88/1000 | Loss: 0.00003803
Iteration 89/1000 | Loss: 0.00003803
Iteration 90/1000 | Loss: 0.00003802
Iteration 91/1000 | Loss: 0.00003802
Iteration 92/1000 | Loss: 0.00003802
Iteration 93/1000 | Loss: 0.00003802
Iteration 94/1000 | Loss: 0.00003802
Iteration 95/1000 | Loss: 0.00003801
Iteration 96/1000 | Loss: 0.00003801
Iteration 97/1000 | Loss: 0.00003801
Iteration 98/1000 | Loss: 0.00003801
Iteration 99/1000 | Loss: 0.00003801
Iteration 100/1000 | Loss: 0.00003801
Iteration 101/1000 | Loss: 0.00003801
Iteration 102/1000 | Loss: 0.00003801
Iteration 103/1000 | Loss: 0.00003801
Iteration 104/1000 | Loss: 0.00003801
Iteration 105/1000 | Loss: 0.00003800
Iteration 106/1000 | Loss: 0.00003800
Iteration 107/1000 | Loss: 0.00003800
Iteration 108/1000 | Loss: 0.00003800
Iteration 109/1000 | Loss: 0.00003800
Iteration 110/1000 | Loss: 0.00003800
Iteration 111/1000 | Loss: 0.00003800
Iteration 112/1000 | Loss: 0.00003800
Iteration 113/1000 | Loss: 0.00003800
Iteration 114/1000 | Loss: 0.00003800
Iteration 115/1000 | Loss: 0.00003800
Iteration 116/1000 | Loss: 0.00003800
Iteration 117/1000 | Loss: 0.00003800
Iteration 118/1000 | Loss: 0.00003800
Iteration 119/1000 | Loss: 0.00003800
Iteration 120/1000 | Loss: 0.00003800
Iteration 121/1000 | Loss: 0.00003800
Iteration 122/1000 | Loss: 0.00003799
Iteration 123/1000 | Loss: 0.00003799
Iteration 124/1000 | Loss: 0.00003799
Iteration 125/1000 | Loss: 0.00003799
Iteration 126/1000 | Loss: 0.00003799
Iteration 127/1000 | Loss: 0.00003799
Iteration 128/1000 | Loss: 0.00003799
Iteration 129/1000 | Loss: 0.00003799
Iteration 130/1000 | Loss: 0.00003799
Iteration 131/1000 | Loss: 0.00003799
Iteration 132/1000 | Loss: 0.00003799
Iteration 133/1000 | Loss: 0.00003799
Iteration 134/1000 | Loss: 0.00003799
Iteration 135/1000 | Loss: 0.00003799
Iteration 136/1000 | Loss: 0.00003799
Iteration 137/1000 | Loss: 0.00003799
Iteration 138/1000 | Loss: 0.00003799
Iteration 139/1000 | Loss: 0.00003799
Iteration 140/1000 | Loss: 0.00003799
Iteration 141/1000 | Loss: 0.00003798
Iteration 142/1000 | Loss: 0.00003798
Iteration 143/1000 | Loss: 0.00003798
Iteration 144/1000 | Loss: 0.00003798
Iteration 145/1000 | Loss: 0.00003798
Iteration 146/1000 | Loss: 0.00003798
Iteration 147/1000 | Loss: 0.00003798
Iteration 148/1000 | Loss: 0.00003798
Iteration 149/1000 | Loss: 0.00003798
Iteration 150/1000 | Loss: 0.00003798
Iteration 151/1000 | Loss: 0.00003798
Iteration 152/1000 | Loss: 0.00003798
Iteration 153/1000 | Loss: 0.00003798
Iteration 154/1000 | Loss: 0.00003798
Iteration 155/1000 | Loss: 0.00003797
Iteration 156/1000 | Loss: 0.00003797
Iteration 157/1000 | Loss: 0.00003797
Iteration 158/1000 | Loss: 0.00003797
Iteration 159/1000 | Loss: 0.00003797
Iteration 160/1000 | Loss: 0.00003797
Iteration 161/1000 | Loss: 0.00003797
Iteration 162/1000 | Loss: 0.00003797
Iteration 163/1000 | Loss: 0.00003797
Iteration 164/1000 | Loss: 0.00003796
Iteration 165/1000 | Loss: 0.00003796
Iteration 166/1000 | Loss: 0.00003796
Iteration 167/1000 | Loss: 0.00003796
Iteration 168/1000 | Loss: 0.00003796
Iteration 169/1000 | Loss: 0.00003796
Iteration 170/1000 | Loss: 0.00003796
Iteration 171/1000 | Loss: 0.00003796
Iteration 172/1000 | Loss: 0.00003796
Iteration 173/1000 | Loss: 0.00003796
Iteration 174/1000 | Loss: 0.00003796
Iteration 175/1000 | Loss: 0.00003796
Iteration 176/1000 | Loss: 0.00003796
Iteration 177/1000 | Loss: 0.00003796
Iteration 178/1000 | Loss: 0.00003796
Iteration 179/1000 | Loss: 0.00003796
Iteration 180/1000 | Loss: 0.00003796
Iteration 181/1000 | Loss: 0.00003796
Iteration 182/1000 | Loss: 0.00003796
Iteration 183/1000 | Loss: 0.00003796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [3.795818702201359e-05, 3.795818702201359e-05, 3.795818702201359e-05, 3.795818702201359e-05, 3.795818702201359e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.795818702201359e-05

Optimization complete. Final v2v error: 5.279169082641602 mm

Highest mean error: 12.411182403564453 mm for frame 55

Lowest mean error: 5.0647077560424805 mm for frame 153

Saving results

Total time: 87.61611104011536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904385
Iteration 2/25 | Loss: 0.00244368
Iteration 3/25 | Loss: 0.00223227
Iteration 4/25 | Loss: 0.00219258
Iteration 5/25 | Loss: 0.00218401
Iteration 6/25 | Loss: 0.00218271
Iteration 7/25 | Loss: 0.00217422
Iteration 8/25 | Loss: 0.00216811
Iteration 9/25 | Loss: 0.00217033
Iteration 10/25 | Loss: 0.00216218
Iteration 11/25 | Loss: 0.00215690
Iteration 12/25 | Loss: 0.00215503
Iteration 13/25 | Loss: 0.00215799
Iteration 14/25 | Loss: 0.00215827
Iteration 15/25 | Loss: 0.00215709
Iteration 16/25 | Loss: 0.00215351
Iteration 17/25 | Loss: 0.00215843
Iteration 18/25 | Loss: 0.00215721
Iteration 19/25 | Loss: 0.00215741
Iteration 20/25 | Loss: 0.00215808
Iteration 21/25 | Loss: 0.00215381
Iteration 22/25 | Loss: 0.00215304
Iteration 23/25 | Loss: 0.00215260
Iteration 24/25 | Loss: 0.00215268
Iteration 25/25 | Loss: 0.00215244

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38478971
Iteration 2/25 | Loss: 0.00227301
Iteration 3/25 | Loss: 0.00227301
Iteration 4/25 | Loss: 0.00227301
Iteration 5/25 | Loss: 0.00227301
Iteration 6/25 | Loss: 0.00227301
Iteration 7/25 | Loss: 0.00227301
Iteration 8/25 | Loss: 0.00227301
Iteration 9/25 | Loss: 0.00227301
Iteration 10/25 | Loss: 0.00227301
Iteration 11/25 | Loss: 0.00227301
Iteration 12/25 | Loss: 0.00227301
Iteration 13/25 | Loss: 0.00227301
Iteration 14/25 | Loss: 0.00227301
Iteration 15/25 | Loss: 0.00227301
Iteration 16/25 | Loss: 0.00227301
Iteration 17/25 | Loss: 0.00227301
Iteration 18/25 | Loss: 0.00227301
Iteration 19/25 | Loss: 0.00227301
Iteration 20/25 | Loss: 0.00227301
Iteration 21/25 | Loss: 0.00227301
Iteration 22/25 | Loss: 0.00227301
Iteration 23/25 | Loss: 0.00227301
Iteration 24/25 | Loss: 0.00227301
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0022730063647031784, 0.0022730063647031784, 0.0022730063647031784, 0.0022730063647031784, 0.0022730063647031784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022730063647031784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227301
Iteration 2/1000 | Loss: 0.00011551
Iteration 3/1000 | Loss: 0.00008415
Iteration 4/1000 | Loss: 0.00007897
Iteration 5/1000 | Loss: 0.00007876
Iteration 6/1000 | Loss: 0.00006423
Iteration 7/1000 | Loss: 0.00007040
Iteration 8/1000 | Loss: 0.00007054
Iteration 9/1000 | Loss: 0.00006502
Iteration 10/1000 | Loss: 0.00007403
Iteration 11/1000 | Loss: 0.00007104
Iteration 12/1000 | Loss: 0.00037477
Iteration 13/1000 | Loss: 0.00057133
Iteration 14/1000 | Loss: 0.00013214
Iteration 15/1000 | Loss: 0.00008014
Iteration 16/1000 | Loss: 0.00006318
Iteration 17/1000 | Loss: 0.00006866
Iteration 18/1000 | Loss: 0.00006656
Iteration 19/1000 | Loss: 0.00006292
Iteration 20/1000 | Loss: 0.00005679
Iteration 21/1000 | Loss: 0.00005289
Iteration 22/1000 | Loss: 0.00006087
Iteration 23/1000 | Loss: 0.00005239
Iteration 24/1000 | Loss: 0.00005023
Iteration 25/1000 | Loss: 0.00005867
Iteration 26/1000 | Loss: 0.00006565
Iteration 27/1000 | Loss: 0.00005847
Iteration 28/1000 | Loss: 0.00006002
Iteration 29/1000 | Loss: 0.00008446
Iteration 30/1000 | Loss: 0.00005736
Iteration 31/1000 | Loss: 0.00005321
Iteration 32/1000 | Loss: 0.00004572
Iteration 33/1000 | Loss: 0.00005103
Iteration 34/1000 | Loss: 0.00005738
Iteration 35/1000 | Loss: 0.00007095
Iteration 36/1000 | Loss: 0.00005632
Iteration 37/1000 | Loss: 0.00005500
Iteration 38/1000 | Loss: 0.00005549
Iteration 39/1000 | Loss: 0.00006136
Iteration 40/1000 | Loss: 0.00006010
Iteration 41/1000 | Loss: 0.00006140
Iteration 42/1000 | Loss: 0.00005990
Iteration 43/1000 | Loss: 0.00006250
Iteration 44/1000 | Loss: 0.00005583
Iteration 45/1000 | Loss: 0.00005473
Iteration 46/1000 | Loss: 0.00005226
Iteration 47/1000 | Loss: 0.00005124
Iteration 48/1000 | Loss: 0.00005246
Iteration 49/1000 | Loss: 0.00005268
Iteration 50/1000 | Loss: 0.00006112
Iteration 51/1000 | Loss: 0.00005494
Iteration 52/1000 | Loss: 0.00004759
Iteration 53/1000 | Loss: 0.00006110
Iteration 54/1000 | Loss: 0.00005462
Iteration 55/1000 | Loss: 0.00004920
Iteration 56/1000 | Loss: 0.00004715
Iteration 57/1000 | Loss: 0.00004609
Iteration 58/1000 | Loss: 0.00004493
Iteration 59/1000 | Loss: 0.00004442
Iteration 60/1000 | Loss: 0.00004664
Iteration 61/1000 | Loss: 0.00004408
Iteration 62/1000 | Loss: 0.00004400
Iteration 63/1000 | Loss: 0.00004397
Iteration 64/1000 | Loss: 0.00004394
Iteration 65/1000 | Loss: 0.00004393
Iteration 66/1000 | Loss: 0.00004392
Iteration 67/1000 | Loss: 0.00004638
Iteration 68/1000 | Loss: 0.00004388
Iteration 69/1000 | Loss: 0.00004388
Iteration 70/1000 | Loss: 0.00004388
Iteration 71/1000 | Loss: 0.00004388
Iteration 72/1000 | Loss: 0.00004388
Iteration 73/1000 | Loss: 0.00004388
Iteration 74/1000 | Loss: 0.00004388
Iteration 75/1000 | Loss: 0.00004387
Iteration 76/1000 | Loss: 0.00004387
Iteration 77/1000 | Loss: 0.00004387
Iteration 78/1000 | Loss: 0.00004387
Iteration 79/1000 | Loss: 0.00004387
Iteration 80/1000 | Loss: 0.00004387
Iteration 81/1000 | Loss: 0.00004386
Iteration 82/1000 | Loss: 0.00004383
Iteration 83/1000 | Loss: 0.00004383
Iteration 84/1000 | Loss: 0.00004382
Iteration 85/1000 | Loss: 0.00004382
Iteration 86/1000 | Loss: 0.00004382
Iteration 87/1000 | Loss: 0.00004378
Iteration 88/1000 | Loss: 0.00004378
Iteration 89/1000 | Loss: 0.00004378
Iteration 90/1000 | Loss: 0.00004378
Iteration 91/1000 | Loss: 0.00004378
Iteration 92/1000 | Loss: 0.00004378
Iteration 93/1000 | Loss: 0.00004378
Iteration 94/1000 | Loss: 0.00004378
Iteration 95/1000 | Loss: 0.00004377
Iteration 96/1000 | Loss: 0.00004377
Iteration 97/1000 | Loss: 0.00004375
Iteration 98/1000 | Loss: 0.00004374
Iteration 99/1000 | Loss: 0.00004373
Iteration 100/1000 | Loss: 0.00004373
Iteration 101/1000 | Loss: 0.00004373
Iteration 102/1000 | Loss: 0.00004373
Iteration 103/1000 | Loss: 0.00004373
Iteration 104/1000 | Loss: 0.00004372
Iteration 105/1000 | Loss: 0.00004371
Iteration 106/1000 | Loss: 0.00004371
Iteration 107/1000 | Loss: 0.00004370
Iteration 108/1000 | Loss: 0.00004368
Iteration 109/1000 | Loss: 0.00004367
Iteration 110/1000 | Loss: 0.00004365
Iteration 111/1000 | Loss: 0.00004363
Iteration 112/1000 | Loss: 0.00004363
Iteration 113/1000 | Loss: 0.00004363
Iteration 114/1000 | Loss: 0.00004363
Iteration 115/1000 | Loss: 0.00004363
Iteration 116/1000 | Loss: 0.00004363
Iteration 117/1000 | Loss: 0.00004362
Iteration 118/1000 | Loss: 0.00004362
Iteration 119/1000 | Loss: 0.00004362
Iteration 120/1000 | Loss: 0.00004357
Iteration 121/1000 | Loss: 0.00004355
Iteration 122/1000 | Loss: 0.00004355
Iteration 123/1000 | Loss: 0.00004354
Iteration 124/1000 | Loss: 0.00004354
Iteration 125/1000 | Loss: 0.00004354
Iteration 126/1000 | Loss: 0.00004354
Iteration 127/1000 | Loss: 0.00004354
Iteration 128/1000 | Loss: 0.00004354
Iteration 129/1000 | Loss: 0.00004354
Iteration 130/1000 | Loss: 0.00004354
Iteration 131/1000 | Loss: 0.00004354
Iteration 132/1000 | Loss: 0.00004354
Iteration 133/1000 | Loss: 0.00004354
Iteration 134/1000 | Loss: 0.00004353
Iteration 135/1000 | Loss: 0.00004353
Iteration 136/1000 | Loss: 0.00004351
Iteration 137/1000 | Loss: 0.00004351
Iteration 138/1000 | Loss: 0.00004351
Iteration 139/1000 | Loss: 0.00004351
Iteration 140/1000 | Loss: 0.00004351
Iteration 141/1000 | Loss: 0.00004351
Iteration 142/1000 | Loss: 0.00004351
Iteration 143/1000 | Loss: 0.00004351
Iteration 144/1000 | Loss: 0.00004350
Iteration 145/1000 | Loss: 0.00004350
Iteration 146/1000 | Loss: 0.00004350
Iteration 147/1000 | Loss: 0.00004350
Iteration 148/1000 | Loss: 0.00004349
Iteration 149/1000 | Loss: 0.00004349
Iteration 150/1000 | Loss: 0.00004348
Iteration 151/1000 | Loss: 0.00004348
Iteration 152/1000 | Loss: 0.00004347
Iteration 153/1000 | Loss: 0.00004347
Iteration 154/1000 | Loss: 0.00004346
Iteration 155/1000 | Loss: 0.00004346
Iteration 156/1000 | Loss: 0.00004346
Iteration 157/1000 | Loss: 0.00004345
Iteration 158/1000 | Loss: 0.00004345
Iteration 159/1000 | Loss: 0.00004345
Iteration 160/1000 | Loss: 0.00004345
Iteration 161/1000 | Loss: 0.00004345
Iteration 162/1000 | Loss: 0.00004345
Iteration 163/1000 | Loss: 0.00004344
Iteration 164/1000 | Loss: 0.00004343
Iteration 165/1000 | Loss: 0.00004343
Iteration 166/1000 | Loss: 0.00004343
Iteration 167/1000 | Loss: 0.00004343
Iteration 168/1000 | Loss: 0.00004343
Iteration 169/1000 | Loss: 0.00004343
Iteration 170/1000 | Loss: 0.00004343
Iteration 171/1000 | Loss: 0.00004343
Iteration 172/1000 | Loss: 0.00004343
Iteration 173/1000 | Loss: 0.00004343
Iteration 174/1000 | Loss: 0.00004342
Iteration 175/1000 | Loss: 0.00004342
Iteration 176/1000 | Loss: 0.00004342
Iteration 177/1000 | Loss: 0.00004342
Iteration 178/1000 | Loss: 0.00004342
Iteration 179/1000 | Loss: 0.00004341
Iteration 180/1000 | Loss: 0.00004341
Iteration 181/1000 | Loss: 0.00004341
Iteration 182/1000 | Loss: 0.00004340
Iteration 183/1000 | Loss: 0.00004340
Iteration 184/1000 | Loss: 0.00004340
Iteration 185/1000 | Loss: 0.00004340
Iteration 186/1000 | Loss: 0.00004339
Iteration 187/1000 | Loss: 0.00004339
Iteration 188/1000 | Loss: 0.00004339
Iteration 189/1000 | Loss: 0.00004339
Iteration 190/1000 | Loss: 0.00004339
Iteration 191/1000 | Loss: 0.00004338
Iteration 192/1000 | Loss: 0.00004338
Iteration 193/1000 | Loss: 0.00004338
Iteration 194/1000 | Loss: 0.00004338
Iteration 195/1000 | Loss: 0.00004338
Iteration 196/1000 | Loss: 0.00004338
Iteration 197/1000 | Loss: 0.00004338
Iteration 198/1000 | Loss: 0.00004337
Iteration 199/1000 | Loss: 0.00004337
Iteration 200/1000 | Loss: 0.00004337
Iteration 201/1000 | Loss: 0.00004337
Iteration 202/1000 | Loss: 0.00004337
Iteration 203/1000 | Loss: 0.00004337
Iteration 204/1000 | Loss: 0.00004337
Iteration 205/1000 | Loss: 0.00004337
Iteration 206/1000 | Loss: 0.00004337
Iteration 207/1000 | Loss: 0.00004336
Iteration 208/1000 | Loss: 0.00004336
Iteration 209/1000 | Loss: 0.00004336
Iteration 210/1000 | Loss: 0.00004336
Iteration 211/1000 | Loss: 0.00004335
Iteration 212/1000 | Loss: 0.00004335
Iteration 213/1000 | Loss: 0.00004335
Iteration 214/1000 | Loss: 0.00004334
Iteration 215/1000 | Loss: 0.00004334
Iteration 216/1000 | Loss: 0.00004334
Iteration 217/1000 | Loss: 0.00004334
Iteration 218/1000 | Loss: 0.00004333
Iteration 219/1000 | Loss: 0.00004333
Iteration 220/1000 | Loss: 0.00004333
Iteration 221/1000 | Loss: 0.00004333
Iteration 222/1000 | Loss: 0.00004333
Iteration 223/1000 | Loss: 0.00004332
Iteration 224/1000 | Loss: 0.00004332
Iteration 225/1000 | Loss: 0.00004332
Iteration 226/1000 | Loss: 0.00004332
Iteration 227/1000 | Loss: 0.00004332
Iteration 228/1000 | Loss: 0.00004332
Iteration 229/1000 | Loss: 0.00004332
Iteration 230/1000 | Loss: 0.00004332
Iteration 231/1000 | Loss: 0.00004331
Iteration 232/1000 | Loss: 0.00004331
Iteration 233/1000 | Loss: 0.00004331
Iteration 234/1000 | Loss: 0.00004331
Iteration 235/1000 | Loss: 0.00004331
Iteration 236/1000 | Loss: 0.00004331
Iteration 237/1000 | Loss: 0.00004331
Iteration 238/1000 | Loss: 0.00004331
Iteration 239/1000 | Loss: 0.00004330
Iteration 240/1000 | Loss: 0.00004330
Iteration 241/1000 | Loss: 0.00004330
Iteration 242/1000 | Loss: 0.00004330
Iteration 243/1000 | Loss: 0.00004330
Iteration 244/1000 | Loss: 0.00004330
Iteration 245/1000 | Loss: 0.00004442
Iteration 246/1000 | Loss: 0.00004328
Iteration 247/1000 | Loss: 0.00004328
Iteration 248/1000 | Loss: 0.00004328
Iteration 249/1000 | Loss: 0.00004328
Iteration 250/1000 | Loss: 0.00004328
Iteration 251/1000 | Loss: 0.00004327
Iteration 252/1000 | Loss: 0.00004327
Iteration 253/1000 | Loss: 0.00004327
Iteration 254/1000 | Loss: 0.00004327
Iteration 255/1000 | Loss: 0.00004327
Iteration 256/1000 | Loss: 0.00004327
Iteration 257/1000 | Loss: 0.00004327
Iteration 258/1000 | Loss: 0.00004327
Iteration 259/1000 | Loss: 0.00004327
Iteration 260/1000 | Loss: 0.00004327
Iteration 261/1000 | Loss: 0.00004327
Iteration 262/1000 | Loss: 0.00004327
Iteration 263/1000 | Loss: 0.00004327
Iteration 264/1000 | Loss: 0.00004327
Iteration 265/1000 | Loss: 0.00004326
Iteration 266/1000 | Loss: 0.00004326
Iteration 267/1000 | Loss: 0.00004326
Iteration 268/1000 | Loss: 0.00004326
Iteration 269/1000 | Loss: 0.00004326
Iteration 270/1000 | Loss: 0.00004326
Iteration 271/1000 | Loss: 0.00004326
Iteration 272/1000 | Loss: 0.00004326
Iteration 273/1000 | Loss: 0.00004326
Iteration 274/1000 | Loss: 0.00004326
Iteration 275/1000 | Loss: 0.00004326
Iteration 276/1000 | Loss: 0.00004326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [4.3261923565296456e-05, 4.3261923565296456e-05, 4.3261923565296456e-05, 4.3261923565296456e-05, 4.3261923565296456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.3261923565296456e-05

Optimization complete. Final v2v error: 5.655707836151123 mm

Highest mean error: 11.93033504486084 mm for frame 97

Lowest mean error: 5.099313259124756 mm for frame 54

Saving results

Total time: 173.63108158111572
