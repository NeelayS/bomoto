Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=25, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1400-1455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592879
Iteration 2/25 | Loss: 0.00148982
Iteration 3/25 | Loss: 0.00138371
Iteration 4/25 | Loss: 0.00125495
Iteration 5/25 | Loss: 0.00125201
Iteration 6/25 | Loss: 0.00124415
Iteration 7/25 | Loss: 0.00124067
Iteration 8/25 | Loss: 0.00123710
Iteration 9/25 | Loss: 0.00123499
Iteration 10/25 | Loss: 0.00123366
Iteration 11/25 | Loss: 0.00123297
Iteration 12/25 | Loss: 0.00123505
Iteration 13/25 | Loss: 0.00123128
Iteration 14/25 | Loss: 0.00123024
Iteration 15/25 | Loss: 0.00122977
Iteration 16/25 | Loss: 0.00122966
Iteration 17/25 | Loss: 0.00122966
Iteration 18/25 | Loss: 0.00122966
Iteration 19/25 | Loss: 0.00122966
Iteration 20/25 | Loss: 0.00122966
Iteration 21/25 | Loss: 0.00122966
Iteration 22/25 | Loss: 0.00122966
Iteration 23/25 | Loss: 0.00122966
Iteration 24/25 | Loss: 0.00122966
Iteration 25/25 | Loss: 0.00122966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.27383423
Iteration 2/25 | Loss: 0.00087130
Iteration 3/25 | Loss: 0.00078377
Iteration 4/25 | Loss: 0.00078377
Iteration 5/25 | Loss: 0.00078377
Iteration 6/25 | Loss: 0.00078377
Iteration 7/25 | Loss: 0.00078377
Iteration 8/25 | Loss: 0.00078377
Iteration 9/25 | Loss: 0.00078377
Iteration 10/25 | Loss: 0.00078377
Iteration 11/25 | Loss: 0.00078377
Iteration 12/25 | Loss: 0.00078377
Iteration 13/25 | Loss: 0.00078377
Iteration 14/25 | Loss: 0.00078377
Iteration 15/25 | Loss: 0.00078377
Iteration 16/25 | Loss: 0.00078377
Iteration 17/25 | Loss: 0.00078377
Iteration 18/25 | Loss: 0.00078377
Iteration 19/25 | Loss: 0.00078377
Iteration 20/25 | Loss: 0.00078377
Iteration 21/25 | Loss: 0.00078377
Iteration 22/25 | Loss: 0.00078377
Iteration 23/25 | Loss: 0.00078377
Iteration 24/25 | Loss: 0.00078377
Iteration 25/25 | Loss: 0.00078377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078377
Iteration 2/1000 | Loss: 0.00002794
Iteration 3/1000 | Loss: 0.00001873
Iteration 4/1000 | Loss: 0.00001651
Iteration 5/1000 | Loss: 0.00001564
Iteration 6/1000 | Loss: 0.00001509
Iteration 7/1000 | Loss: 0.00001469
Iteration 8/1000 | Loss: 0.00001446
Iteration 9/1000 | Loss: 0.00001423
Iteration 10/1000 | Loss: 0.00001418
Iteration 11/1000 | Loss: 0.00001404
Iteration 12/1000 | Loss: 0.00001400
Iteration 13/1000 | Loss: 0.00001398
Iteration 14/1000 | Loss: 0.00001398
Iteration 15/1000 | Loss: 0.00001395
Iteration 16/1000 | Loss: 0.00001392
Iteration 17/1000 | Loss: 0.00001391
Iteration 18/1000 | Loss: 0.00001391
Iteration 19/1000 | Loss: 0.00001391
Iteration 20/1000 | Loss: 0.00001390
Iteration 21/1000 | Loss: 0.00001385
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001385
Iteration 24/1000 | Loss: 0.00001385
Iteration 25/1000 | Loss: 0.00001385
Iteration 26/1000 | Loss: 0.00001385
Iteration 27/1000 | Loss: 0.00001385
Iteration 28/1000 | Loss: 0.00001384
Iteration 29/1000 | Loss: 0.00001384
Iteration 30/1000 | Loss: 0.00001384
Iteration 31/1000 | Loss: 0.00001384
Iteration 32/1000 | Loss: 0.00001382
Iteration 33/1000 | Loss: 0.00001381
Iteration 34/1000 | Loss: 0.00001381
Iteration 35/1000 | Loss: 0.00001380
Iteration 36/1000 | Loss: 0.00001380
Iteration 37/1000 | Loss: 0.00001379
Iteration 38/1000 | Loss: 0.00001379
Iteration 39/1000 | Loss: 0.00001379
Iteration 40/1000 | Loss: 0.00001378
Iteration 41/1000 | Loss: 0.00001378
Iteration 42/1000 | Loss: 0.00001377
Iteration 43/1000 | Loss: 0.00001376
Iteration 44/1000 | Loss: 0.00001376
Iteration 45/1000 | Loss: 0.00001376
Iteration 46/1000 | Loss: 0.00001376
Iteration 47/1000 | Loss: 0.00001375
Iteration 48/1000 | Loss: 0.00001375
Iteration 49/1000 | Loss: 0.00001375
Iteration 50/1000 | Loss: 0.00001374
Iteration 51/1000 | Loss: 0.00001374
Iteration 52/1000 | Loss: 0.00001374
Iteration 53/1000 | Loss: 0.00001373
Iteration 54/1000 | Loss: 0.00001373
Iteration 55/1000 | Loss: 0.00001373
Iteration 56/1000 | Loss: 0.00001373
Iteration 57/1000 | Loss: 0.00001373
Iteration 58/1000 | Loss: 0.00001373
Iteration 59/1000 | Loss: 0.00001373
Iteration 60/1000 | Loss: 0.00001372
Iteration 61/1000 | Loss: 0.00001372
Iteration 62/1000 | Loss: 0.00001372
Iteration 63/1000 | Loss: 0.00001372
Iteration 64/1000 | Loss: 0.00001372
Iteration 65/1000 | Loss: 0.00001372
Iteration 66/1000 | Loss: 0.00001371
Iteration 67/1000 | Loss: 0.00001371
Iteration 68/1000 | Loss: 0.00001371
Iteration 69/1000 | Loss: 0.00001370
Iteration 70/1000 | Loss: 0.00001370
Iteration 71/1000 | Loss: 0.00001370
Iteration 72/1000 | Loss: 0.00001370
Iteration 73/1000 | Loss: 0.00001369
Iteration 74/1000 | Loss: 0.00001369
Iteration 75/1000 | Loss: 0.00001369
Iteration 76/1000 | Loss: 0.00001369
Iteration 77/1000 | Loss: 0.00001369
Iteration 78/1000 | Loss: 0.00001368
Iteration 79/1000 | Loss: 0.00001368
Iteration 80/1000 | Loss: 0.00001368
Iteration 81/1000 | Loss: 0.00001368
Iteration 82/1000 | Loss: 0.00001368
Iteration 83/1000 | Loss: 0.00001367
Iteration 84/1000 | Loss: 0.00001367
Iteration 85/1000 | Loss: 0.00001367
Iteration 86/1000 | Loss: 0.00001367
Iteration 87/1000 | Loss: 0.00001367
Iteration 88/1000 | Loss: 0.00001367
Iteration 89/1000 | Loss: 0.00001367
Iteration 90/1000 | Loss: 0.00001367
Iteration 91/1000 | Loss: 0.00001367
Iteration 92/1000 | Loss: 0.00001366
Iteration 93/1000 | Loss: 0.00001366
Iteration 94/1000 | Loss: 0.00001366
Iteration 95/1000 | Loss: 0.00001366
Iteration 96/1000 | Loss: 0.00001366
Iteration 97/1000 | Loss: 0.00001366
Iteration 98/1000 | Loss: 0.00001366
Iteration 99/1000 | Loss: 0.00001366
Iteration 100/1000 | Loss: 0.00001366
Iteration 101/1000 | Loss: 0.00001366
Iteration 102/1000 | Loss: 0.00001366
Iteration 103/1000 | Loss: 0.00001366
Iteration 104/1000 | Loss: 0.00001365
Iteration 105/1000 | Loss: 0.00001364
Iteration 106/1000 | Loss: 0.00001364
Iteration 107/1000 | Loss: 0.00001364
Iteration 108/1000 | Loss: 0.00001363
Iteration 109/1000 | Loss: 0.00001363
Iteration 110/1000 | Loss: 0.00001362
Iteration 111/1000 | Loss: 0.00001362
Iteration 112/1000 | Loss: 0.00001362
Iteration 113/1000 | Loss: 0.00001362
Iteration 114/1000 | Loss: 0.00001361
Iteration 115/1000 | Loss: 0.00001361
Iteration 116/1000 | Loss: 0.00001361
Iteration 117/1000 | Loss: 0.00001360
Iteration 118/1000 | Loss: 0.00001360
Iteration 119/1000 | Loss: 0.00001360
Iteration 120/1000 | Loss: 0.00001360
Iteration 121/1000 | Loss: 0.00001360
Iteration 122/1000 | Loss: 0.00001360
Iteration 123/1000 | Loss: 0.00001360
Iteration 124/1000 | Loss: 0.00001360
Iteration 125/1000 | Loss: 0.00001359
Iteration 126/1000 | Loss: 0.00001359
Iteration 127/1000 | Loss: 0.00001359
Iteration 128/1000 | Loss: 0.00001359
Iteration 129/1000 | Loss: 0.00001359
Iteration 130/1000 | Loss: 0.00001359
Iteration 131/1000 | Loss: 0.00001359
Iteration 132/1000 | Loss: 0.00001359
Iteration 133/1000 | Loss: 0.00001358
Iteration 134/1000 | Loss: 0.00001358
Iteration 135/1000 | Loss: 0.00001358
Iteration 136/1000 | Loss: 0.00001357
Iteration 137/1000 | Loss: 0.00001357
Iteration 138/1000 | Loss: 0.00001357
Iteration 139/1000 | Loss: 0.00001357
Iteration 140/1000 | Loss: 0.00001357
Iteration 141/1000 | Loss: 0.00001357
Iteration 142/1000 | Loss: 0.00001357
Iteration 143/1000 | Loss: 0.00001357
Iteration 144/1000 | Loss: 0.00001357
Iteration 145/1000 | Loss: 0.00001357
Iteration 146/1000 | Loss: 0.00001357
Iteration 147/1000 | Loss: 0.00001357
Iteration 148/1000 | Loss: 0.00001357
Iteration 149/1000 | Loss: 0.00001357
Iteration 150/1000 | Loss: 0.00001357
Iteration 151/1000 | Loss: 0.00001357
Iteration 152/1000 | Loss: 0.00001357
Iteration 153/1000 | Loss: 0.00001357
Iteration 154/1000 | Loss: 0.00001357
Iteration 155/1000 | Loss: 0.00001357
Iteration 156/1000 | Loss: 0.00001357
Iteration 157/1000 | Loss: 0.00001357
Iteration 158/1000 | Loss: 0.00001357
Iteration 159/1000 | Loss: 0.00001357
Iteration 160/1000 | Loss: 0.00001357
Iteration 161/1000 | Loss: 0.00001357
Iteration 162/1000 | Loss: 0.00001357
Iteration 163/1000 | Loss: 0.00001357
Iteration 164/1000 | Loss: 0.00001357
Iteration 165/1000 | Loss: 0.00001357
Iteration 166/1000 | Loss: 0.00001357
Iteration 167/1000 | Loss: 0.00001357
Iteration 168/1000 | Loss: 0.00001357
Iteration 169/1000 | Loss: 0.00001357
Iteration 170/1000 | Loss: 0.00001357
Iteration 171/1000 | Loss: 0.00001357
Iteration 172/1000 | Loss: 0.00001357
Iteration 173/1000 | Loss: 0.00001357
Iteration 174/1000 | Loss: 0.00001357
Iteration 175/1000 | Loss: 0.00001357
Iteration 176/1000 | Loss: 0.00001357
Iteration 177/1000 | Loss: 0.00001357
Iteration 178/1000 | Loss: 0.00001357
Iteration 179/1000 | Loss: 0.00001357
Iteration 180/1000 | Loss: 0.00001357
Iteration 181/1000 | Loss: 0.00001357
Iteration 182/1000 | Loss: 0.00001357
Iteration 183/1000 | Loss: 0.00001357
Iteration 184/1000 | Loss: 0.00001357
Iteration 185/1000 | Loss: 0.00001357
Iteration 186/1000 | Loss: 0.00001357
Iteration 187/1000 | Loss: 0.00001357
Iteration 188/1000 | Loss: 0.00001357
Iteration 189/1000 | Loss: 0.00001357
Iteration 190/1000 | Loss: 0.00001357
Iteration 191/1000 | Loss: 0.00001357
Iteration 192/1000 | Loss: 0.00001357
Iteration 193/1000 | Loss: 0.00001357
Iteration 194/1000 | Loss: 0.00001357
Iteration 195/1000 | Loss: 0.00001357
Iteration 196/1000 | Loss: 0.00001357
Iteration 197/1000 | Loss: 0.00001357
Iteration 198/1000 | Loss: 0.00001357
Iteration 199/1000 | Loss: 0.00001357
Iteration 200/1000 | Loss: 0.00001357
Iteration 201/1000 | Loss: 0.00001357
Iteration 202/1000 | Loss: 0.00001357
Iteration 203/1000 | Loss: 0.00001357
Iteration 204/1000 | Loss: 0.00001357
Iteration 205/1000 | Loss: 0.00001357
Iteration 206/1000 | Loss: 0.00001357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.357022028969368e-05, 1.357022028969368e-05, 1.357022028969368e-05, 1.357022028969368e-05, 1.357022028969368e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.357022028969368e-05

Optimization complete. Final v2v error: 3.117365598678589 mm

Highest mean error: 3.506077766418457 mm for frame 66

Lowest mean error: 2.8391661643981934 mm for frame 128

Saving results

Total time: 64.6602680683136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020477
Iteration 2/25 | Loss: 0.00169085
Iteration 3/25 | Loss: 0.00141935
Iteration 4/25 | Loss: 0.00139475
Iteration 5/25 | Loss: 0.00138617
Iteration 6/25 | Loss: 0.00138436
Iteration 7/25 | Loss: 0.00138436
Iteration 8/25 | Loss: 0.00138431
Iteration 9/25 | Loss: 0.00138431
Iteration 10/25 | Loss: 0.00138431
Iteration 11/25 | Loss: 0.00138431
Iteration 12/25 | Loss: 0.00138431
Iteration 13/25 | Loss: 0.00138431
Iteration 14/25 | Loss: 0.00138431
Iteration 15/25 | Loss: 0.00138431
Iteration 16/25 | Loss: 0.00138431
Iteration 17/25 | Loss: 0.00138431
Iteration 18/25 | Loss: 0.00138431
Iteration 19/25 | Loss: 0.00138431
Iteration 20/25 | Loss: 0.00138431
Iteration 21/25 | Loss: 0.00138431
Iteration 22/25 | Loss: 0.00138431
Iteration 23/25 | Loss: 0.00138431
Iteration 24/25 | Loss: 0.00138431
Iteration 25/25 | Loss: 0.00138431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94083291
Iteration 2/25 | Loss: 0.00103773
Iteration 3/25 | Loss: 0.00103770
Iteration 4/25 | Loss: 0.00103770
Iteration 5/25 | Loss: 0.00103770
Iteration 6/25 | Loss: 0.00103770
Iteration 7/25 | Loss: 0.00103770
Iteration 8/25 | Loss: 0.00103770
Iteration 9/25 | Loss: 0.00103770
Iteration 10/25 | Loss: 0.00103770
Iteration 11/25 | Loss: 0.00103770
Iteration 12/25 | Loss: 0.00103770
Iteration 13/25 | Loss: 0.00103770
Iteration 14/25 | Loss: 0.00103770
Iteration 15/25 | Loss: 0.00103770
Iteration 16/25 | Loss: 0.00103770
Iteration 17/25 | Loss: 0.00103770
Iteration 18/25 | Loss: 0.00103770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001037701265886426, 0.001037701265886426, 0.001037701265886426, 0.001037701265886426, 0.001037701265886426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001037701265886426

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103770
Iteration 2/1000 | Loss: 0.00006224
Iteration 3/1000 | Loss: 0.00003988
Iteration 4/1000 | Loss: 0.00003357
Iteration 5/1000 | Loss: 0.00003165
Iteration 6/1000 | Loss: 0.00003056
Iteration 7/1000 | Loss: 0.00002965
Iteration 8/1000 | Loss: 0.00002891
Iteration 9/1000 | Loss: 0.00002849
Iteration 10/1000 | Loss: 0.00002800
Iteration 11/1000 | Loss: 0.00002772
Iteration 12/1000 | Loss: 0.00002747
Iteration 13/1000 | Loss: 0.00002725
Iteration 14/1000 | Loss: 0.00002703
Iteration 15/1000 | Loss: 0.00002691
Iteration 16/1000 | Loss: 0.00002683
Iteration 17/1000 | Loss: 0.00002681
Iteration 18/1000 | Loss: 0.00002680
Iteration 19/1000 | Loss: 0.00002680
Iteration 20/1000 | Loss: 0.00002679
Iteration 21/1000 | Loss: 0.00002668
Iteration 22/1000 | Loss: 0.00002662
Iteration 23/1000 | Loss: 0.00002661
Iteration 24/1000 | Loss: 0.00002658
Iteration 25/1000 | Loss: 0.00002657
Iteration 26/1000 | Loss: 0.00002657
Iteration 27/1000 | Loss: 0.00002657
Iteration 28/1000 | Loss: 0.00002655
Iteration 29/1000 | Loss: 0.00002654
Iteration 30/1000 | Loss: 0.00002653
Iteration 31/1000 | Loss: 0.00002652
Iteration 32/1000 | Loss: 0.00002652
Iteration 33/1000 | Loss: 0.00002650
Iteration 34/1000 | Loss: 0.00002648
Iteration 35/1000 | Loss: 0.00002648
Iteration 36/1000 | Loss: 0.00002645
Iteration 37/1000 | Loss: 0.00002644
Iteration 38/1000 | Loss: 0.00002639
Iteration 39/1000 | Loss: 0.00002639
Iteration 40/1000 | Loss: 0.00002636
Iteration 41/1000 | Loss: 0.00002636
Iteration 42/1000 | Loss: 0.00002636
Iteration 43/1000 | Loss: 0.00002635
Iteration 44/1000 | Loss: 0.00002635
Iteration 45/1000 | Loss: 0.00002635
Iteration 46/1000 | Loss: 0.00002634
Iteration 47/1000 | Loss: 0.00002633
Iteration 48/1000 | Loss: 0.00002632
Iteration 49/1000 | Loss: 0.00002632
Iteration 50/1000 | Loss: 0.00002632
Iteration 51/1000 | Loss: 0.00002632
Iteration 52/1000 | Loss: 0.00002632
Iteration 53/1000 | Loss: 0.00002632
Iteration 54/1000 | Loss: 0.00002632
Iteration 55/1000 | Loss: 0.00002632
Iteration 56/1000 | Loss: 0.00002632
Iteration 57/1000 | Loss: 0.00002632
Iteration 58/1000 | Loss: 0.00002632
Iteration 59/1000 | Loss: 0.00002632
Iteration 60/1000 | Loss: 0.00002631
Iteration 61/1000 | Loss: 0.00002631
Iteration 62/1000 | Loss: 0.00002631
Iteration 63/1000 | Loss: 0.00002631
Iteration 64/1000 | Loss: 0.00002631
Iteration 65/1000 | Loss: 0.00002631
Iteration 66/1000 | Loss: 0.00002631
Iteration 67/1000 | Loss: 0.00002631
Iteration 68/1000 | Loss: 0.00002631
Iteration 69/1000 | Loss: 0.00002631
Iteration 70/1000 | Loss: 0.00002630
Iteration 71/1000 | Loss: 0.00002630
Iteration 72/1000 | Loss: 0.00002630
Iteration 73/1000 | Loss: 0.00002630
Iteration 74/1000 | Loss: 0.00002630
Iteration 75/1000 | Loss: 0.00002629
Iteration 76/1000 | Loss: 0.00002628
Iteration 77/1000 | Loss: 0.00002628
Iteration 78/1000 | Loss: 0.00002627
Iteration 79/1000 | Loss: 0.00002626
Iteration 80/1000 | Loss: 0.00002626
Iteration 81/1000 | Loss: 0.00002625
Iteration 82/1000 | Loss: 0.00002625
Iteration 83/1000 | Loss: 0.00002625
Iteration 84/1000 | Loss: 0.00002624
Iteration 85/1000 | Loss: 0.00002624
Iteration 86/1000 | Loss: 0.00002623
Iteration 87/1000 | Loss: 0.00002623
Iteration 88/1000 | Loss: 0.00002622
Iteration 89/1000 | Loss: 0.00002622
Iteration 90/1000 | Loss: 0.00002622
Iteration 91/1000 | Loss: 0.00002622
Iteration 92/1000 | Loss: 0.00002622
Iteration 93/1000 | Loss: 0.00002621
Iteration 94/1000 | Loss: 0.00002621
Iteration 95/1000 | Loss: 0.00002621
Iteration 96/1000 | Loss: 0.00002620
Iteration 97/1000 | Loss: 0.00002620
Iteration 98/1000 | Loss: 0.00002620
Iteration 99/1000 | Loss: 0.00002620
Iteration 100/1000 | Loss: 0.00002620
Iteration 101/1000 | Loss: 0.00002620
Iteration 102/1000 | Loss: 0.00002620
Iteration 103/1000 | Loss: 0.00002620
Iteration 104/1000 | Loss: 0.00002620
Iteration 105/1000 | Loss: 0.00002620
Iteration 106/1000 | Loss: 0.00002620
Iteration 107/1000 | Loss: 0.00002620
Iteration 108/1000 | Loss: 0.00002619
Iteration 109/1000 | Loss: 0.00002619
Iteration 110/1000 | Loss: 0.00002619
Iteration 111/1000 | Loss: 0.00002619
Iteration 112/1000 | Loss: 0.00002619
Iteration 113/1000 | Loss: 0.00002618
Iteration 114/1000 | Loss: 0.00002618
Iteration 115/1000 | Loss: 0.00002618
Iteration 116/1000 | Loss: 0.00002618
Iteration 117/1000 | Loss: 0.00002618
Iteration 118/1000 | Loss: 0.00002618
Iteration 119/1000 | Loss: 0.00002618
Iteration 120/1000 | Loss: 0.00002618
Iteration 121/1000 | Loss: 0.00002618
Iteration 122/1000 | Loss: 0.00002618
Iteration 123/1000 | Loss: 0.00002618
Iteration 124/1000 | Loss: 0.00002617
Iteration 125/1000 | Loss: 0.00002617
Iteration 126/1000 | Loss: 0.00002617
Iteration 127/1000 | Loss: 0.00002617
Iteration 128/1000 | Loss: 0.00002617
Iteration 129/1000 | Loss: 0.00002617
Iteration 130/1000 | Loss: 0.00002617
Iteration 131/1000 | Loss: 0.00002616
Iteration 132/1000 | Loss: 0.00002616
Iteration 133/1000 | Loss: 0.00002616
Iteration 134/1000 | Loss: 0.00002616
Iteration 135/1000 | Loss: 0.00002616
Iteration 136/1000 | Loss: 0.00002616
Iteration 137/1000 | Loss: 0.00002616
Iteration 138/1000 | Loss: 0.00002616
Iteration 139/1000 | Loss: 0.00002616
Iteration 140/1000 | Loss: 0.00002616
Iteration 141/1000 | Loss: 0.00002615
Iteration 142/1000 | Loss: 0.00002615
Iteration 143/1000 | Loss: 0.00002615
Iteration 144/1000 | Loss: 0.00002615
Iteration 145/1000 | Loss: 0.00002615
Iteration 146/1000 | Loss: 0.00002615
Iteration 147/1000 | Loss: 0.00002615
Iteration 148/1000 | Loss: 0.00002615
Iteration 149/1000 | Loss: 0.00002615
Iteration 150/1000 | Loss: 0.00002615
Iteration 151/1000 | Loss: 0.00002615
Iteration 152/1000 | Loss: 0.00002615
Iteration 153/1000 | Loss: 0.00002614
Iteration 154/1000 | Loss: 0.00002614
Iteration 155/1000 | Loss: 0.00002614
Iteration 156/1000 | Loss: 0.00002614
Iteration 157/1000 | Loss: 0.00002614
Iteration 158/1000 | Loss: 0.00002614
Iteration 159/1000 | Loss: 0.00002614
Iteration 160/1000 | Loss: 0.00002614
Iteration 161/1000 | Loss: 0.00002614
Iteration 162/1000 | Loss: 0.00002614
Iteration 163/1000 | Loss: 0.00002614
Iteration 164/1000 | Loss: 0.00002614
Iteration 165/1000 | Loss: 0.00002614
Iteration 166/1000 | Loss: 0.00002614
Iteration 167/1000 | Loss: 0.00002614
Iteration 168/1000 | Loss: 0.00002614
Iteration 169/1000 | Loss: 0.00002614
Iteration 170/1000 | Loss: 0.00002614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.61430031969212e-05, 2.61430031969212e-05, 2.61430031969212e-05, 2.61430031969212e-05, 2.61430031969212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.61430031969212e-05

Optimization complete. Final v2v error: 4.274460792541504 mm

Highest mean error: 5.087158203125 mm for frame 65

Lowest mean error: 3.671281337738037 mm for frame 28

Saving results

Total time: 49.126898527145386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00931373
Iteration 2/25 | Loss: 0.00279801
Iteration 3/25 | Loss: 0.00203147
Iteration 4/25 | Loss: 0.00193827
Iteration 5/25 | Loss: 0.00172238
Iteration 6/25 | Loss: 0.00167910
Iteration 7/25 | Loss: 0.00152438
Iteration 8/25 | Loss: 0.00147441
Iteration 9/25 | Loss: 0.00145243
Iteration 10/25 | Loss: 0.00145894
Iteration 11/25 | Loss: 0.00143331
Iteration 12/25 | Loss: 0.00143075
Iteration 13/25 | Loss: 0.00143168
Iteration 14/25 | Loss: 0.00142943
Iteration 15/25 | Loss: 0.00142695
Iteration 16/25 | Loss: 0.00142708
Iteration 17/25 | Loss: 0.00142470
Iteration 18/25 | Loss: 0.00143354
Iteration 19/25 | Loss: 0.00143079
Iteration 20/25 | Loss: 0.00142738
Iteration 21/25 | Loss: 0.00141922
Iteration 22/25 | Loss: 0.00141344
Iteration 23/25 | Loss: 0.00141294
Iteration 24/25 | Loss: 0.00141276
Iteration 25/25 | Loss: 0.00141274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43059301
Iteration 2/25 | Loss: 0.00083424
Iteration 3/25 | Loss: 0.00083423
Iteration 4/25 | Loss: 0.00083423
Iteration 5/25 | Loss: 0.00083423
Iteration 6/25 | Loss: 0.00083423
Iteration 7/25 | Loss: 0.00083423
Iteration 8/25 | Loss: 0.00083423
Iteration 9/25 | Loss: 0.00083423
Iteration 10/25 | Loss: 0.00083423
Iteration 11/25 | Loss: 0.00083423
Iteration 12/25 | Loss: 0.00083423
Iteration 13/25 | Loss: 0.00083423
Iteration 14/25 | Loss: 0.00083423
Iteration 15/25 | Loss: 0.00083423
Iteration 16/25 | Loss: 0.00083423
Iteration 17/25 | Loss: 0.00083423
Iteration 18/25 | Loss: 0.00083423
Iteration 19/25 | Loss: 0.00083423
Iteration 20/25 | Loss: 0.00083423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008342291112057865, 0.0008342291112057865, 0.0008342291112057865, 0.0008342291112057865, 0.0008342291112057865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008342291112057865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083423
Iteration 2/1000 | Loss: 0.00653755
Iteration 3/1000 | Loss: 0.00303621
Iteration 4/1000 | Loss: 0.00723083
Iteration 5/1000 | Loss: 0.00139353
Iteration 6/1000 | Loss: 0.00103606
Iteration 7/1000 | Loss: 0.00074468
Iteration 8/1000 | Loss: 0.00053693
Iteration 9/1000 | Loss: 0.00028077
Iteration 10/1000 | Loss: 0.00055462
Iteration 11/1000 | Loss: 0.00058235
Iteration 12/1000 | Loss: 0.00026417
Iteration 13/1000 | Loss: 0.00013166
Iteration 14/1000 | Loss: 0.00118545
Iteration 15/1000 | Loss: 0.00052492
Iteration 16/1000 | Loss: 0.00114611
Iteration 17/1000 | Loss: 0.00070493
Iteration 18/1000 | Loss: 0.00074228
Iteration 19/1000 | Loss: 0.00072450
Iteration 20/1000 | Loss: 0.00090524
Iteration 21/1000 | Loss: 0.00083144
Iteration 22/1000 | Loss: 0.00041313
Iteration 23/1000 | Loss: 0.00020595
Iteration 24/1000 | Loss: 0.00022905
Iteration 25/1000 | Loss: 0.00019041
Iteration 26/1000 | Loss: 0.00015643
Iteration 27/1000 | Loss: 0.00017691
Iteration 28/1000 | Loss: 0.00043592
Iteration 29/1000 | Loss: 0.00033835
Iteration 30/1000 | Loss: 0.00013710
Iteration 31/1000 | Loss: 0.00012521
Iteration 32/1000 | Loss: 0.00126430
Iteration 33/1000 | Loss: 0.00078909
Iteration 34/1000 | Loss: 0.00148924
Iteration 35/1000 | Loss: 0.00378199
Iteration 36/1000 | Loss: 0.00111537
Iteration 37/1000 | Loss: 0.00039471
Iteration 38/1000 | Loss: 0.00013397
Iteration 39/1000 | Loss: 0.00013088
Iteration 40/1000 | Loss: 0.00032849
Iteration 41/1000 | Loss: 0.00097689
Iteration 42/1000 | Loss: 0.00012328
Iteration 43/1000 | Loss: 0.00035487
Iteration 44/1000 | Loss: 0.00012332
Iteration 45/1000 | Loss: 0.00013134
Iteration 46/1000 | Loss: 0.00011688
Iteration 47/1000 | Loss: 0.00057036
Iteration 48/1000 | Loss: 0.00042769
Iteration 49/1000 | Loss: 0.00046052
Iteration 50/1000 | Loss: 0.00028345
Iteration 51/1000 | Loss: 0.00056065
Iteration 52/1000 | Loss: 0.00010128
Iteration 53/1000 | Loss: 0.00018710
Iteration 54/1000 | Loss: 0.00032394
Iteration 55/1000 | Loss: 0.00016959
Iteration 56/1000 | Loss: 0.00076387
Iteration 57/1000 | Loss: 0.00033990
Iteration 58/1000 | Loss: 0.00027383
Iteration 59/1000 | Loss: 0.00032841
Iteration 60/1000 | Loss: 0.00031314
Iteration 61/1000 | Loss: 0.00055220
Iteration 62/1000 | Loss: 0.00035370
Iteration 63/1000 | Loss: 0.00019892
Iteration 64/1000 | Loss: 0.00040781
Iteration 65/1000 | Loss: 0.00009662
Iteration 66/1000 | Loss: 0.00094598
Iteration 67/1000 | Loss: 0.00010568
Iteration 68/1000 | Loss: 0.00009550
Iteration 69/1000 | Loss: 0.00038791
Iteration 70/1000 | Loss: 0.00068755
Iteration 71/1000 | Loss: 0.00014206
Iteration 72/1000 | Loss: 0.00012381
Iteration 73/1000 | Loss: 0.00011725
Iteration 74/1000 | Loss: 0.00010657
Iteration 75/1000 | Loss: 0.00010280
Iteration 76/1000 | Loss: 0.00007598
Iteration 77/1000 | Loss: 0.00010302
Iteration 78/1000 | Loss: 0.00008377
Iteration 79/1000 | Loss: 0.00007646
Iteration 80/1000 | Loss: 0.00009069
Iteration 81/1000 | Loss: 0.00008143
Iteration 82/1000 | Loss: 0.00005894
Iteration 83/1000 | Loss: 0.00005315
Iteration 84/1000 | Loss: 0.00004984
Iteration 85/1000 | Loss: 0.00004741
Iteration 86/1000 | Loss: 0.00004573
Iteration 87/1000 | Loss: 0.00036173
Iteration 88/1000 | Loss: 0.00005313
Iteration 89/1000 | Loss: 0.00004898
Iteration 90/1000 | Loss: 0.00007157
Iteration 91/1000 | Loss: 0.00006485
Iteration 92/1000 | Loss: 0.00004153
Iteration 93/1000 | Loss: 0.00004103
Iteration 94/1000 | Loss: 0.00005879
Iteration 95/1000 | Loss: 0.00005903
Iteration 96/1000 | Loss: 0.00004007
Iteration 97/1000 | Loss: 0.00003949
Iteration 98/1000 | Loss: 0.00003900
Iteration 99/1000 | Loss: 0.00003865
Iteration 100/1000 | Loss: 0.00003842
Iteration 101/1000 | Loss: 0.00003808
Iteration 102/1000 | Loss: 0.00003781
Iteration 103/1000 | Loss: 0.00003764
Iteration 104/1000 | Loss: 0.00006519
Iteration 105/1000 | Loss: 0.00005991
Iteration 106/1000 | Loss: 0.00003838
Iteration 107/1000 | Loss: 0.00003773
Iteration 108/1000 | Loss: 0.00005600
Iteration 109/1000 | Loss: 0.00005098
Iteration 110/1000 | Loss: 0.00005819
Iteration 111/1000 | Loss: 0.00006228
Iteration 112/1000 | Loss: 0.00005683
Iteration 113/1000 | Loss: 0.00006142
Iteration 114/1000 | Loss: 0.00003983
Iteration 115/1000 | Loss: 0.00003840
Iteration 116/1000 | Loss: 0.00003762
Iteration 117/1000 | Loss: 0.00005868
Iteration 118/1000 | Loss: 0.00003846
Iteration 119/1000 | Loss: 0.00003818
Iteration 120/1000 | Loss: 0.00003793
Iteration 121/1000 | Loss: 0.00003775
Iteration 122/1000 | Loss: 0.00003772
Iteration 123/1000 | Loss: 0.00003772
Iteration 124/1000 | Loss: 0.00003763
Iteration 125/1000 | Loss: 0.00003762
Iteration 126/1000 | Loss: 0.00003762
Iteration 127/1000 | Loss: 0.00003761
Iteration 128/1000 | Loss: 0.00003758
Iteration 129/1000 | Loss: 0.00003758
Iteration 130/1000 | Loss: 0.00003758
Iteration 131/1000 | Loss: 0.00003758
Iteration 132/1000 | Loss: 0.00003758
Iteration 133/1000 | Loss: 0.00003758
Iteration 134/1000 | Loss: 0.00003758
Iteration 135/1000 | Loss: 0.00003758
Iteration 136/1000 | Loss: 0.00003757
Iteration 137/1000 | Loss: 0.00003757
Iteration 138/1000 | Loss: 0.00003757
Iteration 139/1000 | Loss: 0.00003757
Iteration 140/1000 | Loss: 0.00003757
Iteration 141/1000 | Loss: 0.00003757
Iteration 142/1000 | Loss: 0.00003757
Iteration 143/1000 | Loss: 0.00003757
Iteration 144/1000 | Loss: 0.00003756
Iteration 145/1000 | Loss: 0.00003755
Iteration 146/1000 | Loss: 0.00003755
Iteration 147/1000 | Loss: 0.00003755
Iteration 148/1000 | Loss: 0.00003754
Iteration 149/1000 | Loss: 0.00003754
Iteration 150/1000 | Loss: 0.00003754
Iteration 151/1000 | Loss: 0.00003754
Iteration 152/1000 | Loss: 0.00003754
Iteration 153/1000 | Loss: 0.00003754
Iteration 154/1000 | Loss: 0.00003754
Iteration 155/1000 | Loss: 0.00003754
Iteration 156/1000 | Loss: 0.00003753
Iteration 157/1000 | Loss: 0.00003753
Iteration 158/1000 | Loss: 0.00003753
Iteration 159/1000 | Loss: 0.00003753
Iteration 160/1000 | Loss: 0.00005286
Iteration 161/1000 | Loss: 0.00004118
Iteration 162/1000 | Loss: 0.00003904
Iteration 163/1000 | Loss: 0.00004224
Iteration 164/1000 | Loss: 0.00004592
Iteration 165/1000 | Loss: 0.00003980
Iteration 166/1000 | Loss: 0.00004629
Iteration 167/1000 | Loss: 0.00003781
Iteration 168/1000 | Loss: 0.00003738
Iteration 169/1000 | Loss: 0.00003707
Iteration 170/1000 | Loss: 0.00003662
Iteration 171/1000 | Loss: 0.00003630
Iteration 172/1000 | Loss: 0.00003601
Iteration 173/1000 | Loss: 0.00003583
Iteration 174/1000 | Loss: 0.00003566
Iteration 175/1000 | Loss: 0.00003542
Iteration 176/1000 | Loss: 0.00003518
Iteration 177/1000 | Loss: 0.00003501
Iteration 178/1000 | Loss: 0.00003494
Iteration 179/1000 | Loss: 0.00003493
Iteration 180/1000 | Loss: 0.00003493
Iteration 181/1000 | Loss: 0.00003492
Iteration 182/1000 | Loss: 0.00003492
Iteration 183/1000 | Loss: 0.00003491
Iteration 184/1000 | Loss: 0.00003491
Iteration 185/1000 | Loss: 0.00003491
Iteration 186/1000 | Loss: 0.00003490
Iteration 187/1000 | Loss: 0.00003490
Iteration 188/1000 | Loss: 0.00003490
Iteration 189/1000 | Loss: 0.00003490
Iteration 190/1000 | Loss: 0.00003490
Iteration 191/1000 | Loss: 0.00003489
Iteration 192/1000 | Loss: 0.00003489
Iteration 193/1000 | Loss: 0.00003488
Iteration 194/1000 | Loss: 0.00003488
Iteration 195/1000 | Loss: 0.00003488
Iteration 196/1000 | Loss: 0.00003488
Iteration 197/1000 | Loss: 0.00003487
Iteration 198/1000 | Loss: 0.00003487
Iteration 199/1000 | Loss: 0.00003487
Iteration 200/1000 | Loss: 0.00003487
Iteration 201/1000 | Loss: 0.00003486
Iteration 202/1000 | Loss: 0.00003486
Iteration 203/1000 | Loss: 0.00003486
Iteration 204/1000 | Loss: 0.00003485
Iteration 205/1000 | Loss: 0.00003485
Iteration 206/1000 | Loss: 0.00003485
Iteration 207/1000 | Loss: 0.00003485
Iteration 208/1000 | Loss: 0.00003485
Iteration 209/1000 | Loss: 0.00003485
Iteration 210/1000 | Loss: 0.00003485
Iteration 211/1000 | Loss: 0.00003485
Iteration 212/1000 | Loss: 0.00003485
Iteration 213/1000 | Loss: 0.00003485
Iteration 214/1000 | Loss: 0.00003485
Iteration 215/1000 | Loss: 0.00003484
Iteration 216/1000 | Loss: 0.00003484
Iteration 217/1000 | Loss: 0.00003484
Iteration 218/1000 | Loss: 0.00003484
Iteration 219/1000 | Loss: 0.00003484
Iteration 220/1000 | Loss: 0.00003484
Iteration 221/1000 | Loss: 0.00003484
Iteration 222/1000 | Loss: 0.00003484
Iteration 223/1000 | Loss: 0.00003484
Iteration 224/1000 | Loss: 0.00003484
Iteration 225/1000 | Loss: 0.00003484
Iteration 226/1000 | Loss: 0.00003484
Iteration 227/1000 | Loss: 0.00003484
Iteration 228/1000 | Loss: 0.00003484
Iteration 229/1000 | Loss: 0.00003484
Iteration 230/1000 | Loss: 0.00003484
Iteration 231/1000 | Loss: 0.00003484
Iteration 232/1000 | Loss: 0.00003484
Iteration 233/1000 | Loss: 0.00003484
Iteration 234/1000 | Loss: 0.00003484
Iteration 235/1000 | Loss: 0.00003484
Iteration 236/1000 | Loss: 0.00003484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [3.484319677227177e-05, 3.484319677227177e-05, 3.484319677227177e-05, 3.484319677227177e-05, 3.484319677227177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.484319677227177e-05

Optimization complete. Final v2v error: 4.559760570526123 mm

Highest mean error: 7.044619560241699 mm for frame 69

Lowest mean error: 3.7229130268096924 mm for frame 124

Saving results

Total time: 232.2410740852356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797541
Iteration 2/25 | Loss: 0.00144469
Iteration 3/25 | Loss: 0.00127040
Iteration 4/25 | Loss: 0.00124790
Iteration 5/25 | Loss: 0.00123956
Iteration 6/25 | Loss: 0.00123773
Iteration 7/25 | Loss: 0.00123991
Iteration 8/25 | Loss: 0.00123432
Iteration 9/25 | Loss: 0.00122946
Iteration 10/25 | Loss: 0.00122892
Iteration 11/25 | Loss: 0.00122883
Iteration 12/25 | Loss: 0.00122883
Iteration 13/25 | Loss: 0.00122883
Iteration 14/25 | Loss: 0.00122883
Iteration 15/25 | Loss: 0.00122883
Iteration 16/25 | Loss: 0.00122883
Iteration 17/25 | Loss: 0.00122883
Iteration 18/25 | Loss: 0.00122883
Iteration 19/25 | Loss: 0.00122883
Iteration 20/25 | Loss: 0.00122883
Iteration 21/25 | Loss: 0.00122883
Iteration 22/25 | Loss: 0.00122883
Iteration 23/25 | Loss: 0.00122883
Iteration 24/25 | Loss: 0.00122883
Iteration 25/25 | Loss: 0.00122883

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60317731
Iteration 2/25 | Loss: 0.00082219
Iteration 3/25 | Loss: 0.00081816
Iteration 4/25 | Loss: 0.00081816
Iteration 5/25 | Loss: 0.00081816
Iteration 6/25 | Loss: 0.00081816
Iteration 7/25 | Loss: 0.00081816
Iteration 8/25 | Loss: 0.00081816
Iteration 9/25 | Loss: 0.00081816
Iteration 10/25 | Loss: 0.00081816
Iteration 11/25 | Loss: 0.00081816
Iteration 12/25 | Loss: 0.00081816
Iteration 13/25 | Loss: 0.00081816
Iteration 14/25 | Loss: 0.00081816
Iteration 15/25 | Loss: 0.00081816
Iteration 16/25 | Loss: 0.00081816
Iteration 17/25 | Loss: 0.00081816
Iteration 18/25 | Loss: 0.00081816
Iteration 19/25 | Loss: 0.00081816
Iteration 20/25 | Loss: 0.00081816
Iteration 21/25 | Loss: 0.00081816
Iteration 22/25 | Loss: 0.00081816
Iteration 23/25 | Loss: 0.00081816
Iteration 24/25 | Loss: 0.00081816
Iteration 25/25 | Loss: 0.00081816

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081816
Iteration 2/1000 | Loss: 0.00002584
Iteration 3/1000 | Loss: 0.00001979
Iteration 4/1000 | Loss: 0.00001789
Iteration 5/1000 | Loss: 0.00001658
Iteration 6/1000 | Loss: 0.00001579
Iteration 7/1000 | Loss: 0.00001529
Iteration 8/1000 | Loss: 0.00001491
Iteration 9/1000 | Loss: 0.00001455
Iteration 10/1000 | Loss: 0.00001433
Iteration 11/1000 | Loss: 0.00001477
Iteration 12/1000 | Loss: 0.00001437
Iteration 13/1000 | Loss: 0.00001411
Iteration 14/1000 | Loss: 0.00001410
Iteration 15/1000 | Loss: 0.00001410
Iteration 16/1000 | Loss: 0.00001410
Iteration 17/1000 | Loss: 0.00001409
Iteration 18/1000 | Loss: 0.00001408
Iteration 19/1000 | Loss: 0.00001407
Iteration 20/1000 | Loss: 0.00001446
Iteration 21/1000 | Loss: 0.00001400
Iteration 22/1000 | Loss: 0.00001400
Iteration 23/1000 | Loss: 0.00001400
Iteration 24/1000 | Loss: 0.00001399
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001399
Iteration 27/1000 | Loss: 0.00001399
Iteration 28/1000 | Loss: 0.00001399
Iteration 29/1000 | Loss: 0.00001399
Iteration 30/1000 | Loss: 0.00001399
Iteration 31/1000 | Loss: 0.00001399
Iteration 32/1000 | Loss: 0.00001399
Iteration 33/1000 | Loss: 0.00001399
Iteration 34/1000 | Loss: 0.00001399
Iteration 35/1000 | Loss: 0.00001398
Iteration 36/1000 | Loss: 0.00001398
Iteration 37/1000 | Loss: 0.00001398
Iteration 38/1000 | Loss: 0.00001397
Iteration 39/1000 | Loss: 0.00001441
Iteration 40/1000 | Loss: 0.00001441
Iteration 41/1000 | Loss: 0.00001391
Iteration 42/1000 | Loss: 0.00001391
Iteration 43/1000 | Loss: 0.00001391
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001391
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001390
Iteration 48/1000 | Loss: 0.00001390
Iteration 49/1000 | Loss: 0.00001390
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001389
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001388
Iteration 55/1000 | Loss: 0.00001388
Iteration 56/1000 | Loss: 0.00001401
Iteration 57/1000 | Loss: 0.00001856
Iteration 58/1000 | Loss: 0.00001409
Iteration 59/1000 | Loss: 0.00001376
Iteration 60/1000 | Loss: 0.00001376
Iteration 61/1000 | Loss: 0.00001376
Iteration 62/1000 | Loss: 0.00001375
Iteration 63/1000 | Loss: 0.00001374
Iteration 64/1000 | Loss: 0.00001374
Iteration 65/1000 | Loss: 0.00001374
Iteration 66/1000 | Loss: 0.00001374
Iteration 67/1000 | Loss: 0.00001374
Iteration 68/1000 | Loss: 0.00001373
Iteration 69/1000 | Loss: 0.00001373
Iteration 70/1000 | Loss: 0.00001373
Iteration 71/1000 | Loss: 0.00001373
Iteration 72/1000 | Loss: 0.00001373
Iteration 73/1000 | Loss: 0.00001372
Iteration 74/1000 | Loss: 0.00001372
Iteration 75/1000 | Loss: 0.00001372
Iteration 76/1000 | Loss: 0.00001372
Iteration 77/1000 | Loss: 0.00001371
Iteration 78/1000 | Loss: 0.00001371
Iteration 79/1000 | Loss: 0.00001371
Iteration 80/1000 | Loss: 0.00001371
Iteration 81/1000 | Loss: 0.00001370
Iteration 82/1000 | Loss: 0.00001369
Iteration 83/1000 | Loss: 0.00001369
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001369
Iteration 87/1000 | Loss: 0.00001369
Iteration 88/1000 | Loss: 0.00001369
Iteration 89/1000 | Loss: 0.00001369
Iteration 90/1000 | Loss: 0.00001369
Iteration 91/1000 | Loss: 0.00001369
Iteration 92/1000 | Loss: 0.00001369
Iteration 93/1000 | Loss: 0.00001369
Iteration 94/1000 | Loss: 0.00001368
Iteration 95/1000 | Loss: 0.00001368
Iteration 96/1000 | Loss: 0.00001368
Iteration 97/1000 | Loss: 0.00001368
Iteration 98/1000 | Loss: 0.00001368
Iteration 99/1000 | Loss: 0.00001368
Iteration 100/1000 | Loss: 0.00001368
Iteration 101/1000 | Loss: 0.00001368
Iteration 102/1000 | Loss: 0.00001368
Iteration 103/1000 | Loss: 0.00001368
Iteration 104/1000 | Loss: 0.00001368
Iteration 105/1000 | Loss: 0.00001368
Iteration 106/1000 | Loss: 0.00001368
Iteration 107/1000 | Loss: 0.00001368
Iteration 108/1000 | Loss: 0.00001368
Iteration 109/1000 | Loss: 0.00001368
Iteration 110/1000 | Loss: 0.00001368
Iteration 111/1000 | Loss: 0.00001368
Iteration 112/1000 | Loss: 0.00001368
Iteration 113/1000 | Loss: 0.00001368
Iteration 114/1000 | Loss: 0.00001368
Iteration 115/1000 | Loss: 0.00001368
Iteration 116/1000 | Loss: 0.00001368
Iteration 117/1000 | Loss: 0.00001368
Iteration 118/1000 | Loss: 0.00001368
Iteration 119/1000 | Loss: 0.00001368
Iteration 120/1000 | Loss: 0.00001368
Iteration 121/1000 | Loss: 0.00001368
Iteration 122/1000 | Loss: 0.00001368
Iteration 123/1000 | Loss: 0.00001368
Iteration 124/1000 | Loss: 0.00001368
Iteration 125/1000 | Loss: 0.00001368
Iteration 126/1000 | Loss: 0.00001368
Iteration 127/1000 | Loss: 0.00001368
Iteration 128/1000 | Loss: 0.00001368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.3684270015801303e-05, 1.3684270015801303e-05, 1.3684270015801303e-05, 1.3684270015801303e-05, 1.3684270015801303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3684270015801303e-05

Optimization complete. Final v2v error: 3.143982410430908 mm

Highest mean error: 3.4562368392944336 mm for frame 125

Lowest mean error: 2.8315327167510986 mm for frame 237

Saving results

Total time: 53.683096408843994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407580
Iteration 2/25 | Loss: 0.00128963
Iteration 3/25 | Loss: 0.00120823
Iteration 4/25 | Loss: 0.00119564
Iteration 5/25 | Loss: 0.00119180
Iteration 6/25 | Loss: 0.00119080
Iteration 7/25 | Loss: 0.00119066
Iteration 8/25 | Loss: 0.00119066
Iteration 9/25 | Loss: 0.00119066
Iteration 10/25 | Loss: 0.00119066
Iteration 11/25 | Loss: 0.00119066
Iteration 12/25 | Loss: 0.00119066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011906608706340194, 0.0011906608706340194, 0.0011906608706340194, 0.0011906608706340194, 0.0011906608706340194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011906608706340194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02959251
Iteration 2/25 | Loss: 0.00072909
Iteration 3/25 | Loss: 0.00072908
Iteration 4/25 | Loss: 0.00072908
Iteration 5/25 | Loss: 0.00072908
Iteration 6/25 | Loss: 0.00072908
Iteration 7/25 | Loss: 0.00072908
Iteration 8/25 | Loss: 0.00072908
Iteration 9/25 | Loss: 0.00072908
Iteration 10/25 | Loss: 0.00072908
Iteration 11/25 | Loss: 0.00072908
Iteration 12/25 | Loss: 0.00072908
Iteration 13/25 | Loss: 0.00072908
Iteration 14/25 | Loss: 0.00072908
Iteration 15/25 | Loss: 0.00072908
Iteration 16/25 | Loss: 0.00072908
Iteration 17/25 | Loss: 0.00072908
Iteration 18/25 | Loss: 0.00072908
Iteration 19/25 | Loss: 0.00072908
Iteration 20/25 | Loss: 0.00072908
Iteration 21/25 | Loss: 0.00072908
Iteration 22/25 | Loss: 0.00072908
Iteration 23/25 | Loss: 0.00072908
Iteration 24/25 | Loss: 0.00072908
Iteration 25/25 | Loss: 0.00072908

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072908
Iteration 2/1000 | Loss: 0.00003844
Iteration 3/1000 | Loss: 0.00002537
Iteration 4/1000 | Loss: 0.00002020
Iteration 5/1000 | Loss: 0.00001853
Iteration 6/1000 | Loss: 0.00001758
Iteration 7/1000 | Loss: 0.00001695
Iteration 8/1000 | Loss: 0.00001641
Iteration 9/1000 | Loss: 0.00001597
Iteration 10/1000 | Loss: 0.00001569
Iteration 11/1000 | Loss: 0.00001562
Iteration 12/1000 | Loss: 0.00001556
Iteration 13/1000 | Loss: 0.00001554
Iteration 14/1000 | Loss: 0.00001534
Iteration 15/1000 | Loss: 0.00001531
Iteration 16/1000 | Loss: 0.00001526
Iteration 17/1000 | Loss: 0.00001513
Iteration 18/1000 | Loss: 0.00001509
Iteration 19/1000 | Loss: 0.00001507
Iteration 20/1000 | Loss: 0.00001507
Iteration 21/1000 | Loss: 0.00001506
Iteration 22/1000 | Loss: 0.00001503
Iteration 23/1000 | Loss: 0.00001503
Iteration 24/1000 | Loss: 0.00001502
Iteration 25/1000 | Loss: 0.00001501
Iteration 26/1000 | Loss: 0.00001500
Iteration 27/1000 | Loss: 0.00001496
Iteration 28/1000 | Loss: 0.00001480
Iteration 29/1000 | Loss: 0.00001479
Iteration 30/1000 | Loss: 0.00001475
Iteration 31/1000 | Loss: 0.00001475
Iteration 32/1000 | Loss: 0.00001470
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001466
Iteration 35/1000 | Loss: 0.00001465
Iteration 36/1000 | Loss: 0.00001465
Iteration 37/1000 | Loss: 0.00001464
Iteration 38/1000 | Loss: 0.00001463
Iteration 39/1000 | Loss: 0.00001463
Iteration 40/1000 | Loss: 0.00001459
Iteration 41/1000 | Loss: 0.00001459
Iteration 42/1000 | Loss: 0.00001453
Iteration 43/1000 | Loss: 0.00001453
Iteration 44/1000 | Loss: 0.00001452
Iteration 45/1000 | Loss: 0.00001451
Iteration 46/1000 | Loss: 0.00001451
Iteration 47/1000 | Loss: 0.00001450
Iteration 48/1000 | Loss: 0.00001449
Iteration 49/1000 | Loss: 0.00001448
Iteration 50/1000 | Loss: 0.00001448
Iteration 51/1000 | Loss: 0.00001448
Iteration 52/1000 | Loss: 0.00001447
Iteration 53/1000 | Loss: 0.00001447
Iteration 54/1000 | Loss: 0.00001446
Iteration 55/1000 | Loss: 0.00001446
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001443
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001442
Iteration 63/1000 | Loss: 0.00001442
Iteration 64/1000 | Loss: 0.00001441
Iteration 65/1000 | Loss: 0.00001441
Iteration 66/1000 | Loss: 0.00001441
Iteration 67/1000 | Loss: 0.00001440
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001439
Iteration 70/1000 | Loss: 0.00001439
Iteration 71/1000 | Loss: 0.00001439
Iteration 72/1000 | Loss: 0.00001438
Iteration 73/1000 | Loss: 0.00001438
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001437
Iteration 77/1000 | Loss: 0.00001437
Iteration 78/1000 | Loss: 0.00001437
Iteration 79/1000 | Loss: 0.00001437
Iteration 80/1000 | Loss: 0.00001437
Iteration 81/1000 | Loss: 0.00001437
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001437
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001437
Iteration 86/1000 | Loss: 0.00001437
Iteration 87/1000 | Loss: 0.00001437
Iteration 88/1000 | Loss: 0.00001437
Iteration 89/1000 | Loss: 0.00001437
Iteration 90/1000 | Loss: 0.00001436
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001436
Iteration 93/1000 | Loss: 0.00001436
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001434
Iteration 97/1000 | Loss: 0.00001434
Iteration 98/1000 | Loss: 0.00001434
Iteration 99/1000 | Loss: 0.00001434
Iteration 100/1000 | Loss: 0.00001433
Iteration 101/1000 | Loss: 0.00001433
Iteration 102/1000 | Loss: 0.00001433
Iteration 103/1000 | Loss: 0.00001433
Iteration 104/1000 | Loss: 0.00001432
Iteration 105/1000 | Loss: 0.00001432
Iteration 106/1000 | Loss: 0.00001432
Iteration 107/1000 | Loss: 0.00001431
Iteration 108/1000 | Loss: 0.00001431
Iteration 109/1000 | Loss: 0.00001431
Iteration 110/1000 | Loss: 0.00001430
Iteration 111/1000 | Loss: 0.00001430
Iteration 112/1000 | Loss: 0.00001430
Iteration 113/1000 | Loss: 0.00001430
Iteration 114/1000 | Loss: 0.00001430
Iteration 115/1000 | Loss: 0.00001430
Iteration 116/1000 | Loss: 0.00001430
Iteration 117/1000 | Loss: 0.00001430
Iteration 118/1000 | Loss: 0.00001430
Iteration 119/1000 | Loss: 0.00001429
Iteration 120/1000 | Loss: 0.00001429
Iteration 121/1000 | Loss: 0.00001429
Iteration 122/1000 | Loss: 0.00001428
Iteration 123/1000 | Loss: 0.00001428
Iteration 124/1000 | Loss: 0.00001428
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001427
Iteration 127/1000 | Loss: 0.00001427
Iteration 128/1000 | Loss: 0.00001427
Iteration 129/1000 | Loss: 0.00001427
Iteration 130/1000 | Loss: 0.00001427
Iteration 131/1000 | Loss: 0.00001427
Iteration 132/1000 | Loss: 0.00001427
Iteration 133/1000 | Loss: 0.00001427
Iteration 134/1000 | Loss: 0.00001426
Iteration 135/1000 | Loss: 0.00001426
Iteration 136/1000 | Loss: 0.00001426
Iteration 137/1000 | Loss: 0.00001426
Iteration 138/1000 | Loss: 0.00001426
Iteration 139/1000 | Loss: 0.00001426
Iteration 140/1000 | Loss: 0.00001426
Iteration 141/1000 | Loss: 0.00001426
Iteration 142/1000 | Loss: 0.00001426
Iteration 143/1000 | Loss: 0.00001426
Iteration 144/1000 | Loss: 0.00001426
Iteration 145/1000 | Loss: 0.00001426
Iteration 146/1000 | Loss: 0.00001425
Iteration 147/1000 | Loss: 0.00001425
Iteration 148/1000 | Loss: 0.00001425
Iteration 149/1000 | Loss: 0.00001425
Iteration 150/1000 | Loss: 0.00001425
Iteration 151/1000 | Loss: 0.00001425
Iteration 152/1000 | Loss: 0.00001425
Iteration 153/1000 | Loss: 0.00001425
Iteration 154/1000 | Loss: 0.00001425
Iteration 155/1000 | Loss: 0.00001425
Iteration 156/1000 | Loss: 0.00001425
Iteration 157/1000 | Loss: 0.00001425
Iteration 158/1000 | Loss: 0.00001424
Iteration 159/1000 | Loss: 0.00001424
Iteration 160/1000 | Loss: 0.00001424
Iteration 161/1000 | Loss: 0.00001424
Iteration 162/1000 | Loss: 0.00001424
Iteration 163/1000 | Loss: 0.00001424
Iteration 164/1000 | Loss: 0.00001424
Iteration 165/1000 | Loss: 0.00001424
Iteration 166/1000 | Loss: 0.00001424
Iteration 167/1000 | Loss: 0.00001424
Iteration 168/1000 | Loss: 0.00001424
Iteration 169/1000 | Loss: 0.00001423
Iteration 170/1000 | Loss: 0.00001423
Iteration 171/1000 | Loss: 0.00001423
Iteration 172/1000 | Loss: 0.00001423
Iteration 173/1000 | Loss: 0.00001423
Iteration 174/1000 | Loss: 0.00001423
Iteration 175/1000 | Loss: 0.00001423
Iteration 176/1000 | Loss: 0.00001423
Iteration 177/1000 | Loss: 0.00001423
Iteration 178/1000 | Loss: 0.00001423
Iteration 179/1000 | Loss: 0.00001423
Iteration 180/1000 | Loss: 0.00001423
Iteration 181/1000 | Loss: 0.00001423
Iteration 182/1000 | Loss: 0.00001423
Iteration 183/1000 | Loss: 0.00001423
Iteration 184/1000 | Loss: 0.00001423
Iteration 185/1000 | Loss: 0.00001423
Iteration 186/1000 | Loss: 0.00001423
Iteration 187/1000 | Loss: 0.00001423
Iteration 188/1000 | Loss: 0.00001423
Iteration 189/1000 | Loss: 0.00001423
Iteration 190/1000 | Loss: 0.00001423
Iteration 191/1000 | Loss: 0.00001423
Iteration 192/1000 | Loss: 0.00001423
Iteration 193/1000 | Loss: 0.00001423
Iteration 194/1000 | Loss: 0.00001423
Iteration 195/1000 | Loss: 0.00001423
Iteration 196/1000 | Loss: 0.00001423
Iteration 197/1000 | Loss: 0.00001423
Iteration 198/1000 | Loss: 0.00001423
Iteration 199/1000 | Loss: 0.00001423
Iteration 200/1000 | Loss: 0.00001423
Iteration 201/1000 | Loss: 0.00001423
Iteration 202/1000 | Loss: 0.00001423
Iteration 203/1000 | Loss: 0.00001423
Iteration 204/1000 | Loss: 0.00001423
Iteration 205/1000 | Loss: 0.00001423
Iteration 206/1000 | Loss: 0.00001423
Iteration 207/1000 | Loss: 0.00001423
Iteration 208/1000 | Loss: 0.00001423
Iteration 209/1000 | Loss: 0.00001423
Iteration 210/1000 | Loss: 0.00001423
Iteration 211/1000 | Loss: 0.00001423
Iteration 212/1000 | Loss: 0.00001423
Iteration 213/1000 | Loss: 0.00001423
Iteration 214/1000 | Loss: 0.00001423
Iteration 215/1000 | Loss: 0.00001423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.422800232830923e-05, 1.422800232830923e-05, 1.422800232830923e-05, 1.422800232830923e-05, 1.422800232830923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.422800232830923e-05

Optimization complete. Final v2v error: 3.2193262577056885 mm

Highest mean error: 4.182717323303223 mm for frame 57

Lowest mean error: 2.9460701942443848 mm for frame 91

Saving results

Total time: 44.36634659767151
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824048
Iteration 2/25 | Loss: 0.00185250
Iteration 3/25 | Loss: 0.00143312
Iteration 4/25 | Loss: 0.00132492
Iteration 5/25 | Loss: 0.00132106
Iteration 6/25 | Loss: 0.00136015
Iteration 7/25 | Loss: 0.00133753
Iteration 8/25 | Loss: 0.00130166
Iteration 9/25 | Loss: 0.00128172
Iteration 10/25 | Loss: 0.00127422
Iteration 11/25 | Loss: 0.00127300
Iteration 12/25 | Loss: 0.00127651
Iteration 13/25 | Loss: 0.00127398
Iteration 14/25 | Loss: 0.00127247
Iteration 15/25 | Loss: 0.00127415
Iteration 16/25 | Loss: 0.00126930
Iteration 17/25 | Loss: 0.00126681
Iteration 18/25 | Loss: 0.00126580
Iteration 19/25 | Loss: 0.00127014
Iteration 20/25 | Loss: 0.00126850
Iteration 21/25 | Loss: 0.00126747
Iteration 22/25 | Loss: 0.00126993
Iteration 23/25 | Loss: 0.00126740
Iteration 24/25 | Loss: 0.00126404
Iteration 25/25 | Loss: 0.00126455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.32208014
Iteration 2/25 | Loss: 0.00089055
Iteration 3/25 | Loss: 0.00082807
Iteration 4/25 | Loss: 0.00082807
Iteration 5/25 | Loss: 0.00082807
Iteration 6/25 | Loss: 0.00082807
Iteration 7/25 | Loss: 0.00082807
Iteration 8/25 | Loss: 0.00082807
Iteration 9/25 | Loss: 0.00082807
Iteration 10/25 | Loss: 0.00082807
Iteration 11/25 | Loss: 0.00082807
Iteration 12/25 | Loss: 0.00082807
Iteration 13/25 | Loss: 0.00082807
Iteration 14/25 | Loss: 0.00082807
Iteration 15/25 | Loss: 0.00082807
Iteration 16/25 | Loss: 0.00082807
Iteration 17/25 | Loss: 0.00082807
Iteration 18/25 | Loss: 0.00082807
Iteration 19/25 | Loss: 0.00082807
Iteration 20/25 | Loss: 0.00082807
Iteration 21/25 | Loss: 0.00082807
Iteration 22/25 | Loss: 0.00082807
Iteration 23/25 | Loss: 0.00082807
Iteration 24/25 | Loss: 0.00082807
Iteration 25/25 | Loss: 0.00082807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082807
Iteration 2/1000 | Loss: 0.00014733
Iteration 3/1000 | Loss: 0.00050850
Iteration 4/1000 | Loss: 0.00003657
Iteration 5/1000 | Loss: 0.00043720
Iteration 6/1000 | Loss: 0.00002883
Iteration 7/1000 | Loss: 0.00016437
Iteration 8/1000 | Loss: 0.00002104
Iteration 9/1000 | Loss: 0.00006709
Iteration 10/1000 | Loss: 0.00052906
Iteration 11/1000 | Loss: 0.00039435
Iteration 12/1000 | Loss: 0.00051273
Iteration 13/1000 | Loss: 0.00058604
Iteration 14/1000 | Loss: 0.00066108
Iteration 15/1000 | Loss: 0.00055765
Iteration 16/1000 | Loss: 0.00052381
Iteration 17/1000 | Loss: 0.00010656
Iteration 18/1000 | Loss: 0.00002061
Iteration 19/1000 | Loss: 0.00001864
Iteration 20/1000 | Loss: 0.00001757
Iteration 21/1000 | Loss: 0.00001692
Iteration 22/1000 | Loss: 0.00001654
Iteration 23/1000 | Loss: 0.00001620
Iteration 24/1000 | Loss: 0.00001599
Iteration 25/1000 | Loss: 0.00001587
Iteration 26/1000 | Loss: 0.00001584
Iteration 27/1000 | Loss: 0.00001575
Iteration 28/1000 | Loss: 0.00001588
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001533
Iteration 31/1000 | Loss: 0.00001526
Iteration 32/1000 | Loss: 0.00001523
Iteration 33/1000 | Loss: 0.00001516
Iteration 34/1000 | Loss: 0.00001516
Iteration 35/1000 | Loss: 0.00001514
Iteration 36/1000 | Loss: 0.00001512
Iteration 37/1000 | Loss: 0.00001512
Iteration 38/1000 | Loss: 0.00001512
Iteration 39/1000 | Loss: 0.00001512
Iteration 40/1000 | Loss: 0.00001512
Iteration 41/1000 | Loss: 0.00001512
Iteration 42/1000 | Loss: 0.00001512
Iteration 43/1000 | Loss: 0.00001512
Iteration 44/1000 | Loss: 0.00001512
Iteration 45/1000 | Loss: 0.00001511
Iteration 46/1000 | Loss: 0.00001511
Iteration 47/1000 | Loss: 0.00001511
Iteration 48/1000 | Loss: 0.00001511
Iteration 49/1000 | Loss: 0.00001511
Iteration 50/1000 | Loss: 0.00001511
Iteration 51/1000 | Loss: 0.00001508
Iteration 52/1000 | Loss: 0.00001508
Iteration 53/1000 | Loss: 0.00001507
Iteration 54/1000 | Loss: 0.00001507
Iteration 55/1000 | Loss: 0.00001507
Iteration 56/1000 | Loss: 0.00001506
Iteration 57/1000 | Loss: 0.00001503
Iteration 58/1000 | Loss: 0.00001502
Iteration 59/1000 | Loss: 0.00001502
Iteration 60/1000 | Loss: 0.00001502
Iteration 61/1000 | Loss: 0.00001501
Iteration 62/1000 | Loss: 0.00001501
Iteration 63/1000 | Loss: 0.00001501
Iteration 64/1000 | Loss: 0.00001500
Iteration 65/1000 | Loss: 0.00001499
Iteration 66/1000 | Loss: 0.00001499
Iteration 67/1000 | Loss: 0.00001498
Iteration 68/1000 | Loss: 0.00001498
Iteration 69/1000 | Loss: 0.00001498
Iteration 70/1000 | Loss: 0.00001498
Iteration 71/1000 | Loss: 0.00001497
Iteration 72/1000 | Loss: 0.00001497
Iteration 73/1000 | Loss: 0.00001496
Iteration 74/1000 | Loss: 0.00001493
Iteration 75/1000 | Loss: 0.00001493
Iteration 76/1000 | Loss: 0.00001491
Iteration 77/1000 | Loss: 0.00001490
Iteration 78/1000 | Loss: 0.00001490
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001489
Iteration 81/1000 | Loss: 0.00001489
Iteration 82/1000 | Loss: 0.00001489
Iteration 83/1000 | Loss: 0.00001489
Iteration 84/1000 | Loss: 0.00001488
Iteration 85/1000 | Loss: 0.00001488
Iteration 86/1000 | Loss: 0.00001488
Iteration 87/1000 | Loss: 0.00001488
Iteration 88/1000 | Loss: 0.00001488
Iteration 89/1000 | Loss: 0.00001488
Iteration 90/1000 | Loss: 0.00001487
Iteration 91/1000 | Loss: 0.00001487
Iteration 92/1000 | Loss: 0.00001487
Iteration 93/1000 | Loss: 0.00001487
Iteration 94/1000 | Loss: 0.00001487
Iteration 95/1000 | Loss: 0.00001487
Iteration 96/1000 | Loss: 0.00001486
Iteration 97/1000 | Loss: 0.00001486
Iteration 98/1000 | Loss: 0.00001486
Iteration 99/1000 | Loss: 0.00001486
Iteration 100/1000 | Loss: 0.00001485
Iteration 101/1000 | Loss: 0.00001485
Iteration 102/1000 | Loss: 0.00001485
Iteration 103/1000 | Loss: 0.00001485
Iteration 104/1000 | Loss: 0.00001485
Iteration 105/1000 | Loss: 0.00001484
Iteration 106/1000 | Loss: 0.00001484
Iteration 107/1000 | Loss: 0.00001484
Iteration 108/1000 | Loss: 0.00001484
Iteration 109/1000 | Loss: 0.00001484
Iteration 110/1000 | Loss: 0.00001484
Iteration 111/1000 | Loss: 0.00001484
Iteration 112/1000 | Loss: 0.00001484
Iteration 113/1000 | Loss: 0.00001483
Iteration 114/1000 | Loss: 0.00001483
Iteration 115/1000 | Loss: 0.00001483
Iteration 116/1000 | Loss: 0.00001483
Iteration 117/1000 | Loss: 0.00001483
Iteration 118/1000 | Loss: 0.00001483
Iteration 119/1000 | Loss: 0.00001483
Iteration 120/1000 | Loss: 0.00001483
Iteration 121/1000 | Loss: 0.00001483
Iteration 122/1000 | Loss: 0.00001482
Iteration 123/1000 | Loss: 0.00001482
Iteration 124/1000 | Loss: 0.00001482
Iteration 125/1000 | Loss: 0.00001482
Iteration 126/1000 | Loss: 0.00001482
Iteration 127/1000 | Loss: 0.00001482
Iteration 128/1000 | Loss: 0.00001481
Iteration 129/1000 | Loss: 0.00001481
Iteration 130/1000 | Loss: 0.00001481
Iteration 131/1000 | Loss: 0.00001481
Iteration 132/1000 | Loss: 0.00001481
Iteration 133/1000 | Loss: 0.00001481
Iteration 134/1000 | Loss: 0.00001481
Iteration 135/1000 | Loss: 0.00001481
Iteration 136/1000 | Loss: 0.00001481
Iteration 137/1000 | Loss: 0.00001481
Iteration 138/1000 | Loss: 0.00001480
Iteration 139/1000 | Loss: 0.00001480
Iteration 140/1000 | Loss: 0.00001480
Iteration 141/1000 | Loss: 0.00001480
Iteration 142/1000 | Loss: 0.00001480
Iteration 143/1000 | Loss: 0.00001480
Iteration 144/1000 | Loss: 0.00001480
Iteration 145/1000 | Loss: 0.00001480
Iteration 146/1000 | Loss: 0.00001480
Iteration 147/1000 | Loss: 0.00001480
Iteration 148/1000 | Loss: 0.00001480
Iteration 149/1000 | Loss: 0.00001480
Iteration 150/1000 | Loss: 0.00001480
Iteration 151/1000 | Loss: 0.00001480
Iteration 152/1000 | Loss: 0.00001480
Iteration 153/1000 | Loss: 0.00001480
Iteration 154/1000 | Loss: 0.00001479
Iteration 155/1000 | Loss: 0.00001479
Iteration 156/1000 | Loss: 0.00001479
Iteration 157/1000 | Loss: 0.00001479
Iteration 158/1000 | Loss: 0.00001479
Iteration 159/1000 | Loss: 0.00001479
Iteration 160/1000 | Loss: 0.00001479
Iteration 161/1000 | Loss: 0.00001479
Iteration 162/1000 | Loss: 0.00001479
Iteration 163/1000 | Loss: 0.00001479
Iteration 164/1000 | Loss: 0.00001479
Iteration 165/1000 | Loss: 0.00001479
Iteration 166/1000 | Loss: 0.00001479
Iteration 167/1000 | Loss: 0.00001479
Iteration 168/1000 | Loss: 0.00001479
Iteration 169/1000 | Loss: 0.00001479
Iteration 170/1000 | Loss: 0.00001479
Iteration 171/1000 | Loss: 0.00001479
Iteration 172/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.4793605259910692e-05, 1.4793605259910692e-05, 1.4793605259910692e-05, 1.4793605259910692e-05, 1.4793605259910692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4793605259910692e-05

Optimization complete. Final v2v error: 3.251495122909546 mm

Highest mean error: 4.017053127288818 mm for frame 166

Lowest mean error: 2.833665609359741 mm for frame 205

Saving results

Total time: 112.50502824783325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00757720
Iteration 2/25 | Loss: 0.00139533
Iteration 3/25 | Loss: 0.00128619
Iteration 4/25 | Loss: 0.00126585
Iteration 5/25 | Loss: 0.00125975
Iteration 6/25 | Loss: 0.00125920
Iteration 7/25 | Loss: 0.00125920
Iteration 8/25 | Loss: 0.00125920
Iteration 9/25 | Loss: 0.00125920
Iteration 10/25 | Loss: 0.00125920
Iteration 11/25 | Loss: 0.00125920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012592036509886384, 0.0012592036509886384, 0.0012592036509886384, 0.0012592036509886384, 0.0012592036509886384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012592036509886384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48623323
Iteration 2/25 | Loss: 0.00086416
Iteration 3/25 | Loss: 0.00086416
Iteration 4/25 | Loss: 0.00086416
Iteration 5/25 | Loss: 0.00086416
Iteration 6/25 | Loss: 0.00086416
Iteration 7/25 | Loss: 0.00086416
Iteration 8/25 | Loss: 0.00086416
Iteration 9/25 | Loss: 0.00086416
Iteration 10/25 | Loss: 0.00086416
Iteration 11/25 | Loss: 0.00086416
Iteration 12/25 | Loss: 0.00086416
Iteration 13/25 | Loss: 0.00086416
Iteration 14/25 | Loss: 0.00086416
Iteration 15/25 | Loss: 0.00086416
Iteration 16/25 | Loss: 0.00086416
Iteration 17/25 | Loss: 0.00086416
Iteration 18/25 | Loss: 0.00086416
Iteration 19/25 | Loss: 0.00086416
Iteration 20/25 | Loss: 0.00086416
Iteration 21/25 | Loss: 0.00086416
Iteration 22/25 | Loss: 0.00086416
Iteration 23/25 | Loss: 0.00086416
Iteration 24/25 | Loss: 0.00086416
Iteration 25/25 | Loss: 0.00086416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086416
Iteration 2/1000 | Loss: 0.00003782
Iteration 3/1000 | Loss: 0.00002531
Iteration 4/1000 | Loss: 0.00002167
Iteration 5/1000 | Loss: 0.00002044
Iteration 6/1000 | Loss: 0.00001956
Iteration 7/1000 | Loss: 0.00001897
Iteration 8/1000 | Loss: 0.00001846
Iteration 9/1000 | Loss: 0.00001815
Iteration 10/1000 | Loss: 0.00001782
Iteration 11/1000 | Loss: 0.00001753
Iteration 12/1000 | Loss: 0.00001747
Iteration 13/1000 | Loss: 0.00001742
Iteration 14/1000 | Loss: 0.00001734
Iteration 15/1000 | Loss: 0.00001713
Iteration 16/1000 | Loss: 0.00001713
Iteration 17/1000 | Loss: 0.00001707
Iteration 18/1000 | Loss: 0.00001703
Iteration 19/1000 | Loss: 0.00001700
Iteration 20/1000 | Loss: 0.00001699
Iteration 21/1000 | Loss: 0.00001699
Iteration 22/1000 | Loss: 0.00001695
Iteration 23/1000 | Loss: 0.00001693
Iteration 24/1000 | Loss: 0.00001692
Iteration 25/1000 | Loss: 0.00001692
Iteration 26/1000 | Loss: 0.00001692
Iteration 27/1000 | Loss: 0.00001691
Iteration 28/1000 | Loss: 0.00001691
Iteration 29/1000 | Loss: 0.00001691
Iteration 30/1000 | Loss: 0.00001690
Iteration 31/1000 | Loss: 0.00001690
Iteration 32/1000 | Loss: 0.00001689
Iteration 33/1000 | Loss: 0.00001689
Iteration 34/1000 | Loss: 0.00001689
Iteration 35/1000 | Loss: 0.00001688
Iteration 36/1000 | Loss: 0.00001687
Iteration 37/1000 | Loss: 0.00001687
Iteration 38/1000 | Loss: 0.00001687
Iteration 39/1000 | Loss: 0.00001686
Iteration 40/1000 | Loss: 0.00001686
Iteration 41/1000 | Loss: 0.00001686
Iteration 42/1000 | Loss: 0.00001685
Iteration 43/1000 | Loss: 0.00001685
Iteration 44/1000 | Loss: 0.00001684
Iteration 45/1000 | Loss: 0.00001684
Iteration 46/1000 | Loss: 0.00001684
Iteration 47/1000 | Loss: 0.00001684
Iteration 48/1000 | Loss: 0.00001684
Iteration 49/1000 | Loss: 0.00001684
Iteration 50/1000 | Loss: 0.00001683
Iteration 51/1000 | Loss: 0.00001683
Iteration 52/1000 | Loss: 0.00001683
Iteration 53/1000 | Loss: 0.00001683
Iteration 54/1000 | Loss: 0.00001682
Iteration 55/1000 | Loss: 0.00001682
Iteration 56/1000 | Loss: 0.00001681
Iteration 57/1000 | Loss: 0.00001681
Iteration 58/1000 | Loss: 0.00001681
Iteration 59/1000 | Loss: 0.00001680
Iteration 60/1000 | Loss: 0.00001680
Iteration 61/1000 | Loss: 0.00001679
Iteration 62/1000 | Loss: 0.00001678
Iteration 63/1000 | Loss: 0.00001678
Iteration 64/1000 | Loss: 0.00001678
Iteration 65/1000 | Loss: 0.00001677
Iteration 66/1000 | Loss: 0.00001677
Iteration 67/1000 | Loss: 0.00001677
Iteration 68/1000 | Loss: 0.00001676
Iteration 69/1000 | Loss: 0.00001676
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001676
Iteration 72/1000 | Loss: 0.00001676
Iteration 73/1000 | Loss: 0.00001676
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00001675
Iteration 76/1000 | Loss: 0.00001675
Iteration 77/1000 | Loss: 0.00001675
Iteration 78/1000 | Loss: 0.00001675
Iteration 79/1000 | Loss: 0.00001675
Iteration 80/1000 | Loss: 0.00001675
Iteration 81/1000 | Loss: 0.00001675
Iteration 82/1000 | Loss: 0.00001675
Iteration 83/1000 | Loss: 0.00001675
Iteration 84/1000 | Loss: 0.00001675
Iteration 85/1000 | Loss: 0.00001675
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001674
Iteration 88/1000 | Loss: 0.00001673
Iteration 89/1000 | Loss: 0.00001673
Iteration 90/1000 | Loss: 0.00001673
Iteration 91/1000 | Loss: 0.00001673
Iteration 92/1000 | Loss: 0.00001673
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001673
Iteration 95/1000 | Loss: 0.00001673
Iteration 96/1000 | Loss: 0.00001673
Iteration 97/1000 | Loss: 0.00001673
Iteration 98/1000 | Loss: 0.00001672
Iteration 99/1000 | Loss: 0.00001672
Iteration 100/1000 | Loss: 0.00001672
Iteration 101/1000 | Loss: 0.00001672
Iteration 102/1000 | Loss: 0.00001672
Iteration 103/1000 | Loss: 0.00001672
Iteration 104/1000 | Loss: 0.00001672
Iteration 105/1000 | Loss: 0.00001671
Iteration 106/1000 | Loss: 0.00001671
Iteration 107/1000 | Loss: 0.00001671
Iteration 108/1000 | Loss: 0.00001671
Iteration 109/1000 | Loss: 0.00001671
Iteration 110/1000 | Loss: 0.00001670
Iteration 111/1000 | Loss: 0.00001670
Iteration 112/1000 | Loss: 0.00001670
Iteration 113/1000 | Loss: 0.00001669
Iteration 114/1000 | Loss: 0.00001669
Iteration 115/1000 | Loss: 0.00001669
Iteration 116/1000 | Loss: 0.00001669
Iteration 117/1000 | Loss: 0.00001668
Iteration 118/1000 | Loss: 0.00001668
Iteration 119/1000 | Loss: 0.00001668
Iteration 120/1000 | Loss: 0.00001667
Iteration 121/1000 | Loss: 0.00001667
Iteration 122/1000 | Loss: 0.00001667
Iteration 123/1000 | Loss: 0.00001667
Iteration 124/1000 | Loss: 0.00001667
Iteration 125/1000 | Loss: 0.00001666
Iteration 126/1000 | Loss: 0.00001666
Iteration 127/1000 | Loss: 0.00001666
Iteration 128/1000 | Loss: 0.00001665
Iteration 129/1000 | Loss: 0.00001665
Iteration 130/1000 | Loss: 0.00001665
Iteration 131/1000 | Loss: 0.00001665
Iteration 132/1000 | Loss: 0.00001665
Iteration 133/1000 | Loss: 0.00001665
Iteration 134/1000 | Loss: 0.00001664
Iteration 135/1000 | Loss: 0.00001664
Iteration 136/1000 | Loss: 0.00001664
Iteration 137/1000 | Loss: 0.00001663
Iteration 138/1000 | Loss: 0.00001663
Iteration 139/1000 | Loss: 0.00001663
Iteration 140/1000 | Loss: 0.00001663
Iteration 141/1000 | Loss: 0.00001662
Iteration 142/1000 | Loss: 0.00001662
Iteration 143/1000 | Loss: 0.00001662
Iteration 144/1000 | Loss: 0.00001662
Iteration 145/1000 | Loss: 0.00001661
Iteration 146/1000 | Loss: 0.00001661
Iteration 147/1000 | Loss: 0.00001661
Iteration 148/1000 | Loss: 0.00001661
Iteration 149/1000 | Loss: 0.00001661
Iteration 150/1000 | Loss: 0.00001661
Iteration 151/1000 | Loss: 0.00001661
Iteration 152/1000 | Loss: 0.00001661
Iteration 153/1000 | Loss: 0.00001660
Iteration 154/1000 | Loss: 0.00001660
Iteration 155/1000 | Loss: 0.00001660
Iteration 156/1000 | Loss: 0.00001660
Iteration 157/1000 | Loss: 0.00001660
Iteration 158/1000 | Loss: 0.00001660
Iteration 159/1000 | Loss: 0.00001660
Iteration 160/1000 | Loss: 0.00001659
Iteration 161/1000 | Loss: 0.00001659
Iteration 162/1000 | Loss: 0.00001659
Iteration 163/1000 | Loss: 0.00001659
Iteration 164/1000 | Loss: 0.00001659
Iteration 165/1000 | Loss: 0.00001659
Iteration 166/1000 | Loss: 0.00001659
Iteration 167/1000 | Loss: 0.00001659
Iteration 168/1000 | Loss: 0.00001659
Iteration 169/1000 | Loss: 0.00001658
Iteration 170/1000 | Loss: 0.00001658
Iteration 171/1000 | Loss: 0.00001658
Iteration 172/1000 | Loss: 0.00001658
Iteration 173/1000 | Loss: 0.00001658
Iteration 174/1000 | Loss: 0.00001658
Iteration 175/1000 | Loss: 0.00001658
Iteration 176/1000 | Loss: 0.00001657
Iteration 177/1000 | Loss: 0.00001657
Iteration 178/1000 | Loss: 0.00001657
Iteration 179/1000 | Loss: 0.00001657
Iteration 180/1000 | Loss: 0.00001657
Iteration 181/1000 | Loss: 0.00001657
Iteration 182/1000 | Loss: 0.00001657
Iteration 183/1000 | Loss: 0.00001657
Iteration 184/1000 | Loss: 0.00001657
Iteration 185/1000 | Loss: 0.00001657
Iteration 186/1000 | Loss: 0.00001657
Iteration 187/1000 | Loss: 0.00001657
Iteration 188/1000 | Loss: 0.00001657
Iteration 189/1000 | Loss: 0.00001657
Iteration 190/1000 | Loss: 0.00001657
Iteration 191/1000 | Loss: 0.00001657
Iteration 192/1000 | Loss: 0.00001657
Iteration 193/1000 | Loss: 0.00001657
Iteration 194/1000 | Loss: 0.00001657
Iteration 195/1000 | Loss: 0.00001657
Iteration 196/1000 | Loss: 0.00001657
Iteration 197/1000 | Loss: 0.00001657
Iteration 198/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.6572912500123493e-05, 1.6572912500123493e-05, 1.6572912500123493e-05, 1.6572912500123493e-05, 1.6572912500123493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6572912500123493e-05

Optimization complete. Final v2v error: 3.4559381008148193 mm

Highest mean error: 3.7891430854797363 mm for frame 19

Lowest mean error: 2.821927070617676 mm for frame 169

Saving results

Total time: 45.28867149353027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872469
Iteration 2/25 | Loss: 0.00130977
Iteration 3/25 | Loss: 0.00122685
Iteration 4/25 | Loss: 0.00121850
Iteration 5/25 | Loss: 0.00121588
Iteration 6/25 | Loss: 0.00121540
Iteration 7/25 | Loss: 0.00121540
Iteration 8/25 | Loss: 0.00121540
Iteration 9/25 | Loss: 0.00121540
Iteration 10/25 | Loss: 0.00121540
Iteration 11/25 | Loss: 0.00121540
Iteration 12/25 | Loss: 0.00121540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012154030846431851, 0.0012154030846431851, 0.0012154030846431851, 0.0012154030846431851, 0.0012154030846431851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012154030846431851

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.21272278
Iteration 2/25 | Loss: 0.00083467
Iteration 3/25 | Loss: 0.00083467
Iteration 4/25 | Loss: 0.00083467
Iteration 5/25 | Loss: 0.00083467
Iteration 6/25 | Loss: 0.00083467
Iteration 7/25 | Loss: 0.00083467
Iteration 8/25 | Loss: 0.00083467
Iteration 9/25 | Loss: 0.00083467
Iteration 10/25 | Loss: 0.00083467
Iteration 11/25 | Loss: 0.00083467
Iteration 12/25 | Loss: 0.00083467
Iteration 13/25 | Loss: 0.00083467
Iteration 14/25 | Loss: 0.00083467
Iteration 15/25 | Loss: 0.00083467
Iteration 16/25 | Loss: 0.00083467
Iteration 17/25 | Loss: 0.00083467
Iteration 18/25 | Loss: 0.00083467
Iteration 19/25 | Loss: 0.00083467
Iteration 20/25 | Loss: 0.00083467
Iteration 21/25 | Loss: 0.00083467
Iteration 22/25 | Loss: 0.00083467
Iteration 23/25 | Loss: 0.00083467
Iteration 24/25 | Loss: 0.00083467
Iteration 25/25 | Loss: 0.00083467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083467
Iteration 2/1000 | Loss: 0.00003036
Iteration 3/1000 | Loss: 0.00001934
Iteration 4/1000 | Loss: 0.00001621
Iteration 5/1000 | Loss: 0.00001512
Iteration 6/1000 | Loss: 0.00001425
Iteration 7/1000 | Loss: 0.00001363
Iteration 8/1000 | Loss: 0.00001332
Iteration 9/1000 | Loss: 0.00001316
Iteration 10/1000 | Loss: 0.00001300
Iteration 11/1000 | Loss: 0.00001298
Iteration 12/1000 | Loss: 0.00001291
Iteration 13/1000 | Loss: 0.00001288
Iteration 14/1000 | Loss: 0.00001276
Iteration 15/1000 | Loss: 0.00001269
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001265
Iteration 19/1000 | Loss: 0.00001263
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001247
Iteration 22/1000 | Loss: 0.00001246
Iteration 23/1000 | Loss: 0.00001245
Iteration 24/1000 | Loss: 0.00001243
Iteration 25/1000 | Loss: 0.00001243
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001240
Iteration 29/1000 | Loss: 0.00001237
Iteration 30/1000 | Loss: 0.00001235
Iteration 31/1000 | Loss: 0.00001235
Iteration 32/1000 | Loss: 0.00001234
Iteration 33/1000 | Loss: 0.00001231
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001230
Iteration 36/1000 | Loss: 0.00001230
Iteration 37/1000 | Loss: 0.00001229
Iteration 38/1000 | Loss: 0.00001229
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001228
Iteration 42/1000 | Loss: 0.00001228
Iteration 43/1000 | Loss: 0.00001228
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001226
Iteration 47/1000 | Loss: 0.00001226
Iteration 48/1000 | Loss: 0.00001225
Iteration 49/1000 | Loss: 0.00001225
Iteration 50/1000 | Loss: 0.00001225
Iteration 51/1000 | Loss: 0.00001224
Iteration 52/1000 | Loss: 0.00001224
Iteration 53/1000 | Loss: 0.00001223
Iteration 54/1000 | Loss: 0.00001222
Iteration 55/1000 | Loss: 0.00001222
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001219
Iteration 59/1000 | Loss: 0.00001219
Iteration 60/1000 | Loss: 0.00001219
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001219
Iteration 63/1000 | Loss: 0.00001219
Iteration 64/1000 | Loss: 0.00001219
Iteration 65/1000 | Loss: 0.00001219
Iteration 66/1000 | Loss: 0.00001219
Iteration 67/1000 | Loss: 0.00001219
Iteration 68/1000 | Loss: 0.00001219
Iteration 69/1000 | Loss: 0.00001218
Iteration 70/1000 | Loss: 0.00001218
Iteration 71/1000 | Loss: 0.00001218
Iteration 72/1000 | Loss: 0.00001218
Iteration 73/1000 | Loss: 0.00001218
Iteration 74/1000 | Loss: 0.00001217
Iteration 75/1000 | Loss: 0.00001217
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001215
Iteration 78/1000 | Loss: 0.00001215
Iteration 79/1000 | Loss: 0.00001215
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001215
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001213
Iteration 86/1000 | Loss: 0.00001212
Iteration 87/1000 | Loss: 0.00001212
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001211
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001210
Iteration 93/1000 | Loss: 0.00001210
Iteration 94/1000 | Loss: 0.00001210
Iteration 95/1000 | Loss: 0.00001209
Iteration 96/1000 | Loss: 0.00001209
Iteration 97/1000 | Loss: 0.00001209
Iteration 98/1000 | Loss: 0.00001208
Iteration 99/1000 | Loss: 0.00001208
Iteration 100/1000 | Loss: 0.00001208
Iteration 101/1000 | Loss: 0.00001207
Iteration 102/1000 | Loss: 0.00001207
Iteration 103/1000 | Loss: 0.00001207
Iteration 104/1000 | Loss: 0.00001206
Iteration 105/1000 | Loss: 0.00001206
Iteration 106/1000 | Loss: 0.00001206
Iteration 107/1000 | Loss: 0.00001205
Iteration 108/1000 | Loss: 0.00001205
Iteration 109/1000 | Loss: 0.00001204
Iteration 110/1000 | Loss: 0.00001204
Iteration 111/1000 | Loss: 0.00001204
Iteration 112/1000 | Loss: 0.00001203
Iteration 113/1000 | Loss: 0.00001203
Iteration 114/1000 | Loss: 0.00001203
Iteration 115/1000 | Loss: 0.00001203
Iteration 116/1000 | Loss: 0.00001203
Iteration 117/1000 | Loss: 0.00001203
Iteration 118/1000 | Loss: 0.00001203
Iteration 119/1000 | Loss: 0.00001202
Iteration 120/1000 | Loss: 0.00001202
Iteration 121/1000 | Loss: 0.00001202
Iteration 122/1000 | Loss: 0.00001202
Iteration 123/1000 | Loss: 0.00001201
Iteration 124/1000 | Loss: 0.00001201
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001201
Iteration 127/1000 | Loss: 0.00001201
Iteration 128/1000 | Loss: 0.00001201
Iteration 129/1000 | Loss: 0.00001200
Iteration 130/1000 | Loss: 0.00001200
Iteration 131/1000 | Loss: 0.00001200
Iteration 132/1000 | Loss: 0.00001200
Iteration 133/1000 | Loss: 0.00001200
Iteration 134/1000 | Loss: 0.00001200
Iteration 135/1000 | Loss: 0.00001200
Iteration 136/1000 | Loss: 0.00001200
Iteration 137/1000 | Loss: 0.00001200
Iteration 138/1000 | Loss: 0.00001200
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001199
Iteration 141/1000 | Loss: 0.00001199
Iteration 142/1000 | Loss: 0.00001199
Iteration 143/1000 | Loss: 0.00001199
Iteration 144/1000 | Loss: 0.00001199
Iteration 145/1000 | Loss: 0.00001199
Iteration 146/1000 | Loss: 0.00001199
Iteration 147/1000 | Loss: 0.00001199
Iteration 148/1000 | Loss: 0.00001198
Iteration 149/1000 | Loss: 0.00001198
Iteration 150/1000 | Loss: 0.00001198
Iteration 151/1000 | Loss: 0.00001198
Iteration 152/1000 | Loss: 0.00001198
Iteration 153/1000 | Loss: 0.00001197
Iteration 154/1000 | Loss: 0.00001197
Iteration 155/1000 | Loss: 0.00001197
Iteration 156/1000 | Loss: 0.00001197
Iteration 157/1000 | Loss: 0.00001197
Iteration 158/1000 | Loss: 0.00001197
Iteration 159/1000 | Loss: 0.00001197
Iteration 160/1000 | Loss: 0.00001197
Iteration 161/1000 | Loss: 0.00001197
Iteration 162/1000 | Loss: 0.00001196
Iteration 163/1000 | Loss: 0.00001196
Iteration 164/1000 | Loss: 0.00001196
Iteration 165/1000 | Loss: 0.00001196
Iteration 166/1000 | Loss: 0.00001196
Iteration 167/1000 | Loss: 0.00001196
Iteration 168/1000 | Loss: 0.00001196
Iteration 169/1000 | Loss: 0.00001196
Iteration 170/1000 | Loss: 0.00001196
Iteration 171/1000 | Loss: 0.00001196
Iteration 172/1000 | Loss: 0.00001196
Iteration 173/1000 | Loss: 0.00001196
Iteration 174/1000 | Loss: 0.00001196
Iteration 175/1000 | Loss: 0.00001196
Iteration 176/1000 | Loss: 0.00001195
Iteration 177/1000 | Loss: 0.00001195
Iteration 178/1000 | Loss: 0.00001195
Iteration 179/1000 | Loss: 0.00001195
Iteration 180/1000 | Loss: 0.00001195
Iteration 181/1000 | Loss: 0.00001195
Iteration 182/1000 | Loss: 0.00001195
Iteration 183/1000 | Loss: 0.00001195
Iteration 184/1000 | Loss: 0.00001195
Iteration 185/1000 | Loss: 0.00001195
Iteration 186/1000 | Loss: 0.00001195
Iteration 187/1000 | Loss: 0.00001195
Iteration 188/1000 | Loss: 0.00001195
Iteration 189/1000 | Loss: 0.00001195
Iteration 190/1000 | Loss: 0.00001195
Iteration 191/1000 | Loss: 0.00001195
Iteration 192/1000 | Loss: 0.00001195
Iteration 193/1000 | Loss: 0.00001195
Iteration 194/1000 | Loss: 0.00001195
Iteration 195/1000 | Loss: 0.00001195
Iteration 196/1000 | Loss: 0.00001195
Iteration 197/1000 | Loss: 0.00001195
Iteration 198/1000 | Loss: 0.00001195
Iteration 199/1000 | Loss: 0.00001195
Iteration 200/1000 | Loss: 0.00001195
Iteration 201/1000 | Loss: 0.00001195
Iteration 202/1000 | Loss: 0.00001195
Iteration 203/1000 | Loss: 0.00001195
Iteration 204/1000 | Loss: 0.00001195
Iteration 205/1000 | Loss: 0.00001195
Iteration 206/1000 | Loss: 0.00001195
Iteration 207/1000 | Loss: 0.00001195
Iteration 208/1000 | Loss: 0.00001195
Iteration 209/1000 | Loss: 0.00001195
Iteration 210/1000 | Loss: 0.00001195
Iteration 211/1000 | Loss: 0.00001195
Iteration 212/1000 | Loss: 0.00001195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.1952703971473966e-05, 1.1952703971473966e-05, 1.1952703971473966e-05, 1.1952703971473966e-05, 1.1952703971473966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1952703971473966e-05

Optimization complete. Final v2v error: 2.9563605785369873 mm

Highest mean error: 3.2901902198791504 mm for frame 45

Lowest mean error: 2.6957454681396484 mm for frame 129

Saving results

Total time: 38.5649151802063
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791157
Iteration 2/25 | Loss: 0.00148836
Iteration 3/25 | Loss: 0.00129276
Iteration 4/25 | Loss: 0.00126707
Iteration 5/25 | Loss: 0.00125752
Iteration 6/25 | Loss: 0.00125627
Iteration 7/25 | Loss: 0.00125627
Iteration 8/25 | Loss: 0.00125627
Iteration 9/25 | Loss: 0.00125627
Iteration 10/25 | Loss: 0.00125627
Iteration 11/25 | Loss: 0.00125627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012562721967697144, 0.0012562721967697144, 0.0012562721967697144, 0.0012562721967697144, 0.0012562721967697144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012562721967697144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83579779
Iteration 2/25 | Loss: 0.00057529
Iteration 3/25 | Loss: 0.00057529
Iteration 4/25 | Loss: 0.00057529
Iteration 5/25 | Loss: 0.00057529
Iteration 6/25 | Loss: 0.00057529
Iteration 7/25 | Loss: 0.00057529
Iteration 8/25 | Loss: 0.00057529
Iteration 9/25 | Loss: 0.00057529
Iteration 10/25 | Loss: 0.00057529
Iteration 11/25 | Loss: 0.00057529
Iteration 12/25 | Loss: 0.00057529
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005752914003096521, 0.0005752914003096521, 0.0005752914003096521, 0.0005752914003096521, 0.0005752914003096521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005752914003096521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057529
Iteration 2/1000 | Loss: 0.00004341
Iteration 3/1000 | Loss: 0.00002973
Iteration 4/1000 | Loss: 0.00002649
Iteration 5/1000 | Loss: 0.00002483
Iteration 6/1000 | Loss: 0.00002338
Iteration 7/1000 | Loss: 0.00002270
Iteration 8/1000 | Loss: 0.00002228
Iteration 9/1000 | Loss: 0.00002173
Iteration 10/1000 | Loss: 0.00002133
Iteration 11/1000 | Loss: 0.00002110
Iteration 12/1000 | Loss: 0.00002104
Iteration 13/1000 | Loss: 0.00002097
Iteration 14/1000 | Loss: 0.00002094
Iteration 15/1000 | Loss: 0.00002080
Iteration 16/1000 | Loss: 0.00002071
Iteration 17/1000 | Loss: 0.00002068
Iteration 18/1000 | Loss: 0.00002067
Iteration 19/1000 | Loss: 0.00002067
Iteration 20/1000 | Loss: 0.00002066
Iteration 21/1000 | Loss: 0.00002065
Iteration 22/1000 | Loss: 0.00002064
Iteration 23/1000 | Loss: 0.00002059
Iteration 24/1000 | Loss: 0.00002059
Iteration 25/1000 | Loss: 0.00002058
Iteration 26/1000 | Loss: 0.00002058
Iteration 27/1000 | Loss: 0.00002055
Iteration 28/1000 | Loss: 0.00002055
Iteration 29/1000 | Loss: 0.00002054
Iteration 30/1000 | Loss: 0.00002053
Iteration 31/1000 | Loss: 0.00002053
Iteration 32/1000 | Loss: 0.00002053
Iteration 33/1000 | Loss: 0.00002053
Iteration 34/1000 | Loss: 0.00002053
Iteration 35/1000 | Loss: 0.00002053
Iteration 36/1000 | Loss: 0.00002052
Iteration 37/1000 | Loss: 0.00002052
Iteration 38/1000 | Loss: 0.00002052
Iteration 39/1000 | Loss: 0.00002052
Iteration 40/1000 | Loss: 0.00002051
Iteration 41/1000 | Loss: 0.00002051
Iteration 42/1000 | Loss: 0.00002051
Iteration 43/1000 | Loss: 0.00002051
Iteration 44/1000 | Loss: 0.00002050
Iteration 45/1000 | Loss: 0.00002050
Iteration 46/1000 | Loss: 0.00002050
Iteration 47/1000 | Loss: 0.00002050
Iteration 48/1000 | Loss: 0.00002050
Iteration 49/1000 | Loss: 0.00002049
Iteration 50/1000 | Loss: 0.00002049
Iteration 51/1000 | Loss: 0.00002048
Iteration 52/1000 | Loss: 0.00002048
Iteration 53/1000 | Loss: 0.00002048
Iteration 54/1000 | Loss: 0.00002048
Iteration 55/1000 | Loss: 0.00002048
Iteration 56/1000 | Loss: 0.00002048
Iteration 57/1000 | Loss: 0.00002048
Iteration 58/1000 | Loss: 0.00002048
Iteration 59/1000 | Loss: 0.00002048
Iteration 60/1000 | Loss: 0.00002047
Iteration 61/1000 | Loss: 0.00002047
Iteration 62/1000 | Loss: 0.00002047
Iteration 63/1000 | Loss: 0.00002047
Iteration 64/1000 | Loss: 0.00002047
Iteration 65/1000 | Loss: 0.00002047
Iteration 66/1000 | Loss: 0.00002047
Iteration 67/1000 | Loss: 0.00002046
Iteration 68/1000 | Loss: 0.00002046
Iteration 69/1000 | Loss: 0.00002046
Iteration 70/1000 | Loss: 0.00002046
Iteration 71/1000 | Loss: 0.00002046
Iteration 72/1000 | Loss: 0.00002046
Iteration 73/1000 | Loss: 0.00002046
Iteration 74/1000 | Loss: 0.00002046
Iteration 75/1000 | Loss: 0.00002046
Iteration 76/1000 | Loss: 0.00002046
Iteration 77/1000 | Loss: 0.00002046
Iteration 78/1000 | Loss: 0.00002046
Iteration 79/1000 | Loss: 0.00002046
Iteration 80/1000 | Loss: 0.00002046
Iteration 81/1000 | Loss: 0.00002046
Iteration 82/1000 | Loss: 0.00002046
Iteration 83/1000 | Loss: 0.00002046
Iteration 84/1000 | Loss: 0.00002046
Iteration 85/1000 | Loss: 0.00002046
Iteration 86/1000 | Loss: 0.00002046
Iteration 87/1000 | Loss: 0.00002046
Iteration 88/1000 | Loss: 0.00002046
Iteration 89/1000 | Loss: 0.00002046
Iteration 90/1000 | Loss: 0.00002046
Iteration 91/1000 | Loss: 0.00002046
Iteration 92/1000 | Loss: 0.00002046
Iteration 93/1000 | Loss: 0.00002046
Iteration 94/1000 | Loss: 0.00002046
Iteration 95/1000 | Loss: 0.00002046
Iteration 96/1000 | Loss: 0.00002046
Iteration 97/1000 | Loss: 0.00002046
Iteration 98/1000 | Loss: 0.00002046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.0464865883695893e-05, 2.0464865883695893e-05, 2.0464865883695893e-05, 2.0464865883695893e-05, 2.0464865883695893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0464865883695893e-05

Optimization complete. Final v2v error: 3.852613687515259 mm

Highest mean error: 4.167953968048096 mm for frame 169

Lowest mean error: 3.5171236991882324 mm for frame 41

Saving results

Total time: 36.666280031204224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022123
Iteration 2/25 | Loss: 0.01022123
Iteration 3/25 | Loss: 0.00394950
Iteration 4/25 | Loss: 0.00273457
Iteration 5/25 | Loss: 0.00203332
Iteration 6/25 | Loss: 0.00200758
Iteration 7/25 | Loss: 0.00152229
Iteration 8/25 | Loss: 0.00144759
Iteration 9/25 | Loss: 0.00135980
Iteration 10/25 | Loss: 0.00133658
Iteration 11/25 | Loss: 0.00132530
Iteration 12/25 | Loss: 0.00133020
Iteration 13/25 | Loss: 0.00132091
Iteration 14/25 | Loss: 0.00131574
Iteration 15/25 | Loss: 0.00131195
Iteration 16/25 | Loss: 0.00131071
Iteration 17/25 | Loss: 0.00131499
Iteration 18/25 | Loss: 0.00131059
Iteration 19/25 | Loss: 0.00130757
Iteration 20/25 | Loss: 0.00131566
Iteration 21/25 | Loss: 0.00131144
Iteration 22/25 | Loss: 0.00130901
Iteration 23/25 | Loss: 0.00130517
Iteration 24/25 | Loss: 0.00130118
Iteration 25/25 | Loss: 0.00129833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40396678
Iteration 2/25 | Loss: 0.00128849
Iteration 3/25 | Loss: 0.00116627
Iteration 4/25 | Loss: 0.00116626
Iteration 5/25 | Loss: 0.00116626
Iteration 6/25 | Loss: 0.00116626
Iteration 7/25 | Loss: 0.00116626
Iteration 8/25 | Loss: 0.00116626
Iteration 9/25 | Loss: 0.00116626
Iteration 10/25 | Loss: 0.00116626
Iteration 11/25 | Loss: 0.00116626
Iteration 12/25 | Loss: 0.00116626
Iteration 13/25 | Loss: 0.00116626
Iteration 14/25 | Loss: 0.00116626
Iteration 15/25 | Loss: 0.00116626
Iteration 16/25 | Loss: 0.00116626
Iteration 17/25 | Loss: 0.00116626
Iteration 18/25 | Loss: 0.00116626
Iteration 19/25 | Loss: 0.00116626
Iteration 20/25 | Loss: 0.00116626
Iteration 21/25 | Loss: 0.00116626
Iteration 22/25 | Loss: 0.00116626
Iteration 23/25 | Loss: 0.00116626
Iteration 24/25 | Loss: 0.00116626
Iteration 25/25 | Loss: 0.00116626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116626
Iteration 2/1000 | Loss: 0.00023125
Iteration 3/1000 | Loss: 0.00008562
Iteration 4/1000 | Loss: 0.00015557
Iteration 5/1000 | Loss: 0.00042550
Iteration 6/1000 | Loss: 0.00014736
Iteration 7/1000 | Loss: 0.00022113
Iteration 8/1000 | Loss: 0.00029834
Iteration 9/1000 | Loss: 0.00019850
Iteration 10/1000 | Loss: 0.00016280
Iteration 11/1000 | Loss: 0.00007369
Iteration 12/1000 | Loss: 0.00006694
Iteration 13/1000 | Loss: 0.00010401
Iteration 14/1000 | Loss: 0.00011373
Iteration 15/1000 | Loss: 0.00005840
Iteration 16/1000 | Loss: 0.00005609
Iteration 17/1000 | Loss: 0.00009982
Iteration 18/1000 | Loss: 0.00007372
Iteration 19/1000 | Loss: 0.00008393
Iteration 20/1000 | Loss: 0.00014745
Iteration 21/1000 | Loss: 0.00030127
Iteration 22/1000 | Loss: 0.00024576
Iteration 23/1000 | Loss: 0.00070542
Iteration 24/1000 | Loss: 0.00005802
Iteration 25/1000 | Loss: 0.00005494
Iteration 26/1000 | Loss: 0.00009103
Iteration 27/1000 | Loss: 0.00021563
Iteration 28/1000 | Loss: 0.00021657
Iteration 29/1000 | Loss: 0.00021885
Iteration 30/1000 | Loss: 0.00019073
Iteration 31/1000 | Loss: 0.00005504
Iteration 32/1000 | Loss: 0.00007373
Iteration 33/1000 | Loss: 0.00022179
Iteration 34/1000 | Loss: 0.00017008
Iteration 35/1000 | Loss: 0.00009883
Iteration 36/1000 | Loss: 0.00007396
Iteration 37/1000 | Loss: 0.00005343
Iteration 38/1000 | Loss: 0.00007343
Iteration 39/1000 | Loss: 0.00007094
Iteration 40/1000 | Loss: 0.00005574
Iteration 41/1000 | Loss: 0.00005351
Iteration 42/1000 | Loss: 0.00010457
Iteration 43/1000 | Loss: 0.00010329
Iteration 44/1000 | Loss: 0.00015689
Iteration 45/1000 | Loss: 0.00011455
Iteration 46/1000 | Loss: 0.00012541
Iteration 47/1000 | Loss: 0.00010959
Iteration 48/1000 | Loss: 0.00026693
Iteration 49/1000 | Loss: 0.00037955
Iteration 50/1000 | Loss: 0.00018881
Iteration 51/1000 | Loss: 0.00017297
Iteration 52/1000 | Loss: 0.00018675
Iteration 53/1000 | Loss: 0.00015691
Iteration 54/1000 | Loss: 0.00010276
Iteration 55/1000 | Loss: 0.00011399
Iteration 56/1000 | Loss: 0.00015918
Iteration 57/1000 | Loss: 0.00056612
Iteration 58/1000 | Loss: 0.00024579
Iteration 59/1000 | Loss: 0.00010031
Iteration 60/1000 | Loss: 0.00005150
Iteration 61/1000 | Loss: 0.00025834
Iteration 62/1000 | Loss: 0.00022241
Iteration 63/1000 | Loss: 0.00026434
Iteration 64/1000 | Loss: 0.00058604
Iteration 65/1000 | Loss: 0.00009473
Iteration 66/1000 | Loss: 0.00009445
Iteration 67/1000 | Loss: 0.00005217
Iteration 68/1000 | Loss: 0.00004932
Iteration 69/1000 | Loss: 0.00004871
Iteration 70/1000 | Loss: 0.00023326
Iteration 71/1000 | Loss: 0.00043504
Iteration 72/1000 | Loss: 0.00067357
Iteration 73/1000 | Loss: 0.00071841
Iteration 74/1000 | Loss: 0.00006683
Iteration 75/1000 | Loss: 0.00025880
Iteration 76/1000 | Loss: 0.00025297
Iteration 77/1000 | Loss: 0.00030132
Iteration 78/1000 | Loss: 0.00024207
Iteration 79/1000 | Loss: 0.00020642
Iteration 80/1000 | Loss: 0.00019345
Iteration 81/1000 | Loss: 0.00005140
Iteration 82/1000 | Loss: 0.00012560
Iteration 83/1000 | Loss: 0.00024631
Iteration 84/1000 | Loss: 0.00022092
Iteration 85/1000 | Loss: 0.00020975
Iteration 86/1000 | Loss: 0.00021502
Iteration 87/1000 | Loss: 0.00018797
Iteration 88/1000 | Loss: 0.00004835
Iteration 89/1000 | Loss: 0.00009770
Iteration 90/1000 | Loss: 0.00004183
Iteration 91/1000 | Loss: 0.00004082
Iteration 92/1000 | Loss: 0.00004002
Iteration 93/1000 | Loss: 0.00003951
Iteration 94/1000 | Loss: 0.00110589
Iteration 95/1000 | Loss: 0.00005129
Iteration 96/1000 | Loss: 0.00004168
Iteration 97/1000 | Loss: 0.00003675
Iteration 98/1000 | Loss: 0.00003489
Iteration 99/1000 | Loss: 0.00003362
Iteration 100/1000 | Loss: 0.00003291
Iteration 101/1000 | Loss: 0.00003245
Iteration 102/1000 | Loss: 0.00003211
Iteration 103/1000 | Loss: 0.00003172
Iteration 104/1000 | Loss: 0.00003138
Iteration 105/1000 | Loss: 0.00003103
Iteration 106/1000 | Loss: 0.00003076
Iteration 107/1000 | Loss: 0.00095149
Iteration 108/1000 | Loss: 0.00098574
Iteration 109/1000 | Loss: 0.00003487
Iteration 110/1000 | Loss: 0.00003113
Iteration 111/1000 | Loss: 0.00003054
Iteration 112/1000 | Loss: 0.00098102
Iteration 113/1000 | Loss: 0.00008029
Iteration 114/1000 | Loss: 0.00004915
Iteration 115/1000 | Loss: 0.00004340
Iteration 116/1000 | Loss: 0.00004019
Iteration 117/1000 | Loss: 0.00003623
Iteration 118/1000 | Loss: 0.00003315
Iteration 119/1000 | Loss: 0.00003082
Iteration 120/1000 | Loss: 0.00002952
Iteration 121/1000 | Loss: 0.00002868
Iteration 122/1000 | Loss: 0.00002836
Iteration 123/1000 | Loss: 0.00002810
Iteration 124/1000 | Loss: 0.00002807
Iteration 125/1000 | Loss: 0.00002793
Iteration 126/1000 | Loss: 0.00002778
Iteration 127/1000 | Loss: 0.00002770
Iteration 128/1000 | Loss: 0.00002766
Iteration 129/1000 | Loss: 0.00002761
Iteration 130/1000 | Loss: 0.00002761
Iteration 131/1000 | Loss: 0.00002761
Iteration 132/1000 | Loss: 0.00002760
Iteration 133/1000 | Loss: 0.00002760
Iteration 134/1000 | Loss: 0.00002760
Iteration 135/1000 | Loss: 0.00002759
Iteration 136/1000 | Loss: 0.00002759
Iteration 137/1000 | Loss: 0.00002759
Iteration 138/1000 | Loss: 0.00002759
Iteration 139/1000 | Loss: 0.00002759
Iteration 140/1000 | Loss: 0.00002759
Iteration 141/1000 | Loss: 0.00002759
Iteration 142/1000 | Loss: 0.00002759
Iteration 143/1000 | Loss: 0.00002759
Iteration 144/1000 | Loss: 0.00002759
Iteration 145/1000 | Loss: 0.00002759
Iteration 146/1000 | Loss: 0.00002759
Iteration 147/1000 | Loss: 0.00002759
Iteration 148/1000 | Loss: 0.00002759
Iteration 149/1000 | Loss: 0.00002759
Iteration 150/1000 | Loss: 0.00002758
Iteration 151/1000 | Loss: 0.00002758
Iteration 152/1000 | Loss: 0.00002758
Iteration 153/1000 | Loss: 0.00002758
Iteration 154/1000 | Loss: 0.00002758
Iteration 155/1000 | Loss: 0.00002758
Iteration 156/1000 | Loss: 0.00002757
Iteration 157/1000 | Loss: 0.00002757
Iteration 158/1000 | Loss: 0.00002757
Iteration 159/1000 | Loss: 0.00002757
Iteration 160/1000 | Loss: 0.00002757
Iteration 161/1000 | Loss: 0.00002757
Iteration 162/1000 | Loss: 0.00002756
Iteration 163/1000 | Loss: 0.00002756
Iteration 164/1000 | Loss: 0.00002756
Iteration 165/1000 | Loss: 0.00002756
Iteration 166/1000 | Loss: 0.00002756
Iteration 167/1000 | Loss: 0.00002756
Iteration 168/1000 | Loss: 0.00002756
Iteration 169/1000 | Loss: 0.00002755
Iteration 170/1000 | Loss: 0.00002755
Iteration 171/1000 | Loss: 0.00002755
Iteration 172/1000 | Loss: 0.00002755
Iteration 173/1000 | Loss: 0.00002755
Iteration 174/1000 | Loss: 0.00002755
Iteration 175/1000 | Loss: 0.00002755
Iteration 176/1000 | Loss: 0.00002754
Iteration 177/1000 | Loss: 0.00002754
Iteration 178/1000 | Loss: 0.00002754
Iteration 179/1000 | Loss: 0.00002754
Iteration 180/1000 | Loss: 0.00002754
Iteration 181/1000 | Loss: 0.00002754
Iteration 182/1000 | Loss: 0.00002754
Iteration 183/1000 | Loss: 0.00002754
Iteration 184/1000 | Loss: 0.00002754
Iteration 185/1000 | Loss: 0.00002754
Iteration 186/1000 | Loss: 0.00002753
Iteration 187/1000 | Loss: 0.00002753
Iteration 188/1000 | Loss: 0.00002753
Iteration 189/1000 | Loss: 0.00002753
Iteration 190/1000 | Loss: 0.00002753
Iteration 191/1000 | Loss: 0.00002753
Iteration 192/1000 | Loss: 0.00002752
Iteration 193/1000 | Loss: 0.00002752
Iteration 194/1000 | Loss: 0.00002752
Iteration 195/1000 | Loss: 0.00002752
Iteration 196/1000 | Loss: 0.00002752
Iteration 197/1000 | Loss: 0.00002752
Iteration 198/1000 | Loss: 0.00002752
Iteration 199/1000 | Loss: 0.00002752
Iteration 200/1000 | Loss: 0.00002752
Iteration 201/1000 | Loss: 0.00002752
Iteration 202/1000 | Loss: 0.00002752
Iteration 203/1000 | Loss: 0.00002752
Iteration 204/1000 | Loss: 0.00002752
Iteration 205/1000 | Loss: 0.00002752
Iteration 206/1000 | Loss: 0.00002752
Iteration 207/1000 | Loss: 0.00002752
Iteration 208/1000 | Loss: 0.00002752
Iteration 209/1000 | Loss: 0.00002752
Iteration 210/1000 | Loss: 0.00002752
Iteration 211/1000 | Loss: 0.00002752
Iteration 212/1000 | Loss: 0.00002752
Iteration 213/1000 | Loss: 0.00002752
Iteration 214/1000 | Loss: 0.00002752
Iteration 215/1000 | Loss: 0.00002752
Iteration 216/1000 | Loss: 0.00002752
Iteration 217/1000 | Loss: 0.00002752
Iteration 218/1000 | Loss: 0.00002752
Iteration 219/1000 | Loss: 0.00002752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [2.7518377464730293e-05, 2.7518377464730293e-05, 2.7518377464730293e-05, 2.7518377464730293e-05, 2.7518377464730293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7518377464730293e-05

Optimization complete. Final v2v error: 4.447788238525391 mm

Highest mean error: 6.6086859703063965 mm for frame 146

Lowest mean error: 3.407435178756714 mm for frame 153

Saving results

Total time: 221.70455956459045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996798
Iteration 2/25 | Loss: 0.00305496
Iteration 3/25 | Loss: 0.00218917
Iteration 4/25 | Loss: 0.00206468
Iteration 5/25 | Loss: 0.00183298
Iteration 6/25 | Loss: 0.00163960
Iteration 7/25 | Loss: 0.00146900
Iteration 8/25 | Loss: 0.00137379
Iteration 9/25 | Loss: 0.00133780
Iteration 10/25 | Loss: 0.00131088
Iteration 11/25 | Loss: 0.00129115
Iteration 12/25 | Loss: 0.00127261
Iteration 13/25 | Loss: 0.00127122
Iteration 14/25 | Loss: 0.00126232
Iteration 15/25 | Loss: 0.00125885
Iteration 16/25 | Loss: 0.00124719
Iteration 17/25 | Loss: 0.00125081
Iteration 18/25 | Loss: 0.00125458
Iteration 19/25 | Loss: 0.00125275
Iteration 20/25 | Loss: 0.00124717
Iteration 21/25 | Loss: 0.00124811
Iteration 22/25 | Loss: 0.00124039
Iteration 23/25 | Loss: 0.00124226
Iteration 24/25 | Loss: 0.00123733
Iteration 25/25 | Loss: 0.00124034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40789127
Iteration 2/25 | Loss: 0.00078010
Iteration 3/25 | Loss: 0.00076842
Iteration 4/25 | Loss: 0.00076842
Iteration 5/25 | Loss: 0.00076842
Iteration 6/25 | Loss: 0.00076842
Iteration 7/25 | Loss: 0.00076842
Iteration 8/25 | Loss: 0.00076841
Iteration 9/25 | Loss: 0.00076841
Iteration 10/25 | Loss: 0.00076841
Iteration 11/25 | Loss: 0.00076841
Iteration 12/25 | Loss: 0.00076841
Iteration 13/25 | Loss: 0.00076841
Iteration 14/25 | Loss: 0.00076841
Iteration 15/25 | Loss: 0.00076841
Iteration 16/25 | Loss: 0.00076841
Iteration 17/25 | Loss: 0.00076841
Iteration 18/25 | Loss: 0.00076841
Iteration 19/25 | Loss: 0.00076841
Iteration 20/25 | Loss: 0.00076841
Iteration 21/25 | Loss: 0.00076841
Iteration 22/25 | Loss: 0.00076841
Iteration 23/25 | Loss: 0.00076841
Iteration 24/25 | Loss: 0.00076841
Iteration 25/25 | Loss: 0.00076841

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076841
Iteration 2/1000 | Loss: 0.00004908
Iteration 3/1000 | Loss: 0.00005377
Iteration 4/1000 | Loss: 0.00006572
Iteration 5/1000 | Loss: 0.00003481
Iteration 6/1000 | Loss: 0.00002494
Iteration 7/1000 | Loss: 0.00002242
Iteration 8/1000 | Loss: 0.00006177
Iteration 9/1000 | Loss: 0.00004264
Iteration 10/1000 | Loss: 0.00003542
Iteration 11/1000 | Loss: 0.00004043
Iteration 12/1000 | Loss: 0.00003655
Iteration 13/1000 | Loss: 0.00004319
Iteration 14/1000 | Loss: 0.00003432
Iteration 15/1000 | Loss: 0.00004041
Iteration 16/1000 | Loss: 0.00002920
Iteration 17/1000 | Loss: 0.00002501
Iteration 18/1000 | Loss: 0.00002878
Iteration 19/1000 | Loss: 0.00002156
Iteration 20/1000 | Loss: 0.00002018
Iteration 21/1000 | Loss: 0.00003420
Iteration 22/1000 | Loss: 0.00003442
Iteration 23/1000 | Loss: 0.00002273
Iteration 24/1000 | Loss: 0.00001901
Iteration 25/1000 | Loss: 0.00001858
Iteration 26/1000 | Loss: 0.00002995
Iteration 27/1000 | Loss: 0.00042665
Iteration 28/1000 | Loss: 0.00033830
Iteration 29/1000 | Loss: 0.00021286
Iteration 30/1000 | Loss: 0.00021021
Iteration 31/1000 | Loss: 0.00023284
Iteration 32/1000 | Loss: 0.00021570
Iteration 33/1000 | Loss: 0.00024061
Iteration 34/1000 | Loss: 0.00003017
Iteration 35/1000 | Loss: 0.00002053
Iteration 36/1000 | Loss: 0.00003951
Iteration 37/1000 | Loss: 0.00001718
Iteration 38/1000 | Loss: 0.00003625
Iteration 39/1000 | Loss: 0.00001624
Iteration 40/1000 | Loss: 0.00001604
Iteration 41/1000 | Loss: 0.00001587
Iteration 42/1000 | Loss: 0.00001586
Iteration 43/1000 | Loss: 0.00001574
Iteration 44/1000 | Loss: 0.00001569
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001558
Iteration 47/1000 | Loss: 0.00001548
Iteration 48/1000 | Loss: 0.00001545
Iteration 49/1000 | Loss: 0.00001545
Iteration 50/1000 | Loss: 0.00001544
Iteration 51/1000 | Loss: 0.00001543
Iteration 52/1000 | Loss: 0.00001543
Iteration 53/1000 | Loss: 0.00001542
Iteration 54/1000 | Loss: 0.00001541
Iteration 55/1000 | Loss: 0.00001541
Iteration 56/1000 | Loss: 0.00001541
Iteration 57/1000 | Loss: 0.00001540
Iteration 58/1000 | Loss: 0.00001540
Iteration 59/1000 | Loss: 0.00001540
Iteration 60/1000 | Loss: 0.00001540
Iteration 61/1000 | Loss: 0.00001539
Iteration 62/1000 | Loss: 0.00001539
Iteration 63/1000 | Loss: 0.00001539
Iteration 64/1000 | Loss: 0.00001538
Iteration 65/1000 | Loss: 0.00001538
Iteration 66/1000 | Loss: 0.00001535
Iteration 67/1000 | Loss: 0.00001534
Iteration 68/1000 | Loss: 0.00001534
Iteration 69/1000 | Loss: 0.00001533
Iteration 70/1000 | Loss: 0.00001533
Iteration 71/1000 | Loss: 0.00003791
Iteration 72/1000 | Loss: 0.00001758
Iteration 73/1000 | Loss: 0.00001723
Iteration 74/1000 | Loss: 0.00001532
Iteration 75/1000 | Loss: 0.00001531
Iteration 76/1000 | Loss: 0.00001531
Iteration 77/1000 | Loss: 0.00001531
Iteration 78/1000 | Loss: 0.00001531
Iteration 79/1000 | Loss: 0.00001531
Iteration 80/1000 | Loss: 0.00001531
Iteration 81/1000 | Loss: 0.00001531
Iteration 82/1000 | Loss: 0.00001531
Iteration 83/1000 | Loss: 0.00001530
Iteration 84/1000 | Loss: 0.00001529
Iteration 85/1000 | Loss: 0.00001529
Iteration 86/1000 | Loss: 0.00001529
Iteration 87/1000 | Loss: 0.00001529
Iteration 88/1000 | Loss: 0.00001528
Iteration 89/1000 | Loss: 0.00001528
Iteration 90/1000 | Loss: 0.00001528
Iteration 91/1000 | Loss: 0.00001528
Iteration 92/1000 | Loss: 0.00001528
Iteration 93/1000 | Loss: 0.00001528
Iteration 94/1000 | Loss: 0.00001528
Iteration 95/1000 | Loss: 0.00001528
Iteration 96/1000 | Loss: 0.00001528
Iteration 97/1000 | Loss: 0.00001528
Iteration 98/1000 | Loss: 0.00001528
Iteration 99/1000 | Loss: 0.00001528
Iteration 100/1000 | Loss: 0.00001528
Iteration 101/1000 | Loss: 0.00001528
Iteration 102/1000 | Loss: 0.00001527
Iteration 103/1000 | Loss: 0.00001527
Iteration 104/1000 | Loss: 0.00001527
Iteration 105/1000 | Loss: 0.00001527
Iteration 106/1000 | Loss: 0.00001527
Iteration 107/1000 | Loss: 0.00001526
Iteration 108/1000 | Loss: 0.00001526
Iteration 109/1000 | Loss: 0.00001526
Iteration 110/1000 | Loss: 0.00001526
Iteration 111/1000 | Loss: 0.00001526
Iteration 112/1000 | Loss: 0.00001526
Iteration 113/1000 | Loss: 0.00001526
Iteration 114/1000 | Loss: 0.00001526
Iteration 115/1000 | Loss: 0.00001525
Iteration 116/1000 | Loss: 0.00001525
Iteration 117/1000 | Loss: 0.00001525
Iteration 118/1000 | Loss: 0.00001525
Iteration 119/1000 | Loss: 0.00001525
Iteration 120/1000 | Loss: 0.00001525
Iteration 121/1000 | Loss: 0.00001525
Iteration 122/1000 | Loss: 0.00001525
Iteration 123/1000 | Loss: 0.00001525
Iteration 124/1000 | Loss: 0.00001524
Iteration 125/1000 | Loss: 0.00001524
Iteration 126/1000 | Loss: 0.00001524
Iteration 127/1000 | Loss: 0.00001524
Iteration 128/1000 | Loss: 0.00001524
Iteration 129/1000 | Loss: 0.00001524
Iteration 130/1000 | Loss: 0.00001523
Iteration 131/1000 | Loss: 0.00001523
Iteration 132/1000 | Loss: 0.00001523
Iteration 133/1000 | Loss: 0.00001523
Iteration 134/1000 | Loss: 0.00001523
Iteration 135/1000 | Loss: 0.00001523
Iteration 136/1000 | Loss: 0.00001523
Iteration 137/1000 | Loss: 0.00001523
Iteration 138/1000 | Loss: 0.00001523
Iteration 139/1000 | Loss: 0.00001523
Iteration 140/1000 | Loss: 0.00001523
Iteration 141/1000 | Loss: 0.00001523
Iteration 142/1000 | Loss: 0.00001523
Iteration 143/1000 | Loss: 0.00001523
Iteration 144/1000 | Loss: 0.00001523
Iteration 145/1000 | Loss: 0.00001523
Iteration 146/1000 | Loss: 0.00001523
Iteration 147/1000 | Loss: 0.00001523
Iteration 148/1000 | Loss: 0.00001523
Iteration 149/1000 | Loss: 0.00001523
Iteration 150/1000 | Loss: 0.00001523
Iteration 151/1000 | Loss: 0.00001523
Iteration 152/1000 | Loss: 0.00001523
Iteration 153/1000 | Loss: 0.00001523
Iteration 154/1000 | Loss: 0.00001523
Iteration 155/1000 | Loss: 0.00001523
Iteration 156/1000 | Loss: 0.00001523
Iteration 157/1000 | Loss: 0.00001523
Iteration 158/1000 | Loss: 0.00001523
Iteration 159/1000 | Loss: 0.00001523
Iteration 160/1000 | Loss: 0.00001523
Iteration 161/1000 | Loss: 0.00001523
Iteration 162/1000 | Loss: 0.00001523
Iteration 163/1000 | Loss: 0.00001523
Iteration 164/1000 | Loss: 0.00001523
Iteration 165/1000 | Loss: 0.00001523
Iteration 166/1000 | Loss: 0.00001523
Iteration 167/1000 | Loss: 0.00001523
Iteration 168/1000 | Loss: 0.00001523
Iteration 169/1000 | Loss: 0.00001523
Iteration 170/1000 | Loss: 0.00001523
Iteration 171/1000 | Loss: 0.00001523
Iteration 172/1000 | Loss: 0.00001523
Iteration 173/1000 | Loss: 0.00001523
Iteration 174/1000 | Loss: 0.00001523
Iteration 175/1000 | Loss: 0.00001523
Iteration 176/1000 | Loss: 0.00001523
Iteration 177/1000 | Loss: 0.00001523
Iteration 178/1000 | Loss: 0.00001523
Iteration 179/1000 | Loss: 0.00001523
Iteration 180/1000 | Loss: 0.00001523
Iteration 181/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.5225438801280688e-05, 1.5225438801280688e-05, 1.5225438801280688e-05, 1.5225438801280688e-05, 1.5225438801280688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5225438801280688e-05

Optimization complete. Final v2v error: 3.2087690830230713 mm

Highest mean error: 6.394410133361816 mm for frame 106

Lowest mean error: 2.8132448196411133 mm for frame 83

Saving results

Total time: 113.76855111122131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526909
Iteration 2/25 | Loss: 0.00141471
Iteration 3/25 | Loss: 0.00130059
Iteration 4/25 | Loss: 0.00129432
Iteration 5/25 | Loss: 0.00129271
Iteration 6/25 | Loss: 0.00129271
Iteration 7/25 | Loss: 0.00129271
Iteration 8/25 | Loss: 0.00129271
Iteration 9/25 | Loss: 0.00129271
Iteration 10/25 | Loss: 0.00129271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001292714849114418, 0.001292714849114418, 0.001292714849114418, 0.001292714849114418, 0.001292714849114418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001292714849114418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86950856
Iteration 2/25 | Loss: 0.00072580
Iteration 3/25 | Loss: 0.00072579
Iteration 4/25 | Loss: 0.00072579
Iteration 5/25 | Loss: 0.00072579
Iteration 6/25 | Loss: 0.00072578
Iteration 7/25 | Loss: 0.00072578
Iteration 8/25 | Loss: 0.00072578
Iteration 9/25 | Loss: 0.00072578
Iteration 10/25 | Loss: 0.00072578
Iteration 11/25 | Loss: 0.00072578
Iteration 12/25 | Loss: 0.00072578
Iteration 13/25 | Loss: 0.00072578
Iteration 14/25 | Loss: 0.00072578
Iteration 15/25 | Loss: 0.00072578
Iteration 16/25 | Loss: 0.00072578
Iteration 17/25 | Loss: 0.00072578
Iteration 18/25 | Loss: 0.00072578
Iteration 19/25 | Loss: 0.00072578
Iteration 20/25 | Loss: 0.00072578
Iteration 21/25 | Loss: 0.00072578
Iteration 22/25 | Loss: 0.00072578
Iteration 23/25 | Loss: 0.00072578
Iteration 24/25 | Loss: 0.00072578
Iteration 25/25 | Loss: 0.00072578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072578
Iteration 2/1000 | Loss: 0.00003030
Iteration 3/1000 | Loss: 0.00002040
Iteration 4/1000 | Loss: 0.00001877
Iteration 5/1000 | Loss: 0.00001787
Iteration 6/1000 | Loss: 0.00001735
Iteration 7/1000 | Loss: 0.00001699
Iteration 8/1000 | Loss: 0.00001677
Iteration 9/1000 | Loss: 0.00001655
Iteration 10/1000 | Loss: 0.00001639
Iteration 11/1000 | Loss: 0.00001638
Iteration 12/1000 | Loss: 0.00001634
Iteration 13/1000 | Loss: 0.00001616
Iteration 14/1000 | Loss: 0.00001615
Iteration 15/1000 | Loss: 0.00001612
Iteration 16/1000 | Loss: 0.00001586
Iteration 17/1000 | Loss: 0.00001585
Iteration 18/1000 | Loss: 0.00001584
Iteration 19/1000 | Loss: 0.00001584
Iteration 20/1000 | Loss: 0.00001584
Iteration 21/1000 | Loss: 0.00001583
Iteration 22/1000 | Loss: 0.00001581
Iteration 23/1000 | Loss: 0.00001579
Iteration 24/1000 | Loss: 0.00001579
Iteration 25/1000 | Loss: 0.00001579
Iteration 26/1000 | Loss: 0.00001579
Iteration 27/1000 | Loss: 0.00001579
Iteration 28/1000 | Loss: 0.00001579
Iteration 29/1000 | Loss: 0.00001579
Iteration 30/1000 | Loss: 0.00001579
Iteration 31/1000 | Loss: 0.00001579
Iteration 32/1000 | Loss: 0.00001579
Iteration 33/1000 | Loss: 0.00001575
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001573
Iteration 36/1000 | Loss: 0.00001572
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001569
Iteration 39/1000 | Loss: 0.00001564
Iteration 40/1000 | Loss: 0.00001563
Iteration 41/1000 | Loss: 0.00001562
Iteration 42/1000 | Loss: 0.00001561
Iteration 43/1000 | Loss: 0.00001561
Iteration 44/1000 | Loss: 0.00001561
Iteration 45/1000 | Loss: 0.00001560
Iteration 46/1000 | Loss: 0.00001560
Iteration 47/1000 | Loss: 0.00001560
Iteration 48/1000 | Loss: 0.00001560
Iteration 49/1000 | Loss: 0.00001560
Iteration 50/1000 | Loss: 0.00001560
Iteration 51/1000 | Loss: 0.00001560
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001560
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001558
Iteration 56/1000 | Loss: 0.00001558
Iteration 57/1000 | Loss: 0.00001558
Iteration 58/1000 | Loss: 0.00001558
Iteration 59/1000 | Loss: 0.00001558
Iteration 60/1000 | Loss: 0.00001558
Iteration 61/1000 | Loss: 0.00001558
Iteration 62/1000 | Loss: 0.00001558
Iteration 63/1000 | Loss: 0.00001558
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001557
Iteration 66/1000 | Loss: 0.00001557
Iteration 67/1000 | Loss: 0.00001556
Iteration 68/1000 | Loss: 0.00001556
Iteration 69/1000 | Loss: 0.00001554
Iteration 70/1000 | Loss: 0.00001554
Iteration 71/1000 | Loss: 0.00001554
Iteration 72/1000 | Loss: 0.00001554
Iteration 73/1000 | Loss: 0.00001554
Iteration 74/1000 | Loss: 0.00001554
Iteration 75/1000 | Loss: 0.00001554
Iteration 76/1000 | Loss: 0.00001554
Iteration 77/1000 | Loss: 0.00001553
Iteration 78/1000 | Loss: 0.00001552
Iteration 79/1000 | Loss: 0.00001552
Iteration 80/1000 | Loss: 0.00001552
Iteration 81/1000 | Loss: 0.00001551
Iteration 82/1000 | Loss: 0.00001551
Iteration 83/1000 | Loss: 0.00001551
Iteration 84/1000 | Loss: 0.00001551
Iteration 85/1000 | Loss: 0.00001551
Iteration 86/1000 | Loss: 0.00001551
Iteration 87/1000 | Loss: 0.00001551
Iteration 88/1000 | Loss: 0.00001551
Iteration 89/1000 | Loss: 0.00001551
Iteration 90/1000 | Loss: 0.00001551
Iteration 91/1000 | Loss: 0.00001551
Iteration 92/1000 | Loss: 0.00001551
Iteration 93/1000 | Loss: 0.00001551
Iteration 94/1000 | Loss: 0.00001551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.5505031115026213e-05, 1.5505031115026213e-05, 1.5505031115026213e-05, 1.5505031115026213e-05, 1.5505031115026213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5505031115026213e-05

Optimization complete. Final v2v error: 3.324221611022949 mm

Highest mean error: 3.444039821624756 mm for frame 5

Lowest mean error: 3.238370656967163 mm for frame 168

Saving results

Total time: 31.313288927078247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017597
Iteration 2/25 | Loss: 0.00307569
Iteration 3/25 | Loss: 0.00186483
Iteration 4/25 | Loss: 0.00172804
Iteration 5/25 | Loss: 0.00159000
Iteration 6/25 | Loss: 0.00149612
Iteration 7/25 | Loss: 0.00146957
Iteration 8/25 | Loss: 0.00139534
Iteration 9/25 | Loss: 0.00139188
Iteration 10/25 | Loss: 0.00135634
Iteration 11/25 | Loss: 0.00132309
Iteration 12/25 | Loss: 0.00130922
Iteration 13/25 | Loss: 0.00129623
Iteration 14/25 | Loss: 0.00130315
Iteration 15/25 | Loss: 0.00134854
Iteration 16/25 | Loss: 0.00139407
Iteration 17/25 | Loss: 0.00135421
Iteration 18/25 | Loss: 0.00131478
Iteration 19/25 | Loss: 0.00130008
Iteration 20/25 | Loss: 0.00126919
Iteration 21/25 | Loss: 0.00124292
Iteration 22/25 | Loss: 0.00124154
Iteration 23/25 | Loss: 0.00124085
Iteration 24/25 | Loss: 0.00123434
Iteration 25/25 | Loss: 0.00122635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.49861765
Iteration 2/25 | Loss: 0.00099279
Iteration 3/25 | Loss: 0.00094624
Iteration 4/25 | Loss: 0.00094624
Iteration 5/25 | Loss: 0.00094624
Iteration 6/25 | Loss: 0.00094624
Iteration 7/25 | Loss: 0.00094624
Iteration 8/25 | Loss: 0.00094624
Iteration 9/25 | Loss: 0.00094624
Iteration 10/25 | Loss: 0.00094624
Iteration 11/25 | Loss: 0.00094624
Iteration 12/25 | Loss: 0.00094624
Iteration 13/25 | Loss: 0.00094624
Iteration 14/25 | Loss: 0.00094624
Iteration 15/25 | Loss: 0.00094624
Iteration 16/25 | Loss: 0.00094624
Iteration 17/25 | Loss: 0.00094624
Iteration 18/25 | Loss: 0.00094624
Iteration 19/25 | Loss: 0.00094624
Iteration 20/25 | Loss: 0.00094624
Iteration 21/25 | Loss: 0.00094624
Iteration 22/25 | Loss: 0.00094624
Iteration 23/25 | Loss: 0.00094624
Iteration 24/25 | Loss: 0.00094624
Iteration 25/25 | Loss: 0.00094624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094624
Iteration 2/1000 | Loss: 0.00020430
Iteration 3/1000 | Loss: 0.00007978
Iteration 4/1000 | Loss: 0.00053121
Iteration 5/1000 | Loss: 0.00006989
Iteration 6/1000 | Loss: 0.00004812
Iteration 7/1000 | Loss: 0.00009415
Iteration 8/1000 | Loss: 0.00005390
Iteration 9/1000 | Loss: 0.00008137
Iteration 10/1000 | Loss: 0.00012752
Iteration 11/1000 | Loss: 0.00019194
Iteration 12/1000 | Loss: 0.00013161
Iteration 13/1000 | Loss: 0.00004247
Iteration 14/1000 | Loss: 0.00009991
Iteration 15/1000 | Loss: 0.00025019
Iteration 16/1000 | Loss: 0.00012409
Iteration 17/1000 | Loss: 0.00018026
Iteration 18/1000 | Loss: 0.00006965
Iteration 19/1000 | Loss: 0.00010764
Iteration 20/1000 | Loss: 0.00017574
Iteration 21/1000 | Loss: 0.00014424
Iteration 22/1000 | Loss: 0.00011308
Iteration 23/1000 | Loss: 0.00012524
Iteration 24/1000 | Loss: 0.00012381
Iteration 25/1000 | Loss: 0.00007748
Iteration 26/1000 | Loss: 0.00017692
Iteration 27/1000 | Loss: 0.00053799
Iteration 28/1000 | Loss: 0.00045316
Iteration 29/1000 | Loss: 0.00003011
Iteration 30/1000 | Loss: 0.00044932
Iteration 31/1000 | Loss: 0.00032404
Iteration 32/1000 | Loss: 0.00005955
Iteration 33/1000 | Loss: 0.00004044
Iteration 34/1000 | Loss: 0.00002427
Iteration 35/1000 | Loss: 0.00005300
Iteration 36/1000 | Loss: 0.00012014
Iteration 37/1000 | Loss: 0.00002623
Iteration 38/1000 | Loss: 0.00002124
Iteration 39/1000 | Loss: 0.00003956
Iteration 40/1000 | Loss: 0.00002052
Iteration 41/1000 | Loss: 0.00015348
Iteration 42/1000 | Loss: 0.00007073
Iteration 43/1000 | Loss: 0.00002656
Iteration 44/1000 | Loss: 0.00002603
Iteration 45/1000 | Loss: 0.00012077
Iteration 46/1000 | Loss: 0.00007922
Iteration 47/1000 | Loss: 0.00035699
Iteration 48/1000 | Loss: 0.00001930
Iteration 49/1000 | Loss: 0.00001852
Iteration 50/1000 | Loss: 0.00001813
Iteration 51/1000 | Loss: 0.00001786
Iteration 52/1000 | Loss: 0.00014301
Iteration 53/1000 | Loss: 0.00001774
Iteration 54/1000 | Loss: 0.00007075
Iteration 55/1000 | Loss: 0.00029109
Iteration 56/1000 | Loss: 0.00001780
Iteration 57/1000 | Loss: 0.00002009
Iteration 58/1000 | Loss: 0.00001728
Iteration 59/1000 | Loss: 0.00001727
Iteration 60/1000 | Loss: 0.00001726
Iteration 61/1000 | Loss: 0.00001726
Iteration 62/1000 | Loss: 0.00001725
Iteration 63/1000 | Loss: 0.00001715
Iteration 64/1000 | Loss: 0.00003904
Iteration 65/1000 | Loss: 0.00001761
Iteration 66/1000 | Loss: 0.00002787
Iteration 67/1000 | Loss: 0.00010411
Iteration 68/1000 | Loss: 0.00002335
Iteration 69/1000 | Loss: 0.00001701
Iteration 70/1000 | Loss: 0.00001770
Iteration 71/1000 | Loss: 0.00001686
Iteration 72/1000 | Loss: 0.00001845
Iteration 73/1000 | Loss: 0.00001677
Iteration 74/1000 | Loss: 0.00001677
Iteration 75/1000 | Loss: 0.00001676
Iteration 76/1000 | Loss: 0.00001676
Iteration 77/1000 | Loss: 0.00001676
Iteration 78/1000 | Loss: 0.00001676
Iteration 79/1000 | Loss: 0.00001676
Iteration 80/1000 | Loss: 0.00001676
Iteration 81/1000 | Loss: 0.00001675
Iteration 82/1000 | Loss: 0.00001674
Iteration 83/1000 | Loss: 0.00001678
Iteration 84/1000 | Loss: 0.00001669
Iteration 85/1000 | Loss: 0.00005229
Iteration 86/1000 | Loss: 0.00001659
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001656
Iteration 90/1000 | Loss: 0.00001656
Iteration 91/1000 | Loss: 0.00001655
Iteration 92/1000 | Loss: 0.00001655
Iteration 93/1000 | Loss: 0.00001655
Iteration 94/1000 | Loss: 0.00001655
Iteration 95/1000 | Loss: 0.00001654
Iteration 96/1000 | Loss: 0.00001654
Iteration 97/1000 | Loss: 0.00001654
Iteration 98/1000 | Loss: 0.00001653
Iteration 99/1000 | Loss: 0.00001653
Iteration 100/1000 | Loss: 0.00001653
Iteration 101/1000 | Loss: 0.00001652
Iteration 102/1000 | Loss: 0.00001652
Iteration 103/1000 | Loss: 0.00001651
Iteration 104/1000 | Loss: 0.00002598
Iteration 105/1000 | Loss: 0.00001947
Iteration 106/1000 | Loss: 0.00003742
Iteration 107/1000 | Loss: 0.00002512
Iteration 108/1000 | Loss: 0.00004921
Iteration 109/1000 | Loss: 0.00003086
Iteration 110/1000 | Loss: 0.00002131
Iteration 111/1000 | Loss: 0.00001652
Iteration 112/1000 | Loss: 0.00001916
Iteration 113/1000 | Loss: 0.00001647
Iteration 114/1000 | Loss: 0.00001647
Iteration 115/1000 | Loss: 0.00001647
Iteration 116/1000 | Loss: 0.00001647
Iteration 117/1000 | Loss: 0.00001647
Iteration 118/1000 | Loss: 0.00001647
Iteration 119/1000 | Loss: 0.00001647
Iteration 120/1000 | Loss: 0.00001647
Iteration 121/1000 | Loss: 0.00001646
Iteration 122/1000 | Loss: 0.00001646
Iteration 123/1000 | Loss: 0.00001646
Iteration 124/1000 | Loss: 0.00003091
Iteration 125/1000 | Loss: 0.00001851
Iteration 126/1000 | Loss: 0.00001645
Iteration 127/1000 | Loss: 0.00001645
Iteration 128/1000 | Loss: 0.00001645
Iteration 129/1000 | Loss: 0.00001645
Iteration 130/1000 | Loss: 0.00001645
Iteration 131/1000 | Loss: 0.00001645
Iteration 132/1000 | Loss: 0.00001645
Iteration 133/1000 | Loss: 0.00001644
Iteration 134/1000 | Loss: 0.00001644
Iteration 135/1000 | Loss: 0.00001644
Iteration 136/1000 | Loss: 0.00001936
Iteration 137/1000 | Loss: 0.00001645
Iteration 138/1000 | Loss: 0.00001645
Iteration 139/1000 | Loss: 0.00001644
Iteration 140/1000 | Loss: 0.00001644
Iteration 141/1000 | Loss: 0.00001644
Iteration 142/1000 | Loss: 0.00001644
Iteration 143/1000 | Loss: 0.00001644
Iteration 144/1000 | Loss: 0.00001644
Iteration 145/1000 | Loss: 0.00001644
Iteration 146/1000 | Loss: 0.00001644
Iteration 147/1000 | Loss: 0.00001644
Iteration 148/1000 | Loss: 0.00001644
Iteration 149/1000 | Loss: 0.00001644
Iteration 150/1000 | Loss: 0.00001644
Iteration 151/1000 | Loss: 0.00001717
Iteration 152/1000 | Loss: 0.00001652
Iteration 153/1000 | Loss: 0.00001644
Iteration 154/1000 | Loss: 0.00001643
Iteration 155/1000 | Loss: 0.00001643
Iteration 156/1000 | Loss: 0.00001643
Iteration 157/1000 | Loss: 0.00001643
Iteration 158/1000 | Loss: 0.00001643
Iteration 159/1000 | Loss: 0.00001643
Iteration 160/1000 | Loss: 0.00001643
Iteration 161/1000 | Loss: 0.00001643
Iteration 162/1000 | Loss: 0.00001643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.643022733333055e-05, 1.643022733333055e-05, 1.643022733333055e-05, 1.643022733333055e-05, 1.643022733333055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.643022733333055e-05

Optimization complete. Final v2v error: 3.3984975814819336 mm

Highest mean error: 4.495145797729492 mm for frame 70

Lowest mean error: 2.691202163696289 mm for frame 164

Saving results

Total time: 182.50753259658813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465807
Iteration 2/25 | Loss: 0.00137003
Iteration 3/25 | Loss: 0.00127193
Iteration 4/25 | Loss: 0.00126543
Iteration 5/25 | Loss: 0.00126299
Iteration 6/25 | Loss: 0.00126293
Iteration 7/25 | Loss: 0.00126293
Iteration 8/25 | Loss: 0.00126293
Iteration 9/25 | Loss: 0.00126293
Iteration 10/25 | Loss: 0.00126293
Iteration 11/25 | Loss: 0.00126293
Iteration 12/25 | Loss: 0.00126293
Iteration 13/25 | Loss: 0.00126293
Iteration 14/25 | Loss: 0.00126293
Iteration 15/25 | Loss: 0.00126293
Iteration 16/25 | Loss: 0.00126293
Iteration 17/25 | Loss: 0.00126293
Iteration 18/25 | Loss: 0.00126293
Iteration 19/25 | Loss: 0.00126293
Iteration 20/25 | Loss: 0.00126293
Iteration 21/25 | Loss: 0.00126293
Iteration 22/25 | Loss: 0.00126293
Iteration 23/25 | Loss: 0.00126293
Iteration 24/25 | Loss: 0.00126293
Iteration 25/25 | Loss: 0.00126293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55986714
Iteration 2/25 | Loss: 0.00070284
Iteration 3/25 | Loss: 0.00070283
Iteration 4/25 | Loss: 0.00070283
Iteration 5/25 | Loss: 0.00070283
Iteration 6/25 | Loss: 0.00070283
Iteration 7/25 | Loss: 0.00070283
Iteration 8/25 | Loss: 0.00070283
Iteration 9/25 | Loss: 0.00070283
Iteration 10/25 | Loss: 0.00070283
Iteration 11/25 | Loss: 0.00070283
Iteration 12/25 | Loss: 0.00070283
Iteration 13/25 | Loss: 0.00070283
Iteration 14/25 | Loss: 0.00070283
Iteration 15/25 | Loss: 0.00070283
Iteration 16/25 | Loss: 0.00070283
Iteration 17/25 | Loss: 0.00070283
Iteration 18/25 | Loss: 0.00070283
Iteration 19/25 | Loss: 0.00070283
Iteration 20/25 | Loss: 0.00070283
Iteration 21/25 | Loss: 0.00070283
Iteration 22/25 | Loss: 0.00070283
Iteration 23/25 | Loss: 0.00070283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007028260151855648, 0.0007028260151855648, 0.0007028260151855648, 0.0007028260151855648, 0.0007028260151855648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007028260151855648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070283
Iteration 2/1000 | Loss: 0.00003819
Iteration 3/1000 | Loss: 0.00002558
Iteration 4/1000 | Loss: 0.00002336
Iteration 5/1000 | Loss: 0.00002248
Iteration 6/1000 | Loss: 0.00002172
Iteration 7/1000 | Loss: 0.00002105
Iteration 8/1000 | Loss: 0.00002062
Iteration 9/1000 | Loss: 0.00002034
Iteration 10/1000 | Loss: 0.00002005
Iteration 11/1000 | Loss: 0.00001987
Iteration 12/1000 | Loss: 0.00001975
Iteration 13/1000 | Loss: 0.00001960
Iteration 14/1000 | Loss: 0.00001949
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001945
Iteration 17/1000 | Loss: 0.00001945
Iteration 18/1000 | Loss: 0.00001944
Iteration 19/1000 | Loss: 0.00001944
Iteration 20/1000 | Loss: 0.00001941
Iteration 21/1000 | Loss: 0.00001940
Iteration 22/1000 | Loss: 0.00001938
Iteration 23/1000 | Loss: 0.00001938
Iteration 24/1000 | Loss: 0.00001938
Iteration 25/1000 | Loss: 0.00001937
Iteration 26/1000 | Loss: 0.00001937
Iteration 27/1000 | Loss: 0.00001936
Iteration 28/1000 | Loss: 0.00001935
Iteration 29/1000 | Loss: 0.00001935
Iteration 30/1000 | Loss: 0.00001935
Iteration 31/1000 | Loss: 0.00001934
Iteration 32/1000 | Loss: 0.00001934
Iteration 33/1000 | Loss: 0.00001933
Iteration 34/1000 | Loss: 0.00001932
Iteration 35/1000 | Loss: 0.00001930
Iteration 36/1000 | Loss: 0.00001930
Iteration 37/1000 | Loss: 0.00001929
Iteration 38/1000 | Loss: 0.00001929
Iteration 39/1000 | Loss: 0.00001928
Iteration 40/1000 | Loss: 0.00001927
Iteration 41/1000 | Loss: 0.00001926
Iteration 42/1000 | Loss: 0.00001926
Iteration 43/1000 | Loss: 0.00001925
Iteration 44/1000 | Loss: 0.00001925
Iteration 45/1000 | Loss: 0.00001924
Iteration 46/1000 | Loss: 0.00001924
Iteration 47/1000 | Loss: 0.00001924
Iteration 48/1000 | Loss: 0.00001918
Iteration 49/1000 | Loss: 0.00001918
Iteration 50/1000 | Loss: 0.00001915
Iteration 51/1000 | Loss: 0.00001915
Iteration 52/1000 | Loss: 0.00001915
Iteration 53/1000 | Loss: 0.00001915
Iteration 54/1000 | Loss: 0.00001914
Iteration 55/1000 | Loss: 0.00001914
Iteration 56/1000 | Loss: 0.00001914
Iteration 57/1000 | Loss: 0.00001913
Iteration 58/1000 | Loss: 0.00001913
Iteration 59/1000 | Loss: 0.00001911
Iteration 60/1000 | Loss: 0.00001910
Iteration 61/1000 | Loss: 0.00001910
Iteration 62/1000 | Loss: 0.00001910
Iteration 63/1000 | Loss: 0.00001908
Iteration 64/1000 | Loss: 0.00001907
Iteration 65/1000 | Loss: 0.00001907
Iteration 66/1000 | Loss: 0.00001907
Iteration 67/1000 | Loss: 0.00001907
Iteration 68/1000 | Loss: 0.00001907
Iteration 69/1000 | Loss: 0.00001907
Iteration 70/1000 | Loss: 0.00001906
Iteration 71/1000 | Loss: 0.00001904
Iteration 72/1000 | Loss: 0.00001904
Iteration 73/1000 | Loss: 0.00001904
Iteration 74/1000 | Loss: 0.00001904
Iteration 75/1000 | Loss: 0.00001903
Iteration 76/1000 | Loss: 0.00001903
Iteration 77/1000 | Loss: 0.00001903
Iteration 78/1000 | Loss: 0.00001903
Iteration 79/1000 | Loss: 0.00001903
Iteration 80/1000 | Loss: 0.00001903
Iteration 81/1000 | Loss: 0.00001903
Iteration 82/1000 | Loss: 0.00001903
Iteration 83/1000 | Loss: 0.00001903
Iteration 84/1000 | Loss: 0.00001902
Iteration 85/1000 | Loss: 0.00001902
Iteration 86/1000 | Loss: 0.00001902
Iteration 87/1000 | Loss: 0.00001901
Iteration 88/1000 | Loss: 0.00001901
Iteration 89/1000 | Loss: 0.00001901
Iteration 90/1000 | Loss: 0.00001901
Iteration 91/1000 | Loss: 0.00001901
Iteration 92/1000 | Loss: 0.00001901
Iteration 93/1000 | Loss: 0.00001900
Iteration 94/1000 | Loss: 0.00001900
Iteration 95/1000 | Loss: 0.00001900
Iteration 96/1000 | Loss: 0.00001900
Iteration 97/1000 | Loss: 0.00001900
Iteration 98/1000 | Loss: 0.00001900
Iteration 99/1000 | Loss: 0.00001900
Iteration 100/1000 | Loss: 0.00001900
Iteration 101/1000 | Loss: 0.00001900
Iteration 102/1000 | Loss: 0.00001900
Iteration 103/1000 | Loss: 0.00001900
Iteration 104/1000 | Loss: 0.00001900
Iteration 105/1000 | Loss: 0.00001900
Iteration 106/1000 | Loss: 0.00001899
Iteration 107/1000 | Loss: 0.00001899
Iteration 108/1000 | Loss: 0.00001899
Iteration 109/1000 | Loss: 0.00001899
Iteration 110/1000 | Loss: 0.00001898
Iteration 111/1000 | Loss: 0.00001898
Iteration 112/1000 | Loss: 0.00001898
Iteration 113/1000 | Loss: 0.00001898
Iteration 114/1000 | Loss: 0.00001898
Iteration 115/1000 | Loss: 0.00001898
Iteration 116/1000 | Loss: 0.00001898
Iteration 117/1000 | Loss: 0.00001898
Iteration 118/1000 | Loss: 0.00001898
Iteration 119/1000 | Loss: 0.00001898
Iteration 120/1000 | Loss: 0.00001898
Iteration 121/1000 | Loss: 0.00001898
Iteration 122/1000 | Loss: 0.00001898
Iteration 123/1000 | Loss: 0.00001898
Iteration 124/1000 | Loss: 0.00001897
Iteration 125/1000 | Loss: 0.00001897
Iteration 126/1000 | Loss: 0.00001897
Iteration 127/1000 | Loss: 0.00001897
Iteration 128/1000 | Loss: 0.00001897
Iteration 129/1000 | Loss: 0.00001897
Iteration 130/1000 | Loss: 0.00001897
Iteration 131/1000 | Loss: 0.00001897
Iteration 132/1000 | Loss: 0.00001897
Iteration 133/1000 | Loss: 0.00001897
Iteration 134/1000 | Loss: 0.00001896
Iteration 135/1000 | Loss: 0.00001896
Iteration 136/1000 | Loss: 0.00001896
Iteration 137/1000 | Loss: 0.00001896
Iteration 138/1000 | Loss: 0.00001896
Iteration 139/1000 | Loss: 0.00001896
Iteration 140/1000 | Loss: 0.00001896
Iteration 141/1000 | Loss: 0.00001896
Iteration 142/1000 | Loss: 0.00001896
Iteration 143/1000 | Loss: 0.00001896
Iteration 144/1000 | Loss: 0.00001896
Iteration 145/1000 | Loss: 0.00001895
Iteration 146/1000 | Loss: 0.00001895
Iteration 147/1000 | Loss: 0.00001895
Iteration 148/1000 | Loss: 0.00001895
Iteration 149/1000 | Loss: 0.00001895
Iteration 150/1000 | Loss: 0.00001895
Iteration 151/1000 | Loss: 0.00001894
Iteration 152/1000 | Loss: 0.00001894
Iteration 153/1000 | Loss: 0.00001894
Iteration 154/1000 | Loss: 0.00001894
Iteration 155/1000 | Loss: 0.00001894
Iteration 156/1000 | Loss: 0.00001894
Iteration 157/1000 | Loss: 0.00001894
Iteration 158/1000 | Loss: 0.00001894
Iteration 159/1000 | Loss: 0.00001893
Iteration 160/1000 | Loss: 0.00001893
Iteration 161/1000 | Loss: 0.00001893
Iteration 162/1000 | Loss: 0.00001893
Iteration 163/1000 | Loss: 0.00001893
Iteration 164/1000 | Loss: 0.00001893
Iteration 165/1000 | Loss: 0.00001892
Iteration 166/1000 | Loss: 0.00001892
Iteration 167/1000 | Loss: 0.00001892
Iteration 168/1000 | Loss: 0.00001892
Iteration 169/1000 | Loss: 0.00001892
Iteration 170/1000 | Loss: 0.00001892
Iteration 171/1000 | Loss: 0.00001892
Iteration 172/1000 | Loss: 0.00001892
Iteration 173/1000 | Loss: 0.00001892
Iteration 174/1000 | Loss: 0.00001892
Iteration 175/1000 | Loss: 0.00001892
Iteration 176/1000 | Loss: 0.00001892
Iteration 177/1000 | Loss: 0.00001892
Iteration 178/1000 | Loss: 0.00001891
Iteration 179/1000 | Loss: 0.00001891
Iteration 180/1000 | Loss: 0.00001891
Iteration 181/1000 | Loss: 0.00001891
Iteration 182/1000 | Loss: 0.00001891
Iteration 183/1000 | Loss: 0.00001891
Iteration 184/1000 | Loss: 0.00001891
Iteration 185/1000 | Loss: 0.00001890
Iteration 186/1000 | Loss: 0.00001890
Iteration 187/1000 | Loss: 0.00001890
Iteration 188/1000 | Loss: 0.00001890
Iteration 189/1000 | Loss: 0.00001890
Iteration 190/1000 | Loss: 0.00001890
Iteration 191/1000 | Loss: 0.00001890
Iteration 192/1000 | Loss: 0.00001890
Iteration 193/1000 | Loss: 0.00001890
Iteration 194/1000 | Loss: 0.00001890
Iteration 195/1000 | Loss: 0.00001890
Iteration 196/1000 | Loss: 0.00001890
Iteration 197/1000 | Loss: 0.00001890
Iteration 198/1000 | Loss: 0.00001890
Iteration 199/1000 | Loss: 0.00001890
Iteration 200/1000 | Loss: 0.00001890
Iteration 201/1000 | Loss: 0.00001890
Iteration 202/1000 | Loss: 0.00001890
Iteration 203/1000 | Loss: 0.00001890
Iteration 204/1000 | Loss: 0.00001890
Iteration 205/1000 | Loss: 0.00001890
Iteration 206/1000 | Loss: 0.00001890
Iteration 207/1000 | Loss: 0.00001890
Iteration 208/1000 | Loss: 0.00001890
Iteration 209/1000 | Loss: 0.00001890
Iteration 210/1000 | Loss: 0.00001890
Iteration 211/1000 | Loss: 0.00001890
Iteration 212/1000 | Loss: 0.00001890
Iteration 213/1000 | Loss: 0.00001890
Iteration 214/1000 | Loss: 0.00001890
Iteration 215/1000 | Loss: 0.00001890
Iteration 216/1000 | Loss: 0.00001890
Iteration 217/1000 | Loss: 0.00001890
Iteration 218/1000 | Loss: 0.00001890
Iteration 219/1000 | Loss: 0.00001890
Iteration 220/1000 | Loss: 0.00001890
Iteration 221/1000 | Loss: 0.00001890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.8895072571467608e-05, 1.8895072571467608e-05, 1.8895072571467608e-05, 1.8895072571467608e-05, 1.8895072571467608e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8895072571467608e-05

Optimization complete. Final v2v error: 3.592865228652954 mm

Highest mean error: 4.449840068817139 mm for frame 147

Lowest mean error: 2.7344319820404053 mm for frame 1

Saving results

Total time: 42.79005670547485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826684
Iteration 2/25 | Loss: 0.00129735
Iteration 3/25 | Loss: 0.00121715
Iteration 4/25 | Loss: 0.00120746
Iteration 5/25 | Loss: 0.00120401
Iteration 6/25 | Loss: 0.00120346
Iteration 7/25 | Loss: 0.00120346
Iteration 8/25 | Loss: 0.00120346
Iteration 9/25 | Loss: 0.00120346
Iteration 10/25 | Loss: 0.00120346
Iteration 11/25 | Loss: 0.00120346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012034636456519365, 0.0012034636456519365, 0.0012034636456519365, 0.0012034636456519365, 0.0012034636456519365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012034636456519365

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66069138
Iteration 2/25 | Loss: 0.00083417
Iteration 3/25 | Loss: 0.00083417
Iteration 4/25 | Loss: 0.00083417
Iteration 5/25 | Loss: 0.00083417
Iteration 6/25 | Loss: 0.00083417
Iteration 7/25 | Loss: 0.00083417
Iteration 8/25 | Loss: 0.00083417
Iteration 9/25 | Loss: 0.00083417
Iteration 10/25 | Loss: 0.00083417
Iteration 11/25 | Loss: 0.00083417
Iteration 12/25 | Loss: 0.00083417
Iteration 13/25 | Loss: 0.00083417
Iteration 14/25 | Loss: 0.00083417
Iteration 15/25 | Loss: 0.00083417
Iteration 16/25 | Loss: 0.00083417
Iteration 17/25 | Loss: 0.00083417
Iteration 18/25 | Loss: 0.00083417
Iteration 19/25 | Loss: 0.00083417
Iteration 20/25 | Loss: 0.00083417
Iteration 21/25 | Loss: 0.00083417
Iteration 22/25 | Loss: 0.00083417
Iteration 23/25 | Loss: 0.00083417
Iteration 24/25 | Loss: 0.00083417
Iteration 25/25 | Loss: 0.00083417

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083417
Iteration 2/1000 | Loss: 0.00002670
Iteration 3/1000 | Loss: 0.00001788
Iteration 4/1000 | Loss: 0.00001472
Iteration 5/1000 | Loss: 0.00001381
Iteration 6/1000 | Loss: 0.00001313
Iteration 7/1000 | Loss: 0.00001261
Iteration 8/1000 | Loss: 0.00001228
Iteration 9/1000 | Loss: 0.00001205
Iteration 10/1000 | Loss: 0.00001196
Iteration 11/1000 | Loss: 0.00001191
Iteration 12/1000 | Loss: 0.00001186
Iteration 13/1000 | Loss: 0.00001183
Iteration 14/1000 | Loss: 0.00001166
Iteration 15/1000 | Loss: 0.00001159
Iteration 16/1000 | Loss: 0.00001158
Iteration 17/1000 | Loss: 0.00001157
Iteration 18/1000 | Loss: 0.00001156
Iteration 19/1000 | Loss: 0.00001150
Iteration 20/1000 | Loss: 0.00001146
Iteration 21/1000 | Loss: 0.00001146
Iteration 22/1000 | Loss: 0.00001145
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001144
Iteration 25/1000 | Loss: 0.00001142
Iteration 26/1000 | Loss: 0.00001142
Iteration 27/1000 | Loss: 0.00001140
Iteration 28/1000 | Loss: 0.00001140
Iteration 29/1000 | Loss: 0.00001140
Iteration 30/1000 | Loss: 0.00001140
Iteration 31/1000 | Loss: 0.00001140
Iteration 32/1000 | Loss: 0.00001139
Iteration 33/1000 | Loss: 0.00001139
Iteration 34/1000 | Loss: 0.00001139
Iteration 35/1000 | Loss: 0.00001139
Iteration 36/1000 | Loss: 0.00001137
Iteration 37/1000 | Loss: 0.00001135
Iteration 38/1000 | Loss: 0.00001135
Iteration 39/1000 | Loss: 0.00001135
Iteration 40/1000 | Loss: 0.00001134
Iteration 41/1000 | Loss: 0.00001133
Iteration 42/1000 | Loss: 0.00001133
Iteration 43/1000 | Loss: 0.00001132
Iteration 44/1000 | Loss: 0.00001132
Iteration 45/1000 | Loss: 0.00001132
Iteration 46/1000 | Loss: 0.00001132
Iteration 47/1000 | Loss: 0.00001131
Iteration 48/1000 | Loss: 0.00001131
Iteration 49/1000 | Loss: 0.00001131
Iteration 50/1000 | Loss: 0.00001131
Iteration 51/1000 | Loss: 0.00001131
Iteration 52/1000 | Loss: 0.00001130
Iteration 53/1000 | Loss: 0.00001130
Iteration 54/1000 | Loss: 0.00001129
Iteration 55/1000 | Loss: 0.00001129
Iteration 56/1000 | Loss: 0.00001128
Iteration 57/1000 | Loss: 0.00001128
Iteration 58/1000 | Loss: 0.00001128
Iteration 59/1000 | Loss: 0.00001127
Iteration 60/1000 | Loss: 0.00001127
Iteration 61/1000 | Loss: 0.00001127
Iteration 62/1000 | Loss: 0.00001126
Iteration 63/1000 | Loss: 0.00001124
Iteration 64/1000 | Loss: 0.00001124
Iteration 65/1000 | Loss: 0.00001124
Iteration 66/1000 | Loss: 0.00001124
Iteration 67/1000 | Loss: 0.00001123
Iteration 68/1000 | Loss: 0.00001123
Iteration 69/1000 | Loss: 0.00001123
Iteration 70/1000 | Loss: 0.00001123
Iteration 71/1000 | Loss: 0.00001123
Iteration 72/1000 | Loss: 0.00001123
Iteration 73/1000 | Loss: 0.00001123
Iteration 74/1000 | Loss: 0.00001123
Iteration 75/1000 | Loss: 0.00001122
Iteration 76/1000 | Loss: 0.00001122
Iteration 77/1000 | Loss: 0.00001122
Iteration 78/1000 | Loss: 0.00001121
Iteration 79/1000 | Loss: 0.00001121
Iteration 80/1000 | Loss: 0.00001121
Iteration 81/1000 | Loss: 0.00001120
Iteration 82/1000 | Loss: 0.00001120
Iteration 83/1000 | Loss: 0.00001120
Iteration 84/1000 | Loss: 0.00001120
Iteration 85/1000 | Loss: 0.00001120
Iteration 86/1000 | Loss: 0.00001119
Iteration 87/1000 | Loss: 0.00001119
Iteration 88/1000 | Loss: 0.00001118
Iteration 89/1000 | Loss: 0.00001118
Iteration 90/1000 | Loss: 0.00001118
Iteration 91/1000 | Loss: 0.00001118
Iteration 92/1000 | Loss: 0.00001117
Iteration 93/1000 | Loss: 0.00001117
Iteration 94/1000 | Loss: 0.00001117
Iteration 95/1000 | Loss: 0.00001117
Iteration 96/1000 | Loss: 0.00001117
Iteration 97/1000 | Loss: 0.00001117
Iteration 98/1000 | Loss: 0.00001117
Iteration 99/1000 | Loss: 0.00001116
Iteration 100/1000 | Loss: 0.00001115
Iteration 101/1000 | Loss: 0.00001115
Iteration 102/1000 | Loss: 0.00001114
Iteration 103/1000 | Loss: 0.00001114
Iteration 104/1000 | Loss: 0.00001114
Iteration 105/1000 | Loss: 0.00001114
Iteration 106/1000 | Loss: 0.00001114
Iteration 107/1000 | Loss: 0.00001114
Iteration 108/1000 | Loss: 0.00001114
Iteration 109/1000 | Loss: 0.00001114
Iteration 110/1000 | Loss: 0.00001114
Iteration 111/1000 | Loss: 0.00001114
Iteration 112/1000 | Loss: 0.00001114
Iteration 113/1000 | Loss: 0.00001114
Iteration 114/1000 | Loss: 0.00001114
Iteration 115/1000 | Loss: 0.00001114
Iteration 116/1000 | Loss: 0.00001113
Iteration 117/1000 | Loss: 0.00001112
Iteration 118/1000 | Loss: 0.00001112
Iteration 119/1000 | Loss: 0.00001112
Iteration 120/1000 | Loss: 0.00001111
Iteration 121/1000 | Loss: 0.00001111
Iteration 122/1000 | Loss: 0.00001111
Iteration 123/1000 | Loss: 0.00001110
Iteration 124/1000 | Loss: 0.00001110
Iteration 125/1000 | Loss: 0.00001110
Iteration 126/1000 | Loss: 0.00001110
Iteration 127/1000 | Loss: 0.00001110
Iteration 128/1000 | Loss: 0.00001110
Iteration 129/1000 | Loss: 0.00001110
Iteration 130/1000 | Loss: 0.00001110
Iteration 131/1000 | Loss: 0.00001109
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001109
Iteration 134/1000 | Loss: 0.00001109
Iteration 135/1000 | Loss: 0.00001109
Iteration 136/1000 | Loss: 0.00001109
Iteration 137/1000 | Loss: 0.00001109
Iteration 138/1000 | Loss: 0.00001109
Iteration 139/1000 | Loss: 0.00001108
Iteration 140/1000 | Loss: 0.00001108
Iteration 141/1000 | Loss: 0.00001108
Iteration 142/1000 | Loss: 0.00001108
Iteration 143/1000 | Loss: 0.00001108
Iteration 144/1000 | Loss: 0.00001108
Iteration 145/1000 | Loss: 0.00001108
Iteration 146/1000 | Loss: 0.00001108
Iteration 147/1000 | Loss: 0.00001108
Iteration 148/1000 | Loss: 0.00001108
Iteration 149/1000 | Loss: 0.00001107
Iteration 150/1000 | Loss: 0.00001107
Iteration 151/1000 | Loss: 0.00001107
Iteration 152/1000 | Loss: 0.00001107
Iteration 153/1000 | Loss: 0.00001107
Iteration 154/1000 | Loss: 0.00001107
Iteration 155/1000 | Loss: 0.00001107
Iteration 156/1000 | Loss: 0.00001107
Iteration 157/1000 | Loss: 0.00001107
Iteration 158/1000 | Loss: 0.00001107
Iteration 159/1000 | Loss: 0.00001107
Iteration 160/1000 | Loss: 0.00001107
Iteration 161/1000 | Loss: 0.00001107
Iteration 162/1000 | Loss: 0.00001107
Iteration 163/1000 | Loss: 0.00001107
Iteration 164/1000 | Loss: 0.00001107
Iteration 165/1000 | Loss: 0.00001107
Iteration 166/1000 | Loss: 0.00001107
Iteration 167/1000 | Loss: 0.00001107
Iteration 168/1000 | Loss: 0.00001107
Iteration 169/1000 | Loss: 0.00001107
Iteration 170/1000 | Loss: 0.00001107
Iteration 171/1000 | Loss: 0.00001107
Iteration 172/1000 | Loss: 0.00001107
Iteration 173/1000 | Loss: 0.00001107
Iteration 174/1000 | Loss: 0.00001107
Iteration 175/1000 | Loss: 0.00001107
Iteration 176/1000 | Loss: 0.00001107
Iteration 177/1000 | Loss: 0.00001107
Iteration 178/1000 | Loss: 0.00001107
Iteration 179/1000 | Loss: 0.00001107
Iteration 180/1000 | Loss: 0.00001107
Iteration 181/1000 | Loss: 0.00001107
Iteration 182/1000 | Loss: 0.00001107
Iteration 183/1000 | Loss: 0.00001107
Iteration 184/1000 | Loss: 0.00001107
Iteration 185/1000 | Loss: 0.00001107
Iteration 186/1000 | Loss: 0.00001107
Iteration 187/1000 | Loss: 0.00001107
Iteration 188/1000 | Loss: 0.00001107
Iteration 189/1000 | Loss: 0.00001107
Iteration 190/1000 | Loss: 0.00001107
Iteration 191/1000 | Loss: 0.00001107
Iteration 192/1000 | Loss: 0.00001107
Iteration 193/1000 | Loss: 0.00001107
Iteration 194/1000 | Loss: 0.00001107
Iteration 195/1000 | Loss: 0.00001107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.1069903848692775e-05, 1.1069903848692775e-05, 1.1069903848692775e-05, 1.1069903848692775e-05, 1.1069903848692775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1069903848692775e-05

Optimization complete. Final v2v error: 2.853102207183838 mm

Highest mean error: 3.1885857582092285 mm for frame 87

Lowest mean error: 2.7018346786499023 mm for frame 121

Saving results

Total time: 36.21974468231201
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752363
Iteration 2/25 | Loss: 0.00148955
Iteration 3/25 | Loss: 0.00132639
Iteration 4/25 | Loss: 0.00129199
Iteration 5/25 | Loss: 0.00128851
Iteration 6/25 | Loss: 0.00128430
Iteration 7/25 | Loss: 0.00129401
Iteration 8/25 | Loss: 0.00127543
Iteration 9/25 | Loss: 0.00127054
Iteration 10/25 | Loss: 0.00127009
Iteration 11/25 | Loss: 0.00126999
Iteration 12/25 | Loss: 0.00126991
Iteration 13/25 | Loss: 0.00126990
Iteration 14/25 | Loss: 0.00126990
Iteration 15/25 | Loss: 0.00126989
Iteration 16/25 | Loss: 0.00126989
Iteration 17/25 | Loss: 0.00126989
Iteration 18/25 | Loss: 0.00126989
Iteration 19/25 | Loss: 0.00126989
Iteration 20/25 | Loss: 0.00126989
Iteration 21/25 | Loss: 0.00126989
Iteration 22/25 | Loss: 0.00126989
Iteration 23/25 | Loss: 0.00126988
Iteration 24/25 | Loss: 0.00126988
Iteration 25/25 | Loss: 0.00126988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.04201984
Iteration 2/25 | Loss: 0.00088120
Iteration 3/25 | Loss: 0.00088113
Iteration 4/25 | Loss: 0.00088113
Iteration 5/25 | Loss: 0.00088113
Iteration 6/25 | Loss: 0.00088113
Iteration 7/25 | Loss: 0.00088113
Iteration 8/25 | Loss: 0.00088113
Iteration 9/25 | Loss: 0.00088113
Iteration 10/25 | Loss: 0.00088113
Iteration 11/25 | Loss: 0.00088113
Iteration 12/25 | Loss: 0.00088113
Iteration 13/25 | Loss: 0.00088113
Iteration 14/25 | Loss: 0.00088113
Iteration 15/25 | Loss: 0.00088113
Iteration 16/25 | Loss: 0.00088113
Iteration 17/25 | Loss: 0.00088113
Iteration 18/25 | Loss: 0.00088113
Iteration 19/25 | Loss: 0.00088113
Iteration 20/25 | Loss: 0.00088113
Iteration 21/25 | Loss: 0.00088113
Iteration 22/25 | Loss: 0.00088113
Iteration 23/25 | Loss: 0.00088113
Iteration 24/25 | Loss: 0.00088113
Iteration 25/25 | Loss: 0.00088113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088113
Iteration 2/1000 | Loss: 0.00013949
Iteration 3/1000 | Loss: 0.00020657
Iteration 4/1000 | Loss: 0.00002620
Iteration 5/1000 | Loss: 0.00015140
Iteration 6/1000 | Loss: 0.00002246
Iteration 7/1000 | Loss: 0.00002128
Iteration 8/1000 | Loss: 0.00002050
Iteration 9/1000 | Loss: 0.00001999
Iteration 10/1000 | Loss: 0.00001963
Iteration 11/1000 | Loss: 0.00001929
Iteration 12/1000 | Loss: 0.00001902
Iteration 13/1000 | Loss: 0.00001894
Iteration 14/1000 | Loss: 0.00001877
Iteration 15/1000 | Loss: 0.00001873
Iteration 16/1000 | Loss: 0.00001873
Iteration 17/1000 | Loss: 0.00001872
Iteration 18/1000 | Loss: 0.00001866
Iteration 19/1000 | Loss: 0.00001866
Iteration 20/1000 | Loss: 0.00001864
Iteration 21/1000 | Loss: 0.00001862
Iteration 22/1000 | Loss: 0.00001857
Iteration 23/1000 | Loss: 0.00001854
Iteration 24/1000 | Loss: 0.00001853
Iteration 25/1000 | Loss: 0.00001853
Iteration 26/1000 | Loss: 0.00001852
Iteration 27/1000 | Loss: 0.00001852
Iteration 28/1000 | Loss: 0.00001851
Iteration 29/1000 | Loss: 0.00001851
Iteration 30/1000 | Loss: 0.00001850
Iteration 31/1000 | Loss: 0.00001850
Iteration 32/1000 | Loss: 0.00001848
Iteration 33/1000 | Loss: 0.00001848
Iteration 34/1000 | Loss: 0.00001847
Iteration 35/1000 | Loss: 0.00001845
Iteration 36/1000 | Loss: 0.00001845
Iteration 37/1000 | Loss: 0.00001844
Iteration 38/1000 | Loss: 0.00001844
Iteration 39/1000 | Loss: 0.00001840
Iteration 40/1000 | Loss: 0.00001840
Iteration 41/1000 | Loss: 0.00001838
Iteration 42/1000 | Loss: 0.00001837
Iteration 43/1000 | Loss: 0.00001837
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00001836
Iteration 46/1000 | Loss: 0.00001836
Iteration 47/1000 | Loss: 0.00001835
Iteration 48/1000 | Loss: 0.00001835
Iteration 49/1000 | Loss: 0.00001835
Iteration 50/1000 | Loss: 0.00001835
Iteration 51/1000 | Loss: 0.00001835
Iteration 52/1000 | Loss: 0.00001835
Iteration 53/1000 | Loss: 0.00001835
Iteration 54/1000 | Loss: 0.00001834
Iteration 55/1000 | Loss: 0.00001834
Iteration 56/1000 | Loss: 0.00001834
Iteration 57/1000 | Loss: 0.00001833
Iteration 58/1000 | Loss: 0.00001833
Iteration 59/1000 | Loss: 0.00001833
Iteration 60/1000 | Loss: 0.00001833
Iteration 61/1000 | Loss: 0.00001833
Iteration 62/1000 | Loss: 0.00001832
Iteration 63/1000 | Loss: 0.00001832
Iteration 64/1000 | Loss: 0.00001831
Iteration 65/1000 | Loss: 0.00001831
Iteration 66/1000 | Loss: 0.00001830
Iteration 67/1000 | Loss: 0.00001830
Iteration 68/1000 | Loss: 0.00001830
Iteration 69/1000 | Loss: 0.00001829
Iteration 70/1000 | Loss: 0.00001829
Iteration 71/1000 | Loss: 0.00001828
Iteration 72/1000 | Loss: 0.00001828
Iteration 73/1000 | Loss: 0.00001827
Iteration 74/1000 | Loss: 0.00001827
Iteration 75/1000 | Loss: 0.00001827
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001826
Iteration 78/1000 | Loss: 0.00001825
Iteration 79/1000 | Loss: 0.00001825
Iteration 80/1000 | Loss: 0.00001825
Iteration 81/1000 | Loss: 0.00001825
Iteration 82/1000 | Loss: 0.00001824
Iteration 83/1000 | Loss: 0.00001824
Iteration 84/1000 | Loss: 0.00001824
Iteration 85/1000 | Loss: 0.00001823
Iteration 86/1000 | Loss: 0.00001823
Iteration 87/1000 | Loss: 0.00001823
Iteration 88/1000 | Loss: 0.00001823
Iteration 89/1000 | Loss: 0.00001822
Iteration 90/1000 | Loss: 0.00001822
Iteration 91/1000 | Loss: 0.00001822
Iteration 92/1000 | Loss: 0.00001822
Iteration 93/1000 | Loss: 0.00001821
Iteration 94/1000 | Loss: 0.00001821
Iteration 95/1000 | Loss: 0.00001821
Iteration 96/1000 | Loss: 0.00001821
Iteration 97/1000 | Loss: 0.00001821
Iteration 98/1000 | Loss: 0.00001821
Iteration 99/1000 | Loss: 0.00001820
Iteration 100/1000 | Loss: 0.00001820
Iteration 101/1000 | Loss: 0.00001820
Iteration 102/1000 | Loss: 0.00001820
Iteration 103/1000 | Loss: 0.00001819
Iteration 104/1000 | Loss: 0.00001819
Iteration 105/1000 | Loss: 0.00001818
Iteration 106/1000 | Loss: 0.00001818
Iteration 107/1000 | Loss: 0.00001818
Iteration 108/1000 | Loss: 0.00001817
Iteration 109/1000 | Loss: 0.00001817
Iteration 110/1000 | Loss: 0.00001817
Iteration 111/1000 | Loss: 0.00001817
Iteration 112/1000 | Loss: 0.00001816
Iteration 113/1000 | Loss: 0.00001816
Iteration 114/1000 | Loss: 0.00001816
Iteration 115/1000 | Loss: 0.00001816
Iteration 116/1000 | Loss: 0.00001815
Iteration 117/1000 | Loss: 0.00001815
Iteration 118/1000 | Loss: 0.00001815
Iteration 119/1000 | Loss: 0.00001815
Iteration 120/1000 | Loss: 0.00001815
Iteration 121/1000 | Loss: 0.00001815
Iteration 122/1000 | Loss: 0.00001815
Iteration 123/1000 | Loss: 0.00001814
Iteration 124/1000 | Loss: 0.00001814
Iteration 125/1000 | Loss: 0.00001814
Iteration 126/1000 | Loss: 0.00001814
Iteration 127/1000 | Loss: 0.00001814
Iteration 128/1000 | Loss: 0.00001814
Iteration 129/1000 | Loss: 0.00001814
Iteration 130/1000 | Loss: 0.00001813
Iteration 131/1000 | Loss: 0.00001813
Iteration 132/1000 | Loss: 0.00001813
Iteration 133/1000 | Loss: 0.00001813
Iteration 134/1000 | Loss: 0.00001812
Iteration 135/1000 | Loss: 0.00001812
Iteration 136/1000 | Loss: 0.00001812
Iteration 137/1000 | Loss: 0.00001812
Iteration 138/1000 | Loss: 0.00001812
Iteration 139/1000 | Loss: 0.00001812
Iteration 140/1000 | Loss: 0.00001811
Iteration 141/1000 | Loss: 0.00001811
Iteration 142/1000 | Loss: 0.00001811
Iteration 143/1000 | Loss: 0.00001811
Iteration 144/1000 | Loss: 0.00001811
Iteration 145/1000 | Loss: 0.00001811
Iteration 146/1000 | Loss: 0.00001811
Iteration 147/1000 | Loss: 0.00001811
Iteration 148/1000 | Loss: 0.00001810
Iteration 149/1000 | Loss: 0.00001810
Iteration 150/1000 | Loss: 0.00001810
Iteration 151/1000 | Loss: 0.00001810
Iteration 152/1000 | Loss: 0.00001810
Iteration 153/1000 | Loss: 0.00001810
Iteration 154/1000 | Loss: 0.00001810
Iteration 155/1000 | Loss: 0.00001809
Iteration 156/1000 | Loss: 0.00001809
Iteration 157/1000 | Loss: 0.00001809
Iteration 158/1000 | Loss: 0.00001809
Iteration 159/1000 | Loss: 0.00001809
Iteration 160/1000 | Loss: 0.00001809
Iteration 161/1000 | Loss: 0.00001809
Iteration 162/1000 | Loss: 0.00001809
Iteration 163/1000 | Loss: 0.00001808
Iteration 164/1000 | Loss: 0.00001808
Iteration 165/1000 | Loss: 0.00001808
Iteration 166/1000 | Loss: 0.00001808
Iteration 167/1000 | Loss: 0.00001808
Iteration 168/1000 | Loss: 0.00001808
Iteration 169/1000 | Loss: 0.00001808
Iteration 170/1000 | Loss: 0.00001807
Iteration 171/1000 | Loss: 0.00001807
Iteration 172/1000 | Loss: 0.00001807
Iteration 173/1000 | Loss: 0.00001807
Iteration 174/1000 | Loss: 0.00001807
Iteration 175/1000 | Loss: 0.00001807
Iteration 176/1000 | Loss: 0.00001807
Iteration 177/1000 | Loss: 0.00001807
Iteration 178/1000 | Loss: 0.00001807
Iteration 179/1000 | Loss: 0.00001807
Iteration 180/1000 | Loss: 0.00001807
Iteration 181/1000 | Loss: 0.00001807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.806972795748152e-05, 1.806972795748152e-05, 1.806972795748152e-05, 1.806972795748152e-05, 1.806972795748152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.806972795748152e-05

Optimization complete. Final v2v error: 3.5336172580718994 mm

Highest mean error: 4.47241735458374 mm for frame 138

Lowest mean error: 2.958857774734497 mm for frame 191

Saving results

Total time: 63.251322746276855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980290
Iteration 2/25 | Loss: 0.00195774
Iteration 3/25 | Loss: 0.00150784
Iteration 4/25 | Loss: 0.00138101
Iteration 5/25 | Loss: 0.00135687
Iteration 6/25 | Loss: 0.00135065
Iteration 7/25 | Loss: 0.00135587
Iteration 8/25 | Loss: 0.00140371
Iteration 9/25 | Loss: 0.00140691
Iteration 10/25 | Loss: 0.00141189
Iteration 11/25 | Loss: 0.00136621
Iteration 12/25 | Loss: 0.00133485
Iteration 13/25 | Loss: 0.00130215
Iteration 14/25 | Loss: 0.00129428
Iteration 15/25 | Loss: 0.00128584
Iteration 16/25 | Loss: 0.00128388
Iteration 17/25 | Loss: 0.00128432
Iteration 18/25 | Loss: 0.00128315
Iteration 19/25 | Loss: 0.00128343
Iteration 20/25 | Loss: 0.00128299
Iteration 21/25 | Loss: 0.00128295
Iteration 22/25 | Loss: 0.00128267
Iteration 23/25 | Loss: 0.00128262
Iteration 24/25 | Loss: 0.00128374
Iteration 25/25 | Loss: 0.00128315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73374021
Iteration 2/25 | Loss: 0.00082319
Iteration 3/25 | Loss: 0.00080270
Iteration 4/25 | Loss: 0.00080270
Iteration 5/25 | Loss: 0.00080270
Iteration 6/25 | Loss: 0.00080270
Iteration 7/25 | Loss: 0.00080270
Iteration 8/25 | Loss: 0.00080270
Iteration 9/25 | Loss: 0.00080270
Iteration 10/25 | Loss: 0.00080270
Iteration 11/25 | Loss: 0.00080270
Iteration 12/25 | Loss: 0.00080270
Iteration 13/25 | Loss: 0.00080270
Iteration 14/25 | Loss: 0.00080270
Iteration 15/25 | Loss: 0.00080270
Iteration 16/25 | Loss: 0.00080270
Iteration 17/25 | Loss: 0.00080270
Iteration 18/25 | Loss: 0.00080270
Iteration 19/25 | Loss: 0.00080270
Iteration 20/25 | Loss: 0.00080270
Iteration 21/25 | Loss: 0.00080270
Iteration 22/25 | Loss: 0.00080270
Iteration 23/25 | Loss: 0.00080270
Iteration 24/25 | Loss: 0.00080270
Iteration 25/25 | Loss: 0.00080270

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080270
Iteration 2/1000 | Loss: 0.00010898
Iteration 3/1000 | Loss: 0.00007590
Iteration 4/1000 | Loss: 0.00003620
Iteration 5/1000 | Loss: 0.00006754
Iteration 6/1000 | Loss: 0.00003215
Iteration 7/1000 | Loss: 0.00004044
Iteration 8/1000 | Loss: 0.00003912
Iteration 9/1000 | Loss: 0.00004918
Iteration 10/1000 | Loss: 0.00003918
Iteration 11/1000 | Loss: 0.00008301
Iteration 12/1000 | Loss: 0.00014259
Iteration 13/1000 | Loss: 0.00014442
Iteration 14/1000 | Loss: 0.00006594
Iteration 15/1000 | Loss: 0.00007307
Iteration 16/1000 | Loss: 0.00004243
Iteration 17/1000 | Loss: 0.00003864
Iteration 18/1000 | Loss: 0.00004046
Iteration 19/1000 | Loss: 0.00006934
Iteration 20/1000 | Loss: 0.00004111
Iteration 21/1000 | Loss: 0.00003745
Iteration 22/1000 | Loss: 0.00004320
Iteration 23/1000 | Loss: 0.00003787
Iteration 24/1000 | Loss: 0.00004858
Iteration 25/1000 | Loss: 0.00003836
Iteration 26/1000 | Loss: 0.00004472
Iteration 27/1000 | Loss: 0.00006433
Iteration 28/1000 | Loss: 0.00003733
Iteration 29/1000 | Loss: 0.00004852
Iteration 30/1000 | Loss: 0.00004944
Iteration 31/1000 | Loss: 0.00004465
Iteration 32/1000 | Loss: 0.00012609
Iteration 33/1000 | Loss: 0.00004116
Iteration 34/1000 | Loss: 0.00004853
Iteration 35/1000 | Loss: 0.00004418
Iteration 36/1000 | Loss: 0.00005158
Iteration 37/1000 | Loss: 0.00004592
Iteration 38/1000 | Loss: 0.00004405
Iteration 39/1000 | Loss: 0.00004351
Iteration 40/1000 | Loss: 0.00002603
Iteration 41/1000 | Loss: 0.00004522
Iteration 42/1000 | Loss: 0.00003389
Iteration 43/1000 | Loss: 0.00002658
Iteration 44/1000 | Loss: 0.00004676
Iteration 45/1000 | Loss: 0.00004569
Iteration 46/1000 | Loss: 0.00005539
Iteration 47/1000 | Loss: 0.00004634
Iteration 48/1000 | Loss: 0.00005336
Iteration 49/1000 | Loss: 0.00003247
Iteration 50/1000 | Loss: 0.00002694
Iteration 51/1000 | Loss: 0.00004128
Iteration 52/1000 | Loss: 0.00003052
Iteration 53/1000 | Loss: 0.00003596
Iteration 54/1000 | Loss: 0.00004602
Iteration 55/1000 | Loss: 0.00006962
Iteration 56/1000 | Loss: 0.00002802
Iteration 57/1000 | Loss: 0.00002712
Iteration 58/1000 | Loss: 0.00002181
Iteration 59/1000 | Loss: 0.00001958
Iteration 60/1000 | Loss: 0.00001878
Iteration 61/1000 | Loss: 0.00006942
Iteration 62/1000 | Loss: 0.00001838
Iteration 63/1000 | Loss: 0.00001775
Iteration 64/1000 | Loss: 0.00001767
Iteration 65/1000 | Loss: 0.00001754
Iteration 66/1000 | Loss: 0.00001747
Iteration 67/1000 | Loss: 0.00001746
Iteration 68/1000 | Loss: 0.00001744
Iteration 69/1000 | Loss: 0.00001740
Iteration 70/1000 | Loss: 0.00001739
Iteration 71/1000 | Loss: 0.00001737
Iteration 72/1000 | Loss: 0.00001731
Iteration 73/1000 | Loss: 0.00001728
Iteration 74/1000 | Loss: 0.00001724
Iteration 75/1000 | Loss: 0.00001721
Iteration 76/1000 | Loss: 0.00001720
Iteration 77/1000 | Loss: 0.00001717
Iteration 78/1000 | Loss: 0.00001717
Iteration 79/1000 | Loss: 0.00001717
Iteration 80/1000 | Loss: 0.00001717
Iteration 81/1000 | Loss: 0.00001716
Iteration 82/1000 | Loss: 0.00001716
Iteration 83/1000 | Loss: 0.00001716
Iteration 84/1000 | Loss: 0.00001715
Iteration 85/1000 | Loss: 0.00001715
Iteration 86/1000 | Loss: 0.00001714
Iteration 87/1000 | Loss: 0.00001714
Iteration 88/1000 | Loss: 0.00001714
Iteration 89/1000 | Loss: 0.00001714
Iteration 90/1000 | Loss: 0.00001714
Iteration 91/1000 | Loss: 0.00001714
Iteration 92/1000 | Loss: 0.00001714
Iteration 93/1000 | Loss: 0.00001713
Iteration 94/1000 | Loss: 0.00001713
Iteration 95/1000 | Loss: 0.00001713
Iteration 96/1000 | Loss: 0.00001713
Iteration 97/1000 | Loss: 0.00001713
Iteration 98/1000 | Loss: 0.00001713
Iteration 99/1000 | Loss: 0.00001713
Iteration 100/1000 | Loss: 0.00001713
Iteration 101/1000 | Loss: 0.00001712
Iteration 102/1000 | Loss: 0.00001712
Iteration 103/1000 | Loss: 0.00001712
Iteration 104/1000 | Loss: 0.00001712
Iteration 105/1000 | Loss: 0.00001712
Iteration 106/1000 | Loss: 0.00001712
Iteration 107/1000 | Loss: 0.00001711
Iteration 108/1000 | Loss: 0.00001711
Iteration 109/1000 | Loss: 0.00001711
Iteration 110/1000 | Loss: 0.00001711
Iteration 111/1000 | Loss: 0.00001711
Iteration 112/1000 | Loss: 0.00001711
Iteration 113/1000 | Loss: 0.00001711
Iteration 114/1000 | Loss: 0.00001711
Iteration 115/1000 | Loss: 0.00001711
Iteration 116/1000 | Loss: 0.00001710
Iteration 117/1000 | Loss: 0.00001710
Iteration 118/1000 | Loss: 0.00001710
Iteration 119/1000 | Loss: 0.00001710
Iteration 120/1000 | Loss: 0.00001710
Iteration 121/1000 | Loss: 0.00001710
Iteration 122/1000 | Loss: 0.00001710
Iteration 123/1000 | Loss: 0.00001710
Iteration 124/1000 | Loss: 0.00001710
Iteration 125/1000 | Loss: 0.00001710
Iteration 126/1000 | Loss: 0.00001710
Iteration 127/1000 | Loss: 0.00001710
Iteration 128/1000 | Loss: 0.00001710
Iteration 129/1000 | Loss: 0.00001709
Iteration 130/1000 | Loss: 0.00001709
Iteration 131/1000 | Loss: 0.00001709
Iteration 132/1000 | Loss: 0.00001709
Iteration 133/1000 | Loss: 0.00001709
Iteration 134/1000 | Loss: 0.00001709
Iteration 135/1000 | Loss: 0.00001709
Iteration 136/1000 | Loss: 0.00001709
Iteration 137/1000 | Loss: 0.00001709
Iteration 138/1000 | Loss: 0.00001709
Iteration 139/1000 | Loss: 0.00001709
Iteration 140/1000 | Loss: 0.00001709
Iteration 141/1000 | Loss: 0.00001708
Iteration 142/1000 | Loss: 0.00001708
Iteration 143/1000 | Loss: 0.00001708
Iteration 144/1000 | Loss: 0.00001708
Iteration 145/1000 | Loss: 0.00001708
Iteration 146/1000 | Loss: 0.00001708
Iteration 147/1000 | Loss: 0.00001708
Iteration 148/1000 | Loss: 0.00001708
Iteration 149/1000 | Loss: 0.00001708
Iteration 150/1000 | Loss: 0.00001708
Iteration 151/1000 | Loss: 0.00001708
Iteration 152/1000 | Loss: 0.00001707
Iteration 153/1000 | Loss: 0.00001707
Iteration 154/1000 | Loss: 0.00001707
Iteration 155/1000 | Loss: 0.00001707
Iteration 156/1000 | Loss: 0.00001707
Iteration 157/1000 | Loss: 0.00001707
Iteration 158/1000 | Loss: 0.00001706
Iteration 159/1000 | Loss: 0.00001706
Iteration 160/1000 | Loss: 0.00001706
Iteration 161/1000 | Loss: 0.00001706
Iteration 162/1000 | Loss: 0.00001706
Iteration 163/1000 | Loss: 0.00001706
Iteration 164/1000 | Loss: 0.00001706
Iteration 165/1000 | Loss: 0.00001706
Iteration 166/1000 | Loss: 0.00001706
Iteration 167/1000 | Loss: 0.00001706
Iteration 168/1000 | Loss: 0.00001706
Iteration 169/1000 | Loss: 0.00001706
Iteration 170/1000 | Loss: 0.00001706
Iteration 171/1000 | Loss: 0.00001706
Iteration 172/1000 | Loss: 0.00001706
Iteration 173/1000 | Loss: 0.00001706
Iteration 174/1000 | Loss: 0.00001706
Iteration 175/1000 | Loss: 0.00001705
Iteration 176/1000 | Loss: 0.00001705
Iteration 177/1000 | Loss: 0.00001705
Iteration 178/1000 | Loss: 0.00001705
Iteration 179/1000 | Loss: 0.00001705
Iteration 180/1000 | Loss: 0.00001705
Iteration 181/1000 | Loss: 0.00001705
Iteration 182/1000 | Loss: 0.00001705
Iteration 183/1000 | Loss: 0.00001704
Iteration 184/1000 | Loss: 0.00001704
Iteration 185/1000 | Loss: 0.00001704
Iteration 186/1000 | Loss: 0.00001704
Iteration 187/1000 | Loss: 0.00001704
Iteration 188/1000 | Loss: 0.00001704
Iteration 189/1000 | Loss: 0.00001704
Iteration 190/1000 | Loss: 0.00001704
Iteration 191/1000 | Loss: 0.00001704
Iteration 192/1000 | Loss: 0.00001704
Iteration 193/1000 | Loss: 0.00001704
Iteration 194/1000 | Loss: 0.00001704
Iteration 195/1000 | Loss: 0.00001704
Iteration 196/1000 | Loss: 0.00001704
Iteration 197/1000 | Loss: 0.00001704
Iteration 198/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.7037798897945322e-05, 1.7037798897945322e-05, 1.7037798897945322e-05, 1.7037798897945322e-05, 1.7037798897945322e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7037798897945322e-05

Optimization complete. Final v2v error: 3.4546902179718018 mm

Highest mean error: 4.265076637268066 mm for frame 87

Lowest mean error: 2.9632301330566406 mm for frame 129

Saving results

Total time: 139.49806022644043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01171961
Iteration 2/25 | Loss: 0.00417022
Iteration 3/25 | Loss: 0.00261003
Iteration 4/25 | Loss: 0.00238671
Iteration 5/25 | Loss: 0.00212404
Iteration 6/25 | Loss: 0.00195938
Iteration 7/25 | Loss: 0.00190363
Iteration 8/25 | Loss: 0.00177382
Iteration 9/25 | Loss: 0.00171868
Iteration 10/25 | Loss: 0.00169283
Iteration 11/25 | Loss: 0.00165833
Iteration 12/25 | Loss: 0.00163545
Iteration 13/25 | Loss: 0.00162557
Iteration 14/25 | Loss: 0.00162340
Iteration 15/25 | Loss: 0.00166204
Iteration 16/25 | Loss: 0.00162231
Iteration 17/25 | Loss: 0.00157187
Iteration 18/25 | Loss: 0.00156431
Iteration 19/25 | Loss: 0.00156509
Iteration 20/25 | Loss: 0.00156080
Iteration 21/25 | Loss: 0.00155864
Iteration 22/25 | Loss: 0.00155497
Iteration 23/25 | Loss: 0.00155371
Iteration 24/25 | Loss: 0.00155312
Iteration 25/25 | Loss: 0.00155683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67031044
Iteration 2/25 | Loss: 0.00125443
Iteration 3/25 | Loss: 0.00125443
Iteration 4/25 | Loss: 0.00125443
Iteration 5/25 | Loss: 0.00125443
Iteration 6/25 | Loss: 0.00125442
Iteration 7/25 | Loss: 0.00125442
Iteration 8/25 | Loss: 0.00125442
Iteration 9/25 | Loss: 0.00125442
Iteration 10/25 | Loss: 0.00125442
Iteration 11/25 | Loss: 0.00125442
Iteration 12/25 | Loss: 0.00125442
Iteration 13/25 | Loss: 0.00125442
Iteration 14/25 | Loss: 0.00125442
Iteration 15/25 | Loss: 0.00125442
Iteration 16/25 | Loss: 0.00125442
Iteration 17/25 | Loss: 0.00125442
Iteration 18/25 | Loss: 0.00125442
Iteration 19/25 | Loss: 0.00125442
Iteration 20/25 | Loss: 0.00125442
Iteration 21/25 | Loss: 0.00125442
Iteration 22/25 | Loss: 0.00125442
Iteration 23/25 | Loss: 0.00125442
Iteration 24/25 | Loss: 0.00125442
Iteration 25/25 | Loss: 0.00125442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125442
Iteration 2/1000 | Loss: 0.00018685
Iteration 3/1000 | Loss: 0.00009906
Iteration 4/1000 | Loss: 0.00009440
Iteration 5/1000 | Loss: 0.00009482
Iteration 6/1000 | Loss: 0.00009930
Iteration 7/1000 | Loss: 0.00006973
Iteration 8/1000 | Loss: 0.00006151
Iteration 9/1000 | Loss: 0.00005770
Iteration 10/1000 | Loss: 0.00005485
Iteration 11/1000 | Loss: 0.00005253
Iteration 12/1000 | Loss: 0.00107722
Iteration 13/1000 | Loss: 0.00027622
Iteration 14/1000 | Loss: 0.00009782
Iteration 15/1000 | Loss: 0.00011124
Iteration 16/1000 | Loss: 0.00075372
Iteration 17/1000 | Loss: 0.00053618
Iteration 18/1000 | Loss: 0.00006569
Iteration 19/1000 | Loss: 0.00005935
Iteration 20/1000 | Loss: 0.00005570
Iteration 21/1000 | Loss: 0.00005343
Iteration 22/1000 | Loss: 0.00005181
Iteration 23/1000 | Loss: 0.00005070
Iteration 24/1000 | Loss: 0.00004925
Iteration 25/1000 | Loss: 0.00004840
Iteration 26/1000 | Loss: 0.00004773
Iteration 27/1000 | Loss: 0.00004693
Iteration 28/1000 | Loss: 0.00004630
Iteration 29/1000 | Loss: 0.00004582
Iteration 30/1000 | Loss: 0.00004538
Iteration 31/1000 | Loss: 0.00004502
Iteration 32/1000 | Loss: 0.00004472
Iteration 33/1000 | Loss: 0.00004436
Iteration 34/1000 | Loss: 0.00004371
Iteration 35/1000 | Loss: 0.00004336
Iteration 36/1000 | Loss: 0.00004320
Iteration 37/1000 | Loss: 0.00004318
Iteration 38/1000 | Loss: 0.00004317
Iteration 39/1000 | Loss: 0.00004317
Iteration 40/1000 | Loss: 0.00004316
Iteration 41/1000 | Loss: 0.00004316
Iteration 42/1000 | Loss: 0.00004316
Iteration 43/1000 | Loss: 0.00004315
Iteration 44/1000 | Loss: 0.00004315
Iteration 45/1000 | Loss: 0.00004314
Iteration 46/1000 | Loss: 0.00004314
Iteration 47/1000 | Loss: 0.00004313
Iteration 48/1000 | Loss: 0.00004313
Iteration 49/1000 | Loss: 0.00004312
Iteration 50/1000 | Loss: 0.00004312
Iteration 51/1000 | Loss: 0.00004312
Iteration 52/1000 | Loss: 0.00004310
Iteration 53/1000 | Loss: 0.00004310
Iteration 54/1000 | Loss: 0.00004310
Iteration 55/1000 | Loss: 0.00004310
Iteration 56/1000 | Loss: 0.00004310
Iteration 57/1000 | Loss: 0.00004310
Iteration 58/1000 | Loss: 0.00004309
Iteration 59/1000 | Loss: 0.00004309
Iteration 60/1000 | Loss: 0.00004309
Iteration 61/1000 | Loss: 0.00004308
Iteration 62/1000 | Loss: 0.00004308
Iteration 63/1000 | Loss: 0.00004308
Iteration 64/1000 | Loss: 0.00004307
Iteration 65/1000 | Loss: 0.00004307
Iteration 66/1000 | Loss: 0.00004307
Iteration 67/1000 | Loss: 0.00004307
Iteration 68/1000 | Loss: 0.00004307
Iteration 69/1000 | Loss: 0.00004307
Iteration 70/1000 | Loss: 0.00004306
Iteration 71/1000 | Loss: 0.00004306
Iteration 72/1000 | Loss: 0.00004306
Iteration 73/1000 | Loss: 0.00004306
Iteration 74/1000 | Loss: 0.00004306
Iteration 75/1000 | Loss: 0.00004306
Iteration 76/1000 | Loss: 0.00004306
Iteration 77/1000 | Loss: 0.00004306
Iteration 78/1000 | Loss: 0.00004306
Iteration 79/1000 | Loss: 0.00004306
Iteration 80/1000 | Loss: 0.00004306
Iteration 81/1000 | Loss: 0.00004305
Iteration 82/1000 | Loss: 0.00004305
Iteration 83/1000 | Loss: 0.00004305
Iteration 84/1000 | Loss: 0.00004305
Iteration 85/1000 | Loss: 0.00004305
Iteration 86/1000 | Loss: 0.00004304
Iteration 87/1000 | Loss: 0.00004304
Iteration 88/1000 | Loss: 0.00004304
Iteration 89/1000 | Loss: 0.00004304
Iteration 90/1000 | Loss: 0.00004304
Iteration 91/1000 | Loss: 0.00004304
Iteration 92/1000 | Loss: 0.00004304
Iteration 93/1000 | Loss: 0.00004304
Iteration 94/1000 | Loss: 0.00004304
Iteration 95/1000 | Loss: 0.00004304
Iteration 96/1000 | Loss: 0.00004304
Iteration 97/1000 | Loss: 0.00004303
Iteration 98/1000 | Loss: 0.00004303
Iteration 99/1000 | Loss: 0.00004303
Iteration 100/1000 | Loss: 0.00004303
Iteration 101/1000 | Loss: 0.00004303
Iteration 102/1000 | Loss: 0.00004302
Iteration 103/1000 | Loss: 0.00004302
Iteration 104/1000 | Loss: 0.00004302
Iteration 105/1000 | Loss: 0.00004302
Iteration 106/1000 | Loss: 0.00004302
Iteration 107/1000 | Loss: 0.00004302
Iteration 108/1000 | Loss: 0.00004301
Iteration 109/1000 | Loss: 0.00004301
Iteration 110/1000 | Loss: 0.00004301
Iteration 111/1000 | Loss: 0.00004301
Iteration 112/1000 | Loss: 0.00004301
Iteration 113/1000 | Loss: 0.00004301
Iteration 114/1000 | Loss: 0.00004300
Iteration 115/1000 | Loss: 0.00004300
Iteration 116/1000 | Loss: 0.00004300
Iteration 117/1000 | Loss: 0.00004300
Iteration 118/1000 | Loss: 0.00004299
Iteration 119/1000 | Loss: 0.00004299
Iteration 120/1000 | Loss: 0.00004299
Iteration 121/1000 | Loss: 0.00004299
Iteration 122/1000 | Loss: 0.00004299
Iteration 123/1000 | Loss: 0.00004299
Iteration 124/1000 | Loss: 0.00004299
Iteration 125/1000 | Loss: 0.00004298
Iteration 126/1000 | Loss: 0.00004298
Iteration 127/1000 | Loss: 0.00004298
Iteration 128/1000 | Loss: 0.00004298
Iteration 129/1000 | Loss: 0.00004298
Iteration 130/1000 | Loss: 0.00004298
Iteration 131/1000 | Loss: 0.00004298
Iteration 132/1000 | Loss: 0.00004298
Iteration 133/1000 | Loss: 0.00004298
Iteration 134/1000 | Loss: 0.00004297
Iteration 135/1000 | Loss: 0.00004297
Iteration 136/1000 | Loss: 0.00004297
Iteration 137/1000 | Loss: 0.00004297
Iteration 138/1000 | Loss: 0.00004297
Iteration 139/1000 | Loss: 0.00004297
Iteration 140/1000 | Loss: 0.00004297
Iteration 141/1000 | Loss: 0.00004297
Iteration 142/1000 | Loss: 0.00004297
Iteration 143/1000 | Loss: 0.00004297
Iteration 144/1000 | Loss: 0.00004296
Iteration 145/1000 | Loss: 0.00004296
Iteration 146/1000 | Loss: 0.00004296
Iteration 147/1000 | Loss: 0.00004296
Iteration 148/1000 | Loss: 0.00004296
Iteration 149/1000 | Loss: 0.00004296
Iteration 150/1000 | Loss: 0.00004296
Iteration 151/1000 | Loss: 0.00004296
Iteration 152/1000 | Loss: 0.00004296
Iteration 153/1000 | Loss: 0.00004296
Iteration 154/1000 | Loss: 0.00004296
Iteration 155/1000 | Loss: 0.00004296
Iteration 156/1000 | Loss: 0.00004296
Iteration 157/1000 | Loss: 0.00004295
Iteration 158/1000 | Loss: 0.00004295
Iteration 159/1000 | Loss: 0.00004295
Iteration 160/1000 | Loss: 0.00004295
Iteration 161/1000 | Loss: 0.00004295
Iteration 162/1000 | Loss: 0.00004295
Iteration 163/1000 | Loss: 0.00004295
Iteration 164/1000 | Loss: 0.00004295
Iteration 165/1000 | Loss: 0.00004295
Iteration 166/1000 | Loss: 0.00004295
Iteration 167/1000 | Loss: 0.00004294
Iteration 168/1000 | Loss: 0.00004294
Iteration 169/1000 | Loss: 0.00004294
Iteration 170/1000 | Loss: 0.00004294
Iteration 171/1000 | Loss: 0.00004294
Iteration 172/1000 | Loss: 0.00004294
Iteration 173/1000 | Loss: 0.00004293
Iteration 174/1000 | Loss: 0.00004293
Iteration 175/1000 | Loss: 0.00004293
Iteration 176/1000 | Loss: 0.00004292
Iteration 177/1000 | Loss: 0.00004292
Iteration 178/1000 | Loss: 0.00004292
Iteration 179/1000 | Loss: 0.00004292
Iteration 180/1000 | Loss: 0.00004292
Iteration 181/1000 | Loss: 0.00004292
Iteration 182/1000 | Loss: 0.00004292
Iteration 183/1000 | Loss: 0.00004291
Iteration 184/1000 | Loss: 0.00004291
Iteration 185/1000 | Loss: 0.00004291
Iteration 186/1000 | Loss: 0.00004291
Iteration 187/1000 | Loss: 0.00004291
Iteration 188/1000 | Loss: 0.00004291
Iteration 189/1000 | Loss: 0.00004291
Iteration 190/1000 | Loss: 0.00004290
Iteration 191/1000 | Loss: 0.00004290
Iteration 192/1000 | Loss: 0.00004290
Iteration 193/1000 | Loss: 0.00004289
Iteration 194/1000 | Loss: 0.00004289
Iteration 195/1000 | Loss: 0.00004289
Iteration 196/1000 | Loss: 0.00004289
Iteration 197/1000 | Loss: 0.00004289
Iteration 198/1000 | Loss: 0.00004289
Iteration 199/1000 | Loss: 0.00004289
Iteration 200/1000 | Loss: 0.00004289
Iteration 201/1000 | Loss: 0.00004289
Iteration 202/1000 | Loss: 0.00004289
Iteration 203/1000 | Loss: 0.00004289
Iteration 204/1000 | Loss: 0.00004288
Iteration 205/1000 | Loss: 0.00004288
Iteration 206/1000 | Loss: 0.00004288
Iteration 207/1000 | Loss: 0.00004288
Iteration 208/1000 | Loss: 0.00004288
Iteration 209/1000 | Loss: 0.00004288
Iteration 210/1000 | Loss: 0.00004288
Iteration 211/1000 | Loss: 0.00004288
Iteration 212/1000 | Loss: 0.00004288
Iteration 213/1000 | Loss: 0.00004287
Iteration 214/1000 | Loss: 0.00004287
Iteration 215/1000 | Loss: 0.00004287
Iteration 216/1000 | Loss: 0.00004287
Iteration 217/1000 | Loss: 0.00004287
Iteration 218/1000 | Loss: 0.00004287
Iteration 219/1000 | Loss: 0.00004287
Iteration 220/1000 | Loss: 0.00004287
Iteration 221/1000 | Loss: 0.00004287
Iteration 222/1000 | Loss: 0.00004287
Iteration 223/1000 | Loss: 0.00004287
Iteration 224/1000 | Loss: 0.00004287
Iteration 225/1000 | Loss: 0.00004287
Iteration 226/1000 | Loss: 0.00004287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [4.287308547645807e-05, 4.287308547645807e-05, 4.287308547645807e-05, 4.287308547645807e-05, 4.287308547645807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.287308547645807e-05

Optimization complete. Final v2v error: 5.434025287628174 mm

Highest mean error: 8.790508270263672 mm for frame 4

Lowest mean error: 4.771747589111328 mm for frame 19

Saving results

Total time: 103.44712591171265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977005
Iteration 2/25 | Loss: 0.00977004
Iteration 3/25 | Loss: 0.00977004
Iteration 4/25 | Loss: 0.00977004
Iteration 5/25 | Loss: 0.00977004
Iteration 6/25 | Loss: 0.00977003
Iteration 7/25 | Loss: 0.00977003
Iteration 8/25 | Loss: 0.00977003
Iteration 9/25 | Loss: 0.00977003
Iteration 10/25 | Loss: 0.00977003
Iteration 11/25 | Loss: 0.00977002
Iteration 12/25 | Loss: 0.00977002
Iteration 13/25 | Loss: 0.00977002
Iteration 14/25 | Loss: 0.00977002
Iteration 15/25 | Loss: 0.00977001
Iteration 16/25 | Loss: 0.00977001
Iteration 17/25 | Loss: 0.00977001
Iteration 18/25 | Loss: 0.00977000
Iteration 19/25 | Loss: 0.00977000
Iteration 20/25 | Loss: 0.00977000
Iteration 21/25 | Loss: 0.00977000
Iteration 22/25 | Loss: 0.00976999
Iteration 23/25 | Loss: 0.00976999
Iteration 24/25 | Loss: 0.00976999
Iteration 25/25 | Loss: 0.00976999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57818305
Iteration 2/25 | Loss: 0.16770500
Iteration 3/25 | Loss: 0.16769063
Iteration 4/25 | Loss: 0.16769062
Iteration 5/25 | Loss: 0.16769060
Iteration 6/25 | Loss: 0.16769059
Iteration 7/25 | Loss: 0.16769056
Iteration 8/25 | Loss: 0.16769056
Iteration 9/25 | Loss: 0.16769056
Iteration 10/25 | Loss: 0.16769055
Iteration 11/25 | Loss: 0.16769055
Iteration 12/25 | Loss: 0.16769056
Iteration 13/25 | Loss: 0.16769056
Iteration 14/25 | Loss: 0.16769055
Iteration 15/25 | Loss: 0.16769055
Iteration 16/25 | Loss: 0.16769055
Iteration 17/25 | Loss: 0.16769055
Iteration 18/25 | Loss: 0.16769055
Iteration 19/25 | Loss: 0.16769055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.16769054532051086, 0.16769054532051086, 0.16769054532051086, 0.16769054532051086, 0.16769054532051086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16769054532051086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16769055
Iteration 2/1000 | Loss: 0.00293427
Iteration 3/1000 | Loss: 0.00079840
Iteration 4/1000 | Loss: 0.00053884
Iteration 5/1000 | Loss: 0.00050334
Iteration 6/1000 | Loss: 0.00033358
Iteration 7/1000 | Loss: 0.00026952
Iteration 8/1000 | Loss: 0.00010235
Iteration 9/1000 | Loss: 0.00048097
Iteration 10/1000 | Loss: 0.00017190
Iteration 11/1000 | Loss: 0.00016184
Iteration 12/1000 | Loss: 0.00028659
Iteration 13/1000 | Loss: 0.00003686
Iteration 14/1000 | Loss: 0.00022736
Iteration 15/1000 | Loss: 0.00003137
Iteration 16/1000 | Loss: 0.00004271
Iteration 17/1000 | Loss: 0.00010376
Iteration 18/1000 | Loss: 0.00021951
Iteration 19/1000 | Loss: 0.00015601
Iteration 20/1000 | Loss: 0.00008656
Iteration 21/1000 | Loss: 0.00002523
Iteration 22/1000 | Loss: 0.00002455
Iteration 23/1000 | Loss: 0.00002369
Iteration 24/1000 | Loss: 0.00004533
Iteration 25/1000 | Loss: 0.00008150
Iteration 26/1000 | Loss: 0.00004874
Iteration 27/1000 | Loss: 0.00004731
Iteration 28/1000 | Loss: 0.00003014
Iteration 29/1000 | Loss: 0.00016569
Iteration 30/1000 | Loss: 0.00006750
Iteration 31/1000 | Loss: 0.00002756
Iteration 32/1000 | Loss: 0.00003477
Iteration 33/1000 | Loss: 0.00003393
Iteration 34/1000 | Loss: 0.00015102
Iteration 35/1000 | Loss: 0.00007875
Iteration 36/1000 | Loss: 0.00002939
Iteration 37/1000 | Loss: 0.00003937
Iteration 38/1000 | Loss: 0.00002171
Iteration 39/1000 | Loss: 0.00002058
Iteration 40/1000 | Loss: 0.00002204
Iteration 41/1000 | Loss: 0.00002741
Iteration 42/1000 | Loss: 0.00002138
Iteration 43/1000 | Loss: 0.00002048
Iteration 44/1000 | Loss: 0.00002048
Iteration 45/1000 | Loss: 0.00002048
Iteration 46/1000 | Loss: 0.00002048
Iteration 47/1000 | Loss: 0.00002048
Iteration 48/1000 | Loss: 0.00002048
Iteration 49/1000 | Loss: 0.00002048
Iteration 50/1000 | Loss: 0.00002048
Iteration 51/1000 | Loss: 0.00002047
Iteration 52/1000 | Loss: 0.00002047
Iteration 53/1000 | Loss: 0.00002047
Iteration 54/1000 | Loss: 0.00002047
Iteration 55/1000 | Loss: 0.00002047
Iteration 56/1000 | Loss: 0.00002047
Iteration 57/1000 | Loss: 0.00002047
Iteration 58/1000 | Loss: 0.00002047
Iteration 59/1000 | Loss: 0.00002047
Iteration 60/1000 | Loss: 0.00002047
Iteration 61/1000 | Loss: 0.00002047
Iteration 62/1000 | Loss: 0.00002047
Iteration 63/1000 | Loss: 0.00002047
Iteration 64/1000 | Loss: 0.00002047
Iteration 65/1000 | Loss: 0.00002047
Iteration 66/1000 | Loss: 0.00002047
Iteration 67/1000 | Loss: 0.00002047
Iteration 68/1000 | Loss: 0.00002047
Iteration 69/1000 | Loss: 0.00002047
Iteration 70/1000 | Loss: 0.00002047
Iteration 71/1000 | Loss: 0.00002047
Iteration 72/1000 | Loss: 0.00002047
Iteration 73/1000 | Loss: 0.00002047
Iteration 74/1000 | Loss: 0.00002047
Iteration 75/1000 | Loss: 0.00002047
Iteration 76/1000 | Loss: 0.00002047
Iteration 77/1000 | Loss: 0.00002047
Iteration 78/1000 | Loss: 0.00002047
Iteration 79/1000 | Loss: 0.00002047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [2.0473587937885895e-05, 2.0473587937885895e-05, 2.0473587937885895e-05, 2.0473587937885895e-05, 2.0473587937885895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0473587937885895e-05

Optimization complete. Final v2v error: 3.8121824264526367 mm

Highest mean error: 4.022097587585449 mm for frame 232

Lowest mean error: 3.7307779788970947 mm for frame 29

Saving results

Total time: 74.365567445755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416102
Iteration 2/25 | Loss: 0.00146202
Iteration 3/25 | Loss: 0.00126838
Iteration 4/25 | Loss: 0.00124150
Iteration 5/25 | Loss: 0.00123763
Iteration 6/25 | Loss: 0.00123671
Iteration 7/25 | Loss: 0.00123671
Iteration 8/25 | Loss: 0.00123671
Iteration 9/25 | Loss: 0.00123671
Iteration 10/25 | Loss: 0.00123671
Iteration 11/25 | Loss: 0.00123671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012367067392915487, 0.0012367067392915487, 0.0012367067392915487, 0.0012367067392915487, 0.0012367067392915487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012367067392915487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44006670
Iteration 2/25 | Loss: 0.00078604
Iteration 3/25 | Loss: 0.00078604
Iteration 4/25 | Loss: 0.00078604
Iteration 5/25 | Loss: 0.00078604
Iteration 6/25 | Loss: 0.00078604
Iteration 7/25 | Loss: 0.00078604
Iteration 8/25 | Loss: 0.00078604
Iteration 9/25 | Loss: 0.00078604
Iteration 10/25 | Loss: 0.00078604
Iteration 11/25 | Loss: 0.00078604
Iteration 12/25 | Loss: 0.00078604
Iteration 13/25 | Loss: 0.00078604
Iteration 14/25 | Loss: 0.00078604
Iteration 15/25 | Loss: 0.00078604
Iteration 16/25 | Loss: 0.00078604
Iteration 17/25 | Loss: 0.00078604
Iteration 18/25 | Loss: 0.00078604
Iteration 19/25 | Loss: 0.00078604
Iteration 20/25 | Loss: 0.00078604
Iteration 21/25 | Loss: 0.00078604
Iteration 22/25 | Loss: 0.00078604
Iteration 23/25 | Loss: 0.00078604
Iteration 24/25 | Loss: 0.00078604
Iteration 25/25 | Loss: 0.00078604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078604
Iteration 2/1000 | Loss: 0.00003700
Iteration 3/1000 | Loss: 0.00002143
Iteration 4/1000 | Loss: 0.00001803
Iteration 5/1000 | Loss: 0.00001662
Iteration 6/1000 | Loss: 0.00001554
Iteration 7/1000 | Loss: 0.00001492
Iteration 8/1000 | Loss: 0.00001439
Iteration 9/1000 | Loss: 0.00001421
Iteration 10/1000 | Loss: 0.00001396
Iteration 11/1000 | Loss: 0.00001387
Iteration 12/1000 | Loss: 0.00001374
Iteration 13/1000 | Loss: 0.00001359
Iteration 14/1000 | Loss: 0.00001358
Iteration 15/1000 | Loss: 0.00001357
Iteration 16/1000 | Loss: 0.00001357
Iteration 17/1000 | Loss: 0.00001356
Iteration 18/1000 | Loss: 0.00001355
Iteration 19/1000 | Loss: 0.00001351
Iteration 20/1000 | Loss: 0.00001347
Iteration 21/1000 | Loss: 0.00001344
Iteration 22/1000 | Loss: 0.00001343
Iteration 23/1000 | Loss: 0.00001336
Iteration 24/1000 | Loss: 0.00001336
Iteration 25/1000 | Loss: 0.00001333
Iteration 26/1000 | Loss: 0.00001333
Iteration 27/1000 | Loss: 0.00001332
Iteration 28/1000 | Loss: 0.00001332
Iteration 29/1000 | Loss: 0.00001332
Iteration 30/1000 | Loss: 0.00001331
Iteration 31/1000 | Loss: 0.00001331
Iteration 32/1000 | Loss: 0.00001330
Iteration 33/1000 | Loss: 0.00001328
Iteration 34/1000 | Loss: 0.00001328
Iteration 35/1000 | Loss: 0.00001328
Iteration 36/1000 | Loss: 0.00001328
Iteration 37/1000 | Loss: 0.00001328
Iteration 38/1000 | Loss: 0.00001328
Iteration 39/1000 | Loss: 0.00001328
Iteration 40/1000 | Loss: 0.00001328
Iteration 41/1000 | Loss: 0.00001327
Iteration 42/1000 | Loss: 0.00001327
Iteration 43/1000 | Loss: 0.00001327
Iteration 44/1000 | Loss: 0.00001327
Iteration 45/1000 | Loss: 0.00001327
Iteration 46/1000 | Loss: 0.00001327
Iteration 47/1000 | Loss: 0.00001326
Iteration 48/1000 | Loss: 0.00001326
Iteration 49/1000 | Loss: 0.00001326
Iteration 50/1000 | Loss: 0.00001325
Iteration 51/1000 | Loss: 0.00001325
Iteration 52/1000 | Loss: 0.00001325
Iteration 53/1000 | Loss: 0.00001324
Iteration 54/1000 | Loss: 0.00001324
Iteration 55/1000 | Loss: 0.00001324
Iteration 56/1000 | Loss: 0.00001324
Iteration 57/1000 | Loss: 0.00001324
Iteration 58/1000 | Loss: 0.00001323
Iteration 59/1000 | Loss: 0.00001323
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001323
Iteration 62/1000 | Loss: 0.00001323
Iteration 63/1000 | Loss: 0.00001322
Iteration 64/1000 | Loss: 0.00001322
Iteration 65/1000 | Loss: 0.00001322
Iteration 66/1000 | Loss: 0.00001322
Iteration 67/1000 | Loss: 0.00001321
Iteration 68/1000 | Loss: 0.00001321
Iteration 69/1000 | Loss: 0.00001321
Iteration 70/1000 | Loss: 0.00001321
Iteration 71/1000 | Loss: 0.00001321
Iteration 72/1000 | Loss: 0.00001320
Iteration 73/1000 | Loss: 0.00001320
Iteration 74/1000 | Loss: 0.00001320
Iteration 75/1000 | Loss: 0.00001320
Iteration 76/1000 | Loss: 0.00001320
Iteration 77/1000 | Loss: 0.00001320
Iteration 78/1000 | Loss: 0.00001320
Iteration 79/1000 | Loss: 0.00001320
Iteration 80/1000 | Loss: 0.00001320
Iteration 81/1000 | Loss: 0.00001319
Iteration 82/1000 | Loss: 0.00001319
Iteration 83/1000 | Loss: 0.00001319
Iteration 84/1000 | Loss: 0.00001318
Iteration 85/1000 | Loss: 0.00001318
Iteration 86/1000 | Loss: 0.00001318
Iteration 87/1000 | Loss: 0.00001317
Iteration 88/1000 | Loss: 0.00001317
Iteration 89/1000 | Loss: 0.00001317
Iteration 90/1000 | Loss: 0.00001317
Iteration 91/1000 | Loss: 0.00001317
Iteration 92/1000 | Loss: 0.00001317
Iteration 93/1000 | Loss: 0.00001317
Iteration 94/1000 | Loss: 0.00001316
Iteration 95/1000 | Loss: 0.00001316
Iteration 96/1000 | Loss: 0.00001316
Iteration 97/1000 | Loss: 0.00001315
Iteration 98/1000 | Loss: 0.00001315
Iteration 99/1000 | Loss: 0.00001315
Iteration 100/1000 | Loss: 0.00001314
Iteration 101/1000 | Loss: 0.00001314
Iteration 102/1000 | Loss: 0.00001314
Iteration 103/1000 | Loss: 0.00001313
Iteration 104/1000 | Loss: 0.00001313
Iteration 105/1000 | Loss: 0.00001313
Iteration 106/1000 | Loss: 0.00001313
Iteration 107/1000 | Loss: 0.00001313
Iteration 108/1000 | Loss: 0.00001312
Iteration 109/1000 | Loss: 0.00001312
Iteration 110/1000 | Loss: 0.00001312
Iteration 111/1000 | Loss: 0.00001312
Iteration 112/1000 | Loss: 0.00001311
Iteration 113/1000 | Loss: 0.00001311
Iteration 114/1000 | Loss: 0.00001311
Iteration 115/1000 | Loss: 0.00001311
Iteration 116/1000 | Loss: 0.00001310
Iteration 117/1000 | Loss: 0.00001310
Iteration 118/1000 | Loss: 0.00001310
Iteration 119/1000 | Loss: 0.00001310
Iteration 120/1000 | Loss: 0.00001310
Iteration 121/1000 | Loss: 0.00001310
Iteration 122/1000 | Loss: 0.00001310
Iteration 123/1000 | Loss: 0.00001310
Iteration 124/1000 | Loss: 0.00001310
Iteration 125/1000 | Loss: 0.00001309
Iteration 126/1000 | Loss: 0.00001309
Iteration 127/1000 | Loss: 0.00001309
Iteration 128/1000 | Loss: 0.00001309
Iteration 129/1000 | Loss: 0.00001308
Iteration 130/1000 | Loss: 0.00001308
Iteration 131/1000 | Loss: 0.00001308
Iteration 132/1000 | Loss: 0.00001308
Iteration 133/1000 | Loss: 0.00001308
Iteration 134/1000 | Loss: 0.00001307
Iteration 135/1000 | Loss: 0.00001307
Iteration 136/1000 | Loss: 0.00001307
Iteration 137/1000 | Loss: 0.00001306
Iteration 138/1000 | Loss: 0.00001306
Iteration 139/1000 | Loss: 0.00001306
Iteration 140/1000 | Loss: 0.00001306
Iteration 141/1000 | Loss: 0.00001306
Iteration 142/1000 | Loss: 0.00001306
Iteration 143/1000 | Loss: 0.00001306
Iteration 144/1000 | Loss: 0.00001306
Iteration 145/1000 | Loss: 0.00001305
Iteration 146/1000 | Loss: 0.00001305
Iteration 147/1000 | Loss: 0.00001305
Iteration 148/1000 | Loss: 0.00001305
Iteration 149/1000 | Loss: 0.00001305
Iteration 150/1000 | Loss: 0.00001305
Iteration 151/1000 | Loss: 0.00001304
Iteration 152/1000 | Loss: 0.00001304
Iteration 153/1000 | Loss: 0.00001304
Iteration 154/1000 | Loss: 0.00001303
Iteration 155/1000 | Loss: 0.00001303
Iteration 156/1000 | Loss: 0.00001303
Iteration 157/1000 | Loss: 0.00001303
Iteration 158/1000 | Loss: 0.00001302
Iteration 159/1000 | Loss: 0.00001302
Iteration 160/1000 | Loss: 0.00001302
Iteration 161/1000 | Loss: 0.00001302
Iteration 162/1000 | Loss: 0.00001302
Iteration 163/1000 | Loss: 0.00001302
Iteration 164/1000 | Loss: 0.00001302
Iteration 165/1000 | Loss: 0.00001302
Iteration 166/1000 | Loss: 0.00001302
Iteration 167/1000 | Loss: 0.00001302
Iteration 168/1000 | Loss: 0.00001301
Iteration 169/1000 | Loss: 0.00001301
Iteration 170/1000 | Loss: 0.00001301
Iteration 171/1000 | Loss: 0.00001301
Iteration 172/1000 | Loss: 0.00001301
Iteration 173/1000 | Loss: 0.00001301
Iteration 174/1000 | Loss: 0.00001300
Iteration 175/1000 | Loss: 0.00001300
Iteration 176/1000 | Loss: 0.00001300
Iteration 177/1000 | Loss: 0.00001300
Iteration 178/1000 | Loss: 0.00001300
Iteration 179/1000 | Loss: 0.00001300
Iteration 180/1000 | Loss: 0.00001300
Iteration 181/1000 | Loss: 0.00001300
Iteration 182/1000 | Loss: 0.00001300
Iteration 183/1000 | Loss: 0.00001300
Iteration 184/1000 | Loss: 0.00001300
Iteration 185/1000 | Loss: 0.00001300
Iteration 186/1000 | Loss: 0.00001300
Iteration 187/1000 | Loss: 0.00001300
Iteration 188/1000 | Loss: 0.00001300
Iteration 189/1000 | Loss: 0.00001299
Iteration 190/1000 | Loss: 0.00001299
Iteration 191/1000 | Loss: 0.00001299
Iteration 192/1000 | Loss: 0.00001299
Iteration 193/1000 | Loss: 0.00001299
Iteration 194/1000 | Loss: 0.00001299
Iteration 195/1000 | Loss: 0.00001299
Iteration 196/1000 | Loss: 0.00001299
Iteration 197/1000 | Loss: 0.00001299
Iteration 198/1000 | Loss: 0.00001298
Iteration 199/1000 | Loss: 0.00001298
Iteration 200/1000 | Loss: 0.00001298
Iteration 201/1000 | Loss: 0.00001298
Iteration 202/1000 | Loss: 0.00001298
Iteration 203/1000 | Loss: 0.00001298
Iteration 204/1000 | Loss: 0.00001298
Iteration 205/1000 | Loss: 0.00001298
Iteration 206/1000 | Loss: 0.00001298
Iteration 207/1000 | Loss: 0.00001298
Iteration 208/1000 | Loss: 0.00001297
Iteration 209/1000 | Loss: 0.00001297
Iteration 210/1000 | Loss: 0.00001297
Iteration 211/1000 | Loss: 0.00001297
Iteration 212/1000 | Loss: 0.00001297
Iteration 213/1000 | Loss: 0.00001297
Iteration 214/1000 | Loss: 0.00001297
Iteration 215/1000 | Loss: 0.00001297
Iteration 216/1000 | Loss: 0.00001297
Iteration 217/1000 | Loss: 0.00001297
Iteration 218/1000 | Loss: 0.00001297
Iteration 219/1000 | Loss: 0.00001297
Iteration 220/1000 | Loss: 0.00001297
Iteration 221/1000 | Loss: 0.00001297
Iteration 222/1000 | Loss: 0.00001297
Iteration 223/1000 | Loss: 0.00001297
Iteration 224/1000 | Loss: 0.00001297
Iteration 225/1000 | Loss: 0.00001296
Iteration 226/1000 | Loss: 0.00001296
Iteration 227/1000 | Loss: 0.00001296
Iteration 228/1000 | Loss: 0.00001296
Iteration 229/1000 | Loss: 0.00001296
Iteration 230/1000 | Loss: 0.00001296
Iteration 231/1000 | Loss: 0.00001296
Iteration 232/1000 | Loss: 0.00001296
Iteration 233/1000 | Loss: 0.00001296
Iteration 234/1000 | Loss: 0.00001296
Iteration 235/1000 | Loss: 0.00001295
Iteration 236/1000 | Loss: 0.00001295
Iteration 237/1000 | Loss: 0.00001295
Iteration 238/1000 | Loss: 0.00001295
Iteration 239/1000 | Loss: 0.00001295
Iteration 240/1000 | Loss: 0.00001295
Iteration 241/1000 | Loss: 0.00001295
Iteration 242/1000 | Loss: 0.00001295
Iteration 243/1000 | Loss: 0.00001295
Iteration 244/1000 | Loss: 0.00001295
Iteration 245/1000 | Loss: 0.00001295
Iteration 246/1000 | Loss: 0.00001295
Iteration 247/1000 | Loss: 0.00001295
Iteration 248/1000 | Loss: 0.00001295
Iteration 249/1000 | Loss: 0.00001295
Iteration 250/1000 | Loss: 0.00001295
Iteration 251/1000 | Loss: 0.00001295
Iteration 252/1000 | Loss: 0.00001294
Iteration 253/1000 | Loss: 0.00001294
Iteration 254/1000 | Loss: 0.00001294
Iteration 255/1000 | Loss: 0.00001294
Iteration 256/1000 | Loss: 0.00001294
Iteration 257/1000 | Loss: 0.00001294
Iteration 258/1000 | Loss: 0.00001294
Iteration 259/1000 | Loss: 0.00001294
Iteration 260/1000 | Loss: 0.00001294
Iteration 261/1000 | Loss: 0.00001294
Iteration 262/1000 | Loss: 0.00001294
Iteration 263/1000 | Loss: 0.00001294
Iteration 264/1000 | Loss: 0.00001294
Iteration 265/1000 | Loss: 0.00001293
Iteration 266/1000 | Loss: 0.00001293
Iteration 267/1000 | Loss: 0.00001293
Iteration 268/1000 | Loss: 0.00001293
Iteration 269/1000 | Loss: 0.00001293
Iteration 270/1000 | Loss: 0.00001293
Iteration 271/1000 | Loss: 0.00001293
Iteration 272/1000 | Loss: 0.00001293
Iteration 273/1000 | Loss: 0.00001293
Iteration 274/1000 | Loss: 0.00001293
Iteration 275/1000 | Loss: 0.00001293
Iteration 276/1000 | Loss: 0.00001293
Iteration 277/1000 | Loss: 0.00001293
Iteration 278/1000 | Loss: 0.00001293
Iteration 279/1000 | Loss: 0.00001293
Iteration 280/1000 | Loss: 0.00001293
Iteration 281/1000 | Loss: 0.00001293
Iteration 282/1000 | Loss: 0.00001293
Iteration 283/1000 | Loss: 0.00001293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [1.2930579032399692e-05, 1.2930579032399692e-05, 1.2930579032399692e-05, 1.2930579032399692e-05, 1.2930579032399692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2930579032399692e-05

Optimization complete. Final v2v error: 3.082094430923462 mm

Highest mean error: 3.6549510955810547 mm for frame 76

Lowest mean error: 2.8450169563293457 mm for frame 171

Saving results

Total time: 45.23705863952637
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056654
Iteration 2/25 | Loss: 0.00173181
Iteration 3/25 | Loss: 0.00138028
Iteration 4/25 | Loss: 0.00134017
Iteration 5/25 | Loss: 0.00135572
Iteration 6/25 | Loss: 0.00137897
Iteration 7/25 | Loss: 0.00135350
Iteration 8/25 | Loss: 0.00135423
Iteration 9/25 | Loss: 0.00136434
Iteration 10/25 | Loss: 0.00136210
Iteration 11/25 | Loss: 0.00135101
Iteration 12/25 | Loss: 0.00135692
Iteration 13/25 | Loss: 0.00135318
Iteration 14/25 | Loss: 0.00134058
Iteration 15/25 | Loss: 0.00133484
Iteration 16/25 | Loss: 0.00134770
Iteration 17/25 | Loss: 0.00134173
Iteration 18/25 | Loss: 0.00133097
Iteration 19/25 | Loss: 0.00132244
Iteration 20/25 | Loss: 0.00131126
Iteration 21/25 | Loss: 0.00130770
Iteration 22/25 | Loss: 0.00129920
Iteration 23/25 | Loss: 0.00129939
Iteration 24/25 | Loss: 0.00129815
Iteration 25/25 | Loss: 0.00129818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45488238
Iteration 2/25 | Loss: 0.00099969
Iteration 3/25 | Loss: 0.00099968
Iteration 4/25 | Loss: 0.00099968
Iteration 5/25 | Loss: 0.00099968
Iteration 6/25 | Loss: 0.00099968
Iteration 7/25 | Loss: 0.00099968
Iteration 8/25 | Loss: 0.00099968
Iteration 9/25 | Loss: 0.00099968
Iteration 10/25 | Loss: 0.00099968
Iteration 11/25 | Loss: 0.00099968
Iteration 12/25 | Loss: 0.00099968
Iteration 13/25 | Loss: 0.00099968
Iteration 14/25 | Loss: 0.00099968
Iteration 15/25 | Loss: 0.00099968
Iteration 16/25 | Loss: 0.00099968
Iteration 17/25 | Loss: 0.00099968
Iteration 18/25 | Loss: 0.00099968
Iteration 19/25 | Loss: 0.00099968
Iteration 20/25 | Loss: 0.00099968
Iteration 21/25 | Loss: 0.00099968
Iteration 22/25 | Loss: 0.00099968
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000999681418761611, 0.000999681418761611, 0.000999681418761611, 0.000999681418761611, 0.000999681418761611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000999681418761611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099968
Iteration 2/1000 | Loss: 0.00003865
Iteration 3/1000 | Loss: 0.00056958
Iteration 4/1000 | Loss: 0.00005072
Iteration 5/1000 | Loss: 0.00002480
Iteration 6/1000 | Loss: 0.00025292
Iteration 7/1000 | Loss: 0.00004839
Iteration 8/1000 | Loss: 0.00004877
Iteration 9/1000 | Loss: 0.00003558
Iteration 10/1000 | Loss: 0.00004481
Iteration 11/1000 | Loss: 0.00004908
Iteration 12/1000 | Loss: 0.00007293
Iteration 13/1000 | Loss: 0.00004400
Iteration 14/1000 | Loss: 0.00003844
Iteration 15/1000 | Loss: 0.00006505
Iteration 16/1000 | Loss: 0.00005500
Iteration 17/1000 | Loss: 0.00005747
Iteration 18/1000 | Loss: 0.00006973
Iteration 19/1000 | Loss: 0.00003479
Iteration 20/1000 | Loss: 0.00005742
Iteration 21/1000 | Loss: 0.00005505
Iteration 22/1000 | Loss: 0.00007175
Iteration 23/1000 | Loss: 0.00005752
Iteration 24/1000 | Loss: 0.00005171
Iteration 25/1000 | Loss: 0.00005585
Iteration 26/1000 | Loss: 0.00002528
Iteration 27/1000 | Loss: 0.00002798
Iteration 28/1000 | Loss: 0.00002089
Iteration 29/1000 | Loss: 0.00001979
Iteration 30/1000 | Loss: 0.00001860
Iteration 31/1000 | Loss: 0.00001808
Iteration 32/1000 | Loss: 0.00001773
Iteration 33/1000 | Loss: 0.00001740
Iteration 34/1000 | Loss: 0.00001716
Iteration 35/1000 | Loss: 0.00001691
Iteration 36/1000 | Loss: 0.00001682
Iteration 37/1000 | Loss: 0.00001670
Iteration 38/1000 | Loss: 0.00001665
Iteration 39/1000 | Loss: 0.00001665
Iteration 40/1000 | Loss: 0.00001664
Iteration 41/1000 | Loss: 0.00001657
Iteration 42/1000 | Loss: 0.00001657
Iteration 43/1000 | Loss: 0.00001653
Iteration 44/1000 | Loss: 0.00001653
Iteration 45/1000 | Loss: 0.00001652
Iteration 46/1000 | Loss: 0.00001652
Iteration 47/1000 | Loss: 0.00001648
Iteration 48/1000 | Loss: 0.00001645
Iteration 49/1000 | Loss: 0.00001643
Iteration 50/1000 | Loss: 0.00001642
Iteration 51/1000 | Loss: 0.00001642
Iteration 52/1000 | Loss: 0.00001639
Iteration 53/1000 | Loss: 0.00001638
Iteration 54/1000 | Loss: 0.00001637
Iteration 55/1000 | Loss: 0.00001636
Iteration 56/1000 | Loss: 0.00001635
Iteration 57/1000 | Loss: 0.00001632
Iteration 58/1000 | Loss: 0.00001632
Iteration 59/1000 | Loss: 0.00001629
Iteration 60/1000 | Loss: 0.00001628
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001625
Iteration 65/1000 | Loss: 0.00001625
Iteration 66/1000 | Loss: 0.00001624
Iteration 67/1000 | Loss: 0.00001624
Iteration 68/1000 | Loss: 0.00001624
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001624
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001623
Iteration 73/1000 | Loss: 0.00001623
Iteration 74/1000 | Loss: 0.00001622
Iteration 75/1000 | Loss: 0.00001622
Iteration 76/1000 | Loss: 0.00001622
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001621
Iteration 79/1000 | Loss: 0.00001621
Iteration 80/1000 | Loss: 0.00001621
Iteration 81/1000 | Loss: 0.00001621
Iteration 82/1000 | Loss: 0.00001620
Iteration 83/1000 | Loss: 0.00001620
Iteration 84/1000 | Loss: 0.00001620
Iteration 85/1000 | Loss: 0.00001620
Iteration 86/1000 | Loss: 0.00001620
Iteration 87/1000 | Loss: 0.00001620
Iteration 88/1000 | Loss: 0.00001620
Iteration 89/1000 | Loss: 0.00001620
Iteration 90/1000 | Loss: 0.00001620
Iteration 91/1000 | Loss: 0.00001620
Iteration 92/1000 | Loss: 0.00001620
Iteration 93/1000 | Loss: 0.00001619
Iteration 94/1000 | Loss: 0.00001619
Iteration 95/1000 | Loss: 0.00001619
Iteration 96/1000 | Loss: 0.00001619
Iteration 97/1000 | Loss: 0.00001619
Iteration 98/1000 | Loss: 0.00001619
Iteration 99/1000 | Loss: 0.00001619
Iteration 100/1000 | Loss: 0.00001619
Iteration 101/1000 | Loss: 0.00001619
Iteration 102/1000 | Loss: 0.00001619
Iteration 103/1000 | Loss: 0.00001618
Iteration 104/1000 | Loss: 0.00001618
Iteration 105/1000 | Loss: 0.00001618
Iteration 106/1000 | Loss: 0.00001618
Iteration 107/1000 | Loss: 0.00001618
Iteration 108/1000 | Loss: 0.00001618
Iteration 109/1000 | Loss: 0.00001618
Iteration 110/1000 | Loss: 0.00001618
Iteration 111/1000 | Loss: 0.00001618
Iteration 112/1000 | Loss: 0.00001618
Iteration 113/1000 | Loss: 0.00001618
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001618
Iteration 117/1000 | Loss: 0.00001618
Iteration 118/1000 | Loss: 0.00001618
Iteration 119/1000 | Loss: 0.00001617
Iteration 120/1000 | Loss: 0.00001617
Iteration 121/1000 | Loss: 0.00001617
Iteration 122/1000 | Loss: 0.00001617
Iteration 123/1000 | Loss: 0.00001617
Iteration 124/1000 | Loss: 0.00001617
Iteration 125/1000 | Loss: 0.00001617
Iteration 126/1000 | Loss: 0.00001617
Iteration 127/1000 | Loss: 0.00001617
Iteration 128/1000 | Loss: 0.00001617
Iteration 129/1000 | Loss: 0.00001617
Iteration 130/1000 | Loss: 0.00001617
Iteration 131/1000 | Loss: 0.00001617
Iteration 132/1000 | Loss: 0.00001616
Iteration 133/1000 | Loss: 0.00001616
Iteration 134/1000 | Loss: 0.00001616
Iteration 135/1000 | Loss: 0.00001616
Iteration 136/1000 | Loss: 0.00001616
Iteration 137/1000 | Loss: 0.00001616
Iteration 138/1000 | Loss: 0.00001616
Iteration 139/1000 | Loss: 0.00001616
Iteration 140/1000 | Loss: 0.00001616
Iteration 141/1000 | Loss: 0.00001616
Iteration 142/1000 | Loss: 0.00001615
Iteration 143/1000 | Loss: 0.00001615
Iteration 144/1000 | Loss: 0.00001615
Iteration 145/1000 | Loss: 0.00001615
Iteration 146/1000 | Loss: 0.00001615
Iteration 147/1000 | Loss: 0.00001615
Iteration 148/1000 | Loss: 0.00001615
Iteration 149/1000 | Loss: 0.00001615
Iteration 150/1000 | Loss: 0.00001614
Iteration 151/1000 | Loss: 0.00001614
Iteration 152/1000 | Loss: 0.00001614
Iteration 153/1000 | Loss: 0.00001614
Iteration 154/1000 | Loss: 0.00001614
Iteration 155/1000 | Loss: 0.00001614
Iteration 156/1000 | Loss: 0.00001614
Iteration 157/1000 | Loss: 0.00001614
Iteration 158/1000 | Loss: 0.00001614
Iteration 159/1000 | Loss: 0.00001614
Iteration 160/1000 | Loss: 0.00001614
Iteration 161/1000 | Loss: 0.00001614
Iteration 162/1000 | Loss: 0.00001614
Iteration 163/1000 | Loss: 0.00001613
Iteration 164/1000 | Loss: 0.00001613
Iteration 165/1000 | Loss: 0.00001613
Iteration 166/1000 | Loss: 0.00001613
Iteration 167/1000 | Loss: 0.00001613
Iteration 168/1000 | Loss: 0.00001613
Iteration 169/1000 | Loss: 0.00001613
Iteration 170/1000 | Loss: 0.00001613
Iteration 171/1000 | Loss: 0.00001613
Iteration 172/1000 | Loss: 0.00001613
Iteration 173/1000 | Loss: 0.00001613
Iteration 174/1000 | Loss: 0.00001613
Iteration 175/1000 | Loss: 0.00001612
Iteration 176/1000 | Loss: 0.00001612
Iteration 177/1000 | Loss: 0.00001612
Iteration 178/1000 | Loss: 0.00001612
Iteration 179/1000 | Loss: 0.00001612
Iteration 180/1000 | Loss: 0.00001612
Iteration 181/1000 | Loss: 0.00001612
Iteration 182/1000 | Loss: 0.00001612
Iteration 183/1000 | Loss: 0.00001612
Iteration 184/1000 | Loss: 0.00001612
Iteration 185/1000 | Loss: 0.00001612
Iteration 186/1000 | Loss: 0.00001612
Iteration 187/1000 | Loss: 0.00001612
Iteration 188/1000 | Loss: 0.00001612
Iteration 189/1000 | Loss: 0.00001612
Iteration 190/1000 | Loss: 0.00001612
Iteration 191/1000 | Loss: 0.00001612
Iteration 192/1000 | Loss: 0.00001612
Iteration 193/1000 | Loss: 0.00001611
Iteration 194/1000 | Loss: 0.00001611
Iteration 195/1000 | Loss: 0.00001611
Iteration 196/1000 | Loss: 0.00001611
Iteration 197/1000 | Loss: 0.00001611
Iteration 198/1000 | Loss: 0.00001611
Iteration 199/1000 | Loss: 0.00001611
Iteration 200/1000 | Loss: 0.00001611
Iteration 201/1000 | Loss: 0.00001611
Iteration 202/1000 | Loss: 0.00001611
Iteration 203/1000 | Loss: 0.00001611
Iteration 204/1000 | Loss: 0.00001610
Iteration 205/1000 | Loss: 0.00001610
Iteration 206/1000 | Loss: 0.00001610
Iteration 207/1000 | Loss: 0.00001610
Iteration 208/1000 | Loss: 0.00001610
Iteration 209/1000 | Loss: 0.00001610
Iteration 210/1000 | Loss: 0.00001610
Iteration 211/1000 | Loss: 0.00001610
Iteration 212/1000 | Loss: 0.00001610
Iteration 213/1000 | Loss: 0.00001610
Iteration 214/1000 | Loss: 0.00001610
Iteration 215/1000 | Loss: 0.00001610
Iteration 216/1000 | Loss: 0.00001610
Iteration 217/1000 | Loss: 0.00001610
Iteration 218/1000 | Loss: 0.00001610
Iteration 219/1000 | Loss: 0.00001610
Iteration 220/1000 | Loss: 0.00001610
Iteration 221/1000 | Loss: 0.00001610
Iteration 222/1000 | Loss: 0.00001610
Iteration 223/1000 | Loss: 0.00001610
Iteration 224/1000 | Loss: 0.00001609
Iteration 225/1000 | Loss: 0.00001609
Iteration 226/1000 | Loss: 0.00001609
Iteration 227/1000 | Loss: 0.00001609
Iteration 228/1000 | Loss: 0.00001609
Iteration 229/1000 | Loss: 0.00001609
Iteration 230/1000 | Loss: 0.00001609
Iteration 231/1000 | Loss: 0.00001609
Iteration 232/1000 | Loss: 0.00001609
Iteration 233/1000 | Loss: 0.00001609
Iteration 234/1000 | Loss: 0.00001609
Iteration 235/1000 | Loss: 0.00001609
Iteration 236/1000 | Loss: 0.00001609
Iteration 237/1000 | Loss: 0.00001609
Iteration 238/1000 | Loss: 0.00001609
Iteration 239/1000 | Loss: 0.00001609
Iteration 240/1000 | Loss: 0.00001609
Iteration 241/1000 | Loss: 0.00001609
Iteration 242/1000 | Loss: 0.00001609
Iteration 243/1000 | Loss: 0.00001609
Iteration 244/1000 | Loss: 0.00001609
Iteration 245/1000 | Loss: 0.00001609
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.6089821656350978e-05, 1.6089821656350978e-05, 1.6089821656350978e-05, 1.6089821656350978e-05, 1.6089821656350978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6089821656350978e-05

Optimization complete. Final v2v error: 3.3262062072753906 mm

Highest mean error: 3.9278385639190674 mm for frame 14

Lowest mean error: 2.908931016921997 mm for frame 75

Saving results

Total time: 111.01620864868164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592943
Iteration 2/25 | Loss: 0.00126327
Iteration 3/25 | Loss: 0.00119405
Iteration 4/25 | Loss: 0.00118508
Iteration 5/25 | Loss: 0.00118213
Iteration 6/25 | Loss: 0.00118194
Iteration 7/25 | Loss: 0.00118194
Iteration 8/25 | Loss: 0.00118194
Iteration 9/25 | Loss: 0.00118194
Iteration 10/25 | Loss: 0.00118194
Iteration 11/25 | Loss: 0.00118194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011819357750937343, 0.0011819357750937343, 0.0011819357750937343, 0.0011819357750937343, 0.0011819357750937343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011819357750937343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.13961101
Iteration 2/25 | Loss: 0.00076020
Iteration 3/25 | Loss: 0.00076020
Iteration 4/25 | Loss: 0.00076019
Iteration 5/25 | Loss: 0.00076019
Iteration 6/25 | Loss: 0.00076019
Iteration 7/25 | Loss: 0.00076019
Iteration 8/25 | Loss: 0.00076019
Iteration 9/25 | Loss: 0.00076019
Iteration 10/25 | Loss: 0.00076019
Iteration 11/25 | Loss: 0.00076019
Iteration 12/25 | Loss: 0.00076019
Iteration 13/25 | Loss: 0.00076019
Iteration 14/25 | Loss: 0.00076019
Iteration 15/25 | Loss: 0.00076019
Iteration 16/25 | Loss: 0.00076019
Iteration 17/25 | Loss: 0.00076019
Iteration 18/25 | Loss: 0.00076019
Iteration 19/25 | Loss: 0.00076019
Iteration 20/25 | Loss: 0.00076019
Iteration 21/25 | Loss: 0.00076019
Iteration 22/25 | Loss: 0.00076019
Iteration 23/25 | Loss: 0.00076019
Iteration 24/25 | Loss: 0.00076019
Iteration 25/25 | Loss: 0.00076019

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076019
Iteration 2/1000 | Loss: 0.00002006
Iteration 3/1000 | Loss: 0.00001620
Iteration 4/1000 | Loss: 0.00001480
Iteration 5/1000 | Loss: 0.00001387
Iteration 6/1000 | Loss: 0.00001326
Iteration 7/1000 | Loss: 0.00001294
Iteration 8/1000 | Loss: 0.00001267
Iteration 9/1000 | Loss: 0.00001244
Iteration 10/1000 | Loss: 0.00001239
Iteration 11/1000 | Loss: 0.00001236
Iteration 12/1000 | Loss: 0.00001234
Iteration 13/1000 | Loss: 0.00001234
Iteration 14/1000 | Loss: 0.00001232
Iteration 15/1000 | Loss: 0.00001232
Iteration 16/1000 | Loss: 0.00001223
Iteration 17/1000 | Loss: 0.00001220
Iteration 18/1000 | Loss: 0.00001216
Iteration 19/1000 | Loss: 0.00001214
Iteration 20/1000 | Loss: 0.00001209
Iteration 21/1000 | Loss: 0.00001208
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001206
Iteration 24/1000 | Loss: 0.00001205
Iteration 25/1000 | Loss: 0.00001204
Iteration 26/1000 | Loss: 0.00001204
Iteration 27/1000 | Loss: 0.00001204
Iteration 28/1000 | Loss: 0.00001203
Iteration 29/1000 | Loss: 0.00001202
Iteration 30/1000 | Loss: 0.00001202
Iteration 31/1000 | Loss: 0.00001201
Iteration 32/1000 | Loss: 0.00001200
Iteration 33/1000 | Loss: 0.00001200
Iteration 34/1000 | Loss: 0.00001199
Iteration 35/1000 | Loss: 0.00001196
Iteration 36/1000 | Loss: 0.00001194
Iteration 37/1000 | Loss: 0.00001193
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001188
Iteration 40/1000 | Loss: 0.00001188
Iteration 41/1000 | Loss: 0.00001184
Iteration 42/1000 | Loss: 0.00001183
Iteration 43/1000 | Loss: 0.00001183
Iteration 44/1000 | Loss: 0.00001183
Iteration 45/1000 | Loss: 0.00001181
Iteration 46/1000 | Loss: 0.00001177
Iteration 47/1000 | Loss: 0.00001176
Iteration 48/1000 | Loss: 0.00001176
Iteration 49/1000 | Loss: 0.00001175
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001171
Iteration 54/1000 | Loss: 0.00001171
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001170
Iteration 58/1000 | Loss: 0.00001170
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001168
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001168
Iteration 65/1000 | Loss: 0.00001167
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001165
Iteration 68/1000 | Loss: 0.00001165
Iteration 69/1000 | Loss: 0.00001165
Iteration 70/1000 | Loss: 0.00001165
Iteration 71/1000 | Loss: 0.00001165
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001164
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001163
Iteration 78/1000 | Loss: 0.00001163
Iteration 79/1000 | Loss: 0.00001163
Iteration 80/1000 | Loss: 0.00001162
Iteration 81/1000 | Loss: 0.00001162
Iteration 82/1000 | Loss: 0.00001162
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001160
Iteration 86/1000 | Loss: 0.00001160
Iteration 87/1000 | Loss: 0.00001160
Iteration 88/1000 | Loss: 0.00001160
Iteration 89/1000 | Loss: 0.00001160
Iteration 90/1000 | Loss: 0.00001160
Iteration 91/1000 | Loss: 0.00001159
Iteration 92/1000 | Loss: 0.00001159
Iteration 93/1000 | Loss: 0.00001159
Iteration 94/1000 | Loss: 0.00001159
Iteration 95/1000 | Loss: 0.00001159
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001159
Iteration 99/1000 | Loss: 0.00001158
Iteration 100/1000 | Loss: 0.00001158
Iteration 101/1000 | Loss: 0.00001157
Iteration 102/1000 | Loss: 0.00001157
Iteration 103/1000 | Loss: 0.00001156
Iteration 104/1000 | Loss: 0.00001156
Iteration 105/1000 | Loss: 0.00001156
Iteration 106/1000 | Loss: 0.00001156
Iteration 107/1000 | Loss: 0.00001156
Iteration 108/1000 | Loss: 0.00001156
Iteration 109/1000 | Loss: 0.00001156
Iteration 110/1000 | Loss: 0.00001156
Iteration 111/1000 | Loss: 0.00001156
Iteration 112/1000 | Loss: 0.00001155
Iteration 113/1000 | Loss: 0.00001155
Iteration 114/1000 | Loss: 0.00001155
Iteration 115/1000 | Loss: 0.00001155
Iteration 116/1000 | Loss: 0.00001155
Iteration 117/1000 | Loss: 0.00001155
Iteration 118/1000 | Loss: 0.00001155
Iteration 119/1000 | Loss: 0.00001155
Iteration 120/1000 | Loss: 0.00001155
Iteration 121/1000 | Loss: 0.00001155
Iteration 122/1000 | Loss: 0.00001155
Iteration 123/1000 | Loss: 0.00001154
Iteration 124/1000 | Loss: 0.00001154
Iteration 125/1000 | Loss: 0.00001154
Iteration 126/1000 | Loss: 0.00001154
Iteration 127/1000 | Loss: 0.00001154
Iteration 128/1000 | Loss: 0.00001154
Iteration 129/1000 | Loss: 0.00001154
Iteration 130/1000 | Loss: 0.00001154
Iteration 131/1000 | Loss: 0.00001154
Iteration 132/1000 | Loss: 0.00001153
Iteration 133/1000 | Loss: 0.00001153
Iteration 134/1000 | Loss: 0.00001153
Iteration 135/1000 | Loss: 0.00001153
Iteration 136/1000 | Loss: 0.00001153
Iteration 137/1000 | Loss: 0.00001153
Iteration 138/1000 | Loss: 0.00001153
Iteration 139/1000 | Loss: 0.00001153
Iteration 140/1000 | Loss: 0.00001153
Iteration 141/1000 | Loss: 0.00001153
Iteration 142/1000 | Loss: 0.00001153
Iteration 143/1000 | Loss: 0.00001153
Iteration 144/1000 | Loss: 0.00001153
Iteration 145/1000 | Loss: 0.00001153
Iteration 146/1000 | Loss: 0.00001153
Iteration 147/1000 | Loss: 0.00001153
Iteration 148/1000 | Loss: 0.00001153
Iteration 149/1000 | Loss: 0.00001153
Iteration 150/1000 | Loss: 0.00001153
Iteration 151/1000 | Loss: 0.00001152
Iteration 152/1000 | Loss: 0.00001152
Iteration 153/1000 | Loss: 0.00001152
Iteration 154/1000 | Loss: 0.00001152
Iteration 155/1000 | Loss: 0.00001152
Iteration 156/1000 | Loss: 0.00001152
Iteration 157/1000 | Loss: 0.00001152
Iteration 158/1000 | Loss: 0.00001152
Iteration 159/1000 | Loss: 0.00001152
Iteration 160/1000 | Loss: 0.00001152
Iteration 161/1000 | Loss: 0.00001151
Iteration 162/1000 | Loss: 0.00001151
Iteration 163/1000 | Loss: 0.00001151
Iteration 164/1000 | Loss: 0.00001151
Iteration 165/1000 | Loss: 0.00001151
Iteration 166/1000 | Loss: 0.00001151
Iteration 167/1000 | Loss: 0.00001151
Iteration 168/1000 | Loss: 0.00001151
Iteration 169/1000 | Loss: 0.00001151
Iteration 170/1000 | Loss: 0.00001151
Iteration 171/1000 | Loss: 0.00001151
Iteration 172/1000 | Loss: 0.00001151
Iteration 173/1000 | Loss: 0.00001151
Iteration 174/1000 | Loss: 0.00001151
Iteration 175/1000 | Loss: 0.00001151
Iteration 176/1000 | Loss: 0.00001151
Iteration 177/1000 | Loss: 0.00001151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.1510437616379932e-05, 1.1510437616379932e-05, 1.1510437616379932e-05, 1.1510437616379932e-05, 1.1510437616379932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1510437616379932e-05

Optimization complete. Final v2v error: 2.919407367706299 mm

Highest mean error: 3.1877176761627197 mm for frame 111

Lowest mean error: 2.7594051361083984 mm for frame 80

Saving results

Total time: 37.579217195510864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00357458
Iteration 2/25 | Loss: 0.00134030
Iteration 3/25 | Loss: 0.00121248
Iteration 4/25 | Loss: 0.00120063
Iteration 5/25 | Loss: 0.00119656
Iteration 6/25 | Loss: 0.00119632
Iteration 7/25 | Loss: 0.00119632
Iteration 8/25 | Loss: 0.00119632
Iteration 9/25 | Loss: 0.00119632
Iteration 10/25 | Loss: 0.00119632
Iteration 11/25 | Loss: 0.00119632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011963210999965668, 0.0011963210999965668, 0.0011963210999965668, 0.0011963210999965668, 0.0011963210999965668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011963210999965668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43277943
Iteration 2/25 | Loss: 0.00070192
Iteration 3/25 | Loss: 0.00070192
Iteration 4/25 | Loss: 0.00070191
Iteration 5/25 | Loss: 0.00070191
Iteration 6/25 | Loss: 0.00070191
Iteration 7/25 | Loss: 0.00070191
Iteration 8/25 | Loss: 0.00070191
Iteration 9/25 | Loss: 0.00070191
Iteration 10/25 | Loss: 0.00070191
Iteration 11/25 | Loss: 0.00070191
Iteration 12/25 | Loss: 0.00070191
Iteration 13/25 | Loss: 0.00070191
Iteration 14/25 | Loss: 0.00070191
Iteration 15/25 | Loss: 0.00070191
Iteration 16/25 | Loss: 0.00070191
Iteration 17/25 | Loss: 0.00070191
Iteration 18/25 | Loss: 0.00070191
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007019117474555969, 0.0007019117474555969, 0.0007019117474555969, 0.0007019117474555969, 0.0007019117474555969]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007019117474555969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070191
Iteration 2/1000 | Loss: 0.00003008
Iteration 3/1000 | Loss: 0.00001941
Iteration 4/1000 | Loss: 0.00001772
Iteration 5/1000 | Loss: 0.00001646
Iteration 6/1000 | Loss: 0.00001552
Iteration 7/1000 | Loss: 0.00001488
Iteration 8/1000 | Loss: 0.00001453
Iteration 9/1000 | Loss: 0.00001414
Iteration 10/1000 | Loss: 0.00001384
Iteration 11/1000 | Loss: 0.00001380
Iteration 12/1000 | Loss: 0.00001379
Iteration 13/1000 | Loss: 0.00001378
Iteration 14/1000 | Loss: 0.00001377
Iteration 15/1000 | Loss: 0.00001375
Iteration 16/1000 | Loss: 0.00001358
Iteration 17/1000 | Loss: 0.00001357
Iteration 18/1000 | Loss: 0.00001355
Iteration 19/1000 | Loss: 0.00001349
Iteration 20/1000 | Loss: 0.00001348
Iteration 21/1000 | Loss: 0.00001343
Iteration 22/1000 | Loss: 0.00001339
Iteration 23/1000 | Loss: 0.00001339
Iteration 24/1000 | Loss: 0.00001337
Iteration 25/1000 | Loss: 0.00001336
Iteration 26/1000 | Loss: 0.00001336
Iteration 27/1000 | Loss: 0.00001336
Iteration 28/1000 | Loss: 0.00001334
Iteration 29/1000 | Loss: 0.00001334
Iteration 30/1000 | Loss: 0.00001331
Iteration 31/1000 | Loss: 0.00001328
Iteration 32/1000 | Loss: 0.00001325
Iteration 33/1000 | Loss: 0.00001323
Iteration 34/1000 | Loss: 0.00001319
Iteration 35/1000 | Loss: 0.00001316
Iteration 36/1000 | Loss: 0.00001315
Iteration 37/1000 | Loss: 0.00001315
Iteration 38/1000 | Loss: 0.00001314
Iteration 39/1000 | Loss: 0.00001314
Iteration 40/1000 | Loss: 0.00001313
Iteration 41/1000 | Loss: 0.00001312
Iteration 42/1000 | Loss: 0.00001311
Iteration 43/1000 | Loss: 0.00001311
Iteration 44/1000 | Loss: 0.00001310
Iteration 45/1000 | Loss: 0.00001309
Iteration 46/1000 | Loss: 0.00001309
Iteration 47/1000 | Loss: 0.00001309
Iteration 48/1000 | Loss: 0.00001309
Iteration 49/1000 | Loss: 0.00001309
Iteration 50/1000 | Loss: 0.00001309
Iteration 51/1000 | Loss: 0.00001309
Iteration 52/1000 | Loss: 0.00001309
Iteration 53/1000 | Loss: 0.00001308
Iteration 54/1000 | Loss: 0.00001308
Iteration 55/1000 | Loss: 0.00001308
Iteration 56/1000 | Loss: 0.00001308
Iteration 57/1000 | Loss: 0.00001308
Iteration 58/1000 | Loss: 0.00001308
Iteration 59/1000 | Loss: 0.00001308
Iteration 60/1000 | Loss: 0.00001308
Iteration 61/1000 | Loss: 0.00001308
Iteration 62/1000 | Loss: 0.00001308
Iteration 63/1000 | Loss: 0.00001307
Iteration 64/1000 | Loss: 0.00001307
Iteration 65/1000 | Loss: 0.00001306
Iteration 66/1000 | Loss: 0.00001306
Iteration 67/1000 | Loss: 0.00001306
Iteration 68/1000 | Loss: 0.00001305
Iteration 69/1000 | Loss: 0.00001305
Iteration 70/1000 | Loss: 0.00001305
Iteration 71/1000 | Loss: 0.00001305
Iteration 72/1000 | Loss: 0.00001304
Iteration 73/1000 | Loss: 0.00001304
Iteration 74/1000 | Loss: 0.00001304
Iteration 75/1000 | Loss: 0.00001304
Iteration 76/1000 | Loss: 0.00001304
Iteration 77/1000 | Loss: 0.00001304
Iteration 78/1000 | Loss: 0.00001304
Iteration 79/1000 | Loss: 0.00001304
Iteration 80/1000 | Loss: 0.00001303
Iteration 81/1000 | Loss: 0.00001303
Iteration 82/1000 | Loss: 0.00001303
Iteration 83/1000 | Loss: 0.00001303
Iteration 84/1000 | Loss: 0.00001302
Iteration 85/1000 | Loss: 0.00001302
Iteration 86/1000 | Loss: 0.00001302
Iteration 87/1000 | Loss: 0.00001302
Iteration 88/1000 | Loss: 0.00001302
Iteration 89/1000 | Loss: 0.00001301
Iteration 90/1000 | Loss: 0.00001301
Iteration 91/1000 | Loss: 0.00001301
Iteration 92/1000 | Loss: 0.00001301
Iteration 93/1000 | Loss: 0.00001301
Iteration 94/1000 | Loss: 0.00001300
Iteration 95/1000 | Loss: 0.00001300
Iteration 96/1000 | Loss: 0.00001300
Iteration 97/1000 | Loss: 0.00001300
Iteration 98/1000 | Loss: 0.00001299
Iteration 99/1000 | Loss: 0.00001299
Iteration 100/1000 | Loss: 0.00001299
Iteration 101/1000 | Loss: 0.00001299
Iteration 102/1000 | Loss: 0.00001299
Iteration 103/1000 | Loss: 0.00001299
Iteration 104/1000 | Loss: 0.00001299
Iteration 105/1000 | Loss: 0.00001299
Iteration 106/1000 | Loss: 0.00001298
Iteration 107/1000 | Loss: 0.00001298
Iteration 108/1000 | Loss: 0.00001298
Iteration 109/1000 | Loss: 0.00001298
Iteration 110/1000 | Loss: 0.00001298
Iteration 111/1000 | Loss: 0.00001298
Iteration 112/1000 | Loss: 0.00001298
Iteration 113/1000 | Loss: 0.00001298
Iteration 114/1000 | Loss: 0.00001298
Iteration 115/1000 | Loss: 0.00001298
Iteration 116/1000 | Loss: 0.00001298
Iteration 117/1000 | Loss: 0.00001298
Iteration 118/1000 | Loss: 0.00001298
Iteration 119/1000 | Loss: 0.00001298
Iteration 120/1000 | Loss: 0.00001298
Iteration 121/1000 | Loss: 0.00001298
Iteration 122/1000 | Loss: 0.00001298
Iteration 123/1000 | Loss: 0.00001298
Iteration 124/1000 | Loss: 0.00001298
Iteration 125/1000 | Loss: 0.00001298
Iteration 126/1000 | Loss: 0.00001298
Iteration 127/1000 | Loss: 0.00001298
Iteration 128/1000 | Loss: 0.00001298
Iteration 129/1000 | Loss: 0.00001298
Iteration 130/1000 | Loss: 0.00001298
Iteration 131/1000 | Loss: 0.00001298
Iteration 132/1000 | Loss: 0.00001298
Iteration 133/1000 | Loss: 0.00001298
Iteration 134/1000 | Loss: 0.00001298
Iteration 135/1000 | Loss: 0.00001298
Iteration 136/1000 | Loss: 0.00001298
Iteration 137/1000 | Loss: 0.00001298
Iteration 138/1000 | Loss: 0.00001298
Iteration 139/1000 | Loss: 0.00001298
Iteration 140/1000 | Loss: 0.00001298
Iteration 141/1000 | Loss: 0.00001298
Iteration 142/1000 | Loss: 0.00001298
Iteration 143/1000 | Loss: 0.00001298
Iteration 144/1000 | Loss: 0.00001298
Iteration 145/1000 | Loss: 0.00001298
Iteration 146/1000 | Loss: 0.00001298
Iteration 147/1000 | Loss: 0.00001298
Iteration 148/1000 | Loss: 0.00001298
Iteration 149/1000 | Loss: 0.00001298
Iteration 150/1000 | Loss: 0.00001298
Iteration 151/1000 | Loss: 0.00001298
Iteration 152/1000 | Loss: 0.00001298
Iteration 153/1000 | Loss: 0.00001298
Iteration 154/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.2979561688553076e-05, 1.2979561688553076e-05, 1.2979561688553076e-05, 1.2979561688553076e-05, 1.2979561688553076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2979561688553076e-05

Optimization complete. Final v2v error: 3.0692861080169678 mm

Highest mean error: 3.37375545501709 mm for frame 162

Lowest mean error: 2.8017008304595947 mm for frame 68

Saving results

Total time: 42.04507040977478
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593490
Iteration 2/25 | Loss: 0.00142128
Iteration 3/25 | Loss: 0.00133465
Iteration 4/25 | Loss: 0.00130907
Iteration 5/25 | Loss: 0.00130412
Iteration 6/25 | Loss: 0.00130243
Iteration 7/25 | Loss: 0.00130210
Iteration 8/25 | Loss: 0.00130210
Iteration 9/25 | Loss: 0.00130210
Iteration 10/25 | Loss: 0.00130210
Iteration 11/25 | Loss: 0.00130210
Iteration 12/25 | Loss: 0.00130210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001302101882174611, 0.001302101882174611, 0.001302101882174611, 0.001302101882174611, 0.001302101882174611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001302101882174611

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05947888
Iteration 2/25 | Loss: 0.00104433
Iteration 3/25 | Loss: 0.00104424
Iteration 4/25 | Loss: 0.00104424
Iteration 5/25 | Loss: 0.00104424
Iteration 6/25 | Loss: 0.00104424
Iteration 7/25 | Loss: 0.00104424
Iteration 8/25 | Loss: 0.00104424
Iteration 9/25 | Loss: 0.00104424
Iteration 10/25 | Loss: 0.00104424
Iteration 11/25 | Loss: 0.00104424
Iteration 12/25 | Loss: 0.00104424
Iteration 13/25 | Loss: 0.00104424
Iteration 14/25 | Loss: 0.00104424
Iteration 15/25 | Loss: 0.00104424
Iteration 16/25 | Loss: 0.00104424
Iteration 17/25 | Loss: 0.00104424
Iteration 18/25 | Loss: 0.00104424
Iteration 19/25 | Loss: 0.00104424
Iteration 20/25 | Loss: 0.00104424
Iteration 21/25 | Loss: 0.00104424
Iteration 22/25 | Loss: 0.00104424
Iteration 23/25 | Loss: 0.00104424
Iteration 24/25 | Loss: 0.00104424
Iteration 25/25 | Loss: 0.00104424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104424
Iteration 2/1000 | Loss: 0.00005912
Iteration 3/1000 | Loss: 0.00004209
Iteration 4/1000 | Loss: 0.00003572
Iteration 5/1000 | Loss: 0.00003360
Iteration 6/1000 | Loss: 0.00003231
Iteration 7/1000 | Loss: 0.00003134
Iteration 8/1000 | Loss: 0.00003059
Iteration 9/1000 | Loss: 0.00003000
Iteration 10/1000 | Loss: 0.00002951
Iteration 11/1000 | Loss: 0.00002927
Iteration 12/1000 | Loss: 0.00002904
Iteration 13/1000 | Loss: 0.00002888
Iteration 14/1000 | Loss: 0.00002867
Iteration 15/1000 | Loss: 0.00002866
Iteration 16/1000 | Loss: 0.00002851
Iteration 17/1000 | Loss: 0.00002845
Iteration 18/1000 | Loss: 0.00002844
Iteration 19/1000 | Loss: 0.00002843
Iteration 20/1000 | Loss: 0.00002843
Iteration 21/1000 | Loss: 0.00002842
Iteration 22/1000 | Loss: 0.00002841
Iteration 23/1000 | Loss: 0.00002837
Iteration 24/1000 | Loss: 0.00002837
Iteration 25/1000 | Loss: 0.00002837
Iteration 26/1000 | Loss: 0.00002836
Iteration 27/1000 | Loss: 0.00002836
Iteration 28/1000 | Loss: 0.00002836
Iteration 29/1000 | Loss: 0.00002836
Iteration 30/1000 | Loss: 0.00002836
Iteration 31/1000 | Loss: 0.00002836
Iteration 32/1000 | Loss: 0.00002836
Iteration 33/1000 | Loss: 0.00002836
Iteration 34/1000 | Loss: 0.00002836
Iteration 35/1000 | Loss: 0.00002836
Iteration 36/1000 | Loss: 0.00002836
Iteration 37/1000 | Loss: 0.00002835
Iteration 38/1000 | Loss: 0.00002830
Iteration 39/1000 | Loss: 0.00002829
Iteration 40/1000 | Loss: 0.00002828
Iteration 41/1000 | Loss: 0.00002828
Iteration 42/1000 | Loss: 0.00002828
Iteration 43/1000 | Loss: 0.00002828
Iteration 44/1000 | Loss: 0.00002828
Iteration 45/1000 | Loss: 0.00002827
Iteration 46/1000 | Loss: 0.00002827
Iteration 47/1000 | Loss: 0.00002827
Iteration 48/1000 | Loss: 0.00002827
Iteration 49/1000 | Loss: 0.00002823
Iteration 50/1000 | Loss: 0.00002821
Iteration 51/1000 | Loss: 0.00002820
Iteration 52/1000 | Loss: 0.00002820
Iteration 53/1000 | Loss: 0.00002820
Iteration 54/1000 | Loss: 0.00002819
Iteration 55/1000 | Loss: 0.00002819
Iteration 56/1000 | Loss: 0.00002819
Iteration 57/1000 | Loss: 0.00002818
Iteration 58/1000 | Loss: 0.00002817
Iteration 59/1000 | Loss: 0.00002817
Iteration 60/1000 | Loss: 0.00002816
Iteration 61/1000 | Loss: 0.00002816
Iteration 62/1000 | Loss: 0.00002815
Iteration 63/1000 | Loss: 0.00002815
Iteration 64/1000 | Loss: 0.00002814
Iteration 65/1000 | Loss: 0.00002813
Iteration 66/1000 | Loss: 0.00002813
Iteration 67/1000 | Loss: 0.00002813
Iteration 68/1000 | Loss: 0.00002812
Iteration 69/1000 | Loss: 0.00002812
Iteration 70/1000 | Loss: 0.00002812
Iteration 71/1000 | Loss: 0.00002812
Iteration 72/1000 | Loss: 0.00002811
Iteration 73/1000 | Loss: 0.00002811
Iteration 74/1000 | Loss: 0.00002811
Iteration 75/1000 | Loss: 0.00002811
Iteration 76/1000 | Loss: 0.00002811
Iteration 77/1000 | Loss: 0.00002811
Iteration 78/1000 | Loss: 0.00002810
Iteration 79/1000 | Loss: 0.00002809
Iteration 80/1000 | Loss: 0.00002808
Iteration 81/1000 | Loss: 0.00002808
Iteration 82/1000 | Loss: 0.00002808
Iteration 83/1000 | Loss: 0.00002808
Iteration 84/1000 | Loss: 0.00002808
Iteration 85/1000 | Loss: 0.00002808
Iteration 86/1000 | Loss: 0.00002807
Iteration 87/1000 | Loss: 0.00002807
Iteration 88/1000 | Loss: 0.00002806
Iteration 89/1000 | Loss: 0.00002806
Iteration 90/1000 | Loss: 0.00002806
Iteration 91/1000 | Loss: 0.00002806
Iteration 92/1000 | Loss: 0.00002806
Iteration 93/1000 | Loss: 0.00002805
Iteration 94/1000 | Loss: 0.00002803
Iteration 95/1000 | Loss: 0.00002802
Iteration 96/1000 | Loss: 0.00002802
Iteration 97/1000 | Loss: 0.00002802
Iteration 98/1000 | Loss: 0.00002802
Iteration 99/1000 | Loss: 0.00002802
Iteration 100/1000 | Loss: 0.00002801
Iteration 101/1000 | Loss: 0.00002801
Iteration 102/1000 | Loss: 0.00002801
Iteration 103/1000 | Loss: 0.00002801
Iteration 104/1000 | Loss: 0.00002801
Iteration 105/1000 | Loss: 0.00002801
Iteration 106/1000 | Loss: 0.00002800
Iteration 107/1000 | Loss: 0.00002800
Iteration 108/1000 | Loss: 0.00002799
Iteration 109/1000 | Loss: 0.00002799
Iteration 110/1000 | Loss: 0.00002799
Iteration 111/1000 | Loss: 0.00002798
Iteration 112/1000 | Loss: 0.00002798
Iteration 113/1000 | Loss: 0.00002798
Iteration 114/1000 | Loss: 0.00002798
Iteration 115/1000 | Loss: 0.00002798
Iteration 116/1000 | Loss: 0.00002798
Iteration 117/1000 | Loss: 0.00002798
Iteration 118/1000 | Loss: 0.00002798
Iteration 119/1000 | Loss: 0.00002798
Iteration 120/1000 | Loss: 0.00002798
Iteration 121/1000 | Loss: 0.00002798
Iteration 122/1000 | Loss: 0.00002798
Iteration 123/1000 | Loss: 0.00002797
Iteration 124/1000 | Loss: 0.00002797
Iteration 125/1000 | Loss: 0.00002797
Iteration 126/1000 | Loss: 0.00002797
Iteration 127/1000 | Loss: 0.00002797
Iteration 128/1000 | Loss: 0.00002797
Iteration 129/1000 | Loss: 0.00002797
Iteration 130/1000 | Loss: 0.00002796
Iteration 131/1000 | Loss: 0.00002796
Iteration 132/1000 | Loss: 0.00002796
Iteration 133/1000 | Loss: 0.00002796
Iteration 134/1000 | Loss: 0.00002795
Iteration 135/1000 | Loss: 0.00002795
Iteration 136/1000 | Loss: 0.00002795
Iteration 137/1000 | Loss: 0.00002795
Iteration 138/1000 | Loss: 0.00002794
Iteration 139/1000 | Loss: 0.00002794
Iteration 140/1000 | Loss: 0.00002794
Iteration 141/1000 | Loss: 0.00002794
Iteration 142/1000 | Loss: 0.00002793
Iteration 143/1000 | Loss: 0.00002793
Iteration 144/1000 | Loss: 0.00002793
Iteration 145/1000 | Loss: 0.00002793
Iteration 146/1000 | Loss: 0.00002793
Iteration 147/1000 | Loss: 0.00002793
Iteration 148/1000 | Loss: 0.00002793
Iteration 149/1000 | Loss: 0.00002793
Iteration 150/1000 | Loss: 0.00002793
Iteration 151/1000 | Loss: 0.00002793
Iteration 152/1000 | Loss: 0.00002793
Iteration 153/1000 | Loss: 0.00002793
Iteration 154/1000 | Loss: 0.00002793
Iteration 155/1000 | Loss: 0.00002793
Iteration 156/1000 | Loss: 0.00002793
Iteration 157/1000 | Loss: 0.00002793
Iteration 158/1000 | Loss: 0.00002793
Iteration 159/1000 | Loss: 0.00002793
Iteration 160/1000 | Loss: 0.00002793
Iteration 161/1000 | Loss: 0.00002793
Iteration 162/1000 | Loss: 0.00002793
Iteration 163/1000 | Loss: 0.00002793
Iteration 164/1000 | Loss: 0.00002793
Iteration 165/1000 | Loss: 0.00002793
Iteration 166/1000 | Loss: 0.00002793
Iteration 167/1000 | Loss: 0.00002793
Iteration 168/1000 | Loss: 0.00002793
Iteration 169/1000 | Loss: 0.00002793
Iteration 170/1000 | Loss: 0.00002793
Iteration 171/1000 | Loss: 0.00002793
Iteration 172/1000 | Loss: 0.00002793
Iteration 173/1000 | Loss: 0.00002793
Iteration 174/1000 | Loss: 0.00002793
Iteration 175/1000 | Loss: 0.00002793
Iteration 176/1000 | Loss: 0.00002793
Iteration 177/1000 | Loss: 0.00002793
Iteration 178/1000 | Loss: 0.00002793
Iteration 179/1000 | Loss: 0.00002793
Iteration 180/1000 | Loss: 0.00002793
Iteration 181/1000 | Loss: 0.00002793
Iteration 182/1000 | Loss: 0.00002793
Iteration 183/1000 | Loss: 0.00002793
Iteration 184/1000 | Loss: 0.00002793
Iteration 185/1000 | Loss: 0.00002793
Iteration 186/1000 | Loss: 0.00002793
Iteration 187/1000 | Loss: 0.00002793
Iteration 188/1000 | Loss: 0.00002793
Iteration 189/1000 | Loss: 0.00002793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.7931168006034568e-05, 2.7931168006034568e-05, 2.7931168006034568e-05, 2.7931168006034568e-05, 2.7931168006034568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7931168006034568e-05

Optimization complete. Final v2v error: 4.428097248077393 mm

Highest mean error: 4.740345478057861 mm for frame 62

Lowest mean error: 4.102829933166504 mm for frame 12

Saving results

Total time: 42.18007969856262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494212
Iteration 2/25 | Loss: 0.00137975
Iteration 3/25 | Loss: 0.00127938
Iteration 4/25 | Loss: 0.00126955
Iteration 5/25 | Loss: 0.00126661
Iteration 6/25 | Loss: 0.00126629
Iteration 7/25 | Loss: 0.00126629
Iteration 8/25 | Loss: 0.00126629
Iteration 9/25 | Loss: 0.00126629
Iteration 10/25 | Loss: 0.00126629
Iteration 11/25 | Loss: 0.00126629
Iteration 12/25 | Loss: 0.00126629
Iteration 13/25 | Loss: 0.00126629
Iteration 14/25 | Loss: 0.00126629
Iteration 15/25 | Loss: 0.00126629
Iteration 16/25 | Loss: 0.00126629
Iteration 17/25 | Loss: 0.00126629
Iteration 18/25 | Loss: 0.00126629
Iteration 19/25 | Loss: 0.00126629
Iteration 20/25 | Loss: 0.00126629
Iteration 21/25 | Loss: 0.00126629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012662893859669566, 0.0012662893859669566, 0.0012662893859669566, 0.0012662893859669566, 0.0012662893859669566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012662893859669566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53704965
Iteration 2/25 | Loss: 0.00075659
Iteration 3/25 | Loss: 0.00075657
Iteration 4/25 | Loss: 0.00075657
Iteration 5/25 | Loss: 0.00075657
Iteration 6/25 | Loss: 0.00075657
Iteration 7/25 | Loss: 0.00075657
Iteration 8/25 | Loss: 0.00075657
Iteration 9/25 | Loss: 0.00075657
Iteration 10/25 | Loss: 0.00075657
Iteration 11/25 | Loss: 0.00075657
Iteration 12/25 | Loss: 0.00075657
Iteration 13/25 | Loss: 0.00075657
Iteration 14/25 | Loss: 0.00075657
Iteration 15/25 | Loss: 0.00075657
Iteration 16/25 | Loss: 0.00075657
Iteration 17/25 | Loss: 0.00075657
Iteration 18/25 | Loss: 0.00075657
Iteration 19/25 | Loss: 0.00075657
Iteration 20/25 | Loss: 0.00075657
Iteration 21/25 | Loss: 0.00075657
Iteration 22/25 | Loss: 0.00075657
Iteration 23/25 | Loss: 0.00075657
Iteration 24/25 | Loss: 0.00075657
Iteration 25/25 | Loss: 0.00075657
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007565689156763256, 0.0007565689156763256, 0.0007565689156763256, 0.0007565689156763256, 0.0007565689156763256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007565689156763256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075657
Iteration 2/1000 | Loss: 0.00003689
Iteration 3/1000 | Loss: 0.00002633
Iteration 4/1000 | Loss: 0.00002194
Iteration 5/1000 | Loss: 0.00002056
Iteration 6/1000 | Loss: 0.00001960
Iteration 7/1000 | Loss: 0.00001892
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001808
Iteration 10/1000 | Loss: 0.00001777
Iteration 11/1000 | Loss: 0.00001749
Iteration 12/1000 | Loss: 0.00001732
Iteration 13/1000 | Loss: 0.00001730
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001720
Iteration 16/1000 | Loss: 0.00001709
Iteration 17/1000 | Loss: 0.00001706
Iteration 18/1000 | Loss: 0.00001704
Iteration 19/1000 | Loss: 0.00001702
Iteration 20/1000 | Loss: 0.00001702
Iteration 21/1000 | Loss: 0.00001701
Iteration 22/1000 | Loss: 0.00001701
Iteration 23/1000 | Loss: 0.00001698
Iteration 24/1000 | Loss: 0.00001698
Iteration 25/1000 | Loss: 0.00001697
Iteration 26/1000 | Loss: 0.00001697
Iteration 27/1000 | Loss: 0.00001697
Iteration 28/1000 | Loss: 0.00001697
Iteration 29/1000 | Loss: 0.00001697
Iteration 30/1000 | Loss: 0.00001696
Iteration 31/1000 | Loss: 0.00001696
Iteration 32/1000 | Loss: 0.00001695
Iteration 33/1000 | Loss: 0.00001695
Iteration 34/1000 | Loss: 0.00001695
Iteration 35/1000 | Loss: 0.00001694
Iteration 36/1000 | Loss: 0.00001694
Iteration 37/1000 | Loss: 0.00001693
Iteration 38/1000 | Loss: 0.00001693
Iteration 39/1000 | Loss: 0.00001693
Iteration 40/1000 | Loss: 0.00001693
Iteration 41/1000 | Loss: 0.00001691
Iteration 42/1000 | Loss: 0.00001691
Iteration 43/1000 | Loss: 0.00001691
Iteration 44/1000 | Loss: 0.00001691
Iteration 45/1000 | Loss: 0.00001691
Iteration 46/1000 | Loss: 0.00001691
Iteration 47/1000 | Loss: 0.00001691
Iteration 48/1000 | Loss: 0.00001691
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001691
Iteration 51/1000 | Loss: 0.00001691
Iteration 52/1000 | Loss: 0.00001691
Iteration 53/1000 | Loss: 0.00001691
Iteration 54/1000 | Loss: 0.00001691
Iteration 55/1000 | Loss: 0.00001691
Iteration 56/1000 | Loss: 0.00001691
Iteration 57/1000 | Loss: 0.00001691
Iteration 58/1000 | Loss: 0.00001691
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001691
Iteration 62/1000 | Loss: 0.00001691
Iteration 63/1000 | Loss: 0.00001691
Iteration 64/1000 | Loss: 0.00001691
Iteration 65/1000 | Loss: 0.00001691
Iteration 66/1000 | Loss: 0.00001691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [1.690653334662784e-05, 1.690653334662784e-05, 1.690653334662784e-05, 1.690653334662784e-05, 1.690653334662784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.690653334662784e-05

Optimization complete. Final v2v error: 3.3950695991516113 mm

Highest mean error: 4.5082173347473145 mm for frame 110

Lowest mean error: 2.827129602432251 mm for frame 166

Saving results

Total time: 31.619988918304443
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959937
Iteration 2/25 | Loss: 0.00959937
Iteration 3/25 | Loss: 0.00959937
Iteration 4/25 | Loss: 0.00959937
Iteration 5/25 | Loss: 0.00959936
Iteration 6/25 | Loss: 0.00959936
Iteration 7/25 | Loss: 0.00959936
Iteration 8/25 | Loss: 0.00959936
Iteration 9/25 | Loss: 0.00959936
Iteration 10/25 | Loss: 0.00959936
Iteration 11/25 | Loss: 0.00959935
Iteration 12/25 | Loss: 0.00959935
Iteration 13/25 | Loss: 0.00959935
Iteration 14/25 | Loss: 0.00959935
Iteration 15/25 | Loss: 0.00959934
Iteration 16/25 | Loss: 0.00959934
Iteration 17/25 | Loss: 0.00959934
Iteration 18/25 | Loss: 0.00959934
Iteration 19/25 | Loss: 0.00959934
Iteration 20/25 | Loss: 0.00959934
Iteration 21/25 | Loss: 0.00959933
Iteration 22/25 | Loss: 0.00959933
Iteration 23/25 | Loss: 0.00959933
Iteration 24/25 | Loss: 0.00959933
Iteration 25/25 | Loss: 0.00959933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45932543
Iteration 2/25 | Loss: 0.17885336
Iteration 3/25 | Loss: 0.17831387
Iteration 4/25 | Loss: 0.17753841
Iteration 5/25 | Loss: 0.17753839
Iteration 6/25 | Loss: 0.17753838
Iteration 7/25 | Loss: 0.17753838
Iteration 8/25 | Loss: 0.17753838
Iteration 9/25 | Loss: 0.17753838
Iteration 10/25 | Loss: 0.17753838
Iteration 11/25 | Loss: 0.17753838
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.17753838002681732, 0.17753838002681732, 0.17753838002681732, 0.17753838002681732, 0.17753838002681732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17753838002681732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17753838
Iteration 2/1000 | Loss: 0.00201393
Iteration 3/1000 | Loss: 0.00047255
Iteration 4/1000 | Loss: 0.00018761
Iteration 5/1000 | Loss: 0.00005321
Iteration 6/1000 | Loss: 0.00011023
Iteration 7/1000 | Loss: 0.00003380
Iteration 8/1000 | Loss: 0.00014652
Iteration 9/1000 | Loss: 0.00148220
Iteration 10/1000 | Loss: 0.00048678
Iteration 11/1000 | Loss: 0.00002607
Iteration 12/1000 | Loss: 0.00058266
Iteration 13/1000 | Loss: 0.00037705
Iteration 14/1000 | Loss: 0.00012007
Iteration 15/1000 | Loss: 0.00079638
Iteration 16/1000 | Loss: 0.00013155
Iteration 17/1000 | Loss: 0.00002883
Iteration 18/1000 | Loss: 0.00004364
Iteration 19/1000 | Loss: 0.00031331
Iteration 20/1000 | Loss: 0.00002139
Iteration 21/1000 | Loss: 0.00002577
Iteration 22/1000 | Loss: 0.00020927
Iteration 23/1000 | Loss: 0.00001785
Iteration 24/1000 | Loss: 0.00007370
Iteration 25/1000 | Loss: 0.00134464
Iteration 26/1000 | Loss: 0.00008695
Iteration 27/1000 | Loss: 0.00008317
Iteration 28/1000 | Loss: 0.00002525
Iteration 29/1000 | Loss: 0.00006282
Iteration 30/1000 | Loss: 0.00003518
Iteration 31/1000 | Loss: 0.00004409
Iteration 32/1000 | Loss: 0.00001629
Iteration 33/1000 | Loss: 0.00006997
Iteration 34/1000 | Loss: 0.00001635
Iteration 35/1000 | Loss: 0.00001592
Iteration 36/1000 | Loss: 0.00004480
Iteration 37/1000 | Loss: 0.00001577
Iteration 38/1000 | Loss: 0.00001577
Iteration 39/1000 | Loss: 0.00001576
Iteration 40/1000 | Loss: 0.00001574
Iteration 41/1000 | Loss: 0.00001566
Iteration 42/1000 | Loss: 0.00001563
Iteration 43/1000 | Loss: 0.00001563
Iteration 44/1000 | Loss: 0.00001563
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001561
Iteration 47/1000 | Loss: 0.00001561
Iteration 48/1000 | Loss: 0.00001555
Iteration 49/1000 | Loss: 0.00001555
Iteration 50/1000 | Loss: 0.00001554
Iteration 51/1000 | Loss: 0.00001554
Iteration 52/1000 | Loss: 0.00001553
Iteration 53/1000 | Loss: 0.00010208
Iteration 54/1000 | Loss: 0.00011343
Iteration 55/1000 | Loss: 0.00012814
Iteration 56/1000 | Loss: 0.00001562
Iteration 57/1000 | Loss: 0.00003392
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001533
Iteration 60/1000 | Loss: 0.00003397
Iteration 61/1000 | Loss: 0.00001645
Iteration 62/1000 | Loss: 0.00001532
Iteration 63/1000 | Loss: 0.00001532
Iteration 64/1000 | Loss: 0.00001531
Iteration 65/1000 | Loss: 0.00001531
Iteration 66/1000 | Loss: 0.00001531
Iteration 67/1000 | Loss: 0.00001531
Iteration 68/1000 | Loss: 0.00001531
Iteration 69/1000 | Loss: 0.00001531
Iteration 70/1000 | Loss: 0.00001531
Iteration 71/1000 | Loss: 0.00001531
Iteration 72/1000 | Loss: 0.00001531
Iteration 73/1000 | Loss: 0.00001531
Iteration 74/1000 | Loss: 0.00001531
Iteration 75/1000 | Loss: 0.00001531
Iteration 76/1000 | Loss: 0.00001531
Iteration 77/1000 | Loss: 0.00001531
Iteration 78/1000 | Loss: 0.00001530
Iteration 79/1000 | Loss: 0.00001530
Iteration 80/1000 | Loss: 0.00001530
Iteration 81/1000 | Loss: 0.00001530
Iteration 82/1000 | Loss: 0.00001530
Iteration 83/1000 | Loss: 0.00001530
Iteration 84/1000 | Loss: 0.00001530
Iteration 85/1000 | Loss: 0.00001530
Iteration 86/1000 | Loss: 0.00001530
Iteration 87/1000 | Loss: 0.00001530
Iteration 88/1000 | Loss: 0.00001530
Iteration 89/1000 | Loss: 0.00001530
Iteration 90/1000 | Loss: 0.00001530
Iteration 91/1000 | Loss: 0.00001530
Iteration 92/1000 | Loss: 0.00001530
Iteration 93/1000 | Loss: 0.00001530
Iteration 94/1000 | Loss: 0.00001530
Iteration 95/1000 | Loss: 0.00001530
Iteration 96/1000 | Loss: 0.00001530
Iteration 97/1000 | Loss: 0.00001530
Iteration 98/1000 | Loss: 0.00001530
Iteration 99/1000 | Loss: 0.00001530
Iteration 100/1000 | Loss: 0.00001530
Iteration 101/1000 | Loss: 0.00001530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.530232657387387e-05, 1.530232657387387e-05, 1.530232657387387e-05, 1.530232657387387e-05, 1.530232657387387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.530232657387387e-05

Optimization complete. Final v2v error: 3.3281261920928955 mm

Highest mean error: 3.6660640239715576 mm for frame 167

Lowest mean error: 3.1869590282440186 mm for frame 116

Saving results

Total time: 81.72684860229492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500519
Iteration 2/25 | Loss: 0.00145499
Iteration 3/25 | Loss: 0.00131703
Iteration 4/25 | Loss: 0.00128770
Iteration 5/25 | Loss: 0.00127954
Iteration 6/25 | Loss: 0.00127723
Iteration 7/25 | Loss: 0.00127652
Iteration 8/25 | Loss: 0.00127623
Iteration 9/25 | Loss: 0.00127623
Iteration 10/25 | Loss: 0.00127623
Iteration 11/25 | Loss: 0.00127623
Iteration 12/25 | Loss: 0.00127623
Iteration 13/25 | Loss: 0.00127623
Iteration 14/25 | Loss: 0.00127623
Iteration 15/25 | Loss: 0.00127623
Iteration 16/25 | Loss: 0.00127623
Iteration 17/25 | Loss: 0.00127623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012762267142534256, 0.0012762267142534256, 0.0012762267142534256, 0.0012762267142534256, 0.0012762267142534256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012762267142534256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47749126
Iteration 2/25 | Loss: 0.00106185
Iteration 3/25 | Loss: 0.00106184
Iteration 4/25 | Loss: 0.00106184
Iteration 5/25 | Loss: 0.00106184
Iteration 6/25 | Loss: 0.00106184
Iteration 7/25 | Loss: 0.00106184
Iteration 8/25 | Loss: 0.00106184
Iteration 9/25 | Loss: 0.00106184
Iteration 10/25 | Loss: 0.00106184
Iteration 11/25 | Loss: 0.00106184
Iteration 12/25 | Loss: 0.00106184
Iteration 13/25 | Loss: 0.00106184
Iteration 14/25 | Loss: 0.00106184
Iteration 15/25 | Loss: 0.00106184
Iteration 16/25 | Loss: 0.00106184
Iteration 17/25 | Loss: 0.00106184
Iteration 18/25 | Loss: 0.00106184
Iteration 19/25 | Loss: 0.00106184
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010618382366374135, 0.0010618382366374135, 0.0010618382366374135, 0.0010618382366374135, 0.0010618382366374135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010618382366374135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106184
Iteration 2/1000 | Loss: 0.00006328
Iteration 3/1000 | Loss: 0.00008301
Iteration 4/1000 | Loss: 0.00004108
Iteration 5/1000 | Loss: 0.00003412
Iteration 6/1000 | Loss: 0.00006339
Iteration 7/1000 | Loss: 0.00004085
Iteration 8/1000 | Loss: 0.00003188
Iteration 9/1000 | Loss: 0.00002972
Iteration 10/1000 | Loss: 0.00002725
Iteration 11/1000 | Loss: 0.00004038
Iteration 12/1000 | Loss: 0.00003779
Iteration 13/1000 | Loss: 0.00003698
Iteration 14/1000 | Loss: 0.00002617
Iteration 15/1000 | Loss: 0.00002473
Iteration 16/1000 | Loss: 0.00002404
Iteration 17/1000 | Loss: 0.00002350
Iteration 18/1000 | Loss: 0.00002333
Iteration 19/1000 | Loss: 0.00002326
Iteration 20/1000 | Loss: 0.00002319
Iteration 21/1000 | Loss: 0.00002318
Iteration 22/1000 | Loss: 0.00002317
Iteration 23/1000 | Loss: 0.00002316
Iteration 24/1000 | Loss: 0.00002311
Iteration 25/1000 | Loss: 0.00002304
Iteration 26/1000 | Loss: 0.00002303
Iteration 27/1000 | Loss: 0.00002302
Iteration 28/1000 | Loss: 0.00002297
Iteration 29/1000 | Loss: 0.00002296
Iteration 30/1000 | Loss: 0.00002296
Iteration 31/1000 | Loss: 0.00002295
Iteration 32/1000 | Loss: 0.00002295
Iteration 33/1000 | Loss: 0.00002295
Iteration 34/1000 | Loss: 0.00002294
Iteration 35/1000 | Loss: 0.00002294
Iteration 36/1000 | Loss: 0.00002293
Iteration 37/1000 | Loss: 0.00002293
Iteration 38/1000 | Loss: 0.00002292
Iteration 39/1000 | Loss: 0.00002291
Iteration 40/1000 | Loss: 0.00002291
Iteration 41/1000 | Loss: 0.00002291
Iteration 42/1000 | Loss: 0.00002291
Iteration 43/1000 | Loss: 0.00002291
Iteration 44/1000 | Loss: 0.00002291
Iteration 45/1000 | Loss: 0.00002290
Iteration 46/1000 | Loss: 0.00002290
Iteration 47/1000 | Loss: 0.00002290
Iteration 48/1000 | Loss: 0.00002290
Iteration 49/1000 | Loss: 0.00002290
Iteration 50/1000 | Loss: 0.00002290
Iteration 51/1000 | Loss: 0.00002290
Iteration 52/1000 | Loss: 0.00002289
Iteration 53/1000 | Loss: 0.00002289
Iteration 54/1000 | Loss: 0.00002289
Iteration 55/1000 | Loss: 0.00002289
Iteration 56/1000 | Loss: 0.00002289
Iteration 57/1000 | Loss: 0.00002289
Iteration 58/1000 | Loss: 0.00002288
Iteration 59/1000 | Loss: 0.00002288
Iteration 60/1000 | Loss: 0.00002288
Iteration 61/1000 | Loss: 0.00002288
Iteration 62/1000 | Loss: 0.00002288
Iteration 63/1000 | Loss: 0.00002288
Iteration 64/1000 | Loss: 0.00002288
Iteration 65/1000 | Loss: 0.00002288
Iteration 66/1000 | Loss: 0.00002288
Iteration 67/1000 | Loss: 0.00002288
Iteration 68/1000 | Loss: 0.00002288
Iteration 69/1000 | Loss: 0.00002288
Iteration 70/1000 | Loss: 0.00002288
Iteration 71/1000 | Loss: 0.00002288
Iteration 72/1000 | Loss: 0.00002288
Iteration 73/1000 | Loss: 0.00002288
Iteration 74/1000 | Loss: 0.00002288
Iteration 75/1000 | Loss: 0.00002288
Iteration 76/1000 | Loss: 0.00002288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [2.288037103426177e-05, 2.288037103426177e-05, 2.288037103426177e-05, 2.288037103426177e-05, 2.288037103426177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.288037103426177e-05

Optimization complete. Final v2v error: 3.8902859687805176 mm

Highest mean error: 5.856448650360107 mm for frame 94

Lowest mean error: 2.9421656131744385 mm for frame 49

Saving results

Total time: 42.79426550865173
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805114
Iteration 2/25 | Loss: 0.00130367
Iteration 3/25 | Loss: 0.00121160
Iteration 4/25 | Loss: 0.00120196
Iteration 5/25 | Loss: 0.00119970
Iteration 6/25 | Loss: 0.00119964
Iteration 7/25 | Loss: 0.00119964
Iteration 8/25 | Loss: 0.00119964
Iteration 9/25 | Loss: 0.00119964
Iteration 10/25 | Loss: 0.00119964
Iteration 11/25 | Loss: 0.00119964
Iteration 12/25 | Loss: 0.00119964
Iteration 13/25 | Loss: 0.00119964
Iteration 14/25 | Loss: 0.00119964
Iteration 15/25 | Loss: 0.00119964
Iteration 16/25 | Loss: 0.00119964
Iteration 17/25 | Loss: 0.00119964
Iteration 18/25 | Loss: 0.00119964
Iteration 19/25 | Loss: 0.00119964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001199637190438807, 0.001199637190438807, 0.001199637190438807, 0.001199637190438807, 0.001199637190438807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001199637190438807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43676960
Iteration 2/25 | Loss: 0.00071789
Iteration 3/25 | Loss: 0.00071788
Iteration 4/25 | Loss: 0.00071788
Iteration 5/25 | Loss: 0.00071788
Iteration 6/25 | Loss: 0.00071788
Iteration 7/25 | Loss: 0.00071788
Iteration 8/25 | Loss: 0.00071788
Iteration 9/25 | Loss: 0.00071788
Iteration 10/25 | Loss: 0.00071788
Iteration 11/25 | Loss: 0.00071788
Iteration 12/25 | Loss: 0.00071788
Iteration 13/25 | Loss: 0.00071788
Iteration 14/25 | Loss: 0.00071788
Iteration 15/25 | Loss: 0.00071788
Iteration 16/25 | Loss: 0.00071788
Iteration 17/25 | Loss: 0.00071788
Iteration 18/25 | Loss: 0.00071788
Iteration 19/25 | Loss: 0.00071788
Iteration 20/25 | Loss: 0.00071788
Iteration 21/25 | Loss: 0.00071788
Iteration 22/25 | Loss: 0.00071788
Iteration 23/25 | Loss: 0.00071788
Iteration 24/25 | Loss: 0.00071788
Iteration 25/25 | Loss: 0.00071788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071788
Iteration 2/1000 | Loss: 0.00002525
Iteration 3/1000 | Loss: 0.00001806
Iteration 4/1000 | Loss: 0.00001565
Iteration 5/1000 | Loss: 0.00001453
Iteration 6/1000 | Loss: 0.00001376
Iteration 7/1000 | Loss: 0.00001317
Iteration 8/1000 | Loss: 0.00001282
Iteration 9/1000 | Loss: 0.00001272
Iteration 10/1000 | Loss: 0.00001251
Iteration 11/1000 | Loss: 0.00001231
Iteration 12/1000 | Loss: 0.00001228
Iteration 13/1000 | Loss: 0.00001228
Iteration 14/1000 | Loss: 0.00001227
Iteration 15/1000 | Loss: 0.00001227
Iteration 16/1000 | Loss: 0.00001226
Iteration 17/1000 | Loss: 0.00001225
Iteration 18/1000 | Loss: 0.00001224
Iteration 19/1000 | Loss: 0.00001222
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001220
Iteration 22/1000 | Loss: 0.00001220
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001208
Iteration 25/1000 | Loss: 0.00001207
Iteration 26/1000 | Loss: 0.00001207
Iteration 27/1000 | Loss: 0.00001206
Iteration 28/1000 | Loss: 0.00001206
Iteration 29/1000 | Loss: 0.00001204
Iteration 30/1000 | Loss: 0.00001204
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001202
Iteration 33/1000 | Loss: 0.00001201
Iteration 34/1000 | Loss: 0.00001201
Iteration 35/1000 | Loss: 0.00001201
Iteration 36/1000 | Loss: 0.00001200
Iteration 37/1000 | Loss: 0.00001200
Iteration 38/1000 | Loss: 0.00001200
Iteration 39/1000 | Loss: 0.00001200
Iteration 40/1000 | Loss: 0.00001199
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001198
Iteration 43/1000 | Loss: 0.00001198
Iteration 44/1000 | Loss: 0.00001197
Iteration 45/1000 | Loss: 0.00001197
Iteration 46/1000 | Loss: 0.00001197
Iteration 47/1000 | Loss: 0.00001197
Iteration 48/1000 | Loss: 0.00001196
Iteration 49/1000 | Loss: 0.00001196
Iteration 50/1000 | Loss: 0.00001194
Iteration 51/1000 | Loss: 0.00001194
Iteration 52/1000 | Loss: 0.00001194
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001194
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001194
Iteration 59/1000 | Loss: 0.00001194
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00001193
Iteration 65/1000 | Loss: 0.00001193
Iteration 66/1000 | Loss: 0.00001193
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001190
Iteration 69/1000 | Loss: 0.00001189
Iteration 70/1000 | Loss: 0.00001188
Iteration 71/1000 | Loss: 0.00001187
Iteration 72/1000 | Loss: 0.00001187
Iteration 73/1000 | Loss: 0.00001186
Iteration 74/1000 | Loss: 0.00001186
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001184
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001184
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001183
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001181
Iteration 86/1000 | Loss: 0.00001179
Iteration 87/1000 | Loss: 0.00001179
Iteration 88/1000 | Loss: 0.00001178
Iteration 89/1000 | Loss: 0.00001178
Iteration 90/1000 | Loss: 0.00001178
Iteration 91/1000 | Loss: 0.00001178
Iteration 92/1000 | Loss: 0.00001178
Iteration 93/1000 | Loss: 0.00001178
Iteration 94/1000 | Loss: 0.00001178
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001177
Iteration 97/1000 | Loss: 0.00001177
Iteration 98/1000 | Loss: 0.00001177
Iteration 99/1000 | Loss: 0.00001177
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001176
Iteration 102/1000 | Loss: 0.00001175
Iteration 103/1000 | Loss: 0.00001175
Iteration 104/1000 | Loss: 0.00001175
Iteration 105/1000 | Loss: 0.00001175
Iteration 106/1000 | Loss: 0.00001174
Iteration 107/1000 | Loss: 0.00001174
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001173
Iteration 111/1000 | Loss: 0.00001172
Iteration 112/1000 | Loss: 0.00001171
Iteration 113/1000 | Loss: 0.00001171
Iteration 114/1000 | Loss: 0.00001171
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001169
Iteration 118/1000 | Loss: 0.00001169
Iteration 119/1000 | Loss: 0.00001169
Iteration 120/1000 | Loss: 0.00001169
Iteration 121/1000 | Loss: 0.00001168
Iteration 122/1000 | Loss: 0.00001168
Iteration 123/1000 | Loss: 0.00001168
Iteration 124/1000 | Loss: 0.00001168
Iteration 125/1000 | Loss: 0.00001167
Iteration 126/1000 | Loss: 0.00001167
Iteration 127/1000 | Loss: 0.00001167
Iteration 128/1000 | Loss: 0.00001167
Iteration 129/1000 | Loss: 0.00001167
Iteration 130/1000 | Loss: 0.00001167
Iteration 131/1000 | Loss: 0.00001166
Iteration 132/1000 | Loss: 0.00001166
Iteration 133/1000 | Loss: 0.00001166
Iteration 134/1000 | Loss: 0.00001166
Iteration 135/1000 | Loss: 0.00001165
Iteration 136/1000 | Loss: 0.00001165
Iteration 137/1000 | Loss: 0.00001165
Iteration 138/1000 | Loss: 0.00001165
Iteration 139/1000 | Loss: 0.00001165
Iteration 140/1000 | Loss: 0.00001165
Iteration 141/1000 | Loss: 0.00001164
Iteration 142/1000 | Loss: 0.00001164
Iteration 143/1000 | Loss: 0.00001164
Iteration 144/1000 | Loss: 0.00001164
Iteration 145/1000 | Loss: 0.00001164
Iteration 146/1000 | Loss: 0.00001164
Iteration 147/1000 | Loss: 0.00001163
Iteration 148/1000 | Loss: 0.00001163
Iteration 149/1000 | Loss: 0.00001163
Iteration 150/1000 | Loss: 0.00001163
Iteration 151/1000 | Loss: 0.00001162
Iteration 152/1000 | Loss: 0.00001162
Iteration 153/1000 | Loss: 0.00001162
Iteration 154/1000 | Loss: 0.00001161
Iteration 155/1000 | Loss: 0.00001161
Iteration 156/1000 | Loss: 0.00001161
Iteration 157/1000 | Loss: 0.00001161
Iteration 158/1000 | Loss: 0.00001161
Iteration 159/1000 | Loss: 0.00001161
Iteration 160/1000 | Loss: 0.00001161
Iteration 161/1000 | Loss: 0.00001161
Iteration 162/1000 | Loss: 0.00001161
Iteration 163/1000 | Loss: 0.00001161
Iteration 164/1000 | Loss: 0.00001161
Iteration 165/1000 | Loss: 0.00001160
Iteration 166/1000 | Loss: 0.00001160
Iteration 167/1000 | Loss: 0.00001160
Iteration 168/1000 | Loss: 0.00001160
Iteration 169/1000 | Loss: 0.00001160
Iteration 170/1000 | Loss: 0.00001160
Iteration 171/1000 | Loss: 0.00001160
Iteration 172/1000 | Loss: 0.00001160
Iteration 173/1000 | Loss: 0.00001160
Iteration 174/1000 | Loss: 0.00001160
Iteration 175/1000 | Loss: 0.00001160
Iteration 176/1000 | Loss: 0.00001160
Iteration 177/1000 | Loss: 0.00001160
Iteration 178/1000 | Loss: 0.00001160
Iteration 179/1000 | Loss: 0.00001159
Iteration 180/1000 | Loss: 0.00001159
Iteration 181/1000 | Loss: 0.00001159
Iteration 182/1000 | Loss: 0.00001159
Iteration 183/1000 | Loss: 0.00001159
Iteration 184/1000 | Loss: 0.00001159
Iteration 185/1000 | Loss: 0.00001159
Iteration 186/1000 | Loss: 0.00001159
Iteration 187/1000 | Loss: 0.00001159
Iteration 188/1000 | Loss: 0.00001159
Iteration 189/1000 | Loss: 0.00001158
Iteration 190/1000 | Loss: 0.00001158
Iteration 191/1000 | Loss: 0.00001158
Iteration 192/1000 | Loss: 0.00001158
Iteration 193/1000 | Loss: 0.00001158
Iteration 194/1000 | Loss: 0.00001158
Iteration 195/1000 | Loss: 0.00001158
Iteration 196/1000 | Loss: 0.00001158
Iteration 197/1000 | Loss: 0.00001158
Iteration 198/1000 | Loss: 0.00001158
Iteration 199/1000 | Loss: 0.00001158
Iteration 200/1000 | Loss: 0.00001158
Iteration 201/1000 | Loss: 0.00001158
Iteration 202/1000 | Loss: 0.00001158
Iteration 203/1000 | Loss: 0.00001158
Iteration 204/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.157882888946915e-05, 1.157882888946915e-05, 1.157882888946915e-05, 1.157882888946915e-05, 1.157882888946915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.157882888946915e-05

Optimization complete. Final v2v error: 2.8975324630737305 mm

Highest mean error: 3.1144156455993652 mm for frame 55

Lowest mean error: 2.727228879928589 mm for frame 147

Saving results

Total time: 39.516661643981934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404908
Iteration 2/25 | Loss: 0.00129257
Iteration 3/25 | Loss: 0.00121493
Iteration 4/25 | Loss: 0.00120376
Iteration 5/25 | Loss: 0.00119994
Iteration 6/25 | Loss: 0.00119928
Iteration 7/25 | Loss: 0.00119928
Iteration 8/25 | Loss: 0.00119928
Iteration 9/25 | Loss: 0.00119928
Iteration 10/25 | Loss: 0.00119928
Iteration 11/25 | Loss: 0.00119928
Iteration 12/25 | Loss: 0.00119928
Iteration 13/25 | Loss: 0.00119928
Iteration 14/25 | Loss: 0.00119928
Iteration 15/25 | Loss: 0.00119928
Iteration 16/25 | Loss: 0.00119928
Iteration 17/25 | Loss: 0.00119928
Iteration 18/25 | Loss: 0.00119928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011992808431386948, 0.0011992808431386948, 0.0011992808431386948, 0.0011992808431386948, 0.0011992808431386948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011992808431386948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94068587
Iteration 2/25 | Loss: 0.00072160
Iteration 3/25 | Loss: 0.00072160
Iteration 4/25 | Loss: 0.00072160
Iteration 5/25 | Loss: 0.00072160
Iteration 6/25 | Loss: 0.00072160
Iteration 7/25 | Loss: 0.00072160
Iteration 8/25 | Loss: 0.00072160
Iteration 9/25 | Loss: 0.00072160
Iteration 10/25 | Loss: 0.00072160
Iteration 11/25 | Loss: 0.00072160
Iteration 12/25 | Loss: 0.00072160
Iteration 13/25 | Loss: 0.00072160
Iteration 14/25 | Loss: 0.00072160
Iteration 15/25 | Loss: 0.00072160
Iteration 16/25 | Loss: 0.00072160
Iteration 17/25 | Loss: 0.00072160
Iteration 18/25 | Loss: 0.00072160
Iteration 19/25 | Loss: 0.00072160
Iteration 20/25 | Loss: 0.00072160
Iteration 21/25 | Loss: 0.00072160
Iteration 22/25 | Loss: 0.00072160
Iteration 23/25 | Loss: 0.00072160
Iteration 24/25 | Loss: 0.00072160
Iteration 25/25 | Loss: 0.00072160

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072160
Iteration 2/1000 | Loss: 0.00002909
Iteration 3/1000 | Loss: 0.00002037
Iteration 4/1000 | Loss: 0.00001757
Iteration 5/1000 | Loss: 0.00001650
Iteration 6/1000 | Loss: 0.00001578
Iteration 7/1000 | Loss: 0.00001532
Iteration 8/1000 | Loss: 0.00001490
Iteration 9/1000 | Loss: 0.00001465
Iteration 10/1000 | Loss: 0.00001452
Iteration 11/1000 | Loss: 0.00001445
Iteration 12/1000 | Loss: 0.00001441
Iteration 13/1000 | Loss: 0.00001419
Iteration 14/1000 | Loss: 0.00001416
Iteration 15/1000 | Loss: 0.00001412
Iteration 16/1000 | Loss: 0.00001408
Iteration 17/1000 | Loss: 0.00001402
Iteration 18/1000 | Loss: 0.00001396
Iteration 19/1000 | Loss: 0.00001392
Iteration 20/1000 | Loss: 0.00001388
Iteration 21/1000 | Loss: 0.00001387
Iteration 22/1000 | Loss: 0.00001387
Iteration 23/1000 | Loss: 0.00001383
Iteration 24/1000 | Loss: 0.00001381
Iteration 25/1000 | Loss: 0.00001381
Iteration 26/1000 | Loss: 0.00001379
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001375
Iteration 29/1000 | Loss: 0.00001375
Iteration 30/1000 | Loss: 0.00001375
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001371
Iteration 33/1000 | Loss: 0.00001371
Iteration 34/1000 | Loss: 0.00001371
Iteration 35/1000 | Loss: 0.00001370
Iteration 36/1000 | Loss: 0.00001370
Iteration 37/1000 | Loss: 0.00001367
Iteration 38/1000 | Loss: 0.00001366
Iteration 39/1000 | Loss: 0.00001366
Iteration 40/1000 | Loss: 0.00001366
Iteration 41/1000 | Loss: 0.00001365
Iteration 42/1000 | Loss: 0.00001365
Iteration 43/1000 | Loss: 0.00001364
Iteration 44/1000 | Loss: 0.00001364
Iteration 45/1000 | Loss: 0.00001364
Iteration 46/1000 | Loss: 0.00001363
Iteration 47/1000 | Loss: 0.00001363
Iteration 48/1000 | Loss: 0.00001362
Iteration 49/1000 | Loss: 0.00001361
Iteration 50/1000 | Loss: 0.00001361
Iteration 51/1000 | Loss: 0.00001361
Iteration 52/1000 | Loss: 0.00001361
Iteration 53/1000 | Loss: 0.00001361
Iteration 54/1000 | Loss: 0.00001361
Iteration 55/1000 | Loss: 0.00001360
Iteration 56/1000 | Loss: 0.00001360
Iteration 57/1000 | Loss: 0.00001359
Iteration 58/1000 | Loss: 0.00001358
Iteration 59/1000 | Loss: 0.00001358
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001357
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001356
Iteration 65/1000 | Loss: 0.00001355
Iteration 66/1000 | Loss: 0.00001355
Iteration 67/1000 | Loss: 0.00001355
Iteration 68/1000 | Loss: 0.00001354
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001354
Iteration 71/1000 | Loss: 0.00001354
Iteration 72/1000 | Loss: 0.00001354
Iteration 73/1000 | Loss: 0.00001353
Iteration 74/1000 | Loss: 0.00001353
Iteration 75/1000 | Loss: 0.00001353
Iteration 76/1000 | Loss: 0.00001353
Iteration 77/1000 | Loss: 0.00001353
Iteration 78/1000 | Loss: 0.00001353
Iteration 79/1000 | Loss: 0.00001353
Iteration 80/1000 | Loss: 0.00001353
Iteration 81/1000 | Loss: 0.00001353
Iteration 82/1000 | Loss: 0.00001352
Iteration 83/1000 | Loss: 0.00001352
Iteration 84/1000 | Loss: 0.00001352
Iteration 85/1000 | Loss: 0.00001352
Iteration 86/1000 | Loss: 0.00001352
Iteration 87/1000 | Loss: 0.00001351
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001350
Iteration 90/1000 | Loss: 0.00001349
Iteration 91/1000 | Loss: 0.00001349
Iteration 92/1000 | Loss: 0.00001349
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001348
Iteration 95/1000 | Loss: 0.00001348
Iteration 96/1000 | Loss: 0.00001348
Iteration 97/1000 | Loss: 0.00001348
Iteration 98/1000 | Loss: 0.00001348
Iteration 99/1000 | Loss: 0.00001347
Iteration 100/1000 | Loss: 0.00001347
Iteration 101/1000 | Loss: 0.00001347
Iteration 102/1000 | Loss: 0.00001347
Iteration 103/1000 | Loss: 0.00001347
Iteration 104/1000 | Loss: 0.00001347
Iteration 105/1000 | Loss: 0.00001347
Iteration 106/1000 | Loss: 0.00001347
Iteration 107/1000 | Loss: 0.00001347
Iteration 108/1000 | Loss: 0.00001346
Iteration 109/1000 | Loss: 0.00001346
Iteration 110/1000 | Loss: 0.00001346
Iteration 111/1000 | Loss: 0.00001346
Iteration 112/1000 | Loss: 0.00001346
Iteration 113/1000 | Loss: 0.00001346
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001345
Iteration 116/1000 | Loss: 0.00001345
Iteration 117/1000 | Loss: 0.00001344
Iteration 118/1000 | Loss: 0.00001344
Iteration 119/1000 | Loss: 0.00001344
Iteration 120/1000 | Loss: 0.00001344
Iteration 121/1000 | Loss: 0.00001343
Iteration 122/1000 | Loss: 0.00001343
Iteration 123/1000 | Loss: 0.00001342
Iteration 124/1000 | Loss: 0.00001342
Iteration 125/1000 | Loss: 0.00001342
Iteration 126/1000 | Loss: 0.00001342
Iteration 127/1000 | Loss: 0.00001341
Iteration 128/1000 | Loss: 0.00001341
Iteration 129/1000 | Loss: 0.00001341
Iteration 130/1000 | Loss: 0.00001341
Iteration 131/1000 | Loss: 0.00001341
Iteration 132/1000 | Loss: 0.00001341
Iteration 133/1000 | Loss: 0.00001341
Iteration 134/1000 | Loss: 0.00001341
Iteration 135/1000 | Loss: 0.00001341
Iteration 136/1000 | Loss: 0.00001340
Iteration 137/1000 | Loss: 0.00001340
Iteration 138/1000 | Loss: 0.00001340
Iteration 139/1000 | Loss: 0.00001340
Iteration 140/1000 | Loss: 0.00001340
Iteration 141/1000 | Loss: 0.00001340
Iteration 142/1000 | Loss: 0.00001339
Iteration 143/1000 | Loss: 0.00001339
Iteration 144/1000 | Loss: 0.00001339
Iteration 145/1000 | Loss: 0.00001339
Iteration 146/1000 | Loss: 0.00001338
Iteration 147/1000 | Loss: 0.00001338
Iteration 148/1000 | Loss: 0.00001338
Iteration 149/1000 | Loss: 0.00001338
Iteration 150/1000 | Loss: 0.00001338
Iteration 151/1000 | Loss: 0.00001338
Iteration 152/1000 | Loss: 0.00001338
Iteration 153/1000 | Loss: 0.00001338
Iteration 154/1000 | Loss: 0.00001338
Iteration 155/1000 | Loss: 0.00001338
Iteration 156/1000 | Loss: 0.00001338
Iteration 157/1000 | Loss: 0.00001338
Iteration 158/1000 | Loss: 0.00001338
Iteration 159/1000 | Loss: 0.00001338
Iteration 160/1000 | Loss: 0.00001338
Iteration 161/1000 | Loss: 0.00001338
Iteration 162/1000 | Loss: 0.00001338
Iteration 163/1000 | Loss: 0.00001338
Iteration 164/1000 | Loss: 0.00001338
Iteration 165/1000 | Loss: 0.00001338
Iteration 166/1000 | Loss: 0.00001338
Iteration 167/1000 | Loss: 0.00001338
Iteration 168/1000 | Loss: 0.00001338
Iteration 169/1000 | Loss: 0.00001338
Iteration 170/1000 | Loss: 0.00001338
Iteration 171/1000 | Loss: 0.00001338
Iteration 172/1000 | Loss: 0.00001338
Iteration 173/1000 | Loss: 0.00001338
Iteration 174/1000 | Loss: 0.00001338
Iteration 175/1000 | Loss: 0.00001338
Iteration 176/1000 | Loss: 0.00001338
Iteration 177/1000 | Loss: 0.00001338
Iteration 178/1000 | Loss: 0.00001338
Iteration 179/1000 | Loss: 0.00001338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.3383887562667951e-05, 1.3383887562667951e-05, 1.3383887562667951e-05, 1.3383887562667951e-05, 1.3383887562667951e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3383887562667951e-05

Optimization complete. Final v2v error: 3.122739553451538 mm

Highest mean error: 3.6787118911743164 mm for frame 73

Lowest mean error: 2.867297649383545 mm for frame 95

Saving results

Total time: 38.81657528877258
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00729368
Iteration 2/25 | Loss: 0.00142383
Iteration 3/25 | Loss: 0.00136267
Iteration 4/25 | Loss: 0.00135645
Iteration 5/25 | Loss: 0.00135444
Iteration 6/25 | Loss: 0.00135444
Iteration 7/25 | Loss: 0.00135444
Iteration 8/25 | Loss: 0.00135444
Iteration 9/25 | Loss: 0.00135444
Iteration 10/25 | Loss: 0.00135444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013544359244406223, 0.0013544359244406223, 0.0013544359244406223, 0.0013544359244406223, 0.0013544359244406223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013544359244406223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24461854
Iteration 2/25 | Loss: 0.00093475
Iteration 3/25 | Loss: 0.00093474
Iteration 4/25 | Loss: 0.00093474
Iteration 5/25 | Loss: 0.00093473
Iteration 6/25 | Loss: 0.00093473
Iteration 7/25 | Loss: 0.00093473
Iteration 8/25 | Loss: 0.00093473
Iteration 9/25 | Loss: 0.00093473
Iteration 10/25 | Loss: 0.00093473
Iteration 11/25 | Loss: 0.00093473
Iteration 12/25 | Loss: 0.00093473
Iteration 13/25 | Loss: 0.00093473
Iteration 14/25 | Loss: 0.00093473
Iteration 15/25 | Loss: 0.00093473
Iteration 16/25 | Loss: 0.00093473
Iteration 17/25 | Loss: 0.00093473
Iteration 18/25 | Loss: 0.00093473
Iteration 19/25 | Loss: 0.00093473
Iteration 20/25 | Loss: 0.00093473
Iteration 21/25 | Loss: 0.00093473
Iteration 22/25 | Loss: 0.00093473
Iteration 23/25 | Loss: 0.00093473
Iteration 24/25 | Loss: 0.00093473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000934732030145824, 0.000934732030145824, 0.000934732030145824, 0.000934732030145824, 0.000934732030145824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000934732030145824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093473
Iteration 2/1000 | Loss: 0.00003957
Iteration 3/1000 | Loss: 0.00002771
Iteration 4/1000 | Loss: 0.00002490
Iteration 5/1000 | Loss: 0.00002388
Iteration 6/1000 | Loss: 0.00002346
Iteration 7/1000 | Loss: 0.00002292
Iteration 8/1000 | Loss: 0.00002269
Iteration 9/1000 | Loss: 0.00002232
Iteration 10/1000 | Loss: 0.00002203
Iteration 11/1000 | Loss: 0.00002176
Iteration 12/1000 | Loss: 0.00002160
Iteration 13/1000 | Loss: 0.00002141
Iteration 14/1000 | Loss: 0.00002120
Iteration 15/1000 | Loss: 0.00002115
Iteration 16/1000 | Loss: 0.00002114
Iteration 17/1000 | Loss: 0.00002112
Iteration 18/1000 | Loss: 0.00002111
Iteration 19/1000 | Loss: 0.00002110
Iteration 20/1000 | Loss: 0.00002102
Iteration 21/1000 | Loss: 0.00002101
Iteration 22/1000 | Loss: 0.00002101
Iteration 23/1000 | Loss: 0.00002094
Iteration 24/1000 | Loss: 0.00002085
Iteration 25/1000 | Loss: 0.00002082
Iteration 26/1000 | Loss: 0.00002077
Iteration 27/1000 | Loss: 0.00002072
Iteration 28/1000 | Loss: 0.00002070
Iteration 29/1000 | Loss: 0.00002069
Iteration 30/1000 | Loss: 0.00002069
Iteration 31/1000 | Loss: 0.00002068
Iteration 32/1000 | Loss: 0.00002068
Iteration 33/1000 | Loss: 0.00002068
Iteration 34/1000 | Loss: 0.00002068
Iteration 35/1000 | Loss: 0.00002068
Iteration 36/1000 | Loss: 0.00002068
Iteration 37/1000 | Loss: 0.00002067
Iteration 38/1000 | Loss: 0.00002067
Iteration 39/1000 | Loss: 0.00002067
Iteration 40/1000 | Loss: 0.00002066
Iteration 41/1000 | Loss: 0.00002066
Iteration 42/1000 | Loss: 0.00002066
Iteration 43/1000 | Loss: 0.00002065
Iteration 44/1000 | Loss: 0.00002065
Iteration 45/1000 | Loss: 0.00002064
Iteration 46/1000 | Loss: 0.00002064
Iteration 47/1000 | Loss: 0.00002063
Iteration 48/1000 | Loss: 0.00002063
Iteration 49/1000 | Loss: 0.00002063
Iteration 50/1000 | Loss: 0.00002063
Iteration 51/1000 | Loss: 0.00002063
Iteration 52/1000 | Loss: 0.00002063
Iteration 53/1000 | Loss: 0.00002062
Iteration 54/1000 | Loss: 0.00002062
Iteration 55/1000 | Loss: 0.00002062
Iteration 56/1000 | Loss: 0.00002061
Iteration 57/1000 | Loss: 0.00002061
Iteration 58/1000 | Loss: 0.00002060
Iteration 59/1000 | Loss: 0.00002060
Iteration 60/1000 | Loss: 0.00002060
Iteration 61/1000 | Loss: 0.00002060
Iteration 62/1000 | Loss: 0.00002060
Iteration 63/1000 | Loss: 0.00002060
Iteration 64/1000 | Loss: 0.00002060
Iteration 65/1000 | Loss: 0.00002060
Iteration 66/1000 | Loss: 0.00002060
Iteration 67/1000 | Loss: 0.00002060
Iteration 68/1000 | Loss: 0.00002059
Iteration 69/1000 | Loss: 0.00002059
Iteration 70/1000 | Loss: 0.00002059
Iteration 71/1000 | Loss: 0.00002059
Iteration 72/1000 | Loss: 0.00002059
Iteration 73/1000 | Loss: 0.00002059
Iteration 74/1000 | Loss: 0.00002059
Iteration 75/1000 | Loss: 0.00002058
Iteration 76/1000 | Loss: 0.00002058
Iteration 77/1000 | Loss: 0.00002058
Iteration 78/1000 | Loss: 0.00002058
Iteration 79/1000 | Loss: 0.00002058
Iteration 80/1000 | Loss: 0.00002058
Iteration 81/1000 | Loss: 0.00002057
Iteration 82/1000 | Loss: 0.00002057
Iteration 83/1000 | Loss: 0.00002057
Iteration 84/1000 | Loss: 0.00002057
Iteration 85/1000 | Loss: 0.00002057
Iteration 86/1000 | Loss: 0.00002057
Iteration 87/1000 | Loss: 0.00002056
Iteration 88/1000 | Loss: 0.00002056
Iteration 89/1000 | Loss: 0.00002056
Iteration 90/1000 | Loss: 0.00002056
Iteration 91/1000 | Loss: 0.00002056
Iteration 92/1000 | Loss: 0.00002056
Iteration 93/1000 | Loss: 0.00002056
Iteration 94/1000 | Loss: 0.00002056
Iteration 95/1000 | Loss: 0.00002056
Iteration 96/1000 | Loss: 0.00002056
Iteration 97/1000 | Loss: 0.00002056
Iteration 98/1000 | Loss: 0.00002055
Iteration 99/1000 | Loss: 0.00002055
Iteration 100/1000 | Loss: 0.00002055
Iteration 101/1000 | Loss: 0.00002055
Iteration 102/1000 | Loss: 0.00002055
Iteration 103/1000 | Loss: 0.00002055
Iteration 104/1000 | Loss: 0.00002055
Iteration 105/1000 | Loss: 0.00002055
Iteration 106/1000 | Loss: 0.00002055
Iteration 107/1000 | Loss: 0.00002054
Iteration 108/1000 | Loss: 0.00002054
Iteration 109/1000 | Loss: 0.00002054
Iteration 110/1000 | Loss: 0.00002054
Iteration 111/1000 | Loss: 0.00002054
Iteration 112/1000 | Loss: 0.00002054
Iteration 113/1000 | Loss: 0.00002054
Iteration 114/1000 | Loss: 0.00002053
Iteration 115/1000 | Loss: 0.00002053
Iteration 116/1000 | Loss: 0.00002053
Iteration 117/1000 | Loss: 0.00002053
Iteration 118/1000 | Loss: 0.00002053
Iteration 119/1000 | Loss: 0.00002053
Iteration 120/1000 | Loss: 0.00002052
Iteration 121/1000 | Loss: 0.00002052
Iteration 122/1000 | Loss: 0.00002052
Iteration 123/1000 | Loss: 0.00002052
Iteration 124/1000 | Loss: 0.00002051
Iteration 125/1000 | Loss: 0.00002051
Iteration 126/1000 | Loss: 0.00002051
Iteration 127/1000 | Loss: 0.00002051
Iteration 128/1000 | Loss: 0.00002051
Iteration 129/1000 | Loss: 0.00002051
Iteration 130/1000 | Loss: 0.00002051
Iteration 131/1000 | Loss: 0.00002051
Iteration 132/1000 | Loss: 0.00002051
Iteration 133/1000 | Loss: 0.00002051
Iteration 134/1000 | Loss: 0.00002050
Iteration 135/1000 | Loss: 0.00002050
Iteration 136/1000 | Loss: 0.00002050
Iteration 137/1000 | Loss: 0.00002050
Iteration 138/1000 | Loss: 0.00002050
Iteration 139/1000 | Loss: 0.00002050
Iteration 140/1000 | Loss: 0.00002050
Iteration 141/1000 | Loss: 0.00002050
Iteration 142/1000 | Loss: 0.00002050
Iteration 143/1000 | Loss: 0.00002049
Iteration 144/1000 | Loss: 0.00002049
Iteration 145/1000 | Loss: 0.00002049
Iteration 146/1000 | Loss: 0.00002049
Iteration 147/1000 | Loss: 0.00002049
Iteration 148/1000 | Loss: 0.00002049
Iteration 149/1000 | Loss: 0.00002049
Iteration 150/1000 | Loss: 0.00002049
Iteration 151/1000 | Loss: 0.00002049
Iteration 152/1000 | Loss: 0.00002049
Iteration 153/1000 | Loss: 0.00002049
Iteration 154/1000 | Loss: 0.00002049
Iteration 155/1000 | Loss: 0.00002049
Iteration 156/1000 | Loss: 0.00002049
Iteration 157/1000 | Loss: 0.00002049
Iteration 158/1000 | Loss: 0.00002049
Iteration 159/1000 | Loss: 0.00002049
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.0493451302172616e-05, 2.0493451302172616e-05, 2.0493451302172616e-05, 2.0493451302172616e-05, 2.0493451302172616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0493451302172616e-05

Optimization complete. Final v2v error: 3.7617077827453613 mm

Highest mean error: 3.9199330806732178 mm for frame 10

Lowest mean error: 3.509620428085327 mm for frame 131

Saving results

Total time: 39.91187000274658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954545
Iteration 2/25 | Loss: 0.00244678
Iteration 3/25 | Loss: 0.00198269
Iteration 4/25 | Loss: 0.00163655
Iteration 5/25 | Loss: 0.00159588
Iteration 6/25 | Loss: 0.00163487
Iteration 7/25 | Loss: 0.00156733
Iteration 8/25 | Loss: 0.00151892
Iteration 9/25 | Loss: 0.00148669
Iteration 10/25 | Loss: 0.00146693
Iteration 11/25 | Loss: 0.00146700
Iteration 12/25 | Loss: 0.00145452
Iteration 13/25 | Loss: 0.00145292
Iteration 14/25 | Loss: 0.00144652
Iteration 15/25 | Loss: 0.00144452
Iteration 16/25 | Loss: 0.00144231
Iteration 17/25 | Loss: 0.00144291
Iteration 18/25 | Loss: 0.00144047
Iteration 19/25 | Loss: 0.00143910
Iteration 20/25 | Loss: 0.00143440
Iteration 21/25 | Loss: 0.00143828
Iteration 22/25 | Loss: 0.00143712
Iteration 23/25 | Loss: 0.00143742
Iteration 24/25 | Loss: 0.00143721
Iteration 25/25 | Loss: 0.00143763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.45853043
Iteration 2/25 | Loss: 0.00215373
Iteration 3/25 | Loss: 0.00215373
Iteration 4/25 | Loss: 0.00215373
Iteration 5/25 | Loss: 0.00215373
Iteration 6/25 | Loss: 0.00215373
Iteration 7/25 | Loss: 0.00215373
Iteration 8/25 | Loss: 0.00215373
Iteration 9/25 | Loss: 0.00215373
Iteration 10/25 | Loss: 0.00215373
Iteration 11/25 | Loss: 0.00215373
Iteration 12/25 | Loss: 0.00215373
Iteration 13/25 | Loss: 0.00215373
Iteration 14/25 | Loss: 0.00215373
Iteration 15/25 | Loss: 0.00215373
Iteration 16/25 | Loss: 0.00215373
Iteration 17/25 | Loss: 0.00215373
Iteration 18/25 | Loss: 0.00215373
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021537262946367264, 0.0021537262946367264, 0.0021537262946367264, 0.0021537262946367264, 0.0021537262946367264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021537262946367264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215373
Iteration 2/1000 | Loss: 0.00069212
Iteration 3/1000 | Loss: 0.00096626
Iteration 4/1000 | Loss: 0.00473209
Iteration 5/1000 | Loss: 0.00153426
Iteration 6/1000 | Loss: 0.00040291
Iteration 7/1000 | Loss: 0.00078760
Iteration 8/1000 | Loss: 0.00034304
Iteration 9/1000 | Loss: 0.00104812
Iteration 10/1000 | Loss: 0.00020486
Iteration 11/1000 | Loss: 0.00064527
Iteration 12/1000 | Loss: 0.00099215
Iteration 13/1000 | Loss: 0.00071556
Iteration 14/1000 | Loss: 0.00071010
Iteration 15/1000 | Loss: 0.00078032
Iteration 16/1000 | Loss: 0.00010906
Iteration 17/1000 | Loss: 0.00008156
Iteration 18/1000 | Loss: 0.00006971
Iteration 19/1000 | Loss: 0.00006300
Iteration 20/1000 | Loss: 0.00005612
Iteration 21/1000 | Loss: 0.00005321
Iteration 22/1000 | Loss: 0.00004928
Iteration 23/1000 | Loss: 0.00057316
Iteration 24/1000 | Loss: 0.00007408
Iteration 25/1000 | Loss: 0.00016412
Iteration 26/1000 | Loss: 0.00006065
Iteration 27/1000 | Loss: 0.00005033
Iteration 28/1000 | Loss: 0.00026826
Iteration 29/1000 | Loss: 0.00005935
Iteration 30/1000 | Loss: 0.00049460
Iteration 31/1000 | Loss: 0.00005733
Iteration 32/1000 | Loss: 0.00041890
Iteration 33/1000 | Loss: 0.00038439
Iteration 34/1000 | Loss: 0.00034039
Iteration 35/1000 | Loss: 0.00061244
Iteration 36/1000 | Loss: 0.00038780
Iteration 37/1000 | Loss: 0.00159177
Iteration 38/1000 | Loss: 0.00007158
Iteration 39/1000 | Loss: 0.00005717
Iteration 40/1000 | Loss: 0.00004882
Iteration 41/1000 | Loss: 0.00004120
Iteration 42/1000 | Loss: 0.00003605
Iteration 43/1000 | Loss: 0.00003137
Iteration 44/1000 | Loss: 0.00002848
Iteration 45/1000 | Loss: 0.00002644
Iteration 46/1000 | Loss: 0.00043365
Iteration 47/1000 | Loss: 0.00026451
Iteration 48/1000 | Loss: 0.00037338
Iteration 49/1000 | Loss: 0.00002975
Iteration 50/1000 | Loss: 0.00002597
Iteration 51/1000 | Loss: 0.00002431
Iteration 52/1000 | Loss: 0.00002327
Iteration 53/1000 | Loss: 0.00025757
Iteration 54/1000 | Loss: 0.00026472
Iteration 55/1000 | Loss: 0.00020345
Iteration 56/1000 | Loss: 0.00003111
Iteration 57/1000 | Loss: 0.00002585
Iteration 58/1000 | Loss: 0.00002381
Iteration 59/1000 | Loss: 0.00002216
Iteration 60/1000 | Loss: 0.00002080
Iteration 61/1000 | Loss: 0.00002013
Iteration 62/1000 | Loss: 0.00001967
Iteration 63/1000 | Loss: 0.00001927
Iteration 64/1000 | Loss: 0.00001907
Iteration 65/1000 | Loss: 0.00001902
Iteration 66/1000 | Loss: 0.00001893
Iteration 67/1000 | Loss: 0.00001875
Iteration 68/1000 | Loss: 0.00001856
Iteration 69/1000 | Loss: 0.00001848
Iteration 70/1000 | Loss: 0.00001848
Iteration 71/1000 | Loss: 0.00001847
Iteration 72/1000 | Loss: 0.00001847
Iteration 73/1000 | Loss: 0.00001846
Iteration 74/1000 | Loss: 0.00001846
Iteration 75/1000 | Loss: 0.00001846
Iteration 76/1000 | Loss: 0.00001845
Iteration 77/1000 | Loss: 0.00001845
Iteration 78/1000 | Loss: 0.00001843
Iteration 79/1000 | Loss: 0.00001843
Iteration 80/1000 | Loss: 0.00001843
Iteration 81/1000 | Loss: 0.00001843
Iteration 82/1000 | Loss: 0.00001843
Iteration 83/1000 | Loss: 0.00001843
Iteration 84/1000 | Loss: 0.00001843
Iteration 85/1000 | Loss: 0.00001843
Iteration 86/1000 | Loss: 0.00001843
Iteration 87/1000 | Loss: 0.00001843
Iteration 88/1000 | Loss: 0.00001843
Iteration 89/1000 | Loss: 0.00001843
Iteration 90/1000 | Loss: 0.00001842
Iteration 91/1000 | Loss: 0.00001842
Iteration 92/1000 | Loss: 0.00001842
Iteration 93/1000 | Loss: 0.00001841
Iteration 94/1000 | Loss: 0.00001841
Iteration 95/1000 | Loss: 0.00001841
Iteration 96/1000 | Loss: 0.00001841
Iteration 97/1000 | Loss: 0.00001841
Iteration 98/1000 | Loss: 0.00001840
Iteration 99/1000 | Loss: 0.00001840
Iteration 100/1000 | Loss: 0.00001840
Iteration 101/1000 | Loss: 0.00001840
Iteration 102/1000 | Loss: 0.00001840
Iteration 103/1000 | Loss: 0.00001840
Iteration 104/1000 | Loss: 0.00001839
Iteration 105/1000 | Loss: 0.00001839
Iteration 106/1000 | Loss: 0.00001839
Iteration 107/1000 | Loss: 0.00001839
Iteration 108/1000 | Loss: 0.00001838
Iteration 109/1000 | Loss: 0.00001838
Iteration 110/1000 | Loss: 0.00001838
Iteration 111/1000 | Loss: 0.00001837
Iteration 112/1000 | Loss: 0.00001837
Iteration 113/1000 | Loss: 0.00001837
Iteration 114/1000 | Loss: 0.00001837
Iteration 115/1000 | Loss: 0.00001837
Iteration 116/1000 | Loss: 0.00001836
Iteration 117/1000 | Loss: 0.00001835
Iteration 118/1000 | Loss: 0.00001835
Iteration 119/1000 | Loss: 0.00001835
Iteration 120/1000 | Loss: 0.00001834
Iteration 121/1000 | Loss: 0.00001834
Iteration 122/1000 | Loss: 0.00001834
Iteration 123/1000 | Loss: 0.00001834
Iteration 124/1000 | Loss: 0.00001833
Iteration 125/1000 | Loss: 0.00001833
Iteration 126/1000 | Loss: 0.00001833
Iteration 127/1000 | Loss: 0.00001833
Iteration 128/1000 | Loss: 0.00001833
Iteration 129/1000 | Loss: 0.00001832
Iteration 130/1000 | Loss: 0.00001832
Iteration 131/1000 | Loss: 0.00001832
Iteration 132/1000 | Loss: 0.00001832
Iteration 133/1000 | Loss: 0.00001832
Iteration 134/1000 | Loss: 0.00001831
Iteration 135/1000 | Loss: 0.00001831
Iteration 136/1000 | Loss: 0.00001831
Iteration 137/1000 | Loss: 0.00001831
Iteration 138/1000 | Loss: 0.00001831
Iteration 139/1000 | Loss: 0.00001831
Iteration 140/1000 | Loss: 0.00001831
Iteration 141/1000 | Loss: 0.00001831
Iteration 142/1000 | Loss: 0.00001830
Iteration 143/1000 | Loss: 0.00001830
Iteration 144/1000 | Loss: 0.00001830
Iteration 145/1000 | Loss: 0.00001830
Iteration 146/1000 | Loss: 0.00001830
Iteration 147/1000 | Loss: 0.00001830
Iteration 148/1000 | Loss: 0.00001830
Iteration 149/1000 | Loss: 0.00001830
Iteration 150/1000 | Loss: 0.00001830
Iteration 151/1000 | Loss: 0.00001830
Iteration 152/1000 | Loss: 0.00001829
Iteration 153/1000 | Loss: 0.00001829
Iteration 154/1000 | Loss: 0.00001829
Iteration 155/1000 | Loss: 0.00001829
Iteration 156/1000 | Loss: 0.00001829
Iteration 157/1000 | Loss: 0.00001829
Iteration 158/1000 | Loss: 0.00001829
Iteration 159/1000 | Loss: 0.00001829
Iteration 160/1000 | Loss: 0.00001829
Iteration 161/1000 | Loss: 0.00001829
Iteration 162/1000 | Loss: 0.00001829
Iteration 163/1000 | Loss: 0.00001829
Iteration 164/1000 | Loss: 0.00001829
Iteration 165/1000 | Loss: 0.00001829
Iteration 166/1000 | Loss: 0.00001829
Iteration 167/1000 | Loss: 0.00001829
Iteration 168/1000 | Loss: 0.00001828
Iteration 169/1000 | Loss: 0.00001828
Iteration 170/1000 | Loss: 0.00001828
Iteration 171/1000 | Loss: 0.00001828
Iteration 172/1000 | Loss: 0.00001828
Iteration 173/1000 | Loss: 0.00001828
Iteration 174/1000 | Loss: 0.00001828
Iteration 175/1000 | Loss: 0.00001828
Iteration 176/1000 | Loss: 0.00001828
Iteration 177/1000 | Loss: 0.00001828
Iteration 178/1000 | Loss: 0.00001828
Iteration 179/1000 | Loss: 0.00001828
Iteration 180/1000 | Loss: 0.00001828
Iteration 181/1000 | Loss: 0.00001827
Iteration 182/1000 | Loss: 0.00001827
Iteration 183/1000 | Loss: 0.00001827
Iteration 184/1000 | Loss: 0.00001827
Iteration 185/1000 | Loss: 0.00001827
Iteration 186/1000 | Loss: 0.00001827
Iteration 187/1000 | Loss: 0.00001827
Iteration 188/1000 | Loss: 0.00001827
Iteration 189/1000 | Loss: 0.00001827
Iteration 190/1000 | Loss: 0.00001827
Iteration 191/1000 | Loss: 0.00001827
Iteration 192/1000 | Loss: 0.00001827
Iteration 193/1000 | Loss: 0.00001827
Iteration 194/1000 | Loss: 0.00001826
Iteration 195/1000 | Loss: 0.00001826
Iteration 196/1000 | Loss: 0.00001826
Iteration 197/1000 | Loss: 0.00001826
Iteration 198/1000 | Loss: 0.00001826
Iteration 199/1000 | Loss: 0.00001826
Iteration 200/1000 | Loss: 0.00001826
Iteration 201/1000 | Loss: 0.00001826
Iteration 202/1000 | Loss: 0.00001826
Iteration 203/1000 | Loss: 0.00001826
Iteration 204/1000 | Loss: 0.00001826
Iteration 205/1000 | Loss: 0.00001826
Iteration 206/1000 | Loss: 0.00001826
Iteration 207/1000 | Loss: 0.00001826
Iteration 208/1000 | Loss: 0.00001826
Iteration 209/1000 | Loss: 0.00001826
Iteration 210/1000 | Loss: 0.00001826
Iteration 211/1000 | Loss: 0.00001826
Iteration 212/1000 | Loss: 0.00001826
Iteration 213/1000 | Loss: 0.00001826
Iteration 214/1000 | Loss: 0.00001826
Iteration 215/1000 | Loss: 0.00001826
Iteration 216/1000 | Loss: 0.00001826
Iteration 217/1000 | Loss: 0.00001826
Iteration 218/1000 | Loss: 0.00001826
Iteration 219/1000 | Loss: 0.00001826
Iteration 220/1000 | Loss: 0.00001826
Iteration 221/1000 | Loss: 0.00001826
Iteration 222/1000 | Loss: 0.00001826
Iteration 223/1000 | Loss: 0.00001826
Iteration 224/1000 | Loss: 0.00001826
Iteration 225/1000 | Loss: 0.00001826
Iteration 226/1000 | Loss: 0.00001826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.826303378038574e-05, 1.826303378038574e-05, 1.826303378038574e-05, 1.826303378038574e-05, 1.826303378038574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.826303378038574e-05

Optimization complete. Final v2v error: 3.489150047302246 mm

Highest mean error: 5.210071563720703 mm for frame 71

Lowest mean error: 2.8355658054351807 mm for frame 126

Saving results

Total time: 150.77765154838562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413105
Iteration 2/25 | Loss: 0.00130605
Iteration 3/25 | Loss: 0.00123212
Iteration 4/25 | Loss: 0.00122265
Iteration 5/25 | Loss: 0.00122036
Iteration 6/25 | Loss: 0.00121985
Iteration 7/25 | Loss: 0.00121980
Iteration 8/25 | Loss: 0.00121980
Iteration 9/25 | Loss: 0.00121980
Iteration 10/25 | Loss: 0.00121980
Iteration 11/25 | Loss: 0.00121980
Iteration 12/25 | Loss: 0.00121980
Iteration 13/25 | Loss: 0.00121980
Iteration 14/25 | Loss: 0.00121980
Iteration 15/25 | Loss: 0.00121980
Iteration 16/25 | Loss: 0.00121980
Iteration 17/25 | Loss: 0.00121980
Iteration 18/25 | Loss: 0.00121980
Iteration 19/25 | Loss: 0.00121980
Iteration 20/25 | Loss: 0.00121980
Iteration 21/25 | Loss: 0.00121980
Iteration 22/25 | Loss: 0.00121980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00121979764662683, 0.00121979764662683, 0.00121979764662683, 0.00121979764662683, 0.00121979764662683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00121979764662683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51850343
Iteration 2/25 | Loss: 0.00083698
Iteration 3/25 | Loss: 0.00083697
Iteration 4/25 | Loss: 0.00083697
Iteration 5/25 | Loss: 0.00083697
Iteration 6/25 | Loss: 0.00083697
Iteration 7/25 | Loss: 0.00083697
Iteration 8/25 | Loss: 0.00083697
Iteration 9/25 | Loss: 0.00083697
Iteration 10/25 | Loss: 0.00083697
Iteration 11/25 | Loss: 0.00083697
Iteration 12/25 | Loss: 0.00083697
Iteration 13/25 | Loss: 0.00083697
Iteration 14/25 | Loss: 0.00083697
Iteration 15/25 | Loss: 0.00083697
Iteration 16/25 | Loss: 0.00083697
Iteration 17/25 | Loss: 0.00083697
Iteration 18/25 | Loss: 0.00083697
Iteration 19/25 | Loss: 0.00083697
Iteration 20/25 | Loss: 0.00083697
Iteration 21/25 | Loss: 0.00083697
Iteration 22/25 | Loss: 0.00083697
Iteration 23/25 | Loss: 0.00083697
Iteration 24/25 | Loss: 0.00083697
Iteration 25/25 | Loss: 0.00083697
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000836971215903759, 0.000836971215903759, 0.000836971215903759, 0.000836971215903759, 0.000836971215903759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000836971215903759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083697
Iteration 2/1000 | Loss: 0.00002714
Iteration 3/1000 | Loss: 0.00001727
Iteration 4/1000 | Loss: 0.00001503
Iteration 5/1000 | Loss: 0.00001407
Iteration 6/1000 | Loss: 0.00001350
Iteration 7/1000 | Loss: 0.00001306
Iteration 8/1000 | Loss: 0.00001278
Iteration 9/1000 | Loss: 0.00001262
Iteration 10/1000 | Loss: 0.00001247
Iteration 11/1000 | Loss: 0.00001246
Iteration 12/1000 | Loss: 0.00001231
Iteration 13/1000 | Loss: 0.00001227
Iteration 14/1000 | Loss: 0.00001217
Iteration 15/1000 | Loss: 0.00001214
Iteration 16/1000 | Loss: 0.00001211
Iteration 17/1000 | Loss: 0.00001209
Iteration 18/1000 | Loss: 0.00001208
Iteration 19/1000 | Loss: 0.00001202
Iteration 20/1000 | Loss: 0.00001201
Iteration 21/1000 | Loss: 0.00001199
Iteration 22/1000 | Loss: 0.00001194
Iteration 23/1000 | Loss: 0.00001194
Iteration 24/1000 | Loss: 0.00001194
Iteration 25/1000 | Loss: 0.00001192
Iteration 26/1000 | Loss: 0.00001191
Iteration 27/1000 | Loss: 0.00001191
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001191
Iteration 30/1000 | Loss: 0.00001191
Iteration 31/1000 | Loss: 0.00001191
Iteration 32/1000 | Loss: 0.00001190
Iteration 33/1000 | Loss: 0.00001190
Iteration 34/1000 | Loss: 0.00001187
Iteration 35/1000 | Loss: 0.00001187
Iteration 36/1000 | Loss: 0.00001187
Iteration 37/1000 | Loss: 0.00001187
Iteration 38/1000 | Loss: 0.00001187
Iteration 39/1000 | Loss: 0.00001187
Iteration 40/1000 | Loss: 0.00001186
Iteration 41/1000 | Loss: 0.00001185
Iteration 42/1000 | Loss: 0.00001185
Iteration 43/1000 | Loss: 0.00001185
Iteration 44/1000 | Loss: 0.00001184
Iteration 45/1000 | Loss: 0.00001184
Iteration 46/1000 | Loss: 0.00001184
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001183
Iteration 49/1000 | Loss: 0.00001183
Iteration 50/1000 | Loss: 0.00001183
Iteration 51/1000 | Loss: 0.00001183
Iteration 52/1000 | Loss: 0.00001182
Iteration 53/1000 | Loss: 0.00001182
Iteration 54/1000 | Loss: 0.00001182
Iteration 55/1000 | Loss: 0.00001182
Iteration 56/1000 | Loss: 0.00001181
Iteration 57/1000 | Loss: 0.00001181
Iteration 58/1000 | Loss: 0.00001180
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001179
Iteration 62/1000 | Loss: 0.00001179
Iteration 63/1000 | Loss: 0.00001179
Iteration 64/1000 | Loss: 0.00001179
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001178
Iteration 67/1000 | Loss: 0.00001178
Iteration 68/1000 | Loss: 0.00001178
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001177
Iteration 72/1000 | Loss: 0.00001177
Iteration 73/1000 | Loss: 0.00001177
Iteration 74/1000 | Loss: 0.00001177
Iteration 75/1000 | Loss: 0.00001177
Iteration 76/1000 | Loss: 0.00001177
Iteration 77/1000 | Loss: 0.00001177
Iteration 78/1000 | Loss: 0.00001176
Iteration 79/1000 | Loss: 0.00001176
Iteration 80/1000 | Loss: 0.00001176
Iteration 81/1000 | Loss: 0.00001176
Iteration 82/1000 | Loss: 0.00001175
Iteration 83/1000 | Loss: 0.00001175
Iteration 84/1000 | Loss: 0.00001175
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001175
Iteration 88/1000 | Loss: 0.00001175
Iteration 89/1000 | Loss: 0.00001175
Iteration 90/1000 | Loss: 0.00001175
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001174
Iteration 93/1000 | Loss: 0.00001174
Iteration 94/1000 | Loss: 0.00001174
Iteration 95/1000 | Loss: 0.00001174
Iteration 96/1000 | Loss: 0.00001174
Iteration 97/1000 | Loss: 0.00001174
Iteration 98/1000 | Loss: 0.00001174
Iteration 99/1000 | Loss: 0.00001174
Iteration 100/1000 | Loss: 0.00001174
Iteration 101/1000 | Loss: 0.00001174
Iteration 102/1000 | Loss: 0.00001173
Iteration 103/1000 | Loss: 0.00001173
Iteration 104/1000 | Loss: 0.00001173
Iteration 105/1000 | Loss: 0.00001173
Iteration 106/1000 | Loss: 0.00001173
Iteration 107/1000 | Loss: 0.00001173
Iteration 108/1000 | Loss: 0.00001173
Iteration 109/1000 | Loss: 0.00001173
Iteration 110/1000 | Loss: 0.00001173
Iteration 111/1000 | Loss: 0.00001173
Iteration 112/1000 | Loss: 0.00001173
Iteration 113/1000 | Loss: 0.00001172
Iteration 114/1000 | Loss: 0.00001172
Iteration 115/1000 | Loss: 0.00001172
Iteration 116/1000 | Loss: 0.00001172
Iteration 117/1000 | Loss: 0.00001172
Iteration 118/1000 | Loss: 0.00001171
Iteration 119/1000 | Loss: 0.00001171
Iteration 120/1000 | Loss: 0.00001171
Iteration 121/1000 | Loss: 0.00001171
Iteration 122/1000 | Loss: 0.00001171
Iteration 123/1000 | Loss: 0.00001171
Iteration 124/1000 | Loss: 0.00001171
Iteration 125/1000 | Loss: 0.00001171
Iteration 126/1000 | Loss: 0.00001170
Iteration 127/1000 | Loss: 0.00001170
Iteration 128/1000 | Loss: 0.00001170
Iteration 129/1000 | Loss: 0.00001169
Iteration 130/1000 | Loss: 0.00001169
Iteration 131/1000 | Loss: 0.00001169
Iteration 132/1000 | Loss: 0.00001169
Iteration 133/1000 | Loss: 0.00001169
Iteration 134/1000 | Loss: 0.00001169
Iteration 135/1000 | Loss: 0.00001168
Iteration 136/1000 | Loss: 0.00001168
Iteration 137/1000 | Loss: 0.00001168
Iteration 138/1000 | Loss: 0.00001168
Iteration 139/1000 | Loss: 0.00001168
Iteration 140/1000 | Loss: 0.00001168
Iteration 141/1000 | Loss: 0.00001168
Iteration 142/1000 | Loss: 0.00001168
Iteration 143/1000 | Loss: 0.00001168
Iteration 144/1000 | Loss: 0.00001167
Iteration 145/1000 | Loss: 0.00001167
Iteration 146/1000 | Loss: 0.00001167
Iteration 147/1000 | Loss: 0.00001167
Iteration 148/1000 | Loss: 0.00001167
Iteration 149/1000 | Loss: 0.00001167
Iteration 150/1000 | Loss: 0.00001166
Iteration 151/1000 | Loss: 0.00001166
Iteration 152/1000 | Loss: 0.00001166
Iteration 153/1000 | Loss: 0.00001166
Iteration 154/1000 | Loss: 0.00001166
Iteration 155/1000 | Loss: 0.00001166
Iteration 156/1000 | Loss: 0.00001166
Iteration 157/1000 | Loss: 0.00001166
Iteration 158/1000 | Loss: 0.00001166
Iteration 159/1000 | Loss: 0.00001166
Iteration 160/1000 | Loss: 0.00001166
Iteration 161/1000 | Loss: 0.00001166
Iteration 162/1000 | Loss: 0.00001166
Iteration 163/1000 | Loss: 0.00001166
Iteration 164/1000 | Loss: 0.00001165
Iteration 165/1000 | Loss: 0.00001165
Iteration 166/1000 | Loss: 0.00001165
Iteration 167/1000 | Loss: 0.00001165
Iteration 168/1000 | Loss: 0.00001165
Iteration 169/1000 | Loss: 0.00001165
Iteration 170/1000 | Loss: 0.00001165
Iteration 171/1000 | Loss: 0.00001165
Iteration 172/1000 | Loss: 0.00001165
Iteration 173/1000 | Loss: 0.00001165
Iteration 174/1000 | Loss: 0.00001165
Iteration 175/1000 | Loss: 0.00001165
Iteration 176/1000 | Loss: 0.00001165
Iteration 177/1000 | Loss: 0.00001165
Iteration 178/1000 | Loss: 0.00001165
Iteration 179/1000 | Loss: 0.00001165
Iteration 180/1000 | Loss: 0.00001165
Iteration 181/1000 | Loss: 0.00001165
Iteration 182/1000 | Loss: 0.00001165
Iteration 183/1000 | Loss: 0.00001165
Iteration 184/1000 | Loss: 0.00001165
Iteration 185/1000 | Loss: 0.00001165
Iteration 186/1000 | Loss: 0.00001165
Iteration 187/1000 | Loss: 0.00001165
Iteration 188/1000 | Loss: 0.00001165
Iteration 189/1000 | Loss: 0.00001165
Iteration 190/1000 | Loss: 0.00001165
Iteration 191/1000 | Loss: 0.00001165
Iteration 192/1000 | Loss: 0.00001165
Iteration 193/1000 | Loss: 0.00001165
Iteration 194/1000 | Loss: 0.00001165
Iteration 195/1000 | Loss: 0.00001165
Iteration 196/1000 | Loss: 0.00001165
Iteration 197/1000 | Loss: 0.00001165
Iteration 198/1000 | Loss: 0.00001165
Iteration 199/1000 | Loss: 0.00001165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.1646914572338574e-05, 1.1646914572338574e-05, 1.1646914572338574e-05, 1.1646914572338574e-05, 1.1646914572338574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1646914572338574e-05

Optimization complete. Final v2v error: 2.8778228759765625 mm

Highest mean error: 3.795966625213623 mm for frame 74

Lowest mean error: 2.65818452835083 mm for frame 121

Saving results

Total time: 39.69101309776306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464637
Iteration 2/25 | Loss: 0.00128069
Iteration 3/25 | Loss: 0.00123126
Iteration 4/25 | Loss: 0.00122473
Iteration 5/25 | Loss: 0.00122297
Iteration 6/25 | Loss: 0.00122297
Iteration 7/25 | Loss: 0.00122297
Iteration 8/25 | Loss: 0.00122297
Iteration 9/25 | Loss: 0.00122297
Iteration 10/25 | Loss: 0.00122297
Iteration 11/25 | Loss: 0.00122297
Iteration 12/25 | Loss: 0.00122297
Iteration 13/25 | Loss: 0.00122297
Iteration 14/25 | Loss: 0.00122297
Iteration 15/25 | Loss: 0.00122297
Iteration 16/25 | Loss: 0.00122297
Iteration 17/25 | Loss: 0.00122297
Iteration 18/25 | Loss: 0.00122297
Iteration 19/25 | Loss: 0.00122297
Iteration 20/25 | Loss: 0.00122297
Iteration 21/25 | Loss: 0.00122297
Iteration 22/25 | Loss: 0.00122297
Iteration 23/25 | Loss: 0.00122297
Iteration 24/25 | Loss: 0.00122297
Iteration 25/25 | Loss: 0.00122297

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45435083
Iteration 2/25 | Loss: 0.00080459
Iteration 3/25 | Loss: 0.00080459
Iteration 4/25 | Loss: 0.00080459
Iteration 5/25 | Loss: 0.00080459
Iteration 6/25 | Loss: 0.00080459
Iteration 7/25 | Loss: 0.00080459
Iteration 8/25 | Loss: 0.00080459
Iteration 9/25 | Loss: 0.00080459
Iteration 10/25 | Loss: 0.00080459
Iteration 11/25 | Loss: 0.00080459
Iteration 12/25 | Loss: 0.00080459
Iteration 13/25 | Loss: 0.00080459
Iteration 14/25 | Loss: 0.00080459
Iteration 15/25 | Loss: 0.00080459
Iteration 16/25 | Loss: 0.00080459
Iteration 17/25 | Loss: 0.00080459
Iteration 18/25 | Loss: 0.00080459
Iteration 19/25 | Loss: 0.00080459
Iteration 20/25 | Loss: 0.00080459
Iteration 21/25 | Loss: 0.00080459
Iteration 22/25 | Loss: 0.00080459
Iteration 23/25 | Loss: 0.00080459
Iteration 24/25 | Loss: 0.00080459
Iteration 25/25 | Loss: 0.00080459

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080459
Iteration 2/1000 | Loss: 0.00002728
Iteration 3/1000 | Loss: 0.00002053
Iteration 4/1000 | Loss: 0.00001872
Iteration 5/1000 | Loss: 0.00001767
Iteration 6/1000 | Loss: 0.00001706
Iteration 7/1000 | Loss: 0.00001675
Iteration 8/1000 | Loss: 0.00001645
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001596
Iteration 11/1000 | Loss: 0.00001595
Iteration 12/1000 | Loss: 0.00001589
Iteration 13/1000 | Loss: 0.00001584
Iteration 14/1000 | Loss: 0.00001583
Iteration 15/1000 | Loss: 0.00001567
Iteration 16/1000 | Loss: 0.00001563
Iteration 17/1000 | Loss: 0.00001558
Iteration 18/1000 | Loss: 0.00001548
Iteration 19/1000 | Loss: 0.00001535
Iteration 20/1000 | Loss: 0.00001527
Iteration 21/1000 | Loss: 0.00001527
Iteration 22/1000 | Loss: 0.00001527
Iteration 23/1000 | Loss: 0.00001527
Iteration 24/1000 | Loss: 0.00001527
Iteration 25/1000 | Loss: 0.00001527
Iteration 26/1000 | Loss: 0.00001527
Iteration 27/1000 | Loss: 0.00001527
Iteration 28/1000 | Loss: 0.00001527
Iteration 29/1000 | Loss: 0.00001527
Iteration 30/1000 | Loss: 0.00001527
Iteration 31/1000 | Loss: 0.00001527
Iteration 32/1000 | Loss: 0.00001527
Iteration 33/1000 | Loss: 0.00001527
Iteration 34/1000 | Loss: 0.00001527
Iteration 35/1000 | Loss: 0.00001527
Iteration 36/1000 | Loss: 0.00001527
Iteration 37/1000 | Loss: 0.00001526
Iteration 38/1000 | Loss: 0.00001526
Iteration 39/1000 | Loss: 0.00001524
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001520
Iteration 42/1000 | Loss: 0.00001519
Iteration 43/1000 | Loss: 0.00001518
Iteration 44/1000 | Loss: 0.00001517
Iteration 45/1000 | Loss: 0.00001515
Iteration 46/1000 | Loss: 0.00001515
Iteration 47/1000 | Loss: 0.00001510
Iteration 48/1000 | Loss: 0.00001509
Iteration 49/1000 | Loss: 0.00001508
Iteration 50/1000 | Loss: 0.00001504
Iteration 51/1000 | Loss: 0.00001503
Iteration 52/1000 | Loss: 0.00001503
Iteration 53/1000 | Loss: 0.00001503
Iteration 54/1000 | Loss: 0.00001503
Iteration 55/1000 | Loss: 0.00001503
Iteration 56/1000 | Loss: 0.00001499
Iteration 57/1000 | Loss: 0.00001499
Iteration 58/1000 | Loss: 0.00001499
Iteration 59/1000 | Loss: 0.00001499
Iteration 60/1000 | Loss: 0.00001497
Iteration 61/1000 | Loss: 0.00001497
Iteration 62/1000 | Loss: 0.00001496
Iteration 63/1000 | Loss: 0.00001495
Iteration 64/1000 | Loss: 0.00001495
Iteration 65/1000 | Loss: 0.00001494
Iteration 66/1000 | Loss: 0.00001494
Iteration 67/1000 | Loss: 0.00001490
Iteration 68/1000 | Loss: 0.00001490
Iteration 69/1000 | Loss: 0.00001488
Iteration 70/1000 | Loss: 0.00001487
Iteration 71/1000 | Loss: 0.00001487
Iteration 72/1000 | Loss: 0.00001487
Iteration 73/1000 | Loss: 0.00001487
Iteration 74/1000 | Loss: 0.00001487
Iteration 75/1000 | Loss: 0.00001487
Iteration 76/1000 | Loss: 0.00001487
Iteration 77/1000 | Loss: 0.00001487
Iteration 78/1000 | Loss: 0.00001487
Iteration 79/1000 | Loss: 0.00001487
Iteration 80/1000 | Loss: 0.00001487
Iteration 81/1000 | Loss: 0.00001486
Iteration 82/1000 | Loss: 0.00001486
Iteration 83/1000 | Loss: 0.00001486
Iteration 84/1000 | Loss: 0.00001486
Iteration 85/1000 | Loss: 0.00001485
Iteration 86/1000 | Loss: 0.00001485
Iteration 87/1000 | Loss: 0.00001484
Iteration 88/1000 | Loss: 0.00001484
Iteration 89/1000 | Loss: 0.00001484
Iteration 90/1000 | Loss: 0.00001484
Iteration 91/1000 | Loss: 0.00001484
Iteration 92/1000 | Loss: 0.00001484
Iteration 93/1000 | Loss: 0.00001483
Iteration 94/1000 | Loss: 0.00001483
Iteration 95/1000 | Loss: 0.00001483
Iteration 96/1000 | Loss: 0.00001483
Iteration 97/1000 | Loss: 0.00001483
Iteration 98/1000 | Loss: 0.00001482
Iteration 99/1000 | Loss: 0.00001482
Iteration 100/1000 | Loss: 0.00001482
Iteration 101/1000 | Loss: 0.00001482
Iteration 102/1000 | Loss: 0.00001482
Iteration 103/1000 | Loss: 0.00001481
Iteration 104/1000 | Loss: 0.00001481
Iteration 105/1000 | Loss: 0.00001481
Iteration 106/1000 | Loss: 0.00001481
Iteration 107/1000 | Loss: 0.00001481
Iteration 108/1000 | Loss: 0.00001480
Iteration 109/1000 | Loss: 0.00001480
Iteration 110/1000 | Loss: 0.00001480
Iteration 111/1000 | Loss: 0.00001480
Iteration 112/1000 | Loss: 0.00001480
Iteration 113/1000 | Loss: 0.00001480
Iteration 114/1000 | Loss: 0.00001480
Iteration 115/1000 | Loss: 0.00001480
Iteration 116/1000 | Loss: 0.00001480
Iteration 117/1000 | Loss: 0.00001480
Iteration 118/1000 | Loss: 0.00001480
Iteration 119/1000 | Loss: 0.00001480
Iteration 120/1000 | Loss: 0.00001480
Iteration 121/1000 | Loss: 0.00001480
Iteration 122/1000 | Loss: 0.00001480
Iteration 123/1000 | Loss: 0.00001480
Iteration 124/1000 | Loss: 0.00001480
Iteration 125/1000 | Loss: 0.00001480
Iteration 126/1000 | Loss: 0.00001480
Iteration 127/1000 | Loss: 0.00001480
Iteration 128/1000 | Loss: 0.00001480
Iteration 129/1000 | Loss: 0.00001480
Iteration 130/1000 | Loss: 0.00001480
Iteration 131/1000 | Loss: 0.00001480
Iteration 132/1000 | Loss: 0.00001480
Iteration 133/1000 | Loss: 0.00001480
Iteration 134/1000 | Loss: 0.00001480
Iteration 135/1000 | Loss: 0.00001480
Iteration 136/1000 | Loss: 0.00001480
Iteration 137/1000 | Loss: 0.00001480
Iteration 138/1000 | Loss: 0.00001480
Iteration 139/1000 | Loss: 0.00001480
Iteration 140/1000 | Loss: 0.00001480
Iteration 141/1000 | Loss: 0.00001480
Iteration 142/1000 | Loss: 0.00001480
Iteration 143/1000 | Loss: 0.00001480
Iteration 144/1000 | Loss: 0.00001480
Iteration 145/1000 | Loss: 0.00001480
Iteration 146/1000 | Loss: 0.00001480
Iteration 147/1000 | Loss: 0.00001480
Iteration 148/1000 | Loss: 0.00001480
Iteration 149/1000 | Loss: 0.00001480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.4797890798945446e-05, 1.4797890798945446e-05, 1.4797890798945446e-05, 1.4797890798945446e-05, 1.4797890798945446e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4797890798945446e-05

Optimization complete. Final v2v error: 3.2703030109405518 mm

Highest mean error: 3.479458808898926 mm for frame 49

Lowest mean error: 2.922097682952881 mm for frame 7

Saving results

Total time: 44.201887130737305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00576059
Iteration 2/25 | Loss: 0.00150165
Iteration 3/25 | Loss: 0.00134231
Iteration 4/25 | Loss: 0.00132703
Iteration 5/25 | Loss: 0.00132327
Iteration 6/25 | Loss: 0.00132266
Iteration 7/25 | Loss: 0.00132266
Iteration 8/25 | Loss: 0.00132266
Iteration 9/25 | Loss: 0.00132266
Iteration 10/25 | Loss: 0.00132266
Iteration 11/25 | Loss: 0.00132266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013226561713963747, 0.0013226561713963747, 0.0013226561713963747, 0.0013226561713963747, 0.0013226561713963747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013226561713963747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65523112
Iteration 2/25 | Loss: 0.00080314
Iteration 3/25 | Loss: 0.00080312
Iteration 4/25 | Loss: 0.00080312
Iteration 5/25 | Loss: 0.00080311
Iteration 6/25 | Loss: 0.00080311
Iteration 7/25 | Loss: 0.00080311
Iteration 8/25 | Loss: 0.00080311
Iteration 9/25 | Loss: 0.00080311
Iteration 10/25 | Loss: 0.00080311
Iteration 11/25 | Loss: 0.00080311
Iteration 12/25 | Loss: 0.00080311
Iteration 13/25 | Loss: 0.00080311
Iteration 14/25 | Loss: 0.00080311
Iteration 15/25 | Loss: 0.00080311
Iteration 16/25 | Loss: 0.00080311
Iteration 17/25 | Loss: 0.00080311
Iteration 18/25 | Loss: 0.00080311
Iteration 19/25 | Loss: 0.00080311
Iteration 20/25 | Loss: 0.00080311
Iteration 21/25 | Loss: 0.00080311
Iteration 22/25 | Loss: 0.00080311
Iteration 23/25 | Loss: 0.00080311
Iteration 24/25 | Loss: 0.00080311
Iteration 25/25 | Loss: 0.00080311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080311
Iteration 2/1000 | Loss: 0.00004360
Iteration 3/1000 | Loss: 0.00003322
Iteration 4/1000 | Loss: 0.00002902
Iteration 5/1000 | Loss: 0.00002711
Iteration 6/1000 | Loss: 0.00002598
Iteration 7/1000 | Loss: 0.00002503
Iteration 8/1000 | Loss: 0.00002445
Iteration 9/1000 | Loss: 0.00002403
Iteration 10/1000 | Loss: 0.00002367
Iteration 11/1000 | Loss: 0.00002334
Iteration 12/1000 | Loss: 0.00002304
Iteration 13/1000 | Loss: 0.00002278
Iteration 14/1000 | Loss: 0.00002256
Iteration 15/1000 | Loss: 0.00002241
Iteration 16/1000 | Loss: 0.00002239
Iteration 17/1000 | Loss: 0.00002238
Iteration 18/1000 | Loss: 0.00002236
Iteration 19/1000 | Loss: 0.00002231
Iteration 20/1000 | Loss: 0.00002225
Iteration 21/1000 | Loss: 0.00002218
Iteration 22/1000 | Loss: 0.00002217
Iteration 23/1000 | Loss: 0.00002213
Iteration 24/1000 | Loss: 0.00002213
Iteration 25/1000 | Loss: 0.00002211
Iteration 26/1000 | Loss: 0.00002210
Iteration 27/1000 | Loss: 0.00002209
Iteration 28/1000 | Loss: 0.00002209
Iteration 29/1000 | Loss: 0.00002201
Iteration 30/1000 | Loss: 0.00002201
Iteration 31/1000 | Loss: 0.00002199
Iteration 32/1000 | Loss: 0.00002199
Iteration 33/1000 | Loss: 0.00002199
Iteration 34/1000 | Loss: 0.00002198
Iteration 35/1000 | Loss: 0.00002198
Iteration 36/1000 | Loss: 0.00002198
Iteration 37/1000 | Loss: 0.00002198
Iteration 38/1000 | Loss: 0.00002198
Iteration 39/1000 | Loss: 0.00002198
Iteration 40/1000 | Loss: 0.00002197
Iteration 41/1000 | Loss: 0.00002197
Iteration 42/1000 | Loss: 0.00002197
Iteration 43/1000 | Loss: 0.00002197
Iteration 44/1000 | Loss: 0.00002197
Iteration 45/1000 | Loss: 0.00002197
Iteration 46/1000 | Loss: 0.00002197
Iteration 47/1000 | Loss: 0.00002196
Iteration 48/1000 | Loss: 0.00002196
Iteration 49/1000 | Loss: 0.00002196
Iteration 50/1000 | Loss: 0.00002196
Iteration 51/1000 | Loss: 0.00002196
Iteration 52/1000 | Loss: 0.00002196
Iteration 53/1000 | Loss: 0.00002195
Iteration 54/1000 | Loss: 0.00002195
Iteration 55/1000 | Loss: 0.00002195
Iteration 56/1000 | Loss: 0.00002195
Iteration 57/1000 | Loss: 0.00002195
Iteration 58/1000 | Loss: 0.00002195
Iteration 59/1000 | Loss: 0.00002195
Iteration 60/1000 | Loss: 0.00002195
Iteration 61/1000 | Loss: 0.00002195
Iteration 62/1000 | Loss: 0.00002194
Iteration 63/1000 | Loss: 0.00002194
Iteration 64/1000 | Loss: 0.00002194
Iteration 65/1000 | Loss: 0.00002194
Iteration 66/1000 | Loss: 0.00002194
Iteration 67/1000 | Loss: 0.00002194
Iteration 68/1000 | Loss: 0.00002194
Iteration 69/1000 | Loss: 0.00002193
Iteration 70/1000 | Loss: 0.00002193
Iteration 71/1000 | Loss: 0.00002193
Iteration 72/1000 | Loss: 0.00002193
Iteration 73/1000 | Loss: 0.00002193
Iteration 74/1000 | Loss: 0.00002193
Iteration 75/1000 | Loss: 0.00002193
Iteration 76/1000 | Loss: 0.00002193
Iteration 77/1000 | Loss: 0.00002193
Iteration 78/1000 | Loss: 0.00002193
Iteration 79/1000 | Loss: 0.00002193
Iteration 80/1000 | Loss: 0.00002193
Iteration 81/1000 | Loss: 0.00002193
Iteration 82/1000 | Loss: 0.00002192
Iteration 83/1000 | Loss: 0.00002192
Iteration 84/1000 | Loss: 0.00002192
Iteration 85/1000 | Loss: 0.00002192
Iteration 86/1000 | Loss: 0.00002192
Iteration 87/1000 | Loss: 0.00002192
Iteration 88/1000 | Loss: 0.00002192
Iteration 89/1000 | Loss: 0.00002192
Iteration 90/1000 | Loss: 0.00002192
Iteration 91/1000 | Loss: 0.00002191
Iteration 92/1000 | Loss: 0.00002191
Iteration 93/1000 | Loss: 0.00002191
Iteration 94/1000 | Loss: 0.00002191
Iteration 95/1000 | Loss: 0.00002191
Iteration 96/1000 | Loss: 0.00002191
Iteration 97/1000 | Loss: 0.00002190
Iteration 98/1000 | Loss: 0.00002190
Iteration 99/1000 | Loss: 0.00002190
Iteration 100/1000 | Loss: 0.00002190
Iteration 101/1000 | Loss: 0.00002190
Iteration 102/1000 | Loss: 0.00002190
Iteration 103/1000 | Loss: 0.00002189
Iteration 104/1000 | Loss: 0.00002189
Iteration 105/1000 | Loss: 0.00002189
Iteration 106/1000 | Loss: 0.00002189
Iteration 107/1000 | Loss: 0.00002188
Iteration 108/1000 | Loss: 0.00002188
Iteration 109/1000 | Loss: 0.00002188
Iteration 110/1000 | Loss: 0.00002188
Iteration 111/1000 | Loss: 0.00002188
Iteration 112/1000 | Loss: 0.00002188
Iteration 113/1000 | Loss: 0.00002187
Iteration 114/1000 | Loss: 0.00002187
Iteration 115/1000 | Loss: 0.00002187
Iteration 116/1000 | Loss: 0.00002187
Iteration 117/1000 | Loss: 0.00002187
Iteration 118/1000 | Loss: 0.00002187
Iteration 119/1000 | Loss: 0.00002187
Iteration 120/1000 | Loss: 0.00002187
Iteration 121/1000 | Loss: 0.00002187
Iteration 122/1000 | Loss: 0.00002186
Iteration 123/1000 | Loss: 0.00002186
Iteration 124/1000 | Loss: 0.00002186
Iteration 125/1000 | Loss: 0.00002186
Iteration 126/1000 | Loss: 0.00002186
Iteration 127/1000 | Loss: 0.00002186
Iteration 128/1000 | Loss: 0.00002186
Iteration 129/1000 | Loss: 0.00002186
Iteration 130/1000 | Loss: 0.00002186
Iteration 131/1000 | Loss: 0.00002186
Iteration 132/1000 | Loss: 0.00002186
Iteration 133/1000 | Loss: 0.00002186
Iteration 134/1000 | Loss: 0.00002185
Iteration 135/1000 | Loss: 0.00002185
Iteration 136/1000 | Loss: 0.00002185
Iteration 137/1000 | Loss: 0.00002185
Iteration 138/1000 | Loss: 0.00002185
Iteration 139/1000 | Loss: 0.00002185
Iteration 140/1000 | Loss: 0.00002185
Iteration 141/1000 | Loss: 0.00002184
Iteration 142/1000 | Loss: 0.00002184
Iteration 143/1000 | Loss: 0.00002184
Iteration 144/1000 | Loss: 0.00002184
Iteration 145/1000 | Loss: 0.00002184
Iteration 146/1000 | Loss: 0.00002183
Iteration 147/1000 | Loss: 0.00002183
Iteration 148/1000 | Loss: 0.00002183
Iteration 149/1000 | Loss: 0.00002183
Iteration 150/1000 | Loss: 0.00002183
Iteration 151/1000 | Loss: 0.00002183
Iteration 152/1000 | Loss: 0.00002183
Iteration 153/1000 | Loss: 0.00002183
Iteration 154/1000 | Loss: 0.00002183
Iteration 155/1000 | Loss: 0.00002183
Iteration 156/1000 | Loss: 0.00002183
Iteration 157/1000 | Loss: 0.00002183
Iteration 158/1000 | Loss: 0.00002183
Iteration 159/1000 | Loss: 0.00002182
Iteration 160/1000 | Loss: 0.00002182
Iteration 161/1000 | Loss: 0.00002182
Iteration 162/1000 | Loss: 0.00002182
Iteration 163/1000 | Loss: 0.00002182
Iteration 164/1000 | Loss: 0.00002182
Iteration 165/1000 | Loss: 0.00002182
Iteration 166/1000 | Loss: 0.00002182
Iteration 167/1000 | Loss: 0.00002182
Iteration 168/1000 | Loss: 0.00002182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.182276512030512e-05, 2.182276512030512e-05, 2.182276512030512e-05, 2.182276512030512e-05, 2.182276512030512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.182276512030512e-05

Optimization complete. Final v2v error: 3.8726770877838135 mm

Highest mean error: 4.366250038146973 mm for frame 117

Lowest mean error: 3.1537721157073975 mm for frame 11

Saving results

Total time: 45.34700632095337
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795997
Iteration 2/25 | Loss: 0.00143486
Iteration 3/25 | Loss: 0.00135016
Iteration 4/25 | Loss: 0.00132681
Iteration 5/25 | Loss: 0.00132033
Iteration 6/25 | Loss: 0.00131963
Iteration 7/25 | Loss: 0.00131963
Iteration 8/25 | Loss: 0.00131963
Iteration 9/25 | Loss: 0.00131963
Iteration 10/25 | Loss: 0.00131963
Iteration 11/25 | Loss: 0.00131963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013196252984926105, 0.0013196252984926105, 0.0013196252984926105, 0.0013196252984926105, 0.0013196252984926105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013196252984926105

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08426094
Iteration 2/25 | Loss: 0.00097790
Iteration 3/25 | Loss: 0.00097790
Iteration 4/25 | Loss: 0.00097790
Iteration 5/25 | Loss: 0.00097790
Iteration 6/25 | Loss: 0.00097790
Iteration 7/25 | Loss: 0.00097790
Iteration 8/25 | Loss: 0.00097790
Iteration 9/25 | Loss: 0.00097790
Iteration 10/25 | Loss: 0.00097790
Iteration 11/25 | Loss: 0.00097790
Iteration 12/25 | Loss: 0.00097790
Iteration 13/25 | Loss: 0.00097790
Iteration 14/25 | Loss: 0.00097790
Iteration 15/25 | Loss: 0.00097790
Iteration 16/25 | Loss: 0.00097790
Iteration 17/25 | Loss: 0.00097790
Iteration 18/25 | Loss: 0.00097790
Iteration 19/25 | Loss: 0.00097790
Iteration 20/25 | Loss: 0.00097790
Iteration 21/25 | Loss: 0.00097790
Iteration 22/25 | Loss: 0.00097790
Iteration 23/25 | Loss: 0.00097790
Iteration 24/25 | Loss: 0.00097790
Iteration 25/25 | Loss: 0.00097790

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097790
Iteration 2/1000 | Loss: 0.00003667
Iteration 3/1000 | Loss: 0.00003000
Iteration 4/1000 | Loss: 0.00002828
Iteration 5/1000 | Loss: 0.00002754
Iteration 6/1000 | Loss: 0.00002704
Iteration 7/1000 | Loss: 0.00002680
Iteration 8/1000 | Loss: 0.00002648
Iteration 9/1000 | Loss: 0.00002631
Iteration 10/1000 | Loss: 0.00002629
Iteration 11/1000 | Loss: 0.00002619
Iteration 12/1000 | Loss: 0.00002615
Iteration 13/1000 | Loss: 0.00002612
Iteration 14/1000 | Loss: 0.00002598
Iteration 15/1000 | Loss: 0.00002595
Iteration 16/1000 | Loss: 0.00002585
Iteration 17/1000 | Loss: 0.00002585
Iteration 18/1000 | Loss: 0.00002577
Iteration 19/1000 | Loss: 0.00002573
Iteration 20/1000 | Loss: 0.00002573
Iteration 21/1000 | Loss: 0.00002572
Iteration 22/1000 | Loss: 0.00002572
Iteration 23/1000 | Loss: 0.00002572
Iteration 24/1000 | Loss: 0.00002571
Iteration 25/1000 | Loss: 0.00002570
Iteration 26/1000 | Loss: 0.00002567
Iteration 27/1000 | Loss: 0.00002567
Iteration 28/1000 | Loss: 0.00002564
Iteration 29/1000 | Loss: 0.00002564
Iteration 30/1000 | Loss: 0.00002564
Iteration 31/1000 | Loss: 0.00002564
Iteration 32/1000 | Loss: 0.00002563
Iteration 33/1000 | Loss: 0.00002563
Iteration 34/1000 | Loss: 0.00002563
Iteration 35/1000 | Loss: 0.00002563
Iteration 36/1000 | Loss: 0.00002563
Iteration 37/1000 | Loss: 0.00002563
Iteration 38/1000 | Loss: 0.00002563
Iteration 39/1000 | Loss: 0.00002563
Iteration 40/1000 | Loss: 0.00002563
Iteration 41/1000 | Loss: 0.00002563
Iteration 42/1000 | Loss: 0.00002563
Iteration 43/1000 | Loss: 0.00002562
Iteration 44/1000 | Loss: 0.00002562
Iteration 45/1000 | Loss: 0.00002562
Iteration 46/1000 | Loss: 0.00002562
Iteration 47/1000 | Loss: 0.00002560
Iteration 48/1000 | Loss: 0.00002560
Iteration 49/1000 | Loss: 0.00002559
Iteration 50/1000 | Loss: 0.00002558
Iteration 51/1000 | Loss: 0.00002558
Iteration 52/1000 | Loss: 0.00002558
Iteration 53/1000 | Loss: 0.00002558
Iteration 54/1000 | Loss: 0.00002558
Iteration 55/1000 | Loss: 0.00002558
Iteration 56/1000 | Loss: 0.00002558
Iteration 57/1000 | Loss: 0.00002558
Iteration 58/1000 | Loss: 0.00002558
Iteration 59/1000 | Loss: 0.00002558
Iteration 60/1000 | Loss: 0.00002558
Iteration 61/1000 | Loss: 0.00002558
Iteration 62/1000 | Loss: 0.00002558
Iteration 63/1000 | Loss: 0.00002558
Iteration 64/1000 | Loss: 0.00002558
Iteration 65/1000 | Loss: 0.00002558
Iteration 66/1000 | Loss: 0.00002557
Iteration 67/1000 | Loss: 0.00002557
Iteration 68/1000 | Loss: 0.00002556
Iteration 69/1000 | Loss: 0.00002555
Iteration 70/1000 | Loss: 0.00002555
Iteration 71/1000 | Loss: 0.00002555
Iteration 72/1000 | Loss: 0.00002555
Iteration 73/1000 | Loss: 0.00002554
Iteration 74/1000 | Loss: 0.00002554
Iteration 75/1000 | Loss: 0.00002554
Iteration 76/1000 | Loss: 0.00002554
Iteration 77/1000 | Loss: 0.00002554
Iteration 78/1000 | Loss: 0.00002554
Iteration 79/1000 | Loss: 0.00002553
Iteration 80/1000 | Loss: 0.00002553
Iteration 81/1000 | Loss: 0.00002553
Iteration 82/1000 | Loss: 0.00002552
Iteration 83/1000 | Loss: 0.00002552
Iteration 84/1000 | Loss: 0.00002552
Iteration 85/1000 | Loss: 0.00002551
Iteration 86/1000 | Loss: 0.00002551
Iteration 87/1000 | Loss: 0.00002551
Iteration 88/1000 | Loss: 0.00002550
Iteration 89/1000 | Loss: 0.00002550
Iteration 90/1000 | Loss: 0.00002550
Iteration 91/1000 | Loss: 0.00002549
Iteration 92/1000 | Loss: 0.00002549
Iteration 93/1000 | Loss: 0.00002549
Iteration 94/1000 | Loss: 0.00002549
Iteration 95/1000 | Loss: 0.00002549
Iteration 96/1000 | Loss: 0.00002548
Iteration 97/1000 | Loss: 0.00002548
Iteration 98/1000 | Loss: 0.00002548
Iteration 99/1000 | Loss: 0.00002548
Iteration 100/1000 | Loss: 0.00002548
Iteration 101/1000 | Loss: 0.00002548
Iteration 102/1000 | Loss: 0.00002548
Iteration 103/1000 | Loss: 0.00002548
Iteration 104/1000 | Loss: 0.00002548
Iteration 105/1000 | Loss: 0.00002548
Iteration 106/1000 | Loss: 0.00002548
Iteration 107/1000 | Loss: 0.00002548
Iteration 108/1000 | Loss: 0.00002548
Iteration 109/1000 | Loss: 0.00002548
Iteration 110/1000 | Loss: 0.00002548
Iteration 111/1000 | Loss: 0.00002548
Iteration 112/1000 | Loss: 0.00002548
Iteration 113/1000 | Loss: 0.00002548
Iteration 114/1000 | Loss: 0.00002548
Iteration 115/1000 | Loss: 0.00002548
Iteration 116/1000 | Loss: 0.00002548
Iteration 117/1000 | Loss: 0.00002548
Iteration 118/1000 | Loss: 0.00002548
Iteration 119/1000 | Loss: 0.00002548
Iteration 120/1000 | Loss: 0.00002548
Iteration 121/1000 | Loss: 0.00002548
Iteration 122/1000 | Loss: 0.00002548
Iteration 123/1000 | Loss: 0.00002548
Iteration 124/1000 | Loss: 0.00002548
Iteration 125/1000 | Loss: 0.00002548
Iteration 126/1000 | Loss: 0.00002548
Iteration 127/1000 | Loss: 0.00002548
Iteration 128/1000 | Loss: 0.00002548
Iteration 129/1000 | Loss: 0.00002548
Iteration 130/1000 | Loss: 0.00002548
Iteration 131/1000 | Loss: 0.00002548
Iteration 132/1000 | Loss: 0.00002548
Iteration 133/1000 | Loss: 0.00002548
Iteration 134/1000 | Loss: 0.00002548
Iteration 135/1000 | Loss: 0.00002548
Iteration 136/1000 | Loss: 0.00002548
Iteration 137/1000 | Loss: 0.00002548
Iteration 138/1000 | Loss: 0.00002548
Iteration 139/1000 | Loss: 0.00002548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.5475328584434465e-05, 2.5475328584434465e-05, 2.5475328584434465e-05, 2.5475328584434465e-05, 2.5475328584434465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5475328584434465e-05

Optimization complete. Final v2v error: 4.098682403564453 mm

Highest mean error: 4.845944404602051 mm for frame 130

Lowest mean error: 3.861253499984741 mm for frame 121

Saving results

Total time: 39.02539014816284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462925
Iteration 2/25 | Loss: 0.00145058
Iteration 3/25 | Loss: 0.00129286
Iteration 4/25 | Loss: 0.00127450
Iteration 5/25 | Loss: 0.00126942
Iteration 6/25 | Loss: 0.00126916
Iteration 7/25 | Loss: 0.00126916
Iteration 8/25 | Loss: 0.00126916
Iteration 9/25 | Loss: 0.00126916
Iteration 10/25 | Loss: 0.00126916
Iteration 11/25 | Loss: 0.00126916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012691583251580596, 0.0012691583251580596, 0.0012691583251580596, 0.0012691583251580596, 0.0012691583251580596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012691583251580596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88679874
Iteration 2/25 | Loss: 0.00075807
Iteration 3/25 | Loss: 0.00075807
Iteration 4/25 | Loss: 0.00075807
Iteration 5/25 | Loss: 0.00075807
Iteration 6/25 | Loss: 0.00075807
Iteration 7/25 | Loss: 0.00075807
Iteration 8/25 | Loss: 0.00075806
Iteration 9/25 | Loss: 0.00075806
Iteration 10/25 | Loss: 0.00075806
Iteration 11/25 | Loss: 0.00075806
Iteration 12/25 | Loss: 0.00075806
Iteration 13/25 | Loss: 0.00075806
Iteration 14/25 | Loss: 0.00075806
Iteration 15/25 | Loss: 0.00075806
Iteration 16/25 | Loss: 0.00075806
Iteration 17/25 | Loss: 0.00075806
Iteration 18/25 | Loss: 0.00075806
Iteration 19/25 | Loss: 0.00075806
Iteration 20/25 | Loss: 0.00075806
Iteration 21/25 | Loss: 0.00075806
Iteration 22/25 | Loss: 0.00075806
Iteration 23/25 | Loss: 0.00075806
Iteration 24/25 | Loss: 0.00075806
Iteration 25/25 | Loss: 0.00075806

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075806
Iteration 2/1000 | Loss: 0.00003690
Iteration 3/1000 | Loss: 0.00002489
Iteration 4/1000 | Loss: 0.00002211
Iteration 5/1000 | Loss: 0.00002092
Iteration 6/1000 | Loss: 0.00001959
Iteration 7/1000 | Loss: 0.00001892
Iteration 8/1000 | Loss: 0.00001851
Iteration 9/1000 | Loss: 0.00001815
Iteration 10/1000 | Loss: 0.00001789
Iteration 11/1000 | Loss: 0.00001757
Iteration 12/1000 | Loss: 0.00001736
Iteration 13/1000 | Loss: 0.00001718
Iteration 14/1000 | Loss: 0.00001717
Iteration 15/1000 | Loss: 0.00001709
Iteration 16/1000 | Loss: 0.00001706
Iteration 17/1000 | Loss: 0.00001697
Iteration 18/1000 | Loss: 0.00001682
Iteration 19/1000 | Loss: 0.00001679
Iteration 20/1000 | Loss: 0.00001677
Iteration 21/1000 | Loss: 0.00001672
Iteration 22/1000 | Loss: 0.00001663
Iteration 23/1000 | Loss: 0.00001663
Iteration 24/1000 | Loss: 0.00001663
Iteration 25/1000 | Loss: 0.00001663
Iteration 26/1000 | Loss: 0.00001663
Iteration 27/1000 | Loss: 0.00001663
Iteration 28/1000 | Loss: 0.00001663
Iteration 29/1000 | Loss: 0.00001662
Iteration 30/1000 | Loss: 0.00001660
Iteration 31/1000 | Loss: 0.00001660
Iteration 32/1000 | Loss: 0.00001660
Iteration 33/1000 | Loss: 0.00001659
Iteration 34/1000 | Loss: 0.00001659
Iteration 35/1000 | Loss: 0.00001659
Iteration 36/1000 | Loss: 0.00001659
Iteration 37/1000 | Loss: 0.00001658
Iteration 38/1000 | Loss: 0.00001658
Iteration 39/1000 | Loss: 0.00001658
Iteration 40/1000 | Loss: 0.00001657
Iteration 41/1000 | Loss: 0.00001656
Iteration 42/1000 | Loss: 0.00001656
Iteration 43/1000 | Loss: 0.00001656
Iteration 44/1000 | Loss: 0.00001655
Iteration 45/1000 | Loss: 0.00001655
Iteration 46/1000 | Loss: 0.00001655
Iteration 47/1000 | Loss: 0.00001655
Iteration 48/1000 | Loss: 0.00001655
Iteration 49/1000 | Loss: 0.00001655
Iteration 50/1000 | Loss: 0.00001655
Iteration 51/1000 | Loss: 0.00001654
Iteration 52/1000 | Loss: 0.00001653
Iteration 53/1000 | Loss: 0.00001652
Iteration 54/1000 | Loss: 0.00001652
Iteration 55/1000 | Loss: 0.00001652
Iteration 56/1000 | Loss: 0.00001651
Iteration 57/1000 | Loss: 0.00001650
Iteration 58/1000 | Loss: 0.00001650
Iteration 59/1000 | Loss: 0.00001650
Iteration 60/1000 | Loss: 0.00001649
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001647
Iteration 63/1000 | Loss: 0.00001647
Iteration 64/1000 | Loss: 0.00001647
Iteration 65/1000 | Loss: 0.00001646
Iteration 66/1000 | Loss: 0.00001646
Iteration 67/1000 | Loss: 0.00001646
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001645
Iteration 71/1000 | Loss: 0.00001645
Iteration 72/1000 | Loss: 0.00001645
Iteration 73/1000 | Loss: 0.00001644
Iteration 74/1000 | Loss: 0.00001644
Iteration 75/1000 | Loss: 0.00001644
Iteration 76/1000 | Loss: 0.00001644
Iteration 77/1000 | Loss: 0.00001643
Iteration 78/1000 | Loss: 0.00001643
Iteration 79/1000 | Loss: 0.00001643
Iteration 80/1000 | Loss: 0.00001642
Iteration 81/1000 | Loss: 0.00001642
Iteration 82/1000 | Loss: 0.00001642
Iteration 83/1000 | Loss: 0.00001641
Iteration 84/1000 | Loss: 0.00001641
Iteration 85/1000 | Loss: 0.00001641
Iteration 86/1000 | Loss: 0.00001641
Iteration 87/1000 | Loss: 0.00001641
Iteration 88/1000 | Loss: 0.00001641
Iteration 89/1000 | Loss: 0.00001641
Iteration 90/1000 | Loss: 0.00001641
Iteration 91/1000 | Loss: 0.00001641
Iteration 92/1000 | Loss: 0.00001641
Iteration 93/1000 | Loss: 0.00001641
Iteration 94/1000 | Loss: 0.00001640
Iteration 95/1000 | Loss: 0.00001640
Iteration 96/1000 | Loss: 0.00001640
Iteration 97/1000 | Loss: 0.00001640
Iteration 98/1000 | Loss: 0.00001640
Iteration 99/1000 | Loss: 0.00001640
Iteration 100/1000 | Loss: 0.00001640
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001639
Iteration 104/1000 | Loss: 0.00001639
Iteration 105/1000 | Loss: 0.00001639
Iteration 106/1000 | Loss: 0.00001639
Iteration 107/1000 | Loss: 0.00001639
Iteration 108/1000 | Loss: 0.00001639
Iteration 109/1000 | Loss: 0.00001639
Iteration 110/1000 | Loss: 0.00001639
Iteration 111/1000 | Loss: 0.00001639
Iteration 112/1000 | Loss: 0.00001639
Iteration 113/1000 | Loss: 0.00001639
Iteration 114/1000 | Loss: 0.00001639
Iteration 115/1000 | Loss: 0.00001639
Iteration 116/1000 | Loss: 0.00001639
Iteration 117/1000 | Loss: 0.00001638
Iteration 118/1000 | Loss: 0.00001638
Iteration 119/1000 | Loss: 0.00001638
Iteration 120/1000 | Loss: 0.00001638
Iteration 121/1000 | Loss: 0.00001638
Iteration 122/1000 | Loss: 0.00001638
Iteration 123/1000 | Loss: 0.00001638
Iteration 124/1000 | Loss: 0.00001638
Iteration 125/1000 | Loss: 0.00001637
Iteration 126/1000 | Loss: 0.00001637
Iteration 127/1000 | Loss: 0.00001637
Iteration 128/1000 | Loss: 0.00001637
Iteration 129/1000 | Loss: 0.00001636
Iteration 130/1000 | Loss: 0.00001636
Iteration 131/1000 | Loss: 0.00001636
Iteration 132/1000 | Loss: 0.00001636
Iteration 133/1000 | Loss: 0.00001636
Iteration 134/1000 | Loss: 0.00001636
Iteration 135/1000 | Loss: 0.00001636
Iteration 136/1000 | Loss: 0.00001635
Iteration 137/1000 | Loss: 0.00001635
Iteration 138/1000 | Loss: 0.00001635
Iteration 139/1000 | Loss: 0.00001635
Iteration 140/1000 | Loss: 0.00001635
Iteration 141/1000 | Loss: 0.00001635
Iteration 142/1000 | Loss: 0.00001634
Iteration 143/1000 | Loss: 0.00001634
Iteration 144/1000 | Loss: 0.00001634
Iteration 145/1000 | Loss: 0.00001634
Iteration 146/1000 | Loss: 0.00001633
Iteration 147/1000 | Loss: 0.00001633
Iteration 148/1000 | Loss: 0.00001633
Iteration 149/1000 | Loss: 0.00001633
Iteration 150/1000 | Loss: 0.00001633
Iteration 151/1000 | Loss: 0.00001633
Iteration 152/1000 | Loss: 0.00001632
Iteration 153/1000 | Loss: 0.00001632
Iteration 154/1000 | Loss: 0.00001632
Iteration 155/1000 | Loss: 0.00001632
Iteration 156/1000 | Loss: 0.00001632
Iteration 157/1000 | Loss: 0.00001631
Iteration 158/1000 | Loss: 0.00001631
Iteration 159/1000 | Loss: 0.00001631
Iteration 160/1000 | Loss: 0.00001631
Iteration 161/1000 | Loss: 0.00001631
Iteration 162/1000 | Loss: 0.00001631
Iteration 163/1000 | Loss: 0.00001631
Iteration 164/1000 | Loss: 0.00001631
Iteration 165/1000 | Loss: 0.00001631
Iteration 166/1000 | Loss: 0.00001630
Iteration 167/1000 | Loss: 0.00001630
Iteration 168/1000 | Loss: 0.00001630
Iteration 169/1000 | Loss: 0.00001630
Iteration 170/1000 | Loss: 0.00001630
Iteration 171/1000 | Loss: 0.00001630
Iteration 172/1000 | Loss: 0.00001630
Iteration 173/1000 | Loss: 0.00001629
Iteration 174/1000 | Loss: 0.00001629
Iteration 175/1000 | Loss: 0.00001629
Iteration 176/1000 | Loss: 0.00001629
Iteration 177/1000 | Loss: 0.00001629
Iteration 178/1000 | Loss: 0.00001628
Iteration 179/1000 | Loss: 0.00001628
Iteration 180/1000 | Loss: 0.00001628
Iteration 181/1000 | Loss: 0.00001628
Iteration 182/1000 | Loss: 0.00001628
Iteration 183/1000 | Loss: 0.00001628
Iteration 184/1000 | Loss: 0.00001627
Iteration 185/1000 | Loss: 0.00001627
Iteration 186/1000 | Loss: 0.00001627
Iteration 187/1000 | Loss: 0.00001627
Iteration 188/1000 | Loss: 0.00001627
Iteration 189/1000 | Loss: 0.00001627
Iteration 190/1000 | Loss: 0.00001627
Iteration 191/1000 | Loss: 0.00001627
Iteration 192/1000 | Loss: 0.00001627
Iteration 193/1000 | Loss: 0.00001626
Iteration 194/1000 | Loss: 0.00001626
Iteration 195/1000 | Loss: 0.00001626
Iteration 196/1000 | Loss: 0.00001626
Iteration 197/1000 | Loss: 0.00001626
Iteration 198/1000 | Loss: 0.00001626
Iteration 199/1000 | Loss: 0.00001626
Iteration 200/1000 | Loss: 0.00001626
Iteration 201/1000 | Loss: 0.00001626
Iteration 202/1000 | Loss: 0.00001626
Iteration 203/1000 | Loss: 0.00001626
Iteration 204/1000 | Loss: 0.00001626
Iteration 205/1000 | Loss: 0.00001626
Iteration 206/1000 | Loss: 0.00001626
Iteration 207/1000 | Loss: 0.00001626
Iteration 208/1000 | Loss: 0.00001626
Iteration 209/1000 | Loss: 0.00001626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.625834556762129e-05, 1.625834556762129e-05, 1.625834556762129e-05, 1.625834556762129e-05, 1.625834556762129e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.625834556762129e-05

Optimization complete. Final v2v error: 3.413022518157959 mm

Highest mean error: 3.6622633934020996 mm for frame 250

Lowest mean error: 3.285475969314575 mm for frame 176

Saving results

Total time: 52.264079093933105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386021
Iteration 2/25 | Loss: 0.00130542
Iteration 3/25 | Loss: 0.00122094
Iteration 4/25 | Loss: 0.00121094
Iteration 5/25 | Loss: 0.00120841
Iteration 6/25 | Loss: 0.00120798
Iteration 7/25 | Loss: 0.00120798
Iteration 8/25 | Loss: 0.00120798
Iteration 9/25 | Loss: 0.00120798
Iteration 10/25 | Loss: 0.00120798
Iteration 11/25 | Loss: 0.00120798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012079825391992927, 0.0012079825391992927, 0.0012079825391992927, 0.0012079825391992927, 0.0012079825391992927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012079825391992927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72801650
Iteration 2/25 | Loss: 0.00080510
Iteration 3/25 | Loss: 0.00080510
Iteration 4/25 | Loss: 0.00080509
Iteration 5/25 | Loss: 0.00080509
Iteration 6/25 | Loss: 0.00080509
Iteration 7/25 | Loss: 0.00080509
Iteration 8/25 | Loss: 0.00080509
Iteration 9/25 | Loss: 0.00080509
Iteration 10/25 | Loss: 0.00080509
Iteration 11/25 | Loss: 0.00080509
Iteration 12/25 | Loss: 0.00080509
Iteration 13/25 | Loss: 0.00080509
Iteration 14/25 | Loss: 0.00080509
Iteration 15/25 | Loss: 0.00080509
Iteration 16/25 | Loss: 0.00080509
Iteration 17/25 | Loss: 0.00080509
Iteration 18/25 | Loss: 0.00080509
Iteration 19/25 | Loss: 0.00080509
Iteration 20/25 | Loss: 0.00080509
Iteration 21/25 | Loss: 0.00080509
Iteration 22/25 | Loss: 0.00080509
Iteration 23/25 | Loss: 0.00080509
Iteration 24/25 | Loss: 0.00080509
Iteration 25/25 | Loss: 0.00080509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080509
Iteration 2/1000 | Loss: 0.00002871
Iteration 3/1000 | Loss: 0.00001800
Iteration 4/1000 | Loss: 0.00001443
Iteration 5/1000 | Loss: 0.00001333
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001209
Iteration 8/1000 | Loss: 0.00001179
Iteration 9/1000 | Loss: 0.00001176
Iteration 10/1000 | Loss: 0.00001173
Iteration 11/1000 | Loss: 0.00001168
Iteration 12/1000 | Loss: 0.00001167
Iteration 13/1000 | Loss: 0.00001151
Iteration 14/1000 | Loss: 0.00001147
Iteration 15/1000 | Loss: 0.00001140
Iteration 16/1000 | Loss: 0.00001138
Iteration 17/1000 | Loss: 0.00001136
Iteration 18/1000 | Loss: 0.00001136
Iteration 19/1000 | Loss: 0.00001127
Iteration 20/1000 | Loss: 0.00001123
Iteration 21/1000 | Loss: 0.00001120
Iteration 22/1000 | Loss: 0.00001120
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001118
Iteration 25/1000 | Loss: 0.00001118
Iteration 26/1000 | Loss: 0.00001118
Iteration 27/1000 | Loss: 0.00001117
Iteration 28/1000 | Loss: 0.00001117
Iteration 29/1000 | Loss: 0.00001116
Iteration 30/1000 | Loss: 0.00001115
Iteration 31/1000 | Loss: 0.00001115
Iteration 32/1000 | Loss: 0.00001114
Iteration 33/1000 | Loss: 0.00001114
Iteration 34/1000 | Loss: 0.00001113
Iteration 35/1000 | Loss: 0.00001113
Iteration 36/1000 | Loss: 0.00001113
Iteration 37/1000 | Loss: 0.00001112
Iteration 38/1000 | Loss: 0.00001112
Iteration 39/1000 | Loss: 0.00001111
Iteration 40/1000 | Loss: 0.00001111
Iteration 41/1000 | Loss: 0.00001110
Iteration 42/1000 | Loss: 0.00001110
Iteration 43/1000 | Loss: 0.00001110
Iteration 44/1000 | Loss: 0.00001110
Iteration 45/1000 | Loss: 0.00001109
Iteration 46/1000 | Loss: 0.00001109
Iteration 47/1000 | Loss: 0.00001109
Iteration 48/1000 | Loss: 0.00001109
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001108
Iteration 51/1000 | Loss: 0.00001108
Iteration 52/1000 | Loss: 0.00001108
Iteration 53/1000 | Loss: 0.00001107
Iteration 54/1000 | Loss: 0.00001107
Iteration 55/1000 | Loss: 0.00001106
Iteration 56/1000 | Loss: 0.00001106
Iteration 57/1000 | Loss: 0.00001106
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001104
Iteration 62/1000 | Loss: 0.00001104
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001103
Iteration 66/1000 | Loss: 0.00001103
Iteration 67/1000 | Loss: 0.00001103
Iteration 68/1000 | Loss: 0.00001103
Iteration 69/1000 | Loss: 0.00001103
Iteration 70/1000 | Loss: 0.00001103
Iteration 71/1000 | Loss: 0.00001103
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001102
Iteration 75/1000 | Loss: 0.00001102
Iteration 76/1000 | Loss: 0.00001102
Iteration 77/1000 | Loss: 0.00001102
Iteration 78/1000 | Loss: 0.00001102
Iteration 79/1000 | Loss: 0.00001102
Iteration 80/1000 | Loss: 0.00001102
Iteration 81/1000 | Loss: 0.00001101
Iteration 82/1000 | Loss: 0.00001101
Iteration 83/1000 | Loss: 0.00001101
Iteration 84/1000 | Loss: 0.00001101
Iteration 85/1000 | Loss: 0.00001100
Iteration 86/1000 | Loss: 0.00001100
Iteration 87/1000 | Loss: 0.00001100
Iteration 88/1000 | Loss: 0.00001100
Iteration 89/1000 | Loss: 0.00001099
Iteration 90/1000 | Loss: 0.00001099
Iteration 91/1000 | Loss: 0.00001099
Iteration 92/1000 | Loss: 0.00001099
Iteration 93/1000 | Loss: 0.00001099
Iteration 94/1000 | Loss: 0.00001098
Iteration 95/1000 | Loss: 0.00001098
Iteration 96/1000 | Loss: 0.00001098
Iteration 97/1000 | Loss: 0.00001098
Iteration 98/1000 | Loss: 0.00001098
Iteration 99/1000 | Loss: 0.00001098
Iteration 100/1000 | Loss: 0.00001097
Iteration 101/1000 | Loss: 0.00001097
Iteration 102/1000 | Loss: 0.00001097
Iteration 103/1000 | Loss: 0.00001097
Iteration 104/1000 | Loss: 0.00001096
Iteration 105/1000 | Loss: 0.00001096
Iteration 106/1000 | Loss: 0.00001096
Iteration 107/1000 | Loss: 0.00001096
Iteration 108/1000 | Loss: 0.00001095
Iteration 109/1000 | Loss: 0.00001095
Iteration 110/1000 | Loss: 0.00001095
Iteration 111/1000 | Loss: 0.00001094
Iteration 112/1000 | Loss: 0.00001094
Iteration 113/1000 | Loss: 0.00001094
Iteration 114/1000 | Loss: 0.00001093
Iteration 115/1000 | Loss: 0.00001093
Iteration 116/1000 | Loss: 0.00001093
Iteration 117/1000 | Loss: 0.00001093
Iteration 118/1000 | Loss: 0.00001093
Iteration 119/1000 | Loss: 0.00001092
Iteration 120/1000 | Loss: 0.00001092
Iteration 121/1000 | Loss: 0.00001092
Iteration 122/1000 | Loss: 0.00001091
Iteration 123/1000 | Loss: 0.00001090
Iteration 124/1000 | Loss: 0.00001090
Iteration 125/1000 | Loss: 0.00001090
Iteration 126/1000 | Loss: 0.00001089
Iteration 127/1000 | Loss: 0.00001089
Iteration 128/1000 | Loss: 0.00001089
Iteration 129/1000 | Loss: 0.00001089
Iteration 130/1000 | Loss: 0.00001089
Iteration 131/1000 | Loss: 0.00001089
Iteration 132/1000 | Loss: 0.00001089
Iteration 133/1000 | Loss: 0.00001089
Iteration 134/1000 | Loss: 0.00001088
Iteration 135/1000 | Loss: 0.00001088
Iteration 136/1000 | Loss: 0.00001088
Iteration 137/1000 | Loss: 0.00001088
Iteration 138/1000 | Loss: 0.00001088
Iteration 139/1000 | Loss: 0.00001088
Iteration 140/1000 | Loss: 0.00001088
Iteration 141/1000 | Loss: 0.00001088
Iteration 142/1000 | Loss: 0.00001087
Iteration 143/1000 | Loss: 0.00001087
Iteration 144/1000 | Loss: 0.00001087
Iteration 145/1000 | Loss: 0.00001087
Iteration 146/1000 | Loss: 0.00001087
Iteration 147/1000 | Loss: 0.00001087
Iteration 148/1000 | Loss: 0.00001086
Iteration 149/1000 | Loss: 0.00001086
Iteration 150/1000 | Loss: 0.00001086
Iteration 151/1000 | Loss: 0.00001086
Iteration 152/1000 | Loss: 0.00001086
Iteration 153/1000 | Loss: 0.00001086
Iteration 154/1000 | Loss: 0.00001086
Iteration 155/1000 | Loss: 0.00001086
Iteration 156/1000 | Loss: 0.00001085
Iteration 157/1000 | Loss: 0.00001085
Iteration 158/1000 | Loss: 0.00001085
Iteration 159/1000 | Loss: 0.00001085
Iteration 160/1000 | Loss: 0.00001084
Iteration 161/1000 | Loss: 0.00001084
Iteration 162/1000 | Loss: 0.00001084
Iteration 163/1000 | Loss: 0.00001084
Iteration 164/1000 | Loss: 0.00001084
Iteration 165/1000 | Loss: 0.00001084
Iteration 166/1000 | Loss: 0.00001083
Iteration 167/1000 | Loss: 0.00001083
Iteration 168/1000 | Loss: 0.00001083
Iteration 169/1000 | Loss: 0.00001083
Iteration 170/1000 | Loss: 0.00001083
Iteration 171/1000 | Loss: 0.00001083
Iteration 172/1000 | Loss: 0.00001083
Iteration 173/1000 | Loss: 0.00001083
Iteration 174/1000 | Loss: 0.00001083
Iteration 175/1000 | Loss: 0.00001082
Iteration 176/1000 | Loss: 0.00001082
Iteration 177/1000 | Loss: 0.00001082
Iteration 178/1000 | Loss: 0.00001082
Iteration 179/1000 | Loss: 0.00001082
Iteration 180/1000 | Loss: 0.00001082
Iteration 181/1000 | Loss: 0.00001081
Iteration 182/1000 | Loss: 0.00001081
Iteration 183/1000 | Loss: 0.00001081
Iteration 184/1000 | Loss: 0.00001081
Iteration 185/1000 | Loss: 0.00001081
Iteration 186/1000 | Loss: 0.00001081
Iteration 187/1000 | Loss: 0.00001081
Iteration 188/1000 | Loss: 0.00001081
Iteration 189/1000 | Loss: 0.00001081
Iteration 190/1000 | Loss: 0.00001081
Iteration 191/1000 | Loss: 0.00001081
Iteration 192/1000 | Loss: 0.00001081
Iteration 193/1000 | Loss: 0.00001081
Iteration 194/1000 | Loss: 0.00001081
Iteration 195/1000 | Loss: 0.00001081
Iteration 196/1000 | Loss: 0.00001081
Iteration 197/1000 | Loss: 0.00001081
Iteration 198/1000 | Loss: 0.00001081
Iteration 199/1000 | Loss: 0.00001081
Iteration 200/1000 | Loss: 0.00001081
Iteration 201/1000 | Loss: 0.00001080
Iteration 202/1000 | Loss: 0.00001080
Iteration 203/1000 | Loss: 0.00001080
Iteration 204/1000 | Loss: 0.00001080
Iteration 205/1000 | Loss: 0.00001080
Iteration 206/1000 | Loss: 0.00001080
Iteration 207/1000 | Loss: 0.00001080
Iteration 208/1000 | Loss: 0.00001080
Iteration 209/1000 | Loss: 0.00001080
Iteration 210/1000 | Loss: 0.00001080
Iteration 211/1000 | Loss: 0.00001080
Iteration 212/1000 | Loss: 0.00001080
Iteration 213/1000 | Loss: 0.00001080
Iteration 214/1000 | Loss: 0.00001080
Iteration 215/1000 | Loss: 0.00001080
Iteration 216/1000 | Loss: 0.00001080
Iteration 217/1000 | Loss: 0.00001080
Iteration 218/1000 | Loss: 0.00001080
Iteration 219/1000 | Loss: 0.00001080
Iteration 220/1000 | Loss: 0.00001080
Iteration 221/1000 | Loss: 0.00001080
Iteration 222/1000 | Loss: 0.00001080
Iteration 223/1000 | Loss: 0.00001079
Iteration 224/1000 | Loss: 0.00001079
Iteration 225/1000 | Loss: 0.00001079
Iteration 226/1000 | Loss: 0.00001079
Iteration 227/1000 | Loss: 0.00001079
Iteration 228/1000 | Loss: 0.00001079
Iteration 229/1000 | Loss: 0.00001079
Iteration 230/1000 | Loss: 0.00001079
Iteration 231/1000 | Loss: 0.00001079
Iteration 232/1000 | Loss: 0.00001079
Iteration 233/1000 | Loss: 0.00001079
Iteration 234/1000 | Loss: 0.00001079
Iteration 235/1000 | Loss: 0.00001079
Iteration 236/1000 | Loss: 0.00001079
Iteration 237/1000 | Loss: 0.00001079
Iteration 238/1000 | Loss: 0.00001079
Iteration 239/1000 | Loss: 0.00001079
Iteration 240/1000 | Loss: 0.00001079
Iteration 241/1000 | Loss: 0.00001079
Iteration 242/1000 | Loss: 0.00001079
Iteration 243/1000 | Loss: 0.00001079
Iteration 244/1000 | Loss: 0.00001079
Iteration 245/1000 | Loss: 0.00001079
Iteration 246/1000 | Loss: 0.00001079
Iteration 247/1000 | Loss: 0.00001079
Iteration 248/1000 | Loss: 0.00001079
Iteration 249/1000 | Loss: 0.00001079
Iteration 250/1000 | Loss: 0.00001079
Iteration 251/1000 | Loss: 0.00001079
Iteration 252/1000 | Loss: 0.00001079
Iteration 253/1000 | Loss: 0.00001079
Iteration 254/1000 | Loss: 0.00001079
Iteration 255/1000 | Loss: 0.00001079
Iteration 256/1000 | Loss: 0.00001079
Iteration 257/1000 | Loss: 0.00001079
Iteration 258/1000 | Loss: 0.00001079
Iteration 259/1000 | Loss: 0.00001079
Iteration 260/1000 | Loss: 0.00001079
Iteration 261/1000 | Loss: 0.00001079
Iteration 262/1000 | Loss: 0.00001079
Iteration 263/1000 | Loss: 0.00001079
Iteration 264/1000 | Loss: 0.00001079
Iteration 265/1000 | Loss: 0.00001079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.0793846740853041e-05, 1.0793846740853041e-05, 1.0793846740853041e-05, 1.0793846740853041e-05, 1.0793846740853041e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0793846740853041e-05

Optimization complete. Final v2v error: 2.813683271408081 mm

Highest mean error: 3.306885004043579 mm for frame 83

Lowest mean error: 2.704246759414673 mm for frame 22

Saving results

Total time: 40.53676414489746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903854
Iteration 2/25 | Loss: 0.00178720
Iteration 3/25 | Loss: 0.00166774
Iteration 4/25 | Loss: 0.00164643
Iteration 5/25 | Loss: 0.00164142
Iteration 6/25 | Loss: 0.00163675
Iteration 7/25 | Loss: 0.00163424
Iteration 8/25 | Loss: 0.00163597
Iteration 9/25 | Loss: 0.00163449
Iteration 10/25 | Loss: 0.00163382
Iteration 11/25 | Loss: 0.00163166
Iteration 12/25 | Loss: 0.00163055
Iteration 13/25 | Loss: 0.00163005
Iteration 14/25 | Loss: 0.00162985
Iteration 15/25 | Loss: 0.00162983
Iteration 16/25 | Loss: 0.00162983
Iteration 17/25 | Loss: 0.00162983
Iteration 18/25 | Loss: 0.00162983
Iteration 19/25 | Loss: 0.00162983
Iteration 20/25 | Loss: 0.00162983
Iteration 21/25 | Loss: 0.00162983
Iteration 22/25 | Loss: 0.00162982
Iteration 23/25 | Loss: 0.00162982
Iteration 24/25 | Loss: 0.00162982
Iteration 25/25 | Loss: 0.00162982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35352588
Iteration 2/25 | Loss: 0.00361977
Iteration 3/25 | Loss: 0.00361968
Iteration 4/25 | Loss: 0.00361968
Iteration 5/25 | Loss: 0.00361968
Iteration 6/25 | Loss: 0.00361968
Iteration 7/25 | Loss: 0.00361968
Iteration 8/25 | Loss: 0.00361968
Iteration 9/25 | Loss: 0.00361968
Iteration 10/25 | Loss: 0.00361968
Iteration 11/25 | Loss: 0.00361968
Iteration 12/25 | Loss: 0.00361968
Iteration 13/25 | Loss: 0.00361968
Iteration 14/25 | Loss: 0.00361968
Iteration 15/25 | Loss: 0.00361968
Iteration 16/25 | Loss: 0.00361968
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0036196764558553696, 0.0036196764558553696, 0.0036196764558553696, 0.0036196764558553696, 0.0036196764558553696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036196764558553696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00361968
Iteration 2/1000 | Loss: 0.00036701
Iteration 3/1000 | Loss: 0.00024328
Iteration 4/1000 | Loss: 0.00051022
Iteration 5/1000 | Loss: 0.00060834
Iteration 6/1000 | Loss: 0.00070202
Iteration 7/1000 | Loss: 0.00113623
Iteration 8/1000 | Loss: 0.00090744
Iteration 9/1000 | Loss: 0.00215903
Iteration 10/1000 | Loss: 0.00040889
Iteration 11/1000 | Loss: 0.00136992
Iteration 12/1000 | Loss: 0.00135886
Iteration 13/1000 | Loss: 0.00124054
Iteration 14/1000 | Loss: 0.00476111
Iteration 15/1000 | Loss: 0.00076893
Iteration 16/1000 | Loss: 0.00049034
Iteration 17/1000 | Loss: 0.00025487
Iteration 18/1000 | Loss: 0.00013209
Iteration 19/1000 | Loss: 0.00377796
Iteration 20/1000 | Loss: 0.00710929
Iteration 21/1000 | Loss: 0.00316418
Iteration 22/1000 | Loss: 0.00070053
Iteration 23/1000 | Loss: 0.00066332
Iteration 24/1000 | Loss: 0.00011853
Iteration 25/1000 | Loss: 0.00077458
Iteration 26/1000 | Loss: 0.00011055
Iteration 27/1000 | Loss: 0.00008651
Iteration 28/1000 | Loss: 0.00007752
Iteration 29/1000 | Loss: 0.00057041
Iteration 30/1000 | Loss: 0.00458016
Iteration 31/1000 | Loss: 0.00205269
Iteration 32/1000 | Loss: 0.00024683
Iteration 33/1000 | Loss: 0.00008004
Iteration 34/1000 | Loss: 0.00006934
Iteration 35/1000 | Loss: 0.00006342
Iteration 36/1000 | Loss: 0.00006037
Iteration 37/1000 | Loss: 0.00031729
Iteration 38/1000 | Loss: 0.00006026
Iteration 39/1000 | Loss: 0.00200509
Iteration 40/1000 | Loss: 0.00131842
Iteration 41/1000 | Loss: 0.00021179
Iteration 42/1000 | Loss: 0.00071336
Iteration 43/1000 | Loss: 0.00040846
Iteration 44/1000 | Loss: 0.00007370
Iteration 45/1000 | Loss: 0.00046569
Iteration 46/1000 | Loss: 0.00005373
Iteration 47/1000 | Loss: 0.00004437
Iteration 48/1000 | Loss: 0.00003927
Iteration 49/1000 | Loss: 0.00003659
Iteration 50/1000 | Loss: 0.00003457
Iteration 51/1000 | Loss: 0.00003299
Iteration 52/1000 | Loss: 0.00003194
Iteration 53/1000 | Loss: 0.00003140
Iteration 54/1000 | Loss: 0.00003099
Iteration 55/1000 | Loss: 0.00003038
Iteration 56/1000 | Loss: 0.00003002
Iteration 57/1000 | Loss: 0.00002974
Iteration 58/1000 | Loss: 0.00002950
Iteration 59/1000 | Loss: 0.00002924
Iteration 60/1000 | Loss: 0.00002903
Iteration 61/1000 | Loss: 0.00002886
Iteration 62/1000 | Loss: 0.00002878
Iteration 63/1000 | Loss: 0.00002878
Iteration 64/1000 | Loss: 0.00002872
Iteration 65/1000 | Loss: 0.00002872
Iteration 66/1000 | Loss: 0.00002870
Iteration 67/1000 | Loss: 0.00002869
Iteration 68/1000 | Loss: 0.00002867
Iteration 69/1000 | Loss: 0.00002860
Iteration 70/1000 | Loss: 0.00002857
Iteration 71/1000 | Loss: 0.00002856
Iteration 72/1000 | Loss: 0.00002855
Iteration 73/1000 | Loss: 0.00002855
Iteration 74/1000 | Loss: 0.00002854
Iteration 75/1000 | Loss: 0.00002854
Iteration 76/1000 | Loss: 0.00002851
Iteration 77/1000 | Loss: 0.00002851
Iteration 78/1000 | Loss: 0.00002849
Iteration 79/1000 | Loss: 0.00002847
Iteration 80/1000 | Loss: 0.00002847
Iteration 81/1000 | Loss: 0.00002846
Iteration 82/1000 | Loss: 0.00002845
Iteration 83/1000 | Loss: 0.00002845
Iteration 84/1000 | Loss: 0.00002844
Iteration 85/1000 | Loss: 0.00002844
Iteration 86/1000 | Loss: 0.00002843
Iteration 87/1000 | Loss: 0.00002843
Iteration 88/1000 | Loss: 0.00002842
Iteration 89/1000 | Loss: 0.00002842
Iteration 90/1000 | Loss: 0.00002842
Iteration 91/1000 | Loss: 0.00002841
Iteration 92/1000 | Loss: 0.00002841
Iteration 93/1000 | Loss: 0.00002841
Iteration 94/1000 | Loss: 0.00002840
Iteration 95/1000 | Loss: 0.00002840
Iteration 96/1000 | Loss: 0.00002840
Iteration 97/1000 | Loss: 0.00002840
Iteration 98/1000 | Loss: 0.00002840
Iteration 99/1000 | Loss: 0.00002840
Iteration 100/1000 | Loss: 0.00002840
Iteration 101/1000 | Loss: 0.00002840
Iteration 102/1000 | Loss: 0.00002840
Iteration 103/1000 | Loss: 0.00002840
Iteration 104/1000 | Loss: 0.00002840
Iteration 105/1000 | Loss: 0.00002840
Iteration 106/1000 | Loss: 0.00002840
Iteration 107/1000 | Loss: 0.00002840
Iteration 108/1000 | Loss: 0.00002839
Iteration 109/1000 | Loss: 0.00002839
Iteration 110/1000 | Loss: 0.00002839
Iteration 111/1000 | Loss: 0.00002839
Iteration 112/1000 | Loss: 0.00002839
Iteration 113/1000 | Loss: 0.00002839
Iteration 114/1000 | Loss: 0.00002839
Iteration 115/1000 | Loss: 0.00002839
Iteration 116/1000 | Loss: 0.00002838
Iteration 117/1000 | Loss: 0.00002838
Iteration 118/1000 | Loss: 0.00002838
Iteration 119/1000 | Loss: 0.00002838
Iteration 120/1000 | Loss: 0.00002838
Iteration 121/1000 | Loss: 0.00002838
Iteration 122/1000 | Loss: 0.00002838
Iteration 123/1000 | Loss: 0.00002838
Iteration 124/1000 | Loss: 0.00002837
Iteration 125/1000 | Loss: 0.00002837
Iteration 126/1000 | Loss: 0.00002837
Iteration 127/1000 | Loss: 0.00002837
Iteration 128/1000 | Loss: 0.00002836
Iteration 129/1000 | Loss: 0.00002836
Iteration 130/1000 | Loss: 0.00002836
Iteration 131/1000 | Loss: 0.00002835
Iteration 132/1000 | Loss: 0.00002835
Iteration 133/1000 | Loss: 0.00002835
Iteration 134/1000 | Loss: 0.00002835
Iteration 135/1000 | Loss: 0.00002835
Iteration 136/1000 | Loss: 0.00002835
Iteration 137/1000 | Loss: 0.00002834
Iteration 138/1000 | Loss: 0.00002834
Iteration 139/1000 | Loss: 0.00002834
Iteration 140/1000 | Loss: 0.00002834
Iteration 141/1000 | Loss: 0.00002834
Iteration 142/1000 | Loss: 0.00002833
Iteration 143/1000 | Loss: 0.00002833
Iteration 144/1000 | Loss: 0.00002833
Iteration 145/1000 | Loss: 0.00002832
Iteration 146/1000 | Loss: 0.00002832
Iteration 147/1000 | Loss: 0.00002832
Iteration 148/1000 | Loss: 0.00002832
Iteration 149/1000 | Loss: 0.00002832
Iteration 150/1000 | Loss: 0.00002832
Iteration 151/1000 | Loss: 0.00002832
Iteration 152/1000 | Loss: 0.00002831
Iteration 153/1000 | Loss: 0.00002831
Iteration 154/1000 | Loss: 0.00002831
Iteration 155/1000 | Loss: 0.00002831
Iteration 156/1000 | Loss: 0.00002831
Iteration 157/1000 | Loss: 0.00002831
Iteration 158/1000 | Loss: 0.00002831
Iteration 159/1000 | Loss: 0.00002831
Iteration 160/1000 | Loss: 0.00002831
Iteration 161/1000 | Loss: 0.00002830
Iteration 162/1000 | Loss: 0.00002830
Iteration 163/1000 | Loss: 0.00002830
Iteration 164/1000 | Loss: 0.00002830
Iteration 165/1000 | Loss: 0.00002830
Iteration 166/1000 | Loss: 0.00002830
Iteration 167/1000 | Loss: 0.00002830
Iteration 168/1000 | Loss: 0.00002830
Iteration 169/1000 | Loss: 0.00002830
Iteration 170/1000 | Loss: 0.00002830
Iteration 171/1000 | Loss: 0.00002830
Iteration 172/1000 | Loss: 0.00002830
Iteration 173/1000 | Loss: 0.00002829
Iteration 174/1000 | Loss: 0.00002829
Iteration 175/1000 | Loss: 0.00002829
Iteration 176/1000 | Loss: 0.00002829
Iteration 177/1000 | Loss: 0.00002829
Iteration 178/1000 | Loss: 0.00002829
Iteration 179/1000 | Loss: 0.00002829
Iteration 180/1000 | Loss: 0.00002829
Iteration 181/1000 | Loss: 0.00002828
Iteration 182/1000 | Loss: 0.00002828
Iteration 183/1000 | Loss: 0.00002828
Iteration 184/1000 | Loss: 0.00002828
Iteration 185/1000 | Loss: 0.00002827
Iteration 186/1000 | Loss: 0.00002827
Iteration 187/1000 | Loss: 0.00002827
Iteration 188/1000 | Loss: 0.00002827
Iteration 189/1000 | Loss: 0.00002827
Iteration 190/1000 | Loss: 0.00002827
Iteration 191/1000 | Loss: 0.00002826
Iteration 192/1000 | Loss: 0.00002826
Iteration 193/1000 | Loss: 0.00002826
Iteration 194/1000 | Loss: 0.00002826
Iteration 195/1000 | Loss: 0.00002826
Iteration 196/1000 | Loss: 0.00002825
Iteration 197/1000 | Loss: 0.00002825
Iteration 198/1000 | Loss: 0.00002825
Iteration 199/1000 | Loss: 0.00002825
Iteration 200/1000 | Loss: 0.00002825
Iteration 201/1000 | Loss: 0.00002825
Iteration 202/1000 | Loss: 0.00002825
Iteration 203/1000 | Loss: 0.00002825
Iteration 204/1000 | Loss: 0.00002825
Iteration 205/1000 | Loss: 0.00002825
Iteration 206/1000 | Loss: 0.00002825
Iteration 207/1000 | Loss: 0.00002824
Iteration 208/1000 | Loss: 0.00002824
Iteration 209/1000 | Loss: 0.00002824
Iteration 210/1000 | Loss: 0.00002824
Iteration 211/1000 | Loss: 0.00002824
Iteration 212/1000 | Loss: 0.00002824
Iteration 213/1000 | Loss: 0.00002824
Iteration 214/1000 | Loss: 0.00002823
Iteration 215/1000 | Loss: 0.00002823
Iteration 216/1000 | Loss: 0.00002823
Iteration 217/1000 | Loss: 0.00002823
Iteration 218/1000 | Loss: 0.00002822
Iteration 219/1000 | Loss: 0.00002822
Iteration 220/1000 | Loss: 0.00002822
Iteration 221/1000 | Loss: 0.00002822
Iteration 222/1000 | Loss: 0.00002821
Iteration 223/1000 | Loss: 0.00002821
Iteration 224/1000 | Loss: 0.00002821
Iteration 225/1000 | Loss: 0.00002821
Iteration 226/1000 | Loss: 0.00002821
Iteration 227/1000 | Loss: 0.00002821
Iteration 228/1000 | Loss: 0.00002821
Iteration 229/1000 | Loss: 0.00002821
Iteration 230/1000 | Loss: 0.00002821
Iteration 231/1000 | Loss: 0.00002821
Iteration 232/1000 | Loss: 0.00002821
Iteration 233/1000 | Loss: 0.00002821
Iteration 234/1000 | Loss: 0.00002821
Iteration 235/1000 | Loss: 0.00002821
Iteration 236/1000 | Loss: 0.00002821
Iteration 237/1000 | Loss: 0.00002821
Iteration 238/1000 | Loss: 0.00002821
Iteration 239/1000 | Loss: 0.00002821
Iteration 240/1000 | Loss: 0.00002821
Iteration 241/1000 | Loss: 0.00002821
Iteration 242/1000 | Loss: 0.00002821
Iteration 243/1000 | Loss: 0.00002821
Iteration 244/1000 | Loss: 0.00002820
Iteration 245/1000 | Loss: 0.00002820
Iteration 246/1000 | Loss: 0.00002820
Iteration 247/1000 | Loss: 0.00002820
Iteration 248/1000 | Loss: 0.00002820
Iteration 249/1000 | Loss: 0.00002820
Iteration 250/1000 | Loss: 0.00002820
Iteration 251/1000 | Loss: 0.00002820
Iteration 252/1000 | Loss: 0.00002820
Iteration 253/1000 | Loss: 0.00002820
Iteration 254/1000 | Loss: 0.00002820
Iteration 255/1000 | Loss: 0.00002820
Iteration 256/1000 | Loss: 0.00002820
Iteration 257/1000 | Loss: 0.00002820
Iteration 258/1000 | Loss: 0.00002820
Iteration 259/1000 | Loss: 0.00002820
Iteration 260/1000 | Loss: 0.00002820
Iteration 261/1000 | Loss: 0.00002820
Iteration 262/1000 | Loss: 0.00002820
Iteration 263/1000 | Loss: 0.00002820
Iteration 264/1000 | Loss: 0.00002820
Iteration 265/1000 | Loss: 0.00002820
Iteration 266/1000 | Loss: 0.00002820
Iteration 267/1000 | Loss: 0.00002820
Iteration 268/1000 | Loss: 0.00002820
Iteration 269/1000 | Loss: 0.00002820
Iteration 270/1000 | Loss: 0.00002820
Iteration 271/1000 | Loss: 0.00002820
Iteration 272/1000 | Loss: 0.00002820
Iteration 273/1000 | Loss: 0.00002820
Iteration 274/1000 | Loss: 0.00002820
Iteration 275/1000 | Loss: 0.00002820
Iteration 276/1000 | Loss: 0.00002820
Iteration 277/1000 | Loss: 0.00002820
Iteration 278/1000 | Loss: 0.00002820
Iteration 279/1000 | Loss: 0.00002820
Iteration 280/1000 | Loss: 0.00002820
Iteration 281/1000 | Loss: 0.00002820
Iteration 282/1000 | Loss: 0.00002820
Iteration 283/1000 | Loss: 0.00002820
Iteration 284/1000 | Loss: 0.00002820
Iteration 285/1000 | Loss: 0.00002820
Iteration 286/1000 | Loss: 0.00002820
Iteration 287/1000 | Loss: 0.00002820
Iteration 288/1000 | Loss: 0.00002820
Iteration 289/1000 | Loss: 0.00002820
Iteration 290/1000 | Loss: 0.00002820
Iteration 291/1000 | Loss: 0.00002820
Iteration 292/1000 | Loss: 0.00002820
Iteration 293/1000 | Loss: 0.00002820
Iteration 294/1000 | Loss: 0.00002820
Iteration 295/1000 | Loss: 0.00002820
Iteration 296/1000 | Loss: 0.00002820
Iteration 297/1000 | Loss: 0.00002820
Iteration 298/1000 | Loss: 0.00002820
Iteration 299/1000 | Loss: 0.00002820
Iteration 300/1000 | Loss: 0.00002820
Iteration 301/1000 | Loss: 0.00002820
Iteration 302/1000 | Loss: 0.00002820
Iteration 303/1000 | Loss: 0.00002820
Iteration 304/1000 | Loss: 0.00002820
Iteration 305/1000 | Loss: 0.00002820
Iteration 306/1000 | Loss: 0.00002820
Iteration 307/1000 | Loss: 0.00002820
Iteration 308/1000 | Loss: 0.00002820
Iteration 309/1000 | Loss: 0.00002820
Iteration 310/1000 | Loss: 0.00002820
Iteration 311/1000 | Loss: 0.00002820
Iteration 312/1000 | Loss: 0.00002820
Iteration 313/1000 | Loss: 0.00002820
Iteration 314/1000 | Loss: 0.00002820
Iteration 315/1000 | Loss: 0.00002820
Iteration 316/1000 | Loss: 0.00002820
Iteration 317/1000 | Loss: 0.00002820
Iteration 318/1000 | Loss: 0.00002820
Iteration 319/1000 | Loss: 0.00002820
Iteration 320/1000 | Loss: 0.00002820
Iteration 321/1000 | Loss: 0.00002820
Iteration 322/1000 | Loss: 0.00002820
Iteration 323/1000 | Loss: 0.00002820
Iteration 324/1000 | Loss: 0.00002820
Iteration 325/1000 | Loss: 0.00002820
Iteration 326/1000 | Loss: 0.00002820
Iteration 327/1000 | Loss: 0.00002820
Iteration 328/1000 | Loss: 0.00002820
Iteration 329/1000 | Loss: 0.00002820
Iteration 330/1000 | Loss: 0.00002820
Iteration 331/1000 | Loss: 0.00002820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [2.8201749955769628e-05, 2.8201749955769628e-05, 2.8201749955769628e-05, 2.8201749955769628e-05, 2.8201749955769628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8201749955769628e-05

Optimization complete. Final v2v error: 4.231534481048584 mm

Highest mean error: 5.759584903717041 mm for frame 176

Lowest mean error: 3.229870319366455 mm for frame 18

Saving results

Total time: 144.18658185005188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063626
Iteration 2/25 | Loss: 0.01063626
Iteration 3/25 | Loss: 0.01063626
Iteration 4/25 | Loss: 0.01063626
Iteration 5/25 | Loss: 0.00504769
Iteration 6/25 | Loss: 0.00278656
Iteration 7/25 | Loss: 0.00216877
Iteration 8/25 | Loss: 0.00206185
Iteration 9/25 | Loss: 0.00205237
Iteration 10/25 | Loss: 0.00184566
Iteration 11/25 | Loss: 0.00177405
Iteration 12/25 | Loss: 0.00173729
Iteration 13/25 | Loss: 0.00172925
Iteration 14/25 | Loss: 0.00172515
Iteration 15/25 | Loss: 0.00172225
Iteration 16/25 | Loss: 0.00171210
Iteration 17/25 | Loss: 0.00170733
Iteration 18/25 | Loss: 0.00170562
Iteration 19/25 | Loss: 0.00170311
Iteration 20/25 | Loss: 0.00170928
Iteration 21/25 | Loss: 0.00170454
Iteration 22/25 | Loss: 0.00170366
Iteration 23/25 | Loss: 0.00170486
Iteration 24/25 | Loss: 0.00170657
Iteration 25/25 | Loss: 0.00170134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.57364857
Iteration 2/25 | Loss: 0.00338309
Iteration 3/25 | Loss: 0.00264364
Iteration 4/25 | Loss: 0.00264364
Iteration 5/25 | Loss: 0.00264364
Iteration 6/25 | Loss: 0.00264364
Iteration 7/25 | Loss: 0.00264364
Iteration 8/25 | Loss: 0.00264364
Iteration 9/25 | Loss: 0.00264363
Iteration 10/25 | Loss: 0.00264363
Iteration 11/25 | Loss: 0.00264363
Iteration 12/25 | Loss: 0.00264363
Iteration 13/25 | Loss: 0.00264363
Iteration 14/25 | Loss: 0.00264363
Iteration 15/25 | Loss: 0.00264363
Iteration 16/25 | Loss: 0.00264363
Iteration 17/25 | Loss: 0.00264363
Iteration 18/25 | Loss: 0.00264363
Iteration 19/25 | Loss: 0.00264363
Iteration 20/25 | Loss: 0.00264363
Iteration 21/25 | Loss: 0.00264363
Iteration 22/25 | Loss: 0.00264363
Iteration 23/25 | Loss: 0.00264363
Iteration 24/25 | Loss: 0.00264363
Iteration 25/25 | Loss: 0.00264363
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002643633633852005, 0.002643633633852005, 0.002643633633852005, 0.002643633633852005, 0.002643633633852005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002643633633852005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00264363
Iteration 2/1000 | Loss: 0.00038811
Iteration 3/1000 | Loss: 0.00069763
Iteration 4/1000 | Loss: 0.00049272
Iteration 5/1000 | Loss: 0.00026133
Iteration 6/1000 | Loss: 0.00204719
Iteration 7/1000 | Loss: 0.00029658
Iteration 8/1000 | Loss: 0.00026909
Iteration 9/1000 | Loss: 0.00037942
Iteration 10/1000 | Loss: 0.00023037
Iteration 11/1000 | Loss: 0.00019755
Iteration 12/1000 | Loss: 0.00039829
Iteration 13/1000 | Loss: 0.00039314
Iteration 14/1000 | Loss: 0.00078191
Iteration 15/1000 | Loss: 0.00032766
Iteration 16/1000 | Loss: 0.00025216
Iteration 17/1000 | Loss: 0.00017326
Iteration 18/1000 | Loss: 0.00067668
Iteration 19/1000 | Loss: 0.00605079
Iteration 20/1000 | Loss: 0.00205828
Iteration 21/1000 | Loss: 0.00068706
Iteration 22/1000 | Loss: 0.00041468
Iteration 23/1000 | Loss: 0.00058693
Iteration 24/1000 | Loss: 0.00026753
Iteration 25/1000 | Loss: 0.00026876
Iteration 26/1000 | Loss: 0.00019978
Iteration 27/1000 | Loss: 0.00022732
Iteration 28/1000 | Loss: 0.00008385
Iteration 29/1000 | Loss: 0.00009085
Iteration 30/1000 | Loss: 0.00012634
Iteration 31/1000 | Loss: 0.00013920
Iteration 32/1000 | Loss: 0.00019193
Iteration 33/1000 | Loss: 0.00016685
Iteration 34/1000 | Loss: 0.00006770
Iteration 35/1000 | Loss: 0.00006124
Iteration 36/1000 | Loss: 0.00005754
Iteration 37/1000 | Loss: 0.00004697
Iteration 38/1000 | Loss: 0.00014907
Iteration 39/1000 | Loss: 0.00029788
Iteration 40/1000 | Loss: 0.00004145
Iteration 41/1000 | Loss: 0.00006077
Iteration 42/1000 | Loss: 0.00023981
Iteration 43/1000 | Loss: 0.00030176
Iteration 44/1000 | Loss: 0.00003805
Iteration 45/1000 | Loss: 0.00006719
Iteration 46/1000 | Loss: 0.00005353
Iteration 47/1000 | Loss: 0.00006389
Iteration 48/1000 | Loss: 0.00009969
Iteration 49/1000 | Loss: 0.00005075
Iteration 50/1000 | Loss: 0.00004916
Iteration 51/1000 | Loss: 0.00004576
Iteration 52/1000 | Loss: 0.00008778
Iteration 53/1000 | Loss: 0.00009527
Iteration 54/1000 | Loss: 0.00008327
Iteration 55/1000 | Loss: 0.00003598
Iteration 56/1000 | Loss: 0.00002891
Iteration 57/1000 | Loss: 0.00002843
Iteration 58/1000 | Loss: 0.00007152
Iteration 59/1000 | Loss: 0.00003276
Iteration 60/1000 | Loss: 0.00002874
Iteration 61/1000 | Loss: 0.00002742
Iteration 62/1000 | Loss: 0.00002736
Iteration 63/1000 | Loss: 0.00002736
Iteration 64/1000 | Loss: 0.00002736
Iteration 65/1000 | Loss: 0.00002735
Iteration 66/1000 | Loss: 0.00002927
Iteration 67/1000 | Loss: 0.00003901
Iteration 68/1000 | Loss: 0.00002792
Iteration 69/1000 | Loss: 0.00002791
Iteration 70/1000 | Loss: 0.00004910
Iteration 71/1000 | Loss: 0.00002803
Iteration 72/1000 | Loss: 0.00002764
Iteration 73/1000 | Loss: 0.00002674
Iteration 74/1000 | Loss: 0.00002667
Iteration 75/1000 | Loss: 0.00002667
Iteration 76/1000 | Loss: 0.00002667
Iteration 77/1000 | Loss: 0.00002667
Iteration 78/1000 | Loss: 0.00002664
Iteration 79/1000 | Loss: 0.00002664
Iteration 80/1000 | Loss: 0.00002664
Iteration 81/1000 | Loss: 0.00002663
Iteration 82/1000 | Loss: 0.00002906
Iteration 83/1000 | Loss: 0.00002650
Iteration 84/1000 | Loss: 0.00002648
Iteration 85/1000 | Loss: 0.00002647
Iteration 86/1000 | Loss: 0.00002674
Iteration 87/1000 | Loss: 0.00002674
Iteration 88/1000 | Loss: 0.00003547
Iteration 89/1000 | Loss: 0.00002710
Iteration 90/1000 | Loss: 0.00002783
Iteration 91/1000 | Loss: 0.00002783
Iteration 92/1000 | Loss: 0.00002783
Iteration 93/1000 | Loss: 0.00002783
Iteration 94/1000 | Loss: 0.00002652
Iteration 95/1000 | Loss: 0.00002635
Iteration 96/1000 | Loss: 0.00002635
Iteration 97/1000 | Loss: 0.00002633
Iteration 98/1000 | Loss: 0.00003072
Iteration 99/1000 | Loss: 0.00002636
Iteration 100/1000 | Loss: 0.00002636
Iteration 101/1000 | Loss: 0.00002636
Iteration 102/1000 | Loss: 0.00002635
Iteration 103/1000 | Loss: 0.00002640
Iteration 104/1000 | Loss: 0.00002640
Iteration 105/1000 | Loss: 0.00002629
Iteration 106/1000 | Loss: 0.00002629
Iteration 107/1000 | Loss: 0.00002629
Iteration 108/1000 | Loss: 0.00002629
Iteration 109/1000 | Loss: 0.00002629
Iteration 110/1000 | Loss: 0.00002629
Iteration 111/1000 | Loss: 0.00002629
Iteration 112/1000 | Loss: 0.00002629
Iteration 113/1000 | Loss: 0.00002629
Iteration 114/1000 | Loss: 0.00002629
Iteration 115/1000 | Loss: 0.00002629
Iteration 116/1000 | Loss: 0.00002629
Iteration 117/1000 | Loss: 0.00002629
Iteration 118/1000 | Loss: 0.00002629
Iteration 119/1000 | Loss: 0.00002629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.6286790671292692e-05, 2.6286790671292692e-05, 2.6286790671292692e-05, 2.6286790671292692e-05, 2.6286790671292692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6286790671292692e-05

Optimization complete. Final v2v error: 4.220211982727051 mm

Highest mean error: 9.560832023620605 mm for frame 34

Lowest mean error: 3.9220120906829834 mm for frame 122

Saving results

Total time: 149.63061499595642
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950574
Iteration 2/25 | Loss: 0.00950573
Iteration 3/25 | Loss: 0.00410376
Iteration 4/25 | Loss: 0.00284608
Iteration 5/25 | Loss: 0.00237622
Iteration 6/25 | Loss: 0.00221914
Iteration 7/25 | Loss: 0.00223603
Iteration 8/25 | Loss: 0.00210027
Iteration 9/25 | Loss: 0.00191825
Iteration 10/25 | Loss: 0.00181678
Iteration 11/25 | Loss: 0.00183762
Iteration 12/25 | Loss: 0.00174400
Iteration 13/25 | Loss: 0.00169408
Iteration 14/25 | Loss: 0.00167036
Iteration 15/25 | Loss: 0.00165829
Iteration 16/25 | Loss: 0.00165569
Iteration 17/25 | Loss: 0.00164495
Iteration 18/25 | Loss: 0.00163310
Iteration 19/25 | Loss: 0.00162827
Iteration 20/25 | Loss: 0.00162728
Iteration 21/25 | Loss: 0.00162640
Iteration 22/25 | Loss: 0.00162594
Iteration 23/25 | Loss: 0.00162552
Iteration 24/25 | Loss: 0.00162524
Iteration 25/25 | Loss: 0.00162512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42516673
Iteration 2/25 | Loss: 0.00244276
Iteration 3/25 | Loss: 0.00244274
Iteration 4/25 | Loss: 0.00244274
Iteration 5/25 | Loss: 0.00244274
Iteration 6/25 | Loss: 0.00244274
Iteration 7/25 | Loss: 0.00244274
Iteration 8/25 | Loss: 0.00244274
Iteration 9/25 | Loss: 0.00244274
Iteration 10/25 | Loss: 0.00244274
Iteration 11/25 | Loss: 0.00244274
Iteration 12/25 | Loss: 0.00244274
Iteration 13/25 | Loss: 0.00244274
Iteration 14/25 | Loss: 0.00244274
Iteration 15/25 | Loss: 0.00244274
Iteration 16/25 | Loss: 0.00244274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0024427380412817, 0.0024427380412817, 0.0024427380412817, 0.0024427380412817, 0.0024427380412817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024427380412817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244274
Iteration 2/1000 | Loss: 0.00198947
Iteration 3/1000 | Loss: 0.00095038
Iteration 4/1000 | Loss: 0.00046826
Iteration 5/1000 | Loss: 0.00027583
Iteration 6/1000 | Loss: 0.00025225
Iteration 7/1000 | Loss: 0.00036674
Iteration 8/1000 | Loss: 0.00022536
Iteration 9/1000 | Loss: 0.00041228
Iteration 10/1000 | Loss: 0.00019870
Iteration 11/1000 | Loss: 0.00122575
Iteration 12/1000 | Loss: 0.00555512
Iteration 13/1000 | Loss: 0.00366771
Iteration 14/1000 | Loss: 0.00046160
Iteration 15/1000 | Loss: 0.00066714
Iteration 16/1000 | Loss: 0.00020664
Iteration 17/1000 | Loss: 0.00037515
Iteration 18/1000 | Loss: 0.00012988
Iteration 19/1000 | Loss: 0.00011935
Iteration 20/1000 | Loss: 0.00031791
Iteration 21/1000 | Loss: 0.00011971
Iteration 22/1000 | Loss: 0.00006742
Iteration 23/1000 | Loss: 0.00005349
Iteration 24/1000 | Loss: 0.00005842
Iteration 25/1000 | Loss: 0.00005166
Iteration 26/1000 | Loss: 0.00005045
Iteration 27/1000 | Loss: 0.00003469
Iteration 28/1000 | Loss: 0.00003563
Iteration 29/1000 | Loss: 0.00003953
Iteration 30/1000 | Loss: 0.00003591
Iteration 31/1000 | Loss: 0.00004206
Iteration 32/1000 | Loss: 0.00003594
Iteration 33/1000 | Loss: 0.00004036
Iteration 34/1000 | Loss: 0.00005091
Iteration 35/1000 | Loss: 0.00002807
Iteration 36/1000 | Loss: 0.00002243
Iteration 37/1000 | Loss: 0.00002113
Iteration 38/1000 | Loss: 0.00002025
Iteration 39/1000 | Loss: 0.00001943
Iteration 40/1000 | Loss: 0.00001892
Iteration 41/1000 | Loss: 0.00001850
Iteration 42/1000 | Loss: 0.00001813
Iteration 43/1000 | Loss: 0.00001787
Iteration 44/1000 | Loss: 0.00001771
Iteration 45/1000 | Loss: 0.00001768
Iteration 46/1000 | Loss: 0.00001764
Iteration 47/1000 | Loss: 0.00001748
Iteration 48/1000 | Loss: 0.00001743
Iteration 49/1000 | Loss: 0.00001743
Iteration 50/1000 | Loss: 0.00001742
Iteration 51/1000 | Loss: 0.00001742
Iteration 52/1000 | Loss: 0.00001742
Iteration 53/1000 | Loss: 0.00001742
Iteration 54/1000 | Loss: 0.00001741
Iteration 55/1000 | Loss: 0.00001741
Iteration 56/1000 | Loss: 0.00001741
Iteration 57/1000 | Loss: 0.00001741
Iteration 58/1000 | Loss: 0.00001741
Iteration 59/1000 | Loss: 0.00001741
Iteration 60/1000 | Loss: 0.00001741
Iteration 61/1000 | Loss: 0.00001741
Iteration 62/1000 | Loss: 0.00001741
Iteration 63/1000 | Loss: 0.00001740
Iteration 64/1000 | Loss: 0.00001740
Iteration 65/1000 | Loss: 0.00001740
Iteration 66/1000 | Loss: 0.00001739
Iteration 67/1000 | Loss: 0.00001739
Iteration 68/1000 | Loss: 0.00001739
Iteration 69/1000 | Loss: 0.00001738
Iteration 70/1000 | Loss: 0.00001738
Iteration 71/1000 | Loss: 0.00001738
Iteration 72/1000 | Loss: 0.00001738
Iteration 73/1000 | Loss: 0.00001738
Iteration 74/1000 | Loss: 0.00001738
Iteration 75/1000 | Loss: 0.00001738
Iteration 76/1000 | Loss: 0.00001738
Iteration 77/1000 | Loss: 0.00001738
Iteration 78/1000 | Loss: 0.00001738
Iteration 79/1000 | Loss: 0.00001738
Iteration 80/1000 | Loss: 0.00001737
Iteration 81/1000 | Loss: 0.00001737
Iteration 82/1000 | Loss: 0.00001737
Iteration 83/1000 | Loss: 0.00001737
Iteration 84/1000 | Loss: 0.00001737
Iteration 85/1000 | Loss: 0.00001737
Iteration 86/1000 | Loss: 0.00001737
Iteration 87/1000 | Loss: 0.00001737
Iteration 88/1000 | Loss: 0.00001736
Iteration 89/1000 | Loss: 0.00001736
Iteration 90/1000 | Loss: 0.00001736
Iteration 91/1000 | Loss: 0.00001736
Iteration 92/1000 | Loss: 0.00001736
Iteration 93/1000 | Loss: 0.00001736
Iteration 94/1000 | Loss: 0.00001736
Iteration 95/1000 | Loss: 0.00001736
Iteration 96/1000 | Loss: 0.00001736
Iteration 97/1000 | Loss: 0.00001736
Iteration 98/1000 | Loss: 0.00001736
Iteration 99/1000 | Loss: 0.00001736
Iteration 100/1000 | Loss: 0.00001735
Iteration 101/1000 | Loss: 0.00001735
Iteration 102/1000 | Loss: 0.00001735
Iteration 103/1000 | Loss: 0.00001734
Iteration 104/1000 | Loss: 0.00001734
Iteration 105/1000 | Loss: 0.00001734
Iteration 106/1000 | Loss: 0.00001734
Iteration 107/1000 | Loss: 0.00001734
Iteration 108/1000 | Loss: 0.00001733
Iteration 109/1000 | Loss: 0.00001733
Iteration 110/1000 | Loss: 0.00001733
Iteration 111/1000 | Loss: 0.00001733
Iteration 112/1000 | Loss: 0.00001733
Iteration 113/1000 | Loss: 0.00001733
Iteration 114/1000 | Loss: 0.00001733
Iteration 115/1000 | Loss: 0.00001733
Iteration 116/1000 | Loss: 0.00001732
Iteration 117/1000 | Loss: 0.00001732
Iteration 118/1000 | Loss: 0.00001732
Iteration 119/1000 | Loss: 0.00001732
Iteration 120/1000 | Loss: 0.00001732
Iteration 121/1000 | Loss: 0.00001732
Iteration 122/1000 | Loss: 0.00001732
Iteration 123/1000 | Loss: 0.00001732
Iteration 124/1000 | Loss: 0.00001732
Iteration 125/1000 | Loss: 0.00001732
Iteration 126/1000 | Loss: 0.00001732
Iteration 127/1000 | Loss: 0.00001732
Iteration 128/1000 | Loss: 0.00001732
Iteration 129/1000 | Loss: 0.00001732
Iteration 130/1000 | Loss: 0.00001731
Iteration 131/1000 | Loss: 0.00001731
Iteration 132/1000 | Loss: 0.00001731
Iteration 133/1000 | Loss: 0.00001731
Iteration 134/1000 | Loss: 0.00001731
Iteration 135/1000 | Loss: 0.00001731
Iteration 136/1000 | Loss: 0.00001731
Iteration 137/1000 | Loss: 0.00001731
Iteration 138/1000 | Loss: 0.00001731
Iteration 139/1000 | Loss: 0.00001730
Iteration 140/1000 | Loss: 0.00009598
Iteration 141/1000 | Loss: 0.00089826
Iteration 142/1000 | Loss: 0.00004987
Iteration 143/1000 | Loss: 0.00020148
Iteration 144/1000 | Loss: 0.00009195
Iteration 145/1000 | Loss: 0.00007118
Iteration 146/1000 | Loss: 0.00002312
Iteration 147/1000 | Loss: 0.00001863
Iteration 148/1000 | Loss: 0.00001788
Iteration 149/1000 | Loss: 0.00001748
Iteration 150/1000 | Loss: 0.00001729
Iteration 151/1000 | Loss: 0.00001728
Iteration 152/1000 | Loss: 0.00001727
Iteration 153/1000 | Loss: 0.00001726
Iteration 154/1000 | Loss: 0.00001726
Iteration 155/1000 | Loss: 0.00001726
Iteration 156/1000 | Loss: 0.00001726
Iteration 157/1000 | Loss: 0.00001726
Iteration 158/1000 | Loss: 0.00001726
Iteration 159/1000 | Loss: 0.00001726
Iteration 160/1000 | Loss: 0.00001726
Iteration 161/1000 | Loss: 0.00001726
Iteration 162/1000 | Loss: 0.00001726
Iteration 163/1000 | Loss: 0.00001725
Iteration 164/1000 | Loss: 0.00001725
Iteration 165/1000 | Loss: 0.00001725
Iteration 166/1000 | Loss: 0.00001725
Iteration 167/1000 | Loss: 0.00001724
Iteration 168/1000 | Loss: 0.00001724
Iteration 169/1000 | Loss: 0.00001724
Iteration 170/1000 | Loss: 0.00001724
Iteration 171/1000 | Loss: 0.00001724
Iteration 172/1000 | Loss: 0.00001724
Iteration 173/1000 | Loss: 0.00001724
Iteration 174/1000 | Loss: 0.00001724
Iteration 175/1000 | Loss: 0.00001723
Iteration 176/1000 | Loss: 0.00001723
Iteration 177/1000 | Loss: 0.00001723
Iteration 178/1000 | Loss: 0.00001723
Iteration 179/1000 | Loss: 0.00001723
Iteration 180/1000 | Loss: 0.00001723
Iteration 181/1000 | Loss: 0.00001723
Iteration 182/1000 | Loss: 0.00001723
Iteration 183/1000 | Loss: 0.00001723
Iteration 184/1000 | Loss: 0.00001723
Iteration 185/1000 | Loss: 0.00001723
Iteration 186/1000 | Loss: 0.00001723
Iteration 187/1000 | Loss: 0.00001723
Iteration 188/1000 | Loss: 0.00001723
Iteration 189/1000 | Loss: 0.00001722
Iteration 190/1000 | Loss: 0.00001722
Iteration 191/1000 | Loss: 0.00001722
Iteration 192/1000 | Loss: 0.00001722
Iteration 193/1000 | Loss: 0.00001722
Iteration 194/1000 | Loss: 0.00001722
Iteration 195/1000 | Loss: 0.00001722
Iteration 196/1000 | Loss: 0.00001722
Iteration 197/1000 | Loss: 0.00001722
Iteration 198/1000 | Loss: 0.00001722
Iteration 199/1000 | Loss: 0.00001722
Iteration 200/1000 | Loss: 0.00001722
Iteration 201/1000 | Loss: 0.00001722
Iteration 202/1000 | Loss: 0.00001722
Iteration 203/1000 | Loss: 0.00001722
Iteration 204/1000 | Loss: 0.00001722
Iteration 205/1000 | Loss: 0.00001721
Iteration 206/1000 | Loss: 0.00001721
Iteration 207/1000 | Loss: 0.00001721
Iteration 208/1000 | Loss: 0.00001721
Iteration 209/1000 | Loss: 0.00001721
Iteration 210/1000 | Loss: 0.00001721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.7214957551914267e-05, 1.7214957551914267e-05, 1.7214957551914267e-05, 1.7214957551914267e-05, 1.7214957551914267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7214957551914267e-05

Optimization complete. Final v2v error: 3.4991543292999268 mm

Highest mean error: 5.977541446685791 mm for frame 209

Lowest mean error: 3.3211073875427246 mm for frame 19

Saving results

Total time: 146.0740315914154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402138
Iteration 2/25 | Loss: 0.00131051
Iteration 3/25 | Loss: 0.00120644
Iteration 4/25 | Loss: 0.00119620
Iteration 5/25 | Loss: 0.00119416
Iteration 6/25 | Loss: 0.00119411
Iteration 7/25 | Loss: 0.00119411
Iteration 8/25 | Loss: 0.00119411
Iteration 9/25 | Loss: 0.00119411
Iteration 10/25 | Loss: 0.00119411
Iteration 11/25 | Loss: 0.00119411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001194108510389924, 0.001194108510389924, 0.001194108510389924, 0.001194108510389924, 0.001194108510389924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001194108510389924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42822564
Iteration 2/25 | Loss: 0.00062454
Iteration 3/25 | Loss: 0.00062453
Iteration 4/25 | Loss: 0.00062453
Iteration 5/25 | Loss: 0.00062453
Iteration 6/25 | Loss: 0.00062453
Iteration 7/25 | Loss: 0.00062453
Iteration 8/25 | Loss: 0.00062453
Iteration 9/25 | Loss: 0.00062453
Iteration 10/25 | Loss: 0.00062453
Iteration 11/25 | Loss: 0.00062453
Iteration 12/25 | Loss: 0.00062453
Iteration 13/25 | Loss: 0.00062453
Iteration 14/25 | Loss: 0.00062453
Iteration 15/25 | Loss: 0.00062453
Iteration 16/25 | Loss: 0.00062453
Iteration 17/25 | Loss: 0.00062453
Iteration 18/25 | Loss: 0.00062453
Iteration 19/25 | Loss: 0.00062453
Iteration 20/25 | Loss: 0.00062453
Iteration 21/25 | Loss: 0.00062453
Iteration 22/25 | Loss: 0.00062453
Iteration 23/25 | Loss: 0.00062453
Iteration 24/25 | Loss: 0.00062453
Iteration 25/25 | Loss: 0.00062453

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062453
Iteration 2/1000 | Loss: 0.00002632
Iteration 3/1000 | Loss: 0.00001892
Iteration 4/1000 | Loss: 0.00001717
Iteration 5/1000 | Loss: 0.00001594
Iteration 6/1000 | Loss: 0.00001528
Iteration 7/1000 | Loss: 0.00001468
Iteration 8/1000 | Loss: 0.00001442
Iteration 9/1000 | Loss: 0.00001413
Iteration 10/1000 | Loss: 0.00001391
Iteration 11/1000 | Loss: 0.00001391
Iteration 12/1000 | Loss: 0.00001382
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001381
Iteration 15/1000 | Loss: 0.00001378
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001350
Iteration 18/1000 | Loss: 0.00001349
Iteration 19/1000 | Loss: 0.00001348
Iteration 20/1000 | Loss: 0.00001347
Iteration 21/1000 | Loss: 0.00001347
Iteration 22/1000 | Loss: 0.00001346
Iteration 23/1000 | Loss: 0.00001343
Iteration 24/1000 | Loss: 0.00001342
Iteration 25/1000 | Loss: 0.00001342
Iteration 26/1000 | Loss: 0.00001341
Iteration 27/1000 | Loss: 0.00001339
Iteration 28/1000 | Loss: 0.00001339
Iteration 29/1000 | Loss: 0.00001338
Iteration 30/1000 | Loss: 0.00001337
Iteration 31/1000 | Loss: 0.00001336
Iteration 32/1000 | Loss: 0.00001336
Iteration 33/1000 | Loss: 0.00001335
Iteration 34/1000 | Loss: 0.00001334
Iteration 35/1000 | Loss: 0.00001331
Iteration 36/1000 | Loss: 0.00001330
Iteration 37/1000 | Loss: 0.00001329
Iteration 38/1000 | Loss: 0.00001329
Iteration 39/1000 | Loss: 0.00001328
Iteration 40/1000 | Loss: 0.00001328
Iteration 41/1000 | Loss: 0.00001327
Iteration 42/1000 | Loss: 0.00001323
Iteration 43/1000 | Loss: 0.00001317
Iteration 44/1000 | Loss: 0.00001316
Iteration 45/1000 | Loss: 0.00001315
Iteration 46/1000 | Loss: 0.00001313
Iteration 47/1000 | Loss: 0.00001311
Iteration 48/1000 | Loss: 0.00001310
Iteration 49/1000 | Loss: 0.00001309
Iteration 50/1000 | Loss: 0.00001309
Iteration 51/1000 | Loss: 0.00001309
Iteration 52/1000 | Loss: 0.00001304
Iteration 53/1000 | Loss: 0.00001303
Iteration 54/1000 | Loss: 0.00001302
Iteration 55/1000 | Loss: 0.00001302
Iteration 56/1000 | Loss: 0.00001301
Iteration 57/1000 | Loss: 0.00001301
Iteration 58/1000 | Loss: 0.00001301
Iteration 59/1000 | Loss: 0.00001301
Iteration 60/1000 | Loss: 0.00001301
Iteration 61/1000 | Loss: 0.00001300
Iteration 62/1000 | Loss: 0.00001300
Iteration 63/1000 | Loss: 0.00001299
Iteration 64/1000 | Loss: 0.00001299
Iteration 65/1000 | Loss: 0.00001298
Iteration 66/1000 | Loss: 0.00001298
Iteration 67/1000 | Loss: 0.00001298
Iteration 68/1000 | Loss: 0.00001297
Iteration 69/1000 | Loss: 0.00001296
Iteration 70/1000 | Loss: 0.00001296
Iteration 71/1000 | Loss: 0.00001296
Iteration 72/1000 | Loss: 0.00001296
Iteration 73/1000 | Loss: 0.00001296
Iteration 74/1000 | Loss: 0.00001295
Iteration 75/1000 | Loss: 0.00001295
Iteration 76/1000 | Loss: 0.00001295
Iteration 77/1000 | Loss: 0.00001294
Iteration 78/1000 | Loss: 0.00001294
Iteration 79/1000 | Loss: 0.00001294
Iteration 80/1000 | Loss: 0.00001294
Iteration 81/1000 | Loss: 0.00001294
Iteration 82/1000 | Loss: 0.00001294
Iteration 83/1000 | Loss: 0.00001293
Iteration 84/1000 | Loss: 0.00001293
Iteration 85/1000 | Loss: 0.00001293
Iteration 86/1000 | Loss: 0.00001292
Iteration 87/1000 | Loss: 0.00001292
Iteration 88/1000 | Loss: 0.00001292
Iteration 89/1000 | Loss: 0.00001292
Iteration 90/1000 | Loss: 0.00001292
Iteration 91/1000 | Loss: 0.00001292
Iteration 92/1000 | Loss: 0.00001292
Iteration 93/1000 | Loss: 0.00001292
Iteration 94/1000 | Loss: 0.00001292
Iteration 95/1000 | Loss: 0.00001291
Iteration 96/1000 | Loss: 0.00001291
Iteration 97/1000 | Loss: 0.00001291
Iteration 98/1000 | Loss: 0.00001291
Iteration 99/1000 | Loss: 0.00001291
Iteration 100/1000 | Loss: 0.00001291
Iteration 101/1000 | Loss: 0.00001291
Iteration 102/1000 | Loss: 0.00001290
Iteration 103/1000 | Loss: 0.00001290
Iteration 104/1000 | Loss: 0.00001290
Iteration 105/1000 | Loss: 0.00001290
Iteration 106/1000 | Loss: 0.00001290
Iteration 107/1000 | Loss: 0.00001290
Iteration 108/1000 | Loss: 0.00001289
Iteration 109/1000 | Loss: 0.00001289
Iteration 110/1000 | Loss: 0.00001289
Iteration 111/1000 | Loss: 0.00001289
Iteration 112/1000 | Loss: 0.00001289
Iteration 113/1000 | Loss: 0.00001289
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001289
Iteration 116/1000 | Loss: 0.00001288
Iteration 117/1000 | Loss: 0.00001288
Iteration 118/1000 | Loss: 0.00001288
Iteration 119/1000 | Loss: 0.00001288
Iteration 120/1000 | Loss: 0.00001288
Iteration 121/1000 | Loss: 0.00001288
Iteration 122/1000 | Loss: 0.00001288
Iteration 123/1000 | Loss: 0.00001287
Iteration 124/1000 | Loss: 0.00001287
Iteration 125/1000 | Loss: 0.00001287
Iteration 126/1000 | Loss: 0.00001287
Iteration 127/1000 | Loss: 0.00001287
Iteration 128/1000 | Loss: 0.00001287
Iteration 129/1000 | Loss: 0.00001287
Iteration 130/1000 | Loss: 0.00001287
Iteration 131/1000 | Loss: 0.00001287
Iteration 132/1000 | Loss: 0.00001287
Iteration 133/1000 | Loss: 0.00001287
Iteration 134/1000 | Loss: 0.00001287
Iteration 135/1000 | Loss: 0.00001287
Iteration 136/1000 | Loss: 0.00001287
Iteration 137/1000 | Loss: 0.00001287
Iteration 138/1000 | Loss: 0.00001286
Iteration 139/1000 | Loss: 0.00001286
Iteration 140/1000 | Loss: 0.00001286
Iteration 141/1000 | Loss: 0.00001286
Iteration 142/1000 | Loss: 0.00001285
Iteration 143/1000 | Loss: 0.00001285
Iteration 144/1000 | Loss: 0.00001285
Iteration 145/1000 | Loss: 0.00001285
Iteration 146/1000 | Loss: 0.00001284
Iteration 147/1000 | Loss: 0.00001284
Iteration 148/1000 | Loss: 0.00001284
Iteration 149/1000 | Loss: 0.00001284
Iteration 150/1000 | Loss: 0.00001284
Iteration 151/1000 | Loss: 0.00001284
Iteration 152/1000 | Loss: 0.00001284
Iteration 153/1000 | Loss: 0.00001284
Iteration 154/1000 | Loss: 0.00001284
Iteration 155/1000 | Loss: 0.00001284
Iteration 156/1000 | Loss: 0.00001284
Iteration 157/1000 | Loss: 0.00001284
Iteration 158/1000 | Loss: 0.00001284
Iteration 159/1000 | Loss: 0.00001284
Iteration 160/1000 | Loss: 0.00001284
Iteration 161/1000 | Loss: 0.00001284
Iteration 162/1000 | Loss: 0.00001284
Iteration 163/1000 | Loss: 0.00001283
Iteration 164/1000 | Loss: 0.00001283
Iteration 165/1000 | Loss: 0.00001283
Iteration 166/1000 | Loss: 0.00001283
Iteration 167/1000 | Loss: 0.00001283
Iteration 168/1000 | Loss: 0.00001283
Iteration 169/1000 | Loss: 0.00001283
Iteration 170/1000 | Loss: 0.00001283
Iteration 171/1000 | Loss: 0.00001283
Iteration 172/1000 | Loss: 0.00001283
Iteration 173/1000 | Loss: 0.00001283
Iteration 174/1000 | Loss: 0.00001283
Iteration 175/1000 | Loss: 0.00001283
Iteration 176/1000 | Loss: 0.00001283
Iteration 177/1000 | Loss: 0.00001283
Iteration 178/1000 | Loss: 0.00001283
Iteration 179/1000 | Loss: 0.00001283
Iteration 180/1000 | Loss: 0.00001283
Iteration 181/1000 | Loss: 0.00001283
Iteration 182/1000 | Loss: 0.00001283
Iteration 183/1000 | Loss: 0.00001283
Iteration 184/1000 | Loss: 0.00001283
Iteration 185/1000 | Loss: 0.00001283
Iteration 186/1000 | Loss: 0.00001283
Iteration 187/1000 | Loss: 0.00001283
Iteration 188/1000 | Loss: 0.00001283
Iteration 189/1000 | Loss: 0.00001283
Iteration 190/1000 | Loss: 0.00001283
Iteration 191/1000 | Loss: 0.00001283
Iteration 192/1000 | Loss: 0.00001283
Iteration 193/1000 | Loss: 0.00001283
Iteration 194/1000 | Loss: 0.00001283
Iteration 195/1000 | Loss: 0.00001283
Iteration 196/1000 | Loss: 0.00001283
Iteration 197/1000 | Loss: 0.00001283
Iteration 198/1000 | Loss: 0.00001283
Iteration 199/1000 | Loss: 0.00001283
Iteration 200/1000 | Loss: 0.00001283
Iteration 201/1000 | Loss: 0.00001283
Iteration 202/1000 | Loss: 0.00001283
Iteration 203/1000 | Loss: 0.00001283
Iteration 204/1000 | Loss: 0.00001283
Iteration 205/1000 | Loss: 0.00001283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.2830209016101435e-05, 1.2830209016101435e-05, 1.2830209016101435e-05, 1.2830209016101435e-05, 1.2830209016101435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2830209016101435e-05

Optimization complete. Final v2v error: 3.0726945400238037 mm

Highest mean error: 3.391528367996216 mm for frame 76

Lowest mean error: 2.869046926498413 mm for frame 19

Saving results

Total time: 39.55259847640991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00756249
Iteration 2/25 | Loss: 0.00196337
Iteration 3/25 | Loss: 0.00150433
Iteration 4/25 | Loss: 0.00142658
Iteration 5/25 | Loss: 0.00139430
Iteration 6/25 | Loss: 0.00140774
Iteration 7/25 | Loss: 0.00140535
Iteration 8/25 | Loss: 0.00139889
Iteration 9/25 | Loss: 0.00139283
Iteration 10/25 | Loss: 0.00137994
Iteration 11/25 | Loss: 0.00135748
Iteration 12/25 | Loss: 0.00134167
Iteration 13/25 | Loss: 0.00134027
Iteration 14/25 | Loss: 0.00134262
Iteration 15/25 | Loss: 0.00133620
Iteration 16/25 | Loss: 0.00134110
Iteration 17/25 | Loss: 0.00133624
Iteration 18/25 | Loss: 0.00133952
Iteration 19/25 | Loss: 0.00133851
Iteration 20/25 | Loss: 0.00133646
Iteration 21/25 | Loss: 0.00133320
Iteration 22/25 | Loss: 0.00133303
Iteration 23/25 | Loss: 0.00133299
Iteration 24/25 | Loss: 0.00133299
Iteration 25/25 | Loss: 0.00133299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.23983717
Iteration 2/25 | Loss: 0.00080766
Iteration 3/25 | Loss: 0.00080759
Iteration 4/25 | Loss: 0.00080759
Iteration 5/25 | Loss: 0.00080759
Iteration 6/25 | Loss: 0.00080759
Iteration 7/25 | Loss: 0.00080759
Iteration 8/25 | Loss: 0.00080759
Iteration 9/25 | Loss: 0.00080759
Iteration 10/25 | Loss: 0.00080759
Iteration 11/25 | Loss: 0.00080759
Iteration 12/25 | Loss: 0.00080759
Iteration 13/25 | Loss: 0.00080759
Iteration 14/25 | Loss: 0.00080759
Iteration 15/25 | Loss: 0.00080759
Iteration 16/25 | Loss: 0.00080759
Iteration 17/25 | Loss: 0.00080759
Iteration 18/25 | Loss: 0.00080759
Iteration 19/25 | Loss: 0.00080759
Iteration 20/25 | Loss: 0.00080759
Iteration 21/25 | Loss: 0.00080759
Iteration 22/25 | Loss: 0.00080759
Iteration 23/25 | Loss: 0.00080759
Iteration 24/25 | Loss: 0.00080759
Iteration 25/25 | Loss: 0.00080759
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008075873483903706, 0.0008075873483903706, 0.0008075873483903706, 0.0008075873483903706, 0.0008075873483903706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008075873483903706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080759
Iteration 2/1000 | Loss: 0.00004983
Iteration 3/1000 | Loss: 0.00002469
Iteration 4/1000 | Loss: 0.00004206
Iteration 5/1000 | Loss: 0.00003114
Iteration 6/1000 | Loss: 0.00002315
Iteration 7/1000 | Loss: 0.00002041
Iteration 8/1000 | Loss: 0.00002013
Iteration 9/1000 | Loss: 0.00001983
Iteration 10/1000 | Loss: 0.00001962
Iteration 11/1000 | Loss: 0.00001955
Iteration 12/1000 | Loss: 0.00001953
Iteration 13/1000 | Loss: 0.00001936
Iteration 14/1000 | Loss: 0.00003525
Iteration 15/1000 | Loss: 0.00001924
Iteration 16/1000 | Loss: 0.00001920
Iteration 17/1000 | Loss: 0.00001919
Iteration 18/1000 | Loss: 0.00001919
Iteration 19/1000 | Loss: 0.00001919
Iteration 20/1000 | Loss: 0.00001919
Iteration 21/1000 | Loss: 0.00001919
Iteration 22/1000 | Loss: 0.00001918
Iteration 23/1000 | Loss: 0.00001917
Iteration 24/1000 | Loss: 0.00001917
Iteration 25/1000 | Loss: 0.00003024
Iteration 26/1000 | Loss: 0.00001919
Iteration 27/1000 | Loss: 0.00001913
Iteration 28/1000 | Loss: 0.00001913
Iteration 29/1000 | Loss: 0.00001913
Iteration 30/1000 | Loss: 0.00001913
Iteration 31/1000 | Loss: 0.00001913
Iteration 32/1000 | Loss: 0.00001913
Iteration 33/1000 | Loss: 0.00001913
Iteration 34/1000 | Loss: 0.00001913
Iteration 35/1000 | Loss: 0.00001912
Iteration 36/1000 | Loss: 0.00001912
Iteration 37/1000 | Loss: 0.00001912
Iteration 38/1000 | Loss: 0.00001912
Iteration 39/1000 | Loss: 0.00002428
Iteration 40/1000 | Loss: 0.00001913
Iteration 41/1000 | Loss: 0.00001912
Iteration 42/1000 | Loss: 0.00001912
Iteration 43/1000 | Loss: 0.00001912
Iteration 44/1000 | Loss: 0.00001912
Iteration 45/1000 | Loss: 0.00001912
Iteration 46/1000 | Loss: 0.00001912
Iteration 47/1000 | Loss: 0.00001912
Iteration 48/1000 | Loss: 0.00001912
Iteration 49/1000 | Loss: 0.00001912
Iteration 50/1000 | Loss: 0.00001911
Iteration 51/1000 | Loss: 0.00001911
Iteration 52/1000 | Loss: 0.00001911
Iteration 53/1000 | Loss: 0.00001911
Iteration 54/1000 | Loss: 0.00001911
Iteration 55/1000 | Loss: 0.00001911
Iteration 56/1000 | Loss: 0.00001910
Iteration 57/1000 | Loss: 0.00001910
Iteration 58/1000 | Loss: 0.00001910
Iteration 59/1000 | Loss: 0.00001910
Iteration 60/1000 | Loss: 0.00001910
Iteration 61/1000 | Loss: 0.00001910
Iteration 62/1000 | Loss: 0.00001910
Iteration 63/1000 | Loss: 0.00001909
Iteration 64/1000 | Loss: 0.00002936
Iteration 65/1000 | Loss: 0.00001903
Iteration 66/1000 | Loss: 0.00001902
Iteration 67/1000 | Loss: 0.00001902
Iteration 68/1000 | Loss: 0.00001902
Iteration 69/1000 | Loss: 0.00001902
Iteration 70/1000 | Loss: 0.00001902
Iteration 71/1000 | Loss: 0.00001901
Iteration 72/1000 | Loss: 0.00001901
Iteration 73/1000 | Loss: 0.00001901
Iteration 74/1000 | Loss: 0.00001901
Iteration 75/1000 | Loss: 0.00001901
Iteration 76/1000 | Loss: 0.00001901
Iteration 77/1000 | Loss: 0.00001901
Iteration 78/1000 | Loss: 0.00001901
Iteration 79/1000 | Loss: 0.00001901
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001900
Iteration 82/1000 | Loss: 0.00001900
Iteration 83/1000 | Loss: 0.00001900
Iteration 84/1000 | Loss: 0.00001900
Iteration 85/1000 | Loss: 0.00001900
Iteration 86/1000 | Loss: 0.00001900
Iteration 87/1000 | Loss: 0.00001900
Iteration 88/1000 | Loss: 0.00001900
Iteration 89/1000 | Loss: 0.00001900
Iteration 90/1000 | Loss: 0.00001899
Iteration 91/1000 | Loss: 0.00001899
Iteration 92/1000 | Loss: 0.00001899
Iteration 93/1000 | Loss: 0.00001899
Iteration 94/1000 | Loss: 0.00001899
Iteration 95/1000 | Loss: 0.00001899
Iteration 96/1000 | Loss: 0.00001899
Iteration 97/1000 | Loss: 0.00001899
Iteration 98/1000 | Loss: 0.00001899
Iteration 99/1000 | Loss: 0.00001899
Iteration 100/1000 | Loss: 0.00001899
Iteration 101/1000 | Loss: 0.00001899
Iteration 102/1000 | Loss: 0.00001899
Iteration 103/1000 | Loss: 0.00001899
Iteration 104/1000 | Loss: 0.00001899
Iteration 105/1000 | Loss: 0.00001898
Iteration 106/1000 | Loss: 0.00001898
Iteration 107/1000 | Loss: 0.00001897
Iteration 108/1000 | Loss: 0.00001897
Iteration 109/1000 | Loss: 0.00001897
Iteration 110/1000 | Loss: 0.00001897
Iteration 111/1000 | Loss: 0.00001897
Iteration 112/1000 | Loss: 0.00001897
Iteration 113/1000 | Loss: 0.00001896
Iteration 114/1000 | Loss: 0.00001896
Iteration 115/1000 | Loss: 0.00001895
Iteration 116/1000 | Loss: 0.00001895
Iteration 117/1000 | Loss: 0.00001892
Iteration 118/1000 | Loss: 0.00001892
Iteration 119/1000 | Loss: 0.00001892
Iteration 120/1000 | Loss: 0.00001891
Iteration 121/1000 | Loss: 0.00001891
Iteration 122/1000 | Loss: 0.00001891
Iteration 123/1000 | Loss: 0.00001891
Iteration 124/1000 | Loss: 0.00001891
Iteration 125/1000 | Loss: 0.00001891
Iteration 126/1000 | Loss: 0.00001890
Iteration 127/1000 | Loss: 0.00001890
Iteration 128/1000 | Loss: 0.00001890
Iteration 129/1000 | Loss: 0.00001890
Iteration 130/1000 | Loss: 0.00001889
Iteration 131/1000 | Loss: 0.00001889
Iteration 132/1000 | Loss: 0.00001889
Iteration 133/1000 | Loss: 0.00001889
Iteration 134/1000 | Loss: 0.00001889
Iteration 135/1000 | Loss: 0.00001889
Iteration 136/1000 | Loss: 0.00001888
Iteration 137/1000 | Loss: 0.00001888
Iteration 138/1000 | Loss: 0.00001888
Iteration 139/1000 | Loss: 0.00001888
Iteration 140/1000 | Loss: 0.00001888
Iteration 141/1000 | Loss: 0.00001888
Iteration 142/1000 | Loss: 0.00001888
Iteration 143/1000 | Loss: 0.00001888
Iteration 144/1000 | Loss: 0.00001888
Iteration 145/1000 | Loss: 0.00001888
Iteration 146/1000 | Loss: 0.00001888
Iteration 147/1000 | Loss: 0.00001888
Iteration 148/1000 | Loss: 0.00001888
Iteration 149/1000 | Loss: 0.00001888
Iteration 150/1000 | Loss: 0.00001888
Iteration 151/1000 | Loss: 0.00001888
Iteration 152/1000 | Loss: 0.00001887
Iteration 153/1000 | Loss: 0.00001887
Iteration 154/1000 | Loss: 0.00001887
Iteration 155/1000 | Loss: 0.00001887
Iteration 156/1000 | Loss: 0.00001887
Iteration 157/1000 | Loss: 0.00001887
Iteration 158/1000 | Loss: 0.00001887
Iteration 159/1000 | Loss: 0.00001887
Iteration 160/1000 | Loss: 0.00001887
Iteration 161/1000 | Loss: 0.00001887
Iteration 162/1000 | Loss: 0.00001887
Iteration 163/1000 | Loss: 0.00001887
Iteration 164/1000 | Loss: 0.00001887
Iteration 165/1000 | Loss: 0.00001887
Iteration 166/1000 | Loss: 0.00001886
Iteration 167/1000 | Loss: 0.00001886
Iteration 168/1000 | Loss: 0.00001886
Iteration 169/1000 | Loss: 0.00001886
Iteration 170/1000 | Loss: 0.00001886
Iteration 171/1000 | Loss: 0.00004165
Iteration 172/1000 | Loss: 0.00005552
Iteration 173/1000 | Loss: 0.00001884
Iteration 174/1000 | Loss: 0.00001884
Iteration 175/1000 | Loss: 0.00001884
Iteration 176/1000 | Loss: 0.00001884
Iteration 177/1000 | Loss: 0.00001884
Iteration 178/1000 | Loss: 0.00001884
Iteration 179/1000 | Loss: 0.00001884
Iteration 180/1000 | Loss: 0.00001884
Iteration 181/1000 | Loss: 0.00001883
Iteration 182/1000 | Loss: 0.00001883
Iteration 183/1000 | Loss: 0.00001883
Iteration 184/1000 | Loss: 0.00001883
Iteration 185/1000 | Loss: 0.00001883
Iteration 186/1000 | Loss: 0.00001883
Iteration 187/1000 | Loss: 0.00001883
Iteration 188/1000 | Loss: 0.00001883
Iteration 189/1000 | Loss: 0.00001883
Iteration 190/1000 | Loss: 0.00001883
Iteration 191/1000 | Loss: 0.00001883
Iteration 192/1000 | Loss: 0.00001883
Iteration 193/1000 | Loss: 0.00001883
Iteration 194/1000 | Loss: 0.00001883
Iteration 195/1000 | Loss: 0.00001883
Iteration 196/1000 | Loss: 0.00001883
Iteration 197/1000 | Loss: 0.00001883
Iteration 198/1000 | Loss: 0.00001883
Iteration 199/1000 | Loss: 0.00001883
Iteration 200/1000 | Loss: 0.00001883
Iteration 201/1000 | Loss: 0.00001883
Iteration 202/1000 | Loss: 0.00001883
Iteration 203/1000 | Loss: 0.00001883
Iteration 204/1000 | Loss: 0.00001883
Iteration 205/1000 | Loss: 0.00001883
Iteration 206/1000 | Loss: 0.00001883
Iteration 207/1000 | Loss: 0.00001883
Iteration 208/1000 | Loss: 0.00001883
Iteration 209/1000 | Loss: 0.00001883
Iteration 210/1000 | Loss: 0.00001883
Iteration 211/1000 | Loss: 0.00001883
Iteration 212/1000 | Loss: 0.00001883
Iteration 213/1000 | Loss: 0.00001883
Iteration 214/1000 | Loss: 0.00001883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.8833521608030424e-05, 1.8833521608030424e-05, 1.8833521608030424e-05, 1.8833521608030424e-05, 1.8833521608030424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8833521608030424e-05

Optimization complete. Final v2v error: 3.572901725769043 mm

Highest mean error: 4.125461578369141 mm for frame 177

Lowest mean error: 3.210036277770996 mm for frame 147

Saving results

Total time: 80.12076997756958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762189
Iteration 2/25 | Loss: 0.00207104
Iteration 3/25 | Loss: 0.00146782
Iteration 4/25 | Loss: 0.00137280
Iteration 5/25 | Loss: 0.00134994
Iteration 6/25 | Loss: 0.00135051
Iteration 7/25 | Loss: 0.00135794
Iteration 8/25 | Loss: 0.00135588
Iteration 9/25 | Loss: 0.00134913
Iteration 10/25 | Loss: 0.00134688
Iteration 11/25 | Loss: 0.00134638
Iteration 12/25 | Loss: 0.00134103
Iteration 13/25 | Loss: 0.00133897
Iteration 14/25 | Loss: 0.00133979
Iteration 15/25 | Loss: 0.00133417
Iteration 16/25 | Loss: 0.00133705
Iteration 17/25 | Loss: 0.00133527
Iteration 18/25 | Loss: 0.00133491
Iteration 19/25 | Loss: 0.00133729
Iteration 20/25 | Loss: 0.00133757
Iteration 21/25 | Loss: 0.00134023
Iteration 22/25 | Loss: 0.00133858
Iteration 23/25 | Loss: 0.00133824
Iteration 24/25 | Loss: 0.00133896
Iteration 25/25 | Loss: 0.00133759

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44127309
Iteration 2/25 | Loss: 0.00055978
Iteration 3/25 | Loss: 0.00055978
Iteration 4/25 | Loss: 0.00055978
Iteration 5/25 | Loss: 0.00055978
Iteration 6/25 | Loss: 0.00055978
Iteration 7/25 | Loss: 0.00055978
Iteration 8/25 | Loss: 0.00055978
Iteration 9/25 | Loss: 0.00055978
Iteration 10/25 | Loss: 0.00055978
Iteration 11/25 | Loss: 0.00055978
Iteration 12/25 | Loss: 0.00055978
Iteration 13/25 | Loss: 0.00055978
Iteration 14/25 | Loss: 0.00055978
Iteration 15/25 | Loss: 0.00055978
Iteration 16/25 | Loss: 0.00055978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005597755662165582, 0.0005597755662165582, 0.0005597755662165582, 0.0005597755662165582, 0.0005597755662165582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005597755662165582

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055978
Iteration 2/1000 | Loss: 0.00005929
Iteration 3/1000 | Loss: 0.00004447
Iteration 4/1000 | Loss: 0.00003858
Iteration 5/1000 | Loss: 0.00003412
Iteration 6/1000 | Loss: 0.00003304
Iteration 7/1000 | Loss: 0.00003221
Iteration 8/1000 | Loss: 0.00003173
Iteration 9/1000 | Loss: 0.00003146
Iteration 10/1000 | Loss: 0.00003114
Iteration 11/1000 | Loss: 0.00003079
Iteration 12/1000 | Loss: 0.00011205
Iteration 13/1000 | Loss: 0.00003070
Iteration 14/1000 | Loss: 0.00003043
Iteration 15/1000 | Loss: 0.00003037
Iteration 16/1000 | Loss: 0.00003034
Iteration 17/1000 | Loss: 0.00003032
Iteration 18/1000 | Loss: 0.00003021
Iteration 19/1000 | Loss: 0.00003020
Iteration 20/1000 | Loss: 0.00003019
Iteration 21/1000 | Loss: 0.00003019
Iteration 22/1000 | Loss: 0.00003016
Iteration 23/1000 | Loss: 0.00003016
Iteration 24/1000 | Loss: 0.00003015
Iteration 25/1000 | Loss: 0.00003015
Iteration 26/1000 | Loss: 0.00003015
Iteration 27/1000 | Loss: 0.00003014
Iteration 28/1000 | Loss: 0.00003014
Iteration 29/1000 | Loss: 0.00003012
Iteration 30/1000 | Loss: 0.00003011
Iteration 31/1000 | Loss: 0.00003011
Iteration 32/1000 | Loss: 0.00003010
Iteration 33/1000 | Loss: 0.00003010
Iteration 34/1000 | Loss: 0.00003010
Iteration 35/1000 | Loss: 0.00003010
Iteration 36/1000 | Loss: 0.00003009
Iteration 37/1000 | Loss: 0.00003008
Iteration 38/1000 | Loss: 0.00003007
Iteration 39/1000 | Loss: 0.00003007
Iteration 40/1000 | Loss: 0.00003007
Iteration 41/1000 | Loss: 0.00003007
Iteration 42/1000 | Loss: 0.00003007
Iteration 43/1000 | Loss: 0.00003007
Iteration 44/1000 | Loss: 0.00003007
Iteration 45/1000 | Loss: 0.00003007
Iteration 46/1000 | Loss: 0.00003007
Iteration 47/1000 | Loss: 0.00003007
Iteration 48/1000 | Loss: 0.00003007
Iteration 49/1000 | Loss: 0.00003007
Iteration 50/1000 | Loss: 0.00003006
Iteration 51/1000 | Loss: 0.00003006
Iteration 52/1000 | Loss: 0.00003004
Iteration 53/1000 | Loss: 0.00003003
Iteration 54/1000 | Loss: 0.00003003
Iteration 55/1000 | Loss: 0.00003000
Iteration 56/1000 | Loss: 0.00003000
Iteration 57/1000 | Loss: 0.00002999
Iteration 58/1000 | Loss: 0.00002999
Iteration 59/1000 | Loss: 0.00005871
Iteration 60/1000 | Loss: 0.00004164
Iteration 61/1000 | Loss: 0.00010941
Iteration 62/1000 | Loss: 0.00004318
Iteration 63/1000 | Loss: 0.00007155
Iteration 64/1000 | Loss: 0.00004714
Iteration 65/1000 | Loss: 0.00007532
Iteration 66/1000 | Loss: 0.00005643
Iteration 67/1000 | Loss: 0.00005549
Iteration 68/1000 | Loss: 0.00003390
Iteration 69/1000 | Loss: 0.00008871
Iteration 70/1000 | Loss: 0.00003689
Iteration 71/1000 | Loss: 0.00004442
Iteration 72/1000 | Loss: 0.00004530
Iteration 73/1000 | Loss: 0.00003252
Iteration 74/1000 | Loss: 0.00003482
Iteration 75/1000 | Loss: 0.00003190
Iteration 76/1000 | Loss: 0.00008454
Iteration 77/1000 | Loss: 0.00004632
Iteration 78/1000 | Loss: 0.00006762
Iteration 79/1000 | Loss: 0.00003914
Iteration 80/1000 | Loss: 0.00003338
Iteration 81/1000 | Loss: 0.00003113
Iteration 82/1000 | Loss: 0.00003046
Iteration 83/1000 | Loss: 0.00008985
Iteration 84/1000 | Loss: 0.00003010
Iteration 85/1000 | Loss: 0.00002989
Iteration 86/1000 | Loss: 0.00002983
Iteration 87/1000 | Loss: 0.00002981
Iteration 88/1000 | Loss: 0.00002980
Iteration 89/1000 | Loss: 0.00002980
Iteration 90/1000 | Loss: 0.00002980
Iteration 91/1000 | Loss: 0.00002979
Iteration 92/1000 | Loss: 0.00002978
Iteration 93/1000 | Loss: 0.00002978
Iteration 94/1000 | Loss: 0.00002976
Iteration 95/1000 | Loss: 0.00002976
Iteration 96/1000 | Loss: 0.00002976
Iteration 97/1000 | Loss: 0.00002976
Iteration 98/1000 | Loss: 0.00002976
Iteration 99/1000 | Loss: 0.00002975
Iteration 100/1000 | Loss: 0.00002975
Iteration 101/1000 | Loss: 0.00002975
Iteration 102/1000 | Loss: 0.00002975
Iteration 103/1000 | Loss: 0.00002975
Iteration 104/1000 | Loss: 0.00002974
Iteration 105/1000 | Loss: 0.00002974
Iteration 106/1000 | Loss: 0.00002974
Iteration 107/1000 | Loss: 0.00002974
Iteration 108/1000 | Loss: 0.00002974
Iteration 109/1000 | Loss: 0.00002974
Iteration 110/1000 | Loss: 0.00002974
Iteration 111/1000 | Loss: 0.00002974
Iteration 112/1000 | Loss: 0.00002973
Iteration 113/1000 | Loss: 0.00002973
Iteration 114/1000 | Loss: 0.00002973
Iteration 115/1000 | Loss: 0.00002973
Iteration 116/1000 | Loss: 0.00002972
Iteration 117/1000 | Loss: 0.00002972
Iteration 118/1000 | Loss: 0.00002972
Iteration 119/1000 | Loss: 0.00002972
Iteration 120/1000 | Loss: 0.00002972
Iteration 121/1000 | Loss: 0.00002971
Iteration 122/1000 | Loss: 0.00002971
Iteration 123/1000 | Loss: 0.00002971
Iteration 124/1000 | Loss: 0.00002971
Iteration 125/1000 | Loss: 0.00002970
Iteration 126/1000 | Loss: 0.00002970
Iteration 127/1000 | Loss: 0.00002970
Iteration 128/1000 | Loss: 0.00002970
Iteration 129/1000 | Loss: 0.00002970
Iteration 130/1000 | Loss: 0.00002970
Iteration 131/1000 | Loss: 0.00002970
Iteration 132/1000 | Loss: 0.00002969
Iteration 133/1000 | Loss: 0.00002969
Iteration 134/1000 | Loss: 0.00002969
Iteration 135/1000 | Loss: 0.00002969
Iteration 136/1000 | Loss: 0.00002968
Iteration 137/1000 | Loss: 0.00002968
Iteration 138/1000 | Loss: 0.00002967
Iteration 139/1000 | Loss: 0.00002967
Iteration 140/1000 | Loss: 0.00002967
Iteration 141/1000 | Loss: 0.00002967
Iteration 142/1000 | Loss: 0.00002967
Iteration 143/1000 | Loss: 0.00002967
Iteration 144/1000 | Loss: 0.00002967
Iteration 145/1000 | Loss: 0.00002966
Iteration 146/1000 | Loss: 0.00002966
Iteration 147/1000 | Loss: 0.00002966
Iteration 148/1000 | Loss: 0.00002966
Iteration 149/1000 | Loss: 0.00002966
Iteration 150/1000 | Loss: 0.00002965
Iteration 151/1000 | Loss: 0.00002965
Iteration 152/1000 | Loss: 0.00002965
Iteration 153/1000 | Loss: 0.00002965
Iteration 154/1000 | Loss: 0.00002965
Iteration 155/1000 | Loss: 0.00002965
Iteration 156/1000 | Loss: 0.00002965
Iteration 157/1000 | Loss: 0.00002965
Iteration 158/1000 | Loss: 0.00002965
Iteration 159/1000 | Loss: 0.00002965
Iteration 160/1000 | Loss: 0.00002965
Iteration 161/1000 | Loss: 0.00002965
Iteration 162/1000 | Loss: 0.00002965
Iteration 163/1000 | Loss: 0.00002965
Iteration 164/1000 | Loss: 0.00002965
Iteration 165/1000 | Loss: 0.00002965
Iteration 166/1000 | Loss: 0.00002964
Iteration 167/1000 | Loss: 0.00002964
Iteration 168/1000 | Loss: 0.00002964
Iteration 169/1000 | Loss: 0.00002964
Iteration 170/1000 | Loss: 0.00002964
Iteration 171/1000 | Loss: 0.00002964
Iteration 172/1000 | Loss: 0.00002964
Iteration 173/1000 | Loss: 0.00002964
Iteration 174/1000 | Loss: 0.00002964
Iteration 175/1000 | Loss: 0.00002964
Iteration 176/1000 | Loss: 0.00002963
Iteration 177/1000 | Loss: 0.00002963
Iteration 178/1000 | Loss: 0.00002963
Iteration 179/1000 | Loss: 0.00002963
Iteration 180/1000 | Loss: 0.00002962
Iteration 181/1000 | Loss: 0.00002962
Iteration 182/1000 | Loss: 0.00002962
Iteration 183/1000 | Loss: 0.00002962
Iteration 184/1000 | Loss: 0.00002962
Iteration 185/1000 | Loss: 0.00002962
Iteration 186/1000 | Loss: 0.00002962
Iteration 187/1000 | Loss: 0.00002962
Iteration 188/1000 | Loss: 0.00002962
Iteration 189/1000 | Loss: 0.00002962
Iteration 190/1000 | Loss: 0.00002962
Iteration 191/1000 | Loss: 0.00002962
Iteration 192/1000 | Loss: 0.00002962
Iteration 193/1000 | Loss: 0.00002962
Iteration 194/1000 | Loss: 0.00002961
Iteration 195/1000 | Loss: 0.00002961
Iteration 196/1000 | Loss: 0.00002961
Iteration 197/1000 | Loss: 0.00002961
Iteration 198/1000 | Loss: 0.00002961
Iteration 199/1000 | Loss: 0.00002961
Iteration 200/1000 | Loss: 0.00002961
Iteration 201/1000 | Loss: 0.00002961
Iteration 202/1000 | Loss: 0.00002961
Iteration 203/1000 | Loss: 0.00002961
Iteration 204/1000 | Loss: 0.00002961
Iteration 205/1000 | Loss: 0.00002961
Iteration 206/1000 | Loss: 0.00002961
Iteration 207/1000 | Loss: 0.00002960
Iteration 208/1000 | Loss: 0.00002960
Iteration 209/1000 | Loss: 0.00002960
Iteration 210/1000 | Loss: 0.00002960
Iteration 211/1000 | Loss: 0.00002960
Iteration 212/1000 | Loss: 0.00002960
Iteration 213/1000 | Loss: 0.00002960
Iteration 214/1000 | Loss: 0.00002960
Iteration 215/1000 | Loss: 0.00002960
Iteration 216/1000 | Loss: 0.00002960
Iteration 217/1000 | Loss: 0.00002960
Iteration 218/1000 | Loss: 0.00002960
Iteration 219/1000 | Loss: 0.00002960
Iteration 220/1000 | Loss: 0.00002960
Iteration 221/1000 | Loss: 0.00002960
Iteration 222/1000 | Loss: 0.00002960
Iteration 223/1000 | Loss: 0.00002959
Iteration 224/1000 | Loss: 0.00002959
Iteration 225/1000 | Loss: 0.00002959
Iteration 226/1000 | Loss: 0.00002959
Iteration 227/1000 | Loss: 0.00002959
Iteration 228/1000 | Loss: 0.00002959
Iteration 229/1000 | Loss: 0.00002959
Iteration 230/1000 | Loss: 0.00002959
Iteration 231/1000 | Loss: 0.00002959
Iteration 232/1000 | Loss: 0.00002959
Iteration 233/1000 | Loss: 0.00002959
Iteration 234/1000 | Loss: 0.00002959
Iteration 235/1000 | Loss: 0.00002959
Iteration 236/1000 | Loss: 0.00002959
Iteration 237/1000 | Loss: 0.00002959
Iteration 238/1000 | Loss: 0.00002959
Iteration 239/1000 | Loss: 0.00002959
Iteration 240/1000 | Loss: 0.00002959
Iteration 241/1000 | Loss: 0.00002959
Iteration 242/1000 | Loss: 0.00002959
Iteration 243/1000 | Loss: 0.00002959
Iteration 244/1000 | Loss: 0.00002959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [2.958844015665818e-05, 2.958844015665818e-05, 2.958844015665818e-05, 2.958844015665818e-05, 2.958844015665818e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.958844015665818e-05

Optimization complete. Final v2v error: 4.421505928039551 mm

Highest mean error: 6.90130615234375 mm for frame 96

Lowest mean error: 3.922024726867676 mm for frame 183

Saving results

Total time: 133.90795993804932
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806407
Iteration 2/25 | Loss: 0.00130582
Iteration 3/25 | Loss: 0.00121160
Iteration 4/25 | Loss: 0.00120216
Iteration 5/25 | Loss: 0.00120020
Iteration 6/25 | Loss: 0.00120018
Iteration 7/25 | Loss: 0.00120018
Iteration 8/25 | Loss: 0.00120018
Iteration 9/25 | Loss: 0.00120018
Iteration 10/25 | Loss: 0.00120018
Iteration 11/25 | Loss: 0.00120018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001200183411128819, 0.001200183411128819, 0.001200183411128819, 0.001200183411128819, 0.001200183411128819]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001200183411128819

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44040012
Iteration 2/25 | Loss: 0.00070552
Iteration 3/25 | Loss: 0.00070552
Iteration 4/25 | Loss: 0.00070552
Iteration 5/25 | Loss: 0.00070552
Iteration 6/25 | Loss: 0.00070552
Iteration 7/25 | Loss: 0.00070552
Iteration 8/25 | Loss: 0.00070552
Iteration 9/25 | Loss: 0.00070552
Iteration 10/25 | Loss: 0.00070552
Iteration 11/25 | Loss: 0.00070552
Iteration 12/25 | Loss: 0.00070552
Iteration 13/25 | Loss: 0.00070552
Iteration 14/25 | Loss: 0.00070552
Iteration 15/25 | Loss: 0.00070552
Iteration 16/25 | Loss: 0.00070552
Iteration 17/25 | Loss: 0.00070552
Iteration 18/25 | Loss: 0.00070552
Iteration 19/25 | Loss: 0.00070552
Iteration 20/25 | Loss: 0.00070552
Iteration 21/25 | Loss: 0.00070552
Iteration 22/25 | Loss: 0.00070552
Iteration 23/25 | Loss: 0.00070552
Iteration 24/25 | Loss: 0.00070552
Iteration 25/25 | Loss: 0.00070552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070552
Iteration 2/1000 | Loss: 0.00002454
Iteration 3/1000 | Loss: 0.00001728
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001414
Iteration 6/1000 | Loss: 0.00001333
Iteration 7/1000 | Loss: 0.00001288
Iteration 8/1000 | Loss: 0.00001255
Iteration 9/1000 | Loss: 0.00001235
Iteration 10/1000 | Loss: 0.00001215
Iteration 11/1000 | Loss: 0.00001210
Iteration 12/1000 | Loss: 0.00001207
Iteration 13/1000 | Loss: 0.00001206
Iteration 14/1000 | Loss: 0.00001206
Iteration 15/1000 | Loss: 0.00001205
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001203
Iteration 18/1000 | Loss: 0.00001201
Iteration 19/1000 | Loss: 0.00001200
Iteration 20/1000 | Loss: 0.00001199
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001192
Iteration 24/1000 | Loss: 0.00001192
Iteration 25/1000 | Loss: 0.00001191
Iteration 26/1000 | Loss: 0.00001188
Iteration 27/1000 | Loss: 0.00001187
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001173
Iteration 35/1000 | Loss: 0.00001173
Iteration 36/1000 | Loss: 0.00001172
Iteration 37/1000 | Loss: 0.00001172
Iteration 38/1000 | Loss: 0.00001171
Iteration 39/1000 | Loss: 0.00001171
Iteration 40/1000 | Loss: 0.00001171
Iteration 41/1000 | Loss: 0.00001171
Iteration 42/1000 | Loss: 0.00001170
Iteration 43/1000 | Loss: 0.00001170
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001169
Iteration 46/1000 | Loss: 0.00001169
Iteration 47/1000 | Loss: 0.00001169
Iteration 48/1000 | Loss: 0.00001169
Iteration 49/1000 | Loss: 0.00001169
Iteration 50/1000 | Loss: 0.00001168
Iteration 51/1000 | Loss: 0.00001168
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001168
Iteration 55/1000 | Loss: 0.00001167
Iteration 56/1000 | Loss: 0.00001167
Iteration 57/1000 | Loss: 0.00001167
Iteration 58/1000 | Loss: 0.00001167
Iteration 59/1000 | Loss: 0.00001166
Iteration 60/1000 | Loss: 0.00001166
Iteration 61/1000 | Loss: 0.00001165
Iteration 62/1000 | Loss: 0.00001165
Iteration 63/1000 | Loss: 0.00001164
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001163
Iteration 66/1000 | Loss: 0.00001162
Iteration 67/1000 | Loss: 0.00001162
Iteration 68/1000 | Loss: 0.00001161
Iteration 69/1000 | Loss: 0.00001161
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001159
Iteration 72/1000 | Loss: 0.00001159
Iteration 73/1000 | Loss: 0.00001158
Iteration 74/1000 | Loss: 0.00001158
Iteration 75/1000 | Loss: 0.00001158
Iteration 76/1000 | Loss: 0.00001158
Iteration 77/1000 | Loss: 0.00001157
Iteration 78/1000 | Loss: 0.00001157
Iteration 79/1000 | Loss: 0.00001157
Iteration 80/1000 | Loss: 0.00001156
Iteration 81/1000 | Loss: 0.00001156
Iteration 82/1000 | Loss: 0.00001156
Iteration 83/1000 | Loss: 0.00001155
Iteration 84/1000 | Loss: 0.00001155
Iteration 85/1000 | Loss: 0.00001155
Iteration 86/1000 | Loss: 0.00001155
Iteration 87/1000 | Loss: 0.00001154
Iteration 88/1000 | Loss: 0.00001154
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001152
Iteration 91/1000 | Loss: 0.00001152
Iteration 92/1000 | Loss: 0.00001151
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001150
Iteration 95/1000 | Loss: 0.00001149
Iteration 96/1000 | Loss: 0.00001149
Iteration 97/1000 | Loss: 0.00001149
Iteration 98/1000 | Loss: 0.00001149
Iteration 99/1000 | Loss: 0.00001149
Iteration 100/1000 | Loss: 0.00001149
Iteration 101/1000 | Loss: 0.00001149
Iteration 102/1000 | Loss: 0.00001148
Iteration 103/1000 | Loss: 0.00001148
Iteration 104/1000 | Loss: 0.00001148
Iteration 105/1000 | Loss: 0.00001148
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001147
Iteration 108/1000 | Loss: 0.00001146
Iteration 109/1000 | Loss: 0.00001146
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001146
Iteration 112/1000 | Loss: 0.00001146
Iteration 113/1000 | Loss: 0.00001146
Iteration 114/1000 | Loss: 0.00001145
Iteration 115/1000 | Loss: 0.00001145
Iteration 116/1000 | Loss: 0.00001145
Iteration 117/1000 | Loss: 0.00001145
Iteration 118/1000 | Loss: 0.00001145
Iteration 119/1000 | Loss: 0.00001145
Iteration 120/1000 | Loss: 0.00001144
Iteration 121/1000 | Loss: 0.00001144
Iteration 122/1000 | Loss: 0.00001144
Iteration 123/1000 | Loss: 0.00001143
Iteration 124/1000 | Loss: 0.00001143
Iteration 125/1000 | Loss: 0.00001143
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001142
Iteration 128/1000 | Loss: 0.00001142
Iteration 129/1000 | Loss: 0.00001142
Iteration 130/1000 | Loss: 0.00001142
Iteration 131/1000 | Loss: 0.00001142
Iteration 132/1000 | Loss: 0.00001141
Iteration 133/1000 | Loss: 0.00001141
Iteration 134/1000 | Loss: 0.00001141
Iteration 135/1000 | Loss: 0.00001140
Iteration 136/1000 | Loss: 0.00001140
Iteration 137/1000 | Loss: 0.00001140
Iteration 138/1000 | Loss: 0.00001140
Iteration 139/1000 | Loss: 0.00001140
Iteration 140/1000 | Loss: 0.00001140
Iteration 141/1000 | Loss: 0.00001140
Iteration 142/1000 | Loss: 0.00001140
Iteration 143/1000 | Loss: 0.00001140
Iteration 144/1000 | Loss: 0.00001139
Iteration 145/1000 | Loss: 0.00001139
Iteration 146/1000 | Loss: 0.00001139
Iteration 147/1000 | Loss: 0.00001139
Iteration 148/1000 | Loss: 0.00001139
Iteration 149/1000 | Loss: 0.00001139
Iteration 150/1000 | Loss: 0.00001139
Iteration 151/1000 | Loss: 0.00001138
Iteration 152/1000 | Loss: 0.00001138
Iteration 153/1000 | Loss: 0.00001138
Iteration 154/1000 | Loss: 0.00001138
Iteration 155/1000 | Loss: 0.00001138
Iteration 156/1000 | Loss: 0.00001138
Iteration 157/1000 | Loss: 0.00001138
Iteration 158/1000 | Loss: 0.00001138
Iteration 159/1000 | Loss: 0.00001138
Iteration 160/1000 | Loss: 0.00001138
Iteration 161/1000 | Loss: 0.00001138
Iteration 162/1000 | Loss: 0.00001138
Iteration 163/1000 | Loss: 0.00001138
Iteration 164/1000 | Loss: 0.00001138
Iteration 165/1000 | Loss: 0.00001137
Iteration 166/1000 | Loss: 0.00001137
Iteration 167/1000 | Loss: 0.00001137
Iteration 168/1000 | Loss: 0.00001137
Iteration 169/1000 | Loss: 0.00001137
Iteration 170/1000 | Loss: 0.00001136
Iteration 171/1000 | Loss: 0.00001136
Iteration 172/1000 | Loss: 0.00001136
Iteration 173/1000 | Loss: 0.00001136
Iteration 174/1000 | Loss: 0.00001136
Iteration 175/1000 | Loss: 0.00001136
Iteration 176/1000 | Loss: 0.00001136
Iteration 177/1000 | Loss: 0.00001136
Iteration 178/1000 | Loss: 0.00001136
Iteration 179/1000 | Loss: 0.00001136
Iteration 180/1000 | Loss: 0.00001136
Iteration 181/1000 | Loss: 0.00001136
Iteration 182/1000 | Loss: 0.00001136
Iteration 183/1000 | Loss: 0.00001136
Iteration 184/1000 | Loss: 0.00001136
Iteration 185/1000 | Loss: 0.00001136
Iteration 186/1000 | Loss: 0.00001136
Iteration 187/1000 | Loss: 0.00001136
Iteration 188/1000 | Loss: 0.00001136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.1358984011167195e-05, 1.1358984011167195e-05, 1.1358984011167195e-05, 1.1358984011167195e-05, 1.1358984011167195e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1358984011167195e-05

Optimization complete. Final v2v error: 2.876803159713745 mm

Highest mean error: 3.0213727951049805 mm for frame 55

Lowest mean error: 2.7401645183563232 mm for frame 168

Saving results

Total time: 39.290026903152466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457130
Iteration 2/25 | Loss: 0.00134018
Iteration 3/25 | Loss: 0.00125924
Iteration 4/25 | Loss: 0.00124473
Iteration 5/25 | Loss: 0.00123978
Iteration 6/25 | Loss: 0.00123960
Iteration 7/25 | Loss: 0.00123960
Iteration 8/25 | Loss: 0.00123960
Iteration 9/25 | Loss: 0.00123960
Iteration 10/25 | Loss: 0.00123960
Iteration 11/25 | Loss: 0.00123960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012396043166518211, 0.0012396043166518211, 0.0012396043166518211, 0.0012396043166518211, 0.0012396043166518211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012396043166518211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42589521
Iteration 2/25 | Loss: 0.00074319
Iteration 3/25 | Loss: 0.00074318
Iteration 4/25 | Loss: 0.00074318
Iteration 5/25 | Loss: 0.00074318
Iteration 6/25 | Loss: 0.00074318
Iteration 7/25 | Loss: 0.00074318
Iteration 8/25 | Loss: 0.00074318
Iteration 9/25 | Loss: 0.00074318
Iteration 10/25 | Loss: 0.00074318
Iteration 11/25 | Loss: 0.00074318
Iteration 12/25 | Loss: 0.00074318
Iteration 13/25 | Loss: 0.00074318
Iteration 14/25 | Loss: 0.00074318
Iteration 15/25 | Loss: 0.00074318
Iteration 16/25 | Loss: 0.00074318
Iteration 17/25 | Loss: 0.00074318
Iteration 18/25 | Loss: 0.00074318
Iteration 19/25 | Loss: 0.00074318
Iteration 20/25 | Loss: 0.00074318
Iteration 21/25 | Loss: 0.00074318
Iteration 22/25 | Loss: 0.00074318
Iteration 23/25 | Loss: 0.00074318
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007431789417751133, 0.0007431789417751133, 0.0007431789417751133, 0.0007431789417751133, 0.0007431789417751133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007431789417751133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074318
Iteration 2/1000 | Loss: 0.00002899
Iteration 3/1000 | Loss: 0.00002281
Iteration 4/1000 | Loss: 0.00002132
Iteration 5/1000 | Loss: 0.00002049
Iteration 6/1000 | Loss: 0.00001989
Iteration 7/1000 | Loss: 0.00001946
Iteration 8/1000 | Loss: 0.00001921
Iteration 9/1000 | Loss: 0.00001908
Iteration 10/1000 | Loss: 0.00001883
Iteration 11/1000 | Loss: 0.00001857
Iteration 12/1000 | Loss: 0.00001855
Iteration 13/1000 | Loss: 0.00001838
Iteration 14/1000 | Loss: 0.00001822
Iteration 15/1000 | Loss: 0.00001820
Iteration 16/1000 | Loss: 0.00001816
Iteration 17/1000 | Loss: 0.00001816
Iteration 18/1000 | Loss: 0.00001814
Iteration 19/1000 | Loss: 0.00001811
Iteration 20/1000 | Loss: 0.00001808
Iteration 21/1000 | Loss: 0.00001804
Iteration 22/1000 | Loss: 0.00001798
Iteration 23/1000 | Loss: 0.00001798
Iteration 24/1000 | Loss: 0.00001798
Iteration 25/1000 | Loss: 0.00001797
Iteration 26/1000 | Loss: 0.00001790
Iteration 27/1000 | Loss: 0.00001790
Iteration 28/1000 | Loss: 0.00001788
Iteration 29/1000 | Loss: 0.00001788
Iteration 30/1000 | Loss: 0.00001787
Iteration 31/1000 | Loss: 0.00001785
Iteration 32/1000 | Loss: 0.00001784
Iteration 33/1000 | Loss: 0.00001784
Iteration 34/1000 | Loss: 0.00001784
Iteration 35/1000 | Loss: 0.00001783
Iteration 36/1000 | Loss: 0.00001783
Iteration 37/1000 | Loss: 0.00001779
Iteration 38/1000 | Loss: 0.00001778
Iteration 39/1000 | Loss: 0.00001777
Iteration 40/1000 | Loss: 0.00001774
Iteration 41/1000 | Loss: 0.00001774
Iteration 42/1000 | Loss: 0.00001773
Iteration 43/1000 | Loss: 0.00001773
Iteration 44/1000 | Loss: 0.00001773
Iteration 45/1000 | Loss: 0.00001772
Iteration 46/1000 | Loss: 0.00001772
Iteration 47/1000 | Loss: 0.00001772
Iteration 48/1000 | Loss: 0.00001772
Iteration 49/1000 | Loss: 0.00001772
Iteration 50/1000 | Loss: 0.00001772
Iteration 51/1000 | Loss: 0.00001771
Iteration 52/1000 | Loss: 0.00001771
Iteration 53/1000 | Loss: 0.00001771
Iteration 54/1000 | Loss: 0.00001771
Iteration 55/1000 | Loss: 0.00001771
Iteration 56/1000 | Loss: 0.00001771
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001771
Iteration 62/1000 | Loss: 0.00001771
Iteration 63/1000 | Loss: 0.00001771
Iteration 64/1000 | Loss: 0.00001771
Iteration 65/1000 | Loss: 0.00001771
Iteration 66/1000 | Loss: 0.00001771
Iteration 67/1000 | Loss: 0.00001771
Iteration 68/1000 | Loss: 0.00001770
Iteration 69/1000 | Loss: 0.00001769
Iteration 70/1000 | Loss: 0.00001768
Iteration 71/1000 | Loss: 0.00001768
Iteration 72/1000 | Loss: 0.00001768
Iteration 73/1000 | Loss: 0.00001768
Iteration 74/1000 | Loss: 0.00001767
Iteration 75/1000 | Loss: 0.00001767
Iteration 76/1000 | Loss: 0.00001767
Iteration 77/1000 | Loss: 0.00001767
Iteration 78/1000 | Loss: 0.00001767
Iteration 79/1000 | Loss: 0.00001766
Iteration 80/1000 | Loss: 0.00001766
Iteration 81/1000 | Loss: 0.00001765
Iteration 82/1000 | Loss: 0.00001765
Iteration 83/1000 | Loss: 0.00001765
Iteration 84/1000 | Loss: 0.00001765
Iteration 85/1000 | Loss: 0.00001765
Iteration 86/1000 | Loss: 0.00001764
Iteration 87/1000 | Loss: 0.00001764
Iteration 88/1000 | Loss: 0.00001764
Iteration 89/1000 | Loss: 0.00001764
Iteration 90/1000 | Loss: 0.00001764
Iteration 91/1000 | Loss: 0.00001764
Iteration 92/1000 | Loss: 0.00001764
Iteration 93/1000 | Loss: 0.00001763
Iteration 94/1000 | Loss: 0.00001763
Iteration 95/1000 | Loss: 0.00001763
Iteration 96/1000 | Loss: 0.00001763
Iteration 97/1000 | Loss: 0.00001763
Iteration 98/1000 | Loss: 0.00001763
Iteration 99/1000 | Loss: 0.00001763
Iteration 100/1000 | Loss: 0.00001763
Iteration 101/1000 | Loss: 0.00001762
Iteration 102/1000 | Loss: 0.00001762
Iteration 103/1000 | Loss: 0.00001762
Iteration 104/1000 | Loss: 0.00001762
Iteration 105/1000 | Loss: 0.00001762
Iteration 106/1000 | Loss: 0.00001762
Iteration 107/1000 | Loss: 0.00001761
Iteration 108/1000 | Loss: 0.00001761
Iteration 109/1000 | Loss: 0.00001761
Iteration 110/1000 | Loss: 0.00001761
Iteration 111/1000 | Loss: 0.00001761
Iteration 112/1000 | Loss: 0.00001761
Iteration 113/1000 | Loss: 0.00001761
Iteration 114/1000 | Loss: 0.00001761
Iteration 115/1000 | Loss: 0.00001761
Iteration 116/1000 | Loss: 0.00001761
Iteration 117/1000 | Loss: 0.00001760
Iteration 118/1000 | Loss: 0.00001760
Iteration 119/1000 | Loss: 0.00001760
Iteration 120/1000 | Loss: 0.00001760
Iteration 121/1000 | Loss: 0.00001760
Iteration 122/1000 | Loss: 0.00001759
Iteration 123/1000 | Loss: 0.00001759
Iteration 124/1000 | Loss: 0.00001759
Iteration 125/1000 | Loss: 0.00001759
Iteration 126/1000 | Loss: 0.00001759
Iteration 127/1000 | Loss: 0.00001759
Iteration 128/1000 | Loss: 0.00001759
Iteration 129/1000 | Loss: 0.00001759
Iteration 130/1000 | Loss: 0.00001759
Iteration 131/1000 | Loss: 0.00001758
Iteration 132/1000 | Loss: 0.00001758
Iteration 133/1000 | Loss: 0.00001758
Iteration 134/1000 | Loss: 0.00001758
Iteration 135/1000 | Loss: 0.00001758
Iteration 136/1000 | Loss: 0.00001758
Iteration 137/1000 | Loss: 0.00001758
Iteration 138/1000 | Loss: 0.00001758
Iteration 139/1000 | Loss: 0.00001758
Iteration 140/1000 | Loss: 0.00001758
Iteration 141/1000 | Loss: 0.00001758
Iteration 142/1000 | Loss: 0.00001758
Iteration 143/1000 | Loss: 0.00001758
Iteration 144/1000 | Loss: 0.00001758
Iteration 145/1000 | Loss: 0.00001758
Iteration 146/1000 | Loss: 0.00001758
Iteration 147/1000 | Loss: 0.00001757
Iteration 148/1000 | Loss: 0.00001757
Iteration 149/1000 | Loss: 0.00001757
Iteration 150/1000 | Loss: 0.00001757
Iteration 151/1000 | Loss: 0.00001757
Iteration 152/1000 | Loss: 0.00001757
Iteration 153/1000 | Loss: 0.00001757
Iteration 154/1000 | Loss: 0.00001757
Iteration 155/1000 | Loss: 0.00001757
Iteration 156/1000 | Loss: 0.00001757
Iteration 157/1000 | Loss: 0.00001757
Iteration 158/1000 | Loss: 0.00001757
Iteration 159/1000 | Loss: 0.00001757
Iteration 160/1000 | Loss: 0.00001757
Iteration 161/1000 | Loss: 0.00001757
Iteration 162/1000 | Loss: 0.00001757
Iteration 163/1000 | Loss: 0.00001757
Iteration 164/1000 | Loss: 0.00001757
Iteration 165/1000 | Loss: 0.00001756
Iteration 166/1000 | Loss: 0.00001756
Iteration 167/1000 | Loss: 0.00001756
Iteration 168/1000 | Loss: 0.00001756
Iteration 169/1000 | Loss: 0.00001756
Iteration 170/1000 | Loss: 0.00001756
Iteration 171/1000 | Loss: 0.00001756
Iteration 172/1000 | Loss: 0.00001756
Iteration 173/1000 | Loss: 0.00001756
Iteration 174/1000 | Loss: 0.00001756
Iteration 175/1000 | Loss: 0.00001756
Iteration 176/1000 | Loss: 0.00001756
Iteration 177/1000 | Loss: 0.00001756
Iteration 178/1000 | Loss: 0.00001756
Iteration 179/1000 | Loss: 0.00001756
Iteration 180/1000 | Loss: 0.00001756
Iteration 181/1000 | Loss: 0.00001756
Iteration 182/1000 | Loss: 0.00001756
Iteration 183/1000 | Loss: 0.00001756
Iteration 184/1000 | Loss: 0.00001756
Iteration 185/1000 | Loss: 0.00001756
Iteration 186/1000 | Loss: 0.00001756
Iteration 187/1000 | Loss: 0.00001756
Iteration 188/1000 | Loss: 0.00001756
Iteration 189/1000 | Loss: 0.00001756
Iteration 190/1000 | Loss: 0.00001756
Iteration 191/1000 | Loss: 0.00001756
Iteration 192/1000 | Loss: 0.00001756
Iteration 193/1000 | Loss: 0.00001756
Iteration 194/1000 | Loss: 0.00001756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.7563441360834986e-05, 1.7563441360834986e-05, 1.7563441360834986e-05, 1.7563441360834986e-05, 1.7563441360834986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7563441360834986e-05

Optimization complete. Final v2v error: 3.5631940364837646 mm

Highest mean error: 3.6812450885772705 mm for frame 117

Lowest mean error: 3.4095423221588135 mm for frame 53

Saving results

Total time: 38.45857763290405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960624
Iteration 2/25 | Loss: 0.00165426
Iteration 3/25 | Loss: 0.00144601
Iteration 4/25 | Loss: 0.00142518
Iteration 5/25 | Loss: 0.00141873
Iteration 6/25 | Loss: 0.00141843
Iteration 7/25 | Loss: 0.00141843
Iteration 8/25 | Loss: 0.00141843
Iteration 9/25 | Loss: 0.00141843
Iteration 10/25 | Loss: 0.00141843
Iteration 11/25 | Loss: 0.00141843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014184328028932214, 0.0014184328028932214, 0.0014184328028932214, 0.0014184328028932214, 0.0014184328028932214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014184328028932214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85434014
Iteration 2/25 | Loss: 0.00089794
Iteration 3/25 | Loss: 0.00089794
Iteration 4/25 | Loss: 0.00089794
Iteration 5/25 | Loss: 0.00089794
Iteration 6/25 | Loss: 0.00089794
Iteration 7/25 | Loss: 0.00089794
Iteration 8/25 | Loss: 0.00089793
Iteration 9/25 | Loss: 0.00089793
Iteration 10/25 | Loss: 0.00089793
Iteration 11/25 | Loss: 0.00089793
Iteration 12/25 | Loss: 0.00089793
Iteration 13/25 | Loss: 0.00089793
Iteration 14/25 | Loss: 0.00089793
Iteration 15/25 | Loss: 0.00089793
Iteration 16/25 | Loss: 0.00089793
Iteration 17/25 | Loss: 0.00089793
Iteration 18/25 | Loss: 0.00089793
Iteration 19/25 | Loss: 0.00089793
Iteration 20/25 | Loss: 0.00089793
Iteration 21/25 | Loss: 0.00089793
Iteration 22/25 | Loss: 0.00089793
Iteration 23/25 | Loss: 0.00089793
Iteration 24/25 | Loss: 0.00089793
Iteration 25/25 | Loss: 0.00089793
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000897933728992939, 0.000897933728992939, 0.000897933728992939, 0.000897933728992939, 0.000897933728992939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000897933728992939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089793
Iteration 2/1000 | Loss: 0.00007371
Iteration 3/1000 | Loss: 0.00005031
Iteration 4/1000 | Loss: 0.00004669
Iteration 5/1000 | Loss: 0.00004443
Iteration 6/1000 | Loss: 0.00004309
Iteration 7/1000 | Loss: 0.00004188
Iteration 8/1000 | Loss: 0.00004124
Iteration 9/1000 | Loss: 0.00004061
Iteration 10/1000 | Loss: 0.00004003
Iteration 11/1000 | Loss: 0.00003948
Iteration 12/1000 | Loss: 0.00003900
Iteration 13/1000 | Loss: 0.00003857
Iteration 14/1000 | Loss: 0.00003819
Iteration 15/1000 | Loss: 0.00003778
Iteration 16/1000 | Loss: 0.00003750
Iteration 17/1000 | Loss: 0.00003731
Iteration 18/1000 | Loss: 0.00003712
Iteration 19/1000 | Loss: 0.00003698
Iteration 20/1000 | Loss: 0.00003688
Iteration 21/1000 | Loss: 0.00003682
Iteration 22/1000 | Loss: 0.00003670
Iteration 23/1000 | Loss: 0.00003669
Iteration 24/1000 | Loss: 0.00003667
Iteration 25/1000 | Loss: 0.00003667
Iteration 26/1000 | Loss: 0.00003666
Iteration 27/1000 | Loss: 0.00003665
Iteration 28/1000 | Loss: 0.00003665
Iteration 29/1000 | Loss: 0.00003664
Iteration 30/1000 | Loss: 0.00003664
Iteration 31/1000 | Loss: 0.00003663
Iteration 32/1000 | Loss: 0.00003659
Iteration 33/1000 | Loss: 0.00003658
Iteration 34/1000 | Loss: 0.00003657
Iteration 35/1000 | Loss: 0.00003656
Iteration 36/1000 | Loss: 0.00003656
Iteration 37/1000 | Loss: 0.00003656
Iteration 38/1000 | Loss: 0.00003655
Iteration 39/1000 | Loss: 0.00003655
Iteration 40/1000 | Loss: 0.00003653
Iteration 41/1000 | Loss: 0.00003653
Iteration 42/1000 | Loss: 0.00003653
Iteration 43/1000 | Loss: 0.00003652
Iteration 44/1000 | Loss: 0.00003652
Iteration 45/1000 | Loss: 0.00003652
Iteration 46/1000 | Loss: 0.00003652
Iteration 47/1000 | Loss: 0.00003652
Iteration 48/1000 | Loss: 0.00003652
Iteration 49/1000 | Loss: 0.00003652
Iteration 50/1000 | Loss: 0.00003651
Iteration 51/1000 | Loss: 0.00003650
Iteration 52/1000 | Loss: 0.00003650
Iteration 53/1000 | Loss: 0.00003650
Iteration 54/1000 | Loss: 0.00003650
Iteration 55/1000 | Loss: 0.00003649
Iteration 56/1000 | Loss: 0.00003648
Iteration 57/1000 | Loss: 0.00003648
Iteration 58/1000 | Loss: 0.00003648
Iteration 59/1000 | Loss: 0.00003648
Iteration 60/1000 | Loss: 0.00003648
Iteration 61/1000 | Loss: 0.00003648
Iteration 62/1000 | Loss: 0.00003647
Iteration 63/1000 | Loss: 0.00003647
Iteration 64/1000 | Loss: 0.00003647
Iteration 65/1000 | Loss: 0.00003647
Iteration 66/1000 | Loss: 0.00003647
Iteration 67/1000 | Loss: 0.00003647
Iteration 68/1000 | Loss: 0.00003645
Iteration 69/1000 | Loss: 0.00003645
Iteration 70/1000 | Loss: 0.00003645
Iteration 71/1000 | Loss: 0.00003645
Iteration 72/1000 | Loss: 0.00003645
Iteration 73/1000 | Loss: 0.00003645
Iteration 74/1000 | Loss: 0.00003645
Iteration 75/1000 | Loss: 0.00003645
Iteration 76/1000 | Loss: 0.00003645
Iteration 77/1000 | Loss: 0.00003644
Iteration 78/1000 | Loss: 0.00003644
Iteration 79/1000 | Loss: 0.00003644
Iteration 80/1000 | Loss: 0.00003644
Iteration 81/1000 | Loss: 0.00003644
Iteration 82/1000 | Loss: 0.00003644
Iteration 83/1000 | Loss: 0.00003644
Iteration 84/1000 | Loss: 0.00003644
Iteration 85/1000 | Loss: 0.00003643
Iteration 86/1000 | Loss: 0.00003642
Iteration 87/1000 | Loss: 0.00003642
Iteration 88/1000 | Loss: 0.00003642
Iteration 89/1000 | Loss: 0.00003642
Iteration 90/1000 | Loss: 0.00003641
Iteration 91/1000 | Loss: 0.00003641
Iteration 92/1000 | Loss: 0.00003641
Iteration 93/1000 | Loss: 0.00003641
Iteration 94/1000 | Loss: 0.00003641
Iteration 95/1000 | Loss: 0.00003640
Iteration 96/1000 | Loss: 0.00003640
Iteration 97/1000 | Loss: 0.00003640
Iteration 98/1000 | Loss: 0.00003640
Iteration 99/1000 | Loss: 0.00003640
Iteration 100/1000 | Loss: 0.00003639
Iteration 101/1000 | Loss: 0.00003639
Iteration 102/1000 | Loss: 0.00003639
Iteration 103/1000 | Loss: 0.00003639
Iteration 104/1000 | Loss: 0.00003639
Iteration 105/1000 | Loss: 0.00003639
Iteration 106/1000 | Loss: 0.00003639
Iteration 107/1000 | Loss: 0.00003639
Iteration 108/1000 | Loss: 0.00003638
Iteration 109/1000 | Loss: 0.00003638
Iteration 110/1000 | Loss: 0.00003638
Iteration 111/1000 | Loss: 0.00003638
Iteration 112/1000 | Loss: 0.00003638
Iteration 113/1000 | Loss: 0.00003638
Iteration 114/1000 | Loss: 0.00003638
Iteration 115/1000 | Loss: 0.00003637
Iteration 116/1000 | Loss: 0.00003637
Iteration 117/1000 | Loss: 0.00003637
Iteration 118/1000 | Loss: 0.00003637
Iteration 119/1000 | Loss: 0.00003637
Iteration 120/1000 | Loss: 0.00003637
Iteration 121/1000 | Loss: 0.00003637
Iteration 122/1000 | Loss: 0.00003636
Iteration 123/1000 | Loss: 0.00003636
Iteration 124/1000 | Loss: 0.00003636
Iteration 125/1000 | Loss: 0.00003636
Iteration 126/1000 | Loss: 0.00003636
Iteration 127/1000 | Loss: 0.00003636
Iteration 128/1000 | Loss: 0.00003636
Iteration 129/1000 | Loss: 0.00003636
Iteration 130/1000 | Loss: 0.00003635
Iteration 131/1000 | Loss: 0.00003635
Iteration 132/1000 | Loss: 0.00003635
Iteration 133/1000 | Loss: 0.00003635
Iteration 134/1000 | Loss: 0.00003635
Iteration 135/1000 | Loss: 0.00003635
Iteration 136/1000 | Loss: 0.00003635
Iteration 137/1000 | Loss: 0.00003635
Iteration 138/1000 | Loss: 0.00003635
Iteration 139/1000 | Loss: 0.00003634
Iteration 140/1000 | Loss: 0.00003634
Iteration 141/1000 | Loss: 0.00003634
Iteration 142/1000 | Loss: 0.00003634
Iteration 143/1000 | Loss: 0.00003633
Iteration 144/1000 | Loss: 0.00003633
Iteration 145/1000 | Loss: 0.00003633
Iteration 146/1000 | Loss: 0.00003633
Iteration 147/1000 | Loss: 0.00003633
Iteration 148/1000 | Loss: 0.00003633
Iteration 149/1000 | Loss: 0.00003633
Iteration 150/1000 | Loss: 0.00003633
Iteration 151/1000 | Loss: 0.00003633
Iteration 152/1000 | Loss: 0.00003633
Iteration 153/1000 | Loss: 0.00003633
Iteration 154/1000 | Loss: 0.00003633
Iteration 155/1000 | Loss: 0.00003633
Iteration 156/1000 | Loss: 0.00003632
Iteration 157/1000 | Loss: 0.00003632
Iteration 158/1000 | Loss: 0.00003632
Iteration 159/1000 | Loss: 0.00003632
Iteration 160/1000 | Loss: 0.00003632
Iteration 161/1000 | Loss: 0.00003631
Iteration 162/1000 | Loss: 0.00003631
Iteration 163/1000 | Loss: 0.00003631
Iteration 164/1000 | Loss: 0.00003631
Iteration 165/1000 | Loss: 0.00003631
Iteration 166/1000 | Loss: 0.00003631
Iteration 167/1000 | Loss: 0.00003630
Iteration 168/1000 | Loss: 0.00003630
Iteration 169/1000 | Loss: 0.00003630
Iteration 170/1000 | Loss: 0.00003630
Iteration 171/1000 | Loss: 0.00003630
Iteration 172/1000 | Loss: 0.00003630
Iteration 173/1000 | Loss: 0.00003630
Iteration 174/1000 | Loss: 0.00003630
Iteration 175/1000 | Loss: 0.00003630
Iteration 176/1000 | Loss: 0.00003630
Iteration 177/1000 | Loss: 0.00003629
Iteration 178/1000 | Loss: 0.00003629
Iteration 179/1000 | Loss: 0.00003629
Iteration 180/1000 | Loss: 0.00003629
Iteration 181/1000 | Loss: 0.00003628
Iteration 182/1000 | Loss: 0.00003628
Iteration 183/1000 | Loss: 0.00003628
Iteration 184/1000 | Loss: 0.00003628
Iteration 185/1000 | Loss: 0.00003628
Iteration 186/1000 | Loss: 0.00003628
Iteration 187/1000 | Loss: 0.00003628
Iteration 188/1000 | Loss: 0.00003628
Iteration 189/1000 | Loss: 0.00003628
Iteration 190/1000 | Loss: 0.00003628
Iteration 191/1000 | Loss: 0.00003627
Iteration 192/1000 | Loss: 0.00003627
Iteration 193/1000 | Loss: 0.00003627
Iteration 194/1000 | Loss: 0.00003627
Iteration 195/1000 | Loss: 0.00003627
Iteration 196/1000 | Loss: 0.00003627
Iteration 197/1000 | Loss: 0.00003627
Iteration 198/1000 | Loss: 0.00003627
Iteration 199/1000 | Loss: 0.00003627
Iteration 200/1000 | Loss: 0.00003627
Iteration 201/1000 | Loss: 0.00003627
Iteration 202/1000 | Loss: 0.00003627
Iteration 203/1000 | Loss: 0.00003627
Iteration 204/1000 | Loss: 0.00003627
Iteration 205/1000 | Loss: 0.00003627
Iteration 206/1000 | Loss: 0.00003627
Iteration 207/1000 | Loss: 0.00003626
Iteration 208/1000 | Loss: 0.00003626
Iteration 209/1000 | Loss: 0.00003626
Iteration 210/1000 | Loss: 0.00003626
Iteration 211/1000 | Loss: 0.00003626
Iteration 212/1000 | Loss: 0.00003626
Iteration 213/1000 | Loss: 0.00003626
Iteration 214/1000 | Loss: 0.00003626
Iteration 215/1000 | Loss: 0.00003626
Iteration 216/1000 | Loss: 0.00003626
Iteration 217/1000 | Loss: 0.00003626
Iteration 218/1000 | Loss: 0.00003625
Iteration 219/1000 | Loss: 0.00003625
Iteration 220/1000 | Loss: 0.00003625
Iteration 221/1000 | Loss: 0.00003625
Iteration 222/1000 | Loss: 0.00003625
Iteration 223/1000 | Loss: 0.00003625
Iteration 224/1000 | Loss: 0.00003625
Iteration 225/1000 | Loss: 0.00003625
Iteration 226/1000 | Loss: 0.00003625
Iteration 227/1000 | Loss: 0.00003625
Iteration 228/1000 | Loss: 0.00003625
Iteration 229/1000 | Loss: 0.00003624
Iteration 230/1000 | Loss: 0.00003624
Iteration 231/1000 | Loss: 0.00003624
Iteration 232/1000 | Loss: 0.00003624
Iteration 233/1000 | Loss: 0.00003624
Iteration 234/1000 | Loss: 0.00003624
Iteration 235/1000 | Loss: 0.00003624
Iteration 236/1000 | Loss: 0.00003624
Iteration 237/1000 | Loss: 0.00003624
Iteration 238/1000 | Loss: 0.00003624
Iteration 239/1000 | Loss: 0.00003624
Iteration 240/1000 | Loss: 0.00003623
Iteration 241/1000 | Loss: 0.00003623
Iteration 242/1000 | Loss: 0.00003623
Iteration 243/1000 | Loss: 0.00003623
Iteration 244/1000 | Loss: 0.00003623
Iteration 245/1000 | Loss: 0.00003623
Iteration 246/1000 | Loss: 0.00003623
Iteration 247/1000 | Loss: 0.00003623
Iteration 248/1000 | Loss: 0.00003623
Iteration 249/1000 | Loss: 0.00003623
Iteration 250/1000 | Loss: 0.00003623
Iteration 251/1000 | Loss: 0.00003623
Iteration 252/1000 | Loss: 0.00003623
Iteration 253/1000 | Loss: 0.00003623
Iteration 254/1000 | Loss: 0.00003623
Iteration 255/1000 | Loss: 0.00003623
Iteration 256/1000 | Loss: 0.00003623
Iteration 257/1000 | Loss: 0.00003623
Iteration 258/1000 | Loss: 0.00003623
Iteration 259/1000 | Loss: 0.00003623
Iteration 260/1000 | Loss: 0.00003623
Iteration 261/1000 | Loss: 0.00003623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [3.6225355870556086e-05, 3.6225355870556086e-05, 3.6225355870556086e-05, 3.6225355870556086e-05, 3.6225355870556086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6225355870556086e-05

Optimization complete. Final v2v error: 5.054563999176025 mm

Highest mean error: 5.550785541534424 mm for frame 97

Lowest mean error: 4.150124549865723 mm for frame 48

Saving results

Total time: 62.32061767578125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039481
Iteration 2/25 | Loss: 0.00241990
Iteration 3/25 | Loss: 0.00156813
Iteration 4/25 | Loss: 0.00169622
Iteration 5/25 | Loss: 0.00158798
Iteration 6/25 | Loss: 0.00149463
Iteration 7/25 | Loss: 0.00146742
Iteration 8/25 | Loss: 0.00137095
Iteration 9/25 | Loss: 0.00136760
Iteration 10/25 | Loss: 0.00131015
Iteration 11/25 | Loss: 0.00130093
Iteration 12/25 | Loss: 0.00129433
Iteration 13/25 | Loss: 0.00129716
Iteration 14/25 | Loss: 0.00131336
Iteration 15/25 | Loss: 0.00129762
Iteration 16/25 | Loss: 0.00128482
Iteration 17/25 | Loss: 0.00127552
Iteration 18/25 | Loss: 0.00128529
Iteration 19/25 | Loss: 0.00127390
Iteration 20/25 | Loss: 0.00127384
Iteration 21/25 | Loss: 0.00127384
Iteration 22/25 | Loss: 0.00127384
Iteration 23/25 | Loss: 0.00127384
Iteration 24/25 | Loss: 0.00127384
Iteration 25/25 | Loss: 0.00127383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41737568
Iteration 2/25 | Loss: 0.00205005
Iteration 3/25 | Loss: 0.00093691
Iteration 4/25 | Loss: 0.00093691
Iteration 5/25 | Loss: 0.00093691
Iteration 6/25 | Loss: 0.00093690
Iteration 7/25 | Loss: 0.00093690
Iteration 8/25 | Loss: 0.00093690
Iteration 9/25 | Loss: 0.00093690
Iteration 10/25 | Loss: 0.00093690
Iteration 11/25 | Loss: 0.00093690
Iteration 12/25 | Loss: 0.00093690
Iteration 13/25 | Loss: 0.00093690
Iteration 14/25 | Loss: 0.00093690
Iteration 15/25 | Loss: 0.00093690
Iteration 16/25 | Loss: 0.00093690
Iteration 17/25 | Loss: 0.00093690
Iteration 18/25 | Loss: 0.00093690
Iteration 19/25 | Loss: 0.00093690
Iteration 20/25 | Loss: 0.00093690
Iteration 21/25 | Loss: 0.00093690
Iteration 22/25 | Loss: 0.00093690
Iteration 23/25 | Loss: 0.00093690
Iteration 24/25 | Loss: 0.00093690
Iteration 25/25 | Loss: 0.00093690

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093690
Iteration 2/1000 | Loss: 0.00003282
Iteration 3/1000 | Loss: 0.00002429
Iteration 4/1000 | Loss: 0.00002293
Iteration 5/1000 | Loss: 0.00002213
Iteration 6/1000 | Loss: 0.00002169
Iteration 7/1000 | Loss: 0.00002142
Iteration 8/1000 | Loss: 0.00002111
Iteration 9/1000 | Loss: 0.00002088
Iteration 10/1000 | Loss: 0.00002065
Iteration 11/1000 | Loss: 0.00002042
Iteration 12/1000 | Loss: 0.00002026
Iteration 13/1000 | Loss: 0.00002012
Iteration 14/1000 | Loss: 0.00001997
Iteration 15/1000 | Loss: 0.00001996
Iteration 16/1000 | Loss: 0.00001995
Iteration 17/1000 | Loss: 0.00001994
Iteration 18/1000 | Loss: 0.00001993
Iteration 19/1000 | Loss: 0.00001989
Iteration 20/1000 | Loss: 0.00001989
Iteration 21/1000 | Loss: 0.00001989
Iteration 22/1000 | Loss: 0.00001988
Iteration 23/1000 | Loss: 0.00001987
Iteration 24/1000 | Loss: 0.00001986
Iteration 25/1000 | Loss: 0.00001986
Iteration 26/1000 | Loss: 0.00001984
Iteration 27/1000 | Loss: 0.00001984
Iteration 28/1000 | Loss: 0.00001983
Iteration 29/1000 | Loss: 0.00001983
Iteration 30/1000 | Loss: 0.00001981
Iteration 31/1000 | Loss: 0.00001980
Iteration 32/1000 | Loss: 0.00001980
Iteration 33/1000 | Loss: 0.00001980
Iteration 34/1000 | Loss: 0.00001979
Iteration 35/1000 | Loss: 0.00001979
Iteration 36/1000 | Loss: 0.00001978
Iteration 37/1000 | Loss: 0.00001978
Iteration 38/1000 | Loss: 0.00001977
Iteration 39/1000 | Loss: 0.00001977
Iteration 40/1000 | Loss: 0.00001977
Iteration 41/1000 | Loss: 0.00001977
Iteration 42/1000 | Loss: 0.00001976
Iteration 43/1000 | Loss: 0.00001976
Iteration 44/1000 | Loss: 0.00001976
Iteration 45/1000 | Loss: 0.00001976
Iteration 46/1000 | Loss: 0.00001976
Iteration 47/1000 | Loss: 0.00001975
Iteration 48/1000 | Loss: 0.00001975
Iteration 49/1000 | Loss: 0.00001975
Iteration 50/1000 | Loss: 0.00001975
Iteration 51/1000 | Loss: 0.00001975
Iteration 52/1000 | Loss: 0.00001975
Iteration 53/1000 | Loss: 0.00001974
Iteration 54/1000 | Loss: 0.00001974
Iteration 55/1000 | Loss: 0.00001973
Iteration 56/1000 | Loss: 0.00001973
Iteration 57/1000 | Loss: 0.00001973
Iteration 58/1000 | Loss: 0.00001972
Iteration 59/1000 | Loss: 0.00001972
Iteration 60/1000 | Loss: 0.00001972
Iteration 61/1000 | Loss: 0.00001971
Iteration 62/1000 | Loss: 0.00001971
Iteration 63/1000 | Loss: 0.00001971
Iteration 64/1000 | Loss: 0.00001970
Iteration 65/1000 | Loss: 0.00001970
Iteration 66/1000 | Loss: 0.00001970
Iteration 67/1000 | Loss: 0.00001967
Iteration 68/1000 | Loss: 0.00001967
Iteration 69/1000 | Loss: 0.00001966
Iteration 70/1000 | Loss: 0.00001966
Iteration 71/1000 | Loss: 0.00001965
Iteration 72/1000 | Loss: 0.00001965
Iteration 73/1000 | Loss: 0.00001964
Iteration 74/1000 | Loss: 0.00001964
Iteration 75/1000 | Loss: 0.00001964
Iteration 76/1000 | Loss: 0.00001964
Iteration 77/1000 | Loss: 0.00001964
Iteration 78/1000 | Loss: 0.00001963
Iteration 79/1000 | Loss: 0.00001963
Iteration 80/1000 | Loss: 0.00001963
Iteration 81/1000 | Loss: 0.00001963
Iteration 82/1000 | Loss: 0.00001963
Iteration 83/1000 | Loss: 0.00001963
Iteration 84/1000 | Loss: 0.00001963
Iteration 85/1000 | Loss: 0.00001963
Iteration 86/1000 | Loss: 0.00001962
Iteration 87/1000 | Loss: 0.00001962
Iteration 88/1000 | Loss: 0.00001962
Iteration 89/1000 | Loss: 0.00001962
Iteration 90/1000 | Loss: 0.00001962
Iteration 91/1000 | Loss: 0.00001962
Iteration 92/1000 | Loss: 0.00001962
Iteration 93/1000 | Loss: 0.00001962
Iteration 94/1000 | Loss: 0.00001962
Iteration 95/1000 | Loss: 0.00001962
Iteration 96/1000 | Loss: 0.00001962
Iteration 97/1000 | Loss: 0.00001962
Iteration 98/1000 | Loss: 0.00001962
Iteration 99/1000 | Loss: 0.00001962
Iteration 100/1000 | Loss: 0.00001962
Iteration 101/1000 | Loss: 0.00001962
Iteration 102/1000 | Loss: 0.00001962
Iteration 103/1000 | Loss: 0.00001962
Iteration 104/1000 | Loss: 0.00001962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.9619104932644404e-05, 1.9619104932644404e-05, 1.9619104932644404e-05, 1.9619104932644404e-05, 1.9619104932644404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9619104932644404e-05

Optimization complete. Final v2v error: 3.7542879581451416 mm

Highest mean error: 4.023768424987793 mm for frame 238

Lowest mean error: 3.591383457183838 mm for frame 13

Saving results

Total time: 70.10160207748413
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410209
Iteration 2/25 | Loss: 0.00140148
Iteration 3/25 | Loss: 0.00123914
Iteration 4/25 | Loss: 0.00122698
Iteration 5/25 | Loss: 0.00122508
Iteration 6/25 | Loss: 0.00122464
Iteration 7/25 | Loss: 0.00122464
Iteration 8/25 | Loss: 0.00122464
Iteration 9/25 | Loss: 0.00122464
Iteration 10/25 | Loss: 0.00122464
Iteration 11/25 | Loss: 0.00122464
Iteration 12/25 | Loss: 0.00122464
Iteration 13/25 | Loss: 0.00122464
Iteration 14/25 | Loss: 0.00122464
Iteration 15/25 | Loss: 0.00122464
Iteration 16/25 | Loss: 0.00122464
Iteration 17/25 | Loss: 0.00122464
Iteration 18/25 | Loss: 0.00122464
Iteration 19/25 | Loss: 0.00122464
Iteration 20/25 | Loss: 0.00122464
Iteration 21/25 | Loss: 0.00122464
Iteration 22/25 | Loss: 0.00122464
Iteration 23/25 | Loss: 0.00122464
Iteration 24/25 | Loss: 0.00122464
Iteration 25/25 | Loss: 0.00122464

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43072271
Iteration 2/25 | Loss: 0.00063115
Iteration 3/25 | Loss: 0.00063114
Iteration 4/25 | Loss: 0.00063114
Iteration 5/25 | Loss: 0.00063114
Iteration 6/25 | Loss: 0.00063114
Iteration 7/25 | Loss: 0.00063114
Iteration 8/25 | Loss: 0.00063114
Iteration 9/25 | Loss: 0.00063114
Iteration 10/25 | Loss: 0.00063114
Iteration 11/25 | Loss: 0.00063114
Iteration 12/25 | Loss: 0.00063114
Iteration 13/25 | Loss: 0.00063114
Iteration 14/25 | Loss: 0.00063114
Iteration 15/25 | Loss: 0.00063114
Iteration 16/25 | Loss: 0.00063114
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006311377510428429, 0.0006311377510428429, 0.0006311377510428429, 0.0006311377510428429, 0.0006311377510428429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006311377510428429

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063114
Iteration 2/1000 | Loss: 0.00003127
Iteration 3/1000 | Loss: 0.00002258
Iteration 4/1000 | Loss: 0.00001933
Iteration 5/1000 | Loss: 0.00001787
Iteration 6/1000 | Loss: 0.00001695
Iteration 7/1000 | Loss: 0.00001631
Iteration 8/1000 | Loss: 0.00001582
Iteration 9/1000 | Loss: 0.00001559
Iteration 10/1000 | Loss: 0.00001535
Iteration 11/1000 | Loss: 0.00001527
Iteration 12/1000 | Loss: 0.00001524
Iteration 13/1000 | Loss: 0.00001523
Iteration 14/1000 | Loss: 0.00001508
Iteration 15/1000 | Loss: 0.00001505
Iteration 16/1000 | Loss: 0.00001501
Iteration 17/1000 | Loss: 0.00001501
Iteration 18/1000 | Loss: 0.00001499
Iteration 19/1000 | Loss: 0.00001499
Iteration 20/1000 | Loss: 0.00001498
Iteration 21/1000 | Loss: 0.00001498
Iteration 22/1000 | Loss: 0.00001494
Iteration 23/1000 | Loss: 0.00001493
Iteration 24/1000 | Loss: 0.00001491
Iteration 25/1000 | Loss: 0.00001488
Iteration 26/1000 | Loss: 0.00001487
Iteration 27/1000 | Loss: 0.00001486
Iteration 28/1000 | Loss: 0.00001483
Iteration 29/1000 | Loss: 0.00001483
Iteration 30/1000 | Loss: 0.00001483
Iteration 31/1000 | Loss: 0.00001483
Iteration 32/1000 | Loss: 0.00001483
Iteration 33/1000 | Loss: 0.00001482
Iteration 34/1000 | Loss: 0.00001482
Iteration 35/1000 | Loss: 0.00001482
Iteration 36/1000 | Loss: 0.00001481
Iteration 37/1000 | Loss: 0.00001481
Iteration 38/1000 | Loss: 0.00001475
Iteration 39/1000 | Loss: 0.00001474
Iteration 40/1000 | Loss: 0.00001473
Iteration 41/1000 | Loss: 0.00001473
Iteration 42/1000 | Loss: 0.00001473
Iteration 43/1000 | Loss: 0.00001473
Iteration 44/1000 | Loss: 0.00001472
Iteration 45/1000 | Loss: 0.00001471
Iteration 46/1000 | Loss: 0.00001471
Iteration 47/1000 | Loss: 0.00001470
Iteration 48/1000 | Loss: 0.00001470
Iteration 49/1000 | Loss: 0.00001470
Iteration 50/1000 | Loss: 0.00001470
Iteration 51/1000 | Loss: 0.00001470
Iteration 52/1000 | Loss: 0.00001469
Iteration 53/1000 | Loss: 0.00001469
Iteration 54/1000 | Loss: 0.00001469
Iteration 55/1000 | Loss: 0.00001467
Iteration 56/1000 | Loss: 0.00001466
Iteration 57/1000 | Loss: 0.00001465
Iteration 58/1000 | Loss: 0.00001465
Iteration 59/1000 | Loss: 0.00001465
Iteration 60/1000 | Loss: 0.00001464
Iteration 61/1000 | Loss: 0.00001463
Iteration 62/1000 | Loss: 0.00001463
Iteration 63/1000 | Loss: 0.00001462
Iteration 64/1000 | Loss: 0.00001461
Iteration 65/1000 | Loss: 0.00001461
Iteration 66/1000 | Loss: 0.00001461
Iteration 67/1000 | Loss: 0.00001460
Iteration 68/1000 | Loss: 0.00001460
Iteration 69/1000 | Loss: 0.00001459
Iteration 70/1000 | Loss: 0.00001458
Iteration 71/1000 | Loss: 0.00001457
Iteration 72/1000 | Loss: 0.00001453
Iteration 73/1000 | Loss: 0.00001451
Iteration 74/1000 | Loss: 0.00001451
Iteration 75/1000 | Loss: 0.00001449
Iteration 76/1000 | Loss: 0.00001449
Iteration 77/1000 | Loss: 0.00001449
Iteration 78/1000 | Loss: 0.00001449
Iteration 79/1000 | Loss: 0.00001449
Iteration 80/1000 | Loss: 0.00001448
Iteration 81/1000 | Loss: 0.00001448
Iteration 82/1000 | Loss: 0.00001447
Iteration 83/1000 | Loss: 0.00001447
Iteration 84/1000 | Loss: 0.00001446
Iteration 85/1000 | Loss: 0.00001446
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001445
Iteration 88/1000 | Loss: 0.00001445
Iteration 89/1000 | Loss: 0.00001445
Iteration 90/1000 | Loss: 0.00001444
Iteration 91/1000 | Loss: 0.00001443
Iteration 92/1000 | Loss: 0.00001443
Iteration 93/1000 | Loss: 0.00001443
Iteration 94/1000 | Loss: 0.00001443
Iteration 95/1000 | Loss: 0.00001442
Iteration 96/1000 | Loss: 0.00001442
Iteration 97/1000 | Loss: 0.00001442
Iteration 98/1000 | Loss: 0.00001442
Iteration 99/1000 | Loss: 0.00001441
Iteration 100/1000 | Loss: 0.00001441
Iteration 101/1000 | Loss: 0.00001441
Iteration 102/1000 | Loss: 0.00001441
Iteration 103/1000 | Loss: 0.00001441
Iteration 104/1000 | Loss: 0.00001441
Iteration 105/1000 | Loss: 0.00001440
Iteration 106/1000 | Loss: 0.00001440
Iteration 107/1000 | Loss: 0.00001440
Iteration 108/1000 | Loss: 0.00001440
Iteration 109/1000 | Loss: 0.00001440
Iteration 110/1000 | Loss: 0.00001440
Iteration 111/1000 | Loss: 0.00001440
Iteration 112/1000 | Loss: 0.00001439
Iteration 113/1000 | Loss: 0.00001439
Iteration 114/1000 | Loss: 0.00001439
Iteration 115/1000 | Loss: 0.00001439
Iteration 116/1000 | Loss: 0.00001439
Iteration 117/1000 | Loss: 0.00001438
Iteration 118/1000 | Loss: 0.00001438
Iteration 119/1000 | Loss: 0.00001438
Iteration 120/1000 | Loss: 0.00001438
Iteration 121/1000 | Loss: 0.00001438
Iteration 122/1000 | Loss: 0.00001438
Iteration 123/1000 | Loss: 0.00001438
Iteration 124/1000 | Loss: 0.00001437
Iteration 125/1000 | Loss: 0.00001437
Iteration 126/1000 | Loss: 0.00001437
Iteration 127/1000 | Loss: 0.00001437
Iteration 128/1000 | Loss: 0.00001437
Iteration 129/1000 | Loss: 0.00001437
Iteration 130/1000 | Loss: 0.00001437
Iteration 131/1000 | Loss: 0.00001437
Iteration 132/1000 | Loss: 0.00001437
Iteration 133/1000 | Loss: 0.00001436
Iteration 134/1000 | Loss: 0.00001436
Iteration 135/1000 | Loss: 0.00001436
Iteration 136/1000 | Loss: 0.00001436
Iteration 137/1000 | Loss: 0.00001436
Iteration 138/1000 | Loss: 0.00001436
Iteration 139/1000 | Loss: 0.00001436
Iteration 140/1000 | Loss: 0.00001436
Iteration 141/1000 | Loss: 0.00001436
Iteration 142/1000 | Loss: 0.00001436
Iteration 143/1000 | Loss: 0.00001436
Iteration 144/1000 | Loss: 0.00001436
Iteration 145/1000 | Loss: 0.00001436
Iteration 146/1000 | Loss: 0.00001435
Iteration 147/1000 | Loss: 0.00001435
Iteration 148/1000 | Loss: 0.00001435
Iteration 149/1000 | Loss: 0.00001435
Iteration 150/1000 | Loss: 0.00001435
Iteration 151/1000 | Loss: 0.00001435
Iteration 152/1000 | Loss: 0.00001435
Iteration 153/1000 | Loss: 0.00001435
Iteration 154/1000 | Loss: 0.00001435
Iteration 155/1000 | Loss: 0.00001435
Iteration 156/1000 | Loss: 0.00001435
Iteration 157/1000 | Loss: 0.00001435
Iteration 158/1000 | Loss: 0.00001435
Iteration 159/1000 | Loss: 0.00001435
Iteration 160/1000 | Loss: 0.00001435
Iteration 161/1000 | Loss: 0.00001435
Iteration 162/1000 | Loss: 0.00001435
Iteration 163/1000 | Loss: 0.00001435
Iteration 164/1000 | Loss: 0.00001435
Iteration 165/1000 | Loss: 0.00001435
Iteration 166/1000 | Loss: 0.00001435
Iteration 167/1000 | Loss: 0.00001435
Iteration 168/1000 | Loss: 0.00001435
Iteration 169/1000 | Loss: 0.00001435
Iteration 170/1000 | Loss: 0.00001435
Iteration 171/1000 | Loss: 0.00001435
Iteration 172/1000 | Loss: 0.00001435
Iteration 173/1000 | Loss: 0.00001435
Iteration 174/1000 | Loss: 0.00001435
Iteration 175/1000 | Loss: 0.00001435
Iteration 176/1000 | Loss: 0.00001435
Iteration 177/1000 | Loss: 0.00001435
Iteration 178/1000 | Loss: 0.00001435
Iteration 179/1000 | Loss: 0.00001435
Iteration 180/1000 | Loss: 0.00001435
Iteration 181/1000 | Loss: 0.00001435
Iteration 182/1000 | Loss: 0.00001435
Iteration 183/1000 | Loss: 0.00001435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.4347653632285073e-05, 1.4347653632285073e-05, 1.4347653632285073e-05, 1.4347653632285073e-05, 1.4347653632285073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4347653632285073e-05

Optimization complete. Final v2v error: 3.237471103668213 mm

Highest mean error: 3.3654658794403076 mm for frame 29

Lowest mean error: 3.0804948806762695 mm for frame 0

Saving results

Total time: 39.49209713935852
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469390
Iteration 2/25 | Loss: 0.00127869
Iteration 3/25 | Loss: 0.00120572
Iteration 4/25 | Loss: 0.00119319
Iteration 5/25 | Loss: 0.00118952
Iteration 6/25 | Loss: 0.00118870
Iteration 7/25 | Loss: 0.00118870
Iteration 8/25 | Loss: 0.00118870
Iteration 9/25 | Loss: 0.00118870
Iteration 10/25 | Loss: 0.00118870
Iteration 11/25 | Loss: 0.00118870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001188704278320074, 0.001188704278320074, 0.001188704278320074, 0.001188704278320074, 0.001188704278320074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001188704278320074

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16786194
Iteration 2/25 | Loss: 0.00052997
Iteration 3/25 | Loss: 0.00052992
Iteration 4/25 | Loss: 0.00052992
Iteration 5/25 | Loss: 0.00052992
Iteration 6/25 | Loss: 0.00052992
Iteration 7/25 | Loss: 0.00052992
Iteration 8/25 | Loss: 0.00052992
Iteration 9/25 | Loss: 0.00052992
Iteration 10/25 | Loss: 0.00052992
Iteration 11/25 | Loss: 0.00052992
Iteration 12/25 | Loss: 0.00052992
Iteration 13/25 | Loss: 0.00052992
Iteration 14/25 | Loss: 0.00052992
Iteration 15/25 | Loss: 0.00052992
Iteration 16/25 | Loss: 0.00052992
Iteration 17/25 | Loss: 0.00052992
Iteration 18/25 | Loss: 0.00052992
Iteration 19/25 | Loss: 0.00052992
Iteration 20/25 | Loss: 0.00052992
Iteration 21/25 | Loss: 0.00052992
Iteration 22/25 | Loss: 0.00052992
Iteration 23/25 | Loss: 0.00052992
Iteration 24/25 | Loss: 0.00052992
Iteration 25/25 | Loss: 0.00052992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052992
Iteration 2/1000 | Loss: 0.00004470
Iteration 3/1000 | Loss: 0.00003151
Iteration 4/1000 | Loss: 0.00002603
Iteration 5/1000 | Loss: 0.00002461
Iteration 6/1000 | Loss: 0.00002325
Iteration 7/1000 | Loss: 0.00002219
Iteration 8/1000 | Loss: 0.00002161
Iteration 9/1000 | Loss: 0.00002111
Iteration 10/1000 | Loss: 0.00002079
Iteration 11/1000 | Loss: 0.00002054
Iteration 12/1000 | Loss: 0.00002018
Iteration 13/1000 | Loss: 0.00002002
Iteration 14/1000 | Loss: 0.00001976
Iteration 15/1000 | Loss: 0.00001966
Iteration 16/1000 | Loss: 0.00001966
Iteration 17/1000 | Loss: 0.00001966
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001964
Iteration 20/1000 | Loss: 0.00001964
Iteration 21/1000 | Loss: 0.00001964
Iteration 22/1000 | Loss: 0.00001963
Iteration 23/1000 | Loss: 0.00001963
Iteration 24/1000 | Loss: 0.00001963
Iteration 25/1000 | Loss: 0.00001962
Iteration 26/1000 | Loss: 0.00001962
Iteration 27/1000 | Loss: 0.00001961
Iteration 28/1000 | Loss: 0.00001960
Iteration 29/1000 | Loss: 0.00001960
Iteration 30/1000 | Loss: 0.00001960
Iteration 31/1000 | Loss: 0.00001959
Iteration 32/1000 | Loss: 0.00001959
Iteration 33/1000 | Loss: 0.00001959
Iteration 34/1000 | Loss: 0.00001959
Iteration 35/1000 | Loss: 0.00001958
Iteration 36/1000 | Loss: 0.00001958
Iteration 37/1000 | Loss: 0.00001958
Iteration 38/1000 | Loss: 0.00001958
Iteration 39/1000 | Loss: 0.00001958
Iteration 40/1000 | Loss: 0.00001958
Iteration 41/1000 | Loss: 0.00001958
Iteration 42/1000 | Loss: 0.00001957
Iteration 43/1000 | Loss: 0.00001957
Iteration 44/1000 | Loss: 0.00001957
Iteration 45/1000 | Loss: 0.00001957
Iteration 46/1000 | Loss: 0.00001957
Iteration 47/1000 | Loss: 0.00001957
Iteration 48/1000 | Loss: 0.00001956
Iteration 49/1000 | Loss: 0.00001956
Iteration 50/1000 | Loss: 0.00001956
Iteration 51/1000 | Loss: 0.00001956
Iteration 52/1000 | Loss: 0.00001956
Iteration 53/1000 | Loss: 0.00001956
Iteration 54/1000 | Loss: 0.00001955
Iteration 55/1000 | Loss: 0.00001955
Iteration 56/1000 | Loss: 0.00001954
Iteration 57/1000 | Loss: 0.00001953
Iteration 58/1000 | Loss: 0.00001953
Iteration 59/1000 | Loss: 0.00001952
Iteration 60/1000 | Loss: 0.00001952
Iteration 61/1000 | Loss: 0.00001951
Iteration 62/1000 | Loss: 0.00001951
Iteration 63/1000 | Loss: 0.00001951
Iteration 64/1000 | Loss: 0.00001950
Iteration 65/1000 | Loss: 0.00001950
Iteration 66/1000 | Loss: 0.00001949
Iteration 67/1000 | Loss: 0.00001949
Iteration 68/1000 | Loss: 0.00001948
Iteration 69/1000 | Loss: 0.00001948
Iteration 70/1000 | Loss: 0.00001947
Iteration 71/1000 | Loss: 0.00001947
Iteration 72/1000 | Loss: 0.00001947
Iteration 73/1000 | Loss: 0.00001946
Iteration 74/1000 | Loss: 0.00001946
Iteration 75/1000 | Loss: 0.00001946
Iteration 76/1000 | Loss: 0.00001945
Iteration 77/1000 | Loss: 0.00001945
Iteration 78/1000 | Loss: 0.00001945
Iteration 79/1000 | Loss: 0.00001945
Iteration 80/1000 | Loss: 0.00001945
Iteration 81/1000 | Loss: 0.00001945
Iteration 82/1000 | Loss: 0.00001945
Iteration 83/1000 | Loss: 0.00001945
Iteration 84/1000 | Loss: 0.00001944
Iteration 85/1000 | Loss: 0.00001944
Iteration 86/1000 | Loss: 0.00001944
Iteration 87/1000 | Loss: 0.00001944
Iteration 88/1000 | Loss: 0.00001944
Iteration 89/1000 | Loss: 0.00001944
Iteration 90/1000 | Loss: 0.00001944
Iteration 91/1000 | Loss: 0.00001944
Iteration 92/1000 | Loss: 0.00001944
Iteration 93/1000 | Loss: 0.00001943
Iteration 94/1000 | Loss: 0.00001943
Iteration 95/1000 | Loss: 0.00001943
Iteration 96/1000 | Loss: 0.00001942
Iteration 97/1000 | Loss: 0.00001942
Iteration 98/1000 | Loss: 0.00001942
Iteration 99/1000 | Loss: 0.00001941
Iteration 100/1000 | Loss: 0.00001941
Iteration 101/1000 | Loss: 0.00001941
Iteration 102/1000 | Loss: 0.00001941
Iteration 103/1000 | Loss: 0.00001941
Iteration 104/1000 | Loss: 0.00001941
Iteration 105/1000 | Loss: 0.00001941
Iteration 106/1000 | Loss: 0.00001941
Iteration 107/1000 | Loss: 0.00001941
Iteration 108/1000 | Loss: 0.00001940
Iteration 109/1000 | Loss: 0.00001940
Iteration 110/1000 | Loss: 0.00001940
Iteration 111/1000 | Loss: 0.00001940
Iteration 112/1000 | Loss: 0.00001940
Iteration 113/1000 | Loss: 0.00001940
Iteration 114/1000 | Loss: 0.00001940
Iteration 115/1000 | Loss: 0.00001940
Iteration 116/1000 | Loss: 0.00001940
Iteration 117/1000 | Loss: 0.00001940
Iteration 118/1000 | Loss: 0.00001940
Iteration 119/1000 | Loss: 0.00001940
Iteration 120/1000 | Loss: 0.00001940
Iteration 121/1000 | Loss: 0.00001940
Iteration 122/1000 | Loss: 0.00001940
Iteration 123/1000 | Loss: 0.00001940
Iteration 124/1000 | Loss: 0.00001940
Iteration 125/1000 | Loss: 0.00001940
Iteration 126/1000 | Loss: 0.00001940
Iteration 127/1000 | Loss: 0.00001940
Iteration 128/1000 | Loss: 0.00001940
Iteration 129/1000 | Loss: 0.00001940
Iteration 130/1000 | Loss: 0.00001940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.9401557437959127e-05, 1.9401557437959127e-05, 1.9401557437959127e-05, 1.9401557437959127e-05, 1.9401557437959127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9401557437959127e-05

Optimization complete. Final v2v error: 3.708094835281372 mm

Highest mean error: 3.7358622550964355 mm for frame 25

Lowest mean error: 3.6836564540863037 mm for frame 94

Saving results

Total time: 35.263548135757446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00585932
Iteration 2/25 | Loss: 0.00152221
Iteration 3/25 | Loss: 0.00131754
Iteration 4/25 | Loss: 0.00123136
Iteration 5/25 | Loss: 0.00122616
Iteration 6/25 | Loss: 0.00122806
Iteration 7/25 | Loss: 0.00122645
Iteration 8/25 | Loss: 0.00122460
Iteration 9/25 | Loss: 0.00122372
Iteration 10/25 | Loss: 0.00122311
Iteration 11/25 | Loss: 0.00122270
Iteration 12/25 | Loss: 0.00122252
Iteration 13/25 | Loss: 0.00122248
Iteration 14/25 | Loss: 0.00122248
Iteration 15/25 | Loss: 0.00122248
Iteration 16/25 | Loss: 0.00122248
Iteration 17/25 | Loss: 0.00122248
Iteration 18/25 | Loss: 0.00122248
Iteration 19/25 | Loss: 0.00122247
Iteration 20/25 | Loss: 0.00122247
Iteration 21/25 | Loss: 0.00122247
Iteration 22/25 | Loss: 0.00122247
Iteration 23/25 | Loss: 0.00122247
Iteration 24/25 | Loss: 0.00122247
Iteration 25/25 | Loss: 0.00122247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.60681820
Iteration 2/25 | Loss: 0.00078824
Iteration 3/25 | Loss: 0.00081306
Iteration 4/25 | Loss: 0.00081306
Iteration 5/25 | Loss: 0.00081306
Iteration 6/25 | Loss: 0.00078824
Iteration 7/25 | Loss: 0.00078824
Iteration 8/25 | Loss: 0.00078824
Iteration 9/25 | Loss: 0.00078824
Iteration 10/25 | Loss: 0.00078824
Iteration 11/25 | Loss: 0.00078824
Iteration 12/25 | Loss: 0.00078824
Iteration 13/25 | Loss: 0.00078824
Iteration 14/25 | Loss: 0.00078824
Iteration 15/25 | Loss: 0.00078824
Iteration 16/25 | Loss: 0.00078824
Iteration 17/25 | Loss: 0.00078824
Iteration 18/25 | Loss: 0.00078824
Iteration 19/25 | Loss: 0.00078824
Iteration 20/25 | Loss: 0.00078824
Iteration 21/25 | Loss: 0.00078824
Iteration 22/25 | Loss: 0.00078824
Iteration 23/25 | Loss: 0.00078824
Iteration 24/25 | Loss: 0.00078824
Iteration 25/25 | Loss: 0.00078824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078824
Iteration 2/1000 | Loss: 0.00005578
Iteration 3/1000 | Loss: 0.00001856
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001447
Iteration 6/1000 | Loss: 0.00001386
Iteration 7/1000 | Loss: 0.00001346
Iteration 8/1000 | Loss: 0.00001343
Iteration 9/1000 | Loss: 0.00001334
Iteration 10/1000 | Loss: 0.00001329
Iteration 11/1000 | Loss: 0.00001328
Iteration 12/1000 | Loss: 0.00001309
Iteration 13/1000 | Loss: 0.00001294
Iteration 14/1000 | Loss: 0.00001288
Iteration 15/1000 | Loss: 0.00001280
Iteration 16/1000 | Loss: 0.00001277
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001269
Iteration 19/1000 | Loss: 0.00001269
Iteration 20/1000 | Loss: 0.00001266
Iteration 21/1000 | Loss: 0.00001266
Iteration 22/1000 | Loss: 0.00001265
Iteration 23/1000 | Loss: 0.00001264
Iteration 24/1000 | Loss: 0.00001264
Iteration 25/1000 | Loss: 0.00001263
Iteration 26/1000 | Loss: 0.00001262
Iteration 27/1000 | Loss: 0.00001262
Iteration 28/1000 | Loss: 0.00001261
Iteration 29/1000 | Loss: 0.00001261
Iteration 30/1000 | Loss: 0.00001260
Iteration 31/1000 | Loss: 0.00001259
Iteration 32/1000 | Loss: 0.00001259
Iteration 33/1000 | Loss: 0.00001259
Iteration 34/1000 | Loss: 0.00001258
Iteration 35/1000 | Loss: 0.00001258
Iteration 36/1000 | Loss: 0.00001258
Iteration 37/1000 | Loss: 0.00001258
Iteration 38/1000 | Loss: 0.00001258
Iteration 39/1000 | Loss: 0.00001258
Iteration 40/1000 | Loss: 0.00001258
Iteration 41/1000 | Loss: 0.00001258
Iteration 42/1000 | Loss: 0.00001258
Iteration 43/1000 | Loss: 0.00001258
Iteration 44/1000 | Loss: 0.00001257
Iteration 45/1000 | Loss: 0.00001257
Iteration 46/1000 | Loss: 0.00001256
Iteration 47/1000 | Loss: 0.00001256
Iteration 48/1000 | Loss: 0.00001255
Iteration 49/1000 | Loss: 0.00001255
Iteration 50/1000 | Loss: 0.00001255
Iteration 51/1000 | Loss: 0.00001255
Iteration 52/1000 | Loss: 0.00001255
Iteration 53/1000 | Loss: 0.00001254
Iteration 54/1000 | Loss: 0.00001254
Iteration 55/1000 | Loss: 0.00001254
Iteration 56/1000 | Loss: 0.00001253
Iteration 57/1000 | Loss: 0.00001253
Iteration 58/1000 | Loss: 0.00001252
Iteration 59/1000 | Loss: 0.00001252
Iteration 60/1000 | Loss: 0.00001252
Iteration 61/1000 | Loss: 0.00001252
Iteration 62/1000 | Loss: 0.00001252
Iteration 63/1000 | Loss: 0.00001252
Iteration 64/1000 | Loss: 0.00001252
Iteration 65/1000 | Loss: 0.00001252
Iteration 66/1000 | Loss: 0.00001251
Iteration 67/1000 | Loss: 0.00001251
Iteration 68/1000 | Loss: 0.00001251
Iteration 69/1000 | Loss: 0.00001251
Iteration 70/1000 | Loss: 0.00001251
Iteration 71/1000 | Loss: 0.00001250
Iteration 72/1000 | Loss: 0.00001250
Iteration 73/1000 | Loss: 0.00001249
Iteration 74/1000 | Loss: 0.00001249
Iteration 75/1000 | Loss: 0.00001249
Iteration 76/1000 | Loss: 0.00001249
Iteration 77/1000 | Loss: 0.00001249
Iteration 78/1000 | Loss: 0.00001249
Iteration 79/1000 | Loss: 0.00001249
Iteration 80/1000 | Loss: 0.00001248
Iteration 81/1000 | Loss: 0.00001248
Iteration 82/1000 | Loss: 0.00001248
Iteration 83/1000 | Loss: 0.00001248
Iteration 84/1000 | Loss: 0.00001248
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001248
Iteration 88/1000 | Loss: 0.00001248
Iteration 89/1000 | Loss: 0.00001247
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00001246
Iteration 93/1000 | Loss: 0.00001245
Iteration 94/1000 | Loss: 0.00001245
Iteration 95/1000 | Loss: 0.00001244
Iteration 96/1000 | Loss: 0.00001244
Iteration 97/1000 | Loss: 0.00001244
Iteration 98/1000 | Loss: 0.00001244
Iteration 99/1000 | Loss: 0.00001243
Iteration 100/1000 | Loss: 0.00001243
Iteration 101/1000 | Loss: 0.00001243
Iteration 102/1000 | Loss: 0.00001243
Iteration 103/1000 | Loss: 0.00001242
Iteration 104/1000 | Loss: 0.00001242
Iteration 105/1000 | Loss: 0.00001242
Iteration 106/1000 | Loss: 0.00001241
Iteration 107/1000 | Loss: 0.00001241
Iteration 108/1000 | Loss: 0.00001241
Iteration 109/1000 | Loss: 0.00001241
Iteration 110/1000 | Loss: 0.00001241
Iteration 111/1000 | Loss: 0.00001241
Iteration 112/1000 | Loss: 0.00001241
Iteration 113/1000 | Loss: 0.00001241
Iteration 114/1000 | Loss: 0.00001241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.2410415365593508e-05, 1.2410415365593508e-05, 1.2410415365593508e-05, 1.2410415365593508e-05, 1.2410415365593508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2410415365593508e-05

Optimization complete. Final v2v error: 2.978074312210083 mm

Highest mean error: 3.4017159938812256 mm for frame 167

Lowest mean error: 2.6948931217193604 mm for frame 134

Saving results

Total time: 51.56274175643921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855495
Iteration 2/25 | Loss: 0.00245589
Iteration 3/25 | Loss: 0.00170787
Iteration 4/25 | Loss: 0.00155300
Iteration 5/25 | Loss: 0.00146720
Iteration 6/25 | Loss: 0.00137622
Iteration 7/25 | Loss: 0.00132969
Iteration 8/25 | Loss: 0.00129475
Iteration 9/25 | Loss: 0.00127192
Iteration 10/25 | Loss: 0.00125956
Iteration 11/25 | Loss: 0.00125672
Iteration 12/25 | Loss: 0.00125264
Iteration 13/25 | Loss: 0.00125196
Iteration 14/25 | Loss: 0.00125306
Iteration 15/25 | Loss: 0.00125046
Iteration 16/25 | Loss: 0.00124942
Iteration 17/25 | Loss: 0.00124867
Iteration 18/25 | Loss: 0.00124660
Iteration 19/25 | Loss: 0.00124640
Iteration 20/25 | Loss: 0.00124629
Iteration 21/25 | Loss: 0.00124628
Iteration 22/25 | Loss: 0.00124628
Iteration 23/25 | Loss: 0.00124628
Iteration 24/25 | Loss: 0.00124628
Iteration 25/25 | Loss: 0.00124627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94797969
Iteration 2/25 | Loss: 0.00074956
Iteration 3/25 | Loss: 0.00074956
Iteration 4/25 | Loss: 0.00074956
Iteration 5/25 | Loss: 0.00074956
Iteration 6/25 | Loss: 0.00074956
Iteration 7/25 | Loss: 0.00074956
Iteration 8/25 | Loss: 0.00074956
Iteration 9/25 | Loss: 0.00074956
Iteration 10/25 | Loss: 0.00074956
Iteration 11/25 | Loss: 0.00074956
Iteration 12/25 | Loss: 0.00074956
Iteration 13/25 | Loss: 0.00074956
Iteration 14/25 | Loss: 0.00074956
Iteration 15/25 | Loss: 0.00074956
Iteration 16/25 | Loss: 0.00074956
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007495554746128619, 0.0007495554746128619, 0.0007495554746128619, 0.0007495554746128619, 0.0007495554746128619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007495554746128619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074956
Iteration 2/1000 | Loss: 0.00005537
Iteration 3/1000 | Loss: 0.00003038
Iteration 4/1000 | Loss: 0.00002555
Iteration 5/1000 | Loss: 0.00002429
Iteration 6/1000 | Loss: 0.00002352
Iteration 7/1000 | Loss: 0.00002286
Iteration 8/1000 | Loss: 0.00002252
Iteration 9/1000 | Loss: 0.00002218
Iteration 10/1000 | Loss: 0.00002192
Iteration 11/1000 | Loss: 0.00002173
Iteration 12/1000 | Loss: 0.00002165
Iteration 13/1000 | Loss: 0.00002154
Iteration 14/1000 | Loss: 0.00002151
Iteration 15/1000 | Loss: 0.00002140
Iteration 16/1000 | Loss: 0.00002130
Iteration 17/1000 | Loss: 0.00002127
Iteration 18/1000 | Loss: 0.00002125
Iteration 19/1000 | Loss: 0.00002124
Iteration 20/1000 | Loss: 0.00002122
Iteration 21/1000 | Loss: 0.00002122
Iteration 22/1000 | Loss: 0.00002114
Iteration 23/1000 | Loss: 0.00002111
Iteration 24/1000 | Loss: 0.00002110
Iteration 25/1000 | Loss: 0.00002110
Iteration 26/1000 | Loss: 0.00002110
Iteration 27/1000 | Loss: 0.00002110
Iteration 28/1000 | Loss: 0.00002110
Iteration 29/1000 | Loss: 0.00002110
Iteration 30/1000 | Loss: 0.00002109
Iteration 31/1000 | Loss: 0.00002109
Iteration 32/1000 | Loss: 0.00002109
Iteration 33/1000 | Loss: 0.00002108
Iteration 34/1000 | Loss: 0.00002108
Iteration 35/1000 | Loss: 0.00002108
Iteration 36/1000 | Loss: 0.00002107
Iteration 37/1000 | Loss: 0.00002107
Iteration 38/1000 | Loss: 0.00002107
Iteration 39/1000 | Loss: 0.00002104
Iteration 40/1000 | Loss: 0.00002104
Iteration 41/1000 | Loss: 0.00002104
Iteration 42/1000 | Loss: 0.00002103
Iteration 43/1000 | Loss: 0.00002102
Iteration 44/1000 | Loss: 0.00002101
Iteration 45/1000 | Loss: 0.00002100
Iteration 46/1000 | Loss: 0.00002100
Iteration 47/1000 | Loss: 0.00002099
Iteration 48/1000 | Loss: 0.00002099
Iteration 49/1000 | Loss: 0.00002099
Iteration 50/1000 | Loss: 0.00002098
Iteration 51/1000 | Loss: 0.00002098
Iteration 52/1000 | Loss: 0.00002098
Iteration 53/1000 | Loss: 0.00002097
Iteration 54/1000 | Loss: 0.00002096
Iteration 55/1000 | Loss: 0.00002096
Iteration 56/1000 | Loss: 0.00002095
Iteration 57/1000 | Loss: 0.00002095
Iteration 58/1000 | Loss: 0.00002095
Iteration 59/1000 | Loss: 0.00002094
Iteration 60/1000 | Loss: 0.00002094
Iteration 61/1000 | Loss: 0.00002094
Iteration 62/1000 | Loss: 0.00002093
Iteration 63/1000 | Loss: 0.00002092
Iteration 64/1000 | Loss: 0.00002092
Iteration 65/1000 | Loss: 0.00002091
Iteration 66/1000 | Loss: 0.00002091
Iteration 67/1000 | Loss: 0.00002091
Iteration 68/1000 | Loss: 0.00002090
Iteration 69/1000 | Loss: 0.00002090
Iteration 70/1000 | Loss: 0.00002090
Iteration 71/1000 | Loss: 0.00002089
Iteration 72/1000 | Loss: 0.00002089
Iteration 73/1000 | Loss: 0.00002089
Iteration 74/1000 | Loss: 0.00002089
Iteration 75/1000 | Loss: 0.00002088
Iteration 76/1000 | Loss: 0.00002088
Iteration 77/1000 | Loss: 0.00002088
Iteration 78/1000 | Loss: 0.00002088
Iteration 79/1000 | Loss: 0.00002088
Iteration 80/1000 | Loss: 0.00002088
Iteration 81/1000 | Loss: 0.00002088
Iteration 82/1000 | Loss: 0.00002088
Iteration 83/1000 | Loss: 0.00002088
Iteration 84/1000 | Loss: 0.00002088
Iteration 85/1000 | Loss: 0.00002088
Iteration 86/1000 | Loss: 0.00002088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [2.0880028387182392e-05, 2.0880028387182392e-05, 2.0880028387182392e-05, 2.0880028387182392e-05, 2.0880028387182392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0880028387182392e-05

Optimization complete. Final v2v error: 3.77864670753479 mm

Highest mean error: 4.780580043792725 mm for frame 141

Lowest mean error: 3.2682745456695557 mm for frame 63

Saving results

Total time: 69.83765363693237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799071
Iteration 2/25 | Loss: 0.00121334
Iteration 3/25 | Loss: 0.00109286
Iteration 4/25 | Loss: 0.00107490
Iteration 5/25 | Loss: 0.00107098
Iteration 6/25 | Loss: 0.00107087
Iteration 7/25 | Loss: 0.00107087
Iteration 8/25 | Loss: 0.00107087
Iteration 9/25 | Loss: 0.00107087
Iteration 10/25 | Loss: 0.00107087
Iteration 11/25 | Loss: 0.00107087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001070873229764402, 0.001070873229764402, 0.001070873229764402, 0.001070873229764402, 0.001070873229764402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001070873229764402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34657764
Iteration 2/25 | Loss: 0.00084284
Iteration 3/25 | Loss: 0.00084284
Iteration 4/25 | Loss: 0.00084284
Iteration 5/25 | Loss: 0.00084284
Iteration 6/25 | Loss: 0.00084284
Iteration 7/25 | Loss: 0.00084284
Iteration 8/25 | Loss: 0.00084284
Iteration 9/25 | Loss: 0.00084284
Iteration 10/25 | Loss: 0.00084284
Iteration 11/25 | Loss: 0.00084284
Iteration 12/25 | Loss: 0.00084284
Iteration 13/25 | Loss: 0.00084284
Iteration 14/25 | Loss: 0.00084283
Iteration 15/25 | Loss: 0.00084283
Iteration 16/25 | Loss: 0.00084283
Iteration 17/25 | Loss: 0.00084283
Iteration 18/25 | Loss: 0.00084283
Iteration 19/25 | Loss: 0.00084283
Iteration 20/25 | Loss: 0.00084283
Iteration 21/25 | Loss: 0.00084283
Iteration 22/25 | Loss: 0.00084283
Iteration 23/25 | Loss: 0.00084283
Iteration 24/25 | Loss: 0.00084283
Iteration 25/25 | Loss: 0.00084283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084283
Iteration 2/1000 | Loss: 0.00002036
Iteration 3/1000 | Loss: 0.00001464
Iteration 4/1000 | Loss: 0.00001321
Iteration 5/1000 | Loss: 0.00001217
Iteration 6/1000 | Loss: 0.00001140
Iteration 7/1000 | Loss: 0.00001103
Iteration 8/1000 | Loss: 0.00001076
Iteration 9/1000 | Loss: 0.00001044
Iteration 10/1000 | Loss: 0.00001028
Iteration 11/1000 | Loss: 0.00001026
Iteration 12/1000 | Loss: 0.00001025
Iteration 13/1000 | Loss: 0.00001022
Iteration 14/1000 | Loss: 0.00001021
Iteration 15/1000 | Loss: 0.00001018
Iteration 16/1000 | Loss: 0.00001018
Iteration 17/1000 | Loss: 0.00001011
Iteration 18/1000 | Loss: 0.00001010
Iteration 19/1000 | Loss: 0.00001009
Iteration 20/1000 | Loss: 0.00001008
Iteration 21/1000 | Loss: 0.00001008
Iteration 22/1000 | Loss: 0.00001007
Iteration 23/1000 | Loss: 0.00001004
Iteration 24/1000 | Loss: 0.00001004
Iteration 25/1000 | Loss: 0.00001004
Iteration 26/1000 | Loss: 0.00001003
Iteration 27/1000 | Loss: 0.00001003
Iteration 28/1000 | Loss: 0.00001002
Iteration 29/1000 | Loss: 0.00001001
Iteration 30/1000 | Loss: 0.00001001
Iteration 31/1000 | Loss: 0.00001001
Iteration 32/1000 | Loss: 0.00001001
Iteration 33/1000 | Loss: 0.00001001
Iteration 34/1000 | Loss: 0.00001000
Iteration 35/1000 | Loss: 0.00001000
Iteration 36/1000 | Loss: 0.00001000
Iteration 37/1000 | Loss: 0.00001000
Iteration 38/1000 | Loss: 0.00000999
Iteration 39/1000 | Loss: 0.00000999
Iteration 40/1000 | Loss: 0.00000999
Iteration 41/1000 | Loss: 0.00000998
Iteration 42/1000 | Loss: 0.00000998
Iteration 43/1000 | Loss: 0.00000997
Iteration 44/1000 | Loss: 0.00000995
Iteration 45/1000 | Loss: 0.00000993
Iteration 46/1000 | Loss: 0.00000993
Iteration 47/1000 | Loss: 0.00000991
Iteration 48/1000 | Loss: 0.00000990
Iteration 49/1000 | Loss: 0.00000989
Iteration 50/1000 | Loss: 0.00000988
Iteration 51/1000 | Loss: 0.00000987
Iteration 52/1000 | Loss: 0.00000987
Iteration 53/1000 | Loss: 0.00000987
Iteration 54/1000 | Loss: 0.00000987
Iteration 55/1000 | Loss: 0.00000986
Iteration 56/1000 | Loss: 0.00000986
Iteration 57/1000 | Loss: 0.00000985
Iteration 58/1000 | Loss: 0.00000985
Iteration 59/1000 | Loss: 0.00000984
Iteration 60/1000 | Loss: 0.00000984
Iteration 61/1000 | Loss: 0.00000983
Iteration 62/1000 | Loss: 0.00000982
Iteration 63/1000 | Loss: 0.00000982
Iteration 64/1000 | Loss: 0.00000981
Iteration 65/1000 | Loss: 0.00000980
Iteration 66/1000 | Loss: 0.00000980
Iteration 67/1000 | Loss: 0.00000980
Iteration 68/1000 | Loss: 0.00000979
Iteration 69/1000 | Loss: 0.00000979
Iteration 70/1000 | Loss: 0.00000979
Iteration 71/1000 | Loss: 0.00000978
Iteration 72/1000 | Loss: 0.00000978
Iteration 73/1000 | Loss: 0.00000978
Iteration 74/1000 | Loss: 0.00000978
Iteration 75/1000 | Loss: 0.00000977
Iteration 76/1000 | Loss: 0.00000977
Iteration 77/1000 | Loss: 0.00000977
Iteration 78/1000 | Loss: 0.00000977
Iteration 79/1000 | Loss: 0.00000977
Iteration 80/1000 | Loss: 0.00000977
Iteration 81/1000 | Loss: 0.00000977
Iteration 82/1000 | Loss: 0.00000976
Iteration 83/1000 | Loss: 0.00000976
Iteration 84/1000 | Loss: 0.00000976
Iteration 85/1000 | Loss: 0.00000976
Iteration 86/1000 | Loss: 0.00000976
Iteration 87/1000 | Loss: 0.00000975
Iteration 88/1000 | Loss: 0.00000975
Iteration 89/1000 | Loss: 0.00000975
Iteration 90/1000 | Loss: 0.00000974
Iteration 91/1000 | Loss: 0.00000974
Iteration 92/1000 | Loss: 0.00000973
Iteration 93/1000 | Loss: 0.00000973
Iteration 94/1000 | Loss: 0.00000973
Iteration 95/1000 | Loss: 0.00000972
Iteration 96/1000 | Loss: 0.00000972
Iteration 97/1000 | Loss: 0.00000972
Iteration 98/1000 | Loss: 0.00000971
Iteration 99/1000 | Loss: 0.00000971
Iteration 100/1000 | Loss: 0.00000971
Iteration 101/1000 | Loss: 0.00000971
Iteration 102/1000 | Loss: 0.00000970
Iteration 103/1000 | Loss: 0.00000970
Iteration 104/1000 | Loss: 0.00000970
Iteration 105/1000 | Loss: 0.00000969
Iteration 106/1000 | Loss: 0.00000969
Iteration 107/1000 | Loss: 0.00000969
Iteration 108/1000 | Loss: 0.00000969
Iteration 109/1000 | Loss: 0.00000969
Iteration 110/1000 | Loss: 0.00000969
Iteration 111/1000 | Loss: 0.00000969
Iteration 112/1000 | Loss: 0.00000969
Iteration 113/1000 | Loss: 0.00000968
Iteration 114/1000 | Loss: 0.00000968
Iteration 115/1000 | Loss: 0.00000968
Iteration 116/1000 | Loss: 0.00000968
Iteration 117/1000 | Loss: 0.00000967
Iteration 118/1000 | Loss: 0.00000967
Iteration 119/1000 | Loss: 0.00000966
Iteration 120/1000 | Loss: 0.00000966
Iteration 121/1000 | Loss: 0.00000966
Iteration 122/1000 | Loss: 0.00000966
Iteration 123/1000 | Loss: 0.00000966
Iteration 124/1000 | Loss: 0.00000965
Iteration 125/1000 | Loss: 0.00000965
Iteration 126/1000 | Loss: 0.00000965
Iteration 127/1000 | Loss: 0.00000965
Iteration 128/1000 | Loss: 0.00000965
Iteration 129/1000 | Loss: 0.00000965
Iteration 130/1000 | Loss: 0.00000964
Iteration 131/1000 | Loss: 0.00000964
Iteration 132/1000 | Loss: 0.00000964
Iteration 133/1000 | Loss: 0.00000964
Iteration 134/1000 | Loss: 0.00000963
Iteration 135/1000 | Loss: 0.00000963
Iteration 136/1000 | Loss: 0.00000963
Iteration 137/1000 | Loss: 0.00000963
Iteration 138/1000 | Loss: 0.00000963
Iteration 139/1000 | Loss: 0.00000963
Iteration 140/1000 | Loss: 0.00000962
Iteration 141/1000 | Loss: 0.00000962
Iteration 142/1000 | Loss: 0.00000962
Iteration 143/1000 | Loss: 0.00000962
Iteration 144/1000 | Loss: 0.00000962
Iteration 145/1000 | Loss: 0.00000962
Iteration 146/1000 | Loss: 0.00000962
Iteration 147/1000 | Loss: 0.00000962
Iteration 148/1000 | Loss: 0.00000962
Iteration 149/1000 | Loss: 0.00000962
Iteration 150/1000 | Loss: 0.00000962
Iteration 151/1000 | Loss: 0.00000962
Iteration 152/1000 | Loss: 0.00000962
Iteration 153/1000 | Loss: 0.00000962
Iteration 154/1000 | Loss: 0.00000962
Iteration 155/1000 | Loss: 0.00000962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [9.620484888728242e-06, 9.620484888728242e-06, 9.620484888728242e-06, 9.620484888728242e-06, 9.620484888728242e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.620484888728242e-06

Optimization complete. Final v2v error: 2.6386590003967285 mm

Highest mean error: 3.026907205581665 mm for frame 105

Lowest mean error: 2.3949544429779053 mm for frame 249

Saving results

Total time: 40.665050745010376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016773
Iteration 2/25 | Loss: 0.00352032
Iteration 3/25 | Loss: 0.00272496
Iteration 4/25 | Loss: 0.00220032
Iteration 5/25 | Loss: 0.00210279
Iteration 6/25 | Loss: 0.00194234
Iteration 7/25 | Loss: 0.00180185
Iteration 8/25 | Loss: 0.00170335
Iteration 9/25 | Loss: 0.00160038
Iteration 10/25 | Loss: 0.00159616
Iteration 11/25 | Loss: 0.00154719
Iteration 12/25 | Loss: 0.00153968
Iteration 13/25 | Loss: 0.00152411
Iteration 14/25 | Loss: 0.00150597
Iteration 15/25 | Loss: 0.00149843
Iteration 16/25 | Loss: 0.00149630
Iteration 17/25 | Loss: 0.00149705
Iteration 18/25 | Loss: 0.00149909
Iteration 19/25 | Loss: 0.00149899
Iteration 20/25 | Loss: 0.00149634
Iteration 21/25 | Loss: 0.00149243
Iteration 22/25 | Loss: 0.00148929
Iteration 23/25 | Loss: 0.00148828
Iteration 24/25 | Loss: 0.00148772
Iteration 25/25 | Loss: 0.00148699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33006954
Iteration 2/25 | Loss: 0.00634835
Iteration 3/25 | Loss: 0.00493933
Iteration 4/25 | Loss: 0.00493933
Iteration 5/25 | Loss: 0.00493933
Iteration 6/25 | Loss: 0.00493932
Iteration 7/25 | Loss: 0.00493932
Iteration 8/25 | Loss: 0.00493932
Iteration 9/25 | Loss: 0.00493932
Iteration 10/25 | Loss: 0.00493932
Iteration 11/25 | Loss: 0.00493932
Iteration 12/25 | Loss: 0.00493932
Iteration 13/25 | Loss: 0.00493932
Iteration 14/25 | Loss: 0.00493932
Iteration 15/25 | Loss: 0.00493932
Iteration 16/25 | Loss: 0.00493932
Iteration 17/25 | Loss: 0.00493932
Iteration 18/25 | Loss: 0.00493932
Iteration 19/25 | Loss: 0.00493932
Iteration 20/25 | Loss: 0.00493932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004939321428537369, 0.004939321428537369, 0.004939321428537369, 0.004939321428537369, 0.004939321428537369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004939321428537369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00493932
Iteration 2/1000 | Loss: 0.00070257
Iteration 3/1000 | Loss: 0.00097029
Iteration 4/1000 | Loss: 0.00063251
Iteration 5/1000 | Loss: 0.00063983
Iteration 6/1000 | Loss: 0.00052676
Iteration 7/1000 | Loss: 0.00031628
Iteration 8/1000 | Loss: 0.00049359
Iteration 9/1000 | Loss: 0.00080959
Iteration 10/1000 | Loss: 0.00029195
Iteration 11/1000 | Loss: 0.00028205
Iteration 12/1000 | Loss: 0.00040577
Iteration 13/1000 | Loss: 0.00026949
Iteration 14/1000 | Loss: 0.00026390
Iteration 15/1000 | Loss: 0.00036425
Iteration 16/1000 | Loss: 0.00081734
Iteration 17/1000 | Loss: 0.00026837
Iteration 18/1000 | Loss: 0.00025801
Iteration 19/1000 | Loss: 0.00120991
Iteration 20/1000 | Loss: 0.00214461
Iteration 21/1000 | Loss: 0.00025321
Iteration 22/1000 | Loss: 0.00027014
Iteration 23/1000 | Loss: 0.00024567
Iteration 24/1000 | Loss: 0.00063278
Iteration 25/1000 | Loss: 0.00229173
Iteration 26/1000 | Loss: 0.00101734
Iteration 27/1000 | Loss: 0.00083784
Iteration 28/1000 | Loss: 0.00081959
Iteration 29/1000 | Loss: 0.00025093
Iteration 30/1000 | Loss: 0.00027336
Iteration 31/1000 | Loss: 0.00101830
Iteration 32/1000 | Loss: 0.00153293
Iteration 33/1000 | Loss: 0.00027080
Iteration 34/1000 | Loss: 0.00025692
Iteration 35/1000 | Loss: 0.00023883
Iteration 36/1000 | Loss: 0.00026604
Iteration 37/1000 | Loss: 0.00080282
Iteration 38/1000 | Loss: 0.00023320
Iteration 39/1000 | Loss: 0.00026768
Iteration 40/1000 | Loss: 0.00028009
Iteration 41/1000 | Loss: 0.00058648
Iteration 42/1000 | Loss: 0.00056760
Iteration 43/1000 | Loss: 0.00024351
Iteration 44/1000 | Loss: 0.00022770
Iteration 45/1000 | Loss: 0.00065204
Iteration 46/1000 | Loss: 0.00047755
Iteration 47/1000 | Loss: 0.00051565
Iteration 48/1000 | Loss: 0.00035165
Iteration 49/1000 | Loss: 0.00036530
Iteration 50/1000 | Loss: 0.00058420
Iteration 51/1000 | Loss: 0.00024290
Iteration 52/1000 | Loss: 0.00022108
Iteration 53/1000 | Loss: 0.00099460
Iteration 54/1000 | Loss: 0.00036905
Iteration 55/1000 | Loss: 0.00024044
Iteration 56/1000 | Loss: 0.00037301
Iteration 57/1000 | Loss: 0.00020920
Iteration 58/1000 | Loss: 0.00046187
Iteration 59/1000 | Loss: 0.00037938
Iteration 60/1000 | Loss: 0.00039643
Iteration 61/1000 | Loss: 0.00083217
Iteration 62/1000 | Loss: 0.00020058
Iteration 63/1000 | Loss: 0.00020920
Iteration 64/1000 | Loss: 0.00032871
Iteration 65/1000 | Loss: 0.00064134
Iteration 66/1000 | Loss: 0.00026194
Iteration 67/1000 | Loss: 0.00112813
Iteration 68/1000 | Loss: 0.00018266
Iteration 69/1000 | Loss: 0.00017916
Iteration 70/1000 | Loss: 0.00059611
Iteration 71/1000 | Loss: 0.00070653
Iteration 72/1000 | Loss: 0.00019062
Iteration 73/1000 | Loss: 0.00017571
Iteration 74/1000 | Loss: 0.00045806
Iteration 75/1000 | Loss: 0.00080545
Iteration 76/1000 | Loss: 0.00022691
Iteration 77/1000 | Loss: 0.00017371
Iteration 78/1000 | Loss: 0.00017063
Iteration 79/1000 | Loss: 0.00016820
Iteration 80/1000 | Loss: 0.00016680
Iteration 81/1000 | Loss: 0.00029108
Iteration 82/1000 | Loss: 0.00159309
Iteration 83/1000 | Loss: 0.00202027
Iteration 84/1000 | Loss: 0.00258969
Iteration 85/1000 | Loss: 0.00243697
Iteration 86/1000 | Loss: 0.00241837
Iteration 87/1000 | Loss: 0.00318381
Iteration 88/1000 | Loss: 0.00075854
Iteration 89/1000 | Loss: 0.00164873
Iteration 90/1000 | Loss: 0.00080508
Iteration 91/1000 | Loss: 0.00093990
Iteration 92/1000 | Loss: 0.00079823
Iteration 93/1000 | Loss: 0.00044884
Iteration 94/1000 | Loss: 0.00035094
Iteration 95/1000 | Loss: 0.00138356
Iteration 96/1000 | Loss: 0.00054315
Iteration 97/1000 | Loss: 0.00059184
Iteration 98/1000 | Loss: 0.00014741
Iteration 99/1000 | Loss: 0.00108992
Iteration 100/1000 | Loss: 0.00013171
Iteration 101/1000 | Loss: 0.00012494
Iteration 102/1000 | Loss: 0.00011934
Iteration 103/1000 | Loss: 0.00033070
Iteration 104/1000 | Loss: 0.00096289
Iteration 105/1000 | Loss: 0.00011275
Iteration 106/1000 | Loss: 0.00011030
Iteration 107/1000 | Loss: 0.00010866
Iteration 108/1000 | Loss: 0.00072807
Iteration 109/1000 | Loss: 0.00029500
Iteration 110/1000 | Loss: 0.00012455
Iteration 111/1000 | Loss: 0.00010677
Iteration 112/1000 | Loss: 0.00014169
Iteration 113/1000 | Loss: 0.00010551
Iteration 114/1000 | Loss: 0.00010491
Iteration 115/1000 | Loss: 0.00010459
Iteration 116/1000 | Loss: 0.00010434
Iteration 117/1000 | Loss: 0.00010416
Iteration 118/1000 | Loss: 0.00010412
Iteration 119/1000 | Loss: 0.00010411
Iteration 120/1000 | Loss: 0.00010401
Iteration 121/1000 | Loss: 0.00010400
Iteration 122/1000 | Loss: 0.00010400
Iteration 123/1000 | Loss: 0.00010399
Iteration 124/1000 | Loss: 0.00010399
Iteration 125/1000 | Loss: 0.00010398
Iteration 126/1000 | Loss: 0.00010397
Iteration 127/1000 | Loss: 0.00010397
Iteration 128/1000 | Loss: 0.00010397
Iteration 129/1000 | Loss: 0.00010396
Iteration 130/1000 | Loss: 0.00010396
Iteration 131/1000 | Loss: 0.00010396
Iteration 132/1000 | Loss: 0.00010395
Iteration 133/1000 | Loss: 0.00010395
Iteration 134/1000 | Loss: 0.00010394
Iteration 135/1000 | Loss: 0.00010394
Iteration 136/1000 | Loss: 0.00010394
Iteration 137/1000 | Loss: 0.00010394
Iteration 138/1000 | Loss: 0.00010394
Iteration 139/1000 | Loss: 0.00010394
Iteration 140/1000 | Loss: 0.00010393
Iteration 141/1000 | Loss: 0.00010392
Iteration 142/1000 | Loss: 0.00010391
Iteration 143/1000 | Loss: 0.00010391
Iteration 144/1000 | Loss: 0.00010390
Iteration 145/1000 | Loss: 0.00010390
Iteration 146/1000 | Loss: 0.00010390
Iteration 147/1000 | Loss: 0.00010390
Iteration 148/1000 | Loss: 0.00010389
Iteration 149/1000 | Loss: 0.00010387
Iteration 150/1000 | Loss: 0.00010387
Iteration 151/1000 | Loss: 0.00010387
Iteration 152/1000 | Loss: 0.00010387
Iteration 153/1000 | Loss: 0.00010387
Iteration 154/1000 | Loss: 0.00010387
Iteration 155/1000 | Loss: 0.00010387
Iteration 156/1000 | Loss: 0.00010387
Iteration 157/1000 | Loss: 0.00010387
Iteration 158/1000 | Loss: 0.00010387
Iteration 159/1000 | Loss: 0.00010386
Iteration 160/1000 | Loss: 0.00010386
Iteration 161/1000 | Loss: 0.00010385
Iteration 162/1000 | Loss: 0.00010385
Iteration 163/1000 | Loss: 0.00010385
Iteration 164/1000 | Loss: 0.00010384
Iteration 165/1000 | Loss: 0.00010384
Iteration 166/1000 | Loss: 0.00010384
Iteration 167/1000 | Loss: 0.00010383
Iteration 168/1000 | Loss: 0.00010383
Iteration 169/1000 | Loss: 0.00010383
Iteration 170/1000 | Loss: 0.00024611
Iteration 171/1000 | Loss: 0.00010428
Iteration 172/1000 | Loss: 0.00010392
Iteration 173/1000 | Loss: 0.00010383
Iteration 174/1000 | Loss: 0.00010382
Iteration 175/1000 | Loss: 0.00010381
Iteration 176/1000 | Loss: 0.00010379
Iteration 177/1000 | Loss: 0.00010378
Iteration 178/1000 | Loss: 0.00010378
Iteration 179/1000 | Loss: 0.00010378
Iteration 180/1000 | Loss: 0.00010378
Iteration 181/1000 | Loss: 0.00010378
Iteration 182/1000 | Loss: 0.00010378
Iteration 183/1000 | Loss: 0.00010378
Iteration 184/1000 | Loss: 0.00010377
Iteration 185/1000 | Loss: 0.00010377
Iteration 186/1000 | Loss: 0.00010377
Iteration 187/1000 | Loss: 0.00010377
Iteration 188/1000 | Loss: 0.00010377
Iteration 189/1000 | Loss: 0.00010376
Iteration 190/1000 | Loss: 0.00010376
Iteration 191/1000 | Loss: 0.00010375
Iteration 192/1000 | Loss: 0.00010375
Iteration 193/1000 | Loss: 0.00010375
Iteration 194/1000 | Loss: 0.00010375
Iteration 195/1000 | Loss: 0.00010375
Iteration 196/1000 | Loss: 0.00010375
Iteration 197/1000 | Loss: 0.00010375
Iteration 198/1000 | Loss: 0.00010375
Iteration 199/1000 | Loss: 0.00010375
Iteration 200/1000 | Loss: 0.00010374
Iteration 201/1000 | Loss: 0.00010374
Iteration 202/1000 | Loss: 0.00010374
Iteration 203/1000 | Loss: 0.00010374
Iteration 204/1000 | Loss: 0.00010374
Iteration 205/1000 | Loss: 0.00010374
Iteration 206/1000 | Loss: 0.00010374
Iteration 207/1000 | Loss: 0.00010374
Iteration 208/1000 | Loss: 0.00010374
Iteration 209/1000 | Loss: 0.00010374
Iteration 210/1000 | Loss: 0.00010374
Iteration 211/1000 | Loss: 0.00010373
Iteration 212/1000 | Loss: 0.00010373
Iteration 213/1000 | Loss: 0.00010373
Iteration 214/1000 | Loss: 0.00010373
Iteration 215/1000 | Loss: 0.00010373
Iteration 216/1000 | Loss: 0.00010373
Iteration 217/1000 | Loss: 0.00010373
Iteration 218/1000 | Loss: 0.00010373
Iteration 219/1000 | Loss: 0.00010373
Iteration 220/1000 | Loss: 0.00010373
Iteration 221/1000 | Loss: 0.00010373
Iteration 222/1000 | Loss: 0.00010373
Iteration 223/1000 | Loss: 0.00010373
Iteration 224/1000 | Loss: 0.00010373
Iteration 225/1000 | Loss: 0.00010373
Iteration 226/1000 | Loss: 0.00010373
Iteration 227/1000 | Loss: 0.00010373
Iteration 228/1000 | Loss: 0.00010372
Iteration 229/1000 | Loss: 0.00010372
Iteration 230/1000 | Loss: 0.00010372
Iteration 231/1000 | Loss: 0.00010372
Iteration 232/1000 | Loss: 0.00010372
Iteration 233/1000 | Loss: 0.00010372
Iteration 234/1000 | Loss: 0.00010372
Iteration 235/1000 | Loss: 0.00010372
Iteration 236/1000 | Loss: 0.00010372
Iteration 237/1000 | Loss: 0.00010372
Iteration 238/1000 | Loss: 0.00010372
Iteration 239/1000 | Loss: 0.00010372
Iteration 240/1000 | Loss: 0.00010372
Iteration 241/1000 | Loss: 0.00010372
Iteration 242/1000 | Loss: 0.00010372
Iteration 243/1000 | Loss: 0.00010372
Iteration 244/1000 | Loss: 0.00010372
Iteration 245/1000 | Loss: 0.00010372
Iteration 246/1000 | Loss: 0.00010372
Iteration 247/1000 | Loss: 0.00010372
Iteration 248/1000 | Loss: 0.00010372
Iteration 249/1000 | Loss: 0.00010372
Iteration 250/1000 | Loss: 0.00010372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [0.0001037169131450355, 0.0001037169131450355, 0.0001037169131450355, 0.0001037169131450355, 0.0001037169131450355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001037169131450355

Optimization complete. Final v2v error: 5.258304595947266 mm

Highest mean error: 11.300283432006836 mm for frame 41

Lowest mean error: 3.2431232929229736 mm for frame 150

Saving results

Total time: 219.1241979598999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468536
Iteration 2/25 | Loss: 0.00125572
Iteration 3/25 | Loss: 0.00117410
Iteration 4/25 | Loss: 0.00115793
Iteration 5/25 | Loss: 0.00115169
Iteration 6/25 | Loss: 0.00115002
Iteration 7/25 | Loss: 0.00115002
Iteration 8/25 | Loss: 0.00115002
Iteration 9/25 | Loss: 0.00115002
Iteration 10/25 | Loss: 0.00115002
Iteration 11/25 | Loss: 0.00115002
Iteration 12/25 | Loss: 0.00115002
Iteration 13/25 | Loss: 0.00115002
Iteration 14/25 | Loss: 0.00115002
Iteration 15/25 | Loss: 0.00115002
Iteration 16/25 | Loss: 0.00115002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011500203981995583, 0.0011500203981995583, 0.0011500203981995583, 0.0011500203981995583, 0.0011500203981995583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011500203981995583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10407710
Iteration 2/25 | Loss: 0.00093307
Iteration 3/25 | Loss: 0.00093306
Iteration 4/25 | Loss: 0.00093306
Iteration 5/25 | Loss: 0.00093306
Iteration 6/25 | Loss: 0.00093306
Iteration 7/25 | Loss: 0.00093306
Iteration 8/25 | Loss: 0.00093306
Iteration 9/25 | Loss: 0.00093306
Iteration 10/25 | Loss: 0.00093306
Iteration 11/25 | Loss: 0.00093306
Iteration 12/25 | Loss: 0.00093306
Iteration 13/25 | Loss: 0.00093306
Iteration 14/25 | Loss: 0.00093306
Iteration 15/25 | Loss: 0.00093306
Iteration 16/25 | Loss: 0.00093306
Iteration 17/25 | Loss: 0.00093306
Iteration 18/25 | Loss: 0.00093306
Iteration 19/25 | Loss: 0.00093306
Iteration 20/25 | Loss: 0.00093306
Iteration 21/25 | Loss: 0.00093306
Iteration 22/25 | Loss: 0.00093306
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009330576867796481, 0.0009330576867796481, 0.0009330576867796481, 0.0009330576867796481, 0.0009330576867796481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009330576867796481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093306
Iteration 2/1000 | Loss: 0.00002955
Iteration 3/1000 | Loss: 0.00002120
Iteration 4/1000 | Loss: 0.00001944
Iteration 5/1000 | Loss: 0.00001864
Iteration 6/1000 | Loss: 0.00001825
Iteration 7/1000 | Loss: 0.00001793
Iteration 8/1000 | Loss: 0.00001774
Iteration 9/1000 | Loss: 0.00001750
Iteration 10/1000 | Loss: 0.00001726
Iteration 11/1000 | Loss: 0.00001713
Iteration 12/1000 | Loss: 0.00001710
Iteration 13/1000 | Loss: 0.00001709
Iteration 14/1000 | Loss: 0.00001707
Iteration 15/1000 | Loss: 0.00001707
Iteration 16/1000 | Loss: 0.00001707
Iteration 17/1000 | Loss: 0.00001707
Iteration 18/1000 | Loss: 0.00001706
Iteration 19/1000 | Loss: 0.00001706
Iteration 20/1000 | Loss: 0.00001702
Iteration 21/1000 | Loss: 0.00001701
Iteration 22/1000 | Loss: 0.00001701
Iteration 23/1000 | Loss: 0.00001701
Iteration 24/1000 | Loss: 0.00001701
Iteration 25/1000 | Loss: 0.00001699
Iteration 26/1000 | Loss: 0.00001698
Iteration 27/1000 | Loss: 0.00001696
Iteration 28/1000 | Loss: 0.00001695
Iteration 29/1000 | Loss: 0.00001694
Iteration 30/1000 | Loss: 0.00001693
Iteration 31/1000 | Loss: 0.00001691
Iteration 32/1000 | Loss: 0.00001690
Iteration 33/1000 | Loss: 0.00001690
Iteration 34/1000 | Loss: 0.00001687
Iteration 35/1000 | Loss: 0.00001687
Iteration 36/1000 | Loss: 0.00001685
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001684
Iteration 39/1000 | Loss: 0.00001683
Iteration 40/1000 | Loss: 0.00001683
Iteration 41/1000 | Loss: 0.00001683
Iteration 42/1000 | Loss: 0.00001683
Iteration 43/1000 | Loss: 0.00001682
Iteration 44/1000 | Loss: 0.00001682
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00001681
Iteration 47/1000 | Loss: 0.00001681
Iteration 48/1000 | Loss: 0.00001680
Iteration 49/1000 | Loss: 0.00001680
Iteration 50/1000 | Loss: 0.00001680
Iteration 51/1000 | Loss: 0.00001680
Iteration 52/1000 | Loss: 0.00001680
Iteration 53/1000 | Loss: 0.00001679
Iteration 54/1000 | Loss: 0.00001679
Iteration 55/1000 | Loss: 0.00001679
Iteration 56/1000 | Loss: 0.00001678
Iteration 57/1000 | Loss: 0.00001678
Iteration 58/1000 | Loss: 0.00001678
Iteration 59/1000 | Loss: 0.00001678
Iteration 60/1000 | Loss: 0.00001678
Iteration 61/1000 | Loss: 0.00001678
Iteration 62/1000 | Loss: 0.00001678
Iteration 63/1000 | Loss: 0.00001678
Iteration 64/1000 | Loss: 0.00001677
Iteration 65/1000 | Loss: 0.00001677
Iteration 66/1000 | Loss: 0.00001677
Iteration 67/1000 | Loss: 0.00001676
Iteration 68/1000 | Loss: 0.00001676
Iteration 69/1000 | Loss: 0.00001676
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001676
Iteration 72/1000 | Loss: 0.00001675
Iteration 73/1000 | Loss: 0.00001675
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00001675
Iteration 76/1000 | Loss: 0.00001674
Iteration 77/1000 | Loss: 0.00001674
Iteration 78/1000 | Loss: 0.00001674
Iteration 79/1000 | Loss: 0.00001674
Iteration 80/1000 | Loss: 0.00001674
Iteration 81/1000 | Loss: 0.00001674
Iteration 82/1000 | Loss: 0.00001674
Iteration 83/1000 | Loss: 0.00001674
Iteration 84/1000 | Loss: 0.00001674
Iteration 85/1000 | Loss: 0.00001674
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001674
Iteration 88/1000 | Loss: 0.00001673
Iteration 89/1000 | Loss: 0.00001673
Iteration 90/1000 | Loss: 0.00001673
Iteration 91/1000 | Loss: 0.00001673
Iteration 92/1000 | Loss: 0.00001673
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001673
Iteration 95/1000 | Loss: 0.00001673
Iteration 96/1000 | Loss: 0.00001672
Iteration 97/1000 | Loss: 0.00001672
Iteration 98/1000 | Loss: 0.00001672
Iteration 99/1000 | Loss: 0.00001671
Iteration 100/1000 | Loss: 0.00001671
Iteration 101/1000 | Loss: 0.00001671
Iteration 102/1000 | Loss: 0.00001671
Iteration 103/1000 | Loss: 0.00001671
Iteration 104/1000 | Loss: 0.00001671
Iteration 105/1000 | Loss: 0.00001671
Iteration 106/1000 | Loss: 0.00001671
Iteration 107/1000 | Loss: 0.00001671
Iteration 108/1000 | Loss: 0.00001671
Iteration 109/1000 | Loss: 0.00001671
Iteration 110/1000 | Loss: 0.00001671
Iteration 111/1000 | Loss: 0.00001671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.6713014701963402e-05, 1.6713014701963402e-05, 1.6713014701963402e-05, 1.6713014701963402e-05, 1.6713014701963402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6713014701963402e-05

Optimization complete. Final v2v error: 3.4347031116485596 mm

Highest mean error: 4.154880046844482 mm for frame 183

Lowest mean error: 3.2874536514282227 mm for frame 126

Saving results

Total time: 35.66685199737549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384389
Iteration 2/25 | Loss: 0.00120298
Iteration 3/25 | Loss: 0.00111313
Iteration 4/25 | Loss: 0.00110115
Iteration 5/25 | Loss: 0.00109709
Iteration 6/25 | Loss: 0.00109581
Iteration 7/25 | Loss: 0.00109556
Iteration 8/25 | Loss: 0.00109556
Iteration 9/25 | Loss: 0.00109556
Iteration 10/25 | Loss: 0.00109556
Iteration 11/25 | Loss: 0.00109556
Iteration 12/25 | Loss: 0.00109556
Iteration 13/25 | Loss: 0.00109556
Iteration 14/25 | Loss: 0.00109556
Iteration 15/25 | Loss: 0.00109556
Iteration 16/25 | Loss: 0.00109556
Iteration 17/25 | Loss: 0.00109556
Iteration 18/25 | Loss: 0.00109556
Iteration 19/25 | Loss: 0.00109556
Iteration 20/25 | Loss: 0.00109556
Iteration 21/25 | Loss: 0.00109556
Iteration 22/25 | Loss: 0.00109556
Iteration 23/25 | Loss: 0.00109556
Iteration 24/25 | Loss: 0.00109556
Iteration 25/25 | Loss: 0.00109556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.73062611
Iteration 2/25 | Loss: 0.00109219
Iteration 3/25 | Loss: 0.00109203
Iteration 4/25 | Loss: 0.00109202
Iteration 5/25 | Loss: 0.00109202
Iteration 6/25 | Loss: 0.00109202
Iteration 7/25 | Loss: 0.00109202
Iteration 8/25 | Loss: 0.00109202
Iteration 9/25 | Loss: 0.00109202
Iteration 10/25 | Loss: 0.00109202
Iteration 11/25 | Loss: 0.00109202
Iteration 12/25 | Loss: 0.00109202
Iteration 13/25 | Loss: 0.00109202
Iteration 14/25 | Loss: 0.00109202
Iteration 15/25 | Loss: 0.00109202
Iteration 16/25 | Loss: 0.00109202
Iteration 17/25 | Loss: 0.00109202
Iteration 18/25 | Loss: 0.00109202
Iteration 19/25 | Loss: 0.00109202
Iteration 20/25 | Loss: 0.00109202
Iteration 21/25 | Loss: 0.00109202
Iteration 22/25 | Loss: 0.00109202
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010920224012807012, 0.0010920224012807012, 0.0010920224012807012, 0.0010920224012807012, 0.0010920224012807012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010920224012807012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109202
Iteration 2/1000 | Loss: 0.00004548
Iteration 3/1000 | Loss: 0.00002961
Iteration 4/1000 | Loss: 0.00001814
Iteration 5/1000 | Loss: 0.00001604
Iteration 6/1000 | Loss: 0.00001508
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001391
Iteration 9/1000 | Loss: 0.00001356
Iteration 10/1000 | Loss: 0.00001328
Iteration 11/1000 | Loss: 0.00001314
Iteration 12/1000 | Loss: 0.00001301
Iteration 13/1000 | Loss: 0.00001299
Iteration 14/1000 | Loss: 0.00001296
Iteration 15/1000 | Loss: 0.00001296
Iteration 16/1000 | Loss: 0.00001295
Iteration 17/1000 | Loss: 0.00001294
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001293
Iteration 20/1000 | Loss: 0.00001293
Iteration 21/1000 | Loss: 0.00001292
Iteration 22/1000 | Loss: 0.00001291
Iteration 23/1000 | Loss: 0.00001291
Iteration 24/1000 | Loss: 0.00001290
Iteration 25/1000 | Loss: 0.00001289
Iteration 26/1000 | Loss: 0.00001286
Iteration 27/1000 | Loss: 0.00001284
Iteration 28/1000 | Loss: 0.00001283
Iteration 29/1000 | Loss: 0.00001283
Iteration 30/1000 | Loss: 0.00001282
Iteration 31/1000 | Loss: 0.00001282
Iteration 32/1000 | Loss: 0.00001282
Iteration 33/1000 | Loss: 0.00001280
Iteration 34/1000 | Loss: 0.00001280
Iteration 35/1000 | Loss: 0.00001280
Iteration 36/1000 | Loss: 0.00001280
Iteration 37/1000 | Loss: 0.00001280
Iteration 38/1000 | Loss: 0.00001280
Iteration 39/1000 | Loss: 0.00001280
Iteration 40/1000 | Loss: 0.00001280
Iteration 41/1000 | Loss: 0.00001279
Iteration 42/1000 | Loss: 0.00001279
Iteration 43/1000 | Loss: 0.00001279
Iteration 44/1000 | Loss: 0.00001278
Iteration 45/1000 | Loss: 0.00001277
Iteration 46/1000 | Loss: 0.00001277
Iteration 47/1000 | Loss: 0.00001277
Iteration 48/1000 | Loss: 0.00001277
Iteration 49/1000 | Loss: 0.00001276
Iteration 50/1000 | Loss: 0.00001276
Iteration 51/1000 | Loss: 0.00001275
Iteration 52/1000 | Loss: 0.00001275
Iteration 53/1000 | Loss: 0.00001274
Iteration 54/1000 | Loss: 0.00001274
Iteration 55/1000 | Loss: 0.00001274
Iteration 56/1000 | Loss: 0.00001273
Iteration 57/1000 | Loss: 0.00001273
Iteration 58/1000 | Loss: 0.00001273
Iteration 59/1000 | Loss: 0.00001272
Iteration 60/1000 | Loss: 0.00001269
Iteration 61/1000 | Loss: 0.00001269
Iteration 62/1000 | Loss: 0.00001269
Iteration 63/1000 | Loss: 0.00001269
Iteration 64/1000 | Loss: 0.00001269
Iteration 65/1000 | Loss: 0.00001269
Iteration 66/1000 | Loss: 0.00001269
Iteration 67/1000 | Loss: 0.00001269
Iteration 68/1000 | Loss: 0.00001269
Iteration 69/1000 | Loss: 0.00001268
Iteration 70/1000 | Loss: 0.00001268
Iteration 71/1000 | Loss: 0.00001267
Iteration 72/1000 | Loss: 0.00001267
Iteration 73/1000 | Loss: 0.00001265
Iteration 74/1000 | Loss: 0.00001265
Iteration 75/1000 | Loss: 0.00001265
Iteration 76/1000 | Loss: 0.00001264
Iteration 77/1000 | Loss: 0.00001264
Iteration 78/1000 | Loss: 0.00001263
Iteration 79/1000 | Loss: 0.00001263
Iteration 80/1000 | Loss: 0.00001262
Iteration 81/1000 | Loss: 0.00001261
Iteration 82/1000 | Loss: 0.00001261
Iteration 83/1000 | Loss: 0.00001261
Iteration 84/1000 | Loss: 0.00001261
Iteration 85/1000 | Loss: 0.00001261
Iteration 86/1000 | Loss: 0.00001260
Iteration 87/1000 | Loss: 0.00001260
Iteration 88/1000 | Loss: 0.00001260
Iteration 89/1000 | Loss: 0.00001259
Iteration 90/1000 | Loss: 0.00001259
Iteration 91/1000 | Loss: 0.00001259
Iteration 92/1000 | Loss: 0.00001258
Iteration 93/1000 | Loss: 0.00001258
Iteration 94/1000 | Loss: 0.00001258
Iteration 95/1000 | Loss: 0.00001257
Iteration 96/1000 | Loss: 0.00001257
Iteration 97/1000 | Loss: 0.00001257
Iteration 98/1000 | Loss: 0.00001256
Iteration 99/1000 | Loss: 0.00001256
Iteration 100/1000 | Loss: 0.00001256
Iteration 101/1000 | Loss: 0.00001256
Iteration 102/1000 | Loss: 0.00001255
Iteration 103/1000 | Loss: 0.00001255
Iteration 104/1000 | Loss: 0.00001255
Iteration 105/1000 | Loss: 0.00001255
Iteration 106/1000 | Loss: 0.00001254
Iteration 107/1000 | Loss: 0.00001254
Iteration 108/1000 | Loss: 0.00001254
Iteration 109/1000 | Loss: 0.00001254
Iteration 110/1000 | Loss: 0.00001254
Iteration 111/1000 | Loss: 0.00001253
Iteration 112/1000 | Loss: 0.00001253
Iteration 113/1000 | Loss: 0.00001253
Iteration 114/1000 | Loss: 0.00001253
Iteration 115/1000 | Loss: 0.00001253
Iteration 116/1000 | Loss: 0.00001253
Iteration 117/1000 | Loss: 0.00001253
Iteration 118/1000 | Loss: 0.00001253
Iteration 119/1000 | Loss: 0.00001253
Iteration 120/1000 | Loss: 0.00001253
Iteration 121/1000 | Loss: 0.00001253
Iteration 122/1000 | Loss: 0.00001253
Iteration 123/1000 | Loss: 0.00001253
Iteration 124/1000 | Loss: 0.00001253
Iteration 125/1000 | Loss: 0.00001252
Iteration 126/1000 | Loss: 0.00001252
Iteration 127/1000 | Loss: 0.00001252
Iteration 128/1000 | Loss: 0.00001252
Iteration 129/1000 | Loss: 0.00001252
Iteration 130/1000 | Loss: 0.00001252
Iteration 131/1000 | Loss: 0.00001252
Iteration 132/1000 | Loss: 0.00001252
Iteration 133/1000 | Loss: 0.00001252
Iteration 134/1000 | Loss: 0.00001252
Iteration 135/1000 | Loss: 0.00001251
Iteration 136/1000 | Loss: 0.00001251
Iteration 137/1000 | Loss: 0.00001251
Iteration 138/1000 | Loss: 0.00001251
Iteration 139/1000 | Loss: 0.00001251
Iteration 140/1000 | Loss: 0.00001251
Iteration 141/1000 | Loss: 0.00001251
Iteration 142/1000 | Loss: 0.00001251
Iteration 143/1000 | Loss: 0.00001251
Iteration 144/1000 | Loss: 0.00001251
Iteration 145/1000 | Loss: 0.00001251
Iteration 146/1000 | Loss: 0.00001251
Iteration 147/1000 | Loss: 0.00001251
Iteration 148/1000 | Loss: 0.00001250
Iteration 149/1000 | Loss: 0.00001250
Iteration 150/1000 | Loss: 0.00001250
Iteration 151/1000 | Loss: 0.00001250
Iteration 152/1000 | Loss: 0.00001250
Iteration 153/1000 | Loss: 0.00001250
Iteration 154/1000 | Loss: 0.00001250
Iteration 155/1000 | Loss: 0.00001250
Iteration 156/1000 | Loss: 0.00001250
Iteration 157/1000 | Loss: 0.00001250
Iteration 158/1000 | Loss: 0.00001250
Iteration 159/1000 | Loss: 0.00001250
Iteration 160/1000 | Loss: 0.00001250
Iteration 161/1000 | Loss: 0.00001250
Iteration 162/1000 | Loss: 0.00001250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.2502446224971209e-05, 1.2502446224971209e-05, 1.2502446224971209e-05, 1.2502446224971209e-05, 1.2502446224971209e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2502446224971209e-05

Optimization complete. Final v2v error: 2.9508488178253174 mm

Highest mean error: 3.70017409324646 mm for frame 10

Lowest mean error: 2.4052886962890625 mm for frame 138

Saving results

Total time: 37.614140033721924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799815
Iteration 2/25 | Loss: 0.00115383
Iteration 3/25 | Loss: 0.00108000
Iteration 4/25 | Loss: 0.00107015
Iteration 5/25 | Loss: 0.00106677
Iteration 6/25 | Loss: 0.00106597
Iteration 7/25 | Loss: 0.00106597
Iteration 8/25 | Loss: 0.00106597
Iteration 9/25 | Loss: 0.00106597
Iteration 10/25 | Loss: 0.00106597
Iteration 11/25 | Loss: 0.00106597
Iteration 12/25 | Loss: 0.00106597
Iteration 13/25 | Loss: 0.00106597
Iteration 14/25 | Loss: 0.00106597
Iteration 15/25 | Loss: 0.00106597
Iteration 16/25 | Loss: 0.00106597
Iteration 17/25 | Loss: 0.00106597
Iteration 18/25 | Loss: 0.00106597
Iteration 19/25 | Loss: 0.00106597
Iteration 20/25 | Loss: 0.00106597
Iteration 21/25 | Loss: 0.00106597
Iteration 22/25 | Loss: 0.00106597
Iteration 23/25 | Loss: 0.00106597
Iteration 24/25 | Loss: 0.00106597
Iteration 25/25 | Loss: 0.00106597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35813844
Iteration 2/25 | Loss: 0.00086705
Iteration 3/25 | Loss: 0.00086705
Iteration 4/25 | Loss: 0.00086705
Iteration 5/25 | Loss: 0.00086705
Iteration 6/25 | Loss: 0.00086705
Iteration 7/25 | Loss: 0.00086705
Iteration 8/25 | Loss: 0.00086705
Iteration 9/25 | Loss: 0.00086705
Iteration 10/25 | Loss: 0.00086705
Iteration 11/25 | Loss: 0.00086705
Iteration 12/25 | Loss: 0.00086705
Iteration 13/25 | Loss: 0.00086705
Iteration 14/25 | Loss: 0.00086705
Iteration 15/25 | Loss: 0.00086705
Iteration 16/25 | Loss: 0.00086705
Iteration 17/25 | Loss: 0.00086705
Iteration 18/25 | Loss: 0.00086705
Iteration 19/25 | Loss: 0.00086705
Iteration 20/25 | Loss: 0.00086705
Iteration 21/25 | Loss: 0.00086705
Iteration 22/25 | Loss: 0.00086705
Iteration 23/25 | Loss: 0.00086705
Iteration 24/25 | Loss: 0.00086705
Iteration 25/25 | Loss: 0.00086705
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008670483948662877, 0.0008670483948662877, 0.0008670483948662877, 0.0008670483948662877, 0.0008670483948662877]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008670483948662877

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086705
Iteration 2/1000 | Loss: 0.00001912
Iteration 3/1000 | Loss: 0.00001206
Iteration 4/1000 | Loss: 0.00001058
Iteration 5/1000 | Loss: 0.00001010
Iteration 6/1000 | Loss: 0.00000968
Iteration 7/1000 | Loss: 0.00000933
Iteration 8/1000 | Loss: 0.00000918
Iteration 9/1000 | Loss: 0.00000915
Iteration 10/1000 | Loss: 0.00000909
Iteration 11/1000 | Loss: 0.00000897
Iteration 12/1000 | Loss: 0.00000896
Iteration 13/1000 | Loss: 0.00000891
Iteration 14/1000 | Loss: 0.00000891
Iteration 15/1000 | Loss: 0.00000890
Iteration 16/1000 | Loss: 0.00000889
Iteration 17/1000 | Loss: 0.00000888
Iteration 18/1000 | Loss: 0.00000883
Iteration 19/1000 | Loss: 0.00000881
Iteration 20/1000 | Loss: 0.00000881
Iteration 21/1000 | Loss: 0.00000881
Iteration 22/1000 | Loss: 0.00000880
Iteration 23/1000 | Loss: 0.00000879
Iteration 24/1000 | Loss: 0.00000879
Iteration 25/1000 | Loss: 0.00000878
Iteration 26/1000 | Loss: 0.00000878
Iteration 27/1000 | Loss: 0.00000871
Iteration 28/1000 | Loss: 0.00000871
Iteration 29/1000 | Loss: 0.00000871
Iteration 30/1000 | Loss: 0.00000870
Iteration 31/1000 | Loss: 0.00000869
Iteration 32/1000 | Loss: 0.00000869
Iteration 33/1000 | Loss: 0.00000868
Iteration 34/1000 | Loss: 0.00000865
Iteration 35/1000 | Loss: 0.00000865
Iteration 36/1000 | Loss: 0.00000865
Iteration 37/1000 | Loss: 0.00000865
Iteration 38/1000 | Loss: 0.00000865
Iteration 39/1000 | Loss: 0.00000865
Iteration 40/1000 | Loss: 0.00000864
Iteration 41/1000 | Loss: 0.00000864
Iteration 42/1000 | Loss: 0.00000863
Iteration 43/1000 | Loss: 0.00000863
Iteration 44/1000 | Loss: 0.00000863
Iteration 45/1000 | Loss: 0.00000863
Iteration 46/1000 | Loss: 0.00000863
Iteration 47/1000 | Loss: 0.00000863
Iteration 48/1000 | Loss: 0.00000863
Iteration 49/1000 | Loss: 0.00000863
Iteration 50/1000 | Loss: 0.00000863
Iteration 51/1000 | Loss: 0.00000863
Iteration 52/1000 | Loss: 0.00000863
Iteration 53/1000 | Loss: 0.00000863
Iteration 54/1000 | Loss: 0.00000863
Iteration 55/1000 | Loss: 0.00000863
Iteration 56/1000 | Loss: 0.00000863
Iteration 57/1000 | Loss: 0.00000863
Iteration 58/1000 | Loss: 0.00000863
Iteration 59/1000 | Loss: 0.00000863
Iteration 60/1000 | Loss: 0.00000863
Iteration 61/1000 | Loss: 0.00000863
Iteration 62/1000 | Loss: 0.00000863
Iteration 63/1000 | Loss: 0.00000863
Iteration 64/1000 | Loss: 0.00000863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [8.628850991954096e-06, 8.628850991954096e-06, 8.628850991954096e-06, 8.628850991954096e-06, 8.628850991954096e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.628850991954096e-06

Optimization complete. Final v2v error: 2.5428266525268555 mm

Highest mean error: 2.745783805847168 mm for frame 106

Lowest mean error: 2.4283177852630615 mm for frame 87

Saving results

Total time: 25.0926616191864
