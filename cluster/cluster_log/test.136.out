Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=136, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7616-7671
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989570
Iteration 2/25 | Loss: 0.00989570
Iteration 3/25 | Loss: 0.00989570
Iteration 4/25 | Loss: 0.00989570
Iteration 5/25 | Loss: 0.00989570
Iteration 6/25 | Loss: 0.00989569
Iteration 7/25 | Loss: 0.00989569
Iteration 8/25 | Loss: 0.00989569
Iteration 9/25 | Loss: 0.00989569
Iteration 10/25 | Loss: 0.00989569
Iteration 11/25 | Loss: 0.00989568
Iteration 12/25 | Loss: 0.00989568
Iteration 13/25 | Loss: 0.00989568
Iteration 14/25 | Loss: 0.00989568
Iteration 15/25 | Loss: 0.00989567
Iteration 16/25 | Loss: 0.00989567
Iteration 17/25 | Loss: 0.00989567
Iteration 18/25 | Loss: 0.00989567
Iteration 19/25 | Loss: 0.00989566
Iteration 20/25 | Loss: 0.00989566
Iteration 21/25 | Loss: 0.00989566
Iteration 22/25 | Loss: 0.00989566
Iteration 23/25 | Loss: 0.00989566
Iteration 24/25 | Loss: 0.00989565
Iteration 25/25 | Loss: 0.00989565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51870489
Iteration 2/25 | Loss: 0.13650678
Iteration 3/25 | Loss: 0.13611096
Iteration 4/25 | Loss: 0.13611096
Iteration 5/25 | Loss: 0.13611095
Iteration 6/25 | Loss: 0.13611093
Iteration 7/25 | Loss: 0.13611093
Iteration 8/25 | Loss: 0.13611093
Iteration 9/25 | Loss: 0.13611093
Iteration 10/25 | Loss: 0.13611093
Iteration 11/25 | Loss: 0.13611093
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.13611093163490295, 0.13611093163490295, 0.13611093163490295, 0.13611093163490295, 0.13611093163490295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.13611093163490295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.13611093
Iteration 2/1000 | Loss: 0.00286267
Iteration 3/1000 | Loss: 0.00111588
Iteration 4/1000 | Loss: 0.00198609
Iteration 5/1000 | Loss: 0.00206484
Iteration 6/1000 | Loss: 0.00058043
Iteration 7/1000 | Loss: 0.00025025
Iteration 8/1000 | Loss: 0.00158710
Iteration 9/1000 | Loss: 0.00126599
Iteration 10/1000 | Loss: 0.00072289
Iteration 11/1000 | Loss: 0.00015518
Iteration 12/1000 | Loss: 0.00008611
Iteration 13/1000 | Loss: 0.00009563
Iteration 14/1000 | Loss: 0.00005897
Iteration 15/1000 | Loss: 0.00005557
Iteration 16/1000 | Loss: 0.00003245
Iteration 17/1000 | Loss: 0.00007000
Iteration 18/1000 | Loss: 0.00004581
Iteration 19/1000 | Loss: 0.00002829
Iteration 20/1000 | Loss: 0.00011103
Iteration 21/1000 | Loss: 0.00009256
Iteration 22/1000 | Loss: 0.00002282
Iteration 23/1000 | Loss: 0.00002179
Iteration 24/1000 | Loss: 0.00009855
Iteration 25/1000 | Loss: 0.00002324
Iteration 26/1000 | Loss: 0.00003521
Iteration 27/1000 | Loss: 0.00001950
Iteration 28/1000 | Loss: 0.00003919
Iteration 29/1000 | Loss: 0.00004436
Iteration 30/1000 | Loss: 0.00002287
Iteration 31/1000 | Loss: 0.00001815
Iteration 32/1000 | Loss: 0.00004643
Iteration 33/1000 | Loss: 0.00001762
Iteration 34/1000 | Loss: 0.00005728
Iteration 35/1000 | Loss: 0.00001715
Iteration 36/1000 | Loss: 0.00002910
Iteration 37/1000 | Loss: 0.00001923
Iteration 38/1000 | Loss: 0.00001689
Iteration 39/1000 | Loss: 0.00001651
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001651
Iteration 42/1000 | Loss: 0.00001651
Iteration 43/1000 | Loss: 0.00001651
Iteration 44/1000 | Loss: 0.00001651
Iteration 45/1000 | Loss: 0.00001651
Iteration 46/1000 | Loss: 0.00001651
Iteration 47/1000 | Loss: 0.00001651
Iteration 48/1000 | Loss: 0.00001651
Iteration 49/1000 | Loss: 0.00001651
Iteration 50/1000 | Loss: 0.00001651
Iteration 51/1000 | Loss: 0.00001650
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001637
Iteration 54/1000 | Loss: 0.00001841
Iteration 55/1000 | Loss: 0.00001632
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001631
Iteration 58/1000 | Loss: 0.00001631
Iteration 59/1000 | Loss: 0.00001631
Iteration 60/1000 | Loss: 0.00001631
Iteration 61/1000 | Loss: 0.00001631
Iteration 62/1000 | Loss: 0.00001631
Iteration 63/1000 | Loss: 0.00001631
Iteration 64/1000 | Loss: 0.00001631
Iteration 65/1000 | Loss: 0.00001630
Iteration 66/1000 | Loss: 0.00001630
Iteration 67/1000 | Loss: 0.00001630
Iteration 68/1000 | Loss: 0.00001629
Iteration 69/1000 | Loss: 0.00001629
Iteration 70/1000 | Loss: 0.00001629
Iteration 71/1000 | Loss: 0.00001629
Iteration 72/1000 | Loss: 0.00001628
Iteration 73/1000 | Loss: 0.00001628
Iteration 74/1000 | Loss: 0.00001627
Iteration 75/1000 | Loss: 0.00001627
Iteration 76/1000 | Loss: 0.00001627
Iteration 77/1000 | Loss: 0.00001626
Iteration 78/1000 | Loss: 0.00001626
Iteration 79/1000 | Loss: 0.00001626
Iteration 80/1000 | Loss: 0.00001626
Iteration 81/1000 | Loss: 0.00001626
Iteration 82/1000 | Loss: 0.00001626
Iteration 83/1000 | Loss: 0.00001625
Iteration 84/1000 | Loss: 0.00001624
Iteration 85/1000 | Loss: 0.00001624
Iteration 86/1000 | Loss: 0.00001624
Iteration 87/1000 | Loss: 0.00001624
Iteration 88/1000 | Loss: 0.00001624
Iteration 89/1000 | Loss: 0.00001623
Iteration 90/1000 | Loss: 0.00001623
Iteration 91/1000 | Loss: 0.00001623
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001623
Iteration 95/1000 | Loss: 0.00001623
Iteration 96/1000 | Loss: 0.00001623
Iteration 97/1000 | Loss: 0.00001623
Iteration 98/1000 | Loss: 0.00001622
Iteration 99/1000 | Loss: 0.00001622
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001622
Iteration 102/1000 | Loss: 0.00001621
Iteration 103/1000 | Loss: 0.00001620
Iteration 104/1000 | Loss: 0.00001620
Iteration 105/1000 | Loss: 0.00001620
Iteration 106/1000 | Loss: 0.00001619
Iteration 107/1000 | Loss: 0.00001619
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001619
Iteration 111/1000 | Loss: 0.00001619
Iteration 112/1000 | Loss: 0.00001619
Iteration 113/1000 | Loss: 0.00001619
Iteration 114/1000 | Loss: 0.00002468
Iteration 115/1000 | Loss: 0.00002183
Iteration 116/1000 | Loss: 0.00001730
Iteration 117/1000 | Loss: 0.00001616
Iteration 118/1000 | Loss: 0.00001616
Iteration 119/1000 | Loss: 0.00001616
Iteration 120/1000 | Loss: 0.00001616
Iteration 121/1000 | Loss: 0.00001616
Iteration 122/1000 | Loss: 0.00001616
Iteration 123/1000 | Loss: 0.00001616
Iteration 124/1000 | Loss: 0.00001616
Iteration 125/1000 | Loss: 0.00001616
Iteration 126/1000 | Loss: 0.00001616
Iteration 127/1000 | Loss: 0.00001616
Iteration 128/1000 | Loss: 0.00001616
Iteration 129/1000 | Loss: 0.00001616
Iteration 130/1000 | Loss: 0.00001616
Iteration 131/1000 | Loss: 0.00001615
Iteration 132/1000 | Loss: 0.00001615
Iteration 133/1000 | Loss: 0.00001615
Iteration 134/1000 | Loss: 0.00001615
Iteration 135/1000 | Loss: 0.00001615
Iteration 136/1000 | Loss: 0.00001615
Iteration 137/1000 | Loss: 0.00001615
Iteration 138/1000 | Loss: 0.00001615
Iteration 139/1000 | Loss: 0.00001615
Iteration 140/1000 | Loss: 0.00001615
Iteration 141/1000 | Loss: 0.00001615
Iteration 142/1000 | Loss: 0.00001615
Iteration 143/1000 | Loss: 0.00001615
Iteration 144/1000 | Loss: 0.00001615
Iteration 145/1000 | Loss: 0.00001615
Iteration 146/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.615119617781602e-05, 1.615119617781602e-05, 1.615119617781602e-05, 1.615119617781602e-05, 1.615119617781602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.615119617781602e-05

Optimization complete. Final v2v error: 3.3716728687286377 mm

Highest mean error: 3.619802951812744 mm for frame 184

Lowest mean error: 3.1665728092193604 mm for frame 8

Saving results

Total time: 85.17149901390076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00943621
Iteration 2/25 | Loss: 0.00216255
Iteration 3/25 | Loss: 0.00181897
Iteration 4/25 | Loss: 0.00167394
Iteration 5/25 | Loss: 0.00183926
Iteration 6/25 | Loss: 0.00161183
Iteration 7/25 | Loss: 0.00153716
Iteration 8/25 | Loss: 0.00152720
Iteration 9/25 | Loss: 0.00148268
Iteration 10/25 | Loss: 0.00148045
Iteration 11/25 | Loss: 0.00145230
Iteration 12/25 | Loss: 0.00142187
Iteration 13/25 | Loss: 0.00141408
Iteration 14/25 | Loss: 0.00140061
Iteration 15/25 | Loss: 0.00140858
Iteration 16/25 | Loss: 0.00139852
Iteration 17/25 | Loss: 0.00140051
Iteration 18/25 | Loss: 0.00139575
Iteration 19/25 | Loss: 0.00138986
Iteration 20/25 | Loss: 0.00138775
Iteration 21/25 | Loss: 0.00138727
Iteration 22/25 | Loss: 0.00138617
Iteration 23/25 | Loss: 0.00138811
Iteration 24/25 | Loss: 0.00138610
Iteration 25/25 | Loss: 0.00138778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39289975
Iteration 2/25 | Loss: 0.00123957
Iteration 3/25 | Loss: 0.00113417
Iteration 4/25 | Loss: 0.00113417
Iteration 5/25 | Loss: 0.00113417
Iteration 6/25 | Loss: 0.00113417
Iteration 7/25 | Loss: 0.00113417
Iteration 8/25 | Loss: 0.00113417
Iteration 9/25 | Loss: 0.00113417
Iteration 10/25 | Loss: 0.00113417
Iteration 11/25 | Loss: 0.00113417
Iteration 12/25 | Loss: 0.00113417
Iteration 13/25 | Loss: 0.00113417
Iteration 14/25 | Loss: 0.00113417
Iteration 15/25 | Loss: 0.00113417
Iteration 16/25 | Loss: 0.00113417
Iteration 17/25 | Loss: 0.00113417
Iteration 18/25 | Loss: 0.00113417
Iteration 19/25 | Loss: 0.00113417
Iteration 20/25 | Loss: 0.00113417
Iteration 21/25 | Loss: 0.00113417
Iteration 22/25 | Loss: 0.00113417
Iteration 23/25 | Loss: 0.00113417
Iteration 24/25 | Loss: 0.00113417
Iteration 25/25 | Loss: 0.00113417
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00113416847307235, 0.00113416847307235, 0.00113416847307235, 0.00113416847307235, 0.00113416847307235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00113416847307235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113417
Iteration 2/1000 | Loss: 0.00015651
Iteration 3/1000 | Loss: 0.00007547
Iteration 4/1000 | Loss: 0.00003960
Iteration 5/1000 | Loss: 0.00004205
Iteration 6/1000 | Loss: 0.00004631
Iteration 7/1000 | Loss: 0.00032640
Iteration 8/1000 | Loss: 0.00023950
Iteration 9/1000 | Loss: 0.00042102
Iteration 10/1000 | Loss: 0.00021578
Iteration 11/1000 | Loss: 0.00009641
Iteration 12/1000 | Loss: 0.00005566
Iteration 13/1000 | Loss: 0.00004928
Iteration 14/1000 | Loss: 0.00005611
Iteration 15/1000 | Loss: 0.00004517
Iteration 16/1000 | Loss: 0.00003357
Iteration 17/1000 | Loss: 0.00005546
Iteration 18/1000 | Loss: 0.00005103
Iteration 19/1000 | Loss: 0.00004699
Iteration 20/1000 | Loss: 0.00004468
Iteration 21/1000 | Loss: 0.00004255
Iteration 22/1000 | Loss: 0.00004486
Iteration 23/1000 | Loss: 0.00009938
Iteration 24/1000 | Loss: 0.00009964
Iteration 25/1000 | Loss: 0.00006236
Iteration 26/1000 | Loss: 0.00003722
Iteration 27/1000 | Loss: 0.00004937
Iteration 28/1000 | Loss: 0.00008411
Iteration 29/1000 | Loss: 0.00005092
Iteration 30/1000 | Loss: 0.00004625
Iteration 31/1000 | Loss: 0.00004865
Iteration 32/1000 | Loss: 0.00004973
Iteration 33/1000 | Loss: 0.00003991
Iteration 34/1000 | Loss: 0.00003034
Iteration 35/1000 | Loss: 0.00002704
Iteration 36/1000 | Loss: 0.00004001
Iteration 37/1000 | Loss: 0.00003873
Iteration 38/1000 | Loss: 0.00004823
Iteration 39/1000 | Loss: 0.00003739
Iteration 40/1000 | Loss: 0.00004712
Iteration 41/1000 | Loss: 0.00004091
Iteration 42/1000 | Loss: 0.00004514
Iteration 43/1000 | Loss: 0.00003487
Iteration 44/1000 | Loss: 0.00002762
Iteration 45/1000 | Loss: 0.00002654
Iteration 46/1000 | Loss: 0.00004026
Iteration 47/1000 | Loss: 0.00003384
Iteration 48/1000 | Loss: 0.00003516
Iteration 49/1000 | Loss: 0.00004046
Iteration 50/1000 | Loss: 0.00004411
Iteration 51/1000 | Loss: 0.00004479
Iteration 52/1000 | Loss: 0.00003807
Iteration 53/1000 | Loss: 0.00003073
Iteration 54/1000 | Loss: 0.00004317
Iteration 55/1000 | Loss: 0.00004224
Iteration 56/1000 | Loss: 0.00004122
Iteration 57/1000 | Loss: 0.00003918
Iteration 58/1000 | Loss: 0.00004014
Iteration 59/1000 | Loss: 0.00003724
Iteration 60/1000 | Loss: 0.00003998
Iteration 61/1000 | Loss: 0.00004674
Iteration 62/1000 | Loss: 0.00005135
Iteration 63/1000 | Loss: 0.00003789
Iteration 64/1000 | Loss: 0.00003398
Iteration 65/1000 | Loss: 0.00004270
Iteration 66/1000 | Loss: 0.00005526
Iteration 67/1000 | Loss: 0.00007091
Iteration 68/1000 | Loss: 0.00004408
Iteration 69/1000 | Loss: 0.00005366
Iteration 70/1000 | Loss: 0.00004436
Iteration 71/1000 | Loss: 0.00004098
Iteration 72/1000 | Loss: 0.00004075
Iteration 73/1000 | Loss: 0.00003974
Iteration 74/1000 | Loss: 0.00003364
Iteration 75/1000 | Loss: 0.00003915
Iteration 76/1000 | Loss: 0.00003296
Iteration 77/1000 | Loss: 0.00004247
Iteration 78/1000 | Loss: 0.00004867
Iteration 79/1000 | Loss: 0.00004235
Iteration 80/1000 | Loss: 0.00002789
Iteration 81/1000 | Loss: 0.00002528
Iteration 82/1000 | Loss: 0.00002401
Iteration 83/1000 | Loss: 0.00002335
Iteration 84/1000 | Loss: 0.00002213
Iteration 85/1000 | Loss: 0.00002166
Iteration 86/1000 | Loss: 0.00002147
Iteration 87/1000 | Loss: 0.00002131
Iteration 88/1000 | Loss: 0.00002130
Iteration 89/1000 | Loss: 0.00002129
Iteration 90/1000 | Loss: 0.00002129
Iteration 91/1000 | Loss: 0.00002117
Iteration 92/1000 | Loss: 0.00002115
Iteration 93/1000 | Loss: 0.00002090
Iteration 94/1000 | Loss: 0.00002271
Iteration 95/1000 | Loss: 0.00002060
Iteration 96/1000 | Loss: 0.00034887
Iteration 97/1000 | Loss: 0.00005519
Iteration 98/1000 | Loss: 0.00008882
Iteration 99/1000 | Loss: 0.00002576
Iteration 100/1000 | Loss: 0.00008150
Iteration 101/1000 | Loss: 0.00019281
Iteration 102/1000 | Loss: 0.00012348
Iteration 103/1000 | Loss: 0.00016626
Iteration 104/1000 | Loss: 0.00002446
Iteration 105/1000 | Loss: 0.00002050
Iteration 106/1000 | Loss: 0.00002015
Iteration 107/1000 | Loss: 0.00002006
Iteration 108/1000 | Loss: 0.00002005
Iteration 109/1000 | Loss: 0.00002004
Iteration 110/1000 | Loss: 0.00001998
Iteration 111/1000 | Loss: 0.00001996
Iteration 112/1000 | Loss: 0.00001996
Iteration 113/1000 | Loss: 0.00001996
Iteration 114/1000 | Loss: 0.00001996
Iteration 115/1000 | Loss: 0.00001996
Iteration 116/1000 | Loss: 0.00001996
Iteration 117/1000 | Loss: 0.00001995
Iteration 118/1000 | Loss: 0.00001995
Iteration 119/1000 | Loss: 0.00001995
Iteration 120/1000 | Loss: 0.00001995
Iteration 121/1000 | Loss: 0.00001995
Iteration 122/1000 | Loss: 0.00001995
Iteration 123/1000 | Loss: 0.00001995
Iteration 124/1000 | Loss: 0.00001995
Iteration 125/1000 | Loss: 0.00001995
Iteration 126/1000 | Loss: 0.00001995
Iteration 127/1000 | Loss: 0.00001995
Iteration 128/1000 | Loss: 0.00001994
Iteration 129/1000 | Loss: 0.00001994
Iteration 130/1000 | Loss: 0.00001994
Iteration 131/1000 | Loss: 0.00001994
Iteration 132/1000 | Loss: 0.00001993
Iteration 133/1000 | Loss: 0.00001993
Iteration 134/1000 | Loss: 0.00001992
Iteration 135/1000 | Loss: 0.00001992
Iteration 136/1000 | Loss: 0.00001987
Iteration 137/1000 | Loss: 0.00001987
Iteration 138/1000 | Loss: 0.00001986
Iteration 139/1000 | Loss: 0.00001985
Iteration 140/1000 | Loss: 0.00001982
Iteration 141/1000 | Loss: 0.00001982
Iteration 142/1000 | Loss: 0.00001982
Iteration 143/1000 | Loss: 0.00001982
Iteration 144/1000 | Loss: 0.00001982
Iteration 145/1000 | Loss: 0.00001982
Iteration 146/1000 | Loss: 0.00001981
Iteration 147/1000 | Loss: 0.00001981
Iteration 148/1000 | Loss: 0.00001981
Iteration 149/1000 | Loss: 0.00001981
Iteration 150/1000 | Loss: 0.00001980
Iteration 151/1000 | Loss: 0.00001979
Iteration 152/1000 | Loss: 0.00001979
Iteration 153/1000 | Loss: 0.00001978
Iteration 154/1000 | Loss: 0.00001978
Iteration 155/1000 | Loss: 0.00001977
Iteration 156/1000 | Loss: 0.00001976
Iteration 157/1000 | Loss: 0.00001976
Iteration 158/1000 | Loss: 0.00001975
Iteration 159/1000 | Loss: 0.00001975
Iteration 160/1000 | Loss: 0.00001975
Iteration 161/1000 | Loss: 0.00001974
Iteration 162/1000 | Loss: 0.00001974
Iteration 163/1000 | Loss: 0.00001974
Iteration 164/1000 | Loss: 0.00001974
Iteration 165/1000 | Loss: 0.00001973
Iteration 166/1000 | Loss: 0.00001973
Iteration 167/1000 | Loss: 0.00001973
Iteration 168/1000 | Loss: 0.00001973
Iteration 169/1000 | Loss: 0.00001973
Iteration 170/1000 | Loss: 0.00001973
Iteration 171/1000 | Loss: 0.00001973
Iteration 172/1000 | Loss: 0.00001973
Iteration 173/1000 | Loss: 0.00001972
Iteration 174/1000 | Loss: 0.00001972
Iteration 175/1000 | Loss: 0.00001971
Iteration 176/1000 | Loss: 0.00001971
Iteration 177/1000 | Loss: 0.00001971
Iteration 178/1000 | Loss: 0.00001971
Iteration 179/1000 | Loss: 0.00001971
Iteration 180/1000 | Loss: 0.00001971
Iteration 181/1000 | Loss: 0.00001971
Iteration 182/1000 | Loss: 0.00001971
Iteration 183/1000 | Loss: 0.00001970
Iteration 184/1000 | Loss: 0.00001970
Iteration 185/1000 | Loss: 0.00001970
Iteration 186/1000 | Loss: 0.00001970
Iteration 187/1000 | Loss: 0.00001970
Iteration 188/1000 | Loss: 0.00001970
Iteration 189/1000 | Loss: 0.00001970
Iteration 190/1000 | Loss: 0.00001970
Iteration 191/1000 | Loss: 0.00001970
Iteration 192/1000 | Loss: 0.00001970
Iteration 193/1000 | Loss: 0.00001970
Iteration 194/1000 | Loss: 0.00001969
Iteration 195/1000 | Loss: 0.00001969
Iteration 196/1000 | Loss: 0.00001969
Iteration 197/1000 | Loss: 0.00001969
Iteration 198/1000 | Loss: 0.00001969
Iteration 199/1000 | Loss: 0.00001969
Iteration 200/1000 | Loss: 0.00001969
Iteration 201/1000 | Loss: 0.00001968
Iteration 202/1000 | Loss: 0.00001968
Iteration 203/1000 | Loss: 0.00001968
Iteration 204/1000 | Loss: 0.00001968
Iteration 205/1000 | Loss: 0.00001967
Iteration 206/1000 | Loss: 0.00001967
Iteration 207/1000 | Loss: 0.00001967
Iteration 208/1000 | Loss: 0.00001967
Iteration 209/1000 | Loss: 0.00001967
Iteration 210/1000 | Loss: 0.00001967
Iteration 211/1000 | Loss: 0.00001967
Iteration 212/1000 | Loss: 0.00001967
Iteration 213/1000 | Loss: 0.00021551
Iteration 214/1000 | Loss: 0.00026330
Iteration 215/1000 | Loss: 0.00010146
Iteration 216/1000 | Loss: 0.00017163
Iteration 217/1000 | Loss: 0.00008960
Iteration 218/1000 | Loss: 0.00029301
Iteration 219/1000 | Loss: 0.00031156
Iteration 220/1000 | Loss: 0.00015636
Iteration 221/1000 | Loss: 0.00002719
Iteration 222/1000 | Loss: 0.00018682
Iteration 223/1000 | Loss: 0.00018189
Iteration 224/1000 | Loss: 0.00026731
Iteration 225/1000 | Loss: 0.00018549
Iteration 226/1000 | Loss: 0.00024601
Iteration 227/1000 | Loss: 0.00019118
Iteration 228/1000 | Loss: 0.00021287
Iteration 229/1000 | Loss: 0.00003802
Iteration 230/1000 | Loss: 0.00037674
Iteration 231/1000 | Loss: 0.00027108
Iteration 232/1000 | Loss: 0.00023286
Iteration 233/1000 | Loss: 0.00027059
Iteration 234/1000 | Loss: 0.00027983
Iteration 235/1000 | Loss: 0.00009121
Iteration 236/1000 | Loss: 0.00023306
Iteration 237/1000 | Loss: 0.00002324
Iteration 238/1000 | Loss: 0.00002208
Iteration 239/1000 | Loss: 0.00002160
Iteration 240/1000 | Loss: 0.00002124
Iteration 241/1000 | Loss: 0.00002101
Iteration 242/1000 | Loss: 0.00011047
Iteration 243/1000 | Loss: 0.00002747
Iteration 244/1000 | Loss: 0.00003720
Iteration 245/1000 | Loss: 0.00002444
Iteration 246/1000 | Loss: 0.00002049
Iteration 247/1000 | Loss: 0.00002041
Iteration 248/1000 | Loss: 0.00002033
Iteration 249/1000 | Loss: 0.00002033
Iteration 250/1000 | Loss: 0.00002030
Iteration 251/1000 | Loss: 0.00042308
Iteration 252/1000 | Loss: 0.00018734
Iteration 253/1000 | Loss: 0.00002894
Iteration 254/1000 | Loss: 0.00002340
Iteration 255/1000 | Loss: 0.00002041
Iteration 256/1000 | Loss: 0.00041858
Iteration 257/1000 | Loss: 0.00012230
Iteration 258/1000 | Loss: 0.00003092
Iteration 259/1000 | Loss: 0.00002430
Iteration 260/1000 | Loss: 0.00002273
Iteration 261/1000 | Loss: 0.00002147
Iteration 262/1000 | Loss: 0.00031119
Iteration 263/1000 | Loss: 0.00039350
Iteration 264/1000 | Loss: 0.00002397
Iteration 265/1000 | Loss: 0.00002185
Iteration 266/1000 | Loss: 0.00002121
Iteration 267/1000 | Loss: 0.00002067
Iteration 268/1000 | Loss: 0.00008120
Iteration 269/1000 | Loss: 0.00002048
Iteration 270/1000 | Loss: 0.00001979
Iteration 271/1000 | Loss: 0.00001952
Iteration 272/1000 | Loss: 0.00001935
Iteration 273/1000 | Loss: 0.00001935
Iteration 274/1000 | Loss: 0.00001929
Iteration 275/1000 | Loss: 0.00001925
Iteration 276/1000 | Loss: 0.00001925
Iteration 277/1000 | Loss: 0.00001925
Iteration 278/1000 | Loss: 0.00001925
Iteration 279/1000 | Loss: 0.00001925
Iteration 280/1000 | Loss: 0.00001925
Iteration 281/1000 | Loss: 0.00001924
Iteration 282/1000 | Loss: 0.00001924
Iteration 283/1000 | Loss: 0.00001924
Iteration 284/1000 | Loss: 0.00001924
Iteration 285/1000 | Loss: 0.00001924
Iteration 286/1000 | Loss: 0.00001924
Iteration 287/1000 | Loss: 0.00001924
Iteration 288/1000 | Loss: 0.00001924
Iteration 289/1000 | Loss: 0.00001924
Iteration 290/1000 | Loss: 0.00001924
Iteration 291/1000 | Loss: 0.00001922
Iteration 292/1000 | Loss: 0.00001917
Iteration 293/1000 | Loss: 0.00001917
Iteration 294/1000 | Loss: 0.00001917
Iteration 295/1000 | Loss: 0.00001916
Iteration 296/1000 | Loss: 0.00001916
Iteration 297/1000 | Loss: 0.00001916
Iteration 298/1000 | Loss: 0.00001916
Iteration 299/1000 | Loss: 0.00001916
Iteration 300/1000 | Loss: 0.00001916
Iteration 301/1000 | Loss: 0.00001916
Iteration 302/1000 | Loss: 0.00001916
Iteration 303/1000 | Loss: 0.00001916
Iteration 304/1000 | Loss: 0.00001916
Iteration 305/1000 | Loss: 0.00001916
Iteration 306/1000 | Loss: 0.00001916
Iteration 307/1000 | Loss: 0.00001916
Iteration 308/1000 | Loss: 0.00001915
Iteration 309/1000 | Loss: 0.00001915
Iteration 310/1000 | Loss: 0.00001915
Iteration 311/1000 | Loss: 0.00001915
Iteration 312/1000 | Loss: 0.00001915
Iteration 313/1000 | Loss: 0.00001914
Iteration 314/1000 | Loss: 0.00001914
Iteration 315/1000 | Loss: 0.00001914
Iteration 316/1000 | Loss: 0.00001913
Iteration 317/1000 | Loss: 0.00001913
Iteration 318/1000 | Loss: 0.00001913
Iteration 319/1000 | Loss: 0.00001913
Iteration 320/1000 | Loss: 0.00001913
Iteration 321/1000 | Loss: 0.00001912
Iteration 322/1000 | Loss: 0.00001912
Iteration 323/1000 | Loss: 0.00001912
Iteration 324/1000 | Loss: 0.00001912
Iteration 325/1000 | Loss: 0.00001912
Iteration 326/1000 | Loss: 0.00001911
Iteration 327/1000 | Loss: 0.00001911
Iteration 328/1000 | Loss: 0.00001911
Iteration 329/1000 | Loss: 0.00001910
Iteration 330/1000 | Loss: 0.00001910
Iteration 331/1000 | Loss: 0.00001909
Iteration 332/1000 | Loss: 0.00001909
Iteration 333/1000 | Loss: 0.00001909
Iteration 334/1000 | Loss: 0.00001909
Iteration 335/1000 | Loss: 0.00001909
Iteration 336/1000 | Loss: 0.00001908
Iteration 337/1000 | Loss: 0.00001908
Iteration 338/1000 | Loss: 0.00001908
Iteration 339/1000 | Loss: 0.00001908
Iteration 340/1000 | Loss: 0.00001908
Iteration 341/1000 | Loss: 0.00001907
Iteration 342/1000 | Loss: 0.00001907
Iteration 343/1000 | Loss: 0.00001907
Iteration 344/1000 | Loss: 0.00001907
Iteration 345/1000 | Loss: 0.00001906
Iteration 346/1000 | Loss: 0.00001906
Iteration 347/1000 | Loss: 0.00001906
Iteration 348/1000 | Loss: 0.00001906
Iteration 349/1000 | Loss: 0.00001906
Iteration 350/1000 | Loss: 0.00001906
Iteration 351/1000 | Loss: 0.00001906
Iteration 352/1000 | Loss: 0.00001906
Iteration 353/1000 | Loss: 0.00001906
Iteration 354/1000 | Loss: 0.00001906
Iteration 355/1000 | Loss: 0.00001906
Iteration 356/1000 | Loss: 0.00001906
Iteration 357/1000 | Loss: 0.00001905
Iteration 358/1000 | Loss: 0.00007384
Iteration 359/1000 | Loss: 0.00001910
Iteration 360/1000 | Loss: 0.00001906
Iteration 361/1000 | Loss: 0.00001906
Iteration 362/1000 | Loss: 0.00001905
Iteration 363/1000 | Loss: 0.00001905
Iteration 364/1000 | Loss: 0.00001905
Iteration 365/1000 | Loss: 0.00001905
Iteration 366/1000 | Loss: 0.00001905
Iteration 367/1000 | Loss: 0.00001905
Iteration 368/1000 | Loss: 0.00001905
Iteration 369/1000 | Loss: 0.00001905
Iteration 370/1000 | Loss: 0.00001905
Iteration 371/1000 | Loss: 0.00001905
Iteration 372/1000 | Loss: 0.00001905
Iteration 373/1000 | Loss: 0.00001904
Iteration 374/1000 | Loss: 0.00001904
Iteration 375/1000 | Loss: 0.00001904
Iteration 376/1000 | Loss: 0.00001903
Iteration 377/1000 | Loss: 0.00001903
Iteration 378/1000 | Loss: 0.00001903
Iteration 379/1000 | Loss: 0.00001902
Iteration 380/1000 | Loss: 0.00001902
Iteration 381/1000 | Loss: 0.00001902
Iteration 382/1000 | Loss: 0.00001901
Iteration 383/1000 | Loss: 0.00001901
Iteration 384/1000 | Loss: 0.00001901
Iteration 385/1000 | Loss: 0.00001900
Iteration 386/1000 | Loss: 0.00001900
Iteration 387/1000 | Loss: 0.00001900
Iteration 388/1000 | Loss: 0.00001900
Iteration 389/1000 | Loss: 0.00001900
Iteration 390/1000 | Loss: 0.00001900
Iteration 391/1000 | Loss: 0.00001900
Iteration 392/1000 | Loss: 0.00001899
Iteration 393/1000 | Loss: 0.00001899
Iteration 394/1000 | Loss: 0.00001899
Iteration 395/1000 | Loss: 0.00001899
Iteration 396/1000 | Loss: 0.00001899
Iteration 397/1000 | Loss: 0.00001899
Iteration 398/1000 | Loss: 0.00001899
Iteration 399/1000 | Loss: 0.00001899
Iteration 400/1000 | Loss: 0.00001899
Iteration 401/1000 | Loss: 0.00001899
Iteration 402/1000 | Loss: 0.00001899
Iteration 403/1000 | Loss: 0.00001899
Iteration 404/1000 | Loss: 0.00001899
Iteration 405/1000 | Loss: 0.00001899
Iteration 406/1000 | Loss: 0.00001899
Iteration 407/1000 | Loss: 0.00001899
Iteration 408/1000 | Loss: 0.00001899
Iteration 409/1000 | Loss: 0.00001899
Iteration 410/1000 | Loss: 0.00001899
Iteration 411/1000 | Loss: 0.00001898
Iteration 412/1000 | Loss: 0.00001898
Iteration 413/1000 | Loss: 0.00001898
Iteration 414/1000 | Loss: 0.00001898
Iteration 415/1000 | Loss: 0.00001898
Iteration 416/1000 | Loss: 0.00001898
Iteration 417/1000 | Loss: 0.00001898
Iteration 418/1000 | Loss: 0.00001898
Iteration 419/1000 | Loss: 0.00001898
Iteration 420/1000 | Loss: 0.00001898
Iteration 421/1000 | Loss: 0.00001897
Iteration 422/1000 | Loss: 0.00001897
Iteration 423/1000 | Loss: 0.00001897
Iteration 424/1000 | Loss: 0.00007178
Iteration 425/1000 | Loss: 0.00002313
Iteration 426/1000 | Loss: 0.00001904
Iteration 427/1000 | Loss: 0.00004845
Iteration 428/1000 | Loss: 0.00001908
Iteration 429/1000 | Loss: 0.00001900
Iteration 430/1000 | Loss: 0.00001899
Iteration 431/1000 | Loss: 0.00001899
Iteration 432/1000 | Loss: 0.00001899
Iteration 433/1000 | Loss: 0.00001899
Iteration 434/1000 | Loss: 0.00001898
Iteration 435/1000 | Loss: 0.00001898
Iteration 436/1000 | Loss: 0.00001897
Iteration 437/1000 | Loss: 0.00001897
Iteration 438/1000 | Loss: 0.00001897
Iteration 439/1000 | Loss: 0.00001897
Iteration 440/1000 | Loss: 0.00001897
Iteration 441/1000 | Loss: 0.00001897
Iteration 442/1000 | Loss: 0.00001897
Iteration 443/1000 | Loss: 0.00001896
Iteration 444/1000 | Loss: 0.00001896
Iteration 445/1000 | Loss: 0.00001896
Iteration 446/1000 | Loss: 0.00001896
Iteration 447/1000 | Loss: 0.00001896
Iteration 448/1000 | Loss: 0.00001896
Iteration 449/1000 | Loss: 0.00001895
Iteration 450/1000 | Loss: 0.00001895
Iteration 451/1000 | Loss: 0.00001895
Iteration 452/1000 | Loss: 0.00001895
Iteration 453/1000 | Loss: 0.00001895
Iteration 454/1000 | Loss: 0.00001895
Iteration 455/1000 | Loss: 0.00001895
Iteration 456/1000 | Loss: 0.00001894
Iteration 457/1000 | Loss: 0.00001894
Iteration 458/1000 | Loss: 0.00001894
Iteration 459/1000 | Loss: 0.00001894
Iteration 460/1000 | Loss: 0.00001894
Iteration 461/1000 | Loss: 0.00001894
Iteration 462/1000 | Loss: 0.00001894
Iteration 463/1000 | Loss: 0.00001894
Iteration 464/1000 | Loss: 0.00001894
Iteration 465/1000 | Loss: 0.00001894
Iteration 466/1000 | Loss: 0.00001894
Iteration 467/1000 | Loss: 0.00001894
Iteration 468/1000 | Loss: 0.00001894
Iteration 469/1000 | Loss: 0.00001894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 469. Stopping optimization.
Last 5 losses: [1.8940048903459683e-05, 1.8940048903459683e-05, 1.8940048903459683e-05, 1.8940048903459683e-05, 1.8940048903459683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8940048903459683e-05

Optimization complete. Final v2v error: 3.6058785915374756 mm

Highest mean error: 4.751214027404785 mm for frame 47

Lowest mean error: 3.257612466812134 mm for frame 34

Saving results

Total time: 301.84999561309814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844090
Iteration 2/25 | Loss: 0.00205564
Iteration 3/25 | Loss: 0.00162620
Iteration 4/25 | Loss: 0.00154398
Iteration 5/25 | Loss: 0.00152333
Iteration 6/25 | Loss: 0.00150699
Iteration 7/25 | Loss: 0.00151119
Iteration 8/25 | Loss: 0.00148483
Iteration 9/25 | Loss: 0.00148048
Iteration 10/25 | Loss: 0.00146336
Iteration 11/25 | Loss: 0.00145468
Iteration 12/25 | Loss: 0.00146355
Iteration 13/25 | Loss: 0.00146172
Iteration 14/25 | Loss: 0.00144905
Iteration 15/25 | Loss: 0.00143980
Iteration 16/25 | Loss: 0.00143704
Iteration 17/25 | Loss: 0.00143631
Iteration 18/25 | Loss: 0.00143908
Iteration 19/25 | Loss: 0.00143964
Iteration 20/25 | Loss: 0.00143712
Iteration 21/25 | Loss: 0.00143492
Iteration 22/25 | Loss: 0.00143421
Iteration 23/25 | Loss: 0.00143393
Iteration 24/25 | Loss: 0.00143380
Iteration 25/25 | Loss: 0.00143375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45789123
Iteration 2/25 | Loss: 0.00076738
Iteration 3/25 | Loss: 0.00076735
Iteration 4/25 | Loss: 0.00076735
Iteration 5/25 | Loss: 0.00076735
Iteration 6/25 | Loss: 0.00076734
Iteration 7/25 | Loss: 0.00076734
Iteration 8/25 | Loss: 0.00076734
Iteration 9/25 | Loss: 0.00076734
Iteration 10/25 | Loss: 0.00076734
Iteration 11/25 | Loss: 0.00076734
Iteration 12/25 | Loss: 0.00076734
Iteration 13/25 | Loss: 0.00076734
Iteration 14/25 | Loss: 0.00076734
Iteration 15/25 | Loss: 0.00076734
Iteration 16/25 | Loss: 0.00076734
Iteration 17/25 | Loss: 0.00076734
Iteration 18/25 | Loss: 0.00076734
Iteration 19/25 | Loss: 0.00076734
Iteration 20/25 | Loss: 0.00076734
Iteration 21/25 | Loss: 0.00076734
Iteration 22/25 | Loss: 0.00076734
Iteration 23/25 | Loss: 0.00076734
Iteration 24/25 | Loss: 0.00076734
Iteration 25/25 | Loss: 0.00076734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076734
Iteration 2/1000 | Loss: 0.00004088
Iteration 3/1000 | Loss: 0.00003233
Iteration 4/1000 | Loss: 0.00003046
Iteration 5/1000 | Loss: 0.00002943
Iteration 6/1000 | Loss: 0.00002859
Iteration 7/1000 | Loss: 0.00002806
Iteration 8/1000 | Loss: 0.00002767
Iteration 9/1000 | Loss: 0.00016407
Iteration 10/1000 | Loss: 0.00003263
Iteration 11/1000 | Loss: 0.00011061
Iteration 12/1000 | Loss: 0.00014875
Iteration 13/1000 | Loss: 0.00012606
Iteration 14/1000 | Loss: 0.00013932
Iteration 15/1000 | Loss: 0.00012042
Iteration 16/1000 | Loss: 0.00014880
Iteration 17/1000 | Loss: 0.00009401
Iteration 18/1000 | Loss: 0.00012380
Iteration 19/1000 | Loss: 0.00003333
Iteration 20/1000 | Loss: 0.00007956
Iteration 21/1000 | Loss: 0.00010326
Iteration 22/1000 | Loss: 0.00003560
Iteration 23/1000 | Loss: 0.00003145
Iteration 24/1000 | Loss: 0.00036960
Iteration 25/1000 | Loss: 0.00008885
Iteration 26/1000 | Loss: 0.00003233
Iteration 27/1000 | Loss: 0.00003541
Iteration 28/1000 | Loss: 0.00002702
Iteration 29/1000 | Loss: 0.00002632
Iteration 30/1000 | Loss: 0.00002576
Iteration 31/1000 | Loss: 0.00002545
Iteration 32/1000 | Loss: 0.00003138
Iteration 33/1000 | Loss: 0.00002525
Iteration 34/1000 | Loss: 0.00002893
Iteration 35/1000 | Loss: 0.00002526
Iteration 36/1000 | Loss: 0.00002519
Iteration 37/1000 | Loss: 0.00002519
Iteration 38/1000 | Loss: 0.00002518
Iteration 39/1000 | Loss: 0.00002518
Iteration 40/1000 | Loss: 0.00002518
Iteration 41/1000 | Loss: 0.00002517
Iteration 42/1000 | Loss: 0.00002516
Iteration 43/1000 | Loss: 0.00002516
Iteration 44/1000 | Loss: 0.00002838
Iteration 45/1000 | Loss: 0.00002510
Iteration 46/1000 | Loss: 0.00002593
Iteration 47/1000 | Loss: 0.00002503
Iteration 48/1000 | Loss: 0.00002503
Iteration 49/1000 | Loss: 0.00002503
Iteration 50/1000 | Loss: 0.00002503
Iteration 51/1000 | Loss: 0.00002503
Iteration 52/1000 | Loss: 0.00002503
Iteration 53/1000 | Loss: 0.00002503
Iteration 54/1000 | Loss: 0.00002499
Iteration 55/1000 | Loss: 0.00002498
Iteration 56/1000 | Loss: 0.00002498
Iteration 57/1000 | Loss: 0.00002498
Iteration 58/1000 | Loss: 0.00002498
Iteration 59/1000 | Loss: 0.00002498
Iteration 60/1000 | Loss: 0.00002497
Iteration 61/1000 | Loss: 0.00002497
Iteration 62/1000 | Loss: 0.00002496
Iteration 63/1000 | Loss: 0.00002496
Iteration 64/1000 | Loss: 0.00002496
Iteration 65/1000 | Loss: 0.00002496
Iteration 66/1000 | Loss: 0.00002496
Iteration 67/1000 | Loss: 0.00002495
Iteration 68/1000 | Loss: 0.00002491
Iteration 69/1000 | Loss: 0.00002490
Iteration 70/1000 | Loss: 0.00002488
Iteration 71/1000 | Loss: 0.00002488
Iteration 72/1000 | Loss: 0.00002488
Iteration 73/1000 | Loss: 0.00002488
Iteration 74/1000 | Loss: 0.00002488
Iteration 75/1000 | Loss: 0.00002488
Iteration 76/1000 | Loss: 0.00002488
Iteration 77/1000 | Loss: 0.00002488
Iteration 78/1000 | Loss: 0.00002488
Iteration 79/1000 | Loss: 0.00002487
Iteration 80/1000 | Loss: 0.00002486
Iteration 81/1000 | Loss: 0.00002486
Iteration 82/1000 | Loss: 0.00002485
Iteration 83/1000 | Loss: 0.00002485
Iteration 84/1000 | Loss: 0.00002484
Iteration 85/1000 | Loss: 0.00002484
Iteration 86/1000 | Loss: 0.00002484
Iteration 87/1000 | Loss: 0.00002484
Iteration 88/1000 | Loss: 0.00002484
Iteration 89/1000 | Loss: 0.00002484
Iteration 90/1000 | Loss: 0.00002484
Iteration 91/1000 | Loss: 0.00002484
Iteration 92/1000 | Loss: 0.00002484
Iteration 93/1000 | Loss: 0.00002484
Iteration 94/1000 | Loss: 0.00002484
Iteration 95/1000 | Loss: 0.00002483
Iteration 96/1000 | Loss: 0.00002483
Iteration 97/1000 | Loss: 0.00002483
Iteration 98/1000 | Loss: 0.00002483
Iteration 99/1000 | Loss: 0.00002483
Iteration 100/1000 | Loss: 0.00002483
Iteration 101/1000 | Loss: 0.00002483
Iteration 102/1000 | Loss: 0.00002483
Iteration 103/1000 | Loss: 0.00002483
Iteration 104/1000 | Loss: 0.00002483
Iteration 105/1000 | Loss: 0.00002482
Iteration 106/1000 | Loss: 0.00002482
Iteration 107/1000 | Loss: 0.00002482
Iteration 108/1000 | Loss: 0.00002482
Iteration 109/1000 | Loss: 0.00002482
Iteration 110/1000 | Loss: 0.00002482
Iteration 111/1000 | Loss: 0.00002482
Iteration 112/1000 | Loss: 0.00002482
Iteration 113/1000 | Loss: 0.00002482
Iteration 114/1000 | Loss: 0.00002482
Iteration 115/1000 | Loss: 0.00002482
Iteration 116/1000 | Loss: 0.00002481
Iteration 117/1000 | Loss: 0.00002481
Iteration 118/1000 | Loss: 0.00002481
Iteration 119/1000 | Loss: 0.00002481
Iteration 120/1000 | Loss: 0.00002481
Iteration 121/1000 | Loss: 0.00002480
Iteration 122/1000 | Loss: 0.00002480
Iteration 123/1000 | Loss: 0.00002480
Iteration 124/1000 | Loss: 0.00002480
Iteration 125/1000 | Loss: 0.00002480
Iteration 126/1000 | Loss: 0.00002480
Iteration 127/1000 | Loss: 0.00002480
Iteration 128/1000 | Loss: 0.00002480
Iteration 129/1000 | Loss: 0.00002480
Iteration 130/1000 | Loss: 0.00002480
Iteration 131/1000 | Loss: 0.00002479
Iteration 132/1000 | Loss: 0.00002479
Iteration 133/1000 | Loss: 0.00002479
Iteration 134/1000 | Loss: 0.00002479
Iteration 135/1000 | Loss: 0.00002479
Iteration 136/1000 | Loss: 0.00002479
Iteration 137/1000 | Loss: 0.00002479
Iteration 138/1000 | Loss: 0.00002479
Iteration 139/1000 | Loss: 0.00002479
Iteration 140/1000 | Loss: 0.00002479
Iteration 141/1000 | Loss: 0.00002479
Iteration 142/1000 | Loss: 0.00002479
Iteration 143/1000 | Loss: 0.00002479
Iteration 144/1000 | Loss: 0.00002479
Iteration 145/1000 | Loss: 0.00002479
Iteration 146/1000 | Loss: 0.00002479
Iteration 147/1000 | Loss: 0.00002479
Iteration 148/1000 | Loss: 0.00002479
Iteration 149/1000 | Loss: 0.00002479
Iteration 150/1000 | Loss: 0.00002479
Iteration 151/1000 | Loss: 0.00002479
Iteration 152/1000 | Loss: 0.00002479
Iteration 153/1000 | Loss: 0.00002479
Iteration 154/1000 | Loss: 0.00002479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.478662645444274e-05, 2.478662645444274e-05, 2.478662645444274e-05, 2.478662645444274e-05, 2.478662645444274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.478662645444274e-05

Optimization complete. Final v2v error: 4.0554327964782715 mm

Highest mean error: 6.47847843170166 mm for frame 12

Lowest mean error: 3.605555534362793 mm for frame 203

Saving results

Total time: 117.92904949188232
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046627
Iteration 2/25 | Loss: 0.00200579
Iteration 3/25 | Loss: 0.00153017
Iteration 4/25 | Loss: 0.00148296
Iteration 5/25 | Loss: 0.00150717
Iteration 6/25 | Loss: 0.00142932
Iteration 7/25 | Loss: 0.00142110
Iteration 8/25 | Loss: 0.00142542
Iteration 9/25 | Loss: 0.00139685
Iteration 10/25 | Loss: 0.00137809
Iteration 11/25 | Loss: 0.00137543
Iteration 12/25 | Loss: 0.00137066
Iteration 13/25 | Loss: 0.00136484
Iteration 14/25 | Loss: 0.00136305
Iteration 15/25 | Loss: 0.00136193
Iteration 16/25 | Loss: 0.00136148
Iteration 17/25 | Loss: 0.00136086
Iteration 18/25 | Loss: 0.00136210
Iteration 19/25 | Loss: 0.00136421
Iteration 20/25 | Loss: 0.00136258
Iteration 21/25 | Loss: 0.00135926
Iteration 22/25 | Loss: 0.00135910
Iteration 23/25 | Loss: 0.00135906
Iteration 24/25 | Loss: 0.00135905
Iteration 25/25 | Loss: 0.00135905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.05475616
Iteration 2/25 | Loss: 0.00110760
Iteration 3/25 | Loss: 0.00110747
Iteration 4/25 | Loss: 0.00110747
Iteration 5/25 | Loss: 0.00110747
Iteration 6/25 | Loss: 0.00110747
Iteration 7/25 | Loss: 0.00110747
Iteration 8/25 | Loss: 0.00110747
Iteration 9/25 | Loss: 0.00110747
Iteration 10/25 | Loss: 0.00110747
Iteration 11/25 | Loss: 0.00110747
Iteration 12/25 | Loss: 0.00110747
Iteration 13/25 | Loss: 0.00110747
Iteration 14/25 | Loss: 0.00110747
Iteration 15/25 | Loss: 0.00110747
Iteration 16/25 | Loss: 0.00110747
Iteration 17/25 | Loss: 0.00110747
Iteration 18/25 | Loss: 0.00110747
Iteration 19/25 | Loss: 0.00110747
Iteration 20/25 | Loss: 0.00110747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011074653593823314, 0.0011074653593823314, 0.0011074653593823314, 0.0011074653593823314, 0.0011074653593823314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011074653593823314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110747
Iteration 2/1000 | Loss: 0.00008231
Iteration 3/1000 | Loss: 0.00005188
Iteration 4/1000 | Loss: 0.00003986
Iteration 5/1000 | Loss: 0.00003666
Iteration 6/1000 | Loss: 0.00003499
Iteration 7/1000 | Loss: 0.00003357
Iteration 8/1000 | Loss: 0.00003264
Iteration 9/1000 | Loss: 0.00003196
Iteration 10/1000 | Loss: 0.00003136
Iteration 11/1000 | Loss: 0.00019268
Iteration 12/1000 | Loss: 0.00015553
Iteration 13/1000 | Loss: 0.00003105
Iteration 14/1000 | Loss: 0.00003060
Iteration 15/1000 | Loss: 0.00019551
Iteration 16/1000 | Loss: 0.00016935
Iteration 17/1000 | Loss: 0.00004866
Iteration 18/1000 | Loss: 0.00017141
Iteration 19/1000 | Loss: 0.00015685
Iteration 20/1000 | Loss: 0.00020910
Iteration 21/1000 | Loss: 0.00004205
Iteration 22/1000 | Loss: 0.00003611
Iteration 23/1000 | Loss: 0.00003331
Iteration 24/1000 | Loss: 0.00003226
Iteration 25/1000 | Loss: 0.00003163
Iteration 26/1000 | Loss: 0.00024754
Iteration 27/1000 | Loss: 0.00017225
Iteration 28/1000 | Loss: 0.00016646
Iteration 29/1000 | Loss: 0.00027434
Iteration 30/1000 | Loss: 0.00013606
Iteration 31/1000 | Loss: 0.00016495
Iteration 32/1000 | Loss: 0.00010490
Iteration 33/1000 | Loss: 0.00015547
Iteration 34/1000 | Loss: 0.00029935
Iteration 35/1000 | Loss: 0.00008216
Iteration 36/1000 | Loss: 0.00014269
Iteration 37/1000 | Loss: 0.00014978
Iteration 38/1000 | Loss: 0.00027954
Iteration 39/1000 | Loss: 0.00033516
Iteration 40/1000 | Loss: 0.00017658
Iteration 41/1000 | Loss: 0.00027384
Iteration 42/1000 | Loss: 0.00013496
Iteration 43/1000 | Loss: 0.00030069
Iteration 44/1000 | Loss: 0.00016713
Iteration 45/1000 | Loss: 0.00013701
Iteration 46/1000 | Loss: 0.00017018
Iteration 47/1000 | Loss: 0.00009405
Iteration 48/1000 | Loss: 0.00010966
Iteration 49/1000 | Loss: 0.00017651
Iteration 50/1000 | Loss: 0.00017201
Iteration 51/1000 | Loss: 0.00012912
Iteration 52/1000 | Loss: 0.00032405
Iteration 53/1000 | Loss: 0.00022167
Iteration 54/1000 | Loss: 0.00016266
Iteration 55/1000 | Loss: 0.00004663
Iteration 56/1000 | Loss: 0.00011220
Iteration 57/1000 | Loss: 0.00010801
Iteration 58/1000 | Loss: 0.00007624
Iteration 59/1000 | Loss: 0.00003920
Iteration 60/1000 | Loss: 0.00013510
Iteration 61/1000 | Loss: 0.00010503
Iteration 62/1000 | Loss: 0.00010849
Iteration 63/1000 | Loss: 0.00018426
Iteration 64/1000 | Loss: 0.00006634
Iteration 65/1000 | Loss: 0.00013286
Iteration 66/1000 | Loss: 0.00007549
Iteration 67/1000 | Loss: 0.00011619
Iteration 68/1000 | Loss: 0.00015364
Iteration 69/1000 | Loss: 0.00024068
Iteration 70/1000 | Loss: 0.00011432
Iteration 71/1000 | Loss: 0.00015762
Iteration 72/1000 | Loss: 0.00017605
Iteration 73/1000 | Loss: 0.00019159
Iteration 74/1000 | Loss: 0.00013116
Iteration 75/1000 | Loss: 0.00017168
Iteration 76/1000 | Loss: 0.00012277
Iteration 77/1000 | Loss: 0.00022384
Iteration 78/1000 | Loss: 0.00021607
Iteration 79/1000 | Loss: 0.00010174
Iteration 80/1000 | Loss: 0.00006081
Iteration 81/1000 | Loss: 0.00014145
Iteration 82/1000 | Loss: 0.00017571
Iteration 83/1000 | Loss: 0.00033551
Iteration 84/1000 | Loss: 0.00019326
Iteration 85/1000 | Loss: 0.00004419
Iteration 86/1000 | Loss: 0.00021247
Iteration 87/1000 | Loss: 0.00008115
Iteration 88/1000 | Loss: 0.00014158
Iteration 89/1000 | Loss: 0.00003726
Iteration 90/1000 | Loss: 0.00010732
Iteration 91/1000 | Loss: 0.00003609
Iteration 92/1000 | Loss: 0.00024728
Iteration 93/1000 | Loss: 0.00004979
Iteration 94/1000 | Loss: 0.00003981
Iteration 95/1000 | Loss: 0.00009192
Iteration 96/1000 | Loss: 0.00010443
Iteration 97/1000 | Loss: 0.00008973
Iteration 98/1000 | Loss: 0.00009374
Iteration 99/1000 | Loss: 0.00009646
Iteration 100/1000 | Loss: 0.00008284
Iteration 101/1000 | Loss: 0.00007872
Iteration 102/1000 | Loss: 0.00007917
Iteration 103/1000 | Loss: 0.00007895
Iteration 104/1000 | Loss: 0.00004141
Iteration 105/1000 | Loss: 0.00010285
Iteration 106/1000 | Loss: 0.00008988
Iteration 107/1000 | Loss: 0.00013528
Iteration 108/1000 | Loss: 0.00010632
Iteration 109/1000 | Loss: 0.00008445
Iteration 110/1000 | Loss: 0.00011716
Iteration 111/1000 | Loss: 0.00016870
Iteration 112/1000 | Loss: 0.00019402
Iteration 113/1000 | Loss: 0.00021571
Iteration 114/1000 | Loss: 0.00018370
Iteration 115/1000 | Loss: 0.00012506
Iteration 116/1000 | Loss: 0.00019081
Iteration 117/1000 | Loss: 0.00003490
Iteration 118/1000 | Loss: 0.00022028
Iteration 119/1000 | Loss: 0.00032968
Iteration 120/1000 | Loss: 0.00016744
Iteration 121/1000 | Loss: 0.00006339
Iteration 122/1000 | Loss: 0.00020799
Iteration 123/1000 | Loss: 0.00023959
Iteration 124/1000 | Loss: 0.00028440
Iteration 125/1000 | Loss: 0.00004696
Iteration 126/1000 | Loss: 0.00003458
Iteration 127/1000 | Loss: 0.00003112
Iteration 128/1000 | Loss: 0.00003139
Iteration 129/1000 | Loss: 0.00003006
Iteration 130/1000 | Loss: 0.00002961
Iteration 131/1000 | Loss: 0.00003211
Iteration 132/1000 | Loss: 0.00002982
Iteration 133/1000 | Loss: 0.00002881
Iteration 134/1000 | Loss: 0.00002866
Iteration 135/1000 | Loss: 0.00002845
Iteration 136/1000 | Loss: 0.00002835
Iteration 137/1000 | Loss: 0.00002835
Iteration 138/1000 | Loss: 0.00002831
Iteration 139/1000 | Loss: 0.00002829
Iteration 140/1000 | Loss: 0.00002828
Iteration 141/1000 | Loss: 0.00002828
Iteration 142/1000 | Loss: 0.00002827
Iteration 143/1000 | Loss: 0.00002827
Iteration 144/1000 | Loss: 0.00002826
Iteration 145/1000 | Loss: 0.00002825
Iteration 146/1000 | Loss: 0.00002825
Iteration 147/1000 | Loss: 0.00002823
Iteration 148/1000 | Loss: 0.00002823
Iteration 149/1000 | Loss: 0.00002822
Iteration 150/1000 | Loss: 0.00002822
Iteration 151/1000 | Loss: 0.00002822
Iteration 152/1000 | Loss: 0.00002821
Iteration 153/1000 | Loss: 0.00002821
Iteration 154/1000 | Loss: 0.00002820
Iteration 155/1000 | Loss: 0.00002820
Iteration 156/1000 | Loss: 0.00002820
Iteration 157/1000 | Loss: 0.00002819
Iteration 158/1000 | Loss: 0.00002819
Iteration 159/1000 | Loss: 0.00002818
Iteration 160/1000 | Loss: 0.00002818
Iteration 161/1000 | Loss: 0.00002818
Iteration 162/1000 | Loss: 0.00002817
Iteration 163/1000 | Loss: 0.00002817
Iteration 164/1000 | Loss: 0.00002817
Iteration 165/1000 | Loss: 0.00002816
Iteration 166/1000 | Loss: 0.00002815
Iteration 167/1000 | Loss: 0.00002815
Iteration 168/1000 | Loss: 0.00002815
Iteration 169/1000 | Loss: 0.00002815
Iteration 170/1000 | Loss: 0.00002815
Iteration 171/1000 | Loss: 0.00002815
Iteration 172/1000 | Loss: 0.00002815
Iteration 173/1000 | Loss: 0.00002814
Iteration 174/1000 | Loss: 0.00002814
Iteration 175/1000 | Loss: 0.00002814
Iteration 176/1000 | Loss: 0.00002813
Iteration 177/1000 | Loss: 0.00002812
Iteration 178/1000 | Loss: 0.00002811
Iteration 179/1000 | Loss: 0.00002811
Iteration 180/1000 | Loss: 0.00002810
Iteration 181/1000 | Loss: 0.00002809
Iteration 182/1000 | Loss: 0.00002809
Iteration 183/1000 | Loss: 0.00002809
Iteration 184/1000 | Loss: 0.00002808
Iteration 185/1000 | Loss: 0.00002808
Iteration 186/1000 | Loss: 0.00002808
Iteration 187/1000 | Loss: 0.00002808
Iteration 188/1000 | Loss: 0.00002807
Iteration 189/1000 | Loss: 0.00002807
Iteration 190/1000 | Loss: 0.00002807
Iteration 191/1000 | Loss: 0.00002807
Iteration 192/1000 | Loss: 0.00002807
Iteration 193/1000 | Loss: 0.00002806
Iteration 194/1000 | Loss: 0.00002806
Iteration 195/1000 | Loss: 0.00002805
Iteration 196/1000 | Loss: 0.00002805
Iteration 197/1000 | Loss: 0.00002805
Iteration 198/1000 | Loss: 0.00002804
Iteration 199/1000 | Loss: 0.00004207
Iteration 200/1000 | Loss: 0.00002802
Iteration 201/1000 | Loss: 0.00002801
Iteration 202/1000 | Loss: 0.00002801
Iteration 203/1000 | Loss: 0.00002801
Iteration 204/1000 | Loss: 0.00002801
Iteration 205/1000 | Loss: 0.00002801
Iteration 206/1000 | Loss: 0.00002801
Iteration 207/1000 | Loss: 0.00002801
Iteration 208/1000 | Loss: 0.00002801
Iteration 209/1000 | Loss: 0.00002800
Iteration 210/1000 | Loss: 0.00002800
Iteration 211/1000 | Loss: 0.00002800
Iteration 212/1000 | Loss: 0.00002800
Iteration 213/1000 | Loss: 0.00002799
Iteration 214/1000 | Loss: 0.00002799
Iteration 215/1000 | Loss: 0.00002799
Iteration 216/1000 | Loss: 0.00002799
Iteration 217/1000 | Loss: 0.00002799
Iteration 218/1000 | Loss: 0.00002799
Iteration 219/1000 | Loss: 0.00002799
Iteration 220/1000 | Loss: 0.00002799
Iteration 221/1000 | Loss: 0.00002799
Iteration 222/1000 | Loss: 0.00002799
Iteration 223/1000 | Loss: 0.00002799
Iteration 224/1000 | Loss: 0.00003091
Iteration 225/1000 | Loss: 0.00003194
Iteration 226/1000 | Loss: 0.00003679
Iteration 227/1000 | Loss: 0.00002798
Iteration 228/1000 | Loss: 0.00002798
Iteration 229/1000 | Loss: 0.00002798
Iteration 230/1000 | Loss: 0.00002798
Iteration 231/1000 | Loss: 0.00002798
Iteration 232/1000 | Loss: 0.00002798
Iteration 233/1000 | Loss: 0.00002798
Iteration 234/1000 | Loss: 0.00002798
Iteration 235/1000 | Loss: 0.00002797
Iteration 236/1000 | Loss: 0.00002797
Iteration 237/1000 | Loss: 0.00002797
Iteration 238/1000 | Loss: 0.00002797
Iteration 239/1000 | Loss: 0.00002797
Iteration 240/1000 | Loss: 0.00002797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [2.797477827698458e-05, 2.797477827698458e-05, 2.797477827698458e-05, 2.797477827698458e-05, 2.797477827698458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.797477827698458e-05

Optimization complete. Final v2v error: 4.389308929443359 mm

Highest mean error: 5.862296104431152 mm for frame 35

Lowest mean error: 3.266329288482666 mm for frame 7

Saving results

Total time: 249.2459533214569
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00611249
Iteration 2/25 | Loss: 0.00133230
Iteration 3/25 | Loss: 0.00127113
Iteration 4/25 | Loss: 0.00126127
Iteration 5/25 | Loss: 0.00125811
Iteration 6/25 | Loss: 0.00125811
Iteration 7/25 | Loss: 0.00125811
Iteration 8/25 | Loss: 0.00125811
Iteration 9/25 | Loss: 0.00125811
Iteration 10/25 | Loss: 0.00125811
Iteration 11/25 | Loss: 0.00125811
Iteration 12/25 | Loss: 0.00125811
Iteration 13/25 | Loss: 0.00125811
Iteration 14/25 | Loss: 0.00125811
Iteration 15/25 | Loss: 0.00125811
Iteration 16/25 | Loss: 0.00125811
Iteration 17/25 | Loss: 0.00125811
Iteration 18/25 | Loss: 0.00125811
Iteration 19/25 | Loss: 0.00125811
Iteration 20/25 | Loss: 0.00125811
Iteration 21/25 | Loss: 0.00125811
Iteration 22/25 | Loss: 0.00125811
Iteration 23/25 | Loss: 0.00125811
Iteration 24/25 | Loss: 0.00125811
Iteration 25/25 | Loss: 0.00125811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.67875767
Iteration 2/25 | Loss: 0.00081206
Iteration 3/25 | Loss: 0.00081206
Iteration 4/25 | Loss: 0.00081206
Iteration 5/25 | Loss: 0.00081206
Iteration 6/25 | Loss: 0.00081206
Iteration 7/25 | Loss: 0.00081206
Iteration 8/25 | Loss: 0.00081206
Iteration 9/25 | Loss: 0.00081206
Iteration 10/25 | Loss: 0.00081206
Iteration 11/25 | Loss: 0.00081206
Iteration 12/25 | Loss: 0.00081206
Iteration 13/25 | Loss: 0.00081206
Iteration 14/25 | Loss: 0.00081206
Iteration 15/25 | Loss: 0.00081206
Iteration 16/25 | Loss: 0.00081206
Iteration 17/25 | Loss: 0.00081206
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008120570564642549, 0.0008120570564642549, 0.0008120570564642549, 0.0008120570564642549, 0.0008120570564642549]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008120570564642549

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081206
Iteration 2/1000 | Loss: 0.00002784
Iteration 3/1000 | Loss: 0.00001942
Iteration 4/1000 | Loss: 0.00001754
Iteration 5/1000 | Loss: 0.00001674
Iteration 6/1000 | Loss: 0.00001596
Iteration 7/1000 | Loss: 0.00001538
Iteration 8/1000 | Loss: 0.00001512
Iteration 9/1000 | Loss: 0.00001478
Iteration 10/1000 | Loss: 0.00001452
Iteration 11/1000 | Loss: 0.00001441
Iteration 12/1000 | Loss: 0.00001431
Iteration 13/1000 | Loss: 0.00001429
Iteration 14/1000 | Loss: 0.00001412
Iteration 15/1000 | Loss: 0.00001408
Iteration 16/1000 | Loss: 0.00001405
Iteration 17/1000 | Loss: 0.00001403
Iteration 18/1000 | Loss: 0.00001397
Iteration 19/1000 | Loss: 0.00001397
Iteration 20/1000 | Loss: 0.00001397
Iteration 21/1000 | Loss: 0.00001396
Iteration 22/1000 | Loss: 0.00001394
Iteration 23/1000 | Loss: 0.00001391
Iteration 24/1000 | Loss: 0.00001390
Iteration 25/1000 | Loss: 0.00001390
Iteration 26/1000 | Loss: 0.00001390
Iteration 27/1000 | Loss: 0.00001382
Iteration 28/1000 | Loss: 0.00001379
Iteration 29/1000 | Loss: 0.00001372
Iteration 30/1000 | Loss: 0.00001368
Iteration 31/1000 | Loss: 0.00001365
Iteration 32/1000 | Loss: 0.00001361
Iteration 33/1000 | Loss: 0.00001361
Iteration 34/1000 | Loss: 0.00001361
Iteration 35/1000 | Loss: 0.00001361
Iteration 36/1000 | Loss: 0.00001361
Iteration 37/1000 | Loss: 0.00001361
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001361
Iteration 40/1000 | Loss: 0.00001361
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001359
Iteration 44/1000 | Loss: 0.00001359
Iteration 45/1000 | Loss: 0.00001359
Iteration 46/1000 | Loss: 0.00001358
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001358
Iteration 49/1000 | Loss: 0.00001358
Iteration 50/1000 | Loss: 0.00001358
Iteration 51/1000 | Loss: 0.00001357
Iteration 52/1000 | Loss: 0.00001355
Iteration 53/1000 | Loss: 0.00001355
Iteration 54/1000 | Loss: 0.00001355
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001354
Iteration 57/1000 | Loss: 0.00001354
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001353
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001351
Iteration 63/1000 | Loss: 0.00001346
Iteration 64/1000 | Loss: 0.00001346
Iteration 65/1000 | Loss: 0.00001343
Iteration 66/1000 | Loss: 0.00001342
Iteration 67/1000 | Loss: 0.00001341
Iteration 68/1000 | Loss: 0.00001341
Iteration 69/1000 | Loss: 0.00001341
Iteration 70/1000 | Loss: 0.00001339
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001339
Iteration 74/1000 | Loss: 0.00001339
Iteration 75/1000 | Loss: 0.00001338
Iteration 76/1000 | Loss: 0.00001338
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001338
Iteration 80/1000 | Loss: 0.00001337
Iteration 81/1000 | Loss: 0.00001337
Iteration 82/1000 | Loss: 0.00001337
Iteration 83/1000 | Loss: 0.00001337
Iteration 84/1000 | Loss: 0.00001337
Iteration 85/1000 | Loss: 0.00001337
Iteration 86/1000 | Loss: 0.00001337
Iteration 87/1000 | Loss: 0.00001337
Iteration 88/1000 | Loss: 0.00001337
Iteration 89/1000 | Loss: 0.00001337
Iteration 90/1000 | Loss: 0.00001337
Iteration 91/1000 | Loss: 0.00001337
Iteration 92/1000 | Loss: 0.00001337
Iteration 93/1000 | Loss: 0.00001337
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001337
Iteration 96/1000 | Loss: 0.00001337
Iteration 97/1000 | Loss: 0.00001337
Iteration 98/1000 | Loss: 0.00001337
Iteration 99/1000 | Loss: 0.00001337
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.3366700841288548e-05, 1.3366700841288548e-05, 1.3366700841288548e-05, 1.3366700841288548e-05, 1.3366700841288548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3366700841288548e-05

Optimization complete. Final v2v error: 3.1464743614196777 mm

Highest mean error: 3.4274039268493652 mm for frame 53

Lowest mean error: 2.96297287940979 mm for frame 126

Saving results

Total time: 37.43855261802673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894985
Iteration 2/25 | Loss: 0.00257958
Iteration 3/25 | Loss: 0.00196873
Iteration 4/25 | Loss: 0.00196896
Iteration 5/25 | Loss: 0.00174499
Iteration 6/25 | Loss: 0.00165280
Iteration 7/25 | Loss: 0.00157821
Iteration 8/25 | Loss: 0.00156102
Iteration 9/25 | Loss: 0.00153596
Iteration 10/25 | Loss: 0.00153249
Iteration 11/25 | Loss: 0.00152216
Iteration 12/25 | Loss: 0.00151227
Iteration 13/25 | Loss: 0.00153315
Iteration 14/25 | Loss: 0.00147619
Iteration 15/25 | Loss: 0.00145049
Iteration 16/25 | Loss: 0.00143577
Iteration 17/25 | Loss: 0.00142926
Iteration 18/25 | Loss: 0.00141300
Iteration 19/25 | Loss: 0.00141195
Iteration 20/25 | Loss: 0.00141162
Iteration 21/25 | Loss: 0.00141126
Iteration 22/25 | Loss: 0.00141095
Iteration 23/25 | Loss: 0.00141065
Iteration 24/25 | Loss: 0.00141332
Iteration 25/25 | Loss: 0.00140907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36056387
Iteration 2/25 | Loss: 0.00095678
Iteration 3/25 | Loss: 0.00086572
Iteration 4/25 | Loss: 0.00086572
Iteration 5/25 | Loss: 0.00086572
Iteration 6/25 | Loss: 0.00086572
Iteration 7/25 | Loss: 0.00086572
Iteration 8/25 | Loss: 0.00086572
Iteration 9/25 | Loss: 0.00086572
Iteration 10/25 | Loss: 0.00086571
Iteration 11/25 | Loss: 0.00086571
Iteration 12/25 | Loss: 0.00086571
Iteration 13/25 | Loss: 0.00086571
Iteration 14/25 | Loss: 0.00086571
Iteration 15/25 | Loss: 0.00086571
Iteration 16/25 | Loss: 0.00086571
Iteration 17/25 | Loss: 0.00086571
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008657145663164556, 0.0008657145663164556, 0.0008657145663164556, 0.0008657145663164556, 0.0008657145663164556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008657145663164556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086571
Iteration 2/1000 | Loss: 0.00010843
Iteration 3/1000 | Loss: 0.00003965
Iteration 4/1000 | Loss: 0.00003485
Iteration 5/1000 | Loss: 0.00003313
Iteration 6/1000 | Loss: 0.00003185
Iteration 7/1000 | Loss: 0.00003097
Iteration 8/1000 | Loss: 0.00030386
Iteration 9/1000 | Loss: 0.00004165
Iteration 10/1000 | Loss: 0.00003314
Iteration 11/1000 | Loss: 0.00002937
Iteration 12/1000 | Loss: 0.00012486
Iteration 13/1000 | Loss: 0.00010050
Iteration 14/1000 | Loss: 0.00002692
Iteration 15/1000 | Loss: 0.00002612
Iteration 16/1000 | Loss: 0.00002550
Iteration 17/1000 | Loss: 0.00002510
Iteration 18/1000 | Loss: 0.00009390
Iteration 19/1000 | Loss: 0.00002610
Iteration 20/1000 | Loss: 0.00002461
Iteration 21/1000 | Loss: 0.00002421
Iteration 22/1000 | Loss: 0.00011219
Iteration 23/1000 | Loss: 0.00013115
Iteration 24/1000 | Loss: 0.00009211
Iteration 25/1000 | Loss: 0.00010812
Iteration 26/1000 | Loss: 0.00003329
Iteration 27/1000 | Loss: 0.00017776
Iteration 28/1000 | Loss: 0.00005802
Iteration 29/1000 | Loss: 0.00002451
Iteration 30/1000 | Loss: 0.00002378
Iteration 31/1000 | Loss: 0.00002355
Iteration 32/1000 | Loss: 0.00002353
Iteration 33/1000 | Loss: 0.00002347
Iteration 34/1000 | Loss: 0.00002347
Iteration 35/1000 | Loss: 0.00002341
Iteration 36/1000 | Loss: 0.00002338
Iteration 37/1000 | Loss: 0.00002337
Iteration 38/1000 | Loss: 0.00002337
Iteration 39/1000 | Loss: 0.00002336
Iteration 40/1000 | Loss: 0.00002334
Iteration 41/1000 | Loss: 0.00002333
Iteration 42/1000 | Loss: 0.00002333
Iteration 43/1000 | Loss: 0.00002332
Iteration 44/1000 | Loss: 0.00002332
Iteration 45/1000 | Loss: 0.00002331
Iteration 46/1000 | Loss: 0.00002330
Iteration 47/1000 | Loss: 0.00002328
Iteration 48/1000 | Loss: 0.00002327
Iteration 49/1000 | Loss: 0.00002327
Iteration 50/1000 | Loss: 0.00002326
Iteration 51/1000 | Loss: 0.00002326
Iteration 52/1000 | Loss: 0.00002325
Iteration 53/1000 | Loss: 0.00002325
Iteration 54/1000 | Loss: 0.00002324
Iteration 55/1000 | Loss: 0.00002323
Iteration 56/1000 | Loss: 0.00002322
Iteration 57/1000 | Loss: 0.00002321
Iteration 58/1000 | Loss: 0.00002320
Iteration 59/1000 | Loss: 0.00002319
Iteration 60/1000 | Loss: 0.00002319
Iteration 61/1000 | Loss: 0.00002318
Iteration 62/1000 | Loss: 0.00002318
Iteration 63/1000 | Loss: 0.00002318
Iteration 64/1000 | Loss: 0.00002317
Iteration 65/1000 | Loss: 0.00002317
Iteration 66/1000 | Loss: 0.00002317
Iteration 67/1000 | Loss: 0.00002317
Iteration 68/1000 | Loss: 0.00002316
Iteration 69/1000 | Loss: 0.00002316
Iteration 70/1000 | Loss: 0.00002316
Iteration 71/1000 | Loss: 0.00002315
Iteration 72/1000 | Loss: 0.00002315
Iteration 73/1000 | Loss: 0.00002315
Iteration 74/1000 | Loss: 0.00002315
Iteration 75/1000 | Loss: 0.00002315
Iteration 76/1000 | Loss: 0.00002315
Iteration 77/1000 | Loss: 0.00002315
Iteration 78/1000 | Loss: 0.00002314
Iteration 79/1000 | Loss: 0.00002314
Iteration 80/1000 | Loss: 0.00002313
Iteration 81/1000 | Loss: 0.00002313
Iteration 82/1000 | Loss: 0.00002313
Iteration 83/1000 | Loss: 0.00002313
Iteration 84/1000 | Loss: 0.00002313
Iteration 85/1000 | Loss: 0.00002313
Iteration 86/1000 | Loss: 0.00002313
Iteration 87/1000 | Loss: 0.00002313
Iteration 88/1000 | Loss: 0.00002313
Iteration 89/1000 | Loss: 0.00002313
Iteration 90/1000 | Loss: 0.00002312
Iteration 91/1000 | Loss: 0.00002312
Iteration 92/1000 | Loss: 0.00002312
Iteration 93/1000 | Loss: 0.00002312
Iteration 94/1000 | Loss: 0.00002311
Iteration 95/1000 | Loss: 0.00002310
Iteration 96/1000 | Loss: 0.00002310
Iteration 97/1000 | Loss: 0.00002310
Iteration 98/1000 | Loss: 0.00002310
Iteration 99/1000 | Loss: 0.00002309
Iteration 100/1000 | Loss: 0.00002309
Iteration 101/1000 | Loss: 0.00002309
Iteration 102/1000 | Loss: 0.00002309
Iteration 103/1000 | Loss: 0.00002309
Iteration 104/1000 | Loss: 0.00002309
Iteration 105/1000 | Loss: 0.00002309
Iteration 106/1000 | Loss: 0.00002309
Iteration 107/1000 | Loss: 0.00002309
Iteration 108/1000 | Loss: 0.00002309
Iteration 109/1000 | Loss: 0.00002308
Iteration 110/1000 | Loss: 0.00002308
Iteration 111/1000 | Loss: 0.00002308
Iteration 112/1000 | Loss: 0.00002308
Iteration 113/1000 | Loss: 0.00002308
Iteration 114/1000 | Loss: 0.00002308
Iteration 115/1000 | Loss: 0.00002308
Iteration 116/1000 | Loss: 0.00002307
Iteration 117/1000 | Loss: 0.00002307
Iteration 118/1000 | Loss: 0.00002307
Iteration 119/1000 | Loss: 0.00002307
Iteration 120/1000 | Loss: 0.00002307
Iteration 121/1000 | Loss: 0.00002306
Iteration 122/1000 | Loss: 0.00002306
Iteration 123/1000 | Loss: 0.00002305
Iteration 124/1000 | Loss: 0.00002305
Iteration 125/1000 | Loss: 0.00002304
Iteration 126/1000 | Loss: 0.00002304
Iteration 127/1000 | Loss: 0.00002303
Iteration 128/1000 | Loss: 0.00002303
Iteration 129/1000 | Loss: 0.00002303
Iteration 130/1000 | Loss: 0.00002302
Iteration 131/1000 | Loss: 0.00002302
Iteration 132/1000 | Loss: 0.00002302
Iteration 133/1000 | Loss: 0.00002302
Iteration 134/1000 | Loss: 0.00002301
Iteration 135/1000 | Loss: 0.00002301
Iteration 136/1000 | Loss: 0.00002301
Iteration 137/1000 | Loss: 0.00002301
Iteration 138/1000 | Loss: 0.00002301
Iteration 139/1000 | Loss: 0.00002301
Iteration 140/1000 | Loss: 0.00002301
Iteration 141/1000 | Loss: 0.00002300
Iteration 142/1000 | Loss: 0.00002300
Iteration 143/1000 | Loss: 0.00002300
Iteration 144/1000 | Loss: 0.00002300
Iteration 145/1000 | Loss: 0.00002300
Iteration 146/1000 | Loss: 0.00002299
Iteration 147/1000 | Loss: 0.00002299
Iteration 148/1000 | Loss: 0.00002299
Iteration 149/1000 | Loss: 0.00002299
Iteration 150/1000 | Loss: 0.00002299
Iteration 151/1000 | Loss: 0.00002299
Iteration 152/1000 | Loss: 0.00002298
Iteration 153/1000 | Loss: 0.00002298
Iteration 154/1000 | Loss: 0.00002298
Iteration 155/1000 | Loss: 0.00002297
Iteration 156/1000 | Loss: 0.00002297
Iteration 157/1000 | Loss: 0.00002296
Iteration 158/1000 | Loss: 0.00002296
Iteration 159/1000 | Loss: 0.00002296
Iteration 160/1000 | Loss: 0.00002296
Iteration 161/1000 | Loss: 0.00002296
Iteration 162/1000 | Loss: 0.00002295
Iteration 163/1000 | Loss: 0.00002295
Iteration 164/1000 | Loss: 0.00002295
Iteration 165/1000 | Loss: 0.00002295
Iteration 166/1000 | Loss: 0.00002295
Iteration 167/1000 | Loss: 0.00002295
Iteration 168/1000 | Loss: 0.00002295
Iteration 169/1000 | Loss: 0.00002295
Iteration 170/1000 | Loss: 0.00002295
Iteration 171/1000 | Loss: 0.00002295
Iteration 172/1000 | Loss: 0.00002295
Iteration 173/1000 | Loss: 0.00002295
Iteration 174/1000 | Loss: 0.00002295
Iteration 175/1000 | Loss: 0.00002295
Iteration 176/1000 | Loss: 0.00002295
Iteration 177/1000 | Loss: 0.00002295
Iteration 178/1000 | Loss: 0.00002295
Iteration 179/1000 | Loss: 0.00002295
Iteration 180/1000 | Loss: 0.00002295
Iteration 181/1000 | Loss: 0.00002295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.2947036995901726e-05, 2.2947036995901726e-05, 2.2947036995901726e-05, 2.2947036995901726e-05, 2.2947036995901726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2947036995901726e-05

Optimization complete. Final v2v error: 3.9721872806549072 mm

Highest mean error: 4.815212249755859 mm for frame 146

Lowest mean error: 3.770277500152588 mm for frame 29

Saving results

Total time: 117.72033166885376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819707
Iteration 2/25 | Loss: 0.00135053
Iteration 3/25 | Loss: 0.00128244
Iteration 4/25 | Loss: 0.00127467
Iteration 5/25 | Loss: 0.00127317
Iteration 6/25 | Loss: 0.00127317
Iteration 7/25 | Loss: 0.00127317
Iteration 8/25 | Loss: 0.00127317
Iteration 9/25 | Loss: 0.00127317
Iteration 10/25 | Loss: 0.00127317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012731662718579173, 0.0012731662718579173, 0.0012731662718579173, 0.0012731662718579173, 0.0012731662718579173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012731662718579173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.54659271
Iteration 2/25 | Loss: 0.00082273
Iteration 3/25 | Loss: 0.00082273
Iteration 4/25 | Loss: 0.00082273
Iteration 5/25 | Loss: 0.00082273
Iteration 6/25 | Loss: 0.00082273
Iteration 7/25 | Loss: 0.00082273
Iteration 8/25 | Loss: 0.00082273
Iteration 9/25 | Loss: 0.00082273
Iteration 10/25 | Loss: 0.00082273
Iteration 11/25 | Loss: 0.00082273
Iteration 12/25 | Loss: 0.00082273
Iteration 13/25 | Loss: 0.00082273
Iteration 14/25 | Loss: 0.00082273
Iteration 15/25 | Loss: 0.00082273
Iteration 16/25 | Loss: 0.00082273
Iteration 17/25 | Loss: 0.00082273
Iteration 18/25 | Loss: 0.00082273
Iteration 19/25 | Loss: 0.00082273
Iteration 20/25 | Loss: 0.00082273
Iteration 21/25 | Loss: 0.00082273
Iteration 22/25 | Loss: 0.00082273
Iteration 23/25 | Loss: 0.00082273
Iteration 24/25 | Loss: 0.00082273
Iteration 25/25 | Loss: 0.00082273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082273
Iteration 2/1000 | Loss: 0.00002240
Iteration 3/1000 | Loss: 0.00001677
Iteration 4/1000 | Loss: 0.00001542
Iteration 5/1000 | Loss: 0.00001450
Iteration 6/1000 | Loss: 0.00001394
Iteration 7/1000 | Loss: 0.00001363
Iteration 8/1000 | Loss: 0.00001325
Iteration 9/1000 | Loss: 0.00001298
Iteration 10/1000 | Loss: 0.00001281
Iteration 11/1000 | Loss: 0.00001281
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001271
Iteration 14/1000 | Loss: 0.00001270
Iteration 15/1000 | Loss: 0.00001269
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001253
Iteration 18/1000 | Loss: 0.00001252
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001246
Iteration 21/1000 | Loss: 0.00001246
Iteration 22/1000 | Loss: 0.00001245
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001230
Iteration 25/1000 | Loss: 0.00001230
Iteration 26/1000 | Loss: 0.00001224
Iteration 27/1000 | Loss: 0.00001223
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001221
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001221
Iteration 32/1000 | Loss: 0.00001220
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001217
Iteration 36/1000 | Loss: 0.00001216
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001215
Iteration 40/1000 | Loss: 0.00001215
Iteration 41/1000 | Loss: 0.00001215
Iteration 42/1000 | Loss: 0.00001215
Iteration 43/1000 | Loss: 0.00001215
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001215
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001212
Iteration 50/1000 | Loss: 0.00001212
Iteration 51/1000 | Loss: 0.00001211
Iteration 52/1000 | Loss: 0.00001211
Iteration 53/1000 | Loss: 0.00001211
Iteration 54/1000 | Loss: 0.00001210
Iteration 55/1000 | Loss: 0.00001210
Iteration 56/1000 | Loss: 0.00001210
Iteration 57/1000 | Loss: 0.00001209
Iteration 58/1000 | Loss: 0.00001209
Iteration 59/1000 | Loss: 0.00001209
Iteration 60/1000 | Loss: 0.00001209
Iteration 61/1000 | Loss: 0.00001209
Iteration 62/1000 | Loss: 0.00001209
Iteration 63/1000 | Loss: 0.00001209
Iteration 64/1000 | Loss: 0.00001209
Iteration 65/1000 | Loss: 0.00001208
Iteration 66/1000 | Loss: 0.00001208
Iteration 67/1000 | Loss: 0.00001208
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001207
Iteration 71/1000 | Loss: 0.00001207
Iteration 72/1000 | Loss: 0.00001207
Iteration 73/1000 | Loss: 0.00001207
Iteration 74/1000 | Loss: 0.00001206
Iteration 75/1000 | Loss: 0.00001206
Iteration 76/1000 | Loss: 0.00001206
Iteration 77/1000 | Loss: 0.00001206
Iteration 78/1000 | Loss: 0.00001206
Iteration 79/1000 | Loss: 0.00001206
Iteration 80/1000 | Loss: 0.00001205
Iteration 81/1000 | Loss: 0.00001205
Iteration 82/1000 | Loss: 0.00001204
Iteration 83/1000 | Loss: 0.00001204
Iteration 84/1000 | Loss: 0.00001204
Iteration 85/1000 | Loss: 0.00001203
Iteration 86/1000 | Loss: 0.00001203
Iteration 87/1000 | Loss: 0.00001202
Iteration 88/1000 | Loss: 0.00001202
Iteration 89/1000 | Loss: 0.00001202
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001201
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001199
Iteration 95/1000 | Loss: 0.00001199
Iteration 96/1000 | Loss: 0.00001199
Iteration 97/1000 | Loss: 0.00001199
Iteration 98/1000 | Loss: 0.00001199
Iteration 99/1000 | Loss: 0.00001199
Iteration 100/1000 | Loss: 0.00001199
Iteration 101/1000 | Loss: 0.00001199
Iteration 102/1000 | Loss: 0.00001199
Iteration 103/1000 | Loss: 0.00001199
Iteration 104/1000 | Loss: 0.00001198
Iteration 105/1000 | Loss: 0.00001198
Iteration 106/1000 | Loss: 0.00001198
Iteration 107/1000 | Loss: 0.00001198
Iteration 108/1000 | Loss: 0.00001198
Iteration 109/1000 | Loss: 0.00001198
Iteration 110/1000 | Loss: 0.00001198
Iteration 111/1000 | Loss: 0.00001198
Iteration 112/1000 | Loss: 0.00001198
Iteration 113/1000 | Loss: 0.00001198
Iteration 114/1000 | Loss: 0.00001198
Iteration 115/1000 | Loss: 0.00001198
Iteration 116/1000 | Loss: 0.00001198
Iteration 117/1000 | Loss: 0.00001198
Iteration 118/1000 | Loss: 0.00001198
Iteration 119/1000 | Loss: 0.00001198
Iteration 120/1000 | Loss: 0.00001198
Iteration 121/1000 | Loss: 0.00001198
Iteration 122/1000 | Loss: 0.00001198
Iteration 123/1000 | Loss: 0.00001198
Iteration 124/1000 | Loss: 0.00001198
Iteration 125/1000 | Loss: 0.00001198
Iteration 126/1000 | Loss: 0.00001198
Iteration 127/1000 | Loss: 0.00001198
Iteration 128/1000 | Loss: 0.00001198
Iteration 129/1000 | Loss: 0.00001198
Iteration 130/1000 | Loss: 0.00001198
Iteration 131/1000 | Loss: 0.00001198
Iteration 132/1000 | Loss: 0.00001198
Iteration 133/1000 | Loss: 0.00001198
Iteration 134/1000 | Loss: 0.00001198
Iteration 135/1000 | Loss: 0.00001198
Iteration 136/1000 | Loss: 0.00001198
Iteration 137/1000 | Loss: 0.00001198
Iteration 138/1000 | Loss: 0.00001198
Iteration 139/1000 | Loss: 0.00001198
Iteration 140/1000 | Loss: 0.00001198
Iteration 141/1000 | Loss: 0.00001198
Iteration 142/1000 | Loss: 0.00001198
Iteration 143/1000 | Loss: 0.00001198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.198157224280294e-05, 1.198157224280294e-05, 1.198157224280294e-05, 1.198157224280294e-05, 1.198157224280294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.198157224280294e-05

Optimization complete. Final v2v error: 2.959665060043335 mm

Highest mean error: 3.30474591255188 mm for frame 195

Lowest mean error: 2.7921535968780518 mm for frame 41

Saving results

Total time: 39.7349169254303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822047
Iteration 2/25 | Loss: 0.00154358
Iteration 3/25 | Loss: 0.00134071
Iteration 4/25 | Loss: 0.00131124
Iteration 5/25 | Loss: 0.00130507
Iteration 6/25 | Loss: 0.00130398
Iteration 7/25 | Loss: 0.00130398
Iteration 8/25 | Loss: 0.00130398
Iteration 9/25 | Loss: 0.00130398
Iteration 10/25 | Loss: 0.00130398
Iteration 11/25 | Loss: 0.00130398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013039843179285526, 0.0013039843179285526, 0.0013039843179285526, 0.0013039843179285526, 0.0013039843179285526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013039843179285526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99798858
Iteration 2/25 | Loss: 0.00062604
Iteration 3/25 | Loss: 0.00062603
Iteration 4/25 | Loss: 0.00062603
Iteration 5/25 | Loss: 0.00062603
Iteration 6/25 | Loss: 0.00062603
Iteration 7/25 | Loss: 0.00062603
Iteration 8/25 | Loss: 0.00062603
Iteration 9/25 | Loss: 0.00062603
Iteration 10/25 | Loss: 0.00062603
Iteration 11/25 | Loss: 0.00062603
Iteration 12/25 | Loss: 0.00062603
Iteration 13/25 | Loss: 0.00062603
Iteration 14/25 | Loss: 0.00062603
Iteration 15/25 | Loss: 0.00062603
Iteration 16/25 | Loss: 0.00062603
Iteration 17/25 | Loss: 0.00062603
Iteration 18/25 | Loss: 0.00062603
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006260261870920658, 0.0006260261870920658, 0.0006260261870920658, 0.0006260261870920658, 0.0006260261870920658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006260261870920658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062603
Iteration 2/1000 | Loss: 0.00004567
Iteration 3/1000 | Loss: 0.00003292
Iteration 4/1000 | Loss: 0.00003002
Iteration 5/1000 | Loss: 0.00002866
Iteration 6/1000 | Loss: 0.00002722
Iteration 7/1000 | Loss: 0.00002633
Iteration 8/1000 | Loss: 0.00002560
Iteration 9/1000 | Loss: 0.00002523
Iteration 10/1000 | Loss: 0.00002489
Iteration 11/1000 | Loss: 0.00002464
Iteration 12/1000 | Loss: 0.00002439
Iteration 13/1000 | Loss: 0.00002417
Iteration 14/1000 | Loss: 0.00002408
Iteration 15/1000 | Loss: 0.00002386
Iteration 16/1000 | Loss: 0.00002381
Iteration 17/1000 | Loss: 0.00002371
Iteration 18/1000 | Loss: 0.00002368
Iteration 19/1000 | Loss: 0.00002364
Iteration 20/1000 | Loss: 0.00002364
Iteration 21/1000 | Loss: 0.00002363
Iteration 22/1000 | Loss: 0.00002363
Iteration 23/1000 | Loss: 0.00002362
Iteration 24/1000 | Loss: 0.00002361
Iteration 25/1000 | Loss: 0.00002361
Iteration 26/1000 | Loss: 0.00002361
Iteration 27/1000 | Loss: 0.00002356
Iteration 28/1000 | Loss: 0.00002356
Iteration 29/1000 | Loss: 0.00002355
Iteration 30/1000 | Loss: 0.00002355
Iteration 31/1000 | Loss: 0.00002354
Iteration 32/1000 | Loss: 0.00002354
Iteration 33/1000 | Loss: 0.00002353
Iteration 34/1000 | Loss: 0.00002353
Iteration 35/1000 | Loss: 0.00002353
Iteration 36/1000 | Loss: 0.00002353
Iteration 37/1000 | Loss: 0.00002353
Iteration 38/1000 | Loss: 0.00002352
Iteration 39/1000 | Loss: 0.00002352
Iteration 40/1000 | Loss: 0.00002351
Iteration 41/1000 | Loss: 0.00002351
Iteration 42/1000 | Loss: 0.00002351
Iteration 43/1000 | Loss: 0.00002350
Iteration 44/1000 | Loss: 0.00002349
Iteration 45/1000 | Loss: 0.00002349
Iteration 46/1000 | Loss: 0.00002349
Iteration 47/1000 | Loss: 0.00002349
Iteration 48/1000 | Loss: 0.00002348
Iteration 49/1000 | Loss: 0.00002348
Iteration 50/1000 | Loss: 0.00002348
Iteration 51/1000 | Loss: 0.00002348
Iteration 52/1000 | Loss: 0.00002348
Iteration 53/1000 | Loss: 0.00002348
Iteration 54/1000 | Loss: 0.00002348
Iteration 55/1000 | Loss: 0.00002348
Iteration 56/1000 | Loss: 0.00002348
Iteration 57/1000 | Loss: 0.00002348
Iteration 58/1000 | Loss: 0.00002347
Iteration 59/1000 | Loss: 0.00002347
Iteration 60/1000 | Loss: 0.00002346
Iteration 61/1000 | Loss: 0.00002346
Iteration 62/1000 | Loss: 0.00002346
Iteration 63/1000 | Loss: 0.00002346
Iteration 64/1000 | Loss: 0.00002346
Iteration 65/1000 | Loss: 0.00002346
Iteration 66/1000 | Loss: 0.00002346
Iteration 67/1000 | Loss: 0.00002346
Iteration 68/1000 | Loss: 0.00002346
Iteration 69/1000 | Loss: 0.00002346
Iteration 70/1000 | Loss: 0.00002345
Iteration 71/1000 | Loss: 0.00002345
Iteration 72/1000 | Loss: 0.00002343
Iteration 73/1000 | Loss: 0.00002343
Iteration 74/1000 | Loss: 0.00002341
Iteration 75/1000 | Loss: 0.00002341
Iteration 76/1000 | Loss: 0.00002340
Iteration 77/1000 | Loss: 0.00002340
Iteration 78/1000 | Loss: 0.00002340
Iteration 79/1000 | Loss: 0.00002340
Iteration 80/1000 | Loss: 0.00002340
Iteration 81/1000 | Loss: 0.00002339
Iteration 82/1000 | Loss: 0.00002338
Iteration 83/1000 | Loss: 0.00002338
Iteration 84/1000 | Loss: 0.00002338
Iteration 85/1000 | Loss: 0.00002336
Iteration 86/1000 | Loss: 0.00002336
Iteration 87/1000 | Loss: 0.00002336
Iteration 88/1000 | Loss: 0.00002336
Iteration 89/1000 | Loss: 0.00002336
Iteration 90/1000 | Loss: 0.00002336
Iteration 91/1000 | Loss: 0.00002336
Iteration 92/1000 | Loss: 0.00002336
Iteration 93/1000 | Loss: 0.00002336
Iteration 94/1000 | Loss: 0.00002335
Iteration 95/1000 | Loss: 0.00002335
Iteration 96/1000 | Loss: 0.00002335
Iteration 97/1000 | Loss: 0.00002335
Iteration 98/1000 | Loss: 0.00002335
Iteration 99/1000 | Loss: 0.00002335
Iteration 100/1000 | Loss: 0.00002335
Iteration 101/1000 | Loss: 0.00002334
Iteration 102/1000 | Loss: 0.00002334
Iteration 103/1000 | Loss: 0.00002334
Iteration 104/1000 | Loss: 0.00002334
Iteration 105/1000 | Loss: 0.00002334
Iteration 106/1000 | Loss: 0.00002334
Iteration 107/1000 | Loss: 0.00002334
Iteration 108/1000 | Loss: 0.00002334
Iteration 109/1000 | Loss: 0.00002334
Iteration 110/1000 | Loss: 0.00002334
Iteration 111/1000 | Loss: 0.00002334
Iteration 112/1000 | Loss: 0.00002334
Iteration 113/1000 | Loss: 0.00002334
Iteration 114/1000 | Loss: 0.00002334
Iteration 115/1000 | Loss: 0.00002334
Iteration 116/1000 | Loss: 0.00002334
Iteration 117/1000 | Loss: 0.00002334
Iteration 118/1000 | Loss: 0.00002334
Iteration 119/1000 | Loss: 0.00002333
Iteration 120/1000 | Loss: 0.00002333
Iteration 121/1000 | Loss: 0.00002333
Iteration 122/1000 | Loss: 0.00002333
Iteration 123/1000 | Loss: 0.00002333
Iteration 124/1000 | Loss: 0.00002333
Iteration 125/1000 | Loss: 0.00002333
Iteration 126/1000 | Loss: 0.00002333
Iteration 127/1000 | Loss: 0.00002332
Iteration 128/1000 | Loss: 0.00002332
Iteration 129/1000 | Loss: 0.00002332
Iteration 130/1000 | Loss: 0.00002332
Iteration 131/1000 | Loss: 0.00002332
Iteration 132/1000 | Loss: 0.00002332
Iteration 133/1000 | Loss: 0.00002332
Iteration 134/1000 | Loss: 0.00002332
Iteration 135/1000 | Loss: 0.00002331
Iteration 136/1000 | Loss: 0.00002331
Iteration 137/1000 | Loss: 0.00002331
Iteration 138/1000 | Loss: 0.00002331
Iteration 139/1000 | Loss: 0.00002331
Iteration 140/1000 | Loss: 0.00002330
Iteration 141/1000 | Loss: 0.00002330
Iteration 142/1000 | Loss: 0.00002330
Iteration 143/1000 | Loss: 0.00002329
Iteration 144/1000 | Loss: 0.00002329
Iteration 145/1000 | Loss: 0.00002329
Iteration 146/1000 | Loss: 0.00002329
Iteration 147/1000 | Loss: 0.00002329
Iteration 148/1000 | Loss: 0.00002329
Iteration 149/1000 | Loss: 0.00002329
Iteration 150/1000 | Loss: 0.00002329
Iteration 151/1000 | Loss: 0.00002329
Iteration 152/1000 | Loss: 0.00002329
Iteration 153/1000 | Loss: 0.00002329
Iteration 154/1000 | Loss: 0.00002329
Iteration 155/1000 | Loss: 0.00002328
Iteration 156/1000 | Loss: 0.00002328
Iteration 157/1000 | Loss: 0.00002328
Iteration 158/1000 | Loss: 0.00002328
Iteration 159/1000 | Loss: 0.00002328
Iteration 160/1000 | Loss: 0.00002328
Iteration 161/1000 | Loss: 0.00002328
Iteration 162/1000 | Loss: 0.00002328
Iteration 163/1000 | Loss: 0.00002328
Iteration 164/1000 | Loss: 0.00002328
Iteration 165/1000 | Loss: 0.00002328
Iteration 166/1000 | Loss: 0.00002328
Iteration 167/1000 | Loss: 0.00002328
Iteration 168/1000 | Loss: 0.00002328
Iteration 169/1000 | Loss: 0.00002328
Iteration 170/1000 | Loss: 0.00002328
Iteration 171/1000 | Loss: 0.00002328
Iteration 172/1000 | Loss: 0.00002328
Iteration 173/1000 | Loss: 0.00002328
Iteration 174/1000 | Loss: 0.00002328
Iteration 175/1000 | Loss: 0.00002327
Iteration 176/1000 | Loss: 0.00002327
Iteration 177/1000 | Loss: 0.00002327
Iteration 178/1000 | Loss: 0.00002327
Iteration 179/1000 | Loss: 0.00002327
Iteration 180/1000 | Loss: 0.00002327
Iteration 181/1000 | Loss: 0.00002327
Iteration 182/1000 | Loss: 0.00002327
Iteration 183/1000 | Loss: 0.00002327
Iteration 184/1000 | Loss: 0.00002327
Iteration 185/1000 | Loss: 0.00002327
Iteration 186/1000 | Loss: 0.00002327
Iteration 187/1000 | Loss: 0.00002327
Iteration 188/1000 | Loss: 0.00002327
Iteration 189/1000 | Loss: 0.00002327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.326984395040199e-05, 2.326984395040199e-05, 2.326984395040199e-05, 2.326984395040199e-05, 2.326984395040199e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.326984395040199e-05

Optimization complete. Final v2v error: 4.167120933532715 mm

Highest mean error: 4.394049644470215 mm for frame 43

Lowest mean error: 4.041603088378906 mm for frame 119

Saving results

Total time: 44.81210112571716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052027
Iteration 2/25 | Loss: 0.00209960
Iteration 3/25 | Loss: 0.00195001
Iteration 4/25 | Loss: 0.00173188
Iteration 5/25 | Loss: 0.00162207
Iteration 6/25 | Loss: 0.00156298
Iteration 7/25 | Loss: 0.00152988
Iteration 8/25 | Loss: 0.00151489
Iteration 9/25 | Loss: 0.00151047
Iteration 10/25 | Loss: 0.00157270
Iteration 11/25 | Loss: 0.00148810
Iteration 12/25 | Loss: 0.00148320
Iteration 13/25 | Loss: 0.00148268
Iteration 14/25 | Loss: 0.00148215
Iteration 15/25 | Loss: 0.00148197
Iteration 16/25 | Loss: 0.00148192
Iteration 17/25 | Loss: 0.00148192
Iteration 18/25 | Loss: 0.00148192
Iteration 19/25 | Loss: 0.00148192
Iteration 20/25 | Loss: 0.00148192
Iteration 21/25 | Loss: 0.00148192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014819216448813677, 0.0014819216448813677, 0.0014819216448813677, 0.0014819216448813677, 0.0014819216448813677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014819216448813677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94335479
Iteration 2/25 | Loss: 0.00112157
Iteration 3/25 | Loss: 0.00112157
Iteration 4/25 | Loss: 0.00112157
Iteration 5/25 | Loss: 0.00112157
Iteration 6/25 | Loss: 0.00112157
Iteration 7/25 | Loss: 0.00112157
Iteration 8/25 | Loss: 0.00112157
Iteration 9/25 | Loss: 0.00112157
Iteration 10/25 | Loss: 0.00112157
Iteration 11/25 | Loss: 0.00112157
Iteration 12/25 | Loss: 0.00112157
Iteration 13/25 | Loss: 0.00112157
Iteration 14/25 | Loss: 0.00112157
Iteration 15/25 | Loss: 0.00112157
Iteration 16/25 | Loss: 0.00112157
Iteration 17/25 | Loss: 0.00112157
Iteration 18/25 | Loss: 0.00112157
Iteration 19/25 | Loss: 0.00112157
Iteration 20/25 | Loss: 0.00112157
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001121567445807159, 0.001121567445807159, 0.001121567445807159, 0.001121567445807159, 0.001121567445807159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001121567445807159

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112157
Iteration 2/1000 | Loss: 0.00005958
Iteration 3/1000 | Loss: 0.00004500
Iteration 4/1000 | Loss: 0.00004078
Iteration 5/1000 | Loss: 0.00003896
Iteration 6/1000 | Loss: 0.00003799
Iteration 7/1000 | Loss: 0.00003708
Iteration 8/1000 | Loss: 0.00137030
Iteration 9/1000 | Loss: 0.00014015
Iteration 10/1000 | Loss: 0.00003789
Iteration 11/1000 | Loss: 0.00003414
Iteration 12/1000 | Loss: 0.00003135
Iteration 13/1000 | Loss: 0.00003002
Iteration 14/1000 | Loss: 0.00002919
Iteration 15/1000 | Loss: 0.00002866
Iteration 16/1000 | Loss: 0.00002829
Iteration 17/1000 | Loss: 0.00002786
Iteration 18/1000 | Loss: 0.00002749
Iteration 19/1000 | Loss: 0.00002728
Iteration 20/1000 | Loss: 0.00002707
Iteration 21/1000 | Loss: 0.00002690
Iteration 22/1000 | Loss: 0.00002676
Iteration 23/1000 | Loss: 0.00002671
Iteration 24/1000 | Loss: 0.00002670
Iteration 25/1000 | Loss: 0.00002670
Iteration 26/1000 | Loss: 0.00002670
Iteration 27/1000 | Loss: 0.00002669
Iteration 28/1000 | Loss: 0.00002668
Iteration 29/1000 | Loss: 0.00002668
Iteration 30/1000 | Loss: 0.00002668
Iteration 31/1000 | Loss: 0.00002668
Iteration 32/1000 | Loss: 0.00002667
Iteration 33/1000 | Loss: 0.00002666
Iteration 34/1000 | Loss: 0.00002666
Iteration 35/1000 | Loss: 0.00002666
Iteration 36/1000 | Loss: 0.00002665
Iteration 37/1000 | Loss: 0.00002663
Iteration 38/1000 | Loss: 0.00002663
Iteration 39/1000 | Loss: 0.00002663
Iteration 40/1000 | Loss: 0.00002663
Iteration 41/1000 | Loss: 0.00002663
Iteration 42/1000 | Loss: 0.00002662
Iteration 43/1000 | Loss: 0.00002662
Iteration 44/1000 | Loss: 0.00002662
Iteration 45/1000 | Loss: 0.00002661
Iteration 46/1000 | Loss: 0.00002661
Iteration 47/1000 | Loss: 0.00002659
Iteration 48/1000 | Loss: 0.00002659
Iteration 49/1000 | Loss: 0.00002658
Iteration 50/1000 | Loss: 0.00002657
Iteration 51/1000 | Loss: 0.00002654
Iteration 52/1000 | Loss: 0.00002654
Iteration 53/1000 | Loss: 0.00002648
Iteration 54/1000 | Loss: 0.00002646
Iteration 55/1000 | Loss: 0.00002644
Iteration 56/1000 | Loss: 0.00002644
Iteration 57/1000 | Loss: 0.00002643
Iteration 58/1000 | Loss: 0.00002641
Iteration 59/1000 | Loss: 0.00002641
Iteration 60/1000 | Loss: 0.00002640
Iteration 61/1000 | Loss: 0.00002640
Iteration 62/1000 | Loss: 0.00002638
Iteration 63/1000 | Loss: 0.00002637
Iteration 64/1000 | Loss: 0.00002637
Iteration 65/1000 | Loss: 0.00002637
Iteration 66/1000 | Loss: 0.00002637
Iteration 67/1000 | Loss: 0.00002635
Iteration 68/1000 | Loss: 0.00002635
Iteration 69/1000 | Loss: 0.00002633
Iteration 70/1000 | Loss: 0.00002633
Iteration 71/1000 | Loss: 0.00002633
Iteration 72/1000 | Loss: 0.00002633
Iteration 73/1000 | Loss: 0.00002633
Iteration 74/1000 | Loss: 0.00002633
Iteration 75/1000 | Loss: 0.00002632
Iteration 76/1000 | Loss: 0.00002632
Iteration 77/1000 | Loss: 0.00002631
Iteration 78/1000 | Loss: 0.00002631
Iteration 79/1000 | Loss: 0.00002630
Iteration 80/1000 | Loss: 0.00002629
Iteration 81/1000 | Loss: 0.00002628
Iteration 82/1000 | Loss: 0.00002628
Iteration 83/1000 | Loss: 0.00002627
Iteration 84/1000 | Loss: 0.00002627
Iteration 85/1000 | Loss: 0.00002626
Iteration 86/1000 | Loss: 0.00002626
Iteration 87/1000 | Loss: 0.00002626
Iteration 88/1000 | Loss: 0.00002626
Iteration 89/1000 | Loss: 0.00002626
Iteration 90/1000 | Loss: 0.00002625
Iteration 91/1000 | Loss: 0.00002625
Iteration 92/1000 | Loss: 0.00002625
Iteration 93/1000 | Loss: 0.00002625
Iteration 94/1000 | Loss: 0.00002625
Iteration 95/1000 | Loss: 0.00002625
Iteration 96/1000 | Loss: 0.00002625
Iteration 97/1000 | Loss: 0.00002624
Iteration 98/1000 | Loss: 0.00002624
Iteration 99/1000 | Loss: 0.00002624
Iteration 100/1000 | Loss: 0.00002624
Iteration 101/1000 | Loss: 0.00002624
Iteration 102/1000 | Loss: 0.00002624
Iteration 103/1000 | Loss: 0.00002624
Iteration 104/1000 | Loss: 0.00002623
Iteration 105/1000 | Loss: 0.00002623
Iteration 106/1000 | Loss: 0.00002623
Iteration 107/1000 | Loss: 0.00002623
Iteration 108/1000 | Loss: 0.00002623
Iteration 109/1000 | Loss: 0.00002622
Iteration 110/1000 | Loss: 0.00002622
Iteration 111/1000 | Loss: 0.00002622
Iteration 112/1000 | Loss: 0.00002622
Iteration 113/1000 | Loss: 0.00002622
Iteration 114/1000 | Loss: 0.00002622
Iteration 115/1000 | Loss: 0.00002622
Iteration 116/1000 | Loss: 0.00002622
Iteration 117/1000 | Loss: 0.00002621
Iteration 118/1000 | Loss: 0.00002621
Iteration 119/1000 | Loss: 0.00002621
Iteration 120/1000 | Loss: 0.00002621
Iteration 121/1000 | Loss: 0.00002621
Iteration 122/1000 | Loss: 0.00002621
Iteration 123/1000 | Loss: 0.00002621
Iteration 124/1000 | Loss: 0.00002621
Iteration 125/1000 | Loss: 0.00002621
Iteration 126/1000 | Loss: 0.00002621
Iteration 127/1000 | Loss: 0.00002621
Iteration 128/1000 | Loss: 0.00002620
Iteration 129/1000 | Loss: 0.00002620
Iteration 130/1000 | Loss: 0.00002620
Iteration 131/1000 | Loss: 0.00002620
Iteration 132/1000 | Loss: 0.00002620
Iteration 133/1000 | Loss: 0.00002620
Iteration 134/1000 | Loss: 0.00002620
Iteration 135/1000 | Loss: 0.00002620
Iteration 136/1000 | Loss: 0.00002620
Iteration 137/1000 | Loss: 0.00002620
Iteration 138/1000 | Loss: 0.00002620
Iteration 139/1000 | Loss: 0.00002620
Iteration 140/1000 | Loss: 0.00002620
Iteration 141/1000 | Loss: 0.00002620
Iteration 142/1000 | Loss: 0.00002620
Iteration 143/1000 | Loss: 0.00002620
Iteration 144/1000 | Loss: 0.00002620
Iteration 145/1000 | Loss: 0.00002619
Iteration 146/1000 | Loss: 0.00002619
Iteration 147/1000 | Loss: 0.00002619
Iteration 148/1000 | Loss: 0.00002619
Iteration 149/1000 | Loss: 0.00002619
Iteration 150/1000 | Loss: 0.00002619
Iteration 151/1000 | Loss: 0.00002619
Iteration 152/1000 | Loss: 0.00002619
Iteration 153/1000 | Loss: 0.00002619
Iteration 154/1000 | Loss: 0.00002619
Iteration 155/1000 | Loss: 0.00002619
Iteration 156/1000 | Loss: 0.00002619
Iteration 157/1000 | Loss: 0.00002619
Iteration 158/1000 | Loss: 0.00002619
Iteration 159/1000 | Loss: 0.00002619
Iteration 160/1000 | Loss: 0.00002619
Iteration 161/1000 | Loss: 0.00002619
Iteration 162/1000 | Loss: 0.00002619
Iteration 163/1000 | Loss: 0.00002619
Iteration 164/1000 | Loss: 0.00002619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.6186387913185172e-05, 2.6186387913185172e-05, 2.6186387913185172e-05, 2.6186387913185172e-05, 2.6186387913185172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6186387913185172e-05

Optimization complete. Final v2v error: 4.2615814208984375 mm

Highest mean error: 5.144484519958496 mm for frame 139

Lowest mean error: 3.578657865524292 mm for frame 25

Saving results

Total time: 76.12322998046875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770673
Iteration 2/25 | Loss: 0.00161400
Iteration 3/25 | Loss: 0.00139168
Iteration 4/25 | Loss: 0.00136254
Iteration 5/25 | Loss: 0.00135644
Iteration 6/25 | Loss: 0.00135569
Iteration 7/25 | Loss: 0.00135569
Iteration 8/25 | Loss: 0.00135569
Iteration 9/25 | Loss: 0.00135569
Iteration 10/25 | Loss: 0.00135569
Iteration 11/25 | Loss: 0.00135569
Iteration 12/25 | Loss: 0.00135569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013556939084082842, 0.0013556939084082842, 0.0013556939084082842, 0.0013556939084082842, 0.0013556939084082842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013556939084082842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27459812
Iteration 2/25 | Loss: 0.00075404
Iteration 3/25 | Loss: 0.00075402
Iteration 4/25 | Loss: 0.00075402
Iteration 5/25 | Loss: 0.00075402
Iteration 6/25 | Loss: 0.00075402
Iteration 7/25 | Loss: 0.00075402
Iteration 8/25 | Loss: 0.00075402
Iteration 9/25 | Loss: 0.00075402
Iteration 10/25 | Loss: 0.00075402
Iteration 11/25 | Loss: 0.00075402
Iteration 12/25 | Loss: 0.00075402
Iteration 13/25 | Loss: 0.00075402
Iteration 14/25 | Loss: 0.00075402
Iteration 15/25 | Loss: 0.00075402
Iteration 16/25 | Loss: 0.00075402
Iteration 17/25 | Loss: 0.00075402
Iteration 18/25 | Loss: 0.00075402
Iteration 19/25 | Loss: 0.00075402
Iteration 20/25 | Loss: 0.00075402
Iteration 21/25 | Loss: 0.00075402
Iteration 22/25 | Loss: 0.00075402
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007540208171121776, 0.0007540208171121776, 0.0007540208171121776, 0.0007540208171121776, 0.0007540208171121776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007540208171121776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075402
Iteration 2/1000 | Loss: 0.00005716
Iteration 3/1000 | Loss: 0.00003789
Iteration 4/1000 | Loss: 0.00003252
Iteration 5/1000 | Loss: 0.00003111
Iteration 6/1000 | Loss: 0.00002991
Iteration 7/1000 | Loss: 0.00002881
Iteration 8/1000 | Loss: 0.00002822
Iteration 9/1000 | Loss: 0.00002776
Iteration 10/1000 | Loss: 0.00002737
Iteration 11/1000 | Loss: 0.00002701
Iteration 12/1000 | Loss: 0.00002675
Iteration 13/1000 | Loss: 0.00002651
Iteration 14/1000 | Loss: 0.00002650
Iteration 15/1000 | Loss: 0.00002630
Iteration 16/1000 | Loss: 0.00002627
Iteration 17/1000 | Loss: 0.00002606
Iteration 18/1000 | Loss: 0.00002605
Iteration 19/1000 | Loss: 0.00002594
Iteration 20/1000 | Loss: 0.00002590
Iteration 21/1000 | Loss: 0.00002590
Iteration 22/1000 | Loss: 0.00002589
Iteration 23/1000 | Loss: 0.00002589
Iteration 24/1000 | Loss: 0.00002584
Iteration 25/1000 | Loss: 0.00002584
Iteration 26/1000 | Loss: 0.00002584
Iteration 27/1000 | Loss: 0.00002584
Iteration 28/1000 | Loss: 0.00002583
Iteration 29/1000 | Loss: 0.00002580
Iteration 30/1000 | Loss: 0.00002580
Iteration 31/1000 | Loss: 0.00002580
Iteration 32/1000 | Loss: 0.00002579
Iteration 33/1000 | Loss: 0.00002579
Iteration 34/1000 | Loss: 0.00002579
Iteration 35/1000 | Loss: 0.00002579
Iteration 36/1000 | Loss: 0.00002578
Iteration 37/1000 | Loss: 0.00002578
Iteration 38/1000 | Loss: 0.00002578
Iteration 39/1000 | Loss: 0.00002577
Iteration 40/1000 | Loss: 0.00002577
Iteration 41/1000 | Loss: 0.00002576
Iteration 42/1000 | Loss: 0.00002576
Iteration 43/1000 | Loss: 0.00002576
Iteration 44/1000 | Loss: 0.00002575
Iteration 45/1000 | Loss: 0.00002575
Iteration 46/1000 | Loss: 0.00002575
Iteration 47/1000 | Loss: 0.00002575
Iteration 48/1000 | Loss: 0.00002574
Iteration 49/1000 | Loss: 0.00002574
Iteration 50/1000 | Loss: 0.00002574
Iteration 51/1000 | Loss: 0.00002573
Iteration 52/1000 | Loss: 0.00002573
Iteration 53/1000 | Loss: 0.00002573
Iteration 54/1000 | Loss: 0.00002572
Iteration 55/1000 | Loss: 0.00002572
Iteration 56/1000 | Loss: 0.00002572
Iteration 57/1000 | Loss: 0.00002572
Iteration 58/1000 | Loss: 0.00002572
Iteration 59/1000 | Loss: 0.00002572
Iteration 60/1000 | Loss: 0.00002571
Iteration 61/1000 | Loss: 0.00002571
Iteration 62/1000 | Loss: 0.00002571
Iteration 63/1000 | Loss: 0.00002571
Iteration 64/1000 | Loss: 0.00002571
Iteration 65/1000 | Loss: 0.00002570
Iteration 66/1000 | Loss: 0.00002570
Iteration 67/1000 | Loss: 0.00002570
Iteration 68/1000 | Loss: 0.00002570
Iteration 69/1000 | Loss: 0.00002570
Iteration 70/1000 | Loss: 0.00002570
Iteration 71/1000 | Loss: 0.00002570
Iteration 72/1000 | Loss: 0.00002570
Iteration 73/1000 | Loss: 0.00002569
Iteration 74/1000 | Loss: 0.00002569
Iteration 75/1000 | Loss: 0.00002569
Iteration 76/1000 | Loss: 0.00002569
Iteration 77/1000 | Loss: 0.00002569
Iteration 78/1000 | Loss: 0.00002569
Iteration 79/1000 | Loss: 0.00002569
Iteration 80/1000 | Loss: 0.00002568
Iteration 81/1000 | Loss: 0.00002568
Iteration 82/1000 | Loss: 0.00002568
Iteration 83/1000 | Loss: 0.00002568
Iteration 84/1000 | Loss: 0.00002568
Iteration 85/1000 | Loss: 0.00002568
Iteration 86/1000 | Loss: 0.00002567
Iteration 87/1000 | Loss: 0.00002567
Iteration 88/1000 | Loss: 0.00002567
Iteration 89/1000 | Loss: 0.00002567
Iteration 90/1000 | Loss: 0.00002567
Iteration 91/1000 | Loss: 0.00002567
Iteration 92/1000 | Loss: 0.00002567
Iteration 93/1000 | Loss: 0.00002567
Iteration 94/1000 | Loss: 0.00002567
Iteration 95/1000 | Loss: 0.00002567
Iteration 96/1000 | Loss: 0.00002567
Iteration 97/1000 | Loss: 0.00002567
Iteration 98/1000 | Loss: 0.00002566
Iteration 99/1000 | Loss: 0.00002566
Iteration 100/1000 | Loss: 0.00002566
Iteration 101/1000 | Loss: 0.00002566
Iteration 102/1000 | Loss: 0.00002566
Iteration 103/1000 | Loss: 0.00002566
Iteration 104/1000 | Loss: 0.00002566
Iteration 105/1000 | Loss: 0.00002566
Iteration 106/1000 | Loss: 0.00002566
Iteration 107/1000 | Loss: 0.00002566
Iteration 108/1000 | Loss: 0.00002566
Iteration 109/1000 | Loss: 0.00002566
Iteration 110/1000 | Loss: 0.00002566
Iteration 111/1000 | Loss: 0.00002565
Iteration 112/1000 | Loss: 0.00002565
Iteration 113/1000 | Loss: 0.00002565
Iteration 114/1000 | Loss: 0.00002565
Iteration 115/1000 | Loss: 0.00002565
Iteration 116/1000 | Loss: 0.00002565
Iteration 117/1000 | Loss: 0.00002565
Iteration 118/1000 | Loss: 0.00002565
Iteration 119/1000 | Loss: 0.00002565
Iteration 120/1000 | Loss: 0.00002565
Iteration 121/1000 | Loss: 0.00002565
Iteration 122/1000 | Loss: 0.00002565
Iteration 123/1000 | Loss: 0.00002565
Iteration 124/1000 | Loss: 0.00002565
Iteration 125/1000 | Loss: 0.00002565
Iteration 126/1000 | Loss: 0.00002565
Iteration 127/1000 | Loss: 0.00002565
Iteration 128/1000 | Loss: 0.00002565
Iteration 129/1000 | Loss: 0.00002565
Iteration 130/1000 | Loss: 0.00002565
Iteration 131/1000 | Loss: 0.00002565
Iteration 132/1000 | Loss: 0.00002564
Iteration 133/1000 | Loss: 0.00002564
Iteration 134/1000 | Loss: 0.00002564
Iteration 135/1000 | Loss: 0.00002564
Iteration 136/1000 | Loss: 0.00002564
Iteration 137/1000 | Loss: 0.00002564
Iteration 138/1000 | Loss: 0.00002564
Iteration 139/1000 | Loss: 0.00002564
Iteration 140/1000 | Loss: 0.00002564
Iteration 141/1000 | Loss: 0.00002564
Iteration 142/1000 | Loss: 0.00002564
Iteration 143/1000 | Loss: 0.00002564
Iteration 144/1000 | Loss: 0.00002564
Iteration 145/1000 | Loss: 0.00002564
Iteration 146/1000 | Loss: 0.00002564
Iteration 147/1000 | Loss: 0.00002564
Iteration 148/1000 | Loss: 0.00002564
Iteration 149/1000 | Loss: 0.00002564
Iteration 150/1000 | Loss: 0.00002564
Iteration 151/1000 | Loss: 0.00002564
Iteration 152/1000 | Loss: 0.00002564
Iteration 153/1000 | Loss: 0.00002564
Iteration 154/1000 | Loss: 0.00002564
Iteration 155/1000 | Loss: 0.00002564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.56357070611557e-05, 2.56357070611557e-05, 2.56357070611557e-05, 2.56357070611557e-05, 2.56357070611557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.56357070611557e-05

Optimization complete. Final v2v error: 4.199776649475098 mm

Highest mean error: 5.197974681854248 mm for frame 137

Lowest mean error: 3.6405181884765625 mm for frame 58

Saving results

Total time: 48.04657793045044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490512
Iteration 2/25 | Loss: 0.00141312
Iteration 3/25 | Loss: 0.00132221
Iteration 4/25 | Loss: 0.00131388
Iteration 5/25 | Loss: 0.00131200
Iteration 6/25 | Loss: 0.00131200
Iteration 7/25 | Loss: 0.00131200
Iteration 8/25 | Loss: 0.00131200
Iteration 9/25 | Loss: 0.00131200
Iteration 10/25 | Loss: 0.00131200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013119973009452224, 0.0013119973009452224, 0.0013119973009452224, 0.0013119973009452224, 0.0013119973009452224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013119973009452224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.70164919
Iteration 2/25 | Loss: 0.00090780
Iteration 3/25 | Loss: 0.00090779
Iteration 4/25 | Loss: 0.00090779
Iteration 5/25 | Loss: 0.00090779
Iteration 6/25 | Loss: 0.00090779
Iteration 7/25 | Loss: 0.00090779
Iteration 8/25 | Loss: 0.00090779
Iteration 9/25 | Loss: 0.00090779
Iteration 10/25 | Loss: 0.00090779
Iteration 11/25 | Loss: 0.00090779
Iteration 12/25 | Loss: 0.00090779
Iteration 13/25 | Loss: 0.00090779
Iteration 14/25 | Loss: 0.00090779
Iteration 15/25 | Loss: 0.00090779
Iteration 16/25 | Loss: 0.00090779
Iteration 17/25 | Loss: 0.00090779
Iteration 18/25 | Loss: 0.00090779
Iteration 19/25 | Loss: 0.00090779
Iteration 20/25 | Loss: 0.00090779
Iteration 21/25 | Loss: 0.00090779
Iteration 22/25 | Loss: 0.00090779
Iteration 23/25 | Loss: 0.00090779
Iteration 24/25 | Loss: 0.00090779
Iteration 25/25 | Loss: 0.00090779
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009077875292859972, 0.0009077875292859972, 0.0009077875292859972, 0.0009077875292859972, 0.0009077875292859972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009077875292859972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090779
Iteration 2/1000 | Loss: 0.00002926
Iteration 3/1000 | Loss: 0.00002120
Iteration 4/1000 | Loss: 0.00001879
Iteration 5/1000 | Loss: 0.00001789
Iteration 6/1000 | Loss: 0.00001708
Iteration 7/1000 | Loss: 0.00001659
Iteration 8/1000 | Loss: 0.00001631
Iteration 9/1000 | Loss: 0.00001591
Iteration 10/1000 | Loss: 0.00001567
Iteration 11/1000 | Loss: 0.00001553
Iteration 12/1000 | Loss: 0.00001546
Iteration 13/1000 | Loss: 0.00001537
Iteration 14/1000 | Loss: 0.00001529
Iteration 15/1000 | Loss: 0.00001519
Iteration 16/1000 | Loss: 0.00001516
Iteration 17/1000 | Loss: 0.00001513
Iteration 18/1000 | Loss: 0.00001511
Iteration 19/1000 | Loss: 0.00001510
Iteration 20/1000 | Loss: 0.00001509
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001505
Iteration 23/1000 | Loss: 0.00001504
Iteration 24/1000 | Loss: 0.00001504
Iteration 25/1000 | Loss: 0.00001503
Iteration 26/1000 | Loss: 0.00001495
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001495
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001489
Iteration 31/1000 | Loss: 0.00001489
Iteration 32/1000 | Loss: 0.00001488
Iteration 33/1000 | Loss: 0.00001488
Iteration 34/1000 | Loss: 0.00001488
Iteration 35/1000 | Loss: 0.00001487
Iteration 36/1000 | Loss: 0.00001484
Iteration 37/1000 | Loss: 0.00001484
Iteration 38/1000 | Loss: 0.00001483
Iteration 39/1000 | Loss: 0.00001483
Iteration 40/1000 | Loss: 0.00001483
Iteration 41/1000 | Loss: 0.00001483
Iteration 42/1000 | Loss: 0.00001483
Iteration 43/1000 | Loss: 0.00001483
Iteration 44/1000 | Loss: 0.00001483
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001482
Iteration 48/1000 | Loss: 0.00001482
Iteration 49/1000 | Loss: 0.00001482
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001482
Iteration 52/1000 | Loss: 0.00001481
Iteration 53/1000 | Loss: 0.00001481
Iteration 54/1000 | Loss: 0.00001480
Iteration 55/1000 | Loss: 0.00001480
Iteration 56/1000 | Loss: 0.00001479
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001478
Iteration 60/1000 | Loss: 0.00001478
Iteration 61/1000 | Loss: 0.00001477
Iteration 62/1000 | Loss: 0.00001476
Iteration 63/1000 | Loss: 0.00001474
Iteration 64/1000 | Loss: 0.00001473
Iteration 65/1000 | Loss: 0.00001473
Iteration 66/1000 | Loss: 0.00001472
Iteration 67/1000 | Loss: 0.00001472
Iteration 68/1000 | Loss: 0.00001472
Iteration 69/1000 | Loss: 0.00001472
Iteration 70/1000 | Loss: 0.00001472
Iteration 71/1000 | Loss: 0.00001471
Iteration 72/1000 | Loss: 0.00001470
Iteration 73/1000 | Loss: 0.00001470
Iteration 74/1000 | Loss: 0.00001469
Iteration 75/1000 | Loss: 0.00001469
Iteration 76/1000 | Loss: 0.00001469
Iteration 77/1000 | Loss: 0.00001468
Iteration 78/1000 | Loss: 0.00001468
Iteration 79/1000 | Loss: 0.00001467
Iteration 80/1000 | Loss: 0.00001466
Iteration 81/1000 | Loss: 0.00001466
Iteration 82/1000 | Loss: 0.00001465
Iteration 83/1000 | Loss: 0.00001465
Iteration 84/1000 | Loss: 0.00001465
Iteration 85/1000 | Loss: 0.00001465
Iteration 86/1000 | Loss: 0.00001464
Iteration 87/1000 | Loss: 0.00001464
Iteration 88/1000 | Loss: 0.00001464
Iteration 89/1000 | Loss: 0.00001463
Iteration 90/1000 | Loss: 0.00001463
Iteration 91/1000 | Loss: 0.00001462
Iteration 92/1000 | Loss: 0.00001462
Iteration 93/1000 | Loss: 0.00001462
Iteration 94/1000 | Loss: 0.00001461
Iteration 95/1000 | Loss: 0.00001461
Iteration 96/1000 | Loss: 0.00001461
Iteration 97/1000 | Loss: 0.00001461
Iteration 98/1000 | Loss: 0.00001460
Iteration 99/1000 | Loss: 0.00001460
Iteration 100/1000 | Loss: 0.00001460
Iteration 101/1000 | Loss: 0.00001459
Iteration 102/1000 | Loss: 0.00001459
Iteration 103/1000 | Loss: 0.00001458
Iteration 104/1000 | Loss: 0.00001458
Iteration 105/1000 | Loss: 0.00001458
Iteration 106/1000 | Loss: 0.00001458
Iteration 107/1000 | Loss: 0.00001458
Iteration 108/1000 | Loss: 0.00001457
Iteration 109/1000 | Loss: 0.00001457
Iteration 110/1000 | Loss: 0.00001457
Iteration 111/1000 | Loss: 0.00001457
Iteration 112/1000 | Loss: 0.00001457
Iteration 113/1000 | Loss: 0.00001457
Iteration 114/1000 | Loss: 0.00001457
Iteration 115/1000 | Loss: 0.00001457
Iteration 116/1000 | Loss: 0.00001457
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001456
Iteration 120/1000 | Loss: 0.00001456
Iteration 121/1000 | Loss: 0.00001455
Iteration 122/1000 | Loss: 0.00001455
Iteration 123/1000 | Loss: 0.00001455
Iteration 124/1000 | Loss: 0.00001455
Iteration 125/1000 | Loss: 0.00001455
Iteration 126/1000 | Loss: 0.00001455
Iteration 127/1000 | Loss: 0.00001455
Iteration 128/1000 | Loss: 0.00001455
Iteration 129/1000 | Loss: 0.00001455
Iteration 130/1000 | Loss: 0.00001454
Iteration 131/1000 | Loss: 0.00001454
Iteration 132/1000 | Loss: 0.00001454
Iteration 133/1000 | Loss: 0.00001454
Iteration 134/1000 | Loss: 0.00001454
Iteration 135/1000 | Loss: 0.00001454
Iteration 136/1000 | Loss: 0.00001454
Iteration 137/1000 | Loss: 0.00001454
Iteration 138/1000 | Loss: 0.00001454
Iteration 139/1000 | Loss: 0.00001454
Iteration 140/1000 | Loss: 0.00001454
Iteration 141/1000 | Loss: 0.00001454
Iteration 142/1000 | Loss: 0.00001454
Iteration 143/1000 | Loss: 0.00001454
Iteration 144/1000 | Loss: 0.00001454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.45408594107721e-05, 1.45408594107721e-05, 1.45408594107721e-05, 1.45408594107721e-05, 1.45408594107721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.45408594107721e-05

Optimization complete. Final v2v error: 3.2483389377593994 mm

Highest mean error: 3.5373001098632812 mm for frame 214

Lowest mean error: 2.9713401794433594 mm for frame 124

Saving results

Total time: 43.40717077255249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035396
Iteration 2/25 | Loss: 0.00189838
Iteration 3/25 | Loss: 0.00167854
Iteration 4/25 | Loss: 0.00134357
Iteration 5/25 | Loss: 0.00132195
Iteration 6/25 | Loss: 0.00129830
Iteration 7/25 | Loss: 0.00129599
Iteration 8/25 | Loss: 0.00129511
Iteration 9/25 | Loss: 0.00129478
Iteration 10/25 | Loss: 0.00129457
Iteration 11/25 | Loss: 0.00129452
Iteration 12/25 | Loss: 0.00129452
Iteration 13/25 | Loss: 0.00129451
Iteration 14/25 | Loss: 0.00129451
Iteration 15/25 | Loss: 0.00129451
Iteration 16/25 | Loss: 0.00129451
Iteration 17/25 | Loss: 0.00129451
Iteration 18/25 | Loss: 0.00129451
Iteration 19/25 | Loss: 0.00129451
Iteration 20/25 | Loss: 0.00129451
Iteration 21/25 | Loss: 0.00129451
Iteration 22/25 | Loss: 0.00129451
Iteration 23/25 | Loss: 0.00129451
Iteration 24/25 | Loss: 0.00129450
Iteration 25/25 | Loss: 0.00129450

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.95690393
Iteration 2/25 | Loss: 0.00132918
Iteration 3/25 | Loss: 0.00089331
Iteration 4/25 | Loss: 0.00089331
Iteration 5/25 | Loss: 0.00089331
Iteration 6/25 | Loss: 0.00089331
Iteration 7/25 | Loss: 0.00089331
Iteration 8/25 | Loss: 0.00089331
Iteration 9/25 | Loss: 0.00089331
Iteration 10/25 | Loss: 0.00089331
Iteration 11/25 | Loss: 0.00089331
Iteration 12/25 | Loss: 0.00089331
Iteration 13/25 | Loss: 0.00089331
Iteration 14/25 | Loss: 0.00089331
Iteration 15/25 | Loss: 0.00089331
Iteration 16/25 | Loss: 0.00089331
Iteration 17/25 | Loss: 0.00089331
Iteration 18/25 | Loss: 0.00089331
Iteration 19/25 | Loss: 0.00089331
Iteration 20/25 | Loss: 0.00089331
Iteration 21/25 | Loss: 0.00089331
Iteration 22/25 | Loss: 0.00089331
Iteration 23/25 | Loss: 0.00089331
Iteration 24/25 | Loss: 0.00089331
Iteration 25/25 | Loss: 0.00089331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089331
Iteration 2/1000 | Loss: 0.00046595
Iteration 3/1000 | Loss: 0.00016494
Iteration 4/1000 | Loss: 0.00002023
Iteration 5/1000 | Loss: 0.00001856
Iteration 6/1000 | Loss: 0.00006496
Iteration 7/1000 | Loss: 0.00040599
Iteration 8/1000 | Loss: 0.00003741
Iteration 9/1000 | Loss: 0.00002611
Iteration 10/1000 | Loss: 0.00001746
Iteration 11/1000 | Loss: 0.00001662
Iteration 12/1000 | Loss: 0.00009731
Iteration 13/1000 | Loss: 0.00001635
Iteration 14/1000 | Loss: 0.00001596
Iteration 15/1000 | Loss: 0.00001591
Iteration 16/1000 | Loss: 0.00001587
Iteration 17/1000 | Loss: 0.00007854
Iteration 18/1000 | Loss: 0.00006575
Iteration 19/1000 | Loss: 0.00001586
Iteration 20/1000 | Loss: 0.00004484
Iteration 21/1000 | Loss: 0.00003151
Iteration 22/1000 | Loss: 0.00020560
Iteration 23/1000 | Loss: 0.00003447
Iteration 24/1000 | Loss: 0.00002359
Iteration 25/1000 | Loss: 0.00001544
Iteration 26/1000 | Loss: 0.00002500
Iteration 27/1000 | Loss: 0.00017969
Iteration 28/1000 | Loss: 0.00002923
Iteration 29/1000 | Loss: 0.00003583
Iteration 30/1000 | Loss: 0.00001665
Iteration 31/1000 | Loss: 0.00001652
Iteration 32/1000 | Loss: 0.00002746
Iteration 33/1000 | Loss: 0.00001711
Iteration 34/1000 | Loss: 0.00001530
Iteration 35/1000 | Loss: 0.00002535
Iteration 36/1000 | Loss: 0.00001716
Iteration 37/1000 | Loss: 0.00001747
Iteration 38/1000 | Loss: 0.00001513
Iteration 39/1000 | Loss: 0.00001962
Iteration 40/1000 | Loss: 0.00001962
Iteration 41/1000 | Loss: 0.00001499
Iteration 42/1000 | Loss: 0.00001496
Iteration 43/1000 | Loss: 0.00001495
Iteration 44/1000 | Loss: 0.00001495
Iteration 45/1000 | Loss: 0.00001495
Iteration 46/1000 | Loss: 0.00001495
Iteration 47/1000 | Loss: 0.00001495
Iteration 48/1000 | Loss: 0.00001495
Iteration 49/1000 | Loss: 0.00001494
Iteration 50/1000 | Loss: 0.00001494
Iteration 51/1000 | Loss: 0.00001494
Iteration 52/1000 | Loss: 0.00001493
Iteration 53/1000 | Loss: 0.00001493
Iteration 54/1000 | Loss: 0.00001493
Iteration 55/1000 | Loss: 0.00001492
Iteration 56/1000 | Loss: 0.00001492
Iteration 57/1000 | Loss: 0.00001490
Iteration 58/1000 | Loss: 0.00001489
Iteration 59/1000 | Loss: 0.00001489
Iteration 60/1000 | Loss: 0.00005444
Iteration 61/1000 | Loss: 0.00012876
Iteration 62/1000 | Loss: 0.00001500
Iteration 63/1000 | Loss: 0.00002673
Iteration 64/1000 | Loss: 0.00001479
Iteration 65/1000 | Loss: 0.00001479
Iteration 66/1000 | Loss: 0.00001479
Iteration 67/1000 | Loss: 0.00001479
Iteration 68/1000 | Loss: 0.00001478
Iteration 69/1000 | Loss: 0.00001478
Iteration 70/1000 | Loss: 0.00001478
Iteration 71/1000 | Loss: 0.00001478
Iteration 72/1000 | Loss: 0.00001478
Iteration 73/1000 | Loss: 0.00001478
Iteration 74/1000 | Loss: 0.00001478
Iteration 75/1000 | Loss: 0.00001478
Iteration 76/1000 | Loss: 0.00001477
Iteration 77/1000 | Loss: 0.00001477
Iteration 78/1000 | Loss: 0.00001476
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001476
Iteration 81/1000 | Loss: 0.00001476
Iteration 82/1000 | Loss: 0.00001476
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001476
Iteration 85/1000 | Loss: 0.00001476
Iteration 86/1000 | Loss: 0.00001476
Iteration 87/1000 | Loss: 0.00001476
Iteration 88/1000 | Loss: 0.00001476
Iteration 89/1000 | Loss: 0.00001476
Iteration 90/1000 | Loss: 0.00001476
Iteration 91/1000 | Loss: 0.00001476
Iteration 92/1000 | Loss: 0.00001476
Iteration 93/1000 | Loss: 0.00001476
Iteration 94/1000 | Loss: 0.00001476
Iteration 95/1000 | Loss: 0.00001476
Iteration 96/1000 | Loss: 0.00001476
Iteration 97/1000 | Loss: 0.00001476
Iteration 98/1000 | Loss: 0.00001476
Iteration 99/1000 | Loss: 0.00001476
Iteration 100/1000 | Loss: 0.00001476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.476249144616304e-05, 1.476249144616304e-05, 1.476249144616304e-05, 1.476249144616304e-05, 1.476249144616304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.476249144616304e-05

Optimization complete. Final v2v error: 3.269862651824951 mm

Highest mean error: 3.825795888900757 mm for frame 85

Lowest mean error: 2.985196352005005 mm for frame 31

Saving results

Total time: 78.71034526824951
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759419
Iteration 2/25 | Loss: 0.00157018
Iteration 3/25 | Loss: 0.00134896
Iteration 4/25 | Loss: 0.00132820
Iteration 5/25 | Loss: 0.00132428
Iteration 6/25 | Loss: 0.00132413
Iteration 7/25 | Loss: 0.00132413
Iteration 8/25 | Loss: 0.00132413
Iteration 9/25 | Loss: 0.00132413
Iteration 10/25 | Loss: 0.00132413
Iteration 11/25 | Loss: 0.00132413
Iteration 12/25 | Loss: 0.00132413
Iteration 13/25 | Loss: 0.00132413
Iteration 14/25 | Loss: 0.00132413
Iteration 15/25 | Loss: 0.00132413
Iteration 16/25 | Loss: 0.00132413
Iteration 17/25 | Loss: 0.00132413
Iteration 18/25 | Loss: 0.00132413
Iteration 19/25 | Loss: 0.00132413
Iteration 20/25 | Loss: 0.00132413
Iteration 21/25 | Loss: 0.00132413
Iteration 22/25 | Loss: 0.00132413
Iteration 23/25 | Loss: 0.00132413
Iteration 24/25 | Loss: 0.00132413
Iteration 25/25 | Loss: 0.00132413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35994303
Iteration 2/25 | Loss: 0.00074088
Iteration 3/25 | Loss: 0.00074084
Iteration 4/25 | Loss: 0.00074084
Iteration 5/25 | Loss: 0.00074084
Iteration 6/25 | Loss: 0.00074084
Iteration 7/25 | Loss: 0.00074084
Iteration 8/25 | Loss: 0.00074084
Iteration 9/25 | Loss: 0.00074084
Iteration 10/25 | Loss: 0.00074084
Iteration 11/25 | Loss: 0.00074084
Iteration 12/25 | Loss: 0.00074084
Iteration 13/25 | Loss: 0.00074084
Iteration 14/25 | Loss: 0.00074084
Iteration 15/25 | Loss: 0.00074084
Iteration 16/25 | Loss: 0.00074084
Iteration 17/25 | Loss: 0.00074084
Iteration 18/25 | Loss: 0.00074084
Iteration 19/25 | Loss: 0.00074084
Iteration 20/25 | Loss: 0.00074084
Iteration 21/25 | Loss: 0.00074084
Iteration 22/25 | Loss: 0.00074084
Iteration 23/25 | Loss: 0.00074084
Iteration 24/25 | Loss: 0.00074084
Iteration 25/25 | Loss: 0.00074084
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007408400997519493, 0.0007408400997519493, 0.0007408400997519493, 0.0007408400997519493, 0.0007408400997519493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007408400997519493

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074084
Iteration 2/1000 | Loss: 0.00003785
Iteration 3/1000 | Loss: 0.00002602
Iteration 4/1000 | Loss: 0.00002261
Iteration 5/1000 | Loss: 0.00002129
Iteration 6/1000 | Loss: 0.00002036
Iteration 7/1000 | Loss: 0.00001958
Iteration 8/1000 | Loss: 0.00001900
Iteration 9/1000 | Loss: 0.00001854
Iteration 10/1000 | Loss: 0.00001799
Iteration 11/1000 | Loss: 0.00001769
Iteration 12/1000 | Loss: 0.00001756
Iteration 13/1000 | Loss: 0.00001735
Iteration 14/1000 | Loss: 0.00001729
Iteration 15/1000 | Loss: 0.00001726
Iteration 16/1000 | Loss: 0.00001707
Iteration 17/1000 | Loss: 0.00001707
Iteration 18/1000 | Loss: 0.00001706
Iteration 19/1000 | Loss: 0.00001700
Iteration 20/1000 | Loss: 0.00001696
Iteration 21/1000 | Loss: 0.00001693
Iteration 22/1000 | Loss: 0.00001693
Iteration 23/1000 | Loss: 0.00001692
Iteration 24/1000 | Loss: 0.00001692
Iteration 25/1000 | Loss: 0.00001692
Iteration 26/1000 | Loss: 0.00001691
Iteration 27/1000 | Loss: 0.00001691
Iteration 28/1000 | Loss: 0.00001691
Iteration 29/1000 | Loss: 0.00001691
Iteration 30/1000 | Loss: 0.00001691
Iteration 31/1000 | Loss: 0.00001691
Iteration 32/1000 | Loss: 0.00001691
Iteration 33/1000 | Loss: 0.00001691
Iteration 34/1000 | Loss: 0.00001691
Iteration 35/1000 | Loss: 0.00001691
Iteration 36/1000 | Loss: 0.00001691
Iteration 37/1000 | Loss: 0.00001691
Iteration 38/1000 | Loss: 0.00001691
Iteration 39/1000 | Loss: 0.00001691
Iteration 40/1000 | Loss: 0.00001691
Iteration 41/1000 | Loss: 0.00001691
Iteration 42/1000 | Loss: 0.00001691
Iteration 43/1000 | Loss: 0.00001691
Iteration 44/1000 | Loss: 0.00001690
Iteration 45/1000 | Loss: 0.00001690
Iteration 46/1000 | Loss: 0.00001689
Iteration 47/1000 | Loss: 0.00001689
Iteration 48/1000 | Loss: 0.00001689
Iteration 49/1000 | Loss: 0.00001689
Iteration 50/1000 | Loss: 0.00001689
Iteration 51/1000 | Loss: 0.00001688
Iteration 52/1000 | Loss: 0.00001688
Iteration 53/1000 | Loss: 0.00001688
Iteration 54/1000 | Loss: 0.00001687
Iteration 55/1000 | Loss: 0.00001687
Iteration 56/1000 | Loss: 0.00001687
Iteration 57/1000 | Loss: 0.00001687
Iteration 58/1000 | Loss: 0.00001686
Iteration 59/1000 | Loss: 0.00001686
Iteration 60/1000 | Loss: 0.00001686
Iteration 61/1000 | Loss: 0.00001685
Iteration 62/1000 | Loss: 0.00001684
Iteration 63/1000 | Loss: 0.00001684
Iteration 64/1000 | Loss: 0.00001683
Iteration 65/1000 | Loss: 0.00001683
Iteration 66/1000 | Loss: 0.00001683
Iteration 67/1000 | Loss: 0.00001682
Iteration 68/1000 | Loss: 0.00001682
Iteration 69/1000 | Loss: 0.00001682
Iteration 70/1000 | Loss: 0.00001682
Iteration 71/1000 | Loss: 0.00001682
Iteration 72/1000 | Loss: 0.00001682
Iteration 73/1000 | Loss: 0.00001682
Iteration 74/1000 | Loss: 0.00001682
Iteration 75/1000 | Loss: 0.00001681
Iteration 76/1000 | Loss: 0.00001681
Iteration 77/1000 | Loss: 0.00001681
Iteration 78/1000 | Loss: 0.00001681
Iteration 79/1000 | Loss: 0.00001680
Iteration 80/1000 | Loss: 0.00001680
Iteration 81/1000 | Loss: 0.00001680
Iteration 82/1000 | Loss: 0.00001680
Iteration 83/1000 | Loss: 0.00001679
Iteration 84/1000 | Loss: 0.00001679
Iteration 85/1000 | Loss: 0.00001679
Iteration 86/1000 | Loss: 0.00001679
Iteration 87/1000 | Loss: 0.00001679
Iteration 88/1000 | Loss: 0.00001678
Iteration 89/1000 | Loss: 0.00001678
Iteration 90/1000 | Loss: 0.00001678
Iteration 91/1000 | Loss: 0.00001678
Iteration 92/1000 | Loss: 0.00001677
Iteration 93/1000 | Loss: 0.00001677
Iteration 94/1000 | Loss: 0.00001676
Iteration 95/1000 | Loss: 0.00001676
Iteration 96/1000 | Loss: 0.00001676
Iteration 97/1000 | Loss: 0.00001675
Iteration 98/1000 | Loss: 0.00001674
Iteration 99/1000 | Loss: 0.00001674
Iteration 100/1000 | Loss: 0.00001674
Iteration 101/1000 | Loss: 0.00001674
Iteration 102/1000 | Loss: 0.00001674
Iteration 103/1000 | Loss: 0.00001673
Iteration 104/1000 | Loss: 0.00001673
Iteration 105/1000 | Loss: 0.00001673
Iteration 106/1000 | Loss: 0.00001673
Iteration 107/1000 | Loss: 0.00001673
Iteration 108/1000 | Loss: 0.00001673
Iteration 109/1000 | Loss: 0.00001673
Iteration 110/1000 | Loss: 0.00001673
Iteration 111/1000 | Loss: 0.00001672
Iteration 112/1000 | Loss: 0.00001672
Iteration 113/1000 | Loss: 0.00001672
Iteration 114/1000 | Loss: 0.00001672
Iteration 115/1000 | Loss: 0.00001672
Iteration 116/1000 | Loss: 0.00001672
Iteration 117/1000 | Loss: 0.00001672
Iteration 118/1000 | Loss: 0.00001672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.6720059647923335e-05, 1.6720059647923335e-05, 1.6720059647923335e-05, 1.6720059647923335e-05, 1.6720059647923335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6720059647923335e-05

Optimization complete. Final v2v error: 3.475395917892456 mm

Highest mean error: 4.078636169433594 mm for frame 21

Lowest mean error: 3.07527494430542 mm for frame 77

Saving results

Total time: 41.86626720428467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809375
Iteration 2/25 | Loss: 0.00136944
Iteration 3/25 | Loss: 0.00129090
Iteration 4/25 | Loss: 0.00127696
Iteration 5/25 | Loss: 0.00127517
Iteration 6/25 | Loss: 0.00127502
Iteration 7/25 | Loss: 0.00127502
Iteration 8/25 | Loss: 0.00127502
Iteration 9/25 | Loss: 0.00127502
Iteration 10/25 | Loss: 0.00127502
Iteration 11/25 | Loss: 0.00127502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012750212335959077, 0.0012750212335959077, 0.0012750212335959077, 0.0012750212335959077, 0.0012750212335959077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012750212335959077

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41790485
Iteration 2/25 | Loss: 0.00079851
Iteration 3/25 | Loss: 0.00079851
Iteration 4/25 | Loss: 0.00079851
Iteration 5/25 | Loss: 0.00079851
Iteration 6/25 | Loss: 0.00079851
Iteration 7/25 | Loss: 0.00079851
Iteration 8/25 | Loss: 0.00079851
Iteration 9/25 | Loss: 0.00079851
Iteration 10/25 | Loss: 0.00079851
Iteration 11/25 | Loss: 0.00079851
Iteration 12/25 | Loss: 0.00079851
Iteration 13/25 | Loss: 0.00079851
Iteration 14/25 | Loss: 0.00079851
Iteration 15/25 | Loss: 0.00079851
Iteration 16/25 | Loss: 0.00079851
Iteration 17/25 | Loss: 0.00079851
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007985075353644788, 0.0007985075353644788, 0.0007985075353644788, 0.0007985075353644788, 0.0007985075353644788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007985075353644788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079851
Iteration 2/1000 | Loss: 0.00002761
Iteration 3/1000 | Loss: 0.00001897
Iteration 4/1000 | Loss: 0.00001766
Iteration 5/1000 | Loss: 0.00001706
Iteration 6/1000 | Loss: 0.00001664
Iteration 7/1000 | Loss: 0.00001618
Iteration 8/1000 | Loss: 0.00001588
Iteration 9/1000 | Loss: 0.00001566
Iteration 10/1000 | Loss: 0.00001539
Iteration 11/1000 | Loss: 0.00001531
Iteration 12/1000 | Loss: 0.00001510
Iteration 13/1000 | Loss: 0.00001509
Iteration 14/1000 | Loss: 0.00001507
Iteration 15/1000 | Loss: 0.00001506
Iteration 16/1000 | Loss: 0.00001506
Iteration 17/1000 | Loss: 0.00001503
Iteration 18/1000 | Loss: 0.00001501
Iteration 19/1000 | Loss: 0.00001500
Iteration 20/1000 | Loss: 0.00001500
Iteration 21/1000 | Loss: 0.00001500
Iteration 22/1000 | Loss: 0.00001500
Iteration 23/1000 | Loss: 0.00001500
Iteration 24/1000 | Loss: 0.00001497
Iteration 25/1000 | Loss: 0.00001491
Iteration 26/1000 | Loss: 0.00001489
Iteration 27/1000 | Loss: 0.00001485
Iteration 28/1000 | Loss: 0.00001485
Iteration 29/1000 | Loss: 0.00001485
Iteration 30/1000 | Loss: 0.00001484
Iteration 31/1000 | Loss: 0.00001484
Iteration 32/1000 | Loss: 0.00001484
Iteration 33/1000 | Loss: 0.00001484
Iteration 34/1000 | Loss: 0.00001484
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001484
Iteration 37/1000 | Loss: 0.00001484
Iteration 38/1000 | Loss: 0.00001484
Iteration 39/1000 | Loss: 0.00001484
Iteration 40/1000 | Loss: 0.00001484
Iteration 41/1000 | Loss: 0.00001484
Iteration 42/1000 | Loss: 0.00001482
Iteration 43/1000 | Loss: 0.00001481
Iteration 44/1000 | Loss: 0.00001481
Iteration 45/1000 | Loss: 0.00001481
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001481
Iteration 48/1000 | Loss: 0.00001481
Iteration 49/1000 | Loss: 0.00001481
Iteration 50/1000 | Loss: 0.00001481
Iteration 51/1000 | Loss: 0.00001481
Iteration 52/1000 | Loss: 0.00001480
Iteration 53/1000 | Loss: 0.00001480
Iteration 54/1000 | Loss: 0.00001480
Iteration 55/1000 | Loss: 0.00001478
Iteration 56/1000 | Loss: 0.00001477
Iteration 57/1000 | Loss: 0.00001477
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001476
Iteration 61/1000 | Loss: 0.00001475
Iteration 62/1000 | Loss: 0.00001475
Iteration 63/1000 | Loss: 0.00001475
Iteration 64/1000 | Loss: 0.00001474
Iteration 65/1000 | Loss: 0.00001474
Iteration 66/1000 | Loss: 0.00001474
Iteration 67/1000 | Loss: 0.00001474
Iteration 68/1000 | Loss: 0.00001473
Iteration 69/1000 | Loss: 0.00001473
Iteration 70/1000 | Loss: 0.00001468
Iteration 71/1000 | Loss: 0.00001468
Iteration 72/1000 | Loss: 0.00001466
Iteration 73/1000 | Loss: 0.00001464
Iteration 74/1000 | Loss: 0.00001463
Iteration 75/1000 | Loss: 0.00001463
Iteration 76/1000 | Loss: 0.00001461
Iteration 77/1000 | Loss: 0.00001461
Iteration 78/1000 | Loss: 0.00001461
Iteration 79/1000 | Loss: 0.00001461
Iteration 80/1000 | Loss: 0.00001461
Iteration 81/1000 | Loss: 0.00001461
Iteration 82/1000 | Loss: 0.00001461
Iteration 83/1000 | Loss: 0.00001461
Iteration 84/1000 | Loss: 0.00001461
Iteration 85/1000 | Loss: 0.00001460
Iteration 86/1000 | Loss: 0.00001459
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Iteration 89/1000 | Loss: 0.00001457
Iteration 90/1000 | Loss: 0.00001457
Iteration 91/1000 | Loss: 0.00001457
Iteration 92/1000 | Loss: 0.00001456
Iteration 93/1000 | Loss: 0.00001456
Iteration 94/1000 | Loss: 0.00001456
Iteration 95/1000 | Loss: 0.00001456
Iteration 96/1000 | Loss: 0.00001455
Iteration 97/1000 | Loss: 0.00001455
Iteration 98/1000 | Loss: 0.00001455
Iteration 99/1000 | Loss: 0.00001454
Iteration 100/1000 | Loss: 0.00001454
Iteration 101/1000 | Loss: 0.00001453
Iteration 102/1000 | Loss: 0.00001453
Iteration 103/1000 | Loss: 0.00001453
Iteration 104/1000 | Loss: 0.00001453
Iteration 105/1000 | Loss: 0.00001453
Iteration 106/1000 | Loss: 0.00001453
Iteration 107/1000 | Loss: 0.00001453
Iteration 108/1000 | Loss: 0.00001453
Iteration 109/1000 | Loss: 0.00001453
Iteration 110/1000 | Loss: 0.00001453
Iteration 111/1000 | Loss: 0.00001453
Iteration 112/1000 | Loss: 0.00001453
Iteration 113/1000 | Loss: 0.00001453
Iteration 114/1000 | Loss: 0.00001453
Iteration 115/1000 | Loss: 0.00001453
Iteration 116/1000 | Loss: 0.00001453
Iteration 117/1000 | Loss: 0.00001453
Iteration 118/1000 | Loss: 0.00001453
Iteration 119/1000 | Loss: 0.00001453
Iteration 120/1000 | Loss: 0.00001453
Iteration 121/1000 | Loss: 0.00001453
Iteration 122/1000 | Loss: 0.00001453
Iteration 123/1000 | Loss: 0.00001453
Iteration 124/1000 | Loss: 0.00001453
Iteration 125/1000 | Loss: 0.00001453
Iteration 126/1000 | Loss: 0.00001453
Iteration 127/1000 | Loss: 0.00001453
Iteration 128/1000 | Loss: 0.00001453
Iteration 129/1000 | Loss: 0.00001453
Iteration 130/1000 | Loss: 0.00001453
Iteration 131/1000 | Loss: 0.00001453
Iteration 132/1000 | Loss: 0.00001453
Iteration 133/1000 | Loss: 0.00001453
Iteration 134/1000 | Loss: 0.00001453
Iteration 135/1000 | Loss: 0.00001453
Iteration 136/1000 | Loss: 0.00001453
Iteration 137/1000 | Loss: 0.00001453
Iteration 138/1000 | Loss: 0.00001453
Iteration 139/1000 | Loss: 0.00001453
Iteration 140/1000 | Loss: 0.00001453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.4526687664329074e-05, 1.4526687664329074e-05, 1.4526687664329074e-05, 1.4526687664329074e-05, 1.4526687664329074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4526687664329074e-05

Optimization complete. Final v2v error: 3.282878875732422 mm

Highest mean error: 3.352518081665039 mm for frame 70

Lowest mean error: 3.2078330516815186 mm for frame 53

Saving results

Total time: 36.01234745979309
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765513
Iteration 2/25 | Loss: 0.00156129
Iteration 3/25 | Loss: 0.00133132
Iteration 4/25 | Loss: 0.00131335
Iteration 5/25 | Loss: 0.00131011
Iteration 6/25 | Loss: 0.00131011
Iteration 7/25 | Loss: 0.00131011
Iteration 8/25 | Loss: 0.00131011
Iteration 9/25 | Loss: 0.00131005
Iteration 10/25 | Loss: 0.00131005
Iteration 11/25 | Loss: 0.00131005
Iteration 12/25 | Loss: 0.00131005
Iteration 13/25 | Loss: 0.00131005
Iteration 14/25 | Loss: 0.00131005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013100455980747938, 0.0013100455980747938, 0.0013100455980747938, 0.0013100455980747938, 0.0013100455980747938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013100455980747938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37244368
Iteration 2/25 | Loss: 0.00065527
Iteration 3/25 | Loss: 0.00065526
Iteration 4/25 | Loss: 0.00065526
Iteration 5/25 | Loss: 0.00065526
Iteration 6/25 | Loss: 0.00065526
Iteration 7/25 | Loss: 0.00065526
Iteration 8/25 | Loss: 0.00065526
Iteration 9/25 | Loss: 0.00065526
Iteration 10/25 | Loss: 0.00065526
Iteration 11/25 | Loss: 0.00065526
Iteration 12/25 | Loss: 0.00065526
Iteration 13/25 | Loss: 0.00065526
Iteration 14/25 | Loss: 0.00065526
Iteration 15/25 | Loss: 0.00065526
Iteration 16/25 | Loss: 0.00065526
Iteration 17/25 | Loss: 0.00065526
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006552618579007685, 0.0006552618579007685, 0.0006552618579007685, 0.0006552618579007685, 0.0006552618579007685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006552618579007685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065526
Iteration 2/1000 | Loss: 0.00003237
Iteration 3/1000 | Loss: 0.00002380
Iteration 4/1000 | Loss: 0.00002227
Iteration 5/1000 | Loss: 0.00002078
Iteration 6/1000 | Loss: 0.00001994
Iteration 7/1000 | Loss: 0.00001919
Iteration 8/1000 | Loss: 0.00001855
Iteration 9/1000 | Loss: 0.00001800
Iteration 10/1000 | Loss: 0.00001767
Iteration 11/1000 | Loss: 0.00001744
Iteration 12/1000 | Loss: 0.00001720
Iteration 13/1000 | Loss: 0.00001708
Iteration 14/1000 | Loss: 0.00001703
Iteration 15/1000 | Loss: 0.00001699
Iteration 16/1000 | Loss: 0.00001689
Iteration 17/1000 | Loss: 0.00001680
Iteration 18/1000 | Loss: 0.00001680
Iteration 19/1000 | Loss: 0.00001677
Iteration 20/1000 | Loss: 0.00001676
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001673
Iteration 23/1000 | Loss: 0.00001672
Iteration 24/1000 | Loss: 0.00001671
Iteration 25/1000 | Loss: 0.00001671
Iteration 26/1000 | Loss: 0.00001671
Iteration 27/1000 | Loss: 0.00001670
Iteration 28/1000 | Loss: 0.00001670
Iteration 29/1000 | Loss: 0.00001669
Iteration 30/1000 | Loss: 0.00001669
Iteration 31/1000 | Loss: 0.00001665
Iteration 32/1000 | Loss: 0.00001663
Iteration 33/1000 | Loss: 0.00001662
Iteration 34/1000 | Loss: 0.00001661
Iteration 35/1000 | Loss: 0.00001661
Iteration 36/1000 | Loss: 0.00001661
Iteration 37/1000 | Loss: 0.00001660
Iteration 38/1000 | Loss: 0.00001659
Iteration 39/1000 | Loss: 0.00001659
Iteration 40/1000 | Loss: 0.00001658
Iteration 41/1000 | Loss: 0.00001658
Iteration 42/1000 | Loss: 0.00001658
Iteration 43/1000 | Loss: 0.00001657
Iteration 44/1000 | Loss: 0.00001657
Iteration 45/1000 | Loss: 0.00001657
Iteration 46/1000 | Loss: 0.00001657
Iteration 47/1000 | Loss: 0.00001656
Iteration 48/1000 | Loss: 0.00001655
Iteration 49/1000 | Loss: 0.00001655
Iteration 50/1000 | Loss: 0.00001654
Iteration 51/1000 | Loss: 0.00001650
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001648
Iteration 54/1000 | Loss: 0.00001648
Iteration 55/1000 | Loss: 0.00001648
Iteration 56/1000 | Loss: 0.00001647
Iteration 57/1000 | Loss: 0.00001646
Iteration 58/1000 | Loss: 0.00001645
Iteration 59/1000 | Loss: 0.00001645
Iteration 60/1000 | Loss: 0.00001644
Iteration 61/1000 | Loss: 0.00001644
Iteration 62/1000 | Loss: 0.00001644
Iteration 63/1000 | Loss: 0.00001644
Iteration 64/1000 | Loss: 0.00001643
Iteration 65/1000 | Loss: 0.00001643
Iteration 66/1000 | Loss: 0.00001643
Iteration 67/1000 | Loss: 0.00001642
Iteration 68/1000 | Loss: 0.00001641
Iteration 69/1000 | Loss: 0.00001641
Iteration 70/1000 | Loss: 0.00001641
Iteration 71/1000 | Loss: 0.00001640
Iteration 72/1000 | Loss: 0.00001640
Iteration 73/1000 | Loss: 0.00001640
Iteration 74/1000 | Loss: 0.00001639
Iteration 75/1000 | Loss: 0.00001639
Iteration 76/1000 | Loss: 0.00001639
Iteration 77/1000 | Loss: 0.00001639
Iteration 78/1000 | Loss: 0.00001639
Iteration 79/1000 | Loss: 0.00001639
Iteration 80/1000 | Loss: 0.00001639
Iteration 81/1000 | Loss: 0.00001638
Iteration 82/1000 | Loss: 0.00001638
Iteration 83/1000 | Loss: 0.00001638
Iteration 84/1000 | Loss: 0.00001638
Iteration 85/1000 | Loss: 0.00001638
Iteration 86/1000 | Loss: 0.00001638
Iteration 87/1000 | Loss: 0.00001638
Iteration 88/1000 | Loss: 0.00001638
Iteration 89/1000 | Loss: 0.00001638
Iteration 90/1000 | Loss: 0.00001638
Iteration 91/1000 | Loss: 0.00001638
Iteration 92/1000 | Loss: 0.00001637
Iteration 93/1000 | Loss: 0.00001637
Iteration 94/1000 | Loss: 0.00001637
Iteration 95/1000 | Loss: 0.00001636
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001636
Iteration 98/1000 | Loss: 0.00001636
Iteration 99/1000 | Loss: 0.00001636
Iteration 100/1000 | Loss: 0.00001636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.6364330804208294e-05, 1.6364330804208294e-05, 1.6364330804208294e-05, 1.6364330804208294e-05, 1.6364330804208294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6364330804208294e-05

Optimization complete. Final v2v error: 3.4083054065704346 mm

Highest mean error: 3.766636610031128 mm for frame 9

Lowest mean error: 3.288621664047241 mm for frame 121

Saving results

Total time: 42.919190645217896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013351
Iteration 2/25 | Loss: 0.00165358
Iteration 3/25 | Loss: 0.00139101
Iteration 4/25 | Loss: 0.00135894
Iteration 5/25 | Loss: 0.00135098
Iteration 6/25 | Loss: 0.00135018
Iteration 7/25 | Loss: 0.00135018
Iteration 8/25 | Loss: 0.00135018
Iteration 9/25 | Loss: 0.00135018
Iteration 10/25 | Loss: 0.00135014
Iteration 11/25 | Loss: 0.00135014
Iteration 12/25 | Loss: 0.00135014
Iteration 13/25 | Loss: 0.00135014
Iteration 14/25 | Loss: 0.00135014
Iteration 15/25 | Loss: 0.00135014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013501449720934033, 0.0013501449720934033, 0.0013501449720934033, 0.0013501449720934033, 0.0013501449720934033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013501449720934033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60030937
Iteration 2/25 | Loss: 0.00106856
Iteration 3/25 | Loss: 0.00106855
Iteration 4/25 | Loss: 0.00106855
Iteration 5/25 | Loss: 0.00106855
Iteration 6/25 | Loss: 0.00106855
Iteration 7/25 | Loss: 0.00106855
Iteration 8/25 | Loss: 0.00106855
Iteration 9/25 | Loss: 0.00106855
Iteration 10/25 | Loss: 0.00106855
Iteration 11/25 | Loss: 0.00106855
Iteration 12/25 | Loss: 0.00106855
Iteration 13/25 | Loss: 0.00106855
Iteration 14/25 | Loss: 0.00106855
Iteration 15/25 | Loss: 0.00106855
Iteration 16/25 | Loss: 0.00106855
Iteration 17/25 | Loss: 0.00106855
Iteration 18/25 | Loss: 0.00106855
Iteration 19/25 | Loss: 0.00106855
Iteration 20/25 | Loss: 0.00106855
Iteration 21/25 | Loss: 0.00106855
Iteration 22/25 | Loss: 0.00106855
Iteration 23/25 | Loss: 0.00106855
Iteration 24/25 | Loss: 0.00106855
Iteration 25/25 | Loss: 0.00106855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106855
Iteration 2/1000 | Loss: 0.00003720
Iteration 3/1000 | Loss: 0.00002603
Iteration 4/1000 | Loss: 0.00002431
Iteration 5/1000 | Loss: 0.00002317
Iteration 6/1000 | Loss: 0.00002244
Iteration 7/1000 | Loss: 0.00002188
Iteration 8/1000 | Loss: 0.00002131
Iteration 9/1000 | Loss: 0.00002094
Iteration 10/1000 | Loss: 0.00002052
Iteration 11/1000 | Loss: 0.00002017
Iteration 12/1000 | Loss: 0.00001993
Iteration 13/1000 | Loss: 0.00001969
Iteration 14/1000 | Loss: 0.00001955
Iteration 15/1000 | Loss: 0.00001954
Iteration 16/1000 | Loss: 0.00001953
Iteration 17/1000 | Loss: 0.00001952
Iteration 18/1000 | Loss: 0.00001949
Iteration 19/1000 | Loss: 0.00001944
Iteration 20/1000 | Loss: 0.00001943
Iteration 21/1000 | Loss: 0.00001943
Iteration 22/1000 | Loss: 0.00001942
Iteration 23/1000 | Loss: 0.00001940
Iteration 24/1000 | Loss: 0.00001940
Iteration 25/1000 | Loss: 0.00001940
Iteration 26/1000 | Loss: 0.00001940
Iteration 27/1000 | Loss: 0.00001940
Iteration 28/1000 | Loss: 0.00001940
Iteration 29/1000 | Loss: 0.00001939
Iteration 30/1000 | Loss: 0.00001939
Iteration 31/1000 | Loss: 0.00001939
Iteration 32/1000 | Loss: 0.00001939
Iteration 33/1000 | Loss: 0.00001939
Iteration 34/1000 | Loss: 0.00001939
Iteration 35/1000 | Loss: 0.00001939
Iteration 36/1000 | Loss: 0.00001937
Iteration 37/1000 | Loss: 0.00001936
Iteration 38/1000 | Loss: 0.00001935
Iteration 39/1000 | Loss: 0.00001935
Iteration 40/1000 | Loss: 0.00001934
Iteration 41/1000 | Loss: 0.00001933
Iteration 42/1000 | Loss: 0.00001933
Iteration 43/1000 | Loss: 0.00001932
Iteration 44/1000 | Loss: 0.00001932
Iteration 45/1000 | Loss: 0.00001932
Iteration 46/1000 | Loss: 0.00001931
Iteration 47/1000 | Loss: 0.00001931
Iteration 48/1000 | Loss: 0.00001931
Iteration 49/1000 | Loss: 0.00001930
Iteration 50/1000 | Loss: 0.00001930
Iteration 51/1000 | Loss: 0.00001930
Iteration 52/1000 | Loss: 0.00001929
Iteration 53/1000 | Loss: 0.00001929
Iteration 54/1000 | Loss: 0.00001929
Iteration 55/1000 | Loss: 0.00001929
Iteration 56/1000 | Loss: 0.00001929
Iteration 57/1000 | Loss: 0.00001929
Iteration 58/1000 | Loss: 0.00001929
Iteration 59/1000 | Loss: 0.00001929
Iteration 60/1000 | Loss: 0.00001929
Iteration 61/1000 | Loss: 0.00001928
Iteration 62/1000 | Loss: 0.00001928
Iteration 63/1000 | Loss: 0.00001928
Iteration 64/1000 | Loss: 0.00001928
Iteration 65/1000 | Loss: 0.00001928
Iteration 66/1000 | Loss: 0.00001927
Iteration 67/1000 | Loss: 0.00001927
Iteration 68/1000 | Loss: 0.00001927
Iteration 69/1000 | Loss: 0.00001927
Iteration 70/1000 | Loss: 0.00001927
Iteration 71/1000 | Loss: 0.00001927
Iteration 72/1000 | Loss: 0.00001927
Iteration 73/1000 | Loss: 0.00001927
Iteration 74/1000 | Loss: 0.00001927
Iteration 75/1000 | Loss: 0.00001927
Iteration 76/1000 | Loss: 0.00001927
Iteration 77/1000 | Loss: 0.00001927
Iteration 78/1000 | Loss: 0.00001926
Iteration 79/1000 | Loss: 0.00001926
Iteration 80/1000 | Loss: 0.00001926
Iteration 81/1000 | Loss: 0.00001926
Iteration 82/1000 | Loss: 0.00001926
Iteration 83/1000 | Loss: 0.00001926
Iteration 84/1000 | Loss: 0.00001926
Iteration 85/1000 | Loss: 0.00001926
Iteration 86/1000 | Loss: 0.00001925
Iteration 87/1000 | Loss: 0.00001925
Iteration 88/1000 | Loss: 0.00001925
Iteration 89/1000 | Loss: 0.00001925
Iteration 90/1000 | Loss: 0.00001925
Iteration 91/1000 | Loss: 0.00001925
Iteration 92/1000 | Loss: 0.00001925
Iteration 93/1000 | Loss: 0.00001925
Iteration 94/1000 | Loss: 0.00001925
Iteration 95/1000 | Loss: 0.00001925
Iteration 96/1000 | Loss: 0.00001925
Iteration 97/1000 | Loss: 0.00001925
Iteration 98/1000 | Loss: 0.00001925
Iteration 99/1000 | Loss: 0.00001925
Iteration 100/1000 | Loss: 0.00001925
Iteration 101/1000 | Loss: 0.00001925
Iteration 102/1000 | Loss: 0.00001925
Iteration 103/1000 | Loss: 0.00001925
Iteration 104/1000 | Loss: 0.00001925
Iteration 105/1000 | Loss: 0.00001925
Iteration 106/1000 | Loss: 0.00001925
Iteration 107/1000 | Loss: 0.00001925
Iteration 108/1000 | Loss: 0.00001925
Iteration 109/1000 | Loss: 0.00001925
Iteration 110/1000 | Loss: 0.00001925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.9251316189183854e-05, 1.9251316189183854e-05, 1.9251316189183854e-05, 1.9251316189183854e-05, 1.9251316189183854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9251316189183854e-05

Optimization complete. Final v2v error: 3.760516405105591 mm

Highest mean error: 4.3344292640686035 mm for frame 111

Lowest mean error: 3.1315860748291016 mm for frame 170

Saving results

Total time: 37.22323679924011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954759
Iteration 2/25 | Loss: 0.00225864
Iteration 3/25 | Loss: 0.00170272
Iteration 4/25 | Loss: 0.00164737
Iteration 5/25 | Loss: 0.00164000
Iteration 6/25 | Loss: 0.00151665
Iteration 7/25 | Loss: 0.00145950
Iteration 8/25 | Loss: 0.00145400
Iteration 9/25 | Loss: 0.00144062
Iteration 10/25 | Loss: 0.00140438
Iteration 11/25 | Loss: 0.00139014
Iteration 12/25 | Loss: 0.00138024
Iteration 13/25 | Loss: 0.00138488
Iteration 14/25 | Loss: 0.00137855
Iteration 15/25 | Loss: 0.00137836
Iteration 16/25 | Loss: 0.00137826
Iteration 17/25 | Loss: 0.00137826
Iteration 18/25 | Loss: 0.00137826
Iteration 19/25 | Loss: 0.00137825
Iteration 20/25 | Loss: 0.00137825
Iteration 21/25 | Loss: 0.00137825
Iteration 22/25 | Loss: 0.00137825
Iteration 23/25 | Loss: 0.00137825
Iteration 24/25 | Loss: 0.00137825
Iteration 25/25 | Loss: 0.00137825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53436327
Iteration 2/25 | Loss: 0.00112667
Iteration 3/25 | Loss: 0.00104871
Iteration 4/25 | Loss: 0.00104870
Iteration 5/25 | Loss: 0.00104870
Iteration 6/25 | Loss: 0.00104870
Iteration 7/25 | Loss: 0.00104870
Iteration 8/25 | Loss: 0.00104870
Iteration 9/25 | Loss: 0.00104870
Iteration 10/25 | Loss: 0.00104870
Iteration 11/25 | Loss: 0.00104870
Iteration 12/25 | Loss: 0.00104870
Iteration 13/25 | Loss: 0.00104870
Iteration 14/25 | Loss: 0.00104870
Iteration 15/25 | Loss: 0.00104870
Iteration 16/25 | Loss: 0.00104870
Iteration 17/25 | Loss: 0.00104870
Iteration 18/25 | Loss: 0.00104870
Iteration 19/25 | Loss: 0.00104870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010487016988918185, 0.0010487016988918185, 0.0010487016988918185, 0.0010487016988918185, 0.0010487016988918185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010487016988918185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104870
Iteration 2/1000 | Loss: 0.00034376
Iteration 3/1000 | Loss: 0.00054259
Iteration 4/1000 | Loss: 0.00024592
Iteration 5/1000 | Loss: 0.00008377
Iteration 6/1000 | Loss: 0.00003144
Iteration 7/1000 | Loss: 0.00002728
Iteration 8/1000 | Loss: 0.00023588
Iteration 9/1000 | Loss: 0.00003216
Iteration 10/1000 | Loss: 0.00002820
Iteration 11/1000 | Loss: 0.00002655
Iteration 12/1000 | Loss: 0.00020683
Iteration 13/1000 | Loss: 0.00010326
Iteration 14/1000 | Loss: 0.00002725
Iteration 15/1000 | Loss: 0.00003906
Iteration 16/1000 | Loss: 0.00003192
Iteration 17/1000 | Loss: 0.00002358
Iteration 18/1000 | Loss: 0.00002618
Iteration 19/1000 | Loss: 0.00002202
Iteration 20/1000 | Loss: 0.00002087
Iteration 21/1000 | Loss: 0.00002017
Iteration 22/1000 | Loss: 0.00001971
Iteration 23/1000 | Loss: 0.00001951
Iteration 24/1000 | Loss: 0.00001941
Iteration 25/1000 | Loss: 0.00001940
Iteration 26/1000 | Loss: 0.00001929
Iteration 27/1000 | Loss: 0.00001925
Iteration 28/1000 | Loss: 0.00001923
Iteration 29/1000 | Loss: 0.00001917
Iteration 30/1000 | Loss: 0.00001915
Iteration 31/1000 | Loss: 0.00001914
Iteration 32/1000 | Loss: 0.00001913
Iteration 33/1000 | Loss: 0.00001912
Iteration 34/1000 | Loss: 0.00001912
Iteration 35/1000 | Loss: 0.00001912
Iteration 36/1000 | Loss: 0.00001912
Iteration 37/1000 | Loss: 0.00001912
Iteration 38/1000 | Loss: 0.00001911
Iteration 39/1000 | Loss: 0.00001911
Iteration 40/1000 | Loss: 0.00001911
Iteration 41/1000 | Loss: 0.00001911
Iteration 42/1000 | Loss: 0.00001910
Iteration 43/1000 | Loss: 0.00001910
Iteration 44/1000 | Loss: 0.00001910
Iteration 45/1000 | Loss: 0.00001909
Iteration 46/1000 | Loss: 0.00001909
Iteration 47/1000 | Loss: 0.00001908
Iteration 48/1000 | Loss: 0.00001908
Iteration 49/1000 | Loss: 0.00001908
Iteration 50/1000 | Loss: 0.00001908
Iteration 51/1000 | Loss: 0.00001907
Iteration 52/1000 | Loss: 0.00006071
Iteration 53/1000 | Loss: 0.00001958
Iteration 54/1000 | Loss: 0.00002416
Iteration 55/1000 | Loss: 0.00001904
Iteration 56/1000 | Loss: 0.00001903
Iteration 57/1000 | Loss: 0.00001903
Iteration 58/1000 | Loss: 0.00001902
Iteration 59/1000 | Loss: 0.00001901
Iteration 60/1000 | Loss: 0.00001901
Iteration 61/1000 | Loss: 0.00001901
Iteration 62/1000 | Loss: 0.00001901
Iteration 63/1000 | Loss: 0.00001901
Iteration 64/1000 | Loss: 0.00001900
Iteration 65/1000 | Loss: 0.00001900
Iteration 66/1000 | Loss: 0.00001900
Iteration 67/1000 | Loss: 0.00001900
Iteration 68/1000 | Loss: 0.00001900
Iteration 69/1000 | Loss: 0.00001900
Iteration 70/1000 | Loss: 0.00001900
Iteration 71/1000 | Loss: 0.00001900
Iteration 72/1000 | Loss: 0.00001900
Iteration 73/1000 | Loss: 0.00001899
Iteration 74/1000 | Loss: 0.00001899
Iteration 75/1000 | Loss: 0.00001899
Iteration 76/1000 | Loss: 0.00001899
Iteration 77/1000 | Loss: 0.00001899
Iteration 78/1000 | Loss: 0.00001899
Iteration 79/1000 | Loss: 0.00001899
Iteration 80/1000 | Loss: 0.00001899
Iteration 81/1000 | Loss: 0.00001899
Iteration 82/1000 | Loss: 0.00001899
Iteration 83/1000 | Loss: 0.00001899
Iteration 84/1000 | Loss: 0.00001899
Iteration 85/1000 | Loss: 0.00001899
Iteration 86/1000 | Loss: 0.00001899
Iteration 87/1000 | Loss: 0.00001899
Iteration 88/1000 | Loss: 0.00001899
Iteration 89/1000 | Loss: 0.00001899
Iteration 90/1000 | Loss: 0.00001899
Iteration 91/1000 | Loss: 0.00001899
Iteration 92/1000 | Loss: 0.00001899
Iteration 93/1000 | Loss: 0.00001899
Iteration 94/1000 | Loss: 0.00001899
Iteration 95/1000 | Loss: 0.00001899
Iteration 96/1000 | Loss: 0.00001899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.898756636364851e-05, 1.898756636364851e-05, 1.898756636364851e-05, 1.898756636364851e-05, 1.898756636364851e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.898756636364851e-05

Optimization complete. Final v2v error: 3.6260175704956055 mm

Highest mean error: 4.269104480743408 mm for frame 86

Lowest mean error: 3.0990240573883057 mm for frame 5

Saving results

Total time: 72.62144041061401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784203
Iteration 2/25 | Loss: 0.00141360
Iteration 3/25 | Loss: 0.00132975
Iteration 4/25 | Loss: 0.00131697
Iteration 5/25 | Loss: 0.00131387
Iteration 6/25 | Loss: 0.00131368
Iteration 7/25 | Loss: 0.00131368
Iteration 8/25 | Loss: 0.00131368
Iteration 9/25 | Loss: 0.00131368
Iteration 10/25 | Loss: 0.00131368
Iteration 11/25 | Loss: 0.00131368
Iteration 12/25 | Loss: 0.00131368
Iteration 13/25 | Loss: 0.00131368
Iteration 14/25 | Loss: 0.00131368
Iteration 15/25 | Loss: 0.00131368
Iteration 16/25 | Loss: 0.00131368
Iteration 17/25 | Loss: 0.00131368
Iteration 18/25 | Loss: 0.00131368
Iteration 19/25 | Loss: 0.00131368
Iteration 20/25 | Loss: 0.00131368
Iteration 21/25 | Loss: 0.00131368
Iteration 22/25 | Loss: 0.00131368
Iteration 23/25 | Loss: 0.00131368
Iteration 24/25 | Loss: 0.00131368
Iteration 25/25 | Loss: 0.00131368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36525810
Iteration 2/25 | Loss: 0.00075487
Iteration 3/25 | Loss: 0.00075485
Iteration 4/25 | Loss: 0.00075485
Iteration 5/25 | Loss: 0.00075484
Iteration 6/25 | Loss: 0.00075484
Iteration 7/25 | Loss: 0.00075484
Iteration 8/25 | Loss: 0.00075484
Iteration 9/25 | Loss: 0.00075484
Iteration 10/25 | Loss: 0.00075484
Iteration 11/25 | Loss: 0.00075484
Iteration 12/25 | Loss: 0.00075484
Iteration 13/25 | Loss: 0.00075484
Iteration 14/25 | Loss: 0.00075484
Iteration 15/25 | Loss: 0.00075484
Iteration 16/25 | Loss: 0.00075484
Iteration 17/25 | Loss: 0.00075484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007548435823991895, 0.0007548435823991895, 0.0007548435823991895, 0.0007548435823991895, 0.0007548435823991895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007548435823991895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075484
Iteration 2/1000 | Loss: 0.00003457
Iteration 3/1000 | Loss: 0.00002496
Iteration 4/1000 | Loss: 0.00002289
Iteration 5/1000 | Loss: 0.00002176
Iteration 6/1000 | Loss: 0.00002079
Iteration 7/1000 | Loss: 0.00002008
Iteration 8/1000 | Loss: 0.00001966
Iteration 9/1000 | Loss: 0.00001930
Iteration 10/1000 | Loss: 0.00001892
Iteration 11/1000 | Loss: 0.00001869
Iteration 12/1000 | Loss: 0.00001868
Iteration 13/1000 | Loss: 0.00001861
Iteration 14/1000 | Loss: 0.00001843
Iteration 15/1000 | Loss: 0.00001827
Iteration 16/1000 | Loss: 0.00001824
Iteration 17/1000 | Loss: 0.00001818
Iteration 18/1000 | Loss: 0.00001808
Iteration 19/1000 | Loss: 0.00001801
Iteration 20/1000 | Loss: 0.00001796
Iteration 21/1000 | Loss: 0.00001795
Iteration 22/1000 | Loss: 0.00001795
Iteration 23/1000 | Loss: 0.00001795
Iteration 24/1000 | Loss: 0.00001794
Iteration 25/1000 | Loss: 0.00001794
Iteration 26/1000 | Loss: 0.00001793
Iteration 27/1000 | Loss: 0.00001792
Iteration 28/1000 | Loss: 0.00001787
Iteration 29/1000 | Loss: 0.00001785
Iteration 30/1000 | Loss: 0.00001785
Iteration 31/1000 | Loss: 0.00001781
Iteration 32/1000 | Loss: 0.00001780
Iteration 33/1000 | Loss: 0.00001779
Iteration 34/1000 | Loss: 0.00001777
Iteration 35/1000 | Loss: 0.00001775
Iteration 36/1000 | Loss: 0.00001774
Iteration 37/1000 | Loss: 0.00001773
Iteration 38/1000 | Loss: 0.00001773
Iteration 39/1000 | Loss: 0.00001773
Iteration 40/1000 | Loss: 0.00001772
Iteration 41/1000 | Loss: 0.00001772
Iteration 42/1000 | Loss: 0.00001772
Iteration 43/1000 | Loss: 0.00001770
Iteration 44/1000 | Loss: 0.00001769
Iteration 45/1000 | Loss: 0.00001768
Iteration 46/1000 | Loss: 0.00001768
Iteration 47/1000 | Loss: 0.00001767
Iteration 48/1000 | Loss: 0.00001767
Iteration 49/1000 | Loss: 0.00001766
Iteration 50/1000 | Loss: 0.00001764
Iteration 51/1000 | Loss: 0.00001764
Iteration 52/1000 | Loss: 0.00001764
Iteration 53/1000 | Loss: 0.00001763
Iteration 54/1000 | Loss: 0.00001763
Iteration 55/1000 | Loss: 0.00001763
Iteration 56/1000 | Loss: 0.00001763
Iteration 57/1000 | Loss: 0.00001763
Iteration 58/1000 | Loss: 0.00001763
Iteration 59/1000 | Loss: 0.00001763
Iteration 60/1000 | Loss: 0.00001762
Iteration 61/1000 | Loss: 0.00001762
Iteration 62/1000 | Loss: 0.00001762
Iteration 63/1000 | Loss: 0.00001762
Iteration 64/1000 | Loss: 0.00001762
Iteration 65/1000 | Loss: 0.00001762
Iteration 66/1000 | Loss: 0.00001762
Iteration 67/1000 | Loss: 0.00001762
Iteration 68/1000 | Loss: 0.00001762
Iteration 69/1000 | Loss: 0.00001762
Iteration 70/1000 | Loss: 0.00001762
Iteration 71/1000 | Loss: 0.00001762
Iteration 72/1000 | Loss: 0.00001762
Iteration 73/1000 | Loss: 0.00001761
Iteration 74/1000 | Loss: 0.00001761
Iteration 75/1000 | Loss: 0.00001761
Iteration 76/1000 | Loss: 0.00001761
Iteration 77/1000 | Loss: 0.00001761
Iteration 78/1000 | Loss: 0.00001761
Iteration 79/1000 | Loss: 0.00001761
Iteration 80/1000 | Loss: 0.00001761
Iteration 81/1000 | Loss: 0.00001761
Iteration 82/1000 | Loss: 0.00001760
Iteration 83/1000 | Loss: 0.00001760
Iteration 84/1000 | Loss: 0.00001760
Iteration 85/1000 | Loss: 0.00001760
Iteration 86/1000 | Loss: 0.00001759
Iteration 87/1000 | Loss: 0.00001759
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00001758
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001757
Iteration 92/1000 | Loss: 0.00001757
Iteration 93/1000 | Loss: 0.00001757
Iteration 94/1000 | Loss: 0.00001756
Iteration 95/1000 | Loss: 0.00001756
Iteration 96/1000 | Loss: 0.00001756
Iteration 97/1000 | Loss: 0.00001756
Iteration 98/1000 | Loss: 0.00001755
Iteration 99/1000 | Loss: 0.00001755
Iteration 100/1000 | Loss: 0.00001755
Iteration 101/1000 | Loss: 0.00001755
Iteration 102/1000 | Loss: 0.00001755
Iteration 103/1000 | Loss: 0.00001755
Iteration 104/1000 | Loss: 0.00001755
Iteration 105/1000 | Loss: 0.00001754
Iteration 106/1000 | Loss: 0.00001754
Iteration 107/1000 | Loss: 0.00001754
Iteration 108/1000 | Loss: 0.00001754
Iteration 109/1000 | Loss: 0.00001754
Iteration 110/1000 | Loss: 0.00001754
Iteration 111/1000 | Loss: 0.00001754
Iteration 112/1000 | Loss: 0.00001754
Iteration 113/1000 | Loss: 0.00001754
Iteration 114/1000 | Loss: 0.00001754
Iteration 115/1000 | Loss: 0.00001754
Iteration 116/1000 | Loss: 0.00001754
Iteration 117/1000 | Loss: 0.00001754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.753855394781567e-05, 1.753855394781567e-05, 1.753855394781567e-05, 1.753855394781567e-05, 1.753855394781567e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.753855394781567e-05

Optimization complete. Final v2v error: 3.516583204269409 mm

Highest mean error: 3.738938093185425 mm for frame 67

Lowest mean error: 3.2931246757507324 mm for frame 0

Saving results

Total time: 40.7588996887207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436591
Iteration 2/25 | Loss: 0.00137926
Iteration 3/25 | Loss: 0.00130517
Iteration 4/25 | Loss: 0.00129223
Iteration 5/25 | Loss: 0.00128753
Iteration 6/25 | Loss: 0.00128648
Iteration 7/25 | Loss: 0.00128648
Iteration 8/25 | Loss: 0.00128648
Iteration 9/25 | Loss: 0.00128648
Iteration 10/25 | Loss: 0.00128648
Iteration 11/25 | Loss: 0.00128648
Iteration 12/25 | Loss: 0.00128648
Iteration 13/25 | Loss: 0.00128648
Iteration 14/25 | Loss: 0.00128648
Iteration 15/25 | Loss: 0.00128648
Iteration 16/25 | Loss: 0.00128648
Iteration 17/25 | Loss: 0.00128648
Iteration 18/25 | Loss: 0.00128648
Iteration 19/25 | Loss: 0.00128648
Iteration 20/25 | Loss: 0.00128648
Iteration 21/25 | Loss: 0.00128648
Iteration 22/25 | Loss: 0.00128648
Iteration 23/25 | Loss: 0.00128648
Iteration 24/25 | Loss: 0.00128648
Iteration 25/25 | Loss: 0.00128648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88623500
Iteration 2/25 | Loss: 0.00091461
Iteration 3/25 | Loss: 0.00091459
Iteration 4/25 | Loss: 0.00091459
Iteration 5/25 | Loss: 0.00091459
Iteration 6/25 | Loss: 0.00091459
Iteration 7/25 | Loss: 0.00091459
Iteration 8/25 | Loss: 0.00091459
Iteration 9/25 | Loss: 0.00091459
Iteration 10/25 | Loss: 0.00091459
Iteration 11/25 | Loss: 0.00091459
Iteration 12/25 | Loss: 0.00091459
Iteration 13/25 | Loss: 0.00091459
Iteration 14/25 | Loss: 0.00091459
Iteration 15/25 | Loss: 0.00091459
Iteration 16/25 | Loss: 0.00091459
Iteration 17/25 | Loss: 0.00091459
Iteration 18/25 | Loss: 0.00091459
Iteration 19/25 | Loss: 0.00091459
Iteration 20/25 | Loss: 0.00091459
Iteration 21/25 | Loss: 0.00091459
Iteration 22/25 | Loss: 0.00091459
Iteration 23/25 | Loss: 0.00091459
Iteration 24/25 | Loss: 0.00091459
Iteration 25/25 | Loss: 0.00091459

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091459
Iteration 2/1000 | Loss: 0.00003636
Iteration 3/1000 | Loss: 0.00002294
Iteration 4/1000 | Loss: 0.00001932
Iteration 5/1000 | Loss: 0.00001827
Iteration 6/1000 | Loss: 0.00001740
Iteration 7/1000 | Loss: 0.00001691
Iteration 8/1000 | Loss: 0.00001631
Iteration 9/1000 | Loss: 0.00001603
Iteration 10/1000 | Loss: 0.00001568
Iteration 11/1000 | Loss: 0.00001541
Iteration 12/1000 | Loss: 0.00001528
Iteration 13/1000 | Loss: 0.00001511
Iteration 14/1000 | Loss: 0.00001506
Iteration 15/1000 | Loss: 0.00001495
Iteration 16/1000 | Loss: 0.00001494
Iteration 17/1000 | Loss: 0.00001493
Iteration 18/1000 | Loss: 0.00001492
Iteration 19/1000 | Loss: 0.00001490
Iteration 20/1000 | Loss: 0.00001490
Iteration 21/1000 | Loss: 0.00001490
Iteration 22/1000 | Loss: 0.00001489
Iteration 23/1000 | Loss: 0.00001488
Iteration 24/1000 | Loss: 0.00001488
Iteration 25/1000 | Loss: 0.00001488
Iteration 26/1000 | Loss: 0.00001487
Iteration 27/1000 | Loss: 0.00001487
Iteration 28/1000 | Loss: 0.00001486
Iteration 29/1000 | Loss: 0.00001486
Iteration 30/1000 | Loss: 0.00001485
Iteration 31/1000 | Loss: 0.00001485
Iteration 32/1000 | Loss: 0.00001482
Iteration 33/1000 | Loss: 0.00001482
Iteration 34/1000 | Loss: 0.00001482
Iteration 35/1000 | Loss: 0.00001482
Iteration 36/1000 | Loss: 0.00001482
Iteration 37/1000 | Loss: 0.00001482
Iteration 38/1000 | Loss: 0.00001482
Iteration 39/1000 | Loss: 0.00001482
Iteration 40/1000 | Loss: 0.00001482
Iteration 41/1000 | Loss: 0.00001481
Iteration 42/1000 | Loss: 0.00001481
Iteration 43/1000 | Loss: 0.00001481
Iteration 44/1000 | Loss: 0.00001481
Iteration 45/1000 | Loss: 0.00001481
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001481
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001478
Iteration 51/1000 | Loss: 0.00001478
Iteration 52/1000 | Loss: 0.00001477
Iteration 53/1000 | Loss: 0.00001477
Iteration 54/1000 | Loss: 0.00001476
Iteration 55/1000 | Loss: 0.00001476
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001475
Iteration 58/1000 | Loss: 0.00001474
Iteration 59/1000 | Loss: 0.00001474
Iteration 60/1000 | Loss: 0.00001474
Iteration 61/1000 | Loss: 0.00001473
Iteration 62/1000 | Loss: 0.00001473
Iteration 63/1000 | Loss: 0.00001472
Iteration 64/1000 | Loss: 0.00001472
Iteration 65/1000 | Loss: 0.00001472
Iteration 66/1000 | Loss: 0.00001472
Iteration 67/1000 | Loss: 0.00001472
Iteration 68/1000 | Loss: 0.00001471
Iteration 69/1000 | Loss: 0.00001471
Iteration 70/1000 | Loss: 0.00001471
Iteration 71/1000 | Loss: 0.00001471
Iteration 72/1000 | Loss: 0.00001470
Iteration 73/1000 | Loss: 0.00001470
Iteration 74/1000 | Loss: 0.00001470
Iteration 75/1000 | Loss: 0.00001470
Iteration 76/1000 | Loss: 0.00001469
Iteration 77/1000 | Loss: 0.00001469
Iteration 78/1000 | Loss: 0.00001469
Iteration 79/1000 | Loss: 0.00001468
Iteration 80/1000 | Loss: 0.00001468
Iteration 81/1000 | Loss: 0.00001468
Iteration 82/1000 | Loss: 0.00001467
Iteration 83/1000 | Loss: 0.00001467
Iteration 84/1000 | Loss: 0.00001466
Iteration 85/1000 | Loss: 0.00001466
Iteration 86/1000 | Loss: 0.00001466
Iteration 87/1000 | Loss: 0.00001465
Iteration 88/1000 | Loss: 0.00001465
Iteration 89/1000 | Loss: 0.00001464
Iteration 90/1000 | Loss: 0.00001464
Iteration 91/1000 | Loss: 0.00001464
Iteration 92/1000 | Loss: 0.00001463
Iteration 93/1000 | Loss: 0.00001463
Iteration 94/1000 | Loss: 0.00001463
Iteration 95/1000 | Loss: 0.00001463
Iteration 96/1000 | Loss: 0.00001462
Iteration 97/1000 | Loss: 0.00001462
Iteration 98/1000 | Loss: 0.00001462
Iteration 99/1000 | Loss: 0.00001461
Iteration 100/1000 | Loss: 0.00001461
Iteration 101/1000 | Loss: 0.00001461
Iteration 102/1000 | Loss: 0.00001461
Iteration 103/1000 | Loss: 0.00001461
Iteration 104/1000 | Loss: 0.00001461
Iteration 105/1000 | Loss: 0.00001461
Iteration 106/1000 | Loss: 0.00001461
Iteration 107/1000 | Loss: 0.00001461
Iteration 108/1000 | Loss: 0.00001460
Iteration 109/1000 | Loss: 0.00001460
Iteration 110/1000 | Loss: 0.00001460
Iteration 111/1000 | Loss: 0.00001460
Iteration 112/1000 | Loss: 0.00001460
Iteration 113/1000 | Loss: 0.00001459
Iteration 114/1000 | Loss: 0.00001459
Iteration 115/1000 | Loss: 0.00001459
Iteration 116/1000 | Loss: 0.00001458
Iteration 117/1000 | Loss: 0.00001458
Iteration 118/1000 | Loss: 0.00001458
Iteration 119/1000 | Loss: 0.00001458
Iteration 120/1000 | Loss: 0.00001457
Iteration 121/1000 | Loss: 0.00001457
Iteration 122/1000 | Loss: 0.00001457
Iteration 123/1000 | Loss: 0.00001457
Iteration 124/1000 | Loss: 0.00001457
Iteration 125/1000 | Loss: 0.00001457
Iteration 126/1000 | Loss: 0.00001456
Iteration 127/1000 | Loss: 0.00001456
Iteration 128/1000 | Loss: 0.00001456
Iteration 129/1000 | Loss: 0.00001455
Iteration 130/1000 | Loss: 0.00001455
Iteration 131/1000 | Loss: 0.00001455
Iteration 132/1000 | Loss: 0.00001455
Iteration 133/1000 | Loss: 0.00001455
Iteration 134/1000 | Loss: 0.00001454
Iteration 135/1000 | Loss: 0.00001454
Iteration 136/1000 | Loss: 0.00001454
Iteration 137/1000 | Loss: 0.00001453
Iteration 138/1000 | Loss: 0.00001453
Iteration 139/1000 | Loss: 0.00001453
Iteration 140/1000 | Loss: 0.00001452
Iteration 141/1000 | Loss: 0.00001452
Iteration 142/1000 | Loss: 0.00001452
Iteration 143/1000 | Loss: 0.00001452
Iteration 144/1000 | Loss: 0.00001452
Iteration 145/1000 | Loss: 0.00001452
Iteration 146/1000 | Loss: 0.00001452
Iteration 147/1000 | Loss: 0.00001452
Iteration 148/1000 | Loss: 0.00001451
Iteration 149/1000 | Loss: 0.00001451
Iteration 150/1000 | Loss: 0.00001451
Iteration 151/1000 | Loss: 0.00001451
Iteration 152/1000 | Loss: 0.00001451
Iteration 153/1000 | Loss: 0.00001451
Iteration 154/1000 | Loss: 0.00001450
Iteration 155/1000 | Loss: 0.00001450
Iteration 156/1000 | Loss: 0.00001450
Iteration 157/1000 | Loss: 0.00001450
Iteration 158/1000 | Loss: 0.00001450
Iteration 159/1000 | Loss: 0.00001450
Iteration 160/1000 | Loss: 0.00001450
Iteration 161/1000 | Loss: 0.00001450
Iteration 162/1000 | Loss: 0.00001449
Iteration 163/1000 | Loss: 0.00001449
Iteration 164/1000 | Loss: 0.00001449
Iteration 165/1000 | Loss: 0.00001449
Iteration 166/1000 | Loss: 0.00001449
Iteration 167/1000 | Loss: 0.00001449
Iteration 168/1000 | Loss: 0.00001448
Iteration 169/1000 | Loss: 0.00001448
Iteration 170/1000 | Loss: 0.00001448
Iteration 171/1000 | Loss: 0.00001448
Iteration 172/1000 | Loss: 0.00001448
Iteration 173/1000 | Loss: 0.00001448
Iteration 174/1000 | Loss: 0.00001448
Iteration 175/1000 | Loss: 0.00001448
Iteration 176/1000 | Loss: 0.00001448
Iteration 177/1000 | Loss: 0.00001448
Iteration 178/1000 | Loss: 0.00001448
Iteration 179/1000 | Loss: 0.00001448
Iteration 180/1000 | Loss: 0.00001447
Iteration 181/1000 | Loss: 0.00001447
Iteration 182/1000 | Loss: 0.00001447
Iteration 183/1000 | Loss: 0.00001447
Iteration 184/1000 | Loss: 0.00001447
Iteration 185/1000 | Loss: 0.00001447
Iteration 186/1000 | Loss: 0.00001447
Iteration 187/1000 | Loss: 0.00001447
Iteration 188/1000 | Loss: 0.00001447
Iteration 189/1000 | Loss: 0.00001447
Iteration 190/1000 | Loss: 0.00001447
Iteration 191/1000 | Loss: 0.00001447
Iteration 192/1000 | Loss: 0.00001447
Iteration 193/1000 | Loss: 0.00001446
Iteration 194/1000 | Loss: 0.00001446
Iteration 195/1000 | Loss: 0.00001446
Iteration 196/1000 | Loss: 0.00001446
Iteration 197/1000 | Loss: 0.00001446
Iteration 198/1000 | Loss: 0.00001446
Iteration 199/1000 | Loss: 0.00001446
Iteration 200/1000 | Loss: 0.00001446
Iteration 201/1000 | Loss: 0.00001446
Iteration 202/1000 | Loss: 0.00001446
Iteration 203/1000 | Loss: 0.00001446
Iteration 204/1000 | Loss: 0.00001446
Iteration 205/1000 | Loss: 0.00001446
Iteration 206/1000 | Loss: 0.00001446
Iteration 207/1000 | Loss: 0.00001446
Iteration 208/1000 | Loss: 0.00001446
Iteration 209/1000 | Loss: 0.00001445
Iteration 210/1000 | Loss: 0.00001445
Iteration 211/1000 | Loss: 0.00001445
Iteration 212/1000 | Loss: 0.00001445
Iteration 213/1000 | Loss: 0.00001445
Iteration 214/1000 | Loss: 0.00001445
Iteration 215/1000 | Loss: 0.00001445
Iteration 216/1000 | Loss: 0.00001445
Iteration 217/1000 | Loss: 0.00001445
Iteration 218/1000 | Loss: 0.00001445
Iteration 219/1000 | Loss: 0.00001445
Iteration 220/1000 | Loss: 0.00001445
Iteration 221/1000 | Loss: 0.00001445
Iteration 222/1000 | Loss: 0.00001445
Iteration 223/1000 | Loss: 0.00001445
Iteration 224/1000 | Loss: 0.00001445
Iteration 225/1000 | Loss: 0.00001445
Iteration 226/1000 | Loss: 0.00001445
Iteration 227/1000 | Loss: 0.00001445
Iteration 228/1000 | Loss: 0.00001445
Iteration 229/1000 | Loss: 0.00001445
Iteration 230/1000 | Loss: 0.00001445
Iteration 231/1000 | Loss: 0.00001445
Iteration 232/1000 | Loss: 0.00001445
Iteration 233/1000 | Loss: 0.00001445
Iteration 234/1000 | Loss: 0.00001445
Iteration 235/1000 | Loss: 0.00001445
Iteration 236/1000 | Loss: 0.00001445
Iteration 237/1000 | Loss: 0.00001445
Iteration 238/1000 | Loss: 0.00001445
Iteration 239/1000 | Loss: 0.00001445
Iteration 240/1000 | Loss: 0.00001445
Iteration 241/1000 | Loss: 0.00001445
Iteration 242/1000 | Loss: 0.00001445
Iteration 243/1000 | Loss: 0.00001445
Iteration 244/1000 | Loss: 0.00001445
Iteration 245/1000 | Loss: 0.00001445
Iteration 246/1000 | Loss: 0.00001445
Iteration 247/1000 | Loss: 0.00001445
Iteration 248/1000 | Loss: 0.00001445
Iteration 249/1000 | Loss: 0.00001445
Iteration 250/1000 | Loss: 0.00001445
Iteration 251/1000 | Loss: 0.00001445
Iteration 252/1000 | Loss: 0.00001445
Iteration 253/1000 | Loss: 0.00001445
Iteration 254/1000 | Loss: 0.00001445
Iteration 255/1000 | Loss: 0.00001445
Iteration 256/1000 | Loss: 0.00001445
Iteration 257/1000 | Loss: 0.00001445
Iteration 258/1000 | Loss: 0.00001445
Iteration 259/1000 | Loss: 0.00001445
Iteration 260/1000 | Loss: 0.00001445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.4445194210566115e-05, 1.4445194210566115e-05, 1.4445194210566115e-05, 1.4445194210566115e-05, 1.4445194210566115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4445194210566115e-05

Optimization complete. Final v2v error: 3.236563205718994 mm

Highest mean error: 3.6312255859375 mm for frame 62

Lowest mean error: 2.8505492210388184 mm for frame 130

Saving results

Total time: 44.843525886535645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790249
Iteration 2/25 | Loss: 0.00156178
Iteration 3/25 | Loss: 0.00142700
Iteration 4/25 | Loss: 0.00140579
Iteration 5/25 | Loss: 0.00139801
Iteration 6/25 | Loss: 0.00139581
Iteration 7/25 | Loss: 0.00139578
Iteration 8/25 | Loss: 0.00139578
Iteration 9/25 | Loss: 0.00139578
Iteration 10/25 | Loss: 0.00139578
Iteration 11/25 | Loss: 0.00139578
Iteration 12/25 | Loss: 0.00139578
Iteration 13/25 | Loss: 0.00139578
Iteration 14/25 | Loss: 0.00139578
Iteration 15/25 | Loss: 0.00139578
Iteration 16/25 | Loss: 0.00139578
Iteration 17/25 | Loss: 0.00139578
Iteration 18/25 | Loss: 0.00139578
Iteration 19/25 | Loss: 0.00139578
Iteration 20/25 | Loss: 0.00139578
Iteration 21/25 | Loss: 0.00139578
Iteration 22/25 | Loss: 0.00139578
Iteration 23/25 | Loss: 0.00139578
Iteration 24/25 | Loss: 0.00139578
Iteration 25/25 | Loss: 0.00139578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11787677
Iteration 2/25 | Loss: 0.00114742
Iteration 3/25 | Loss: 0.00114742
Iteration 4/25 | Loss: 0.00114741
Iteration 5/25 | Loss: 0.00114741
Iteration 6/25 | Loss: 0.00114741
Iteration 7/25 | Loss: 0.00114741
Iteration 8/25 | Loss: 0.00114741
Iteration 9/25 | Loss: 0.00114741
Iteration 10/25 | Loss: 0.00114741
Iteration 11/25 | Loss: 0.00114741
Iteration 12/25 | Loss: 0.00114741
Iteration 13/25 | Loss: 0.00114741
Iteration 14/25 | Loss: 0.00114741
Iteration 15/25 | Loss: 0.00114741
Iteration 16/25 | Loss: 0.00114741
Iteration 17/25 | Loss: 0.00114741
Iteration 18/25 | Loss: 0.00114741
Iteration 19/25 | Loss: 0.00114741
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011474130442366004, 0.0011474130442366004, 0.0011474130442366004, 0.0011474130442366004, 0.0011474130442366004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011474130442366004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114741
Iteration 2/1000 | Loss: 0.00005990
Iteration 3/1000 | Loss: 0.00003343
Iteration 4/1000 | Loss: 0.00002903
Iteration 5/1000 | Loss: 0.00002692
Iteration 6/1000 | Loss: 0.00002555
Iteration 7/1000 | Loss: 0.00002472
Iteration 8/1000 | Loss: 0.00002395
Iteration 9/1000 | Loss: 0.00002358
Iteration 10/1000 | Loss: 0.00002318
Iteration 11/1000 | Loss: 0.00002292
Iteration 12/1000 | Loss: 0.00002268
Iteration 13/1000 | Loss: 0.00002268
Iteration 14/1000 | Loss: 0.00002258
Iteration 15/1000 | Loss: 0.00002250
Iteration 16/1000 | Loss: 0.00002241
Iteration 17/1000 | Loss: 0.00002236
Iteration 18/1000 | Loss: 0.00002236
Iteration 19/1000 | Loss: 0.00002235
Iteration 20/1000 | Loss: 0.00002230
Iteration 21/1000 | Loss: 0.00002225
Iteration 22/1000 | Loss: 0.00002225
Iteration 23/1000 | Loss: 0.00002224
Iteration 24/1000 | Loss: 0.00002224
Iteration 25/1000 | Loss: 0.00002223
Iteration 26/1000 | Loss: 0.00002222
Iteration 27/1000 | Loss: 0.00002220
Iteration 28/1000 | Loss: 0.00002220
Iteration 29/1000 | Loss: 0.00002217
Iteration 30/1000 | Loss: 0.00002217
Iteration 31/1000 | Loss: 0.00002215
Iteration 32/1000 | Loss: 0.00002215
Iteration 33/1000 | Loss: 0.00002215
Iteration 34/1000 | Loss: 0.00002214
Iteration 35/1000 | Loss: 0.00002214
Iteration 36/1000 | Loss: 0.00002214
Iteration 37/1000 | Loss: 0.00002214
Iteration 38/1000 | Loss: 0.00002213
Iteration 39/1000 | Loss: 0.00002213
Iteration 40/1000 | Loss: 0.00002213
Iteration 41/1000 | Loss: 0.00002212
Iteration 42/1000 | Loss: 0.00002212
Iteration 43/1000 | Loss: 0.00002211
Iteration 44/1000 | Loss: 0.00002211
Iteration 45/1000 | Loss: 0.00002211
Iteration 46/1000 | Loss: 0.00002211
Iteration 47/1000 | Loss: 0.00002210
Iteration 48/1000 | Loss: 0.00002210
Iteration 49/1000 | Loss: 0.00002210
Iteration 50/1000 | Loss: 0.00002210
Iteration 51/1000 | Loss: 0.00002209
Iteration 52/1000 | Loss: 0.00002209
Iteration 53/1000 | Loss: 0.00002209
Iteration 54/1000 | Loss: 0.00002208
Iteration 55/1000 | Loss: 0.00002208
Iteration 56/1000 | Loss: 0.00002208
Iteration 57/1000 | Loss: 0.00002208
Iteration 58/1000 | Loss: 0.00002208
Iteration 59/1000 | Loss: 0.00002208
Iteration 60/1000 | Loss: 0.00002208
Iteration 61/1000 | Loss: 0.00002208
Iteration 62/1000 | Loss: 0.00002208
Iteration 63/1000 | Loss: 0.00002208
Iteration 64/1000 | Loss: 0.00002208
Iteration 65/1000 | Loss: 0.00002208
Iteration 66/1000 | Loss: 0.00002208
Iteration 67/1000 | Loss: 0.00002208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [2.2082274881540798e-05, 2.2082274881540798e-05, 2.2082274881540798e-05, 2.2082274881540798e-05, 2.2082274881540798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2082274881540798e-05

Optimization complete. Final v2v error: 3.9841361045837402 mm

Highest mean error: 4.569608211517334 mm for frame 108

Lowest mean error: 3.4704902172088623 mm for frame 214

Saving results

Total time: 39.9916775226593
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404525
Iteration 2/25 | Loss: 0.00132184
Iteration 3/25 | Loss: 0.00126289
Iteration 4/25 | Loss: 0.00125634
Iteration 5/25 | Loss: 0.00125482
Iteration 6/25 | Loss: 0.00125482
Iteration 7/25 | Loss: 0.00125482
Iteration 8/25 | Loss: 0.00125482
Iteration 9/25 | Loss: 0.00125482
Iteration 10/25 | Loss: 0.00125482
Iteration 11/25 | Loss: 0.00125482
Iteration 12/25 | Loss: 0.00125482
Iteration 13/25 | Loss: 0.00125482
Iteration 14/25 | Loss: 0.00125482
Iteration 15/25 | Loss: 0.00125482
Iteration 16/25 | Loss: 0.00125482
Iteration 17/25 | Loss: 0.00125482
Iteration 18/25 | Loss: 0.00125482
Iteration 19/25 | Loss: 0.00125482
Iteration 20/25 | Loss: 0.00125482
Iteration 21/25 | Loss: 0.00125482
Iteration 22/25 | Loss: 0.00125482
Iteration 23/25 | Loss: 0.00125482
Iteration 24/25 | Loss: 0.00125482
Iteration 25/25 | Loss: 0.00125482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06343460
Iteration 2/25 | Loss: 0.00081214
Iteration 3/25 | Loss: 0.00081213
Iteration 4/25 | Loss: 0.00081213
Iteration 5/25 | Loss: 0.00081213
Iteration 6/25 | Loss: 0.00081213
Iteration 7/25 | Loss: 0.00081213
Iteration 8/25 | Loss: 0.00081213
Iteration 9/25 | Loss: 0.00081213
Iteration 10/25 | Loss: 0.00081213
Iteration 11/25 | Loss: 0.00081213
Iteration 12/25 | Loss: 0.00081213
Iteration 13/25 | Loss: 0.00081213
Iteration 14/25 | Loss: 0.00081213
Iteration 15/25 | Loss: 0.00081213
Iteration 16/25 | Loss: 0.00081213
Iteration 17/25 | Loss: 0.00081213
Iteration 18/25 | Loss: 0.00081213
Iteration 19/25 | Loss: 0.00081213
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008121261489577591, 0.0008121261489577591, 0.0008121261489577591, 0.0008121261489577591, 0.0008121261489577591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008121261489577591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081213
Iteration 2/1000 | Loss: 0.00002256
Iteration 3/1000 | Loss: 0.00001587
Iteration 4/1000 | Loss: 0.00001423
Iteration 5/1000 | Loss: 0.00001336
Iteration 6/1000 | Loss: 0.00001293
Iteration 7/1000 | Loss: 0.00001256
Iteration 8/1000 | Loss: 0.00001254
Iteration 9/1000 | Loss: 0.00001225
Iteration 10/1000 | Loss: 0.00001198
Iteration 11/1000 | Loss: 0.00001197
Iteration 12/1000 | Loss: 0.00001187
Iteration 13/1000 | Loss: 0.00001175
Iteration 14/1000 | Loss: 0.00001172
Iteration 15/1000 | Loss: 0.00001171
Iteration 16/1000 | Loss: 0.00001170
Iteration 17/1000 | Loss: 0.00001170
Iteration 18/1000 | Loss: 0.00001162
Iteration 19/1000 | Loss: 0.00001158
Iteration 20/1000 | Loss: 0.00001157
Iteration 21/1000 | Loss: 0.00001156
Iteration 22/1000 | Loss: 0.00001155
Iteration 23/1000 | Loss: 0.00001155
Iteration 24/1000 | Loss: 0.00001154
Iteration 25/1000 | Loss: 0.00001154
Iteration 26/1000 | Loss: 0.00001153
Iteration 27/1000 | Loss: 0.00001152
Iteration 28/1000 | Loss: 0.00001151
Iteration 29/1000 | Loss: 0.00001150
Iteration 30/1000 | Loss: 0.00001149
Iteration 31/1000 | Loss: 0.00001149
Iteration 32/1000 | Loss: 0.00001148
Iteration 33/1000 | Loss: 0.00001148
Iteration 34/1000 | Loss: 0.00001147
Iteration 35/1000 | Loss: 0.00001147
Iteration 36/1000 | Loss: 0.00001146
Iteration 37/1000 | Loss: 0.00001146
Iteration 38/1000 | Loss: 0.00001146
Iteration 39/1000 | Loss: 0.00001146
Iteration 40/1000 | Loss: 0.00001146
Iteration 41/1000 | Loss: 0.00001145
Iteration 42/1000 | Loss: 0.00001145
Iteration 43/1000 | Loss: 0.00001145
Iteration 44/1000 | Loss: 0.00001144
Iteration 45/1000 | Loss: 0.00001143
Iteration 46/1000 | Loss: 0.00001142
Iteration 47/1000 | Loss: 0.00001142
Iteration 48/1000 | Loss: 0.00001141
Iteration 49/1000 | Loss: 0.00001141
Iteration 50/1000 | Loss: 0.00001141
Iteration 51/1000 | Loss: 0.00001140
Iteration 52/1000 | Loss: 0.00001134
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001133
Iteration 55/1000 | Loss: 0.00001132
Iteration 56/1000 | Loss: 0.00001131
Iteration 57/1000 | Loss: 0.00001130
Iteration 58/1000 | Loss: 0.00001130
Iteration 59/1000 | Loss: 0.00001128
Iteration 60/1000 | Loss: 0.00001128
Iteration 61/1000 | Loss: 0.00001127
Iteration 62/1000 | Loss: 0.00001127
Iteration 63/1000 | Loss: 0.00001126
Iteration 64/1000 | Loss: 0.00001126
Iteration 65/1000 | Loss: 0.00001126
Iteration 66/1000 | Loss: 0.00001126
Iteration 67/1000 | Loss: 0.00001126
Iteration 68/1000 | Loss: 0.00001125
Iteration 69/1000 | Loss: 0.00001125
Iteration 70/1000 | Loss: 0.00001125
Iteration 71/1000 | Loss: 0.00001125
Iteration 72/1000 | Loss: 0.00001125
Iteration 73/1000 | Loss: 0.00001124
Iteration 74/1000 | Loss: 0.00001124
Iteration 75/1000 | Loss: 0.00001123
Iteration 76/1000 | Loss: 0.00001123
Iteration 77/1000 | Loss: 0.00001123
Iteration 78/1000 | Loss: 0.00001123
Iteration 79/1000 | Loss: 0.00001123
Iteration 80/1000 | Loss: 0.00001123
Iteration 81/1000 | Loss: 0.00001123
Iteration 82/1000 | Loss: 0.00001123
Iteration 83/1000 | Loss: 0.00001123
Iteration 84/1000 | Loss: 0.00001123
Iteration 85/1000 | Loss: 0.00001122
Iteration 86/1000 | Loss: 0.00001122
Iteration 87/1000 | Loss: 0.00001122
Iteration 88/1000 | Loss: 0.00001121
Iteration 89/1000 | Loss: 0.00001121
Iteration 90/1000 | Loss: 0.00001120
Iteration 91/1000 | Loss: 0.00001120
Iteration 92/1000 | Loss: 0.00001120
Iteration 93/1000 | Loss: 0.00001119
Iteration 94/1000 | Loss: 0.00001119
Iteration 95/1000 | Loss: 0.00001118
Iteration 96/1000 | Loss: 0.00001117
Iteration 97/1000 | Loss: 0.00001117
Iteration 98/1000 | Loss: 0.00001116
Iteration 99/1000 | Loss: 0.00001116
Iteration 100/1000 | Loss: 0.00001116
Iteration 101/1000 | Loss: 0.00001116
Iteration 102/1000 | Loss: 0.00001115
Iteration 103/1000 | Loss: 0.00001115
Iteration 104/1000 | Loss: 0.00001115
Iteration 105/1000 | Loss: 0.00001115
Iteration 106/1000 | Loss: 0.00001114
Iteration 107/1000 | Loss: 0.00001114
Iteration 108/1000 | Loss: 0.00001114
Iteration 109/1000 | Loss: 0.00001114
Iteration 110/1000 | Loss: 0.00001114
Iteration 111/1000 | Loss: 0.00001113
Iteration 112/1000 | Loss: 0.00001113
Iteration 113/1000 | Loss: 0.00001113
Iteration 114/1000 | Loss: 0.00001112
Iteration 115/1000 | Loss: 0.00001112
Iteration 116/1000 | Loss: 0.00001112
Iteration 117/1000 | Loss: 0.00001112
Iteration 118/1000 | Loss: 0.00001111
Iteration 119/1000 | Loss: 0.00001111
Iteration 120/1000 | Loss: 0.00001111
Iteration 121/1000 | Loss: 0.00001111
Iteration 122/1000 | Loss: 0.00001111
Iteration 123/1000 | Loss: 0.00001111
Iteration 124/1000 | Loss: 0.00001110
Iteration 125/1000 | Loss: 0.00001110
Iteration 126/1000 | Loss: 0.00001110
Iteration 127/1000 | Loss: 0.00001110
Iteration 128/1000 | Loss: 0.00001110
Iteration 129/1000 | Loss: 0.00001110
Iteration 130/1000 | Loss: 0.00001110
Iteration 131/1000 | Loss: 0.00001110
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001109
Iteration 134/1000 | Loss: 0.00001109
Iteration 135/1000 | Loss: 0.00001109
Iteration 136/1000 | Loss: 0.00001109
Iteration 137/1000 | Loss: 0.00001109
Iteration 138/1000 | Loss: 0.00001109
Iteration 139/1000 | Loss: 0.00001109
Iteration 140/1000 | Loss: 0.00001109
Iteration 141/1000 | Loss: 0.00001109
Iteration 142/1000 | Loss: 0.00001109
Iteration 143/1000 | Loss: 0.00001109
Iteration 144/1000 | Loss: 0.00001109
Iteration 145/1000 | Loss: 0.00001109
Iteration 146/1000 | Loss: 0.00001109
Iteration 147/1000 | Loss: 0.00001109
Iteration 148/1000 | Loss: 0.00001109
Iteration 149/1000 | Loss: 0.00001109
Iteration 150/1000 | Loss: 0.00001109
Iteration 151/1000 | Loss: 0.00001109
Iteration 152/1000 | Loss: 0.00001109
Iteration 153/1000 | Loss: 0.00001109
Iteration 154/1000 | Loss: 0.00001109
Iteration 155/1000 | Loss: 0.00001108
Iteration 156/1000 | Loss: 0.00001108
Iteration 157/1000 | Loss: 0.00001108
Iteration 158/1000 | Loss: 0.00001108
Iteration 159/1000 | Loss: 0.00001108
Iteration 160/1000 | Loss: 0.00001108
Iteration 161/1000 | Loss: 0.00001108
Iteration 162/1000 | Loss: 0.00001108
Iteration 163/1000 | Loss: 0.00001108
Iteration 164/1000 | Loss: 0.00001107
Iteration 165/1000 | Loss: 0.00001107
Iteration 166/1000 | Loss: 0.00001107
Iteration 167/1000 | Loss: 0.00001107
Iteration 168/1000 | Loss: 0.00001107
Iteration 169/1000 | Loss: 0.00001107
Iteration 170/1000 | Loss: 0.00001106
Iteration 171/1000 | Loss: 0.00001106
Iteration 172/1000 | Loss: 0.00001106
Iteration 173/1000 | Loss: 0.00001106
Iteration 174/1000 | Loss: 0.00001106
Iteration 175/1000 | Loss: 0.00001105
Iteration 176/1000 | Loss: 0.00001105
Iteration 177/1000 | Loss: 0.00001105
Iteration 178/1000 | Loss: 0.00001104
Iteration 179/1000 | Loss: 0.00001104
Iteration 180/1000 | Loss: 0.00001104
Iteration 181/1000 | Loss: 0.00001104
Iteration 182/1000 | Loss: 0.00001104
Iteration 183/1000 | Loss: 0.00001104
Iteration 184/1000 | Loss: 0.00001104
Iteration 185/1000 | Loss: 0.00001104
Iteration 186/1000 | Loss: 0.00001104
Iteration 187/1000 | Loss: 0.00001104
Iteration 188/1000 | Loss: 0.00001103
Iteration 189/1000 | Loss: 0.00001103
Iteration 190/1000 | Loss: 0.00001103
Iteration 191/1000 | Loss: 0.00001103
Iteration 192/1000 | Loss: 0.00001103
Iteration 193/1000 | Loss: 0.00001103
Iteration 194/1000 | Loss: 0.00001103
Iteration 195/1000 | Loss: 0.00001103
Iteration 196/1000 | Loss: 0.00001103
Iteration 197/1000 | Loss: 0.00001103
Iteration 198/1000 | Loss: 0.00001102
Iteration 199/1000 | Loss: 0.00001102
Iteration 200/1000 | Loss: 0.00001102
Iteration 201/1000 | Loss: 0.00001102
Iteration 202/1000 | Loss: 0.00001102
Iteration 203/1000 | Loss: 0.00001102
Iteration 204/1000 | Loss: 0.00001102
Iteration 205/1000 | Loss: 0.00001102
Iteration 206/1000 | Loss: 0.00001102
Iteration 207/1000 | Loss: 0.00001102
Iteration 208/1000 | Loss: 0.00001102
Iteration 209/1000 | Loss: 0.00001102
Iteration 210/1000 | Loss: 0.00001102
Iteration 211/1000 | Loss: 0.00001101
Iteration 212/1000 | Loss: 0.00001101
Iteration 213/1000 | Loss: 0.00001101
Iteration 214/1000 | Loss: 0.00001101
Iteration 215/1000 | Loss: 0.00001101
Iteration 216/1000 | Loss: 0.00001101
Iteration 217/1000 | Loss: 0.00001101
Iteration 218/1000 | Loss: 0.00001101
Iteration 219/1000 | Loss: 0.00001101
Iteration 220/1000 | Loss: 0.00001101
Iteration 221/1000 | Loss: 0.00001101
Iteration 222/1000 | Loss: 0.00001101
Iteration 223/1000 | Loss: 0.00001101
Iteration 224/1000 | Loss: 0.00001100
Iteration 225/1000 | Loss: 0.00001100
Iteration 226/1000 | Loss: 0.00001100
Iteration 227/1000 | Loss: 0.00001100
Iteration 228/1000 | Loss: 0.00001100
Iteration 229/1000 | Loss: 0.00001100
Iteration 230/1000 | Loss: 0.00001100
Iteration 231/1000 | Loss: 0.00001100
Iteration 232/1000 | Loss: 0.00001100
Iteration 233/1000 | Loss: 0.00001100
Iteration 234/1000 | Loss: 0.00001100
Iteration 235/1000 | Loss: 0.00001100
Iteration 236/1000 | Loss: 0.00001100
Iteration 237/1000 | Loss: 0.00001100
Iteration 238/1000 | Loss: 0.00001100
Iteration 239/1000 | Loss: 0.00001100
Iteration 240/1000 | Loss: 0.00001100
Iteration 241/1000 | Loss: 0.00001100
Iteration 242/1000 | Loss: 0.00001100
Iteration 243/1000 | Loss: 0.00001100
Iteration 244/1000 | Loss: 0.00001100
Iteration 245/1000 | Loss: 0.00001100
Iteration 246/1000 | Loss: 0.00001100
Iteration 247/1000 | Loss: 0.00001100
Iteration 248/1000 | Loss: 0.00001100
Iteration 249/1000 | Loss: 0.00001100
Iteration 250/1000 | Loss: 0.00001100
Iteration 251/1000 | Loss: 0.00001100
Iteration 252/1000 | Loss: 0.00001100
Iteration 253/1000 | Loss: 0.00001100
Iteration 254/1000 | Loss: 0.00001100
Iteration 255/1000 | Loss: 0.00001100
Iteration 256/1000 | Loss: 0.00001100
Iteration 257/1000 | Loss: 0.00001100
Iteration 258/1000 | Loss: 0.00001100
Iteration 259/1000 | Loss: 0.00001100
Iteration 260/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.1004735824826639e-05, 1.1004735824826639e-05, 1.1004735824826639e-05, 1.1004735824826639e-05, 1.1004735824826639e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1004735824826639e-05

Optimization complete. Final v2v error: 2.83642315864563 mm

Highest mean error: 2.9789247512817383 mm for frame 104

Lowest mean error: 2.747915267944336 mm for frame 16

Saving results

Total time: 42.49488973617554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01104243
Iteration 2/25 | Loss: 0.00209699
Iteration 3/25 | Loss: 0.00151476
Iteration 4/25 | Loss: 0.00147804
Iteration 5/25 | Loss: 0.00146922
Iteration 6/25 | Loss: 0.00146780
Iteration 7/25 | Loss: 0.00146780
Iteration 8/25 | Loss: 0.00146780
Iteration 9/25 | Loss: 0.00146780
Iteration 10/25 | Loss: 0.00146780
Iteration 11/25 | Loss: 0.00146780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014678029110655189, 0.0014678029110655189, 0.0014678029110655189, 0.0014678029110655189, 0.0014678029110655189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014678029110655189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96809423
Iteration 2/25 | Loss: 0.00109435
Iteration 3/25 | Loss: 0.00109431
Iteration 4/25 | Loss: 0.00109431
Iteration 5/25 | Loss: 0.00109431
Iteration 6/25 | Loss: 0.00109431
Iteration 7/25 | Loss: 0.00109431
Iteration 8/25 | Loss: 0.00109431
Iteration 9/25 | Loss: 0.00109431
Iteration 10/25 | Loss: 0.00109431
Iteration 11/25 | Loss: 0.00109431
Iteration 12/25 | Loss: 0.00109431
Iteration 13/25 | Loss: 0.00109431
Iteration 14/25 | Loss: 0.00109431
Iteration 15/25 | Loss: 0.00109431
Iteration 16/25 | Loss: 0.00109431
Iteration 17/25 | Loss: 0.00109431
Iteration 18/25 | Loss: 0.00109431
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001094309613108635, 0.001094309613108635, 0.001094309613108635, 0.001094309613108635, 0.001094309613108635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001094309613108635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109431
Iteration 2/1000 | Loss: 0.00008458
Iteration 3/1000 | Loss: 0.00004563
Iteration 4/1000 | Loss: 0.00003770
Iteration 5/1000 | Loss: 0.00003496
Iteration 6/1000 | Loss: 0.00003322
Iteration 7/1000 | Loss: 0.00003201
Iteration 8/1000 | Loss: 0.00003136
Iteration 9/1000 | Loss: 0.00003081
Iteration 10/1000 | Loss: 0.00003027
Iteration 11/1000 | Loss: 0.00002997
Iteration 12/1000 | Loss: 0.00002967
Iteration 13/1000 | Loss: 0.00002940
Iteration 14/1000 | Loss: 0.00002921
Iteration 15/1000 | Loss: 0.00002903
Iteration 16/1000 | Loss: 0.00002886
Iteration 17/1000 | Loss: 0.00002884
Iteration 18/1000 | Loss: 0.00002880
Iteration 19/1000 | Loss: 0.00002873
Iteration 20/1000 | Loss: 0.00002861
Iteration 21/1000 | Loss: 0.00002855
Iteration 22/1000 | Loss: 0.00002849
Iteration 23/1000 | Loss: 0.00002839
Iteration 24/1000 | Loss: 0.00002836
Iteration 25/1000 | Loss: 0.00002836
Iteration 26/1000 | Loss: 0.00002836
Iteration 27/1000 | Loss: 0.00002833
Iteration 28/1000 | Loss: 0.00002833
Iteration 29/1000 | Loss: 0.00002832
Iteration 30/1000 | Loss: 0.00002832
Iteration 31/1000 | Loss: 0.00002831
Iteration 32/1000 | Loss: 0.00002831
Iteration 33/1000 | Loss: 0.00002830
Iteration 34/1000 | Loss: 0.00002830
Iteration 35/1000 | Loss: 0.00002830
Iteration 36/1000 | Loss: 0.00002829
Iteration 37/1000 | Loss: 0.00002828
Iteration 38/1000 | Loss: 0.00002827
Iteration 39/1000 | Loss: 0.00002827
Iteration 40/1000 | Loss: 0.00002827
Iteration 41/1000 | Loss: 0.00002827
Iteration 42/1000 | Loss: 0.00002826
Iteration 43/1000 | Loss: 0.00002825
Iteration 44/1000 | Loss: 0.00002825
Iteration 45/1000 | Loss: 0.00002825
Iteration 46/1000 | Loss: 0.00002824
Iteration 47/1000 | Loss: 0.00002824
Iteration 48/1000 | Loss: 0.00002824
Iteration 49/1000 | Loss: 0.00002824
Iteration 50/1000 | Loss: 0.00002823
Iteration 51/1000 | Loss: 0.00002823
Iteration 52/1000 | Loss: 0.00002823
Iteration 53/1000 | Loss: 0.00002823
Iteration 54/1000 | Loss: 0.00002823
Iteration 55/1000 | Loss: 0.00002823
Iteration 56/1000 | Loss: 0.00002823
Iteration 57/1000 | Loss: 0.00002823
Iteration 58/1000 | Loss: 0.00002823
Iteration 59/1000 | Loss: 0.00002823
Iteration 60/1000 | Loss: 0.00002823
Iteration 61/1000 | Loss: 0.00002823
Iteration 62/1000 | Loss: 0.00002822
Iteration 63/1000 | Loss: 0.00002822
Iteration 64/1000 | Loss: 0.00002822
Iteration 65/1000 | Loss: 0.00002821
Iteration 66/1000 | Loss: 0.00002821
Iteration 67/1000 | Loss: 0.00002821
Iteration 68/1000 | Loss: 0.00002821
Iteration 69/1000 | Loss: 0.00002821
Iteration 70/1000 | Loss: 0.00002820
Iteration 71/1000 | Loss: 0.00002820
Iteration 72/1000 | Loss: 0.00002820
Iteration 73/1000 | Loss: 0.00002820
Iteration 74/1000 | Loss: 0.00002820
Iteration 75/1000 | Loss: 0.00002820
Iteration 76/1000 | Loss: 0.00002820
Iteration 77/1000 | Loss: 0.00002820
Iteration 78/1000 | Loss: 0.00002819
Iteration 79/1000 | Loss: 0.00002819
Iteration 80/1000 | Loss: 0.00002819
Iteration 81/1000 | Loss: 0.00002818
Iteration 82/1000 | Loss: 0.00002818
Iteration 83/1000 | Loss: 0.00002818
Iteration 84/1000 | Loss: 0.00002818
Iteration 85/1000 | Loss: 0.00002818
Iteration 86/1000 | Loss: 0.00002818
Iteration 87/1000 | Loss: 0.00002818
Iteration 88/1000 | Loss: 0.00002818
Iteration 89/1000 | Loss: 0.00002818
Iteration 90/1000 | Loss: 0.00002818
Iteration 91/1000 | Loss: 0.00002818
Iteration 92/1000 | Loss: 0.00002818
Iteration 93/1000 | Loss: 0.00002817
Iteration 94/1000 | Loss: 0.00002817
Iteration 95/1000 | Loss: 0.00002817
Iteration 96/1000 | Loss: 0.00002817
Iteration 97/1000 | Loss: 0.00002817
Iteration 98/1000 | Loss: 0.00002817
Iteration 99/1000 | Loss: 0.00002817
Iteration 100/1000 | Loss: 0.00002817
Iteration 101/1000 | Loss: 0.00002817
Iteration 102/1000 | Loss: 0.00002817
Iteration 103/1000 | Loss: 0.00002817
Iteration 104/1000 | Loss: 0.00002817
Iteration 105/1000 | Loss: 0.00002817
Iteration 106/1000 | Loss: 0.00002817
Iteration 107/1000 | Loss: 0.00002817
Iteration 108/1000 | Loss: 0.00002817
Iteration 109/1000 | Loss: 0.00002817
Iteration 110/1000 | Loss: 0.00002817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.8167527489131317e-05, 2.8167527489131317e-05, 2.8167527489131317e-05, 2.8167527489131317e-05, 2.8167527489131317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8167527489131317e-05

Optimization complete. Final v2v error: 4.432013988494873 mm

Highest mean error: 5.70275354385376 mm for frame 68

Lowest mean error: 3.5913145542144775 mm for frame 27

Saving results

Total time: 50.58516073226929
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892290
Iteration 2/25 | Loss: 0.00182315
Iteration 3/25 | Loss: 0.00155988
Iteration 4/25 | Loss: 0.00153506
Iteration 5/25 | Loss: 0.00153155
Iteration 6/25 | Loss: 0.00153048
Iteration 7/25 | Loss: 0.00153013
Iteration 8/25 | Loss: 0.00153003
Iteration 9/25 | Loss: 0.00153003
Iteration 10/25 | Loss: 0.00153003
Iteration 11/25 | Loss: 0.00153003
Iteration 12/25 | Loss: 0.00153003
Iteration 13/25 | Loss: 0.00153003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0015300328377634287, 0.0015300328377634287, 0.0015300328377634287, 0.0015300328377634287, 0.0015300328377634287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015300328377634287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56186259
Iteration 2/25 | Loss: 0.00098705
Iteration 3/25 | Loss: 0.00098705
Iteration 4/25 | Loss: 0.00098704
Iteration 5/25 | Loss: 0.00098704
Iteration 6/25 | Loss: 0.00098704
Iteration 7/25 | Loss: 0.00098704
Iteration 8/25 | Loss: 0.00098704
Iteration 9/25 | Loss: 0.00098704
Iteration 10/25 | Loss: 0.00098704
Iteration 11/25 | Loss: 0.00098704
Iteration 12/25 | Loss: 0.00098704
Iteration 13/25 | Loss: 0.00098704
Iteration 14/25 | Loss: 0.00098704
Iteration 15/25 | Loss: 0.00098704
Iteration 16/25 | Loss: 0.00098704
Iteration 17/25 | Loss: 0.00098704
Iteration 18/25 | Loss: 0.00098704
Iteration 19/25 | Loss: 0.00098704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000987041974440217, 0.000987041974440217, 0.000987041974440217, 0.000987041974440217, 0.000987041974440217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000987041974440217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098704
Iteration 2/1000 | Loss: 0.00007010
Iteration 3/1000 | Loss: 0.00004321
Iteration 4/1000 | Loss: 0.00003715
Iteration 5/1000 | Loss: 0.00003475
Iteration 6/1000 | Loss: 0.00003312
Iteration 7/1000 | Loss: 0.00003236
Iteration 8/1000 | Loss: 0.00003149
Iteration 9/1000 | Loss: 0.00003076
Iteration 10/1000 | Loss: 0.00003034
Iteration 11/1000 | Loss: 0.00002998
Iteration 12/1000 | Loss: 0.00002972
Iteration 13/1000 | Loss: 0.00002951
Iteration 14/1000 | Loss: 0.00002927
Iteration 15/1000 | Loss: 0.00002922
Iteration 16/1000 | Loss: 0.00002918
Iteration 17/1000 | Loss: 0.00002913
Iteration 18/1000 | Loss: 0.00002910
Iteration 19/1000 | Loss: 0.00002910
Iteration 20/1000 | Loss: 0.00002910
Iteration 21/1000 | Loss: 0.00002910
Iteration 22/1000 | Loss: 0.00002909
Iteration 23/1000 | Loss: 0.00002909
Iteration 24/1000 | Loss: 0.00002909
Iteration 25/1000 | Loss: 0.00002908
Iteration 26/1000 | Loss: 0.00002908
Iteration 27/1000 | Loss: 0.00002907
Iteration 28/1000 | Loss: 0.00002907
Iteration 29/1000 | Loss: 0.00002907
Iteration 30/1000 | Loss: 0.00002907
Iteration 31/1000 | Loss: 0.00002907
Iteration 32/1000 | Loss: 0.00002907
Iteration 33/1000 | Loss: 0.00002907
Iteration 34/1000 | Loss: 0.00002907
Iteration 35/1000 | Loss: 0.00002906
Iteration 36/1000 | Loss: 0.00002906
Iteration 37/1000 | Loss: 0.00002905
Iteration 38/1000 | Loss: 0.00002905
Iteration 39/1000 | Loss: 0.00002905
Iteration 40/1000 | Loss: 0.00002905
Iteration 41/1000 | Loss: 0.00002905
Iteration 42/1000 | Loss: 0.00002904
Iteration 43/1000 | Loss: 0.00002904
Iteration 44/1000 | Loss: 0.00002904
Iteration 45/1000 | Loss: 0.00002904
Iteration 46/1000 | Loss: 0.00002904
Iteration 47/1000 | Loss: 0.00002904
Iteration 48/1000 | Loss: 0.00002904
Iteration 49/1000 | Loss: 0.00002904
Iteration 50/1000 | Loss: 0.00002903
Iteration 51/1000 | Loss: 0.00002903
Iteration 52/1000 | Loss: 0.00002902
Iteration 53/1000 | Loss: 0.00002902
Iteration 54/1000 | Loss: 0.00002902
Iteration 55/1000 | Loss: 0.00002902
Iteration 56/1000 | Loss: 0.00002901
Iteration 57/1000 | Loss: 0.00002901
Iteration 58/1000 | Loss: 0.00002900
Iteration 59/1000 | Loss: 0.00002900
Iteration 60/1000 | Loss: 0.00002900
Iteration 61/1000 | Loss: 0.00002900
Iteration 62/1000 | Loss: 0.00002900
Iteration 63/1000 | Loss: 0.00002900
Iteration 64/1000 | Loss: 0.00002900
Iteration 65/1000 | Loss: 0.00002900
Iteration 66/1000 | Loss: 0.00002900
Iteration 67/1000 | Loss: 0.00002899
Iteration 68/1000 | Loss: 0.00002899
Iteration 69/1000 | Loss: 0.00002899
Iteration 70/1000 | Loss: 0.00002899
Iteration 71/1000 | Loss: 0.00002899
Iteration 72/1000 | Loss: 0.00002899
Iteration 73/1000 | Loss: 0.00002899
Iteration 74/1000 | Loss: 0.00002899
Iteration 75/1000 | Loss: 0.00002899
Iteration 76/1000 | Loss: 0.00002899
Iteration 77/1000 | Loss: 0.00002899
Iteration 78/1000 | Loss: 0.00002899
Iteration 79/1000 | Loss: 0.00002898
Iteration 80/1000 | Loss: 0.00002898
Iteration 81/1000 | Loss: 0.00002898
Iteration 82/1000 | Loss: 0.00002898
Iteration 83/1000 | Loss: 0.00002898
Iteration 84/1000 | Loss: 0.00002898
Iteration 85/1000 | Loss: 0.00002898
Iteration 86/1000 | Loss: 0.00002898
Iteration 87/1000 | Loss: 0.00002898
Iteration 88/1000 | Loss: 0.00002898
Iteration 89/1000 | Loss: 0.00002897
Iteration 90/1000 | Loss: 0.00002897
Iteration 91/1000 | Loss: 0.00002897
Iteration 92/1000 | Loss: 0.00002897
Iteration 93/1000 | Loss: 0.00002897
Iteration 94/1000 | Loss: 0.00002897
Iteration 95/1000 | Loss: 0.00002896
Iteration 96/1000 | Loss: 0.00002896
Iteration 97/1000 | Loss: 0.00002896
Iteration 98/1000 | Loss: 0.00002895
Iteration 99/1000 | Loss: 0.00002895
Iteration 100/1000 | Loss: 0.00002894
Iteration 101/1000 | Loss: 0.00002894
Iteration 102/1000 | Loss: 0.00002894
Iteration 103/1000 | Loss: 0.00002893
Iteration 104/1000 | Loss: 0.00002893
Iteration 105/1000 | Loss: 0.00002893
Iteration 106/1000 | Loss: 0.00002892
Iteration 107/1000 | Loss: 0.00002892
Iteration 108/1000 | Loss: 0.00002892
Iteration 109/1000 | Loss: 0.00002892
Iteration 110/1000 | Loss: 0.00002892
Iteration 111/1000 | Loss: 0.00002892
Iteration 112/1000 | Loss: 0.00002892
Iteration 113/1000 | Loss: 0.00002892
Iteration 114/1000 | Loss: 0.00002892
Iteration 115/1000 | Loss: 0.00002891
Iteration 116/1000 | Loss: 0.00002891
Iteration 117/1000 | Loss: 0.00002891
Iteration 118/1000 | Loss: 0.00002891
Iteration 119/1000 | Loss: 0.00002891
Iteration 120/1000 | Loss: 0.00002891
Iteration 121/1000 | Loss: 0.00002891
Iteration 122/1000 | Loss: 0.00002891
Iteration 123/1000 | Loss: 0.00002891
Iteration 124/1000 | Loss: 0.00002891
Iteration 125/1000 | Loss: 0.00002891
Iteration 126/1000 | Loss: 0.00002891
Iteration 127/1000 | Loss: 0.00002891
Iteration 128/1000 | Loss: 0.00002891
Iteration 129/1000 | Loss: 0.00002890
Iteration 130/1000 | Loss: 0.00002890
Iteration 131/1000 | Loss: 0.00002890
Iteration 132/1000 | Loss: 0.00002890
Iteration 133/1000 | Loss: 0.00002890
Iteration 134/1000 | Loss: 0.00002890
Iteration 135/1000 | Loss: 0.00002890
Iteration 136/1000 | Loss: 0.00002890
Iteration 137/1000 | Loss: 0.00002890
Iteration 138/1000 | Loss: 0.00002890
Iteration 139/1000 | Loss: 0.00002890
Iteration 140/1000 | Loss: 0.00002890
Iteration 141/1000 | Loss: 0.00002890
Iteration 142/1000 | Loss: 0.00002890
Iteration 143/1000 | Loss: 0.00002890
Iteration 144/1000 | Loss: 0.00002890
Iteration 145/1000 | Loss: 0.00002890
Iteration 146/1000 | Loss: 0.00002890
Iteration 147/1000 | Loss: 0.00002889
Iteration 148/1000 | Loss: 0.00002889
Iteration 149/1000 | Loss: 0.00002889
Iteration 150/1000 | Loss: 0.00002889
Iteration 151/1000 | Loss: 0.00002889
Iteration 152/1000 | Loss: 0.00002889
Iteration 153/1000 | Loss: 0.00002889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.8894341085106134e-05, 2.8894341085106134e-05, 2.8894341085106134e-05, 2.8894341085106134e-05, 2.8894341085106134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8894341085106134e-05

Optimization complete. Final v2v error: 4.4788713455200195 mm

Highest mean error: 5.057079315185547 mm for frame 17

Lowest mean error: 4.237645149230957 mm for frame 31

Saving results

Total time: 45.205153942108154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412977
Iteration 2/25 | Loss: 0.00151547
Iteration 3/25 | Loss: 0.00131424
Iteration 4/25 | Loss: 0.00129930
Iteration 5/25 | Loss: 0.00129718
Iteration 6/25 | Loss: 0.00129665
Iteration 7/25 | Loss: 0.00129665
Iteration 8/25 | Loss: 0.00129665
Iteration 9/25 | Loss: 0.00129665
Iteration 10/25 | Loss: 0.00129665
Iteration 11/25 | Loss: 0.00129665
Iteration 12/25 | Loss: 0.00129665
Iteration 13/25 | Loss: 0.00129665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012966545764356852, 0.0012966545764356852, 0.0012966545764356852, 0.0012966545764356852, 0.0012966545764356852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012966545764356852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38963974
Iteration 2/25 | Loss: 0.00073046
Iteration 3/25 | Loss: 0.00073046
Iteration 4/25 | Loss: 0.00073046
Iteration 5/25 | Loss: 0.00073046
Iteration 6/25 | Loss: 0.00073046
Iteration 7/25 | Loss: 0.00073046
Iteration 8/25 | Loss: 0.00073046
Iteration 9/25 | Loss: 0.00073046
Iteration 10/25 | Loss: 0.00073046
Iteration 11/25 | Loss: 0.00073046
Iteration 12/25 | Loss: 0.00073046
Iteration 13/25 | Loss: 0.00073046
Iteration 14/25 | Loss: 0.00073046
Iteration 15/25 | Loss: 0.00073046
Iteration 16/25 | Loss: 0.00073046
Iteration 17/25 | Loss: 0.00073046
Iteration 18/25 | Loss: 0.00073046
Iteration 19/25 | Loss: 0.00073046
Iteration 20/25 | Loss: 0.00073046
Iteration 21/25 | Loss: 0.00073046
Iteration 22/25 | Loss: 0.00073046
Iteration 23/25 | Loss: 0.00073046
Iteration 24/25 | Loss: 0.00073046
Iteration 25/25 | Loss: 0.00073046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073046
Iteration 2/1000 | Loss: 0.00002968
Iteration 3/1000 | Loss: 0.00002130
Iteration 4/1000 | Loss: 0.00001943
Iteration 5/1000 | Loss: 0.00001834
Iteration 6/1000 | Loss: 0.00001758
Iteration 7/1000 | Loss: 0.00001703
Iteration 8/1000 | Loss: 0.00001671
Iteration 9/1000 | Loss: 0.00001643
Iteration 10/1000 | Loss: 0.00001619
Iteration 11/1000 | Loss: 0.00001600
Iteration 12/1000 | Loss: 0.00001600
Iteration 13/1000 | Loss: 0.00001597
Iteration 14/1000 | Loss: 0.00001593
Iteration 15/1000 | Loss: 0.00001593
Iteration 16/1000 | Loss: 0.00001589
Iteration 17/1000 | Loss: 0.00001584
Iteration 18/1000 | Loss: 0.00001574
Iteration 19/1000 | Loss: 0.00001573
Iteration 20/1000 | Loss: 0.00001572
Iteration 21/1000 | Loss: 0.00001571
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001567
Iteration 24/1000 | Loss: 0.00001566
Iteration 25/1000 | Loss: 0.00001566
Iteration 26/1000 | Loss: 0.00001566
Iteration 27/1000 | Loss: 0.00001566
Iteration 28/1000 | Loss: 0.00001566
Iteration 29/1000 | Loss: 0.00001565
Iteration 30/1000 | Loss: 0.00001565
Iteration 31/1000 | Loss: 0.00001564
Iteration 32/1000 | Loss: 0.00001563
Iteration 33/1000 | Loss: 0.00001561
Iteration 34/1000 | Loss: 0.00001561
Iteration 35/1000 | Loss: 0.00001561
Iteration 36/1000 | Loss: 0.00001560
Iteration 37/1000 | Loss: 0.00001560
Iteration 38/1000 | Loss: 0.00001560
Iteration 39/1000 | Loss: 0.00001559
Iteration 40/1000 | Loss: 0.00001559
Iteration 41/1000 | Loss: 0.00001558
Iteration 42/1000 | Loss: 0.00001558
Iteration 43/1000 | Loss: 0.00001557
Iteration 44/1000 | Loss: 0.00001556
Iteration 45/1000 | Loss: 0.00001556
Iteration 46/1000 | Loss: 0.00001556
Iteration 47/1000 | Loss: 0.00001555
Iteration 48/1000 | Loss: 0.00001555
Iteration 49/1000 | Loss: 0.00001554
Iteration 50/1000 | Loss: 0.00001553
Iteration 51/1000 | Loss: 0.00001552
Iteration 52/1000 | Loss: 0.00001551
Iteration 53/1000 | Loss: 0.00001551
Iteration 54/1000 | Loss: 0.00001550
Iteration 55/1000 | Loss: 0.00001545
Iteration 56/1000 | Loss: 0.00001542
Iteration 57/1000 | Loss: 0.00001542
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001540
Iteration 60/1000 | Loss: 0.00001539
Iteration 61/1000 | Loss: 0.00001538
Iteration 62/1000 | Loss: 0.00001537
Iteration 63/1000 | Loss: 0.00001537
Iteration 64/1000 | Loss: 0.00001537
Iteration 65/1000 | Loss: 0.00001536
Iteration 66/1000 | Loss: 0.00001536
Iteration 67/1000 | Loss: 0.00001535
Iteration 68/1000 | Loss: 0.00001534
Iteration 69/1000 | Loss: 0.00001533
Iteration 70/1000 | Loss: 0.00001532
Iteration 71/1000 | Loss: 0.00001532
Iteration 72/1000 | Loss: 0.00001532
Iteration 73/1000 | Loss: 0.00001532
Iteration 74/1000 | Loss: 0.00001531
Iteration 75/1000 | Loss: 0.00001531
Iteration 76/1000 | Loss: 0.00001529
Iteration 77/1000 | Loss: 0.00001529
Iteration 78/1000 | Loss: 0.00001528
Iteration 79/1000 | Loss: 0.00001528
Iteration 80/1000 | Loss: 0.00001527
Iteration 81/1000 | Loss: 0.00001527
Iteration 82/1000 | Loss: 0.00001527
Iteration 83/1000 | Loss: 0.00001526
Iteration 84/1000 | Loss: 0.00001526
Iteration 85/1000 | Loss: 0.00001526
Iteration 86/1000 | Loss: 0.00001525
Iteration 87/1000 | Loss: 0.00001525
Iteration 88/1000 | Loss: 0.00001524
Iteration 89/1000 | Loss: 0.00001524
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001523
Iteration 93/1000 | Loss: 0.00001523
Iteration 94/1000 | Loss: 0.00001523
Iteration 95/1000 | Loss: 0.00001522
Iteration 96/1000 | Loss: 0.00001522
Iteration 97/1000 | Loss: 0.00001522
Iteration 98/1000 | Loss: 0.00001522
Iteration 99/1000 | Loss: 0.00001521
Iteration 100/1000 | Loss: 0.00001521
Iteration 101/1000 | Loss: 0.00001521
Iteration 102/1000 | Loss: 0.00001521
Iteration 103/1000 | Loss: 0.00001521
Iteration 104/1000 | Loss: 0.00001520
Iteration 105/1000 | Loss: 0.00001520
Iteration 106/1000 | Loss: 0.00001520
Iteration 107/1000 | Loss: 0.00001520
Iteration 108/1000 | Loss: 0.00001520
Iteration 109/1000 | Loss: 0.00001520
Iteration 110/1000 | Loss: 0.00001520
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001519
Iteration 113/1000 | Loss: 0.00001519
Iteration 114/1000 | Loss: 0.00001519
Iteration 115/1000 | Loss: 0.00001519
Iteration 116/1000 | Loss: 0.00001519
Iteration 117/1000 | Loss: 0.00001519
Iteration 118/1000 | Loss: 0.00001519
Iteration 119/1000 | Loss: 0.00001518
Iteration 120/1000 | Loss: 0.00001518
Iteration 121/1000 | Loss: 0.00001518
Iteration 122/1000 | Loss: 0.00001518
Iteration 123/1000 | Loss: 0.00001518
Iteration 124/1000 | Loss: 0.00001518
Iteration 125/1000 | Loss: 0.00001518
Iteration 126/1000 | Loss: 0.00001518
Iteration 127/1000 | Loss: 0.00001518
Iteration 128/1000 | Loss: 0.00001518
Iteration 129/1000 | Loss: 0.00001518
Iteration 130/1000 | Loss: 0.00001518
Iteration 131/1000 | Loss: 0.00001518
Iteration 132/1000 | Loss: 0.00001518
Iteration 133/1000 | Loss: 0.00001518
Iteration 134/1000 | Loss: 0.00001518
Iteration 135/1000 | Loss: 0.00001517
Iteration 136/1000 | Loss: 0.00001517
Iteration 137/1000 | Loss: 0.00001517
Iteration 138/1000 | Loss: 0.00001517
Iteration 139/1000 | Loss: 0.00001517
Iteration 140/1000 | Loss: 0.00001517
Iteration 141/1000 | Loss: 0.00001517
Iteration 142/1000 | Loss: 0.00001516
Iteration 143/1000 | Loss: 0.00001516
Iteration 144/1000 | Loss: 0.00001516
Iteration 145/1000 | Loss: 0.00001516
Iteration 146/1000 | Loss: 0.00001516
Iteration 147/1000 | Loss: 0.00001516
Iteration 148/1000 | Loss: 0.00001516
Iteration 149/1000 | Loss: 0.00001516
Iteration 150/1000 | Loss: 0.00001515
Iteration 151/1000 | Loss: 0.00001515
Iteration 152/1000 | Loss: 0.00001515
Iteration 153/1000 | Loss: 0.00001515
Iteration 154/1000 | Loss: 0.00001515
Iteration 155/1000 | Loss: 0.00001515
Iteration 156/1000 | Loss: 0.00001515
Iteration 157/1000 | Loss: 0.00001515
Iteration 158/1000 | Loss: 0.00001514
Iteration 159/1000 | Loss: 0.00001514
Iteration 160/1000 | Loss: 0.00001514
Iteration 161/1000 | Loss: 0.00001514
Iteration 162/1000 | Loss: 0.00001514
Iteration 163/1000 | Loss: 0.00001514
Iteration 164/1000 | Loss: 0.00001514
Iteration 165/1000 | Loss: 0.00001513
Iteration 166/1000 | Loss: 0.00001513
Iteration 167/1000 | Loss: 0.00001513
Iteration 168/1000 | Loss: 0.00001513
Iteration 169/1000 | Loss: 0.00001513
Iteration 170/1000 | Loss: 0.00001513
Iteration 171/1000 | Loss: 0.00001513
Iteration 172/1000 | Loss: 0.00001513
Iteration 173/1000 | Loss: 0.00001513
Iteration 174/1000 | Loss: 0.00001513
Iteration 175/1000 | Loss: 0.00001513
Iteration 176/1000 | Loss: 0.00001513
Iteration 177/1000 | Loss: 0.00001513
Iteration 178/1000 | Loss: 0.00001512
Iteration 179/1000 | Loss: 0.00001512
Iteration 180/1000 | Loss: 0.00001512
Iteration 181/1000 | Loss: 0.00001512
Iteration 182/1000 | Loss: 0.00001512
Iteration 183/1000 | Loss: 0.00001512
Iteration 184/1000 | Loss: 0.00001512
Iteration 185/1000 | Loss: 0.00001512
Iteration 186/1000 | Loss: 0.00001511
Iteration 187/1000 | Loss: 0.00001511
Iteration 188/1000 | Loss: 0.00001511
Iteration 189/1000 | Loss: 0.00001511
Iteration 190/1000 | Loss: 0.00001511
Iteration 191/1000 | Loss: 0.00001511
Iteration 192/1000 | Loss: 0.00001511
Iteration 193/1000 | Loss: 0.00001511
Iteration 194/1000 | Loss: 0.00001511
Iteration 195/1000 | Loss: 0.00001510
Iteration 196/1000 | Loss: 0.00001510
Iteration 197/1000 | Loss: 0.00001510
Iteration 198/1000 | Loss: 0.00001510
Iteration 199/1000 | Loss: 0.00001510
Iteration 200/1000 | Loss: 0.00001509
Iteration 201/1000 | Loss: 0.00001509
Iteration 202/1000 | Loss: 0.00001509
Iteration 203/1000 | Loss: 0.00001509
Iteration 204/1000 | Loss: 0.00001509
Iteration 205/1000 | Loss: 0.00001509
Iteration 206/1000 | Loss: 0.00001509
Iteration 207/1000 | Loss: 0.00001509
Iteration 208/1000 | Loss: 0.00001509
Iteration 209/1000 | Loss: 0.00001509
Iteration 210/1000 | Loss: 0.00001509
Iteration 211/1000 | Loss: 0.00001509
Iteration 212/1000 | Loss: 0.00001509
Iteration 213/1000 | Loss: 0.00001508
Iteration 214/1000 | Loss: 0.00001508
Iteration 215/1000 | Loss: 0.00001508
Iteration 216/1000 | Loss: 0.00001507
Iteration 217/1000 | Loss: 0.00001507
Iteration 218/1000 | Loss: 0.00001507
Iteration 219/1000 | Loss: 0.00001507
Iteration 220/1000 | Loss: 0.00001507
Iteration 221/1000 | Loss: 0.00001507
Iteration 222/1000 | Loss: 0.00001507
Iteration 223/1000 | Loss: 0.00001507
Iteration 224/1000 | Loss: 0.00001507
Iteration 225/1000 | Loss: 0.00001506
Iteration 226/1000 | Loss: 0.00001506
Iteration 227/1000 | Loss: 0.00001506
Iteration 228/1000 | Loss: 0.00001506
Iteration 229/1000 | Loss: 0.00001506
Iteration 230/1000 | Loss: 0.00001506
Iteration 231/1000 | Loss: 0.00001506
Iteration 232/1000 | Loss: 0.00001505
Iteration 233/1000 | Loss: 0.00001505
Iteration 234/1000 | Loss: 0.00001505
Iteration 235/1000 | Loss: 0.00001505
Iteration 236/1000 | Loss: 0.00001505
Iteration 237/1000 | Loss: 0.00001505
Iteration 238/1000 | Loss: 0.00001505
Iteration 239/1000 | Loss: 0.00001505
Iteration 240/1000 | Loss: 0.00001505
Iteration 241/1000 | Loss: 0.00001505
Iteration 242/1000 | Loss: 0.00001505
Iteration 243/1000 | Loss: 0.00001505
Iteration 244/1000 | Loss: 0.00001505
Iteration 245/1000 | Loss: 0.00001505
Iteration 246/1000 | Loss: 0.00001505
Iteration 247/1000 | Loss: 0.00001505
Iteration 248/1000 | Loss: 0.00001505
Iteration 249/1000 | Loss: 0.00001505
Iteration 250/1000 | Loss: 0.00001505
Iteration 251/1000 | Loss: 0.00001505
Iteration 252/1000 | Loss: 0.00001505
Iteration 253/1000 | Loss: 0.00001505
Iteration 254/1000 | Loss: 0.00001505
Iteration 255/1000 | Loss: 0.00001505
Iteration 256/1000 | Loss: 0.00001505
Iteration 257/1000 | Loss: 0.00001505
Iteration 258/1000 | Loss: 0.00001505
Iteration 259/1000 | Loss: 0.00001505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.5045120562717784e-05, 1.5045120562717784e-05, 1.5045120562717784e-05, 1.5045120562717784e-05, 1.5045120562717784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5045120562717784e-05

Optimization complete. Final v2v error: 3.314850330352783 mm

Highest mean error: 3.4465105533599854 mm for frame 27

Lowest mean error: 3.170478343963623 mm for frame 137

Saving results

Total time: 45.837697982788086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470553
Iteration 2/25 | Loss: 0.00168117
Iteration 3/25 | Loss: 0.00147847
Iteration 4/25 | Loss: 0.00144390
Iteration 5/25 | Loss: 0.00143351
Iteration 6/25 | Loss: 0.00142474
Iteration 7/25 | Loss: 0.00140249
Iteration 8/25 | Loss: 0.00138391
Iteration 9/25 | Loss: 0.00137021
Iteration 10/25 | Loss: 0.00134682
Iteration 11/25 | Loss: 0.00133722
Iteration 12/25 | Loss: 0.00133429
Iteration 13/25 | Loss: 0.00133687
Iteration 14/25 | Loss: 0.00133341
Iteration 15/25 | Loss: 0.00133109
Iteration 16/25 | Loss: 0.00133004
Iteration 17/25 | Loss: 0.00132971
Iteration 18/25 | Loss: 0.00132966
Iteration 19/25 | Loss: 0.00132964
Iteration 20/25 | Loss: 0.00132964
Iteration 21/25 | Loss: 0.00132964
Iteration 22/25 | Loss: 0.00132964
Iteration 23/25 | Loss: 0.00132964
Iteration 24/25 | Loss: 0.00132964
Iteration 25/25 | Loss: 0.00132964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42755342
Iteration 2/25 | Loss: 0.00082536
Iteration 3/25 | Loss: 0.00082535
Iteration 4/25 | Loss: 0.00082535
Iteration 5/25 | Loss: 0.00082535
Iteration 6/25 | Loss: 0.00082535
Iteration 7/25 | Loss: 0.00082535
Iteration 8/25 | Loss: 0.00082535
Iteration 9/25 | Loss: 0.00082535
Iteration 10/25 | Loss: 0.00082535
Iteration 11/25 | Loss: 0.00082535
Iteration 12/25 | Loss: 0.00082535
Iteration 13/25 | Loss: 0.00082535
Iteration 14/25 | Loss: 0.00082535
Iteration 15/25 | Loss: 0.00082535
Iteration 16/25 | Loss: 0.00082535
Iteration 17/25 | Loss: 0.00082535
Iteration 18/25 | Loss: 0.00082535
Iteration 19/25 | Loss: 0.00082535
Iteration 20/25 | Loss: 0.00082535
Iteration 21/25 | Loss: 0.00082535
Iteration 22/25 | Loss: 0.00082535
Iteration 23/25 | Loss: 0.00082535
Iteration 24/25 | Loss: 0.00082535
Iteration 25/25 | Loss: 0.00082535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082535
Iteration 2/1000 | Loss: 0.00003494
Iteration 3/1000 | Loss: 0.00002521
Iteration 4/1000 | Loss: 0.00002158
Iteration 5/1000 | Loss: 0.00002040
Iteration 6/1000 | Loss: 0.00001968
Iteration 7/1000 | Loss: 0.00001913
Iteration 8/1000 | Loss: 0.00001860
Iteration 9/1000 | Loss: 0.00001832
Iteration 10/1000 | Loss: 0.00001818
Iteration 11/1000 | Loss: 0.00001791
Iteration 12/1000 | Loss: 0.00001769
Iteration 13/1000 | Loss: 0.00001759
Iteration 14/1000 | Loss: 0.00001747
Iteration 15/1000 | Loss: 0.00001747
Iteration 16/1000 | Loss: 0.00001746
Iteration 17/1000 | Loss: 0.00001746
Iteration 18/1000 | Loss: 0.00001739
Iteration 19/1000 | Loss: 0.00001739
Iteration 20/1000 | Loss: 0.00001734
Iteration 21/1000 | Loss: 0.00001731
Iteration 22/1000 | Loss: 0.00001731
Iteration 23/1000 | Loss: 0.00001731
Iteration 24/1000 | Loss: 0.00001729
Iteration 25/1000 | Loss: 0.00001725
Iteration 26/1000 | Loss: 0.00001725
Iteration 27/1000 | Loss: 0.00001725
Iteration 28/1000 | Loss: 0.00001725
Iteration 29/1000 | Loss: 0.00001725
Iteration 30/1000 | Loss: 0.00001725
Iteration 31/1000 | Loss: 0.00001724
Iteration 32/1000 | Loss: 0.00001724
Iteration 33/1000 | Loss: 0.00001722
Iteration 34/1000 | Loss: 0.00001722
Iteration 35/1000 | Loss: 0.00001722
Iteration 36/1000 | Loss: 0.00001722
Iteration 37/1000 | Loss: 0.00001721
Iteration 38/1000 | Loss: 0.00001721
Iteration 39/1000 | Loss: 0.00001721
Iteration 40/1000 | Loss: 0.00001720
Iteration 41/1000 | Loss: 0.00001720
Iteration 42/1000 | Loss: 0.00001720
Iteration 43/1000 | Loss: 0.00001719
Iteration 44/1000 | Loss: 0.00001719
Iteration 45/1000 | Loss: 0.00001719
Iteration 46/1000 | Loss: 0.00001718
Iteration 47/1000 | Loss: 0.00001718
Iteration 48/1000 | Loss: 0.00001718
Iteration 49/1000 | Loss: 0.00001718
Iteration 50/1000 | Loss: 0.00001717
Iteration 51/1000 | Loss: 0.00001717
Iteration 52/1000 | Loss: 0.00001717
Iteration 53/1000 | Loss: 0.00001717
Iteration 54/1000 | Loss: 0.00001716
Iteration 55/1000 | Loss: 0.00001716
Iteration 56/1000 | Loss: 0.00001715
Iteration 57/1000 | Loss: 0.00001714
Iteration 58/1000 | Loss: 0.00001714
Iteration 59/1000 | Loss: 0.00001714
Iteration 60/1000 | Loss: 0.00001714
Iteration 61/1000 | Loss: 0.00001714
Iteration 62/1000 | Loss: 0.00001713
Iteration 63/1000 | Loss: 0.00001713
Iteration 64/1000 | Loss: 0.00001713
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001710
Iteration 69/1000 | Loss: 0.00001710
Iteration 70/1000 | Loss: 0.00001710
Iteration 71/1000 | Loss: 0.00001710
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001710
Iteration 79/1000 | Loss: 0.00001710
Iteration 80/1000 | Loss: 0.00001710
Iteration 81/1000 | Loss: 0.00001710
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001707
Iteration 89/1000 | Loss: 0.00001707
Iteration 90/1000 | Loss: 0.00001707
Iteration 91/1000 | Loss: 0.00001707
Iteration 92/1000 | Loss: 0.00001706
Iteration 93/1000 | Loss: 0.00001706
Iteration 94/1000 | Loss: 0.00001706
Iteration 95/1000 | Loss: 0.00001706
Iteration 96/1000 | Loss: 0.00001706
Iteration 97/1000 | Loss: 0.00001705
Iteration 98/1000 | Loss: 0.00001705
Iteration 99/1000 | Loss: 0.00001705
Iteration 100/1000 | Loss: 0.00001704
Iteration 101/1000 | Loss: 0.00001704
Iteration 102/1000 | Loss: 0.00001704
Iteration 103/1000 | Loss: 0.00001704
Iteration 104/1000 | Loss: 0.00001704
Iteration 105/1000 | Loss: 0.00001703
Iteration 106/1000 | Loss: 0.00001703
Iteration 107/1000 | Loss: 0.00001703
Iteration 108/1000 | Loss: 0.00001703
Iteration 109/1000 | Loss: 0.00001703
Iteration 110/1000 | Loss: 0.00001703
Iteration 111/1000 | Loss: 0.00001702
Iteration 112/1000 | Loss: 0.00001702
Iteration 113/1000 | Loss: 0.00001702
Iteration 114/1000 | Loss: 0.00001702
Iteration 115/1000 | Loss: 0.00001701
Iteration 116/1000 | Loss: 0.00001701
Iteration 117/1000 | Loss: 0.00001700
Iteration 118/1000 | Loss: 0.00001700
Iteration 119/1000 | Loss: 0.00001700
Iteration 120/1000 | Loss: 0.00001700
Iteration 121/1000 | Loss: 0.00001700
Iteration 122/1000 | Loss: 0.00001700
Iteration 123/1000 | Loss: 0.00001700
Iteration 124/1000 | Loss: 0.00001700
Iteration 125/1000 | Loss: 0.00001699
Iteration 126/1000 | Loss: 0.00001699
Iteration 127/1000 | Loss: 0.00001699
Iteration 128/1000 | Loss: 0.00001698
Iteration 129/1000 | Loss: 0.00001698
Iteration 130/1000 | Loss: 0.00001698
Iteration 131/1000 | Loss: 0.00001698
Iteration 132/1000 | Loss: 0.00001698
Iteration 133/1000 | Loss: 0.00001698
Iteration 134/1000 | Loss: 0.00001698
Iteration 135/1000 | Loss: 0.00001698
Iteration 136/1000 | Loss: 0.00001697
Iteration 137/1000 | Loss: 0.00001697
Iteration 138/1000 | Loss: 0.00001697
Iteration 139/1000 | Loss: 0.00001697
Iteration 140/1000 | Loss: 0.00001697
Iteration 141/1000 | Loss: 0.00001697
Iteration 142/1000 | Loss: 0.00001697
Iteration 143/1000 | Loss: 0.00001697
Iteration 144/1000 | Loss: 0.00001697
Iteration 145/1000 | Loss: 0.00001697
Iteration 146/1000 | Loss: 0.00001697
Iteration 147/1000 | Loss: 0.00001697
Iteration 148/1000 | Loss: 0.00001696
Iteration 149/1000 | Loss: 0.00001696
Iteration 150/1000 | Loss: 0.00001696
Iteration 151/1000 | Loss: 0.00001696
Iteration 152/1000 | Loss: 0.00001696
Iteration 153/1000 | Loss: 0.00001696
Iteration 154/1000 | Loss: 0.00001696
Iteration 155/1000 | Loss: 0.00001696
Iteration 156/1000 | Loss: 0.00001696
Iteration 157/1000 | Loss: 0.00001696
Iteration 158/1000 | Loss: 0.00001696
Iteration 159/1000 | Loss: 0.00001696
Iteration 160/1000 | Loss: 0.00001696
Iteration 161/1000 | Loss: 0.00001696
Iteration 162/1000 | Loss: 0.00001696
Iteration 163/1000 | Loss: 0.00001696
Iteration 164/1000 | Loss: 0.00001696
Iteration 165/1000 | Loss: 0.00001696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.6961288565653376e-05, 1.6961288565653376e-05, 1.6961288565653376e-05, 1.6961288565653376e-05, 1.6961288565653376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6961288565653376e-05

Optimization complete. Final v2v error: 3.4836206436157227 mm

Highest mean error: 3.9178738594055176 mm for frame 122

Lowest mean error: 3.3053905963897705 mm for frame 54

Saving results

Total time: 61.67487692832947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00228451
Iteration 2/25 | Loss: 0.00136373
Iteration 3/25 | Loss: 0.00127800
Iteration 4/25 | Loss: 0.00125573
Iteration 5/25 | Loss: 0.00124628
Iteration 6/25 | Loss: 0.00124403
Iteration 7/25 | Loss: 0.00124313
Iteration 8/25 | Loss: 0.00124311
Iteration 9/25 | Loss: 0.00124311
Iteration 10/25 | Loss: 0.00124311
Iteration 11/25 | Loss: 0.00124311
Iteration 12/25 | Loss: 0.00124311
Iteration 13/25 | Loss: 0.00124311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00124310830142349, 0.00124310830142349, 0.00124310830142349, 0.00124310830142349, 0.00124310830142349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00124310830142349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36027586
Iteration 2/25 | Loss: 0.00116467
Iteration 3/25 | Loss: 0.00116467
Iteration 4/25 | Loss: 0.00116467
Iteration 5/25 | Loss: 0.00116467
Iteration 6/25 | Loss: 0.00116467
Iteration 7/25 | Loss: 0.00116467
Iteration 8/25 | Loss: 0.00116467
Iteration 9/25 | Loss: 0.00116467
Iteration 10/25 | Loss: 0.00116467
Iteration 11/25 | Loss: 0.00116467
Iteration 12/25 | Loss: 0.00116467
Iteration 13/25 | Loss: 0.00116467
Iteration 14/25 | Loss: 0.00116467
Iteration 15/25 | Loss: 0.00116467
Iteration 16/25 | Loss: 0.00116467
Iteration 17/25 | Loss: 0.00116467
Iteration 18/25 | Loss: 0.00116467
Iteration 19/25 | Loss: 0.00116467
Iteration 20/25 | Loss: 0.00116467
Iteration 21/25 | Loss: 0.00116467
Iteration 22/25 | Loss: 0.00116467
Iteration 23/25 | Loss: 0.00116467
Iteration 24/25 | Loss: 0.00116467
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011646714992821217, 0.0011646714992821217, 0.0011646714992821217, 0.0011646714992821217, 0.0011646714992821217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011646714992821217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116467
Iteration 2/1000 | Loss: 0.00004411
Iteration 3/1000 | Loss: 0.00002944
Iteration 4/1000 | Loss: 0.00002087
Iteration 5/1000 | Loss: 0.00001898
Iteration 6/1000 | Loss: 0.00001808
Iteration 7/1000 | Loss: 0.00001755
Iteration 8/1000 | Loss: 0.00001705
Iteration 9/1000 | Loss: 0.00001667
Iteration 10/1000 | Loss: 0.00001646
Iteration 11/1000 | Loss: 0.00001638
Iteration 12/1000 | Loss: 0.00001628
Iteration 13/1000 | Loss: 0.00001610
Iteration 14/1000 | Loss: 0.00001603
Iteration 15/1000 | Loss: 0.00001600
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001595
Iteration 18/1000 | Loss: 0.00001588
Iteration 19/1000 | Loss: 0.00001586
Iteration 20/1000 | Loss: 0.00001585
Iteration 21/1000 | Loss: 0.00001584
Iteration 22/1000 | Loss: 0.00001584
Iteration 23/1000 | Loss: 0.00001583
Iteration 24/1000 | Loss: 0.00001582
Iteration 25/1000 | Loss: 0.00001581
Iteration 26/1000 | Loss: 0.00001575
Iteration 27/1000 | Loss: 0.00001575
Iteration 28/1000 | Loss: 0.00001573
Iteration 29/1000 | Loss: 0.00001572
Iteration 30/1000 | Loss: 0.00001572
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001571
Iteration 34/1000 | Loss: 0.00001570
Iteration 35/1000 | Loss: 0.00001570
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00001566
Iteration 38/1000 | Loss: 0.00001566
Iteration 39/1000 | Loss: 0.00001566
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001565
Iteration 42/1000 | Loss: 0.00001565
Iteration 43/1000 | Loss: 0.00001565
Iteration 44/1000 | Loss: 0.00001565
Iteration 45/1000 | Loss: 0.00001565
Iteration 46/1000 | Loss: 0.00001564
Iteration 47/1000 | Loss: 0.00001564
Iteration 48/1000 | Loss: 0.00001563
Iteration 49/1000 | Loss: 0.00001563
Iteration 50/1000 | Loss: 0.00001562
Iteration 51/1000 | Loss: 0.00001562
Iteration 52/1000 | Loss: 0.00001561
Iteration 53/1000 | Loss: 0.00001561
Iteration 54/1000 | Loss: 0.00001561
Iteration 55/1000 | Loss: 0.00001561
Iteration 56/1000 | Loss: 0.00001560
Iteration 57/1000 | Loss: 0.00001560
Iteration 58/1000 | Loss: 0.00001559
Iteration 59/1000 | Loss: 0.00001559
Iteration 60/1000 | Loss: 0.00001558
Iteration 61/1000 | Loss: 0.00001558
Iteration 62/1000 | Loss: 0.00001558
Iteration 63/1000 | Loss: 0.00001558
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001557
Iteration 66/1000 | Loss: 0.00001557
Iteration 67/1000 | Loss: 0.00001557
Iteration 68/1000 | Loss: 0.00001556
Iteration 69/1000 | Loss: 0.00001556
Iteration 70/1000 | Loss: 0.00001556
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001555
Iteration 73/1000 | Loss: 0.00001555
Iteration 74/1000 | Loss: 0.00001555
Iteration 75/1000 | Loss: 0.00001554
Iteration 76/1000 | Loss: 0.00001554
Iteration 77/1000 | Loss: 0.00001553
Iteration 78/1000 | Loss: 0.00001553
Iteration 79/1000 | Loss: 0.00001553
Iteration 80/1000 | Loss: 0.00001553
Iteration 81/1000 | Loss: 0.00001553
Iteration 82/1000 | Loss: 0.00001553
Iteration 83/1000 | Loss: 0.00001553
Iteration 84/1000 | Loss: 0.00001553
Iteration 85/1000 | Loss: 0.00001552
Iteration 86/1000 | Loss: 0.00001552
Iteration 87/1000 | Loss: 0.00001552
Iteration 88/1000 | Loss: 0.00001552
Iteration 89/1000 | Loss: 0.00001552
Iteration 90/1000 | Loss: 0.00001552
Iteration 91/1000 | Loss: 0.00001552
Iteration 92/1000 | Loss: 0.00001552
Iteration 93/1000 | Loss: 0.00001552
Iteration 94/1000 | Loss: 0.00001552
Iteration 95/1000 | Loss: 0.00001552
Iteration 96/1000 | Loss: 0.00001552
Iteration 97/1000 | Loss: 0.00001552
Iteration 98/1000 | Loss: 0.00001551
Iteration 99/1000 | Loss: 0.00001551
Iteration 100/1000 | Loss: 0.00001551
Iteration 101/1000 | Loss: 0.00001551
Iteration 102/1000 | Loss: 0.00001551
Iteration 103/1000 | Loss: 0.00001551
Iteration 104/1000 | Loss: 0.00001550
Iteration 105/1000 | Loss: 0.00001550
Iteration 106/1000 | Loss: 0.00001550
Iteration 107/1000 | Loss: 0.00001550
Iteration 108/1000 | Loss: 0.00001550
Iteration 109/1000 | Loss: 0.00001550
Iteration 110/1000 | Loss: 0.00001550
Iteration 111/1000 | Loss: 0.00001550
Iteration 112/1000 | Loss: 0.00001550
Iteration 113/1000 | Loss: 0.00001550
Iteration 114/1000 | Loss: 0.00001550
Iteration 115/1000 | Loss: 0.00001549
Iteration 116/1000 | Loss: 0.00001549
Iteration 117/1000 | Loss: 0.00001549
Iteration 118/1000 | Loss: 0.00001549
Iteration 119/1000 | Loss: 0.00001549
Iteration 120/1000 | Loss: 0.00001549
Iteration 121/1000 | Loss: 0.00001549
Iteration 122/1000 | Loss: 0.00001549
Iteration 123/1000 | Loss: 0.00001548
Iteration 124/1000 | Loss: 0.00001548
Iteration 125/1000 | Loss: 0.00001548
Iteration 126/1000 | Loss: 0.00001548
Iteration 127/1000 | Loss: 0.00001547
Iteration 128/1000 | Loss: 0.00001547
Iteration 129/1000 | Loss: 0.00001547
Iteration 130/1000 | Loss: 0.00001547
Iteration 131/1000 | Loss: 0.00001547
Iteration 132/1000 | Loss: 0.00001547
Iteration 133/1000 | Loss: 0.00001547
Iteration 134/1000 | Loss: 0.00001547
Iteration 135/1000 | Loss: 0.00001547
Iteration 136/1000 | Loss: 0.00001547
Iteration 137/1000 | Loss: 0.00001547
Iteration 138/1000 | Loss: 0.00001547
Iteration 139/1000 | Loss: 0.00001547
Iteration 140/1000 | Loss: 0.00001546
Iteration 141/1000 | Loss: 0.00001546
Iteration 142/1000 | Loss: 0.00001546
Iteration 143/1000 | Loss: 0.00001546
Iteration 144/1000 | Loss: 0.00001546
Iteration 145/1000 | Loss: 0.00001546
Iteration 146/1000 | Loss: 0.00001545
Iteration 147/1000 | Loss: 0.00001545
Iteration 148/1000 | Loss: 0.00001545
Iteration 149/1000 | Loss: 0.00001545
Iteration 150/1000 | Loss: 0.00001545
Iteration 151/1000 | Loss: 0.00001544
Iteration 152/1000 | Loss: 0.00001544
Iteration 153/1000 | Loss: 0.00001544
Iteration 154/1000 | Loss: 0.00001544
Iteration 155/1000 | Loss: 0.00001544
Iteration 156/1000 | Loss: 0.00001544
Iteration 157/1000 | Loss: 0.00001544
Iteration 158/1000 | Loss: 0.00001544
Iteration 159/1000 | Loss: 0.00001544
Iteration 160/1000 | Loss: 0.00001544
Iteration 161/1000 | Loss: 0.00001544
Iteration 162/1000 | Loss: 0.00001544
Iteration 163/1000 | Loss: 0.00001544
Iteration 164/1000 | Loss: 0.00001543
Iteration 165/1000 | Loss: 0.00001543
Iteration 166/1000 | Loss: 0.00001543
Iteration 167/1000 | Loss: 0.00001543
Iteration 168/1000 | Loss: 0.00001543
Iteration 169/1000 | Loss: 0.00001543
Iteration 170/1000 | Loss: 0.00001543
Iteration 171/1000 | Loss: 0.00001543
Iteration 172/1000 | Loss: 0.00001543
Iteration 173/1000 | Loss: 0.00001543
Iteration 174/1000 | Loss: 0.00001543
Iteration 175/1000 | Loss: 0.00001542
Iteration 176/1000 | Loss: 0.00001542
Iteration 177/1000 | Loss: 0.00001542
Iteration 178/1000 | Loss: 0.00001542
Iteration 179/1000 | Loss: 0.00001542
Iteration 180/1000 | Loss: 0.00001542
Iteration 181/1000 | Loss: 0.00001542
Iteration 182/1000 | Loss: 0.00001542
Iteration 183/1000 | Loss: 0.00001542
Iteration 184/1000 | Loss: 0.00001542
Iteration 185/1000 | Loss: 0.00001542
Iteration 186/1000 | Loss: 0.00001542
Iteration 187/1000 | Loss: 0.00001542
Iteration 188/1000 | Loss: 0.00001541
Iteration 189/1000 | Loss: 0.00001541
Iteration 190/1000 | Loss: 0.00001541
Iteration 191/1000 | Loss: 0.00001541
Iteration 192/1000 | Loss: 0.00001541
Iteration 193/1000 | Loss: 0.00001541
Iteration 194/1000 | Loss: 0.00001541
Iteration 195/1000 | Loss: 0.00001541
Iteration 196/1000 | Loss: 0.00001541
Iteration 197/1000 | Loss: 0.00001541
Iteration 198/1000 | Loss: 0.00001541
Iteration 199/1000 | Loss: 0.00001541
Iteration 200/1000 | Loss: 0.00001541
Iteration 201/1000 | Loss: 0.00001541
Iteration 202/1000 | Loss: 0.00001541
Iteration 203/1000 | Loss: 0.00001541
Iteration 204/1000 | Loss: 0.00001540
Iteration 205/1000 | Loss: 0.00001540
Iteration 206/1000 | Loss: 0.00001540
Iteration 207/1000 | Loss: 0.00001540
Iteration 208/1000 | Loss: 0.00001540
Iteration 209/1000 | Loss: 0.00001540
Iteration 210/1000 | Loss: 0.00001540
Iteration 211/1000 | Loss: 0.00001540
Iteration 212/1000 | Loss: 0.00001540
Iteration 213/1000 | Loss: 0.00001540
Iteration 214/1000 | Loss: 0.00001540
Iteration 215/1000 | Loss: 0.00001540
Iteration 216/1000 | Loss: 0.00001539
Iteration 217/1000 | Loss: 0.00001539
Iteration 218/1000 | Loss: 0.00001539
Iteration 219/1000 | Loss: 0.00001539
Iteration 220/1000 | Loss: 0.00001539
Iteration 221/1000 | Loss: 0.00001539
Iteration 222/1000 | Loss: 0.00001539
Iteration 223/1000 | Loss: 0.00001539
Iteration 224/1000 | Loss: 0.00001539
Iteration 225/1000 | Loss: 0.00001539
Iteration 226/1000 | Loss: 0.00001539
Iteration 227/1000 | Loss: 0.00001539
Iteration 228/1000 | Loss: 0.00001539
Iteration 229/1000 | Loss: 0.00001539
Iteration 230/1000 | Loss: 0.00001539
Iteration 231/1000 | Loss: 0.00001539
Iteration 232/1000 | Loss: 0.00001539
Iteration 233/1000 | Loss: 0.00001539
Iteration 234/1000 | Loss: 0.00001539
Iteration 235/1000 | Loss: 0.00001538
Iteration 236/1000 | Loss: 0.00001538
Iteration 237/1000 | Loss: 0.00001538
Iteration 238/1000 | Loss: 0.00001538
Iteration 239/1000 | Loss: 0.00001538
Iteration 240/1000 | Loss: 0.00001538
Iteration 241/1000 | Loss: 0.00001538
Iteration 242/1000 | Loss: 0.00001538
Iteration 243/1000 | Loss: 0.00001538
Iteration 244/1000 | Loss: 0.00001538
Iteration 245/1000 | Loss: 0.00001538
Iteration 246/1000 | Loss: 0.00001538
Iteration 247/1000 | Loss: 0.00001538
Iteration 248/1000 | Loss: 0.00001538
Iteration 249/1000 | Loss: 0.00001538
Iteration 250/1000 | Loss: 0.00001538
Iteration 251/1000 | Loss: 0.00001538
Iteration 252/1000 | Loss: 0.00001538
Iteration 253/1000 | Loss: 0.00001538
Iteration 254/1000 | Loss: 0.00001538
Iteration 255/1000 | Loss: 0.00001538
Iteration 256/1000 | Loss: 0.00001538
Iteration 257/1000 | Loss: 0.00001538
Iteration 258/1000 | Loss: 0.00001537
Iteration 259/1000 | Loss: 0.00001537
Iteration 260/1000 | Loss: 0.00001537
Iteration 261/1000 | Loss: 0.00001537
Iteration 262/1000 | Loss: 0.00001537
Iteration 263/1000 | Loss: 0.00001537
Iteration 264/1000 | Loss: 0.00001537
Iteration 265/1000 | Loss: 0.00001537
Iteration 266/1000 | Loss: 0.00001537
Iteration 267/1000 | Loss: 0.00001537
Iteration 268/1000 | Loss: 0.00001537
Iteration 269/1000 | Loss: 0.00001537
Iteration 270/1000 | Loss: 0.00001537
Iteration 271/1000 | Loss: 0.00001537
Iteration 272/1000 | Loss: 0.00001537
Iteration 273/1000 | Loss: 0.00001537
Iteration 274/1000 | Loss: 0.00001537
Iteration 275/1000 | Loss: 0.00001537
Iteration 276/1000 | Loss: 0.00001537
Iteration 277/1000 | Loss: 0.00001537
Iteration 278/1000 | Loss: 0.00001537
Iteration 279/1000 | Loss: 0.00001537
Iteration 280/1000 | Loss: 0.00001537
Iteration 281/1000 | Loss: 0.00001537
Iteration 282/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 282. Stopping optimization.
Last 5 losses: [1.5368592357845046e-05, 1.5368592357845046e-05, 1.5368592357845046e-05, 1.5368592357845046e-05, 1.5368592357845046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5368592357845046e-05

Optimization complete. Final v2v error: 3.361938238143921 mm

Highest mean error: 3.712484121322632 mm for frame 48

Lowest mean error: 3.0275979042053223 mm for frame 120

Saving results

Total time: 47.19902992248535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476898
Iteration 2/25 | Loss: 0.00135564
Iteration 3/25 | Loss: 0.00129689
Iteration 4/25 | Loss: 0.00128802
Iteration 5/25 | Loss: 0.00128447
Iteration 6/25 | Loss: 0.00128411
Iteration 7/25 | Loss: 0.00128411
Iteration 8/25 | Loss: 0.00128411
Iteration 9/25 | Loss: 0.00128411
Iteration 10/25 | Loss: 0.00128411
Iteration 11/25 | Loss: 0.00128411
Iteration 12/25 | Loss: 0.00128411
Iteration 13/25 | Loss: 0.00128411
Iteration 14/25 | Loss: 0.00128411
Iteration 15/25 | Loss: 0.00128411
Iteration 16/25 | Loss: 0.00128411
Iteration 17/25 | Loss: 0.00128411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012841133866459131, 0.0012841133866459131, 0.0012841133866459131, 0.0012841133866459131, 0.0012841133866459131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012841133866459131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.71466780
Iteration 2/25 | Loss: 0.00084397
Iteration 3/25 | Loss: 0.00084396
Iteration 4/25 | Loss: 0.00084396
Iteration 5/25 | Loss: 0.00084396
Iteration 6/25 | Loss: 0.00084396
Iteration 7/25 | Loss: 0.00084396
Iteration 8/25 | Loss: 0.00084396
Iteration 9/25 | Loss: 0.00084396
Iteration 10/25 | Loss: 0.00084396
Iteration 11/25 | Loss: 0.00084396
Iteration 12/25 | Loss: 0.00084396
Iteration 13/25 | Loss: 0.00084396
Iteration 14/25 | Loss: 0.00084396
Iteration 15/25 | Loss: 0.00084396
Iteration 16/25 | Loss: 0.00084396
Iteration 17/25 | Loss: 0.00084396
Iteration 18/25 | Loss: 0.00084396
Iteration 19/25 | Loss: 0.00084396
Iteration 20/25 | Loss: 0.00084396
Iteration 21/25 | Loss: 0.00084396
Iteration 22/25 | Loss: 0.00084396
Iteration 23/25 | Loss: 0.00084396
Iteration 24/25 | Loss: 0.00084396
Iteration 25/25 | Loss: 0.00084396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084396
Iteration 2/1000 | Loss: 0.00003241
Iteration 3/1000 | Loss: 0.00002110
Iteration 4/1000 | Loss: 0.00001798
Iteration 5/1000 | Loss: 0.00001664
Iteration 6/1000 | Loss: 0.00001583
Iteration 7/1000 | Loss: 0.00001528
Iteration 8/1000 | Loss: 0.00001478
Iteration 9/1000 | Loss: 0.00001445
Iteration 10/1000 | Loss: 0.00001414
Iteration 11/1000 | Loss: 0.00001391
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001370
Iteration 14/1000 | Loss: 0.00001361
Iteration 15/1000 | Loss: 0.00001354
Iteration 16/1000 | Loss: 0.00001353
Iteration 17/1000 | Loss: 0.00001353
Iteration 18/1000 | Loss: 0.00001353
Iteration 19/1000 | Loss: 0.00001352
Iteration 20/1000 | Loss: 0.00001352
Iteration 21/1000 | Loss: 0.00001351
Iteration 22/1000 | Loss: 0.00001349
Iteration 23/1000 | Loss: 0.00001349
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001347
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001345
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001342
Iteration 30/1000 | Loss: 0.00001342
Iteration 31/1000 | Loss: 0.00001341
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001341
Iteration 34/1000 | Loss: 0.00001340
Iteration 35/1000 | Loss: 0.00001340
Iteration 36/1000 | Loss: 0.00001339
Iteration 37/1000 | Loss: 0.00001339
Iteration 38/1000 | Loss: 0.00001336
Iteration 39/1000 | Loss: 0.00001332
Iteration 40/1000 | Loss: 0.00001330
Iteration 41/1000 | Loss: 0.00001329
Iteration 42/1000 | Loss: 0.00001328
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001326
Iteration 45/1000 | Loss: 0.00001326
Iteration 46/1000 | Loss: 0.00001326
Iteration 47/1000 | Loss: 0.00001325
Iteration 48/1000 | Loss: 0.00001325
Iteration 49/1000 | Loss: 0.00001324
Iteration 50/1000 | Loss: 0.00001323
Iteration 51/1000 | Loss: 0.00001322
Iteration 52/1000 | Loss: 0.00001322
Iteration 53/1000 | Loss: 0.00001322
Iteration 54/1000 | Loss: 0.00001322
Iteration 55/1000 | Loss: 0.00001322
Iteration 56/1000 | Loss: 0.00001322
Iteration 57/1000 | Loss: 0.00001322
Iteration 58/1000 | Loss: 0.00001322
Iteration 59/1000 | Loss: 0.00001321
Iteration 60/1000 | Loss: 0.00001321
Iteration 61/1000 | Loss: 0.00001319
Iteration 62/1000 | Loss: 0.00001319
Iteration 63/1000 | Loss: 0.00001319
Iteration 64/1000 | Loss: 0.00001319
Iteration 65/1000 | Loss: 0.00001319
Iteration 66/1000 | Loss: 0.00001318
Iteration 67/1000 | Loss: 0.00001315
Iteration 68/1000 | Loss: 0.00001315
Iteration 69/1000 | Loss: 0.00001314
Iteration 70/1000 | Loss: 0.00001314
Iteration 71/1000 | Loss: 0.00001314
Iteration 72/1000 | Loss: 0.00001313
Iteration 73/1000 | Loss: 0.00001313
Iteration 74/1000 | Loss: 0.00001312
Iteration 75/1000 | Loss: 0.00001312
Iteration 76/1000 | Loss: 0.00001312
Iteration 77/1000 | Loss: 0.00001311
Iteration 78/1000 | Loss: 0.00001311
Iteration 79/1000 | Loss: 0.00001310
Iteration 80/1000 | Loss: 0.00001310
Iteration 81/1000 | Loss: 0.00001310
Iteration 82/1000 | Loss: 0.00001310
Iteration 83/1000 | Loss: 0.00001309
Iteration 84/1000 | Loss: 0.00001309
Iteration 85/1000 | Loss: 0.00001308
Iteration 86/1000 | Loss: 0.00001308
Iteration 87/1000 | Loss: 0.00001306
Iteration 88/1000 | Loss: 0.00001306
Iteration 89/1000 | Loss: 0.00001306
Iteration 90/1000 | Loss: 0.00001306
Iteration 91/1000 | Loss: 0.00001306
Iteration 92/1000 | Loss: 0.00001306
Iteration 93/1000 | Loss: 0.00001305
Iteration 94/1000 | Loss: 0.00001305
Iteration 95/1000 | Loss: 0.00001305
Iteration 96/1000 | Loss: 0.00001305
Iteration 97/1000 | Loss: 0.00001305
Iteration 98/1000 | Loss: 0.00001305
Iteration 99/1000 | Loss: 0.00001304
Iteration 100/1000 | Loss: 0.00001304
Iteration 101/1000 | Loss: 0.00001303
Iteration 102/1000 | Loss: 0.00001303
Iteration 103/1000 | Loss: 0.00001303
Iteration 104/1000 | Loss: 0.00001302
Iteration 105/1000 | Loss: 0.00001302
Iteration 106/1000 | Loss: 0.00001302
Iteration 107/1000 | Loss: 0.00001301
Iteration 108/1000 | Loss: 0.00001301
Iteration 109/1000 | Loss: 0.00001301
Iteration 110/1000 | Loss: 0.00001301
Iteration 111/1000 | Loss: 0.00001301
Iteration 112/1000 | Loss: 0.00001301
Iteration 113/1000 | Loss: 0.00001300
Iteration 114/1000 | Loss: 0.00001300
Iteration 115/1000 | Loss: 0.00001300
Iteration 116/1000 | Loss: 0.00001300
Iteration 117/1000 | Loss: 0.00001300
Iteration 118/1000 | Loss: 0.00001300
Iteration 119/1000 | Loss: 0.00001300
Iteration 120/1000 | Loss: 0.00001300
Iteration 121/1000 | Loss: 0.00001300
Iteration 122/1000 | Loss: 0.00001299
Iteration 123/1000 | Loss: 0.00001299
Iteration 124/1000 | Loss: 0.00001299
Iteration 125/1000 | Loss: 0.00001299
Iteration 126/1000 | Loss: 0.00001299
Iteration 127/1000 | Loss: 0.00001298
Iteration 128/1000 | Loss: 0.00001298
Iteration 129/1000 | Loss: 0.00001298
Iteration 130/1000 | Loss: 0.00001298
Iteration 131/1000 | Loss: 0.00001298
Iteration 132/1000 | Loss: 0.00001298
Iteration 133/1000 | Loss: 0.00001298
Iteration 134/1000 | Loss: 0.00001298
Iteration 135/1000 | Loss: 0.00001298
Iteration 136/1000 | Loss: 0.00001298
Iteration 137/1000 | Loss: 0.00001298
Iteration 138/1000 | Loss: 0.00001298
Iteration 139/1000 | Loss: 0.00001297
Iteration 140/1000 | Loss: 0.00001297
Iteration 141/1000 | Loss: 0.00001297
Iteration 142/1000 | Loss: 0.00001297
Iteration 143/1000 | Loss: 0.00001297
Iteration 144/1000 | Loss: 0.00001297
Iteration 145/1000 | Loss: 0.00001297
Iteration 146/1000 | Loss: 0.00001297
Iteration 147/1000 | Loss: 0.00001296
Iteration 148/1000 | Loss: 0.00001296
Iteration 149/1000 | Loss: 0.00001296
Iteration 150/1000 | Loss: 0.00001296
Iteration 151/1000 | Loss: 0.00001296
Iteration 152/1000 | Loss: 0.00001296
Iteration 153/1000 | Loss: 0.00001296
Iteration 154/1000 | Loss: 0.00001296
Iteration 155/1000 | Loss: 0.00001296
Iteration 156/1000 | Loss: 0.00001296
Iteration 157/1000 | Loss: 0.00001296
Iteration 158/1000 | Loss: 0.00001296
Iteration 159/1000 | Loss: 0.00001295
Iteration 160/1000 | Loss: 0.00001295
Iteration 161/1000 | Loss: 0.00001295
Iteration 162/1000 | Loss: 0.00001295
Iteration 163/1000 | Loss: 0.00001295
Iteration 164/1000 | Loss: 0.00001295
Iteration 165/1000 | Loss: 0.00001295
Iteration 166/1000 | Loss: 0.00001295
Iteration 167/1000 | Loss: 0.00001295
Iteration 168/1000 | Loss: 0.00001294
Iteration 169/1000 | Loss: 0.00001294
Iteration 170/1000 | Loss: 0.00001294
Iteration 171/1000 | Loss: 0.00001294
Iteration 172/1000 | Loss: 0.00001294
Iteration 173/1000 | Loss: 0.00001294
Iteration 174/1000 | Loss: 0.00001294
Iteration 175/1000 | Loss: 0.00001294
Iteration 176/1000 | Loss: 0.00001294
Iteration 177/1000 | Loss: 0.00001294
Iteration 178/1000 | Loss: 0.00001294
Iteration 179/1000 | Loss: 0.00001294
Iteration 180/1000 | Loss: 0.00001294
Iteration 181/1000 | Loss: 0.00001294
Iteration 182/1000 | Loss: 0.00001294
Iteration 183/1000 | Loss: 0.00001293
Iteration 184/1000 | Loss: 0.00001293
Iteration 185/1000 | Loss: 0.00001293
Iteration 186/1000 | Loss: 0.00001293
Iteration 187/1000 | Loss: 0.00001293
Iteration 188/1000 | Loss: 0.00001293
Iteration 189/1000 | Loss: 0.00001293
Iteration 190/1000 | Loss: 0.00001293
Iteration 191/1000 | Loss: 0.00001293
Iteration 192/1000 | Loss: 0.00001293
Iteration 193/1000 | Loss: 0.00001293
Iteration 194/1000 | Loss: 0.00001293
Iteration 195/1000 | Loss: 0.00001293
Iteration 196/1000 | Loss: 0.00001293
Iteration 197/1000 | Loss: 0.00001293
Iteration 198/1000 | Loss: 0.00001293
Iteration 199/1000 | Loss: 0.00001293
Iteration 200/1000 | Loss: 0.00001293
Iteration 201/1000 | Loss: 0.00001292
Iteration 202/1000 | Loss: 0.00001292
Iteration 203/1000 | Loss: 0.00001292
Iteration 204/1000 | Loss: 0.00001292
Iteration 205/1000 | Loss: 0.00001292
Iteration 206/1000 | Loss: 0.00001292
Iteration 207/1000 | Loss: 0.00001292
Iteration 208/1000 | Loss: 0.00001292
Iteration 209/1000 | Loss: 0.00001292
Iteration 210/1000 | Loss: 0.00001292
Iteration 211/1000 | Loss: 0.00001292
Iteration 212/1000 | Loss: 0.00001292
Iteration 213/1000 | Loss: 0.00001292
Iteration 214/1000 | Loss: 0.00001292
Iteration 215/1000 | Loss: 0.00001292
Iteration 216/1000 | Loss: 0.00001292
Iteration 217/1000 | Loss: 0.00001292
Iteration 218/1000 | Loss: 0.00001292
Iteration 219/1000 | Loss: 0.00001292
Iteration 220/1000 | Loss: 0.00001292
Iteration 221/1000 | Loss: 0.00001292
Iteration 222/1000 | Loss: 0.00001292
Iteration 223/1000 | Loss: 0.00001292
Iteration 224/1000 | Loss: 0.00001292
Iteration 225/1000 | Loss: 0.00001292
Iteration 226/1000 | Loss: 0.00001292
Iteration 227/1000 | Loss: 0.00001292
Iteration 228/1000 | Loss: 0.00001292
Iteration 229/1000 | Loss: 0.00001292
Iteration 230/1000 | Loss: 0.00001292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.291997978114523e-05, 1.291997978114523e-05, 1.291997978114523e-05, 1.291997978114523e-05, 1.291997978114523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.291997978114523e-05

Optimization complete. Final v2v error: 3.081165313720703 mm

Highest mean error: 3.3815484046936035 mm for frame 79

Lowest mean error: 2.8501203060150146 mm for frame 124

Saving results

Total time: 43.0407497882843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409704
Iteration 2/25 | Loss: 0.00146974
Iteration 3/25 | Loss: 0.00134179
Iteration 4/25 | Loss: 0.00131881
Iteration 5/25 | Loss: 0.00131080
Iteration 6/25 | Loss: 0.00130699
Iteration 7/25 | Loss: 0.00132140
Iteration 8/25 | Loss: 0.00129507
Iteration 9/25 | Loss: 0.00128855
Iteration 10/25 | Loss: 0.00128648
Iteration 11/25 | Loss: 0.00128603
Iteration 12/25 | Loss: 0.00128571
Iteration 13/25 | Loss: 0.00128559
Iteration 14/25 | Loss: 0.00128558
Iteration 15/25 | Loss: 0.00128558
Iteration 16/25 | Loss: 0.00128558
Iteration 17/25 | Loss: 0.00128558
Iteration 18/25 | Loss: 0.00128558
Iteration 19/25 | Loss: 0.00128558
Iteration 20/25 | Loss: 0.00128558
Iteration 21/25 | Loss: 0.00128558
Iteration 22/25 | Loss: 0.00128558
Iteration 23/25 | Loss: 0.00128558
Iteration 24/25 | Loss: 0.00128558
Iteration 25/25 | Loss: 0.00128558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38803113
Iteration 2/25 | Loss: 0.00077334
Iteration 3/25 | Loss: 0.00077333
Iteration 4/25 | Loss: 0.00077333
Iteration 5/25 | Loss: 0.00077333
Iteration 6/25 | Loss: 0.00077333
Iteration 7/25 | Loss: 0.00077333
Iteration 8/25 | Loss: 0.00077333
Iteration 9/25 | Loss: 0.00077333
Iteration 10/25 | Loss: 0.00077333
Iteration 11/25 | Loss: 0.00077333
Iteration 12/25 | Loss: 0.00077333
Iteration 13/25 | Loss: 0.00077333
Iteration 14/25 | Loss: 0.00077333
Iteration 15/25 | Loss: 0.00077333
Iteration 16/25 | Loss: 0.00077333
Iteration 17/25 | Loss: 0.00077333
Iteration 18/25 | Loss: 0.00077333
Iteration 19/25 | Loss: 0.00077333
Iteration 20/25 | Loss: 0.00077333
Iteration 21/25 | Loss: 0.00077333
Iteration 22/25 | Loss: 0.00077333
Iteration 23/25 | Loss: 0.00077333
Iteration 24/25 | Loss: 0.00077333
Iteration 25/25 | Loss: 0.00077333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077333
Iteration 2/1000 | Loss: 0.00002405
Iteration 3/1000 | Loss: 0.00001906
Iteration 4/1000 | Loss: 0.00001764
Iteration 5/1000 | Loss: 0.00001662
Iteration 6/1000 | Loss: 0.00001602
Iteration 7/1000 | Loss: 0.00001566
Iteration 8/1000 | Loss: 0.00001523
Iteration 9/1000 | Loss: 0.00001499
Iteration 10/1000 | Loss: 0.00001487
Iteration 11/1000 | Loss: 0.00001479
Iteration 12/1000 | Loss: 0.00001478
Iteration 13/1000 | Loss: 0.00001478
Iteration 14/1000 | Loss: 0.00001477
Iteration 15/1000 | Loss: 0.00001475
Iteration 16/1000 | Loss: 0.00001473
Iteration 17/1000 | Loss: 0.00001470
Iteration 18/1000 | Loss: 0.00001465
Iteration 19/1000 | Loss: 0.00001460
Iteration 20/1000 | Loss: 0.00001460
Iteration 21/1000 | Loss: 0.00001460
Iteration 22/1000 | Loss: 0.00001460
Iteration 23/1000 | Loss: 0.00001460
Iteration 24/1000 | Loss: 0.00001459
Iteration 25/1000 | Loss: 0.00001459
Iteration 26/1000 | Loss: 0.00001457
Iteration 27/1000 | Loss: 0.00001456
Iteration 28/1000 | Loss: 0.00001455
Iteration 29/1000 | Loss: 0.00001454
Iteration 30/1000 | Loss: 0.00001454
Iteration 31/1000 | Loss: 0.00001446
Iteration 32/1000 | Loss: 0.00001443
Iteration 33/1000 | Loss: 0.00001442
Iteration 34/1000 | Loss: 0.00001442
Iteration 35/1000 | Loss: 0.00001439
Iteration 36/1000 | Loss: 0.00001437
Iteration 37/1000 | Loss: 0.00001437
Iteration 38/1000 | Loss: 0.00001436
Iteration 39/1000 | Loss: 0.00001435
Iteration 40/1000 | Loss: 0.00001434
Iteration 41/1000 | Loss: 0.00001434
Iteration 42/1000 | Loss: 0.00001433
Iteration 43/1000 | Loss: 0.00001433
Iteration 44/1000 | Loss: 0.00001433
Iteration 45/1000 | Loss: 0.00001432
Iteration 46/1000 | Loss: 0.00001431
Iteration 47/1000 | Loss: 0.00001431
Iteration 48/1000 | Loss: 0.00001430
Iteration 49/1000 | Loss: 0.00001430
Iteration 50/1000 | Loss: 0.00001429
Iteration 51/1000 | Loss: 0.00001429
Iteration 52/1000 | Loss: 0.00001429
Iteration 53/1000 | Loss: 0.00001429
Iteration 54/1000 | Loss: 0.00001429
Iteration 55/1000 | Loss: 0.00001428
Iteration 56/1000 | Loss: 0.00001428
Iteration 57/1000 | Loss: 0.00001428
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001427
Iteration 61/1000 | Loss: 0.00001427
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001426
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001426
Iteration 66/1000 | Loss: 0.00001425
Iteration 67/1000 | Loss: 0.00001425
Iteration 68/1000 | Loss: 0.00001425
Iteration 69/1000 | Loss: 0.00001425
Iteration 70/1000 | Loss: 0.00001424
Iteration 71/1000 | Loss: 0.00001424
Iteration 72/1000 | Loss: 0.00001423
Iteration 73/1000 | Loss: 0.00001423
Iteration 74/1000 | Loss: 0.00001423
Iteration 75/1000 | Loss: 0.00001423
Iteration 76/1000 | Loss: 0.00001422
Iteration 77/1000 | Loss: 0.00001422
Iteration 78/1000 | Loss: 0.00001422
Iteration 79/1000 | Loss: 0.00001421
Iteration 80/1000 | Loss: 0.00001421
Iteration 81/1000 | Loss: 0.00001421
Iteration 82/1000 | Loss: 0.00001421
Iteration 83/1000 | Loss: 0.00001421
Iteration 84/1000 | Loss: 0.00001421
Iteration 85/1000 | Loss: 0.00001421
Iteration 86/1000 | Loss: 0.00001421
Iteration 87/1000 | Loss: 0.00001421
Iteration 88/1000 | Loss: 0.00001421
Iteration 89/1000 | Loss: 0.00001421
Iteration 90/1000 | Loss: 0.00001421
Iteration 91/1000 | Loss: 0.00001421
Iteration 92/1000 | Loss: 0.00001421
Iteration 93/1000 | Loss: 0.00001421
Iteration 94/1000 | Loss: 0.00001421
Iteration 95/1000 | Loss: 0.00001421
Iteration 96/1000 | Loss: 0.00001421
Iteration 97/1000 | Loss: 0.00001421
Iteration 98/1000 | Loss: 0.00001421
Iteration 99/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.4213840586307924e-05, 1.4213840586307924e-05, 1.4213840586307924e-05, 1.4213840586307924e-05, 1.4213840586307924e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4213840586307924e-05

Optimization complete. Final v2v error: 3.237640857696533 mm

Highest mean error: 3.5801990032196045 mm for frame 132

Lowest mean error: 3.058971881866455 mm for frame 238

Saving results

Total time: 54.201783657073975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880499
Iteration 2/25 | Loss: 0.00181328
Iteration 3/25 | Loss: 0.00156778
Iteration 4/25 | Loss: 0.00155606
Iteration 5/25 | Loss: 0.00155373
Iteration 6/25 | Loss: 0.00155373
Iteration 7/25 | Loss: 0.00155373
Iteration 8/25 | Loss: 0.00155373
Iteration 9/25 | Loss: 0.00155373
Iteration 10/25 | Loss: 0.00155373
Iteration 11/25 | Loss: 0.00155373
Iteration 12/25 | Loss: 0.00155373
Iteration 13/25 | Loss: 0.00155373
Iteration 14/25 | Loss: 0.00155373
Iteration 15/25 | Loss: 0.00155373
Iteration 16/25 | Loss: 0.00155373
Iteration 17/25 | Loss: 0.00155373
Iteration 18/25 | Loss: 0.00155373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015537263825535774, 0.0015537263825535774, 0.0015537263825535774, 0.0015537263825535774, 0.0015537263825535774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015537263825535774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58579630
Iteration 2/25 | Loss: 0.00121594
Iteration 3/25 | Loss: 0.00121594
Iteration 4/25 | Loss: 0.00121594
Iteration 5/25 | Loss: 0.00121594
Iteration 6/25 | Loss: 0.00121594
Iteration 7/25 | Loss: 0.00121594
Iteration 8/25 | Loss: 0.00121594
Iteration 9/25 | Loss: 0.00121594
Iteration 10/25 | Loss: 0.00121594
Iteration 11/25 | Loss: 0.00121594
Iteration 12/25 | Loss: 0.00121594
Iteration 13/25 | Loss: 0.00121594
Iteration 14/25 | Loss: 0.00121594
Iteration 15/25 | Loss: 0.00121594
Iteration 16/25 | Loss: 0.00121594
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012159354519098997, 0.0012159354519098997, 0.0012159354519098997, 0.0012159354519098997, 0.0012159354519098997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012159354519098997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121594
Iteration 2/1000 | Loss: 0.00005562
Iteration 3/1000 | Loss: 0.00003837
Iteration 4/1000 | Loss: 0.00003531
Iteration 5/1000 | Loss: 0.00003405
Iteration 6/1000 | Loss: 0.00003293
Iteration 7/1000 | Loss: 0.00003232
Iteration 8/1000 | Loss: 0.00003171
Iteration 9/1000 | Loss: 0.00003140
Iteration 10/1000 | Loss: 0.00003114
Iteration 11/1000 | Loss: 0.00003095
Iteration 12/1000 | Loss: 0.00003078
Iteration 13/1000 | Loss: 0.00003071
Iteration 14/1000 | Loss: 0.00003069
Iteration 15/1000 | Loss: 0.00003068
Iteration 16/1000 | Loss: 0.00003067
Iteration 17/1000 | Loss: 0.00003066
Iteration 18/1000 | Loss: 0.00003066
Iteration 19/1000 | Loss: 0.00003066
Iteration 20/1000 | Loss: 0.00003066
Iteration 21/1000 | Loss: 0.00003066
Iteration 22/1000 | Loss: 0.00003066
Iteration 23/1000 | Loss: 0.00003066
Iteration 24/1000 | Loss: 0.00003066
Iteration 25/1000 | Loss: 0.00003066
Iteration 26/1000 | Loss: 0.00003066
Iteration 27/1000 | Loss: 0.00003066
Iteration 28/1000 | Loss: 0.00003065
Iteration 29/1000 | Loss: 0.00003065
Iteration 30/1000 | Loss: 0.00003065
Iteration 31/1000 | Loss: 0.00003065
Iteration 32/1000 | Loss: 0.00003065
Iteration 33/1000 | Loss: 0.00003065
Iteration 34/1000 | Loss: 0.00003065
Iteration 35/1000 | Loss: 0.00003065
Iteration 36/1000 | Loss: 0.00003065
Iteration 37/1000 | Loss: 0.00003065
Iteration 38/1000 | Loss: 0.00003065
Iteration 39/1000 | Loss: 0.00003064
Iteration 40/1000 | Loss: 0.00003064
Iteration 41/1000 | Loss: 0.00003064
Iteration 42/1000 | Loss: 0.00003064
Iteration 43/1000 | Loss: 0.00003063
Iteration 44/1000 | Loss: 0.00003063
Iteration 45/1000 | Loss: 0.00003063
Iteration 46/1000 | Loss: 0.00003063
Iteration 47/1000 | Loss: 0.00003063
Iteration 48/1000 | Loss: 0.00003063
Iteration 49/1000 | Loss: 0.00003063
Iteration 50/1000 | Loss: 0.00003063
Iteration 51/1000 | Loss: 0.00003063
Iteration 52/1000 | Loss: 0.00003063
Iteration 53/1000 | Loss: 0.00003063
Iteration 54/1000 | Loss: 0.00003062
Iteration 55/1000 | Loss: 0.00003062
Iteration 56/1000 | Loss: 0.00003062
Iteration 57/1000 | Loss: 0.00003061
Iteration 58/1000 | Loss: 0.00003061
Iteration 59/1000 | Loss: 0.00003061
Iteration 60/1000 | Loss: 0.00003061
Iteration 61/1000 | Loss: 0.00003060
Iteration 62/1000 | Loss: 0.00003060
Iteration 63/1000 | Loss: 0.00003060
Iteration 64/1000 | Loss: 0.00003060
Iteration 65/1000 | Loss: 0.00003060
Iteration 66/1000 | Loss: 0.00003060
Iteration 67/1000 | Loss: 0.00003060
Iteration 68/1000 | Loss: 0.00003060
Iteration 69/1000 | Loss: 0.00003060
Iteration 70/1000 | Loss: 0.00003060
Iteration 71/1000 | Loss: 0.00003060
Iteration 72/1000 | Loss: 0.00003060
Iteration 73/1000 | Loss: 0.00003060
Iteration 74/1000 | Loss: 0.00003060
Iteration 75/1000 | Loss: 0.00003060
Iteration 76/1000 | Loss: 0.00003060
Iteration 77/1000 | Loss: 0.00003060
Iteration 78/1000 | Loss: 0.00003060
Iteration 79/1000 | Loss: 0.00003060
Iteration 80/1000 | Loss: 0.00003060
Iteration 81/1000 | Loss: 0.00003060
Iteration 82/1000 | Loss: 0.00003060
Iteration 83/1000 | Loss: 0.00003060
Iteration 84/1000 | Loss: 0.00003060
Iteration 85/1000 | Loss: 0.00003060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [3.060164090129547e-05, 3.060164090129547e-05, 3.060164090129547e-05, 3.060164090129547e-05, 3.060164090129547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.060164090129547e-05

Optimization complete. Final v2v error: 4.634220123291016 mm

Highest mean error: 4.925600051879883 mm for frame 98

Lowest mean error: 4.457430839538574 mm for frame 47

Saving results

Total time: 30.44986629486084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754047
Iteration 2/25 | Loss: 0.00156725
Iteration 3/25 | Loss: 0.00140067
Iteration 4/25 | Loss: 0.00139071
Iteration 5/25 | Loss: 0.00138956
Iteration 6/25 | Loss: 0.00138956
Iteration 7/25 | Loss: 0.00138956
Iteration 8/25 | Loss: 0.00138956
Iteration 9/25 | Loss: 0.00138956
Iteration 10/25 | Loss: 0.00138956
Iteration 11/25 | Loss: 0.00138956
Iteration 12/25 | Loss: 0.00138956
Iteration 13/25 | Loss: 0.00138956
Iteration 14/25 | Loss: 0.00138956
Iteration 15/25 | Loss: 0.00138956
Iteration 16/25 | Loss: 0.00138956
Iteration 17/25 | Loss: 0.00138956
Iteration 18/25 | Loss: 0.00138956
Iteration 19/25 | Loss: 0.00138956
Iteration 20/25 | Loss: 0.00138956
Iteration 21/25 | Loss: 0.00138956
Iteration 22/25 | Loss: 0.00138956
Iteration 23/25 | Loss: 0.00138956
Iteration 24/25 | Loss: 0.00138956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001389556797221303, 0.001389556797221303, 0.001389556797221303, 0.001389556797221303, 0.001389556797221303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001389556797221303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30928993
Iteration 2/25 | Loss: 0.00084926
Iteration 3/25 | Loss: 0.00084924
Iteration 4/25 | Loss: 0.00084924
Iteration 5/25 | Loss: 0.00084924
Iteration 6/25 | Loss: 0.00084924
Iteration 7/25 | Loss: 0.00084924
Iteration 8/25 | Loss: 0.00084924
Iteration 9/25 | Loss: 0.00084923
Iteration 10/25 | Loss: 0.00084923
Iteration 11/25 | Loss: 0.00084923
Iteration 12/25 | Loss: 0.00084923
Iteration 13/25 | Loss: 0.00084923
Iteration 14/25 | Loss: 0.00084923
Iteration 15/25 | Loss: 0.00084923
Iteration 16/25 | Loss: 0.00084923
Iteration 17/25 | Loss: 0.00084923
Iteration 18/25 | Loss: 0.00084923
Iteration 19/25 | Loss: 0.00084923
Iteration 20/25 | Loss: 0.00084923
Iteration 21/25 | Loss: 0.00084923
Iteration 22/25 | Loss: 0.00084923
Iteration 23/25 | Loss: 0.00084923
Iteration 24/25 | Loss: 0.00084923
Iteration 25/25 | Loss: 0.00084923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084923
Iteration 2/1000 | Loss: 0.00003892
Iteration 3/1000 | Loss: 0.00002754
Iteration 4/1000 | Loss: 0.00002499
Iteration 5/1000 | Loss: 0.00002392
Iteration 6/1000 | Loss: 0.00002340
Iteration 7/1000 | Loss: 0.00002266
Iteration 8/1000 | Loss: 0.00002225
Iteration 9/1000 | Loss: 0.00002184
Iteration 10/1000 | Loss: 0.00002163
Iteration 11/1000 | Loss: 0.00002139
Iteration 12/1000 | Loss: 0.00002138
Iteration 13/1000 | Loss: 0.00002124
Iteration 14/1000 | Loss: 0.00002123
Iteration 15/1000 | Loss: 0.00002116
Iteration 16/1000 | Loss: 0.00002101
Iteration 17/1000 | Loss: 0.00002096
Iteration 18/1000 | Loss: 0.00002089
Iteration 19/1000 | Loss: 0.00002082
Iteration 20/1000 | Loss: 0.00002075
Iteration 21/1000 | Loss: 0.00002075
Iteration 22/1000 | Loss: 0.00002074
Iteration 23/1000 | Loss: 0.00002074
Iteration 24/1000 | Loss: 0.00002070
Iteration 25/1000 | Loss: 0.00002068
Iteration 26/1000 | Loss: 0.00002068
Iteration 27/1000 | Loss: 0.00002068
Iteration 28/1000 | Loss: 0.00002066
Iteration 29/1000 | Loss: 0.00002066
Iteration 30/1000 | Loss: 0.00002065
Iteration 31/1000 | Loss: 0.00002065
Iteration 32/1000 | Loss: 0.00002065
Iteration 33/1000 | Loss: 0.00002065
Iteration 34/1000 | Loss: 0.00002065
Iteration 35/1000 | Loss: 0.00002065
Iteration 36/1000 | Loss: 0.00002065
Iteration 37/1000 | Loss: 0.00002065
Iteration 38/1000 | Loss: 0.00002065
Iteration 39/1000 | Loss: 0.00002065
Iteration 40/1000 | Loss: 0.00002064
Iteration 41/1000 | Loss: 0.00002064
Iteration 42/1000 | Loss: 0.00002063
Iteration 43/1000 | Loss: 0.00002062
Iteration 44/1000 | Loss: 0.00002061
Iteration 45/1000 | Loss: 0.00002061
Iteration 46/1000 | Loss: 0.00002059
Iteration 47/1000 | Loss: 0.00002058
Iteration 48/1000 | Loss: 0.00002057
Iteration 49/1000 | Loss: 0.00002057
Iteration 50/1000 | Loss: 0.00002057
Iteration 51/1000 | Loss: 0.00002054
Iteration 52/1000 | Loss: 0.00002052
Iteration 53/1000 | Loss: 0.00002051
Iteration 54/1000 | Loss: 0.00002051
Iteration 55/1000 | Loss: 0.00002050
Iteration 56/1000 | Loss: 0.00002049
Iteration 57/1000 | Loss: 0.00002049
Iteration 58/1000 | Loss: 0.00002049
Iteration 59/1000 | Loss: 0.00002049
Iteration 60/1000 | Loss: 0.00002049
Iteration 61/1000 | Loss: 0.00002049
Iteration 62/1000 | Loss: 0.00002049
Iteration 63/1000 | Loss: 0.00002049
Iteration 64/1000 | Loss: 0.00002049
Iteration 65/1000 | Loss: 0.00002049
Iteration 66/1000 | Loss: 0.00002049
Iteration 67/1000 | Loss: 0.00002049
Iteration 68/1000 | Loss: 0.00002047
Iteration 69/1000 | Loss: 0.00002047
Iteration 70/1000 | Loss: 0.00002047
Iteration 71/1000 | Loss: 0.00002047
Iteration 72/1000 | Loss: 0.00002046
Iteration 73/1000 | Loss: 0.00002046
Iteration 74/1000 | Loss: 0.00002046
Iteration 75/1000 | Loss: 0.00002046
Iteration 76/1000 | Loss: 0.00002046
Iteration 77/1000 | Loss: 0.00002046
Iteration 78/1000 | Loss: 0.00002045
Iteration 79/1000 | Loss: 0.00002045
Iteration 80/1000 | Loss: 0.00002045
Iteration 81/1000 | Loss: 0.00002045
Iteration 82/1000 | Loss: 0.00002045
Iteration 83/1000 | Loss: 0.00002045
Iteration 84/1000 | Loss: 0.00002045
Iteration 85/1000 | Loss: 0.00002045
Iteration 86/1000 | Loss: 0.00002045
Iteration 87/1000 | Loss: 0.00002045
Iteration 88/1000 | Loss: 0.00002045
Iteration 89/1000 | Loss: 0.00002045
Iteration 90/1000 | Loss: 0.00002045
Iteration 91/1000 | Loss: 0.00002045
Iteration 92/1000 | Loss: 0.00002045
Iteration 93/1000 | Loss: 0.00002045
Iteration 94/1000 | Loss: 0.00002045
Iteration 95/1000 | Loss: 0.00002045
Iteration 96/1000 | Loss: 0.00002045
Iteration 97/1000 | Loss: 0.00002045
Iteration 98/1000 | Loss: 0.00002045
Iteration 99/1000 | Loss: 0.00002045
Iteration 100/1000 | Loss: 0.00002045
Iteration 101/1000 | Loss: 0.00002045
Iteration 102/1000 | Loss: 0.00002045
Iteration 103/1000 | Loss: 0.00002045
Iteration 104/1000 | Loss: 0.00002045
Iteration 105/1000 | Loss: 0.00002045
Iteration 106/1000 | Loss: 0.00002045
Iteration 107/1000 | Loss: 0.00002045
Iteration 108/1000 | Loss: 0.00002045
Iteration 109/1000 | Loss: 0.00002045
Iteration 110/1000 | Loss: 0.00002045
Iteration 111/1000 | Loss: 0.00002045
Iteration 112/1000 | Loss: 0.00002045
Iteration 113/1000 | Loss: 0.00002045
Iteration 114/1000 | Loss: 0.00002045
Iteration 115/1000 | Loss: 0.00002045
Iteration 116/1000 | Loss: 0.00002045
Iteration 117/1000 | Loss: 0.00002045
Iteration 118/1000 | Loss: 0.00002045
Iteration 119/1000 | Loss: 0.00002045
Iteration 120/1000 | Loss: 0.00002045
Iteration 121/1000 | Loss: 0.00002045
Iteration 122/1000 | Loss: 0.00002045
Iteration 123/1000 | Loss: 0.00002045
Iteration 124/1000 | Loss: 0.00002045
Iteration 125/1000 | Loss: 0.00002045
Iteration 126/1000 | Loss: 0.00002045
Iteration 127/1000 | Loss: 0.00002045
Iteration 128/1000 | Loss: 0.00002045
Iteration 129/1000 | Loss: 0.00002045
Iteration 130/1000 | Loss: 0.00002045
Iteration 131/1000 | Loss: 0.00002045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.0445326299523003e-05, 2.0445326299523003e-05, 2.0445326299523003e-05, 2.0445326299523003e-05, 2.0445326299523003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0445326299523003e-05

Optimization complete. Final v2v error: 3.7800161838531494 mm

Highest mean error: 4.010488510131836 mm for frame 30

Lowest mean error: 3.6485369205474854 mm for frame 37

Saving results

Total time: 38.51969003677368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396604
Iteration 2/25 | Loss: 0.00138959
Iteration 3/25 | Loss: 0.00127967
Iteration 4/25 | Loss: 0.00126306
Iteration 5/25 | Loss: 0.00125774
Iteration 6/25 | Loss: 0.00125640
Iteration 7/25 | Loss: 0.00125619
Iteration 8/25 | Loss: 0.00125619
Iteration 9/25 | Loss: 0.00125619
Iteration 10/25 | Loss: 0.00125619
Iteration 11/25 | Loss: 0.00125619
Iteration 12/25 | Loss: 0.00125619
Iteration 13/25 | Loss: 0.00125619
Iteration 14/25 | Loss: 0.00125619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012561932671815157, 0.0012561932671815157, 0.0012561932671815157, 0.0012561932671815157, 0.0012561932671815157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012561932671815157

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36122251
Iteration 2/25 | Loss: 0.00084623
Iteration 3/25 | Loss: 0.00084623
Iteration 4/25 | Loss: 0.00084623
Iteration 5/25 | Loss: 0.00084623
Iteration 6/25 | Loss: 0.00084623
Iteration 7/25 | Loss: 0.00084623
Iteration 8/25 | Loss: 0.00084623
Iteration 9/25 | Loss: 0.00084623
Iteration 10/25 | Loss: 0.00084623
Iteration 11/25 | Loss: 0.00084623
Iteration 12/25 | Loss: 0.00084623
Iteration 13/25 | Loss: 0.00084623
Iteration 14/25 | Loss: 0.00084623
Iteration 15/25 | Loss: 0.00084623
Iteration 16/25 | Loss: 0.00084623
Iteration 17/25 | Loss: 0.00084623
Iteration 18/25 | Loss: 0.00084623
Iteration 19/25 | Loss: 0.00084623
Iteration 20/25 | Loss: 0.00084623
Iteration 21/25 | Loss: 0.00084623
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008462278638035059, 0.0008462278638035059, 0.0008462278638035059, 0.0008462278638035059, 0.0008462278638035059]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008462278638035059

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084623
Iteration 2/1000 | Loss: 0.00005141
Iteration 3/1000 | Loss: 0.00003238
Iteration 4/1000 | Loss: 0.00002507
Iteration 5/1000 | Loss: 0.00002133
Iteration 6/1000 | Loss: 0.00001984
Iteration 7/1000 | Loss: 0.00001866
Iteration 8/1000 | Loss: 0.00001804
Iteration 9/1000 | Loss: 0.00001756
Iteration 10/1000 | Loss: 0.00001700
Iteration 11/1000 | Loss: 0.00001672
Iteration 12/1000 | Loss: 0.00001650
Iteration 13/1000 | Loss: 0.00001630
Iteration 14/1000 | Loss: 0.00001607
Iteration 15/1000 | Loss: 0.00001601
Iteration 16/1000 | Loss: 0.00001598
Iteration 17/1000 | Loss: 0.00001588
Iteration 18/1000 | Loss: 0.00001576
Iteration 19/1000 | Loss: 0.00001564
Iteration 20/1000 | Loss: 0.00001547
Iteration 21/1000 | Loss: 0.00001545
Iteration 22/1000 | Loss: 0.00001542
Iteration 23/1000 | Loss: 0.00001541
Iteration 24/1000 | Loss: 0.00001540
Iteration 25/1000 | Loss: 0.00001539
Iteration 26/1000 | Loss: 0.00001539
Iteration 27/1000 | Loss: 0.00001538
Iteration 28/1000 | Loss: 0.00001538
Iteration 29/1000 | Loss: 0.00001537
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001536
Iteration 32/1000 | Loss: 0.00001536
Iteration 33/1000 | Loss: 0.00001536
Iteration 34/1000 | Loss: 0.00001536
Iteration 35/1000 | Loss: 0.00001536
Iteration 36/1000 | Loss: 0.00001534
Iteration 37/1000 | Loss: 0.00001533
Iteration 38/1000 | Loss: 0.00001533
Iteration 39/1000 | Loss: 0.00001533
Iteration 40/1000 | Loss: 0.00001532
Iteration 41/1000 | Loss: 0.00001532
Iteration 42/1000 | Loss: 0.00001532
Iteration 43/1000 | Loss: 0.00001531
Iteration 44/1000 | Loss: 0.00001531
Iteration 45/1000 | Loss: 0.00001530
Iteration 46/1000 | Loss: 0.00001530
Iteration 47/1000 | Loss: 0.00001529
Iteration 48/1000 | Loss: 0.00001529
Iteration 49/1000 | Loss: 0.00001528
Iteration 50/1000 | Loss: 0.00001528
Iteration 51/1000 | Loss: 0.00001528
Iteration 52/1000 | Loss: 0.00001527
Iteration 53/1000 | Loss: 0.00001527
Iteration 54/1000 | Loss: 0.00001527
Iteration 55/1000 | Loss: 0.00001527
Iteration 56/1000 | Loss: 0.00001527
Iteration 57/1000 | Loss: 0.00001526
Iteration 58/1000 | Loss: 0.00001526
Iteration 59/1000 | Loss: 0.00001526
Iteration 60/1000 | Loss: 0.00001526
Iteration 61/1000 | Loss: 0.00001526
Iteration 62/1000 | Loss: 0.00001525
Iteration 63/1000 | Loss: 0.00001525
Iteration 64/1000 | Loss: 0.00001525
Iteration 65/1000 | Loss: 0.00001525
Iteration 66/1000 | Loss: 0.00001525
Iteration 67/1000 | Loss: 0.00001525
Iteration 68/1000 | Loss: 0.00001525
Iteration 69/1000 | Loss: 0.00001525
Iteration 70/1000 | Loss: 0.00001524
Iteration 71/1000 | Loss: 0.00001524
Iteration 72/1000 | Loss: 0.00001524
Iteration 73/1000 | Loss: 0.00001524
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001524
Iteration 79/1000 | Loss: 0.00001524
Iteration 80/1000 | Loss: 0.00001524
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001524
Iteration 83/1000 | Loss: 0.00001524
Iteration 84/1000 | Loss: 0.00001523
Iteration 85/1000 | Loss: 0.00001523
Iteration 86/1000 | Loss: 0.00001523
Iteration 87/1000 | Loss: 0.00001523
Iteration 88/1000 | Loss: 0.00001523
Iteration 89/1000 | Loss: 0.00001523
Iteration 90/1000 | Loss: 0.00001523
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001522
Iteration 93/1000 | Loss: 0.00001522
Iteration 94/1000 | Loss: 0.00001522
Iteration 95/1000 | Loss: 0.00001522
Iteration 96/1000 | Loss: 0.00001522
Iteration 97/1000 | Loss: 0.00001522
Iteration 98/1000 | Loss: 0.00001522
Iteration 99/1000 | Loss: 0.00001522
Iteration 100/1000 | Loss: 0.00001522
Iteration 101/1000 | Loss: 0.00001521
Iteration 102/1000 | Loss: 0.00001521
Iteration 103/1000 | Loss: 0.00001521
Iteration 104/1000 | Loss: 0.00001521
Iteration 105/1000 | Loss: 0.00001521
Iteration 106/1000 | Loss: 0.00001521
Iteration 107/1000 | Loss: 0.00001520
Iteration 108/1000 | Loss: 0.00001520
Iteration 109/1000 | Loss: 0.00001520
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001519
Iteration 113/1000 | Loss: 0.00001519
Iteration 114/1000 | Loss: 0.00001519
Iteration 115/1000 | Loss: 0.00001518
Iteration 116/1000 | Loss: 0.00001518
Iteration 117/1000 | Loss: 0.00001518
Iteration 118/1000 | Loss: 0.00001518
Iteration 119/1000 | Loss: 0.00001518
Iteration 120/1000 | Loss: 0.00001518
Iteration 121/1000 | Loss: 0.00001518
Iteration 122/1000 | Loss: 0.00001517
Iteration 123/1000 | Loss: 0.00001517
Iteration 124/1000 | Loss: 0.00001517
Iteration 125/1000 | Loss: 0.00001517
Iteration 126/1000 | Loss: 0.00001516
Iteration 127/1000 | Loss: 0.00001516
Iteration 128/1000 | Loss: 0.00001516
Iteration 129/1000 | Loss: 0.00001516
Iteration 130/1000 | Loss: 0.00001516
Iteration 131/1000 | Loss: 0.00001516
Iteration 132/1000 | Loss: 0.00001516
Iteration 133/1000 | Loss: 0.00001515
Iteration 134/1000 | Loss: 0.00001515
Iteration 135/1000 | Loss: 0.00001515
Iteration 136/1000 | Loss: 0.00001515
Iteration 137/1000 | Loss: 0.00001515
Iteration 138/1000 | Loss: 0.00001515
Iteration 139/1000 | Loss: 0.00001515
Iteration 140/1000 | Loss: 0.00001515
Iteration 141/1000 | Loss: 0.00001515
Iteration 142/1000 | Loss: 0.00001515
Iteration 143/1000 | Loss: 0.00001515
Iteration 144/1000 | Loss: 0.00001515
Iteration 145/1000 | Loss: 0.00001514
Iteration 146/1000 | Loss: 0.00001514
Iteration 147/1000 | Loss: 0.00001514
Iteration 148/1000 | Loss: 0.00001514
Iteration 149/1000 | Loss: 0.00001514
Iteration 150/1000 | Loss: 0.00001514
Iteration 151/1000 | Loss: 0.00001514
Iteration 152/1000 | Loss: 0.00001514
Iteration 153/1000 | Loss: 0.00001513
Iteration 154/1000 | Loss: 0.00001513
Iteration 155/1000 | Loss: 0.00001513
Iteration 156/1000 | Loss: 0.00001512
Iteration 157/1000 | Loss: 0.00001512
Iteration 158/1000 | Loss: 0.00001512
Iteration 159/1000 | Loss: 0.00001512
Iteration 160/1000 | Loss: 0.00001512
Iteration 161/1000 | Loss: 0.00001512
Iteration 162/1000 | Loss: 0.00001512
Iteration 163/1000 | Loss: 0.00001512
Iteration 164/1000 | Loss: 0.00001512
Iteration 165/1000 | Loss: 0.00001512
Iteration 166/1000 | Loss: 0.00001512
Iteration 167/1000 | Loss: 0.00001512
Iteration 168/1000 | Loss: 0.00001512
Iteration 169/1000 | Loss: 0.00001511
Iteration 170/1000 | Loss: 0.00001511
Iteration 171/1000 | Loss: 0.00001511
Iteration 172/1000 | Loss: 0.00001511
Iteration 173/1000 | Loss: 0.00001511
Iteration 174/1000 | Loss: 0.00001511
Iteration 175/1000 | Loss: 0.00001511
Iteration 176/1000 | Loss: 0.00001511
Iteration 177/1000 | Loss: 0.00001511
Iteration 178/1000 | Loss: 0.00001511
Iteration 179/1000 | Loss: 0.00001511
Iteration 180/1000 | Loss: 0.00001511
Iteration 181/1000 | Loss: 0.00001511
Iteration 182/1000 | Loss: 0.00001511
Iteration 183/1000 | Loss: 0.00001511
Iteration 184/1000 | Loss: 0.00001511
Iteration 185/1000 | Loss: 0.00001511
Iteration 186/1000 | Loss: 0.00001511
Iteration 187/1000 | Loss: 0.00001511
Iteration 188/1000 | Loss: 0.00001510
Iteration 189/1000 | Loss: 0.00001510
Iteration 190/1000 | Loss: 0.00001510
Iteration 191/1000 | Loss: 0.00001510
Iteration 192/1000 | Loss: 0.00001510
Iteration 193/1000 | Loss: 0.00001510
Iteration 194/1000 | Loss: 0.00001510
Iteration 195/1000 | Loss: 0.00001510
Iteration 196/1000 | Loss: 0.00001510
Iteration 197/1000 | Loss: 0.00001510
Iteration 198/1000 | Loss: 0.00001510
Iteration 199/1000 | Loss: 0.00001510
Iteration 200/1000 | Loss: 0.00001510
Iteration 201/1000 | Loss: 0.00001510
Iteration 202/1000 | Loss: 0.00001510
Iteration 203/1000 | Loss: 0.00001510
Iteration 204/1000 | Loss: 0.00001510
Iteration 205/1000 | Loss: 0.00001510
Iteration 206/1000 | Loss: 0.00001510
Iteration 207/1000 | Loss: 0.00001510
Iteration 208/1000 | Loss: 0.00001510
Iteration 209/1000 | Loss: 0.00001510
Iteration 210/1000 | Loss: 0.00001509
Iteration 211/1000 | Loss: 0.00001509
Iteration 212/1000 | Loss: 0.00001509
Iteration 213/1000 | Loss: 0.00001509
Iteration 214/1000 | Loss: 0.00001509
Iteration 215/1000 | Loss: 0.00001509
Iteration 216/1000 | Loss: 0.00001509
Iteration 217/1000 | Loss: 0.00001509
Iteration 218/1000 | Loss: 0.00001509
Iteration 219/1000 | Loss: 0.00001509
Iteration 220/1000 | Loss: 0.00001509
Iteration 221/1000 | Loss: 0.00001509
Iteration 222/1000 | Loss: 0.00001509
Iteration 223/1000 | Loss: 0.00001509
Iteration 224/1000 | Loss: 0.00001509
Iteration 225/1000 | Loss: 0.00001509
Iteration 226/1000 | Loss: 0.00001509
Iteration 227/1000 | Loss: 0.00001509
Iteration 228/1000 | Loss: 0.00001509
Iteration 229/1000 | Loss: 0.00001509
Iteration 230/1000 | Loss: 0.00001509
Iteration 231/1000 | Loss: 0.00001509
Iteration 232/1000 | Loss: 0.00001509
Iteration 233/1000 | Loss: 0.00001509
Iteration 234/1000 | Loss: 0.00001509
Iteration 235/1000 | Loss: 0.00001509
Iteration 236/1000 | Loss: 0.00001509
Iteration 237/1000 | Loss: 0.00001509
Iteration 238/1000 | Loss: 0.00001509
Iteration 239/1000 | Loss: 0.00001509
Iteration 240/1000 | Loss: 0.00001509
Iteration 241/1000 | Loss: 0.00001509
Iteration 242/1000 | Loss: 0.00001509
Iteration 243/1000 | Loss: 0.00001509
Iteration 244/1000 | Loss: 0.00001509
Iteration 245/1000 | Loss: 0.00001509
Iteration 246/1000 | Loss: 0.00001509
Iteration 247/1000 | Loss: 0.00001509
Iteration 248/1000 | Loss: 0.00001509
Iteration 249/1000 | Loss: 0.00001509
Iteration 250/1000 | Loss: 0.00001509
Iteration 251/1000 | Loss: 0.00001509
Iteration 252/1000 | Loss: 0.00001509
Iteration 253/1000 | Loss: 0.00001509
Iteration 254/1000 | Loss: 0.00001509
Iteration 255/1000 | Loss: 0.00001509
Iteration 256/1000 | Loss: 0.00001509
Iteration 257/1000 | Loss: 0.00001509
Iteration 258/1000 | Loss: 0.00001509
Iteration 259/1000 | Loss: 0.00001509
Iteration 260/1000 | Loss: 0.00001509
Iteration 261/1000 | Loss: 0.00001509
Iteration 262/1000 | Loss: 0.00001509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 262. Stopping optimization.
Last 5 losses: [1.5092901776370127e-05, 1.5092901776370127e-05, 1.5092901776370127e-05, 1.5092901776370127e-05, 1.5092901776370127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5092901776370127e-05

Optimization complete. Final v2v error: 3.339909553527832 mm

Highest mean error: 3.7107601165771484 mm for frame 74

Lowest mean error: 3.037846565246582 mm for frame 117

Saving results

Total time: 49.028478384017944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990598
Iteration 2/25 | Loss: 0.00206241
Iteration 3/25 | Loss: 0.00165119
Iteration 4/25 | Loss: 0.00158765
Iteration 5/25 | Loss: 0.00159404
Iteration 6/25 | Loss: 0.00154075
Iteration 7/25 | Loss: 0.00151007
Iteration 8/25 | Loss: 0.00150370
Iteration 9/25 | Loss: 0.00143845
Iteration 10/25 | Loss: 0.00147827
Iteration 11/25 | Loss: 0.00138944
Iteration 12/25 | Loss: 0.00139504
Iteration 13/25 | Loss: 0.00137838
Iteration 14/25 | Loss: 0.00139264
Iteration 15/25 | Loss: 0.00137696
Iteration 16/25 | Loss: 0.00137629
Iteration 17/25 | Loss: 0.00137684
Iteration 18/25 | Loss: 0.00137359
Iteration 19/25 | Loss: 0.00136663
Iteration 20/25 | Loss: 0.00135585
Iteration 21/25 | Loss: 0.00136436
Iteration 22/25 | Loss: 0.00136856
Iteration 23/25 | Loss: 0.00136150
Iteration 24/25 | Loss: 0.00136593
Iteration 25/25 | Loss: 0.00136181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50880504
Iteration 2/25 | Loss: 0.00121571
Iteration 3/25 | Loss: 0.00119615
Iteration 4/25 | Loss: 0.00119615
Iteration 5/25 | Loss: 0.00119615
Iteration 6/25 | Loss: 0.00119615
Iteration 7/25 | Loss: 0.00119615
Iteration 8/25 | Loss: 0.00119615
Iteration 9/25 | Loss: 0.00119615
Iteration 10/25 | Loss: 0.00119615
Iteration 11/25 | Loss: 0.00119615
Iteration 12/25 | Loss: 0.00119615
Iteration 13/25 | Loss: 0.00119615
Iteration 14/25 | Loss: 0.00119615
Iteration 15/25 | Loss: 0.00119615
Iteration 16/25 | Loss: 0.00119615
Iteration 17/25 | Loss: 0.00119615
Iteration 18/25 | Loss: 0.00119615
Iteration 19/25 | Loss: 0.00119615
Iteration 20/25 | Loss: 0.00119615
Iteration 21/25 | Loss: 0.00119615
Iteration 22/25 | Loss: 0.00119615
Iteration 23/25 | Loss: 0.00119615
Iteration 24/25 | Loss: 0.00119615
Iteration 25/25 | Loss: 0.00119615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119615
Iteration 2/1000 | Loss: 0.00034601
Iteration 3/1000 | Loss: 0.00037873
Iteration 4/1000 | Loss: 0.00015590
Iteration 5/1000 | Loss: 0.00005213
Iteration 6/1000 | Loss: 0.00020154
Iteration 7/1000 | Loss: 0.00025998
Iteration 8/1000 | Loss: 0.00021711
Iteration 9/1000 | Loss: 0.00016095
Iteration 10/1000 | Loss: 0.00048374
Iteration 11/1000 | Loss: 0.00043191
Iteration 12/1000 | Loss: 0.00011880
Iteration 13/1000 | Loss: 0.00016008
Iteration 14/1000 | Loss: 0.00037028
Iteration 15/1000 | Loss: 0.00022668
Iteration 16/1000 | Loss: 0.00007415
Iteration 17/1000 | Loss: 0.00012238
Iteration 18/1000 | Loss: 0.00024355
Iteration 19/1000 | Loss: 0.00019722
Iteration 20/1000 | Loss: 0.00041591
Iteration 21/1000 | Loss: 0.00042410
Iteration 22/1000 | Loss: 0.00004260
Iteration 23/1000 | Loss: 0.00025242
Iteration 24/1000 | Loss: 0.00016506
Iteration 25/1000 | Loss: 0.00042779
Iteration 26/1000 | Loss: 0.00024673
Iteration 27/1000 | Loss: 0.00028634
Iteration 28/1000 | Loss: 0.00020935
Iteration 29/1000 | Loss: 0.00028837
Iteration 30/1000 | Loss: 0.00048366
Iteration 31/1000 | Loss: 0.00071590
Iteration 32/1000 | Loss: 0.00057136
Iteration 33/1000 | Loss: 0.00059099
Iteration 34/1000 | Loss: 0.00064910
Iteration 35/1000 | Loss: 0.00039791
Iteration 36/1000 | Loss: 0.00040126
Iteration 37/1000 | Loss: 0.00046436
Iteration 38/1000 | Loss: 0.00029262
Iteration 39/1000 | Loss: 0.00004014
Iteration 40/1000 | Loss: 0.00008779
Iteration 41/1000 | Loss: 0.00012977
Iteration 42/1000 | Loss: 0.00016871
Iteration 43/1000 | Loss: 0.00046573
Iteration 44/1000 | Loss: 0.00038395
Iteration 45/1000 | Loss: 0.00046314
Iteration 46/1000 | Loss: 0.00046151
Iteration 47/1000 | Loss: 0.00057621
Iteration 48/1000 | Loss: 0.00054928
Iteration 49/1000 | Loss: 0.00009786
Iteration 50/1000 | Loss: 0.00005590
Iteration 51/1000 | Loss: 0.00018293
Iteration 52/1000 | Loss: 0.00015072
Iteration 53/1000 | Loss: 0.00017004
Iteration 54/1000 | Loss: 0.00019130
Iteration 55/1000 | Loss: 0.00022005
Iteration 56/1000 | Loss: 0.00018626
Iteration 57/1000 | Loss: 0.00014890
Iteration 58/1000 | Loss: 0.00018758
Iteration 59/1000 | Loss: 0.00018158
Iteration 60/1000 | Loss: 0.00018115
Iteration 61/1000 | Loss: 0.00016709
Iteration 62/1000 | Loss: 0.00006718
Iteration 63/1000 | Loss: 0.00003754
Iteration 64/1000 | Loss: 0.00013788
Iteration 65/1000 | Loss: 0.00010753
Iteration 66/1000 | Loss: 0.00007961
Iteration 67/1000 | Loss: 0.00019805
Iteration 68/1000 | Loss: 0.00016832
Iteration 69/1000 | Loss: 0.00005592
Iteration 70/1000 | Loss: 0.00012241
Iteration 71/1000 | Loss: 0.00008550
Iteration 72/1000 | Loss: 0.00004314
Iteration 73/1000 | Loss: 0.00004769
Iteration 74/1000 | Loss: 0.00017581
Iteration 75/1000 | Loss: 0.00005239
Iteration 76/1000 | Loss: 0.00005396
Iteration 77/1000 | Loss: 0.00003173
Iteration 78/1000 | Loss: 0.00004402
Iteration 79/1000 | Loss: 0.00028974
Iteration 80/1000 | Loss: 0.00087562
Iteration 81/1000 | Loss: 0.00009904
Iteration 82/1000 | Loss: 0.00004632
Iteration 83/1000 | Loss: 0.00004507
Iteration 84/1000 | Loss: 0.00004752
Iteration 85/1000 | Loss: 0.00028580
Iteration 86/1000 | Loss: 0.00023130
Iteration 87/1000 | Loss: 0.00027459
Iteration 88/1000 | Loss: 0.00039563
Iteration 89/1000 | Loss: 0.00005697
Iteration 90/1000 | Loss: 0.00032169
Iteration 91/1000 | Loss: 0.00033624
Iteration 92/1000 | Loss: 0.00039033
Iteration 93/1000 | Loss: 0.00032109
Iteration 94/1000 | Loss: 0.00036569
Iteration 95/1000 | Loss: 0.00025576
Iteration 96/1000 | Loss: 0.00023144
Iteration 97/1000 | Loss: 0.00019229
Iteration 98/1000 | Loss: 0.00014282
Iteration 99/1000 | Loss: 0.00025295
Iteration 100/1000 | Loss: 0.00035309
Iteration 101/1000 | Loss: 0.00019909
Iteration 102/1000 | Loss: 0.00017680
Iteration 103/1000 | Loss: 0.00019716
Iteration 104/1000 | Loss: 0.00011612
Iteration 105/1000 | Loss: 0.00020593
Iteration 106/1000 | Loss: 0.00018787
Iteration 107/1000 | Loss: 0.00026059
Iteration 108/1000 | Loss: 0.00021328
Iteration 109/1000 | Loss: 0.00014859
Iteration 110/1000 | Loss: 0.00020322
Iteration 111/1000 | Loss: 0.00052441
Iteration 112/1000 | Loss: 0.00027088
Iteration 113/1000 | Loss: 0.00005016
Iteration 114/1000 | Loss: 0.00003122
Iteration 115/1000 | Loss: 0.00005069
Iteration 116/1000 | Loss: 0.00004699
Iteration 117/1000 | Loss: 0.00004494
Iteration 118/1000 | Loss: 0.00002643
Iteration 119/1000 | Loss: 0.00005143
Iteration 120/1000 | Loss: 0.00002994
Iteration 121/1000 | Loss: 0.00002491
Iteration 122/1000 | Loss: 0.00002304
Iteration 123/1000 | Loss: 0.00003436
Iteration 124/1000 | Loss: 0.00002211
Iteration 125/1000 | Loss: 0.00025481
Iteration 126/1000 | Loss: 0.00009762
Iteration 127/1000 | Loss: 0.00017242
Iteration 128/1000 | Loss: 0.00025497
Iteration 129/1000 | Loss: 0.00003907
Iteration 130/1000 | Loss: 0.00002751
Iteration 131/1000 | Loss: 0.00002368
Iteration 132/1000 | Loss: 0.00002187
Iteration 133/1000 | Loss: 0.00002111
Iteration 134/1000 | Loss: 0.00003586
Iteration 135/1000 | Loss: 0.00002416
Iteration 136/1000 | Loss: 0.00023304
Iteration 137/1000 | Loss: 0.00034658
Iteration 138/1000 | Loss: 0.00024909
Iteration 139/1000 | Loss: 0.00042390
Iteration 140/1000 | Loss: 0.00022741
Iteration 141/1000 | Loss: 0.00011613
Iteration 142/1000 | Loss: 0.00002880
Iteration 143/1000 | Loss: 0.00020436
Iteration 144/1000 | Loss: 0.00048779
Iteration 145/1000 | Loss: 0.00021856
Iteration 146/1000 | Loss: 0.00009106
Iteration 147/1000 | Loss: 0.00006989
Iteration 148/1000 | Loss: 0.00017227
Iteration 149/1000 | Loss: 0.00005468
Iteration 150/1000 | Loss: 0.00003280
Iteration 151/1000 | Loss: 0.00003662
Iteration 152/1000 | Loss: 0.00002760
Iteration 153/1000 | Loss: 0.00026447
Iteration 154/1000 | Loss: 0.00008236
Iteration 155/1000 | Loss: 0.00003176
Iteration 156/1000 | Loss: 0.00026594
Iteration 157/1000 | Loss: 0.00007318
Iteration 158/1000 | Loss: 0.00002854
Iteration 159/1000 | Loss: 0.00017596
Iteration 160/1000 | Loss: 0.00009751
Iteration 161/1000 | Loss: 0.00088301
Iteration 162/1000 | Loss: 0.00031180
Iteration 163/1000 | Loss: 0.00005172
Iteration 164/1000 | Loss: 0.00002967
Iteration 165/1000 | Loss: 0.00003007
Iteration 166/1000 | Loss: 0.00002548
Iteration 167/1000 | Loss: 0.00002442
Iteration 168/1000 | Loss: 0.00002262
Iteration 169/1000 | Loss: 0.00002065
Iteration 170/1000 | Loss: 0.00001923
Iteration 171/1000 | Loss: 0.00001870
Iteration 172/1000 | Loss: 0.00001837
Iteration 173/1000 | Loss: 0.00001799
Iteration 174/1000 | Loss: 0.00001778
Iteration 175/1000 | Loss: 0.00001761
Iteration 176/1000 | Loss: 0.00001761
Iteration 177/1000 | Loss: 0.00001758
Iteration 178/1000 | Loss: 0.00001745
Iteration 179/1000 | Loss: 0.00001744
Iteration 180/1000 | Loss: 0.00001743
Iteration 181/1000 | Loss: 0.00001740
Iteration 182/1000 | Loss: 0.00001739
Iteration 183/1000 | Loss: 0.00001736
Iteration 184/1000 | Loss: 0.00001736
Iteration 185/1000 | Loss: 0.00001734
Iteration 186/1000 | Loss: 0.00001734
Iteration 187/1000 | Loss: 0.00001733
Iteration 188/1000 | Loss: 0.00001733
Iteration 189/1000 | Loss: 0.00001733
Iteration 190/1000 | Loss: 0.00001732
Iteration 191/1000 | Loss: 0.00001732
Iteration 192/1000 | Loss: 0.00001731
Iteration 193/1000 | Loss: 0.00001729
Iteration 194/1000 | Loss: 0.00001725
Iteration 195/1000 | Loss: 0.00001724
Iteration 196/1000 | Loss: 0.00001723
Iteration 197/1000 | Loss: 0.00001723
Iteration 198/1000 | Loss: 0.00001722
Iteration 199/1000 | Loss: 0.00001722
Iteration 200/1000 | Loss: 0.00001722
Iteration 201/1000 | Loss: 0.00001722
Iteration 202/1000 | Loss: 0.00001722
Iteration 203/1000 | Loss: 0.00001722
Iteration 204/1000 | Loss: 0.00001722
Iteration 205/1000 | Loss: 0.00001722
Iteration 206/1000 | Loss: 0.00001722
Iteration 207/1000 | Loss: 0.00001721
Iteration 208/1000 | Loss: 0.00001721
Iteration 209/1000 | Loss: 0.00001721
Iteration 210/1000 | Loss: 0.00001721
Iteration 211/1000 | Loss: 0.00001721
Iteration 212/1000 | Loss: 0.00001720
Iteration 213/1000 | Loss: 0.00001720
Iteration 214/1000 | Loss: 0.00001719
Iteration 215/1000 | Loss: 0.00001719
Iteration 216/1000 | Loss: 0.00001719
Iteration 217/1000 | Loss: 0.00001719
Iteration 218/1000 | Loss: 0.00001719
Iteration 219/1000 | Loss: 0.00001719
Iteration 220/1000 | Loss: 0.00001719
Iteration 221/1000 | Loss: 0.00001719
Iteration 222/1000 | Loss: 0.00001719
Iteration 223/1000 | Loss: 0.00001719
Iteration 224/1000 | Loss: 0.00001719
Iteration 225/1000 | Loss: 0.00001719
Iteration 226/1000 | Loss: 0.00001718
Iteration 227/1000 | Loss: 0.00001718
Iteration 228/1000 | Loss: 0.00001718
Iteration 229/1000 | Loss: 0.00001718
Iteration 230/1000 | Loss: 0.00001717
Iteration 231/1000 | Loss: 0.00001717
Iteration 232/1000 | Loss: 0.00001717
Iteration 233/1000 | Loss: 0.00001717
Iteration 234/1000 | Loss: 0.00001717
Iteration 235/1000 | Loss: 0.00001717
Iteration 236/1000 | Loss: 0.00001717
Iteration 237/1000 | Loss: 0.00001716
Iteration 238/1000 | Loss: 0.00001716
Iteration 239/1000 | Loss: 0.00001716
Iteration 240/1000 | Loss: 0.00001716
Iteration 241/1000 | Loss: 0.00001716
Iteration 242/1000 | Loss: 0.00001716
Iteration 243/1000 | Loss: 0.00001716
Iteration 244/1000 | Loss: 0.00001716
Iteration 245/1000 | Loss: 0.00001716
Iteration 246/1000 | Loss: 0.00001716
Iteration 247/1000 | Loss: 0.00001716
Iteration 248/1000 | Loss: 0.00001716
Iteration 249/1000 | Loss: 0.00001716
Iteration 250/1000 | Loss: 0.00001716
Iteration 251/1000 | Loss: 0.00001716
Iteration 252/1000 | Loss: 0.00001715
Iteration 253/1000 | Loss: 0.00001715
Iteration 254/1000 | Loss: 0.00001715
Iteration 255/1000 | Loss: 0.00001715
Iteration 256/1000 | Loss: 0.00001715
Iteration 257/1000 | Loss: 0.00001715
Iteration 258/1000 | Loss: 0.00001715
Iteration 259/1000 | Loss: 0.00001715
Iteration 260/1000 | Loss: 0.00001715
Iteration 261/1000 | Loss: 0.00001715
Iteration 262/1000 | Loss: 0.00001715
Iteration 263/1000 | Loss: 0.00001715
Iteration 264/1000 | Loss: 0.00001715
Iteration 265/1000 | Loss: 0.00001715
Iteration 266/1000 | Loss: 0.00001715
Iteration 267/1000 | Loss: 0.00001715
Iteration 268/1000 | Loss: 0.00001715
Iteration 269/1000 | Loss: 0.00001714
Iteration 270/1000 | Loss: 0.00001714
Iteration 271/1000 | Loss: 0.00001714
Iteration 272/1000 | Loss: 0.00001714
Iteration 273/1000 | Loss: 0.00001714
Iteration 274/1000 | Loss: 0.00001714
Iteration 275/1000 | Loss: 0.00001714
Iteration 276/1000 | Loss: 0.00001714
Iteration 277/1000 | Loss: 0.00001714
Iteration 278/1000 | Loss: 0.00001714
Iteration 279/1000 | Loss: 0.00001714
Iteration 280/1000 | Loss: 0.00001714
Iteration 281/1000 | Loss: 0.00001714
Iteration 282/1000 | Loss: 0.00001714
Iteration 283/1000 | Loss: 0.00001714
Iteration 284/1000 | Loss: 0.00001714
Iteration 285/1000 | Loss: 0.00001714
Iteration 286/1000 | Loss: 0.00001714
Iteration 287/1000 | Loss: 0.00001714
Iteration 288/1000 | Loss: 0.00001714
Iteration 289/1000 | Loss: 0.00001714
Iteration 290/1000 | Loss: 0.00001714
Iteration 291/1000 | Loss: 0.00001714
Iteration 292/1000 | Loss: 0.00001714
Iteration 293/1000 | Loss: 0.00001714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 293. Stopping optimization.
Last 5 losses: [1.7142183423857205e-05, 1.7142183423857205e-05, 1.7142183423857205e-05, 1.7142183423857205e-05, 1.7142183423857205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7142183423857205e-05

Optimization complete. Final v2v error: 3.494817018508911 mm

Highest mean error: 4.76648473739624 mm for frame 75

Lowest mean error: 3.0849599838256836 mm for frame 24

Saving results

Total time: 305.2169032096863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478712
Iteration 2/25 | Loss: 0.00134290
Iteration 3/25 | Loss: 0.00129330
Iteration 4/25 | Loss: 0.00128700
Iteration 5/25 | Loss: 0.00128497
Iteration 6/25 | Loss: 0.00128493
Iteration 7/25 | Loss: 0.00128493
Iteration 8/25 | Loss: 0.00128493
Iteration 9/25 | Loss: 0.00128493
Iteration 10/25 | Loss: 0.00128493
Iteration 11/25 | Loss: 0.00128493
Iteration 12/25 | Loss: 0.00128493
Iteration 13/25 | Loss: 0.00128493
Iteration 14/25 | Loss: 0.00128493
Iteration 15/25 | Loss: 0.00128493
Iteration 16/25 | Loss: 0.00128493
Iteration 17/25 | Loss: 0.00128493
Iteration 18/25 | Loss: 0.00128493
Iteration 19/25 | Loss: 0.00128493
Iteration 20/25 | Loss: 0.00128493
Iteration 21/25 | Loss: 0.00128493
Iteration 22/25 | Loss: 0.00128493
Iteration 23/25 | Loss: 0.00128493
Iteration 24/25 | Loss: 0.00128493
Iteration 25/25 | Loss: 0.00128493

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.12862778
Iteration 2/25 | Loss: 0.00087471
Iteration 3/25 | Loss: 0.00087470
Iteration 4/25 | Loss: 0.00087470
Iteration 5/25 | Loss: 0.00087470
Iteration 6/25 | Loss: 0.00087470
Iteration 7/25 | Loss: 0.00087470
Iteration 8/25 | Loss: 0.00087470
Iteration 9/25 | Loss: 0.00087470
Iteration 10/25 | Loss: 0.00087470
Iteration 11/25 | Loss: 0.00087470
Iteration 12/25 | Loss: 0.00087470
Iteration 13/25 | Loss: 0.00087470
Iteration 14/25 | Loss: 0.00087470
Iteration 15/25 | Loss: 0.00087470
Iteration 16/25 | Loss: 0.00087470
Iteration 17/25 | Loss: 0.00087470
Iteration 18/25 | Loss: 0.00087470
Iteration 19/25 | Loss: 0.00087470
Iteration 20/25 | Loss: 0.00087470
Iteration 21/25 | Loss: 0.00087470
Iteration 22/25 | Loss: 0.00087470
Iteration 23/25 | Loss: 0.00087470
Iteration 24/25 | Loss: 0.00087470
Iteration 25/25 | Loss: 0.00087470

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087470
Iteration 2/1000 | Loss: 0.00003027
Iteration 3/1000 | Loss: 0.00002162
Iteration 4/1000 | Loss: 0.00001870
Iteration 5/1000 | Loss: 0.00001721
Iteration 6/1000 | Loss: 0.00001636
Iteration 7/1000 | Loss: 0.00001565
Iteration 8/1000 | Loss: 0.00001510
Iteration 9/1000 | Loss: 0.00001483
Iteration 10/1000 | Loss: 0.00001457
Iteration 11/1000 | Loss: 0.00001434
Iteration 12/1000 | Loss: 0.00001421
Iteration 13/1000 | Loss: 0.00001404
Iteration 14/1000 | Loss: 0.00001393
Iteration 15/1000 | Loss: 0.00001391
Iteration 16/1000 | Loss: 0.00001390
Iteration 17/1000 | Loss: 0.00001390
Iteration 18/1000 | Loss: 0.00001390
Iteration 19/1000 | Loss: 0.00001389
Iteration 20/1000 | Loss: 0.00001387
Iteration 21/1000 | Loss: 0.00001386
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001385
Iteration 24/1000 | Loss: 0.00001384
Iteration 25/1000 | Loss: 0.00001384
Iteration 26/1000 | Loss: 0.00001379
Iteration 27/1000 | Loss: 0.00001379
Iteration 28/1000 | Loss: 0.00001378
Iteration 29/1000 | Loss: 0.00001378
Iteration 30/1000 | Loss: 0.00001378
Iteration 31/1000 | Loss: 0.00001378
Iteration 32/1000 | Loss: 0.00001378
Iteration 33/1000 | Loss: 0.00001377
Iteration 34/1000 | Loss: 0.00001377
Iteration 35/1000 | Loss: 0.00001373
Iteration 36/1000 | Loss: 0.00001373
Iteration 37/1000 | Loss: 0.00001373
Iteration 38/1000 | Loss: 0.00001372
Iteration 39/1000 | Loss: 0.00001372
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001371
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001368
Iteration 44/1000 | Loss: 0.00001367
Iteration 45/1000 | Loss: 0.00001363
Iteration 46/1000 | Loss: 0.00001362
Iteration 47/1000 | Loss: 0.00001362
Iteration 48/1000 | Loss: 0.00001361
Iteration 49/1000 | Loss: 0.00001360
Iteration 50/1000 | Loss: 0.00001355
Iteration 51/1000 | Loss: 0.00001354
Iteration 52/1000 | Loss: 0.00001352
Iteration 53/1000 | Loss: 0.00001352
Iteration 54/1000 | Loss: 0.00001351
Iteration 55/1000 | Loss: 0.00001351
Iteration 56/1000 | Loss: 0.00001351
Iteration 57/1000 | Loss: 0.00001350
Iteration 58/1000 | Loss: 0.00001350
Iteration 59/1000 | Loss: 0.00001349
Iteration 60/1000 | Loss: 0.00001348
Iteration 61/1000 | Loss: 0.00001348
Iteration 62/1000 | Loss: 0.00001347
Iteration 63/1000 | Loss: 0.00001346
Iteration 64/1000 | Loss: 0.00001346
Iteration 65/1000 | Loss: 0.00001345
Iteration 66/1000 | Loss: 0.00001345
Iteration 67/1000 | Loss: 0.00001345
Iteration 68/1000 | Loss: 0.00001345
Iteration 69/1000 | Loss: 0.00001345
Iteration 70/1000 | Loss: 0.00001344
Iteration 71/1000 | Loss: 0.00001344
Iteration 72/1000 | Loss: 0.00001344
Iteration 73/1000 | Loss: 0.00001344
Iteration 74/1000 | Loss: 0.00001344
Iteration 75/1000 | Loss: 0.00001344
Iteration 76/1000 | Loss: 0.00001342
Iteration 77/1000 | Loss: 0.00001342
Iteration 78/1000 | Loss: 0.00001342
Iteration 79/1000 | Loss: 0.00001342
Iteration 80/1000 | Loss: 0.00001341
Iteration 81/1000 | Loss: 0.00001341
Iteration 82/1000 | Loss: 0.00001340
Iteration 83/1000 | Loss: 0.00001340
Iteration 84/1000 | Loss: 0.00001340
Iteration 85/1000 | Loss: 0.00001340
Iteration 86/1000 | Loss: 0.00001340
Iteration 87/1000 | Loss: 0.00001340
Iteration 88/1000 | Loss: 0.00001340
Iteration 89/1000 | Loss: 0.00001340
Iteration 90/1000 | Loss: 0.00001339
Iteration 91/1000 | Loss: 0.00001339
Iteration 92/1000 | Loss: 0.00001339
Iteration 93/1000 | Loss: 0.00001338
Iteration 94/1000 | Loss: 0.00001338
Iteration 95/1000 | Loss: 0.00001338
Iteration 96/1000 | Loss: 0.00001338
Iteration 97/1000 | Loss: 0.00001338
Iteration 98/1000 | Loss: 0.00001338
Iteration 99/1000 | Loss: 0.00001338
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001337
Iteration 102/1000 | Loss: 0.00001337
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001336
Iteration 105/1000 | Loss: 0.00001336
Iteration 106/1000 | Loss: 0.00001336
Iteration 107/1000 | Loss: 0.00001336
Iteration 108/1000 | Loss: 0.00001336
Iteration 109/1000 | Loss: 0.00001335
Iteration 110/1000 | Loss: 0.00001335
Iteration 111/1000 | Loss: 0.00001335
Iteration 112/1000 | Loss: 0.00001335
Iteration 113/1000 | Loss: 0.00001335
Iteration 114/1000 | Loss: 0.00001334
Iteration 115/1000 | Loss: 0.00001334
Iteration 116/1000 | Loss: 0.00001334
Iteration 117/1000 | Loss: 0.00001334
Iteration 118/1000 | Loss: 0.00001334
Iteration 119/1000 | Loss: 0.00001334
Iteration 120/1000 | Loss: 0.00001334
Iteration 121/1000 | Loss: 0.00001334
Iteration 122/1000 | Loss: 0.00001333
Iteration 123/1000 | Loss: 0.00001333
Iteration 124/1000 | Loss: 0.00001333
Iteration 125/1000 | Loss: 0.00001333
Iteration 126/1000 | Loss: 0.00001333
Iteration 127/1000 | Loss: 0.00001333
Iteration 128/1000 | Loss: 0.00001332
Iteration 129/1000 | Loss: 0.00001332
Iteration 130/1000 | Loss: 0.00001332
Iteration 131/1000 | Loss: 0.00001332
Iteration 132/1000 | Loss: 0.00001331
Iteration 133/1000 | Loss: 0.00001331
Iteration 134/1000 | Loss: 0.00001331
Iteration 135/1000 | Loss: 0.00001331
Iteration 136/1000 | Loss: 0.00001331
Iteration 137/1000 | Loss: 0.00001331
Iteration 138/1000 | Loss: 0.00001331
Iteration 139/1000 | Loss: 0.00001331
Iteration 140/1000 | Loss: 0.00001331
Iteration 141/1000 | Loss: 0.00001331
Iteration 142/1000 | Loss: 0.00001331
Iteration 143/1000 | Loss: 0.00001330
Iteration 144/1000 | Loss: 0.00001330
Iteration 145/1000 | Loss: 0.00001330
Iteration 146/1000 | Loss: 0.00001330
Iteration 147/1000 | Loss: 0.00001330
Iteration 148/1000 | Loss: 0.00001330
Iteration 149/1000 | Loss: 0.00001330
Iteration 150/1000 | Loss: 0.00001330
Iteration 151/1000 | Loss: 0.00001330
Iteration 152/1000 | Loss: 0.00001330
Iteration 153/1000 | Loss: 0.00001330
Iteration 154/1000 | Loss: 0.00001330
Iteration 155/1000 | Loss: 0.00001329
Iteration 156/1000 | Loss: 0.00001329
Iteration 157/1000 | Loss: 0.00001329
Iteration 158/1000 | Loss: 0.00001329
Iteration 159/1000 | Loss: 0.00001329
Iteration 160/1000 | Loss: 0.00001329
Iteration 161/1000 | Loss: 0.00001329
Iteration 162/1000 | Loss: 0.00001329
Iteration 163/1000 | Loss: 0.00001329
Iteration 164/1000 | Loss: 0.00001329
Iteration 165/1000 | Loss: 0.00001329
Iteration 166/1000 | Loss: 0.00001329
Iteration 167/1000 | Loss: 0.00001329
Iteration 168/1000 | Loss: 0.00001329
Iteration 169/1000 | Loss: 0.00001329
Iteration 170/1000 | Loss: 0.00001329
Iteration 171/1000 | Loss: 0.00001329
Iteration 172/1000 | Loss: 0.00001328
Iteration 173/1000 | Loss: 0.00001328
Iteration 174/1000 | Loss: 0.00001328
Iteration 175/1000 | Loss: 0.00001328
Iteration 176/1000 | Loss: 0.00001328
Iteration 177/1000 | Loss: 0.00001328
Iteration 178/1000 | Loss: 0.00001328
Iteration 179/1000 | Loss: 0.00001328
Iteration 180/1000 | Loss: 0.00001328
Iteration 181/1000 | Loss: 0.00001328
Iteration 182/1000 | Loss: 0.00001328
Iteration 183/1000 | Loss: 0.00001328
Iteration 184/1000 | Loss: 0.00001328
Iteration 185/1000 | Loss: 0.00001327
Iteration 186/1000 | Loss: 0.00001327
Iteration 187/1000 | Loss: 0.00001327
Iteration 188/1000 | Loss: 0.00001327
Iteration 189/1000 | Loss: 0.00001327
Iteration 190/1000 | Loss: 0.00001327
Iteration 191/1000 | Loss: 0.00001327
Iteration 192/1000 | Loss: 0.00001327
Iteration 193/1000 | Loss: 0.00001327
Iteration 194/1000 | Loss: 0.00001327
Iteration 195/1000 | Loss: 0.00001327
Iteration 196/1000 | Loss: 0.00001326
Iteration 197/1000 | Loss: 0.00001326
Iteration 198/1000 | Loss: 0.00001326
Iteration 199/1000 | Loss: 0.00001326
Iteration 200/1000 | Loss: 0.00001326
Iteration 201/1000 | Loss: 0.00001326
Iteration 202/1000 | Loss: 0.00001326
Iteration 203/1000 | Loss: 0.00001326
Iteration 204/1000 | Loss: 0.00001326
Iteration 205/1000 | Loss: 0.00001326
Iteration 206/1000 | Loss: 0.00001326
Iteration 207/1000 | Loss: 0.00001326
Iteration 208/1000 | Loss: 0.00001326
Iteration 209/1000 | Loss: 0.00001326
Iteration 210/1000 | Loss: 0.00001326
Iteration 211/1000 | Loss: 0.00001326
Iteration 212/1000 | Loss: 0.00001326
Iteration 213/1000 | Loss: 0.00001326
Iteration 214/1000 | Loss: 0.00001326
Iteration 215/1000 | Loss: 0.00001326
Iteration 216/1000 | Loss: 0.00001326
Iteration 217/1000 | Loss: 0.00001326
Iteration 218/1000 | Loss: 0.00001326
Iteration 219/1000 | Loss: 0.00001326
Iteration 220/1000 | Loss: 0.00001325
Iteration 221/1000 | Loss: 0.00001325
Iteration 222/1000 | Loss: 0.00001325
Iteration 223/1000 | Loss: 0.00001325
Iteration 224/1000 | Loss: 0.00001325
Iteration 225/1000 | Loss: 0.00001325
Iteration 226/1000 | Loss: 0.00001325
Iteration 227/1000 | Loss: 0.00001325
Iteration 228/1000 | Loss: 0.00001325
Iteration 229/1000 | Loss: 0.00001325
Iteration 230/1000 | Loss: 0.00001325
Iteration 231/1000 | Loss: 0.00001325
Iteration 232/1000 | Loss: 0.00001325
Iteration 233/1000 | Loss: 0.00001325
Iteration 234/1000 | Loss: 0.00001325
Iteration 235/1000 | Loss: 0.00001325
Iteration 236/1000 | Loss: 0.00001325
Iteration 237/1000 | Loss: 0.00001325
Iteration 238/1000 | Loss: 0.00001325
Iteration 239/1000 | Loss: 0.00001325
Iteration 240/1000 | Loss: 0.00001325
Iteration 241/1000 | Loss: 0.00001325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.3251286873128265e-05, 1.3251286873128265e-05, 1.3251286873128265e-05, 1.3251286873128265e-05, 1.3251286873128265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3251286873128265e-05

Optimization complete. Final v2v error: 3.1128392219543457 mm

Highest mean error: 3.533482074737549 mm for frame 75

Lowest mean error: 2.8934948444366455 mm for frame 139

Saving results

Total time: 45.75896883010864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494508
Iteration 2/25 | Loss: 0.00154188
Iteration 3/25 | Loss: 0.00141726
Iteration 4/25 | Loss: 0.00140304
Iteration 5/25 | Loss: 0.00139957
Iteration 6/25 | Loss: 0.00139889
Iteration 7/25 | Loss: 0.00139889
Iteration 8/25 | Loss: 0.00139889
Iteration 9/25 | Loss: 0.00139889
Iteration 10/25 | Loss: 0.00139889
Iteration 11/25 | Loss: 0.00139889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013988917926326394, 0.0013988917926326394, 0.0013988917926326394, 0.0013988917926326394, 0.0013988917926326394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013988917926326394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40387249
Iteration 2/25 | Loss: 0.00096237
Iteration 3/25 | Loss: 0.00096235
Iteration 4/25 | Loss: 0.00096235
Iteration 5/25 | Loss: 0.00096235
Iteration 6/25 | Loss: 0.00096235
Iteration 7/25 | Loss: 0.00096235
Iteration 8/25 | Loss: 0.00096235
Iteration 9/25 | Loss: 0.00096235
Iteration 10/25 | Loss: 0.00096235
Iteration 11/25 | Loss: 0.00096235
Iteration 12/25 | Loss: 0.00096235
Iteration 13/25 | Loss: 0.00096235
Iteration 14/25 | Loss: 0.00096235
Iteration 15/25 | Loss: 0.00096235
Iteration 16/25 | Loss: 0.00096235
Iteration 17/25 | Loss: 0.00096235
Iteration 18/25 | Loss: 0.00096235
Iteration 19/25 | Loss: 0.00096235
Iteration 20/25 | Loss: 0.00096235
Iteration 21/25 | Loss: 0.00096235
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009623493533581495, 0.0009623493533581495, 0.0009623493533581495, 0.0009623493533581495, 0.0009623493533581495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009623493533581495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096235
Iteration 2/1000 | Loss: 0.00004721
Iteration 3/1000 | Loss: 0.00003141
Iteration 4/1000 | Loss: 0.00002829
Iteration 5/1000 | Loss: 0.00002691
Iteration 6/1000 | Loss: 0.00002587
Iteration 7/1000 | Loss: 0.00002515
Iteration 8/1000 | Loss: 0.00002470
Iteration 9/1000 | Loss: 0.00002438
Iteration 10/1000 | Loss: 0.00002416
Iteration 11/1000 | Loss: 0.00002409
Iteration 12/1000 | Loss: 0.00002407
Iteration 13/1000 | Loss: 0.00002391
Iteration 14/1000 | Loss: 0.00002381
Iteration 15/1000 | Loss: 0.00002380
Iteration 16/1000 | Loss: 0.00002379
Iteration 17/1000 | Loss: 0.00002378
Iteration 18/1000 | Loss: 0.00002376
Iteration 19/1000 | Loss: 0.00002374
Iteration 20/1000 | Loss: 0.00002370
Iteration 21/1000 | Loss: 0.00002369
Iteration 22/1000 | Loss: 0.00002365
Iteration 23/1000 | Loss: 0.00002365
Iteration 24/1000 | Loss: 0.00002360
Iteration 25/1000 | Loss: 0.00002359
Iteration 26/1000 | Loss: 0.00002358
Iteration 27/1000 | Loss: 0.00002356
Iteration 28/1000 | Loss: 0.00002355
Iteration 29/1000 | Loss: 0.00002355
Iteration 30/1000 | Loss: 0.00002355
Iteration 31/1000 | Loss: 0.00002354
Iteration 32/1000 | Loss: 0.00002354
Iteration 33/1000 | Loss: 0.00002353
Iteration 34/1000 | Loss: 0.00002350
Iteration 35/1000 | Loss: 0.00002349
Iteration 36/1000 | Loss: 0.00002348
Iteration 37/1000 | Loss: 0.00002348
Iteration 38/1000 | Loss: 0.00002346
Iteration 39/1000 | Loss: 0.00002346
Iteration 40/1000 | Loss: 0.00002345
Iteration 41/1000 | Loss: 0.00002345
Iteration 42/1000 | Loss: 0.00002345
Iteration 43/1000 | Loss: 0.00002344
Iteration 44/1000 | Loss: 0.00002342
Iteration 45/1000 | Loss: 0.00002342
Iteration 46/1000 | Loss: 0.00002342
Iteration 47/1000 | Loss: 0.00002341
Iteration 48/1000 | Loss: 0.00002341
Iteration 49/1000 | Loss: 0.00002340
Iteration 50/1000 | Loss: 0.00002340
Iteration 51/1000 | Loss: 0.00002340
Iteration 52/1000 | Loss: 0.00002340
Iteration 53/1000 | Loss: 0.00002340
Iteration 54/1000 | Loss: 0.00002339
Iteration 55/1000 | Loss: 0.00002339
Iteration 56/1000 | Loss: 0.00002339
Iteration 57/1000 | Loss: 0.00002339
Iteration 58/1000 | Loss: 0.00002339
Iteration 59/1000 | Loss: 0.00002338
Iteration 60/1000 | Loss: 0.00002338
Iteration 61/1000 | Loss: 0.00002338
Iteration 62/1000 | Loss: 0.00002338
Iteration 63/1000 | Loss: 0.00002338
Iteration 64/1000 | Loss: 0.00002337
Iteration 65/1000 | Loss: 0.00002337
Iteration 66/1000 | Loss: 0.00002336
Iteration 67/1000 | Loss: 0.00002336
Iteration 68/1000 | Loss: 0.00002335
Iteration 69/1000 | Loss: 0.00002335
Iteration 70/1000 | Loss: 0.00002335
Iteration 71/1000 | Loss: 0.00002335
Iteration 72/1000 | Loss: 0.00002335
Iteration 73/1000 | Loss: 0.00002334
Iteration 74/1000 | Loss: 0.00002334
Iteration 75/1000 | Loss: 0.00002334
Iteration 76/1000 | Loss: 0.00002333
Iteration 77/1000 | Loss: 0.00002333
Iteration 78/1000 | Loss: 0.00002333
Iteration 79/1000 | Loss: 0.00002333
Iteration 80/1000 | Loss: 0.00002333
Iteration 81/1000 | Loss: 0.00002332
Iteration 82/1000 | Loss: 0.00002332
Iteration 83/1000 | Loss: 0.00002331
Iteration 84/1000 | Loss: 0.00002331
Iteration 85/1000 | Loss: 0.00002331
Iteration 86/1000 | Loss: 0.00002331
Iteration 87/1000 | Loss: 0.00002331
Iteration 88/1000 | Loss: 0.00002331
Iteration 89/1000 | Loss: 0.00002331
Iteration 90/1000 | Loss: 0.00002330
Iteration 91/1000 | Loss: 0.00002330
Iteration 92/1000 | Loss: 0.00002330
Iteration 93/1000 | Loss: 0.00002330
Iteration 94/1000 | Loss: 0.00002330
Iteration 95/1000 | Loss: 0.00002330
Iteration 96/1000 | Loss: 0.00002329
Iteration 97/1000 | Loss: 0.00002329
Iteration 98/1000 | Loss: 0.00002329
Iteration 99/1000 | Loss: 0.00002328
Iteration 100/1000 | Loss: 0.00002328
Iteration 101/1000 | Loss: 0.00002328
Iteration 102/1000 | Loss: 0.00002328
Iteration 103/1000 | Loss: 0.00002328
Iteration 104/1000 | Loss: 0.00002327
Iteration 105/1000 | Loss: 0.00002327
Iteration 106/1000 | Loss: 0.00002327
Iteration 107/1000 | Loss: 0.00002327
Iteration 108/1000 | Loss: 0.00002327
Iteration 109/1000 | Loss: 0.00002326
Iteration 110/1000 | Loss: 0.00002326
Iteration 111/1000 | Loss: 0.00002326
Iteration 112/1000 | Loss: 0.00002326
Iteration 113/1000 | Loss: 0.00002326
Iteration 114/1000 | Loss: 0.00002326
Iteration 115/1000 | Loss: 0.00002326
Iteration 116/1000 | Loss: 0.00002326
Iteration 117/1000 | Loss: 0.00002326
Iteration 118/1000 | Loss: 0.00002326
Iteration 119/1000 | Loss: 0.00002325
Iteration 120/1000 | Loss: 0.00002325
Iteration 121/1000 | Loss: 0.00002325
Iteration 122/1000 | Loss: 0.00002325
Iteration 123/1000 | Loss: 0.00002325
Iteration 124/1000 | Loss: 0.00002325
Iteration 125/1000 | Loss: 0.00002324
Iteration 126/1000 | Loss: 0.00002324
Iteration 127/1000 | Loss: 0.00002324
Iteration 128/1000 | Loss: 0.00002324
Iteration 129/1000 | Loss: 0.00002324
Iteration 130/1000 | Loss: 0.00002324
Iteration 131/1000 | Loss: 0.00002323
Iteration 132/1000 | Loss: 0.00002323
Iteration 133/1000 | Loss: 0.00002323
Iteration 134/1000 | Loss: 0.00002323
Iteration 135/1000 | Loss: 0.00002323
Iteration 136/1000 | Loss: 0.00002322
Iteration 137/1000 | Loss: 0.00002322
Iteration 138/1000 | Loss: 0.00002322
Iteration 139/1000 | Loss: 0.00002322
Iteration 140/1000 | Loss: 0.00002322
Iteration 141/1000 | Loss: 0.00002322
Iteration 142/1000 | Loss: 0.00002322
Iteration 143/1000 | Loss: 0.00002321
Iteration 144/1000 | Loss: 0.00002321
Iteration 145/1000 | Loss: 0.00002321
Iteration 146/1000 | Loss: 0.00002321
Iteration 147/1000 | Loss: 0.00002321
Iteration 148/1000 | Loss: 0.00002321
Iteration 149/1000 | Loss: 0.00002321
Iteration 150/1000 | Loss: 0.00002321
Iteration 151/1000 | Loss: 0.00002321
Iteration 152/1000 | Loss: 0.00002321
Iteration 153/1000 | Loss: 0.00002321
Iteration 154/1000 | Loss: 0.00002321
Iteration 155/1000 | Loss: 0.00002321
Iteration 156/1000 | Loss: 0.00002321
Iteration 157/1000 | Loss: 0.00002321
Iteration 158/1000 | Loss: 0.00002320
Iteration 159/1000 | Loss: 0.00002320
Iteration 160/1000 | Loss: 0.00002320
Iteration 161/1000 | Loss: 0.00002320
Iteration 162/1000 | Loss: 0.00002320
Iteration 163/1000 | Loss: 0.00002320
Iteration 164/1000 | Loss: 0.00002320
Iteration 165/1000 | Loss: 0.00002320
Iteration 166/1000 | Loss: 0.00002320
Iteration 167/1000 | Loss: 0.00002319
Iteration 168/1000 | Loss: 0.00002319
Iteration 169/1000 | Loss: 0.00002319
Iteration 170/1000 | Loss: 0.00002319
Iteration 171/1000 | Loss: 0.00002319
Iteration 172/1000 | Loss: 0.00002319
Iteration 173/1000 | Loss: 0.00002319
Iteration 174/1000 | Loss: 0.00002319
Iteration 175/1000 | Loss: 0.00002319
Iteration 176/1000 | Loss: 0.00002319
Iteration 177/1000 | Loss: 0.00002319
Iteration 178/1000 | Loss: 0.00002319
Iteration 179/1000 | Loss: 0.00002319
Iteration 180/1000 | Loss: 0.00002319
Iteration 181/1000 | Loss: 0.00002319
Iteration 182/1000 | Loss: 0.00002319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.3187361875898205e-05, 2.3187361875898205e-05, 2.3187361875898205e-05, 2.3187361875898205e-05, 2.3187361875898205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3187361875898205e-05

Optimization complete. Final v2v error: 3.940429449081421 mm

Highest mean error: 4.518795490264893 mm for frame 132

Lowest mean error: 3.443690776824951 mm for frame 1

Saving results

Total time: 41.73816537857056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393098
Iteration 2/25 | Loss: 0.00109803
Iteration 3/25 | Loss: 0.00096332
Iteration 4/25 | Loss: 0.00095177
Iteration 5/25 | Loss: 0.00094934
Iteration 6/25 | Loss: 0.00094862
Iteration 7/25 | Loss: 0.00094862
Iteration 8/25 | Loss: 0.00094862
Iteration 9/25 | Loss: 0.00094862
Iteration 10/25 | Loss: 0.00094862
Iteration 11/25 | Loss: 0.00094862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000948618573602289, 0.000948618573602289, 0.000948618573602289, 0.000948618573602289, 0.000948618573602289]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000948618573602289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22204494
Iteration 2/25 | Loss: 0.00191246
Iteration 3/25 | Loss: 0.00191246
Iteration 4/25 | Loss: 0.00191246
Iteration 5/25 | Loss: 0.00191246
Iteration 6/25 | Loss: 0.00191246
Iteration 7/25 | Loss: 0.00191246
Iteration 8/25 | Loss: 0.00191246
Iteration 9/25 | Loss: 0.00191246
Iteration 10/25 | Loss: 0.00191246
Iteration 11/25 | Loss: 0.00191246
Iteration 12/25 | Loss: 0.00191246
Iteration 13/25 | Loss: 0.00191246
Iteration 14/25 | Loss: 0.00191246
Iteration 15/25 | Loss: 0.00191246
Iteration 16/25 | Loss: 0.00191246
Iteration 17/25 | Loss: 0.00191246
Iteration 18/25 | Loss: 0.00191246
Iteration 19/25 | Loss: 0.00191246
Iteration 20/25 | Loss: 0.00191246
Iteration 21/25 | Loss: 0.00191246
Iteration 22/25 | Loss: 0.00191246
Iteration 23/25 | Loss: 0.00191246
Iteration 24/25 | Loss: 0.00191246
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0019124584505334496, 0.0019124584505334496, 0.0019124584505334496, 0.0019124584505334496, 0.0019124584505334496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019124584505334496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191246
Iteration 2/1000 | Loss: 0.00002106
Iteration 3/1000 | Loss: 0.00001048
Iteration 4/1000 | Loss: 0.00000942
Iteration 5/1000 | Loss: 0.00000878
Iteration 6/1000 | Loss: 0.00000829
Iteration 7/1000 | Loss: 0.00000793
Iteration 8/1000 | Loss: 0.00000762
Iteration 9/1000 | Loss: 0.00000744
Iteration 10/1000 | Loss: 0.00000743
Iteration 11/1000 | Loss: 0.00000743
Iteration 12/1000 | Loss: 0.00000742
Iteration 13/1000 | Loss: 0.00000742
Iteration 14/1000 | Loss: 0.00000741
Iteration 15/1000 | Loss: 0.00000741
Iteration 16/1000 | Loss: 0.00000741
Iteration 17/1000 | Loss: 0.00000740
Iteration 18/1000 | Loss: 0.00000735
Iteration 19/1000 | Loss: 0.00000733
Iteration 20/1000 | Loss: 0.00000732
Iteration 21/1000 | Loss: 0.00000731
Iteration 22/1000 | Loss: 0.00000730
Iteration 23/1000 | Loss: 0.00000726
Iteration 24/1000 | Loss: 0.00000726
Iteration 25/1000 | Loss: 0.00000723
Iteration 26/1000 | Loss: 0.00000723
Iteration 27/1000 | Loss: 0.00000722
Iteration 28/1000 | Loss: 0.00000722
Iteration 29/1000 | Loss: 0.00000721
Iteration 30/1000 | Loss: 0.00000721
Iteration 31/1000 | Loss: 0.00000721
Iteration 32/1000 | Loss: 0.00000720
Iteration 33/1000 | Loss: 0.00000720
Iteration 34/1000 | Loss: 0.00000720
Iteration 35/1000 | Loss: 0.00000720
Iteration 36/1000 | Loss: 0.00000720
Iteration 37/1000 | Loss: 0.00000720
Iteration 38/1000 | Loss: 0.00000720
Iteration 39/1000 | Loss: 0.00000720
Iteration 40/1000 | Loss: 0.00000720
Iteration 41/1000 | Loss: 0.00000720
Iteration 42/1000 | Loss: 0.00000719
Iteration 43/1000 | Loss: 0.00000719
Iteration 44/1000 | Loss: 0.00000719
Iteration 45/1000 | Loss: 0.00000719
Iteration 46/1000 | Loss: 0.00000718
Iteration 47/1000 | Loss: 0.00000718
Iteration 48/1000 | Loss: 0.00000718
Iteration 49/1000 | Loss: 0.00000718
Iteration 50/1000 | Loss: 0.00000717
Iteration 51/1000 | Loss: 0.00000717
Iteration 52/1000 | Loss: 0.00000717
Iteration 53/1000 | Loss: 0.00000717
Iteration 54/1000 | Loss: 0.00000717
Iteration 55/1000 | Loss: 0.00000716
Iteration 56/1000 | Loss: 0.00000716
Iteration 57/1000 | Loss: 0.00000715
Iteration 58/1000 | Loss: 0.00000715
Iteration 59/1000 | Loss: 0.00000714
Iteration 60/1000 | Loss: 0.00000710
Iteration 61/1000 | Loss: 0.00000709
Iteration 62/1000 | Loss: 0.00000708
Iteration 63/1000 | Loss: 0.00000707
Iteration 64/1000 | Loss: 0.00000706
Iteration 65/1000 | Loss: 0.00000706
Iteration 66/1000 | Loss: 0.00000705
Iteration 67/1000 | Loss: 0.00000704
Iteration 68/1000 | Loss: 0.00000704
Iteration 69/1000 | Loss: 0.00000703
Iteration 70/1000 | Loss: 0.00000703
Iteration 71/1000 | Loss: 0.00000703
Iteration 72/1000 | Loss: 0.00000703
Iteration 73/1000 | Loss: 0.00000702
Iteration 74/1000 | Loss: 0.00000702
Iteration 75/1000 | Loss: 0.00000702
Iteration 76/1000 | Loss: 0.00000702
Iteration 77/1000 | Loss: 0.00000702
Iteration 78/1000 | Loss: 0.00000701
Iteration 79/1000 | Loss: 0.00000701
Iteration 80/1000 | Loss: 0.00000701
Iteration 81/1000 | Loss: 0.00000701
Iteration 82/1000 | Loss: 0.00000701
Iteration 83/1000 | Loss: 0.00000701
Iteration 84/1000 | Loss: 0.00000700
Iteration 85/1000 | Loss: 0.00000700
Iteration 86/1000 | Loss: 0.00000700
Iteration 87/1000 | Loss: 0.00000700
Iteration 88/1000 | Loss: 0.00000700
Iteration 89/1000 | Loss: 0.00000700
Iteration 90/1000 | Loss: 0.00000700
Iteration 91/1000 | Loss: 0.00000700
Iteration 92/1000 | Loss: 0.00000700
Iteration 93/1000 | Loss: 0.00000700
Iteration 94/1000 | Loss: 0.00000700
Iteration 95/1000 | Loss: 0.00000700
Iteration 96/1000 | Loss: 0.00000699
Iteration 97/1000 | Loss: 0.00000699
Iteration 98/1000 | Loss: 0.00000699
Iteration 99/1000 | Loss: 0.00000699
Iteration 100/1000 | Loss: 0.00000699
Iteration 101/1000 | Loss: 0.00000699
Iteration 102/1000 | Loss: 0.00000699
Iteration 103/1000 | Loss: 0.00000699
Iteration 104/1000 | Loss: 0.00000699
Iteration 105/1000 | Loss: 0.00000699
Iteration 106/1000 | Loss: 0.00000698
Iteration 107/1000 | Loss: 0.00000698
Iteration 108/1000 | Loss: 0.00000698
Iteration 109/1000 | Loss: 0.00000698
Iteration 110/1000 | Loss: 0.00000698
Iteration 111/1000 | Loss: 0.00000697
Iteration 112/1000 | Loss: 0.00000697
Iteration 113/1000 | Loss: 0.00000697
Iteration 114/1000 | Loss: 0.00000697
Iteration 115/1000 | Loss: 0.00000697
Iteration 116/1000 | Loss: 0.00000697
Iteration 117/1000 | Loss: 0.00000697
Iteration 118/1000 | Loss: 0.00000697
Iteration 119/1000 | Loss: 0.00000697
Iteration 120/1000 | Loss: 0.00000697
Iteration 121/1000 | Loss: 0.00000697
Iteration 122/1000 | Loss: 0.00000696
Iteration 123/1000 | Loss: 0.00000696
Iteration 124/1000 | Loss: 0.00000696
Iteration 125/1000 | Loss: 0.00000696
Iteration 126/1000 | Loss: 0.00000696
Iteration 127/1000 | Loss: 0.00000696
Iteration 128/1000 | Loss: 0.00000696
Iteration 129/1000 | Loss: 0.00000696
Iteration 130/1000 | Loss: 0.00000696
Iteration 131/1000 | Loss: 0.00000696
Iteration 132/1000 | Loss: 0.00000696
Iteration 133/1000 | Loss: 0.00000695
Iteration 134/1000 | Loss: 0.00000695
Iteration 135/1000 | Loss: 0.00000695
Iteration 136/1000 | Loss: 0.00000695
Iteration 137/1000 | Loss: 0.00000695
Iteration 138/1000 | Loss: 0.00000695
Iteration 139/1000 | Loss: 0.00000695
Iteration 140/1000 | Loss: 0.00000695
Iteration 141/1000 | Loss: 0.00000695
Iteration 142/1000 | Loss: 0.00000695
Iteration 143/1000 | Loss: 0.00000695
Iteration 144/1000 | Loss: 0.00000694
Iteration 145/1000 | Loss: 0.00000694
Iteration 146/1000 | Loss: 0.00000694
Iteration 147/1000 | Loss: 0.00000694
Iteration 148/1000 | Loss: 0.00000694
Iteration 149/1000 | Loss: 0.00000694
Iteration 150/1000 | Loss: 0.00000694
Iteration 151/1000 | Loss: 0.00000694
Iteration 152/1000 | Loss: 0.00000694
Iteration 153/1000 | Loss: 0.00000694
Iteration 154/1000 | Loss: 0.00000694
Iteration 155/1000 | Loss: 0.00000694
Iteration 156/1000 | Loss: 0.00000694
Iteration 157/1000 | Loss: 0.00000694
Iteration 158/1000 | Loss: 0.00000694
Iteration 159/1000 | Loss: 0.00000694
Iteration 160/1000 | Loss: 0.00000694
Iteration 161/1000 | Loss: 0.00000694
Iteration 162/1000 | Loss: 0.00000693
Iteration 163/1000 | Loss: 0.00000693
Iteration 164/1000 | Loss: 0.00000693
Iteration 165/1000 | Loss: 0.00000693
Iteration 166/1000 | Loss: 0.00000693
Iteration 167/1000 | Loss: 0.00000693
Iteration 168/1000 | Loss: 0.00000693
Iteration 169/1000 | Loss: 0.00000693
Iteration 170/1000 | Loss: 0.00000693
Iteration 171/1000 | Loss: 0.00000693
Iteration 172/1000 | Loss: 0.00000693
Iteration 173/1000 | Loss: 0.00000693
Iteration 174/1000 | Loss: 0.00000693
Iteration 175/1000 | Loss: 0.00000693
Iteration 176/1000 | Loss: 0.00000693
Iteration 177/1000 | Loss: 0.00000693
Iteration 178/1000 | Loss: 0.00000693
Iteration 179/1000 | Loss: 0.00000693
Iteration 180/1000 | Loss: 0.00000693
Iteration 181/1000 | Loss: 0.00000693
Iteration 182/1000 | Loss: 0.00000693
Iteration 183/1000 | Loss: 0.00000693
Iteration 184/1000 | Loss: 0.00000693
Iteration 185/1000 | Loss: 0.00000693
Iteration 186/1000 | Loss: 0.00000693
Iteration 187/1000 | Loss: 0.00000693
Iteration 188/1000 | Loss: 0.00000693
Iteration 189/1000 | Loss: 0.00000693
Iteration 190/1000 | Loss: 0.00000693
Iteration 191/1000 | Loss: 0.00000693
Iteration 192/1000 | Loss: 0.00000693
Iteration 193/1000 | Loss: 0.00000693
Iteration 194/1000 | Loss: 0.00000693
Iteration 195/1000 | Loss: 0.00000693
Iteration 196/1000 | Loss: 0.00000693
Iteration 197/1000 | Loss: 0.00000693
Iteration 198/1000 | Loss: 0.00000693
Iteration 199/1000 | Loss: 0.00000693
Iteration 200/1000 | Loss: 0.00000693
Iteration 201/1000 | Loss: 0.00000693
Iteration 202/1000 | Loss: 0.00000693
Iteration 203/1000 | Loss: 0.00000693
Iteration 204/1000 | Loss: 0.00000693
Iteration 205/1000 | Loss: 0.00000693
Iteration 206/1000 | Loss: 0.00000693
Iteration 207/1000 | Loss: 0.00000693
Iteration 208/1000 | Loss: 0.00000693
Iteration 209/1000 | Loss: 0.00000693
Iteration 210/1000 | Loss: 0.00000693
Iteration 211/1000 | Loss: 0.00000693
Iteration 212/1000 | Loss: 0.00000693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [6.934295470273355e-06, 6.934295470273355e-06, 6.934295470273355e-06, 6.934295470273355e-06, 6.934295470273355e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.934295470273355e-06

Optimization complete. Final v2v error: 2.1844570636749268 mm

Highest mean error: 3.2596607208251953 mm for frame 61

Lowest mean error: 1.980242133140564 mm for frame 83

Saving results

Total time: 36.0013473033905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950333
Iteration 2/25 | Loss: 0.00132831
Iteration 3/25 | Loss: 0.00113439
Iteration 4/25 | Loss: 0.00106959
Iteration 5/25 | Loss: 0.00108569
Iteration 6/25 | Loss: 0.00104742
Iteration 7/25 | Loss: 0.00104712
Iteration 8/25 | Loss: 0.00104428
Iteration 9/25 | Loss: 0.00104406
Iteration 10/25 | Loss: 0.00104044
Iteration 11/25 | Loss: 0.00103832
Iteration 12/25 | Loss: 0.00103749
Iteration 13/25 | Loss: 0.00103740
Iteration 14/25 | Loss: 0.00103738
Iteration 15/25 | Loss: 0.00103738
Iteration 16/25 | Loss: 0.00103738
Iteration 17/25 | Loss: 0.00103732
Iteration 18/25 | Loss: 0.00103731
Iteration 19/25 | Loss: 0.00103727
Iteration 20/25 | Loss: 0.00103727
Iteration 21/25 | Loss: 0.00103727
Iteration 22/25 | Loss: 0.00103727
Iteration 23/25 | Loss: 0.00103727
Iteration 24/25 | Loss: 0.00103727
Iteration 25/25 | Loss: 0.00103727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16402984
Iteration 2/25 | Loss: 0.00189642
Iteration 3/25 | Loss: 0.00189642
Iteration 4/25 | Loss: 0.00189642
Iteration 5/25 | Loss: 0.00189642
Iteration 6/25 | Loss: 0.00189642
Iteration 7/25 | Loss: 0.00189642
Iteration 8/25 | Loss: 0.00189642
Iteration 9/25 | Loss: 0.00189642
Iteration 10/25 | Loss: 0.00189642
Iteration 11/25 | Loss: 0.00189642
Iteration 12/25 | Loss: 0.00189642
Iteration 13/25 | Loss: 0.00189642
Iteration 14/25 | Loss: 0.00189642
Iteration 15/25 | Loss: 0.00189642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0018964176997542381, 0.0018964176997542381, 0.0018964176997542381, 0.0018964176997542381, 0.0018964176997542381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018964176997542381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189642
Iteration 2/1000 | Loss: 0.00007206
Iteration 3/1000 | Loss: 0.00011686
Iteration 4/1000 | Loss: 0.00003930
Iteration 5/1000 | Loss: 0.00003054
Iteration 6/1000 | Loss: 0.00002653
Iteration 7/1000 | Loss: 0.00002401
Iteration 8/1000 | Loss: 0.00002259
Iteration 9/1000 | Loss: 0.00037618
Iteration 10/1000 | Loss: 0.00003202
Iteration 11/1000 | Loss: 0.00002172
Iteration 12/1000 | Loss: 0.00002003
Iteration 13/1000 | Loss: 0.00001895
Iteration 14/1000 | Loss: 0.00001801
Iteration 15/1000 | Loss: 0.00001737
Iteration 16/1000 | Loss: 0.00001709
Iteration 17/1000 | Loss: 0.00001677
Iteration 18/1000 | Loss: 0.00001664
Iteration 19/1000 | Loss: 0.00001658
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001650
Iteration 22/1000 | Loss: 0.00001650
Iteration 23/1000 | Loss: 0.00001647
Iteration 24/1000 | Loss: 0.00001642
Iteration 25/1000 | Loss: 0.00001641
Iteration 26/1000 | Loss: 0.00001641
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001637
Iteration 29/1000 | Loss: 0.00001626
Iteration 30/1000 | Loss: 0.00001625
Iteration 31/1000 | Loss: 0.00001619
Iteration 32/1000 | Loss: 0.00001613
Iteration 33/1000 | Loss: 0.00001613
Iteration 34/1000 | Loss: 0.00001611
Iteration 35/1000 | Loss: 0.00001610
Iteration 36/1000 | Loss: 0.00001609
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001609
Iteration 39/1000 | Loss: 0.00001608
Iteration 40/1000 | Loss: 0.00001608
Iteration 41/1000 | Loss: 0.00001607
Iteration 42/1000 | Loss: 0.00001607
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001607
Iteration 45/1000 | Loss: 0.00001607
Iteration 46/1000 | Loss: 0.00001607
Iteration 47/1000 | Loss: 0.00001606
Iteration 48/1000 | Loss: 0.00001606
Iteration 49/1000 | Loss: 0.00001606
Iteration 50/1000 | Loss: 0.00001606
Iteration 51/1000 | Loss: 0.00001606
Iteration 52/1000 | Loss: 0.00001606
Iteration 53/1000 | Loss: 0.00001606
Iteration 54/1000 | Loss: 0.00001606
Iteration 55/1000 | Loss: 0.00001606
Iteration 56/1000 | Loss: 0.00001606
Iteration 57/1000 | Loss: 0.00001605
Iteration 58/1000 | Loss: 0.00001605
Iteration 59/1000 | Loss: 0.00001605
Iteration 60/1000 | Loss: 0.00001604
Iteration 61/1000 | Loss: 0.00001604
Iteration 62/1000 | Loss: 0.00001603
Iteration 63/1000 | Loss: 0.00001602
Iteration 64/1000 | Loss: 0.00001602
Iteration 65/1000 | Loss: 0.00001602
Iteration 66/1000 | Loss: 0.00001601
Iteration 67/1000 | Loss: 0.00001601
Iteration 68/1000 | Loss: 0.00001601
Iteration 69/1000 | Loss: 0.00001601
Iteration 70/1000 | Loss: 0.00001601
Iteration 71/1000 | Loss: 0.00001601
Iteration 72/1000 | Loss: 0.00001600
Iteration 73/1000 | Loss: 0.00001600
Iteration 74/1000 | Loss: 0.00001600
Iteration 75/1000 | Loss: 0.00001600
Iteration 76/1000 | Loss: 0.00001600
Iteration 77/1000 | Loss: 0.00001600
Iteration 78/1000 | Loss: 0.00001600
Iteration 79/1000 | Loss: 0.00001599
Iteration 80/1000 | Loss: 0.00001599
Iteration 81/1000 | Loss: 0.00001599
Iteration 82/1000 | Loss: 0.00001599
Iteration 83/1000 | Loss: 0.00001599
Iteration 84/1000 | Loss: 0.00001599
Iteration 85/1000 | Loss: 0.00001598
Iteration 86/1000 | Loss: 0.00001598
Iteration 87/1000 | Loss: 0.00001598
Iteration 88/1000 | Loss: 0.00001598
Iteration 89/1000 | Loss: 0.00001598
Iteration 90/1000 | Loss: 0.00001597
Iteration 91/1000 | Loss: 0.00001597
Iteration 92/1000 | Loss: 0.00001597
Iteration 93/1000 | Loss: 0.00001597
Iteration 94/1000 | Loss: 0.00001597
Iteration 95/1000 | Loss: 0.00001596
Iteration 96/1000 | Loss: 0.00001596
Iteration 97/1000 | Loss: 0.00001596
Iteration 98/1000 | Loss: 0.00001596
Iteration 99/1000 | Loss: 0.00001596
Iteration 100/1000 | Loss: 0.00001596
Iteration 101/1000 | Loss: 0.00001596
Iteration 102/1000 | Loss: 0.00001596
Iteration 103/1000 | Loss: 0.00001596
Iteration 104/1000 | Loss: 0.00001595
Iteration 105/1000 | Loss: 0.00001595
Iteration 106/1000 | Loss: 0.00001595
Iteration 107/1000 | Loss: 0.00001595
Iteration 108/1000 | Loss: 0.00001595
Iteration 109/1000 | Loss: 0.00001595
Iteration 110/1000 | Loss: 0.00001595
Iteration 111/1000 | Loss: 0.00001595
Iteration 112/1000 | Loss: 0.00001595
Iteration 113/1000 | Loss: 0.00001595
Iteration 114/1000 | Loss: 0.00001595
Iteration 115/1000 | Loss: 0.00001595
Iteration 116/1000 | Loss: 0.00001595
Iteration 117/1000 | Loss: 0.00001595
Iteration 118/1000 | Loss: 0.00001595
Iteration 119/1000 | Loss: 0.00001595
Iteration 120/1000 | Loss: 0.00001595
Iteration 121/1000 | Loss: 0.00001595
Iteration 122/1000 | Loss: 0.00001595
Iteration 123/1000 | Loss: 0.00001595
Iteration 124/1000 | Loss: 0.00001595
Iteration 125/1000 | Loss: 0.00001595
Iteration 126/1000 | Loss: 0.00001595
Iteration 127/1000 | Loss: 0.00001595
Iteration 128/1000 | Loss: 0.00001595
Iteration 129/1000 | Loss: 0.00001595
Iteration 130/1000 | Loss: 0.00001595
Iteration 131/1000 | Loss: 0.00001595
Iteration 132/1000 | Loss: 0.00001595
Iteration 133/1000 | Loss: 0.00001595
Iteration 134/1000 | Loss: 0.00001595
Iteration 135/1000 | Loss: 0.00001595
Iteration 136/1000 | Loss: 0.00001595
Iteration 137/1000 | Loss: 0.00001595
Iteration 138/1000 | Loss: 0.00001595
Iteration 139/1000 | Loss: 0.00001595
Iteration 140/1000 | Loss: 0.00001595
Iteration 141/1000 | Loss: 0.00001595
Iteration 142/1000 | Loss: 0.00001595
Iteration 143/1000 | Loss: 0.00001595
Iteration 144/1000 | Loss: 0.00001595
Iteration 145/1000 | Loss: 0.00001595
Iteration 146/1000 | Loss: 0.00001595
Iteration 147/1000 | Loss: 0.00001595
Iteration 148/1000 | Loss: 0.00001595
Iteration 149/1000 | Loss: 0.00001595
Iteration 150/1000 | Loss: 0.00001595
Iteration 151/1000 | Loss: 0.00001595
Iteration 152/1000 | Loss: 0.00001595
Iteration 153/1000 | Loss: 0.00001595
Iteration 154/1000 | Loss: 0.00001595
Iteration 155/1000 | Loss: 0.00001595
Iteration 156/1000 | Loss: 0.00001595
Iteration 157/1000 | Loss: 0.00001595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.594626701262314e-05, 1.594626701262314e-05, 1.594626701262314e-05, 1.594626701262314e-05, 1.594626701262314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.594626701262314e-05

Optimization complete. Final v2v error: 3.3560402393341064 mm

Highest mean error: 4.813529968261719 mm for frame 136

Lowest mean error: 2.8118550777435303 mm for frame 35

Saving results

Total time: 64.84709668159485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459050
Iteration 2/25 | Loss: 0.00130714
Iteration 3/25 | Loss: 0.00100510
Iteration 4/25 | Loss: 0.00097673
Iteration 5/25 | Loss: 0.00097467
Iteration 6/25 | Loss: 0.00097412
Iteration 7/25 | Loss: 0.00097412
Iteration 8/25 | Loss: 0.00097412
Iteration 9/25 | Loss: 0.00097412
Iteration 10/25 | Loss: 0.00097412
Iteration 11/25 | Loss: 0.00097412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009741150424815714, 0.0009741150424815714, 0.0009741150424815714, 0.0009741150424815714, 0.0009741150424815714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009741150424815714

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22417700
Iteration 2/25 | Loss: 0.00171978
Iteration 3/25 | Loss: 0.00171978
Iteration 4/25 | Loss: 0.00171978
Iteration 5/25 | Loss: 0.00171978
Iteration 6/25 | Loss: 0.00171978
Iteration 7/25 | Loss: 0.00171978
Iteration 8/25 | Loss: 0.00171978
Iteration 9/25 | Loss: 0.00171978
Iteration 10/25 | Loss: 0.00171978
Iteration 11/25 | Loss: 0.00171978
Iteration 12/25 | Loss: 0.00171978
Iteration 13/25 | Loss: 0.00171978
Iteration 14/25 | Loss: 0.00171978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0017197818960994482, 0.0017197818960994482, 0.0017197818960994482, 0.0017197818960994482, 0.0017197818960994482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017197818960994482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171978
Iteration 2/1000 | Loss: 0.00002425
Iteration 3/1000 | Loss: 0.00001281
Iteration 4/1000 | Loss: 0.00001137
Iteration 5/1000 | Loss: 0.00001059
Iteration 6/1000 | Loss: 0.00001003
Iteration 7/1000 | Loss: 0.00000971
Iteration 8/1000 | Loss: 0.00000943
Iteration 9/1000 | Loss: 0.00000913
Iteration 10/1000 | Loss: 0.00000898
Iteration 11/1000 | Loss: 0.00000896
Iteration 12/1000 | Loss: 0.00000890
Iteration 13/1000 | Loss: 0.00000890
Iteration 14/1000 | Loss: 0.00000889
Iteration 15/1000 | Loss: 0.00000885
Iteration 16/1000 | Loss: 0.00000885
Iteration 17/1000 | Loss: 0.00000883
Iteration 18/1000 | Loss: 0.00000883
Iteration 19/1000 | Loss: 0.00000882
Iteration 20/1000 | Loss: 0.00000882
Iteration 21/1000 | Loss: 0.00000881
Iteration 22/1000 | Loss: 0.00000881
Iteration 23/1000 | Loss: 0.00000880
Iteration 24/1000 | Loss: 0.00000880
Iteration 25/1000 | Loss: 0.00000880
Iteration 26/1000 | Loss: 0.00000879
Iteration 27/1000 | Loss: 0.00000879
Iteration 28/1000 | Loss: 0.00000879
Iteration 29/1000 | Loss: 0.00000878
Iteration 30/1000 | Loss: 0.00000878
Iteration 31/1000 | Loss: 0.00000877
Iteration 32/1000 | Loss: 0.00000876
Iteration 33/1000 | Loss: 0.00000876
Iteration 34/1000 | Loss: 0.00000875
Iteration 35/1000 | Loss: 0.00000875
Iteration 36/1000 | Loss: 0.00000875
Iteration 37/1000 | Loss: 0.00000874
Iteration 38/1000 | Loss: 0.00000873
Iteration 39/1000 | Loss: 0.00000872
Iteration 40/1000 | Loss: 0.00000872
Iteration 41/1000 | Loss: 0.00000871
Iteration 42/1000 | Loss: 0.00000870
Iteration 43/1000 | Loss: 0.00000870
Iteration 44/1000 | Loss: 0.00000869
Iteration 45/1000 | Loss: 0.00000868
Iteration 46/1000 | Loss: 0.00000868
Iteration 47/1000 | Loss: 0.00000867
Iteration 48/1000 | Loss: 0.00000867
Iteration 49/1000 | Loss: 0.00000867
Iteration 50/1000 | Loss: 0.00000867
Iteration 51/1000 | Loss: 0.00000867
Iteration 52/1000 | Loss: 0.00000866
Iteration 53/1000 | Loss: 0.00000866
Iteration 54/1000 | Loss: 0.00000866
Iteration 55/1000 | Loss: 0.00000865
Iteration 56/1000 | Loss: 0.00000865
Iteration 57/1000 | Loss: 0.00000865
Iteration 58/1000 | Loss: 0.00000865
Iteration 59/1000 | Loss: 0.00000865
Iteration 60/1000 | Loss: 0.00000865
Iteration 61/1000 | Loss: 0.00000864
Iteration 62/1000 | Loss: 0.00000864
Iteration 63/1000 | Loss: 0.00000864
Iteration 64/1000 | Loss: 0.00000864
Iteration 65/1000 | Loss: 0.00000863
Iteration 66/1000 | Loss: 0.00000863
Iteration 67/1000 | Loss: 0.00000863
Iteration 68/1000 | Loss: 0.00000863
Iteration 69/1000 | Loss: 0.00000863
Iteration 70/1000 | Loss: 0.00000863
Iteration 71/1000 | Loss: 0.00000863
Iteration 72/1000 | Loss: 0.00000863
Iteration 73/1000 | Loss: 0.00000863
Iteration 74/1000 | Loss: 0.00000862
Iteration 75/1000 | Loss: 0.00000862
Iteration 76/1000 | Loss: 0.00000862
Iteration 77/1000 | Loss: 0.00000862
Iteration 78/1000 | Loss: 0.00000862
Iteration 79/1000 | Loss: 0.00000862
Iteration 80/1000 | Loss: 0.00000862
Iteration 81/1000 | Loss: 0.00000862
Iteration 82/1000 | Loss: 0.00000862
Iteration 83/1000 | Loss: 0.00000862
Iteration 84/1000 | Loss: 0.00000861
Iteration 85/1000 | Loss: 0.00000861
Iteration 86/1000 | Loss: 0.00000861
Iteration 87/1000 | Loss: 0.00000860
Iteration 88/1000 | Loss: 0.00000860
Iteration 89/1000 | Loss: 0.00000859
Iteration 90/1000 | Loss: 0.00000859
Iteration 91/1000 | Loss: 0.00000859
Iteration 92/1000 | Loss: 0.00000858
Iteration 93/1000 | Loss: 0.00000858
Iteration 94/1000 | Loss: 0.00000858
Iteration 95/1000 | Loss: 0.00000858
Iteration 96/1000 | Loss: 0.00000857
Iteration 97/1000 | Loss: 0.00000857
Iteration 98/1000 | Loss: 0.00000857
Iteration 99/1000 | Loss: 0.00000857
Iteration 100/1000 | Loss: 0.00000856
Iteration 101/1000 | Loss: 0.00000856
Iteration 102/1000 | Loss: 0.00000856
Iteration 103/1000 | Loss: 0.00000856
Iteration 104/1000 | Loss: 0.00000856
Iteration 105/1000 | Loss: 0.00000856
Iteration 106/1000 | Loss: 0.00000855
Iteration 107/1000 | Loss: 0.00000855
Iteration 108/1000 | Loss: 0.00000855
Iteration 109/1000 | Loss: 0.00000855
Iteration 110/1000 | Loss: 0.00000855
Iteration 111/1000 | Loss: 0.00000855
Iteration 112/1000 | Loss: 0.00000855
Iteration 113/1000 | Loss: 0.00000854
Iteration 114/1000 | Loss: 0.00000854
Iteration 115/1000 | Loss: 0.00000854
Iteration 116/1000 | Loss: 0.00000854
Iteration 117/1000 | Loss: 0.00000854
Iteration 118/1000 | Loss: 0.00000854
Iteration 119/1000 | Loss: 0.00000854
Iteration 120/1000 | Loss: 0.00000854
Iteration 121/1000 | Loss: 0.00000853
Iteration 122/1000 | Loss: 0.00000853
Iteration 123/1000 | Loss: 0.00000853
Iteration 124/1000 | Loss: 0.00000853
Iteration 125/1000 | Loss: 0.00000852
Iteration 126/1000 | Loss: 0.00000852
Iteration 127/1000 | Loss: 0.00000852
Iteration 128/1000 | Loss: 0.00000852
Iteration 129/1000 | Loss: 0.00000851
Iteration 130/1000 | Loss: 0.00000851
Iteration 131/1000 | Loss: 0.00000851
Iteration 132/1000 | Loss: 0.00000851
Iteration 133/1000 | Loss: 0.00000851
Iteration 134/1000 | Loss: 0.00000851
Iteration 135/1000 | Loss: 0.00000851
Iteration 136/1000 | Loss: 0.00000851
Iteration 137/1000 | Loss: 0.00000851
Iteration 138/1000 | Loss: 0.00000851
Iteration 139/1000 | Loss: 0.00000851
Iteration 140/1000 | Loss: 0.00000851
Iteration 141/1000 | Loss: 0.00000851
Iteration 142/1000 | Loss: 0.00000850
Iteration 143/1000 | Loss: 0.00000850
Iteration 144/1000 | Loss: 0.00000850
Iteration 145/1000 | Loss: 0.00000850
Iteration 146/1000 | Loss: 0.00000850
Iteration 147/1000 | Loss: 0.00000850
Iteration 148/1000 | Loss: 0.00000850
Iteration 149/1000 | Loss: 0.00000850
Iteration 150/1000 | Loss: 0.00000849
Iteration 151/1000 | Loss: 0.00000849
Iteration 152/1000 | Loss: 0.00000849
Iteration 153/1000 | Loss: 0.00000849
Iteration 154/1000 | Loss: 0.00000849
Iteration 155/1000 | Loss: 0.00000849
Iteration 156/1000 | Loss: 0.00000849
Iteration 157/1000 | Loss: 0.00000849
Iteration 158/1000 | Loss: 0.00000849
Iteration 159/1000 | Loss: 0.00000848
Iteration 160/1000 | Loss: 0.00000848
Iteration 161/1000 | Loss: 0.00000848
Iteration 162/1000 | Loss: 0.00000848
Iteration 163/1000 | Loss: 0.00000848
Iteration 164/1000 | Loss: 0.00000848
Iteration 165/1000 | Loss: 0.00000848
Iteration 166/1000 | Loss: 0.00000848
Iteration 167/1000 | Loss: 0.00000848
Iteration 168/1000 | Loss: 0.00000848
Iteration 169/1000 | Loss: 0.00000848
Iteration 170/1000 | Loss: 0.00000848
Iteration 171/1000 | Loss: 0.00000848
Iteration 172/1000 | Loss: 0.00000848
Iteration 173/1000 | Loss: 0.00000848
Iteration 174/1000 | Loss: 0.00000847
Iteration 175/1000 | Loss: 0.00000847
Iteration 176/1000 | Loss: 0.00000847
Iteration 177/1000 | Loss: 0.00000847
Iteration 178/1000 | Loss: 0.00000847
Iteration 179/1000 | Loss: 0.00000847
Iteration 180/1000 | Loss: 0.00000847
Iteration 181/1000 | Loss: 0.00000847
Iteration 182/1000 | Loss: 0.00000847
Iteration 183/1000 | Loss: 0.00000847
Iteration 184/1000 | Loss: 0.00000847
Iteration 185/1000 | Loss: 0.00000847
Iteration 186/1000 | Loss: 0.00000847
Iteration 187/1000 | Loss: 0.00000847
Iteration 188/1000 | Loss: 0.00000847
Iteration 189/1000 | Loss: 0.00000847
Iteration 190/1000 | Loss: 0.00000847
Iteration 191/1000 | Loss: 0.00000847
Iteration 192/1000 | Loss: 0.00000847
Iteration 193/1000 | Loss: 0.00000847
Iteration 194/1000 | Loss: 0.00000847
Iteration 195/1000 | Loss: 0.00000847
Iteration 196/1000 | Loss: 0.00000847
Iteration 197/1000 | Loss: 0.00000847
Iteration 198/1000 | Loss: 0.00000847
Iteration 199/1000 | Loss: 0.00000847
Iteration 200/1000 | Loss: 0.00000847
Iteration 201/1000 | Loss: 0.00000847
Iteration 202/1000 | Loss: 0.00000847
Iteration 203/1000 | Loss: 0.00000847
Iteration 204/1000 | Loss: 0.00000847
Iteration 205/1000 | Loss: 0.00000847
Iteration 206/1000 | Loss: 0.00000847
Iteration 207/1000 | Loss: 0.00000847
Iteration 208/1000 | Loss: 0.00000847
Iteration 209/1000 | Loss: 0.00000847
Iteration 210/1000 | Loss: 0.00000847
Iteration 211/1000 | Loss: 0.00000847
Iteration 212/1000 | Loss: 0.00000847
Iteration 213/1000 | Loss: 0.00000847
Iteration 214/1000 | Loss: 0.00000847
Iteration 215/1000 | Loss: 0.00000847
Iteration 216/1000 | Loss: 0.00000847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [8.468911801173817e-06, 8.468911801173817e-06, 8.468911801173817e-06, 8.468911801173817e-06, 8.468911801173817e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.468911801173817e-06

Optimization complete. Final v2v error: 2.3566572666168213 mm

Highest mean error: 3.4367847442626953 mm for frame 69

Lowest mean error: 1.9628056287765503 mm for frame 121

Saving results

Total time: 38.29523944854736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544355
Iteration 2/25 | Loss: 0.00124946
Iteration 3/25 | Loss: 0.00101933
Iteration 4/25 | Loss: 0.00100469
Iteration 5/25 | Loss: 0.00100211
Iteration 6/25 | Loss: 0.00100169
Iteration 7/25 | Loss: 0.00100169
Iteration 8/25 | Loss: 0.00100169
Iteration 9/25 | Loss: 0.00100169
Iteration 10/25 | Loss: 0.00100169
Iteration 11/25 | Loss: 0.00100169
Iteration 12/25 | Loss: 0.00100169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010016862070187926, 0.0010016862070187926, 0.0010016862070187926, 0.0010016862070187926, 0.0010016862070187926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010016862070187926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00146413
Iteration 2/25 | Loss: 0.00117761
Iteration 3/25 | Loss: 0.00117759
Iteration 4/25 | Loss: 0.00117759
Iteration 5/25 | Loss: 0.00117759
Iteration 6/25 | Loss: 0.00117759
Iteration 7/25 | Loss: 0.00117759
Iteration 8/25 | Loss: 0.00117759
Iteration 9/25 | Loss: 0.00117759
Iteration 10/25 | Loss: 0.00117759
Iteration 11/25 | Loss: 0.00117759
Iteration 12/25 | Loss: 0.00117759
Iteration 13/25 | Loss: 0.00117759
Iteration 14/25 | Loss: 0.00117759
Iteration 15/25 | Loss: 0.00117759
Iteration 16/25 | Loss: 0.00117759
Iteration 17/25 | Loss: 0.00117759
Iteration 18/25 | Loss: 0.00117759
Iteration 19/25 | Loss: 0.00117759
Iteration 20/25 | Loss: 0.00117759
Iteration 21/25 | Loss: 0.00117759
Iteration 22/25 | Loss: 0.00117759
Iteration 23/25 | Loss: 0.00117759
Iteration 24/25 | Loss: 0.00117759
Iteration 25/25 | Loss: 0.00117759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117759
Iteration 2/1000 | Loss: 0.00002694
Iteration 3/1000 | Loss: 0.00001932
Iteration 4/1000 | Loss: 0.00001745
Iteration 5/1000 | Loss: 0.00001595
Iteration 6/1000 | Loss: 0.00001552
Iteration 7/1000 | Loss: 0.00001509
Iteration 8/1000 | Loss: 0.00001470
Iteration 9/1000 | Loss: 0.00001448
Iteration 10/1000 | Loss: 0.00001425
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001423
Iteration 13/1000 | Loss: 0.00001422
Iteration 14/1000 | Loss: 0.00001417
Iteration 15/1000 | Loss: 0.00001415
Iteration 16/1000 | Loss: 0.00001402
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001399
Iteration 19/1000 | Loss: 0.00001390
Iteration 20/1000 | Loss: 0.00001390
Iteration 21/1000 | Loss: 0.00001388
Iteration 22/1000 | Loss: 0.00001386
Iteration 23/1000 | Loss: 0.00001386
Iteration 24/1000 | Loss: 0.00001386
Iteration 25/1000 | Loss: 0.00001386
Iteration 26/1000 | Loss: 0.00001385
Iteration 27/1000 | Loss: 0.00001385
Iteration 28/1000 | Loss: 0.00001385
Iteration 29/1000 | Loss: 0.00001385
Iteration 30/1000 | Loss: 0.00001385
Iteration 31/1000 | Loss: 0.00001385
Iteration 32/1000 | Loss: 0.00001385
Iteration 33/1000 | Loss: 0.00001385
Iteration 34/1000 | Loss: 0.00001385
Iteration 35/1000 | Loss: 0.00001384
Iteration 36/1000 | Loss: 0.00001384
Iteration 37/1000 | Loss: 0.00001384
Iteration 38/1000 | Loss: 0.00001384
Iteration 39/1000 | Loss: 0.00001384
Iteration 40/1000 | Loss: 0.00001384
Iteration 41/1000 | Loss: 0.00001384
Iteration 42/1000 | Loss: 0.00001383
Iteration 43/1000 | Loss: 0.00001382
Iteration 44/1000 | Loss: 0.00001382
Iteration 45/1000 | Loss: 0.00001382
Iteration 46/1000 | Loss: 0.00001382
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001382
Iteration 49/1000 | Loss: 0.00001382
Iteration 50/1000 | Loss: 0.00001382
Iteration 51/1000 | Loss: 0.00001381
Iteration 52/1000 | Loss: 0.00001381
Iteration 53/1000 | Loss: 0.00001381
Iteration 54/1000 | Loss: 0.00001381
Iteration 55/1000 | Loss: 0.00001381
Iteration 56/1000 | Loss: 0.00001381
Iteration 57/1000 | Loss: 0.00001381
Iteration 58/1000 | Loss: 0.00001381
Iteration 59/1000 | Loss: 0.00001381
Iteration 60/1000 | Loss: 0.00001381
Iteration 61/1000 | Loss: 0.00001380
Iteration 62/1000 | Loss: 0.00001380
Iteration 63/1000 | Loss: 0.00001380
Iteration 64/1000 | Loss: 0.00001380
Iteration 65/1000 | Loss: 0.00001379
Iteration 66/1000 | Loss: 0.00001379
Iteration 67/1000 | Loss: 0.00001379
Iteration 68/1000 | Loss: 0.00001379
Iteration 69/1000 | Loss: 0.00001379
Iteration 70/1000 | Loss: 0.00001379
Iteration 71/1000 | Loss: 0.00001379
Iteration 72/1000 | Loss: 0.00001379
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001378
Iteration 75/1000 | Loss: 0.00001378
Iteration 76/1000 | Loss: 0.00001378
Iteration 77/1000 | Loss: 0.00001378
Iteration 78/1000 | Loss: 0.00001378
Iteration 79/1000 | Loss: 0.00001377
Iteration 80/1000 | Loss: 0.00001376
Iteration 81/1000 | Loss: 0.00001376
Iteration 82/1000 | Loss: 0.00001376
Iteration 83/1000 | Loss: 0.00001376
Iteration 84/1000 | Loss: 0.00001376
Iteration 85/1000 | Loss: 0.00001376
Iteration 86/1000 | Loss: 0.00001376
Iteration 87/1000 | Loss: 0.00001376
Iteration 88/1000 | Loss: 0.00001376
Iteration 89/1000 | Loss: 0.00001376
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001375
Iteration 92/1000 | Loss: 0.00001375
Iteration 93/1000 | Loss: 0.00001375
Iteration 94/1000 | Loss: 0.00001375
Iteration 95/1000 | Loss: 0.00001375
Iteration 96/1000 | Loss: 0.00001375
Iteration 97/1000 | Loss: 0.00001375
Iteration 98/1000 | Loss: 0.00001375
Iteration 99/1000 | Loss: 0.00001374
Iteration 100/1000 | Loss: 0.00001374
Iteration 101/1000 | Loss: 0.00001374
Iteration 102/1000 | Loss: 0.00001374
Iteration 103/1000 | Loss: 0.00001373
Iteration 104/1000 | Loss: 0.00001373
Iteration 105/1000 | Loss: 0.00001373
Iteration 106/1000 | Loss: 0.00001373
Iteration 107/1000 | Loss: 0.00001373
Iteration 108/1000 | Loss: 0.00001373
Iteration 109/1000 | Loss: 0.00001373
Iteration 110/1000 | Loss: 0.00001373
Iteration 111/1000 | Loss: 0.00001372
Iteration 112/1000 | Loss: 0.00001372
Iteration 113/1000 | Loss: 0.00001372
Iteration 114/1000 | Loss: 0.00001372
Iteration 115/1000 | Loss: 0.00001372
Iteration 116/1000 | Loss: 0.00001372
Iteration 117/1000 | Loss: 0.00001372
Iteration 118/1000 | Loss: 0.00001372
Iteration 119/1000 | Loss: 0.00001372
Iteration 120/1000 | Loss: 0.00001372
Iteration 121/1000 | Loss: 0.00001372
Iteration 122/1000 | Loss: 0.00001372
Iteration 123/1000 | Loss: 0.00001372
Iteration 124/1000 | Loss: 0.00001372
Iteration 125/1000 | Loss: 0.00001372
Iteration 126/1000 | Loss: 0.00001372
Iteration 127/1000 | Loss: 0.00001372
Iteration 128/1000 | Loss: 0.00001372
Iteration 129/1000 | Loss: 0.00001372
Iteration 130/1000 | Loss: 0.00001372
Iteration 131/1000 | Loss: 0.00001372
Iteration 132/1000 | Loss: 0.00001372
Iteration 133/1000 | Loss: 0.00001372
Iteration 134/1000 | Loss: 0.00001372
Iteration 135/1000 | Loss: 0.00001372
Iteration 136/1000 | Loss: 0.00001372
Iteration 137/1000 | Loss: 0.00001372
Iteration 138/1000 | Loss: 0.00001372
Iteration 139/1000 | Loss: 0.00001372
Iteration 140/1000 | Loss: 0.00001372
Iteration 141/1000 | Loss: 0.00001372
Iteration 142/1000 | Loss: 0.00001372
Iteration 143/1000 | Loss: 0.00001372
Iteration 144/1000 | Loss: 0.00001372
Iteration 145/1000 | Loss: 0.00001372
Iteration 146/1000 | Loss: 0.00001372
Iteration 147/1000 | Loss: 0.00001372
Iteration 148/1000 | Loss: 0.00001372
Iteration 149/1000 | Loss: 0.00001372
Iteration 150/1000 | Loss: 0.00001372
Iteration 151/1000 | Loss: 0.00001372
Iteration 152/1000 | Loss: 0.00001372
Iteration 153/1000 | Loss: 0.00001372
Iteration 154/1000 | Loss: 0.00001372
Iteration 155/1000 | Loss: 0.00001372
Iteration 156/1000 | Loss: 0.00001372
Iteration 157/1000 | Loss: 0.00001372
Iteration 158/1000 | Loss: 0.00001372
Iteration 159/1000 | Loss: 0.00001372
Iteration 160/1000 | Loss: 0.00001372
Iteration 161/1000 | Loss: 0.00001372
Iteration 162/1000 | Loss: 0.00001372
Iteration 163/1000 | Loss: 0.00001372
Iteration 164/1000 | Loss: 0.00001372
Iteration 165/1000 | Loss: 0.00001372
Iteration 166/1000 | Loss: 0.00001372
Iteration 167/1000 | Loss: 0.00001372
Iteration 168/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.3720409697270952e-05, 1.3720409697270952e-05, 1.3720409697270952e-05, 1.3720409697270952e-05, 1.3720409697270952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3720409697270952e-05

Optimization complete. Final v2v error: 3.1340129375457764 mm

Highest mean error: 3.606980085372925 mm for frame 50

Lowest mean error: 2.7627291679382324 mm for frame 74

Saving results

Total time: 40.65087628364563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00352875
Iteration 2/25 | Loss: 0.00102427
Iteration 3/25 | Loss: 0.00096090
Iteration 4/25 | Loss: 0.00094992
Iteration 5/25 | Loss: 0.00094636
Iteration 6/25 | Loss: 0.00094516
Iteration 7/25 | Loss: 0.00094516
Iteration 8/25 | Loss: 0.00094516
Iteration 9/25 | Loss: 0.00094516
Iteration 10/25 | Loss: 0.00094516
Iteration 11/25 | Loss: 0.00094516
Iteration 12/25 | Loss: 0.00094516
Iteration 13/25 | Loss: 0.00094516
Iteration 14/25 | Loss: 0.00094516
Iteration 15/25 | Loss: 0.00094516
Iteration 16/25 | Loss: 0.00094516
Iteration 17/25 | Loss: 0.00094516
Iteration 18/25 | Loss: 0.00094516
Iteration 19/25 | Loss: 0.00094516
Iteration 20/25 | Loss: 0.00094516
Iteration 21/25 | Loss: 0.00094516
Iteration 22/25 | Loss: 0.00094516
Iteration 23/25 | Loss: 0.00094516
Iteration 24/25 | Loss: 0.00094516
Iteration 25/25 | Loss: 0.00094516

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48475015
Iteration 2/25 | Loss: 0.00198874
Iteration 3/25 | Loss: 0.00198874
Iteration 4/25 | Loss: 0.00198874
Iteration 5/25 | Loss: 0.00198874
Iteration 6/25 | Loss: 0.00198874
Iteration 7/25 | Loss: 0.00198874
Iteration 8/25 | Loss: 0.00198874
Iteration 9/25 | Loss: 0.00198874
Iteration 10/25 | Loss: 0.00198874
Iteration 11/25 | Loss: 0.00198874
Iteration 12/25 | Loss: 0.00198874
Iteration 13/25 | Loss: 0.00198874
Iteration 14/25 | Loss: 0.00198874
Iteration 15/25 | Loss: 0.00198874
Iteration 16/25 | Loss: 0.00198874
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019887362141162157, 0.0019887362141162157, 0.0019887362141162157, 0.0019887362141162157, 0.0019887362141162157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019887362141162157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198874
Iteration 2/1000 | Loss: 0.00001339
Iteration 3/1000 | Loss: 0.00000990
Iteration 4/1000 | Loss: 0.00000894
Iteration 5/1000 | Loss: 0.00000850
Iteration 6/1000 | Loss: 0.00000806
Iteration 7/1000 | Loss: 0.00000780
Iteration 8/1000 | Loss: 0.00000775
Iteration 9/1000 | Loss: 0.00000765
Iteration 10/1000 | Loss: 0.00000765
Iteration 11/1000 | Loss: 0.00000765
Iteration 12/1000 | Loss: 0.00000756
Iteration 13/1000 | Loss: 0.00000754
Iteration 14/1000 | Loss: 0.00000754
Iteration 15/1000 | Loss: 0.00000752
Iteration 16/1000 | Loss: 0.00000751
Iteration 17/1000 | Loss: 0.00000750
Iteration 18/1000 | Loss: 0.00000749
Iteration 19/1000 | Loss: 0.00000748
Iteration 20/1000 | Loss: 0.00000748
Iteration 21/1000 | Loss: 0.00000748
Iteration 22/1000 | Loss: 0.00000747
Iteration 23/1000 | Loss: 0.00000747
Iteration 24/1000 | Loss: 0.00000747
Iteration 25/1000 | Loss: 0.00000746
Iteration 26/1000 | Loss: 0.00000746
Iteration 27/1000 | Loss: 0.00000745
Iteration 28/1000 | Loss: 0.00000745
Iteration 29/1000 | Loss: 0.00000744
Iteration 30/1000 | Loss: 0.00000744
Iteration 31/1000 | Loss: 0.00000743
Iteration 32/1000 | Loss: 0.00000743
Iteration 33/1000 | Loss: 0.00000743
Iteration 34/1000 | Loss: 0.00000743
Iteration 35/1000 | Loss: 0.00000743
Iteration 36/1000 | Loss: 0.00000743
Iteration 37/1000 | Loss: 0.00000743
Iteration 38/1000 | Loss: 0.00000743
Iteration 39/1000 | Loss: 0.00000742
Iteration 40/1000 | Loss: 0.00000741
Iteration 41/1000 | Loss: 0.00000741
Iteration 42/1000 | Loss: 0.00000741
Iteration 43/1000 | Loss: 0.00000741
Iteration 44/1000 | Loss: 0.00000740
Iteration 45/1000 | Loss: 0.00000740
Iteration 46/1000 | Loss: 0.00000740
Iteration 47/1000 | Loss: 0.00000740
Iteration 48/1000 | Loss: 0.00000740
Iteration 49/1000 | Loss: 0.00000739
Iteration 50/1000 | Loss: 0.00000739
Iteration 51/1000 | Loss: 0.00000739
Iteration 52/1000 | Loss: 0.00000736
Iteration 53/1000 | Loss: 0.00000736
Iteration 54/1000 | Loss: 0.00000736
Iteration 55/1000 | Loss: 0.00000735
Iteration 56/1000 | Loss: 0.00000735
Iteration 57/1000 | Loss: 0.00000735
Iteration 58/1000 | Loss: 0.00000735
Iteration 59/1000 | Loss: 0.00000734
Iteration 60/1000 | Loss: 0.00000734
Iteration 61/1000 | Loss: 0.00000734
Iteration 62/1000 | Loss: 0.00000734
Iteration 63/1000 | Loss: 0.00000731
Iteration 64/1000 | Loss: 0.00000730
Iteration 65/1000 | Loss: 0.00000730
Iteration 66/1000 | Loss: 0.00000730
Iteration 67/1000 | Loss: 0.00000729
Iteration 68/1000 | Loss: 0.00000729
Iteration 69/1000 | Loss: 0.00000729
Iteration 70/1000 | Loss: 0.00000729
Iteration 71/1000 | Loss: 0.00000729
Iteration 72/1000 | Loss: 0.00000729
Iteration 73/1000 | Loss: 0.00000729
Iteration 74/1000 | Loss: 0.00000728
Iteration 75/1000 | Loss: 0.00000728
Iteration 76/1000 | Loss: 0.00000727
Iteration 77/1000 | Loss: 0.00000727
Iteration 78/1000 | Loss: 0.00000727
Iteration 79/1000 | Loss: 0.00000726
Iteration 80/1000 | Loss: 0.00000726
Iteration 81/1000 | Loss: 0.00000726
Iteration 82/1000 | Loss: 0.00000726
Iteration 83/1000 | Loss: 0.00000726
Iteration 84/1000 | Loss: 0.00000726
Iteration 85/1000 | Loss: 0.00000726
Iteration 86/1000 | Loss: 0.00000726
Iteration 87/1000 | Loss: 0.00000726
Iteration 88/1000 | Loss: 0.00000726
Iteration 89/1000 | Loss: 0.00000725
Iteration 90/1000 | Loss: 0.00000725
Iteration 91/1000 | Loss: 0.00000725
Iteration 92/1000 | Loss: 0.00000725
Iteration 93/1000 | Loss: 0.00000725
Iteration 94/1000 | Loss: 0.00000725
Iteration 95/1000 | Loss: 0.00000725
Iteration 96/1000 | Loss: 0.00000725
Iteration 97/1000 | Loss: 0.00000724
Iteration 98/1000 | Loss: 0.00000724
Iteration 99/1000 | Loss: 0.00000724
Iteration 100/1000 | Loss: 0.00000724
Iteration 101/1000 | Loss: 0.00000723
Iteration 102/1000 | Loss: 0.00000723
Iteration 103/1000 | Loss: 0.00000723
Iteration 104/1000 | Loss: 0.00000723
Iteration 105/1000 | Loss: 0.00000723
Iteration 106/1000 | Loss: 0.00000723
Iteration 107/1000 | Loss: 0.00000723
Iteration 108/1000 | Loss: 0.00000723
Iteration 109/1000 | Loss: 0.00000723
Iteration 110/1000 | Loss: 0.00000723
Iteration 111/1000 | Loss: 0.00000723
Iteration 112/1000 | Loss: 0.00000723
Iteration 113/1000 | Loss: 0.00000723
Iteration 114/1000 | Loss: 0.00000723
Iteration 115/1000 | Loss: 0.00000723
Iteration 116/1000 | Loss: 0.00000723
Iteration 117/1000 | Loss: 0.00000723
Iteration 118/1000 | Loss: 0.00000723
Iteration 119/1000 | Loss: 0.00000723
Iteration 120/1000 | Loss: 0.00000723
Iteration 121/1000 | Loss: 0.00000723
Iteration 122/1000 | Loss: 0.00000723
Iteration 123/1000 | Loss: 0.00000723
Iteration 124/1000 | Loss: 0.00000723
Iteration 125/1000 | Loss: 0.00000723
Iteration 126/1000 | Loss: 0.00000723
Iteration 127/1000 | Loss: 0.00000723
Iteration 128/1000 | Loss: 0.00000723
Iteration 129/1000 | Loss: 0.00000723
Iteration 130/1000 | Loss: 0.00000723
Iteration 131/1000 | Loss: 0.00000723
Iteration 132/1000 | Loss: 0.00000723
Iteration 133/1000 | Loss: 0.00000723
Iteration 134/1000 | Loss: 0.00000723
Iteration 135/1000 | Loss: 0.00000723
Iteration 136/1000 | Loss: 0.00000723
Iteration 137/1000 | Loss: 0.00000723
Iteration 138/1000 | Loss: 0.00000723
Iteration 139/1000 | Loss: 0.00000723
Iteration 140/1000 | Loss: 0.00000723
Iteration 141/1000 | Loss: 0.00000723
Iteration 142/1000 | Loss: 0.00000723
Iteration 143/1000 | Loss: 0.00000723
Iteration 144/1000 | Loss: 0.00000723
Iteration 145/1000 | Loss: 0.00000723
Iteration 146/1000 | Loss: 0.00000723
Iteration 147/1000 | Loss: 0.00000723
Iteration 148/1000 | Loss: 0.00000723
Iteration 149/1000 | Loss: 0.00000723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [7.225180979730794e-06, 7.225180979730794e-06, 7.225180979730794e-06, 7.225180979730794e-06, 7.225180979730794e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.225180979730794e-06

Optimization complete. Final v2v error: 2.270146369934082 mm

Highest mean error: 2.770068883895874 mm for frame 139

Lowest mean error: 2.0540943145751953 mm for frame 156

Saving results

Total time: 36.19629120826721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046238
Iteration 2/25 | Loss: 0.00261421
Iteration 3/25 | Loss: 0.00192169
Iteration 4/25 | Loss: 0.00172207
Iteration 5/25 | Loss: 0.00161464
Iteration 6/25 | Loss: 0.00149549
Iteration 7/25 | Loss: 0.00142216
Iteration 8/25 | Loss: 0.00134696
Iteration 9/25 | Loss: 0.00129435
Iteration 10/25 | Loss: 0.00126750
Iteration 11/25 | Loss: 0.00128916
Iteration 12/25 | Loss: 0.00124149
Iteration 13/25 | Loss: 0.00123768
Iteration 14/25 | Loss: 0.00123863
Iteration 15/25 | Loss: 0.00122833
Iteration 16/25 | Loss: 0.00121947
Iteration 17/25 | Loss: 0.00121812
Iteration 18/25 | Loss: 0.00121302
Iteration 19/25 | Loss: 0.00120770
Iteration 20/25 | Loss: 0.00120587
Iteration 21/25 | Loss: 0.00120530
Iteration 22/25 | Loss: 0.00120516
Iteration 23/25 | Loss: 0.00120506
Iteration 24/25 | Loss: 0.00120504
Iteration 25/25 | Loss: 0.00120504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32138526
Iteration 2/25 | Loss: 0.00463102
Iteration 3/25 | Loss: 0.00349697
Iteration 4/25 | Loss: 0.00349690
Iteration 5/25 | Loss: 0.00349690
Iteration 6/25 | Loss: 0.00349690
Iteration 7/25 | Loss: 0.00349690
Iteration 8/25 | Loss: 0.00349690
Iteration 9/25 | Loss: 0.00349690
Iteration 10/25 | Loss: 0.00349689
Iteration 11/25 | Loss: 0.00349689
Iteration 12/25 | Loss: 0.00349689
Iteration 13/25 | Loss: 0.00349689
Iteration 14/25 | Loss: 0.00349689
Iteration 15/25 | Loss: 0.00349689
Iteration 16/25 | Loss: 0.00349689
Iteration 17/25 | Loss: 0.00349689
Iteration 18/25 | Loss: 0.00349689
Iteration 19/25 | Loss: 0.00349689
Iteration 20/25 | Loss: 0.00349689
Iteration 21/25 | Loss: 0.00349689
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.003496894147247076, 0.003496894147247076, 0.003496894147247076, 0.003496894147247076, 0.003496894147247076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003496894147247076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00349689
Iteration 2/1000 | Loss: 0.00083894
Iteration 3/1000 | Loss: 0.00108433
Iteration 4/1000 | Loss: 0.00106792
Iteration 5/1000 | Loss: 0.00029374
Iteration 6/1000 | Loss: 0.00019787
Iteration 7/1000 | Loss: 0.00116352
Iteration 8/1000 | Loss: 0.00023060
Iteration 9/1000 | Loss: 0.00015561
Iteration 10/1000 | Loss: 0.00020869
Iteration 11/1000 | Loss: 0.00037540
Iteration 12/1000 | Loss: 0.00073152
Iteration 13/1000 | Loss: 0.00025202
Iteration 14/1000 | Loss: 0.00016778
Iteration 15/1000 | Loss: 0.00014869
Iteration 16/1000 | Loss: 0.00033299
Iteration 17/1000 | Loss: 0.00014369
Iteration 18/1000 | Loss: 0.00014118
Iteration 19/1000 | Loss: 0.00013983
Iteration 20/1000 | Loss: 0.00048592
Iteration 21/1000 | Loss: 0.00013732
Iteration 22/1000 | Loss: 0.00013468
Iteration 23/1000 | Loss: 0.00044754
Iteration 24/1000 | Loss: 0.00130703
Iteration 25/1000 | Loss: 0.00125180
Iteration 26/1000 | Loss: 0.00079758
Iteration 27/1000 | Loss: 0.00080034
Iteration 28/1000 | Loss: 0.00094677
Iteration 29/1000 | Loss: 0.00033157
Iteration 30/1000 | Loss: 0.00092531
Iteration 31/1000 | Loss: 0.00043086
Iteration 32/1000 | Loss: 0.00069657
Iteration 33/1000 | Loss: 0.00019130
Iteration 34/1000 | Loss: 0.00046360
Iteration 35/1000 | Loss: 0.00012865
Iteration 36/1000 | Loss: 0.00012431
Iteration 37/1000 | Loss: 0.00012224
Iteration 38/1000 | Loss: 0.00040053
Iteration 39/1000 | Loss: 0.00084160
Iteration 40/1000 | Loss: 0.00031598
Iteration 41/1000 | Loss: 0.00084119
Iteration 42/1000 | Loss: 0.00057155
Iteration 43/1000 | Loss: 0.00019458
Iteration 44/1000 | Loss: 0.00011864
Iteration 45/1000 | Loss: 0.00044272
Iteration 46/1000 | Loss: 0.00191547
Iteration 47/1000 | Loss: 0.00127129
Iteration 48/1000 | Loss: 0.00065598
Iteration 49/1000 | Loss: 0.00011720
Iteration 50/1000 | Loss: 0.00011107
Iteration 51/1000 | Loss: 0.00010822
Iteration 52/1000 | Loss: 0.00010666
Iteration 53/1000 | Loss: 0.00076081
Iteration 54/1000 | Loss: 0.00017372
Iteration 55/1000 | Loss: 0.00011079
Iteration 56/1000 | Loss: 0.00023956
Iteration 57/1000 | Loss: 0.00010453
Iteration 58/1000 | Loss: 0.00010248
Iteration 59/1000 | Loss: 0.00010071
Iteration 60/1000 | Loss: 0.00009994
Iteration 61/1000 | Loss: 0.00023894
Iteration 62/1000 | Loss: 0.00032613
Iteration 63/1000 | Loss: 0.00033993
Iteration 64/1000 | Loss: 0.00011495
Iteration 65/1000 | Loss: 0.00010650
Iteration 66/1000 | Loss: 0.00026231
Iteration 67/1000 | Loss: 0.00047280
Iteration 68/1000 | Loss: 0.00083645
Iteration 69/1000 | Loss: 0.00010721
Iteration 70/1000 | Loss: 0.00010033
Iteration 71/1000 | Loss: 0.00024621
Iteration 72/1000 | Loss: 0.00036648
Iteration 73/1000 | Loss: 0.00013558
Iteration 74/1000 | Loss: 0.00029905
Iteration 75/1000 | Loss: 0.00009778
Iteration 76/1000 | Loss: 0.00009146
Iteration 77/1000 | Loss: 0.00008816
Iteration 78/1000 | Loss: 0.00008598
Iteration 79/1000 | Loss: 0.00008470
Iteration 80/1000 | Loss: 0.00008357
Iteration 81/1000 | Loss: 0.00008255
Iteration 82/1000 | Loss: 0.00008167
Iteration 83/1000 | Loss: 0.00008104
Iteration 84/1000 | Loss: 0.00008044
Iteration 85/1000 | Loss: 0.00008000
Iteration 86/1000 | Loss: 0.00007929
Iteration 87/1000 | Loss: 0.00007861
Iteration 88/1000 | Loss: 0.00007805
Iteration 89/1000 | Loss: 0.00007762
Iteration 90/1000 | Loss: 0.00007719
Iteration 91/1000 | Loss: 0.00007667
Iteration 92/1000 | Loss: 0.00007607
Iteration 93/1000 | Loss: 0.00007560
Iteration 94/1000 | Loss: 0.00007518
Iteration 95/1000 | Loss: 0.00007466
Iteration 96/1000 | Loss: 0.00007417
Iteration 97/1000 | Loss: 0.00007371
Iteration 98/1000 | Loss: 0.00019730
Iteration 99/1000 | Loss: 0.00024177
Iteration 100/1000 | Loss: 0.00049784
Iteration 101/1000 | Loss: 0.00022222
Iteration 102/1000 | Loss: 0.00007727
Iteration 103/1000 | Loss: 0.00025149
Iteration 104/1000 | Loss: 0.00024874
Iteration 105/1000 | Loss: 0.00012086
Iteration 106/1000 | Loss: 0.00007389
Iteration 107/1000 | Loss: 0.00007165
Iteration 108/1000 | Loss: 0.00007025
Iteration 109/1000 | Loss: 0.00006949
Iteration 110/1000 | Loss: 0.00006915
Iteration 111/1000 | Loss: 0.00006885
Iteration 112/1000 | Loss: 0.00006865
Iteration 113/1000 | Loss: 0.00006844
Iteration 114/1000 | Loss: 0.00044437
Iteration 115/1000 | Loss: 0.00006957
Iteration 116/1000 | Loss: 0.00006842
Iteration 117/1000 | Loss: 0.00006818
Iteration 118/1000 | Loss: 0.00006816
Iteration 119/1000 | Loss: 0.00006816
Iteration 120/1000 | Loss: 0.00006816
Iteration 121/1000 | Loss: 0.00006816
Iteration 122/1000 | Loss: 0.00006816
Iteration 123/1000 | Loss: 0.00006816
Iteration 124/1000 | Loss: 0.00006816
Iteration 125/1000 | Loss: 0.00006815
Iteration 126/1000 | Loss: 0.00006815
Iteration 127/1000 | Loss: 0.00006815
Iteration 128/1000 | Loss: 0.00006815
Iteration 129/1000 | Loss: 0.00006815
Iteration 130/1000 | Loss: 0.00006815
Iteration 131/1000 | Loss: 0.00006815
Iteration 132/1000 | Loss: 0.00006815
Iteration 133/1000 | Loss: 0.00006812
Iteration 134/1000 | Loss: 0.00006812
Iteration 135/1000 | Loss: 0.00006811
Iteration 136/1000 | Loss: 0.00006811
Iteration 137/1000 | Loss: 0.00006808
Iteration 138/1000 | Loss: 0.00006808
Iteration 139/1000 | Loss: 0.00006806
Iteration 140/1000 | Loss: 0.00006806
Iteration 141/1000 | Loss: 0.00006805
Iteration 142/1000 | Loss: 0.00006805
Iteration 143/1000 | Loss: 0.00006804
Iteration 144/1000 | Loss: 0.00006803
Iteration 145/1000 | Loss: 0.00006803
Iteration 146/1000 | Loss: 0.00006803
Iteration 147/1000 | Loss: 0.00006802
Iteration 148/1000 | Loss: 0.00006801
Iteration 149/1000 | Loss: 0.00006800
Iteration 150/1000 | Loss: 0.00006800
Iteration 151/1000 | Loss: 0.00006800
Iteration 152/1000 | Loss: 0.00006800
Iteration 153/1000 | Loss: 0.00006800
Iteration 154/1000 | Loss: 0.00006800
Iteration 155/1000 | Loss: 0.00006800
Iteration 156/1000 | Loss: 0.00006800
Iteration 157/1000 | Loss: 0.00006800
Iteration 158/1000 | Loss: 0.00006800
Iteration 159/1000 | Loss: 0.00006800
Iteration 160/1000 | Loss: 0.00006800
Iteration 161/1000 | Loss: 0.00006799
Iteration 162/1000 | Loss: 0.00006799
Iteration 163/1000 | Loss: 0.00006799
Iteration 164/1000 | Loss: 0.00006799
Iteration 165/1000 | Loss: 0.00006799
Iteration 166/1000 | Loss: 0.00006799
Iteration 167/1000 | Loss: 0.00006799
Iteration 168/1000 | Loss: 0.00006798
Iteration 169/1000 | Loss: 0.00006797
Iteration 170/1000 | Loss: 0.00006797
Iteration 171/1000 | Loss: 0.00006797
Iteration 172/1000 | Loss: 0.00006797
Iteration 173/1000 | Loss: 0.00006796
Iteration 174/1000 | Loss: 0.00006796
Iteration 175/1000 | Loss: 0.00006796
Iteration 176/1000 | Loss: 0.00006796
Iteration 177/1000 | Loss: 0.00006796
Iteration 178/1000 | Loss: 0.00006796
Iteration 179/1000 | Loss: 0.00006796
Iteration 180/1000 | Loss: 0.00006796
Iteration 181/1000 | Loss: 0.00006796
Iteration 182/1000 | Loss: 0.00006796
Iteration 183/1000 | Loss: 0.00006796
Iteration 184/1000 | Loss: 0.00006795
Iteration 185/1000 | Loss: 0.00006795
Iteration 186/1000 | Loss: 0.00006795
Iteration 187/1000 | Loss: 0.00006795
Iteration 188/1000 | Loss: 0.00006795
Iteration 189/1000 | Loss: 0.00006795
Iteration 190/1000 | Loss: 0.00006794
Iteration 191/1000 | Loss: 0.00006794
Iteration 192/1000 | Loss: 0.00006794
Iteration 193/1000 | Loss: 0.00006794
Iteration 194/1000 | Loss: 0.00006794
Iteration 195/1000 | Loss: 0.00006794
Iteration 196/1000 | Loss: 0.00006794
Iteration 197/1000 | Loss: 0.00006794
Iteration 198/1000 | Loss: 0.00006794
Iteration 199/1000 | Loss: 0.00006794
Iteration 200/1000 | Loss: 0.00006794
Iteration 201/1000 | Loss: 0.00006794
Iteration 202/1000 | Loss: 0.00006794
Iteration 203/1000 | Loss: 0.00006794
Iteration 204/1000 | Loss: 0.00006794
Iteration 205/1000 | Loss: 0.00006794
Iteration 206/1000 | Loss: 0.00006794
Iteration 207/1000 | Loss: 0.00006794
Iteration 208/1000 | Loss: 0.00006794
Iteration 209/1000 | Loss: 0.00006794
Iteration 210/1000 | Loss: 0.00006794
Iteration 211/1000 | Loss: 0.00006794
Iteration 212/1000 | Loss: 0.00006794
Iteration 213/1000 | Loss: 0.00006794
Iteration 214/1000 | Loss: 0.00006794
Iteration 215/1000 | Loss: 0.00006794
Iteration 216/1000 | Loss: 0.00006794
Iteration 217/1000 | Loss: 0.00006794
Iteration 218/1000 | Loss: 0.00006794
Iteration 219/1000 | Loss: 0.00006794
Iteration 220/1000 | Loss: 0.00006794
Iteration 221/1000 | Loss: 0.00006794
Iteration 222/1000 | Loss: 0.00006794
Iteration 223/1000 | Loss: 0.00006794
Iteration 224/1000 | Loss: 0.00006794
Iteration 225/1000 | Loss: 0.00006794
Iteration 226/1000 | Loss: 0.00006794
Iteration 227/1000 | Loss: 0.00006794
Iteration 228/1000 | Loss: 0.00006794
Iteration 229/1000 | Loss: 0.00006794
Iteration 230/1000 | Loss: 0.00006794
Iteration 231/1000 | Loss: 0.00006794
Iteration 232/1000 | Loss: 0.00006794
Iteration 233/1000 | Loss: 0.00006794
Iteration 234/1000 | Loss: 0.00006794
Iteration 235/1000 | Loss: 0.00006794
Iteration 236/1000 | Loss: 0.00006794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [6.794076034566388e-05, 6.794076034566388e-05, 6.794076034566388e-05, 6.794076034566388e-05, 6.794076034566388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.794076034566388e-05

Optimization complete. Final v2v error: 4.258245468139648 mm

Highest mean error: 21.28554916381836 mm for frame 101

Lowest mean error: 2.769874334335327 mm for frame 8

Saving results

Total time: 215.46668362617493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495069
Iteration 2/25 | Loss: 0.00124312
Iteration 3/25 | Loss: 0.00100660
Iteration 4/25 | Loss: 0.00099074
Iteration 5/25 | Loss: 0.00098877
Iteration 6/25 | Loss: 0.00098819
Iteration 7/25 | Loss: 0.00098819
Iteration 8/25 | Loss: 0.00098819
Iteration 9/25 | Loss: 0.00098819
Iteration 10/25 | Loss: 0.00098819
Iteration 11/25 | Loss: 0.00098819
Iteration 12/25 | Loss: 0.00098819
Iteration 13/25 | Loss: 0.00098819
Iteration 14/25 | Loss: 0.00098819
Iteration 15/25 | Loss: 0.00098819
Iteration 16/25 | Loss: 0.00098819
Iteration 17/25 | Loss: 0.00098819
Iteration 18/25 | Loss: 0.00098819
Iteration 19/25 | Loss: 0.00098819
Iteration 20/25 | Loss: 0.00098819
Iteration 21/25 | Loss: 0.00098819
Iteration 22/25 | Loss: 0.00098819
Iteration 23/25 | Loss: 0.00098819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009881900623440742, 0.0009881900623440742, 0.0009881900623440742, 0.0009881900623440742, 0.0009881900623440742]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009881900623440742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22422469
Iteration 2/25 | Loss: 0.00167367
Iteration 3/25 | Loss: 0.00167367
Iteration 4/25 | Loss: 0.00167367
Iteration 5/25 | Loss: 0.00167367
Iteration 6/25 | Loss: 0.00167367
Iteration 7/25 | Loss: 0.00167367
Iteration 8/25 | Loss: 0.00167367
Iteration 9/25 | Loss: 0.00167367
Iteration 10/25 | Loss: 0.00167367
Iteration 11/25 | Loss: 0.00167367
Iteration 12/25 | Loss: 0.00167367
Iteration 13/25 | Loss: 0.00167367
Iteration 14/25 | Loss: 0.00167367
Iteration 15/25 | Loss: 0.00167367
Iteration 16/25 | Loss: 0.00167367
Iteration 17/25 | Loss: 0.00167367
Iteration 18/25 | Loss: 0.00167367
Iteration 19/25 | Loss: 0.00167367
Iteration 20/25 | Loss: 0.00167367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016736708348616958, 0.0016736708348616958, 0.0016736708348616958, 0.0016736708348616958, 0.0016736708348616958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016736708348616958

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167367
Iteration 2/1000 | Loss: 0.00003326
Iteration 3/1000 | Loss: 0.00002016
Iteration 4/1000 | Loss: 0.00001693
Iteration 5/1000 | Loss: 0.00001578
Iteration 6/1000 | Loss: 0.00001518
Iteration 7/1000 | Loss: 0.00001472
Iteration 8/1000 | Loss: 0.00001441
Iteration 9/1000 | Loss: 0.00001408
Iteration 10/1000 | Loss: 0.00001388
Iteration 11/1000 | Loss: 0.00001386
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001372
Iteration 14/1000 | Loss: 0.00001365
Iteration 15/1000 | Loss: 0.00001364
Iteration 16/1000 | Loss: 0.00001361
Iteration 17/1000 | Loss: 0.00001361
Iteration 18/1000 | Loss: 0.00001360
Iteration 19/1000 | Loss: 0.00001358
Iteration 20/1000 | Loss: 0.00001358
Iteration 21/1000 | Loss: 0.00001357
Iteration 22/1000 | Loss: 0.00001354
Iteration 23/1000 | Loss: 0.00001353
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001351
Iteration 27/1000 | Loss: 0.00001344
Iteration 28/1000 | Loss: 0.00001341
Iteration 29/1000 | Loss: 0.00001341
Iteration 30/1000 | Loss: 0.00001340
Iteration 31/1000 | Loss: 0.00001339
Iteration 32/1000 | Loss: 0.00001335
Iteration 33/1000 | Loss: 0.00001335
Iteration 34/1000 | Loss: 0.00001335
Iteration 35/1000 | Loss: 0.00001334
Iteration 36/1000 | Loss: 0.00001334
Iteration 37/1000 | Loss: 0.00001334
Iteration 38/1000 | Loss: 0.00001332
Iteration 39/1000 | Loss: 0.00001332
Iteration 40/1000 | Loss: 0.00001332
Iteration 41/1000 | Loss: 0.00001332
Iteration 42/1000 | Loss: 0.00001332
Iteration 43/1000 | Loss: 0.00001332
Iteration 44/1000 | Loss: 0.00001332
Iteration 45/1000 | Loss: 0.00001331
Iteration 46/1000 | Loss: 0.00001331
Iteration 47/1000 | Loss: 0.00001331
Iteration 48/1000 | Loss: 0.00001331
Iteration 49/1000 | Loss: 0.00001331
Iteration 50/1000 | Loss: 0.00001331
Iteration 51/1000 | Loss: 0.00001331
Iteration 52/1000 | Loss: 0.00001331
Iteration 53/1000 | Loss: 0.00001330
Iteration 54/1000 | Loss: 0.00001329
Iteration 55/1000 | Loss: 0.00001328
Iteration 56/1000 | Loss: 0.00001328
Iteration 57/1000 | Loss: 0.00001327
Iteration 58/1000 | Loss: 0.00001327
Iteration 59/1000 | Loss: 0.00001327
Iteration 60/1000 | Loss: 0.00001327
Iteration 61/1000 | Loss: 0.00001327
Iteration 62/1000 | Loss: 0.00001327
Iteration 63/1000 | Loss: 0.00001327
Iteration 64/1000 | Loss: 0.00001326
Iteration 65/1000 | Loss: 0.00001326
Iteration 66/1000 | Loss: 0.00001326
Iteration 67/1000 | Loss: 0.00001326
Iteration 68/1000 | Loss: 0.00001326
Iteration 69/1000 | Loss: 0.00001326
Iteration 70/1000 | Loss: 0.00001326
Iteration 71/1000 | Loss: 0.00001326
Iteration 72/1000 | Loss: 0.00001325
Iteration 73/1000 | Loss: 0.00001324
Iteration 74/1000 | Loss: 0.00001324
Iteration 75/1000 | Loss: 0.00001323
Iteration 76/1000 | Loss: 0.00001323
Iteration 77/1000 | Loss: 0.00001323
Iteration 78/1000 | Loss: 0.00001323
Iteration 79/1000 | Loss: 0.00001323
Iteration 80/1000 | Loss: 0.00001323
Iteration 81/1000 | Loss: 0.00001323
Iteration 82/1000 | Loss: 0.00001323
Iteration 83/1000 | Loss: 0.00001323
Iteration 84/1000 | Loss: 0.00001322
Iteration 85/1000 | Loss: 0.00001322
Iteration 86/1000 | Loss: 0.00001322
Iteration 87/1000 | Loss: 0.00001322
Iteration 88/1000 | Loss: 0.00001321
Iteration 89/1000 | Loss: 0.00001321
Iteration 90/1000 | Loss: 0.00001321
Iteration 91/1000 | Loss: 0.00001321
Iteration 92/1000 | Loss: 0.00001321
Iteration 93/1000 | Loss: 0.00001320
Iteration 94/1000 | Loss: 0.00001320
Iteration 95/1000 | Loss: 0.00001320
Iteration 96/1000 | Loss: 0.00001320
Iteration 97/1000 | Loss: 0.00001320
Iteration 98/1000 | Loss: 0.00001320
Iteration 99/1000 | Loss: 0.00001320
Iteration 100/1000 | Loss: 0.00001320
Iteration 101/1000 | Loss: 0.00001320
Iteration 102/1000 | Loss: 0.00001320
Iteration 103/1000 | Loss: 0.00001320
Iteration 104/1000 | Loss: 0.00001320
Iteration 105/1000 | Loss: 0.00001320
Iteration 106/1000 | Loss: 0.00001320
Iteration 107/1000 | Loss: 0.00001320
Iteration 108/1000 | Loss: 0.00001320
Iteration 109/1000 | Loss: 0.00001319
Iteration 110/1000 | Loss: 0.00001319
Iteration 111/1000 | Loss: 0.00001319
Iteration 112/1000 | Loss: 0.00001319
Iteration 113/1000 | Loss: 0.00001319
Iteration 114/1000 | Loss: 0.00001319
Iteration 115/1000 | Loss: 0.00001319
Iteration 116/1000 | Loss: 0.00001319
Iteration 117/1000 | Loss: 0.00001319
Iteration 118/1000 | Loss: 0.00001319
Iteration 119/1000 | Loss: 0.00001319
Iteration 120/1000 | Loss: 0.00001319
Iteration 121/1000 | Loss: 0.00001319
Iteration 122/1000 | Loss: 0.00001319
Iteration 123/1000 | Loss: 0.00001319
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001319
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001319
Iteration 128/1000 | Loss: 0.00001319
Iteration 129/1000 | Loss: 0.00001319
Iteration 130/1000 | Loss: 0.00001319
Iteration 131/1000 | Loss: 0.00001319
Iteration 132/1000 | Loss: 0.00001319
Iteration 133/1000 | Loss: 0.00001319
Iteration 134/1000 | Loss: 0.00001319
Iteration 135/1000 | Loss: 0.00001319
Iteration 136/1000 | Loss: 0.00001319
Iteration 137/1000 | Loss: 0.00001319
Iteration 138/1000 | Loss: 0.00001319
Iteration 139/1000 | Loss: 0.00001319
Iteration 140/1000 | Loss: 0.00001319
Iteration 141/1000 | Loss: 0.00001319
Iteration 142/1000 | Loss: 0.00001319
Iteration 143/1000 | Loss: 0.00001319
Iteration 144/1000 | Loss: 0.00001319
Iteration 145/1000 | Loss: 0.00001319
Iteration 146/1000 | Loss: 0.00001319
Iteration 147/1000 | Loss: 0.00001319
Iteration 148/1000 | Loss: 0.00001319
Iteration 149/1000 | Loss: 0.00001319
Iteration 150/1000 | Loss: 0.00001319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.319267539656721e-05, 1.319267539656721e-05, 1.319267539656721e-05, 1.319267539656721e-05, 1.319267539656721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.319267539656721e-05

Optimization complete. Final v2v error: 2.655517578125 mm

Highest mean error: 4.305469036102295 mm for frame 61

Lowest mean error: 2.0816221237182617 mm for frame 101

Saving results

Total time: 36.570812940597534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824581
Iteration 2/25 | Loss: 0.00114437
Iteration 3/25 | Loss: 0.00098468
Iteration 4/25 | Loss: 0.00096618
Iteration 5/25 | Loss: 0.00096254
Iteration 6/25 | Loss: 0.00096195
Iteration 7/25 | Loss: 0.00096195
Iteration 8/25 | Loss: 0.00096195
Iteration 9/25 | Loss: 0.00096195
Iteration 10/25 | Loss: 0.00096195
Iteration 11/25 | Loss: 0.00096195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009619512129575014, 0.0009619512129575014, 0.0009619512129575014, 0.0009619512129575014, 0.0009619512129575014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009619512129575014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20954239
Iteration 2/25 | Loss: 0.00202733
Iteration 3/25 | Loss: 0.00202733
Iteration 4/25 | Loss: 0.00202733
Iteration 5/25 | Loss: 0.00202733
Iteration 6/25 | Loss: 0.00202732
Iteration 7/25 | Loss: 0.00202732
Iteration 8/25 | Loss: 0.00202732
Iteration 9/25 | Loss: 0.00202732
Iteration 10/25 | Loss: 0.00202732
Iteration 11/25 | Loss: 0.00202732
Iteration 12/25 | Loss: 0.00202732
Iteration 13/25 | Loss: 0.00202732
Iteration 14/25 | Loss: 0.00202732
Iteration 15/25 | Loss: 0.00202732
Iteration 16/25 | Loss: 0.00202732
Iteration 17/25 | Loss: 0.00202732
Iteration 18/25 | Loss: 0.00202732
Iteration 19/25 | Loss: 0.00202732
Iteration 20/25 | Loss: 0.00202732
Iteration 21/25 | Loss: 0.00202732
Iteration 22/25 | Loss: 0.00202732
Iteration 23/25 | Loss: 0.00202732
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0020273225381970406, 0.0020273225381970406, 0.0020273225381970406, 0.0020273225381970406, 0.0020273225381970406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020273225381970406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202732
Iteration 2/1000 | Loss: 0.00002146
Iteration 3/1000 | Loss: 0.00001089
Iteration 4/1000 | Loss: 0.00000971
Iteration 5/1000 | Loss: 0.00000895
Iteration 6/1000 | Loss: 0.00000848
Iteration 7/1000 | Loss: 0.00000804
Iteration 8/1000 | Loss: 0.00000772
Iteration 9/1000 | Loss: 0.00000742
Iteration 10/1000 | Loss: 0.00000724
Iteration 11/1000 | Loss: 0.00000716
Iteration 12/1000 | Loss: 0.00000715
Iteration 13/1000 | Loss: 0.00000714
Iteration 14/1000 | Loss: 0.00000713
Iteration 15/1000 | Loss: 0.00000713
Iteration 16/1000 | Loss: 0.00000712
Iteration 17/1000 | Loss: 0.00000709
Iteration 18/1000 | Loss: 0.00000708
Iteration 19/1000 | Loss: 0.00000706
Iteration 20/1000 | Loss: 0.00000705
Iteration 21/1000 | Loss: 0.00000702
Iteration 22/1000 | Loss: 0.00000701
Iteration 23/1000 | Loss: 0.00000701
Iteration 24/1000 | Loss: 0.00000701
Iteration 25/1000 | Loss: 0.00000701
Iteration 26/1000 | Loss: 0.00000700
Iteration 27/1000 | Loss: 0.00000699
Iteration 28/1000 | Loss: 0.00000699
Iteration 29/1000 | Loss: 0.00000698
Iteration 30/1000 | Loss: 0.00000697
Iteration 31/1000 | Loss: 0.00000696
Iteration 32/1000 | Loss: 0.00000696
Iteration 33/1000 | Loss: 0.00000696
Iteration 34/1000 | Loss: 0.00000696
Iteration 35/1000 | Loss: 0.00000695
Iteration 36/1000 | Loss: 0.00000695
Iteration 37/1000 | Loss: 0.00000695
Iteration 38/1000 | Loss: 0.00000694
Iteration 39/1000 | Loss: 0.00000694
Iteration 40/1000 | Loss: 0.00000692
Iteration 41/1000 | Loss: 0.00000692
Iteration 42/1000 | Loss: 0.00000692
Iteration 43/1000 | Loss: 0.00000690
Iteration 44/1000 | Loss: 0.00000690
Iteration 45/1000 | Loss: 0.00000689
Iteration 46/1000 | Loss: 0.00000688
Iteration 47/1000 | Loss: 0.00000688
Iteration 48/1000 | Loss: 0.00000688
Iteration 49/1000 | Loss: 0.00000687
Iteration 50/1000 | Loss: 0.00000686
Iteration 51/1000 | Loss: 0.00000686
Iteration 52/1000 | Loss: 0.00000686
Iteration 53/1000 | Loss: 0.00000686
Iteration 54/1000 | Loss: 0.00000685
Iteration 55/1000 | Loss: 0.00000685
Iteration 56/1000 | Loss: 0.00000685
Iteration 57/1000 | Loss: 0.00000684
Iteration 58/1000 | Loss: 0.00000684
Iteration 59/1000 | Loss: 0.00000684
Iteration 60/1000 | Loss: 0.00000684
Iteration 61/1000 | Loss: 0.00000683
Iteration 62/1000 | Loss: 0.00000683
Iteration 63/1000 | Loss: 0.00000682
Iteration 64/1000 | Loss: 0.00000682
Iteration 65/1000 | Loss: 0.00000682
Iteration 66/1000 | Loss: 0.00000682
Iteration 67/1000 | Loss: 0.00000681
Iteration 68/1000 | Loss: 0.00000681
Iteration 69/1000 | Loss: 0.00000680
Iteration 70/1000 | Loss: 0.00000680
Iteration 71/1000 | Loss: 0.00000680
Iteration 72/1000 | Loss: 0.00000680
Iteration 73/1000 | Loss: 0.00000680
Iteration 74/1000 | Loss: 0.00000680
Iteration 75/1000 | Loss: 0.00000679
Iteration 76/1000 | Loss: 0.00000679
Iteration 77/1000 | Loss: 0.00000679
Iteration 78/1000 | Loss: 0.00000679
Iteration 79/1000 | Loss: 0.00000679
Iteration 80/1000 | Loss: 0.00000678
Iteration 81/1000 | Loss: 0.00000678
Iteration 82/1000 | Loss: 0.00000678
Iteration 83/1000 | Loss: 0.00000677
Iteration 84/1000 | Loss: 0.00000677
Iteration 85/1000 | Loss: 0.00000677
Iteration 86/1000 | Loss: 0.00000677
Iteration 87/1000 | Loss: 0.00000677
Iteration 88/1000 | Loss: 0.00000677
Iteration 89/1000 | Loss: 0.00000677
Iteration 90/1000 | Loss: 0.00000676
Iteration 91/1000 | Loss: 0.00000676
Iteration 92/1000 | Loss: 0.00000676
Iteration 93/1000 | Loss: 0.00000675
Iteration 94/1000 | Loss: 0.00000675
Iteration 95/1000 | Loss: 0.00000675
Iteration 96/1000 | Loss: 0.00000675
Iteration 97/1000 | Loss: 0.00000675
Iteration 98/1000 | Loss: 0.00000675
Iteration 99/1000 | Loss: 0.00000674
Iteration 100/1000 | Loss: 0.00000674
Iteration 101/1000 | Loss: 0.00000674
Iteration 102/1000 | Loss: 0.00000674
Iteration 103/1000 | Loss: 0.00000674
Iteration 104/1000 | Loss: 0.00000674
Iteration 105/1000 | Loss: 0.00000673
Iteration 106/1000 | Loss: 0.00000673
Iteration 107/1000 | Loss: 0.00000673
Iteration 108/1000 | Loss: 0.00000673
Iteration 109/1000 | Loss: 0.00000672
Iteration 110/1000 | Loss: 0.00000672
Iteration 111/1000 | Loss: 0.00000672
Iteration 112/1000 | Loss: 0.00000672
Iteration 113/1000 | Loss: 0.00000672
Iteration 114/1000 | Loss: 0.00000672
Iteration 115/1000 | Loss: 0.00000672
Iteration 116/1000 | Loss: 0.00000672
Iteration 117/1000 | Loss: 0.00000672
Iteration 118/1000 | Loss: 0.00000672
Iteration 119/1000 | Loss: 0.00000672
Iteration 120/1000 | Loss: 0.00000672
Iteration 121/1000 | Loss: 0.00000672
Iteration 122/1000 | Loss: 0.00000672
Iteration 123/1000 | Loss: 0.00000672
Iteration 124/1000 | Loss: 0.00000671
Iteration 125/1000 | Loss: 0.00000671
Iteration 126/1000 | Loss: 0.00000671
Iteration 127/1000 | Loss: 0.00000671
Iteration 128/1000 | Loss: 0.00000671
Iteration 129/1000 | Loss: 0.00000671
Iteration 130/1000 | Loss: 0.00000671
Iteration 131/1000 | Loss: 0.00000671
Iteration 132/1000 | Loss: 0.00000671
Iteration 133/1000 | Loss: 0.00000671
Iteration 134/1000 | Loss: 0.00000671
Iteration 135/1000 | Loss: 0.00000671
Iteration 136/1000 | Loss: 0.00000670
Iteration 137/1000 | Loss: 0.00000670
Iteration 138/1000 | Loss: 0.00000670
Iteration 139/1000 | Loss: 0.00000670
Iteration 140/1000 | Loss: 0.00000670
Iteration 141/1000 | Loss: 0.00000670
Iteration 142/1000 | Loss: 0.00000670
Iteration 143/1000 | Loss: 0.00000670
Iteration 144/1000 | Loss: 0.00000670
Iteration 145/1000 | Loss: 0.00000670
Iteration 146/1000 | Loss: 0.00000670
Iteration 147/1000 | Loss: 0.00000670
Iteration 148/1000 | Loss: 0.00000670
Iteration 149/1000 | Loss: 0.00000670
Iteration 150/1000 | Loss: 0.00000670
Iteration 151/1000 | Loss: 0.00000670
Iteration 152/1000 | Loss: 0.00000670
Iteration 153/1000 | Loss: 0.00000670
Iteration 154/1000 | Loss: 0.00000669
Iteration 155/1000 | Loss: 0.00000669
Iteration 156/1000 | Loss: 0.00000669
Iteration 157/1000 | Loss: 0.00000669
Iteration 158/1000 | Loss: 0.00000669
Iteration 159/1000 | Loss: 0.00000669
Iteration 160/1000 | Loss: 0.00000669
Iteration 161/1000 | Loss: 0.00000668
Iteration 162/1000 | Loss: 0.00000668
Iteration 163/1000 | Loss: 0.00000668
Iteration 164/1000 | Loss: 0.00000668
Iteration 165/1000 | Loss: 0.00000668
Iteration 166/1000 | Loss: 0.00000668
Iteration 167/1000 | Loss: 0.00000668
Iteration 168/1000 | Loss: 0.00000668
Iteration 169/1000 | Loss: 0.00000668
Iteration 170/1000 | Loss: 0.00000668
Iteration 171/1000 | Loss: 0.00000668
Iteration 172/1000 | Loss: 0.00000668
Iteration 173/1000 | Loss: 0.00000668
Iteration 174/1000 | Loss: 0.00000668
Iteration 175/1000 | Loss: 0.00000668
Iteration 176/1000 | Loss: 0.00000668
Iteration 177/1000 | Loss: 0.00000668
Iteration 178/1000 | Loss: 0.00000668
Iteration 179/1000 | Loss: 0.00000668
Iteration 180/1000 | Loss: 0.00000668
Iteration 181/1000 | Loss: 0.00000668
Iteration 182/1000 | Loss: 0.00000668
Iteration 183/1000 | Loss: 0.00000668
Iteration 184/1000 | Loss: 0.00000668
Iteration 185/1000 | Loss: 0.00000668
Iteration 186/1000 | Loss: 0.00000668
Iteration 187/1000 | Loss: 0.00000668
Iteration 188/1000 | Loss: 0.00000668
Iteration 189/1000 | Loss: 0.00000668
Iteration 190/1000 | Loss: 0.00000668
Iteration 191/1000 | Loss: 0.00000668
Iteration 192/1000 | Loss: 0.00000668
Iteration 193/1000 | Loss: 0.00000668
Iteration 194/1000 | Loss: 0.00000668
Iteration 195/1000 | Loss: 0.00000668
Iteration 196/1000 | Loss: 0.00000668
Iteration 197/1000 | Loss: 0.00000668
Iteration 198/1000 | Loss: 0.00000668
Iteration 199/1000 | Loss: 0.00000668
Iteration 200/1000 | Loss: 0.00000668
Iteration 201/1000 | Loss: 0.00000668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [6.681893864879385e-06, 6.681893864879385e-06, 6.681893864879385e-06, 6.681893864879385e-06, 6.681893864879385e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.681893864879385e-06

Optimization complete. Final v2v error: 2.216383457183838 mm

Highest mean error: 2.6360127925872803 mm for frame 90

Lowest mean error: 2.0461628437042236 mm for frame 65

Saving results

Total time: 37.73188066482544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382953
Iteration 2/25 | Loss: 0.00109913
Iteration 3/25 | Loss: 0.00097529
Iteration 4/25 | Loss: 0.00096392
Iteration 5/25 | Loss: 0.00096115
Iteration 6/25 | Loss: 0.00096115
Iteration 7/25 | Loss: 0.00096115
Iteration 8/25 | Loss: 0.00096115
Iteration 9/25 | Loss: 0.00096115
Iteration 10/25 | Loss: 0.00096115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009611494024284184, 0.0009611494024284184, 0.0009611494024284184, 0.0009611494024284184, 0.0009611494024284184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009611494024284184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34402716
Iteration 2/25 | Loss: 0.00188059
Iteration 3/25 | Loss: 0.00188059
Iteration 4/25 | Loss: 0.00188059
Iteration 5/25 | Loss: 0.00188059
Iteration 6/25 | Loss: 0.00188059
Iteration 7/25 | Loss: 0.00188059
Iteration 8/25 | Loss: 0.00188059
Iteration 9/25 | Loss: 0.00188059
Iteration 10/25 | Loss: 0.00188059
Iteration 11/25 | Loss: 0.00188059
Iteration 12/25 | Loss: 0.00188059
Iteration 13/25 | Loss: 0.00188059
Iteration 14/25 | Loss: 0.00188059
Iteration 15/25 | Loss: 0.00188059
Iteration 16/25 | Loss: 0.00188059
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0018805883591994643, 0.0018805883591994643, 0.0018805883591994643, 0.0018805883591994643, 0.0018805883591994643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018805883591994643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188059
Iteration 2/1000 | Loss: 0.00002443
Iteration 3/1000 | Loss: 0.00001359
Iteration 4/1000 | Loss: 0.00001011
Iteration 5/1000 | Loss: 0.00000899
Iteration 6/1000 | Loss: 0.00000836
Iteration 7/1000 | Loss: 0.00000807
Iteration 8/1000 | Loss: 0.00000778
Iteration 9/1000 | Loss: 0.00000747
Iteration 10/1000 | Loss: 0.00000731
Iteration 11/1000 | Loss: 0.00000714
Iteration 12/1000 | Loss: 0.00000712
Iteration 13/1000 | Loss: 0.00000711
Iteration 14/1000 | Loss: 0.00000709
Iteration 15/1000 | Loss: 0.00000708
Iteration 16/1000 | Loss: 0.00000707
Iteration 17/1000 | Loss: 0.00000701
Iteration 18/1000 | Loss: 0.00000700
Iteration 19/1000 | Loss: 0.00000699
Iteration 20/1000 | Loss: 0.00000698
Iteration 21/1000 | Loss: 0.00000697
Iteration 22/1000 | Loss: 0.00000697
Iteration 23/1000 | Loss: 0.00000696
Iteration 24/1000 | Loss: 0.00000695
Iteration 25/1000 | Loss: 0.00000695
Iteration 26/1000 | Loss: 0.00000695
Iteration 27/1000 | Loss: 0.00000694
Iteration 28/1000 | Loss: 0.00000694
Iteration 29/1000 | Loss: 0.00000694
Iteration 30/1000 | Loss: 0.00000694
Iteration 31/1000 | Loss: 0.00000694
Iteration 32/1000 | Loss: 0.00000693
Iteration 33/1000 | Loss: 0.00000693
Iteration 34/1000 | Loss: 0.00000692
Iteration 35/1000 | Loss: 0.00000691
Iteration 36/1000 | Loss: 0.00000690
Iteration 37/1000 | Loss: 0.00000690
Iteration 38/1000 | Loss: 0.00000690
Iteration 39/1000 | Loss: 0.00000689
Iteration 40/1000 | Loss: 0.00000689
Iteration 41/1000 | Loss: 0.00000689
Iteration 42/1000 | Loss: 0.00000688
Iteration 43/1000 | Loss: 0.00000688
Iteration 44/1000 | Loss: 0.00000687
Iteration 45/1000 | Loss: 0.00000687
Iteration 46/1000 | Loss: 0.00000686
Iteration 47/1000 | Loss: 0.00000685
Iteration 48/1000 | Loss: 0.00000685
Iteration 49/1000 | Loss: 0.00000684
Iteration 50/1000 | Loss: 0.00000683
Iteration 51/1000 | Loss: 0.00000682
Iteration 52/1000 | Loss: 0.00000682
Iteration 53/1000 | Loss: 0.00000681
Iteration 54/1000 | Loss: 0.00000680
Iteration 55/1000 | Loss: 0.00000680
Iteration 56/1000 | Loss: 0.00000680
Iteration 57/1000 | Loss: 0.00000680
Iteration 58/1000 | Loss: 0.00000679
Iteration 59/1000 | Loss: 0.00000675
Iteration 60/1000 | Loss: 0.00000673
Iteration 61/1000 | Loss: 0.00000673
Iteration 62/1000 | Loss: 0.00000673
Iteration 63/1000 | Loss: 0.00000673
Iteration 64/1000 | Loss: 0.00000672
Iteration 65/1000 | Loss: 0.00000672
Iteration 66/1000 | Loss: 0.00000672
Iteration 67/1000 | Loss: 0.00000672
Iteration 68/1000 | Loss: 0.00000672
Iteration 69/1000 | Loss: 0.00000672
Iteration 70/1000 | Loss: 0.00000672
Iteration 71/1000 | Loss: 0.00000672
Iteration 72/1000 | Loss: 0.00000672
Iteration 73/1000 | Loss: 0.00000671
Iteration 74/1000 | Loss: 0.00000671
Iteration 75/1000 | Loss: 0.00000671
Iteration 76/1000 | Loss: 0.00000670
Iteration 77/1000 | Loss: 0.00000670
Iteration 78/1000 | Loss: 0.00000669
Iteration 79/1000 | Loss: 0.00000669
Iteration 80/1000 | Loss: 0.00000668
Iteration 81/1000 | Loss: 0.00000668
Iteration 82/1000 | Loss: 0.00000668
Iteration 83/1000 | Loss: 0.00000668
Iteration 84/1000 | Loss: 0.00000668
Iteration 85/1000 | Loss: 0.00000667
Iteration 86/1000 | Loss: 0.00000667
Iteration 87/1000 | Loss: 0.00000667
Iteration 88/1000 | Loss: 0.00000667
Iteration 89/1000 | Loss: 0.00000667
Iteration 90/1000 | Loss: 0.00000666
Iteration 91/1000 | Loss: 0.00000666
Iteration 92/1000 | Loss: 0.00000666
Iteration 93/1000 | Loss: 0.00000665
Iteration 94/1000 | Loss: 0.00000665
Iteration 95/1000 | Loss: 0.00000665
Iteration 96/1000 | Loss: 0.00000665
Iteration 97/1000 | Loss: 0.00000665
Iteration 98/1000 | Loss: 0.00000665
Iteration 99/1000 | Loss: 0.00000665
Iteration 100/1000 | Loss: 0.00000665
Iteration 101/1000 | Loss: 0.00000664
Iteration 102/1000 | Loss: 0.00000664
Iteration 103/1000 | Loss: 0.00000664
Iteration 104/1000 | Loss: 0.00000664
Iteration 105/1000 | Loss: 0.00000664
Iteration 106/1000 | Loss: 0.00000663
Iteration 107/1000 | Loss: 0.00000663
Iteration 108/1000 | Loss: 0.00000663
Iteration 109/1000 | Loss: 0.00000662
Iteration 110/1000 | Loss: 0.00000662
Iteration 111/1000 | Loss: 0.00000662
Iteration 112/1000 | Loss: 0.00000662
Iteration 113/1000 | Loss: 0.00000662
Iteration 114/1000 | Loss: 0.00000662
Iteration 115/1000 | Loss: 0.00000662
Iteration 116/1000 | Loss: 0.00000662
Iteration 117/1000 | Loss: 0.00000662
Iteration 118/1000 | Loss: 0.00000662
Iteration 119/1000 | Loss: 0.00000662
Iteration 120/1000 | Loss: 0.00000662
Iteration 121/1000 | Loss: 0.00000662
Iteration 122/1000 | Loss: 0.00000662
Iteration 123/1000 | Loss: 0.00000661
Iteration 124/1000 | Loss: 0.00000661
Iteration 125/1000 | Loss: 0.00000661
Iteration 126/1000 | Loss: 0.00000661
Iteration 127/1000 | Loss: 0.00000661
Iteration 128/1000 | Loss: 0.00000661
Iteration 129/1000 | Loss: 0.00000661
Iteration 130/1000 | Loss: 0.00000661
Iteration 131/1000 | Loss: 0.00000661
Iteration 132/1000 | Loss: 0.00000661
Iteration 133/1000 | Loss: 0.00000661
Iteration 134/1000 | Loss: 0.00000661
Iteration 135/1000 | Loss: 0.00000661
Iteration 136/1000 | Loss: 0.00000661
Iteration 137/1000 | Loss: 0.00000661
Iteration 138/1000 | Loss: 0.00000661
Iteration 139/1000 | Loss: 0.00000661
Iteration 140/1000 | Loss: 0.00000660
Iteration 141/1000 | Loss: 0.00000660
Iteration 142/1000 | Loss: 0.00000660
Iteration 143/1000 | Loss: 0.00000660
Iteration 144/1000 | Loss: 0.00000660
Iteration 145/1000 | Loss: 0.00000660
Iteration 146/1000 | Loss: 0.00000660
Iteration 147/1000 | Loss: 0.00000660
Iteration 148/1000 | Loss: 0.00000660
Iteration 149/1000 | Loss: 0.00000660
Iteration 150/1000 | Loss: 0.00000660
Iteration 151/1000 | Loss: 0.00000660
Iteration 152/1000 | Loss: 0.00000660
Iteration 153/1000 | Loss: 0.00000660
Iteration 154/1000 | Loss: 0.00000660
Iteration 155/1000 | Loss: 0.00000659
Iteration 156/1000 | Loss: 0.00000659
Iteration 157/1000 | Loss: 0.00000659
Iteration 158/1000 | Loss: 0.00000659
Iteration 159/1000 | Loss: 0.00000659
Iteration 160/1000 | Loss: 0.00000659
Iteration 161/1000 | Loss: 0.00000659
Iteration 162/1000 | Loss: 0.00000659
Iteration 163/1000 | Loss: 0.00000659
Iteration 164/1000 | Loss: 0.00000659
Iteration 165/1000 | Loss: 0.00000659
Iteration 166/1000 | Loss: 0.00000659
Iteration 167/1000 | Loss: 0.00000659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [6.591672445210861e-06, 6.591672445210861e-06, 6.591672445210861e-06, 6.591672445210861e-06, 6.591672445210861e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.591672445210861e-06

Optimization complete. Final v2v error: 2.209003210067749 mm

Highest mean error: 2.455042600631714 mm for frame 24

Lowest mean error: 2.028733491897583 mm for frame 180

Saving results

Total time: 40.87139415740967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876954
Iteration 2/25 | Loss: 0.00103215
Iteration 3/25 | Loss: 0.00096538
Iteration 4/25 | Loss: 0.00095397
Iteration 5/25 | Loss: 0.00095051
Iteration 6/25 | Loss: 0.00094948
Iteration 7/25 | Loss: 0.00094948
Iteration 8/25 | Loss: 0.00094948
Iteration 9/25 | Loss: 0.00094948
Iteration 10/25 | Loss: 0.00094948
Iteration 11/25 | Loss: 0.00094948
Iteration 12/25 | Loss: 0.00094948
Iteration 13/25 | Loss: 0.00094948
Iteration 14/25 | Loss: 0.00094948
Iteration 15/25 | Loss: 0.00094947
Iteration 16/25 | Loss: 0.00094947
Iteration 17/25 | Loss: 0.00094947
Iteration 18/25 | Loss: 0.00094947
Iteration 19/25 | Loss: 0.00094947
Iteration 20/25 | Loss: 0.00094947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009494656696915627, 0.0009494656696915627, 0.0009494656696915627, 0.0009494656696915627, 0.0009494656696915627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009494656696915627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22320867
Iteration 2/25 | Loss: 0.00192705
Iteration 3/25 | Loss: 0.00192705
Iteration 4/25 | Loss: 0.00192705
Iteration 5/25 | Loss: 0.00192705
Iteration 6/25 | Loss: 0.00192705
Iteration 7/25 | Loss: 0.00192705
Iteration 8/25 | Loss: 0.00192705
Iteration 9/25 | Loss: 0.00192705
Iteration 10/25 | Loss: 0.00192705
Iteration 11/25 | Loss: 0.00192705
Iteration 12/25 | Loss: 0.00192705
Iteration 13/25 | Loss: 0.00192705
Iteration 14/25 | Loss: 0.00192705
Iteration 15/25 | Loss: 0.00192705
Iteration 16/25 | Loss: 0.00192705
Iteration 17/25 | Loss: 0.00192705
Iteration 18/25 | Loss: 0.00192705
Iteration 19/25 | Loss: 0.00192705
Iteration 20/25 | Loss: 0.00192705
Iteration 21/25 | Loss: 0.00192705
Iteration 22/25 | Loss: 0.00192705
Iteration 23/25 | Loss: 0.00192705
Iteration 24/25 | Loss: 0.00192705
Iteration 25/25 | Loss: 0.00192705
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0019270493648946285, 0.0019270493648946285, 0.0019270493648946285, 0.0019270493648946285, 0.0019270493648946285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019270493648946285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192705
Iteration 2/1000 | Loss: 0.00001951
Iteration 3/1000 | Loss: 0.00001013
Iteration 4/1000 | Loss: 0.00000907
Iteration 5/1000 | Loss: 0.00000832
Iteration 6/1000 | Loss: 0.00000804
Iteration 7/1000 | Loss: 0.00000764
Iteration 8/1000 | Loss: 0.00000746
Iteration 9/1000 | Loss: 0.00000746
Iteration 10/1000 | Loss: 0.00000745
Iteration 11/1000 | Loss: 0.00000737
Iteration 12/1000 | Loss: 0.00000737
Iteration 13/1000 | Loss: 0.00000735
Iteration 14/1000 | Loss: 0.00000731
Iteration 15/1000 | Loss: 0.00000730
Iteration 16/1000 | Loss: 0.00000730
Iteration 17/1000 | Loss: 0.00000729
Iteration 18/1000 | Loss: 0.00000727
Iteration 19/1000 | Loss: 0.00000723
Iteration 20/1000 | Loss: 0.00000722
Iteration 21/1000 | Loss: 0.00000722
Iteration 22/1000 | Loss: 0.00000720
Iteration 23/1000 | Loss: 0.00000719
Iteration 24/1000 | Loss: 0.00000718
Iteration 25/1000 | Loss: 0.00000718
Iteration 26/1000 | Loss: 0.00000717
Iteration 27/1000 | Loss: 0.00000717
Iteration 28/1000 | Loss: 0.00000717
Iteration 29/1000 | Loss: 0.00000717
Iteration 30/1000 | Loss: 0.00000716
Iteration 31/1000 | Loss: 0.00000716
Iteration 32/1000 | Loss: 0.00000716
Iteration 33/1000 | Loss: 0.00000716
Iteration 34/1000 | Loss: 0.00000716
Iteration 35/1000 | Loss: 0.00000715
Iteration 36/1000 | Loss: 0.00000714
Iteration 37/1000 | Loss: 0.00000714
Iteration 38/1000 | Loss: 0.00000713
Iteration 39/1000 | Loss: 0.00000713
Iteration 40/1000 | Loss: 0.00000713
Iteration 41/1000 | Loss: 0.00000713
Iteration 42/1000 | Loss: 0.00000712
Iteration 43/1000 | Loss: 0.00000712
Iteration 44/1000 | Loss: 0.00000712
Iteration 45/1000 | Loss: 0.00000712
Iteration 46/1000 | Loss: 0.00000711
Iteration 47/1000 | Loss: 0.00000710
Iteration 48/1000 | Loss: 0.00000710
Iteration 49/1000 | Loss: 0.00000710
Iteration 50/1000 | Loss: 0.00000710
Iteration 51/1000 | Loss: 0.00000709
Iteration 52/1000 | Loss: 0.00000709
Iteration 53/1000 | Loss: 0.00000708
Iteration 54/1000 | Loss: 0.00000707
Iteration 55/1000 | Loss: 0.00000706
Iteration 56/1000 | Loss: 0.00000706
Iteration 57/1000 | Loss: 0.00000704
Iteration 58/1000 | Loss: 0.00000704
Iteration 59/1000 | Loss: 0.00000702
Iteration 60/1000 | Loss: 0.00000702
Iteration 61/1000 | Loss: 0.00000702
Iteration 62/1000 | Loss: 0.00000702
Iteration 63/1000 | Loss: 0.00000702
Iteration 64/1000 | Loss: 0.00000701
Iteration 65/1000 | Loss: 0.00000701
Iteration 66/1000 | Loss: 0.00000701
Iteration 67/1000 | Loss: 0.00000700
Iteration 68/1000 | Loss: 0.00000700
Iteration 69/1000 | Loss: 0.00000700
Iteration 70/1000 | Loss: 0.00000699
Iteration 71/1000 | Loss: 0.00000699
Iteration 72/1000 | Loss: 0.00000698
Iteration 73/1000 | Loss: 0.00000698
Iteration 74/1000 | Loss: 0.00000698
Iteration 75/1000 | Loss: 0.00000698
Iteration 76/1000 | Loss: 0.00000698
Iteration 77/1000 | Loss: 0.00000698
Iteration 78/1000 | Loss: 0.00000698
Iteration 79/1000 | Loss: 0.00000698
Iteration 80/1000 | Loss: 0.00000698
Iteration 81/1000 | Loss: 0.00000698
Iteration 82/1000 | Loss: 0.00000698
Iteration 83/1000 | Loss: 0.00000698
Iteration 84/1000 | Loss: 0.00000698
Iteration 85/1000 | Loss: 0.00000698
Iteration 86/1000 | Loss: 0.00000698
Iteration 87/1000 | Loss: 0.00000698
Iteration 88/1000 | Loss: 0.00000698
Iteration 89/1000 | Loss: 0.00000698
Iteration 90/1000 | Loss: 0.00000698
Iteration 91/1000 | Loss: 0.00000698
Iteration 92/1000 | Loss: 0.00000698
Iteration 93/1000 | Loss: 0.00000698
Iteration 94/1000 | Loss: 0.00000698
Iteration 95/1000 | Loss: 0.00000698
Iteration 96/1000 | Loss: 0.00000698
Iteration 97/1000 | Loss: 0.00000698
Iteration 98/1000 | Loss: 0.00000698
Iteration 99/1000 | Loss: 0.00000698
Iteration 100/1000 | Loss: 0.00000698
Iteration 101/1000 | Loss: 0.00000698
Iteration 102/1000 | Loss: 0.00000698
Iteration 103/1000 | Loss: 0.00000698
Iteration 104/1000 | Loss: 0.00000698
Iteration 105/1000 | Loss: 0.00000698
Iteration 106/1000 | Loss: 0.00000698
Iteration 107/1000 | Loss: 0.00000698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [6.979618319746805e-06, 6.979618319746805e-06, 6.979618319746805e-06, 6.979618319746805e-06, 6.979618319746805e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.979618319746805e-06

Optimization complete. Final v2v error: 2.233826160430908 mm

Highest mean error: 2.412787914276123 mm for frame 109

Lowest mean error: 2.1935129165649414 mm for frame 78

Saving results

Total time: 28.103687524795532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830292
Iteration 2/25 | Loss: 0.00124975
Iteration 3/25 | Loss: 0.00098213
Iteration 4/25 | Loss: 0.00096414
Iteration 5/25 | Loss: 0.00095898
Iteration 6/25 | Loss: 0.00095718
Iteration 7/25 | Loss: 0.00095695
Iteration 8/25 | Loss: 0.00095695
Iteration 9/25 | Loss: 0.00095695
Iteration 10/25 | Loss: 0.00095695
Iteration 11/25 | Loss: 0.00095695
Iteration 12/25 | Loss: 0.00095695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009569498361088336, 0.0009569498361088336, 0.0009569498361088336, 0.0009569498361088336, 0.0009569498361088336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009569498361088336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09877706
Iteration 2/25 | Loss: 0.00190614
Iteration 3/25 | Loss: 0.00190613
Iteration 4/25 | Loss: 0.00190613
Iteration 5/25 | Loss: 0.00190613
Iteration 6/25 | Loss: 0.00190613
Iteration 7/25 | Loss: 0.00190613
Iteration 8/25 | Loss: 0.00190613
Iteration 9/25 | Loss: 0.00190613
Iteration 10/25 | Loss: 0.00190613
Iteration 11/25 | Loss: 0.00190613
Iteration 12/25 | Loss: 0.00190613
Iteration 13/25 | Loss: 0.00190613
Iteration 14/25 | Loss: 0.00190613
Iteration 15/25 | Loss: 0.00190613
Iteration 16/25 | Loss: 0.00190613
Iteration 17/25 | Loss: 0.00190613
Iteration 18/25 | Loss: 0.00190613
Iteration 19/25 | Loss: 0.00190613
Iteration 20/25 | Loss: 0.00190613
Iteration 21/25 | Loss: 0.00190613
Iteration 22/25 | Loss: 0.00190613
Iteration 23/25 | Loss: 0.00190613
Iteration 24/25 | Loss: 0.00190613
Iteration 25/25 | Loss: 0.00190613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190613
Iteration 2/1000 | Loss: 0.00003872
Iteration 3/1000 | Loss: 0.00002205
Iteration 4/1000 | Loss: 0.00001668
Iteration 5/1000 | Loss: 0.00001534
Iteration 6/1000 | Loss: 0.00001448
Iteration 7/1000 | Loss: 0.00001368
Iteration 8/1000 | Loss: 0.00001309
Iteration 9/1000 | Loss: 0.00001265
Iteration 10/1000 | Loss: 0.00001234
Iteration 11/1000 | Loss: 0.00001216
Iteration 12/1000 | Loss: 0.00001201
Iteration 13/1000 | Loss: 0.00001195
Iteration 14/1000 | Loss: 0.00001184
Iteration 15/1000 | Loss: 0.00001183
Iteration 16/1000 | Loss: 0.00001182
Iteration 17/1000 | Loss: 0.00001174
Iteration 18/1000 | Loss: 0.00001173
Iteration 19/1000 | Loss: 0.00001164
Iteration 20/1000 | Loss: 0.00001162
Iteration 21/1000 | Loss: 0.00001162
Iteration 22/1000 | Loss: 0.00001160
Iteration 23/1000 | Loss: 0.00001159
Iteration 24/1000 | Loss: 0.00001158
Iteration 25/1000 | Loss: 0.00001156
Iteration 26/1000 | Loss: 0.00001155
Iteration 27/1000 | Loss: 0.00001154
Iteration 28/1000 | Loss: 0.00001153
Iteration 29/1000 | Loss: 0.00001153
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001151
Iteration 32/1000 | Loss: 0.00001151
Iteration 33/1000 | Loss: 0.00001151
Iteration 34/1000 | Loss: 0.00001151
Iteration 35/1000 | Loss: 0.00001150
Iteration 36/1000 | Loss: 0.00001150
Iteration 37/1000 | Loss: 0.00001150
Iteration 38/1000 | Loss: 0.00001149
Iteration 39/1000 | Loss: 0.00001149
Iteration 40/1000 | Loss: 0.00001149
Iteration 41/1000 | Loss: 0.00001149
Iteration 42/1000 | Loss: 0.00001148
Iteration 43/1000 | Loss: 0.00001147
Iteration 44/1000 | Loss: 0.00001147
Iteration 45/1000 | Loss: 0.00001147
Iteration 46/1000 | Loss: 0.00001146
Iteration 47/1000 | Loss: 0.00001146
Iteration 48/1000 | Loss: 0.00001146
Iteration 49/1000 | Loss: 0.00001146
Iteration 50/1000 | Loss: 0.00001146
Iteration 51/1000 | Loss: 0.00001145
Iteration 52/1000 | Loss: 0.00001145
Iteration 53/1000 | Loss: 0.00001145
Iteration 54/1000 | Loss: 0.00001144
Iteration 55/1000 | Loss: 0.00001144
Iteration 56/1000 | Loss: 0.00001144
Iteration 57/1000 | Loss: 0.00001144
Iteration 58/1000 | Loss: 0.00001143
Iteration 59/1000 | Loss: 0.00001142
Iteration 60/1000 | Loss: 0.00001142
Iteration 61/1000 | Loss: 0.00001142
Iteration 62/1000 | Loss: 0.00001142
Iteration 63/1000 | Loss: 0.00001142
Iteration 64/1000 | Loss: 0.00001142
Iteration 65/1000 | Loss: 0.00001142
Iteration 66/1000 | Loss: 0.00001142
Iteration 67/1000 | Loss: 0.00001142
Iteration 68/1000 | Loss: 0.00001142
Iteration 69/1000 | Loss: 0.00001142
Iteration 70/1000 | Loss: 0.00001142
Iteration 71/1000 | Loss: 0.00001141
Iteration 72/1000 | Loss: 0.00001141
Iteration 73/1000 | Loss: 0.00001141
Iteration 74/1000 | Loss: 0.00001140
Iteration 75/1000 | Loss: 0.00001140
Iteration 76/1000 | Loss: 0.00001140
Iteration 77/1000 | Loss: 0.00001140
Iteration 78/1000 | Loss: 0.00001139
Iteration 79/1000 | Loss: 0.00001139
Iteration 80/1000 | Loss: 0.00001139
Iteration 81/1000 | Loss: 0.00001139
Iteration 82/1000 | Loss: 0.00001139
Iteration 83/1000 | Loss: 0.00001139
Iteration 84/1000 | Loss: 0.00001139
Iteration 85/1000 | Loss: 0.00001139
Iteration 86/1000 | Loss: 0.00001139
Iteration 87/1000 | Loss: 0.00001139
Iteration 88/1000 | Loss: 0.00001138
Iteration 89/1000 | Loss: 0.00001138
Iteration 90/1000 | Loss: 0.00001138
Iteration 91/1000 | Loss: 0.00001138
Iteration 92/1000 | Loss: 0.00001138
Iteration 93/1000 | Loss: 0.00001138
Iteration 94/1000 | Loss: 0.00001137
Iteration 95/1000 | Loss: 0.00001137
Iteration 96/1000 | Loss: 0.00001136
Iteration 97/1000 | Loss: 0.00001136
Iteration 98/1000 | Loss: 0.00001136
Iteration 99/1000 | Loss: 0.00001136
Iteration 100/1000 | Loss: 0.00001136
Iteration 101/1000 | Loss: 0.00001136
Iteration 102/1000 | Loss: 0.00001136
Iteration 103/1000 | Loss: 0.00001136
Iteration 104/1000 | Loss: 0.00001136
Iteration 105/1000 | Loss: 0.00001136
Iteration 106/1000 | Loss: 0.00001136
Iteration 107/1000 | Loss: 0.00001135
Iteration 108/1000 | Loss: 0.00001135
Iteration 109/1000 | Loss: 0.00001135
Iteration 110/1000 | Loss: 0.00001135
Iteration 111/1000 | Loss: 0.00001135
Iteration 112/1000 | Loss: 0.00001135
Iteration 113/1000 | Loss: 0.00001135
Iteration 114/1000 | Loss: 0.00001135
Iteration 115/1000 | Loss: 0.00001135
Iteration 116/1000 | Loss: 0.00001135
Iteration 117/1000 | Loss: 0.00001135
Iteration 118/1000 | Loss: 0.00001135
Iteration 119/1000 | Loss: 0.00001135
Iteration 120/1000 | Loss: 0.00001135
Iteration 121/1000 | Loss: 0.00001135
Iteration 122/1000 | Loss: 0.00001135
Iteration 123/1000 | Loss: 0.00001134
Iteration 124/1000 | Loss: 0.00001134
Iteration 125/1000 | Loss: 0.00001134
Iteration 126/1000 | Loss: 0.00001134
Iteration 127/1000 | Loss: 0.00001134
Iteration 128/1000 | Loss: 0.00001134
Iteration 129/1000 | Loss: 0.00001134
Iteration 130/1000 | Loss: 0.00001134
Iteration 131/1000 | Loss: 0.00001134
Iteration 132/1000 | Loss: 0.00001134
Iteration 133/1000 | Loss: 0.00001133
Iteration 134/1000 | Loss: 0.00001133
Iteration 135/1000 | Loss: 0.00001133
Iteration 136/1000 | Loss: 0.00001133
Iteration 137/1000 | Loss: 0.00001133
Iteration 138/1000 | Loss: 0.00001133
Iteration 139/1000 | Loss: 0.00001133
Iteration 140/1000 | Loss: 0.00001133
Iteration 141/1000 | Loss: 0.00001133
Iteration 142/1000 | Loss: 0.00001133
Iteration 143/1000 | Loss: 0.00001133
Iteration 144/1000 | Loss: 0.00001133
Iteration 145/1000 | Loss: 0.00001132
Iteration 146/1000 | Loss: 0.00001132
Iteration 147/1000 | Loss: 0.00001132
Iteration 148/1000 | Loss: 0.00001132
Iteration 149/1000 | Loss: 0.00001132
Iteration 150/1000 | Loss: 0.00001132
Iteration 151/1000 | Loss: 0.00001132
Iteration 152/1000 | Loss: 0.00001132
Iteration 153/1000 | Loss: 0.00001131
Iteration 154/1000 | Loss: 0.00001131
Iteration 155/1000 | Loss: 0.00001131
Iteration 156/1000 | Loss: 0.00001131
Iteration 157/1000 | Loss: 0.00001131
Iteration 158/1000 | Loss: 0.00001130
Iteration 159/1000 | Loss: 0.00001130
Iteration 160/1000 | Loss: 0.00001130
Iteration 161/1000 | Loss: 0.00001130
Iteration 162/1000 | Loss: 0.00001130
Iteration 163/1000 | Loss: 0.00001130
Iteration 164/1000 | Loss: 0.00001130
Iteration 165/1000 | Loss: 0.00001130
Iteration 166/1000 | Loss: 0.00001130
Iteration 167/1000 | Loss: 0.00001130
Iteration 168/1000 | Loss: 0.00001130
Iteration 169/1000 | Loss: 0.00001130
Iteration 170/1000 | Loss: 0.00001130
Iteration 171/1000 | Loss: 0.00001130
Iteration 172/1000 | Loss: 0.00001130
Iteration 173/1000 | Loss: 0.00001130
Iteration 174/1000 | Loss: 0.00001130
Iteration 175/1000 | Loss: 0.00001129
Iteration 176/1000 | Loss: 0.00001129
Iteration 177/1000 | Loss: 0.00001129
Iteration 178/1000 | Loss: 0.00001129
Iteration 179/1000 | Loss: 0.00001129
Iteration 180/1000 | Loss: 0.00001129
Iteration 181/1000 | Loss: 0.00001129
Iteration 182/1000 | Loss: 0.00001129
Iteration 183/1000 | Loss: 0.00001129
Iteration 184/1000 | Loss: 0.00001129
Iteration 185/1000 | Loss: 0.00001128
Iteration 186/1000 | Loss: 0.00001128
Iteration 187/1000 | Loss: 0.00001128
Iteration 188/1000 | Loss: 0.00001128
Iteration 189/1000 | Loss: 0.00001128
Iteration 190/1000 | Loss: 0.00001128
Iteration 191/1000 | Loss: 0.00001128
Iteration 192/1000 | Loss: 0.00001128
Iteration 193/1000 | Loss: 0.00001128
Iteration 194/1000 | Loss: 0.00001128
Iteration 195/1000 | Loss: 0.00001128
Iteration 196/1000 | Loss: 0.00001128
Iteration 197/1000 | Loss: 0.00001128
Iteration 198/1000 | Loss: 0.00001128
Iteration 199/1000 | Loss: 0.00001128
Iteration 200/1000 | Loss: 0.00001128
Iteration 201/1000 | Loss: 0.00001128
Iteration 202/1000 | Loss: 0.00001128
Iteration 203/1000 | Loss: 0.00001128
Iteration 204/1000 | Loss: 0.00001128
Iteration 205/1000 | Loss: 0.00001127
Iteration 206/1000 | Loss: 0.00001127
Iteration 207/1000 | Loss: 0.00001127
Iteration 208/1000 | Loss: 0.00001127
Iteration 209/1000 | Loss: 0.00001127
Iteration 210/1000 | Loss: 0.00001127
Iteration 211/1000 | Loss: 0.00001127
Iteration 212/1000 | Loss: 0.00001127
Iteration 213/1000 | Loss: 0.00001127
Iteration 214/1000 | Loss: 0.00001127
Iteration 215/1000 | Loss: 0.00001127
Iteration 216/1000 | Loss: 0.00001127
Iteration 217/1000 | Loss: 0.00001127
Iteration 218/1000 | Loss: 0.00001127
Iteration 219/1000 | Loss: 0.00001126
Iteration 220/1000 | Loss: 0.00001126
Iteration 221/1000 | Loss: 0.00001126
Iteration 222/1000 | Loss: 0.00001126
Iteration 223/1000 | Loss: 0.00001126
Iteration 224/1000 | Loss: 0.00001126
Iteration 225/1000 | Loss: 0.00001126
Iteration 226/1000 | Loss: 0.00001126
Iteration 227/1000 | Loss: 0.00001126
Iteration 228/1000 | Loss: 0.00001126
Iteration 229/1000 | Loss: 0.00001126
Iteration 230/1000 | Loss: 0.00001126
Iteration 231/1000 | Loss: 0.00001126
Iteration 232/1000 | Loss: 0.00001126
Iteration 233/1000 | Loss: 0.00001126
Iteration 234/1000 | Loss: 0.00001126
Iteration 235/1000 | Loss: 0.00001126
Iteration 236/1000 | Loss: 0.00001126
Iteration 237/1000 | Loss: 0.00001126
Iteration 238/1000 | Loss: 0.00001126
Iteration 239/1000 | Loss: 0.00001126
Iteration 240/1000 | Loss: 0.00001126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.125604467233643e-05, 1.125604467233643e-05, 1.125604467233643e-05, 1.125604467233643e-05, 1.125604467233643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.125604467233643e-05

Optimization complete. Final v2v error: 2.6155641078948975 mm

Highest mean error: 4.1318135261535645 mm for frame 62

Lowest mean error: 1.9948533773422241 mm for frame 91

Saving results

Total time: 46.498802185058594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461539
Iteration 2/25 | Loss: 0.00125871
Iteration 3/25 | Loss: 0.00103350
Iteration 4/25 | Loss: 0.00101226
Iteration 5/25 | Loss: 0.00101022
Iteration 6/25 | Loss: 0.00101001
Iteration 7/25 | Loss: 0.00101001
Iteration 8/25 | Loss: 0.00101001
Iteration 9/25 | Loss: 0.00101001
Iteration 10/25 | Loss: 0.00101001
Iteration 11/25 | Loss: 0.00101001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010100089712068439, 0.0010100089712068439, 0.0010100089712068439, 0.0010100089712068439, 0.0010100089712068439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010100089712068439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21982479
Iteration 2/25 | Loss: 0.00166321
Iteration 3/25 | Loss: 0.00166319
Iteration 4/25 | Loss: 0.00166319
Iteration 5/25 | Loss: 0.00166319
Iteration 6/25 | Loss: 0.00166319
Iteration 7/25 | Loss: 0.00166319
Iteration 8/25 | Loss: 0.00166319
Iteration 9/25 | Loss: 0.00166319
Iteration 10/25 | Loss: 0.00166319
Iteration 11/25 | Loss: 0.00166319
Iteration 12/25 | Loss: 0.00166319
Iteration 13/25 | Loss: 0.00166319
Iteration 14/25 | Loss: 0.00166319
Iteration 15/25 | Loss: 0.00166319
Iteration 16/25 | Loss: 0.00166319
Iteration 17/25 | Loss: 0.00166319
Iteration 18/25 | Loss: 0.00166319
Iteration 19/25 | Loss: 0.00166319
Iteration 20/25 | Loss: 0.00166319
Iteration 21/25 | Loss: 0.00166319
Iteration 22/25 | Loss: 0.00166319
Iteration 23/25 | Loss: 0.00166319
Iteration 24/25 | Loss: 0.00166319
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0016631914768368006, 0.0016631914768368006, 0.0016631914768368006, 0.0016631914768368006, 0.0016631914768368006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016631914768368006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166319
Iteration 2/1000 | Loss: 0.00002303
Iteration 3/1000 | Loss: 0.00001395
Iteration 4/1000 | Loss: 0.00001242
Iteration 5/1000 | Loss: 0.00001177
Iteration 6/1000 | Loss: 0.00001140
Iteration 7/1000 | Loss: 0.00001108
Iteration 8/1000 | Loss: 0.00001092
Iteration 9/1000 | Loss: 0.00001073
Iteration 10/1000 | Loss: 0.00001054
Iteration 11/1000 | Loss: 0.00001053
Iteration 12/1000 | Loss: 0.00001053
Iteration 13/1000 | Loss: 0.00001051
Iteration 14/1000 | Loss: 0.00001049
Iteration 15/1000 | Loss: 0.00001047
Iteration 16/1000 | Loss: 0.00001042
Iteration 17/1000 | Loss: 0.00001040
Iteration 18/1000 | Loss: 0.00001034
Iteration 19/1000 | Loss: 0.00001034
Iteration 20/1000 | Loss: 0.00001033
Iteration 21/1000 | Loss: 0.00001033
Iteration 22/1000 | Loss: 0.00001032
Iteration 23/1000 | Loss: 0.00001031
Iteration 24/1000 | Loss: 0.00001031
Iteration 25/1000 | Loss: 0.00001031
Iteration 26/1000 | Loss: 0.00001030
Iteration 27/1000 | Loss: 0.00001029
Iteration 28/1000 | Loss: 0.00001029
Iteration 29/1000 | Loss: 0.00001028
Iteration 30/1000 | Loss: 0.00001028
Iteration 31/1000 | Loss: 0.00001028
Iteration 32/1000 | Loss: 0.00001028
Iteration 33/1000 | Loss: 0.00001028
Iteration 34/1000 | Loss: 0.00001028
Iteration 35/1000 | Loss: 0.00001028
Iteration 36/1000 | Loss: 0.00001028
Iteration 37/1000 | Loss: 0.00001027
Iteration 38/1000 | Loss: 0.00001027
Iteration 39/1000 | Loss: 0.00001026
Iteration 40/1000 | Loss: 0.00001026
Iteration 41/1000 | Loss: 0.00001026
Iteration 42/1000 | Loss: 0.00001026
Iteration 43/1000 | Loss: 0.00001025
Iteration 44/1000 | Loss: 0.00001025
Iteration 45/1000 | Loss: 0.00001025
Iteration 46/1000 | Loss: 0.00001025
Iteration 47/1000 | Loss: 0.00001024
Iteration 48/1000 | Loss: 0.00001024
Iteration 49/1000 | Loss: 0.00001024
Iteration 50/1000 | Loss: 0.00001024
Iteration 51/1000 | Loss: 0.00001024
Iteration 52/1000 | Loss: 0.00001024
Iteration 53/1000 | Loss: 0.00001024
Iteration 54/1000 | Loss: 0.00001023
Iteration 55/1000 | Loss: 0.00001023
Iteration 56/1000 | Loss: 0.00001023
Iteration 57/1000 | Loss: 0.00001023
Iteration 58/1000 | Loss: 0.00001022
Iteration 59/1000 | Loss: 0.00001022
Iteration 60/1000 | Loss: 0.00001022
Iteration 61/1000 | Loss: 0.00001021
Iteration 62/1000 | Loss: 0.00001021
Iteration 63/1000 | Loss: 0.00001021
Iteration 64/1000 | Loss: 0.00001021
Iteration 65/1000 | Loss: 0.00001021
Iteration 66/1000 | Loss: 0.00001020
Iteration 67/1000 | Loss: 0.00001020
Iteration 68/1000 | Loss: 0.00001020
Iteration 69/1000 | Loss: 0.00001020
Iteration 70/1000 | Loss: 0.00001020
Iteration 71/1000 | Loss: 0.00001020
Iteration 72/1000 | Loss: 0.00001020
Iteration 73/1000 | Loss: 0.00001020
Iteration 74/1000 | Loss: 0.00001019
Iteration 75/1000 | Loss: 0.00001019
Iteration 76/1000 | Loss: 0.00001019
Iteration 77/1000 | Loss: 0.00001019
Iteration 78/1000 | Loss: 0.00001018
Iteration 79/1000 | Loss: 0.00001018
Iteration 80/1000 | Loss: 0.00001018
Iteration 81/1000 | Loss: 0.00001018
Iteration 82/1000 | Loss: 0.00001018
Iteration 83/1000 | Loss: 0.00001018
Iteration 84/1000 | Loss: 0.00001018
Iteration 85/1000 | Loss: 0.00001018
Iteration 86/1000 | Loss: 0.00001018
Iteration 87/1000 | Loss: 0.00001017
Iteration 88/1000 | Loss: 0.00001017
Iteration 89/1000 | Loss: 0.00001017
Iteration 90/1000 | Loss: 0.00001017
Iteration 91/1000 | Loss: 0.00001017
Iteration 92/1000 | Loss: 0.00001017
Iteration 93/1000 | Loss: 0.00001017
Iteration 94/1000 | Loss: 0.00001017
Iteration 95/1000 | Loss: 0.00001017
Iteration 96/1000 | Loss: 0.00001016
Iteration 97/1000 | Loss: 0.00001016
Iteration 98/1000 | Loss: 0.00001016
Iteration 99/1000 | Loss: 0.00001016
Iteration 100/1000 | Loss: 0.00001015
Iteration 101/1000 | Loss: 0.00001015
Iteration 102/1000 | Loss: 0.00001015
Iteration 103/1000 | Loss: 0.00001015
Iteration 104/1000 | Loss: 0.00001015
Iteration 105/1000 | Loss: 0.00001015
Iteration 106/1000 | Loss: 0.00001014
Iteration 107/1000 | Loss: 0.00001014
Iteration 108/1000 | Loss: 0.00001014
Iteration 109/1000 | Loss: 0.00001014
Iteration 110/1000 | Loss: 0.00001014
Iteration 111/1000 | Loss: 0.00001014
Iteration 112/1000 | Loss: 0.00001014
Iteration 113/1000 | Loss: 0.00001014
Iteration 114/1000 | Loss: 0.00001013
Iteration 115/1000 | Loss: 0.00001013
Iteration 116/1000 | Loss: 0.00001013
Iteration 117/1000 | Loss: 0.00001013
Iteration 118/1000 | Loss: 0.00001013
Iteration 119/1000 | Loss: 0.00001013
Iteration 120/1000 | Loss: 0.00001013
Iteration 121/1000 | Loss: 0.00001013
Iteration 122/1000 | Loss: 0.00001013
Iteration 123/1000 | Loss: 0.00001013
Iteration 124/1000 | Loss: 0.00001013
Iteration 125/1000 | Loss: 0.00001013
Iteration 126/1000 | Loss: 0.00001012
Iteration 127/1000 | Loss: 0.00001012
Iteration 128/1000 | Loss: 0.00001012
Iteration 129/1000 | Loss: 0.00001012
Iteration 130/1000 | Loss: 0.00001012
Iteration 131/1000 | Loss: 0.00001012
Iteration 132/1000 | Loss: 0.00001012
Iteration 133/1000 | Loss: 0.00001012
Iteration 134/1000 | Loss: 0.00001012
Iteration 135/1000 | Loss: 0.00001012
Iteration 136/1000 | Loss: 0.00001012
Iteration 137/1000 | Loss: 0.00001012
Iteration 138/1000 | Loss: 0.00001012
Iteration 139/1000 | Loss: 0.00001012
Iteration 140/1000 | Loss: 0.00001012
Iteration 141/1000 | Loss: 0.00001012
Iteration 142/1000 | Loss: 0.00001012
Iteration 143/1000 | Loss: 0.00001012
Iteration 144/1000 | Loss: 0.00001012
Iteration 145/1000 | Loss: 0.00001012
Iteration 146/1000 | Loss: 0.00001012
Iteration 147/1000 | Loss: 0.00001012
Iteration 148/1000 | Loss: 0.00001012
Iteration 149/1000 | Loss: 0.00001012
Iteration 150/1000 | Loss: 0.00001012
Iteration 151/1000 | Loss: 0.00001012
Iteration 152/1000 | Loss: 0.00001012
Iteration 153/1000 | Loss: 0.00001012
Iteration 154/1000 | Loss: 0.00001012
Iteration 155/1000 | Loss: 0.00001012
Iteration 156/1000 | Loss: 0.00001012
Iteration 157/1000 | Loss: 0.00001012
Iteration 158/1000 | Loss: 0.00001012
Iteration 159/1000 | Loss: 0.00001012
Iteration 160/1000 | Loss: 0.00001012
Iteration 161/1000 | Loss: 0.00001012
Iteration 162/1000 | Loss: 0.00001012
Iteration 163/1000 | Loss: 0.00001012
Iteration 164/1000 | Loss: 0.00001012
Iteration 165/1000 | Loss: 0.00001012
Iteration 166/1000 | Loss: 0.00001012
Iteration 167/1000 | Loss: 0.00001012
Iteration 168/1000 | Loss: 0.00001012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.0120184924744535e-05, 1.0120184924744535e-05, 1.0120184924744535e-05, 1.0120184924744535e-05, 1.0120184924744535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0120184924744535e-05

Optimization complete. Final v2v error: 2.653968334197998 mm

Highest mean error: 2.933425188064575 mm for frame 95

Lowest mean error: 2.335419178009033 mm for frame 33

Saving results

Total time: 33.762616872787476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00525799
Iteration 2/25 | Loss: 0.00125690
Iteration 3/25 | Loss: 0.00108157
Iteration 4/25 | Loss: 0.00107255
Iteration 5/25 | Loss: 0.00107065
Iteration 6/25 | Loss: 0.00107022
Iteration 7/25 | Loss: 0.00107022
Iteration 8/25 | Loss: 0.00107021
Iteration 9/25 | Loss: 0.00107021
Iteration 10/25 | Loss: 0.00107021
Iteration 11/25 | Loss: 0.00107021
Iteration 12/25 | Loss: 0.00107021
Iteration 13/25 | Loss: 0.00107021
Iteration 14/25 | Loss: 0.00107021
Iteration 15/25 | Loss: 0.00107021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010702103609219193, 0.0010702103609219193, 0.0010702103609219193, 0.0010702103609219193, 0.0010702103609219193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010702103609219193

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32454300
Iteration 2/25 | Loss: 0.00156142
Iteration 3/25 | Loss: 0.00156142
Iteration 4/25 | Loss: 0.00156142
Iteration 5/25 | Loss: 0.00156142
Iteration 6/25 | Loss: 0.00156142
Iteration 7/25 | Loss: 0.00156142
Iteration 8/25 | Loss: 0.00156142
Iteration 9/25 | Loss: 0.00156142
Iteration 10/25 | Loss: 0.00156141
Iteration 11/25 | Loss: 0.00156141
Iteration 12/25 | Loss: 0.00156141
Iteration 13/25 | Loss: 0.00156142
Iteration 14/25 | Loss: 0.00156142
Iteration 15/25 | Loss: 0.00156142
Iteration 16/25 | Loss: 0.00156142
Iteration 17/25 | Loss: 0.00156141
Iteration 18/25 | Loss: 0.00156141
Iteration 19/25 | Loss: 0.00156141
Iteration 20/25 | Loss: 0.00156141
Iteration 21/25 | Loss: 0.00156142
Iteration 22/25 | Loss: 0.00156142
Iteration 23/25 | Loss: 0.00156142
Iteration 24/25 | Loss: 0.00156142
Iteration 25/25 | Loss: 0.00156142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156141
Iteration 2/1000 | Loss: 0.00004206
Iteration 3/1000 | Loss: 0.00002941
Iteration 4/1000 | Loss: 0.00002293
Iteration 5/1000 | Loss: 0.00002022
Iteration 6/1000 | Loss: 0.00001888
Iteration 7/1000 | Loss: 0.00001815
Iteration 8/1000 | Loss: 0.00001768
Iteration 9/1000 | Loss: 0.00001722
Iteration 10/1000 | Loss: 0.00001695
Iteration 11/1000 | Loss: 0.00001673
Iteration 12/1000 | Loss: 0.00001660
Iteration 13/1000 | Loss: 0.00001642
Iteration 14/1000 | Loss: 0.00001632
Iteration 15/1000 | Loss: 0.00001626
Iteration 16/1000 | Loss: 0.00001624
Iteration 17/1000 | Loss: 0.00001622
Iteration 18/1000 | Loss: 0.00001622
Iteration 19/1000 | Loss: 0.00001621
Iteration 20/1000 | Loss: 0.00001621
Iteration 21/1000 | Loss: 0.00001621
Iteration 22/1000 | Loss: 0.00001621
Iteration 23/1000 | Loss: 0.00001621
Iteration 24/1000 | Loss: 0.00001620
Iteration 25/1000 | Loss: 0.00001620
Iteration 26/1000 | Loss: 0.00001620
Iteration 27/1000 | Loss: 0.00001620
Iteration 28/1000 | Loss: 0.00001620
Iteration 29/1000 | Loss: 0.00001620
Iteration 30/1000 | Loss: 0.00001620
Iteration 31/1000 | Loss: 0.00001619
Iteration 32/1000 | Loss: 0.00001619
Iteration 33/1000 | Loss: 0.00001618
Iteration 34/1000 | Loss: 0.00001618
Iteration 35/1000 | Loss: 0.00001618
Iteration 36/1000 | Loss: 0.00001618
Iteration 37/1000 | Loss: 0.00001617
Iteration 38/1000 | Loss: 0.00001617
Iteration 39/1000 | Loss: 0.00001617
Iteration 40/1000 | Loss: 0.00001617
Iteration 41/1000 | Loss: 0.00001617
Iteration 42/1000 | Loss: 0.00001617
Iteration 43/1000 | Loss: 0.00001617
Iteration 44/1000 | Loss: 0.00001617
Iteration 45/1000 | Loss: 0.00001617
Iteration 46/1000 | Loss: 0.00001616
Iteration 47/1000 | Loss: 0.00001616
Iteration 48/1000 | Loss: 0.00001616
Iteration 49/1000 | Loss: 0.00001615
Iteration 50/1000 | Loss: 0.00001614
Iteration 51/1000 | Loss: 0.00001614
Iteration 52/1000 | Loss: 0.00001614
Iteration 53/1000 | Loss: 0.00001613
Iteration 54/1000 | Loss: 0.00001613
Iteration 55/1000 | Loss: 0.00001613
Iteration 56/1000 | Loss: 0.00001613
Iteration 57/1000 | Loss: 0.00001613
Iteration 58/1000 | Loss: 0.00001613
Iteration 59/1000 | Loss: 0.00001613
Iteration 60/1000 | Loss: 0.00001613
Iteration 61/1000 | Loss: 0.00001613
Iteration 62/1000 | Loss: 0.00001613
Iteration 63/1000 | Loss: 0.00001613
Iteration 64/1000 | Loss: 0.00001613
Iteration 65/1000 | Loss: 0.00001613
Iteration 66/1000 | Loss: 0.00001613
Iteration 67/1000 | Loss: 0.00001612
Iteration 68/1000 | Loss: 0.00001612
Iteration 69/1000 | Loss: 0.00001611
Iteration 70/1000 | Loss: 0.00001611
Iteration 71/1000 | Loss: 0.00001611
Iteration 72/1000 | Loss: 0.00001610
Iteration 73/1000 | Loss: 0.00001610
Iteration 74/1000 | Loss: 0.00001610
Iteration 75/1000 | Loss: 0.00001610
Iteration 76/1000 | Loss: 0.00001610
Iteration 77/1000 | Loss: 0.00001610
Iteration 78/1000 | Loss: 0.00001610
Iteration 79/1000 | Loss: 0.00001610
Iteration 80/1000 | Loss: 0.00001610
Iteration 81/1000 | Loss: 0.00001610
Iteration 82/1000 | Loss: 0.00001610
Iteration 83/1000 | Loss: 0.00001610
Iteration 84/1000 | Loss: 0.00001610
Iteration 85/1000 | Loss: 0.00001610
Iteration 86/1000 | Loss: 0.00001610
Iteration 87/1000 | Loss: 0.00001610
Iteration 88/1000 | Loss: 0.00001610
Iteration 89/1000 | Loss: 0.00001610
Iteration 90/1000 | Loss: 0.00001610
Iteration 91/1000 | Loss: 0.00001610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.6096320905489847e-05, 1.6096320905489847e-05, 1.6096320905489847e-05, 1.6096320905489847e-05, 1.6096320905489847e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6096320905489847e-05

Optimization complete. Final v2v error: 3.347069501876831 mm

Highest mean error: 4.255011081695557 mm for frame 78

Lowest mean error: 2.9214491844177246 mm for frame 13

Saving results

Total time: 33.7156183719635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00672092
Iteration 2/25 | Loss: 0.00123931
Iteration 3/25 | Loss: 0.00106705
Iteration 4/25 | Loss: 0.00104671
Iteration 5/25 | Loss: 0.00104473
Iteration 6/25 | Loss: 0.00104469
Iteration 7/25 | Loss: 0.00104469
Iteration 8/25 | Loss: 0.00104469
Iteration 9/25 | Loss: 0.00104469
Iteration 10/25 | Loss: 0.00104469
Iteration 11/25 | Loss: 0.00104469
Iteration 12/25 | Loss: 0.00104469
Iteration 13/25 | Loss: 0.00104469
Iteration 14/25 | Loss: 0.00104469
Iteration 15/25 | Loss: 0.00104469
Iteration 16/25 | Loss: 0.00104469
Iteration 17/25 | Loss: 0.00104469
Iteration 18/25 | Loss: 0.00104469
Iteration 19/25 | Loss: 0.00104469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010446876985952258, 0.0010446876985952258, 0.0010446876985952258, 0.0010446876985952258, 0.0010446876985952258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010446876985952258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15260601
Iteration 2/25 | Loss: 0.00207201
Iteration 3/25 | Loss: 0.00207201
Iteration 4/25 | Loss: 0.00207201
Iteration 5/25 | Loss: 0.00207201
Iteration 6/25 | Loss: 0.00207201
Iteration 7/25 | Loss: 0.00207201
Iteration 8/25 | Loss: 0.00207201
Iteration 9/25 | Loss: 0.00207201
Iteration 10/25 | Loss: 0.00207201
Iteration 11/25 | Loss: 0.00207201
Iteration 12/25 | Loss: 0.00207201
Iteration 13/25 | Loss: 0.00207201
Iteration 14/25 | Loss: 0.00207201
Iteration 15/25 | Loss: 0.00207201
Iteration 16/25 | Loss: 0.00207201
Iteration 17/25 | Loss: 0.00207201
Iteration 18/25 | Loss: 0.00207201
Iteration 19/25 | Loss: 0.00207201
Iteration 20/25 | Loss: 0.00207201
Iteration 21/25 | Loss: 0.00207201
Iteration 22/25 | Loss: 0.00207201
Iteration 23/25 | Loss: 0.00207201
Iteration 24/25 | Loss: 0.00207201
Iteration 25/25 | Loss: 0.00207201

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207201
Iteration 2/1000 | Loss: 0.00003582
Iteration 3/1000 | Loss: 0.00002375
Iteration 4/1000 | Loss: 0.00001614
Iteration 5/1000 | Loss: 0.00001451
Iteration 6/1000 | Loss: 0.00001343
Iteration 7/1000 | Loss: 0.00001295
Iteration 8/1000 | Loss: 0.00001267
Iteration 9/1000 | Loss: 0.00001235
Iteration 10/1000 | Loss: 0.00001211
Iteration 11/1000 | Loss: 0.00001192
Iteration 12/1000 | Loss: 0.00001186
Iteration 13/1000 | Loss: 0.00001170
Iteration 14/1000 | Loss: 0.00001167
Iteration 15/1000 | Loss: 0.00001163
Iteration 16/1000 | Loss: 0.00001161
Iteration 17/1000 | Loss: 0.00001159
Iteration 18/1000 | Loss: 0.00001158
Iteration 19/1000 | Loss: 0.00001157
Iteration 20/1000 | Loss: 0.00001156
Iteration 21/1000 | Loss: 0.00001155
Iteration 22/1000 | Loss: 0.00001153
Iteration 23/1000 | Loss: 0.00001153
Iteration 24/1000 | Loss: 0.00001152
Iteration 25/1000 | Loss: 0.00001151
Iteration 26/1000 | Loss: 0.00001151
Iteration 27/1000 | Loss: 0.00001150
Iteration 28/1000 | Loss: 0.00001150
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001148
Iteration 31/1000 | Loss: 0.00001147
Iteration 32/1000 | Loss: 0.00001146
Iteration 33/1000 | Loss: 0.00001145
Iteration 34/1000 | Loss: 0.00001144
Iteration 35/1000 | Loss: 0.00001141
Iteration 36/1000 | Loss: 0.00001140
Iteration 37/1000 | Loss: 0.00001138
Iteration 38/1000 | Loss: 0.00001138
Iteration 39/1000 | Loss: 0.00001137
Iteration 40/1000 | Loss: 0.00001137
Iteration 41/1000 | Loss: 0.00001137
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001137
Iteration 44/1000 | Loss: 0.00001136
Iteration 45/1000 | Loss: 0.00001136
Iteration 46/1000 | Loss: 0.00001135
Iteration 47/1000 | Loss: 0.00001135
Iteration 48/1000 | Loss: 0.00001135
Iteration 49/1000 | Loss: 0.00001135
Iteration 50/1000 | Loss: 0.00001135
Iteration 51/1000 | Loss: 0.00001135
Iteration 52/1000 | Loss: 0.00001134
Iteration 53/1000 | Loss: 0.00001134
Iteration 54/1000 | Loss: 0.00001134
Iteration 55/1000 | Loss: 0.00001134
Iteration 56/1000 | Loss: 0.00001134
Iteration 57/1000 | Loss: 0.00001133
Iteration 58/1000 | Loss: 0.00001133
Iteration 59/1000 | Loss: 0.00001133
Iteration 60/1000 | Loss: 0.00001133
Iteration 61/1000 | Loss: 0.00001133
Iteration 62/1000 | Loss: 0.00001132
Iteration 63/1000 | Loss: 0.00001132
Iteration 64/1000 | Loss: 0.00001132
Iteration 65/1000 | Loss: 0.00001131
Iteration 66/1000 | Loss: 0.00001131
Iteration 67/1000 | Loss: 0.00001131
Iteration 68/1000 | Loss: 0.00001130
Iteration 69/1000 | Loss: 0.00001130
Iteration 70/1000 | Loss: 0.00001130
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001129
Iteration 73/1000 | Loss: 0.00001129
Iteration 74/1000 | Loss: 0.00001129
Iteration 75/1000 | Loss: 0.00001129
Iteration 76/1000 | Loss: 0.00001128
Iteration 77/1000 | Loss: 0.00001128
Iteration 78/1000 | Loss: 0.00001128
Iteration 79/1000 | Loss: 0.00001128
Iteration 80/1000 | Loss: 0.00001128
Iteration 81/1000 | Loss: 0.00001127
Iteration 82/1000 | Loss: 0.00001127
Iteration 83/1000 | Loss: 0.00001127
Iteration 84/1000 | Loss: 0.00001127
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001127
Iteration 87/1000 | Loss: 0.00001127
Iteration 88/1000 | Loss: 0.00001127
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001126
Iteration 92/1000 | Loss: 0.00001126
Iteration 93/1000 | Loss: 0.00001126
Iteration 94/1000 | Loss: 0.00001126
Iteration 95/1000 | Loss: 0.00001126
Iteration 96/1000 | Loss: 0.00001125
Iteration 97/1000 | Loss: 0.00001125
Iteration 98/1000 | Loss: 0.00001125
Iteration 99/1000 | Loss: 0.00001125
Iteration 100/1000 | Loss: 0.00001125
Iteration 101/1000 | Loss: 0.00001125
Iteration 102/1000 | Loss: 0.00001125
Iteration 103/1000 | Loss: 0.00001125
Iteration 104/1000 | Loss: 0.00001125
Iteration 105/1000 | Loss: 0.00001125
Iteration 106/1000 | Loss: 0.00001125
Iteration 107/1000 | Loss: 0.00001125
Iteration 108/1000 | Loss: 0.00001124
Iteration 109/1000 | Loss: 0.00001124
Iteration 110/1000 | Loss: 0.00001124
Iteration 111/1000 | Loss: 0.00001124
Iteration 112/1000 | Loss: 0.00001124
Iteration 113/1000 | Loss: 0.00001124
Iteration 114/1000 | Loss: 0.00001124
Iteration 115/1000 | Loss: 0.00001124
Iteration 116/1000 | Loss: 0.00001124
Iteration 117/1000 | Loss: 0.00001123
Iteration 118/1000 | Loss: 0.00001123
Iteration 119/1000 | Loss: 0.00001123
Iteration 120/1000 | Loss: 0.00001123
Iteration 121/1000 | Loss: 0.00001123
Iteration 122/1000 | Loss: 0.00001123
Iteration 123/1000 | Loss: 0.00001123
Iteration 124/1000 | Loss: 0.00001122
Iteration 125/1000 | Loss: 0.00001122
Iteration 126/1000 | Loss: 0.00001122
Iteration 127/1000 | Loss: 0.00001122
Iteration 128/1000 | Loss: 0.00001122
Iteration 129/1000 | Loss: 0.00001122
Iteration 130/1000 | Loss: 0.00001122
Iteration 131/1000 | Loss: 0.00001122
Iteration 132/1000 | Loss: 0.00001122
Iteration 133/1000 | Loss: 0.00001122
Iteration 134/1000 | Loss: 0.00001122
Iteration 135/1000 | Loss: 0.00001122
Iteration 136/1000 | Loss: 0.00001122
Iteration 137/1000 | Loss: 0.00001122
Iteration 138/1000 | Loss: 0.00001122
Iteration 139/1000 | Loss: 0.00001122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.1220667147426866e-05, 1.1220667147426866e-05, 1.1220667147426866e-05, 1.1220667147426866e-05, 1.1220667147426866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1220667147426866e-05

Optimization complete. Final v2v error: 2.805332660675049 mm

Highest mean error: 3.166517972946167 mm for frame 107

Lowest mean error: 2.3000214099884033 mm for frame 56

Saving results

Total time: 36.71308183670044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395434
Iteration 2/25 | Loss: 0.00117694
Iteration 3/25 | Loss: 0.00103710
Iteration 4/25 | Loss: 0.00101011
Iteration 5/25 | Loss: 0.00100388
Iteration 6/25 | Loss: 0.00100186
Iteration 7/25 | Loss: 0.00100186
Iteration 8/25 | Loss: 0.00100186
Iteration 9/25 | Loss: 0.00100186
Iteration 10/25 | Loss: 0.00100186
Iteration 11/25 | Loss: 0.00100186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010018603643402457, 0.0010018603643402457, 0.0010018603643402457, 0.0010018603643402457, 0.0010018603643402457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010018603643402457

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16440272
Iteration 2/25 | Loss: 0.00246266
Iteration 3/25 | Loss: 0.00246266
Iteration 4/25 | Loss: 0.00246266
Iteration 5/25 | Loss: 0.00246266
Iteration 6/25 | Loss: 0.00246266
Iteration 7/25 | Loss: 0.00246266
Iteration 8/25 | Loss: 0.00246266
Iteration 9/25 | Loss: 0.00246266
Iteration 10/25 | Loss: 0.00246266
Iteration 11/25 | Loss: 0.00246266
Iteration 12/25 | Loss: 0.00246266
Iteration 13/25 | Loss: 0.00246266
Iteration 14/25 | Loss: 0.00246266
Iteration 15/25 | Loss: 0.00246266
Iteration 16/25 | Loss: 0.00246266
Iteration 17/25 | Loss: 0.00246266
Iteration 18/25 | Loss: 0.00246266
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002462660428136587, 0.002462660428136587, 0.002462660428136587, 0.002462660428136587, 0.002462660428136587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002462660428136587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00246266
Iteration 2/1000 | Loss: 0.00003697
Iteration 3/1000 | Loss: 0.00002731
Iteration 4/1000 | Loss: 0.00002315
Iteration 5/1000 | Loss: 0.00002134
Iteration 6/1000 | Loss: 0.00002033
Iteration 7/1000 | Loss: 0.00001951
Iteration 8/1000 | Loss: 0.00001902
Iteration 9/1000 | Loss: 0.00001873
Iteration 10/1000 | Loss: 0.00001851
Iteration 11/1000 | Loss: 0.00001844
Iteration 12/1000 | Loss: 0.00001841
Iteration 13/1000 | Loss: 0.00001828
Iteration 14/1000 | Loss: 0.00001825
Iteration 15/1000 | Loss: 0.00001814
Iteration 16/1000 | Loss: 0.00001798
Iteration 17/1000 | Loss: 0.00001795
Iteration 18/1000 | Loss: 0.00001791
Iteration 19/1000 | Loss: 0.00001789
Iteration 20/1000 | Loss: 0.00001788
Iteration 21/1000 | Loss: 0.00001786
Iteration 22/1000 | Loss: 0.00001785
Iteration 23/1000 | Loss: 0.00001785
Iteration 24/1000 | Loss: 0.00001784
Iteration 25/1000 | Loss: 0.00001784
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00001783
Iteration 28/1000 | Loss: 0.00001783
Iteration 29/1000 | Loss: 0.00001782
Iteration 30/1000 | Loss: 0.00001781
Iteration 31/1000 | Loss: 0.00001781
Iteration 32/1000 | Loss: 0.00001781
Iteration 33/1000 | Loss: 0.00001780
Iteration 34/1000 | Loss: 0.00001780
Iteration 35/1000 | Loss: 0.00001779
Iteration 36/1000 | Loss: 0.00001778
Iteration 37/1000 | Loss: 0.00001777
Iteration 38/1000 | Loss: 0.00001777
Iteration 39/1000 | Loss: 0.00001777
Iteration 40/1000 | Loss: 0.00001776
Iteration 41/1000 | Loss: 0.00001776
Iteration 42/1000 | Loss: 0.00001774
Iteration 43/1000 | Loss: 0.00001774
Iteration 44/1000 | Loss: 0.00001773
Iteration 45/1000 | Loss: 0.00001773
Iteration 46/1000 | Loss: 0.00001772
Iteration 47/1000 | Loss: 0.00001770
Iteration 48/1000 | Loss: 0.00001768
Iteration 49/1000 | Loss: 0.00001768
Iteration 50/1000 | Loss: 0.00001767
Iteration 51/1000 | Loss: 0.00001767
Iteration 52/1000 | Loss: 0.00001767
Iteration 53/1000 | Loss: 0.00001767
Iteration 54/1000 | Loss: 0.00001766
Iteration 55/1000 | Loss: 0.00001766
Iteration 56/1000 | Loss: 0.00001766
Iteration 57/1000 | Loss: 0.00001766
Iteration 58/1000 | Loss: 0.00001766
Iteration 59/1000 | Loss: 0.00001765
Iteration 60/1000 | Loss: 0.00001765
Iteration 61/1000 | Loss: 0.00001765
Iteration 62/1000 | Loss: 0.00001764
Iteration 63/1000 | Loss: 0.00001764
Iteration 64/1000 | Loss: 0.00001764
Iteration 65/1000 | Loss: 0.00001764
Iteration 66/1000 | Loss: 0.00001763
Iteration 67/1000 | Loss: 0.00001763
Iteration 68/1000 | Loss: 0.00001763
Iteration 69/1000 | Loss: 0.00001763
Iteration 70/1000 | Loss: 0.00001763
Iteration 71/1000 | Loss: 0.00001763
Iteration 72/1000 | Loss: 0.00001763
Iteration 73/1000 | Loss: 0.00001763
Iteration 74/1000 | Loss: 0.00001763
Iteration 75/1000 | Loss: 0.00001762
Iteration 76/1000 | Loss: 0.00001762
Iteration 77/1000 | Loss: 0.00001762
Iteration 78/1000 | Loss: 0.00001762
Iteration 79/1000 | Loss: 0.00001762
Iteration 80/1000 | Loss: 0.00001762
Iteration 81/1000 | Loss: 0.00001762
Iteration 82/1000 | Loss: 0.00001762
Iteration 83/1000 | Loss: 0.00001762
Iteration 84/1000 | Loss: 0.00001761
Iteration 85/1000 | Loss: 0.00001761
Iteration 86/1000 | Loss: 0.00001761
Iteration 87/1000 | Loss: 0.00001761
Iteration 88/1000 | Loss: 0.00001761
Iteration 89/1000 | Loss: 0.00001761
Iteration 90/1000 | Loss: 0.00001761
Iteration 91/1000 | Loss: 0.00001761
Iteration 92/1000 | Loss: 0.00001761
Iteration 93/1000 | Loss: 0.00001760
Iteration 94/1000 | Loss: 0.00001760
Iteration 95/1000 | Loss: 0.00001760
Iteration 96/1000 | Loss: 0.00001760
Iteration 97/1000 | Loss: 0.00001760
Iteration 98/1000 | Loss: 0.00001760
Iteration 99/1000 | Loss: 0.00001760
Iteration 100/1000 | Loss: 0.00001760
Iteration 101/1000 | Loss: 0.00001760
Iteration 102/1000 | Loss: 0.00001760
Iteration 103/1000 | Loss: 0.00001760
Iteration 104/1000 | Loss: 0.00001760
Iteration 105/1000 | Loss: 0.00001760
Iteration 106/1000 | Loss: 0.00001760
Iteration 107/1000 | Loss: 0.00001760
Iteration 108/1000 | Loss: 0.00001760
Iteration 109/1000 | Loss: 0.00001760
Iteration 110/1000 | Loss: 0.00001760
Iteration 111/1000 | Loss: 0.00001760
Iteration 112/1000 | Loss: 0.00001760
Iteration 113/1000 | Loss: 0.00001760
Iteration 114/1000 | Loss: 0.00001760
Iteration 115/1000 | Loss: 0.00001760
Iteration 116/1000 | Loss: 0.00001760
Iteration 117/1000 | Loss: 0.00001760
Iteration 118/1000 | Loss: 0.00001760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.7595644749235362e-05, 1.7595644749235362e-05, 1.7595644749235362e-05, 1.7595644749235362e-05, 1.7595644749235362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7595644749235362e-05

Optimization complete. Final v2v error: 3.359178066253662 mm

Highest mean error: 3.6894822120666504 mm for frame 82

Lowest mean error: 2.952355146408081 mm for frame 209

Saving results

Total time: 41.631266355514526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451457
Iteration 2/25 | Loss: 0.00108764
Iteration 3/25 | Loss: 0.00099637
Iteration 4/25 | Loss: 0.00097661
Iteration 5/25 | Loss: 0.00097295
Iteration 6/25 | Loss: 0.00097206
Iteration 7/25 | Loss: 0.00097206
Iteration 8/25 | Loss: 0.00097206
Iteration 9/25 | Loss: 0.00097206
Iteration 10/25 | Loss: 0.00097206
Iteration 11/25 | Loss: 0.00097206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009720648522488773, 0.0009720648522488773, 0.0009720648522488773, 0.0009720648522488773, 0.0009720648522488773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009720648522488773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.05813789
Iteration 2/25 | Loss: 0.00187664
Iteration 3/25 | Loss: 0.00187662
Iteration 4/25 | Loss: 0.00187661
Iteration 5/25 | Loss: 0.00187661
Iteration 6/25 | Loss: 0.00187661
Iteration 7/25 | Loss: 0.00187661
Iteration 8/25 | Loss: 0.00187661
Iteration 9/25 | Loss: 0.00187661
Iteration 10/25 | Loss: 0.00187661
Iteration 11/25 | Loss: 0.00187661
Iteration 12/25 | Loss: 0.00187661
Iteration 13/25 | Loss: 0.00187661
Iteration 14/25 | Loss: 0.00187661
Iteration 15/25 | Loss: 0.00187661
Iteration 16/25 | Loss: 0.00187661
Iteration 17/25 | Loss: 0.00187661
Iteration 18/25 | Loss: 0.00187661
Iteration 19/25 | Loss: 0.00187661
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018766121938824654, 0.0018766121938824654, 0.0018766121938824654, 0.0018766121938824654, 0.0018766121938824654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018766121938824654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187661
Iteration 2/1000 | Loss: 0.00003289
Iteration 3/1000 | Loss: 0.00001873
Iteration 4/1000 | Loss: 0.00001488
Iteration 5/1000 | Loss: 0.00001368
Iteration 6/1000 | Loss: 0.00001299
Iteration 7/1000 | Loss: 0.00001246
Iteration 8/1000 | Loss: 0.00001195
Iteration 9/1000 | Loss: 0.00001154
Iteration 10/1000 | Loss: 0.00001121
Iteration 11/1000 | Loss: 0.00001097
Iteration 12/1000 | Loss: 0.00001085
Iteration 13/1000 | Loss: 0.00001084
Iteration 14/1000 | Loss: 0.00001084
Iteration 15/1000 | Loss: 0.00001083
Iteration 16/1000 | Loss: 0.00001075
Iteration 17/1000 | Loss: 0.00001067
Iteration 18/1000 | Loss: 0.00001067
Iteration 19/1000 | Loss: 0.00001063
Iteration 20/1000 | Loss: 0.00001063
Iteration 21/1000 | Loss: 0.00001062
Iteration 22/1000 | Loss: 0.00001062
Iteration 23/1000 | Loss: 0.00001061
Iteration 24/1000 | Loss: 0.00001061
Iteration 25/1000 | Loss: 0.00001060
Iteration 26/1000 | Loss: 0.00001060
Iteration 27/1000 | Loss: 0.00001060
Iteration 28/1000 | Loss: 0.00001057
Iteration 29/1000 | Loss: 0.00001056
Iteration 30/1000 | Loss: 0.00001056
Iteration 31/1000 | Loss: 0.00001056
Iteration 32/1000 | Loss: 0.00001055
Iteration 33/1000 | Loss: 0.00001055
Iteration 34/1000 | Loss: 0.00001055
Iteration 35/1000 | Loss: 0.00001054
Iteration 36/1000 | Loss: 0.00001054
Iteration 37/1000 | Loss: 0.00001054
Iteration 38/1000 | Loss: 0.00001053
Iteration 39/1000 | Loss: 0.00001052
Iteration 40/1000 | Loss: 0.00001052
Iteration 41/1000 | Loss: 0.00001052
Iteration 42/1000 | Loss: 0.00001051
Iteration 43/1000 | Loss: 0.00001051
Iteration 44/1000 | Loss: 0.00001051
Iteration 45/1000 | Loss: 0.00001050
Iteration 46/1000 | Loss: 0.00001050
Iteration 47/1000 | Loss: 0.00001049
Iteration 48/1000 | Loss: 0.00001049
Iteration 49/1000 | Loss: 0.00001049
Iteration 50/1000 | Loss: 0.00001049
Iteration 51/1000 | Loss: 0.00001048
Iteration 52/1000 | Loss: 0.00001048
Iteration 53/1000 | Loss: 0.00001047
Iteration 54/1000 | Loss: 0.00001047
Iteration 55/1000 | Loss: 0.00001045
Iteration 56/1000 | Loss: 0.00001045
Iteration 57/1000 | Loss: 0.00001044
Iteration 58/1000 | Loss: 0.00001044
Iteration 59/1000 | Loss: 0.00001044
Iteration 60/1000 | Loss: 0.00001044
Iteration 61/1000 | Loss: 0.00001044
Iteration 62/1000 | Loss: 0.00001044
Iteration 63/1000 | Loss: 0.00001044
Iteration 64/1000 | Loss: 0.00001044
Iteration 65/1000 | Loss: 0.00001043
Iteration 66/1000 | Loss: 0.00001043
Iteration 67/1000 | Loss: 0.00001043
Iteration 68/1000 | Loss: 0.00001042
Iteration 69/1000 | Loss: 0.00001042
Iteration 70/1000 | Loss: 0.00001041
Iteration 71/1000 | Loss: 0.00001041
Iteration 72/1000 | Loss: 0.00001041
Iteration 73/1000 | Loss: 0.00001041
Iteration 74/1000 | Loss: 0.00001041
Iteration 75/1000 | Loss: 0.00001040
Iteration 76/1000 | Loss: 0.00001040
Iteration 77/1000 | Loss: 0.00001040
Iteration 78/1000 | Loss: 0.00001040
Iteration 79/1000 | Loss: 0.00001040
Iteration 80/1000 | Loss: 0.00001040
Iteration 81/1000 | Loss: 0.00001040
Iteration 82/1000 | Loss: 0.00001040
Iteration 83/1000 | Loss: 0.00001040
Iteration 84/1000 | Loss: 0.00001039
Iteration 85/1000 | Loss: 0.00001039
Iteration 86/1000 | Loss: 0.00001039
Iteration 87/1000 | Loss: 0.00001039
Iteration 88/1000 | Loss: 0.00001039
Iteration 89/1000 | Loss: 0.00001039
Iteration 90/1000 | Loss: 0.00001039
Iteration 91/1000 | Loss: 0.00001039
Iteration 92/1000 | Loss: 0.00001038
Iteration 93/1000 | Loss: 0.00001038
Iteration 94/1000 | Loss: 0.00001038
Iteration 95/1000 | Loss: 0.00001038
Iteration 96/1000 | Loss: 0.00001038
Iteration 97/1000 | Loss: 0.00001038
Iteration 98/1000 | Loss: 0.00001038
Iteration 99/1000 | Loss: 0.00001038
Iteration 100/1000 | Loss: 0.00001038
Iteration 101/1000 | Loss: 0.00001038
Iteration 102/1000 | Loss: 0.00001038
Iteration 103/1000 | Loss: 0.00001038
Iteration 104/1000 | Loss: 0.00001038
Iteration 105/1000 | Loss: 0.00001038
Iteration 106/1000 | Loss: 0.00001038
Iteration 107/1000 | Loss: 0.00001038
Iteration 108/1000 | Loss: 0.00001038
Iteration 109/1000 | Loss: 0.00001038
Iteration 110/1000 | Loss: 0.00001038
Iteration 111/1000 | Loss: 0.00001038
Iteration 112/1000 | Loss: 0.00001038
Iteration 113/1000 | Loss: 0.00001038
Iteration 114/1000 | Loss: 0.00001038
Iteration 115/1000 | Loss: 0.00001038
Iteration 116/1000 | Loss: 0.00001038
Iteration 117/1000 | Loss: 0.00001038
Iteration 118/1000 | Loss: 0.00001038
Iteration 119/1000 | Loss: 0.00001038
Iteration 120/1000 | Loss: 0.00001038
Iteration 121/1000 | Loss: 0.00001038
Iteration 122/1000 | Loss: 0.00001038
Iteration 123/1000 | Loss: 0.00001038
Iteration 124/1000 | Loss: 0.00001038
Iteration 125/1000 | Loss: 0.00001038
Iteration 126/1000 | Loss: 0.00001038
Iteration 127/1000 | Loss: 0.00001038
Iteration 128/1000 | Loss: 0.00001038
Iteration 129/1000 | Loss: 0.00001038
Iteration 130/1000 | Loss: 0.00001038
Iteration 131/1000 | Loss: 0.00001038
Iteration 132/1000 | Loss: 0.00001038
Iteration 133/1000 | Loss: 0.00001038
Iteration 134/1000 | Loss: 0.00001038
Iteration 135/1000 | Loss: 0.00001038
Iteration 136/1000 | Loss: 0.00001038
Iteration 137/1000 | Loss: 0.00001038
Iteration 138/1000 | Loss: 0.00001038
Iteration 139/1000 | Loss: 0.00001038
Iteration 140/1000 | Loss: 0.00001038
Iteration 141/1000 | Loss: 0.00001038
Iteration 142/1000 | Loss: 0.00001038
Iteration 143/1000 | Loss: 0.00001038
Iteration 144/1000 | Loss: 0.00001038
Iteration 145/1000 | Loss: 0.00001038
Iteration 146/1000 | Loss: 0.00001038
Iteration 147/1000 | Loss: 0.00001038
Iteration 148/1000 | Loss: 0.00001038
Iteration 149/1000 | Loss: 0.00001038
Iteration 150/1000 | Loss: 0.00001038
Iteration 151/1000 | Loss: 0.00001038
Iteration 152/1000 | Loss: 0.00001038
Iteration 153/1000 | Loss: 0.00001038
Iteration 154/1000 | Loss: 0.00001038
Iteration 155/1000 | Loss: 0.00001038
Iteration 156/1000 | Loss: 0.00001038
Iteration 157/1000 | Loss: 0.00001038
Iteration 158/1000 | Loss: 0.00001038
Iteration 159/1000 | Loss: 0.00001038
Iteration 160/1000 | Loss: 0.00001038
Iteration 161/1000 | Loss: 0.00001038
Iteration 162/1000 | Loss: 0.00001038
Iteration 163/1000 | Loss: 0.00001038
Iteration 164/1000 | Loss: 0.00001038
Iteration 165/1000 | Loss: 0.00001038
Iteration 166/1000 | Loss: 0.00001038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.0382542313891463e-05, 1.0382542313891463e-05, 1.0382542313891463e-05, 1.0382542313891463e-05, 1.0382542313891463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0382542313891463e-05

Optimization complete. Final v2v error: 2.7071573734283447 mm

Highest mean error: 3.200887680053711 mm for frame 67

Lowest mean error: 2.2507646083831787 mm for frame 13

Saving results

Total time: 40.72685503959656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00702506
Iteration 2/25 | Loss: 0.00146795
Iteration 3/25 | Loss: 0.00114639
Iteration 4/25 | Loss: 0.00101857
Iteration 5/25 | Loss: 0.00100151
Iteration 6/25 | Loss: 0.00098741
Iteration 7/25 | Loss: 0.00098271
Iteration 8/25 | Loss: 0.00098180
Iteration 9/25 | Loss: 0.00098024
Iteration 10/25 | Loss: 0.00097932
Iteration 11/25 | Loss: 0.00097908
Iteration 12/25 | Loss: 0.00098023
Iteration 13/25 | Loss: 0.00097995
Iteration 14/25 | Loss: 0.00097912
Iteration 15/25 | Loss: 0.00097891
Iteration 16/25 | Loss: 0.00097879
Iteration 17/25 | Loss: 0.00097874
Iteration 18/25 | Loss: 0.00097873
Iteration 19/25 | Loss: 0.00097873
Iteration 20/25 | Loss: 0.00097873
Iteration 21/25 | Loss: 0.00097873
Iteration 22/25 | Loss: 0.00097873
Iteration 23/25 | Loss: 0.00097873
Iteration 24/25 | Loss: 0.00097873
Iteration 25/25 | Loss: 0.00097873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54362655
Iteration 2/25 | Loss: 0.00203640
Iteration 3/25 | Loss: 0.00203640
Iteration 4/25 | Loss: 0.00203640
Iteration 5/25 | Loss: 0.00203640
Iteration 6/25 | Loss: 0.00203640
Iteration 7/25 | Loss: 0.00203640
Iteration 8/25 | Loss: 0.00203640
Iteration 9/25 | Loss: 0.00203639
Iteration 10/25 | Loss: 0.00203639
Iteration 11/25 | Loss: 0.00203639
Iteration 12/25 | Loss: 0.00203639
Iteration 13/25 | Loss: 0.00203639
Iteration 14/25 | Loss: 0.00203639
Iteration 15/25 | Loss: 0.00203639
Iteration 16/25 | Loss: 0.00203639
Iteration 17/25 | Loss: 0.00203639
Iteration 18/25 | Loss: 0.00203639
Iteration 19/25 | Loss: 0.00203639
Iteration 20/25 | Loss: 0.00203639
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002036393852904439, 0.002036393852904439, 0.002036393852904439, 0.002036393852904439, 0.002036393852904439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002036393852904439

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203639
Iteration 2/1000 | Loss: 0.00002912
Iteration 3/1000 | Loss: 0.00001521
Iteration 4/1000 | Loss: 0.00001367
Iteration 5/1000 | Loss: 0.00001281
Iteration 6/1000 | Loss: 0.00001239
Iteration 7/1000 | Loss: 0.00001194
Iteration 8/1000 | Loss: 0.00001169
Iteration 9/1000 | Loss: 0.00011610
Iteration 10/1000 | Loss: 0.00001143
Iteration 11/1000 | Loss: 0.00001115
Iteration 12/1000 | Loss: 0.00003570
Iteration 13/1000 | Loss: 0.00002486
Iteration 14/1000 | Loss: 0.00001519
Iteration 15/1000 | Loss: 0.00001339
Iteration 16/1000 | Loss: 0.00003241
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00003330
Iteration 19/1000 | Loss: 0.00001745
Iteration 20/1000 | Loss: 0.00003362
Iteration 21/1000 | Loss: 0.00001818
Iteration 22/1000 | Loss: 0.00003401
Iteration 23/1000 | Loss: 0.00002073
Iteration 24/1000 | Loss: 0.00003331
Iteration 25/1000 | Loss: 0.00002341
Iteration 26/1000 | Loss: 0.00003339
Iteration 27/1000 | Loss: 0.00002883
Iteration 28/1000 | Loss: 0.00003258
Iteration 29/1000 | Loss: 0.00002984
Iteration 30/1000 | Loss: 0.00001929
Iteration 31/1000 | Loss: 0.00001335
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001168
Iteration 34/1000 | Loss: 0.00002610
Iteration 35/1000 | Loss: 0.00002637
Iteration 36/1000 | Loss: 0.00002925
Iteration 37/1000 | Loss: 0.00001112
Iteration 38/1000 | Loss: 0.00001085
Iteration 39/1000 | Loss: 0.00001078
Iteration 40/1000 | Loss: 0.00001078
Iteration 41/1000 | Loss: 0.00001078
Iteration 42/1000 | Loss: 0.00001077
Iteration 43/1000 | Loss: 0.00001076
Iteration 44/1000 | Loss: 0.00001076
Iteration 45/1000 | Loss: 0.00001074
Iteration 46/1000 | Loss: 0.00001071
Iteration 47/1000 | Loss: 0.00001070
Iteration 48/1000 | Loss: 0.00001070
Iteration 49/1000 | Loss: 0.00001069
Iteration 50/1000 | Loss: 0.00001068
Iteration 51/1000 | Loss: 0.00001066
Iteration 52/1000 | Loss: 0.00001065
Iteration 53/1000 | Loss: 0.00001064
Iteration 54/1000 | Loss: 0.00001064
Iteration 55/1000 | Loss: 0.00001064
Iteration 56/1000 | Loss: 0.00001060
Iteration 57/1000 | Loss: 0.00001057
Iteration 58/1000 | Loss: 0.00001057
Iteration 59/1000 | Loss: 0.00001057
Iteration 60/1000 | Loss: 0.00001056
Iteration 61/1000 | Loss: 0.00001056
Iteration 62/1000 | Loss: 0.00001056
Iteration 63/1000 | Loss: 0.00001056
Iteration 64/1000 | Loss: 0.00001056
Iteration 65/1000 | Loss: 0.00001056
Iteration 66/1000 | Loss: 0.00001056
Iteration 67/1000 | Loss: 0.00001056
Iteration 68/1000 | Loss: 0.00001056
Iteration 69/1000 | Loss: 0.00001056
Iteration 70/1000 | Loss: 0.00001055
Iteration 71/1000 | Loss: 0.00001055
Iteration 72/1000 | Loss: 0.00001055
Iteration 73/1000 | Loss: 0.00001055
Iteration 74/1000 | Loss: 0.00001055
Iteration 75/1000 | Loss: 0.00001055
Iteration 76/1000 | Loss: 0.00001054
Iteration 77/1000 | Loss: 0.00001054
Iteration 78/1000 | Loss: 0.00001053
Iteration 79/1000 | Loss: 0.00001053
Iteration 80/1000 | Loss: 0.00001052
Iteration 81/1000 | Loss: 0.00001052
Iteration 82/1000 | Loss: 0.00001052
Iteration 83/1000 | Loss: 0.00001051
Iteration 84/1000 | Loss: 0.00001051
Iteration 85/1000 | Loss: 0.00001051
Iteration 86/1000 | Loss: 0.00001050
Iteration 87/1000 | Loss: 0.00001050
Iteration 88/1000 | Loss: 0.00001050
Iteration 89/1000 | Loss: 0.00001050
Iteration 90/1000 | Loss: 0.00001049
Iteration 91/1000 | Loss: 0.00001049
Iteration 92/1000 | Loss: 0.00001049
Iteration 93/1000 | Loss: 0.00001049
Iteration 94/1000 | Loss: 0.00001048
Iteration 95/1000 | Loss: 0.00001048
Iteration 96/1000 | Loss: 0.00001048
Iteration 97/1000 | Loss: 0.00001048
Iteration 98/1000 | Loss: 0.00001048
Iteration 99/1000 | Loss: 0.00001047
Iteration 100/1000 | Loss: 0.00001047
Iteration 101/1000 | Loss: 0.00001047
Iteration 102/1000 | Loss: 0.00001044
Iteration 103/1000 | Loss: 0.00001044
Iteration 104/1000 | Loss: 0.00001044
Iteration 105/1000 | Loss: 0.00001044
Iteration 106/1000 | Loss: 0.00001044
Iteration 107/1000 | Loss: 0.00001044
Iteration 108/1000 | Loss: 0.00001044
Iteration 109/1000 | Loss: 0.00001043
Iteration 110/1000 | Loss: 0.00001043
Iteration 111/1000 | Loss: 0.00001043
Iteration 112/1000 | Loss: 0.00001043
Iteration 113/1000 | Loss: 0.00001043
Iteration 114/1000 | Loss: 0.00001043
Iteration 115/1000 | Loss: 0.00001042
Iteration 116/1000 | Loss: 0.00001042
Iteration 117/1000 | Loss: 0.00001042
Iteration 118/1000 | Loss: 0.00001042
Iteration 119/1000 | Loss: 0.00001042
Iteration 120/1000 | Loss: 0.00001042
Iteration 121/1000 | Loss: 0.00001042
Iteration 122/1000 | Loss: 0.00001042
Iteration 123/1000 | Loss: 0.00001041
Iteration 124/1000 | Loss: 0.00001041
Iteration 125/1000 | Loss: 0.00001041
Iteration 126/1000 | Loss: 0.00001041
Iteration 127/1000 | Loss: 0.00001041
Iteration 128/1000 | Loss: 0.00001041
Iteration 129/1000 | Loss: 0.00001041
Iteration 130/1000 | Loss: 0.00001041
Iteration 131/1000 | Loss: 0.00001041
Iteration 132/1000 | Loss: 0.00001041
Iteration 133/1000 | Loss: 0.00001041
Iteration 134/1000 | Loss: 0.00001041
Iteration 135/1000 | Loss: 0.00001041
Iteration 136/1000 | Loss: 0.00001041
Iteration 137/1000 | Loss: 0.00001041
Iteration 138/1000 | Loss: 0.00001041
Iteration 139/1000 | Loss: 0.00001041
Iteration 140/1000 | Loss: 0.00001041
Iteration 141/1000 | Loss: 0.00001041
Iteration 142/1000 | Loss: 0.00001041
Iteration 143/1000 | Loss: 0.00001041
Iteration 144/1000 | Loss: 0.00001041
Iteration 145/1000 | Loss: 0.00001040
Iteration 146/1000 | Loss: 0.00001040
Iteration 147/1000 | Loss: 0.00001040
Iteration 148/1000 | Loss: 0.00001040
Iteration 149/1000 | Loss: 0.00001040
Iteration 150/1000 | Loss: 0.00001040
Iteration 151/1000 | Loss: 0.00001040
Iteration 152/1000 | Loss: 0.00001040
Iteration 153/1000 | Loss: 0.00001040
Iteration 154/1000 | Loss: 0.00001040
Iteration 155/1000 | Loss: 0.00001040
Iteration 156/1000 | Loss: 0.00001039
Iteration 157/1000 | Loss: 0.00001039
Iteration 158/1000 | Loss: 0.00001039
Iteration 159/1000 | Loss: 0.00001039
Iteration 160/1000 | Loss: 0.00001039
Iteration 161/1000 | Loss: 0.00001039
Iteration 162/1000 | Loss: 0.00001039
Iteration 163/1000 | Loss: 0.00001039
Iteration 164/1000 | Loss: 0.00001039
Iteration 165/1000 | Loss: 0.00001039
Iteration 166/1000 | Loss: 0.00001039
Iteration 167/1000 | Loss: 0.00001039
Iteration 168/1000 | Loss: 0.00001039
Iteration 169/1000 | Loss: 0.00001039
Iteration 170/1000 | Loss: 0.00001039
Iteration 171/1000 | Loss: 0.00001038
Iteration 172/1000 | Loss: 0.00001038
Iteration 173/1000 | Loss: 0.00001038
Iteration 174/1000 | Loss: 0.00001038
Iteration 175/1000 | Loss: 0.00001037
Iteration 176/1000 | Loss: 0.00001037
Iteration 177/1000 | Loss: 0.00001037
Iteration 178/1000 | Loss: 0.00001037
Iteration 179/1000 | Loss: 0.00001037
Iteration 180/1000 | Loss: 0.00001037
Iteration 181/1000 | Loss: 0.00001037
Iteration 182/1000 | Loss: 0.00001037
Iteration 183/1000 | Loss: 0.00001037
Iteration 184/1000 | Loss: 0.00001037
Iteration 185/1000 | Loss: 0.00001037
Iteration 186/1000 | Loss: 0.00001037
Iteration 187/1000 | Loss: 0.00001037
Iteration 188/1000 | Loss: 0.00001037
Iteration 189/1000 | Loss: 0.00001037
Iteration 190/1000 | Loss: 0.00001037
Iteration 191/1000 | Loss: 0.00001037
Iteration 192/1000 | Loss: 0.00001037
Iteration 193/1000 | Loss: 0.00001037
Iteration 194/1000 | Loss: 0.00001037
Iteration 195/1000 | Loss: 0.00001037
Iteration 196/1000 | Loss: 0.00001037
Iteration 197/1000 | Loss: 0.00001037
Iteration 198/1000 | Loss: 0.00001037
Iteration 199/1000 | Loss: 0.00001037
Iteration 200/1000 | Loss: 0.00001037
Iteration 201/1000 | Loss: 0.00001037
Iteration 202/1000 | Loss: 0.00001037
Iteration 203/1000 | Loss: 0.00001037
Iteration 204/1000 | Loss: 0.00001037
Iteration 205/1000 | Loss: 0.00001037
Iteration 206/1000 | Loss: 0.00001037
Iteration 207/1000 | Loss: 0.00001037
Iteration 208/1000 | Loss: 0.00001037
Iteration 209/1000 | Loss: 0.00001037
Iteration 210/1000 | Loss: 0.00001037
Iteration 211/1000 | Loss: 0.00001037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.036702815326862e-05, 1.036702815326862e-05, 1.036702815326862e-05, 1.036702815326862e-05, 1.036702815326862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.036702815326862e-05

Optimization complete. Final v2v error: 2.672276735305786 mm

Highest mean error: 9.102899551391602 mm for frame 106

Lowest mean error: 2.2303290367126465 mm for frame 40

Saving results

Total time: 94.3949499130249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412309
Iteration 2/25 | Loss: 0.00133739
Iteration 3/25 | Loss: 0.00107716
Iteration 4/25 | Loss: 0.00103576
Iteration 5/25 | Loss: 0.00103032
Iteration 6/25 | Loss: 0.00102865
Iteration 7/25 | Loss: 0.00102782
Iteration 8/25 | Loss: 0.00102782
Iteration 9/25 | Loss: 0.00102782
Iteration 10/25 | Loss: 0.00102782
Iteration 11/25 | Loss: 0.00102782
Iteration 12/25 | Loss: 0.00102782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010278167901560664, 0.0010278167901560664, 0.0010278167901560664, 0.0010278167901560664, 0.0010278167901560664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010278167901560664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21601176
Iteration 2/25 | Loss: 0.00163022
Iteration 3/25 | Loss: 0.00163022
Iteration 4/25 | Loss: 0.00163022
Iteration 5/25 | Loss: 0.00163022
Iteration 6/25 | Loss: 0.00163022
Iteration 7/25 | Loss: 0.00163021
Iteration 8/25 | Loss: 0.00163021
Iteration 9/25 | Loss: 0.00163021
Iteration 10/25 | Loss: 0.00163021
Iteration 11/25 | Loss: 0.00163021
Iteration 12/25 | Loss: 0.00163021
Iteration 13/25 | Loss: 0.00163021
Iteration 14/25 | Loss: 0.00163021
Iteration 15/25 | Loss: 0.00163021
Iteration 16/25 | Loss: 0.00163021
Iteration 17/25 | Loss: 0.00163021
Iteration 18/25 | Loss: 0.00163021
Iteration 19/25 | Loss: 0.00163021
Iteration 20/25 | Loss: 0.00163021
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016302139265462756, 0.0016302139265462756, 0.0016302139265462756, 0.0016302139265462756, 0.0016302139265462756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016302139265462756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163021
Iteration 2/1000 | Loss: 0.00002883
Iteration 3/1000 | Loss: 0.00002138
Iteration 4/1000 | Loss: 0.00001985
Iteration 5/1000 | Loss: 0.00001896
Iteration 6/1000 | Loss: 0.00001822
Iteration 7/1000 | Loss: 0.00001766
Iteration 8/1000 | Loss: 0.00001721
Iteration 9/1000 | Loss: 0.00001691
Iteration 10/1000 | Loss: 0.00001662
Iteration 11/1000 | Loss: 0.00001640
Iteration 12/1000 | Loss: 0.00001637
Iteration 13/1000 | Loss: 0.00001635
Iteration 14/1000 | Loss: 0.00001634
Iteration 15/1000 | Loss: 0.00001630
Iteration 16/1000 | Loss: 0.00001623
Iteration 17/1000 | Loss: 0.00001623
Iteration 18/1000 | Loss: 0.00001621
Iteration 19/1000 | Loss: 0.00001621
Iteration 20/1000 | Loss: 0.00001620
Iteration 21/1000 | Loss: 0.00001617
Iteration 22/1000 | Loss: 0.00001616
Iteration 23/1000 | Loss: 0.00001616
Iteration 24/1000 | Loss: 0.00001615
Iteration 25/1000 | Loss: 0.00001615
Iteration 26/1000 | Loss: 0.00001614
Iteration 27/1000 | Loss: 0.00001614
Iteration 28/1000 | Loss: 0.00001614
Iteration 29/1000 | Loss: 0.00001613
Iteration 30/1000 | Loss: 0.00001613
Iteration 31/1000 | Loss: 0.00001613
Iteration 32/1000 | Loss: 0.00001613
Iteration 33/1000 | Loss: 0.00001613
Iteration 34/1000 | Loss: 0.00001613
Iteration 35/1000 | Loss: 0.00001613
Iteration 36/1000 | Loss: 0.00001613
Iteration 37/1000 | Loss: 0.00001613
Iteration 38/1000 | Loss: 0.00001613
Iteration 39/1000 | Loss: 0.00001612
Iteration 40/1000 | Loss: 0.00001612
Iteration 41/1000 | Loss: 0.00001612
Iteration 42/1000 | Loss: 0.00001612
Iteration 43/1000 | Loss: 0.00001611
Iteration 44/1000 | Loss: 0.00001611
Iteration 45/1000 | Loss: 0.00001611
Iteration 46/1000 | Loss: 0.00001611
Iteration 47/1000 | Loss: 0.00001611
Iteration 48/1000 | Loss: 0.00001611
Iteration 49/1000 | Loss: 0.00001611
Iteration 50/1000 | Loss: 0.00001610
Iteration 51/1000 | Loss: 0.00001610
Iteration 52/1000 | Loss: 0.00001610
Iteration 53/1000 | Loss: 0.00001610
Iteration 54/1000 | Loss: 0.00001610
Iteration 55/1000 | Loss: 0.00001610
Iteration 56/1000 | Loss: 0.00001610
Iteration 57/1000 | Loss: 0.00001610
Iteration 58/1000 | Loss: 0.00001610
Iteration 59/1000 | Loss: 0.00001610
Iteration 60/1000 | Loss: 0.00001609
Iteration 61/1000 | Loss: 0.00001609
Iteration 62/1000 | Loss: 0.00001609
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00001609
Iteration 65/1000 | Loss: 0.00001609
Iteration 66/1000 | Loss: 0.00001609
Iteration 67/1000 | Loss: 0.00001609
Iteration 68/1000 | Loss: 0.00001609
Iteration 69/1000 | Loss: 0.00001609
Iteration 70/1000 | Loss: 0.00001609
Iteration 71/1000 | Loss: 0.00001608
Iteration 72/1000 | Loss: 0.00001608
Iteration 73/1000 | Loss: 0.00001608
Iteration 74/1000 | Loss: 0.00001608
Iteration 75/1000 | Loss: 0.00001608
Iteration 76/1000 | Loss: 0.00001608
Iteration 77/1000 | Loss: 0.00001608
Iteration 78/1000 | Loss: 0.00001608
Iteration 79/1000 | Loss: 0.00001608
Iteration 80/1000 | Loss: 0.00001608
Iteration 81/1000 | Loss: 0.00001607
Iteration 82/1000 | Loss: 0.00001607
Iteration 83/1000 | Loss: 0.00001607
Iteration 84/1000 | Loss: 0.00001607
Iteration 85/1000 | Loss: 0.00001607
Iteration 86/1000 | Loss: 0.00001607
Iteration 87/1000 | Loss: 0.00001607
Iteration 88/1000 | Loss: 0.00001607
Iteration 89/1000 | Loss: 0.00001607
Iteration 90/1000 | Loss: 0.00001606
Iteration 91/1000 | Loss: 0.00001606
Iteration 92/1000 | Loss: 0.00001606
Iteration 93/1000 | Loss: 0.00001606
Iteration 94/1000 | Loss: 0.00001606
Iteration 95/1000 | Loss: 0.00001606
Iteration 96/1000 | Loss: 0.00001606
Iteration 97/1000 | Loss: 0.00001606
Iteration 98/1000 | Loss: 0.00001606
Iteration 99/1000 | Loss: 0.00001606
Iteration 100/1000 | Loss: 0.00001606
Iteration 101/1000 | Loss: 0.00001606
Iteration 102/1000 | Loss: 0.00001606
Iteration 103/1000 | Loss: 0.00001606
Iteration 104/1000 | Loss: 0.00001606
Iteration 105/1000 | Loss: 0.00001606
Iteration 106/1000 | Loss: 0.00001606
Iteration 107/1000 | Loss: 0.00001606
Iteration 108/1000 | Loss: 0.00001605
Iteration 109/1000 | Loss: 0.00001605
Iteration 110/1000 | Loss: 0.00001605
Iteration 111/1000 | Loss: 0.00001605
Iteration 112/1000 | Loss: 0.00001605
Iteration 113/1000 | Loss: 0.00001605
Iteration 114/1000 | Loss: 0.00001605
Iteration 115/1000 | Loss: 0.00001605
Iteration 116/1000 | Loss: 0.00001604
Iteration 117/1000 | Loss: 0.00001604
Iteration 118/1000 | Loss: 0.00001604
Iteration 119/1000 | Loss: 0.00001604
Iteration 120/1000 | Loss: 0.00001604
Iteration 121/1000 | Loss: 0.00001604
Iteration 122/1000 | Loss: 0.00001604
Iteration 123/1000 | Loss: 0.00001604
Iteration 124/1000 | Loss: 0.00001604
Iteration 125/1000 | Loss: 0.00001604
Iteration 126/1000 | Loss: 0.00001604
Iteration 127/1000 | Loss: 0.00001604
Iteration 128/1000 | Loss: 0.00001604
Iteration 129/1000 | Loss: 0.00001604
Iteration 130/1000 | Loss: 0.00001604
Iteration 131/1000 | Loss: 0.00001604
Iteration 132/1000 | Loss: 0.00001604
Iteration 133/1000 | Loss: 0.00001604
Iteration 134/1000 | Loss: 0.00001604
Iteration 135/1000 | Loss: 0.00001604
Iteration 136/1000 | Loss: 0.00001604
Iteration 137/1000 | Loss: 0.00001604
Iteration 138/1000 | Loss: 0.00001604
Iteration 139/1000 | Loss: 0.00001603
Iteration 140/1000 | Loss: 0.00001603
Iteration 141/1000 | Loss: 0.00001603
Iteration 142/1000 | Loss: 0.00001603
Iteration 143/1000 | Loss: 0.00001603
Iteration 144/1000 | Loss: 0.00001603
Iteration 145/1000 | Loss: 0.00001603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.6034813597798347e-05, 1.6034813597798347e-05, 1.6034813597798347e-05, 1.6034813597798347e-05, 1.6034813597798347e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6034813597798347e-05

Optimization complete. Final v2v error: 3.2869231700897217 mm

Highest mean error: 3.617769956588745 mm for frame 61

Lowest mean error: 2.8608884811401367 mm for frame 0

Saving results

Total time: 39.5692093372345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840457
Iteration 2/25 | Loss: 0.00108965
Iteration 3/25 | Loss: 0.00096013
Iteration 4/25 | Loss: 0.00094458
Iteration 5/25 | Loss: 0.00094241
Iteration 6/25 | Loss: 0.00094224
Iteration 7/25 | Loss: 0.00094224
Iteration 8/25 | Loss: 0.00094224
Iteration 9/25 | Loss: 0.00094224
Iteration 10/25 | Loss: 0.00094224
Iteration 11/25 | Loss: 0.00094224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000942241691518575, 0.000942241691518575, 0.000942241691518575, 0.000942241691518575, 0.000942241691518575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000942241691518575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21392763
Iteration 2/25 | Loss: 0.00193518
Iteration 3/25 | Loss: 0.00193517
Iteration 4/25 | Loss: 0.00193517
Iteration 5/25 | Loss: 0.00193517
Iteration 6/25 | Loss: 0.00193517
Iteration 7/25 | Loss: 0.00193517
Iteration 8/25 | Loss: 0.00193517
Iteration 9/25 | Loss: 0.00193517
Iteration 10/25 | Loss: 0.00193517
Iteration 11/25 | Loss: 0.00193517
Iteration 12/25 | Loss: 0.00193517
Iteration 13/25 | Loss: 0.00193517
Iteration 14/25 | Loss: 0.00193517
Iteration 15/25 | Loss: 0.00193517
Iteration 16/25 | Loss: 0.00193517
Iteration 17/25 | Loss: 0.00193517
Iteration 18/25 | Loss: 0.00193517
Iteration 19/25 | Loss: 0.00193517
Iteration 20/25 | Loss: 0.00193517
Iteration 21/25 | Loss: 0.00193517
Iteration 22/25 | Loss: 0.00193517
Iteration 23/25 | Loss: 0.00193517
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0019351706141605973, 0.0019351706141605973, 0.0019351706141605973, 0.0019351706141605973, 0.0019351706141605973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019351706141605973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193517
Iteration 2/1000 | Loss: 0.00002223
Iteration 3/1000 | Loss: 0.00001019
Iteration 4/1000 | Loss: 0.00000881
Iteration 5/1000 | Loss: 0.00000818
Iteration 6/1000 | Loss: 0.00000776
Iteration 7/1000 | Loss: 0.00000733
Iteration 8/1000 | Loss: 0.00000711
Iteration 9/1000 | Loss: 0.00000697
Iteration 10/1000 | Loss: 0.00000694
Iteration 11/1000 | Loss: 0.00000682
Iteration 12/1000 | Loss: 0.00000682
Iteration 13/1000 | Loss: 0.00000682
Iteration 14/1000 | Loss: 0.00000681
Iteration 15/1000 | Loss: 0.00000680
Iteration 16/1000 | Loss: 0.00000679
Iteration 17/1000 | Loss: 0.00000678
Iteration 18/1000 | Loss: 0.00000678
Iteration 19/1000 | Loss: 0.00000677
Iteration 20/1000 | Loss: 0.00000677
Iteration 21/1000 | Loss: 0.00000677
Iteration 22/1000 | Loss: 0.00000677
Iteration 23/1000 | Loss: 0.00000676
Iteration 24/1000 | Loss: 0.00000673
Iteration 25/1000 | Loss: 0.00000673
Iteration 26/1000 | Loss: 0.00000673
Iteration 27/1000 | Loss: 0.00000673
Iteration 28/1000 | Loss: 0.00000673
Iteration 29/1000 | Loss: 0.00000673
Iteration 30/1000 | Loss: 0.00000672
Iteration 31/1000 | Loss: 0.00000672
Iteration 32/1000 | Loss: 0.00000672
Iteration 33/1000 | Loss: 0.00000672
Iteration 34/1000 | Loss: 0.00000672
Iteration 35/1000 | Loss: 0.00000672
Iteration 36/1000 | Loss: 0.00000672
Iteration 37/1000 | Loss: 0.00000671
Iteration 38/1000 | Loss: 0.00000670
Iteration 39/1000 | Loss: 0.00000670
Iteration 40/1000 | Loss: 0.00000669
Iteration 41/1000 | Loss: 0.00000669
Iteration 42/1000 | Loss: 0.00000669
Iteration 43/1000 | Loss: 0.00000669
Iteration 44/1000 | Loss: 0.00000669
Iteration 45/1000 | Loss: 0.00000669
Iteration 46/1000 | Loss: 0.00000668
Iteration 47/1000 | Loss: 0.00000668
Iteration 48/1000 | Loss: 0.00000668
Iteration 49/1000 | Loss: 0.00000665
Iteration 50/1000 | Loss: 0.00000665
Iteration 51/1000 | Loss: 0.00000665
Iteration 52/1000 | Loss: 0.00000665
Iteration 53/1000 | Loss: 0.00000664
Iteration 54/1000 | Loss: 0.00000664
Iteration 55/1000 | Loss: 0.00000664
Iteration 56/1000 | Loss: 0.00000663
Iteration 57/1000 | Loss: 0.00000659
Iteration 58/1000 | Loss: 0.00000659
Iteration 59/1000 | Loss: 0.00000658
Iteration 60/1000 | Loss: 0.00000657
Iteration 61/1000 | Loss: 0.00000655
Iteration 62/1000 | Loss: 0.00000655
Iteration 63/1000 | Loss: 0.00000655
Iteration 64/1000 | Loss: 0.00000655
Iteration 65/1000 | Loss: 0.00000655
Iteration 66/1000 | Loss: 0.00000654
Iteration 67/1000 | Loss: 0.00000654
Iteration 68/1000 | Loss: 0.00000654
Iteration 69/1000 | Loss: 0.00000654
Iteration 70/1000 | Loss: 0.00000654
Iteration 71/1000 | Loss: 0.00000653
Iteration 72/1000 | Loss: 0.00000653
Iteration 73/1000 | Loss: 0.00000652
Iteration 74/1000 | Loss: 0.00000652
Iteration 75/1000 | Loss: 0.00000651
Iteration 76/1000 | Loss: 0.00000651
Iteration 77/1000 | Loss: 0.00000650
Iteration 78/1000 | Loss: 0.00000650
Iteration 79/1000 | Loss: 0.00000650
Iteration 80/1000 | Loss: 0.00000650
Iteration 81/1000 | Loss: 0.00000649
Iteration 82/1000 | Loss: 0.00000648
Iteration 83/1000 | Loss: 0.00000648
Iteration 84/1000 | Loss: 0.00000648
Iteration 85/1000 | Loss: 0.00000648
Iteration 86/1000 | Loss: 0.00000648
Iteration 87/1000 | Loss: 0.00000648
Iteration 88/1000 | Loss: 0.00000647
Iteration 89/1000 | Loss: 0.00000647
Iteration 90/1000 | Loss: 0.00000646
Iteration 91/1000 | Loss: 0.00000646
Iteration 92/1000 | Loss: 0.00000646
Iteration 93/1000 | Loss: 0.00000646
Iteration 94/1000 | Loss: 0.00000646
Iteration 95/1000 | Loss: 0.00000646
Iteration 96/1000 | Loss: 0.00000646
Iteration 97/1000 | Loss: 0.00000646
Iteration 98/1000 | Loss: 0.00000646
Iteration 99/1000 | Loss: 0.00000646
Iteration 100/1000 | Loss: 0.00000646
Iteration 101/1000 | Loss: 0.00000645
Iteration 102/1000 | Loss: 0.00000645
Iteration 103/1000 | Loss: 0.00000645
Iteration 104/1000 | Loss: 0.00000644
Iteration 105/1000 | Loss: 0.00000644
Iteration 106/1000 | Loss: 0.00000644
Iteration 107/1000 | Loss: 0.00000644
Iteration 108/1000 | Loss: 0.00000644
Iteration 109/1000 | Loss: 0.00000644
Iteration 110/1000 | Loss: 0.00000644
Iteration 111/1000 | Loss: 0.00000644
Iteration 112/1000 | Loss: 0.00000644
Iteration 113/1000 | Loss: 0.00000644
Iteration 114/1000 | Loss: 0.00000644
Iteration 115/1000 | Loss: 0.00000644
Iteration 116/1000 | Loss: 0.00000644
Iteration 117/1000 | Loss: 0.00000643
Iteration 118/1000 | Loss: 0.00000643
Iteration 119/1000 | Loss: 0.00000643
Iteration 120/1000 | Loss: 0.00000643
Iteration 121/1000 | Loss: 0.00000643
Iteration 122/1000 | Loss: 0.00000643
Iteration 123/1000 | Loss: 0.00000642
Iteration 124/1000 | Loss: 0.00000642
Iteration 125/1000 | Loss: 0.00000642
Iteration 126/1000 | Loss: 0.00000642
Iteration 127/1000 | Loss: 0.00000642
Iteration 128/1000 | Loss: 0.00000642
Iteration 129/1000 | Loss: 0.00000642
Iteration 130/1000 | Loss: 0.00000642
Iteration 131/1000 | Loss: 0.00000642
Iteration 132/1000 | Loss: 0.00000642
Iteration 133/1000 | Loss: 0.00000641
Iteration 134/1000 | Loss: 0.00000641
Iteration 135/1000 | Loss: 0.00000641
Iteration 136/1000 | Loss: 0.00000641
Iteration 137/1000 | Loss: 0.00000641
Iteration 138/1000 | Loss: 0.00000641
Iteration 139/1000 | Loss: 0.00000641
Iteration 140/1000 | Loss: 0.00000641
Iteration 141/1000 | Loss: 0.00000640
Iteration 142/1000 | Loss: 0.00000640
Iteration 143/1000 | Loss: 0.00000640
Iteration 144/1000 | Loss: 0.00000640
Iteration 145/1000 | Loss: 0.00000640
Iteration 146/1000 | Loss: 0.00000640
Iteration 147/1000 | Loss: 0.00000640
Iteration 148/1000 | Loss: 0.00000640
Iteration 149/1000 | Loss: 0.00000640
Iteration 150/1000 | Loss: 0.00000640
Iteration 151/1000 | Loss: 0.00000640
Iteration 152/1000 | Loss: 0.00000640
Iteration 153/1000 | Loss: 0.00000640
Iteration 154/1000 | Loss: 0.00000640
Iteration 155/1000 | Loss: 0.00000640
Iteration 156/1000 | Loss: 0.00000640
Iteration 157/1000 | Loss: 0.00000640
Iteration 158/1000 | Loss: 0.00000639
Iteration 159/1000 | Loss: 0.00000639
Iteration 160/1000 | Loss: 0.00000639
Iteration 161/1000 | Loss: 0.00000639
Iteration 162/1000 | Loss: 0.00000639
Iteration 163/1000 | Loss: 0.00000639
Iteration 164/1000 | Loss: 0.00000639
Iteration 165/1000 | Loss: 0.00000639
Iteration 166/1000 | Loss: 0.00000639
Iteration 167/1000 | Loss: 0.00000639
Iteration 168/1000 | Loss: 0.00000639
Iteration 169/1000 | Loss: 0.00000638
Iteration 170/1000 | Loss: 0.00000638
Iteration 171/1000 | Loss: 0.00000638
Iteration 172/1000 | Loss: 0.00000638
Iteration 173/1000 | Loss: 0.00000638
Iteration 174/1000 | Loss: 0.00000638
Iteration 175/1000 | Loss: 0.00000638
Iteration 176/1000 | Loss: 0.00000638
Iteration 177/1000 | Loss: 0.00000638
Iteration 178/1000 | Loss: 0.00000638
Iteration 179/1000 | Loss: 0.00000638
Iteration 180/1000 | Loss: 0.00000638
Iteration 181/1000 | Loss: 0.00000638
Iteration 182/1000 | Loss: 0.00000638
Iteration 183/1000 | Loss: 0.00000638
Iteration 184/1000 | Loss: 0.00000638
Iteration 185/1000 | Loss: 0.00000638
Iteration 186/1000 | Loss: 0.00000638
Iteration 187/1000 | Loss: 0.00000638
Iteration 188/1000 | Loss: 0.00000637
Iteration 189/1000 | Loss: 0.00000637
Iteration 190/1000 | Loss: 0.00000637
Iteration 191/1000 | Loss: 0.00000637
Iteration 192/1000 | Loss: 0.00000637
Iteration 193/1000 | Loss: 0.00000636
Iteration 194/1000 | Loss: 0.00000636
Iteration 195/1000 | Loss: 0.00000636
Iteration 196/1000 | Loss: 0.00000636
Iteration 197/1000 | Loss: 0.00000636
Iteration 198/1000 | Loss: 0.00000636
Iteration 199/1000 | Loss: 0.00000636
Iteration 200/1000 | Loss: 0.00000636
Iteration 201/1000 | Loss: 0.00000636
Iteration 202/1000 | Loss: 0.00000636
Iteration 203/1000 | Loss: 0.00000636
Iteration 204/1000 | Loss: 0.00000636
Iteration 205/1000 | Loss: 0.00000636
Iteration 206/1000 | Loss: 0.00000636
Iteration 207/1000 | Loss: 0.00000636
Iteration 208/1000 | Loss: 0.00000636
Iteration 209/1000 | Loss: 0.00000636
Iteration 210/1000 | Loss: 0.00000636
Iteration 211/1000 | Loss: 0.00000636
Iteration 212/1000 | Loss: 0.00000636
Iteration 213/1000 | Loss: 0.00000636
Iteration 214/1000 | Loss: 0.00000636
Iteration 215/1000 | Loss: 0.00000635
Iteration 216/1000 | Loss: 0.00000635
Iteration 217/1000 | Loss: 0.00000635
Iteration 218/1000 | Loss: 0.00000635
Iteration 219/1000 | Loss: 0.00000635
Iteration 220/1000 | Loss: 0.00000635
Iteration 221/1000 | Loss: 0.00000635
Iteration 222/1000 | Loss: 0.00000635
Iteration 223/1000 | Loss: 0.00000635
Iteration 224/1000 | Loss: 0.00000635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [6.351420779537875e-06, 6.351420779537875e-06, 6.351420779537875e-06, 6.351420779537875e-06, 6.351420779537875e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.351420779537875e-06

Optimization complete. Final v2v error: 2.1264803409576416 mm

Highest mean error: 2.34371280670166 mm for frame 43

Lowest mean error: 1.9544423818588257 mm for frame 2

Saving results

Total time: 37.451921701431274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408437
Iteration 2/25 | Loss: 0.00104966
Iteration 3/25 | Loss: 0.00097867
Iteration 4/25 | Loss: 0.00096023
Iteration 5/25 | Loss: 0.00095418
Iteration 6/25 | Loss: 0.00095221
Iteration 7/25 | Loss: 0.00095207
Iteration 8/25 | Loss: 0.00095207
Iteration 9/25 | Loss: 0.00095207
Iteration 10/25 | Loss: 0.00095207
Iteration 11/25 | Loss: 0.00095207
Iteration 12/25 | Loss: 0.00095207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009520694147795439, 0.0009520694147795439, 0.0009520694147795439, 0.0009520694147795439, 0.0009520694147795439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009520694147795439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28501606
Iteration 2/25 | Loss: 0.00194258
Iteration 3/25 | Loss: 0.00194258
Iteration 4/25 | Loss: 0.00194258
Iteration 5/25 | Loss: 0.00194258
Iteration 6/25 | Loss: 0.00194258
Iteration 7/25 | Loss: 0.00194258
Iteration 8/25 | Loss: 0.00194258
Iteration 9/25 | Loss: 0.00194258
Iteration 10/25 | Loss: 0.00194258
Iteration 11/25 | Loss: 0.00194258
Iteration 12/25 | Loss: 0.00194258
Iteration 13/25 | Loss: 0.00194258
Iteration 14/25 | Loss: 0.00194258
Iteration 15/25 | Loss: 0.00194258
Iteration 16/25 | Loss: 0.00194258
Iteration 17/25 | Loss: 0.00194258
Iteration 18/25 | Loss: 0.00194258
Iteration 19/25 | Loss: 0.00194258
Iteration 20/25 | Loss: 0.00194258
Iteration 21/25 | Loss: 0.00194258
Iteration 22/25 | Loss: 0.00194258
Iteration 23/25 | Loss: 0.00194258
Iteration 24/25 | Loss: 0.00194258
Iteration 25/25 | Loss: 0.00194258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194258
Iteration 2/1000 | Loss: 0.00002067
Iteration 3/1000 | Loss: 0.00001237
Iteration 4/1000 | Loss: 0.00001137
Iteration 5/1000 | Loss: 0.00001062
Iteration 6/1000 | Loss: 0.00001013
Iteration 7/1000 | Loss: 0.00000963
Iteration 8/1000 | Loss: 0.00000948
Iteration 9/1000 | Loss: 0.00000945
Iteration 10/1000 | Loss: 0.00000938
Iteration 11/1000 | Loss: 0.00000938
Iteration 12/1000 | Loss: 0.00000927
Iteration 13/1000 | Loss: 0.00000915
Iteration 14/1000 | Loss: 0.00000914
Iteration 15/1000 | Loss: 0.00000914
Iteration 16/1000 | Loss: 0.00000913
Iteration 17/1000 | Loss: 0.00000912
Iteration 18/1000 | Loss: 0.00000912
Iteration 19/1000 | Loss: 0.00000911
Iteration 20/1000 | Loss: 0.00000904
Iteration 21/1000 | Loss: 0.00000903
Iteration 22/1000 | Loss: 0.00000902
Iteration 23/1000 | Loss: 0.00000901
Iteration 24/1000 | Loss: 0.00000901
Iteration 25/1000 | Loss: 0.00000898
Iteration 26/1000 | Loss: 0.00000897
Iteration 27/1000 | Loss: 0.00000897
Iteration 28/1000 | Loss: 0.00000897
Iteration 29/1000 | Loss: 0.00000895
Iteration 30/1000 | Loss: 0.00000894
Iteration 31/1000 | Loss: 0.00000894
Iteration 32/1000 | Loss: 0.00000893
Iteration 33/1000 | Loss: 0.00000893
Iteration 34/1000 | Loss: 0.00000888
Iteration 35/1000 | Loss: 0.00000886
Iteration 36/1000 | Loss: 0.00000885
Iteration 37/1000 | Loss: 0.00000884
Iteration 38/1000 | Loss: 0.00000884
Iteration 39/1000 | Loss: 0.00000883
Iteration 40/1000 | Loss: 0.00000882
Iteration 41/1000 | Loss: 0.00000881
Iteration 42/1000 | Loss: 0.00000881
Iteration 43/1000 | Loss: 0.00000881
Iteration 44/1000 | Loss: 0.00000881
Iteration 45/1000 | Loss: 0.00000881
Iteration 46/1000 | Loss: 0.00000881
Iteration 47/1000 | Loss: 0.00000881
Iteration 48/1000 | Loss: 0.00000881
Iteration 49/1000 | Loss: 0.00000880
Iteration 50/1000 | Loss: 0.00000880
Iteration 51/1000 | Loss: 0.00000880
Iteration 52/1000 | Loss: 0.00000879
Iteration 53/1000 | Loss: 0.00000879
Iteration 54/1000 | Loss: 0.00000878
Iteration 55/1000 | Loss: 0.00000878
Iteration 56/1000 | Loss: 0.00000878
Iteration 57/1000 | Loss: 0.00000878
Iteration 58/1000 | Loss: 0.00000877
Iteration 59/1000 | Loss: 0.00000877
Iteration 60/1000 | Loss: 0.00000877
Iteration 61/1000 | Loss: 0.00000877
Iteration 62/1000 | Loss: 0.00000877
Iteration 63/1000 | Loss: 0.00000876
Iteration 64/1000 | Loss: 0.00000876
Iteration 65/1000 | Loss: 0.00000876
Iteration 66/1000 | Loss: 0.00000875
Iteration 67/1000 | Loss: 0.00000875
Iteration 68/1000 | Loss: 0.00000875
Iteration 69/1000 | Loss: 0.00000875
Iteration 70/1000 | Loss: 0.00000875
Iteration 71/1000 | Loss: 0.00000875
Iteration 72/1000 | Loss: 0.00000875
Iteration 73/1000 | Loss: 0.00000875
Iteration 74/1000 | Loss: 0.00000875
Iteration 75/1000 | Loss: 0.00000875
Iteration 76/1000 | Loss: 0.00000874
Iteration 77/1000 | Loss: 0.00000874
Iteration 78/1000 | Loss: 0.00000874
Iteration 79/1000 | Loss: 0.00000874
Iteration 80/1000 | Loss: 0.00000874
Iteration 81/1000 | Loss: 0.00000874
Iteration 82/1000 | Loss: 0.00000874
Iteration 83/1000 | Loss: 0.00000874
Iteration 84/1000 | Loss: 0.00000874
Iteration 85/1000 | Loss: 0.00000874
Iteration 86/1000 | Loss: 0.00000874
Iteration 87/1000 | Loss: 0.00000874
Iteration 88/1000 | Loss: 0.00000874
Iteration 89/1000 | Loss: 0.00000874
Iteration 90/1000 | Loss: 0.00000874
Iteration 91/1000 | Loss: 0.00000874
Iteration 92/1000 | Loss: 0.00000873
Iteration 93/1000 | Loss: 0.00000873
Iteration 94/1000 | Loss: 0.00000873
Iteration 95/1000 | Loss: 0.00000873
Iteration 96/1000 | Loss: 0.00000873
Iteration 97/1000 | Loss: 0.00000873
Iteration 98/1000 | Loss: 0.00000873
Iteration 99/1000 | Loss: 0.00000873
Iteration 100/1000 | Loss: 0.00000873
Iteration 101/1000 | Loss: 0.00000873
Iteration 102/1000 | Loss: 0.00000873
Iteration 103/1000 | Loss: 0.00000873
Iteration 104/1000 | Loss: 0.00000873
Iteration 105/1000 | Loss: 0.00000873
Iteration 106/1000 | Loss: 0.00000873
Iteration 107/1000 | Loss: 0.00000873
Iteration 108/1000 | Loss: 0.00000873
Iteration 109/1000 | Loss: 0.00000872
Iteration 110/1000 | Loss: 0.00000872
Iteration 111/1000 | Loss: 0.00000872
Iteration 112/1000 | Loss: 0.00000872
Iteration 113/1000 | Loss: 0.00000872
Iteration 114/1000 | Loss: 0.00000872
Iteration 115/1000 | Loss: 0.00000872
Iteration 116/1000 | Loss: 0.00000872
Iteration 117/1000 | Loss: 0.00000872
Iteration 118/1000 | Loss: 0.00000872
Iteration 119/1000 | Loss: 0.00000872
Iteration 120/1000 | Loss: 0.00000872
Iteration 121/1000 | Loss: 0.00000872
Iteration 122/1000 | Loss: 0.00000872
Iteration 123/1000 | Loss: 0.00000872
Iteration 124/1000 | Loss: 0.00000872
Iteration 125/1000 | Loss: 0.00000872
Iteration 126/1000 | Loss: 0.00000872
Iteration 127/1000 | Loss: 0.00000872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [8.716581760381814e-06, 8.716581760381814e-06, 8.716581760381814e-06, 8.716581760381814e-06, 8.716581760381814e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.716581760381814e-06

Optimization complete. Final v2v error: 2.5071535110473633 mm

Highest mean error: 2.747713565826416 mm for frame 80

Lowest mean error: 2.2148468494415283 mm for frame 103

Saving results

Total time: 31.552552461624146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00582422
Iteration 2/25 | Loss: 0.00139855
Iteration 3/25 | Loss: 0.00115207
Iteration 4/25 | Loss: 0.00112504
Iteration 5/25 | Loss: 0.00111897
Iteration 6/25 | Loss: 0.00111712
Iteration 7/25 | Loss: 0.00111637
Iteration 8/25 | Loss: 0.00111598
Iteration 9/25 | Loss: 0.00111589
Iteration 10/25 | Loss: 0.00111589
Iteration 11/25 | Loss: 0.00111589
Iteration 12/25 | Loss: 0.00111589
Iteration 13/25 | Loss: 0.00111589
Iteration 14/25 | Loss: 0.00111589
Iteration 15/25 | Loss: 0.00111589
Iteration 16/25 | Loss: 0.00111589
Iteration 17/25 | Loss: 0.00111589
Iteration 18/25 | Loss: 0.00111589
Iteration 19/25 | Loss: 0.00111589
Iteration 20/25 | Loss: 0.00111589
Iteration 21/25 | Loss: 0.00111589
Iteration 22/25 | Loss: 0.00111589
Iteration 23/25 | Loss: 0.00111589
Iteration 24/25 | Loss: 0.00111589
Iteration 25/25 | Loss: 0.00111589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12525451
Iteration 2/25 | Loss: 0.00231461
Iteration 3/25 | Loss: 0.00231458
Iteration 4/25 | Loss: 0.00231458
Iteration 5/25 | Loss: 0.00231458
Iteration 6/25 | Loss: 0.00231458
Iteration 7/25 | Loss: 0.00231458
Iteration 8/25 | Loss: 0.00231458
Iteration 9/25 | Loss: 0.00231458
Iteration 10/25 | Loss: 0.00231458
Iteration 11/25 | Loss: 0.00231458
Iteration 12/25 | Loss: 0.00231458
Iteration 13/25 | Loss: 0.00231458
Iteration 14/25 | Loss: 0.00231458
Iteration 15/25 | Loss: 0.00231458
Iteration 16/25 | Loss: 0.00231458
Iteration 17/25 | Loss: 0.00231458
Iteration 18/25 | Loss: 0.00231458
Iteration 19/25 | Loss: 0.00231458
Iteration 20/25 | Loss: 0.00231458
Iteration 21/25 | Loss: 0.00231458
Iteration 22/25 | Loss: 0.00231458
Iteration 23/25 | Loss: 0.00231458
Iteration 24/25 | Loss: 0.00231458
Iteration 25/25 | Loss: 0.00231458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231458
Iteration 2/1000 | Loss: 0.00014883
Iteration 3/1000 | Loss: 0.00009090
Iteration 4/1000 | Loss: 0.00007589
Iteration 5/1000 | Loss: 0.00006854
Iteration 6/1000 | Loss: 0.00006392
Iteration 7/1000 | Loss: 0.00006146
Iteration 8/1000 | Loss: 0.00049341
Iteration 9/1000 | Loss: 0.00025820
Iteration 10/1000 | Loss: 0.00045188
Iteration 11/1000 | Loss: 0.00006644
Iteration 12/1000 | Loss: 0.00006199
Iteration 13/1000 | Loss: 0.00005787
Iteration 14/1000 | Loss: 0.00005572
Iteration 15/1000 | Loss: 0.00038447
Iteration 16/1000 | Loss: 0.00015703
Iteration 17/1000 | Loss: 0.00067375
Iteration 18/1000 | Loss: 0.00030426
Iteration 19/1000 | Loss: 0.00025226
Iteration 20/1000 | Loss: 0.00006417
Iteration 21/1000 | Loss: 0.00005623
Iteration 22/1000 | Loss: 0.00030612
Iteration 23/1000 | Loss: 0.00005186
Iteration 24/1000 | Loss: 0.00035034
Iteration 25/1000 | Loss: 0.00011164
Iteration 26/1000 | Loss: 0.00004623
Iteration 27/1000 | Loss: 0.00086848
Iteration 28/1000 | Loss: 0.00013084
Iteration 29/1000 | Loss: 0.00005155
Iteration 30/1000 | Loss: 0.00004585
Iteration 31/1000 | Loss: 0.00004286
Iteration 32/1000 | Loss: 0.00004053
Iteration 33/1000 | Loss: 0.00003891
Iteration 34/1000 | Loss: 0.00003808
Iteration 35/1000 | Loss: 0.00003719
Iteration 36/1000 | Loss: 0.00028657
Iteration 37/1000 | Loss: 0.00004418
Iteration 38/1000 | Loss: 0.00004016
Iteration 39/1000 | Loss: 0.00003782
Iteration 40/1000 | Loss: 0.00003664
Iteration 41/1000 | Loss: 0.00003503
Iteration 42/1000 | Loss: 0.00003445
Iteration 43/1000 | Loss: 0.00003395
Iteration 44/1000 | Loss: 0.00003376
Iteration 45/1000 | Loss: 0.00003358
Iteration 46/1000 | Loss: 0.00003351
Iteration 47/1000 | Loss: 0.00003349
Iteration 48/1000 | Loss: 0.00003342
Iteration 49/1000 | Loss: 0.00003338
Iteration 50/1000 | Loss: 0.00003336
Iteration 51/1000 | Loss: 0.00003335
Iteration 52/1000 | Loss: 0.00003334
Iteration 53/1000 | Loss: 0.00003333
Iteration 54/1000 | Loss: 0.00003329
Iteration 55/1000 | Loss: 0.00003328
Iteration 56/1000 | Loss: 0.00003327
Iteration 57/1000 | Loss: 0.00003327
Iteration 58/1000 | Loss: 0.00003327
Iteration 59/1000 | Loss: 0.00003327
Iteration 60/1000 | Loss: 0.00003326
Iteration 61/1000 | Loss: 0.00003326
Iteration 62/1000 | Loss: 0.00003326
Iteration 63/1000 | Loss: 0.00003326
Iteration 64/1000 | Loss: 0.00003326
Iteration 65/1000 | Loss: 0.00003326
Iteration 66/1000 | Loss: 0.00003326
Iteration 67/1000 | Loss: 0.00003324
Iteration 68/1000 | Loss: 0.00003324
Iteration 69/1000 | Loss: 0.00003324
Iteration 70/1000 | Loss: 0.00003324
Iteration 71/1000 | Loss: 0.00003324
Iteration 72/1000 | Loss: 0.00003324
Iteration 73/1000 | Loss: 0.00003324
Iteration 74/1000 | Loss: 0.00003323
Iteration 75/1000 | Loss: 0.00003323
Iteration 76/1000 | Loss: 0.00003323
Iteration 77/1000 | Loss: 0.00003323
Iteration 78/1000 | Loss: 0.00003322
Iteration 79/1000 | Loss: 0.00003320
Iteration 80/1000 | Loss: 0.00003320
Iteration 81/1000 | Loss: 0.00003320
Iteration 82/1000 | Loss: 0.00003317
Iteration 83/1000 | Loss: 0.00003317
Iteration 84/1000 | Loss: 0.00003317
Iteration 85/1000 | Loss: 0.00003316
Iteration 86/1000 | Loss: 0.00003316
Iteration 87/1000 | Loss: 0.00003316
Iteration 88/1000 | Loss: 0.00003316
Iteration 89/1000 | Loss: 0.00003315
Iteration 90/1000 | Loss: 0.00003315
Iteration 91/1000 | Loss: 0.00003315
Iteration 92/1000 | Loss: 0.00003315
Iteration 93/1000 | Loss: 0.00003314
Iteration 94/1000 | Loss: 0.00003314
Iteration 95/1000 | Loss: 0.00003314
Iteration 96/1000 | Loss: 0.00003314
Iteration 97/1000 | Loss: 0.00003313
Iteration 98/1000 | Loss: 0.00003313
Iteration 99/1000 | Loss: 0.00003313
Iteration 100/1000 | Loss: 0.00003313
Iteration 101/1000 | Loss: 0.00003313
Iteration 102/1000 | Loss: 0.00003313
Iteration 103/1000 | Loss: 0.00003313
Iteration 104/1000 | Loss: 0.00003312
Iteration 105/1000 | Loss: 0.00003312
Iteration 106/1000 | Loss: 0.00003312
Iteration 107/1000 | Loss: 0.00003311
Iteration 108/1000 | Loss: 0.00003311
Iteration 109/1000 | Loss: 0.00003311
Iteration 110/1000 | Loss: 0.00003310
Iteration 111/1000 | Loss: 0.00003310
Iteration 112/1000 | Loss: 0.00003310
Iteration 113/1000 | Loss: 0.00003310
Iteration 114/1000 | Loss: 0.00003310
Iteration 115/1000 | Loss: 0.00003310
Iteration 116/1000 | Loss: 0.00003310
Iteration 117/1000 | Loss: 0.00003309
Iteration 118/1000 | Loss: 0.00003309
Iteration 119/1000 | Loss: 0.00003309
Iteration 120/1000 | Loss: 0.00003309
Iteration 121/1000 | Loss: 0.00003309
Iteration 122/1000 | Loss: 0.00003308
Iteration 123/1000 | Loss: 0.00003308
Iteration 124/1000 | Loss: 0.00003308
Iteration 125/1000 | Loss: 0.00003308
Iteration 126/1000 | Loss: 0.00003308
Iteration 127/1000 | Loss: 0.00003308
Iteration 128/1000 | Loss: 0.00003308
Iteration 129/1000 | Loss: 0.00003308
Iteration 130/1000 | Loss: 0.00003308
Iteration 131/1000 | Loss: 0.00003308
Iteration 132/1000 | Loss: 0.00003308
Iteration 133/1000 | Loss: 0.00003308
Iteration 134/1000 | Loss: 0.00003308
Iteration 135/1000 | Loss: 0.00003308
Iteration 136/1000 | Loss: 0.00003308
Iteration 137/1000 | Loss: 0.00003308
Iteration 138/1000 | Loss: 0.00003308
Iteration 139/1000 | Loss: 0.00003307
Iteration 140/1000 | Loss: 0.00003307
Iteration 141/1000 | Loss: 0.00003307
Iteration 142/1000 | Loss: 0.00003307
Iteration 143/1000 | Loss: 0.00003307
Iteration 144/1000 | Loss: 0.00003306
Iteration 145/1000 | Loss: 0.00003306
Iteration 146/1000 | Loss: 0.00003306
Iteration 147/1000 | Loss: 0.00003306
Iteration 148/1000 | Loss: 0.00003306
Iteration 149/1000 | Loss: 0.00003306
Iteration 150/1000 | Loss: 0.00003305
Iteration 151/1000 | Loss: 0.00003305
Iteration 152/1000 | Loss: 0.00003305
Iteration 153/1000 | Loss: 0.00003305
Iteration 154/1000 | Loss: 0.00003305
Iteration 155/1000 | Loss: 0.00003305
Iteration 156/1000 | Loss: 0.00003305
Iteration 157/1000 | Loss: 0.00003304
Iteration 158/1000 | Loss: 0.00003304
Iteration 159/1000 | Loss: 0.00003304
Iteration 160/1000 | Loss: 0.00003304
Iteration 161/1000 | Loss: 0.00003304
Iteration 162/1000 | Loss: 0.00003304
Iteration 163/1000 | Loss: 0.00003304
Iteration 164/1000 | Loss: 0.00003304
Iteration 165/1000 | Loss: 0.00003304
Iteration 166/1000 | Loss: 0.00003304
Iteration 167/1000 | Loss: 0.00003303
Iteration 168/1000 | Loss: 0.00003303
Iteration 169/1000 | Loss: 0.00003303
Iteration 170/1000 | Loss: 0.00003303
Iteration 171/1000 | Loss: 0.00003303
Iteration 172/1000 | Loss: 0.00003303
Iteration 173/1000 | Loss: 0.00003303
Iteration 174/1000 | Loss: 0.00003303
Iteration 175/1000 | Loss: 0.00003303
Iteration 176/1000 | Loss: 0.00003302
Iteration 177/1000 | Loss: 0.00003302
Iteration 178/1000 | Loss: 0.00003302
Iteration 179/1000 | Loss: 0.00003302
Iteration 180/1000 | Loss: 0.00003302
Iteration 181/1000 | Loss: 0.00003302
Iteration 182/1000 | Loss: 0.00003302
Iteration 183/1000 | Loss: 0.00003302
Iteration 184/1000 | Loss: 0.00003302
Iteration 185/1000 | Loss: 0.00003301
Iteration 186/1000 | Loss: 0.00003301
Iteration 187/1000 | Loss: 0.00003301
Iteration 188/1000 | Loss: 0.00003301
Iteration 189/1000 | Loss: 0.00003301
Iteration 190/1000 | Loss: 0.00003301
Iteration 191/1000 | Loss: 0.00003300
Iteration 192/1000 | Loss: 0.00003300
Iteration 193/1000 | Loss: 0.00003300
Iteration 194/1000 | Loss: 0.00003300
Iteration 195/1000 | Loss: 0.00003300
Iteration 196/1000 | Loss: 0.00003300
Iteration 197/1000 | Loss: 0.00003300
Iteration 198/1000 | Loss: 0.00003300
Iteration 199/1000 | Loss: 0.00003300
Iteration 200/1000 | Loss: 0.00003300
Iteration 201/1000 | Loss: 0.00003300
Iteration 202/1000 | Loss: 0.00003300
Iteration 203/1000 | Loss: 0.00003299
Iteration 204/1000 | Loss: 0.00003299
Iteration 205/1000 | Loss: 0.00003299
Iteration 206/1000 | Loss: 0.00003299
Iteration 207/1000 | Loss: 0.00003299
Iteration 208/1000 | Loss: 0.00003299
Iteration 209/1000 | Loss: 0.00003299
Iteration 210/1000 | Loss: 0.00003299
Iteration 211/1000 | Loss: 0.00003299
Iteration 212/1000 | Loss: 0.00003299
Iteration 213/1000 | Loss: 0.00003299
Iteration 214/1000 | Loss: 0.00003299
Iteration 215/1000 | Loss: 0.00003299
Iteration 216/1000 | Loss: 0.00003299
Iteration 217/1000 | Loss: 0.00003299
Iteration 218/1000 | Loss: 0.00003299
Iteration 219/1000 | Loss: 0.00003299
Iteration 220/1000 | Loss: 0.00003299
Iteration 221/1000 | Loss: 0.00003298
Iteration 222/1000 | Loss: 0.00003298
Iteration 223/1000 | Loss: 0.00003298
Iteration 224/1000 | Loss: 0.00003298
Iteration 225/1000 | Loss: 0.00003298
Iteration 226/1000 | Loss: 0.00003298
Iteration 227/1000 | Loss: 0.00003298
Iteration 228/1000 | Loss: 0.00003298
Iteration 229/1000 | Loss: 0.00003298
Iteration 230/1000 | Loss: 0.00003298
Iteration 231/1000 | Loss: 0.00003298
Iteration 232/1000 | Loss: 0.00003298
Iteration 233/1000 | Loss: 0.00003298
Iteration 234/1000 | Loss: 0.00003298
Iteration 235/1000 | Loss: 0.00003298
Iteration 236/1000 | Loss: 0.00003298
Iteration 237/1000 | Loss: 0.00003298
Iteration 238/1000 | Loss: 0.00003298
Iteration 239/1000 | Loss: 0.00003298
Iteration 240/1000 | Loss: 0.00003297
Iteration 241/1000 | Loss: 0.00003297
Iteration 242/1000 | Loss: 0.00003297
Iteration 243/1000 | Loss: 0.00003297
Iteration 244/1000 | Loss: 0.00003297
Iteration 245/1000 | Loss: 0.00003297
Iteration 246/1000 | Loss: 0.00003297
Iteration 247/1000 | Loss: 0.00003297
Iteration 248/1000 | Loss: 0.00003297
Iteration 249/1000 | Loss: 0.00003297
Iteration 250/1000 | Loss: 0.00003297
Iteration 251/1000 | Loss: 0.00003297
Iteration 252/1000 | Loss: 0.00003297
Iteration 253/1000 | Loss: 0.00003297
Iteration 254/1000 | Loss: 0.00003297
Iteration 255/1000 | Loss: 0.00003297
Iteration 256/1000 | Loss: 0.00003297
Iteration 257/1000 | Loss: 0.00003297
Iteration 258/1000 | Loss: 0.00003297
Iteration 259/1000 | Loss: 0.00003297
Iteration 260/1000 | Loss: 0.00003297
Iteration 261/1000 | Loss: 0.00003297
Iteration 262/1000 | Loss: 0.00003297
Iteration 263/1000 | Loss: 0.00003297
Iteration 264/1000 | Loss: 0.00003297
Iteration 265/1000 | Loss: 0.00003297
Iteration 266/1000 | Loss: 0.00003297
Iteration 267/1000 | Loss: 0.00003297
Iteration 268/1000 | Loss: 0.00003297
Iteration 269/1000 | Loss: 0.00003297
Iteration 270/1000 | Loss: 0.00003297
Iteration 271/1000 | Loss: 0.00003297
Iteration 272/1000 | Loss: 0.00003297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [3.297396324342117e-05, 3.297396324342117e-05, 3.297396324342117e-05, 3.297396324342117e-05, 3.297396324342117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.297396324342117e-05

Optimization complete. Final v2v error: 3.4653563499450684 mm

Highest mean error: 11.002577781677246 mm for frame 90

Lowest mean error: 2.291640520095825 mm for frame 130

Saving results

Total time: 106.80145049095154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_debra_posed_014/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_debra_posed_014/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007645
Iteration 2/25 | Loss: 0.00225964
Iteration 3/25 | Loss: 0.00196653
Iteration 4/25 | Loss: 0.00182732
Iteration 5/25 | Loss: 0.00174220
Iteration 6/25 | Loss: 0.00190843
Iteration 7/25 | Loss: 0.00148833
Iteration 8/25 | Loss: 0.00131820
Iteration 9/25 | Loss: 0.00120841
Iteration 10/25 | Loss: 0.00110427
Iteration 11/25 | Loss: 0.00108382
Iteration 12/25 | Loss: 0.00107204
Iteration 13/25 | Loss: 0.00107192
Iteration 14/25 | Loss: 0.00107168
Iteration 15/25 | Loss: 0.00106358
Iteration 16/25 | Loss: 0.00105902
Iteration 17/25 | Loss: 0.00105847
Iteration 18/25 | Loss: 0.00105830
Iteration 19/25 | Loss: 0.00105829
Iteration 20/25 | Loss: 0.00105829
Iteration 21/25 | Loss: 0.00105829
Iteration 22/25 | Loss: 0.00105828
Iteration 23/25 | Loss: 0.00105828
Iteration 24/25 | Loss: 0.00105828
Iteration 25/25 | Loss: 0.00105828

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20081151
Iteration 2/25 | Loss: 0.00167588
Iteration 3/25 | Loss: 0.00167588
Iteration 4/25 | Loss: 0.00167588
Iteration 5/25 | Loss: 0.00167588
Iteration 6/25 | Loss: 0.00167588
Iteration 7/25 | Loss: 0.00167588
Iteration 8/25 | Loss: 0.00167588
Iteration 9/25 | Loss: 0.00167588
Iteration 10/25 | Loss: 0.00167588
Iteration 11/25 | Loss: 0.00167588
Iteration 12/25 | Loss: 0.00167588
Iteration 13/25 | Loss: 0.00167588
Iteration 14/25 | Loss: 0.00167588
Iteration 15/25 | Loss: 0.00167588
Iteration 16/25 | Loss: 0.00167588
Iteration 17/25 | Loss: 0.00167588
Iteration 18/25 | Loss: 0.00167588
Iteration 19/25 | Loss: 0.00167588
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016758779529482126, 0.0016758779529482126, 0.0016758779529482126, 0.0016758779529482126, 0.0016758779529482126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016758779529482126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167588
Iteration 2/1000 | Loss: 0.00003680
Iteration 3/1000 | Loss: 0.00002595
Iteration 4/1000 | Loss: 0.00002279
Iteration 5/1000 | Loss: 0.00002100
Iteration 6/1000 | Loss: 0.00001983
Iteration 7/1000 | Loss: 0.00001913
Iteration 8/1000 | Loss: 0.00001868
Iteration 9/1000 | Loss: 0.00001830
Iteration 10/1000 | Loss: 0.00001800
Iteration 11/1000 | Loss: 0.00001774
Iteration 12/1000 | Loss: 0.00001760
Iteration 13/1000 | Loss: 0.00001759
Iteration 14/1000 | Loss: 0.00001753
Iteration 15/1000 | Loss: 0.00001751
Iteration 16/1000 | Loss: 0.00001750
Iteration 17/1000 | Loss: 0.00001750
Iteration 18/1000 | Loss: 0.00001747
Iteration 19/1000 | Loss: 0.00001747
Iteration 20/1000 | Loss: 0.00001746
Iteration 21/1000 | Loss: 0.00001746
Iteration 22/1000 | Loss: 0.00001746
Iteration 23/1000 | Loss: 0.00001746
Iteration 24/1000 | Loss: 0.00001746
Iteration 25/1000 | Loss: 0.00001746
Iteration 26/1000 | Loss: 0.00001745
Iteration 27/1000 | Loss: 0.00001745
Iteration 28/1000 | Loss: 0.00001745
Iteration 29/1000 | Loss: 0.00001744
Iteration 30/1000 | Loss: 0.00001743
Iteration 31/1000 | Loss: 0.00001743
Iteration 32/1000 | Loss: 0.00001743
Iteration 33/1000 | Loss: 0.00001742
Iteration 34/1000 | Loss: 0.00001742
Iteration 35/1000 | Loss: 0.00001741
Iteration 36/1000 | Loss: 0.00001740
Iteration 37/1000 | Loss: 0.00001737
Iteration 38/1000 | Loss: 0.00001737
Iteration 39/1000 | Loss: 0.00001736
Iteration 40/1000 | Loss: 0.00001732
Iteration 41/1000 | Loss: 0.00001732
Iteration 42/1000 | Loss: 0.00001732
Iteration 43/1000 | Loss: 0.00001731
Iteration 44/1000 | Loss: 0.00001730
Iteration 45/1000 | Loss: 0.00001730
Iteration 46/1000 | Loss: 0.00001730
Iteration 47/1000 | Loss: 0.00001730
Iteration 48/1000 | Loss: 0.00001730
Iteration 49/1000 | Loss: 0.00001730
Iteration 50/1000 | Loss: 0.00001729
Iteration 51/1000 | Loss: 0.00001729
Iteration 52/1000 | Loss: 0.00001729
Iteration 53/1000 | Loss: 0.00001729
Iteration 54/1000 | Loss: 0.00001728
Iteration 55/1000 | Loss: 0.00001728
Iteration 56/1000 | Loss: 0.00001728
Iteration 57/1000 | Loss: 0.00001728
Iteration 58/1000 | Loss: 0.00001727
Iteration 59/1000 | Loss: 0.00001727
Iteration 60/1000 | Loss: 0.00001727
Iteration 61/1000 | Loss: 0.00001727
Iteration 62/1000 | Loss: 0.00001727
Iteration 63/1000 | Loss: 0.00001727
Iteration 64/1000 | Loss: 0.00001727
Iteration 65/1000 | Loss: 0.00001727
Iteration 66/1000 | Loss: 0.00001727
Iteration 67/1000 | Loss: 0.00001727
Iteration 68/1000 | Loss: 0.00001726
Iteration 69/1000 | Loss: 0.00001726
Iteration 70/1000 | Loss: 0.00001726
Iteration 71/1000 | Loss: 0.00001724
Iteration 72/1000 | Loss: 0.00001724
Iteration 73/1000 | Loss: 0.00001724
Iteration 74/1000 | Loss: 0.00001724
Iteration 75/1000 | Loss: 0.00001724
Iteration 76/1000 | Loss: 0.00001724
Iteration 77/1000 | Loss: 0.00001724
Iteration 78/1000 | Loss: 0.00001724
Iteration 79/1000 | Loss: 0.00001724
Iteration 80/1000 | Loss: 0.00001724
Iteration 81/1000 | Loss: 0.00001724
Iteration 82/1000 | Loss: 0.00001724
Iteration 83/1000 | Loss: 0.00001724
Iteration 84/1000 | Loss: 0.00001724
Iteration 85/1000 | Loss: 0.00001724
Iteration 86/1000 | Loss: 0.00001724
Iteration 87/1000 | Loss: 0.00001724
Iteration 88/1000 | Loss: 0.00001724
Iteration 89/1000 | Loss: 0.00001724
Iteration 90/1000 | Loss: 0.00001724
Iteration 91/1000 | Loss: 0.00001724
Iteration 92/1000 | Loss: 0.00001724
Iteration 93/1000 | Loss: 0.00001724
Iteration 94/1000 | Loss: 0.00001724
Iteration 95/1000 | Loss: 0.00001724
Iteration 96/1000 | Loss: 0.00001724
Iteration 97/1000 | Loss: 0.00001724
Iteration 98/1000 | Loss: 0.00001724
Iteration 99/1000 | Loss: 0.00001724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.7237251086044125e-05, 1.7237251086044125e-05, 1.7237251086044125e-05, 1.7237251086044125e-05, 1.7237251086044125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7237251086044125e-05

Optimization complete. Final v2v error: 3.060744047164917 mm

Highest mean error: 11.170308113098145 mm for frame 1

Lowest mean error: 2.864363193511963 mm for frame 133

Saving results

Total time: 54.392592668533325
