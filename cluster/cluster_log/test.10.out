Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=10, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 560-615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408998
Iteration 2/25 | Loss: 0.00089233
Iteration 3/25 | Loss: 0.00074863
Iteration 4/25 | Loss: 0.00071826
Iteration 5/25 | Loss: 0.00070792
Iteration 6/25 | Loss: 0.00070646
Iteration 7/25 | Loss: 0.00070597
Iteration 8/25 | Loss: 0.00070591
Iteration 9/25 | Loss: 0.00070591
Iteration 10/25 | Loss: 0.00070591
Iteration 11/25 | Loss: 0.00070591
Iteration 12/25 | Loss: 0.00070591
Iteration 13/25 | Loss: 0.00070591
Iteration 14/25 | Loss: 0.00070591
Iteration 15/25 | Loss: 0.00070591
Iteration 16/25 | Loss: 0.00070591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007059117197059095, 0.0007059117197059095, 0.0007059117197059095, 0.0007059117197059095, 0.0007059117197059095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007059117197059095

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45554531
Iteration 2/25 | Loss: 0.00031702
Iteration 3/25 | Loss: 0.00031702
Iteration 4/25 | Loss: 0.00031702
Iteration 5/25 | Loss: 0.00031702
Iteration 6/25 | Loss: 0.00031702
Iteration 7/25 | Loss: 0.00031702
Iteration 8/25 | Loss: 0.00031702
Iteration 9/25 | Loss: 0.00031702
Iteration 10/25 | Loss: 0.00031702
Iteration 11/25 | Loss: 0.00031702
Iteration 12/25 | Loss: 0.00031702
Iteration 13/25 | Loss: 0.00031702
Iteration 14/25 | Loss: 0.00031702
Iteration 15/25 | Loss: 0.00031702
Iteration 16/25 | Loss: 0.00031702
Iteration 17/25 | Loss: 0.00031702
Iteration 18/25 | Loss: 0.00031702
Iteration 19/25 | Loss: 0.00031702
Iteration 20/25 | Loss: 0.00031702
Iteration 21/25 | Loss: 0.00031702
Iteration 22/25 | Loss: 0.00031702
Iteration 23/25 | Loss: 0.00031702
Iteration 24/25 | Loss: 0.00031702
Iteration 25/25 | Loss: 0.00031702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031702
Iteration 2/1000 | Loss: 0.00005149
Iteration 3/1000 | Loss: 0.00003736
Iteration 4/1000 | Loss: 0.00003125
Iteration 5/1000 | Loss: 0.00003010
Iteration 6/1000 | Loss: 0.00002870
Iteration 7/1000 | Loss: 0.00002808
Iteration 8/1000 | Loss: 0.00002735
Iteration 9/1000 | Loss: 0.00002689
Iteration 10/1000 | Loss: 0.00002688
Iteration 11/1000 | Loss: 0.00002664
Iteration 12/1000 | Loss: 0.00002658
Iteration 13/1000 | Loss: 0.00002640
Iteration 14/1000 | Loss: 0.00002630
Iteration 15/1000 | Loss: 0.00002621
Iteration 16/1000 | Loss: 0.00002621
Iteration 17/1000 | Loss: 0.00002621
Iteration 18/1000 | Loss: 0.00002621
Iteration 19/1000 | Loss: 0.00002621
Iteration 20/1000 | Loss: 0.00002621
Iteration 21/1000 | Loss: 0.00002621
Iteration 22/1000 | Loss: 0.00002621
Iteration 23/1000 | Loss: 0.00002621
Iteration 24/1000 | Loss: 0.00002612
Iteration 25/1000 | Loss: 0.00002611
Iteration 26/1000 | Loss: 0.00002611
Iteration 27/1000 | Loss: 0.00002609
Iteration 28/1000 | Loss: 0.00002609
Iteration 29/1000 | Loss: 0.00002605
Iteration 30/1000 | Loss: 0.00002604
Iteration 31/1000 | Loss: 0.00002604
Iteration 32/1000 | Loss: 0.00002604
Iteration 33/1000 | Loss: 0.00002604
Iteration 34/1000 | Loss: 0.00002604
Iteration 35/1000 | Loss: 0.00002604
Iteration 36/1000 | Loss: 0.00002604
Iteration 37/1000 | Loss: 0.00002604
Iteration 38/1000 | Loss: 0.00002604
Iteration 39/1000 | Loss: 0.00002604
Iteration 40/1000 | Loss: 0.00002604
Iteration 41/1000 | Loss: 0.00002602
Iteration 42/1000 | Loss: 0.00002602
Iteration 43/1000 | Loss: 0.00002595
Iteration 44/1000 | Loss: 0.00002595
Iteration 45/1000 | Loss: 0.00002591
Iteration 46/1000 | Loss: 0.00002590
Iteration 47/1000 | Loss: 0.00002585
Iteration 48/1000 | Loss: 0.00002585
Iteration 49/1000 | Loss: 0.00002585
Iteration 50/1000 | Loss: 0.00002584
Iteration 51/1000 | Loss: 0.00002584
Iteration 52/1000 | Loss: 0.00002584
Iteration 53/1000 | Loss: 0.00002583
Iteration 54/1000 | Loss: 0.00002583
Iteration 55/1000 | Loss: 0.00002583
Iteration 56/1000 | Loss: 0.00002583
Iteration 57/1000 | Loss: 0.00002583
Iteration 58/1000 | Loss: 0.00002583
Iteration 59/1000 | Loss: 0.00002583
Iteration 60/1000 | Loss: 0.00002582
Iteration 61/1000 | Loss: 0.00002582
Iteration 62/1000 | Loss: 0.00002582
Iteration 63/1000 | Loss: 0.00002582
Iteration 64/1000 | Loss: 0.00002582
Iteration 65/1000 | Loss: 0.00002582
Iteration 66/1000 | Loss: 0.00002582
Iteration 67/1000 | Loss: 0.00002582
Iteration 68/1000 | Loss: 0.00002582
Iteration 69/1000 | Loss: 0.00002582
Iteration 70/1000 | Loss: 0.00002582
Iteration 71/1000 | Loss: 0.00002582
Iteration 72/1000 | Loss: 0.00002582
Iteration 73/1000 | Loss: 0.00002582
Iteration 74/1000 | Loss: 0.00002582
Iteration 75/1000 | Loss: 0.00002581
Iteration 76/1000 | Loss: 0.00002581
Iteration 77/1000 | Loss: 0.00002581
Iteration 78/1000 | Loss: 0.00002581
Iteration 79/1000 | Loss: 0.00002581
Iteration 80/1000 | Loss: 0.00002581
Iteration 81/1000 | Loss: 0.00002580
Iteration 82/1000 | Loss: 0.00002580
Iteration 83/1000 | Loss: 0.00002580
Iteration 84/1000 | Loss: 0.00002580
Iteration 85/1000 | Loss: 0.00002580
Iteration 86/1000 | Loss: 0.00002580
Iteration 87/1000 | Loss: 0.00002580
Iteration 88/1000 | Loss: 0.00002580
Iteration 89/1000 | Loss: 0.00002580
Iteration 90/1000 | Loss: 0.00002580
Iteration 91/1000 | Loss: 0.00002580
Iteration 92/1000 | Loss: 0.00002580
Iteration 93/1000 | Loss: 0.00002580
Iteration 94/1000 | Loss: 0.00002580
Iteration 95/1000 | Loss: 0.00002579
Iteration 96/1000 | Loss: 0.00002579
Iteration 97/1000 | Loss: 0.00002579
Iteration 98/1000 | Loss: 0.00002579
Iteration 99/1000 | Loss: 0.00002579
Iteration 100/1000 | Loss: 0.00002579
Iteration 101/1000 | Loss: 0.00002579
Iteration 102/1000 | Loss: 0.00002578
Iteration 103/1000 | Loss: 0.00002578
Iteration 104/1000 | Loss: 0.00002578
Iteration 105/1000 | Loss: 0.00002578
Iteration 106/1000 | Loss: 0.00002578
Iteration 107/1000 | Loss: 0.00002578
Iteration 108/1000 | Loss: 0.00002578
Iteration 109/1000 | Loss: 0.00002578
Iteration 110/1000 | Loss: 0.00002578
Iteration 111/1000 | Loss: 0.00002578
Iteration 112/1000 | Loss: 0.00002577
Iteration 113/1000 | Loss: 0.00002577
Iteration 114/1000 | Loss: 0.00002577
Iteration 115/1000 | Loss: 0.00002577
Iteration 116/1000 | Loss: 0.00002577
Iteration 117/1000 | Loss: 0.00002577
Iteration 118/1000 | Loss: 0.00002576
Iteration 119/1000 | Loss: 0.00002576
Iteration 120/1000 | Loss: 0.00002576
Iteration 121/1000 | Loss: 0.00002576
Iteration 122/1000 | Loss: 0.00002576
Iteration 123/1000 | Loss: 0.00002576
Iteration 124/1000 | Loss: 0.00002576
Iteration 125/1000 | Loss: 0.00002576
Iteration 126/1000 | Loss: 0.00002576
Iteration 127/1000 | Loss: 0.00002575
Iteration 128/1000 | Loss: 0.00002575
Iteration 129/1000 | Loss: 0.00002575
Iteration 130/1000 | Loss: 0.00002575
Iteration 131/1000 | Loss: 0.00002575
Iteration 132/1000 | Loss: 0.00002575
Iteration 133/1000 | Loss: 0.00002575
Iteration 134/1000 | Loss: 0.00002575
Iteration 135/1000 | Loss: 0.00002575
Iteration 136/1000 | Loss: 0.00002575
Iteration 137/1000 | Loss: 0.00002575
Iteration 138/1000 | Loss: 0.00002575
Iteration 139/1000 | Loss: 0.00002575
Iteration 140/1000 | Loss: 0.00002575
Iteration 141/1000 | Loss: 0.00002575
Iteration 142/1000 | Loss: 0.00002575
Iteration 143/1000 | Loss: 0.00002575
Iteration 144/1000 | Loss: 0.00002575
Iteration 145/1000 | Loss: 0.00002575
Iteration 146/1000 | Loss: 0.00002575
Iteration 147/1000 | Loss: 0.00002575
Iteration 148/1000 | Loss: 0.00002575
Iteration 149/1000 | Loss: 0.00002575
Iteration 150/1000 | Loss: 0.00002575
Iteration 151/1000 | Loss: 0.00002575
Iteration 152/1000 | Loss: 0.00002575
Iteration 153/1000 | Loss: 0.00002575
Iteration 154/1000 | Loss: 0.00002575
Iteration 155/1000 | Loss: 0.00002575
Iteration 156/1000 | Loss: 0.00002575
Iteration 157/1000 | Loss: 0.00002575
Iteration 158/1000 | Loss: 0.00002575
Iteration 159/1000 | Loss: 0.00002575
Iteration 160/1000 | Loss: 0.00002575
Iteration 161/1000 | Loss: 0.00002575
Iteration 162/1000 | Loss: 0.00002575
Iteration 163/1000 | Loss: 0.00002575
Iteration 164/1000 | Loss: 0.00002575
Iteration 165/1000 | Loss: 0.00002575
Iteration 166/1000 | Loss: 0.00002575
Iteration 167/1000 | Loss: 0.00002575
Iteration 168/1000 | Loss: 0.00002575
Iteration 169/1000 | Loss: 0.00002575
Iteration 170/1000 | Loss: 0.00002575
Iteration 171/1000 | Loss: 0.00002575
Iteration 172/1000 | Loss: 0.00002575
Iteration 173/1000 | Loss: 0.00002575
Iteration 174/1000 | Loss: 0.00002575
Iteration 175/1000 | Loss: 0.00002575
Iteration 176/1000 | Loss: 0.00002575
Iteration 177/1000 | Loss: 0.00002575
Iteration 178/1000 | Loss: 0.00002575
Iteration 179/1000 | Loss: 0.00002575
Iteration 180/1000 | Loss: 0.00002575
Iteration 181/1000 | Loss: 0.00002575
Iteration 182/1000 | Loss: 0.00002575
Iteration 183/1000 | Loss: 0.00002575
Iteration 184/1000 | Loss: 0.00002575
Iteration 185/1000 | Loss: 0.00002575
Iteration 186/1000 | Loss: 0.00002575
Iteration 187/1000 | Loss: 0.00002575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.5750659915502183e-05, 2.5750659915502183e-05, 2.5750659915502183e-05, 2.5750659915502183e-05, 2.5750659915502183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5750659915502183e-05

Optimization complete. Final v2v error: 4.202322483062744 mm

Highest mean error: 4.577592849731445 mm for frame 15

Lowest mean error: 3.792618989944458 mm for frame 33

Saving results

Total time: 41.907944202423096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815358
Iteration 2/25 | Loss: 0.00086157
Iteration 3/25 | Loss: 0.00071334
Iteration 4/25 | Loss: 0.00067872
Iteration 5/25 | Loss: 0.00066346
Iteration 6/25 | Loss: 0.00065996
Iteration 7/25 | Loss: 0.00065951
Iteration 8/25 | Loss: 0.00065951
Iteration 9/25 | Loss: 0.00065951
Iteration 10/25 | Loss: 0.00065951
Iteration 11/25 | Loss: 0.00065951
Iteration 12/25 | Loss: 0.00065951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006595111335627735, 0.0006595111335627735, 0.0006595111335627735, 0.0006595111335627735, 0.0006595111335627735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006595111335627735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44292843
Iteration 2/25 | Loss: 0.00031121
Iteration 3/25 | Loss: 0.00031121
Iteration 4/25 | Loss: 0.00031121
Iteration 5/25 | Loss: 0.00031121
Iteration 6/25 | Loss: 0.00031121
Iteration 7/25 | Loss: 0.00031121
Iteration 8/25 | Loss: 0.00031121
Iteration 9/25 | Loss: 0.00031121
Iteration 10/25 | Loss: 0.00031121
Iteration 11/25 | Loss: 0.00031121
Iteration 12/25 | Loss: 0.00031121
Iteration 13/25 | Loss: 0.00031121
Iteration 14/25 | Loss: 0.00031121
Iteration 15/25 | Loss: 0.00031121
Iteration 16/25 | Loss: 0.00031121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000311211304506287, 0.000311211304506287, 0.000311211304506287, 0.000311211304506287, 0.000311211304506287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000311211304506287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031121
Iteration 2/1000 | Loss: 0.00003895
Iteration 3/1000 | Loss: 0.00002685
Iteration 4/1000 | Loss: 0.00002305
Iteration 5/1000 | Loss: 0.00002196
Iteration 6/1000 | Loss: 0.00002109
Iteration 7/1000 | Loss: 0.00002053
Iteration 8/1000 | Loss: 0.00001999
Iteration 9/1000 | Loss: 0.00001967
Iteration 10/1000 | Loss: 0.00001942
Iteration 11/1000 | Loss: 0.00001938
Iteration 12/1000 | Loss: 0.00001917
Iteration 13/1000 | Loss: 0.00001903
Iteration 14/1000 | Loss: 0.00001900
Iteration 15/1000 | Loss: 0.00001895
Iteration 16/1000 | Loss: 0.00001894
Iteration 17/1000 | Loss: 0.00001892
Iteration 18/1000 | Loss: 0.00001888
Iteration 19/1000 | Loss: 0.00001887
Iteration 20/1000 | Loss: 0.00001883
Iteration 21/1000 | Loss: 0.00001880
Iteration 22/1000 | Loss: 0.00001872
Iteration 23/1000 | Loss: 0.00001867
Iteration 24/1000 | Loss: 0.00001867
Iteration 25/1000 | Loss: 0.00001867
Iteration 26/1000 | Loss: 0.00001867
Iteration 27/1000 | Loss: 0.00001867
Iteration 28/1000 | Loss: 0.00001867
Iteration 29/1000 | Loss: 0.00001867
Iteration 30/1000 | Loss: 0.00001866
Iteration 31/1000 | Loss: 0.00001866
Iteration 32/1000 | Loss: 0.00001866
Iteration 33/1000 | Loss: 0.00001865
Iteration 34/1000 | Loss: 0.00001865
Iteration 35/1000 | Loss: 0.00001865
Iteration 36/1000 | Loss: 0.00001864
Iteration 37/1000 | Loss: 0.00001864
Iteration 38/1000 | Loss: 0.00001864
Iteration 39/1000 | Loss: 0.00001864
Iteration 40/1000 | Loss: 0.00001864
Iteration 41/1000 | Loss: 0.00001864
Iteration 42/1000 | Loss: 0.00001864
Iteration 43/1000 | Loss: 0.00001864
Iteration 44/1000 | Loss: 0.00001864
Iteration 45/1000 | Loss: 0.00001864
Iteration 46/1000 | Loss: 0.00001864
Iteration 47/1000 | Loss: 0.00001864
Iteration 48/1000 | Loss: 0.00001864
Iteration 49/1000 | Loss: 0.00001863
Iteration 50/1000 | Loss: 0.00001863
Iteration 51/1000 | Loss: 0.00001863
Iteration 52/1000 | Loss: 0.00001863
Iteration 53/1000 | Loss: 0.00001863
Iteration 54/1000 | Loss: 0.00001863
Iteration 55/1000 | Loss: 0.00001863
Iteration 56/1000 | Loss: 0.00001863
Iteration 57/1000 | Loss: 0.00001863
Iteration 58/1000 | Loss: 0.00001862
Iteration 59/1000 | Loss: 0.00001862
Iteration 60/1000 | Loss: 0.00001862
Iteration 61/1000 | Loss: 0.00001862
Iteration 62/1000 | Loss: 0.00001862
Iteration 63/1000 | Loss: 0.00001862
Iteration 64/1000 | Loss: 0.00001862
Iteration 65/1000 | Loss: 0.00001862
Iteration 66/1000 | Loss: 0.00001862
Iteration 67/1000 | Loss: 0.00001862
Iteration 68/1000 | Loss: 0.00001862
Iteration 69/1000 | Loss: 0.00001861
Iteration 70/1000 | Loss: 0.00001861
Iteration 71/1000 | Loss: 0.00001861
Iteration 72/1000 | Loss: 0.00001861
Iteration 73/1000 | Loss: 0.00001861
Iteration 74/1000 | Loss: 0.00001861
Iteration 75/1000 | Loss: 0.00001861
Iteration 76/1000 | Loss: 0.00001861
Iteration 77/1000 | Loss: 0.00001861
Iteration 78/1000 | Loss: 0.00001861
Iteration 79/1000 | Loss: 0.00001861
Iteration 80/1000 | Loss: 0.00001861
Iteration 81/1000 | Loss: 0.00001861
Iteration 82/1000 | Loss: 0.00001860
Iteration 83/1000 | Loss: 0.00001860
Iteration 84/1000 | Loss: 0.00001860
Iteration 85/1000 | Loss: 0.00001860
Iteration 86/1000 | Loss: 0.00001860
Iteration 87/1000 | Loss: 0.00001860
Iteration 88/1000 | Loss: 0.00001860
Iteration 89/1000 | Loss: 0.00001860
Iteration 90/1000 | Loss: 0.00001860
Iteration 91/1000 | Loss: 0.00001859
Iteration 92/1000 | Loss: 0.00001859
Iteration 93/1000 | Loss: 0.00001859
Iteration 94/1000 | Loss: 0.00001859
Iteration 95/1000 | Loss: 0.00001859
Iteration 96/1000 | Loss: 0.00001859
Iteration 97/1000 | Loss: 0.00001858
Iteration 98/1000 | Loss: 0.00001858
Iteration 99/1000 | Loss: 0.00001858
Iteration 100/1000 | Loss: 0.00001858
Iteration 101/1000 | Loss: 0.00001858
Iteration 102/1000 | Loss: 0.00001858
Iteration 103/1000 | Loss: 0.00001857
Iteration 104/1000 | Loss: 0.00001857
Iteration 105/1000 | Loss: 0.00001857
Iteration 106/1000 | Loss: 0.00001857
Iteration 107/1000 | Loss: 0.00001857
Iteration 108/1000 | Loss: 0.00001857
Iteration 109/1000 | Loss: 0.00001857
Iteration 110/1000 | Loss: 0.00001856
Iteration 111/1000 | Loss: 0.00001856
Iteration 112/1000 | Loss: 0.00001856
Iteration 113/1000 | Loss: 0.00001856
Iteration 114/1000 | Loss: 0.00001856
Iteration 115/1000 | Loss: 0.00001855
Iteration 116/1000 | Loss: 0.00001855
Iteration 117/1000 | Loss: 0.00001855
Iteration 118/1000 | Loss: 0.00001855
Iteration 119/1000 | Loss: 0.00001855
Iteration 120/1000 | Loss: 0.00001855
Iteration 121/1000 | Loss: 0.00001855
Iteration 122/1000 | Loss: 0.00001855
Iteration 123/1000 | Loss: 0.00001855
Iteration 124/1000 | Loss: 0.00001855
Iteration 125/1000 | Loss: 0.00001855
Iteration 126/1000 | Loss: 0.00001855
Iteration 127/1000 | Loss: 0.00001855
Iteration 128/1000 | Loss: 0.00001855
Iteration 129/1000 | Loss: 0.00001855
Iteration 130/1000 | Loss: 0.00001855
Iteration 131/1000 | Loss: 0.00001855
Iteration 132/1000 | Loss: 0.00001855
Iteration 133/1000 | Loss: 0.00001855
Iteration 134/1000 | Loss: 0.00001855
Iteration 135/1000 | Loss: 0.00001855
Iteration 136/1000 | Loss: 0.00001855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.8549188098404557e-05, 1.8549188098404557e-05, 1.8549188098404557e-05, 1.8549188098404557e-05, 1.8549188098404557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8549188098404557e-05

Optimization complete. Final v2v error: 3.6475765705108643 mm

Highest mean error: 4.2757110595703125 mm for frame 239

Lowest mean error: 3.3901681900024414 mm for frame 40

Saving results

Total time: 43.77542018890381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863358
Iteration 2/25 | Loss: 0.00116262
Iteration 3/25 | Loss: 0.00084076
Iteration 4/25 | Loss: 0.00075128
Iteration 5/25 | Loss: 0.00073872
Iteration 6/25 | Loss: 0.00073693
Iteration 7/25 | Loss: 0.00073667
Iteration 8/25 | Loss: 0.00073667
Iteration 9/25 | Loss: 0.00073667
Iteration 10/25 | Loss: 0.00073667
Iteration 11/25 | Loss: 0.00073667
Iteration 12/25 | Loss: 0.00073667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007366700447164476, 0.0007366700447164476, 0.0007366700447164476, 0.0007366700447164476, 0.0007366700447164476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007366700447164476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03424799
Iteration 2/25 | Loss: 0.00030640
Iteration 3/25 | Loss: 0.00030639
Iteration 4/25 | Loss: 0.00030639
Iteration 5/25 | Loss: 0.00030639
Iteration 6/25 | Loss: 0.00030639
Iteration 7/25 | Loss: 0.00030638
Iteration 8/25 | Loss: 0.00030638
Iteration 9/25 | Loss: 0.00030638
Iteration 10/25 | Loss: 0.00030638
Iteration 11/25 | Loss: 0.00030638
Iteration 12/25 | Loss: 0.00030638
Iteration 13/25 | Loss: 0.00030638
Iteration 14/25 | Loss: 0.00030638
Iteration 15/25 | Loss: 0.00030638
Iteration 16/25 | Loss: 0.00030638
Iteration 17/25 | Loss: 0.00030638
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003063840267714113, 0.0003063840267714113, 0.0003063840267714113, 0.0003063840267714113, 0.0003063840267714113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003063840267714113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030638
Iteration 2/1000 | Loss: 0.00005471
Iteration 3/1000 | Loss: 0.00004209
Iteration 4/1000 | Loss: 0.00003705
Iteration 5/1000 | Loss: 0.00003411
Iteration 6/1000 | Loss: 0.00003244
Iteration 7/1000 | Loss: 0.00003098
Iteration 8/1000 | Loss: 0.00003001
Iteration 9/1000 | Loss: 0.00002953
Iteration 10/1000 | Loss: 0.00002920
Iteration 11/1000 | Loss: 0.00002898
Iteration 12/1000 | Loss: 0.00002877
Iteration 13/1000 | Loss: 0.00002869
Iteration 14/1000 | Loss: 0.00002868
Iteration 15/1000 | Loss: 0.00002867
Iteration 16/1000 | Loss: 0.00002865
Iteration 17/1000 | Loss: 0.00002865
Iteration 18/1000 | Loss: 0.00002862
Iteration 19/1000 | Loss: 0.00002862
Iteration 20/1000 | Loss: 0.00002862
Iteration 21/1000 | Loss: 0.00002862
Iteration 22/1000 | Loss: 0.00002862
Iteration 23/1000 | Loss: 0.00002862
Iteration 24/1000 | Loss: 0.00002861
Iteration 25/1000 | Loss: 0.00002861
Iteration 26/1000 | Loss: 0.00002861
Iteration 27/1000 | Loss: 0.00002860
Iteration 28/1000 | Loss: 0.00002860
Iteration 29/1000 | Loss: 0.00002860
Iteration 30/1000 | Loss: 0.00002860
Iteration 31/1000 | Loss: 0.00002859
Iteration 32/1000 | Loss: 0.00002859
Iteration 33/1000 | Loss: 0.00002858
Iteration 34/1000 | Loss: 0.00002855
Iteration 35/1000 | Loss: 0.00002854
Iteration 36/1000 | Loss: 0.00002854
Iteration 37/1000 | Loss: 0.00002853
Iteration 38/1000 | Loss: 0.00002852
Iteration 39/1000 | Loss: 0.00002852
Iteration 40/1000 | Loss: 0.00002852
Iteration 41/1000 | Loss: 0.00002852
Iteration 42/1000 | Loss: 0.00002852
Iteration 43/1000 | Loss: 0.00002852
Iteration 44/1000 | Loss: 0.00002852
Iteration 45/1000 | Loss: 0.00002851
Iteration 46/1000 | Loss: 0.00002851
Iteration 47/1000 | Loss: 0.00002851
Iteration 48/1000 | Loss: 0.00002851
Iteration 49/1000 | Loss: 0.00002851
Iteration 50/1000 | Loss: 0.00002851
Iteration 51/1000 | Loss: 0.00002851
Iteration 52/1000 | Loss: 0.00002851
Iteration 53/1000 | Loss: 0.00002851
Iteration 54/1000 | Loss: 0.00002850
Iteration 55/1000 | Loss: 0.00002850
Iteration 56/1000 | Loss: 0.00002850
Iteration 57/1000 | Loss: 0.00002850
Iteration 58/1000 | Loss: 0.00002850
Iteration 59/1000 | Loss: 0.00002850
Iteration 60/1000 | Loss: 0.00002850
Iteration 61/1000 | Loss: 0.00002850
Iteration 62/1000 | Loss: 0.00002849
Iteration 63/1000 | Loss: 0.00002849
Iteration 64/1000 | Loss: 0.00002849
Iteration 65/1000 | Loss: 0.00002849
Iteration 66/1000 | Loss: 0.00002849
Iteration 67/1000 | Loss: 0.00002849
Iteration 68/1000 | Loss: 0.00002849
Iteration 69/1000 | Loss: 0.00002849
Iteration 70/1000 | Loss: 0.00002849
Iteration 71/1000 | Loss: 0.00002849
Iteration 72/1000 | Loss: 0.00002849
Iteration 73/1000 | Loss: 0.00002849
Iteration 74/1000 | Loss: 0.00002849
Iteration 75/1000 | Loss: 0.00002849
Iteration 76/1000 | Loss: 0.00002849
Iteration 77/1000 | Loss: 0.00002849
Iteration 78/1000 | Loss: 0.00002848
Iteration 79/1000 | Loss: 0.00002848
Iteration 80/1000 | Loss: 0.00002848
Iteration 81/1000 | Loss: 0.00002848
Iteration 82/1000 | Loss: 0.00002848
Iteration 83/1000 | Loss: 0.00002848
Iteration 84/1000 | Loss: 0.00002848
Iteration 85/1000 | Loss: 0.00002848
Iteration 86/1000 | Loss: 0.00002848
Iteration 87/1000 | Loss: 0.00002848
Iteration 88/1000 | Loss: 0.00002848
Iteration 89/1000 | Loss: 0.00002848
Iteration 90/1000 | Loss: 0.00002848
Iteration 91/1000 | Loss: 0.00002848
Iteration 92/1000 | Loss: 0.00002848
Iteration 93/1000 | Loss: 0.00002848
Iteration 94/1000 | Loss: 0.00002848
Iteration 95/1000 | Loss: 0.00002848
Iteration 96/1000 | Loss: 0.00002848
Iteration 97/1000 | Loss: 0.00002847
Iteration 98/1000 | Loss: 0.00002847
Iteration 99/1000 | Loss: 0.00002847
Iteration 100/1000 | Loss: 0.00002847
Iteration 101/1000 | Loss: 0.00002847
Iteration 102/1000 | Loss: 0.00002847
Iteration 103/1000 | Loss: 0.00002847
Iteration 104/1000 | Loss: 0.00002847
Iteration 105/1000 | Loss: 0.00002847
Iteration 106/1000 | Loss: 0.00002847
Iteration 107/1000 | Loss: 0.00002847
Iteration 108/1000 | Loss: 0.00002847
Iteration 109/1000 | Loss: 0.00002847
Iteration 110/1000 | Loss: 0.00002847
Iteration 111/1000 | Loss: 0.00002847
Iteration 112/1000 | Loss: 0.00002847
Iteration 113/1000 | Loss: 0.00002847
Iteration 114/1000 | Loss: 0.00002847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.8473459678934887e-05, 2.8473459678934887e-05, 2.8473459678934887e-05, 2.8473459678934887e-05, 2.8473459678934887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8473459678934887e-05

Optimization complete. Final v2v error: 4.435927391052246 mm

Highest mean error: 5.013143062591553 mm for frame 131

Lowest mean error: 4.09661340713501 mm for frame 30

Saving results

Total time: 33.85874009132385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449413
Iteration 2/25 | Loss: 0.00083246
Iteration 3/25 | Loss: 0.00072226
Iteration 4/25 | Loss: 0.00070000
Iteration 5/25 | Loss: 0.00069644
Iteration 6/25 | Loss: 0.00069571
Iteration 7/25 | Loss: 0.00069571
Iteration 8/25 | Loss: 0.00069571
Iteration 9/25 | Loss: 0.00069571
Iteration 10/25 | Loss: 0.00069571
Iteration 11/25 | Loss: 0.00069571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006957060541026294, 0.0006957060541026294, 0.0006957060541026294, 0.0006957060541026294, 0.0006957060541026294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006957060541026294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42665076
Iteration 2/25 | Loss: 0.00027313
Iteration 3/25 | Loss: 0.00027312
Iteration 4/25 | Loss: 0.00027312
Iteration 5/25 | Loss: 0.00027312
Iteration 6/25 | Loss: 0.00027312
Iteration 7/25 | Loss: 0.00027312
Iteration 8/25 | Loss: 0.00027312
Iteration 9/25 | Loss: 0.00027312
Iteration 10/25 | Loss: 0.00027312
Iteration 11/25 | Loss: 0.00027312
Iteration 12/25 | Loss: 0.00027312
Iteration 13/25 | Loss: 0.00027312
Iteration 14/25 | Loss: 0.00027312
Iteration 15/25 | Loss: 0.00027312
Iteration 16/25 | Loss: 0.00027312
Iteration 17/25 | Loss: 0.00027312
Iteration 18/25 | Loss: 0.00027312
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000273119134362787, 0.000273119134362787, 0.000273119134362787, 0.000273119134362787, 0.000273119134362787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000273119134362787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027312
Iteration 2/1000 | Loss: 0.00005324
Iteration 3/1000 | Loss: 0.00003776
Iteration 4/1000 | Loss: 0.00003185
Iteration 5/1000 | Loss: 0.00002974
Iteration 6/1000 | Loss: 0.00002839
Iteration 7/1000 | Loss: 0.00002760
Iteration 8/1000 | Loss: 0.00002685
Iteration 9/1000 | Loss: 0.00002642
Iteration 10/1000 | Loss: 0.00002615
Iteration 11/1000 | Loss: 0.00002592
Iteration 12/1000 | Loss: 0.00002591
Iteration 13/1000 | Loss: 0.00002590
Iteration 14/1000 | Loss: 0.00002588
Iteration 15/1000 | Loss: 0.00002570
Iteration 16/1000 | Loss: 0.00002559
Iteration 17/1000 | Loss: 0.00002559
Iteration 18/1000 | Loss: 0.00002558
Iteration 19/1000 | Loss: 0.00002558
Iteration 20/1000 | Loss: 0.00002557
Iteration 21/1000 | Loss: 0.00002555
Iteration 22/1000 | Loss: 0.00002555
Iteration 23/1000 | Loss: 0.00002555
Iteration 24/1000 | Loss: 0.00002554
Iteration 25/1000 | Loss: 0.00002554
Iteration 26/1000 | Loss: 0.00002554
Iteration 27/1000 | Loss: 0.00002553
Iteration 28/1000 | Loss: 0.00002553
Iteration 29/1000 | Loss: 0.00002552
Iteration 30/1000 | Loss: 0.00002551
Iteration 31/1000 | Loss: 0.00002551
Iteration 32/1000 | Loss: 0.00002550
Iteration 33/1000 | Loss: 0.00002550
Iteration 34/1000 | Loss: 0.00002550
Iteration 35/1000 | Loss: 0.00002550
Iteration 36/1000 | Loss: 0.00002549
Iteration 37/1000 | Loss: 0.00002549
Iteration 38/1000 | Loss: 0.00002548
Iteration 39/1000 | Loss: 0.00002546
Iteration 40/1000 | Loss: 0.00002540
Iteration 41/1000 | Loss: 0.00002540
Iteration 42/1000 | Loss: 0.00002538
Iteration 43/1000 | Loss: 0.00002537
Iteration 44/1000 | Loss: 0.00002536
Iteration 45/1000 | Loss: 0.00002536
Iteration 46/1000 | Loss: 0.00002533
Iteration 47/1000 | Loss: 0.00002532
Iteration 48/1000 | Loss: 0.00002531
Iteration 49/1000 | Loss: 0.00002531
Iteration 50/1000 | Loss: 0.00002531
Iteration 51/1000 | Loss: 0.00002531
Iteration 52/1000 | Loss: 0.00002531
Iteration 53/1000 | Loss: 0.00002531
Iteration 54/1000 | Loss: 0.00002531
Iteration 55/1000 | Loss: 0.00002531
Iteration 56/1000 | Loss: 0.00002531
Iteration 57/1000 | Loss: 0.00002530
Iteration 58/1000 | Loss: 0.00002530
Iteration 59/1000 | Loss: 0.00002530
Iteration 60/1000 | Loss: 0.00002530
Iteration 61/1000 | Loss: 0.00002529
Iteration 62/1000 | Loss: 0.00002529
Iteration 63/1000 | Loss: 0.00002528
Iteration 64/1000 | Loss: 0.00002528
Iteration 65/1000 | Loss: 0.00002528
Iteration 66/1000 | Loss: 0.00002528
Iteration 67/1000 | Loss: 0.00002527
Iteration 68/1000 | Loss: 0.00002527
Iteration 69/1000 | Loss: 0.00002527
Iteration 70/1000 | Loss: 0.00002527
Iteration 71/1000 | Loss: 0.00002527
Iteration 72/1000 | Loss: 0.00002527
Iteration 73/1000 | Loss: 0.00002527
Iteration 74/1000 | Loss: 0.00002527
Iteration 75/1000 | Loss: 0.00002527
Iteration 76/1000 | Loss: 0.00002527
Iteration 77/1000 | Loss: 0.00002527
Iteration 78/1000 | Loss: 0.00002526
Iteration 79/1000 | Loss: 0.00002526
Iteration 80/1000 | Loss: 0.00002526
Iteration 81/1000 | Loss: 0.00002526
Iteration 82/1000 | Loss: 0.00002526
Iteration 83/1000 | Loss: 0.00002526
Iteration 84/1000 | Loss: 0.00002526
Iteration 85/1000 | Loss: 0.00002526
Iteration 86/1000 | Loss: 0.00002526
Iteration 87/1000 | Loss: 0.00002526
Iteration 88/1000 | Loss: 0.00002526
Iteration 89/1000 | Loss: 0.00002526
Iteration 90/1000 | Loss: 0.00002526
Iteration 91/1000 | Loss: 0.00002525
Iteration 92/1000 | Loss: 0.00002525
Iteration 93/1000 | Loss: 0.00002525
Iteration 94/1000 | Loss: 0.00002525
Iteration 95/1000 | Loss: 0.00002525
Iteration 96/1000 | Loss: 0.00002525
Iteration 97/1000 | Loss: 0.00002525
Iteration 98/1000 | Loss: 0.00002525
Iteration 99/1000 | Loss: 0.00002525
Iteration 100/1000 | Loss: 0.00002524
Iteration 101/1000 | Loss: 0.00002523
Iteration 102/1000 | Loss: 0.00002523
Iteration 103/1000 | Loss: 0.00002523
Iteration 104/1000 | Loss: 0.00002523
Iteration 105/1000 | Loss: 0.00002523
Iteration 106/1000 | Loss: 0.00002523
Iteration 107/1000 | Loss: 0.00002523
Iteration 108/1000 | Loss: 0.00002523
Iteration 109/1000 | Loss: 0.00002523
Iteration 110/1000 | Loss: 0.00002523
Iteration 111/1000 | Loss: 0.00002522
Iteration 112/1000 | Loss: 0.00002522
Iteration 113/1000 | Loss: 0.00002522
Iteration 114/1000 | Loss: 0.00002522
Iteration 115/1000 | Loss: 0.00002521
Iteration 116/1000 | Loss: 0.00002521
Iteration 117/1000 | Loss: 0.00002521
Iteration 118/1000 | Loss: 0.00002520
Iteration 119/1000 | Loss: 0.00002520
Iteration 120/1000 | Loss: 0.00002520
Iteration 121/1000 | Loss: 0.00002520
Iteration 122/1000 | Loss: 0.00002520
Iteration 123/1000 | Loss: 0.00002519
Iteration 124/1000 | Loss: 0.00002519
Iteration 125/1000 | Loss: 0.00002519
Iteration 126/1000 | Loss: 0.00002519
Iteration 127/1000 | Loss: 0.00002519
Iteration 128/1000 | Loss: 0.00002519
Iteration 129/1000 | Loss: 0.00002518
Iteration 130/1000 | Loss: 0.00002518
Iteration 131/1000 | Loss: 0.00002518
Iteration 132/1000 | Loss: 0.00002518
Iteration 133/1000 | Loss: 0.00002518
Iteration 134/1000 | Loss: 0.00002518
Iteration 135/1000 | Loss: 0.00002518
Iteration 136/1000 | Loss: 0.00002518
Iteration 137/1000 | Loss: 0.00002518
Iteration 138/1000 | Loss: 0.00002518
Iteration 139/1000 | Loss: 0.00002518
Iteration 140/1000 | Loss: 0.00002518
Iteration 141/1000 | Loss: 0.00002518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.517730899853632e-05, 2.517730899853632e-05, 2.517730899853632e-05, 2.517730899853632e-05, 2.517730899853632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.517730899853632e-05

Optimization complete. Final v2v error: 4.173539638519287 mm

Highest mean error: 4.3829803466796875 mm for frame 115

Lowest mean error: 3.885529041290283 mm for frame 65

Saving results

Total time: 36.72525358200073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841344
Iteration 2/25 | Loss: 0.00093261
Iteration 3/25 | Loss: 0.00077135
Iteration 4/25 | Loss: 0.00071946
Iteration 5/25 | Loss: 0.00071111
Iteration 6/25 | Loss: 0.00070927
Iteration 7/25 | Loss: 0.00070881
Iteration 8/25 | Loss: 0.00070881
Iteration 9/25 | Loss: 0.00070878
Iteration 10/25 | Loss: 0.00070878
Iteration 11/25 | Loss: 0.00070878
Iteration 12/25 | Loss: 0.00070878
Iteration 13/25 | Loss: 0.00070878
Iteration 14/25 | Loss: 0.00070878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007087782141752541, 0.0007087782141752541, 0.0007087782141752541, 0.0007087782141752541, 0.0007087782141752541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007087782141752541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27252376
Iteration 2/25 | Loss: 0.00033702
Iteration 3/25 | Loss: 0.00033702
Iteration 4/25 | Loss: 0.00033702
Iteration 5/25 | Loss: 0.00033702
Iteration 6/25 | Loss: 0.00033702
Iteration 7/25 | Loss: 0.00033702
Iteration 8/25 | Loss: 0.00033702
Iteration 9/25 | Loss: 0.00033702
Iteration 10/25 | Loss: 0.00033702
Iteration 11/25 | Loss: 0.00033702
Iteration 12/25 | Loss: 0.00033702
Iteration 13/25 | Loss: 0.00033702
Iteration 14/25 | Loss: 0.00033702
Iteration 15/25 | Loss: 0.00033702
Iteration 16/25 | Loss: 0.00033702
Iteration 17/25 | Loss: 0.00033702
Iteration 18/25 | Loss: 0.00033702
Iteration 19/25 | Loss: 0.00033702
Iteration 20/25 | Loss: 0.00033702
Iteration 21/25 | Loss: 0.00033702
Iteration 22/25 | Loss: 0.00033702
Iteration 23/25 | Loss: 0.00033702
Iteration 24/25 | Loss: 0.00033702
Iteration 25/25 | Loss: 0.00033702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033702
Iteration 2/1000 | Loss: 0.00004681
Iteration 3/1000 | Loss: 0.00003155
Iteration 4/1000 | Loss: 0.00002884
Iteration 5/1000 | Loss: 0.00002796
Iteration 6/1000 | Loss: 0.00002708
Iteration 7/1000 | Loss: 0.00002650
Iteration 8/1000 | Loss: 0.00002601
Iteration 9/1000 | Loss: 0.00002560
Iteration 10/1000 | Loss: 0.00002525
Iteration 11/1000 | Loss: 0.00002512
Iteration 12/1000 | Loss: 0.00002509
Iteration 13/1000 | Loss: 0.00002509
Iteration 14/1000 | Loss: 0.00002492
Iteration 15/1000 | Loss: 0.00002481
Iteration 16/1000 | Loss: 0.00002481
Iteration 17/1000 | Loss: 0.00002475
Iteration 18/1000 | Loss: 0.00002475
Iteration 19/1000 | Loss: 0.00002472
Iteration 20/1000 | Loss: 0.00002471
Iteration 21/1000 | Loss: 0.00002471
Iteration 22/1000 | Loss: 0.00002470
Iteration 23/1000 | Loss: 0.00002470
Iteration 24/1000 | Loss: 0.00002470
Iteration 25/1000 | Loss: 0.00002469
Iteration 26/1000 | Loss: 0.00002469
Iteration 27/1000 | Loss: 0.00002469
Iteration 28/1000 | Loss: 0.00002469
Iteration 29/1000 | Loss: 0.00002469
Iteration 30/1000 | Loss: 0.00002468
Iteration 31/1000 | Loss: 0.00002468
Iteration 32/1000 | Loss: 0.00002468
Iteration 33/1000 | Loss: 0.00002468
Iteration 34/1000 | Loss: 0.00002468
Iteration 35/1000 | Loss: 0.00002467
Iteration 36/1000 | Loss: 0.00002467
Iteration 37/1000 | Loss: 0.00002467
Iteration 38/1000 | Loss: 0.00002467
Iteration 39/1000 | Loss: 0.00002467
Iteration 40/1000 | Loss: 0.00002466
Iteration 41/1000 | Loss: 0.00002466
Iteration 42/1000 | Loss: 0.00002466
Iteration 43/1000 | Loss: 0.00002466
Iteration 44/1000 | Loss: 0.00002466
Iteration 45/1000 | Loss: 0.00002466
Iteration 46/1000 | Loss: 0.00002466
Iteration 47/1000 | Loss: 0.00002465
Iteration 48/1000 | Loss: 0.00002465
Iteration 49/1000 | Loss: 0.00002465
Iteration 50/1000 | Loss: 0.00002465
Iteration 51/1000 | Loss: 0.00002464
Iteration 52/1000 | Loss: 0.00002464
Iteration 53/1000 | Loss: 0.00002464
Iteration 54/1000 | Loss: 0.00002464
Iteration 55/1000 | Loss: 0.00002463
Iteration 56/1000 | Loss: 0.00002463
Iteration 57/1000 | Loss: 0.00002463
Iteration 58/1000 | Loss: 0.00002463
Iteration 59/1000 | Loss: 0.00002463
Iteration 60/1000 | Loss: 0.00002462
Iteration 61/1000 | Loss: 0.00002462
Iteration 62/1000 | Loss: 0.00002462
Iteration 63/1000 | Loss: 0.00002462
Iteration 64/1000 | Loss: 0.00002462
Iteration 65/1000 | Loss: 0.00002462
Iteration 66/1000 | Loss: 0.00002462
Iteration 67/1000 | Loss: 0.00002462
Iteration 68/1000 | Loss: 0.00002462
Iteration 69/1000 | Loss: 0.00002461
Iteration 70/1000 | Loss: 0.00002461
Iteration 71/1000 | Loss: 0.00002461
Iteration 72/1000 | Loss: 0.00002461
Iteration 73/1000 | Loss: 0.00002461
Iteration 74/1000 | Loss: 0.00002460
Iteration 75/1000 | Loss: 0.00002460
Iteration 76/1000 | Loss: 0.00002460
Iteration 77/1000 | Loss: 0.00002459
Iteration 78/1000 | Loss: 0.00002459
Iteration 79/1000 | Loss: 0.00002459
Iteration 80/1000 | Loss: 0.00002459
Iteration 81/1000 | Loss: 0.00002458
Iteration 82/1000 | Loss: 0.00002458
Iteration 83/1000 | Loss: 0.00002458
Iteration 84/1000 | Loss: 0.00002458
Iteration 85/1000 | Loss: 0.00002457
Iteration 86/1000 | Loss: 0.00002457
Iteration 87/1000 | Loss: 0.00002456
Iteration 88/1000 | Loss: 0.00002456
Iteration 89/1000 | Loss: 0.00002456
Iteration 90/1000 | Loss: 0.00002456
Iteration 91/1000 | Loss: 0.00002456
Iteration 92/1000 | Loss: 0.00002455
Iteration 93/1000 | Loss: 0.00002455
Iteration 94/1000 | Loss: 0.00002455
Iteration 95/1000 | Loss: 0.00002455
Iteration 96/1000 | Loss: 0.00002455
Iteration 97/1000 | Loss: 0.00002455
Iteration 98/1000 | Loss: 0.00002454
Iteration 99/1000 | Loss: 0.00002454
Iteration 100/1000 | Loss: 0.00002454
Iteration 101/1000 | Loss: 0.00002454
Iteration 102/1000 | Loss: 0.00002454
Iteration 103/1000 | Loss: 0.00002454
Iteration 104/1000 | Loss: 0.00002453
Iteration 105/1000 | Loss: 0.00002453
Iteration 106/1000 | Loss: 0.00002453
Iteration 107/1000 | Loss: 0.00002453
Iteration 108/1000 | Loss: 0.00002452
Iteration 109/1000 | Loss: 0.00002452
Iteration 110/1000 | Loss: 0.00002452
Iteration 111/1000 | Loss: 0.00002452
Iteration 112/1000 | Loss: 0.00002452
Iteration 113/1000 | Loss: 0.00002452
Iteration 114/1000 | Loss: 0.00002452
Iteration 115/1000 | Loss: 0.00002452
Iteration 116/1000 | Loss: 0.00002452
Iteration 117/1000 | Loss: 0.00002452
Iteration 118/1000 | Loss: 0.00002452
Iteration 119/1000 | Loss: 0.00002451
Iteration 120/1000 | Loss: 0.00002451
Iteration 121/1000 | Loss: 0.00002451
Iteration 122/1000 | Loss: 0.00002451
Iteration 123/1000 | Loss: 0.00002451
Iteration 124/1000 | Loss: 0.00002451
Iteration 125/1000 | Loss: 0.00002451
Iteration 126/1000 | Loss: 0.00002451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.451448563078884e-05, 2.451448563078884e-05, 2.451448563078884e-05, 2.451448563078884e-05, 2.451448563078884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.451448563078884e-05

Optimization complete. Final v2v error: 4.138974666595459 mm

Highest mean error: 4.502895355224609 mm for frame 0

Lowest mean error: 3.966096878051758 mm for frame 185

Saving results

Total time: 38.165528297424316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777732
Iteration 2/25 | Loss: 0.00096346
Iteration 3/25 | Loss: 0.00075906
Iteration 4/25 | Loss: 0.00069382
Iteration 5/25 | Loss: 0.00067276
Iteration 6/25 | Loss: 0.00066973
Iteration 7/25 | Loss: 0.00066843
Iteration 8/25 | Loss: 0.00066829
Iteration 9/25 | Loss: 0.00066829
Iteration 10/25 | Loss: 0.00066829
Iteration 11/25 | Loss: 0.00066829
Iteration 12/25 | Loss: 0.00066829
Iteration 13/25 | Loss: 0.00066829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000668285705614835, 0.000668285705614835, 0.000668285705614835, 0.000668285705614835, 0.000668285705614835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000668285705614835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46813750
Iteration 2/25 | Loss: 0.00031351
Iteration 3/25 | Loss: 0.00031351
Iteration 4/25 | Loss: 0.00031351
Iteration 5/25 | Loss: 0.00031350
Iteration 6/25 | Loss: 0.00031350
Iteration 7/25 | Loss: 0.00031350
Iteration 8/25 | Loss: 0.00031350
Iteration 9/25 | Loss: 0.00031350
Iteration 10/25 | Loss: 0.00031350
Iteration 11/25 | Loss: 0.00031350
Iteration 12/25 | Loss: 0.00031350
Iteration 13/25 | Loss: 0.00031350
Iteration 14/25 | Loss: 0.00031350
Iteration 15/25 | Loss: 0.00031350
Iteration 16/25 | Loss: 0.00031350
Iteration 17/25 | Loss: 0.00031350
Iteration 18/25 | Loss: 0.00031350
Iteration 19/25 | Loss: 0.00031350
Iteration 20/25 | Loss: 0.00031350
Iteration 21/25 | Loss: 0.00031350
Iteration 22/25 | Loss: 0.00031350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003135034057777375, 0.0003135034057777375, 0.0003135034057777375, 0.0003135034057777375, 0.0003135034057777375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003135034057777375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031350
Iteration 2/1000 | Loss: 0.00003904
Iteration 3/1000 | Loss: 0.00002601
Iteration 4/1000 | Loss: 0.00002330
Iteration 5/1000 | Loss: 0.00002226
Iteration 6/1000 | Loss: 0.00002134
Iteration 7/1000 | Loss: 0.00002087
Iteration 8/1000 | Loss: 0.00002037
Iteration 9/1000 | Loss: 0.00002011
Iteration 10/1000 | Loss: 0.00001981
Iteration 11/1000 | Loss: 0.00001963
Iteration 12/1000 | Loss: 0.00001961
Iteration 13/1000 | Loss: 0.00001945
Iteration 14/1000 | Loss: 0.00001943
Iteration 15/1000 | Loss: 0.00001941
Iteration 16/1000 | Loss: 0.00001939
Iteration 17/1000 | Loss: 0.00001938
Iteration 18/1000 | Loss: 0.00001935
Iteration 19/1000 | Loss: 0.00001935
Iteration 20/1000 | Loss: 0.00001933
Iteration 21/1000 | Loss: 0.00001933
Iteration 22/1000 | Loss: 0.00001932
Iteration 23/1000 | Loss: 0.00001931
Iteration 24/1000 | Loss: 0.00001931
Iteration 25/1000 | Loss: 0.00001931
Iteration 26/1000 | Loss: 0.00001930
Iteration 27/1000 | Loss: 0.00001930
Iteration 28/1000 | Loss: 0.00001929
Iteration 29/1000 | Loss: 0.00001929
Iteration 30/1000 | Loss: 0.00001928
Iteration 31/1000 | Loss: 0.00001928
Iteration 32/1000 | Loss: 0.00001928
Iteration 33/1000 | Loss: 0.00001927
Iteration 34/1000 | Loss: 0.00001927
Iteration 35/1000 | Loss: 0.00001925
Iteration 36/1000 | Loss: 0.00001925
Iteration 37/1000 | Loss: 0.00001925
Iteration 38/1000 | Loss: 0.00001925
Iteration 39/1000 | Loss: 0.00001924
Iteration 40/1000 | Loss: 0.00001924
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00001923
Iteration 43/1000 | Loss: 0.00001922
Iteration 44/1000 | Loss: 0.00001922
Iteration 45/1000 | Loss: 0.00001921
Iteration 46/1000 | Loss: 0.00001921
Iteration 47/1000 | Loss: 0.00001920
Iteration 48/1000 | Loss: 0.00001920
Iteration 49/1000 | Loss: 0.00001920
Iteration 50/1000 | Loss: 0.00001919
Iteration 51/1000 | Loss: 0.00001919
Iteration 52/1000 | Loss: 0.00001919
Iteration 53/1000 | Loss: 0.00001918
Iteration 54/1000 | Loss: 0.00001918
Iteration 55/1000 | Loss: 0.00001918
Iteration 56/1000 | Loss: 0.00001917
Iteration 57/1000 | Loss: 0.00001917
Iteration 58/1000 | Loss: 0.00001917
Iteration 59/1000 | Loss: 0.00001917
Iteration 60/1000 | Loss: 0.00001916
Iteration 61/1000 | Loss: 0.00001916
Iteration 62/1000 | Loss: 0.00001916
Iteration 63/1000 | Loss: 0.00001916
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001915
Iteration 66/1000 | Loss: 0.00001915
Iteration 67/1000 | Loss: 0.00001915
Iteration 68/1000 | Loss: 0.00001915
Iteration 69/1000 | Loss: 0.00001915
Iteration 70/1000 | Loss: 0.00001915
Iteration 71/1000 | Loss: 0.00001915
Iteration 72/1000 | Loss: 0.00001915
Iteration 73/1000 | Loss: 0.00001915
Iteration 74/1000 | Loss: 0.00001915
Iteration 75/1000 | Loss: 0.00001915
Iteration 76/1000 | Loss: 0.00001914
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001914
Iteration 79/1000 | Loss: 0.00001914
Iteration 80/1000 | Loss: 0.00001914
Iteration 81/1000 | Loss: 0.00001914
Iteration 82/1000 | Loss: 0.00001914
Iteration 83/1000 | Loss: 0.00001914
Iteration 84/1000 | Loss: 0.00001913
Iteration 85/1000 | Loss: 0.00001913
Iteration 86/1000 | Loss: 0.00001913
Iteration 87/1000 | Loss: 0.00001913
Iteration 88/1000 | Loss: 0.00001913
Iteration 89/1000 | Loss: 0.00001913
Iteration 90/1000 | Loss: 0.00001913
Iteration 91/1000 | Loss: 0.00001913
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001912
Iteration 94/1000 | Loss: 0.00001912
Iteration 95/1000 | Loss: 0.00001912
Iteration 96/1000 | Loss: 0.00001912
Iteration 97/1000 | Loss: 0.00001912
Iteration 98/1000 | Loss: 0.00001912
Iteration 99/1000 | Loss: 0.00001911
Iteration 100/1000 | Loss: 0.00001911
Iteration 101/1000 | Loss: 0.00001911
Iteration 102/1000 | Loss: 0.00001911
Iteration 103/1000 | Loss: 0.00001911
Iteration 104/1000 | Loss: 0.00001911
Iteration 105/1000 | Loss: 0.00001911
Iteration 106/1000 | Loss: 0.00001911
Iteration 107/1000 | Loss: 0.00001911
Iteration 108/1000 | Loss: 0.00001911
Iteration 109/1000 | Loss: 0.00001911
Iteration 110/1000 | Loss: 0.00001911
Iteration 111/1000 | Loss: 0.00001911
Iteration 112/1000 | Loss: 0.00001911
Iteration 113/1000 | Loss: 0.00001911
Iteration 114/1000 | Loss: 0.00001911
Iteration 115/1000 | Loss: 0.00001911
Iteration 116/1000 | Loss: 0.00001911
Iteration 117/1000 | Loss: 0.00001910
Iteration 118/1000 | Loss: 0.00001910
Iteration 119/1000 | Loss: 0.00001910
Iteration 120/1000 | Loss: 0.00001910
Iteration 121/1000 | Loss: 0.00001910
Iteration 122/1000 | Loss: 0.00001910
Iteration 123/1000 | Loss: 0.00001910
Iteration 124/1000 | Loss: 0.00001910
Iteration 125/1000 | Loss: 0.00001910
Iteration 126/1000 | Loss: 0.00001910
Iteration 127/1000 | Loss: 0.00001910
Iteration 128/1000 | Loss: 0.00001910
Iteration 129/1000 | Loss: 0.00001910
Iteration 130/1000 | Loss: 0.00001910
Iteration 131/1000 | Loss: 0.00001909
Iteration 132/1000 | Loss: 0.00001909
Iteration 133/1000 | Loss: 0.00001909
Iteration 134/1000 | Loss: 0.00001909
Iteration 135/1000 | Loss: 0.00001909
Iteration 136/1000 | Loss: 0.00001909
Iteration 137/1000 | Loss: 0.00001909
Iteration 138/1000 | Loss: 0.00001909
Iteration 139/1000 | Loss: 0.00001909
Iteration 140/1000 | Loss: 0.00001909
Iteration 141/1000 | Loss: 0.00001909
Iteration 142/1000 | Loss: 0.00001909
Iteration 143/1000 | Loss: 0.00001909
Iteration 144/1000 | Loss: 0.00001909
Iteration 145/1000 | Loss: 0.00001909
Iteration 146/1000 | Loss: 0.00001909
Iteration 147/1000 | Loss: 0.00001909
Iteration 148/1000 | Loss: 0.00001909
Iteration 149/1000 | Loss: 0.00001909
Iteration 150/1000 | Loss: 0.00001909
Iteration 151/1000 | Loss: 0.00001909
Iteration 152/1000 | Loss: 0.00001909
Iteration 153/1000 | Loss: 0.00001909
Iteration 154/1000 | Loss: 0.00001909
Iteration 155/1000 | Loss: 0.00001909
Iteration 156/1000 | Loss: 0.00001909
Iteration 157/1000 | Loss: 0.00001909
Iteration 158/1000 | Loss: 0.00001909
Iteration 159/1000 | Loss: 0.00001909
Iteration 160/1000 | Loss: 0.00001909
Iteration 161/1000 | Loss: 0.00001909
Iteration 162/1000 | Loss: 0.00001909
Iteration 163/1000 | Loss: 0.00001909
Iteration 164/1000 | Loss: 0.00001909
Iteration 165/1000 | Loss: 0.00001909
Iteration 166/1000 | Loss: 0.00001909
Iteration 167/1000 | Loss: 0.00001909
Iteration 168/1000 | Loss: 0.00001909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.9085919120698236e-05, 1.9085919120698236e-05, 1.9085919120698236e-05, 1.9085919120698236e-05, 1.9085919120698236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9085919120698236e-05

Optimization complete. Final v2v error: 3.6471471786499023 mm

Highest mean error: 4.5077643394470215 mm for frame 67

Lowest mean error: 2.9919793605804443 mm for frame 110

Saving results

Total time: 44.03767108917236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01116585
Iteration 2/25 | Loss: 0.00350690
Iteration 3/25 | Loss: 0.00163245
Iteration 4/25 | Loss: 0.00143847
Iteration 5/25 | Loss: 0.00178627
Iteration 6/25 | Loss: 0.00127191
Iteration 7/25 | Loss: 0.00103399
Iteration 8/25 | Loss: 0.00088617
Iteration 9/25 | Loss: 0.00085191
Iteration 10/25 | Loss: 0.00085857
Iteration 11/25 | Loss: 0.00082606
Iteration 12/25 | Loss: 0.00082250
Iteration 13/25 | Loss: 0.00081907
Iteration 14/25 | Loss: 0.00081629
Iteration 15/25 | Loss: 0.00081762
Iteration 16/25 | Loss: 0.00081518
Iteration 17/25 | Loss: 0.00081272
Iteration 18/25 | Loss: 0.00081323
Iteration 19/25 | Loss: 0.00080794
Iteration 20/25 | Loss: 0.00081080
Iteration 21/25 | Loss: 0.00081153
Iteration 22/25 | Loss: 0.00080749
Iteration 23/25 | Loss: 0.00080813
Iteration 24/25 | Loss: 0.00080925
Iteration 25/25 | Loss: 0.00080873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56590348
Iteration 2/25 | Loss: 0.00086934
Iteration 3/25 | Loss: 0.00040105
Iteration 4/25 | Loss: 0.00037484
Iteration 5/25 | Loss: 0.00037484
Iteration 6/25 | Loss: 0.00037484
Iteration 7/25 | Loss: 0.00037484
Iteration 8/25 | Loss: 0.00037484
Iteration 9/25 | Loss: 0.00037484
Iteration 10/25 | Loss: 0.00037484
Iteration 11/25 | Loss: 0.00037483
Iteration 12/25 | Loss: 0.00037483
Iteration 13/25 | Loss: 0.00037483
Iteration 14/25 | Loss: 0.00037483
Iteration 15/25 | Loss: 0.00037483
Iteration 16/25 | Loss: 0.00037483
Iteration 17/25 | Loss: 0.00037483
Iteration 18/25 | Loss: 0.00037483
Iteration 19/25 | Loss: 0.00037483
Iteration 20/25 | Loss: 0.00037483
Iteration 21/25 | Loss: 0.00037483
Iteration 22/25 | Loss: 0.00037483
Iteration 23/25 | Loss: 0.00037483
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003748344606719911, 0.0003748344606719911, 0.0003748344606719911, 0.0003748344606719911, 0.0003748344606719911]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003748344606719911

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037483
Iteration 2/1000 | Loss: 0.00051358
Iteration 3/1000 | Loss: 0.00010341
Iteration 4/1000 | Loss: 0.00035036
Iteration 5/1000 | Loss: 0.00013386
Iteration 6/1000 | Loss: 0.00018267
Iteration 7/1000 | Loss: 0.00006177
Iteration 8/1000 | Loss: 0.00002322
Iteration 9/1000 | Loss: 0.00003810
Iteration 10/1000 | Loss: 0.00002399
Iteration 11/1000 | Loss: 0.00002351
Iteration 12/1000 | Loss: 0.00003943
Iteration 13/1000 | Loss: 0.00005408
Iteration 14/1000 | Loss: 0.00016573
Iteration 15/1000 | Loss: 0.00007566
Iteration 16/1000 | Loss: 0.00007594
Iteration 17/1000 | Loss: 0.00002232
Iteration 18/1000 | Loss: 0.00004315
Iteration 19/1000 | Loss: 0.00002407
Iteration 20/1000 | Loss: 0.00002024
Iteration 21/1000 | Loss: 0.00002024
Iteration 22/1000 | Loss: 0.00002000
Iteration 23/1000 | Loss: 0.00002000
Iteration 24/1000 | Loss: 0.00001999
Iteration 25/1000 | Loss: 0.00001999
Iteration 26/1000 | Loss: 0.00001999
Iteration 27/1000 | Loss: 0.00001999
Iteration 28/1000 | Loss: 0.00001999
Iteration 29/1000 | Loss: 0.00001999
Iteration 30/1000 | Loss: 0.00001999
Iteration 31/1000 | Loss: 0.00001999
Iteration 32/1000 | Loss: 0.00001999
Iteration 33/1000 | Loss: 0.00002008
Iteration 34/1000 | Loss: 0.00001998
Iteration 35/1000 | Loss: 0.00001997
Iteration 36/1000 | Loss: 0.00002257
Iteration 37/1000 | Loss: 0.00001993
Iteration 38/1000 | Loss: 0.00002176
Iteration 39/1000 | Loss: 0.00002708
Iteration 40/1000 | Loss: 0.00002671
Iteration 41/1000 | Loss: 0.00002279
Iteration 42/1000 | Loss: 0.00002231
Iteration 43/1000 | Loss: 0.00002069
Iteration 44/1000 | Loss: 0.00002010
Iteration 45/1000 | Loss: 0.00001982
Iteration 46/1000 | Loss: 0.00001982
Iteration 47/1000 | Loss: 0.00001982
Iteration 48/1000 | Loss: 0.00001982
Iteration 49/1000 | Loss: 0.00001982
Iteration 50/1000 | Loss: 0.00001982
Iteration 51/1000 | Loss: 0.00001982
Iteration 52/1000 | Loss: 0.00001982
Iteration 53/1000 | Loss: 0.00001982
Iteration 54/1000 | Loss: 0.00001982
Iteration 55/1000 | Loss: 0.00001982
Iteration 56/1000 | Loss: 0.00001982
Iteration 57/1000 | Loss: 0.00001982
Iteration 58/1000 | Loss: 0.00001982
Iteration 59/1000 | Loss: 0.00001982
Iteration 60/1000 | Loss: 0.00001982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.981626519409474e-05, 1.981626519409474e-05, 1.981626519409474e-05, 1.981626519409474e-05, 1.981626519409474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.981626519409474e-05

Optimization complete. Final v2v error: 3.7715137004852295 mm

Highest mean error: 9.579310417175293 mm for frame 51

Lowest mean error: 3.5297067165374756 mm for frame 115

Saving results

Total time: 98.11911797523499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043126
Iteration 2/25 | Loss: 0.00161907
Iteration 3/25 | Loss: 0.00102152
Iteration 4/25 | Loss: 0.00094330
Iteration 5/25 | Loss: 0.00090179
Iteration 6/25 | Loss: 0.00087257
Iteration 7/25 | Loss: 0.00088106
Iteration 8/25 | Loss: 0.00085177
Iteration 9/25 | Loss: 0.00080996
Iteration 10/25 | Loss: 0.00079659
Iteration 11/25 | Loss: 0.00078525
Iteration 12/25 | Loss: 0.00076491
Iteration 13/25 | Loss: 0.00078304
Iteration 14/25 | Loss: 0.00077656
Iteration 15/25 | Loss: 0.00075778
Iteration 16/25 | Loss: 0.00075141
Iteration 17/25 | Loss: 0.00073917
Iteration 18/25 | Loss: 0.00073539
Iteration 19/25 | Loss: 0.00073363
Iteration 20/25 | Loss: 0.00073264
Iteration 21/25 | Loss: 0.00073234
Iteration 22/25 | Loss: 0.00073222
Iteration 23/25 | Loss: 0.00073209
Iteration 24/25 | Loss: 0.00073206
Iteration 25/25 | Loss: 0.00073206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95828247
Iteration 2/25 | Loss: 0.00020613
Iteration 3/25 | Loss: 0.00020611
Iteration 4/25 | Loss: 0.00020611
Iteration 5/25 | Loss: 0.00020611
Iteration 6/25 | Loss: 0.00020611
Iteration 7/25 | Loss: 0.00020611
Iteration 8/25 | Loss: 0.00020611
Iteration 9/25 | Loss: 0.00020611
Iteration 10/25 | Loss: 0.00020611
Iteration 11/25 | Loss: 0.00020611
Iteration 12/25 | Loss: 0.00020611
Iteration 13/25 | Loss: 0.00020611
Iteration 14/25 | Loss: 0.00020611
Iteration 15/25 | Loss: 0.00020611
Iteration 16/25 | Loss: 0.00020611
Iteration 17/25 | Loss: 0.00020611
Iteration 18/25 | Loss: 0.00020611
Iteration 19/25 | Loss: 0.00020611
Iteration 20/25 | Loss: 0.00020611
Iteration 21/25 | Loss: 0.00020611
Iteration 22/25 | Loss: 0.00020611
Iteration 23/25 | Loss: 0.00020611
Iteration 24/25 | Loss: 0.00020611
Iteration 25/25 | Loss: 0.00020611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020611
Iteration 2/1000 | Loss: 0.00003187
Iteration 3/1000 | Loss: 0.00002580
Iteration 4/1000 | Loss: 0.00002473
Iteration 5/1000 | Loss: 0.00002369
Iteration 6/1000 | Loss: 0.00022543
Iteration 7/1000 | Loss: 0.00002276
Iteration 8/1000 | Loss: 0.00002227
Iteration 9/1000 | Loss: 0.00002202
Iteration 10/1000 | Loss: 0.00002197
Iteration 11/1000 | Loss: 0.00002193
Iteration 12/1000 | Loss: 0.00002193
Iteration 13/1000 | Loss: 0.00002192
Iteration 14/1000 | Loss: 0.00002181
Iteration 15/1000 | Loss: 0.00002177
Iteration 16/1000 | Loss: 0.00002176
Iteration 17/1000 | Loss: 0.00002176
Iteration 18/1000 | Loss: 0.00002175
Iteration 19/1000 | Loss: 0.00002175
Iteration 20/1000 | Loss: 0.00002174
Iteration 21/1000 | Loss: 0.00002173
Iteration 22/1000 | Loss: 0.00002170
Iteration 23/1000 | Loss: 0.00002170
Iteration 24/1000 | Loss: 0.00002170
Iteration 25/1000 | Loss: 0.00002169
Iteration 26/1000 | Loss: 0.00002169
Iteration 27/1000 | Loss: 0.00002169
Iteration 28/1000 | Loss: 0.00002168
Iteration 29/1000 | Loss: 0.00002168
Iteration 30/1000 | Loss: 0.00002168
Iteration 31/1000 | Loss: 0.00002167
Iteration 32/1000 | Loss: 0.00002167
Iteration 33/1000 | Loss: 0.00002166
Iteration 34/1000 | Loss: 0.00002166
Iteration 35/1000 | Loss: 0.00002166
Iteration 36/1000 | Loss: 0.00002166
Iteration 37/1000 | Loss: 0.00002166
Iteration 38/1000 | Loss: 0.00002164
Iteration 39/1000 | Loss: 0.00002163
Iteration 40/1000 | Loss: 0.00002163
Iteration 41/1000 | Loss: 0.00002163
Iteration 42/1000 | Loss: 0.00002163
Iteration 43/1000 | Loss: 0.00002162
Iteration 44/1000 | Loss: 0.00002162
Iteration 45/1000 | Loss: 0.00002161
Iteration 46/1000 | Loss: 0.00002161
Iteration 47/1000 | Loss: 0.00002161
Iteration 48/1000 | Loss: 0.00002160
Iteration 49/1000 | Loss: 0.00002160
Iteration 50/1000 | Loss: 0.00002160
Iteration 51/1000 | Loss: 0.00002160
Iteration 52/1000 | Loss: 0.00002160
Iteration 53/1000 | Loss: 0.00002160
Iteration 54/1000 | Loss: 0.00002160
Iteration 55/1000 | Loss: 0.00002160
Iteration 56/1000 | Loss: 0.00002160
Iteration 57/1000 | Loss: 0.00002159
Iteration 58/1000 | Loss: 0.00002159
Iteration 59/1000 | Loss: 0.00002159
Iteration 60/1000 | Loss: 0.00002159
Iteration 61/1000 | Loss: 0.00002159
Iteration 62/1000 | Loss: 0.00002159
Iteration 63/1000 | Loss: 0.00002159
Iteration 64/1000 | Loss: 0.00002159
Iteration 65/1000 | Loss: 0.00002159
Iteration 66/1000 | Loss: 0.00002158
Iteration 67/1000 | Loss: 0.00002158
Iteration 68/1000 | Loss: 0.00002158
Iteration 69/1000 | Loss: 0.00002158
Iteration 70/1000 | Loss: 0.00002158
Iteration 71/1000 | Loss: 0.00002158
Iteration 72/1000 | Loss: 0.00002158
Iteration 73/1000 | Loss: 0.00002158
Iteration 74/1000 | Loss: 0.00002158
Iteration 75/1000 | Loss: 0.00002158
Iteration 76/1000 | Loss: 0.00002158
Iteration 77/1000 | Loss: 0.00002157
Iteration 78/1000 | Loss: 0.00002157
Iteration 79/1000 | Loss: 0.00002157
Iteration 80/1000 | Loss: 0.00002157
Iteration 81/1000 | Loss: 0.00002157
Iteration 82/1000 | Loss: 0.00002157
Iteration 83/1000 | Loss: 0.00002157
Iteration 84/1000 | Loss: 0.00002157
Iteration 85/1000 | Loss: 0.00002157
Iteration 86/1000 | Loss: 0.00002157
Iteration 87/1000 | Loss: 0.00002157
Iteration 88/1000 | Loss: 0.00002157
Iteration 89/1000 | Loss: 0.00002157
Iteration 90/1000 | Loss: 0.00002157
Iteration 91/1000 | Loss: 0.00002157
Iteration 92/1000 | Loss: 0.00002157
Iteration 93/1000 | Loss: 0.00002157
Iteration 94/1000 | Loss: 0.00002157
Iteration 95/1000 | Loss: 0.00002157
Iteration 96/1000 | Loss: 0.00002157
Iteration 97/1000 | Loss: 0.00002157
Iteration 98/1000 | Loss: 0.00002157
Iteration 99/1000 | Loss: 0.00002157
Iteration 100/1000 | Loss: 0.00002157
Iteration 101/1000 | Loss: 0.00002157
Iteration 102/1000 | Loss: 0.00002157
Iteration 103/1000 | Loss: 0.00002157
Iteration 104/1000 | Loss: 0.00002157
Iteration 105/1000 | Loss: 0.00002157
Iteration 106/1000 | Loss: 0.00002157
Iteration 107/1000 | Loss: 0.00002157
Iteration 108/1000 | Loss: 0.00002157
Iteration 109/1000 | Loss: 0.00002157
Iteration 110/1000 | Loss: 0.00002157
Iteration 111/1000 | Loss: 0.00002157
Iteration 112/1000 | Loss: 0.00002157
Iteration 113/1000 | Loss: 0.00002157
Iteration 114/1000 | Loss: 0.00002157
Iteration 115/1000 | Loss: 0.00002157
Iteration 116/1000 | Loss: 0.00002157
Iteration 117/1000 | Loss: 0.00002157
Iteration 118/1000 | Loss: 0.00002157
Iteration 119/1000 | Loss: 0.00002157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.1574422135017812e-05, 2.1574422135017812e-05, 2.1574422135017812e-05, 2.1574422135017812e-05, 2.1574422135017812e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1574422135017812e-05

Optimization complete. Final v2v error: 3.974649667739868 mm

Highest mean error: 4.742827892303467 mm for frame 94

Lowest mean error: 3.713886022567749 mm for frame 18

Saving results

Total time: 71.77689838409424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544777
Iteration 2/25 | Loss: 0.00110444
Iteration 3/25 | Loss: 0.00069838
Iteration 4/25 | Loss: 0.00068049
Iteration 5/25 | Loss: 0.00062218
Iteration 6/25 | Loss: 0.00061881
Iteration 7/25 | Loss: 0.00061768
Iteration 8/25 | Loss: 0.00061735
Iteration 9/25 | Loss: 0.00061728
Iteration 10/25 | Loss: 0.00061728
Iteration 11/25 | Loss: 0.00061728
Iteration 12/25 | Loss: 0.00061728
Iteration 13/25 | Loss: 0.00061728
Iteration 14/25 | Loss: 0.00061728
Iteration 15/25 | Loss: 0.00061728
Iteration 16/25 | Loss: 0.00061728
Iteration 17/25 | Loss: 0.00061728
Iteration 18/25 | Loss: 0.00061728
Iteration 19/25 | Loss: 0.00061728
Iteration 20/25 | Loss: 0.00061728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006172816501930356, 0.0006172816501930356, 0.0006172816501930356, 0.0006172816501930356, 0.0006172816501930356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006172816501930356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.31916142
Iteration 2/25 | Loss: 0.00037940
Iteration 3/25 | Loss: 0.00037937
Iteration 4/25 | Loss: 0.00037937
Iteration 5/25 | Loss: 0.00037937
Iteration 6/25 | Loss: 0.00037937
Iteration 7/25 | Loss: 0.00037937
Iteration 8/25 | Loss: 0.00037937
Iteration 9/25 | Loss: 0.00037937
Iteration 10/25 | Loss: 0.00037937
Iteration 11/25 | Loss: 0.00037937
Iteration 12/25 | Loss: 0.00037937
Iteration 13/25 | Loss: 0.00037937
Iteration 14/25 | Loss: 0.00037937
Iteration 15/25 | Loss: 0.00037937
Iteration 16/25 | Loss: 0.00037937
Iteration 17/25 | Loss: 0.00037937
Iteration 18/25 | Loss: 0.00037937
Iteration 19/25 | Loss: 0.00037937
Iteration 20/25 | Loss: 0.00037937
Iteration 21/25 | Loss: 0.00037937
Iteration 22/25 | Loss: 0.00037937
Iteration 23/25 | Loss: 0.00037937
Iteration 24/25 | Loss: 0.00037937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003793695359490812, 0.0003793695359490812, 0.0003793695359490812, 0.0003793695359490812, 0.0003793695359490812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003793695359490812

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037937
Iteration 2/1000 | Loss: 0.00001936
Iteration 3/1000 | Loss: 0.00001349
Iteration 4/1000 | Loss: 0.00001259
Iteration 5/1000 | Loss: 0.00002907
Iteration 6/1000 | Loss: 0.00001193
Iteration 7/1000 | Loss: 0.00001159
Iteration 8/1000 | Loss: 0.00001154
Iteration 9/1000 | Loss: 0.00001153
Iteration 10/1000 | Loss: 0.00001134
Iteration 11/1000 | Loss: 0.00001129
Iteration 12/1000 | Loss: 0.00001129
Iteration 13/1000 | Loss: 0.00001129
Iteration 14/1000 | Loss: 0.00001128
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001126
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001125
Iteration 20/1000 | Loss: 0.00001125
Iteration 21/1000 | Loss: 0.00001124
Iteration 22/1000 | Loss: 0.00003070
Iteration 23/1000 | Loss: 0.00010058
Iteration 24/1000 | Loss: 0.00001121
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001114
Iteration 28/1000 | Loss: 0.00001113
Iteration 29/1000 | Loss: 0.00001112
Iteration 30/1000 | Loss: 0.00001111
Iteration 31/1000 | Loss: 0.00001111
Iteration 32/1000 | Loss: 0.00001110
Iteration 33/1000 | Loss: 0.00001109
Iteration 34/1000 | Loss: 0.00002925
Iteration 35/1000 | Loss: 0.00001108
Iteration 36/1000 | Loss: 0.00001105
Iteration 37/1000 | Loss: 0.00001105
Iteration 38/1000 | Loss: 0.00001104
Iteration 39/1000 | Loss: 0.00001103
Iteration 40/1000 | Loss: 0.00001103
Iteration 41/1000 | Loss: 0.00001103
Iteration 42/1000 | Loss: 0.00001102
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001101
Iteration 45/1000 | Loss: 0.00001101
Iteration 46/1000 | Loss: 0.00001101
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001100
Iteration 49/1000 | Loss: 0.00001100
Iteration 50/1000 | Loss: 0.00001100
Iteration 51/1000 | Loss: 0.00001100
Iteration 52/1000 | Loss: 0.00001100
Iteration 53/1000 | Loss: 0.00001100
Iteration 54/1000 | Loss: 0.00001099
Iteration 55/1000 | Loss: 0.00001099
Iteration 56/1000 | Loss: 0.00001099
Iteration 57/1000 | Loss: 0.00001099
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001099
Iteration 60/1000 | Loss: 0.00001099
Iteration 61/1000 | Loss: 0.00001098
Iteration 62/1000 | Loss: 0.00001098
Iteration 63/1000 | Loss: 0.00001098
Iteration 64/1000 | Loss: 0.00001097
Iteration 65/1000 | Loss: 0.00001097
Iteration 66/1000 | Loss: 0.00001097
Iteration 67/1000 | Loss: 0.00001097
Iteration 68/1000 | Loss: 0.00001096
Iteration 69/1000 | Loss: 0.00001096
Iteration 70/1000 | Loss: 0.00001096
Iteration 71/1000 | Loss: 0.00001096
Iteration 72/1000 | Loss: 0.00001096
Iteration 73/1000 | Loss: 0.00001096
Iteration 74/1000 | Loss: 0.00001096
Iteration 75/1000 | Loss: 0.00001096
Iteration 76/1000 | Loss: 0.00001096
Iteration 77/1000 | Loss: 0.00001096
Iteration 78/1000 | Loss: 0.00001095
Iteration 79/1000 | Loss: 0.00001095
Iteration 80/1000 | Loss: 0.00001095
Iteration 81/1000 | Loss: 0.00001095
Iteration 82/1000 | Loss: 0.00001095
Iteration 83/1000 | Loss: 0.00001095
Iteration 84/1000 | Loss: 0.00001094
Iteration 85/1000 | Loss: 0.00001094
Iteration 86/1000 | Loss: 0.00001094
Iteration 87/1000 | Loss: 0.00001094
Iteration 88/1000 | Loss: 0.00001093
Iteration 89/1000 | Loss: 0.00001093
Iteration 90/1000 | Loss: 0.00001093
Iteration 91/1000 | Loss: 0.00001093
Iteration 92/1000 | Loss: 0.00001093
Iteration 93/1000 | Loss: 0.00001092
Iteration 94/1000 | Loss: 0.00001092
Iteration 95/1000 | Loss: 0.00001092
Iteration 96/1000 | Loss: 0.00001092
Iteration 97/1000 | Loss: 0.00001092
Iteration 98/1000 | Loss: 0.00001092
Iteration 99/1000 | Loss: 0.00001092
Iteration 100/1000 | Loss: 0.00001091
Iteration 101/1000 | Loss: 0.00001091
Iteration 102/1000 | Loss: 0.00001091
Iteration 103/1000 | Loss: 0.00001091
Iteration 104/1000 | Loss: 0.00001091
Iteration 105/1000 | Loss: 0.00001091
Iteration 106/1000 | Loss: 0.00001091
Iteration 107/1000 | Loss: 0.00001091
Iteration 108/1000 | Loss: 0.00001091
Iteration 109/1000 | Loss: 0.00001091
Iteration 110/1000 | Loss: 0.00001091
Iteration 111/1000 | Loss: 0.00001091
Iteration 112/1000 | Loss: 0.00001091
Iteration 113/1000 | Loss: 0.00001091
Iteration 114/1000 | Loss: 0.00001091
Iteration 115/1000 | Loss: 0.00001090
Iteration 116/1000 | Loss: 0.00001090
Iteration 117/1000 | Loss: 0.00001090
Iteration 118/1000 | Loss: 0.00001090
Iteration 119/1000 | Loss: 0.00001090
Iteration 120/1000 | Loss: 0.00001090
Iteration 121/1000 | Loss: 0.00001090
Iteration 122/1000 | Loss: 0.00001090
Iteration 123/1000 | Loss: 0.00001090
Iteration 124/1000 | Loss: 0.00002854
Iteration 125/1000 | Loss: 0.00001093
Iteration 126/1000 | Loss: 0.00001090
Iteration 127/1000 | Loss: 0.00001090
Iteration 128/1000 | Loss: 0.00001089
Iteration 129/1000 | Loss: 0.00001088
Iteration 130/1000 | Loss: 0.00001088
Iteration 131/1000 | Loss: 0.00001088
Iteration 132/1000 | Loss: 0.00001087
Iteration 133/1000 | Loss: 0.00001087
Iteration 134/1000 | Loss: 0.00001087
Iteration 135/1000 | Loss: 0.00001087
Iteration 136/1000 | Loss: 0.00001087
Iteration 137/1000 | Loss: 0.00001087
Iteration 138/1000 | Loss: 0.00001087
Iteration 139/1000 | Loss: 0.00001087
Iteration 140/1000 | Loss: 0.00001087
Iteration 141/1000 | Loss: 0.00001087
Iteration 142/1000 | Loss: 0.00001087
Iteration 143/1000 | Loss: 0.00001087
Iteration 144/1000 | Loss: 0.00001087
Iteration 145/1000 | Loss: 0.00001087
Iteration 146/1000 | Loss: 0.00001087
Iteration 147/1000 | Loss: 0.00001087
Iteration 148/1000 | Loss: 0.00001087
Iteration 149/1000 | Loss: 0.00001087
Iteration 150/1000 | Loss: 0.00001087
Iteration 151/1000 | Loss: 0.00001087
Iteration 152/1000 | Loss: 0.00001087
Iteration 153/1000 | Loss: 0.00001087
Iteration 154/1000 | Loss: 0.00001087
Iteration 155/1000 | Loss: 0.00001087
Iteration 156/1000 | Loss: 0.00001087
Iteration 157/1000 | Loss: 0.00001087
Iteration 158/1000 | Loss: 0.00001087
Iteration 159/1000 | Loss: 0.00001087
Iteration 160/1000 | Loss: 0.00001087
Iteration 161/1000 | Loss: 0.00001087
Iteration 162/1000 | Loss: 0.00001087
Iteration 163/1000 | Loss: 0.00001087
Iteration 164/1000 | Loss: 0.00001087
Iteration 165/1000 | Loss: 0.00001087
Iteration 166/1000 | Loss: 0.00001087
Iteration 167/1000 | Loss: 0.00001087
Iteration 168/1000 | Loss: 0.00001087
Iteration 169/1000 | Loss: 0.00001087
Iteration 170/1000 | Loss: 0.00001087
Iteration 171/1000 | Loss: 0.00001087
Iteration 172/1000 | Loss: 0.00001087
Iteration 173/1000 | Loss: 0.00001087
Iteration 174/1000 | Loss: 0.00001087
Iteration 175/1000 | Loss: 0.00001087
Iteration 176/1000 | Loss: 0.00001087
Iteration 177/1000 | Loss: 0.00001087
Iteration 178/1000 | Loss: 0.00001087
Iteration 179/1000 | Loss: 0.00001087
Iteration 180/1000 | Loss: 0.00001087
Iteration 181/1000 | Loss: 0.00001087
Iteration 182/1000 | Loss: 0.00001087
Iteration 183/1000 | Loss: 0.00001087
Iteration 184/1000 | Loss: 0.00001087
Iteration 185/1000 | Loss: 0.00001087
Iteration 186/1000 | Loss: 0.00001087
Iteration 187/1000 | Loss: 0.00001087
Iteration 188/1000 | Loss: 0.00001087
Iteration 189/1000 | Loss: 0.00001087
Iteration 190/1000 | Loss: 0.00001087
Iteration 191/1000 | Loss: 0.00001087
Iteration 192/1000 | Loss: 0.00001087
Iteration 193/1000 | Loss: 0.00001087
Iteration 194/1000 | Loss: 0.00001087
Iteration 195/1000 | Loss: 0.00001087
Iteration 196/1000 | Loss: 0.00001087
Iteration 197/1000 | Loss: 0.00001087
Iteration 198/1000 | Loss: 0.00001087
Iteration 199/1000 | Loss: 0.00001087
Iteration 200/1000 | Loss: 0.00001087
Iteration 201/1000 | Loss: 0.00001087
Iteration 202/1000 | Loss: 0.00001087
Iteration 203/1000 | Loss: 0.00001087
Iteration 204/1000 | Loss: 0.00001087
Iteration 205/1000 | Loss: 0.00001087
Iteration 206/1000 | Loss: 0.00001087
Iteration 207/1000 | Loss: 0.00001087
Iteration 208/1000 | Loss: 0.00001087
Iteration 209/1000 | Loss: 0.00001087
Iteration 210/1000 | Loss: 0.00001087
Iteration 211/1000 | Loss: 0.00001087
Iteration 212/1000 | Loss: 0.00001087
Iteration 213/1000 | Loss: 0.00001087
Iteration 214/1000 | Loss: 0.00001087
Iteration 215/1000 | Loss: 0.00001087
Iteration 216/1000 | Loss: 0.00001087
Iteration 217/1000 | Loss: 0.00001087
Iteration 218/1000 | Loss: 0.00001087
Iteration 219/1000 | Loss: 0.00001087
Iteration 220/1000 | Loss: 0.00001087
Iteration 221/1000 | Loss: 0.00001087
Iteration 222/1000 | Loss: 0.00001087
Iteration 223/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.0870247024286073e-05, 1.0870247024286073e-05, 1.0870247024286073e-05, 1.0870247024286073e-05, 1.0870247024286073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0870247024286073e-05

Optimization complete. Final v2v error: 2.769230365753174 mm

Highest mean error: 3.478316068649292 mm for frame 166

Lowest mean error: 2.410102128982544 mm for frame 143

Saving results

Total time: 50.09783148765564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426178
Iteration 2/25 | Loss: 0.00094143
Iteration 3/25 | Loss: 0.00066946
Iteration 4/25 | Loss: 0.00064160
Iteration 5/25 | Loss: 0.00063632
Iteration 6/25 | Loss: 0.00063469
Iteration 7/25 | Loss: 0.00063427
Iteration 8/25 | Loss: 0.00063427
Iteration 9/25 | Loss: 0.00063427
Iteration 10/25 | Loss: 0.00063427
Iteration 11/25 | Loss: 0.00063427
Iteration 12/25 | Loss: 0.00063427
Iteration 13/25 | Loss: 0.00063427
Iteration 14/25 | Loss: 0.00063427
Iteration 15/25 | Loss: 0.00063427
Iteration 16/25 | Loss: 0.00063427
Iteration 17/25 | Loss: 0.00063427
Iteration 18/25 | Loss: 0.00063427
Iteration 19/25 | Loss: 0.00063427
Iteration 20/25 | Loss: 0.00063427
Iteration 21/25 | Loss: 0.00063427
Iteration 22/25 | Loss: 0.00063427
Iteration 23/25 | Loss: 0.00063427
Iteration 24/25 | Loss: 0.00063427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006342710694298148, 0.0006342710694298148, 0.0006342710694298148, 0.0006342710694298148, 0.0006342710694298148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006342710694298148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64333057
Iteration 2/25 | Loss: 0.00037008
Iteration 3/25 | Loss: 0.00037008
Iteration 4/25 | Loss: 0.00037008
Iteration 5/25 | Loss: 0.00037008
Iteration 6/25 | Loss: 0.00037008
Iteration 7/25 | Loss: 0.00037008
Iteration 8/25 | Loss: 0.00037007
Iteration 9/25 | Loss: 0.00037007
Iteration 10/25 | Loss: 0.00037007
Iteration 11/25 | Loss: 0.00037007
Iteration 12/25 | Loss: 0.00037007
Iteration 13/25 | Loss: 0.00037007
Iteration 14/25 | Loss: 0.00037007
Iteration 15/25 | Loss: 0.00037007
Iteration 16/25 | Loss: 0.00037007
Iteration 17/25 | Loss: 0.00037007
Iteration 18/25 | Loss: 0.00037007
Iteration 19/25 | Loss: 0.00037007
Iteration 20/25 | Loss: 0.00037007
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000370074063539505, 0.000370074063539505, 0.000370074063539505, 0.000370074063539505, 0.000370074063539505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000370074063539505

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037007
Iteration 2/1000 | Loss: 0.00002013
Iteration 3/1000 | Loss: 0.00001523
Iteration 4/1000 | Loss: 0.00001401
Iteration 5/1000 | Loss: 0.00001351
Iteration 6/1000 | Loss: 0.00001317
Iteration 7/1000 | Loss: 0.00001288
Iteration 8/1000 | Loss: 0.00001273
Iteration 9/1000 | Loss: 0.00001259
Iteration 10/1000 | Loss: 0.00001256
Iteration 11/1000 | Loss: 0.00001252
Iteration 12/1000 | Loss: 0.00001248
Iteration 13/1000 | Loss: 0.00001245
Iteration 14/1000 | Loss: 0.00001237
Iteration 15/1000 | Loss: 0.00001234
Iteration 16/1000 | Loss: 0.00001234
Iteration 17/1000 | Loss: 0.00001233
Iteration 18/1000 | Loss: 0.00001233
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001231
Iteration 21/1000 | Loss: 0.00001231
Iteration 22/1000 | Loss: 0.00001231
Iteration 23/1000 | Loss: 0.00001229
Iteration 24/1000 | Loss: 0.00001229
Iteration 25/1000 | Loss: 0.00001229
Iteration 26/1000 | Loss: 0.00001229
Iteration 27/1000 | Loss: 0.00001229
Iteration 28/1000 | Loss: 0.00001228
Iteration 29/1000 | Loss: 0.00001228
Iteration 30/1000 | Loss: 0.00001227
Iteration 31/1000 | Loss: 0.00001226
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001225
Iteration 34/1000 | Loss: 0.00001225
Iteration 35/1000 | Loss: 0.00001225
Iteration 36/1000 | Loss: 0.00001224
Iteration 37/1000 | Loss: 0.00001224
Iteration 38/1000 | Loss: 0.00001224
Iteration 39/1000 | Loss: 0.00001224
Iteration 40/1000 | Loss: 0.00001224
Iteration 41/1000 | Loss: 0.00001224
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001223
Iteration 45/1000 | Loss: 0.00001223
Iteration 46/1000 | Loss: 0.00001223
Iteration 47/1000 | Loss: 0.00001223
Iteration 48/1000 | Loss: 0.00001223
Iteration 49/1000 | Loss: 0.00001223
Iteration 50/1000 | Loss: 0.00001223
Iteration 51/1000 | Loss: 0.00001223
Iteration 52/1000 | Loss: 0.00001223
Iteration 53/1000 | Loss: 0.00001223
Iteration 54/1000 | Loss: 0.00001223
Iteration 55/1000 | Loss: 0.00001223
Iteration 56/1000 | Loss: 0.00001223
Iteration 57/1000 | Loss: 0.00001222
Iteration 58/1000 | Loss: 0.00001222
Iteration 59/1000 | Loss: 0.00001222
Iteration 60/1000 | Loss: 0.00001222
Iteration 61/1000 | Loss: 0.00001222
Iteration 62/1000 | Loss: 0.00001222
Iteration 63/1000 | Loss: 0.00001222
Iteration 64/1000 | Loss: 0.00001221
Iteration 65/1000 | Loss: 0.00001221
Iteration 66/1000 | Loss: 0.00001221
Iteration 67/1000 | Loss: 0.00001221
Iteration 68/1000 | Loss: 0.00001221
Iteration 69/1000 | Loss: 0.00001220
Iteration 70/1000 | Loss: 0.00001220
Iteration 71/1000 | Loss: 0.00001220
Iteration 72/1000 | Loss: 0.00001219
Iteration 73/1000 | Loss: 0.00001219
Iteration 74/1000 | Loss: 0.00001219
Iteration 75/1000 | Loss: 0.00001219
Iteration 76/1000 | Loss: 0.00001219
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001218
Iteration 79/1000 | Loss: 0.00001218
Iteration 80/1000 | Loss: 0.00001218
Iteration 81/1000 | Loss: 0.00001218
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001217
Iteration 85/1000 | Loss: 0.00001217
Iteration 86/1000 | Loss: 0.00001217
Iteration 87/1000 | Loss: 0.00001217
Iteration 88/1000 | Loss: 0.00001217
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001216
Iteration 94/1000 | Loss: 0.00001216
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001215
Iteration 99/1000 | Loss: 0.00001215
Iteration 100/1000 | Loss: 0.00001215
Iteration 101/1000 | Loss: 0.00001215
Iteration 102/1000 | Loss: 0.00001215
Iteration 103/1000 | Loss: 0.00001215
Iteration 104/1000 | Loss: 0.00001215
Iteration 105/1000 | Loss: 0.00001215
Iteration 106/1000 | Loss: 0.00001215
Iteration 107/1000 | Loss: 0.00001215
Iteration 108/1000 | Loss: 0.00001215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.2151421287853736e-05, 1.2151421287853736e-05, 1.2151421287853736e-05, 1.2151421287853736e-05, 1.2151421287853736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2151421287853736e-05

Optimization complete. Final v2v error: 2.992748498916626 mm

Highest mean error: 3.245913505554199 mm for frame 104

Lowest mean error: 2.824157238006592 mm for frame 71

Saving results

Total time: 30.64938497543335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925335
Iteration 2/25 | Loss: 0.00314456
Iteration 3/25 | Loss: 0.00182994
Iteration 4/25 | Loss: 0.00125160
Iteration 5/25 | Loss: 0.00098485
Iteration 6/25 | Loss: 0.00094886
Iteration 7/25 | Loss: 0.00092950
Iteration 8/25 | Loss: 0.00092024
Iteration 9/25 | Loss: 0.00092440
Iteration 10/25 | Loss: 0.00091655
Iteration 11/25 | Loss: 0.00091065
Iteration 12/25 | Loss: 0.00089857
Iteration 13/25 | Loss: 0.00091052
Iteration 14/25 | Loss: 0.00090246
Iteration 15/25 | Loss: 0.00090534
Iteration 16/25 | Loss: 0.00089799
Iteration 17/25 | Loss: 0.00089406
Iteration 18/25 | Loss: 0.00089440
Iteration 19/25 | Loss: 0.00089253
Iteration 20/25 | Loss: 0.00089038
Iteration 21/25 | Loss: 0.00089297
Iteration 22/25 | Loss: 0.00088773
Iteration 23/25 | Loss: 0.00088891
Iteration 24/25 | Loss: 0.00089114
Iteration 25/25 | Loss: 0.00088677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.57401514
Iteration 2/25 | Loss: 0.00329128
Iteration 3/25 | Loss: 0.00213892
Iteration 4/25 | Loss: 0.00213885
Iteration 5/25 | Loss: 0.00213885
Iteration 6/25 | Loss: 0.00213885
Iteration 7/25 | Loss: 0.00213885
Iteration 8/25 | Loss: 0.00213885
Iteration 9/25 | Loss: 0.00213885
Iteration 10/25 | Loss: 0.00213885
Iteration 11/25 | Loss: 0.00213885
Iteration 12/25 | Loss: 0.00213885
Iteration 13/25 | Loss: 0.00213885
Iteration 14/25 | Loss: 0.00213885
Iteration 15/25 | Loss: 0.00213885
Iteration 16/25 | Loss: 0.00213885
Iteration 17/25 | Loss: 0.00213885
Iteration 18/25 | Loss: 0.00213885
Iteration 19/25 | Loss: 0.00213885
Iteration 20/25 | Loss: 0.00213885
Iteration 21/25 | Loss: 0.00213885
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0021388460882008076, 0.0021388460882008076, 0.0021388460882008076, 0.0021388460882008076, 0.0021388460882008076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021388460882008076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00213885
Iteration 2/1000 | Loss: 0.00297334
Iteration 3/1000 | Loss: 0.00076357
Iteration 4/1000 | Loss: 0.00046141
Iteration 5/1000 | Loss: 0.00200246
Iteration 6/1000 | Loss: 0.00210149
Iteration 7/1000 | Loss: 0.00171341
Iteration 8/1000 | Loss: 0.00139490
Iteration 9/1000 | Loss: 0.00478640
Iteration 10/1000 | Loss: 0.00185064
Iteration 11/1000 | Loss: 0.00523590
Iteration 12/1000 | Loss: 0.00117456
Iteration 13/1000 | Loss: 0.00229647
Iteration 14/1000 | Loss: 0.00206146
Iteration 15/1000 | Loss: 0.00457415
Iteration 16/1000 | Loss: 0.00298303
Iteration 17/1000 | Loss: 0.00564649
Iteration 18/1000 | Loss: 0.00336274
Iteration 19/1000 | Loss: 0.00183014
Iteration 20/1000 | Loss: 0.00205242
Iteration 21/1000 | Loss: 0.00382586
Iteration 22/1000 | Loss: 0.00231491
Iteration 23/1000 | Loss: 0.00425426
Iteration 24/1000 | Loss: 0.00311011
Iteration 25/1000 | Loss: 0.00219822
Iteration 26/1000 | Loss: 0.00132719
Iteration 27/1000 | Loss: 0.00373230
Iteration 28/1000 | Loss: 0.00321504
Iteration 29/1000 | Loss: 0.00178531
Iteration 30/1000 | Loss: 0.00189350
Iteration 31/1000 | Loss: 0.00121740
Iteration 32/1000 | Loss: 0.00291582
Iteration 33/1000 | Loss: 0.00316682
Iteration 34/1000 | Loss: 0.00342297
Iteration 35/1000 | Loss: 0.00204070
Iteration 36/1000 | Loss: 0.00188601
Iteration 37/1000 | Loss: 0.00212023
Iteration 38/1000 | Loss: 0.00222460
Iteration 39/1000 | Loss: 0.00231302
Iteration 40/1000 | Loss: 0.00288151
Iteration 41/1000 | Loss: 0.00198246
Iteration 42/1000 | Loss: 0.00153946
Iteration 43/1000 | Loss: 0.00077674
Iteration 44/1000 | Loss: 0.00185360
Iteration 45/1000 | Loss: 0.00231710
Iteration 46/1000 | Loss: 0.00167320
Iteration 47/1000 | Loss: 0.00090828
Iteration 48/1000 | Loss: 0.00158648
Iteration 49/1000 | Loss: 0.00115755
Iteration 50/1000 | Loss: 0.00172566
Iteration 51/1000 | Loss: 0.00164111
Iteration 52/1000 | Loss: 0.00158608
Iteration 53/1000 | Loss: 0.00101426
Iteration 54/1000 | Loss: 0.00144676
Iteration 55/1000 | Loss: 0.00161104
Iteration 56/1000 | Loss: 0.00164272
Iteration 57/1000 | Loss: 0.00255171
Iteration 58/1000 | Loss: 0.00146785
Iteration 59/1000 | Loss: 0.00174762
Iteration 60/1000 | Loss: 0.00244115
Iteration 61/1000 | Loss: 0.00083758
Iteration 62/1000 | Loss: 0.00037955
Iteration 63/1000 | Loss: 0.00022480
Iteration 64/1000 | Loss: 0.00063799
Iteration 65/1000 | Loss: 0.00115947
Iteration 66/1000 | Loss: 0.00084599
Iteration 67/1000 | Loss: 0.00159923
Iteration 68/1000 | Loss: 0.00149049
Iteration 69/1000 | Loss: 0.00082512
Iteration 70/1000 | Loss: 0.00087556
Iteration 71/1000 | Loss: 0.00098146
Iteration 72/1000 | Loss: 0.00121086
Iteration 73/1000 | Loss: 0.00044060
Iteration 74/1000 | Loss: 0.00151051
Iteration 75/1000 | Loss: 0.00080067
Iteration 76/1000 | Loss: 0.00110182
Iteration 77/1000 | Loss: 0.00174048
Iteration 78/1000 | Loss: 0.00092480
Iteration 79/1000 | Loss: 0.00238279
Iteration 80/1000 | Loss: 0.00164911
Iteration 81/1000 | Loss: 0.00106455
Iteration 82/1000 | Loss: 0.00185344
Iteration 83/1000 | Loss: 0.00292888
Iteration 84/1000 | Loss: 0.00189773
Iteration 85/1000 | Loss: 0.00135599
Iteration 86/1000 | Loss: 0.00170625
Iteration 87/1000 | Loss: 0.00139899
Iteration 88/1000 | Loss: 0.00173891
Iteration 89/1000 | Loss: 0.00214660
Iteration 90/1000 | Loss: 0.00153374
Iteration 91/1000 | Loss: 0.00154738
Iteration 92/1000 | Loss: 0.00107970
Iteration 93/1000 | Loss: 0.00041174
Iteration 94/1000 | Loss: 0.00136477
Iteration 95/1000 | Loss: 0.00116028
Iteration 96/1000 | Loss: 0.00065362
Iteration 97/1000 | Loss: 0.00088082
Iteration 98/1000 | Loss: 0.00085429
Iteration 99/1000 | Loss: 0.00109477
Iteration 100/1000 | Loss: 0.00173985
Iteration 101/1000 | Loss: 0.00076926
Iteration 102/1000 | Loss: 0.00081175
Iteration 103/1000 | Loss: 0.00092463
Iteration 104/1000 | Loss: 0.00107663
Iteration 105/1000 | Loss: 0.00140787
Iteration 106/1000 | Loss: 0.00132816
Iteration 107/1000 | Loss: 0.00137786
Iteration 108/1000 | Loss: 0.00434331
Iteration 109/1000 | Loss: 0.00337746
Iteration 110/1000 | Loss: 0.00092022
Iteration 111/1000 | Loss: 0.00265856
Iteration 112/1000 | Loss: 0.00326761
Iteration 113/1000 | Loss: 0.00171936
Iteration 114/1000 | Loss: 0.00157043
Iteration 115/1000 | Loss: 0.00116479
Iteration 116/1000 | Loss: 0.00184275
Iteration 117/1000 | Loss: 0.00111695
Iteration 118/1000 | Loss: 0.00161089
Iteration 119/1000 | Loss: 0.00137103
Iteration 120/1000 | Loss: 0.00100331
Iteration 121/1000 | Loss: 0.00048548
Iteration 122/1000 | Loss: 0.00081383
Iteration 123/1000 | Loss: 0.00045409
Iteration 124/1000 | Loss: 0.00082510
Iteration 125/1000 | Loss: 0.00253997
Iteration 126/1000 | Loss: 0.00053279
Iteration 127/1000 | Loss: 0.00059020
Iteration 128/1000 | Loss: 0.00059930
Iteration 129/1000 | Loss: 0.00060621
Iteration 130/1000 | Loss: 0.00041238
Iteration 131/1000 | Loss: 0.00035536
Iteration 132/1000 | Loss: 0.00032780
Iteration 133/1000 | Loss: 0.00078822
Iteration 134/1000 | Loss: 0.00131175
Iteration 135/1000 | Loss: 0.00032085
Iteration 136/1000 | Loss: 0.00006136
Iteration 137/1000 | Loss: 0.00012154
Iteration 138/1000 | Loss: 0.00041840
Iteration 139/1000 | Loss: 0.00023554
Iteration 140/1000 | Loss: 0.00004924
Iteration 141/1000 | Loss: 0.00005628
Iteration 142/1000 | Loss: 0.00006162
Iteration 143/1000 | Loss: 0.00064831
Iteration 144/1000 | Loss: 0.00012577
Iteration 145/1000 | Loss: 0.00050883
Iteration 146/1000 | Loss: 0.00016381
Iteration 147/1000 | Loss: 0.00029171
Iteration 148/1000 | Loss: 0.00006336
Iteration 149/1000 | Loss: 0.00026184
Iteration 150/1000 | Loss: 0.00016242
Iteration 151/1000 | Loss: 0.00023611
Iteration 152/1000 | Loss: 0.00145697
Iteration 153/1000 | Loss: 0.00078658
Iteration 154/1000 | Loss: 0.00083469
Iteration 155/1000 | Loss: 0.00097459
Iteration 156/1000 | Loss: 0.00020614
Iteration 157/1000 | Loss: 0.00033498
Iteration 158/1000 | Loss: 0.00040771
Iteration 159/1000 | Loss: 0.00007502
Iteration 160/1000 | Loss: 0.00049912
Iteration 161/1000 | Loss: 0.00066040
Iteration 162/1000 | Loss: 0.00044420
Iteration 163/1000 | Loss: 0.00130935
Iteration 164/1000 | Loss: 0.00054474
Iteration 165/1000 | Loss: 0.00004261
Iteration 166/1000 | Loss: 0.00038542
Iteration 167/1000 | Loss: 0.00094224
Iteration 168/1000 | Loss: 0.00025730
Iteration 169/1000 | Loss: 0.00042880
Iteration 170/1000 | Loss: 0.00019377
Iteration 171/1000 | Loss: 0.00076599
Iteration 172/1000 | Loss: 0.00047678
Iteration 173/1000 | Loss: 0.00065627
Iteration 174/1000 | Loss: 0.00110906
Iteration 175/1000 | Loss: 0.00051837
Iteration 176/1000 | Loss: 0.00045722
Iteration 177/1000 | Loss: 0.00046593
Iteration 178/1000 | Loss: 0.00147080
Iteration 179/1000 | Loss: 0.00050724
Iteration 180/1000 | Loss: 0.00059224
Iteration 181/1000 | Loss: 0.00082402
Iteration 182/1000 | Loss: 0.00104614
Iteration 183/1000 | Loss: 0.00145595
Iteration 184/1000 | Loss: 0.00103788
Iteration 185/1000 | Loss: 0.00078500
Iteration 186/1000 | Loss: 0.00139251
Iteration 187/1000 | Loss: 0.00143911
Iteration 188/1000 | Loss: 0.00096742
Iteration 189/1000 | Loss: 0.00133588
Iteration 190/1000 | Loss: 0.00028446
Iteration 191/1000 | Loss: 0.00022511
Iteration 192/1000 | Loss: 0.00035839
Iteration 193/1000 | Loss: 0.00104357
Iteration 194/1000 | Loss: 0.00043211
Iteration 195/1000 | Loss: 0.00029672
Iteration 196/1000 | Loss: 0.00080817
Iteration 197/1000 | Loss: 0.00029799
Iteration 198/1000 | Loss: 0.00049139
Iteration 199/1000 | Loss: 0.00042149
Iteration 200/1000 | Loss: 0.00055538
Iteration 201/1000 | Loss: 0.00072307
Iteration 202/1000 | Loss: 0.00019308
Iteration 203/1000 | Loss: 0.00068927
Iteration 204/1000 | Loss: 0.00175729
Iteration 205/1000 | Loss: 0.00050924
Iteration 206/1000 | Loss: 0.00083827
Iteration 207/1000 | Loss: 0.00051045
Iteration 208/1000 | Loss: 0.00079129
Iteration 209/1000 | Loss: 0.00036572
Iteration 210/1000 | Loss: 0.00083640
Iteration 211/1000 | Loss: 0.00070765
Iteration 212/1000 | Loss: 0.00058586
Iteration 213/1000 | Loss: 0.00007226
Iteration 214/1000 | Loss: 0.00008894
Iteration 215/1000 | Loss: 0.00010513
Iteration 216/1000 | Loss: 0.00008000
Iteration 217/1000 | Loss: 0.00040497
Iteration 218/1000 | Loss: 0.00012787
Iteration 219/1000 | Loss: 0.00044466
Iteration 220/1000 | Loss: 0.00014451
Iteration 221/1000 | Loss: 0.00020529
Iteration 222/1000 | Loss: 0.00008686
Iteration 223/1000 | Loss: 0.00027134
Iteration 224/1000 | Loss: 0.00011523
Iteration 225/1000 | Loss: 0.00006256
Iteration 226/1000 | Loss: 0.00006197
Iteration 227/1000 | Loss: 0.00006232
Iteration 228/1000 | Loss: 0.00006511
Iteration 229/1000 | Loss: 0.00005780
Iteration 230/1000 | Loss: 0.00005795
Iteration 231/1000 | Loss: 0.00017685
Iteration 232/1000 | Loss: 0.00019286
Iteration 233/1000 | Loss: 0.00043242
Iteration 234/1000 | Loss: 0.00053671
Iteration 235/1000 | Loss: 0.00047525
Iteration 236/1000 | Loss: 0.00144907
Iteration 237/1000 | Loss: 0.00052973
Iteration 238/1000 | Loss: 0.00043955
Iteration 239/1000 | Loss: 0.00051041
Iteration 240/1000 | Loss: 0.00008502
Iteration 241/1000 | Loss: 0.00006268
Iteration 242/1000 | Loss: 0.00005813
Iteration 243/1000 | Loss: 0.00004817
Iteration 244/1000 | Loss: 0.00050783
Iteration 245/1000 | Loss: 0.00094011
Iteration 246/1000 | Loss: 0.00009421
Iteration 247/1000 | Loss: 0.00006627
Iteration 248/1000 | Loss: 0.00005068
Iteration 249/1000 | Loss: 0.00109040
Iteration 250/1000 | Loss: 0.00030970
Iteration 251/1000 | Loss: 0.00042962
Iteration 252/1000 | Loss: 0.00044242
Iteration 253/1000 | Loss: 0.00028861
Iteration 254/1000 | Loss: 0.00015932
Iteration 255/1000 | Loss: 0.00028530
Iteration 256/1000 | Loss: 0.00010916
Iteration 257/1000 | Loss: 0.00007273
Iteration 258/1000 | Loss: 0.00011257
Iteration 259/1000 | Loss: 0.00017170
Iteration 260/1000 | Loss: 0.00005319
Iteration 261/1000 | Loss: 0.00018221
Iteration 262/1000 | Loss: 0.00005604
Iteration 263/1000 | Loss: 0.00003918
Iteration 264/1000 | Loss: 0.00010912
Iteration 265/1000 | Loss: 0.00003697
Iteration 266/1000 | Loss: 0.00005689
Iteration 267/1000 | Loss: 0.00004400
Iteration 268/1000 | Loss: 0.00006038
Iteration 269/1000 | Loss: 0.00004314
Iteration 270/1000 | Loss: 0.00005080
Iteration 271/1000 | Loss: 0.00004204
Iteration 272/1000 | Loss: 0.00076555
Iteration 273/1000 | Loss: 0.00004683
Iteration 274/1000 | Loss: 0.00004566
Iteration 275/1000 | Loss: 0.00022811
Iteration 276/1000 | Loss: 0.00004522
Iteration 277/1000 | Loss: 0.00004206
Iteration 278/1000 | Loss: 0.00004414
Iteration 279/1000 | Loss: 0.00003330
Iteration 280/1000 | Loss: 0.00004165
Iteration 281/1000 | Loss: 0.00004267
Iteration 282/1000 | Loss: 0.00004342
Iteration 283/1000 | Loss: 0.00004062
Iteration 284/1000 | Loss: 0.00003738
Iteration 285/1000 | Loss: 0.00002005
Iteration 286/1000 | Loss: 0.00004021
Iteration 287/1000 | Loss: 0.00004018
Iteration 288/1000 | Loss: 0.00003722
Iteration 289/1000 | Loss: 0.00004042
Iteration 290/1000 | Loss: 0.00003778
Iteration 291/1000 | Loss: 0.00003994
Iteration 292/1000 | Loss: 0.00003830
Iteration 293/1000 | Loss: 0.00005575
Iteration 294/1000 | Loss: 0.00004875
Iteration 295/1000 | Loss: 0.00003468
Iteration 296/1000 | Loss: 0.00004085
Iteration 297/1000 | Loss: 0.00003488
Iteration 298/1000 | Loss: 0.00003784
Iteration 299/1000 | Loss: 0.00003690
Iteration 300/1000 | Loss: 0.00004177
Iteration 301/1000 | Loss: 0.00003705
Iteration 302/1000 | Loss: 0.00003976
Iteration 303/1000 | Loss: 0.00003495
Iteration 304/1000 | Loss: 0.00005362
Iteration 305/1000 | Loss: 0.00003476
Iteration 306/1000 | Loss: 0.00005329
Iteration 307/1000 | Loss: 0.00003455
Iteration 308/1000 | Loss: 0.00005083
Iteration 309/1000 | Loss: 0.00002375
Iteration 310/1000 | Loss: 0.00001970
Iteration 311/1000 | Loss: 0.00001853
Iteration 312/1000 | Loss: 0.00001815
Iteration 313/1000 | Loss: 0.00001779
Iteration 314/1000 | Loss: 0.00001769
Iteration 315/1000 | Loss: 0.00001765
Iteration 316/1000 | Loss: 0.00001764
Iteration 317/1000 | Loss: 0.00001755
Iteration 318/1000 | Loss: 0.00002084
Iteration 319/1000 | Loss: 0.00019264
Iteration 320/1000 | Loss: 0.00021911
Iteration 321/1000 | Loss: 0.00001762
Iteration 322/1000 | Loss: 0.00001727
Iteration 323/1000 | Loss: 0.00001725
Iteration 324/1000 | Loss: 0.00001724
Iteration 325/1000 | Loss: 0.00001724
Iteration 326/1000 | Loss: 0.00001721
Iteration 327/1000 | Loss: 0.00001719
Iteration 328/1000 | Loss: 0.00001718
Iteration 329/1000 | Loss: 0.00001718
Iteration 330/1000 | Loss: 0.00001717
Iteration 331/1000 | Loss: 0.00001717
Iteration 332/1000 | Loss: 0.00017912
Iteration 333/1000 | Loss: 0.00003303
Iteration 334/1000 | Loss: 0.00008738
Iteration 335/1000 | Loss: 0.00001736
Iteration 336/1000 | Loss: 0.00001709
Iteration 337/1000 | Loss: 0.00001705
Iteration 338/1000 | Loss: 0.00001705
Iteration 339/1000 | Loss: 0.00001705
Iteration 340/1000 | Loss: 0.00001705
Iteration 341/1000 | Loss: 0.00001705
Iteration 342/1000 | Loss: 0.00001705
Iteration 343/1000 | Loss: 0.00001705
Iteration 344/1000 | Loss: 0.00001705
Iteration 345/1000 | Loss: 0.00001704
Iteration 346/1000 | Loss: 0.00001704
Iteration 347/1000 | Loss: 0.00001704
Iteration 348/1000 | Loss: 0.00001704
Iteration 349/1000 | Loss: 0.00001704
Iteration 350/1000 | Loss: 0.00001704
Iteration 351/1000 | Loss: 0.00001704
Iteration 352/1000 | Loss: 0.00001704
Iteration 353/1000 | Loss: 0.00001704
Iteration 354/1000 | Loss: 0.00001704
Iteration 355/1000 | Loss: 0.00001704
Iteration 356/1000 | Loss: 0.00001704
Iteration 357/1000 | Loss: 0.00001704
Iteration 358/1000 | Loss: 0.00001704
Iteration 359/1000 | Loss: 0.00001704
Iteration 360/1000 | Loss: 0.00001704
Iteration 361/1000 | Loss: 0.00001704
Iteration 362/1000 | Loss: 0.00001704
Iteration 363/1000 | Loss: 0.00001704
Iteration 364/1000 | Loss: 0.00001703
Iteration 365/1000 | Loss: 0.00001703
Iteration 366/1000 | Loss: 0.00001703
Iteration 367/1000 | Loss: 0.00001703
Iteration 368/1000 | Loss: 0.00001703
Iteration 369/1000 | Loss: 0.00001703
Iteration 370/1000 | Loss: 0.00001703
Iteration 371/1000 | Loss: 0.00001703
Iteration 372/1000 | Loss: 0.00001703
Iteration 373/1000 | Loss: 0.00001703
Iteration 374/1000 | Loss: 0.00001703
Iteration 375/1000 | Loss: 0.00001703
Iteration 376/1000 | Loss: 0.00001703
Iteration 377/1000 | Loss: 0.00001703
Iteration 378/1000 | Loss: 0.00001703
Iteration 379/1000 | Loss: 0.00001703
Iteration 380/1000 | Loss: 0.00001703
Iteration 381/1000 | Loss: 0.00001703
Iteration 382/1000 | Loss: 0.00001703
Iteration 383/1000 | Loss: 0.00001703
Iteration 384/1000 | Loss: 0.00001703
Iteration 385/1000 | Loss: 0.00001703
Iteration 386/1000 | Loss: 0.00001703
Iteration 387/1000 | Loss: 0.00001703
Iteration 388/1000 | Loss: 0.00001703
Iteration 389/1000 | Loss: 0.00001703
Iteration 390/1000 | Loss: 0.00001703
Iteration 391/1000 | Loss: 0.00001703
Iteration 392/1000 | Loss: 0.00001703
Iteration 393/1000 | Loss: 0.00001703
Iteration 394/1000 | Loss: 0.00001703
Iteration 395/1000 | Loss: 0.00001703
Iteration 396/1000 | Loss: 0.00001703
Iteration 397/1000 | Loss: 0.00001703
Iteration 398/1000 | Loss: 0.00001703
Iteration 399/1000 | Loss: 0.00001703
Iteration 400/1000 | Loss: 0.00001703
Iteration 401/1000 | Loss: 0.00001703
Iteration 402/1000 | Loss: 0.00001703
Iteration 403/1000 | Loss: 0.00001703
Iteration 404/1000 | Loss: 0.00001703
Iteration 405/1000 | Loss: 0.00001703
Iteration 406/1000 | Loss: 0.00001703
Iteration 407/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 407. Stopping optimization.
Last 5 losses: [1.7033198673743755e-05, 1.7033198673743755e-05, 1.7033198673743755e-05, 1.7033198673743755e-05, 1.7033198673743755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7033198673743755e-05

Optimization complete. Final v2v error: 3.3311057090759277 mm

Highest mean error: 5.237219333648682 mm for frame 85

Lowest mean error: 2.563624382019043 mm for frame 156

Saving results

Total time: 490.21088337898254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00362224
Iteration 2/25 | Loss: 0.00105115
Iteration 3/25 | Loss: 0.00072890
Iteration 4/25 | Loss: 0.00065051
Iteration 5/25 | Loss: 0.00063623
Iteration 6/25 | Loss: 0.00063357
Iteration 7/25 | Loss: 0.00063315
Iteration 8/25 | Loss: 0.00063315
Iteration 9/25 | Loss: 0.00063315
Iteration 10/25 | Loss: 0.00063315
Iteration 11/25 | Loss: 0.00063315
Iteration 12/25 | Loss: 0.00063315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000633151619695127, 0.000633151619695127, 0.000633151619695127, 0.000633151619695127, 0.000633151619695127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000633151619695127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54836345
Iteration 2/25 | Loss: 0.00032041
Iteration 3/25 | Loss: 0.00032041
Iteration 4/25 | Loss: 0.00032041
Iteration 5/25 | Loss: 0.00032041
Iteration 6/25 | Loss: 0.00032041
Iteration 7/25 | Loss: 0.00032041
Iteration 8/25 | Loss: 0.00032041
Iteration 9/25 | Loss: 0.00032041
Iteration 10/25 | Loss: 0.00032041
Iteration 11/25 | Loss: 0.00032041
Iteration 12/25 | Loss: 0.00032041
Iteration 13/25 | Loss: 0.00032041
Iteration 14/25 | Loss: 0.00032041
Iteration 15/25 | Loss: 0.00032041
Iteration 16/25 | Loss: 0.00032041
Iteration 17/25 | Loss: 0.00032041
Iteration 18/25 | Loss: 0.00032041
Iteration 19/25 | Loss: 0.00032041
Iteration 20/25 | Loss: 0.00032041
Iteration 21/25 | Loss: 0.00032041
Iteration 22/25 | Loss: 0.00032041
Iteration 23/25 | Loss: 0.00032041
Iteration 24/25 | Loss: 0.00032041
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003204116364941001, 0.0003204116364941001, 0.0003204116364941001, 0.0003204116364941001, 0.0003204116364941001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003204116364941001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032041
Iteration 2/1000 | Loss: 0.00003266
Iteration 3/1000 | Loss: 0.00002150
Iteration 4/1000 | Loss: 0.00001457
Iteration 5/1000 | Loss: 0.00001330
Iteration 6/1000 | Loss: 0.00001240
Iteration 7/1000 | Loss: 0.00001197
Iteration 8/1000 | Loss: 0.00001191
Iteration 9/1000 | Loss: 0.00001170
Iteration 10/1000 | Loss: 0.00001150
Iteration 11/1000 | Loss: 0.00001137
Iteration 12/1000 | Loss: 0.00001131
Iteration 13/1000 | Loss: 0.00001131
Iteration 14/1000 | Loss: 0.00001129
Iteration 15/1000 | Loss: 0.00001129
Iteration 16/1000 | Loss: 0.00001128
Iteration 17/1000 | Loss: 0.00001127
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001126
Iteration 20/1000 | Loss: 0.00001125
Iteration 21/1000 | Loss: 0.00001125
Iteration 22/1000 | Loss: 0.00001125
Iteration 23/1000 | Loss: 0.00001125
Iteration 24/1000 | Loss: 0.00001125
Iteration 25/1000 | Loss: 0.00001124
Iteration 26/1000 | Loss: 0.00001120
Iteration 27/1000 | Loss: 0.00001119
Iteration 28/1000 | Loss: 0.00001118
Iteration 29/1000 | Loss: 0.00001118
Iteration 30/1000 | Loss: 0.00001117
Iteration 31/1000 | Loss: 0.00001115
Iteration 32/1000 | Loss: 0.00001114
Iteration 33/1000 | Loss: 0.00001113
Iteration 34/1000 | Loss: 0.00001112
Iteration 35/1000 | Loss: 0.00001112
Iteration 36/1000 | Loss: 0.00001112
Iteration 37/1000 | Loss: 0.00001112
Iteration 38/1000 | Loss: 0.00001112
Iteration 39/1000 | Loss: 0.00001112
Iteration 40/1000 | Loss: 0.00001112
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001111
Iteration 43/1000 | Loss: 0.00001111
Iteration 44/1000 | Loss: 0.00001111
Iteration 45/1000 | Loss: 0.00001111
Iteration 46/1000 | Loss: 0.00001111
Iteration 47/1000 | Loss: 0.00001111
Iteration 48/1000 | Loss: 0.00001111
Iteration 49/1000 | Loss: 0.00001111
Iteration 50/1000 | Loss: 0.00001111
Iteration 51/1000 | Loss: 0.00001110
Iteration 52/1000 | Loss: 0.00001110
Iteration 53/1000 | Loss: 0.00001109
Iteration 54/1000 | Loss: 0.00001109
Iteration 55/1000 | Loss: 0.00001108
Iteration 56/1000 | Loss: 0.00001108
Iteration 57/1000 | Loss: 0.00001108
Iteration 58/1000 | Loss: 0.00001108
Iteration 59/1000 | Loss: 0.00001108
Iteration 60/1000 | Loss: 0.00001108
Iteration 61/1000 | Loss: 0.00001108
Iteration 62/1000 | Loss: 0.00001108
Iteration 63/1000 | Loss: 0.00001108
Iteration 64/1000 | Loss: 0.00001108
Iteration 65/1000 | Loss: 0.00001108
Iteration 66/1000 | Loss: 0.00001108
Iteration 67/1000 | Loss: 0.00001107
Iteration 68/1000 | Loss: 0.00001107
Iteration 69/1000 | Loss: 0.00001107
Iteration 70/1000 | Loss: 0.00001106
Iteration 71/1000 | Loss: 0.00001106
Iteration 72/1000 | Loss: 0.00001106
Iteration 73/1000 | Loss: 0.00001106
Iteration 74/1000 | Loss: 0.00001106
Iteration 75/1000 | Loss: 0.00001106
Iteration 76/1000 | Loss: 0.00001106
Iteration 77/1000 | Loss: 0.00001106
Iteration 78/1000 | Loss: 0.00001105
Iteration 79/1000 | Loss: 0.00001105
Iteration 80/1000 | Loss: 0.00001105
Iteration 81/1000 | Loss: 0.00001105
Iteration 82/1000 | Loss: 0.00001105
Iteration 83/1000 | Loss: 0.00001105
Iteration 84/1000 | Loss: 0.00001105
Iteration 85/1000 | Loss: 0.00001104
Iteration 86/1000 | Loss: 0.00001104
Iteration 87/1000 | Loss: 0.00001104
Iteration 88/1000 | Loss: 0.00001104
Iteration 89/1000 | Loss: 0.00001103
Iteration 90/1000 | Loss: 0.00001103
Iteration 91/1000 | Loss: 0.00001103
Iteration 92/1000 | Loss: 0.00001103
Iteration 93/1000 | Loss: 0.00001103
Iteration 94/1000 | Loss: 0.00001102
Iteration 95/1000 | Loss: 0.00001102
Iteration 96/1000 | Loss: 0.00001102
Iteration 97/1000 | Loss: 0.00001101
Iteration 98/1000 | Loss: 0.00001101
Iteration 99/1000 | Loss: 0.00001101
Iteration 100/1000 | Loss: 0.00001101
Iteration 101/1000 | Loss: 0.00001100
Iteration 102/1000 | Loss: 0.00001100
Iteration 103/1000 | Loss: 0.00001100
Iteration 104/1000 | Loss: 0.00001100
Iteration 105/1000 | Loss: 0.00001100
Iteration 106/1000 | Loss: 0.00001099
Iteration 107/1000 | Loss: 0.00001099
Iteration 108/1000 | Loss: 0.00001099
Iteration 109/1000 | Loss: 0.00001099
Iteration 110/1000 | Loss: 0.00001099
Iteration 111/1000 | Loss: 0.00001099
Iteration 112/1000 | Loss: 0.00001099
Iteration 113/1000 | Loss: 0.00001099
Iteration 114/1000 | Loss: 0.00001099
Iteration 115/1000 | Loss: 0.00001099
Iteration 116/1000 | Loss: 0.00001099
Iteration 117/1000 | Loss: 0.00001099
Iteration 118/1000 | Loss: 0.00001099
Iteration 119/1000 | Loss: 0.00001099
Iteration 120/1000 | Loss: 0.00001099
Iteration 121/1000 | Loss: 0.00001099
Iteration 122/1000 | Loss: 0.00001099
Iteration 123/1000 | Loss: 0.00001099
Iteration 124/1000 | Loss: 0.00001099
Iteration 125/1000 | Loss: 0.00001099
Iteration 126/1000 | Loss: 0.00001099
Iteration 127/1000 | Loss: 0.00001099
Iteration 128/1000 | Loss: 0.00001099
Iteration 129/1000 | Loss: 0.00001099
Iteration 130/1000 | Loss: 0.00001099
Iteration 131/1000 | Loss: 0.00001099
Iteration 132/1000 | Loss: 0.00001099
Iteration 133/1000 | Loss: 0.00001099
Iteration 134/1000 | Loss: 0.00001099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.0985639164573513e-05, 1.0985639164573513e-05, 1.0985639164573513e-05, 1.0985639164573513e-05, 1.0985639164573513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0985639164573513e-05

Optimization complete. Final v2v error: 2.840458393096924 mm

Highest mean error: 3.0238654613494873 mm for frame 42

Lowest mean error: 2.642228841781616 mm for frame 134

Saving results

Total time: 33.753337144851685
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944541
Iteration 2/25 | Loss: 0.00107958
Iteration 3/25 | Loss: 0.00081973
Iteration 4/25 | Loss: 0.00077770
Iteration 5/25 | Loss: 0.00075702
Iteration 6/25 | Loss: 0.00075137
Iteration 7/25 | Loss: 0.00074996
Iteration 8/25 | Loss: 0.00074986
Iteration 9/25 | Loss: 0.00074986
Iteration 10/25 | Loss: 0.00074986
Iteration 11/25 | Loss: 0.00074986
Iteration 12/25 | Loss: 0.00074986
Iteration 13/25 | Loss: 0.00074986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007498555351048708, 0.0007498555351048708, 0.0007498555351048708, 0.0007498555351048708, 0.0007498555351048708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007498555351048708

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46027863
Iteration 2/25 | Loss: 0.00028865
Iteration 3/25 | Loss: 0.00028863
Iteration 4/25 | Loss: 0.00028863
Iteration 5/25 | Loss: 0.00028863
Iteration 6/25 | Loss: 0.00028863
Iteration 7/25 | Loss: 0.00028863
Iteration 8/25 | Loss: 0.00028863
Iteration 9/25 | Loss: 0.00028863
Iteration 10/25 | Loss: 0.00028863
Iteration 11/25 | Loss: 0.00028863
Iteration 12/25 | Loss: 0.00028863
Iteration 13/25 | Loss: 0.00028863
Iteration 14/25 | Loss: 0.00028863
Iteration 15/25 | Loss: 0.00028863
Iteration 16/25 | Loss: 0.00028863
Iteration 17/25 | Loss: 0.00028863
Iteration 18/25 | Loss: 0.00028863
Iteration 19/25 | Loss: 0.00028863
Iteration 20/25 | Loss: 0.00028863
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002886250731535256, 0.0002886250731535256, 0.0002886250731535256, 0.0002886250731535256, 0.0002886250731535256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002886250731535256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028863
Iteration 2/1000 | Loss: 0.00005396
Iteration 3/1000 | Loss: 0.00003986
Iteration 4/1000 | Loss: 0.00003571
Iteration 5/1000 | Loss: 0.00003442
Iteration 6/1000 | Loss: 0.00003310
Iteration 7/1000 | Loss: 0.00003181
Iteration 8/1000 | Loss: 0.00003086
Iteration 9/1000 | Loss: 0.00003027
Iteration 10/1000 | Loss: 0.00002993
Iteration 11/1000 | Loss: 0.00002964
Iteration 12/1000 | Loss: 0.00002945
Iteration 13/1000 | Loss: 0.00002935
Iteration 14/1000 | Loss: 0.00002933
Iteration 15/1000 | Loss: 0.00002932
Iteration 16/1000 | Loss: 0.00002926
Iteration 17/1000 | Loss: 0.00002916
Iteration 18/1000 | Loss: 0.00002914
Iteration 19/1000 | Loss: 0.00002914
Iteration 20/1000 | Loss: 0.00002913
Iteration 21/1000 | Loss: 0.00002913
Iteration 22/1000 | Loss: 0.00002912
Iteration 23/1000 | Loss: 0.00002912
Iteration 24/1000 | Loss: 0.00002912
Iteration 25/1000 | Loss: 0.00002911
Iteration 26/1000 | Loss: 0.00002910
Iteration 27/1000 | Loss: 0.00002910
Iteration 28/1000 | Loss: 0.00002909
Iteration 29/1000 | Loss: 0.00002909
Iteration 30/1000 | Loss: 0.00002908
Iteration 31/1000 | Loss: 0.00002908
Iteration 32/1000 | Loss: 0.00002907
Iteration 33/1000 | Loss: 0.00002907
Iteration 34/1000 | Loss: 0.00002906
Iteration 35/1000 | Loss: 0.00002906
Iteration 36/1000 | Loss: 0.00002905
Iteration 37/1000 | Loss: 0.00002904
Iteration 38/1000 | Loss: 0.00002904
Iteration 39/1000 | Loss: 0.00002904
Iteration 40/1000 | Loss: 0.00002904
Iteration 41/1000 | Loss: 0.00002904
Iteration 42/1000 | Loss: 0.00002904
Iteration 43/1000 | Loss: 0.00002903
Iteration 44/1000 | Loss: 0.00002903
Iteration 45/1000 | Loss: 0.00002901
Iteration 46/1000 | Loss: 0.00002901
Iteration 47/1000 | Loss: 0.00002901
Iteration 48/1000 | Loss: 0.00002900
Iteration 49/1000 | Loss: 0.00002900
Iteration 50/1000 | Loss: 0.00002899
Iteration 51/1000 | Loss: 0.00002899
Iteration 52/1000 | Loss: 0.00002899
Iteration 53/1000 | Loss: 0.00002897
Iteration 54/1000 | Loss: 0.00002897
Iteration 55/1000 | Loss: 0.00002897
Iteration 56/1000 | Loss: 0.00002897
Iteration 57/1000 | Loss: 0.00002896
Iteration 58/1000 | Loss: 0.00002896
Iteration 59/1000 | Loss: 0.00002896
Iteration 60/1000 | Loss: 0.00002896
Iteration 61/1000 | Loss: 0.00002896
Iteration 62/1000 | Loss: 0.00002896
Iteration 63/1000 | Loss: 0.00002896
Iteration 64/1000 | Loss: 0.00002896
Iteration 65/1000 | Loss: 0.00002895
Iteration 66/1000 | Loss: 0.00002894
Iteration 67/1000 | Loss: 0.00002894
Iteration 68/1000 | Loss: 0.00002894
Iteration 69/1000 | Loss: 0.00002894
Iteration 70/1000 | Loss: 0.00002894
Iteration 71/1000 | Loss: 0.00002894
Iteration 72/1000 | Loss: 0.00002894
Iteration 73/1000 | Loss: 0.00002893
Iteration 74/1000 | Loss: 0.00002893
Iteration 75/1000 | Loss: 0.00002893
Iteration 76/1000 | Loss: 0.00002893
Iteration 77/1000 | Loss: 0.00002892
Iteration 78/1000 | Loss: 0.00002892
Iteration 79/1000 | Loss: 0.00002892
Iteration 80/1000 | Loss: 0.00002892
Iteration 81/1000 | Loss: 0.00002891
Iteration 82/1000 | Loss: 0.00002891
Iteration 83/1000 | Loss: 0.00002890
Iteration 84/1000 | Loss: 0.00002890
Iteration 85/1000 | Loss: 0.00002890
Iteration 86/1000 | Loss: 0.00002890
Iteration 87/1000 | Loss: 0.00002890
Iteration 88/1000 | Loss: 0.00002890
Iteration 89/1000 | Loss: 0.00002889
Iteration 90/1000 | Loss: 0.00002889
Iteration 91/1000 | Loss: 0.00002889
Iteration 92/1000 | Loss: 0.00002889
Iteration 93/1000 | Loss: 0.00002889
Iteration 94/1000 | Loss: 0.00002889
Iteration 95/1000 | Loss: 0.00002889
Iteration 96/1000 | Loss: 0.00002889
Iteration 97/1000 | Loss: 0.00002889
Iteration 98/1000 | Loss: 0.00002889
Iteration 99/1000 | Loss: 0.00002889
Iteration 100/1000 | Loss: 0.00002889
Iteration 101/1000 | Loss: 0.00002888
Iteration 102/1000 | Loss: 0.00002888
Iteration 103/1000 | Loss: 0.00002888
Iteration 104/1000 | Loss: 0.00002888
Iteration 105/1000 | Loss: 0.00002888
Iteration 106/1000 | Loss: 0.00002888
Iteration 107/1000 | Loss: 0.00002888
Iteration 108/1000 | Loss: 0.00002888
Iteration 109/1000 | Loss: 0.00002887
Iteration 110/1000 | Loss: 0.00002887
Iteration 111/1000 | Loss: 0.00002887
Iteration 112/1000 | Loss: 0.00002887
Iteration 113/1000 | Loss: 0.00002887
Iteration 114/1000 | Loss: 0.00002887
Iteration 115/1000 | Loss: 0.00002886
Iteration 116/1000 | Loss: 0.00002886
Iteration 117/1000 | Loss: 0.00002886
Iteration 118/1000 | Loss: 0.00002885
Iteration 119/1000 | Loss: 0.00002885
Iteration 120/1000 | Loss: 0.00002885
Iteration 121/1000 | Loss: 0.00002885
Iteration 122/1000 | Loss: 0.00002885
Iteration 123/1000 | Loss: 0.00002885
Iteration 124/1000 | Loss: 0.00002885
Iteration 125/1000 | Loss: 0.00002885
Iteration 126/1000 | Loss: 0.00002885
Iteration 127/1000 | Loss: 0.00002885
Iteration 128/1000 | Loss: 0.00002885
Iteration 129/1000 | Loss: 0.00002885
Iteration 130/1000 | Loss: 0.00002885
Iteration 131/1000 | Loss: 0.00002885
Iteration 132/1000 | Loss: 0.00002885
Iteration 133/1000 | Loss: 0.00002885
Iteration 134/1000 | Loss: 0.00002885
Iteration 135/1000 | Loss: 0.00002885
Iteration 136/1000 | Loss: 0.00002885
Iteration 137/1000 | Loss: 0.00002885
Iteration 138/1000 | Loss: 0.00002885
Iteration 139/1000 | Loss: 0.00002885
Iteration 140/1000 | Loss: 0.00002885
Iteration 141/1000 | Loss: 0.00002885
Iteration 142/1000 | Loss: 0.00002885
Iteration 143/1000 | Loss: 0.00002885
Iteration 144/1000 | Loss: 0.00002885
Iteration 145/1000 | Loss: 0.00002885
Iteration 146/1000 | Loss: 0.00002885
Iteration 147/1000 | Loss: 0.00002885
Iteration 148/1000 | Loss: 0.00002885
Iteration 149/1000 | Loss: 0.00002885
Iteration 150/1000 | Loss: 0.00002885
Iteration 151/1000 | Loss: 0.00002885
Iteration 152/1000 | Loss: 0.00002885
Iteration 153/1000 | Loss: 0.00002885
Iteration 154/1000 | Loss: 0.00002885
Iteration 155/1000 | Loss: 0.00002885
Iteration 156/1000 | Loss: 0.00002885
Iteration 157/1000 | Loss: 0.00002885
Iteration 158/1000 | Loss: 0.00002885
Iteration 159/1000 | Loss: 0.00002885
Iteration 160/1000 | Loss: 0.00002885
Iteration 161/1000 | Loss: 0.00002885
Iteration 162/1000 | Loss: 0.00002885
Iteration 163/1000 | Loss: 0.00002885
Iteration 164/1000 | Loss: 0.00002885
Iteration 165/1000 | Loss: 0.00002885
Iteration 166/1000 | Loss: 0.00002885
Iteration 167/1000 | Loss: 0.00002885
Iteration 168/1000 | Loss: 0.00002885
Iteration 169/1000 | Loss: 0.00002885
Iteration 170/1000 | Loss: 0.00002885
Iteration 171/1000 | Loss: 0.00002885
Iteration 172/1000 | Loss: 0.00002885
Iteration 173/1000 | Loss: 0.00002885
Iteration 174/1000 | Loss: 0.00002885
Iteration 175/1000 | Loss: 0.00002885
Iteration 176/1000 | Loss: 0.00002885
Iteration 177/1000 | Loss: 0.00002885
Iteration 178/1000 | Loss: 0.00002885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.884694367821794e-05, 2.884694367821794e-05, 2.884694367821794e-05, 2.884694367821794e-05, 2.884694367821794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.884694367821794e-05

Optimization complete. Final v2v error: 4.47902774810791 mm

Highest mean error: 6.039506435394287 mm for frame 68

Lowest mean error: 3.8275768756866455 mm for frame 42

Saving results

Total time: 39.235164642333984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00376883
Iteration 2/25 | Loss: 0.00071704
Iteration 3/25 | Loss: 0.00060471
Iteration 4/25 | Loss: 0.00058185
Iteration 5/25 | Loss: 0.00057738
Iteration 6/25 | Loss: 0.00057645
Iteration 7/25 | Loss: 0.00057645
Iteration 8/25 | Loss: 0.00057645
Iteration 9/25 | Loss: 0.00057645
Iteration 10/25 | Loss: 0.00057645
Iteration 11/25 | Loss: 0.00057645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005764549132436514, 0.0005764549132436514, 0.0005764549132436514, 0.0005764549132436514, 0.0005764549132436514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005764549132436514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47197580
Iteration 2/25 | Loss: 0.00028926
Iteration 3/25 | Loss: 0.00028926
Iteration 4/25 | Loss: 0.00028926
Iteration 5/25 | Loss: 0.00028926
Iteration 6/25 | Loss: 0.00028926
Iteration 7/25 | Loss: 0.00028926
Iteration 8/25 | Loss: 0.00028926
Iteration 9/25 | Loss: 0.00028926
Iteration 10/25 | Loss: 0.00028926
Iteration 11/25 | Loss: 0.00028926
Iteration 12/25 | Loss: 0.00028926
Iteration 13/25 | Loss: 0.00028926
Iteration 14/25 | Loss: 0.00028926
Iteration 15/25 | Loss: 0.00028926
Iteration 16/25 | Loss: 0.00028926
Iteration 17/25 | Loss: 0.00028926
Iteration 18/25 | Loss: 0.00028926
Iteration 19/25 | Loss: 0.00028926
Iteration 20/25 | Loss: 0.00028926
Iteration 21/25 | Loss: 0.00028926
Iteration 22/25 | Loss: 0.00028926
Iteration 23/25 | Loss: 0.00028926
Iteration 24/25 | Loss: 0.00028926
Iteration 25/25 | Loss: 0.00028926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028926
Iteration 2/1000 | Loss: 0.00002294
Iteration 3/1000 | Loss: 0.00001491
Iteration 4/1000 | Loss: 0.00001319
Iteration 5/1000 | Loss: 0.00001245
Iteration 6/1000 | Loss: 0.00001222
Iteration 7/1000 | Loss: 0.00001187
Iteration 8/1000 | Loss: 0.00001186
Iteration 9/1000 | Loss: 0.00001170
Iteration 10/1000 | Loss: 0.00001164
Iteration 11/1000 | Loss: 0.00001161
Iteration 12/1000 | Loss: 0.00001157
Iteration 13/1000 | Loss: 0.00001156
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00001148
Iteration 16/1000 | Loss: 0.00001148
Iteration 17/1000 | Loss: 0.00001147
Iteration 18/1000 | Loss: 0.00001146
Iteration 19/1000 | Loss: 0.00001146
Iteration 20/1000 | Loss: 0.00001145
Iteration 21/1000 | Loss: 0.00001145
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001143
Iteration 25/1000 | Loss: 0.00001143
Iteration 26/1000 | Loss: 0.00001142
Iteration 27/1000 | Loss: 0.00001142
Iteration 28/1000 | Loss: 0.00001142
Iteration 29/1000 | Loss: 0.00001141
Iteration 30/1000 | Loss: 0.00001141
Iteration 31/1000 | Loss: 0.00001141
Iteration 32/1000 | Loss: 0.00001141
Iteration 33/1000 | Loss: 0.00001138
Iteration 34/1000 | Loss: 0.00001138
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001135
Iteration 37/1000 | Loss: 0.00001135
Iteration 38/1000 | Loss: 0.00001134
Iteration 39/1000 | Loss: 0.00001134
Iteration 40/1000 | Loss: 0.00001133
Iteration 41/1000 | Loss: 0.00001133
Iteration 42/1000 | Loss: 0.00001133
Iteration 43/1000 | Loss: 0.00001132
Iteration 44/1000 | Loss: 0.00001132
Iteration 45/1000 | Loss: 0.00001131
Iteration 46/1000 | Loss: 0.00001131
Iteration 47/1000 | Loss: 0.00001130
Iteration 48/1000 | Loss: 0.00001129
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001128
Iteration 51/1000 | Loss: 0.00001128
Iteration 52/1000 | Loss: 0.00001128
Iteration 53/1000 | Loss: 0.00001128
Iteration 54/1000 | Loss: 0.00001128
Iteration 55/1000 | Loss: 0.00001128
Iteration 56/1000 | Loss: 0.00001128
Iteration 57/1000 | Loss: 0.00001128
Iteration 58/1000 | Loss: 0.00001128
Iteration 59/1000 | Loss: 0.00001128
Iteration 60/1000 | Loss: 0.00001128
Iteration 61/1000 | Loss: 0.00001128
Iteration 62/1000 | Loss: 0.00001128
Iteration 63/1000 | Loss: 0.00001128
Iteration 64/1000 | Loss: 0.00001128
Iteration 65/1000 | Loss: 0.00001128
Iteration 66/1000 | Loss: 0.00001128
Iteration 67/1000 | Loss: 0.00001128
Iteration 68/1000 | Loss: 0.00001128
Iteration 69/1000 | Loss: 0.00001128
Iteration 70/1000 | Loss: 0.00001128
Iteration 71/1000 | Loss: 0.00001128
Iteration 72/1000 | Loss: 0.00001128
Iteration 73/1000 | Loss: 0.00001128
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001128
Iteration 77/1000 | Loss: 0.00001128
Iteration 78/1000 | Loss: 0.00001128
Iteration 79/1000 | Loss: 0.00001128
Iteration 80/1000 | Loss: 0.00001128
Iteration 81/1000 | Loss: 0.00001128
Iteration 82/1000 | Loss: 0.00001128
Iteration 83/1000 | Loss: 0.00001128
Iteration 84/1000 | Loss: 0.00001128
Iteration 85/1000 | Loss: 0.00001128
Iteration 86/1000 | Loss: 0.00001128
Iteration 87/1000 | Loss: 0.00001128
Iteration 88/1000 | Loss: 0.00001128
Iteration 89/1000 | Loss: 0.00001128
Iteration 90/1000 | Loss: 0.00001128
Iteration 91/1000 | Loss: 0.00001128
Iteration 92/1000 | Loss: 0.00001128
Iteration 93/1000 | Loss: 0.00001128
Iteration 94/1000 | Loss: 0.00001128
Iteration 95/1000 | Loss: 0.00001128
Iteration 96/1000 | Loss: 0.00001128
Iteration 97/1000 | Loss: 0.00001128
Iteration 98/1000 | Loss: 0.00001128
Iteration 99/1000 | Loss: 0.00001128
Iteration 100/1000 | Loss: 0.00001128
Iteration 101/1000 | Loss: 0.00001128
Iteration 102/1000 | Loss: 0.00001128
Iteration 103/1000 | Loss: 0.00001128
Iteration 104/1000 | Loss: 0.00001128
Iteration 105/1000 | Loss: 0.00001128
Iteration 106/1000 | Loss: 0.00001128
Iteration 107/1000 | Loss: 0.00001128
Iteration 108/1000 | Loss: 0.00001128
Iteration 109/1000 | Loss: 0.00001128
Iteration 110/1000 | Loss: 0.00001128
Iteration 111/1000 | Loss: 0.00001128
Iteration 112/1000 | Loss: 0.00001128
Iteration 113/1000 | Loss: 0.00001128
Iteration 114/1000 | Loss: 0.00001128
Iteration 115/1000 | Loss: 0.00001128
Iteration 116/1000 | Loss: 0.00001128
Iteration 117/1000 | Loss: 0.00001128
Iteration 118/1000 | Loss: 0.00001128
Iteration 119/1000 | Loss: 0.00001128
Iteration 120/1000 | Loss: 0.00001128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.1276972145424224e-05, 1.1276972145424224e-05, 1.1276972145424224e-05, 1.1276972145424224e-05, 1.1276972145424224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1276972145424224e-05

Optimization complete. Final v2v error: 2.864346742630005 mm

Highest mean error: 3.051406145095825 mm for frame 105

Lowest mean error: 2.717494487762451 mm for frame 15

Saving results

Total time: 26.843238353729248
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064687
Iteration 2/25 | Loss: 0.00190383
Iteration 3/25 | Loss: 0.00117523
Iteration 4/25 | Loss: 0.00097031
Iteration 5/25 | Loss: 0.00083593
Iteration 6/25 | Loss: 0.00080911
Iteration 7/25 | Loss: 0.00079763
Iteration 8/25 | Loss: 0.00078094
Iteration 9/25 | Loss: 0.00077166
Iteration 10/25 | Loss: 0.00078381
Iteration 11/25 | Loss: 0.00078367
Iteration 12/25 | Loss: 0.00075662
Iteration 13/25 | Loss: 0.00075631
Iteration 14/25 | Loss: 0.00074896
Iteration 15/25 | Loss: 0.00074836
Iteration 16/25 | Loss: 0.00074824
Iteration 17/25 | Loss: 0.00074824
Iteration 18/25 | Loss: 0.00074823
Iteration 19/25 | Loss: 0.00074823
Iteration 20/25 | Loss: 0.00074821
Iteration 21/25 | Loss: 0.00074821
Iteration 22/25 | Loss: 0.00074820
Iteration 23/25 | Loss: 0.00074820
Iteration 24/25 | Loss: 0.00074820
Iteration 25/25 | Loss: 0.00074820

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32230008
Iteration 2/25 | Loss: 0.00029512
Iteration 3/25 | Loss: 0.00029512
Iteration 4/25 | Loss: 0.00029512
Iteration 5/25 | Loss: 0.00029512
Iteration 6/25 | Loss: 0.00029512
Iteration 7/25 | Loss: 0.00029512
Iteration 8/25 | Loss: 0.00029512
Iteration 9/25 | Loss: 0.00029512
Iteration 10/25 | Loss: 0.00029512
Iteration 11/25 | Loss: 0.00029511
Iteration 12/25 | Loss: 0.00029511
Iteration 13/25 | Loss: 0.00029511
Iteration 14/25 | Loss: 0.00029511
Iteration 15/25 | Loss: 0.00029511
Iteration 16/25 | Loss: 0.00029511
Iteration 17/25 | Loss: 0.00029511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002951147616840899, 0.0002951147616840899, 0.0002951147616840899, 0.0002951147616840899, 0.0002951147616840899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002951147616840899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029511
Iteration 2/1000 | Loss: 0.00003602
Iteration 3/1000 | Loss: 0.00002838
Iteration 4/1000 | Loss: 0.00004192
Iteration 5/1000 | Loss: 0.00002380
Iteration 6/1000 | Loss: 0.00002197
Iteration 7/1000 | Loss: 0.00002021
Iteration 8/1000 | Loss: 0.00026152
Iteration 9/1000 | Loss: 0.00001929
Iteration 10/1000 | Loss: 0.00013261
Iteration 11/1000 | Loss: 0.00003893
Iteration 12/1000 | Loss: 0.00002561
Iteration 13/1000 | Loss: 0.00001840
Iteration 14/1000 | Loss: 0.00001811
Iteration 15/1000 | Loss: 0.00001787
Iteration 16/1000 | Loss: 0.00001770
Iteration 17/1000 | Loss: 0.00001769
Iteration 18/1000 | Loss: 0.00001766
Iteration 19/1000 | Loss: 0.00001766
Iteration 20/1000 | Loss: 0.00001765
Iteration 21/1000 | Loss: 0.00001765
Iteration 22/1000 | Loss: 0.00001764
Iteration 23/1000 | Loss: 0.00001763
Iteration 24/1000 | Loss: 0.00001762
Iteration 25/1000 | Loss: 0.00001756
Iteration 26/1000 | Loss: 0.00001756
Iteration 27/1000 | Loss: 0.00001750
Iteration 28/1000 | Loss: 0.00001750
Iteration 29/1000 | Loss: 0.00001750
Iteration 30/1000 | Loss: 0.00001750
Iteration 31/1000 | Loss: 0.00001749
Iteration 32/1000 | Loss: 0.00001749
Iteration 33/1000 | Loss: 0.00001748
Iteration 34/1000 | Loss: 0.00001747
Iteration 35/1000 | Loss: 0.00001745
Iteration 36/1000 | Loss: 0.00001745
Iteration 37/1000 | Loss: 0.00001745
Iteration 38/1000 | Loss: 0.00001745
Iteration 39/1000 | Loss: 0.00001745
Iteration 40/1000 | Loss: 0.00001744
Iteration 41/1000 | Loss: 0.00001744
Iteration 42/1000 | Loss: 0.00001744
Iteration 43/1000 | Loss: 0.00001744
Iteration 44/1000 | Loss: 0.00001743
Iteration 45/1000 | Loss: 0.00001743
Iteration 46/1000 | Loss: 0.00001743
Iteration 47/1000 | Loss: 0.00001743
Iteration 48/1000 | Loss: 0.00001743
Iteration 49/1000 | Loss: 0.00001743
Iteration 50/1000 | Loss: 0.00001742
Iteration 51/1000 | Loss: 0.00001742
Iteration 52/1000 | Loss: 0.00001742
Iteration 53/1000 | Loss: 0.00001742
Iteration 54/1000 | Loss: 0.00001742
Iteration 55/1000 | Loss: 0.00001742
Iteration 56/1000 | Loss: 0.00001742
Iteration 57/1000 | Loss: 0.00001742
Iteration 58/1000 | Loss: 0.00001742
Iteration 59/1000 | Loss: 0.00001742
Iteration 60/1000 | Loss: 0.00001742
Iteration 61/1000 | Loss: 0.00001742
Iteration 62/1000 | Loss: 0.00001741
Iteration 63/1000 | Loss: 0.00001741
Iteration 64/1000 | Loss: 0.00001741
Iteration 65/1000 | Loss: 0.00001741
Iteration 66/1000 | Loss: 0.00001741
Iteration 67/1000 | Loss: 0.00001741
Iteration 68/1000 | Loss: 0.00001741
Iteration 69/1000 | Loss: 0.00001741
Iteration 70/1000 | Loss: 0.00001741
Iteration 71/1000 | Loss: 0.00001741
Iteration 72/1000 | Loss: 0.00001741
Iteration 73/1000 | Loss: 0.00001740
Iteration 74/1000 | Loss: 0.00001740
Iteration 75/1000 | Loss: 0.00001740
Iteration 76/1000 | Loss: 0.00001740
Iteration 77/1000 | Loss: 0.00001740
Iteration 78/1000 | Loss: 0.00001740
Iteration 79/1000 | Loss: 0.00001740
Iteration 80/1000 | Loss: 0.00001740
Iteration 81/1000 | Loss: 0.00001740
Iteration 82/1000 | Loss: 0.00001740
Iteration 83/1000 | Loss: 0.00001740
Iteration 84/1000 | Loss: 0.00001740
Iteration 85/1000 | Loss: 0.00001740
Iteration 86/1000 | Loss: 0.00001740
Iteration 87/1000 | Loss: 0.00001740
Iteration 88/1000 | Loss: 0.00001740
Iteration 89/1000 | Loss: 0.00001740
Iteration 90/1000 | Loss: 0.00001740
Iteration 91/1000 | Loss: 0.00001740
Iteration 92/1000 | Loss: 0.00001740
Iteration 93/1000 | Loss: 0.00001740
Iteration 94/1000 | Loss: 0.00001740
Iteration 95/1000 | Loss: 0.00001740
Iteration 96/1000 | Loss: 0.00001740
Iteration 97/1000 | Loss: 0.00001740
Iteration 98/1000 | Loss: 0.00001740
Iteration 99/1000 | Loss: 0.00001740
Iteration 100/1000 | Loss: 0.00001740
Iteration 101/1000 | Loss: 0.00001740
Iteration 102/1000 | Loss: 0.00001740
Iteration 103/1000 | Loss: 0.00001740
Iteration 104/1000 | Loss: 0.00001740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.7397220290149562e-05, 1.7397220290149562e-05, 1.7397220290149562e-05, 1.7397220290149562e-05, 1.7397220290149562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7397220290149562e-05

Optimization complete. Final v2v error: 3.4825401306152344 mm

Highest mean error: 4.99052619934082 mm for frame 229

Lowest mean error: 3.220919370651245 mm for frame 97

Saving results

Total time: 64.95258712768555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827116
Iteration 2/25 | Loss: 0.00085599
Iteration 3/25 | Loss: 0.00069334
Iteration 4/25 | Loss: 0.00066103
Iteration 5/25 | Loss: 0.00065295
Iteration 6/25 | Loss: 0.00065156
Iteration 7/25 | Loss: 0.00065109
Iteration 8/25 | Loss: 0.00065109
Iteration 9/25 | Loss: 0.00065109
Iteration 10/25 | Loss: 0.00065109
Iteration 11/25 | Loss: 0.00065109
Iteration 12/25 | Loss: 0.00065108
Iteration 13/25 | Loss: 0.00065108
Iteration 14/25 | Loss: 0.00065108
Iteration 15/25 | Loss: 0.00065108
Iteration 16/25 | Loss: 0.00065108
Iteration 17/25 | Loss: 0.00065108
Iteration 18/25 | Loss: 0.00065108
Iteration 19/25 | Loss: 0.00065108
Iteration 20/25 | Loss: 0.00065108
Iteration 21/25 | Loss: 0.00065108
Iteration 22/25 | Loss: 0.00065108
Iteration 23/25 | Loss: 0.00065108
Iteration 24/25 | Loss: 0.00065108
Iteration 25/25 | Loss: 0.00065108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51032913
Iteration 2/25 | Loss: 0.00032462
Iteration 3/25 | Loss: 0.00032461
Iteration 4/25 | Loss: 0.00032461
Iteration 5/25 | Loss: 0.00032461
Iteration 6/25 | Loss: 0.00032461
Iteration 7/25 | Loss: 0.00032461
Iteration 8/25 | Loss: 0.00032461
Iteration 9/25 | Loss: 0.00032461
Iteration 10/25 | Loss: 0.00032461
Iteration 11/25 | Loss: 0.00032461
Iteration 12/25 | Loss: 0.00032461
Iteration 13/25 | Loss: 0.00032461
Iteration 14/25 | Loss: 0.00032461
Iteration 15/25 | Loss: 0.00032461
Iteration 16/25 | Loss: 0.00032461
Iteration 17/25 | Loss: 0.00032461
Iteration 18/25 | Loss: 0.00032461
Iteration 19/25 | Loss: 0.00032461
Iteration 20/25 | Loss: 0.00032461
Iteration 21/25 | Loss: 0.00032461
Iteration 22/25 | Loss: 0.00032461
Iteration 23/25 | Loss: 0.00032461
Iteration 24/25 | Loss: 0.00032461
Iteration 25/25 | Loss: 0.00032461

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032461
Iteration 2/1000 | Loss: 0.00002922
Iteration 3/1000 | Loss: 0.00002030
Iteration 4/1000 | Loss: 0.00001899
Iteration 5/1000 | Loss: 0.00001818
Iteration 6/1000 | Loss: 0.00001773
Iteration 7/1000 | Loss: 0.00001729
Iteration 8/1000 | Loss: 0.00001695
Iteration 9/1000 | Loss: 0.00001668
Iteration 10/1000 | Loss: 0.00001668
Iteration 11/1000 | Loss: 0.00001667
Iteration 12/1000 | Loss: 0.00001667
Iteration 13/1000 | Loss: 0.00001657
Iteration 14/1000 | Loss: 0.00001656
Iteration 15/1000 | Loss: 0.00001647
Iteration 16/1000 | Loss: 0.00001638
Iteration 17/1000 | Loss: 0.00001635
Iteration 18/1000 | Loss: 0.00001635
Iteration 19/1000 | Loss: 0.00001634
Iteration 20/1000 | Loss: 0.00001634
Iteration 21/1000 | Loss: 0.00001633
Iteration 22/1000 | Loss: 0.00001632
Iteration 23/1000 | Loss: 0.00001631
Iteration 24/1000 | Loss: 0.00001631
Iteration 25/1000 | Loss: 0.00001629
Iteration 26/1000 | Loss: 0.00001627
Iteration 27/1000 | Loss: 0.00001627
Iteration 28/1000 | Loss: 0.00001626
Iteration 29/1000 | Loss: 0.00001626
Iteration 30/1000 | Loss: 0.00001626
Iteration 31/1000 | Loss: 0.00001625
Iteration 32/1000 | Loss: 0.00001624
Iteration 33/1000 | Loss: 0.00001624
Iteration 34/1000 | Loss: 0.00001622
Iteration 35/1000 | Loss: 0.00001622
Iteration 36/1000 | Loss: 0.00001622
Iteration 37/1000 | Loss: 0.00001621
Iteration 38/1000 | Loss: 0.00001621
Iteration 39/1000 | Loss: 0.00001621
Iteration 40/1000 | Loss: 0.00001620
Iteration 41/1000 | Loss: 0.00001620
Iteration 42/1000 | Loss: 0.00001620
Iteration 43/1000 | Loss: 0.00001619
Iteration 44/1000 | Loss: 0.00001618
Iteration 45/1000 | Loss: 0.00001617
Iteration 46/1000 | Loss: 0.00001617
Iteration 47/1000 | Loss: 0.00001617
Iteration 48/1000 | Loss: 0.00001617
Iteration 49/1000 | Loss: 0.00001616
Iteration 50/1000 | Loss: 0.00001616
Iteration 51/1000 | Loss: 0.00001615
Iteration 52/1000 | Loss: 0.00001614
Iteration 53/1000 | Loss: 0.00001614
Iteration 54/1000 | Loss: 0.00001614
Iteration 55/1000 | Loss: 0.00001614
Iteration 56/1000 | Loss: 0.00001614
Iteration 57/1000 | Loss: 0.00001614
Iteration 58/1000 | Loss: 0.00001614
Iteration 59/1000 | Loss: 0.00001614
Iteration 60/1000 | Loss: 0.00001614
Iteration 61/1000 | Loss: 0.00001614
Iteration 62/1000 | Loss: 0.00001614
Iteration 63/1000 | Loss: 0.00001614
Iteration 64/1000 | Loss: 0.00001613
Iteration 65/1000 | Loss: 0.00001613
Iteration 66/1000 | Loss: 0.00001613
Iteration 67/1000 | Loss: 0.00001613
Iteration 68/1000 | Loss: 0.00001612
Iteration 69/1000 | Loss: 0.00001612
Iteration 70/1000 | Loss: 0.00001611
Iteration 71/1000 | Loss: 0.00001611
Iteration 72/1000 | Loss: 0.00001610
Iteration 73/1000 | Loss: 0.00001610
Iteration 74/1000 | Loss: 0.00001610
Iteration 75/1000 | Loss: 0.00001610
Iteration 76/1000 | Loss: 0.00001610
Iteration 77/1000 | Loss: 0.00001608
Iteration 78/1000 | Loss: 0.00001608
Iteration 79/1000 | Loss: 0.00001607
Iteration 80/1000 | Loss: 0.00001607
Iteration 81/1000 | Loss: 0.00001606
Iteration 82/1000 | Loss: 0.00001606
Iteration 83/1000 | Loss: 0.00001606
Iteration 84/1000 | Loss: 0.00001605
Iteration 85/1000 | Loss: 0.00001605
Iteration 86/1000 | Loss: 0.00001605
Iteration 87/1000 | Loss: 0.00001604
Iteration 88/1000 | Loss: 0.00001604
Iteration 89/1000 | Loss: 0.00001604
Iteration 90/1000 | Loss: 0.00001604
Iteration 91/1000 | Loss: 0.00001603
Iteration 92/1000 | Loss: 0.00001603
Iteration 93/1000 | Loss: 0.00001603
Iteration 94/1000 | Loss: 0.00001603
Iteration 95/1000 | Loss: 0.00001603
Iteration 96/1000 | Loss: 0.00001603
Iteration 97/1000 | Loss: 0.00001603
Iteration 98/1000 | Loss: 0.00001603
Iteration 99/1000 | Loss: 0.00001603
Iteration 100/1000 | Loss: 0.00001603
Iteration 101/1000 | Loss: 0.00001602
Iteration 102/1000 | Loss: 0.00001602
Iteration 103/1000 | Loss: 0.00001602
Iteration 104/1000 | Loss: 0.00001602
Iteration 105/1000 | Loss: 0.00001601
Iteration 106/1000 | Loss: 0.00001601
Iteration 107/1000 | Loss: 0.00001601
Iteration 108/1000 | Loss: 0.00001601
Iteration 109/1000 | Loss: 0.00001601
Iteration 110/1000 | Loss: 0.00001601
Iteration 111/1000 | Loss: 0.00001601
Iteration 112/1000 | Loss: 0.00001601
Iteration 113/1000 | Loss: 0.00001601
Iteration 114/1000 | Loss: 0.00001601
Iteration 115/1000 | Loss: 0.00001601
Iteration 116/1000 | Loss: 0.00001601
Iteration 117/1000 | Loss: 0.00001601
Iteration 118/1000 | Loss: 0.00001601
Iteration 119/1000 | Loss: 0.00001601
Iteration 120/1000 | Loss: 0.00001601
Iteration 121/1000 | Loss: 0.00001601
Iteration 122/1000 | Loss: 0.00001601
Iteration 123/1000 | Loss: 0.00001601
Iteration 124/1000 | Loss: 0.00001601
Iteration 125/1000 | Loss: 0.00001601
Iteration 126/1000 | Loss: 0.00001601
Iteration 127/1000 | Loss: 0.00001600
Iteration 128/1000 | Loss: 0.00001600
Iteration 129/1000 | Loss: 0.00001600
Iteration 130/1000 | Loss: 0.00001600
Iteration 131/1000 | Loss: 0.00001600
Iteration 132/1000 | Loss: 0.00001600
Iteration 133/1000 | Loss: 0.00001600
Iteration 134/1000 | Loss: 0.00001600
Iteration 135/1000 | Loss: 0.00001600
Iteration 136/1000 | Loss: 0.00001600
Iteration 137/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.6004330973373726e-05, 1.6004330973373726e-05, 1.6004330973373726e-05, 1.6004330973373726e-05, 1.6004330973373726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6004330973373726e-05

Optimization complete. Final v2v error: 3.419658660888672 mm

Highest mean error: 3.987464666366577 mm for frame 99

Lowest mean error: 3.198568820953369 mm for frame 40

Saving results

Total time: 34.80430746078491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830768
Iteration 2/25 | Loss: 0.00109648
Iteration 3/25 | Loss: 0.00080504
Iteration 4/25 | Loss: 0.00074087
Iteration 5/25 | Loss: 0.00071561
Iteration 6/25 | Loss: 0.00069844
Iteration 7/25 | Loss: 0.00068589
Iteration 8/25 | Loss: 0.00068588
Iteration 9/25 | Loss: 0.00067808
Iteration 10/25 | Loss: 0.00067367
Iteration 11/25 | Loss: 0.00067191
Iteration 12/25 | Loss: 0.00067111
Iteration 13/25 | Loss: 0.00067082
Iteration 14/25 | Loss: 0.00067063
Iteration 15/25 | Loss: 0.00067045
Iteration 16/25 | Loss: 0.00067037
Iteration 17/25 | Loss: 0.00067037
Iteration 18/25 | Loss: 0.00067036
Iteration 19/25 | Loss: 0.00067036
Iteration 20/25 | Loss: 0.00067036
Iteration 21/25 | Loss: 0.00067036
Iteration 22/25 | Loss: 0.00067036
Iteration 23/25 | Loss: 0.00067036
Iteration 24/25 | Loss: 0.00067036
Iteration 25/25 | Loss: 0.00067036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90756917
Iteration 2/25 | Loss: 0.00055891
Iteration 3/25 | Loss: 0.00054621
Iteration 4/25 | Loss: 0.00054621
Iteration 5/25 | Loss: 0.00054621
Iteration 6/25 | Loss: 0.00054621
Iteration 7/25 | Loss: 0.00054621
Iteration 8/25 | Loss: 0.00054621
Iteration 9/25 | Loss: 0.00054621
Iteration 10/25 | Loss: 0.00054621
Iteration 11/25 | Loss: 0.00054621
Iteration 12/25 | Loss: 0.00054621
Iteration 13/25 | Loss: 0.00054621
Iteration 14/25 | Loss: 0.00054621
Iteration 15/25 | Loss: 0.00054621
Iteration 16/25 | Loss: 0.00054621
Iteration 17/25 | Loss: 0.00054621
Iteration 18/25 | Loss: 0.00054621
Iteration 19/25 | Loss: 0.00054621
Iteration 20/25 | Loss: 0.00054621
Iteration 21/25 | Loss: 0.00054621
Iteration 22/25 | Loss: 0.00054621
Iteration 23/25 | Loss: 0.00054621
Iteration 24/25 | Loss: 0.00054621
Iteration 25/25 | Loss: 0.00054621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054621
Iteration 2/1000 | Loss: 0.00006870
Iteration 3/1000 | Loss: 0.00004076
Iteration 4/1000 | Loss: 0.00003394
Iteration 5/1000 | Loss: 0.00003051
Iteration 6/1000 | Loss: 0.00002769
Iteration 7/1000 | Loss: 0.00002632
Iteration 8/1000 | Loss: 0.00002533
Iteration 9/1000 | Loss: 0.00002468
Iteration 10/1000 | Loss: 0.00002438
Iteration 11/1000 | Loss: 0.00002407
Iteration 12/1000 | Loss: 0.00002375
Iteration 13/1000 | Loss: 0.00002352
Iteration 14/1000 | Loss: 0.00002347
Iteration 15/1000 | Loss: 0.00002344
Iteration 16/1000 | Loss: 0.00002335
Iteration 17/1000 | Loss: 0.00079136
Iteration 18/1000 | Loss: 0.00072273
Iteration 19/1000 | Loss: 0.00004822
Iteration 20/1000 | Loss: 0.00014059
Iteration 21/1000 | Loss: 0.00002822
Iteration 22/1000 | Loss: 0.00004812
Iteration 23/1000 | Loss: 0.00002174
Iteration 24/1000 | Loss: 0.00001959
Iteration 25/1000 | Loss: 0.00001854
Iteration 26/1000 | Loss: 0.00001769
Iteration 27/1000 | Loss: 0.00001709
Iteration 28/1000 | Loss: 0.00006887
Iteration 29/1000 | Loss: 0.00001654
Iteration 30/1000 | Loss: 0.00001631
Iteration 31/1000 | Loss: 0.00001623
Iteration 32/1000 | Loss: 0.00001600
Iteration 33/1000 | Loss: 0.00001576
Iteration 34/1000 | Loss: 0.00001556
Iteration 35/1000 | Loss: 0.00001537
Iteration 36/1000 | Loss: 0.00001516
Iteration 37/1000 | Loss: 0.00001497
Iteration 38/1000 | Loss: 0.00001493
Iteration 39/1000 | Loss: 0.00001484
Iteration 40/1000 | Loss: 0.00001483
Iteration 41/1000 | Loss: 0.00001482
Iteration 42/1000 | Loss: 0.00001480
Iteration 43/1000 | Loss: 0.00001479
Iteration 44/1000 | Loss: 0.00001479
Iteration 45/1000 | Loss: 0.00001478
Iteration 46/1000 | Loss: 0.00001478
Iteration 47/1000 | Loss: 0.00001478
Iteration 48/1000 | Loss: 0.00001478
Iteration 49/1000 | Loss: 0.00001478
Iteration 50/1000 | Loss: 0.00001478
Iteration 51/1000 | Loss: 0.00001477
Iteration 52/1000 | Loss: 0.00001477
Iteration 53/1000 | Loss: 0.00001477
Iteration 54/1000 | Loss: 0.00001477
Iteration 55/1000 | Loss: 0.00001477
Iteration 56/1000 | Loss: 0.00001477
Iteration 57/1000 | Loss: 0.00001477
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001476
Iteration 61/1000 | Loss: 0.00001476
Iteration 62/1000 | Loss: 0.00001476
Iteration 63/1000 | Loss: 0.00001476
Iteration 64/1000 | Loss: 0.00001475
Iteration 65/1000 | Loss: 0.00001475
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001475
Iteration 74/1000 | Loss: 0.00001475
Iteration 75/1000 | Loss: 0.00001474
Iteration 76/1000 | Loss: 0.00001474
Iteration 77/1000 | Loss: 0.00001474
Iteration 78/1000 | Loss: 0.00001474
Iteration 79/1000 | Loss: 0.00001474
Iteration 80/1000 | Loss: 0.00001474
Iteration 81/1000 | Loss: 0.00001474
Iteration 82/1000 | Loss: 0.00001474
Iteration 83/1000 | Loss: 0.00001474
Iteration 84/1000 | Loss: 0.00001474
Iteration 85/1000 | Loss: 0.00001474
Iteration 86/1000 | Loss: 0.00001474
Iteration 87/1000 | Loss: 0.00001474
Iteration 88/1000 | Loss: 0.00001474
Iteration 89/1000 | Loss: 0.00001474
Iteration 90/1000 | Loss: 0.00001474
Iteration 91/1000 | Loss: 0.00001474
Iteration 92/1000 | Loss: 0.00001474
Iteration 93/1000 | Loss: 0.00001474
Iteration 94/1000 | Loss: 0.00001474
Iteration 95/1000 | Loss: 0.00001474
Iteration 96/1000 | Loss: 0.00001474
Iteration 97/1000 | Loss: 0.00001474
Iteration 98/1000 | Loss: 0.00001474
Iteration 99/1000 | Loss: 0.00001474
Iteration 100/1000 | Loss: 0.00001474
Iteration 101/1000 | Loss: 0.00001474
Iteration 102/1000 | Loss: 0.00001474
Iteration 103/1000 | Loss: 0.00001474
Iteration 104/1000 | Loss: 0.00001474
Iteration 105/1000 | Loss: 0.00001474
Iteration 106/1000 | Loss: 0.00001474
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001474
Iteration 109/1000 | Loss: 0.00001474
Iteration 110/1000 | Loss: 0.00001474
Iteration 111/1000 | Loss: 0.00001474
Iteration 112/1000 | Loss: 0.00001474
Iteration 113/1000 | Loss: 0.00001474
Iteration 114/1000 | Loss: 0.00001474
Iteration 115/1000 | Loss: 0.00001474
Iteration 116/1000 | Loss: 0.00001474
Iteration 117/1000 | Loss: 0.00001474
Iteration 118/1000 | Loss: 0.00001474
Iteration 119/1000 | Loss: 0.00001474
Iteration 120/1000 | Loss: 0.00001474
Iteration 121/1000 | Loss: 0.00001474
Iteration 122/1000 | Loss: 0.00001474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.4738880054210313e-05, 1.4738880054210313e-05, 1.4738880054210313e-05, 1.4738880054210313e-05, 1.4738880054210313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4738880054210313e-05

Optimization complete. Final v2v error: 3.2927675247192383 mm

Highest mean error: 4.50434684753418 mm for frame 48

Lowest mean error: 3.0200228691101074 mm for frame 62

Saving results

Total time: 78.47566986083984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078783
Iteration 2/25 | Loss: 0.00209416
Iteration 3/25 | Loss: 0.00135848
Iteration 4/25 | Loss: 0.00133802
Iteration 5/25 | Loss: 0.00087874
Iteration 6/25 | Loss: 0.00094925
Iteration 7/25 | Loss: 0.00093209
Iteration 8/25 | Loss: 0.00082951
Iteration 9/25 | Loss: 0.00079496
Iteration 10/25 | Loss: 0.00075298
Iteration 11/25 | Loss: 0.00072640
Iteration 12/25 | Loss: 0.00070510
Iteration 13/25 | Loss: 0.00070397
Iteration 14/25 | Loss: 0.00069635
Iteration 15/25 | Loss: 0.00069108
Iteration 16/25 | Loss: 0.00068980
Iteration 17/25 | Loss: 0.00067940
Iteration 18/25 | Loss: 0.00068158
Iteration 19/25 | Loss: 0.00067981
Iteration 20/25 | Loss: 0.00067639
Iteration 21/25 | Loss: 0.00067447
Iteration 22/25 | Loss: 0.00067333
Iteration 23/25 | Loss: 0.00067013
Iteration 24/25 | Loss: 0.00067048
Iteration 25/25 | Loss: 0.00067329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49538434
Iteration 2/25 | Loss: 0.00042706
Iteration 3/25 | Loss: 0.00042706
Iteration 4/25 | Loss: 0.00042706
Iteration 5/25 | Loss: 0.00042706
Iteration 6/25 | Loss: 0.00042706
Iteration 7/25 | Loss: 0.00042706
Iteration 8/25 | Loss: 0.00042706
Iteration 9/25 | Loss: 0.00042706
Iteration 10/25 | Loss: 0.00042706
Iteration 11/25 | Loss: 0.00042706
Iteration 12/25 | Loss: 0.00042706
Iteration 13/25 | Loss: 0.00042706
Iteration 14/25 | Loss: 0.00042706
Iteration 15/25 | Loss: 0.00042706
Iteration 16/25 | Loss: 0.00042706
Iteration 17/25 | Loss: 0.00042706
Iteration 18/25 | Loss: 0.00042706
Iteration 19/25 | Loss: 0.00042706
Iteration 20/25 | Loss: 0.00042706
Iteration 21/25 | Loss: 0.00042706
Iteration 22/25 | Loss: 0.00042706
Iteration 23/25 | Loss: 0.00042706
Iteration 24/25 | Loss: 0.00042706
Iteration 25/25 | Loss: 0.00042706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042706
Iteration 2/1000 | Loss: 0.00047169
Iteration 3/1000 | Loss: 0.00007269
Iteration 4/1000 | Loss: 0.00044604
Iteration 5/1000 | Loss: 0.00028356
Iteration 6/1000 | Loss: 0.00020385
Iteration 7/1000 | Loss: 0.00018641
Iteration 8/1000 | Loss: 0.00026374
Iteration 9/1000 | Loss: 0.00024973
Iteration 10/1000 | Loss: 0.00023414
Iteration 11/1000 | Loss: 0.00022900
Iteration 12/1000 | Loss: 0.00028666
Iteration 13/1000 | Loss: 0.00025965
Iteration 14/1000 | Loss: 0.00014841
Iteration 15/1000 | Loss: 0.00016712
Iteration 16/1000 | Loss: 0.00038337
Iteration 17/1000 | Loss: 0.00034882
Iteration 18/1000 | Loss: 0.00029522
Iteration 19/1000 | Loss: 0.00035445
Iteration 20/1000 | Loss: 0.00022401
Iteration 21/1000 | Loss: 0.00012538
Iteration 22/1000 | Loss: 0.00033489
Iteration 23/1000 | Loss: 0.00021900
Iteration 24/1000 | Loss: 0.00020848
Iteration 25/1000 | Loss: 0.00023406
Iteration 26/1000 | Loss: 0.00035069
Iteration 27/1000 | Loss: 0.00040230
Iteration 28/1000 | Loss: 0.00029310
Iteration 29/1000 | Loss: 0.00016838
Iteration 30/1000 | Loss: 0.00027178
Iteration 31/1000 | Loss: 0.00035930
Iteration 32/1000 | Loss: 0.00029017
Iteration 33/1000 | Loss: 0.00034241
Iteration 34/1000 | Loss: 0.00033187
Iteration 35/1000 | Loss: 0.00033685
Iteration 36/1000 | Loss: 0.00042023
Iteration 37/1000 | Loss: 0.00035824
Iteration 38/1000 | Loss: 0.00027382
Iteration 39/1000 | Loss: 0.00029026
Iteration 40/1000 | Loss: 0.00031681
Iteration 41/1000 | Loss: 0.00018913
Iteration 42/1000 | Loss: 0.00029008
Iteration 43/1000 | Loss: 0.00030015
Iteration 44/1000 | Loss: 0.00033959
Iteration 45/1000 | Loss: 0.00019888
Iteration 46/1000 | Loss: 0.00023174
Iteration 47/1000 | Loss: 0.00020893
Iteration 48/1000 | Loss: 0.00023670
Iteration 49/1000 | Loss: 0.00025203
Iteration 50/1000 | Loss: 0.00007339
Iteration 51/1000 | Loss: 0.00022138
Iteration 52/1000 | Loss: 0.00006856
Iteration 53/1000 | Loss: 0.00016553
Iteration 54/1000 | Loss: 0.00016448
Iteration 55/1000 | Loss: 0.00014186
Iteration 56/1000 | Loss: 0.00006130
Iteration 57/1000 | Loss: 0.00030613
Iteration 58/1000 | Loss: 0.00005160
Iteration 59/1000 | Loss: 0.00022830
Iteration 60/1000 | Loss: 0.00039484
Iteration 61/1000 | Loss: 0.00031045
Iteration 62/1000 | Loss: 0.00017931
Iteration 63/1000 | Loss: 0.00030642
Iteration 64/1000 | Loss: 0.00026439
Iteration 65/1000 | Loss: 0.00025279
Iteration 66/1000 | Loss: 0.00029500
Iteration 67/1000 | Loss: 0.00033267
Iteration 68/1000 | Loss: 0.00033579
Iteration 69/1000 | Loss: 0.00034228
Iteration 70/1000 | Loss: 0.00038462
Iteration 71/1000 | Loss: 0.00035321
Iteration 72/1000 | Loss: 0.00021585
Iteration 73/1000 | Loss: 0.00019037
Iteration 74/1000 | Loss: 0.00019236
Iteration 75/1000 | Loss: 0.00028647
Iteration 76/1000 | Loss: 0.00015601
Iteration 77/1000 | Loss: 0.00024657
Iteration 78/1000 | Loss: 0.00038235
Iteration 79/1000 | Loss: 0.00033123
Iteration 80/1000 | Loss: 0.00041287
Iteration 81/1000 | Loss: 0.00023785
Iteration 82/1000 | Loss: 0.00032990
Iteration 83/1000 | Loss: 0.00025708
Iteration 84/1000 | Loss: 0.00024260
Iteration 85/1000 | Loss: 0.00022247
Iteration 86/1000 | Loss: 0.00044877
Iteration 87/1000 | Loss: 0.00038377
Iteration 88/1000 | Loss: 0.00041045
Iteration 89/1000 | Loss: 0.00035961
Iteration 90/1000 | Loss: 0.00032677
Iteration 91/1000 | Loss: 0.00032278
Iteration 92/1000 | Loss: 0.00029341
Iteration 93/1000 | Loss: 0.00030827
Iteration 94/1000 | Loss: 0.00030805
Iteration 95/1000 | Loss: 0.00029495
Iteration 96/1000 | Loss: 0.00035675
Iteration 97/1000 | Loss: 0.00033480
Iteration 98/1000 | Loss: 0.00029247
Iteration 99/1000 | Loss: 0.00012607
Iteration 100/1000 | Loss: 0.00030903
Iteration 101/1000 | Loss: 0.00031276
Iteration 102/1000 | Loss: 0.00032964
Iteration 103/1000 | Loss: 0.00026129
Iteration 104/1000 | Loss: 0.00028045
Iteration 105/1000 | Loss: 0.00027637
Iteration 106/1000 | Loss: 0.00028419
Iteration 107/1000 | Loss: 0.00019178
Iteration 108/1000 | Loss: 0.00024251
Iteration 109/1000 | Loss: 0.00043845
Iteration 110/1000 | Loss: 0.00051838
Iteration 111/1000 | Loss: 0.00046024
Iteration 112/1000 | Loss: 0.00051570
Iteration 113/1000 | Loss: 0.00022494
Iteration 114/1000 | Loss: 0.00035089
Iteration 115/1000 | Loss: 0.00029679
Iteration 116/1000 | Loss: 0.00034740
Iteration 117/1000 | Loss: 0.00039830
Iteration 118/1000 | Loss: 0.00026748
Iteration 119/1000 | Loss: 0.00030905
Iteration 120/1000 | Loss: 0.00072158
Iteration 121/1000 | Loss: 0.00051768
Iteration 122/1000 | Loss: 0.00024549
Iteration 123/1000 | Loss: 0.00014315
Iteration 124/1000 | Loss: 0.00023568
Iteration 125/1000 | Loss: 0.00023749
Iteration 126/1000 | Loss: 0.00009590
Iteration 127/1000 | Loss: 0.00009218
Iteration 128/1000 | Loss: 0.00019441
Iteration 129/1000 | Loss: 0.00038652
Iteration 130/1000 | Loss: 0.00019725
Iteration 131/1000 | Loss: 0.00005041
Iteration 132/1000 | Loss: 0.00013612
Iteration 133/1000 | Loss: 0.00013785
Iteration 134/1000 | Loss: 0.00007502
Iteration 135/1000 | Loss: 0.00014622
Iteration 136/1000 | Loss: 0.00007452
Iteration 137/1000 | Loss: 0.00013027
Iteration 138/1000 | Loss: 0.00018462
Iteration 139/1000 | Loss: 0.00013457
Iteration 140/1000 | Loss: 0.00020568
Iteration 141/1000 | Loss: 0.00017525
Iteration 142/1000 | Loss: 0.00015950
Iteration 143/1000 | Loss: 0.00006747
Iteration 144/1000 | Loss: 0.00010881
Iteration 145/1000 | Loss: 0.00017802
Iteration 146/1000 | Loss: 0.00018884
Iteration 147/1000 | Loss: 0.00020520
Iteration 148/1000 | Loss: 0.00021201
Iteration 149/1000 | Loss: 0.00023248
Iteration 150/1000 | Loss: 0.00015680
Iteration 151/1000 | Loss: 0.00014589
Iteration 152/1000 | Loss: 0.00015705
Iteration 153/1000 | Loss: 0.00018248
Iteration 154/1000 | Loss: 0.00013046
Iteration 155/1000 | Loss: 0.00016045
Iteration 156/1000 | Loss: 0.00012251
Iteration 157/1000 | Loss: 0.00016806
Iteration 158/1000 | Loss: 0.00015418
Iteration 159/1000 | Loss: 0.00006799
Iteration 160/1000 | Loss: 0.00023584
Iteration 161/1000 | Loss: 0.00019655
Iteration 162/1000 | Loss: 0.00016869
Iteration 163/1000 | Loss: 0.00013821
Iteration 164/1000 | Loss: 0.00018777
Iteration 165/1000 | Loss: 0.00018722
Iteration 166/1000 | Loss: 0.00013199
Iteration 167/1000 | Loss: 0.00017653
Iteration 168/1000 | Loss: 0.00027503
Iteration 169/1000 | Loss: 0.00021936
Iteration 170/1000 | Loss: 0.00014058
Iteration 171/1000 | Loss: 0.00024130
Iteration 172/1000 | Loss: 0.00024591
Iteration 173/1000 | Loss: 0.00010153
Iteration 174/1000 | Loss: 0.00004164
Iteration 175/1000 | Loss: 0.00004629
Iteration 176/1000 | Loss: 0.00005549
Iteration 177/1000 | Loss: 0.00005037
Iteration 178/1000 | Loss: 0.00004560
Iteration 179/1000 | Loss: 0.00004683
Iteration 180/1000 | Loss: 0.00004378
Iteration 181/1000 | Loss: 0.00003821
Iteration 182/1000 | Loss: 0.00005295
Iteration 183/1000 | Loss: 0.00004653
Iteration 184/1000 | Loss: 0.00004575
Iteration 185/1000 | Loss: 0.00004038
Iteration 186/1000 | Loss: 0.00006281
Iteration 187/1000 | Loss: 0.00004882
Iteration 188/1000 | Loss: 0.00004517
Iteration 189/1000 | Loss: 0.00004899
Iteration 190/1000 | Loss: 0.00005432
Iteration 191/1000 | Loss: 0.00006263
Iteration 192/1000 | Loss: 0.00004914
Iteration 193/1000 | Loss: 0.00022194
Iteration 194/1000 | Loss: 0.00007474
Iteration 195/1000 | Loss: 0.00005405
Iteration 196/1000 | Loss: 0.00005068
Iteration 197/1000 | Loss: 0.00004935
Iteration 198/1000 | Loss: 0.00004458
Iteration 199/1000 | Loss: 0.00003052
Iteration 200/1000 | Loss: 0.00004394
Iteration 201/1000 | Loss: 0.00005761
Iteration 202/1000 | Loss: 0.00005385
Iteration 203/1000 | Loss: 0.00006272
Iteration 204/1000 | Loss: 0.00005838
Iteration 205/1000 | Loss: 0.00005354
Iteration 206/1000 | Loss: 0.00005940
Iteration 207/1000 | Loss: 0.00005759
Iteration 208/1000 | Loss: 0.00004673
Iteration 209/1000 | Loss: 0.00005249
Iteration 210/1000 | Loss: 0.00004963
Iteration 211/1000 | Loss: 0.00005050
Iteration 212/1000 | Loss: 0.00005617
Iteration 213/1000 | Loss: 0.00005822
Iteration 214/1000 | Loss: 0.00007157
Iteration 215/1000 | Loss: 0.00003593
Iteration 216/1000 | Loss: 0.00006184
Iteration 217/1000 | Loss: 0.00002345
Iteration 218/1000 | Loss: 0.00001798
Iteration 219/1000 | Loss: 0.00001646
Iteration 220/1000 | Loss: 0.00001563
Iteration 221/1000 | Loss: 0.00001536
Iteration 222/1000 | Loss: 0.00001526
Iteration 223/1000 | Loss: 0.00001508
Iteration 224/1000 | Loss: 0.00001507
Iteration 225/1000 | Loss: 0.00001492
Iteration 226/1000 | Loss: 0.00001490
Iteration 227/1000 | Loss: 0.00001488
Iteration 228/1000 | Loss: 0.00001484
Iteration 229/1000 | Loss: 0.00001484
Iteration 230/1000 | Loss: 0.00001483
Iteration 231/1000 | Loss: 0.00001483
Iteration 232/1000 | Loss: 0.00001482
Iteration 233/1000 | Loss: 0.00001482
Iteration 234/1000 | Loss: 0.00001481
Iteration 235/1000 | Loss: 0.00001481
Iteration 236/1000 | Loss: 0.00001480
Iteration 237/1000 | Loss: 0.00001480
Iteration 238/1000 | Loss: 0.00001480
Iteration 239/1000 | Loss: 0.00001480
Iteration 240/1000 | Loss: 0.00001480
Iteration 241/1000 | Loss: 0.00001480
Iteration 242/1000 | Loss: 0.00001479
Iteration 243/1000 | Loss: 0.00001479
Iteration 244/1000 | Loss: 0.00001479
Iteration 245/1000 | Loss: 0.00001479
Iteration 246/1000 | Loss: 0.00001479
Iteration 247/1000 | Loss: 0.00001479
Iteration 248/1000 | Loss: 0.00001479
Iteration 249/1000 | Loss: 0.00001479
Iteration 250/1000 | Loss: 0.00001478
Iteration 251/1000 | Loss: 0.00001478
Iteration 252/1000 | Loss: 0.00001477
Iteration 253/1000 | Loss: 0.00001477
Iteration 254/1000 | Loss: 0.00001477
Iteration 255/1000 | Loss: 0.00001476
Iteration 256/1000 | Loss: 0.00001476
Iteration 257/1000 | Loss: 0.00001476
Iteration 258/1000 | Loss: 0.00001475
Iteration 259/1000 | Loss: 0.00001475
Iteration 260/1000 | Loss: 0.00001475
Iteration 261/1000 | Loss: 0.00001475
Iteration 262/1000 | Loss: 0.00001475
Iteration 263/1000 | Loss: 0.00001475
Iteration 264/1000 | Loss: 0.00001474
Iteration 265/1000 | Loss: 0.00001474
Iteration 266/1000 | Loss: 0.00001474
Iteration 267/1000 | Loss: 0.00001474
Iteration 268/1000 | Loss: 0.00001474
Iteration 269/1000 | Loss: 0.00001474
Iteration 270/1000 | Loss: 0.00001474
Iteration 271/1000 | Loss: 0.00001474
Iteration 272/1000 | Loss: 0.00001473
Iteration 273/1000 | Loss: 0.00001473
Iteration 274/1000 | Loss: 0.00001473
Iteration 275/1000 | Loss: 0.00001472
Iteration 276/1000 | Loss: 0.00001472
Iteration 277/1000 | Loss: 0.00001472
Iteration 278/1000 | Loss: 0.00001472
Iteration 279/1000 | Loss: 0.00001472
Iteration 280/1000 | Loss: 0.00001472
Iteration 281/1000 | Loss: 0.00001472
Iteration 282/1000 | Loss: 0.00001472
Iteration 283/1000 | Loss: 0.00001472
Iteration 284/1000 | Loss: 0.00001472
Iteration 285/1000 | Loss: 0.00001472
Iteration 286/1000 | Loss: 0.00001471
Iteration 287/1000 | Loss: 0.00001471
Iteration 288/1000 | Loss: 0.00001471
Iteration 289/1000 | Loss: 0.00001471
Iteration 290/1000 | Loss: 0.00001471
Iteration 291/1000 | Loss: 0.00001471
Iteration 292/1000 | Loss: 0.00001471
Iteration 293/1000 | Loss: 0.00001471
Iteration 294/1000 | Loss: 0.00001471
Iteration 295/1000 | Loss: 0.00001470
Iteration 296/1000 | Loss: 0.00001470
Iteration 297/1000 | Loss: 0.00001470
Iteration 298/1000 | Loss: 0.00001470
Iteration 299/1000 | Loss: 0.00001470
Iteration 300/1000 | Loss: 0.00001470
Iteration 301/1000 | Loss: 0.00001470
Iteration 302/1000 | Loss: 0.00001470
Iteration 303/1000 | Loss: 0.00001469
Iteration 304/1000 | Loss: 0.00001469
Iteration 305/1000 | Loss: 0.00001469
Iteration 306/1000 | Loss: 0.00001469
Iteration 307/1000 | Loss: 0.00001469
Iteration 308/1000 | Loss: 0.00001469
Iteration 309/1000 | Loss: 0.00001469
Iteration 310/1000 | Loss: 0.00001469
Iteration 311/1000 | Loss: 0.00001469
Iteration 312/1000 | Loss: 0.00001469
Iteration 313/1000 | Loss: 0.00001469
Iteration 314/1000 | Loss: 0.00001469
Iteration 315/1000 | Loss: 0.00001469
Iteration 316/1000 | Loss: 0.00001469
Iteration 317/1000 | Loss: 0.00001468
Iteration 318/1000 | Loss: 0.00001468
Iteration 319/1000 | Loss: 0.00001468
Iteration 320/1000 | Loss: 0.00001468
Iteration 321/1000 | Loss: 0.00001468
Iteration 322/1000 | Loss: 0.00001468
Iteration 323/1000 | Loss: 0.00001468
Iteration 324/1000 | Loss: 0.00001468
Iteration 325/1000 | Loss: 0.00001468
Iteration 326/1000 | Loss: 0.00001467
Iteration 327/1000 | Loss: 0.00001467
Iteration 328/1000 | Loss: 0.00001467
Iteration 329/1000 | Loss: 0.00001467
Iteration 330/1000 | Loss: 0.00001467
Iteration 331/1000 | Loss: 0.00001467
Iteration 332/1000 | Loss: 0.00001467
Iteration 333/1000 | Loss: 0.00001467
Iteration 334/1000 | Loss: 0.00001467
Iteration 335/1000 | Loss: 0.00001467
Iteration 336/1000 | Loss: 0.00001466
Iteration 337/1000 | Loss: 0.00001466
Iteration 338/1000 | Loss: 0.00001466
Iteration 339/1000 | Loss: 0.00001466
Iteration 340/1000 | Loss: 0.00001466
Iteration 341/1000 | Loss: 0.00001466
Iteration 342/1000 | Loss: 0.00001466
Iteration 343/1000 | Loss: 0.00001466
Iteration 344/1000 | Loss: 0.00001465
Iteration 345/1000 | Loss: 0.00001465
Iteration 346/1000 | Loss: 0.00001465
Iteration 347/1000 | Loss: 0.00001465
Iteration 348/1000 | Loss: 0.00001465
Iteration 349/1000 | Loss: 0.00001465
Iteration 350/1000 | Loss: 0.00001465
Iteration 351/1000 | Loss: 0.00001465
Iteration 352/1000 | Loss: 0.00001465
Iteration 353/1000 | Loss: 0.00001465
Iteration 354/1000 | Loss: 0.00001465
Iteration 355/1000 | Loss: 0.00001464
Iteration 356/1000 | Loss: 0.00001464
Iteration 357/1000 | Loss: 0.00001464
Iteration 358/1000 | Loss: 0.00001464
Iteration 359/1000 | Loss: 0.00001464
Iteration 360/1000 | Loss: 0.00001464
Iteration 361/1000 | Loss: 0.00001464
Iteration 362/1000 | Loss: 0.00001464
Iteration 363/1000 | Loss: 0.00001464
Iteration 364/1000 | Loss: 0.00001464
Iteration 365/1000 | Loss: 0.00001464
Iteration 366/1000 | Loss: 0.00001464
Iteration 367/1000 | Loss: 0.00001464
Iteration 368/1000 | Loss: 0.00001464
Iteration 369/1000 | Loss: 0.00001464
Iteration 370/1000 | Loss: 0.00001463
Iteration 371/1000 | Loss: 0.00001463
Iteration 372/1000 | Loss: 0.00001463
Iteration 373/1000 | Loss: 0.00001463
Iteration 374/1000 | Loss: 0.00001463
Iteration 375/1000 | Loss: 0.00001463
Iteration 376/1000 | Loss: 0.00001463
Iteration 377/1000 | Loss: 0.00001463
Iteration 378/1000 | Loss: 0.00001463
Iteration 379/1000 | Loss: 0.00001463
Iteration 380/1000 | Loss: 0.00001463
Iteration 381/1000 | Loss: 0.00001463
Iteration 382/1000 | Loss: 0.00001463
Iteration 383/1000 | Loss: 0.00001463
Iteration 384/1000 | Loss: 0.00001463
Iteration 385/1000 | Loss: 0.00001463
Iteration 386/1000 | Loss: 0.00001463
Iteration 387/1000 | Loss: 0.00001463
Iteration 388/1000 | Loss: 0.00001463
Iteration 389/1000 | Loss: 0.00001463
Iteration 390/1000 | Loss: 0.00001463
Iteration 391/1000 | Loss: 0.00001463
Iteration 392/1000 | Loss: 0.00001463
Iteration 393/1000 | Loss: 0.00001463
Iteration 394/1000 | Loss: 0.00001463
Iteration 395/1000 | Loss: 0.00001463
Iteration 396/1000 | Loss: 0.00001463
Iteration 397/1000 | Loss: 0.00001463
Iteration 398/1000 | Loss: 0.00001463
Iteration 399/1000 | Loss: 0.00001463
Iteration 400/1000 | Loss: 0.00001463
Iteration 401/1000 | Loss: 0.00001463
Iteration 402/1000 | Loss: 0.00001463
Iteration 403/1000 | Loss: 0.00001463
Iteration 404/1000 | Loss: 0.00001463
Iteration 405/1000 | Loss: 0.00001463
Iteration 406/1000 | Loss: 0.00001463
Iteration 407/1000 | Loss: 0.00001463
Iteration 408/1000 | Loss: 0.00001463
Iteration 409/1000 | Loss: 0.00001463
Iteration 410/1000 | Loss: 0.00001463
Iteration 411/1000 | Loss: 0.00001463
Iteration 412/1000 | Loss: 0.00001463
Iteration 413/1000 | Loss: 0.00001463
Iteration 414/1000 | Loss: 0.00001463
Iteration 415/1000 | Loss: 0.00001463
Iteration 416/1000 | Loss: 0.00001463
Iteration 417/1000 | Loss: 0.00001463
Iteration 418/1000 | Loss: 0.00001463
Iteration 419/1000 | Loss: 0.00001463
Iteration 420/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 420. Stopping optimization.
Last 5 losses: [1.4626110896642786e-05, 1.4626110896642786e-05, 1.4626110896642786e-05, 1.4626110896642786e-05, 1.4626110896642786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4626110896642786e-05

Optimization complete. Final v2v error: 3.249035358428955 mm

Highest mean error: 4.2054243087768555 mm for frame 54

Lowest mean error: 2.8058879375457764 mm for frame 30

Saving results

Total time: 369.32718420028687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860243
Iteration 2/25 | Loss: 0.00126824
Iteration 3/25 | Loss: 0.00088317
Iteration 4/25 | Loss: 0.00083487
Iteration 5/25 | Loss: 0.00082748
Iteration 6/25 | Loss: 0.00082570
Iteration 7/25 | Loss: 0.00082564
Iteration 8/25 | Loss: 0.00082564
Iteration 9/25 | Loss: 0.00082564
Iteration 10/25 | Loss: 0.00082564
Iteration 11/25 | Loss: 0.00082564
Iteration 12/25 | Loss: 0.00082564
Iteration 13/25 | Loss: 0.00082564
Iteration 14/25 | Loss: 0.00082564
Iteration 15/25 | Loss: 0.00082564
Iteration 16/25 | Loss: 0.00082564
Iteration 17/25 | Loss: 0.00082564
Iteration 18/25 | Loss: 0.00082564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008256438304670155, 0.0008256438304670155, 0.0008256438304670155, 0.0008256438304670155, 0.0008256438304670155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008256438304670155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46697295
Iteration 2/25 | Loss: 0.00040631
Iteration 3/25 | Loss: 0.00040631
Iteration 4/25 | Loss: 0.00040631
Iteration 5/25 | Loss: 0.00040631
Iteration 6/25 | Loss: 0.00040631
Iteration 7/25 | Loss: 0.00040631
Iteration 8/25 | Loss: 0.00040631
Iteration 9/25 | Loss: 0.00040631
Iteration 10/25 | Loss: 0.00040631
Iteration 11/25 | Loss: 0.00040631
Iteration 12/25 | Loss: 0.00040631
Iteration 13/25 | Loss: 0.00040631
Iteration 14/25 | Loss: 0.00040631
Iteration 15/25 | Loss: 0.00040631
Iteration 16/25 | Loss: 0.00040631
Iteration 17/25 | Loss: 0.00040631
Iteration 18/25 | Loss: 0.00040631
Iteration 19/25 | Loss: 0.00040631
Iteration 20/25 | Loss: 0.00040631
Iteration 21/25 | Loss: 0.00040631
Iteration 22/25 | Loss: 0.00040631
Iteration 23/25 | Loss: 0.00040631
Iteration 24/25 | Loss: 0.00040631
Iteration 25/25 | Loss: 0.00040631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040631
Iteration 2/1000 | Loss: 0.00005439
Iteration 3/1000 | Loss: 0.00004452
Iteration 4/1000 | Loss: 0.00004232
Iteration 5/1000 | Loss: 0.00004051
Iteration 6/1000 | Loss: 0.00003911
Iteration 7/1000 | Loss: 0.00003817
Iteration 8/1000 | Loss: 0.00003758
Iteration 9/1000 | Loss: 0.00003724
Iteration 10/1000 | Loss: 0.00003704
Iteration 11/1000 | Loss: 0.00003689
Iteration 12/1000 | Loss: 0.00003687
Iteration 13/1000 | Loss: 0.00003686
Iteration 14/1000 | Loss: 0.00003686
Iteration 15/1000 | Loss: 0.00003685
Iteration 16/1000 | Loss: 0.00003683
Iteration 17/1000 | Loss: 0.00003681
Iteration 18/1000 | Loss: 0.00003681
Iteration 19/1000 | Loss: 0.00003681
Iteration 20/1000 | Loss: 0.00003680
Iteration 21/1000 | Loss: 0.00003680
Iteration 22/1000 | Loss: 0.00003679
Iteration 23/1000 | Loss: 0.00003679
Iteration 24/1000 | Loss: 0.00003679
Iteration 25/1000 | Loss: 0.00003679
Iteration 26/1000 | Loss: 0.00003679
Iteration 27/1000 | Loss: 0.00003679
Iteration 28/1000 | Loss: 0.00003679
Iteration 29/1000 | Loss: 0.00003679
Iteration 30/1000 | Loss: 0.00003679
Iteration 31/1000 | Loss: 0.00003679
Iteration 32/1000 | Loss: 0.00003678
Iteration 33/1000 | Loss: 0.00003678
Iteration 34/1000 | Loss: 0.00003678
Iteration 35/1000 | Loss: 0.00003678
Iteration 36/1000 | Loss: 0.00003678
Iteration 37/1000 | Loss: 0.00003678
Iteration 38/1000 | Loss: 0.00003678
Iteration 39/1000 | Loss: 0.00003678
Iteration 40/1000 | Loss: 0.00003678
Iteration 41/1000 | Loss: 0.00003678
Iteration 42/1000 | Loss: 0.00003678
Iteration 43/1000 | Loss: 0.00003678
Iteration 44/1000 | Loss: 0.00003678
Iteration 45/1000 | Loss: 0.00003678
Iteration 46/1000 | Loss: 0.00003678
Iteration 47/1000 | Loss: 0.00003678
Iteration 48/1000 | Loss: 0.00003678
Iteration 49/1000 | Loss: 0.00003678
Iteration 50/1000 | Loss: 0.00003678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 50. Stopping optimization.
Last 5 losses: [3.67759566870518e-05, 3.67759566870518e-05, 3.67759566870518e-05, 3.67759566870518e-05, 3.67759566870518e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.67759566870518e-05

Optimization complete. Final v2v error: 4.969834804534912 mm

Highest mean error: 5.319087028503418 mm for frame 18

Lowest mean error: 4.511356830596924 mm for frame 30

Saving results

Total time: 26.7207989692688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040519
Iteration 2/25 | Loss: 0.00277231
Iteration 3/25 | Loss: 0.00156088
Iteration 4/25 | Loss: 0.00143930
Iteration 5/25 | Loss: 0.00134275
Iteration 6/25 | Loss: 0.00125378
Iteration 7/25 | Loss: 0.00131928
Iteration 8/25 | Loss: 0.00110109
Iteration 9/25 | Loss: 0.00105660
Iteration 10/25 | Loss: 0.00097012
Iteration 11/25 | Loss: 0.00095186
Iteration 12/25 | Loss: 0.00094156
Iteration 13/25 | Loss: 0.00091742
Iteration 14/25 | Loss: 0.00090459
Iteration 15/25 | Loss: 0.00090252
Iteration 16/25 | Loss: 0.00089399
Iteration 17/25 | Loss: 0.00088129
Iteration 18/25 | Loss: 0.00087092
Iteration 19/25 | Loss: 0.00087391
Iteration 20/25 | Loss: 0.00088420
Iteration 21/25 | Loss: 0.00088149
Iteration 22/25 | Loss: 0.00087215
Iteration 23/25 | Loss: 0.00085972
Iteration 24/25 | Loss: 0.00086001
Iteration 25/25 | Loss: 0.00085817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46803498
Iteration 2/25 | Loss: 0.00161514
Iteration 3/25 | Loss: 0.00161513
Iteration 4/25 | Loss: 0.00161513
Iteration 5/25 | Loss: 0.00161513
Iteration 6/25 | Loss: 0.00161513
Iteration 7/25 | Loss: 0.00161513
Iteration 8/25 | Loss: 0.00161513
Iteration 9/25 | Loss: 0.00161513
Iteration 10/25 | Loss: 0.00161513
Iteration 11/25 | Loss: 0.00161513
Iteration 12/25 | Loss: 0.00161513
Iteration 13/25 | Loss: 0.00161513
Iteration 14/25 | Loss: 0.00161513
Iteration 15/25 | Loss: 0.00161513
Iteration 16/25 | Loss: 0.00161513
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016151313902810216, 0.0016151313902810216, 0.0016151313902810216, 0.0016151313902810216, 0.0016151313902810216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016151313902810216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161513
Iteration 2/1000 | Loss: 0.00021227
Iteration 3/1000 | Loss: 0.00086691
Iteration 4/1000 | Loss: 0.00076484
Iteration 5/1000 | Loss: 0.00049274
Iteration 6/1000 | Loss: 0.00024628
Iteration 7/1000 | Loss: 0.00022146
Iteration 8/1000 | Loss: 0.00017228
Iteration 9/1000 | Loss: 0.00012311
Iteration 10/1000 | Loss: 0.00014246
Iteration 11/1000 | Loss: 0.00014928
Iteration 12/1000 | Loss: 0.00015343
Iteration 13/1000 | Loss: 0.00088630
Iteration 14/1000 | Loss: 0.00058890
Iteration 15/1000 | Loss: 0.00047087
Iteration 16/1000 | Loss: 0.00031391
Iteration 17/1000 | Loss: 0.00057431
Iteration 18/1000 | Loss: 0.00037296
Iteration 19/1000 | Loss: 0.00011385
Iteration 20/1000 | Loss: 0.00020711
Iteration 21/1000 | Loss: 0.00015632
Iteration 22/1000 | Loss: 0.00009889
Iteration 23/1000 | Loss: 0.00014492
Iteration 24/1000 | Loss: 0.00092215
Iteration 25/1000 | Loss: 0.00022536
Iteration 26/1000 | Loss: 0.00021284
Iteration 27/1000 | Loss: 0.00068546
Iteration 28/1000 | Loss: 0.00043502
Iteration 29/1000 | Loss: 0.00009002
Iteration 30/1000 | Loss: 0.00017477
Iteration 31/1000 | Loss: 0.00011128
Iteration 32/1000 | Loss: 0.00009801
Iteration 33/1000 | Loss: 0.00010379
Iteration 34/1000 | Loss: 0.00011172
Iteration 35/1000 | Loss: 0.00009645
Iteration 36/1000 | Loss: 0.00008139
Iteration 37/1000 | Loss: 0.00007613
Iteration 38/1000 | Loss: 0.00008228
Iteration 39/1000 | Loss: 0.00007455
Iteration 40/1000 | Loss: 0.00019833
Iteration 41/1000 | Loss: 0.00019505
Iteration 42/1000 | Loss: 0.00008278
Iteration 43/1000 | Loss: 0.00007684
Iteration 44/1000 | Loss: 0.00007266
Iteration 45/1000 | Loss: 0.00036787
Iteration 46/1000 | Loss: 0.00015850
Iteration 47/1000 | Loss: 0.00085035
Iteration 48/1000 | Loss: 0.00024826
Iteration 49/1000 | Loss: 0.00015200
Iteration 50/1000 | Loss: 0.00011873
Iteration 51/1000 | Loss: 0.00018266
Iteration 52/1000 | Loss: 0.00007286
Iteration 53/1000 | Loss: 0.00007004
Iteration 54/1000 | Loss: 0.00031930
Iteration 55/1000 | Loss: 0.00023864
Iteration 56/1000 | Loss: 0.00007504
Iteration 57/1000 | Loss: 0.00007047
Iteration 58/1000 | Loss: 0.00006878
Iteration 59/1000 | Loss: 0.00006746
Iteration 60/1000 | Loss: 0.00006689
Iteration 61/1000 | Loss: 0.00006662
Iteration 62/1000 | Loss: 0.00026138
Iteration 63/1000 | Loss: 0.00016347
Iteration 64/1000 | Loss: 0.00007656
Iteration 65/1000 | Loss: 0.00006737
Iteration 66/1000 | Loss: 0.00019362
Iteration 67/1000 | Loss: 0.00006479
Iteration 68/1000 | Loss: 0.00006410
Iteration 69/1000 | Loss: 0.00009517
Iteration 70/1000 | Loss: 0.00006763
Iteration 71/1000 | Loss: 0.00006355
Iteration 72/1000 | Loss: 0.00008456
Iteration 73/1000 | Loss: 0.00006480
Iteration 74/1000 | Loss: 0.00006339
Iteration 75/1000 | Loss: 0.00006339
Iteration 76/1000 | Loss: 0.00006338
Iteration 77/1000 | Loss: 0.00006338
Iteration 78/1000 | Loss: 0.00006338
Iteration 79/1000 | Loss: 0.00010412
Iteration 80/1000 | Loss: 0.00006809
Iteration 81/1000 | Loss: 0.00010744
Iteration 82/1000 | Loss: 0.00006325
Iteration 83/1000 | Loss: 0.00006309
Iteration 84/1000 | Loss: 0.00006302
Iteration 85/1000 | Loss: 0.00008124
Iteration 86/1000 | Loss: 0.00006297
Iteration 87/1000 | Loss: 0.00010399
Iteration 88/1000 | Loss: 0.00006329
Iteration 89/1000 | Loss: 0.00006288
Iteration 90/1000 | Loss: 0.00006286
Iteration 91/1000 | Loss: 0.00006286
Iteration 92/1000 | Loss: 0.00006285
Iteration 93/1000 | Loss: 0.00006285
Iteration 94/1000 | Loss: 0.00006285
Iteration 95/1000 | Loss: 0.00006285
Iteration 96/1000 | Loss: 0.00006285
Iteration 97/1000 | Loss: 0.00006285
Iteration 98/1000 | Loss: 0.00006285
Iteration 99/1000 | Loss: 0.00006285
Iteration 100/1000 | Loss: 0.00006285
Iteration 101/1000 | Loss: 0.00006285
Iteration 102/1000 | Loss: 0.00006285
Iteration 103/1000 | Loss: 0.00006285
Iteration 104/1000 | Loss: 0.00006285
Iteration 105/1000 | Loss: 0.00006285
Iteration 106/1000 | Loss: 0.00006285
Iteration 107/1000 | Loss: 0.00006285
Iteration 108/1000 | Loss: 0.00006285
Iteration 109/1000 | Loss: 0.00006285
Iteration 110/1000 | Loss: 0.00006285
Iteration 111/1000 | Loss: 0.00006285
Iteration 112/1000 | Loss: 0.00006285
Iteration 113/1000 | Loss: 0.00006285
Iteration 114/1000 | Loss: 0.00006285
Iteration 115/1000 | Loss: 0.00006285
Iteration 116/1000 | Loss: 0.00006285
Iteration 117/1000 | Loss: 0.00006285
Iteration 118/1000 | Loss: 0.00006285
Iteration 119/1000 | Loss: 0.00006285
Iteration 120/1000 | Loss: 0.00006285
Iteration 121/1000 | Loss: 0.00006285
Iteration 122/1000 | Loss: 0.00006285
Iteration 123/1000 | Loss: 0.00006285
Iteration 124/1000 | Loss: 0.00006285
Iteration 125/1000 | Loss: 0.00006285
Iteration 126/1000 | Loss: 0.00006285
Iteration 127/1000 | Loss: 0.00006285
Iteration 128/1000 | Loss: 0.00006285
Iteration 129/1000 | Loss: 0.00006285
Iteration 130/1000 | Loss: 0.00006285
Iteration 131/1000 | Loss: 0.00006285
Iteration 132/1000 | Loss: 0.00006285
Iteration 133/1000 | Loss: 0.00006285
Iteration 134/1000 | Loss: 0.00006285
Iteration 135/1000 | Loss: 0.00006285
Iteration 136/1000 | Loss: 0.00006285
Iteration 137/1000 | Loss: 0.00006285
Iteration 138/1000 | Loss: 0.00006285
Iteration 139/1000 | Loss: 0.00006285
Iteration 140/1000 | Loss: 0.00006285
Iteration 141/1000 | Loss: 0.00006285
Iteration 142/1000 | Loss: 0.00006285
Iteration 143/1000 | Loss: 0.00006285
Iteration 144/1000 | Loss: 0.00006285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [6.284998380579054e-05, 6.284998380579054e-05, 6.284998380579054e-05, 6.284998380579054e-05, 6.284998380579054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.284998380579054e-05

Optimization complete. Final v2v error: 4.443202018737793 mm

Highest mean error: 11.486180305480957 mm for frame 164

Lowest mean error: 3.669095277786255 mm for frame 37

Saving results

Total time: 186.86960434913635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046241
Iteration 2/25 | Loss: 0.00404540
Iteration 3/25 | Loss: 0.00192442
Iteration 4/25 | Loss: 0.00162520
Iteration 5/25 | Loss: 0.00154485
Iteration 6/25 | Loss: 0.00135824
Iteration 7/25 | Loss: 0.00135957
Iteration 8/25 | Loss: 0.00130776
Iteration 9/25 | Loss: 0.00124448
Iteration 10/25 | Loss: 0.00120010
Iteration 11/25 | Loss: 0.00115897
Iteration 12/25 | Loss: 0.00113836
Iteration 13/25 | Loss: 0.00113013
Iteration 14/25 | Loss: 0.00109686
Iteration 15/25 | Loss: 0.00109009
Iteration 16/25 | Loss: 0.00108609
Iteration 17/25 | Loss: 0.00108020
Iteration 18/25 | Loss: 0.00108417
Iteration 19/25 | Loss: 0.00107664
Iteration 20/25 | Loss: 0.00106253
Iteration 21/25 | Loss: 0.00105782
Iteration 22/25 | Loss: 0.00105612
Iteration 23/25 | Loss: 0.00105255
Iteration 24/25 | Loss: 0.00105019
Iteration 25/25 | Loss: 0.00105191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44884217
Iteration 2/25 | Loss: 0.00763788
Iteration 3/25 | Loss: 0.00532542
Iteration 4/25 | Loss: 0.00464645
Iteration 5/25 | Loss: 0.00459179
Iteration 6/25 | Loss: 0.00412122
Iteration 7/25 | Loss: 0.00412121
Iteration 8/25 | Loss: 0.00412121
Iteration 9/25 | Loss: 0.00412121
Iteration 10/25 | Loss: 0.00412121
Iteration 11/25 | Loss: 0.00412121
Iteration 12/25 | Loss: 0.00412121
Iteration 13/25 | Loss: 0.00412121
Iteration 14/25 | Loss: 0.00412121
Iteration 15/25 | Loss: 0.00412121
Iteration 16/25 | Loss: 0.00412121
Iteration 17/25 | Loss: 0.00412121
Iteration 18/25 | Loss: 0.00412121
Iteration 19/25 | Loss: 0.00412121
Iteration 20/25 | Loss: 0.00412121
Iteration 21/25 | Loss: 0.00412121
Iteration 22/25 | Loss: 0.00412121
Iteration 23/25 | Loss: 0.00412121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.004121210426092148, 0.004121210426092148, 0.004121210426092148, 0.004121210426092148, 0.004121210426092148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004121210426092148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00412121
Iteration 2/1000 | Loss: 0.00341124
Iteration 3/1000 | Loss: 0.00401621
Iteration 4/1000 | Loss: 0.00181424
Iteration 5/1000 | Loss: 0.00188052
Iteration 6/1000 | Loss: 0.00434172
Iteration 7/1000 | Loss: 0.00819589
Iteration 8/1000 | Loss: 0.00537044
Iteration 9/1000 | Loss: 0.00451221
Iteration 10/1000 | Loss: 0.00204722
Iteration 11/1000 | Loss: 0.00195392
Iteration 12/1000 | Loss: 0.00097849
Iteration 13/1000 | Loss: 0.00115246
Iteration 14/1000 | Loss: 0.00108900
Iteration 15/1000 | Loss: 0.00287962
Iteration 16/1000 | Loss: 0.00078492
Iteration 17/1000 | Loss: 0.00186039
Iteration 18/1000 | Loss: 0.00058011
Iteration 19/1000 | Loss: 0.00095817
Iteration 20/1000 | Loss: 0.00135894
Iteration 21/1000 | Loss: 0.00243759
Iteration 22/1000 | Loss: 0.00162007
Iteration 23/1000 | Loss: 0.00167410
Iteration 24/1000 | Loss: 0.00199466
Iteration 25/1000 | Loss: 0.00118713
Iteration 26/1000 | Loss: 0.00185169
Iteration 27/1000 | Loss: 0.00471134
Iteration 28/1000 | Loss: 0.00070469
Iteration 29/1000 | Loss: 0.00069692
Iteration 30/1000 | Loss: 0.00227010
Iteration 31/1000 | Loss: 0.00078031
Iteration 32/1000 | Loss: 0.00105097
Iteration 33/1000 | Loss: 0.00305835
Iteration 34/1000 | Loss: 0.00160956
Iteration 35/1000 | Loss: 0.00067494
Iteration 36/1000 | Loss: 0.00050575
Iteration 37/1000 | Loss: 0.00058302
Iteration 38/1000 | Loss: 0.00070497
Iteration 39/1000 | Loss: 0.00192027
Iteration 40/1000 | Loss: 0.00101052
Iteration 41/1000 | Loss: 0.00038624
Iteration 42/1000 | Loss: 0.00149792
Iteration 43/1000 | Loss: 0.00085569
Iteration 44/1000 | Loss: 0.00073271
Iteration 45/1000 | Loss: 0.00206079
Iteration 46/1000 | Loss: 0.00399274
Iteration 47/1000 | Loss: 0.00420103
Iteration 48/1000 | Loss: 0.00188940
Iteration 49/1000 | Loss: 0.00212983
Iteration 50/1000 | Loss: 0.00023408
Iteration 51/1000 | Loss: 0.00051797
Iteration 52/1000 | Loss: 0.00139566
Iteration 53/1000 | Loss: 0.00055073
Iteration 54/1000 | Loss: 0.00343277
Iteration 55/1000 | Loss: 0.00160979
Iteration 56/1000 | Loss: 0.00033440
Iteration 57/1000 | Loss: 0.00024739
Iteration 58/1000 | Loss: 0.00013223
Iteration 59/1000 | Loss: 0.00018224
Iteration 60/1000 | Loss: 0.00157665
Iteration 61/1000 | Loss: 0.00073419
Iteration 62/1000 | Loss: 0.00043631
Iteration 63/1000 | Loss: 0.00053222
Iteration 64/1000 | Loss: 0.00054374
Iteration 65/1000 | Loss: 0.00065148
Iteration 66/1000 | Loss: 0.00027735
Iteration 67/1000 | Loss: 0.00088413
Iteration 68/1000 | Loss: 0.00029732
Iteration 69/1000 | Loss: 0.00121011
Iteration 70/1000 | Loss: 0.00030821
Iteration 71/1000 | Loss: 0.00047494
Iteration 72/1000 | Loss: 0.00032193
Iteration 73/1000 | Loss: 0.00032840
Iteration 74/1000 | Loss: 0.00060496
Iteration 75/1000 | Loss: 0.00026381
Iteration 76/1000 | Loss: 0.00028592
Iteration 77/1000 | Loss: 0.00014116
Iteration 78/1000 | Loss: 0.00054699
Iteration 79/1000 | Loss: 0.00033209
Iteration 80/1000 | Loss: 0.00038543
Iteration 81/1000 | Loss: 0.00038538
Iteration 82/1000 | Loss: 0.00101750
Iteration 83/1000 | Loss: 0.00034448
Iteration 84/1000 | Loss: 0.00036678
Iteration 85/1000 | Loss: 0.00028284
Iteration 86/1000 | Loss: 0.00033668
Iteration 87/1000 | Loss: 0.00016727
Iteration 88/1000 | Loss: 0.00017752
Iteration 89/1000 | Loss: 0.00016354
Iteration 90/1000 | Loss: 0.00078740
Iteration 91/1000 | Loss: 0.00038978
Iteration 92/1000 | Loss: 0.00031908
Iteration 93/1000 | Loss: 0.00023256
Iteration 94/1000 | Loss: 0.00026437
Iteration 95/1000 | Loss: 0.00026063
Iteration 96/1000 | Loss: 0.00018186
Iteration 97/1000 | Loss: 0.00026206
Iteration 98/1000 | Loss: 0.00020886
Iteration 99/1000 | Loss: 0.00114473
Iteration 100/1000 | Loss: 0.00063308
Iteration 101/1000 | Loss: 0.00019747
Iteration 102/1000 | Loss: 0.00026056
Iteration 103/1000 | Loss: 0.00027898
Iteration 104/1000 | Loss: 0.00028504
Iteration 105/1000 | Loss: 0.00039640
Iteration 106/1000 | Loss: 0.00042035
Iteration 107/1000 | Loss: 0.00054073
Iteration 108/1000 | Loss: 0.00044568
Iteration 109/1000 | Loss: 0.00033483
Iteration 110/1000 | Loss: 0.00026563
Iteration 111/1000 | Loss: 0.00023834
Iteration 112/1000 | Loss: 0.00027429
Iteration 113/1000 | Loss: 0.00022379
Iteration 114/1000 | Loss: 0.00029648
Iteration 115/1000 | Loss: 0.00032295
Iteration 116/1000 | Loss: 0.00019016
Iteration 117/1000 | Loss: 0.00047899
Iteration 118/1000 | Loss: 0.00024619
Iteration 119/1000 | Loss: 0.00016698
Iteration 120/1000 | Loss: 0.00012234
Iteration 121/1000 | Loss: 0.00050773
Iteration 122/1000 | Loss: 0.00069230
Iteration 123/1000 | Loss: 0.00033829
Iteration 124/1000 | Loss: 0.00041529
Iteration 125/1000 | Loss: 0.00008152
Iteration 126/1000 | Loss: 0.00008415
Iteration 127/1000 | Loss: 0.00009167
Iteration 128/1000 | Loss: 0.00032990
Iteration 129/1000 | Loss: 0.00044660
Iteration 130/1000 | Loss: 0.00055594
Iteration 131/1000 | Loss: 0.00019517
Iteration 132/1000 | Loss: 0.00024465
Iteration 133/1000 | Loss: 0.00036757
Iteration 134/1000 | Loss: 0.00139660
Iteration 135/1000 | Loss: 0.00062412
Iteration 136/1000 | Loss: 0.00047804
Iteration 137/1000 | Loss: 0.00018644
Iteration 138/1000 | Loss: 0.00021338
Iteration 139/1000 | Loss: 0.00021598
Iteration 140/1000 | Loss: 0.00010336
Iteration 141/1000 | Loss: 0.00013642
Iteration 142/1000 | Loss: 0.00013990
Iteration 143/1000 | Loss: 0.00008465
Iteration 144/1000 | Loss: 0.00029712
Iteration 145/1000 | Loss: 0.00015458
Iteration 146/1000 | Loss: 0.00007263
Iteration 147/1000 | Loss: 0.00006263
Iteration 148/1000 | Loss: 0.00024136
Iteration 149/1000 | Loss: 0.00029314
Iteration 150/1000 | Loss: 0.00030124
Iteration 151/1000 | Loss: 0.00048157
Iteration 152/1000 | Loss: 0.00018436
Iteration 153/1000 | Loss: 0.00019652
Iteration 154/1000 | Loss: 0.00021102
Iteration 155/1000 | Loss: 0.00044804
Iteration 156/1000 | Loss: 0.00021054
Iteration 157/1000 | Loss: 0.00028016
Iteration 158/1000 | Loss: 0.00024315
Iteration 159/1000 | Loss: 0.00032040
Iteration 160/1000 | Loss: 0.00031961
Iteration 161/1000 | Loss: 0.00025297
Iteration 162/1000 | Loss: 0.00024228
Iteration 163/1000 | Loss: 0.00023191
Iteration 164/1000 | Loss: 0.00081595
Iteration 165/1000 | Loss: 0.00146425
Iteration 166/1000 | Loss: 0.00117096
Iteration 167/1000 | Loss: 0.00069754
Iteration 168/1000 | Loss: 0.00053121
Iteration 169/1000 | Loss: 0.00061826
Iteration 170/1000 | Loss: 0.00070399
Iteration 171/1000 | Loss: 0.00038868
Iteration 172/1000 | Loss: 0.00035686
Iteration 173/1000 | Loss: 0.00050741
Iteration 174/1000 | Loss: 0.00054123
Iteration 175/1000 | Loss: 0.00014002
Iteration 176/1000 | Loss: 0.00096066
Iteration 177/1000 | Loss: 0.00061259
Iteration 178/1000 | Loss: 0.00055325
Iteration 179/1000 | Loss: 0.00063191
Iteration 180/1000 | Loss: 0.00119065
Iteration 181/1000 | Loss: 0.00066500
Iteration 182/1000 | Loss: 0.00026384
Iteration 183/1000 | Loss: 0.00008252
Iteration 184/1000 | Loss: 0.00014861
Iteration 185/1000 | Loss: 0.00012675
Iteration 186/1000 | Loss: 0.00009536
Iteration 187/1000 | Loss: 0.00009951
Iteration 188/1000 | Loss: 0.00006028
Iteration 189/1000 | Loss: 0.00007523
Iteration 190/1000 | Loss: 0.00005207
Iteration 191/1000 | Loss: 0.00012413
Iteration 192/1000 | Loss: 0.00010324
Iteration 193/1000 | Loss: 0.00017661
Iteration 194/1000 | Loss: 0.00020944
Iteration 195/1000 | Loss: 0.00094775
Iteration 196/1000 | Loss: 0.00006237
Iteration 197/1000 | Loss: 0.00007820
Iteration 198/1000 | Loss: 0.00009127
Iteration 199/1000 | Loss: 0.00008934
Iteration 200/1000 | Loss: 0.00048102
Iteration 201/1000 | Loss: 0.00009443
Iteration 202/1000 | Loss: 0.00008733
Iteration 203/1000 | Loss: 0.00011621
Iteration 204/1000 | Loss: 0.00007114
Iteration 205/1000 | Loss: 0.00006919
Iteration 206/1000 | Loss: 0.00005560
Iteration 207/1000 | Loss: 0.00005479
Iteration 208/1000 | Loss: 0.00015015
Iteration 209/1000 | Loss: 0.00013397
Iteration 210/1000 | Loss: 0.00006947
Iteration 211/1000 | Loss: 0.00010262
Iteration 212/1000 | Loss: 0.00005183
Iteration 213/1000 | Loss: 0.00007249
Iteration 214/1000 | Loss: 0.00053569
Iteration 215/1000 | Loss: 0.00045130
Iteration 216/1000 | Loss: 0.00027003
Iteration 217/1000 | Loss: 0.00029434
Iteration 218/1000 | Loss: 0.00004316
Iteration 219/1000 | Loss: 0.00003736
Iteration 220/1000 | Loss: 0.00005817
Iteration 221/1000 | Loss: 0.00004056
Iteration 222/1000 | Loss: 0.00027780
Iteration 223/1000 | Loss: 0.00041064
Iteration 224/1000 | Loss: 0.00023408
Iteration 225/1000 | Loss: 0.00046543
Iteration 226/1000 | Loss: 0.00017731
Iteration 227/1000 | Loss: 0.00018295
Iteration 228/1000 | Loss: 0.00016339
Iteration 229/1000 | Loss: 0.00014628
Iteration 230/1000 | Loss: 0.00011430
Iteration 231/1000 | Loss: 0.00017923
Iteration 232/1000 | Loss: 0.00010034
Iteration 233/1000 | Loss: 0.00004541
Iteration 234/1000 | Loss: 0.00005139
Iteration 235/1000 | Loss: 0.00003957
Iteration 236/1000 | Loss: 0.00018765
Iteration 237/1000 | Loss: 0.00019294
Iteration 238/1000 | Loss: 0.00006203
Iteration 239/1000 | Loss: 0.00005196
Iteration 240/1000 | Loss: 0.00022225
Iteration 241/1000 | Loss: 0.00004433
Iteration 242/1000 | Loss: 0.00003181
Iteration 243/1000 | Loss: 0.00005005
Iteration 244/1000 | Loss: 0.00003102
Iteration 245/1000 | Loss: 0.00005632
Iteration 246/1000 | Loss: 0.00003066
Iteration 247/1000 | Loss: 0.00003045
Iteration 248/1000 | Loss: 0.00020654
Iteration 249/1000 | Loss: 0.00007311
Iteration 250/1000 | Loss: 0.00003869
Iteration 251/1000 | Loss: 0.00020769
Iteration 252/1000 | Loss: 0.00006508
Iteration 253/1000 | Loss: 0.00021067
Iteration 254/1000 | Loss: 0.00061447
Iteration 255/1000 | Loss: 0.00011681
Iteration 256/1000 | Loss: 0.00037864
Iteration 257/1000 | Loss: 0.00006152
Iteration 258/1000 | Loss: 0.00019795
Iteration 259/1000 | Loss: 0.00009411
Iteration 260/1000 | Loss: 0.00020883
Iteration 261/1000 | Loss: 0.00010791
Iteration 262/1000 | Loss: 0.00020247
Iteration 263/1000 | Loss: 0.00004913
Iteration 264/1000 | Loss: 0.00006325
Iteration 265/1000 | Loss: 0.00031637
Iteration 266/1000 | Loss: 0.00005713
Iteration 267/1000 | Loss: 0.00005831
Iteration 268/1000 | Loss: 0.00010683
Iteration 269/1000 | Loss: 0.00004900
Iteration 270/1000 | Loss: 0.00003485
Iteration 271/1000 | Loss: 0.00009749
Iteration 272/1000 | Loss: 0.00003212
Iteration 273/1000 | Loss: 0.00003046
Iteration 274/1000 | Loss: 0.00003015
Iteration 275/1000 | Loss: 0.00007351
Iteration 276/1000 | Loss: 0.00003091
Iteration 277/1000 | Loss: 0.00002973
Iteration 278/1000 | Loss: 0.00004101
Iteration 279/1000 | Loss: 0.00003173
Iteration 280/1000 | Loss: 0.00003098
Iteration 281/1000 | Loss: 0.00004637
Iteration 282/1000 | Loss: 0.00026137
Iteration 283/1000 | Loss: 0.00007454
Iteration 284/1000 | Loss: 0.00005200
Iteration 285/1000 | Loss: 0.00008425
Iteration 286/1000 | Loss: 0.00004712
Iteration 287/1000 | Loss: 0.00010393
Iteration 288/1000 | Loss: 0.00063165
Iteration 289/1000 | Loss: 0.00020117
Iteration 290/1000 | Loss: 0.00007351
Iteration 291/1000 | Loss: 0.00016497
Iteration 292/1000 | Loss: 0.00013326
Iteration 293/1000 | Loss: 0.00005982
Iteration 294/1000 | Loss: 0.00019292
Iteration 295/1000 | Loss: 0.00013255
Iteration 296/1000 | Loss: 0.00014363
Iteration 297/1000 | Loss: 0.00014152
Iteration 298/1000 | Loss: 0.00004319
Iteration 299/1000 | Loss: 0.00013435
Iteration 300/1000 | Loss: 0.00008151
Iteration 301/1000 | Loss: 0.00014429
Iteration 302/1000 | Loss: 0.00009253
Iteration 303/1000 | Loss: 0.00007274
Iteration 304/1000 | Loss: 0.00018169
Iteration 305/1000 | Loss: 0.00017971
Iteration 306/1000 | Loss: 0.00013758
Iteration 307/1000 | Loss: 0.00012164
Iteration 308/1000 | Loss: 0.00014665
Iteration 309/1000 | Loss: 0.00012773
Iteration 310/1000 | Loss: 0.00014784
Iteration 311/1000 | Loss: 0.00012793
Iteration 312/1000 | Loss: 0.00019193
Iteration 313/1000 | Loss: 0.00013209
Iteration 314/1000 | Loss: 0.00008148
Iteration 315/1000 | Loss: 0.00024259
Iteration 316/1000 | Loss: 0.00079191
Iteration 317/1000 | Loss: 0.00014468
Iteration 318/1000 | Loss: 0.00013095
Iteration 319/1000 | Loss: 0.00005027
Iteration 320/1000 | Loss: 0.00007245
Iteration 321/1000 | Loss: 0.00004354
Iteration 322/1000 | Loss: 0.00006073
Iteration 323/1000 | Loss: 0.00003600
Iteration 324/1000 | Loss: 0.00004029
Iteration 325/1000 | Loss: 0.00010475
Iteration 326/1000 | Loss: 0.00006766
Iteration 327/1000 | Loss: 0.00013953
Iteration 328/1000 | Loss: 0.00010784
Iteration 329/1000 | Loss: 0.00005867
Iteration 330/1000 | Loss: 0.00003866
Iteration 331/1000 | Loss: 0.00003311
Iteration 332/1000 | Loss: 0.00004346
Iteration 333/1000 | Loss: 0.00005422
Iteration 334/1000 | Loss: 0.00004158
Iteration 335/1000 | Loss: 0.00007002
Iteration 336/1000 | Loss: 0.00004143
Iteration 337/1000 | Loss: 0.00004355
Iteration 338/1000 | Loss: 0.00003951
Iteration 339/1000 | Loss: 0.00004017
Iteration 340/1000 | Loss: 0.00004044
Iteration 341/1000 | Loss: 0.00003356
Iteration 342/1000 | Loss: 0.00003300
Iteration 343/1000 | Loss: 0.00004225
Iteration 344/1000 | Loss: 0.00003611
Iteration 345/1000 | Loss: 0.00004495
Iteration 346/1000 | Loss: 0.00004234
Iteration 347/1000 | Loss: 0.00004148
Iteration 348/1000 | Loss: 0.00004005
Iteration 349/1000 | Loss: 0.00003986
Iteration 350/1000 | Loss: 0.00004009
Iteration 351/1000 | Loss: 0.00004051
Iteration 352/1000 | Loss: 0.00004374
Iteration 353/1000 | Loss: 0.00003950
Iteration 354/1000 | Loss: 0.00006179
Iteration 355/1000 | Loss: 0.00008452
Iteration 356/1000 | Loss: 0.00004608
Iteration 357/1000 | Loss: 0.00004507
Iteration 358/1000 | Loss: 0.00004038
Iteration 359/1000 | Loss: 0.00003880
Iteration 360/1000 | Loss: 0.00003660
Iteration 361/1000 | Loss: 0.00007909
Iteration 362/1000 | Loss: 0.00004552
Iteration 363/1000 | Loss: 0.00004729
Iteration 364/1000 | Loss: 0.00005660
Iteration 365/1000 | Loss: 0.00004638
Iteration 366/1000 | Loss: 0.00003773
Iteration 367/1000 | Loss: 0.00003624
Iteration 368/1000 | Loss: 0.00006883
Iteration 369/1000 | Loss: 0.00004233
Iteration 370/1000 | Loss: 0.00003504
Iteration 371/1000 | Loss: 0.00003463
Iteration 372/1000 | Loss: 0.00003938
Iteration 373/1000 | Loss: 0.00004006
Iteration 374/1000 | Loss: 0.00003907
Iteration 375/1000 | Loss: 0.00003715
Iteration 376/1000 | Loss: 0.00005422
Iteration 377/1000 | Loss: 0.00003506
Iteration 378/1000 | Loss: 0.00003768
Iteration 379/1000 | Loss: 0.00004776
Iteration 380/1000 | Loss: 0.00003783
Iteration 381/1000 | Loss: 0.00004342
Iteration 382/1000 | Loss: 0.00003675
Iteration 383/1000 | Loss: 0.00004136
Iteration 384/1000 | Loss: 0.00003882
Iteration 385/1000 | Loss: 0.00004573
Iteration 386/1000 | Loss: 0.00007634
Iteration 387/1000 | Loss: 0.00005068
Iteration 388/1000 | Loss: 0.00003906
Iteration 389/1000 | Loss: 0.00003413
Iteration 390/1000 | Loss: 0.00003607
Iteration 391/1000 | Loss: 0.00003611
Iteration 392/1000 | Loss: 0.00003586
Iteration 393/1000 | Loss: 0.00003785
Iteration 394/1000 | Loss: 0.00003692
Iteration 395/1000 | Loss: 0.00004000
Iteration 396/1000 | Loss: 0.00003961
Iteration 397/1000 | Loss: 0.00003916
Iteration 398/1000 | Loss: 0.00003901
Iteration 399/1000 | Loss: 0.00003824
Iteration 400/1000 | Loss: 0.00003821
Iteration 401/1000 | Loss: 0.00004415
Iteration 402/1000 | Loss: 0.00003934
Iteration 403/1000 | Loss: 0.00004063
Iteration 404/1000 | Loss: 0.00003902
Iteration 405/1000 | Loss: 0.00009416
Iteration 406/1000 | Loss: 0.00004990
Iteration 407/1000 | Loss: 0.00003816
Iteration 408/1000 | Loss: 0.00003892
Iteration 409/1000 | Loss: 0.00003779
Iteration 410/1000 | Loss: 0.00003779
Iteration 411/1000 | Loss: 0.00004052
Iteration 412/1000 | Loss: 0.00003909
Iteration 413/1000 | Loss: 0.00004043
Iteration 414/1000 | Loss: 0.00006960
Iteration 415/1000 | Loss: 0.00005465
Iteration 416/1000 | Loss: 0.00004213
Iteration 417/1000 | Loss: 0.00005522
Iteration 418/1000 | Loss: 0.00002978
Iteration 419/1000 | Loss: 0.00004467
Iteration 420/1000 | Loss: 0.00003720
Iteration 421/1000 | Loss: 0.00003736
Iteration 422/1000 | Loss: 0.00004040
Iteration 423/1000 | Loss: 0.00003988
Iteration 424/1000 | Loss: 0.00003707
Iteration 425/1000 | Loss: 0.00003950
Iteration 426/1000 | Loss: 0.00003765
Iteration 427/1000 | Loss: 0.00003994
Iteration 428/1000 | Loss: 0.00004887
Iteration 429/1000 | Loss: 0.00004049
Iteration 430/1000 | Loss: 0.00004831
Iteration 431/1000 | Loss: 0.00003410
Iteration 432/1000 | Loss: 0.00007057
Iteration 433/1000 | Loss: 0.00003630
Iteration 434/1000 | Loss: 0.00003862
Iteration 435/1000 | Loss: 0.00003677
Iteration 436/1000 | Loss: 0.00004502
Iteration 437/1000 | Loss: 0.00003882
Iteration 438/1000 | Loss: 0.00003394
Iteration 439/1000 | Loss: 0.00003918
Iteration 440/1000 | Loss: 0.00004483
Iteration 441/1000 | Loss: 0.00003567
Iteration 442/1000 | Loss: 0.00008453
Iteration 443/1000 | Loss: 0.00003143
Iteration 444/1000 | Loss: 0.00003415
Iteration 445/1000 | Loss: 0.00003703
Iteration 446/1000 | Loss: 0.00005562
Iteration 447/1000 | Loss: 0.00004019
Iteration 448/1000 | Loss: 0.00005157
Iteration 449/1000 | Loss: 0.00004134
Iteration 450/1000 | Loss: 0.00004041
Iteration 451/1000 | Loss: 0.00004210
Iteration 452/1000 | Loss: 0.00003356
Iteration 453/1000 | Loss: 0.00003518
Iteration 454/1000 | Loss: 0.00003651
Iteration 455/1000 | Loss: 0.00003958
Iteration 456/1000 | Loss: 0.00003815
Iteration 457/1000 | Loss: 0.00003901
Iteration 458/1000 | Loss: 0.00005391
Iteration 459/1000 | Loss: 0.00003943
Iteration 460/1000 | Loss: 0.00003235
Iteration 461/1000 | Loss: 0.00003800
Iteration 462/1000 | Loss: 0.00004488
Iteration 463/1000 | Loss: 0.00004033
Iteration 464/1000 | Loss: 0.00003830
Iteration 465/1000 | Loss: 0.00004015
Iteration 466/1000 | Loss: 0.00003817
Iteration 467/1000 | Loss: 0.00003882
Iteration 468/1000 | Loss: 0.00004168
Iteration 469/1000 | Loss: 0.00003897
Iteration 470/1000 | Loss: 0.00003779
Iteration 471/1000 | Loss: 0.00003883
Iteration 472/1000 | Loss: 0.00003901
Iteration 473/1000 | Loss: 0.00003791
Iteration 474/1000 | Loss: 0.00004104
Iteration 475/1000 | Loss: 0.00004316
Iteration 476/1000 | Loss: 0.00003897
Iteration 477/1000 | Loss: 0.00003977
Iteration 478/1000 | Loss: 0.00003860
Iteration 479/1000 | Loss: 0.00003923
Iteration 480/1000 | Loss: 0.00003950
Iteration 481/1000 | Loss: 0.00004698
Iteration 482/1000 | Loss: 0.00004810
Iteration 483/1000 | Loss: 0.00003907
Iteration 484/1000 | Loss: 0.00003936
Iteration 485/1000 | Loss: 0.00003908
Iteration 486/1000 | Loss: 0.00004233
Iteration 487/1000 | Loss: 0.00003966
Iteration 488/1000 | Loss: 0.00003876
Iteration 489/1000 | Loss: 0.00003941
Iteration 490/1000 | Loss: 0.00004711
Iteration 491/1000 | Loss: 0.00003771
Iteration 492/1000 | Loss: 0.00005177
Iteration 493/1000 | Loss: 0.00003794
Iteration 494/1000 | Loss: 0.00003912
Iteration 495/1000 | Loss: 0.00004349
Iteration 496/1000 | Loss: 0.00005396
Iteration 497/1000 | Loss: 0.00003972
Iteration 498/1000 | Loss: 0.00003401
Iteration 499/1000 | Loss: 0.00004284
Iteration 500/1000 | Loss: 0.00005923
Iteration 501/1000 | Loss: 0.00007867
Iteration 502/1000 | Loss: 0.00003759
Iteration 503/1000 | Loss: 0.00003677
Iteration 504/1000 | Loss: 0.00003892
Iteration 505/1000 | Loss: 0.00004349
Iteration 506/1000 | Loss: 0.00003997
Iteration 507/1000 | Loss: 0.00006177
Iteration 508/1000 | Loss: 0.00004135
Iteration 509/1000 | Loss: 0.00003819
Iteration 510/1000 | Loss: 0.00004442
Iteration 511/1000 | Loss: 0.00003931
Iteration 512/1000 | Loss: 0.00003928
Iteration 513/1000 | Loss: 0.00003807
Iteration 514/1000 | Loss: 0.00003916
Iteration 515/1000 | Loss: 0.00008535
Iteration 516/1000 | Loss: 0.00004151
Iteration 517/1000 | Loss: 0.00003938
Iteration 518/1000 | Loss: 0.00004055
Iteration 519/1000 | Loss: 0.00004606
Iteration 520/1000 | Loss: 0.00003719
Iteration 521/1000 | Loss: 0.00004450
Iteration 522/1000 | Loss: 0.00004029
Iteration 523/1000 | Loss: 0.00004988
Iteration 524/1000 | Loss: 0.00005409
Iteration 525/1000 | Loss: 0.00004286
Iteration 526/1000 | Loss: 0.00003897
Iteration 527/1000 | Loss: 0.00004160
Iteration 528/1000 | Loss: 0.00003895
Iteration 529/1000 | Loss: 0.00004581
Iteration 530/1000 | Loss: 0.00003981
Iteration 531/1000 | Loss: 0.00003643
Iteration 532/1000 | Loss: 0.00003981
Iteration 533/1000 | Loss: 0.00004123
Iteration 534/1000 | Loss: 0.00004531
Iteration 535/1000 | Loss: 0.00003849
Iteration 536/1000 | Loss: 0.00003868
Iteration 537/1000 | Loss: 0.00004116
Iteration 538/1000 | Loss: 0.00003847
Iteration 539/1000 | Loss: 0.00003904
Iteration 540/1000 | Loss: 0.00003885
Iteration 541/1000 | Loss: 0.00003882
Iteration 542/1000 | Loss: 0.00003762
Iteration 543/1000 | Loss: 0.00003862
Iteration 544/1000 | Loss: 0.00009635
Iteration 545/1000 | Loss: 0.00020498
Iteration 546/1000 | Loss: 0.00026019
Iteration 547/1000 | Loss: 0.00014305
Iteration 548/1000 | Loss: 0.00005068
Iteration 549/1000 | Loss: 0.00009793
Iteration 550/1000 | Loss: 0.00006268
Iteration 551/1000 | Loss: 0.00010715
Iteration 552/1000 | Loss: 0.00030348
Iteration 553/1000 | Loss: 0.00018442
Iteration 554/1000 | Loss: 0.00003418
Iteration 555/1000 | Loss: 0.00002884
Iteration 556/1000 | Loss: 0.00006174
Iteration 557/1000 | Loss: 0.00002702
Iteration 558/1000 | Loss: 0.00003127
Iteration 559/1000 | Loss: 0.00004758
Iteration 560/1000 | Loss: 0.00002597
Iteration 561/1000 | Loss: 0.00004239
Iteration 562/1000 | Loss: 0.00002572
Iteration 563/1000 | Loss: 0.00002784
Iteration 564/1000 | Loss: 0.00002565
Iteration 565/1000 | Loss: 0.00002562
Iteration 566/1000 | Loss: 0.00002561
Iteration 567/1000 | Loss: 0.00002561
Iteration 568/1000 | Loss: 0.00002561
Iteration 569/1000 | Loss: 0.00003203
Iteration 570/1000 | Loss: 0.00002561
Iteration 571/1000 | Loss: 0.00002560
Iteration 572/1000 | Loss: 0.00002560
Iteration 573/1000 | Loss: 0.00002560
Iteration 574/1000 | Loss: 0.00002559
Iteration 575/1000 | Loss: 0.00002559
Iteration 576/1000 | Loss: 0.00002559
Iteration 577/1000 | Loss: 0.00002559
Iteration 578/1000 | Loss: 0.00002558
Iteration 579/1000 | Loss: 0.00002558
Iteration 580/1000 | Loss: 0.00002558
Iteration 581/1000 | Loss: 0.00002558
Iteration 582/1000 | Loss: 0.00002558
Iteration 583/1000 | Loss: 0.00002558
Iteration 584/1000 | Loss: 0.00002558
Iteration 585/1000 | Loss: 0.00002558
Iteration 586/1000 | Loss: 0.00002558
Iteration 587/1000 | Loss: 0.00002558
Iteration 588/1000 | Loss: 0.00002558
Iteration 589/1000 | Loss: 0.00002558
Iteration 590/1000 | Loss: 0.00002557
Iteration 591/1000 | Loss: 0.00002557
Iteration 592/1000 | Loss: 0.00002557
Iteration 593/1000 | Loss: 0.00002556
Iteration 594/1000 | Loss: 0.00002556
Iteration 595/1000 | Loss: 0.00002556
Iteration 596/1000 | Loss: 0.00002556
Iteration 597/1000 | Loss: 0.00002556
Iteration 598/1000 | Loss: 0.00002555
Iteration 599/1000 | Loss: 0.00002555
Iteration 600/1000 | Loss: 0.00002555
Iteration 601/1000 | Loss: 0.00002554
Iteration 602/1000 | Loss: 0.00002554
Iteration 603/1000 | Loss: 0.00002554
Iteration 604/1000 | Loss: 0.00002554
Iteration 605/1000 | Loss: 0.00002553
Iteration 606/1000 | Loss: 0.00002553
Iteration 607/1000 | Loss: 0.00002553
Iteration 608/1000 | Loss: 0.00002553
Iteration 609/1000 | Loss: 0.00002553
Iteration 610/1000 | Loss: 0.00002553
Iteration 611/1000 | Loss: 0.00002552
Iteration 612/1000 | Loss: 0.00002552
Iteration 613/1000 | Loss: 0.00002552
Iteration 614/1000 | Loss: 0.00002552
Iteration 615/1000 | Loss: 0.00004105
Iteration 616/1000 | Loss: 0.00002552
Iteration 617/1000 | Loss: 0.00002551
Iteration 618/1000 | Loss: 0.00002551
Iteration 619/1000 | Loss: 0.00002551
Iteration 620/1000 | Loss: 0.00002551
Iteration 621/1000 | Loss: 0.00002551
Iteration 622/1000 | Loss: 0.00002551
Iteration 623/1000 | Loss: 0.00002551
Iteration 624/1000 | Loss: 0.00002551
Iteration 625/1000 | Loss: 0.00002551
Iteration 626/1000 | Loss: 0.00002551
Iteration 627/1000 | Loss: 0.00002551
Iteration 628/1000 | Loss: 0.00002551
Iteration 629/1000 | Loss: 0.00002551
Iteration 630/1000 | Loss: 0.00002551
Iteration 631/1000 | Loss: 0.00007870
Iteration 632/1000 | Loss: 0.00004717
Iteration 633/1000 | Loss: 0.00002552
Iteration 634/1000 | Loss: 0.00002552
Iteration 635/1000 | Loss: 0.00002552
Iteration 636/1000 | Loss: 0.00002551
Iteration 637/1000 | Loss: 0.00002551
Iteration 638/1000 | Loss: 0.00002551
Iteration 639/1000 | Loss: 0.00002551
Iteration 640/1000 | Loss: 0.00002551
Iteration 641/1000 | Loss: 0.00002551
Iteration 642/1000 | Loss: 0.00002551
Iteration 643/1000 | Loss: 0.00002551
Iteration 644/1000 | Loss: 0.00002551
Iteration 645/1000 | Loss: 0.00002551
Iteration 646/1000 | Loss: 0.00002551
Iteration 647/1000 | Loss: 0.00002551
Iteration 648/1000 | Loss: 0.00002551
Iteration 649/1000 | Loss: 0.00002551
Iteration 650/1000 | Loss: 0.00002551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 650. Stopping optimization.
Last 5 losses: [2.5507004465907812e-05, 2.5507004465907812e-05, 2.5507004465907812e-05, 2.5507004465907812e-05, 2.5507004465907812e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5507004465907812e-05

Optimization complete. Final v2v error: 3.4569263458251953 mm

Highest mean error: 11.900040626525879 mm for frame 144

Lowest mean error: 2.5658950805664062 mm for frame 18

Saving results

Total time: 966.6807281970978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01137726
Iteration 2/25 | Loss: 0.00173611
Iteration 3/25 | Loss: 0.00101431
Iteration 4/25 | Loss: 0.00092304
Iteration 5/25 | Loss: 0.00088988
Iteration 6/25 | Loss: 0.00089213
Iteration 7/25 | Loss: 0.00090235
Iteration 8/25 | Loss: 0.00088277
Iteration 9/25 | Loss: 0.00089555
Iteration 10/25 | Loss: 0.00089238
Iteration 11/25 | Loss: 0.00087567
Iteration 12/25 | Loss: 0.00087874
Iteration 13/25 | Loss: 0.00087032
Iteration 14/25 | Loss: 0.00086113
Iteration 15/25 | Loss: 0.00086159
Iteration 16/25 | Loss: 0.00086492
Iteration 17/25 | Loss: 0.00086843
Iteration 18/25 | Loss: 0.00086580
Iteration 19/25 | Loss: 0.00086924
Iteration 20/25 | Loss: 0.00086612
Iteration 21/25 | Loss: 0.00086862
Iteration 22/25 | Loss: 0.00086086
Iteration 23/25 | Loss: 0.00086651
Iteration 24/25 | Loss: 0.00087393
Iteration 25/25 | Loss: 0.00086540

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05378997
Iteration 2/25 | Loss: 0.00090494
Iteration 3/25 | Loss: 0.00090493
Iteration 4/25 | Loss: 0.00090492
Iteration 5/25 | Loss: 0.00090492
Iteration 6/25 | Loss: 0.00090492
Iteration 7/25 | Loss: 0.00090492
Iteration 8/25 | Loss: 0.00090492
Iteration 9/25 | Loss: 0.00090492
Iteration 10/25 | Loss: 0.00090492
Iteration 11/25 | Loss: 0.00090492
Iteration 12/25 | Loss: 0.00090492
Iteration 13/25 | Loss: 0.00090492
Iteration 14/25 | Loss: 0.00090492
Iteration 15/25 | Loss: 0.00090492
Iteration 16/25 | Loss: 0.00090492
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009049225482158363, 0.0009049225482158363, 0.0009049225482158363, 0.0009049225482158363, 0.0009049225482158363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009049225482158363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090492
Iteration 2/1000 | Loss: 0.00096440
Iteration 3/1000 | Loss: 0.00078073
Iteration 4/1000 | Loss: 0.00108348
Iteration 5/1000 | Loss: 0.00101092
Iteration 6/1000 | Loss: 0.00080937
Iteration 7/1000 | Loss: 0.00055330
Iteration 8/1000 | Loss: 0.00084959
Iteration 9/1000 | Loss: 0.00083509
Iteration 10/1000 | Loss: 0.00076780
Iteration 11/1000 | Loss: 0.00096453
Iteration 12/1000 | Loss: 0.00114030
Iteration 13/1000 | Loss: 0.00097343
Iteration 14/1000 | Loss: 0.00091330
Iteration 15/1000 | Loss: 0.00066766
Iteration 16/1000 | Loss: 0.00079559
Iteration 17/1000 | Loss: 0.00082196
Iteration 18/1000 | Loss: 0.00140640
Iteration 19/1000 | Loss: 0.00086122
Iteration 20/1000 | Loss: 0.00075013
Iteration 21/1000 | Loss: 0.00137910
Iteration 22/1000 | Loss: 0.00181443
Iteration 23/1000 | Loss: 0.00165344
Iteration 24/1000 | Loss: 0.00197393
Iteration 25/1000 | Loss: 0.00180516
Iteration 26/1000 | Loss: 0.00118523
Iteration 27/1000 | Loss: 0.00144765
Iteration 28/1000 | Loss: 0.00143289
Iteration 29/1000 | Loss: 0.00163604
Iteration 30/1000 | Loss: 0.00110922
Iteration 31/1000 | Loss: 0.00100592
Iteration 32/1000 | Loss: 0.00085256
Iteration 33/1000 | Loss: 0.00097105
Iteration 34/1000 | Loss: 0.00072040
Iteration 35/1000 | Loss: 0.00068714
Iteration 36/1000 | Loss: 0.00077816
Iteration 37/1000 | Loss: 0.00064127
Iteration 38/1000 | Loss: 0.00102566
Iteration 39/1000 | Loss: 0.00083173
Iteration 40/1000 | Loss: 0.00069984
Iteration 41/1000 | Loss: 0.00075764
Iteration 42/1000 | Loss: 0.00068119
Iteration 43/1000 | Loss: 0.00101454
Iteration 44/1000 | Loss: 0.00117431
Iteration 45/1000 | Loss: 0.00085722
Iteration 46/1000 | Loss: 0.00093884
Iteration 47/1000 | Loss: 0.00095703
Iteration 48/1000 | Loss: 0.00090661
Iteration 49/1000 | Loss: 0.00091725
Iteration 50/1000 | Loss: 0.00087129
Iteration 51/1000 | Loss: 0.00099092
Iteration 52/1000 | Loss: 0.00106781
Iteration 53/1000 | Loss: 0.00112236
Iteration 54/1000 | Loss: 0.00090602
Iteration 55/1000 | Loss: 0.00109590
Iteration 56/1000 | Loss: 0.00115525
Iteration 57/1000 | Loss: 0.00111798
Iteration 58/1000 | Loss: 0.00059771
Iteration 59/1000 | Loss: 0.00041770
Iteration 60/1000 | Loss: 0.00064790
Iteration 61/1000 | Loss: 0.00080270
Iteration 62/1000 | Loss: 0.00085987
Iteration 63/1000 | Loss: 0.00088376
Iteration 64/1000 | Loss: 0.00080353
Iteration 65/1000 | Loss: 0.00080129
Iteration 66/1000 | Loss: 0.00090689
Iteration 67/1000 | Loss: 0.00072129
Iteration 68/1000 | Loss: 0.00082539
Iteration 69/1000 | Loss: 0.00092367
Iteration 70/1000 | Loss: 0.00093803
Iteration 71/1000 | Loss: 0.00099919
Iteration 72/1000 | Loss: 0.00089700
Iteration 73/1000 | Loss: 0.00083849
Iteration 74/1000 | Loss: 0.00097518
Iteration 75/1000 | Loss: 0.00099191
Iteration 76/1000 | Loss: 0.00107596
Iteration 77/1000 | Loss: 0.00114212
Iteration 78/1000 | Loss: 0.00093834
Iteration 79/1000 | Loss: 0.00085822
Iteration 80/1000 | Loss: 0.00078914
Iteration 81/1000 | Loss: 0.00075889
Iteration 82/1000 | Loss: 0.00111343
Iteration 83/1000 | Loss: 0.00097675
Iteration 84/1000 | Loss: 0.00074182
Iteration 85/1000 | Loss: 0.00131825
Iteration 86/1000 | Loss: 0.00112461
Iteration 87/1000 | Loss: 0.00123310
Iteration 88/1000 | Loss: 0.00098900
Iteration 89/1000 | Loss: 0.00102870
Iteration 90/1000 | Loss: 0.00069813
Iteration 91/1000 | Loss: 0.00058520
Iteration 92/1000 | Loss: 0.00072955
Iteration 93/1000 | Loss: 0.00086828
Iteration 94/1000 | Loss: 0.00074346
Iteration 95/1000 | Loss: 0.00065022
Iteration 96/1000 | Loss: 0.00096844
Iteration 97/1000 | Loss: 0.00048742
Iteration 98/1000 | Loss: 0.00059318
Iteration 99/1000 | Loss: 0.00053371
Iteration 100/1000 | Loss: 0.00074260
Iteration 101/1000 | Loss: 0.00076890
Iteration 102/1000 | Loss: 0.00073982
Iteration 103/1000 | Loss: 0.00085722
Iteration 104/1000 | Loss: 0.00077869
Iteration 105/1000 | Loss: 0.00085871
Iteration 106/1000 | Loss: 0.00079183
Iteration 107/1000 | Loss: 0.00071747
Iteration 108/1000 | Loss: 0.00088377
Iteration 109/1000 | Loss: 0.00073064
Iteration 110/1000 | Loss: 0.00090702
Iteration 111/1000 | Loss: 0.00084888
Iteration 112/1000 | Loss: 0.00091969
Iteration 113/1000 | Loss: 0.00064488
Iteration 114/1000 | Loss: 0.00089782
Iteration 115/1000 | Loss: 0.00085829
Iteration 116/1000 | Loss: 0.00089750
Iteration 117/1000 | Loss: 0.00079831
Iteration 118/1000 | Loss: 0.00076824
Iteration 119/1000 | Loss: 0.00071398
Iteration 120/1000 | Loss: 0.00065119
Iteration 121/1000 | Loss: 0.00072694
Iteration 122/1000 | Loss: 0.00089254
Iteration 123/1000 | Loss: 0.00101307
Iteration 124/1000 | Loss: 0.00072614
Iteration 125/1000 | Loss: 0.00087342
Iteration 126/1000 | Loss: 0.00072210
Iteration 127/1000 | Loss: 0.00071925
Iteration 128/1000 | Loss: 0.00082272
Iteration 129/1000 | Loss: 0.00069324
Iteration 130/1000 | Loss: 0.00063491
Iteration 131/1000 | Loss: 0.00096507
Iteration 132/1000 | Loss: 0.00081342
Iteration 133/1000 | Loss: 0.00054727
Iteration 134/1000 | Loss: 0.00060061
Iteration 135/1000 | Loss: 0.00062446
Iteration 136/1000 | Loss: 0.00047668
Iteration 137/1000 | Loss: 0.00067862
Iteration 138/1000 | Loss: 0.00052516
Iteration 139/1000 | Loss: 0.00058244
Iteration 140/1000 | Loss: 0.00042383
Iteration 141/1000 | Loss: 0.00089341
Iteration 142/1000 | Loss: 0.00080176
Iteration 143/1000 | Loss: 0.00080159
Iteration 144/1000 | Loss: 0.00067647
Iteration 145/1000 | Loss: 0.00094399
Iteration 146/1000 | Loss: 0.00072729
Iteration 147/1000 | Loss: 0.00071923
Iteration 148/1000 | Loss: 0.00079035
Iteration 149/1000 | Loss: 0.00135747
Iteration 150/1000 | Loss: 0.00101509
Iteration 151/1000 | Loss: 0.00075228
Iteration 152/1000 | Loss: 0.00048408
Iteration 153/1000 | Loss: 0.00060894
Iteration 154/1000 | Loss: 0.00067370
Iteration 155/1000 | Loss: 0.00108942
Iteration 156/1000 | Loss: 0.00102513
Iteration 157/1000 | Loss: 0.00088025
Iteration 158/1000 | Loss: 0.00094895
Iteration 159/1000 | Loss: 0.00093471
Iteration 160/1000 | Loss: 0.00067876
Iteration 161/1000 | Loss: 0.00085395
Iteration 162/1000 | Loss: 0.00066118
Iteration 163/1000 | Loss: 0.00082300
Iteration 164/1000 | Loss: 0.00089286
Iteration 165/1000 | Loss: 0.00124584
Iteration 166/1000 | Loss: 0.00060391
Iteration 167/1000 | Loss: 0.00068381
Iteration 168/1000 | Loss: 0.00086191
Iteration 169/1000 | Loss: 0.00051860
Iteration 170/1000 | Loss: 0.00049350
Iteration 171/1000 | Loss: 0.00067432
Iteration 172/1000 | Loss: 0.00079072
Iteration 173/1000 | Loss: 0.00096861
Iteration 174/1000 | Loss: 0.00083124
Iteration 175/1000 | Loss: 0.00122150
Iteration 176/1000 | Loss: 0.00178846
Iteration 177/1000 | Loss: 0.00072468
Iteration 178/1000 | Loss: 0.00055036
Iteration 179/1000 | Loss: 0.00093025
Iteration 180/1000 | Loss: 0.00068953
Iteration 181/1000 | Loss: 0.00077852
Iteration 182/1000 | Loss: 0.00092803
Iteration 183/1000 | Loss: 0.00098065
Iteration 184/1000 | Loss: 0.00091116
Iteration 185/1000 | Loss: 0.00073641
Iteration 186/1000 | Loss: 0.00066269
Iteration 187/1000 | Loss: 0.00075257
Iteration 188/1000 | Loss: 0.00082813
Iteration 189/1000 | Loss: 0.00065154
Iteration 190/1000 | Loss: 0.00071335
Iteration 191/1000 | Loss: 0.00055990
Iteration 192/1000 | Loss: 0.00056204
Iteration 193/1000 | Loss: 0.00053309
Iteration 194/1000 | Loss: 0.00040695
Iteration 195/1000 | Loss: 0.00048040
Iteration 196/1000 | Loss: 0.00068742
Iteration 197/1000 | Loss: 0.00081289
Iteration 198/1000 | Loss: 0.00073984
Iteration 199/1000 | Loss: 0.00072248
Iteration 200/1000 | Loss: 0.00097025
Iteration 201/1000 | Loss: 0.00072124
Iteration 202/1000 | Loss: 0.00054841
Iteration 203/1000 | Loss: 0.00124153
Iteration 204/1000 | Loss: 0.00088453
Iteration 205/1000 | Loss: 0.00087315
Iteration 206/1000 | Loss: 0.00065392
Iteration 207/1000 | Loss: 0.00063632
Iteration 208/1000 | Loss: 0.00067302
Iteration 209/1000 | Loss: 0.00075844
Iteration 210/1000 | Loss: 0.00089089
Iteration 211/1000 | Loss: 0.00088445
Iteration 212/1000 | Loss: 0.00107188
Iteration 213/1000 | Loss: 0.00075413
Iteration 214/1000 | Loss: 0.00090947
Iteration 215/1000 | Loss: 0.00092648
Iteration 216/1000 | Loss: 0.00091034
Iteration 217/1000 | Loss: 0.00091479
Iteration 218/1000 | Loss: 0.00085679
Iteration 219/1000 | Loss: 0.00075601
Iteration 220/1000 | Loss: 0.00097175
Iteration 221/1000 | Loss: 0.00075593
Iteration 222/1000 | Loss: 0.00082622
Iteration 223/1000 | Loss: 0.00070890
Iteration 224/1000 | Loss: 0.00042460
Iteration 225/1000 | Loss: 0.00052206
Iteration 226/1000 | Loss: 0.00042754
Iteration 227/1000 | Loss: 0.00043917
Iteration 228/1000 | Loss: 0.00040812
Iteration 229/1000 | Loss: 0.00047771
Iteration 230/1000 | Loss: 0.00063128
Iteration 231/1000 | Loss: 0.00057490
Iteration 232/1000 | Loss: 0.00058098
Iteration 233/1000 | Loss: 0.00070938
Iteration 234/1000 | Loss: 0.00062592
Iteration 235/1000 | Loss: 0.00058231
Iteration 236/1000 | Loss: 0.00063667
Iteration 237/1000 | Loss: 0.00058708
Iteration 238/1000 | Loss: 0.00061432
Iteration 239/1000 | Loss: 0.00052215
Iteration 240/1000 | Loss: 0.00028748
Iteration 241/1000 | Loss: 0.00055252
Iteration 242/1000 | Loss: 0.00052069
Iteration 243/1000 | Loss: 0.00034054
Iteration 244/1000 | Loss: 0.00031001
Iteration 245/1000 | Loss: 0.00044075
Iteration 246/1000 | Loss: 0.00038774
Iteration 247/1000 | Loss: 0.00052671
Iteration 248/1000 | Loss: 0.00050377
Iteration 249/1000 | Loss: 0.00045748
Iteration 250/1000 | Loss: 0.00049856
Iteration 251/1000 | Loss: 0.00050245
Iteration 252/1000 | Loss: 0.00053543
Iteration 253/1000 | Loss: 0.00059085
Iteration 254/1000 | Loss: 0.00054335
Iteration 255/1000 | Loss: 0.00051606
Iteration 256/1000 | Loss: 0.00022154
Iteration 257/1000 | Loss: 0.00049496
Iteration 258/1000 | Loss: 0.00039814
Iteration 259/1000 | Loss: 0.00052719
Iteration 260/1000 | Loss: 0.00050223
Iteration 261/1000 | Loss: 0.00038961
Iteration 262/1000 | Loss: 0.00048772
Iteration 263/1000 | Loss: 0.00050675
Iteration 264/1000 | Loss: 0.00049978
Iteration 265/1000 | Loss: 0.00037943
Iteration 266/1000 | Loss: 0.00048463
Iteration 267/1000 | Loss: 0.00047274
Iteration 268/1000 | Loss: 0.00056564
Iteration 269/1000 | Loss: 0.00055925
Iteration 270/1000 | Loss: 0.00042755
Iteration 271/1000 | Loss: 0.00031009
Iteration 272/1000 | Loss: 0.00024996
Iteration 273/1000 | Loss: 0.00027155
Iteration 274/1000 | Loss: 0.00047663
Iteration 275/1000 | Loss: 0.00038495
Iteration 276/1000 | Loss: 0.00032279
Iteration 277/1000 | Loss: 0.00035981
Iteration 278/1000 | Loss: 0.00032659
Iteration 279/1000 | Loss: 0.00028312
Iteration 280/1000 | Loss: 0.00029389
Iteration 281/1000 | Loss: 0.00037266
Iteration 282/1000 | Loss: 0.00033834
Iteration 283/1000 | Loss: 0.00029327
Iteration 284/1000 | Loss: 0.00016784
Iteration 285/1000 | Loss: 0.00037653
Iteration 286/1000 | Loss: 0.00060044
Iteration 287/1000 | Loss: 0.00042731
Iteration 288/1000 | Loss: 0.00066526
Iteration 289/1000 | Loss: 0.00034093
Iteration 290/1000 | Loss: 0.00033959
Iteration 291/1000 | Loss: 0.00035219
Iteration 292/1000 | Loss: 0.00019501
Iteration 293/1000 | Loss: 0.00019570
Iteration 294/1000 | Loss: 0.00036273
Iteration 295/1000 | Loss: 0.00038434
Iteration 296/1000 | Loss: 0.00035985
Iteration 297/1000 | Loss: 0.00036528
Iteration 298/1000 | Loss: 0.00036523
Iteration 299/1000 | Loss: 0.00036862
Iteration 300/1000 | Loss: 0.00038316
Iteration 301/1000 | Loss: 0.00044582
Iteration 302/1000 | Loss: 0.00031080
Iteration 303/1000 | Loss: 0.00035043
Iteration 304/1000 | Loss: 0.00018297
Iteration 305/1000 | Loss: 0.00038892
Iteration 306/1000 | Loss: 0.00042758
Iteration 307/1000 | Loss: 0.00040087
Iteration 308/1000 | Loss: 0.00031190
Iteration 309/1000 | Loss: 0.00032813
Iteration 310/1000 | Loss: 0.00036986
Iteration 311/1000 | Loss: 0.00031405
Iteration 312/1000 | Loss: 0.00044487
Iteration 313/1000 | Loss: 0.00045597
Iteration 314/1000 | Loss: 0.00034907
Iteration 315/1000 | Loss: 0.00024007
Iteration 316/1000 | Loss: 0.00036141
Iteration 317/1000 | Loss: 0.00033363
Iteration 318/1000 | Loss: 0.00037719
Iteration 319/1000 | Loss: 0.00028070
Iteration 320/1000 | Loss: 0.00042538
Iteration 321/1000 | Loss: 0.00085552
Iteration 322/1000 | Loss: 0.00060638
Iteration 323/1000 | Loss: 0.00023833
Iteration 324/1000 | Loss: 0.00031263
Iteration 325/1000 | Loss: 0.00014020
Iteration 326/1000 | Loss: 0.00027627
Iteration 327/1000 | Loss: 0.00027529
Iteration 328/1000 | Loss: 0.00040920
Iteration 329/1000 | Loss: 0.00037327
Iteration 330/1000 | Loss: 0.00026069
Iteration 331/1000 | Loss: 0.00028705
Iteration 332/1000 | Loss: 0.00031329
Iteration 333/1000 | Loss: 0.00027174
Iteration 334/1000 | Loss: 0.00029635
Iteration 335/1000 | Loss: 0.00019084
Iteration 336/1000 | Loss: 0.00013170
Iteration 337/1000 | Loss: 0.00009930
Iteration 338/1000 | Loss: 0.00020297
Iteration 339/1000 | Loss: 0.00014151
Iteration 340/1000 | Loss: 0.00030019
Iteration 341/1000 | Loss: 0.00027383
Iteration 342/1000 | Loss: 0.00026819
Iteration 343/1000 | Loss: 0.00052109
Iteration 344/1000 | Loss: 0.00081509
Iteration 345/1000 | Loss: 0.00045814
Iteration 346/1000 | Loss: 0.00073029
Iteration 347/1000 | Loss: 0.00041741
Iteration 348/1000 | Loss: 0.00026214
Iteration 349/1000 | Loss: 0.00032425
Iteration 350/1000 | Loss: 0.00028606
Iteration 351/1000 | Loss: 0.00025028
Iteration 352/1000 | Loss: 0.00033219
Iteration 353/1000 | Loss: 0.00025627
Iteration 354/1000 | Loss: 0.00023572
Iteration 355/1000 | Loss: 0.00030868
Iteration 356/1000 | Loss: 0.00033241
Iteration 357/1000 | Loss: 0.00035122
Iteration 358/1000 | Loss: 0.00005818
Iteration 359/1000 | Loss: 0.00016673
Iteration 360/1000 | Loss: 0.00015297
Iteration 361/1000 | Loss: 0.00015447
Iteration 362/1000 | Loss: 0.00013398
Iteration 363/1000 | Loss: 0.00008271
Iteration 364/1000 | Loss: 0.00013850
Iteration 365/1000 | Loss: 0.00011859
Iteration 366/1000 | Loss: 0.00013001
Iteration 367/1000 | Loss: 0.00018973
Iteration 368/1000 | Loss: 0.00014881
Iteration 369/1000 | Loss: 0.00007096
Iteration 370/1000 | Loss: 0.00019207
Iteration 371/1000 | Loss: 0.00012616
Iteration 372/1000 | Loss: 0.00017087
Iteration 373/1000 | Loss: 0.00016871
Iteration 374/1000 | Loss: 0.00006535
Iteration 375/1000 | Loss: 0.00016199
Iteration 376/1000 | Loss: 0.00011900
Iteration 377/1000 | Loss: 0.00015692
Iteration 378/1000 | Loss: 0.00017921
Iteration 379/1000 | Loss: 0.00005689
Iteration 380/1000 | Loss: 0.00005755
Iteration 381/1000 | Loss: 0.00010044
Iteration 382/1000 | Loss: 0.00016413
Iteration 383/1000 | Loss: 0.00016367
Iteration 384/1000 | Loss: 0.00019567
Iteration 385/1000 | Loss: 0.00018073
Iteration 386/1000 | Loss: 0.00029738
Iteration 387/1000 | Loss: 0.00010514
Iteration 388/1000 | Loss: 0.00014936
Iteration 389/1000 | Loss: 0.00016120
Iteration 390/1000 | Loss: 0.00032949
Iteration 391/1000 | Loss: 0.00058919
Iteration 392/1000 | Loss: 0.00014080
Iteration 393/1000 | Loss: 0.00007211
Iteration 394/1000 | Loss: 0.00050276
Iteration 395/1000 | Loss: 0.00010699
Iteration 396/1000 | Loss: 0.00006771
Iteration 397/1000 | Loss: 0.00005394
Iteration 398/1000 | Loss: 0.00006072
Iteration 399/1000 | Loss: 0.00004664
Iteration 400/1000 | Loss: 0.00016604
Iteration 401/1000 | Loss: 0.00006582
Iteration 402/1000 | Loss: 0.00006146
Iteration 403/1000 | Loss: 0.00006402
Iteration 404/1000 | Loss: 0.00004690
Iteration 405/1000 | Loss: 0.00003688
Iteration 406/1000 | Loss: 0.00004119
Iteration 407/1000 | Loss: 0.00004819
Iteration 408/1000 | Loss: 0.00005420
Iteration 409/1000 | Loss: 0.00005731
Iteration 410/1000 | Loss: 0.00005950
Iteration 411/1000 | Loss: 0.00005362
Iteration 412/1000 | Loss: 0.00006070
Iteration 413/1000 | Loss: 0.00005187
Iteration 414/1000 | Loss: 0.00005960
Iteration 415/1000 | Loss: 0.00005949
Iteration 416/1000 | Loss: 0.00005794
Iteration 417/1000 | Loss: 0.00005871
Iteration 418/1000 | Loss: 0.00005248
Iteration 419/1000 | Loss: 0.00005663
Iteration 420/1000 | Loss: 0.00005280
Iteration 421/1000 | Loss: 0.00005591
Iteration 422/1000 | Loss: 0.00005299
Iteration 423/1000 | Loss: 0.00005408
Iteration 424/1000 | Loss: 0.00005105
Iteration 425/1000 | Loss: 0.00005000
Iteration 426/1000 | Loss: 0.00005289
Iteration 427/1000 | Loss: 0.00005320
Iteration 428/1000 | Loss: 0.00005141
Iteration 429/1000 | Loss: 0.00005927
Iteration 430/1000 | Loss: 0.00005383
Iteration 431/1000 | Loss: 0.00005464
Iteration 432/1000 | Loss: 0.00005284
Iteration 433/1000 | Loss: 0.00005283
Iteration 434/1000 | Loss: 0.00005246
Iteration 435/1000 | Loss: 0.00006273
Iteration 436/1000 | Loss: 0.00005512
Iteration 437/1000 | Loss: 0.00005410
Iteration 438/1000 | Loss: 0.00005671
Iteration 439/1000 | Loss: 0.00006691
Iteration 440/1000 | Loss: 0.00005960
Iteration 441/1000 | Loss: 0.00006062
Iteration 442/1000 | Loss: 0.00005115
Iteration 443/1000 | Loss: 0.00005271
Iteration 444/1000 | Loss: 0.00005192
Iteration 445/1000 | Loss: 0.00005306
Iteration 446/1000 | Loss: 0.00005117
Iteration 447/1000 | Loss: 0.00005397
Iteration 448/1000 | Loss: 0.00005444
Iteration 449/1000 | Loss: 0.00005371
Iteration 450/1000 | Loss: 0.00005456
Iteration 451/1000 | Loss: 0.00005013
Iteration 452/1000 | Loss: 0.00005580
Iteration 453/1000 | Loss: 0.00005360
Iteration 454/1000 | Loss: 0.00005096
Iteration 455/1000 | Loss: 0.00005282
Iteration 456/1000 | Loss: 0.00005144
Iteration 457/1000 | Loss: 0.00005212
Iteration 458/1000 | Loss: 0.00005182
Iteration 459/1000 | Loss: 0.00005155
Iteration 460/1000 | Loss: 0.00005422
Iteration 461/1000 | Loss: 0.00005402
Iteration 462/1000 | Loss: 0.00005084
Iteration 463/1000 | Loss: 0.00005226
Iteration 464/1000 | Loss: 0.00004985
Iteration 465/1000 | Loss: 0.00005222
Iteration 466/1000 | Loss: 0.00004984
Iteration 467/1000 | Loss: 0.00004317
Iteration 468/1000 | Loss: 0.00004913
Iteration 469/1000 | Loss: 0.00005333
Iteration 470/1000 | Loss: 0.00005573
Iteration 471/1000 | Loss: 0.00005180
Iteration 472/1000 | Loss: 0.00005391
Iteration 473/1000 | Loss: 0.00005299
Iteration 474/1000 | Loss: 0.00005473
Iteration 475/1000 | Loss: 0.00005275
Iteration 476/1000 | Loss: 0.00005208
Iteration 477/1000 | Loss: 0.00005255
Iteration 478/1000 | Loss: 0.00005485
Iteration 479/1000 | Loss: 0.00006117
Iteration 480/1000 | Loss: 0.00005505
Iteration 481/1000 | Loss: 0.00005541
Iteration 482/1000 | Loss: 0.00005506
Iteration 483/1000 | Loss: 0.00005565
Iteration 484/1000 | Loss: 0.00005654
Iteration 485/1000 | Loss: 0.00005015
Iteration 486/1000 | Loss: 0.00006291
Iteration 487/1000 | Loss: 0.00004395
Iteration 488/1000 | Loss: 0.00007041
Iteration 489/1000 | Loss: 0.00005322
Iteration 490/1000 | Loss: 0.00006299
Iteration 491/1000 | Loss: 0.00005016
Iteration 492/1000 | Loss: 0.00006157
Iteration 493/1000 | Loss: 0.00004991
Iteration 494/1000 | Loss: 0.00006067
Iteration 495/1000 | Loss: 0.00005484
Iteration 496/1000 | Loss: 0.00005473
Iteration 497/1000 | Loss: 0.00007059
Iteration 498/1000 | Loss: 0.00003547
Iteration 499/1000 | Loss: 0.00003144
Iteration 500/1000 | Loss: 0.00002981
Iteration 501/1000 | Loss: 0.00002863
Iteration 502/1000 | Loss: 0.00002801
Iteration 503/1000 | Loss: 0.00002749
Iteration 504/1000 | Loss: 0.00002705
Iteration 505/1000 | Loss: 0.00002682
Iteration 506/1000 | Loss: 0.00002662
Iteration 507/1000 | Loss: 0.00002654
Iteration 508/1000 | Loss: 0.00002641
Iteration 509/1000 | Loss: 0.00002640
Iteration 510/1000 | Loss: 0.00002635
Iteration 511/1000 | Loss: 0.00002635
Iteration 512/1000 | Loss: 0.00002635
Iteration 513/1000 | Loss: 0.00002635
Iteration 514/1000 | Loss: 0.00002635
Iteration 515/1000 | Loss: 0.00002635
Iteration 516/1000 | Loss: 0.00002635
Iteration 517/1000 | Loss: 0.00002635
Iteration 518/1000 | Loss: 0.00002635
Iteration 519/1000 | Loss: 0.00002635
Iteration 520/1000 | Loss: 0.00002634
Iteration 521/1000 | Loss: 0.00002634
Iteration 522/1000 | Loss: 0.00002634
Iteration 523/1000 | Loss: 0.00002634
Iteration 524/1000 | Loss: 0.00002633
Iteration 525/1000 | Loss: 0.00002629
Iteration 526/1000 | Loss: 0.00002626
Iteration 527/1000 | Loss: 0.00002626
Iteration 528/1000 | Loss: 0.00002626
Iteration 529/1000 | Loss: 0.00002625
Iteration 530/1000 | Loss: 0.00002624
Iteration 531/1000 | Loss: 0.00002624
Iteration 532/1000 | Loss: 0.00002623
Iteration 533/1000 | Loss: 0.00002623
Iteration 534/1000 | Loss: 0.00002623
Iteration 535/1000 | Loss: 0.00002622
Iteration 536/1000 | Loss: 0.00002622
Iteration 537/1000 | Loss: 0.00002622
Iteration 538/1000 | Loss: 0.00002621
Iteration 539/1000 | Loss: 0.00002621
Iteration 540/1000 | Loss: 0.00002621
Iteration 541/1000 | Loss: 0.00002620
Iteration 542/1000 | Loss: 0.00002620
Iteration 543/1000 | Loss: 0.00002620
Iteration 544/1000 | Loss: 0.00002619
Iteration 545/1000 | Loss: 0.00002619
Iteration 546/1000 | Loss: 0.00002619
Iteration 547/1000 | Loss: 0.00002617
Iteration 548/1000 | Loss: 0.00002617
Iteration 549/1000 | Loss: 0.00002617
Iteration 550/1000 | Loss: 0.00002617
Iteration 551/1000 | Loss: 0.00002617
Iteration 552/1000 | Loss: 0.00002617
Iteration 553/1000 | Loss: 0.00002616
Iteration 554/1000 | Loss: 0.00002616
Iteration 555/1000 | Loss: 0.00002616
Iteration 556/1000 | Loss: 0.00002616
Iteration 557/1000 | Loss: 0.00002616
Iteration 558/1000 | Loss: 0.00002616
Iteration 559/1000 | Loss: 0.00002615
Iteration 560/1000 | Loss: 0.00002615
Iteration 561/1000 | Loss: 0.00002615
Iteration 562/1000 | Loss: 0.00002615
Iteration 563/1000 | Loss: 0.00002614
Iteration 564/1000 | Loss: 0.00002614
Iteration 565/1000 | Loss: 0.00002614
Iteration 566/1000 | Loss: 0.00002614
Iteration 567/1000 | Loss: 0.00002614
Iteration 568/1000 | Loss: 0.00002614
Iteration 569/1000 | Loss: 0.00002614
Iteration 570/1000 | Loss: 0.00002614
Iteration 571/1000 | Loss: 0.00002614
Iteration 572/1000 | Loss: 0.00002614
Iteration 573/1000 | Loss: 0.00002614
Iteration 574/1000 | Loss: 0.00002614
Iteration 575/1000 | Loss: 0.00002614
Iteration 576/1000 | Loss: 0.00002613
Iteration 577/1000 | Loss: 0.00002613
Iteration 578/1000 | Loss: 0.00002613
Iteration 579/1000 | Loss: 0.00002613
Iteration 580/1000 | Loss: 0.00002613
Iteration 581/1000 | Loss: 0.00002613
Iteration 582/1000 | Loss: 0.00002613
Iteration 583/1000 | Loss: 0.00002613
Iteration 584/1000 | Loss: 0.00002613
Iteration 585/1000 | Loss: 0.00002613
Iteration 586/1000 | Loss: 0.00002613
Iteration 587/1000 | Loss: 0.00002612
Iteration 588/1000 | Loss: 0.00002612
Iteration 589/1000 | Loss: 0.00002612
Iteration 590/1000 | Loss: 0.00002612
Iteration 591/1000 | Loss: 0.00002612
Iteration 592/1000 | Loss: 0.00002612
Iteration 593/1000 | Loss: 0.00002612
Iteration 594/1000 | Loss: 0.00002612
Iteration 595/1000 | Loss: 0.00002612
Iteration 596/1000 | Loss: 0.00002612
Iteration 597/1000 | Loss: 0.00002612
Iteration 598/1000 | Loss: 0.00002611
Iteration 599/1000 | Loss: 0.00002611
Iteration 600/1000 | Loss: 0.00002611
Iteration 601/1000 | Loss: 0.00002611
Iteration 602/1000 | Loss: 0.00002610
Iteration 603/1000 | Loss: 0.00002610
Iteration 604/1000 | Loss: 0.00002610
Iteration 605/1000 | Loss: 0.00002610
Iteration 606/1000 | Loss: 0.00002610
Iteration 607/1000 | Loss: 0.00002610
Iteration 608/1000 | Loss: 0.00002610
Iteration 609/1000 | Loss: 0.00002610
Iteration 610/1000 | Loss: 0.00002610
Iteration 611/1000 | Loss: 0.00002610
Iteration 612/1000 | Loss: 0.00002610
Iteration 613/1000 | Loss: 0.00002610
Iteration 614/1000 | Loss: 0.00002610
Iteration 615/1000 | Loss: 0.00002610
Iteration 616/1000 | Loss: 0.00002609
Iteration 617/1000 | Loss: 0.00002609
Iteration 618/1000 | Loss: 0.00002609
Iteration 619/1000 | Loss: 0.00002609
Iteration 620/1000 | Loss: 0.00002608
Iteration 621/1000 | Loss: 0.00002608
Iteration 622/1000 | Loss: 0.00002608
Iteration 623/1000 | Loss: 0.00002607
Iteration 624/1000 | Loss: 0.00002607
Iteration 625/1000 | Loss: 0.00002606
Iteration 626/1000 | Loss: 0.00002606
Iteration 627/1000 | Loss: 0.00002606
Iteration 628/1000 | Loss: 0.00002606
Iteration 629/1000 | Loss: 0.00002606
Iteration 630/1000 | Loss: 0.00002606
Iteration 631/1000 | Loss: 0.00002606
Iteration 632/1000 | Loss: 0.00002606
Iteration 633/1000 | Loss: 0.00002606
Iteration 634/1000 | Loss: 0.00002606
Iteration 635/1000 | Loss: 0.00002605
Iteration 636/1000 | Loss: 0.00002605
Iteration 637/1000 | Loss: 0.00002605
Iteration 638/1000 | Loss: 0.00002605
Iteration 639/1000 | Loss: 0.00002605
Iteration 640/1000 | Loss: 0.00002604
Iteration 641/1000 | Loss: 0.00002604
Iteration 642/1000 | Loss: 0.00002604
Iteration 643/1000 | Loss: 0.00002604
Iteration 644/1000 | Loss: 0.00002604
Iteration 645/1000 | Loss: 0.00002604
Iteration 646/1000 | Loss: 0.00002604
Iteration 647/1000 | Loss: 0.00002604
Iteration 648/1000 | Loss: 0.00002604
Iteration 649/1000 | Loss: 0.00002604
Iteration 650/1000 | Loss: 0.00002604
Iteration 651/1000 | Loss: 0.00002603
Iteration 652/1000 | Loss: 0.00002603
Iteration 653/1000 | Loss: 0.00002603
Iteration 654/1000 | Loss: 0.00002603
Iteration 655/1000 | Loss: 0.00002603
Iteration 656/1000 | Loss: 0.00002603
Iteration 657/1000 | Loss: 0.00002603
Iteration 658/1000 | Loss: 0.00002603
Iteration 659/1000 | Loss: 0.00002603
Iteration 660/1000 | Loss: 0.00002603
Iteration 661/1000 | Loss: 0.00002603
Iteration 662/1000 | Loss: 0.00002603
Iteration 663/1000 | Loss: 0.00002603
Iteration 664/1000 | Loss: 0.00002603
Iteration 665/1000 | Loss: 0.00002603
Iteration 666/1000 | Loss: 0.00002603
Iteration 667/1000 | Loss: 0.00002603
Iteration 668/1000 | Loss: 0.00002602
Iteration 669/1000 | Loss: 0.00002602
Iteration 670/1000 | Loss: 0.00002602
Iteration 671/1000 | Loss: 0.00002602
Iteration 672/1000 | Loss: 0.00002602
Iteration 673/1000 | Loss: 0.00002602
Iteration 674/1000 | Loss: 0.00002602
Iteration 675/1000 | Loss: 0.00002602
Iteration 676/1000 | Loss: 0.00002602
Iteration 677/1000 | Loss: 0.00002602
Iteration 678/1000 | Loss: 0.00002602
Iteration 679/1000 | Loss: 0.00002602
Iteration 680/1000 | Loss: 0.00002602
Iteration 681/1000 | Loss: 0.00002602
Iteration 682/1000 | Loss: 0.00002602
Iteration 683/1000 | Loss: 0.00002601
Iteration 684/1000 | Loss: 0.00002601
Iteration 685/1000 | Loss: 0.00002601
Iteration 686/1000 | Loss: 0.00002601
Iteration 687/1000 | Loss: 0.00002601
Iteration 688/1000 | Loss: 0.00002601
Iteration 689/1000 | Loss: 0.00002601
Iteration 690/1000 | Loss: 0.00002601
Iteration 691/1000 | Loss: 0.00002601
Iteration 692/1000 | Loss: 0.00002601
Iteration 693/1000 | Loss: 0.00002601
Iteration 694/1000 | Loss: 0.00002601
Iteration 695/1000 | Loss: 0.00002601
Iteration 696/1000 | Loss: 0.00002601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 696. Stopping optimization.
Last 5 losses: [2.6008458007709123e-05, 2.6008458007709123e-05, 2.6008458007709123e-05, 2.6008458007709123e-05, 2.6008458007709123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6008458007709123e-05

Optimization complete. Final v2v error: 4.159385681152344 mm

Highest mean error: 4.881392002105713 mm for frame 153

Lowest mean error: 3.7078585624694824 mm for frame 126

Saving results

Total time: 794.8940405845642
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871061
Iteration 2/25 | Loss: 0.00094079
Iteration 3/25 | Loss: 0.00075489
Iteration 4/25 | Loss: 0.00071762
Iteration 5/25 | Loss: 0.00070429
Iteration 6/25 | Loss: 0.00070076
Iteration 7/25 | Loss: 0.00069992
Iteration 8/25 | Loss: 0.00069992
Iteration 9/25 | Loss: 0.00069992
Iteration 10/25 | Loss: 0.00069992
Iteration 11/25 | Loss: 0.00069992
Iteration 12/25 | Loss: 0.00069992
Iteration 13/25 | Loss: 0.00069992
Iteration 14/25 | Loss: 0.00069992
Iteration 15/25 | Loss: 0.00069992
Iteration 16/25 | Loss: 0.00069992
Iteration 17/25 | Loss: 0.00069992
Iteration 18/25 | Loss: 0.00069992
Iteration 19/25 | Loss: 0.00069992
Iteration 20/25 | Loss: 0.00069992
Iteration 21/25 | Loss: 0.00069992
Iteration 22/25 | Loss: 0.00069992
Iteration 23/25 | Loss: 0.00069992
Iteration 24/25 | Loss: 0.00069992
Iteration 25/25 | Loss: 0.00069992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39528596
Iteration 2/25 | Loss: 0.00035337
Iteration 3/25 | Loss: 0.00035336
Iteration 4/25 | Loss: 0.00035336
Iteration 5/25 | Loss: 0.00035336
Iteration 6/25 | Loss: 0.00035336
Iteration 7/25 | Loss: 0.00035336
Iteration 8/25 | Loss: 0.00035336
Iteration 9/25 | Loss: 0.00035336
Iteration 10/25 | Loss: 0.00035336
Iteration 11/25 | Loss: 0.00035336
Iteration 12/25 | Loss: 0.00035336
Iteration 13/25 | Loss: 0.00035336
Iteration 14/25 | Loss: 0.00035336
Iteration 15/25 | Loss: 0.00035336
Iteration 16/25 | Loss: 0.00035336
Iteration 17/25 | Loss: 0.00035336
Iteration 18/25 | Loss: 0.00035336
Iteration 19/25 | Loss: 0.00035336
Iteration 20/25 | Loss: 0.00035336
Iteration 21/25 | Loss: 0.00035336
Iteration 22/25 | Loss: 0.00035336
Iteration 23/25 | Loss: 0.00035336
Iteration 24/25 | Loss: 0.00035336
Iteration 25/25 | Loss: 0.00035336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035336
Iteration 2/1000 | Loss: 0.00004236
Iteration 3/1000 | Loss: 0.00002984
Iteration 4/1000 | Loss: 0.00002632
Iteration 5/1000 | Loss: 0.00002515
Iteration 6/1000 | Loss: 0.00002429
Iteration 7/1000 | Loss: 0.00002360
Iteration 8/1000 | Loss: 0.00002305
Iteration 9/1000 | Loss: 0.00002276
Iteration 10/1000 | Loss: 0.00002247
Iteration 11/1000 | Loss: 0.00002226
Iteration 12/1000 | Loss: 0.00002210
Iteration 13/1000 | Loss: 0.00002205
Iteration 14/1000 | Loss: 0.00002205
Iteration 15/1000 | Loss: 0.00002198
Iteration 16/1000 | Loss: 0.00002194
Iteration 17/1000 | Loss: 0.00002192
Iteration 18/1000 | Loss: 0.00002190
Iteration 19/1000 | Loss: 0.00002184
Iteration 20/1000 | Loss: 0.00002179
Iteration 21/1000 | Loss: 0.00002177
Iteration 22/1000 | Loss: 0.00002176
Iteration 23/1000 | Loss: 0.00002174
Iteration 24/1000 | Loss: 0.00002171
Iteration 25/1000 | Loss: 0.00002170
Iteration 26/1000 | Loss: 0.00002170
Iteration 27/1000 | Loss: 0.00002170
Iteration 28/1000 | Loss: 0.00002170
Iteration 29/1000 | Loss: 0.00002170
Iteration 30/1000 | Loss: 0.00002170
Iteration 31/1000 | Loss: 0.00002170
Iteration 32/1000 | Loss: 0.00002169
Iteration 33/1000 | Loss: 0.00002169
Iteration 34/1000 | Loss: 0.00002169
Iteration 35/1000 | Loss: 0.00002169
Iteration 36/1000 | Loss: 0.00002168
Iteration 37/1000 | Loss: 0.00002168
Iteration 38/1000 | Loss: 0.00002167
Iteration 39/1000 | Loss: 0.00002166
Iteration 40/1000 | Loss: 0.00002166
Iteration 41/1000 | Loss: 0.00002166
Iteration 42/1000 | Loss: 0.00002166
Iteration 43/1000 | Loss: 0.00002165
Iteration 44/1000 | Loss: 0.00002165
Iteration 45/1000 | Loss: 0.00002165
Iteration 46/1000 | Loss: 0.00002165
Iteration 47/1000 | Loss: 0.00002165
Iteration 48/1000 | Loss: 0.00002165
Iteration 49/1000 | Loss: 0.00002165
Iteration 50/1000 | Loss: 0.00002165
Iteration 51/1000 | Loss: 0.00002165
Iteration 52/1000 | Loss: 0.00002164
Iteration 53/1000 | Loss: 0.00002164
Iteration 54/1000 | Loss: 0.00002163
Iteration 55/1000 | Loss: 0.00002163
Iteration 56/1000 | Loss: 0.00002162
Iteration 57/1000 | Loss: 0.00002162
Iteration 58/1000 | Loss: 0.00002162
Iteration 59/1000 | Loss: 0.00002161
Iteration 60/1000 | Loss: 0.00002161
Iteration 61/1000 | Loss: 0.00002161
Iteration 62/1000 | Loss: 0.00002160
Iteration 63/1000 | Loss: 0.00002160
Iteration 64/1000 | Loss: 0.00002159
Iteration 65/1000 | Loss: 0.00002159
Iteration 66/1000 | Loss: 0.00002159
Iteration 67/1000 | Loss: 0.00002159
Iteration 68/1000 | Loss: 0.00002159
Iteration 69/1000 | Loss: 0.00002158
Iteration 70/1000 | Loss: 0.00002158
Iteration 71/1000 | Loss: 0.00002158
Iteration 72/1000 | Loss: 0.00002157
Iteration 73/1000 | Loss: 0.00002157
Iteration 74/1000 | Loss: 0.00002157
Iteration 75/1000 | Loss: 0.00002157
Iteration 76/1000 | Loss: 0.00002156
Iteration 77/1000 | Loss: 0.00002156
Iteration 78/1000 | Loss: 0.00002156
Iteration 79/1000 | Loss: 0.00002156
Iteration 80/1000 | Loss: 0.00002156
Iteration 81/1000 | Loss: 0.00002156
Iteration 82/1000 | Loss: 0.00002155
Iteration 83/1000 | Loss: 0.00002155
Iteration 84/1000 | Loss: 0.00002155
Iteration 85/1000 | Loss: 0.00002155
Iteration 86/1000 | Loss: 0.00002155
Iteration 87/1000 | Loss: 0.00002155
Iteration 88/1000 | Loss: 0.00002155
Iteration 89/1000 | Loss: 0.00002155
Iteration 90/1000 | Loss: 0.00002155
Iteration 91/1000 | Loss: 0.00002154
Iteration 92/1000 | Loss: 0.00002154
Iteration 93/1000 | Loss: 0.00002154
Iteration 94/1000 | Loss: 0.00002154
Iteration 95/1000 | Loss: 0.00002154
Iteration 96/1000 | Loss: 0.00002154
Iteration 97/1000 | Loss: 0.00002154
Iteration 98/1000 | Loss: 0.00002154
Iteration 99/1000 | Loss: 0.00002154
Iteration 100/1000 | Loss: 0.00002154
Iteration 101/1000 | Loss: 0.00002153
Iteration 102/1000 | Loss: 0.00002153
Iteration 103/1000 | Loss: 0.00002153
Iteration 104/1000 | Loss: 0.00002153
Iteration 105/1000 | Loss: 0.00002153
Iteration 106/1000 | Loss: 0.00002153
Iteration 107/1000 | Loss: 0.00002153
Iteration 108/1000 | Loss: 0.00002153
Iteration 109/1000 | Loss: 0.00002153
Iteration 110/1000 | Loss: 0.00002153
Iteration 111/1000 | Loss: 0.00002153
Iteration 112/1000 | Loss: 0.00002153
Iteration 113/1000 | Loss: 0.00002153
Iteration 114/1000 | Loss: 0.00002153
Iteration 115/1000 | Loss: 0.00002153
Iteration 116/1000 | Loss: 0.00002153
Iteration 117/1000 | Loss: 0.00002153
Iteration 118/1000 | Loss: 0.00002152
Iteration 119/1000 | Loss: 0.00002152
Iteration 120/1000 | Loss: 0.00002152
Iteration 121/1000 | Loss: 0.00002152
Iteration 122/1000 | Loss: 0.00002152
Iteration 123/1000 | Loss: 0.00002152
Iteration 124/1000 | Loss: 0.00002152
Iteration 125/1000 | Loss: 0.00002152
Iteration 126/1000 | Loss: 0.00002152
Iteration 127/1000 | Loss: 0.00002152
Iteration 128/1000 | Loss: 0.00002152
Iteration 129/1000 | Loss: 0.00002152
Iteration 130/1000 | Loss: 0.00002152
Iteration 131/1000 | Loss: 0.00002152
Iteration 132/1000 | Loss: 0.00002152
Iteration 133/1000 | Loss: 0.00002152
Iteration 134/1000 | Loss: 0.00002152
Iteration 135/1000 | Loss: 0.00002152
Iteration 136/1000 | Loss: 0.00002152
Iteration 137/1000 | Loss: 0.00002152
Iteration 138/1000 | Loss: 0.00002152
Iteration 139/1000 | Loss: 0.00002152
Iteration 140/1000 | Loss: 0.00002152
Iteration 141/1000 | Loss: 0.00002151
Iteration 142/1000 | Loss: 0.00002151
Iteration 143/1000 | Loss: 0.00002151
Iteration 144/1000 | Loss: 0.00002151
Iteration 145/1000 | Loss: 0.00002151
Iteration 146/1000 | Loss: 0.00002151
Iteration 147/1000 | Loss: 0.00002151
Iteration 148/1000 | Loss: 0.00002151
Iteration 149/1000 | Loss: 0.00002151
Iteration 150/1000 | Loss: 0.00002151
Iteration 151/1000 | Loss: 0.00002151
Iteration 152/1000 | Loss: 0.00002151
Iteration 153/1000 | Loss: 0.00002151
Iteration 154/1000 | Loss: 0.00002151
Iteration 155/1000 | Loss: 0.00002151
Iteration 156/1000 | Loss: 0.00002151
Iteration 157/1000 | Loss: 0.00002151
Iteration 158/1000 | Loss: 0.00002151
Iteration 159/1000 | Loss: 0.00002151
Iteration 160/1000 | Loss: 0.00002151
Iteration 161/1000 | Loss: 0.00002151
Iteration 162/1000 | Loss: 0.00002151
Iteration 163/1000 | Loss: 0.00002151
Iteration 164/1000 | Loss: 0.00002151
Iteration 165/1000 | Loss: 0.00002151
Iteration 166/1000 | Loss: 0.00002151
Iteration 167/1000 | Loss: 0.00002151
Iteration 168/1000 | Loss: 0.00002151
Iteration 169/1000 | Loss: 0.00002151
Iteration 170/1000 | Loss: 0.00002151
Iteration 171/1000 | Loss: 0.00002151
Iteration 172/1000 | Loss: 0.00002151
Iteration 173/1000 | Loss: 0.00002151
Iteration 174/1000 | Loss: 0.00002151
Iteration 175/1000 | Loss: 0.00002151
Iteration 176/1000 | Loss: 0.00002151
Iteration 177/1000 | Loss: 0.00002151
Iteration 178/1000 | Loss: 0.00002151
Iteration 179/1000 | Loss: 0.00002151
Iteration 180/1000 | Loss: 0.00002151
Iteration 181/1000 | Loss: 0.00002151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.150988620996941e-05, 2.150988620996941e-05, 2.150988620996941e-05, 2.150988620996941e-05, 2.150988620996941e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.150988620996941e-05

Optimization complete. Final v2v error: 3.84458327293396 mm

Highest mean error: 5.111922740936279 mm for frame 173

Lowest mean error: 2.9549992084503174 mm for frame 124

Saving results

Total time: 47.309961795806885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827488
Iteration 2/25 | Loss: 0.00132836
Iteration 3/25 | Loss: 0.00083692
Iteration 4/25 | Loss: 0.00077715
Iteration 5/25 | Loss: 0.00075955
Iteration 6/25 | Loss: 0.00075488
Iteration 7/25 | Loss: 0.00075350
Iteration 8/25 | Loss: 0.00075320
Iteration 9/25 | Loss: 0.00075320
Iteration 10/25 | Loss: 0.00075320
Iteration 11/25 | Loss: 0.00075320
Iteration 12/25 | Loss: 0.00075320
Iteration 13/25 | Loss: 0.00075320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000753199215978384, 0.000753199215978384, 0.000753199215978384, 0.000753199215978384, 0.000753199215978384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000753199215978384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19507778
Iteration 2/25 | Loss: 0.00033124
Iteration 3/25 | Loss: 0.00033123
Iteration 4/25 | Loss: 0.00033123
Iteration 5/25 | Loss: 0.00033123
Iteration 6/25 | Loss: 0.00033123
Iteration 7/25 | Loss: 0.00033123
Iteration 8/25 | Loss: 0.00033123
Iteration 9/25 | Loss: 0.00033123
Iteration 10/25 | Loss: 0.00033123
Iteration 11/25 | Loss: 0.00033123
Iteration 12/25 | Loss: 0.00033123
Iteration 13/25 | Loss: 0.00033123
Iteration 14/25 | Loss: 0.00033123
Iteration 15/25 | Loss: 0.00033123
Iteration 16/25 | Loss: 0.00033123
Iteration 17/25 | Loss: 0.00033123
Iteration 18/25 | Loss: 0.00033123
Iteration 19/25 | Loss: 0.00033123
Iteration 20/25 | Loss: 0.00033123
Iteration 21/25 | Loss: 0.00033123
Iteration 22/25 | Loss: 0.00033123
Iteration 23/25 | Loss: 0.00033123
Iteration 24/25 | Loss: 0.00033123
Iteration 25/25 | Loss: 0.00033123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033123
Iteration 2/1000 | Loss: 0.00004964
Iteration 3/1000 | Loss: 0.00003978
Iteration 4/1000 | Loss: 0.00003328
Iteration 5/1000 | Loss: 0.00003152
Iteration 6/1000 | Loss: 0.00003018
Iteration 7/1000 | Loss: 0.00002931
Iteration 8/1000 | Loss: 0.00002853
Iteration 9/1000 | Loss: 0.00002805
Iteration 10/1000 | Loss: 0.00002762
Iteration 11/1000 | Loss: 0.00002725
Iteration 12/1000 | Loss: 0.00002695
Iteration 13/1000 | Loss: 0.00002666
Iteration 14/1000 | Loss: 0.00002642
Iteration 15/1000 | Loss: 0.00002621
Iteration 16/1000 | Loss: 0.00002618
Iteration 17/1000 | Loss: 0.00002609
Iteration 18/1000 | Loss: 0.00002608
Iteration 19/1000 | Loss: 0.00002607
Iteration 20/1000 | Loss: 0.00002606
Iteration 21/1000 | Loss: 0.00002603
Iteration 22/1000 | Loss: 0.00002597
Iteration 23/1000 | Loss: 0.00002593
Iteration 24/1000 | Loss: 0.00002589
Iteration 25/1000 | Loss: 0.00002588
Iteration 26/1000 | Loss: 0.00002587
Iteration 27/1000 | Loss: 0.00002586
Iteration 28/1000 | Loss: 0.00002586
Iteration 29/1000 | Loss: 0.00002586
Iteration 30/1000 | Loss: 0.00002585
Iteration 31/1000 | Loss: 0.00002585
Iteration 32/1000 | Loss: 0.00002585
Iteration 33/1000 | Loss: 0.00002585
Iteration 34/1000 | Loss: 0.00002585
Iteration 35/1000 | Loss: 0.00002584
Iteration 36/1000 | Loss: 0.00002584
Iteration 37/1000 | Loss: 0.00002584
Iteration 38/1000 | Loss: 0.00002584
Iteration 39/1000 | Loss: 0.00002584
Iteration 40/1000 | Loss: 0.00002583
Iteration 41/1000 | Loss: 0.00002582
Iteration 42/1000 | Loss: 0.00002582
Iteration 43/1000 | Loss: 0.00002582
Iteration 44/1000 | Loss: 0.00002581
Iteration 45/1000 | Loss: 0.00002581
Iteration 46/1000 | Loss: 0.00002581
Iteration 47/1000 | Loss: 0.00002580
Iteration 48/1000 | Loss: 0.00002580
Iteration 49/1000 | Loss: 0.00002580
Iteration 50/1000 | Loss: 0.00002579
Iteration 51/1000 | Loss: 0.00002579
Iteration 52/1000 | Loss: 0.00002579
Iteration 53/1000 | Loss: 0.00002578
Iteration 54/1000 | Loss: 0.00002578
Iteration 55/1000 | Loss: 0.00002578
Iteration 56/1000 | Loss: 0.00002577
Iteration 57/1000 | Loss: 0.00002577
Iteration 58/1000 | Loss: 0.00002577
Iteration 59/1000 | Loss: 0.00002576
Iteration 60/1000 | Loss: 0.00002576
Iteration 61/1000 | Loss: 0.00002576
Iteration 62/1000 | Loss: 0.00002576
Iteration 63/1000 | Loss: 0.00002575
Iteration 64/1000 | Loss: 0.00002575
Iteration 65/1000 | Loss: 0.00002574
Iteration 66/1000 | Loss: 0.00002574
Iteration 67/1000 | Loss: 0.00002574
Iteration 68/1000 | Loss: 0.00002574
Iteration 69/1000 | Loss: 0.00002573
Iteration 70/1000 | Loss: 0.00002573
Iteration 71/1000 | Loss: 0.00002573
Iteration 72/1000 | Loss: 0.00002572
Iteration 73/1000 | Loss: 0.00002572
Iteration 74/1000 | Loss: 0.00002572
Iteration 75/1000 | Loss: 0.00002571
Iteration 76/1000 | Loss: 0.00002571
Iteration 77/1000 | Loss: 0.00002571
Iteration 78/1000 | Loss: 0.00002571
Iteration 79/1000 | Loss: 0.00002571
Iteration 80/1000 | Loss: 0.00002570
Iteration 81/1000 | Loss: 0.00002570
Iteration 82/1000 | Loss: 0.00002570
Iteration 83/1000 | Loss: 0.00002570
Iteration 84/1000 | Loss: 0.00002570
Iteration 85/1000 | Loss: 0.00002569
Iteration 86/1000 | Loss: 0.00002569
Iteration 87/1000 | Loss: 0.00002569
Iteration 88/1000 | Loss: 0.00002569
Iteration 89/1000 | Loss: 0.00002569
Iteration 90/1000 | Loss: 0.00002569
Iteration 91/1000 | Loss: 0.00002569
Iteration 92/1000 | Loss: 0.00002569
Iteration 93/1000 | Loss: 0.00002569
Iteration 94/1000 | Loss: 0.00002568
Iteration 95/1000 | Loss: 0.00002568
Iteration 96/1000 | Loss: 0.00002568
Iteration 97/1000 | Loss: 0.00002568
Iteration 98/1000 | Loss: 0.00002568
Iteration 99/1000 | Loss: 0.00002568
Iteration 100/1000 | Loss: 0.00002568
Iteration 101/1000 | Loss: 0.00002568
Iteration 102/1000 | Loss: 0.00002568
Iteration 103/1000 | Loss: 0.00002567
Iteration 104/1000 | Loss: 0.00002567
Iteration 105/1000 | Loss: 0.00002567
Iteration 106/1000 | Loss: 0.00002567
Iteration 107/1000 | Loss: 0.00002567
Iteration 108/1000 | Loss: 0.00002566
Iteration 109/1000 | Loss: 0.00002566
Iteration 110/1000 | Loss: 0.00002566
Iteration 111/1000 | Loss: 0.00002566
Iteration 112/1000 | Loss: 0.00002566
Iteration 113/1000 | Loss: 0.00002566
Iteration 114/1000 | Loss: 0.00002566
Iteration 115/1000 | Loss: 0.00002566
Iteration 116/1000 | Loss: 0.00002566
Iteration 117/1000 | Loss: 0.00002566
Iteration 118/1000 | Loss: 0.00002566
Iteration 119/1000 | Loss: 0.00002566
Iteration 120/1000 | Loss: 0.00002565
Iteration 121/1000 | Loss: 0.00002565
Iteration 122/1000 | Loss: 0.00002565
Iteration 123/1000 | Loss: 0.00002565
Iteration 124/1000 | Loss: 0.00002565
Iteration 125/1000 | Loss: 0.00002565
Iteration 126/1000 | Loss: 0.00002565
Iteration 127/1000 | Loss: 0.00002565
Iteration 128/1000 | Loss: 0.00002565
Iteration 129/1000 | Loss: 0.00002565
Iteration 130/1000 | Loss: 0.00002565
Iteration 131/1000 | Loss: 0.00002565
Iteration 132/1000 | Loss: 0.00002564
Iteration 133/1000 | Loss: 0.00002564
Iteration 134/1000 | Loss: 0.00002564
Iteration 135/1000 | Loss: 0.00002564
Iteration 136/1000 | Loss: 0.00002564
Iteration 137/1000 | Loss: 0.00002564
Iteration 138/1000 | Loss: 0.00002564
Iteration 139/1000 | Loss: 0.00002564
Iteration 140/1000 | Loss: 0.00002564
Iteration 141/1000 | Loss: 0.00002564
Iteration 142/1000 | Loss: 0.00002564
Iteration 143/1000 | Loss: 0.00002564
Iteration 144/1000 | Loss: 0.00002563
Iteration 145/1000 | Loss: 0.00002563
Iteration 146/1000 | Loss: 0.00002563
Iteration 147/1000 | Loss: 0.00002563
Iteration 148/1000 | Loss: 0.00002563
Iteration 149/1000 | Loss: 0.00002563
Iteration 150/1000 | Loss: 0.00002563
Iteration 151/1000 | Loss: 0.00002563
Iteration 152/1000 | Loss: 0.00002563
Iteration 153/1000 | Loss: 0.00002563
Iteration 154/1000 | Loss: 0.00002563
Iteration 155/1000 | Loss: 0.00002563
Iteration 156/1000 | Loss: 0.00002563
Iteration 157/1000 | Loss: 0.00002563
Iteration 158/1000 | Loss: 0.00002562
Iteration 159/1000 | Loss: 0.00002562
Iteration 160/1000 | Loss: 0.00002562
Iteration 161/1000 | Loss: 0.00002562
Iteration 162/1000 | Loss: 0.00002562
Iteration 163/1000 | Loss: 0.00002562
Iteration 164/1000 | Loss: 0.00002562
Iteration 165/1000 | Loss: 0.00002562
Iteration 166/1000 | Loss: 0.00002562
Iteration 167/1000 | Loss: 0.00002561
Iteration 168/1000 | Loss: 0.00002561
Iteration 169/1000 | Loss: 0.00002561
Iteration 170/1000 | Loss: 0.00002561
Iteration 171/1000 | Loss: 0.00002561
Iteration 172/1000 | Loss: 0.00002561
Iteration 173/1000 | Loss: 0.00002561
Iteration 174/1000 | Loss: 0.00002561
Iteration 175/1000 | Loss: 0.00002561
Iteration 176/1000 | Loss: 0.00002561
Iteration 177/1000 | Loss: 0.00002561
Iteration 178/1000 | Loss: 0.00002561
Iteration 179/1000 | Loss: 0.00002561
Iteration 180/1000 | Loss: 0.00002561
Iteration 181/1000 | Loss: 0.00002561
Iteration 182/1000 | Loss: 0.00002560
Iteration 183/1000 | Loss: 0.00002560
Iteration 184/1000 | Loss: 0.00002560
Iteration 185/1000 | Loss: 0.00002560
Iteration 186/1000 | Loss: 0.00002560
Iteration 187/1000 | Loss: 0.00002560
Iteration 188/1000 | Loss: 0.00002560
Iteration 189/1000 | Loss: 0.00002560
Iteration 190/1000 | Loss: 0.00002560
Iteration 191/1000 | Loss: 0.00002560
Iteration 192/1000 | Loss: 0.00002559
Iteration 193/1000 | Loss: 0.00002559
Iteration 194/1000 | Loss: 0.00002559
Iteration 195/1000 | Loss: 0.00002559
Iteration 196/1000 | Loss: 0.00002559
Iteration 197/1000 | Loss: 0.00002559
Iteration 198/1000 | Loss: 0.00002559
Iteration 199/1000 | Loss: 0.00002559
Iteration 200/1000 | Loss: 0.00002559
Iteration 201/1000 | Loss: 0.00002559
Iteration 202/1000 | Loss: 0.00002559
Iteration 203/1000 | Loss: 0.00002559
Iteration 204/1000 | Loss: 0.00002559
Iteration 205/1000 | Loss: 0.00002558
Iteration 206/1000 | Loss: 0.00002558
Iteration 207/1000 | Loss: 0.00002558
Iteration 208/1000 | Loss: 0.00002558
Iteration 209/1000 | Loss: 0.00002558
Iteration 210/1000 | Loss: 0.00002558
Iteration 211/1000 | Loss: 0.00002558
Iteration 212/1000 | Loss: 0.00002558
Iteration 213/1000 | Loss: 0.00002558
Iteration 214/1000 | Loss: 0.00002558
Iteration 215/1000 | Loss: 0.00002558
Iteration 216/1000 | Loss: 0.00002558
Iteration 217/1000 | Loss: 0.00002558
Iteration 218/1000 | Loss: 0.00002558
Iteration 219/1000 | Loss: 0.00002558
Iteration 220/1000 | Loss: 0.00002558
Iteration 221/1000 | Loss: 0.00002558
Iteration 222/1000 | Loss: 0.00002557
Iteration 223/1000 | Loss: 0.00002557
Iteration 224/1000 | Loss: 0.00002557
Iteration 225/1000 | Loss: 0.00002557
Iteration 226/1000 | Loss: 0.00002557
Iteration 227/1000 | Loss: 0.00002557
Iteration 228/1000 | Loss: 0.00002557
Iteration 229/1000 | Loss: 0.00002557
Iteration 230/1000 | Loss: 0.00002557
Iteration 231/1000 | Loss: 0.00002557
Iteration 232/1000 | Loss: 0.00002557
Iteration 233/1000 | Loss: 0.00002557
Iteration 234/1000 | Loss: 0.00002557
Iteration 235/1000 | Loss: 0.00002557
Iteration 236/1000 | Loss: 0.00002557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [2.556917752372101e-05, 2.556917752372101e-05, 2.556917752372101e-05, 2.556917752372101e-05, 2.556917752372101e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.556917752372101e-05

Optimization complete. Final v2v error: 4.110376834869385 mm

Highest mean error: 5.52044153213501 mm for frame 159

Lowest mean error: 3.3126957416534424 mm for frame 203

Saving results

Total time: 52.320008754730225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794304
Iteration 2/25 | Loss: 0.00108523
Iteration 3/25 | Loss: 0.00077485
Iteration 4/25 | Loss: 0.00073316
Iteration 5/25 | Loss: 0.00066591
Iteration 6/25 | Loss: 0.00065611
Iteration 7/25 | Loss: 0.00065725
Iteration 8/25 | Loss: 0.00065186
Iteration 9/25 | Loss: 0.00065146
Iteration 10/25 | Loss: 0.00065123
Iteration 11/25 | Loss: 0.00065116
Iteration 12/25 | Loss: 0.00065115
Iteration 13/25 | Loss: 0.00065115
Iteration 14/25 | Loss: 0.00065115
Iteration 15/25 | Loss: 0.00065115
Iteration 16/25 | Loss: 0.00065115
Iteration 17/25 | Loss: 0.00065115
Iteration 18/25 | Loss: 0.00065115
Iteration 19/25 | Loss: 0.00065115
Iteration 20/25 | Loss: 0.00065115
Iteration 21/25 | Loss: 0.00065115
Iteration 22/25 | Loss: 0.00065115
Iteration 23/25 | Loss: 0.00065115
Iteration 24/25 | Loss: 0.00065115
Iteration 25/25 | Loss: 0.00065115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.53673553
Iteration 2/25 | Loss: 0.00039145
Iteration 3/25 | Loss: 0.00039142
Iteration 4/25 | Loss: 0.00039142
Iteration 5/25 | Loss: 0.00039142
Iteration 6/25 | Loss: 0.00039142
Iteration 7/25 | Loss: 0.00039142
Iteration 8/25 | Loss: 0.00039142
Iteration 9/25 | Loss: 0.00039142
Iteration 10/25 | Loss: 0.00039142
Iteration 11/25 | Loss: 0.00039142
Iteration 12/25 | Loss: 0.00039142
Iteration 13/25 | Loss: 0.00039142
Iteration 14/25 | Loss: 0.00039142
Iteration 15/25 | Loss: 0.00039142
Iteration 16/25 | Loss: 0.00039142
Iteration 17/25 | Loss: 0.00039142
Iteration 18/25 | Loss: 0.00039142
Iteration 19/25 | Loss: 0.00039142
Iteration 20/25 | Loss: 0.00039142
Iteration 21/25 | Loss: 0.00039142
Iteration 22/25 | Loss: 0.00039142
Iteration 23/25 | Loss: 0.00039142
Iteration 24/25 | Loss: 0.00039142
Iteration 25/25 | Loss: 0.00039142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039142
Iteration 2/1000 | Loss: 0.00003525
Iteration 3/1000 | Loss: 0.00002039
Iteration 4/1000 | Loss: 0.00001797
Iteration 5/1000 | Loss: 0.00001716
Iteration 6/1000 | Loss: 0.00001631
Iteration 7/1000 | Loss: 0.00001593
Iteration 8/1000 | Loss: 0.00001559
Iteration 9/1000 | Loss: 0.00001532
Iteration 10/1000 | Loss: 0.00001512
Iteration 11/1000 | Loss: 0.00001502
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001494
Iteration 14/1000 | Loss: 0.00001492
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00001482
Iteration 17/1000 | Loss: 0.00001481
Iteration 18/1000 | Loss: 0.00001481
Iteration 19/1000 | Loss: 0.00001480
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001477
Iteration 24/1000 | Loss: 0.00001476
Iteration 25/1000 | Loss: 0.00001476
Iteration 26/1000 | Loss: 0.00001476
Iteration 27/1000 | Loss: 0.00001476
Iteration 28/1000 | Loss: 0.00001470
Iteration 29/1000 | Loss: 0.00001469
Iteration 30/1000 | Loss: 0.00001468
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001467
Iteration 34/1000 | Loss: 0.00001466
Iteration 35/1000 | Loss: 0.00001466
Iteration 36/1000 | Loss: 0.00001466
Iteration 37/1000 | Loss: 0.00001466
Iteration 38/1000 | Loss: 0.00001465
Iteration 39/1000 | Loss: 0.00001465
Iteration 40/1000 | Loss: 0.00001465
Iteration 41/1000 | Loss: 0.00001464
Iteration 42/1000 | Loss: 0.00001464
Iteration 43/1000 | Loss: 0.00001464
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001464
Iteration 47/1000 | Loss: 0.00001463
Iteration 48/1000 | Loss: 0.00001463
Iteration 49/1000 | Loss: 0.00001463
Iteration 50/1000 | Loss: 0.00001463
Iteration 51/1000 | Loss: 0.00001463
Iteration 52/1000 | Loss: 0.00001463
Iteration 53/1000 | Loss: 0.00001462
Iteration 54/1000 | Loss: 0.00001462
Iteration 55/1000 | Loss: 0.00001462
Iteration 56/1000 | Loss: 0.00001462
Iteration 57/1000 | Loss: 0.00001462
Iteration 58/1000 | Loss: 0.00001461
Iteration 59/1000 | Loss: 0.00001461
Iteration 60/1000 | Loss: 0.00001461
Iteration 61/1000 | Loss: 0.00001460
Iteration 62/1000 | Loss: 0.00001460
Iteration 63/1000 | Loss: 0.00001460
Iteration 64/1000 | Loss: 0.00001459
Iteration 65/1000 | Loss: 0.00001459
Iteration 66/1000 | Loss: 0.00001459
Iteration 67/1000 | Loss: 0.00001459
Iteration 68/1000 | Loss: 0.00001459
Iteration 69/1000 | Loss: 0.00001459
Iteration 70/1000 | Loss: 0.00001459
Iteration 71/1000 | Loss: 0.00001459
Iteration 72/1000 | Loss: 0.00001458
Iteration 73/1000 | Loss: 0.00001458
Iteration 74/1000 | Loss: 0.00001458
Iteration 75/1000 | Loss: 0.00001458
Iteration 76/1000 | Loss: 0.00001458
Iteration 77/1000 | Loss: 0.00001458
Iteration 78/1000 | Loss: 0.00001457
Iteration 79/1000 | Loss: 0.00001457
Iteration 80/1000 | Loss: 0.00001457
Iteration 81/1000 | Loss: 0.00001457
Iteration 82/1000 | Loss: 0.00001456
Iteration 83/1000 | Loss: 0.00001456
Iteration 84/1000 | Loss: 0.00001456
Iteration 85/1000 | Loss: 0.00001456
Iteration 86/1000 | Loss: 0.00001455
Iteration 87/1000 | Loss: 0.00001455
Iteration 88/1000 | Loss: 0.00001455
Iteration 89/1000 | Loss: 0.00001455
Iteration 90/1000 | Loss: 0.00001455
Iteration 91/1000 | Loss: 0.00001454
Iteration 92/1000 | Loss: 0.00001454
Iteration 93/1000 | Loss: 0.00001454
Iteration 94/1000 | Loss: 0.00001454
Iteration 95/1000 | Loss: 0.00001454
Iteration 96/1000 | Loss: 0.00001454
Iteration 97/1000 | Loss: 0.00001453
Iteration 98/1000 | Loss: 0.00001453
Iteration 99/1000 | Loss: 0.00001453
Iteration 100/1000 | Loss: 0.00001453
Iteration 101/1000 | Loss: 0.00001453
Iteration 102/1000 | Loss: 0.00001453
Iteration 103/1000 | Loss: 0.00001453
Iteration 104/1000 | Loss: 0.00001453
Iteration 105/1000 | Loss: 0.00001453
Iteration 106/1000 | Loss: 0.00001452
Iteration 107/1000 | Loss: 0.00001452
Iteration 108/1000 | Loss: 0.00001452
Iteration 109/1000 | Loss: 0.00001452
Iteration 110/1000 | Loss: 0.00001451
Iteration 111/1000 | Loss: 0.00001451
Iteration 112/1000 | Loss: 0.00001451
Iteration 113/1000 | Loss: 0.00001451
Iteration 114/1000 | Loss: 0.00001451
Iteration 115/1000 | Loss: 0.00001451
Iteration 116/1000 | Loss: 0.00001451
Iteration 117/1000 | Loss: 0.00001451
Iteration 118/1000 | Loss: 0.00001451
Iteration 119/1000 | Loss: 0.00001451
Iteration 120/1000 | Loss: 0.00001451
Iteration 121/1000 | Loss: 0.00001450
Iteration 122/1000 | Loss: 0.00001450
Iteration 123/1000 | Loss: 0.00001450
Iteration 124/1000 | Loss: 0.00001450
Iteration 125/1000 | Loss: 0.00001450
Iteration 126/1000 | Loss: 0.00001450
Iteration 127/1000 | Loss: 0.00001449
Iteration 128/1000 | Loss: 0.00001449
Iteration 129/1000 | Loss: 0.00001449
Iteration 130/1000 | Loss: 0.00001449
Iteration 131/1000 | Loss: 0.00001449
Iteration 132/1000 | Loss: 0.00001449
Iteration 133/1000 | Loss: 0.00001449
Iteration 134/1000 | Loss: 0.00001449
Iteration 135/1000 | Loss: 0.00001449
Iteration 136/1000 | Loss: 0.00001449
Iteration 137/1000 | Loss: 0.00001449
Iteration 138/1000 | Loss: 0.00001448
Iteration 139/1000 | Loss: 0.00001448
Iteration 140/1000 | Loss: 0.00001448
Iteration 141/1000 | Loss: 0.00001448
Iteration 142/1000 | Loss: 0.00001448
Iteration 143/1000 | Loss: 0.00001448
Iteration 144/1000 | Loss: 0.00001448
Iteration 145/1000 | Loss: 0.00001448
Iteration 146/1000 | Loss: 0.00001448
Iteration 147/1000 | Loss: 0.00001448
Iteration 148/1000 | Loss: 0.00001448
Iteration 149/1000 | Loss: 0.00001448
Iteration 150/1000 | Loss: 0.00001448
Iteration 151/1000 | Loss: 0.00001448
Iteration 152/1000 | Loss: 0.00001447
Iteration 153/1000 | Loss: 0.00001447
Iteration 154/1000 | Loss: 0.00001447
Iteration 155/1000 | Loss: 0.00001447
Iteration 156/1000 | Loss: 0.00001447
Iteration 157/1000 | Loss: 0.00001447
Iteration 158/1000 | Loss: 0.00001447
Iteration 159/1000 | Loss: 0.00001447
Iteration 160/1000 | Loss: 0.00001447
Iteration 161/1000 | Loss: 0.00001447
Iteration 162/1000 | Loss: 0.00001447
Iteration 163/1000 | Loss: 0.00001447
Iteration 164/1000 | Loss: 0.00001447
Iteration 165/1000 | Loss: 0.00001447
Iteration 166/1000 | Loss: 0.00001447
Iteration 167/1000 | Loss: 0.00001446
Iteration 168/1000 | Loss: 0.00001446
Iteration 169/1000 | Loss: 0.00001446
Iteration 170/1000 | Loss: 0.00001446
Iteration 171/1000 | Loss: 0.00001446
Iteration 172/1000 | Loss: 0.00001446
Iteration 173/1000 | Loss: 0.00001446
Iteration 174/1000 | Loss: 0.00001446
Iteration 175/1000 | Loss: 0.00001446
Iteration 176/1000 | Loss: 0.00001446
Iteration 177/1000 | Loss: 0.00001446
Iteration 178/1000 | Loss: 0.00001446
Iteration 179/1000 | Loss: 0.00001446
Iteration 180/1000 | Loss: 0.00001445
Iteration 181/1000 | Loss: 0.00001445
Iteration 182/1000 | Loss: 0.00001445
Iteration 183/1000 | Loss: 0.00001445
Iteration 184/1000 | Loss: 0.00001445
Iteration 185/1000 | Loss: 0.00001445
Iteration 186/1000 | Loss: 0.00001444
Iteration 187/1000 | Loss: 0.00001444
Iteration 188/1000 | Loss: 0.00001444
Iteration 189/1000 | Loss: 0.00001444
Iteration 190/1000 | Loss: 0.00001444
Iteration 191/1000 | Loss: 0.00001444
Iteration 192/1000 | Loss: 0.00001444
Iteration 193/1000 | Loss: 0.00001444
Iteration 194/1000 | Loss: 0.00001444
Iteration 195/1000 | Loss: 0.00001444
Iteration 196/1000 | Loss: 0.00001444
Iteration 197/1000 | Loss: 0.00001443
Iteration 198/1000 | Loss: 0.00001443
Iteration 199/1000 | Loss: 0.00001443
Iteration 200/1000 | Loss: 0.00001443
Iteration 201/1000 | Loss: 0.00001443
Iteration 202/1000 | Loss: 0.00001443
Iteration 203/1000 | Loss: 0.00001443
Iteration 204/1000 | Loss: 0.00001443
Iteration 205/1000 | Loss: 0.00001443
Iteration 206/1000 | Loss: 0.00001443
Iteration 207/1000 | Loss: 0.00001443
Iteration 208/1000 | Loss: 0.00001443
Iteration 209/1000 | Loss: 0.00001443
Iteration 210/1000 | Loss: 0.00001443
Iteration 211/1000 | Loss: 0.00001443
Iteration 212/1000 | Loss: 0.00001443
Iteration 213/1000 | Loss: 0.00001442
Iteration 214/1000 | Loss: 0.00001442
Iteration 215/1000 | Loss: 0.00001442
Iteration 216/1000 | Loss: 0.00001442
Iteration 217/1000 | Loss: 0.00001442
Iteration 218/1000 | Loss: 0.00001442
Iteration 219/1000 | Loss: 0.00001442
Iteration 220/1000 | Loss: 0.00001442
Iteration 221/1000 | Loss: 0.00001442
Iteration 222/1000 | Loss: 0.00001442
Iteration 223/1000 | Loss: 0.00001442
Iteration 224/1000 | Loss: 0.00001442
Iteration 225/1000 | Loss: 0.00001442
Iteration 226/1000 | Loss: 0.00001442
Iteration 227/1000 | Loss: 0.00001442
Iteration 228/1000 | Loss: 0.00001442
Iteration 229/1000 | Loss: 0.00001442
Iteration 230/1000 | Loss: 0.00001442
Iteration 231/1000 | Loss: 0.00001442
Iteration 232/1000 | Loss: 0.00001442
Iteration 233/1000 | Loss: 0.00001442
Iteration 234/1000 | Loss: 0.00001442
Iteration 235/1000 | Loss: 0.00001442
Iteration 236/1000 | Loss: 0.00001442
Iteration 237/1000 | Loss: 0.00001442
Iteration 238/1000 | Loss: 0.00001442
Iteration 239/1000 | Loss: 0.00001442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.4416244994208682e-05, 1.4416244994208682e-05, 1.4416244994208682e-05, 1.4416244994208682e-05, 1.4416244994208682e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4416244994208682e-05

Optimization complete. Final v2v error: 3.2015974521636963 mm

Highest mean error: 3.579902172088623 mm for frame 33

Lowest mean error: 2.79095196723938 mm for frame 121

Saving results

Total time: 52.375858783721924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996523
Iteration 2/25 | Loss: 0.00369401
Iteration 3/25 | Loss: 0.00199981
Iteration 4/25 | Loss: 0.00183154
Iteration 5/25 | Loss: 0.00155987
Iteration 6/25 | Loss: 0.00160491
Iteration 7/25 | Loss: 0.00140013
Iteration 8/25 | Loss: 0.00117255
Iteration 9/25 | Loss: 0.00106473
Iteration 10/25 | Loss: 0.00101555
Iteration 11/25 | Loss: 0.00098966
Iteration 12/25 | Loss: 0.00094337
Iteration 13/25 | Loss: 0.00093646
Iteration 14/25 | Loss: 0.00092939
Iteration 15/25 | Loss: 0.00092580
Iteration 16/25 | Loss: 0.00091656
Iteration 17/25 | Loss: 0.00091422
Iteration 18/25 | Loss: 0.00091296
Iteration 19/25 | Loss: 0.00091235
Iteration 20/25 | Loss: 0.00091175
Iteration 21/25 | Loss: 0.00091122
Iteration 22/25 | Loss: 0.00094999
Iteration 23/25 | Loss: 0.00094967
Iteration 24/25 | Loss: 0.00089775
Iteration 25/25 | Loss: 0.00083028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46571231
Iteration 2/25 | Loss: 0.00078226
Iteration 3/25 | Loss: 0.00078225
Iteration 4/25 | Loss: 0.00078225
Iteration 5/25 | Loss: 0.00078225
Iteration 6/25 | Loss: 0.00070384
Iteration 7/25 | Loss: 0.00070384
Iteration 8/25 | Loss: 0.00070384
Iteration 9/25 | Loss: 0.00070383
Iteration 10/25 | Loss: 0.00070383
Iteration 11/25 | Loss: 0.00070383
Iteration 12/25 | Loss: 0.00070383
Iteration 13/25 | Loss: 0.00070383
Iteration 14/25 | Loss: 0.00070383
Iteration 15/25 | Loss: 0.00070383
Iteration 16/25 | Loss: 0.00070383
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007038334151729941, 0.0007038334151729941, 0.0007038334151729941, 0.0007038334151729941, 0.0007038334151729941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007038334151729941

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070383
Iteration 2/1000 | Loss: 0.00057847
Iteration 3/1000 | Loss: 0.00038836
Iteration 4/1000 | Loss: 0.00011161
Iteration 5/1000 | Loss: 0.00020397
Iteration 6/1000 | Loss: 0.00005078
Iteration 7/1000 | Loss: 0.00023728
Iteration 8/1000 | Loss: 0.00006329
Iteration 9/1000 | Loss: 0.00052339
Iteration 10/1000 | Loss: 0.00005396
Iteration 11/1000 | Loss: 0.00017652
Iteration 12/1000 | Loss: 0.00016152
Iteration 13/1000 | Loss: 0.00033681
Iteration 14/1000 | Loss: 0.00081585
Iteration 15/1000 | Loss: 0.00054589
Iteration 16/1000 | Loss: 0.00006624
Iteration 17/1000 | Loss: 0.00004754
Iteration 18/1000 | Loss: 0.00007727
Iteration 19/1000 | Loss: 0.00003476
Iteration 20/1000 | Loss: 0.00002798
Iteration 21/1000 | Loss: 0.00008167
Iteration 22/1000 | Loss: 0.00002274
Iteration 23/1000 | Loss: 0.00004004
Iteration 24/1000 | Loss: 0.00002197
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001857
Iteration 27/1000 | Loss: 0.00001818
Iteration 28/1000 | Loss: 0.00001792
Iteration 29/1000 | Loss: 0.00001779
Iteration 30/1000 | Loss: 0.00001762
Iteration 31/1000 | Loss: 0.00001755
Iteration 32/1000 | Loss: 0.00003537
Iteration 33/1000 | Loss: 0.00002483
Iteration 34/1000 | Loss: 0.00003456
Iteration 35/1000 | Loss: 0.00001963
Iteration 36/1000 | Loss: 0.00002333
Iteration 37/1000 | Loss: 0.00003380
Iteration 38/1000 | Loss: 0.00001903
Iteration 39/1000 | Loss: 0.00001790
Iteration 40/1000 | Loss: 0.00001911
Iteration 41/1000 | Loss: 0.00002891
Iteration 42/1000 | Loss: 0.00001999
Iteration 43/1000 | Loss: 0.00002998
Iteration 44/1000 | Loss: 0.00002050
Iteration 45/1000 | Loss: 0.00003042
Iteration 46/1000 | Loss: 0.00002091
Iteration 47/1000 | Loss: 0.00001774
Iteration 48/1000 | Loss: 0.00001737
Iteration 49/1000 | Loss: 0.00001734
Iteration 50/1000 | Loss: 0.00001733
Iteration 51/1000 | Loss: 0.00001732
Iteration 52/1000 | Loss: 0.00001731
Iteration 53/1000 | Loss: 0.00001730
Iteration 54/1000 | Loss: 0.00001730
Iteration 55/1000 | Loss: 0.00001730
Iteration 56/1000 | Loss: 0.00001729
Iteration 57/1000 | Loss: 0.00001729
Iteration 58/1000 | Loss: 0.00001729
Iteration 59/1000 | Loss: 0.00001729
Iteration 60/1000 | Loss: 0.00001729
Iteration 61/1000 | Loss: 0.00001729
Iteration 62/1000 | Loss: 0.00001729
Iteration 63/1000 | Loss: 0.00001729
Iteration 64/1000 | Loss: 0.00001729
Iteration 65/1000 | Loss: 0.00001728
Iteration 66/1000 | Loss: 0.00003129
Iteration 67/1000 | Loss: 0.00003129
Iteration 68/1000 | Loss: 0.00003129
Iteration 69/1000 | Loss: 0.00002393
Iteration 70/1000 | Loss: 0.00003248
Iteration 71/1000 | Loss: 0.00001730
Iteration 72/1000 | Loss: 0.00001729
Iteration 73/1000 | Loss: 0.00001729
Iteration 74/1000 | Loss: 0.00001729
Iteration 75/1000 | Loss: 0.00001729
Iteration 76/1000 | Loss: 0.00001729
Iteration 77/1000 | Loss: 0.00001729
Iteration 78/1000 | Loss: 0.00001729
Iteration 79/1000 | Loss: 0.00001729
Iteration 80/1000 | Loss: 0.00001729
Iteration 81/1000 | Loss: 0.00001728
Iteration 82/1000 | Loss: 0.00001728
Iteration 83/1000 | Loss: 0.00001728
Iteration 84/1000 | Loss: 0.00001728
Iteration 85/1000 | Loss: 0.00001727
Iteration 86/1000 | Loss: 0.00001727
Iteration 87/1000 | Loss: 0.00001727
Iteration 88/1000 | Loss: 0.00001727
Iteration 89/1000 | Loss: 0.00001727
Iteration 90/1000 | Loss: 0.00001727
Iteration 91/1000 | Loss: 0.00001727
Iteration 92/1000 | Loss: 0.00001727
Iteration 93/1000 | Loss: 0.00001727
Iteration 94/1000 | Loss: 0.00001727
Iteration 95/1000 | Loss: 0.00001727
Iteration 96/1000 | Loss: 0.00001727
Iteration 97/1000 | Loss: 0.00001727
Iteration 98/1000 | Loss: 0.00001727
Iteration 99/1000 | Loss: 0.00001727
Iteration 100/1000 | Loss: 0.00001727
Iteration 101/1000 | Loss: 0.00001727
Iteration 102/1000 | Loss: 0.00001727
Iteration 103/1000 | Loss: 0.00001727
Iteration 104/1000 | Loss: 0.00001727
Iteration 105/1000 | Loss: 0.00001727
Iteration 106/1000 | Loss: 0.00001727
Iteration 107/1000 | Loss: 0.00001727
Iteration 108/1000 | Loss: 0.00001727
Iteration 109/1000 | Loss: 0.00001727
Iteration 110/1000 | Loss: 0.00001727
Iteration 111/1000 | Loss: 0.00001727
Iteration 112/1000 | Loss: 0.00001727
Iteration 113/1000 | Loss: 0.00001727
Iteration 114/1000 | Loss: 0.00001727
Iteration 115/1000 | Loss: 0.00001727
Iteration 116/1000 | Loss: 0.00001727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.7269014279008843e-05, 1.7269014279008843e-05, 1.7269014279008843e-05, 1.7269014279008843e-05, 1.7269014279008843e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7269014279008843e-05

Optimization complete. Final v2v error: 3.484103202819824 mm

Highest mean error: 10.025299072265625 mm for frame 14

Lowest mean error: 3.1604180335998535 mm for frame 102

Saving results

Total time: 121.28401923179626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544131
Iteration 2/25 | Loss: 0.00106190
Iteration 3/25 | Loss: 0.00076842
Iteration 4/25 | Loss: 0.00072077
Iteration 5/25 | Loss: 0.00070539
Iteration 6/25 | Loss: 0.00070169
Iteration 7/25 | Loss: 0.00070070
Iteration 8/25 | Loss: 0.00070042
Iteration 9/25 | Loss: 0.00070032
Iteration 10/25 | Loss: 0.00070032
Iteration 11/25 | Loss: 0.00070032
Iteration 12/25 | Loss: 0.00070032
Iteration 13/25 | Loss: 0.00070032
Iteration 14/25 | Loss: 0.00070032
Iteration 15/25 | Loss: 0.00070032
Iteration 16/25 | Loss: 0.00070032
Iteration 17/25 | Loss: 0.00070032
Iteration 18/25 | Loss: 0.00070032
Iteration 19/25 | Loss: 0.00070032
Iteration 20/25 | Loss: 0.00070032
Iteration 21/25 | Loss: 0.00070032
Iteration 22/25 | Loss: 0.00070032
Iteration 23/25 | Loss: 0.00070032
Iteration 24/25 | Loss: 0.00070032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007003220380283892, 0.0007003220380283892, 0.0007003220380283892, 0.0007003220380283892, 0.0007003220380283892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007003220380283892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48573315
Iteration 2/25 | Loss: 0.00032826
Iteration 3/25 | Loss: 0.00032825
Iteration 4/25 | Loss: 0.00032825
Iteration 5/25 | Loss: 0.00032825
Iteration 6/25 | Loss: 0.00032825
Iteration 7/25 | Loss: 0.00032825
Iteration 8/25 | Loss: 0.00032825
Iteration 9/25 | Loss: 0.00032825
Iteration 10/25 | Loss: 0.00032825
Iteration 11/25 | Loss: 0.00032825
Iteration 12/25 | Loss: 0.00032825
Iteration 13/25 | Loss: 0.00032824
Iteration 14/25 | Loss: 0.00032824
Iteration 15/25 | Loss: 0.00032824
Iteration 16/25 | Loss: 0.00032824
Iteration 17/25 | Loss: 0.00032824
Iteration 18/25 | Loss: 0.00032824
Iteration 19/25 | Loss: 0.00032824
Iteration 20/25 | Loss: 0.00032824
Iteration 21/25 | Loss: 0.00032824
Iteration 22/25 | Loss: 0.00032824
Iteration 23/25 | Loss: 0.00032824
Iteration 24/25 | Loss: 0.00032824
Iteration 25/25 | Loss: 0.00032824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032824
Iteration 2/1000 | Loss: 0.00004161
Iteration 3/1000 | Loss: 0.00002830
Iteration 4/1000 | Loss: 0.00002577
Iteration 5/1000 | Loss: 0.00002431
Iteration 6/1000 | Loss: 0.00002354
Iteration 7/1000 | Loss: 0.00002290
Iteration 8/1000 | Loss: 0.00002245
Iteration 9/1000 | Loss: 0.00002209
Iteration 10/1000 | Loss: 0.00002185
Iteration 11/1000 | Loss: 0.00002167
Iteration 12/1000 | Loss: 0.00002164
Iteration 13/1000 | Loss: 0.00002148
Iteration 14/1000 | Loss: 0.00002137
Iteration 15/1000 | Loss: 0.00002135
Iteration 16/1000 | Loss: 0.00002132
Iteration 17/1000 | Loss: 0.00002132
Iteration 18/1000 | Loss: 0.00002132
Iteration 19/1000 | Loss: 0.00002132
Iteration 20/1000 | Loss: 0.00002131
Iteration 21/1000 | Loss: 0.00002131
Iteration 22/1000 | Loss: 0.00002130
Iteration 23/1000 | Loss: 0.00002130
Iteration 24/1000 | Loss: 0.00002129
Iteration 25/1000 | Loss: 0.00002129
Iteration 26/1000 | Loss: 0.00002128
Iteration 27/1000 | Loss: 0.00002128
Iteration 28/1000 | Loss: 0.00002128
Iteration 29/1000 | Loss: 0.00002128
Iteration 30/1000 | Loss: 0.00002128
Iteration 31/1000 | Loss: 0.00002128
Iteration 32/1000 | Loss: 0.00002128
Iteration 33/1000 | Loss: 0.00002128
Iteration 34/1000 | Loss: 0.00002128
Iteration 35/1000 | Loss: 0.00002128
Iteration 36/1000 | Loss: 0.00002128
Iteration 37/1000 | Loss: 0.00002127
Iteration 38/1000 | Loss: 0.00002127
Iteration 39/1000 | Loss: 0.00002126
Iteration 40/1000 | Loss: 0.00002126
Iteration 41/1000 | Loss: 0.00002126
Iteration 42/1000 | Loss: 0.00002126
Iteration 43/1000 | Loss: 0.00002126
Iteration 44/1000 | Loss: 0.00002126
Iteration 45/1000 | Loss: 0.00002126
Iteration 46/1000 | Loss: 0.00002125
Iteration 47/1000 | Loss: 0.00002125
Iteration 48/1000 | Loss: 0.00002125
Iteration 49/1000 | Loss: 0.00002125
Iteration 50/1000 | Loss: 0.00002125
Iteration 51/1000 | Loss: 0.00002125
Iteration 52/1000 | Loss: 0.00002125
Iteration 53/1000 | Loss: 0.00002125
Iteration 54/1000 | Loss: 0.00002125
Iteration 55/1000 | Loss: 0.00002125
Iteration 56/1000 | Loss: 0.00002125
Iteration 57/1000 | Loss: 0.00002125
Iteration 58/1000 | Loss: 0.00002125
Iteration 59/1000 | Loss: 0.00002124
Iteration 60/1000 | Loss: 0.00002124
Iteration 61/1000 | Loss: 0.00002124
Iteration 62/1000 | Loss: 0.00002124
Iteration 63/1000 | Loss: 0.00002124
Iteration 64/1000 | Loss: 0.00002123
Iteration 65/1000 | Loss: 0.00002123
Iteration 66/1000 | Loss: 0.00002123
Iteration 67/1000 | Loss: 0.00002123
Iteration 68/1000 | Loss: 0.00002123
Iteration 69/1000 | Loss: 0.00002123
Iteration 70/1000 | Loss: 0.00002122
Iteration 71/1000 | Loss: 0.00002122
Iteration 72/1000 | Loss: 0.00002121
Iteration 73/1000 | Loss: 0.00002121
Iteration 74/1000 | Loss: 0.00002121
Iteration 75/1000 | Loss: 0.00002121
Iteration 76/1000 | Loss: 0.00002121
Iteration 77/1000 | Loss: 0.00002121
Iteration 78/1000 | Loss: 0.00002120
Iteration 79/1000 | Loss: 0.00002120
Iteration 80/1000 | Loss: 0.00002120
Iteration 81/1000 | Loss: 0.00002120
Iteration 82/1000 | Loss: 0.00002119
Iteration 83/1000 | Loss: 0.00002119
Iteration 84/1000 | Loss: 0.00002119
Iteration 85/1000 | Loss: 0.00002119
Iteration 86/1000 | Loss: 0.00002119
Iteration 87/1000 | Loss: 0.00002119
Iteration 88/1000 | Loss: 0.00002119
Iteration 89/1000 | Loss: 0.00002119
Iteration 90/1000 | Loss: 0.00002118
Iteration 91/1000 | Loss: 0.00002118
Iteration 92/1000 | Loss: 0.00002118
Iteration 93/1000 | Loss: 0.00002118
Iteration 94/1000 | Loss: 0.00002118
Iteration 95/1000 | Loss: 0.00002117
Iteration 96/1000 | Loss: 0.00002117
Iteration 97/1000 | Loss: 0.00002117
Iteration 98/1000 | Loss: 0.00002117
Iteration 99/1000 | Loss: 0.00002117
Iteration 100/1000 | Loss: 0.00002116
Iteration 101/1000 | Loss: 0.00002116
Iteration 102/1000 | Loss: 0.00002116
Iteration 103/1000 | Loss: 0.00002116
Iteration 104/1000 | Loss: 0.00002116
Iteration 105/1000 | Loss: 0.00002116
Iteration 106/1000 | Loss: 0.00002115
Iteration 107/1000 | Loss: 0.00002115
Iteration 108/1000 | Loss: 0.00002115
Iteration 109/1000 | Loss: 0.00002115
Iteration 110/1000 | Loss: 0.00002115
Iteration 111/1000 | Loss: 0.00002115
Iteration 112/1000 | Loss: 0.00002114
Iteration 113/1000 | Loss: 0.00002114
Iteration 114/1000 | Loss: 0.00002114
Iteration 115/1000 | Loss: 0.00002114
Iteration 116/1000 | Loss: 0.00002114
Iteration 117/1000 | Loss: 0.00002114
Iteration 118/1000 | Loss: 0.00002114
Iteration 119/1000 | Loss: 0.00002114
Iteration 120/1000 | Loss: 0.00002114
Iteration 121/1000 | Loss: 0.00002114
Iteration 122/1000 | Loss: 0.00002113
Iteration 123/1000 | Loss: 0.00002113
Iteration 124/1000 | Loss: 0.00002113
Iteration 125/1000 | Loss: 0.00002113
Iteration 126/1000 | Loss: 0.00002113
Iteration 127/1000 | Loss: 0.00002113
Iteration 128/1000 | Loss: 0.00002113
Iteration 129/1000 | Loss: 0.00002112
Iteration 130/1000 | Loss: 0.00002112
Iteration 131/1000 | Loss: 0.00002112
Iteration 132/1000 | Loss: 0.00002112
Iteration 133/1000 | Loss: 0.00002112
Iteration 134/1000 | Loss: 0.00002111
Iteration 135/1000 | Loss: 0.00002111
Iteration 136/1000 | Loss: 0.00002111
Iteration 137/1000 | Loss: 0.00002111
Iteration 138/1000 | Loss: 0.00002111
Iteration 139/1000 | Loss: 0.00002111
Iteration 140/1000 | Loss: 0.00002111
Iteration 141/1000 | Loss: 0.00002111
Iteration 142/1000 | Loss: 0.00002111
Iteration 143/1000 | Loss: 0.00002111
Iteration 144/1000 | Loss: 0.00002111
Iteration 145/1000 | Loss: 0.00002110
Iteration 146/1000 | Loss: 0.00002110
Iteration 147/1000 | Loss: 0.00002110
Iteration 148/1000 | Loss: 0.00002110
Iteration 149/1000 | Loss: 0.00002110
Iteration 150/1000 | Loss: 0.00002110
Iteration 151/1000 | Loss: 0.00002110
Iteration 152/1000 | Loss: 0.00002110
Iteration 153/1000 | Loss: 0.00002110
Iteration 154/1000 | Loss: 0.00002110
Iteration 155/1000 | Loss: 0.00002110
Iteration 156/1000 | Loss: 0.00002110
Iteration 157/1000 | Loss: 0.00002110
Iteration 158/1000 | Loss: 0.00002110
Iteration 159/1000 | Loss: 0.00002110
Iteration 160/1000 | Loss: 0.00002110
Iteration 161/1000 | Loss: 0.00002109
Iteration 162/1000 | Loss: 0.00002109
Iteration 163/1000 | Loss: 0.00002109
Iteration 164/1000 | Loss: 0.00002109
Iteration 165/1000 | Loss: 0.00002109
Iteration 166/1000 | Loss: 0.00002109
Iteration 167/1000 | Loss: 0.00002109
Iteration 168/1000 | Loss: 0.00002109
Iteration 169/1000 | Loss: 0.00002109
Iteration 170/1000 | Loss: 0.00002108
Iteration 171/1000 | Loss: 0.00002108
Iteration 172/1000 | Loss: 0.00002108
Iteration 173/1000 | Loss: 0.00002108
Iteration 174/1000 | Loss: 0.00002108
Iteration 175/1000 | Loss: 0.00002108
Iteration 176/1000 | Loss: 0.00002108
Iteration 177/1000 | Loss: 0.00002108
Iteration 178/1000 | Loss: 0.00002108
Iteration 179/1000 | Loss: 0.00002108
Iteration 180/1000 | Loss: 0.00002108
Iteration 181/1000 | Loss: 0.00002108
Iteration 182/1000 | Loss: 0.00002108
Iteration 183/1000 | Loss: 0.00002108
Iteration 184/1000 | Loss: 0.00002108
Iteration 185/1000 | Loss: 0.00002108
Iteration 186/1000 | Loss: 0.00002108
Iteration 187/1000 | Loss: 0.00002108
Iteration 188/1000 | Loss: 0.00002108
Iteration 189/1000 | Loss: 0.00002108
Iteration 190/1000 | Loss: 0.00002108
Iteration 191/1000 | Loss: 0.00002108
Iteration 192/1000 | Loss: 0.00002108
Iteration 193/1000 | Loss: 0.00002108
Iteration 194/1000 | Loss: 0.00002108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.1083949832245708e-05, 2.1083949832245708e-05, 2.1083949832245708e-05, 2.1083949832245708e-05, 2.1083949832245708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1083949832245708e-05

Optimization complete. Final v2v error: 3.783944606781006 mm

Highest mean error: 4.553245544433594 mm for frame 71

Lowest mean error: 3.084491729736328 mm for frame 213

Saving results

Total time: 45.8651909828186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846465
Iteration 2/25 | Loss: 0.00152815
Iteration 3/25 | Loss: 0.00080106
Iteration 4/25 | Loss: 0.00069401
Iteration 5/25 | Loss: 0.00068027
Iteration 6/25 | Loss: 0.00067734
Iteration 7/25 | Loss: 0.00067672
Iteration 8/25 | Loss: 0.00067670
Iteration 9/25 | Loss: 0.00067670
Iteration 10/25 | Loss: 0.00067670
Iteration 11/25 | Loss: 0.00067670
Iteration 12/25 | Loss: 0.00067670
Iteration 13/25 | Loss: 0.00067670
Iteration 14/25 | Loss: 0.00067670
Iteration 15/25 | Loss: 0.00067670
Iteration 16/25 | Loss: 0.00067670
Iteration 17/25 | Loss: 0.00067670
Iteration 18/25 | Loss: 0.00067670
Iteration 19/25 | Loss: 0.00067670
Iteration 20/25 | Loss: 0.00067670
Iteration 21/25 | Loss: 0.00067670
Iteration 22/25 | Loss: 0.00067670
Iteration 23/25 | Loss: 0.00067670
Iteration 24/25 | Loss: 0.00067670
Iteration 25/25 | Loss: 0.00067670

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45997643
Iteration 2/25 | Loss: 0.00032042
Iteration 3/25 | Loss: 0.00032042
Iteration 4/25 | Loss: 0.00032042
Iteration 5/25 | Loss: 0.00032042
Iteration 6/25 | Loss: 0.00032042
Iteration 7/25 | Loss: 0.00032042
Iteration 8/25 | Loss: 0.00032042
Iteration 9/25 | Loss: 0.00032042
Iteration 10/25 | Loss: 0.00032042
Iteration 11/25 | Loss: 0.00032042
Iteration 12/25 | Loss: 0.00032042
Iteration 13/25 | Loss: 0.00032042
Iteration 14/25 | Loss: 0.00032042
Iteration 15/25 | Loss: 0.00032042
Iteration 16/25 | Loss: 0.00032042
Iteration 17/25 | Loss: 0.00032042
Iteration 18/25 | Loss: 0.00032042
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003204150125384331, 0.0003204150125384331, 0.0003204150125384331, 0.0003204150125384331, 0.0003204150125384331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003204150125384331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032042
Iteration 2/1000 | Loss: 0.00003312
Iteration 3/1000 | Loss: 0.00002415
Iteration 4/1000 | Loss: 0.00002258
Iteration 5/1000 | Loss: 0.00002118
Iteration 6/1000 | Loss: 0.00002028
Iteration 7/1000 | Loss: 0.00001959
Iteration 8/1000 | Loss: 0.00001918
Iteration 9/1000 | Loss: 0.00001892
Iteration 10/1000 | Loss: 0.00001875
Iteration 11/1000 | Loss: 0.00001867
Iteration 12/1000 | Loss: 0.00001864
Iteration 13/1000 | Loss: 0.00001862
Iteration 14/1000 | Loss: 0.00001859
Iteration 15/1000 | Loss: 0.00001858
Iteration 16/1000 | Loss: 0.00001858
Iteration 17/1000 | Loss: 0.00001854
Iteration 18/1000 | Loss: 0.00001853
Iteration 19/1000 | Loss: 0.00001853
Iteration 20/1000 | Loss: 0.00001852
Iteration 21/1000 | Loss: 0.00001852
Iteration 22/1000 | Loss: 0.00001852
Iteration 23/1000 | Loss: 0.00001852
Iteration 24/1000 | Loss: 0.00001852
Iteration 25/1000 | Loss: 0.00001852
Iteration 26/1000 | Loss: 0.00001851
Iteration 27/1000 | Loss: 0.00001851
Iteration 28/1000 | Loss: 0.00001851
Iteration 29/1000 | Loss: 0.00001851
Iteration 30/1000 | Loss: 0.00001851
Iteration 31/1000 | Loss: 0.00001851
Iteration 32/1000 | Loss: 0.00001851
Iteration 33/1000 | Loss: 0.00001851
Iteration 34/1000 | Loss: 0.00001851
Iteration 35/1000 | Loss: 0.00001851
Iteration 36/1000 | Loss: 0.00001851
Iteration 37/1000 | Loss: 0.00001850
Iteration 38/1000 | Loss: 0.00001850
Iteration 39/1000 | Loss: 0.00001850
Iteration 40/1000 | Loss: 0.00001849
Iteration 41/1000 | Loss: 0.00001849
Iteration 42/1000 | Loss: 0.00001849
Iteration 43/1000 | Loss: 0.00001849
Iteration 44/1000 | Loss: 0.00001849
Iteration 45/1000 | Loss: 0.00001848
Iteration 46/1000 | Loss: 0.00001848
Iteration 47/1000 | Loss: 0.00001848
Iteration 48/1000 | Loss: 0.00001848
Iteration 49/1000 | Loss: 0.00001848
Iteration 50/1000 | Loss: 0.00001848
Iteration 51/1000 | Loss: 0.00001848
Iteration 52/1000 | Loss: 0.00001848
Iteration 53/1000 | Loss: 0.00001847
Iteration 54/1000 | Loss: 0.00001847
Iteration 55/1000 | Loss: 0.00001847
Iteration 56/1000 | Loss: 0.00001847
Iteration 57/1000 | Loss: 0.00001847
Iteration 58/1000 | Loss: 0.00001846
Iteration 59/1000 | Loss: 0.00001846
Iteration 60/1000 | Loss: 0.00001846
Iteration 61/1000 | Loss: 0.00001846
Iteration 62/1000 | Loss: 0.00001845
Iteration 63/1000 | Loss: 0.00001845
Iteration 64/1000 | Loss: 0.00001845
Iteration 65/1000 | Loss: 0.00001845
Iteration 66/1000 | Loss: 0.00001845
Iteration 67/1000 | Loss: 0.00001845
Iteration 68/1000 | Loss: 0.00001845
Iteration 69/1000 | Loss: 0.00001845
Iteration 70/1000 | Loss: 0.00001845
Iteration 71/1000 | Loss: 0.00001845
Iteration 72/1000 | Loss: 0.00001844
Iteration 73/1000 | Loss: 0.00001844
Iteration 74/1000 | Loss: 0.00001844
Iteration 75/1000 | Loss: 0.00001843
Iteration 76/1000 | Loss: 0.00001843
Iteration 77/1000 | Loss: 0.00001843
Iteration 78/1000 | Loss: 0.00001843
Iteration 79/1000 | Loss: 0.00001843
Iteration 80/1000 | Loss: 0.00001843
Iteration 81/1000 | Loss: 0.00001843
Iteration 82/1000 | Loss: 0.00001843
Iteration 83/1000 | Loss: 0.00001843
Iteration 84/1000 | Loss: 0.00001843
Iteration 85/1000 | Loss: 0.00001843
Iteration 86/1000 | Loss: 0.00001843
Iteration 87/1000 | Loss: 0.00001843
Iteration 88/1000 | Loss: 0.00001843
Iteration 89/1000 | Loss: 0.00001843
Iteration 90/1000 | Loss: 0.00001843
Iteration 91/1000 | Loss: 0.00001843
Iteration 92/1000 | Loss: 0.00001843
Iteration 93/1000 | Loss: 0.00001843
Iteration 94/1000 | Loss: 0.00001843
Iteration 95/1000 | Loss: 0.00001843
Iteration 96/1000 | Loss: 0.00001843
Iteration 97/1000 | Loss: 0.00001843
Iteration 98/1000 | Loss: 0.00001843
Iteration 99/1000 | Loss: 0.00001843
Iteration 100/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.8426941096549854e-05, 1.8426941096549854e-05, 1.8426941096549854e-05, 1.8426941096549854e-05, 1.8426941096549854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8426941096549854e-05

Optimization complete. Final v2v error: 3.61000657081604 mm

Highest mean error: 4.346690654754639 mm for frame 171

Lowest mean error: 2.8462746143341064 mm for frame 62

Saving results

Total time: 37.13896989822388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00638798
Iteration 2/25 | Loss: 0.00078796
Iteration 3/25 | Loss: 0.00063885
Iteration 4/25 | Loss: 0.00061422
Iteration 5/25 | Loss: 0.00060511
Iteration 6/25 | Loss: 0.00060330
Iteration 7/25 | Loss: 0.00060287
Iteration 8/25 | Loss: 0.00060287
Iteration 9/25 | Loss: 0.00060287
Iteration 10/25 | Loss: 0.00060287
Iteration 11/25 | Loss: 0.00060287
Iteration 12/25 | Loss: 0.00060287
Iteration 13/25 | Loss: 0.00060287
Iteration 14/25 | Loss: 0.00060287
Iteration 15/25 | Loss: 0.00060287
Iteration 16/25 | Loss: 0.00060287
Iteration 17/25 | Loss: 0.00060287
Iteration 18/25 | Loss: 0.00060287
Iteration 19/25 | Loss: 0.00060287
Iteration 20/25 | Loss: 0.00060287
Iteration 21/25 | Loss: 0.00060287
Iteration 22/25 | Loss: 0.00060287
Iteration 23/25 | Loss: 0.00060287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006028712959960103, 0.0006028712959960103, 0.0006028712959960103, 0.0006028712959960103, 0.0006028712959960103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006028712959960103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49615049
Iteration 2/25 | Loss: 0.00034593
Iteration 3/25 | Loss: 0.00034593
Iteration 4/25 | Loss: 0.00034593
Iteration 5/25 | Loss: 0.00034593
Iteration 6/25 | Loss: 0.00034593
Iteration 7/25 | Loss: 0.00034593
Iteration 8/25 | Loss: 0.00034593
Iteration 9/25 | Loss: 0.00034593
Iteration 10/25 | Loss: 0.00034593
Iteration 11/25 | Loss: 0.00034593
Iteration 12/25 | Loss: 0.00034593
Iteration 13/25 | Loss: 0.00034593
Iteration 14/25 | Loss: 0.00034593
Iteration 15/25 | Loss: 0.00034593
Iteration 16/25 | Loss: 0.00034593
Iteration 17/25 | Loss: 0.00034593
Iteration 18/25 | Loss: 0.00034593
Iteration 19/25 | Loss: 0.00034593
Iteration 20/25 | Loss: 0.00034593
Iteration 21/25 | Loss: 0.00034593
Iteration 22/25 | Loss: 0.00034593
Iteration 23/25 | Loss: 0.00034593
Iteration 24/25 | Loss: 0.00034593
Iteration 25/25 | Loss: 0.00034593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034593
Iteration 2/1000 | Loss: 0.00002051
Iteration 3/1000 | Loss: 0.00001397
Iteration 4/1000 | Loss: 0.00001294
Iteration 5/1000 | Loss: 0.00001243
Iteration 6/1000 | Loss: 0.00001211
Iteration 7/1000 | Loss: 0.00001190
Iteration 8/1000 | Loss: 0.00001187
Iteration 9/1000 | Loss: 0.00001185
Iteration 10/1000 | Loss: 0.00001175
Iteration 11/1000 | Loss: 0.00001173
Iteration 12/1000 | Loss: 0.00001172
Iteration 13/1000 | Loss: 0.00001171
Iteration 14/1000 | Loss: 0.00001170
Iteration 15/1000 | Loss: 0.00001168
Iteration 16/1000 | Loss: 0.00001160
Iteration 17/1000 | Loss: 0.00001157
Iteration 18/1000 | Loss: 0.00001156
Iteration 19/1000 | Loss: 0.00001155
Iteration 20/1000 | Loss: 0.00001155
Iteration 21/1000 | Loss: 0.00001153
Iteration 22/1000 | Loss: 0.00001153
Iteration 23/1000 | Loss: 0.00001152
Iteration 24/1000 | Loss: 0.00001152
Iteration 25/1000 | Loss: 0.00001152
Iteration 26/1000 | Loss: 0.00001152
Iteration 27/1000 | Loss: 0.00001152
Iteration 28/1000 | Loss: 0.00001152
Iteration 29/1000 | Loss: 0.00001152
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001152
Iteration 32/1000 | Loss: 0.00001152
Iteration 33/1000 | Loss: 0.00001152
Iteration 34/1000 | Loss: 0.00001151
Iteration 35/1000 | Loss: 0.00001151
Iteration 36/1000 | Loss: 0.00001151
Iteration 37/1000 | Loss: 0.00001151
Iteration 38/1000 | Loss: 0.00001150
Iteration 39/1000 | Loss: 0.00001149
Iteration 40/1000 | Loss: 0.00001147
Iteration 41/1000 | Loss: 0.00001146
Iteration 42/1000 | Loss: 0.00001146
Iteration 43/1000 | Loss: 0.00001146
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001145
Iteration 46/1000 | Loss: 0.00001145
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001144
Iteration 49/1000 | Loss: 0.00001144
Iteration 50/1000 | Loss: 0.00001143
Iteration 51/1000 | Loss: 0.00001142
Iteration 52/1000 | Loss: 0.00001142
Iteration 53/1000 | Loss: 0.00001142
Iteration 54/1000 | Loss: 0.00001142
Iteration 55/1000 | Loss: 0.00001142
Iteration 56/1000 | Loss: 0.00001142
Iteration 57/1000 | Loss: 0.00001141
Iteration 58/1000 | Loss: 0.00001141
Iteration 59/1000 | Loss: 0.00001141
Iteration 60/1000 | Loss: 0.00001140
Iteration 61/1000 | Loss: 0.00001139
Iteration 62/1000 | Loss: 0.00001137
Iteration 63/1000 | Loss: 0.00001137
Iteration 64/1000 | Loss: 0.00001137
Iteration 65/1000 | Loss: 0.00001136
Iteration 66/1000 | Loss: 0.00001136
Iteration 67/1000 | Loss: 0.00001135
Iteration 68/1000 | Loss: 0.00001135
Iteration 69/1000 | Loss: 0.00001134
Iteration 70/1000 | Loss: 0.00001134
Iteration 71/1000 | Loss: 0.00001132
Iteration 72/1000 | Loss: 0.00001132
Iteration 73/1000 | Loss: 0.00001132
Iteration 74/1000 | Loss: 0.00001132
Iteration 75/1000 | Loss: 0.00001132
Iteration 76/1000 | Loss: 0.00001132
Iteration 77/1000 | Loss: 0.00001132
Iteration 78/1000 | Loss: 0.00001131
Iteration 79/1000 | Loss: 0.00001131
Iteration 80/1000 | Loss: 0.00001131
Iteration 81/1000 | Loss: 0.00001131
Iteration 82/1000 | Loss: 0.00001131
Iteration 83/1000 | Loss: 0.00001130
Iteration 84/1000 | Loss: 0.00001130
Iteration 85/1000 | Loss: 0.00001128
Iteration 86/1000 | Loss: 0.00001128
Iteration 87/1000 | Loss: 0.00001128
Iteration 88/1000 | Loss: 0.00001128
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001126
Iteration 92/1000 | Loss: 0.00001126
Iteration 93/1000 | Loss: 0.00001126
Iteration 94/1000 | Loss: 0.00001125
Iteration 95/1000 | Loss: 0.00001125
Iteration 96/1000 | Loss: 0.00001124
Iteration 97/1000 | Loss: 0.00001124
Iteration 98/1000 | Loss: 0.00001124
Iteration 99/1000 | Loss: 0.00001124
Iteration 100/1000 | Loss: 0.00001124
Iteration 101/1000 | Loss: 0.00001124
Iteration 102/1000 | Loss: 0.00001123
Iteration 103/1000 | Loss: 0.00001123
Iteration 104/1000 | Loss: 0.00001123
Iteration 105/1000 | Loss: 0.00001123
Iteration 106/1000 | Loss: 0.00001123
Iteration 107/1000 | Loss: 0.00001123
Iteration 108/1000 | Loss: 0.00001122
Iteration 109/1000 | Loss: 0.00001122
Iteration 110/1000 | Loss: 0.00001122
Iteration 111/1000 | Loss: 0.00001121
Iteration 112/1000 | Loss: 0.00001121
Iteration 113/1000 | Loss: 0.00001121
Iteration 114/1000 | Loss: 0.00001121
Iteration 115/1000 | Loss: 0.00001121
Iteration 116/1000 | Loss: 0.00001121
Iteration 117/1000 | Loss: 0.00001121
Iteration 118/1000 | Loss: 0.00001121
Iteration 119/1000 | Loss: 0.00001120
Iteration 120/1000 | Loss: 0.00001120
Iteration 121/1000 | Loss: 0.00001120
Iteration 122/1000 | Loss: 0.00001120
Iteration 123/1000 | Loss: 0.00001120
Iteration 124/1000 | Loss: 0.00001120
Iteration 125/1000 | Loss: 0.00001120
Iteration 126/1000 | Loss: 0.00001119
Iteration 127/1000 | Loss: 0.00001119
Iteration 128/1000 | Loss: 0.00001119
Iteration 129/1000 | Loss: 0.00001119
Iteration 130/1000 | Loss: 0.00001118
Iteration 131/1000 | Loss: 0.00001118
Iteration 132/1000 | Loss: 0.00001118
Iteration 133/1000 | Loss: 0.00001118
Iteration 134/1000 | Loss: 0.00001118
Iteration 135/1000 | Loss: 0.00001118
Iteration 136/1000 | Loss: 0.00001118
Iteration 137/1000 | Loss: 0.00001118
Iteration 138/1000 | Loss: 0.00001118
Iteration 139/1000 | Loss: 0.00001117
Iteration 140/1000 | Loss: 0.00001117
Iteration 141/1000 | Loss: 0.00001117
Iteration 142/1000 | Loss: 0.00001117
Iteration 143/1000 | Loss: 0.00001117
Iteration 144/1000 | Loss: 0.00001117
Iteration 145/1000 | Loss: 0.00001117
Iteration 146/1000 | Loss: 0.00001117
Iteration 147/1000 | Loss: 0.00001117
Iteration 148/1000 | Loss: 0.00001117
Iteration 149/1000 | Loss: 0.00001117
Iteration 150/1000 | Loss: 0.00001117
Iteration 151/1000 | Loss: 0.00001117
Iteration 152/1000 | Loss: 0.00001117
Iteration 153/1000 | Loss: 0.00001117
Iteration 154/1000 | Loss: 0.00001117
Iteration 155/1000 | Loss: 0.00001117
Iteration 156/1000 | Loss: 0.00001117
Iteration 157/1000 | Loss: 0.00001117
Iteration 158/1000 | Loss: 0.00001117
Iteration 159/1000 | Loss: 0.00001117
Iteration 160/1000 | Loss: 0.00001117
Iteration 161/1000 | Loss: 0.00001117
Iteration 162/1000 | Loss: 0.00001117
Iteration 163/1000 | Loss: 0.00001117
Iteration 164/1000 | Loss: 0.00001117
Iteration 165/1000 | Loss: 0.00001117
Iteration 166/1000 | Loss: 0.00001117
Iteration 167/1000 | Loss: 0.00001117
Iteration 168/1000 | Loss: 0.00001117
Iteration 169/1000 | Loss: 0.00001117
Iteration 170/1000 | Loss: 0.00001117
Iteration 171/1000 | Loss: 0.00001117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.1165000614710152e-05, 1.1165000614710152e-05, 1.1165000614710152e-05, 1.1165000614710152e-05, 1.1165000614710152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1165000614710152e-05

Optimization complete. Final v2v error: 2.8356399536132812 mm

Highest mean error: 3.2988946437835693 mm for frame 80

Lowest mean error: 2.6435577869415283 mm for frame 104

Saving results

Total time: 35.14202165603638
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00699535
Iteration 2/25 | Loss: 0.00138199
Iteration 3/25 | Loss: 0.00089046
Iteration 4/25 | Loss: 0.00089053
Iteration 5/25 | Loss: 0.00078189
Iteration 6/25 | Loss: 0.00075784
Iteration 7/25 | Loss: 0.00073489
Iteration 8/25 | Loss: 0.00073034
Iteration 9/25 | Loss: 0.00072529
Iteration 10/25 | Loss: 0.00072252
Iteration 11/25 | Loss: 0.00072370
Iteration 12/25 | Loss: 0.00072056
Iteration 13/25 | Loss: 0.00071913
Iteration 14/25 | Loss: 0.00071861
Iteration 15/25 | Loss: 0.00071690
Iteration 16/25 | Loss: 0.00071503
Iteration 17/25 | Loss: 0.00071426
Iteration 18/25 | Loss: 0.00071465
Iteration 19/25 | Loss: 0.00071163
Iteration 20/25 | Loss: 0.00070955
Iteration 21/25 | Loss: 0.00070878
Iteration 22/25 | Loss: 0.00070860
Iteration 23/25 | Loss: 0.00070856
Iteration 24/25 | Loss: 0.00070856
Iteration 25/25 | Loss: 0.00070856

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.34203339
Iteration 2/25 | Loss: 0.00038028
Iteration 3/25 | Loss: 0.00038023
Iteration 4/25 | Loss: 0.00038023
Iteration 5/25 | Loss: 0.00038023
Iteration 6/25 | Loss: 0.00038023
Iteration 7/25 | Loss: 0.00038023
Iteration 8/25 | Loss: 0.00038023
Iteration 9/25 | Loss: 0.00038023
Iteration 10/25 | Loss: 0.00038023
Iteration 11/25 | Loss: 0.00038023
Iteration 12/25 | Loss: 0.00038023
Iteration 13/25 | Loss: 0.00038023
Iteration 14/25 | Loss: 0.00038023
Iteration 15/25 | Loss: 0.00038023
Iteration 16/25 | Loss: 0.00038023
Iteration 17/25 | Loss: 0.00038023
Iteration 18/25 | Loss: 0.00038023
Iteration 19/25 | Loss: 0.00038023
Iteration 20/25 | Loss: 0.00038023
Iteration 21/25 | Loss: 0.00038023
Iteration 22/25 | Loss: 0.00038023
Iteration 23/25 | Loss: 0.00038023
Iteration 24/25 | Loss: 0.00038023
Iteration 25/25 | Loss: 0.00038023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038023
Iteration 2/1000 | Loss: 0.00003177
Iteration 3/1000 | Loss: 0.00002330
Iteration 4/1000 | Loss: 0.00002171
Iteration 5/1000 | Loss: 0.00002051
Iteration 6/1000 | Loss: 0.00001959
Iteration 7/1000 | Loss: 0.00001903
Iteration 8/1000 | Loss: 0.00001868
Iteration 9/1000 | Loss: 0.00001841
Iteration 10/1000 | Loss: 0.00011985
Iteration 11/1000 | Loss: 0.00001857
Iteration 12/1000 | Loss: 0.00001765
Iteration 13/1000 | Loss: 0.00001703
Iteration 14/1000 | Loss: 0.00001647
Iteration 15/1000 | Loss: 0.00001620
Iteration 16/1000 | Loss: 0.00001617
Iteration 17/1000 | Loss: 0.00001613
Iteration 18/1000 | Loss: 0.00001612
Iteration 19/1000 | Loss: 0.00001609
Iteration 20/1000 | Loss: 0.00001608
Iteration 21/1000 | Loss: 0.00001602
Iteration 22/1000 | Loss: 0.00001600
Iteration 23/1000 | Loss: 0.00001600
Iteration 24/1000 | Loss: 0.00001599
Iteration 25/1000 | Loss: 0.00001598
Iteration 26/1000 | Loss: 0.00001597
Iteration 27/1000 | Loss: 0.00001597
Iteration 28/1000 | Loss: 0.00001595
Iteration 29/1000 | Loss: 0.00001594
Iteration 30/1000 | Loss: 0.00001594
Iteration 31/1000 | Loss: 0.00001594
Iteration 32/1000 | Loss: 0.00001593
Iteration 33/1000 | Loss: 0.00001593
Iteration 34/1000 | Loss: 0.00001590
Iteration 35/1000 | Loss: 0.00001590
Iteration 36/1000 | Loss: 0.00001589
Iteration 37/1000 | Loss: 0.00001589
Iteration 38/1000 | Loss: 0.00001588
Iteration 39/1000 | Loss: 0.00001588
Iteration 40/1000 | Loss: 0.00001588
Iteration 41/1000 | Loss: 0.00001588
Iteration 42/1000 | Loss: 0.00001588
Iteration 43/1000 | Loss: 0.00001587
Iteration 44/1000 | Loss: 0.00001587
Iteration 45/1000 | Loss: 0.00001587
Iteration 46/1000 | Loss: 0.00001586
Iteration 47/1000 | Loss: 0.00001586
Iteration 48/1000 | Loss: 0.00001586
Iteration 49/1000 | Loss: 0.00001586
Iteration 50/1000 | Loss: 0.00001586
Iteration 51/1000 | Loss: 0.00001585
Iteration 52/1000 | Loss: 0.00001585
Iteration 53/1000 | Loss: 0.00001585
Iteration 54/1000 | Loss: 0.00001585
Iteration 55/1000 | Loss: 0.00001585
Iteration 56/1000 | Loss: 0.00001584
Iteration 57/1000 | Loss: 0.00001584
Iteration 58/1000 | Loss: 0.00001584
Iteration 59/1000 | Loss: 0.00001583
Iteration 60/1000 | Loss: 0.00001583
Iteration 61/1000 | Loss: 0.00001582
Iteration 62/1000 | Loss: 0.00001582
Iteration 63/1000 | Loss: 0.00001582
Iteration 64/1000 | Loss: 0.00001582
Iteration 65/1000 | Loss: 0.00001582
Iteration 66/1000 | Loss: 0.00001582
Iteration 67/1000 | Loss: 0.00001581
Iteration 68/1000 | Loss: 0.00001581
Iteration 69/1000 | Loss: 0.00001581
Iteration 70/1000 | Loss: 0.00001581
Iteration 71/1000 | Loss: 0.00001581
Iteration 72/1000 | Loss: 0.00001580
Iteration 73/1000 | Loss: 0.00001580
Iteration 74/1000 | Loss: 0.00001580
Iteration 75/1000 | Loss: 0.00001580
Iteration 76/1000 | Loss: 0.00001580
Iteration 77/1000 | Loss: 0.00001579
Iteration 78/1000 | Loss: 0.00001579
Iteration 79/1000 | Loss: 0.00001578
Iteration 80/1000 | Loss: 0.00001578
Iteration 81/1000 | Loss: 0.00001577
Iteration 82/1000 | Loss: 0.00001577
Iteration 83/1000 | Loss: 0.00001577
Iteration 84/1000 | Loss: 0.00001577
Iteration 85/1000 | Loss: 0.00001577
Iteration 86/1000 | Loss: 0.00001577
Iteration 87/1000 | Loss: 0.00001577
Iteration 88/1000 | Loss: 0.00001577
Iteration 89/1000 | Loss: 0.00001577
Iteration 90/1000 | Loss: 0.00001577
Iteration 91/1000 | Loss: 0.00001576
Iteration 92/1000 | Loss: 0.00001576
Iteration 93/1000 | Loss: 0.00001576
Iteration 94/1000 | Loss: 0.00001575
Iteration 95/1000 | Loss: 0.00001575
Iteration 96/1000 | Loss: 0.00001575
Iteration 97/1000 | Loss: 0.00001574
Iteration 98/1000 | Loss: 0.00001574
Iteration 99/1000 | Loss: 0.00001574
Iteration 100/1000 | Loss: 0.00001574
Iteration 101/1000 | Loss: 0.00001573
Iteration 102/1000 | Loss: 0.00001573
Iteration 103/1000 | Loss: 0.00001573
Iteration 104/1000 | Loss: 0.00001572
Iteration 105/1000 | Loss: 0.00001572
Iteration 106/1000 | Loss: 0.00001572
Iteration 107/1000 | Loss: 0.00001572
Iteration 108/1000 | Loss: 0.00001571
Iteration 109/1000 | Loss: 0.00001571
Iteration 110/1000 | Loss: 0.00001571
Iteration 111/1000 | Loss: 0.00001571
Iteration 112/1000 | Loss: 0.00001571
Iteration 113/1000 | Loss: 0.00001571
Iteration 114/1000 | Loss: 0.00001571
Iteration 115/1000 | Loss: 0.00001571
Iteration 116/1000 | Loss: 0.00001571
Iteration 117/1000 | Loss: 0.00001571
Iteration 118/1000 | Loss: 0.00001571
Iteration 119/1000 | Loss: 0.00001571
Iteration 120/1000 | Loss: 0.00001570
Iteration 121/1000 | Loss: 0.00001570
Iteration 122/1000 | Loss: 0.00001570
Iteration 123/1000 | Loss: 0.00001570
Iteration 124/1000 | Loss: 0.00001570
Iteration 125/1000 | Loss: 0.00001570
Iteration 126/1000 | Loss: 0.00001570
Iteration 127/1000 | Loss: 0.00001570
Iteration 128/1000 | Loss: 0.00001570
Iteration 129/1000 | Loss: 0.00001570
Iteration 130/1000 | Loss: 0.00001570
Iteration 131/1000 | Loss: 0.00001569
Iteration 132/1000 | Loss: 0.00001569
Iteration 133/1000 | Loss: 0.00001569
Iteration 134/1000 | Loss: 0.00001569
Iteration 135/1000 | Loss: 0.00001569
Iteration 136/1000 | Loss: 0.00001569
Iteration 137/1000 | Loss: 0.00001569
Iteration 138/1000 | Loss: 0.00001569
Iteration 139/1000 | Loss: 0.00001569
Iteration 140/1000 | Loss: 0.00001569
Iteration 141/1000 | Loss: 0.00001569
Iteration 142/1000 | Loss: 0.00001569
Iteration 143/1000 | Loss: 0.00001569
Iteration 144/1000 | Loss: 0.00001569
Iteration 145/1000 | Loss: 0.00001569
Iteration 146/1000 | Loss: 0.00001568
Iteration 147/1000 | Loss: 0.00001568
Iteration 148/1000 | Loss: 0.00001568
Iteration 149/1000 | Loss: 0.00001568
Iteration 150/1000 | Loss: 0.00001568
Iteration 151/1000 | Loss: 0.00001568
Iteration 152/1000 | Loss: 0.00001568
Iteration 153/1000 | Loss: 0.00001568
Iteration 154/1000 | Loss: 0.00001568
Iteration 155/1000 | Loss: 0.00001568
Iteration 156/1000 | Loss: 0.00001567
Iteration 157/1000 | Loss: 0.00001567
Iteration 158/1000 | Loss: 0.00001567
Iteration 159/1000 | Loss: 0.00001567
Iteration 160/1000 | Loss: 0.00001567
Iteration 161/1000 | Loss: 0.00001567
Iteration 162/1000 | Loss: 0.00001567
Iteration 163/1000 | Loss: 0.00001567
Iteration 164/1000 | Loss: 0.00001567
Iteration 165/1000 | Loss: 0.00001567
Iteration 166/1000 | Loss: 0.00001567
Iteration 167/1000 | Loss: 0.00001567
Iteration 168/1000 | Loss: 0.00001567
Iteration 169/1000 | Loss: 0.00001566
Iteration 170/1000 | Loss: 0.00001566
Iteration 171/1000 | Loss: 0.00001566
Iteration 172/1000 | Loss: 0.00001566
Iteration 173/1000 | Loss: 0.00001566
Iteration 174/1000 | Loss: 0.00001566
Iteration 175/1000 | Loss: 0.00001566
Iteration 176/1000 | Loss: 0.00001566
Iteration 177/1000 | Loss: 0.00001566
Iteration 178/1000 | Loss: 0.00001566
Iteration 179/1000 | Loss: 0.00001566
Iteration 180/1000 | Loss: 0.00001566
Iteration 181/1000 | Loss: 0.00001566
Iteration 182/1000 | Loss: 0.00001566
Iteration 183/1000 | Loss: 0.00001566
Iteration 184/1000 | Loss: 0.00001566
Iteration 185/1000 | Loss: 0.00001566
Iteration 186/1000 | Loss: 0.00001566
Iteration 187/1000 | Loss: 0.00001566
Iteration 188/1000 | Loss: 0.00001566
Iteration 189/1000 | Loss: 0.00001566
Iteration 190/1000 | Loss: 0.00001566
Iteration 191/1000 | Loss: 0.00001566
Iteration 192/1000 | Loss: 0.00001566
Iteration 193/1000 | Loss: 0.00001566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.5659801647416316e-05, 1.5659801647416316e-05, 1.5659801647416316e-05, 1.5659801647416316e-05, 1.5659801647416316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5659801647416316e-05

Optimization complete. Final v2v error: 3.385219097137451 mm

Highest mean error: 3.9049923419952393 mm for frame 34

Lowest mean error: 2.847846508026123 mm for frame 132

Saving results

Total time: 74.9302966594696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879087
Iteration 2/25 | Loss: 0.00078281
Iteration 3/25 | Loss: 0.00063506
Iteration 4/25 | Loss: 0.00061316
Iteration 5/25 | Loss: 0.00060458
Iteration 6/25 | Loss: 0.00060323
Iteration 7/25 | Loss: 0.00060296
Iteration 8/25 | Loss: 0.00060296
Iteration 9/25 | Loss: 0.00060296
Iteration 10/25 | Loss: 0.00060296
Iteration 11/25 | Loss: 0.00060296
Iteration 12/25 | Loss: 0.00060296
Iteration 13/25 | Loss: 0.00060296
Iteration 14/25 | Loss: 0.00060296
Iteration 15/25 | Loss: 0.00060296
Iteration 16/25 | Loss: 0.00060296
Iteration 17/25 | Loss: 0.00060296
Iteration 18/25 | Loss: 0.00060296
Iteration 19/25 | Loss: 0.00060296
Iteration 20/25 | Loss: 0.00060296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006029614596627653, 0.0006029614596627653, 0.0006029614596627653, 0.0006029614596627653, 0.0006029614596627653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006029614596627653

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79848909
Iteration 2/25 | Loss: 0.00029518
Iteration 3/25 | Loss: 0.00029518
Iteration 4/25 | Loss: 0.00029518
Iteration 5/25 | Loss: 0.00029518
Iteration 6/25 | Loss: 0.00029518
Iteration 7/25 | Loss: 0.00029518
Iteration 8/25 | Loss: 0.00029518
Iteration 9/25 | Loss: 0.00029518
Iteration 10/25 | Loss: 0.00029518
Iteration 11/25 | Loss: 0.00029518
Iteration 12/25 | Loss: 0.00029518
Iteration 13/25 | Loss: 0.00029518
Iteration 14/25 | Loss: 0.00029518
Iteration 15/25 | Loss: 0.00029518
Iteration 16/25 | Loss: 0.00029518
Iteration 17/25 | Loss: 0.00029518
Iteration 18/25 | Loss: 0.00029518
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00029518167139030993, 0.00029518167139030993, 0.00029518167139030993, 0.00029518167139030993, 0.00029518167139030993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029518167139030993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029518
Iteration 2/1000 | Loss: 0.00002196
Iteration 3/1000 | Loss: 0.00001516
Iteration 4/1000 | Loss: 0.00001416
Iteration 5/1000 | Loss: 0.00001353
Iteration 6/1000 | Loss: 0.00001312
Iteration 7/1000 | Loss: 0.00001283
Iteration 8/1000 | Loss: 0.00001265
Iteration 9/1000 | Loss: 0.00001262
Iteration 10/1000 | Loss: 0.00001257
Iteration 11/1000 | Loss: 0.00001256
Iteration 12/1000 | Loss: 0.00001256
Iteration 13/1000 | Loss: 0.00001254
Iteration 14/1000 | Loss: 0.00001252
Iteration 15/1000 | Loss: 0.00001251
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001250
Iteration 18/1000 | Loss: 0.00001250
Iteration 19/1000 | Loss: 0.00001249
Iteration 20/1000 | Loss: 0.00001248
Iteration 21/1000 | Loss: 0.00001245
Iteration 22/1000 | Loss: 0.00001240
Iteration 23/1000 | Loss: 0.00001240
Iteration 24/1000 | Loss: 0.00001239
Iteration 25/1000 | Loss: 0.00001238
Iteration 26/1000 | Loss: 0.00001238
Iteration 27/1000 | Loss: 0.00001237
Iteration 28/1000 | Loss: 0.00001236
Iteration 29/1000 | Loss: 0.00001236
Iteration 30/1000 | Loss: 0.00001236
Iteration 31/1000 | Loss: 0.00001235
Iteration 32/1000 | Loss: 0.00001233
Iteration 33/1000 | Loss: 0.00001233
Iteration 34/1000 | Loss: 0.00001232
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001231
Iteration 37/1000 | Loss: 0.00001231
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001226
Iteration 40/1000 | Loss: 0.00001226
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001225
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001224
Iteration 46/1000 | Loss: 0.00001221
Iteration 47/1000 | Loss: 0.00001221
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001221
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001221
Iteration 53/1000 | Loss: 0.00001221
Iteration 54/1000 | Loss: 0.00001221
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001220
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001220
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001220
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001220
Iteration 67/1000 | Loss: 0.00001220
Iteration 68/1000 | Loss: 0.00001220
Iteration 69/1000 | Loss: 0.00001220
Iteration 70/1000 | Loss: 0.00001220
Iteration 71/1000 | Loss: 0.00001220
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [1.2203421647427604e-05, 1.2203421647427604e-05, 1.2203421647427604e-05, 1.2203421647427604e-05, 1.2203421647427604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2203421647427604e-05

Optimization complete. Final v2v error: 2.9599030017852783 mm

Highest mean error: 3.310110569000244 mm for frame 91

Lowest mean error: 2.8369250297546387 mm for frame 129

Saving results

Total time: 28.192906141281128
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494469
Iteration 2/25 | Loss: 0.00115273
Iteration 3/25 | Loss: 0.00075597
Iteration 4/25 | Loss: 0.00065742
Iteration 5/25 | Loss: 0.00063845
Iteration 6/25 | Loss: 0.00063541
Iteration 7/25 | Loss: 0.00063495
Iteration 8/25 | Loss: 0.00063495
Iteration 9/25 | Loss: 0.00063495
Iteration 10/25 | Loss: 0.00063495
Iteration 11/25 | Loss: 0.00063495
Iteration 12/25 | Loss: 0.00063495
Iteration 13/25 | Loss: 0.00063495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006349486648105085, 0.0006349486648105085, 0.0006349486648105085, 0.0006349486648105085, 0.0006349486648105085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006349486648105085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47261012
Iteration 2/25 | Loss: 0.00027114
Iteration 3/25 | Loss: 0.00027114
Iteration 4/25 | Loss: 0.00027113
Iteration 5/25 | Loss: 0.00027113
Iteration 6/25 | Loss: 0.00027113
Iteration 7/25 | Loss: 0.00027113
Iteration 8/25 | Loss: 0.00027113
Iteration 9/25 | Loss: 0.00027113
Iteration 10/25 | Loss: 0.00027113
Iteration 11/25 | Loss: 0.00027113
Iteration 12/25 | Loss: 0.00027113
Iteration 13/25 | Loss: 0.00027113
Iteration 14/25 | Loss: 0.00027113
Iteration 15/25 | Loss: 0.00027113
Iteration 16/25 | Loss: 0.00027113
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00027113291434943676, 0.00027113291434943676, 0.00027113291434943676, 0.00027113291434943676, 0.00027113291434943676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027113291434943676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027113
Iteration 2/1000 | Loss: 0.00002667
Iteration 3/1000 | Loss: 0.00001887
Iteration 4/1000 | Loss: 0.00001700
Iteration 5/1000 | Loss: 0.00001606
Iteration 6/1000 | Loss: 0.00001538
Iteration 7/1000 | Loss: 0.00001500
Iteration 8/1000 | Loss: 0.00001466
Iteration 9/1000 | Loss: 0.00001447
Iteration 10/1000 | Loss: 0.00001447
Iteration 11/1000 | Loss: 0.00001440
Iteration 12/1000 | Loss: 0.00001436
Iteration 13/1000 | Loss: 0.00001435
Iteration 14/1000 | Loss: 0.00001430
Iteration 15/1000 | Loss: 0.00001430
Iteration 16/1000 | Loss: 0.00001424
Iteration 17/1000 | Loss: 0.00001421
Iteration 18/1000 | Loss: 0.00001416
Iteration 19/1000 | Loss: 0.00001413
Iteration 20/1000 | Loss: 0.00001412
Iteration 21/1000 | Loss: 0.00001412
Iteration 22/1000 | Loss: 0.00001410
Iteration 23/1000 | Loss: 0.00001407
Iteration 24/1000 | Loss: 0.00001405
Iteration 25/1000 | Loss: 0.00001405
Iteration 26/1000 | Loss: 0.00001404
Iteration 27/1000 | Loss: 0.00001403
Iteration 28/1000 | Loss: 0.00001402
Iteration 29/1000 | Loss: 0.00001401
Iteration 30/1000 | Loss: 0.00001401
Iteration 31/1000 | Loss: 0.00001400
Iteration 32/1000 | Loss: 0.00001399
Iteration 33/1000 | Loss: 0.00001399
Iteration 34/1000 | Loss: 0.00001398
Iteration 35/1000 | Loss: 0.00001398
Iteration 36/1000 | Loss: 0.00001398
Iteration 37/1000 | Loss: 0.00001397
Iteration 38/1000 | Loss: 0.00001397
Iteration 39/1000 | Loss: 0.00001397
Iteration 40/1000 | Loss: 0.00001396
Iteration 41/1000 | Loss: 0.00001396
Iteration 42/1000 | Loss: 0.00001396
Iteration 43/1000 | Loss: 0.00001396
Iteration 44/1000 | Loss: 0.00001395
Iteration 45/1000 | Loss: 0.00001395
Iteration 46/1000 | Loss: 0.00001395
Iteration 47/1000 | Loss: 0.00001394
Iteration 48/1000 | Loss: 0.00001394
Iteration 49/1000 | Loss: 0.00001394
Iteration 50/1000 | Loss: 0.00001394
Iteration 51/1000 | Loss: 0.00001393
Iteration 52/1000 | Loss: 0.00001393
Iteration 53/1000 | Loss: 0.00001393
Iteration 54/1000 | Loss: 0.00001393
Iteration 55/1000 | Loss: 0.00001392
Iteration 56/1000 | Loss: 0.00001392
Iteration 57/1000 | Loss: 0.00001392
Iteration 58/1000 | Loss: 0.00001392
Iteration 59/1000 | Loss: 0.00001392
Iteration 60/1000 | Loss: 0.00001392
Iteration 61/1000 | Loss: 0.00001392
Iteration 62/1000 | Loss: 0.00001392
Iteration 63/1000 | Loss: 0.00001392
Iteration 64/1000 | Loss: 0.00001392
Iteration 65/1000 | Loss: 0.00001391
Iteration 66/1000 | Loss: 0.00001391
Iteration 67/1000 | Loss: 0.00001391
Iteration 68/1000 | Loss: 0.00001391
Iteration 69/1000 | Loss: 0.00001391
Iteration 70/1000 | Loss: 0.00001391
Iteration 71/1000 | Loss: 0.00001391
Iteration 72/1000 | Loss: 0.00001391
Iteration 73/1000 | Loss: 0.00001390
Iteration 74/1000 | Loss: 0.00001390
Iteration 75/1000 | Loss: 0.00001390
Iteration 76/1000 | Loss: 0.00001390
Iteration 77/1000 | Loss: 0.00001390
Iteration 78/1000 | Loss: 0.00001390
Iteration 79/1000 | Loss: 0.00001389
Iteration 80/1000 | Loss: 0.00001389
Iteration 81/1000 | Loss: 0.00001389
Iteration 82/1000 | Loss: 0.00001389
Iteration 83/1000 | Loss: 0.00001389
Iteration 84/1000 | Loss: 0.00001389
Iteration 85/1000 | Loss: 0.00001388
Iteration 86/1000 | Loss: 0.00001388
Iteration 87/1000 | Loss: 0.00001388
Iteration 88/1000 | Loss: 0.00001388
Iteration 89/1000 | Loss: 0.00001388
Iteration 90/1000 | Loss: 0.00001388
Iteration 91/1000 | Loss: 0.00001388
Iteration 92/1000 | Loss: 0.00001388
Iteration 93/1000 | Loss: 0.00001387
Iteration 94/1000 | Loss: 0.00001387
Iteration 95/1000 | Loss: 0.00001387
Iteration 96/1000 | Loss: 0.00001387
Iteration 97/1000 | Loss: 0.00001387
Iteration 98/1000 | Loss: 0.00001386
Iteration 99/1000 | Loss: 0.00001386
Iteration 100/1000 | Loss: 0.00001386
Iteration 101/1000 | Loss: 0.00001386
Iteration 102/1000 | Loss: 0.00001385
Iteration 103/1000 | Loss: 0.00001385
Iteration 104/1000 | Loss: 0.00001385
Iteration 105/1000 | Loss: 0.00001385
Iteration 106/1000 | Loss: 0.00001385
Iteration 107/1000 | Loss: 0.00001385
Iteration 108/1000 | Loss: 0.00001385
Iteration 109/1000 | Loss: 0.00001385
Iteration 110/1000 | Loss: 0.00001385
Iteration 111/1000 | Loss: 0.00001385
Iteration 112/1000 | Loss: 0.00001385
Iteration 113/1000 | Loss: 0.00001385
Iteration 114/1000 | Loss: 0.00001385
Iteration 115/1000 | Loss: 0.00001385
Iteration 116/1000 | Loss: 0.00001385
Iteration 117/1000 | Loss: 0.00001384
Iteration 118/1000 | Loss: 0.00001384
Iteration 119/1000 | Loss: 0.00001384
Iteration 120/1000 | Loss: 0.00001384
Iteration 121/1000 | Loss: 0.00001384
Iteration 122/1000 | Loss: 0.00001384
Iteration 123/1000 | Loss: 0.00001384
Iteration 124/1000 | Loss: 0.00001384
Iteration 125/1000 | Loss: 0.00001384
Iteration 126/1000 | Loss: 0.00001384
Iteration 127/1000 | Loss: 0.00001383
Iteration 128/1000 | Loss: 0.00001383
Iteration 129/1000 | Loss: 0.00001383
Iteration 130/1000 | Loss: 0.00001383
Iteration 131/1000 | Loss: 0.00001383
Iteration 132/1000 | Loss: 0.00001383
Iteration 133/1000 | Loss: 0.00001382
Iteration 134/1000 | Loss: 0.00001382
Iteration 135/1000 | Loss: 0.00001382
Iteration 136/1000 | Loss: 0.00001382
Iteration 137/1000 | Loss: 0.00001382
Iteration 138/1000 | Loss: 0.00001381
Iteration 139/1000 | Loss: 0.00001381
Iteration 140/1000 | Loss: 0.00001381
Iteration 141/1000 | Loss: 0.00001381
Iteration 142/1000 | Loss: 0.00001381
Iteration 143/1000 | Loss: 0.00001381
Iteration 144/1000 | Loss: 0.00001381
Iteration 145/1000 | Loss: 0.00001381
Iteration 146/1000 | Loss: 0.00001381
Iteration 147/1000 | Loss: 0.00001381
Iteration 148/1000 | Loss: 0.00001381
Iteration 149/1000 | Loss: 0.00001380
Iteration 150/1000 | Loss: 0.00001380
Iteration 151/1000 | Loss: 0.00001380
Iteration 152/1000 | Loss: 0.00001380
Iteration 153/1000 | Loss: 0.00001380
Iteration 154/1000 | Loss: 0.00001380
Iteration 155/1000 | Loss: 0.00001380
Iteration 156/1000 | Loss: 0.00001380
Iteration 157/1000 | Loss: 0.00001380
Iteration 158/1000 | Loss: 0.00001380
Iteration 159/1000 | Loss: 0.00001380
Iteration 160/1000 | Loss: 0.00001380
Iteration 161/1000 | Loss: 0.00001380
Iteration 162/1000 | Loss: 0.00001379
Iteration 163/1000 | Loss: 0.00001379
Iteration 164/1000 | Loss: 0.00001379
Iteration 165/1000 | Loss: 0.00001379
Iteration 166/1000 | Loss: 0.00001379
Iteration 167/1000 | Loss: 0.00001379
Iteration 168/1000 | Loss: 0.00001379
Iteration 169/1000 | Loss: 0.00001379
Iteration 170/1000 | Loss: 0.00001379
Iteration 171/1000 | Loss: 0.00001379
Iteration 172/1000 | Loss: 0.00001379
Iteration 173/1000 | Loss: 0.00001379
Iteration 174/1000 | Loss: 0.00001379
Iteration 175/1000 | Loss: 0.00001378
Iteration 176/1000 | Loss: 0.00001378
Iteration 177/1000 | Loss: 0.00001378
Iteration 178/1000 | Loss: 0.00001378
Iteration 179/1000 | Loss: 0.00001378
Iteration 180/1000 | Loss: 0.00001378
Iteration 181/1000 | Loss: 0.00001378
Iteration 182/1000 | Loss: 0.00001378
Iteration 183/1000 | Loss: 0.00001378
Iteration 184/1000 | Loss: 0.00001378
Iteration 185/1000 | Loss: 0.00001377
Iteration 186/1000 | Loss: 0.00001377
Iteration 187/1000 | Loss: 0.00001377
Iteration 188/1000 | Loss: 0.00001377
Iteration 189/1000 | Loss: 0.00001377
Iteration 190/1000 | Loss: 0.00001377
Iteration 191/1000 | Loss: 0.00001376
Iteration 192/1000 | Loss: 0.00001376
Iteration 193/1000 | Loss: 0.00001376
Iteration 194/1000 | Loss: 0.00001376
Iteration 195/1000 | Loss: 0.00001376
Iteration 196/1000 | Loss: 0.00001376
Iteration 197/1000 | Loss: 0.00001376
Iteration 198/1000 | Loss: 0.00001376
Iteration 199/1000 | Loss: 0.00001376
Iteration 200/1000 | Loss: 0.00001376
Iteration 201/1000 | Loss: 0.00001376
Iteration 202/1000 | Loss: 0.00001376
Iteration 203/1000 | Loss: 0.00001376
Iteration 204/1000 | Loss: 0.00001376
Iteration 205/1000 | Loss: 0.00001376
Iteration 206/1000 | Loss: 0.00001376
Iteration 207/1000 | Loss: 0.00001376
Iteration 208/1000 | Loss: 0.00001376
Iteration 209/1000 | Loss: 0.00001376
Iteration 210/1000 | Loss: 0.00001376
Iteration 211/1000 | Loss: 0.00001376
Iteration 212/1000 | Loss: 0.00001376
Iteration 213/1000 | Loss: 0.00001376
Iteration 214/1000 | Loss: 0.00001376
Iteration 215/1000 | Loss: 0.00001376
Iteration 216/1000 | Loss: 0.00001376
Iteration 217/1000 | Loss: 0.00001376
Iteration 218/1000 | Loss: 0.00001376
Iteration 219/1000 | Loss: 0.00001376
Iteration 220/1000 | Loss: 0.00001376
Iteration 221/1000 | Loss: 0.00001376
Iteration 222/1000 | Loss: 0.00001376
Iteration 223/1000 | Loss: 0.00001376
Iteration 224/1000 | Loss: 0.00001376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.3756673070020042e-05, 1.3756673070020042e-05, 1.3756673070020042e-05, 1.3756673070020042e-05, 1.3756673070020042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3756673070020042e-05

Optimization complete. Final v2v error: 3.0895793437957764 mm

Highest mean error: 4.290849685668945 mm for frame 90

Lowest mean error: 2.8535826206207275 mm for frame 45

Saving results

Total time: 42.19228959083557
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760901
Iteration 2/25 | Loss: 0.00184097
Iteration 3/25 | Loss: 0.00133903
Iteration 4/25 | Loss: 0.00127254
Iteration 5/25 | Loss: 0.00116549
Iteration 6/25 | Loss: 0.00107707
Iteration 7/25 | Loss: 0.00102180
Iteration 8/25 | Loss: 0.00100592
Iteration 9/25 | Loss: 0.00100321
Iteration 10/25 | Loss: 0.00100129
Iteration 11/25 | Loss: 0.00100091
Iteration 12/25 | Loss: 0.00100033
Iteration 13/25 | Loss: 0.00099750
Iteration 14/25 | Loss: 0.00099614
Iteration 15/25 | Loss: 0.00099592
Iteration 16/25 | Loss: 0.00099559
Iteration 17/25 | Loss: 0.00099615
Iteration 18/25 | Loss: 0.00099557
Iteration 19/25 | Loss: 0.00099576
Iteration 20/25 | Loss: 0.00099550
Iteration 21/25 | Loss: 0.00099572
Iteration 22/25 | Loss: 0.00099548
Iteration 23/25 | Loss: 0.00099589
Iteration 24/25 | Loss: 0.00099540
Iteration 25/25 | Loss: 0.00099589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42735994
Iteration 2/25 | Loss: 0.00062665
Iteration 3/25 | Loss: 0.00062663
Iteration 4/25 | Loss: 0.00062663
Iteration 5/25 | Loss: 0.00062663
Iteration 6/25 | Loss: 0.00062663
Iteration 7/25 | Loss: 0.00062663
Iteration 8/25 | Loss: 0.00062663
Iteration 9/25 | Loss: 0.00062663
Iteration 10/25 | Loss: 0.00062663
Iteration 11/25 | Loss: 0.00062663
Iteration 12/25 | Loss: 0.00062663
Iteration 13/25 | Loss: 0.00062663
Iteration 14/25 | Loss: 0.00062663
Iteration 15/25 | Loss: 0.00062663
Iteration 16/25 | Loss: 0.00062663
Iteration 17/25 | Loss: 0.00062663
Iteration 18/25 | Loss: 0.00062663
Iteration 19/25 | Loss: 0.00062663
Iteration 20/25 | Loss: 0.00062663
Iteration 21/25 | Loss: 0.00062663
Iteration 22/25 | Loss: 0.00062663
Iteration 23/25 | Loss: 0.00062663
Iteration 24/25 | Loss: 0.00062663
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006266297423280776, 0.0006266297423280776, 0.0006266297423280776, 0.0006266297423280776, 0.0006266297423280776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006266297423280776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062663
Iteration 2/1000 | Loss: 0.00008717
Iteration 3/1000 | Loss: 0.00006425
Iteration 4/1000 | Loss: 0.00006371
Iteration 5/1000 | Loss: 0.00005931
Iteration 6/1000 | Loss: 0.00005886
Iteration 7/1000 | Loss: 0.00005511
Iteration 8/1000 | Loss: 0.00005308
Iteration 9/1000 | Loss: 0.00005562
Iteration 10/1000 | Loss: 0.00005739
Iteration 11/1000 | Loss: 0.00005510
Iteration 12/1000 | Loss: 0.00005890
Iteration 13/1000 | Loss: 0.00005662
Iteration 14/1000 | Loss: 0.00005312
Iteration 15/1000 | Loss: 0.00005850
Iteration 16/1000 | Loss: 0.00005587
Iteration 17/1000 | Loss: 0.00005260
Iteration 18/1000 | Loss: 0.00006064
Iteration 19/1000 | Loss: 0.00005257
Iteration 20/1000 | Loss: 0.00005168
Iteration 21/1000 | Loss: 0.00005117
Iteration 22/1000 | Loss: 0.00005070
Iteration 23/1000 | Loss: 0.00005039
Iteration 24/1000 | Loss: 0.00005036
Iteration 25/1000 | Loss: 0.00005020
Iteration 26/1000 | Loss: 0.00005019
Iteration 27/1000 | Loss: 0.00005011
Iteration 28/1000 | Loss: 0.00005011
Iteration 29/1000 | Loss: 0.00005011
Iteration 30/1000 | Loss: 0.00005011
Iteration 31/1000 | Loss: 0.00005011
Iteration 32/1000 | Loss: 0.00005011
Iteration 33/1000 | Loss: 0.00005011
Iteration 34/1000 | Loss: 0.00005011
Iteration 35/1000 | Loss: 0.00005011
Iteration 36/1000 | Loss: 0.00005011
Iteration 37/1000 | Loss: 0.00005010
Iteration 38/1000 | Loss: 0.00005009
Iteration 39/1000 | Loss: 0.00005009
Iteration 40/1000 | Loss: 0.00005009
Iteration 41/1000 | Loss: 0.00005008
Iteration 42/1000 | Loss: 0.00005008
Iteration 43/1000 | Loss: 0.00005007
Iteration 44/1000 | Loss: 0.00005007
Iteration 45/1000 | Loss: 0.00005007
Iteration 46/1000 | Loss: 0.00005007
Iteration 47/1000 | Loss: 0.00005007
Iteration 48/1000 | Loss: 0.00005007
Iteration 49/1000 | Loss: 0.00005007
Iteration 50/1000 | Loss: 0.00005007
Iteration 51/1000 | Loss: 0.00005006
Iteration 52/1000 | Loss: 0.00005006
Iteration 53/1000 | Loss: 0.00005006
Iteration 54/1000 | Loss: 0.00005006
Iteration 55/1000 | Loss: 0.00005005
Iteration 56/1000 | Loss: 0.00005005
Iteration 57/1000 | Loss: 0.00005005
Iteration 58/1000 | Loss: 0.00005005
Iteration 59/1000 | Loss: 0.00005005
Iteration 60/1000 | Loss: 0.00005004
Iteration 61/1000 | Loss: 0.00005004
Iteration 62/1000 | Loss: 0.00005004
Iteration 63/1000 | Loss: 0.00005004
Iteration 64/1000 | Loss: 0.00005004
Iteration 65/1000 | Loss: 0.00005004
Iteration 66/1000 | Loss: 0.00005004
Iteration 67/1000 | Loss: 0.00005004
Iteration 68/1000 | Loss: 0.00005004
Iteration 69/1000 | Loss: 0.00005004
Iteration 70/1000 | Loss: 0.00005004
Iteration 71/1000 | Loss: 0.00005004
Iteration 72/1000 | Loss: 0.00005004
Iteration 73/1000 | Loss: 0.00005003
Iteration 74/1000 | Loss: 0.00005003
Iteration 75/1000 | Loss: 0.00005003
Iteration 76/1000 | Loss: 0.00005003
Iteration 77/1000 | Loss: 0.00005002
Iteration 78/1000 | Loss: 0.00005002
Iteration 79/1000 | Loss: 0.00005002
Iteration 80/1000 | Loss: 0.00005002
Iteration 81/1000 | Loss: 0.00005002
Iteration 82/1000 | Loss: 0.00005002
Iteration 83/1000 | Loss: 0.00005002
Iteration 84/1000 | Loss: 0.00005001
Iteration 85/1000 | Loss: 0.00005001
Iteration 86/1000 | Loss: 0.00005001
Iteration 87/1000 | Loss: 0.00005001
Iteration 88/1000 | Loss: 0.00005001
Iteration 89/1000 | Loss: 0.00005000
Iteration 90/1000 | Loss: 0.00005000
Iteration 91/1000 | Loss: 0.00005000
Iteration 92/1000 | Loss: 0.00005000
Iteration 93/1000 | Loss: 0.00005000
Iteration 94/1000 | Loss: 0.00005000
Iteration 95/1000 | Loss: 0.00005000
Iteration 96/1000 | Loss: 0.00005000
Iteration 97/1000 | Loss: 0.00005000
Iteration 98/1000 | Loss: 0.00005000
Iteration 99/1000 | Loss: 0.00005000
Iteration 100/1000 | Loss: 0.00005000
Iteration 101/1000 | Loss: 0.00005000
Iteration 102/1000 | Loss: 0.00005000
Iteration 103/1000 | Loss: 0.00005000
Iteration 104/1000 | Loss: 0.00005000
Iteration 105/1000 | Loss: 0.00005000
Iteration 106/1000 | Loss: 0.00005000
Iteration 107/1000 | Loss: 0.00005000
Iteration 108/1000 | Loss: 0.00005000
Iteration 109/1000 | Loss: 0.00005000
Iteration 110/1000 | Loss: 0.00005000
Iteration 111/1000 | Loss: 0.00005000
Iteration 112/1000 | Loss: 0.00005000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [4.999928569304757e-05, 4.999928569304757e-05, 4.999928569304757e-05, 4.999928569304757e-05, 4.999928569304757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.999928569304757e-05

Optimization complete. Final v2v error: 5.6523661613464355 mm

Highest mean error: 6.713179111480713 mm for frame 45

Lowest mean error: 4.859531402587891 mm for frame 197

Saving results

Total time: 96.46005821228027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00731707
Iteration 2/25 | Loss: 0.00133863
Iteration 3/25 | Loss: 0.00084617
Iteration 4/25 | Loss: 0.00076486
Iteration 5/25 | Loss: 0.00074663
Iteration 6/25 | Loss: 0.00074360
Iteration 7/25 | Loss: 0.00074327
Iteration 8/25 | Loss: 0.00074327
Iteration 9/25 | Loss: 0.00074327
Iteration 10/25 | Loss: 0.00074327
Iteration 11/25 | Loss: 0.00074327
Iteration 12/25 | Loss: 0.00074327
Iteration 13/25 | Loss: 0.00074327
Iteration 14/25 | Loss: 0.00074327
Iteration 15/25 | Loss: 0.00074327
Iteration 16/25 | Loss: 0.00074327
Iteration 17/25 | Loss: 0.00074327
Iteration 18/25 | Loss: 0.00074327
Iteration 19/25 | Loss: 0.00074327
Iteration 20/25 | Loss: 0.00074327
Iteration 21/25 | Loss: 0.00074327
Iteration 22/25 | Loss: 0.00074327
Iteration 23/25 | Loss: 0.00074327
Iteration 24/25 | Loss: 0.00074327
Iteration 25/25 | Loss: 0.00074327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51319337
Iteration 2/25 | Loss: 0.00054229
Iteration 3/25 | Loss: 0.00054229
Iteration 4/25 | Loss: 0.00054229
Iteration 5/25 | Loss: 0.00054229
Iteration 6/25 | Loss: 0.00054229
Iteration 7/25 | Loss: 0.00054229
Iteration 8/25 | Loss: 0.00054229
Iteration 9/25 | Loss: 0.00054229
Iteration 10/25 | Loss: 0.00054229
Iteration 11/25 | Loss: 0.00054229
Iteration 12/25 | Loss: 0.00054229
Iteration 13/25 | Loss: 0.00054229
Iteration 14/25 | Loss: 0.00054229
Iteration 15/25 | Loss: 0.00054229
Iteration 16/25 | Loss: 0.00054229
Iteration 17/25 | Loss: 0.00054229
Iteration 18/25 | Loss: 0.00054229
Iteration 19/25 | Loss: 0.00054229
Iteration 20/25 | Loss: 0.00054229
Iteration 21/25 | Loss: 0.00054229
Iteration 22/25 | Loss: 0.00054229
Iteration 23/25 | Loss: 0.00054229
Iteration 24/25 | Loss: 0.00054229
Iteration 25/25 | Loss: 0.00054229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054229
Iteration 2/1000 | Loss: 0.00003646
Iteration 3/1000 | Loss: 0.00002623
Iteration 4/1000 | Loss: 0.00002240
Iteration 5/1000 | Loss: 0.00002118
Iteration 6/1000 | Loss: 0.00002027
Iteration 7/1000 | Loss: 0.00001974
Iteration 8/1000 | Loss: 0.00001939
Iteration 9/1000 | Loss: 0.00001913
Iteration 10/1000 | Loss: 0.00001883
Iteration 11/1000 | Loss: 0.00001863
Iteration 12/1000 | Loss: 0.00001859
Iteration 13/1000 | Loss: 0.00001852
Iteration 14/1000 | Loss: 0.00001852
Iteration 15/1000 | Loss: 0.00001851
Iteration 16/1000 | Loss: 0.00001848
Iteration 17/1000 | Loss: 0.00001848
Iteration 18/1000 | Loss: 0.00001848
Iteration 19/1000 | Loss: 0.00001847
Iteration 20/1000 | Loss: 0.00001846
Iteration 21/1000 | Loss: 0.00001845
Iteration 22/1000 | Loss: 0.00001845
Iteration 23/1000 | Loss: 0.00001843
Iteration 24/1000 | Loss: 0.00001842
Iteration 25/1000 | Loss: 0.00001842
Iteration 26/1000 | Loss: 0.00001841
Iteration 27/1000 | Loss: 0.00001839
Iteration 28/1000 | Loss: 0.00001836
Iteration 29/1000 | Loss: 0.00001833
Iteration 30/1000 | Loss: 0.00001831
Iteration 31/1000 | Loss: 0.00001831
Iteration 32/1000 | Loss: 0.00001830
Iteration 33/1000 | Loss: 0.00001830
Iteration 34/1000 | Loss: 0.00001830
Iteration 35/1000 | Loss: 0.00001829
Iteration 36/1000 | Loss: 0.00001829
Iteration 37/1000 | Loss: 0.00001828
Iteration 38/1000 | Loss: 0.00001828
Iteration 39/1000 | Loss: 0.00001828
Iteration 40/1000 | Loss: 0.00001827
Iteration 41/1000 | Loss: 0.00001827
Iteration 42/1000 | Loss: 0.00001827
Iteration 43/1000 | Loss: 0.00001827
Iteration 44/1000 | Loss: 0.00001826
Iteration 45/1000 | Loss: 0.00001826
Iteration 46/1000 | Loss: 0.00001825
Iteration 47/1000 | Loss: 0.00001825
Iteration 48/1000 | Loss: 0.00001825
Iteration 49/1000 | Loss: 0.00001825
Iteration 50/1000 | Loss: 0.00001825
Iteration 51/1000 | Loss: 0.00001825
Iteration 52/1000 | Loss: 0.00001824
Iteration 53/1000 | Loss: 0.00001824
Iteration 54/1000 | Loss: 0.00001824
Iteration 55/1000 | Loss: 0.00001824
Iteration 56/1000 | Loss: 0.00001824
Iteration 57/1000 | Loss: 0.00001824
Iteration 58/1000 | Loss: 0.00001823
Iteration 59/1000 | Loss: 0.00001823
Iteration 60/1000 | Loss: 0.00001823
Iteration 61/1000 | Loss: 0.00001823
Iteration 62/1000 | Loss: 0.00001822
Iteration 63/1000 | Loss: 0.00001822
Iteration 64/1000 | Loss: 0.00001822
Iteration 65/1000 | Loss: 0.00001822
Iteration 66/1000 | Loss: 0.00001822
Iteration 67/1000 | Loss: 0.00001822
Iteration 68/1000 | Loss: 0.00001821
Iteration 69/1000 | Loss: 0.00001821
Iteration 70/1000 | Loss: 0.00001821
Iteration 71/1000 | Loss: 0.00001821
Iteration 72/1000 | Loss: 0.00001821
Iteration 73/1000 | Loss: 0.00001821
Iteration 74/1000 | Loss: 0.00001821
Iteration 75/1000 | Loss: 0.00001821
Iteration 76/1000 | Loss: 0.00001821
Iteration 77/1000 | Loss: 0.00001821
Iteration 78/1000 | Loss: 0.00001821
Iteration 79/1000 | Loss: 0.00001821
Iteration 80/1000 | Loss: 0.00001821
Iteration 81/1000 | Loss: 0.00001821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.820738907554187e-05, 1.820738907554187e-05, 1.820738907554187e-05, 1.820738907554187e-05, 1.820738907554187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.820738907554187e-05

Optimization complete. Final v2v error: 3.5374646186828613 mm

Highest mean error: 3.9614248275756836 mm for frame 51

Lowest mean error: 3.041985273361206 mm for frame 0

Saving results

Total time: 36.837719678878784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099286
Iteration 2/25 | Loss: 0.00210424
Iteration 3/25 | Loss: 0.00123017
Iteration 4/25 | Loss: 0.00103651
Iteration 5/25 | Loss: 0.00097107
Iteration 6/25 | Loss: 0.00096723
Iteration 7/25 | Loss: 0.00089034
Iteration 8/25 | Loss: 0.00087636
Iteration 9/25 | Loss: 0.00084605
Iteration 10/25 | Loss: 0.00082105
Iteration 11/25 | Loss: 0.00078150
Iteration 12/25 | Loss: 0.00077635
Iteration 13/25 | Loss: 0.00076073
Iteration 14/25 | Loss: 0.00075723
Iteration 15/25 | Loss: 0.00076056
Iteration 16/25 | Loss: 0.00075573
Iteration 17/25 | Loss: 0.00075300
Iteration 18/25 | Loss: 0.00075667
Iteration 19/25 | Loss: 0.00076232
Iteration 20/25 | Loss: 0.00075402
Iteration 21/25 | Loss: 0.00075275
Iteration 22/25 | Loss: 0.00074867
Iteration 23/25 | Loss: 0.00075276
Iteration 24/25 | Loss: 0.00074874
Iteration 25/25 | Loss: 0.00075150

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61723542
Iteration 2/25 | Loss: 0.00114044
Iteration 3/25 | Loss: 0.00112017
Iteration 4/25 | Loss: 0.00112017
Iteration 5/25 | Loss: 0.00112017
Iteration 6/25 | Loss: 0.00112017
Iteration 7/25 | Loss: 0.00112017
Iteration 8/25 | Loss: 0.00112017
Iteration 9/25 | Loss: 0.00112017
Iteration 10/25 | Loss: 0.00112017
Iteration 11/25 | Loss: 0.00112017
Iteration 12/25 | Loss: 0.00112017
Iteration 13/25 | Loss: 0.00112017
Iteration 14/25 | Loss: 0.00112017
Iteration 15/25 | Loss: 0.00112017
Iteration 16/25 | Loss: 0.00112017
Iteration 17/25 | Loss: 0.00112017
Iteration 18/25 | Loss: 0.00112017
Iteration 19/25 | Loss: 0.00112017
Iteration 20/25 | Loss: 0.00112017
Iteration 21/25 | Loss: 0.00112017
Iteration 22/25 | Loss: 0.00112017
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001120168948546052, 0.001120168948546052, 0.001120168948546052, 0.001120168948546052, 0.001120168948546052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001120168948546052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112017
Iteration 2/1000 | Loss: 0.00037673
Iteration 3/1000 | Loss: 0.00043289
Iteration 4/1000 | Loss: 0.00025789
Iteration 5/1000 | Loss: 0.00008229
Iteration 6/1000 | Loss: 0.00010614
Iteration 7/1000 | Loss: 0.00009170
Iteration 8/1000 | Loss: 0.00039015
Iteration 9/1000 | Loss: 0.00043927
Iteration 10/1000 | Loss: 0.00011379
Iteration 11/1000 | Loss: 0.00008194
Iteration 12/1000 | Loss: 0.00008380
Iteration 13/1000 | Loss: 0.00006955
Iteration 14/1000 | Loss: 0.00006118
Iteration 15/1000 | Loss: 0.00005587
Iteration 16/1000 | Loss: 0.00010322
Iteration 17/1000 | Loss: 0.00006211
Iteration 18/1000 | Loss: 0.00006088
Iteration 19/1000 | Loss: 0.00009078
Iteration 20/1000 | Loss: 0.00005305
Iteration 21/1000 | Loss: 0.00008809
Iteration 22/1000 | Loss: 0.00008170
Iteration 23/1000 | Loss: 0.00014624
Iteration 24/1000 | Loss: 0.00035091
Iteration 25/1000 | Loss: 0.00016105
Iteration 26/1000 | Loss: 0.00011417
Iteration 27/1000 | Loss: 0.00011375
Iteration 28/1000 | Loss: 0.00010471
Iteration 29/1000 | Loss: 0.00011437
Iteration 30/1000 | Loss: 0.00009071
Iteration 31/1000 | Loss: 0.00004419
Iteration 32/1000 | Loss: 0.00007605
Iteration 33/1000 | Loss: 0.00027094
Iteration 34/1000 | Loss: 0.00011052
Iteration 35/1000 | Loss: 0.00028701
Iteration 36/1000 | Loss: 0.00082932
Iteration 37/1000 | Loss: 0.00030157
Iteration 38/1000 | Loss: 0.00022907
Iteration 39/1000 | Loss: 0.00008788
Iteration 40/1000 | Loss: 0.00005936
Iteration 41/1000 | Loss: 0.00003989
Iteration 42/1000 | Loss: 0.00005352
Iteration 43/1000 | Loss: 0.00027369
Iteration 44/1000 | Loss: 0.00029943
Iteration 45/1000 | Loss: 0.00032332
Iteration 46/1000 | Loss: 0.00006058
Iteration 47/1000 | Loss: 0.00026735
Iteration 48/1000 | Loss: 0.00041952
Iteration 49/1000 | Loss: 0.00025831
Iteration 50/1000 | Loss: 0.00015116
Iteration 51/1000 | Loss: 0.00003618
Iteration 52/1000 | Loss: 0.00004413
Iteration 53/1000 | Loss: 0.00003972
Iteration 54/1000 | Loss: 0.00004328
Iteration 55/1000 | Loss: 0.00002691
Iteration 56/1000 | Loss: 0.00002843
Iteration 57/1000 | Loss: 0.00004744
Iteration 58/1000 | Loss: 0.00004504
Iteration 59/1000 | Loss: 0.00004387
Iteration 60/1000 | Loss: 0.00004229
Iteration 61/1000 | Loss: 0.00022340
Iteration 62/1000 | Loss: 0.00020767
Iteration 63/1000 | Loss: 0.00015350
Iteration 64/1000 | Loss: 0.00005502
Iteration 65/1000 | Loss: 0.00003394
Iteration 66/1000 | Loss: 0.00003135
Iteration 67/1000 | Loss: 0.00024157
Iteration 68/1000 | Loss: 0.00029409
Iteration 69/1000 | Loss: 0.00023669
Iteration 70/1000 | Loss: 0.00029622
Iteration 71/1000 | Loss: 0.00020385
Iteration 72/1000 | Loss: 0.00003116
Iteration 73/1000 | Loss: 0.00002755
Iteration 74/1000 | Loss: 0.00003561
Iteration 75/1000 | Loss: 0.00022854
Iteration 76/1000 | Loss: 0.00032117
Iteration 77/1000 | Loss: 0.00029954
Iteration 78/1000 | Loss: 0.00022230
Iteration 79/1000 | Loss: 0.00022271
Iteration 80/1000 | Loss: 0.00021300
Iteration 81/1000 | Loss: 0.00003355
Iteration 82/1000 | Loss: 0.00002689
Iteration 83/1000 | Loss: 0.00002507
Iteration 84/1000 | Loss: 0.00002216
Iteration 85/1000 | Loss: 0.00001954
Iteration 86/1000 | Loss: 0.00001865
Iteration 87/1000 | Loss: 0.00001825
Iteration 88/1000 | Loss: 0.00001810
Iteration 89/1000 | Loss: 0.00001794
Iteration 90/1000 | Loss: 0.00001787
Iteration 91/1000 | Loss: 0.00001784
Iteration 92/1000 | Loss: 0.00001772
Iteration 93/1000 | Loss: 0.00001770
Iteration 94/1000 | Loss: 0.00001765
Iteration 95/1000 | Loss: 0.00001762
Iteration 96/1000 | Loss: 0.00001761
Iteration 97/1000 | Loss: 0.00001761
Iteration 98/1000 | Loss: 0.00001760
Iteration 99/1000 | Loss: 0.00001760
Iteration 100/1000 | Loss: 0.00001759
Iteration 101/1000 | Loss: 0.00001759
Iteration 102/1000 | Loss: 0.00001758
Iteration 103/1000 | Loss: 0.00001758
Iteration 104/1000 | Loss: 0.00001757
Iteration 105/1000 | Loss: 0.00001757
Iteration 106/1000 | Loss: 0.00001757
Iteration 107/1000 | Loss: 0.00001756
Iteration 108/1000 | Loss: 0.00001756
Iteration 109/1000 | Loss: 0.00001755
Iteration 110/1000 | Loss: 0.00001755
Iteration 111/1000 | Loss: 0.00001754
Iteration 112/1000 | Loss: 0.00001754
Iteration 113/1000 | Loss: 0.00001753
Iteration 114/1000 | Loss: 0.00001753
Iteration 115/1000 | Loss: 0.00001752
Iteration 116/1000 | Loss: 0.00001752
Iteration 117/1000 | Loss: 0.00001752
Iteration 118/1000 | Loss: 0.00001751
Iteration 119/1000 | Loss: 0.00001751
Iteration 120/1000 | Loss: 0.00001751
Iteration 121/1000 | Loss: 0.00001750
Iteration 122/1000 | Loss: 0.00001750
Iteration 123/1000 | Loss: 0.00001750
Iteration 124/1000 | Loss: 0.00001750
Iteration 125/1000 | Loss: 0.00001750
Iteration 126/1000 | Loss: 0.00001749
Iteration 127/1000 | Loss: 0.00001749
Iteration 128/1000 | Loss: 0.00001749
Iteration 129/1000 | Loss: 0.00001749
Iteration 130/1000 | Loss: 0.00001749
Iteration 131/1000 | Loss: 0.00001748
Iteration 132/1000 | Loss: 0.00001748
Iteration 133/1000 | Loss: 0.00001747
Iteration 134/1000 | Loss: 0.00001747
Iteration 135/1000 | Loss: 0.00001747
Iteration 136/1000 | Loss: 0.00001747
Iteration 137/1000 | Loss: 0.00001746
Iteration 138/1000 | Loss: 0.00001746
Iteration 139/1000 | Loss: 0.00001746
Iteration 140/1000 | Loss: 0.00001746
Iteration 141/1000 | Loss: 0.00001746
Iteration 142/1000 | Loss: 0.00001746
Iteration 143/1000 | Loss: 0.00001746
Iteration 144/1000 | Loss: 0.00001746
Iteration 145/1000 | Loss: 0.00001746
Iteration 146/1000 | Loss: 0.00001746
Iteration 147/1000 | Loss: 0.00001745
Iteration 148/1000 | Loss: 0.00001745
Iteration 149/1000 | Loss: 0.00001745
Iteration 150/1000 | Loss: 0.00001745
Iteration 151/1000 | Loss: 0.00001745
Iteration 152/1000 | Loss: 0.00001745
Iteration 153/1000 | Loss: 0.00001745
Iteration 154/1000 | Loss: 0.00001745
Iteration 155/1000 | Loss: 0.00001745
Iteration 156/1000 | Loss: 0.00001745
Iteration 157/1000 | Loss: 0.00001745
Iteration 158/1000 | Loss: 0.00001745
Iteration 159/1000 | Loss: 0.00001745
Iteration 160/1000 | Loss: 0.00001745
Iteration 161/1000 | Loss: 0.00001745
Iteration 162/1000 | Loss: 0.00001745
Iteration 163/1000 | Loss: 0.00001745
Iteration 164/1000 | Loss: 0.00001745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.7453092368668877e-05, 1.7453092368668877e-05, 1.7453092368668877e-05, 1.7453092368668877e-05, 1.7453092368668877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7453092368668877e-05

Optimization complete. Final v2v error: 3.533843994140625 mm

Highest mean error: 4.38257360458374 mm for frame 98

Lowest mean error: 3.057809352874756 mm for frame 91

Saving results

Total time: 171.86467623710632
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461352
Iteration 2/25 | Loss: 0.00117628
Iteration 3/25 | Loss: 0.00081081
Iteration 4/25 | Loss: 0.00075246
Iteration 5/25 | Loss: 0.00074101
Iteration 6/25 | Loss: 0.00073863
Iteration 7/25 | Loss: 0.00073785
Iteration 8/25 | Loss: 0.00073785
Iteration 9/25 | Loss: 0.00073785
Iteration 10/25 | Loss: 0.00073785
Iteration 11/25 | Loss: 0.00073785
Iteration 12/25 | Loss: 0.00073785
Iteration 13/25 | Loss: 0.00073785
Iteration 14/25 | Loss: 0.00073785
Iteration 15/25 | Loss: 0.00073785
Iteration 16/25 | Loss: 0.00073785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007378481677733362, 0.0007378481677733362, 0.0007378481677733362, 0.0007378481677733362, 0.0007378481677733362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007378481677733362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21322715
Iteration 2/25 | Loss: 0.00024994
Iteration 3/25 | Loss: 0.00024994
Iteration 4/25 | Loss: 0.00024994
Iteration 5/25 | Loss: 0.00024994
Iteration 6/25 | Loss: 0.00024994
Iteration 7/25 | Loss: 0.00024994
Iteration 8/25 | Loss: 0.00024994
Iteration 9/25 | Loss: 0.00024994
Iteration 10/25 | Loss: 0.00024994
Iteration 11/25 | Loss: 0.00024994
Iteration 12/25 | Loss: 0.00024994
Iteration 13/25 | Loss: 0.00024994
Iteration 14/25 | Loss: 0.00024994
Iteration 15/25 | Loss: 0.00024994
Iteration 16/25 | Loss: 0.00024994
Iteration 17/25 | Loss: 0.00024994
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00024993502302095294, 0.00024993502302095294, 0.00024993502302095294, 0.00024993502302095294, 0.00024993502302095294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024993502302095294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024994
Iteration 2/1000 | Loss: 0.00006436
Iteration 3/1000 | Loss: 0.00004550
Iteration 4/1000 | Loss: 0.00003741
Iteration 5/1000 | Loss: 0.00003520
Iteration 6/1000 | Loss: 0.00003392
Iteration 7/1000 | Loss: 0.00003285
Iteration 8/1000 | Loss: 0.00003226
Iteration 9/1000 | Loss: 0.00003168
Iteration 10/1000 | Loss: 0.00003126
Iteration 11/1000 | Loss: 0.00003091
Iteration 12/1000 | Loss: 0.00003065
Iteration 13/1000 | Loss: 0.00003062
Iteration 14/1000 | Loss: 0.00003045
Iteration 15/1000 | Loss: 0.00003044
Iteration 16/1000 | Loss: 0.00003031
Iteration 17/1000 | Loss: 0.00003019
Iteration 18/1000 | Loss: 0.00003018
Iteration 19/1000 | Loss: 0.00003006
Iteration 20/1000 | Loss: 0.00002993
Iteration 21/1000 | Loss: 0.00002993
Iteration 22/1000 | Loss: 0.00002991
Iteration 23/1000 | Loss: 0.00002986
Iteration 24/1000 | Loss: 0.00002985
Iteration 25/1000 | Loss: 0.00002980
Iteration 26/1000 | Loss: 0.00002979
Iteration 27/1000 | Loss: 0.00002979
Iteration 28/1000 | Loss: 0.00002978
Iteration 29/1000 | Loss: 0.00002978
Iteration 30/1000 | Loss: 0.00002977
Iteration 31/1000 | Loss: 0.00002977
Iteration 32/1000 | Loss: 0.00002976
Iteration 33/1000 | Loss: 0.00002976
Iteration 34/1000 | Loss: 0.00002975
Iteration 35/1000 | Loss: 0.00002975
Iteration 36/1000 | Loss: 0.00002975
Iteration 37/1000 | Loss: 0.00002975
Iteration 38/1000 | Loss: 0.00002974
Iteration 39/1000 | Loss: 0.00002974
Iteration 40/1000 | Loss: 0.00002974
Iteration 41/1000 | Loss: 0.00002974
Iteration 42/1000 | Loss: 0.00002973
Iteration 43/1000 | Loss: 0.00002973
Iteration 44/1000 | Loss: 0.00002973
Iteration 45/1000 | Loss: 0.00002972
Iteration 46/1000 | Loss: 0.00002971
Iteration 47/1000 | Loss: 0.00002971
Iteration 48/1000 | Loss: 0.00002970
Iteration 49/1000 | Loss: 0.00002969
Iteration 50/1000 | Loss: 0.00002969
Iteration 51/1000 | Loss: 0.00002969
Iteration 52/1000 | Loss: 0.00002969
Iteration 53/1000 | Loss: 0.00002969
Iteration 54/1000 | Loss: 0.00002969
Iteration 55/1000 | Loss: 0.00002969
Iteration 56/1000 | Loss: 0.00002968
Iteration 57/1000 | Loss: 0.00002968
Iteration 58/1000 | Loss: 0.00002968
Iteration 59/1000 | Loss: 0.00002967
Iteration 60/1000 | Loss: 0.00002967
Iteration 61/1000 | Loss: 0.00002966
Iteration 62/1000 | Loss: 0.00002966
Iteration 63/1000 | Loss: 0.00002964
Iteration 64/1000 | Loss: 0.00002964
Iteration 65/1000 | Loss: 0.00002964
Iteration 66/1000 | Loss: 0.00002964
Iteration 67/1000 | Loss: 0.00002964
Iteration 68/1000 | Loss: 0.00002964
Iteration 69/1000 | Loss: 0.00002963
Iteration 70/1000 | Loss: 0.00002963
Iteration 71/1000 | Loss: 0.00002963
Iteration 72/1000 | Loss: 0.00002962
Iteration 73/1000 | Loss: 0.00002962
Iteration 74/1000 | Loss: 0.00002961
Iteration 75/1000 | Loss: 0.00002961
Iteration 76/1000 | Loss: 0.00002960
Iteration 77/1000 | Loss: 0.00002960
Iteration 78/1000 | Loss: 0.00002960
Iteration 79/1000 | Loss: 0.00002960
Iteration 80/1000 | Loss: 0.00002960
Iteration 81/1000 | Loss: 0.00002960
Iteration 82/1000 | Loss: 0.00002959
Iteration 83/1000 | Loss: 0.00002959
Iteration 84/1000 | Loss: 0.00002959
Iteration 85/1000 | Loss: 0.00002959
Iteration 86/1000 | Loss: 0.00002959
Iteration 87/1000 | Loss: 0.00002959
Iteration 88/1000 | Loss: 0.00002959
Iteration 89/1000 | Loss: 0.00002959
Iteration 90/1000 | Loss: 0.00002959
Iteration 91/1000 | Loss: 0.00002959
Iteration 92/1000 | Loss: 0.00002959
Iteration 93/1000 | Loss: 0.00002959
Iteration 94/1000 | Loss: 0.00002959
Iteration 95/1000 | Loss: 0.00002958
Iteration 96/1000 | Loss: 0.00002958
Iteration 97/1000 | Loss: 0.00002958
Iteration 98/1000 | Loss: 0.00002957
Iteration 99/1000 | Loss: 0.00002957
Iteration 100/1000 | Loss: 0.00002957
Iteration 101/1000 | Loss: 0.00002957
Iteration 102/1000 | Loss: 0.00002956
Iteration 103/1000 | Loss: 0.00002956
Iteration 104/1000 | Loss: 0.00002956
Iteration 105/1000 | Loss: 0.00002955
Iteration 106/1000 | Loss: 0.00002955
Iteration 107/1000 | Loss: 0.00002955
Iteration 108/1000 | Loss: 0.00002955
Iteration 109/1000 | Loss: 0.00002955
Iteration 110/1000 | Loss: 0.00002954
Iteration 111/1000 | Loss: 0.00002954
Iteration 112/1000 | Loss: 0.00002954
Iteration 113/1000 | Loss: 0.00002954
Iteration 114/1000 | Loss: 0.00002954
Iteration 115/1000 | Loss: 0.00002953
Iteration 116/1000 | Loss: 0.00002953
Iteration 117/1000 | Loss: 0.00002953
Iteration 118/1000 | Loss: 0.00002953
Iteration 119/1000 | Loss: 0.00002953
Iteration 120/1000 | Loss: 0.00002952
Iteration 121/1000 | Loss: 0.00002952
Iteration 122/1000 | Loss: 0.00002952
Iteration 123/1000 | Loss: 0.00002951
Iteration 124/1000 | Loss: 0.00002951
Iteration 125/1000 | Loss: 0.00002951
Iteration 126/1000 | Loss: 0.00002951
Iteration 127/1000 | Loss: 0.00002951
Iteration 128/1000 | Loss: 0.00002950
Iteration 129/1000 | Loss: 0.00002950
Iteration 130/1000 | Loss: 0.00002950
Iteration 131/1000 | Loss: 0.00002950
Iteration 132/1000 | Loss: 0.00002950
Iteration 133/1000 | Loss: 0.00002950
Iteration 134/1000 | Loss: 0.00002949
Iteration 135/1000 | Loss: 0.00002949
Iteration 136/1000 | Loss: 0.00002949
Iteration 137/1000 | Loss: 0.00002949
Iteration 138/1000 | Loss: 0.00002949
Iteration 139/1000 | Loss: 0.00002948
Iteration 140/1000 | Loss: 0.00002948
Iteration 141/1000 | Loss: 0.00002948
Iteration 142/1000 | Loss: 0.00002948
Iteration 143/1000 | Loss: 0.00002948
Iteration 144/1000 | Loss: 0.00002948
Iteration 145/1000 | Loss: 0.00002948
Iteration 146/1000 | Loss: 0.00002948
Iteration 147/1000 | Loss: 0.00002947
Iteration 148/1000 | Loss: 0.00002947
Iteration 149/1000 | Loss: 0.00002947
Iteration 150/1000 | Loss: 0.00002947
Iteration 151/1000 | Loss: 0.00002947
Iteration 152/1000 | Loss: 0.00002947
Iteration 153/1000 | Loss: 0.00002947
Iteration 154/1000 | Loss: 0.00002946
Iteration 155/1000 | Loss: 0.00002946
Iteration 156/1000 | Loss: 0.00002946
Iteration 157/1000 | Loss: 0.00002946
Iteration 158/1000 | Loss: 0.00002946
Iteration 159/1000 | Loss: 0.00002946
Iteration 160/1000 | Loss: 0.00002946
Iteration 161/1000 | Loss: 0.00002945
Iteration 162/1000 | Loss: 0.00002945
Iteration 163/1000 | Loss: 0.00002945
Iteration 164/1000 | Loss: 0.00002945
Iteration 165/1000 | Loss: 0.00002945
Iteration 166/1000 | Loss: 0.00002945
Iteration 167/1000 | Loss: 0.00002945
Iteration 168/1000 | Loss: 0.00002945
Iteration 169/1000 | Loss: 0.00002945
Iteration 170/1000 | Loss: 0.00002944
Iteration 171/1000 | Loss: 0.00002944
Iteration 172/1000 | Loss: 0.00002944
Iteration 173/1000 | Loss: 0.00002944
Iteration 174/1000 | Loss: 0.00002944
Iteration 175/1000 | Loss: 0.00002944
Iteration 176/1000 | Loss: 0.00002944
Iteration 177/1000 | Loss: 0.00002944
Iteration 178/1000 | Loss: 0.00002944
Iteration 179/1000 | Loss: 0.00002944
Iteration 180/1000 | Loss: 0.00002944
Iteration 181/1000 | Loss: 0.00002943
Iteration 182/1000 | Loss: 0.00002943
Iteration 183/1000 | Loss: 0.00002943
Iteration 184/1000 | Loss: 0.00002943
Iteration 185/1000 | Loss: 0.00002943
Iteration 186/1000 | Loss: 0.00002943
Iteration 187/1000 | Loss: 0.00002943
Iteration 188/1000 | Loss: 0.00002943
Iteration 189/1000 | Loss: 0.00002943
Iteration 190/1000 | Loss: 0.00002943
Iteration 191/1000 | Loss: 0.00002943
Iteration 192/1000 | Loss: 0.00002943
Iteration 193/1000 | Loss: 0.00002942
Iteration 194/1000 | Loss: 0.00002942
Iteration 195/1000 | Loss: 0.00002942
Iteration 196/1000 | Loss: 0.00002942
Iteration 197/1000 | Loss: 0.00002942
Iteration 198/1000 | Loss: 0.00002942
Iteration 199/1000 | Loss: 0.00002942
Iteration 200/1000 | Loss: 0.00002942
Iteration 201/1000 | Loss: 0.00002942
Iteration 202/1000 | Loss: 0.00002942
Iteration 203/1000 | Loss: 0.00002942
Iteration 204/1000 | Loss: 0.00002942
Iteration 205/1000 | Loss: 0.00002942
Iteration 206/1000 | Loss: 0.00002942
Iteration 207/1000 | Loss: 0.00002942
Iteration 208/1000 | Loss: 0.00002942
Iteration 209/1000 | Loss: 0.00002942
Iteration 210/1000 | Loss: 0.00002942
Iteration 211/1000 | Loss: 0.00002942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.9421185899991542e-05, 2.9421185899991542e-05, 2.9421185899991542e-05, 2.9421185899991542e-05, 2.9421185899991542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9421185899991542e-05

Optimization complete. Final v2v error: 4.389805316925049 mm

Highest mean error: 5.872516632080078 mm for frame 74

Lowest mean error: 3.408764123916626 mm for frame 35

Saving results

Total time: 47.562679290771484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863125
Iteration 2/25 | Loss: 0.00109177
Iteration 3/25 | Loss: 0.00066391
Iteration 4/25 | Loss: 0.00062036
Iteration 5/25 | Loss: 0.00060983
Iteration 6/25 | Loss: 0.00060680
Iteration 7/25 | Loss: 0.00060591
Iteration 8/25 | Loss: 0.00060591
Iteration 9/25 | Loss: 0.00060591
Iteration 10/25 | Loss: 0.00060591
Iteration 11/25 | Loss: 0.00060591
Iteration 12/25 | Loss: 0.00060591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006059068837203085, 0.0006059068837203085, 0.0006059068837203085, 0.0006059068837203085, 0.0006059068837203085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006059068837203085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.93610787
Iteration 2/25 | Loss: 0.00032594
Iteration 3/25 | Loss: 0.00032594
Iteration 4/25 | Loss: 0.00032594
Iteration 5/25 | Loss: 0.00032594
Iteration 6/25 | Loss: 0.00032594
Iteration 7/25 | Loss: 0.00032594
Iteration 8/25 | Loss: 0.00032594
Iteration 9/25 | Loss: 0.00032594
Iteration 10/25 | Loss: 0.00032594
Iteration 11/25 | Loss: 0.00032594
Iteration 12/25 | Loss: 0.00032594
Iteration 13/25 | Loss: 0.00032594
Iteration 14/25 | Loss: 0.00032594
Iteration 15/25 | Loss: 0.00032594
Iteration 16/25 | Loss: 0.00032594
Iteration 17/25 | Loss: 0.00032594
Iteration 18/25 | Loss: 0.00032594
Iteration 19/25 | Loss: 0.00032594
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00032593519426882267, 0.00032593519426882267, 0.00032593519426882267, 0.00032593519426882267, 0.00032593519426882267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032593519426882267

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032594
Iteration 2/1000 | Loss: 0.00002331
Iteration 3/1000 | Loss: 0.00001604
Iteration 4/1000 | Loss: 0.00001493
Iteration 5/1000 | Loss: 0.00001413
Iteration 6/1000 | Loss: 0.00001375
Iteration 7/1000 | Loss: 0.00001345
Iteration 8/1000 | Loss: 0.00001321
Iteration 9/1000 | Loss: 0.00001306
Iteration 10/1000 | Loss: 0.00001297
Iteration 11/1000 | Loss: 0.00001288
Iteration 12/1000 | Loss: 0.00001284
Iteration 13/1000 | Loss: 0.00001282
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001275
Iteration 19/1000 | Loss: 0.00001270
Iteration 20/1000 | Loss: 0.00001268
Iteration 21/1000 | Loss: 0.00001266
Iteration 22/1000 | Loss: 0.00001266
Iteration 23/1000 | Loss: 0.00001265
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001264
Iteration 26/1000 | Loss: 0.00001264
Iteration 27/1000 | Loss: 0.00001263
Iteration 28/1000 | Loss: 0.00001263
Iteration 29/1000 | Loss: 0.00001262
Iteration 30/1000 | Loss: 0.00001261
Iteration 31/1000 | Loss: 0.00001260
Iteration 32/1000 | Loss: 0.00001260
Iteration 33/1000 | Loss: 0.00001260
Iteration 34/1000 | Loss: 0.00001260
Iteration 35/1000 | Loss: 0.00001259
Iteration 36/1000 | Loss: 0.00001259
Iteration 37/1000 | Loss: 0.00001258
Iteration 38/1000 | Loss: 0.00001258
Iteration 39/1000 | Loss: 0.00001258
Iteration 40/1000 | Loss: 0.00001257
Iteration 41/1000 | Loss: 0.00001257
Iteration 42/1000 | Loss: 0.00001256
Iteration 43/1000 | Loss: 0.00001256
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001255
Iteration 46/1000 | Loss: 0.00001253
Iteration 47/1000 | Loss: 0.00001251
Iteration 48/1000 | Loss: 0.00001250
Iteration 49/1000 | Loss: 0.00001250
Iteration 50/1000 | Loss: 0.00001250
Iteration 51/1000 | Loss: 0.00001249
Iteration 52/1000 | Loss: 0.00001249
Iteration 53/1000 | Loss: 0.00001248
Iteration 54/1000 | Loss: 0.00001248
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001247
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001246
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001246
Iteration 65/1000 | Loss: 0.00001245
Iteration 66/1000 | Loss: 0.00001245
Iteration 67/1000 | Loss: 0.00001245
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001244
Iteration 72/1000 | Loss: 0.00001243
Iteration 73/1000 | Loss: 0.00001243
Iteration 74/1000 | Loss: 0.00001243
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001242
Iteration 77/1000 | Loss: 0.00001242
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00001242
Iteration 80/1000 | Loss: 0.00001242
Iteration 81/1000 | Loss: 0.00001242
Iteration 82/1000 | Loss: 0.00001242
Iteration 83/1000 | Loss: 0.00001241
Iteration 84/1000 | Loss: 0.00001241
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001240
Iteration 91/1000 | Loss: 0.00001240
Iteration 92/1000 | Loss: 0.00001240
Iteration 93/1000 | Loss: 0.00001240
Iteration 94/1000 | Loss: 0.00001240
Iteration 95/1000 | Loss: 0.00001240
Iteration 96/1000 | Loss: 0.00001240
Iteration 97/1000 | Loss: 0.00001240
Iteration 98/1000 | Loss: 0.00001240
Iteration 99/1000 | Loss: 0.00001239
Iteration 100/1000 | Loss: 0.00001239
Iteration 101/1000 | Loss: 0.00001239
Iteration 102/1000 | Loss: 0.00001239
Iteration 103/1000 | Loss: 0.00001239
Iteration 104/1000 | Loss: 0.00001239
Iteration 105/1000 | Loss: 0.00001239
Iteration 106/1000 | Loss: 0.00001239
Iteration 107/1000 | Loss: 0.00001239
Iteration 108/1000 | Loss: 0.00001239
Iteration 109/1000 | Loss: 0.00001239
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001238
Iteration 112/1000 | Loss: 0.00001238
Iteration 113/1000 | Loss: 0.00001238
Iteration 114/1000 | Loss: 0.00001238
Iteration 115/1000 | Loss: 0.00001238
Iteration 116/1000 | Loss: 0.00001237
Iteration 117/1000 | Loss: 0.00001237
Iteration 118/1000 | Loss: 0.00001237
Iteration 119/1000 | Loss: 0.00001237
Iteration 120/1000 | Loss: 0.00001237
Iteration 121/1000 | Loss: 0.00001237
Iteration 122/1000 | Loss: 0.00001237
Iteration 123/1000 | Loss: 0.00001237
Iteration 124/1000 | Loss: 0.00001237
Iteration 125/1000 | Loss: 0.00001237
Iteration 126/1000 | Loss: 0.00001237
Iteration 127/1000 | Loss: 0.00001237
Iteration 128/1000 | Loss: 0.00001237
Iteration 129/1000 | Loss: 0.00001237
Iteration 130/1000 | Loss: 0.00001237
Iteration 131/1000 | Loss: 0.00001237
Iteration 132/1000 | Loss: 0.00001237
Iteration 133/1000 | Loss: 0.00001237
Iteration 134/1000 | Loss: 0.00001237
Iteration 135/1000 | Loss: 0.00001237
Iteration 136/1000 | Loss: 0.00001237
Iteration 137/1000 | Loss: 0.00001237
Iteration 138/1000 | Loss: 0.00001237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.2369617252261378e-05, 1.2369617252261378e-05, 1.2369617252261378e-05, 1.2369617252261378e-05, 1.2369617252261378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2369617252261378e-05

Optimization complete. Final v2v error: 3.011942148208618 mm

Highest mean error: 3.404195785522461 mm for frame 172

Lowest mean error: 2.764132261276245 mm for frame 0

Saving results

Total time: 39.06957507133484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770705
Iteration 2/25 | Loss: 0.00132710
Iteration 3/25 | Loss: 0.00083591
Iteration 4/25 | Loss: 0.00075186
Iteration 5/25 | Loss: 0.00071401
Iteration 6/25 | Loss: 0.00070429
Iteration 7/25 | Loss: 0.00070865
Iteration 8/25 | Loss: 0.00070237
Iteration 9/25 | Loss: 0.00070402
Iteration 10/25 | Loss: 0.00070068
Iteration 11/25 | Loss: 0.00070168
Iteration 12/25 | Loss: 0.00070051
Iteration 13/25 | Loss: 0.00069917
Iteration 14/25 | Loss: 0.00069687
Iteration 15/25 | Loss: 0.00069573
Iteration 16/25 | Loss: 0.00069505
Iteration 17/25 | Loss: 0.00069425
Iteration 18/25 | Loss: 0.00069366
Iteration 19/25 | Loss: 0.00069326
Iteration 20/25 | Loss: 0.00069307
Iteration 21/25 | Loss: 0.00069295
Iteration 22/25 | Loss: 0.00069280
Iteration 23/25 | Loss: 0.00069303
Iteration 24/25 | Loss: 0.00069249
Iteration 25/25 | Loss: 0.00069161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.77847195
Iteration 2/25 | Loss: 0.00043502
Iteration 3/25 | Loss: 0.00043490
Iteration 4/25 | Loss: 0.00043489
Iteration 5/25 | Loss: 0.00043489
Iteration 6/25 | Loss: 0.00043489
Iteration 7/25 | Loss: 0.00043489
Iteration 8/25 | Loss: 0.00043489
Iteration 9/25 | Loss: 0.00043489
Iteration 10/25 | Loss: 0.00043489
Iteration 11/25 | Loss: 0.00043489
Iteration 12/25 | Loss: 0.00043489
Iteration 13/25 | Loss: 0.00043489
Iteration 14/25 | Loss: 0.00043489
Iteration 15/25 | Loss: 0.00043489
Iteration 16/25 | Loss: 0.00043489
Iteration 17/25 | Loss: 0.00043489
Iteration 18/25 | Loss: 0.00043489
Iteration 19/25 | Loss: 0.00043489
Iteration 20/25 | Loss: 0.00043489
Iteration 21/25 | Loss: 0.00043489
Iteration 22/25 | Loss: 0.00043489
Iteration 23/25 | Loss: 0.00043489
Iteration 24/25 | Loss: 0.00043489
Iteration 25/25 | Loss: 0.00043489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043489
Iteration 2/1000 | Loss: 0.00009391
Iteration 3/1000 | Loss: 0.00006540
Iteration 4/1000 | Loss: 0.00003007
Iteration 5/1000 | Loss: 0.00002319
Iteration 6/1000 | Loss: 0.00002222
Iteration 7/1000 | Loss: 0.00002118
Iteration 8/1000 | Loss: 0.00002061
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00001974
Iteration 11/1000 | Loss: 0.00001940
Iteration 12/1000 | Loss: 0.00001919
Iteration 13/1000 | Loss: 0.00001916
Iteration 14/1000 | Loss: 0.00001898
Iteration 15/1000 | Loss: 0.00001892
Iteration 16/1000 | Loss: 0.00001885
Iteration 17/1000 | Loss: 0.00001882
Iteration 18/1000 | Loss: 0.00001878
Iteration 19/1000 | Loss: 0.00001873
Iteration 20/1000 | Loss: 0.00001869
Iteration 21/1000 | Loss: 0.00001868
Iteration 22/1000 | Loss: 0.00001866
Iteration 23/1000 | Loss: 0.00001866
Iteration 24/1000 | Loss: 0.00001865
Iteration 25/1000 | Loss: 0.00001865
Iteration 26/1000 | Loss: 0.00001864
Iteration 27/1000 | Loss: 0.00001864
Iteration 28/1000 | Loss: 0.00001862
Iteration 29/1000 | Loss: 0.00001862
Iteration 30/1000 | Loss: 0.00001861
Iteration 31/1000 | Loss: 0.00001861
Iteration 32/1000 | Loss: 0.00001860
Iteration 33/1000 | Loss: 0.00001860
Iteration 34/1000 | Loss: 0.00001859
Iteration 35/1000 | Loss: 0.00001859
Iteration 36/1000 | Loss: 0.00001858
Iteration 37/1000 | Loss: 0.00001858
Iteration 38/1000 | Loss: 0.00001858
Iteration 39/1000 | Loss: 0.00001857
Iteration 40/1000 | Loss: 0.00001857
Iteration 41/1000 | Loss: 0.00001857
Iteration 42/1000 | Loss: 0.00001856
Iteration 43/1000 | Loss: 0.00001856
Iteration 44/1000 | Loss: 0.00001855
Iteration 45/1000 | Loss: 0.00001855
Iteration 46/1000 | Loss: 0.00001855
Iteration 47/1000 | Loss: 0.00001855
Iteration 48/1000 | Loss: 0.00001855
Iteration 49/1000 | Loss: 0.00001854
Iteration 50/1000 | Loss: 0.00001854
Iteration 51/1000 | Loss: 0.00001854
Iteration 52/1000 | Loss: 0.00001853
Iteration 53/1000 | Loss: 0.00001853
Iteration 54/1000 | Loss: 0.00001853
Iteration 55/1000 | Loss: 0.00001852
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001852
Iteration 58/1000 | Loss: 0.00001851
Iteration 59/1000 | Loss: 0.00001851
Iteration 60/1000 | Loss: 0.00001851
Iteration 61/1000 | Loss: 0.00001851
Iteration 62/1000 | Loss: 0.00001851
Iteration 63/1000 | Loss: 0.00001850
Iteration 64/1000 | Loss: 0.00001850
Iteration 65/1000 | Loss: 0.00001850
Iteration 66/1000 | Loss: 0.00001849
Iteration 67/1000 | Loss: 0.00001849
Iteration 68/1000 | Loss: 0.00001849
Iteration 69/1000 | Loss: 0.00001849
Iteration 70/1000 | Loss: 0.00001849
Iteration 71/1000 | Loss: 0.00001849
Iteration 72/1000 | Loss: 0.00001849
Iteration 73/1000 | Loss: 0.00001848
Iteration 74/1000 | Loss: 0.00001848
Iteration 75/1000 | Loss: 0.00001848
Iteration 76/1000 | Loss: 0.00001848
Iteration 77/1000 | Loss: 0.00001848
Iteration 78/1000 | Loss: 0.00001848
Iteration 79/1000 | Loss: 0.00001848
Iteration 80/1000 | Loss: 0.00001847
Iteration 81/1000 | Loss: 0.00001847
Iteration 82/1000 | Loss: 0.00001847
Iteration 83/1000 | Loss: 0.00001847
Iteration 84/1000 | Loss: 0.00001847
Iteration 85/1000 | Loss: 0.00001847
Iteration 86/1000 | Loss: 0.00001847
Iteration 87/1000 | Loss: 0.00001847
Iteration 88/1000 | Loss: 0.00001847
Iteration 89/1000 | Loss: 0.00001847
Iteration 90/1000 | Loss: 0.00001847
Iteration 91/1000 | Loss: 0.00001847
Iteration 92/1000 | Loss: 0.00001847
Iteration 93/1000 | Loss: 0.00001846
Iteration 94/1000 | Loss: 0.00001846
Iteration 95/1000 | Loss: 0.00001846
Iteration 96/1000 | Loss: 0.00001846
Iteration 97/1000 | Loss: 0.00001846
Iteration 98/1000 | Loss: 0.00001846
Iteration 99/1000 | Loss: 0.00001846
Iteration 100/1000 | Loss: 0.00001846
Iteration 101/1000 | Loss: 0.00001846
Iteration 102/1000 | Loss: 0.00001846
Iteration 103/1000 | Loss: 0.00001845
Iteration 104/1000 | Loss: 0.00001845
Iteration 105/1000 | Loss: 0.00001845
Iteration 106/1000 | Loss: 0.00001845
Iteration 107/1000 | Loss: 0.00001845
Iteration 108/1000 | Loss: 0.00001845
Iteration 109/1000 | Loss: 0.00001845
Iteration 110/1000 | Loss: 0.00001845
Iteration 111/1000 | Loss: 0.00001845
Iteration 112/1000 | Loss: 0.00001845
Iteration 113/1000 | Loss: 0.00001844
Iteration 114/1000 | Loss: 0.00001844
Iteration 115/1000 | Loss: 0.00001844
Iteration 116/1000 | Loss: 0.00001844
Iteration 117/1000 | Loss: 0.00001844
Iteration 118/1000 | Loss: 0.00001844
Iteration 119/1000 | Loss: 0.00001844
Iteration 120/1000 | Loss: 0.00001844
Iteration 121/1000 | Loss: 0.00001844
Iteration 122/1000 | Loss: 0.00001844
Iteration 123/1000 | Loss: 0.00001844
Iteration 124/1000 | Loss: 0.00001844
Iteration 125/1000 | Loss: 0.00001843
Iteration 126/1000 | Loss: 0.00001843
Iteration 127/1000 | Loss: 0.00001843
Iteration 128/1000 | Loss: 0.00001843
Iteration 129/1000 | Loss: 0.00001843
Iteration 130/1000 | Loss: 0.00001843
Iteration 131/1000 | Loss: 0.00001843
Iteration 132/1000 | Loss: 0.00001843
Iteration 133/1000 | Loss: 0.00001843
Iteration 134/1000 | Loss: 0.00001843
Iteration 135/1000 | Loss: 0.00001843
Iteration 136/1000 | Loss: 0.00001842
Iteration 137/1000 | Loss: 0.00001842
Iteration 138/1000 | Loss: 0.00001842
Iteration 139/1000 | Loss: 0.00001842
Iteration 140/1000 | Loss: 0.00001842
Iteration 141/1000 | Loss: 0.00001842
Iteration 142/1000 | Loss: 0.00001841
Iteration 143/1000 | Loss: 0.00001841
Iteration 144/1000 | Loss: 0.00001841
Iteration 145/1000 | Loss: 0.00001841
Iteration 146/1000 | Loss: 0.00001841
Iteration 147/1000 | Loss: 0.00001841
Iteration 148/1000 | Loss: 0.00001841
Iteration 149/1000 | Loss: 0.00001841
Iteration 150/1000 | Loss: 0.00001841
Iteration 151/1000 | Loss: 0.00001840
Iteration 152/1000 | Loss: 0.00001840
Iteration 153/1000 | Loss: 0.00001840
Iteration 154/1000 | Loss: 0.00001840
Iteration 155/1000 | Loss: 0.00001840
Iteration 156/1000 | Loss: 0.00001840
Iteration 157/1000 | Loss: 0.00001839
Iteration 158/1000 | Loss: 0.00001839
Iteration 159/1000 | Loss: 0.00001839
Iteration 160/1000 | Loss: 0.00001839
Iteration 161/1000 | Loss: 0.00001839
Iteration 162/1000 | Loss: 0.00001839
Iteration 163/1000 | Loss: 0.00001839
Iteration 164/1000 | Loss: 0.00001839
Iteration 165/1000 | Loss: 0.00001839
Iteration 166/1000 | Loss: 0.00001839
Iteration 167/1000 | Loss: 0.00001838
Iteration 168/1000 | Loss: 0.00001838
Iteration 169/1000 | Loss: 0.00001838
Iteration 170/1000 | Loss: 0.00001838
Iteration 171/1000 | Loss: 0.00001838
Iteration 172/1000 | Loss: 0.00001838
Iteration 173/1000 | Loss: 0.00001838
Iteration 174/1000 | Loss: 0.00001838
Iteration 175/1000 | Loss: 0.00001838
Iteration 176/1000 | Loss: 0.00001838
Iteration 177/1000 | Loss: 0.00001838
Iteration 178/1000 | Loss: 0.00001838
Iteration 179/1000 | Loss: 0.00001837
Iteration 180/1000 | Loss: 0.00001837
Iteration 181/1000 | Loss: 0.00001837
Iteration 182/1000 | Loss: 0.00001837
Iteration 183/1000 | Loss: 0.00001837
Iteration 184/1000 | Loss: 0.00001837
Iteration 185/1000 | Loss: 0.00001837
Iteration 186/1000 | Loss: 0.00001837
Iteration 187/1000 | Loss: 0.00001836
Iteration 188/1000 | Loss: 0.00001836
Iteration 189/1000 | Loss: 0.00001836
Iteration 190/1000 | Loss: 0.00001836
Iteration 191/1000 | Loss: 0.00001836
Iteration 192/1000 | Loss: 0.00001836
Iteration 193/1000 | Loss: 0.00001836
Iteration 194/1000 | Loss: 0.00001836
Iteration 195/1000 | Loss: 0.00001836
Iteration 196/1000 | Loss: 0.00001836
Iteration 197/1000 | Loss: 0.00001836
Iteration 198/1000 | Loss: 0.00001836
Iteration 199/1000 | Loss: 0.00001836
Iteration 200/1000 | Loss: 0.00001836
Iteration 201/1000 | Loss: 0.00001836
Iteration 202/1000 | Loss: 0.00001836
Iteration 203/1000 | Loss: 0.00001836
Iteration 204/1000 | Loss: 0.00001836
Iteration 205/1000 | Loss: 0.00001836
Iteration 206/1000 | Loss: 0.00001836
Iteration 207/1000 | Loss: 0.00001836
Iteration 208/1000 | Loss: 0.00001836
Iteration 209/1000 | Loss: 0.00001836
Iteration 210/1000 | Loss: 0.00001836
Iteration 211/1000 | Loss: 0.00001836
Iteration 212/1000 | Loss: 0.00001836
Iteration 213/1000 | Loss: 0.00001836
Iteration 214/1000 | Loss: 0.00001836
Iteration 215/1000 | Loss: 0.00001836
Iteration 216/1000 | Loss: 0.00001836
Iteration 217/1000 | Loss: 0.00001836
Iteration 218/1000 | Loss: 0.00001836
Iteration 219/1000 | Loss: 0.00001836
Iteration 220/1000 | Loss: 0.00001836
Iteration 221/1000 | Loss: 0.00001836
Iteration 222/1000 | Loss: 0.00001836
Iteration 223/1000 | Loss: 0.00001836
Iteration 224/1000 | Loss: 0.00001836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.8355784050072543e-05, 1.8355784050072543e-05, 1.8355784050072543e-05, 1.8355784050072543e-05, 1.8355784050072543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8355784050072543e-05

Optimization complete. Final v2v error: 3.517158031463623 mm

Highest mean error: 5.826282978057861 mm for frame 170

Lowest mean error: 2.6754159927368164 mm for frame 94

Saving results

Total time: 89.45045328140259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789533
Iteration 2/25 | Loss: 0.00137132
Iteration 3/25 | Loss: 0.00079643
Iteration 4/25 | Loss: 0.00072315
Iteration 5/25 | Loss: 0.00071049
Iteration 6/25 | Loss: 0.00070575
Iteration 7/25 | Loss: 0.00070415
Iteration 8/25 | Loss: 0.00070338
Iteration 9/25 | Loss: 0.00070338
Iteration 10/25 | Loss: 0.00070338
Iteration 11/25 | Loss: 0.00070338
Iteration 12/25 | Loss: 0.00070338
Iteration 13/25 | Loss: 0.00070338
Iteration 14/25 | Loss: 0.00070338
Iteration 15/25 | Loss: 0.00070338
Iteration 16/25 | Loss: 0.00070338
Iteration 17/25 | Loss: 0.00070338
Iteration 18/25 | Loss: 0.00070338
Iteration 19/25 | Loss: 0.00070338
Iteration 20/25 | Loss: 0.00070338
Iteration 21/25 | Loss: 0.00070338
Iteration 22/25 | Loss: 0.00070338
Iteration 23/25 | Loss: 0.00070338
Iteration 24/25 | Loss: 0.00070338
Iteration 25/25 | Loss: 0.00070338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32328761
Iteration 2/25 | Loss: 0.00034632
Iteration 3/25 | Loss: 0.00034632
Iteration 4/25 | Loss: 0.00034632
Iteration 5/25 | Loss: 0.00034632
Iteration 6/25 | Loss: 0.00034632
Iteration 7/25 | Loss: 0.00034632
Iteration 8/25 | Loss: 0.00034632
Iteration 9/25 | Loss: 0.00034632
Iteration 10/25 | Loss: 0.00034632
Iteration 11/25 | Loss: 0.00034632
Iteration 12/25 | Loss: 0.00034632
Iteration 13/25 | Loss: 0.00034632
Iteration 14/25 | Loss: 0.00034632
Iteration 15/25 | Loss: 0.00034632
Iteration 16/25 | Loss: 0.00034632
Iteration 17/25 | Loss: 0.00034632
Iteration 18/25 | Loss: 0.00034632
Iteration 19/25 | Loss: 0.00034632
Iteration 20/25 | Loss: 0.00034632
Iteration 21/25 | Loss: 0.00034632
Iteration 22/25 | Loss: 0.00034632
Iteration 23/25 | Loss: 0.00034632
Iteration 24/25 | Loss: 0.00034632
Iteration 25/25 | Loss: 0.00034632

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034632
Iteration 2/1000 | Loss: 0.00003516
Iteration 3/1000 | Loss: 0.00002482
Iteration 4/1000 | Loss: 0.00002260
Iteration 5/1000 | Loss: 0.00002142
Iteration 6/1000 | Loss: 0.00002068
Iteration 7/1000 | Loss: 0.00002018
Iteration 8/1000 | Loss: 0.00001984
Iteration 9/1000 | Loss: 0.00001943
Iteration 10/1000 | Loss: 0.00001920
Iteration 11/1000 | Loss: 0.00001915
Iteration 12/1000 | Loss: 0.00001901
Iteration 13/1000 | Loss: 0.00001895
Iteration 14/1000 | Loss: 0.00001890
Iteration 15/1000 | Loss: 0.00001890
Iteration 16/1000 | Loss: 0.00001888
Iteration 17/1000 | Loss: 0.00001888
Iteration 18/1000 | Loss: 0.00001887
Iteration 19/1000 | Loss: 0.00001887
Iteration 20/1000 | Loss: 0.00001886
Iteration 21/1000 | Loss: 0.00001886
Iteration 22/1000 | Loss: 0.00001885
Iteration 23/1000 | Loss: 0.00001885
Iteration 24/1000 | Loss: 0.00001883
Iteration 25/1000 | Loss: 0.00001883
Iteration 26/1000 | Loss: 0.00001883
Iteration 27/1000 | Loss: 0.00001883
Iteration 28/1000 | Loss: 0.00001883
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001882
Iteration 31/1000 | Loss: 0.00001878
Iteration 32/1000 | Loss: 0.00001878
Iteration 33/1000 | Loss: 0.00001876
Iteration 34/1000 | Loss: 0.00001876
Iteration 35/1000 | Loss: 0.00001876
Iteration 36/1000 | Loss: 0.00001876
Iteration 37/1000 | Loss: 0.00001876
Iteration 38/1000 | Loss: 0.00001876
Iteration 39/1000 | Loss: 0.00001876
Iteration 40/1000 | Loss: 0.00001876
Iteration 41/1000 | Loss: 0.00001876
Iteration 42/1000 | Loss: 0.00001875
Iteration 43/1000 | Loss: 0.00001875
Iteration 44/1000 | Loss: 0.00001875
Iteration 45/1000 | Loss: 0.00001875
Iteration 46/1000 | Loss: 0.00001875
Iteration 47/1000 | Loss: 0.00001875
Iteration 48/1000 | Loss: 0.00001875
Iteration 49/1000 | Loss: 0.00001875
Iteration 50/1000 | Loss: 0.00001875
Iteration 51/1000 | Loss: 0.00001875
Iteration 52/1000 | Loss: 0.00001875
Iteration 53/1000 | Loss: 0.00001875
Iteration 54/1000 | Loss: 0.00001874
Iteration 55/1000 | Loss: 0.00001874
Iteration 56/1000 | Loss: 0.00001874
Iteration 57/1000 | Loss: 0.00001874
Iteration 58/1000 | Loss: 0.00001873
Iteration 59/1000 | Loss: 0.00001873
Iteration 60/1000 | Loss: 0.00001873
Iteration 61/1000 | Loss: 0.00001873
Iteration 62/1000 | Loss: 0.00001872
Iteration 63/1000 | Loss: 0.00001872
Iteration 64/1000 | Loss: 0.00001872
Iteration 65/1000 | Loss: 0.00001872
Iteration 66/1000 | Loss: 0.00001871
Iteration 67/1000 | Loss: 0.00001871
Iteration 68/1000 | Loss: 0.00001871
Iteration 69/1000 | Loss: 0.00001871
Iteration 70/1000 | Loss: 0.00001871
Iteration 71/1000 | Loss: 0.00001871
Iteration 72/1000 | Loss: 0.00001871
Iteration 73/1000 | Loss: 0.00001870
Iteration 74/1000 | Loss: 0.00001870
Iteration 75/1000 | Loss: 0.00001870
Iteration 76/1000 | Loss: 0.00001870
Iteration 77/1000 | Loss: 0.00001870
Iteration 78/1000 | Loss: 0.00001870
Iteration 79/1000 | Loss: 0.00001870
Iteration 80/1000 | Loss: 0.00001870
Iteration 81/1000 | Loss: 0.00001870
Iteration 82/1000 | Loss: 0.00001870
Iteration 83/1000 | Loss: 0.00001870
Iteration 84/1000 | Loss: 0.00001869
Iteration 85/1000 | Loss: 0.00001869
Iteration 86/1000 | Loss: 0.00001869
Iteration 87/1000 | Loss: 0.00001869
Iteration 88/1000 | Loss: 0.00001869
Iteration 89/1000 | Loss: 0.00001869
Iteration 90/1000 | Loss: 0.00001869
Iteration 91/1000 | Loss: 0.00001869
Iteration 92/1000 | Loss: 0.00001869
Iteration 93/1000 | Loss: 0.00001869
Iteration 94/1000 | Loss: 0.00001869
Iteration 95/1000 | Loss: 0.00001869
Iteration 96/1000 | Loss: 0.00001869
Iteration 97/1000 | Loss: 0.00001869
Iteration 98/1000 | Loss: 0.00001869
Iteration 99/1000 | Loss: 0.00001869
Iteration 100/1000 | Loss: 0.00001869
Iteration 101/1000 | Loss: 0.00001868
Iteration 102/1000 | Loss: 0.00001868
Iteration 103/1000 | Loss: 0.00001868
Iteration 104/1000 | Loss: 0.00001868
Iteration 105/1000 | Loss: 0.00001868
Iteration 106/1000 | Loss: 0.00001868
Iteration 107/1000 | Loss: 0.00001868
Iteration 108/1000 | Loss: 0.00001868
Iteration 109/1000 | Loss: 0.00001868
Iteration 110/1000 | Loss: 0.00001868
Iteration 111/1000 | Loss: 0.00001868
Iteration 112/1000 | Loss: 0.00001868
Iteration 113/1000 | Loss: 0.00001868
Iteration 114/1000 | Loss: 0.00001868
Iteration 115/1000 | Loss: 0.00001868
Iteration 116/1000 | Loss: 0.00001868
Iteration 117/1000 | Loss: 0.00001868
Iteration 118/1000 | Loss: 0.00001868
Iteration 119/1000 | Loss: 0.00001868
Iteration 120/1000 | Loss: 0.00001868
Iteration 121/1000 | Loss: 0.00001868
Iteration 122/1000 | Loss: 0.00001868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.868428626039531e-05, 1.868428626039531e-05, 1.868428626039531e-05, 1.868428626039531e-05, 1.868428626039531e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.868428626039531e-05

Optimization complete. Final v2v error: 3.6272668838500977 mm

Highest mean error: 4.427778244018555 mm for frame 161

Lowest mean error: 3.172931432723999 mm for frame 181

Saving results

Total time: 36.53399872779846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_030/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_030/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00368725
Iteration 2/25 | Loss: 0.00071351
Iteration 3/25 | Loss: 0.00059591
Iteration 4/25 | Loss: 0.00058024
Iteration 5/25 | Loss: 0.00057504
Iteration 6/25 | Loss: 0.00057362
Iteration 7/25 | Loss: 0.00057320
Iteration 8/25 | Loss: 0.00057320
Iteration 9/25 | Loss: 0.00057320
Iteration 10/25 | Loss: 0.00057320
Iteration 11/25 | Loss: 0.00057320
Iteration 12/25 | Loss: 0.00057317
Iteration 13/25 | Loss: 0.00057317
Iteration 14/25 | Loss: 0.00057317
Iteration 15/25 | Loss: 0.00057317
Iteration 16/25 | Loss: 0.00057317
Iteration 17/25 | Loss: 0.00057317
Iteration 18/25 | Loss: 0.00057317
Iteration 19/25 | Loss: 0.00057317
Iteration 20/25 | Loss: 0.00057317
Iteration 21/25 | Loss: 0.00057317
Iteration 22/25 | Loss: 0.00057317
Iteration 23/25 | Loss: 0.00057317
Iteration 24/25 | Loss: 0.00057317
Iteration 25/25 | Loss: 0.00057317

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71557963
Iteration 2/25 | Loss: 0.00028273
Iteration 3/25 | Loss: 0.00028272
Iteration 4/25 | Loss: 0.00028272
Iteration 5/25 | Loss: 0.00028272
Iteration 6/25 | Loss: 0.00028272
Iteration 7/25 | Loss: 0.00028272
Iteration 8/25 | Loss: 0.00028272
Iteration 9/25 | Loss: 0.00028272
Iteration 10/25 | Loss: 0.00028272
Iteration 11/25 | Loss: 0.00028272
Iteration 12/25 | Loss: 0.00028272
Iteration 13/25 | Loss: 0.00028272
Iteration 14/25 | Loss: 0.00028272
Iteration 15/25 | Loss: 0.00028272
Iteration 16/25 | Loss: 0.00028272
Iteration 17/25 | Loss: 0.00028272
Iteration 18/25 | Loss: 0.00028272
Iteration 19/25 | Loss: 0.00028272
Iteration 20/25 | Loss: 0.00028272
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002827210701070726, 0.0002827210701070726, 0.0002827210701070726, 0.0002827210701070726, 0.0002827210701070726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002827210701070726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028272
Iteration 2/1000 | Loss: 0.00001839
Iteration 3/1000 | Loss: 0.00001171
Iteration 4/1000 | Loss: 0.00001070
Iteration 5/1000 | Loss: 0.00001004
Iteration 6/1000 | Loss: 0.00000966
Iteration 7/1000 | Loss: 0.00000958
Iteration 8/1000 | Loss: 0.00000941
Iteration 9/1000 | Loss: 0.00000932
Iteration 10/1000 | Loss: 0.00000927
Iteration 11/1000 | Loss: 0.00000924
Iteration 12/1000 | Loss: 0.00000921
Iteration 13/1000 | Loss: 0.00000919
Iteration 14/1000 | Loss: 0.00000919
Iteration 15/1000 | Loss: 0.00000919
Iteration 16/1000 | Loss: 0.00000919
Iteration 17/1000 | Loss: 0.00000918
Iteration 18/1000 | Loss: 0.00000918
Iteration 19/1000 | Loss: 0.00000918
Iteration 20/1000 | Loss: 0.00000918
Iteration 21/1000 | Loss: 0.00000918
Iteration 22/1000 | Loss: 0.00000918
Iteration 23/1000 | Loss: 0.00000918
Iteration 24/1000 | Loss: 0.00000917
Iteration 25/1000 | Loss: 0.00000916
Iteration 26/1000 | Loss: 0.00000915
Iteration 27/1000 | Loss: 0.00000915
Iteration 28/1000 | Loss: 0.00000915
Iteration 29/1000 | Loss: 0.00000915
Iteration 30/1000 | Loss: 0.00000915
Iteration 31/1000 | Loss: 0.00000914
Iteration 32/1000 | Loss: 0.00000913
Iteration 33/1000 | Loss: 0.00000912
Iteration 34/1000 | Loss: 0.00000912
Iteration 35/1000 | Loss: 0.00000910
Iteration 36/1000 | Loss: 0.00000909
Iteration 37/1000 | Loss: 0.00000906
Iteration 38/1000 | Loss: 0.00000906
Iteration 39/1000 | Loss: 0.00000906
Iteration 40/1000 | Loss: 0.00000905
Iteration 41/1000 | Loss: 0.00000905
Iteration 42/1000 | Loss: 0.00000905
Iteration 43/1000 | Loss: 0.00000904
Iteration 44/1000 | Loss: 0.00000904
Iteration 45/1000 | Loss: 0.00000904
Iteration 46/1000 | Loss: 0.00000904
Iteration 47/1000 | Loss: 0.00000903
Iteration 48/1000 | Loss: 0.00000903
Iteration 49/1000 | Loss: 0.00000903
Iteration 50/1000 | Loss: 0.00000903
Iteration 51/1000 | Loss: 0.00000903
Iteration 52/1000 | Loss: 0.00000903
Iteration 53/1000 | Loss: 0.00000903
Iteration 54/1000 | Loss: 0.00000903
Iteration 55/1000 | Loss: 0.00000902
Iteration 56/1000 | Loss: 0.00000902
Iteration 57/1000 | Loss: 0.00000901
Iteration 58/1000 | Loss: 0.00000900
Iteration 59/1000 | Loss: 0.00000900
Iteration 60/1000 | Loss: 0.00000900
Iteration 61/1000 | Loss: 0.00000899
Iteration 62/1000 | Loss: 0.00000899
Iteration 63/1000 | Loss: 0.00000899
Iteration 64/1000 | Loss: 0.00000898
Iteration 65/1000 | Loss: 0.00000898
Iteration 66/1000 | Loss: 0.00000897
Iteration 67/1000 | Loss: 0.00000897
Iteration 68/1000 | Loss: 0.00000896
Iteration 69/1000 | Loss: 0.00000896
Iteration 70/1000 | Loss: 0.00000895
Iteration 71/1000 | Loss: 0.00000895
Iteration 72/1000 | Loss: 0.00000895
Iteration 73/1000 | Loss: 0.00000894
Iteration 74/1000 | Loss: 0.00000894
Iteration 75/1000 | Loss: 0.00000894
Iteration 76/1000 | Loss: 0.00000893
Iteration 77/1000 | Loss: 0.00000892
Iteration 78/1000 | Loss: 0.00000892
Iteration 79/1000 | Loss: 0.00000890
Iteration 80/1000 | Loss: 0.00000890
Iteration 81/1000 | Loss: 0.00000890
Iteration 82/1000 | Loss: 0.00000890
Iteration 83/1000 | Loss: 0.00000890
Iteration 84/1000 | Loss: 0.00000889
Iteration 85/1000 | Loss: 0.00000889
Iteration 86/1000 | Loss: 0.00000889
Iteration 87/1000 | Loss: 0.00000889
Iteration 88/1000 | Loss: 0.00000888
Iteration 89/1000 | Loss: 0.00000888
Iteration 90/1000 | Loss: 0.00000888
Iteration 91/1000 | Loss: 0.00000887
Iteration 92/1000 | Loss: 0.00000887
Iteration 93/1000 | Loss: 0.00000887
Iteration 94/1000 | Loss: 0.00000886
Iteration 95/1000 | Loss: 0.00000886
Iteration 96/1000 | Loss: 0.00000886
Iteration 97/1000 | Loss: 0.00000886
Iteration 98/1000 | Loss: 0.00000886
Iteration 99/1000 | Loss: 0.00000886
Iteration 100/1000 | Loss: 0.00000886
Iteration 101/1000 | Loss: 0.00000885
Iteration 102/1000 | Loss: 0.00000885
Iteration 103/1000 | Loss: 0.00000884
Iteration 104/1000 | Loss: 0.00000884
Iteration 105/1000 | Loss: 0.00000884
Iteration 106/1000 | Loss: 0.00000884
Iteration 107/1000 | Loss: 0.00000884
Iteration 108/1000 | Loss: 0.00000884
Iteration 109/1000 | Loss: 0.00000884
Iteration 110/1000 | Loss: 0.00000884
Iteration 111/1000 | Loss: 0.00000884
Iteration 112/1000 | Loss: 0.00000884
Iteration 113/1000 | Loss: 0.00000884
Iteration 114/1000 | Loss: 0.00000884
Iteration 115/1000 | Loss: 0.00000884
Iteration 116/1000 | Loss: 0.00000883
Iteration 117/1000 | Loss: 0.00000883
Iteration 118/1000 | Loss: 0.00000883
Iteration 119/1000 | Loss: 0.00000883
Iteration 120/1000 | Loss: 0.00000883
Iteration 121/1000 | Loss: 0.00000883
Iteration 122/1000 | Loss: 0.00000883
Iteration 123/1000 | Loss: 0.00000883
Iteration 124/1000 | Loss: 0.00000883
Iteration 125/1000 | Loss: 0.00000883
Iteration 126/1000 | Loss: 0.00000883
Iteration 127/1000 | Loss: 0.00000883
Iteration 128/1000 | Loss: 0.00000883
Iteration 129/1000 | Loss: 0.00000883
Iteration 130/1000 | Loss: 0.00000883
Iteration 131/1000 | Loss: 0.00000883
Iteration 132/1000 | Loss: 0.00000883
Iteration 133/1000 | Loss: 0.00000882
Iteration 134/1000 | Loss: 0.00000882
Iteration 135/1000 | Loss: 0.00000882
Iteration 136/1000 | Loss: 0.00000882
Iteration 137/1000 | Loss: 0.00000881
Iteration 138/1000 | Loss: 0.00000881
Iteration 139/1000 | Loss: 0.00000881
Iteration 140/1000 | Loss: 0.00000881
Iteration 141/1000 | Loss: 0.00000881
Iteration 142/1000 | Loss: 0.00000881
Iteration 143/1000 | Loss: 0.00000881
Iteration 144/1000 | Loss: 0.00000881
Iteration 145/1000 | Loss: 0.00000881
Iteration 146/1000 | Loss: 0.00000881
Iteration 147/1000 | Loss: 0.00000881
Iteration 148/1000 | Loss: 0.00000880
Iteration 149/1000 | Loss: 0.00000880
Iteration 150/1000 | Loss: 0.00000880
Iteration 151/1000 | Loss: 0.00000880
Iteration 152/1000 | Loss: 0.00000880
Iteration 153/1000 | Loss: 0.00000880
Iteration 154/1000 | Loss: 0.00000880
Iteration 155/1000 | Loss: 0.00000880
Iteration 156/1000 | Loss: 0.00000880
Iteration 157/1000 | Loss: 0.00000880
Iteration 158/1000 | Loss: 0.00000880
Iteration 159/1000 | Loss: 0.00000879
Iteration 160/1000 | Loss: 0.00000879
Iteration 161/1000 | Loss: 0.00000879
Iteration 162/1000 | Loss: 0.00000879
Iteration 163/1000 | Loss: 0.00000879
Iteration 164/1000 | Loss: 0.00000879
Iteration 165/1000 | Loss: 0.00000879
Iteration 166/1000 | Loss: 0.00000879
Iteration 167/1000 | Loss: 0.00000879
Iteration 168/1000 | Loss: 0.00000879
Iteration 169/1000 | Loss: 0.00000878
Iteration 170/1000 | Loss: 0.00000878
Iteration 171/1000 | Loss: 0.00000878
Iteration 172/1000 | Loss: 0.00000878
Iteration 173/1000 | Loss: 0.00000878
Iteration 174/1000 | Loss: 0.00000878
Iteration 175/1000 | Loss: 0.00000877
Iteration 176/1000 | Loss: 0.00000877
Iteration 177/1000 | Loss: 0.00000877
Iteration 178/1000 | Loss: 0.00000877
Iteration 179/1000 | Loss: 0.00000877
Iteration 180/1000 | Loss: 0.00000877
Iteration 181/1000 | Loss: 0.00000877
Iteration 182/1000 | Loss: 0.00000877
Iteration 183/1000 | Loss: 0.00000877
Iteration 184/1000 | Loss: 0.00000877
Iteration 185/1000 | Loss: 0.00000877
Iteration 186/1000 | Loss: 0.00000877
Iteration 187/1000 | Loss: 0.00000877
Iteration 188/1000 | Loss: 0.00000876
Iteration 189/1000 | Loss: 0.00000876
Iteration 190/1000 | Loss: 0.00000876
Iteration 191/1000 | Loss: 0.00000876
Iteration 192/1000 | Loss: 0.00000876
Iteration 193/1000 | Loss: 0.00000876
Iteration 194/1000 | Loss: 0.00000876
Iteration 195/1000 | Loss: 0.00000876
Iteration 196/1000 | Loss: 0.00000875
Iteration 197/1000 | Loss: 0.00000875
Iteration 198/1000 | Loss: 0.00000875
Iteration 199/1000 | Loss: 0.00000875
Iteration 200/1000 | Loss: 0.00000875
Iteration 201/1000 | Loss: 0.00000875
Iteration 202/1000 | Loss: 0.00000875
Iteration 203/1000 | Loss: 0.00000875
Iteration 204/1000 | Loss: 0.00000875
Iteration 205/1000 | Loss: 0.00000875
Iteration 206/1000 | Loss: 0.00000875
Iteration 207/1000 | Loss: 0.00000875
Iteration 208/1000 | Loss: 0.00000875
Iteration 209/1000 | Loss: 0.00000875
Iteration 210/1000 | Loss: 0.00000875
Iteration 211/1000 | Loss: 0.00000875
Iteration 212/1000 | Loss: 0.00000875
Iteration 213/1000 | Loss: 0.00000875
Iteration 214/1000 | Loss: 0.00000875
Iteration 215/1000 | Loss: 0.00000875
Iteration 216/1000 | Loss: 0.00000875
Iteration 217/1000 | Loss: 0.00000875
Iteration 218/1000 | Loss: 0.00000875
Iteration 219/1000 | Loss: 0.00000874
Iteration 220/1000 | Loss: 0.00000874
Iteration 221/1000 | Loss: 0.00000874
Iteration 222/1000 | Loss: 0.00000874
Iteration 223/1000 | Loss: 0.00000874
Iteration 224/1000 | Loss: 0.00000874
Iteration 225/1000 | Loss: 0.00000874
Iteration 226/1000 | Loss: 0.00000874
Iteration 227/1000 | Loss: 0.00000874
Iteration 228/1000 | Loss: 0.00000874
Iteration 229/1000 | Loss: 0.00000874
Iteration 230/1000 | Loss: 0.00000874
Iteration 231/1000 | Loss: 0.00000874
Iteration 232/1000 | Loss: 0.00000874
Iteration 233/1000 | Loss: 0.00000874
Iteration 234/1000 | Loss: 0.00000874
Iteration 235/1000 | Loss: 0.00000874
Iteration 236/1000 | Loss: 0.00000874
Iteration 237/1000 | Loss: 0.00000874
Iteration 238/1000 | Loss: 0.00000874
Iteration 239/1000 | Loss: 0.00000874
Iteration 240/1000 | Loss: 0.00000874
Iteration 241/1000 | Loss: 0.00000874
Iteration 242/1000 | Loss: 0.00000874
Iteration 243/1000 | Loss: 0.00000874
Iteration 244/1000 | Loss: 0.00000874
Iteration 245/1000 | Loss: 0.00000874
Iteration 246/1000 | Loss: 0.00000874
Iteration 247/1000 | Loss: 0.00000874
Iteration 248/1000 | Loss: 0.00000874
Iteration 249/1000 | Loss: 0.00000874
Iteration 250/1000 | Loss: 0.00000874
Iteration 251/1000 | Loss: 0.00000874
Iteration 252/1000 | Loss: 0.00000874
Iteration 253/1000 | Loss: 0.00000874
Iteration 254/1000 | Loss: 0.00000874
Iteration 255/1000 | Loss: 0.00000874
Iteration 256/1000 | Loss: 0.00000874
Iteration 257/1000 | Loss: 0.00000874
Iteration 258/1000 | Loss: 0.00000874
Iteration 259/1000 | Loss: 0.00000874
Iteration 260/1000 | Loss: 0.00000874
Iteration 261/1000 | Loss: 0.00000874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [8.738261385587975e-06, 8.738261385587975e-06, 8.738261385587975e-06, 8.738261385587975e-06, 8.738261385587975e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.738261385587975e-06

Optimization complete. Final v2v error: 2.526801824569702 mm

Highest mean error: 2.9326443672180176 mm for frame 68

Lowest mean error: 2.4444780349731445 mm for frame 101

Saving results

Total time: 38.43448805809021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053689
Iteration 2/25 | Loss: 0.00315680
Iteration 3/25 | Loss: 0.00225763
Iteration 4/25 | Loss: 0.00211760
Iteration 5/25 | Loss: 0.00198595
Iteration 6/25 | Loss: 0.00191344
Iteration 7/25 | Loss: 0.00187529
Iteration 8/25 | Loss: 0.00177433
Iteration 9/25 | Loss: 0.00171783
Iteration 10/25 | Loss: 0.00170703
Iteration 11/25 | Loss: 0.00169015
Iteration 12/25 | Loss: 0.00167825
Iteration 13/25 | Loss: 0.00166860
Iteration 14/25 | Loss: 0.00166555
Iteration 15/25 | Loss: 0.00165836
Iteration 16/25 | Loss: 0.00165378
Iteration 17/25 | Loss: 0.00165041
Iteration 18/25 | Loss: 0.00164706
Iteration 19/25 | Loss: 0.00164482
Iteration 20/25 | Loss: 0.00164379
Iteration 21/25 | Loss: 0.00164534
Iteration 22/25 | Loss: 0.00164381
Iteration 23/25 | Loss: 0.00164342
Iteration 24/25 | Loss: 0.00164121
Iteration 25/25 | Loss: 0.00163823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31504643
Iteration 2/25 | Loss: 0.01128816
Iteration 3/25 | Loss: 0.00717733
Iteration 4/25 | Loss: 0.00717728
Iteration 5/25 | Loss: 0.00717728
Iteration 6/25 | Loss: 0.00717728
Iteration 7/25 | Loss: 0.00717728
Iteration 8/25 | Loss: 0.00717728
Iteration 9/25 | Loss: 0.00717728
Iteration 10/25 | Loss: 0.00717728
Iteration 11/25 | Loss: 0.00717728
Iteration 12/25 | Loss: 0.00717728
Iteration 13/25 | Loss: 0.00717728
Iteration 14/25 | Loss: 0.00717728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.007177276071161032, 0.007177276071161032, 0.007177276071161032, 0.007177276071161032, 0.007177276071161032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007177276071161032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00717728
Iteration 2/1000 | Loss: 0.00882216
Iteration 3/1000 | Loss: 0.01784836
Iteration 4/1000 | Loss: 0.00920268
Iteration 5/1000 | Loss: 0.00582681
Iteration 6/1000 | Loss: 0.00475777
Iteration 7/1000 | Loss: 0.00315545
Iteration 8/1000 | Loss: 0.00449719
Iteration 9/1000 | Loss: 0.00184287
Iteration 10/1000 | Loss: 0.00349855
Iteration 11/1000 | Loss: 0.00419270
Iteration 12/1000 | Loss: 0.00343784
Iteration 13/1000 | Loss: 0.00410564
Iteration 14/1000 | Loss: 0.00119430
Iteration 15/1000 | Loss: 0.00036496
Iteration 16/1000 | Loss: 0.00073599
Iteration 17/1000 | Loss: 0.00138643
Iteration 18/1000 | Loss: 0.00283153
Iteration 19/1000 | Loss: 0.00282136
Iteration 20/1000 | Loss: 0.00324580
Iteration 21/1000 | Loss: 0.00122630
Iteration 22/1000 | Loss: 0.00137694
Iteration 23/1000 | Loss: 0.00056687
Iteration 24/1000 | Loss: 0.00057487
Iteration 25/1000 | Loss: 0.00021784
Iteration 26/1000 | Loss: 0.00036966
Iteration 27/1000 | Loss: 0.00029361
Iteration 28/1000 | Loss: 0.00164881
Iteration 29/1000 | Loss: 0.00043912
Iteration 30/1000 | Loss: 0.00136548
Iteration 31/1000 | Loss: 0.00074380
Iteration 32/1000 | Loss: 0.00035060
Iteration 33/1000 | Loss: 0.00053446
Iteration 34/1000 | Loss: 0.00038973
Iteration 35/1000 | Loss: 0.00014099
Iteration 36/1000 | Loss: 0.00103760
Iteration 37/1000 | Loss: 0.00077660
Iteration 38/1000 | Loss: 0.00083504
Iteration 39/1000 | Loss: 0.00115266
Iteration 40/1000 | Loss: 0.00055713
Iteration 41/1000 | Loss: 0.00160227
Iteration 42/1000 | Loss: 0.00042445
Iteration 43/1000 | Loss: 0.00014005
Iteration 44/1000 | Loss: 0.00033309
Iteration 45/1000 | Loss: 0.00051578
Iteration 46/1000 | Loss: 0.00092557
Iteration 47/1000 | Loss: 0.00046990
Iteration 48/1000 | Loss: 0.00052983
Iteration 49/1000 | Loss: 0.00019797
Iteration 50/1000 | Loss: 0.00036458
Iteration 51/1000 | Loss: 0.00018666
Iteration 52/1000 | Loss: 0.00011066
Iteration 53/1000 | Loss: 0.00009259
Iteration 54/1000 | Loss: 0.00048134
Iteration 55/1000 | Loss: 0.00142496
Iteration 56/1000 | Loss: 0.00098075
Iteration 57/1000 | Loss: 0.00033177
Iteration 58/1000 | Loss: 0.00014953
Iteration 59/1000 | Loss: 0.00009229
Iteration 60/1000 | Loss: 0.00008117
Iteration 61/1000 | Loss: 0.00128656
Iteration 62/1000 | Loss: 0.00108937
Iteration 63/1000 | Loss: 0.00037211
Iteration 64/1000 | Loss: 0.00048007
Iteration 65/1000 | Loss: 0.00058914
Iteration 66/1000 | Loss: 0.00060796
Iteration 67/1000 | Loss: 0.00050427
Iteration 68/1000 | Loss: 0.00012937
Iteration 69/1000 | Loss: 0.00006842
Iteration 70/1000 | Loss: 0.00036500
Iteration 71/1000 | Loss: 0.00054556
Iteration 72/1000 | Loss: 0.00013678
Iteration 73/1000 | Loss: 0.00046039
Iteration 74/1000 | Loss: 0.00049749
Iteration 75/1000 | Loss: 0.00006068
Iteration 76/1000 | Loss: 0.00061833
Iteration 77/1000 | Loss: 0.00063188
Iteration 78/1000 | Loss: 0.00074988
Iteration 79/1000 | Loss: 0.00041616
Iteration 80/1000 | Loss: 0.00027026
Iteration 81/1000 | Loss: 0.00005367
Iteration 82/1000 | Loss: 0.00005023
Iteration 83/1000 | Loss: 0.00012690
Iteration 84/1000 | Loss: 0.00037227
Iteration 85/1000 | Loss: 0.00005524
Iteration 86/1000 | Loss: 0.00043709
Iteration 87/1000 | Loss: 0.00094575
Iteration 88/1000 | Loss: 0.00154557
Iteration 89/1000 | Loss: 0.00007494
Iteration 90/1000 | Loss: 0.00005202
Iteration 91/1000 | Loss: 0.00019323
Iteration 92/1000 | Loss: 0.00004951
Iteration 93/1000 | Loss: 0.00023262
Iteration 94/1000 | Loss: 0.00004440
Iteration 95/1000 | Loss: 0.00004126
Iteration 96/1000 | Loss: 0.00041612
Iteration 97/1000 | Loss: 0.00004218
Iteration 98/1000 | Loss: 0.00003866
Iteration 99/1000 | Loss: 0.00003655
Iteration 100/1000 | Loss: 0.00003569
Iteration 101/1000 | Loss: 0.00003516
Iteration 102/1000 | Loss: 0.00003470
Iteration 103/1000 | Loss: 0.00010616
Iteration 104/1000 | Loss: 0.00040542
Iteration 105/1000 | Loss: 0.00004482
Iteration 106/1000 | Loss: 0.00003700
Iteration 107/1000 | Loss: 0.00003346
Iteration 108/1000 | Loss: 0.00003146
Iteration 109/1000 | Loss: 0.00003077
Iteration 110/1000 | Loss: 0.00003021
Iteration 111/1000 | Loss: 0.00002978
Iteration 112/1000 | Loss: 0.00002953
Iteration 113/1000 | Loss: 0.00002948
Iteration 114/1000 | Loss: 0.00002947
Iteration 115/1000 | Loss: 0.00002933
Iteration 116/1000 | Loss: 0.00038371
Iteration 117/1000 | Loss: 0.00036097
Iteration 118/1000 | Loss: 0.00003331
Iteration 119/1000 | Loss: 0.00002943
Iteration 120/1000 | Loss: 0.00002828
Iteration 121/1000 | Loss: 0.00053237
Iteration 122/1000 | Loss: 0.00003038
Iteration 123/1000 | Loss: 0.00002665
Iteration 124/1000 | Loss: 0.00009583
Iteration 125/1000 | Loss: 0.00002479
Iteration 126/1000 | Loss: 0.00002388
Iteration 127/1000 | Loss: 0.00002329
Iteration 128/1000 | Loss: 0.00002285
Iteration 129/1000 | Loss: 0.00002271
Iteration 130/1000 | Loss: 0.00002269
Iteration 131/1000 | Loss: 0.00002262
Iteration 132/1000 | Loss: 0.00002255
Iteration 133/1000 | Loss: 0.00002253
Iteration 134/1000 | Loss: 0.00002251
Iteration 135/1000 | Loss: 0.00002250
Iteration 136/1000 | Loss: 0.00002250
Iteration 137/1000 | Loss: 0.00002249
Iteration 138/1000 | Loss: 0.00002247
Iteration 139/1000 | Loss: 0.00002244
Iteration 140/1000 | Loss: 0.00002243
Iteration 141/1000 | Loss: 0.00002243
Iteration 142/1000 | Loss: 0.00002242
Iteration 143/1000 | Loss: 0.00002242
Iteration 144/1000 | Loss: 0.00002240
Iteration 145/1000 | Loss: 0.00002239
Iteration 146/1000 | Loss: 0.00002239
Iteration 147/1000 | Loss: 0.00002239
Iteration 148/1000 | Loss: 0.00002239
Iteration 149/1000 | Loss: 0.00002239
Iteration 150/1000 | Loss: 0.00002239
Iteration 151/1000 | Loss: 0.00002239
Iteration 152/1000 | Loss: 0.00002239
Iteration 153/1000 | Loss: 0.00002239
Iteration 154/1000 | Loss: 0.00002238
Iteration 155/1000 | Loss: 0.00002238
Iteration 156/1000 | Loss: 0.00002237
Iteration 157/1000 | Loss: 0.00002237
Iteration 158/1000 | Loss: 0.00002236
Iteration 159/1000 | Loss: 0.00002236
Iteration 160/1000 | Loss: 0.00002236
Iteration 161/1000 | Loss: 0.00002236
Iteration 162/1000 | Loss: 0.00002236
Iteration 163/1000 | Loss: 0.00002236
Iteration 164/1000 | Loss: 0.00002236
Iteration 165/1000 | Loss: 0.00002236
Iteration 166/1000 | Loss: 0.00002235
Iteration 167/1000 | Loss: 0.00002235
Iteration 168/1000 | Loss: 0.00002235
Iteration 169/1000 | Loss: 0.00002235
Iteration 170/1000 | Loss: 0.00002234
Iteration 171/1000 | Loss: 0.00002234
Iteration 172/1000 | Loss: 0.00002234
Iteration 173/1000 | Loss: 0.00002234
Iteration 174/1000 | Loss: 0.00002234
Iteration 175/1000 | Loss: 0.00002234
Iteration 176/1000 | Loss: 0.00002234
Iteration 177/1000 | Loss: 0.00002234
Iteration 178/1000 | Loss: 0.00002234
Iteration 179/1000 | Loss: 0.00002234
Iteration 180/1000 | Loss: 0.00002234
Iteration 181/1000 | Loss: 0.00002234
Iteration 182/1000 | Loss: 0.00002234
Iteration 183/1000 | Loss: 0.00002234
Iteration 184/1000 | Loss: 0.00002234
Iteration 185/1000 | Loss: 0.00002233
Iteration 186/1000 | Loss: 0.00002233
Iteration 187/1000 | Loss: 0.00002233
Iteration 188/1000 | Loss: 0.00002233
Iteration 189/1000 | Loss: 0.00002233
Iteration 190/1000 | Loss: 0.00002233
Iteration 191/1000 | Loss: 0.00002233
Iteration 192/1000 | Loss: 0.00002233
Iteration 193/1000 | Loss: 0.00002233
Iteration 194/1000 | Loss: 0.00002233
Iteration 195/1000 | Loss: 0.00002233
Iteration 196/1000 | Loss: 0.00002233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.2334905224852264e-05, 2.2334905224852264e-05, 2.2334905224852264e-05, 2.2334905224852264e-05, 2.2334905224852264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2334905224852264e-05

Optimization complete. Final v2v error: 3.4819774627685547 mm

Highest mean error: 12.419899940490723 mm for frame 235

Lowest mean error: 3.0393338203430176 mm for frame 142

Saving results

Total time: 257.1822159290314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949388
Iteration 2/25 | Loss: 0.00122152
Iteration 3/25 | Loss: 0.00107256
Iteration 4/25 | Loss: 0.00104842
Iteration 5/25 | Loss: 0.00103974
Iteration 6/25 | Loss: 0.00103736
Iteration 7/25 | Loss: 0.00103717
Iteration 8/25 | Loss: 0.00103717
Iteration 9/25 | Loss: 0.00103717
Iteration 10/25 | Loss: 0.00103717
Iteration 11/25 | Loss: 0.00103717
Iteration 12/25 | Loss: 0.00103717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010371708776801825, 0.0010371708776801825, 0.0010371708776801825, 0.0010371708776801825, 0.0010371708776801825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010371708776801825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35067439
Iteration 2/25 | Loss: 0.00101390
Iteration 3/25 | Loss: 0.00101389
Iteration 4/25 | Loss: 0.00101389
Iteration 5/25 | Loss: 0.00101389
Iteration 6/25 | Loss: 0.00101389
Iteration 7/25 | Loss: 0.00101389
Iteration 8/25 | Loss: 0.00101389
Iteration 9/25 | Loss: 0.00101389
Iteration 10/25 | Loss: 0.00101389
Iteration 11/25 | Loss: 0.00101389
Iteration 12/25 | Loss: 0.00101389
Iteration 13/25 | Loss: 0.00101389
Iteration 14/25 | Loss: 0.00101389
Iteration 15/25 | Loss: 0.00101389
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001013892819173634, 0.001013892819173634, 0.001013892819173634, 0.001013892819173634, 0.001013892819173634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001013892819173634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101389
Iteration 2/1000 | Loss: 0.00005435
Iteration 3/1000 | Loss: 0.00003526
Iteration 4/1000 | Loss: 0.00002785
Iteration 5/1000 | Loss: 0.00002438
Iteration 6/1000 | Loss: 0.00002311
Iteration 7/1000 | Loss: 0.00002217
Iteration 8/1000 | Loss: 0.00002124
Iteration 9/1000 | Loss: 0.00002055
Iteration 10/1000 | Loss: 0.00002004
Iteration 11/1000 | Loss: 0.00001972
Iteration 12/1000 | Loss: 0.00001961
Iteration 13/1000 | Loss: 0.00001954
Iteration 14/1000 | Loss: 0.00001943
Iteration 15/1000 | Loss: 0.00001933
Iteration 16/1000 | Loss: 0.00001932
Iteration 17/1000 | Loss: 0.00001932
Iteration 18/1000 | Loss: 0.00001927
Iteration 19/1000 | Loss: 0.00001926
Iteration 20/1000 | Loss: 0.00001924
Iteration 21/1000 | Loss: 0.00001924
Iteration 22/1000 | Loss: 0.00001923
Iteration 23/1000 | Loss: 0.00001923
Iteration 24/1000 | Loss: 0.00001922
Iteration 25/1000 | Loss: 0.00001922
Iteration 26/1000 | Loss: 0.00001922
Iteration 27/1000 | Loss: 0.00001922
Iteration 28/1000 | Loss: 0.00001922
Iteration 29/1000 | Loss: 0.00001922
Iteration 30/1000 | Loss: 0.00001922
Iteration 31/1000 | Loss: 0.00001922
Iteration 32/1000 | Loss: 0.00001922
Iteration 33/1000 | Loss: 0.00001922
Iteration 34/1000 | Loss: 0.00001922
Iteration 35/1000 | Loss: 0.00001921
Iteration 36/1000 | Loss: 0.00001921
Iteration 37/1000 | Loss: 0.00001921
Iteration 38/1000 | Loss: 0.00001921
Iteration 39/1000 | Loss: 0.00001921
Iteration 40/1000 | Loss: 0.00001921
Iteration 41/1000 | Loss: 0.00001920
Iteration 42/1000 | Loss: 0.00001920
Iteration 43/1000 | Loss: 0.00001920
Iteration 44/1000 | Loss: 0.00001920
Iteration 45/1000 | Loss: 0.00001919
Iteration 46/1000 | Loss: 0.00001919
Iteration 47/1000 | Loss: 0.00001919
Iteration 48/1000 | Loss: 0.00001919
Iteration 49/1000 | Loss: 0.00001919
Iteration 50/1000 | Loss: 0.00001919
Iteration 51/1000 | Loss: 0.00001919
Iteration 52/1000 | Loss: 0.00001919
Iteration 53/1000 | Loss: 0.00001918
Iteration 54/1000 | Loss: 0.00001918
Iteration 55/1000 | Loss: 0.00001918
Iteration 56/1000 | Loss: 0.00001918
Iteration 57/1000 | Loss: 0.00001918
Iteration 58/1000 | Loss: 0.00001918
Iteration 59/1000 | Loss: 0.00001918
Iteration 60/1000 | Loss: 0.00001918
Iteration 61/1000 | Loss: 0.00001918
Iteration 62/1000 | Loss: 0.00001917
Iteration 63/1000 | Loss: 0.00001917
Iteration 64/1000 | Loss: 0.00001917
Iteration 65/1000 | Loss: 0.00001917
Iteration 66/1000 | Loss: 0.00001916
Iteration 67/1000 | Loss: 0.00001916
Iteration 68/1000 | Loss: 0.00001916
Iteration 69/1000 | Loss: 0.00001916
Iteration 70/1000 | Loss: 0.00001916
Iteration 71/1000 | Loss: 0.00001916
Iteration 72/1000 | Loss: 0.00001916
Iteration 73/1000 | Loss: 0.00001915
Iteration 74/1000 | Loss: 0.00001915
Iteration 75/1000 | Loss: 0.00001915
Iteration 76/1000 | Loss: 0.00001915
Iteration 77/1000 | Loss: 0.00001915
Iteration 78/1000 | Loss: 0.00001915
Iteration 79/1000 | Loss: 0.00001915
Iteration 80/1000 | Loss: 0.00001914
Iteration 81/1000 | Loss: 0.00001914
Iteration 82/1000 | Loss: 0.00001914
Iteration 83/1000 | Loss: 0.00001914
Iteration 84/1000 | Loss: 0.00001914
Iteration 85/1000 | Loss: 0.00001913
Iteration 86/1000 | Loss: 0.00001913
Iteration 87/1000 | Loss: 0.00001913
Iteration 88/1000 | Loss: 0.00001913
Iteration 89/1000 | Loss: 0.00001913
Iteration 90/1000 | Loss: 0.00001913
Iteration 91/1000 | Loss: 0.00001913
Iteration 92/1000 | Loss: 0.00001913
Iteration 93/1000 | Loss: 0.00001913
Iteration 94/1000 | Loss: 0.00001913
Iteration 95/1000 | Loss: 0.00001913
Iteration 96/1000 | Loss: 0.00001912
Iteration 97/1000 | Loss: 0.00001912
Iteration 98/1000 | Loss: 0.00001912
Iteration 99/1000 | Loss: 0.00001912
Iteration 100/1000 | Loss: 0.00001912
Iteration 101/1000 | Loss: 0.00001912
Iteration 102/1000 | Loss: 0.00001912
Iteration 103/1000 | Loss: 0.00001912
Iteration 104/1000 | Loss: 0.00001912
Iteration 105/1000 | Loss: 0.00001912
Iteration 106/1000 | Loss: 0.00001912
Iteration 107/1000 | Loss: 0.00001912
Iteration 108/1000 | Loss: 0.00001911
Iteration 109/1000 | Loss: 0.00001911
Iteration 110/1000 | Loss: 0.00001911
Iteration 111/1000 | Loss: 0.00001911
Iteration 112/1000 | Loss: 0.00001911
Iteration 113/1000 | Loss: 0.00001911
Iteration 114/1000 | Loss: 0.00001911
Iteration 115/1000 | Loss: 0.00001911
Iteration 116/1000 | Loss: 0.00001911
Iteration 117/1000 | Loss: 0.00001911
Iteration 118/1000 | Loss: 0.00001911
Iteration 119/1000 | Loss: 0.00001911
Iteration 120/1000 | Loss: 0.00001911
Iteration 121/1000 | Loss: 0.00001911
Iteration 122/1000 | Loss: 0.00001911
Iteration 123/1000 | Loss: 0.00001911
Iteration 124/1000 | Loss: 0.00001911
Iteration 125/1000 | Loss: 0.00001910
Iteration 126/1000 | Loss: 0.00001910
Iteration 127/1000 | Loss: 0.00001910
Iteration 128/1000 | Loss: 0.00001910
Iteration 129/1000 | Loss: 0.00001910
Iteration 130/1000 | Loss: 0.00001910
Iteration 131/1000 | Loss: 0.00001910
Iteration 132/1000 | Loss: 0.00001910
Iteration 133/1000 | Loss: 0.00001910
Iteration 134/1000 | Loss: 0.00001910
Iteration 135/1000 | Loss: 0.00001910
Iteration 136/1000 | Loss: 0.00001910
Iteration 137/1000 | Loss: 0.00001910
Iteration 138/1000 | Loss: 0.00001910
Iteration 139/1000 | Loss: 0.00001910
Iteration 140/1000 | Loss: 0.00001909
Iteration 141/1000 | Loss: 0.00001909
Iteration 142/1000 | Loss: 0.00001909
Iteration 143/1000 | Loss: 0.00001909
Iteration 144/1000 | Loss: 0.00001909
Iteration 145/1000 | Loss: 0.00001909
Iteration 146/1000 | Loss: 0.00001909
Iteration 147/1000 | Loss: 0.00001909
Iteration 148/1000 | Loss: 0.00001909
Iteration 149/1000 | Loss: 0.00001909
Iteration 150/1000 | Loss: 0.00001909
Iteration 151/1000 | Loss: 0.00001909
Iteration 152/1000 | Loss: 0.00001909
Iteration 153/1000 | Loss: 0.00001909
Iteration 154/1000 | Loss: 0.00001909
Iteration 155/1000 | Loss: 0.00001909
Iteration 156/1000 | Loss: 0.00001909
Iteration 157/1000 | Loss: 0.00001909
Iteration 158/1000 | Loss: 0.00001909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.9093638911726885e-05, 1.9093638911726885e-05, 1.9093638911726885e-05, 1.9093638911726885e-05, 1.9093638911726885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9093638911726885e-05

Optimization complete. Final v2v error: 3.5795445442199707 mm

Highest mean error: 5.63806676864624 mm for frame 70

Lowest mean error: 3.0202414989471436 mm for frame 144

Saving results

Total time: 35.84642744064331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399039
Iteration 2/25 | Loss: 0.00105540
Iteration 3/25 | Loss: 0.00096347
Iteration 4/25 | Loss: 0.00095591
Iteration 5/25 | Loss: 0.00095328
Iteration 6/25 | Loss: 0.00095253
Iteration 7/25 | Loss: 0.00095253
Iteration 8/25 | Loss: 0.00095253
Iteration 9/25 | Loss: 0.00095253
Iteration 10/25 | Loss: 0.00095253
Iteration 11/25 | Loss: 0.00095253
Iteration 12/25 | Loss: 0.00095253
Iteration 13/25 | Loss: 0.00095253
Iteration 14/25 | Loss: 0.00095253
Iteration 15/25 | Loss: 0.00095253
Iteration 16/25 | Loss: 0.00095253
Iteration 17/25 | Loss: 0.00095253
Iteration 18/25 | Loss: 0.00095253
Iteration 19/25 | Loss: 0.00095253
Iteration 20/25 | Loss: 0.00095253
Iteration 21/25 | Loss: 0.00095253
Iteration 22/25 | Loss: 0.00095253
Iteration 23/25 | Loss: 0.00095253
Iteration 24/25 | Loss: 0.00095253
Iteration 25/25 | Loss: 0.00095253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33954000
Iteration 2/25 | Loss: 0.00095635
Iteration 3/25 | Loss: 0.00095635
Iteration 4/25 | Loss: 0.00095634
Iteration 5/25 | Loss: 0.00095634
Iteration 6/25 | Loss: 0.00095634
Iteration 7/25 | Loss: 0.00095634
Iteration 8/25 | Loss: 0.00095634
Iteration 9/25 | Loss: 0.00095634
Iteration 10/25 | Loss: 0.00095634
Iteration 11/25 | Loss: 0.00095634
Iteration 12/25 | Loss: 0.00095634
Iteration 13/25 | Loss: 0.00095634
Iteration 14/25 | Loss: 0.00095634
Iteration 15/25 | Loss: 0.00095634
Iteration 16/25 | Loss: 0.00095634
Iteration 17/25 | Loss: 0.00095634
Iteration 18/25 | Loss: 0.00095634
Iteration 19/25 | Loss: 0.00095634
Iteration 20/25 | Loss: 0.00095634
Iteration 21/25 | Loss: 0.00095634
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009563424973748624, 0.0009563424973748624, 0.0009563424973748624, 0.0009563424973748624, 0.0009563424973748624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009563424973748624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095634
Iteration 2/1000 | Loss: 0.00001999
Iteration 3/1000 | Loss: 0.00001050
Iteration 4/1000 | Loss: 0.00000868
Iteration 5/1000 | Loss: 0.00000820
Iteration 6/1000 | Loss: 0.00000795
Iteration 7/1000 | Loss: 0.00000794
Iteration 8/1000 | Loss: 0.00000763
Iteration 9/1000 | Loss: 0.00000747
Iteration 10/1000 | Loss: 0.00000747
Iteration 11/1000 | Loss: 0.00000746
Iteration 12/1000 | Loss: 0.00000742
Iteration 13/1000 | Loss: 0.00000741
Iteration 14/1000 | Loss: 0.00000740
Iteration 15/1000 | Loss: 0.00000740
Iteration 16/1000 | Loss: 0.00000739
Iteration 17/1000 | Loss: 0.00000738
Iteration 18/1000 | Loss: 0.00000734
Iteration 19/1000 | Loss: 0.00000733
Iteration 20/1000 | Loss: 0.00000732
Iteration 21/1000 | Loss: 0.00000732
Iteration 22/1000 | Loss: 0.00000732
Iteration 23/1000 | Loss: 0.00000731
Iteration 24/1000 | Loss: 0.00000730
Iteration 25/1000 | Loss: 0.00000730
Iteration 26/1000 | Loss: 0.00000729
Iteration 27/1000 | Loss: 0.00000728
Iteration 28/1000 | Loss: 0.00000728
Iteration 29/1000 | Loss: 0.00000728
Iteration 30/1000 | Loss: 0.00000727
Iteration 31/1000 | Loss: 0.00000726
Iteration 32/1000 | Loss: 0.00000725
Iteration 33/1000 | Loss: 0.00000724
Iteration 34/1000 | Loss: 0.00000724
Iteration 35/1000 | Loss: 0.00000724
Iteration 36/1000 | Loss: 0.00000723
Iteration 37/1000 | Loss: 0.00000723
Iteration 38/1000 | Loss: 0.00000723
Iteration 39/1000 | Loss: 0.00000723
Iteration 40/1000 | Loss: 0.00000723
Iteration 41/1000 | Loss: 0.00000723
Iteration 42/1000 | Loss: 0.00000722
Iteration 43/1000 | Loss: 0.00000722
Iteration 44/1000 | Loss: 0.00000722
Iteration 45/1000 | Loss: 0.00000722
Iteration 46/1000 | Loss: 0.00000721
Iteration 47/1000 | Loss: 0.00000721
Iteration 48/1000 | Loss: 0.00000720
Iteration 49/1000 | Loss: 0.00000720
Iteration 50/1000 | Loss: 0.00000719
Iteration 51/1000 | Loss: 0.00000719
Iteration 52/1000 | Loss: 0.00000719
Iteration 53/1000 | Loss: 0.00000719
Iteration 54/1000 | Loss: 0.00000719
Iteration 55/1000 | Loss: 0.00000719
Iteration 56/1000 | Loss: 0.00000718
Iteration 57/1000 | Loss: 0.00000718
Iteration 58/1000 | Loss: 0.00000718
Iteration 59/1000 | Loss: 0.00000718
Iteration 60/1000 | Loss: 0.00000717
Iteration 61/1000 | Loss: 0.00000717
Iteration 62/1000 | Loss: 0.00000717
Iteration 63/1000 | Loss: 0.00000717
Iteration 64/1000 | Loss: 0.00000716
Iteration 65/1000 | Loss: 0.00000716
Iteration 66/1000 | Loss: 0.00000715
Iteration 67/1000 | Loss: 0.00000715
Iteration 68/1000 | Loss: 0.00000715
Iteration 69/1000 | Loss: 0.00000715
Iteration 70/1000 | Loss: 0.00000714
Iteration 71/1000 | Loss: 0.00000714
Iteration 72/1000 | Loss: 0.00000714
Iteration 73/1000 | Loss: 0.00000714
Iteration 74/1000 | Loss: 0.00000714
Iteration 75/1000 | Loss: 0.00000714
Iteration 76/1000 | Loss: 0.00000714
Iteration 77/1000 | Loss: 0.00000714
Iteration 78/1000 | Loss: 0.00000714
Iteration 79/1000 | Loss: 0.00000714
Iteration 80/1000 | Loss: 0.00000714
Iteration 81/1000 | Loss: 0.00000714
Iteration 82/1000 | Loss: 0.00000714
Iteration 83/1000 | Loss: 0.00000713
Iteration 84/1000 | Loss: 0.00000713
Iteration 85/1000 | Loss: 0.00000713
Iteration 86/1000 | Loss: 0.00000713
Iteration 87/1000 | Loss: 0.00000712
Iteration 88/1000 | Loss: 0.00000712
Iteration 89/1000 | Loss: 0.00000712
Iteration 90/1000 | Loss: 0.00000712
Iteration 91/1000 | Loss: 0.00000712
Iteration 92/1000 | Loss: 0.00000712
Iteration 93/1000 | Loss: 0.00000712
Iteration 94/1000 | Loss: 0.00000712
Iteration 95/1000 | Loss: 0.00000712
Iteration 96/1000 | Loss: 0.00000712
Iteration 97/1000 | Loss: 0.00000712
Iteration 98/1000 | Loss: 0.00000712
Iteration 99/1000 | Loss: 0.00000712
Iteration 100/1000 | Loss: 0.00000712
Iteration 101/1000 | Loss: 0.00000711
Iteration 102/1000 | Loss: 0.00000711
Iteration 103/1000 | Loss: 0.00000711
Iteration 104/1000 | Loss: 0.00000710
Iteration 105/1000 | Loss: 0.00000710
Iteration 106/1000 | Loss: 0.00000709
Iteration 107/1000 | Loss: 0.00000709
Iteration 108/1000 | Loss: 0.00000709
Iteration 109/1000 | Loss: 0.00000709
Iteration 110/1000 | Loss: 0.00000708
Iteration 111/1000 | Loss: 0.00000708
Iteration 112/1000 | Loss: 0.00000708
Iteration 113/1000 | Loss: 0.00000708
Iteration 114/1000 | Loss: 0.00000708
Iteration 115/1000 | Loss: 0.00000708
Iteration 116/1000 | Loss: 0.00000708
Iteration 117/1000 | Loss: 0.00000708
Iteration 118/1000 | Loss: 0.00000708
Iteration 119/1000 | Loss: 0.00000707
Iteration 120/1000 | Loss: 0.00000707
Iteration 121/1000 | Loss: 0.00000707
Iteration 122/1000 | Loss: 0.00000707
Iteration 123/1000 | Loss: 0.00000706
Iteration 124/1000 | Loss: 0.00000706
Iteration 125/1000 | Loss: 0.00000705
Iteration 126/1000 | Loss: 0.00000705
Iteration 127/1000 | Loss: 0.00000705
Iteration 128/1000 | Loss: 0.00000705
Iteration 129/1000 | Loss: 0.00000704
Iteration 130/1000 | Loss: 0.00000704
Iteration 131/1000 | Loss: 0.00000704
Iteration 132/1000 | Loss: 0.00000704
Iteration 133/1000 | Loss: 0.00000704
Iteration 134/1000 | Loss: 0.00000704
Iteration 135/1000 | Loss: 0.00000704
Iteration 136/1000 | Loss: 0.00000704
Iteration 137/1000 | Loss: 0.00000704
Iteration 138/1000 | Loss: 0.00000704
Iteration 139/1000 | Loss: 0.00000704
Iteration 140/1000 | Loss: 0.00000703
Iteration 141/1000 | Loss: 0.00000703
Iteration 142/1000 | Loss: 0.00000703
Iteration 143/1000 | Loss: 0.00000703
Iteration 144/1000 | Loss: 0.00000703
Iteration 145/1000 | Loss: 0.00000703
Iteration 146/1000 | Loss: 0.00000703
Iteration 147/1000 | Loss: 0.00000703
Iteration 148/1000 | Loss: 0.00000702
Iteration 149/1000 | Loss: 0.00000702
Iteration 150/1000 | Loss: 0.00000702
Iteration 151/1000 | Loss: 0.00000702
Iteration 152/1000 | Loss: 0.00000702
Iteration 153/1000 | Loss: 0.00000702
Iteration 154/1000 | Loss: 0.00000702
Iteration 155/1000 | Loss: 0.00000701
Iteration 156/1000 | Loss: 0.00000701
Iteration 157/1000 | Loss: 0.00000700
Iteration 158/1000 | Loss: 0.00000700
Iteration 159/1000 | Loss: 0.00000700
Iteration 160/1000 | Loss: 0.00000700
Iteration 161/1000 | Loss: 0.00000700
Iteration 162/1000 | Loss: 0.00000699
Iteration 163/1000 | Loss: 0.00000699
Iteration 164/1000 | Loss: 0.00000699
Iteration 165/1000 | Loss: 0.00000699
Iteration 166/1000 | Loss: 0.00000699
Iteration 167/1000 | Loss: 0.00000699
Iteration 168/1000 | Loss: 0.00000699
Iteration 169/1000 | Loss: 0.00000699
Iteration 170/1000 | Loss: 0.00000699
Iteration 171/1000 | Loss: 0.00000699
Iteration 172/1000 | Loss: 0.00000699
Iteration 173/1000 | Loss: 0.00000698
Iteration 174/1000 | Loss: 0.00000698
Iteration 175/1000 | Loss: 0.00000698
Iteration 176/1000 | Loss: 0.00000698
Iteration 177/1000 | Loss: 0.00000698
Iteration 178/1000 | Loss: 0.00000698
Iteration 179/1000 | Loss: 0.00000698
Iteration 180/1000 | Loss: 0.00000698
Iteration 181/1000 | Loss: 0.00000698
Iteration 182/1000 | Loss: 0.00000698
Iteration 183/1000 | Loss: 0.00000698
Iteration 184/1000 | Loss: 0.00000698
Iteration 185/1000 | Loss: 0.00000698
Iteration 186/1000 | Loss: 0.00000698
Iteration 187/1000 | Loss: 0.00000697
Iteration 188/1000 | Loss: 0.00000697
Iteration 189/1000 | Loss: 0.00000697
Iteration 190/1000 | Loss: 0.00000697
Iteration 191/1000 | Loss: 0.00000697
Iteration 192/1000 | Loss: 0.00000697
Iteration 193/1000 | Loss: 0.00000697
Iteration 194/1000 | Loss: 0.00000697
Iteration 195/1000 | Loss: 0.00000697
Iteration 196/1000 | Loss: 0.00000697
Iteration 197/1000 | Loss: 0.00000697
Iteration 198/1000 | Loss: 0.00000696
Iteration 199/1000 | Loss: 0.00000696
Iteration 200/1000 | Loss: 0.00000696
Iteration 201/1000 | Loss: 0.00000696
Iteration 202/1000 | Loss: 0.00000696
Iteration 203/1000 | Loss: 0.00000696
Iteration 204/1000 | Loss: 0.00000695
Iteration 205/1000 | Loss: 0.00000695
Iteration 206/1000 | Loss: 0.00000695
Iteration 207/1000 | Loss: 0.00000695
Iteration 208/1000 | Loss: 0.00000695
Iteration 209/1000 | Loss: 0.00000694
Iteration 210/1000 | Loss: 0.00000694
Iteration 211/1000 | Loss: 0.00000694
Iteration 212/1000 | Loss: 0.00000693
Iteration 213/1000 | Loss: 0.00000693
Iteration 214/1000 | Loss: 0.00000693
Iteration 215/1000 | Loss: 0.00000693
Iteration 216/1000 | Loss: 0.00000693
Iteration 217/1000 | Loss: 0.00000693
Iteration 218/1000 | Loss: 0.00000693
Iteration 219/1000 | Loss: 0.00000693
Iteration 220/1000 | Loss: 0.00000692
Iteration 221/1000 | Loss: 0.00000692
Iteration 222/1000 | Loss: 0.00000692
Iteration 223/1000 | Loss: 0.00000692
Iteration 224/1000 | Loss: 0.00000692
Iteration 225/1000 | Loss: 0.00000692
Iteration 226/1000 | Loss: 0.00000692
Iteration 227/1000 | Loss: 0.00000692
Iteration 228/1000 | Loss: 0.00000692
Iteration 229/1000 | Loss: 0.00000692
Iteration 230/1000 | Loss: 0.00000692
Iteration 231/1000 | Loss: 0.00000692
Iteration 232/1000 | Loss: 0.00000692
Iteration 233/1000 | Loss: 0.00000692
Iteration 234/1000 | Loss: 0.00000692
Iteration 235/1000 | Loss: 0.00000692
Iteration 236/1000 | Loss: 0.00000691
Iteration 237/1000 | Loss: 0.00000691
Iteration 238/1000 | Loss: 0.00000691
Iteration 239/1000 | Loss: 0.00000691
Iteration 240/1000 | Loss: 0.00000691
Iteration 241/1000 | Loss: 0.00000691
Iteration 242/1000 | Loss: 0.00000691
Iteration 243/1000 | Loss: 0.00000691
Iteration 244/1000 | Loss: 0.00000691
Iteration 245/1000 | Loss: 0.00000691
Iteration 246/1000 | Loss: 0.00000691
Iteration 247/1000 | Loss: 0.00000691
Iteration 248/1000 | Loss: 0.00000691
Iteration 249/1000 | Loss: 0.00000691
Iteration 250/1000 | Loss: 0.00000691
Iteration 251/1000 | Loss: 0.00000691
Iteration 252/1000 | Loss: 0.00000691
Iteration 253/1000 | Loss: 0.00000691
Iteration 254/1000 | Loss: 0.00000691
Iteration 255/1000 | Loss: 0.00000691
Iteration 256/1000 | Loss: 0.00000691
Iteration 257/1000 | Loss: 0.00000691
Iteration 258/1000 | Loss: 0.00000691
Iteration 259/1000 | Loss: 0.00000691
Iteration 260/1000 | Loss: 0.00000691
Iteration 261/1000 | Loss: 0.00000691
Iteration 262/1000 | Loss: 0.00000691
Iteration 263/1000 | Loss: 0.00000691
Iteration 264/1000 | Loss: 0.00000691
Iteration 265/1000 | Loss: 0.00000691
Iteration 266/1000 | Loss: 0.00000691
Iteration 267/1000 | Loss: 0.00000691
Iteration 268/1000 | Loss: 0.00000691
Iteration 269/1000 | Loss: 0.00000691
Iteration 270/1000 | Loss: 0.00000691
Iteration 271/1000 | Loss: 0.00000691
Iteration 272/1000 | Loss: 0.00000691
Iteration 273/1000 | Loss: 0.00000691
Iteration 274/1000 | Loss: 0.00000691
Iteration 275/1000 | Loss: 0.00000691
Iteration 276/1000 | Loss: 0.00000691
Iteration 277/1000 | Loss: 0.00000691
Iteration 278/1000 | Loss: 0.00000691
Iteration 279/1000 | Loss: 0.00000691
Iteration 280/1000 | Loss: 0.00000691
Iteration 281/1000 | Loss: 0.00000691
Iteration 282/1000 | Loss: 0.00000691
Iteration 283/1000 | Loss: 0.00000691
Iteration 284/1000 | Loss: 0.00000691
Iteration 285/1000 | Loss: 0.00000691
Iteration 286/1000 | Loss: 0.00000691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [6.905340342200361e-06, 6.905340342200361e-06, 6.905340342200361e-06, 6.905340342200361e-06, 6.905340342200361e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.905340342200361e-06

Optimization complete. Final v2v error: 2.2064712047576904 mm

Highest mean error: 2.5634119510650635 mm for frame 29

Lowest mean error: 1.9907166957855225 mm for frame 42

Saving results

Total time: 37.767868518829346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857539
Iteration 2/25 | Loss: 0.00108365
Iteration 3/25 | Loss: 0.00099258
Iteration 4/25 | Loss: 0.00097927
Iteration 5/25 | Loss: 0.00097487
Iteration 6/25 | Loss: 0.00097379
Iteration 7/25 | Loss: 0.00097379
Iteration 8/25 | Loss: 0.00097379
Iteration 9/25 | Loss: 0.00097379
Iteration 10/25 | Loss: 0.00097379
Iteration 11/25 | Loss: 0.00097379
Iteration 12/25 | Loss: 0.00097379
Iteration 13/25 | Loss: 0.00097379
Iteration 14/25 | Loss: 0.00097379
Iteration 15/25 | Loss: 0.00097379
Iteration 16/25 | Loss: 0.00097379
Iteration 17/25 | Loss: 0.00097379
Iteration 18/25 | Loss: 0.00097379
Iteration 19/25 | Loss: 0.00097379
Iteration 20/25 | Loss: 0.00097379
Iteration 21/25 | Loss: 0.00097379
Iteration 22/25 | Loss: 0.00097379
Iteration 23/25 | Loss: 0.00097379
Iteration 24/25 | Loss: 0.00097379
Iteration 25/25 | Loss: 0.00097379

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.21114349
Iteration 2/25 | Loss: 0.00110270
Iteration 3/25 | Loss: 0.00110270
Iteration 4/25 | Loss: 0.00110270
Iteration 5/25 | Loss: 0.00110270
Iteration 6/25 | Loss: 0.00110270
Iteration 7/25 | Loss: 0.00110270
Iteration 8/25 | Loss: 0.00110270
Iteration 9/25 | Loss: 0.00110270
Iteration 10/25 | Loss: 0.00110270
Iteration 11/25 | Loss: 0.00110270
Iteration 12/25 | Loss: 0.00110270
Iteration 13/25 | Loss: 0.00110270
Iteration 14/25 | Loss: 0.00110270
Iteration 15/25 | Loss: 0.00110270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011026993161067367, 0.0011026993161067367, 0.0011026993161067367, 0.0011026993161067367, 0.0011026993161067367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011026993161067367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110270
Iteration 2/1000 | Loss: 0.00002558
Iteration 3/1000 | Loss: 0.00001594
Iteration 4/1000 | Loss: 0.00001328
Iteration 5/1000 | Loss: 0.00001262
Iteration 6/1000 | Loss: 0.00001208
Iteration 7/1000 | Loss: 0.00001166
Iteration 8/1000 | Loss: 0.00001139
Iteration 9/1000 | Loss: 0.00001125
Iteration 10/1000 | Loss: 0.00001122
Iteration 11/1000 | Loss: 0.00001122
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001121
Iteration 14/1000 | Loss: 0.00001121
Iteration 15/1000 | Loss: 0.00001120
Iteration 16/1000 | Loss: 0.00001120
Iteration 17/1000 | Loss: 0.00001120
Iteration 18/1000 | Loss: 0.00001120
Iteration 19/1000 | Loss: 0.00001120
Iteration 20/1000 | Loss: 0.00001120
Iteration 21/1000 | Loss: 0.00001120
Iteration 22/1000 | Loss: 0.00001120
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001119
Iteration 25/1000 | Loss: 0.00001119
Iteration 26/1000 | Loss: 0.00001119
Iteration 27/1000 | Loss: 0.00001119
Iteration 28/1000 | Loss: 0.00001119
Iteration 29/1000 | Loss: 0.00001119
Iteration 30/1000 | Loss: 0.00001119
Iteration 31/1000 | Loss: 0.00001119
Iteration 32/1000 | Loss: 0.00001118
Iteration 33/1000 | Loss: 0.00001118
Iteration 34/1000 | Loss: 0.00001117
Iteration 35/1000 | Loss: 0.00001117
Iteration 36/1000 | Loss: 0.00001115
Iteration 37/1000 | Loss: 0.00001115
Iteration 38/1000 | Loss: 0.00001114
Iteration 39/1000 | Loss: 0.00001114
Iteration 40/1000 | Loss: 0.00001114
Iteration 41/1000 | Loss: 0.00001113
Iteration 42/1000 | Loss: 0.00001113
Iteration 43/1000 | Loss: 0.00001112
Iteration 44/1000 | Loss: 0.00001112
Iteration 45/1000 | Loss: 0.00001110
Iteration 46/1000 | Loss: 0.00001109
Iteration 47/1000 | Loss: 0.00001109
Iteration 48/1000 | Loss: 0.00001109
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001109
Iteration 52/1000 | Loss: 0.00001108
Iteration 53/1000 | Loss: 0.00001107
Iteration 54/1000 | Loss: 0.00001106
Iteration 55/1000 | Loss: 0.00001106
Iteration 56/1000 | Loss: 0.00001106
Iteration 57/1000 | Loss: 0.00001106
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001105
Iteration 64/1000 | Loss: 0.00001105
Iteration 65/1000 | Loss: 0.00001105
Iteration 66/1000 | Loss: 0.00001105
Iteration 67/1000 | Loss: 0.00001105
Iteration 68/1000 | Loss: 0.00001105
Iteration 69/1000 | Loss: 0.00001104
Iteration 70/1000 | Loss: 0.00001104
Iteration 71/1000 | Loss: 0.00001103
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001102
Iteration 74/1000 | Loss: 0.00001102
Iteration 75/1000 | Loss: 0.00001102
Iteration 76/1000 | Loss: 0.00001102
Iteration 77/1000 | Loss: 0.00001102
Iteration 78/1000 | Loss: 0.00001102
Iteration 79/1000 | Loss: 0.00001101
Iteration 80/1000 | Loss: 0.00001101
Iteration 81/1000 | Loss: 0.00001101
Iteration 82/1000 | Loss: 0.00001101
Iteration 83/1000 | Loss: 0.00001101
Iteration 84/1000 | Loss: 0.00001101
Iteration 85/1000 | Loss: 0.00001101
Iteration 86/1000 | Loss: 0.00001100
Iteration 87/1000 | Loss: 0.00001100
Iteration 88/1000 | Loss: 0.00001100
Iteration 89/1000 | Loss: 0.00001100
Iteration 90/1000 | Loss: 0.00001100
Iteration 91/1000 | Loss: 0.00001100
Iteration 92/1000 | Loss: 0.00001100
Iteration 93/1000 | Loss: 0.00001100
Iteration 94/1000 | Loss: 0.00001099
Iteration 95/1000 | Loss: 0.00001099
Iteration 96/1000 | Loss: 0.00001099
Iteration 97/1000 | Loss: 0.00001099
Iteration 98/1000 | Loss: 0.00001098
Iteration 99/1000 | Loss: 0.00001098
Iteration 100/1000 | Loss: 0.00001098
Iteration 101/1000 | Loss: 0.00001097
Iteration 102/1000 | Loss: 0.00001097
Iteration 103/1000 | Loss: 0.00001097
Iteration 104/1000 | Loss: 0.00001097
Iteration 105/1000 | Loss: 0.00001097
Iteration 106/1000 | Loss: 0.00001097
Iteration 107/1000 | Loss: 0.00001096
Iteration 108/1000 | Loss: 0.00001096
Iteration 109/1000 | Loss: 0.00001096
Iteration 110/1000 | Loss: 0.00001096
Iteration 111/1000 | Loss: 0.00001096
Iteration 112/1000 | Loss: 0.00001095
Iteration 113/1000 | Loss: 0.00001095
Iteration 114/1000 | Loss: 0.00001095
Iteration 115/1000 | Loss: 0.00001095
Iteration 116/1000 | Loss: 0.00001095
Iteration 117/1000 | Loss: 0.00001094
Iteration 118/1000 | Loss: 0.00001094
Iteration 119/1000 | Loss: 0.00001094
Iteration 120/1000 | Loss: 0.00001094
Iteration 121/1000 | Loss: 0.00001094
Iteration 122/1000 | Loss: 0.00001094
Iteration 123/1000 | Loss: 0.00001094
Iteration 124/1000 | Loss: 0.00001094
Iteration 125/1000 | Loss: 0.00001093
Iteration 126/1000 | Loss: 0.00001093
Iteration 127/1000 | Loss: 0.00001093
Iteration 128/1000 | Loss: 0.00001093
Iteration 129/1000 | Loss: 0.00001093
Iteration 130/1000 | Loss: 0.00001093
Iteration 131/1000 | Loss: 0.00001093
Iteration 132/1000 | Loss: 0.00001093
Iteration 133/1000 | Loss: 0.00001093
Iteration 134/1000 | Loss: 0.00001092
Iteration 135/1000 | Loss: 0.00001092
Iteration 136/1000 | Loss: 0.00001092
Iteration 137/1000 | Loss: 0.00001092
Iteration 138/1000 | Loss: 0.00001092
Iteration 139/1000 | Loss: 0.00001092
Iteration 140/1000 | Loss: 0.00001092
Iteration 141/1000 | Loss: 0.00001092
Iteration 142/1000 | Loss: 0.00001092
Iteration 143/1000 | Loss: 0.00001092
Iteration 144/1000 | Loss: 0.00001092
Iteration 145/1000 | Loss: 0.00001092
Iteration 146/1000 | Loss: 0.00001092
Iteration 147/1000 | Loss: 0.00001092
Iteration 148/1000 | Loss: 0.00001092
Iteration 149/1000 | Loss: 0.00001091
Iteration 150/1000 | Loss: 0.00001091
Iteration 151/1000 | Loss: 0.00001091
Iteration 152/1000 | Loss: 0.00001091
Iteration 153/1000 | Loss: 0.00001091
Iteration 154/1000 | Loss: 0.00001091
Iteration 155/1000 | Loss: 0.00001091
Iteration 156/1000 | Loss: 0.00001091
Iteration 157/1000 | Loss: 0.00001091
Iteration 158/1000 | Loss: 0.00001091
Iteration 159/1000 | Loss: 0.00001091
Iteration 160/1000 | Loss: 0.00001091
Iteration 161/1000 | Loss: 0.00001091
Iteration 162/1000 | Loss: 0.00001091
Iteration 163/1000 | Loss: 0.00001091
Iteration 164/1000 | Loss: 0.00001091
Iteration 165/1000 | Loss: 0.00001091
Iteration 166/1000 | Loss: 0.00001091
Iteration 167/1000 | Loss: 0.00001091
Iteration 168/1000 | Loss: 0.00001091
Iteration 169/1000 | Loss: 0.00001091
Iteration 170/1000 | Loss: 0.00001091
Iteration 171/1000 | Loss: 0.00001091
Iteration 172/1000 | Loss: 0.00001091
Iteration 173/1000 | Loss: 0.00001091
Iteration 174/1000 | Loss: 0.00001091
Iteration 175/1000 | Loss: 0.00001091
Iteration 176/1000 | Loss: 0.00001091
Iteration 177/1000 | Loss: 0.00001091
Iteration 178/1000 | Loss: 0.00001091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.0907464456977323e-05, 1.0907464456977323e-05, 1.0907464456977323e-05, 1.0907464456977323e-05, 1.0907464456977323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0907464456977323e-05

Optimization complete. Final v2v error: 2.781309127807617 mm

Highest mean error: 3.1555068492889404 mm for frame 77

Lowest mean error: 2.3975791931152344 mm for frame 136

Saving results

Total time: 31.162729024887085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831250
Iteration 2/25 | Loss: 0.00118641
Iteration 3/25 | Loss: 0.00106488
Iteration 4/25 | Loss: 0.00104839
Iteration 5/25 | Loss: 0.00104445
Iteration 6/25 | Loss: 0.00104317
Iteration 7/25 | Loss: 0.00104310
Iteration 8/25 | Loss: 0.00104310
Iteration 9/25 | Loss: 0.00104310
Iteration 10/25 | Loss: 0.00104310
Iteration 11/25 | Loss: 0.00104310
Iteration 12/25 | Loss: 0.00104310
Iteration 13/25 | Loss: 0.00104310
Iteration 14/25 | Loss: 0.00104310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001043104100972414, 0.001043104100972414, 0.001043104100972414, 0.001043104100972414, 0.001043104100972414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001043104100972414

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43722999
Iteration 2/25 | Loss: 0.00102729
Iteration 3/25 | Loss: 0.00102728
Iteration 4/25 | Loss: 0.00102728
Iteration 5/25 | Loss: 0.00102728
Iteration 6/25 | Loss: 0.00102728
Iteration 7/25 | Loss: 0.00102728
Iteration 8/25 | Loss: 0.00102728
Iteration 9/25 | Loss: 0.00102728
Iteration 10/25 | Loss: 0.00102728
Iteration 11/25 | Loss: 0.00102728
Iteration 12/25 | Loss: 0.00102728
Iteration 13/25 | Loss: 0.00102728
Iteration 14/25 | Loss: 0.00102728
Iteration 15/25 | Loss: 0.00102728
Iteration 16/25 | Loss: 0.00102728
Iteration 17/25 | Loss: 0.00102728
Iteration 18/25 | Loss: 0.00102728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001027278252877295, 0.001027278252877295, 0.001027278252877295, 0.001027278252877295, 0.001027278252877295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001027278252877295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102728
Iteration 2/1000 | Loss: 0.00004768
Iteration 3/1000 | Loss: 0.00002644
Iteration 4/1000 | Loss: 0.00001888
Iteration 5/1000 | Loss: 0.00001705
Iteration 6/1000 | Loss: 0.00001611
Iteration 7/1000 | Loss: 0.00001567
Iteration 8/1000 | Loss: 0.00001534
Iteration 9/1000 | Loss: 0.00001503
Iteration 10/1000 | Loss: 0.00001500
Iteration 11/1000 | Loss: 0.00001485
Iteration 12/1000 | Loss: 0.00001476
Iteration 13/1000 | Loss: 0.00001473
Iteration 14/1000 | Loss: 0.00001471
Iteration 15/1000 | Loss: 0.00001467
Iteration 16/1000 | Loss: 0.00001467
Iteration 17/1000 | Loss: 0.00001466
Iteration 18/1000 | Loss: 0.00001465
Iteration 19/1000 | Loss: 0.00001465
Iteration 20/1000 | Loss: 0.00001460
Iteration 21/1000 | Loss: 0.00001459
Iteration 22/1000 | Loss: 0.00001458
Iteration 23/1000 | Loss: 0.00001456
Iteration 24/1000 | Loss: 0.00001455
Iteration 25/1000 | Loss: 0.00001454
Iteration 26/1000 | Loss: 0.00001453
Iteration 27/1000 | Loss: 0.00001452
Iteration 28/1000 | Loss: 0.00001452
Iteration 29/1000 | Loss: 0.00001451
Iteration 30/1000 | Loss: 0.00001451
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001443
Iteration 34/1000 | Loss: 0.00001438
Iteration 35/1000 | Loss: 0.00001438
Iteration 36/1000 | Loss: 0.00001437
Iteration 37/1000 | Loss: 0.00001436
Iteration 38/1000 | Loss: 0.00001436
Iteration 39/1000 | Loss: 0.00001433
Iteration 40/1000 | Loss: 0.00001433
Iteration 41/1000 | Loss: 0.00001433
Iteration 42/1000 | Loss: 0.00001433
Iteration 43/1000 | Loss: 0.00001433
Iteration 44/1000 | Loss: 0.00001433
Iteration 45/1000 | Loss: 0.00001432
Iteration 46/1000 | Loss: 0.00001432
Iteration 47/1000 | Loss: 0.00001432
Iteration 48/1000 | Loss: 0.00001432
Iteration 49/1000 | Loss: 0.00001432
Iteration 50/1000 | Loss: 0.00001432
Iteration 51/1000 | Loss: 0.00001432
Iteration 52/1000 | Loss: 0.00001431
Iteration 53/1000 | Loss: 0.00001431
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001430
Iteration 56/1000 | Loss: 0.00001430
Iteration 57/1000 | Loss: 0.00001430
Iteration 58/1000 | Loss: 0.00001429
Iteration 59/1000 | Loss: 0.00001429
Iteration 60/1000 | Loss: 0.00001429
Iteration 61/1000 | Loss: 0.00001429
Iteration 62/1000 | Loss: 0.00001429
Iteration 63/1000 | Loss: 0.00001429
Iteration 64/1000 | Loss: 0.00001429
Iteration 65/1000 | Loss: 0.00001429
Iteration 66/1000 | Loss: 0.00001428
Iteration 67/1000 | Loss: 0.00001428
Iteration 68/1000 | Loss: 0.00001428
Iteration 69/1000 | Loss: 0.00001428
Iteration 70/1000 | Loss: 0.00001427
Iteration 71/1000 | Loss: 0.00001427
Iteration 72/1000 | Loss: 0.00001427
Iteration 73/1000 | Loss: 0.00001427
Iteration 74/1000 | Loss: 0.00001427
Iteration 75/1000 | Loss: 0.00001427
Iteration 76/1000 | Loss: 0.00001426
Iteration 77/1000 | Loss: 0.00001426
Iteration 78/1000 | Loss: 0.00001426
Iteration 79/1000 | Loss: 0.00001426
Iteration 80/1000 | Loss: 0.00001426
Iteration 81/1000 | Loss: 0.00001426
Iteration 82/1000 | Loss: 0.00001426
Iteration 83/1000 | Loss: 0.00001426
Iteration 84/1000 | Loss: 0.00001426
Iteration 85/1000 | Loss: 0.00001426
Iteration 86/1000 | Loss: 0.00001426
Iteration 87/1000 | Loss: 0.00001426
Iteration 88/1000 | Loss: 0.00001426
Iteration 89/1000 | Loss: 0.00001425
Iteration 90/1000 | Loss: 0.00001425
Iteration 91/1000 | Loss: 0.00001425
Iteration 92/1000 | Loss: 0.00001425
Iteration 93/1000 | Loss: 0.00001425
Iteration 94/1000 | Loss: 0.00001425
Iteration 95/1000 | Loss: 0.00001425
Iteration 96/1000 | Loss: 0.00001424
Iteration 97/1000 | Loss: 0.00001424
Iteration 98/1000 | Loss: 0.00001424
Iteration 99/1000 | Loss: 0.00001424
Iteration 100/1000 | Loss: 0.00001424
Iteration 101/1000 | Loss: 0.00001424
Iteration 102/1000 | Loss: 0.00001424
Iteration 103/1000 | Loss: 0.00001424
Iteration 104/1000 | Loss: 0.00001424
Iteration 105/1000 | Loss: 0.00001424
Iteration 106/1000 | Loss: 0.00001424
Iteration 107/1000 | Loss: 0.00001424
Iteration 108/1000 | Loss: 0.00001423
Iteration 109/1000 | Loss: 0.00001423
Iteration 110/1000 | Loss: 0.00001423
Iteration 111/1000 | Loss: 0.00001423
Iteration 112/1000 | Loss: 0.00001423
Iteration 113/1000 | Loss: 0.00001423
Iteration 114/1000 | Loss: 0.00001423
Iteration 115/1000 | Loss: 0.00001423
Iteration 116/1000 | Loss: 0.00001423
Iteration 117/1000 | Loss: 0.00001423
Iteration 118/1000 | Loss: 0.00001423
Iteration 119/1000 | Loss: 0.00001423
Iteration 120/1000 | Loss: 0.00001423
Iteration 121/1000 | Loss: 0.00001423
Iteration 122/1000 | Loss: 0.00001423
Iteration 123/1000 | Loss: 0.00001423
Iteration 124/1000 | Loss: 0.00001422
Iteration 125/1000 | Loss: 0.00001422
Iteration 126/1000 | Loss: 0.00001422
Iteration 127/1000 | Loss: 0.00001422
Iteration 128/1000 | Loss: 0.00001422
Iteration 129/1000 | Loss: 0.00001422
Iteration 130/1000 | Loss: 0.00001422
Iteration 131/1000 | Loss: 0.00001422
Iteration 132/1000 | Loss: 0.00001422
Iteration 133/1000 | Loss: 0.00001422
Iteration 134/1000 | Loss: 0.00001422
Iteration 135/1000 | Loss: 0.00001422
Iteration 136/1000 | Loss: 0.00001422
Iteration 137/1000 | Loss: 0.00001422
Iteration 138/1000 | Loss: 0.00001422
Iteration 139/1000 | Loss: 0.00001422
Iteration 140/1000 | Loss: 0.00001422
Iteration 141/1000 | Loss: 0.00001422
Iteration 142/1000 | Loss: 0.00001422
Iteration 143/1000 | Loss: 0.00001422
Iteration 144/1000 | Loss: 0.00001422
Iteration 145/1000 | Loss: 0.00001422
Iteration 146/1000 | Loss: 0.00001422
Iteration 147/1000 | Loss: 0.00001422
Iteration 148/1000 | Loss: 0.00001422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.4222956451703794e-05, 1.4222956451703794e-05, 1.4222956451703794e-05, 1.4222956451703794e-05, 1.4222956451703794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4222956451703794e-05

Optimization complete. Final v2v error: 3.1288199424743652 mm

Highest mean error: 4.036074161529541 mm for frame 122

Lowest mean error: 2.252919912338257 mm for frame 1

Saving results

Total time: 35.365864753723145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01139477
Iteration 2/25 | Loss: 0.00371132
Iteration 3/25 | Loss: 0.00317854
Iteration 4/25 | Loss: 0.00189111
Iteration 5/25 | Loss: 0.00199375
Iteration 6/25 | Loss: 0.00169627
Iteration 7/25 | Loss: 0.00160309
Iteration 8/25 | Loss: 0.00150325
Iteration 9/25 | Loss: 0.00147613
Iteration 10/25 | Loss: 0.00147022
Iteration 11/25 | Loss: 0.00146966
Iteration 12/25 | Loss: 0.00146940
Iteration 13/25 | Loss: 0.00146903
Iteration 14/25 | Loss: 0.00146881
Iteration 15/25 | Loss: 0.00146869
Iteration 16/25 | Loss: 0.00146862
Iteration 17/25 | Loss: 0.00146862
Iteration 18/25 | Loss: 0.00146862
Iteration 19/25 | Loss: 0.00146860
Iteration 20/25 | Loss: 0.00146859
Iteration 21/25 | Loss: 0.00146859
Iteration 22/25 | Loss: 0.00146859
Iteration 23/25 | Loss: 0.00146858
Iteration 24/25 | Loss: 0.00146858
Iteration 25/25 | Loss: 0.00146858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.41426027
Iteration 2/25 | Loss: 0.00320202
Iteration 3/25 | Loss: 0.00082995
Iteration 4/25 | Loss: 0.00082994
Iteration 5/25 | Loss: 0.00082994
Iteration 6/25 | Loss: 0.00082994
Iteration 7/25 | Loss: 0.00082994
Iteration 8/25 | Loss: 0.00082994
Iteration 9/25 | Loss: 0.00082994
Iteration 10/25 | Loss: 0.00082994
Iteration 11/25 | Loss: 0.00082994
Iteration 12/25 | Loss: 0.00082994
Iteration 13/25 | Loss: 0.00082994
Iteration 14/25 | Loss: 0.00082994
Iteration 15/25 | Loss: 0.00082994
Iteration 16/25 | Loss: 0.00082994
Iteration 17/25 | Loss: 0.00082994
Iteration 18/25 | Loss: 0.00082994
Iteration 19/25 | Loss: 0.00082994
Iteration 20/25 | Loss: 0.00082994
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008299413020722568, 0.0008299413020722568, 0.0008299413020722568, 0.0008299413020722568, 0.0008299413020722568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008299413020722568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082994
Iteration 2/1000 | Loss: 0.00119454
Iteration 3/1000 | Loss: 0.00158096
Iteration 4/1000 | Loss: 0.00132841
Iteration 5/1000 | Loss: 0.00103892
Iteration 6/1000 | Loss: 0.00102508
Iteration 7/1000 | Loss: 0.00028598
Iteration 8/1000 | Loss: 0.00020764
Iteration 9/1000 | Loss: 0.00012649
Iteration 10/1000 | Loss: 0.00011113
Iteration 11/1000 | Loss: 0.00020109
Iteration 12/1000 | Loss: 0.00016154
Iteration 13/1000 | Loss: 0.00017106
Iteration 14/1000 | Loss: 0.00034171
Iteration 15/1000 | Loss: 0.00013361
Iteration 16/1000 | Loss: 0.00011747
Iteration 17/1000 | Loss: 0.00010946
Iteration 18/1000 | Loss: 0.00014745
Iteration 19/1000 | Loss: 0.00008776
Iteration 20/1000 | Loss: 0.00008073
Iteration 21/1000 | Loss: 0.00007837
Iteration 22/1000 | Loss: 0.00007511
Iteration 23/1000 | Loss: 0.00007276
Iteration 24/1000 | Loss: 0.00007103
Iteration 25/1000 | Loss: 0.00006968
Iteration 26/1000 | Loss: 0.00006852
Iteration 27/1000 | Loss: 0.00006728
Iteration 28/1000 | Loss: 0.00006624
Iteration 29/1000 | Loss: 0.00006547
Iteration 30/1000 | Loss: 0.00006501
Iteration 31/1000 | Loss: 0.00006469
Iteration 32/1000 | Loss: 0.00006451
Iteration 33/1000 | Loss: 0.00006428
Iteration 34/1000 | Loss: 0.00006410
Iteration 35/1000 | Loss: 0.00006394
Iteration 36/1000 | Loss: 0.00006378
Iteration 37/1000 | Loss: 0.00006364
Iteration 38/1000 | Loss: 0.00006343
Iteration 39/1000 | Loss: 0.00006328
Iteration 40/1000 | Loss: 0.00006328
Iteration 41/1000 | Loss: 0.00006323
Iteration 42/1000 | Loss: 0.00006323
Iteration 43/1000 | Loss: 0.00006321
Iteration 44/1000 | Loss: 0.00006318
Iteration 45/1000 | Loss: 0.00006316
Iteration 46/1000 | Loss: 0.00006315
Iteration 47/1000 | Loss: 0.00006312
Iteration 48/1000 | Loss: 0.00006311
Iteration 49/1000 | Loss: 0.00006308
Iteration 50/1000 | Loss: 0.00006304
Iteration 51/1000 | Loss: 0.00006304
Iteration 52/1000 | Loss: 0.00006304
Iteration 53/1000 | Loss: 0.00006303
Iteration 54/1000 | Loss: 0.00006303
Iteration 55/1000 | Loss: 0.00006303
Iteration 56/1000 | Loss: 0.00006303
Iteration 57/1000 | Loss: 0.00006303
Iteration 58/1000 | Loss: 0.00006303
Iteration 59/1000 | Loss: 0.00006303
Iteration 60/1000 | Loss: 0.00006303
Iteration 61/1000 | Loss: 0.00006302
Iteration 62/1000 | Loss: 0.00006301
Iteration 63/1000 | Loss: 0.00006301
Iteration 64/1000 | Loss: 0.00006301
Iteration 65/1000 | Loss: 0.00006301
Iteration 66/1000 | Loss: 0.00006301
Iteration 67/1000 | Loss: 0.00006300
Iteration 68/1000 | Loss: 0.00006300
Iteration 69/1000 | Loss: 0.00006300
Iteration 70/1000 | Loss: 0.00006300
Iteration 71/1000 | Loss: 0.00006299
Iteration 72/1000 | Loss: 0.00006299
Iteration 73/1000 | Loss: 0.00006299
Iteration 74/1000 | Loss: 0.00006299
Iteration 75/1000 | Loss: 0.00006299
Iteration 76/1000 | Loss: 0.00006299
Iteration 77/1000 | Loss: 0.00006299
Iteration 78/1000 | Loss: 0.00006299
Iteration 79/1000 | Loss: 0.00006299
Iteration 80/1000 | Loss: 0.00006298
Iteration 81/1000 | Loss: 0.00006298
Iteration 82/1000 | Loss: 0.00006298
Iteration 83/1000 | Loss: 0.00006298
Iteration 84/1000 | Loss: 0.00006298
Iteration 85/1000 | Loss: 0.00006298
Iteration 86/1000 | Loss: 0.00006298
Iteration 87/1000 | Loss: 0.00006298
Iteration 88/1000 | Loss: 0.00006298
Iteration 89/1000 | Loss: 0.00006298
Iteration 90/1000 | Loss: 0.00006298
Iteration 91/1000 | Loss: 0.00006298
Iteration 92/1000 | Loss: 0.00006298
Iteration 93/1000 | Loss: 0.00006297
Iteration 94/1000 | Loss: 0.00006297
Iteration 95/1000 | Loss: 0.00006297
Iteration 96/1000 | Loss: 0.00006297
Iteration 97/1000 | Loss: 0.00006297
Iteration 98/1000 | Loss: 0.00006297
Iteration 99/1000 | Loss: 0.00006297
Iteration 100/1000 | Loss: 0.00006297
Iteration 101/1000 | Loss: 0.00006297
Iteration 102/1000 | Loss: 0.00006296
Iteration 103/1000 | Loss: 0.00006295
Iteration 104/1000 | Loss: 0.00006295
Iteration 105/1000 | Loss: 0.00006295
Iteration 106/1000 | Loss: 0.00006295
Iteration 107/1000 | Loss: 0.00006295
Iteration 108/1000 | Loss: 0.00006294
Iteration 109/1000 | Loss: 0.00006294
Iteration 110/1000 | Loss: 0.00006294
Iteration 111/1000 | Loss: 0.00006294
Iteration 112/1000 | Loss: 0.00006294
Iteration 113/1000 | Loss: 0.00006293
Iteration 114/1000 | Loss: 0.00006293
Iteration 115/1000 | Loss: 0.00006293
Iteration 116/1000 | Loss: 0.00006293
Iteration 117/1000 | Loss: 0.00006293
Iteration 118/1000 | Loss: 0.00006293
Iteration 119/1000 | Loss: 0.00006293
Iteration 120/1000 | Loss: 0.00006293
Iteration 121/1000 | Loss: 0.00006293
Iteration 122/1000 | Loss: 0.00006293
Iteration 123/1000 | Loss: 0.00006293
Iteration 124/1000 | Loss: 0.00006293
Iteration 125/1000 | Loss: 0.00006293
Iteration 126/1000 | Loss: 0.00006293
Iteration 127/1000 | Loss: 0.00006293
Iteration 128/1000 | Loss: 0.00006292
Iteration 129/1000 | Loss: 0.00006292
Iteration 130/1000 | Loss: 0.00006292
Iteration 131/1000 | Loss: 0.00006292
Iteration 132/1000 | Loss: 0.00006292
Iteration 133/1000 | Loss: 0.00006292
Iteration 134/1000 | Loss: 0.00006292
Iteration 135/1000 | Loss: 0.00006292
Iteration 136/1000 | Loss: 0.00006292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [6.292413308983669e-05, 6.292413308983669e-05, 6.292413308983669e-05, 6.292413308983669e-05, 6.292413308983669e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.292413308983669e-05

Optimization complete. Final v2v error: 6.039093017578125 mm

Highest mean error: 7.46660852432251 mm for frame 63

Lowest mean error: 4.693183422088623 mm for frame 121

Saving results

Total time: 87.7390067577362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988103
Iteration 2/25 | Loss: 0.00503242
Iteration 3/25 | Loss: 0.00381683
Iteration 4/25 | Loss: 0.00308512
Iteration 5/25 | Loss: 0.00260804
Iteration 6/25 | Loss: 0.00228817
Iteration 7/25 | Loss: 0.00201884
Iteration 8/25 | Loss: 0.00189555
Iteration 9/25 | Loss: 0.00181959
Iteration 10/25 | Loss: 0.00181424
Iteration 11/25 | Loss: 0.00183387
Iteration 12/25 | Loss: 0.00177857
Iteration 13/25 | Loss: 0.00166542
Iteration 14/25 | Loss: 0.00162357
Iteration 15/25 | Loss: 0.00157395
Iteration 16/25 | Loss: 0.00157660
Iteration 17/25 | Loss: 0.00156400
Iteration 18/25 | Loss: 0.00154342
Iteration 19/25 | Loss: 0.00151284
Iteration 20/25 | Loss: 0.00153307
Iteration 21/25 | Loss: 0.00150656
Iteration 22/25 | Loss: 0.00150898
Iteration 23/25 | Loss: 0.00149144
Iteration 24/25 | Loss: 0.00148533
Iteration 25/25 | Loss: 0.00148701

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31957662
Iteration 2/25 | Loss: 0.00301574
Iteration 3/25 | Loss: 0.00247218
Iteration 4/25 | Loss: 0.00247218
Iteration 5/25 | Loss: 0.00247218
Iteration 6/25 | Loss: 0.00247217
Iteration 7/25 | Loss: 0.00247217
Iteration 8/25 | Loss: 0.00247217
Iteration 9/25 | Loss: 0.00247217
Iteration 10/25 | Loss: 0.00247217
Iteration 11/25 | Loss: 0.00247217
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002472174819558859, 0.002472174819558859, 0.002472174819558859, 0.002472174819558859, 0.002472174819558859]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002472174819558859

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00247217
Iteration 2/1000 | Loss: 0.00157653
Iteration 3/1000 | Loss: 0.00088679
Iteration 4/1000 | Loss: 0.00304848
Iteration 5/1000 | Loss: 0.00199585
Iteration 6/1000 | Loss: 0.00119612
Iteration 7/1000 | Loss: 0.00119791
Iteration 8/1000 | Loss: 0.00106968
Iteration 9/1000 | Loss: 0.00241091
Iteration 10/1000 | Loss: 0.00143500
Iteration 11/1000 | Loss: 0.00112523
Iteration 12/1000 | Loss: 0.00122833
Iteration 13/1000 | Loss: 0.00144168
Iteration 14/1000 | Loss: 0.00445624
Iteration 15/1000 | Loss: 0.00770023
Iteration 16/1000 | Loss: 0.00609804
Iteration 17/1000 | Loss: 0.00504137
Iteration 18/1000 | Loss: 0.00237395
Iteration 19/1000 | Loss: 0.00192129
Iteration 20/1000 | Loss: 0.00224274
Iteration 21/1000 | Loss: 0.00164404
Iteration 22/1000 | Loss: 0.00157874
Iteration 23/1000 | Loss: 0.00167931
Iteration 24/1000 | Loss: 0.00086824
Iteration 25/1000 | Loss: 0.00087766
Iteration 26/1000 | Loss: 0.00195912
Iteration 27/1000 | Loss: 0.00140326
Iteration 28/1000 | Loss: 0.00166418
Iteration 29/1000 | Loss: 0.00200022
Iteration 30/1000 | Loss: 0.00301946
Iteration 31/1000 | Loss: 0.00286931
Iteration 32/1000 | Loss: 0.00334795
Iteration 33/1000 | Loss: 0.00239678
Iteration 34/1000 | Loss: 0.00144614
Iteration 35/1000 | Loss: 0.00145243
Iteration 36/1000 | Loss: 0.00132418
Iteration 37/1000 | Loss: 0.00207574
Iteration 38/1000 | Loss: 0.00127007
Iteration 39/1000 | Loss: 0.00334657
Iteration 40/1000 | Loss: 0.00387136
Iteration 41/1000 | Loss: 0.00130821
Iteration 42/1000 | Loss: 0.00117126
Iteration 43/1000 | Loss: 0.00227656
Iteration 44/1000 | Loss: 0.00084751
Iteration 45/1000 | Loss: 0.00119361
Iteration 46/1000 | Loss: 0.00204361
Iteration 47/1000 | Loss: 0.00190186
Iteration 48/1000 | Loss: 0.00088738
Iteration 49/1000 | Loss: 0.00142616
Iteration 50/1000 | Loss: 0.00076281
Iteration 51/1000 | Loss: 0.00085903
Iteration 52/1000 | Loss: 0.00070695
Iteration 53/1000 | Loss: 0.00068419
Iteration 54/1000 | Loss: 0.00074184
Iteration 55/1000 | Loss: 0.00072181
Iteration 56/1000 | Loss: 0.00087873
Iteration 57/1000 | Loss: 0.00078445
Iteration 58/1000 | Loss: 0.00084625
Iteration 59/1000 | Loss: 0.00069644
Iteration 60/1000 | Loss: 0.00073429
Iteration 61/1000 | Loss: 0.00111559
Iteration 62/1000 | Loss: 0.00087957
Iteration 63/1000 | Loss: 0.00075355
Iteration 64/1000 | Loss: 0.00225270
Iteration 65/1000 | Loss: 0.00239189
Iteration 66/1000 | Loss: 0.00136600
Iteration 67/1000 | Loss: 0.00084202
Iteration 68/1000 | Loss: 0.00080286
Iteration 69/1000 | Loss: 0.00095794
Iteration 70/1000 | Loss: 0.00185982
Iteration 71/1000 | Loss: 0.00086117
Iteration 72/1000 | Loss: 0.00108452
Iteration 73/1000 | Loss: 0.00084679
Iteration 74/1000 | Loss: 0.00088519
Iteration 75/1000 | Loss: 0.00083960
Iteration 76/1000 | Loss: 0.00085545
Iteration 77/1000 | Loss: 0.00080477
Iteration 78/1000 | Loss: 0.00121808
Iteration 79/1000 | Loss: 0.00198075
Iteration 80/1000 | Loss: 0.00277834
Iteration 81/1000 | Loss: 0.00341667
Iteration 82/1000 | Loss: 0.00274950
Iteration 83/1000 | Loss: 0.00386715
Iteration 84/1000 | Loss: 0.00245876
Iteration 85/1000 | Loss: 0.00362671
Iteration 86/1000 | Loss: 0.00079903
Iteration 87/1000 | Loss: 0.00156757
Iteration 88/1000 | Loss: 0.00083357
Iteration 89/1000 | Loss: 0.00069777
Iteration 90/1000 | Loss: 0.00101866
Iteration 91/1000 | Loss: 0.00072762
Iteration 92/1000 | Loss: 0.00081793
Iteration 93/1000 | Loss: 0.00143732
Iteration 94/1000 | Loss: 0.00079195
Iteration 95/1000 | Loss: 0.00419141
Iteration 96/1000 | Loss: 0.00460083
Iteration 97/1000 | Loss: 0.00116873
Iteration 98/1000 | Loss: 0.00164571
Iteration 99/1000 | Loss: 0.00077531
Iteration 100/1000 | Loss: 0.00016855
Iteration 101/1000 | Loss: 0.00042122
Iteration 102/1000 | Loss: 0.00074376
Iteration 103/1000 | Loss: 0.00025521
Iteration 104/1000 | Loss: 0.00009137
Iteration 105/1000 | Loss: 0.00097455
Iteration 106/1000 | Loss: 0.00006575
Iteration 107/1000 | Loss: 0.00008345
Iteration 108/1000 | Loss: 0.00008188
Iteration 109/1000 | Loss: 0.00008002
Iteration 110/1000 | Loss: 0.00069045
Iteration 111/1000 | Loss: 0.00165306
Iteration 112/1000 | Loss: 0.00007924
Iteration 113/1000 | Loss: 0.00007027
Iteration 114/1000 | Loss: 0.00007190
Iteration 115/1000 | Loss: 0.00007484
Iteration 116/1000 | Loss: 0.00014267
Iteration 117/1000 | Loss: 0.00019394
Iteration 118/1000 | Loss: 0.00013023
Iteration 119/1000 | Loss: 0.00008957
Iteration 120/1000 | Loss: 0.00010326
Iteration 121/1000 | Loss: 0.00021475
Iteration 122/1000 | Loss: 0.00009056
Iteration 123/1000 | Loss: 0.00013277
Iteration 124/1000 | Loss: 0.00007253
Iteration 125/1000 | Loss: 0.00006954
Iteration 126/1000 | Loss: 0.00007539
Iteration 127/1000 | Loss: 0.00005040
Iteration 128/1000 | Loss: 0.00004400
Iteration 129/1000 | Loss: 0.00004148
Iteration 130/1000 | Loss: 0.00008080
Iteration 131/1000 | Loss: 0.00004604
Iteration 132/1000 | Loss: 0.00002391
Iteration 133/1000 | Loss: 0.00002246
Iteration 134/1000 | Loss: 0.00002170
Iteration 135/1000 | Loss: 0.00017363
Iteration 136/1000 | Loss: 0.00002074
Iteration 137/1000 | Loss: 0.00001965
Iteration 138/1000 | Loss: 0.00001899
Iteration 139/1000 | Loss: 0.00001844
Iteration 140/1000 | Loss: 0.00001802
Iteration 141/1000 | Loss: 0.00001782
Iteration 142/1000 | Loss: 0.00001756
Iteration 143/1000 | Loss: 0.00001751
Iteration 144/1000 | Loss: 0.00001741
Iteration 145/1000 | Loss: 0.00001741
Iteration 146/1000 | Loss: 0.00001737
Iteration 147/1000 | Loss: 0.00001736
Iteration 148/1000 | Loss: 0.00001734
Iteration 149/1000 | Loss: 0.00001734
Iteration 150/1000 | Loss: 0.00001733
Iteration 151/1000 | Loss: 0.00001733
Iteration 152/1000 | Loss: 0.00001733
Iteration 153/1000 | Loss: 0.00001732
Iteration 154/1000 | Loss: 0.00001731
Iteration 155/1000 | Loss: 0.00001731
Iteration 156/1000 | Loss: 0.00001730
Iteration 157/1000 | Loss: 0.00001730
Iteration 158/1000 | Loss: 0.00001730
Iteration 159/1000 | Loss: 0.00001730
Iteration 160/1000 | Loss: 0.00001726
Iteration 161/1000 | Loss: 0.00001726
Iteration 162/1000 | Loss: 0.00001726
Iteration 163/1000 | Loss: 0.00001726
Iteration 164/1000 | Loss: 0.00001726
Iteration 165/1000 | Loss: 0.00001726
Iteration 166/1000 | Loss: 0.00001726
Iteration 167/1000 | Loss: 0.00001725
Iteration 168/1000 | Loss: 0.00001725
Iteration 169/1000 | Loss: 0.00001725
Iteration 170/1000 | Loss: 0.00001725
Iteration 171/1000 | Loss: 0.00001724
Iteration 172/1000 | Loss: 0.00001723
Iteration 173/1000 | Loss: 0.00001723
Iteration 174/1000 | Loss: 0.00001723
Iteration 175/1000 | Loss: 0.00001723
Iteration 176/1000 | Loss: 0.00001723
Iteration 177/1000 | Loss: 0.00001723
Iteration 178/1000 | Loss: 0.00001723
Iteration 179/1000 | Loss: 0.00001723
Iteration 180/1000 | Loss: 0.00001722
Iteration 181/1000 | Loss: 0.00001721
Iteration 182/1000 | Loss: 0.00001721
Iteration 183/1000 | Loss: 0.00001720
Iteration 184/1000 | Loss: 0.00001720
Iteration 185/1000 | Loss: 0.00001720
Iteration 186/1000 | Loss: 0.00001720
Iteration 187/1000 | Loss: 0.00001720
Iteration 188/1000 | Loss: 0.00001720
Iteration 189/1000 | Loss: 0.00001720
Iteration 190/1000 | Loss: 0.00001720
Iteration 191/1000 | Loss: 0.00001720
Iteration 192/1000 | Loss: 0.00001720
Iteration 193/1000 | Loss: 0.00001720
Iteration 194/1000 | Loss: 0.00001719
Iteration 195/1000 | Loss: 0.00001719
Iteration 196/1000 | Loss: 0.00001719
Iteration 197/1000 | Loss: 0.00001719
Iteration 198/1000 | Loss: 0.00001718
Iteration 199/1000 | Loss: 0.00001718
Iteration 200/1000 | Loss: 0.00001718
Iteration 201/1000 | Loss: 0.00001717
Iteration 202/1000 | Loss: 0.00001717
Iteration 203/1000 | Loss: 0.00001717
Iteration 204/1000 | Loss: 0.00001717
Iteration 205/1000 | Loss: 0.00001717
Iteration 206/1000 | Loss: 0.00001717
Iteration 207/1000 | Loss: 0.00001717
Iteration 208/1000 | Loss: 0.00001717
Iteration 209/1000 | Loss: 0.00001717
Iteration 210/1000 | Loss: 0.00001717
Iteration 211/1000 | Loss: 0.00001717
Iteration 212/1000 | Loss: 0.00001717
Iteration 213/1000 | Loss: 0.00001717
Iteration 214/1000 | Loss: 0.00001717
Iteration 215/1000 | Loss: 0.00001717
Iteration 216/1000 | Loss: 0.00001717
Iteration 217/1000 | Loss: 0.00001717
Iteration 218/1000 | Loss: 0.00001717
Iteration 219/1000 | Loss: 0.00001717
Iteration 220/1000 | Loss: 0.00001717
Iteration 221/1000 | Loss: 0.00001717
Iteration 222/1000 | Loss: 0.00001717
Iteration 223/1000 | Loss: 0.00001717
Iteration 224/1000 | Loss: 0.00001717
Iteration 225/1000 | Loss: 0.00001717
Iteration 226/1000 | Loss: 0.00001717
Iteration 227/1000 | Loss: 0.00001717
Iteration 228/1000 | Loss: 0.00001717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.7169377315440215e-05, 1.7169377315440215e-05, 1.7169377315440215e-05, 1.7169377315440215e-05, 1.7169377315440215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7169377315440215e-05

Optimization complete. Final v2v error: 3.5901129245758057 mm

Highest mean error: 4.022246360778809 mm for frame 148

Lowest mean error: 3.113121747970581 mm for frame 18

Saving results

Total time: 265.7210614681244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01098989
Iteration 2/25 | Loss: 0.01098988
Iteration 3/25 | Loss: 0.00195491
Iteration 4/25 | Loss: 0.00127238
Iteration 5/25 | Loss: 0.00141380
Iteration 6/25 | Loss: 0.00120927
Iteration 7/25 | Loss: 0.00125361
Iteration 8/25 | Loss: 0.00125534
Iteration 9/25 | Loss: 0.00131010
Iteration 10/25 | Loss: 0.00127389
Iteration 11/25 | Loss: 0.00114842
Iteration 12/25 | Loss: 0.00108834
Iteration 13/25 | Loss: 0.00104121
Iteration 14/25 | Loss: 0.00106705
Iteration 15/25 | Loss: 0.00100710
Iteration 16/25 | Loss: 0.00104438
Iteration 17/25 | Loss: 0.00101721
Iteration 18/25 | Loss: 0.00100631
Iteration 19/25 | Loss: 0.00102430
Iteration 20/25 | Loss: 0.00098812
Iteration 21/25 | Loss: 0.00097949
Iteration 22/25 | Loss: 0.00097049
Iteration 23/25 | Loss: 0.00097047
Iteration 24/25 | Loss: 0.00096676
Iteration 25/25 | Loss: 0.00097146

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45568109
Iteration 2/25 | Loss: 0.00109069
Iteration 3/25 | Loss: 0.00109069
Iteration 4/25 | Loss: 0.00109069
Iteration 5/25 | Loss: 0.00109068
Iteration 6/25 | Loss: 0.00109068
Iteration 7/25 | Loss: 0.00109068
Iteration 8/25 | Loss: 0.00109068
Iteration 9/25 | Loss: 0.00109068
Iteration 10/25 | Loss: 0.00109068
Iteration 11/25 | Loss: 0.00109068
Iteration 12/25 | Loss: 0.00109068
Iteration 13/25 | Loss: 0.00109068
Iteration 14/25 | Loss: 0.00109068
Iteration 15/25 | Loss: 0.00109068
Iteration 16/25 | Loss: 0.00109068
Iteration 17/25 | Loss: 0.00109068
Iteration 18/25 | Loss: 0.00109068
Iteration 19/25 | Loss: 0.00109068
Iteration 20/25 | Loss: 0.00109068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010906829265877604, 0.0010906829265877604, 0.0010906829265877604, 0.0010906829265877604, 0.0010906829265877604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010906829265877604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109068
Iteration 2/1000 | Loss: 0.00304593
Iteration 3/1000 | Loss: 0.00076805
Iteration 4/1000 | Loss: 0.00044774
Iteration 5/1000 | Loss: 0.00056576
Iteration 6/1000 | Loss: 0.00024393
Iteration 7/1000 | Loss: 0.00047768
Iteration 8/1000 | Loss: 0.00035092
Iteration 9/1000 | Loss: 0.00028836
Iteration 10/1000 | Loss: 0.00039205
Iteration 11/1000 | Loss: 0.00032596
Iteration 12/1000 | Loss: 0.00014877
Iteration 13/1000 | Loss: 0.00039483
Iteration 14/1000 | Loss: 0.00027624
Iteration 15/1000 | Loss: 0.00038733
Iteration 16/1000 | Loss: 0.00021829
Iteration 17/1000 | Loss: 0.00039579
Iteration 18/1000 | Loss: 0.00040978
Iteration 19/1000 | Loss: 0.00014535
Iteration 20/1000 | Loss: 0.00044567
Iteration 21/1000 | Loss: 0.00050641
Iteration 22/1000 | Loss: 0.00150216
Iteration 23/1000 | Loss: 0.00041417
Iteration 24/1000 | Loss: 0.00025151
Iteration 25/1000 | Loss: 0.00048456
Iteration 26/1000 | Loss: 0.00045907
Iteration 27/1000 | Loss: 0.00039385
Iteration 28/1000 | Loss: 0.00005918
Iteration 29/1000 | Loss: 0.00006270
Iteration 30/1000 | Loss: 0.00022693
Iteration 31/1000 | Loss: 0.00012778
Iteration 32/1000 | Loss: 0.00003980
Iteration 33/1000 | Loss: 0.00030039
Iteration 34/1000 | Loss: 0.00027835
Iteration 35/1000 | Loss: 0.00023543
Iteration 36/1000 | Loss: 0.00039644
Iteration 37/1000 | Loss: 0.00020221
Iteration 38/1000 | Loss: 0.00028150
Iteration 39/1000 | Loss: 0.00019733
Iteration 40/1000 | Loss: 0.00022204
Iteration 41/1000 | Loss: 0.00026721
Iteration 42/1000 | Loss: 0.00012811
Iteration 43/1000 | Loss: 0.00014349
Iteration 44/1000 | Loss: 0.00034756
Iteration 45/1000 | Loss: 0.00025628
Iteration 46/1000 | Loss: 0.00005488
Iteration 47/1000 | Loss: 0.00018702
Iteration 48/1000 | Loss: 0.00005899
Iteration 49/1000 | Loss: 0.00004075
Iteration 50/1000 | Loss: 0.00079825
Iteration 51/1000 | Loss: 0.00017634
Iteration 52/1000 | Loss: 0.00064012
Iteration 53/1000 | Loss: 0.00023130
Iteration 54/1000 | Loss: 0.00013486
Iteration 55/1000 | Loss: 0.00055753
Iteration 56/1000 | Loss: 0.00030287
Iteration 57/1000 | Loss: 0.00011986
Iteration 58/1000 | Loss: 0.00002693
Iteration 59/1000 | Loss: 0.00083589
Iteration 60/1000 | Loss: 0.00078990
Iteration 61/1000 | Loss: 0.00012713
Iteration 62/1000 | Loss: 0.00059097
Iteration 63/1000 | Loss: 0.00016696
Iteration 64/1000 | Loss: 0.00058098
Iteration 65/1000 | Loss: 0.00020982
Iteration 66/1000 | Loss: 0.00010861
Iteration 67/1000 | Loss: 0.00057605
Iteration 68/1000 | Loss: 0.00010306
Iteration 69/1000 | Loss: 0.00076107
Iteration 70/1000 | Loss: 0.00018290
Iteration 71/1000 | Loss: 0.00065534
Iteration 72/1000 | Loss: 0.00002725
Iteration 73/1000 | Loss: 0.00003457
Iteration 74/1000 | Loss: 0.00002340
Iteration 75/1000 | Loss: 0.00001593
Iteration 76/1000 | Loss: 0.00003300
Iteration 77/1000 | Loss: 0.00001611
Iteration 78/1000 | Loss: 0.00001544
Iteration 79/1000 | Loss: 0.00003502
Iteration 80/1000 | Loss: 0.00002872
Iteration 81/1000 | Loss: 0.00003217
Iteration 82/1000 | Loss: 0.00002650
Iteration 83/1000 | Loss: 0.00003198
Iteration 84/1000 | Loss: 0.00002551
Iteration 85/1000 | Loss: 0.00003296
Iteration 86/1000 | Loss: 0.00002466
Iteration 87/1000 | Loss: 0.00003019
Iteration 88/1000 | Loss: 0.00002560
Iteration 89/1000 | Loss: 0.00002493
Iteration 90/1000 | Loss: 0.00001659
Iteration 91/1000 | Loss: 0.00001391
Iteration 92/1000 | Loss: 0.00003076
Iteration 93/1000 | Loss: 0.00004491
Iteration 94/1000 | Loss: 0.00001689
Iteration 95/1000 | Loss: 0.00001275
Iteration 96/1000 | Loss: 0.00001087
Iteration 97/1000 | Loss: 0.00000974
Iteration 98/1000 | Loss: 0.00000934
Iteration 99/1000 | Loss: 0.00000895
Iteration 100/1000 | Loss: 0.00000882
Iteration 101/1000 | Loss: 0.00000881
Iteration 102/1000 | Loss: 0.00000863
Iteration 103/1000 | Loss: 0.00000861
Iteration 104/1000 | Loss: 0.00000857
Iteration 105/1000 | Loss: 0.00000856
Iteration 106/1000 | Loss: 0.00000855
Iteration 107/1000 | Loss: 0.00000855
Iteration 108/1000 | Loss: 0.00000854
Iteration 109/1000 | Loss: 0.00000854
Iteration 110/1000 | Loss: 0.00000853
Iteration 111/1000 | Loss: 0.00000841
Iteration 112/1000 | Loss: 0.00000841
Iteration 113/1000 | Loss: 0.00000841
Iteration 114/1000 | Loss: 0.00000841
Iteration 115/1000 | Loss: 0.00000839
Iteration 116/1000 | Loss: 0.00000839
Iteration 117/1000 | Loss: 0.00000839
Iteration 118/1000 | Loss: 0.00000838
Iteration 119/1000 | Loss: 0.00000838
Iteration 120/1000 | Loss: 0.00000838
Iteration 121/1000 | Loss: 0.00000837
Iteration 122/1000 | Loss: 0.00000837
Iteration 123/1000 | Loss: 0.00000836
Iteration 124/1000 | Loss: 0.00000836
Iteration 125/1000 | Loss: 0.00000836
Iteration 126/1000 | Loss: 0.00000835
Iteration 127/1000 | Loss: 0.00000835
Iteration 128/1000 | Loss: 0.00000834
Iteration 129/1000 | Loss: 0.00000834
Iteration 130/1000 | Loss: 0.00000834
Iteration 131/1000 | Loss: 0.00000834
Iteration 132/1000 | Loss: 0.00000833
Iteration 133/1000 | Loss: 0.00000833
Iteration 134/1000 | Loss: 0.00000833
Iteration 135/1000 | Loss: 0.00000832
Iteration 136/1000 | Loss: 0.00000832
Iteration 137/1000 | Loss: 0.00000831
Iteration 138/1000 | Loss: 0.00000831
Iteration 139/1000 | Loss: 0.00000831
Iteration 140/1000 | Loss: 0.00000830
Iteration 141/1000 | Loss: 0.00000830
Iteration 142/1000 | Loss: 0.00000829
Iteration 143/1000 | Loss: 0.00000829
Iteration 144/1000 | Loss: 0.00000829
Iteration 145/1000 | Loss: 0.00000829
Iteration 146/1000 | Loss: 0.00000829
Iteration 147/1000 | Loss: 0.00000828
Iteration 148/1000 | Loss: 0.00000828
Iteration 149/1000 | Loss: 0.00000828
Iteration 150/1000 | Loss: 0.00000827
Iteration 151/1000 | Loss: 0.00000827
Iteration 152/1000 | Loss: 0.00000827
Iteration 153/1000 | Loss: 0.00000827
Iteration 154/1000 | Loss: 0.00000827
Iteration 155/1000 | Loss: 0.00000826
Iteration 156/1000 | Loss: 0.00000826
Iteration 157/1000 | Loss: 0.00000826
Iteration 158/1000 | Loss: 0.00000826
Iteration 159/1000 | Loss: 0.00000825
Iteration 160/1000 | Loss: 0.00000825
Iteration 161/1000 | Loss: 0.00000825
Iteration 162/1000 | Loss: 0.00000825
Iteration 163/1000 | Loss: 0.00000825
Iteration 164/1000 | Loss: 0.00000825
Iteration 165/1000 | Loss: 0.00000824
Iteration 166/1000 | Loss: 0.00000824
Iteration 167/1000 | Loss: 0.00000824
Iteration 168/1000 | Loss: 0.00000824
Iteration 169/1000 | Loss: 0.00000824
Iteration 170/1000 | Loss: 0.00000824
Iteration 171/1000 | Loss: 0.00000824
Iteration 172/1000 | Loss: 0.00000824
Iteration 173/1000 | Loss: 0.00000823
Iteration 174/1000 | Loss: 0.00000823
Iteration 175/1000 | Loss: 0.00000823
Iteration 176/1000 | Loss: 0.00000823
Iteration 177/1000 | Loss: 0.00000822
Iteration 178/1000 | Loss: 0.00000822
Iteration 179/1000 | Loss: 0.00000822
Iteration 180/1000 | Loss: 0.00000822
Iteration 181/1000 | Loss: 0.00000822
Iteration 182/1000 | Loss: 0.00000822
Iteration 183/1000 | Loss: 0.00000821
Iteration 184/1000 | Loss: 0.00000821
Iteration 185/1000 | Loss: 0.00000821
Iteration 186/1000 | Loss: 0.00000821
Iteration 187/1000 | Loss: 0.00000821
Iteration 188/1000 | Loss: 0.00000821
Iteration 189/1000 | Loss: 0.00000821
Iteration 190/1000 | Loss: 0.00000821
Iteration 191/1000 | Loss: 0.00000820
Iteration 192/1000 | Loss: 0.00000820
Iteration 193/1000 | Loss: 0.00000820
Iteration 194/1000 | Loss: 0.00000820
Iteration 195/1000 | Loss: 0.00000820
Iteration 196/1000 | Loss: 0.00000820
Iteration 197/1000 | Loss: 0.00000820
Iteration 198/1000 | Loss: 0.00000820
Iteration 199/1000 | Loss: 0.00000820
Iteration 200/1000 | Loss: 0.00000819
Iteration 201/1000 | Loss: 0.00000819
Iteration 202/1000 | Loss: 0.00000819
Iteration 203/1000 | Loss: 0.00000819
Iteration 204/1000 | Loss: 0.00000819
Iteration 205/1000 | Loss: 0.00000819
Iteration 206/1000 | Loss: 0.00000819
Iteration 207/1000 | Loss: 0.00000819
Iteration 208/1000 | Loss: 0.00000819
Iteration 209/1000 | Loss: 0.00000819
Iteration 210/1000 | Loss: 0.00000819
Iteration 211/1000 | Loss: 0.00000819
Iteration 212/1000 | Loss: 0.00000819
Iteration 213/1000 | Loss: 0.00000819
Iteration 214/1000 | Loss: 0.00000819
Iteration 215/1000 | Loss: 0.00000819
Iteration 216/1000 | Loss: 0.00000819
Iteration 217/1000 | Loss: 0.00000819
Iteration 218/1000 | Loss: 0.00000819
Iteration 219/1000 | Loss: 0.00000819
Iteration 220/1000 | Loss: 0.00000819
Iteration 221/1000 | Loss: 0.00000819
Iteration 222/1000 | Loss: 0.00000819
Iteration 223/1000 | Loss: 0.00000819
Iteration 224/1000 | Loss: 0.00000819
Iteration 225/1000 | Loss: 0.00000819
Iteration 226/1000 | Loss: 0.00000819
Iteration 227/1000 | Loss: 0.00000819
Iteration 228/1000 | Loss: 0.00000819
Iteration 229/1000 | Loss: 0.00000819
Iteration 230/1000 | Loss: 0.00000819
Iteration 231/1000 | Loss: 0.00000819
Iteration 232/1000 | Loss: 0.00000819
Iteration 233/1000 | Loss: 0.00000819
Iteration 234/1000 | Loss: 0.00000819
Iteration 235/1000 | Loss: 0.00000819
Iteration 236/1000 | Loss: 0.00000819
Iteration 237/1000 | Loss: 0.00000819
Iteration 238/1000 | Loss: 0.00000819
Iteration 239/1000 | Loss: 0.00000819
Iteration 240/1000 | Loss: 0.00000819
Iteration 241/1000 | Loss: 0.00000819
Iteration 242/1000 | Loss: 0.00000819
Iteration 243/1000 | Loss: 0.00000819
Iteration 244/1000 | Loss: 0.00000819
Iteration 245/1000 | Loss: 0.00000819
Iteration 246/1000 | Loss: 0.00000819
Iteration 247/1000 | Loss: 0.00000819
Iteration 248/1000 | Loss: 0.00000819
Iteration 249/1000 | Loss: 0.00000819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [8.1892721937038e-06, 8.1892721937038e-06, 8.1892721937038e-06, 8.1892721937038e-06, 8.1892721937038e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.1892721937038e-06

Optimization complete. Final v2v error: 2.3806068897247314 mm

Highest mean error: 3.4523799419403076 mm for frame 57

Lowest mean error: 1.9563380479812622 mm for frame 9

Saving results

Total time: 187.9302101135254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912923
Iteration 2/25 | Loss: 0.00345234
Iteration 3/25 | Loss: 0.00304211
Iteration 4/25 | Loss: 0.00239167
Iteration 5/25 | Loss: 0.00215550
Iteration 6/25 | Loss: 0.00202623
Iteration 7/25 | Loss: 0.00196059
Iteration 8/25 | Loss: 0.00200644
Iteration 9/25 | Loss: 0.00190836
Iteration 10/25 | Loss: 0.00152646
Iteration 11/25 | Loss: 0.00141801
Iteration 12/25 | Loss: 0.00136437
Iteration 13/25 | Loss: 0.00139547
Iteration 14/25 | Loss: 0.00131087
Iteration 15/25 | Loss: 0.00130103
Iteration 16/25 | Loss: 0.00130000
Iteration 17/25 | Loss: 0.00129991
Iteration 18/25 | Loss: 0.00129991
Iteration 19/25 | Loss: 0.00129991
Iteration 20/25 | Loss: 0.00129991
Iteration 21/25 | Loss: 0.00129991
Iteration 22/25 | Loss: 0.00129991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00129991106223315, 0.00129991106223315, 0.00129991106223315, 0.00129991106223315, 0.00129991106223315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00129991106223315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29966986
Iteration 2/25 | Loss: 0.00146763
Iteration 3/25 | Loss: 0.00086099
Iteration 4/25 | Loss: 0.00086099
Iteration 5/25 | Loss: 0.00086099
Iteration 6/25 | Loss: 0.00086099
Iteration 7/25 | Loss: 0.00086099
Iteration 8/25 | Loss: 0.00086099
Iteration 9/25 | Loss: 0.00086099
Iteration 10/25 | Loss: 0.00086099
Iteration 11/25 | Loss: 0.00086099
Iteration 12/25 | Loss: 0.00086099
Iteration 13/25 | Loss: 0.00086099
Iteration 14/25 | Loss: 0.00086099
Iteration 15/25 | Loss: 0.00086099
Iteration 16/25 | Loss: 0.00086099
Iteration 17/25 | Loss: 0.00086099
Iteration 18/25 | Loss: 0.00086099
Iteration 19/25 | Loss: 0.00086099
Iteration 20/25 | Loss: 0.00086099
Iteration 21/25 | Loss: 0.00086099
Iteration 22/25 | Loss: 0.00086099
Iteration 23/25 | Loss: 0.00086099
Iteration 24/25 | Loss: 0.00086099
Iteration 25/25 | Loss: 0.00086099
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008609851938672364, 0.0008609851938672364, 0.0008609851938672364, 0.0008609851938672364, 0.0008609851938672364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008609851938672364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086099
Iteration 2/1000 | Loss: 0.00026729
Iteration 3/1000 | Loss: 0.00014073
Iteration 4/1000 | Loss: 0.00009531
Iteration 5/1000 | Loss: 0.00079576
Iteration 6/1000 | Loss: 0.00031687
Iteration 7/1000 | Loss: 0.00008851
Iteration 8/1000 | Loss: 0.00021436
Iteration 9/1000 | Loss: 0.00008544
Iteration 10/1000 | Loss: 0.00008421
Iteration 11/1000 | Loss: 0.00008341
Iteration 12/1000 | Loss: 0.00008284
Iteration 13/1000 | Loss: 0.00008217
Iteration 14/1000 | Loss: 0.00008173
Iteration 15/1000 | Loss: 0.00008138
Iteration 16/1000 | Loss: 0.00008110
Iteration 17/1000 | Loss: 0.00008101
Iteration 18/1000 | Loss: 0.00008097
Iteration 19/1000 | Loss: 0.00008086
Iteration 20/1000 | Loss: 0.00008079
Iteration 21/1000 | Loss: 0.00008070
Iteration 22/1000 | Loss: 0.00008069
Iteration 23/1000 | Loss: 0.00008068
Iteration 24/1000 | Loss: 0.00008067
Iteration 25/1000 | Loss: 0.00008066
Iteration 26/1000 | Loss: 0.00008066
Iteration 27/1000 | Loss: 0.00008065
Iteration 28/1000 | Loss: 0.00008065
Iteration 29/1000 | Loss: 0.00008065
Iteration 30/1000 | Loss: 0.00008064
Iteration 31/1000 | Loss: 0.00008064
Iteration 32/1000 | Loss: 0.00008061
Iteration 33/1000 | Loss: 0.00008061
Iteration 34/1000 | Loss: 0.00008060
Iteration 35/1000 | Loss: 0.00008060
Iteration 36/1000 | Loss: 0.00008059
Iteration 37/1000 | Loss: 0.00008058
Iteration 38/1000 | Loss: 0.00008057
Iteration 39/1000 | Loss: 0.00008056
Iteration 40/1000 | Loss: 0.00008056
Iteration 41/1000 | Loss: 0.00008056
Iteration 42/1000 | Loss: 0.00008056
Iteration 43/1000 | Loss: 0.00008056
Iteration 44/1000 | Loss: 0.00008054
Iteration 45/1000 | Loss: 0.00008053
Iteration 46/1000 | Loss: 0.00008053
Iteration 47/1000 | Loss: 0.00008053
Iteration 48/1000 | Loss: 0.00008051
Iteration 49/1000 | Loss: 0.00008050
Iteration 50/1000 | Loss: 0.00008049
Iteration 51/1000 | Loss: 0.00008049
Iteration 52/1000 | Loss: 0.00008049
Iteration 53/1000 | Loss: 0.00008049
Iteration 54/1000 | Loss: 0.00008049
Iteration 55/1000 | Loss: 0.00008048
Iteration 56/1000 | Loss: 0.00008048
Iteration 57/1000 | Loss: 0.00008048
Iteration 58/1000 | Loss: 0.00008048
Iteration 59/1000 | Loss: 0.00008048
Iteration 60/1000 | Loss: 0.00008048
Iteration 61/1000 | Loss: 0.00008047
Iteration 62/1000 | Loss: 0.00008047
Iteration 63/1000 | Loss: 0.00008046
Iteration 64/1000 | Loss: 0.00008046
Iteration 65/1000 | Loss: 0.00008046
Iteration 66/1000 | Loss: 0.00008045
Iteration 67/1000 | Loss: 0.00008045
Iteration 68/1000 | Loss: 0.00008045
Iteration 69/1000 | Loss: 0.00008045
Iteration 70/1000 | Loss: 0.00008045
Iteration 71/1000 | Loss: 0.00008045
Iteration 72/1000 | Loss: 0.00008045
Iteration 73/1000 | Loss: 0.00008044
Iteration 74/1000 | Loss: 0.00008044
Iteration 75/1000 | Loss: 0.00008044
Iteration 76/1000 | Loss: 0.00008043
Iteration 77/1000 | Loss: 0.00008043
Iteration 78/1000 | Loss: 0.00008043
Iteration 79/1000 | Loss: 0.00008043
Iteration 80/1000 | Loss: 0.00008043
Iteration 81/1000 | Loss: 0.00008043
Iteration 82/1000 | Loss: 0.00008043
Iteration 83/1000 | Loss: 0.00008043
Iteration 84/1000 | Loss: 0.00008043
Iteration 85/1000 | Loss: 0.00008042
Iteration 86/1000 | Loss: 0.00008042
Iteration 87/1000 | Loss: 0.00008042
Iteration 88/1000 | Loss: 0.00008042
Iteration 89/1000 | Loss: 0.00008042
Iteration 90/1000 | Loss: 0.00008042
Iteration 91/1000 | Loss: 0.00008042
Iteration 92/1000 | Loss: 0.00008042
Iteration 93/1000 | Loss: 0.00008042
Iteration 94/1000 | Loss: 0.00008042
Iteration 95/1000 | Loss: 0.00008042
Iteration 96/1000 | Loss: 0.00008042
Iteration 97/1000 | Loss: 0.00008042
Iteration 98/1000 | Loss: 0.00008042
Iteration 99/1000 | Loss: 0.00008041
Iteration 100/1000 | Loss: 0.00008041
Iteration 101/1000 | Loss: 0.00008041
Iteration 102/1000 | Loss: 0.00008041
Iteration 103/1000 | Loss: 0.00008041
Iteration 104/1000 | Loss: 0.00008041
Iteration 105/1000 | Loss: 0.00008040
Iteration 106/1000 | Loss: 0.00008040
Iteration 107/1000 | Loss: 0.00008040
Iteration 108/1000 | Loss: 0.00008039
Iteration 109/1000 | Loss: 0.00008039
Iteration 110/1000 | Loss: 0.00008039
Iteration 111/1000 | Loss: 0.00008039
Iteration 112/1000 | Loss: 0.00008039
Iteration 113/1000 | Loss: 0.00008039
Iteration 114/1000 | Loss: 0.00008039
Iteration 115/1000 | Loss: 0.00008039
Iteration 116/1000 | Loss: 0.00008039
Iteration 117/1000 | Loss: 0.00008039
Iteration 118/1000 | Loss: 0.00008038
Iteration 119/1000 | Loss: 0.00008038
Iteration 120/1000 | Loss: 0.00008038
Iteration 121/1000 | Loss: 0.00008038
Iteration 122/1000 | Loss: 0.00008038
Iteration 123/1000 | Loss: 0.00008038
Iteration 124/1000 | Loss: 0.00008038
Iteration 125/1000 | Loss: 0.00008038
Iteration 126/1000 | Loss: 0.00008038
Iteration 127/1000 | Loss: 0.00008038
Iteration 128/1000 | Loss: 0.00008038
Iteration 129/1000 | Loss: 0.00008038
Iteration 130/1000 | Loss: 0.00008038
Iteration 131/1000 | Loss: 0.00008038
Iteration 132/1000 | Loss: 0.00008038
Iteration 133/1000 | Loss: 0.00008038
Iteration 134/1000 | Loss: 0.00008038
Iteration 135/1000 | Loss: 0.00008037
Iteration 136/1000 | Loss: 0.00008037
Iteration 137/1000 | Loss: 0.00008037
Iteration 138/1000 | Loss: 0.00008037
Iteration 139/1000 | Loss: 0.00008037
Iteration 140/1000 | Loss: 0.00008037
Iteration 141/1000 | Loss: 0.00008037
Iteration 142/1000 | Loss: 0.00008037
Iteration 143/1000 | Loss: 0.00008037
Iteration 144/1000 | Loss: 0.00008037
Iteration 145/1000 | Loss: 0.00008036
Iteration 146/1000 | Loss: 0.00008036
Iteration 147/1000 | Loss: 0.00008036
Iteration 148/1000 | Loss: 0.00008036
Iteration 149/1000 | Loss: 0.00008036
Iteration 150/1000 | Loss: 0.00008036
Iteration 151/1000 | Loss: 0.00008036
Iteration 152/1000 | Loss: 0.00008036
Iteration 153/1000 | Loss: 0.00008036
Iteration 154/1000 | Loss: 0.00008035
Iteration 155/1000 | Loss: 0.00008035
Iteration 156/1000 | Loss: 0.00008035
Iteration 157/1000 | Loss: 0.00008035
Iteration 158/1000 | Loss: 0.00008035
Iteration 159/1000 | Loss: 0.00008035
Iteration 160/1000 | Loss: 0.00008035
Iteration 161/1000 | Loss: 0.00008035
Iteration 162/1000 | Loss: 0.00008035
Iteration 163/1000 | Loss: 0.00008035
Iteration 164/1000 | Loss: 0.00008035
Iteration 165/1000 | Loss: 0.00008035
Iteration 166/1000 | Loss: 0.00008035
Iteration 167/1000 | Loss: 0.00008035
Iteration 168/1000 | Loss: 0.00008035
Iteration 169/1000 | Loss: 0.00008035
Iteration 170/1000 | Loss: 0.00008035
Iteration 171/1000 | Loss: 0.00008035
Iteration 172/1000 | Loss: 0.00008035
Iteration 173/1000 | Loss: 0.00008035
Iteration 174/1000 | Loss: 0.00008034
Iteration 175/1000 | Loss: 0.00008034
Iteration 176/1000 | Loss: 0.00008034
Iteration 177/1000 | Loss: 0.00008034
Iteration 178/1000 | Loss: 0.00008034
Iteration 179/1000 | Loss: 0.00008034
Iteration 180/1000 | Loss: 0.00008034
Iteration 181/1000 | Loss: 0.00008034
Iteration 182/1000 | Loss: 0.00008034
Iteration 183/1000 | Loss: 0.00008034
Iteration 184/1000 | Loss: 0.00008034
Iteration 185/1000 | Loss: 0.00008034
Iteration 186/1000 | Loss: 0.00008034
Iteration 187/1000 | Loss: 0.00008034
Iteration 188/1000 | Loss: 0.00008034
Iteration 189/1000 | Loss: 0.00008034
Iteration 190/1000 | Loss: 0.00008034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [8.033607446122915e-05, 8.033607446122915e-05, 8.033607446122915e-05, 8.033607446122915e-05, 8.033607446122915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.033607446122915e-05

Optimization complete. Final v2v error: 4.621739864349365 mm

Highest mean error: 18.500478744506836 mm for frame 3

Lowest mean error: 3.205536127090454 mm for frame 131

Saving results

Total time: 69.20778393745422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840949
Iteration 2/25 | Loss: 0.00114742
Iteration 3/25 | Loss: 0.00104660
Iteration 4/25 | Loss: 0.00103648
Iteration 5/25 | Loss: 0.00103492
Iteration 6/25 | Loss: 0.00103492
Iteration 7/25 | Loss: 0.00103492
Iteration 8/25 | Loss: 0.00103492
Iteration 9/25 | Loss: 0.00103492
Iteration 10/25 | Loss: 0.00103492
Iteration 11/25 | Loss: 0.00103492
Iteration 12/25 | Loss: 0.00103492
Iteration 13/25 | Loss: 0.00103492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010349248768761754, 0.0010349248768761754, 0.0010349248768761754, 0.0010349248768761754, 0.0010349248768761754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010349248768761754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31147051
Iteration 2/25 | Loss: 0.00080399
Iteration 3/25 | Loss: 0.00080399
Iteration 4/25 | Loss: 0.00080399
Iteration 5/25 | Loss: 0.00080399
Iteration 6/25 | Loss: 0.00080398
Iteration 7/25 | Loss: 0.00080398
Iteration 8/25 | Loss: 0.00080398
Iteration 9/25 | Loss: 0.00080398
Iteration 10/25 | Loss: 0.00080398
Iteration 11/25 | Loss: 0.00080398
Iteration 12/25 | Loss: 0.00080398
Iteration 13/25 | Loss: 0.00080398
Iteration 14/25 | Loss: 0.00080398
Iteration 15/25 | Loss: 0.00080398
Iteration 16/25 | Loss: 0.00080398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000803983595687896, 0.000803983595687896, 0.000803983595687896, 0.000803983595687896, 0.000803983595687896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000803983595687896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080398
Iteration 2/1000 | Loss: 0.00002697
Iteration 3/1000 | Loss: 0.00001962
Iteration 4/1000 | Loss: 0.00001709
Iteration 5/1000 | Loss: 0.00001619
Iteration 6/1000 | Loss: 0.00001551
Iteration 7/1000 | Loss: 0.00001496
Iteration 8/1000 | Loss: 0.00001455
Iteration 9/1000 | Loss: 0.00001427
Iteration 10/1000 | Loss: 0.00001408
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001404
Iteration 13/1000 | Loss: 0.00001404
Iteration 14/1000 | Loss: 0.00001403
Iteration 15/1000 | Loss: 0.00001401
Iteration 16/1000 | Loss: 0.00001401
Iteration 17/1000 | Loss: 0.00001401
Iteration 18/1000 | Loss: 0.00001401
Iteration 19/1000 | Loss: 0.00001401
Iteration 20/1000 | Loss: 0.00001400
Iteration 21/1000 | Loss: 0.00001398
Iteration 22/1000 | Loss: 0.00001398
Iteration 23/1000 | Loss: 0.00001397
Iteration 24/1000 | Loss: 0.00001396
Iteration 25/1000 | Loss: 0.00001395
Iteration 26/1000 | Loss: 0.00001392
Iteration 27/1000 | Loss: 0.00001391
Iteration 28/1000 | Loss: 0.00001390
Iteration 29/1000 | Loss: 0.00001388
Iteration 30/1000 | Loss: 0.00001388
Iteration 31/1000 | Loss: 0.00001388
Iteration 32/1000 | Loss: 0.00001387
Iteration 33/1000 | Loss: 0.00001387
Iteration 34/1000 | Loss: 0.00001387
Iteration 35/1000 | Loss: 0.00001386
Iteration 36/1000 | Loss: 0.00001384
Iteration 37/1000 | Loss: 0.00001384
Iteration 38/1000 | Loss: 0.00001384
Iteration 39/1000 | Loss: 0.00001384
Iteration 40/1000 | Loss: 0.00001383
Iteration 41/1000 | Loss: 0.00001383
Iteration 42/1000 | Loss: 0.00001383
Iteration 43/1000 | Loss: 0.00001383
Iteration 44/1000 | Loss: 0.00001383
Iteration 45/1000 | Loss: 0.00001382
Iteration 46/1000 | Loss: 0.00001381
Iteration 47/1000 | Loss: 0.00001381
Iteration 48/1000 | Loss: 0.00001380
Iteration 49/1000 | Loss: 0.00001380
Iteration 50/1000 | Loss: 0.00001380
Iteration 51/1000 | Loss: 0.00001380
Iteration 52/1000 | Loss: 0.00001380
Iteration 53/1000 | Loss: 0.00001379
Iteration 54/1000 | Loss: 0.00001379
Iteration 55/1000 | Loss: 0.00001379
Iteration 56/1000 | Loss: 0.00001379
Iteration 57/1000 | Loss: 0.00001379
Iteration 58/1000 | Loss: 0.00001379
Iteration 59/1000 | Loss: 0.00001378
Iteration 60/1000 | Loss: 0.00001378
Iteration 61/1000 | Loss: 0.00001378
Iteration 62/1000 | Loss: 0.00001378
Iteration 63/1000 | Loss: 0.00001378
Iteration 64/1000 | Loss: 0.00001378
Iteration 65/1000 | Loss: 0.00001378
Iteration 66/1000 | Loss: 0.00001378
Iteration 67/1000 | Loss: 0.00001378
Iteration 68/1000 | Loss: 0.00001378
Iteration 69/1000 | Loss: 0.00001378
Iteration 70/1000 | Loss: 0.00001377
Iteration 71/1000 | Loss: 0.00001377
Iteration 72/1000 | Loss: 0.00001377
Iteration 73/1000 | Loss: 0.00001377
Iteration 74/1000 | Loss: 0.00001377
Iteration 75/1000 | Loss: 0.00001377
Iteration 76/1000 | Loss: 0.00001376
Iteration 77/1000 | Loss: 0.00001376
Iteration 78/1000 | Loss: 0.00001376
Iteration 79/1000 | Loss: 0.00001376
Iteration 80/1000 | Loss: 0.00001375
Iteration 81/1000 | Loss: 0.00001375
Iteration 82/1000 | Loss: 0.00001375
Iteration 83/1000 | Loss: 0.00001375
Iteration 84/1000 | Loss: 0.00001375
Iteration 85/1000 | Loss: 0.00001375
Iteration 86/1000 | Loss: 0.00001375
Iteration 87/1000 | Loss: 0.00001375
Iteration 88/1000 | Loss: 0.00001375
Iteration 89/1000 | Loss: 0.00001375
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001375
Iteration 92/1000 | Loss: 0.00001375
Iteration 93/1000 | Loss: 0.00001375
Iteration 94/1000 | Loss: 0.00001375
Iteration 95/1000 | Loss: 0.00001374
Iteration 96/1000 | Loss: 0.00001374
Iteration 97/1000 | Loss: 0.00001374
Iteration 98/1000 | Loss: 0.00001374
Iteration 99/1000 | Loss: 0.00001374
Iteration 100/1000 | Loss: 0.00001374
Iteration 101/1000 | Loss: 0.00001374
Iteration 102/1000 | Loss: 0.00001374
Iteration 103/1000 | Loss: 0.00001373
Iteration 104/1000 | Loss: 0.00001373
Iteration 105/1000 | Loss: 0.00001373
Iteration 106/1000 | Loss: 0.00001373
Iteration 107/1000 | Loss: 0.00001373
Iteration 108/1000 | Loss: 0.00001372
Iteration 109/1000 | Loss: 0.00001372
Iteration 110/1000 | Loss: 0.00001372
Iteration 111/1000 | Loss: 0.00001372
Iteration 112/1000 | Loss: 0.00001372
Iteration 113/1000 | Loss: 0.00001371
Iteration 114/1000 | Loss: 0.00001371
Iteration 115/1000 | Loss: 0.00001371
Iteration 116/1000 | Loss: 0.00001370
Iteration 117/1000 | Loss: 0.00001370
Iteration 118/1000 | Loss: 0.00001370
Iteration 119/1000 | Loss: 0.00001370
Iteration 120/1000 | Loss: 0.00001370
Iteration 121/1000 | Loss: 0.00001369
Iteration 122/1000 | Loss: 0.00001369
Iteration 123/1000 | Loss: 0.00001369
Iteration 124/1000 | Loss: 0.00001369
Iteration 125/1000 | Loss: 0.00001369
Iteration 126/1000 | Loss: 0.00001369
Iteration 127/1000 | Loss: 0.00001368
Iteration 128/1000 | Loss: 0.00001368
Iteration 129/1000 | Loss: 0.00001368
Iteration 130/1000 | Loss: 0.00001368
Iteration 131/1000 | Loss: 0.00001367
Iteration 132/1000 | Loss: 0.00001367
Iteration 133/1000 | Loss: 0.00001367
Iteration 134/1000 | Loss: 0.00001367
Iteration 135/1000 | Loss: 0.00001367
Iteration 136/1000 | Loss: 0.00001367
Iteration 137/1000 | Loss: 0.00001367
Iteration 138/1000 | Loss: 0.00001367
Iteration 139/1000 | Loss: 0.00001367
Iteration 140/1000 | Loss: 0.00001367
Iteration 141/1000 | Loss: 0.00001366
Iteration 142/1000 | Loss: 0.00001366
Iteration 143/1000 | Loss: 0.00001366
Iteration 144/1000 | Loss: 0.00001366
Iteration 145/1000 | Loss: 0.00001366
Iteration 146/1000 | Loss: 0.00001366
Iteration 147/1000 | Loss: 0.00001366
Iteration 148/1000 | Loss: 0.00001366
Iteration 149/1000 | Loss: 0.00001366
Iteration 150/1000 | Loss: 0.00001366
Iteration 151/1000 | Loss: 0.00001365
Iteration 152/1000 | Loss: 0.00001365
Iteration 153/1000 | Loss: 0.00001365
Iteration 154/1000 | Loss: 0.00001365
Iteration 155/1000 | Loss: 0.00001365
Iteration 156/1000 | Loss: 0.00001365
Iteration 157/1000 | Loss: 0.00001365
Iteration 158/1000 | Loss: 0.00001364
Iteration 159/1000 | Loss: 0.00001364
Iteration 160/1000 | Loss: 0.00001364
Iteration 161/1000 | Loss: 0.00001363
Iteration 162/1000 | Loss: 0.00001363
Iteration 163/1000 | Loss: 0.00001363
Iteration 164/1000 | Loss: 0.00001362
Iteration 165/1000 | Loss: 0.00001362
Iteration 166/1000 | Loss: 0.00001362
Iteration 167/1000 | Loss: 0.00001362
Iteration 168/1000 | Loss: 0.00001362
Iteration 169/1000 | Loss: 0.00001362
Iteration 170/1000 | Loss: 0.00001362
Iteration 171/1000 | Loss: 0.00001362
Iteration 172/1000 | Loss: 0.00001362
Iteration 173/1000 | Loss: 0.00001362
Iteration 174/1000 | Loss: 0.00001361
Iteration 175/1000 | Loss: 0.00001361
Iteration 176/1000 | Loss: 0.00001361
Iteration 177/1000 | Loss: 0.00001361
Iteration 178/1000 | Loss: 0.00001360
Iteration 179/1000 | Loss: 0.00001360
Iteration 180/1000 | Loss: 0.00001360
Iteration 181/1000 | Loss: 0.00001360
Iteration 182/1000 | Loss: 0.00001360
Iteration 183/1000 | Loss: 0.00001360
Iteration 184/1000 | Loss: 0.00001360
Iteration 185/1000 | Loss: 0.00001359
Iteration 186/1000 | Loss: 0.00001359
Iteration 187/1000 | Loss: 0.00001359
Iteration 188/1000 | Loss: 0.00001359
Iteration 189/1000 | Loss: 0.00001359
Iteration 190/1000 | Loss: 0.00001359
Iteration 191/1000 | Loss: 0.00001359
Iteration 192/1000 | Loss: 0.00001359
Iteration 193/1000 | Loss: 0.00001359
Iteration 194/1000 | Loss: 0.00001359
Iteration 195/1000 | Loss: 0.00001359
Iteration 196/1000 | Loss: 0.00001359
Iteration 197/1000 | Loss: 0.00001359
Iteration 198/1000 | Loss: 0.00001359
Iteration 199/1000 | Loss: 0.00001359
Iteration 200/1000 | Loss: 0.00001359
Iteration 201/1000 | Loss: 0.00001359
Iteration 202/1000 | Loss: 0.00001359
Iteration 203/1000 | Loss: 0.00001359
Iteration 204/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.35927512019407e-05, 1.35927512019407e-05, 1.35927512019407e-05, 1.35927512019407e-05, 1.35927512019407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.35927512019407e-05

Optimization complete. Final v2v error: 3.179593324661255 mm

Highest mean error: 3.498133659362793 mm for frame 150

Lowest mean error: 2.840437650680542 mm for frame 48

Saving results

Total time: 39.66391968727112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837218
Iteration 2/25 | Loss: 0.00117929
Iteration 3/25 | Loss: 0.00096810
Iteration 4/25 | Loss: 0.00094721
Iteration 5/25 | Loss: 0.00094368
Iteration 6/25 | Loss: 0.00094305
Iteration 7/25 | Loss: 0.00094305
Iteration 8/25 | Loss: 0.00094305
Iteration 9/25 | Loss: 0.00094305
Iteration 10/25 | Loss: 0.00094305
Iteration 11/25 | Loss: 0.00094305
Iteration 12/25 | Loss: 0.00094305
Iteration 13/25 | Loss: 0.00094305
Iteration 14/25 | Loss: 0.00094305
Iteration 15/25 | Loss: 0.00094305
Iteration 16/25 | Loss: 0.00094305
Iteration 17/25 | Loss: 0.00094305
Iteration 18/25 | Loss: 0.00094305
Iteration 19/25 | Loss: 0.00094305
Iteration 20/25 | Loss: 0.00094305
Iteration 21/25 | Loss: 0.00094305
Iteration 22/25 | Loss: 0.00094305
Iteration 23/25 | Loss: 0.00094305
Iteration 24/25 | Loss: 0.00094305
Iteration 25/25 | Loss: 0.00094305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32702327
Iteration 2/25 | Loss: 0.00085995
Iteration 3/25 | Loss: 0.00085995
Iteration 4/25 | Loss: 0.00085995
Iteration 5/25 | Loss: 0.00085995
Iteration 6/25 | Loss: 0.00085995
Iteration 7/25 | Loss: 0.00085995
Iteration 8/25 | Loss: 0.00085995
Iteration 9/25 | Loss: 0.00085995
Iteration 10/25 | Loss: 0.00085995
Iteration 11/25 | Loss: 0.00085995
Iteration 12/25 | Loss: 0.00085995
Iteration 13/25 | Loss: 0.00085995
Iteration 14/25 | Loss: 0.00085995
Iteration 15/25 | Loss: 0.00085995
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008599492139182985, 0.0008599492139182985, 0.0008599492139182985, 0.0008599492139182985, 0.0008599492139182985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008599492139182985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085995
Iteration 2/1000 | Loss: 0.00003029
Iteration 3/1000 | Loss: 0.00001643
Iteration 4/1000 | Loss: 0.00001228
Iteration 5/1000 | Loss: 0.00001054
Iteration 6/1000 | Loss: 0.00000961
Iteration 7/1000 | Loss: 0.00000922
Iteration 8/1000 | Loss: 0.00000892
Iteration 9/1000 | Loss: 0.00000870
Iteration 10/1000 | Loss: 0.00000858
Iteration 11/1000 | Loss: 0.00000850
Iteration 12/1000 | Loss: 0.00000846
Iteration 13/1000 | Loss: 0.00000845
Iteration 14/1000 | Loss: 0.00000843
Iteration 15/1000 | Loss: 0.00000842
Iteration 16/1000 | Loss: 0.00000841
Iteration 17/1000 | Loss: 0.00000839
Iteration 18/1000 | Loss: 0.00000838
Iteration 19/1000 | Loss: 0.00000837
Iteration 20/1000 | Loss: 0.00000837
Iteration 21/1000 | Loss: 0.00000836
Iteration 22/1000 | Loss: 0.00000835
Iteration 23/1000 | Loss: 0.00000833
Iteration 24/1000 | Loss: 0.00000832
Iteration 25/1000 | Loss: 0.00000831
Iteration 26/1000 | Loss: 0.00000830
Iteration 27/1000 | Loss: 0.00000829
Iteration 28/1000 | Loss: 0.00000828
Iteration 29/1000 | Loss: 0.00000826
Iteration 30/1000 | Loss: 0.00000825
Iteration 31/1000 | Loss: 0.00000825
Iteration 32/1000 | Loss: 0.00000824
Iteration 33/1000 | Loss: 0.00000824
Iteration 34/1000 | Loss: 0.00000824
Iteration 35/1000 | Loss: 0.00000824
Iteration 36/1000 | Loss: 0.00000823
Iteration 37/1000 | Loss: 0.00000823
Iteration 38/1000 | Loss: 0.00000823
Iteration 39/1000 | Loss: 0.00000823
Iteration 40/1000 | Loss: 0.00000823
Iteration 41/1000 | Loss: 0.00000823
Iteration 42/1000 | Loss: 0.00000823
Iteration 43/1000 | Loss: 0.00000823
Iteration 44/1000 | Loss: 0.00000823
Iteration 45/1000 | Loss: 0.00000823
Iteration 46/1000 | Loss: 0.00000823
Iteration 47/1000 | Loss: 0.00000823
Iteration 48/1000 | Loss: 0.00000822
Iteration 49/1000 | Loss: 0.00000822
Iteration 50/1000 | Loss: 0.00000822
Iteration 51/1000 | Loss: 0.00000822
Iteration 52/1000 | Loss: 0.00000822
Iteration 53/1000 | Loss: 0.00000822
Iteration 54/1000 | Loss: 0.00000822
Iteration 55/1000 | Loss: 0.00000822
Iteration 56/1000 | Loss: 0.00000821
Iteration 57/1000 | Loss: 0.00000821
Iteration 58/1000 | Loss: 0.00000821
Iteration 59/1000 | Loss: 0.00000821
Iteration 60/1000 | Loss: 0.00000820
Iteration 61/1000 | Loss: 0.00000820
Iteration 62/1000 | Loss: 0.00000820
Iteration 63/1000 | Loss: 0.00000820
Iteration 64/1000 | Loss: 0.00000820
Iteration 65/1000 | Loss: 0.00000820
Iteration 66/1000 | Loss: 0.00000820
Iteration 67/1000 | Loss: 0.00000820
Iteration 68/1000 | Loss: 0.00000819
Iteration 69/1000 | Loss: 0.00000819
Iteration 70/1000 | Loss: 0.00000819
Iteration 71/1000 | Loss: 0.00000819
Iteration 72/1000 | Loss: 0.00000819
Iteration 73/1000 | Loss: 0.00000819
Iteration 74/1000 | Loss: 0.00000819
Iteration 75/1000 | Loss: 0.00000819
Iteration 76/1000 | Loss: 0.00000819
Iteration 77/1000 | Loss: 0.00000819
Iteration 78/1000 | Loss: 0.00000819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [8.192599125322886e-06, 8.192599125322886e-06, 8.192599125322886e-06, 8.192599125322886e-06, 8.192599125322886e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.192599125322886e-06

Optimization complete. Final v2v error: 2.42706036567688 mm

Highest mean error: 2.7860472202301025 mm for frame 45

Lowest mean error: 2.1757240295410156 mm for frame 83

Saving results

Total time: 31.755792140960693
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01102976
Iteration 2/25 | Loss: 0.00147590
Iteration 3/25 | Loss: 0.00120406
Iteration 4/25 | Loss: 0.00113520
Iteration 5/25 | Loss: 0.00112017
Iteration 6/25 | Loss: 0.00111329
Iteration 7/25 | Loss: 0.00112118
Iteration 8/25 | Loss: 0.00110362
Iteration 9/25 | Loss: 0.00110121
Iteration 10/25 | Loss: 0.00109611
Iteration 11/25 | Loss: 0.00108990
Iteration 12/25 | Loss: 0.00108914
Iteration 13/25 | Loss: 0.00108894
Iteration 14/25 | Loss: 0.00108886
Iteration 15/25 | Loss: 0.00108886
Iteration 16/25 | Loss: 0.00108886
Iteration 17/25 | Loss: 0.00108886
Iteration 18/25 | Loss: 0.00108886
Iteration 19/25 | Loss: 0.00108886
Iteration 20/25 | Loss: 0.00108886
Iteration 21/25 | Loss: 0.00108886
Iteration 22/25 | Loss: 0.00108885
Iteration 23/25 | Loss: 0.00108885
Iteration 24/25 | Loss: 0.00108885
Iteration 25/25 | Loss: 0.00108885

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.03472900
Iteration 2/25 | Loss: 0.02029873
Iteration 3/25 | Loss: 0.01288909
Iteration 4/25 | Loss: 0.00366208
Iteration 5/25 | Loss: 0.00111578
Iteration 6/25 | Loss: 0.00110528
Iteration 7/25 | Loss: 0.00110528
Iteration 8/25 | Loss: 0.00110528
Iteration 9/25 | Loss: 0.00110528
Iteration 10/25 | Loss: 0.00110528
Iteration 11/25 | Loss: 0.00110528
Iteration 12/25 | Loss: 0.00110528
Iteration 13/25 | Loss: 0.00110528
Iteration 14/25 | Loss: 0.00110528
Iteration 15/25 | Loss: 0.00110528
Iteration 16/25 | Loss: 0.00110528
Iteration 17/25 | Loss: 0.00110528
Iteration 18/25 | Loss: 0.00110528
Iteration 19/25 | Loss: 0.00110528
Iteration 20/25 | Loss: 0.00110528
Iteration 21/25 | Loss: 0.00110528
Iteration 22/25 | Loss: 0.00110528
Iteration 23/25 | Loss: 0.00110528
Iteration 24/25 | Loss: 0.00110528
Iteration 25/25 | Loss: 0.00110528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110528
Iteration 2/1000 | Loss: 0.00010216
Iteration 3/1000 | Loss: 0.00006311
Iteration 4/1000 | Loss: 0.00011230
Iteration 5/1000 | Loss: 0.00004619
Iteration 6/1000 | Loss: 0.00004188
Iteration 7/1000 | Loss: 0.00012220
Iteration 8/1000 | Loss: 0.00003996
Iteration 9/1000 | Loss: 0.00009151
Iteration 10/1000 | Loss: 0.00003895
Iteration 11/1000 | Loss: 0.00003841
Iteration 12/1000 | Loss: 0.00003781
Iteration 13/1000 | Loss: 0.00003733
Iteration 14/1000 | Loss: 0.00003695
Iteration 15/1000 | Loss: 0.00003667
Iteration 16/1000 | Loss: 0.00003642
Iteration 17/1000 | Loss: 0.00003617
Iteration 18/1000 | Loss: 0.00003598
Iteration 19/1000 | Loss: 0.00003590
Iteration 20/1000 | Loss: 0.00003576
Iteration 21/1000 | Loss: 0.00003572
Iteration 22/1000 | Loss: 0.00003566
Iteration 23/1000 | Loss: 0.00003559
Iteration 24/1000 | Loss: 0.00003551
Iteration 25/1000 | Loss: 0.00003546
Iteration 26/1000 | Loss: 0.00003545
Iteration 27/1000 | Loss: 0.00003544
Iteration 28/1000 | Loss: 0.00003543
Iteration 29/1000 | Loss: 0.00003543
Iteration 30/1000 | Loss: 0.00003541
Iteration 31/1000 | Loss: 0.00003531
Iteration 32/1000 | Loss: 0.00003527
Iteration 33/1000 | Loss: 0.00003526
Iteration 34/1000 | Loss: 0.00003526
Iteration 35/1000 | Loss: 0.00003524
Iteration 36/1000 | Loss: 0.00003523
Iteration 37/1000 | Loss: 0.00003523
Iteration 38/1000 | Loss: 0.00003523
Iteration 39/1000 | Loss: 0.00003522
Iteration 40/1000 | Loss: 0.00003522
Iteration 41/1000 | Loss: 0.00003520
Iteration 42/1000 | Loss: 0.00003520
Iteration 43/1000 | Loss: 0.00003519
Iteration 44/1000 | Loss: 0.00003519
Iteration 45/1000 | Loss: 0.00003519
Iteration 46/1000 | Loss: 0.00003517
Iteration 47/1000 | Loss: 0.00003517
Iteration 48/1000 | Loss: 0.00003517
Iteration 49/1000 | Loss: 0.00003516
Iteration 50/1000 | Loss: 0.00003516
Iteration 51/1000 | Loss: 0.00003515
Iteration 52/1000 | Loss: 0.00003515
Iteration 53/1000 | Loss: 0.00003514
Iteration 54/1000 | Loss: 0.00003514
Iteration 55/1000 | Loss: 0.00003514
Iteration 56/1000 | Loss: 0.00003514
Iteration 57/1000 | Loss: 0.00003513
Iteration 58/1000 | Loss: 0.00003513
Iteration 59/1000 | Loss: 0.00003513
Iteration 60/1000 | Loss: 0.00003513
Iteration 61/1000 | Loss: 0.00003513
Iteration 62/1000 | Loss: 0.00003513
Iteration 63/1000 | Loss: 0.00003512
Iteration 64/1000 | Loss: 0.00003512
Iteration 65/1000 | Loss: 0.00003510
Iteration 66/1000 | Loss: 0.00003509
Iteration 67/1000 | Loss: 0.00003509
Iteration 68/1000 | Loss: 0.00003509
Iteration 69/1000 | Loss: 0.00003509
Iteration 70/1000 | Loss: 0.00003509
Iteration 71/1000 | Loss: 0.00003509
Iteration 72/1000 | Loss: 0.00003509
Iteration 73/1000 | Loss: 0.00003509
Iteration 74/1000 | Loss: 0.00003509
Iteration 75/1000 | Loss: 0.00003509
Iteration 76/1000 | Loss: 0.00003509
Iteration 77/1000 | Loss: 0.00003509
Iteration 78/1000 | Loss: 0.00003508
Iteration 79/1000 | Loss: 0.00003508
Iteration 80/1000 | Loss: 0.00003508
Iteration 81/1000 | Loss: 0.00003508
Iteration 82/1000 | Loss: 0.00003507
Iteration 83/1000 | Loss: 0.00003506
Iteration 84/1000 | Loss: 0.00003506
Iteration 85/1000 | Loss: 0.00003506
Iteration 86/1000 | Loss: 0.00003505
Iteration 87/1000 | Loss: 0.00003505
Iteration 88/1000 | Loss: 0.00003505
Iteration 89/1000 | Loss: 0.00003504
Iteration 90/1000 | Loss: 0.00003504
Iteration 91/1000 | Loss: 0.00003504
Iteration 92/1000 | Loss: 0.00003503
Iteration 93/1000 | Loss: 0.00003503
Iteration 94/1000 | Loss: 0.00003503
Iteration 95/1000 | Loss: 0.00003503
Iteration 96/1000 | Loss: 0.00003503
Iteration 97/1000 | Loss: 0.00003502
Iteration 98/1000 | Loss: 0.00003502
Iteration 99/1000 | Loss: 0.00003502
Iteration 100/1000 | Loss: 0.00003502
Iteration 101/1000 | Loss: 0.00003502
Iteration 102/1000 | Loss: 0.00003501
Iteration 103/1000 | Loss: 0.00003501
Iteration 104/1000 | Loss: 0.00003501
Iteration 105/1000 | Loss: 0.00003500
Iteration 106/1000 | Loss: 0.00003500
Iteration 107/1000 | Loss: 0.00003499
Iteration 108/1000 | Loss: 0.00003499
Iteration 109/1000 | Loss: 0.00003499
Iteration 110/1000 | Loss: 0.00003499
Iteration 111/1000 | Loss: 0.00003498
Iteration 112/1000 | Loss: 0.00003498
Iteration 113/1000 | Loss: 0.00003498
Iteration 114/1000 | Loss: 0.00003498
Iteration 115/1000 | Loss: 0.00003498
Iteration 116/1000 | Loss: 0.00003497
Iteration 117/1000 | Loss: 0.00003497
Iteration 118/1000 | Loss: 0.00003497
Iteration 119/1000 | Loss: 0.00003497
Iteration 120/1000 | Loss: 0.00003497
Iteration 121/1000 | Loss: 0.00003497
Iteration 122/1000 | Loss: 0.00003497
Iteration 123/1000 | Loss: 0.00003496
Iteration 124/1000 | Loss: 0.00003496
Iteration 125/1000 | Loss: 0.00003496
Iteration 126/1000 | Loss: 0.00003496
Iteration 127/1000 | Loss: 0.00003496
Iteration 128/1000 | Loss: 0.00003496
Iteration 129/1000 | Loss: 0.00003496
Iteration 130/1000 | Loss: 0.00003496
Iteration 131/1000 | Loss: 0.00003496
Iteration 132/1000 | Loss: 0.00003496
Iteration 133/1000 | Loss: 0.00003495
Iteration 134/1000 | Loss: 0.00003495
Iteration 135/1000 | Loss: 0.00003495
Iteration 136/1000 | Loss: 0.00003495
Iteration 137/1000 | Loss: 0.00003495
Iteration 138/1000 | Loss: 0.00003495
Iteration 139/1000 | Loss: 0.00003495
Iteration 140/1000 | Loss: 0.00003495
Iteration 141/1000 | Loss: 0.00003495
Iteration 142/1000 | Loss: 0.00003495
Iteration 143/1000 | Loss: 0.00003494
Iteration 144/1000 | Loss: 0.00003494
Iteration 145/1000 | Loss: 0.00003494
Iteration 146/1000 | Loss: 0.00003494
Iteration 147/1000 | Loss: 0.00003494
Iteration 148/1000 | Loss: 0.00003494
Iteration 149/1000 | Loss: 0.00003494
Iteration 150/1000 | Loss: 0.00003494
Iteration 151/1000 | Loss: 0.00003494
Iteration 152/1000 | Loss: 0.00003494
Iteration 153/1000 | Loss: 0.00003494
Iteration 154/1000 | Loss: 0.00003494
Iteration 155/1000 | Loss: 0.00003494
Iteration 156/1000 | Loss: 0.00003493
Iteration 157/1000 | Loss: 0.00003493
Iteration 158/1000 | Loss: 0.00003493
Iteration 159/1000 | Loss: 0.00003493
Iteration 160/1000 | Loss: 0.00003493
Iteration 161/1000 | Loss: 0.00003493
Iteration 162/1000 | Loss: 0.00003492
Iteration 163/1000 | Loss: 0.00003492
Iteration 164/1000 | Loss: 0.00003492
Iteration 165/1000 | Loss: 0.00003492
Iteration 166/1000 | Loss: 0.00003492
Iteration 167/1000 | Loss: 0.00003492
Iteration 168/1000 | Loss: 0.00003492
Iteration 169/1000 | Loss: 0.00003492
Iteration 170/1000 | Loss: 0.00003492
Iteration 171/1000 | Loss: 0.00003492
Iteration 172/1000 | Loss: 0.00003492
Iteration 173/1000 | Loss: 0.00003492
Iteration 174/1000 | Loss: 0.00003491
Iteration 175/1000 | Loss: 0.00003491
Iteration 176/1000 | Loss: 0.00003491
Iteration 177/1000 | Loss: 0.00003491
Iteration 178/1000 | Loss: 0.00003491
Iteration 179/1000 | Loss: 0.00003491
Iteration 180/1000 | Loss: 0.00003491
Iteration 181/1000 | Loss: 0.00003491
Iteration 182/1000 | Loss: 0.00003491
Iteration 183/1000 | Loss: 0.00003491
Iteration 184/1000 | Loss: 0.00003491
Iteration 185/1000 | Loss: 0.00003491
Iteration 186/1000 | Loss: 0.00003490
Iteration 187/1000 | Loss: 0.00003490
Iteration 188/1000 | Loss: 0.00003490
Iteration 189/1000 | Loss: 0.00003490
Iteration 190/1000 | Loss: 0.00003490
Iteration 191/1000 | Loss: 0.00003490
Iteration 192/1000 | Loss: 0.00003490
Iteration 193/1000 | Loss: 0.00003490
Iteration 194/1000 | Loss: 0.00003489
Iteration 195/1000 | Loss: 0.00003489
Iteration 196/1000 | Loss: 0.00003489
Iteration 197/1000 | Loss: 0.00003489
Iteration 198/1000 | Loss: 0.00003489
Iteration 199/1000 | Loss: 0.00003489
Iteration 200/1000 | Loss: 0.00003489
Iteration 201/1000 | Loss: 0.00003489
Iteration 202/1000 | Loss: 0.00003489
Iteration 203/1000 | Loss: 0.00003489
Iteration 204/1000 | Loss: 0.00003489
Iteration 205/1000 | Loss: 0.00003488
Iteration 206/1000 | Loss: 0.00003488
Iteration 207/1000 | Loss: 0.00003488
Iteration 208/1000 | Loss: 0.00003488
Iteration 209/1000 | Loss: 0.00003488
Iteration 210/1000 | Loss: 0.00003488
Iteration 211/1000 | Loss: 0.00003488
Iteration 212/1000 | Loss: 0.00003488
Iteration 213/1000 | Loss: 0.00003488
Iteration 214/1000 | Loss: 0.00003488
Iteration 215/1000 | Loss: 0.00003488
Iteration 216/1000 | Loss: 0.00003487
Iteration 217/1000 | Loss: 0.00003487
Iteration 218/1000 | Loss: 0.00003487
Iteration 219/1000 | Loss: 0.00003487
Iteration 220/1000 | Loss: 0.00003487
Iteration 221/1000 | Loss: 0.00003487
Iteration 222/1000 | Loss: 0.00003487
Iteration 223/1000 | Loss: 0.00003487
Iteration 224/1000 | Loss: 0.00003487
Iteration 225/1000 | Loss: 0.00003487
Iteration 226/1000 | Loss: 0.00003487
Iteration 227/1000 | Loss: 0.00003487
Iteration 228/1000 | Loss: 0.00003487
Iteration 229/1000 | Loss: 0.00003486
Iteration 230/1000 | Loss: 0.00003486
Iteration 231/1000 | Loss: 0.00003486
Iteration 232/1000 | Loss: 0.00003486
Iteration 233/1000 | Loss: 0.00003486
Iteration 234/1000 | Loss: 0.00003486
Iteration 235/1000 | Loss: 0.00003486
Iteration 236/1000 | Loss: 0.00003486
Iteration 237/1000 | Loss: 0.00003486
Iteration 238/1000 | Loss: 0.00003486
Iteration 239/1000 | Loss: 0.00003486
Iteration 240/1000 | Loss: 0.00003486
Iteration 241/1000 | Loss: 0.00003486
Iteration 242/1000 | Loss: 0.00003486
Iteration 243/1000 | Loss: 0.00003486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [3.485776323941536e-05, 3.485776323941536e-05, 3.485776323941536e-05, 3.485776323941536e-05, 3.485776323941536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.485776323941536e-05

Optimization complete. Final v2v error: 4.686722278594971 mm

Highest mean error: 6.653797149658203 mm for frame 98

Lowest mean error: 2.6924760341644287 mm for frame 141

Saving results

Total time: 75.89974427223206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00362199
Iteration 2/25 | Loss: 0.00117878
Iteration 3/25 | Loss: 0.00098479
Iteration 4/25 | Loss: 0.00094965
Iteration 5/25 | Loss: 0.00094265
Iteration 6/25 | Loss: 0.00093954
Iteration 7/25 | Loss: 0.00093918
Iteration 8/25 | Loss: 0.00093918
Iteration 9/25 | Loss: 0.00093918
Iteration 10/25 | Loss: 0.00093918
Iteration 11/25 | Loss: 0.00093918
Iteration 12/25 | Loss: 0.00093918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000939180376008153, 0.000939180376008153, 0.000939180376008153, 0.000939180376008153, 0.000939180376008153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000939180376008153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31032372
Iteration 2/25 | Loss: 0.00086225
Iteration 3/25 | Loss: 0.00086225
Iteration 4/25 | Loss: 0.00086225
Iteration 5/25 | Loss: 0.00086225
Iteration 6/25 | Loss: 0.00086225
Iteration 7/25 | Loss: 0.00086224
Iteration 8/25 | Loss: 0.00086224
Iteration 9/25 | Loss: 0.00086224
Iteration 10/25 | Loss: 0.00086224
Iteration 11/25 | Loss: 0.00086224
Iteration 12/25 | Loss: 0.00086224
Iteration 13/25 | Loss: 0.00086224
Iteration 14/25 | Loss: 0.00086224
Iteration 15/25 | Loss: 0.00086224
Iteration 16/25 | Loss: 0.00086224
Iteration 17/25 | Loss: 0.00086224
Iteration 18/25 | Loss: 0.00086224
Iteration 19/25 | Loss: 0.00086224
Iteration 20/25 | Loss: 0.00086224
Iteration 21/25 | Loss: 0.00086224
Iteration 22/25 | Loss: 0.00086224
Iteration 23/25 | Loss: 0.00086224
Iteration 24/25 | Loss: 0.00086224
Iteration 25/25 | Loss: 0.00086224

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086224
Iteration 2/1000 | Loss: 0.00002028
Iteration 3/1000 | Loss: 0.00001334
Iteration 4/1000 | Loss: 0.00001156
Iteration 5/1000 | Loss: 0.00001058
Iteration 6/1000 | Loss: 0.00001015
Iteration 7/1000 | Loss: 0.00000983
Iteration 8/1000 | Loss: 0.00000964
Iteration 9/1000 | Loss: 0.00000951
Iteration 10/1000 | Loss: 0.00000947
Iteration 11/1000 | Loss: 0.00000946
Iteration 12/1000 | Loss: 0.00000944
Iteration 13/1000 | Loss: 0.00000944
Iteration 14/1000 | Loss: 0.00000936
Iteration 15/1000 | Loss: 0.00000932
Iteration 16/1000 | Loss: 0.00000927
Iteration 17/1000 | Loss: 0.00000925
Iteration 18/1000 | Loss: 0.00000923
Iteration 19/1000 | Loss: 0.00000923
Iteration 20/1000 | Loss: 0.00000923
Iteration 21/1000 | Loss: 0.00000922
Iteration 22/1000 | Loss: 0.00000922
Iteration 23/1000 | Loss: 0.00000921
Iteration 24/1000 | Loss: 0.00000920
Iteration 25/1000 | Loss: 0.00000919
Iteration 26/1000 | Loss: 0.00000919
Iteration 27/1000 | Loss: 0.00000918
Iteration 28/1000 | Loss: 0.00000917
Iteration 29/1000 | Loss: 0.00000917
Iteration 30/1000 | Loss: 0.00000913
Iteration 31/1000 | Loss: 0.00000913
Iteration 32/1000 | Loss: 0.00000913
Iteration 33/1000 | Loss: 0.00000912
Iteration 34/1000 | Loss: 0.00000911
Iteration 35/1000 | Loss: 0.00000910
Iteration 36/1000 | Loss: 0.00000909
Iteration 37/1000 | Loss: 0.00000909
Iteration 38/1000 | Loss: 0.00000908
Iteration 39/1000 | Loss: 0.00000908
Iteration 40/1000 | Loss: 0.00000908
Iteration 41/1000 | Loss: 0.00000908
Iteration 42/1000 | Loss: 0.00000908
Iteration 43/1000 | Loss: 0.00000907
Iteration 44/1000 | Loss: 0.00000907
Iteration 45/1000 | Loss: 0.00000907
Iteration 46/1000 | Loss: 0.00000907
Iteration 47/1000 | Loss: 0.00000907
Iteration 48/1000 | Loss: 0.00000907
Iteration 49/1000 | Loss: 0.00000906
Iteration 50/1000 | Loss: 0.00000906
Iteration 51/1000 | Loss: 0.00000906
Iteration 52/1000 | Loss: 0.00000905
Iteration 53/1000 | Loss: 0.00000905
Iteration 54/1000 | Loss: 0.00000905
Iteration 55/1000 | Loss: 0.00000905
Iteration 56/1000 | Loss: 0.00000905
Iteration 57/1000 | Loss: 0.00000905
Iteration 58/1000 | Loss: 0.00000905
Iteration 59/1000 | Loss: 0.00000904
Iteration 60/1000 | Loss: 0.00000904
Iteration 61/1000 | Loss: 0.00000904
Iteration 62/1000 | Loss: 0.00000904
Iteration 63/1000 | Loss: 0.00000904
Iteration 64/1000 | Loss: 0.00000904
Iteration 65/1000 | Loss: 0.00000904
Iteration 66/1000 | Loss: 0.00000903
Iteration 67/1000 | Loss: 0.00000903
Iteration 68/1000 | Loss: 0.00000903
Iteration 69/1000 | Loss: 0.00000903
Iteration 70/1000 | Loss: 0.00000903
Iteration 71/1000 | Loss: 0.00000902
Iteration 72/1000 | Loss: 0.00000902
Iteration 73/1000 | Loss: 0.00000902
Iteration 74/1000 | Loss: 0.00000902
Iteration 75/1000 | Loss: 0.00000902
Iteration 76/1000 | Loss: 0.00000902
Iteration 77/1000 | Loss: 0.00000902
Iteration 78/1000 | Loss: 0.00000902
Iteration 79/1000 | Loss: 0.00000902
Iteration 80/1000 | Loss: 0.00000902
Iteration 81/1000 | Loss: 0.00000902
Iteration 82/1000 | Loss: 0.00000902
Iteration 83/1000 | Loss: 0.00000901
Iteration 84/1000 | Loss: 0.00000901
Iteration 85/1000 | Loss: 0.00000901
Iteration 86/1000 | Loss: 0.00000901
Iteration 87/1000 | Loss: 0.00000901
Iteration 88/1000 | Loss: 0.00000901
Iteration 89/1000 | Loss: 0.00000901
Iteration 90/1000 | Loss: 0.00000900
Iteration 91/1000 | Loss: 0.00000900
Iteration 92/1000 | Loss: 0.00000900
Iteration 93/1000 | Loss: 0.00000900
Iteration 94/1000 | Loss: 0.00000899
Iteration 95/1000 | Loss: 0.00000899
Iteration 96/1000 | Loss: 0.00000899
Iteration 97/1000 | Loss: 0.00000899
Iteration 98/1000 | Loss: 0.00000899
Iteration 99/1000 | Loss: 0.00000899
Iteration 100/1000 | Loss: 0.00000899
Iteration 101/1000 | Loss: 0.00000899
Iteration 102/1000 | Loss: 0.00000899
Iteration 103/1000 | Loss: 0.00000899
Iteration 104/1000 | Loss: 0.00000899
Iteration 105/1000 | Loss: 0.00000899
Iteration 106/1000 | Loss: 0.00000899
Iteration 107/1000 | Loss: 0.00000898
Iteration 108/1000 | Loss: 0.00000898
Iteration 109/1000 | Loss: 0.00000898
Iteration 110/1000 | Loss: 0.00000898
Iteration 111/1000 | Loss: 0.00000898
Iteration 112/1000 | Loss: 0.00000898
Iteration 113/1000 | Loss: 0.00000897
Iteration 114/1000 | Loss: 0.00000897
Iteration 115/1000 | Loss: 0.00000897
Iteration 116/1000 | Loss: 0.00000897
Iteration 117/1000 | Loss: 0.00000897
Iteration 118/1000 | Loss: 0.00000897
Iteration 119/1000 | Loss: 0.00000897
Iteration 120/1000 | Loss: 0.00000897
Iteration 121/1000 | Loss: 0.00000897
Iteration 122/1000 | Loss: 0.00000897
Iteration 123/1000 | Loss: 0.00000896
Iteration 124/1000 | Loss: 0.00000896
Iteration 125/1000 | Loss: 0.00000896
Iteration 126/1000 | Loss: 0.00000896
Iteration 127/1000 | Loss: 0.00000896
Iteration 128/1000 | Loss: 0.00000896
Iteration 129/1000 | Loss: 0.00000895
Iteration 130/1000 | Loss: 0.00000895
Iteration 131/1000 | Loss: 0.00000895
Iteration 132/1000 | Loss: 0.00000895
Iteration 133/1000 | Loss: 0.00000895
Iteration 134/1000 | Loss: 0.00000895
Iteration 135/1000 | Loss: 0.00000895
Iteration 136/1000 | Loss: 0.00000895
Iteration 137/1000 | Loss: 0.00000895
Iteration 138/1000 | Loss: 0.00000895
Iteration 139/1000 | Loss: 0.00000895
Iteration 140/1000 | Loss: 0.00000895
Iteration 141/1000 | Loss: 0.00000895
Iteration 142/1000 | Loss: 0.00000895
Iteration 143/1000 | Loss: 0.00000895
Iteration 144/1000 | Loss: 0.00000895
Iteration 145/1000 | Loss: 0.00000895
Iteration 146/1000 | Loss: 0.00000895
Iteration 147/1000 | Loss: 0.00000895
Iteration 148/1000 | Loss: 0.00000895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [8.950377377914265e-06, 8.950377377914265e-06, 8.950377377914265e-06, 8.950377377914265e-06, 8.950377377914265e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.950377377914265e-06

Optimization complete. Final v2v error: 2.4663217067718506 mm

Highest mean error: 2.5941481590270996 mm for frame 70

Lowest mean error: 2.1396265029907227 mm for frame 5

Saving results

Total time: 41.019636392593384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047733
Iteration 2/25 | Loss: 0.00130610
Iteration 3/25 | Loss: 0.00113146
Iteration 4/25 | Loss: 0.00102318
Iteration 5/25 | Loss: 0.00100787
Iteration 6/25 | Loss: 0.00100409
Iteration 7/25 | Loss: 0.00099562
Iteration 8/25 | Loss: 0.00099103
Iteration 9/25 | Loss: 0.00098932
Iteration 10/25 | Loss: 0.00099368
Iteration 11/25 | Loss: 0.00099136
Iteration 12/25 | Loss: 0.00098730
Iteration 13/25 | Loss: 0.00098668
Iteration 14/25 | Loss: 0.00098867
Iteration 15/25 | Loss: 0.00099017
Iteration 16/25 | Loss: 0.00098900
Iteration 17/25 | Loss: 0.00099131
Iteration 18/25 | Loss: 0.00098850
Iteration 19/25 | Loss: 0.00098684
Iteration 20/25 | Loss: 0.00098598
Iteration 21/25 | Loss: 0.00098542
Iteration 22/25 | Loss: 0.00098511
Iteration 23/25 | Loss: 0.00098501
Iteration 24/25 | Loss: 0.00098500
Iteration 25/25 | Loss: 0.00098500

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.12762952
Iteration 2/25 | Loss: 0.00109118
Iteration 3/25 | Loss: 0.00109118
Iteration 4/25 | Loss: 0.00109118
Iteration 5/25 | Loss: 0.00109118
Iteration 6/25 | Loss: 0.00109118
Iteration 7/25 | Loss: 0.00109118
Iteration 8/25 | Loss: 0.00109118
Iteration 9/25 | Loss: 0.00109118
Iteration 10/25 | Loss: 0.00109118
Iteration 11/25 | Loss: 0.00109118
Iteration 12/25 | Loss: 0.00109118
Iteration 13/25 | Loss: 0.00109118
Iteration 14/25 | Loss: 0.00109118
Iteration 15/25 | Loss: 0.00109118
Iteration 16/25 | Loss: 0.00109118
Iteration 17/25 | Loss: 0.00109118
Iteration 18/25 | Loss: 0.00109118
Iteration 19/25 | Loss: 0.00109118
Iteration 20/25 | Loss: 0.00109118
Iteration 21/25 | Loss: 0.00109118
Iteration 22/25 | Loss: 0.00109118
Iteration 23/25 | Loss: 0.00109118
Iteration 24/25 | Loss: 0.00109118
Iteration 25/25 | Loss: 0.00109118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109118
Iteration 2/1000 | Loss: 0.00003712
Iteration 3/1000 | Loss: 0.00002357
Iteration 4/1000 | Loss: 0.00003136
Iteration 5/1000 | Loss: 0.00001908
Iteration 6/1000 | Loss: 0.00002685
Iteration 7/1000 | Loss: 0.00002436
Iteration 8/1000 | Loss: 0.00002985
Iteration 9/1000 | Loss: 0.00003005
Iteration 10/1000 | Loss: 0.00002994
Iteration 11/1000 | Loss: 0.00002921
Iteration 12/1000 | Loss: 0.00002973
Iteration 13/1000 | Loss: 0.00002970
Iteration 14/1000 | Loss: 0.00002981
Iteration 15/1000 | Loss: 0.00001904
Iteration 16/1000 | Loss: 0.00003150
Iteration 17/1000 | Loss: 0.00001802
Iteration 18/1000 | Loss: 0.00002394
Iteration 19/1000 | Loss: 0.00002647
Iteration 20/1000 | Loss: 0.00002937
Iteration 21/1000 | Loss: 0.00002622
Iteration 22/1000 | Loss: 0.00002717
Iteration 23/1000 | Loss: 0.00002638
Iteration 24/1000 | Loss: 0.00002648
Iteration 25/1000 | Loss: 0.00002596
Iteration 26/1000 | Loss: 0.00002576
Iteration 27/1000 | Loss: 0.00002724
Iteration 28/1000 | Loss: 0.00002514
Iteration 29/1000 | Loss: 0.00002776
Iteration 30/1000 | Loss: 0.00004030
Iteration 31/1000 | Loss: 0.00002482
Iteration 32/1000 | Loss: 0.00002805
Iteration 33/1000 | Loss: 0.00002548
Iteration 34/1000 | Loss: 0.00002775
Iteration 35/1000 | Loss: 0.00002443
Iteration 36/1000 | Loss: 0.00002705
Iteration 37/1000 | Loss: 0.00002328
Iteration 38/1000 | Loss: 0.00001871
Iteration 39/1000 | Loss: 0.00002581
Iteration 40/1000 | Loss: 0.00003435
Iteration 41/1000 | Loss: 0.00001545
Iteration 42/1000 | Loss: 0.00001388
Iteration 43/1000 | Loss: 0.00001303
Iteration 44/1000 | Loss: 0.00001256
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001233
Iteration 47/1000 | Loss: 0.00001226
Iteration 48/1000 | Loss: 0.00001212
Iteration 49/1000 | Loss: 0.00001209
Iteration 50/1000 | Loss: 0.00001207
Iteration 51/1000 | Loss: 0.00001205
Iteration 52/1000 | Loss: 0.00001204
Iteration 53/1000 | Loss: 0.00001204
Iteration 54/1000 | Loss: 0.00001199
Iteration 55/1000 | Loss: 0.00001199
Iteration 56/1000 | Loss: 0.00001198
Iteration 57/1000 | Loss: 0.00001197
Iteration 58/1000 | Loss: 0.00001197
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001196
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001196
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001195
Iteration 65/1000 | Loss: 0.00001195
Iteration 66/1000 | Loss: 0.00001194
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001193
Iteration 70/1000 | Loss: 0.00001192
Iteration 71/1000 | Loss: 0.00001192
Iteration 72/1000 | Loss: 0.00001192
Iteration 73/1000 | Loss: 0.00001192
Iteration 74/1000 | Loss: 0.00001192
Iteration 75/1000 | Loss: 0.00001192
Iteration 76/1000 | Loss: 0.00001191
Iteration 77/1000 | Loss: 0.00001191
Iteration 78/1000 | Loss: 0.00001191
Iteration 79/1000 | Loss: 0.00001191
Iteration 80/1000 | Loss: 0.00001190
Iteration 81/1000 | Loss: 0.00001190
Iteration 82/1000 | Loss: 0.00001190
Iteration 83/1000 | Loss: 0.00001189
Iteration 84/1000 | Loss: 0.00001188
Iteration 85/1000 | Loss: 0.00001188
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001188
Iteration 88/1000 | Loss: 0.00001188
Iteration 89/1000 | Loss: 0.00001188
Iteration 90/1000 | Loss: 0.00001188
Iteration 91/1000 | Loss: 0.00001188
Iteration 92/1000 | Loss: 0.00001188
Iteration 93/1000 | Loss: 0.00001188
Iteration 94/1000 | Loss: 0.00001188
Iteration 95/1000 | Loss: 0.00001187
Iteration 96/1000 | Loss: 0.00001187
Iteration 97/1000 | Loss: 0.00001187
Iteration 98/1000 | Loss: 0.00001186
Iteration 99/1000 | Loss: 0.00001186
Iteration 100/1000 | Loss: 0.00001186
Iteration 101/1000 | Loss: 0.00001186
Iteration 102/1000 | Loss: 0.00001186
Iteration 103/1000 | Loss: 0.00001185
Iteration 104/1000 | Loss: 0.00001185
Iteration 105/1000 | Loss: 0.00001185
Iteration 106/1000 | Loss: 0.00001185
Iteration 107/1000 | Loss: 0.00001185
Iteration 108/1000 | Loss: 0.00001185
Iteration 109/1000 | Loss: 0.00001185
Iteration 110/1000 | Loss: 0.00001185
Iteration 111/1000 | Loss: 0.00001185
Iteration 112/1000 | Loss: 0.00001185
Iteration 113/1000 | Loss: 0.00001184
Iteration 114/1000 | Loss: 0.00001184
Iteration 115/1000 | Loss: 0.00001184
Iteration 116/1000 | Loss: 0.00001184
Iteration 117/1000 | Loss: 0.00001184
Iteration 118/1000 | Loss: 0.00001184
Iteration 119/1000 | Loss: 0.00001184
Iteration 120/1000 | Loss: 0.00001184
Iteration 121/1000 | Loss: 0.00001184
Iteration 122/1000 | Loss: 0.00001183
Iteration 123/1000 | Loss: 0.00001183
Iteration 124/1000 | Loss: 0.00001183
Iteration 125/1000 | Loss: 0.00001183
Iteration 126/1000 | Loss: 0.00001183
Iteration 127/1000 | Loss: 0.00001182
Iteration 128/1000 | Loss: 0.00001182
Iteration 129/1000 | Loss: 0.00001182
Iteration 130/1000 | Loss: 0.00001182
Iteration 131/1000 | Loss: 0.00001182
Iteration 132/1000 | Loss: 0.00001182
Iteration 133/1000 | Loss: 0.00001182
Iteration 134/1000 | Loss: 0.00001182
Iteration 135/1000 | Loss: 0.00001181
Iteration 136/1000 | Loss: 0.00001181
Iteration 137/1000 | Loss: 0.00001181
Iteration 138/1000 | Loss: 0.00001181
Iteration 139/1000 | Loss: 0.00001180
Iteration 140/1000 | Loss: 0.00001179
Iteration 141/1000 | Loss: 0.00001179
Iteration 142/1000 | Loss: 0.00001179
Iteration 143/1000 | Loss: 0.00001178
Iteration 144/1000 | Loss: 0.00001178
Iteration 145/1000 | Loss: 0.00001178
Iteration 146/1000 | Loss: 0.00001177
Iteration 147/1000 | Loss: 0.00001177
Iteration 148/1000 | Loss: 0.00001177
Iteration 149/1000 | Loss: 0.00001176
Iteration 150/1000 | Loss: 0.00001176
Iteration 151/1000 | Loss: 0.00001176
Iteration 152/1000 | Loss: 0.00001176
Iteration 153/1000 | Loss: 0.00001176
Iteration 154/1000 | Loss: 0.00001176
Iteration 155/1000 | Loss: 0.00001176
Iteration 156/1000 | Loss: 0.00001176
Iteration 157/1000 | Loss: 0.00001175
Iteration 158/1000 | Loss: 0.00001175
Iteration 159/1000 | Loss: 0.00001175
Iteration 160/1000 | Loss: 0.00001175
Iteration 161/1000 | Loss: 0.00001175
Iteration 162/1000 | Loss: 0.00001175
Iteration 163/1000 | Loss: 0.00001175
Iteration 164/1000 | Loss: 0.00001175
Iteration 165/1000 | Loss: 0.00001175
Iteration 166/1000 | Loss: 0.00001175
Iteration 167/1000 | Loss: 0.00001175
Iteration 168/1000 | Loss: 0.00001175
Iteration 169/1000 | Loss: 0.00001175
Iteration 170/1000 | Loss: 0.00001175
Iteration 171/1000 | Loss: 0.00001175
Iteration 172/1000 | Loss: 0.00001175
Iteration 173/1000 | Loss: 0.00001175
Iteration 174/1000 | Loss: 0.00001174
Iteration 175/1000 | Loss: 0.00001174
Iteration 176/1000 | Loss: 0.00001174
Iteration 177/1000 | Loss: 0.00001174
Iteration 178/1000 | Loss: 0.00001173
Iteration 179/1000 | Loss: 0.00001173
Iteration 180/1000 | Loss: 0.00001173
Iteration 181/1000 | Loss: 0.00001173
Iteration 182/1000 | Loss: 0.00001172
Iteration 183/1000 | Loss: 0.00001172
Iteration 184/1000 | Loss: 0.00001172
Iteration 185/1000 | Loss: 0.00001172
Iteration 186/1000 | Loss: 0.00001172
Iteration 187/1000 | Loss: 0.00001172
Iteration 188/1000 | Loss: 0.00001172
Iteration 189/1000 | Loss: 0.00001172
Iteration 190/1000 | Loss: 0.00001172
Iteration 191/1000 | Loss: 0.00001172
Iteration 192/1000 | Loss: 0.00001172
Iteration 193/1000 | Loss: 0.00001172
Iteration 194/1000 | Loss: 0.00001172
Iteration 195/1000 | Loss: 0.00001172
Iteration 196/1000 | Loss: 0.00001172
Iteration 197/1000 | Loss: 0.00001172
Iteration 198/1000 | Loss: 0.00001172
Iteration 199/1000 | Loss: 0.00001172
Iteration 200/1000 | Loss: 0.00001172
Iteration 201/1000 | Loss: 0.00001172
Iteration 202/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.1715560503944289e-05, 1.1715560503944289e-05, 1.1715560503944289e-05, 1.1715560503944289e-05, 1.1715560503944289e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1715560503944289e-05

Optimization complete. Final v2v error: 2.828843116760254 mm

Highest mean error: 4.327789783477783 mm for frame 141

Lowest mean error: 2.4096009731292725 mm for frame 10

Saving results

Total time: 125.82712626457214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527334
Iteration 2/25 | Loss: 0.00129610
Iteration 3/25 | Loss: 0.00106488
Iteration 4/25 | Loss: 0.00105588
Iteration 5/25 | Loss: 0.00105407
Iteration 6/25 | Loss: 0.00105407
Iteration 7/25 | Loss: 0.00105407
Iteration 8/25 | Loss: 0.00105407
Iteration 9/25 | Loss: 0.00105407
Iteration 10/25 | Loss: 0.00105407
Iteration 11/25 | Loss: 0.00105407
Iteration 12/25 | Loss: 0.00105407
Iteration 13/25 | Loss: 0.00105407
Iteration 14/25 | Loss: 0.00105407
Iteration 15/25 | Loss: 0.00105407
Iteration 16/25 | Loss: 0.00105407
Iteration 17/25 | Loss: 0.00105407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010540690273046494, 0.0010540690273046494, 0.0010540690273046494, 0.0010540690273046494, 0.0010540690273046494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010540690273046494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75111300
Iteration 2/25 | Loss: 0.00088663
Iteration 3/25 | Loss: 0.00088662
Iteration 4/25 | Loss: 0.00088662
Iteration 5/25 | Loss: 0.00088662
Iteration 6/25 | Loss: 0.00088662
Iteration 7/25 | Loss: 0.00088662
Iteration 8/25 | Loss: 0.00088662
Iteration 9/25 | Loss: 0.00088662
Iteration 10/25 | Loss: 0.00088662
Iteration 11/25 | Loss: 0.00088662
Iteration 12/25 | Loss: 0.00088662
Iteration 13/25 | Loss: 0.00088662
Iteration 14/25 | Loss: 0.00088662
Iteration 15/25 | Loss: 0.00088662
Iteration 16/25 | Loss: 0.00088662
Iteration 17/25 | Loss: 0.00088662
Iteration 18/25 | Loss: 0.00088662
Iteration 19/25 | Loss: 0.00088662
Iteration 20/25 | Loss: 0.00088662
Iteration 21/25 | Loss: 0.00088662
Iteration 22/25 | Loss: 0.00088662
Iteration 23/25 | Loss: 0.00088662
Iteration 24/25 | Loss: 0.00088662
Iteration 25/25 | Loss: 0.00088662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088662
Iteration 2/1000 | Loss: 0.00004004
Iteration 3/1000 | Loss: 0.00002525
Iteration 4/1000 | Loss: 0.00002254
Iteration 5/1000 | Loss: 0.00002108
Iteration 6/1000 | Loss: 0.00002042
Iteration 7/1000 | Loss: 0.00002002
Iteration 8/1000 | Loss: 0.00001956
Iteration 9/1000 | Loss: 0.00001923
Iteration 10/1000 | Loss: 0.00001890
Iteration 11/1000 | Loss: 0.00001868
Iteration 12/1000 | Loss: 0.00001847
Iteration 13/1000 | Loss: 0.00001842
Iteration 14/1000 | Loss: 0.00001835
Iteration 15/1000 | Loss: 0.00001824
Iteration 16/1000 | Loss: 0.00001815
Iteration 17/1000 | Loss: 0.00001809
Iteration 18/1000 | Loss: 0.00001796
Iteration 19/1000 | Loss: 0.00001788
Iteration 20/1000 | Loss: 0.00001777
Iteration 21/1000 | Loss: 0.00001768
Iteration 22/1000 | Loss: 0.00001761
Iteration 23/1000 | Loss: 0.00001744
Iteration 24/1000 | Loss: 0.00001737
Iteration 25/1000 | Loss: 0.00001724
Iteration 26/1000 | Loss: 0.00001711
Iteration 27/1000 | Loss: 0.00001699
Iteration 28/1000 | Loss: 0.00001695
Iteration 29/1000 | Loss: 0.00001694
Iteration 30/1000 | Loss: 0.00001694
Iteration 31/1000 | Loss: 0.00001693
Iteration 32/1000 | Loss: 0.00001693
Iteration 33/1000 | Loss: 0.00001693
Iteration 34/1000 | Loss: 0.00001693
Iteration 35/1000 | Loss: 0.00001693
Iteration 36/1000 | Loss: 0.00001693
Iteration 37/1000 | Loss: 0.00001693
Iteration 38/1000 | Loss: 0.00001693
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001691
Iteration 42/1000 | Loss: 0.00001689
Iteration 43/1000 | Loss: 0.00001689
Iteration 44/1000 | Loss: 0.00001688
Iteration 45/1000 | Loss: 0.00001688
Iteration 46/1000 | Loss: 0.00001687
Iteration 47/1000 | Loss: 0.00001687
Iteration 48/1000 | Loss: 0.00001683
Iteration 49/1000 | Loss: 0.00001683
Iteration 50/1000 | Loss: 0.00001682
Iteration 51/1000 | Loss: 0.00001682
Iteration 52/1000 | Loss: 0.00001680
Iteration 53/1000 | Loss: 0.00001680
Iteration 54/1000 | Loss: 0.00001677
Iteration 55/1000 | Loss: 0.00001677
Iteration 56/1000 | Loss: 0.00001677
Iteration 57/1000 | Loss: 0.00001676
Iteration 58/1000 | Loss: 0.00001675
Iteration 59/1000 | Loss: 0.00001675
Iteration 60/1000 | Loss: 0.00001675
Iteration 61/1000 | Loss: 0.00001674
Iteration 62/1000 | Loss: 0.00001674
Iteration 63/1000 | Loss: 0.00001673
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001673
Iteration 66/1000 | Loss: 0.00001673
Iteration 67/1000 | Loss: 0.00001673
Iteration 68/1000 | Loss: 0.00001673
Iteration 69/1000 | Loss: 0.00001673
Iteration 70/1000 | Loss: 0.00001673
Iteration 71/1000 | Loss: 0.00001672
Iteration 72/1000 | Loss: 0.00001672
Iteration 73/1000 | Loss: 0.00001672
Iteration 74/1000 | Loss: 0.00001672
Iteration 75/1000 | Loss: 0.00001670
Iteration 76/1000 | Loss: 0.00001670
Iteration 77/1000 | Loss: 0.00001670
Iteration 78/1000 | Loss: 0.00001669
Iteration 79/1000 | Loss: 0.00001669
Iteration 80/1000 | Loss: 0.00001669
Iteration 81/1000 | Loss: 0.00001669
Iteration 82/1000 | Loss: 0.00001669
Iteration 83/1000 | Loss: 0.00001669
Iteration 84/1000 | Loss: 0.00001669
Iteration 85/1000 | Loss: 0.00001669
Iteration 86/1000 | Loss: 0.00001668
Iteration 87/1000 | Loss: 0.00001668
Iteration 88/1000 | Loss: 0.00001668
Iteration 89/1000 | Loss: 0.00001668
Iteration 90/1000 | Loss: 0.00001668
Iteration 91/1000 | Loss: 0.00001667
Iteration 92/1000 | Loss: 0.00001667
Iteration 93/1000 | Loss: 0.00001667
Iteration 94/1000 | Loss: 0.00001667
Iteration 95/1000 | Loss: 0.00001666
Iteration 96/1000 | Loss: 0.00001666
Iteration 97/1000 | Loss: 0.00001666
Iteration 98/1000 | Loss: 0.00001666
Iteration 99/1000 | Loss: 0.00001665
Iteration 100/1000 | Loss: 0.00001665
Iteration 101/1000 | Loss: 0.00001665
Iteration 102/1000 | Loss: 0.00001665
Iteration 103/1000 | Loss: 0.00001665
Iteration 104/1000 | Loss: 0.00001665
Iteration 105/1000 | Loss: 0.00001665
Iteration 106/1000 | Loss: 0.00001665
Iteration 107/1000 | Loss: 0.00001664
Iteration 108/1000 | Loss: 0.00001664
Iteration 109/1000 | Loss: 0.00001664
Iteration 110/1000 | Loss: 0.00001664
Iteration 111/1000 | Loss: 0.00001664
Iteration 112/1000 | Loss: 0.00001664
Iteration 113/1000 | Loss: 0.00001664
Iteration 114/1000 | Loss: 0.00001664
Iteration 115/1000 | Loss: 0.00001664
Iteration 116/1000 | Loss: 0.00001663
Iteration 117/1000 | Loss: 0.00001663
Iteration 118/1000 | Loss: 0.00001663
Iteration 119/1000 | Loss: 0.00001663
Iteration 120/1000 | Loss: 0.00001663
Iteration 121/1000 | Loss: 0.00001663
Iteration 122/1000 | Loss: 0.00001663
Iteration 123/1000 | Loss: 0.00001663
Iteration 124/1000 | Loss: 0.00001663
Iteration 125/1000 | Loss: 0.00001663
Iteration 126/1000 | Loss: 0.00001662
Iteration 127/1000 | Loss: 0.00001662
Iteration 128/1000 | Loss: 0.00001662
Iteration 129/1000 | Loss: 0.00001662
Iteration 130/1000 | Loss: 0.00001662
Iteration 131/1000 | Loss: 0.00001662
Iteration 132/1000 | Loss: 0.00001662
Iteration 133/1000 | Loss: 0.00001662
Iteration 134/1000 | Loss: 0.00001662
Iteration 135/1000 | Loss: 0.00001662
Iteration 136/1000 | Loss: 0.00001662
Iteration 137/1000 | Loss: 0.00001662
Iteration 138/1000 | Loss: 0.00001662
Iteration 139/1000 | Loss: 0.00001662
Iteration 140/1000 | Loss: 0.00001662
Iteration 141/1000 | Loss: 0.00001662
Iteration 142/1000 | Loss: 0.00001662
Iteration 143/1000 | Loss: 0.00001662
Iteration 144/1000 | Loss: 0.00001662
Iteration 145/1000 | Loss: 0.00001662
Iteration 146/1000 | Loss: 0.00001662
Iteration 147/1000 | Loss: 0.00001662
Iteration 148/1000 | Loss: 0.00001662
Iteration 149/1000 | Loss: 0.00001662
Iteration 150/1000 | Loss: 0.00001662
Iteration 151/1000 | Loss: 0.00001662
Iteration 152/1000 | Loss: 0.00001662
Iteration 153/1000 | Loss: 0.00001662
Iteration 154/1000 | Loss: 0.00001662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.6619265807094052e-05, 1.6619265807094052e-05, 1.6619265807094052e-05, 1.6619265807094052e-05, 1.6619265807094052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6619265807094052e-05

Optimization complete. Final v2v error: 3.4608092308044434 mm

Highest mean error: 3.713679790496826 mm for frame 205

Lowest mean error: 3.1601383686065674 mm for frame 20

Saving results

Total time: 56.58130431175232
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_1198/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_1198/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454675
Iteration 2/25 | Loss: 0.00124793
Iteration 3/25 | Loss: 0.00100585
Iteration 4/25 | Loss: 0.00098389
Iteration 5/25 | Loss: 0.00098029
Iteration 6/25 | Loss: 0.00097919
Iteration 7/25 | Loss: 0.00097904
Iteration 8/25 | Loss: 0.00097904
Iteration 9/25 | Loss: 0.00097904
Iteration 10/25 | Loss: 0.00097904
Iteration 11/25 | Loss: 0.00097904
Iteration 12/25 | Loss: 0.00097904
Iteration 13/25 | Loss: 0.00097904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009790431940928102, 0.0009790431940928102, 0.0009790431940928102, 0.0009790431940928102, 0.0009790431940928102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009790431940928102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45297897
Iteration 2/25 | Loss: 0.00066741
Iteration 3/25 | Loss: 0.00066740
Iteration 4/25 | Loss: 0.00066740
Iteration 5/25 | Loss: 0.00066740
Iteration 6/25 | Loss: 0.00066740
Iteration 7/25 | Loss: 0.00066740
Iteration 8/25 | Loss: 0.00066740
Iteration 9/25 | Loss: 0.00066740
Iteration 10/25 | Loss: 0.00066739
Iteration 11/25 | Loss: 0.00066739
Iteration 12/25 | Loss: 0.00066739
Iteration 13/25 | Loss: 0.00066739
Iteration 14/25 | Loss: 0.00066739
Iteration 15/25 | Loss: 0.00066739
Iteration 16/25 | Loss: 0.00066739
Iteration 17/25 | Loss: 0.00066739
Iteration 18/25 | Loss: 0.00066739
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006673947209492326, 0.0006673947209492326, 0.0006673947209492326, 0.0006673947209492326, 0.0006673947209492326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006673947209492326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066739
Iteration 2/1000 | Loss: 0.00004961
Iteration 3/1000 | Loss: 0.00002457
Iteration 4/1000 | Loss: 0.00001813
Iteration 5/1000 | Loss: 0.00001613
Iteration 6/1000 | Loss: 0.00001519
Iteration 7/1000 | Loss: 0.00001470
Iteration 8/1000 | Loss: 0.00001434
Iteration 9/1000 | Loss: 0.00001403
Iteration 10/1000 | Loss: 0.00001382
Iteration 11/1000 | Loss: 0.00001374
Iteration 12/1000 | Loss: 0.00001369
Iteration 13/1000 | Loss: 0.00001368
Iteration 14/1000 | Loss: 0.00001366
Iteration 15/1000 | Loss: 0.00001365
Iteration 16/1000 | Loss: 0.00001359
Iteration 17/1000 | Loss: 0.00001356
Iteration 18/1000 | Loss: 0.00001356
Iteration 19/1000 | Loss: 0.00001355
Iteration 20/1000 | Loss: 0.00001354
Iteration 21/1000 | Loss: 0.00001353
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001352
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001347
Iteration 26/1000 | Loss: 0.00001344
Iteration 27/1000 | Loss: 0.00001342
Iteration 28/1000 | Loss: 0.00001341
Iteration 29/1000 | Loss: 0.00001341
Iteration 30/1000 | Loss: 0.00001341
Iteration 31/1000 | Loss: 0.00001341
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001341
Iteration 34/1000 | Loss: 0.00001340
Iteration 35/1000 | Loss: 0.00001340
Iteration 36/1000 | Loss: 0.00001340
Iteration 37/1000 | Loss: 0.00001340
Iteration 38/1000 | Loss: 0.00001340
Iteration 39/1000 | Loss: 0.00001340
Iteration 40/1000 | Loss: 0.00001340
Iteration 41/1000 | Loss: 0.00001339
Iteration 42/1000 | Loss: 0.00001339
Iteration 43/1000 | Loss: 0.00001338
Iteration 44/1000 | Loss: 0.00001337
Iteration 45/1000 | Loss: 0.00001337
Iteration 46/1000 | Loss: 0.00001337
Iteration 47/1000 | Loss: 0.00001336
Iteration 48/1000 | Loss: 0.00001336
Iteration 49/1000 | Loss: 0.00001336
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001335
Iteration 55/1000 | Loss: 0.00001335
Iteration 56/1000 | Loss: 0.00001335
Iteration 57/1000 | Loss: 0.00001331
Iteration 58/1000 | Loss: 0.00001331
Iteration 59/1000 | Loss: 0.00001326
Iteration 60/1000 | Loss: 0.00001326
Iteration 61/1000 | Loss: 0.00001326
Iteration 62/1000 | Loss: 0.00001326
Iteration 63/1000 | Loss: 0.00001326
Iteration 64/1000 | Loss: 0.00001326
Iteration 65/1000 | Loss: 0.00001326
Iteration 66/1000 | Loss: 0.00001326
Iteration 67/1000 | Loss: 0.00001325
Iteration 68/1000 | Loss: 0.00001324
Iteration 69/1000 | Loss: 0.00001324
Iteration 70/1000 | Loss: 0.00001324
Iteration 71/1000 | Loss: 0.00001324
Iteration 72/1000 | Loss: 0.00001323
Iteration 73/1000 | Loss: 0.00001323
Iteration 74/1000 | Loss: 0.00001323
Iteration 75/1000 | Loss: 0.00001323
Iteration 76/1000 | Loss: 0.00001323
Iteration 77/1000 | Loss: 0.00001323
Iteration 78/1000 | Loss: 0.00001323
Iteration 79/1000 | Loss: 0.00001323
Iteration 80/1000 | Loss: 0.00001323
Iteration 81/1000 | Loss: 0.00001323
Iteration 82/1000 | Loss: 0.00001323
Iteration 83/1000 | Loss: 0.00001323
Iteration 84/1000 | Loss: 0.00001323
Iteration 85/1000 | Loss: 0.00001322
Iteration 86/1000 | Loss: 0.00001322
Iteration 87/1000 | Loss: 0.00001322
Iteration 88/1000 | Loss: 0.00001322
Iteration 89/1000 | Loss: 0.00001322
Iteration 90/1000 | Loss: 0.00001322
Iteration 91/1000 | Loss: 0.00001322
Iteration 92/1000 | Loss: 0.00001322
Iteration 93/1000 | Loss: 0.00001322
Iteration 94/1000 | Loss: 0.00001322
Iteration 95/1000 | Loss: 0.00001322
Iteration 96/1000 | Loss: 0.00001322
Iteration 97/1000 | Loss: 0.00001321
Iteration 98/1000 | Loss: 0.00001321
Iteration 99/1000 | Loss: 0.00001321
Iteration 100/1000 | Loss: 0.00001320
Iteration 101/1000 | Loss: 0.00001320
Iteration 102/1000 | Loss: 0.00001320
Iteration 103/1000 | Loss: 0.00001320
Iteration 104/1000 | Loss: 0.00001320
Iteration 105/1000 | Loss: 0.00001319
Iteration 106/1000 | Loss: 0.00001319
Iteration 107/1000 | Loss: 0.00001319
Iteration 108/1000 | Loss: 0.00001318
Iteration 109/1000 | Loss: 0.00001318
Iteration 110/1000 | Loss: 0.00001317
Iteration 111/1000 | Loss: 0.00001317
Iteration 112/1000 | Loss: 0.00001316
Iteration 113/1000 | Loss: 0.00001316
Iteration 114/1000 | Loss: 0.00001316
Iteration 115/1000 | Loss: 0.00001316
Iteration 116/1000 | Loss: 0.00001316
Iteration 117/1000 | Loss: 0.00001316
Iteration 118/1000 | Loss: 0.00001316
Iteration 119/1000 | Loss: 0.00001316
Iteration 120/1000 | Loss: 0.00001316
Iteration 121/1000 | Loss: 0.00001316
Iteration 122/1000 | Loss: 0.00001316
Iteration 123/1000 | Loss: 0.00001316
Iteration 124/1000 | Loss: 0.00001315
Iteration 125/1000 | Loss: 0.00001315
Iteration 126/1000 | Loss: 0.00001315
Iteration 127/1000 | Loss: 0.00001314
Iteration 128/1000 | Loss: 0.00001314
Iteration 129/1000 | Loss: 0.00001314
Iteration 130/1000 | Loss: 0.00001314
Iteration 131/1000 | Loss: 0.00001313
Iteration 132/1000 | Loss: 0.00001313
Iteration 133/1000 | Loss: 0.00001313
Iteration 134/1000 | Loss: 0.00001313
Iteration 135/1000 | Loss: 0.00001313
Iteration 136/1000 | Loss: 0.00001313
Iteration 137/1000 | Loss: 0.00001313
Iteration 138/1000 | Loss: 0.00001313
Iteration 139/1000 | Loss: 0.00001312
Iteration 140/1000 | Loss: 0.00001312
Iteration 141/1000 | Loss: 0.00001312
Iteration 142/1000 | Loss: 0.00001312
Iteration 143/1000 | Loss: 0.00001312
Iteration 144/1000 | Loss: 0.00001311
Iteration 145/1000 | Loss: 0.00001311
Iteration 146/1000 | Loss: 0.00001311
Iteration 147/1000 | Loss: 0.00001311
Iteration 148/1000 | Loss: 0.00001311
Iteration 149/1000 | Loss: 0.00001311
Iteration 150/1000 | Loss: 0.00001310
Iteration 151/1000 | Loss: 0.00001310
Iteration 152/1000 | Loss: 0.00001310
Iteration 153/1000 | Loss: 0.00001309
Iteration 154/1000 | Loss: 0.00001309
Iteration 155/1000 | Loss: 0.00001309
Iteration 156/1000 | Loss: 0.00001309
Iteration 157/1000 | Loss: 0.00001308
Iteration 158/1000 | Loss: 0.00001308
Iteration 159/1000 | Loss: 0.00001308
Iteration 160/1000 | Loss: 0.00001307
Iteration 161/1000 | Loss: 0.00001306
Iteration 162/1000 | Loss: 0.00001306
Iteration 163/1000 | Loss: 0.00001306
Iteration 164/1000 | Loss: 0.00001306
Iteration 165/1000 | Loss: 0.00001305
Iteration 166/1000 | Loss: 0.00001305
Iteration 167/1000 | Loss: 0.00001305
Iteration 168/1000 | Loss: 0.00001304
Iteration 169/1000 | Loss: 0.00001304
Iteration 170/1000 | Loss: 0.00001304
Iteration 171/1000 | Loss: 0.00001304
Iteration 172/1000 | Loss: 0.00001304
Iteration 173/1000 | Loss: 0.00001303
Iteration 174/1000 | Loss: 0.00001303
Iteration 175/1000 | Loss: 0.00001303
Iteration 176/1000 | Loss: 0.00001303
Iteration 177/1000 | Loss: 0.00001303
Iteration 178/1000 | Loss: 0.00001303
Iteration 179/1000 | Loss: 0.00001303
Iteration 180/1000 | Loss: 0.00001302
Iteration 181/1000 | Loss: 0.00001302
Iteration 182/1000 | Loss: 0.00001302
Iteration 183/1000 | Loss: 0.00001302
Iteration 184/1000 | Loss: 0.00001302
Iteration 185/1000 | Loss: 0.00001302
Iteration 186/1000 | Loss: 0.00001302
Iteration 187/1000 | Loss: 0.00001302
Iteration 188/1000 | Loss: 0.00001301
Iteration 189/1000 | Loss: 0.00001301
Iteration 190/1000 | Loss: 0.00001301
Iteration 191/1000 | Loss: 0.00001301
Iteration 192/1000 | Loss: 0.00001300
Iteration 193/1000 | Loss: 0.00001300
Iteration 194/1000 | Loss: 0.00001300
Iteration 195/1000 | Loss: 0.00001300
Iteration 196/1000 | Loss: 0.00001300
Iteration 197/1000 | Loss: 0.00001300
Iteration 198/1000 | Loss: 0.00001300
Iteration 199/1000 | Loss: 0.00001300
Iteration 200/1000 | Loss: 0.00001300
Iteration 201/1000 | Loss: 0.00001300
Iteration 202/1000 | Loss: 0.00001300
Iteration 203/1000 | Loss: 0.00001300
Iteration 204/1000 | Loss: 0.00001300
Iteration 205/1000 | Loss: 0.00001300
Iteration 206/1000 | Loss: 0.00001300
Iteration 207/1000 | Loss: 0.00001299
Iteration 208/1000 | Loss: 0.00001299
Iteration 209/1000 | Loss: 0.00001299
Iteration 210/1000 | Loss: 0.00001299
Iteration 211/1000 | Loss: 0.00001299
Iteration 212/1000 | Loss: 0.00001299
Iteration 213/1000 | Loss: 0.00001299
Iteration 214/1000 | Loss: 0.00001299
Iteration 215/1000 | Loss: 0.00001299
Iteration 216/1000 | Loss: 0.00001299
Iteration 217/1000 | Loss: 0.00001299
Iteration 218/1000 | Loss: 0.00001299
Iteration 219/1000 | Loss: 0.00001298
Iteration 220/1000 | Loss: 0.00001298
Iteration 221/1000 | Loss: 0.00001298
Iteration 222/1000 | Loss: 0.00001298
Iteration 223/1000 | Loss: 0.00001298
Iteration 224/1000 | Loss: 0.00001298
Iteration 225/1000 | Loss: 0.00001298
Iteration 226/1000 | Loss: 0.00001298
Iteration 227/1000 | Loss: 0.00001298
Iteration 228/1000 | Loss: 0.00001298
Iteration 229/1000 | Loss: 0.00001298
Iteration 230/1000 | Loss: 0.00001298
Iteration 231/1000 | Loss: 0.00001298
Iteration 232/1000 | Loss: 0.00001298
Iteration 233/1000 | Loss: 0.00001298
Iteration 234/1000 | Loss: 0.00001298
Iteration 235/1000 | Loss: 0.00001298
Iteration 236/1000 | Loss: 0.00001298
Iteration 237/1000 | Loss: 0.00001298
Iteration 238/1000 | Loss: 0.00001298
Iteration 239/1000 | Loss: 0.00001298
Iteration 240/1000 | Loss: 0.00001298
Iteration 241/1000 | Loss: 0.00001298
Iteration 242/1000 | Loss: 0.00001298
Iteration 243/1000 | Loss: 0.00001298
Iteration 244/1000 | Loss: 0.00001298
Iteration 245/1000 | Loss: 0.00001298
Iteration 246/1000 | Loss: 0.00001298
Iteration 247/1000 | Loss: 0.00001298
Iteration 248/1000 | Loss: 0.00001298
Iteration 249/1000 | Loss: 0.00001298
Iteration 250/1000 | Loss: 0.00001298
Iteration 251/1000 | Loss: 0.00001298
Iteration 252/1000 | Loss: 0.00001298
Iteration 253/1000 | Loss: 0.00001298
Iteration 254/1000 | Loss: 0.00001298
Iteration 255/1000 | Loss: 0.00001298
Iteration 256/1000 | Loss: 0.00001298
Iteration 257/1000 | Loss: 0.00001298
Iteration 258/1000 | Loss: 0.00001298
Iteration 259/1000 | Loss: 0.00001298
Iteration 260/1000 | Loss: 0.00001298
Iteration 261/1000 | Loss: 0.00001298
Iteration 262/1000 | Loss: 0.00001298
Iteration 263/1000 | Loss: 0.00001298
Iteration 264/1000 | Loss: 0.00001298
Iteration 265/1000 | Loss: 0.00001298
Iteration 266/1000 | Loss: 0.00001298
Iteration 267/1000 | Loss: 0.00001298
Iteration 268/1000 | Loss: 0.00001298
Iteration 269/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [1.2981288818991743e-05, 1.2981288818991743e-05, 1.2981288818991743e-05, 1.2981288818991743e-05, 1.2981288818991743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2981288818991743e-05

Optimization complete. Final v2v error: 2.9450860023498535 mm

Highest mean error: 4.360500812530518 mm for frame 54

Lowest mean error: 2.2943718433380127 mm for frame 113

Saving results

Total time: 44.41937494277954
