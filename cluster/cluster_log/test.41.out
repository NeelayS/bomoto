Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=41, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2296-2351
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00578143
Iteration 2/25 | Loss: 0.00122862
Iteration 3/25 | Loss: 0.00115708
Iteration 4/25 | Loss: 0.00114601
Iteration 5/25 | Loss: 0.00114230
Iteration 6/25 | Loss: 0.00114119
Iteration 7/25 | Loss: 0.00114119
Iteration 8/25 | Loss: 0.00114119
Iteration 9/25 | Loss: 0.00114119
Iteration 10/25 | Loss: 0.00114119
Iteration 11/25 | Loss: 0.00114119
Iteration 12/25 | Loss: 0.00114119
Iteration 13/25 | Loss: 0.00114119
Iteration 14/25 | Loss: 0.00114119
Iteration 15/25 | Loss: 0.00114119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011411897139623761, 0.0011411897139623761, 0.0011411897139623761, 0.0011411897139623761, 0.0011411897139623761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011411897139623761

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.92400265
Iteration 2/25 | Loss: 0.00189123
Iteration 3/25 | Loss: 0.00189123
Iteration 4/25 | Loss: 0.00189123
Iteration 5/25 | Loss: 0.00189123
Iteration 6/25 | Loss: 0.00189123
Iteration 7/25 | Loss: 0.00189123
Iteration 8/25 | Loss: 0.00189123
Iteration 9/25 | Loss: 0.00189123
Iteration 10/25 | Loss: 0.00189123
Iteration 11/25 | Loss: 0.00189123
Iteration 12/25 | Loss: 0.00189123
Iteration 13/25 | Loss: 0.00189123
Iteration 14/25 | Loss: 0.00189123
Iteration 15/25 | Loss: 0.00189123
Iteration 16/25 | Loss: 0.00189123
Iteration 17/25 | Loss: 0.00189123
Iteration 18/25 | Loss: 0.00189123
Iteration 19/25 | Loss: 0.00189123
Iteration 20/25 | Loss: 0.00189123
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018912255764007568, 0.0018912255764007568, 0.0018912255764007568, 0.0018912255764007568, 0.0018912255764007568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018912255764007568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189123
Iteration 2/1000 | Loss: 0.00001835
Iteration 3/1000 | Loss: 0.00001319
Iteration 4/1000 | Loss: 0.00001170
Iteration 5/1000 | Loss: 0.00001104
Iteration 6/1000 | Loss: 0.00001045
Iteration 7/1000 | Loss: 0.00001000
Iteration 8/1000 | Loss: 0.00000986
Iteration 9/1000 | Loss: 0.00000973
Iteration 10/1000 | Loss: 0.00000967
Iteration 11/1000 | Loss: 0.00000967
Iteration 12/1000 | Loss: 0.00000953
Iteration 13/1000 | Loss: 0.00000944
Iteration 14/1000 | Loss: 0.00000942
Iteration 15/1000 | Loss: 0.00000940
Iteration 16/1000 | Loss: 0.00000932
Iteration 17/1000 | Loss: 0.00000922
Iteration 18/1000 | Loss: 0.00000918
Iteration 19/1000 | Loss: 0.00000917
Iteration 20/1000 | Loss: 0.00000916
Iteration 21/1000 | Loss: 0.00000906
Iteration 22/1000 | Loss: 0.00000906
Iteration 23/1000 | Loss: 0.00000901
Iteration 24/1000 | Loss: 0.00000899
Iteration 25/1000 | Loss: 0.00000898
Iteration 26/1000 | Loss: 0.00000897
Iteration 27/1000 | Loss: 0.00000897
Iteration 28/1000 | Loss: 0.00000896
Iteration 29/1000 | Loss: 0.00000896
Iteration 30/1000 | Loss: 0.00000896
Iteration 31/1000 | Loss: 0.00000896
Iteration 32/1000 | Loss: 0.00000895
Iteration 33/1000 | Loss: 0.00000895
Iteration 34/1000 | Loss: 0.00000895
Iteration 35/1000 | Loss: 0.00000892
Iteration 36/1000 | Loss: 0.00000890
Iteration 37/1000 | Loss: 0.00000886
Iteration 38/1000 | Loss: 0.00000886
Iteration 39/1000 | Loss: 0.00000886
Iteration 40/1000 | Loss: 0.00000886
Iteration 41/1000 | Loss: 0.00000885
Iteration 42/1000 | Loss: 0.00000885
Iteration 43/1000 | Loss: 0.00000885
Iteration 44/1000 | Loss: 0.00000882
Iteration 45/1000 | Loss: 0.00000882
Iteration 46/1000 | Loss: 0.00000882
Iteration 47/1000 | Loss: 0.00000881
Iteration 48/1000 | Loss: 0.00000881
Iteration 49/1000 | Loss: 0.00000881
Iteration 50/1000 | Loss: 0.00000881
Iteration 51/1000 | Loss: 0.00000881
Iteration 52/1000 | Loss: 0.00000880
Iteration 53/1000 | Loss: 0.00000880
Iteration 54/1000 | Loss: 0.00000879
Iteration 55/1000 | Loss: 0.00000879
Iteration 56/1000 | Loss: 0.00000878
Iteration 57/1000 | Loss: 0.00000878
Iteration 58/1000 | Loss: 0.00000877
Iteration 59/1000 | Loss: 0.00000877
Iteration 60/1000 | Loss: 0.00000877
Iteration 61/1000 | Loss: 0.00000877
Iteration 62/1000 | Loss: 0.00000877
Iteration 63/1000 | Loss: 0.00000877
Iteration 64/1000 | Loss: 0.00000877
Iteration 65/1000 | Loss: 0.00000877
Iteration 66/1000 | Loss: 0.00000877
Iteration 67/1000 | Loss: 0.00000876
Iteration 68/1000 | Loss: 0.00000876
Iteration 69/1000 | Loss: 0.00000876
Iteration 70/1000 | Loss: 0.00000876
Iteration 71/1000 | Loss: 0.00000876
Iteration 72/1000 | Loss: 0.00000876
Iteration 73/1000 | Loss: 0.00000876
Iteration 74/1000 | Loss: 0.00000876
Iteration 75/1000 | Loss: 0.00000875
Iteration 76/1000 | Loss: 0.00000875
Iteration 77/1000 | Loss: 0.00000875
Iteration 78/1000 | Loss: 0.00000875
Iteration 79/1000 | Loss: 0.00000874
Iteration 80/1000 | Loss: 0.00000874
Iteration 81/1000 | Loss: 0.00000874
Iteration 82/1000 | Loss: 0.00000874
Iteration 83/1000 | Loss: 0.00000874
Iteration 84/1000 | Loss: 0.00000873
Iteration 85/1000 | Loss: 0.00000873
Iteration 86/1000 | Loss: 0.00000873
Iteration 87/1000 | Loss: 0.00000873
Iteration 88/1000 | Loss: 0.00000873
Iteration 89/1000 | Loss: 0.00000873
Iteration 90/1000 | Loss: 0.00000873
Iteration 91/1000 | Loss: 0.00000873
Iteration 92/1000 | Loss: 0.00000873
Iteration 93/1000 | Loss: 0.00000873
Iteration 94/1000 | Loss: 0.00000872
Iteration 95/1000 | Loss: 0.00000872
Iteration 96/1000 | Loss: 0.00000872
Iteration 97/1000 | Loss: 0.00000872
Iteration 98/1000 | Loss: 0.00000871
Iteration 99/1000 | Loss: 0.00000871
Iteration 100/1000 | Loss: 0.00000871
Iteration 101/1000 | Loss: 0.00000871
Iteration 102/1000 | Loss: 0.00000871
Iteration 103/1000 | Loss: 0.00000871
Iteration 104/1000 | Loss: 0.00000870
Iteration 105/1000 | Loss: 0.00000870
Iteration 106/1000 | Loss: 0.00000870
Iteration 107/1000 | Loss: 0.00000870
Iteration 108/1000 | Loss: 0.00000870
Iteration 109/1000 | Loss: 0.00000870
Iteration 110/1000 | Loss: 0.00000870
Iteration 111/1000 | Loss: 0.00000869
Iteration 112/1000 | Loss: 0.00000869
Iteration 113/1000 | Loss: 0.00000869
Iteration 114/1000 | Loss: 0.00000869
Iteration 115/1000 | Loss: 0.00000869
Iteration 116/1000 | Loss: 0.00000869
Iteration 117/1000 | Loss: 0.00000869
Iteration 118/1000 | Loss: 0.00000869
Iteration 119/1000 | Loss: 0.00000869
Iteration 120/1000 | Loss: 0.00000869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [8.690793947607744e-06, 8.690793947607744e-06, 8.690793947607744e-06, 8.690793947607744e-06, 8.690793947607744e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.690793947607744e-06

Optimization complete. Final v2v error: 2.5812246799468994 mm

Highest mean error: 2.817519187927246 mm for frame 97

Lowest mean error: 2.4251506328582764 mm for frame 28

Saving results

Total time: 37.2840850353241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822273
Iteration 2/25 | Loss: 0.00122637
Iteration 3/25 | Loss: 0.00114257
Iteration 4/25 | Loss: 0.00113471
Iteration 5/25 | Loss: 0.00113303
Iteration 6/25 | Loss: 0.00113294
Iteration 7/25 | Loss: 0.00113294
Iteration 8/25 | Loss: 0.00113294
Iteration 9/25 | Loss: 0.00113294
Iteration 10/25 | Loss: 0.00113294
Iteration 11/25 | Loss: 0.00113294
Iteration 12/25 | Loss: 0.00113294
Iteration 13/25 | Loss: 0.00113294
Iteration 14/25 | Loss: 0.00113294
Iteration 15/25 | Loss: 0.00113294
Iteration 16/25 | Loss: 0.00113294
Iteration 17/25 | Loss: 0.00113294
Iteration 18/25 | Loss: 0.00113294
Iteration 19/25 | Loss: 0.00113294
Iteration 20/25 | Loss: 0.00113294
Iteration 21/25 | Loss: 0.00113294
Iteration 22/25 | Loss: 0.00113294
Iteration 23/25 | Loss: 0.00113294
Iteration 24/25 | Loss: 0.00113294
Iteration 25/25 | Loss: 0.00113294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24182773
Iteration 2/25 | Loss: 0.00169035
Iteration 3/25 | Loss: 0.00169034
Iteration 4/25 | Loss: 0.00169034
Iteration 5/25 | Loss: 0.00169033
Iteration 6/25 | Loss: 0.00169033
Iteration 7/25 | Loss: 0.00169033
Iteration 8/25 | Loss: 0.00169033
Iteration 9/25 | Loss: 0.00169033
Iteration 10/25 | Loss: 0.00169033
Iteration 11/25 | Loss: 0.00169033
Iteration 12/25 | Loss: 0.00169033
Iteration 13/25 | Loss: 0.00169033
Iteration 14/25 | Loss: 0.00169033
Iteration 15/25 | Loss: 0.00169033
Iteration 16/25 | Loss: 0.00169033
Iteration 17/25 | Loss: 0.00169033
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016903325449675322, 0.0016903325449675322, 0.0016903325449675322, 0.0016903325449675322, 0.0016903325449675322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016903325449675322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169033
Iteration 2/1000 | Loss: 0.00002121
Iteration 3/1000 | Loss: 0.00001282
Iteration 4/1000 | Loss: 0.00001051
Iteration 5/1000 | Loss: 0.00000946
Iteration 6/1000 | Loss: 0.00000881
Iteration 7/1000 | Loss: 0.00000853
Iteration 8/1000 | Loss: 0.00000809
Iteration 9/1000 | Loss: 0.00000808
Iteration 10/1000 | Loss: 0.00000796
Iteration 11/1000 | Loss: 0.00000774
Iteration 12/1000 | Loss: 0.00000770
Iteration 13/1000 | Loss: 0.00000762
Iteration 14/1000 | Loss: 0.00000761
Iteration 15/1000 | Loss: 0.00000757
Iteration 16/1000 | Loss: 0.00000756
Iteration 17/1000 | Loss: 0.00000753
Iteration 18/1000 | Loss: 0.00000753
Iteration 19/1000 | Loss: 0.00000752
Iteration 20/1000 | Loss: 0.00000750
Iteration 21/1000 | Loss: 0.00000749
Iteration 22/1000 | Loss: 0.00000748
Iteration 23/1000 | Loss: 0.00000747
Iteration 24/1000 | Loss: 0.00000747
Iteration 25/1000 | Loss: 0.00000746
Iteration 26/1000 | Loss: 0.00000744
Iteration 27/1000 | Loss: 0.00000738
Iteration 28/1000 | Loss: 0.00000736
Iteration 29/1000 | Loss: 0.00000735
Iteration 30/1000 | Loss: 0.00000735
Iteration 31/1000 | Loss: 0.00000735
Iteration 32/1000 | Loss: 0.00000734
Iteration 33/1000 | Loss: 0.00000733
Iteration 34/1000 | Loss: 0.00000732
Iteration 35/1000 | Loss: 0.00000732
Iteration 36/1000 | Loss: 0.00000731
Iteration 37/1000 | Loss: 0.00000731
Iteration 38/1000 | Loss: 0.00000731
Iteration 39/1000 | Loss: 0.00000730
Iteration 40/1000 | Loss: 0.00000730
Iteration 41/1000 | Loss: 0.00000730
Iteration 42/1000 | Loss: 0.00000730
Iteration 43/1000 | Loss: 0.00000730
Iteration 44/1000 | Loss: 0.00000730
Iteration 45/1000 | Loss: 0.00000729
Iteration 46/1000 | Loss: 0.00000729
Iteration 47/1000 | Loss: 0.00000728
Iteration 48/1000 | Loss: 0.00000728
Iteration 49/1000 | Loss: 0.00000728
Iteration 50/1000 | Loss: 0.00000728
Iteration 51/1000 | Loss: 0.00000728
Iteration 52/1000 | Loss: 0.00000728
Iteration 53/1000 | Loss: 0.00000728
Iteration 54/1000 | Loss: 0.00000728
Iteration 55/1000 | Loss: 0.00000728
Iteration 56/1000 | Loss: 0.00000728
Iteration 57/1000 | Loss: 0.00000728
Iteration 58/1000 | Loss: 0.00000727
Iteration 59/1000 | Loss: 0.00000727
Iteration 60/1000 | Loss: 0.00000727
Iteration 61/1000 | Loss: 0.00000727
Iteration 62/1000 | Loss: 0.00000727
Iteration 63/1000 | Loss: 0.00000727
Iteration 64/1000 | Loss: 0.00000727
Iteration 65/1000 | Loss: 0.00000727
Iteration 66/1000 | Loss: 0.00000726
Iteration 67/1000 | Loss: 0.00000726
Iteration 68/1000 | Loss: 0.00000726
Iteration 69/1000 | Loss: 0.00000726
Iteration 70/1000 | Loss: 0.00000725
Iteration 71/1000 | Loss: 0.00000725
Iteration 72/1000 | Loss: 0.00000725
Iteration 73/1000 | Loss: 0.00000724
Iteration 74/1000 | Loss: 0.00000724
Iteration 75/1000 | Loss: 0.00000724
Iteration 76/1000 | Loss: 0.00000724
Iteration 77/1000 | Loss: 0.00000724
Iteration 78/1000 | Loss: 0.00000724
Iteration 79/1000 | Loss: 0.00000724
Iteration 80/1000 | Loss: 0.00000724
Iteration 81/1000 | Loss: 0.00000724
Iteration 82/1000 | Loss: 0.00000724
Iteration 83/1000 | Loss: 0.00000723
Iteration 84/1000 | Loss: 0.00000722
Iteration 85/1000 | Loss: 0.00000722
Iteration 86/1000 | Loss: 0.00000722
Iteration 87/1000 | Loss: 0.00000722
Iteration 88/1000 | Loss: 0.00000722
Iteration 89/1000 | Loss: 0.00000722
Iteration 90/1000 | Loss: 0.00000722
Iteration 91/1000 | Loss: 0.00000722
Iteration 92/1000 | Loss: 0.00000722
Iteration 93/1000 | Loss: 0.00000722
Iteration 94/1000 | Loss: 0.00000722
Iteration 95/1000 | Loss: 0.00000721
Iteration 96/1000 | Loss: 0.00000721
Iteration 97/1000 | Loss: 0.00000721
Iteration 98/1000 | Loss: 0.00000721
Iteration 99/1000 | Loss: 0.00000721
Iteration 100/1000 | Loss: 0.00000720
Iteration 101/1000 | Loss: 0.00000720
Iteration 102/1000 | Loss: 0.00000720
Iteration 103/1000 | Loss: 0.00000719
Iteration 104/1000 | Loss: 0.00000719
Iteration 105/1000 | Loss: 0.00000719
Iteration 106/1000 | Loss: 0.00000719
Iteration 107/1000 | Loss: 0.00000719
Iteration 108/1000 | Loss: 0.00000718
Iteration 109/1000 | Loss: 0.00000718
Iteration 110/1000 | Loss: 0.00000718
Iteration 111/1000 | Loss: 0.00000718
Iteration 112/1000 | Loss: 0.00000718
Iteration 113/1000 | Loss: 0.00000718
Iteration 114/1000 | Loss: 0.00000718
Iteration 115/1000 | Loss: 0.00000717
Iteration 116/1000 | Loss: 0.00000717
Iteration 117/1000 | Loss: 0.00000717
Iteration 118/1000 | Loss: 0.00000717
Iteration 119/1000 | Loss: 0.00000717
Iteration 120/1000 | Loss: 0.00000716
Iteration 121/1000 | Loss: 0.00000715
Iteration 122/1000 | Loss: 0.00000715
Iteration 123/1000 | Loss: 0.00000715
Iteration 124/1000 | Loss: 0.00000715
Iteration 125/1000 | Loss: 0.00000715
Iteration 126/1000 | Loss: 0.00000715
Iteration 127/1000 | Loss: 0.00000715
Iteration 128/1000 | Loss: 0.00000715
Iteration 129/1000 | Loss: 0.00000715
Iteration 130/1000 | Loss: 0.00000715
Iteration 131/1000 | Loss: 0.00000715
Iteration 132/1000 | Loss: 0.00000714
Iteration 133/1000 | Loss: 0.00000714
Iteration 134/1000 | Loss: 0.00000714
Iteration 135/1000 | Loss: 0.00000714
Iteration 136/1000 | Loss: 0.00000714
Iteration 137/1000 | Loss: 0.00000714
Iteration 138/1000 | Loss: 0.00000714
Iteration 139/1000 | Loss: 0.00000713
Iteration 140/1000 | Loss: 0.00000713
Iteration 141/1000 | Loss: 0.00000713
Iteration 142/1000 | Loss: 0.00000713
Iteration 143/1000 | Loss: 0.00000713
Iteration 144/1000 | Loss: 0.00000713
Iteration 145/1000 | Loss: 0.00000713
Iteration 146/1000 | Loss: 0.00000713
Iteration 147/1000 | Loss: 0.00000713
Iteration 148/1000 | Loss: 0.00000712
Iteration 149/1000 | Loss: 0.00000712
Iteration 150/1000 | Loss: 0.00000712
Iteration 151/1000 | Loss: 0.00000712
Iteration 152/1000 | Loss: 0.00000712
Iteration 153/1000 | Loss: 0.00000712
Iteration 154/1000 | Loss: 0.00000712
Iteration 155/1000 | Loss: 0.00000712
Iteration 156/1000 | Loss: 0.00000712
Iteration 157/1000 | Loss: 0.00000711
Iteration 158/1000 | Loss: 0.00000711
Iteration 159/1000 | Loss: 0.00000711
Iteration 160/1000 | Loss: 0.00000711
Iteration 161/1000 | Loss: 0.00000711
Iteration 162/1000 | Loss: 0.00000711
Iteration 163/1000 | Loss: 0.00000710
Iteration 164/1000 | Loss: 0.00000710
Iteration 165/1000 | Loss: 0.00000710
Iteration 166/1000 | Loss: 0.00000710
Iteration 167/1000 | Loss: 0.00000710
Iteration 168/1000 | Loss: 0.00000710
Iteration 169/1000 | Loss: 0.00000710
Iteration 170/1000 | Loss: 0.00000710
Iteration 171/1000 | Loss: 0.00000710
Iteration 172/1000 | Loss: 0.00000710
Iteration 173/1000 | Loss: 0.00000710
Iteration 174/1000 | Loss: 0.00000710
Iteration 175/1000 | Loss: 0.00000710
Iteration 176/1000 | Loss: 0.00000710
Iteration 177/1000 | Loss: 0.00000710
Iteration 178/1000 | Loss: 0.00000710
Iteration 179/1000 | Loss: 0.00000710
Iteration 180/1000 | Loss: 0.00000710
Iteration 181/1000 | Loss: 0.00000710
Iteration 182/1000 | Loss: 0.00000710
Iteration 183/1000 | Loss: 0.00000710
Iteration 184/1000 | Loss: 0.00000710
Iteration 185/1000 | Loss: 0.00000710
Iteration 186/1000 | Loss: 0.00000710
Iteration 187/1000 | Loss: 0.00000710
Iteration 188/1000 | Loss: 0.00000710
Iteration 189/1000 | Loss: 0.00000710
Iteration 190/1000 | Loss: 0.00000710
Iteration 191/1000 | Loss: 0.00000710
Iteration 192/1000 | Loss: 0.00000710
Iteration 193/1000 | Loss: 0.00000710
Iteration 194/1000 | Loss: 0.00000710
Iteration 195/1000 | Loss: 0.00000710
Iteration 196/1000 | Loss: 0.00000710
Iteration 197/1000 | Loss: 0.00000710
Iteration 198/1000 | Loss: 0.00000710
Iteration 199/1000 | Loss: 0.00000710
Iteration 200/1000 | Loss: 0.00000710
Iteration 201/1000 | Loss: 0.00000710
Iteration 202/1000 | Loss: 0.00000710
Iteration 203/1000 | Loss: 0.00000710
Iteration 204/1000 | Loss: 0.00000710
Iteration 205/1000 | Loss: 0.00000710
Iteration 206/1000 | Loss: 0.00000710
Iteration 207/1000 | Loss: 0.00000710
Iteration 208/1000 | Loss: 0.00000710
Iteration 209/1000 | Loss: 0.00000710
Iteration 210/1000 | Loss: 0.00000710
Iteration 211/1000 | Loss: 0.00000710
Iteration 212/1000 | Loss: 0.00000710
Iteration 213/1000 | Loss: 0.00000710
Iteration 214/1000 | Loss: 0.00000710
Iteration 215/1000 | Loss: 0.00000710
Iteration 216/1000 | Loss: 0.00000710
Iteration 217/1000 | Loss: 0.00000710
Iteration 218/1000 | Loss: 0.00000710
Iteration 219/1000 | Loss: 0.00000710
Iteration 220/1000 | Loss: 0.00000710
Iteration 221/1000 | Loss: 0.00000710
Iteration 222/1000 | Loss: 0.00000710
Iteration 223/1000 | Loss: 0.00000710
Iteration 224/1000 | Loss: 0.00000710
Iteration 225/1000 | Loss: 0.00000710
Iteration 226/1000 | Loss: 0.00000710
Iteration 227/1000 | Loss: 0.00000710
Iteration 228/1000 | Loss: 0.00000710
Iteration 229/1000 | Loss: 0.00000710
Iteration 230/1000 | Loss: 0.00000710
Iteration 231/1000 | Loss: 0.00000710
Iteration 232/1000 | Loss: 0.00000710
Iteration 233/1000 | Loss: 0.00000710
Iteration 234/1000 | Loss: 0.00000710
Iteration 235/1000 | Loss: 0.00000710
Iteration 236/1000 | Loss: 0.00000710
Iteration 237/1000 | Loss: 0.00000710
Iteration 238/1000 | Loss: 0.00000710
Iteration 239/1000 | Loss: 0.00000710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [7.101818482624367e-06, 7.101818482624367e-06, 7.101818482624367e-06, 7.101818482624367e-06, 7.101818482624367e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.101818482624367e-06

Optimization complete. Final v2v error: 2.318713426589966 mm

Highest mean error: 2.48335337638855 mm for frame 21

Lowest mean error: 2.210679054260254 mm for frame 40

Saving results

Total time: 38.789329051971436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790074
Iteration 2/25 | Loss: 0.00185330
Iteration 3/25 | Loss: 0.00160832
Iteration 4/25 | Loss: 0.00139750
Iteration 5/25 | Loss: 0.00132209
Iteration 6/25 | Loss: 0.00127960
Iteration 7/25 | Loss: 0.00126962
Iteration 8/25 | Loss: 0.00125781
Iteration 9/25 | Loss: 0.00125609
Iteration 10/25 | Loss: 0.00126116
Iteration 11/25 | Loss: 0.00125175
Iteration 12/25 | Loss: 0.00124822
Iteration 13/25 | Loss: 0.00124223
Iteration 14/25 | Loss: 0.00124076
Iteration 15/25 | Loss: 0.00124219
Iteration 16/25 | Loss: 0.00123963
Iteration 17/25 | Loss: 0.00123895
Iteration 18/25 | Loss: 0.00123885
Iteration 19/25 | Loss: 0.00123878
Iteration 20/25 | Loss: 0.00123878
Iteration 21/25 | Loss: 0.00123878
Iteration 22/25 | Loss: 0.00123878
Iteration 23/25 | Loss: 0.00123878
Iteration 24/25 | Loss: 0.00123878
Iteration 25/25 | Loss: 0.00123877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.82968521
Iteration 2/25 | Loss: 0.00203424
Iteration 3/25 | Loss: 0.00187014
Iteration 4/25 | Loss: 0.00187014
Iteration 5/25 | Loss: 0.00187014
Iteration 6/25 | Loss: 0.00187014
Iteration 7/25 | Loss: 0.00187014
Iteration 8/25 | Loss: 0.00187014
Iteration 9/25 | Loss: 0.00187014
Iteration 10/25 | Loss: 0.00187014
Iteration 11/25 | Loss: 0.00187014
Iteration 12/25 | Loss: 0.00187014
Iteration 13/25 | Loss: 0.00187014
Iteration 14/25 | Loss: 0.00187014
Iteration 15/25 | Loss: 0.00187014
Iteration 16/25 | Loss: 0.00187014
Iteration 17/25 | Loss: 0.00187014
Iteration 18/25 | Loss: 0.00187014
Iteration 19/25 | Loss: 0.00187014
Iteration 20/25 | Loss: 0.00187014
Iteration 21/25 | Loss: 0.00187014
Iteration 22/25 | Loss: 0.00187014
Iteration 23/25 | Loss: 0.00187014
Iteration 24/25 | Loss: 0.00187014
Iteration 25/25 | Loss: 0.00187014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187014
Iteration 2/1000 | Loss: 0.00004867
Iteration 3/1000 | Loss: 0.00003489
Iteration 4/1000 | Loss: 0.00003089
Iteration 5/1000 | Loss: 0.00002893
Iteration 6/1000 | Loss: 0.00002737
Iteration 7/1000 | Loss: 0.00002673
Iteration 8/1000 | Loss: 0.00002617
Iteration 9/1000 | Loss: 0.00002574
Iteration 10/1000 | Loss: 0.00002532
Iteration 11/1000 | Loss: 0.00002499
Iteration 12/1000 | Loss: 0.00002461
Iteration 13/1000 | Loss: 0.00002433
Iteration 14/1000 | Loss: 0.00002422
Iteration 15/1000 | Loss: 0.00002419
Iteration 16/1000 | Loss: 0.00002419
Iteration 17/1000 | Loss: 0.00002418
Iteration 18/1000 | Loss: 0.00002414
Iteration 19/1000 | Loss: 0.00002409
Iteration 20/1000 | Loss: 0.00002402
Iteration 21/1000 | Loss: 0.00002395
Iteration 22/1000 | Loss: 0.00002392
Iteration 23/1000 | Loss: 0.00002392
Iteration 24/1000 | Loss: 0.00002392
Iteration 25/1000 | Loss: 0.00002391
Iteration 26/1000 | Loss: 0.00002391
Iteration 27/1000 | Loss: 0.00002390
Iteration 28/1000 | Loss: 0.00002390
Iteration 29/1000 | Loss: 0.00002390
Iteration 30/1000 | Loss: 0.00002389
Iteration 31/1000 | Loss: 0.00002389
Iteration 32/1000 | Loss: 0.00002388
Iteration 33/1000 | Loss: 0.00002388
Iteration 34/1000 | Loss: 0.00002388
Iteration 35/1000 | Loss: 0.00002387
Iteration 36/1000 | Loss: 0.00002387
Iteration 37/1000 | Loss: 0.00002386
Iteration 38/1000 | Loss: 0.00002386
Iteration 39/1000 | Loss: 0.00002385
Iteration 40/1000 | Loss: 0.00002385
Iteration 41/1000 | Loss: 0.00002385
Iteration 42/1000 | Loss: 0.00002385
Iteration 43/1000 | Loss: 0.00002385
Iteration 44/1000 | Loss: 0.00002384
Iteration 45/1000 | Loss: 0.00002384
Iteration 46/1000 | Loss: 0.00002384
Iteration 47/1000 | Loss: 0.00002384
Iteration 48/1000 | Loss: 0.00002383
Iteration 49/1000 | Loss: 0.00002383
Iteration 50/1000 | Loss: 0.00002382
Iteration 51/1000 | Loss: 0.00002382
Iteration 52/1000 | Loss: 0.00002382
Iteration 53/1000 | Loss: 0.00002381
Iteration 54/1000 | Loss: 0.00002381
Iteration 55/1000 | Loss: 0.00002381
Iteration 56/1000 | Loss: 0.00002381
Iteration 57/1000 | Loss: 0.00002381
Iteration 58/1000 | Loss: 0.00002381
Iteration 59/1000 | Loss: 0.00002380
Iteration 60/1000 | Loss: 0.00002380
Iteration 61/1000 | Loss: 0.00002380
Iteration 62/1000 | Loss: 0.00002380
Iteration 63/1000 | Loss: 0.00002379
Iteration 64/1000 | Loss: 0.00002379
Iteration 65/1000 | Loss: 0.00002378
Iteration 66/1000 | Loss: 0.00002378
Iteration 67/1000 | Loss: 0.00002377
Iteration 68/1000 | Loss: 0.00002377
Iteration 69/1000 | Loss: 0.00002377
Iteration 70/1000 | Loss: 0.00002376
Iteration 71/1000 | Loss: 0.00002376
Iteration 72/1000 | Loss: 0.00002375
Iteration 73/1000 | Loss: 0.00002375
Iteration 74/1000 | Loss: 0.00002374
Iteration 75/1000 | Loss: 0.00002373
Iteration 76/1000 | Loss: 0.00002372
Iteration 77/1000 | Loss: 0.00002371
Iteration 78/1000 | Loss: 0.00002371
Iteration 79/1000 | Loss: 0.00002371
Iteration 80/1000 | Loss: 0.00002371
Iteration 81/1000 | Loss: 0.00002370
Iteration 82/1000 | Loss: 0.00002370
Iteration 83/1000 | Loss: 0.00002369
Iteration 84/1000 | Loss: 0.00002369
Iteration 85/1000 | Loss: 0.00002369
Iteration 86/1000 | Loss: 0.00002368
Iteration 87/1000 | Loss: 0.00002368
Iteration 88/1000 | Loss: 0.00002368
Iteration 89/1000 | Loss: 0.00002368
Iteration 90/1000 | Loss: 0.00002368
Iteration 91/1000 | Loss: 0.00002368
Iteration 92/1000 | Loss: 0.00002368
Iteration 93/1000 | Loss: 0.00002367
Iteration 94/1000 | Loss: 0.00002367
Iteration 95/1000 | Loss: 0.00002367
Iteration 96/1000 | Loss: 0.00002367
Iteration 97/1000 | Loss: 0.00002366
Iteration 98/1000 | Loss: 0.00002366
Iteration 99/1000 | Loss: 0.00002366
Iteration 100/1000 | Loss: 0.00002366
Iteration 101/1000 | Loss: 0.00002366
Iteration 102/1000 | Loss: 0.00002366
Iteration 103/1000 | Loss: 0.00002365
Iteration 104/1000 | Loss: 0.00002365
Iteration 105/1000 | Loss: 0.00002364
Iteration 106/1000 | Loss: 0.00002364
Iteration 107/1000 | Loss: 0.00002364
Iteration 108/1000 | Loss: 0.00002364
Iteration 109/1000 | Loss: 0.00002364
Iteration 110/1000 | Loss: 0.00002364
Iteration 111/1000 | Loss: 0.00002364
Iteration 112/1000 | Loss: 0.00002364
Iteration 113/1000 | Loss: 0.00002364
Iteration 114/1000 | Loss: 0.00002364
Iteration 115/1000 | Loss: 0.00002364
Iteration 116/1000 | Loss: 0.00002363
Iteration 117/1000 | Loss: 0.00002363
Iteration 118/1000 | Loss: 0.00002363
Iteration 119/1000 | Loss: 0.00002363
Iteration 120/1000 | Loss: 0.00002363
Iteration 121/1000 | Loss: 0.00002363
Iteration 122/1000 | Loss: 0.00002363
Iteration 123/1000 | Loss: 0.00002362
Iteration 124/1000 | Loss: 0.00002362
Iteration 125/1000 | Loss: 0.00002362
Iteration 126/1000 | Loss: 0.00002362
Iteration 127/1000 | Loss: 0.00002362
Iteration 128/1000 | Loss: 0.00002362
Iteration 129/1000 | Loss: 0.00002362
Iteration 130/1000 | Loss: 0.00002362
Iteration 131/1000 | Loss: 0.00002362
Iteration 132/1000 | Loss: 0.00002362
Iteration 133/1000 | Loss: 0.00002362
Iteration 134/1000 | Loss: 0.00002362
Iteration 135/1000 | Loss: 0.00002362
Iteration 136/1000 | Loss: 0.00002362
Iteration 137/1000 | Loss: 0.00002362
Iteration 138/1000 | Loss: 0.00002362
Iteration 139/1000 | Loss: 0.00002362
Iteration 140/1000 | Loss: 0.00002362
Iteration 141/1000 | Loss: 0.00002362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.3620274077984504e-05, 2.3620274077984504e-05, 2.3620274077984504e-05, 2.3620274077984504e-05, 2.3620274077984504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3620274077984504e-05

Optimization complete. Final v2v error: 3.8143110275268555 mm

Highest mean error: 11.596172332763672 mm for frame 194

Lowest mean error: 3.492593288421631 mm for frame 151

Saving results

Total time: 76.48773980140686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801402
Iteration 2/25 | Loss: 0.00163100
Iteration 3/25 | Loss: 0.00137667
Iteration 4/25 | Loss: 0.00133373
Iteration 5/25 | Loss: 0.00139332
Iteration 6/25 | Loss: 0.00145330
Iteration 7/25 | Loss: 0.00142018
Iteration 8/25 | Loss: 0.00140768
Iteration 9/25 | Loss: 0.00127417
Iteration 10/25 | Loss: 0.00121621
Iteration 11/25 | Loss: 0.00120544
Iteration 12/25 | Loss: 0.00120443
Iteration 13/25 | Loss: 0.00120364
Iteration 14/25 | Loss: 0.00120205
Iteration 15/25 | Loss: 0.00120143
Iteration 16/25 | Loss: 0.00120092
Iteration 17/25 | Loss: 0.00120056
Iteration 18/25 | Loss: 0.00120654
Iteration 19/25 | Loss: 0.00119879
Iteration 20/25 | Loss: 0.00119583
Iteration 21/25 | Loss: 0.00119520
Iteration 22/25 | Loss: 0.00119491
Iteration 23/25 | Loss: 0.00120715
Iteration 24/25 | Loss: 0.00118952
Iteration 25/25 | Loss: 0.00118539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15875959
Iteration 2/25 | Loss: 0.00289965
Iteration 3/25 | Loss: 0.00289964
Iteration 4/25 | Loss: 0.00289964
Iteration 5/25 | Loss: 0.00289964
Iteration 6/25 | Loss: 0.00289964
Iteration 7/25 | Loss: 0.00289964
Iteration 8/25 | Loss: 0.00289964
Iteration 9/25 | Loss: 0.00289964
Iteration 10/25 | Loss: 0.00289964
Iteration 11/25 | Loss: 0.00289964
Iteration 12/25 | Loss: 0.00289964
Iteration 13/25 | Loss: 0.00289964
Iteration 14/25 | Loss: 0.00289964
Iteration 15/25 | Loss: 0.00289964
Iteration 16/25 | Loss: 0.00289964
Iteration 17/25 | Loss: 0.00289964
Iteration 18/25 | Loss: 0.00289964
Iteration 19/25 | Loss: 0.00289964
Iteration 20/25 | Loss: 0.00289964
Iteration 21/25 | Loss: 0.00289964
Iteration 22/25 | Loss: 0.00289964
Iteration 23/25 | Loss: 0.00289964
Iteration 24/25 | Loss: 0.00289964
Iteration 25/25 | Loss: 0.00289964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00289964
Iteration 2/1000 | Loss: 0.00006638
Iteration 3/1000 | Loss: 0.00053714
Iteration 4/1000 | Loss: 0.00004237
Iteration 5/1000 | Loss: 0.00003170
Iteration 6/1000 | Loss: 0.00002644
Iteration 7/1000 | Loss: 0.00016852
Iteration 8/1000 | Loss: 0.00024182
Iteration 9/1000 | Loss: 0.00017929
Iteration 10/1000 | Loss: 0.00003482
Iteration 11/1000 | Loss: 0.00002486
Iteration 12/1000 | Loss: 0.00002229
Iteration 13/1000 | Loss: 0.00002011
Iteration 14/1000 | Loss: 0.00001812
Iteration 15/1000 | Loss: 0.00001721
Iteration 16/1000 | Loss: 0.00001673
Iteration 17/1000 | Loss: 0.00001637
Iteration 18/1000 | Loss: 0.00001601
Iteration 19/1000 | Loss: 0.00001576
Iteration 20/1000 | Loss: 0.00001554
Iteration 21/1000 | Loss: 0.00001535
Iteration 22/1000 | Loss: 0.00001532
Iteration 23/1000 | Loss: 0.00001524
Iteration 24/1000 | Loss: 0.00001522
Iteration 25/1000 | Loss: 0.00001521
Iteration 26/1000 | Loss: 0.00001517
Iteration 27/1000 | Loss: 0.00001516
Iteration 28/1000 | Loss: 0.00001515
Iteration 29/1000 | Loss: 0.00001514
Iteration 30/1000 | Loss: 0.00001514
Iteration 31/1000 | Loss: 0.00001513
Iteration 32/1000 | Loss: 0.00001512
Iteration 33/1000 | Loss: 0.00001512
Iteration 34/1000 | Loss: 0.00001511
Iteration 35/1000 | Loss: 0.00001511
Iteration 36/1000 | Loss: 0.00001509
Iteration 37/1000 | Loss: 0.00001508
Iteration 38/1000 | Loss: 0.00001508
Iteration 39/1000 | Loss: 0.00001505
Iteration 40/1000 | Loss: 0.00001504
Iteration 41/1000 | Loss: 0.00001503
Iteration 42/1000 | Loss: 0.00001498
Iteration 43/1000 | Loss: 0.00001498
Iteration 44/1000 | Loss: 0.00001496
Iteration 45/1000 | Loss: 0.00001495
Iteration 46/1000 | Loss: 0.00001495
Iteration 47/1000 | Loss: 0.00001495
Iteration 48/1000 | Loss: 0.00001494
Iteration 49/1000 | Loss: 0.00001494
Iteration 50/1000 | Loss: 0.00001494
Iteration 51/1000 | Loss: 0.00001494
Iteration 52/1000 | Loss: 0.00001494
Iteration 53/1000 | Loss: 0.00001494
Iteration 54/1000 | Loss: 0.00001493
Iteration 55/1000 | Loss: 0.00001493
Iteration 56/1000 | Loss: 0.00001491
Iteration 57/1000 | Loss: 0.00001491
Iteration 58/1000 | Loss: 0.00001491
Iteration 59/1000 | Loss: 0.00001490
Iteration 60/1000 | Loss: 0.00001490
Iteration 61/1000 | Loss: 0.00001490
Iteration 62/1000 | Loss: 0.00001490
Iteration 63/1000 | Loss: 0.00001489
Iteration 64/1000 | Loss: 0.00001489
Iteration 65/1000 | Loss: 0.00001489
Iteration 66/1000 | Loss: 0.00001489
Iteration 67/1000 | Loss: 0.00001489
Iteration 68/1000 | Loss: 0.00001489
Iteration 69/1000 | Loss: 0.00001489
Iteration 70/1000 | Loss: 0.00001489
Iteration 71/1000 | Loss: 0.00001488
Iteration 72/1000 | Loss: 0.00001487
Iteration 73/1000 | Loss: 0.00001486
Iteration 74/1000 | Loss: 0.00001484
Iteration 75/1000 | Loss: 0.00001484
Iteration 76/1000 | Loss: 0.00001484
Iteration 77/1000 | Loss: 0.00001484
Iteration 78/1000 | Loss: 0.00001481
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001480
Iteration 81/1000 | Loss: 0.00001480
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001479
Iteration 86/1000 | Loss: 0.00001479
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001478
Iteration 90/1000 | Loss: 0.00001478
Iteration 91/1000 | Loss: 0.00001478
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001478
Iteration 94/1000 | Loss: 0.00001477
Iteration 95/1000 | Loss: 0.00001477
Iteration 96/1000 | Loss: 0.00001477
Iteration 97/1000 | Loss: 0.00001476
Iteration 98/1000 | Loss: 0.00001476
Iteration 99/1000 | Loss: 0.00001476
Iteration 100/1000 | Loss: 0.00001476
Iteration 101/1000 | Loss: 0.00001476
Iteration 102/1000 | Loss: 0.00001476
Iteration 103/1000 | Loss: 0.00001475
Iteration 104/1000 | Loss: 0.00001475
Iteration 105/1000 | Loss: 0.00001474
Iteration 106/1000 | Loss: 0.00001474
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001474
Iteration 109/1000 | Loss: 0.00001474
Iteration 110/1000 | Loss: 0.00001474
Iteration 111/1000 | Loss: 0.00001474
Iteration 112/1000 | Loss: 0.00001474
Iteration 113/1000 | Loss: 0.00001474
Iteration 114/1000 | Loss: 0.00001474
Iteration 115/1000 | Loss: 0.00001474
Iteration 116/1000 | Loss: 0.00001474
Iteration 117/1000 | Loss: 0.00001473
Iteration 118/1000 | Loss: 0.00001473
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001473
Iteration 121/1000 | Loss: 0.00001473
Iteration 122/1000 | Loss: 0.00001473
Iteration 123/1000 | Loss: 0.00001473
Iteration 124/1000 | Loss: 0.00001473
Iteration 125/1000 | Loss: 0.00001473
Iteration 126/1000 | Loss: 0.00001473
Iteration 127/1000 | Loss: 0.00001472
Iteration 128/1000 | Loss: 0.00001472
Iteration 129/1000 | Loss: 0.00001472
Iteration 130/1000 | Loss: 0.00001472
Iteration 131/1000 | Loss: 0.00001472
Iteration 132/1000 | Loss: 0.00001472
Iteration 133/1000 | Loss: 0.00001472
Iteration 134/1000 | Loss: 0.00001472
Iteration 135/1000 | Loss: 0.00001472
Iteration 136/1000 | Loss: 0.00001472
Iteration 137/1000 | Loss: 0.00001472
Iteration 138/1000 | Loss: 0.00001472
Iteration 139/1000 | Loss: 0.00001472
Iteration 140/1000 | Loss: 0.00001472
Iteration 141/1000 | Loss: 0.00001472
Iteration 142/1000 | Loss: 0.00001471
Iteration 143/1000 | Loss: 0.00001471
Iteration 144/1000 | Loss: 0.00001471
Iteration 145/1000 | Loss: 0.00001471
Iteration 146/1000 | Loss: 0.00001471
Iteration 147/1000 | Loss: 0.00001471
Iteration 148/1000 | Loss: 0.00001471
Iteration 149/1000 | Loss: 0.00001471
Iteration 150/1000 | Loss: 0.00001471
Iteration 151/1000 | Loss: 0.00001471
Iteration 152/1000 | Loss: 0.00001471
Iteration 153/1000 | Loss: 0.00001471
Iteration 154/1000 | Loss: 0.00001470
Iteration 155/1000 | Loss: 0.00001470
Iteration 156/1000 | Loss: 0.00001470
Iteration 157/1000 | Loss: 0.00001470
Iteration 158/1000 | Loss: 0.00001470
Iteration 159/1000 | Loss: 0.00001470
Iteration 160/1000 | Loss: 0.00001470
Iteration 161/1000 | Loss: 0.00001470
Iteration 162/1000 | Loss: 0.00001470
Iteration 163/1000 | Loss: 0.00001470
Iteration 164/1000 | Loss: 0.00001470
Iteration 165/1000 | Loss: 0.00001470
Iteration 166/1000 | Loss: 0.00001470
Iteration 167/1000 | Loss: 0.00001470
Iteration 168/1000 | Loss: 0.00001469
Iteration 169/1000 | Loss: 0.00001469
Iteration 170/1000 | Loss: 0.00001469
Iteration 171/1000 | Loss: 0.00001469
Iteration 172/1000 | Loss: 0.00001469
Iteration 173/1000 | Loss: 0.00001469
Iteration 174/1000 | Loss: 0.00001469
Iteration 175/1000 | Loss: 0.00001469
Iteration 176/1000 | Loss: 0.00001469
Iteration 177/1000 | Loss: 0.00001469
Iteration 178/1000 | Loss: 0.00001469
Iteration 179/1000 | Loss: 0.00001469
Iteration 180/1000 | Loss: 0.00001469
Iteration 181/1000 | Loss: 0.00001469
Iteration 182/1000 | Loss: 0.00001469
Iteration 183/1000 | Loss: 0.00001469
Iteration 184/1000 | Loss: 0.00001468
Iteration 185/1000 | Loss: 0.00001468
Iteration 186/1000 | Loss: 0.00001468
Iteration 187/1000 | Loss: 0.00001468
Iteration 188/1000 | Loss: 0.00001468
Iteration 189/1000 | Loss: 0.00001468
Iteration 190/1000 | Loss: 0.00001468
Iteration 191/1000 | Loss: 0.00001467
Iteration 192/1000 | Loss: 0.00001467
Iteration 193/1000 | Loss: 0.00001467
Iteration 194/1000 | Loss: 0.00001467
Iteration 195/1000 | Loss: 0.00001467
Iteration 196/1000 | Loss: 0.00001466
Iteration 197/1000 | Loss: 0.00001466
Iteration 198/1000 | Loss: 0.00001466
Iteration 199/1000 | Loss: 0.00001466
Iteration 200/1000 | Loss: 0.00001466
Iteration 201/1000 | Loss: 0.00001466
Iteration 202/1000 | Loss: 0.00001466
Iteration 203/1000 | Loss: 0.00001466
Iteration 204/1000 | Loss: 0.00001466
Iteration 205/1000 | Loss: 0.00001466
Iteration 206/1000 | Loss: 0.00001466
Iteration 207/1000 | Loss: 0.00001466
Iteration 208/1000 | Loss: 0.00001466
Iteration 209/1000 | Loss: 0.00001466
Iteration 210/1000 | Loss: 0.00001466
Iteration 211/1000 | Loss: 0.00001466
Iteration 212/1000 | Loss: 0.00001466
Iteration 213/1000 | Loss: 0.00001465
Iteration 214/1000 | Loss: 0.00001465
Iteration 215/1000 | Loss: 0.00001465
Iteration 216/1000 | Loss: 0.00001465
Iteration 217/1000 | Loss: 0.00001464
Iteration 218/1000 | Loss: 0.00001464
Iteration 219/1000 | Loss: 0.00001464
Iteration 220/1000 | Loss: 0.00001464
Iteration 221/1000 | Loss: 0.00001464
Iteration 222/1000 | Loss: 0.00001464
Iteration 223/1000 | Loss: 0.00001464
Iteration 224/1000 | Loss: 0.00001464
Iteration 225/1000 | Loss: 0.00001464
Iteration 226/1000 | Loss: 0.00001463
Iteration 227/1000 | Loss: 0.00001463
Iteration 228/1000 | Loss: 0.00001463
Iteration 229/1000 | Loss: 0.00001463
Iteration 230/1000 | Loss: 0.00001463
Iteration 231/1000 | Loss: 0.00001463
Iteration 232/1000 | Loss: 0.00001463
Iteration 233/1000 | Loss: 0.00001463
Iteration 234/1000 | Loss: 0.00001463
Iteration 235/1000 | Loss: 0.00001463
Iteration 236/1000 | Loss: 0.00001463
Iteration 237/1000 | Loss: 0.00001463
Iteration 238/1000 | Loss: 0.00001463
Iteration 239/1000 | Loss: 0.00001463
Iteration 240/1000 | Loss: 0.00001463
Iteration 241/1000 | Loss: 0.00001463
Iteration 242/1000 | Loss: 0.00001463
Iteration 243/1000 | Loss: 0.00001463
Iteration 244/1000 | Loss: 0.00001463
Iteration 245/1000 | Loss: 0.00001463
Iteration 246/1000 | Loss: 0.00001463
Iteration 247/1000 | Loss: 0.00001463
Iteration 248/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.4633562386734411e-05, 1.4633562386734411e-05, 1.4633562386734411e-05, 1.4633562386734411e-05, 1.4633562386734411e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4633562386734411e-05

Optimization complete. Final v2v error: 3.2441823482513428 mm

Highest mean error: 4.014501094818115 mm for frame 45

Lowest mean error: 2.781672716140747 mm for frame 138

Saving results

Total time: 94.37141799926758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016588
Iteration 2/25 | Loss: 0.01016588
Iteration 3/25 | Loss: 0.01016588
Iteration 4/25 | Loss: 0.01016588
Iteration 5/25 | Loss: 0.01016588
Iteration 6/25 | Loss: 0.01016588
Iteration 7/25 | Loss: 0.01016587
Iteration 8/25 | Loss: 0.00231521
Iteration 9/25 | Loss: 0.00154994
Iteration 10/25 | Loss: 0.00150074
Iteration 11/25 | Loss: 0.00150407
Iteration 12/25 | Loss: 0.00148742
Iteration 13/25 | Loss: 0.00141308
Iteration 14/25 | Loss: 0.00138472
Iteration 15/25 | Loss: 0.00136285
Iteration 16/25 | Loss: 0.00133222
Iteration 17/25 | Loss: 0.00133138
Iteration 18/25 | Loss: 0.00132774
Iteration 19/25 | Loss: 0.00133183
Iteration 20/25 | Loss: 0.00133101
Iteration 21/25 | Loss: 0.00133173
Iteration 22/25 | Loss: 0.00132440
Iteration 23/25 | Loss: 0.00132281
Iteration 24/25 | Loss: 0.00131987
Iteration 25/25 | Loss: 0.00131659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24566412
Iteration 2/25 | Loss: 0.00333549
Iteration 3/25 | Loss: 0.00333549
Iteration 4/25 | Loss: 0.00333548
Iteration 5/25 | Loss: 0.00315164
Iteration 6/25 | Loss: 0.00320224
Iteration 7/25 | Loss: 0.00320223
Iteration 8/25 | Loss: 0.00315167
Iteration 9/25 | Loss: 0.00315153
Iteration 10/25 | Loss: 0.00315153
Iteration 11/25 | Loss: 0.00315153
Iteration 12/25 | Loss: 0.00315153
Iteration 13/25 | Loss: 0.00315153
Iteration 14/25 | Loss: 0.00315153
Iteration 15/25 | Loss: 0.00315153
Iteration 16/25 | Loss: 0.00315153
Iteration 17/25 | Loss: 0.00315153
Iteration 18/25 | Loss: 0.00315153
Iteration 19/25 | Loss: 0.00315153
Iteration 20/25 | Loss: 0.00315153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0031515276059508324, 0.0031515276059508324, 0.0031515276059508324, 0.0031515276059508324, 0.0031515276059508324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031515276059508324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00315153
Iteration 2/1000 | Loss: 0.00025962
Iteration 3/1000 | Loss: 0.00038490
Iteration 4/1000 | Loss: 0.00075981
Iteration 5/1000 | Loss: 0.00020140
Iteration 6/1000 | Loss: 0.00020805
Iteration 7/1000 | Loss: 0.00040788
Iteration 8/1000 | Loss: 0.00034231
Iteration 9/1000 | Loss: 0.00077854
Iteration 10/1000 | Loss: 0.00017746
Iteration 11/1000 | Loss: 0.00015878
Iteration 12/1000 | Loss: 0.00052761
Iteration 13/1000 | Loss: 0.00094249
Iteration 14/1000 | Loss: 0.00036798
Iteration 15/1000 | Loss: 0.00029944
Iteration 16/1000 | Loss: 0.00065251
Iteration 17/1000 | Loss: 0.00153706
Iteration 18/1000 | Loss: 0.00146071
Iteration 19/1000 | Loss: 0.00069269
Iteration 20/1000 | Loss: 0.00061802
Iteration 21/1000 | Loss: 0.00044799
Iteration 22/1000 | Loss: 0.00130843
Iteration 23/1000 | Loss: 0.00165323
Iteration 24/1000 | Loss: 0.00160173
Iteration 25/1000 | Loss: 0.00034172
Iteration 26/1000 | Loss: 0.00041619
Iteration 27/1000 | Loss: 0.00120288
Iteration 28/1000 | Loss: 0.00091961
Iteration 29/1000 | Loss: 0.00157748
Iteration 30/1000 | Loss: 0.00055685
Iteration 31/1000 | Loss: 0.00093613
Iteration 32/1000 | Loss: 0.00025911
Iteration 33/1000 | Loss: 0.00025667
Iteration 34/1000 | Loss: 0.00013796
Iteration 35/1000 | Loss: 0.00023674
Iteration 36/1000 | Loss: 0.00024778
Iteration 37/1000 | Loss: 0.00047184
Iteration 38/1000 | Loss: 0.00044249
Iteration 39/1000 | Loss: 0.00042118
Iteration 40/1000 | Loss: 0.00026953
Iteration 41/1000 | Loss: 0.00022361
Iteration 42/1000 | Loss: 0.00046929
Iteration 43/1000 | Loss: 0.00086979
Iteration 44/1000 | Loss: 0.00024932
Iteration 45/1000 | Loss: 0.00026154
Iteration 46/1000 | Loss: 0.00013583
Iteration 47/1000 | Loss: 0.00021272
Iteration 48/1000 | Loss: 0.00070728
Iteration 49/1000 | Loss: 0.00024116
Iteration 50/1000 | Loss: 0.00023834
Iteration 51/1000 | Loss: 0.00012407
Iteration 52/1000 | Loss: 0.00009607
Iteration 53/1000 | Loss: 0.00037764
Iteration 54/1000 | Loss: 0.00130196
Iteration 55/1000 | Loss: 0.00074756
Iteration 56/1000 | Loss: 0.00016889
Iteration 57/1000 | Loss: 0.00020807
Iteration 58/1000 | Loss: 0.00020279
Iteration 59/1000 | Loss: 0.00010430
Iteration 60/1000 | Loss: 0.00013668
Iteration 61/1000 | Loss: 0.00010287
Iteration 62/1000 | Loss: 0.00009416
Iteration 63/1000 | Loss: 0.00009239
Iteration 64/1000 | Loss: 0.00009763
Iteration 65/1000 | Loss: 0.00008823
Iteration 66/1000 | Loss: 0.00008663
Iteration 67/1000 | Loss: 0.00007137
Iteration 68/1000 | Loss: 0.00022544
Iteration 69/1000 | Loss: 0.00007636
Iteration 70/1000 | Loss: 0.00035925
Iteration 71/1000 | Loss: 0.00009378
Iteration 72/1000 | Loss: 0.00008441
Iteration 73/1000 | Loss: 0.00016171
Iteration 74/1000 | Loss: 0.00012807
Iteration 75/1000 | Loss: 0.00012435
Iteration 76/1000 | Loss: 0.00014645
Iteration 77/1000 | Loss: 0.00016966
Iteration 78/1000 | Loss: 0.00015385
Iteration 79/1000 | Loss: 0.00016211
Iteration 80/1000 | Loss: 0.00042367
Iteration 81/1000 | Loss: 0.00021002
Iteration 82/1000 | Loss: 0.00014551
Iteration 83/1000 | Loss: 0.00021107
Iteration 84/1000 | Loss: 0.00016229
Iteration 85/1000 | Loss: 0.00016037
Iteration 86/1000 | Loss: 0.00016912
Iteration 87/1000 | Loss: 0.00011516
Iteration 88/1000 | Loss: 0.00029126
Iteration 89/1000 | Loss: 0.00007752
Iteration 90/1000 | Loss: 0.00007337
Iteration 91/1000 | Loss: 0.00006726
Iteration 92/1000 | Loss: 0.00006875
Iteration 93/1000 | Loss: 0.00021838
Iteration 94/1000 | Loss: 0.00031332
Iteration 95/1000 | Loss: 0.00052219
Iteration 96/1000 | Loss: 0.00016008
Iteration 97/1000 | Loss: 0.00009375
Iteration 98/1000 | Loss: 0.00008710
Iteration 99/1000 | Loss: 0.00007184
Iteration 100/1000 | Loss: 0.00022509
Iteration 101/1000 | Loss: 0.00022020
Iteration 102/1000 | Loss: 0.00022260
Iteration 103/1000 | Loss: 0.00006505
Iteration 104/1000 | Loss: 0.00007498
Iteration 105/1000 | Loss: 0.00006490
Iteration 106/1000 | Loss: 0.00006714
Iteration 107/1000 | Loss: 0.00029138
Iteration 108/1000 | Loss: 0.00044681
Iteration 109/1000 | Loss: 0.00010383
Iteration 110/1000 | Loss: 0.00022269
Iteration 111/1000 | Loss: 0.00006807
Iteration 112/1000 | Loss: 0.00006674
Iteration 113/1000 | Loss: 0.00007616
Iteration 114/1000 | Loss: 0.00005879
Iteration 115/1000 | Loss: 0.00043717
Iteration 116/1000 | Loss: 0.00024739
Iteration 117/1000 | Loss: 0.00033849
Iteration 118/1000 | Loss: 0.00021692
Iteration 119/1000 | Loss: 0.00028975
Iteration 120/1000 | Loss: 0.00025475
Iteration 121/1000 | Loss: 0.00060941
Iteration 122/1000 | Loss: 0.00037600
Iteration 123/1000 | Loss: 0.00033670
Iteration 124/1000 | Loss: 0.00023947
Iteration 125/1000 | Loss: 0.00011288
Iteration 126/1000 | Loss: 0.00005886
Iteration 127/1000 | Loss: 0.00004914
Iteration 128/1000 | Loss: 0.00012424
Iteration 129/1000 | Loss: 0.00004640
Iteration 130/1000 | Loss: 0.00013005
Iteration 131/1000 | Loss: 0.00011782
Iteration 132/1000 | Loss: 0.00012785
Iteration 133/1000 | Loss: 0.00009606
Iteration 134/1000 | Loss: 0.00010099
Iteration 135/1000 | Loss: 0.00004772
Iteration 136/1000 | Loss: 0.00004399
Iteration 137/1000 | Loss: 0.00004220
Iteration 138/1000 | Loss: 0.00047533
Iteration 139/1000 | Loss: 0.00031141
Iteration 140/1000 | Loss: 0.00013170
Iteration 141/1000 | Loss: 0.00084397
Iteration 142/1000 | Loss: 0.00060170
Iteration 143/1000 | Loss: 0.00031102
Iteration 144/1000 | Loss: 0.00035321
Iteration 145/1000 | Loss: 0.00005906
Iteration 146/1000 | Loss: 0.00004982
Iteration 147/1000 | Loss: 0.00004326
Iteration 148/1000 | Loss: 0.00012934
Iteration 149/1000 | Loss: 0.00003622
Iteration 150/1000 | Loss: 0.00003378
Iteration 151/1000 | Loss: 0.00003664
Iteration 152/1000 | Loss: 0.00003041
Iteration 153/1000 | Loss: 0.00002935
Iteration 154/1000 | Loss: 0.00005927
Iteration 155/1000 | Loss: 0.00003325
Iteration 156/1000 | Loss: 0.00002812
Iteration 157/1000 | Loss: 0.00031444
Iteration 158/1000 | Loss: 0.00115480
Iteration 159/1000 | Loss: 0.00105571
Iteration 160/1000 | Loss: 0.00004215
Iteration 161/1000 | Loss: 0.00008882
Iteration 162/1000 | Loss: 0.00002975
Iteration 163/1000 | Loss: 0.00002726
Iteration 164/1000 | Loss: 0.00022927
Iteration 165/1000 | Loss: 0.00017675
Iteration 166/1000 | Loss: 0.00007325
Iteration 167/1000 | Loss: 0.00009303
Iteration 168/1000 | Loss: 0.00003385
Iteration 169/1000 | Loss: 0.00002449
Iteration 170/1000 | Loss: 0.00002345
Iteration 171/1000 | Loss: 0.00003102
Iteration 172/1000 | Loss: 0.00024590
Iteration 173/1000 | Loss: 0.00021637
Iteration 174/1000 | Loss: 0.00023143
Iteration 175/1000 | Loss: 0.00019161
Iteration 176/1000 | Loss: 0.00008192
Iteration 177/1000 | Loss: 0.00002828
Iteration 178/1000 | Loss: 0.00031639
Iteration 179/1000 | Loss: 0.00100450
Iteration 180/1000 | Loss: 0.00004837
Iteration 181/1000 | Loss: 0.00008281
Iteration 182/1000 | Loss: 0.00019547
Iteration 183/1000 | Loss: 0.00009284
Iteration 184/1000 | Loss: 0.00002623
Iteration 185/1000 | Loss: 0.00002120
Iteration 186/1000 | Loss: 0.00002703
Iteration 187/1000 | Loss: 0.00001807
Iteration 188/1000 | Loss: 0.00001697
Iteration 189/1000 | Loss: 0.00002443
Iteration 190/1000 | Loss: 0.00001583
Iteration 191/1000 | Loss: 0.00006350
Iteration 192/1000 | Loss: 0.00002439
Iteration 193/1000 | Loss: 0.00001527
Iteration 194/1000 | Loss: 0.00002015
Iteration 195/1000 | Loss: 0.00004993
Iteration 196/1000 | Loss: 0.00001483
Iteration 197/1000 | Loss: 0.00001481
Iteration 198/1000 | Loss: 0.00001478
Iteration 199/1000 | Loss: 0.00001478
Iteration 200/1000 | Loss: 0.00001478
Iteration 201/1000 | Loss: 0.00001478
Iteration 202/1000 | Loss: 0.00001477
Iteration 203/1000 | Loss: 0.00001475
Iteration 204/1000 | Loss: 0.00001474
Iteration 205/1000 | Loss: 0.00001473
Iteration 206/1000 | Loss: 0.00001471
Iteration 207/1000 | Loss: 0.00001470
Iteration 208/1000 | Loss: 0.00003492
Iteration 209/1000 | Loss: 0.00001652
Iteration 210/1000 | Loss: 0.00002368
Iteration 211/1000 | Loss: 0.00001577
Iteration 212/1000 | Loss: 0.00001467
Iteration 213/1000 | Loss: 0.00001464
Iteration 214/1000 | Loss: 0.00001464
Iteration 215/1000 | Loss: 0.00001464
Iteration 216/1000 | Loss: 0.00001464
Iteration 217/1000 | Loss: 0.00001464
Iteration 218/1000 | Loss: 0.00001464
Iteration 219/1000 | Loss: 0.00001464
Iteration 220/1000 | Loss: 0.00001464
Iteration 221/1000 | Loss: 0.00001464
Iteration 222/1000 | Loss: 0.00001464
Iteration 223/1000 | Loss: 0.00001464
Iteration 224/1000 | Loss: 0.00001464
Iteration 225/1000 | Loss: 0.00001462
Iteration 226/1000 | Loss: 0.00028432
Iteration 227/1000 | Loss: 0.00001974
Iteration 228/1000 | Loss: 0.00001575
Iteration 229/1000 | Loss: 0.00001605
Iteration 230/1000 | Loss: 0.00001434
Iteration 231/1000 | Loss: 0.00001946
Iteration 232/1000 | Loss: 0.00001332
Iteration 233/1000 | Loss: 0.00001710
Iteration 234/1000 | Loss: 0.00001282
Iteration 235/1000 | Loss: 0.00001280
Iteration 236/1000 | Loss: 0.00001279
Iteration 237/1000 | Loss: 0.00001279
Iteration 238/1000 | Loss: 0.00001279
Iteration 239/1000 | Loss: 0.00001278
Iteration 240/1000 | Loss: 0.00002150
Iteration 241/1000 | Loss: 0.00001275
Iteration 242/1000 | Loss: 0.00001272
Iteration 243/1000 | Loss: 0.00001271
Iteration 244/1000 | Loss: 0.00001271
Iteration 245/1000 | Loss: 0.00017598
Iteration 246/1000 | Loss: 0.00036261
Iteration 247/1000 | Loss: 0.00015318
Iteration 248/1000 | Loss: 0.00001745
Iteration 249/1000 | Loss: 0.00001421
Iteration 250/1000 | Loss: 0.00001364
Iteration 251/1000 | Loss: 0.00001619
Iteration 252/1000 | Loss: 0.00001095
Iteration 253/1000 | Loss: 0.00001020
Iteration 254/1000 | Loss: 0.00000990
Iteration 255/1000 | Loss: 0.00002042
Iteration 256/1000 | Loss: 0.00001041
Iteration 257/1000 | Loss: 0.00000984
Iteration 258/1000 | Loss: 0.00000957
Iteration 259/1000 | Loss: 0.00000957
Iteration 260/1000 | Loss: 0.00000957
Iteration 261/1000 | Loss: 0.00000957
Iteration 262/1000 | Loss: 0.00000957
Iteration 263/1000 | Loss: 0.00000957
Iteration 264/1000 | Loss: 0.00000956
Iteration 265/1000 | Loss: 0.00000956
Iteration 266/1000 | Loss: 0.00000956
Iteration 267/1000 | Loss: 0.00000956
Iteration 268/1000 | Loss: 0.00000956
Iteration 269/1000 | Loss: 0.00000999
Iteration 270/1000 | Loss: 0.00000951
Iteration 271/1000 | Loss: 0.00000950
Iteration 272/1000 | Loss: 0.00000949
Iteration 273/1000 | Loss: 0.00000947
Iteration 274/1000 | Loss: 0.00000946
Iteration 275/1000 | Loss: 0.00000944
Iteration 276/1000 | Loss: 0.00000943
Iteration 277/1000 | Loss: 0.00000939
Iteration 278/1000 | Loss: 0.00000975
Iteration 279/1000 | Loss: 0.00000945
Iteration 280/1000 | Loss: 0.00000948
Iteration 281/1000 | Loss: 0.00000928
Iteration 282/1000 | Loss: 0.00000927
Iteration 283/1000 | Loss: 0.00000926
Iteration 284/1000 | Loss: 0.00000926
Iteration 285/1000 | Loss: 0.00000926
Iteration 286/1000 | Loss: 0.00000925
Iteration 287/1000 | Loss: 0.00000924
Iteration 288/1000 | Loss: 0.00000924
Iteration 289/1000 | Loss: 0.00000924
Iteration 290/1000 | Loss: 0.00000924
Iteration 291/1000 | Loss: 0.00000924
Iteration 292/1000 | Loss: 0.00000924
Iteration 293/1000 | Loss: 0.00000924
Iteration 294/1000 | Loss: 0.00000924
Iteration 295/1000 | Loss: 0.00000924
Iteration 296/1000 | Loss: 0.00000924
Iteration 297/1000 | Loss: 0.00000924
Iteration 298/1000 | Loss: 0.00000924
Iteration 299/1000 | Loss: 0.00000924
Iteration 300/1000 | Loss: 0.00000924
Iteration 301/1000 | Loss: 0.00000924
Iteration 302/1000 | Loss: 0.00000924
Iteration 303/1000 | Loss: 0.00000924
Iteration 304/1000 | Loss: 0.00000924
Iteration 305/1000 | Loss: 0.00000924
Iteration 306/1000 | Loss: 0.00000924
Iteration 307/1000 | Loss: 0.00000924
Iteration 308/1000 | Loss: 0.00000924
Iteration 309/1000 | Loss: 0.00000924
Iteration 310/1000 | Loss: 0.00000924
Iteration 311/1000 | Loss: 0.00000924
Iteration 312/1000 | Loss: 0.00000924
Iteration 313/1000 | Loss: 0.00000924
Iteration 314/1000 | Loss: 0.00000924
Iteration 315/1000 | Loss: 0.00000924
Iteration 316/1000 | Loss: 0.00000924
Iteration 317/1000 | Loss: 0.00000924
Iteration 318/1000 | Loss: 0.00000924
Iteration 319/1000 | Loss: 0.00000924
Iteration 320/1000 | Loss: 0.00000924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 320. Stopping optimization.
Last 5 losses: [9.235667675966397e-06, 9.235667675966397e-06, 9.235667675966397e-06, 9.235667675966397e-06, 9.235667675966397e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.235667675966397e-06

Optimization complete. Final v2v error: 2.6170644760131836 mm

Highest mean error: 5.117651462554932 mm for frame 131

Lowest mean error: 2.2707855701446533 mm for frame 192

Saving results

Total time: 416.0317053794861
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813901
Iteration 2/25 | Loss: 0.00142661
Iteration 3/25 | Loss: 0.00120903
Iteration 4/25 | Loss: 0.00119111
Iteration 5/25 | Loss: 0.00118862
Iteration 6/25 | Loss: 0.00118848
Iteration 7/25 | Loss: 0.00118848
Iteration 8/25 | Loss: 0.00118848
Iteration 9/25 | Loss: 0.00118848
Iteration 10/25 | Loss: 0.00118848
Iteration 11/25 | Loss: 0.00118848
Iteration 12/25 | Loss: 0.00118848
Iteration 13/25 | Loss: 0.00118848
Iteration 14/25 | Loss: 0.00118848
Iteration 15/25 | Loss: 0.00118848
Iteration 16/25 | Loss: 0.00118848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011884799459949136, 0.0011884799459949136, 0.0011884799459949136, 0.0011884799459949136, 0.0011884799459949136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011884799459949136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82951450
Iteration 2/25 | Loss: 0.00103847
Iteration 3/25 | Loss: 0.00103847
Iteration 4/25 | Loss: 0.00103847
Iteration 5/25 | Loss: 0.00103847
Iteration 6/25 | Loss: 0.00103847
Iteration 7/25 | Loss: 0.00103847
Iteration 8/25 | Loss: 0.00103847
Iteration 9/25 | Loss: 0.00103847
Iteration 10/25 | Loss: 0.00103847
Iteration 11/25 | Loss: 0.00103847
Iteration 12/25 | Loss: 0.00103847
Iteration 13/25 | Loss: 0.00103847
Iteration 14/25 | Loss: 0.00103847
Iteration 15/25 | Loss: 0.00103847
Iteration 16/25 | Loss: 0.00103847
Iteration 17/25 | Loss: 0.00103847
Iteration 18/25 | Loss: 0.00103847
Iteration 19/25 | Loss: 0.00103847
Iteration 20/25 | Loss: 0.00103847
Iteration 21/25 | Loss: 0.00103847
Iteration 22/25 | Loss: 0.00103847
Iteration 23/25 | Loss: 0.00103847
Iteration 24/25 | Loss: 0.00103847
Iteration 25/25 | Loss: 0.00103847

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103847
Iteration 2/1000 | Loss: 0.00003183
Iteration 3/1000 | Loss: 0.00002418
Iteration 4/1000 | Loss: 0.00002207
Iteration 5/1000 | Loss: 0.00002099
Iteration 6/1000 | Loss: 0.00002037
Iteration 7/1000 | Loss: 0.00001986
Iteration 8/1000 | Loss: 0.00001952
Iteration 9/1000 | Loss: 0.00001925
Iteration 10/1000 | Loss: 0.00001909
Iteration 11/1000 | Loss: 0.00001904
Iteration 12/1000 | Loss: 0.00001904
Iteration 13/1000 | Loss: 0.00001903
Iteration 14/1000 | Loss: 0.00001888
Iteration 15/1000 | Loss: 0.00001876
Iteration 16/1000 | Loss: 0.00001875
Iteration 17/1000 | Loss: 0.00001870
Iteration 18/1000 | Loss: 0.00001869
Iteration 19/1000 | Loss: 0.00001869
Iteration 20/1000 | Loss: 0.00001869
Iteration 21/1000 | Loss: 0.00001868
Iteration 22/1000 | Loss: 0.00001868
Iteration 23/1000 | Loss: 0.00001868
Iteration 24/1000 | Loss: 0.00001868
Iteration 25/1000 | Loss: 0.00001868
Iteration 26/1000 | Loss: 0.00001868
Iteration 27/1000 | Loss: 0.00001865
Iteration 28/1000 | Loss: 0.00001864
Iteration 29/1000 | Loss: 0.00001860
Iteration 30/1000 | Loss: 0.00001859
Iteration 31/1000 | Loss: 0.00001857
Iteration 32/1000 | Loss: 0.00001857
Iteration 33/1000 | Loss: 0.00001857
Iteration 34/1000 | Loss: 0.00001857
Iteration 35/1000 | Loss: 0.00001857
Iteration 36/1000 | Loss: 0.00001857
Iteration 37/1000 | Loss: 0.00001856
Iteration 38/1000 | Loss: 0.00001856
Iteration 39/1000 | Loss: 0.00001855
Iteration 40/1000 | Loss: 0.00001855
Iteration 41/1000 | Loss: 0.00001855
Iteration 42/1000 | Loss: 0.00001853
Iteration 43/1000 | Loss: 0.00001852
Iteration 44/1000 | Loss: 0.00001852
Iteration 45/1000 | Loss: 0.00001852
Iteration 46/1000 | Loss: 0.00001851
Iteration 47/1000 | Loss: 0.00001851
Iteration 48/1000 | Loss: 0.00001851
Iteration 49/1000 | Loss: 0.00001851
Iteration 50/1000 | Loss: 0.00001851
Iteration 51/1000 | Loss: 0.00001850
Iteration 52/1000 | Loss: 0.00001850
Iteration 53/1000 | Loss: 0.00001848
Iteration 54/1000 | Loss: 0.00001848
Iteration 55/1000 | Loss: 0.00001848
Iteration 56/1000 | Loss: 0.00001848
Iteration 57/1000 | Loss: 0.00001848
Iteration 58/1000 | Loss: 0.00001848
Iteration 59/1000 | Loss: 0.00001848
Iteration 60/1000 | Loss: 0.00001848
Iteration 61/1000 | Loss: 0.00001848
Iteration 62/1000 | Loss: 0.00001848
Iteration 63/1000 | Loss: 0.00001847
Iteration 64/1000 | Loss: 0.00001847
Iteration 65/1000 | Loss: 0.00001847
Iteration 66/1000 | Loss: 0.00001846
Iteration 67/1000 | Loss: 0.00001846
Iteration 68/1000 | Loss: 0.00001846
Iteration 69/1000 | Loss: 0.00001846
Iteration 70/1000 | Loss: 0.00001846
Iteration 71/1000 | Loss: 0.00001846
Iteration 72/1000 | Loss: 0.00001846
Iteration 73/1000 | Loss: 0.00001846
Iteration 74/1000 | Loss: 0.00001846
Iteration 75/1000 | Loss: 0.00001845
Iteration 76/1000 | Loss: 0.00001845
Iteration 77/1000 | Loss: 0.00001845
Iteration 78/1000 | Loss: 0.00001845
Iteration 79/1000 | Loss: 0.00001844
Iteration 80/1000 | Loss: 0.00001844
Iteration 81/1000 | Loss: 0.00001844
Iteration 82/1000 | Loss: 0.00001844
Iteration 83/1000 | Loss: 0.00001844
Iteration 84/1000 | Loss: 0.00001844
Iteration 85/1000 | Loss: 0.00001844
Iteration 86/1000 | Loss: 0.00001844
Iteration 87/1000 | Loss: 0.00001843
Iteration 88/1000 | Loss: 0.00001843
Iteration 89/1000 | Loss: 0.00001843
Iteration 90/1000 | Loss: 0.00001843
Iteration 91/1000 | Loss: 0.00001843
Iteration 92/1000 | Loss: 0.00001843
Iteration 93/1000 | Loss: 0.00001842
Iteration 94/1000 | Loss: 0.00001842
Iteration 95/1000 | Loss: 0.00001842
Iteration 96/1000 | Loss: 0.00001842
Iteration 97/1000 | Loss: 0.00001841
Iteration 98/1000 | Loss: 0.00001841
Iteration 99/1000 | Loss: 0.00001841
Iteration 100/1000 | Loss: 0.00001841
Iteration 101/1000 | Loss: 0.00001841
Iteration 102/1000 | Loss: 0.00001841
Iteration 103/1000 | Loss: 0.00001841
Iteration 104/1000 | Loss: 0.00001841
Iteration 105/1000 | Loss: 0.00001840
Iteration 106/1000 | Loss: 0.00001840
Iteration 107/1000 | Loss: 0.00001840
Iteration 108/1000 | Loss: 0.00001840
Iteration 109/1000 | Loss: 0.00001840
Iteration 110/1000 | Loss: 0.00001840
Iteration 111/1000 | Loss: 0.00001839
Iteration 112/1000 | Loss: 0.00001839
Iteration 113/1000 | Loss: 0.00001839
Iteration 114/1000 | Loss: 0.00001838
Iteration 115/1000 | Loss: 0.00001838
Iteration 116/1000 | Loss: 0.00001838
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001836
Iteration 120/1000 | Loss: 0.00001836
Iteration 121/1000 | Loss: 0.00001836
Iteration 122/1000 | Loss: 0.00001836
Iteration 123/1000 | Loss: 0.00001836
Iteration 124/1000 | Loss: 0.00001835
Iteration 125/1000 | Loss: 0.00001835
Iteration 126/1000 | Loss: 0.00001835
Iteration 127/1000 | Loss: 0.00001835
Iteration 128/1000 | Loss: 0.00001835
Iteration 129/1000 | Loss: 0.00001835
Iteration 130/1000 | Loss: 0.00001835
Iteration 131/1000 | Loss: 0.00001835
Iteration 132/1000 | Loss: 0.00001835
Iteration 133/1000 | Loss: 0.00001835
Iteration 134/1000 | Loss: 0.00001835
Iteration 135/1000 | Loss: 0.00001835
Iteration 136/1000 | Loss: 0.00001835
Iteration 137/1000 | Loss: 0.00001835
Iteration 138/1000 | Loss: 0.00001835
Iteration 139/1000 | Loss: 0.00001835
Iteration 140/1000 | Loss: 0.00001835
Iteration 141/1000 | Loss: 0.00001835
Iteration 142/1000 | Loss: 0.00001835
Iteration 143/1000 | Loss: 0.00001835
Iteration 144/1000 | Loss: 0.00001835
Iteration 145/1000 | Loss: 0.00001835
Iteration 146/1000 | Loss: 0.00001835
Iteration 147/1000 | Loss: 0.00001835
Iteration 148/1000 | Loss: 0.00001835
Iteration 149/1000 | Loss: 0.00001835
Iteration 150/1000 | Loss: 0.00001835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.835205875977408e-05, 1.835205875977408e-05, 1.835205875977408e-05, 1.835205875977408e-05, 1.835205875977408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.835205875977408e-05

Optimization complete. Final v2v error: 3.612128496170044 mm

Highest mean error: 3.795729637145996 mm for frame 121

Lowest mean error: 3.4812865257263184 mm for frame 137

Saving results

Total time: 35.3177535533905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00940219
Iteration 2/25 | Loss: 0.00170744
Iteration 3/25 | Loss: 0.00133285
Iteration 4/25 | Loss: 0.00128944
Iteration 5/25 | Loss: 0.00128569
Iteration 6/25 | Loss: 0.00128693
Iteration 7/25 | Loss: 0.00127132
Iteration 8/25 | Loss: 0.00126206
Iteration 9/25 | Loss: 0.00126122
Iteration 10/25 | Loss: 0.00124616
Iteration 11/25 | Loss: 0.00123589
Iteration 12/25 | Loss: 0.00123368
Iteration 13/25 | Loss: 0.00123328
Iteration 14/25 | Loss: 0.00123313
Iteration 15/25 | Loss: 0.00123305
Iteration 16/25 | Loss: 0.00123305
Iteration 17/25 | Loss: 0.00123305
Iteration 18/25 | Loss: 0.00123305
Iteration 19/25 | Loss: 0.00123305
Iteration 20/25 | Loss: 0.00123305
Iteration 21/25 | Loss: 0.00123305
Iteration 22/25 | Loss: 0.00123305
Iteration 23/25 | Loss: 0.00123305
Iteration 24/25 | Loss: 0.00123305
Iteration 25/25 | Loss: 0.00123304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.81276393
Iteration 2/25 | Loss: 0.00165738
Iteration 3/25 | Loss: 0.00165737
Iteration 4/25 | Loss: 0.00165737
Iteration 5/25 | Loss: 0.00165737
Iteration 6/25 | Loss: 0.00165736
Iteration 7/25 | Loss: 0.00165736
Iteration 8/25 | Loss: 0.00165736
Iteration 9/25 | Loss: 0.00165736
Iteration 10/25 | Loss: 0.00165736
Iteration 11/25 | Loss: 0.00165736
Iteration 12/25 | Loss: 0.00165736
Iteration 13/25 | Loss: 0.00165736
Iteration 14/25 | Loss: 0.00165736
Iteration 15/25 | Loss: 0.00165736
Iteration 16/25 | Loss: 0.00165736
Iteration 17/25 | Loss: 0.00165736
Iteration 18/25 | Loss: 0.00165736
Iteration 19/25 | Loss: 0.00165736
Iteration 20/25 | Loss: 0.00165736
Iteration 21/25 | Loss: 0.00165736
Iteration 22/25 | Loss: 0.00165736
Iteration 23/25 | Loss: 0.00165736
Iteration 24/25 | Loss: 0.00165736
Iteration 25/25 | Loss: 0.00165736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165736
Iteration 2/1000 | Loss: 0.00003756
Iteration 3/1000 | Loss: 0.00002670
Iteration 4/1000 | Loss: 0.00002424
Iteration 5/1000 | Loss: 0.00002313
Iteration 6/1000 | Loss: 0.00002251
Iteration 7/1000 | Loss: 0.00002195
Iteration 8/1000 | Loss: 0.00002159
Iteration 9/1000 | Loss: 0.00002132
Iteration 10/1000 | Loss: 0.00002112
Iteration 11/1000 | Loss: 0.00002094
Iteration 12/1000 | Loss: 0.00002089
Iteration 13/1000 | Loss: 0.00002074
Iteration 14/1000 | Loss: 0.00002073
Iteration 15/1000 | Loss: 0.00002073
Iteration 16/1000 | Loss: 0.00002072
Iteration 17/1000 | Loss: 0.00002072
Iteration 18/1000 | Loss: 0.00002071
Iteration 19/1000 | Loss: 0.00002068
Iteration 20/1000 | Loss: 0.00002067
Iteration 21/1000 | Loss: 0.00002066
Iteration 22/1000 | Loss: 0.00002060
Iteration 23/1000 | Loss: 0.00002060
Iteration 24/1000 | Loss: 0.00002059
Iteration 25/1000 | Loss: 0.00002055
Iteration 26/1000 | Loss: 0.00002049
Iteration 27/1000 | Loss: 0.00002049
Iteration 28/1000 | Loss: 0.00002047
Iteration 29/1000 | Loss: 0.00002046
Iteration 30/1000 | Loss: 0.00002045
Iteration 31/1000 | Loss: 0.00002045
Iteration 32/1000 | Loss: 0.00002041
Iteration 33/1000 | Loss: 0.00002040
Iteration 34/1000 | Loss: 0.00002039
Iteration 35/1000 | Loss: 0.00002039
Iteration 36/1000 | Loss: 0.00002038
Iteration 37/1000 | Loss: 0.00002038
Iteration 38/1000 | Loss: 0.00002037
Iteration 39/1000 | Loss: 0.00002037
Iteration 40/1000 | Loss: 0.00002036
Iteration 41/1000 | Loss: 0.00002036
Iteration 42/1000 | Loss: 0.00002036
Iteration 43/1000 | Loss: 0.00002035
Iteration 44/1000 | Loss: 0.00002035
Iteration 45/1000 | Loss: 0.00002035
Iteration 46/1000 | Loss: 0.00002034
Iteration 47/1000 | Loss: 0.00002033
Iteration 48/1000 | Loss: 0.00002032
Iteration 49/1000 | Loss: 0.00002031
Iteration 50/1000 | Loss: 0.00002031
Iteration 51/1000 | Loss: 0.00002031
Iteration 52/1000 | Loss: 0.00002031
Iteration 53/1000 | Loss: 0.00002030
Iteration 54/1000 | Loss: 0.00002030
Iteration 55/1000 | Loss: 0.00002029
Iteration 56/1000 | Loss: 0.00002029
Iteration 57/1000 | Loss: 0.00002028
Iteration 58/1000 | Loss: 0.00002027
Iteration 59/1000 | Loss: 0.00002026
Iteration 60/1000 | Loss: 0.00002026
Iteration 61/1000 | Loss: 0.00002026
Iteration 62/1000 | Loss: 0.00002026
Iteration 63/1000 | Loss: 0.00002025
Iteration 64/1000 | Loss: 0.00002025
Iteration 65/1000 | Loss: 0.00002024
Iteration 66/1000 | Loss: 0.00002024
Iteration 67/1000 | Loss: 0.00002023
Iteration 68/1000 | Loss: 0.00002023
Iteration 69/1000 | Loss: 0.00002023
Iteration 70/1000 | Loss: 0.00002023
Iteration 71/1000 | Loss: 0.00002023
Iteration 72/1000 | Loss: 0.00002022
Iteration 73/1000 | Loss: 0.00002022
Iteration 74/1000 | Loss: 0.00002022
Iteration 75/1000 | Loss: 0.00002022
Iteration 76/1000 | Loss: 0.00002021
Iteration 77/1000 | Loss: 0.00002021
Iteration 78/1000 | Loss: 0.00002021
Iteration 79/1000 | Loss: 0.00002021
Iteration 80/1000 | Loss: 0.00002021
Iteration 81/1000 | Loss: 0.00002021
Iteration 82/1000 | Loss: 0.00002020
Iteration 83/1000 | Loss: 0.00002019
Iteration 84/1000 | Loss: 0.00002019
Iteration 85/1000 | Loss: 0.00002019
Iteration 86/1000 | Loss: 0.00002019
Iteration 87/1000 | Loss: 0.00002019
Iteration 88/1000 | Loss: 0.00002019
Iteration 89/1000 | Loss: 0.00002019
Iteration 90/1000 | Loss: 0.00002019
Iteration 91/1000 | Loss: 0.00002018
Iteration 92/1000 | Loss: 0.00002018
Iteration 93/1000 | Loss: 0.00002018
Iteration 94/1000 | Loss: 0.00002018
Iteration 95/1000 | Loss: 0.00002017
Iteration 96/1000 | Loss: 0.00002017
Iteration 97/1000 | Loss: 0.00002017
Iteration 98/1000 | Loss: 0.00002017
Iteration 99/1000 | Loss: 0.00002017
Iteration 100/1000 | Loss: 0.00002016
Iteration 101/1000 | Loss: 0.00002016
Iteration 102/1000 | Loss: 0.00002016
Iteration 103/1000 | Loss: 0.00002015
Iteration 104/1000 | Loss: 0.00002015
Iteration 105/1000 | Loss: 0.00002015
Iteration 106/1000 | Loss: 0.00002015
Iteration 107/1000 | Loss: 0.00002014
Iteration 108/1000 | Loss: 0.00002014
Iteration 109/1000 | Loss: 0.00002014
Iteration 110/1000 | Loss: 0.00002013
Iteration 111/1000 | Loss: 0.00002013
Iteration 112/1000 | Loss: 0.00002013
Iteration 113/1000 | Loss: 0.00002013
Iteration 114/1000 | Loss: 0.00002012
Iteration 115/1000 | Loss: 0.00002012
Iteration 116/1000 | Loss: 0.00002012
Iteration 117/1000 | Loss: 0.00002012
Iteration 118/1000 | Loss: 0.00002012
Iteration 119/1000 | Loss: 0.00002012
Iteration 120/1000 | Loss: 0.00002012
Iteration 121/1000 | Loss: 0.00002012
Iteration 122/1000 | Loss: 0.00002012
Iteration 123/1000 | Loss: 0.00002012
Iteration 124/1000 | Loss: 0.00002012
Iteration 125/1000 | Loss: 0.00002011
Iteration 126/1000 | Loss: 0.00002011
Iteration 127/1000 | Loss: 0.00002011
Iteration 128/1000 | Loss: 0.00002011
Iteration 129/1000 | Loss: 0.00002011
Iteration 130/1000 | Loss: 0.00002011
Iteration 131/1000 | Loss: 0.00002011
Iteration 132/1000 | Loss: 0.00002011
Iteration 133/1000 | Loss: 0.00002011
Iteration 134/1000 | Loss: 0.00002011
Iteration 135/1000 | Loss: 0.00002011
Iteration 136/1000 | Loss: 0.00002011
Iteration 137/1000 | Loss: 0.00002010
Iteration 138/1000 | Loss: 0.00002010
Iteration 139/1000 | Loss: 0.00002010
Iteration 140/1000 | Loss: 0.00002010
Iteration 141/1000 | Loss: 0.00002010
Iteration 142/1000 | Loss: 0.00002010
Iteration 143/1000 | Loss: 0.00002010
Iteration 144/1000 | Loss: 0.00002010
Iteration 145/1000 | Loss: 0.00002010
Iteration 146/1000 | Loss: 0.00002010
Iteration 147/1000 | Loss: 0.00002010
Iteration 148/1000 | Loss: 0.00002010
Iteration 149/1000 | Loss: 0.00002010
Iteration 150/1000 | Loss: 0.00002010
Iteration 151/1000 | Loss: 0.00002010
Iteration 152/1000 | Loss: 0.00002010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.0103490896872245e-05, 2.0103490896872245e-05, 2.0103490896872245e-05, 2.0103490896872245e-05, 2.0103490896872245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0103490896872245e-05

Optimization complete. Final v2v error: 3.077540636062622 mm

Highest mean error: 20.42608642578125 mm for frame 83

Lowest mean error: 2.437204122543335 mm for frame 33

Saving results

Total time: 56.33795666694641
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781815
Iteration 2/25 | Loss: 0.00123186
Iteration 3/25 | Loss: 0.00114439
Iteration 4/25 | Loss: 0.00113545
Iteration 5/25 | Loss: 0.00113354
Iteration 6/25 | Loss: 0.00113354
Iteration 7/25 | Loss: 0.00113354
Iteration 8/25 | Loss: 0.00113354
Iteration 9/25 | Loss: 0.00113354
Iteration 10/25 | Loss: 0.00113354
Iteration 11/25 | Loss: 0.00113354
Iteration 12/25 | Loss: 0.00113354
Iteration 13/25 | Loss: 0.00113354
Iteration 14/25 | Loss: 0.00113354
Iteration 15/25 | Loss: 0.00113354
Iteration 16/25 | Loss: 0.00113354
Iteration 17/25 | Loss: 0.00113354
Iteration 18/25 | Loss: 0.00113354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011335444869473577, 0.0011335444869473577, 0.0011335444869473577, 0.0011335444869473577, 0.0011335444869473577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011335444869473577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24923205
Iteration 2/25 | Loss: 0.00181770
Iteration 3/25 | Loss: 0.00181770
Iteration 4/25 | Loss: 0.00181770
Iteration 5/25 | Loss: 0.00181770
Iteration 6/25 | Loss: 0.00181770
Iteration 7/25 | Loss: 0.00181770
Iteration 8/25 | Loss: 0.00181770
Iteration 9/25 | Loss: 0.00181770
Iteration 10/25 | Loss: 0.00181770
Iteration 11/25 | Loss: 0.00181770
Iteration 12/25 | Loss: 0.00181770
Iteration 13/25 | Loss: 0.00181770
Iteration 14/25 | Loss: 0.00181770
Iteration 15/25 | Loss: 0.00181770
Iteration 16/25 | Loss: 0.00181770
Iteration 17/25 | Loss: 0.00181770
Iteration 18/25 | Loss: 0.00181770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018176964949816465, 0.0018176964949816465, 0.0018176964949816465, 0.0018176964949816465, 0.0018176964949816465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018176964949816465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181770
Iteration 2/1000 | Loss: 0.00001952
Iteration 3/1000 | Loss: 0.00001334
Iteration 4/1000 | Loss: 0.00001168
Iteration 5/1000 | Loss: 0.00001086
Iteration 6/1000 | Loss: 0.00001023
Iteration 7/1000 | Loss: 0.00000969
Iteration 8/1000 | Loss: 0.00000940
Iteration 9/1000 | Loss: 0.00000908
Iteration 10/1000 | Loss: 0.00000888
Iteration 11/1000 | Loss: 0.00000887
Iteration 12/1000 | Loss: 0.00000878
Iteration 13/1000 | Loss: 0.00000866
Iteration 14/1000 | Loss: 0.00000864
Iteration 15/1000 | Loss: 0.00000858
Iteration 16/1000 | Loss: 0.00000858
Iteration 17/1000 | Loss: 0.00000856
Iteration 18/1000 | Loss: 0.00000855
Iteration 19/1000 | Loss: 0.00000853
Iteration 20/1000 | Loss: 0.00000853
Iteration 21/1000 | Loss: 0.00000853
Iteration 22/1000 | Loss: 0.00000851
Iteration 23/1000 | Loss: 0.00000848
Iteration 24/1000 | Loss: 0.00000847
Iteration 25/1000 | Loss: 0.00000847
Iteration 26/1000 | Loss: 0.00000846
Iteration 27/1000 | Loss: 0.00000846
Iteration 28/1000 | Loss: 0.00000842
Iteration 29/1000 | Loss: 0.00000840
Iteration 30/1000 | Loss: 0.00000839
Iteration 31/1000 | Loss: 0.00000836
Iteration 32/1000 | Loss: 0.00000831
Iteration 33/1000 | Loss: 0.00000827
Iteration 34/1000 | Loss: 0.00000827
Iteration 35/1000 | Loss: 0.00000826
Iteration 36/1000 | Loss: 0.00000826
Iteration 37/1000 | Loss: 0.00000826
Iteration 38/1000 | Loss: 0.00000825
Iteration 39/1000 | Loss: 0.00000825
Iteration 40/1000 | Loss: 0.00000824
Iteration 41/1000 | Loss: 0.00000823
Iteration 42/1000 | Loss: 0.00000822
Iteration 43/1000 | Loss: 0.00000822
Iteration 44/1000 | Loss: 0.00000821
Iteration 45/1000 | Loss: 0.00000820
Iteration 46/1000 | Loss: 0.00000819
Iteration 47/1000 | Loss: 0.00000819
Iteration 48/1000 | Loss: 0.00000819
Iteration 49/1000 | Loss: 0.00000818
Iteration 50/1000 | Loss: 0.00000818
Iteration 51/1000 | Loss: 0.00000818
Iteration 52/1000 | Loss: 0.00000817
Iteration 53/1000 | Loss: 0.00000817
Iteration 54/1000 | Loss: 0.00000817
Iteration 55/1000 | Loss: 0.00000816
Iteration 56/1000 | Loss: 0.00000816
Iteration 57/1000 | Loss: 0.00000816
Iteration 58/1000 | Loss: 0.00000816
Iteration 59/1000 | Loss: 0.00000816
Iteration 60/1000 | Loss: 0.00000815
Iteration 61/1000 | Loss: 0.00000815
Iteration 62/1000 | Loss: 0.00000815
Iteration 63/1000 | Loss: 0.00000814
Iteration 64/1000 | Loss: 0.00000814
Iteration 65/1000 | Loss: 0.00000814
Iteration 66/1000 | Loss: 0.00000814
Iteration 67/1000 | Loss: 0.00000814
Iteration 68/1000 | Loss: 0.00000814
Iteration 69/1000 | Loss: 0.00000814
Iteration 70/1000 | Loss: 0.00000814
Iteration 71/1000 | Loss: 0.00000814
Iteration 72/1000 | Loss: 0.00000814
Iteration 73/1000 | Loss: 0.00000813
Iteration 74/1000 | Loss: 0.00000813
Iteration 75/1000 | Loss: 0.00000813
Iteration 76/1000 | Loss: 0.00000813
Iteration 77/1000 | Loss: 0.00000813
Iteration 78/1000 | Loss: 0.00000812
Iteration 79/1000 | Loss: 0.00000812
Iteration 80/1000 | Loss: 0.00000812
Iteration 81/1000 | Loss: 0.00000811
Iteration 82/1000 | Loss: 0.00000811
Iteration 83/1000 | Loss: 0.00000811
Iteration 84/1000 | Loss: 0.00000810
Iteration 85/1000 | Loss: 0.00000810
Iteration 86/1000 | Loss: 0.00000809
Iteration 87/1000 | Loss: 0.00000809
Iteration 88/1000 | Loss: 0.00000809
Iteration 89/1000 | Loss: 0.00000809
Iteration 90/1000 | Loss: 0.00000809
Iteration 91/1000 | Loss: 0.00000808
Iteration 92/1000 | Loss: 0.00000808
Iteration 93/1000 | Loss: 0.00000808
Iteration 94/1000 | Loss: 0.00000808
Iteration 95/1000 | Loss: 0.00000808
Iteration 96/1000 | Loss: 0.00000808
Iteration 97/1000 | Loss: 0.00000808
Iteration 98/1000 | Loss: 0.00000808
Iteration 99/1000 | Loss: 0.00000807
Iteration 100/1000 | Loss: 0.00000807
Iteration 101/1000 | Loss: 0.00000807
Iteration 102/1000 | Loss: 0.00000807
Iteration 103/1000 | Loss: 0.00000807
Iteration 104/1000 | Loss: 0.00000807
Iteration 105/1000 | Loss: 0.00000806
Iteration 106/1000 | Loss: 0.00000806
Iteration 107/1000 | Loss: 0.00000806
Iteration 108/1000 | Loss: 0.00000806
Iteration 109/1000 | Loss: 0.00000806
Iteration 110/1000 | Loss: 0.00000805
Iteration 111/1000 | Loss: 0.00000805
Iteration 112/1000 | Loss: 0.00000805
Iteration 113/1000 | Loss: 0.00000805
Iteration 114/1000 | Loss: 0.00000805
Iteration 115/1000 | Loss: 0.00000805
Iteration 116/1000 | Loss: 0.00000805
Iteration 117/1000 | Loss: 0.00000805
Iteration 118/1000 | Loss: 0.00000805
Iteration 119/1000 | Loss: 0.00000804
Iteration 120/1000 | Loss: 0.00000804
Iteration 121/1000 | Loss: 0.00000804
Iteration 122/1000 | Loss: 0.00000804
Iteration 123/1000 | Loss: 0.00000804
Iteration 124/1000 | Loss: 0.00000804
Iteration 125/1000 | Loss: 0.00000804
Iteration 126/1000 | Loss: 0.00000804
Iteration 127/1000 | Loss: 0.00000804
Iteration 128/1000 | Loss: 0.00000804
Iteration 129/1000 | Loss: 0.00000804
Iteration 130/1000 | Loss: 0.00000804
Iteration 131/1000 | Loss: 0.00000804
Iteration 132/1000 | Loss: 0.00000804
Iteration 133/1000 | Loss: 0.00000803
Iteration 134/1000 | Loss: 0.00000803
Iteration 135/1000 | Loss: 0.00000803
Iteration 136/1000 | Loss: 0.00000802
Iteration 137/1000 | Loss: 0.00000802
Iteration 138/1000 | Loss: 0.00000802
Iteration 139/1000 | Loss: 0.00000802
Iteration 140/1000 | Loss: 0.00000802
Iteration 141/1000 | Loss: 0.00000801
Iteration 142/1000 | Loss: 0.00000801
Iteration 143/1000 | Loss: 0.00000801
Iteration 144/1000 | Loss: 0.00000801
Iteration 145/1000 | Loss: 0.00000801
Iteration 146/1000 | Loss: 0.00000801
Iteration 147/1000 | Loss: 0.00000801
Iteration 148/1000 | Loss: 0.00000801
Iteration 149/1000 | Loss: 0.00000801
Iteration 150/1000 | Loss: 0.00000801
Iteration 151/1000 | Loss: 0.00000801
Iteration 152/1000 | Loss: 0.00000801
Iteration 153/1000 | Loss: 0.00000801
Iteration 154/1000 | Loss: 0.00000801
Iteration 155/1000 | Loss: 0.00000801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [8.01337773737032e-06, 8.01337773737032e-06, 8.01337773737032e-06, 8.01337773737032e-06, 8.01337773737032e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.01337773737032e-06

Optimization complete. Final v2v error: 2.4530134201049805 mm

Highest mean error: 2.627026319503784 mm for frame 78

Lowest mean error: 2.325953483581543 mm for frame 172

Saving results

Total time: 37.18270540237427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800828
Iteration 2/25 | Loss: 0.00139444
Iteration 3/25 | Loss: 0.00122000
Iteration 4/25 | Loss: 0.00119059
Iteration 5/25 | Loss: 0.00118224
Iteration 6/25 | Loss: 0.00117968
Iteration 7/25 | Loss: 0.00117940
Iteration 8/25 | Loss: 0.00117940
Iteration 9/25 | Loss: 0.00117940
Iteration 10/25 | Loss: 0.00117940
Iteration 11/25 | Loss: 0.00117940
Iteration 12/25 | Loss: 0.00117940
Iteration 13/25 | Loss: 0.00117940
Iteration 14/25 | Loss: 0.00117940
Iteration 15/25 | Loss: 0.00117940
Iteration 16/25 | Loss: 0.00117940
Iteration 17/25 | Loss: 0.00117940
Iteration 18/25 | Loss: 0.00117940
Iteration 19/25 | Loss: 0.00117940
Iteration 20/25 | Loss: 0.00117940
Iteration 21/25 | Loss: 0.00117940
Iteration 22/25 | Loss: 0.00117940
Iteration 23/25 | Loss: 0.00117940
Iteration 24/25 | Loss: 0.00117940
Iteration 25/25 | Loss: 0.00117940

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.34328890
Iteration 2/25 | Loss: 0.00279901
Iteration 3/25 | Loss: 0.00279899
Iteration 4/25 | Loss: 0.00279899
Iteration 5/25 | Loss: 0.00279899
Iteration 6/25 | Loss: 0.00279899
Iteration 7/25 | Loss: 0.00279899
Iteration 8/25 | Loss: 0.00279899
Iteration 9/25 | Loss: 0.00279899
Iteration 10/25 | Loss: 0.00279899
Iteration 11/25 | Loss: 0.00279899
Iteration 12/25 | Loss: 0.00279899
Iteration 13/25 | Loss: 0.00279899
Iteration 14/25 | Loss: 0.00279899
Iteration 15/25 | Loss: 0.00279899
Iteration 16/25 | Loss: 0.00279899
Iteration 17/25 | Loss: 0.00279899
Iteration 18/25 | Loss: 0.00279899
Iteration 19/25 | Loss: 0.00279899
Iteration 20/25 | Loss: 0.00279899
Iteration 21/25 | Loss: 0.00279899
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0027989856898784637, 0.0027989856898784637, 0.0027989856898784637, 0.0027989856898784637, 0.0027989856898784637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027989856898784637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00279899
Iteration 2/1000 | Loss: 0.00004171
Iteration 3/1000 | Loss: 0.00002474
Iteration 4/1000 | Loss: 0.00002020
Iteration 5/1000 | Loss: 0.00001877
Iteration 6/1000 | Loss: 0.00001779
Iteration 7/1000 | Loss: 0.00001722
Iteration 8/1000 | Loss: 0.00001685
Iteration 9/1000 | Loss: 0.00001645
Iteration 10/1000 | Loss: 0.00001620
Iteration 11/1000 | Loss: 0.00001602
Iteration 12/1000 | Loss: 0.00001585
Iteration 13/1000 | Loss: 0.00001578
Iteration 14/1000 | Loss: 0.00001577
Iteration 15/1000 | Loss: 0.00001563
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001557
Iteration 18/1000 | Loss: 0.00001555
Iteration 19/1000 | Loss: 0.00001554
Iteration 20/1000 | Loss: 0.00001552
Iteration 21/1000 | Loss: 0.00001551
Iteration 22/1000 | Loss: 0.00001550
Iteration 23/1000 | Loss: 0.00001548
Iteration 24/1000 | Loss: 0.00001547
Iteration 25/1000 | Loss: 0.00001544
Iteration 26/1000 | Loss: 0.00001543
Iteration 27/1000 | Loss: 0.00001543
Iteration 28/1000 | Loss: 0.00001542
Iteration 29/1000 | Loss: 0.00001540
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001534
Iteration 32/1000 | Loss: 0.00001530
Iteration 33/1000 | Loss: 0.00001529
Iteration 34/1000 | Loss: 0.00001529
Iteration 35/1000 | Loss: 0.00001528
Iteration 36/1000 | Loss: 0.00001528
Iteration 37/1000 | Loss: 0.00001527
Iteration 38/1000 | Loss: 0.00001527
Iteration 39/1000 | Loss: 0.00001527
Iteration 40/1000 | Loss: 0.00001526
Iteration 41/1000 | Loss: 0.00001526
Iteration 42/1000 | Loss: 0.00001526
Iteration 43/1000 | Loss: 0.00001526
Iteration 44/1000 | Loss: 0.00001526
Iteration 45/1000 | Loss: 0.00001526
Iteration 46/1000 | Loss: 0.00001526
Iteration 47/1000 | Loss: 0.00001526
Iteration 48/1000 | Loss: 0.00001526
Iteration 49/1000 | Loss: 0.00001525
Iteration 50/1000 | Loss: 0.00001525
Iteration 51/1000 | Loss: 0.00001525
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001524
Iteration 54/1000 | Loss: 0.00001524
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001524
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001524
Iteration 59/1000 | Loss: 0.00001524
Iteration 60/1000 | Loss: 0.00001524
Iteration 61/1000 | Loss: 0.00001524
Iteration 62/1000 | Loss: 0.00001524
Iteration 63/1000 | Loss: 0.00001523
Iteration 64/1000 | Loss: 0.00001523
Iteration 65/1000 | Loss: 0.00001523
Iteration 66/1000 | Loss: 0.00001523
Iteration 67/1000 | Loss: 0.00001523
Iteration 68/1000 | Loss: 0.00001523
Iteration 69/1000 | Loss: 0.00001523
Iteration 70/1000 | Loss: 0.00001523
Iteration 71/1000 | Loss: 0.00001523
Iteration 72/1000 | Loss: 0.00001523
Iteration 73/1000 | Loss: 0.00001523
Iteration 74/1000 | Loss: 0.00001523
Iteration 75/1000 | Loss: 0.00001523
Iteration 76/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.5226863979478367e-05, 1.5226863979478367e-05, 1.5226863979478367e-05, 1.5226863979478367e-05, 1.5226863979478367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5226863979478367e-05

Optimization complete. Final v2v error: 3.349130392074585 mm

Highest mean error: 4.020925045013428 mm for frame 10

Lowest mean error: 2.539388656616211 mm for frame 148

Saving results

Total time: 36.00052189826965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00212244
Iteration 2/25 | Loss: 0.00119849
Iteration 3/25 | Loss: 0.00112612
Iteration 4/25 | Loss: 0.00110864
Iteration 5/25 | Loss: 0.00110095
Iteration 6/25 | Loss: 0.00109811
Iteration 7/25 | Loss: 0.00109766
Iteration 8/25 | Loss: 0.00109766
Iteration 9/25 | Loss: 0.00109766
Iteration 10/25 | Loss: 0.00109766
Iteration 11/25 | Loss: 0.00109766
Iteration 12/25 | Loss: 0.00109766
Iteration 13/25 | Loss: 0.00109766
Iteration 14/25 | Loss: 0.00109766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001097658067010343, 0.001097658067010343, 0.001097658067010343, 0.001097658067010343, 0.001097658067010343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001097658067010343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19415116
Iteration 2/25 | Loss: 0.00349522
Iteration 3/25 | Loss: 0.00349521
Iteration 4/25 | Loss: 0.00349521
Iteration 5/25 | Loss: 0.00349521
Iteration 6/25 | Loss: 0.00349521
Iteration 7/25 | Loss: 0.00349521
Iteration 8/25 | Loss: 0.00349521
Iteration 9/25 | Loss: 0.00349521
Iteration 10/25 | Loss: 0.00349521
Iteration 11/25 | Loss: 0.00349521
Iteration 12/25 | Loss: 0.00349521
Iteration 13/25 | Loss: 0.00349521
Iteration 14/25 | Loss: 0.00349521
Iteration 15/25 | Loss: 0.00349521
Iteration 16/25 | Loss: 0.00349521
Iteration 17/25 | Loss: 0.00349521
Iteration 18/25 | Loss: 0.00349521
Iteration 19/25 | Loss: 0.00349521
Iteration 20/25 | Loss: 0.00349521
Iteration 21/25 | Loss: 0.00349521
Iteration 22/25 | Loss: 0.00349521
Iteration 23/25 | Loss: 0.00349521
Iteration 24/25 | Loss: 0.00349521
Iteration 25/25 | Loss: 0.00349521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00349521
Iteration 2/1000 | Loss: 0.00003425
Iteration 3/1000 | Loss: 0.00002079
Iteration 4/1000 | Loss: 0.00001545
Iteration 5/1000 | Loss: 0.00001405
Iteration 6/1000 | Loss: 0.00001335
Iteration 7/1000 | Loss: 0.00001229
Iteration 8/1000 | Loss: 0.00001176
Iteration 9/1000 | Loss: 0.00001141
Iteration 10/1000 | Loss: 0.00001115
Iteration 11/1000 | Loss: 0.00001102
Iteration 12/1000 | Loss: 0.00001087
Iteration 13/1000 | Loss: 0.00001087
Iteration 14/1000 | Loss: 0.00001080
Iteration 15/1000 | Loss: 0.00001079
Iteration 16/1000 | Loss: 0.00001078
Iteration 17/1000 | Loss: 0.00001078
Iteration 18/1000 | Loss: 0.00001078
Iteration 19/1000 | Loss: 0.00001078
Iteration 20/1000 | Loss: 0.00001078
Iteration 21/1000 | Loss: 0.00001078
Iteration 22/1000 | Loss: 0.00001078
Iteration 23/1000 | Loss: 0.00001078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [1.078258082998218e-05, 1.078258082998218e-05, 1.078258082998218e-05, 1.078258082998218e-05, 1.078258082998218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.078258082998218e-05

Optimization complete. Final v2v error: 2.8315470218658447 mm

Highest mean error: 3.137361526489258 mm for frame 6

Lowest mean error: 2.588688611984253 mm for frame 172

Saving results

Total time: 27.728079319000244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967329
Iteration 2/25 | Loss: 0.00315539
Iteration 3/25 | Loss: 0.00216161
Iteration 4/25 | Loss: 0.00198857
Iteration 5/25 | Loss: 0.00188754
Iteration 6/25 | Loss: 0.00183207
Iteration 7/25 | Loss: 0.00183568
Iteration 8/25 | Loss: 0.00182381
Iteration 9/25 | Loss: 0.00176556
Iteration 10/25 | Loss: 0.00165564
Iteration 11/25 | Loss: 0.00158203
Iteration 12/25 | Loss: 0.00152844
Iteration 13/25 | Loss: 0.00148954
Iteration 14/25 | Loss: 0.00147203
Iteration 15/25 | Loss: 0.00144801
Iteration 16/25 | Loss: 0.00142949
Iteration 17/25 | Loss: 0.00142060
Iteration 18/25 | Loss: 0.00141640
Iteration 19/25 | Loss: 0.00140964
Iteration 20/25 | Loss: 0.00140017
Iteration 21/25 | Loss: 0.00139609
Iteration 22/25 | Loss: 0.00139304
Iteration 23/25 | Loss: 0.00139207
Iteration 24/25 | Loss: 0.00139261
Iteration 25/25 | Loss: 0.00139096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21003199
Iteration 2/25 | Loss: 0.00324545
Iteration 3/25 | Loss: 0.00295308
Iteration 4/25 | Loss: 0.00295308
Iteration 5/25 | Loss: 0.00295307
Iteration 6/25 | Loss: 0.00295307
Iteration 7/25 | Loss: 0.00295307
Iteration 8/25 | Loss: 0.00295307
Iteration 9/25 | Loss: 0.00295307
Iteration 10/25 | Loss: 0.00295307
Iteration 11/25 | Loss: 0.00295307
Iteration 12/25 | Loss: 0.00295307
Iteration 13/25 | Loss: 0.00295307
Iteration 14/25 | Loss: 0.00295307
Iteration 15/25 | Loss: 0.00295307
Iteration 16/25 | Loss: 0.00295307
Iteration 17/25 | Loss: 0.00295307
Iteration 18/25 | Loss: 0.00295307
Iteration 19/25 | Loss: 0.00295307
Iteration 20/25 | Loss: 0.00295307
Iteration 21/25 | Loss: 0.00295307
Iteration 22/25 | Loss: 0.00295307
Iteration 23/25 | Loss: 0.00295307
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0029530723113566637, 0.0029530723113566637, 0.0029530723113566637, 0.0029530723113566637, 0.0029530723113566637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029530723113566637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00295307
Iteration 2/1000 | Loss: 0.00053096
Iteration 3/1000 | Loss: 0.00104443
Iteration 4/1000 | Loss: 0.00028503
Iteration 5/1000 | Loss: 0.00029878
Iteration 6/1000 | Loss: 0.00018094
Iteration 7/1000 | Loss: 0.00103206
Iteration 8/1000 | Loss: 0.00017980
Iteration 9/1000 | Loss: 0.00031395
Iteration 10/1000 | Loss: 0.00017001
Iteration 11/1000 | Loss: 0.00013254
Iteration 12/1000 | Loss: 0.00029569
Iteration 13/1000 | Loss: 0.00016480
Iteration 14/1000 | Loss: 0.00017288
Iteration 15/1000 | Loss: 0.00019378
Iteration 16/1000 | Loss: 0.00047696
Iteration 17/1000 | Loss: 0.00105159
Iteration 18/1000 | Loss: 0.00043877
Iteration 19/1000 | Loss: 0.00028831
Iteration 20/1000 | Loss: 0.00057932
Iteration 21/1000 | Loss: 0.00021297
Iteration 22/1000 | Loss: 0.00027776
Iteration 23/1000 | Loss: 0.00052746
Iteration 24/1000 | Loss: 0.00042252
Iteration 25/1000 | Loss: 0.00045635
Iteration 26/1000 | Loss: 0.00059288
Iteration 27/1000 | Loss: 0.00067412
Iteration 28/1000 | Loss: 0.00023016
Iteration 29/1000 | Loss: 0.00019211
Iteration 30/1000 | Loss: 0.00048715
Iteration 31/1000 | Loss: 0.00014939
Iteration 32/1000 | Loss: 0.00017167
Iteration 33/1000 | Loss: 0.00088286
Iteration 34/1000 | Loss: 0.00020592
Iteration 35/1000 | Loss: 0.00018592
Iteration 36/1000 | Loss: 0.00014911
Iteration 37/1000 | Loss: 0.00021598
Iteration 38/1000 | Loss: 0.00018485
Iteration 39/1000 | Loss: 0.00042308
Iteration 40/1000 | Loss: 0.00020271
Iteration 41/1000 | Loss: 0.00043889
Iteration 42/1000 | Loss: 0.00013191
Iteration 43/1000 | Loss: 0.00012880
Iteration 44/1000 | Loss: 0.00012216
Iteration 45/1000 | Loss: 0.00012962
Iteration 46/1000 | Loss: 0.00011528
Iteration 47/1000 | Loss: 0.00011973
Iteration 48/1000 | Loss: 0.00012066
Iteration 49/1000 | Loss: 0.00011410
Iteration 50/1000 | Loss: 0.00011262
Iteration 51/1000 | Loss: 0.00010530
Iteration 52/1000 | Loss: 0.00011494
Iteration 53/1000 | Loss: 0.00069129
Iteration 54/1000 | Loss: 0.00013259
Iteration 55/1000 | Loss: 0.00038531
Iteration 56/1000 | Loss: 0.00239911
Iteration 57/1000 | Loss: 0.00268978
Iteration 58/1000 | Loss: 0.00176285
Iteration 59/1000 | Loss: 0.00099089
Iteration 60/1000 | Loss: 0.00236846
Iteration 61/1000 | Loss: 0.00111960
Iteration 62/1000 | Loss: 0.00016666
Iteration 63/1000 | Loss: 0.00012563
Iteration 64/1000 | Loss: 0.00023735
Iteration 65/1000 | Loss: 0.00040787
Iteration 66/1000 | Loss: 0.00034656
Iteration 67/1000 | Loss: 0.00010019
Iteration 68/1000 | Loss: 0.00035746
Iteration 69/1000 | Loss: 0.00036240
Iteration 70/1000 | Loss: 0.00028609
Iteration 71/1000 | Loss: 0.00035443
Iteration 72/1000 | Loss: 0.00026061
Iteration 73/1000 | Loss: 0.00021294
Iteration 74/1000 | Loss: 0.00016426
Iteration 75/1000 | Loss: 0.00028323
Iteration 76/1000 | Loss: 0.00006715
Iteration 77/1000 | Loss: 0.00009163
Iteration 78/1000 | Loss: 0.00006065
Iteration 79/1000 | Loss: 0.00058179
Iteration 80/1000 | Loss: 0.00058451
Iteration 81/1000 | Loss: 0.00051238
Iteration 82/1000 | Loss: 0.00008915
Iteration 83/1000 | Loss: 0.00007341
Iteration 84/1000 | Loss: 0.00028453
Iteration 85/1000 | Loss: 0.00024029
Iteration 86/1000 | Loss: 0.00015922
Iteration 87/1000 | Loss: 0.00018334
Iteration 88/1000 | Loss: 0.00036621
Iteration 89/1000 | Loss: 0.00023417
Iteration 90/1000 | Loss: 0.00070842
Iteration 91/1000 | Loss: 0.00053512
Iteration 92/1000 | Loss: 0.00009964
Iteration 93/1000 | Loss: 0.00019931
Iteration 94/1000 | Loss: 0.00022217
Iteration 95/1000 | Loss: 0.00006072
Iteration 96/1000 | Loss: 0.00023909
Iteration 97/1000 | Loss: 0.00029237
Iteration 98/1000 | Loss: 0.00019223
Iteration 99/1000 | Loss: 0.00010455
Iteration 100/1000 | Loss: 0.00005335
Iteration 101/1000 | Loss: 0.00017748
Iteration 102/1000 | Loss: 0.00015167
Iteration 103/1000 | Loss: 0.00023437
Iteration 104/1000 | Loss: 0.00019983
Iteration 105/1000 | Loss: 0.00018971
Iteration 106/1000 | Loss: 0.00005632
Iteration 107/1000 | Loss: 0.00005199
Iteration 108/1000 | Loss: 0.00006503
Iteration 109/1000 | Loss: 0.00006001
Iteration 110/1000 | Loss: 0.00016703
Iteration 111/1000 | Loss: 0.00007352
Iteration 112/1000 | Loss: 0.00006611
Iteration 113/1000 | Loss: 0.00008756
Iteration 114/1000 | Loss: 0.00006124
Iteration 115/1000 | Loss: 0.00006881
Iteration 116/1000 | Loss: 0.00004652
Iteration 117/1000 | Loss: 0.00010351
Iteration 118/1000 | Loss: 0.00004447
Iteration 119/1000 | Loss: 0.00004370
Iteration 120/1000 | Loss: 0.00004325
Iteration 121/1000 | Loss: 0.00028256
Iteration 122/1000 | Loss: 0.00023237
Iteration 123/1000 | Loss: 0.00015132
Iteration 124/1000 | Loss: 0.00006625
Iteration 125/1000 | Loss: 0.00007677
Iteration 126/1000 | Loss: 0.00025076
Iteration 127/1000 | Loss: 0.00014299
Iteration 128/1000 | Loss: 0.00026277
Iteration 129/1000 | Loss: 0.00025416
Iteration 130/1000 | Loss: 0.00009256
Iteration 131/1000 | Loss: 0.00005124
Iteration 132/1000 | Loss: 0.00004807
Iteration 133/1000 | Loss: 0.00004653
Iteration 134/1000 | Loss: 0.00004561
Iteration 135/1000 | Loss: 0.00012759
Iteration 136/1000 | Loss: 0.00004816
Iteration 137/1000 | Loss: 0.00005278
Iteration 138/1000 | Loss: 0.00023379
Iteration 139/1000 | Loss: 0.00007656
Iteration 140/1000 | Loss: 0.00008167
Iteration 141/1000 | Loss: 0.00005072
Iteration 142/1000 | Loss: 0.00034267
Iteration 143/1000 | Loss: 0.00013875
Iteration 144/1000 | Loss: 0.00025107
Iteration 145/1000 | Loss: 0.00019624
Iteration 146/1000 | Loss: 0.00032127
Iteration 147/1000 | Loss: 0.00016336
Iteration 148/1000 | Loss: 0.00025664
Iteration 149/1000 | Loss: 0.00005978
Iteration 150/1000 | Loss: 0.00005168
Iteration 151/1000 | Loss: 0.00028018
Iteration 152/1000 | Loss: 0.00041702
Iteration 153/1000 | Loss: 0.00043844
Iteration 154/1000 | Loss: 0.00042317
Iteration 155/1000 | Loss: 0.00040357
Iteration 156/1000 | Loss: 0.00014089
Iteration 157/1000 | Loss: 0.00020169
Iteration 158/1000 | Loss: 0.00051917
Iteration 159/1000 | Loss: 0.00025073
Iteration 160/1000 | Loss: 0.00020323
Iteration 161/1000 | Loss: 0.00007257
Iteration 162/1000 | Loss: 0.00020084
Iteration 163/1000 | Loss: 0.00019300
Iteration 164/1000 | Loss: 0.00025616
Iteration 165/1000 | Loss: 0.00037310
Iteration 166/1000 | Loss: 0.00028028
Iteration 167/1000 | Loss: 0.00005614
Iteration 168/1000 | Loss: 0.00017412
Iteration 169/1000 | Loss: 0.00026738
Iteration 170/1000 | Loss: 0.00036046
Iteration 171/1000 | Loss: 0.00062684
Iteration 172/1000 | Loss: 0.00016652
Iteration 173/1000 | Loss: 0.00007435
Iteration 174/1000 | Loss: 0.00004873
Iteration 175/1000 | Loss: 0.00008605
Iteration 176/1000 | Loss: 0.00010441
Iteration 177/1000 | Loss: 0.00006113
Iteration 178/1000 | Loss: 0.00036535
Iteration 179/1000 | Loss: 0.00039406
Iteration 180/1000 | Loss: 0.00043487
Iteration 181/1000 | Loss: 0.00005273
Iteration 182/1000 | Loss: 0.00004822
Iteration 183/1000 | Loss: 0.00010638
Iteration 184/1000 | Loss: 0.00043727
Iteration 185/1000 | Loss: 0.00010187
Iteration 186/1000 | Loss: 0.00004980
Iteration 187/1000 | Loss: 0.00004789
Iteration 188/1000 | Loss: 0.00004698
Iteration 189/1000 | Loss: 0.00004613
Iteration 190/1000 | Loss: 0.00007631
Iteration 191/1000 | Loss: 0.00004800
Iteration 192/1000 | Loss: 0.00009795
Iteration 193/1000 | Loss: 0.00009166
Iteration 194/1000 | Loss: 0.00007989
Iteration 195/1000 | Loss: 0.00006713
Iteration 196/1000 | Loss: 0.00007949
Iteration 197/1000 | Loss: 0.00006245
Iteration 198/1000 | Loss: 0.00007858
Iteration 199/1000 | Loss: 0.00008864
Iteration 200/1000 | Loss: 0.00027141
Iteration 201/1000 | Loss: 0.00011301
Iteration 202/1000 | Loss: 0.00007623
Iteration 203/1000 | Loss: 0.00030254
Iteration 204/1000 | Loss: 0.00022872
Iteration 205/1000 | Loss: 0.00005861
Iteration 206/1000 | Loss: 0.00007842
Iteration 207/1000 | Loss: 0.00006130
Iteration 208/1000 | Loss: 0.00008636
Iteration 209/1000 | Loss: 0.00006581
Iteration 210/1000 | Loss: 0.00004635
Iteration 211/1000 | Loss: 0.00009159
Iteration 212/1000 | Loss: 0.00007759
Iteration 213/1000 | Loss: 0.00012764
Iteration 214/1000 | Loss: 0.00007682
Iteration 215/1000 | Loss: 0.00013527
Iteration 216/1000 | Loss: 0.00017701
Iteration 217/1000 | Loss: 0.00015983
Iteration 218/1000 | Loss: 0.00009438
Iteration 219/1000 | Loss: 0.00026031
Iteration 220/1000 | Loss: 0.00022637
Iteration 221/1000 | Loss: 0.00015399
Iteration 222/1000 | Loss: 0.00005939
Iteration 223/1000 | Loss: 0.00004868
Iteration 224/1000 | Loss: 0.00004629
Iteration 225/1000 | Loss: 0.00048607
Iteration 226/1000 | Loss: 0.00167130
Iteration 227/1000 | Loss: 0.00313348
Iteration 228/1000 | Loss: 0.00282319
Iteration 229/1000 | Loss: 0.00392930
Iteration 230/1000 | Loss: 0.00048183
Iteration 231/1000 | Loss: 0.00065237
Iteration 232/1000 | Loss: 0.00019971
Iteration 233/1000 | Loss: 0.00006419
Iteration 234/1000 | Loss: 0.00005541
Iteration 235/1000 | Loss: 0.00004921
Iteration 236/1000 | Loss: 0.00004589
Iteration 237/1000 | Loss: 0.00004216
Iteration 238/1000 | Loss: 0.00004058
Iteration 239/1000 | Loss: 0.00009270
Iteration 240/1000 | Loss: 0.00003954
Iteration 241/1000 | Loss: 0.00007555
Iteration 242/1000 | Loss: 0.00003885
Iteration 243/1000 | Loss: 0.00003847
Iteration 244/1000 | Loss: 0.00003842
Iteration 245/1000 | Loss: 0.00009329
Iteration 246/1000 | Loss: 0.00004018
Iteration 247/1000 | Loss: 0.00003831
Iteration 248/1000 | Loss: 0.00003805
Iteration 249/1000 | Loss: 0.00003801
Iteration 250/1000 | Loss: 0.00003801
Iteration 251/1000 | Loss: 0.00003796
Iteration 252/1000 | Loss: 0.00010139
Iteration 253/1000 | Loss: 0.00005318
Iteration 254/1000 | Loss: 0.00005438
Iteration 255/1000 | Loss: 0.00004104
Iteration 256/1000 | Loss: 0.00004060
Iteration 257/1000 | Loss: 0.00004030
Iteration 258/1000 | Loss: 0.00004412
Iteration 259/1000 | Loss: 0.00003798
Iteration 260/1000 | Loss: 0.00003798
Iteration 261/1000 | Loss: 0.00003798
Iteration 262/1000 | Loss: 0.00003798
Iteration 263/1000 | Loss: 0.00003798
Iteration 264/1000 | Loss: 0.00003798
Iteration 265/1000 | Loss: 0.00003797
Iteration 266/1000 | Loss: 0.00003797
Iteration 267/1000 | Loss: 0.00003797
Iteration 268/1000 | Loss: 0.00003795
Iteration 269/1000 | Loss: 0.00003795
Iteration 270/1000 | Loss: 0.00003795
Iteration 271/1000 | Loss: 0.00003795
Iteration 272/1000 | Loss: 0.00003795
Iteration 273/1000 | Loss: 0.00003795
Iteration 274/1000 | Loss: 0.00003795
Iteration 275/1000 | Loss: 0.00003795
Iteration 276/1000 | Loss: 0.00003795
Iteration 277/1000 | Loss: 0.00003795
Iteration 278/1000 | Loss: 0.00003795
Iteration 279/1000 | Loss: 0.00003795
Iteration 280/1000 | Loss: 0.00003795
Iteration 281/1000 | Loss: 0.00003795
Iteration 282/1000 | Loss: 0.00003795
Iteration 283/1000 | Loss: 0.00003795
Iteration 284/1000 | Loss: 0.00003795
Iteration 285/1000 | Loss: 0.00003795
Iteration 286/1000 | Loss: 0.00003795
Iteration 287/1000 | Loss: 0.00003795
Iteration 288/1000 | Loss: 0.00003795
Iteration 289/1000 | Loss: 0.00003795
Iteration 290/1000 | Loss: 0.00003795
Iteration 291/1000 | Loss: 0.00003795
Iteration 292/1000 | Loss: 0.00003795
Iteration 293/1000 | Loss: 0.00003795
Iteration 294/1000 | Loss: 0.00003795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [3.794740405282937e-05, 3.794740405282937e-05, 3.794740405282937e-05, 3.794740405282937e-05, 3.794740405282937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.794740405282937e-05

Optimization complete. Final v2v error: 3.747434139251709 mm

Highest mean error: 11.720014572143555 mm for frame 40

Lowest mean error: 2.9424102306365967 mm for frame 138

Saving results

Total time: 408.0417892932892
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968425
Iteration 2/25 | Loss: 0.00167290
Iteration 3/25 | Loss: 0.00137563
Iteration 4/25 | Loss: 0.00134188
Iteration 5/25 | Loss: 0.00133629
Iteration 6/25 | Loss: 0.00133542
Iteration 7/25 | Loss: 0.00133542
Iteration 8/25 | Loss: 0.00133542
Iteration 9/25 | Loss: 0.00133542
Iteration 10/25 | Loss: 0.00133542
Iteration 11/25 | Loss: 0.00133542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013354249531403184, 0.0013354249531403184, 0.0013354249531403184, 0.0013354249531403184, 0.0013354249531403184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013354249531403184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93310881
Iteration 2/25 | Loss: 0.00160681
Iteration 3/25 | Loss: 0.00160679
Iteration 4/25 | Loss: 0.00160679
Iteration 5/25 | Loss: 0.00160678
Iteration 6/25 | Loss: 0.00160678
Iteration 7/25 | Loss: 0.00160678
Iteration 8/25 | Loss: 0.00160678
Iteration 9/25 | Loss: 0.00160678
Iteration 10/25 | Loss: 0.00160678
Iteration 11/25 | Loss: 0.00160678
Iteration 12/25 | Loss: 0.00160678
Iteration 13/25 | Loss: 0.00160678
Iteration 14/25 | Loss: 0.00160678
Iteration 15/25 | Loss: 0.00160678
Iteration 16/25 | Loss: 0.00160678
Iteration 17/25 | Loss: 0.00160678
Iteration 18/25 | Loss: 0.00160678
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016067818505689502, 0.0016067818505689502, 0.0016067818505689502, 0.0016067818505689502, 0.0016067818505689502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016067818505689502

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160678
Iteration 2/1000 | Loss: 0.00006580
Iteration 3/1000 | Loss: 0.00004136
Iteration 4/1000 | Loss: 0.00003344
Iteration 5/1000 | Loss: 0.00003143
Iteration 6/1000 | Loss: 0.00003032
Iteration 7/1000 | Loss: 0.00002953
Iteration 8/1000 | Loss: 0.00002886
Iteration 9/1000 | Loss: 0.00002832
Iteration 10/1000 | Loss: 0.00002800
Iteration 11/1000 | Loss: 0.00002770
Iteration 12/1000 | Loss: 0.00002752
Iteration 13/1000 | Loss: 0.00002730
Iteration 14/1000 | Loss: 0.00002709
Iteration 15/1000 | Loss: 0.00002691
Iteration 16/1000 | Loss: 0.00002682
Iteration 17/1000 | Loss: 0.00002676
Iteration 18/1000 | Loss: 0.00002671
Iteration 19/1000 | Loss: 0.00002670
Iteration 20/1000 | Loss: 0.00002670
Iteration 21/1000 | Loss: 0.00002668
Iteration 22/1000 | Loss: 0.00002668
Iteration 23/1000 | Loss: 0.00002668
Iteration 24/1000 | Loss: 0.00002667
Iteration 25/1000 | Loss: 0.00002667
Iteration 26/1000 | Loss: 0.00002666
Iteration 27/1000 | Loss: 0.00002666
Iteration 28/1000 | Loss: 0.00002665
Iteration 29/1000 | Loss: 0.00002663
Iteration 30/1000 | Loss: 0.00002662
Iteration 31/1000 | Loss: 0.00002661
Iteration 32/1000 | Loss: 0.00002660
Iteration 33/1000 | Loss: 0.00002660
Iteration 34/1000 | Loss: 0.00002660
Iteration 35/1000 | Loss: 0.00002660
Iteration 36/1000 | Loss: 0.00002659
Iteration 37/1000 | Loss: 0.00002659
Iteration 38/1000 | Loss: 0.00002658
Iteration 39/1000 | Loss: 0.00002658
Iteration 40/1000 | Loss: 0.00002657
Iteration 41/1000 | Loss: 0.00002657
Iteration 42/1000 | Loss: 0.00002656
Iteration 43/1000 | Loss: 0.00002656
Iteration 44/1000 | Loss: 0.00002655
Iteration 45/1000 | Loss: 0.00002655
Iteration 46/1000 | Loss: 0.00002654
Iteration 47/1000 | Loss: 0.00002654
Iteration 48/1000 | Loss: 0.00002654
Iteration 49/1000 | Loss: 0.00002653
Iteration 50/1000 | Loss: 0.00002653
Iteration 51/1000 | Loss: 0.00002652
Iteration 52/1000 | Loss: 0.00002652
Iteration 53/1000 | Loss: 0.00002651
Iteration 54/1000 | Loss: 0.00002651
Iteration 55/1000 | Loss: 0.00002651
Iteration 56/1000 | Loss: 0.00002650
Iteration 57/1000 | Loss: 0.00002649
Iteration 58/1000 | Loss: 0.00002649
Iteration 59/1000 | Loss: 0.00002649
Iteration 60/1000 | Loss: 0.00002649
Iteration 61/1000 | Loss: 0.00002649
Iteration 62/1000 | Loss: 0.00002649
Iteration 63/1000 | Loss: 0.00002649
Iteration 64/1000 | Loss: 0.00002649
Iteration 65/1000 | Loss: 0.00002648
Iteration 66/1000 | Loss: 0.00002648
Iteration 67/1000 | Loss: 0.00002648
Iteration 68/1000 | Loss: 0.00002648
Iteration 69/1000 | Loss: 0.00002648
Iteration 70/1000 | Loss: 0.00002648
Iteration 71/1000 | Loss: 0.00002647
Iteration 72/1000 | Loss: 0.00002646
Iteration 73/1000 | Loss: 0.00002646
Iteration 74/1000 | Loss: 0.00002646
Iteration 75/1000 | Loss: 0.00002646
Iteration 76/1000 | Loss: 0.00002646
Iteration 77/1000 | Loss: 0.00002646
Iteration 78/1000 | Loss: 0.00002646
Iteration 79/1000 | Loss: 0.00002645
Iteration 80/1000 | Loss: 0.00002645
Iteration 81/1000 | Loss: 0.00002645
Iteration 82/1000 | Loss: 0.00002645
Iteration 83/1000 | Loss: 0.00002644
Iteration 84/1000 | Loss: 0.00002644
Iteration 85/1000 | Loss: 0.00002644
Iteration 86/1000 | Loss: 0.00002643
Iteration 87/1000 | Loss: 0.00002643
Iteration 88/1000 | Loss: 0.00002642
Iteration 89/1000 | Loss: 0.00002642
Iteration 90/1000 | Loss: 0.00002642
Iteration 91/1000 | Loss: 0.00002641
Iteration 92/1000 | Loss: 0.00002641
Iteration 93/1000 | Loss: 0.00002641
Iteration 94/1000 | Loss: 0.00002641
Iteration 95/1000 | Loss: 0.00002640
Iteration 96/1000 | Loss: 0.00002640
Iteration 97/1000 | Loss: 0.00002640
Iteration 98/1000 | Loss: 0.00002639
Iteration 99/1000 | Loss: 0.00002639
Iteration 100/1000 | Loss: 0.00002639
Iteration 101/1000 | Loss: 0.00002639
Iteration 102/1000 | Loss: 0.00002639
Iteration 103/1000 | Loss: 0.00002638
Iteration 104/1000 | Loss: 0.00002638
Iteration 105/1000 | Loss: 0.00002638
Iteration 106/1000 | Loss: 0.00002638
Iteration 107/1000 | Loss: 0.00002638
Iteration 108/1000 | Loss: 0.00002638
Iteration 109/1000 | Loss: 0.00002638
Iteration 110/1000 | Loss: 0.00002638
Iteration 111/1000 | Loss: 0.00002638
Iteration 112/1000 | Loss: 0.00002638
Iteration 113/1000 | Loss: 0.00002638
Iteration 114/1000 | Loss: 0.00002638
Iteration 115/1000 | Loss: 0.00002638
Iteration 116/1000 | Loss: 0.00002638
Iteration 117/1000 | Loss: 0.00002638
Iteration 118/1000 | Loss: 0.00002638
Iteration 119/1000 | Loss: 0.00002638
Iteration 120/1000 | Loss: 0.00002638
Iteration 121/1000 | Loss: 0.00002638
Iteration 122/1000 | Loss: 0.00002638
Iteration 123/1000 | Loss: 0.00002638
Iteration 124/1000 | Loss: 0.00002638
Iteration 125/1000 | Loss: 0.00002638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.6378613256383687e-05, 2.6378613256383687e-05, 2.6378613256383687e-05, 2.6378613256383687e-05, 2.6378613256383687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6378613256383687e-05

Optimization complete. Final v2v error: 4.1851348876953125 mm

Highest mean error: 4.987459659576416 mm for frame 81

Lowest mean error: 3.210803985595703 mm for frame 25

Saving results

Total time: 44.258565187454224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996895
Iteration 2/25 | Loss: 0.00226823
Iteration 3/25 | Loss: 0.00176430
Iteration 4/25 | Loss: 0.00165138
Iteration 5/25 | Loss: 0.00171567
Iteration 6/25 | Loss: 0.00171424
Iteration 7/25 | Loss: 0.00160762
Iteration 8/25 | Loss: 0.00152446
Iteration 9/25 | Loss: 0.00147444
Iteration 10/25 | Loss: 0.00143198
Iteration 11/25 | Loss: 0.00141234
Iteration 12/25 | Loss: 0.00139588
Iteration 13/25 | Loss: 0.00139266
Iteration 14/25 | Loss: 0.00137563
Iteration 15/25 | Loss: 0.00137485
Iteration 16/25 | Loss: 0.00136828
Iteration 17/25 | Loss: 0.00136325
Iteration 18/25 | Loss: 0.00135702
Iteration 19/25 | Loss: 0.00136221
Iteration 20/25 | Loss: 0.00135801
Iteration 21/25 | Loss: 0.00135727
Iteration 22/25 | Loss: 0.00135636
Iteration 23/25 | Loss: 0.00135460
Iteration 24/25 | Loss: 0.00134790
Iteration 25/25 | Loss: 0.00134659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24867153
Iteration 2/25 | Loss: 0.00330305
Iteration 3/25 | Loss: 0.00311838
Iteration 4/25 | Loss: 0.00311838
Iteration 5/25 | Loss: 0.00311838
Iteration 6/25 | Loss: 0.00311838
Iteration 7/25 | Loss: 0.00311838
Iteration 8/25 | Loss: 0.00311838
Iteration 9/25 | Loss: 0.00311838
Iteration 10/25 | Loss: 0.00311838
Iteration 11/25 | Loss: 0.00311838
Iteration 12/25 | Loss: 0.00311838
Iteration 13/25 | Loss: 0.00311838
Iteration 14/25 | Loss: 0.00311838
Iteration 15/25 | Loss: 0.00311838
Iteration 16/25 | Loss: 0.00311838
Iteration 17/25 | Loss: 0.00311838
Iteration 18/25 | Loss: 0.00311838
Iteration 19/25 | Loss: 0.00311838
Iteration 20/25 | Loss: 0.00311838
Iteration 21/25 | Loss: 0.00311838
Iteration 22/25 | Loss: 0.00311838
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0031183790415525436, 0.0031183790415525436, 0.0031183790415525436, 0.0031183790415525436, 0.0031183790415525436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031183790415525436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00311838
Iteration 2/1000 | Loss: 0.00092945
Iteration 3/1000 | Loss: 0.00153782
Iteration 4/1000 | Loss: 0.00027004
Iteration 5/1000 | Loss: 0.00022812
Iteration 6/1000 | Loss: 0.00025069
Iteration 7/1000 | Loss: 0.00029917
Iteration 8/1000 | Loss: 0.00070488
Iteration 9/1000 | Loss: 0.00044845
Iteration 10/1000 | Loss: 0.00044361
Iteration 11/1000 | Loss: 0.00071817
Iteration 12/1000 | Loss: 0.00050260
Iteration 13/1000 | Loss: 0.00082802
Iteration 14/1000 | Loss: 0.00139139
Iteration 15/1000 | Loss: 0.00062821
Iteration 16/1000 | Loss: 0.00074495
Iteration 17/1000 | Loss: 0.00113026
Iteration 18/1000 | Loss: 0.00054247
Iteration 19/1000 | Loss: 0.00075344
Iteration 20/1000 | Loss: 0.00033288
Iteration 21/1000 | Loss: 0.00016270
Iteration 22/1000 | Loss: 0.00019549
Iteration 23/1000 | Loss: 0.00015861
Iteration 24/1000 | Loss: 0.00021386
Iteration 25/1000 | Loss: 0.00023219
Iteration 26/1000 | Loss: 0.00018313
Iteration 27/1000 | Loss: 0.00028973
Iteration 28/1000 | Loss: 0.00024175
Iteration 29/1000 | Loss: 0.00026743
Iteration 30/1000 | Loss: 0.00047271
Iteration 31/1000 | Loss: 0.00071582
Iteration 32/1000 | Loss: 0.00026956
Iteration 33/1000 | Loss: 0.00115477
Iteration 34/1000 | Loss: 0.00061048
Iteration 35/1000 | Loss: 0.00104851
Iteration 36/1000 | Loss: 0.00059353
Iteration 37/1000 | Loss: 0.00078810
Iteration 38/1000 | Loss: 0.00053825
Iteration 39/1000 | Loss: 0.00031807
Iteration 40/1000 | Loss: 0.00055072
Iteration 41/1000 | Loss: 0.00054356
Iteration 42/1000 | Loss: 0.00054708
Iteration 43/1000 | Loss: 0.00065090
Iteration 44/1000 | Loss: 0.00075017
Iteration 45/1000 | Loss: 0.00069012
Iteration 46/1000 | Loss: 0.00041535
Iteration 47/1000 | Loss: 0.00025196
Iteration 48/1000 | Loss: 0.00057849
Iteration 49/1000 | Loss: 0.00040282
Iteration 50/1000 | Loss: 0.00033399
Iteration 51/1000 | Loss: 0.00029756
Iteration 52/1000 | Loss: 0.00032337
Iteration 53/1000 | Loss: 0.00059676
Iteration 54/1000 | Loss: 0.00037863
Iteration 55/1000 | Loss: 0.00054794
Iteration 56/1000 | Loss: 0.00052237
Iteration 57/1000 | Loss: 0.00063171
Iteration 58/1000 | Loss: 0.00051850
Iteration 59/1000 | Loss: 0.00041422
Iteration 60/1000 | Loss: 0.00041472
Iteration 61/1000 | Loss: 0.00032076
Iteration 62/1000 | Loss: 0.00025835
Iteration 63/1000 | Loss: 0.00041559
Iteration 64/1000 | Loss: 0.00036685
Iteration 65/1000 | Loss: 0.00060080
Iteration 66/1000 | Loss: 0.00021145
Iteration 67/1000 | Loss: 0.00012848
Iteration 68/1000 | Loss: 0.00012032
Iteration 69/1000 | Loss: 0.00042519
Iteration 70/1000 | Loss: 0.00032723
Iteration 71/1000 | Loss: 0.00027875
Iteration 72/1000 | Loss: 0.00050857
Iteration 73/1000 | Loss: 0.00044685
Iteration 74/1000 | Loss: 0.00038917
Iteration 75/1000 | Loss: 0.00037445
Iteration 76/1000 | Loss: 0.00018567
Iteration 77/1000 | Loss: 0.00012746
Iteration 78/1000 | Loss: 0.00010349
Iteration 79/1000 | Loss: 0.00031030
Iteration 80/1000 | Loss: 0.00010406
Iteration 81/1000 | Loss: 0.00071840
Iteration 82/1000 | Loss: 0.00028753
Iteration 83/1000 | Loss: 0.00011023
Iteration 84/1000 | Loss: 0.00012573
Iteration 85/1000 | Loss: 0.00008781
Iteration 86/1000 | Loss: 0.00021147
Iteration 87/1000 | Loss: 0.00014468
Iteration 88/1000 | Loss: 0.00009456
Iteration 89/1000 | Loss: 0.00009353
Iteration 90/1000 | Loss: 0.00028381
Iteration 91/1000 | Loss: 0.00009690
Iteration 92/1000 | Loss: 0.00011386
Iteration 93/1000 | Loss: 0.00012328
Iteration 94/1000 | Loss: 0.00009687
Iteration 95/1000 | Loss: 0.00010521
Iteration 96/1000 | Loss: 0.00013483
Iteration 97/1000 | Loss: 0.00007846
Iteration 98/1000 | Loss: 0.00009758
Iteration 99/1000 | Loss: 0.00008836
Iteration 100/1000 | Loss: 0.00009175
Iteration 101/1000 | Loss: 0.00058083
Iteration 102/1000 | Loss: 0.00011121
Iteration 103/1000 | Loss: 0.00056236
Iteration 104/1000 | Loss: 0.00009855
Iteration 105/1000 | Loss: 0.00007611
Iteration 106/1000 | Loss: 0.00012374
Iteration 107/1000 | Loss: 0.00058208
Iteration 108/1000 | Loss: 0.00088881
Iteration 109/1000 | Loss: 0.00020578
Iteration 110/1000 | Loss: 0.00024822
Iteration 111/1000 | Loss: 0.00010157
Iteration 112/1000 | Loss: 0.00011367
Iteration 113/1000 | Loss: 0.00018225
Iteration 114/1000 | Loss: 0.00010092
Iteration 115/1000 | Loss: 0.00006185
Iteration 116/1000 | Loss: 0.00010060
Iteration 117/1000 | Loss: 0.00009535
Iteration 118/1000 | Loss: 0.00009362
Iteration 119/1000 | Loss: 0.00010159
Iteration 120/1000 | Loss: 0.00056781
Iteration 121/1000 | Loss: 0.00008419
Iteration 122/1000 | Loss: 0.00010109
Iteration 123/1000 | Loss: 0.00008473
Iteration 124/1000 | Loss: 0.00009419
Iteration 125/1000 | Loss: 0.00010279
Iteration 126/1000 | Loss: 0.00011351
Iteration 127/1000 | Loss: 0.00027379
Iteration 128/1000 | Loss: 0.00009478
Iteration 129/1000 | Loss: 0.00010866
Iteration 130/1000 | Loss: 0.00007802
Iteration 131/1000 | Loss: 0.00008600
Iteration 132/1000 | Loss: 0.00008972
Iteration 133/1000 | Loss: 0.00008322
Iteration 134/1000 | Loss: 0.00008555
Iteration 135/1000 | Loss: 0.00009748
Iteration 136/1000 | Loss: 0.00010376
Iteration 137/1000 | Loss: 0.00008207
Iteration 138/1000 | Loss: 0.00008524
Iteration 139/1000 | Loss: 0.00008754
Iteration 140/1000 | Loss: 0.00008763
Iteration 141/1000 | Loss: 0.00009853
Iteration 142/1000 | Loss: 0.00015415
Iteration 143/1000 | Loss: 0.00010278
Iteration 144/1000 | Loss: 0.00009765
Iteration 145/1000 | Loss: 0.00010992
Iteration 146/1000 | Loss: 0.00008011
Iteration 147/1000 | Loss: 0.00008480
Iteration 148/1000 | Loss: 0.00013729
Iteration 149/1000 | Loss: 0.00010222
Iteration 150/1000 | Loss: 0.00007398
Iteration 151/1000 | Loss: 0.00008114
Iteration 152/1000 | Loss: 0.00007655
Iteration 153/1000 | Loss: 0.00008415
Iteration 154/1000 | Loss: 0.00007751
Iteration 155/1000 | Loss: 0.00009184
Iteration 156/1000 | Loss: 0.00009405
Iteration 157/1000 | Loss: 0.00007933
Iteration 158/1000 | Loss: 0.00028552
Iteration 159/1000 | Loss: 0.00008332
Iteration 160/1000 | Loss: 0.00007813
Iteration 161/1000 | Loss: 0.00012717
Iteration 162/1000 | Loss: 0.00011425
Iteration 163/1000 | Loss: 0.00034142
Iteration 164/1000 | Loss: 0.00022587
Iteration 165/1000 | Loss: 0.00009891
Iteration 166/1000 | Loss: 0.00024927
Iteration 167/1000 | Loss: 0.00007704
Iteration 168/1000 | Loss: 0.00006137
Iteration 169/1000 | Loss: 0.00006919
Iteration 170/1000 | Loss: 0.00006634
Iteration 171/1000 | Loss: 0.00008197
Iteration 172/1000 | Loss: 0.00005363
Iteration 173/1000 | Loss: 0.00008534
Iteration 174/1000 | Loss: 0.00006328
Iteration 175/1000 | Loss: 0.00007849
Iteration 176/1000 | Loss: 0.00007888
Iteration 177/1000 | Loss: 0.00017729
Iteration 178/1000 | Loss: 0.00007860
Iteration 179/1000 | Loss: 0.00008687
Iteration 180/1000 | Loss: 0.00007297
Iteration 181/1000 | Loss: 0.00007474
Iteration 182/1000 | Loss: 0.00007552
Iteration 183/1000 | Loss: 0.00007888
Iteration 184/1000 | Loss: 0.00008241
Iteration 185/1000 | Loss: 0.00009410
Iteration 186/1000 | Loss: 0.00008552
Iteration 187/1000 | Loss: 0.00008242
Iteration 188/1000 | Loss: 0.00008437
Iteration 189/1000 | Loss: 0.00008127
Iteration 190/1000 | Loss: 0.00012195
Iteration 191/1000 | Loss: 0.00008429
Iteration 192/1000 | Loss: 0.00007072
Iteration 193/1000 | Loss: 0.00008497
Iteration 194/1000 | Loss: 0.00008323
Iteration 195/1000 | Loss: 0.00037042
Iteration 196/1000 | Loss: 0.00155818
Iteration 197/1000 | Loss: 0.00239611
Iteration 198/1000 | Loss: 0.00257533
Iteration 199/1000 | Loss: 0.00244218
Iteration 200/1000 | Loss: 0.00168820
Iteration 201/1000 | Loss: 0.00005816
Iteration 202/1000 | Loss: 0.00005625
Iteration 203/1000 | Loss: 0.00102440
Iteration 204/1000 | Loss: 0.00076660
Iteration 205/1000 | Loss: 0.00118340
Iteration 206/1000 | Loss: 0.00068197
Iteration 207/1000 | Loss: 0.00089826
Iteration 208/1000 | Loss: 0.00013530
Iteration 209/1000 | Loss: 0.00019311
Iteration 210/1000 | Loss: 0.00018114
Iteration 211/1000 | Loss: 0.00012789
Iteration 212/1000 | Loss: 0.00005095
Iteration 213/1000 | Loss: 0.00004046
Iteration 214/1000 | Loss: 0.00004195
Iteration 215/1000 | Loss: 0.00004886
Iteration 216/1000 | Loss: 0.00005767
Iteration 217/1000 | Loss: 0.00002989
Iteration 218/1000 | Loss: 0.00004373
Iteration 219/1000 | Loss: 0.00024612
Iteration 220/1000 | Loss: 0.00006516
Iteration 221/1000 | Loss: 0.00005098
Iteration 222/1000 | Loss: 0.00004396
Iteration 223/1000 | Loss: 0.00004987
Iteration 224/1000 | Loss: 0.00007316
Iteration 225/1000 | Loss: 0.00003175
Iteration 226/1000 | Loss: 0.00004291
Iteration 227/1000 | Loss: 0.00006391
Iteration 228/1000 | Loss: 0.00005151
Iteration 229/1000 | Loss: 0.00008543
Iteration 230/1000 | Loss: 0.00006472
Iteration 231/1000 | Loss: 0.00004477
Iteration 232/1000 | Loss: 0.00004003
Iteration 233/1000 | Loss: 0.00004147
Iteration 234/1000 | Loss: 0.00003666
Iteration 235/1000 | Loss: 0.00005382
Iteration 236/1000 | Loss: 0.00004091
Iteration 237/1000 | Loss: 0.00004313
Iteration 238/1000 | Loss: 0.00004426
Iteration 239/1000 | Loss: 0.00006617
Iteration 240/1000 | Loss: 0.00004819
Iteration 241/1000 | Loss: 0.00003233
Iteration 242/1000 | Loss: 0.00009968
Iteration 243/1000 | Loss: 0.00044223
Iteration 244/1000 | Loss: 0.00084634
Iteration 245/1000 | Loss: 0.00183401
Iteration 246/1000 | Loss: 0.00007250
Iteration 247/1000 | Loss: 0.00003877
Iteration 248/1000 | Loss: 0.00002252
Iteration 249/1000 | Loss: 0.00003788
Iteration 250/1000 | Loss: 0.00002365
Iteration 251/1000 | Loss: 0.00002802
Iteration 252/1000 | Loss: 0.00003501
Iteration 253/1000 | Loss: 0.00001764
Iteration 254/1000 | Loss: 0.00002186
Iteration 255/1000 | Loss: 0.00002770
Iteration 256/1000 | Loss: 0.00003313
Iteration 257/1000 | Loss: 0.00002359
Iteration 258/1000 | Loss: 0.00002932
Iteration 259/1000 | Loss: 0.00002558
Iteration 260/1000 | Loss: 0.00002015
Iteration 261/1000 | Loss: 0.00002061
Iteration 262/1000 | Loss: 0.00002590
Iteration 263/1000 | Loss: 0.00002947
Iteration 264/1000 | Loss: 0.00002989
Iteration 265/1000 | Loss: 0.00002789
Iteration 266/1000 | Loss: 0.00002548
Iteration 267/1000 | Loss: 0.00002274
Iteration 268/1000 | Loss: 0.00002031
Iteration 269/1000 | Loss: 0.00017378
Iteration 270/1000 | Loss: 0.00002826
Iteration 271/1000 | Loss: 0.00002725
Iteration 272/1000 | Loss: 0.00002745
Iteration 273/1000 | Loss: 0.00005112
Iteration 274/1000 | Loss: 0.00002054
Iteration 275/1000 | Loss: 0.00002722
Iteration 276/1000 | Loss: 0.00001798
Iteration 277/1000 | Loss: 0.00002591
Iteration 278/1000 | Loss: 0.00002147
Iteration 279/1000 | Loss: 0.00001954
Iteration 280/1000 | Loss: 0.00004064
Iteration 281/1000 | Loss: 0.00002421
Iteration 282/1000 | Loss: 0.00002382
Iteration 283/1000 | Loss: 0.00002418
Iteration 284/1000 | Loss: 0.00003777
Iteration 285/1000 | Loss: 0.00002398
Iteration 286/1000 | Loss: 0.00002376
Iteration 287/1000 | Loss: 0.00002362
Iteration 288/1000 | Loss: 0.00001402
Iteration 289/1000 | Loss: 0.00002107
Iteration 290/1000 | Loss: 0.00002754
Iteration 291/1000 | Loss: 0.00002427
Iteration 292/1000 | Loss: 0.00002314
Iteration 293/1000 | Loss: 0.00002382
Iteration 294/1000 | Loss: 0.00003371
Iteration 295/1000 | Loss: 0.00002705
Iteration 296/1000 | Loss: 0.00002279
Iteration 297/1000 | Loss: 0.00003785
Iteration 298/1000 | Loss: 0.00002420
Iteration 299/1000 | Loss: 0.00001857
Iteration 300/1000 | Loss: 0.00001695
Iteration 301/1000 | Loss: 0.00002293
Iteration 302/1000 | Loss: 0.00002466
Iteration 303/1000 | Loss: 0.00002281
Iteration 304/1000 | Loss: 0.00003586
Iteration 305/1000 | Loss: 0.00004902
Iteration 306/1000 | Loss: 0.00003602
Iteration 307/1000 | Loss: 0.00002494
Iteration 308/1000 | Loss: 0.00003148
Iteration 309/1000 | Loss: 0.00002825
Iteration 310/1000 | Loss: 0.00002238
Iteration 311/1000 | Loss: 0.00002254
Iteration 312/1000 | Loss: 0.00002688
Iteration 313/1000 | Loss: 0.00002156
Iteration 314/1000 | Loss: 0.00002292
Iteration 315/1000 | Loss: 0.00003539
Iteration 316/1000 | Loss: 0.00006596
Iteration 317/1000 | Loss: 0.00002669
Iteration 318/1000 | Loss: 0.00015499
Iteration 319/1000 | Loss: 0.00010071
Iteration 320/1000 | Loss: 0.00016622
Iteration 321/1000 | Loss: 0.00001643
Iteration 322/1000 | Loss: 0.00003612
Iteration 323/1000 | Loss: 0.00007441
Iteration 324/1000 | Loss: 0.00001314
Iteration 325/1000 | Loss: 0.00001260
Iteration 326/1000 | Loss: 0.00005659
Iteration 327/1000 | Loss: 0.00001515
Iteration 328/1000 | Loss: 0.00001733
Iteration 329/1000 | Loss: 0.00001266
Iteration 330/1000 | Loss: 0.00003654
Iteration 331/1000 | Loss: 0.00001197
Iteration 332/1000 | Loss: 0.00001189
Iteration 333/1000 | Loss: 0.00001343
Iteration 334/1000 | Loss: 0.00001343
Iteration 335/1000 | Loss: 0.00001174
Iteration 336/1000 | Loss: 0.00001173
Iteration 337/1000 | Loss: 0.00001173
Iteration 338/1000 | Loss: 0.00001234
Iteration 339/1000 | Loss: 0.00001410
Iteration 340/1000 | Loss: 0.00001164
Iteration 341/1000 | Loss: 0.00001163
Iteration 342/1000 | Loss: 0.00001163
Iteration 343/1000 | Loss: 0.00001163
Iteration 344/1000 | Loss: 0.00001163
Iteration 345/1000 | Loss: 0.00001163
Iteration 346/1000 | Loss: 0.00001163
Iteration 347/1000 | Loss: 0.00001163
Iteration 348/1000 | Loss: 0.00001163
Iteration 349/1000 | Loss: 0.00001163
Iteration 350/1000 | Loss: 0.00001163
Iteration 351/1000 | Loss: 0.00001163
Iteration 352/1000 | Loss: 0.00001163
Iteration 353/1000 | Loss: 0.00001163
Iteration 354/1000 | Loss: 0.00001163
Iteration 355/1000 | Loss: 0.00001163
Iteration 356/1000 | Loss: 0.00001163
Iteration 357/1000 | Loss: 0.00001163
Iteration 358/1000 | Loss: 0.00001163
Iteration 359/1000 | Loss: 0.00001163
Iteration 360/1000 | Loss: 0.00001163
Iteration 361/1000 | Loss: 0.00001163
Iteration 362/1000 | Loss: 0.00001163
Iteration 363/1000 | Loss: 0.00001163
Iteration 364/1000 | Loss: 0.00001163
Iteration 365/1000 | Loss: 0.00001163
Iteration 366/1000 | Loss: 0.00001163
Iteration 367/1000 | Loss: 0.00001163
Iteration 368/1000 | Loss: 0.00001163
Iteration 369/1000 | Loss: 0.00001163
Iteration 370/1000 | Loss: 0.00001163
Iteration 371/1000 | Loss: 0.00001163
Iteration 372/1000 | Loss: 0.00001163
Iteration 373/1000 | Loss: 0.00001163
Iteration 374/1000 | Loss: 0.00001163
Iteration 375/1000 | Loss: 0.00001163
Iteration 376/1000 | Loss: 0.00001163
Iteration 377/1000 | Loss: 0.00001163
Iteration 378/1000 | Loss: 0.00001163
Iteration 379/1000 | Loss: 0.00001163
Iteration 380/1000 | Loss: 0.00001163
Iteration 381/1000 | Loss: 0.00001163
Iteration 382/1000 | Loss: 0.00001163
Iteration 383/1000 | Loss: 0.00001163
Iteration 384/1000 | Loss: 0.00001163
Iteration 385/1000 | Loss: 0.00001163
Iteration 386/1000 | Loss: 0.00001163
Iteration 387/1000 | Loss: 0.00001163
Iteration 388/1000 | Loss: 0.00001163
Iteration 389/1000 | Loss: 0.00001163
Iteration 390/1000 | Loss: 0.00001163
Iteration 391/1000 | Loss: 0.00001163
Iteration 392/1000 | Loss: 0.00001163
Iteration 393/1000 | Loss: 0.00001163
Iteration 394/1000 | Loss: 0.00001163
Iteration 395/1000 | Loss: 0.00001163
Iteration 396/1000 | Loss: 0.00001163
Iteration 397/1000 | Loss: 0.00001163
Iteration 398/1000 | Loss: 0.00001163
Iteration 399/1000 | Loss: 0.00001163
Iteration 400/1000 | Loss: 0.00001163
Iteration 401/1000 | Loss: 0.00001163
Iteration 402/1000 | Loss: 0.00001163
Iteration 403/1000 | Loss: 0.00001163
Iteration 404/1000 | Loss: 0.00001163
Iteration 405/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 405. Stopping optimization.
Last 5 losses: [1.1627486856014002e-05, 1.1627486856014002e-05, 1.1627486856014002e-05, 1.1627486856014002e-05, 1.1627486856014002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1627486856014002e-05

Optimization complete. Final v2v error: 2.7200984954833984 mm

Highest mean error: 10.506248474121094 mm for frame 53

Lowest mean error: 2.1976442337036133 mm for frame 67

Saving results

Total time: 591.7872169017792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444928
Iteration 2/25 | Loss: 0.00130714
Iteration 3/25 | Loss: 0.00119086
Iteration 4/25 | Loss: 0.00117549
Iteration 5/25 | Loss: 0.00117182
Iteration 6/25 | Loss: 0.00117158
Iteration 7/25 | Loss: 0.00117158
Iteration 8/25 | Loss: 0.00117158
Iteration 9/25 | Loss: 0.00117158
Iteration 10/25 | Loss: 0.00117158
Iteration 11/25 | Loss: 0.00117158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001171584241092205, 0.001171584241092205, 0.001171584241092205, 0.001171584241092205, 0.001171584241092205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001171584241092205

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.55807304
Iteration 2/25 | Loss: 0.00163675
Iteration 3/25 | Loss: 0.00163674
Iteration 4/25 | Loss: 0.00163674
Iteration 5/25 | Loss: 0.00163674
Iteration 6/25 | Loss: 0.00163674
Iteration 7/25 | Loss: 0.00163674
Iteration 8/25 | Loss: 0.00163674
Iteration 9/25 | Loss: 0.00163674
Iteration 10/25 | Loss: 0.00163674
Iteration 11/25 | Loss: 0.00163674
Iteration 12/25 | Loss: 0.00163674
Iteration 13/25 | Loss: 0.00163674
Iteration 14/25 | Loss: 0.00163674
Iteration 15/25 | Loss: 0.00163674
Iteration 16/25 | Loss: 0.00163674
Iteration 17/25 | Loss: 0.00163674
Iteration 18/25 | Loss: 0.00163674
Iteration 19/25 | Loss: 0.00163674
Iteration 20/25 | Loss: 0.00163674
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001636738539673388, 0.001636738539673388, 0.001636738539673388, 0.001636738539673388, 0.001636738539673388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001636738539673388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163674
Iteration 2/1000 | Loss: 0.00002012
Iteration 3/1000 | Loss: 0.00001634
Iteration 4/1000 | Loss: 0.00001503
Iteration 5/1000 | Loss: 0.00001441
Iteration 6/1000 | Loss: 0.00001383
Iteration 7/1000 | Loss: 0.00001329
Iteration 8/1000 | Loss: 0.00001300
Iteration 9/1000 | Loss: 0.00001267
Iteration 10/1000 | Loss: 0.00001245
Iteration 11/1000 | Loss: 0.00001221
Iteration 12/1000 | Loss: 0.00001214
Iteration 13/1000 | Loss: 0.00001211
Iteration 14/1000 | Loss: 0.00001210
Iteration 15/1000 | Loss: 0.00001205
Iteration 16/1000 | Loss: 0.00001201
Iteration 17/1000 | Loss: 0.00001199
Iteration 18/1000 | Loss: 0.00001190
Iteration 19/1000 | Loss: 0.00001188
Iteration 20/1000 | Loss: 0.00001180
Iteration 21/1000 | Loss: 0.00001170
Iteration 22/1000 | Loss: 0.00001168
Iteration 23/1000 | Loss: 0.00001167
Iteration 24/1000 | Loss: 0.00001166
Iteration 25/1000 | Loss: 0.00001166
Iteration 26/1000 | Loss: 0.00001165
Iteration 27/1000 | Loss: 0.00001164
Iteration 28/1000 | Loss: 0.00001164
Iteration 29/1000 | Loss: 0.00001164
Iteration 30/1000 | Loss: 0.00001164
Iteration 31/1000 | Loss: 0.00001163
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001163
Iteration 35/1000 | Loss: 0.00001162
Iteration 36/1000 | Loss: 0.00001162
Iteration 37/1000 | Loss: 0.00001162
Iteration 38/1000 | Loss: 0.00001162
Iteration 39/1000 | Loss: 0.00001162
Iteration 40/1000 | Loss: 0.00001162
Iteration 41/1000 | Loss: 0.00001161
Iteration 42/1000 | Loss: 0.00001160
Iteration 43/1000 | Loss: 0.00001160
Iteration 44/1000 | Loss: 0.00001159
Iteration 45/1000 | Loss: 0.00001159
Iteration 46/1000 | Loss: 0.00001159
Iteration 47/1000 | Loss: 0.00001159
Iteration 48/1000 | Loss: 0.00001159
Iteration 49/1000 | Loss: 0.00001159
Iteration 50/1000 | Loss: 0.00001158
Iteration 51/1000 | Loss: 0.00001158
Iteration 52/1000 | Loss: 0.00001157
Iteration 53/1000 | Loss: 0.00001156
Iteration 54/1000 | Loss: 0.00001156
Iteration 55/1000 | Loss: 0.00001155
Iteration 56/1000 | Loss: 0.00001155
Iteration 57/1000 | Loss: 0.00001155
Iteration 58/1000 | Loss: 0.00001155
Iteration 59/1000 | Loss: 0.00001155
Iteration 60/1000 | Loss: 0.00001154
Iteration 61/1000 | Loss: 0.00001154
Iteration 62/1000 | Loss: 0.00001154
Iteration 63/1000 | Loss: 0.00001154
Iteration 64/1000 | Loss: 0.00001153
Iteration 65/1000 | Loss: 0.00001153
Iteration 66/1000 | Loss: 0.00001153
Iteration 67/1000 | Loss: 0.00001153
Iteration 68/1000 | Loss: 0.00001152
Iteration 69/1000 | Loss: 0.00001152
Iteration 70/1000 | Loss: 0.00001152
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001151
Iteration 73/1000 | Loss: 0.00001151
Iteration 74/1000 | Loss: 0.00001151
Iteration 75/1000 | Loss: 0.00001151
Iteration 76/1000 | Loss: 0.00001151
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001151
Iteration 82/1000 | Loss: 0.00001151
Iteration 83/1000 | Loss: 0.00001150
Iteration 84/1000 | Loss: 0.00001150
Iteration 85/1000 | Loss: 0.00001150
Iteration 86/1000 | Loss: 0.00001150
Iteration 87/1000 | Loss: 0.00001150
Iteration 88/1000 | Loss: 0.00001150
Iteration 89/1000 | Loss: 0.00001150
Iteration 90/1000 | Loss: 0.00001150
Iteration 91/1000 | Loss: 0.00001149
Iteration 92/1000 | Loss: 0.00001149
Iteration 93/1000 | Loss: 0.00001148
Iteration 94/1000 | Loss: 0.00001148
Iteration 95/1000 | Loss: 0.00001148
Iteration 96/1000 | Loss: 0.00001148
Iteration 97/1000 | Loss: 0.00001147
Iteration 98/1000 | Loss: 0.00001147
Iteration 99/1000 | Loss: 0.00001147
Iteration 100/1000 | Loss: 0.00001146
Iteration 101/1000 | Loss: 0.00001146
Iteration 102/1000 | Loss: 0.00001146
Iteration 103/1000 | Loss: 0.00001146
Iteration 104/1000 | Loss: 0.00001146
Iteration 105/1000 | Loss: 0.00001146
Iteration 106/1000 | Loss: 0.00001146
Iteration 107/1000 | Loss: 0.00001145
Iteration 108/1000 | Loss: 0.00001145
Iteration 109/1000 | Loss: 0.00001145
Iteration 110/1000 | Loss: 0.00001145
Iteration 111/1000 | Loss: 0.00001144
Iteration 112/1000 | Loss: 0.00001144
Iteration 113/1000 | Loss: 0.00001144
Iteration 114/1000 | Loss: 0.00001144
Iteration 115/1000 | Loss: 0.00001144
Iteration 116/1000 | Loss: 0.00001144
Iteration 117/1000 | Loss: 0.00001144
Iteration 118/1000 | Loss: 0.00001144
Iteration 119/1000 | Loss: 0.00001143
Iteration 120/1000 | Loss: 0.00001143
Iteration 121/1000 | Loss: 0.00001143
Iteration 122/1000 | Loss: 0.00001143
Iteration 123/1000 | Loss: 0.00001143
Iteration 124/1000 | Loss: 0.00001143
Iteration 125/1000 | Loss: 0.00001143
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001143
Iteration 129/1000 | Loss: 0.00001143
Iteration 130/1000 | Loss: 0.00001143
Iteration 131/1000 | Loss: 0.00001142
Iteration 132/1000 | Loss: 0.00001142
Iteration 133/1000 | Loss: 0.00001142
Iteration 134/1000 | Loss: 0.00001141
Iteration 135/1000 | Loss: 0.00001141
Iteration 136/1000 | Loss: 0.00001141
Iteration 137/1000 | Loss: 0.00001141
Iteration 138/1000 | Loss: 0.00001141
Iteration 139/1000 | Loss: 0.00001141
Iteration 140/1000 | Loss: 0.00001141
Iteration 141/1000 | Loss: 0.00001141
Iteration 142/1000 | Loss: 0.00001141
Iteration 143/1000 | Loss: 0.00001141
Iteration 144/1000 | Loss: 0.00001141
Iteration 145/1000 | Loss: 0.00001141
Iteration 146/1000 | Loss: 0.00001141
Iteration 147/1000 | Loss: 0.00001141
Iteration 148/1000 | Loss: 0.00001141
Iteration 149/1000 | Loss: 0.00001141
Iteration 150/1000 | Loss: 0.00001141
Iteration 151/1000 | Loss: 0.00001141
Iteration 152/1000 | Loss: 0.00001141
Iteration 153/1000 | Loss: 0.00001141
Iteration 154/1000 | Loss: 0.00001141
Iteration 155/1000 | Loss: 0.00001141
Iteration 156/1000 | Loss: 0.00001141
Iteration 157/1000 | Loss: 0.00001141
Iteration 158/1000 | Loss: 0.00001141
Iteration 159/1000 | Loss: 0.00001141
Iteration 160/1000 | Loss: 0.00001141
Iteration 161/1000 | Loss: 0.00001141
Iteration 162/1000 | Loss: 0.00001141
Iteration 163/1000 | Loss: 0.00001141
Iteration 164/1000 | Loss: 0.00001141
Iteration 165/1000 | Loss: 0.00001141
Iteration 166/1000 | Loss: 0.00001141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.1411313607823104e-05, 1.1411313607823104e-05, 1.1411313607823104e-05, 1.1411313607823104e-05, 1.1411313607823104e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1411313607823104e-05

Optimization complete. Final v2v error: 2.875046730041504 mm

Highest mean error: 3.30191707611084 mm for frame 86

Lowest mean error: 2.6066908836364746 mm for frame 37

Saving results

Total time: 41.323896408081055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854960
Iteration 2/25 | Loss: 0.00284200
Iteration 3/25 | Loss: 0.00176421
Iteration 4/25 | Loss: 0.00149607
Iteration 5/25 | Loss: 0.00147122
Iteration 6/25 | Loss: 0.00147526
Iteration 7/25 | Loss: 0.00144466
Iteration 8/25 | Loss: 0.00143117
Iteration 9/25 | Loss: 0.00142248
Iteration 10/25 | Loss: 0.00141879
Iteration 11/25 | Loss: 0.00141743
Iteration 12/25 | Loss: 0.00141686
Iteration 13/25 | Loss: 0.00141649
Iteration 14/25 | Loss: 0.00141605
Iteration 15/25 | Loss: 0.00141855
Iteration 16/25 | Loss: 0.00141350
Iteration 17/25 | Loss: 0.00141204
Iteration 18/25 | Loss: 0.00141161
Iteration 19/25 | Loss: 0.00141138
Iteration 20/25 | Loss: 0.00141124
Iteration 21/25 | Loss: 0.00141118
Iteration 22/25 | Loss: 0.00141117
Iteration 23/25 | Loss: 0.00141117
Iteration 24/25 | Loss: 0.00141117
Iteration 25/25 | Loss: 0.00141117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.96005583
Iteration 2/25 | Loss: 0.00511629
Iteration 3/25 | Loss: 0.00352509
Iteration 4/25 | Loss: 0.00352509
Iteration 5/25 | Loss: 0.00352508
Iteration 6/25 | Loss: 0.00352508
Iteration 7/25 | Loss: 0.00352508
Iteration 8/25 | Loss: 0.00352508
Iteration 9/25 | Loss: 0.00352508
Iteration 10/25 | Loss: 0.00352508
Iteration 11/25 | Loss: 0.00352508
Iteration 12/25 | Loss: 0.00352508
Iteration 13/25 | Loss: 0.00352508
Iteration 14/25 | Loss: 0.00352508
Iteration 15/25 | Loss: 0.00352508
Iteration 16/25 | Loss: 0.00352508
Iteration 17/25 | Loss: 0.00352508
Iteration 18/25 | Loss: 0.00352508
Iteration 19/25 | Loss: 0.00352508
Iteration 20/25 | Loss: 0.00352508
Iteration 21/25 | Loss: 0.00352508
Iteration 22/25 | Loss: 0.00352508
Iteration 23/25 | Loss: 0.00352508
Iteration 24/25 | Loss: 0.00352508
Iteration 25/25 | Loss: 0.00352508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00352508
Iteration 2/1000 | Loss: 0.00288950
Iteration 3/1000 | Loss: 0.00218268
Iteration 4/1000 | Loss: 0.00305071
Iteration 5/1000 | Loss: 0.00033363
Iteration 6/1000 | Loss: 0.00229432
Iteration 7/1000 | Loss: 0.00183743
Iteration 8/1000 | Loss: 0.00312456
Iteration 9/1000 | Loss: 0.00083273
Iteration 10/1000 | Loss: 0.00071807
Iteration 11/1000 | Loss: 0.00235339
Iteration 12/1000 | Loss: 0.00020844
Iteration 13/1000 | Loss: 0.00111956
Iteration 14/1000 | Loss: 0.00085391
Iteration 15/1000 | Loss: 0.00156041
Iteration 16/1000 | Loss: 0.00142809
Iteration 17/1000 | Loss: 0.00115631
Iteration 18/1000 | Loss: 0.00015902
Iteration 19/1000 | Loss: 0.00016567
Iteration 20/1000 | Loss: 0.00082953
Iteration 21/1000 | Loss: 0.00069738
Iteration 22/1000 | Loss: 0.00105586
Iteration 23/1000 | Loss: 0.00029077
Iteration 24/1000 | Loss: 0.00011631
Iteration 25/1000 | Loss: 0.00042884
Iteration 26/1000 | Loss: 0.00057697
Iteration 27/1000 | Loss: 0.00038618
Iteration 28/1000 | Loss: 0.00010553
Iteration 29/1000 | Loss: 0.00026012
Iteration 30/1000 | Loss: 0.00105252
Iteration 31/1000 | Loss: 0.00256735
Iteration 32/1000 | Loss: 0.00363969
Iteration 33/1000 | Loss: 0.00544531
Iteration 34/1000 | Loss: 0.00316481
Iteration 35/1000 | Loss: 0.00175255
Iteration 36/1000 | Loss: 0.00187872
Iteration 37/1000 | Loss: 0.00486382
Iteration 38/1000 | Loss: 0.00139997
Iteration 39/1000 | Loss: 0.00371891
Iteration 40/1000 | Loss: 0.00413988
Iteration 41/1000 | Loss: 0.00561208
Iteration 42/1000 | Loss: 0.00359200
Iteration 43/1000 | Loss: 0.00220795
Iteration 44/1000 | Loss: 0.00372175
Iteration 45/1000 | Loss: 0.00194455
Iteration 46/1000 | Loss: 0.00140667
Iteration 47/1000 | Loss: 0.00099823
Iteration 48/1000 | Loss: 0.00170428
Iteration 49/1000 | Loss: 0.00087379
Iteration 50/1000 | Loss: 0.00034576
Iteration 51/1000 | Loss: 0.00117022
Iteration 52/1000 | Loss: 0.00048516
Iteration 53/1000 | Loss: 0.00205791
Iteration 54/1000 | Loss: 0.00040229
Iteration 55/1000 | Loss: 0.00082542
Iteration 56/1000 | Loss: 0.00138203
Iteration 57/1000 | Loss: 0.00252554
Iteration 58/1000 | Loss: 0.00030363
Iteration 59/1000 | Loss: 0.00018177
Iteration 60/1000 | Loss: 0.00155735
Iteration 61/1000 | Loss: 0.00061993
Iteration 62/1000 | Loss: 0.00019782
Iteration 63/1000 | Loss: 0.00026995
Iteration 64/1000 | Loss: 0.00030975
Iteration 65/1000 | Loss: 0.00079987
Iteration 66/1000 | Loss: 0.00051033
Iteration 67/1000 | Loss: 0.00071540
Iteration 68/1000 | Loss: 0.00088371
Iteration 69/1000 | Loss: 0.00038361
Iteration 70/1000 | Loss: 0.00040925
Iteration 71/1000 | Loss: 0.00037736
Iteration 72/1000 | Loss: 0.00085185
Iteration 73/1000 | Loss: 0.00066540
Iteration 74/1000 | Loss: 0.00079860
Iteration 75/1000 | Loss: 0.00009479
Iteration 76/1000 | Loss: 0.00005070
Iteration 77/1000 | Loss: 0.00004737
Iteration 78/1000 | Loss: 0.00225349
Iteration 79/1000 | Loss: 0.00004934
Iteration 80/1000 | Loss: 0.00004298
Iteration 81/1000 | Loss: 0.00144220
Iteration 82/1000 | Loss: 0.00086125
Iteration 83/1000 | Loss: 0.00005225
Iteration 84/1000 | Loss: 0.00003627
Iteration 85/1000 | Loss: 0.00003177
Iteration 86/1000 | Loss: 0.00035054
Iteration 87/1000 | Loss: 0.00069566
Iteration 88/1000 | Loss: 0.00038544
Iteration 89/1000 | Loss: 0.00056425
Iteration 90/1000 | Loss: 0.00013310
Iteration 91/1000 | Loss: 0.00054645
Iteration 92/1000 | Loss: 0.00117263
Iteration 93/1000 | Loss: 0.00008945
Iteration 94/1000 | Loss: 0.00041411
Iteration 95/1000 | Loss: 0.00064675
Iteration 96/1000 | Loss: 0.00019022
Iteration 97/1000 | Loss: 0.00002686
Iteration 98/1000 | Loss: 0.00062485
Iteration 99/1000 | Loss: 0.00018340
Iteration 100/1000 | Loss: 0.00105555
Iteration 101/1000 | Loss: 0.00018540
Iteration 102/1000 | Loss: 0.00004613
Iteration 103/1000 | Loss: 0.00033214
Iteration 104/1000 | Loss: 0.00041050
Iteration 105/1000 | Loss: 0.00054617
Iteration 106/1000 | Loss: 0.00154839
Iteration 107/1000 | Loss: 0.00043889
Iteration 108/1000 | Loss: 0.00056438
Iteration 109/1000 | Loss: 0.00012034
Iteration 110/1000 | Loss: 0.00052949
Iteration 111/1000 | Loss: 0.00055492
Iteration 112/1000 | Loss: 0.00030376
Iteration 113/1000 | Loss: 0.00035852
Iteration 114/1000 | Loss: 0.00037741
Iteration 115/1000 | Loss: 0.00043694
Iteration 116/1000 | Loss: 0.00011801
Iteration 117/1000 | Loss: 0.00021689
Iteration 118/1000 | Loss: 0.00065348
Iteration 119/1000 | Loss: 0.00004317
Iteration 120/1000 | Loss: 0.00003216
Iteration 121/1000 | Loss: 0.00045853
Iteration 122/1000 | Loss: 0.00002584
Iteration 123/1000 | Loss: 0.00002347
Iteration 124/1000 | Loss: 0.00002218
Iteration 125/1000 | Loss: 0.00002016
Iteration 126/1000 | Loss: 0.00001852
Iteration 127/1000 | Loss: 0.00001780
Iteration 128/1000 | Loss: 0.00001710
Iteration 129/1000 | Loss: 0.00080233
Iteration 130/1000 | Loss: 0.00007470
Iteration 131/1000 | Loss: 0.00020285
Iteration 132/1000 | Loss: 0.00001757
Iteration 133/1000 | Loss: 0.00001668
Iteration 134/1000 | Loss: 0.00001635
Iteration 135/1000 | Loss: 0.00001607
Iteration 136/1000 | Loss: 0.00001595
Iteration 137/1000 | Loss: 0.00001587
Iteration 138/1000 | Loss: 0.00001564
Iteration 139/1000 | Loss: 0.00001547
Iteration 140/1000 | Loss: 0.00001538
Iteration 141/1000 | Loss: 0.00001537
Iteration 142/1000 | Loss: 0.00001532
Iteration 143/1000 | Loss: 0.00001530
Iteration 144/1000 | Loss: 0.00001530
Iteration 145/1000 | Loss: 0.00001526
Iteration 146/1000 | Loss: 0.00001523
Iteration 147/1000 | Loss: 0.00059316
Iteration 148/1000 | Loss: 0.00029940
Iteration 149/1000 | Loss: 0.00001543
Iteration 150/1000 | Loss: 0.00001515
Iteration 151/1000 | Loss: 0.00001513
Iteration 152/1000 | Loss: 0.00001512
Iteration 153/1000 | Loss: 0.00001512
Iteration 154/1000 | Loss: 0.00060431
Iteration 155/1000 | Loss: 0.00001813
Iteration 156/1000 | Loss: 0.00001547
Iteration 157/1000 | Loss: 0.00001494
Iteration 158/1000 | Loss: 0.00001413
Iteration 159/1000 | Loss: 0.00001353
Iteration 160/1000 | Loss: 0.00001333
Iteration 161/1000 | Loss: 0.00001317
Iteration 162/1000 | Loss: 0.00001315
Iteration 163/1000 | Loss: 0.00001311
Iteration 164/1000 | Loss: 0.00001307
Iteration 165/1000 | Loss: 0.00001299
Iteration 166/1000 | Loss: 0.00001293
Iteration 167/1000 | Loss: 0.00001292
Iteration 168/1000 | Loss: 0.00001292
Iteration 169/1000 | Loss: 0.00001291
Iteration 170/1000 | Loss: 0.00001291
Iteration 171/1000 | Loss: 0.00001285
Iteration 172/1000 | Loss: 0.00001284
Iteration 173/1000 | Loss: 0.00001283
Iteration 174/1000 | Loss: 0.00001283
Iteration 175/1000 | Loss: 0.00001283
Iteration 176/1000 | Loss: 0.00001283
Iteration 177/1000 | Loss: 0.00001282
Iteration 178/1000 | Loss: 0.00001282
Iteration 179/1000 | Loss: 0.00001282
Iteration 180/1000 | Loss: 0.00001282
Iteration 181/1000 | Loss: 0.00001282
Iteration 182/1000 | Loss: 0.00001282
Iteration 183/1000 | Loss: 0.00001282
Iteration 184/1000 | Loss: 0.00001282
Iteration 185/1000 | Loss: 0.00001282
Iteration 186/1000 | Loss: 0.00001282
Iteration 187/1000 | Loss: 0.00001279
Iteration 188/1000 | Loss: 0.00001279
Iteration 189/1000 | Loss: 0.00001278
Iteration 190/1000 | Loss: 0.00001276
Iteration 191/1000 | Loss: 0.00001276
Iteration 192/1000 | Loss: 0.00001276
Iteration 193/1000 | Loss: 0.00001276
Iteration 194/1000 | Loss: 0.00001276
Iteration 195/1000 | Loss: 0.00001276
Iteration 196/1000 | Loss: 0.00001275
Iteration 197/1000 | Loss: 0.00001275
Iteration 198/1000 | Loss: 0.00001275
Iteration 199/1000 | Loss: 0.00001275
Iteration 200/1000 | Loss: 0.00001275
Iteration 201/1000 | Loss: 0.00001275
Iteration 202/1000 | Loss: 0.00001274
Iteration 203/1000 | Loss: 0.00001274
Iteration 204/1000 | Loss: 0.00001274
Iteration 205/1000 | Loss: 0.00001274
Iteration 206/1000 | Loss: 0.00001274
Iteration 207/1000 | Loss: 0.00001274
Iteration 208/1000 | Loss: 0.00001274
Iteration 209/1000 | Loss: 0.00001273
Iteration 210/1000 | Loss: 0.00001273
Iteration 211/1000 | Loss: 0.00001273
Iteration 212/1000 | Loss: 0.00001273
Iteration 213/1000 | Loss: 0.00001272
Iteration 214/1000 | Loss: 0.00001272
Iteration 215/1000 | Loss: 0.00001272
Iteration 216/1000 | Loss: 0.00001272
Iteration 217/1000 | Loss: 0.00001272
Iteration 218/1000 | Loss: 0.00001272
Iteration 219/1000 | Loss: 0.00001271
Iteration 220/1000 | Loss: 0.00001271
Iteration 221/1000 | Loss: 0.00001271
Iteration 222/1000 | Loss: 0.00001271
Iteration 223/1000 | Loss: 0.00001270
Iteration 224/1000 | Loss: 0.00001270
Iteration 225/1000 | Loss: 0.00001270
Iteration 226/1000 | Loss: 0.00001270
Iteration 227/1000 | Loss: 0.00001270
Iteration 228/1000 | Loss: 0.00001269
Iteration 229/1000 | Loss: 0.00001269
Iteration 230/1000 | Loss: 0.00001269
Iteration 231/1000 | Loss: 0.00001269
Iteration 232/1000 | Loss: 0.00001269
Iteration 233/1000 | Loss: 0.00001269
Iteration 234/1000 | Loss: 0.00001269
Iteration 235/1000 | Loss: 0.00001269
Iteration 236/1000 | Loss: 0.00001269
Iteration 237/1000 | Loss: 0.00001269
Iteration 238/1000 | Loss: 0.00001269
Iteration 239/1000 | Loss: 0.00001269
Iteration 240/1000 | Loss: 0.00001268
Iteration 241/1000 | Loss: 0.00001268
Iteration 242/1000 | Loss: 0.00001268
Iteration 243/1000 | Loss: 0.00001268
Iteration 244/1000 | Loss: 0.00001268
Iteration 245/1000 | Loss: 0.00001268
Iteration 246/1000 | Loss: 0.00001268
Iteration 247/1000 | Loss: 0.00001268
Iteration 248/1000 | Loss: 0.00001268
Iteration 249/1000 | Loss: 0.00001267
Iteration 250/1000 | Loss: 0.00001267
Iteration 251/1000 | Loss: 0.00001267
Iteration 252/1000 | Loss: 0.00001267
Iteration 253/1000 | Loss: 0.00001267
Iteration 254/1000 | Loss: 0.00001267
Iteration 255/1000 | Loss: 0.00001267
Iteration 256/1000 | Loss: 0.00001267
Iteration 257/1000 | Loss: 0.00001267
Iteration 258/1000 | Loss: 0.00001267
Iteration 259/1000 | Loss: 0.00001267
Iteration 260/1000 | Loss: 0.00001267
Iteration 261/1000 | Loss: 0.00001267
Iteration 262/1000 | Loss: 0.00001267
Iteration 263/1000 | Loss: 0.00001267
Iteration 264/1000 | Loss: 0.00001267
Iteration 265/1000 | Loss: 0.00001267
Iteration 266/1000 | Loss: 0.00001267
Iteration 267/1000 | Loss: 0.00001267
Iteration 268/1000 | Loss: 0.00001267
Iteration 269/1000 | Loss: 0.00001267
Iteration 270/1000 | Loss: 0.00001267
Iteration 271/1000 | Loss: 0.00001267
Iteration 272/1000 | Loss: 0.00001267
Iteration 273/1000 | Loss: 0.00001267
Iteration 274/1000 | Loss: 0.00001267
Iteration 275/1000 | Loss: 0.00001267
Iteration 276/1000 | Loss: 0.00001267
Iteration 277/1000 | Loss: 0.00001267
Iteration 278/1000 | Loss: 0.00001267
Iteration 279/1000 | Loss: 0.00001267
Iteration 280/1000 | Loss: 0.00001267
Iteration 281/1000 | Loss: 0.00001267
Iteration 282/1000 | Loss: 0.00001267
Iteration 283/1000 | Loss: 0.00001267
Iteration 284/1000 | Loss: 0.00001267
Iteration 285/1000 | Loss: 0.00001267
Iteration 286/1000 | Loss: 0.00001267
Iteration 287/1000 | Loss: 0.00001267
Iteration 288/1000 | Loss: 0.00001267
Iteration 289/1000 | Loss: 0.00001267
Iteration 290/1000 | Loss: 0.00001267
Iteration 291/1000 | Loss: 0.00001267
Iteration 292/1000 | Loss: 0.00001267
Iteration 293/1000 | Loss: 0.00001267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 293. Stopping optimization.
Last 5 losses: [1.2666767361224629e-05, 1.2666767361224629e-05, 1.2666767361224629e-05, 1.2666767361224629e-05, 1.2666767361224629e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2666767361224629e-05

Optimization complete. Final v2v error: 2.9337172508239746 mm

Highest mean error: 4.561094760894775 mm for frame 93

Lowest mean error: 2.3930914402008057 mm for frame 142

Saving results

Total time: 293.3561248779297
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465964
Iteration 2/25 | Loss: 0.00133266
Iteration 3/25 | Loss: 0.00122297
Iteration 4/25 | Loss: 0.00120347
Iteration 5/25 | Loss: 0.00119944
Iteration 6/25 | Loss: 0.00119944
Iteration 7/25 | Loss: 0.00119944
Iteration 8/25 | Loss: 0.00119944
Iteration 9/25 | Loss: 0.00119944
Iteration 10/25 | Loss: 0.00119944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011994350934401155, 0.0011994350934401155, 0.0011994350934401155, 0.0011994350934401155, 0.0011994350934401155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011994350934401155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23136389
Iteration 2/25 | Loss: 0.00174737
Iteration 3/25 | Loss: 0.00174737
Iteration 4/25 | Loss: 0.00174736
Iteration 5/25 | Loss: 0.00174736
Iteration 6/25 | Loss: 0.00174736
Iteration 7/25 | Loss: 0.00174736
Iteration 8/25 | Loss: 0.00174736
Iteration 9/25 | Loss: 0.00174736
Iteration 10/25 | Loss: 0.00174736
Iteration 11/25 | Loss: 0.00174736
Iteration 12/25 | Loss: 0.00174736
Iteration 13/25 | Loss: 0.00174736
Iteration 14/25 | Loss: 0.00174736
Iteration 15/25 | Loss: 0.00174736
Iteration 16/25 | Loss: 0.00174736
Iteration 17/25 | Loss: 0.00174736
Iteration 18/25 | Loss: 0.00174736
Iteration 19/25 | Loss: 0.00174736
Iteration 20/25 | Loss: 0.00174736
Iteration 21/25 | Loss: 0.00174736
Iteration 22/25 | Loss: 0.00174736
Iteration 23/25 | Loss: 0.00174736
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017473632469773293, 0.0017473632469773293, 0.0017473632469773293, 0.0017473632469773293, 0.0017473632469773293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017473632469773293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174736
Iteration 2/1000 | Loss: 0.00001961
Iteration 3/1000 | Loss: 0.00001605
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001449
Iteration 6/1000 | Loss: 0.00001397
Iteration 7/1000 | Loss: 0.00001374
Iteration 8/1000 | Loss: 0.00001344
Iteration 9/1000 | Loss: 0.00001330
Iteration 10/1000 | Loss: 0.00001329
Iteration 11/1000 | Loss: 0.00001320
Iteration 12/1000 | Loss: 0.00001318
Iteration 13/1000 | Loss: 0.00001316
Iteration 14/1000 | Loss: 0.00001314
Iteration 15/1000 | Loss: 0.00001313
Iteration 16/1000 | Loss: 0.00001312
Iteration 17/1000 | Loss: 0.00001307
Iteration 18/1000 | Loss: 0.00001297
Iteration 19/1000 | Loss: 0.00001280
Iteration 20/1000 | Loss: 0.00001268
Iteration 21/1000 | Loss: 0.00001264
Iteration 22/1000 | Loss: 0.00001264
Iteration 23/1000 | Loss: 0.00001256
Iteration 24/1000 | Loss: 0.00001252
Iteration 25/1000 | Loss: 0.00001252
Iteration 26/1000 | Loss: 0.00001248
Iteration 27/1000 | Loss: 0.00001247
Iteration 28/1000 | Loss: 0.00001247
Iteration 29/1000 | Loss: 0.00001244
Iteration 30/1000 | Loss: 0.00001244
Iteration 31/1000 | Loss: 0.00001244
Iteration 32/1000 | Loss: 0.00001243
Iteration 33/1000 | Loss: 0.00001243
Iteration 34/1000 | Loss: 0.00001242
Iteration 35/1000 | Loss: 0.00001242
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001241
Iteration 39/1000 | Loss: 0.00001241
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001240
Iteration 43/1000 | Loss: 0.00001240
Iteration 44/1000 | Loss: 0.00001240
Iteration 45/1000 | Loss: 0.00001239
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001238
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001238
Iteration 53/1000 | Loss: 0.00001238
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001238
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001238
Iteration 67/1000 | Loss: 0.00001238
Iteration 68/1000 | Loss: 0.00001238
Iteration 69/1000 | Loss: 0.00001238
Iteration 70/1000 | Loss: 0.00001238
Iteration 71/1000 | Loss: 0.00001238
Iteration 72/1000 | Loss: 0.00001238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [1.237669221154647e-05, 1.237669221154647e-05, 1.237669221154647e-05, 1.237669221154647e-05, 1.237669221154647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.237669221154647e-05

Optimization complete. Final v2v error: 2.977067708969116 mm

Highest mean error: 3.3467700481414795 mm for frame 147

Lowest mean error: 2.749924898147583 mm for frame 5

Saving results

Total time: 35.143890142440796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00710584
Iteration 2/25 | Loss: 0.00168361
Iteration 3/25 | Loss: 0.00136953
Iteration 4/25 | Loss: 0.00131365
Iteration 5/25 | Loss: 0.00129491
Iteration 6/25 | Loss: 0.00130166
Iteration 7/25 | Loss: 0.00131123
Iteration 8/25 | Loss: 0.00130541
Iteration 9/25 | Loss: 0.00128404
Iteration 10/25 | Loss: 0.00127627
Iteration 11/25 | Loss: 0.00128538
Iteration 12/25 | Loss: 0.00128299
Iteration 13/25 | Loss: 0.00127451
Iteration 14/25 | Loss: 0.00126881
Iteration 15/25 | Loss: 0.00127768
Iteration 16/25 | Loss: 0.00127538
Iteration 17/25 | Loss: 0.00125743
Iteration 18/25 | Loss: 0.00124713
Iteration 19/25 | Loss: 0.00124976
Iteration 20/25 | Loss: 0.00125166
Iteration 21/25 | Loss: 0.00124902
Iteration 22/25 | Loss: 0.00124980
Iteration 23/25 | Loss: 0.00125608
Iteration 24/25 | Loss: 0.00124711
Iteration 25/25 | Loss: 0.00124234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.98644066
Iteration 2/25 | Loss: 0.00885388
Iteration 3/25 | Loss: 0.00243171
Iteration 4/25 | Loss: 0.00240449
Iteration 5/25 | Loss: 0.00240449
Iteration 6/25 | Loss: 0.00240449
Iteration 7/25 | Loss: 0.00240449
Iteration 8/25 | Loss: 0.00240449
Iteration 9/25 | Loss: 0.00240449
Iteration 10/25 | Loss: 0.00240449
Iteration 11/25 | Loss: 0.00240449
Iteration 12/25 | Loss: 0.00240449
Iteration 13/25 | Loss: 0.00240449
Iteration 14/25 | Loss: 0.00240449
Iteration 15/25 | Loss: 0.00240449
Iteration 16/25 | Loss: 0.00240449
Iteration 17/25 | Loss: 0.00240449
Iteration 18/25 | Loss: 0.00240449
Iteration 19/25 | Loss: 0.00240449
Iteration 20/25 | Loss: 0.00240448
Iteration 21/25 | Loss: 0.00240448
Iteration 22/25 | Loss: 0.00240448
Iteration 23/25 | Loss: 0.00240448
Iteration 24/25 | Loss: 0.00240448
Iteration 25/25 | Loss: 0.00240448
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0024044844321906567, 0.0024044844321906567, 0.0024044844321906567, 0.0024044844321906567, 0.0024044844321906567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024044844321906567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00240448
Iteration 2/1000 | Loss: 0.00012895
Iteration 3/1000 | Loss: 0.00007147
Iteration 4/1000 | Loss: 0.00432726
Iteration 5/1000 | Loss: 0.00400284
Iteration 6/1000 | Loss: 0.00017254
Iteration 7/1000 | Loss: 0.00331411
Iteration 8/1000 | Loss: 0.00206643
Iteration 9/1000 | Loss: 0.00238996
Iteration 10/1000 | Loss: 0.00247985
Iteration 11/1000 | Loss: 0.00117877
Iteration 12/1000 | Loss: 0.00048676
Iteration 13/1000 | Loss: 0.00163588
Iteration 14/1000 | Loss: 0.00290334
Iteration 15/1000 | Loss: 0.00006798
Iteration 16/1000 | Loss: 0.00005310
Iteration 17/1000 | Loss: 0.00234590
Iteration 18/1000 | Loss: 0.00110170
Iteration 19/1000 | Loss: 0.00193243
Iteration 20/1000 | Loss: 0.00083475
Iteration 21/1000 | Loss: 0.00181212
Iteration 22/1000 | Loss: 0.00192756
Iteration 23/1000 | Loss: 0.00151568
Iteration 24/1000 | Loss: 0.00454939
Iteration 25/1000 | Loss: 0.00142898
Iteration 26/1000 | Loss: 0.00185850
Iteration 27/1000 | Loss: 0.00358643
Iteration 28/1000 | Loss: 0.00334358
Iteration 29/1000 | Loss: 0.00061371
Iteration 30/1000 | Loss: 0.00137098
Iteration 31/1000 | Loss: 0.00076368
Iteration 32/1000 | Loss: 0.00091420
Iteration 33/1000 | Loss: 0.00082588
Iteration 34/1000 | Loss: 0.00076673
Iteration 35/1000 | Loss: 0.00124109
Iteration 36/1000 | Loss: 0.00030073
Iteration 37/1000 | Loss: 0.00046180
Iteration 38/1000 | Loss: 0.00082046
Iteration 39/1000 | Loss: 0.00032328
Iteration 40/1000 | Loss: 0.00070244
Iteration 41/1000 | Loss: 0.00070538
Iteration 42/1000 | Loss: 0.00100102
Iteration 43/1000 | Loss: 0.00034082
Iteration 44/1000 | Loss: 0.00005638
Iteration 45/1000 | Loss: 0.00005593
Iteration 46/1000 | Loss: 0.00003542
Iteration 47/1000 | Loss: 0.00003107
Iteration 48/1000 | Loss: 0.00002772
Iteration 49/1000 | Loss: 0.00002558
Iteration 50/1000 | Loss: 0.00002440
Iteration 51/1000 | Loss: 0.00002351
Iteration 52/1000 | Loss: 0.00002277
Iteration 53/1000 | Loss: 0.00002229
Iteration 54/1000 | Loss: 0.00002195
Iteration 55/1000 | Loss: 0.00002169
Iteration 56/1000 | Loss: 0.00002144
Iteration 57/1000 | Loss: 0.00002142
Iteration 58/1000 | Loss: 0.00002122
Iteration 59/1000 | Loss: 0.00002112
Iteration 60/1000 | Loss: 0.00002095
Iteration 61/1000 | Loss: 0.00002094
Iteration 62/1000 | Loss: 0.00002093
Iteration 63/1000 | Loss: 0.00002093
Iteration 64/1000 | Loss: 0.00002090
Iteration 65/1000 | Loss: 0.00002084
Iteration 66/1000 | Loss: 0.00002079
Iteration 67/1000 | Loss: 0.00002078
Iteration 68/1000 | Loss: 0.00002077
Iteration 69/1000 | Loss: 0.00002077
Iteration 70/1000 | Loss: 0.00002076
Iteration 71/1000 | Loss: 0.00002073
Iteration 72/1000 | Loss: 0.00002073
Iteration 73/1000 | Loss: 0.00002072
Iteration 74/1000 | Loss: 0.00002071
Iteration 75/1000 | Loss: 0.00002068
Iteration 76/1000 | Loss: 0.00002068
Iteration 77/1000 | Loss: 0.00002067
Iteration 78/1000 | Loss: 0.00002066
Iteration 79/1000 | Loss: 0.00002065
Iteration 80/1000 | Loss: 0.00002064
Iteration 81/1000 | Loss: 0.00002064
Iteration 82/1000 | Loss: 0.00002062
Iteration 83/1000 | Loss: 0.00002062
Iteration 84/1000 | Loss: 0.00002061
Iteration 85/1000 | Loss: 0.00002060
Iteration 86/1000 | Loss: 0.00002060
Iteration 87/1000 | Loss: 0.00002059
Iteration 88/1000 | Loss: 0.00002059
Iteration 89/1000 | Loss: 0.00002059
Iteration 90/1000 | Loss: 0.00002059
Iteration 91/1000 | Loss: 0.00002059
Iteration 92/1000 | Loss: 0.00002059
Iteration 93/1000 | Loss: 0.00002058
Iteration 94/1000 | Loss: 0.00002058
Iteration 95/1000 | Loss: 0.00002057
Iteration 96/1000 | Loss: 0.00002057
Iteration 97/1000 | Loss: 0.00002057
Iteration 98/1000 | Loss: 0.00002057
Iteration 99/1000 | Loss: 0.00002057
Iteration 100/1000 | Loss: 0.00002056
Iteration 101/1000 | Loss: 0.00002056
Iteration 102/1000 | Loss: 0.00002056
Iteration 103/1000 | Loss: 0.00002056
Iteration 104/1000 | Loss: 0.00002055
Iteration 105/1000 | Loss: 0.00002055
Iteration 106/1000 | Loss: 0.00002053
Iteration 107/1000 | Loss: 0.00002052
Iteration 108/1000 | Loss: 0.00002049
Iteration 109/1000 | Loss: 0.00002048
Iteration 110/1000 | Loss: 0.00002048
Iteration 111/1000 | Loss: 0.00002047
Iteration 112/1000 | Loss: 0.00002047
Iteration 113/1000 | Loss: 0.00002046
Iteration 114/1000 | Loss: 0.00002046
Iteration 115/1000 | Loss: 0.00002045
Iteration 116/1000 | Loss: 0.00002045
Iteration 117/1000 | Loss: 0.00002044
Iteration 118/1000 | Loss: 0.00002044
Iteration 119/1000 | Loss: 0.00002043
Iteration 120/1000 | Loss: 0.00002043
Iteration 121/1000 | Loss: 0.00002040
Iteration 122/1000 | Loss: 0.00002040
Iteration 123/1000 | Loss: 0.00002039
Iteration 124/1000 | Loss: 0.00002039
Iteration 125/1000 | Loss: 0.00002039
Iteration 126/1000 | Loss: 0.00002038
Iteration 127/1000 | Loss: 0.00002038
Iteration 128/1000 | Loss: 0.00002038
Iteration 129/1000 | Loss: 0.00002037
Iteration 130/1000 | Loss: 0.00002037
Iteration 131/1000 | Loss: 0.00002037
Iteration 132/1000 | Loss: 0.00002037
Iteration 133/1000 | Loss: 0.00002036
Iteration 134/1000 | Loss: 0.00002036
Iteration 135/1000 | Loss: 0.00002036
Iteration 136/1000 | Loss: 0.00002035
Iteration 137/1000 | Loss: 0.00002035
Iteration 138/1000 | Loss: 0.00002034
Iteration 139/1000 | Loss: 0.00002034
Iteration 140/1000 | Loss: 0.00002033
Iteration 141/1000 | Loss: 0.00002033
Iteration 142/1000 | Loss: 0.00002033
Iteration 143/1000 | Loss: 0.00002032
Iteration 144/1000 | Loss: 0.00002032
Iteration 145/1000 | Loss: 0.00002031
Iteration 146/1000 | Loss: 0.00002030
Iteration 147/1000 | Loss: 0.00002030
Iteration 148/1000 | Loss: 0.00002030
Iteration 149/1000 | Loss: 0.00002030
Iteration 150/1000 | Loss: 0.00002030
Iteration 151/1000 | Loss: 0.00002030
Iteration 152/1000 | Loss: 0.00002030
Iteration 153/1000 | Loss: 0.00002030
Iteration 154/1000 | Loss: 0.00002029
Iteration 155/1000 | Loss: 0.00002029
Iteration 156/1000 | Loss: 0.00002029
Iteration 157/1000 | Loss: 0.00002029
Iteration 158/1000 | Loss: 0.00002029
Iteration 159/1000 | Loss: 0.00002029
Iteration 160/1000 | Loss: 0.00002028
Iteration 161/1000 | Loss: 0.00002028
Iteration 162/1000 | Loss: 0.00002028
Iteration 163/1000 | Loss: 0.00002028
Iteration 164/1000 | Loss: 0.00002028
Iteration 165/1000 | Loss: 0.00002028
Iteration 166/1000 | Loss: 0.00002028
Iteration 167/1000 | Loss: 0.00002028
Iteration 168/1000 | Loss: 0.00002027
Iteration 169/1000 | Loss: 0.00002027
Iteration 170/1000 | Loss: 0.00002027
Iteration 171/1000 | Loss: 0.00002027
Iteration 172/1000 | Loss: 0.00002026
Iteration 173/1000 | Loss: 0.00002026
Iteration 174/1000 | Loss: 0.00002026
Iteration 175/1000 | Loss: 0.00002026
Iteration 176/1000 | Loss: 0.00002026
Iteration 177/1000 | Loss: 0.00002026
Iteration 178/1000 | Loss: 0.00002025
Iteration 179/1000 | Loss: 0.00002025
Iteration 180/1000 | Loss: 0.00002025
Iteration 181/1000 | Loss: 0.00002025
Iteration 182/1000 | Loss: 0.00002025
Iteration 183/1000 | Loss: 0.00002025
Iteration 184/1000 | Loss: 0.00002024
Iteration 185/1000 | Loss: 0.00002024
Iteration 186/1000 | Loss: 0.00002024
Iteration 187/1000 | Loss: 0.00002024
Iteration 188/1000 | Loss: 0.00002024
Iteration 189/1000 | Loss: 0.00002024
Iteration 190/1000 | Loss: 0.00002024
Iteration 191/1000 | Loss: 0.00002024
Iteration 192/1000 | Loss: 0.00002024
Iteration 193/1000 | Loss: 0.00002024
Iteration 194/1000 | Loss: 0.00002024
Iteration 195/1000 | Loss: 0.00002024
Iteration 196/1000 | Loss: 0.00002024
Iteration 197/1000 | Loss: 0.00002024
Iteration 198/1000 | Loss: 0.00002024
Iteration 199/1000 | Loss: 0.00002024
Iteration 200/1000 | Loss: 0.00002024
Iteration 201/1000 | Loss: 0.00002024
Iteration 202/1000 | Loss: 0.00002024
Iteration 203/1000 | Loss: 0.00002023
Iteration 204/1000 | Loss: 0.00002023
Iteration 205/1000 | Loss: 0.00002023
Iteration 206/1000 | Loss: 0.00002023
Iteration 207/1000 | Loss: 0.00002023
Iteration 208/1000 | Loss: 0.00002023
Iteration 209/1000 | Loss: 0.00002023
Iteration 210/1000 | Loss: 0.00002023
Iteration 211/1000 | Loss: 0.00002023
Iteration 212/1000 | Loss: 0.00002023
Iteration 213/1000 | Loss: 0.00002023
Iteration 214/1000 | Loss: 0.00002023
Iteration 215/1000 | Loss: 0.00002023
Iteration 216/1000 | Loss: 0.00002023
Iteration 217/1000 | Loss: 0.00002022
Iteration 218/1000 | Loss: 0.00002022
Iteration 219/1000 | Loss: 0.00002022
Iteration 220/1000 | Loss: 0.00002022
Iteration 221/1000 | Loss: 0.00002022
Iteration 222/1000 | Loss: 0.00002022
Iteration 223/1000 | Loss: 0.00002022
Iteration 224/1000 | Loss: 0.00002022
Iteration 225/1000 | Loss: 0.00002022
Iteration 226/1000 | Loss: 0.00002022
Iteration 227/1000 | Loss: 0.00002022
Iteration 228/1000 | Loss: 0.00002022
Iteration 229/1000 | Loss: 0.00002022
Iteration 230/1000 | Loss: 0.00002022
Iteration 231/1000 | Loss: 0.00002022
Iteration 232/1000 | Loss: 0.00002022
Iteration 233/1000 | Loss: 0.00002022
Iteration 234/1000 | Loss: 0.00002022
Iteration 235/1000 | Loss: 0.00002022
Iteration 236/1000 | Loss: 0.00002022
Iteration 237/1000 | Loss: 0.00002022
Iteration 238/1000 | Loss: 0.00002022
Iteration 239/1000 | Loss: 0.00002022
Iteration 240/1000 | Loss: 0.00002022
Iteration 241/1000 | Loss: 0.00002022
Iteration 242/1000 | Loss: 0.00002022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [2.0221905288053676e-05, 2.0221905288053676e-05, 2.0221905288053676e-05, 2.0221905288053676e-05, 2.0221905288053676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0221905288053676e-05

Optimization complete. Final v2v error: 3.607998847961426 mm

Highest mean error: 5.651632308959961 mm for frame 72

Lowest mean error: 2.590317487716675 mm for frame 15

Saving results

Total time: 147.0814437866211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490038
Iteration 2/25 | Loss: 0.00134699
Iteration 3/25 | Loss: 0.00125007
Iteration 4/25 | Loss: 0.00122647
Iteration 5/25 | Loss: 0.00121721
Iteration 6/25 | Loss: 0.00121514
Iteration 7/25 | Loss: 0.00121514
Iteration 8/25 | Loss: 0.00121514
Iteration 9/25 | Loss: 0.00121514
Iteration 10/25 | Loss: 0.00121514
Iteration 11/25 | Loss: 0.00121514
Iteration 12/25 | Loss: 0.00121514
Iteration 13/25 | Loss: 0.00121514
Iteration 14/25 | Loss: 0.00121514
Iteration 15/25 | Loss: 0.00121514
Iteration 16/25 | Loss: 0.00121514
Iteration 17/25 | Loss: 0.00121514
Iteration 18/25 | Loss: 0.00121514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012151433620601892, 0.0012151433620601892, 0.0012151433620601892, 0.0012151433620601892, 0.0012151433620601892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012151433620601892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40213406
Iteration 2/25 | Loss: 0.00278779
Iteration 3/25 | Loss: 0.00278777
Iteration 4/25 | Loss: 0.00278777
Iteration 5/25 | Loss: 0.00278777
Iteration 6/25 | Loss: 0.00278777
Iteration 7/25 | Loss: 0.00278777
Iteration 8/25 | Loss: 0.00278777
Iteration 9/25 | Loss: 0.00278777
Iteration 10/25 | Loss: 0.00278776
Iteration 11/25 | Loss: 0.00278776
Iteration 12/25 | Loss: 0.00278776
Iteration 13/25 | Loss: 0.00278776
Iteration 14/25 | Loss: 0.00278776
Iteration 15/25 | Loss: 0.00278776
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002787764649838209, 0.002787764649838209, 0.002787764649838209, 0.002787764649838209, 0.002787764649838209]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002787764649838209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00278776
Iteration 2/1000 | Loss: 0.00005609
Iteration 3/1000 | Loss: 0.00003898
Iteration 4/1000 | Loss: 0.00003266
Iteration 5/1000 | Loss: 0.00002998
Iteration 6/1000 | Loss: 0.00002819
Iteration 7/1000 | Loss: 0.00002682
Iteration 8/1000 | Loss: 0.00002621
Iteration 9/1000 | Loss: 0.00002560
Iteration 10/1000 | Loss: 0.00002519
Iteration 11/1000 | Loss: 0.00002486
Iteration 12/1000 | Loss: 0.00002455
Iteration 13/1000 | Loss: 0.00002433
Iteration 14/1000 | Loss: 0.00002415
Iteration 15/1000 | Loss: 0.00002415
Iteration 16/1000 | Loss: 0.00002408
Iteration 17/1000 | Loss: 0.00002403
Iteration 18/1000 | Loss: 0.00002396
Iteration 19/1000 | Loss: 0.00002392
Iteration 20/1000 | Loss: 0.00002390
Iteration 21/1000 | Loss: 0.00002389
Iteration 22/1000 | Loss: 0.00002388
Iteration 23/1000 | Loss: 0.00002388
Iteration 24/1000 | Loss: 0.00002387
Iteration 25/1000 | Loss: 0.00002384
Iteration 26/1000 | Loss: 0.00002383
Iteration 27/1000 | Loss: 0.00002383
Iteration 28/1000 | Loss: 0.00002382
Iteration 29/1000 | Loss: 0.00002382
Iteration 30/1000 | Loss: 0.00002381
Iteration 31/1000 | Loss: 0.00002373
Iteration 32/1000 | Loss: 0.00002370
Iteration 33/1000 | Loss: 0.00002370
Iteration 34/1000 | Loss: 0.00002370
Iteration 35/1000 | Loss: 0.00002369
Iteration 36/1000 | Loss: 0.00002369
Iteration 37/1000 | Loss: 0.00002369
Iteration 38/1000 | Loss: 0.00002369
Iteration 39/1000 | Loss: 0.00002369
Iteration 40/1000 | Loss: 0.00002369
Iteration 41/1000 | Loss: 0.00002369
Iteration 42/1000 | Loss: 0.00002368
Iteration 43/1000 | Loss: 0.00002368
Iteration 44/1000 | Loss: 0.00002368
Iteration 45/1000 | Loss: 0.00002368
Iteration 46/1000 | Loss: 0.00002367
Iteration 47/1000 | Loss: 0.00002367
Iteration 48/1000 | Loss: 0.00002367
Iteration 49/1000 | Loss: 0.00002366
Iteration 50/1000 | Loss: 0.00002366
Iteration 51/1000 | Loss: 0.00002366
Iteration 52/1000 | Loss: 0.00002365
Iteration 53/1000 | Loss: 0.00002365
Iteration 54/1000 | Loss: 0.00002364
Iteration 55/1000 | Loss: 0.00002364
Iteration 56/1000 | Loss: 0.00002364
Iteration 57/1000 | Loss: 0.00002363
Iteration 58/1000 | Loss: 0.00002363
Iteration 59/1000 | Loss: 0.00002363
Iteration 60/1000 | Loss: 0.00002362
Iteration 61/1000 | Loss: 0.00002362
Iteration 62/1000 | Loss: 0.00002362
Iteration 63/1000 | Loss: 0.00002362
Iteration 64/1000 | Loss: 0.00002361
Iteration 65/1000 | Loss: 0.00002361
Iteration 66/1000 | Loss: 0.00002361
Iteration 67/1000 | Loss: 0.00002360
Iteration 68/1000 | Loss: 0.00002360
Iteration 69/1000 | Loss: 0.00002360
Iteration 70/1000 | Loss: 0.00002359
Iteration 71/1000 | Loss: 0.00002359
Iteration 72/1000 | Loss: 0.00002359
Iteration 73/1000 | Loss: 0.00002359
Iteration 74/1000 | Loss: 0.00002358
Iteration 75/1000 | Loss: 0.00002358
Iteration 76/1000 | Loss: 0.00002358
Iteration 77/1000 | Loss: 0.00002357
Iteration 78/1000 | Loss: 0.00002357
Iteration 79/1000 | Loss: 0.00002357
Iteration 80/1000 | Loss: 0.00002357
Iteration 81/1000 | Loss: 0.00002356
Iteration 82/1000 | Loss: 0.00002356
Iteration 83/1000 | Loss: 0.00002356
Iteration 84/1000 | Loss: 0.00002355
Iteration 85/1000 | Loss: 0.00002355
Iteration 86/1000 | Loss: 0.00002355
Iteration 87/1000 | Loss: 0.00002355
Iteration 88/1000 | Loss: 0.00002355
Iteration 89/1000 | Loss: 0.00002355
Iteration 90/1000 | Loss: 0.00002354
Iteration 91/1000 | Loss: 0.00002354
Iteration 92/1000 | Loss: 0.00002354
Iteration 93/1000 | Loss: 0.00002354
Iteration 94/1000 | Loss: 0.00002354
Iteration 95/1000 | Loss: 0.00002354
Iteration 96/1000 | Loss: 0.00002353
Iteration 97/1000 | Loss: 0.00002353
Iteration 98/1000 | Loss: 0.00002353
Iteration 99/1000 | Loss: 0.00002352
Iteration 100/1000 | Loss: 0.00002352
Iteration 101/1000 | Loss: 0.00002352
Iteration 102/1000 | Loss: 0.00002352
Iteration 103/1000 | Loss: 0.00002352
Iteration 104/1000 | Loss: 0.00002351
Iteration 105/1000 | Loss: 0.00002351
Iteration 106/1000 | Loss: 0.00002351
Iteration 107/1000 | Loss: 0.00002351
Iteration 108/1000 | Loss: 0.00002351
Iteration 109/1000 | Loss: 0.00002350
Iteration 110/1000 | Loss: 0.00002350
Iteration 111/1000 | Loss: 0.00002350
Iteration 112/1000 | Loss: 0.00002350
Iteration 113/1000 | Loss: 0.00002350
Iteration 114/1000 | Loss: 0.00002350
Iteration 115/1000 | Loss: 0.00002350
Iteration 116/1000 | Loss: 0.00002350
Iteration 117/1000 | Loss: 0.00002350
Iteration 118/1000 | Loss: 0.00002350
Iteration 119/1000 | Loss: 0.00002350
Iteration 120/1000 | Loss: 0.00002350
Iteration 121/1000 | Loss: 0.00002350
Iteration 122/1000 | Loss: 0.00002350
Iteration 123/1000 | Loss: 0.00002350
Iteration 124/1000 | Loss: 0.00002350
Iteration 125/1000 | Loss: 0.00002350
Iteration 126/1000 | Loss: 0.00002350
Iteration 127/1000 | Loss: 0.00002350
Iteration 128/1000 | Loss: 0.00002350
Iteration 129/1000 | Loss: 0.00002350
Iteration 130/1000 | Loss: 0.00002350
Iteration 131/1000 | Loss: 0.00002350
Iteration 132/1000 | Loss: 0.00002350
Iteration 133/1000 | Loss: 0.00002350
Iteration 134/1000 | Loss: 0.00002350
Iteration 135/1000 | Loss: 0.00002350
Iteration 136/1000 | Loss: 0.00002350
Iteration 137/1000 | Loss: 0.00002350
Iteration 138/1000 | Loss: 0.00002350
Iteration 139/1000 | Loss: 0.00002350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.35047973546898e-05, 2.35047973546898e-05, 2.35047973546898e-05, 2.35047973546898e-05, 2.35047973546898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.35047973546898e-05

Optimization complete. Final v2v error: 4.029105186462402 mm

Highest mean error: 5.1437482833862305 mm for frame 40

Lowest mean error: 3.220674514770508 mm for frame 174

Saving results

Total time: 46.982566118240356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776036
Iteration 2/25 | Loss: 0.00130668
Iteration 3/25 | Loss: 0.00118582
Iteration 4/25 | Loss: 0.00117098
Iteration 5/25 | Loss: 0.00116836
Iteration 6/25 | Loss: 0.00116836
Iteration 7/25 | Loss: 0.00116836
Iteration 8/25 | Loss: 0.00116836
Iteration 9/25 | Loss: 0.00116836
Iteration 10/25 | Loss: 0.00116836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00116836023516953, 0.00116836023516953, 0.00116836023516953, 0.00116836023516953, 0.00116836023516953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00116836023516953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19784868
Iteration 2/25 | Loss: 0.00158835
Iteration 3/25 | Loss: 0.00158832
Iteration 4/25 | Loss: 0.00158832
Iteration 5/25 | Loss: 0.00158832
Iteration 6/25 | Loss: 0.00158831
Iteration 7/25 | Loss: 0.00158831
Iteration 8/25 | Loss: 0.00158831
Iteration 9/25 | Loss: 0.00158831
Iteration 10/25 | Loss: 0.00158831
Iteration 11/25 | Loss: 0.00158831
Iteration 12/25 | Loss: 0.00158831
Iteration 13/25 | Loss: 0.00158831
Iteration 14/25 | Loss: 0.00158831
Iteration 15/25 | Loss: 0.00158831
Iteration 16/25 | Loss: 0.00158831
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015883133746683598, 0.0015883133746683598, 0.0015883133746683598, 0.0015883133746683598, 0.0015883133746683598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015883133746683598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158831
Iteration 2/1000 | Loss: 0.00002596
Iteration 3/1000 | Loss: 0.00001829
Iteration 4/1000 | Loss: 0.00001593
Iteration 5/1000 | Loss: 0.00001459
Iteration 6/1000 | Loss: 0.00001359
Iteration 7/1000 | Loss: 0.00001304
Iteration 8/1000 | Loss: 0.00001264
Iteration 9/1000 | Loss: 0.00001220
Iteration 10/1000 | Loss: 0.00001194
Iteration 11/1000 | Loss: 0.00001171
Iteration 12/1000 | Loss: 0.00001159
Iteration 13/1000 | Loss: 0.00001133
Iteration 14/1000 | Loss: 0.00001132
Iteration 15/1000 | Loss: 0.00001126
Iteration 16/1000 | Loss: 0.00001106
Iteration 17/1000 | Loss: 0.00001104
Iteration 18/1000 | Loss: 0.00001101
Iteration 19/1000 | Loss: 0.00001100
Iteration 20/1000 | Loss: 0.00001100
Iteration 21/1000 | Loss: 0.00001099
Iteration 22/1000 | Loss: 0.00001098
Iteration 23/1000 | Loss: 0.00001093
Iteration 24/1000 | Loss: 0.00001092
Iteration 25/1000 | Loss: 0.00001089
Iteration 26/1000 | Loss: 0.00001088
Iteration 27/1000 | Loss: 0.00001088
Iteration 28/1000 | Loss: 0.00001088
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001087
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001087
Iteration 33/1000 | Loss: 0.00001086
Iteration 34/1000 | Loss: 0.00001086
Iteration 35/1000 | Loss: 0.00001085
Iteration 36/1000 | Loss: 0.00001085
Iteration 37/1000 | Loss: 0.00001085
Iteration 38/1000 | Loss: 0.00001085
Iteration 39/1000 | Loss: 0.00001085
Iteration 40/1000 | Loss: 0.00001085
Iteration 41/1000 | Loss: 0.00001084
Iteration 42/1000 | Loss: 0.00001083
Iteration 43/1000 | Loss: 0.00001083
Iteration 44/1000 | Loss: 0.00001082
Iteration 45/1000 | Loss: 0.00001082
Iteration 46/1000 | Loss: 0.00001081
Iteration 47/1000 | Loss: 0.00001081
Iteration 48/1000 | Loss: 0.00001079
Iteration 49/1000 | Loss: 0.00001078
Iteration 50/1000 | Loss: 0.00001076
Iteration 51/1000 | Loss: 0.00001076
Iteration 52/1000 | Loss: 0.00001074
Iteration 53/1000 | Loss: 0.00001072
Iteration 54/1000 | Loss: 0.00001070
Iteration 55/1000 | Loss: 0.00001070
Iteration 56/1000 | Loss: 0.00001069
Iteration 57/1000 | Loss: 0.00001069
Iteration 58/1000 | Loss: 0.00001068
Iteration 59/1000 | Loss: 0.00001067
Iteration 60/1000 | Loss: 0.00001067
Iteration 61/1000 | Loss: 0.00001066
Iteration 62/1000 | Loss: 0.00001066
Iteration 63/1000 | Loss: 0.00001065
Iteration 64/1000 | Loss: 0.00001064
Iteration 65/1000 | Loss: 0.00001064
Iteration 66/1000 | Loss: 0.00001063
Iteration 67/1000 | Loss: 0.00001063
Iteration 68/1000 | Loss: 0.00001063
Iteration 69/1000 | Loss: 0.00001063
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001063
Iteration 72/1000 | Loss: 0.00001063
Iteration 73/1000 | Loss: 0.00001062
Iteration 74/1000 | Loss: 0.00001062
Iteration 75/1000 | Loss: 0.00001062
Iteration 76/1000 | Loss: 0.00001061
Iteration 77/1000 | Loss: 0.00001061
Iteration 78/1000 | Loss: 0.00001060
Iteration 79/1000 | Loss: 0.00001060
Iteration 80/1000 | Loss: 0.00001060
Iteration 81/1000 | Loss: 0.00001059
Iteration 82/1000 | Loss: 0.00001059
Iteration 83/1000 | Loss: 0.00001058
Iteration 84/1000 | Loss: 0.00001058
Iteration 85/1000 | Loss: 0.00001057
Iteration 86/1000 | Loss: 0.00001057
Iteration 87/1000 | Loss: 0.00001057
Iteration 88/1000 | Loss: 0.00001057
Iteration 89/1000 | Loss: 0.00001056
Iteration 90/1000 | Loss: 0.00001056
Iteration 91/1000 | Loss: 0.00001056
Iteration 92/1000 | Loss: 0.00001056
Iteration 93/1000 | Loss: 0.00001056
Iteration 94/1000 | Loss: 0.00001056
Iteration 95/1000 | Loss: 0.00001056
Iteration 96/1000 | Loss: 0.00001056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.055817301676143e-05, 1.055817301676143e-05, 1.055817301676143e-05, 1.055817301676143e-05, 1.055817301676143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.055817301676143e-05

Optimization complete. Final v2v error: 2.790867328643799 mm

Highest mean error: 3.2493252754211426 mm for frame 112

Lowest mean error: 2.508563280105591 mm for frame 23

Saving results

Total time: 41.00685667991638
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761923
Iteration 2/25 | Loss: 0.00129052
Iteration 3/25 | Loss: 0.00120063
Iteration 4/25 | Loss: 0.00118035
Iteration 5/25 | Loss: 0.00119392
Iteration 6/25 | Loss: 0.00114506
Iteration 7/25 | Loss: 0.00113894
Iteration 8/25 | Loss: 0.00113813
Iteration 9/25 | Loss: 0.00113792
Iteration 10/25 | Loss: 0.00113792
Iteration 11/25 | Loss: 0.00113792
Iteration 12/25 | Loss: 0.00113792
Iteration 13/25 | Loss: 0.00113792
Iteration 14/25 | Loss: 0.00113792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011379220522940159, 0.0011379220522940159, 0.0011379220522940159, 0.0011379220522940159, 0.0011379220522940159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011379220522940159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22917819
Iteration 2/25 | Loss: 0.00214824
Iteration 3/25 | Loss: 0.00214823
Iteration 4/25 | Loss: 0.00214823
Iteration 5/25 | Loss: 0.00214823
Iteration 6/25 | Loss: 0.00214823
Iteration 7/25 | Loss: 0.00214823
Iteration 8/25 | Loss: 0.00214823
Iteration 9/25 | Loss: 0.00214823
Iteration 10/25 | Loss: 0.00214823
Iteration 11/25 | Loss: 0.00214823
Iteration 12/25 | Loss: 0.00214823
Iteration 13/25 | Loss: 0.00214823
Iteration 14/25 | Loss: 0.00214823
Iteration 15/25 | Loss: 0.00214823
Iteration 16/25 | Loss: 0.00214823
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021482279989868402, 0.0021482279989868402, 0.0021482279989868402, 0.0021482279989868402, 0.0021482279989868402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021482279989868402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214823
Iteration 2/1000 | Loss: 0.00002391
Iteration 3/1000 | Loss: 0.00001714
Iteration 4/1000 | Loss: 0.00001485
Iteration 5/1000 | Loss: 0.00001365
Iteration 6/1000 | Loss: 0.00001284
Iteration 7/1000 | Loss: 0.00001223
Iteration 8/1000 | Loss: 0.00001187
Iteration 9/1000 | Loss: 0.00001157
Iteration 10/1000 | Loss: 0.00001133
Iteration 11/1000 | Loss: 0.00001111
Iteration 12/1000 | Loss: 0.00001105
Iteration 13/1000 | Loss: 0.00001103
Iteration 14/1000 | Loss: 0.00001101
Iteration 15/1000 | Loss: 0.00001085
Iteration 16/1000 | Loss: 0.00001080
Iteration 17/1000 | Loss: 0.00001071
Iteration 18/1000 | Loss: 0.00001066
Iteration 19/1000 | Loss: 0.00001063
Iteration 20/1000 | Loss: 0.00001062
Iteration 21/1000 | Loss: 0.00001057
Iteration 22/1000 | Loss: 0.00001054
Iteration 23/1000 | Loss: 0.00001050
Iteration 24/1000 | Loss: 0.00001049
Iteration 25/1000 | Loss: 0.00001044
Iteration 26/1000 | Loss: 0.00001039
Iteration 27/1000 | Loss: 0.00001036
Iteration 28/1000 | Loss: 0.00001036
Iteration 29/1000 | Loss: 0.00001036
Iteration 30/1000 | Loss: 0.00001036
Iteration 31/1000 | Loss: 0.00001035
Iteration 32/1000 | Loss: 0.00001032
Iteration 33/1000 | Loss: 0.00001032
Iteration 34/1000 | Loss: 0.00001032
Iteration 35/1000 | Loss: 0.00001031
Iteration 36/1000 | Loss: 0.00001030
Iteration 37/1000 | Loss: 0.00001030
Iteration 38/1000 | Loss: 0.00001030
Iteration 39/1000 | Loss: 0.00001030
Iteration 40/1000 | Loss: 0.00001029
Iteration 41/1000 | Loss: 0.00001029
Iteration 42/1000 | Loss: 0.00001029
Iteration 43/1000 | Loss: 0.00001029
Iteration 44/1000 | Loss: 0.00001029
Iteration 45/1000 | Loss: 0.00001029
Iteration 46/1000 | Loss: 0.00001029
Iteration 47/1000 | Loss: 0.00001029
Iteration 48/1000 | Loss: 0.00001029
Iteration 49/1000 | Loss: 0.00001029
Iteration 50/1000 | Loss: 0.00001029
Iteration 51/1000 | Loss: 0.00001029
Iteration 52/1000 | Loss: 0.00001028
Iteration 53/1000 | Loss: 0.00001028
Iteration 54/1000 | Loss: 0.00001028
Iteration 55/1000 | Loss: 0.00001028
Iteration 56/1000 | Loss: 0.00001028
Iteration 57/1000 | Loss: 0.00001028
Iteration 58/1000 | Loss: 0.00001028
Iteration 59/1000 | Loss: 0.00001028
Iteration 60/1000 | Loss: 0.00001027
Iteration 61/1000 | Loss: 0.00001027
Iteration 62/1000 | Loss: 0.00001026
Iteration 63/1000 | Loss: 0.00001026
Iteration 64/1000 | Loss: 0.00001026
Iteration 65/1000 | Loss: 0.00001026
Iteration 66/1000 | Loss: 0.00001026
Iteration 67/1000 | Loss: 0.00001026
Iteration 68/1000 | Loss: 0.00001026
Iteration 69/1000 | Loss: 0.00001026
Iteration 70/1000 | Loss: 0.00001026
Iteration 71/1000 | Loss: 0.00001026
Iteration 72/1000 | Loss: 0.00001026
Iteration 73/1000 | Loss: 0.00001026
Iteration 74/1000 | Loss: 0.00001026
Iteration 75/1000 | Loss: 0.00001026
Iteration 76/1000 | Loss: 0.00001026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.0260887393087614e-05, 1.0260887393087614e-05, 1.0260887393087614e-05, 1.0260887393087614e-05, 1.0260887393087614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0260887393087614e-05

Optimization complete. Final v2v error: 2.775118112564087 mm

Highest mean error: 3.2144720554351807 mm for frame 111

Lowest mean error: 2.2855565547943115 mm for frame 203

Saving results

Total time: 47.292614459991455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486913
Iteration 2/25 | Loss: 0.00140305
Iteration 3/25 | Loss: 0.00126745
Iteration 4/25 | Loss: 0.00125302
Iteration 5/25 | Loss: 0.00124977
Iteration 6/25 | Loss: 0.00124927
Iteration 7/25 | Loss: 0.00124927
Iteration 8/25 | Loss: 0.00124927
Iteration 9/25 | Loss: 0.00124927
Iteration 10/25 | Loss: 0.00124927
Iteration 11/25 | Loss: 0.00124927
Iteration 12/25 | Loss: 0.00124927
Iteration 13/25 | Loss: 0.00124927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001249267952516675, 0.001249267952516675, 0.001249267952516675, 0.001249267952516675, 0.001249267952516675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001249267952516675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25650525
Iteration 2/25 | Loss: 0.00183146
Iteration 3/25 | Loss: 0.00183144
Iteration 4/25 | Loss: 0.00183144
Iteration 5/25 | Loss: 0.00183144
Iteration 6/25 | Loss: 0.00183144
Iteration 7/25 | Loss: 0.00183144
Iteration 8/25 | Loss: 0.00183144
Iteration 9/25 | Loss: 0.00183144
Iteration 10/25 | Loss: 0.00183144
Iteration 11/25 | Loss: 0.00183144
Iteration 12/25 | Loss: 0.00183144
Iteration 13/25 | Loss: 0.00183144
Iteration 14/25 | Loss: 0.00183144
Iteration 15/25 | Loss: 0.00183144
Iteration 16/25 | Loss: 0.00183144
Iteration 17/25 | Loss: 0.00183144
Iteration 18/25 | Loss: 0.00183144
Iteration 19/25 | Loss: 0.00183144
Iteration 20/25 | Loss: 0.00183144
Iteration 21/25 | Loss: 0.00183144
Iteration 22/25 | Loss: 0.00183144
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0018314407207071781, 0.0018314407207071781, 0.0018314407207071781, 0.0018314407207071781, 0.0018314407207071781]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018314407207071781

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183144
Iteration 2/1000 | Loss: 0.00003870
Iteration 3/1000 | Loss: 0.00002396
Iteration 4/1000 | Loss: 0.00002178
Iteration 5/1000 | Loss: 0.00002051
Iteration 6/1000 | Loss: 0.00001986
Iteration 7/1000 | Loss: 0.00001938
Iteration 8/1000 | Loss: 0.00001890
Iteration 9/1000 | Loss: 0.00001856
Iteration 10/1000 | Loss: 0.00001836
Iteration 11/1000 | Loss: 0.00001831
Iteration 12/1000 | Loss: 0.00001823
Iteration 13/1000 | Loss: 0.00001821
Iteration 14/1000 | Loss: 0.00001818
Iteration 15/1000 | Loss: 0.00001813
Iteration 16/1000 | Loss: 0.00001810
Iteration 17/1000 | Loss: 0.00001805
Iteration 18/1000 | Loss: 0.00001797
Iteration 19/1000 | Loss: 0.00001795
Iteration 20/1000 | Loss: 0.00001793
Iteration 21/1000 | Loss: 0.00001792
Iteration 22/1000 | Loss: 0.00001780
Iteration 23/1000 | Loss: 0.00001778
Iteration 24/1000 | Loss: 0.00001777
Iteration 25/1000 | Loss: 0.00001777
Iteration 26/1000 | Loss: 0.00001776
Iteration 27/1000 | Loss: 0.00001775
Iteration 28/1000 | Loss: 0.00001771
Iteration 29/1000 | Loss: 0.00001769
Iteration 30/1000 | Loss: 0.00001768
Iteration 31/1000 | Loss: 0.00001768
Iteration 32/1000 | Loss: 0.00001767
Iteration 33/1000 | Loss: 0.00001767
Iteration 34/1000 | Loss: 0.00001766
Iteration 35/1000 | Loss: 0.00001766
Iteration 36/1000 | Loss: 0.00001765
Iteration 37/1000 | Loss: 0.00001764
Iteration 38/1000 | Loss: 0.00001764
Iteration 39/1000 | Loss: 0.00001764
Iteration 40/1000 | Loss: 0.00001764
Iteration 41/1000 | Loss: 0.00001763
Iteration 42/1000 | Loss: 0.00001763
Iteration 43/1000 | Loss: 0.00001763
Iteration 44/1000 | Loss: 0.00001763
Iteration 45/1000 | Loss: 0.00001762
Iteration 46/1000 | Loss: 0.00001762
Iteration 47/1000 | Loss: 0.00001761
Iteration 48/1000 | Loss: 0.00001761
Iteration 49/1000 | Loss: 0.00001760
Iteration 50/1000 | Loss: 0.00001759
Iteration 51/1000 | Loss: 0.00001757
Iteration 52/1000 | Loss: 0.00001757
Iteration 53/1000 | Loss: 0.00001757
Iteration 54/1000 | Loss: 0.00001757
Iteration 55/1000 | Loss: 0.00001757
Iteration 56/1000 | Loss: 0.00001757
Iteration 57/1000 | Loss: 0.00001757
Iteration 58/1000 | Loss: 0.00001757
Iteration 59/1000 | Loss: 0.00001757
Iteration 60/1000 | Loss: 0.00001756
Iteration 61/1000 | Loss: 0.00001756
Iteration 62/1000 | Loss: 0.00001756
Iteration 63/1000 | Loss: 0.00001756
Iteration 64/1000 | Loss: 0.00001756
Iteration 65/1000 | Loss: 0.00001756
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001756
Iteration 69/1000 | Loss: 0.00001756
Iteration 70/1000 | Loss: 0.00001756
Iteration 71/1000 | Loss: 0.00001756
Iteration 72/1000 | Loss: 0.00001755
Iteration 73/1000 | Loss: 0.00001755
Iteration 74/1000 | Loss: 0.00001754
Iteration 75/1000 | Loss: 0.00001754
Iteration 76/1000 | Loss: 0.00001753
Iteration 77/1000 | Loss: 0.00001753
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001752
Iteration 80/1000 | Loss: 0.00001752
Iteration 81/1000 | Loss: 0.00001752
Iteration 82/1000 | Loss: 0.00001752
Iteration 83/1000 | Loss: 0.00001752
Iteration 84/1000 | Loss: 0.00001752
Iteration 85/1000 | Loss: 0.00001752
Iteration 86/1000 | Loss: 0.00001751
Iteration 87/1000 | Loss: 0.00001751
Iteration 88/1000 | Loss: 0.00001750
Iteration 89/1000 | Loss: 0.00001750
Iteration 90/1000 | Loss: 0.00001750
Iteration 91/1000 | Loss: 0.00001750
Iteration 92/1000 | Loss: 0.00001749
Iteration 93/1000 | Loss: 0.00001749
Iteration 94/1000 | Loss: 0.00001749
Iteration 95/1000 | Loss: 0.00001749
Iteration 96/1000 | Loss: 0.00001749
Iteration 97/1000 | Loss: 0.00001748
Iteration 98/1000 | Loss: 0.00001748
Iteration 99/1000 | Loss: 0.00001748
Iteration 100/1000 | Loss: 0.00001748
Iteration 101/1000 | Loss: 0.00001747
Iteration 102/1000 | Loss: 0.00001747
Iteration 103/1000 | Loss: 0.00001747
Iteration 104/1000 | Loss: 0.00001747
Iteration 105/1000 | Loss: 0.00001746
Iteration 106/1000 | Loss: 0.00001746
Iteration 107/1000 | Loss: 0.00001746
Iteration 108/1000 | Loss: 0.00001746
Iteration 109/1000 | Loss: 0.00001746
Iteration 110/1000 | Loss: 0.00001745
Iteration 111/1000 | Loss: 0.00001745
Iteration 112/1000 | Loss: 0.00001745
Iteration 113/1000 | Loss: 0.00001745
Iteration 114/1000 | Loss: 0.00001745
Iteration 115/1000 | Loss: 0.00001745
Iteration 116/1000 | Loss: 0.00001745
Iteration 117/1000 | Loss: 0.00001744
Iteration 118/1000 | Loss: 0.00001743
Iteration 119/1000 | Loss: 0.00001743
Iteration 120/1000 | Loss: 0.00001743
Iteration 121/1000 | Loss: 0.00001743
Iteration 122/1000 | Loss: 0.00001743
Iteration 123/1000 | Loss: 0.00001743
Iteration 124/1000 | Loss: 0.00001743
Iteration 125/1000 | Loss: 0.00001743
Iteration 126/1000 | Loss: 0.00001743
Iteration 127/1000 | Loss: 0.00001743
Iteration 128/1000 | Loss: 0.00001743
Iteration 129/1000 | Loss: 0.00001743
Iteration 130/1000 | Loss: 0.00001742
Iteration 131/1000 | Loss: 0.00001742
Iteration 132/1000 | Loss: 0.00001741
Iteration 133/1000 | Loss: 0.00001741
Iteration 134/1000 | Loss: 0.00001741
Iteration 135/1000 | Loss: 0.00001741
Iteration 136/1000 | Loss: 0.00001741
Iteration 137/1000 | Loss: 0.00001741
Iteration 138/1000 | Loss: 0.00001741
Iteration 139/1000 | Loss: 0.00001741
Iteration 140/1000 | Loss: 0.00001741
Iteration 141/1000 | Loss: 0.00001741
Iteration 142/1000 | Loss: 0.00001741
Iteration 143/1000 | Loss: 0.00001740
Iteration 144/1000 | Loss: 0.00001740
Iteration 145/1000 | Loss: 0.00001740
Iteration 146/1000 | Loss: 0.00001740
Iteration 147/1000 | Loss: 0.00001740
Iteration 148/1000 | Loss: 0.00001740
Iteration 149/1000 | Loss: 0.00001739
Iteration 150/1000 | Loss: 0.00001739
Iteration 151/1000 | Loss: 0.00001739
Iteration 152/1000 | Loss: 0.00001739
Iteration 153/1000 | Loss: 0.00001739
Iteration 154/1000 | Loss: 0.00001739
Iteration 155/1000 | Loss: 0.00001738
Iteration 156/1000 | Loss: 0.00001738
Iteration 157/1000 | Loss: 0.00001738
Iteration 158/1000 | Loss: 0.00001738
Iteration 159/1000 | Loss: 0.00001738
Iteration 160/1000 | Loss: 0.00001738
Iteration 161/1000 | Loss: 0.00001738
Iteration 162/1000 | Loss: 0.00001738
Iteration 163/1000 | Loss: 0.00001737
Iteration 164/1000 | Loss: 0.00001737
Iteration 165/1000 | Loss: 0.00001737
Iteration 166/1000 | Loss: 0.00001737
Iteration 167/1000 | Loss: 0.00001737
Iteration 168/1000 | Loss: 0.00001737
Iteration 169/1000 | Loss: 0.00001737
Iteration 170/1000 | Loss: 0.00001737
Iteration 171/1000 | Loss: 0.00001736
Iteration 172/1000 | Loss: 0.00001736
Iteration 173/1000 | Loss: 0.00001736
Iteration 174/1000 | Loss: 0.00001736
Iteration 175/1000 | Loss: 0.00001735
Iteration 176/1000 | Loss: 0.00001735
Iteration 177/1000 | Loss: 0.00001735
Iteration 178/1000 | Loss: 0.00001735
Iteration 179/1000 | Loss: 0.00001735
Iteration 180/1000 | Loss: 0.00001735
Iteration 181/1000 | Loss: 0.00001735
Iteration 182/1000 | Loss: 0.00001735
Iteration 183/1000 | Loss: 0.00001734
Iteration 184/1000 | Loss: 0.00001734
Iteration 185/1000 | Loss: 0.00001734
Iteration 186/1000 | Loss: 0.00001734
Iteration 187/1000 | Loss: 0.00001734
Iteration 188/1000 | Loss: 0.00001734
Iteration 189/1000 | Loss: 0.00001734
Iteration 190/1000 | Loss: 0.00001734
Iteration 191/1000 | Loss: 0.00001734
Iteration 192/1000 | Loss: 0.00001734
Iteration 193/1000 | Loss: 0.00001734
Iteration 194/1000 | Loss: 0.00001733
Iteration 195/1000 | Loss: 0.00001733
Iteration 196/1000 | Loss: 0.00001733
Iteration 197/1000 | Loss: 0.00001733
Iteration 198/1000 | Loss: 0.00001733
Iteration 199/1000 | Loss: 0.00001733
Iteration 200/1000 | Loss: 0.00001733
Iteration 201/1000 | Loss: 0.00001733
Iteration 202/1000 | Loss: 0.00001733
Iteration 203/1000 | Loss: 0.00001733
Iteration 204/1000 | Loss: 0.00001733
Iteration 205/1000 | Loss: 0.00001733
Iteration 206/1000 | Loss: 0.00001732
Iteration 207/1000 | Loss: 0.00001732
Iteration 208/1000 | Loss: 0.00001732
Iteration 209/1000 | Loss: 0.00001732
Iteration 210/1000 | Loss: 0.00001732
Iteration 211/1000 | Loss: 0.00001732
Iteration 212/1000 | Loss: 0.00001732
Iteration 213/1000 | Loss: 0.00001732
Iteration 214/1000 | Loss: 0.00001732
Iteration 215/1000 | Loss: 0.00001732
Iteration 216/1000 | Loss: 0.00001731
Iteration 217/1000 | Loss: 0.00001731
Iteration 218/1000 | Loss: 0.00001731
Iteration 219/1000 | Loss: 0.00001731
Iteration 220/1000 | Loss: 0.00001731
Iteration 221/1000 | Loss: 0.00001731
Iteration 222/1000 | Loss: 0.00001731
Iteration 223/1000 | Loss: 0.00001730
Iteration 224/1000 | Loss: 0.00001730
Iteration 225/1000 | Loss: 0.00001730
Iteration 226/1000 | Loss: 0.00001730
Iteration 227/1000 | Loss: 0.00001730
Iteration 228/1000 | Loss: 0.00001729
Iteration 229/1000 | Loss: 0.00001729
Iteration 230/1000 | Loss: 0.00001729
Iteration 231/1000 | Loss: 0.00001729
Iteration 232/1000 | Loss: 0.00001729
Iteration 233/1000 | Loss: 0.00001729
Iteration 234/1000 | Loss: 0.00001729
Iteration 235/1000 | Loss: 0.00001729
Iteration 236/1000 | Loss: 0.00001729
Iteration 237/1000 | Loss: 0.00001728
Iteration 238/1000 | Loss: 0.00001728
Iteration 239/1000 | Loss: 0.00001728
Iteration 240/1000 | Loss: 0.00001728
Iteration 241/1000 | Loss: 0.00001728
Iteration 242/1000 | Loss: 0.00001728
Iteration 243/1000 | Loss: 0.00001728
Iteration 244/1000 | Loss: 0.00001728
Iteration 245/1000 | Loss: 0.00001728
Iteration 246/1000 | Loss: 0.00001728
Iteration 247/1000 | Loss: 0.00001728
Iteration 248/1000 | Loss: 0.00001728
Iteration 249/1000 | Loss: 0.00001728
Iteration 250/1000 | Loss: 0.00001728
Iteration 251/1000 | Loss: 0.00001728
Iteration 252/1000 | Loss: 0.00001728
Iteration 253/1000 | Loss: 0.00001728
Iteration 254/1000 | Loss: 0.00001728
Iteration 255/1000 | Loss: 0.00001727
Iteration 256/1000 | Loss: 0.00001727
Iteration 257/1000 | Loss: 0.00001727
Iteration 258/1000 | Loss: 0.00001727
Iteration 259/1000 | Loss: 0.00001727
Iteration 260/1000 | Loss: 0.00001727
Iteration 261/1000 | Loss: 0.00001727
Iteration 262/1000 | Loss: 0.00001727
Iteration 263/1000 | Loss: 0.00001727
Iteration 264/1000 | Loss: 0.00001727
Iteration 265/1000 | Loss: 0.00001727
Iteration 266/1000 | Loss: 0.00001727
Iteration 267/1000 | Loss: 0.00001727
Iteration 268/1000 | Loss: 0.00001726
Iteration 269/1000 | Loss: 0.00001726
Iteration 270/1000 | Loss: 0.00001726
Iteration 271/1000 | Loss: 0.00001726
Iteration 272/1000 | Loss: 0.00001726
Iteration 273/1000 | Loss: 0.00001726
Iteration 274/1000 | Loss: 0.00001726
Iteration 275/1000 | Loss: 0.00001726
Iteration 276/1000 | Loss: 0.00001726
Iteration 277/1000 | Loss: 0.00001726
Iteration 278/1000 | Loss: 0.00001726
Iteration 279/1000 | Loss: 0.00001726
Iteration 280/1000 | Loss: 0.00001726
Iteration 281/1000 | Loss: 0.00001726
Iteration 282/1000 | Loss: 0.00001725
Iteration 283/1000 | Loss: 0.00001725
Iteration 284/1000 | Loss: 0.00001725
Iteration 285/1000 | Loss: 0.00001725
Iteration 286/1000 | Loss: 0.00001725
Iteration 287/1000 | Loss: 0.00001725
Iteration 288/1000 | Loss: 0.00001725
Iteration 289/1000 | Loss: 0.00001725
Iteration 290/1000 | Loss: 0.00001725
Iteration 291/1000 | Loss: 0.00001725
Iteration 292/1000 | Loss: 0.00001725
Iteration 293/1000 | Loss: 0.00001725
Iteration 294/1000 | Loss: 0.00001725
Iteration 295/1000 | Loss: 0.00001725
Iteration 296/1000 | Loss: 0.00001725
Iteration 297/1000 | Loss: 0.00001725
Iteration 298/1000 | Loss: 0.00001725
Iteration 299/1000 | Loss: 0.00001724
Iteration 300/1000 | Loss: 0.00001724
Iteration 301/1000 | Loss: 0.00001724
Iteration 302/1000 | Loss: 0.00001724
Iteration 303/1000 | Loss: 0.00001724
Iteration 304/1000 | Loss: 0.00001724
Iteration 305/1000 | Loss: 0.00001724
Iteration 306/1000 | Loss: 0.00001724
Iteration 307/1000 | Loss: 0.00001724
Iteration 308/1000 | Loss: 0.00001724
Iteration 309/1000 | Loss: 0.00001724
Iteration 310/1000 | Loss: 0.00001724
Iteration 311/1000 | Loss: 0.00001724
Iteration 312/1000 | Loss: 0.00001724
Iteration 313/1000 | Loss: 0.00001723
Iteration 314/1000 | Loss: 0.00001723
Iteration 315/1000 | Loss: 0.00001723
Iteration 316/1000 | Loss: 0.00001723
Iteration 317/1000 | Loss: 0.00001723
Iteration 318/1000 | Loss: 0.00001723
Iteration 319/1000 | Loss: 0.00001723
Iteration 320/1000 | Loss: 0.00001723
Iteration 321/1000 | Loss: 0.00001723
Iteration 322/1000 | Loss: 0.00001723
Iteration 323/1000 | Loss: 0.00001723
Iteration 324/1000 | Loss: 0.00001723
Iteration 325/1000 | Loss: 0.00001723
Iteration 326/1000 | Loss: 0.00001723
Iteration 327/1000 | Loss: 0.00001723
Iteration 328/1000 | Loss: 0.00001723
Iteration 329/1000 | Loss: 0.00001723
Iteration 330/1000 | Loss: 0.00001723
Iteration 331/1000 | Loss: 0.00001723
Iteration 332/1000 | Loss: 0.00001722
Iteration 333/1000 | Loss: 0.00001722
Iteration 334/1000 | Loss: 0.00001722
Iteration 335/1000 | Loss: 0.00001722
Iteration 336/1000 | Loss: 0.00001722
Iteration 337/1000 | Loss: 0.00001722
Iteration 338/1000 | Loss: 0.00001722
Iteration 339/1000 | Loss: 0.00001722
Iteration 340/1000 | Loss: 0.00001722
Iteration 341/1000 | Loss: 0.00001722
Iteration 342/1000 | Loss: 0.00001722
Iteration 343/1000 | Loss: 0.00001722
Iteration 344/1000 | Loss: 0.00001722
Iteration 345/1000 | Loss: 0.00001722
Iteration 346/1000 | Loss: 0.00001722
Iteration 347/1000 | Loss: 0.00001722
Iteration 348/1000 | Loss: 0.00001722
Iteration 349/1000 | Loss: 0.00001722
Iteration 350/1000 | Loss: 0.00001722
Iteration 351/1000 | Loss: 0.00001722
Iteration 352/1000 | Loss: 0.00001721
Iteration 353/1000 | Loss: 0.00001721
Iteration 354/1000 | Loss: 0.00001721
Iteration 355/1000 | Loss: 0.00001721
Iteration 356/1000 | Loss: 0.00001721
Iteration 357/1000 | Loss: 0.00001721
Iteration 358/1000 | Loss: 0.00001721
Iteration 359/1000 | Loss: 0.00001721
Iteration 360/1000 | Loss: 0.00001721
Iteration 361/1000 | Loss: 0.00001721
Iteration 362/1000 | Loss: 0.00001721
Iteration 363/1000 | Loss: 0.00001721
Iteration 364/1000 | Loss: 0.00001721
Iteration 365/1000 | Loss: 0.00001721
Iteration 366/1000 | Loss: 0.00001721
Iteration 367/1000 | Loss: 0.00001721
Iteration 368/1000 | Loss: 0.00001721
Iteration 369/1000 | Loss: 0.00001721
Iteration 370/1000 | Loss: 0.00001721
Iteration 371/1000 | Loss: 0.00001721
Iteration 372/1000 | Loss: 0.00001721
Iteration 373/1000 | Loss: 0.00001720
Iteration 374/1000 | Loss: 0.00001720
Iteration 375/1000 | Loss: 0.00001720
Iteration 376/1000 | Loss: 0.00001720
Iteration 377/1000 | Loss: 0.00001720
Iteration 378/1000 | Loss: 0.00001720
Iteration 379/1000 | Loss: 0.00001720
Iteration 380/1000 | Loss: 0.00001720
Iteration 381/1000 | Loss: 0.00001720
Iteration 382/1000 | Loss: 0.00001720
Iteration 383/1000 | Loss: 0.00001720
Iteration 384/1000 | Loss: 0.00001720
Iteration 385/1000 | Loss: 0.00001720
Iteration 386/1000 | Loss: 0.00001720
Iteration 387/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 387. Stopping optimization.
Last 5 losses: [1.720046748232562e-05, 1.720046748232562e-05, 1.720046748232562e-05, 1.720046748232562e-05, 1.720046748232562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.720046748232562e-05

Optimization complete. Final v2v error: 3.374028444290161 mm

Highest mean error: 3.9971554279327393 mm for frame 132

Lowest mean error: 2.846024990081787 mm for frame 1

Saving results

Total time: 52.99256610870361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464769
Iteration 2/25 | Loss: 0.00127203
Iteration 3/25 | Loss: 0.00119594
Iteration 4/25 | Loss: 0.00118795
Iteration 5/25 | Loss: 0.00118564
Iteration 6/25 | Loss: 0.00118564
Iteration 7/25 | Loss: 0.00118564
Iteration 8/25 | Loss: 0.00118564
Iteration 9/25 | Loss: 0.00118564
Iteration 10/25 | Loss: 0.00118564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011856368510052562, 0.0011856368510052562, 0.0011856368510052562, 0.0011856368510052562, 0.0011856368510052562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011856368510052562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25928497
Iteration 2/25 | Loss: 0.00170366
Iteration 3/25 | Loss: 0.00170366
Iteration 4/25 | Loss: 0.00170366
Iteration 5/25 | Loss: 0.00170366
Iteration 6/25 | Loss: 0.00170366
Iteration 7/25 | Loss: 0.00170366
Iteration 8/25 | Loss: 0.00170365
Iteration 9/25 | Loss: 0.00170365
Iteration 10/25 | Loss: 0.00170365
Iteration 11/25 | Loss: 0.00170365
Iteration 12/25 | Loss: 0.00170365
Iteration 13/25 | Loss: 0.00170365
Iteration 14/25 | Loss: 0.00170365
Iteration 15/25 | Loss: 0.00170365
Iteration 16/25 | Loss: 0.00170365
Iteration 17/25 | Loss: 0.00170365
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017036536009982228, 0.0017036536009982228, 0.0017036536009982228, 0.0017036536009982228, 0.0017036536009982228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017036536009982228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170365
Iteration 2/1000 | Loss: 0.00002023
Iteration 3/1000 | Loss: 0.00001614
Iteration 4/1000 | Loss: 0.00001493
Iteration 5/1000 | Loss: 0.00001436
Iteration 6/1000 | Loss: 0.00001398
Iteration 7/1000 | Loss: 0.00001371
Iteration 8/1000 | Loss: 0.00001340
Iteration 9/1000 | Loss: 0.00001317
Iteration 10/1000 | Loss: 0.00001302
Iteration 11/1000 | Loss: 0.00001300
Iteration 12/1000 | Loss: 0.00001293
Iteration 13/1000 | Loss: 0.00001285
Iteration 14/1000 | Loss: 0.00001284
Iteration 15/1000 | Loss: 0.00001277
Iteration 16/1000 | Loss: 0.00001273
Iteration 17/1000 | Loss: 0.00001268
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001266
Iteration 20/1000 | Loss: 0.00001266
Iteration 21/1000 | Loss: 0.00001265
Iteration 22/1000 | Loss: 0.00001265
Iteration 23/1000 | Loss: 0.00001264
Iteration 24/1000 | Loss: 0.00001262
Iteration 25/1000 | Loss: 0.00001261
Iteration 26/1000 | Loss: 0.00001261
Iteration 27/1000 | Loss: 0.00001260
Iteration 28/1000 | Loss: 0.00001260
Iteration 29/1000 | Loss: 0.00001259
Iteration 30/1000 | Loss: 0.00001259
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001256
Iteration 33/1000 | Loss: 0.00001252
Iteration 34/1000 | Loss: 0.00001252
Iteration 35/1000 | Loss: 0.00001251
Iteration 36/1000 | Loss: 0.00001251
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001250
Iteration 39/1000 | Loss: 0.00001249
Iteration 40/1000 | Loss: 0.00001249
Iteration 41/1000 | Loss: 0.00001249
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001249
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001247
Iteration 47/1000 | Loss: 0.00001247
Iteration 48/1000 | Loss: 0.00001247
Iteration 49/1000 | Loss: 0.00001247
Iteration 50/1000 | Loss: 0.00001247
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001246
Iteration 55/1000 | Loss: 0.00001246
Iteration 56/1000 | Loss: 0.00001246
Iteration 57/1000 | Loss: 0.00001246
Iteration 58/1000 | Loss: 0.00001246
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001246
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001245
Iteration 64/1000 | Loss: 0.00001245
Iteration 65/1000 | Loss: 0.00001245
Iteration 66/1000 | Loss: 0.00001244
Iteration 67/1000 | Loss: 0.00001244
Iteration 68/1000 | Loss: 0.00001243
Iteration 69/1000 | Loss: 0.00001243
Iteration 70/1000 | Loss: 0.00001243
Iteration 71/1000 | Loss: 0.00001243
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001242
Iteration 74/1000 | Loss: 0.00001242
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001242
Iteration 77/1000 | Loss: 0.00001242
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00001242
Iteration 80/1000 | Loss: 0.00001241
Iteration 81/1000 | Loss: 0.00001241
Iteration 82/1000 | Loss: 0.00001241
Iteration 83/1000 | Loss: 0.00001241
Iteration 84/1000 | Loss: 0.00001240
Iteration 85/1000 | Loss: 0.00001240
Iteration 86/1000 | Loss: 0.00001240
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001240
Iteration 91/1000 | Loss: 0.00001240
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001239
Iteration 94/1000 | Loss: 0.00001239
Iteration 95/1000 | Loss: 0.00001239
Iteration 96/1000 | Loss: 0.00001239
Iteration 97/1000 | Loss: 0.00001239
Iteration 98/1000 | Loss: 0.00001239
Iteration 99/1000 | Loss: 0.00001239
Iteration 100/1000 | Loss: 0.00001239
Iteration 101/1000 | Loss: 0.00001238
Iteration 102/1000 | Loss: 0.00001238
Iteration 103/1000 | Loss: 0.00001238
Iteration 104/1000 | Loss: 0.00001238
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001237
Iteration 108/1000 | Loss: 0.00001237
Iteration 109/1000 | Loss: 0.00001237
Iteration 110/1000 | Loss: 0.00001237
Iteration 111/1000 | Loss: 0.00001237
Iteration 112/1000 | Loss: 0.00001237
Iteration 113/1000 | Loss: 0.00001237
Iteration 114/1000 | Loss: 0.00001236
Iteration 115/1000 | Loss: 0.00001236
Iteration 116/1000 | Loss: 0.00001236
Iteration 117/1000 | Loss: 0.00001235
Iteration 118/1000 | Loss: 0.00001235
Iteration 119/1000 | Loss: 0.00001235
Iteration 120/1000 | Loss: 0.00001235
Iteration 121/1000 | Loss: 0.00001235
Iteration 122/1000 | Loss: 0.00001235
Iteration 123/1000 | Loss: 0.00001235
Iteration 124/1000 | Loss: 0.00001234
Iteration 125/1000 | Loss: 0.00001234
Iteration 126/1000 | Loss: 0.00001234
Iteration 127/1000 | Loss: 0.00001234
Iteration 128/1000 | Loss: 0.00001234
Iteration 129/1000 | Loss: 0.00001234
Iteration 130/1000 | Loss: 0.00001234
Iteration 131/1000 | Loss: 0.00001234
Iteration 132/1000 | Loss: 0.00001234
Iteration 133/1000 | Loss: 0.00001234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.234438423125539e-05, 1.234438423125539e-05, 1.234438423125539e-05, 1.234438423125539e-05, 1.234438423125539e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.234438423125539e-05

Optimization complete. Final v2v error: 2.9358372688293457 mm

Highest mean error: 3.303356647491455 mm for frame 46

Lowest mean error: 2.3827896118164062 mm for frame 185

Saving results

Total time: 39.54736876487732
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970911
Iteration 2/25 | Loss: 0.00382125
Iteration 3/25 | Loss: 0.00279059
Iteration 4/25 | Loss: 0.00223444
Iteration 5/25 | Loss: 0.00215325
Iteration 6/25 | Loss: 0.00202209
Iteration 7/25 | Loss: 0.00190438
Iteration 8/25 | Loss: 0.00180183
Iteration 9/25 | Loss: 0.00176176
Iteration 10/25 | Loss: 0.00169736
Iteration 11/25 | Loss: 0.00165702
Iteration 12/25 | Loss: 0.00165169
Iteration 13/25 | Loss: 0.00165175
Iteration 14/25 | Loss: 0.00161197
Iteration 15/25 | Loss: 0.00161864
Iteration 16/25 | Loss: 0.00161216
Iteration 17/25 | Loss: 0.00161165
Iteration 18/25 | Loss: 0.00161453
Iteration 19/25 | Loss: 0.00160438
Iteration 20/25 | Loss: 0.00160434
Iteration 21/25 | Loss: 0.00161096
Iteration 22/25 | Loss: 0.00159254
Iteration 23/25 | Loss: 0.00158994
Iteration 24/25 | Loss: 0.00158959
Iteration 25/25 | Loss: 0.00159211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88562489
Iteration 2/25 | Loss: 0.00575791
Iteration 3/25 | Loss: 0.00509249
Iteration 4/25 | Loss: 0.00509247
Iteration 5/25 | Loss: 0.00509247
Iteration 6/25 | Loss: 0.00509247
Iteration 7/25 | Loss: 0.00509247
Iteration 8/25 | Loss: 0.00509247
Iteration 9/25 | Loss: 0.00509247
Iteration 10/25 | Loss: 0.00509247
Iteration 11/25 | Loss: 0.00509247
Iteration 12/25 | Loss: 0.00509247
Iteration 13/25 | Loss: 0.00509246
Iteration 14/25 | Loss: 0.00509246
Iteration 15/25 | Loss: 0.00509246
Iteration 16/25 | Loss: 0.00509246
Iteration 17/25 | Loss: 0.00509246
Iteration 18/25 | Loss: 0.00509246
Iteration 19/25 | Loss: 0.00509246
Iteration 20/25 | Loss: 0.00509246
Iteration 21/25 | Loss: 0.00509246
Iteration 22/25 | Loss: 0.00509246
Iteration 23/25 | Loss: 0.00509246
Iteration 24/25 | Loss: 0.00509246
Iteration 25/25 | Loss: 0.00509246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00509246
Iteration 2/1000 | Loss: 0.00066773
Iteration 3/1000 | Loss: 0.00042237
Iteration 4/1000 | Loss: 0.00058558
Iteration 5/1000 | Loss: 0.00155965
Iteration 6/1000 | Loss: 0.00094843
Iteration 7/1000 | Loss: 0.00289255
Iteration 8/1000 | Loss: 0.00062100
Iteration 9/1000 | Loss: 0.00027319
Iteration 10/1000 | Loss: 0.00317826
Iteration 11/1000 | Loss: 0.00060289
Iteration 12/1000 | Loss: 0.00132522
Iteration 13/1000 | Loss: 0.00178402
Iteration 14/1000 | Loss: 0.00132101
Iteration 15/1000 | Loss: 0.00180097
Iteration 16/1000 | Loss: 0.00047087
Iteration 17/1000 | Loss: 0.00312083
Iteration 18/1000 | Loss: 0.00118283
Iteration 19/1000 | Loss: 0.00664701
Iteration 20/1000 | Loss: 0.00029629
Iteration 21/1000 | Loss: 0.00019901
Iteration 22/1000 | Loss: 0.00391725
Iteration 23/1000 | Loss: 0.00019531
Iteration 24/1000 | Loss: 0.00189903
Iteration 25/1000 | Loss: 0.00020050
Iteration 26/1000 | Loss: 0.00014700
Iteration 27/1000 | Loss: 0.00114966
Iteration 28/1000 | Loss: 0.00020187
Iteration 29/1000 | Loss: 0.00134834
Iteration 30/1000 | Loss: 0.00353517
Iteration 31/1000 | Loss: 0.00165637
Iteration 32/1000 | Loss: 0.00018080
Iteration 33/1000 | Loss: 0.00015040
Iteration 34/1000 | Loss: 0.00152456
Iteration 35/1000 | Loss: 0.00066175
Iteration 36/1000 | Loss: 0.00064310
Iteration 37/1000 | Loss: 0.00088287
Iteration 38/1000 | Loss: 0.00012281
Iteration 39/1000 | Loss: 0.00513908
Iteration 40/1000 | Loss: 0.00016285
Iteration 41/1000 | Loss: 0.00159979
Iteration 42/1000 | Loss: 0.00208920
Iteration 43/1000 | Loss: 0.00081798
Iteration 44/1000 | Loss: 0.00013706
Iteration 45/1000 | Loss: 0.00009995
Iteration 46/1000 | Loss: 0.00128463
Iteration 47/1000 | Loss: 0.00012275
Iteration 48/1000 | Loss: 0.00009127
Iteration 49/1000 | Loss: 0.00009319
Iteration 50/1000 | Loss: 0.00008049
Iteration 51/1000 | Loss: 0.00009100
Iteration 52/1000 | Loss: 0.00007902
Iteration 53/1000 | Loss: 0.00177514
Iteration 54/1000 | Loss: 0.00167054
Iteration 55/1000 | Loss: 0.00176686
Iteration 56/1000 | Loss: 0.00011130
Iteration 57/1000 | Loss: 0.00095365
Iteration 58/1000 | Loss: 0.00009031
Iteration 59/1000 | Loss: 0.00088678
Iteration 60/1000 | Loss: 0.00015408
Iteration 61/1000 | Loss: 0.00008140
Iteration 62/1000 | Loss: 0.00008199
Iteration 63/1000 | Loss: 0.00008038
Iteration 64/1000 | Loss: 0.00172272
Iteration 65/1000 | Loss: 0.00008524
Iteration 66/1000 | Loss: 0.00008142
Iteration 67/1000 | Loss: 0.00007956
Iteration 68/1000 | Loss: 0.00007055
Iteration 69/1000 | Loss: 0.00005670
Iteration 70/1000 | Loss: 0.00005607
Iteration 71/1000 | Loss: 0.00007429
Iteration 72/1000 | Loss: 0.00006277
Iteration 73/1000 | Loss: 0.00005980
Iteration 74/1000 | Loss: 0.00005523
Iteration 75/1000 | Loss: 0.00072375
Iteration 76/1000 | Loss: 0.00006598
Iteration 77/1000 | Loss: 0.00071315
Iteration 78/1000 | Loss: 0.00005541
Iteration 79/1000 | Loss: 0.00006185
Iteration 80/1000 | Loss: 0.00004814
Iteration 81/1000 | Loss: 0.00068425
Iteration 82/1000 | Loss: 0.00004794
Iteration 83/1000 | Loss: 0.00131451
Iteration 84/1000 | Loss: 0.00194170
Iteration 85/1000 | Loss: 0.00068107
Iteration 86/1000 | Loss: 0.00055466
Iteration 87/1000 | Loss: 0.00006267
Iteration 88/1000 | Loss: 0.00187428
Iteration 89/1000 | Loss: 0.00010492
Iteration 90/1000 | Loss: 0.00075426
Iteration 91/1000 | Loss: 0.00006700
Iteration 92/1000 | Loss: 0.00006176
Iteration 93/1000 | Loss: 0.00004922
Iteration 94/1000 | Loss: 0.00005895
Iteration 95/1000 | Loss: 0.00004683
Iteration 96/1000 | Loss: 0.00005351
Iteration 97/1000 | Loss: 0.00005860
Iteration 98/1000 | Loss: 0.00005405
Iteration 99/1000 | Loss: 0.00005600
Iteration 100/1000 | Loss: 0.00005588
Iteration 101/1000 | Loss: 0.00005433
Iteration 102/1000 | Loss: 0.00006052
Iteration 103/1000 | Loss: 0.00100800
Iteration 104/1000 | Loss: 0.00026104
Iteration 105/1000 | Loss: 0.00005229
Iteration 106/1000 | Loss: 0.00007167
Iteration 107/1000 | Loss: 0.00004610
Iteration 108/1000 | Loss: 0.00005941
Iteration 109/1000 | Loss: 0.00005580
Iteration 110/1000 | Loss: 0.00005258
Iteration 111/1000 | Loss: 0.00005481
Iteration 112/1000 | Loss: 0.00006192
Iteration 113/1000 | Loss: 0.00075918
Iteration 114/1000 | Loss: 0.00013482
Iteration 115/1000 | Loss: 0.00007058
Iteration 116/1000 | Loss: 0.00007270
Iteration 117/1000 | Loss: 0.00005986
Iteration 118/1000 | Loss: 0.00005247
Iteration 119/1000 | Loss: 0.00031479
Iteration 120/1000 | Loss: 0.00006403
Iteration 121/1000 | Loss: 0.00005642
Iteration 122/1000 | Loss: 0.00021353
Iteration 123/1000 | Loss: 0.00015209
Iteration 124/1000 | Loss: 0.00004155
Iteration 125/1000 | Loss: 0.00003732
Iteration 126/1000 | Loss: 0.00003575
Iteration 127/1000 | Loss: 0.00003477
Iteration 128/1000 | Loss: 0.00003405
Iteration 129/1000 | Loss: 0.00003360
Iteration 130/1000 | Loss: 0.00054817
Iteration 131/1000 | Loss: 0.00003418
Iteration 132/1000 | Loss: 0.00003261
Iteration 133/1000 | Loss: 0.00003153
Iteration 134/1000 | Loss: 0.00003033
Iteration 135/1000 | Loss: 0.00002970
Iteration 136/1000 | Loss: 0.00002931
Iteration 137/1000 | Loss: 0.00002900
Iteration 138/1000 | Loss: 0.00002873
Iteration 139/1000 | Loss: 0.00002850
Iteration 140/1000 | Loss: 0.00076632
Iteration 141/1000 | Loss: 0.00047531
Iteration 142/1000 | Loss: 0.00032388
Iteration 143/1000 | Loss: 0.00044568
Iteration 144/1000 | Loss: 0.00036800
Iteration 145/1000 | Loss: 0.00039465
Iteration 146/1000 | Loss: 0.00036178
Iteration 147/1000 | Loss: 0.00003196
Iteration 148/1000 | Loss: 0.00002954
Iteration 149/1000 | Loss: 0.00002763
Iteration 150/1000 | Loss: 0.00002679
Iteration 151/1000 | Loss: 0.00002606
Iteration 152/1000 | Loss: 0.00002569
Iteration 153/1000 | Loss: 0.00002538
Iteration 154/1000 | Loss: 0.00002516
Iteration 155/1000 | Loss: 0.00002512
Iteration 156/1000 | Loss: 0.00002506
Iteration 157/1000 | Loss: 0.00002505
Iteration 158/1000 | Loss: 0.00002496
Iteration 159/1000 | Loss: 0.00002481
Iteration 160/1000 | Loss: 0.00002479
Iteration 161/1000 | Loss: 0.00002478
Iteration 162/1000 | Loss: 0.00002476
Iteration 163/1000 | Loss: 0.00002474
Iteration 164/1000 | Loss: 0.00002472
Iteration 165/1000 | Loss: 0.00002471
Iteration 166/1000 | Loss: 0.00002471
Iteration 167/1000 | Loss: 0.00002471
Iteration 168/1000 | Loss: 0.00002471
Iteration 169/1000 | Loss: 0.00002471
Iteration 170/1000 | Loss: 0.00002470
Iteration 171/1000 | Loss: 0.00002470
Iteration 172/1000 | Loss: 0.00002470
Iteration 173/1000 | Loss: 0.00002469
Iteration 174/1000 | Loss: 0.00002469
Iteration 175/1000 | Loss: 0.00002469
Iteration 176/1000 | Loss: 0.00002468
Iteration 177/1000 | Loss: 0.00002467
Iteration 178/1000 | Loss: 0.00002466
Iteration 179/1000 | Loss: 0.00002466
Iteration 180/1000 | Loss: 0.00002466
Iteration 181/1000 | Loss: 0.00002465
Iteration 182/1000 | Loss: 0.00002465
Iteration 183/1000 | Loss: 0.00002464
Iteration 184/1000 | Loss: 0.00002464
Iteration 185/1000 | Loss: 0.00002464
Iteration 186/1000 | Loss: 0.00002464
Iteration 187/1000 | Loss: 0.00002463
Iteration 188/1000 | Loss: 0.00002463
Iteration 189/1000 | Loss: 0.00002463
Iteration 190/1000 | Loss: 0.00002463
Iteration 191/1000 | Loss: 0.00002463
Iteration 192/1000 | Loss: 0.00002463
Iteration 193/1000 | Loss: 0.00002463
Iteration 194/1000 | Loss: 0.00002463
Iteration 195/1000 | Loss: 0.00002463
Iteration 196/1000 | Loss: 0.00002463
Iteration 197/1000 | Loss: 0.00002462
Iteration 198/1000 | Loss: 0.00002462
Iteration 199/1000 | Loss: 0.00002462
Iteration 200/1000 | Loss: 0.00002462
Iteration 201/1000 | Loss: 0.00002462
Iteration 202/1000 | Loss: 0.00002462
Iteration 203/1000 | Loss: 0.00002462
Iteration 204/1000 | Loss: 0.00002461
Iteration 205/1000 | Loss: 0.00002461
Iteration 206/1000 | Loss: 0.00002461
Iteration 207/1000 | Loss: 0.00002460
Iteration 208/1000 | Loss: 0.00002460
Iteration 209/1000 | Loss: 0.00002459
Iteration 210/1000 | Loss: 0.00002459
Iteration 211/1000 | Loss: 0.00002459
Iteration 212/1000 | Loss: 0.00002459
Iteration 213/1000 | Loss: 0.00002459
Iteration 214/1000 | Loss: 0.00002459
Iteration 215/1000 | Loss: 0.00002459
Iteration 216/1000 | Loss: 0.00002458
Iteration 217/1000 | Loss: 0.00002458
Iteration 218/1000 | Loss: 0.00002458
Iteration 219/1000 | Loss: 0.00002458
Iteration 220/1000 | Loss: 0.00002458
Iteration 221/1000 | Loss: 0.00002458
Iteration 222/1000 | Loss: 0.00002458
Iteration 223/1000 | Loss: 0.00002458
Iteration 224/1000 | Loss: 0.00002458
Iteration 225/1000 | Loss: 0.00002458
Iteration 226/1000 | Loss: 0.00002458
Iteration 227/1000 | Loss: 0.00002458
Iteration 228/1000 | Loss: 0.00002457
Iteration 229/1000 | Loss: 0.00002457
Iteration 230/1000 | Loss: 0.00002457
Iteration 231/1000 | Loss: 0.00002457
Iteration 232/1000 | Loss: 0.00002457
Iteration 233/1000 | Loss: 0.00002457
Iteration 234/1000 | Loss: 0.00002457
Iteration 235/1000 | Loss: 0.00002457
Iteration 236/1000 | Loss: 0.00002457
Iteration 237/1000 | Loss: 0.00002457
Iteration 238/1000 | Loss: 0.00002457
Iteration 239/1000 | Loss: 0.00002457
Iteration 240/1000 | Loss: 0.00002456
Iteration 241/1000 | Loss: 0.00002456
Iteration 242/1000 | Loss: 0.00002456
Iteration 243/1000 | Loss: 0.00002456
Iteration 244/1000 | Loss: 0.00002456
Iteration 245/1000 | Loss: 0.00002456
Iteration 246/1000 | Loss: 0.00002456
Iteration 247/1000 | Loss: 0.00002456
Iteration 248/1000 | Loss: 0.00002456
Iteration 249/1000 | Loss: 0.00002456
Iteration 250/1000 | Loss: 0.00002456
Iteration 251/1000 | Loss: 0.00002456
Iteration 252/1000 | Loss: 0.00002456
Iteration 253/1000 | Loss: 0.00002456
Iteration 254/1000 | Loss: 0.00002455
Iteration 255/1000 | Loss: 0.00002455
Iteration 256/1000 | Loss: 0.00002455
Iteration 257/1000 | Loss: 0.00002455
Iteration 258/1000 | Loss: 0.00002455
Iteration 259/1000 | Loss: 0.00002455
Iteration 260/1000 | Loss: 0.00002454
Iteration 261/1000 | Loss: 0.00002454
Iteration 262/1000 | Loss: 0.00002454
Iteration 263/1000 | Loss: 0.00002454
Iteration 264/1000 | Loss: 0.00002454
Iteration 265/1000 | Loss: 0.00002454
Iteration 266/1000 | Loss: 0.00002454
Iteration 267/1000 | Loss: 0.00002454
Iteration 268/1000 | Loss: 0.00002454
Iteration 269/1000 | Loss: 0.00002454
Iteration 270/1000 | Loss: 0.00002454
Iteration 271/1000 | Loss: 0.00002454
Iteration 272/1000 | Loss: 0.00002454
Iteration 273/1000 | Loss: 0.00002453
Iteration 274/1000 | Loss: 0.00002453
Iteration 275/1000 | Loss: 0.00002453
Iteration 276/1000 | Loss: 0.00002453
Iteration 277/1000 | Loss: 0.00002453
Iteration 278/1000 | Loss: 0.00002453
Iteration 279/1000 | Loss: 0.00002453
Iteration 280/1000 | Loss: 0.00002453
Iteration 281/1000 | Loss: 0.00002453
Iteration 282/1000 | Loss: 0.00002453
Iteration 283/1000 | Loss: 0.00002453
Iteration 284/1000 | Loss: 0.00002453
Iteration 285/1000 | Loss: 0.00002453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [2.4531864255550317e-05, 2.4531864255550317e-05, 2.4531864255550317e-05, 2.4531864255550317e-05, 2.4531864255550317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4531864255550317e-05

Optimization complete. Final v2v error: 3.5911710262298584 mm

Highest mean error: 13.256524085998535 mm for frame 30

Lowest mean error: 2.5621085166931152 mm for frame 96

Saving results

Total time: 277.2121946811676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869392
Iteration 2/25 | Loss: 0.00184024
Iteration 3/25 | Loss: 0.00133329
Iteration 4/25 | Loss: 0.00126328
Iteration 5/25 | Loss: 0.00124421
Iteration 6/25 | Loss: 0.00124296
Iteration 7/25 | Loss: 0.00123149
Iteration 8/25 | Loss: 0.00122216
Iteration 9/25 | Loss: 0.00121401
Iteration 10/25 | Loss: 0.00120720
Iteration 11/25 | Loss: 0.00120468
Iteration 12/25 | Loss: 0.00120396
Iteration 13/25 | Loss: 0.00120380
Iteration 14/25 | Loss: 0.00120375
Iteration 15/25 | Loss: 0.00120375
Iteration 16/25 | Loss: 0.00120375
Iteration 17/25 | Loss: 0.00120375
Iteration 18/25 | Loss: 0.00120375
Iteration 19/25 | Loss: 0.00120375
Iteration 20/25 | Loss: 0.00120375
Iteration 21/25 | Loss: 0.00120374
Iteration 22/25 | Loss: 0.00120374
Iteration 23/25 | Loss: 0.00120374
Iteration 24/25 | Loss: 0.00120374
Iteration 25/25 | Loss: 0.00120374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10944557
Iteration 2/25 | Loss: 0.00196024
Iteration 3/25 | Loss: 0.00196024
Iteration 4/25 | Loss: 0.00196024
Iteration 5/25 | Loss: 0.00196024
Iteration 6/25 | Loss: 0.00196024
Iteration 7/25 | Loss: 0.00196024
Iteration 8/25 | Loss: 0.00196023
Iteration 9/25 | Loss: 0.00196023
Iteration 10/25 | Loss: 0.00196023
Iteration 11/25 | Loss: 0.00196023
Iteration 12/25 | Loss: 0.00196023
Iteration 13/25 | Loss: 0.00196023
Iteration 14/25 | Loss: 0.00196023
Iteration 15/25 | Loss: 0.00196023
Iteration 16/25 | Loss: 0.00196023
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019602342508733273, 0.0019602342508733273, 0.0019602342508733273, 0.0019602342508733273, 0.0019602342508733273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019602342508733273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196023
Iteration 2/1000 | Loss: 0.00003329
Iteration 3/1000 | Loss: 0.00002643
Iteration 4/1000 | Loss: 0.00002438
Iteration 5/1000 | Loss: 0.00002348
Iteration 6/1000 | Loss: 0.00002272
Iteration 7/1000 | Loss: 0.00002229
Iteration 8/1000 | Loss: 0.00002188
Iteration 9/1000 | Loss: 0.00002158
Iteration 10/1000 | Loss: 0.00002149
Iteration 11/1000 | Loss: 0.00002143
Iteration 12/1000 | Loss: 0.00002128
Iteration 13/1000 | Loss: 0.00002119
Iteration 14/1000 | Loss: 0.00002113
Iteration 15/1000 | Loss: 0.00002112
Iteration 16/1000 | Loss: 0.00002111
Iteration 17/1000 | Loss: 0.00002110
Iteration 18/1000 | Loss: 0.00002098
Iteration 19/1000 | Loss: 0.00002096
Iteration 20/1000 | Loss: 0.00002095
Iteration 21/1000 | Loss: 0.00002093
Iteration 22/1000 | Loss: 0.00002093
Iteration 23/1000 | Loss: 0.00002092
Iteration 24/1000 | Loss: 0.00002091
Iteration 25/1000 | Loss: 0.00002089
Iteration 26/1000 | Loss: 0.00002089
Iteration 27/1000 | Loss: 0.00002088
Iteration 28/1000 | Loss: 0.00002087
Iteration 29/1000 | Loss: 0.00002086
Iteration 30/1000 | Loss: 0.00002084
Iteration 31/1000 | Loss: 0.00002084
Iteration 32/1000 | Loss: 0.00002084
Iteration 33/1000 | Loss: 0.00002083
Iteration 34/1000 | Loss: 0.00002083
Iteration 35/1000 | Loss: 0.00002083
Iteration 36/1000 | Loss: 0.00002082
Iteration 37/1000 | Loss: 0.00002081
Iteration 38/1000 | Loss: 0.00002080
Iteration 39/1000 | Loss: 0.00002079
Iteration 40/1000 | Loss: 0.00002078
Iteration 41/1000 | Loss: 0.00002078
Iteration 42/1000 | Loss: 0.00002078
Iteration 43/1000 | Loss: 0.00002078
Iteration 44/1000 | Loss: 0.00002078
Iteration 45/1000 | Loss: 0.00002078
Iteration 46/1000 | Loss: 0.00002078
Iteration 47/1000 | Loss: 0.00002077
Iteration 48/1000 | Loss: 0.00002077
Iteration 49/1000 | Loss: 0.00002076
Iteration 50/1000 | Loss: 0.00002076
Iteration 51/1000 | Loss: 0.00002075
Iteration 52/1000 | Loss: 0.00002075
Iteration 53/1000 | Loss: 0.00002074
Iteration 54/1000 | Loss: 0.00002074
Iteration 55/1000 | Loss: 0.00002074
Iteration 56/1000 | Loss: 0.00002073
Iteration 57/1000 | Loss: 0.00002073
Iteration 58/1000 | Loss: 0.00002073
Iteration 59/1000 | Loss: 0.00002073
Iteration 60/1000 | Loss: 0.00002072
Iteration 61/1000 | Loss: 0.00002072
Iteration 62/1000 | Loss: 0.00002072
Iteration 63/1000 | Loss: 0.00002072
Iteration 64/1000 | Loss: 0.00002072
Iteration 65/1000 | Loss: 0.00002071
Iteration 66/1000 | Loss: 0.00002070
Iteration 67/1000 | Loss: 0.00002070
Iteration 68/1000 | Loss: 0.00002070
Iteration 69/1000 | Loss: 0.00002070
Iteration 70/1000 | Loss: 0.00002070
Iteration 71/1000 | Loss: 0.00002070
Iteration 72/1000 | Loss: 0.00002070
Iteration 73/1000 | Loss: 0.00002070
Iteration 74/1000 | Loss: 0.00002069
Iteration 75/1000 | Loss: 0.00002069
Iteration 76/1000 | Loss: 0.00002069
Iteration 77/1000 | Loss: 0.00002069
Iteration 78/1000 | Loss: 0.00002069
Iteration 79/1000 | Loss: 0.00002068
Iteration 80/1000 | Loss: 0.00002068
Iteration 81/1000 | Loss: 0.00002068
Iteration 82/1000 | Loss: 0.00002068
Iteration 83/1000 | Loss: 0.00002068
Iteration 84/1000 | Loss: 0.00002068
Iteration 85/1000 | Loss: 0.00002068
Iteration 86/1000 | Loss: 0.00002068
Iteration 87/1000 | Loss: 0.00002068
Iteration 88/1000 | Loss: 0.00002068
Iteration 89/1000 | Loss: 0.00002068
Iteration 90/1000 | Loss: 0.00002067
Iteration 91/1000 | Loss: 0.00002067
Iteration 92/1000 | Loss: 0.00002067
Iteration 93/1000 | Loss: 0.00002067
Iteration 94/1000 | Loss: 0.00002066
Iteration 95/1000 | Loss: 0.00002066
Iteration 96/1000 | Loss: 0.00002065
Iteration 97/1000 | Loss: 0.00002065
Iteration 98/1000 | Loss: 0.00002064
Iteration 99/1000 | Loss: 0.00002064
Iteration 100/1000 | Loss: 0.00002064
Iteration 101/1000 | Loss: 0.00002063
Iteration 102/1000 | Loss: 0.00002063
Iteration 103/1000 | Loss: 0.00002062
Iteration 104/1000 | Loss: 0.00002062
Iteration 105/1000 | Loss: 0.00002062
Iteration 106/1000 | Loss: 0.00002062
Iteration 107/1000 | Loss: 0.00002062
Iteration 108/1000 | Loss: 0.00002062
Iteration 109/1000 | Loss: 0.00002062
Iteration 110/1000 | Loss: 0.00002062
Iteration 111/1000 | Loss: 0.00002062
Iteration 112/1000 | Loss: 0.00002061
Iteration 113/1000 | Loss: 0.00002060
Iteration 114/1000 | Loss: 0.00002060
Iteration 115/1000 | Loss: 0.00002059
Iteration 116/1000 | Loss: 0.00002059
Iteration 117/1000 | Loss: 0.00002059
Iteration 118/1000 | Loss: 0.00002059
Iteration 119/1000 | Loss: 0.00002058
Iteration 120/1000 | Loss: 0.00002057
Iteration 121/1000 | Loss: 0.00002057
Iteration 122/1000 | Loss: 0.00002057
Iteration 123/1000 | Loss: 0.00002056
Iteration 124/1000 | Loss: 0.00002056
Iteration 125/1000 | Loss: 0.00002056
Iteration 126/1000 | Loss: 0.00002056
Iteration 127/1000 | Loss: 0.00002056
Iteration 128/1000 | Loss: 0.00002056
Iteration 129/1000 | Loss: 0.00002056
Iteration 130/1000 | Loss: 0.00002056
Iteration 131/1000 | Loss: 0.00002056
Iteration 132/1000 | Loss: 0.00002056
Iteration 133/1000 | Loss: 0.00002056
Iteration 134/1000 | Loss: 0.00002056
Iteration 135/1000 | Loss: 0.00002056
Iteration 136/1000 | Loss: 0.00002056
Iteration 137/1000 | Loss: 0.00002056
Iteration 138/1000 | Loss: 0.00002056
Iteration 139/1000 | Loss: 0.00002056
Iteration 140/1000 | Loss: 0.00002056
Iteration 141/1000 | Loss: 0.00002056
Iteration 142/1000 | Loss: 0.00002056
Iteration 143/1000 | Loss: 0.00002056
Iteration 144/1000 | Loss: 0.00002056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.0559975382639095e-05, 2.0559975382639095e-05, 2.0559975382639095e-05, 2.0559975382639095e-05, 2.0559975382639095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0559975382639095e-05

Optimization complete. Final v2v error: 3.439300298690796 mm

Highest mean error: 20.790861129760742 mm for frame 206

Lowest mean error: 2.8487179279327393 mm for frame 4

Saving results

Total time: 57.79065418243408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055764
Iteration 2/25 | Loss: 0.01055764
Iteration 3/25 | Loss: 0.01055764
Iteration 4/25 | Loss: 0.01055764
Iteration 5/25 | Loss: 0.01055764
Iteration 6/25 | Loss: 0.01055764
Iteration 7/25 | Loss: 0.01055764
Iteration 8/25 | Loss: 0.01055764
Iteration 9/25 | Loss: 0.01055764
Iteration 10/25 | Loss: 0.01055764
Iteration 11/25 | Loss: 0.01055764
Iteration 12/25 | Loss: 0.01055764
Iteration 13/25 | Loss: 0.01055764
Iteration 14/25 | Loss: 0.01055764
Iteration 15/25 | Loss: 0.01055764
Iteration 16/25 | Loss: 0.01055764
Iteration 17/25 | Loss: 0.01055764
Iteration 18/25 | Loss: 0.01055764
Iteration 19/25 | Loss: 0.01055763
Iteration 20/25 | Loss: 0.01055763
Iteration 21/25 | Loss: 0.01055763
Iteration 22/25 | Loss: 0.01055763
Iteration 23/25 | Loss: 0.01055763
Iteration 24/25 | Loss: 0.01055763
Iteration 25/25 | Loss: 0.01055763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70723116
Iteration 2/25 | Loss: 0.09020722
Iteration 3/25 | Loss: 0.09013620
Iteration 4/25 | Loss: 0.09013620
Iteration 5/25 | Loss: 0.09013177
Iteration 6/25 | Loss: 0.09007809
Iteration 7/25 | Loss: 0.09007809
Iteration 8/25 | Loss: 0.09007808
Iteration 9/25 | Loss: 0.09007806
Iteration 10/25 | Loss: 0.09007806
Iteration 11/25 | Loss: 0.09007806
Iteration 12/25 | Loss: 0.09007806
Iteration 13/25 | Loss: 0.09007806
Iteration 14/25 | Loss: 0.09007806
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.09007806330919266, 0.09007806330919266, 0.09007806330919266, 0.09007806330919266, 0.09007806330919266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09007806330919266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09007806
Iteration 2/1000 | Loss: 0.00430741
Iteration 3/1000 | Loss: 0.00913751
Iteration 4/1000 | Loss: 0.00175253
Iteration 5/1000 | Loss: 0.00432681
Iteration 6/1000 | Loss: 0.00497088
Iteration 7/1000 | Loss: 0.00187932
Iteration 8/1000 | Loss: 0.00013583
Iteration 9/1000 | Loss: 0.00045211
Iteration 10/1000 | Loss: 0.00048285
Iteration 11/1000 | Loss: 0.00044768
Iteration 12/1000 | Loss: 0.00259293
Iteration 13/1000 | Loss: 0.00042728
Iteration 14/1000 | Loss: 0.00037536
Iteration 15/1000 | Loss: 0.00006667
Iteration 16/1000 | Loss: 0.00014489
Iteration 17/1000 | Loss: 0.00003789
Iteration 18/1000 | Loss: 0.00060949
Iteration 19/1000 | Loss: 0.00003332
Iteration 20/1000 | Loss: 0.00044050
Iteration 21/1000 | Loss: 0.00059955
Iteration 22/1000 | Loss: 0.00011530
Iteration 23/1000 | Loss: 0.00007461
Iteration 24/1000 | Loss: 0.00003247
Iteration 25/1000 | Loss: 0.00031274
Iteration 26/1000 | Loss: 0.00002581
Iteration 27/1000 | Loss: 0.00002423
Iteration 28/1000 | Loss: 0.00004002
Iteration 29/1000 | Loss: 0.00002211
Iteration 30/1000 | Loss: 0.00002105
Iteration 31/1000 | Loss: 0.00004314
Iteration 32/1000 | Loss: 0.00002028
Iteration 33/1000 | Loss: 0.00001931
Iteration 34/1000 | Loss: 0.00001881
Iteration 35/1000 | Loss: 0.00001830
Iteration 36/1000 | Loss: 0.00001789
Iteration 37/1000 | Loss: 0.00041509
Iteration 38/1000 | Loss: 0.00014122
Iteration 39/1000 | Loss: 0.00006792
Iteration 40/1000 | Loss: 0.00012550
Iteration 41/1000 | Loss: 0.00001809
Iteration 42/1000 | Loss: 0.00010075
Iteration 43/1000 | Loss: 0.00001728
Iteration 44/1000 | Loss: 0.00001693
Iteration 45/1000 | Loss: 0.00001684
Iteration 46/1000 | Loss: 0.00001663
Iteration 47/1000 | Loss: 0.00001661
Iteration 48/1000 | Loss: 0.00003240
Iteration 49/1000 | Loss: 0.00001675
Iteration 50/1000 | Loss: 0.00001640
Iteration 51/1000 | Loss: 0.00001670
Iteration 52/1000 | Loss: 0.00001670
Iteration 53/1000 | Loss: 0.00001668
Iteration 54/1000 | Loss: 0.00003179
Iteration 55/1000 | Loss: 0.00005241
Iteration 56/1000 | Loss: 0.00023415
Iteration 57/1000 | Loss: 0.00008174
Iteration 58/1000 | Loss: 0.00011668
Iteration 59/1000 | Loss: 0.00004291
Iteration 60/1000 | Loss: 0.00006739
Iteration 61/1000 | Loss: 0.00001637
Iteration 62/1000 | Loss: 0.00003944
Iteration 63/1000 | Loss: 0.00001669
Iteration 64/1000 | Loss: 0.00001636
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001633
Iteration 67/1000 | Loss: 0.00001633
Iteration 68/1000 | Loss: 0.00001633
Iteration 69/1000 | Loss: 0.00001620
Iteration 70/1000 | Loss: 0.00001618
Iteration 71/1000 | Loss: 0.00001618
Iteration 72/1000 | Loss: 0.00001617
Iteration 73/1000 | Loss: 0.00001617
Iteration 74/1000 | Loss: 0.00001605
Iteration 75/1000 | Loss: 0.00001605
Iteration 76/1000 | Loss: 0.00001605
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001622
Iteration 79/1000 | Loss: 0.00004699
Iteration 80/1000 | Loss: 0.00002292
Iteration 81/1000 | Loss: 0.00001616
Iteration 82/1000 | Loss: 0.00002999
Iteration 83/1000 | Loss: 0.00001601
Iteration 84/1000 | Loss: 0.00001599
Iteration 85/1000 | Loss: 0.00001599
Iteration 86/1000 | Loss: 0.00001598
Iteration 87/1000 | Loss: 0.00001598
Iteration 88/1000 | Loss: 0.00001598
Iteration 89/1000 | Loss: 0.00001598
Iteration 90/1000 | Loss: 0.00001598
Iteration 91/1000 | Loss: 0.00001598
Iteration 92/1000 | Loss: 0.00001597
Iteration 93/1000 | Loss: 0.00001597
Iteration 94/1000 | Loss: 0.00001597
Iteration 95/1000 | Loss: 0.00001597
Iteration 96/1000 | Loss: 0.00001597
Iteration 97/1000 | Loss: 0.00001597
Iteration 98/1000 | Loss: 0.00001597
Iteration 99/1000 | Loss: 0.00001597
Iteration 100/1000 | Loss: 0.00001596
Iteration 101/1000 | Loss: 0.00001596
Iteration 102/1000 | Loss: 0.00001596
Iteration 103/1000 | Loss: 0.00001596
Iteration 104/1000 | Loss: 0.00001596
Iteration 105/1000 | Loss: 0.00001596
Iteration 106/1000 | Loss: 0.00001596
Iteration 107/1000 | Loss: 0.00001596
Iteration 108/1000 | Loss: 0.00001596
Iteration 109/1000 | Loss: 0.00001596
Iteration 110/1000 | Loss: 0.00001596
Iteration 111/1000 | Loss: 0.00004296
Iteration 112/1000 | Loss: 0.00001613
Iteration 113/1000 | Loss: 0.00001602
Iteration 114/1000 | Loss: 0.00001609
Iteration 115/1000 | Loss: 0.00001601
Iteration 116/1000 | Loss: 0.00001594
Iteration 117/1000 | Loss: 0.00001594
Iteration 118/1000 | Loss: 0.00001594
Iteration 119/1000 | Loss: 0.00001594
Iteration 120/1000 | Loss: 0.00001606
Iteration 121/1000 | Loss: 0.00004127
Iteration 122/1000 | Loss: 0.00006200
Iteration 123/1000 | Loss: 0.00003111
Iteration 124/1000 | Loss: 0.00001817
Iteration 125/1000 | Loss: 0.00001593
Iteration 126/1000 | Loss: 0.00001590
Iteration 127/1000 | Loss: 0.00001590
Iteration 128/1000 | Loss: 0.00001590
Iteration 129/1000 | Loss: 0.00001590
Iteration 130/1000 | Loss: 0.00001589
Iteration 131/1000 | Loss: 0.00001589
Iteration 132/1000 | Loss: 0.00001588
Iteration 133/1000 | Loss: 0.00001588
Iteration 134/1000 | Loss: 0.00001588
Iteration 135/1000 | Loss: 0.00001587
Iteration 136/1000 | Loss: 0.00001587
Iteration 137/1000 | Loss: 0.00001587
Iteration 138/1000 | Loss: 0.00001587
Iteration 139/1000 | Loss: 0.00001587
Iteration 140/1000 | Loss: 0.00001586
Iteration 141/1000 | Loss: 0.00001586
Iteration 142/1000 | Loss: 0.00001586
Iteration 143/1000 | Loss: 0.00001586
Iteration 144/1000 | Loss: 0.00001586
Iteration 145/1000 | Loss: 0.00001586
Iteration 146/1000 | Loss: 0.00001586
Iteration 147/1000 | Loss: 0.00001586
Iteration 148/1000 | Loss: 0.00001586
Iteration 149/1000 | Loss: 0.00001586
Iteration 150/1000 | Loss: 0.00001586
Iteration 151/1000 | Loss: 0.00001586
Iteration 152/1000 | Loss: 0.00001586
Iteration 153/1000 | Loss: 0.00001586
Iteration 154/1000 | Loss: 0.00001586
Iteration 155/1000 | Loss: 0.00001586
Iteration 156/1000 | Loss: 0.00001586
Iteration 157/1000 | Loss: 0.00001586
Iteration 158/1000 | Loss: 0.00001586
Iteration 159/1000 | Loss: 0.00001586
Iteration 160/1000 | Loss: 0.00001586
Iteration 161/1000 | Loss: 0.00001586
Iteration 162/1000 | Loss: 0.00001586
Iteration 163/1000 | Loss: 0.00001586
Iteration 164/1000 | Loss: 0.00001586
Iteration 165/1000 | Loss: 0.00001586
Iteration 166/1000 | Loss: 0.00001586
Iteration 167/1000 | Loss: 0.00001586
Iteration 168/1000 | Loss: 0.00001586
Iteration 169/1000 | Loss: 0.00001586
Iteration 170/1000 | Loss: 0.00001586
Iteration 171/1000 | Loss: 0.00001586
Iteration 172/1000 | Loss: 0.00001586
Iteration 173/1000 | Loss: 0.00001586
Iteration 174/1000 | Loss: 0.00001586
Iteration 175/1000 | Loss: 0.00001586
Iteration 176/1000 | Loss: 0.00001586
Iteration 177/1000 | Loss: 0.00001586
Iteration 178/1000 | Loss: 0.00001586
Iteration 179/1000 | Loss: 0.00001586
Iteration 180/1000 | Loss: 0.00001586
Iteration 181/1000 | Loss: 0.00001586
Iteration 182/1000 | Loss: 0.00001586
Iteration 183/1000 | Loss: 0.00001586
Iteration 184/1000 | Loss: 0.00001586
Iteration 185/1000 | Loss: 0.00001586
Iteration 186/1000 | Loss: 0.00001586
Iteration 187/1000 | Loss: 0.00001586
Iteration 188/1000 | Loss: 0.00001586
Iteration 189/1000 | Loss: 0.00001586
Iteration 190/1000 | Loss: 0.00001586
Iteration 191/1000 | Loss: 0.00001586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.585645259183366e-05, 1.585645259183366e-05, 1.585645259183366e-05, 1.585645259183366e-05, 1.585645259183366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.585645259183366e-05

Optimization complete. Final v2v error: 3.347964286804199 mm

Highest mean error: 10.878877639770508 mm for frame 6

Lowest mean error: 2.900233745574951 mm for frame 129

Saving results

Total time: 129.72134447097778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822247
Iteration 2/25 | Loss: 0.00163987
Iteration 3/25 | Loss: 0.00129928
Iteration 4/25 | Loss: 0.00127611
Iteration 5/25 | Loss: 0.00127387
Iteration 6/25 | Loss: 0.00126517
Iteration 7/25 | Loss: 0.00126473
Iteration 8/25 | Loss: 0.00126456
Iteration 9/25 | Loss: 0.00126456
Iteration 10/25 | Loss: 0.00126456
Iteration 11/25 | Loss: 0.00126456
Iteration 12/25 | Loss: 0.00126456
Iteration 13/25 | Loss: 0.00126456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012645624810829759, 0.0012645624810829759, 0.0012645624810829759, 0.0012645624810829759, 0.0012645624810829759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012645624810829759

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15474415
Iteration 2/25 | Loss: 0.00157502
Iteration 3/25 | Loss: 0.00157502
Iteration 4/25 | Loss: 0.00157502
Iteration 5/25 | Loss: 0.00157502
Iteration 6/25 | Loss: 0.00157501
Iteration 7/25 | Loss: 0.00157501
Iteration 8/25 | Loss: 0.00157501
Iteration 9/25 | Loss: 0.00157501
Iteration 10/25 | Loss: 0.00157501
Iteration 11/25 | Loss: 0.00157501
Iteration 12/25 | Loss: 0.00157501
Iteration 13/25 | Loss: 0.00157501
Iteration 14/25 | Loss: 0.00157501
Iteration 15/25 | Loss: 0.00157501
Iteration 16/25 | Loss: 0.00157501
Iteration 17/25 | Loss: 0.00157501
Iteration 18/25 | Loss: 0.00157501
Iteration 19/25 | Loss: 0.00157501
Iteration 20/25 | Loss: 0.00157501
Iteration 21/25 | Loss: 0.00157501
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001575013156980276, 0.001575013156980276, 0.001575013156980276, 0.001575013156980276, 0.001575013156980276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001575013156980276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157501
Iteration 2/1000 | Loss: 0.00003180
Iteration 3/1000 | Loss: 0.00002050
Iteration 4/1000 | Loss: 0.00001841
Iteration 5/1000 | Loss: 0.00001736
Iteration 6/1000 | Loss: 0.00001684
Iteration 7/1000 | Loss: 0.00001638
Iteration 8/1000 | Loss: 0.00001596
Iteration 9/1000 | Loss: 0.00001571
Iteration 10/1000 | Loss: 0.00001545
Iteration 11/1000 | Loss: 0.00001534
Iteration 12/1000 | Loss: 0.00001531
Iteration 13/1000 | Loss: 0.00001526
Iteration 14/1000 | Loss: 0.00001525
Iteration 15/1000 | Loss: 0.00001514
Iteration 16/1000 | Loss: 0.00001511
Iteration 17/1000 | Loss: 0.00001510
Iteration 18/1000 | Loss: 0.00001505
Iteration 19/1000 | Loss: 0.00001504
Iteration 20/1000 | Loss: 0.00001502
Iteration 21/1000 | Loss: 0.00001502
Iteration 22/1000 | Loss: 0.00001500
Iteration 23/1000 | Loss: 0.00001500
Iteration 24/1000 | Loss: 0.00001500
Iteration 25/1000 | Loss: 0.00001500
Iteration 26/1000 | Loss: 0.00001500
Iteration 27/1000 | Loss: 0.00001500
Iteration 28/1000 | Loss: 0.00001499
Iteration 29/1000 | Loss: 0.00001499
Iteration 30/1000 | Loss: 0.00001499
Iteration 31/1000 | Loss: 0.00001499
Iteration 32/1000 | Loss: 0.00001499
Iteration 33/1000 | Loss: 0.00001499
Iteration 34/1000 | Loss: 0.00001499
Iteration 35/1000 | Loss: 0.00001499
Iteration 36/1000 | Loss: 0.00001499
Iteration 37/1000 | Loss: 0.00001499
Iteration 38/1000 | Loss: 0.00001498
Iteration 39/1000 | Loss: 0.00001498
Iteration 40/1000 | Loss: 0.00001497
Iteration 41/1000 | Loss: 0.00001497
Iteration 42/1000 | Loss: 0.00001497
Iteration 43/1000 | Loss: 0.00001497
Iteration 44/1000 | Loss: 0.00001496
Iteration 45/1000 | Loss: 0.00001496
Iteration 46/1000 | Loss: 0.00001495
Iteration 47/1000 | Loss: 0.00001495
Iteration 48/1000 | Loss: 0.00001494
Iteration 49/1000 | Loss: 0.00001493
Iteration 50/1000 | Loss: 0.00001493
Iteration 51/1000 | Loss: 0.00001493
Iteration 52/1000 | Loss: 0.00001492
Iteration 53/1000 | Loss: 0.00001491
Iteration 54/1000 | Loss: 0.00001491
Iteration 55/1000 | Loss: 0.00001490
Iteration 56/1000 | Loss: 0.00001490
Iteration 57/1000 | Loss: 0.00001489
Iteration 58/1000 | Loss: 0.00001488
Iteration 59/1000 | Loss: 0.00001488
Iteration 60/1000 | Loss: 0.00001488
Iteration 61/1000 | Loss: 0.00001488
Iteration 62/1000 | Loss: 0.00001488
Iteration 63/1000 | Loss: 0.00001488
Iteration 64/1000 | Loss: 0.00001488
Iteration 65/1000 | Loss: 0.00001488
Iteration 66/1000 | Loss: 0.00001488
Iteration 67/1000 | Loss: 0.00001487
Iteration 68/1000 | Loss: 0.00001487
Iteration 69/1000 | Loss: 0.00001487
Iteration 70/1000 | Loss: 0.00001487
Iteration 71/1000 | Loss: 0.00001487
Iteration 72/1000 | Loss: 0.00001486
Iteration 73/1000 | Loss: 0.00001486
Iteration 74/1000 | Loss: 0.00001486
Iteration 75/1000 | Loss: 0.00001486
Iteration 76/1000 | Loss: 0.00001486
Iteration 77/1000 | Loss: 0.00001485
Iteration 78/1000 | Loss: 0.00001485
Iteration 79/1000 | Loss: 0.00001485
Iteration 80/1000 | Loss: 0.00001485
Iteration 81/1000 | Loss: 0.00001485
Iteration 82/1000 | Loss: 0.00001485
Iteration 83/1000 | Loss: 0.00001485
Iteration 84/1000 | Loss: 0.00001485
Iteration 85/1000 | Loss: 0.00001485
Iteration 86/1000 | Loss: 0.00001485
Iteration 87/1000 | Loss: 0.00001485
Iteration 88/1000 | Loss: 0.00001485
Iteration 89/1000 | Loss: 0.00001484
Iteration 90/1000 | Loss: 0.00001484
Iteration 91/1000 | Loss: 0.00001484
Iteration 92/1000 | Loss: 0.00001484
Iteration 93/1000 | Loss: 0.00001484
Iteration 94/1000 | Loss: 0.00001484
Iteration 95/1000 | Loss: 0.00001483
Iteration 96/1000 | Loss: 0.00001483
Iteration 97/1000 | Loss: 0.00001483
Iteration 98/1000 | Loss: 0.00001483
Iteration 99/1000 | Loss: 0.00001483
Iteration 100/1000 | Loss: 0.00001483
Iteration 101/1000 | Loss: 0.00001483
Iteration 102/1000 | Loss: 0.00001483
Iteration 103/1000 | Loss: 0.00001483
Iteration 104/1000 | Loss: 0.00001482
Iteration 105/1000 | Loss: 0.00001482
Iteration 106/1000 | Loss: 0.00001482
Iteration 107/1000 | Loss: 0.00001482
Iteration 108/1000 | Loss: 0.00001482
Iteration 109/1000 | Loss: 0.00001482
Iteration 110/1000 | Loss: 0.00001482
Iteration 111/1000 | Loss: 0.00001481
Iteration 112/1000 | Loss: 0.00001481
Iteration 113/1000 | Loss: 0.00001481
Iteration 114/1000 | Loss: 0.00001481
Iteration 115/1000 | Loss: 0.00001481
Iteration 116/1000 | Loss: 0.00001481
Iteration 117/1000 | Loss: 0.00001481
Iteration 118/1000 | Loss: 0.00001481
Iteration 119/1000 | Loss: 0.00001481
Iteration 120/1000 | Loss: 0.00001481
Iteration 121/1000 | Loss: 0.00001481
Iteration 122/1000 | Loss: 0.00001481
Iteration 123/1000 | Loss: 0.00001481
Iteration 124/1000 | Loss: 0.00001480
Iteration 125/1000 | Loss: 0.00001480
Iteration 126/1000 | Loss: 0.00001480
Iteration 127/1000 | Loss: 0.00001480
Iteration 128/1000 | Loss: 0.00001480
Iteration 129/1000 | Loss: 0.00001479
Iteration 130/1000 | Loss: 0.00001479
Iteration 131/1000 | Loss: 0.00001479
Iteration 132/1000 | Loss: 0.00001479
Iteration 133/1000 | Loss: 0.00001479
Iteration 134/1000 | Loss: 0.00001478
Iteration 135/1000 | Loss: 0.00001478
Iteration 136/1000 | Loss: 0.00001478
Iteration 137/1000 | Loss: 0.00001477
Iteration 138/1000 | Loss: 0.00001477
Iteration 139/1000 | Loss: 0.00001477
Iteration 140/1000 | Loss: 0.00001477
Iteration 141/1000 | Loss: 0.00001477
Iteration 142/1000 | Loss: 0.00001477
Iteration 143/1000 | Loss: 0.00001477
Iteration 144/1000 | Loss: 0.00001476
Iteration 145/1000 | Loss: 0.00001476
Iteration 146/1000 | Loss: 0.00001476
Iteration 147/1000 | Loss: 0.00001476
Iteration 148/1000 | Loss: 0.00001476
Iteration 149/1000 | Loss: 0.00001476
Iteration 150/1000 | Loss: 0.00001476
Iteration 151/1000 | Loss: 0.00001476
Iteration 152/1000 | Loss: 0.00001476
Iteration 153/1000 | Loss: 0.00001476
Iteration 154/1000 | Loss: 0.00001476
Iteration 155/1000 | Loss: 0.00001476
Iteration 156/1000 | Loss: 0.00001475
Iteration 157/1000 | Loss: 0.00001475
Iteration 158/1000 | Loss: 0.00001475
Iteration 159/1000 | Loss: 0.00001475
Iteration 160/1000 | Loss: 0.00001475
Iteration 161/1000 | Loss: 0.00001475
Iteration 162/1000 | Loss: 0.00001474
Iteration 163/1000 | Loss: 0.00001474
Iteration 164/1000 | Loss: 0.00001474
Iteration 165/1000 | Loss: 0.00001474
Iteration 166/1000 | Loss: 0.00001474
Iteration 167/1000 | Loss: 0.00001474
Iteration 168/1000 | Loss: 0.00001474
Iteration 169/1000 | Loss: 0.00001473
Iteration 170/1000 | Loss: 0.00001473
Iteration 171/1000 | Loss: 0.00001473
Iteration 172/1000 | Loss: 0.00001473
Iteration 173/1000 | Loss: 0.00001473
Iteration 174/1000 | Loss: 0.00001473
Iteration 175/1000 | Loss: 0.00001473
Iteration 176/1000 | Loss: 0.00001472
Iteration 177/1000 | Loss: 0.00001472
Iteration 178/1000 | Loss: 0.00001472
Iteration 179/1000 | Loss: 0.00001472
Iteration 180/1000 | Loss: 0.00001472
Iteration 181/1000 | Loss: 0.00001472
Iteration 182/1000 | Loss: 0.00001472
Iteration 183/1000 | Loss: 0.00001472
Iteration 184/1000 | Loss: 0.00001472
Iteration 185/1000 | Loss: 0.00001472
Iteration 186/1000 | Loss: 0.00001472
Iteration 187/1000 | Loss: 0.00001472
Iteration 188/1000 | Loss: 0.00001472
Iteration 189/1000 | Loss: 0.00001472
Iteration 190/1000 | Loss: 0.00001472
Iteration 191/1000 | Loss: 0.00001471
Iteration 192/1000 | Loss: 0.00001471
Iteration 193/1000 | Loss: 0.00001471
Iteration 194/1000 | Loss: 0.00001471
Iteration 195/1000 | Loss: 0.00001471
Iteration 196/1000 | Loss: 0.00001471
Iteration 197/1000 | Loss: 0.00001471
Iteration 198/1000 | Loss: 0.00001471
Iteration 199/1000 | Loss: 0.00001471
Iteration 200/1000 | Loss: 0.00001471
Iteration 201/1000 | Loss: 0.00001471
Iteration 202/1000 | Loss: 0.00001471
Iteration 203/1000 | Loss: 0.00001471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.4710360119352117e-05, 1.4710360119352117e-05, 1.4710360119352117e-05, 1.4710360119352117e-05, 1.4710360119352117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4710360119352117e-05

Optimization complete. Final v2v error: 3.258350133895874 mm

Highest mean error: 3.9154627323150635 mm for frame 196

Lowest mean error: 2.9015204906463623 mm for frame 232

Saving results

Total time: 49.573357582092285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461600
Iteration 2/25 | Loss: 0.00126510
Iteration 3/25 | Loss: 0.00120177
Iteration 4/25 | Loss: 0.00118909
Iteration 5/25 | Loss: 0.00118548
Iteration 6/25 | Loss: 0.00118480
Iteration 7/25 | Loss: 0.00118480
Iteration 8/25 | Loss: 0.00118480
Iteration 9/25 | Loss: 0.00118480
Iteration 10/25 | Loss: 0.00118480
Iteration 11/25 | Loss: 0.00118480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011848022695630789, 0.0011848022695630789, 0.0011848022695630789, 0.0011848022695630789, 0.0011848022695630789]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011848022695630789

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24347508
Iteration 2/25 | Loss: 0.00173761
Iteration 3/25 | Loss: 0.00173760
Iteration 4/25 | Loss: 0.00173760
Iteration 5/25 | Loss: 0.00173760
Iteration 6/25 | Loss: 0.00173760
Iteration 7/25 | Loss: 0.00173760
Iteration 8/25 | Loss: 0.00173759
Iteration 9/25 | Loss: 0.00173759
Iteration 10/25 | Loss: 0.00173759
Iteration 11/25 | Loss: 0.00173759
Iteration 12/25 | Loss: 0.00173759
Iteration 13/25 | Loss: 0.00173759
Iteration 14/25 | Loss: 0.00173759
Iteration 15/25 | Loss: 0.00173759
Iteration 16/25 | Loss: 0.00173759
Iteration 17/25 | Loss: 0.00173759
Iteration 18/25 | Loss: 0.00173759
Iteration 19/25 | Loss: 0.00173759
Iteration 20/25 | Loss: 0.00173759
Iteration 21/25 | Loss: 0.00173759
Iteration 22/25 | Loss: 0.00173759
Iteration 23/25 | Loss: 0.00173759
Iteration 24/25 | Loss: 0.00173759
Iteration 25/25 | Loss: 0.00173759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173759
Iteration 2/1000 | Loss: 0.00002182
Iteration 3/1000 | Loss: 0.00001674
Iteration 4/1000 | Loss: 0.00001509
Iteration 5/1000 | Loss: 0.00001410
Iteration 6/1000 | Loss: 0.00001350
Iteration 7/1000 | Loss: 0.00001316
Iteration 8/1000 | Loss: 0.00001282
Iteration 9/1000 | Loss: 0.00001258
Iteration 10/1000 | Loss: 0.00001238
Iteration 11/1000 | Loss: 0.00001232
Iteration 12/1000 | Loss: 0.00001231
Iteration 13/1000 | Loss: 0.00001228
Iteration 14/1000 | Loss: 0.00001226
Iteration 15/1000 | Loss: 0.00001226
Iteration 16/1000 | Loss: 0.00001225
Iteration 17/1000 | Loss: 0.00001223
Iteration 18/1000 | Loss: 0.00001223
Iteration 19/1000 | Loss: 0.00001223
Iteration 20/1000 | Loss: 0.00001222
Iteration 21/1000 | Loss: 0.00001221
Iteration 22/1000 | Loss: 0.00001220
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001207
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001195
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001193
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001189
Iteration 31/1000 | Loss: 0.00001189
Iteration 32/1000 | Loss: 0.00001189
Iteration 33/1000 | Loss: 0.00001189
Iteration 34/1000 | Loss: 0.00001188
Iteration 35/1000 | Loss: 0.00001188
Iteration 36/1000 | Loss: 0.00001186
Iteration 37/1000 | Loss: 0.00001182
Iteration 38/1000 | Loss: 0.00001182
Iteration 39/1000 | Loss: 0.00001181
Iteration 40/1000 | Loss: 0.00001181
Iteration 41/1000 | Loss: 0.00001180
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001176
Iteration 44/1000 | Loss: 0.00001176
Iteration 45/1000 | Loss: 0.00001175
Iteration 46/1000 | Loss: 0.00001174
Iteration 47/1000 | Loss: 0.00001174
Iteration 48/1000 | Loss: 0.00001174
Iteration 49/1000 | Loss: 0.00001174
Iteration 50/1000 | Loss: 0.00001174
Iteration 51/1000 | Loss: 0.00001173
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001170
Iteration 58/1000 | Loss: 0.00001170
Iteration 59/1000 | Loss: 0.00001170
Iteration 60/1000 | Loss: 0.00001169
Iteration 61/1000 | Loss: 0.00001169
Iteration 62/1000 | Loss: 0.00001169
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001168
Iteration 65/1000 | Loss: 0.00001168
Iteration 66/1000 | Loss: 0.00001167
Iteration 67/1000 | Loss: 0.00001167
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001167
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001167
Iteration 72/1000 | Loss: 0.00001167
Iteration 73/1000 | Loss: 0.00001167
Iteration 74/1000 | Loss: 0.00001166
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001166
Iteration 77/1000 | Loss: 0.00001166
Iteration 78/1000 | Loss: 0.00001165
Iteration 79/1000 | Loss: 0.00001165
Iteration 80/1000 | Loss: 0.00001165
Iteration 81/1000 | Loss: 0.00001165
Iteration 82/1000 | Loss: 0.00001165
Iteration 83/1000 | Loss: 0.00001165
Iteration 84/1000 | Loss: 0.00001165
Iteration 85/1000 | Loss: 0.00001164
Iteration 86/1000 | Loss: 0.00001164
Iteration 87/1000 | Loss: 0.00001163
Iteration 88/1000 | Loss: 0.00001163
Iteration 89/1000 | Loss: 0.00001163
Iteration 90/1000 | Loss: 0.00001163
Iteration 91/1000 | Loss: 0.00001162
Iteration 92/1000 | Loss: 0.00001162
Iteration 93/1000 | Loss: 0.00001162
Iteration 94/1000 | Loss: 0.00001162
Iteration 95/1000 | Loss: 0.00001162
Iteration 96/1000 | Loss: 0.00001162
Iteration 97/1000 | Loss: 0.00001162
Iteration 98/1000 | Loss: 0.00001162
Iteration 99/1000 | Loss: 0.00001162
Iteration 100/1000 | Loss: 0.00001162
Iteration 101/1000 | Loss: 0.00001162
Iteration 102/1000 | Loss: 0.00001162
Iteration 103/1000 | Loss: 0.00001161
Iteration 104/1000 | Loss: 0.00001161
Iteration 105/1000 | Loss: 0.00001161
Iteration 106/1000 | Loss: 0.00001161
Iteration 107/1000 | Loss: 0.00001161
Iteration 108/1000 | Loss: 0.00001161
Iteration 109/1000 | Loss: 0.00001160
Iteration 110/1000 | Loss: 0.00001160
Iteration 111/1000 | Loss: 0.00001160
Iteration 112/1000 | Loss: 0.00001160
Iteration 113/1000 | Loss: 0.00001160
Iteration 114/1000 | Loss: 0.00001160
Iteration 115/1000 | Loss: 0.00001160
Iteration 116/1000 | Loss: 0.00001160
Iteration 117/1000 | Loss: 0.00001159
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001159
Iteration 120/1000 | Loss: 0.00001159
Iteration 121/1000 | Loss: 0.00001159
Iteration 122/1000 | Loss: 0.00001159
Iteration 123/1000 | Loss: 0.00001159
Iteration 124/1000 | Loss: 0.00001159
Iteration 125/1000 | Loss: 0.00001159
Iteration 126/1000 | Loss: 0.00001159
Iteration 127/1000 | Loss: 0.00001159
Iteration 128/1000 | Loss: 0.00001159
Iteration 129/1000 | Loss: 0.00001159
Iteration 130/1000 | Loss: 0.00001159
Iteration 131/1000 | Loss: 0.00001158
Iteration 132/1000 | Loss: 0.00001158
Iteration 133/1000 | Loss: 0.00001158
Iteration 134/1000 | Loss: 0.00001158
Iteration 135/1000 | Loss: 0.00001158
Iteration 136/1000 | Loss: 0.00001158
Iteration 137/1000 | Loss: 0.00001158
Iteration 138/1000 | Loss: 0.00001158
Iteration 139/1000 | Loss: 0.00001157
Iteration 140/1000 | Loss: 0.00001157
Iteration 141/1000 | Loss: 0.00001157
Iteration 142/1000 | Loss: 0.00001157
Iteration 143/1000 | Loss: 0.00001157
Iteration 144/1000 | Loss: 0.00001157
Iteration 145/1000 | Loss: 0.00001157
Iteration 146/1000 | Loss: 0.00001157
Iteration 147/1000 | Loss: 0.00001157
Iteration 148/1000 | Loss: 0.00001157
Iteration 149/1000 | Loss: 0.00001157
Iteration 150/1000 | Loss: 0.00001157
Iteration 151/1000 | Loss: 0.00001157
Iteration 152/1000 | Loss: 0.00001157
Iteration 153/1000 | Loss: 0.00001157
Iteration 154/1000 | Loss: 0.00001157
Iteration 155/1000 | Loss: 0.00001157
Iteration 156/1000 | Loss: 0.00001157
Iteration 157/1000 | Loss: 0.00001157
Iteration 158/1000 | Loss: 0.00001157
Iteration 159/1000 | Loss: 0.00001156
Iteration 160/1000 | Loss: 0.00001156
Iteration 161/1000 | Loss: 0.00001156
Iteration 162/1000 | Loss: 0.00001156
Iteration 163/1000 | Loss: 0.00001156
Iteration 164/1000 | Loss: 0.00001156
Iteration 165/1000 | Loss: 0.00001156
Iteration 166/1000 | Loss: 0.00001156
Iteration 167/1000 | Loss: 0.00001156
Iteration 168/1000 | Loss: 0.00001156
Iteration 169/1000 | Loss: 0.00001156
Iteration 170/1000 | Loss: 0.00001156
Iteration 171/1000 | Loss: 0.00001156
Iteration 172/1000 | Loss: 0.00001156
Iteration 173/1000 | Loss: 0.00001156
Iteration 174/1000 | Loss: 0.00001156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.1555300261534285e-05, 1.1555300261534285e-05, 1.1555300261534285e-05, 1.1555300261534285e-05, 1.1555300261534285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1555300261534285e-05

Optimization complete. Final v2v error: 2.864934206008911 mm

Highest mean error: 3.2809784412384033 mm for frame 176

Lowest mean error: 2.589372158050537 mm for frame 222

Saving results

Total time: 45.16792607307434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746346
Iteration 2/25 | Loss: 0.00176898
Iteration 3/25 | Loss: 0.00126704
Iteration 4/25 | Loss: 0.00118744
Iteration 5/25 | Loss: 0.00117197
Iteration 6/25 | Loss: 0.00117371
Iteration 7/25 | Loss: 0.00115800
Iteration 8/25 | Loss: 0.00114855
Iteration 9/25 | Loss: 0.00114148
Iteration 10/25 | Loss: 0.00114375
Iteration 11/25 | Loss: 0.00114099
Iteration 12/25 | Loss: 0.00114103
Iteration 13/25 | Loss: 0.00113949
Iteration 14/25 | Loss: 0.00113916
Iteration 15/25 | Loss: 0.00113915
Iteration 16/25 | Loss: 0.00113915
Iteration 17/25 | Loss: 0.00113958
Iteration 18/25 | Loss: 0.00114153
Iteration 19/25 | Loss: 0.00114178
Iteration 20/25 | Loss: 0.00114141
Iteration 21/25 | Loss: 0.00114084
Iteration 22/25 | Loss: 0.00114113
Iteration 23/25 | Loss: 0.00114045
Iteration 24/25 | Loss: 0.00114156
Iteration 25/25 | Loss: 0.00114015

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98349321
Iteration 2/25 | Loss: 0.00188171
Iteration 3/25 | Loss: 0.00188170
Iteration 4/25 | Loss: 0.00188170
Iteration 5/25 | Loss: 0.00188170
Iteration 6/25 | Loss: 0.00188170
Iteration 7/25 | Loss: 0.00188170
Iteration 8/25 | Loss: 0.00188170
Iteration 9/25 | Loss: 0.00188170
Iteration 10/25 | Loss: 0.00188170
Iteration 11/25 | Loss: 0.00188170
Iteration 12/25 | Loss: 0.00188170
Iteration 13/25 | Loss: 0.00188170
Iteration 14/25 | Loss: 0.00188170
Iteration 15/25 | Loss: 0.00188170
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0018817014060914516, 0.0018817014060914516, 0.0018817014060914516, 0.0018817014060914516, 0.0018817014060914516]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018817014060914516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188170
Iteration 2/1000 | Loss: 0.00002416
Iteration 3/1000 | Loss: 0.00014280
Iteration 4/1000 | Loss: 0.00010065
Iteration 5/1000 | Loss: 0.00011769
Iteration 6/1000 | Loss: 0.00005159
Iteration 7/1000 | Loss: 0.00001607
Iteration 8/1000 | Loss: 0.00011880
Iteration 9/1000 | Loss: 0.00009510
Iteration 10/1000 | Loss: 0.00003406
Iteration 11/1000 | Loss: 0.00003939
Iteration 12/1000 | Loss: 0.00001406
Iteration 13/1000 | Loss: 0.00010424
Iteration 14/1000 | Loss: 0.00007948
Iteration 15/1000 | Loss: 0.00006917
Iteration 16/1000 | Loss: 0.00006073
Iteration 17/1000 | Loss: 0.00008272
Iteration 18/1000 | Loss: 0.00010801
Iteration 19/1000 | Loss: 0.00009262
Iteration 20/1000 | Loss: 0.00006573
Iteration 21/1000 | Loss: 0.00001346
Iteration 22/1000 | Loss: 0.00001308
Iteration 23/1000 | Loss: 0.00012189
Iteration 24/1000 | Loss: 0.00011647
Iteration 25/1000 | Loss: 0.00008971
Iteration 26/1000 | Loss: 0.00007185
Iteration 27/1000 | Loss: 0.00006806
Iteration 28/1000 | Loss: 0.00018453
Iteration 29/1000 | Loss: 0.00002911
Iteration 30/1000 | Loss: 0.00003034
Iteration 31/1000 | Loss: 0.00001314
Iteration 32/1000 | Loss: 0.00001171
Iteration 33/1000 | Loss: 0.00001617
Iteration 34/1000 | Loss: 0.00001177
Iteration 35/1000 | Loss: 0.00001096
Iteration 36/1000 | Loss: 0.00001068
Iteration 37/1000 | Loss: 0.00001305
Iteration 38/1000 | Loss: 0.00001232
Iteration 39/1000 | Loss: 0.00001034
Iteration 40/1000 | Loss: 0.00001034
Iteration 41/1000 | Loss: 0.00001034
Iteration 42/1000 | Loss: 0.00001034
Iteration 43/1000 | Loss: 0.00001034
Iteration 44/1000 | Loss: 0.00001034
Iteration 45/1000 | Loss: 0.00001034
Iteration 46/1000 | Loss: 0.00001034
Iteration 47/1000 | Loss: 0.00001034
Iteration 48/1000 | Loss: 0.00001033
Iteration 49/1000 | Loss: 0.00001032
Iteration 50/1000 | Loss: 0.00001031
Iteration 51/1000 | Loss: 0.00001031
Iteration 52/1000 | Loss: 0.00001030
Iteration 53/1000 | Loss: 0.00001029
Iteration 54/1000 | Loss: 0.00001025
Iteration 55/1000 | Loss: 0.00001024
Iteration 56/1000 | Loss: 0.00001023
Iteration 57/1000 | Loss: 0.00001023
Iteration 58/1000 | Loss: 0.00001022
Iteration 59/1000 | Loss: 0.00001022
Iteration 60/1000 | Loss: 0.00001021
Iteration 61/1000 | Loss: 0.00001020
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001047
Iteration 64/1000 | Loss: 0.00001014
Iteration 65/1000 | Loss: 0.00001063
Iteration 66/1000 | Loss: 0.00001076
Iteration 67/1000 | Loss: 0.00001013
Iteration 68/1000 | Loss: 0.00001013
Iteration 69/1000 | Loss: 0.00001013
Iteration 70/1000 | Loss: 0.00001013
Iteration 71/1000 | Loss: 0.00001013
Iteration 72/1000 | Loss: 0.00001012
Iteration 73/1000 | Loss: 0.00001012
Iteration 74/1000 | Loss: 0.00001012
Iteration 75/1000 | Loss: 0.00001088
Iteration 76/1000 | Loss: 0.00001020
Iteration 77/1000 | Loss: 0.00001020
Iteration 78/1000 | Loss: 0.00001020
Iteration 79/1000 | Loss: 0.00001006
Iteration 80/1000 | Loss: 0.00001006
Iteration 81/1000 | Loss: 0.00001005
Iteration 82/1000 | Loss: 0.00001005
Iteration 83/1000 | Loss: 0.00001005
Iteration 84/1000 | Loss: 0.00001005
Iteration 85/1000 | Loss: 0.00001013
Iteration 86/1000 | Loss: 0.00001003
Iteration 87/1000 | Loss: 0.00001002
Iteration 88/1000 | Loss: 0.00001002
Iteration 89/1000 | Loss: 0.00001002
Iteration 90/1000 | Loss: 0.00001002
Iteration 91/1000 | Loss: 0.00001002
Iteration 92/1000 | Loss: 0.00001002
Iteration 93/1000 | Loss: 0.00001002
Iteration 94/1000 | Loss: 0.00001002
Iteration 95/1000 | Loss: 0.00001002
Iteration 96/1000 | Loss: 0.00001002
Iteration 97/1000 | Loss: 0.00001000
Iteration 98/1000 | Loss: 0.00000999
Iteration 99/1000 | Loss: 0.00000999
Iteration 100/1000 | Loss: 0.00000999
Iteration 101/1000 | Loss: 0.00000999
Iteration 102/1000 | Loss: 0.00001000
Iteration 103/1000 | Loss: 0.00000998
Iteration 104/1000 | Loss: 0.00001081
Iteration 105/1000 | Loss: 0.00000994
Iteration 106/1000 | Loss: 0.00000994
Iteration 107/1000 | Loss: 0.00000994
Iteration 108/1000 | Loss: 0.00000993
Iteration 109/1000 | Loss: 0.00000993
Iteration 110/1000 | Loss: 0.00000993
Iteration 111/1000 | Loss: 0.00000993
Iteration 112/1000 | Loss: 0.00000993
Iteration 113/1000 | Loss: 0.00000993
Iteration 114/1000 | Loss: 0.00000993
Iteration 115/1000 | Loss: 0.00000993
Iteration 116/1000 | Loss: 0.00000993
Iteration 117/1000 | Loss: 0.00000992
Iteration 118/1000 | Loss: 0.00000992
Iteration 119/1000 | Loss: 0.00000992
Iteration 120/1000 | Loss: 0.00000992
Iteration 121/1000 | Loss: 0.00000992
Iteration 122/1000 | Loss: 0.00000992
Iteration 123/1000 | Loss: 0.00000991
Iteration 124/1000 | Loss: 0.00000991
Iteration 125/1000 | Loss: 0.00000990
Iteration 126/1000 | Loss: 0.00000989
Iteration 127/1000 | Loss: 0.00000989
Iteration 128/1000 | Loss: 0.00000988
Iteration 129/1000 | Loss: 0.00000988
Iteration 130/1000 | Loss: 0.00000988
Iteration 131/1000 | Loss: 0.00000988
Iteration 132/1000 | Loss: 0.00000988
Iteration 133/1000 | Loss: 0.00000988
Iteration 134/1000 | Loss: 0.00000988
Iteration 135/1000 | Loss: 0.00000988
Iteration 136/1000 | Loss: 0.00000988
Iteration 137/1000 | Loss: 0.00000987
Iteration 138/1000 | Loss: 0.00000987
Iteration 139/1000 | Loss: 0.00000986
Iteration 140/1000 | Loss: 0.00000986
Iteration 141/1000 | Loss: 0.00000986
Iteration 142/1000 | Loss: 0.00000986
Iteration 143/1000 | Loss: 0.00000986
Iteration 144/1000 | Loss: 0.00000986
Iteration 145/1000 | Loss: 0.00000986
Iteration 146/1000 | Loss: 0.00000986
Iteration 147/1000 | Loss: 0.00000986
Iteration 148/1000 | Loss: 0.00000986
Iteration 149/1000 | Loss: 0.00000986
Iteration 150/1000 | Loss: 0.00000986
Iteration 151/1000 | Loss: 0.00000986
Iteration 152/1000 | Loss: 0.00000986
Iteration 153/1000 | Loss: 0.00000986
Iteration 154/1000 | Loss: 0.00000986
Iteration 155/1000 | Loss: 0.00000985
Iteration 156/1000 | Loss: 0.00000985
Iteration 157/1000 | Loss: 0.00000985
Iteration 158/1000 | Loss: 0.00000985
Iteration 159/1000 | Loss: 0.00000985
Iteration 160/1000 | Loss: 0.00000984
Iteration 161/1000 | Loss: 0.00000984
Iteration 162/1000 | Loss: 0.00000984
Iteration 163/1000 | Loss: 0.00000984
Iteration 164/1000 | Loss: 0.00000984
Iteration 165/1000 | Loss: 0.00000984
Iteration 166/1000 | Loss: 0.00000984
Iteration 167/1000 | Loss: 0.00000984
Iteration 168/1000 | Loss: 0.00000984
Iteration 169/1000 | Loss: 0.00000983
Iteration 170/1000 | Loss: 0.00000983
Iteration 171/1000 | Loss: 0.00000983
Iteration 172/1000 | Loss: 0.00000983
Iteration 173/1000 | Loss: 0.00000983
Iteration 174/1000 | Loss: 0.00000983
Iteration 175/1000 | Loss: 0.00000983
Iteration 176/1000 | Loss: 0.00000983
Iteration 177/1000 | Loss: 0.00000983
Iteration 178/1000 | Loss: 0.00000983
Iteration 179/1000 | Loss: 0.00000983
Iteration 180/1000 | Loss: 0.00000983
Iteration 181/1000 | Loss: 0.00000983
Iteration 182/1000 | Loss: 0.00001015
Iteration 183/1000 | Loss: 0.00001024
Iteration 184/1000 | Loss: 0.00000981
Iteration 185/1000 | Loss: 0.00000981
Iteration 186/1000 | Loss: 0.00000980
Iteration 187/1000 | Loss: 0.00000980
Iteration 188/1000 | Loss: 0.00000980
Iteration 189/1000 | Loss: 0.00000980
Iteration 190/1000 | Loss: 0.00000980
Iteration 191/1000 | Loss: 0.00000980
Iteration 192/1000 | Loss: 0.00000980
Iteration 193/1000 | Loss: 0.00000980
Iteration 194/1000 | Loss: 0.00000980
Iteration 195/1000 | Loss: 0.00000980
Iteration 196/1000 | Loss: 0.00000980
Iteration 197/1000 | Loss: 0.00000980
Iteration 198/1000 | Loss: 0.00000980
Iteration 199/1000 | Loss: 0.00000980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [9.797194252314512e-06, 9.797194252314512e-06, 9.797194252314512e-06, 9.797194252314512e-06, 9.797194252314512e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.797194252314512e-06

Optimization complete. Final v2v error: 2.7209835052490234 mm

Highest mean error: 3.3798046112060547 mm for frame 131

Lowest mean error: 2.449803590774536 mm for frame 2

Saving results

Total time: 121.13944292068481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770479
Iteration 2/25 | Loss: 0.00152494
Iteration 3/25 | Loss: 0.00134213
Iteration 4/25 | Loss: 0.00132174
Iteration 5/25 | Loss: 0.00131854
Iteration 6/25 | Loss: 0.00131854
Iteration 7/25 | Loss: 0.00131854
Iteration 8/25 | Loss: 0.00131854
Iteration 9/25 | Loss: 0.00131854
Iteration 10/25 | Loss: 0.00131854
Iteration 11/25 | Loss: 0.00131854
Iteration 12/25 | Loss: 0.00131854
Iteration 13/25 | Loss: 0.00131854
Iteration 14/25 | Loss: 0.00131854
Iteration 15/25 | Loss: 0.00131854
Iteration 16/25 | Loss: 0.00131854
Iteration 17/25 | Loss: 0.00131854
Iteration 18/25 | Loss: 0.00131854
Iteration 19/25 | Loss: 0.00131854
Iteration 20/25 | Loss: 0.00131854
Iteration 21/25 | Loss: 0.00131854
Iteration 22/25 | Loss: 0.00131854
Iteration 23/25 | Loss: 0.00131854
Iteration 24/25 | Loss: 0.00131854
Iteration 25/25 | Loss: 0.00131854

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.09136009
Iteration 2/25 | Loss: 0.00164808
Iteration 3/25 | Loss: 0.00164805
Iteration 4/25 | Loss: 0.00164805
Iteration 5/25 | Loss: 0.00164805
Iteration 6/25 | Loss: 0.00164805
Iteration 7/25 | Loss: 0.00164805
Iteration 8/25 | Loss: 0.00164804
Iteration 9/25 | Loss: 0.00164804
Iteration 10/25 | Loss: 0.00164804
Iteration 11/25 | Loss: 0.00164804
Iteration 12/25 | Loss: 0.00164804
Iteration 13/25 | Loss: 0.00164804
Iteration 14/25 | Loss: 0.00164804
Iteration 15/25 | Loss: 0.00164804
Iteration 16/25 | Loss: 0.00164804
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016480442136526108, 0.0016480442136526108, 0.0016480442136526108, 0.0016480442136526108, 0.0016480442136526108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016480442136526108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164804
Iteration 2/1000 | Loss: 0.00004430
Iteration 3/1000 | Loss: 0.00003064
Iteration 4/1000 | Loss: 0.00002761
Iteration 5/1000 | Loss: 0.00002651
Iteration 6/1000 | Loss: 0.00002559
Iteration 7/1000 | Loss: 0.00002507
Iteration 8/1000 | Loss: 0.00002472
Iteration 9/1000 | Loss: 0.00002429
Iteration 10/1000 | Loss: 0.00002386
Iteration 11/1000 | Loss: 0.00002345
Iteration 12/1000 | Loss: 0.00002314
Iteration 13/1000 | Loss: 0.00002286
Iteration 14/1000 | Loss: 0.00002259
Iteration 15/1000 | Loss: 0.00002244
Iteration 16/1000 | Loss: 0.00002229
Iteration 17/1000 | Loss: 0.00002215
Iteration 18/1000 | Loss: 0.00002214
Iteration 19/1000 | Loss: 0.00002214
Iteration 20/1000 | Loss: 0.00002213
Iteration 21/1000 | Loss: 0.00002212
Iteration 22/1000 | Loss: 0.00002208
Iteration 23/1000 | Loss: 0.00002208
Iteration 24/1000 | Loss: 0.00002207
Iteration 25/1000 | Loss: 0.00002205
Iteration 26/1000 | Loss: 0.00002204
Iteration 27/1000 | Loss: 0.00002204
Iteration 28/1000 | Loss: 0.00002204
Iteration 29/1000 | Loss: 0.00002203
Iteration 30/1000 | Loss: 0.00002203
Iteration 31/1000 | Loss: 0.00002203
Iteration 32/1000 | Loss: 0.00002203
Iteration 33/1000 | Loss: 0.00002203
Iteration 34/1000 | Loss: 0.00002203
Iteration 35/1000 | Loss: 0.00002203
Iteration 36/1000 | Loss: 0.00002203
Iteration 37/1000 | Loss: 0.00002202
Iteration 38/1000 | Loss: 0.00002202
Iteration 39/1000 | Loss: 0.00002202
Iteration 40/1000 | Loss: 0.00002202
Iteration 41/1000 | Loss: 0.00002201
Iteration 42/1000 | Loss: 0.00002200
Iteration 43/1000 | Loss: 0.00002200
Iteration 44/1000 | Loss: 0.00002199
Iteration 45/1000 | Loss: 0.00002199
Iteration 46/1000 | Loss: 0.00002199
Iteration 47/1000 | Loss: 0.00002199
Iteration 48/1000 | Loss: 0.00002199
Iteration 49/1000 | Loss: 0.00002199
Iteration 50/1000 | Loss: 0.00002199
Iteration 51/1000 | Loss: 0.00002199
Iteration 52/1000 | Loss: 0.00002199
Iteration 53/1000 | Loss: 0.00002198
Iteration 54/1000 | Loss: 0.00002198
Iteration 55/1000 | Loss: 0.00002198
Iteration 56/1000 | Loss: 0.00002198
Iteration 57/1000 | Loss: 0.00002197
Iteration 58/1000 | Loss: 0.00002197
Iteration 59/1000 | Loss: 0.00002197
Iteration 60/1000 | Loss: 0.00002197
Iteration 61/1000 | Loss: 0.00002197
Iteration 62/1000 | Loss: 0.00002197
Iteration 63/1000 | Loss: 0.00002196
Iteration 64/1000 | Loss: 0.00002196
Iteration 65/1000 | Loss: 0.00002195
Iteration 66/1000 | Loss: 0.00002195
Iteration 67/1000 | Loss: 0.00002194
Iteration 68/1000 | Loss: 0.00002194
Iteration 69/1000 | Loss: 0.00002192
Iteration 70/1000 | Loss: 0.00002192
Iteration 71/1000 | Loss: 0.00002192
Iteration 72/1000 | Loss: 0.00002192
Iteration 73/1000 | Loss: 0.00002192
Iteration 74/1000 | Loss: 0.00002192
Iteration 75/1000 | Loss: 0.00002192
Iteration 76/1000 | Loss: 0.00002191
Iteration 77/1000 | Loss: 0.00002191
Iteration 78/1000 | Loss: 0.00002190
Iteration 79/1000 | Loss: 0.00002190
Iteration 80/1000 | Loss: 0.00002190
Iteration 81/1000 | Loss: 0.00002190
Iteration 82/1000 | Loss: 0.00002190
Iteration 83/1000 | Loss: 0.00002189
Iteration 84/1000 | Loss: 0.00002189
Iteration 85/1000 | Loss: 0.00002189
Iteration 86/1000 | Loss: 0.00002189
Iteration 87/1000 | Loss: 0.00002189
Iteration 88/1000 | Loss: 0.00002189
Iteration 89/1000 | Loss: 0.00002189
Iteration 90/1000 | Loss: 0.00002189
Iteration 91/1000 | Loss: 0.00002189
Iteration 92/1000 | Loss: 0.00002189
Iteration 93/1000 | Loss: 0.00002189
Iteration 94/1000 | Loss: 0.00002189
Iteration 95/1000 | Loss: 0.00002189
Iteration 96/1000 | Loss: 0.00002189
Iteration 97/1000 | Loss: 0.00002189
Iteration 98/1000 | Loss: 0.00002189
Iteration 99/1000 | Loss: 0.00002189
Iteration 100/1000 | Loss: 0.00002189
Iteration 101/1000 | Loss: 0.00002189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.188640610256698e-05, 2.188640610256698e-05, 2.188640610256698e-05, 2.188640610256698e-05, 2.188640610256698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.188640610256698e-05

Optimization complete. Final v2v error: 3.76831316947937 mm

Highest mean error: 4.988459587097168 mm for frame 207

Lowest mean error: 3.241462230682373 mm for frame 64

Saving results

Total time: 43.657241344451904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970231
Iteration 2/25 | Loss: 0.00290856
Iteration 3/25 | Loss: 0.00199007
Iteration 4/25 | Loss: 0.00186246
Iteration 5/25 | Loss: 0.00163774
Iteration 6/25 | Loss: 0.00160542
Iteration 7/25 | Loss: 0.00152356
Iteration 8/25 | Loss: 0.00147674
Iteration 9/25 | Loss: 0.00138434
Iteration 10/25 | Loss: 0.00136127
Iteration 11/25 | Loss: 0.00135047
Iteration 12/25 | Loss: 0.00135548
Iteration 13/25 | Loss: 0.00135786
Iteration 14/25 | Loss: 0.00135463
Iteration 15/25 | Loss: 0.00134569
Iteration 16/25 | Loss: 0.00134081
Iteration 17/25 | Loss: 0.00133342
Iteration 18/25 | Loss: 0.00132321
Iteration 19/25 | Loss: 0.00131562
Iteration 20/25 | Loss: 0.00130847
Iteration 21/25 | Loss: 0.00130630
Iteration 22/25 | Loss: 0.00130940
Iteration 23/25 | Loss: 0.00130937
Iteration 24/25 | Loss: 0.00131129
Iteration 25/25 | Loss: 0.00130764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20378554
Iteration 2/25 | Loss: 0.00303128
Iteration 3/25 | Loss: 0.00295497
Iteration 4/25 | Loss: 0.00295497
Iteration 5/25 | Loss: 0.00295497
Iteration 6/25 | Loss: 0.00295497
Iteration 7/25 | Loss: 0.00295496
Iteration 8/25 | Loss: 0.00295496
Iteration 9/25 | Loss: 0.00295496
Iteration 10/25 | Loss: 0.00295496
Iteration 11/25 | Loss: 0.00295496
Iteration 12/25 | Loss: 0.00295496
Iteration 13/25 | Loss: 0.00295496
Iteration 14/25 | Loss: 0.00295496
Iteration 15/25 | Loss: 0.00295496
Iteration 16/25 | Loss: 0.00295496
Iteration 17/25 | Loss: 0.00295496
Iteration 18/25 | Loss: 0.00295496
Iteration 19/25 | Loss: 0.00295496
Iteration 20/25 | Loss: 0.00295496
Iteration 21/25 | Loss: 0.00295496
Iteration 22/25 | Loss: 0.00295496
Iteration 23/25 | Loss: 0.00295496
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002954962896183133, 0.002954962896183133, 0.002954962896183133, 0.002954962896183133, 0.002954962896183133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002954962896183133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00295496
Iteration 2/1000 | Loss: 0.00042307
Iteration 3/1000 | Loss: 0.00058887
Iteration 4/1000 | Loss: 0.00027570
Iteration 5/1000 | Loss: 0.00039878
Iteration 6/1000 | Loss: 0.00081377
Iteration 7/1000 | Loss: 0.00076439
Iteration 8/1000 | Loss: 0.00021417
Iteration 9/1000 | Loss: 0.00018131
Iteration 10/1000 | Loss: 0.00019572
Iteration 11/1000 | Loss: 0.00018095
Iteration 12/1000 | Loss: 0.00011878
Iteration 13/1000 | Loss: 0.00025334
Iteration 14/1000 | Loss: 0.00012147
Iteration 15/1000 | Loss: 0.00012963
Iteration 16/1000 | Loss: 0.00018155
Iteration 17/1000 | Loss: 0.00008592
Iteration 18/1000 | Loss: 0.00009041
Iteration 19/1000 | Loss: 0.00007442
Iteration 20/1000 | Loss: 0.00007990
Iteration 21/1000 | Loss: 0.00026242
Iteration 22/1000 | Loss: 0.00019043
Iteration 23/1000 | Loss: 0.00008510
Iteration 24/1000 | Loss: 0.00014128
Iteration 25/1000 | Loss: 0.00081275
Iteration 26/1000 | Loss: 0.00032034
Iteration 27/1000 | Loss: 0.00035916
Iteration 28/1000 | Loss: 0.00009090
Iteration 29/1000 | Loss: 0.00007395
Iteration 30/1000 | Loss: 0.00019736
Iteration 31/1000 | Loss: 0.00007120
Iteration 32/1000 | Loss: 0.00006779
Iteration 33/1000 | Loss: 0.00006563
Iteration 34/1000 | Loss: 0.00025497
Iteration 35/1000 | Loss: 0.00014998
Iteration 36/1000 | Loss: 0.00007371
Iteration 37/1000 | Loss: 0.00017370
Iteration 38/1000 | Loss: 0.00010327
Iteration 39/1000 | Loss: 0.00025745
Iteration 40/1000 | Loss: 0.00022510
Iteration 41/1000 | Loss: 0.00007307
Iteration 42/1000 | Loss: 0.00006254
Iteration 43/1000 | Loss: 0.00005908
Iteration 44/1000 | Loss: 0.00005708
Iteration 45/1000 | Loss: 0.00005583
Iteration 46/1000 | Loss: 0.00005511
Iteration 47/1000 | Loss: 0.00007030
Iteration 48/1000 | Loss: 0.00026225
Iteration 49/1000 | Loss: 0.00026752
Iteration 50/1000 | Loss: 0.00005663
Iteration 51/1000 | Loss: 0.00006278
Iteration 52/1000 | Loss: 0.00005417
Iteration 53/1000 | Loss: 0.00005675
Iteration 54/1000 | Loss: 0.00005227
Iteration 55/1000 | Loss: 0.00005188
Iteration 56/1000 | Loss: 0.00005153
Iteration 57/1000 | Loss: 0.00005122
Iteration 58/1000 | Loss: 0.00035410
Iteration 59/1000 | Loss: 0.00025789
Iteration 60/1000 | Loss: 0.00005307
Iteration 61/1000 | Loss: 0.00034291
Iteration 62/1000 | Loss: 0.00025268
Iteration 63/1000 | Loss: 0.00032252
Iteration 64/1000 | Loss: 0.00030751
Iteration 65/1000 | Loss: 0.00005227
Iteration 66/1000 | Loss: 0.00032724
Iteration 67/1000 | Loss: 0.00344102
Iteration 68/1000 | Loss: 0.00165564
Iteration 69/1000 | Loss: 0.00073958
Iteration 70/1000 | Loss: 0.00045907
Iteration 71/1000 | Loss: 0.00039952
Iteration 72/1000 | Loss: 0.00063641
Iteration 73/1000 | Loss: 0.00014188
Iteration 74/1000 | Loss: 0.00006445
Iteration 75/1000 | Loss: 0.00014957
Iteration 76/1000 | Loss: 0.00040272
Iteration 77/1000 | Loss: 0.00093205
Iteration 78/1000 | Loss: 0.00035856
Iteration 79/1000 | Loss: 0.00039992
Iteration 80/1000 | Loss: 0.00008284
Iteration 81/1000 | Loss: 0.00013096
Iteration 82/1000 | Loss: 0.00013530
Iteration 83/1000 | Loss: 0.00009774
Iteration 84/1000 | Loss: 0.00011467
Iteration 85/1000 | Loss: 0.00008062
Iteration 86/1000 | Loss: 0.00005398
Iteration 87/1000 | Loss: 0.00006207
Iteration 88/1000 | Loss: 0.00018359
Iteration 89/1000 | Loss: 0.00008387
Iteration 90/1000 | Loss: 0.00007382
Iteration 91/1000 | Loss: 0.00005568
Iteration 92/1000 | Loss: 0.00020965
Iteration 93/1000 | Loss: 0.00013492
Iteration 94/1000 | Loss: 0.00036392
Iteration 95/1000 | Loss: 0.00009768
Iteration 96/1000 | Loss: 0.00006649
Iteration 97/1000 | Loss: 0.00013232
Iteration 98/1000 | Loss: 0.00005342
Iteration 99/1000 | Loss: 0.00005556
Iteration 100/1000 | Loss: 0.00004889
Iteration 101/1000 | Loss: 0.00004768
Iteration 102/1000 | Loss: 0.00004999
Iteration 103/1000 | Loss: 0.00004746
Iteration 104/1000 | Loss: 0.00004657
Iteration 105/1000 | Loss: 0.00004728
Iteration 106/1000 | Loss: 0.00004791
Iteration 107/1000 | Loss: 0.00004628
Iteration 108/1000 | Loss: 0.00004615
Iteration 109/1000 | Loss: 0.00004645
Iteration 110/1000 | Loss: 0.00004599
Iteration 111/1000 | Loss: 0.00004588
Iteration 112/1000 | Loss: 0.00004587
Iteration 113/1000 | Loss: 0.00004586
Iteration 114/1000 | Loss: 0.00004586
Iteration 115/1000 | Loss: 0.00004585
Iteration 116/1000 | Loss: 0.00004585
Iteration 117/1000 | Loss: 0.00004585
Iteration 118/1000 | Loss: 0.00004585
Iteration 119/1000 | Loss: 0.00004584
Iteration 120/1000 | Loss: 0.00004584
Iteration 121/1000 | Loss: 0.00004583
Iteration 122/1000 | Loss: 0.00004582
Iteration 123/1000 | Loss: 0.00004582
Iteration 124/1000 | Loss: 0.00004579
Iteration 125/1000 | Loss: 0.00004579
Iteration 126/1000 | Loss: 0.00004578
Iteration 127/1000 | Loss: 0.00004578
Iteration 128/1000 | Loss: 0.00004578
Iteration 129/1000 | Loss: 0.00004578
Iteration 130/1000 | Loss: 0.00004578
Iteration 131/1000 | Loss: 0.00004577
Iteration 132/1000 | Loss: 0.00004577
Iteration 133/1000 | Loss: 0.00004576
Iteration 134/1000 | Loss: 0.00004576
Iteration 135/1000 | Loss: 0.00004576
Iteration 136/1000 | Loss: 0.00004576
Iteration 137/1000 | Loss: 0.00004576
Iteration 138/1000 | Loss: 0.00004575
Iteration 139/1000 | Loss: 0.00004575
Iteration 140/1000 | Loss: 0.00004575
Iteration 141/1000 | Loss: 0.00004574
Iteration 142/1000 | Loss: 0.00004573
Iteration 143/1000 | Loss: 0.00004573
Iteration 144/1000 | Loss: 0.00004572
Iteration 145/1000 | Loss: 0.00004571
Iteration 146/1000 | Loss: 0.00004571
Iteration 147/1000 | Loss: 0.00004571
Iteration 148/1000 | Loss: 0.00004571
Iteration 149/1000 | Loss: 0.00004570
Iteration 150/1000 | Loss: 0.00004570
Iteration 151/1000 | Loss: 0.00004570
Iteration 152/1000 | Loss: 0.00004570
Iteration 153/1000 | Loss: 0.00004569
Iteration 154/1000 | Loss: 0.00004567
Iteration 155/1000 | Loss: 0.00004567
Iteration 156/1000 | Loss: 0.00004567
Iteration 157/1000 | Loss: 0.00004567
Iteration 158/1000 | Loss: 0.00004567
Iteration 159/1000 | Loss: 0.00004567
Iteration 160/1000 | Loss: 0.00004567
Iteration 161/1000 | Loss: 0.00004567
Iteration 162/1000 | Loss: 0.00004567
Iteration 163/1000 | Loss: 0.00004566
Iteration 164/1000 | Loss: 0.00004566
Iteration 165/1000 | Loss: 0.00004566
Iteration 166/1000 | Loss: 0.00004566
Iteration 167/1000 | Loss: 0.00004566
Iteration 168/1000 | Loss: 0.00004566
Iteration 169/1000 | Loss: 0.00004566
Iteration 170/1000 | Loss: 0.00004565
Iteration 171/1000 | Loss: 0.00004565
Iteration 172/1000 | Loss: 0.00004564
Iteration 173/1000 | Loss: 0.00004564
Iteration 174/1000 | Loss: 0.00004564
Iteration 175/1000 | Loss: 0.00004564
Iteration 176/1000 | Loss: 0.00004564
Iteration 177/1000 | Loss: 0.00004564
Iteration 178/1000 | Loss: 0.00004564
Iteration 179/1000 | Loss: 0.00004564
Iteration 180/1000 | Loss: 0.00004564
Iteration 181/1000 | Loss: 0.00004564
Iteration 182/1000 | Loss: 0.00004564
Iteration 183/1000 | Loss: 0.00004564
Iteration 184/1000 | Loss: 0.00004563
Iteration 185/1000 | Loss: 0.00004563
Iteration 186/1000 | Loss: 0.00004563
Iteration 187/1000 | Loss: 0.00005403
Iteration 188/1000 | Loss: 0.00004565
Iteration 189/1000 | Loss: 0.00004557
Iteration 190/1000 | Loss: 0.00004556
Iteration 191/1000 | Loss: 0.00004556
Iteration 192/1000 | Loss: 0.00004556
Iteration 193/1000 | Loss: 0.00004556
Iteration 194/1000 | Loss: 0.00004556
Iteration 195/1000 | Loss: 0.00004555
Iteration 196/1000 | Loss: 0.00004555
Iteration 197/1000 | Loss: 0.00004555
Iteration 198/1000 | Loss: 0.00004554
Iteration 199/1000 | Loss: 0.00004554
Iteration 200/1000 | Loss: 0.00004554
Iteration 201/1000 | Loss: 0.00004554
Iteration 202/1000 | Loss: 0.00004554
Iteration 203/1000 | Loss: 0.00004554
Iteration 204/1000 | Loss: 0.00004554
Iteration 205/1000 | Loss: 0.00004554
Iteration 206/1000 | Loss: 0.00004554
Iteration 207/1000 | Loss: 0.00004554
Iteration 208/1000 | Loss: 0.00004554
Iteration 209/1000 | Loss: 0.00004554
Iteration 210/1000 | Loss: 0.00004554
Iteration 211/1000 | Loss: 0.00004553
Iteration 212/1000 | Loss: 0.00004553
Iteration 213/1000 | Loss: 0.00004553
Iteration 214/1000 | Loss: 0.00004553
Iteration 215/1000 | Loss: 0.00004553
Iteration 216/1000 | Loss: 0.00004553
Iteration 217/1000 | Loss: 0.00004553
Iteration 218/1000 | Loss: 0.00004553
Iteration 219/1000 | Loss: 0.00004553
Iteration 220/1000 | Loss: 0.00004553
Iteration 221/1000 | Loss: 0.00004553
Iteration 222/1000 | Loss: 0.00004553
Iteration 223/1000 | Loss: 0.00004553
Iteration 224/1000 | Loss: 0.00004553
Iteration 225/1000 | Loss: 0.00004553
Iteration 226/1000 | Loss: 0.00004553
Iteration 227/1000 | Loss: 0.00004552
Iteration 228/1000 | Loss: 0.00004552
Iteration 229/1000 | Loss: 0.00004552
Iteration 230/1000 | Loss: 0.00004552
Iteration 231/1000 | Loss: 0.00004551
Iteration 232/1000 | Loss: 0.00004551
Iteration 233/1000 | Loss: 0.00004551
Iteration 234/1000 | Loss: 0.00004551
Iteration 235/1000 | Loss: 0.00004551
Iteration 236/1000 | Loss: 0.00004551
Iteration 237/1000 | Loss: 0.00004551
Iteration 238/1000 | Loss: 0.00004551
Iteration 239/1000 | Loss: 0.00004551
Iteration 240/1000 | Loss: 0.00004551
Iteration 241/1000 | Loss: 0.00004550
Iteration 242/1000 | Loss: 0.00004550
Iteration 243/1000 | Loss: 0.00004550
Iteration 244/1000 | Loss: 0.00004550
Iteration 245/1000 | Loss: 0.00004550
Iteration 246/1000 | Loss: 0.00004550
Iteration 247/1000 | Loss: 0.00004550
Iteration 248/1000 | Loss: 0.00004549
Iteration 249/1000 | Loss: 0.00004549
Iteration 250/1000 | Loss: 0.00004549
Iteration 251/1000 | Loss: 0.00004549
Iteration 252/1000 | Loss: 0.00004549
Iteration 253/1000 | Loss: 0.00004549
Iteration 254/1000 | Loss: 0.00004549
Iteration 255/1000 | Loss: 0.00004549
Iteration 256/1000 | Loss: 0.00004549
Iteration 257/1000 | Loss: 0.00004549
Iteration 258/1000 | Loss: 0.00004549
Iteration 259/1000 | Loss: 0.00004549
Iteration 260/1000 | Loss: 0.00004946
Iteration 261/1000 | Loss: 0.00004547
Iteration 262/1000 | Loss: 0.00004547
Iteration 263/1000 | Loss: 0.00004547
Iteration 264/1000 | Loss: 0.00004547
Iteration 265/1000 | Loss: 0.00004547
Iteration 266/1000 | Loss: 0.00004547
Iteration 267/1000 | Loss: 0.00004547
Iteration 268/1000 | Loss: 0.00004547
Iteration 269/1000 | Loss: 0.00004547
Iteration 270/1000 | Loss: 0.00004547
Iteration 271/1000 | Loss: 0.00004547
Iteration 272/1000 | Loss: 0.00004547
Iteration 273/1000 | Loss: 0.00004547
Iteration 274/1000 | Loss: 0.00004547
Iteration 275/1000 | Loss: 0.00004547
Iteration 276/1000 | Loss: 0.00004547
Iteration 277/1000 | Loss: 0.00004547
Iteration 278/1000 | Loss: 0.00004547
Iteration 279/1000 | Loss: 0.00004547
Iteration 280/1000 | Loss: 0.00004547
Iteration 281/1000 | Loss: 0.00004547
Iteration 282/1000 | Loss: 0.00004547
Iteration 283/1000 | Loss: 0.00004547
Iteration 284/1000 | Loss: 0.00004547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [4.546874697552994e-05, 4.546874697552994e-05, 4.546874697552994e-05, 4.546874697552994e-05, 4.546874697552994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.546874697552994e-05

Optimization complete. Final v2v error: 4.158187389373779 mm

Highest mean error: 10.546332359313965 mm for frame 46

Lowest mean error: 2.4437990188598633 mm for frame 39

Saving results

Total time: 211.89737176895142
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00578956
Iteration 2/25 | Loss: 0.00146084
Iteration 3/25 | Loss: 0.00125721
Iteration 4/25 | Loss: 0.00117173
Iteration 5/25 | Loss: 0.00117184
Iteration 6/25 | Loss: 0.00115762
Iteration 7/25 | Loss: 0.00115776
Iteration 8/25 | Loss: 0.00115414
Iteration 9/25 | Loss: 0.00115341
Iteration 10/25 | Loss: 0.00115268
Iteration 11/25 | Loss: 0.00115238
Iteration 12/25 | Loss: 0.00115224
Iteration 13/25 | Loss: 0.00115222
Iteration 14/25 | Loss: 0.00115222
Iteration 15/25 | Loss: 0.00115222
Iteration 16/25 | Loss: 0.00115222
Iteration 17/25 | Loss: 0.00115222
Iteration 18/25 | Loss: 0.00115222
Iteration 19/25 | Loss: 0.00115222
Iteration 20/25 | Loss: 0.00115222
Iteration 21/25 | Loss: 0.00115221
Iteration 22/25 | Loss: 0.00115221
Iteration 23/25 | Loss: 0.00115221
Iteration 24/25 | Loss: 0.00115221
Iteration 25/25 | Loss: 0.00115221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.42170238
Iteration 2/25 | Loss: 0.00175342
Iteration 3/25 | Loss: 0.00175338
Iteration 4/25 | Loss: 0.00175337
Iteration 5/25 | Loss: 0.00175337
Iteration 6/25 | Loss: 0.00175337
Iteration 7/25 | Loss: 0.00175337
Iteration 8/25 | Loss: 0.00175337
Iteration 9/25 | Loss: 0.00175337
Iteration 10/25 | Loss: 0.00175337
Iteration 11/25 | Loss: 0.00175337
Iteration 12/25 | Loss: 0.00175337
Iteration 13/25 | Loss: 0.00175337
Iteration 14/25 | Loss: 0.00175337
Iteration 15/25 | Loss: 0.00175337
Iteration 16/25 | Loss: 0.00175337
Iteration 17/25 | Loss: 0.00175337
Iteration 18/25 | Loss: 0.00175337
Iteration 19/25 | Loss: 0.00175337
Iteration 20/25 | Loss: 0.00175337
Iteration 21/25 | Loss: 0.00175337
Iteration 22/25 | Loss: 0.00175337
Iteration 23/25 | Loss: 0.00175337
Iteration 24/25 | Loss: 0.00175337
Iteration 25/25 | Loss: 0.00175337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175337
Iteration 2/1000 | Loss: 0.00001930
Iteration 3/1000 | Loss: 0.00001360
Iteration 4/1000 | Loss: 0.00001211
Iteration 5/1000 | Loss: 0.00001125
Iteration 6/1000 | Loss: 0.00001069
Iteration 7/1000 | Loss: 0.00001032
Iteration 8/1000 | Loss: 0.00001008
Iteration 9/1000 | Loss: 0.00000980
Iteration 10/1000 | Loss: 0.00000977
Iteration 11/1000 | Loss: 0.00000956
Iteration 12/1000 | Loss: 0.00000954
Iteration 13/1000 | Loss: 0.00000948
Iteration 14/1000 | Loss: 0.00000946
Iteration 15/1000 | Loss: 0.00000946
Iteration 16/1000 | Loss: 0.00000945
Iteration 17/1000 | Loss: 0.00000944
Iteration 18/1000 | Loss: 0.00000943
Iteration 19/1000 | Loss: 0.00000942
Iteration 20/1000 | Loss: 0.00000941
Iteration 21/1000 | Loss: 0.00000937
Iteration 22/1000 | Loss: 0.00000934
Iteration 23/1000 | Loss: 0.00000933
Iteration 24/1000 | Loss: 0.00000933
Iteration 25/1000 | Loss: 0.00000933
Iteration 26/1000 | Loss: 0.00000932
Iteration 27/1000 | Loss: 0.00000931
Iteration 28/1000 | Loss: 0.00000930
Iteration 29/1000 | Loss: 0.00000929
Iteration 30/1000 | Loss: 0.00000929
Iteration 31/1000 | Loss: 0.00000928
Iteration 32/1000 | Loss: 0.00000928
Iteration 33/1000 | Loss: 0.00000928
Iteration 34/1000 | Loss: 0.00000926
Iteration 35/1000 | Loss: 0.00000926
Iteration 36/1000 | Loss: 0.00000925
Iteration 37/1000 | Loss: 0.00000924
Iteration 38/1000 | Loss: 0.00000924
Iteration 39/1000 | Loss: 0.00000924
Iteration 40/1000 | Loss: 0.00000924
Iteration 41/1000 | Loss: 0.00000924
Iteration 42/1000 | Loss: 0.00000923
Iteration 43/1000 | Loss: 0.00000923
Iteration 44/1000 | Loss: 0.00000922
Iteration 45/1000 | Loss: 0.00000922
Iteration 46/1000 | Loss: 0.00000921
Iteration 47/1000 | Loss: 0.00000921
Iteration 48/1000 | Loss: 0.00000920
Iteration 49/1000 | Loss: 0.00000920
Iteration 50/1000 | Loss: 0.00000920
Iteration 51/1000 | Loss: 0.00000920
Iteration 52/1000 | Loss: 0.00000920
Iteration 53/1000 | Loss: 0.00000920
Iteration 54/1000 | Loss: 0.00000920
Iteration 55/1000 | Loss: 0.00000919
Iteration 56/1000 | Loss: 0.00000919
Iteration 57/1000 | Loss: 0.00000919
Iteration 58/1000 | Loss: 0.00000918
Iteration 59/1000 | Loss: 0.00000917
Iteration 60/1000 | Loss: 0.00000917
Iteration 61/1000 | Loss: 0.00000917
Iteration 62/1000 | Loss: 0.00000917
Iteration 63/1000 | Loss: 0.00000917
Iteration 64/1000 | Loss: 0.00000917
Iteration 65/1000 | Loss: 0.00000917
Iteration 66/1000 | Loss: 0.00000916
Iteration 67/1000 | Loss: 0.00000916
Iteration 68/1000 | Loss: 0.00000916
Iteration 69/1000 | Loss: 0.00000916
Iteration 70/1000 | Loss: 0.00000916
Iteration 71/1000 | Loss: 0.00000916
Iteration 72/1000 | Loss: 0.00000916
Iteration 73/1000 | Loss: 0.00000916
Iteration 74/1000 | Loss: 0.00000916
Iteration 75/1000 | Loss: 0.00000916
Iteration 76/1000 | Loss: 0.00000915
Iteration 77/1000 | Loss: 0.00000915
Iteration 78/1000 | Loss: 0.00000914
Iteration 79/1000 | Loss: 0.00000914
Iteration 80/1000 | Loss: 0.00000913
Iteration 81/1000 | Loss: 0.00000913
Iteration 82/1000 | Loss: 0.00000913
Iteration 83/1000 | Loss: 0.00000913
Iteration 84/1000 | Loss: 0.00000912
Iteration 85/1000 | Loss: 0.00000912
Iteration 86/1000 | Loss: 0.00000912
Iteration 87/1000 | Loss: 0.00000911
Iteration 88/1000 | Loss: 0.00000911
Iteration 89/1000 | Loss: 0.00000911
Iteration 90/1000 | Loss: 0.00000910
Iteration 91/1000 | Loss: 0.00000910
Iteration 92/1000 | Loss: 0.00000910
Iteration 93/1000 | Loss: 0.00000909
Iteration 94/1000 | Loss: 0.00000909
Iteration 95/1000 | Loss: 0.00000909
Iteration 96/1000 | Loss: 0.00000909
Iteration 97/1000 | Loss: 0.00000909
Iteration 98/1000 | Loss: 0.00000908
Iteration 99/1000 | Loss: 0.00000908
Iteration 100/1000 | Loss: 0.00000908
Iteration 101/1000 | Loss: 0.00000908
Iteration 102/1000 | Loss: 0.00000907
Iteration 103/1000 | Loss: 0.00000907
Iteration 104/1000 | Loss: 0.00000907
Iteration 105/1000 | Loss: 0.00000907
Iteration 106/1000 | Loss: 0.00000906
Iteration 107/1000 | Loss: 0.00000906
Iteration 108/1000 | Loss: 0.00000906
Iteration 109/1000 | Loss: 0.00000906
Iteration 110/1000 | Loss: 0.00000906
Iteration 111/1000 | Loss: 0.00000906
Iteration 112/1000 | Loss: 0.00000906
Iteration 113/1000 | Loss: 0.00000905
Iteration 114/1000 | Loss: 0.00000905
Iteration 115/1000 | Loss: 0.00000905
Iteration 116/1000 | Loss: 0.00000905
Iteration 117/1000 | Loss: 0.00000905
Iteration 118/1000 | Loss: 0.00000905
Iteration 119/1000 | Loss: 0.00000905
Iteration 120/1000 | Loss: 0.00000905
Iteration 121/1000 | Loss: 0.00000904
Iteration 122/1000 | Loss: 0.00000904
Iteration 123/1000 | Loss: 0.00000904
Iteration 124/1000 | Loss: 0.00000904
Iteration 125/1000 | Loss: 0.00000904
Iteration 126/1000 | Loss: 0.00000904
Iteration 127/1000 | Loss: 0.00000904
Iteration 128/1000 | Loss: 0.00000903
Iteration 129/1000 | Loss: 0.00000903
Iteration 130/1000 | Loss: 0.00000903
Iteration 131/1000 | Loss: 0.00000903
Iteration 132/1000 | Loss: 0.00000903
Iteration 133/1000 | Loss: 0.00000903
Iteration 134/1000 | Loss: 0.00000902
Iteration 135/1000 | Loss: 0.00000902
Iteration 136/1000 | Loss: 0.00000902
Iteration 137/1000 | Loss: 0.00000902
Iteration 138/1000 | Loss: 0.00000902
Iteration 139/1000 | Loss: 0.00000902
Iteration 140/1000 | Loss: 0.00000902
Iteration 141/1000 | Loss: 0.00000902
Iteration 142/1000 | Loss: 0.00000901
Iteration 143/1000 | Loss: 0.00000901
Iteration 144/1000 | Loss: 0.00000901
Iteration 145/1000 | Loss: 0.00000901
Iteration 146/1000 | Loss: 0.00000901
Iteration 147/1000 | Loss: 0.00000900
Iteration 148/1000 | Loss: 0.00000900
Iteration 149/1000 | Loss: 0.00000900
Iteration 150/1000 | Loss: 0.00000900
Iteration 151/1000 | Loss: 0.00000900
Iteration 152/1000 | Loss: 0.00000900
Iteration 153/1000 | Loss: 0.00000900
Iteration 154/1000 | Loss: 0.00000899
Iteration 155/1000 | Loss: 0.00000899
Iteration 156/1000 | Loss: 0.00000899
Iteration 157/1000 | Loss: 0.00000899
Iteration 158/1000 | Loss: 0.00000899
Iteration 159/1000 | Loss: 0.00000899
Iteration 160/1000 | Loss: 0.00000898
Iteration 161/1000 | Loss: 0.00000898
Iteration 162/1000 | Loss: 0.00000898
Iteration 163/1000 | Loss: 0.00000898
Iteration 164/1000 | Loss: 0.00000898
Iteration 165/1000 | Loss: 0.00000898
Iteration 166/1000 | Loss: 0.00000898
Iteration 167/1000 | Loss: 0.00000897
Iteration 168/1000 | Loss: 0.00000897
Iteration 169/1000 | Loss: 0.00000897
Iteration 170/1000 | Loss: 0.00000897
Iteration 171/1000 | Loss: 0.00000897
Iteration 172/1000 | Loss: 0.00000897
Iteration 173/1000 | Loss: 0.00000897
Iteration 174/1000 | Loss: 0.00000897
Iteration 175/1000 | Loss: 0.00000897
Iteration 176/1000 | Loss: 0.00000896
Iteration 177/1000 | Loss: 0.00000896
Iteration 178/1000 | Loss: 0.00000896
Iteration 179/1000 | Loss: 0.00000896
Iteration 180/1000 | Loss: 0.00000896
Iteration 181/1000 | Loss: 0.00000896
Iteration 182/1000 | Loss: 0.00000896
Iteration 183/1000 | Loss: 0.00000896
Iteration 184/1000 | Loss: 0.00000896
Iteration 185/1000 | Loss: 0.00000896
Iteration 186/1000 | Loss: 0.00000896
Iteration 187/1000 | Loss: 0.00000896
Iteration 188/1000 | Loss: 0.00000896
Iteration 189/1000 | Loss: 0.00000896
Iteration 190/1000 | Loss: 0.00000896
Iteration 191/1000 | Loss: 0.00000896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [8.964499102148693e-06, 8.964499102148693e-06, 8.964499102148693e-06, 8.964499102148693e-06, 8.964499102148693e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.964499102148693e-06

Optimization complete. Final v2v error: 2.5505454540252686 mm

Highest mean error: 2.9769492149353027 mm for frame 166

Lowest mean error: 2.2574570178985596 mm for frame 140

Saving results

Total time: 57.40679669380188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00766862
Iteration 2/25 | Loss: 0.00149816
Iteration 3/25 | Loss: 0.00131332
Iteration 4/25 | Loss: 0.00128595
Iteration 5/25 | Loss: 0.00127119
Iteration 6/25 | Loss: 0.00126795
Iteration 7/25 | Loss: 0.00126491
Iteration 8/25 | Loss: 0.00126314
Iteration 9/25 | Loss: 0.00126287
Iteration 10/25 | Loss: 0.00126281
Iteration 11/25 | Loss: 0.00126281
Iteration 12/25 | Loss: 0.00126281
Iteration 13/25 | Loss: 0.00126280
Iteration 14/25 | Loss: 0.00126280
Iteration 15/25 | Loss: 0.00126280
Iteration 16/25 | Loss: 0.00126280
Iteration 17/25 | Loss: 0.00126280
Iteration 18/25 | Loss: 0.00126279
Iteration 19/25 | Loss: 0.00126279
Iteration 20/25 | Loss: 0.00126279
Iteration 21/25 | Loss: 0.00126279
Iteration 22/25 | Loss: 0.00126279
Iteration 23/25 | Loss: 0.00126279
Iteration 24/25 | Loss: 0.00126279
Iteration 25/25 | Loss: 0.00126279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.00585794
Iteration 2/25 | Loss: 0.00200643
Iteration 3/25 | Loss: 0.00200643
Iteration 4/25 | Loss: 0.00200643
Iteration 5/25 | Loss: 0.00200643
Iteration 6/25 | Loss: 0.00200643
Iteration 7/25 | Loss: 0.00200643
Iteration 8/25 | Loss: 0.00200643
Iteration 9/25 | Loss: 0.00200643
Iteration 10/25 | Loss: 0.00200643
Iteration 11/25 | Loss: 0.00200643
Iteration 12/25 | Loss: 0.00200643
Iteration 13/25 | Loss: 0.00200643
Iteration 14/25 | Loss: 0.00200643
Iteration 15/25 | Loss: 0.00200643
Iteration 16/25 | Loss: 0.00200643
Iteration 17/25 | Loss: 0.00200643
Iteration 18/25 | Loss: 0.00200643
Iteration 19/25 | Loss: 0.00200643
Iteration 20/25 | Loss: 0.00200643
Iteration 21/25 | Loss: 0.00200643
Iteration 22/25 | Loss: 0.00200643
Iteration 23/25 | Loss: 0.00200643
Iteration 24/25 | Loss: 0.00200643
Iteration 25/25 | Loss: 0.00200643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200643
Iteration 2/1000 | Loss: 0.00003958
Iteration 3/1000 | Loss: 0.00002285
Iteration 4/1000 | Loss: 0.00002050
Iteration 5/1000 | Loss: 0.00001959
Iteration 6/1000 | Loss: 0.00001895
Iteration 7/1000 | Loss: 0.00001859
Iteration 8/1000 | Loss: 0.00001813
Iteration 9/1000 | Loss: 0.00001785
Iteration 10/1000 | Loss: 0.00001771
Iteration 11/1000 | Loss: 0.00001755
Iteration 12/1000 | Loss: 0.00001753
Iteration 13/1000 | Loss: 0.00001745
Iteration 14/1000 | Loss: 0.00001744
Iteration 15/1000 | Loss: 0.00001744
Iteration 16/1000 | Loss: 0.00001743
Iteration 17/1000 | Loss: 0.00001740
Iteration 18/1000 | Loss: 0.00001732
Iteration 19/1000 | Loss: 0.00001731
Iteration 20/1000 | Loss: 0.00001727
Iteration 21/1000 | Loss: 0.00001727
Iteration 22/1000 | Loss: 0.00001726
Iteration 23/1000 | Loss: 0.00001726
Iteration 24/1000 | Loss: 0.00001725
Iteration 25/1000 | Loss: 0.00001725
Iteration 26/1000 | Loss: 0.00001724
Iteration 27/1000 | Loss: 0.00001723
Iteration 28/1000 | Loss: 0.00001723
Iteration 29/1000 | Loss: 0.00001722
Iteration 30/1000 | Loss: 0.00001721
Iteration 31/1000 | Loss: 0.00001721
Iteration 32/1000 | Loss: 0.00001721
Iteration 33/1000 | Loss: 0.00001721
Iteration 34/1000 | Loss: 0.00001721
Iteration 35/1000 | Loss: 0.00001721
Iteration 36/1000 | Loss: 0.00001720
Iteration 37/1000 | Loss: 0.00001720
Iteration 38/1000 | Loss: 0.00001720
Iteration 39/1000 | Loss: 0.00001720
Iteration 40/1000 | Loss: 0.00001719
Iteration 41/1000 | Loss: 0.00001719
Iteration 42/1000 | Loss: 0.00001719
Iteration 43/1000 | Loss: 0.00001718
Iteration 44/1000 | Loss: 0.00001718
Iteration 45/1000 | Loss: 0.00001718
Iteration 46/1000 | Loss: 0.00001718
Iteration 47/1000 | Loss: 0.00001718
Iteration 48/1000 | Loss: 0.00001717
Iteration 49/1000 | Loss: 0.00001717
Iteration 50/1000 | Loss: 0.00001717
Iteration 51/1000 | Loss: 0.00001717
Iteration 52/1000 | Loss: 0.00001716
Iteration 53/1000 | Loss: 0.00001716
Iteration 54/1000 | Loss: 0.00001715
Iteration 55/1000 | Loss: 0.00001715
Iteration 56/1000 | Loss: 0.00001715
Iteration 57/1000 | Loss: 0.00001715
Iteration 58/1000 | Loss: 0.00001714
Iteration 59/1000 | Loss: 0.00001714
Iteration 60/1000 | Loss: 0.00001714
Iteration 61/1000 | Loss: 0.00001714
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00001713
Iteration 64/1000 | Loss: 0.00001713
Iteration 65/1000 | Loss: 0.00001713
Iteration 66/1000 | Loss: 0.00001713
Iteration 67/1000 | Loss: 0.00001713
Iteration 68/1000 | Loss: 0.00001713
Iteration 69/1000 | Loss: 0.00001713
Iteration 70/1000 | Loss: 0.00001713
Iteration 71/1000 | Loss: 0.00001712
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00001711
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001711
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001710
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001710
Iteration 88/1000 | Loss: 0.00001710
Iteration 89/1000 | Loss: 0.00001710
Iteration 90/1000 | Loss: 0.00001710
Iteration 91/1000 | Loss: 0.00001710
Iteration 92/1000 | Loss: 0.00001710
Iteration 93/1000 | Loss: 0.00001710
Iteration 94/1000 | Loss: 0.00001710
Iteration 95/1000 | Loss: 0.00001710
Iteration 96/1000 | Loss: 0.00001710
Iteration 97/1000 | Loss: 0.00001709
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001709
Iteration 101/1000 | Loss: 0.00001709
Iteration 102/1000 | Loss: 0.00001709
Iteration 103/1000 | Loss: 0.00001709
Iteration 104/1000 | Loss: 0.00001709
Iteration 105/1000 | Loss: 0.00001709
Iteration 106/1000 | Loss: 0.00001709
Iteration 107/1000 | Loss: 0.00001709
Iteration 108/1000 | Loss: 0.00001709
Iteration 109/1000 | Loss: 0.00001708
Iteration 110/1000 | Loss: 0.00001708
Iteration 111/1000 | Loss: 0.00001708
Iteration 112/1000 | Loss: 0.00001708
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001708
Iteration 116/1000 | Loss: 0.00001708
Iteration 117/1000 | Loss: 0.00001708
Iteration 118/1000 | Loss: 0.00001708
Iteration 119/1000 | Loss: 0.00001708
Iteration 120/1000 | Loss: 0.00001708
Iteration 121/1000 | Loss: 0.00001708
Iteration 122/1000 | Loss: 0.00001708
Iteration 123/1000 | Loss: 0.00001708
Iteration 124/1000 | Loss: 0.00001708
Iteration 125/1000 | Loss: 0.00001708
Iteration 126/1000 | Loss: 0.00001708
Iteration 127/1000 | Loss: 0.00001708
Iteration 128/1000 | Loss: 0.00001708
Iteration 129/1000 | Loss: 0.00001708
Iteration 130/1000 | Loss: 0.00001707
Iteration 131/1000 | Loss: 0.00001707
Iteration 132/1000 | Loss: 0.00001707
Iteration 133/1000 | Loss: 0.00001707
Iteration 134/1000 | Loss: 0.00001707
Iteration 135/1000 | Loss: 0.00001707
Iteration 136/1000 | Loss: 0.00001707
Iteration 137/1000 | Loss: 0.00001707
Iteration 138/1000 | Loss: 0.00001707
Iteration 139/1000 | Loss: 0.00001707
Iteration 140/1000 | Loss: 0.00001707
Iteration 141/1000 | Loss: 0.00001707
Iteration 142/1000 | Loss: 0.00001707
Iteration 143/1000 | Loss: 0.00001706
Iteration 144/1000 | Loss: 0.00001706
Iteration 145/1000 | Loss: 0.00001706
Iteration 146/1000 | Loss: 0.00001706
Iteration 147/1000 | Loss: 0.00001706
Iteration 148/1000 | Loss: 0.00001706
Iteration 149/1000 | Loss: 0.00001706
Iteration 150/1000 | Loss: 0.00001706
Iteration 151/1000 | Loss: 0.00001706
Iteration 152/1000 | Loss: 0.00001706
Iteration 153/1000 | Loss: 0.00001706
Iteration 154/1000 | Loss: 0.00001706
Iteration 155/1000 | Loss: 0.00001706
Iteration 156/1000 | Loss: 0.00001706
Iteration 157/1000 | Loss: 0.00001706
Iteration 158/1000 | Loss: 0.00001706
Iteration 159/1000 | Loss: 0.00001706
Iteration 160/1000 | Loss: 0.00001706
Iteration 161/1000 | Loss: 0.00001706
Iteration 162/1000 | Loss: 0.00001705
Iteration 163/1000 | Loss: 0.00001705
Iteration 164/1000 | Loss: 0.00001705
Iteration 165/1000 | Loss: 0.00001705
Iteration 166/1000 | Loss: 0.00001705
Iteration 167/1000 | Loss: 0.00001705
Iteration 168/1000 | Loss: 0.00001705
Iteration 169/1000 | Loss: 0.00001705
Iteration 170/1000 | Loss: 0.00001705
Iteration 171/1000 | Loss: 0.00001705
Iteration 172/1000 | Loss: 0.00001705
Iteration 173/1000 | Loss: 0.00001705
Iteration 174/1000 | Loss: 0.00001705
Iteration 175/1000 | Loss: 0.00001705
Iteration 176/1000 | Loss: 0.00001705
Iteration 177/1000 | Loss: 0.00001705
Iteration 178/1000 | Loss: 0.00001705
Iteration 179/1000 | Loss: 0.00001705
Iteration 180/1000 | Loss: 0.00001705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.7046952052623965e-05, 1.7046952052623965e-05, 1.7046952052623965e-05, 1.7046952052623965e-05, 1.7046952052623965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7046952052623965e-05

Optimization complete. Final v2v error: 3.5170183181762695 mm

Highest mean error: 3.8938941955566406 mm for frame 186

Lowest mean error: 3.343379259109497 mm for frame 145

Saving results

Total time: 46.099207401275635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986340
Iteration 2/25 | Loss: 0.00189776
Iteration 3/25 | Loss: 0.00145571
Iteration 4/25 | Loss: 0.00136884
Iteration 5/25 | Loss: 0.00132060
Iteration 6/25 | Loss: 0.00129945
Iteration 7/25 | Loss: 0.00128933
Iteration 8/25 | Loss: 0.00127492
Iteration 9/25 | Loss: 0.00127100
Iteration 10/25 | Loss: 0.00126331
Iteration 11/25 | Loss: 0.00126487
Iteration 12/25 | Loss: 0.00126681
Iteration 13/25 | Loss: 0.00126302
Iteration 14/25 | Loss: 0.00125419
Iteration 15/25 | Loss: 0.00125313
Iteration 16/25 | Loss: 0.00125280
Iteration 17/25 | Loss: 0.00125576
Iteration 18/25 | Loss: 0.00125481
Iteration 19/25 | Loss: 0.00125148
Iteration 20/25 | Loss: 0.00124994
Iteration 21/25 | Loss: 0.00124916
Iteration 22/25 | Loss: 0.00125299
Iteration 23/25 | Loss: 0.00125103
Iteration 24/25 | Loss: 0.00124943
Iteration 25/25 | Loss: 0.00124914

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16063201
Iteration 2/25 | Loss: 0.00281836
Iteration 3/25 | Loss: 0.00250219
Iteration 4/25 | Loss: 0.00250218
Iteration 5/25 | Loss: 0.00250218
Iteration 6/25 | Loss: 0.00250218
Iteration 7/25 | Loss: 0.00250218
Iteration 8/25 | Loss: 0.00250218
Iteration 9/25 | Loss: 0.00250218
Iteration 10/25 | Loss: 0.00250218
Iteration 11/25 | Loss: 0.00250218
Iteration 12/25 | Loss: 0.00250218
Iteration 13/25 | Loss: 0.00250218
Iteration 14/25 | Loss: 0.00250218
Iteration 15/25 | Loss: 0.00250218
Iteration 16/25 | Loss: 0.00250218
Iteration 17/25 | Loss: 0.00250218
Iteration 18/25 | Loss: 0.00250218
Iteration 19/25 | Loss: 0.00250218
Iteration 20/25 | Loss: 0.00250218
Iteration 21/25 | Loss: 0.00250218
Iteration 22/25 | Loss: 0.00250218
Iteration 23/25 | Loss: 0.00250218
Iteration 24/25 | Loss: 0.00250218
Iteration 25/25 | Loss: 0.00250218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250218
Iteration 2/1000 | Loss: 0.00064869
Iteration 3/1000 | Loss: 0.00023424
Iteration 4/1000 | Loss: 0.00028366
Iteration 5/1000 | Loss: 0.00031487
Iteration 6/1000 | Loss: 0.00019173
Iteration 7/1000 | Loss: 0.00007412
Iteration 8/1000 | Loss: 0.00007069
Iteration 9/1000 | Loss: 0.00002578
Iteration 10/1000 | Loss: 0.00002287
Iteration 11/1000 | Loss: 0.00020023
Iteration 12/1000 | Loss: 0.00012499
Iteration 13/1000 | Loss: 0.00008283
Iteration 14/1000 | Loss: 0.00009889
Iteration 15/1000 | Loss: 0.00011102
Iteration 16/1000 | Loss: 0.00003823
Iteration 17/1000 | Loss: 0.00003544
Iteration 18/1000 | Loss: 0.00032250
Iteration 19/1000 | Loss: 0.00023503
Iteration 20/1000 | Loss: 0.00006770
Iteration 21/1000 | Loss: 0.00009905
Iteration 22/1000 | Loss: 0.00012880
Iteration 23/1000 | Loss: 0.00008917
Iteration 24/1000 | Loss: 0.00012661
Iteration 25/1000 | Loss: 0.00009690
Iteration 26/1000 | Loss: 0.00011579
Iteration 27/1000 | Loss: 0.00018955
Iteration 28/1000 | Loss: 0.00014364
Iteration 29/1000 | Loss: 0.00029296
Iteration 30/1000 | Loss: 0.00056579
Iteration 31/1000 | Loss: 0.00031220
Iteration 32/1000 | Loss: 0.00018824
Iteration 33/1000 | Loss: 0.00032095
Iteration 34/1000 | Loss: 0.00048372
Iteration 35/1000 | Loss: 0.00003674
Iteration 36/1000 | Loss: 0.00026162
Iteration 37/1000 | Loss: 0.00010376
Iteration 38/1000 | Loss: 0.00002211
Iteration 39/1000 | Loss: 0.00021235
Iteration 40/1000 | Loss: 0.00010987
Iteration 41/1000 | Loss: 0.00011009
Iteration 42/1000 | Loss: 0.00031068
Iteration 43/1000 | Loss: 0.00022948
Iteration 44/1000 | Loss: 0.00008985
Iteration 45/1000 | Loss: 0.00053847
Iteration 46/1000 | Loss: 0.00007805
Iteration 47/1000 | Loss: 0.00021674
Iteration 48/1000 | Loss: 0.00057726
Iteration 49/1000 | Loss: 0.00004471
Iteration 50/1000 | Loss: 0.00015536
Iteration 51/1000 | Loss: 0.00008701
Iteration 52/1000 | Loss: 0.00008772
Iteration 53/1000 | Loss: 0.00017607
Iteration 54/1000 | Loss: 0.00007880
Iteration 55/1000 | Loss: 0.00009562
Iteration 56/1000 | Loss: 0.00024697
Iteration 57/1000 | Loss: 0.00024999
Iteration 58/1000 | Loss: 0.00009138
Iteration 59/1000 | Loss: 0.00004453
Iteration 60/1000 | Loss: 0.00012420
Iteration 61/1000 | Loss: 0.00014454
Iteration 62/1000 | Loss: 0.00009467
Iteration 63/1000 | Loss: 0.00010118
Iteration 64/1000 | Loss: 0.00023920
Iteration 65/1000 | Loss: 0.00014574
Iteration 66/1000 | Loss: 0.00005266
Iteration 67/1000 | Loss: 0.00008372
Iteration 68/1000 | Loss: 0.00002491
Iteration 69/1000 | Loss: 0.00001891
Iteration 70/1000 | Loss: 0.00001867
Iteration 71/1000 | Loss: 0.00007888
Iteration 72/1000 | Loss: 0.00002174
Iteration 73/1000 | Loss: 0.00019342
Iteration 74/1000 | Loss: 0.00015653
Iteration 75/1000 | Loss: 0.00015052
Iteration 76/1000 | Loss: 0.00011216
Iteration 77/1000 | Loss: 0.00059668
Iteration 78/1000 | Loss: 0.00074487
Iteration 79/1000 | Loss: 0.00118162
Iteration 80/1000 | Loss: 0.00013512
Iteration 81/1000 | Loss: 0.00009621
Iteration 82/1000 | Loss: 0.00011895
Iteration 83/1000 | Loss: 0.00021299
Iteration 84/1000 | Loss: 0.00018330
Iteration 85/1000 | Loss: 0.00005216
Iteration 86/1000 | Loss: 0.00006596
Iteration 87/1000 | Loss: 0.00006011
Iteration 88/1000 | Loss: 0.00006923
Iteration 89/1000 | Loss: 0.00008838
Iteration 90/1000 | Loss: 0.00014361
Iteration 91/1000 | Loss: 0.00005629
Iteration 92/1000 | Loss: 0.00018150
Iteration 93/1000 | Loss: 0.00044202
Iteration 94/1000 | Loss: 0.00018236
Iteration 95/1000 | Loss: 0.00007935
Iteration 96/1000 | Loss: 0.00038781
Iteration 97/1000 | Loss: 0.00014072
Iteration 98/1000 | Loss: 0.00005548
Iteration 99/1000 | Loss: 0.00006738
Iteration 100/1000 | Loss: 0.00010848
Iteration 101/1000 | Loss: 0.00005856
Iteration 102/1000 | Loss: 0.00008227
Iteration 103/1000 | Loss: 0.00004807
Iteration 104/1000 | Loss: 0.00002329
Iteration 105/1000 | Loss: 0.00001858
Iteration 106/1000 | Loss: 0.00014543
Iteration 107/1000 | Loss: 0.00011630
Iteration 108/1000 | Loss: 0.00001729
Iteration 109/1000 | Loss: 0.00013748
Iteration 110/1000 | Loss: 0.00012210
Iteration 111/1000 | Loss: 0.00017996
Iteration 112/1000 | Loss: 0.00024646
Iteration 113/1000 | Loss: 0.00002784
Iteration 114/1000 | Loss: 0.00005097
Iteration 115/1000 | Loss: 0.00001843
Iteration 116/1000 | Loss: 0.00006712
Iteration 117/1000 | Loss: 0.00001986
Iteration 118/1000 | Loss: 0.00001862
Iteration 119/1000 | Loss: 0.00008057
Iteration 120/1000 | Loss: 0.00003520
Iteration 121/1000 | Loss: 0.00018553
Iteration 122/1000 | Loss: 0.00002562
Iteration 123/1000 | Loss: 0.00002188
Iteration 124/1000 | Loss: 0.00005811
Iteration 125/1000 | Loss: 0.00002038
Iteration 126/1000 | Loss: 0.00001978
Iteration 127/1000 | Loss: 0.00009283
Iteration 128/1000 | Loss: 0.00009182
Iteration 129/1000 | Loss: 0.00008721
Iteration 130/1000 | Loss: 0.00021818
Iteration 131/1000 | Loss: 0.00033771
Iteration 132/1000 | Loss: 0.00023252
Iteration 133/1000 | Loss: 0.00021275
Iteration 134/1000 | Loss: 0.00021799
Iteration 135/1000 | Loss: 0.00005307
Iteration 136/1000 | Loss: 0.00006313
Iteration 137/1000 | Loss: 0.00016982
Iteration 138/1000 | Loss: 0.00012672
Iteration 139/1000 | Loss: 0.00035286
Iteration 140/1000 | Loss: 0.00018737
Iteration 141/1000 | Loss: 0.00007764
Iteration 142/1000 | Loss: 0.00077865
Iteration 143/1000 | Loss: 0.00016082
Iteration 144/1000 | Loss: 0.00046932
Iteration 145/1000 | Loss: 0.00008968
Iteration 146/1000 | Loss: 0.00002312
Iteration 147/1000 | Loss: 0.00001968
Iteration 148/1000 | Loss: 0.00029189
Iteration 149/1000 | Loss: 0.00001677
Iteration 150/1000 | Loss: 0.00001573
Iteration 151/1000 | Loss: 0.00001519
Iteration 152/1000 | Loss: 0.00008734
Iteration 153/1000 | Loss: 0.00001454
Iteration 154/1000 | Loss: 0.00001424
Iteration 155/1000 | Loss: 0.00001402
Iteration 156/1000 | Loss: 0.00001384
Iteration 157/1000 | Loss: 0.00001383
Iteration 158/1000 | Loss: 0.00001383
Iteration 159/1000 | Loss: 0.00016006
Iteration 160/1000 | Loss: 0.00001634
Iteration 161/1000 | Loss: 0.00001414
Iteration 162/1000 | Loss: 0.00001378
Iteration 163/1000 | Loss: 0.00001378
Iteration 164/1000 | Loss: 0.00001377
Iteration 165/1000 | Loss: 0.00001372
Iteration 166/1000 | Loss: 0.00001366
Iteration 167/1000 | Loss: 0.00001365
Iteration 168/1000 | Loss: 0.00001363
Iteration 169/1000 | Loss: 0.00001363
Iteration 170/1000 | Loss: 0.00001363
Iteration 171/1000 | Loss: 0.00001363
Iteration 172/1000 | Loss: 0.00001363
Iteration 173/1000 | Loss: 0.00001363
Iteration 174/1000 | Loss: 0.00001363
Iteration 175/1000 | Loss: 0.00001363
Iteration 176/1000 | Loss: 0.00001363
Iteration 177/1000 | Loss: 0.00001363
Iteration 178/1000 | Loss: 0.00001362
Iteration 179/1000 | Loss: 0.00001362
Iteration 180/1000 | Loss: 0.00001362
Iteration 181/1000 | Loss: 0.00001362
Iteration 182/1000 | Loss: 0.00001362
Iteration 183/1000 | Loss: 0.00001362
Iteration 184/1000 | Loss: 0.00001361
Iteration 185/1000 | Loss: 0.00001361
Iteration 186/1000 | Loss: 0.00001361
Iteration 187/1000 | Loss: 0.00001361
Iteration 188/1000 | Loss: 0.00001361
Iteration 189/1000 | Loss: 0.00001361
Iteration 190/1000 | Loss: 0.00001360
Iteration 191/1000 | Loss: 0.00001360
Iteration 192/1000 | Loss: 0.00001360
Iteration 193/1000 | Loss: 0.00001360
Iteration 194/1000 | Loss: 0.00001360
Iteration 195/1000 | Loss: 0.00001360
Iteration 196/1000 | Loss: 0.00001360
Iteration 197/1000 | Loss: 0.00001360
Iteration 198/1000 | Loss: 0.00001360
Iteration 199/1000 | Loss: 0.00001360
Iteration 200/1000 | Loss: 0.00001360
Iteration 201/1000 | Loss: 0.00001359
Iteration 202/1000 | Loss: 0.00001359
Iteration 203/1000 | Loss: 0.00001359
Iteration 204/1000 | Loss: 0.00001359
Iteration 205/1000 | Loss: 0.00001359
Iteration 206/1000 | Loss: 0.00001359
Iteration 207/1000 | Loss: 0.00001359
Iteration 208/1000 | Loss: 0.00001359
Iteration 209/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.359376710752258e-05, 1.359376710752258e-05, 1.359376710752258e-05, 1.359376710752258e-05, 1.359376710752258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.359376710752258e-05

Optimization complete. Final v2v error: 3.117928981781006 mm

Highest mean error: 4.536785125732422 mm for frame 178

Lowest mean error: 2.5983200073242188 mm for frame 18

Saving results

Total time: 311.42772936820984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901823
Iteration 2/25 | Loss: 0.00130855
Iteration 3/25 | Loss: 0.00120584
Iteration 4/25 | Loss: 0.00118627
Iteration 5/25 | Loss: 0.00118015
Iteration 6/25 | Loss: 0.00117809
Iteration 7/25 | Loss: 0.00117787
Iteration 8/25 | Loss: 0.00117787
Iteration 9/25 | Loss: 0.00117787
Iteration 10/25 | Loss: 0.00117787
Iteration 11/25 | Loss: 0.00117787
Iteration 12/25 | Loss: 0.00117787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011778733460232615, 0.0011778733460232615, 0.0011778733460232615, 0.0011778733460232615, 0.0011778733460232615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011778733460232615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26956165
Iteration 2/25 | Loss: 0.00187744
Iteration 3/25 | Loss: 0.00187744
Iteration 4/25 | Loss: 0.00187744
Iteration 5/25 | Loss: 0.00187744
Iteration 6/25 | Loss: 0.00187744
Iteration 7/25 | Loss: 0.00187744
Iteration 8/25 | Loss: 0.00187744
Iteration 9/25 | Loss: 0.00187744
Iteration 10/25 | Loss: 0.00187744
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0018774393247440457, 0.0018774393247440457, 0.0018774393247440457, 0.0018774393247440457, 0.0018774393247440457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018774393247440457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187744
Iteration 2/1000 | Loss: 0.00003699
Iteration 3/1000 | Loss: 0.00002473
Iteration 4/1000 | Loss: 0.00002018
Iteration 5/1000 | Loss: 0.00001809
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001647
Iteration 8/1000 | Loss: 0.00001602
Iteration 9/1000 | Loss: 0.00001551
Iteration 10/1000 | Loss: 0.00001523
Iteration 11/1000 | Loss: 0.00001522
Iteration 12/1000 | Loss: 0.00001504
Iteration 13/1000 | Loss: 0.00001495
Iteration 14/1000 | Loss: 0.00001493
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001487
Iteration 17/1000 | Loss: 0.00001486
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001473
Iteration 20/1000 | Loss: 0.00001472
Iteration 21/1000 | Loss: 0.00001470
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001463
Iteration 25/1000 | Loss: 0.00001460
Iteration 26/1000 | Loss: 0.00001459
Iteration 27/1000 | Loss: 0.00001458
Iteration 28/1000 | Loss: 0.00001457
Iteration 29/1000 | Loss: 0.00001456
Iteration 30/1000 | Loss: 0.00001456
Iteration 31/1000 | Loss: 0.00001455
Iteration 32/1000 | Loss: 0.00001454
Iteration 33/1000 | Loss: 0.00001452
Iteration 34/1000 | Loss: 0.00001451
Iteration 35/1000 | Loss: 0.00001450
Iteration 36/1000 | Loss: 0.00001449
Iteration 37/1000 | Loss: 0.00001449
Iteration 38/1000 | Loss: 0.00001448
Iteration 39/1000 | Loss: 0.00001448
Iteration 40/1000 | Loss: 0.00001448
Iteration 41/1000 | Loss: 0.00001447
Iteration 42/1000 | Loss: 0.00001447
Iteration 43/1000 | Loss: 0.00001447
Iteration 44/1000 | Loss: 0.00001446
Iteration 45/1000 | Loss: 0.00001446
Iteration 46/1000 | Loss: 0.00001445
Iteration 47/1000 | Loss: 0.00001445
Iteration 48/1000 | Loss: 0.00001445
Iteration 49/1000 | Loss: 0.00001444
Iteration 50/1000 | Loss: 0.00001444
Iteration 51/1000 | Loss: 0.00001443
Iteration 52/1000 | Loss: 0.00001443
Iteration 53/1000 | Loss: 0.00001443
Iteration 54/1000 | Loss: 0.00001442
Iteration 55/1000 | Loss: 0.00001442
Iteration 56/1000 | Loss: 0.00001441
Iteration 57/1000 | Loss: 0.00001441
Iteration 58/1000 | Loss: 0.00001441
Iteration 59/1000 | Loss: 0.00001440
Iteration 60/1000 | Loss: 0.00001440
Iteration 61/1000 | Loss: 0.00001440
Iteration 62/1000 | Loss: 0.00001439
Iteration 63/1000 | Loss: 0.00001439
Iteration 64/1000 | Loss: 0.00001439
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001438
Iteration 67/1000 | Loss: 0.00001438
Iteration 68/1000 | Loss: 0.00001438
Iteration 69/1000 | Loss: 0.00001437
Iteration 70/1000 | Loss: 0.00001436
Iteration 71/1000 | Loss: 0.00001436
Iteration 72/1000 | Loss: 0.00001436
Iteration 73/1000 | Loss: 0.00001435
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001435
Iteration 77/1000 | Loss: 0.00001435
Iteration 78/1000 | Loss: 0.00001435
Iteration 79/1000 | Loss: 0.00001434
Iteration 80/1000 | Loss: 0.00001434
Iteration 81/1000 | Loss: 0.00001434
Iteration 82/1000 | Loss: 0.00001434
Iteration 83/1000 | Loss: 0.00001434
Iteration 84/1000 | Loss: 0.00001434
Iteration 85/1000 | Loss: 0.00001434
Iteration 86/1000 | Loss: 0.00001434
Iteration 87/1000 | Loss: 0.00001433
Iteration 88/1000 | Loss: 0.00001433
Iteration 89/1000 | Loss: 0.00001433
Iteration 90/1000 | Loss: 0.00001432
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001432
Iteration 94/1000 | Loss: 0.00001431
Iteration 95/1000 | Loss: 0.00001431
Iteration 96/1000 | Loss: 0.00001431
Iteration 97/1000 | Loss: 0.00001431
Iteration 98/1000 | Loss: 0.00001431
Iteration 99/1000 | Loss: 0.00001430
Iteration 100/1000 | Loss: 0.00001430
Iteration 101/1000 | Loss: 0.00001430
Iteration 102/1000 | Loss: 0.00001430
Iteration 103/1000 | Loss: 0.00001429
Iteration 104/1000 | Loss: 0.00001429
Iteration 105/1000 | Loss: 0.00001429
Iteration 106/1000 | Loss: 0.00001428
Iteration 107/1000 | Loss: 0.00001428
Iteration 108/1000 | Loss: 0.00001428
Iteration 109/1000 | Loss: 0.00001428
Iteration 110/1000 | Loss: 0.00001428
Iteration 111/1000 | Loss: 0.00001428
Iteration 112/1000 | Loss: 0.00001427
Iteration 113/1000 | Loss: 0.00001427
Iteration 114/1000 | Loss: 0.00001427
Iteration 115/1000 | Loss: 0.00001427
Iteration 116/1000 | Loss: 0.00001427
Iteration 117/1000 | Loss: 0.00001427
Iteration 118/1000 | Loss: 0.00001427
Iteration 119/1000 | Loss: 0.00001426
Iteration 120/1000 | Loss: 0.00001426
Iteration 121/1000 | Loss: 0.00001426
Iteration 122/1000 | Loss: 0.00001426
Iteration 123/1000 | Loss: 0.00001426
Iteration 124/1000 | Loss: 0.00001426
Iteration 125/1000 | Loss: 0.00001426
Iteration 126/1000 | Loss: 0.00001426
Iteration 127/1000 | Loss: 0.00001425
Iteration 128/1000 | Loss: 0.00001425
Iteration 129/1000 | Loss: 0.00001425
Iteration 130/1000 | Loss: 0.00001425
Iteration 131/1000 | Loss: 0.00001425
Iteration 132/1000 | Loss: 0.00001425
Iteration 133/1000 | Loss: 0.00001425
Iteration 134/1000 | Loss: 0.00001425
Iteration 135/1000 | Loss: 0.00001425
Iteration 136/1000 | Loss: 0.00001425
Iteration 137/1000 | Loss: 0.00001425
Iteration 138/1000 | Loss: 0.00001425
Iteration 139/1000 | Loss: 0.00001424
Iteration 140/1000 | Loss: 0.00001424
Iteration 141/1000 | Loss: 0.00001424
Iteration 142/1000 | Loss: 0.00001424
Iteration 143/1000 | Loss: 0.00001424
Iteration 144/1000 | Loss: 0.00001424
Iteration 145/1000 | Loss: 0.00001424
Iteration 146/1000 | Loss: 0.00001424
Iteration 147/1000 | Loss: 0.00001424
Iteration 148/1000 | Loss: 0.00001424
Iteration 149/1000 | Loss: 0.00001424
Iteration 150/1000 | Loss: 0.00001423
Iteration 151/1000 | Loss: 0.00001423
Iteration 152/1000 | Loss: 0.00001423
Iteration 153/1000 | Loss: 0.00001423
Iteration 154/1000 | Loss: 0.00001423
Iteration 155/1000 | Loss: 0.00001423
Iteration 156/1000 | Loss: 0.00001423
Iteration 157/1000 | Loss: 0.00001423
Iteration 158/1000 | Loss: 0.00001423
Iteration 159/1000 | Loss: 0.00001423
Iteration 160/1000 | Loss: 0.00001423
Iteration 161/1000 | Loss: 0.00001423
Iteration 162/1000 | Loss: 0.00001423
Iteration 163/1000 | Loss: 0.00001423
Iteration 164/1000 | Loss: 0.00001423
Iteration 165/1000 | Loss: 0.00001423
Iteration 166/1000 | Loss: 0.00001423
Iteration 167/1000 | Loss: 0.00001423
Iteration 168/1000 | Loss: 0.00001423
Iteration 169/1000 | Loss: 0.00001423
Iteration 170/1000 | Loss: 0.00001423
Iteration 171/1000 | Loss: 0.00001423
Iteration 172/1000 | Loss: 0.00001423
Iteration 173/1000 | Loss: 0.00001423
Iteration 174/1000 | Loss: 0.00001423
Iteration 175/1000 | Loss: 0.00001423
Iteration 176/1000 | Loss: 0.00001423
Iteration 177/1000 | Loss: 0.00001423
Iteration 178/1000 | Loss: 0.00001423
Iteration 179/1000 | Loss: 0.00001423
Iteration 180/1000 | Loss: 0.00001423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.422965397068765e-05, 1.422965397068765e-05, 1.422965397068765e-05, 1.422965397068765e-05, 1.422965397068765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.422965397068765e-05

Optimization complete. Final v2v error: 3.179164409637451 mm

Highest mean error: 5.033851146697998 mm for frame 77

Lowest mean error: 2.6781728267669678 mm for frame 102

Saving results

Total time: 40.286118268966675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935721
Iteration 2/25 | Loss: 0.00935721
Iteration 3/25 | Loss: 0.00935721
Iteration 4/25 | Loss: 0.00935721
Iteration 5/25 | Loss: 0.00935720
Iteration 6/25 | Loss: 0.00935720
Iteration 7/25 | Loss: 0.00935720
Iteration 8/25 | Loss: 0.00935720
Iteration 9/25 | Loss: 0.00935719
Iteration 10/25 | Loss: 0.00935719
Iteration 11/25 | Loss: 0.00935719
Iteration 12/25 | Loss: 0.00288007
Iteration 13/25 | Loss: 0.00201809
Iteration 14/25 | Loss: 0.00174915
Iteration 15/25 | Loss: 0.00149452
Iteration 16/25 | Loss: 0.00138254
Iteration 17/25 | Loss: 0.00134067
Iteration 18/25 | Loss: 0.00129137
Iteration 19/25 | Loss: 0.00127744
Iteration 20/25 | Loss: 0.00127631
Iteration 21/25 | Loss: 0.00127008
Iteration 22/25 | Loss: 0.00126683
Iteration 23/25 | Loss: 0.00126635
Iteration 24/25 | Loss: 0.00126429
Iteration 25/25 | Loss: 0.00126221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21907341
Iteration 2/25 | Loss: 0.00192484
Iteration 3/25 | Loss: 0.00191210
Iteration 4/25 | Loss: 0.00191210
Iteration 5/25 | Loss: 0.00191210
Iteration 6/25 | Loss: 0.00191210
Iteration 7/25 | Loss: 0.00191210
Iteration 8/25 | Loss: 0.00191210
Iteration 9/25 | Loss: 0.00191210
Iteration 10/25 | Loss: 0.00191210
Iteration 11/25 | Loss: 0.00191210
Iteration 12/25 | Loss: 0.00191210
Iteration 13/25 | Loss: 0.00191210
Iteration 14/25 | Loss: 0.00191210
Iteration 15/25 | Loss: 0.00191210
Iteration 16/25 | Loss: 0.00191210
Iteration 17/25 | Loss: 0.00191210
Iteration 18/25 | Loss: 0.00191210
Iteration 19/25 | Loss: 0.00191210
Iteration 20/25 | Loss: 0.00191210
Iteration 21/25 | Loss: 0.00191210
Iteration 22/25 | Loss: 0.00191210
Iteration 23/25 | Loss: 0.00191210
Iteration 24/25 | Loss: 0.00191210
Iteration 25/25 | Loss: 0.00191210

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191210
Iteration 2/1000 | Loss: 0.00008162
Iteration 3/1000 | Loss: 0.00036450
Iteration 4/1000 | Loss: 0.00017776
Iteration 5/1000 | Loss: 0.00008321
Iteration 6/1000 | Loss: 0.00007210
Iteration 7/1000 | Loss: 0.00004256
Iteration 8/1000 | Loss: 0.00011659
Iteration 9/1000 | Loss: 0.00041393
Iteration 10/1000 | Loss: 0.00005039
Iteration 11/1000 | Loss: 0.00003933
Iteration 12/1000 | Loss: 0.00003534
Iteration 13/1000 | Loss: 0.00023091
Iteration 14/1000 | Loss: 0.00015799
Iteration 15/1000 | Loss: 0.00012415
Iteration 16/1000 | Loss: 0.00004961
Iteration 17/1000 | Loss: 0.00003976
Iteration 18/1000 | Loss: 0.00010984
Iteration 19/1000 | Loss: 0.00019347
Iteration 20/1000 | Loss: 0.00014668
Iteration 21/1000 | Loss: 0.00008637
Iteration 22/1000 | Loss: 0.00005832
Iteration 23/1000 | Loss: 0.00006665
Iteration 24/1000 | Loss: 0.00004663
Iteration 25/1000 | Loss: 0.00003033
Iteration 26/1000 | Loss: 0.00002751
Iteration 27/1000 | Loss: 0.00012454
Iteration 28/1000 | Loss: 0.00018432
Iteration 29/1000 | Loss: 0.00004171
Iteration 30/1000 | Loss: 0.00016245
Iteration 31/1000 | Loss: 0.00008239
Iteration 32/1000 | Loss: 0.00005048
Iteration 33/1000 | Loss: 0.00002784
Iteration 34/1000 | Loss: 0.00002305
Iteration 35/1000 | Loss: 0.00002172
Iteration 36/1000 | Loss: 0.00020314
Iteration 37/1000 | Loss: 0.00002985
Iteration 38/1000 | Loss: 0.00020085
Iteration 39/1000 | Loss: 0.00012321
Iteration 40/1000 | Loss: 0.00005232
Iteration 41/1000 | Loss: 0.00013083
Iteration 42/1000 | Loss: 0.00002624
Iteration 43/1000 | Loss: 0.00002130
Iteration 44/1000 | Loss: 0.00001866
Iteration 45/1000 | Loss: 0.00001729
Iteration 46/1000 | Loss: 0.00001612
Iteration 47/1000 | Loss: 0.00002514
Iteration 48/1000 | Loss: 0.00015849
Iteration 49/1000 | Loss: 0.00002634
Iteration 50/1000 | Loss: 0.00002331
Iteration 51/1000 | Loss: 0.00003168
Iteration 52/1000 | Loss: 0.00001421
Iteration 53/1000 | Loss: 0.00002794
Iteration 54/1000 | Loss: 0.00001395
Iteration 55/1000 | Loss: 0.00001528
Iteration 56/1000 | Loss: 0.00001368
Iteration 57/1000 | Loss: 0.00001363
Iteration 58/1000 | Loss: 0.00001363
Iteration 59/1000 | Loss: 0.00001361
Iteration 60/1000 | Loss: 0.00001361
Iteration 61/1000 | Loss: 0.00001345
Iteration 62/1000 | Loss: 0.00001341
Iteration 63/1000 | Loss: 0.00001331
Iteration 64/1000 | Loss: 0.00001327
Iteration 65/1000 | Loss: 0.00001326
Iteration 66/1000 | Loss: 0.00003721
Iteration 67/1000 | Loss: 0.00001371
Iteration 68/1000 | Loss: 0.00001324
Iteration 69/1000 | Loss: 0.00001322
Iteration 70/1000 | Loss: 0.00001322
Iteration 71/1000 | Loss: 0.00001318
Iteration 72/1000 | Loss: 0.00001317
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001315
Iteration 76/1000 | Loss: 0.00001314
Iteration 77/1000 | Loss: 0.00001314
Iteration 78/1000 | Loss: 0.00001314
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001313
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001312
Iteration 85/1000 | Loss: 0.00001312
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001312
Iteration 88/1000 | Loss: 0.00001311
Iteration 89/1000 | Loss: 0.00001311
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001311
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001311
Iteration 94/1000 | Loss: 0.00001310
Iteration 95/1000 | Loss: 0.00001310
Iteration 96/1000 | Loss: 0.00001310
Iteration 97/1000 | Loss: 0.00001310
Iteration 98/1000 | Loss: 0.00001310
Iteration 99/1000 | Loss: 0.00001310
Iteration 100/1000 | Loss: 0.00001310
Iteration 101/1000 | Loss: 0.00001310
Iteration 102/1000 | Loss: 0.00001310
Iteration 103/1000 | Loss: 0.00001309
Iteration 104/1000 | Loss: 0.00001309
Iteration 105/1000 | Loss: 0.00001309
Iteration 106/1000 | Loss: 0.00001309
Iteration 107/1000 | Loss: 0.00001309
Iteration 108/1000 | Loss: 0.00001308
Iteration 109/1000 | Loss: 0.00001308
Iteration 110/1000 | Loss: 0.00001308
Iteration 111/1000 | Loss: 0.00001308
Iteration 112/1000 | Loss: 0.00001308
Iteration 113/1000 | Loss: 0.00001308
Iteration 114/1000 | Loss: 0.00001308
Iteration 115/1000 | Loss: 0.00001308
Iteration 116/1000 | Loss: 0.00001308
Iteration 117/1000 | Loss: 0.00001308
Iteration 118/1000 | Loss: 0.00001308
Iteration 119/1000 | Loss: 0.00001308
Iteration 120/1000 | Loss: 0.00001308
Iteration 121/1000 | Loss: 0.00001308
Iteration 122/1000 | Loss: 0.00001308
Iteration 123/1000 | Loss: 0.00001308
Iteration 124/1000 | Loss: 0.00001308
Iteration 125/1000 | Loss: 0.00001308
Iteration 126/1000 | Loss: 0.00001308
Iteration 127/1000 | Loss: 0.00001308
Iteration 128/1000 | Loss: 0.00001308
Iteration 129/1000 | Loss: 0.00001308
Iteration 130/1000 | Loss: 0.00001308
Iteration 131/1000 | Loss: 0.00001308
Iteration 132/1000 | Loss: 0.00001308
Iteration 133/1000 | Loss: 0.00001308
Iteration 134/1000 | Loss: 0.00001308
Iteration 135/1000 | Loss: 0.00001308
Iteration 136/1000 | Loss: 0.00001308
Iteration 137/1000 | Loss: 0.00001308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.3081613360554911e-05, 1.3081613360554911e-05, 1.3081613360554911e-05, 1.3081613360554911e-05, 1.3081613360554911e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3081613360554911e-05

Optimization complete. Final v2v error: 3.08622145652771 mm

Highest mean error: 4.27473783493042 mm for frame 79

Lowest mean error: 2.7682528495788574 mm for frame 121

Saving results

Total time: 136.45989561080933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811276
Iteration 2/25 | Loss: 0.00143290
Iteration 3/25 | Loss: 0.00123619
Iteration 4/25 | Loss: 0.00121805
Iteration 5/25 | Loss: 0.00121684
Iteration 6/25 | Loss: 0.00121684
Iteration 7/25 | Loss: 0.00121684
Iteration 8/25 | Loss: 0.00121684
Iteration 9/25 | Loss: 0.00121684
Iteration 10/25 | Loss: 0.00121684
Iteration 11/25 | Loss: 0.00121684
Iteration 12/25 | Loss: 0.00121684
Iteration 13/25 | Loss: 0.00121684
Iteration 14/25 | Loss: 0.00121684
Iteration 15/25 | Loss: 0.00121684
Iteration 16/25 | Loss: 0.00121684
Iteration 17/25 | Loss: 0.00121684
Iteration 18/25 | Loss: 0.00121684
Iteration 19/25 | Loss: 0.00121684
Iteration 20/25 | Loss: 0.00121684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012168434914201498, 0.0012168434914201498, 0.0012168434914201498, 0.0012168434914201498, 0.0012168434914201498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012168434914201498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15301478
Iteration 2/25 | Loss: 0.00140553
Iteration 3/25 | Loss: 0.00140552
Iteration 4/25 | Loss: 0.00140552
Iteration 5/25 | Loss: 0.00140552
Iteration 6/25 | Loss: 0.00140552
Iteration 7/25 | Loss: 0.00140552
Iteration 8/25 | Loss: 0.00140552
Iteration 9/25 | Loss: 0.00140552
Iteration 10/25 | Loss: 0.00140552
Iteration 11/25 | Loss: 0.00140552
Iteration 12/25 | Loss: 0.00140551
Iteration 13/25 | Loss: 0.00140551
Iteration 14/25 | Loss: 0.00140551
Iteration 15/25 | Loss: 0.00140551
Iteration 16/25 | Loss: 0.00140551
Iteration 17/25 | Loss: 0.00140551
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014055146602913737, 0.0014055146602913737, 0.0014055146602913737, 0.0014055146602913737, 0.0014055146602913737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014055146602913737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140551
Iteration 2/1000 | Loss: 0.00002597
Iteration 3/1000 | Loss: 0.00002015
Iteration 4/1000 | Loss: 0.00001829
Iteration 5/1000 | Loss: 0.00001749
Iteration 6/1000 | Loss: 0.00001692
Iteration 7/1000 | Loss: 0.00001641
Iteration 8/1000 | Loss: 0.00001590
Iteration 9/1000 | Loss: 0.00001570
Iteration 10/1000 | Loss: 0.00001559
Iteration 11/1000 | Loss: 0.00001536
Iteration 12/1000 | Loss: 0.00001515
Iteration 13/1000 | Loss: 0.00001497
Iteration 14/1000 | Loss: 0.00001482
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001479
Iteration 17/1000 | Loss: 0.00001478
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001475
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001470
Iteration 22/1000 | Loss: 0.00001461
Iteration 23/1000 | Loss: 0.00001461
Iteration 24/1000 | Loss: 0.00001458
Iteration 25/1000 | Loss: 0.00001458
Iteration 26/1000 | Loss: 0.00001453
Iteration 27/1000 | Loss: 0.00001448
Iteration 28/1000 | Loss: 0.00001448
Iteration 29/1000 | Loss: 0.00001447
Iteration 30/1000 | Loss: 0.00001446
Iteration 31/1000 | Loss: 0.00001444
Iteration 32/1000 | Loss: 0.00001444
Iteration 33/1000 | Loss: 0.00001444
Iteration 34/1000 | Loss: 0.00001444
Iteration 35/1000 | Loss: 0.00001444
Iteration 36/1000 | Loss: 0.00001444
Iteration 37/1000 | Loss: 0.00001444
Iteration 38/1000 | Loss: 0.00001444
Iteration 39/1000 | Loss: 0.00001444
Iteration 40/1000 | Loss: 0.00001443
Iteration 41/1000 | Loss: 0.00001443
Iteration 42/1000 | Loss: 0.00001443
Iteration 43/1000 | Loss: 0.00001442
Iteration 44/1000 | Loss: 0.00001441
Iteration 45/1000 | Loss: 0.00001441
Iteration 46/1000 | Loss: 0.00001441
Iteration 47/1000 | Loss: 0.00001441
Iteration 48/1000 | Loss: 0.00001440
Iteration 49/1000 | Loss: 0.00001440
Iteration 50/1000 | Loss: 0.00001440
Iteration 51/1000 | Loss: 0.00001440
Iteration 52/1000 | Loss: 0.00001439
Iteration 53/1000 | Loss: 0.00001439
Iteration 54/1000 | Loss: 0.00001439
Iteration 55/1000 | Loss: 0.00001439
Iteration 56/1000 | Loss: 0.00001439
Iteration 57/1000 | Loss: 0.00001439
Iteration 58/1000 | Loss: 0.00001439
Iteration 59/1000 | Loss: 0.00001439
Iteration 60/1000 | Loss: 0.00001439
Iteration 61/1000 | Loss: 0.00001439
Iteration 62/1000 | Loss: 0.00001439
Iteration 63/1000 | Loss: 0.00001439
Iteration 64/1000 | Loss: 0.00001439
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001439
Iteration 67/1000 | Loss: 0.00001439
Iteration 68/1000 | Loss: 0.00001438
Iteration 69/1000 | Loss: 0.00001438
Iteration 70/1000 | Loss: 0.00001438
Iteration 71/1000 | Loss: 0.00001438
Iteration 72/1000 | Loss: 0.00001438
Iteration 73/1000 | Loss: 0.00001437
Iteration 74/1000 | Loss: 0.00001437
Iteration 75/1000 | Loss: 0.00001437
Iteration 76/1000 | Loss: 0.00001437
Iteration 77/1000 | Loss: 0.00001436
Iteration 78/1000 | Loss: 0.00001436
Iteration 79/1000 | Loss: 0.00001436
Iteration 80/1000 | Loss: 0.00001436
Iteration 81/1000 | Loss: 0.00001436
Iteration 82/1000 | Loss: 0.00001436
Iteration 83/1000 | Loss: 0.00001436
Iteration 84/1000 | Loss: 0.00001436
Iteration 85/1000 | Loss: 0.00001436
Iteration 86/1000 | Loss: 0.00001436
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001435
Iteration 90/1000 | Loss: 0.00001435
Iteration 91/1000 | Loss: 0.00001435
Iteration 92/1000 | Loss: 0.00001435
Iteration 93/1000 | Loss: 0.00001435
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001434
Iteration 98/1000 | Loss: 0.00001434
Iteration 99/1000 | Loss: 0.00001434
Iteration 100/1000 | Loss: 0.00001434
Iteration 101/1000 | Loss: 0.00001434
Iteration 102/1000 | Loss: 0.00001434
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001434
Iteration 107/1000 | Loss: 0.00001434
Iteration 108/1000 | Loss: 0.00001434
Iteration 109/1000 | Loss: 0.00001434
Iteration 110/1000 | Loss: 0.00001434
Iteration 111/1000 | Loss: 0.00001434
Iteration 112/1000 | Loss: 0.00001434
Iteration 113/1000 | Loss: 0.00001434
Iteration 114/1000 | Loss: 0.00001434
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00001434
Iteration 117/1000 | Loss: 0.00001434
Iteration 118/1000 | Loss: 0.00001434
Iteration 119/1000 | Loss: 0.00001434
Iteration 120/1000 | Loss: 0.00001434
Iteration 121/1000 | Loss: 0.00001434
Iteration 122/1000 | Loss: 0.00001434
Iteration 123/1000 | Loss: 0.00001434
Iteration 124/1000 | Loss: 0.00001434
Iteration 125/1000 | Loss: 0.00001434
Iteration 126/1000 | Loss: 0.00001434
Iteration 127/1000 | Loss: 0.00001434
Iteration 128/1000 | Loss: 0.00001434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.4341795576910954e-05, 1.4341795576910954e-05, 1.4341795576910954e-05, 1.4341795576910954e-05, 1.4341795576910954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4341795576910954e-05

Optimization complete. Final v2v error: 3.2441036701202393 mm

Highest mean error: 3.4185919761657715 mm for frame 211

Lowest mean error: 3.116847038269043 mm for frame 62

Saving results

Total time: 39.914286613464355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978696
Iteration 2/25 | Loss: 0.00978696
Iteration 3/25 | Loss: 0.00261310
Iteration 4/25 | Loss: 0.00211175
Iteration 5/25 | Loss: 0.00185700
Iteration 6/25 | Loss: 0.00178787
Iteration 7/25 | Loss: 0.00175199
Iteration 8/25 | Loss: 0.00180817
Iteration 9/25 | Loss: 0.00171088
Iteration 10/25 | Loss: 0.00169225
Iteration 11/25 | Loss: 0.00163647
Iteration 12/25 | Loss: 0.00162426
Iteration 13/25 | Loss: 0.00159776
Iteration 14/25 | Loss: 0.00158430
Iteration 15/25 | Loss: 0.00155487
Iteration 16/25 | Loss: 0.00154351
Iteration 17/25 | Loss: 0.00153312
Iteration 18/25 | Loss: 0.00153298
Iteration 19/25 | Loss: 0.00150996
Iteration 20/25 | Loss: 0.00151093
Iteration 21/25 | Loss: 0.00149129
Iteration 22/25 | Loss: 0.00147819
Iteration 23/25 | Loss: 0.00146599
Iteration 24/25 | Loss: 0.00147168
Iteration 25/25 | Loss: 0.00145792

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24301934
Iteration 2/25 | Loss: 0.00531980
Iteration 3/25 | Loss: 0.00369451
Iteration 4/25 | Loss: 0.00369450
Iteration 5/25 | Loss: 0.00362321
Iteration 6/25 | Loss: 0.00362321
Iteration 7/25 | Loss: 0.00362321
Iteration 8/25 | Loss: 0.00362320
Iteration 9/25 | Loss: 0.00362320
Iteration 10/25 | Loss: 0.00362320
Iteration 11/25 | Loss: 0.00362320
Iteration 12/25 | Loss: 0.00362320
Iteration 13/25 | Loss: 0.00362320
Iteration 14/25 | Loss: 0.00362320
Iteration 15/25 | Loss: 0.00362320
Iteration 16/25 | Loss: 0.00362320
Iteration 17/25 | Loss: 0.00362320
Iteration 18/25 | Loss: 0.00362320
Iteration 19/25 | Loss: 0.00362320
Iteration 20/25 | Loss: 0.00362320
Iteration 21/25 | Loss: 0.00362320
Iteration 22/25 | Loss: 0.00362320
Iteration 23/25 | Loss: 0.00362320
Iteration 24/25 | Loss: 0.00362320
Iteration 25/25 | Loss: 0.00362320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.003623201511800289, 0.003623201511800289, 0.003623201511800289, 0.003623201511800289, 0.003623201511800289]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003623201511800289

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00362320
Iteration 2/1000 | Loss: 0.00255749
Iteration 3/1000 | Loss: 0.00158949
Iteration 4/1000 | Loss: 0.00372162
Iteration 5/1000 | Loss: 0.00203941
Iteration 6/1000 | Loss: 0.00048553
Iteration 7/1000 | Loss: 0.00117326
Iteration 8/1000 | Loss: 0.00035681
Iteration 9/1000 | Loss: 0.00133300
Iteration 10/1000 | Loss: 0.00116633
Iteration 11/1000 | Loss: 0.00301026
Iteration 12/1000 | Loss: 0.00109611
Iteration 13/1000 | Loss: 0.00040390
Iteration 14/1000 | Loss: 0.00136127
Iteration 15/1000 | Loss: 0.00048311
Iteration 16/1000 | Loss: 0.00089386
Iteration 17/1000 | Loss: 0.00036474
Iteration 18/1000 | Loss: 0.00023728
Iteration 19/1000 | Loss: 0.00030591
Iteration 20/1000 | Loss: 0.00020709
Iteration 21/1000 | Loss: 0.00016442
Iteration 22/1000 | Loss: 0.00046201
Iteration 23/1000 | Loss: 0.00041740
Iteration 24/1000 | Loss: 0.00032420
Iteration 25/1000 | Loss: 0.00149984
Iteration 26/1000 | Loss: 0.00082366
Iteration 27/1000 | Loss: 0.00051978
Iteration 28/1000 | Loss: 0.00025937
Iteration 29/1000 | Loss: 0.00022105
Iteration 30/1000 | Loss: 0.00053008
Iteration 31/1000 | Loss: 0.00113629
Iteration 32/1000 | Loss: 0.00067379
Iteration 33/1000 | Loss: 0.00035269
Iteration 34/1000 | Loss: 0.00023719
Iteration 35/1000 | Loss: 0.00049317
Iteration 36/1000 | Loss: 0.00045013
Iteration 37/1000 | Loss: 0.00021356
Iteration 38/1000 | Loss: 0.00019408
Iteration 39/1000 | Loss: 0.00057529
Iteration 40/1000 | Loss: 0.00116797
Iteration 41/1000 | Loss: 0.00043703
Iteration 42/1000 | Loss: 0.00105717
Iteration 43/1000 | Loss: 0.00123160
Iteration 44/1000 | Loss: 0.00070418
Iteration 45/1000 | Loss: 0.00025456
Iteration 46/1000 | Loss: 0.00013092
Iteration 47/1000 | Loss: 0.00039081
Iteration 48/1000 | Loss: 0.00050699
Iteration 49/1000 | Loss: 0.00195964
Iteration 50/1000 | Loss: 0.00025774
Iteration 51/1000 | Loss: 0.00076147
Iteration 52/1000 | Loss: 0.00018083
Iteration 53/1000 | Loss: 0.00029409
Iteration 54/1000 | Loss: 0.00100466
Iteration 55/1000 | Loss: 0.00018415
Iteration 56/1000 | Loss: 0.00028671
Iteration 57/1000 | Loss: 0.00046951
Iteration 58/1000 | Loss: 0.00087930
Iteration 59/1000 | Loss: 0.00139220
Iteration 60/1000 | Loss: 0.00031571
Iteration 61/1000 | Loss: 0.00020728
Iteration 62/1000 | Loss: 0.00060247
Iteration 63/1000 | Loss: 0.00175435
Iteration 64/1000 | Loss: 0.00014539
Iteration 65/1000 | Loss: 0.00026101
Iteration 66/1000 | Loss: 0.00134554
Iteration 67/1000 | Loss: 0.00015946
Iteration 68/1000 | Loss: 0.00028593
Iteration 69/1000 | Loss: 0.00037999
Iteration 70/1000 | Loss: 0.00076430
Iteration 71/1000 | Loss: 0.00036841
Iteration 72/1000 | Loss: 0.00072056
Iteration 73/1000 | Loss: 0.00013988
Iteration 74/1000 | Loss: 0.00021528
Iteration 75/1000 | Loss: 0.00056303
Iteration 76/1000 | Loss: 0.00029105
Iteration 77/1000 | Loss: 0.00108295
Iteration 78/1000 | Loss: 0.00046085
Iteration 79/1000 | Loss: 0.00025928
Iteration 80/1000 | Loss: 0.00028555
Iteration 81/1000 | Loss: 0.00009967
Iteration 82/1000 | Loss: 0.00011567
Iteration 83/1000 | Loss: 0.00006253
Iteration 84/1000 | Loss: 0.00051619
Iteration 85/1000 | Loss: 0.00079359
Iteration 86/1000 | Loss: 0.00011325
Iteration 87/1000 | Loss: 0.00007690
Iteration 88/1000 | Loss: 0.00005687
Iteration 89/1000 | Loss: 0.00015044
Iteration 90/1000 | Loss: 0.00080198
Iteration 91/1000 | Loss: 0.00018173
Iteration 92/1000 | Loss: 0.00008210
Iteration 93/1000 | Loss: 0.00019527
Iteration 94/1000 | Loss: 0.00011404
Iteration 95/1000 | Loss: 0.00050473
Iteration 96/1000 | Loss: 0.00062444
Iteration 97/1000 | Loss: 0.00045097
Iteration 98/1000 | Loss: 0.00069060
Iteration 99/1000 | Loss: 0.00019775
Iteration 100/1000 | Loss: 0.00012197
Iteration 101/1000 | Loss: 0.00007648
Iteration 102/1000 | Loss: 0.00030713
Iteration 103/1000 | Loss: 0.00005141
Iteration 104/1000 | Loss: 0.00008165
Iteration 105/1000 | Loss: 0.00005466
Iteration 106/1000 | Loss: 0.00043390
Iteration 107/1000 | Loss: 0.00052003
Iteration 108/1000 | Loss: 0.00138366
Iteration 109/1000 | Loss: 0.00053676
Iteration 110/1000 | Loss: 0.00019856
Iteration 111/1000 | Loss: 0.00004272
Iteration 112/1000 | Loss: 0.00022970
Iteration 113/1000 | Loss: 0.00008990
Iteration 114/1000 | Loss: 0.00003715
Iteration 115/1000 | Loss: 0.00004847
Iteration 116/1000 | Loss: 0.00004530
Iteration 117/1000 | Loss: 0.00047725
Iteration 118/1000 | Loss: 0.00021598
Iteration 119/1000 | Loss: 0.00007146
Iteration 120/1000 | Loss: 0.00018571
Iteration 121/1000 | Loss: 0.00007498
Iteration 122/1000 | Loss: 0.00003835
Iteration 123/1000 | Loss: 0.00016957
Iteration 124/1000 | Loss: 0.00009437
Iteration 125/1000 | Loss: 0.00003789
Iteration 126/1000 | Loss: 0.00043345
Iteration 127/1000 | Loss: 0.00013498
Iteration 128/1000 | Loss: 0.00003349
Iteration 129/1000 | Loss: 0.00047264
Iteration 130/1000 | Loss: 0.00059520
Iteration 131/1000 | Loss: 0.00021615
Iteration 132/1000 | Loss: 0.00031644
Iteration 133/1000 | Loss: 0.00003772
Iteration 134/1000 | Loss: 0.00023642
Iteration 135/1000 | Loss: 0.00003210
Iteration 136/1000 | Loss: 0.00011348
Iteration 137/1000 | Loss: 0.00020813
Iteration 138/1000 | Loss: 0.00080945
Iteration 139/1000 | Loss: 0.00027188
Iteration 140/1000 | Loss: 0.00016690
Iteration 141/1000 | Loss: 0.00004178
Iteration 142/1000 | Loss: 0.00003174
Iteration 143/1000 | Loss: 0.00019067
Iteration 144/1000 | Loss: 0.00007286
Iteration 145/1000 | Loss: 0.00003108
Iteration 146/1000 | Loss: 0.00005270
Iteration 147/1000 | Loss: 0.00005561
Iteration 148/1000 | Loss: 0.00018543
Iteration 149/1000 | Loss: 0.00126312
Iteration 150/1000 | Loss: 0.00018287
Iteration 151/1000 | Loss: 0.00016626
Iteration 152/1000 | Loss: 0.00018247
Iteration 153/1000 | Loss: 0.00013268
Iteration 154/1000 | Loss: 0.00015814
Iteration 155/1000 | Loss: 0.00002492
Iteration 156/1000 | Loss: 0.00007285
Iteration 157/1000 | Loss: 0.00004148
Iteration 158/1000 | Loss: 0.00006039
Iteration 159/1000 | Loss: 0.00002379
Iteration 160/1000 | Loss: 0.00002020
Iteration 161/1000 | Loss: 0.00002676
Iteration 162/1000 | Loss: 0.00029722
Iteration 163/1000 | Loss: 0.00145167
Iteration 164/1000 | Loss: 0.00035680
Iteration 165/1000 | Loss: 0.00047120
Iteration 166/1000 | Loss: 0.00012164
Iteration 167/1000 | Loss: 0.00035088
Iteration 168/1000 | Loss: 0.00003194
Iteration 169/1000 | Loss: 0.00010494
Iteration 170/1000 | Loss: 0.00011457
Iteration 171/1000 | Loss: 0.00002667
Iteration 172/1000 | Loss: 0.00004511
Iteration 173/1000 | Loss: 0.00010241
Iteration 174/1000 | Loss: 0.00058466
Iteration 175/1000 | Loss: 0.00017870
Iteration 176/1000 | Loss: 0.00071347
Iteration 177/1000 | Loss: 0.00184108
Iteration 178/1000 | Loss: 0.00082240
Iteration 179/1000 | Loss: 0.00032941
Iteration 180/1000 | Loss: 0.00037332
Iteration 181/1000 | Loss: 0.00220420
Iteration 182/1000 | Loss: 0.00022540
Iteration 183/1000 | Loss: 0.00024797
Iteration 184/1000 | Loss: 0.00005324
Iteration 185/1000 | Loss: 0.00010558
Iteration 186/1000 | Loss: 0.00002033
Iteration 187/1000 | Loss: 0.00007513
Iteration 188/1000 | Loss: 0.00001503
Iteration 189/1000 | Loss: 0.00004573
Iteration 190/1000 | Loss: 0.00005007
Iteration 191/1000 | Loss: 0.00022053
Iteration 192/1000 | Loss: 0.00002416
Iteration 193/1000 | Loss: 0.00003549
Iteration 194/1000 | Loss: 0.00003774
Iteration 195/1000 | Loss: 0.00001138
Iteration 196/1000 | Loss: 0.00003731
Iteration 197/1000 | Loss: 0.00001093
Iteration 198/1000 | Loss: 0.00007562
Iteration 199/1000 | Loss: 0.00001062
Iteration 200/1000 | Loss: 0.00001051
Iteration 201/1000 | Loss: 0.00001043
Iteration 202/1000 | Loss: 0.00001042
Iteration 203/1000 | Loss: 0.00001041
Iteration 204/1000 | Loss: 0.00006146
Iteration 205/1000 | Loss: 0.00001029
Iteration 206/1000 | Loss: 0.00001025
Iteration 207/1000 | Loss: 0.00001024
Iteration 208/1000 | Loss: 0.00001024
Iteration 209/1000 | Loss: 0.00001023
Iteration 210/1000 | Loss: 0.00001023
Iteration 211/1000 | Loss: 0.00001023
Iteration 212/1000 | Loss: 0.00001023
Iteration 213/1000 | Loss: 0.00001022
Iteration 214/1000 | Loss: 0.00001022
Iteration 215/1000 | Loss: 0.00001018
Iteration 216/1000 | Loss: 0.00001018
Iteration 217/1000 | Loss: 0.00001018
Iteration 218/1000 | Loss: 0.00001018
Iteration 219/1000 | Loss: 0.00001018
Iteration 220/1000 | Loss: 0.00001018
Iteration 221/1000 | Loss: 0.00001018
Iteration 222/1000 | Loss: 0.00001017
Iteration 223/1000 | Loss: 0.00001017
Iteration 224/1000 | Loss: 0.00001016
Iteration 225/1000 | Loss: 0.00001015
Iteration 226/1000 | Loss: 0.00001014
Iteration 227/1000 | Loss: 0.00001014
Iteration 228/1000 | Loss: 0.00001014
Iteration 229/1000 | Loss: 0.00001014
Iteration 230/1000 | Loss: 0.00001014
Iteration 231/1000 | Loss: 0.00001013
Iteration 232/1000 | Loss: 0.00001013
Iteration 233/1000 | Loss: 0.00001013
Iteration 234/1000 | Loss: 0.00001013
Iteration 235/1000 | Loss: 0.00001013
Iteration 236/1000 | Loss: 0.00001013
Iteration 237/1000 | Loss: 0.00001012
Iteration 238/1000 | Loss: 0.00001011
Iteration 239/1000 | Loss: 0.00001011
Iteration 240/1000 | Loss: 0.00001011
Iteration 241/1000 | Loss: 0.00001011
Iteration 242/1000 | Loss: 0.00001011
Iteration 243/1000 | Loss: 0.00001011
Iteration 244/1000 | Loss: 0.00001011
Iteration 245/1000 | Loss: 0.00001011
Iteration 246/1000 | Loss: 0.00001010
Iteration 247/1000 | Loss: 0.00001010
Iteration 248/1000 | Loss: 0.00001010
Iteration 249/1000 | Loss: 0.00001010
Iteration 250/1000 | Loss: 0.00001010
Iteration 251/1000 | Loss: 0.00001010
Iteration 252/1000 | Loss: 0.00001010
Iteration 253/1000 | Loss: 0.00001010
Iteration 254/1000 | Loss: 0.00001009
Iteration 255/1000 | Loss: 0.00001009
Iteration 256/1000 | Loss: 0.00004676
Iteration 257/1000 | Loss: 0.00006514
Iteration 258/1000 | Loss: 0.00001101
Iteration 259/1000 | Loss: 0.00002151
Iteration 260/1000 | Loss: 0.00001016
Iteration 261/1000 | Loss: 0.00001796
Iteration 262/1000 | Loss: 0.00001006
Iteration 263/1000 | Loss: 0.00001006
Iteration 264/1000 | Loss: 0.00005404
Iteration 265/1000 | Loss: 0.00016889
Iteration 266/1000 | Loss: 0.00003549
Iteration 267/1000 | Loss: 0.00071630
Iteration 268/1000 | Loss: 0.00003805
Iteration 269/1000 | Loss: 0.00002766
Iteration 270/1000 | Loss: 0.00001019
Iteration 271/1000 | Loss: 0.00005677
Iteration 272/1000 | Loss: 0.00005677
Iteration 273/1000 | Loss: 0.00005828
Iteration 274/1000 | Loss: 0.00053724
Iteration 275/1000 | Loss: 0.00005863
Iteration 276/1000 | Loss: 0.00003457
Iteration 277/1000 | Loss: 0.00002443
Iteration 278/1000 | Loss: 0.00010424
Iteration 279/1000 | Loss: 0.00006940
Iteration 280/1000 | Loss: 0.00002173
Iteration 281/1000 | Loss: 0.00001009
Iteration 282/1000 | Loss: 0.00001007
Iteration 283/1000 | Loss: 0.00001005
Iteration 284/1000 | Loss: 0.00001004
Iteration 285/1000 | Loss: 0.00002617
Iteration 286/1000 | Loss: 0.00003525
Iteration 287/1000 | Loss: 0.00004542
Iteration 288/1000 | Loss: 0.00001594
Iteration 289/1000 | Loss: 0.00002810
Iteration 290/1000 | Loss: 0.00042085
Iteration 291/1000 | Loss: 0.00004519
Iteration 292/1000 | Loss: 0.00002337
Iteration 293/1000 | Loss: 0.00002666
Iteration 294/1000 | Loss: 0.00001761
Iteration 295/1000 | Loss: 0.00001960
Iteration 296/1000 | Loss: 0.00009820
Iteration 297/1000 | Loss: 0.00003873
Iteration 298/1000 | Loss: 0.00001048
Iteration 299/1000 | Loss: 0.00002183
Iteration 300/1000 | Loss: 0.00001339
Iteration 301/1000 | Loss: 0.00001038
Iteration 302/1000 | Loss: 0.00002807
Iteration 303/1000 | Loss: 0.00001149
Iteration 304/1000 | Loss: 0.00001801
Iteration 305/1000 | Loss: 0.00007252
Iteration 306/1000 | Loss: 0.00002253
Iteration 307/1000 | Loss: 0.00001197
Iteration 308/1000 | Loss: 0.00000998
Iteration 309/1000 | Loss: 0.00000998
Iteration 310/1000 | Loss: 0.00000998
Iteration 311/1000 | Loss: 0.00000998
Iteration 312/1000 | Loss: 0.00000998
Iteration 313/1000 | Loss: 0.00000998
Iteration 314/1000 | Loss: 0.00000998
Iteration 315/1000 | Loss: 0.00000998
Iteration 316/1000 | Loss: 0.00000998
Iteration 317/1000 | Loss: 0.00000998
Iteration 318/1000 | Loss: 0.00000998
Iteration 319/1000 | Loss: 0.00000997
Iteration 320/1000 | Loss: 0.00000997
Iteration 321/1000 | Loss: 0.00000997
Iteration 322/1000 | Loss: 0.00000997
Iteration 323/1000 | Loss: 0.00000997
Iteration 324/1000 | Loss: 0.00000997
Iteration 325/1000 | Loss: 0.00000997
Iteration 326/1000 | Loss: 0.00000997
Iteration 327/1000 | Loss: 0.00000997
Iteration 328/1000 | Loss: 0.00000997
Iteration 329/1000 | Loss: 0.00000997
Iteration 330/1000 | Loss: 0.00000997
Iteration 331/1000 | Loss: 0.00000996
Iteration 332/1000 | Loss: 0.00000996
Iteration 333/1000 | Loss: 0.00000996
Iteration 334/1000 | Loss: 0.00000996
Iteration 335/1000 | Loss: 0.00000996
Iteration 336/1000 | Loss: 0.00000996
Iteration 337/1000 | Loss: 0.00000996
Iteration 338/1000 | Loss: 0.00000996
Iteration 339/1000 | Loss: 0.00000996
Iteration 340/1000 | Loss: 0.00000996
Iteration 341/1000 | Loss: 0.00000995
Iteration 342/1000 | Loss: 0.00000995
Iteration 343/1000 | Loss: 0.00000995
Iteration 344/1000 | Loss: 0.00000995
Iteration 345/1000 | Loss: 0.00000995
Iteration 346/1000 | Loss: 0.00000995
Iteration 347/1000 | Loss: 0.00000995
Iteration 348/1000 | Loss: 0.00000995
Iteration 349/1000 | Loss: 0.00000995
Iteration 350/1000 | Loss: 0.00000995
Iteration 351/1000 | Loss: 0.00000995
Iteration 352/1000 | Loss: 0.00000995
Iteration 353/1000 | Loss: 0.00000995
Iteration 354/1000 | Loss: 0.00000995
Iteration 355/1000 | Loss: 0.00000995
Iteration 356/1000 | Loss: 0.00000995
Iteration 357/1000 | Loss: 0.00000995
Iteration 358/1000 | Loss: 0.00000995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 358. Stopping optimization.
Last 5 losses: [9.952784239430912e-06, 9.952784239430912e-06, 9.952784239430912e-06, 9.952784239430912e-06, 9.952784239430912e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.952784239430912e-06

Optimization complete. Final v2v error: 2.7051594257354736 mm

Highest mean error: 3.6804420948028564 mm for frame 122

Lowest mean error: 2.351875066757202 mm for frame 66

Saving results

Total time: 451.8626685142517
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00638800
Iteration 2/25 | Loss: 0.00142004
Iteration 3/25 | Loss: 0.00126892
Iteration 4/25 | Loss: 0.00125675
Iteration 5/25 | Loss: 0.00125353
Iteration 6/25 | Loss: 0.00125326
Iteration 7/25 | Loss: 0.00125326
Iteration 8/25 | Loss: 0.00125326
Iteration 9/25 | Loss: 0.00125326
Iteration 10/25 | Loss: 0.00125326
Iteration 11/25 | Loss: 0.00125326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012532641412690282, 0.0012532641412690282, 0.0012532641412690282, 0.0012532641412690282, 0.0012532641412690282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012532641412690282

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46790767
Iteration 2/25 | Loss: 0.00192000
Iteration 3/25 | Loss: 0.00191999
Iteration 4/25 | Loss: 0.00191999
Iteration 5/25 | Loss: 0.00191999
Iteration 6/25 | Loss: 0.00191999
Iteration 7/25 | Loss: 0.00191999
Iteration 8/25 | Loss: 0.00191998
Iteration 9/25 | Loss: 0.00191998
Iteration 10/25 | Loss: 0.00191998
Iteration 11/25 | Loss: 0.00191998
Iteration 12/25 | Loss: 0.00191998
Iteration 13/25 | Loss: 0.00191998
Iteration 14/25 | Loss: 0.00191998
Iteration 15/25 | Loss: 0.00191998
Iteration 16/25 | Loss: 0.00191998
Iteration 17/25 | Loss: 0.00191998
Iteration 18/25 | Loss: 0.00191998
Iteration 19/25 | Loss: 0.00191998
Iteration 20/25 | Loss: 0.00191998
Iteration 21/25 | Loss: 0.00191998
Iteration 22/25 | Loss: 0.00191998
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0019199837697669864, 0.0019199837697669864, 0.0019199837697669864, 0.0019199837697669864, 0.0019199837697669864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019199837697669864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191998
Iteration 2/1000 | Loss: 0.00005973
Iteration 3/1000 | Loss: 0.00003447
Iteration 4/1000 | Loss: 0.00002747
Iteration 5/1000 | Loss: 0.00002421
Iteration 6/1000 | Loss: 0.00002260
Iteration 7/1000 | Loss: 0.00002159
Iteration 8/1000 | Loss: 0.00002092
Iteration 9/1000 | Loss: 0.00002032
Iteration 10/1000 | Loss: 0.00001995
Iteration 11/1000 | Loss: 0.00001967
Iteration 12/1000 | Loss: 0.00001940
Iteration 13/1000 | Loss: 0.00001916
Iteration 14/1000 | Loss: 0.00001899
Iteration 15/1000 | Loss: 0.00001886
Iteration 16/1000 | Loss: 0.00001884
Iteration 17/1000 | Loss: 0.00001879
Iteration 18/1000 | Loss: 0.00001877
Iteration 19/1000 | Loss: 0.00001876
Iteration 20/1000 | Loss: 0.00001875
Iteration 21/1000 | Loss: 0.00001875
Iteration 22/1000 | Loss: 0.00001874
Iteration 23/1000 | Loss: 0.00001873
Iteration 24/1000 | Loss: 0.00001872
Iteration 25/1000 | Loss: 0.00001872
Iteration 26/1000 | Loss: 0.00001871
Iteration 27/1000 | Loss: 0.00001867
Iteration 28/1000 | Loss: 0.00001864
Iteration 29/1000 | Loss: 0.00001863
Iteration 30/1000 | Loss: 0.00001863
Iteration 31/1000 | Loss: 0.00001861
Iteration 32/1000 | Loss: 0.00001858
Iteration 33/1000 | Loss: 0.00001857
Iteration 34/1000 | Loss: 0.00001857
Iteration 35/1000 | Loss: 0.00001857
Iteration 36/1000 | Loss: 0.00001856
Iteration 37/1000 | Loss: 0.00001856
Iteration 38/1000 | Loss: 0.00001855
Iteration 39/1000 | Loss: 0.00001854
Iteration 40/1000 | Loss: 0.00001854
Iteration 41/1000 | Loss: 0.00001854
Iteration 42/1000 | Loss: 0.00001853
Iteration 43/1000 | Loss: 0.00001852
Iteration 44/1000 | Loss: 0.00001852
Iteration 45/1000 | Loss: 0.00001851
Iteration 46/1000 | Loss: 0.00001851
Iteration 47/1000 | Loss: 0.00001850
Iteration 48/1000 | Loss: 0.00001850
Iteration 49/1000 | Loss: 0.00001849
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00001848
Iteration 52/1000 | Loss: 0.00001847
Iteration 53/1000 | Loss: 0.00001846
Iteration 54/1000 | Loss: 0.00001846
Iteration 55/1000 | Loss: 0.00001845
Iteration 56/1000 | Loss: 0.00001845
Iteration 57/1000 | Loss: 0.00001844
Iteration 58/1000 | Loss: 0.00001843
Iteration 59/1000 | Loss: 0.00001843
Iteration 60/1000 | Loss: 0.00001842
Iteration 61/1000 | Loss: 0.00001839
Iteration 62/1000 | Loss: 0.00001837
Iteration 63/1000 | Loss: 0.00001835
Iteration 64/1000 | Loss: 0.00001835
Iteration 65/1000 | Loss: 0.00001834
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001833
Iteration 68/1000 | Loss: 0.00001833
Iteration 69/1000 | Loss: 0.00001830
Iteration 70/1000 | Loss: 0.00001827
Iteration 71/1000 | Loss: 0.00001827
Iteration 72/1000 | Loss: 0.00001826
Iteration 73/1000 | Loss: 0.00001825
Iteration 74/1000 | Loss: 0.00001825
Iteration 75/1000 | Loss: 0.00001824
Iteration 76/1000 | Loss: 0.00001824
Iteration 77/1000 | Loss: 0.00001823
Iteration 78/1000 | Loss: 0.00001823
Iteration 79/1000 | Loss: 0.00001823
Iteration 80/1000 | Loss: 0.00001823
Iteration 81/1000 | Loss: 0.00001822
Iteration 82/1000 | Loss: 0.00001822
Iteration 83/1000 | Loss: 0.00001822
Iteration 84/1000 | Loss: 0.00001821
Iteration 85/1000 | Loss: 0.00001821
Iteration 86/1000 | Loss: 0.00001821
Iteration 87/1000 | Loss: 0.00001821
Iteration 88/1000 | Loss: 0.00001821
Iteration 89/1000 | Loss: 0.00001820
Iteration 90/1000 | Loss: 0.00001820
Iteration 91/1000 | Loss: 0.00001819
Iteration 92/1000 | Loss: 0.00001819
Iteration 93/1000 | Loss: 0.00001818
Iteration 94/1000 | Loss: 0.00001818
Iteration 95/1000 | Loss: 0.00001817
Iteration 96/1000 | Loss: 0.00001817
Iteration 97/1000 | Loss: 0.00001816
Iteration 98/1000 | Loss: 0.00001816
Iteration 99/1000 | Loss: 0.00001815
Iteration 100/1000 | Loss: 0.00001815
Iteration 101/1000 | Loss: 0.00001814
Iteration 102/1000 | Loss: 0.00001813
Iteration 103/1000 | Loss: 0.00001813
Iteration 104/1000 | Loss: 0.00001812
Iteration 105/1000 | Loss: 0.00001811
Iteration 106/1000 | Loss: 0.00001811
Iteration 107/1000 | Loss: 0.00001810
Iteration 108/1000 | Loss: 0.00001810
Iteration 109/1000 | Loss: 0.00001807
Iteration 110/1000 | Loss: 0.00001807
Iteration 111/1000 | Loss: 0.00001806
Iteration 112/1000 | Loss: 0.00001806
Iteration 113/1000 | Loss: 0.00001805
Iteration 114/1000 | Loss: 0.00001805
Iteration 115/1000 | Loss: 0.00001805
Iteration 116/1000 | Loss: 0.00001804
Iteration 117/1000 | Loss: 0.00001804
Iteration 118/1000 | Loss: 0.00001803
Iteration 119/1000 | Loss: 0.00001803
Iteration 120/1000 | Loss: 0.00001802
Iteration 121/1000 | Loss: 0.00001802
Iteration 122/1000 | Loss: 0.00001801
Iteration 123/1000 | Loss: 0.00001801
Iteration 124/1000 | Loss: 0.00001800
Iteration 125/1000 | Loss: 0.00001800
Iteration 126/1000 | Loss: 0.00001800
Iteration 127/1000 | Loss: 0.00001799
Iteration 128/1000 | Loss: 0.00001799
Iteration 129/1000 | Loss: 0.00001799
Iteration 130/1000 | Loss: 0.00001798
Iteration 131/1000 | Loss: 0.00001798
Iteration 132/1000 | Loss: 0.00001798
Iteration 133/1000 | Loss: 0.00001798
Iteration 134/1000 | Loss: 0.00001798
Iteration 135/1000 | Loss: 0.00001798
Iteration 136/1000 | Loss: 0.00001798
Iteration 137/1000 | Loss: 0.00001798
Iteration 138/1000 | Loss: 0.00001798
Iteration 139/1000 | Loss: 0.00001798
Iteration 140/1000 | Loss: 0.00001798
Iteration 141/1000 | Loss: 0.00001798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.7977330571739003e-05, 1.7977330571739003e-05, 1.7977330571739003e-05, 1.7977330571739003e-05, 1.7977330571739003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7977330571739003e-05

Optimization complete. Final v2v error: 3.4554343223571777 mm

Highest mean error: 4.453593730926514 mm for frame 15

Lowest mean error: 2.789761543273926 mm for frame 38

Saving results

Total time: 50.77284836769104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007799
Iteration 2/25 | Loss: 0.00192397
Iteration 3/25 | Loss: 0.00142095
Iteration 4/25 | Loss: 0.00135727
Iteration 5/25 | Loss: 0.00132867
Iteration 6/25 | Loss: 0.00132350
Iteration 7/25 | Loss: 0.00131000
Iteration 8/25 | Loss: 0.00128587
Iteration 9/25 | Loss: 0.00126780
Iteration 10/25 | Loss: 0.00126156
Iteration 11/25 | Loss: 0.00125881
Iteration 12/25 | Loss: 0.00125466
Iteration 13/25 | Loss: 0.00124916
Iteration 14/25 | Loss: 0.00125460
Iteration 15/25 | Loss: 0.00124858
Iteration 16/25 | Loss: 0.00125156
Iteration 17/25 | Loss: 0.00124870
Iteration 18/25 | Loss: 0.00124864
Iteration 19/25 | Loss: 0.00125042
Iteration 20/25 | Loss: 0.00124809
Iteration 21/25 | Loss: 0.00124874
Iteration 22/25 | Loss: 0.00124298
Iteration 23/25 | Loss: 0.00124510
Iteration 24/25 | Loss: 0.00124469
Iteration 25/25 | Loss: 0.00124254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24258626
Iteration 2/25 | Loss: 0.00283265
Iteration 3/25 | Loss: 0.00259759
Iteration 4/25 | Loss: 0.00259759
Iteration 5/25 | Loss: 0.00259759
Iteration 6/25 | Loss: 0.00259759
Iteration 7/25 | Loss: 0.00259759
Iteration 8/25 | Loss: 0.00259759
Iteration 9/25 | Loss: 0.00259759
Iteration 10/25 | Loss: 0.00259759
Iteration 11/25 | Loss: 0.00259759
Iteration 12/25 | Loss: 0.00259759
Iteration 13/25 | Loss: 0.00259759
Iteration 14/25 | Loss: 0.00259759
Iteration 15/25 | Loss: 0.00259759
Iteration 16/25 | Loss: 0.00259759
Iteration 17/25 | Loss: 0.00259759
Iteration 18/25 | Loss: 0.00259759
Iteration 19/25 | Loss: 0.00259759
Iteration 20/25 | Loss: 0.00259759
Iteration 21/25 | Loss: 0.00259759
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0025975853204727173, 0.0025975853204727173, 0.0025975853204727173, 0.0025975853204727173, 0.0025975853204727173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025975853204727173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259759
Iteration 2/1000 | Loss: 0.00104192
Iteration 3/1000 | Loss: 0.00128491
Iteration 4/1000 | Loss: 0.00012244
Iteration 5/1000 | Loss: 0.00011137
Iteration 6/1000 | Loss: 0.00007522
Iteration 7/1000 | Loss: 0.00011158
Iteration 8/1000 | Loss: 0.00008773
Iteration 9/1000 | Loss: 0.00106349
Iteration 10/1000 | Loss: 0.00119015
Iteration 11/1000 | Loss: 0.00017662
Iteration 12/1000 | Loss: 0.00017415
Iteration 13/1000 | Loss: 0.00044428
Iteration 14/1000 | Loss: 0.00079683
Iteration 15/1000 | Loss: 0.00026750
Iteration 16/1000 | Loss: 0.00006023
Iteration 17/1000 | Loss: 0.00150933
Iteration 18/1000 | Loss: 0.00118588
Iteration 19/1000 | Loss: 0.00079733
Iteration 20/1000 | Loss: 0.00055588
Iteration 21/1000 | Loss: 0.00011834
Iteration 22/1000 | Loss: 0.00007148
Iteration 23/1000 | Loss: 0.00077076
Iteration 24/1000 | Loss: 0.00020994
Iteration 25/1000 | Loss: 0.00037121
Iteration 26/1000 | Loss: 0.00012211
Iteration 27/1000 | Loss: 0.00014504
Iteration 28/1000 | Loss: 0.00038421
Iteration 29/1000 | Loss: 0.00051400
Iteration 30/1000 | Loss: 0.00093822
Iteration 31/1000 | Loss: 0.00019456
Iteration 32/1000 | Loss: 0.00079370
Iteration 33/1000 | Loss: 0.00034239
Iteration 34/1000 | Loss: 0.00023700
Iteration 35/1000 | Loss: 0.00026682
Iteration 36/1000 | Loss: 0.00060115
Iteration 37/1000 | Loss: 0.00006852
Iteration 38/1000 | Loss: 0.00008930
Iteration 39/1000 | Loss: 0.00004810
Iteration 40/1000 | Loss: 0.00007739
Iteration 41/1000 | Loss: 0.00006030
Iteration 42/1000 | Loss: 0.00004965
Iteration 43/1000 | Loss: 0.00005734
Iteration 44/1000 | Loss: 0.00003516
Iteration 45/1000 | Loss: 0.00004522
Iteration 46/1000 | Loss: 0.00004063
Iteration 47/1000 | Loss: 0.00004225
Iteration 48/1000 | Loss: 0.00004941
Iteration 49/1000 | Loss: 0.00004180
Iteration 50/1000 | Loss: 0.00004051
Iteration 51/1000 | Loss: 0.00004015
Iteration 52/1000 | Loss: 0.00004290
Iteration 53/1000 | Loss: 0.00003364
Iteration 54/1000 | Loss: 0.00007595
Iteration 55/1000 | Loss: 0.00035240
Iteration 56/1000 | Loss: 0.00006055
Iteration 57/1000 | Loss: 0.00003576
Iteration 58/1000 | Loss: 0.00004413
Iteration 59/1000 | Loss: 0.00023226
Iteration 60/1000 | Loss: 0.00010798
Iteration 61/1000 | Loss: 0.00002954
Iteration 62/1000 | Loss: 0.00014669
Iteration 63/1000 | Loss: 0.00002977
Iteration 64/1000 | Loss: 0.00021152
Iteration 65/1000 | Loss: 0.00010641
Iteration 66/1000 | Loss: 0.00014049
Iteration 67/1000 | Loss: 0.00019997
Iteration 68/1000 | Loss: 0.00011471
Iteration 69/1000 | Loss: 0.00005814
Iteration 70/1000 | Loss: 0.00015809
Iteration 71/1000 | Loss: 0.00017258
Iteration 72/1000 | Loss: 0.00013786
Iteration 73/1000 | Loss: 0.00014898
Iteration 74/1000 | Loss: 0.00003080
Iteration 75/1000 | Loss: 0.00013852
Iteration 76/1000 | Loss: 0.00020515
Iteration 77/1000 | Loss: 0.00037785
Iteration 78/1000 | Loss: 0.00004970
Iteration 79/1000 | Loss: 0.00006574
Iteration 80/1000 | Loss: 0.00003176
Iteration 81/1000 | Loss: 0.00003237
Iteration 82/1000 | Loss: 0.00011264
Iteration 83/1000 | Loss: 0.00014274
Iteration 84/1000 | Loss: 0.00003774
Iteration 85/1000 | Loss: 0.00002659
Iteration 86/1000 | Loss: 0.00003148
Iteration 87/1000 | Loss: 0.00002786
Iteration 88/1000 | Loss: 0.00002471
Iteration 89/1000 | Loss: 0.00002436
Iteration 90/1000 | Loss: 0.00002434
Iteration 91/1000 | Loss: 0.00002408
Iteration 92/1000 | Loss: 0.00004627
Iteration 93/1000 | Loss: 0.00005374
Iteration 94/1000 | Loss: 0.00002465
Iteration 95/1000 | Loss: 0.00002351
Iteration 96/1000 | Loss: 0.00002350
Iteration 97/1000 | Loss: 0.00004835
Iteration 98/1000 | Loss: 0.00002927
Iteration 99/1000 | Loss: 0.00002323
Iteration 100/1000 | Loss: 0.00002318
Iteration 101/1000 | Loss: 0.00002318
Iteration 102/1000 | Loss: 0.00002317
Iteration 103/1000 | Loss: 0.00002316
Iteration 104/1000 | Loss: 0.00002811
Iteration 105/1000 | Loss: 0.00008245
Iteration 106/1000 | Loss: 0.00003729
Iteration 107/1000 | Loss: 0.00005335
Iteration 108/1000 | Loss: 0.00002389
Iteration 109/1000 | Loss: 0.00003469
Iteration 110/1000 | Loss: 0.00002671
Iteration 111/1000 | Loss: 0.00003300
Iteration 112/1000 | Loss: 0.00002517
Iteration 113/1000 | Loss: 0.00002368
Iteration 114/1000 | Loss: 0.00039321
Iteration 115/1000 | Loss: 0.00004224
Iteration 116/1000 | Loss: 0.00002601
Iteration 117/1000 | Loss: 0.00002423
Iteration 118/1000 | Loss: 0.00006125
Iteration 119/1000 | Loss: 0.00003195
Iteration 120/1000 | Loss: 0.00010911
Iteration 121/1000 | Loss: 0.00002217
Iteration 122/1000 | Loss: 0.00002347
Iteration 123/1000 | Loss: 0.00003215
Iteration 124/1000 | Loss: 0.00002125
Iteration 125/1000 | Loss: 0.00002108
Iteration 126/1000 | Loss: 0.00002136
Iteration 127/1000 | Loss: 0.00007725
Iteration 128/1000 | Loss: 0.00002095
Iteration 129/1000 | Loss: 0.00002091
Iteration 130/1000 | Loss: 0.00002089
Iteration 131/1000 | Loss: 0.00002089
Iteration 132/1000 | Loss: 0.00002214
Iteration 133/1000 | Loss: 0.00002082
Iteration 134/1000 | Loss: 0.00002102
Iteration 135/1000 | Loss: 0.00002078
Iteration 136/1000 | Loss: 0.00002079
Iteration 137/1000 | Loss: 0.00002079
Iteration 138/1000 | Loss: 0.00002076
Iteration 139/1000 | Loss: 0.00002076
Iteration 140/1000 | Loss: 0.00002075
Iteration 141/1000 | Loss: 0.00002075
Iteration 142/1000 | Loss: 0.00002075
Iteration 143/1000 | Loss: 0.00002075
Iteration 144/1000 | Loss: 0.00002075
Iteration 145/1000 | Loss: 0.00002074
Iteration 146/1000 | Loss: 0.00002073
Iteration 147/1000 | Loss: 0.00002072
Iteration 148/1000 | Loss: 0.00002067
Iteration 149/1000 | Loss: 0.00002067
Iteration 150/1000 | Loss: 0.00002067
Iteration 151/1000 | Loss: 0.00002067
Iteration 152/1000 | Loss: 0.00002067
Iteration 153/1000 | Loss: 0.00002067
Iteration 154/1000 | Loss: 0.00002629
Iteration 155/1000 | Loss: 0.00002067
Iteration 156/1000 | Loss: 0.00002066
Iteration 157/1000 | Loss: 0.00002066
Iteration 158/1000 | Loss: 0.00002066
Iteration 159/1000 | Loss: 0.00002066
Iteration 160/1000 | Loss: 0.00002066
Iteration 161/1000 | Loss: 0.00002065
Iteration 162/1000 | Loss: 0.00002062
Iteration 163/1000 | Loss: 0.00002061
Iteration 164/1000 | Loss: 0.00002060
Iteration 165/1000 | Loss: 0.00003112
Iteration 166/1000 | Loss: 0.00005469
Iteration 167/1000 | Loss: 0.00002319
Iteration 168/1000 | Loss: 0.00002051
Iteration 169/1000 | Loss: 0.00002051
Iteration 170/1000 | Loss: 0.00002051
Iteration 171/1000 | Loss: 0.00002050
Iteration 172/1000 | Loss: 0.00002050
Iteration 173/1000 | Loss: 0.00002050
Iteration 174/1000 | Loss: 0.00002050
Iteration 175/1000 | Loss: 0.00002050
Iteration 176/1000 | Loss: 0.00002050
Iteration 177/1000 | Loss: 0.00002050
Iteration 178/1000 | Loss: 0.00002050
Iteration 179/1000 | Loss: 0.00002050
Iteration 180/1000 | Loss: 0.00002050
Iteration 181/1000 | Loss: 0.00002050
Iteration 182/1000 | Loss: 0.00002050
Iteration 183/1000 | Loss: 0.00002049
Iteration 184/1000 | Loss: 0.00002049
Iteration 185/1000 | Loss: 0.00002048
Iteration 186/1000 | Loss: 0.00002048
Iteration 187/1000 | Loss: 0.00002048
Iteration 188/1000 | Loss: 0.00002048
Iteration 189/1000 | Loss: 0.00002048
Iteration 190/1000 | Loss: 0.00002048
Iteration 191/1000 | Loss: 0.00002047
Iteration 192/1000 | Loss: 0.00002047
Iteration 193/1000 | Loss: 0.00002047
Iteration 194/1000 | Loss: 0.00002046
Iteration 195/1000 | Loss: 0.00002046
Iteration 196/1000 | Loss: 0.00002046
Iteration 197/1000 | Loss: 0.00002046
Iteration 198/1000 | Loss: 0.00002046
Iteration 199/1000 | Loss: 0.00002046
Iteration 200/1000 | Loss: 0.00002045
Iteration 201/1000 | Loss: 0.00002045
Iteration 202/1000 | Loss: 0.00002371
Iteration 203/1000 | Loss: 0.00002043
Iteration 204/1000 | Loss: 0.00002043
Iteration 205/1000 | Loss: 0.00002043
Iteration 206/1000 | Loss: 0.00002043
Iteration 207/1000 | Loss: 0.00002043
Iteration 208/1000 | Loss: 0.00002043
Iteration 209/1000 | Loss: 0.00002043
Iteration 210/1000 | Loss: 0.00002043
Iteration 211/1000 | Loss: 0.00002043
Iteration 212/1000 | Loss: 0.00002043
Iteration 213/1000 | Loss: 0.00002043
Iteration 214/1000 | Loss: 0.00002043
Iteration 215/1000 | Loss: 0.00002042
Iteration 216/1000 | Loss: 0.00002042
Iteration 217/1000 | Loss: 0.00002042
Iteration 218/1000 | Loss: 0.00002042
Iteration 219/1000 | Loss: 0.00002042
Iteration 220/1000 | Loss: 0.00002042
Iteration 221/1000 | Loss: 0.00002042
Iteration 222/1000 | Loss: 0.00002042
Iteration 223/1000 | Loss: 0.00002042
Iteration 224/1000 | Loss: 0.00002042
Iteration 225/1000 | Loss: 0.00002041
Iteration 226/1000 | Loss: 0.00002041
Iteration 227/1000 | Loss: 0.00002041
Iteration 228/1000 | Loss: 0.00002040
Iteration 229/1000 | Loss: 0.00002040
Iteration 230/1000 | Loss: 0.00004186
Iteration 231/1000 | Loss: 0.00002041
Iteration 232/1000 | Loss: 0.00002040
Iteration 233/1000 | Loss: 0.00002040
Iteration 234/1000 | Loss: 0.00002040
Iteration 235/1000 | Loss: 0.00002040
Iteration 236/1000 | Loss: 0.00002040
Iteration 237/1000 | Loss: 0.00002040
Iteration 238/1000 | Loss: 0.00002040
Iteration 239/1000 | Loss: 0.00002039
Iteration 240/1000 | Loss: 0.00002039
Iteration 241/1000 | Loss: 0.00002039
Iteration 242/1000 | Loss: 0.00002039
Iteration 243/1000 | Loss: 0.00002039
Iteration 244/1000 | Loss: 0.00002039
Iteration 245/1000 | Loss: 0.00002038
Iteration 246/1000 | Loss: 0.00002038
Iteration 247/1000 | Loss: 0.00002038
Iteration 248/1000 | Loss: 0.00002038
Iteration 249/1000 | Loss: 0.00007537
Iteration 250/1000 | Loss: 0.00002739
Iteration 251/1000 | Loss: 0.00002585
Iteration 252/1000 | Loss: 0.00002648
Iteration 253/1000 | Loss: 0.00003542
Iteration 254/1000 | Loss: 0.00008992
Iteration 255/1000 | Loss: 0.00009352
Iteration 256/1000 | Loss: 0.00002906
Iteration 257/1000 | Loss: 0.00006329
Iteration 258/1000 | Loss: 0.00006394
Iteration 259/1000 | Loss: 0.00003055
Iteration 260/1000 | Loss: 0.00002046
Iteration 261/1000 | Loss: 0.00002157
Iteration 262/1000 | Loss: 0.00002129
Iteration 263/1000 | Loss: 0.00002057
Iteration 264/1000 | Loss: 0.00002103
Iteration 265/1000 | Loss: 0.00002103
Iteration 266/1000 | Loss: 0.00008568
Iteration 267/1000 | Loss: 0.00002568
Iteration 268/1000 | Loss: 0.00002618
Iteration 269/1000 | Loss: 0.00002371
Iteration 270/1000 | Loss: 0.00002298
Iteration 271/1000 | Loss: 0.00002193
Iteration 272/1000 | Loss: 0.00002180
Iteration 273/1000 | Loss: 0.00002104
Iteration 274/1000 | Loss: 0.00003166
Iteration 275/1000 | Loss: 0.00002065
Iteration 276/1000 | Loss: 0.00002334
Iteration 277/1000 | Loss: 0.00002079
Iteration 278/1000 | Loss: 0.00002082
Iteration 279/1000 | Loss: 0.00002072
Iteration 280/1000 | Loss: 0.00002029
Iteration 281/1000 | Loss: 0.00002029
Iteration 282/1000 | Loss: 0.00002028
Iteration 283/1000 | Loss: 0.00002028
Iteration 284/1000 | Loss: 0.00002028
Iteration 285/1000 | Loss: 0.00002028
Iteration 286/1000 | Loss: 0.00002028
Iteration 287/1000 | Loss: 0.00002028
Iteration 288/1000 | Loss: 0.00002028
Iteration 289/1000 | Loss: 0.00002028
Iteration 290/1000 | Loss: 0.00002027
Iteration 291/1000 | Loss: 0.00002027
Iteration 292/1000 | Loss: 0.00002027
Iteration 293/1000 | Loss: 0.00002027
Iteration 294/1000 | Loss: 0.00002027
Iteration 295/1000 | Loss: 0.00002027
Iteration 296/1000 | Loss: 0.00002027
Iteration 297/1000 | Loss: 0.00002027
Iteration 298/1000 | Loss: 0.00002027
Iteration 299/1000 | Loss: 0.00002027
Iteration 300/1000 | Loss: 0.00002027
Iteration 301/1000 | Loss: 0.00002026
Iteration 302/1000 | Loss: 0.00002026
Iteration 303/1000 | Loss: 0.00002026
Iteration 304/1000 | Loss: 0.00002034
Iteration 305/1000 | Loss: 0.00002026
Iteration 306/1000 | Loss: 0.00002026
Iteration 307/1000 | Loss: 0.00002026
Iteration 308/1000 | Loss: 0.00002026
Iteration 309/1000 | Loss: 0.00002026
Iteration 310/1000 | Loss: 0.00002026
Iteration 311/1000 | Loss: 0.00002062
Iteration 312/1000 | Loss: 0.00002029
Iteration 313/1000 | Loss: 0.00002029
Iteration 314/1000 | Loss: 0.00002026
Iteration 315/1000 | Loss: 0.00002026
Iteration 316/1000 | Loss: 0.00002026
Iteration 317/1000 | Loss: 0.00002026
Iteration 318/1000 | Loss: 0.00002026
Iteration 319/1000 | Loss: 0.00002026
Iteration 320/1000 | Loss: 0.00002026
Iteration 321/1000 | Loss: 0.00002026
Iteration 322/1000 | Loss: 0.00002026
Iteration 323/1000 | Loss: 0.00002026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 323. Stopping optimization.
Last 5 losses: [2.0256577045074664e-05, 2.0256577045074664e-05, 2.0256577045074664e-05, 2.0256577045074664e-05, 2.0256577045074664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0256577045074664e-05

Optimization complete. Final v2v error: 2.9225540161132812 mm

Highest mean error: 12.06297492980957 mm for frame 4

Lowest mean error: 2.4648568630218506 mm for frame 58

Saving results

Total time: 312.36824774742126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00595969
Iteration 2/25 | Loss: 0.00173071
Iteration 3/25 | Loss: 0.00130339
Iteration 4/25 | Loss: 0.00128357
Iteration 5/25 | Loss: 0.00128002
Iteration 6/25 | Loss: 0.00127859
Iteration 7/25 | Loss: 0.00127837
Iteration 8/25 | Loss: 0.00127837
Iteration 9/25 | Loss: 0.00127837
Iteration 10/25 | Loss: 0.00127837
Iteration 11/25 | Loss: 0.00127837
Iteration 12/25 | Loss: 0.00127837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001278372248634696, 0.001278372248634696, 0.001278372248634696, 0.001278372248634696, 0.001278372248634696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001278372248634696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99229240
Iteration 2/25 | Loss: 0.00149375
Iteration 3/25 | Loss: 0.00149374
Iteration 4/25 | Loss: 0.00149374
Iteration 5/25 | Loss: 0.00149374
Iteration 6/25 | Loss: 0.00149374
Iteration 7/25 | Loss: 0.00149374
Iteration 8/25 | Loss: 0.00149374
Iteration 9/25 | Loss: 0.00149374
Iteration 10/25 | Loss: 0.00149374
Iteration 11/25 | Loss: 0.00149374
Iteration 12/25 | Loss: 0.00149374
Iteration 13/25 | Loss: 0.00149374
Iteration 14/25 | Loss: 0.00149374
Iteration 15/25 | Loss: 0.00149374
Iteration 16/25 | Loss: 0.00149374
Iteration 17/25 | Loss: 0.00149374
Iteration 18/25 | Loss: 0.00149374
Iteration 19/25 | Loss: 0.00149374
Iteration 20/25 | Loss: 0.00149374
Iteration 21/25 | Loss: 0.00149374
Iteration 22/25 | Loss: 0.00149374
Iteration 23/25 | Loss: 0.00149374
Iteration 24/25 | Loss: 0.00149374
Iteration 25/25 | Loss: 0.00149374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149374
Iteration 2/1000 | Loss: 0.00007816
Iteration 3/1000 | Loss: 0.00004996
Iteration 4/1000 | Loss: 0.00003981
Iteration 5/1000 | Loss: 0.00003705
Iteration 6/1000 | Loss: 0.00003605
Iteration 7/1000 | Loss: 0.00003528
Iteration 8/1000 | Loss: 0.00003463
Iteration 9/1000 | Loss: 0.00003391
Iteration 10/1000 | Loss: 0.00003331
Iteration 11/1000 | Loss: 0.00003285
Iteration 12/1000 | Loss: 0.00003245
Iteration 13/1000 | Loss: 0.00003211
Iteration 14/1000 | Loss: 0.00003188
Iteration 15/1000 | Loss: 0.00003159
Iteration 16/1000 | Loss: 0.00003132
Iteration 17/1000 | Loss: 0.00003110
Iteration 18/1000 | Loss: 0.00003095
Iteration 19/1000 | Loss: 0.00003078
Iteration 20/1000 | Loss: 0.00003060
Iteration 21/1000 | Loss: 0.00003052
Iteration 22/1000 | Loss: 0.00003046
Iteration 23/1000 | Loss: 0.00003046
Iteration 24/1000 | Loss: 0.00003046
Iteration 25/1000 | Loss: 0.00003046
Iteration 26/1000 | Loss: 0.00003044
Iteration 27/1000 | Loss: 0.00003043
Iteration 28/1000 | Loss: 0.00003042
Iteration 29/1000 | Loss: 0.00003041
Iteration 30/1000 | Loss: 0.00003041
Iteration 31/1000 | Loss: 0.00003038
Iteration 32/1000 | Loss: 0.00003038
Iteration 33/1000 | Loss: 0.00003037
Iteration 34/1000 | Loss: 0.00003037
Iteration 35/1000 | Loss: 0.00003036
Iteration 36/1000 | Loss: 0.00003034
Iteration 37/1000 | Loss: 0.00003034
Iteration 38/1000 | Loss: 0.00003033
Iteration 39/1000 | Loss: 0.00003033
Iteration 40/1000 | Loss: 0.00003032
Iteration 41/1000 | Loss: 0.00003030
Iteration 42/1000 | Loss: 0.00003030
Iteration 43/1000 | Loss: 0.00003030
Iteration 44/1000 | Loss: 0.00003030
Iteration 45/1000 | Loss: 0.00003030
Iteration 46/1000 | Loss: 0.00003030
Iteration 47/1000 | Loss: 0.00003030
Iteration 48/1000 | Loss: 0.00003030
Iteration 49/1000 | Loss: 0.00003030
Iteration 50/1000 | Loss: 0.00003029
Iteration 51/1000 | Loss: 0.00003029
Iteration 52/1000 | Loss: 0.00003029
Iteration 53/1000 | Loss: 0.00003029
Iteration 54/1000 | Loss: 0.00003029
Iteration 55/1000 | Loss: 0.00003029
Iteration 56/1000 | Loss: 0.00003029
Iteration 57/1000 | Loss: 0.00003029
Iteration 58/1000 | Loss: 0.00003029
Iteration 59/1000 | Loss: 0.00003029
Iteration 60/1000 | Loss: 0.00003029
Iteration 61/1000 | Loss: 0.00003029
Iteration 62/1000 | Loss: 0.00003029
Iteration 63/1000 | Loss: 0.00003029
Iteration 64/1000 | Loss: 0.00003028
Iteration 65/1000 | Loss: 0.00003028
Iteration 66/1000 | Loss: 0.00003028
Iteration 67/1000 | Loss: 0.00003028
Iteration 68/1000 | Loss: 0.00003027
Iteration 69/1000 | Loss: 0.00003026
Iteration 70/1000 | Loss: 0.00003026
Iteration 71/1000 | Loss: 0.00003025
Iteration 72/1000 | Loss: 0.00003024
Iteration 73/1000 | Loss: 0.00003024
Iteration 74/1000 | Loss: 0.00003024
Iteration 75/1000 | Loss: 0.00003024
Iteration 76/1000 | Loss: 0.00003024
Iteration 77/1000 | Loss: 0.00003024
Iteration 78/1000 | Loss: 0.00003024
Iteration 79/1000 | Loss: 0.00003024
Iteration 80/1000 | Loss: 0.00003024
Iteration 81/1000 | Loss: 0.00003024
Iteration 82/1000 | Loss: 0.00003024
Iteration 83/1000 | Loss: 0.00003023
Iteration 84/1000 | Loss: 0.00003023
Iteration 85/1000 | Loss: 0.00003023
Iteration 86/1000 | Loss: 0.00003022
Iteration 87/1000 | Loss: 0.00003022
Iteration 88/1000 | Loss: 0.00003022
Iteration 89/1000 | Loss: 0.00003022
Iteration 90/1000 | Loss: 0.00003022
Iteration 91/1000 | Loss: 0.00003022
Iteration 92/1000 | Loss: 0.00003022
Iteration 93/1000 | Loss: 0.00003021
Iteration 94/1000 | Loss: 0.00003021
Iteration 95/1000 | Loss: 0.00003021
Iteration 96/1000 | Loss: 0.00003021
Iteration 97/1000 | Loss: 0.00003021
Iteration 98/1000 | Loss: 0.00003021
Iteration 99/1000 | Loss: 0.00003021
Iteration 100/1000 | Loss: 0.00003020
Iteration 101/1000 | Loss: 0.00003020
Iteration 102/1000 | Loss: 0.00003020
Iteration 103/1000 | Loss: 0.00003020
Iteration 104/1000 | Loss: 0.00003019
Iteration 105/1000 | Loss: 0.00003019
Iteration 106/1000 | Loss: 0.00003019
Iteration 107/1000 | Loss: 0.00003019
Iteration 108/1000 | Loss: 0.00003019
Iteration 109/1000 | Loss: 0.00003019
Iteration 110/1000 | Loss: 0.00003019
Iteration 111/1000 | Loss: 0.00003018
Iteration 112/1000 | Loss: 0.00003018
Iteration 113/1000 | Loss: 0.00003018
Iteration 114/1000 | Loss: 0.00003018
Iteration 115/1000 | Loss: 0.00003018
Iteration 116/1000 | Loss: 0.00003018
Iteration 117/1000 | Loss: 0.00003018
Iteration 118/1000 | Loss: 0.00003018
Iteration 119/1000 | Loss: 0.00003017
Iteration 120/1000 | Loss: 0.00003017
Iteration 121/1000 | Loss: 0.00003017
Iteration 122/1000 | Loss: 0.00003017
Iteration 123/1000 | Loss: 0.00003017
Iteration 124/1000 | Loss: 0.00003017
Iteration 125/1000 | Loss: 0.00003017
Iteration 126/1000 | Loss: 0.00003017
Iteration 127/1000 | Loss: 0.00003017
Iteration 128/1000 | Loss: 0.00003017
Iteration 129/1000 | Loss: 0.00003017
Iteration 130/1000 | Loss: 0.00003017
Iteration 131/1000 | Loss: 0.00003017
Iteration 132/1000 | Loss: 0.00003016
Iteration 133/1000 | Loss: 0.00003016
Iteration 134/1000 | Loss: 0.00003016
Iteration 135/1000 | Loss: 0.00003016
Iteration 136/1000 | Loss: 0.00003016
Iteration 137/1000 | Loss: 0.00003015
Iteration 138/1000 | Loss: 0.00003015
Iteration 139/1000 | Loss: 0.00003015
Iteration 140/1000 | Loss: 0.00003015
Iteration 141/1000 | Loss: 0.00003015
Iteration 142/1000 | Loss: 0.00003015
Iteration 143/1000 | Loss: 0.00003015
Iteration 144/1000 | Loss: 0.00003015
Iteration 145/1000 | Loss: 0.00003015
Iteration 146/1000 | Loss: 0.00003015
Iteration 147/1000 | Loss: 0.00003014
Iteration 148/1000 | Loss: 0.00003014
Iteration 149/1000 | Loss: 0.00003014
Iteration 150/1000 | Loss: 0.00003014
Iteration 151/1000 | Loss: 0.00003014
Iteration 152/1000 | Loss: 0.00003014
Iteration 153/1000 | Loss: 0.00003014
Iteration 154/1000 | Loss: 0.00003014
Iteration 155/1000 | Loss: 0.00003013
Iteration 156/1000 | Loss: 0.00003013
Iteration 157/1000 | Loss: 0.00003013
Iteration 158/1000 | Loss: 0.00003013
Iteration 159/1000 | Loss: 0.00003013
Iteration 160/1000 | Loss: 0.00003013
Iteration 161/1000 | Loss: 0.00003013
Iteration 162/1000 | Loss: 0.00003013
Iteration 163/1000 | Loss: 0.00003013
Iteration 164/1000 | Loss: 0.00003013
Iteration 165/1000 | Loss: 0.00003013
Iteration 166/1000 | Loss: 0.00003013
Iteration 167/1000 | Loss: 0.00003013
Iteration 168/1000 | Loss: 0.00003013
Iteration 169/1000 | Loss: 0.00003013
Iteration 170/1000 | Loss: 0.00003013
Iteration 171/1000 | Loss: 0.00003013
Iteration 172/1000 | Loss: 0.00003013
Iteration 173/1000 | Loss: 0.00003013
Iteration 174/1000 | Loss: 0.00003012
Iteration 175/1000 | Loss: 0.00003012
Iteration 176/1000 | Loss: 0.00003012
Iteration 177/1000 | Loss: 0.00003012
Iteration 178/1000 | Loss: 0.00003012
Iteration 179/1000 | Loss: 0.00003012
Iteration 180/1000 | Loss: 0.00003012
Iteration 181/1000 | Loss: 0.00003012
Iteration 182/1000 | Loss: 0.00003012
Iteration 183/1000 | Loss: 0.00003012
Iteration 184/1000 | Loss: 0.00003012
Iteration 185/1000 | Loss: 0.00003012
Iteration 186/1000 | Loss: 0.00003012
Iteration 187/1000 | Loss: 0.00003012
Iteration 188/1000 | Loss: 0.00003012
Iteration 189/1000 | Loss: 0.00003012
Iteration 190/1000 | Loss: 0.00003012
Iteration 191/1000 | Loss: 0.00003012
Iteration 192/1000 | Loss: 0.00003012
Iteration 193/1000 | Loss: 0.00003012
Iteration 194/1000 | Loss: 0.00003012
Iteration 195/1000 | Loss: 0.00003012
Iteration 196/1000 | Loss: 0.00003012
Iteration 197/1000 | Loss: 0.00003012
Iteration 198/1000 | Loss: 0.00003012
Iteration 199/1000 | Loss: 0.00003012
Iteration 200/1000 | Loss: 0.00003012
Iteration 201/1000 | Loss: 0.00003012
Iteration 202/1000 | Loss: 0.00003012
Iteration 203/1000 | Loss: 0.00003012
Iteration 204/1000 | Loss: 0.00003012
Iteration 205/1000 | Loss: 0.00003012
Iteration 206/1000 | Loss: 0.00003012
Iteration 207/1000 | Loss: 0.00003012
Iteration 208/1000 | Loss: 0.00003012
Iteration 209/1000 | Loss: 0.00003012
Iteration 210/1000 | Loss: 0.00003012
Iteration 211/1000 | Loss: 0.00003012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [3.01159070659196e-05, 3.01159070659196e-05, 3.01159070659196e-05, 3.01159070659196e-05, 3.01159070659196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.01159070659196e-05

Optimization complete. Final v2v error: 4.098765850067139 mm

Highest mean error: 5.235093116760254 mm for frame 130

Lowest mean error: 2.98923659324646 mm for frame 48

Saving results

Total time: 53.42540001869202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869822
Iteration 2/25 | Loss: 0.00141819
Iteration 3/25 | Loss: 0.00122890
Iteration 4/25 | Loss: 0.00121551
Iteration 5/25 | Loss: 0.00121260
Iteration 6/25 | Loss: 0.00121260
Iteration 7/25 | Loss: 0.00121260
Iteration 8/25 | Loss: 0.00121260
Iteration 9/25 | Loss: 0.00121260
Iteration 10/25 | Loss: 0.00121260
Iteration 11/25 | Loss: 0.00121260
Iteration 12/25 | Loss: 0.00121260
Iteration 13/25 | Loss: 0.00121260
Iteration 14/25 | Loss: 0.00121260
Iteration 15/25 | Loss: 0.00121260
Iteration 16/25 | Loss: 0.00121260
Iteration 17/25 | Loss: 0.00121260
Iteration 18/25 | Loss: 0.00121260
Iteration 19/25 | Loss: 0.00121260
Iteration 20/25 | Loss: 0.00121260
Iteration 21/25 | Loss: 0.00121260
Iteration 22/25 | Loss: 0.00121260
Iteration 23/25 | Loss: 0.00121260
Iteration 24/25 | Loss: 0.00121260
Iteration 25/25 | Loss: 0.00121260

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21758747
Iteration 2/25 | Loss: 0.00176050
Iteration 3/25 | Loss: 0.00176048
Iteration 4/25 | Loss: 0.00176047
Iteration 5/25 | Loss: 0.00176047
Iteration 6/25 | Loss: 0.00176047
Iteration 7/25 | Loss: 0.00176047
Iteration 8/25 | Loss: 0.00176047
Iteration 9/25 | Loss: 0.00176047
Iteration 10/25 | Loss: 0.00176047
Iteration 11/25 | Loss: 0.00176047
Iteration 12/25 | Loss: 0.00176047
Iteration 13/25 | Loss: 0.00176047
Iteration 14/25 | Loss: 0.00176047
Iteration 15/25 | Loss: 0.00176047
Iteration 16/25 | Loss: 0.00176047
Iteration 17/25 | Loss: 0.00176047
Iteration 18/25 | Loss: 0.00176047
Iteration 19/25 | Loss: 0.00176047
Iteration 20/25 | Loss: 0.00176047
Iteration 21/25 | Loss: 0.00176047
Iteration 22/25 | Loss: 0.00176047
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0017604721942916512, 0.0017604721942916512, 0.0017604721942916512, 0.0017604721942916512, 0.0017604721942916512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017604721942916512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176047
Iteration 2/1000 | Loss: 0.00002429
Iteration 3/1000 | Loss: 0.00001679
Iteration 4/1000 | Loss: 0.00001555
Iteration 5/1000 | Loss: 0.00001501
Iteration 6/1000 | Loss: 0.00001450
Iteration 7/1000 | Loss: 0.00001416
Iteration 8/1000 | Loss: 0.00001399
Iteration 9/1000 | Loss: 0.00001391
Iteration 10/1000 | Loss: 0.00001387
Iteration 11/1000 | Loss: 0.00001373
Iteration 12/1000 | Loss: 0.00001355
Iteration 13/1000 | Loss: 0.00001348
Iteration 14/1000 | Loss: 0.00001347
Iteration 15/1000 | Loss: 0.00001345
Iteration 16/1000 | Loss: 0.00001341
Iteration 17/1000 | Loss: 0.00001340
Iteration 18/1000 | Loss: 0.00001340
Iteration 19/1000 | Loss: 0.00001339
Iteration 20/1000 | Loss: 0.00001339
Iteration 21/1000 | Loss: 0.00001338
Iteration 22/1000 | Loss: 0.00001337
Iteration 23/1000 | Loss: 0.00001337
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001332
Iteration 26/1000 | Loss: 0.00001328
Iteration 27/1000 | Loss: 0.00001328
Iteration 28/1000 | Loss: 0.00001321
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001317
Iteration 31/1000 | Loss: 0.00001315
Iteration 32/1000 | Loss: 0.00001310
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001307
Iteration 35/1000 | Loss: 0.00001305
Iteration 36/1000 | Loss: 0.00001304
Iteration 37/1000 | Loss: 0.00001304
Iteration 38/1000 | Loss: 0.00001304
Iteration 39/1000 | Loss: 0.00001304
Iteration 40/1000 | Loss: 0.00001303
Iteration 41/1000 | Loss: 0.00001302
Iteration 42/1000 | Loss: 0.00001301
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001300
Iteration 45/1000 | Loss: 0.00001300
Iteration 46/1000 | Loss: 0.00001300
Iteration 47/1000 | Loss: 0.00001299
Iteration 48/1000 | Loss: 0.00001299
Iteration 49/1000 | Loss: 0.00001299
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001298
Iteration 54/1000 | Loss: 0.00001298
Iteration 55/1000 | Loss: 0.00001298
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001297
Iteration 58/1000 | Loss: 0.00001297
Iteration 59/1000 | Loss: 0.00001297
Iteration 60/1000 | Loss: 0.00001297
Iteration 61/1000 | Loss: 0.00001297
Iteration 62/1000 | Loss: 0.00001297
Iteration 63/1000 | Loss: 0.00001296
Iteration 64/1000 | Loss: 0.00001296
Iteration 65/1000 | Loss: 0.00001296
Iteration 66/1000 | Loss: 0.00001296
Iteration 67/1000 | Loss: 0.00001296
Iteration 68/1000 | Loss: 0.00001296
Iteration 69/1000 | Loss: 0.00001295
Iteration 70/1000 | Loss: 0.00001295
Iteration 71/1000 | Loss: 0.00001295
Iteration 72/1000 | Loss: 0.00001295
Iteration 73/1000 | Loss: 0.00001295
Iteration 74/1000 | Loss: 0.00001295
Iteration 75/1000 | Loss: 0.00001295
Iteration 76/1000 | Loss: 0.00001295
Iteration 77/1000 | Loss: 0.00001295
Iteration 78/1000 | Loss: 0.00001295
Iteration 79/1000 | Loss: 0.00001295
Iteration 80/1000 | Loss: 0.00001295
Iteration 81/1000 | Loss: 0.00001295
Iteration 82/1000 | Loss: 0.00001295
Iteration 83/1000 | Loss: 0.00001295
Iteration 84/1000 | Loss: 0.00001295
Iteration 85/1000 | Loss: 0.00001295
Iteration 86/1000 | Loss: 0.00001294
Iteration 87/1000 | Loss: 0.00001294
Iteration 88/1000 | Loss: 0.00001294
Iteration 89/1000 | Loss: 0.00001294
Iteration 90/1000 | Loss: 0.00001294
Iteration 91/1000 | Loss: 0.00001294
Iteration 92/1000 | Loss: 0.00001294
Iteration 93/1000 | Loss: 0.00001294
Iteration 94/1000 | Loss: 0.00001294
Iteration 95/1000 | Loss: 0.00001294
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001294
Iteration 101/1000 | Loss: 0.00001294
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001293
Iteration 106/1000 | Loss: 0.00001293
Iteration 107/1000 | Loss: 0.00001293
Iteration 108/1000 | Loss: 0.00001293
Iteration 109/1000 | Loss: 0.00001293
Iteration 110/1000 | Loss: 0.00001293
Iteration 111/1000 | Loss: 0.00001293
Iteration 112/1000 | Loss: 0.00001293
Iteration 113/1000 | Loss: 0.00001293
Iteration 114/1000 | Loss: 0.00001293
Iteration 115/1000 | Loss: 0.00001293
Iteration 116/1000 | Loss: 0.00001293
Iteration 117/1000 | Loss: 0.00001293
Iteration 118/1000 | Loss: 0.00001293
Iteration 119/1000 | Loss: 0.00001293
Iteration 120/1000 | Loss: 0.00001293
Iteration 121/1000 | Loss: 0.00001293
Iteration 122/1000 | Loss: 0.00001293
Iteration 123/1000 | Loss: 0.00001293
Iteration 124/1000 | Loss: 0.00001293
Iteration 125/1000 | Loss: 0.00001293
Iteration 126/1000 | Loss: 0.00001293
Iteration 127/1000 | Loss: 0.00001293
Iteration 128/1000 | Loss: 0.00001293
Iteration 129/1000 | Loss: 0.00001292
Iteration 130/1000 | Loss: 0.00001292
Iteration 131/1000 | Loss: 0.00001292
Iteration 132/1000 | Loss: 0.00001292
Iteration 133/1000 | Loss: 0.00001292
Iteration 134/1000 | Loss: 0.00001292
Iteration 135/1000 | Loss: 0.00001292
Iteration 136/1000 | Loss: 0.00001292
Iteration 137/1000 | Loss: 0.00001292
Iteration 138/1000 | Loss: 0.00001291
Iteration 139/1000 | Loss: 0.00001291
Iteration 140/1000 | Loss: 0.00001291
Iteration 141/1000 | Loss: 0.00001291
Iteration 142/1000 | Loss: 0.00001291
Iteration 143/1000 | Loss: 0.00001291
Iteration 144/1000 | Loss: 0.00001291
Iteration 145/1000 | Loss: 0.00001291
Iteration 146/1000 | Loss: 0.00001291
Iteration 147/1000 | Loss: 0.00001291
Iteration 148/1000 | Loss: 0.00001291
Iteration 149/1000 | Loss: 0.00001291
Iteration 150/1000 | Loss: 0.00001291
Iteration 151/1000 | Loss: 0.00001291
Iteration 152/1000 | Loss: 0.00001291
Iteration 153/1000 | Loss: 0.00001291
Iteration 154/1000 | Loss: 0.00001291
Iteration 155/1000 | Loss: 0.00001291
Iteration 156/1000 | Loss: 0.00001291
Iteration 157/1000 | Loss: 0.00001291
Iteration 158/1000 | Loss: 0.00001291
Iteration 159/1000 | Loss: 0.00001291
Iteration 160/1000 | Loss: 0.00001291
Iteration 161/1000 | Loss: 0.00001291
Iteration 162/1000 | Loss: 0.00001291
Iteration 163/1000 | Loss: 0.00001291
Iteration 164/1000 | Loss: 0.00001291
Iteration 165/1000 | Loss: 0.00001291
Iteration 166/1000 | Loss: 0.00001291
Iteration 167/1000 | Loss: 0.00001291
Iteration 168/1000 | Loss: 0.00001291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.290604177484056e-05, 1.290604177484056e-05, 1.290604177484056e-05, 1.290604177484056e-05, 1.290604177484056e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.290604177484056e-05

Optimization complete. Final v2v error: 3.0721170902252197 mm

Highest mean error: 3.1993770599365234 mm for frame 24

Lowest mean error: 2.7365376949310303 mm for frame 0

Saving results

Total time: 36.15144991874695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442279
Iteration 2/25 | Loss: 0.00120264
Iteration 3/25 | Loss: 0.00113635
Iteration 4/25 | Loss: 0.00112539
Iteration 5/25 | Loss: 0.00112188
Iteration 6/25 | Loss: 0.00112083
Iteration 7/25 | Loss: 0.00112083
Iteration 8/25 | Loss: 0.00112083
Iteration 9/25 | Loss: 0.00112083
Iteration 10/25 | Loss: 0.00112083
Iteration 11/25 | Loss: 0.00112083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011208339128643274, 0.0011208339128643274, 0.0011208339128643274, 0.0011208339128643274, 0.0011208339128643274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011208339128643274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39602375
Iteration 2/25 | Loss: 0.00177862
Iteration 3/25 | Loss: 0.00177861
Iteration 4/25 | Loss: 0.00177861
Iteration 5/25 | Loss: 0.00177861
Iteration 6/25 | Loss: 0.00177861
Iteration 7/25 | Loss: 0.00177861
Iteration 8/25 | Loss: 0.00177861
Iteration 9/25 | Loss: 0.00177861
Iteration 10/25 | Loss: 0.00177861
Iteration 11/25 | Loss: 0.00177861
Iteration 12/25 | Loss: 0.00177861
Iteration 13/25 | Loss: 0.00177861
Iteration 14/25 | Loss: 0.00177861
Iteration 15/25 | Loss: 0.00177861
Iteration 16/25 | Loss: 0.00177861
Iteration 17/25 | Loss: 0.00177861
Iteration 18/25 | Loss: 0.00177861
Iteration 19/25 | Loss: 0.00177861
Iteration 20/25 | Loss: 0.00177861
Iteration 21/25 | Loss: 0.00177861
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001778609468601644, 0.001778609468601644, 0.001778609468601644, 0.001778609468601644, 0.001778609468601644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001778609468601644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177861
Iteration 2/1000 | Loss: 0.00002056
Iteration 3/1000 | Loss: 0.00001384
Iteration 4/1000 | Loss: 0.00001168
Iteration 5/1000 | Loss: 0.00001084
Iteration 6/1000 | Loss: 0.00001034
Iteration 7/1000 | Loss: 0.00000975
Iteration 8/1000 | Loss: 0.00000949
Iteration 9/1000 | Loss: 0.00000937
Iteration 10/1000 | Loss: 0.00000911
Iteration 11/1000 | Loss: 0.00000895
Iteration 12/1000 | Loss: 0.00000893
Iteration 13/1000 | Loss: 0.00000888
Iteration 14/1000 | Loss: 0.00000887
Iteration 15/1000 | Loss: 0.00000877
Iteration 16/1000 | Loss: 0.00000876
Iteration 17/1000 | Loss: 0.00000869
Iteration 18/1000 | Loss: 0.00000861
Iteration 19/1000 | Loss: 0.00000860
Iteration 20/1000 | Loss: 0.00000860
Iteration 21/1000 | Loss: 0.00000858
Iteration 22/1000 | Loss: 0.00000857
Iteration 23/1000 | Loss: 0.00000856
Iteration 24/1000 | Loss: 0.00000856
Iteration 25/1000 | Loss: 0.00000854
Iteration 26/1000 | Loss: 0.00000853
Iteration 27/1000 | Loss: 0.00000853
Iteration 28/1000 | Loss: 0.00000852
Iteration 29/1000 | Loss: 0.00000851
Iteration 30/1000 | Loss: 0.00000850
Iteration 31/1000 | Loss: 0.00000849
Iteration 32/1000 | Loss: 0.00000849
Iteration 33/1000 | Loss: 0.00000848
Iteration 34/1000 | Loss: 0.00000848
Iteration 35/1000 | Loss: 0.00000847
Iteration 36/1000 | Loss: 0.00000846
Iteration 37/1000 | Loss: 0.00000846
Iteration 38/1000 | Loss: 0.00000845
Iteration 39/1000 | Loss: 0.00000843
Iteration 40/1000 | Loss: 0.00000842
Iteration 41/1000 | Loss: 0.00000842
Iteration 42/1000 | Loss: 0.00000842
Iteration 43/1000 | Loss: 0.00000842
Iteration 44/1000 | Loss: 0.00000841
Iteration 45/1000 | Loss: 0.00000841
Iteration 46/1000 | Loss: 0.00000841
Iteration 47/1000 | Loss: 0.00000841
Iteration 48/1000 | Loss: 0.00000841
Iteration 49/1000 | Loss: 0.00000841
Iteration 50/1000 | Loss: 0.00000841
Iteration 51/1000 | Loss: 0.00000841
Iteration 52/1000 | Loss: 0.00000841
Iteration 53/1000 | Loss: 0.00000840
Iteration 54/1000 | Loss: 0.00000840
Iteration 55/1000 | Loss: 0.00000840
Iteration 56/1000 | Loss: 0.00000840
Iteration 57/1000 | Loss: 0.00000840
Iteration 58/1000 | Loss: 0.00000839
Iteration 59/1000 | Loss: 0.00000839
Iteration 60/1000 | Loss: 0.00000839
Iteration 61/1000 | Loss: 0.00000838
Iteration 62/1000 | Loss: 0.00000838
Iteration 63/1000 | Loss: 0.00000838
Iteration 64/1000 | Loss: 0.00000838
Iteration 65/1000 | Loss: 0.00000838
Iteration 66/1000 | Loss: 0.00000837
Iteration 67/1000 | Loss: 0.00000837
Iteration 68/1000 | Loss: 0.00000837
Iteration 69/1000 | Loss: 0.00000837
Iteration 70/1000 | Loss: 0.00000837
Iteration 71/1000 | Loss: 0.00000837
Iteration 72/1000 | Loss: 0.00000837
Iteration 73/1000 | Loss: 0.00000837
Iteration 74/1000 | Loss: 0.00000837
Iteration 75/1000 | Loss: 0.00000837
Iteration 76/1000 | Loss: 0.00000837
Iteration 77/1000 | Loss: 0.00000837
Iteration 78/1000 | Loss: 0.00000837
Iteration 79/1000 | Loss: 0.00000836
Iteration 80/1000 | Loss: 0.00000836
Iteration 81/1000 | Loss: 0.00000836
Iteration 82/1000 | Loss: 0.00000835
Iteration 83/1000 | Loss: 0.00000834
Iteration 84/1000 | Loss: 0.00000834
Iteration 85/1000 | Loss: 0.00000834
Iteration 86/1000 | Loss: 0.00000834
Iteration 87/1000 | Loss: 0.00000834
Iteration 88/1000 | Loss: 0.00000834
Iteration 89/1000 | Loss: 0.00000834
Iteration 90/1000 | Loss: 0.00000834
Iteration 91/1000 | Loss: 0.00000834
Iteration 92/1000 | Loss: 0.00000834
Iteration 93/1000 | Loss: 0.00000834
Iteration 94/1000 | Loss: 0.00000834
Iteration 95/1000 | Loss: 0.00000834
Iteration 96/1000 | Loss: 0.00000834
Iteration 97/1000 | Loss: 0.00000833
Iteration 98/1000 | Loss: 0.00000833
Iteration 99/1000 | Loss: 0.00000833
Iteration 100/1000 | Loss: 0.00000832
Iteration 101/1000 | Loss: 0.00000832
Iteration 102/1000 | Loss: 0.00000832
Iteration 103/1000 | Loss: 0.00000832
Iteration 104/1000 | Loss: 0.00000831
Iteration 105/1000 | Loss: 0.00000831
Iteration 106/1000 | Loss: 0.00000831
Iteration 107/1000 | Loss: 0.00000831
Iteration 108/1000 | Loss: 0.00000830
Iteration 109/1000 | Loss: 0.00000830
Iteration 110/1000 | Loss: 0.00000830
Iteration 111/1000 | Loss: 0.00000830
Iteration 112/1000 | Loss: 0.00000830
Iteration 113/1000 | Loss: 0.00000829
Iteration 114/1000 | Loss: 0.00000829
Iteration 115/1000 | Loss: 0.00000829
Iteration 116/1000 | Loss: 0.00000829
Iteration 117/1000 | Loss: 0.00000828
Iteration 118/1000 | Loss: 0.00000828
Iteration 119/1000 | Loss: 0.00000828
Iteration 120/1000 | Loss: 0.00000828
Iteration 121/1000 | Loss: 0.00000828
Iteration 122/1000 | Loss: 0.00000828
Iteration 123/1000 | Loss: 0.00000827
Iteration 124/1000 | Loss: 0.00000827
Iteration 125/1000 | Loss: 0.00000827
Iteration 126/1000 | Loss: 0.00000827
Iteration 127/1000 | Loss: 0.00000827
Iteration 128/1000 | Loss: 0.00000827
Iteration 129/1000 | Loss: 0.00000827
Iteration 130/1000 | Loss: 0.00000827
Iteration 131/1000 | Loss: 0.00000826
Iteration 132/1000 | Loss: 0.00000826
Iteration 133/1000 | Loss: 0.00000826
Iteration 134/1000 | Loss: 0.00000826
Iteration 135/1000 | Loss: 0.00000826
Iteration 136/1000 | Loss: 0.00000826
Iteration 137/1000 | Loss: 0.00000826
Iteration 138/1000 | Loss: 0.00000826
Iteration 139/1000 | Loss: 0.00000825
Iteration 140/1000 | Loss: 0.00000825
Iteration 141/1000 | Loss: 0.00000825
Iteration 142/1000 | Loss: 0.00000825
Iteration 143/1000 | Loss: 0.00000825
Iteration 144/1000 | Loss: 0.00000825
Iteration 145/1000 | Loss: 0.00000825
Iteration 146/1000 | Loss: 0.00000825
Iteration 147/1000 | Loss: 0.00000825
Iteration 148/1000 | Loss: 0.00000825
Iteration 149/1000 | Loss: 0.00000824
Iteration 150/1000 | Loss: 0.00000824
Iteration 151/1000 | Loss: 0.00000824
Iteration 152/1000 | Loss: 0.00000824
Iteration 153/1000 | Loss: 0.00000824
Iteration 154/1000 | Loss: 0.00000824
Iteration 155/1000 | Loss: 0.00000824
Iteration 156/1000 | Loss: 0.00000824
Iteration 157/1000 | Loss: 0.00000824
Iteration 158/1000 | Loss: 0.00000824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [8.244612217822578e-06, 8.244612217822578e-06, 8.244612217822578e-06, 8.244612217822578e-06, 8.244612217822578e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.244612217822578e-06

Optimization complete. Final v2v error: 2.5211946964263916 mm

Highest mean error: 2.6301705837249756 mm for frame 18

Lowest mean error: 2.423078775405884 mm for frame 108

Saving results

Total time: 38.76746416091919
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00618159
Iteration 2/25 | Loss: 0.00144534
Iteration 3/25 | Loss: 0.00132569
Iteration 4/25 | Loss: 0.00131509
Iteration 5/25 | Loss: 0.00131121
Iteration 6/25 | Loss: 0.00131099
Iteration 7/25 | Loss: 0.00131099
Iteration 8/25 | Loss: 0.00131099
Iteration 9/25 | Loss: 0.00131099
Iteration 10/25 | Loss: 0.00131099
Iteration 11/25 | Loss: 0.00131099
Iteration 12/25 | Loss: 0.00131099
Iteration 13/25 | Loss: 0.00131099
Iteration 14/25 | Loss: 0.00131099
Iteration 15/25 | Loss: 0.00131099
Iteration 16/25 | Loss: 0.00131099
Iteration 17/25 | Loss: 0.00131099
Iteration 18/25 | Loss: 0.00131099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013109857682138681, 0.0013109857682138681, 0.0013109857682138681, 0.0013109857682138681, 0.0013109857682138681]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013109857682138681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.60730076
Iteration 2/25 | Loss: 0.00158730
Iteration 3/25 | Loss: 0.00158724
Iteration 4/25 | Loss: 0.00158724
Iteration 5/25 | Loss: 0.00158724
Iteration 6/25 | Loss: 0.00158724
Iteration 7/25 | Loss: 0.00158724
Iteration 8/25 | Loss: 0.00158724
Iteration 9/25 | Loss: 0.00158724
Iteration 10/25 | Loss: 0.00158724
Iteration 11/25 | Loss: 0.00158724
Iteration 12/25 | Loss: 0.00158724
Iteration 13/25 | Loss: 0.00158724
Iteration 14/25 | Loss: 0.00158724
Iteration 15/25 | Loss: 0.00158724
Iteration 16/25 | Loss: 0.00158724
Iteration 17/25 | Loss: 0.00158724
Iteration 18/25 | Loss: 0.00158724
Iteration 19/25 | Loss: 0.00158724
Iteration 20/25 | Loss: 0.00158724
Iteration 21/25 | Loss: 0.00158724
Iteration 22/25 | Loss: 0.00158724
Iteration 23/25 | Loss: 0.00158724
Iteration 24/25 | Loss: 0.00158724
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015872367657721043, 0.0015872367657721043, 0.0015872367657721043, 0.0015872367657721043, 0.0015872367657721043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015872367657721043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158724
Iteration 2/1000 | Loss: 0.00004296
Iteration 3/1000 | Loss: 0.00002719
Iteration 4/1000 | Loss: 0.00002302
Iteration 5/1000 | Loss: 0.00002201
Iteration 6/1000 | Loss: 0.00002127
Iteration 7/1000 | Loss: 0.00002081
Iteration 8/1000 | Loss: 0.00002053
Iteration 9/1000 | Loss: 0.00002019
Iteration 10/1000 | Loss: 0.00002006
Iteration 11/1000 | Loss: 0.00001983
Iteration 12/1000 | Loss: 0.00001961
Iteration 13/1000 | Loss: 0.00001957
Iteration 14/1000 | Loss: 0.00001955
Iteration 15/1000 | Loss: 0.00001955
Iteration 16/1000 | Loss: 0.00001954
Iteration 17/1000 | Loss: 0.00001954
Iteration 18/1000 | Loss: 0.00001953
Iteration 19/1000 | Loss: 0.00001953
Iteration 20/1000 | Loss: 0.00001952
Iteration 21/1000 | Loss: 0.00001951
Iteration 22/1000 | Loss: 0.00001951
Iteration 23/1000 | Loss: 0.00001950
Iteration 24/1000 | Loss: 0.00001949
Iteration 25/1000 | Loss: 0.00001947
Iteration 26/1000 | Loss: 0.00001945
Iteration 27/1000 | Loss: 0.00001940
Iteration 28/1000 | Loss: 0.00001934
Iteration 29/1000 | Loss: 0.00001933
Iteration 30/1000 | Loss: 0.00001933
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001931
Iteration 33/1000 | Loss: 0.00001929
Iteration 34/1000 | Loss: 0.00001924
Iteration 35/1000 | Loss: 0.00001921
Iteration 36/1000 | Loss: 0.00001920
Iteration 37/1000 | Loss: 0.00001919
Iteration 38/1000 | Loss: 0.00001916
Iteration 39/1000 | Loss: 0.00001916
Iteration 40/1000 | Loss: 0.00001916
Iteration 41/1000 | Loss: 0.00001916
Iteration 42/1000 | Loss: 0.00001916
Iteration 43/1000 | Loss: 0.00001916
Iteration 44/1000 | Loss: 0.00001915
Iteration 45/1000 | Loss: 0.00001914
Iteration 46/1000 | Loss: 0.00001913
Iteration 47/1000 | Loss: 0.00001912
Iteration 48/1000 | Loss: 0.00001912
Iteration 49/1000 | Loss: 0.00001912
Iteration 50/1000 | Loss: 0.00001911
Iteration 51/1000 | Loss: 0.00001911
Iteration 52/1000 | Loss: 0.00001911
Iteration 53/1000 | Loss: 0.00001910
Iteration 54/1000 | Loss: 0.00001910
Iteration 55/1000 | Loss: 0.00001909
Iteration 56/1000 | Loss: 0.00001909
Iteration 57/1000 | Loss: 0.00001908
Iteration 58/1000 | Loss: 0.00001908
Iteration 59/1000 | Loss: 0.00001908
Iteration 60/1000 | Loss: 0.00001907
Iteration 61/1000 | Loss: 0.00001907
Iteration 62/1000 | Loss: 0.00001907
Iteration 63/1000 | Loss: 0.00001907
Iteration 64/1000 | Loss: 0.00001905
Iteration 65/1000 | Loss: 0.00001904
Iteration 66/1000 | Loss: 0.00001904
Iteration 67/1000 | Loss: 0.00001903
Iteration 68/1000 | Loss: 0.00001903
Iteration 69/1000 | Loss: 0.00001903
Iteration 70/1000 | Loss: 0.00001903
Iteration 71/1000 | Loss: 0.00001903
Iteration 72/1000 | Loss: 0.00001903
Iteration 73/1000 | Loss: 0.00001903
Iteration 74/1000 | Loss: 0.00001903
Iteration 75/1000 | Loss: 0.00001903
Iteration 76/1000 | Loss: 0.00001902
Iteration 77/1000 | Loss: 0.00001902
Iteration 78/1000 | Loss: 0.00001902
Iteration 79/1000 | Loss: 0.00001902
Iteration 80/1000 | Loss: 0.00001901
Iteration 81/1000 | Loss: 0.00001901
Iteration 82/1000 | Loss: 0.00001901
Iteration 83/1000 | Loss: 0.00001901
Iteration 84/1000 | Loss: 0.00001901
Iteration 85/1000 | Loss: 0.00001901
Iteration 86/1000 | Loss: 0.00001901
Iteration 87/1000 | Loss: 0.00001900
Iteration 88/1000 | Loss: 0.00001900
Iteration 89/1000 | Loss: 0.00001900
Iteration 90/1000 | Loss: 0.00001900
Iteration 91/1000 | Loss: 0.00001900
Iteration 92/1000 | Loss: 0.00001900
Iteration 93/1000 | Loss: 0.00001900
Iteration 94/1000 | Loss: 0.00001899
Iteration 95/1000 | Loss: 0.00001899
Iteration 96/1000 | Loss: 0.00001899
Iteration 97/1000 | Loss: 0.00001899
Iteration 98/1000 | Loss: 0.00001898
Iteration 99/1000 | Loss: 0.00001898
Iteration 100/1000 | Loss: 0.00001898
Iteration 101/1000 | Loss: 0.00001898
Iteration 102/1000 | Loss: 0.00001897
Iteration 103/1000 | Loss: 0.00001897
Iteration 104/1000 | Loss: 0.00001897
Iteration 105/1000 | Loss: 0.00001897
Iteration 106/1000 | Loss: 0.00001897
Iteration 107/1000 | Loss: 0.00001897
Iteration 108/1000 | Loss: 0.00001897
Iteration 109/1000 | Loss: 0.00001897
Iteration 110/1000 | Loss: 0.00001897
Iteration 111/1000 | Loss: 0.00001897
Iteration 112/1000 | Loss: 0.00001897
Iteration 113/1000 | Loss: 0.00001896
Iteration 114/1000 | Loss: 0.00001896
Iteration 115/1000 | Loss: 0.00001896
Iteration 116/1000 | Loss: 0.00001896
Iteration 117/1000 | Loss: 0.00001895
Iteration 118/1000 | Loss: 0.00001895
Iteration 119/1000 | Loss: 0.00001895
Iteration 120/1000 | Loss: 0.00001895
Iteration 121/1000 | Loss: 0.00001894
Iteration 122/1000 | Loss: 0.00001894
Iteration 123/1000 | Loss: 0.00001894
Iteration 124/1000 | Loss: 0.00001894
Iteration 125/1000 | Loss: 0.00001894
Iteration 126/1000 | Loss: 0.00001894
Iteration 127/1000 | Loss: 0.00001894
Iteration 128/1000 | Loss: 0.00001894
Iteration 129/1000 | Loss: 0.00001894
Iteration 130/1000 | Loss: 0.00001894
Iteration 131/1000 | Loss: 0.00001894
Iteration 132/1000 | Loss: 0.00001894
Iteration 133/1000 | Loss: 0.00001893
Iteration 134/1000 | Loss: 0.00001893
Iteration 135/1000 | Loss: 0.00001893
Iteration 136/1000 | Loss: 0.00001892
Iteration 137/1000 | Loss: 0.00001892
Iteration 138/1000 | Loss: 0.00001892
Iteration 139/1000 | Loss: 0.00001892
Iteration 140/1000 | Loss: 0.00001892
Iteration 141/1000 | Loss: 0.00001892
Iteration 142/1000 | Loss: 0.00001892
Iteration 143/1000 | Loss: 0.00001892
Iteration 144/1000 | Loss: 0.00001892
Iteration 145/1000 | Loss: 0.00001892
Iteration 146/1000 | Loss: 0.00001892
Iteration 147/1000 | Loss: 0.00001892
Iteration 148/1000 | Loss: 0.00001892
Iteration 149/1000 | Loss: 0.00001892
Iteration 150/1000 | Loss: 0.00001892
Iteration 151/1000 | Loss: 0.00001892
Iteration 152/1000 | Loss: 0.00001892
Iteration 153/1000 | Loss: 0.00001892
Iteration 154/1000 | Loss: 0.00001892
Iteration 155/1000 | Loss: 0.00001892
Iteration 156/1000 | Loss: 0.00001892
Iteration 157/1000 | Loss: 0.00001892
Iteration 158/1000 | Loss: 0.00001892
Iteration 159/1000 | Loss: 0.00001892
Iteration 160/1000 | Loss: 0.00001892
Iteration 161/1000 | Loss: 0.00001892
Iteration 162/1000 | Loss: 0.00001892
Iteration 163/1000 | Loss: 0.00001892
Iteration 164/1000 | Loss: 0.00001892
Iteration 165/1000 | Loss: 0.00001892
Iteration 166/1000 | Loss: 0.00001892
Iteration 167/1000 | Loss: 0.00001892
Iteration 168/1000 | Loss: 0.00001892
Iteration 169/1000 | Loss: 0.00001892
Iteration 170/1000 | Loss: 0.00001892
Iteration 171/1000 | Loss: 0.00001892
Iteration 172/1000 | Loss: 0.00001892
Iteration 173/1000 | Loss: 0.00001892
Iteration 174/1000 | Loss: 0.00001892
Iteration 175/1000 | Loss: 0.00001892
Iteration 176/1000 | Loss: 0.00001892
Iteration 177/1000 | Loss: 0.00001892
Iteration 178/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.8916744011221454e-05, 1.8916744011221454e-05, 1.8916744011221454e-05, 1.8916744011221454e-05, 1.8916744011221454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8916744011221454e-05

Optimization complete. Final v2v error: 3.606541633605957 mm

Highest mean error: 3.9247944355010986 mm for frame 112

Lowest mean error: 3.098909378051758 mm for frame 214

Saving results

Total time: 45.522369146347046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871543
Iteration 2/25 | Loss: 0.00134717
Iteration 3/25 | Loss: 0.00123638
Iteration 4/25 | Loss: 0.00121413
Iteration 5/25 | Loss: 0.00120740
Iteration 6/25 | Loss: 0.00120519
Iteration 7/25 | Loss: 0.00120497
Iteration 8/25 | Loss: 0.00120497
Iteration 9/25 | Loss: 0.00120497
Iteration 10/25 | Loss: 0.00120497
Iteration 11/25 | Loss: 0.00120497
Iteration 12/25 | Loss: 0.00120497
Iteration 13/25 | Loss: 0.00120497
Iteration 14/25 | Loss: 0.00120497
Iteration 15/25 | Loss: 0.00120497
Iteration 16/25 | Loss: 0.00120497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012049656361341476, 0.0012049656361341476, 0.0012049656361341476, 0.0012049656361341476, 0.0012049656361341476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012049656361341476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28033614
Iteration 2/25 | Loss: 0.00175242
Iteration 3/25 | Loss: 0.00175240
Iteration 4/25 | Loss: 0.00175239
Iteration 5/25 | Loss: 0.00175239
Iteration 6/25 | Loss: 0.00175239
Iteration 7/25 | Loss: 0.00175239
Iteration 8/25 | Loss: 0.00175239
Iteration 9/25 | Loss: 0.00175239
Iteration 10/25 | Loss: 0.00175239
Iteration 11/25 | Loss: 0.00175239
Iteration 12/25 | Loss: 0.00175239
Iteration 13/25 | Loss: 0.00175239
Iteration 14/25 | Loss: 0.00175239
Iteration 15/25 | Loss: 0.00175239
Iteration 16/25 | Loss: 0.00175239
Iteration 17/25 | Loss: 0.00175239
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017523914575576782, 0.0017523914575576782, 0.0017523914575576782, 0.0017523914575576782, 0.0017523914575576782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017523914575576782

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175239
Iteration 2/1000 | Loss: 0.00003792
Iteration 3/1000 | Loss: 0.00002813
Iteration 4/1000 | Loss: 0.00002367
Iteration 5/1000 | Loss: 0.00002247
Iteration 6/1000 | Loss: 0.00002145
Iteration 7/1000 | Loss: 0.00002086
Iteration 8/1000 | Loss: 0.00002032
Iteration 9/1000 | Loss: 0.00001988
Iteration 10/1000 | Loss: 0.00001979
Iteration 11/1000 | Loss: 0.00001963
Iteration 12/1000 | Loss: 0.00001957
Iteration 13/1000 | Loss: 0.00001944
Iteration 14/1000 | Loss: 0.00001943
Iteration 15/1000 | Loss: 0.00001942
Iteration 16/1000 | Loss: 0.00001942
Iteration 17/1000 | Loss: 0.00001939
Iteration 18/1000 | Loss: 0.00001935
Iteration 19/1000 | Loss: 0.00001934
Iteration 20/1000 | Loss: 0.00001927
Iteration 21/1000 | Loss: 0.00001926
Iteration 22/1000 | Loss: 0.00001922
Iteration 23/1000 | Loss: 0.00001921
Iteration 24/1000 | Loss: 0.00001921
Iteration 25/1000 | Loss: 0.00001920
Iteration 26/1000 | Loss: 0.00001918
Iteration 27/1000 | Loss: 0.00001913
Iteration 28/1000 | Loss: 0.00001910
Iteration 29/1000 | Loss: 0.00001909
Iteration 30/1000 | Loss: 0.00001908
Iteration 31/1000 | Loss: 0.00001908
Iteration 32/1000 | Loss: 0.00001907
Iteration 33/1000 | Loss: 0.00001905
Iteration 34/1000 | Loss: 0.00001905
Iteration 35/1000 | Loss: 0.00001905
Iteration 36/1000 | Loss: 0.00001905
Iteration 37/1000 | Loss: 0.00001904
Iteration 38/1000 | Loss: 0.00001904
Iteration 39/1000 | Loss: 0.00001903
Iteration 40/1000 | Loss: 0.00001903
Iteration 41/1000 | Loss: 0.00001903
Iteration 42/1000 | Loss: 0.00001903
Iteration 43/1000 | Loss: 0.00001902
Iteration 44/1000 | Loss: 0.00001902
Iteration 45/1000 | Loss: 0.00001901
Iteration 46/1000 | Loss: 0.00001901
Iteration 47/1000 | Loss: 0.00001901
Iteration 48/1000 | Loss: 0.00001900
Iteration 49/1000 | Loss: 0.00001900
Iteration 50/1000 | Loss: 0.00001900
Iteration 51/1000 | Loss: 0.00001900
Iteration 52/1000 | Loss: 0.00001900
Iteration 53/1000 | Loss: 0.00001900
Iteration 54/1000 | Loss: 0.00001900
Iteration 55/1000 | Loss: 0.00001900
Iteration 56/1000 | Loss: 0.00001899
Iteration 57/1000 | Loss: 0.00001899
Iteration 58/1000 | Loss: 0.00001899
Iteration 59/1000 | Loss: 0.00001899
Iteration 60/1000 | Loss: 0.00001899
Iteration 61/1000 | Loss: 0.00001899
Iteration 62/1000 | Loss: 0.00001899
Iteration 63/1000 | Loss: 0.00001899
Iteration 64/1000 | Loss: 0.00001899
Iteration 65/1000 | Loss: 0.00001899
Iteration 66/1000 | Loss: 0.00001899
Iteration 67/1000 | Loss: 0.00001899
Iteration 68/1000 | Loss: 0.00001898
Iteration 69/1000 | Loss: 0.00001898
Iteration 70/1000 | Loss: 0.00001898
Iteration 71/1000 | Loss: 0.00001898
Iteration 72/1000 | Loss: 0.00001898
Iteration 73/1000 | Loss: 0.00001898
Iteration 74/1000 | Loss: 0.00001898
Iteration 75/1000 | Loss: 0.00001897
Iteration 76/1000 | Loss: 0.00001897
Iteration 77/1000 | Loss: 0.00001897
Iteration 78/1000 | Loss: 0.00001897
Iteration 79/1000 | Loss: 0.00001897
Iteration 80/1000 | Loss: 0.00001897
Iteration 81/1000 | Loss: 0.00001897
Iteration 82/1000 | Loss: 0.00001897
Iteration 83/1000 | Loss: 0.00001896
Iteration 84/1000 | Loss: 0.00001896
Iteration 85/1000 | Loss: 0.00001896
Iteration 86/1000 | Loss: 0.00001896
Iteration 87/1000 | Loss: 0.00001896
Iteration 88/1000 | Loss: 0.00001896
Iteration 89/1000 | Loss: 0.00001896
Iteration 90/1000 | Loss: 0.00001896
Iteration 91/1000 | Loss: 0.00001895
Iteration 92/1000 | Loss: 0.00001895
Iteration 93/1000 | Loss: 0.00001895
Iteration 94/1000 | Loss: 0.00001895
Iteration 95/1000 | Loss: 0.00001895
Iteration 96/1000 | Loss: 0.00001895
Iteration 97/1000 | Loss: 0.00001895
Iteration 98/1000 | Loss: 0.00001895
Iteration 99/1000 | Loss: 0.00001894
Iteration 100/1000 | Loss: 0.00001894
Iteration 101/1000 | Loss: 0.00001894
Iteration 102/1000 | Loss: 0.00001894
Iteration 103/1000 | Loss: 0.00001893
Iteration 104/1000 | Loss: 0.00001893
Iteration 105/1000 | Loss: 0.00001893
Iteration 106/1000 | Loss: 0.00001893
Iteration 107/1000 | Loss: 0.00001893
Iteration 108/1000 | Loss: 0.00001893
Iteration 109/1000 | Loss: 0.00001893
Iteration 110/1000 | Loss: 0.00001893
Iteration 111/1000 | Loss: 0.00001893
Iteration 112/1000 | Loss: 0.00001893
Iteration 113/1000 | Loss: 0.00001893
Iteration 114/1000 | Loss: 0.00001893
Iteration 115/1000 | Loss: 0.00001892
Iteration 116/1000 | Loss: 0.00001892
Iteration 117/1000 | Loss: 0.00001892
Iteration 118/1000 | Loss: 0.00001892
Iteration 119/1000 | Loss: 0.00001892
Iteration 120/1000 | Loss: 0.00001892
Iteration 121/1000 | Loss: 0.00001892
Iteration 122/1000 | Loss: 0.00001892
Iteration 123/1000 | Loss: 0.00001892
Iteration 124/1000 | Loss: 0.00001892
Iteration 125/1000 | Loss: 0.00001891
Iteration 126/1000 | Loss: 0.00001891
Iteration 127/1000 | Loss: 0.00001891
Iteration 128/1000 | Loss: 0.00001891
Iteration 129/1000 | Loss: 0.00001891
Iteration 130/1000 | Loss: 0.00001891
Iteration 131/1000 | Loss: 0.00001891
Iteration 132/1000 | Loss: 0.00001891
Iteration 133/1000 | Loss: 0.00001891
Iteration 134/1000 | Loss: 0.00001891
Iteration 135/1000 | Loss: 0.00001891
Iteration 136/1000 | Loss: 0.00001891
Iteration 137/1000 | Loss: 0.00001891
Iteration 138/1000 | Loss: 0.00001891
Iteration 139/1000 | Loss: 0.00001890
Iteration 140/1000 | Loss: 0.00001890
Iteration 141/1000 | Loss: 0.00001890
Iteration 142/1000 | Loss: 0.00001890
Iteration 143/1000 | Loss: 0.00001890
Iteration 144/1000 | Loss: 0.00001890
Iteration 145/1000 | Loss: 0.00001890
Iteration 146/1000 | Loss: 0.00001890
Iteration 147/1000 | Loss: 0.00001890
Iteration 148/1000 | Loss: 0.00001889
Iteration 149/1000 | Loss: 0.00001889
Iteration 150/1000 | Loss: 0.00001889
Iteration 151/1000 | Loss: 0.00001889
Iteration 152/1000 | Loss: 0.00001889
Iteration 153/1000 | Loss: 0.00001889
Iteration 154/1000 | Loss: 0.00001889
Iteration 155/1000 | Loss: 0.00001889
Iteration 156/1000 | Loss: 0.00001888
Iteration 157/1000 | Loss: 0.00001888
Iteration 158/1000 | Loss: 0.00001888
Iteration 159/1000 | Loss: 0.00001888
Iteration 160/1000 | Loss: 0.00001888
Iteration 161/1000 | Loss: 0.00001888
Iteration 162/1000 | Loss: 0.00001888
Iteration 163/1000 | Loss: 0.00001888
Iteration 164/1000 | Loss: 0.00001888
Iteration 165/1000 | Loss: 0.00001888
Iteration 166/1000 | Loss: 0.00001888
Iteration 167/1000 | Loss: 0.00001888
Iteration 168/1000 | Loss: 0.00001888
Iteration 169/1000 | Loss: 0.00001888
Iteration 170/1000 | Loss: 0.00001887
Iteration 171/1000 | Loss: 0.00001887
Iteration 172/1000 | Loss: 0.00001887
Iteration 173/1000 | Loss: 0.00001887
Iteration 174/1000 | Loss: 0.00001887
Iteration 175/1000 | Loss: 0.00001887
Iteration 176/1000 | Loss: 0.00001887
Iteration 177/1000 | Loss: 0.00001887
Iteration 178/1000 | Loss: 0.00001886
Iteration 179/1000 | Loss: 0.00001886
Iteration 180/1000 | Loss: 0.00001886
Iteration 181/1000 | Loss: 0.00001886
Iteration 182/1000 | Loss: 0.00001886
Iteration 183/1000 | Loss: 0.00001886
Iteration 184/1000 | Loss: 0.00001886
Iteration 185/1000 | Loss: 0.00001886
Iteration 186/1000 | Loss: 0.00001886
Iteration 187/1000 | Loss: 0.00001886
Iteration 188/1000 | Loss: 0.00001886
Iteration 189/1000 | Loss: 0.00001886
Iteration 190/1000 | Loss: 0.00001886
Iteration 191/1000 | Loss: 0.00001886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.8861635908251628e-05, 1.8861635908251628e-05, 1.8861635908251628e-05, 1.8861635908251628e-05, 1.8861635908251628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8861635908251628e-05

Optimization complete. Final v2v error: 3.6453983783721924 mm

Highest mean error: 5.413498401641846 mm for frame 67

Lowest mean error: 3.046076774597168 mm for frame 98

Saving results

Total time: 39.57965707778931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787775
Iteration 2/25 | Loss: 0.00139539
Iteration 3/25 | Loss: 0.00124036
Iteration 4/25 | Loss: 0.00123000
Iteration 5/25 | Loss: 0.00122934
Iteration 6/25 | Loss: 0.00122934
Iteration 7/25 | Loss: 0.00122934
Iteration 8/25 | Loss: 0.00122934
Iteration 9/25 | Loss: 0.00122934
Iteration 10/25 | Loss: 0.00122934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012293405598029494, 0.0012293405598029494, 0.0012293405598029494, 0.0012293405598029494, 0.0012293405598029494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012293405598029494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21912992
Iteration 2/25 | Loss: 0.00131939
Iteration 3/25 | Loss: 0.00131939
Iteration 4/25 | Loss: 0.00131939
Iteration 5/25 | Loss: 0.00131939
Iteration 6/25 | Loss: 0.00131939
Iteration 7/25 | Loss: 0.00131939
Iteration 8/25 | Loss: 0.00131939
Iteration 9/25 | Loss: 0.00131939
Iteration 10/25 | Loss: 0.00131939
Iteration 11/25 | Loss: 0.00131939
Iteration 12/25 | Loss: 0.00131939
Iteration 13/25 | Loss: 0.00131939
Iteration 14/25 | Loss: 0.00131939
Iteration 15/25 | Loss: 0.00131939
Iteration 16/25 | Loss: 0.00131939
Iteration 17/25 | Loss: 0.00131939
Iteration 18/25 | Loss: 0.00131939
Iteration 19/25 | Loss: 0.00131939
Iteration 20/25 | Loss: 0.00131939
Iteration 21/25 | Loss: 0.00131939
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013193885097280145, 0.0013193885097280145, 0.0013193885097280145, 0.0013193885097280145, 0.0013193885097280145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013193885097280145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131939
Iteration 2/1000 | Loss: 0.00002743
Iteration 3/1000 | Loss: 0.00002063
Iteration 4/1000 | Loss: 0.00001917
Iteration 5/1000 | Loss: 0.00001834
Iteration 6/1000 | Loss: 0.00001767
Iteration 7/1000 | Loss: 0.00001702
Iteration 8/1000 | Loss: 0.00001665
Iteration 9/1000 | Loss: 0.00001640
Iteration 10/1000 | Loss: 0.00001628
Iteration 11/1000 | Loss: 0.00001624
Iteration 12/1000 | Loss: 0.00001622
Iteration 13/1000 | Loss: 0.00001617
Iteration 14/1000 | Loss: 0.00001617
Iteration 15/1000 | Loss: 0.00001615
Iteration 16/1000 | Loss: 0.00001615
Iteration 17/1000 | Loss: 0.00001614
Iteration 18/1000 | Loss: 0.00001613
Iteration 19/1000 | Loss: 0.00001613
Iteration 20/1000 | Loss: 0.00001613
Iteration 21/1000 | Loss: 0.00001613
Iteration 22/1000 | Loss: 0.00001613
Iteration 23/1000 | Loss: 0.00001613
Iteration 24/1000 | Loss: 0.00001607
Iteration 25/1000 | Loss: 0.00001604
Iteration 26/1000 | Loss: 0.00001602
Iteration 27/1000 | Loss: 0.00001601
Iteration 28/1000 | Loss: 0.00001601
Iteration 29/1000 | Loss: 0.00001600
Iteration 30/1000 | Loss: 0.00001600
Iteration 31/1000 | Loss: 0.00001599
Iteration 32/1000 | Loss: 0.00001599
Iteration 33/1000 | Loss: 0.00001598
Iteration 34/1000 | Loss: 0.00001598
Iteration 35/1000 | Loss: 0.00001598
Iteration 36/1000 | Loss: 0.00001597
Iteration 37/1000 | Loss: 0.00001597
Iteration 38/1000 | Loss: 0.00001596
Iteration 39/1000 | Loss: 0.00001595
Iteration 40/1000 | Loss: 0.00001595
Iteration 41/1000 | Loss: 0.00001595
Iteration 42/1000 | Loss: 0.00001595
Iteration 43/1000 | Loss: 0.00001594
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00001594
Iteration 46/1000 | Loss: 0.00001594
Iteration 47/1000 | Loss: 0.00001594
Iteration 48/1000 | Loss: 0.00001594
Iteration 49/1000 | Loss: 0.00001593
Iteration 50/1000 | Loss: 0.00001593
Iteration 51/1000 | Loss: 0.00001592
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001592
Iteration 54/1000 | Loss: 0.00001591
Iteration 55/1000 | Loss: 0.00001588
Iteration 56/1000 | Loss: 0.00001588
Iteration 57/1000 | Loss: 0.00001587
Iteration 58/1000 | Loss: 0.00001586
Iteration 59/1000 | Loss: 0.00001585
Iteration 60/1000 | Loss: 0.00001584
Iteration 61/1000 | Loss: 0.00001584
Iteration 62/1000 | Loss: 0.00001584
Iteration 63/1000 | Loss: 0.00001584
Iteration 64/1000 | Loss: 0.00001584
Iteration 65/1000 | Loss: 0.00001584
Iteration 66/1000 | Loss: 0.00001584
Iteration 67/1000 | Loss: 0.00001584
Iteration 68/1000 | Loss: 0.00001584
Iteration 69/1000 | Loss: 0.00001583
Iteration 70/1000 | Loss: 0.00001583
Iteration 71/1000 | Loss: 0.00001583
Iteration 72/1000 | Loss: 0.00001583
Iteration 73/1000 | Loss: 0.00001583
Iteration 74/1000 | Loss: 0.00001583
Iteration 75/1000 | Loss: 0.00001583
Iteration 76/1000 | Loss: 0.00001583
Iteration 77/1000 | Loss: 0.00001583
Iteration 78/1000 | Loss: 0.00001582
Iteration 79/1000 | Loss: 0.00001582
Iteration 80/1000 | Loss: 0.00001582
Iteration 81/1000 | Loss: 0.00001582
Iteration 82/1000 | Loss: 0.00001582
Iteration 83/1000 | Loss: 0.00001582
Iteration 84/1000 | Loss: 0.00001582
Iteration 85/1000 | Loss: 0.00001582
Iteration 86/1000 | Loss: 0.00001582
Iteration 87/1000 | Loss: 0.00001581
Iteration 88/1000 | Loss: 0.00001581
Iteration 89/1000 | Loss: 0.00001581
Iteration 90/1000 | Loss: 0.00001581
Iteration 91/1000 | Loss: 0.00001581
Iteration 92/1000 | Loss: 0.00001581
Iteration 93/1000 | Loss: 0.00001581
Iteration 94/1000 | Loss: 0.00001581
Iteration 95/1000 | Loss: 0.00001580
Iteration 96/1000 | Loss: 0.00001580
Iteration 97/1000 | Loss: 0.00001580
Iteration 98/1000 | Loss: 0.00001580
Iteration 99/1000 | Loss: 0.00001580
Iteration 100/1000 | Loss: 0.00001580
Iteration 101/1000 | Loss: 0.00001580
Iteration 102/1000 | Loss: 0.00001580
Iteration 103/1000 | Loss: 0.00001580
Iteration 104/1000 | Loss: 0.00001580
Iteration 105/1000 | Loss: 0.00001580
Iteration 106/1000 | Loss: 0.00001579
Iteration 107/1000 | Loss: 0.00001579
Iteration 108/1000 | Loss: 0.00001579
Iteration 109/1000 | Loss: 0.00001579
Iteration 110/1000 | Loss: 0.00001579
Iteration 111/1000 | Loss: 0.00001579
Iteration 112/1000 | Loss: 0.00001579
Iteration 113/1000 | Loss: 0.00001579
Iteration 114/1000 | Loss: 0.00001579
Iteration 115/1000 | Loss: 0.00001578
Iteration 116/1000 | Loss: 0.00001578
Iteration 117/1000 | Loss: 0.00001578
Iteration 118/1000 | Loss: 0.00001578
Iteration 119/1000 | Loss: 0.00001578
Iteration 120/1000 | Loss: 0.00001578
Iteration 121/1000 | Loss: 0.00001578
Iteration 122/1000 | Loss: 0.00001577
Iteration 123/1000 | Loss: 0.00001577
Iteration 124/1000 | Loss: 0.00001577
Iteration 125/1000 | Loss: 0.00001577
Iteration 126/1000 | Loss: 0.00001577
Iteration 127/1000 | Loss: 0.00001577
Iteration 128/1000 | Loss: 0.00001577
Iteration 129/1000 | Loss: 0.00001577
Iteration 130/1000 | Loss: 0.00001577
Iteration 131/1000 | Loss: 0.00001577
Iteration 132/1000 | Loss: 0.00001577
Iteration 133/1000 | Loss: 0.00001577
Iteration 134/1000 | Loss: 0.00001576
Iteration 135/1000 | Loss: 0.00001576
Iteration 136/1000 | Loss: 0.00001576
Iteration 137/1000 | Loss: 0.00001576
Iteration 138/1000 | Loss: 0.00001576
Iteration 139/1000 | Loss: 0.00001576
Iteration 140/1000 | Loss: 0.00001576
Iteration 141/1000 | Loss: 0.00001576
Iteration 142/1000 | Loss: 0.00001576
Iteration 143/1000 | Loss: 0.00001576
Iteration 144/1000 | Loss: 0.00001576
Iteration 145/1000 | Loss: 0.00001576
Iteration 146/1000 | Loss: 0.00001576
Iteration 147/1000 | Loss: 0.00001576
Iteration 148/1000 | Loss: 0.00001576
Iteration 149/1000 | Loss: 0.00001576
Iteration 150/1000 | Loss: 0.00001576
Iteration 151/1000 | Loss: 0.00001576
Iteration 152/1000 | Loss: 0.00001576
Iteration 153/1000 | Loss: 0.00001576
Iteration 154/1000 | Loss: 0.00001576
Iteration 155/1000 | Loss: 0.00001576
Iteration 156/1000 | Loss: 0.00001576
Iteration 157/1000 | Loss: 0.00001576
Iteration 158/1000 | Loss: 0.00001576
Iteration 159/1000 | Loss: 0.00001576
Iteration 160/1000 | Loss: 0.00001576
Iteration 161/1000 | Loss: 0.00001576
Iteration 162/1000 | Loss: 0.00001576
Iteration 163/1000 | Loss: 0.00001576
Iteration 164/1000 | Loss: 0.00001576
Iteration 165/1000 | Loss: 0.00001576
Iteration 166/1000 | Loss: 0.00001576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.5757104847580194e-05, 1.5757104847580194e-05, 1.5757104847580194e-05, 1.5757104847580194e-05, 1.5757104847580194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5757104847580194e-05

Optimization complete. Final v2v error: 3.3321189880371094 mm

Highest mean error: 3.551238536834717 mm for frame 161

Lowest mean error: 3.0920615196228027 mm for frame 118

Saving results

Total time: 36.85295629501343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00731781
Iteration 2/25 | Loss: 0.00128582
Iteration 3/25 | Loss: 0.00116695
Iteration 4/25 | Loss: 0.00115948
Iteration 5/25 | Loss: 0.00115772
Iteration 6/25 | Loss: 0.00115772
Iteration 7/25 | Loss: 0.00115772
Iteration 8/25 | Loss: 0.00115772
Iteration 9/25 | Loss: 0.00115772
Iteration 10/25 | Loss: 0.00115772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011577240657061338, 0.0011577240657061338, 0.0011577240657061338, 0.0011577240657061338, 0.0011577240657061338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011577240657061338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21880388
Iteration 2/25 | Loss: 0.00162730
Iteration 3/25 | Loss: 0.00162727
Iteration 4/25 | Loss: 0.00162727
Iteration 5/25 | Loss: 0.00162727
Iteration 6/25 | Loss: 0.00162727
Iteration 7/25 | Loss: 0.00162727
Iteration 8/25 | Loss: 0.00162727
Iteration 9/25 | Loss: 0.00162727
Iteration 10/25 | Loss: 0.00162727
Iteration 11/25 | Loss: 0.00162727
Iteration 12/25 | Loss: 0.00162727
Iteration 13/25 | Loss: 0.00162727
Iteration 14/25 | Loss: 0.00162727
Iteration 15/25 | Loss: 0.00162727
Iteration 16/25 | Loss: 0.00162727
Iteration 17/25 | Loss: 0.00162727
Iteration 18/25 | Loss: 0.00162727
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016272668726742268, 0.0016272668726742268, 0.0016272668726742268, 0.0016272668726742268, 0.0016272668726742268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016272668726742268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162727
Iteration 2/1000 | Loss: 0.00002643
Iteration 3/1000 | Loss: 0.00002157
Iteration 4/1000 | Loss: 0.00001906
Iteration 5/1000 | Loss: 0.00001775
Iteration 6/1000 | Loss: 0.00001668
Iteration 7/1000 | Loss: 0.00001610
Iteration 8/1000 | Loss: 0.00001561
Iteration 9/1000 | Loss: 0.00001518
Iteration 10/1000 | Loss: 0.00001486
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001453
Iteration 13/1000 | Loss: 0.00001451
Iteration 14/1000 | Loss: 0.00001446
Iteration 15/1000 | Loss: 0.00001441
Iteration 16/1000 | Loss: 0.00001436
Iteration 17/1000 | Loss: 0.00001435
Iteration 18/1000 | Loss: 0.00001435
Iteration 19/1000 | Loss: 0.00001434
Iteration 20/1000 | Loss: 0.00001432
Iteration 21/1000 | Loss: 0.00001430
Iteration 22/1000 | Loss: 0.00001429
Iteration 23/1000 | Loss: 0.00001426
Iteration 24/1000 | Loss: 0.00001426
Iteration 25/1000 | Loss: 0.00001422
Iteration 26/1000 | Loss: 0.00001420
Iteration 27/1000 | Loss: 0.00001420
Iteration 28/1000 | Loss: 0.00001420
Iteration 29/1000 | Loss: 0.00001419
Iteration 30/1000 | Loss: 0.00001419
Iteration 31/1000 | Loss: 0.00001418
Iteration 32/1000 | Loss: 0.00001418
Iteration 33/1000 | Loss: 0.00001417
Iteration 34/1000 | Loss: 0.00001416
Iteration 35/1000 | Loss: 0.00001416
Iteration 36/1000 | Loss: 0.00001413
Iteration 37/1000 | Loss: 0.00001413
Iteration 38/1000 | Loss: 0.00001413
Iteration 39/1000 | Loss: 0.00001411
Iteration 40/1000 | Loss: 0.00001408
Iteration 41/1000 | Loss: 0.00001404
Iteration 42/1000 | Loss: 0.00001402
Iteration 43/1000 | Loss: 0.00001401
Iteration 44/1000 | Loss: 0.00001400
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001399
Iteration 48/1000 | Loss: 0.00001398
Iteration 49/1000 | Loss: 0.00001398
Iteration 50/1000 | Loss: 0.00001397
Iteration 51/1000 | Loss: 0.00001396
Iteration 52/1000 | Loss: 0.00001396
Iteration 53/1000 | Loss: 0.00001396
Iteration 54/1000 | Loss: 0.00001396
Iteration 55/1000 | Loss: 0.00001395
Iteration 56/1000 | Loss: 0.00001395
Iteration 57/1000 | Loss: 0.00001395
Iteration 58/1000 | Loss: 0.00001395
Iteration 59/1000 | Loss: 0.00001395
Iteration 60/1000 | Loss: 0.00001394
Iteration 61/1000 | Loss: 0.00001393
Iteration 62/1000 | Loss: 0.00001393
Iteration 63/1000 | Loss: 0.00001393
Iteration 64/1000 | Loss: 0.00001393
Iteration 65/1000 | Loss: 0.00001392
Iteration 66/1000 | Loss: 0.00001392
Iteration 67/1000 | Loss: 0.00001391
Iteration 68/1000 | Loss: 0.00001391
Iteration 69/1000 | Loss: 0.00001391
Iteration 70/1000 | Loss: 0.00001391
Iteration 71/1000 | Loss: 0.00001391
Iteration 72/1000 | Loss: 0.00001391
Iteration 73/1000 | Loss: 0.00001390
Iteration 74/1000 | Loss: 0.00001390
Iteration 75/1000 | Loss: 0.00001390
Iteration 76/1000 | Loss: 0.00001390
Iteration 77/1000 | Loss: 0.00001389
Iteration 78/1000 | Loss: 0.00001389
Iteration 79/1000 | Loss: 0.00001389
Iteration 80/1000 | Loss: 0.00001388
Iteration 81/1000 | Loss: 0.00001388
Iteration 82/1000 | Loss: 0.00001388
Iteration 83/1000 | Loss: 0.00001388
Iteration 84/1000 | Loss: 0.00001387
Iteration 85/1000 | Loss: 0.00001387
Iteration 86/1000 | Loss: 0.00001386
Iteration 87/1000 | Loss: 0.00001386
Iteration 88/1000 | Loss: 0.00001386
Iteration 89/1000 | Loss: 0.00001386
Iteration 90/1000 | Loss: 0.00001386
Iteration 91/1000 | Loss: 0.00001385
Iteration 92/1000 | Loss: 0.00001385
Iteration 93/1000 | Loss: 0.00001385
Iteration 94/1000 | Loss: 0.00001385
Iteration 95/1000 | Loss: 0.00001385
Iteration 96/1000 | Loss: 0.00001384
Iteration 97/1000 | Loss: 0.00001384
Iteration 98/1000 | Loss: 0.00001384
Iteration 99/1000 | Loss: 0.00001384
Iteration 100/1000 | Loss: 0.00001384
Iteration 101/1000 | Loss: 0.00001384
Iteration 102/1000 | Loss: 0.00001384
Iteration 103/1000 | Loss: 0.00001384
Iteration 104/1000 | Loss: 0.00001384
Iteration 105/1000 | Loss: 0.00001384
Iteration 106/1000 | Loss: 0.00001384
Iteration 107/1000 | Loss: 0.00001384
Iteration 108/1000 | Loss: 0.00001384
Iteration 109/1000 | Loss: 0.00001383
Iteration 110/1000 | Loss: 0.00001383
Iteration 111/1000 | Loss: 0.00001383
Iteration 112/1000 | Loss: 0.00001383
Iteration 113/1000 | Loss: 0.00001383
Iteration 114/1000 | Loss: 0.00001383
Iteration 115/1000 | Loss: 0.00001382
Iteration 116/1000 | Loss: 0.00001382
Iteration 117/1000 | Loss: 0.00001382
Iteration 118/1000 | Loss: 0.00001382
Iteration 119/1000 | Loss: 0.00001382
Iteration 120/1000 | Loss: 0.00001382
Iteration 121/1000 | Loss: 0.00001382
Iteration 122/1000 | Loss: 0.00001382
Iteration 123/1000 | Loss: 0.00001381
Iteration 124/1000 | Loss: 0.00001381
Iteration 125/1000 | Loss: 0.00001381
Iteration 126/1000 | Loss: 0.00001380
Iteration 127/1000 | Loss: 0.00001380
Iteration 128/1000 | Loss: 0.00001380
Iteration 129/1000 | Loss: 0.00001380
Iteration 130/1000 | Loss: 0.00001380
Iteration 131/1000 | Loss: 0.00001380
Iteration 132/1000 | Loss: 0.00001379
Iteration 133/1000 | Loss: 0.00001379
Iteration 134/1000 | Loss: 0.00001379
Iteration 135/1000 | Loss: 0.00001379
Iteration 136/1000 | Loss: 0.00001379
Iteration 137/1000 | Loss: 0.00001379
Iteration 138/1000 | Loss: 0.00001378
Iteration 139/1000 | Loss: 0.00001378
Iteration 140/1000 | Loss: 0.00001378
Iteration 141/1000 | Loss: 0.00001378
Iteration 142/1000 | Loss: 0.00001378
Iteration 143/1000 | Loss: 0.00001378
Iteration 144/1000 | Loss: 0.00001377
Iteration 145/1000 | Loss: 0.00001377
Iteration 146/1000 | Loss: 0.00001377
Iteration 147/1000 | Loss: 0.00001376
Iteration 148/1000 | Loss: 0.00001376
Iteration 149/1000 | Loss: 0.00001376
Iteration 150/1000 | Loss: 0.00001376
Iteration 151/1000 | Loss: 0.00001376
Iteration 152/1000 | Loss: 0.00001376
Iteration 153/1000 | Loss: 0.00001376
Iteration 154/1000 | Loss: 0.00001376
Iteration 155/1000 | Loss: 0.00001376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.3761303307546768e-05, 1.3761303307546768e-05, 1.3761303307546768e-05, 1.3761303307546768e-05, 1.3761303307546768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3761303307546768e-05

Optimization complete. Final v2v error: 3.126147508621216 mm

Highest mean error: 3.932260036468506 mm for frame 236

Lowest mean error: 2.5641844272613525 mm for frame 46

Saving results

Total time: 43.92058610916138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874356
Iteration 2/25 | Loss: 0.00138366
Iteration 3/25 | Loss: 0.00121803
Iteration 4/25 | Loss: 0.00116881
Iteration 5/25 | Loss: 0.00115988
Iteration 6/25 | Loss: 0.00115839
Iteration 7/25 | Loss: 0.00115791
Iteration 8/25 | Loss: 0.00115773
Iteration 9/25 | Loss: 0.00115770
Iteration 10/25 | Loss: 0.00115770
Iteration 11/25 | Loss: 0.00115770
Iteration 12/25 | Loss: 0.00115770
Iteration 13/25 | Loss: 0.00115770
Iteration 14/25 | Loss: 0.00115770
Iteration 15/25 | Loss: 0.00115769
Iteration 16/25 | Loss: 0.00115769
Iteration 17/25 | Loss: 0.00115768
Iteration 18/25 | Loss: 0.00115767
Iteration 19/25 | Loss: 0.00115767
Iteration 20/25 | Loss: 0.00115767
Iteration 21/25 | Loss: 0.00115767
Iteration 22/25 | Loss: 0.00115767
Iteration 23/25 | Loss: 0.00115767
Iteration 24/25 | Loss: 0.00115767
Iteration 25/25 | Loss: 0.00115766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.55729294
Iteration 2/25 | Loss: 0.00202817
Iteration 3/25 | Loss: 0.00202814
Iteration 4/25 | Loss: 0.00202814
Iteration 5/25 | Loss: 0.00202814
Iteration 6/25 | Loss: 0.00202813
Iteration 7/25 | Loss: 0.00202813
Iteration 8/25 | Loss: 0.00202813
Iteration 9/25 | Loss: 0.00202813
Iteration 10/25 | Loss: 0.00202813
Iteration 11/25 | Loss: 0.00202813
Iteration 12/25 | Loss: 0.00202813
Iteration 13/25 | Loss: 0.00202813
Iteration 14/25 | Loss: 0.00202813
Iteration 15/25 | Loss: 0.00202813
Iteration 16/25 | Loss: 0.00202813
Iteration 17/25 | Loss: 0.00202813
Iteration 18/25 | Loss: 0.00202813
Iteration 19/25 | Loss: 0.00202813
Iteration 20/25 | Loss: 0.00202813
Iteration 21/25 | Loss: 0.00202813
Iteration 22/25 | Loss: 0.00202813
Iteration 23/25 | Loss: 0.00202813
Iteration 24/25 | Loss: 0.00202813
Iteration 25/25 | Loss: 0.00202813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202813
Iteration 2/1000 | Loss: 0.00001745
Iteration 3/1000 | Loss: 0.00001325
Iteration 4/1000 | Loss: 0.00001194
Iteration 5/1000 | Loss: 0.00001125
Iteration 6/1000 | Loss: 0.00003187
Iteration 7/1000 | Loss: 0.00001067
Iteration 8/1000 | Loss: 0.00002300
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001021
Iteration 11/1000 | Loss: 0.00001020
Iteration 12/1000 | Loss: 0.00000994
Iteration 13/1000 | Loss: 0.00000976
Iteration 14/1000 | Loss: 0.00000972
Iteration 15/1000 | Loss: 0.00000971
Iteration 16/1000 | Loss: 0.00000968
Iteration 17/1000 | Loss: 0.00000967
Iteration 18/1000 | Loss: 0.00000967
Iteration 19/1000 | Loss: 0.00000966
Iteration 20/1000 | Loss: 0.00000965
Iteration 21/1000 | Loss: 0.00000961
Iteration 22/1000 | Loss: 0.00000960
Iteration 23/1000 | Loss: 0.00000960
Iteration 24/1000 | Loss: 0.00000960
Iteration 25/1000 | Loss: 0.00000959
Iteration 26/1000 | Loss: 0.00000959
Iteration 27/1000 | Loss: 0.00000959
Iteration 28/1000 | Loss: 0.00000959
Iteration 29/1000 | Loss: 0.00000958
Iteration 30/1000 | Loss: 0.00000958
Iteration 31/1000 | Loss: 0.00000958
Iteration 32/1000 | Loss: 0.00000958
Iteration 33/1000 | Loss: 0.00000958
Iteration 34/1000 | Loss: 0.00000957
Iteration 35/1000 | Loss: 0.00003642
Iteration 36/1000 | Loss: 0.00001311
Iteration 37/1000 | Loss: 0.00001851
Iteration 38/1000 | Loss: 0.00000945
Iteration 39/1000 | Loss: 0.00000944
Iteration 40/1000 | Loss: 0.00000944
Iteration 41/1000 | Loss: 0.00000944
Iteration 42/1000 | Loss: 0.00000944
Iteration 43/1000 | Loss: 0.00000944
Iteration 44/1000 | Loss: 0.00000944
Iteration 45/1000 | Loss: 0.00000943
Iteration 46/1000 | Loss: 0.00000943
Iteration 47/1000 | Loss: 0.00000943
Iteration 48/1000 | Loss: 0.00000943
Iteration 49/1000 | Loss: 0.00000943
Iteration 50/1000 | Loss: 0.00000942
Iteration 51/1000 | Loss: 0.00000942
Iteration 52/1000 | Loss: 0.00000942
Iteration 53/1000 | Loss: 0.00000942
Iteration 54/1000 | Loss: 0.00000940
Iteration 55/1000 | Loss: 0.00000940
Iteration 56/1000 | Loss: 0.00000940
Iteration 57/1000 | Loss: 0.00000940
Iteration 58/1000 | Loss: 0.00000940
Iteration 59/1000 | Loss: 0.00000939
Iteration 60/1000 | Loss: 0.00000939
Iteration 61/1000 | Loss: 0.00000938
Iteration 62/1000 | Loss: 0.00000937
Iteration 63/1000 | Loss: 0.00000937
Iteration 64/1000 | Loss: 0.00000937
Iteration 65/1000 | Loss: 0.00000936
Iteration 66/1000 | Loss: 0.00000936
Iteration 67/1000 | Loss: 0.00000936
Iteration 68/1000 | Loss: 0.00000934
Iteration 69/1000 | Loss: 0.00000934
Iteration 70/1000 | Loss: 0.00000933
Iteration 71/1000 | Loss: 0.00000933
Iteration 72/1000 | Loss: 0.00000932
Iteration 73/1000 | Loss: 0.00002475
Iteration 74/1000 | Loss: 0.00000930
Iteration 75/1000 | Loss: 0.00000926
Iteration 76/1000 | Loss: 0.00000926
Iteration 77/1000 | Loss: 0.00000925
Iteration 78/1000 | Loss: 0.00000925
Iteration 79/1000 | Loss: 0.00000925
Iteration 80/1000 | Loss: 0.00000925
Iteration 81/1000 | Loss: 0.00000925
Iteration 82/1000 | Loss: 0.00000925
Iteration 83/1000 | Loss: 0.00000925
Iteration 84/1000 | Loss: 0.00000925
Iteration 85/1000 | Loss: 0.00000925
Iteration 86/1000 | Loss: 0.00000925
Iteration 87/1000 | Loss: 0.00000925
Iteration 88/1000 | Loss: 0.00000925
Iteration 89/1000 | Loss: 0.00000925
Iteration 90/1000 | Loss: 0.00000924
Iteration 91/1000 | Loss: 0.00000924
Iteration 92/1000 | Loss: 0.00000924
Iteration 93/1000 | Loss: 0.00000924
Iteration 94/1000 | Loss: 0.00000924
Iteration 95/1000 | Loss: 0.00000924
Iteration 96/1000 | Loss: 0.00000924
Iteration 97/1000 | Loss: 0.00000924
Iteration 98/1000 | Loss: 0.00000924
Iteration 99/1000 | Loss: 0.00000924
Iteration 100/1000 | Loss: 0.00000924
Iteration 101/1000 | Loss: 0.00000924
Iteration 102/1000 | Loss: 0.00000924
Iteration 103/1000 | Loss: 0.00000924
Iteration 104/1000 | Loss: 0.00000924
Iteration 105/1000 | Loss: 0.00000924
Iteration 106/1000 | Loss: 0.00000924
Iteration 107/1000 | Loss: 0.00000924
Iteration 108/1000 | Loss: 0.00000924
Iteration 109/1000 | Loss: 0.00000924
Iteration 110/1000 | Loss: 0.00000923
Iteration 111/1000 | Loss: 0.00000923
Iteration 112/1000 | Loss: 0.00000923
Iteration 113/1000 | Loss: 0.00000923
Iteration 114/1000 | Loss: 0.00000923
Iteration 115/1000 | Loss: 0.00000923
Iteration 116/1000 | Loss: 0.00000923
Iteration 117/1000 | Loss: 0.00000923
Iteration 118/1000 | Loss: 0.00000922
Iteration 119/1000 | Loss: 0.00000922
Iteration 120/1000 | Loss: 0.00000922
Iteration 121/1000 | Loss: 0.00000922
Iteration 122/1000 | Loss: 0.00000922
Iteration 123/1000 | Loss: 0.00000922
Iteration 124/1000 | Loss: 0.00000922
Iteration 125/1000 | Loss: 0.00000922
Iteration 126/1000 | Loss: 0.00000922
Iteration 127/1000 | Loss: 0.00000922
Iteration 128/1000 | Loss: 0.00000922
Iteration 129/1000 | Loss: 0.00000922
Iteration 130/1000 | Loss: 0.00000922
Iteration 131/1000 | Loss: 0.00000921
Iteration 132/1000 | Loss: 0.00000921
Iteration 133/1000 | Loss: 0.00000921
Iteration 134/1000 | Loss: 0.00000921
Iteration 135/1000 | Loss: 0.00000921
Iteration 136/1000 | Loss: 0.00000921
Iteration 137/1000 | Loss: 0.00000921
Iteration 138/1000 | Loss: 0.00000921
Iteration 139/1000 | Loss: 0.00000921
Iteration 140/1000 | Loss: 0.00000920
Iteration 141/1000 | Loss: 0.00000920
Iteration 142/1000 | Loss: 0.00000920
Iteration 143/1000 | Loss: 0.00000920
Iteration 144/1000 | Loss: 0.00000920
Iteration 145/1000 | Loss: 0.00000920
Iteration 146/1000 | Loss: 0.00000920
Iteration 147/1000 | Loss: 0.00000920
Iteration 148/1000 | Loss: 0.00000919
Iteration 149/1000 | Loss: 0.00000919
Iteration 150/1000 | Loss: 0.00000919
Iteration 151/1000 | Loss: 0.00000919
Iteration 152/1000 | Loss: 0.00000919
Iteration 153/1000 | Loss: 0.00000919
Iteration 154/1000 | Loss: 0.00000919
Iteration 155/1000 | Loss: 0.00000919
Iteration 156/1000 | Loss: 0.00000918
Iteration 157/1000 | Loss: 0.00000918
Iteration 158/1000 | Loss: 0.00003105
Iteration 159/1000 | Loss: 0.00000939
Iteration 160/1000 | Loss: 0.00000916
Iteration 161/1000 | Loss: 0.00000916
Iteration 162/1000 | Loss: 0.00000916
Iteration 163/1000 | Loss: 0.00000915
Iteration 164/1000 | Loss: 0.00000915
Iteration 165/1000 | Loss: 0.00000915
Iteration 166/1000 | Loss: 0.00000915
Iteration 167/1000 | Loss: 0.00000915
Iteration 168/1000 | Loss: 0.00000915
Iteration 169/1000 | Loss: 0.00000915
Iteration 170/1000 | Loss: 0.00000915
Iteration 171/1000 | Loss: 0.00000915
Iteration 172/1000 | Loss: 0.00000915
Iteration 173/1000 | Loss: 0.00000914
Iteration 174/1000 | Loss: 0.00000914
Iteration 175/1000 | Loss: 0.00000914
Iteration 176/1000 | Loss: 0.00000914
Iteration 177/1000 | Loss: 0.00000913
Iteration 178/1000 | Loss: 0.00000913
Iteration 179/1000 | Loss: 0.00000913
Iteration 180/1000 | Loss: 0.00000913
Iteration 181/1000 | Loss: 0.00000913
Iteration 182/1000 | Loss: 0.00000913
Iteration 183/1000 | Loss: 0.00000913
Iteration 184/1000 | Loss: 0.00000912
Iteration 185/1000 | Loss: 0.00000912
Iteration 186/1000 | Loss: 0.00000912
Iteration 187/1000 | Loss: 0.00000912
Iteration 188/1000 | Loss: 0.00000912
Iteration 189/1000 | Loss: 0.00000912
Iteration 190/1000 | Loss: 0.00000912
Iteration 191/1000 | Loss: 0.00000912
Iteration 192/1000 | Loss: 0.00000912
Iteration 193/1000 | Loss: 0.00000912
Iteration 194/1000 | Loss: 0.00000912
Iteration 195/1000 | Loss: 0.00000912
Iteration 196/1000 | Loss: 0.00000912
Iteration 197/1000 | Loss: 0.00000912
Iteration 198/1000 | Loss: 0.00000912
Iteration 199/1000 | Loss: 0.00000912
Iteration 200/1000 | Loss: 0.00000912
Iteration 201/1000 | Loss: 0.00000912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [9.123100426222663e-06, 9.123100426222663e-06, 9.123100426222663e-06, 9.123100426222663e-06, 9.123100426222663e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.123100426222663e-06

Optimization complete. Final v2v error: 2.560166835784912 mm

Highest mean error: 3.229275703430176 mm for frame 84

Lowest mean error: 2.2859935760498047 mm for frame 174

Saving results

Total time: 62.053810596466064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00518274
Iteration 2/25 | Loss: 0.00136149
Iteration 3/25 | Loss: 0.00119000
Iteration 4/25 | Loss: 0.00117637
Iteration 5/25 | Loss: 0.00117523
Iteration 6/25 | Loss: 0.00117523
Iteration 7/25 | Loss: 0.00117523
Iteration 8/25 | Loss: 0.00117523
Iteration 9/25 | Loss: 0.00117523
Iteration 10/25 | Loss: 0.00117523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011752344435080886, 0.0011752344435080886, 0.0011752344435080886, 0.0011752344435080886, 0.0011752344435080886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011752344435080886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21978748
Iteration 2/25 | Loss: 0.00129025
Iteration 3/25 | Loss: 0.00129023
Iteration 4/25 | Loss: 0.00129023
Iteration 5/25 | Loss: 0.00129023
Iteration 6/25 | Loss: 0.00129023
Iteration 7/25 | Loss: 0.00129023
Iteration 8/25 | Loss: 0.00129023
Iteration 9/25 | Loss: 0.00129023
Iteration 10/25 | Loss: 0.00129023
Iteration 11/25 | Loss: 0.00129023
Iteration 12/25 | Loss: 0.00129023
Iteration 13/25 | Loss: 0.00129023
Iteration 14/25 | Loss: 0.00129023
Iteration 15/25 | Loss: 0.00129023
Iteration 16/25 | Loss: 0.00129023
Iteration 17/25 | Loss: 0.00129023
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012902312446385622, 0.0012902312446385622, 0.0012902312446385622, 0.0012902312446385622, 0.0012902312446385622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012902312446385622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129023
Iteration 2/1000 | Loss: 0.00003177
Iteration 3/1000 | Loss: 0.00002221
Iteration 4/1000 | Loss: 0.00002017
Iteration 5/1000 | Loss: 0.00001885
Iteration 6/1000 | Loss: 0.00001814
Iteration 7/1000 | Loss: 0.00001737
Iteration 8/1000 | Loss: 0.00001680
Iteration 9/1000 | Loss: 0.00001633
Iteration 10/1000 | Loss: 0.00001607
Iteration 11/1000 | Loss: 0.00001581
Iteration 12/1000 | Loss: 0.00001540
Iteration 13/1000 | Loss: 0.00001521
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001509
Iteration 16/1000 | Loss: 0.00001508
Iteration 17/1000 | Loss: 0.00001508
Iteration 18/1000 | Loss: 0.00001499
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001496
Iteration 21/1000 | Loss: 0.00001496
Iteration 22/1000 | Loss: 0.00001496
Iteration 23/1000 | Loss: 0.00001495
Iteration 24/1000 | Loss: 0.00001495
Iteration 25/1000 | Loss: 0.00001495
Iteration 26/1000 | Loss: 0.00001494
Iteration 27/1000 | Loss: 0.00001494
Iteration 28/1000 | Loss: 0.00001494
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001493
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001493
Iteration 34/1000 | Loss: 0.00001492
Iteration 35/1000 | Loss: 0.00001492
Iteration 36/1000 | Loss: 0.00001492
Iteration 37/1000 | Loss: 0.00001492
Iteration 38/1000 | Loss: 0.00001488
Iteration 39/1000 | Loss: 0.00001488
Iteration 40/1000 | Loss: 0.00001484
Iteration 41/1000 | Loss: 0.00001482
Iteration 42/1000 | Loss: 0.00001482
Iteration 43/1000 | Loss: 0.00001481
Iteration 44/1000 | Loss: 0.00001478
Iteration 45/1000 | Loss: 0.00001477
Iteration 46/1000 | Loss: 0.00001477
Iteration 47/1000 | Loss: 0.00001476
Iteration 48/1000 | Loss: 0.00001476
Iteration 49/1000 | Loss: 0.00001475
Iteration 50/1000 | Loss: 0.00001474
Iteration 51/1000 | Loss: 0.00001473
Iteration 52/1000 | Loss: 0.00001473
Iteration 53/1000 | Loss: 0.00001473
Iteration 54/1000 | Loss: 0.00001473
Iteration 55/1000 | Loss: 0.00001472
Iteration 56/1000 | Loss: 0.00001472
Iteration 57/1000 | Loss: 0.00001472
Iteration 58/1000 | Loss: 0.00001472
Iteration 59/1000 | Loss: 0.00001472
Iteration 60/1000 | Loss: 0.00001472
Iteration 61/1000 | Loss: 0.00001472
Iteration 62/1000 | Loss: 0.00001472
Iteration 63/1000 | Loss: 0.00001472
Iteration 64/1000 | Loss: 0.00001471
Iteration 65/1000 | Loss: 0.00001470
Iteration 66/1000 | Loss: 0.00001470
Iteration 67/1000 | Loss: 0.00001469
Iteration 68/1000 | Loss: 0.00001469
Iteration 69/1000 | Loss: 0.00001468
Iteration 70/1000 | Loss: 0.00001467
Iteration 71/1000 | Loss: 0.00001467
Iteration 72/1000 | Loss: 0.00001465
Iteration 73/1000 | Loss: 0.00001465
Iteration 74/1000 | Loss: 0.00001465
Iteration 75/1000 | Loss: 0.00001465
Iteration 76/1000 | Loss: 0.00001464
Iteration 77/1000 | Loss: 0.00001464
Iteration 78/1000 | Loss: 0.00001464
Iteration 79/1000 | Loss: 0.00001464
Iteration 80/1000 | Loss: 0.00001464
Iteration 81/1000 | Loss: 0.00001464
Iteration 82/1000 | Loss: 0.00001464
Iteration 83/1000 | Loss: 0.00001463
Iteration 84/1000 | Loss: 0.00001463
Iteration 85/1000 | Loss: 0.00001463
Iteration 86/1000 | Loss: 0.00001462
Iteration 87/1000 | Loss: 0.00001462
Iteration 88/1000 | Loss: 0.00001462
Iteration 89/1000 | Loss: 0.00001462
Iteration 90/1000 | Loss: 0.00001462
Iteration 91/1000 | Loss: 0.00001462
Iteration 92/1000 | Loss: 0.00001462
Iteration 93/1000 | Loss: 0.00001462
Iteration 94/1000 | Loss: 0.00001462
Iteration 95/1000 | Loss: 0.00001462
Iteration 96/1000 | Loss: 0.00001462
Iteration 97/1000 | Loss: 0.00001462
Iteration 98/1000 | Loss: 0.00001461
Iteration 99/1000 | Loss: 0.00001461
Iteration 100/1000 | Loss: 0.00001461
Iteration 101/1000 | Loss: 0.00001461
Iteration 102/1000 | Loss: 0.00001461
Iteration 103/1000 | Loss: 0.00001461
Iteration 104/1000 | Loss: 0.00001460
Iteration 105/1000 | Loss: 0.00001458
Iteration 106/1000 | Loss: 0.00001458
Iteration 107/1000 | Loss: 0.00001458
Iteration 108/1000 | Loss: 0.00001458
Iteration 109/1000 | Loss: 0.00001457
Iteration 110/1000 | Loss: 0.00001457
Iteration 111/1000 | Loss: 0.00001457
Iteration 112/1000 | Loss: 0.00001457
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001456
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001455
Iteration 120/1000 | Loss: 0.00001455
Iteration 121/1000 | Loss: 0.00001455
Iteration 122/1000 | Loss: 0.00001455
Iteration 123/1000 | Loss: 0.00001455
Iteration 124/1000 | Loss: 0.00001455
Iteration 125/1000 | Loss: 0.00001455
Iteration 126/1000 | Loss: 0.00001455
Iteration 127/1000 | Loss: 0.00001455
Iteration 128/1000 | Loss: 0.00001455
Iteration 129/1000 | Loss: 0.00001455
Iteration 130/1000 | Loss: 0.00001455
Iteration 131/1000 | Loss: 0.00001454
Iteration 132/1000 | Loss: 0.00001454
Iteration 133/1000 | Loss: 0.00001454
Iteration 134/1000 | Loss: 0.00001454
Iteration 135/1000 | Loss: 0.00001454
Iteration 136/1000 | Loss: 0.00001454
Iteration 137/1000 | Loss: 0.00001454
Iteration 138/1000 | Loss: 0.00001454
Iteration 139/1000 | Loss: 0.00001454
Iteration 140/1000 | Loss: 0.00001454
Iteration 141/1000 | Loss: 0.00001454
Iteration 142/1000 | Loss: 0.00001454
Iteration 143/1000 | Loss: 0.00001454
Iteration 144/1000 | Loss: 0.00001454
Iteration 145/1000 | Loss: 0.00001454
Iteration 146/1000 | Loss: 0.00001454
Iteration 147/1000 | Loss: 0.00001453
Iteration 148/1000 | Loss: 0.00001453
Iteration 149/1000 | Loss: 0.00001453
Iteration 150/1000 | Loss: 0.00001453
Iteration 151/1000 | Loss: 0.00001453
Iteration 152/1000 | Loss: 0.00001453
Iteration 153/1000 | Loss: 0.00001453
Iteration 154/1000 | Loss: 0.00001453
Iteration 155/1000 | Loss: 0.00001453
Iteration 156/1000 | Loss: 0.00001453
Iteration 157/1000 | Loss: 0.00001452
Iteration 158/1000 | Loss: 0.00001452
Iteration 159/1000 | Loss: 0.00001452
Iteration 160/1000 | Loss: 0.00001452
Iteration 161/1000 | Loss: 0.00001452
Iteration 162/1000 | Loss: 0.00001452
Iteration 163/1000 | Loss: 0.00001452
Iteration 164/1000 | Loss: 0.00001452
Iteration 165/1000 | Loss: 0.00001452
Iteration 166/1000 | Loss: 0.00001452
Iteration 167/1000 | Loss: 0.00001452
Iteration 168/1000 | Loss: 0.00001452
Iteration 169/1000 | Loss: 0.00001451
Iteration 170/1000 | Loss: 0.00001451
Iteration 171/1000 | Loss: 0.00001451
Iteration 172/1000 | Loss: 0.00001451
Iteration 173/1000 | Loss: 0.00001451
Iteration 174/1000 | Loss: 0.00001451
Iteration 175/1000 | Loss: 0.00001451
Iteration 176/1000 | Loss: 0.00001451
Iteration 177/1000 | Loss: 0.00001451
Iteration 178/1000 | Loss: 0.00001451
Iteration 179/1000 | Loss: 0.00001451
Iteration 180/1000 | Loss: 0.00001451
Iteration 181/1000 | Loss: 0.00001451
Iteration 182/1000 | Loss: 0.00001451
Iteration 183/1000 | Loss: 0.00001451
Iteration 184/1000 | Loss: 0.00001451
Iteration 185/1000 | Loss: 0.00001451
Iteration 186/1000 | Loss: 0.00001451
Iteration 187/1000 | Loss: 0.00001451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.450552463211352e-05, 1.450552463211352e-05, 1.450552463211352e-05, 1.450552463211352e-05, 1.450552463211352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.450552463211352e-05

Optimization complete. Final v2v error: 3.270291805267334 mm

Highest mean error: 3.446053981781006 mm for frame 49

Lowest mean error: 3.1288180351257324 mm for frame 148

Saving results

Total time: 40.34271788597107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526497
Iteration 2/25 | Loss: 0.00124320
Iteration 3/25 | Loss: 0.00116376
Iteration 4/25 | Loss: 0.00115156
Iteration 5/25 | Loss: 0.00114852
Iteration 6/25 | Loss: 0.00114795
Iteration 7/25 | Loss: 0.00114795
Iteration 8/25 | Loss: 0.00114795
Iteration 9/25 | Loss: 0.00114795
Iteration 10/25 | Loss: 0.00114795
Iteration 11/25 | Loss: 0.00114795
Iteration 12/25 | Loss: 0.00114795
Iteration 13/25 | Loss: 0.00114795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011479544918984175, 0.0011479544918984175, 0.0011479544918984175, 0.0011479544918984175, 0.0011479544918984175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011479544918984175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.61072397
Iteration 2/25 | Loss: 0.00176386
Iteration 3/25 | Loss: 0.00176385
Iteration 4/25 | Loss: 0.00176385
Iteration 5/25 | Loss: 0.00176385
Iteration 6/25 | Loss: 0.00176385
Iteration 7/25 | Loss: 0.00176385
Iteration 8/25 | Loss: 0.00176385
Iteration 9/25 | Loss: 0.00176385
Iteration 10/25 | Loss: 0.00176385
Iteration 11/25 | Loss: 0.00176384
Iteration 12/25 | Loss: 0.00176384
Iteration 13/25 | Loss: 0.00176384
Iteration 14/25 | Loss: 0.00176384
Iteration 15/25 | Loss: 0.00176384
Iteration 16/25 | Loss: 0.00176384
Iteration 17/25 | Loss: 0.00176384
Iteration 18/25 | Loss: 0.00176384
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0017638447461649776, 0.0017638447461649776, 0.0017638447461649776, 0.0017638447461649776, 0.0017638447461649776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017638447461649776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176384
Iteration 2/1000 | Loss: 0.00001949
Iteration 3/1000 | Loss: 0.00001479
Iteration 4/1000 | Loss: 0.00001344
Iteration 5/1000 | Loss: 0.00001258
Iteration 6/1000 | Loss: 0.00001214
Iteration 7/1000 | Loss: 0.00001164
Iteration 8/1000 | Loss: 0.00001139
Iteration 9/1000 | Loss: 0.00001118
Iteration 10/1000 | Loss: 0.00001091
Iteration 11/1000 | Loss: 0.00001073
Iteration 12/1000 | Loss: 0.00001066
Iteration 13/1000 | Loss: 0.00001065
Iteration 14/1000 | Loss: 0.00001064
Iteration 15/1000 | Loss: 0.00001055
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001052
Iteration 18/1000 | Loss: 0.00001051
Iteration 19/1000 | Loss: 0.00001045
Iteration 20/1000 | Loss: 0.00001043
Iteration 21/1000 | Loss: 0.00001043
Iteration 22/1000 | Loss: 0.00001042
Iteration 23/1000 | Loss: 0.00001041
Iteration 24/1000 | Loss: 0.00001041
Iteration 25/1000 | Loss: 0.00001041
Iteration 26/1000 | Loss: 0.00001041
Iteration 27/1000 | Loss: 0.00001041
Iteration 28/1000 | Loss: 0.00001041
Iteration 29/1000 | Loss: 0.00001036
Iteration 30/1000 | Loss: 0.00001036
Iteration 31/1000 | Loss: 0.00001035
Iteration 32/1000 | Loss: 0.00001034
Iteration 33/1000 | Loss: 0.00001030
Iteration 34/1000 | Loss: 0.00001029
Iteration 35/1000 | Loss: 0.00001025
Iteration 36/1000 | Loss: 0.00001020
Iteration 37/1000 | Loss: 0.00001019
Iteration 38/1000 | Loss: 0.00001018
Iteration 39/1000 | Loss: 0.00001018
Iteration 40/1000 | Loss: 0.00001017
Iteration 41/1000 | Loss: 0.00001016
Iteration 42/1000 | Loss: 0.00001015
Iteration 43/1000 | Loss: 0.00001015
Iteration 44/1000 | Loss: 0.00001014
Iteration 45/1000 | Loss: 0.00001014
Iteration 46/1000 | Loss: 0.00001014
Iteration 47/1000 | Loss: 0.00001013
Iteration 48/1000 | Loss: 0.00001013
Iteration 49/1000 | Loss: 0.00001013
Iteration 50/1000 | Loss: 0.00001012
Iteration 51/1000 | Loss: 0.00001011
Iteration 52/1000 | Loss: 0.00001011
Iteration 53/1000 | Loss: 0.00001011
Iteration 54/1000 | Loss: 0.00001010
Iteration 55/1000 | Loss: 0.00001010
Iteration 56/1000 | Loss: 0.00001009
Iteration 57/1000 | Loss: 0.00001009
Iteration 58/1000 | Loss: 0.00001009
Iteration 59/1000 | Loss: 0.00001008
Iteration 60/1000 | Loss: 0.00001008
Iteration 61/1000 | Loss: 0.00001008
Iteration 62/1000 | Loss: 0.00001006
Iteration 63/1000 | Loss: 0.00001006
Iteration 64/1000 | Loss: 0.00001006
Iteration 65/1000 | Loss: 0.00001005
Iteration 66/1000 | Loss: 0.00001005
Iteration 67/1000 | Loss: 0.00001005
Iteration 68/1000 | Loss: 0.00001005
Iteration 69/1000 | Loss: 0.00001005
Iteration 70/1000 | Loss: 0.00001005
Iteration 71/1000 | Loss: 0.00001005
Iteration 72/1000 | Loss: 0.00001005
Iteration 73/1000 | Loss: 0.00001005
Iteration 74/1000 | Loss: 0.00001005
Iteration 75/1000 | Loss: 0.00001004
Iteration 76/1000 | Loss: 0.00001004
Iteration 77/1000 | Loss: 0.00001004
Iteration 78/1000 | Loss: 0.00001004
Iteration 79/1000 | Loss: 0.00001004
Iteration 80/1000 | Loss: 0.00001004
Iteration 81/1000 | Loss: 0.00001003
Iteration 82/1000 | Loss: 0.00001002
Iteration 83/1000 | Loss: 0.00001002
Iteration 84/1000 | Loss: 0.00001002
Iteration 85/1000 | Loss: 0.00001002
Iteration 86/1000 | Loss: 0.00001001
Iteration 87/1000 | Loss: 0.00001001
Iteration 88/1000 | Loss: 0.00001001
Iteration 89/1000 | Loss: 0.00001001
Iteration 90/1000 | Loss: 0.00001001
Iteration 91/1000 | Loss: 0.00001001
Iteration 92/1000 | Loss: 0.00001001
Iteration 93/1000 | Loss: 0.00001000
Iteration 94/1000 | Loss: 0.00001000
Iteration 95/1000 | Loss: 0.00000999
Iteration 96/1000 | Loss: 0.00000999
Iteration 97/1000 | Loss: 0.00000999
Iteration 98/1000 | Loss: 0.00000998
Iteration 99/1000 | Loss: 0.00000998
Iteration 100/1000 | Loss: 0.00000998
Iteration 101/1000 | Loss: 0.00000998
Iteration 102/1000 | Loss: 0.00000998
Iteration 103/1000 | Loss: 0.00000998
Iteration 104/1000 | Loss: 0.00000998
Iteration 105/1000 | Loss: 0.00000998
Iteration 106/1000 | Loss: 0.00000998
Iteration 107/1000 | Loss: 0.00000998
Iteration 108/1000 | Loss: 0.00000998
Iteration 109/1000 | Loss: 0.00000998
Iteration 110/1000 | Loss: 0.00000998
Iteration 111/1000 | Loss: 0.00000998
Iteration 112/1000 | Loss: 0.00000998
Iteration 113/1000 | Loss: 0.00000997
Iteration 114/1000 | Loss: 0.00000997
Iteration 115/1000 | Loss: 0.00000997
Iteration 116/1000 | Loss: 0.00000997
Iteration 117/1000 | Loss: 0.00000997
Iteration 118/1000 | Loss: 0.00000997
Iteration 119/1000 | Loss: 0.00000997
Iteration 120/1000 | Loss: 0.00000997
Iteration 121/1000 | Loss: 0.00000996
Iteration 122/1000 | Loss: 0.00000996
Iteration 123/1000 | Loss: 0.00000996
Iteration 124/1000 | Loss: 0.00000996
Iteration 125/1000 | Loss: 0.00000996
Iteration 126/1000 | Loss: 0.00000996
Iteration 127/1000 | Loss: 0.00000995
Iteration 128/1000 | Loss: 0.00000995
Iteration 129/1000 | Loss: 0.00000995
Iteration 130/1000 | Loss: 0.00000995
Iteration 131/1000 | Loss: 0.00000995
Iteration 132/1000 | Loss: 0.00000995
Iteration 133/1000 | Loss: 0.00000995
Iteration 134/1000 | Loss: 0.00000995
Iteration 135/1000 | Loss: 0.00000995
Iteration 136/1000 | Loss: 0.00000995
Iteration 137/1000 | Loss: 0.00000994
Iteration 138/1000 | Loss: 0.00000994
Iteration 139/1000 | Loss: 0.00000994
Iteration 140/1000 | Loss: 0.00000994
Iteration 141/1000 | Loss: 0.00000994
Iteration 142/1000 | Loss: 0.00000993
Iteration 143/1000 | Loss: 0.00000993
Iteration 144/1000 | Loss: 0.00000993
Iteration 145/1000 | Loss: 0.00000993
Iteration 146/1000 | Loss: 0.00000993
Iteration 147/1000 | Loss: 0.00000993
Iteration 148/1000 | Loss: 0.00000993
Iteration 149/1000 | Loss: 0.00000993
Iteration 150/1000 | Loss: 0.00000993
Iteration 151/1000 | Loss: 0.00000993
Iteration 152/1000 | Loss: 0.00000993
Iteration 153/1000 | Loss: 0.00000993
Iteration 154/1000 | Loss: 0.00000993
Iteration 155/1000 | Loss: 0.00000993
Iteration 156/1000 | Loss: 0.00000992
Iteration 157/1000 | Loss: 0.00000992
Iteration 158/1000 | Loss: 0.00000992
Iteration 159/1000 | Loss: 0.00000992
Iteration 160/1000 | Loss: 0.00000992
Iteration 161/1000 | Loss: 0.00000992
Iteration 162/1000 | Loss: 0.00000992
Iteration 163/1000 | Loss: 0.00000991
Iteration 164/1000 | Loss: 0.00000991
Iteration 165/1000 | Loss: 0.00000991
Iteration 166/1000 | Loss: 0.00000991
Iteration 167/1000 | Loss: 0.00000991
Iteration 168/1000 | Loss: 0.00000991
Iteration 169/1000 | Loss: 0.00000991
Iteration 170/1000 | Loss: 0.00000991
Iteration 171/1000 | Loss: 0.00000990
Iteration 172/1000 | Loss: 0.00000990
Iteration 173/1000 | Loss: 0.00000990
Iteration 174/1000 | Loss: 0.00000990
Iteration 175/1000 | Loss: 0.00000989
Iteration 176/1000 | Loss: 0.00000989
Iteration 177/1000 | Loss: 0.00000989
Iteration 178/1000 | Loss: 0.00000989
Iteration 179/1000 | Loss: 0.00000989
Iteration 180/1000 | Loss: 0.00000989
Iteration 181/1000 | Loss: 0.00000989
Iteration 182/1000 | Loss: 0.00000989
Iteration 183/1000 | Loss: 0.00000989
Iteration 184/1000 | Loss: 0.00000988
Iteration 185/1000 | Loss: 0.00000988
Iteration 186/1000 | Loss: 0.00000988
Iteration 187/1000 | Loss: 0.00000988
Iteration 188/1000 | Loss: 0.00000988
Iteration 189/1000 | Loss: 0.00000988
Iteration 190/1000 | Loss: 0.00000988
Iteration 191/1000 | Loss: 0.00000988
Iteration 192/1000 | Loss: 0.00000988
Iteration 193/1000 | Loss: 0.00000988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [9.884789506031666e-06, 9.884789506031666e-06, 9.884789506031666e-06, 9.884789506031666e-06, 9.884789506031666e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.884789506031666e-06

Optimization complete. Final v2v error: 2.699841022491455 mm

Highest mean error: 3.46364426612854 mm for frame 70

Lowest mean error: 2.3937833309173584 mm for frame 28

Saving results

Total time: 41.07381844520569
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823227
Iteration 2/25 | Loss: 0.00125282
Iteration 3/25 | Loss: 0.00114882
Iteration 4/25 | Loss: 0.00113747
Iteration 5/25 | Loss: 0.00113530
Iteration 6/25 | Loss: 0.00113519
Iteration 7/25 | Loss: 0.00113519
Iteration 8/25 | Loss: 0.00113519
Iteration 9/25 | Loss: 0.00113519
Iteration 10/25 | Loss: 0.00113519
Iteration 11/25 | Loss: 0.00113519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011351873399689794, 0.0011351873399689794, 0.0011351873399689794, 0.0011351873399689794, 0.0011351873399689794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011351873399689794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24369431
Iteration 2/25 | Loss: 0.00166691
Iteration 3/25 | Loss: 0.00166689
Iteration 4/25 | Loss: 0.00166689
Iteration 5/25 | Loss: 0.00166689
Iteration 6/25 | Loss: 0.00166689
Iteration 7/25 | Loss: 0.00166689
Iteration 8/25 | Loss: 0.00166688
Iteration 9/25 | Loss: 0.00166688
Iteration 10/25 | Loss: 0.00166688
Iteration 11/25 | Loss: 0.00166688
Iteration 12/25 | Loss: 0.00166688
Iteration 13/25 | Loss: 0.00166688
Iteration 14/25 | Loss: 0.00166688
Iteration 15/25 | Loss: 0.00166688
Iteration 16/25 | Loss: 0.00166688
Iteration 17/25 | Loss: 0.00166688
Iteration 18/25 | Loss: 0.00166688
Iteration 19/25 | Loss: 0.00166688
Iteration 20/25 | Loss: 0.00166688
Iteration 21/25 | Loss: 0.00166688
Iteration 22/25 | Loss: 0.00166688
Iteration 23/25 | Loss: 0.00166688
Iteration 24/25 | Loss: 0.00166688
Iteration 25/25 | Loss: 0.00166688
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0016668838215991855, 0.0016668838215991855, 0.0016668838215991855, 0.0016668838215991855, 0.0016668838215991855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016668838215991855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166688
Iteration 2/1000 | Loss: 0.00002033
Iteration 3/1000 | Loss: 0.00001347
Iteration 4/1000 | Loss: 0.00001135
Iteration 5/1000 | Loss: 0.00001047
Iteration 6/1000 | Loss: 0.00000980
Iteration 7/1000 | Loss: 0.00000935
Iteration 8/1000 | Loss: 0.00000892
Iteration 9/1000 | Loss: 0.00000872
Iteration 10/1000 | Loss: 0.00000869
Iteration 11/1000 | Loss: 0.00000849
Iteration 12/1000 | Loss: 0.00000837
Iteration 13/1000 | Loss: 0.00000836
Iteration 14/1000 | Loss: 0.00000829
Iteration 15/1000 | Loss: 0.00000829
Iteration 16/1000 | Loss: 0.00000828
Iteration 17/1000 | Loss: 0.00000828
Iteration 18/1000 | Loss: 0.00000824
Iteration 19/1000 | Loss: 0.00000821
Iteration 20/1000 | Loss: 0.00000819
Iteration 21/1000 | Loss: 0.00000818
Iteration 22/1000 | Loss: 0.00000814
Iteration 23/1000 | Loss: 0.00000813
Iteration 24/1000 | Loss: 0.00000812
Iteration 25/1000 | Loss: 0.00000811
Iteration 26/1000 | Loss: 0.00000810
Iteration 27/1000 | Loss: 0.00000809
Iteration 28/1000 | Loss: 0.00000809
Iteration 29/1000 | Loss: 0.00000808
Iteration 30/1000 | Loss: 0.00000808
Iteration 31/1000 | Loss: 0.00000806
Iteration 32/1000 | Loss: 0.00000805
Iteration 33/1000 | Loss: 0.00000803
Iteration 34/1000 | Loss: 0.00000802
Iteration 35/1000 | Loss: 0.00000802
Iteration 36/1000 | Loss: 0.00000802
Iteration 37/1000 | Loss: 0.00000802
Iteration 38/1000 | Loss: 0.00000797
Iteration 39/1000 | Loss: 0.00000797
Iteration 40/1000 | Loss: 0.00000796
Iteration 41/1000 | Loss: 0.00000796
Iteration 42/1000 | Loss: 0.00000796
Iteration 43/1000 | Loss: 0.00000795
Iteration 44/1000 | Loss: 0.00000795
Iteration 45/1000 | Loss: 0.00000794
Iteration 46/1000 | Loss: 0.00000793
Iteration 47/1000 | Loss: 0.00000791
Iteration 48/1000 | Loss: 0.00000791
Iteration 49/1000 | Loss: 0.00000791
Iteration 50/1000 | Loss: 0.00000790
Iteration 51/1000 | Loss: 0.00000790
Iteration 52/1000 | Loss: 0.00000790
Iteration 53/1000 | Loss: 0.00000790
Iteration 54/1000 | Loss: 0.00000789
Iteration 55/1000 | Loss: 0.00000788
Iteration 56/1000 | Loss: 0.00000788
Iteration 57/1000 | Loss: 0.00000787
Iteration 58/1000 | Loss: 0.00000787
Iteration 59/1000 | Loss: 0.00000787
Iteration 60/1000 | Loss: 0.00000787
Iteration 61/1000 | Loss: 0.00000787
Iteration 62/1000 | Loss: 0.00000787
Iteration 63/1000 | Loss: 0.00000786
Iteration 64/1000 | Loss: 0.00000786
Iteration 65/1000 | Loss: 0.00000786
Iteration 66/1000 | Loss: 0.00000786
Iteration 67/1000 | Loss: 0.00000786
Iteration 68/1000 | Loss: 0.00000786
Iteration 69/1000 | Loss: 0.00000785
Iteration 70/1000 | Loss: 0.00000785
Iteration 71/1000 | Loss: 0.00000785
Iteration 72/1000 | Loss: 0.00000785
Iteration 73/1000 | Loss: 0.00000785
Iteration 74/1000 | Loss: 0.00000785
Iteration 75/1000 | Loss: 0.00000785
Iteration 76/1000 | Loss: 0.00000785
Iteration 77/1000 | Loss: 0.00000785
Iteration 78/1000 | Loss: 0.00000784
Iteration 79/1000 | Loss: 0.00000784
Iteration 80/1000 | Loss: 0.00000784
Iteration 81/1000 | Loss: 0.00000784
Iteration 82/1000 | Loss: 0.00000783
Iteration 83/1000 | Loss: 0.00000783
Iteration 84/1000 | Loss: 0.00000783
Iteration 85/1000 | Loss: 0.00000782
Iteration 86/1000 | Loss: 0.00000782
Iteration 87/1000 | Loss: 0.00000781
Iteration 88/1000 | Loss: 0.00000781
Iteration 89/1000 | Loss: 0.00000781
Iteration 90/1000 | Loss: 0.00000781
Iteration 91/1000 | Loss: 0.00000780
Iteration 92/1000 | Loss: 0.00000780
Iteration 93/1000 | Loss: 0.00000780
Iteration 94/1000 | Loss: 0.00000780
Iteration 95/1000 | Loss: 0.00000779
Iteration 96/1000 | Loss: 0.00000779
Iteration 97/1000 | Loss: 0.00000779
Iteration 98/1000 | Loss: 0.00000779
Iteration 99/1000 | Loss: 0.00000778
Iteration 100/1000 | Loss: 0.00000778
Iteration 101/1000 | Loss: 0.00000778
Iteration 102/1000 | Loss: 0.00000778
Iteration 103/1000 | Loss: 0.00000778
Iteration 104/1000 | Loss: 0.00000778
Iteration 105/1000 | Loss: 0.00000777
Iteration 106/1000 | Loss: 0.00000777
Iteration 107/1000 | Loss: 0.00000777
Iteration 108/1000 | Loss: 0.00000776
Iteration 109/1000 | Loss: 0.00000776
Iteration 110/1000 | Loss: 0.00000776
Iteration 111/1000 | Loss: 0.00000776
Iteration 112/1000 | Loss: 0.00000776
Iteration 113/1000 | Loss: 0.00000775
Iteration 114/1000 | Loss: 0.00000775
Iteration 115/1000 | Loss: 0.00000775
Iteration 116/1000 | Loss: 0.00000774
Iteration 117/1000 | Loss: 0.00000774
Iteration 118/1000 | Loss: 0.00000774
Iteration 119/1000 | Loss: 0.00000774
Iteration 120/1000 | Loss: 0.00000773
Iteration 121/1000 | Loss: 0.00000773
Iteration 122/1000 | Loss: 0.00000772
Iteration 123/1000 | Loss: 0.00000772
Iteration 124/1000 | Loss: 0.00000772
Iteration 125/1000 | Loss: 0.00000772
Iteration 126/1000 | Loss: 0.00000771
Iteration 127/1000 | Loss: 0.00000771
Iteration 128/1000 | Loss: 0.00000771
Iteration 129/1000 | Loss: 0.00000771
Iteration 130/1000 | Loss: 0.00000771
Iteration 131/1000 | Loss: 0.00000771
Iteration 132/1000 | Loss: 0.00000771
Iteration 133/1000 | Loss: 0.00000771
Iteration 134/1000 | Loss: 0.00000771
Iteration 135/1000 | Loss: 0.00000771
Iteration 136/1000 | Loss: 0.00000770
Iteration 137/1000 | Loss: 0.00000770
Iteration 138/1000 | Loss: 0.00000770
Iteration 139/1000 | Loss: 0.00000770
Iteration 140/1000 | Loss: 0.00000770
Iteration 141/1000 | Loss: 0.00000769
Iteration 142/1000 | Loss: 0.00000769
Iteration 143/1000 | Loss: 0.00000769
Iteration 144/1000 | Loss: 0.00000769
Iteration 145/1000 | Loss: 0.00000769
Iteration 146/1000 | Loss: 0.00000769
Iteration 147/1000 | Loss: 0.00000769
Iteration 148/1000 | Loss: 0.00000769
Iteration 149/1000 | Loss: 0.00000769
Iteration 150/1000 | Loss: 0.00000768
Iteration 151/1000 | Loss: 0.00000768
Iteration 152/1000 | Loss: 0.00000768
Iteration 153/1000 | Loss: 0.00000768
Iteration 154/1000 | Loss: 0.00000768
Iteration 155/1000 | Loss: 0.00000768
Iteration 156/1000 | Loss: 0.00000768
Iteration 157/1000 | Loss: 0.00000768
Iteration 158/1000 | Loss: 0.00000768
Iteration 159/1000 | Loss: 0.00000768
Iteration 160/1000 | Loss: 0.00000768
Iteration 161/1000 | Loss: 0.00000768
Iteration 162/1000 | Loss: 0.00000768
Iteration 163/1000 | Loss: 0.00000768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [7.678310794290155e-06, 7.678310794290155e-06, 7.678310794290155e-06, 7.678310794290155e-06, 7.678310794290155e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.678310794290155e-06

Optimization complete. Final v2v error: 2.4181532859802246 mm

Highest mean error: 2.5728468894958496 mm for frame 18

Lowest mean error: 2.2903659343719482 mm for frame 40

Saving results

Total time: 36.79268264770508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976520
Iteration 2/25 | Loss: 0.00976520
Iteration 3/25 | Loss: 0.00377279
Iteration 4/25 | Loss: 0.00220537
Iteration 5/25 | Loss: 0.00195794
Iteration 6/25 | Loss: 0.00182592
Iteration 7/25 | Loss: 0.00179698
Iteration 8/25 | Loss: 0.00175335
Iteration 9/25 | Loss: 0.00173851
Iteration 10/25 | Loss: 0.00172958
Iteration 11/25 | Loss: 0.00171644
Iteration 12/25 | Loss: 0.00171468
Iteration 13/25 | Loss: 0.00170953
Iteration 14/25 | Loss: 0.00170796
Iteration 15/25 | Loss: 0.00170714
Iteration 16/25 | Loss: 0.00169846
Iteration 17/25 | Loss: 0.00169761
Iteration 18/25 | Loss: 0.00169605
Iteration 19/25 | Loss: 0.00169798
Iteration 20/25 | Loss: 0.00169396
Iteration 21/25 | Loss: 0.00170201
Iteration 22/25 | Loss: 0.00169393
Iteration 23/25 | Loss: 0.00172873
Iteration 24/25 | Loss: 0.00165987
Iteration 25/25 | Loss: 0.00160506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22190857
Iteration 2/25 | Loss: 0.00685256
Iteration 3/25 | Loss: 0.00658669
Iteration 4/25 | Loss: 0.00658664
Iteration 5/25 | Loss: 0.00658663
Iteration 6/25 | Loss: 0.00658663
Iteration 7/25 | Loss: 0.00658663
Iteration 8/25 | Loss: 0.00658663
Iteration 9/25 | Loss: 0.00658663
Iteration 10/25 | Loss: 0.00658663
Iteration 11/25 | Loss: 0.00658663
Iteration 12/25 | Loss: 0.00658663
Iteration 13/25 | Loss: 0.00658663
Iteration 14/25 | Loss: 0.00658663
Iteration 15/25 | Loss: 0.00658663
Iteration 16/25 | Loss: 0.00658663
Iteration 17/25 | Loss: 0.00658663
Iteration 18/25 | Loss: 0.00658663
Iteration 19/25 | Loss: 0.00658663
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.006586630363017321, 0.006586630363017321, 0.006586630363017321, 0.006586630363017321, 0.006586630363017321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006586630363017321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00658663
Iteration 2/1000 | Loss: 0.00162424
Iteration 3/1000 | Loss: 0.00101952
Iteration 4/1000 | Loss: 0.00065472
Iteration 5/1000 | Loss: 0.00072149
Iteration 6/1000 | Loss: 0.00102971
Iteration 7/1000 | Loss: 0.00118406
Iteration 8/1000 | Loss: 0.00081714
Iteration 9/1000 | Loss: 0.00153308
Iteration 10/1000 | Loss: 0.00072545
Iteration 11/1000 | Loss: 0.00075837
Iteration 12/1000 | Loss: 0.00059053
Iteration 13/1000 | Loss: 0.00101439
Iteration 14/1000 | Loss: 0.00038916
Iteration 15/1000 | Loss: 0.00025834
Iteration 16/1000 | Loss: 0.00072300
Iteration 17/1000 | Loss: 0.00060336
Iteration 18/1000 | Loss: 0.00045115
Iteration 19/1000 | Loss: 0.00046629
Iteration 20/1000 | Loss: 0.00035308
Iteration 21/1000 | Loss: 0.00092390
Iteration 22/1000 | Loss: 0.00072598
Iteration 23/1000 | Loss: 0.00041334
Iteration 24/1000 | Loss: 0.00029668
Iteration 25/1000 | Loss: 0.00051065
Iteration 26/1000 | Loss: 0.00044795
Iteration 27/1000 | Loss: 0.00046192
Iteration 28/1000 | Loss: 0.00056221
Iteration 29/1000 | Loss: 0.00049746
Iteration 30/1000 | Loss: 0.00026205
Iteration 31/1000 | Loss: 0.00036983
Iteration 32/1000 | Loss: 0.00067975
Iteration 33/1000 | Loss: 0.00035054
Iteration 34/1000 | Loss: 0.00057367
Iteration 35/1000 | Loss: 0.00050670
Iteration 36/1000 | Loss: 0.00220229
Iteration 37/1000 | Loss: 0.00047523
Iteration 38/1000 | Loss: 0.00019516
Iteration 39/1000 | Loss: 0.00064784
Iteration 40/1000 | Loss: 0.00055980
Iteration 41/1000 | Loss: 0.00107590
Iteration 42/1000 | Loss: 0.00055329
Iteration 43/1000 | Loss: 0.00057070
Iteration 44/1000 | Loss: 0.00035214
Iteration 45/1000 | Loss: 0.00039022
Iteration 46/1000 | Loss: 0.00045250
Iteration 47/1000 | Loss: 0.00057715
Iteration 48/1000 | Loss: 0.00017916
Iteration 49/1000 | Loss: 0.00027375
Iteration 50/1000 | Loss: 0.00085045
Iteration 51/1000 | Loss: 0.00019716
Iteration 52/1000 | Loss: 0.00025162
Iteration 53/1000 | Loss: 0.00027331
Iteration 54/1000 | Loss: 0.00033738
Iteration 55/1000 | Loss: 0.00018960
Iteration 56/1000 | Loss: 0.00027886
Iteration 57/1000 | Loss: 0.00023257
Iteration 58/1000 | Loss: 0.00024760
Iteration 59/1000 | Loss: 0.00111349
Iteration 60/1000 | Loss: 0.00022993
Iteration 61/1000 | Loss: 0.00059703
Iteration 62/1000 | Loss: 0.00017298
Iteration 63/1000 | Loss: 0.00028393
Iteration 64/1000 | Loss: 0.00071989
Iteration 65/1000 | Loss: 0.00101191
Iteration 66/1000 | Loss: 0.00070532
Iteration 67/1000 | Loss: 0.00028911
Iteration 68/1000 | Loss: 0.00020078
Iteration 69/1000 | Loss: 0.00024872
Iteration 70/1000 | Loss: 0.00016464
Iteration 71/1000 | Loss: 0.00027412
Iteration 72/1000 | Loss: 0.00029613
Iteration 73/1000 | Loss: 0.00030035
Iteration 74/1000 | Loss: 0.00028054
Iteration 75/1000 | Loss: 0.00021581
Iteration 76/1000 | Loss: 0.00024240
Iteration 77/1000 | Loss: 0.00018972
Iteration 78/1000 | Loss: 0.00033501
Iteration 79/1000 | Loss: 0.00029768
Iteration 80/1000 | Loss: 0.00071504
Iteration 81/1000 | Loss: 0.00063514
Iteration 82/1000 | Loss: 0.00051997
Iteration 83/1000 | Loss: 0.00016840
Iteration 84/1000 | Loss: 0.00025721
Iteration 85/1000 | Loss: 0.00026475
Iteration 86/1000 | Loss: 0.00014832
Iteration 87/1000 | Loss: 0.00025917
Iteration 88/1000 | Loss: 0.00040037
Iteration 89/1000 | Loss: 0.00047323
Iteration 90/1000 | Loss: 0.00037298
Iteration 91/1000 | Loss: 0.00042840
Iteration 92/1000 | Loss: 0.00026795
Iteration 93/1000 | Loss: 0.00066499
Iteration 94/1000 | Loss: 0.00028997
Iteration 95/1000 | Loss: 0.00046331
Iteration 96/1000 | Loss: 0.00021961
Iteration 97/1000 | Loss: 0.00020855
Iteration 98/1000 | Loss: 0.00014060
Iteration 99/1000 | Loss: 0.00021491
Iteration 100/1000 | Loss: 0.00015230
Iteration 101/1000 | Loss: 0.00030836
Iteration 102/1000 | Loss: 0.00022579
Iteration 103/1000 | Loss: 0.00013215
Iteration 104/1000 | Loss: 0.00021285
Iteration 105/1000 | Loss: 0.00029754
Iteration 106/1000 | Loss: 0.00022353
Iteration 107/1000 | Loss: 0.00023844
Iteration 108/1000 | Loss: 0.00024697
Iteration 109/1000 | Loss: 0.00013287
Iteration 110/1000 | Loss: 0.00026449
Iteration 111/1000 | Loss: 0.00020457
Iteration 112/1000 | Loss: 0.00014364
Iteration 113/1000 | Loss: 0.00016682
Iteration 114/1000 | Loss: 0.00050449
Iteration 115/1000 | Loss: 0.00022935
Iteration 116/1000 | Loss: 0.00041372
Iteration 117/1000 | Loss: 0.00068163
Iteration 118/1000 | Loss: 0.00014077
Iteration 119/1000 | Loss: 0.00028836
Iteration 120/1000 | Loss: 0.00021272
Iteration 121/1000 | Loss: 0.00034319
Iteration 122/1000 | Loss: 0.00019712
Iteration 123/1000 | Loss: 0.00013570
Iteration 124/1000 | Loss: 0.00018137
Iteration 125/1000 | Loss: 0.00018807
Iteration 126/1000 | Loss: 0.00021188
Iteration 127/1000 | Loss: 0.00022437
Iteration 128/1000 | Loss: 0.00027749
Iteration 129/1000 | Loss: 0.00022792
Iteration 130/1000 | Loss: 0.00015070
Iteration 131/1000 | Loss: 0.00021850
Iteration 132/1000 | Loss: 0.00020575
Iteration 133/1000 | Loss: 0.00019360
Iteration 134/1000 | Loss: 0.00019832
Iteration 135/1000 | Loss: 0.00021811
Iteration 136/1000 | Loss: 0.00024443
Iteration 137/1000 | Loss: 0.00020002
Iteration 138/1000 | Loss: 0.00021675
Iteration 139/1000 | Loss: 0.00014908
Iteration 140/1000 | Loss: 0.00019694
Iteration 141/1000 | Loss: 0.00013201
Iteration 142/1000 | Loss: 0.00023341
Iteration 143/1000 | Loss: 0.00018081
Iteration 144/1000 | Loss: 0.00024977
Iteration 145/1000 | Loss: 0.00019231
Iteration 146/1000 | Loss: 0.00025611
Iteration 147/1000 | Loss: 0.00019497
Iteration 148/1000 | Loss: 0.00012790
Iteration 149/1000 | Loss: 0.00012532
Iteration 150/1000 | Loss: 0.00012394
Iteration 151/1000 | Loss: 0.00035532
Iteration 152/1000 | Loss: 0.00016899
Iteration 153/1000 | Loss: 0.00019540
Iteration 154/1000 | Loss: 0.00012357
Iteration 155/1000 | Loss: 0.00012174
Iteration 156/1000 | Loss: 0.00013056
Iteration 157/1000 | Loss: 0.00012349
Iteration 158/1000 | Loss: 0.00011923
Iteration 159/1000 | Loss: 0.00012955
Iteration 160/1000 | Loss: 0.00016446
Iteration 161/1000 | Loss: 0.00014071
Iteration 162/1000 | Loss: 0.00011799
Iteration 163/1000 | Loss: 0.00011802
Iteration 164/1000 | Loss: 0.00028374
Iteration 165/1000 | Loss: 0.00020885
Iteration 166/1000 | Loss: 0.00037286
Iteration 167/1000 | Loss: 0.00012832
Iteration 168/1000 | Loss: 0.00012125
Iteration 169/1000 | Loss: 0.00011861
Iteration 170/1000 | Loss: 0.00011630
Iteration 171/1000 | Loss: 0.00068021
Iteration 172/1000 | Loss: 0.00036377
Iteration 173/1000 | Loss: 0.00028722
Iteration 174/1000 | Loss: 0.00011704
Iteration 175/1000 | Loss: 0.00011512
Iteration 176/1000 | Loss: 0.00011395
Iteration 177/1000 | Loss: 0.00011325
Iteration 178/1000 | Loss: 0.00011494
Iteration 179/1000 | Loss: 0.00011409
Iteration 180/1000 | Loss: 0.00011051
Iteration 181/1000 | Loss: 0.00011032
Iteration 182/1000 | Loss: 0.00011007
Iteration 183/1000 | Loss: 0.00010976
Iteration 184/1000 | Loss: 0.00011103
Iteration 185/1000 | Loss: 0.00010988
Iteration 186/1000 | Loss: 0.00014265
Iteration 187/1000 | Loss: 0.00010981
Iteration 188/1000 | Loss: 0.00010901
Iteration 189/1000 | Loss: 0.00010884
Iteration 190/1000 | Loss: 0.00010876
Iteration 191/1000 | Loss: 0.00095244
Iteration 192/1000 | Loss: 0.00017339
Iteration 193/1000 | Loss: 0.00013243
Iteration 194/1000 | Loss: 0.00011045
Iteration 195/1000 | Loss: 0.00010900
Iteration 196/1000 | Loss: 0.00010695
Iteration 197/1000 | Loss: 0.00012736
Iteration 198/1000 | Loss: 0.00019087
Iteration 199/1000 | Loss: 0.00023997
Iteration 200/1000 | Loss: 0.00021299
Iteration 201/1000 | Loss: 0.00021206
Iteration 202/1000 | Loss: 0.00012097
Iteration 203/1000 | Loss: 0.00010650
Iteration 204/1000 | Loss: 0.00010447
Iteration 205/1000 | Loss: 0.00010441
Iteration 206/1000 | Loss: 0.00010602
Iteration 207/1000 | Loss: 0.00010393
Iteration 208/1000 | Loss: 0.00010392
Iteration 209/1000 | Loss: 0.00012137
Iteration 210/1000 | Loss: 0.00032650
Iteration 211/1000 | Loss: 0.00011299
Iteration 212/1000 | Loss: 0.00010714
Iteration 213/1000 | Loss: 0.00011805
Iteration 214/1000 | Loss: 0.00010436
Iteration 215/1000 | Loss: 0.00018642
Iteration 216/1000 | Loss: 0.00010709
Iteration 217/1000 | Loss: 0.00010670
Iteration 218/1000 | Loss: 0.00010348
Iteration 219/1000 | Loss: 0.00017742
Iteration 220/1000 | Loss: 0.00012455
Iteration 221/1000 | Loss: 0.00019936
Iteration 222/1000 | Loss: 0.00012565
Iteration 223/1000 | Loss: 0.00018019
Iteration 224/1000 | Loss: 0.00010647
Iteration 225/1000 | Loss: 0.00011735
Iteration 226/1000 | Loss: 0.00018451
Iteration 227/1000 | Loss: 0.00010560
Iteration 228/1000 | Loss: 0.00010316
Iteration 229/1000 | Loss: 0.00015731
Iteration 230/1000 | Loss: 0.00018403
Iteration 231/1000 | Loss: 0.00014694
Iteration 232/1000 | Loss: 0.00017678
Iteration 233/1000 | Loss: 0.00018435
Iteration 234/1000 | Loss: 0.00020300
Iteration 235/1000 | Loss: 0.00020223
Iteration 236/1000 | Loss: 0.00019067
Iteration 237/1000 | Loss: 0.00015934
Iteration 238/1000 | Loss: 0.00011851
Iteration 239/1000 | Loss: 0.00020019
Iteration 240/1000 | Loss: 0.00018654
Iteration 241/1000 | Loss: 0.00013739
Iteration 242/1000 | Loss: 0.00017323
Iteration 243/1000 | Loss: 0.00015628
Iteration 244/1000 | Loss: 0.00020778
Iteration 245/1000 | Loss: 0.00014676
Iteration 246/1000 | Loss: 0.00018449
Iteration 247/1000 | Loss: 0.00013885
Iteration 248/1000 | Loss: 0.00010344
Iteration 249/1000 | Loss: 0.00011115
Iteration 250/1000 | Loss: 0.00010332
Iteration 251/1000 | Loss: 0.00010209
Iteration 252/1000 | Loss: 0.00010392
Iteration 253/1000 | Loss: 0.00010774
Iteration 254/1000 | Loss: 0.00010927
Iteration 255/1000 | Loss: 0.00010514
Iteration 256/1000 | Loss: 0.00010163
Iteration 257/1000 | Loss: 0.00012278
Iteration 258/1000 | Loss: 0.00010255
Iteration 259/1000 | Loss: 0.00033964
Iteration 260/1000 | Loss: 0.00063064
Iteration 261/1000 | Loss: 0.00021858
Iteration 262/1000 | Loss: 0.00030585
Iteration 263/1000 | Loss: 0.00017013
Iteration 264/1000 | Loss: 0.00013177
Iteration 265/1000 | Loss: 0.00012401
Iteration 266/1000 | Loss: 0.00017726
Iteration 267/1000 | Loss: 0.00016599
Iteration 268/1000 | Loss: 0.00017050
Iteration 269/1000 | Loss: 0.00022260
Iteration 270/1000 | Loss: 0.00021465
Iteration 271/1000 | Loss: 0.00018266
Iteration 272/1000 | Loss: 0.00019429
Iteration 273/1000 | Loss: 0.00019595
Iteration 274/1000 | Loss: 0.00018453
Iteration 275/1000 | Loss: 0.00026631
Iteration 276/1000 | Loss: 0.00015068
Iteration 277/1000 | Loss: 0.00014906
Iteration 278/1000 | Loss: 0.00018189
Iteration 279/1000 | Loss: 0.00010162
Iteration 280/1000 | Loss: 0.00010870
Iteration 281/1000 | Loss: 0.00009992
Iteration 282/1000 | Loss: 0.00010641
Iteration 283/1000 | Loss: 0.00009905
Iteration 284/1000 | Loss: 0.00010833
Iteration 285/1000 | Loss: 0.00009872
Iteration 286/1000 | Loss: 0.00010063
Iteration 287/1000 | Loss: 0.00011217
Iteration 288/1000 | Loss: 0.00009909
Iteration 289/1000 | Loss: 0.00009925
Iteration 290/1000 | Loss: 0.00009833
Iteration 291/1000 | Loss: 0.00009832
Iteration 292/1000 | Loss: 0.00009832
Iteration 293/1000 | Loss: 0.00009831
Iteration 294/1000 | Loss: 0.00009831
Iteration 295/1000 | Loss: 0.00009830
Iteration 296/1000 | Loss: 0.00033830
Iteration 297/1000 | Loss: 0.00014686
Iteration 298/1000 | Loss: 0.00025196
Iteration 299/1000 | Loss: 0.00010312
Iteration 300/1000 | Loss: 0.00011787
Iteration 301/1000 | Loss: 0.00010648
Iteration 302/1000 | Loss: 0.00010379
Iteration 303/1000 | Loss: 0.00035176
Iteration 304/1000 | Loss: 0.00071722
Iteration 305/1000 | Loss: 0.00049040
Iteration 306/1000 | Loss: 0.00044572
Iteration 307/1000 | Loss: 0.00025046
Iteration 308/1000 | Loss: 0.00058310
Iteration 309/1000 | Loss: 0.00024138
Iteration 310/1000 | Loss: 0.00037468
Iteration 311/1000 | Loss: 0.00012439
Iteration 312/1000 | Loss: 0.00011192
Iteration 313/1000 | Loss: 0.00010106
Iteration 314/1000 | Loss: 0.00010596
Iteration 315/1000 | Loss: 0.00009848
Iteration 316/1000 | Loss: 0.00011839
Iteration 317/1000 | Loss: 0.00012537
Iteration 318/1000 | Loss: 0.00009772
Iteration 319/1000 | Loss: 0.00011386
Iteration 320/1000 | Loss: 0.00036959
Iteration 321/1000 | Loss: 0.00092555
Iteration 322/1000 | Loss: 0.00024428
Iteration 323/1000 | Loss: 0.00018153
Iteration 324/1000 | Loss: 0.00023167
Iteration 325/1000 | Loss: 0.00011611
Iteration 326/1000 | Loss: 0.00009636
Iteration 327/1000 | Loss: 0.00009330
Iteration 328/1000 | Loss: 0.00009230
Iteration 329/1000 | Loss: 0.00010969
Iteration 330/1000 | Loss: 0.00009184
Iteration 331/1000 | Loss: 0.00009779
Iteration 332/1000 | Loss: 0.00009122
Iteration 333/1000 | Loss: 0.00009110
Iteration 334/1000 | Loss: 0.00009108
Iteration 335/1000 | Loss: 0.00009106
Iteration 336/1000 | Loss: 0.00009090
Iteration 337/1000 | Loss: 0.00009086
Iteration 338/1000 | Loss: 0.00009078
Iteration 339/1000 | Loss: 0.00009069
Iteration 340/1000 | Loss: 0.00009069
Iteration 341/1000 | Loss: 0.00009068
Iteration 342/1000 | Loss: 0.00009067
Iteration 343/1000 | Loss: 0.00009067
Iteration 344/1000 | Loss: 0.00009066
Iteration 345/1000 | Loss: 0.00009066
Iteration 346/1000 | Loss: 0.00009064
Iteration 347/1000 | Loss: 0.00009664
Iteration 348/1000 | Loss: 0.00014973
Iteration 349/1000 | Loss: 0.00010716
Iteration 350/1000 | Loss: 0.00011006
Iteration 351/1000 | Loss: 0.00010055
Iteration 352/1000 | Loss: 0.00013966
Iteration 353/1000 | Loss: 0.00016956
Iteration 354/1000 | Loss: 0.00009089
Iteration 355/1000 | Loss: 0.00013602
Iteration 356/1000 | Loss: 0.00033395
Iteration 357/1000 | Loss: 0.00036745
Iteration 358/1000 | Loss: 0.00016192
Iteration 359/1000 | Loss: 0.00009986
Iteration 360/1000 | Loss: 0.00015663
Iteration 361/1000 | Loss: 0.00012063
Iteration 362/1000 | Loss: 0.00009331
Iteration 363/1000 | Loss: 0.00009154
Iteration 364/1000 | Loss: 0.00009023
Iteration 365/1000 | Loss: 0.00009023
Iteration 366/1000 | Loss: 0.00009023
Iteration 367/1000 | Loss: 0.00009023
Iteration 368/1000 | Loss: 0.00009023
Iteration 369/1000 | Loss: 0.00009023
Iteration 370/1000 | Loss: 0.00009023
Iteration 371/1000 | Loss: 0.00009023
Iteration 372/1000 | Loss: 0.00009023
Iteration 373/1000 | Loss: 0.00009023
Iteration 374/1000 | Loss: 0.00009023
Iteration 375/1000 | Loss: 0.00009022
Iteration 376/1000 | Loss: 0.00009022
Iteration 377/1000 | Loss: 0.00009022
Iteration 378/1000 | Loss: 0.00009022
Iteration 379/1000 | Loss: 0.00009022
Iteration 380/1000 | Loss: 0.00009022
Iteration 381/1000 | Loss: 0.00009022
Iteration 382/1000 | Loss: 0.00009022
Iteration 383/1000 | Loss: 0.00009022
Iteration 384/1000 | Loss: 0.00009022
Iteration 385/1000 | Loss: 0.00009021
Iteration 386/1000 | Loss: 0.00009021
Iteration 387/1000 | Loss: 0.00009020
Iteration 388/1000 | Loss: 0.00009019
Iteration 389/1000 | Loss: 0.00009019
Iteration 390/1000 | Loss: 0.00009019
Iteration 391/1000 | Loss: 0.00009019
Iteration 392/1000 | Loss: 0.00009019
Iteration 393/1000 | Loss: 0.00009019
Iteration 394/1000 | Loss: 0.00009019
Iteration 395/1000 | Loss: 0.00036554
Iteration 396/1000 | Loss: 0.00017664
Iteration 397/1000 | Loss: 0.00021981
Iteration 398/1000 | Loss: 0.00009520
Iteration 399/1000 | Loss: 0.00012021
Iteration 400/1000 | Loss: 0.00009191
Iteration 401/1000 | Loss: 0.00010586
Iteration 402/1000 | Loss: 0.00009016
Iteration 403/1000 | Loss: 0.00009274
Iteration 404/1000 | Loss: 0.00014938
Iteration 405/1000 | Loss: 0.00009519
Iteration 406/1000 | Loss: 0.00008910
Iteration 407/1000 | Loss: 0.00008906
Iteration 408/1000 | Loss: 0.00013833
Iteration 409/1000 | Loss: 0.00010957
Iteration 410/1000 | Loss: 0.00008953
Iteration 411/1000 | Loss: 0.00008950
Iteration 412/1000 | Loss: 0.00008876
Iteration 413/1000 | Loss: 0.00008876
Iteration 414/1000 | Loss: 0.00008876
Iteration 415/1000 | Loss: 0.00008876
Iteration 416/1000 | Loss: 0.00008876
Iteration 417/1000 | Loss: 0.00008876
Iteration 418/1000 | Loss: 0.00008876
Iteration 419/1000 | Loss: 0.00008876
Iteration 420/1000 | Loss: 0.00008876
Iteration 421/1000 | Loss: 0.00008875
Iteration 422/1000 | Loss: 0.00008875
Iteration 423/1000 | Loss: 0.00008875
Iteration 424/1000 | Loss: 0.00008875
Iteration 425/1000 | Loss: 0.00008875
Iteration 426/1000 | Loss: 0.00008874
Iteration 427/1000 | Loss: 0.00009038
Iteration 428/1000 | Loss: 0.00008872
Iteration 429/1000 | Loss: 0.00008872
Iteration 430/1000 | Loss: 0.00008871
Iteration 431/1000 | Loss: 0.00008871
Iteration 432/1000 | Loss: 0.00008870
Iteration 433/1000 | Loss: 0.00008884
Iteration 434/1000 | Loss: 0.00008907
Iteration 435/1000 | Loss: 0.00008860
Iteration 436/1000 | Loss: 0.00008860
Iteration 437/1000 | Loss: 0.00008860
Iteration 438/1000 | Loss: 0.00008860
Iteration 439/1000 | Loss: 0.00008860
Iteration 440/1000 | Loss: 0.00008860
Iteration 441/1000 | Loss: 0.00008860
Iteration 442/1000 | Loss: 0.00008860
Iteration 443/1000 | Loss: 0.00008860
Iteration 444/1000 | Loss: 0.00008860
Iteration 445/1000 | Loss: 0.00008860
Iteration 446/1000 | Loss: 0.00008859
Iteration 447/1000 | Loss: 0.00008859
Iteration 448/1000 | Loss: 0.00013577
Iteration 449/1000 | Loss: 0.00020191
Iteration 450/1000 | Loss: 0.00063364
Iteration 451/1000 | Loss: 0.00019403
Iteration 452/1000 | Loss: 0.00021201
Iteration 453/1000 | Loss: 0.00020617
Iteration 454/1000 | Loss: 0.00018679
Iteration 455/1000 | Loss: 0.00016294
Iteration 456/1000 | Loss: 0.00014731
Iteration 457/1000 | Loss: 0.00014971
Iteration 458/1000 | Loss: 0.00016690
Iteration 459/1000 | Loss: 0.00013687
Iteration 460/1000 | Loss: 0.00025483
Iteration 461/1000 | Loss: 0.00014783
Iteration 462/1000 | Loss: 0.00015488
Iteration 463/1000 | Loss: 0.00018437
Iteration 464/1000 | Loss: 0.00026160
Iteration 465/1000 | Loss: 0.00022737
Iteration 466/1000 | Loss: 0.00019918
Iteration 467/1000 | Loss: 0.00022823
Iteration 468/1000 | Loss: 0.00025228
Iteration 469/1000 | Loss: 0.00022617
Iteration 470/1000 | Loss: 0.00024001
Iteration 471/1000 | Loss: 0.00013993
Iteration 472/1000 | Loss: 0.00019598
Iteration 473/1000 | Loss: 0.00014726
Iteration 474/1000 | Loss: 0.00018215
Iteration 475/1000 | Loss: 0.00015074
Iteration 476/1000 | Loss: 0.00018711
Iteration 477/1000 | Loss: 0.00013734
Iteration 478/1000 | Loss: 0.00018230
Iteration 479/1000 | Loss: 0.00018318
Iteration 480/1000 | Loss: 0.00018431
Iteration 481/1000 | Loss: 0.00018753
Iteration 482/1000 | Loss: 0.00018810
Iteration 483/1000 | Loss: 0.00014601
Iteration 484/1000 | Loss: 0.00017758
Iteration 485/1000 | Loss: 0.00013646
Iteration 486/1000 | Loss: 0.00017488
Iteration 487/1000 | Loss: 0.00010956
Iteration 488/1000 | Loss: 0.00009966
Iteration 489/1000 | Loss: 0.00012319
Iteration 490/1000 | Loss: 0.00014968
Iteration 491/1000 | Loss: 0.00017581
Iteration 492/1000 | Loss: 0.00012538
Iteration 493/1000 | Loss: 0.00013603
Iteration 494/1000 | Loss: 0.00017411
Iteration 495/1000 | Loss: 0.00015377
Iteration 496/1000 | Loss: 0.00012634
Iteration 497/1000 | Loss: 0.00014545
Iteration 498/1000 | Loss: 0.00029270
Iteration 499/1000 | Loss: 0.00019709
Iteration 500/1000 | Loss: 0.00016579
Iteration 501/1000 | Loss: 0.00010752
Iteration 502/1000 | Loss: 0.00012940
Iteration 503/1000 | Loss: 0.00014242
Iteration 504/1000 | Loss: 0.00014202
Iteration 505/1000 | Loss: 0.00015578
Iteration 506/1000 | Loss: 0.00009626
Iteration 507/1000 | Loss: 0.00013607
Iteration 508/1000 | Loss: 0.00014022
Iteration 509/1000 | Loss: 0.00009732
Iteration 510/1000 | Loss: 0.00014117
Iteration 511/1000 | Loss: 0.00017267
Iteration 512/1000 | Loss: 0.00012561
Iteration 513/1000 | Loss: 0.00012460
Iteration 514/1000 | Loss: 0.00010543
Iteration 515/1000 | Loss: 0.00016018
Iteration 516/1000 | Loss: 0.00013336
Iteration 517/1000 | Loss: 0.00013284
Iteration 518/1000 | Loss: 0.00014590
Iteration 519/1000 | Loss: 0.00014630
Iteration 520/1000 | Loss: 0.00011119
Iteration 521/1000 | Loss: 0.00010535
Iteration 522/1000 | Loss: 0.00016632
Iteration 523/1000 | Loss: 0.00014350
Iteration 524/1000 | Loss: 0.00020255
Iteration 525/1000 | Loss: 0.00017649
Iteration 526/1000 | Loss: 0.00016302
Iteration 527/1000 | Loss: 0.00015906
Iteration 528/1000 | Loss: 0.00013582
Iteration 529/1000 | Loss: 0.00016157
Iteration 530/1000 | Loss: 0.00013672
Iteration 531/1000 | Loss: 0.00014534
Iteration 532/1000 | Loss: 0.00013057
Iteration 533/1000 | Loss: 0.00015300
Iteration 534/1000 | Loss: 0.00013990
Iteration 535/1000 | Loss: 0.00010225
Iteration 536/1000 | Loss: 0.00011997
Iteration 537/1000 | Loss: 0.00019777
Iteration 538/1000 | Loss: 0.00012841
Iteration 539/1000 | Loss: 0.00012119
Iteration 540/1000 | Loss: 0.00010246
Iteration 541/1000 | Loss: 0.00010091
Iteration 542/1000 | Loss: 0.00010748
Iteration 543/1000 | Loss: 0.00009931
Iteration 544/1000 | Loss: 0.00009133
Iteration 545/1000 | Loss: 0.00009329
Iteration 546/1000 | Loss: 0.00008757
Iteration 547/1000 | Loss: 0.00008756
Iteration 548/1000 | Loss: 0.00008841
Iteration 549/1000 | Loss: 0.00009479
Iteration 550/1000 | Loss: 0.00009451
Iteration 551/1000 | Loss: 0.00009500
Iteration 552/1000 | Loss: 0.00009258
Iteration 553/1000 | Loss: 0.00009210
Iteration 554/1000 | Loss: 0.00008740
Iteration 555/1000 | Loss: 0.00010822
Iteration 556/1000 | Loss: 0.00008833
Iteration 557/1000 | Loss: 0.00008699
Iteration 558/1000 | Loss: 0.00008671
Iteration 559/1000 | Loss: 0.00008663
Iteration 560/1000 | Loss: 0.00008657
Iteration 561/1000 | Loss: 0.00010977
Iteration 562/1000 | Loss: 0.00034420
Iteration 563/1000 | Loss: 0.00041264
Iteration 564/1000 | Loss: 0.00039770
Iteration 565/1000 | Loss: 0.00017330
Iteration 566/1000 | Loss: 0.00009246
Iteration 567/1000 | Loss: 0.00009064
Iteration 568/1000 | Loss: 0.00008796
Iteration 569/1000 | Loss: 0.00034530
Iteration 570/1000 | Loss: 0.00013680
Iteration 571/1000 | Loss: 0.00025893
Iteration 572/1000 | Loss: 0.00021851
Iteration 573/1000 | Loss: 0.00019738
Iteration 574/1000 | Loss: 0.00010076
Iteration 575/1000 | Loss: 0.00008868
Iteration 576/1000 | Loss: 0.00033233
Iteration 577/1000 | Loss: 0.00028919
Iteration 578/1000 | Loss: 0.00032149
Iteration 579/1000 | Loss: 0.00041959
Iteration 580/1000 | Loss: 0.00072179
Iteration 581/1000 | Loss: 0.00009802
Iteration 582/1000 | Loss: 0.00014588
Iteration 583/1000 | Loss: 0.00008463
Iteration 584/1000 | Loss: 0.00008327
Iteration 585/1000 | Loss: 0.00009169
Iteration 586/1000 | Loss: 0.00035474
Iteration 587/1000 | Loss: 0.00013659
Iteration 588/1000 | Loss: 0.00030316
Iteration 589/1000 | Loss: 0.00014507
Iteration 590/1000 | Loss: 0.00021671
Iteration 591/1000 | Loss: 0.00008583
Iteration 592/1000 | Loss: 0.00008478
Iteration 593/1000 | Loss: 0.00008337
Iteration 594/1000 | Loss: 0.00012602
Iteration 595/1000 | Loss: 0.00008073
Iteration 596/1000 | Loss: 0.00008183
Iteration 597/1000 | Loss: 0.00007993
Iteration 598/1000 | Loss: 0.00008187
Iteration 599/1000 | Loss: 0.00033366
Iteration 600/1000 | Loss: 0.00013679
Iteration 601/1000 | Loss: 0.00019100
Iteration 602/1000 | Loss: 0.00008273
Iteration 603/1000 | Loss: 0.00008107
Iteration 604/1000 | Loss: 0.00008004
Iteration 605/1000 | Loss: 0.00007893
Iteration 606/1000 | Loss: 0.00033291
Iteration 607/1000 | Loss: 0.00014406
Iteration 608/1000 | Loss: 0.00019533
Iteration 609/1000 | Loss: 0.00016929
Iteration 610/1000 | Loss: 0.00027304
Iteration 611/1000 | Loss: 0.00008244
Iteration 612/1000 | Loss: 0.00007972
Iteration 613/1000 | Loss: 0.00007910
Iteration 614/1000 | Loss: 0.00007965
Iteration 615/1000 | Loss: 0.00007714
Iteration 616/1000 | Loss: 0.00007718
Iteration 617/1000 | Loss: 0.00009090
Iteration 618/1000 | Loss: 0.00007647
Iteration 619/1000 | Loss: 0.00007646
Iteration 620/1000 | Loss: 0.00007645
Iteration 621/1000 | Loss: 0.00007704
Iteration 622/1000 | Loss: 0.00033620
Iteration 623/1000 | Loss: 0.00014556
Iteration 624/1000 | Loss: 0.00020606
Iteration 625/1000 | Loss: 0.00016845
Iteration 626/1000 | Loss: 0.00019739
Iteration 627/1000 | Loss: 0.00037559
Iteration 628/1000 | Loss: 0.00022486
Iteration 629/1000 | Loss: 0.00008369
Iteration 630/1000 | Loss: 0.00007910
Iteration 631/1000 | Loss: 0.00007762
Iteration 632/1000 | Loss: 0.00007866
Iteration 633/1000 | Loss: 0.00007671
Iteration 634/1000 | Loss: 0.00007819
Iteration 635/1000 | Loss: 0.00007546
Iteration 636/1000 | Loss: 0.00033022
Iteration 637/1000 | Loss: 0.00028067
Iteration 638/1000 | Loss: 0.00030140
Iteration 639/1000 | Loss: 0.00029349
Iteration 640/1000 | Loss: 0.00028972
Iteration 641/1000 | Loss: 0.00008009
Iteration 642/1000 | Loss: 0.00008072
Iteration 643/1000 | Loss: 0.00007600
Iteration 644/1000 | Loss: 0.00007508
Iteration 645/1000 | Loss: 0.00007494
Iteration 646/1000 | Loss: 0.00007381
Iteration 647/1000 | Loss: 0.00007471
Iteration 648/1000 | Loss: 0.00007352
Iteration 649/1000 | Loss: 0.00007339
Iteration 650/1000 | Loss: 0.00007339
Iteration 651/1000 | Loss: 0.00007338
Iteration 652/1000 | Loss: 0.00007338
Iteration 653/1000 | Loss: 0.00007337
Iteration 654/1000 | Loss: 0.00007334
Iteration 655/1000 | Loss: 0.00007333
Iteration 656/1000 | Loss: 0.00007327
Iteration 657/1000 | Loss: 0.00007472
Iteration 658/1000 | Loss: 0.00007320
Iteration 659/1000 | Loss: 0.00007320
Iteration 660/1000 | Loss: 0.00007318
Iteration 661/1000 | Loss: 0.00007318
Iteration 662/1000 | Loss: 0.00007328
Iteration 663/1000 | Loss: 0.00007314
Iteration 664/1000 | Loss: 0.00007312
Iteration 665/1000 | Loss: 0.00007312
Iteration 666/1000 | Loss: 0.00007311
Iteration 667/1000 | Loss: 0.00007311
Iteration 668/1000 | Loss: 0.00007310
Iteration 669/1000 | Loss: 0.00007310
Iteration 670/1000 | Loss: 0.00007310
Iteration 671/1000 | Loss: 0.00007310
Iteration 672/1000 | Loss: 0.00007310
Iteration 673/1000 | Loss: 0.00007309
Iteration 674/1000 | Loss: 0.00007309
Iteration 675/1000 | Loss: 0.00007309
Iteration 676/1000 | Loss: 0.00007309
Iteration 677/1000 | Loss: 0.00007309
Iteration 678/1000 | Loss: 0.00007308
Iteration 679/1000 | Loss: 0.00007308
Iteration 680/1000 | Loss: 0.00007307
Iteration 681/1000 | Loss: 0.00007307
Iteration 682/1000 | Loss: 0.00007306
Iteration 683/1000 | Loss: 0.00007306
Iteration 684/1000 | Loss: 0.00007306
Iteration 685/1000 | Loss: 0.00007411
Iteration 686/1000 | Loss: 0.00007305
Iteration 687/1000 | Loss: 0.00007305
Iteration 688/1000 | Loss: 0.00007305
Iteration 689/1000 | Loss: 0.00007305
Iteration 690/1000 | Loss: 0.00007304
Iteration 691/1000 | Loss: 0.00007304
Iteration 692/1000 | Loss: 0.00007304
Iteration 693/1000 | Loss: 0.00007304
Iteration 694/1000 | Loss: 0.00007304
Iteration 695/1000 | Loss: 0.00007304
Iteration 696/1000 | Loss: 0.00007304
Iteration 697/1000 | Loss: 0.00007303
Iteration 698/1000 | Loss: 0.00007303
Iteration 699/1000 | Loss: 0.00007339
Iteration 700/1000 | Loss: 0.00007354
Iteration 701/1000 | Loss: 0.00007310
Iteration 702/1000 | Loss: 0.00007496
Iteration 703/1000 | Loss: 0.00007292
Iteration 704/1000 | Loss: 0.00007291
Iteration 705/1000 | Loss: 0.00007324
Iteration 706/1000 | Loss: 0.00007290
Iteration 707/1000 | Loss: 0.00007290
Iteration 708/1000 | Loss: 0.00007290
Iteration 709/1000 | Loss: 0.00007289
Iteration 710/1000 | Loss: 0.00007289
Iteration 711/1000 | Loss: 0.00007289
Iteration 712/1000 | Loss: 0.00007289
Iteration 713/1000 | Loss: 0.00007289
Iteration 714/1000 | Loss: 0.00007289
Iteration 715/1000 | Loss: 0.00007289
Iteration 716/1000 | Loss: 0.00007289
Iteration 717/1000 | Loss: 0.00007289
Iteration 718/1000 | Loss: 0.00007289
Iteration 719/1000 | Loss: 0.00007289
Iteration 720/1000 | Loss: 0.00007289
Iteration 721/1000 | Loss: 0.00007289
Iteration 722/1000 | Loss: 0.00007289
Iteration 723/1000 | Loss: 0.00007289
Iteration 724/1000 | Loss: 0.00007289
Iteration 725/1000 | Loss: 0.00007289
Iteration 726/1000 | Loss: 0.00007289
Iteration 727/1000 | Loss: 0.00007289
Iteration 728/1000 | Loss: 0.00007289
Iteration 729/1000 | Loss: 0.00007289
Iteration 730/1000 | Loss: 0.00007289
Iteration 731/1000 | Loss: 0.00007289
Iteration 732/1000 | Loss: 0.00007289
Iteration 733/1000 | Loss: 0.00007289
Iteration 734/1000 | Loss: 0.00007289
Iteration 735/1000 | Loss: 0.00007289
Iteration 736/1000 | Loss: 0.00007289
Iteration 737/1000 | Loss: 0.00007289
Iteration 738/1000 | Loss: 0.00007289
Iteration 739/1000 | Loss: 0.00007289
Iteration 740/1000 | Loss: 0.00007289
Iteration 741/1000 | Loss: 0.00007289
Iteration 742/1000 | Loss: 0.00007289
Iteration 743/1000 | Loss: 0.00007289
Iteration 744/1000 | Loss: 0.00007289
Iteration 745/1000 | Loss: 0.00007289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 745. Stopping optimization.
Last 5 losses: [7.289242785191163e-05, 7.289242785191163e-05, 7.289242785191163e-05, 7.289242785191163e-05, 7.289242785191163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.289242785191163e-05

Optimization complete. Final v2v error: 4.28104829788208 mm

Highest mean error: 13.158692359924316 mm for frame 5

Lowest mean error: 2.706477642059326 mm for frame 187

Saving results

Total time: 968.1167702674866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813670
Iteration 2/25 | Loss: 0.00132073
Iteration 3/25 | Loss: 0.00119400
Iteration 4/25 | Loss: 0.00118525
Iteration 5/25 | Loss: 0.00118298
Iteration 6/25 | Loss: 0.00118298
Iteration 7/25 | Loss: 0.00118298
Iteration 8/25 | Loss: 0.00118298
Iteration 9/25 | Loss: 0.00118298
Iteration 10/25 | Loss: 0.00118298
Iteration 11/25 | Loss: 0.00118298
Iteration 12/25 | Loss: 0.00118298
Iteration 13/25 | Loss: 0.00118298
Iteration 14/25 | Loss: 0.00118298
Iteration 15/25 | Loss: 0.00118298
Iteration 16/25 | Loss: 0.00118298
Iteration 17/25 | Loss: 0.00118298
Iteration 18/25 | Loss: 0.00118298
Iteration 19/25 | Loss: 0.00118298
Iteration 20/25 | Loss: 0.00118298
Iteration 21/25 | Loss: 0.00118298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011829847935587168, 0.0011829847935587168, 0.0011829847935587168, 0.0011829847935587168, 0.0011829847935587168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011829847935587168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.66179204
Iteration 2/25 | Loss: 0.00176851
Iteration 3/25 | Loss: 0.00176847
Iteration 4/25 | Loss: 0.00176847
Iteration 5/25 | Loss: 0.00176847
Iteration 6/25 | Loss: 0.00176847
Iteration 7/25 | Loss: 0.00176847
Iteration 8/25 | Loss: 0.00176847
Iteration 9/25 | Loss: 0.00176847
Iteration 10/25 | Loss: 0.00176847
Iteration 11/25 | Loss: 0.00176847
Iteration 12/25 | Loss: 0.00176847
Iteration 13/25 | Loss: 0.00176847
Iteration 14/25 | Loss: 0.00176847
Iteration 15/25 | Loss: 0.00176847
Iteration 16/25 | Loss: 0.00176847
Iteration 17/25 | Loss: 0.00176847
Iteration 18/25 | Loss: 0.00176847
Iteration 19/25 | Loss: 0.00176847
Iteration 20/25 | Loss: 0.00176847
Iteration 21/25 | Loss: 0.00176847
Iteration 22/25 | Loss: 0.00176847
Iteration 23/25 | Loss: 0.00176847
Iteration 24/25 | Loss: 0.00176847
Iteration 25/25 | Loss: 0.00176847

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176847
Iteration 2/1000 | Loss: 0.00003075
Iteration 3/1000 | Loss: 0.00002352
Iteration 4/1000 | Loss: 0.00002100
Iteration 5/1000 | Loss: 0.00001945
Iteration 6/1000 | Loss: 0.00001867
Iteration 7/1000 | Loss: 0.00001796
Iteration 8/1000 | Loss: 0.00001745
Iteration 9/1000 | Loss: 0.00001690
Iteration 10/1000 | Loss: 0.00001646
Iteration 11/1000 | Loss: 0.00001622
Iteration 12/1000 | Loss: 0.00001600
Iteration 13/1000 | Loss: 0.00001585
Iteration 14/1000 | Loss: 0.00001573
Iteration 15/1000 | Loss: 0.00001571
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001558
Iteration 18/1000 | Loss: 0.00001552
Iteration 19/1000 | Loss: 0.00001545
Iteration 20/1000 | Loss: 0.00001542
Iteration 21/1000 | Loss: 0.00001541
Iteration 22/1000 | Loss: 0.00001541
Iteration 23/1000 | Loss: 0.00001540
Iteration 24/1000 | Loss: 0.00001536
Iteration 25/1000 | Loss: 0.00001535
Iteration 26/1000 | Loss: 0.00001535
Iteration 27/1000 | Loss: 0.00001534
Iteration 28/1000 | Loss: 0.00001534
Iteration 29/1000 | Loss: 0.00001533
Iteration 30/1000 | Loss: 0.00001533
Iteration 31/1000 | Loss: 0.00001530
Iteration 32/1000 | Loss: 0.00001530
Iteration 33/1000 | Loss: 0.00001527
Iteration 34/1000 | Loss: 0.00001527
Iteration 35/1000 | Loss: 0.00001524
Iteration 36/1000 | Loss: 0.00001521
Iteration 37/1000 | Loss: 0.00001521
Iteration 38/1000 | Loss: 0.00001516
Iteration 39/1000 | Loss: 0.00001516
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001513
Iteration 42/1000 | Loss: 0.00001510
Iteration 43/1000 | Loss: 0.00001510
Iteration 44/1000 | Loss: 0.00001509
Iteration 45/1000 | Loss: 0.00001509
Iteration 46/1000 | Loss: 0.00001509
Iteration 47/1000 | Loss: 0.00001508
Iteration 48/1000 | Loss: 0.00001508
Iteration 49/1000 | Loss: 0.00001508
Iteration 50/1000 | Loss: 0.00001507
Iteration 51/1000 | Loss: 0.00001507
Iteration 52/1000 | Loss: 0.00001506
Iteration 53/1000 | Loss: 0.00001506
Iteration 54/1000 | Loss: 0.00001506
Iteration 55/1000 | Loss: 0.00001506
Iteration 56/1000 | Loss: 0.00001505
Iteration 57/1000 | Loss: 0.00001505
Iteration 58/1000 | Loss: 0.00001505
Iteration 59/1000 | Loss: 0.00001505
Iteration 60/1000 | Loss: 0.00001504
Iteration 61/1000 | Loss: 0.00001504
Iteration 62/1000 | Loss: 0.00001504
Iteration 63/1000 | Loss: 0.00001504
Iteration 64/1000 | Loss: 0.00001504
Iteration 65/1000 | Loss: 0.00001503
Iteration 66/1000 | Loss: 0.00001503
Iteration 67/1000 | Loss: 0.00001503
Iteration 68/1000 | Loss: 0.00001503
Iteration 69/1000 | Loss: 0.00001503
Iteration 70/1000 | Loss: 0.00001503
Iteration 71/1000 | Loss: 0.00001503
Iteration 72/1000 | Loss: 0.00001503
Iteration 73/1000 | Loss: 0.00001502
Iteration 74/1000 | Loss: 0.00001502
Iteration 75/1000 | Loss: 0.00001502
Iteration 76/1000 | Loss: 0.00001502
Iteration 77/1000 | Loss: 0.00001502
Iteration 78/1000 | Loss: 0.00001502
Iteration 79/1000 | Loss: 0.00001502
Iteration 80/1000 | Loss: 0.00001502
Iteration 81/1000 | Loss: 0.00001502
Iteration 82/1000 | Loss: 0.00001502
Iteration 83/1000 | Loss: 0.00001502
Iteration 84/1000 | Loss: 0.00001502
Iteration 85/1000 | Loss: 0.00001502
Iteration 86/1000 | Loss: 0.00001502
Iteration 87/1000 | Loss: 0.00001502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.501975657447474e-05, 1.501975657447474e-05, 1.501975657447474e-05, 1.501975657447474e-05, 1.501975657447474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.501975657447474e-05

Optimization complete. Final v2v error: 3.306993007659912 mm

Highest mean error: 4.2054548263549805 mm for frame 232

Lowest mean error: 2.7529876232147217 mm for frame 109

Saving results

Total time: 44.945632219314575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415351
Iteration 2/25 | Loss: 0.00121709
Iteration 3/25 | Loss: 0.00114627
Iteration 4/25 | Loss: 0.00113882
Iteration 5/25 | Loss: 0.00113663
Iteration 6/25 | Loss: 0.00113616
Iteration 7/25 | Loss: 0.00113616
Iteration 8/25 | Loss: 0.00113616
Iteration 9/25 | Loss: 0.00113616
Iteration 10/25 | Loss: 0.00113616
Iteration 11/25 | Loss: 0.00113616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00113616231828928, 0.00113616231828928, 0.00113616231828928, 0.00113616231828928, 0.00113616231828928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00113616231828928

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02730906
Iteration 2/25 | Loss: 0.00173818
Iteration 3/25 | Loss: 0.00173818
Iteration 4/25 | Loss: 0.00173818
Iteration 5/25 | Loss: 0.00173818
Iteration 6/25 | Loss: 0.00173818
Iteration 7/25 | Loss: 0.00173818
Iteration 8/25 | Loss: 0.00173818
Iteration 9/25 | Loss: 0.00173817
Iteration 10/25 | Loss: 0.00173817
Iteration 11/25 | Loss: 0.00173817
Iteration 12/25 | Loss: 0.00173817
Iteration 13/25 | Loss: 0.00173817
Iteration 14/25 | Loss: 0.00173817
Iteration 15/25 | Loss: 0.00173817
Iteration 16/25 | Loss: 0.00173817
Iteration 17/25 | Loss: 0.00173817
Iteration 18/25 | Loss: 0.00173817
Iteration 19/25 | Loss: 0.00173817
Iteration 20/25 | Loss: 0.00173817
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0017381745856255293, 0.0017381745856255293, 0.0017381745856255293, 0.0017381745856255293, 0.0017381745856255293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017381745856255293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173817
Iteration 2/1000 | Loss: 0.00003275
Iteration 3/1000 | Loss: 0.00001828
Iteration 4/1000 | Loss: 0.00001542
Iteration 5/1000 | Loss: 0.00001413
Iteration 6/1000 | Loss: 0.00001302
Iteration 7/1000 | Loss: 0.00001260
Iteration 8/1000 | Loss: 0.00001211
Iteration 9/1000 | Loss: 0.00001195
Iteration 10/1000 | Loss: 0.00001156
Iteration 11/1000 | Loss: 0.00001118
Iteration 12/1000 | Loss: 0.00001092
Iteration 13/1000 | Loss: 0.00001068
Iteration 14/1000 | Loss: 0.00001054
Iteration 15/1000 | Loss: 0.00001048
Iteration 16/1000 | Loss: 0.00001027
Iteration 17/1000 | Loss: 0.00001020
Iteration 18/1000 | Loss: 0.00001019
Iteration 19/1000 | Loss: 0.00001019
Iteration 20/1000 | Loss: 0.00001018
Iteration 21/1000 | Loss: 0.00001018
Iteration 22/1000 | Loss: 0.00001017
Iteration 23/1000 | Loss: 0.00001017
Iteration 24/1000 | Loss: 0.00001017
Iteration 25/1000 | Loss: 0.00001016
Iteration 26/1000 | Loss: 0.00001016
Iteration 27/1000 | Loss: 0.00001016
Iteration 28/1000 | Loss: 0.00001016
Iteration 29/1000 | Loss: 0.00001014
Iteration 30/1000 | Loss: 0.00001012
Iteration 31/1000 | Loss: 0.00001012
Iteration 32/1000 | Loss: 0.00001011
Iteration 33/1000 | Loss: 0.00001011
Iteration 34/1000 | Loss: 0.00001011
Iteration 35/1000 | Loss: 0.00001010
Iteration 36/1000 | Loss: 0.00001008
Iteration 37/1000 | Loss: 0.00001008
Iteration 38/1000 | Loss: 0.00001008
Iteration 39/1000 | Loss: 0.00001008
Iteration 40/1000 | Loss: 0.00001007
Iteration 41/1000 | Loss: 0.00001007
Iteration 42/1000 | Loss: 0.00001007
Iteration 43/1000 | Loss: 0.00001007
Iteration 44/1000 | Loss: 0.00001007
Iteration 45/1000 | Loss: 0.00001007
Iteration 46/1000 | Loss: 0.00001006
Iteration 47/1000 | Loss: 0.00001006
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001005
Iteration 50/1000 | Loss: 0.00001005
Iteration 51/1000 | Loss: 0.00001005
Iteration 52/1000 | Loss: 0.00001005
Iteration 53/1000 | Loss: 0.00001005
Iteration 54/1000 | Loss: 0.00001005
Iteration 55/1000 | Loss: 0.00001005
Iteration 56/1000 | Loss: 0.00001005
Iteration 57/1000 | Loss: 0.00001005
Iteration 58/1000 | Loss: 0.00001005
Iteration 59/1000 | Loss: 0.00001004
Iteration 60/1000 | Loss: 0.00001004
Iteration 61/1000 | Loss: 0.00001004
Iteration 62/1000 | Loss: 0.00001004
Iteration 63/1000 | Loss: 0.00001004
Iteration 64/1000 | Loss: 0.00001004
Iteration 65/1000 | Loss: 0.00001004
Iteration 66/1000 | Loss: 0.00001004
Iteration 67/1000 | Loss: 0.00001004
Iteration 68/1000 | Loss: 0.00001003
Iteration 69/1000 | Loss: 0.00001003
Iteration 70/1000 | Loss: 0.00001003
Iteration 71/1000 | Loss: 0.00001002
Iteration 72/1000 | Loss: 0.00001002
Iteration 73/1000 | Loss: 0.00001002
Iteration 74/1000 | Loss: 0.00001002
Iteration 75/1000 | Loss: 0.00001002
Iteration 76/1000 | Loss: 0.00001001
Iteration 77/1000 | Loss: 0.00001001
Iteration 78/1000 | Loss: 0.00001001
Iteration 79/1000 | Loss: 0.00001000
Iteration 80/1000 | Loss: 0.00001000
Iteration 81/1000 | Loss: 0.00000999
Iteration 82/1000 | Loss: 0.00000999
Iteration 83/1000 | Loss: 0.00000999
Iteration 84/1000 | Loss: 0.00000999
Iteration 85/1000 | Loss: 0.00000998
Iteration 86/1000 | Loss: 0.00000998
Iteration 87/1000 | Loss: 0.00000998
Iteration 88/1000 | Loss: 0.00000998
Iteration 89/1000 | Loss: 0.00000998
Iteration 90/1000 | Loss: 0.00000998
Iteration 91/1000 | Loss: 0.00000998
Iteration 92/1000 | Loss: 0.00000998
Iteration 93/1000 | Loss: 0.00000998
Iteration 94/1000 | Loss: 0.00000998
Iteration 95/1000 | Loss: 0.00000998
Iteration 96/1000 | Loss: 0.00000998
Iteration 97/1000 | Loss: 0.00000998
Iteration 98/1000 | Loss: 0.00000997
Iteration 99/1000 | Loss: 0.00000997
Iteration 100/1000 | Loss: 0.00000997
Iteration 101/1000 | Loss: 0.00000997
Iteration 102/1000 | Loss: 0.00000997
Iteration 103/1000 | Loss: 0.00000997
Iteration 104/1000 | Loss: 0.00000997
Iteration 105/1000 | Loss: 0.00000997
Iteration 106/1000 | Loss: 0.00000997
Iteration 107/1000 | Loss: 0.00000996
Iteration 108/1000 | Loss: 0.00000996
Iteration 109/1000 | Loss: 0.00000996
Iteration 110/1000 | Loss: 0.00000996
Iteration 111/1000 | Loss: 0.00000996
Iteration 112/1000 | Loss: 0.00000995
Iteration 113/1000 | Loss: 0.00000995
Iteration 114/1000 | Loss: 0.00000995
Iteration 115/1000 | Loss: 0.00000994
Iteration 116/1000 | Loss: 0.00000992
Iteration 117/1000 | Loss: 0.00000992
Iteration 118/1000 | Loss: 0.00000992
Iteration 119/1000 | Loss: 0.00000992
Iteration 120/1000 | Loss: 0.00000991
Iteration 121/1000 | Loss: 0.00000991
Iteration 122/1000 | Loss: 0.00000991
Iteration 123/1000 | Loss: 0.00000991
Iteration 124/1000 | Loss: 0.00000991
Iteration 125/1000 | Loss: 0.00000991
Iteration 126/1000 | Loss: 0.00000991
Iteration 127/1000 | Loss: 0.00000991
Iteration 128/1000 | Loss: 0.00000991
Iteration 129/1000 | Loss: 0.00000991
Iteration 130/1000 | Loss: 0.00000991
Iteration 131/1000 | Loss: 0.00000991
Iteration 132/1000 | Loss: 0.00000990
Iteration 133/1000 | Loss: 0.00000990
Iteration 134/1000 | Loss: 0.00000990
Iteration 135/1000 | Loss: 0.00000990
Iteration 136/1000 | Loss: 0.00000990
Iteration 137/1000 | Loss: 0.00000990
Iteration 138/1000 | Loss: 0.00000989
Iteration 139/1000 | Loss: 0.00000989
Iteration 140/1000 | Loss: 0.00000989
Iteration 141/1000 | Loss: 0.00000989
Iteration 142/1000 | Loss: 0.00000989
Iteration 143/1000 | Loss: 0.00000989
Iteration 144/1000 | Loss: 0.00000989
Iteration 145/1000 | Loss: 0.00000989
Iteration 146/1000 | Loss: 0.00000988
Iteration 147/1000 | Loss: 0.00000988
Iteration 148/1000 | Loss: 0.00000988
Iteration 149/1000 | Loss: 0.00000987
Iteration 150/1000 | Loss: 0.00000987
Iteration 151/1000 | Loss: 0.00000987
Iteration 152/1000 | Loss: 0.00000986
Iteration 153/1000 | Loss: 0.00000986
Iteration 154/1000 | Loss: 0.00000986
Iteration 155/1000 | Loss: 0.00000985
Iteration 156/1000 | Loss: 0.00000985
Iteration 157/1000 | Loss: 0.00000985
Iteration 158/1000 | Loss: 0.00000985
Iteration 159/1000 | Loss: 0.00000985
Iteration 160/1000 | Loss: 0.00000985
Iteration 161/1000 | Loss: 0.00000985
Iteration 162/1000 | Loss: 0.00000985
Iteration 163/1000 | Loss: 0.00000985
Iteration 164/1000 | Loss: 0.00000985
Iteration 165/1000 | Loss: 0.00000985
Iteration 166/1000 | Loss: 0.00000984
Iteration 167/1000 | Loss: 0.00000984
Iteration 168/1000 | Loss: 0.00000984
Iteration 169/1000 | Loss: 0.00000984
Iteration 170/1000 | Loss: 0.00000984
Iteration 171/1000 | Loss: 0.00000984
Iteration 172/1000 | Loss: 0.00000984
Iteration 173/1000 | Loss: 0.00000984
Iteration 174/1000 | Loss: 0.00000984
Iteration 175/1000 | Loss: 0.00000984
Iteration 176/1000 | Loss: 0.00000984
Iteration 177/1000 | Loss: 0.00000984
Iteration 178/1000 | Loss: 0.00000984
Iteration 179/1000 | Loss: 0.00000984
Iteration 180/1000 | Loss: 0.00000984
Iteration 181/1000 | Loss: 0.00000984
Iteration 182/1000 | Loss: 0.00000984
Iteration 183/1000 | Loss: 0.00000984
Iteration 184/1000 | Loss: 0.00000984
Iteration 185/1000 | Loss: 0.00000984
Iteration 186/1000 | Loss: 0.00000984
Iteration 187/1000 | Loss: 0.00000984
Iteration 188/1000 | Loss: 0.00000984
Iteration 189/1000 | Loss: 0.00000984
Iteration 190/1000 | Loss: 0.00000984
Iteration 191/1000 | Loss: 0.00000984
Iteration 192/1000 | Loss: 0.00000984
Iteration 193/1000 | Loss: 0.00000984
Iteration 194/1000 | Loss: 0.00000984
Iteration 195/1000 | Loss: 0.00000984
Iteration 196/1000 | Loss: 0.00000984
Iteration 197/1000 | Loss: 0.00000984
Iteration 198/1000 | Loss: 0.00000984
Iteration 199/1000 | Loss: 0.00000984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [9.836094250204042e-06, 9.836094250204042e-06, 9.836094250204042e-06, 9.836094250204042e-06, 9.836094250204042e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.836094250204042e-06

Optimization complete. Final v2v error: 2.7291622161865234 mm

Highest mean error: 2.7454209327697754 mm for frame 39

Lowest mean error: 2.7182183265686035 mm for frame 7

Saving results

Total time: 39.483304023742676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_004/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_004/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795228
Iteration 2/25 | Loss: 0.00122755
Iteration 3/25 | Loss: 0.00116015
Iteration 4/25 | Loss: 0.00114774
Iteration 5/25 | Loss: 0.00114436
Iteration 6/25 | Loss: 0.00114413
Iteration 7/25 | Loss: 0.00114413
Iteration 8/25 | Loss: 0.00114413
Iteration 9/25 | Loss: 0.00114413
Iteration 10/25 | Loss: 0.00114413
Iteration 11/25 | Loss: 0.00114413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011441310634836555, 0.0011441310634836555, 0.0011441310634836555, 0.0011441310634836555, 0.0011441310634836555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011441310634836555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28256118
Iteration 2/25 | Loss: 0.00185555
Iteration 3/25 | Loss: 0.00185555
Iteration 4/25 | Loss: 0.00185555
Iteration 5/25 | Loss: 0.00185555
Iteration 6/25 | Loss: 0.00185555
Iteration 7/25 | Loss: 0.00185555
Iteration 8/25 | Loss: 0.00185555
Iteration 9/25 | Loss: 0.00185555
Iteration 10/25 | Loss: 0.00185555
Iteration 11/25 | Loss: 0.00185555
Iteration 12/25 | Loss: 0.00185555
Iteration 13/25 | Loss: 0.00185555
Iteration 14/25 | Loss: 0.00185555
Iteration 15/25 | Loss: 0.00185555
Iteration 16/25 | Loss: 0.00185555
Iteration 17/25 | Loss: 0.00185555
Iteration 18/25 | Loss: 0.00185555
Iteration 19/25 | Loss: 0.00185555
Iteration 20/25 | Loss: 0.00185555
Iteration 21/25 | Loss: 0.00185555
Iteration 22/25 | Loss: 0.00185555
Iteration 23/25 | Loss: 0.00185555
Iteration 24/25 | Loss: 0.00185555
Iteration 25/25 | Loss: 0.00185555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185555
Iteration 2/1000 | Loss: 0.00002086
Iteration 3/1000 | Loss: 0.00001433
Iteration 4/1000 | Loss: 0.00001307
Iteration 5/1000 | Loss: 0.00001228
Iteration 6/1000 | Loss: 0.00001175
Iteration 7/1000 | Loss: 0.00001132
Iteration 8/1000 | Loss: 0.00001114
Iteration 9/1000 | Loss: 0.00001093
Iteration 10/1000 | Loss: 0.00001065
Iteration 11/1000 | Loss: 0.00001061
Iteration 12/1000 | Loss: 0.00001057
Iteration 13/1000 | Loss: 0.00001046
Iteration 14/1000 | Loss: 0.00001029
Iteration 15/1000 | Loss: 0.00001020
Iteration 16/1000 | Loss: 0.00001007
Iteration 17/1000 | Loss: 0.00001007
Iteration 18/1000 | Loss: 0.00001000
Iteration 19/1000 | Loss: 0.00000993
Iteration 20/1000 | Loss: 0.00000990
Iteration 21/1000 | Loss: 0.00000989
Iteration 22/1000 | Loss: 0.00000988
Iteration 23/1000 | Loss: 0.00000987
Iteration 24/1000 | Loss: 0.00000986
Iteration 25/1000 | Loss: 0.00000986
Iteration 26/1000 | Loss: 0.00000986
Iteration 27/1000 | Loss: 0.00000985
Iteration 28/1000 | Loss: 0.00000985
Iteration 29/1000 | Loss: 0.00000984
Iteration 30/1000 | Loss: 0.00000984
Iteration 31/1000 | Loss: 0.00000983
Iteration 32/1000 | Loss: 0.00000983
Iteration 33/1000 | Loss: 0.00000983
Iteration 34/1000 | Loss: 0.00000983
Iteration 35/1000 | Loss: 0.00000982
Iteration 36/1000 | Loss: 0.00000982
Iteration 37/1000 | Loss: 0.00000980
Iteration 38/1000 | Loss: 0.00000979
Iteration 39/1000 | Loss: 0.00000979
Iteration 40/1000 | Loss: 0.00000979
Iteration 41/1000 | Loss: 0.00000979
Iteration 42/1000 | Loss: 0.00000979
Iteration 43/1000 | Loss: 0.00000979
Iteration 44/1000 | Loss: 0.00000978
Iteration 45/1000 | Loss: 0.00000977
Iteration 46/1000 | Loss: 0.00000977
Iteration 47/1000 | Loss: 0.00000977
Iteration 48/1000 | Loss: 0.00000977
Iteration 49/1000 | Loss: 0.00000977
Iteration 50/1000 | Loss: 0.00000976
Iteration 51/1000 | Loss: 0.00000976
Iteration 52/1000 | Loss: 0.00000976
Iteration 53/1000 | Loss: 0.00000975
Iteration 54/1000 | Loss: 0.00000975
Iteration 55/1000 | Loss: 0.00000975
Iteration 56/1000 | Loss: 0.00000975
Iteration 57/1000 | Loss: 0.00000975
Iteration 58/1000 | Loss: 0.00000975
Iteration 59/1000 | Loss: 0.00000975
Iteration 60/1000 | Loss: 0.00000975
Iteration 61/1000 | Loss: 0.00000974
Iteration 62/1000 | Loss: 0.00000974
Iteration 63/1000 | Loss: 0.00000974
Iteration 64/1000 | Loss: 0.00000974
Iteration 65/1000 | Loss: 0.00000974
Iteration 66/1000 | Loss: 0.00000973
Iteration 67/1000 | Loss: 0.00000973
Iteration 68/1000 | Loss: 0.00000973
Iteration 69/1000 | Loss: 0.00000973
Iteration 70/1000 | Loss: 0.00000973
Iteration 71/1000 | Loss: 0.00000972
Iteration 72/1000 | Loss: 0.00000972
Iteration 73/1000 | Loss: 0.00000972
Iteration 74/1000 | Loss: 0.00000972
Iteration 75/1000 | Loss: 0.00000972
Iteration 76/1000 | Loss: 0.00000972
Iteration 77/1000 | Loss: 0.00000972
Iteration 78/1000 | Loss: 0.00000972
Iteration 79/1000 | Loss: 0.00000972
Iteration 80/1000 | Loss: 0.00000972
Iteration 81/1000 | Loss: 0.00000971
Iteration 82/1000 | Loss: 0.00000971
Iteration 83/1000 | Loss: 0.00000971
Iteration 84/1000 | Loss: 0.00000971
Iteration 85/1000 | Loss: 0.00000971
Iteration 86/1000 | Loss: 0.00000971
Iteration 87/1000 | Loss: 0.00000971
Iteration 88/1000 | Loss: 0.00000971
Iteration 89/1000 | Loss: 0.00000970
Iteration 90/1000 | Loss: 0.00000970
Iteration 91/1000 | Loss: 0.00000970
Iteration 92/1000 | Loss: 0.00000970
Iteration 93/1000 | Loss: 0.00000970
Iteration 94/1000 | Loss: 0.00000970
Iteration 95/1000 | Loss: 0.00000969
Iteration 96/1000 | Loss: 0.00000969
Iteration 97/1000 | Loss: 0.00000969
Iteration 98/1000 | Loss: 0.00000969
Iteration 99/1000 | Loss: 0.00000969
Iteration 100/1000 | Loss: 0.00000969
Iteration 101/1000 | Loss: 0.00000969
Iteration 102/1000 | Loss: 0.00000969
Iteration 103/1000 | Loss: 0.00000969
Iteration 104/1000 | Loss: 0.00000969
Iteration 105/1000 | Loss: 0.00000969
Iteration 106/1000 | Loss: 0.00000969
Iteration 107/1000 | Loss: 0.00000969
Iteration 108/1000 | Loss: 0.00000969
Iteration 109/1000 | Loss: 0.00000969
Iteration 110/1000 | Loss: 0.00000969
Iteration 111/1000 | Loss: 0.00000968
Iteration 112/1000 | Loss: 0.00000968
Iteration 113/1000 | Loss: 0.00000968
Iteration 114/1000 | Loss: 0.00000968
Iteration 115/1000 | Loss: 0.00000967
Iteration 116/1000 | Loss: 0.00000967
Iteration 117/1000 | Loss: 0.00000967
Iteration 118/1000 | Loss: 0.00000967
Iteration 119/1000 | Loss: 0.00000967
Iteration 120/1000 | Loss: 0.00000967
Iteration 121/1000 | Loss: 0.00000967
Iteration 122/1000 | Loss: 0.00000966
Iteration 123/1000 | Loss: 0.00000966
Iteration 124/1000 | Loss: 0.00000966
Iteration 125/1000 | Loss: 0.00000966
Iteration 126/1000 | Loss: 0.00000966
Iteration 127/1000 | Loss: 0.00000966
Iteration 128/1000 | Loss: 0.00000966
Iteration 129/1000 | Loss: 0.00000966
Iteration 130/1000 | Loss: 0.00000966
Iteration 131/1000 | Loss: 0.00000966
Iteration 132/1000 | Loss: 0.00000966
Iteration 133/1000 | Loss: 0.00000965
Iteration 134/1000 | Loss: 0.00000965
Iteration 135/1000 | Loss: 0.00000965
Iteration 136/1000 | Loss: 0.00000964
Iteration 137/1000 | Loss: 0.00000964
Iteration 138/1000 | Loss: 0.00000964
Iteration 139/1000 | Loss: 0.00000964
Iteration 140/1000 | Loss: 0.00000964
Iteration 141/1000 | Loss: 0.00000964
Iteration 142/1000 | Loss: 0.00000964
Iteration 143/1000 | Loss: 0.00000964
Iteration 144/1000 | Loss: 0.00000964
Iteration 145/1000 | Loss: 0.00000964
Iteration 146/1000 | Loss: 0.00000964
Iteration 147/1000 | Loss: 0.00000964
Iteration 148/1000 | Loss: 0.00000964
Iteration 149/1000 | Loss: 0.00000964
Iteration 150/1000 | Loss: 0.00000964
Iteration 151/1000 | Loss: 0.00000964
Iteration 152/1000 | Loss: 0.00000964
Iteration 153/1000 | Loss: 0.00000964
Iteration 154/1000 | Loss: 0.00000964
Iteration 155/1000 | Loss: 0.00000963
Iteration 156/1000 | Loss: 0.00000963
Iteration 157/1000 | Loss: 0.00000963
Iteration 158/1000 | Loss: 0.00000963
Iteration 159/1000 | Loss: 0.00000963
Iteration 160/1000 | Loss: 0.00000963
Iteration 161/1000 | Loss: 0.00000963
Iteration 162/1000 | Loss: 0.00000963
Iteration 163/1000 | Loss: 0.00000963
Iteration 164/1000 | Loss: 0.00000963
Iteration 165/1000 | Loss: 0.00000963
Iteration 166/1000 | Loss: 0.00000963
Iteration 167/1000 | Loss: 0.00000963
Iteration 168/1000 | Loss: 0.00000963
Iteration 169/1000 | Loss: 0.00000963
Iteration 170/1000 | Loss: 0.00000963
Iteration 171/1000 | Loss: 0.00000962
Iteration 172/1000 | Loss: 0.00000962
Iteration 173/1000 | Loss: 0.00000962
Iteration 174/1000 | Loss: 0.00000962
Iteration 175/1000 | Loss: 0.00000962
Iteration 176/1000 | Loss: 0.00000962
Iteration 177/1000 | Loss: 0.00000962
Iteration 178/1000 | Loss: 0.00000962
Iteration 179/1000 | Loss: 0.00000962
Iteration 180/1000 | Loss: 0.00000962
Iteration 181/1000 | Loss: 0.00000962
Iteration 182/1000 | Loss: 0.00000961
Iteration 183/1000 | Loss: 0.00000961
Iteration 184/1000 | Loss: 0.00000961
Iteration 185/1000 | Loss: 0.00000961
Iteration 186/1000 | Loss: 0.00000961
Iteration 187/1000 | Loss: 0.00000961
Iteration 188/1000 | Loss: 0.00000961
Iteration 189/1000 | Loss: 0.00000961
Iteration 190/1000 | Loss: 0.00000961
Iteration 191/1000 | Loss: 0.00000961
Iteration 192/1000 | Loss: 0.00000961
Iteration 193/1000 | Loss: 0.00000961
Iteration 194/1000 | Loss: 0.00000961
Iteration 195/1000 | Loss: 0.00000961
Iteration 196/1000 | Loss: 0.00000961
Iteration 197/1000 | Loss: 0.00000961
Iteration 198/1000 | Loss: 0.00000961
Iteration 199/1000 | Loss: 0.00000961
Iteration 200/1000 | Loss: 0.00000961
Iteration 201/1000 | Loss: 0.00000961
Iteration 202/1000 | Loss: 0.00000961
Iteration 203/1000 | Loss: 0.00000961
Iteration 204/1000 | Loss: 0.00000961
Iteration 205/1000 | Loss: 0.00000961
Iteration 206/1000 | Loss: 0.00000961
Iteration 207/1000 | Loss: 0.00000961
Iteration 208/1000 | Loss: 0.00000961
Iteration 209/1000 | Loss: 0.00000961
Iteration 210/1000 | Loss: 0.00000961
Iteration 211/1000 | Loss: 0.00000961
Iteration 212/1000 | Loss: 0.00000961
Iteration 213/1000 | Loss: 0.00000961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [9.610706911189482e-06, 9.610706911189482e-06, 9.610706911189482e-06, 9.610706911189482e-06, 9.610706911189482e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.610706911189482e-06

Optimization complete. Final v2v error: 2.6848437786102295 mm

Highest mean error: 2.933549165725708 mm for frame 63

Lowest mean error: 2.5753250122070312 mm for frame 146

Saving results

Total time: 41.697513580322266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891734
Iteration 2/25 | Loss: 0.00093474
Iteration 3/25 | Loss: 0.00085403
Iteration 4/25 | Loss: 0.00083623
Iteration 5/25 | Loss: 0.00083137
Iteration 6/25 | Loss: 0.00083009
Iteration 7/25 | Loss: 0.00083009
Iteration 8/25 | Loss: 0.00083009
Iteration 9/25 | Loss: 0.00083009
Iteration 10/25 | Loss: 0.00083009
Iteration 11/25 | Loss: 0.00083009
Iteration 12/25 | Loss: 0.00083009
Iteration 13/25 | Loss: 0.00083009
Iteration 14/25 | Loss: 0.00083009
Iteration 15/25 | Loss: 0.00083009
Iteration 16/25 | Loss: 0.00083009
Iteration 17/25 | Loss: 0.00083009
Iteration 18/25 | Loss: 0.00083009
Iteration 19/25 | Loss: 0.00083009
Iteration 20/25 | Loss: 0.00083009
Iteration 21/25 | Loss: 0.00083009
Iteration 22/25 | Loss: 0.00083009
Iteration 23/25 | Loss: 0.00083009
Iteration 24/25 | Loss: 0.00083009
Iteration 25/25 | Loss: 0.00083009

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.13464928
Iteration 2/25 | Loss: 0.00071786
Iteration 3/25 | Loss: 0.00071784
Iteration 4/25 | Loss: 0.00071784
Iteration 5/25 | Loss: 0.00071784
Iteration 6/25 | Loss: 0.00071784
Iteration 7/25 | Loss: 0.00071784
Iteration 8/25 | Loss: 0.00071784
Iteration 9/25 | Loss: 0.00071783
Iteration 10/25 | Loss: 0.00071783
Iteration 11/25 | Loss: 0.00071783
Iteration 12/25 | Loss: 0.00071783
Iteration 13/25 | Loss: 0.00071783
Iteration 14/25 | Loss: 0.00071783
Iteration 15/25 | Loss: 0.00071783
Iteration 16/25 | Loss: 0.00071783
Iteration 17/25 | Loss: 0.00071783
Iteration 18/25 | Loss: 0.00071783
Iteration 19/25 | Loss: 0.00071783
Iteration 20/25 | Loss: 0.00071783
Iteration 21/25 | Loss: 0.00071783
Iteration 22/25 | Loss: 0.00071783
Iteration 23/25 | Loss: 0.00071783
Iteration 24/25 | Loss: 0.00071783
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007178346859291196, 0.0007178346859291196, 0.0007178346859291196, 0.0007178346859291196, 0.0007178346859291196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007178346859291196

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071783
Iteration 2/1000 | Loss: 0.00004162
Iteration 3/1000 | Loss: 0.00003034
Iteration 4/1000 | Loss: 0.00002862
Iteration 5/1000 | Loss: 0.00002749
Iteration 6/1000 | Loss: 0.00002647
Iteration 7/1000 | Loss: 0.00002586
Iteration 8/1000 | Loss: 0.00002557
Iteration 9/1000 | Loss: 0.00002547
Iteration 10/1000 | Loss: 0.00002547
Iteration 11/1000 | Loss: 0.00002547
Iteration 12/1000 | Loss: 0.00002547
Iteration 13/1000 | Loss: 0.00002547
Iteration 14/1000 | Loss: 0.00002547
Iteration 15/1000 | Loss: 0.00002547
Iteration 16/1000 | Loss: 0.00002544
Iteration 17/1000 | Loss: 0.00002543
Iteration 18/1000 | Loss: 0.00002542
Iteration 19/1000 | Loss: 0.00002542
Iteration 20/1000 | Loss: 0.00002537
Iteration 21/1000 | Loss: 0.00002536
Iteration 22/1000 | Loss: 0.00002533
Iteration 23/1000 | Loss: 0.00002531
Iteration 24/1000 | Loss: 0.00002531
Iteration 25/1000 | Loss: 0.00002531
Iteration 26/1000 | Loss: 0.00002530
Iteration 27/1000 | Loss: 0.00002528
Iteration 28/1000 | Loss: 0.00002527
Iteration 29/1000 | Loss: 0.00002527
Iteration 30/1000 | Loss: 0.00002527
Iteration 31/1000 | Loss: 0.00002527
Iteration 32/1000 | Loss: 0.00002526
Iteration 33/1000 | Loss: 0.00002526
Iteration 34/1000 | Loss: 0.00002526
Iteration 35/1000 | Loss: 0.00002526
Iteration 36/1000 | Loss: 0.00002525
Iteration 37/1000 | Loss: 0.00002525
Iteration 38/1000 | Loss: 0.00002524
Iteration 39/1000 | Loss: 0.00002523
Iteration 40/1000 | Loss: 0.00002522
Iteration 41/1000 | Loss: 0.00002522
Iteration 42/1000 | Loss: 0.00002521
Iteration 43/1000 | Loss: 0.00002521
Iteration 44/1000 | Loss: 0.00002521
Iteration 45/1000 | Loss: 0.00002521
Iteration 46/1000 | Loss: 0.00002521
Iteration 47/1000 | Loss: 0.00002521
Iteration 48/1000 | Loss: 0.00002521
Iteration 49/1000 | Loss: 0.00002521
Iteration 50/1000 | Loss: 0.00002521
Iteration 51/1000 | Loss: 0.00002521
Iteration 52/1000 | Loss: 0.00002519
Iteration 53/1000 | Loss: 0.00002518
Iteration 54/1000 | Loss: 0.00002518
Iteration 55/1000 | Loss: 0.00002518
Iteration 56/1000 | Loss: 0.00002518
Iteration 57/1000 | Loss: 0.00002517
Iteration 58/1000 | Loss: 0.00002517
Iteration 59/1000 | Loss: 0.00002517
Iteration 60/1000 | Loss: 0.00002516
Iteration 61/1000 | Loss: 0.00002516
Iteration 62/1000 | Loss: 0.00002516
Iteration 63/1000 | Loss: 0.00002515
Iteration 64/1000 | Loss: 0.00002514
Iteration 65/1000 | Loss: 0.00002514
Iteration 66/1000 | Loss: 0.00002513
Iteration 67/1000 | Loss: 0.00002513
Iteration 68/1000 | Loss: 0.00002513
Iteration 69/1000 | Loss: 0.00002513
Iteration 70/1000 | Loss: 0.00002513
Iteration 71/1000 | Loss: 0.00002513
Iteration 72/1000 | Loss: 0.00002513
Iteration 73/1000 | Loss: 0.00002512
Iteration 74/1000 | Loss: 0.00002512
Iteration 75/1000 | Loss: 0.00002512
Iteration 76/1000 | Loss: 0.00002512
Iteration 77/1000 | Loss: 0.00002512
Iteration 78/1000 | Loss: 0.00002512
Iteration 79/1000 | Loss: 0.00002512
Iteration 80/1000 | Loss: 0.00002512
Iteration 81/1000 | Loss: 0.00002512
Iteration 82/1000 | Loss: 0.00002510
Iteration 83/1000 | Loss: 0.00002510
Iteration 84/1000 | Loss: 0.00002510
Iteration 85/1000 | Loss: 0.00002510
Iteration 86/1000 | Loss: 0.00002510
Iteration 87/1000 | Loss: 0.00002510
Iteration 88/1000 | Loss: 0.00002510
Iteration 89/1000 | Loss: 0.00002510
Iteration 90/1000 | Loss: 0.00002510
Iteration 91/1000 | Loss: 0.00002509
Iteration 92/1000 | Loss: 0.00002509
Iteration 93/1000 | Loss: 0.00002509
Iteration 94/1000 | Loss: 0.00002509
Iteration 95/1000 | Loss: 0.00002508
Iteration 96/1000 | Loss: 0.00002508
Iteration 97/1000 | Loss: 0.00002508
Iteration 98/1000 | Loss: 0.00002507
Iteration 99/1000 | Loss: 0.00002507
Iteration 100/1000 | Loss: 0.00002507
Iteration 101/1000 | Loss: 0.00002507
Iteration 102/1000 | Loss: 0.00002507
Iteration 103/1000 | Loss: 0.00002507
Iteration 104/1000 | Loss: 0.00002507
Iteration 105/1000 | Loss: 0.00002506
Iteration 106/1000 | Loss: 0.00002505
Iteration 107/1000 | Loss: 0.00002505
Iteration 108/1000 | Loss: 0.00002505
Iteration 109/1000 | Loss: 0.00002504
Iteration 110/1000 | Loss: 0.00002504
Iteration 111/1000 | Loss: 0.00002504
Iteration 112/1000 | Loss: 0.00002504
Iteration 113/1000 | Loss: 0.00002503
Iteration 114/1000 | Loss: 0.00002503
Iteration 115/1000 | Loss: 0.00002503
Iteration 116/1000 | Loss: 0.00002502
Iteration 117/1000 | Loss: 0.00002502
Iteration 118/1000 | Loss: 0.00002502
Iteration 119/1000 | Loss: 0.00002501
Iteration 120/1000 | Loss: 0.00002501
Iteration 121/1000 | Loss: 0.00002501
Iteration 122/1000 | Loss: 0.00002501
Iteration 123/1000 | Loss: 0.00002501
Iteration 124/1000 | Loss: 0.00002501
Iteration 125/1000 | Loss: 0.00002501
Iteration 126/1000 | Loss: 0.00002501
Iteration 127/1000 | Loss: 0.00002501
Iteration 128/1000 | Loss: 0.00002501
Iteration 129/1000 | Loss: 0.00002501
Iteration 130/1000 | Loss: 0.00002501
Iteration 131/1000 | Loss: 0.00002501
Iteration 132/1000 | Loss: 0.00002501
Iteration 133/1000 | Loss: 0.00002501
Iteration 134/1000 | Loss: 0.00002501
Iteration 135/1000 | Loss: 0.00002501
Iteration 136/1000 | Loss: 0.00002501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.5009465389302932e-05, 2.5009465389302932e-05, 2.5009465389302932e-05, 2.5009465389302932e-05, 2.5009465389302932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5009465389302932e-05

Optimization complete. Final v2v error: 4.249228000640869 mm

Highest mean error: 4.873103618621826 mm for frame 144

Lowest mean error: 3.641383647918701 mm for frame 52

Saving results

Total time: 37.69793510437012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0025/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0025/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857247
Iteration 2/25 | Loss: 0.00126801
Iteration 3/25 | Loss: 0.00103519
Iteration 4/25 | Loss: 0.00088149
Iteration 5/25 | Loss: 0.00085381
Iteration 6/25 | Loss: 0.00083284
Iteration 7/25 | Loss: 0.00083112
Iteration 8/25 | Loss: 0.00083059
Iteration 9/25 | Loss: 0.00083046
Iteration 10/25 | Loss: 0.00083044
Iteration 11/25 | Loss: 0.00083044
Iteration 12/25 | Loss: 0.00083044
Iteration 13/25 | Loss: 0.00083044
Iteration 14/25 | Loss: 0.00083044
Iteration 15/25 | Loss: 0.00083044
Iteration 16/25 | Loss: 0.00083044
Iteration 17/25 | Loss: 0.00083043
Iteration 18/25 | Loss: 0.00083043
Iteration 19/25 | Loss: 0.00083043
Iteration 20/25 | Loss: 0.00083043
Iteration 21/25 | Loss: 0.00083043
Iteration 22/25 | Loss: 0.00083043
Iteration 23/25 | Loss: 0.00083043
Iteration 24/25 | Loss: 0.00083043
Iteration 25/25 | Loss: 0.00083043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.09058523
Iteration 2/25 | Loss: 0.00086034
Iteration 3/25 | Loss: 0.00086033
Iteration 4/25 | Loss: 0.00086033
Iteration 5/25 | Loss: 0.00086033
Iteration 6/25 | Loss: 0.00086033
Iteration 7/25 | Loss: 0.00086033
Iteration 8/25 | Loss: 0.00086033
Iteration 9/25 | Loss: 0.00086033
Iteration 10/25 | Loss: 0.00086033
Iteration 11/25 | Loss: 0.00086033
Iteration 12/25 | Loss: 0.00086033
Iteration 13/25 | Loss: 0.00086033
Iteration 14/25 | Loss: 0.00086033
Iteration 15/25 | Loss: 0.00086033
Iteration 16/25 | Loss: 0.00086033
Iteration 17/25 | Loss: 0.00086033
Iteration 18/25 | Loss: 0.00086033
Iteration 19/25 | Loss: 0.00086033
Iteration 20/25 | Loss: 0.00086033
Iteration 21/25 | Loss: 0.00086033
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008603267488069832, 0.0008603267488069832, 0.0008603267488069832, 0.0008603267488069832, 0.0008603267488069832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008603267488069832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086033
Iteration 2/1000 | Loss: 0.00007687
Iteration 3/1000 | Loss: 0.00019400
Iteration 4/1000 | Loss: 0.00003919
Iteration 5/1000 | Loss: 0.00003047
Iteration 6/1000 | Loss: 0.00002691
Iteration 7/1000 | Loss: 0.00002472
Iteration 8/1000 | Loss: 0.00002406
Iteration 9/1000 | Loss: 0.00002362
Iteration 10/1000 | Loss: 0.00002334
Iteration 11/1000 | Loss: 0.00002329
Iteration 12/1000 | Loss: 0.00002308
Iteration 13/1000 | Loss: 0.00002302
Iteration 14/1000 | Loss: 0.00002294
Iteration 15/1000 | Loss: 0.00002294
Iteration 16/1000 | Loss: 0.00002292
Iteration 17/1000 | Loss: 0.00002290
Iteration 18/1000 | Loss: 0.00002290
Iteration 19/1000 | Loss: 0.00002290
Iteration 20/1000 | Loss: 0.00002290
Iteration 21/1000 | Loss: 0.00002289
Iteration 22/1000 | Loss: 0.00002289
Iteration 23/1000 | Loss: 0.00002288
Iteration 24/1000 | Loss: 0.00002287
Iteration 25/1000 | Loss: 0.00002287
Iteration 26/1000 | Loss: 0.00002287
Iteration 27/1000 | Loss: 0.00002287
Iteration 28/1000 | Loss: 0.00002287
Iteration 29/1000 | Loss: 0.00002286
Iteration 30/1000 | Loss: 0.00002286
Iteration 31/1000 | Loss: 0.00002286
Iteration 32/1000 | Loss: 0.00002285
Iteration 33/1000 | Loss: 0.00002285
Iteration 34/1000 | Loss: 0.00002285
Iteration 35/1000 | Loss: 0.00002285
Iteration 36/1000 | Loss: 0.00002285
Iteration 37/1000 | Loss: 0.00002285
Iteration 38/1000 | Loss: 0.00002284
Iteration 39/1000 | Loss: 0.00002284
Iteration 40/1000 | Loss: 0.00002284
Iteration 41/1000 | Loss: 0.00002283
Iteration 42/1000 | Loss: 0.00002283
Iteration 43/1000 | Loss: 0.00002282
Iteration 44/1000 | Loss: 0.00002282
Iteration 45/1000 | Loss: 0.00002281
Iteration 46/1000 | Loss: 0.00002281
Iteration 47/1000 | Loss: 0.00002281
Iteration 48/1000 | Loss: 0.00002281
Iteration 49/1000 | Loss: 0.00002281
Iteration 50/1000 | Loss: 0.00002281
Iteration 51/1000 | Loss: 0.00002281
Iteration 52/1000 | Loss: 0.00002281
Iteration 53/1000 | Loss: 0.00002281
Iteration 54/1000 | Loss: 0.00002280
Iteration 55/1000 | Loss: 0.00002280
Iteration 56/1000 | Loss: 0.00002280
Iteration 57/1000 | Loss: 0.00002280
Iteration 58/1000 | Loss: 0.00002280
Iteration 59/1000 | Loss: 0.00002280
Iteration 60/1000 | Loss: 0.00002280
Iteration 61/1000 | Loss: 0.00002279
Iteration 62/1000 | Loss: 0.00002279
Iteration 63/1000 | Loss: 0.00002279
Iteration 64/1000 | Loss: 0.00002278
Iteration 65/1000 | Loss: 0.00002278
Iteration 66/1000 | Loss: 0.00002278
Iteration 67/1000 | Loss: 0.00002277
Iteration 68/1000 | Loss: 0.00002276
Iteration 69/1000 | Loss: 0.00002275
Iteration 70/1000 | Loss: 0.00002274
Iteration 71/1000 | Loss: 0.00002274
Iteration 72/1000 | Loss: 0.00002274
Iteration 73/1000 | Loss: 0.00002273
Iteration 74/1000 | Loss: 0.00002273
Iteration 75/1000 | Loss: 0.00002273
Iteration 76/1000 | Loss: 0.00002272
Iteration 77/1000 | Loss: 0.00002272
Iteration 78/1000 | Loss: 0.00002271
Iteration 79/1000 | Loss: 0.00002270
Iteration 80/1000 | Loss: 0.00002270
Iteration 81/1000 | Loss: 0.00002270
Iteration 82/1000 | Loss: 0.00002270
Iteration 83/1000 | Loss: 0.00002270
Iteration 84/1000 | Loss: 0.00002270
Iteration 85/1000 | Loss: 0.00002270
Iteration 86/1000 | Loss: 0.00002270
Iteration 87/1000 | Loss: 0.00002270
Iteration 88/1000 | Loss: 0.00002270
Iteration 89/1000 | Loss: 0.00002270
Iteration 90/1000 | Loss: 0.00002270
Iteration 91/1000 | Loss: 0.00002270
Iteration 92/1000 | Loss: 0.00002269
Iteration 93/1000 | Loss: 0.00002269
Iteration 94/1000 | Loss: 0.00002269
Iteration 95/1000 | Loss: 0.00002269
Iteration 96/1000 | Loss: 0.00002268
Iteration 97/1000 | Loss: 0.00002268
Iteration 98/1000 | Loss: 0.00002268
Iteration 99/1000 | Loss: 0.00002268
Iteration 100/1000 | Loss: 0.00002268
Iteration 101/1000 | Loss: 0.00002268
Iteration 102/1000 | Loss: 0.00002267
Iteration 103/1000 | Loss: 0.00002267
Iteration 104/1000 | Loss: 0.00002267
Iteration 105/1000 | Loss: 0.00002266
Iteration 106/1000 | Loss: 0.00002266
Iteration 107/1000 | Loss: 0.00002266
Iteration 108/1000 | Loss: 0.00002266
Iteration 109/1000 | Loss: 0.00002265
Iteration 110/1000 | Loss: 0.00002265
Iteration 111/1000 | Loss: 0.00002265
Iteration 112/1000 | Loss: 0.00002265
Iteration 113/1000 | Loss: 0.00002265
Iteration 114/1000 | Loss: 0.00002265
Iteration 115/1000 | Loss: 0.00002265
Iteration 116/1000 | Loss: 0.00002264
Iteration 117/1000 | Loss: 0.00002264
Iteration 118/1000 | Loss: 0.00002264
Iteration 119/1000 | Loss: 0.00002264
Iteration 120/1000 | Loss: 0.00002264
Iteration 121/1000 | Loss: 0.00002264
Iteration 122/1000 | Loss: 0.00002264
Iteration 123/1000 | Loss: 0.00002264
Iteration 124/1000 | Loss: 0.00002264
Iteration 125/1000 | Loss: 0.00002264
Iteration 126/1000 | Loss: 0.00002264
Iteration 127/1000 | Loss: 0.00002264
Iteration 128/1000 | Loss: 0.00002264
Iteration 129/1000 | Loss: 0.00002263
Iteration 130/1000 | Loss: 0.00002263
Iteration 131/1000 | Loss: 0.00002263
Iteration 132/1000 | Loss: 0.00002263
Iteration 133/1000 | Loss: 0.00002263
Iteration 134/1000 | Loss: 0.00002263
Iteration 135/1000 | Loss: 0.00002263
Iteration 136/1000 | Loss: 0.00002263
Iteration 137/1000 | Loss: 0.00002263
Iteration 138/1000 | Loss: 0.00002263
Iteration 139/1000 | Loss: 0.00002263
Iteration 140/1000 | Loss: 0.00002263
Iteration 141/1000 | Loss: 0.00002263
Iteration 142/1000 | Loss: 0.00002263
Iteration 143/1000 | Loss: 0.00002263
Iteration 144/1000 | Loss: 0.00002263
Iteration 145/1000 | Loss: 0.00002263
Iteration 146/1000 | Loss: 0.00002263
Iteration 147/1000 | Loss: 0.00002263
Iteration 148/1000 | Loss: 0.00002263
Iteration 149/1000 | Loss: 0.00002263
Iteration 150/1000 | Loss: 0.00002263
Iteration 151/1000 | Loss: 0.00002263
Iteration 152/1000 | Loss: 0.00002263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.2629674276686274e-05, 2.2629674276686274e-05, 2.2629674276686274e-05, 2.2629674276686274e-05, 2.2629674276686274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2629674276686274e-05

Optimization complete. Final v2v error: 4.038965702056885 mm

Highest mean error: 4.510041236877441 mm for frame 28

Lowest mean error: 3.514709711074829 mm for frame 175

Saving results

Total time: 44.14941191673279
