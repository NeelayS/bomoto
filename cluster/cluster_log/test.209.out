Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=209, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11704-11759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030915
Iteration 2/25 | Loss: 0.01030915
Iteration 3/25 | Loss: 0.01030915
Iteration 4/25 | Loss: 0.01030914
Iteration 5/25 | Loss: 0.01030914
Iteration 6/25 | Loss: 0.01030914
Iteration 7/25 | Loss: 0.01030914
Iteration 8/25 | Loss: 0.01030914
Iteration 9/25 | Loss: 0.01030914
Iteration 10/25 | Loss: 0.01030914
Iteration 11/25 | Loss: 0.01030914
Iteration 12/25 | Loss: 0.01030914
Iteration 13/25 | Loss: 0.01030914
Iteration 14/25 | Loss: 0.01030914
Iteration 15/25 | Loss: 0.01030913
Iteration 16/25 | Loss: 0.01030913
Iteration 17/25 | Loss: 0.01030913
Iteration 18/25 | Loss: 0.01030913
Iteration 19/25 | Loss: 0.01030913
Iteration 20/25 | Loss: 0.01030913
Iteration 21/25 | Loss: 0.01030913
Iteration 22/25 | Loss: 0.01030913
Iteration 23/25 | Loss: 0.01030913
Iteration 24/25 | Loss: 0.01030913
Iteration 25/25 | Loss: 0.01030912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97963810
Iteration 2/25 | Loss: 0.07554697
Iteration 3/25 | Loss: 0.07554398
Iteration 4/25 | Loss: 0.07554398
Iteration 5/25 | Loss: 0.07554396
Iteration 6/25 | Loss: 0.07554396
Iteration 7/25 | Loss: 0.07554396
Iteration 8/25 | Loss: 0.07554396
Iteration 9/25 | Loss: 0.07554396
Iteration 10/25 | Loss: 0.07554396
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.07554396241903305, 0.07554396241903305, 0.07554396241903305, 0.07554396241903305, 0.07554396241903305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07554396241903305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07554396
Iteration 2/1000 | Loss: 0.00177231
Iteration 3/1000 | Loss: 0.00080822
Iteration 4/1000 | Loss: 0.00090047
Iteration 5/1000 | Loss: 0.00050581
Iteration 6/1000 | Loss: 0.00043817
Iteration 7/1000 | Loss: 0.00038716
Iteration 8/1000 | Loss: 0.00041170
Iteration 9/1000 | Loss: 0.00038159
Iteration 10/1000 | Loss: 0.00023611
Iteration 11/1000 | Loss: 0.00006293
Iteration 12/1000 | Loss: 0.00005649
Iteration 13/1000 | Loss: 0.00006344
Iteration 14/1000 | Loss: 0.00011403
Iteration 15/1000 | Loss: 0.00042258
Iteration 16/1000 | Loss: 0.00002991
Iteration 17/1000 | Loss: 0.00007567
Iteration 18/1000 | Loss: 0.00003262
Iteration 19/1000 | Loss: 0.00002552
Iteration 20/1000 | Loss: 0.00007199
Iteration 21/1000 | Loss: 0.00009335
Iteration 22/1000 | Loss: 0.00064594
Iteration 23/1000 | Loss: 0.00015823
Iteration 24/1000 | Loss: 0.00007703
Iteration 25/1000 | Loss: 0.00004601
Iteration 26/1000 | Loss: 0.00008578
Iteration 27/1000 | Loss: 0.00003721
Iteration 28/1000 | Loss: 0.00008650
Iteration 29/1000 | Loss: 0.00019348
Iteration 30/1000 | Loss: 0.00002806
Iteration 31/1000 | Loss: 0.00002945
Iteration 32/1000 | Loss: 0.00001972
Iteration 33/1000 | Loss: 0.00003056
Iteration 34/1000 | Loss: 0.00003054
Iteration 35/1000 | Loss: 0.00005300
Iteration 36/1000 | Loss: 0.00002749
Iteration 37/1000 | Loss: 0.00004669
Iteration 38/1000 | Loss: 0.00001832
Iteration 39/1000 | Loss: 0.00002529
Iteration 40/1000 | Loss: 0.00002160
Iteration 41/1000 | Loss: 0.00002628
Iteration 42/1000 | Loss: 0.00003612
Iteration 43/1000 | Loss: 0.00005616
Iteration 44/1000 | Loss: 0.00002384
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00006494
Iteration 47/1000 | Loss: 0.00013570
Iteration 48/1000 | Loss: 0.00002532
Iteration 49/1000 | Loss: 0.00002301
Iteration 50/1000 | Loss: 0.00001738
Iteration 51/1000 | Loss: 0.00002129
Iteration 52/1000 | Loss: 0.00001663
Iteration 53/1000 | Loss: 0.00001616
Iteration 54/1000 | Loss: 0.00001616
Iteration 55/1000 | Loss: 0.00003028
Iteration 56/1000 | Loss: 0.00002684
Iteration 57/1000 | Loss: 0.00001773
Iteration 58/1000 | Loss: 0.00004081
Iteration 59/1000 | Loss: 0.00011390
Iteration 60/1000 | Loss: 0.00002953
Iteration 61/1000 | Loss: 0.00001999
Iteration 62/1000 | Loss: 0.00002812
Iteration 63/1000 | Loss: 0.00001678
Iteration 64/1000 | Loss: 0.00001892
Iteration 65/1000 | Loss: 0.00001596
Iteration 66/1000 | Loss: 0.00002413
Iteration 67/1000 | Loss: 0.00025013
Iteration 68/1000 | Loss: 0.00004155
Iteration 69/1000 | Loss: 0.00005114
Iteration 70/1000 | Loss: 0.00002209
Iteration 71/1000 | Loss: 0.00002130
Iteration 72/1000 | Loss: 0.00001666
Iteration 73/1000 | Loss: 0.00002484
Iteration 74/1000 | Loss: 0.00001641
Iteration 75/1000 | Loss: 0.00001571
Iteration 76/1000 | Loss: 0.00001570
Iteration 77/1000 | Loss: 0.00001570
Iteration 78/1000 | Loss: 0.00001569
Iteration 79/1000 | Loss: 0.00001569
Iteration 80/1000 | Loss: 0.00001569
Iteration 81/1000 | Loss: 0.00001568
Iteration 82/1000 | Loss: 0.00001568
Iteration 83/1000 | Loss: 0.00001568
Iteration 84/1000 | Loss: 0.00001567
Iteration 85/1000 | Loss: 0.00001567
Iteration 86/1000 | Loss: 0.00001567
Iteration 87/1000 | Loss: 0.00001566
Iteration 88/1000 | Loss: 0.00001566
Iteration 89/1000 | Loss: 0.00001566
Iteration 90/1000 | Loss: 0.00001566
Iteration 91/1000 | Loss: 0.00001684
Iteration 92/1000 | Loss: 0.00004463
Iteration 93/1000 | Loss: 0.00001602
Iteration 94/1000 | Loss: 0.00003704
Iteration 95/1000 | Loss: 0.00001552
Iteration 96/1000 | Loss: 0.00001551
Iteration 97/1000 | Loss: 0.00001550
Iteration 98/1000 | Loss: 0.00001549
Iteration 99/1000 | Loss: 0.00001548
Iteration 100/1000 | Loss: 0.00001548
Iteration 101/1000 | Loss: 0.00001547
Iteration 102/1000 | Loss: 0.00001547
Iteration 103/1000 | Loss: 0.00001547
Iteration 104/1000 | Loss: 0.00001547
Iteration 105/1000 | Loss: 0.00001546
Iteration 106/1000 | Loss: 0.00001546
Iteration 107/1000 | Loss: 0.00001546
Iteration 108/1000 | Loss: 0.00001545
Iteration 109/1000 | Loss: 0.00001545
Iteration 110/1000 | Loss: 0.00001545
Iteration 111/1000 | Loss: 0.00001545
Iteration 112/1000 | Loss: 0.00001544
Iteration 113/1000 | Loss: 0.00001544
Iteration 114/1000 | Loss: 0.00001544
Iteration 115/1000 | Loss: 0.00001544
Iteration 116/1000 | Loss: 0.00001544
Iteration 117/1000 | Loss: 0.00001544
Iteration 118/1000 | Loss: 0.00001544
Iteration 119/1000 | Loss: 0.00001544
Iteration 120/1000 | Loss: 0.00001544
Iteration 121/1000 | Loss: 0.00001543
Iteration 122/1000 | Loss: 0.00001543
Iteration 123/1000 | Loss: 0.00001542
Iteration 124/1000 | Loss: 0.00001542
Iteration 125/1000 | Loss: 0.00001542
Iteration 126/1000 | Loss: 0.00001542
Iteration 127/1000 | Loss: 0.00001541
Iteration 128/1000 | Loss: 0.00001541
Iteration 129/1000 | Loss: 0.00001541
Iteration 130/1000 | Loss: 0.00001541
Iteration 131/1000 | Loss: 0.00004610
Iteration 132/1000 | Loss: 0.00004267
Iteration 133/1000 | Loss: 0.00001792
Iteration 134/1000 | Loss: 0.00001724
Iteration 135/1000 | Loss: 0.00001543
Iteration 136/1000 | Loss: 0.00001542
Iteration 137/1000 | Loss: 0.00001603
Iteration 138/1000 | Loss: 0.00001595
Iteration 139/1000 | Loss: 0.00001538
Iteration 140/1000 | Loss: 0.00001538
Iteration 141/1000 | Loss: 0.00001551
Iteration 142/1000 | Loss: 0.00001537
Iteration 143/1000 | Loss: 0.00001537
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001537
Iteration 148/1000 | Loss: 0.00001537
Iteration 149/1000 | Loss: 0.00001537
Iteration 150/1000 | Loss: 0.00001537
Iteration 151/1000 | Loss: 0.00001537
Iteration 152/1000 | Loss: 0.00001537
Iteration 153/1000 | Loss: 0.00001537
Iteration 154/1000 | Loss: 0.00001537
Iteration 155/1000 | Loss: 0.00001537
Iteration 156/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.536773197585717e-05, 1.536773197585717e-05, 1.536773197585717e-05, 1.536773197585717e-05, 1.536773197585717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.536773197585717e-05

Optimization complete. Final v2v error: 3.334667205810547 mm

Highest mean error: 4.5405120849609375 mm for frame 114

Lowest mean error: 2.6959450244903564 mm for frame 69

Saving results

Total time: 138.3724558353424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045612
Iteration 2/25 | Loss: 0.00206079
Iteration 3/25 | Loss: 0.00164258
Iteration 4/25 | Loss: 0.00150877
Iteration 5/25 | Loss: 0.00150015
Iteration 6/25 | Loss: 0.00166671
Iteration 7/25 | Loss: 0.00145911
Iteration 8/25 | Loss: 0.00144579
Iteration 9/25 | Loss: 0.00124854
Iteration 10/25 | Loss: 0.00133593
Iteration 11/25 | Loss: 0.00126333
Iteration 12/25 | Loss: 0.00119189
Iteration 13/25 | Loss: 0.00121638
Iteration 14/25 | Loss: 0.00118576
Iteration 15/25 | Loss: 0.00117073
Iteration 16/25 | Loss: 0.00115650
Iteration 17/25 | Loss: 0.00115543
Iteration 18/25 | Loss: 0.00115394
Iteration 19/25 | Loss: 0.00115346
Iteration 20/25 | Loss: 0.00115356
Iteration 21/25 | Loss: 0.00115176
Iteration 22/25 | Loss: 0.00115356
Iteration 23/25 | Loss: 0.00114782
Iteration 24/25 | Loss: 0.00114544
Iteration 25/25 | Loss: 0.00114505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49597466
Iteration 2/25 | Loss: 0.00093255
Iteration 3/25 | Loss: 0.00093255
Iteration 4/25 | Loss: 0.00090715
Iteration 5/25 | Loss: 0.00090715
Iteration 6/25 | Loss: 0.00090715
Iteration 7/25 | Loss: 0.00090715
Iteration 8/25 | Loss: 0.00090715
Iteration 9/25 | Loss: 0.00090715
Iteration 10/25 | Loss: 0.00090715
Iteration 11/25 | Loss: 0.00090715
Iteration 12/25 | Loss: 0.00090715
Iteration 13/25 | Loss: 0.00090715
Iteration 14/25 | Loss: 0.00090715
Iteration 15/25 | Loss: 0.00090715
Iteration 16/25 | Loss: 0.00090715
Iteration 17/25 | Loss: 0.00090715
Iteration 18/25 | Loss: 0.00090715
Iteration 19/25 | Loss: 0.00090715
Iteration 20/25 | Loss: 0.00090715
Iteration 21/25 | Loss: 0.00090715
Iteration 22/25 | Loss: 0.00090715
Iteration 23/25 | Loss: 0.00090715
Iteration 24/25 | Loss: 0.00090715
Iteration 25/25 | Loss: 0.00090715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090715
Iteration 2/1000 | Loss: 0.00006577
Iteration 3/1000 | Loss: 0.00003921
Iteration 4/1000 | Loss: 0.00002492
Iteration 5/1000 | Loss: 0.00004685
Iteration 6/1000 | Loss: 0.00002981
Iteration 7/1000 | Loss: 0.00002869
Iteration 8/1000 | Loss: 0.00004745
Iteration 9/1000 | Loss: 0.00002816
Iteration 10/1000 | Loss: 0.00004330
Iteration 11/1000 | Loss: 0.00003219
Iteration 12/1000 | Loss: 0.00006284
Iteration 13/1000 | Loss: 0.00003363
Iteration 14/1000 | Loss: 0.00003424
Iteration 15/1000 | Loss: 0.00007830
Iteration 16/1000 | Loss: 0.00005192
Iteration 17/1000 | Loss: 0.00004381
Iteration 18/1000 | Loss: 0.00004698
Iteration 19/1000 | Loss: 0.00005265
Iteration 20/1000 | Loss: 0.00006259
Iteration 21/1000 | Loss: 0.00005173
Iteration 22/1000 | Loss: 0.00006003
Iteration 23/1000 | Loss: 0.00003208
Iteration 24/1000 | Loss: 0.00004821
Iteration 25/1000 | Loss: 0.00003111
Iteration 26/1000 | Loss: 0.00003529
Iteration 27/1000 | Loss: 0.00002079
Iteration 28/1000 | Loss: 0.00003923
Iteration 29/1000 | Loss: 0.00005638
Iteration 30/1000 | Loss: 0.00004857
Iteration 31/1000 | Loss: 0.00004818
Iteration 32/1000 | Loss: 0.00004608
Iteration 33/1000 | Loss: 0.00004560
Iteration 34/1000 | Loss: 0.00004222
Iteration 35/1000 | Loss: 0.00005146
Iteration 36/1000 | Loss: 0.00005115
Iteration 37/1000 | Loss: 0.00004951
Iteration 38/1000 | Loss: 0.00003973
Iteration 39/1000 | Loss: 0.00005248
Iteration 40/1000 | Loss: 0.00001575
Iteration 41/1000 | Loss: 0.00001339
Iteration 42/1000 | Loss: 0.00001236
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001169
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001165
Iteration 48/1000 | Loss: 0.00001151
Iteration 49/1000 | Loss: 0.00001136
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001126
Iteration 52/1000 | Loss: 0.00001122
Iteration 53/1000 | Loss: 0.00001121
Iteration 54/1000 | Loss: 0.00001118
Iteration 55/1000 | Loss: 0.00001118
Iteration 56/1000 | Loss: 0.00001118
Iteration 57/1000 | Loss: 0.00001117
Iteration 58/1000 | Loss: 0.00001117
Iteration 59/1000 | Loss: 0.00001117
Iteration 60/1000 | Loss: 0.00001117
Iteration 61/1000 | Loss: 0.00001117
Iteration 62/1000 | Loss: 0.00001117
Iteration 63/1000 | Loss: 0.00001116
Iteration 64/1000 | Loss: 0.00001115
Iteration 65/1000 | Loss: 0.00001115
Iteration 66/1000 | Loss: 0.00001115
Iteration 67/1000 | Loss: 0.00001114
Iteration 68/1000 | Loss: 0.00001114
Iteration 69/1000 | Loss: 0.00001113
Iteration 70/1000 | Loss: 0.00001113
Iteration 71/1000 | Loss: 0.00001113
Iteration 72/1000 | Loss: 0.00001112
Iteration 73/1000 | Loss: 0.00001112
Iteration 74/1000 | Loss: 0.00001112
Iteration 75/1000 | Loss: 0.00001111
Iteration 76/1000 | Loss: 0.00001111
Iteration 77/1000 | Loss: 0.00001111
Iteration 78/1000 | Loss: 0.00001110
Iteration 79/1000 | Loss: 0.00001109
Iteration 80/1000 | Loss: 0.00001109
Iteration 81/1000 | Loss: 0.00001109
Iteration 82/1000 | Loss: 0.00001109
Iteration 83/1000 | Loss: 0.00001108
Iteration 84/1000 | Loss: 0.00001108
Iteration 85/1000 | Loss: 0.00001108
Iteration 86/1000 | Loss: 0.00001108
Iteration 87/1000 | Loss: 0.00001108
Iteration 88/1000 | Loss: 0.00001107
Iteration 89/1000 | Loss: 0.00001107
Iteration 90/1000 | Loss: 0.00001107
Iteration 91/1000 | Loss: 0.00001107
Iteration 92/1000 | Loss: 0.00001107
Iteration 93/1000 | Loss: 0.00001107
Iteration 94/1000 | Loss: 0.00001107
Iteration 95/1000 | Loss: 0.00001107
Iteration 96/1000 | Loss: 0.00001107
Iteration 97/1000 | Loss: 0.00001107
Iteration 98/1000 | Loss: 0.00001106
Iteration 99/1000 | Loss: 0.00001106
Iteration 100/1000 | Loss: 0.00001105
Iteration 101/1000 | Loss: 0.00001105
Iteration 102/1000 | Loss: 0.00001105
Iteration 103/1000 | Loss: 0.00001105
Iteration 104/1000 | Loss: 0.00001105
Iteration 105/1000 | Loss: 0.00001105
Iteration 106/1000 | Loss: 0.00001104
Iteration 107/1000 | Loss: 0.00001104
Iteration 108/1000 | Loss: 0.00001104
Iteration 109/1000 | Loss: 0.00001104
Iteration 110/1000 | Loss: 0.00001104
Iteration 111/1000 | Loss: 0.00001104
Iteration 112/1000 | Loss: 0.00001104
Iteration 113/1000 | Loss: 0.00001104
Iteration 114/1000 | Loss: 0.00001103
Iteration 115/1000 | Loss: 0.00001103
Iteration 116/1000 | Loss: 0.00001103
Iteration 117/1000 | Loss: 0.00001103
Iteration 118/1000 | Loss: 0.00001102
Iteration 119/1000 | Loss: 0.00001102
Iteration 120/1000 | Loss: 0.00001102
Iteration 121/1000 | Loss: 0.00001102
Iteration 122/1000 | Loss: 0.00001102
Iteration 123/1000 | Loss: 0.00001102
Iteration 124/1000 | Loss: 0.00001101
Iteration 125/1000 | Loss: 0.00001101
Iteration 126/1000 | Loss: 0.00001101
Iteration 127/1000 | Loss: 0.00001101
Iteration 128/1000 | Loss: 0.00001101
Iteration 129/1000 | Loss: 0.00001101
Iteration 130/1000 | Loss: 0.00001101
Iteration 131/1000 | Loss: 0.00001100
Iteration 132/1000 | Loss: 0.00001100
Iteration 133/1000 | Loss: 0.00001100
Iteration 134/1000 | Loss: 0.00001100
Iteration 135/1000 | Loss: 0.00001099
Iteration 136/1000 | Loss: 0.00001099
Iteration 137/1000 | Loss: 0.00001099
Iteration 138/1000 | Loss: 0.00001099
Iteration 139/1000 | Loss: 0.00001099
Iteration 140/1000 | Loss: 0.00001098
Iteration 141/1000 | Loss: 0.00001098
Iteration 142/1000 | Loss: 0.00001098
Iteration 143/1000 | Loss: 0.00001098
Iteration 144/1000 | Loss: 0.00001098
Iteration 145/1000 | Loss: 0.00001098
Iteration 146/1000 | Loss: 0.00001098
Iteration 147/1000 | Loss: 0.00001098
Iteration 148/1000 | Loss: 0.00001098
Iteration 149/1000 | Loss: 0.00001098
Iteration 150/1000 | Loss: 0.00001098
Iteration 151/1000 | Loss: 0.00001098
Iteration 152/1000 | Loss: 0.00001097
Iteration 153/1000 | Loss: 0.00001097
Iteration 154/1000 | Loss: 0.00001097
Iteration 155/1000 | Loss: 0.00001096
Iteration 156/1000 | Loss: 0.00001096
Iteration 157/1000 | Loss: 0.00001096
Iteration 158/1000 | Loss: 0.00001096
Iteration 159/1000 | Loss: 0.00001096
Iteration 160/1000 | Loss: 0.00001096
Iteration 161/1000 | Loss: 0.00001096
Iteration 162/1000 | Loss: 0.00001095
Iteration 163/1000 | Loss: 0.00001095
Iteration 164/1000 | Loss: 0.00001095
Iteration 165/1000 | Loss: 0.00001094
Iteration 166/1000 | Loss: 0.00001094
Iteration 167/1000 | Loss: 0.00001094
Iteration 168/1000 | Loss: 0.00001094
Iteration 169/1000 | Loss: 0.00001094
Iteration 170/1000 | Loss: 0.00001094
Iteration 171/1000 | Loss: 0.00001094
Iteration 172/1000 | Loss: 0.00001094
Iteration 173/1000 | Loss: 0.00001094
Iteration 174/1000 | Loss: 0.00001094
Iteration 175/1000 | Loss: 0.00001094
Iteration 176/1000 | Loss: 0.00001094
Iteration 177/1000 | Loss: 0.00001093
Iteration 178/1000 | Loss: 0.00001093
Iteration 179/1000 | Loss: 0.00001093
Iteration 180/1000 | Loss: 0.00001093
Iteration 181/1000 | Loss: 0.00001093
Iteration 182/1000 | Loss: 0.00001093
Iteration 183/1000 | Loss: 0.00001093
Iteration 184/1000 | Loss: 0.00001093
Iteration 185/1000 | Loss: 0.00001093
Iteration 186/1000 | Loss: 0.00001093
Iteration 187/1000 | Loss: 0.00001093
Iteration 188/1000 | Loss: 0.00001093
Iteration 189/1000 | Loss: 0.00001093
Iteration 190/1000 | Loss: 0.00001093
Iteration 191/1000 | Loss: 0.00001092
Iteration 192/1000 | Loss: 0.00001092
Iteration 193/1000 | Loss: 0.00001092
Iteration 194/1000 | Loss: 0.00001092
Iteration 195/1000 | Loss: 0.00001092
Iteration 196/1000 | Loss: 0.00001092
Iteration 197/1000 | Loss: 0.00001092
Iteration 198/1000 | Loss: 0.00001091
Iteration 199/1000 | Loss: 0.00001091
Iteration 200/1000 | Loss: 0.00001091
Iteration 201/1000 | Loss: 0.00001091
Iteration 202/1000 | Loss: 0.00001091
Iteration 203/1000 | Loss: 0.00001091
Iteration 204/1000 | Loss: 0.00001091
Iteration 205/1000 | Loss: 0.00001091
Iteration 206/1000 | Loss: 0.00001090
Iteration 207/1000 | Loss: 0.00001089
Iteration 208/1000 | Loss: 0.00001089
Iteration 209/1000 | Loss: 0.00001088
Iteration 210/1000 | Loss: 0.00001088
Iteration 211/1000 | Loss: 0.00001087
Iteration 212/1000 | Loss: 0.00001087
Iteration 213/1000 | Loss: 0.00001087
Iteration 214/1000 | Loss: 0.00001087
Iteration 215/1000 | Loss: 0.00001087
Iteration 216/1000 | Loss: 0.00001087
Iteration 217/1000 | Loss: 0.00001086
Iteration 218/1000 | Loss: 0.00001086
Iteration 219/1000 | Loss: 0.00001086
Iteration 220/1000 | Loss: 0.00001086
Iteration 221/1000 | Loss: 0.00001086
Iteration 222/1000 | Loss: 0.00001086
Iteration 223/1000 | Loss: 0.00001086
Iteration 224/1000 | Loss: 0.00001086
Iteration 225/1000 | Loss: 0.00001085
Iteration 226/1000 | Loss: 0.00001085
Iteration 227/1000 | Loss: 0.00001085
Iteration 228/1000 | Loss: 0.00001085
Iteration 229/1000 | Loss: 0.00001085
Iteration 230/1000 | Loss: 0.00001084
Iteration 231/1000 | Loss: 0.00001084
Iteration 232/1000 | Loss: 0.00001084
Iteration 233/1000 | Loss: 0.00001084
Iteration 234/1000 | Loss: 0.00001084
Iteration 235/1000 | Loss: 0.00001084
Iteration 236/1000 | Loss: 0.00001084
Iteration 237/1000 | Loss: 0.00001084
Iteration 238/1000 | Loss: 0.00001084
Iteration 239/1000 | Loss: 0.00001084
Iteration 240/1000 | Loss: 0.00001084
Iteration 241/1000 | Loss: 0.00001084
Iteration 242/1000 | Loss: 0.00001084
Iteration 243/1000 | Loss: 0.00001084
Iteration 244/1000 | Loss: 0.00001084
Iteration 245/1000 | Loss: 0.00001084
Iteration 246/1000 | Loss: 0.00001084
Iteration 247/1000 | Loss: 0.00001084
Iteration 248/1000 | Loss: 0.00001084
Iteration 249/1000 | Loss: 0.00001084
Iteration 250/1000 | Loss: 0.00001084
Iteration 251/1000 | Loss: 0.00001084
Iteration 252/1000 | Loss: 0.00001084
Iteration 253/1000 | Loss: 0.00001084
Iteration 254/1000 | Loss: 0.00001084
Iteration 255/1000 | Loss: 0.00001084
Iteration 256/1000 | Loss: 0.00001084
Iteration 257/1000 | Loss: 0.00001084
Iteration 258/1000 | Loss: 0.00001084
Iteration 259/1000 | Loss: 0.00001084
Iteration 260/1000 | Loss: 0.00001084
Iteration 261/1000 | Loss: 0.00001084
Iteration 262/1000 | Loss: 0.00001084
Iteration 263/1000 | Loss: 0.00001084
Iteration 264/1000 | Loss: 0.00001084
Iteration 265/1000 | Loss: 0.00001084
Iteration 266/1000 | Loss: 0.00001084
Iteration 267/1000 | Loss: 0.00001084
Iteration 268/1000 | Loss: 0.00001084
Iteration 269/1000 | Loss: 0.00001084
Iteration 270/1000 | Loss: 0.00001084
Iteration 271/1000 | Loss: 0.00001084
Iteration 272/1000 | Loss: 0.00001084
Iteration 273/1000 | Loss: 0.00001084
Iteration 274/1000 | Loss: 0.00001084
Iteration 275/1000 | Loss: 0.00001084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.0839276910701301e-05, 1.0839276910701301e-05, 1.0839276910701301e-05, 1.0839276910701301e-05, 1.0839276910701301e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0839276910701301e-05

Optimization complete. Final v2v error: 2.7830095291137695 mm

Highest mean error: 3.709794044494629 mm for frame 58

Lowest mean error: 2.430021286010742 mm for frame 44

Saving results

Total time: 127.14516472816467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805450
Iteration 2/25 | Loss: 0.00137659
Iteration 3/25 | Loss: 0.00119834
Iteration 4/25 | Loss: 0.00117877
Iteration 5/25 | Loss: 0.00117308
Iteration 6/25 | Loss: 0.00117233
Iteration 7/25 | Loss: 0.00117233
Iteration 8/25 | Loss: 0.00117233
Iteration 9/25 | Loss: 0.00117233
Iteration 10/25 | Loss: 0.00117233
Iteration 11/25 | Loss: 0.00117233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011723283678293228, 0.0011723283678293228, 0.0011723283678293228, 0.0011723283678293228, 0.0011723283678293228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011723283678293228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31590617
Iteration 2/25 | Loss: 0.00084544
Iteration 3/25 | Loss: 0.00084542
Iteration 4/25 | Loss: 0.00084541
Iteration 5/25 | Loss: 0.00084541
Iteration 6/25 | Loss: 0.00084541
Iteration 7/25 | Loss: 0.00084541
Iteration 8/25 | Loss: 0.00084541
Iteration 9/25 | Loss: 0.00084541
Iteration 10/25 | Loss: 0.00084541
Iteration 11/25 | Loss: 0.00084541
Iteration 12/25 | Loss: 0.00084541
Iteration 13/25 | Loss: 0.00084541
Iteration 14/25 | Loss: 0.00084541
Iteration 15/25 | Loss: 0.00084541
Iteration 16/25 | Loss: 0.00084541
Iteration 17/25 | Loss: 0.00084541
Iteration 18/25 | Loss: 0.00084541
Iteration 19/25 | Loss: 0.00084541
Iteration 20/25 | Loss: 0.00084541
Iteration 21/25 | Loss: 0.00084541
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008454123162664473, 0.0008454123162664473, 0.0008454123162664473, 0.0008454123162664473, 0.0008454123162664473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008454123162664473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084541
Iteration 2/1000 | Loss: 0.00004636
Iteration 3/1000 | Loss: 0.00003010
Iteration 4/1000 | Loss: 0.00002545
Iteration 5/1000 | Loss: 0.00002373
Iteration 6/1000 | Loss: 0.00002211
Iteration 7/1000 | Loss: 0.00002100
Iteration 8/1000 | Loss: 0.00002032
Iteration 9/1000 | Loss: 0.00001981
Iteration 10/1000 | Loss: 0.00001949
Iteration 11/1000 | Loss: 0.00001916
Iteration 12/1000 | Loss: 0.00001890
Iteration 13/1000 | Loss: 0.00001874
Iteration 14/1000 | Loss: 0.00001865
Iteration 15/1000 | Loss: 0.00001857
Iteration 16/1000 | Loss: 0.00001844
Iteration 17/1000 | Loss: 0.00001839
Iteration 18/1000 | Loss: 0.00001837
Iteration 19/1000 | Loss: 0.00001836
Iteration 20/1000 | Loss: 0.00001835
Iteration 21/1000 | Loss: 0.00001834
Iteration 22/1000 | Loss: 0.00001834
Iteration 23/1000 | Loss: 0.00001832
Iteration 24/1000 | Loss: 0.00001831
Iteration 25/1000 | Loss: 0.00001830
Iteration 26/1000 | Loss: 0.00001829
Iteration 27/1000 | Loss: 0.00001829
Iteration 28/1000 | Loss: 0.00001828
Iteration 29/1000 | Loss: 0.00001827
Iteration 30/1000 | Loss: 0.00001826
Iteration 31/1000 | Loss: 0.00001825
Iteration 32/1000 | Loss: 0.00001824
Iteration 33/1000 | Loss: 0.00001823
Iteration 34/1000 | Loss: 0.00001815
Iteration 35/1000 | Loss: 0.00001808
Iteration 36/1000 | Loss: 0.00001804
Iteration 37/1000 | Loss: 0.00001804
Iteration 38/1000 | Loss: 0.00001804
Iteration 39/1000 | Loss: 0.00001803
Iteration 40/1000 | Loss: 0.00001803
Iteration 41/1000 | Loss: 0.00001803
Iteration 42/1000 | Loss: 0.00001802
Iteration 43/1000 | Loss: 0.00001802
Iteration 44/1000 | Loss: 0.00001801
Iteration 45/1000 | Loss: 0.00001801
Iteration 46/1000 | Loss: 0.00001801
Iteration 47/1000 | Loss: 0.00001801
Iteration 48/1000 | Loss: 0.00001800
Iteration 49/1000 | Loss: 0.00001800
Iteration 50/1000 | Loss: 0.00001800
Iteration 51/1000 | Loss: 0.00001800
Iteration 52/1000 | Loss: 0.00001800
Iteration 53/1000 | Loss: 0.00001800
Iteration 54/1000 | Loss: 0.00001800
Iteration 55/1000 | Loss: 0.00001800
Iteration 56/1000 | Loss: 0.00001799
Iteration 57/1000 | Loss: 0.00001799
Iteration 58/1000 | Loss: 0.00001799
Iteration 59/1000 | Loss: 0.00001799
Iteration 60/1000 | Loss: 0.00001799
Iteration 61/1000 | Loss: 0.00001798
Iteration 62/1000 | Loss: 0.00001798
Iteration 63/1000 | Loss: 0.00001798
Iteration 64/1000 | Loss: 0.00001798
Iteration 65/1000 | Loss: 0.00001798
Iteration 66/1000 | Loss: 0.00001798
Iteration 67/1000 | Loss: 0.00001798
Iteration 68/1000 | Loss: 0.00001797
Iteration 69/1000 | Loss: 0.00001797
Iteration 70/1000 | Loss: 0.00001797
Iteration 71/1000 | Loss: 0.00001797
Iteration 72/1000 | Loss: 0.00001797
Iteration 73/1000 | Loss: 0.00001797
Iteration 74/1000 | Loss: 0.00001797
Iteration 75/1000 | Loss: 0.00001797
Iteration 76/1000 | Loss: 0.00001796
Iteration 77/1000 | Loss: 0.00001796
Iteration 78/1000 | Loss: 0.00001796
Iteration 79/1000 | Loss: 0.00001796
Iteration 80/1000 | Loss: 0.00001796
Iteration 81/1000 | Loss: 0.00001796
Iteration 82/1000 | Loss: 0.00001796
Iteration 83/1000 | Loss: 0.00001796
Iteration 84/1000 | Loss: 0.00001795
Iteration 85/1000 | Loss: 0.00001795
Iteration 86/1000 | Loss: 0.00001795
Iteration 87/1000 | Loss: 0.00001795
Iteration 88/1000 | Loss: 0.00001795
Iteration 89/1000 | Loss: 0.00001794
Iteration 90/1000 | Loss: 0.00001794
Iteration 91/1000 | Loss: 0.00001794
Iteration 92/1000 | Loss: 0.00001794
Iteration 93/1000 | Loss: 0.00001794
Iteration 94/1000 | Loss: 0.00001794
Iteration 95/1000 | Loss: 0.00001794
Iteration 96/1000 | Loss: 0.00001794
Iteration 97/1000 | Loss: 0.00001793
Iteration 98/1000 | Loss: 0.00001793
Iteration 99/1000 | Loss: 0.00001793
Iteration 100/1000 | Loss: 0.00001793
Iteration 101/1000 | Loss: 0.00001793
Iteration 102/1000 | Loss: 0.00001793
Iteration 103/1000 | Loss: 0.00001793
Iteration 104/1000 | Loss: 0.00001792
Iteration 105/1000 | Loss: 0.00001792
Iteration 106/1000 | Loss: 0.00001792
Iteration 107/1000 | Loss: 0.00001792
Iteration 108/1000 | Loss: 0.00001792
Iteration 109/1000 | Loss: 0.00001792
Iteration 110/1000 | Loss: 0.00001792
Iteration 111/1000 | Loss: 0.00001791
Iteration 112/1000 | Loss: 0.00001791
Iteration 113/1000 | Loss: 0.00001791
Iteration 114/1000 | Loss: 0.00001791
Iteration 115/1000 | Loss: 0.00001790
Iteration 116/1000 | Loss: 0.00001790
Iteration 117/1000 | Loss: 0.00001790
Iteration 118/1000 | Loss: 0.00001789
Iteration 119/1000 | Loss: 0.00001789
Iteration 120/1000 | Loss: 0.00001789
Iteration 121/1000 | Loss: 0.00001789
Iteration 122/1000 | Loss: 0.00001789
Iteration 123/1000 | Loss: 0.00001789
Iteration 124/1000 | Loss: 0.00001789
Iteration 125/1000 | Loss: 0.00001788
Iteration 126/1000 | Loss: 0.00001788
Iteration 127/1000 | Loss: 0.00001788
Iteration 128/1000 | Loss: 0.00001788
Iteration 129/1000 | Loss: 0.00001788
Iteration 130/1000 | Loss: 0.00001788
Iteration 131/1000 | Loss: 0.00001788
Iteration 132/1000 | Loss: 0.00001787
Iteration 133/1000 | Loss: 0.00001787
Iteration 134/1000 | Loss: 0.00001787
Iteration 135/1000 | Loss: 0.00001787
Iteration 136/1000 | Loss: 0.00001787
Iteration 137/1000 | Loss: 0.00001787
Iteration 138/1000 | Loss: 0.00001787
Iteration 139/1000 | Loss: 0.00001787
Iteration 140/1000 | Loss: 0.00001787
Iteration 141/1000 | Loss: 0.00001786
Iteration 142/1000 | Loss: 0.00001786
Iteration 143/1000 | Loss: 0.00001786
Iteration 144/1000 | Loss: 0.00001786
Iteration 145/1000 | Loss: 0.00001786
Iteration 146/1000 | Loss: 0.00001786
Iteration 147/1000 | Loss: 0.00001786
Iteration 148/1000 | Loss: 0.00001786
Iteration 149/1000 | Loss: 0.00001785
Iteration 150/1000 | Loss: 0.00001785
Iteration 151/1000 | Loss: 0.00001785
Iteration 152/1000 | Loss: 0.00001785
Iteration 153/1000 | Loss: 0.00001784
Iteration 154/1000 | Loss: 0.00001784
Iteration 155/1000 | Loss: 0.00001784
Iteration 156/1000 | Loss: 0.00001784
Iteration 157/1000 | Loss: 0.00001783
Iteration 158/1000 | Loss: 0.00001783
Iteration 159/1000 | Loss: 0.00001783
Iteration 160/1000 | Loss: 0.00001783
Iteration 161/1000 | Loss: 0.00001783
Iteration 162/1000 | Loss: 0.00001783
Iteration 163/1000 | Loss: 0.00001783
Iteration 164/1000 | Loss: 0.00001783
Iteration 165/1000 | Loss: 0.00001782
Iteration 166/1000 | Loss: 0.00001782
Iteration 167/1000 | Loss: 0.00001782
Iteration 168/1000 | Loss: 0.00001782
Iteration 169/1000 | Loss: 0.00001782
Iteration 170/1000 | Loss: 0.00001782
Iteration 171/1000 | Loss: 0.00001782
Iteration 172/1000 | Loss: 0.00001782
Iteration 173/1000 | Loss: 0.00001781
Iteration 174/1000 | Loss: 0.00001781
Iteration 175/1000 | Loss: 0.00001781
Iteration 176/1000 | Loss: 0.00001781
Iteration 177/1000 | Loss: 0.00001781
Iteration 178/1000 | Loss: 0.00001781
Iteration 179/1000 | Loss: 0.00001781
Iteration 180/1000 | Loss: 0.00001781
Iteration 181/1000 | Loss: 0.00001781
Iteration 182/1000 | Loss: 0.00001781
Iteration 183/1000 | Loss: 0.00001781
Iteration 184/1000 | Loss: 0.00001781
Iteration 185/1000 | Loss: 0.00001781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.7811205907491967e-05, 1.7811205907491967e-05, 1.7811205907491967e-05, 1.7811205907491967e-05, 1.7811205907491967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7811205907491967e-05

Optimization complete. Final v2v error: 3.5328736305236816 mm

Highest mean error: 4.793294429779053 mm for frame 151

Lowest mean error: 2.7139739990234375 mm for frame 2

Saving results

Total time: 51.917057275772095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907199
Iteration 2/25 | Loss: 0.00131839
Iteration 3/25 | Loss: 0.00122116
Iteration 4/25 | Loss: 0.00119835
Iteration 5/25 | Loss: 0.00119260
Iteration 6/25 | Loss: 0.00119145
Iteration 7/25 | Loss: 0.00119145
Iteration 8/25 | Loss: 0.00119145
Iteration 9/25 | Loss: 0.00119145
Iteration 10/25 | Loss: 0.00119145
Iteration 11/25 | Loss: 0.00119145
Iteration 12/25 | Loss: 0.00119145
Iteration 13/25 | Loss: 0.00119145
Iteration 14/25 | Loss: 0.00119145
Iteration 15/25 | Loss: 0.00119145
Iteration 16/25 | Loss: 0.00119145
Iteration 17/25 | Loss: 0.00119145
Iteration 18/25 | Loss: 0.00119145
Iteration 19/25 | Loss: 0.00119145
Iteration 20/25 | Loss: 0.00119145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001191448885947466, 0.001191448885947466, 0.001191448885947466, 0.001191448885947466, 0.001191448885947466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001191448885947466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.39462948
Iteration 2/25 | Loss: 0.00121635
Iteration 3/25 | Loss: 0.00121631
Iteration 4/25 | Loss: 0.00121631
Iteration 5/25 | Loss: 0.00121631
Iteration 6/25 | Loss: 0.00121631
Iteration 7/25 | Loss: 0.00121631
Iteration 8/25 | Loss: 0.00121631
Iteration 9/25 | Loss: 0.00121631
Iteration 10/25 | Loss: 0.00121631
Iteration 11/25 | Loss: 0.00121631
Iteration 12/25 | Loss: 0.00121631
Iteration 13/25 | Loss: 0.00121631
Iteration 14/25 | Loss: 0.00121631
Iteration 15/25 | Loss: 0.00121631
Iteration 16/25 | Loss: 0.00121631
Iteration 17/25 | Loss: 0.00121631
Iteration 18/25 | Loss: 0.00121631
Iteration 19/25 | Loss: 0.00121631
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012163094943389297, 0.0012163094943389297, 0.0012163094943389297, 0.0012163094943389297, 0.0012163094943389297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012163094943389297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121631
Iteration 2/1000 | Loss: 0.00003156
Iteration 3/1000 | Loss: 0.00002130
Iteration 4/1000 | Loss: 0.00001974
Iteration 5/1000 | Loss: 0.00001912
Iteration 6/1000 | Loss: 0.00001864
Iteration 7/1000 | Loss: 0.00001823
Iteration 8/1000 | Loss: 0.00001784
Iteration 9/1000 | Loss: 0.00001750
Iteration 10/1000 | Loss: 0.00001717
Iteration 11/1000 | Loss: 0.00001698
Iteration 12/1000 | Loss: 0.00001691
Iteration 13/1000 | Loss: 0.00001680
Iteration 14/1000 | Loss: 0.00001670
Iteration 15/1000 | Loss: 0.00001667
Iteration 16/1000 | Loss: 0.00001666
Iteration 17/1000 | Loss: 0.00001665
Iteration 18/1000 | Loss: 0.00001661
Iteration 19/1000 | Loss: 0.00001660
Iteration 20/1000 | Loss: 0.00001660
Iteration 21/1000 | Loss: 0.00001660
Iteration 22/1000 | Loss: 0.00001659
Iteration 23/1000 | Loss: 0.00001659
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001657
Iteration 26/1000 | Loss: 0.00001657
Iteration 27/1000 | Loss: 0.00001657
Iteration 28/1000 | Loss: 0.00001656
Iteration 29/1000 | Loss: 0.00001656
Iteration 30/1000 | Loss: 0.00001656
Iteration 31/1000 | Loss: 0.00001655
Iteration 32/1000 | Loss: 0.00001654
Iteration 33/1000 | Loss: 0.00001654
Iteration 34/1000 | Loss: 0.00001653
Iteration 35/1000 | Loss: 0.00001653
Iteration 36/1000 | Loss: 0.00001653
Iteration 37/1000 | Loss: 0.00001652
Iteration 38/1000 | Loss: 0.00001652
Iteration 39/1000 | Loss: 0.00001652
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001651
Iteration 42/1000 | Loss: 0.00001650
Iteration 43/1000 | Loss: 0.00001650
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001650
Iteration 46/1000 | Loss: 0.00001650
Iteration 47/1000 | Loss: 0.00001650
Iteration 48/1000 | Loss: 0.00001650
Iteration 49/1000 | Loss: 0.00001650
Iteration 50/1000 | Loss: 0.00001650
Iteration 51/1000 | Loss: 0.00001650
Iteration 52/1000 | Loss: 0.00001650
Iteration 53/1000 | Loss: 0.00001649
Iteration 54/1000 | Loss: 0.00001649
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001649
Iteration 57/1000 | Loss: 0.00001649
Iteration 58/1000 | Loss: 0.00001648
Iteration 59/1000 | Loss: 0.00001648
Iteration 60/1000 | Loss: 0.00001648
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001648
Iteration 63/1000 | Loss: 0.00001648
Iteration 64/1000 | Loss: 0.00001648
Iteration 65/1000 | Loss: 0.00001648
Iteration 66/1000 | Loss: 0.00001648
Iteration 67/1000 | Loss: 0.00001647
Iteration 68/1000 | Loss: 0.00001647
Iteration 69/1000 | Loss: 0.00001647
Iteration 70/1000 | Loss: 0.00001647
Iteration 71/1000 | Loss: 0.00001647
Iteration 72/1000 | Loss: 0.00001647
Iteration 73/1000 | Loss: 0.00001647
Iteration 74/1000 | Loss: 0.00001647
Iteration 75/1000 | Loss: 0.00001647
Iteration 76/1000 | Loss: 0.00001647
Iteration 77/1000 | Loss: 0.00001647
Iteration 78/1000 | Loss: 0.00001647
Iteration 79/1000 | Loss: 0.00001647
Iteration 80/1000 | Loss: 0.00001647
Iteration 81/1000 | Loss: 0.00001647
Iteration 82/1000 | Loss: 0.00001646
Iteration 83/1000 | Loss: 0.00001646
Iteration 84/1000 | Loss: 0.00001646
Iteration 85/1000 | Loss: 0.00001646
Iteration 86/1000 | Loss: 0.00001646
Iteration 87/1000 | Loss: 0.00001646
Iteration 88/1000 | Loss: 0.00001646
Iteration 89/1000 | Loss: 0.00001646
Iteration 90/1000 | Loss: 0.00001646
Iteration 91/1000 | Loss: 0.00001646
Iteration 92/1000 | Loss: 0.00001645
Iteration 93/1000 | Loss: 0.00001645
Iteration 94/1000 | Loss: 0.00001645
Iteration 95/1000 | Loss: 0.00001645
Iteration 96/1000 | Loss: 0.00001645
Iteration 97/1000 | Loss: 0.00001645
Iteration 98/1000 | Loss: 0.00001645
Iteration 99/1000 | Loss: 0.00001645
Iteration 100/1000 | Loss: 0.00001645
Iteration 101/1000 | Loss: 0.00001645
Iteration 102/1000 | Loss: 0.00001645
Iteration 103/1000 | Loss: 0.00001645
Iteration 104/1000 | Loss: 0.00001644
Iteration 105/1000 | Loss: 0.00001644
Iteration 106/1000 | Loss: 0.00001644
Iteration 107/1000 | Loss: 0.00001644
Iteration 108/1000 | Loss: 0.00001644
Iteration 109/1000 | Loss: 0.00001644
Iteration 110/1000 | Loss: 0.00001644
Iteration 111/1000 | Loss: 0.00001644
Iteration 112/1000 | Loss: 0.00001644
Iteration 113/1000 | Loss: 0.00001644
Iteration 114/1000 | Loss: 0.00001644
Iteration 115/1000 | Loss: 0.00001644
Iteration 116/1000 | Loss: 0.00001644
Iteration 117/1000 | Loss: 0.00001644
Iteration 118/1000 | Loss: 0.00001643
Iteration 119/1000 | Loss: 0.00001643
Iteration 120/1000 | Loss: 0.00001643
Iteration 121/1000 | Loss: 0.00001643
Iteration 122/1000 | Loss: 0.00001643
Iteration 123/1000 | Loss: 0.00001643
Iteration 124/1000 | Loss: 0.00001643
Iteration 125/1000 | Loss: 0.00001643
Iteration 126/1000 | Loss: 0.00001643
Iteration 127/1000 | Loss: 0.00001643
Iteration 128/1000 | Loss: 0.00001643
Iteration 129/1000 | Loss: 0.00001643
Iteration 130/1000 | Loss: 0.00001643
Iteration 131/1000 | Loss: 0.00001643
Iteration 132/1000 | Loss: 0.00001643
Iteration 133/1000 | Loss: 0.00001643
Iteration 134/1000 | Loss: 0.00001643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.6434620192740113e-05, 1.6434620192740113e-05, 1.6434620192740113e-05, 1.6434620192740113e-05, 1.6434620192740113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6434620192740113e-05

Optimization complete. Final v2v error: 3.4701449871063232 mm

Highest mean error: 3.972665786743164 mm for frame 65

Lowest mean error: 3.0888073444366455 mm for frame 77

Saving results

Total time: 36.260798931121826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813105
Iteration 2/25 | Loss: 0.00142418
Iteration 3/25 | Loss: 0.00121131
Iteration 4/25 | Loss: 0.00119457
Iteration 5/25 | Loss: 0.00119120
Iteration 6/25 | Loss: 0.00119099
Iteration 7/25 | Loss: 0.00119099
Iteration 8/25 | Loss: 0.00119099
Iteration 9/25 | Loss: 0.00119099
Iteration 10/25 | Loss: 0.00119099
Iteration 11/25 | Loss: 0.00119099
Iteration 12/25 | Loss: 0.00119099
Iteration 13/25 | Loss: 0.00119099
Iteration 14/25 | Loss: 0.00119099
Iteration 15/25 | Loss: 0.00119099
Iteration 16/25 | Loss: 0.00119099
Iteration 17/25 | Loss: 0.00119099
Iteration 18/25 | Loss: 0.00119099
Iteration 19/25 | Loss: 0.00119099
Iteration 20/25 | Loss: 0.00119099
Iteration 21/25 | Loss: 0.00119099
Iteration 22/25 | Loss: 0.00119099
Iteration 23/25 | Loss: 0.00119099
Iteration 24/25 | Loss: 0.00119099
Iteration 25/25 | Loss: 0.00119099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91229230
Iteration 2/25 | Loss: 0.00051290
Iteration 3/25 | Loss: 0.00051290
Iteration 4/25 | Loss: 0.00051290
Iteration 5/25 | Loss: 0.00051290
Iteration 6/25 | Loss: 0.00051290
Iteration 7/25 | Loss: 0.00051290
Iteration 8/25 | Loss: 0.00051290
Iteration 9/25 | Loss: 0.00051290
Iteration 10/25 | Loss: 0.00051290
Iteration 11/25 | Loss: 0.00051290
Iteration 12/25 | Loss: 0.00051289
Iteration 13/25 | Loss: 0.00051289
Iteration 14/25 | Loss: 0.00051289
Iteration 15/25 | Loss: 0.00051289
Iteration 16/25 | Loss: 0.00051289
Iteration 17/25 | Loss: 0.00051289
Iteration 18/25 | Loss: 0.00051289
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005128947668708861, 0.0005128947668708861, 0.0005128947668708861, 0.0005128947668708861, 0.0005128947668708861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005128947668708861

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051289
Iteration 2/1000 | Loss: 0.00003288
Iteration 3/1000 | Loss: 0.00002522
Iteration 4/1000 | Loss: 0.00002298
Iteration 5/1000 | Loss: 0.00002205
Iteration 6/1000 | Loss: 0.00002142
Iteration 7/1000 | Loss: 0.00002105
Iteration 8/1000 | Loss: 0.00002094
Iteration 9/1000 | Loss: 0.00002071
Iteration 10/1000 | Loss: 0.00002066
Iteration 11/1000 | Loss: 0.00002047
Iteration 12/1000 | Loss: 0.00002039
Iteration 13/1000 | Loss: 0.00002039
Iteration 14/1000 | Loss: 0.00002039
Iteration 15/1000 | Loss: 0.00002039
Iteration 16/1000 | Loss: 0.00002039
Iteration 17/1000 | Loss: 0.00002039
Iteration 18/1000 | Loss: 0.00002039
Iteration 19/1000 | Loss: 0.00002038
Iteration 20/1000 | Loss: 0.00002038
Iteration 21/1000 | Loss: 0.00002038
Iteration 22/1000 | Loss: 0.00002038
Iteration 23/1000 | Loss: 0.00002032
Iteration 24/1000 | Loss: 0.00002029
Iteration 25/1000 | Loss: 0.00002029
Iteration 26/1000 | Loss: 0.00002026
Iteration 27/1000 | Loss: 0.00002023
Iteration 28/1000 | Loss: 0.00002023
Iteration 29/1000 | Loss: 0.00002023
Iteration 30/1000 | Loss: 0.00002023
Iteration 31/1000 | Loss: 0.00002023
Iteration 32/1000 | Loss: 0.00002023
Iteration 33/1000 | Loss: 0.00002023
Iteration 34/1000 | Loss: 0.00002022
Iteration 35/1000 | Loss: 0.00002022
Iteration 36/1000 | Loss: 0.00002021
Iteration 37/1000 | Loss: 0.00002020
Iteration 38/1000 | Loss: 0.00002020
Iteration 39/1000 | Loss: 0.00002017
Iteration 40/1000 | Loss: 0.00002017
Iteration 41/1000 | Loss: 0.00002017
Iteration 42/1000 | Loss: 0.00002017
Iteration 43/1000 | Loss: 0.00002016
Iteration 44/1000 | Loss: 0.00002016
Iteration 45/1000 | Loss: 0.00002016
Iteration 46/1000 | Loss: 0.00002016
Iteration 47/1000 | Loss: 0.00002016
Iteration 48/1000 | Loss: 0.00002016
Iteration 49/1000 | Loss: 0.00002016
Iteration 50/1000 | Loss: 0.00002016
Iteration 51/1000 | Loss: 0.00002016
Iteration 52/1000 | Loss: 0.00002016
Iteration 53/1000 | Loss: 0.00002015
Iteration 54/1000 | Loss: 0.00002015
Iteration 55/1000 | Loss: 0.00002015
Iteration 56/1000 | Loss: 0.00002015
Iteration 57/1000 | Loss: 0.00002014
Iteration 58/1000 | Loss: 0.00002014
Iteration 59/1000 | Loss: 0.00002009
Iteration 60/1000 | Loss: 0.00002009
Iteration 61/1000 | Loss: 0.00002008
Iteration 62/1000 | Loss: 0.00002007
Iteration 63/1000 | Loss: 0.00002007
Iteration 64/1000 | Loss: 0.00002007
Iteration 65/1000 | Loss: 0.00002007
Iteration 66/1000 | Loss: 0.00002007
Iteration 67/1000 | Loss: 0.00002007
Iteration 68/1000 | Loss: 0.00002007
Iteration 69/1000 | Loss: 0.00002007
Iteration 70/1000 | Loss: 0.00002006
Iteration 71/1000 | Loss: 0.00002006
Iteration 72/1000 | Loss: 0.00002006
Iteration 73/1000 | Loss: 0.00002006
Iteration 74/1000 | Loss: 0.00002006
Iteration 75/1000 | Loss: 0.00002006
Iteration 76/1000 | Loss: 0.00002006
Iteration 77/1000 | Loss: 0.00002006
Iteration 78/1000 | Loss: 0.00002006
Iteration 79/1000 | Loss: 0.00002006
Iteration 80/1000 | Loss: 0.00002006
Iteration 81/1000 | Loss: 0.00002005
Iteration 82/1000 | Loss: 0.00002005
Iteration 83/1000 | Loss: 0.00002005
Iteration 84/1000 | Loss: 0.00002005
Iteration 85/1000 | Loss: 0.00002005
Iteration 86/1000 | Loss: 0.00002005
Iteration 87/1000 | Loss: 0.00002005
Iteration 88/1000 | Loss: 0.00002005
Iteration 89/1000 | Loss: 0.00002005
Iteration 90/1000 | Loss: 0.00002005
Iteration 91/1000 | Loss: 0.00002005
Iteration 92/1000 | Loss: 0.00002004
Iteration 93/1000 | Loss: 0.00002004
Iteration 94/1000 | Loss: 0.00002004
Iteration 95/1000 | Loss: 0.00002004
Iteration 96/1000 | Loss: 0.00002004
Iteration 97/1000 | Loss: 0.00002004
Iteration 98/1000 | Loss: 0.00002004
Iteration 99/1000 | Loss: 0.00002004
Iteration 100/1000 | Loss: 0.00002004
Iteration 101/1000 | Loss: 0.00002002
Iteration 102/1000 | Loss: 0.00002002
Iteration 103/1000 | Loss: 0.00002002
Iteration 104/1000 | Loss: 0.00002002
Iteration 105/1000 | Loss: 0.00002002
Iteration 106/1000 | Loss: 0.00002001
Iteration 107/1000 | Loss: 0.00002001
Iteration 108/1000 | Loss: 0.00002001
Iteration 109/1000 | Loss: 0.00002000
Iteration 110/1000 | Loss: 0.00002000
Iteration 111/1000 | Loss: 0.00002000
Iteration 112/1000 | Loss: 0.00002000
Iteration 113/1000 | Loss: 0.00002000
Iteration 114/1000 | Loss: 0.00002000
Iteration 115/1000 | Loss: 0.00001999
Iteration 116/1000 | Loss: 0.00001999
Iteration 117/1000 | Loss: 0.00001999
Iteration 118/1000 | Loss: 0.00001999
Iteration 119/1000 | Loss: 0.00001999
Iteration 120/1000 | Loss: 0.00001998
Iteration 121/1000 | Loss: 0.00001998
Iteration 122/1000 | Loss: 0.00001998
Iteration 123/1000 | Loss: 0.00001998
Iteration 124/1000 | Loss: 0.00001998
Iteration 125/1000 | Loss: 0.00001998
Iteration 126/1000 | Loss: 0.00001998
Iteration 127/1000 | Loss: 0.00001998
Iteration 128/1000 | Loss: 0.00001998
Iteration 129/1000 | Loss: 0.00001998
Iteration 130/1000 | Loss: 0.00001998
Iteration 131/1000 | Loss: 0.00001998
Iteration 132/1000 | Loss: 0.00001998
Iteration 133/1000 | Loss: 0.00001998
Iteration 134/1000 | Loss: 0.00001998
Iteration 135/1000 | Loss: 0.00001997
Iteration 136/1000 | Loss: 0.00001997
Iteration 137/1000 | Loss: 0.00001997
Iteration 138/1000 | Loss: 0.00001997
Iteration 139/1000 | Loss: 0.00001996
Iteration 140/1000 | Loss: 0.00001996
Iteration 141/1000 | Loss: 0.00001996
Iteration 142/1000 | Loss: 0.00001996
Iteration 143/1000 | Loss: 0.00001996
Iteration 144/1000 | Loss: 0.00001996
Iteration 145/1000 | Loss: 0.00001996
Iteration 146/1000 | Loss: 0.00001996
Iteration 147/1000 | Loss: 0.00001995
Iteration 148/1000 | Loss: 0.00001995
Iteration 149/1000 | Loss: 0.00001995
Iteration 150/1000 | Loss: 0.00001995
Iteration 151/1000 | Loss: 0.00001995
Iteration 152/1000 | Loss: 0.00001995
Iteration 153/1000 | Loss: 0.00001995
Iteration 154/1000 | Loss: 0.00001994
Iteration 155/1000 | Loss: 0.00001994
Iteration 156/1000 | Loss: 0.00001994
Iteration 157/1000 | Loss: 0.00001994
Iteration 158/1000 | Loss: 0.00001994
Iteration 159/1000 | Loss: 0.00001994
Iteration 160/1000 | Loss: 0.00001994
Iteration 161/1000 | Loss: 0.00001994
Iteration 162/1000 | Loss: 0.00001994
Iteration 163/1000 | Loss: 0.00001994
Iteration 164/1000 | Loss: 0.00001994
Iteration 165/1000 | Loss: 0.00001994
Iteration 166/1000 | Loss: 0.00001994
Iteration 167/1000 | Loss: 0.00001994
Iteration 168/1000 | Loss: 0.00001994
Iteration 169/1000 | Loss: 0.00001994
Iteration 170/1000 | Loss: 0.00001994
Iteration 171/1000 | Loss: 0.00001994
Iteration 172/1000 | Loss: 0.00001994
Iteration 173/1000 | Loss: 0.00001994
Iteration 174/1000 | Loss: 0.00001994
Iteration 175/1000 | Loss: 0.00001994
Iteration 176/1000 | Loss: 0.00001994
Iteration 177/1000 | Loss: 0.00001994
Iteration 178/1000 | Loss: 0.00001994
Iteration 179/1000 | Loss: 0.00001994
Iteration 180/1000 | Loss: 0.00001994
Iteration 181/1000 | Loss: 0.00001994
Iteration 182/1000 | Loss: 0.00001994
Iteration 183/1000 | Loss: 0.00001994
Iteration 184/1000 | Loss: 0.00001994
Iteration 185/1000 | Loss: 0.00001994
Iteration 186/1000 | Loss: 0.00001994
Iteration 187/1000 | Loss: 0.00001994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.993813566514291e-05, 1.993813566514291e-05, 1.993813566514291e-05, 1.993813566514291e-05, 1.993813566514291e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.993813566514291e-05

Optimization complete. Final v2v error: 3.7163994312286377 mm

Highest mean error: 3.826939105987549 mm for frame 68

Lowest mean error: 3.5971779823303223 mm for frame 149

Saving results

Total time: 35.09303164482117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457688
Iteration 2/25 | Loss: 0.00137259
Iteration 3/25 | Loss: 0.00120465
Iteration 4/25 | Loss: 0.00118871
Iteration 5/25 | Loss: 0.00118519
Iteration 6/25 | Loss: 0.00118425
Iteration 7/25 | Loss: 0.00118415
Iteration 8/25 | Loss: 0.00118415
Iteration 9/25 | Loss: 0.00118415
Iteration 10/25 | Loss: 0.00118415
Iteration 11/25 | Loss: 0.00118415
Iteration 12/25 | Loss: 0.00118415
Iteration 13/25 | Loss: 0.00118415
Iteration 14/25 | Loss: 0.00118415
Iteration 15/25 | Loss: 0.00118415
Iteration 16/25 | Loss: 0.00118415
Iteration 17/25 | Loss: 0.00118415
Iteration 18/25 | Loss: 0.00118415
Iteration 19/25 | Loss: 0.00118415
Iteration 20/25 | Loss: 0.00118415
Iteration 21/25 | Loss: 0.00118415
Iteration 22/25 | Loss: 0.00118415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001184146967716515, 0.001184146967716515, 0.001184146967716515, 0.001184146967716515, 0.001184146967716515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001184146967716515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46767414
Iteration 2/25 | Loss: 0.00069961
Iteration 3/25 | Loss: 0.00069961
Iteration 4/25 | Loss: 0.00069961
Iteration 5/25 | Loss: 0.00069961
Iteration 6/25 | Loss: 0.00069961
Iteration 7/25 | Loss: 0.00069961
Iteration 8/25 | Loss: 0.00069961
Iteration 9/25 | Loss: 0.00069961
Iteration 10/25 | Loss: 0.00069961
Iteration 11/25 | Loss: 0.00069961
Iteration 12/25 | Loss: 0.00069961
Iteration 13/25 | Loss: 0.00069961
Iteration 14/25 | Loss: 0.00069961
Iteration 15/25 | Loss: 0.00069961
Iteration 16/25 | Loss: 0.00069961
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006996109150350094, 0.0006996109150350094, 0.0006996109150350094, 0.0006996109150350094, 0.0006996109150350094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006996109150350094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069961
Iteration 2/1000 | Loss: 0.00003678
Iteration 3/1000 | Loss: 0.00002261
Iteration 4/1000 | Loss: 0.00001866
Iteration 5/1000 | Loss: 0.00001736
Iteration 6/1000 | Loss: 0.00001659
Iteration 7/1000 | Loss: 0.00001601
Iteration 8/1000 | Loss: 0.00001560
Iteration 9/1000 | Loss: 0.00001523
Iteration 10/1000 | Loss: 0.00001515
Iteration 11/1000 | Loss: 0.00001495
Iteration 12/1000 | Loss: 0.00001479
Iteration 13/1000 | Loss: 0.00001472
Iteration 14/1000 | Loss: 0.00001465
Iteration 15/1000 | Loss: 0.00001461
Iteration 16/1000 | Loss: 0.00001460
Iteration 17/1000 | Loss: 0.00001453
Iteration 18/1000 | Loss: 0.00001450
Iteration 19/1000 | Loss: 0.00001450
Iteration 20/1000 | Loss: 0.00001446
Iteration 21/1000 | Loss: 0.00001446
Iteration 22/1000 | Loss: 0.00001445
Iteration 23/1000 | Loss: 0.00001441
Iteration 24/1000 | Loss: 0.00001441
Iteration 25/1000 | Loss: 0.00001439
Iteration 26/1000 | Loss: 0.00001433
Iteration 27/1000 | Loss: 0.00001430
Iteration 28/1000 | Loss: 0.00001427
Iteration 29/1000 | Loss: 0.00001426
Iteration 30/1000 | Loss: 0.00001425
Iteration 31/1000 | Loss: 0.00001424
Iteration 32/1000 | Loss: 0.00001424
Iteration 33/1000 | Loss: 0.00001423
Iteration 34/1000 | Loss: 0.00001422
Iteration 35/1000 | Loss: 0.00001422
Iteration 36/1000 | Loss: 0.00001421
Iteration 37/1000 | Loss: 0.00001421
Iteration 38/1000 | Loss: 0.00001421
Iteration 39/1000 | Loss: 0.00001421
Iteration 40/1000 | Loss: 0.00001419
Iteration 41/1000 | Loss: 0.00001419
Iteration 42/1000 | Loss: 0.00001419
Iteration 43/1000 | Loss: 0.00001418
Iteration 44/1000 | Loss: 0.00001418
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001416
Iteration 49/1000 | Loss: 0.00001416
Iteration 50/1000 | Loss: 0.00001416
Iteration 51/1000 | Loss: 0.00001416
Iteration 52/1000 | Loss: 0.00001416
Iteration 53/1000 | Loss: 0.00001416
Iteration 54/1000 | Loss: 0.00001415
Iteration 55/1000 | Loss: 0.00001415
Iteration 56/1000 | Loss: 0.00001415
Iteration 57/1000 | Loss: 0.00001415
Iteration 58/1000 | Loss: 0.00001414
Iteration 59/1000 | Loss: 0.00001414
Iteration 60/1000 | Loss: 0.00001414
Iteration 61/1000 | Loss: 0.00001414
Iteration 62/1000 | Loss: 0.00001414
Iteration 63/1000 | Loss: 0.00001413
Iteration 64/1000 | Loss: 0.00001413
Iteration 65/1000 | Loss: 0.00001413
Iteration 66/1000 | Loss: 0.00001413
Iteration 67/1000 | Loss: 0.00001413
Iteration 68/1000 | Loss: 0.00001413
Iteration 69/1000 | Loss: 0.00001413
Iteration 70/1000 | Loss: 0.00001412
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001412
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001412
Iteration 75/1000 | Loss: 0.00001411
Iteration 76/1000 | Loss: 0.00001411
Iteration 77/1000 | Loss: 0.00001411
Iteration 78/1000 | Loss: 0.00001411
Iteration 79/1000 | Loss: 0.00001410
Iteration 80/1000 | Loss: 0.00001409
Iteration 81/1000 | Loss: 0.00001409
Iteration 82/1000 | Loss: 0.00001408
Iteration 83/1000 | Loss: 0.00001408
Iteration 84/1000 | Loss: 0.00001408
Iteration 85/1000 | Loss: 0.00001407
Iteration 86/1000 | Loss: 0.00001407
Iteration 87/1000 | Loss: 0.00001407
Iteration 88/1000 | Loss: 0.00001407
Iteration 89/1000 | Loss: 0.00001405
Iteration 90/1000 | Loss: 0.00001405
Iteration 91/1000 | Loss: 0.00001405
Iteration 92/1000 | Loss: 0.00001404
Iteration 93/1000 | Loss: 0.00001404
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001403
Iteration 96/1000 | Loss: 0.00001403
Iteration 97/1000 | Loss: 0.00001403
Iteration 98/1000 | Loss: 0.00001402
Iteration 99/1000 | Loss: 0.00001402
Iteration 100/1000 | Loss: 0.00001402
Iteration 101/1000 | Loss: 0.00001402
Iteration 102/1000 | Loss: 0.00001402
Iteration 103/1000 | Loss: 0.00001401
Iteration 104/1000 | Loss: 0.00001401
Iteration 105/1000 | Loss: 0.00001401
Iteration 106/1000 | Loss: 0.00001401
Iteration 107/1000 | Loss: 0.00001401
Iteration 108/1000 | Loss: 0.00001400
Iteration 109/1000 | Loss: 0.00001400
Iteration 110/1000 | Loss: 0.00001400
Iteration 111/1000 | Loss: 0.00001399
Iteration 112/1000 | Loss: 0.00001399
Iteration 113/1000 | Loss: 0.00001399
Iteration 114/1000 | Loss: 0.00001399
Iteration 115/1000 | Loss: 0.00001399
Iteration 116/1000 | Loss: 0.00001399
Iteration 117/1000 | Loss: 0.00001398
Iteration 118/1000 | Loss: 0.00001398
Iteration 119/1000 | Loss: 0.00001398
Iteration 120/1000 | Loss: 0.00001398
Iteration 121/1000 | Loss: 0.00001398
Iteration 122/1000 | Loss: 0.00001397
Iteration 123/1000 | Loss: 0.00001397
Iteration 124/1000 | Loss: 0.00001397
Iteration 125/1000 | Loss: 0.00001397
Iteration 126/1000 | Loss: 0.00001397
Iteration 127/1000 | Loss: 0.00001396
Iteration 128/1000 | Loss: 0.00001396
Iteration 129/1000 | Loss: 0.00001396
Iteration 130/1000 | Loss: 0.00001396
Iteration 131/1000 | Loss: 0.00001396
Iteration 132/1000 | Loss: 0.00001396
Iteration 133/1000 | Loss: 0.00001395
Iteration 134/1000 | Loss: 0.00001395
Iteration 135/1000 | Loss: 0.00001395
Iteration 136/1000 | Loss: 0.00001395
Iteration 137/1000 | Loss: 0.00001395
Iteration 138/1000 | Loss: 0.00001395
Iteration 139/1000 | Loss: 0.00001395
Iteration 140/1000 | Loss: 0.00001394
Iteration 141/1000 | Loss: 0.00001394
Iteration 142/1000 | Loss: 0.00001394
Iteration 143/1000 | Loss: 0.00001394
Iteration 144/1000 | Loss: 0.00001394
Iteration 145/1000 | Loss: 0.00001394
Iteration 146/1000 | Loss: 0.00001394
Iteration 147/1000 | Loss: 0.00001394
Iteration 148/1000 | Loss: 0.00001394
Iteration 149/1000 | Loss: 0.00001394
Iteration 150/1000 | Loss: 0.00001394
Iteration 151/1000 | Loss: 0.00001393
Iteration 152/1000 | Loss: 0.00001393
Iteration 153/1000 | Loss: 0.00001393
Iteration 154/1000 | Loss: 0.00001393
Iteration 155/1000 | Loss: 0.00001393
Iteration 156/1000 | Loss: 0.00001393
Iteration 157/1000 | Loss: 0.00001393
Iteration 158/1000 | Loss: 0.00001392
Iteration 159/1000 | Loss: 0.00001392
Iteration 160/1000 | Loss: 0.00001392
Iteration 161/1000 | Loss: 0.00001392
Iteration 162/1000 | Loss: 0.00001392
Iteration 163/1000 | Loss: 0.00001392
Iteration 164/1000 | Loss: 0.00001392
Iteration 165/1000 | Loss: 0.00001392
Iteration 166/1000 | Loss: 0.00001392
Iteration 167/1000 | Loss: 0.00001392
Iteration 168/1000 | Loss: 0.00001392
Iteration 169/1000 | Loss: 0.00001392
Iteration 170/1000 | Loss: 0.00001392
Iteration 171/1000 | Loss: 0.00001392
Iteration 172/1000 | Loss: 0.00001392
Iteration 173/1000 | Loss: 0.00001392
Iteration 174/1000 | Loss: 0.00001392
Iteration 175/1000 | Loss: 0.00001392
Iteration 176/1000 | Loss: 0.00001392
Iteration 177/1000 | Loss: 0.00001392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.3918475815444253e-05, 1.3918475815444253e-05, 1.3918475815444253e-05, 1.3918475815444253e-05, 1.3918475815444253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3918475815444253e-05

Optimization complete. Final v2v error: 3.118340015411377 mm

Highest mean error: 4.450564861297607 mm for frame 67

Lowest mean error: 2.6134932041168213 mm for frame 28

Saving results

Total time: 42.456499099731445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996111
Iteration 2/25 | Loss: 0.00165591
Iteration 3/25 | Loss: 0.00141371
Iteration 4/25 | Loss: 0.00129236
Iteration 5/25 | Loss: 0.00123817
Iteration 6/25 | Loss: 0.00122833
Iteration 7/25 | Loss: 0.00121589
Iteration 8/25 | Loss: 0.00120378
Iteration 9/25 | Loss: 0.00119279
Iteration 10/25 | Loss: 0.00118949
Iteration 11/25 | Loss: 0.00118708
Iteration 12/25 | Loss: 0.00118644
Iteration 13/25 | Loss: 0.00118619
Iteration 14/25 | Loss: 0.00118608
Iteration 15/25 | Loss: 0.00118601
Iteration 16/25 | Loss: 0.00118601
Iteration 17/25 | Loss: 0.00118601
Iteration 18/25 | Loss: 0.00118601
Iteration 19/25 | Loss: 0.00118601
Iteration 20/25 | Loss: 0.00118601
Iteration 21/25 | Loss: 0.00118601
Iteration 22/25 | Loss: 0.00118601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011860065860673785, 0.0011860065860673785, 0.0011860065860673785, 0.0011860065860673785, 0.0011860065860673785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011860065860673785

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.15346956
Iteration 2/25 | Loss: 0.00103216
Iteration 3/25 | Loss: 0.00103216
Iteration 4/25 | Loss: 0.00103216
Iteration 5/25 | Loss: 0.00103216
Iteration 6/25 | Loss: 0.00103216
Iteration 7/25 | Loss: 0.00103216
Iteration 8/25 | Loss: 0.00103216
Iteration 9/25 | Loss: 0.00103216
Iteration 10/25 | Loss: 0.00103216
Iteration 11/25 | Loss: 0.00103216
Iteration 12/25 | Loss: 0.00103216
Iteration 13/25 | Loss: 0.00103216
Iteration 14/25 | Loss: 0.00103216
Iteration 15/25 | Loss: 0.00103216
Iteration 16/25 | Loss: 0.00103216
Iteration 17/25 | Loss: 0.00103216
Iteration 18/25 | Loss: 0.00103216
Iteration 19/25 | Loss: 0.00103216
Iteration 20/25 | Loss: 0.00103216
Iteration 21/25 | Loss: 0.00103216
Iteration 22/25 | Loss: 0.00103216
Iteration 23/25 | Loss: 0.00103216
Iteration 24/25 | Loss: 0.00103216
Iteration 25/25 | Loss: 0.00103216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103216
Iteration 2/1000 | Loss: 0.00005167
Iteration 3/1000 | Loss: 0.00003606
Iteration 4/1000 | Loss: 0.00002916
Iteration 5/1000 | Loss: 0.00020828
Iteration 6/1000 | Loss: 0.00003534
Iteration 7/1000 | Loss: 0.00002724
Iteration 8/1000 | Loss: 0.00002519
Iteration 9/1000 | Loss: 0.00002435
Iteration 10/1000 | Loss: 0.00010381
Iteration 11/1000 | Loss: 0.00029058
Iteration 12/1000 | Loss: 0.00018206
Iteration 13/1000 | Loss: 0.00036587
Iteration 14/1000 | Loss: 0.00029689
Iteration 15/1000 | Loss: 0.00056304
Iteration 16/1000 | Loss: 0.00014822
Iteration 17/1000 | Loss: 0.00024579
Iteration 18/1000 | Loss: 0.00002800
Iteration 19/1000 | Loss: 0.00002403
Iteration 20/1000 | Loss: 0.00002192
Iteration 21/1000 | Loss: 0.00002096
Iteration 22/1000 | Loss: 0.00046305
Iteration 23/1000 | Loss: 0.00035086
Iteration 24/1000 | Loss: 0.00034789
Iteration 25/1000 | Loss: 0.00043244
Iteration 26/1000 | Loss: 0.00033071
Iteration 27/1000 | Loss: 0.00016505
Iteration 28/1000 | Loss: 0.00016127
Iteration 29/1000 | Loss: 0.00013969
Iteration 30/1000 | Loss: 0.00002313
Iteration 31/1000 | Loss: 0.00011087
Iteration 32/1000 | Loss: 0.00002944
Iteration 33/1000 | Loss: 0.00003129
Iteration 34/1000 | Loss: 0.00001708
Iteration 35/1000 | Loss: 0.00006754
Iteration 36/1000 | Loss: 0.00013213
Iteration 37/1000 | Loss: 0.00001743
Iteration 38/1000 | Loss: 0.00001619
Iteration 39/1000 | Loss: 0.00001575
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001540
Iteration 42/1000 | Loss: 0.00021550
Iteration 43/1000 | Loss: 0.00002548
Iteration 44/1000 | Loss: 0.00005596
Iteration 45/1000 | Loss: 0.00001514
Iteration 46/1000 | Loss: 0.00001496
Iteration 47/1000 | Loss: 0.00001494
Iteration 48/1000 | Loss: 0.00001493
Iteration 49/1000 | Loss: 0.00001493
Iteration 50/1000 | Loss: 0.00001489
Iteration 51/1000 | Loss: 0.00001486
Iteration 52/1000 | Loss: 0.00001485
Iteration 53/1000 | Loss: 0.00001485
Iteration 54/1000 | Loss: 0.00001478
Iteration 55/1000 | Loss: 0.00001478
Iteration 56/1000 | Loss: 0.00001477
Iteration 57/1000 | Loss: 0.00001476
Iteration 58/1000 | Loss: 0.00001475
Iteration 59/1000 | Loss: 0.00001474
Iteration 60/1000 | Loss: 0.00001474
Iteration 61/1000 | Loss: 0.00001473
Iteration 62/1000 | Loss: 0.00001473
Iteration 63/1000 | Loss: 0.00001473
Iteration 64/1000 | Loss: 0.00001472
Iteration 65/1000 | Loss: 0.00001472
Iteration 66/1000 | Loss: 0.00001471
Iteration 67/1000 | Loss: 0.00001471
Iteration 68/1000 | Loss: 0.00001470
Iteration 69/1000 | Loss: 0.00001468
Iteration 70/1000 | Loss: 0.00001468
Iteration 71/1000 | Loss: 0.00001467
Iteration 72/1000 | Loss: 0.00001467
Iteration 73/1000 | Loss: 0.00001467
Iteration 74/1000 | Loss: 0.00001467
Iteration 75/1000 | Loss: 0.00001467
Iteration 76/1000 | Loss: 0.00001467
Iteration 77/1000 | Loss: 0.00001467
Iteration 78/1000 | Loss: 0.00001466
Iteration 79/1000 | Loss: 0.00001466
Iteration 80/1000 | Loss: 0.00001466
Iteration 81/1000 | Loss: 0.00001466
Iteration 82/1000 | Loss: 0.00001465
Iteration 83/1000 | Loss: 0.00001465
Iteration 84/1000 | Loss: 0.00001464
Iteration 85/1000 | Loss: 0.00001464
Iteration 86/1000 | Loss: 0.00001463
Iteration 87/1000 | Loss: 0.00001463
Iteration 88/1000 | Loss: 0.00001463
Iteration 89/1000 | Loss: 0.00001463
Iteration 90/1000 | Loss: 0.00001462
Iteration 91/1000 | Loss: 0.00001461
Iteration 92/1000 | Loss: 0.00001459
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001457
Iteration 96/1000 | Loss: 0.00001457
Iteration 97/1000 | Loss: 0.00001457
Iteration 98/1000 | Loss: 0.00001457
Iteration 99/1000 | Loss: 0.00001457
Iteration 100/1000 | Loss: 0.00001457
Iteration 101/1000 | Loss: 0.00001456
Iteration 102/1000 | Loss: 0.00001456
Iteration 103/1000 | Loss: 0.00001455
Iteration 104/1000 | Loss: 0.00001455
Iteration 105/1000 | Loss: 0.00001455
Iteration 106/1000 | Loss: 0.00001455
Iteration 107/1000 | Loss: 0.00001455
Iteration 108/1000 | Loss: 0.00001454
Iteration 109/1000 | Loss: 0.00001454
Iteration 110/1000 | Loss: 0.00001454
Iteration 111/1000 | Loss: 0.00001454
Iteration 112/1000 | Loss: 0.00001454
Iteration 113/1000 | Loss: 0.00001454
Iteration 114/1000 | Loss: 0.00001454
Iteration 115/1000 | Loss: 0.00001454
Iteration 116/1000 | Loss: 0.00001454
Iteration 117/1000 | Loss: 0.00001454
Iteration 118/1000 | Loss: 0.00001454
Iteration 119/1000 | Loss: 0.00001454
Iteration 120/1000 | Loss: 0.00001454
Iteration 121/1000 | Loss: 0.00001453
Iteration 122/1000 | Loss: 0.00001453
Iteration 123/1000 | Loss: 0.00001453
Iteration 124/1000 | Loss: 0.00001453
Iteration 125/1000 | Loss: 0.00001453
Iteration 126/1000 | Loss: 0.00001453
Iteration 127/1000 | Loss: 0.00001453
Iteration 128/1000 | Loss: 0.00001453
Iteration 129/1000 | Loss: 0.00001452
Iteration 130/1000 | Loss: 0.00001452
Iteration 131/1000 | Loss: 0.00001452
Iteration 132/1000 | Loss: 0.00001452
Iteration 133/1000 | Loss: 0.00001452
Iteration 134/1000 | Loss: 0.00001452
Iteration 135/1000 | Loss: 0.00001452
Iteration 136/1000 | Loss: 0.00001452
Iteration 137/1000 | Loss: 0.00001451
Iteration 138/1000 | Loss: 0.00001451
Iteration 139/1000 | Loss: 0.00001451
Iteration 140/1000 | Loss: 0.00001451
Iteration 141/1000 | Loss: 0.00001451
Iteration 142/1000 | Loss: 0.00001451
Iteration 143/1000 | Loss: 0.00001451
Iteration 144/1000 | Loss: 0.00001451
Iteration 145/1000 | Loss: 0.00001451
Iteration 146/1000 | Loss: 0.00001451
Iteration 147/1000 | Loss: 0.00001451
Iteration 148/1000 | Loss: 0.00001451
Iteration 149/1000 | Loss: 0.00001451
Iteration 150/1000 | Loss: 0.00001451
Iteration 151/1000 | Loss: 0.00001450
Iteration 152/1000 | Loss: 0.00001450
Iteration 153/1000 | Loss: 0.00001450
Iteration 154/1000 | Loss: 0.00001450
Iteration 155/1000 | Loss: 0.00001450
Iteration 156/1000 | Loss: 0.00001450
Iteration 157/1000 | Loss: 0.00001450
Iteration 158/1000 | Loss: 0.00001450
Iteration 159/1000 | Loss: 0.00001450
Iteration 160/1000 | Loss: 0.00001450
Iteration 161/1000 | Loss: 0.00001449
Iteration 162/1000 | Loss: 0.00001449
Iteration 163/1000 | Loss: 0.00001449
Iteration 164/1000 | Loss: 0.00001449
Iteration 165/1000 | Loss: 0.00001449
Iteration 166/1000 | Loss: 0.00001449
Iteration 167/1000 | Loss: 0.00001449
Iteration 168/1000 | Loss: 0.00001448
Iteration 169/1000 | Loss: 0.00001448
Iteration 170/1000 | Loss: 0.00001448
Iteration 171/1000 | Loss: 0.00001448
Iteration 172/1000 | Loss: 0.00001448
Iteration 173/1000 | Loss: 0.00001448
Iteration 174/1000 | Loss: 0.00001448
Iteration 175/1000 | Loss: 0.00001448
Iteration 176/1000 | Loss: 0.00001448
Iteration 177/1000 | Loss: 0.00001448
Iteration 178/1000 | Loss: 0.00001448
Iteration 179/1000 | Loss: 0.00001448
Iteration 180/1000 | Loss: 0.00001448
Iteration 181/1000 | Loss: 0.00001448
Iteration 182/1000 | Loss: 0.00001448
Iteration 183/1000 | Loss: 0.00001448
Iteration 184/1000 | Loss: 0.00001448
Iteration 185/1000 | Loss: 0.00001447
Iteration 186/1000 | Loss: 0.00001447
Iteration 187/1000 | Loss: 0.00001447
Iteration 188/1000 | Loss: 0.00001447
Iteration 189/1000 | Loss: 0.00001447
Iteration 190/1000 | Loss: 0.00001447
Iteration 191/1000 | Loss: 0.00001447
Iteration 192/1000 | Loss: 0.00001447
Iteration 193/1000 | Loss: 0.00001447
Iteration 194/1000 | Loss: 0.00001447
Iteration 195/1000 | Loss: 0.00001447
Iteration 196/1000 | Loss: 0.00001447
Iteration 197/1000 | Loss: 0.00001447
Iteration 198/1000 | Loss: 0.00001447
Iteration 199/1000 | Loss: 0.00001447
Iteration 200/1000 | Loss: 0.00001447
Iteration 201/1000 | Loss: 0.00001446
Iteration 202/1000 | Loss: 0.00001446
Iteration 203/1000 | Loss: 0.00001446
Iteration 204/1000 | Loss: 0.00001446
Iteration 205/1000 | Loss: 0.00001446
Iteration 206/1000 | Loss: 0.00001446
Iteration 207/1000 | Loss: 0.00001446
Iteration 208/1000 | Loss: 0.00001446
Iteration 209/1000 | Loss: 0.00001446
Iteration 210/1000 | Loss: 0.00001446
Iteration 211/1000 | Loss: 0.00001446
Iteration 212/1000 | Loss: 0.00001446
Iteration 213/1000 | Loss: 0.00001446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.4463579645962454e-05, 1.4463579645962454e-05, 1.4463579645962454e-05, 1.4463579645962454e-05, 1.4463579645962454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4463579645962454e-05

Optimization complete. Final v2v error: 3.2187819480895996 mm

Highest mean error: 4.502595901489258 mm for frame 151

Lowest mean error: 2.7135467529296875 mm for frame 81

Saving results

Total time: 107.16129064559937
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413841
Iteration 2/25 | Loss: 0.00123432
Iteration 3/25 | Loss: 0.00114862
Iteration 4/25 | Loss: 0.00113292
Iteration 5/25 | Loss: 0.00112788
Iteration 6/25 | Loss: 0.00112718
Iteration 7/25 | Loss: 0.00112718
Iteration 8/25 | Loss: 0.00112718
Iteration 9/25 | Loss: 0.00112718
Iteration 10/25 | Loss: 0.00112718
Iteration 11/25 | Loss: 0.00112718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011271791299805045, 0.0011271791299805045, 0.0011271791299805045, 0.0011271791299805045, 0.0011271791299805045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011271791299805045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.49877930
Iteration 2/25 | Loss: 0.00077694
Iteration 3/25 | Loss: 0.00077694
Iteration 4/25 | Loss: 0.00077693
Iteration 5/25 | Loss: 0.00077693
Iteration 6/25 | Loss: 0.00077693
Iteration 7/25 | Loss: 0.00077693
Iteration 8/25 | Loss: 0.00077693
Iteration 9/25 | Loss: 0.00077693
Iteration 10/25 | Loss: 0.00077693
Iteration 11/25 | Loss: 0.00077693
Iteration 12/25 | Loss: 0.00077693
Iteration 13/25 | Loss: 0.00077693
Iteration 14/25 | Loss: 0.00077693
Iteration 15/25 | Loss: 0.00077693
Iteration 16/25 | Loss: 0.00077693
Iteration 17/25 | Loss: 0.00077693
Iteration 18/25 | Loss: 0.00077693
Iteration 19/25 | Loss: 0.00077693
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007769322255626321, 0.0007769322255626321, 0.0007769322255626321, 0.0007769322255626321, 0.0007769322255626321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007769322255626321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077693
Iteration 2/1000 | Loss: 0.00002980
Iteration 3/1000 | Loss: 0.00001740
Iteration 4/1000 | Loss: 0.00001552
Iteration 5/1000 | Loss: 0.00001443
Iteration 6/1000 | Loss: 0.00001382
Iteration 7/1000 | Loss: 0.00001342
Iteration 8/1000 | Loss: 0.00001315
Iteration 9/1000 | Loss: 0.00001285
Iteration 10/1000 | Loss: 0.00001256
Iteration 11/1000 | Loss: 0.00001236
Iteration 12/1000 | Loss: 0.00001220
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001203
Iteration 15/1000 | Loss: 0.00001199
Iteration 16/1000 | Loss: 0.00001195
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001193
Iteration 19/1000 | Loss: 0.00001193
Iteration 20/1000 | Loss: 0.00001192
Iteration 21/1000 | Loss: 0.00001192
Iteration 22/1000 | Loss: 0.00001186
Iteration 23/1000 | Loss: 0.00001186
Iteration 24/1000 | Loss: 0.00001186
Iteration 25/1000 | Loss: 0.00001186
Iteration 26/1000 | Loss: 0.00001186
Iteration 27/1000 | Loss: 0.00001186
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001186
Iteration 30/1000 | Loss: 0.00001185
Iteration 31/1000 | Loss: 0.00001185
Iteration 32/1000 | Loss: 0.00001185
Iteration 33/1000 | Loss: 0.00001185
Iteration 34/1000 | Loss: 0.00001182
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001182
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001181
Iteration 39/1000 | Loss: 0.00001181
Iteration 40/1000 | Loss: 0.00001179
Iteration 41/1000 | Loss: 0.00001179
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001175
Iteration 45/1000 | Loss: 0.00001175
Iteration 46/1000 | Loss: 0.00001175
Iteration 47/1000 | Loss: 0.00001175
Iteration 48/1000 | Loss: 0.00001175
Iteration 49/1000 | Loss: 0.00001175
Iteration 50/1000 | Loss: 0.00001175
Iteration 51/1000 | Loss: 0.00001175
Iteration 52/1000 | Loss: 0.00001175
Iteration 53/1000 | Loss: 0.00001175
Iteration 54/1000 | Loss: 0.00001175
Iteration 55/1000 | Loss: 0.00001175
Iteration 56/1000 | Loss: 0.00001175
Iteration 57/1000 | Loss: 0.00001175
Iteration 58/1000 | Loss: 0.00001175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [1.1754311344702728e-05, 1.1754311344702728e-05, 1.1754311344702728e-05, 1.1754311344702728e-05, 1.1754311344702728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1754311344702728e-05

Optimization complete. Final v2v error: 2.9567151069641113 mm

Highest mean error: 3.3243303298950195 mm for frame 90

Lowest mean error: 2.7868027687072754 mm for frame 73

Saving results

Total time: 32.12002372741699
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434212
Iteration 2/25 | Loss: 0.00125065
Iteration 3/25 | Loss: 0.00114228
Iteration 4/25 | Loss: 0.00113187
Iteration 5/25 | Loss: 0.00112910
Iteration 6/25 | Loss: 0.00112849
Iteration 7/25 | Loss: 0.00112849
Iteration 8/25 | Loss: 0.00112849
Iteration 9/25 | Loss: 0.00112849
Iteration 10/25 | Loss: 0.00112849
Iteration 11/25 | Loss: 0.00112849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011284861247986555, 0.0011284861247986555, 0.0011284861247986555, 0.0011284861247986555, 0.0011284861247986555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011284861247986555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34129941
Iteration 2/25 | Loss: 0.00084525
Iteration 3/25 | Loss: 0.00084523
Iteration 4/25 | Loss: 0.00084523
Iteration 5/25 | Loss: 0.00084523
Iteration 6/25 | Loss: 0.00084523
Iteration 7/25 | Loss: 0.00084523
Iteration 8/25 | Loss: 0.00084523
Iteration 9/25 | Loss: 0.00084523
Iteration 10/25 | Loss: 0.00084523
Iteration 11/25 | Loss: 0.00084523
Iteration 12/25 | Loss: 0.00084523
Iteration 13/25 | Loss: 0.00084523
Iteration 14/25 | Loss: 0.00084523
Iteration 15/25 | Loss: 0.00084523
Iteration 16/25 | Loss: 0.00084523
Iteration 17/25 | Loss: 0.00084523
Iteration 18/25 | Loss: 0.00084523
Iteration 19/25 | Loss: 0.00084523
Iteration 20/25 | Loss: 0.00084523
Iteration 21/25 | Loss: 0.00084523
Iteration 22/25 | Loss: 0.00084523
Iteration 23/25 | Loss: 0.00084523
Iteration 24/25 | Loss: 0.00084523
Iteration 25/25 | Loss: 0.00084523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084523
Iteration 2/1000 | Loss: 0.00002814
Iteration 3/1000 | Loss: 0.00001594
Iteration 4/1000 | Loss: 0.00001385
Iteration 5/1000 | Loss: 0.00001292
Iteration 6/1000 | Loss: 0.00001234
Iteration 7/1000 | Loss: 0.00001193
Iteration 8/1000 | Loss: 0.00001173
Iteration 9/1000 | Loss: 0.00001164
Iteration 10/1000 | Loss: 0.00001164
Iteration 11/1000 | Loss: 0.00001163
Iteration 12/1000 | Loss: 0.00001151
Iteration 13/1000 | Loss: 0.00001147
Iteration 14/1000 | Loss: 0.00001142
Iteration 15/1000 | Loss: 0.00001138
Iteration 16/1000 | Loss: 0.00001137
Iteration 17/1000 | Loss: 0.00001137
Iteration 18/1000 | Loss: 0.00001134
Iteration 19/1000 | Loss: 0.00001134
Iteration 20/1000 | Loss: 0.00001133
Iteration 21/1000 | Loss: 0.00001133
Iteration 22/1000 | Loss: 0.00001131
Iteration 23/1000 | Loss: 0.00001128
Iteration 24/1000 | Loss: 0.00001127
Iteration 25/1000 | Loss: 0.00001127
Iteration 26/1000 | Loss: 0.00001126
Iteration 27/1000 | Loss: 0.00001126
Iteration 28/1000 | Loss: 0.00001126
Iteration 29/1000 | Loss: 0.00001126
Iteration 30/1000 | Loss: 0.00001125
Iteration 31/1000 | Loss: 0.00001124
Iteration 32/1000 | Loss: 0.00001123
Iteration 33/1000 | Loss: 0.00001122
Iteration 34/1000 | Loss: 0.00001121
Iteration 35/1000 | Loss: 0.00001121
Iteration 36/1000 | Loss: 0.00001121
Iteration 37/1000 | Loss: 0.00001120
Iteration 38/1000 | Loss: 0.00001120
Iteration 39/1000 | Loss: 0.00001120
Iteration 40/1000 | Loss: 0.00001120
Iteration 41/1000 | Loss: 0.00001119
Iteration 42/1000 | Loss: 0.00001117
Iteration 43/1000 | Loss: 0.00001115
Iteration 44/1000 | Loss: 0.00001114
Iteration 45/1000 | Loss: 0.00001114
Iteration 46/1000 | Loss: 0.00001113
Iteration 47/1000 | Loss: 0.00001112
Iteration 48/1000 | Loss: 0.00001112
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001109
Iteration 52/1000 | Loss: 0.00001109
Iteration 53/1000 | Loss: 0.00001109
Iteration 54/1000 | Loss: 0.00001108
Iteration 55/1000 | Loss: 0.00001108
Iteration 56/1000 | Loss: 0.00001108
Iteration 57/1000 | Loss: 0.00001105
Iteration 58/1000 | Loss: 0.00001104
Iteration 59/1000 | Loss: 0.00001104
Iteration 60/1000 | Loss: 0.00001104
Iteration 61/1000 | Loss: 0.00001103
Iteration 62/1000 | Loss: 0.00001103
Iteration 63/1000 | Loss: 0.00001103
Iteration 64/1000 | Loss: 0.00001103
Iteration 65/1000 | Loss: 0.00001103
Iteration 66/1000 | Loss: 0.00001102
Iteration 67/1000 | Loss: 0.00001102
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001102
Iteration 70/1000 | Loss: 0.00001101
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001100
Iteration 75/1000 | Loss: 0.00001100
Iteration 76/1000 | Loss: 0.00001100
Iteration 77/1000 | Loss: 0.00001100
Iteration 78/1000 | Loss: 0.00001100
Iteration 79/1000 | Loss: 0.00001100
Iteration 80/1000 | Loss: 0.00001100
Iteration 81/1000 | Loss: 0.00001100
Iteration 82/1000 | Loss: 0.00001099
Iteration 83/1000 | Loss: 0.00001099
Iteration 84/1000 | Loss: 0.00001099
Iteration 85/1000 | Loss: 0.00001098
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001097
Iteration 88/1000 | Loss: 0.00001097
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001096
Iteration 91/1000 | Loss: 0.00001096
Iteration 92/1000 | Loss: 0.00001096
Iteration 93/1000 | Loss: 0.00001096
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001095
Iteration 97/1000 | Loss: 0.00001095
Iteration 98/1000 | Loss: 0.00001094
Iteration 99/1000 | Loss: 0.00001094
Iteration 100/1000 | Loss: 0.00001094
Iteration 101/1000 | Loss: 0.00001094
Iteration 102/1000 | Loss: 0.00001094
Iteration 103/1000 | Loss: 0.00001094
Iteration 104/1000 | Loss: 0.00001094
Iteration 105/1000 | Loss: 0.00001093
Iteration 106/1000 | Loss: 0.00001093
Iteration 107/1000 | Loss: 0.00001093
Iteration 108/1000 | Loss: 0.00001092
Iteration 109/1000 | Loss: 0.00001092
Iteration 110/1000 | Loss: 0.00001091
Iteration 111/1000 | Loss: 0.00001091
Iteration 112/1000 | Loss: 0.00001091
Iteration 113/1000 | Loss: 0.00001091
Iteration 114/1000 | Loss: 0.00001090
Iteration 115/1000 | Loss: 0.00001090
Iteration 116/1000 | Loss: 0.00001089
Iteration 117/1000 | Loss: 0.00001089
Iteration 118/1000 | Loss: 0.00001089
Iteration 119/1000 | Loss: 0.00001088
Iteration 120/1000 | Loss: 0.00001088
Iteration 121/1000 | Loss: 0.00001088
Iteration 122/1000 | Loss: 0.00001088
Iteration 123/1000 | Loss: 0.00001088
Iteration 124/1000 | Loss: 0.00001088
Iteration 125/1000 | Loss: 0.00001088
Iteration 126/1000 | Loss: 0.00001088
Iteration 127/1000 | Loss: 0.00001088
Iteration 128/1000 | Loss: 0.00001087
Iteration 129/1000 | Loss: 0.00001087
Iteration 130/1000 | Loss: 0.00001087
Iteration 131/1000 | Loss: 0.00001086
Iteration 132/1000 | Loss: 0.00001086
Iteration 133/1000 | Loss: 0.00001086
Iteration 134/1000 | Loss: 0.00001086
Iteration 135/1000 | Loss: 0.00001086
Iteration 136/1000 | Loss: 0.00001086
Iteration 137/1000 | Loss: 0.00001086
Iteration 138/1000 | Loss: 0.00001085
Iteration 139/1000 | Loss: 0.00001085
Iteration 140/1000 | Loss: 0.00001085
Iteration 141/1000 | Loss: 0.00001085
Iteration 142/1000 | Loss: 0.00001085
Iteration 143/1000 | Loss: 0.00001085
Iteration 144/1000 | Loss: 0.00001084
Iteration 145/1000 | Loss: 0.00001084
Iteration 146/1000 | Loss: 0.00001084
Iteration 147/1000 | Loss: 0.00001083
Iteration 148/1000 | Loss: 0.00001083
Iteration 149/1000 | Loss: 0.00001082
Iteration 150/1000 | Loss: 0.00001082
Iteration 151/1000 | Loss: 0.00001082
Iteration 152/1000 | Loss: 0.00001082
Iteration 153/1000 | Loss: 0.00001082
Iteration 154/1000 | Loss: 0.00001082
Iteration 155/1000 | Loss: 0.00001082
Iteration 156/1000 | Loss: 0.00001082
Iteration 157/1000 | Loss: 0.00001082
Iteration 158/1000 | Loss: 0.00001082
Iteration 159/1000 | Loss: 0.00001082
Iteration 160/1000 | Loss: 0.00001081
Iteration 161/1000 | Loss: 0.00001081
Iteration 162/1000 | Loss: 0.00001080
Iteration 163/1000 | Loss: 0.00001079
Iteration 164/1000 | Loss: 0.00001079
Iteration 165/1000 | Loss: 0.00001079
Iteration 166/1000 | Loss: 0.00001078
Iteration 167/1000 | Loss: 0.00001078
Iteration 168/1000 | Loss: 0.00001078
Iteration 169/1000 | Loss: 0.00001078
Iteration 170/1000 | Loss: 0.00001078
Iteration 171/1000 | Loss: 0.00001078
Iteration 172/1000 | Loss: 0.00001078
Iteration 173/1000 | Loss: 0.00001077
Iteration 174/1000 | Loss: 0.00001077
Iteration 175/1000 | Loss: 0.00001077
Iteration 176/1000 | Loss: 0.00001077
Iteration 177/1000 | Loss: 0.00001076
Iteration 178/1000 | Loss: 0.00001076
Iteration 179/1000 | Loss: 0.00001076
Iteration 180/1000 | Loss: 0.00001076
Iteration 181/1000 | Loss: 0.00001076
Iteration 182/1000 | Loss: 0.00001075
Iteration 183/1000 | Loss: 0.00001075
Iteration 184/1000 | Loss: 0.00001075
Iteration 185/1000 | Loss: 0.00001075
Iteration 186/1000 | Loss: 0.00001075
Iteration 187/1000 | Loss: 0.00001075
Iteration 188/1000 | Loss: 0.00001074
Iteration 189/1000 | Loss: 0.00001074
Iteration 190/1000 | Loss: 0.00001074
Iteration 191/1000 | Loss: 0.00001074
Iteration 192/1000 | Loss: 0.00001074
Iteration 193/1000 | Loss: 0.00001074
Iteration 194/1000 | Loss: 0.00001074
Iteration 195/1000 | Loss: 0.00001074
Iteration 196/1000 | Loss: 0.00001074
Iteration 197/1000 | Loss: 0.00001074
Iteration 198/1000 | Loss: 0.00001074
Iteration 199/1000 | Loss: 0.00001074
Iteration 200/1000 | Loss: 0.00001074
Iteration 201/1000 | Loss: 0.00001073
Iteration 202/1000 | Loss: 0.00001073
Iteration 203/1000 | Loss: 0.00001073
Iteration 204/1000 | Loss: 0.00001073
Iteration 205/1000 | Loss: 0.00001073
Iteration 206/1000 | Loss: 0.00001073
Iteration 207/1000 | Loss: 0.00001073
Iteration 208/1000 | Loss: 0.00001073
Iteration 209/1000 | Loss: 0.00001073
Iteration 210/1000 | Loss: 0.00001073
Iteration 211/1000 | Loss: 0.00001073
Iteration 212/1000 | Loss: 0.00001073
Iteration 213/1000 | Loss: 0.00001073
Iteration 214/1000 | Loss: 0.00001073
Iteration 215/1000 | Loss: 0.00001073
Iteration 216/1000 | Loss: 0.00001073
Iteration 217/1000 | Loss: 0.00001073
Iteration 218/1000 | Loss: 0.00001073
Iteration 219/1000 | Loss: 0.00001073
Iteration 220/1000 | Loss: 0.00001073
Iteration 221/1000 | Loss: 0.00001073
Iteration 222/1000 | Loss: 0.00001073
Iteration 223/1000 | Loss: 0.00001073
Iteration 224/1000 | Loss: 0.00001073
Iteration 225/1000 | Loss: 0.00001073
Iteration 226/1000 | Loss: 0.00001073
Iteration 227/1000 | Loss: 0.00001073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.0731409020081628e-05, 1.0731409020081628e-05, 1.0731409020081628e-05, 1.0731409020081628e-05, 1.0731409020081628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0731409020081628e-05

Optimization complete. Final v2v error: 2.785409688949585 mm

Highest mean error: 3.535971164703369 mm for frame 62

Lowest mean error: 2.349438190460205 mm for frame 29

Saving results

Total time: 39.86251425743103
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819749
Iteration 2/25 | Loss: 0.00124950
Iteration 3/25 | Loss: 0.00113304
Iteration 4/25 | Loss: 0.00112089
Iteration 5/25 | Loss: 0.00111877
Iteration 6/25 | Loss: 0.00111839
Iteration 7/25 | Loss: 0.00111839
Iteration 8/25 | Loss: 0.00111839
Iteration 9/25 | Loss: 0.00111839
Iteration 10/25 | Loss: 0.00111839
Iteration 11/25 | Loss: 0.00111839
Iteration 12/25 | Loss: 0.00111839
Iteration 13/25 | Loss: 0.00111839
Iteration 14/25 | Loss: 0.00111839
Iteration 15/25 | Loss: 0.00111839
Iteration 16/25 | Loss: 0.00111839
Iteration 17/25 | Loss: 0.00111839
Iteration 18/25 | Loss: 0.00111839
Iteration 19/25 | Loss: 0.00111839
Iteration 20/25 | Loss: 0.00111839
Iteration 21/25 | Loss: 0.00111839
Iteration 22/25 | Loss: 0.00111839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001118386979214847, 0.001118386979214847, 0.001118386979214847, 0.001118386979214847, 0.001118386979214847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001118386979214847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35848761
Iteration 2/25 | Loss: 0.00071402
Iteration 3/25 | Loss: 0.00071400
Iteration 4/25 | Loss: 0.00071400
Iteration 5/25 | Loss: 0.00071400
Iteration 6/25 | Loss: 0.00071400
Iteration 7/25 | Loss: 0.00071400
Iteration 8/25 | Loss: 0.00071400
Iteration 9/25 | Loss: 0.00071400
Iteration 10/25 | Loss: 0.00071400
Iteration 11/25 | Loss: 0.00071400
Iteration 12/25 | Loss: 0.00071400
Iteration 13/25 | Loss: 0.00071400
Iteration 14/25 | Loss: 0.00071400
Iteration 15/25 | Loss: 0.00071400
Iteration 16/25 | Loss: 0.00071400
Iteration 17/25 | Loss: 0.00071400
Iteration 18/25 | Loss: 0.00071400
Iteration 19/25 | Loss: 0.00071400
Iteration 20/25 | Loss: 0.00071400
Iteration 21/25 | Loss: 0.00071400
Iteration 22/25 | Loss: 0.00071400
Iteration 23/25 | Loss: 0.00071400
Iteration 24/25 | Loss: 0.00071400
Iteration 25/25 | Loss: 0.00071400

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071400
Iteration 2/1000 | Loss: 0.00001981
Iteration 3/1000 | Loss: 0.00001322
Iteration 4/1000 | Loss: 0.00001150
Iteration 5/1000 | Loss: 0.00001053
Iteration 6/1000 | Loss: 0.00000987
Iteration 7/1000 | Loss: 0.00000942
Iteration 8/1000 | Loss: 0.00000920
Iteration 9/1000 | Loss: 0.00000916
Iteration 10/1000 | Loss: 0.00000914
Iteration 11/1000 | Loss: 0.00000913
Iteration 12/1000 | Loss: 0.00000912
Iteration 13/1000 | Loss: 0.00000910
Iteration 14/1000 | Loss: 0.00000900
Iteration 15/1000 | Loss: 0.00000892
Iteration 16/1000 | Loss: 0.00000886
Iteration 17/1000 | Loss: 0.00000884
Iteration 18/1000 | Loss: 0.00000883
Iteration 19/1000 | Loss: 0.00000880
Iteration 20/1000 | Loss: 0.00000880
Iteration 21/1000 | Loss: 0.00000880
Iteration 22/1000 | Loss: 0.00000879
Iteration 23/1000 | Loss: 0.00000879
Iteration 24/1000 | Loss: 0.00000879
Iteration 25/1000 | Loss: 0.00000878
Iteration 26/1000 | Loss: 0.00000877
Iteration 27/1000 | Loss: 0.00000877
Iteration 28/1000 | Loss: 0.00000876
Iteration 29/1000 | Loss: 0.00000876
Iteration 30/1000 | Loss: 0.00000875
Iteration 31/1000 | Loss: 0.00000870
Iteration 32/1000 | Loss: 0.00000870
Iteration 33/1000 | Loss: 0.00000869
Iteration 34/1000 | Loss: 0.00000868
Iteration 35/1000 | Loss: 0.00000867
Iteration 36/1000 | Loss: 0.00000867
Iteration 37/1000 | Loss: 0.00000867
Iteration 38/1000 | Loss: 0.00000865
Iteration 39/1000 | Loss: 0.00000865
Iteration 40/1000 | Loss: 0.00000864
Iteration 41/1000 | Loss: 0.00000863
Iteration 42/1000 | Loss: 0.00000862
Iteration 43/1000 | Loss: 0.00000860
Iteration 44/1000 | Loss: 0.00000859
Iteration 45/1000 | Loss: 0.00000858
Iteration 46/1000 | Loss: 0.00000858
Iteration 47/1000 | Loss: 0.00000858
Iteration 48/1000 | Loss: 0.00000857
Iteration 49/1000 | Loss: 0.00000857
Iteration 50/1000 | Loss: 0.00000856
Iteration 51/1000 | Loss: 0.00000856
Iteration 52/1000 | Loss: 0.00000855
Iteration 53/1000 | Loss: 0.00000855
Iteration 54/1000 | Loss: 0.00000855
Iteration 55/1000 | Loss: 0.00000855
Iteration 56/1000 | Loss: 0.00000854
Iteration 57/1000 | Loss: 0.00000854
Iteration 58/1000 | Loss: 0.00000853
Iteration 59/1000 | Loss: 0.00000853
Iteration 60/1000 | Loss: 0.00000853
Iteration 61/1000 | Loss: 0.00000853
Iteration 62/1000 | Loss: 0.00000852
Iteration 63/1000 | Loss: 0.00000852
Iteration 64/1000 | Loss: 0.00000852
Iteration 65/1000 | Loss: 0.00000852
Iteration 66/1000 | Loss: 0.00000852
Iteration 67/1000 | Loss: 0.00000852
Iteration 68/1000 | Loss: 0.00000852
Iteration 69/1000 | Loss: 0.00000851
Iteration 70/1000 | Loss: 0.00000851
Iteration 71/1000 | Loss: 0.00000851
Iteration 72/1000 | Loss: 0.00000850
Iteration 73/1000 | Loss: 0.00000850
Iteration 74/1000 | Loss: 0.00000850
Iteration 75/1000 | Loss: 0.00000850
Iteration 76/1000 | Loss: 0.00000850
Iteration 77/1000 | Loss: 0.00000850
Iteration 78/1000 | Loss: 0.00000850
Iteration 79/1000 | Loss: 0.00000849
Iteration 80/1000 | Loss: 0.00000849
Iteration 81/1000 | Loss: 0.00000849
Iteration 82/1000 | Loss: 0.00000849
Iteration 83/1000 | Loss: 0.00000849
Iteration 84/1000 | Loss: 0.00000849
Iteration 85/1000 | Loss: 0.00000849
Iteration 86/1000 | Loss: 0.00000849
Iteration 87/1000 | Loss: 0.00000849
Iteration 88/1000 | Loss: 0.00000849
Iteration 89/1000 | Loss: 0.00000849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [8.494087524013594e-06, 8.494087524013594e-06, 8.494087524013594e-06, 8.494087524013594e-06, 8.494087524013594e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.494087524013594e-06

Optimization complete. Final v2v error: 2.51943039894104 mm

Highest mean error: 2.8997390270233154 mm for frame 1

Lowest mean error: 2.3925681114196777 mm for frame 93

Saving results

Total time: 30.089837312698364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475460
Iteration 2/25 | Loss: 0.00137657
Iteration 3/25 | Loss: 0.00121932
Iteration 4/25 | Loss: 0.00119141
Iteration 5/25 | Loss: 0.00118315
Iteration 6/25 | Loss: 0.00118100
Iteration 7/25 | Loss: 0.00118098
Iteration 8/25 | Loss: 0.00118098
Iteration 9/25 | Loss: 0.00118098
Iteration 10/25 | Loss: 0.00118098
Iteration 11/25 | Loss: 0.00118098
Iteration 12/25 | Loss: 0.00118098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011809752322733402, 0.0011809752322733402, 0.0011809752322733402, 0.0011809752322733402, 0.0011809752322733402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011809752322733402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92212939
Iteration 2/25 | Loss: 0.00098012
Iteration 3/25 | Loss: 0.00098011
Iteration 4/25 | Loss: 0.00098011
Iteration 5/25 | Loss: 0.00098011
Iteration 6/25 | Loss: 0.00098011
Iteration 7/25 | Loss: 0.00098011
Iteration 8/25 | Loss: 0.00098011
Iteration 9/25 | Loss: 0.00098011
Iteration 10/25 | Loss: 0.00098011
Iteration 11/25 | Loss: 0.00098011
Iteration 12/25 | Loss: 0.00098011
Iteration 13/25 | Loss: 0.00098011
Iteration 14/25 | Loss: 0.00098011
Iteration 15/25 | Loss: 0.00098011
Iteration 16/25 | Loss: 0.00098011
Iteration 17/25 | Loss: 0.00098011
Iteration 18/25 | Loss: 0.00098011
Iteration 19/25 | Loss: 0.00098011
Iteration 20/25 | Loss: 0.00098011
Iteration 21/25 | Loss: 0.00098011
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009801109554246068, 0.0009801109554246068, 0.0009801109554246068, 0.0009801109554246068, 0.0009801109554246068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009801109554246068

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098011
Iteration 2/1000 | Loss: 0.00004402
Iteration 3/1000 | Loss: 0.00002712
Iteration 4/1000 | Loss: 0.00002283
Iteration 5/1000 | Loss: 0.00002125
Iteration 6/1000 | Loss: 0.00002002
Iteration 7/1000 | Loss: 0.00001934
Iteration 8/1000 | Loss: 0.00001876
Iteration 9/1000 | Loss: 0.00001843
Iteration 10/1000 | Loss: 0.00001816
Iteration 11/1000 | Loss: 0.00001789
Iteration 12/1000 | Loss: 0.00001768
Iteration 13/1000 | Loss: 0.00001759
Iteration 14/1000 | Loss: 0.00001754
Iteration 15/1000 | Loss: 0.00001746
Iteration 16/1000 | Loss: 0.00001743
Iteration 17/1000 | Loss: 0.00001743
Iteration 18/1000 | Loss: 0.00001737
Iteration 19/1000 | Loss: 0.00001736
Iteration 20/1000 | Loss: 0.00001735
Iteration 21/1000 | Loss: 0.00001731
Iteration 22/1000 | Loss: 0.00001731
Iteration 23/1000 | Loss: 0.00001729
Iteration 24/1000 | Loss: 0.00001729
Iteration 25/1000 | Loss: 0.00001728
Iteration 26/1000 | Loss: 0.00001728
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001727
Iteration 29/1000 | Loss: 0.00001726
Iteration 30/1000 | Loss: 0.00001726
Iteration 31/1000 | Loss: 0.00001725
Iteration 32/1000 | Loss: 0.00001725
Iteration 33/1000 | Loss: 0.00001724
Iteration 34/1000 | Loss: 0.00001724
Iteration 35/1000 | Loss: 0.00001723
Iteration 36/1000 | Loss: 0.00001723
Iteration 37/1000 | Loss: 0.00001721
Iteration 38/1000 | Loss: 0.00001721
Iteration 39/1000 | Loss: 0.00001720
Iteration 40/1000 | Loss: 0.00001720
Iteration 41/1000 | Loss: 0.00001719
Iteration 42/1000 | Loss: 0.00001718
Iteration 43/1000 | Loss: 0.00001718
Iteration 44/1000 | Loss: 0.00001717
Iteration 45/1000 | Loss: 0.00001716
Iteration 46/1000 | Loss: 0.00001716
Iteration 47/1000 | Loss: 0.00001714
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001710
Iteration 50/1000 | Loss: 0.00001710
Iteration 51/1000 | Loss: 0.00001710
Iteration 52/1000 | Loss: 0.00001709
Iteration 53/1000 | Loss: 0.00001709
Iteration 54/1000 | Loss: 0.00001708
Iteration 55/1000 | Loss: 0.00001708
Iteration 56/1000 | Loss: 0.00001708
Iteration 57/1000 | Loss: 0.00001708
Iteration 58/1000 | Loss: 0.00001707
Iteration 59/1000 | Loss: 0.00001707
Iteration 60/1000 | Loss: 0.00001707
Iteration 61/1000 | Loss: 0.00001707
Iteration 62/1000 | Loss: 0.00001707
Iteration 63/1000 | Loss: 0.00001706
Iteration 64/1000 | Loss: 0.00001706
Iteration 65/1000 | Loss: 0.00001706
Iteration 66/1000 | Loss: 0.00001706
Iteration 67/1000 | Loss: 0.00001706
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001705
Iteration 70/1000 | Loss: 0.00001705
Iteration 71/1000 | Loss: 0.00001704
Iteration 72/1000 | Loss: 0.00001704
Iteration 73/1000 | Loss: 0.00001704
Iteration 74/1000 | Loss: 0.00001704
Iteration 75/1000 | Loss: 0.00001704
Iteration 76/1000 | Loss: 0.00001704
Iteration 77/1000 | Loss: 0.00001703
Iteration 78/1000 | Loss: 0.00001703
Iteration 79/1000 | Loss: 0.00001703
Iteration 80/1000 | Loss: 0.00001703
Iteration 81/1000 | Loss: 0.00001702
Iteration 82/1000 | Loss: 0.00001702
Iteration 83/1000 | Loss: 0.00001702
Iteration 84/1000 | Loss: 0.00001702
Iteration 85/1000 | Loss: 0.00001702
Iteration 86/1000 | Loss: 0.00001702
Iteration 87/1000 | Loss: 0.00001702
Iteration 88/1000 | Loss: 0.00001701
Iteration 89/1000 | Loss: 0.00001701
Iteration 90/1000 | Loss: 0.00001701
Iteration 91/1000 | Loss: 0.00001701
Iteration 92/1000 | Loss: 0.00001701
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001700
Iteration 96/1000 | Loss: 0.00001700
Iteration 97/1000 | Loss: 0.00001700
Iteration 98/1000 | Loss: 0.00001700
Iteration 99/1000 | Loss: 0.00001699
Iteration 100/1000 | Loss: 0.00001699
Iteration 101/1000 | Loss: 0.00001699
Iteration 102/1000 | Loss: 0.00001699
Iteration 103/1000 | Loss: 0.00001699
Iteration 104/1000 | Loss: 0.00001699
Iteration 105/1000 | Loss: 0.00001699
Iteration 106/1000 | Loss: 0.00001699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.6989723008009605e-05, 1.6989723008009605e-05, 1.6989723008009605e-05, 1.6989723008009605e-05, 1.6989723008009605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6989723008009605e-05

Optimization complete. Final v2v error: 3.4237570762634277 mm

Highest mean error: 4.752931118011475 mm for frame 55

Lowest mean error: 2.769899845123291 mm for frame 67

Saving results

Total time: 43.357349157333374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828479
Iteration 2/25 | Loss: 0.00149449
Iteration 3/25 | Loss: 0.00120680
Iteration 4/25 | Loss: 0.00117038
Iteration 5/25 | Loss: 0.00116523
Iteration 6/25 | Loss: 0.00116444
Iteration 7/25 | Loss: 0.00116444
Iteration 8/25 | Loss: 0.00116444
Iteration 9/25 | Loss: 0.00116444
Iteration 10/25 | Loss: 0.00116444
Iteration 11/25 | Loss: 0.00116444
Iteration 12/25 | Loss: 0.00116444
Iteration 13/25 | Loss: 0.00116444
Iteration 14/25 | Loss: 0.00116444
Iteration 15/25 | Loss: 0.00116444
Iteration 16/25 | Loss: 0.00116444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011644352925941348, 0.0011644352925941348, 0.0011644352925941348, 0.0011644352925941348, 0.0011644352925941348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011644352925941348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35824084
Iteration 2/25 | Loss: 0.00069396
Iteration 3/25 | Loss: 0.00069394
Iteration 4/25 | Loss: 0.00069394
Iteration 5/25 | Loss: 0.00069394
Iteration 6/25 | Loss: 0.00069394
Iteration 7/25 | Loss: 0.00069394
Iteration 8/25 | Loss: 0.00069394
Iteration 9/25 | Loss: 0.00069394
Iteration 10/25 | Loss: 0.00069394
Iteration 11/25 | Loss: 0.00069394
Iteration 12/25 | Loss: 0.00069394
Iteration 13/25 | Loss: 0.00069394
Iteration 14/25 | Loss: 0.00069394
Iteration 15/25 | Loss: 0.00069394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006939386366866529, 0.0006939386366866529, 0.0006939386366866529, 0.0006939386366866529, 0.0006939386366866529]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006939386366866529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069394
Iteration 2/1000 | Loss: 0.00001970
Iteration 3/1000 | Loss: 0.00001528
Iteration 4/1000 | Loss: 0.00001398
Iteration 5/1000 | Loss: 0.00001316
Iteration 6/1000 | Loss: 0.00001281
Iteration 7/1000 | Loss: 0.00001252
Iteration 8/1000 | Loss: 0.00001247
Iteration 9/1000 | Loss: 0.00001240
Iteration 10/1000 | Loss: 0.00001215
Iteration 11/1000 | Loss: 0.00001204
Iteration 12/1000 | Loss: 0.00001189
Iteration 13/1000 | Loss: 0.00001188
Iteration 14/1000 | Loss: 0.00001187
Iteration 15/1000 | Loss: 0.00001186
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001185
Iteration 18/1000 | Loss: 0.00001185
Iteration 19/1000 | Loss: 0.00001182
Iteration 20/1000 | Loss: 0.00001180
Iteration 21/1000 | Loss: 0.00001180
Iteration 22/1000 | Loss: 0.00001179
Iteration 23/1000 | Loss: 0.00001179
Iteration 24/1000 | Loss: 0.00001179
Iteration 25/1000 | Loss: 0.00001179
Iteration 26/1000 | Loss: 0.00001178
Iteration 27/1000 | Loss: 0.00001178
Iteration 28/1000 | Loss: 0.00001178
Iteration 29/1000 | Loss: 0.00001178
Iteration 30/1000 | Loss: 0.00001177
Iteration 31/1000 | Loss: 0.00001176
Iteration 32/1000 | Loss: 0.00001176
Iteration 33/1000 | Loss: 0.00001176
Iteration 34/1000 | Loss: 0.00001172
Iteration 35/1000 | Loss: 0.00001172
Iteration 36/1000 | Loss: 0.00001172
Iteration 37/1000 | Loss: 0.00001172
Iteration 38/1000 | Loss: 0.00001171
Iteration 39/1000 | Loss: 0.00001171
Iteration 40/1000 | Loss: 0.00001171
Iteration 41/1000 | Loss: 0.00001171
Iteration 42/1000 | Loss: 0.00001169
Iteration 43/1000 | Loss: 0.00001169
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001167
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001166
Iteration 51/1000 | Loss: 0.00001166
Iteration 52/1000 | Loss: 0.00001166
Iteration 53/1000 | Loss: 0.00001164
Iteration 54/1000 | Loss: 0.00001164
Iteration 55/1000 | Loss: 0.00001163
Iteration 56/1000 | Loss: 0.00001163
Iteration 57/1000 | Loss: 0.00001163
Iteration 58/1000 | Loss: 0.00001162
Iteration 59/1000 | Loss: 0.00001162
Iteration 60/1000 | Loss: 0.00001161
Iteration 61/1000 | Loss: 0.00001161
Iteration 62/1000 | Loss: 0.00001160
Iteration 63/1000 | Loss: 0.00001160
Iteration 64/1000 | Loss: 0.00001160
Iteration 65/1000 | Loss: 0.00001160
Iteration 66/1000 | Loss: 0.00001159
Iteration 67/1000 | Loss: 0.00001159
Iteration 68/1000 | Loss: 0.00001159
Iteration 69/1000 | Loss: 0.00001158
Iteration 70/1000 | Loss: 0.00001158
Iteration 71/1000 | Loss: 0.00001158
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001158
Iteration 74/1000 | Loss: 0.00001158
Iteration 75/1000 | Loss: 0.00001158
Iteration 76/1000 | Loss: 0.00001158
Iteration 77/1000 | Loss: 0.00001158
Iteration 78/1000 | Loss: 0.00001158
Iteration 79/1000 | Loss: 0.00001157
Iteration 80/1000 | Loss: 0.00001157
Iteration 81/1000 | Loss: 0.00001156
Iteration 82/1000 | Loss: 0.00001156
Iteration 83/1000 | Loss: 0.00001156
Iteration 84/1000 | Loss: 0.00001156
Iteration 85/1000 | Loss: 0.00001156
Iteration 86/1000 | Loss: 0.00001155
Iteration 87/1000 | Loss: 0.00001155
Iteration 88/1000 | Loss: 0.00001154
Iteration 89/1000 | Loss: 0.00001154
Iteration 90/1000 | Loss: 0.00001154
Iteration 91/1000 | Loss: 0.00001154
Iteration 92/1000 | Loss: 0.00001153
Iteration 93/1000 | Loss: 0.00001153
Iteration 94/1000 | Loss: 0.00001153
Iteration 95/1000 | Loss: 0.00001152
Iteration 96/1000 | Loss: 0.00001152
Iteration 97/1000 | Loss: 0.00001152
Iteration 98/1000 | Loss: 0.00001152
Iteration 99/1000 | Loss: 0.00001151
Iteration 100/1000 | Loss: 0.00001151
Iteration 101/1000 | Loss: 0.00001150
Iteration 102/1000 | Loss: 0.00001150
Iteration 103/1000 | Loss: 0.00001150
Iteration 104/1000 | Loss: 0.00001150
Iteration 105/1000 | Loss: 0.00001150
Iteration 106/1000 | Loss: 0.00001150
Iteration 107/1000 | Loss: 0.00001150
Iteration 108/1000 | Loss: 0.00001150
Iteration 109/1000 | Loss: 0.00001150
Iteration 110/1000 | Loss: 0.00001150
Iteration 111/1000 | Loss: 0.00001150
Iteration 112/1000 | Loss: 0.00001150
Iteration 113/1000 | Loss: 0.00001149
Iteration 114/1000 | Loss: 0.00001149
Iteration 115/1000 | Loss: 0.00001149
Iteration 116/1000 | Loss: 0.00001149
Iteration 117/1000 | Loss: 0.00001149
Iteration 118/1000 | Loss: 0.00001149
Iteration 119/1000 | Loss: 0.00001149
Iteration 120/1000 | Loss: 0.00001148
Iteration 121/1000 | Loss: 0.00001148
Iteration 122/1000 | Loss: 0.00001148
Iteration 123/1000 | Loss: 0.00001148
Iteration 124/1000 | Loss: 0.00001148
Iteration 125/1000 | Loss: 0.00001147
Iteration 126/1000 | Loss: 0.00001147
Iteration 127/1000 | Loss: 0.00001147
Iteration 128/1000 | Loss: 0.00001147
Iteration 129/1000 | Loss: 0.00001147
Iteration 130/1000 | Loss: 0.00001147
Iteration 131/1000 | Loss: 0.00001147
Iteration 132/1000 | Loss: 0.00001147
Iteration 133/1000 | Loss: 0.00001147
Iteration 134/1000 | Loss: 0.00001147
Iteration 135/1000 | Loss: 0.00001147
Iteration 136/1000 | Loss: 0.00001146
Iteration 137/1000 | Loss: 0.00001146
Iteration 138/1000 | Loss: 0.00001146
Iteration 139/1000 | Loss: 0.00001146
Iteration 140/1000 | Loss: 0.00001146
Iteration 141/1000 | Loss: 0.00001146
Iteration 142/1000 | Loss: 0.00001146
Iteration 143/1000 | Loss: 0.00001146
Iteration 144/1000 | Loss: 0.00001146
Iteration 145/1000 | Loss: 0.00001146
Iteration 146/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.1463433111202903e-05, 1.1463433111202903e-05, 1.1463433111202903e-05, 1.1463433111202903e-05, 1.1463433111202903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1463433111202903e-05

Optimization complete. Final v2v error: 2.9007229804992676 mm

Highest mean error: 3.2505815029144287 mm for frame 117

Lowest mean error: 2.4313247203826904 mm for frame 19

Saving results

Total time: 39.82774806022644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961885
Iteration 2/25 | Loss: 0.00267215
Iteration 3/25 | Loss: 0.00189816
Iteration 4/25 | Loss: 0.00168091
Iteration 5/25 | Loss: 0.00155836
Iteration 6/25 | Loss: 0.00158677
Iteration 7/25 | Loss: 0.00145130
Iteration 8/25 | Loss: 0.00139853
Iteration 9/25 | Loss: 0.00138953
Iteration 10/25 | Loss: 0.00137194
Iteration 11/25 | Loss: 0.00136226
Iteration 12/25 | Loss: 0.00136313
Iteration 13/25 | Loss: 0.00136413
Iteration 14/25 | Loss: 0.00136483
Iteration 15/25 | Loss: 0.00135888
Iteration 16/25 | Loss: 0.00135693
Iteration 17/25 | Loss: 0.00135017
Iteration 18/25 | Loss: 0.00133986
Iteration 19/25 | Loss: 0.00133335
Iteration 20/25 | Loss: 0.00132957
Iteration 21/25 | Loss: 0.00132478
Iteration 22/25 | Loss: 0.00132676
Iteration 23/25 | Loss: 0.00132248
Iteration 24/25 | Loss: 0.00132091
Iteration 25/25 | Loss: 0.00131901

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35209000
Iteration 2/25 | Loss: 0.00332972
Iteration 3/25 | Loss: 0.00177100
Iteration 4/25 | Loss: 0.00177048
Iteration 5/25 | Loss: 0.00177048
Iteration 6/25 | Loss: 0.00177048
Iteration 7/25 | Loss: 0.00177048
Iteration 8/25 | Loss: 0.00177047
Iteration 9/25 | Loss: 0.00177047
Iteration 10/25 | Loss: 0.00177047
Iteration 11/25 | Loss: 0.00177047
Iteration 12/25 | Loss: 0.00177047
Iteration 13/25 | Loss: 0.00177047
Iteration 14/25 | Loss: 0.00177047
Iteration 15/25 | Loss: 0.00177047
Iteration 16/25 | Loss: 0.00177047
Iteration 17/25 | Loss: 0.00177047
Iteration 18/25 | Loss: 0.00177047
Iteration 19/25 | Loss: 0.00177047
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0017704736674204469, 0.0017704736674204469, 0.0017704736674204469, 0.0017704736674204469, 0.0017704736674204469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017704736674204469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177047
Iteration 2/1000 | Loss: 0.00114861
Iteration 3/1000 | Loss: 0.00092367
Iteration 4/1000 | Loss: 0.00047015
Iteration 5/1000 | Loss: 0.00136175
Iteration 6/1000 | Loss: 0.00046916
Iteration 7/1000 | Loss: 0.00058938
Iteration 8/1000 | Loss: 0.00067998
Iteration 9/1000 | Loss: 0.00033322
Iteration 10/1000 | Loss: 0.00077393
Iteration 11/1000 | Loss: 0.00215881
Iteration 12/1000 | Loss: 0.00123081
Iteration 13/1000 | Loss: 0.00320100
Iteration 14/1000 | Loss: 0.00074747
Iteration 15/1000 | Loss: 0.00273363
Iteration 16/1000 | Loss: 0.00162450
Iteration 17/1000 | Loss: 0.00403464
Iteration 18/1000 | Loss: 0.00248819
Iteration 19/1000 | Loss: 0.00258763
Iteration 20/1000 | Loss: 0.00284990
Iteration 21/1000 | Loss: 0.00155496
Iteration 22/1000 | Loss: 0.00063139
Iteration 23/1000 | Loss: 0.00092333
Iteration 24/1000 | Loss: 0.00084207
Iteration 25/1000 | Loss: 0.00095658
Iteration 26/1000 | Loss: 0.00022834
Iteration 27/1000 | Loss: 0.00031539
Iteration 28/1000 | Loss: 0.00039541
Iteration 29/1000 | Loss: 0.00029651
Iteration 30/1000 | Loss: 0.00071511
Iteration 31/1000 | Loss: 0.00008888
Iteration 32/1000 | Loss: 0.00006284
Iteration 33/1000 | Loss: 0.00028485
Iteration 34/1000 | Loss: 0.00006190
Iteration 35/1000 | Loss: 0.00016943
Iteration 36/1000 | Loss: 0.00033508
Iteration 37/1000 | Loss: 0.00040539
Iteration 38/1000 | Loss: 0.00008729
Iteration 39/1000 | Loss: 0.00005543
Iteration 40/1000 | Loss: 0.00021690
Iteration 41/1000 | Loss: 0.00003586
Iteration 42/1000 | Loss: 0.00015646
Iteration 43/1000 | Loss: 0.00012547
Iteration 44/1000 | Loss: 0.00015229
Iteration 45/1000 | Loss: 0.00010567
Iteration 46/1000 | Loss: 0.00015122
Iteration 47/1000 | Loss: 0.00010744
Iteration 48/1000 | Loss: 0.00044032
Iteration 49/1000 | Loss: 0.00010367
Iteration 50/1000 | Loss: 0.00034377
Iteration 51/1000 | Loss: 0.00030172
Iteration 52/1000 | Loss: 0.00004026
Iteration 53/1000 | Loss: 0.00010070
Iteration 54/1000 | Loss: 0.00003210
Iteration 55/1000 | Loss: 0.00003028
Iteration 56/1000 | Loss: 0.00005297
Iteration 57/1000 | Loss: 0.00009084
Iteration 58/1000 | Loss: 0.00002849
Iteration 59/1000 | Loss: 0.00008264
Iteration 60/1000 | Loss: 0.00012553
Iteration 61/1000 | Loss: 0.00003582
Iteration 62/1000 | Loss: 0.00004645
Iteration 63/1000 | Loss: 0.00002747
Iteration 64/1000 | Loss: 0.00018180
Iteration 65/1000 | Loss: 0.00111182
Iteration 66/1000 | Loss: 0.00077843
Iteration 67/1000 | Loss: 0.00094692
Iteration 68/1000 | Loss: 0.00027161
Iteration 69/1000 | Loss: 0.00003626
Iteration 70/1000 | Loss: 0.00005145
Iteration 71/1000 | Loss: 0.00003507
Iteration 72/1000 | Loss: 0.00007763
Iteration 73/1000 | Loss: 0.00002818
Iteration 74/1000 | Loss: 0.00002709
Iteration 75/1000 | Loss: 0.00015587
Iteration 76/1000 | Loss: 0.00002775
Iteration 77/1000 | Loss: 0.00012282
Iteration 78/1000 | Loss: 0.00009189
Iteration 79/1000 | Loss: 0.00004824
Iteration 80/1000 | Loss: 0.00003298
Iteration 81/1000 | Loss: 0.00002485
Iteration 82/1000 | Loss: 0.00007329
Iteration 83/1000 | Loss: 0.00002893
Iteration 84/1000 | Loss: 0.00003269
Iteration 85/1000 | Loss: 0.00002454
Iteration 86/1000 | Loss: 0.00002450
Iteration 87/1000 | Loss: 0.00002450
Iteration 88/1000 | Loss: 0.00002444
Iteration 89/1000 | Loss: 0.00002442
Iteration 90/1000 | Loss: 0.00002440
Iteration 91/1000 | Loss: 0.00008754
Iteration 92/1000 | Loss: 0.00002725
Iteration 93/1000 | Loss: 0.00002678
Iteration 94/1000 | Loss: 0.00002424
Iteration 95/1000 | Loss: 0.00002423
Iteration 96/1000 | Loss: 0.00002423
Iteration 97/1000 | Loss: 0.00002423
Iteration 98/1000 | Loss: 0.00002422
Iteration 99/1000 | Loss: 0.00005930
Iteration 100/1000 | Loss: 0.00002612
Iteration 101/1000 | Loss: 0.00002952
Iteration 102/1000 | Loss: 0.00002421
Iteration 103/1000 | Loss: 0.00002415
Iteration 104/1000 | Loss: 0.00002413
Iteration 105/1000 | Loss: 0.00002413
Iteration 106/1000 | Loss: 0.00002413
Iteration 107/1000 | Loss: 0.00002413
Iteration 108/1000 | Loss: 0.00002413
Iteration 109/1000 | Loss: 0.00002413
Iteration 110/1000 | Loss: 0.00002413
Iteration 111/1000 | Loss: 0.00002412
Iteration 112/1000 | Loss: 0.00002410
Iteration 113/1000 | Loss: 0.00002410
Iteration 114/1000 | Loss: 0.00002410
Iteration 115/1000 | Loss: 0.00002410
Iteration 116/1000 | Loss: 0.00002410
Iteration 117/1000 | Loss: 0.00002409
Iteration 118/1000 | Loss: 0.00002409
Iteration 119/1000 | Loss: 0.00002409
Iteration 120/1000 | Loss: 0.00002409
Iteration 121/1000 | Loss: 0.00002409
Iteration 122/1000 | Loss: 0.00002409
Iteration 123/1000 | Loss: 0.00002408
Iteration 124/1000 | Loss: 0.00002408
Iteration 125/1000 | Loss: 0.00002407
Iteration 126/1000 | Loss: 0.00002407
Iteration 127/1000 | Loss: 0.00002407
Iteration 128/1000 | Loss: 0.00002406
Iteration 129/1000 | Loss: 0.00002406
Iteration 130/1000 | Loss: 0.00002406
Iteration 131/1000 | Loss: 0.00002406
Iteration 132/1000 | Loss: 0.00002405
Iteration 133/1000 | Loss: 0.00002404
Iteration 134/1000 | Loss: 0.00002404
Iteration 135/1000 | Loss: 0.00002403
Iteration 136/1000 | Loss: 0.00002403
Iteration 137/1000 | Loss: 0.00002403
Iteration 138/1000 | Loss: 0.00002402
Iteration 139/1000 | Loss: 0.00002402
Iteration 140/1000 | Loss: 0.00002402
Iteration 141/1000 | Loss: 0.00002401
Iteration 142/1000 | Loss: 0.00002401
Iteration 143/1000 | Loss: 0.00002401
Iteration 144/1000 | Loss: 0.00002401
Iteration 145/1000 | Loss: 0.00002400
Iteration 146/1000 | Loss: 0.00002400
Iteration 147/1000 | Loss: 0.00002400
Iteration 148/1000 | Loss: 0.00002399
Iteration 149/1000 | Loss: 0.00002399
Iteration 150/1000 | Loss: 0.00002399
Iteration 151/1000 | Loss: 0.00002399
Iteration 152/1000 | Loss: 0.00002398
Iteration 153/1000 | Loss: 0.00002398
Iteration 154/1000 | Loss: 0.00002398
Iteration 155/1000 | Loss: 0.00002397
Iteration 156/1000 | Loss: 0.00002397
Iteration 157/1000 | Loss: 0.00002396
Iteration 158/1000 | Loss: 0.00002396
Iteration 159/1000 | Loss: 0.00002396
Iteration 160/1000 | Loss: 0.00002396
Iteration 161/1000 | Loss: 0.00002396
Iteration 162/1000 | Loss: 0.00002396
Iteration 163/1000 | Loss: 0.00002396
Iteration 164/1000 | Loss: 0.00002396
Iteration 165/1000 | Loss: 0.00002395
Iteration 166/1000 | Loss: 0.00002395
Iteration 167/1000 | Loss: 0.00002395
Iteration 168/1000 | Loss: 0.00002395
Iteration 169/1000 | Loss: 0.00005391
Iteration 170/1000 | Loss: 0.00006453
Iteration 171/1000 | Loss: 0.00015036
Iteration 172/1000 | Loss: 0.00002863
Iteration 173/1000 | Loss: 0.00002404
Iteration 174/1000 | Loss: 0.00002389
Iteration 175/1000 | Loss: 0.00004370
Iteration 176/1000 | Loss: 0.00002396
Iteration 177/1000 | Loss: 0.00002389
Iteration 178/1000 | Loss: 0.00002388
Iteration 179/1000 | Loss: 0.00002388
Iteration 180/1000 | Loss: 0.00002388
Iteration 181/1000 | Loss: 0.00002388
Iteration 182/1000 | Loss: 0.00002388
Iteration 183/1000 | Loss: 0.00002388
Iteration 184/1000 | Loss: 0.00002388
Iteration 185/1000 | Loss: 0.00002388
Iteration 186/1000 | Loss: 0.00002388
Iteration 187/1000 | Loss: 0.00002387
Iteration 188/1000 | Loss: 0.00002387
Iteration 189/1000 | Loss: 0.00002387
Iteration 190/1000 | Loss: 0.00002387
Iteration 191/1000 | Loss: 0.00002387
Iteration 192/1000 | Loss: 0.00002387
Iteration 193/1000 | Loss: 0.00002386
Iteration 194/1000 | Loss: 0.00002386
Iteration 195/1000 | Loss: 0.00002386
Iteration 196/1000 | Loss: 0.00002386
Iteration 197/1000 | Loss: 0.00002386
Iteration 198/1000 | Loss: 0.00002386
Iteration 199/1000 | Loss: 0.00003858
Iteration 200/1000 | Loss: 0.00017920
Iteration 201/1000 | Loss: 0.00127402
Iteration 202/1000 | Loss: 0.00023696
Iteration 203/1000 | Loss: 0.00018713
Iteration 204/1000 | Loss: 0.00009046
Iteration 205/1000 | Loss: 0.00004020
Iteration 206/1000 | Loss: 0.00004724
Iteration 207/1000 | Loss: 0.00023141
Iteration 208/1000 | Loss: 0.00032457
Iteration 209/1000 | Loss: 0.00033237
Iteration 210/1000 | Loss: 0.00047241
Iteration 211/1000 | Loss: 0.00076231
Iteration 212/1000 | Loss: 0.00023216
Iteration 213/1000 | Loss: 0.00008595
Iteration 214/1000 | Loss: 0.00005362
Iteration 215/1000 | Loss: 0.00003823
Iteration 216/1000 | Loss: 0.00004600
Iteration 217/1000 | Loss: 0.00003024
Iteration 218/1000 | Loss: 0.00007159
Iteration 219/1000 | Loss: 0.00002492
Iteration 220/1000 | Loss: 0.00002173
Iteration 221/1000 | Loss: 0.00003645
Iteration 222/1000 | Loss: 0.00001929
Iteration 223/1000 | Loss: 0.00001886
Iteration 224/1000 | Loss: 0.00003767
Iteration 225/1000 | Loss: 0.00003563
Iteration 226/1000 | Loss: 0.00001844
Iteration 227/1000 | Loss: 0.00001824
Iteration 228/1000 | Loss: 0.00001822
Iteration 229/1000 | Loss: 0.00001821
Iteration 230/1000 | Loss: 0.00001820
Iteration 231/1000 | Loss: 0.00001820
Iteration 232/1000 | Loss: 0.00001819
Iteration 233/1000 | Loss: 0.00001818
Iteration 234/1000 | Loss: 0.00001818
Iteration 235/1000 | Loss: 0.00004393
Iteration 236/1000 | Loss: 0.00016437
Iteration 237/1000 | Loss: 0.00002717
Iteration 238/1000 | Loss: 0.00001821
Iteration 239/1000 | Loss: 0.00001800
Iteration 240/1000 | Loss: 0.00001800
Iteration 241/1000 | Loss: 0.00001799
Iteration 242/1000 | Loss: 0.00001799
Iteration 243/1000 | Loss: 0.00001798
Iteration 244/1000 | Loss: 0.00001798
Iteration 245/1000 | Loss: 0.00001798
Iteration 246/1000 | Loss: 0.00001798
Iteration 247/1000 | Loss: 0.00001797
Iteration 248/1000 | Loss: 0.00001797
Iteration 249/1000 | Loss: 0.00001796
Iteration 250/1000 | Loss: 0.00001796
Iteration 251/1000 | Loss: 0.00001796
Iteration 252/1000 | Loss: 0.00001796
Iteration 253/1000 | Loss: 0.00001796
Iteration 254/1000 | Loss: 0.00001796
Iteration 255/1000 | Loss: 0.00001796
Iteration 256/1000 | Loss: 0.00001796
Iteration 257/1000 | Loss: 0.00001796
Iteration 258/1000 | Loss: 0.00001796
Iteration 259/1000 | Loss: 0.00001795
Iteration 260/1000 | Loss: 0.00001795
Iteration 261/1000 | Loss: 0.00001795
Iteration 262/1000 | Loss: 0.00001795
Iteration 263/1000 | Loss: 0.00001795
Iteration 264/1000 | Loss: 0.00001795
Iteration 265/1000 | Loss: 0.00001794
Iteration 266/1000 | Loss: 0.00001794
Iteration 267/1000 | Loss: 0.00001794
Iteration 268/1000 | Loss: 0.00001794
Iteration 269/1000 | Loss: 0.00001793
Iteration 270/1000 | Loss: 0.00001793
Iteration 271/1000 | Loss: 0.00001793
Iteration 272/1000 | Loss: 0.00001793
Iteration 273/1000 | Loss: 0.00001792
Iteration 274/1000 | Loss: 0.00001790
Iteration 275/1000 | Loss: 0.00001790
Iteration 276/1000 | Loss: 0.00001788
Iteration 277/1000 | Loss: 0.00001788
Iteration 278/1000 | Loss: 0.00001788
Iteration 279/1000 | Loss: 0.00001788
Iteration 280/1000 | Loss: 0.00001787
Iteration 281/1000 | Loss: 0.00001787
Iteration 282/1000 | Loss: 0.00001787
Iteration 283/1000 | Loss: 0.00001787
Iteration 284/1000 | Loss: 0.00001787
Iteration 285/1000 | Loss: 0.00001787
Iteration 286/1000 | Loss: 0.00001786
Iteration 287/1000 | Loss: 0.00001786
Iteration 288/1000 | Loss: 0.00001786
Iteration 289/1000 | Loss: 0.00001786
Iteration 290/1000 | Loss: 0.00001786
Iteration 291/1000 | Loss: 0.00001785
Iteration 292/1000 | Loss: 0.00001785
Iteration 293/1000 | Loss: 0.00001785
Iteration 294/1000 | Loss: 0.00001785
Iteration 295/1000 | Loss: 0.00001785
Iteration 296/1000 | Loss: 0.00001785
Iteration 297/1000 | Loss: 0.00001785
Iteration 298/1000 | Loss: 0.00001784
Iteration 299/1000 | Loss: 0.00001784
Iteration 300/1000 | Loss: 0.00001784
Iteration 301/1000 | Loss: 0.00001784
Iteration 302/1000 | Loss: 0.00001784
Iteration 303/1000 | Loss: 0.00001784
Iteration 304/1000 | Loss: 0.00001784
Iteration 305/1000 | Loss: 0.00001783
Iteration 306/1000 | Loss: 0.00001783
Iteration 307/1000 | Loss: 0.00001783
Iteration 308/1000 | Loss: 0.00001783
Iteration 309/1000 | Loss: 0.00001783
Iteration 310/1000 | Loss: 0.00001783
Iteration 311/1000 | Loss: 0.00001782
Iteration 312/1000 | Loss: 0.00001782
Iteration 313/1000 | Loss: 0.00001782
Iteration 314/1000 | Loss: 0.00001782
Iteration 315/1000 | Loss: 0.00001782
Iteration 316/1000 | Loss: 0.00001782
Iteration 317/1000 | Loss: 0.00001782
Iteration 318/1000 | Loss: 0.00001782
Iteration 319/1000 | Loss: 0.00001782
Iteration 320/1000 | Loss: 0.00001782
Iteration 321/1000 | Loss: 0.00001781
Iteration 322/1000 | Loss: 0.00001781
Iteration 323/1000 | Loss: 0.00001781
Iteration 324/1000 | Loss: 0.00006505
Iteration 325/1000 | Loss: 0.00002338
Iteration 326/1000 | Loss: 0.00006604
Iteration 327/1000 | Loss: 0.00001822
Iteration 328/1000 | Loss: 0.00002666
Iteration 329/1000 | Loss: 0.00001784
Iteration 330/1000 | Loss: 0.00001783
Iteration 331/1000 | Loss: 0.00001783
Iteration 332/1000 | Loss: 0.00001783
Iteration 333/1000 | Loss: 0.00001783
Iteration 334/1000 | Loss: 0.00001783
Iteration 335/1000 | Loss: 0.00001783
Iteration 336/1000 | Loss: 0.00001783
Iteration 337/1000 | Loss: 0.00001783
Iteration 338/1000 | Loss: 0.00001782
Iteration 339/1000 | Loss: 0.00001781
Iteration 340/1000 | Loss: 0.00001781
Iteration 341/1000 | Loss: 0.00001780
Iteration 342/1000 | Loss: 0.00001780
Iteration 343/1000 | Loss: 0.00001780
Iteration 344/1000 | Loss: 0.00001780
Iteration 345/1000 | Loss: 0.00001780
Iteration 346/1000 | Loss: 0.00001780
Iteration 347/1000 | Loss: 0.00001780
Iteration 348/1000 | Loss: 0.00001780
Iteration 349/1000 | Loss: 0.00001780
Iteration 350/1000 | Loss: 0.00001780
Iteration 351/1000 | Loss: 0.00001780
Iteration 352/1000 | Loss: 0.00001780
Iteration 353/1000 | Loss: 0.00001780
Iteration 354/1000 | Loss: 0.00001780
Iteration 355/1000 | Loss: 0.00001779
Iteration 356/1000 | Loss: 0.00001779
Iteration 357/1000 | Loss: 0.00001779
Iteration 358/1000 | Loss: 0.00001779
Iteration 359/1000 | Loss: 0.00001779
Iteration 360/1000 | Loss: 0.00001779
Iteration 361/1000 | Loss: 0.00001779
Iteration 362/1000 | Loss: 0.00001779
Iteration 363/1000 | Loss: 0.00001779
Iteration 364/1000 | Loss: 0.00001779
Iteration 365/1000 | Loss: 0.00001778
Iteration 366/1000 | Loss: 0.00001778
Iteration 367/1000 | Loss: 0.00001778
Iteration 368/1000 | Loss: 0.00001778
Iteration 369/1000 | Loss: 0.00001778
Iteration 370/1000 | Loss: 0.00001778
Iteration 371/1000 | Loss: 0.00001778
Iteration 372/1000 | Loss: 0.00001778
Iteration 373/1000 | Loss: 0.00001778
Iteration 374/1000 | Loss: 0.00001778
Iteration 375/1000 | Loss: 0.00001778
Iteration 376/1000 | Loss: 0.00001778
Iteration 377/1000 | Loss: 0.00001778
Iteration 378/1000 | Loss: 0.00001778
Iteration 379/1000 | Loss: 0.00001778
Iteration 380/1000 | Loss: 0.00001778
Iteration 381/1000 | Loss: 0.00001778
Iteration 382/1000 | Loss: 0.00001778
Iteration 383/1000 | Loss: 0.00001778
Iteration 384/1000 | Loss: 0.00001778
Iteration 385/1000 | Loss: 0.00001778
Iteration 386/1000 | Loss: 0.00001778
Iteration 387/1000 | Loss: 0.00001778
Iteration 388/1000 | Loss: 0.00001778
Iteration 389/1000 | Loss: 0.00001778
Iteration 390/1000 | Loss: 0.00001778
Iteration 391/1000 | Loss: 0.00001778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 391. Stopping optimization.
Last 5 losses: [1.7780332200345583e-05, 1.7780332200345583e-05, 1.7780332200345583e-05, 1.7780332200345583e-05, 1.7780332200345583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7780332200345583e-05

Optimization complete. Final v2v error: 3.524522542953491 mm

Highest mean error: 6.545600414276123 mm for frame 40

Lowest mean error: 2.7708539962768555 mm for frame 91

Saving results

Total time: 289.57314133644104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484753
Iteration 2/25 | Loss: 0.00126043
Iteration 3/25 | Loss: 0.00118353
Iteration 4/25 | Loss: 0.00117287
Iteration 5/25 | Loss: 0.00116989
Iteration 6/25 | Loss: 0.00116925
Iteration 7/25 | Loss: 0.00116925
Iteration 8/25 | Loss: 0.00116925
Iteration 9/25 | Loss: 0.00116925
Iteration 10/25 | Loss: 0.00116925
Iteration 11/25 | Loss: 0.00116925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011692536063492298, 0.0011692536063492298, 0.0011692536063492298, 0.0011692536063492298, 0.0011692536063492298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011692536063492298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39130628
Iteration 2/25 | Loss: 0.00079768
Iteration 3/25 | Loss: 0.00079764
Iteration 4/25 | Loss: 0.00079764
Iteration 5/25 | Loss: 0.00079764
Iteration 6/25 | Loss: 0.00079764
Iteration 7/25 | Loss: 0.00079764
Iteration 8/25 | Loss: 0.00079764
Iteration 9/25 | Loss: 0.00079764
Iteration 10/25 | Loss: 0.00079764
Iteration 11/25 | Loss: 0.00079764
Iteration 12/25 | Loss: 0.00079764
Iteration 13/25 | Loss: 0.00079764
Iteration 14/25 | Loss: 0.00079764
Iteration 15/25 | Loss: 0.00079764
Iteration 16/25 | Loss: 0.00079764
Iteration 17/25 | Loss: 0.00079764
Iteration 18/25 | Loss: 0.00079764
Iteration 19/25 | Loss: 0.00079764
Iteration 20/25 | Loss: 0.00079764
Iteration 21/25 | Loss: 0.00079764
Iteration 22/25 | Loss: 0.00079764
Iteration 23/25 | Loss: 0.00079764
Iteration 24/25 | Loss: 0.00079764
Iteration 25/25 | Loss: 0.00079764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079764
Iteration 2/1000 | Loss: 0.00002738
Iteration 3/1000 | Loss: 0.00001850
Iteration 4/1000 | Loss: 0.00001649
Iteration 5/1000 | Loss: 0.00001549
Iteration 6/1000 | Loss: 0.00001489
Iteration 7/1000 | Loss: 0.00001454
Iteration 8/1000 | Loss: 0.00001423
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001400
Iteration 11/1000 | Loss: 0.00001380
Iteration 12/1000 | Loss: 0.00001369
Iteration 13/1000 | Loss: 0.00001360
Iteration 14/1000 | Loss: 0.00001359
Iteration 15/1000 | Loss: 0.00001358
Iteration 16/1000 | Loss: 0.00001353
Iteration 17/1000 | Loss: 0.00001352
Iteration 18/1000 | Loss: 0.00001351
Iteration 19/1000 | Loss: 0.00001351
Iteration 20/1000 | Loss: 0.00001349
Iteration 21/1000 | Loss: 0.00001348
Iteration 22/1000 | Loss: 0.00001345
Iteration 23/1000 | Loss: 0.00001337
Iteration 24/1000 | Loss: 0.00001337
Iteration 25/1000 | Loss: 0.00001336
Iteration 26/1000 | Loss: 0.00001334
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001332
Iteration 29/1000 | Loss: 0.00001331
Iteration 30/1000 | Loss: 0.00001331
Iteration 31/1000 | Loss: 0.00001331
Iteration 32/1000 | Loss: 0.00001331
Iteration 33/1000 | Loss: 0.00001330
Iteration 34/1000 | Loss: 0.00001330
Iteration 35/1000 | Loss: 0.00001328
Iteration 36/1000 | Loss: 0.00001327
Iteration 37/1000 | Loss: 0.00001326
Iteration 38/1000 | Loss: 0.00001326
Iteration 39/1000 | Loss: 0.00001325
Iteration 40/1000 | Loss: 0.00001325
Iteration 41/1000 | Loss: 0.00001324
Iteration 42/1000 | Loss: 0.00001322
Iteration 43/1000 | Loss: 0.00001322
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001320
Iteration 47/1000 | Loss: 0.00001320
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001319
Iteration 50/1000 | Loss: 0.00001318
Iteration 51/1000 | Loss: 0.00001318
Iteration 52/1000 | Loss: 0.00001317
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001317
Iteration 55/1000 | Loss: 0.00001317
Iteration 56/1000 | Loss: 0.00001317
Iteration 57/1000 | Loss: 0.00001317
Iteration 58/1000 | Loss: 0.00001317
Iteration 59/1000 | Loss: 0.00001317
Iteration 60/1000 | Loss: 0.00001317
Iteration 61/1000 | Loss: 0.00001317
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001317
Iteration 64/1000 | Loss: 0.00001317
Iteration 65/1000 | Loss: 0.00001317
Iteration 66/1000 | Loss: 0.00001317
Iteration 67/1000 | Loss: 0.00001317
Iteration 68/1000 | Loss: 0.00001317
Iteration 69/1000 | Loss: 0.00001317
Iteration 70/1000 | Loss: 0.00001317
Iteration 71/1000 | Loss: 0.00001317
Iteration 72/1000 | Loss: 0.00001317
Iteration 73/1000 | Loss: 0.00001317
Iteration 74/1000 | Loss: 0.00001317
Iteration 75/1000 | Loss: 0.00001317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.3168445548217278e-05, 1.3168445548217278e-05, 1.3168445548217278e-05, 1.3168445548217278e-05, 1.3168445548217278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3168445548217278e-05

Optimization complete. Final v2v error: 3.0671298503875732 mm

Highest mean error: 3.5040245056152344 mm for frame 48

Lowest mean error: 2.795001983642578 mm for frame 147

Saving results

Total time: 31.147462606430054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960808
Iteration 2/25 | Loss: 0.00300721
Iteration 3/25 | Loss: 0.00226476
Iteration 4/25 | Loss: 0.00204595
Iteration 5/25 | Loss: 0.00192505
Iteration 6/25 | Loss: 0.00178233
Iteration 7/25 | Loss: 0.00202171
Iteration 8/25 | Loss: 0.00200797
Iteration 9/25 | Loss: 0.00141926
Iteration 10/25 | Loss: 0.00128980
Iteration 11/25 | Loss: 0.00126793
Iteration 12/25 | Loss: 0.00126813
Iteration 13/25 | Loss: 0.00126260
Iteration 14/25 | Loss: 0.00125947
Iteration 15/25 | Loss: 0.00124693
Iteration 16/25 | Loss: 0.00123256
Iteration 17/25 | Loss: 0.00123003
Iteration 18/25 | Loss: 0.00122974
Iteration 19/25 | Loss: 0.00122973
Iteration 20/25 | Loss: 0.00122972
Iteration 21/25 | Loss: 0.00122972
Iteration 22/25 | Loss: 0.00122972
Iteration 23/25 | Loss: 0.00122972
Iteration 24/25 | Loss: 0.00122971
Iteration 25/25 | Loss: 0.00122971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.05623341
Iteration 2/25 | Loss: 0.00125654
Iteration 3/25 | Loss: 0.00055274
Iteration 4/25 | Loss: 0.00055226
Iteration 5/25 | Loss: 0.00055226
Iteration 6/25 | Loss: 0.00055226
Iteration 7/25 | Loss: 0.00055226
Iteration 8/25 | Loss: 0.00055226
Iteration 9/25 | Loss: 0.00055226
Iteration 10/25 | Loss: 0.00055226
Iteration 11/25 | Loss: 0.00055226
Iteration 12/25 | Loss: 0.00055226
Iteration 13/25 | Loss: 0.00055226
Iteration 14/25 | Loss: 0.00055226
Iteration 15/25 | Loss: 0.00055226
Iteration 16/25 | Loss: 0.00055226
Iteration 17/25 | Loss: 0.00055226
Iteration 18/25 | Loss: 0.00055226
Iteration 19/25 | Loss: 0.00055226
Iteration 20/25 | Loss: 0.00055226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005522615974768996, 0.0005522615974768996, 0.0005522615974768996, 0.0005522615974768996, 0.0005522615974768996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005522615974768996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055226
Iteration 2/1000 | Loss: 0.00053445
Iteration 3/1000 | Loss: 0.00003590
Iteration 4/1000 | Loss: 0.00023321
Iteration 5/1000 | Loss: 0.00002988
Iteration 6/1000 | Loss: 0.00002778
Iteration 7/1000 | Loss: 0.00002676
Iteration 8/1000 | Loss: 0.00017259
Iteration 9/1000 | Loss: 0.00194540
Iteration 10/1000 | Loss: 0.00041562
Iteration 11/1000 | Loss: 0.00224563
Iteration 12/1000 | Loss: 0.00016494
Iteration 13/1000 | Loss: 0.00002617
Iteration 14/1000 | Loss: 0.00002323
Iteration 15/1000 | Loss: 0.00036110
Iteration 16/1000 | Loss: 0.00002226
Iteration 17/1000 | Loss: 0.00002098
Iteration 18/1000 | Loss: 0.00002051
Iteration 19/1000 | Loss: 0.00002016
Iteration 20/1000 | Loss: 0.00001989
Iteration 21/1000 | Loss: 0.00001962
Iteration 22/1000 | Loss: 0.00001939
Iteration 23/1000 | Loss: 0.00001935
Iteration 24/1000 | Loss: 0.00001926
Iteration 25/1000 | Loss: 0.00001925
Iteration 26/1000 | Loss: 0.00001922
Iteration 27/1000 | Loss: 0.00001922
Iteration 28/1000 | Loss: 0.00001921
Iteration 29/1000 | Loss: 0.00001917
Iteration 30/1000 | Loss: 0.00001917
Iteration 31/1000 | Loss: 0.00001917
Iteration 32/1000 | Loss: 0.00001916
Iteration 33/1000 | Loss: 0.00001916
Iteration 34/1000 | Loss: 0.00001916
Iteration 35/1000 | Loss: 0.00001916
Iteration 36/1000 | Loss: 0.00001916
Iteration 37/1000 | Loss: 0.00001915
Iteration 38/1000 | Loss: 0.00001915
Iteration 39/1000 | Loss: 0.00001915
Iteration 40/1000 | Loss: 0.00001915
Iteration 41/1000 | Loss: 0.00001914
Iteration 42/1000 | Loss: 0.00001914
Iteration 43/1000 | Loss: 0.00001914
Iteration 44/1000 | Loss: 0.00001914
Iteration 45/1000 | Loss: 0.00001914
Iteration 46/1000 | Loss: 0.00001914
Iteration 47/1000 | Loss: 0.00001913
Iteration 48/1000 | Loss: 0.00001913
Iteration 49/1000 | Loss: 0.00001913
Iteration 50/1000 | Loss: 0.00001913
Iteration 51/1000 | Loss: 0.00001913
Iteration 52/1000 | Loss: 0.00001913
Iteration 53/1000 | Loss: 0.00001912
Iteration 54/1000 | Loss: 0.00001912
Iteration 55/1000 | Loss: 0.00001912
Iteration 56/1000 | Loss: 0.00001912
Iteration 57/1000 | Loss: 0.00001911
Iteration 58/1000 | Loss: 0.00001911
Iteration 59/1000 | Loss: 0.00001910
Iteration 60/1000 | Loss: 0.00001910
Iteration 61/1000 | Loss: 0.00001909
Iteration 62/1000 | Loss: 0.00001909
Iteration 63/1000 | Loss: 0.00001909
Iteration 64/1000 | Loss: 0.00001908
Iteration 65/1000 | Loss: 0.00001908
Iteration 66/1000 | Loss: 0.00001908
Iteration 67/1000 | Loss: 0.00001908
Iteration 68/1000 | Loss: 0.00001908
Iteration 69/1000 | Loss: 0.00001908
Iteration 70/1000 | Loss: 0.00001908
Iteration 71/1000 | Loss: 0.00001908
Iteration 72/1000 | Loss: 0.00001908
Iteration 73/1000 | Loss: 0.00001908
Iteration 74/1000 | Loss: 0.00001908
Iteration 75/1000 | Loss: 0.00001908
Iteration 76/1000 | Loss: 0.00001908
Iteration 77/1000 | Loss: 0.00001907
Iteration 78/1000 | Loss: 0.00001907
Iteration 79/1000 | Loss: 0.00001907
Iteration 80/1000 | Loss: 0.00001907
Iteration 81/1000 | Loss: 0.00001907
Iteration 82/1000 | Loss: 0.00001907
Iteration 83/1000 | Loss: 0.00001907
Iteration 84/1000 | Loss: 0.00001907
Iteration 85/1000 | Loss: 0.00001907
Iteration 86/1000 | Loss: 0.00001907
Iteration 87/1000 | Loss: 0.00001907
Iteration 88/1000 | Loss: 0.00001907
Iteration 89/1000 | Loss: 0.00001907
Iteration 90/1000 | Loss: 0.00001907
Iteration 91/1000 | Loss: 0.00001907
Iteration 92/1000 | Loss: 0.00001907
Iteration 93/1000 | Loss: 0.00001907
Iteration 94/1000 | Loss: 0.00001907
Iteration 95/1000 | Loss: 0.00001907
Iteration 96/1000 | Loss: 0.00001907
Iteration 97/1000 | Loss: 0.00001907
Iteration 98/1000 | Loss: 0.00001907
Iteration 99/1000 | Loss: 0.00001907
Iteration 100/1000 | Loss: 0.00001907
Iteration 101/1000 | Loss: 0.00001907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.9068440451519564e-05, 1.9068440451519564e-05, 1.9068440451519564e-05, 1.9068440451519564e-05, 1.9068440451519564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9068440451519564e-05

Optimization complete. Final v2v error: 3.6812307834625244 mm

Highest mean error: 4.172886848449707 mm for frame 58

Lowest mean error: 3.2198095321655273 mm for frame 122

Saving results

Total time: 70.57500672340393
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953892
Iteration 2/25 | Loss: 0.00214936
Iteration 3/25 | Loss: 0.00176625
Iteration 4/25 | Loss: 0.00162708
Iteration 5/25 | Loss: 0.00176681
Iteration 6/25 | Loss: 0.00148531
Iteration 7/25 | Loss: 0.00140772
Iteration 8/25 | Loss: 0.00138557
Iteration 9/25 | Loss: 0.00135780
Iteration 10/25 | Loss: 0.00133198
Iteration 11/25 | Loss: 0.00131651
Iteration 12/25 | Loss: 0.00129824
Iteration 13/25 | Loss: 0.00129925
Iteration 14/25 | Loss: 0.00127966
Iteration 15/25 | Loss: 0.00129071
Iteration 16/25 | Loss: 0.00128508
Iteration 17/25 | Loss: 0.00127508
Iteration 18/25 | Loss: 0.00125562
Iteration 19/25 | Loss: 0.00124972
Iteration 20/25 | Loss: 0.00124431
Iteration 21/25 | Loss: 0.00124327
Iteration 22/25 | Loss: 0.00124766
Iteration 23/25 | Loss: 0.00124508
Iteration 24/25 | Loss: 0.00124307
Iteration 25/25 | Loss: 0.00124292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36432195
Iteration 2/25 | Loss: 0.00081622
Iteration 3/25 | Loss: 0.00081622
Iteration 4/25 | Loss: 0.00081622
Iteration 5/25 | Loss: 0.00081621
Iteration 6/25 | Loss: 0.00081621
Iteration 7/25 | Loss: 0.00081621
Iteration 8/25 | Loss: 0.00081621
Iteration 9/25 | Loss: 0.00081621
Iteration 10/25 | Loss: 0.00081621
Iteration 11/25 | Loss: 0.00081621
Iteration 12/25 | Loss: 0.00081621
Iteration 13/25 | Loss: 0.00081621
Iteration 14/25 | Loss: 0.00081621
Iteration 15/25 | Loss: 0.00081621
Iteration 16/25 | Loss: 0.00081621
Iteration 17/25 | Loss: 0.00081621
Iteration 18/25 | Loss: 0.00081621
Iteration 19/25 | Loss: 0.00081621
Iteration 20/25 | Loss: 0.00081621
Iteration 21/25 | Loss: 0.00081621
Iteration 22/25 | Loss: 0.00081621
Iteration 23/25 | Loss: 0.00081621
Iteration 24/25 | Loss: 0.00081621
Iteration 25/25 | Loss: 0.00081621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081621
Iteration 2/1000 | Loss: 0.00104794
Iteration 3/1000 | Loss: 0.00004926
Iteration 4/1000 | Loss: 0.00003226
Iteration 5/1000 | Loss: 0.00002596
Iteration 6/1000 | Loss: 0.00002338
Iteration 7/1000 | Loss: 0.00002222
Iteration 8/1000 | Loss: 0.00002109
Iteration 9/1000 | Loss: 0.00002058
Iteration 10/1000 | Loss: 0.00002015
Iteration 11/1000 | Loss: 0.00001987
Iteration 12/1000 | Loss: 0.00001966
Iteration 13/1000 | Loss: 0.00020324
Iteration 14/1000 | Loss: 0.00002288
Iteration 15/1000 | Loss: 0.00001976
Iteration 16/1000 | Loss: 0.00001880
Iteration 17/1000 | Loss: 0.00001789
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00001688
Iteration 20/1000 | Loss: 0.00001659
Iteration 21/1000 | Loss: 0.00001642
Iteration 22/1000 | Loss: 0.00001632
Iteration 23/1000 | Loss: 0.00001622
Iteration 24/1000 | Loss: 0.00001612
Iteration 25/1000 | Loss: 0.00001597
Iteration 26/1000 | Loss: 0.00001594
Iteration 27/1000 | Loss: 0.00001594
Iteration 28/1000 | Loss: 0.00001585
Iteration 29/1000 | Loss: 0.00001584
Iteration 30/1000 | Loss: 0.00001583
Iteration 31/1000 | Loss: 0.00001578
Iteration 32/1000 | Loss: 0.00001577
Iteration 33/1000 | Loss: 0.00001577
Iteration 34/1000 | Loss: 0.00001577
Iteration 35/1000 | Loss: 0.00001575
Iteration 36/1000 | Loss: 0.00001574
Iteration 37/1000 | Loss: 0.00001574
Iteration 38/1000 | Loss: 0.00001573
Iteration 39/1000 | Loss: 0.00001573
Iteration 40/1000 | Loss: 0.00001573
Iteration 41/1000 | Loss: 0.00001573
Iteration 42/1000 | Loss: 0.00001572
Iteration 43/1000 | Loss: 0.00001572
Iteration 44/1000 | Loss: 0.00001572
Iteration 45/1000 | Loss: 0.00001572
Iteration 46/1000 | Loss: 0.00001572
Iteration 47/1000 | Loss: 0.00001572
Iteration 48/1000 | Loss: 0.00001571
Iteration 49/1000 | Loss: 0.00001571
Iteration 50/1000 | Loss: 0.00001571
Iteration 51/1000 | Loss: 0.00001570
Iteration 52/1000 | Loss: 0.00001570
Iteration 53/1000 | Loss: 0.00001570
Iteration 54/1000 | Loss: 0.00001570
Iteration 55/1000 | Loss: 0.00001570
Iteration 56/1000 | Loss: 0.00001570
Iteration 57/1000 | Loss: 0.00001569
Iteration 58/1000 | Loss: 0.00001569
Iteration 59/1000 | Loss: 0.00001569
Iteration 60/1000 | Loss: 0.00001569
Iteration 61/1000 | Loss: 0.00001568
Iteration 62/1000 | Loss: 0.00001567
Iteration 63/1000 | Loss: 0.00001567
Iteration 64/1000 | Loss: 0.00001567
Iteration 65/1000 | Loss: 0.00001567
Iteration 66/1000 | Loss: 0.00001567
Iteration 67/1000 | Loss: 0.00001566
Iteration 68/1000 | Loss: 0.00001566
Iteration 69/1000 | Loss: 0.00001566
Iteration 70/1000 | Loss: 0.00001566
Iteration 71/1000 | Loss: 0.00001566
Iteration 72/1000 | Loss: 0.00001566
Iteration 73/1000 | Loss: 0.00001566
Iteration 74/1000 | Loss: 0.00001566
Iteration 75/1000 | Loss: 0.00001565
Iteration 76/1000 | Loss: 0.00001565
Iteration 77/1000 | Loss: 0.00001565
Iteration 78/1000 | Loss: 0.00001564
Iteration 79/1000 | Loss: 0.00001564
Iteration 80/1000 | Loss: 0.00001564
Iteration 81/1000 | Loss: 0.00001564
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001564
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001563
Iteration 86/1000 | Loss: 0.00001563
Iteration 87/1000 | Loss: 0.00001563
Iteration 88/1000 | Loss: 0.00001563
Iteration 89/1000 | Loss: 0.00001563
Iteration 90/1000 | Loss: 0.00001563
Iteration 91/1000 | Loss: 0.00001563
Iteration 92/1000 | Loss: 0.00001563
Iteration 93/1000 | Loss: 0.00001563
Iteration 94/1000 | Loss: 0.00001563
Iteration 95/1000 | Loss: 0.00001563
Iteration 96/1000 | Loss: 0.00001562
Iteration 97/1000 | Loss: 0.00001562
Iteration 98/1000 | Loss: 0.00001562
Iteration 99/1000 | Loss: 0.00001562
Iteration 100/1000 | Loss: 0.00001562
Iteration 101/1000 | Loss: 0.00001562
Iteration 102/1000 | Loss: 0.00001561
Iteration 103/1000 | Loss: 0.00001561
Iteration 104/1000 | Loss: 0.00001561
Iteration 105/1000 | Loss: 0.00001561
Iteration 106/1000 | Loss: 0.00001560
Iteration 107/1000 | Loss: 0.00001560
Iteration 108/1000 | Loss: 0.00001560
Iteration 109/1000 | Loss: 0.00001560
Iteration 110/1000 | Loss: 0.00001560
Iteration 111/1000 | Loss: 0.00001560
Iteration 112/1000 | Loss: 0.00001560
Iteration 113/1000 | Loss: 0.00001560
Iteration 114/1000 | Loss: 0.00001560
Iteration 115/1000 | Loss: 0.00001560
Iteration 116/1000 | Loss: 0.00001560
Iteration 117/1000 | Loss: 0.00001560
Iteration 118/1000 | Loss: 0.00001560
Iteration 119/1000 | Loss: 0.00001559
Iteration 120/1000 | Loss: 0.00001559
Iteration 121/1000 | Loss: 0.00001559
Iteration 122/1000 | Loss: 0.00001559
Iteration 123/1000 | Loss: 0.00001559
Iteration 124/1000 | Loss: 0.00001559
Iteration 125/1000 | Loss: 0.00001559
Iteration 126/1000 | Loss: 0.00001559
Iteration 127/1000 | Loss: 0.00001559
Iteration 128/1000 | Loss: 0.00001559
Iteration 129/1000 | Loss: 0.00001559
Iteration 130/1000 | Loss: 0.00001559
Iteration 131/1000 | Loss: 0.00001559
Iteration 132/1000 | Loss: 0.00001559
Iteration 133/1000 | Loss: 0.00001558
Iteration 134/1000 | Loss: 0.00001558
Iteration 135/1000 | Loss: 0.00001558
Iteration 136/1000 | Loss: 0.00001558
Iteration 137/1000 | Loss: 0.00001557
Iteration 138/1000 | Loss: 0.00001557
Iteration 139/1000 | Loss: 0.00001557
Iteration 140/1000 | Loss: 0.00001557
Iteration 141/1000 | Loss: 0.00001557
Iteration 142/1000 | Loss: 0.00001557
Iteration 143/1000 | Loss: 0.00001557
Iteration 144/1000 | Loss: 0.00001557
Iteration 145/1000 | Loss: 0.00001557
Iteration 146/1000 | Loss: 0.00001557
Iteration 147/1000 | Loss: 0.00001557
Iteration 148/1000 | Loss: 0.00001557
Iteration 149/1000 | Loss: 0.00001557
Iteration 150/1000 | Loss: 0.00001557
Iteration 151/1000 | Loss: 0.00001557
Iteration 152/1000 | Loss: 0.00001557
Iteration 153/1000 | Loss: 0.00001557
Iteration 154/1000 | Loss: 0.00001557
Iteration 155/1000 | Loss: 0.00001557
Iteration 156/1000 | Loss: 0.00001557
Iteration 157/1000 | Loss: 0.00001556
Iteration 158/1000 | Loss: 0.00001556
Iteration 159/1000 | Loss: 0.00001556
Iteration 160/1000 | Loss: 0.00001556
Iteration 161/1000 | Loss: 0.00001556
Iteration 162/1000 | Loss: 0.00001556
Iteration 163/1000 | Loss: 0.00001556
Iteration 164/1000 | Loss: 0.00001556
Iteration 165/1000 | Loss: 0.00001556
Iteration 166/1000 | Loss: 0.00001556
Iteration 167/1000 | Loss: 0.00001556
Iteration 168/1000 | Loss: 0.00001556
Iteration 169/1000 | Loss: 0.00001556
Iteration 170/1000 | Loss: 0.00001556
Iteration 171/1000 | Loss: 0.00001556
Iteration 172/1000 | Loss: 0.00001556
Iteration 173/1000 | Loss: 0.00001556
Iteration 174/1000 | Loss: 0.00001556
Iteration 175/1000 | Loss: 0.00001556
Iteration 176/1000 | Loss: 0.00001556
Iteration 177/1000 | Loss: 0.00001556
Iteration 178/1000 | Loss: 0.00001556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.5557896404061466e-05, 1.5557896404061466e-05, 1.5557896404061466e-05, 1.5557896404061466e-05, 1.5557896404061466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5557896404061466e-05

Optimization complete. Final v2v error: 3.2718849182128906 mm

Highest mean error: 4.431236267089844 mm for frame 47

Lowest mean error: 2.8974268436431885 mm for frame 33

Saving results

Total time: 86.07832384109497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360211
Iteration 2/25 | Loss: 0.00118860
Iteration 3/25 | Loss: 0.00111688
Iteration 4/25 | Loss: 0.00110648
Iteration 5/25 | Loss: 0.00110262
Iteration 6/25 | Loss: 0.00110251
Iteration 7/25 | Loss: 0.00110251
Iteration 8/25 | Loss: 0.00110251
Iteration 9/25 | Loss: 0.00110251
Iteration 10/25 | Loss: 0.00110251
Iteration 11/25 | Loss: 0.00110251
Iteration 12/25 | Loss: 0.00110251
Iteration 13/25 | Loss: 0.00110251
Iteration 14/25 | Loss: 0.00110251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011025118874385953, 0.0011025118874385953, 0.0011025118874385953, 0.0011025118874385953, 0.0011025118874385953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011025118874385953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61985052
Iteration 2/25 | Loss: 0.00085209
Iteration 3/25 | Loss: 0.00085209
Iteration 4/25 | Loss: 0.00085209
Iteration 5/25 | Loss: 0.00085209
Iteration 6/25 | Loss: 0.00085209
Iteration 7/25 | Loss: 0.00085209
Iteration 8/25 | Loss: 0.00085209
Iteration 9/25 | Loss: 0.00085209
Iteration 10/25 | Loss: 0.00085209
Iteration 11/25 | Loss: 0.00085208
Iteration 12/25 | Loss: 0.00085208
Iteration 13/25 | Loss: 0.00085208
Iteration 14/25 | Loss: 0.00085208
Iteration 15/25 | Loss: 0.00085208
Iteration 16/25 | Loss: 0.00085208
Iteration 17/25 | Loss: 0.00085208
Iteration 18/25 | Loss: 0.00085208
Iteration 19/25 | Loss: 0.00085208
Iteration 20/25 | Loss: 0.00085208
Iteration 21/25 | Loss: 0.00085208
Iteration 22/25 | Loss: 0.00085208
Iteration 23/25 | Loss: 0.00085208
Iteration 24/25 | Loss: 0.00085208
Iteration 25/25 | Loss: 0.00085208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085208
Iteration 2/1000 | Loss: 0.00001449
Iteration 3/1000 | Loss: 0.00001181
Iteration 4/1000 | Loss: 0.00001068
Iteration 5/1000 | Loss: 0.00001006
Iteration 6/1000 | Loss: 0.00000971
Iteration 7/1000 | Loss: 0.00000963
Iteration 8/1000 | Loss: 0.00000962
Iteration 9/1000 | Loss: 0.00000931
Iteration 10/1000 | Loss: 0.00000917
Iteration 11/1000 | Loss: 0.00000908
Iteration 12/1000 | Loss: 0.00000904
Iteration 13/1000 | Loss: 0.00000903
Iteration 14/1000 | Loss: 0.00000900
Iteration 15/1000 | Loss: 0.00000899
Iteration 16/1000 | Loss: 0.00000898
Iteration 17/1000 | Loss: 0.00000894
Iteration 18/1000 | Loss: 0.00000891
Iteration 19/1000 | Loss: 0.00000891
Iteration 20/1000 | Loss: 0.00000890
Iteration 21/1000 | Loss: 0.00000890
Iteration 22/1000 | Loss: 0.00000889
Iteration 23/1000 | Loss: 0.00000889
Iteration 24/1000 | Loss: 0.00000888
Iteration 25/1000 | Loss: 0.00000887
Iteration 26/1000 | Loss: 0.00000886
Iteration 27/1000 | Loss: 0.00000885
Iteration 28/1000 | Loss: 0.00000884
Iteration 29/1000 | Loss: 0.00000884
Iteration 30/1000 | Loss: 0.00000883
Iteration 31/1000 | Loss: 0.00000882
Iteration 32/1000 | Loss: 0.00000882
Iteration 33/1000 | Loss: 0.00000882
Iteration 34/1000 | Loss: 0.00000879
Iteration 35/1000 | Loss: 0.00000878
Iteration 36/1000 | Loss: 0.00000878
Iteration 37/1000 | Loss: 0.00000877
Iteration 38/1000 | Loss: 0.00000875
Iteration 39/1000 | Loss: 0.00000875
Iteration 40/1000 | Loss: 0.00000875
Iteration 41/1000 | Loss: 0.00000875
Iteration 42/1000 | Loss: 0.00000875
Iteration 43/1000 | Loss: 0.00000874
Iteration 44/1000 | Loss: 0.00000873
Iteration 45/1000 | Loss: 0.00000873
Iteration 46/1000 | Loss: 0.00000872
Iteration 47/1000 | Loss: 0.00000871
Iteration 48/1000 | Loss: 0.00000868
Iteration 49/1000 | Loss: 0.00000868
Iteration 50/1000 | Loss: 0.00000868
Iteration 51/1000 | Loss: 0.00000868
Iteration 52/1000 | Loss: 0.00000868
Iteration 53/1000 | Loss: 0.00000865
Iteration 54/1000 | Loss: 0.00000865
Iteration 55/1000 | Loss: 0.00000862
Iteration 56/1000 | Loss: 0.00000862
Iteration 57/1000 | Loss: 0.00000861
Iteration 58/1000 | Loss: 0.00000861
Iteration 59/1000 | Loss: 0.00000860
Iteration 60/1000 | Loss: 0.00000860
Iteration 61/1000 | Loss: 0.00000859
Iteration 62/1000 | Loss: 0.00000859
Iteration 63/1000 | Loss: 0.00000858
Iteration 64/1000 | Loss: 0.00000857
Iteration 65/1000 | Loss: 0.00000857
Iteration 66/1000 | Loss: 0.00000857
Iteration 67/1000 | Loss: 0.00000857
Iteration 68/1000 | Loss: 0.00000856
Iteration 69/1000 | Loss: 0.00000856
Iteration 70/1000 | Loss: 0.00000856
Iteration 71/1000 | Loss: 0.00000856
Iteration 72/1000 | Loss: 0.00000855
Iteration 73/1000 | Loss: 0.00000855
Iteration 74/1000 | Loss: 0.00000855
Iteration 75/1000 | Loss: 0.00000855
Iteration 76/1000 | Loss: 0.00000855
Iteration 77/1000 | Loss: 0.00000855
Iteration 78/1000 | Loss: 0.00000855
Iteration 79/1000 | Loss: 0.00000854
Iteration 80/1000 | Loss: 0.00000854
Iteration 81/1000 | Loss: 0.00000854
Iteration 82/1000 | Loss: 0.00000854
Iteration 83/1000 | Loss: 0.00000853
Iteration 84/1000 | Loss: 0.00000853
Iteration 85/1000 | Loss: 0.00000853
Iteration 86/1000 | Loss: 0.00000853
Iteration 87/1000 | Loss: 0.00000853
Iteration 88/1000 | Loss: 0.00000853
Iteration 89/1000 | Loss: 0.00000853
Iteration 90/1000 | Loss: 0.00000853
Iteration 91/1000 | Loss: 0.00000853
Iteration 92/1000 | Loss: 0.00000853
Iteration 93/1000 | Loss: 0.00000853
Iteration 94/1000 | Loss: 0.00000853
Iteration 95/1000 | Loss: 0.00000853
Iteration 96/1000 | Loss: 0.00000853
Iteration 97/1000 | Loss: 0.00000852
Iteration 98/1000 | Loss: 0.00000852
Iteration 99/1000 | Loss: 0.00000852
Iteration 100/1000 | Loss: 0.00000852
Iteration 101/1000 | Loss: 0.00000852
Iteration 102/1000 | Loss: 0.00000852
Iteration 103/1000 | Loss: 0.00000852
Iteration 104/1000 | Loss: 0.00000851
Iteration 105/1000 | Loss: 0.00000851
Iteration 106/1000 | Loss: 0.00000851
Iteration 107/1000 | Loss: 0.00000851
Iteration 108/1000 | Loss: 0.00000851
Iteration 109/1000 | Loss: 0.00000850
Iteration 110/1000 | Loss: 0.00000850
Iteration 111/1000 | Loss: 0.00000850
Iteration 112/1000 | Loss: 0.00000850
Iteration 113/1000 | Loss: 0.00000850
Iteration 114/1000 | Loss: 0.00000850
Iteration 115/1000 | Loss: 0.00000849
Iteration 116/1000 | Loss: 0.00000849
Iteration 117/1000 | Loss: 0.00000849
Iteration 118/1000 | Loss: 0.00000849
Iteration 119/1000 | Loss: 0.00000848
Iteration 120/1000 | Loss: 0.00000848
Iteration 121/1000 | Loss: 0.00000848
Iteration 122/1000 | Loss: 0.00000848
Iteration 123/1000 | Loss: 0.00000848
Iteration 124/1000 | Loss: 0.00000848
Iteration 125/1000 | Loss: 0.00000848
Iteration 126/1000 | Loss: 0.00000848
Iteration 127/1000 | Loss: 0.00000848
Iteration 128/1000 | Loss: 0.00000847
Iteration 129/1000 | Loss: 0.00000847
Iteration 130/1000 | Loss: 0.00000847
Iteration 131/1000 | Loss: 0.00000847
Iteration 132/1000 | Loss: 0.00000847
Iteration 133/1000 | Loss: 0.00000847
Iteration 134/1000 | Loss: 0.00000847
Iteration 135/1000 | Loss: 0.00000847
Iteration 136/1000 | Loss: 0.00000847
Iteration 137/1000 | Loss: 0.00000846
Iteration 138/1000 | Loss: 0.00000846
Iteration 139/1000 | Loss: 0.00000846
Iteration 140/1000 | Loss: 0.00000846
Iteration 141/1000 | Loss: 0.00000846
Iteration 142/1000 | Loss: 0.00000846
Iteration 143/1000 | Loss: 0.00000846
Iteration 144/1000 | Loss: 0.00000846
Iteration 145/1000 | Loss: 0.00000845
Iteration 146/1000 | Loss: 0.00000845
Iteration 147/1000 | Loss: 0.00000845
Iteration 148/1000 | Loss: 0.00000845
Iteration 149/1000 | Loss: 0.00000844
Iteration 150/1000 | Loss: 0.00000844
Iteration 151/1000 | Loss: 0.00000844
Iteration 152/1000 | Loss: 0.00000843
Iteration 153/1000 | Loss: 0.00000843
Iteration 154/1000 | Loss: 0.00000843
Iteration 155/1000 | Loss: 0.00000843
Iteration 156/1000 | Loss: 0.00000843
Iteration 157/1000 | Loss: 0.00000843
Iteration 158/1000 | Loss: 0.00000843
Iteration 159/1000 | Loss: 0.00000843
Iteration 160/1000 | Loss: 0.00000842
Iteration 161/1000 | Loss: 0.00000842
Iteration 162/1000 | Loss: 0.00000842
Iteration 163/1000 | Loss: 0.00000842
Iteration 164/1000 | Loss: 0.00000841
Iteration 165/1000 | Loss: 0.00000841
Iteration 166/1000 | Loss: 0.00000841
Iteration 167/1000 | Loss: 0.00000841
Iteration 168/1000 | Loss: 0.00000841
Iteration 169/1000 | Loss: 0.00000841
Iteration 170/1000 | Loss: 0.00000841
Iteration 171/1000 | Loss: 0.00000841
Iteration 172/1000 | Loss: 0.00000841
Iteration 173/1000 | Loss: 0.00000841
Iteration 174/1000 | Loss: 0.00000841
Iteration 175/1000 | Loss: 0.00000840
Iteration 176/1000 | Loss: 0.00000840
Iteration 177/1000 | Loss: 0.00000840
Iteration 178/1000 | Loss: 0.00000840
Iteration 179/1000 | Loss: 0.00000840
Iteration 180/1000 | Loss: 0.00000840
Iteration 181/1000 | Loss: 0.00000840
Iteration 182/1000 | Loss: 0.00000840
Iteration 183/1000 | Loss: 0.00000840
Iteration 184/1000 | Loss: 0.00000839
Iteration 185/1000 | Loss: 0.00000839
Iteration 186/1000 | Loss: 0.00000839
Iteration 187/1000 | Loss: 0.00000839
Iteration 188/1000 | Loss: 0.00000839
Iteration 189/1000 | Loss: 0.00000839
Iteration 190/1000 | Loss: 0.00000838
Iteration 191/1000 | Loss: 0.00000838
Iteration 192/1000 | Loss: 0.00000838
Iteration 193/1000 | Loss: 0.00000838
Iteration 194/1000 | Loss: 0.00000838
Iteration 195/1000 | Loss: 0.00000837
Iteration 196/1000 | Loss: 0.00000837
Iteration 197/1000 | Loss: 0.00000837
Iteration 198/1000 | Loss: 0.00000837
Iteration 199/1000 | Loss: 0.00000837
Iteration 200/1000 | Loss: 0.00000837
Iteration 201/1000 | Loss: 0.00000837
Iteration 202/1000 | Loss: 0.00000837
Iteration 203/1000 | Loss: 0.00000837
Iteration 204/1000 | Loss: 0.00000837
Iteration 205/1000 | Loss: 0.00000836
Iteration 206/1000 | Loss: 0.00000836
Iteration 207/1000 | Loss: 0.00000836
Iteration 208/1000 | Loss: 0.00000836
Iteration 209/1000 | Loss: 0.00000836
Iteration 210/1000 | Loss: 0.00000836
Iteration 211/1000 | Loss: 0.00000836
Iteration 212/1000 | Loss: 0.00000836
Iteration 213/1000 | Loss: 0.00000836
Iteration 214/1000 | Loss: 0.00000836
Iteration 215/1000 | Loss: 0.00000836
Iteration 216/1000 | Loss: 0.00000836
Iteration 217/1000 | Loss: 0.00000836
Iteration 218/1000 | Loss: 0.00000836
Iteration 219/1000 | Loss: 0.00000836
Iteration 220/1000 | Loss: 0.00000836
Iteration 221/1000 | Loss: 0.00000836
Iteration 222/1000 | Loss: 0.00000836
Iteration 223/1000 | Loss: 0.00000836
Iteration 224/1000 | Loss: 0.00000836
Iteration 225/1000 | Loss: 0.00000836
Iteration 226/1000 | Loss: 0.00000836
Iteration 227/1000 | Loss: 0.00000836
Iteration 228/1000 | Loss: 0.00000836
Iteration 229/1000 | Loss: 0.00000836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [8.36207800603006e-06, 8.36207800603006e-06, 8.36207800603006e-06, 8.36207800603006e-06, 8.36207800603006e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.36207800603006e-06

Optimization complete. Final v2v error: 2.499514579772949 mm

Highest mean error: 2.9355883598327637 mm for frame 131

Lowest mean error: 2.4179153442382812 mm for frame 213

Saving results

Total time: 45.801159143447876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006075
Iteration 2/25 | Loss: 0.00177573
Iteration 3/25 | Loss: 0.00139153
Iteration 4/25 | Loss: 0.00136748
Iteration 5/25 | Loss: 0.00135955
Iteration 6/25 | Loss: 0.00135807
Iteration 7/25 | Loss: 0.00135807
Iteration 8/25 | Loss: 0.00135807
Iteration 9/25 | Loss: 0.00135807
Iteration 10/25 | Loss: 0.00135807
Iteration 11/25 | Loss: 0.00135807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013580713421106339, 0.0013580713421106339, 0.0013580713421106339, 0.0013580713421106339, 0.0013580713421106339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013580713421106339

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54492933
Iteration 2/25 | Loss: 0.00106949
Iteration 3/25 | Loss: 0.00106949
Iteration 4/25 | Loss: 0.00106949
Iteration 5/25 | Loss: 0.00106949
Iteration 6/25 | Loss: 0.00106949
Iteration 7/25 | Loss: 0.00106949
Iteration 8/25 | Loss: 0.00106949
Iteration 9/25 | Loss: 0.00106949
Iteration 10/25 | Loss: 0.00106949
Iteration 11/25 | Loss: 0.00106949
Iteration 12/25 | Loss: 0.00106949
Iteration 13/25 | Loss: 0.00106949
Iteration 14/25 | Loss: 0.00106949
Iteration 15/25 | Loss: 0.00106949
Iteration 16/25 | Loss: 0.00106949
Iteration 17/25 | Loss: 0.00106949
Iteration 18/25 | Loss: 0.00106949
Iteration 19/25 | Loss: 0.00106949
Iteration 20/25 | Loss: 0.00106949
Iteration 21/25 | Loss: 0.00106949
Iteration 22/25 | Loss: 0.00106949
Iteration 23/25 | Loss: 0.00106949
Iteration 24/25 | Loss: 0.00106949
Iteration 25/25 | Loss: 0.00106949

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106949
Iteration 2/1000 | Loss: 0.00006749
Iteration 3/1000 | Loss: 0.00004677
Iteration 4/1000 | Loss: 0.00003820
Iteration 5/1000 | Loss: 0.00003430
Iteration 6/1000 | Loss: 0.00003286
Iteration 7/1000 | Loss: 0.00003215
Iteration 8/1000 | Loss: 0.00003148
Iteration 9/1000 | Loss: 0.00003084
Iteration 10/1000 | Loss: 0.00003050
Iteration 11/1000 | Loss: 0.00003025
Iteration 12/1000 | Loss: 0.00003000
Iteration 13/1000 | Loss: 0.00002975
Iteration 14/1000 | Loss: 0.00002948
Iteration 15/1000 | Loss: 0.00002922
Iteration 16/1000 | Loss: 0.00002898
Iteration 17/1000 | Loss: 0.00002876
Iteration 18/1000 | Loss: 0.00002855
Iteration 19/1000 | Loss: 0.00002840
Iteration 20/1000 | Loss: 0.00002837
Iteration 21/1000 | Loss: 0.00002837
Iteration 22/1000 | Loss: 0.00002835
Iteration 23/1000 | Loss: 0.00002831
Iteration 24/1000 | Loss: 0.00002830
Iteration 25/1000 | Loss: 0.00002830
Iteration 26/1000 | Loss: 0.00002829
Iteration 27/1000 | Loss: 0.00002829
Iteration 28/1000 | Loss: 0.00002826
Iteration 29/1000 | Loss: 0.00002822
Iteration 30/1000 | Loss: 0.00002822
Iteration 31/1000 | Loss: 0.00002820
Iteration 32/1000 | Loss: 0.00002820
Iteration 33/1000 | Loss: 0.00002819
Iteration 34/1000 | Loss: 0.00002815
Iteration 35/1000 | Loss: 0.00002815
Iteration 36/1000 | Loss: 0.00002809
Iteration 37/1000 | Loss: 0.00002809
Iteration 38/1000 | Loss: 0.00002807
Iteration 39/1000 | Loss: 0.00002805
Iteration 40/1000 | Loss: 0.00002805
Iteration 41/1000 | Loss: 0.00002804
Iteration 42/1000 | Loss: 0.00002804
Iteration 43/1000 | Loss: 0.00002800
Iteration 44/1000 | Loss: 0.00002800
Iteration 45/1000 | Loss: 0.00002800
Iteration 46/1000 | Loss: 0.00002800
Iteration 47/1000 | Loss: 0.00002800
Iteration 48/1000 | Loss: 0.00002800
Iteration 49/1000 | Loss: 0.00002800
Iteration 50/1000 | Loss: 0.00002800
Iteration 51/1000 | Loss: 0.00002800
Iteration 52/1000 | Loss: 0.00002799
Iteration 53/1000 | Loss: 0.00002799
Iteration 54/1000 | Loss: 0.00002799
Iteration 55/1000 | Loss: 0.00002799
Iteration 56/1000 | Loss: 0.00002798
Iteration 57/1000 | Loss: 0.00002797
Iteration 58/1000 | Loss: 0.00002797
Iteration 59/1000 | Loss: 0.00002797
Iteration 60/1000 | Loss: 0.00002797
Iteration 61/1000 | Loss: 0.00002797
Iteration 62/1000 | Loss: 0.00002797
Iteration 63/1000 | Loss: 0.00002797
Iteration 64/1000 | Loss: 0.00002797
Iteration 65/1000 | Loss: 0.00002797
Iteration 66/1000 | Loss: 0.00002797
Iteration 67/1000 | Loss: 0.00002797
Iteration 68/1000 | Loss: 0.00002797
Iteration 69/1000 | Loss: 0.00002796
Iteration 70/1000 | Loss: 0.00002796
Iteration 71/1000 | Loss: 0.00002796
Iteration 72/1000 | Loss: 0.00002796
Iteration 73/1000 | Loss: 0.00002796
Iteration 74/1000 | Loss: 0.00002796
Iteration 75/1000 | Loss: 0.00002795
Iteration 76/1000 | Loss: 0.00002795
Iteration 77/1000 | Loss: 0.00002795
Iteration 78/1000 | Loss: 0.00002795
Iteration 79/1000 | Loss: 0.00002795
Iteration 80/1000 | Loss: 0.00002795
Iteration 81/1000 | Loss: 0.00002795
Iteration 82/1000 | Loss: 0.00002795
Iteration 83/1000 | Loss: 0.00002795
Iteration 84/1000 | Loss: 0.00002795
Iteration 85/1000 | Loss: 0.00002795
Iteration 86/1000 | Loss: 0.00002795
Iteration 87/1000 | Loss: 0.00002794
Iteration 88/1000 | Loss: 0.00002794
Iteration 89/1000 | Loss: 0.00002794
Iteration 90/1000 | Loss: 0.00002794
Iteration 91/1000 | Loss: 0.00002794
Iteration 92/1000 | Loss: 0.00002794
Iteration 93/1000 | Loss: 0.00002794
Iteration 94/1000 | Loss: 0.00002793
Iteration 95/1000 | Loss: 0.00002793
Iteration 96/1000 | Loss: 0.00002793
Iteration 97/1000 | Loss: 0.00002793
Iteration 98/1000 | Loss: 0.00002792
Iteration 99/1000 | Loss: 0.00002792
Iteration 100/1000 | Loss: 0.00002792
Iteration 101/1000 | Loss: 0.00002792
Iteration 102/1000 | Loss: 0.00002792
Iteration 103/1000 | Loss: 0.00002792
Iteration 104/1000 | Loss: 0.00002792
Iteration 105/1000 | Loss: 0.00002792
Iteration 106/1000 | Loss: 0.00002792
Iteration 107/1000 | Loss: 0.00002792
Iteration 108/1000 | Loss: 0.00002791
Iteration 109/1000 | Loss: 0.00002791
Iteration 110/1000 | Loss: 0.00002791
Iteration 111/1000 | Loss: 0.00002791
Iteration 112/1000 | Loss: 0.00002791
Iteration 113/1000 | Loss: 0.00002791
Iteration 114/1000 | Loss: 0.00002791
Iteration 115/1000 | Loss: 0.00002791
Iteration 116/1000 | Loss: 0.00002791
Iteration 117/1000 | Loss: 0.00002791
Iteration 118/1000 | Loss: 0.00002791
Iteration 119/1000 | Loss: 0.00002790
Iteration 120/1000 | Loss: 0.00002790
Iteration 121/1000 | Loss: 0.00002790
Iteration 122/1000 | Loss: 0.00002790
Iteration 123/1000 | Loss: 0.00002790
Iteration 124/1000 | Loss: 0.00002790
Iteration 125/1000 | Loss: 0.00002790
Iteration 126/1000 | Loss: 0.00002790
Iteration 127/1000 | Loss: 0.00002790
Iteration 128/1000 | Loss: 0.00002790
Iteration 129/1000 | Loss: 0.00002790
Iteration 130/1000 | Loss: 0.00002790
Iteration 131/1000 | Loss: 0.00002789
Iteration 132/1000 | Loss: 0.00002789
Iteration 133/1000 | Loss: 0.00002789
Iteration 134/1000 | Loss: 0.00002789
Iteration 135/1000 | Loss: 0.00002789
Iteration 136/1000 | Loss: 0.00002789
Iteration 137/1000 | Loss: 0.00002789
Iteration 138/1000 | Loss: 0.00002789
Iteration 139/1000 | Loss: 0.00002789
Iteration 140/1000 | Loss: 0.00002789
Iteration 141/1000 | Loss: 0.00002789
Iteration 142/1000 | Loss: 0.00002789
Iteration 143/1000 | Loss: 0.00002789
Iteration 144/1000 | Loss: 0.00002788
Iteration 145/1000 | Loss: 0.00002788
Iteration 146/1000 | Loss: 0.00002788
Iteration 147/1000 | Loss: 0.00002788
Iteration 148/1000 | Loss: 0.00002788
Iteration 149/1000 | Loss: 0.00002788
Iteration 150/1000 | Loss: 0.00002788
Iteration 151/1000 | Loss: 0.00002787
Iteration 152/1000 | Loss: 0.00002787
Iteration 153/1000 | Loss: 0.00002787
Iteration 154/1000 | Loss: 0.00002787
Iteration 155/1000 | Loss: 0.00002787
Iteration 156/1000 | Loss: 0.00002787
Iteration 157/1000 | Loss: 0.00002787
Iteration 158/1000 | Loss: 0.00002787
Iteration 159/1000 | Loss: 0.00002787
Iteration 160/1000 | Loss: 0.00002787
Iteration 161/1000 | Loss: 0.00002787
Iteration 162/1000 | Loss: 0.00002787
Iteration 163/1000 | Loss: 0.00002786
Iteration 164/1000 | Loss: 0.00002786
Iteration 165/1000 | Loss: 0.00002786
Iteration 166/1000 | Loss: 0.00002786
Iteration 167/1000 | Loss: 0.00002786
Iteration 168/1000 | Loss: 0.00002786
Iteration 169/1000 | Loss: 0.00002786
Iteration 170/1000 | Loss: 0.00002786
Iteration 171/1000 | Loss: 0.00002786
Iteration 172/1000 | Loss: 0.00002786
Iteration 173/1000 | Loss: 0.00002786
Iteration 174/1000 | Loss: 0.00002786
Iteration 175/1000 | Loss: 0.00002786
Iteration 176/1000 | Loss: 0.00002785
Iteration 177/1000 | Loss: 0.00002785
Iteration 178/1000 | Loss: 0.00002785
Iteration 179/1000 | Loss: 0.00002785
Iteration 180/1000 | Loss: 0.00002785
Iteration 181/1000 | Loss: 0.00002785
Iteration 182/1000 | Loss: 0.00002785
Iteration 183/1000 | Loss: 0.00002785
Iteration 184/1000 | Loss: 0.00002785
Iteration 185/1000 | Loss: 0.00002785
Iteration 186/1000 | Loss: 0.00002785
Iteration 187/1000 | Loss: 0.00002785
Iteration 188/1000 | Loss: 0.00002785
Iteration 189/1000 | Loss: 0.00002785
Iteration 190/1000 | Loss: 0.00002785
Iteration 191/1000 | Loss: 0.00002785
Iteration 192/1000 | Loss: 0.00002785
Iteration 193/1000 | Loss: 0.00002785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.7845788281410933e-05, 2.7845788281410933e-05, 2.7845788281410933e-05, 2.7845788281410933e-05, 2.7845788281410933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7845788281410933e-05

Optimization complete. Final v2v error: 4.436430931091309 mm

Highest mean error: 5.349508762359619 mm for frame 0

Lowest mean error: 3.845001220703125 mm for frame 22

Saving results

Total time: 50.209619760513306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418210
Iteration 2/25 | Loss: 0.00119748
Iteration 3/25 | Loss: 0.00113472
Iteration 4/25 | Loss: 0.00112586
Iteration 5/25 | Loss: 0.00112296
Iteration 6/25 | Loss: 0.00112285
Iteration 7/25 | Loss: 0.00112285
Iteration 8/25 | Loss: 0.00112285
Iteration 9/25 | Loss: 0.00112285
Iteration 10/25 | Loss: 0.00112285
Iteration 11/25 | Loss: 0.00112285
Iteration 12/25 | Loss: 0.00112285
Iteration 13/25 | Loss: 0.00112285
Iteration 14/25 | Loss: 0.00112285
Iteration 15/25 | Loss: 0.00112285
Iteration 16/25 | Loss: 0.00112285
Iteration 17/25 | Loss: 0.00112285
Iteration 18/25 | Loss: 0.00112285
Iteration 19/25 | Loss: 0.00112285
Iteration 20/25 | Loss: 0.00112285
Iteration 21/25 | Loss: 0.00112285
Iteration 22/25 | Loss: 0.00112285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011228470830246806, 0.0011228470830246806, 0.0011228470830246806, 0.0011228470830246806, 0.0011228470830246806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011228470830246806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38575196
Iteration 2/25 | Loss: 0.00073817
Iteration 3/25 | Loss: 0.00073817
Iteration 4/25 | Loss: 0.00073816
Iteration 5/25 | Loss: 0.00073816
Iteration 6/25 | Loss: 0.00073816
Iteration 7/25 | Loss: 0.00073816
Iteration 8/25 | Loss: 0.00073816
Iteration 9/25 | Loss: 0.00073816
Iteration 10/25 | Loss: 0.00073816
Iteration 11/25 | Loss: 0.00073816
Iteration 12/25 | Loss: 0.00073816
Iteration 13/25 | Loss: 0.00073816
Iteration 14/25 | Loss: 0.00073816
Iteration 15/25 | Loss: 0.00073816
Iteration 16/25 | Loss: 0.00073816
Iteration 17/25 | Loss: 0.00073816
Iteration 18/25 | Loss: 0.00073816
Iteration 19/25 | Loss: 0.00073816
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007381624891422689, 0.0007381624891422689, 0.0007381624891422689, 0.0007381624891422689, 0.0007381624891422689]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007381624891422689

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073816
Iteration 2/1000 | Loss: 0.00002139
Iteration 3/1000 | Loss: 0.00001657
Iteration 4/1000 | Loss: 0.00001536
Iteration 5/1000 | Loss: 0.00001472
Iteration 6/1000 | Loss: 0.00001402
Iteration 7/1000 | Loss: 0.00001365
Iteration 8/1000 | Loss: 0.00001365
Iteration 9/1000 | Loss: 0.00001337
Iteration 10/1000 | Loss: 0.00001303
Iteration 11/1000 | Loss: 0.00001301
Iteration 12/1000 | Loss: 0.00001299
Iteration 13/1000 | Loss: 0.00001298
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001279
Iteration 16/1000 | Loss: 0.00001273
Iteration 17/1000 | Loss: 0.00001271
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001253
Iteration 20/1000 | Loss: 0.00001252
Iteration 21/1000 | Loss: 0.00001245
Iteration 22/1000 | Loss: 0.00001244
Iteration 23/1000 | Loss: 0.00001242
Iteration 24/1000 | Loss: 0.00001242
Iteration 25/1000 | Loss: 0.00001240
Iteration 26/1000 | Loss: 0.00001240
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001234
Iteration 29/1000 | Loss: 0.00001233
Iteration 30/1000 | Loss: 0.00001229
Iteration 31/1000 | Loss: 0.00001228
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001220
Iteration 34/1000 | Loss: 0.00001220
Iteration 35/1000 | Loss: 0.00001219
Iteration 36/1000 | Loss: 0.00001219
Iteration 37/1000 | Loss: 0.00001219
Iteration 38/1000 | Loss: 0.00001218
Iteration 39/1000 | Loss: 0.00001218
Iteration 40/1000 | Loss: 0.00001218
Iteration 41/1000 | Loss: 0.00001218
Iteration 42/1000 | Loss: 0.00001217
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001215
Iteration 46/1000 | Loss: 0.00001215
Iteration 47/1000 | Loss: 0.00001214
Iteration 48/1000 | Loss: 0.00001214
Iteration 49/1000 | Loss: 0.00001214
Iteration 50/1000 | Loss: 0.00001214
Iteration 51/1000 | Loss: 0.00001214
Iteration 52/1000 | Loss: 0.00001214
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001214
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00001212
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001212
Iteration 60/1000 | Loss: 0.00001211
Iteration 61/1000 | Loss: 0.00001211
Iteration 62/1000 | Loss: 0.00001211
Iteration 63/1000 | Loss: 0.00001211
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001210
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001209
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001206
Iteration 71/1000 | Loss: 0.00001206
Iteration 72/1000 | Loss: 0.00001206
Iteration 73/1000 | Loss: 0.00001205
Iteration 74/1000 | Loss: 0.00001205
Iteration 75/1000 | Loss: 0.00001204
Iteration 76/1000 | Loss: 0.00001204
Iteration 77/1000 | Loss: 0.00001204
Iteration 78/1000 | Loss: 0.00001204
Iteration 79/1000 | Loss: 0.00001203
Iteration 80/1000 | Loss: 0.00001203
Iteration 81/1000 | Loss: 0.00001203
Iteration 82/1000 | Loss: 0.00001203
Iteration 83/1000 | Loss: 0.00001203
Iteration 84/1000 | Loss: 0.00001203
Iteration 85/1000 | Loss: 0.00001203
Iteration 86/1000 | Loss: 0.00001203
Iteration 87/1000 | Loss: 0.00001203
Iteration 88/1000 | Loss: 0.00001203
Iteration 89/1000 | Loss: 0.00001203
Iteration 90/1000 | Loss: 0.00001203
Iteration 91/1000 | Loss: 0.00001203
Iteration 92/1000 | Loss: 0.00001203
Iteration 93/1000 | Loss: 0.00001203
Iteration 94/1000 | Loss: 0.00001203
Iteration 95/1000 | Loss: 0.00001203
Iteration 96/1000 | Loss: 0.00001203
Iteration 97/1000 | Loss: 0.00001203
Iteration 98/1000 | Loss: 0.00001203
Iteration 99/1000 | Loss: 0.00001203
Iteration 100/1000 | Loss: 0.00001203
Iteration 101/1000 | Loss: 0.00001203
Iteration 102/1000 | Loss: 0.00001203
Iteration 103/1000 | Loss: 0.00001203
Iteration 104/1000 | Loss: 0.00001203
Iteration 105/1000 | Loss: 0.00001203
Iteration 106/1000 | Loss: 0.00001203
Iteration 107/1000 | Loss: 0.00001203
Iteration 108/1000 | Loss: 0.00001203
Iteration 109/1000 | Loss: 0.00001203
Iteration 110/1000 | Loss: 0.00001203
Iteration 111/1000 | Loss: 0.00001203
Iteration 112/1000 | Loss: 0.00001203
Iteration 113/1000 | Loss: 0.00001203
Iteration 114/1000 | Loss: 0.00001203
Iteration 115/1000 | Loss: 0.00001203
Iteration 116/1000 | Loss: 0.00001203
Iteration 117/1000 | Loss: 0.00001203
Iteration 118/1000 | Loss: 0.00001203
Iteration 119/1000 | Loss: 0.00001203
Iteration 120/1000 | Loss: 0.00001203
Iteration 121/1000 | Loss: 0.00001203
Iteration 122/1000 | Loss: 0.00001203
Iteration 123/1000 | Loss: 0.00001203
Iteration 124/1000 | Loss: 0.00001203
Iteration 125/1000 | Loss: 0.00001203
Iteration 126/1000 | Loss: 0.00001203
Iteration 127/1000 | Loss: 0.00001203
Iteration 128/1000 | Loss: 0.00001203
Iteration 129/1000 | Loss: 0.00001203
Iteration 130/1000 | Loss: 0.00001203
Iteration 131/1000 | Loss: 0.00001203
Iteration 132/1000 | Loss: 0.00001203
Iteration 133/1000 | Loss: 0.00001203
Iteration 134/1000 | Loss: 0.00001203
Iteration 135/1000 | Loss: 0.00001203
Iteration 136/1000 | Loss: 0.00001203
Iteration 137/1000 | Loss: 0.00001203
Iteration 138/1000 | Loss: 0.00001203
Iteration 139/1000 | Loss: 0.00001203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.2029350727971178e-05, 1.2029350727971178e-05, 1.2029350727971178e-05, 1.2029350727971178e-05, 1.2029350727971178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2029350727971178e-05

Optimization complete. Final v2v error: 2.9943816661834717 mm

Highest mean error: 3.170318126678467 mm for frame 5

Lowest mean error: 2.842200994491577 mm for frame 135

Saving results

Total time: 35.249117851257324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944166
Iteration 2/25 | Loss: 0.00236192
Iteration 3/25 | Loss: 0.00173864
Iteration 4/25 | Loss: 0.00163300
Iteration 5/25 | Loss: 0.00147420
Iteration 6/25 | Loss: 0.00144269
Iteration 7/25 | Loss: 0.00143356
Iteration 8/25 | Loss: 0.00141160
Iteration 9/25 | Loss: 0.00143148
Iteration 10/25 | Loss: 0.00138042
Iteration 11/25 | Loss: 0.00133929
Iteration 12/25 | Loss: 0.00130402
Iteration 13/25 | Loss: 0.00129023
Iteration 14/25 | Loss: 0.00128711
Iteration 15/25 | Loss: 0.00128633
Iteration 16/25 | Loss: 0.00129116
Iteration 17/25 | Loss: 0.00128773
Iteration 18/25 | Loss: 0.00128576
Iteration 19/25 | Loss: 0.00128575
Iteration 20/25 | Loss: 0.00128575
Iteration 21/25 | Loss: 0.00128575
Iteration 22/25 | Loss: 0.00128570
Iteration 23/25 | Loss: 0.00128565
Iteration 24/25 | Loss: 0.00128565
Iteration 25/25 | Loss: 0.00128565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34747767
Iteration 2/25 | Loss: 0.00196521
Iteration 3/25 | Loss: 0.00196521
Iteration 4/25 | Loss: 0.00196521
Iteration 5/25 | Loss: 0.00196521
Iteration 6/25 | Loss: 0.00196521
Iteration 7/25 | Loss: 0.00196521
Iteration 8/25 | Loss: 0.00196521
Iteration 9/25 | Loss: 0.00196521
Iteration 10/25 | Loss: 0.00196521
Iteration 11/25 | Loss: 0.00196521
Iteration 12/25 | Loss: 0.00196521
Iteration 13/25 | Loss: 0.00196521
Iteration 14/25 | Loss: 0.00196521
Iteration 15/25 | Loss: 0.00196521
Iteration 16/25 | Loss: 0.00196521
Iteration 17/25 | Loss: 0.00196521
Iteration 18/25 | Loss: 0.00196521
Iteration 19/25 | Loss: 0.00196521
Iteration 20/25 | Loss: 0.00196521
Iteration 21/25 | Loss: 0.00196521
Iteration 22/25 | Loss: 0.00196521
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0019652058836072683, 0.0019652058836072683, 0.0019652058836072683, 0.0019652058836072683, 0.0019652058836072683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019652058836072683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196521
Iteration 2/1000 | Loss: 0.00031555
Iteration 3/1000 | Loss: 0.00012507
Iteration 4/1000 | Loss: 0.00017792
Iteration 5/1000 | Loss: 0.00025239
Iteration 6/1000 | Loss: 0.00016983
Iteration 7/1000 | Loss: 0.00029092
Iteration 8/1000 | Loss: 0.00010929
Iteration 9/1000 | Loss: 0.00009095
Iteration 10/1000 | Loss: 0.00008410
Iteration 11/1000 | Loss: 0.00008080
Iteration 12/1000 | Loss: 0.00007874
Iteration 13/1000 | Loss: 0.00007599
Iteration 14/1000 | Loss: 0.00043241
Iteration 15/1000 | Loss: 0.00008075
Iteration 16/1000 | Loss: 0.00031396
Iteration 17/1000 | Loss: 0.00007377
Iteration 18/1000 | Loss: 0.00018082
Iteration 19/1000 | Loss: 0.00007188
Iteration 20/1000 | Loss: 0.00006782
Iteration 21/1000 | Loss: 0.00006569
Iteration 22/1000 | Loss: 0.00006430
Iteration 23/1000 | Loss: 0.00006255
Iteration 24/1000 | Loss: 0.00006162
Iteration 25/1000 | Loss: 0.00072671
Iteration 26/1000 | Loss: 0.00006956
Iteration 27/1000 | Loss: 0.00037741
Iteration 28/1000 | Loss: 0.00043300
Iteration 29/1000 | Loss: 0.00026137
Iteration 30/1000 | Loss: 0.00042891
Iteration 31/1000 | Loss: 0.00036244
Iteration 32/1000 | Loss: 0.00013743
Iteration 33/1000 | Loss: 0.00006325
Iteration 34/1000 | Loss: 0.00028035
Iteration 35/1000 | Loss: 0.00008888
Iteration 36/1000 | Loss: 0.00033461
Iteration 37/1000 | Loss: 0.00071895
Iteration 38/1000 | Loss: 0.00043617
Iteration 39/1000 | Loss: 0.00058303
Iteration 40/1000 | Loss: 0.00071504
Iteration 41/1000 | Loss: 0.00055799
Iteration 42/1000 | Loss: 0.00006327
Iteration 43/1000 | Loss: 0.00005803
Iteration 44/1000 | Loss: 0.00005477
Iteration 45/1000 | Loss: 0.00008350
Iteration 46/1000 | Loss: 0.00005562
Iteration 47/1000 | Loss: 0.00007657
Iteration 48/1000 | Loss: 0.00005137
Iteration 49/1000 | Loss: 0.00005067
Iteration 50/1000 | Loss: 0.00005759
Iteration 51/1000 | Loss: 0.00005047
Iteration 52/1000 | Loss: 0.00004978
Iteration 53/1000 | Loss: 0.00004953
Iteration 54/1000 | Loss: 0.00004951
Iteration 55/1000 | Loss: 0.00004940
Iteration 56/1000 | Loss: 0.00005889
Iteration 57/1000 | Loss: 0.00004911
Iteration 58/1000 | Loss: 0.00004896
Iteration 59/1000 | Loss: 0.00004891
Iteration 60/1000 | Loss: 0.00004888
Iteration 61/1000 | Loss: 0.00004883
Iteration 62/1000 | Loss: 0.00004879
Iteration 63/1000 | Loss: 0.00004879
Iteration 64/1000 | Loss: 0.00004877
Iteration 65/1000 | Loss: 0.00004877
Iteration 66/1000 | Loss: 0.00004877
Iteration 67/1000 | Loss: 0.00004876
Iteration 68/1000 | Loss: 0.00004876
Iteration 69/1000 | Loss: 0.00004876
Iteration 70/1000 | Loss: 0.00004876
Iteration 71/1000 | Loss: 0.00004876
Iteration 72/1000 | Loss: 0.00004875
Iteration 73/1000 | Loss: 0.00004875
Iteration 74/1000 | Loss: 0.00004875
Iteration 75/1000 | Loss: 0.00004875
Iteration 76/1000 | Loss: 0.00004875
Iteration 77/1000 | Loss: 0.00004874
Iteration 78/1000 | Loss: 0.00004874
Iteration 79/1000 | Loss: 0.00004873
Iteration 80/1000 | Loss: 0.00004873
Iteration 81/1000 | Loss: 0.00004872
Iteration 82/1000 | Loss: 0.00004872
Iteration 83/1000 | Loss: 0.00004872
Iteration 84/1000 | Loss: 0.00004872
Iteration 85/1000 | Loss: 0.00004871
Iteration 86/1000 | Loss: 0.00004871
Iteration 87/1000 | Loss: 0.00004871
Iteration 88/1000 | Loss: 0.00004870
Iteration 89/1000 | Loss: 0.00004870
Iteration 90/1000 | Loss: 0.00004870
Iteration 91/1000 | Loss: 0.00004869
Iteration 92/1000 | Loss: 0.00004869
Iteration 93/1000 | Loss: 0.00004869
Iteration 94/1000 | Loss: 0.00004869
Iteration 95/1000 | Loss: 0.00004868
Iteration 96/1000 | Loss: 0.00004868
Iteration 97/1000 | Loss: 0.00004868
Iteration 98/1000 | Loss: 0.00004868
Iteration 99/1000 | Loss: 0.00004868
Iteration 100/1000 | Loss: 0.00004867
Iteration 101/1000 | Loss: 0.00004867
Iteration 102/1000 | Loss: 0.00004867
Iteration 103/1000 | Loss: 0.00004867
Iteration 104/1000 | Loss: 0.00004866
Iteration 105/1000 | Loss: 0.00004866
Iteration 106/1000 | Loss: 0.00004866
Iteration 107/1000 | Loss: 0.00004865
Iteration 108/1000 | Loss: 0.00004865
Iteration 109/1000 | Loss: 0.00004865
Iteration 110/1000 | Loss: 0.00004865
Iteration 111/1000 | Loss: 0.00004864
Iteration 112/1000 | Loss: 0.00004864
Iteration 113/1000 | Loss: 0.00004864
Iteration 114/1000 | Loss: 0.00004864
Iteration 115/1000 | Loss: 0.00004864
Iteration 116/1000 | Loss: 0.00004864
Iteration 117/1000 | Loss: 0.00004863
Iteration 118/1000 | Loss: 0.00004863
Iteration 119/1000 | Loss: 0.00004863
Iteration 120/1000 | Loss: 0.00004862
Iteration 121/1000 | Loss: 0.00004862
Iteration 122/1000 | Loss: 0.00004862
Iteration 123/1000 | Loss: 0.00004862
Iteration 124/1000 | Loss: 0.00004861
Iteration 125/1000 | Loss: 0.00004861
Iteration 126/1000 | Loss: 0.00004861
Iteration 127/1000 | Loss: 0.00004861
Iteration 128/1000 | Loss: 0.00004861
Iteration 129/1000 | Loss: 0.00004861
Iteration 130/1000 | Loss: 0.00004861
Iteration 131/1000 | Loss: 0.00004860
Iteration 132/1000 | Loss: 0.00004860
Iteration 133/1000 | Loss: 0.00004860
Iteration 134/1000 | Loss: 0.00004860
Iteration 135/1000 | Loss: 0.00004860
Iteration 136/1000 | Loss: 0.00004860
Iteration 137/1000 | Loss: 0.00004860
Iteration 138/1000 | Loss: 0.00004859
Iteration 139/1000 | Loss: 0.00004859
Iteration 140/1000 | Loss: 0.00004859
Iteration 141/1000 | Loss: 0.00004859
Iteration 142/1000 | Loss: 0.00004858
Iteration 143/1000 | Loss: 0.00004858
Iteration 144/1000 | Loss: 0.00004858
Iteration 145/1000 | Loss: 0.00004858
Iteration 146/1000 | Loss: 0.00055555
Iteration 147/1000 | Loss: 0.00097134
Iteration 148/1000 | Loss: 0.00040247
Iteration 149/1000 | Loss: 0.00066620
Iteration 150/1000 | Loss: 0.00007064
Iteration 151/1000 | Loss: 0.00018785
Iteration 152/1000 | Loss: 0.00005177
Iteration 153/1000 | Loss: 0.00022148
Iteration 154/1000 | Loss: 0.00034232
Iteration 155/1000 | Loss: 0.00006988
Iteration 156/1000 | Loss: 0.00005316
Iteration 157/1000 | Loss: 0.00004690
Iteration 158/1000 | Loss: 0.00004587
Iteration 159/1000 | Loss: 0.00056769
Iteration 160/1000 | Loss: 0.00153232
Iteration 161/1000 | Loss: 0.00076689
Iteration 162/1000 | Loss: 0.00055904
Iteration 163/1000 | Loss: 0.00044432
Iteration 164/1000 | Loss: 0.00008188
Iteration 165/1000 | Loss: 0.00013236
Iteration 166/1000 | Loss: 0.00007389
Iteration 167/1000 | Loss: 0.00007390
Iteration 168/1000 | Loss: 0.00007245
Iteration 169/1000 | Loss: 0.00005060
Iteration 170/1000 | Loss: 0.00004285
Iteration 171/1000 | Loss: 0.00004189
Iteration 172/1000 | Loss: 0.00040865
Iteration 173/1000 | Loss: 0.00050521
Iteration 174/1000 | Loss: 0.00007942
Iteration 175/1000 | Loss: 0.00005516
Iteration 176/1000 | Loss: 0.00004709
Iteration 177/1000 | Loss: 0.00004166
Iteration 178/1000 | Loss: 0.00003923
Iteration 179/1000 | Loss: 0.00003747
Iteration 180/1000 | Loss: 0.00003655
Iteration 181/1000 | Loss: 0.00100247
Iteration 182/1000 | Loss: 0.00004205
Iteration 183/1000 | Loss: 0.00003626
Iteration 184/1000 | Loss: 0.00003426
Iteration 185/1000 | Loss: 0.00003169
Iteration 186/1000 | Loss: 0.00003016
Iteration 187/1000 | Loss: 0.00002903
Iteration 188/1000 | Loss: 0.00002852
Iteration 189/1000 | Loss: 0.00002823
Iteration 190/1000 | Loss: 0.00002788
Iteration 191/1000 | Loss: 0.00002774
Iteration 192/1000 | Loss: 0.00002771
Iteration 193/1000 | Loss: 0.00002749
Iteration 194/1000 | Loss: 0.00002744
Iteration 195/1000 | Loss: 0.00057987
Iteration 196/1000 | Loss: 0.00057491
Iteration 197/1000 | Loss: 0.00052913
Iteration 198/1000 | Loss: 0.00003402
Iteration 199/1000 | Loss: 0.00002735
Iteration 200/1000 | Loss: 0.00002642
Iteration 201/1000 | Loss: 0.00002565
Iteration 202/1000 | Loss: 0.00002508
Iteration 203/1000 | Loss: 0.00002461
Iteration 204/1000 | Loss: 0.00002434
Iteration 205/1000 | Loss: 0.00002418
Iteration 206/1000 | Loss: 0.00002410
Iteration 207/1000 | Loss: 0.00002409
Iteration 208/1000 | Loss: 0.00002409
Iteration 209/1000 | Loss: 0.00002408
Iteration 210/1000 | Loss: 0.00002406
Iteration 211/1000 | Loss: 0.00002405
Iteration 212/1000 | Loss: 0.00002405
Iteration 213/1000 | Loss: 0.00002404
Iteration 214/1000 | Loss: 0.00002404
Iteration 215/1000 | Loss: 0.00002403
Iteration 216/1000 | Loss: 0.00002402
Iteration 217/1000 | Loss: 0.00002401
Iteration 218/1000 | Loss: 0.00002401
Iteration 219/1000 | Loss: 0.00002401
Iteration 220/1000 | Loss: 0.00002400
Iteration 221/1000 | Loss: 0.00002399
Iteration 222/1000 | Loss: 0.00002399
Iteration 223/1000 | Loss: 0.00002397
Iteration 224/1000 | Loss: 0.00002396
Iteration 225/1000 | Loss: 0.00002395
Iteration 226/1000 | Loss: 0.00002395
Iteration 227/1000 | Loss: 0.00002395
Iteration 228/1000 | Loss: 0.00002394
Iteration 229/1000 | Loss: 0.00002394
Iteration 230/1000 | Loss: 0.00002394
Iteration 231/1000 | Loss: 0.00002394
Iteration 232/1000 | Loss: 0.00002394
Iteration 233/1000 | Loss: 0.00002393
Iteration 234/1000 | Loss: 0.00002393
Iteration 235/1000 | Loss: 0.00002393
Iteration 236/1000 | Loss: 0.00002393
Iteration 237/1000 | Loss: 0.00002392
Iteration 238/1000 | Loss: 0.00002392
Iteration 239/1000 | Loss: 0.00002392
Iteration 240/1000 | Loss: 0.00002391
Iteration 241/1000 | Loss: 0.00002390
Iteration 242/1000 | Loss: 0.00002390
Iteration 243/1000 | Loss: 0.00002390
Iteration 244/1000 | Loss: 0.00002389
Iteration 245/1000 | Loss: 0.00002389
Iteration 246/1000 | Loss: 0.00002389
Iteration 247/1000 | Loss: 0.00002389
Iteration 248/1000 | Loss: 0.00002388
Iteration 249/1000 | Loss: 0.00002388
Iteration 250/1000 | Loss: 0.00002388
Iteration 251/1000 | Loss: 0.00002388
Iteration 252/1000 | Loss: 0.00002388
Iteration 253/1000 | Loss: 0.00002388
Iteration 254/1000 | Loss: 0.00002387
Iteration 255/1000 | Loss: 0.00002387
Iteration 256/1000 | Loss: 0.00002387
Iteration 257/1000 | Loss: 0.00002387
Iteration 258/1000 | Loss: 0.00002387
Iteration 259/1000 | Loss: 0.00002387
Iteration 260/1000 | Loss: 0.00002386
Iteration 261/1000 | Loss: 0.00002386
Iteration 262/1000 | Loss: 0.00002386
Iteration 263/1000 | Loss: 0.00002386
Iteration 264/1000 | Loss: 0.00002386
Iteration 265/1000 | Loss: 0.00002386
Iteration 266/1000 | Loss: 0.00002386
Iteration 267/1000 | Loss: 0.00002386
Iteration 268/1000 | Loss: 0.00002386
Iteration 269/1000 | Loss: 0.00002386
Iteration 270/1000 | Loss: 0.00002386
Iteration 271/1000 | Loss: 0.00002386
Iteration 272/1000 | Loss: 0.00002386
Iteration 273/1000 | Loss: 0.00002386
Iteration 274/1000 | Loss: 0.00002386
Iteration 275/1000 | Loss: 0.00002386
Iteration 276/1000 | Loss: 0.00002386
Iteration 277/1000 | Loss: 0.00002386
Iteration 278/1000 | Loss: 0.00002386
Iteration 279/1000 | Loss: 0.00002386
Iteration 280/1000 | Loss: 0.00002386
Iteration 281/1000 | Loss: 0.00002386
Iteration 282/1000 | Loss: 0.00002386
Iteration 283/1000 | Loss: 0.00002386
Iteration 284/1000 | Loss: 0.00002386
Iteration 285/1000 | Loss: 0.00002386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [2.3864833565312438e-05, 2.3864833565312438e-05, 2.3864833565312438e-05, 2.3864833565312438e-05, 2.3864833565312438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3864833565312438e-05

Optimization complete. Final v2v error: 3.7592225074768066 mm

Highest mean error: 10.497676849365234 mm for frame 21

Lowest mean error: 2.514202833175659 mm for frame 11

Saving results

Total time: 202.7527003288269
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471239
Iteration 2/25 | Loss: 0.00135968
Iteration 3/25 | Loss: 0.00125504
Iteration 4/25 | Loss: 0.00124666
Iteration 5/25 | Loss: 0.00124414
Iteration 6/25 | Loss: 0.00124414
Iteration 7/25 | Loss: 0.00124414
Iteration 8/25 | Loss: 0.00124414
Iteration 9/25 | Loss: 0.00124414
Iteration 10/25 | Loss: 0.00124414
Iteration 11/25 | Loss: 0.00124414
Iteration 12/25 | Loss: 0.00124414
Iteration 13/25 | Loss: 0.00124414
Iteration 14/25 | Loss: 0.00124414
Iteration 15/25 | Loss: 0.00124414
Iteration 16/25 | Loss: 0.00124414
Iteration 17/25 | Loss: 0.00124414
Iteration 18/25 | Loss: 0.00124414
Iteration 19/25 | Loss: 0.00124414
Iteration 20/25 | Loss: 0.00124414
Iteration 21/25 | Loss: 0.00124414
Iteration 22/25 | Loss: 0.00124414
Iteration 23/25 | Loss: 0.00124414
Iteration 24/25 | Loss: 0.00124414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001244135550223291, 0.001244135550223291, 0.001244135550223291, 0.001244135550223291, 0.001244135550223291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001244135550223291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34321260
Iteration 2/25 | Loss: 0.00106226
Iteration 3/25 | Loss: 0.00106224
Iteration 4/25 | Loss: 0.00106224
Iteration 5/25 | Loss: 0.00106224
Iteration 6/25 | Loss: 0.00106224
Iteration 7/25 | Loss: 0.00106224
Iteration 8/25 | Loss: 0.00106224
Iteration 9/25 | Loss: 0.00106224
Iteration 10/25 | Loss: 0.00106224
Iteration 11/25 | Loss: 0.00106224
Iteration 12/25 | Loss: 0.00106224
Iteration 13/25 | Loss: 0.00106224
Iteration 14/25 | Loss: 0.00106224
Iteration 15/25 | Loss: 0.00106224
Iteration 16/25 | Loss: 0.00106224
Iteration 17/25 | Loss: 0.00106224
Iteration 18/25 | Loss: 0.00106224
Iteration 19/25 | Loss: 0.00106224
Iteration 20/25 | Loss: 0.00106224
Iteration 21/25 | Loss: 0.00106224
Iteration 22/25 | Loss: 0.00106224
Iteration 23/25 | Loss: 0.00106224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010622410336509347, 0.0010622410336509347, 0.0010622410336509347, 0.0010622410336509347, 0.0010622410336509347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010622410336509347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106224
Iteration 2/1000 | Loss: 0.00003542
Iteration 3/1000 | Loss: 0.00002316
Iteration 4/1000 | Loss: 0.00002052
Iteration 5/1000 | Loss: 0.00001926
Iteration 6/1000 | Loss: 0.00001836
Iteration 7/1000 | Loss: 0.00001767
Iteration 8/1000 | Loss: 0.00001731
Iteration 9/1000 | Loss: 0.00001698
Iteration 10/1000 | Loss: 0.00001678
Iteration 11/1000 | Loss: 0.00001663
Iteration 12/1000 | Loss: 0.00001645
Iteration 13/1000 | Loss: 0.00001644
Iteration 14/1000 | Loss: 0.00001643
Iteration 15/1000 | Loss: 0.00001636
Iteration 16/1000 | Loss: 0.00001627
Iteration 17/1000 | Loss: 0.00001623
Iteration 18/1000 | Loss: 0.00001621
Iteration 19/1000 | Loss: 0.00001620
Iteration 20/1000 | Loss: 0.00001617
Iteration 21/1000 | Loss: 0.00001610
Iteration 22/1000 | Loss: 0.00001607
Iteration 23/1000 | Loss: 0.00001606
Iteration 24/1000 | Loss: 0.00001605
Iteration 25/1000 | Loss: 0.00001604
Iteration 26/1000 | Loss: 0.00001604
Iteration 27/1000 | Loss: 0.00001602
Iteration 28/1000 | Loss: 0.00001602
Iteration 29/1000 | Loss: 0.00001602
Iteration 30/1000 | Loss: 0.00001602
Iteration 31/1000 | Loss: 0.00001601
Iteration 32/1000 | Loss: 0.00001601
Iteration 33/1000 | Loss: 0.00001601
Iteration 34/1000 | Loss: 0.00001599
Iteration 35/1000 | Loss: 0.00001599
Iteration 36/1000 | Loss: 0.00001598
Iteration 37/1000 | Loss: 0.00001598
Iteration 38/1000 | Loss: 0.00001597
Iteration 39/1000 | Loss: 0.00001596
Iteration 40/1000 | Loss: 0.00001596
Iteration 41/1000 | Loss: 0.00001596
Iteration 42/1000 | Loss: 0.00001595
Iteration 43/1000 | Loss: 0.00001592
Iteration 44/1000 | Loss: 0.00001592
Iteration 45/1000 | Loss: 0.00001591
Iteration 46/1000 | Loss: 0.00001591
Iteration 47/1000 | Loss: 0.00001590
Iteration 48/1000 | Loss: 0.00001589
Iteration 49/1000 | Loss: 0.00001589
Iteration 50/1000 | Loss: 0.00001588
Iteration 51/1000 | Loss: 0.00001587
Iteration 52/1000 | Loss: 0.00001587
Iteration 53/1000 | Loss: 0.00001586
Iteration 54/1000 | Loss: 0.00001586
Iteration 55/1000 | Loss: 0.00001585
Iteration 56/1000 | Loss: 0.00001585
Iteration 57/1000 | Loss: 0.00001583
Iteration 58/1000 | Loss: 0.00001583
Iteration 59/1000 | Loss: 0.00001581
Iteration 60/1000 | Loss: 0.00001579
Iteration 61/1000 | Loss: 0.00001579
Iteration 62/1000 | Loss: 0.00001579
Iteration 63/1000 | Loss: 0.00001578
Iteration 64/1000 | Loss: 0.00001578
Iteration 65/1000 | Loss: 0.00001578
Iteration 66/1000 | Loss: 0.00001576
Iteration 67/1000 | Loss: 0.00001575
Iteration 68/1000 | Loss: 0.00001575
Iteration 69/1000 | Loss: 0.00001575
Iteration 70/1000 | Loss: 0.00001574
Iteration 71/1000 | Loss: 0.00001574
Iteration 72/1000 | Loss: 0.00001574
Iteration 73/1000 | Loss: 0.00001573
Iteration 74/1000 | Loss: 0.00001573
Iteration 75/1000 | Loss: 0.00001573
Iteration 76/1000 | Loss: 0.00001569
Iteration 77/1000 | Loss: 0.00001569
Iteration 78/1000 | Loss: 0.00001569
Iteration 79/1000 | Loss: 0.00001566
Iteration 80/1000 | Loss: 0.00001565
Iteration 81/1000 | Loss: 0.00001564
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001564
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001564
Iteration 86/1000 | Loss: 0.00001564
Iteration 87/1000 | Loss: 0.00001564
Iteration 88/1000 | Loss: 0.00001564
Iteration 89/1000 | Loss: 0.00001564
Iteration 90/1000 | Loss: 0.00001564
Iteration 91/1000 | Loss: 0.00001564
Iteration 92/1000 | Loss: 0.00001564
Iteration 93/1000 | Loss: 0.00001564
Iteration 94/1000 | Loss: 0.00001564
Iteration 95/1000 | Loss: 0.00001564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.5640178389730863e-05, 1.5640178389730863e-05, 1.5640178389730863e-05, 1.5640178389730863e-05, 1.5640178389730863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5640178389730863e-05

Optimization complete. Final v2v error: 3.326944589614868 mm

Highest mean error: 3.7258150577545166 mm for frame 66

Lowest mean error: 2.94042706489563 mm for frame 189

Saving results

Total time: 42.17004871368408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775798
Iteration 2/25 | Loss: 0.00179789
Iteration 3/25 | Loss: 0.00135390
Iteration 4/25 | Loss: 0.00130035
Iteration 5/25 | Loss: 0.00128498
Iteration 6/25 | Loss: 0.00127305
Iteration 7/25 | Loss: 0.00126844
Iteration 8/25 | Loss: 0.00124891
Iteration 9/25 | Loss: 0.00124213
Iteration 10/25 | Loss: 0.00124147
Iteration 11/25 | Loss: 0.00124122
Iteration 12/25 | Loss: 0.00124117
Iteration 13/25 | Loss: 0.00124117
Iteration 14/25 | Loss: 0.00124117
Iteration 15/25 | Loss: 0.00124117
Iteration 16/25 | Loss: 0.00124117
Iteration 17/25 | Loss: 0.00124117
Iteration 18/25 | Loss: 0.00124117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001241173129528761, 0.001241173129528761, 0.001241173129528761, 0.001241173129528761, 0.001241173129528761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001241173129528761

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56839693
Iteration 2/25 | Loss: 0.00073440
Iteration 3/25 | Loss: 0.00073439
Iteration 4/25 | Loss: 0.00073439
Iteration 5/25 | Loss: 0.00073438
Iteration 6/25 | Loss: 0.00073438
Iteration 7/25 | Loss: 0.00073438
Iteration 8/25 | Loss: 0.00073438
Iteration 9/25 | Loss: 0.00073438
Iteration 10/25 | Loss: 0.00073438
Iteration 11/25 | Loss: 0.00073438
Iteration 12/25 | Loss: 0.00073438
Iteration 13/25 | Loss: 0.00073438
Iteration 14/25 | Loss: 0.00073438
Iteration 15/25 | Loss: 0.00073438
Iteration 16/25 | Loss: 0.00073438
Iteration 17/25 | Loss: 0.00073438
Iteration 18/25 | Loss: 0.00073438
Iteration 19/25 | Loss: 0.00073438
Iteration 20/25 | Loss: 0.00073438
Iteration 21/25 | Loss: 0.00073438
Iteration 22/25 | Loss: 0.00073438
Iteration 23/25 | Loss: 0.00073438
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007343824254348874, 0.0007343824254348874, 0.0007343824254348874, 0.0007343824254348874, 0.0007343824254348874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007343824254348874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073438
Iteration 2/1000 | Loss: 0.00004801
Iteration 3/1000 | Loss: 0.00003169
Iteration 4/1000 | Loss: 0.00002797
Iteration 5/1000 | Loss: 0.00002664
Iteration 6/1000 | Loss: 0.00002552
Iteration 7/1000 | Loss: 0.00002485
Iteration 8/1000 | Loss: 0.00002454
Iteration 9/1000 | Loss: 0.00002424
Iteration 10/1000 | Loss: 0.00002396
Iteration 11/1000 | Loss: 0.00002381
Iteration 12/1000 | Loss: 0.00002367
Iteration 13/1000 | Loss: 0.00002364
Iteration 14/1000 | Loss: 0.00002364
Iteration 15/1000 | Loss: 0.00002355
Iteration 16/1000 | Loss: 0.00002347
Iteration 17/1000 | Loss: 0.00002343
Iteration 18/1000 | Loss: 0.00002342
Iteration 19/1000 | Loss: 0.00002341
Iteration 20/1000 | Loss: 0.00002341
Iteration 21/1000 | Loss: 0.00002340
Iteration 22/1000 | Loss: 0.00002340
Iteration 23/1000 | Loss: 0.00002339
Iteration 24/1000 | Loss: 0.00002338
Iteration 25/1000 | Loss: 0.00002337
Iteration 26/1000 | Loss: 0.00002337
Iteration 27/1000 | Loss: 0.00002336
Iteration 28/1000 | Loss: 0.00002330
Iteration 29/1000 | Loss: 0.00002329
Iteration 30/1000 | Loss: 0.00002328
Iteration 31/1000 | Loss: 0.00002328
Iteration 32/1000 | Loss: 0.00002327
Iteration 33/1000 | Loss: 0.00002327
Iteration 34/1000 | Loss: 0.00002327
Iteration 35/1000 | Loss: 0.00002327
Iteration 36/1000 | Loss: 0.00002327
Iteration 37/1000 | Loss: 0.00002326
Iteration 38/1000 | Loss: 0.00002326
Iteration 39/1000 | Loss: 0.00002326
Iteration 40/1000 | Loss: 0.00002326
Iteration 41/1000 | Loss: 0.00002325
Iteration 42/1000 | Loss: 0.00002325
Iteration 43/1000 | Loss: 0.00002325
Iteration 44/1000 | Loss: 0.00002325
Iteration 45/1000 | Loss: 0.00002325
Iteration 46/1000 | Loss: 0.00002325
Iteration 47/1000 | Loss: 0.00002325
Iteration 48/1000 | Loss: 0.00002324
Iteration 49/1000 | Loss: 0.00002324
Iteration 50/1000 | Loss: 0.00002324
Iteration 51/1000 | Loss: 0.00002324
Iteration 52/1000 | Loss: 0.00002323
Iteration 53/1000 | Loss: 0.00002323
Iteration 54/1000 | Loss: 0.00002323
Iteration 55/1000 | Loss: 0.00002323
Iteration 56/1000 | Loss: 0.00002323
Iteration 57/1000 | Loss: 0.00002323
Iteration 58/1000 | Loss: 0.00002323
Iteration 59/1000 | Loss: 0.00002323
Iteration 60/1000 | Loss: 0.00002323
Iteration 61/1000 | Loss: 0.00002323
Iteration 62/1000 | Loss: 0.00002322
Iteration 63/1000 | Loss: 0.00002322
Iteration 64/1000 | Loss: 0.00002322
Iteration 65/1000 | Loss: 0.00002322
Iteration 66/1000 | Loss: 0.00002322
Iteration 67/1000 | Loss: 0.00002322
Iteration 68/1000 | Loss: 0.00002322
Iteration 69/1000 | Loss: 0.00002321
Iteration 70/1000 | Loss: 0.00002321
Iteration 71/1000 | Loss: 0.00002321
Iteration 72/1000 | Loss: 0.00002321
Iteration 73/1000 | Loss: 0.00002321
Iteration 74/1000 | Loss: 0.00002320
Iteration 75/1000 | Loss: 0.00002320
Iteration 76/1000 | Loss: 0.00002320
Iteration 77/1000 | Loss: 0.00002320
Iteration 78/1000 | Loss: 0.00002319
Iteration 79/1000 | Loss: 0.00002319
Iteration 80/1000 | Loss: 0.00002319
Iteration 81/1000 | Loss: 0.00002319
Iteration 82/1000 | Loss: 0.00002319
Iteration 83/1000 | Loss: 0.00002319
Iteration 84/1000 | Loss: 0.00002319
Iteration 85/1000 | Loss: 0.00002319
Iteration 86/1000 | Loss: 0.00002319
Iteration 87/1000 | Loss: 0.00002318
Iteration 88/1000 | Loss: 0.00002318
Iteration 89/1000 | Loss: 0.00002318
Iteration 90/1000 | Loss: 0.00002318
Iteration 91/1000 | Loss: 0.00002317
Iteration 92/1000 | Loss: 0.00002317
Iteration 93/1000 | Loss: 0.00002317
Iteration 94/1000 | Loss: 0.00002317
Iteration 95/1000 | Loss: 0.00002317
Iteration 96/1000 | Loss: 0.00002317
Iteration 97/1000 | Loss: 0.00002317
Iteration 98/1000 | Loss: 0.00002316
Iteration 99/1000 | Loss: 0.00002316
Iteration 100/1000 | Loss: 0.00002316
Iteration 101/1000 | Loss: 0.00002316
Iteration 102/1000 | Loss: 0.00002315
Iteration 103/1000 | Loss: 0.00002315
Iteration 104/1000 | Loss: 0.00002315
Iteration 105/1000 | Loss: 0.00002315
Iteration 106/1000 | Loss: 0.00002315
Iteration 107/1000 | Loss: 0.00002315
Iteration 108/1000 | Loss: 0.00002315
Iteration 109/1000 | Loss: 0.00002315
Iteration 110/1000 | Loss: 0.00002315
Iteration 111/1000 | Loss: 0.00002314
Iteration 112/1000 | Loss: 0.00002314
Iteration 113/1000 | Loss: 0.00002314
Iteration 114/1000 | Loss: 0.00002314
Iteration 115/1000 | Loss: 0.00002314
Iteration 116/1000 | Loss: 0.00002313
Iteration 117/1000 | Loss: 0.00002313
Iteration 118/1000 | Loss: 0.00002313
Iteration 119/1000 | Loss: 0.00002313
Iteration 120/1000 | Loss: 0.00002313
Iteration 121/1000 | Loss: 0.00002312
Iteration 122/1000 | Loss: 0.00002312
Iteration 123/1000 | Loss: 0.00002312
Iteration 124/1000 | Loss: 0.00002312
Iteration 125/1000 | Loss: 0.00002312
Iteration 126/1000 | Loss: 0.00002312
Iteration 127/1000 | Loss: 0.00002312
Iteration 128/1000 | Loss: 0.00002311
Iteration 129/1000 | Loss: 0.00002311
Iteration 130/1000 | Loss: 0.00002311
Iteration 131/1000 | Loss: 0.00002310
Iteration 132/1000 | Loss: 0.00002310
Iteration 133/1000 | Loss: 0.00002310
Iteration 134/1000 | Loss: 0.00002310
Iteration 135/1000 | Loss: 0.00002310
Iteration 136/1000 | Loss: 0.00002310
Iteration 137/1000 | Loss: 0.00002309
Iteration 138/1000 | Loss: 0.00002309
Iteration 139/1000 | Loss: 0.00002309
Iteration 140/1000 | Loss: 0.00002309
Iteration 141/1000 | Loss: 0.00002309
Iteration 142/1000 | Loss: 0.00002309
Iteration 143/1000 | Loss: 0.00002309
Iteration 144/1000 | Loss: 0.00002309
Iteration 145/1000 | Loss: 0.00002309
Iteration 146/1000 | Loss: 0.00002309
Iteration 147/1000 | Loss: 0.00002309
Iteration 148/1000 | Loss: 0.00002309
Iteration 149/1000 | Loss: 0.00002309
Iteration 150/1000 | Loss: 0.00002309
Iteration 151/1000 | Loss: 0.00002309
Iteration 152/1000 | Loss: 0.00002309
Iteration 153/1000 | Loss: 0.00002309
Iteration 154/1000 | Loss: 0.00002309
Iteration 155/1000 | Loss: 0.00002309
Iteration 156/1000 | Loss: 0.00002309
Iteration 157/1000 | Loss: 0.00002309
Iteration 158/1000 | Loss: 0.00002309
Iteration 159/1000 | Loss: 0.00002309
Iteration 160/1000 | Loss: 0.00002309
Iteration 161/1000 | Loss: 0.00002309
Iteration 162/1000 | Loss: 0.00002309
Iteration 163/1000 | Loss: 0.00002309
Iteration 164/1000 | Loss: 0.00002309
Iteration 165/1000 | Loss: 0.00002309
Iteration 166/1000 | Loss: 0.00002309
Iteration 167/1000 | Loss: 0.00002309
Iteration 168/1000 | Loss: 0.00002309
Iteration 169/1000 | Loss: 0.00002309
Iteration 170/1000 | Loss: 0.00002309
Iteration 171/1000 | Loss: 0.00002309
Iteration 172/1000 | Loss: 0.00002309
Iteration 173/1000 | Loss: 0.00002309
Iteration 174/1000 | Loss: 0.00002309
Iteration 175/1000 | Loss: 0.00002309
Iteration 176/1000 | Loss: 0.00002309
Iteration 177/1000 | Loss: 0.00002309
Iteration 178/1000 | Loss: 0.00002309
Iteration 179/1000 | Loss: 0.00002309
Iteration 180/1000 | Loss: 0.00002309
Iteration 181/1000 | Loss: 0.00002309
Iteration 182/1000 | Loss: 0.00002309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.3085365683073178e-05, 2.3085365683073178e-05, 2.3085365683073178e-05, 2.3085365683073178e-05, 2.3085365683073178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3085365683073178e-05

Optimization complete. Final v2v error: 3.8830723762512207 mm

Highest mean error: 5.917572975158691 mm for frame 40

Lowest mean error: 2.971346616744995 mm for frame 146

Saving results

Total time: 56.989415407180786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025716
Iteration 2/25 | Loss: 0.01025716
Iteration 3/25 | Loss: 0.01025716
Iteration 4/25 | Loss: 0.01025715
Iteration 5/25 | Loss: 0.01025715
Iteration 6/25 | Loss: 0.01025715
Iteration 7/25 | Loss: 0.01025715
Iteration 8/25 | Loss: 0.01025714
Iteration 9/25 | Loss: 0.00179555
Iteration 10/25 | Loss: 0.00122908
Iteration 11/25 | Loss: 0.00116093
Iteration 12/25 | Loss: 0.00114496
Iteration 13/25 | Loss: 0.00113795
Iteration 14/25 | Loss: 0.00114921
Iteration 15/25 | Loss: 0.00113086
Iteration 16/25 | Loss: 0.00112419
Iteration 17/25 | Loss: 0.00111171
Iteration 18/25 | Loss: 0.00110185
Iteration 19/25 | Loss: 0.00109388
Iteration 20/25 | Loss: 0.00109266
Iteration 21/25 | Loss: 0.00109206
Iteration 22/25 | Loss: 0.00109202
Iteration 23/25 | Loss: 0.00109202
Iteration 24/25 | Loss: 0.00109202
Iteration 25/25 | Loss: 0.00109202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37438226
Iteration 2/25 | Loss: 0.00086224
Iteration 3/25 | Loss: 0.00086224
Iteration 4/25 | Loss: 0.00086224
Iteration 5/25 | Loss: 0.00086224
Iteration 6/25 | Loss: 0.00086224
Iteration 7/25 | Loss: 0.00086224
Iteration 8/25 | Loss: 0.00086224
Iteration 9/25 | Loss: 0.00086224
Iteration 10/25 | Loss: 0.00086224
Iteration 11/25 | Loss: 0.00086224
Iteration 12/25 | Loss: 0.00086224
Iteration 13/25 | Loss: 0.00086224
Iteration 14/25 | Loss: 0.00086224
Iteration 15/25 | Loss: 0.00086224
Iteration 16/25 | Loss: 0.00086224
Iteration 17/25 | Loss: 0.00086224
Iteration 18/25 | Loss: 0.00086224
Iteration 19/25 | Loss: 0.00086224
Iteration 20/25 | Loss: 0.00086224
Iteration 21/25 | Loss: 0.00086224
Iteration 22/25 | Loss: 0.00086224
Iteration 23/25 | Loss: 0.00086224
Iteration 24/25 | Loss: 0.00086224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008622364257462323, 0.0008622364257462323, 0.0008622364257462323, 0.0008622364257462323, 0.0008622364257462323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008622364257462323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086224
Iteration 2/1000 | Loss: 0.00003135
Iteration 3/1000 | Loss: 0.00001861
Iteration 4/1000 | Loss: 0.00001608
Iteration 5/1000 | Loss: 0.00001447
Iteration 6/1000 | Loss: 0.00010457
Iteration 7/1000 | Loss: 0.00001325
Iteration 8/1000 | Loss: 0.00032335
Iteration 9/1000 | Loss: 0.00017728
Iteration 10/1000 | Loss: 0.00029740
Iteration 11/1000 | Loss: 0.00239301
Iteration 12/1000 | Loss: 0.00002527
Iteration 13/1000 | Loss: 0.00001289
Iteration 14/1000 | Loss: 0.00004044
Iteration 15/1000 | Loss: 0.00006744
Iteration 16/1000 | Loss: 0.00012057
Iteration 17/1000 | Loss: 0.00001911
Iteration 18/1000 | Loss: 0.00002443
Iteration 19/1000 | Loss: 0.00004294
Iteration 20/1000 | Loss: 0.00003307
Iteration 21/1000 | Loss: 0.00001446
Iteration 22/1000 | Loss: 0.00001232
Iteration 23/1000 | Loss: 0.00001230
Iteration 24/1000 | Loss: 0.00001385
Iteration 25/1000 | Loss: 0.00001220
Iteration 26/1000 | Loss: 0.00002506
Iteration 27/1000 | Loss: 0.00001566
Iteration 28/1000 | Loss: 0.00001317
Iteration 29/1000 | Loss: 0.00001212
Iteration 30/1000 | Loss: 0.00001211
Iteration 31/1000 | Loss: 0.00001210
Iteration 32/1000 | Loss: 0.00001210
Iteration 33/1000 | Loss: 0.00001210
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00003327
Iteration 36/1000 | Loss: 0.00001398
Iteration 37/1000 | Loss: 0.00001206
Iteration 38/1000 | Loss: 0.00001206
Iteration 39/1000 | Loss: 0.00001205
Iteration 40/1000 | Loss: 0.00001205
Iteration 41/1000 | Loss: 0.00003166
Iteration 42/1000 | Loss: 0.00007470
Iteration 43/1000 | Loss: 0.00002130
Iteration 44/1000 | Loss: 0.00003478
Iteration 45/1000 | Loss: 0.00001982
Iteration 46/1000 | Loss: 0.00002458
Iteration 47/1000 | Loss: 0.00001233
Iteration 48/1000 | Loss: 0.00004470
Iteration 49/1000 | Loss: 0.00006267
Iteration 50/1000 | Loss: 0.00008060
Iteration 51/1000 | Loss: 0.00001834
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001188
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001186
Iteration 58/1000 | Loss: 0.00001186
Iteration 59/1000 | Loss: 0.00001186
Iteration 60/1000 | Loss: 0.00001186
Iteration 61/1000 | Loss: 0.00001186
Iteration 62/1000 | Loss: 0.00001186
Iteration 63/1000 | Loss: 0.00001186
Iteration 64/1000 | Loss: 0.00001186
Iteration 65/1000 | Loss: 0.00001186
Iteration 66/1000 | Loss: 0.00001185
Iteration 67/1000 | Loss: 0.00001185
Iteration 68/1000 | Loss: 0.00001185
Iteration 69/1000 | Loss: 0.00001185
Iteration 70/1000 | Loss: 0.00001185
Iteration 71/1000 | Loss: 0.00001185
Iteration 72/1000 | Loss: 0.00002444
Iteration 73/1000 | Loss: 0.00001407
Iteration 74/1000 | Loss: 0.00003481
Iteration 75/1000 | Loss: 0.00010757
Iteration 76/1000 | Loss: 0.00005321
Iteration 77/1000 | Loss: 0.00020202
Iteration 78/1000 | Loss: 0.00001533
Iteration 79/1000 | Loss: 0.00001664
Iteration 80/1000 | Loss: 0.00003638
Iteration 81/1000 | Loss: 0.00001570
Iteration 82/1000 | Loss: 0.00003344
Iteration 83/1000 | Loss: 0.00001401
Iteration 84/1000 | Loss: 0.00003009
Iteration 85/1000 | Loss: 0.00001187
Iteration 86/1000 | Loss: 0.00001488
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001183
Iteration 89/1000 | Loss: 0.00001183
Iteration 90/1000 | Loss: 0.00001183
Iteration 91/1000 | Loss: 0.00001183
Iteration 92/1000 | Loss: 0.00001183
Iteration 93/1000 | Loss: 0.00001183
Iteration 94/1000 | Loss: 0.00001183
Iteration 95/1000 | Loss: 0.00001183
Iteration 96/1000 | Loss: 0.00001183
Iteration 97/1000 | Loss: 0.00001183
Iteration 98/1000 | Loss: 0.00001183
Iteration 99/1000 | Loss: 0.00001183
Iteration 100/1000 | Loss: 0.00001183
Iteration 101/1000 | Loss: 0.00001183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.1828724382212386e-05, 1.1828724382212386e-05, 1.1828724382212386e-05, 1.1828724382212386e-05, 1.1828724382212386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1828724382212386e-05

Optimization complete. Final v2v error: 2.9386894702911377 mm

Highest mean error: 3.5883865356445312 mm for frame 10

Lowest mean error: 2.594475507736206 mm for frame 37

Saving results

Total time: 113.77495384216309
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764596
Iteration 2/25 | Loss: 0.00143617
Iteration 3/25 | Loss: 0.00126985
Iteration 4/25 | Loss: 0.00118316
Iteration 5/25 | Loss: 0.00114004
Iteration 6/25 | Loss: 0.00112900
Iteration 7/25 | Loss: 0.00112663
Iteration 8/25 | Loss: 0.00112569
Iteration 9/25 | Loss: 0.00112543
Iteration 10/25 | Loss: 0.00112540
Iteration 11/25 | Loss: 0.00112540
Iteration 12/25 | Loss: 0.00112539
Iteration 13/25 | Loss: 0.00112538
Iteration 14/25 | Loss: 0.00112538
Iteration 15/25 | Loss: 0.00112537
Iteration 16/25 | Loss: 0.00112537
Iteration 17/25 | Loss: 0.00112537
Iteration 18/25 | Loss: 0.00112537
Iteration 19/25 | Loss: 0.00112537
Iteration 20/25 | Loss: 0.00112537
Iteration 21/25 | Loss: 0.00112537
Iteration 22/25 | Loss: 0.00112537
Iteration 23/25 | Loss: 0.00112537
Iteration 24/25 | Loss: 0.00112537
Iteration 25/25 | Loss: 0.00112536

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80142081
Iteration 2/25 | Loss: 0.00106913
Iteration 3/25 | Loss: 0.00106912
Iteration 4/25 | Loss: 0.00106912
Iteration 5/25 | Loss: 0.00106912
Iteration 6/25 | Loss: 0.00106912
Iteration 7/25 | Loss: 0.00106912
Iteration 8/25 | Loss: 0.00106912
Iteration 9/25 | Loss: 0.00106912
Iteration 10/25 | Loss: 0.00106912
Iteration 11/25 | Loss: 0.00106912
Iteration 12/25 | Loss: 0.00106912
Iteration 13/25 | Loss: 0.00106912
Iteration 14/25 | Loss: 0.00106912
Iteration 15/25 | Loss: 0.00106912
Iteration 16/25 | Loss: 0.00106912
Iteration 17/25 | Loss: 0.00106912
Iteration 18/25 | Loss: 0.00106912
Iteration 19/25 | Loss: 0.00106912
Iteration 20/25 | Loss: 0.00106912
Iteration 21/25 | Loss: 0.00106912
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010691196657717228, 0.0010691196657717228, 0.0010691196657717228, 0.0010691196657717228, 0.0010691196657717228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010691196657717228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106912
Iteration 2/1000 | Loss: 0.00004025
Iteration 3/1000 | Loss: 0.00002674
Iteration 4/1000 | Loss: 0.00002444
Iteration 5/1000 | Loss: 0.00002353
Iteration 6/1000 | Loss: 0.00002286
Iteration 7/1000 | Loss: 0.00002247
Iteration 8/1000 | Loss: 0.00002206
Iteration 9/1000 | Loss: 0.00002174
Iteration 10/1000 | Loss: 0.00002152
Iteration 11/1000 | Loss: 0.00002134
Iteration 12/1000 | Loss: 0.00002110
Iteration 13/1000 | Loss: 0.00002094
Iteration 14/1000 | Loss: 0.00002087
Iteration 15/1000 | Loss: 0.00002076
Iteration 16/1000 | Loss: 0.00002072
Iteration 17/1000 | Loss: 0.00002070
Iteration 18/1000 | Loss: 0.00002069
Iteration 19/1000 | Loss: 0.00002068
Iteration 20/1000 | Loss: 0.00002067
Iteration 21/1000 | Loss: 0.00002067
Iteration 22/1000 | Loss: 0.00002062
Iteration 23/1000 | Loss: 0.00002062
Iteration 24/1000 | Loss: 0.00002058
Iteration 25/1000 | Loss: 0.00002058
Iteration 26/1000 | Loss: 0.00002058
Iteration 27/1000 | Loss: 0.00002058
Iteration 28/1000 | Loss: 0.00002058
Iteration 29/1000 | Loss: 0.00002058
Iteration 30/1000 | Loss: 0.00002056
Iteration 31/1000 | Loss: 0.00002056
Iteration 32/1000 | Loss: 0.00002056
Iteration 33/1000 | Loss: 0.00002056
Iteration 34/1000 | Loss: 0.00002055
Iteration 35/1000 | Loss: 0.00002055
Iteration 36/1000 | Loss: 0.00002055
Iteration 37/1000 | Loss: 0.00002055
Iteration 38/1000 | Loss: 0.00002054
Iteration 39/1000 | Loss: 0.00002054
Iteration 40/1000 | Loss: 0.00002054
Iteration 41/1000 | Loss: 0.00002054
Iteration 42/1000 | Loss: 0.00002053
Iteration 43/1000 | Loss: 0.00002053
Iteration 44/1000 | Loss: 0.00002053
Iteration 45/1000 | Loss: 0.00002052
Iteration 46/1000 | Loss: 0.00002052
Iteration 47/1000 | Loss: 0.00002052
Iteration 48/1000 | Loss: 0.00002052
Iteration 49/1000 | Loss: 0.00002052
Iteration 50/1000 | Loss: 0.00002051
Iteration 51/1000 | Loss: 0.00002051
Iteration 52/1000 | Loss: 0.00002051
Iteration 53/1000 | Loss: 0.00002051
Iteration 54/1000 | Loss: 0.00002050
Iteration 55/1000 | Loss: 0.00002050
Iteration 56/1000 | Loss: 0.00002050
Iteration 57/1000 | Loss: 0.00002050
Iteration 58/1000 | Loss: 0.00002050
Iteration 59/1000 | Loss: 0.00002050
Iteration 60/1000 | Loss: 0.00002050
Iteration 61/1000 | Loss: 0.00002050
Iteration 62/1000 | Loss: 0.00002050
Iteration 63/1000 | Loss: 0.00002050
Iteration 64/1000 | Loss: 0.00002050
Iteration 65/1000 | Loss: 0.00002049
Iteration 66/1000 | Loss: 0.00002049
Iteration 67/1000 | Loss: 0.00002049
Iteration 68/1000 | Loss: 0.00002049
Iteration 69/1000 | Loss: 0.00002049
Iteration 70/1000 | Loss: 0.00002049
Iteration 71/1000 | Loss: 0.00002049
Iteration 72/1000 | Loss: 0.00002048
Iteration 73/1000 | Loss: 0.00002048
Iteration 74/1000 | Loss: 0.00002048
Iteration 75/1000 | Loss: 0.00002048
Iteration 76/1000 | Loss: 0.00002048
Iteration 77/1000 | Loss: 0.00002048
Iteration 78/1000 | Loss: 0.00002048
Iteration 79/1000 | Loss: 0.00002048
Iteration 80/1000 | Loss: 0.00002048
Iteration 81/1000 | Loss: 0.00002047
Iteration 82/1000 | Loss: 0.00002047
Iteration 83/1000 | Loss: 0.00002047
Iteration 84/1000 | Loss: 0.00002047
Iteration 85/1000 | Loss: 0.00002047
Iteration 86/1000 | Loss: 0.00002047
Iteration 87/1000 | Loss: 0.00002047
Iteration 88/1000 | Loss: 0.00002047
Iteration 89/1000 | Loss: 0.00002047
Iteration 90/1000 | Loss: 0.00002047
Iteration 91/1000 | Loss: 0.00002047
Iteration 92/1000 | Loss: 0.00002046
Iteration 93/1000 | Loss: 0.00002046
Iteration 94/1000 | Loss: 0.00002046
Iteration 95/1000 | Loss: 0.00002046
Iteration 96/1000 | Loss: 0.00002046
Iteration 97/1000 | Loss: 0.00002046
Iteration 98/1000 | Loss: 0.00002046
Iteration 99/1000 | Loss: 0.00002045
Iteration 100/1000 | Loss: 0.00002045
Iteration 101/1000 | Loss: 0.00002045
Iteration 102/1000 | Loss: 0.00002045
Iteration 103/1000 | Loss: 0.00002045
Iteration 104/1000 | Loss: 0.00002045
Iteration 105/1000 | Loss: 0.00002045
Iteration 106/1000 | Loss: 0.00002045
Iteration 107/1000 | Loss: 0.00002045
Iteration 108/1000 | Loss: 0.00002045
Iteration 109/1000 | Loss: 0.00002045
Iteration 110/1000 | Loss: 0.00002044
Iteration 111/1000 | Loss: 0.00002044
Iteration 112/1000 | Loss: 0.00002044
Iteration 113/1000 | Loss: 0.00002044
Iteration 114/1000 | Loss: 0.00002044
Iteration 115/1000 | Loss: 0.00002044
Iteration 116/1000 | Loss: 0.00002044
Iteration 117/1000 | Loss: 0.00002044
Iteration 118/1000 | Loss: 0.00002044
Iteration 119/1000 | Loss: 0.00002044
Iteration 120/1000 | Loss: 0.00002044
Iteration 121/1000 | Loss: 0.00002043
Iteration 122/1000 | Loss: 0.00002043
Iteration 123/1000 | Loss: 0.00002043
Iteration 124/1000 | Loss: 0.00002043
Iteration 125/1000 | Loss: 0.00002043
Iteration 126/1000 | Loss: 0.00002043
Iteration 127/1000 | Loss: 0.00002043
Iteration 128/1000 | Loss: 0.00002043
Iteration 129/1000 | Loss: 0.00002043
Iteration 130/1000 | Loss: 0.00002043
Iteration 131/1000 | Loss: 0.00002043
Iteration 132/1000 | Loss: 0.00002042
Iteration 133/1000 | Loss: 0.00002042
Iteration 134/1000 | Loss: 0.00002042
Iteration 135/1000 | Loss: 0.00002042
Iteration 136/1000 | Loss: 0.00002042
Iteration 137/1000 | Loss: 0.00002042
Iteration 138/1000 | Loss: 0.00002042
Iteration 139/1000 | Loss: 0.00002042
Iteration 140/1000 | Loss: 0.00002042
Iteration 141/1000 | Loss: 0.00002042
Iteration 142/1000 | Loss: 0.00002042
Iteration 143/1000 | Loss: 0.00002042
Iteration 144/1000 | Loss: 0.00002042
Iteration 145/1000 | Loss: 0.00002042
Iteration 146/1000 | Loss: 0.00002041
Iteration 147/1000 | Loss: 0.00002041
Iteration 148/1000 | Loss: 0.00002041
Iteration 149/1000 | Loss: 0.00002041
Iteration 150/1000 | Loss: 0.00002041
Iteration 151/1000 | Loss: 0.00002041
Iteration 152/1000 | Loss: 0.00002041
Iteration 153/1000 | Loss: 0.00002041
Iteration 154/1000 | Loss: 0.00002041
Iteration 155/1000 | Loss: 0.00002041
Iteration 156/1000 | Loss: 0.00002041
Iteration 157/1000 | Loss: 0.00002041
Iteration 158/1000 | Loss: 0.00002041
Iteration 159/1000 | Loss: 0.00002041
Iteration 160/1000 | Loss: 0.00002041
Iteration 161/1000 | Loss: 0.00002041
Iteration 162/1000 | Loss: 0.00002041
Iteration 163/1000 | Loss: 0.00002040
Iteration 164/1000 | Loss: 0.00002040
Iteration 165/1000 | Loss: 0.00002040
Iteration 166/1000 | Loss: 0.00002040
Iteration 167/1000 | Loss: 0.00002040
Iteration 168/1000 | Loss: 0.00002040
Iteration 169/1000 | Loss: 0.00002040
Iteration 170/1000 | Loss: 0.00002040
Iteration 171/1000 | Loss: 0.00002040
Iteration 172/1000 | Loss: 0.00002040
Iteration 173/1000 | Loss: 0.00002040
Iteration 174/1000 | Loss: 0.00002040
Iteration 175/1000 | Loss: 0.00002040
Iteration 176/1000 | Loss: 0.00002040
Iteration 177/1000 | Loss: 0.00002040
Iteration 178/1000 | Loss: 0.00002040
Iteration 179/1000 | Loss: 0.00002040
Iteration 180/1000 | Loss: 0.00002039
Iteration 181/1000 | Loss: 0.00002039
Iteration 182/1000 | Loss: 0.00002039
Iteration 183/1000 | Loss: 0.00002039
Iteration 184/1000 | Loss: 0.00002039
Iteration 185/1000 | Loss: 0.00002039
Iteration 186/1000 | Loss: 0.00002039
Iteration 187/1000 | Loss: 0.00002039
Iteration 188/1000 | Loss: 0.00002039
Iteration 189/1000 | Loss: 0.00002039
Iteration 190/1000 | Loss: 0.00002039
Iteration 191/1000 | Loss: 0.00002039
Iteration 192/1000 | Loss: 0.00002039
Iteration 193/1000 | Loss: 0.00002039
Iteration 194/1000 | Loss: 0.00002039
Iteration 195/1000 | Loss: 0.00002039
Iteration 196/1000 | Loss: 0.00002039
Iteration 197/1000 | Loss: 0.00002039
Iteration 198/1000 | Loss: 0.00002039
Iteration 199/1000 | Loss: 0.00002039
Iteration 200/1000 | Loss: 0.00002039
Iteration 201/1000 | Loss: 0.00002039
Iteration 202/1000 | Loss: 0.00002039
Iteration 203/1000 | Loss: 0.00002039
Iteration 204/1000 | Loss: 0.00002039
Iteration 205/1000 | Loss: 0.00002039
Iteration 206/1000 | Loss: 0.00002039
Iteration 207/1000 | Loss: 0.00002039
Iteration 208/1000 | Loss: 0.00002039
Iteration 209/1000 | Loss: 0.00002039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.0387478798511438e-05, 2.0387478798511438e-05, 2.0387478798511438e-05, 2.0387478798511438e-05, 2.0387478798511438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0387478798511438e-05

Optimization complete. Final v2v error: 3.786221742630005 mm

Highest mean error: 4.522409439086914 mm for frame 2

Lowest mean error: 3.119874954223633 mm for frame 137

Saving results

Total time: 50.001646995544434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807452
Iteration 2/25 | Loss: 0.00138702
Iteration 3/25 | Loss: 0.00112943
Iteration 4/25 | Loss: 0.00110829
Iteration 5/25 | Loss: 0.00110595
Iteration 6/25 | Loss: 0.00110502
Iteration 7/25 | Loss: 0.00110457
Iteration 8/25 | Loss: 0.00110421
Iteration 9/25 | Loss: 0.00110528
Iteration 10/25 | Loss: 0.00110572
Iteration 11/25 | Loss: 0.00110726
Iteration 12/25 | Loss: 0.00110439
Iteration 13/25 | Loss: 0.00109944
Iteration 14/25 | Loss: 0.00109726
Iteration 15/25 | Loss: 0.00109668
Iteration 16/25 | Loss: 0.00109647
Iteration 17/25 | Loss: 0.00109643
Iteration 18/25 | Loss: 0.00109642
Iteration 19/25 | Loss: 0.00109642
Iteration 20/25 | Loss: 0.00109642
Iteration 21/25 | Loss: 0.00109642
Iteration 22/25 | Loss: 0.00109642
Iteration 23/25 | Loss: 0.00109642
Iteration 24/25 | Loss: 0.00109642
Iteration 25/25 | Loss: 0.00109642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35436118
Iteration 2/25 | Loss: 0.00080382
Iteration 3/25 | Loss: 0.00080381
Iteration 4/25 | Loss: 0.00080381
Iteration 5/25 | Loss: 0.00080381
Iteration 6/25 | Loss: 0.00080381
Iteration 7/25 | Loss: 0.00080381
Iteration 8/25 | Loss: 0.00080381
Iteration 9/25 | Loss: 0.00080381
Iteration 10/25 | Loss: 0.00080381
Iteration 11/25 | Loss: 0.00080381
Iteration 12/25 | Loss: 0.00080381
Iteration 13/25 | Loss: 0.00080381
Iteration 14/25 | Loss: 0.00080381
Iteration 15/25 | Loss: 0.00080381
Iteration 16/25 | Loss: 0.00080381
Iteration 17/25 | Loss: 0.00080381
Iteration 18/25 | Loss: 0.00080381
Iteration 19/25 | Loss: 0.00080381
Iteration 20/25 | Loss: 0.00080381
Iteration 21/25 | Loss: 0.00080381
Iteration 22/25 | Loss: 0.00080381
Iteration 23/25 | Loss: 0.00080381
Iteration 24/25 | Loss: 0.00080381
Iteration 25/25 | Loss: 0.00080381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080381
Iteration 2/1000 | Loss: 0.00002124
Iteration 3/1000 | Loss: 0.00001422
Iteration 4/1000 | Loss: 0.00001276
Iteration 5/1000 | Loss: 0.00001193
Iteration 6/1000 | Loss: 0.00001136
Iteration 7/1000 | Loss: 0.00001097
Iteration 8/1000 | Loss: 0.00001075
Iteration 9/1000 | Loss: 0.00001042
Iteration 10/1000 | Loss: 0.00001029
Iteration 11/1000 | Loss: 0.00001021
Iteration 12/1000 | Loss: 0.00001021
Iteration 13/1000 | Loss: 0.00001015
Iteration 14/1000 | Loss: 0.00001015
Iteration 15/1000 | Loss: 0.00001013
Iteration 16/1000 | Loss: 0.00001013
Iteration 17/1000 | Loss: 0.00001011
Iteration 18/1000 | Loss: 0.00001011
Iteration 19/1000 | Loss: 0.00001011
Iteration 20/1000 | Loss: 0.00001011
Iteration 21/1000 | Loss: 0.00001010
Iteration 22/1000 | Loss: 0.00001010
Iteration 23/1000 | Loss: 0.00001010
Iteration 24/1000 | Loss: 0.00001009
Iteration 25/1000 | Loss: 0.00001009
Iteration 26/1000 | Loss: 0.00001008
Iteration 27/1000 | Loss: 0.00001008
Iteration 28/1000 | Loss: 0.00001008
Iteration 29/1000 | Loss: 0.00001007
Iteration 30/1000 | Loss: 0.00001007
Iteration 31/1000 | Loss: 0.00001006
Iteration 32/1000 | Loss: 0.00001005
Iteration 33/1000 | Loss: 0.00001004
Iteration 34/1000 | Loss: 0.00001004
Iteration 35/1000 | Loss: 0.00001003
Iteration 36/1000 | Loss: 0.00001002
Iteration 37/1000 | Loss: 0.00001002
Iteration 38/1000 | Loss: 0.00001002
Iteration 39/1000 | Loss: 0.00001002
Iteration 40/1000 | Loss: 0.00001002
Iteration 41/1000 | Loss: 0.00001001
Iteration 42/1000 | Loss: 0.00001001
Iteration 43/1000 | Loss: 0.00001000
Iteration 44/1000 | Loss: 0.00001000
Iteration 45/1000 | Loss: 0.00000999
Iteration 46/1000 | Loss: 0.00000999
Iteration 47/1000 | Loss: 0.00000998
Iteration 48/1000 | Loss: 0.00000998
Iteration 49/1000 | Loss: 0.00000998
Iteration 50/1000 | Loss: 0.00000998
Iteration 51/1000 | Loss: 0.00000998
Iteration 52/1000 | Loss: 0.00000998
Iteration 53/1000 | Loss: 0.00000998
Iteration 54/1000 | Loss: 0.00000997
Iteration 55/1000 | Loss: 0.00000997
Iteration 56/1000 | Loss: 0.00000997
Iteration 57/1000 | Loss: 0.00000997
Iteration 58/1000 | Loss: 0.00000997
Iteration 59/1000 | Loss: 0.00000997
Iteration 60/1000 | Loss: 0.00000996
Iteration 61/1000 | Loss: 0.00000996
Iteration 62/1000 | Loss: 0.00000996
Iteration 63/1000 | Loss: 0.00000996
Iteration 64/1000 | Loss: 0.00000995
Iteration 65/1000 | Loss: 0.00000995
Iteration 66/1000 | Loss: 0.00000995
Iteration 67/1000 | Loss: 0.00000995
Iteration 68/1000 | Loss: 0.00000995
Iteration 69/1000 | Loss: 0.00000995
Iteration 70/1000 | Loss: 0.00000995
Iteration 71/1000 | Loss: 0.00000994
Iteration 72/1000 | Loss: 0.00000994
Iteration 73/1000 | Loss: 0.00000994
Iteration 74/1000 | Loss: 0.00000993
Iteration 75/1000 | Loss: 0.00000993
Iteration 76/1000 | Loss: 0.00000993
Iteration 77/1000 | Loss: 0.00000993
Iteration 78/1000 | Loss: 0.00000993
Iteration 79/1000 | Loss: 0.00000993
Iteration 80/1000 | Loss: 0.00000993
Iteration 81/1000 | Loss: 0.00000993
Iteration 82/1000 | Loss: 0.00000992
Iteration 83/1000 | Loss: 0.00000992
Iteration 84/1000 | Loss: 0.00000992
Iteration 85/1000 | Loss: 0.00000992
Iteration 86/1000 | Loss: 0.00000992
Iteration 87/1000 | Loss: 0.00000992
Iteration 88/1000 | Loss: 0.00000992
Iteration 89/1000 | Loss: 0.00000992
Iteration 90/1000 | Loss: 0.00000992
Iteration 91/1000 | Loss: 0.00000992
Iteration 92/1000 | Loss: 0.00000992
Iteration 93/1000 | Loss: 0.00000992
Iteration 94/1000 | Loss: 0.00000992
Iteration 95/1000 | Loss: 0.00000992
Iteration 96/1000 | Loss: 0.00000992
Iteration 97/1000 | Loss: 0.00000992
Iteration 98/1000 | Loss: 0.00000991
Iteration 99/1000 | Loss: 0.00000991
Iteration 100/1000 | Loss: 0.00000991
Iteration 101/1000 | Loss: 0.00000990
Iteration 102/1000 | Loss: 0.00000990
Iteration 103/1000 | Loss: 0.00000990
Iteration 104/1000 | Loss: 0.00000990
Iteration 105/1000 | Loss: 0.00000990
Iteration 106/1000 | Loss: 0.00000989
Iteration 107/1000 | Loss: 0.00000989
Iteration 108/1000 | Loss: 0.00000989
Iteration 109/1000 | Loss: 0.00000989
Iteration 110/1000 | Loss: 0.00000988
Iteration 111/1000 | Loss: 0.00000988
Iteration 112/1000 | Loss: 0.00000988
Iteration 113/1000 | Loss: 0.00000988
Iteration 114/1000 | Loss: 0.00000988
Iteration 115/1000 | Loss: 0.00000988
Iteration 116/1000 | Loss: 0.00000988
Iteration 117/1000 | Loss: 0.00000988
Iteration 118/1000 | Loss: 0.00000988
Iteration 119/1000 | Loss: 0.00000988
Iteration 120/1000 | Loss: 0.00000988
Iteration 121/1000 | Loss: 0.00000988
Iteration 122/1000 | Loss: 0.00000988
Iteration 123/1000 | Loss: 0.00000988
Iteration 124/1000 | Loss: 0.00000987
Iteration 125/1000 | Loss: 0.00000987
Iteration 126/1000 | Loss: 0.00000987
Iteration 127/1000 | Loss: 0.00000987
Iteration 128/1000 | Loss: 0.00000987
Iteration 129/1000 | Loss: 0.00000987
Iteration 130/1000 | Loss: 0.00000987
Iteration 131/1000 | Loss: 0.00000987
Iteration 132/1000 | Loss: 0.00000986
Iteration 133/1000 | Loss: 0.00000986
Iteration 134/1000 | Loss: 0.00000986
Iteration 135/1000 | Loss: 0.00000986
Iteration 136/1000 | Loss: 0.00000985
Iteration 137/1000 | Loss: 0.00000985
Iteration 138/1000 | Loss: 0.00000985
Iteration 139/1000 | Loss: 0.00000985
Iteration 140/1000 | Loss: 0.00000985
Iteration 141/1000 | Loss: 0.00000985
Iteration 142/1000 | Loss: 0.00000985
Iteration 143/1000 | Loss: 0.00000985
Iteration 144/1000 | Loss: 0.00000985
Iteration 145/1000 | Loss: 0.00000984
Iteration 146/1000 | Loss: 0.00000984
Iteration 147/1000 | Loss: 0.00000984
Iteration 148/1000 | Loss: 0.00000984
Iteration 149/1000 | Loss: 0.00000983
Iteration 150/1000 | Loss: 0.00000983
Iteration 151/1000 | Loss: 0.00000983
Iteration 152/1000 | Loss: 0.00000983
Iteration 153/1000 | Loss: 0.00000983
Iteration 154/1000 | Loss: 0.00000983
Iteration 155/1000 | Loss: 0.00000983
Iteration 156/1000 | Loss: 0.00000983
Iteration 157/1000 | Loss: 0.00000983
Iteration 158/1000 | Loss: 0.00000983
Iteration 159/1000 | Loss: 0.00000983
Iteration 160/1000 | Loss: 0.00000983
Iteration 161/1000 | Loss: 0.00000983
Iteration 162/1000 | Loss: 0.00000983
Iteration 163/1000 | Loss: 0.00000982
Iteration 164/1000 | Loss: 0.00000982
Iteration 165/1000 | Loss: 0.00000982
Iteration 166/1000 | Loss: 0.00000982
Iteration 167/1000 | Loss: 0.00000982
Iteration 168/1000 | Loss: 0.00000982
Iteration 169/1000 | Loss: 0.00000982
Iteration 170/1000 | Loss: 0.00000981
Iteration 171/1000 | Loss: 0.00000981
Iteration 172/1000 | Loss: 0.00000981
Iteration 173/1000 | Loss: 0.00000981
Iteration 174/1000 | Loss: 0.00000981
Iteration 175/1000 | Loss: 0.00000981
Iteration 176/1000 | Loss: 0.00000981
Iteration 177/1000 | Loss: 0.00000981
Iteration 178/1000 | Loss: 0.00000981
Iteration 179/1000 | Loss: 0.00000980
Iteration 180/1000 | Loss: 0.00000980
Iteration 181/1000 | Loss: 0.00000980
Iteration 182/1000 | Loss: 0.00000980
Iteration 183/1000 | Loss: 0.00000979
Iteration 184/1000 | Loss: 0.00000979
Iteration 185/1000 | Loss: 0.00000979
Iteration 186/1000 | Loss: 0.00000979
Iteration 187/1000 | Loss: 0.00000978
Iteration 188/1000 | Loss: 0.00000978
Iteration 189/1000 | Loss: 0.00000978
Iteration 190/1000 | Loss: 0.00000978
Iteration 191/1000 | Loss: 0.00000978
Iteration 192/1000 | Loss: 0.00000978
Iteration 193/1000 | Loss: 0.00000978
Iteration 194/1000 | Loss: 0.00000978
Iteration 195/1000 | Loss: 0.00000978
Iteration 196/1000 | Loss: 0.00000978
Iteration 197/1000 | Loss: 0.00000977
Iteration 198/1000 | Loss: 0.00000977
Iteration 199/1000 | Loss: 0.00000977
Iteration 200/1000 | Loss: 0.00000977
Iteration 201/1000 | Loss: 0.00000977
Iteration 202/1000 | Loss: 0.00000977
Iteration 203/1000 | Loss: 0.00000977
Iteration 204/1000 | Loss: 0.00000977
Iteration 205/1000 | Loss: 0.00000977
Iteration 206/1000 | Loss: 0.00000976
Iteration 207/1000 | Loss: 0.00000976
Iteration 208/1000 | Loss: 0.00000976
Iteration 209/1000 | Loss: 0.00000976
Iteration 210/1000 | Loss: 0.00000976
Iteration 211/1000 | Loss: 0.00000976
Iteration 212/1000 | Loss: 0.00000976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [9.761995897861198e-06, 9.761995897861198e-06, 9.761995897861198e-06, 9.761995897861198e-06, 9.761995897861198e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.761995897861198e-06

Optimization complete. Final v2v error: 2.70603609085083 mm

Highest mean error: 3.027571678161621 mm for frame 123

Lowest mean error: 2.5233893394470215 mm for frame 3

Saving results

Total time: 67.87069392204285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783546
Iteration 2/25 | Loss: 0.00136196
Iteration 3/25 | Loss: 0.00113270
Iteration 4/25 | Loss: 0.00111436
Iteration 5/25 | Loss: 0.00111222
Iteration 6/25 | Loss: 0.00111222
Iteration 7/25 | Loss: 0.00111222
Iteration 8/25 | Loss: 0.00111222
Iteration 9/25 | Loss: 0.00111222
Iteration 10/25 | Loss: 0.00111222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011122156865894794, 0.0011122156865894794, 0.0011122156865894794, 0.0011122156865894794, 0.0011122156865894794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011122156865894794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34955525
Iteration 2/25 | Loss: 0.00071859
Iteration 3/25 | Loss: 0.00071858
Iteration 4/25 | Loss: 0.00071858
Iteration 5/25 | Loss: 0.00071858
Iteration 6/25 | Loss: 0.00071858
Iteration 7/25 | Loss: 0.00071858
Iteration 8/25 | Loss: 0.00071858
Iteration 9/25 | Loss: 0.00071858
Iteration 10/25 | Loss: 0.00071858
Iteration 11/25 | Loss: 0.00071858
Iteration 12/25 | Loss: 0.00071858
Iteration 13/25 | Loss: 0.00071858
Iteration 14/25 | Loss: 0.00071858
Iteration 15/25 | Loss: 0.00071858
Iteration 16/25 | Loss: 0.00071858
Iteration 17/25 | Loss: 0.00071858
Iteration 18/25 | Loss: 0.00071858
Iteration 19/25 | Loss: 0.00071858
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007185813155956566, 0.0007185813155956566, 0.0007185813155956566, 0.0007185813155956566, 0.0007185813155956566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007185813155956566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071858
Iteration 2/1000 | Loss: 0.00002626
Iteration 3/1000 | Loss: 0.00001877
Iteration 4/1000 | Loss: 0.00001616
Iteration 5/1000 | Loss: 0.00001467
Iteration 6/1000 | Loss: 0.00001374
Iteration 7/1000 | Loss: 0.00001320
Iteration 8/1000 | Loss: 0.00001288
Iteration 9/1000 | Loss: 0.00001281
Iteration 10/1000 | Loss: 0.00001252
Iteration 11/1000 | Loss: 0.00001232
Iteration 12/1000 | Loss: 0.00001222
Iteration 13/1000 | Loss: 0.00001214
Iteration 14/1000 | Loss: 0.00001209
Iteration 15/1000 | Loss: 0.00001208
Iteration 16/1000 | Loss: 0.00001208
Iteration 17/1000 | Loss: 0.00001203
Iteration 18/1000 | Loss: 0.00001202
Iteration 19/1000 | Loss: 0.00001202
Iteration 20/1000 | Loss: 0.00001201
Iteration 21/1000 | Loss: 0.00001200
Iteration 22/1000 | Loss: 0.00001200
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001199
Iteration 25/1000 | Loss: 0.00001198
Iteration 26/1000 | Loss: 0.00001198
Iteration 27/1000 | Loss: 0.00001197
Iteration 28/1000 | Loss: 0.00001196
Iteration 29/1000 | Loss: 0.00001195
Iteration 30/1000 | Loss: 0.00001195
Iteration 31/1000 | Loss: 0.00001195
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001194
Iteration 35/1000 | Loss: 0.00001193
Iteration 36/1000 | Loss: 0.00001193
Iteration 37/1000 | Loss: 0.00001193
Iteration 38/1000 | Loss: 0.00001192
Iteration 39/1000 | Loss: 0.00001191
Iteration 40/1000 | Loss: 0.00001191
Iteration 41/1000 | Loss: 0.00001188
Iteration 42/1000 | Loss: 0.00001188
Iteration 43/1000 | Loss: 0.00001188
Iteration 44/1000 | Loss: 0.00001187
Iteration 45/1000 | Loss: 0.00001187
Iteration 46/1000 | Loss: 0.00001186
Iteration 47/1000 | Loss: 0.00001185
Iteration 48/1000 | Loss: 0.00001185
Iteration 49/1000 | Loss: 0.00001184
Iteration 50/1000 | Loss: 0.00001184
Iteration 51/1000 | Loss: 0.00001183
Iteration 52/1000 | Loss: 0.00001183
Iteration 53/1000 | Loss: 0.00001183
Iteration 54/1000 | Loss: 0.00001183
Iteration 55/1000 | Loss: 0.00001183
Iteration 56/1000 | Loss: 0.00001183
Iteration 57/1000 | Loss: 0.00001183
Iteration 58/1000 | Loss: 0.00001183
Iteration 59/1000 | Loss: 0.00001183
Iteration 60/1000 | Loss: 0.00001182
Iteration 61/1000 | Loss: 0.00001182
Iteration 62/1000 | Loss: 0.00001182
Iteration 63/1000 | Loss: 0.00001181
Iteration 64/1000 | Loss: 0.00001181
Iteration 65/1000 | Loss: 0.00001181
Iteration 66/1000 | Loss: 0.00001181
Iteration 67/1000 | Loss: 0.00001181
Iteration 68/1000 | Loss: 0.00001181
Iteration 69/1000 | Loss: 0.00001181
Iteration 70/1000 | Loss: 0.00001180
Iteration 71/1000 | Loss: 0.00001179
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001179
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001178
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001177
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001176
Iteration 80/1000 | Loss: 0.00001176
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001174
Iteration 83/1000 | Loss: 0.00001174
Iteration 84/1000 | Loss: 0.00001174
Iteration 85/1000 | Loss: 0.00001173
Iteration 86/1000 | Loss: 0.00001173
Iteration 87/1000 | Loss: 0.00001173
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001173
Iteration 90/1000 | Loss: 0.00001173
Iteration 91/1000 | Loss: 0.00001173
Iteration 92/1000 | Loss: 0.00001173
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001172
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001171
Iteration 100/1000 | Loss: 0.00001171
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001169
Iteration 111/1000 | Loss: 0.00001169
Iteration 112/1000 | Loss: 0.00001169
Iteration 113/1000 | Loss: 0.00001168
Iteration 114/1000 | Loss: 0.00001168
Iteration 115/1000 | Loss: 0.00001168
Iteration 116/1000 | Loss: 0.00001168
Iteration 117/1000 | Loss: 0.00001168
Iteration 118/1000 | Loss: 0.00001167
Iteration 119/1000 | Loss: 0.00001167
Iteration 120/1000 | Loss: 0.00001166
Iteration 121/1000 | Loss: 0.00001166
Iteration 122/1000 | Loss: 0.00001166
Iteration 123/1000 | Loss: 0.00001166
Iteration 124/1000 | Loss: 0.00001166
Iteration 125/1000 | Loss: 0.00001166
Iteration 126/1000 | Loss: 0.00001166
Iteration 127/1000 | Loss: 0.00001165
Iteration 128/1000 | Loss: 0.00001165
Iteration 129/1000 | Loss: 0.00001165
Iteration 130/1000 | Loss: 0.00001165
Iteration 131/1000 | Loss: 0.00001165
Iteration 132/1000 | Loss: 0.00001164
Iteration 133/1000 | Loss: 0.00001164
Iteration 134/1000 | Loss: 0.00001164
Iteration 135/1000 | Loss: 0.00001164
Iteration 136/1000 | Loss: 0.00001164
Iteration 137/1000 | Loss: 0.00001164
Iteration 138/1000 | Loss: 0.00001164
Iteration 139/1000 | Loss: 0.00001163
Iteration 140/1000 | Loss: 0.00001163
Iteration 141/1000 | Loss: 0.00001163
Iteration 142/1000 | Loss: 0.00001163
Iteration 143/1000 | Loss: 0.00001163
Iteration 144/1000 | Loss: 0.00001163
Iteration 145/1000 | Loss: 0.00001163
Iteration 146/1000 | Loss: 0.00001163
Iteration 147/1000 | Loss: 0.00001163
Iteration 148/1000 | Loss: 0.00001162
Iteration 149/1000 | Loss: 0.00001162
Iteration 150/1000 | Loss: 0.00001162
Iteration 151/1000 | Loss: 0.00001162
Iteration 152/1000 | Loss: 0.00001162
Iteration 153/1000 | Loss: 0.00001162
Iteration 154/1000 | Loss: 0.00001162
Iteration 155/1000 | Loss: 0.00001162
Iteration 156/1000 | Loss: 0.00001161
Iteration 157/1000 | Loss: 0.00001161
Iteration 158/1000 | Loss: 0.00001161
Iteration 159/1000 | Loss: 0.00001161
Iteration 160/1000 | Loss: 0.00001161
Iteration 161/1000 | Loss: 0.00001160
Iteration 162/1000 | Loss: 0.00001160
Iteration 163/1000 | Loss: 0.00001160
Iteration 164/1000 | Loss: 0.00001159
Iteration 165/1000 | Loss: 0.00001159
Iteration 166/1000 | Loss: 0.00001159
Iteration 167/1000 | Loss: 0.00001159
Iteration 168/1000 | Loss: 0.00001159
Iteration 169/1000 | Loss: 0.00001159
Iteration 170/1000 | Loss: 0.00001159
Iteration 171/1000 | Loss: 0.00001159
Iteration 172/1000 | Loss: 0.00001159
Iteration 173/1000 | Loss: 0.00001158
Iteration 174/1000 | Loss: 0.00001158
Iteration 175/1000 | Loss: 0.00001158
Iteration 176/1000 | Loss: 0.00001158
Iteration 177/1000 | Loss: 0.00001158
Iteration 178/1000 | Loss: 0.00001158
Iteration 179/1000 | Loss: 0.00001157
Iteration 180/1000 | Loss: 0.00001157
Iteration 181/1000 | Loss: 0.00001157
Iteration 182/1000 | Loss: 0.00001157
Iteration 183/1000 | Loss: 0.00001157
Iteration 184/1000 | Loss: 0.00001157
Iteration 185/1000 | Loss: 0.00001157
Iteration 186/1000 | Loss: 0.00001156
Iteration 187/1000 | Loss: 0.00001156
Iteration 188/1000 | Loss: 0.00001156
Iteration 189/1000 | Loss: 0.00001156
Iteration 190/1000 | Loss: 0.00001156
Iteration 191/1000 | Loss: 0.00001156
Iteration 192/1000 | Loss: 0.00001156
Iteration 193/1000 | Loss: 0.00001156
Iteration 194/1000 | Loss: 0.00001156
Iteration 195/1000 | Loss: 0.00001155
Iteration 196/1000 | Loss: 0.00001155
Iteration 197/1000 | Loss: 0.00001155
Iteration 198/1000 | Loss: 0.00001155
Iteration 199/1000 | Loss: 0.00001155
Iteration 200/1000 | Loss: 0.00001155
Iteration 201/1000 | Loss: 0.00001155
Iteration 202/1000 | Loss: 0.00001155
Iteration 203/1000 | Loss: 0.00001155
Iteration 204/1000 | Loss: 0.00001154
Iteration 205/1000 | Loss: 0.00001154
Iteration 206/1000 | Loss: 0.00001154
Iteration 207/1000 | Loss: 0.00001154
Iteration 208/1000 | Loss: 0.00001154
Iteration 209/1000 | Loss: 0.00001154
Iteration 210/1000 | Loss: 0.00001154
Iteration 211/1000 | Loss: 0.00001154
Iteration 212/1000 | Loss: 0.00001154
Iteration 213/1000 | Loss: 0.00001154
Iteration 214/1000 | Loss: 0.00001154
Iteration 215/1000 | Loss: 0.00001154
Iteration 216/1000 | Loss: 0.00001154
Iteration 217/1000 | Loss: 0.00001154
Iteration 218/1000 | Loss: 0.00001154
Iteration 219/1000 | Loss: 0.00001154
Iteration 220/1000 | Loss: 0.00001153
Iteration 221/1000 | Loss: 0.00001153
Iteration 222/1000 | Loss: 0.00001153
Iteration 223/1000 | Loss: 0.00001153
Iteration 224/1000 | Loss: 0.00001153
Iteration 225/1000 | Loss: 0.00001153
Iteration 226/1000 | Loss: 0.00001153
Iteration 227/1000 | Loss: 0.00001153
Iteration 228/1000 | Loss: 0.00001153
Iteration 229/1000 | Loss: 0.00001153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.1531024938449264e-05, 1.1531024938449264e-05, 1.1531024938449264e-05, 1.1531024938449264e-05, 1.1531024938449264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1531024938449264e-05

Optimization complete. Final v2v error: 2.854573965072632 mm

Highest mean error: 3.178393602371216 mm for frame 123

Lowest mean error: 2.577773332595825 mm for frame 78

Saving results

Total time: 40.45296359062195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064348
Iteration 2/25 | Loss: 0.00278802
Iteration 3/25 | Loss: 0.00153320
Iteration 4/25 | Loss: 0.00140770
Iteration 5/25 | Loss: 0.00130478
Iteration 6/25 | Loss: 0.00126800
Iteration 7/25 | Loss: 0.00121892
Iteration 8/25 | Loss: 0.00116749
Iteration 9/25 | Loss: 0.00115716
Iteration 10/25 | Loss: 0.00114776
Iteration 11/25 | Loss: 0.00114319
Iteration 12/25 | Loss: 0.00113023
Iteration 13/25 | Loss: 0.00113104
Iteration 14/25 | Loss: 0.00112899
Iteration 15/25 | Loss: 0.00112020
Iteration 16/25 | Loss: 0.00111159
Iteration 17/25 | Loss: 0.00110844
Iteration 18/25 | Loss: 0.00111192
Iteration 19/25 | Loss: 0.00110774
Iteration 20/25 | Loss: 0.00110481
Iteration 21/25 | Loss: 0.00110112
Iteration 22/25 | Loss: 0.00110071
Iteration 23/25 | Loss: 0.00110061
Iteration 24/25 | Loss: 0.00110061
Iteration 25/25 | Loss: 0.00110060

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36328316
Iteration 2/25 | Loss: 0.00084605
Iteration 3/25 | Loss: 0.00084605
Iteration 4/25 | Loss: 0.00084605
Iteration 5/25 | Loss: 0.00084605
Iteration 6/25 | Loss: 0.00084605
Iteration 7/25 | Loss: 0.00084605
Iteration 8/25 | Loss: 0.00084605
Iteration 9/25 | Loss: 0.00084605
Iteration 10/25 | Loss: 0.00084605
Iteration 11/25 | Loss: 0.00084605
Iteration 12/25 | Loss: 0.00084605
Iteration 13/25 | Loss: 0.00084605
Iteration 14/25 | Loss: 0.00084605
Iteration 15/25 | Loss: 0.00084605
Iteration 16/25 | Loss: 0.00084605
Iteration 17/25 | Loss: 0.00084605
Iteration 18/25 | Loss: 0.00084605
Iteration 19/25 | Loss: 0.00084605
Iteration 20/25 | Loss: 0.00084605
Iteration 21/25 | Loss: 0.00084605
Iteration 22/25 | Loss: 0.00084605
Iteration 23/25 | Loss: 0.00084605
Iteration 24/25 | Loss: 0.00084605
Iteration 25/25 | Loss: 0.00084605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084605
Iteration 2/1000 | Loss: 0.00003044
Iteration 3/1000 | Loss: 0.00002294
Iteration 4/1000 | Loss: 0.00002060
Iteration 5/1000 | Loss: 0.00001937
Iteration 6/1000 | Loss: 0.00019959
Iteration 7/1000 | Loss: 0.00038596
Iteration 8/1000 | Loss: 0.00029246
Iteration 9/1000 | Loss: 0.00036603
Iteration 10/1000 | Loss: 0.00045596
Iteration 11/1000 | Loss: 0.00048039
Iteration 12/1000 | Loss: 0.00043716
Iteration 13/1000 | Loss: 0.00045438
Iteration 14/1000 | Loss: 0.00030467
Iteration 15/1000 | Loss: 0.00030952
Iteration 16/1000 | Loss: 0.00031962
Iteration 17/1000 | Loss: 0.00028437
Iteration 18/1000 | Loss: 0.00024381
Iteration 19/1000 | Loss: 0.00026788
Iteration 20/1000 | Loss: 0.00018722
Iteration 21/1000 | Loss: 0.00017433
Iteration 22/1000 | Loss: 0.00020062
Iteration 23/1000 | Loss: 0.00002353
Iteration 24/1000 | Loss: 0.00002121
Iteration 25/1000 | Loss: 0.00002000
Iteration 26/1000 | Loss: 0.00001895
Iteration 27/1000 | Loss: 0.00001828
Iteration 28/1000 | Loss: 0.00019933
Iteration 29/1000 | Loss: 0.00038374
Iteration 30/1000 | Loss: 0.00014688
Iteration 31/1000 | Loss: 0.00006976
Iteration 32/1000 | Loss: 0.00005180
Iteration 33/1000 | Loss: 0.00002041
Iteration 34/1000 | Loss: 0.00001846
Iteration 35/1000 | Loss: 0.00001800
Iteration 36/1000 | Loss: 0.00001755
Iteration 37/1000 | Loss: 0.00019897
Iteration 38/1000 | Loss: 0.00002890
Iteration 39/1000 | Loss: 0.00018786
Iteration 40/1000 | Loss: 0.00042676
Iteration 41/1000 | Loss: 0.00011945
Iteration 42/1000 | Loss: 0.00002259
Iteration 43/1000 | Loss: 0.00021781
Iteration 44/1000 | Loss: 0.00010885
Iteration 45/1000 | Loss: 0.00009454
Iteration 46/1000 | Loss: 0.00002666
Iteration 47/1000 | Loss: 0.00018020
Iteration 48/1000 | Loss: 0.00015743
Iteration 49/1000 | Loss: 0.00015708
Iteration 50/1000 | Loss: 0.00002674
Iteration 51/1000 | Loss: 0.00002498
Iteration 52/1000 | Loss: 0.00083376
Iteration 53/1000 | Loss: 0.00018130
Iteration 54/1000 | Loss: 0.00016794
Iteration 55/1000 | Loss: 0.00003800
Iteration 56/1000 | Loss: 0.00002880
Iteration 57/1000 | Loss: 0.00002486
Iteration 58/1000 | Loss: 0.00002296
Iteration 59/1000 | Loss: 0.00019719
Iteration 60/1000 | Loss: 0.00013869
Iteration 61/1000 | Loss: 0.00019167
Iteration 62/1000 | Loss: 0.00005576
Iteration 63/1000 | Loss: 0.00002425
Iteration 64/1000 | Loss: 0.00002274
Iteration 65/1000 | Loss: 0.00002095
Iteration 66/1000 | Loss: 0.00001906
Iteration 67/1000 | Loss: 0.00001801
Iteration 68/1000 | Loss: 0.00002368
Iteration 69/1000 | Loss: 0.00001942
Iteration 70/1000 | Loss: 0.00001828
Iteration 71/1000 | Loss: 0.00001758
Iteration 72/1000 | Loss: 0.00001653
Iteration 73/1000 | Loss: 0.00001585
Iteration 74/1000 | Loss: 0.00001555
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001530
Iteration 77/1000 | Loss: 0.00001519
Iteration 78/1000 | Loss: 0.00001517
Iteration 79/1000 | Loss: 0.00001516
Iteration 80/1000 | Loss: 0.00001515
Iteration 81/1000 | Loss: 0.00001510
Iteration 82/1000 | Loss: 0.00001507
Iteration 83/1000 | Loss: 0.00001507
Iteration 84/1000 | Loss: 0.00001504
Iteration 85/1000 | Loss: 0.00001504
Iteration 86/1000 | Loss: 0.00001501
Iteration 87/1000 | Loss: 0.00001500
Iteration 88/1000 | Loss: 0.00001500
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001498
Iteration 91/1000 | Loss: 0.00001498
Iteration 92/1000 | Loss: 0.00001497
Iteration 93/1000 | Loss: 0.00001496
Iteration 94/1000 | Loss: 0.00001495
Iteration 95/1000 | Loss: 0.00001494
Iteration 96/1000 | Loss: 0.00001494
Iteration 97/1000 | Loss: 0.00001493
Iteration 98/1000 | Loss: 0.00001493
Iteration 99/1000 | Loss: 0.00001492
Iteration 100/1000 | Loss: 0.00001491
Iteration 101/1000 | Loss: 0.00001491
Iteration 102/1000 | Loss: 0.00001491
Iteration 103/1000 | Loss: 0.00001491
Iteration 104/1000 | Loss: 0.00001491
Iteration 105/1000 | Loss: 0.00001491
Iteration 106/1000 | Loss: 0.00001491
Iteration 107/1000 | Loss: 0.00001491
Iteration 108/1000 | Loss: 0.00001490
Iteration 109/1000 | Loss: 0.00001490
Iteration 110/1000 | Loss: 0.00001490
Iteration 111/1000 | Loss: 0.00001490
Iteration 112/1000 | Loss: 0.00001490
Iteration 113/1000 | Loss: 0.00001490
Iteration 114/1000 | Loss: 0.00001490
Iteration 115/1000 | Loss: 0.00001490
Iteration 116/1000 | Loss: 0.00001490
Iteration 117/1000 | Loss: 0.00001490
Iteration 118/1000 | Loss: 0.00001490
Iteration 119/1000 | Loss: 0.00001489
Iteration 120/1000 | Loss: 0.00001489
Iteration 121/1000 | Loss: 0.00001489
Iteration 122/1000 | Loss: 0.00001489
Iteration 123/1000 | Loss: 0.00001489
Iteration 124/1000 | Loss: 0.00001489
Iteration 125/1000 | Loss: 0.00001488
Iteration 126/1000 | Loss: 0.00001488
Iteration 127/1000 | Loss: 0.00001488
Iteration 128/1000 | Loss: 0.00001488
Iteration 129/1000 | Loss: 0.00001488
Iteration 130/1000 | Loss: 0.00001488
Iteration 131/1000 | Loss: 0.00001488
Iteration 132/1000 | Loss: 0.00001487
Iteration 133/1000 | Loss: 0.00001487
Iteration 134/1000 | Loss: 0.00001487
Iteration 135/1000 | Loss: 0.00001487
Iteration 136/1000 | Loss: 0.00001487
Iteration 137/1000 | Loss: 0.00001487
Iteration 138/1000 | Loss: 0.00001487
Iteration 139/1000 | Loss: 0.00001487
Iteration 140/1000 | Loss: 0.00001487
Iteration 141/1000 | Loss: 0.00001487
Iteration 142/1000 | Loss: 0.00001487
Iteration 143/1000 | Loss: 0.00001487
Iteration 144/1000 | Loss: 0.00001487
Iteration 145/1000 | Loss: 0.00001487
Iteration 146/1000 | Loss: 0.00001487
Iteration 147/1000 | Loss: 0.00001487
Iteration 148/1000 | Loss: 0.00001486
Iteration 149/1000 | Loss: 0.00001486
Iteration 150/1000 | Loss: 0.00001486
Iteration 151/1000 | Loss: 0.00001486
Iteration 152/1000 | Loss: 0.00001486
Iteration 153/1000 | Loss: 0.00001486
Iteration 154/1000 | Loss: 0.00001486
Iteration 155/1000 | Loss: 0.00001486
Iteration 156/1000 | Loss: 0.00001486
Iteration 157/1000 | Loss: 0.00001486
Iteration 158/1000 | Loss: 0.00001486
Iteration 159/1000 | Loss: 0.00001486
Iteration 160/1000 | Loss: 0.00001486
Iteration 161/1000 | Loss: 0.00001486
Iteration 162/1000 | Loss: 0.00001486
Iteration 163/1000 | Loss: 0.00001486
Iteration 164/1000 | Loss: 0.00001485
Iteration 165/1000 | Loss: 0.00001485
Iteration 166/1000 | Loss: 0.00001485
Iteration 167/1000 | Loss: 0.00001485
Iteration 168/1000 | Loss: 0.00001485
Iteration 169/1000 | Loss: 0.00001485
Iteration 170/1000 | Loss: 0.00001485
Iteration 171/1000 | Loss: 0.00001485
Iteration 172/1000 | Loss: 0.00001485
Iteration 173/1000 | Loss: 0.00001485
Iteration 174/1000 | Loss: 0.00001485
Iteration 175/1000 | Loss: 0.00001485
Iteration 176/1000 | Loss: 0.00001485
Iteration 177/1000 | Loss: 0.00001485
Iteration 178/1000 | Loss: 0.00001485
Iteration 179/1000 | Loss: 0.00001485
Iteration 180/1000 | Loss: 0.00001485
Iteration 181/1000 | Loss: 0.00001485
Iteration 182/1000 | Loss: 0.00001485
Iteration 183/1000 | Loss: 0.00001485
Iteration 184/1000 | Loss: 0.00001485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.4854698747512884e-05, 1.4854698747512884e-05, 1.4854698747512884e-05, 1.4854698747512884e-05, 1.4854698747512884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4854698747512884e-05

Optimization complete. Final v2v error: 3.1767961978912354 mm

Highest mean error: 6.0014801025390625 mm for frame 84

Lowest mean error: 2.9349477291107178 mm for frame 48

Saving results

Total time: 158.6334810256958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917914
Iteration 2/25 | Loss: 0.00148005
Iteration 3/25 | Loss: 0.00132826
Iteration 4/25 | Loss: 0.00114886
Iteration 5/25 | Loss: 0.00113860
Iteration 6/25 | Loss: 0.00113641
Iteration 7/25 | Loss: 0.00114042
Iteration 8/25 | Loss: 0.00114368
Iteration 9/25 | Loss: 0.00113904
Iteration 10/25 | Loss: 0.00113429
Iteration 11/25 | Loss: 0.00113259
Iteration 12/25 | Loss: 0.00113250
Iteration 13/25 | Loss: 0.00113250
Iteration 14/25 | Loss: 0.00113250
Iteration 15/25 | Loss: 0.00113250
Iteration 16/25 | Loss: 0.00113250
Iteration 17/25 | Loss: 0.00113250
Iteration 18/25 | Loss: 0.00113250
Iteration 19/25 | Loss: 0.00113250
Iteration 20/25 | Loss: 0.00113250
Iteration 21/25 | Loss: 0.00113250
Iteration 22/25 | Loss: 0.00113250
Iteration 23/25 | Loss: 0.00113250
Iteration 24/25 | Loss: 0.00113250
Iteration 25/25 | Loss: 0.00113250

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48076165
Iteration 2/25 | Loss: 0.00083806
Iteration 3/25 | Loss: 0.00077916
Iteration 4/25 | Loss: 0.00077915
Iteration 5/25 | Loss: 0.00077915
Iteration 6/25 | Loss: 0.00077915
Iteration 7/25 | Loss: 0.00077915
Iteration 8/25 | Loss: 0.00077915
Iteration 9/25 | Loss: 0.00077915
Iteration 10/25 | Loss: 0.00077915
Iteration 11/25 | Loss: 0.00077915
Iteration 12/25 | Loss: 0.00077915
Iteration 13/25 | Loss: 0.00077915
Iteration 14/25 | Loss: 0.00077915
Iteration 15/25 | Loss: 0.00077915
Iteration 16/25 | Loss: 0.00077915
Iteration 17/25 | Loss: 0.00077915
Iteration 18/25 | Loss: 0.00077915
Iteration 19/25 | Loss: 0.00077915
Iteration 20/25 | Loss: 0.00077915
Iteration 21/25 | Loss: 0.00077915
Iteration 22/25 | Loss: 0.00077915
Iteration 23/25 | Loss: 0.00077915
Iteration 24/25 | Loss: 0.00077915
Iteration 25/25 | Loss: 0.00077915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077915
Iteration 2/1000 | Loss: 0.00007396
Iteration 3/1000 | Loss: 0.00014098
Iteration 4/1000 | Loss: 0.00001869
Iteration 5/1000 | Loss: 0.00001755
Iteration 6/1000 | Loss: 0.00001675
Iteration 7/1000 | Loss: 0.00001623
Iteration 8/1000 | Loss: 0.00001582
Iteration 9/1000 | Loss: 0.00001558
Iteration 10/1000 | Loss: 0.00001529
Iteration 11/1000 | Loss: 0.00001507
Iteration 12/1000 | Loss: 0.00001499
Iteration 13/1000 | Loss: 0.00001497
Iteration 14/1000 | Loss: 0.00001489
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001480
Iteration 17/1000 | Loss: 0.00001476
Iteration 18/1000 | Loss: 0.00001475
Iteration 19/1000 | Loss: 0.00001475
Iteration 20/1000 | Loss: 0.00001473
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001471
Iteration 23/1000 | Loss: 0.00001471
Iteration 24/1000 | Loss: 0.00001470
Iteration 25/1000 | Loss: 0.00001470
Iteration 26/1000 | Loss: 0.00001470
Iteration 27/1000 | Loss: 0.00001470
Iteration 28/1000 | Loss: 0.00001470
Iteration 29/1000 | Loss: 0.00001470
Iteration 30/1000 | Loss: 0.00001470
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001467
Iteration 34/1000 | Loss: 0.00001467
Iteration 35/1000 | Loss: 0.00001466
Iteration 36/1000 | Loss: 0.00001466
Iteration 37/1000 | Loss: 0.00001466
Iteration 38/1000 | Loss: 0.00001466
Iteration 39/1000 | Loss: 0.00001465
Iteration 40/1000 | Loss: 0.00001465
Iteration 41/1000 | Loss: 0.00001463
Iteration 42/1000 | Loss: 0.00001463
Iteration 43/1000 | Loss: 0.00001463
Iteration 44/1000 | Loss: 0.00001463
Iteration 45/1000 | Loss: 0.00001463
Iteration 46/1000 | Loss: 0.00001463
Iteration 47/1000 | Loss: 0.00001462
Iteration 48/1000 | Loss: 0.00001462
Iteration 49/1000 | Loss: 0.00001462
Iteration 50/1000 | Loss: 0.00001462
Iteration 51/1000 | Loss: 0.00001461
Iteration 52/1000 | Loss: 0.00001461
Iteration 53/1000 | Loss: 0.00001461
Iteration 54/1000 | Loss: 0.00001460
Iteration 55/1000 | Loss: 0.00001460
Iteration 56/1000 | Loss: 0.00001460
Iteration 57/1000 | Loss: 0.00001459
Iteration 58/1000 | Loss: 0.00001459
Iteration 59/1000 | Loss: 0.00001459
Iteration 60/1000 | Loss: 0.00001458
Iteration 61/1000 | Loss: 0.00001458
Iteration 62/1000 | Loss: 0.00001458
Iteration 63/1000 | Loss: 0.00001458
Iteration 64/1000 | Loss: 0.00001458
Iteration 65/1000 | Loss: 0.00001458
Iteration 66/1000 | Loss: 0.00001457
Iteration 67/1000 | Loss: 0.00001457
Iteration 68/1000 | Loss: 0.00001457
Iteration 69/1000 | Loss: 0.00001457
Iteration 70/1000 | Loss: 0.00001457
Iteration 71/1000 | Loss: 0.00001457
Iteration 72/1000 | Loss: 0.00001457
Iteration 73/1000 | Loss: 0.00001457
Iteration 74/1000 | Loss: 0.00001457
Iteration 75/1000 | Loss: 0.00001457
Iteration 76/1000 | Loss: 0.00001457
Iteration 77/1000 | Loss: 0.00001457
Iteration 78/1000 | Loss: 0.00001457
Iteration 79/1000 | Loss: 0.00001457
Iteration 80/1000 | Loss: 0.00001456
Iteration 81/1000 | Loss: 0.00001456
Iteration 82/1000 | Loss: 0.00001456
Iteration 83/1000 | Loss: 0.00001456
Iteration 84/1000 | Loss: 0.00001455
Iteration 85/1000 | Loss: 0.00001455
Iteration 86/1000 | Loss: 0.00001455
Iteration 87/1000 | Loss: 0.00001455
Iteration 88/1000 | Loss: 0.00001454
Iteration 89/1000 | Loss: 0.00001454
Iteration 90/1000 | Loss: 0.00001454
Iteration 91/1000 | Loss: 0.00001453
Iteration 92/1000 | Loss: 0.00001453
Iteration 93/1000 | Loss: 0.00001453
Iteration 94/1000 | Loss: 0.00001453
Iteration 95/1000 | Loss: 0.00001453
Iteration 96/1000 | Loss: 0.00001453
Iteration 97/1000 | Loss: 0.00001453
Iteration 98/1000 | Loss: 0.00001453
Iteration 99/1000 | Loss: 0.00001453
Iteration 100/1000 | Loss: 0.00001452
Iteration 101/1000 | Loss: 0.00001452
Iteration 102/1000 | Loss: 0.00001452
Iteration 103/1000 | Loss: 0.00001452
Iteration 104/1000 | Loss: 0.00001452
Iteration 105/1000 | Loss: 0.00001452
Iteration 106/1000 | Loss: 0.00001452
Iteration 107/1000 | Loss: 0.00001452
Iteration 108/1000 | Loss: 0.00001452
Iteration 109/1000 | Loss: 0.00001452
Iteration 110/1000 | Loss: 0.00001452
Iteration 111/1000 | Loss: 0.00001452
Iteration 112/1000 | Loss: 0.00001452
Iteration 113/1000 | Loss: 0.00001452
Iteration 114/1000 | Loss: 0.00001452
Iteration 115/1000 | Loss: 0.00001452
Iteration 116/1000 | Loss: 0.00001452
Iteration 117/1000 | Loss: 0.00001452
Iteration 118/1000 | Loss: 0.00001452
Iteration 119/1000 | Loss: 0.00001452
Iteration 120/1000 | Loss: 0.00001452
Iteration 121/1000 | Loss: 0.00001452
Iteration 122/1000 | Loss: 0.00001452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.452345895813778e-05, 1.452345895813778e-05, 1.452345895813778e-05, 1.452345895813778e-05, 1.452345895813778e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.452345895813778e-05

Optimization complete. Final v2v error: 3.2294375896453857 mm

Highest mean error: 3.9168615341186523 mm for frame 39

Lowest mean error: 2.7549405097961426 mm for frame 121

Saving results

Total time: 48.2007942199707
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794100
Iteration 2/25 | Loss: 0.00138812
Iteration 3/25 | Loss: 0.00111219
Iteration 4/25 | Loss: 0.00109538
Iteration 5/25 | Loss: 0.00108979
Iteration 6/25 | Loss: 0.00108806
Iteration 7/25 | Loss: 0.00108806
Iteration 8/25 | Loss: 0.00108806
Iteration 9/25 | Loss: 0.00108806
Iteration 10/25 | Loss: 0.00108806
Iteration 11/25 | Loss: 0.00108806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001088057062588632, 0.001088057062588632, 0.001088057062588632, 0.001088057062588632, 0.001088057062588632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001088057062588632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11306643
Iteration 2/25 | Loss: 0.00084390
Iteration 3/25 | Loss: 0.00084387
Iteration 4/25 | Loss: 0.00084387
Iteration 5/25 | Loss: 0.00084387
Iteration 6/25 | Loss: 0.00084387
Iteration 7/25 | Loss: 0.00084387
Iteration 8/25 | Loss: 0.00084387
Iteration 9/25 | Loss: 0.00084387
Iteration 10/25 | Loss: 0.00084387
Iteration 11/25 | Loss: 0.00084387
Iteration 12/25 | Loss: 0.00084387
Iteration 13/25 | Loss: 0.00084387
Iteration 14/25 | Loss: 0.00084387
Iteration 15/25 | Loss: 0.00084387
Iteration 16/25 | Loss: 0.00084387
Iteration 17/25 | Loss: 0.00084387
Iteration 18/25 | Loss: 0.00084387
Iteration 19/25 | Loss: 0.00084387
Iteration 20/25 | Loss: 0.00084387
Iteration 21/25 | Loss: 0.00084387
Iteration 22/25 | Loss: 0.00084387
Iteration 23/25 | Loss: 0.00084387
Iteration 24/25 | Loss: 0.00084387
Iteration 25/25 | Loss: 0.00084387

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084387
Iteration 2/1000 | Loss: 0.00004030
Iteration 3/1000 | Loss: 0.00002749
Iteration 4/1000 | Loss: 0.00002166
Iteration 5/1000 | Loss: 0.00001933
Iteration 6/1000 | Loss: 0.00001783
Iteration 7/1000 | Loss: 0.00001675
Iteration 8/1000 | Loss: 0.00001599
Iteration 9/1000 | Loss: 0.00001557
Iteration 10/1000 | Loss: 0.00001519
Iteration 11/1000 | Loss: 0.00001474
Iteration 12/1000 | Loss: 0.00001450
Iteration 13/1000 | Loss: 0.00001432
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001415
Iteration 16/1000 | Loss: 0.00001412
Iteration 17/1000 | Loss: 0.00001409
Iteration 18/1000 | Loss: 0.00001409
Iteration 19/1000 | Loss: 0.00001408
Iteration 20/1000 | Loss: 0.00001395
Iteration 21/1000 | Loss: 0.00001392
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001383
Iteration 24/1000 | Loss: 0.00001379
Iteration 25/1000 | Loss: 0.00001378
Iteration 26/1000 | Loss: 0.00001378
Iteration 27/1000 | Loss: 0.00001375
Iteration 28/1000 | Loss: 0.00001372
Iteration 29/1000 | Loss: 0.00001372
Iteration 30/1000 | Loss: 0.00001372
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001371
Iteration 33/1000 | Loss: 0.00001371
Iteration 34/1000 | Loss: 0.00001371
Iteration 35/1000 | Loss: 0.00001371
Iteration 36/1000 | Loss: 0.00001371
Iteration 37/1000 | Loss: 0.00001371
Iteration 38/1000 | Loss: 0.00001371
Iteration 39/1000 | Loss: 0.00001371
Iteration 40/1000 | Loss: 0.00001371
Iteration 41/1000 | Loss: 0.00001371
Iteration 42/1000 | Loss: 0.00001371
Iteration 43/1000 | Loss: 0.00001370
Iteration 44/1000 | Loss: 0.00001370
Iteration 45/1000 | Loss: 0.00001370
Iteration 46/1000 | Loss: 0.00001368
Iteration 47/1000 | Loss: 0.00001367
Iteration 48/1000 | Loss: 0.00001366
Iteration 49/1000 | Loss: 0.00001366
Iteration 50/1000 | Loss: 0.00001366
Iteration 51/1000 | Loss: 0.00001365
Iteration 52/1000 | Loss: 0.00001364
Iteration 53/1000 | Loss: 0.00001364
Iteration 54/1000 | Loss: 0.00001363
Iteration 55/1000 | Loss: 0.00001362
Iteration 56/1000 | Loss: 0.00001362
Iteration 57/1000 | Loss: 0.00001361
Iteration 58/1000 | Loss: 0.00001361
Iteration 59/1000 | Loss: 0.00001361
Iteration 60/1000 | Loss: 0.00001360
Iteration 61/1000 | Loss: 0.00001360
Iteration 62/1000 | Loss: 0.00001359
Iteration 63/1000 | Loss: 0.00001359
Iteration 64/1000 | Loss: 0.00001359
Iteration 65/1000 | Loss: 0.00001358
Iteration 66/1000 | Loss: 0.00001358
Iteration 67/1000 | Loss: 0.00001358
Iteration 68/1000 | Loss: 0.00001358
Iteration 69/1000 | Loss: 0.00001357
Iteration 70/1000 | Loss: 0.00001357
Iteration 71/1000 | Loss: 0.00001357
Iteration 72/1000 | Loss: 0.00001357
Iteration 73/1000 | Loss: 0.00001356
Iteration 74/1000 | Loss: 0.00001356
Iteration 75/1000 | Loss: 0.00001356
Iteration 76/1000 | Loss: 0.00001355
Iteration 77/1000 | Loss: 0.00001355
Iteration 78/1000 | Loss: 0.00001355
Iteration 79/1000 | Loss: 0.00001355
Iteration 80/1000 | Loss: 0.00001355
Iteration 81/1000 | Loss: 0.00001354
Iteration 82/1000 | Loss: 0.00001354
Iteration 83/1000 | Loss: 0.00001354
Iteration 84/1000 | Loss: 0.00001353
Iteration 85/1000 | Loss: 0.00001353
Iteration 86/1000 | Loss: 0.00001353
Iteration 87/1000 | Loss: 0.00001353
Iteration 88/1000 | Loss: 0.00001353
Iteration 89/1000 | Loss: 0.00001352
Iteration 90/1000 | Loss: 0.00001352
Iteration 91/1000 | Loss: 0.00001352
Iteration 92/1000 | Loss: 0.00001352
Iteration 93/1000 | Loss: 0.00001352
Iteration 94/1000 | Loss: 0.00001352
Iteration 95/1000 | Loss: 0.00001352
Iteration 96/1000 | Loss: 0.00001352
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001352
Iteration 100/1000 | Loss: 0.00001352
Iteration 101/1000 | Loss: 0.00001352
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001352
Iteration 104/1000 | Loss: 0.00001352
Iteration 105/1000 | Loss: 0.00001352
Iteration 106/1000 | Loss: 0.00001352
Iteration 107/1000 | Loss: 0.00001352
Iteration 108/1000 | Loss: 0.00001352
Iteration 109/1000 | Loss: 0.00001352
Iteration 110/1000 | Loss: 0.00001352
Iteration 111/1000 | Loss: 0.00001352
Iteration 112/1000 | Loss: 0.00001352
Iteration 113/1000 | Loss: 0.00001352
Iteration 114/1000 | Loss: 0.00001352
Iteration 115/1000 | Loss: 0.00001352
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001352
Iteration 118/1000 | Loss: 0.00001352
Iteration 119/1000 | Loss: 0.00001352
Iteration 120/1000 | Loss: 0.00001352
Iteration 121/1000 | Loss: 0.00001352
Iteration 122/1000 | Loss: 0.00001352
Iteration 123/1000 | Loss: 0.00001352
Iteration 124/1000 | Loss: 0.00001352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.3515436876332387e-05, 1.3515436876332387e-05, 1.3515436876332387e-05, 1.3515436876332387e-05, 1.3515436876332387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3515436876332387e-05

Optimization complete. Final v2v error: 2.9975929260253906 mm

Highest mean error: 4.15162992477417 mm for frame 82

Lowest mean error: 2.285273790359497 mm for frame 197

Saving results

Total time: 46.266761302948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391612
Iteration 2/25 | Loss: 0.00115036
Iteration 3/25 | Loss: 0.00107154
Iteration 4/25 | Loss: 0.00106060
Iteration 5/25 | Loss: 0.00105683
Iteration 6/25 | Loss: 0.00105576
Iteration 7/25 | Loss: 0.00105576
Iteration 8/25 | Loss: 0.00105576
Iteration 9/25 | Loss: 0.00105576
Iteration 10/25 | Loss: 0.00105576
Iteration 11/25 | Loss: 0.00105576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010557635687291622, 0.0010557635687291622, 0.0010557635687291622, 0.0010557635687291622, 0.0010557635687291622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010557635687291622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.58991766
Iteration 2/25 | Loss: 0.00086338
Iteration 3/25 | Loss: 0.00086337
Iteration 4/25 | Loss: 0.00086337
Iteration 5/25 | Loss: 0.00086337
Iteration 6/25 | Loss: 0.00086337
Iteration 7/25 | Loss: 0.00086337
Iteration 8/25 | Loss: 0.00086337
Iteration 9/25 | Loss: 0.00086337
Iteration 10/25 | Loss: 0.00086337
Iteration 11/25 | Loss: 0.00086337
Iteration 12/25 | Loss: 0.00086337
Iteration 13/25 | Loss: 0.00086337
Iteration 14/25 | Loss: 0.00086337
Iteration 15/25 | Loss: 0.00086337
Iteration 16/25 | Loss: 0.00086337
Iteration 17/25 | Loss: 0.00086337
Iteration 18/25 | Loss: 0.00086337
Iteration 19/25 | Loss: 0.00086337
Iteration 20/25 | Loss: 0.00086337
Iteration 21/25 | Loss: 0.00086337
Iteration 22/25 | Loss: 0.00086337
Iteration 23/25 | Loss: 0.00086337
Iteration 24/25 | Loss: 0.00086337
Iteration 25/25 | Loss: 0.00086337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086337
Iteration 2/1000 | Loss: 0.00002171
Iteration 3/1000 | Loss: 0.00001336
Iteration 4/1000 | Loss: 0.00001112
Iteration 5/1000 | Loss: 0.00001039
Iteration 6/1000 | Loss: 0.00001005
Iteration 7/1000 | Loss: 0.00000977
Iteration 8/1000 | Loss: 0.00000946
Iteration 9/1000 | Loss: 0.00000939
Iteration 10/1000 | Loss: 0.00000933
Iteration 11/1000 | Loss: 0.00000928
Iteration 12/1000 | Loss: 0.00000924
Iteration 13/1000 | Loss: 0.00000906
Iteration 14/1000 | Loss: 0.00000904
Iteration 15/1000 | Loss: 0.00000903
Iteration 16/1000 | Loss: 0.00000903
Iteration 17/1000 | Loss: 0.00000898
Iteration 18/1000 | Loss: 0.00000896
Iteration 19/1000 | Loss: 0.00000896
Iteration 20/1000 | Loss: 0.00000895
Iteration 21/1000 | Loss: 0.00000893
Iteration 22/1000 | Loss: 0.00000892
Iteration 23/1000 | Loss: 0.00000892
Iteration 24/1000 | Loss: 0.00000890
Iteration 25/1000 | Loss: 0.00000890
Iteration 26/1000 | Loss: 0.00000890
Iteration 27/1000 | Loss: 0.00000889
Iteration 28/1000 | Loss: 0.00000889
Iteration 29/1000 | Loss: 0.00000886
Iteration 30/1000 | Loss: 0.00000884
Iteration 31/1000 | Loss: 0.00000882
Iteration 32/1000 | Loss: 0.00000882
Iteration 33/1000 | Loss: 0.00000882
Iteration 34/1000 | Loss: 0.00000881
Iteration 35/1000 | Loss: 0.00000880
Iteration 36/1000 | Loss: 0.00000879
Iteration 37/1000 | Loss: 0.00000877
Iteration 38/1000 | Loss: 0.00000876
Iteration 39/1000 | Loss: 0.00000876
Iteration 40/1000 | Loss: 0.00000872
Iteration 41/1000 | Loss: 0.00000870
Iteration 42/1000 | Loss: 0.00000870
Iteration 43/1000 | Loss: 0.00000869
Iteration 44/1000 | Loss: 0.00000869
Iteration 45/1000 | Loss: 0.00000869
Iteration 46/1000 | Loss: 0.00000868
Iteration 47/1000 | Loss: 0.00000868
Iteration 48/1000 | Loss: 0.00000868
Iteration 49/1000 | Loss: 0.00000868
Iteration 50/1000 | Loss: 0.00000867
Iteration 51/1000 | Loss: 0.00000866
Iteration 52/1000 | Loss: 0.00000866
Iteration 53/1000 | Loss: 0.00000865
Iteration 54/1000 | Loss: 0.00000865
Iteration 55/1000 | Loss: 0.00000865
Iteration 56/1000 | Loss: 0.00000864
Iteration 57/1000 | Loss: 0.00000864
Iteration 58/1000 | Loss: 0.00000863
Iteration 59/1000 | Loss: 0.00000862
Iteration 60/1000 | Loss: 0.00000862
Iteration 61/1000 | Loss: 0.00000861
Iteration 62/1000 | Loss: 0.00000861
Iteration 63/1000 | Loss: 0.00000860
Iteration 64/1000 | Loss: 0.00000860
Iteration 65/1000 | Loss: 0.00000860
Iteration 66/1000 | Loss: 0.00000860
Iteration 67/1000 | Loss: 0.00000860
Iteration 68/1000 | Loss: 0.00000860
Iteration 69/1000 | Loss: 0.00000859
Iteration 70/1000 | Loss: 0.00000859
Iteration 71/1000 | Loss: 0.00000859
Iteration 72/1000 | Loss: 0.00000858
Iteration 73/1000 | Loss: 0.00000858
Iteration 74/1000 | Loss: 0.00000858
Iteration 75/1000 | Loss: 0.00000857
Iteration 76/1000 | Loss: 0.00000857
Iteration 77/1000 | Loss: 0.00000856
Iteration 78/1000 | Loss: 0.00000856
Iteration 79/1000 | Loss: 0.00000856
Iteration 80/1000 | Loss: 0.00000856
Iteration 81/1000 | Loss: 0.00000856
Iteration 82/1000 | Loss: 0.00000856
Iteration 83/1000 | Loss: 0.00000856
Iteration 84/1000 | Loss: 0.00000855
Iteration 85/1000 | Loss: 0.00000855
Iteration 86/1000 | Loss: 0.00000855
Iteration 87/1000 | Loss: 0.00000855
Iteration 88/1000 | Loss: 0.00000855
Iteration 89/1000 | Loss: 0.00000855
Iteration 90/1000 | Loss: 0.00000855
Iteration 91/1000 | Loss: 0.00000855
Iteration 92/1000 | Loss: 0.00000855
Iteration 93/1000 | Loss: 0.00000855
Iteration 94/1000 | Loss: 0.00000855
Iteration 95/1000 | Loss: 0.00000855
Iteration 96/1000 | Loss: 0.00000854
Iteration 97/1000 | Loss: 0.00000854
Iteration 98/1000 | Loss: 0.00000854
Iteration 99/1000 | Loss: 0.00000853
Iteration 100/1000 | Loss: 0.00000853
Iteration 101/1000 | Loss: 0.00000853
Iteration 102/1000 | Loss: 0.00000853
Iteration 103/1000 | Loss: 0.00000853
Iteration 104/1000 | Loss: 0.00000853
Iteration 105/1000 | Loss: 0.00000853
Iteration 106/1000 | Loss: 0.00000852
Iteration 107/1000 | Loss: 0.00000852
Iteration 108/1000 | Loss: 0.00000852
Iteration 109/1000 | Loss: 0.00000852
Iteration 110/1000 | Loss: 0.00000852
Iteration 111/1000 | Loss: 0.00000851
Iteration 112/1000 | Loss: 0.00000851
Iteration 113/1000 | Loss: 0.00000851
Iteration 114/1000 | Loss: 0.00000851
Iteration 115/1000 | Loss: 0.00000851
Iteration 116/1000 | Loss: 0.00000851
Iteration 117/1000 | Loss: 0.00000850
Iteration 118/1000 | Loss: 0.00000850
Iteration 119/1000 | Loss: 0.00000850
Iteration 120/1000 | Loss: 0.00000850
Iteration 121/1000 | Loss: 0.00000850
Iteration 122/1000 | Loss: 0.00000850
Iteration 123/1000 | Loss: 0.00000850
Iteration 124/1000 | Loss: 0.00000850
Iteration 125/1000 | Loss: 0.00000849
Iteration 126/1000 | Loss: 0.00000849
Iteration 127/1000 | Loss: 0.00000848
Iteration 128/1000 | Loss: 0.00000848
Iteration 129/1000 | Loss: 0.00000848
Iteration 130/1000 | Loss: 0.00000848
Iteration 131/1000 | Loss: 0.00000848
Iteration 132/1000 | Loss: 0.00000848
Iteration 133/1000 | Loss: 0.00000848
Iteration 134/1000 | Loss: 0.00000847
Iteration 135/1000 | Loss: 0.00000847
Iteration 136/1000 | Loss: 0.00000847
Iteration 137/1000 | Loss: 0.00000847
Iteration 138/1000 | Loss: 0.00000847
Iteration 139/1000 | Loss: 0.00000847
Iteration 140/1000 | Loss: 0.00000847
Iteration 141/1000 | Loss: 0.00000847
Iteration 142/1000 | Loss: 0.00000847
Iteration 143/1000 | Loss: 0.00000847
Iteration 144/1000 | Loss: 0.00000847
Iteration 145/1000 | Loss: 0.00000847
Iteration 146/1000 | Loss: 0.00000847
Iteration 147/1000 | Loss: 0.00000847
Iteration 148/1000 | Loss: 0.00000847
Iteration 149/1000 | Loss: 0.00000847
Iteration 150/1000 | Loss: 0.00000847
Iteration 151/1000 | Loss: 0.00000847
Iteration 152/1000 | Loss: 0.00000847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [8.465857717965264e-06, 8.465857717965264e-06, 8.465857717965264e-06, 8.465857717965264e-06, 8.465857717965264e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.465857717965264e-06

Optimization complete. Final v2v error: 2.5245769023895264 mm

Highest mean error: 2.8064379692077637 mm for frame 99

Lowest mean error: 2.3146684169769287 mm for frame 12

Saving results

Total time: 35.40641403198242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944412
Iteration 2/25 | Loss: 0.00166509
Iteration 3/25 | Loss: 0.00130115
Iteration 4/25 | Loss: 0.00124666
Iteration 5/25 | Loss: 0.00125072
Iteration 6/25 | Loss: 0.00126376
Iteration 7/25 | Loss: 0.00122134
Iteration 8/25 | Loss: 0.00122037
Iteration 9/25 | Loss: 0.00121912
Iteration 10/25 | Loss: 0.00121932
Iteration 11/25 | Loss: 0.00120619
Iteration 12/25 | Loss: 0.00119706
Iteration 13/25 | Loss: 0.00119599
Iteration 14/25 | Loss: 0.00119577
Iteration 15/25 | Loss: 0.00119568
Iteration 16/25 | Loss: 0.00119553
Iteration 17/25 | Loss: 0.00123189
Iteration 18/25 | Loss: 0.00129547
Iteration 19/25 | Loss: 0.00122657
Iteration 20/25 | Loss: 0.00120749
Iteration 21/25 | Loss: 0.00115777
Iteration 22/25 | Loss: 0.00115563
Iteration 23/25 | Loss: 0.00115521
Iteration 24/25 | Loss: 0.00115508
Iteration 25/25 | Loss: 0.00115506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.91249800
Iteration 2/25 | Loss: 0.00073584
Iteration 3/25 | Loss: 0.00073582
Iteration 4/25 | Loss: 0.00073581
Iteration 5/25 | Loss: 0.00073581
Iteration 6/25 | Loss: 0.00073581
Iteration 7/25 | Loss: 0.00073581
Iteration 8/25 | Loss: 0.00073581
Iteration 9/25 | Loss: 0.00073581
Iteration 10/25 | Loss: 0.00073581
Iteration 11/25 | Loss: 0.00073581
Iteration 12/25 | Loss: 0.00073581
Iteration 13/25 | Loss: 0.00073581
Iteration 14/25 | Loss: 0.00073581
Iteration 15/25 | Loss: 0.00073581
Iteration 16/25 | Loss: 0.00073581
Iteration 17/25 | Loss: 0.00073581
Iteration 18/25 | Loss: 0.00073581
Iteration 19/25 | Loss: 0.00073581
Iteration 20/25 | Loss: 0.00073581
Iteration 21/25 | Loss: 0.00073581
Iteration 22/25 | Loss: 0.00073581
Iteration 23/25 | Loss: 0.00073581
Iteration 24/25 | Loss: 0.00073581
Iteration 25/25 | Loss: 0.00073581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073581
Iteration 2/1000 | Loss: 0.00003855
Iteration 3/1000 | Loss: 0.00002689
Iteration 4/1000 | Loss: 0.00002441
Iteration 5/1000 | Loss: 0.00002328
Iteration 6/1000 | Loss: 0.00235011
Iteration 7/1000 | Loss: 0.00089740
Iteration 8/1000 | Loss: 0.00337085
Iteration 9/1000 | Loss: 0.00264037
Iteration 10/1000 | Loss: 0.00146205
Iteration 11/1000 | Loss: 0.00157952
Iteration 12/1000 | Loss: 0.00169275
Iteration 13/1000 | Loss: 0.00146522
Iteration 14/1000 | Loss: 0.00143714
Iteration 15/1000 | Loss: 0.00220414
Iteration 16/1000 | Loss: 0.00107899
Iteration 17/1000 | Loss: 0.00136807
Iteration 18/1000 | Loss: 0.00167933
Iteration 19/1000 | Loss: 0.00101083
Iteration 20/1000 | Loss: 0.00146562
Iteration 21/1000 | Loss: 0.00122684
Iteration 22/1000 | Loss: 0.00077666
Iteration 23/1000 | Loss: 0.00090179
Iteration 24/1000 | Loss: 0.00048797
Iteration 25/1000 | Loss: 0.00123040
Iteration 26/1000 | Loss: 0.00080445
Iteration 27/1000 | Loss: 0.00112136
Iteration 28/1000 | Loss: 0.00030711
Iteration 29/1000 | Loss: 0.00005990
Iteration 30/1000 | Loss: 0.00004154
Iteration 31/1000 | Loss: 0.00003052
Iteration 32/1000 | Loss: 0.00002695
Iteration 33/1000 | Loss: 0.00002484
Iteration 34/1000 | Loss: 0.00002359
Iteration 35/1000 | Loss: 0.00002242
Iteration 36/1000 | Loss: 0.00167484
Iteration 37/1000 | Loss: 0.00105402
Iteration 38/1000 | Loss: 0.00005850
Iteration 39/1000 | Loss: 0.00002540
Iteration 40/1000 | Loss: 0.00002315
Iteration 41/1000 | Loss: 0.00149068
Iteration 42/1000 | Loss: 0.00088592
Iteration 43/1000 | Loss: 0.00006544
Iteration 44/1000 | Loss: 0.00003619
Iteration 45/1000 | Loss: 0.00003222
Iteration 46/1000 | Loss: 0.00002850
Iteration 47/1000 | Loss: 0.00149882
Iteration 48/1000 | Loss: 0.00147147
Iteration 49/1000 | Loss: 0.00181477
Iteration 50/1000 | Loss: 0.00101743
Iteration 51/1000 | Loss: 0.00004417
Iteration 52/1000 | Loss: 0.00004001
Iteration 53/1000 | Loss: 0.00089546
Iteration 54/1000 | Loss: 0.00176801
Iteration 55/1000 | Loss: 0.00099154
Iteration 56/1000 | Loss: 0.00131372
Iteration 57/1000 | Loss: 0.00058049
Iteration 58/1000 | Loss: 0.00068599
Iteration 59/1000 | Loss: 0.00083839
Iteration 60/1000 | Loss: 0.00099039
Iteration 61/1000 | Loss: 0.00009084
Iteration 62/1000 | Loss: 0.00005554
Iteration 63/1000 | Loss: 0.00142274
Iteration 64/1000 | Loss: 0.00141468
Iteration 65/1000 | Loss: 0.00118832
Iteration 66/1000 | Loss: 0.00094048
Iteration 67/1000 | Loss: 0.00156378
Iteration 68/1000 | Loss: 0.00140979
Iteration 69/1000 | Loss: 0.00115272
Iteration 70/1000 | Loss: 0.00132299
Iteration 71/1000 | Loss: 0.00143843
Iteration 72/1000 | Loss: 0.00110967
Iteration 73/1000 | Loss: 0.00152495
Iteration 74/1000 | Loss: 0.00125966
Iteration 75/1000 | Loss: 0.00140447
Iteration 76/1000 | Loss: 0.00077576
Iteration 77/1000 | Loss: 0.00069418
Iteration 78/1000 | Loss: 0.00296628
Iteration 79/1000 | Loss: 0.00162919
Iteration 80/1000 | Loss: 0.00274486
Iteration 81/1000 | Loss: 0.00274311
Iteration 82/1000 | Loss: 0.00092045
Iteration 83/1000 | Loss: 0.00034833
Iteration 84/1000 | Loss: 0.00033651
Iteration 85/1000 | Loss: 0.00014974
Iteration 86/1000 | Loss: 0.00004195
Iteration 87/1000 | Loss: 0.00080110
Iteration 88/1000 | Loss: 0.00026211
Iteration 89/1000 | Loss: 0.00015164
Iteration 90/1000 | Loss: 0.00013862
Iteration 91/1000 | Loss: 0.00016236
Iteration 92/1000 | Loss: 0.00013455
Iteration 93/1000 | Loss: 0.00040354
Iteration 94/1000 | Loss: 0.00084985
Iteration 95/1000 | Loss: 0.00083041
Iteration 96/1000 | Loss: 0.00005909
Iteration 97/1000 | Loss: 0.00004327
Iteration 98/1000 | Loss: 0.00022622
Iteration 99/1000 | Loss: 0.00003130
Iteration 100/1000 | Loss: 0.00003024
Iteration 101/1000 | Loss: 0.00002589
Iteration 102/1000 | Loss: 0.00002448
Iteration 103/1000 | Loss: 0.00002328
Iteration 104/1000 | Loss: 0.00002177
Iteration 105/1000 | Loss: 0.00001988
Iteration 106/1000 | Loss: 0.00002711
Iteration 107/1000 | Loss: 0.00002336
Iteration 108/1000 | Loss: 0.00002158
Iteration 109/1000 | Loss: 0.00001923
Iteration 110/1000 | Loss: 0.00001791
Iteration 111/1000 | Loss: 0.00001723
Iteration 112/1000 | Loss: 0.00001702
Iteration 113/1000 | Loss: 0.00001675
Iteration 114/1000 | Loss: 0.00001643
Iteration 115/1000 | Loss: 0.00001643
Iteration 116/1000 | Loss: 0.00001640
Iteration 117/1000 | Loss: 0.00001639
Iteration 118/1000 | Loss: 0.00001638
Iteration 119/1000 | Loss: 0.00001638
Iteration 120/1000 | Loss: 0.00001637
Iteration 121/1000 | Loss: 0.00001637
Iteration 122/1000 | Loss: 0.00001636
Iteration 123/1000 | Loss: 0.00001635
Iteration 124/1000 | Loss: 0.00001635
Iteration 125/1000 | Loss: 0.00001634
Iteration 126/1000 | Loss: 0.00001632
Iteration 127/1000 | Loss: 0.00001632
Iteration 128/1000 | Loss: 0.00001632
Iteration 129/1000 | Loss: 0.00001632
Iteration 130/1000 | Loss: 0.00001632
Iteration 131/1000 | Loss: 0.00001632
Iteration 132/1000 | Loss: 0.00001632
Iteration 133/1000 | Loss: 0.00001631
Iteration 134/1000 | Loss: 0.00001631
Iteration 135/1000 | Loss: 0.00001631
Iteration 136/1000 | Loss: 0.00001630
Iteration 137/1000 | Loss: 0.00001630
Iteration 138/1000 | Loss: 0.00001630
Iteration 139/1000 | Loss: 0.00001630
Iteration 140/1000 | Loss: 0.00001630
Iteration 141/1000 | Loss: 0.00001629
Iteration 142/1000 | Loss: 0.00001629
Iteration 143/1000 | Loss: 0.00001629
Iteration 144/1000 | Loss: 0.00001629
Iteration 145/1000 | Loss: 0.00001629
Iteration 146/1000 | Loss: 0.00001629
Iteration 147/1000 | Loss: 0.00001629
Iteration 148/1000 | Loss: 0.00001629
Iteration 149/1000 | Loss: 0.00001629
Iteration 150/1000 | Loss: 0.00001628
Iteration 151/1000 | Loss: 0.00001628
Iteration 152/1000 | Loss: 0.00001628
Iteration 153/1000 | Loss: 0.00001628
Iteration 154/1000 | Loss: 0.00001627
Iteration 155/1000 | Loss: 0.00001625
Iteration 156/1000 | Loss: 0.00001623
Iteration 157/1000 | Loss: 0.00001623
Iteration 158/1000 | Loss: 0.00001623
Iteration 159/1000 | Loss: 0.00001623
Iteration 160/1000 | Loss: 0.00001623
Iteration 161/1000 | Loss: 0.00001623
Iteration 162/1000 | Loss: 0.00001623
Iteration 163/1000 | Loss: 0.00001622
Iteration 164/1000 | Loss: 0.00001622
Iteration 165/1000 | Loss: 0.00001622
Iteration 166/1000 | Loss: 0.00001622
Iteration 167/1000 | Loss: 0.00001622
Iteration 168/1000 | Loss: 0.00001622
Iteration 169/1000 | Loss: 0.00001621
Iteration 170/1000 | Loss: 0.00001621
Iteration 171/1000 | Loss: 0.00001621
Iteration 172/1000 | Loss: 0.00001620
Iteration 173/1000 | Loss: 0.00001620
Iteration 174/1000 | Loss: 0.00001620
Iteration 175/1000 | Loss: 0.00001620
Iteration 176/1000 | Loss: 0.00001620
Iteration 177/1000 | Loss: 0.00001619
Iteration 178/1000 | Loss: 0.00001619
Iteration 179/1000 | Loss: 0.00001619
Iteration 180/1000 | Loss: 0.00001618
Iteration 181/1000 | Loss: 0.00001618
Iteration 182/1000 | Loss: 0.00001618
Iteration 183/1000 | Loss: 0.00001618
Iteration 184/1000 | Loss: 0.00001618
Iteration 185/1000 | Loss: 0.00001618
Iteration 186/1000 | Loss: 0.00001618
Iteration 187/1000 | Loss: 0.00001618
Iteration 188/1000 | Loss: 0.00001617
Iteration 189/1000 | Loss: 0.00001617
Iteration 190/1000 | Loss: 0.00001617
Iteration 191/1000 | Loss: 0.00001617
Iteration 192/1000 | Loss: 0.00001617
Iteration 193/1000 | Loss: 0.00001616
Iteration 194/1000 | Loss: 0.00001616
Iteration 195/1000 | Loss: 0.00001616
Iteration 196/1000 | Loss: 0.00001616
Iteration 197/1000 | Loss: 0.00001616
Iteration 198/1000 | Loss: 0.00001616
Iteration 199/1000 | Loss: 0.00001616
Iteration 200/1000 | Loss: 0.00001615
Iteration 201/1000 | Loss: 0.00001615
Iteration 202/1000 | Loss: 0.00001615
Iteration 203/1000 | Loss: 0.00001614
Iteration 204/1000 | Loss: 0.00001614
Iteration 205/1000 | Loss: 0.00001614
Iteration 206/1000 | Loss: 0.00001614
Iteration 207/1000 | Loss: 0.00001614
Iteration 208/1000 | Loss: 0.00001613
Iteration 209/1000 | Loss: 0.00001610
Iteration 210/1000 | Loss: 0.00001610
Iteration 211/1000 | Loss: 0.00001610
Iteration 212/1000 | Loss: 0.00001610
Iteration 213/1000 | Loss: 0.00001609
Iteration 214/1000 | Loss: 0.00001607
Iteration 215/1000 | Loss: 0.00001607
Iteration 216/1000 | Loss: 0.00001607
Iteration 217/1000 | Loss: 0.00001607
Iteration 218/1000 | Loss: 0.00001607
Iteration 219/1000 | Loss: 0.00001606
Iteration 220/1000 | Loss: 0.00001606
Iteration 221/1000 | Loss: 0.00001605
Iteration 222/1000 | Loss: 0.00001605
Iteration 223/1000 | Loss: 0.00001604
Iteration 224/1000 | Loss: 0.00001603
Iteration 225/1000 | Loss: 0.00001603
Iteration 226/1000 | Loss: 0.00001603
Iteration 227/1000 | Loss: 0.00001603
Iteration 228/1000 | Loss: 0.00001602
Iteration 229/1000 | Loss: 0.00001602
Iteration 230/1000 | Loss: 0.00001602
Iteration 231/1000 | Loss: 0.00001602
Iteration 232/1000 | Loss: 0.00001602
Iteration 233/1000 | Loss: 0.00001601
Iteration 234/1000 | Loss: 0.00030010
Iteration 235/1000 | Loss: 0.00001849
Iteration 236/1000 | Loss: 0.00001663
Iteration 237/1000 | Loss: 0.00001561
Iteration 238/1000 | Loss: 0.00001440
Iteration 239/1000 | Loss: 0.00001385
Iteration 240/1000 | Loss: 0.00001356
Iteration 241/1000 | Loss: 0.00001331
Iteration 242/1000 | Loss: 0.00001320
Iteration 243/1000 | Loss: 0.00001320
Iteration 244/1000 | Loss: 0.00001319
Iteration 245/1000 | Loss: 0.00001319
Iteration 246/1000 | Loss: 0.00001318
Iteration 247/1000 | Loss: 0.00001318
Iteration 248/1000 | Loss: 0.00001318
Iteration 249/1000 | Loss: 0.00001318
Iteration 250/1000 | Loss: 0.00001318
Iteration 251/1000 | Loss: 0.00001318
Iteration 252/1000 | Loss: 0.00001318
Iteration 253/1000 | Loss: 0.00001318
Iteration 254/1000 | Loss: 0.00001318
Iteration 255/1000 | Loss: 0.00001317
Iteration 256/1000 | Loss: 0.00001317
Iteration 257/1000 | Loss: 0.00001317
Iteration 258/1000 | Loss: 0.00001317
Iteration 259/1000 | Loss: 0.00001317
Iteration 260/1000 | Loss: 0.00001317
Iteration 261/1000 | Loss: 0.00001316
Iteration 262/1000 | Loss: 0.00001316
Iteration 263/1000 | Loss: 0.00001316
Iteration 264/1000 | Loss: 0.00001315
Iteration 265/1000 | Loss: 0.00001315
Iteration 266/1000 | Loss: 0.00001310
Iteration 267/1000 | Loss: 0.00001310
Iteration 268/1000 | Loss: 0.00001310
Iteration 269/1000 | Loss: 0.00001310
Iteration 270/1000 | Loss: 0.00001310
Iteration 271/1000 | Loss: 0.00001310
Iteration 272/1000 | Loss: 0.00001310
Iteration 273/1000 | Loss: 0.00001310
Iteration 274/1000 | Loss: 0.00001309
Iteration 275/1000 | Loss: 0.00001309
Iteration 276/1000 | Loss: 0.00001309
Iteration 277/1000 | Loss: 0.00001309
Iteration 278/1000 | Loss: 0.00001308
Iteration 279/1000 | Loss: 0.00001308
Iteration 280/1000 | Loss: 0.00001308
Iteration 281/1000 | Loss: 0.00001307
Iteration 282/1000 | Loss: 0.00001307
Iteration 283/1000 | Loss: 0.00001307
Iteration 284/1000 | Loss: 0.00001307
Iteration 285/1000 | Loss: 0.00001307
Iteration 286/1000 | Loss: 0.00001307
Iteration 287/1000 | Loss: 0.00001306
Iteration 288/1000 | Loss: 0.00001306
Iteration 289/1000 | Loss: 0.00001305
Iteration 290/1000 | Loss: 0.00001305
Iteration 291/1000 | Loss: 0.00001303
Iteration 292/1000 | Loss: 0.00001303
Iteration 293/1000 | Loss: 0.00001303
Iteration 294/1000 | Loss: 0.00001303
Iteration 295/1000 | Loss: 0.00001303
Iteration 296/1000 | Loss: 0.00001303
Iteration 297/1000 | Loss: 0.00001303
Iteration 298/1000 | Loss: 0.00001302
Iteration 299/1000 | Loss: 0.00001302
Iteration 300/1000 | Loss: 0.00001302
Iteration 301/1000 | Loss: 0.00001302
Iteration 302/1000 | Loss: 0.00001302
Iteration 303/1000 | Loss: 0.00001302
Iteration 304/1000 | Loss: 0.00001302
Iteration 305/1000 | Loss: 0.00001302
Iteration 306/1000 | Loss: 0.00001301
Iteration 307/1000 | Loss: 0.00001301
Iteration 308/1000 | Loss: 0.00001301
Iteration 309/1000 | Loss: 0.00001301
Iteration 310/1000 | Loss: 0.00001301
Iteration 311/1000 | Loss: 0.00001301
Iteration 312/1000 | Loss: 0.00001301
Iteration 313/1000 | Loss: 0.00001301
Iteration 314/1000 | Loss: 0.00001301
Iteration 315/1000 | Loss: 0.00001301
Iteration 316/1000 | Loss: 0.00001301
Iteration 317/1000 | Loss: 0.00001301
Iteration 318/1000 | Loss: 0.00001301
Iteration 319/1000 | Loss: 0.00001301
Iteration 320/1000 | Loss: 0.00001301
Iteration 321/1000 | Loss: 0.00001301
Iteration 322/1000 | Loss: 0.00001301
Iteration 323/1000 | Loss: 0.00001301
Iteration 324/1000 | Loss: 0.00001301
Iteration 325/1000 | Loss: 0.00001301
Iteration 326/1000 | Loss: 0.00001301
Iteration 327/1000 | Loss: 0.00001301
Iteration 328/1000 | Loss: 0.00001301
Iteration 329/1000 | Loss: 0.00001301
Iteration 330/1000 | Loss: 0.00001301
Iteration 331/1000 | Loss: 0.00001301
Iteration 332/1000 | Loss: 0.00001301
Iteration 333/1000 | Loss: 0.00001301
Iteration 334/1000 | Loss: 0.00001301
Iteration 335/1000 | Loss: 0.00001301
Iteration 336/1000 | Loss: 0.00001301
Iteration 337/1000 | Loss: 0.00001301
Iteration 338/1000 | Loss: 0.00001301
Iteration 339/1000 | Loss: 0.00001301
Iteration 340/1000 | Loss: 0.00001301
Iteration 341/1000 | Loss: 0.00001301
Iteration 342/1000 | Loss: 0.00001301
Iteration 343/1000 | Loss: 0.00001301
Iteration 344/1000 | Loss: 0.00001301
Iteration 345/1000 | Loss: 0.00001301
Iteration 346/1000 | Loss: 0.00001301
Iteration 347/1000 | Loss: 0.00001301
Iteration 348/1000 | Loss: 0.00001301
Iteration 349/1000 | Loss: 0.00001301
Iteration 350/1000 | Loss: 0.00001301
Iteration 351/1000 | Loss: 0.00001301
Iteration 352/1000 | Loss: 0.00001301
Iteration 353/1000 | Loss: 0.00001301
Iteration 354/1000 | Loss: 0.00001301
Iteration 355/1000 | Loss: 0.00001301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 355. Stopping optimization.
Last 5 losses: [1.30081989482278e-05, 1.30081989482278e-05, 1.30081989482278e-05, 1.30081989482278e-05, 1.30081989482278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.30081989482278e-05

Optimization complete. Final v2v error: 3.014925718307495 mm

Highest mean error: 4.433514595031738 mm for frame 112

Lowest mean error: 2.4793848991394043 mm for frame 34

Saving results

Total time: 224.31541061401367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386624
Iteration 2/25 | Loss: 0.00120144
Iteration 3/25 | Loss: 0.00109071
Iteration 4/25 | Loss: 0.00108082
Iteration 5/25 | Loss: 0.00107766
Iteration 6/25 | Loss: 0.00107766
Iteration 7/25 | Loss: 0.00107766
Iteration 8/25 | Loss: 0.00107766
Iteration 9/25 | Loss: 0.00107766
Iteration 10/25 | Loss: 0.00107766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010776611743494868, 0.0010776611743494868, 0.0010776611743494868, 0.0010776611743494868, 0.0010776611743494868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010776611743494868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63395715
Iteration 2/25 | Loss: 0.00075197
Iteration 3/25 | Loss: 0.00075197
Iteration 4/25 | Loss: 0.00075197
Iteration 5/25 | Loss: 0.00075197
Iteration 6/25 | Loss: 0.00075197
Iteration 7/25 | Loss: 0.00075197
Iteration 8/25 | Loss: 0.00075197
Iteration 9/25 | Loss: 0.00075197
Iteration 10/25 | Loss: 0.00075197
Iteration 11/25 | Loss: 0.00075197
Iteration 12/25 | Loss: 0.00075197
Iteration 13/25 | Loss: 0.00075197
Iteration 14/25 | Loss: 0.00075197
Iteration 15/25 | Loss: 0.00075197
Iteration 16/25 | Loss: 0.00075197
Iteration 17/25 | Loss: 0.00075197
Iteration 18/25 | Loss: 0.00075197
Iteration 19/25 | Loss: 0.00075197
Iteration 20/25 | Loss: 0.00075197
Iteration 21/25 | Loss: 0.00075197
Iteration 22/25 | Loss: 0.00075197
Iteration 23/25 | Loss: 0.00075197
Iteration 24/25 | Loss: 0.00075197
Iteration 25/25 | Loss: 0.00075197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075197
Iteration 2/1000 | Loss: 0.00002230
Iteration 3/1000 | Loss: 0.00001321
Iteration 4/1000 | Loss: 0.00001171
Iteration 5/1000 | Loss: 0.00001080
Iteration 6/1000 | Loss: 0.00001024
Iteration 7/1000 | Loss: 0.00000987
Iteration 8/1000 | Loss: 0.00000973
Iteration 9/1000 | Loss: 0.00000947
Iteration 10/1000 | Loss: 0.00000920
Iteration 11/1000 | Loss: 0.00000916
Iteration 12/1000 | Loss: 0.00000916
Iteration 13/1000 | Loss: 0.00000916
Iteration 14/1000 | Loss: 0.00000911
Iteration 15/1000 | Loss: 0.00000911
Iteration 16/1000 | Loss: 0.00000905
Iteration 17/1000 | Loss: 0.00000899
Iteration 18/1000 | Loss: 0.00000896
Iteration 19/1000 | Loss: 0.00000894
Iteration 20/1000 | Loss: 0.00000894
Iteration 21/1000 | Loss: 0.00000893
Iteration 22/1000 | Loss: 0.00000893
Iteration 23/1000 | Loss: 0.00000893
Iteration 24/1000 | Loss: 0.00000890
Iteration 25/1000 | Loss: 0.00000890
Iteration 26/1000 | Loss: 0.00000888
Iteration 27/1000 | Loss: 0.00000888
Iteration 28/1000 | Loss: 0.00000886
Iteration 29/1000 | Loss: 0.00000886
Iteration 30/1000 | Loss: 0.00000885
Iteration 31/1000 | Loss: 0.00000884
Iteration 32/1000 | Loss: 0.00000884
Iteration 33/1000 | Loss: 0.00000883
Iteration 34/1000 | Loss: 0.00000883
Iteration 35/1000 | Loss: 0.00000883
Iteration 36/1000 | Loss: 0.00000883
Iteration 37/1000 | Loss: 0.00000882
Iteration 38/1000 | Loss: 0.00000882
Iteration 39/1000 | Loss: 0.00000882
Iteration 40/1000 | Loss: 0.00000881
Iteration 41/1000 | Loss: 0.00000880
Iteration 42/1000 | Loss: 0.00000880
Iteration 43/1000 | Loss: 0.00000879
Iteration 44/1000 | Loss: 0.00000879
Iteration 45/1000 | Loss: 0.00000879
Iteration 46/1000 | Loss: 0.00000879
Iteration 47/1000 | Loss: 0.00000879
Iteration 48/1000 | Loss: 0.00000879
Iteration 49/1000 | Loss: 0.00000879
Iteration 50/1000 | Loss: 0.00000879
Iteration 51/1000 | Loss: 0.00000878
Iteration 52/1000 | Loss: 0.00000878
Iteration 53/1000 | Loss: 0.00000878
Iteration 54/1000 | Loss: 0.00000878
Iteration 55/1000 | Loss: 0.00000878
Iteration 56/1000 | Loss: 0.00000878
Iteration 57/1000 | Loss: 0.00000877
Iteration 58/1000 | Loss: 0.00000877
Iteration 59/1000 | Loss: 0.00000877
Iteration 60/1000 | Loss: 0.00000877
Iteration 61/1000 | Loss: 0.00000876
Iteration 62/1000 | Loss: 0.00000876
Iteration 63/1000 | Loss: 0.00000876
Iteration 64/1000 | Loss: 0.00000876
Iteration 65/1000 | Loss: 0.00000875
Iteration 66/1000 | Loss: 0.00000875
Iteration 67/1000 | Loss: 0.00000875
Iteration 68/1000 | Loss: 0.00000874
Iteration 69/1000 | Loss: 0.00000874
Iteration 70/1000 | Loss: 0.00000873
Iteration 71/1000 | Loss: 0.00000872
Iteration 72/1000 | Loss: 0.00000872
Iteration 73/1000 | Loss: 0.00000872
Iteration 74/1000 | Loss: 0.00000871
Iteration 75/1000 | Loss: 0.00000871
Iteration 76/1000 | Loss: 0.00000871
Iteration 77/1000 | Loss: 0.00000870
Iteration 78/1000 | Loss: 0.00000870
Iteration 79/1000 | Loss: 0.00000870
Iteration 80/1000 | Loss: 0.00000870
Iteration 81/1000 | Loss: 0.00000870
Iteration 82/1000 | Loss: 0.00000870
Iteration 83/1000 | Loss: 0.00000869
Iteration 84/1000 | Loss: 0.00000869
Iteration 85/1000 | Loss: 0.00000869
Iteration 86/1000 | Loss: 0.00000869
Iteration 87/1000 | Loss: 0.00000869
Iteration 88/1000 | Loss: 0.00000869
Iteration 89/1000 | Loss: 0.00000869
Iteration 90/1000 | Loss: 0.00000868
Iteration 91/1000 | Loss: 0.00000868
Iteration 92/1000 | Loss: 0.00000868
Iteration 93/1000 | Loss: 0.00000868
Iteration 94/1000 | Loss: 0.00000868
Iteration 95/1000 | Loss: 0.00000867
Iteration 96/1000 | Loss: 0.00000867
Iteration 97/1000 | Loss: 0.00000867
Iteration 98/1000 | Loss: 0.00000867
Iteration 99/1000 | Loss: 0.00000866
Iteration 100/1000 | Loss: 0.00000866
Iteration 101/1000 | Loss: 0.00000866
Iteration 102/1000 | Loss: 0.00000865
Iteration 103/1000 | Loss: 0.00000865
Iteration 104/1000 | Loss: 0.00000865
Iteration 105/1000 | Loss: 0.00000865
Iteration 106/1000 | Loss: 0.00000865
Iteration 107/1000 | Loss: 0.00000865
Iteration 108/1000 | Loss: 0.00000865
Iteration 109/1000 | Loss: 0.00000865
Iteration 110/1000 | Loss: 0.00000864
Iteration 111/1000 | Loss: 0.00000864
Iteration 112/1000 | Loss: 0.00000864
Iteration 113/1000 | Loss: 0.00000863
Iteration 114/1000 | Loss: 0.00000863
Iteration 115/1000 | Loss: 0.00000863
Iteration 116/1000 | Loss: 0.00000863
Iteration 117/1000 | Loss: 0.00000863
Iteration 118/1000 | Loss: 0.00000863
Iteration 119/1000 | Loss: 0.00000863
Iteration 120/1000 | Loss: 0.00000863
Iteration 121/1000 | Loss: 0.00000863
Iteration 122/1000 | Loss: 0.00000862
Iteration 123/1000 | Loss: 0.00000862
Iteration 124/1000 | Loss: 0.00000862
Iteration 125/1000 | Loss: 0.00000862
Iteration 126/1000 | Loss: 0.00000862
Iteration 127/1000 | Loss: 0.00000862
Iteration 128/1000 | Loss: 0.00000862
Iteration 129/1000 | Loss: 0.00000862
Iteration 130/1000 | Loss: 0.00000861
Iteration 131/1000 | Loss: 0.00000861
Iteration 132/1000 | Loss: 0.00000861
Iteration 133/1000 | Loss: 0.00000861
Iteration 134/1000 | Loss: 0.00000861
Iteration 135/1000 | Loss: 0.00000861
Iteration 136/1000 | Loss: 0.00000861
Iteration 137/1000 | Loss: 0.00000861
Iteration 138/1000 | Loss: 0.00000860
Iteration 139/1000 | Loss: 0.00000860
Iteration 140/1000 | Loss: 0.00000860
Iteration 141/1000 | Loss: 0.00000860
Iteration 142/1000 | Loss: 0.00000860
Iteration 143/1000 | Loss: 0.00000860
Iteration 144/1000 | Loss: 0.00000859
Iteration 145/1000 | Loss: 0.00000859
Iteration 146/1000 | Loss: 0.00000859
Iteration 147/1000 | Loss: 0.00000859
Iteration 148/1000 | Loss: 0.00000859
Iteration 149/1000 | Loss: 0.00000859
Iteration 150/1000 | Loss: 0.00000859
Iteration 151/1000 | Loss: 0.00000859
Iteration 152/1000 | Loss: 0.00000859
Iteration 153/1000 | Loss: 0.00000859
Iteration 154/1000 | Loss: 0.00000858
Iteration 155/1000 | Loss: 0.00000858
Iteration 156/1000 | Loss: 0.00000858
Iteration 157/1000 | Loss: 0.00000858
Iteration 158/1000 | Loss: 0.00000857
Iteration 159/1000 | Loss: 0.00000857
Iteration 160/1000 | Loss: 0.00000857
Iteration 161/1000 | Loss: 0.00000857
Iteration 162/1000 | Loss: 0.00000857
Iteration 163/1000 | Loss: 0.00000857
Iteration 164/1000 | Loss: 0.00000857
Iteration 165/1000 | Loss: 0.00000857
Iteration 166/1000 | Loss: 0.00000857
Iteration 167/1000 | Loss: 0.00000857
Iteration 168/1000 | Loss: 0.00000857
Iteration 169/1000 | Loss: 0.00000857
Iteration 170/1000 | Loss: 0.00000857
Iteration 171/1000 | Loss: 0.00000857
Iteration 172/1000 | Loss: 0.00000857
Iteration 173/1000 | Loss: 0.00000857
Iteration 174/1000 | Loss: 0.00000857
Iteration 175/1000 | Loss: 0.00000857
Iteration 176/1000 | Loss: 0.00000857
Iteration 177/1000 | Loss: 0.00000857
Iteration 178/1000 | Loss: 0.00000857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [8.57116265251534e-06, 8.57116265251534e-06, 8.57116265251534e-06, 8.57116265251534e-06, 8.57116265251534e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.57116265251534e-06

Optimization complete. Final v2v error: 2.5298213958740234 mm

Highest mean error: 2.6732337474823 mm for frame 107

Lowest mean error: 2.391270637512207 mm for frame 198

Saving results

Total time: 41.650163412094116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461010
Iteration 2/25 | Loss: 0.00130888
Iteration 3/25 | Loss: 0.00112715
Iteration 4/25 | Loss: 0.00111632
Iteration 5/25 | Loss: 0.00111319
Iteration 6/25 | Loss: 0.00111231
Iteration 7/25 | Loss: 0.00111225
Iteration 8/25 | Loss: 0.00111225
Iteration 9/25 | Loss: 0.00111225
Iteration 10/25 | Loss: 0.00111225
Iteration 11/25 | Loss: 0.00111225
Iteration 12/25 | Loss: 0.00111225
Iteration 13/25 | Loss: 0.00111225
Iteration 14/25 | Loss: 0.00111225
Iteration 15/25 | Loss: 0.00111225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011122514260932803, 0.0011122514260932803, 0.0011122514260932803, 0.0011122514260932803, 0.0011122514260932803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011122514260932803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45668411
Iteration 2/25 | Loss: 0.00095931
Iteration 3/25 | Loss: 0.00095931
Iteration 4/25 | Loss: 0.00095931
Iteration 5/25 | Loss: 0.00095931
Iteration 6/25 | Loss: 0.00095931
Iteration 7/25 | Loss: 0.00095931
Iteration 8/25 | Loss: 0.00095931
Iteration 9/25 | Loss: 0.00095931
Iteration 10/25 | Loss: 0.00095931
Iteration 11/25 | Loss: 0.00095931
Iteration 12/25 | Loss: 0.00095931
Iteration 13/25 | Loss: 0.00095931
Iteration 14/25 | Loss: 0.00095931
Iteration 15/25 | Loss: 0.00095931
Iteration 16/25 | Loss: 0.00095931
Iteration 17/25 | Loss: 0.00095931
Iteration 18/25 | Loss: 0.00095931
Iteration 19/25 | Loss: 0.00095931
Iteration 20/25 | Loss: 0.00095931
Iteration 21/25 | Loss: 0.00095931
Iteration 22/25 | Loss: 0.00095931
Iteration 23/25 | Loss: 0.00095931
Iteration 24/25 | Loss: 0.00095931
Iteration 25/25 | Loss: 0.00095931

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095931
Iteration 2/1000 | Loss: 0.00002471
Iteration 3/1000 | Loss: 0.00001802
Iteration 4/1000 | Loss: 0.00001610
Iteration 5/1000 | Loss: 0.00001552
Iteration 6/1000 | Loss: 0.00001496
Iteration 7/1000 | Loss: 0.00001451
Iteration 8/1000 | Loss: 0.00001423
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001377
Iteration 11/1000 | Loss: 0.00001362
Iteration 12/1000 | Loss: 0.00001348
Iteration 13/1000 | Loss: 0.00001348
Iteration 14/1000 | Loss: 0.00001337
Iteration 15/1000 | Loss: 0.00001334
Iteration 16/1000 | Loss: 0.00001333
Iteration 17/1000 | Loss: 0.00001333
Iteration 18/1000 | Loss: 0.00001332
Iteration 19/1000 | Loss: 0.00001332
Iteration 20/1000 | Loss: 0.00001331
Iteration 21/1000 | Loss: 0.00001330
Iteration 22/1000 | Loss: 0.00001330
Iteration 23/1000 | Loss: 0.00001326
Iteration 24/1000 | Loss: 0.00001325
Iteration 25/1000 | Loss: 0.00001324
Iteration 26/1000 | Loss: 0.00001324
Iteration 27/1000 | Loss: 0.00001323
Iteration 28/1000 | Loss: 0.00001321
Iteration 29/1000 | Loss: 0.00001321
Iteration 30/1000 | Loss: 0.00001321
Iteration 31/1000 | Loss: 0.00001321
Iteration 32/1000 | Loss: 0.00001321
Iteration 33/1000 | Loss: 0.00001321
Iteration 34/1000 | Loss: 0.00001321
Iteration 35/1000 | Loss: 0.00001319
Iteration 36/1000 | Loss: 0.00001319
Iteration 37/1000 | Loss: 0.00001319
Iteration 38/1000 | Loss: 0.00001318
Iteration 39/1000 | Loss: 0.00001318
Iteration 40/1000 | Loss: 0.00001318
Iteration 41/1000 | Loss: 0.00001318
Iteration 42/1000 | Loss: 0.00001317
Iteration 43/1000 | Loss: 0.00001317
Iteration 44/1000 | Loss: 0.00001317
Iteration 45/1000 | Loss: 0.00001317
Iteration 46/1000 | Loss: 0.00001316
Iteration 47/1000 | Loss: 0.00001316
Iteration 48/1000 | Loss: 0.00001314
Iteration 49/1000 | Loss: 0.00001313
Iteration 50/1000 | Loss: 0.00001309
Iteration 51/1000 | Loss: 0.00001307
Iteration 52/1000 | Loss: 0.00001306
Iteration 53/1000 | Loss: 0.00001306
Iteration 54/1000 | Loss: 0.00001306
Iteration 55/1000 | Loss: 0.00001304
Iteration 56/1000 | Loss: 0.00001304
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001304
Iteration 60/1000 | Loss: 0.00001303
Iteration 61/1000 | Loss: 0.00001303
Iteration 62/1000 | Loss: 0.00001303
Iteration 63/1000 | Loss: 0.00001303
Iteration 64/1000 | Loss: 0.00001302
Iteration 65/1000 | Loss: 0.00001301
Iteration 66/1000 | Loss: 0.00001301
Iteration 67/1000 | Loss: 0.00001300
Iteration 68/1000 | Loss: 0.00001300
Iteration 69/1000 | Loss: 0.00001300
Iteration 70/1000 | Loss: 0.00001300
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001300
Iteration 73/1000 | Loss: 0.00001300
Iteration 74/1000 | Loss: 0.00001300
Iteration 75/1000 | Loss: 0.00001299
Iteration 76/1000 | Loss: 0.00001299
Iteration 77/1000 | Loss: 0.00001299
Iteration 78/1000 | Loss: 0.00001299
Iteration 79/1000 | Loss: 0.00001298
Iteration 80/1000 | Loss: 0.00001298
Iteration 81/1000 | Loss: 0.00001298
Iteration 82/1000 | Loss: 0.00001297
Iteration 83/1000 | Loss: 0.00001297
Iteration 84/1000 | Loss: 0.00001297
Iteration 85/1000 | Loss: 0.00001297
Iteration 86/1000 | Loss: 0.00001297
Iteration 87/1000 | Loss: 0.00001297
Iteration 88/1000 | Loss: 0.00001297
Iteration 89/1000 | Loss: 0.00001297
Iteration 90/1000 | Loss: 0.00001297
Iteration 91/1000 | Loss: 0.00001297
Iteration 92/1000 | Loss: 0.00001296
Iteration 93/1000 | Loss: 0.00001296
Iteration 94/1000 | Loss: 0.00001296
Iteration 95/1000 | Loss: 0.00001295
Iteration 96/1000 | Loss: 0.00001295
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001293
Iteration 101/1000 | Loss: 0.00001293
Iteration 102/1000 | Loss: 0.00001293
Iteration 103/1000 | Loss: 0.00001293
Iteration 104/1000 | Loss: 0.00001293
Iteration 105/1000 | Loss: 0.00001293
Iteration 106/1000 | Loss: 0.00001293
Iteration 107/1000 | Loss: 0.00001293
Iteration 108/1000 | Loss: 0.00001293
Iteration 109/1000 | Loss: 0.00001293
Iteration 110/1000 | Loss: 0.00001292
Iteration 111/1000 | Loss: 0.00001292
Iteration 112/1000 | Loss: 0.00001292
Iteration 113/1000 | Loss: 0.00001291
Iteration 114/1000 | Loss: 0.00001291
Iteration 115/1000 | Loss: 0.00001291
Iteration 116/1000 | Loss: 0.00001291
Iteration 117/1000 | Loss: 0.00001291
Iteration 118/1000 | Loss: 0.00001291
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001291
Iteration 121/1000 | Loss: 0.00001290
Iteration 122/1000 | Loss: 0.00001290
Iteration 123/1000 | Loss: 0.00001290
Iteration 124/1000 | Loss: 0.00001290
Iteration 125/1000 | Loss: 0.00001290
Iteration 126/1000 | Loss: 0.00001290
Iteration 127/1000 | Loss: 0.00001290
Iteration 128/1000 | Loss: 0.00001290
Iteration 129/1000 | Loss: 0.00001289
Iteration 130/1000 | Loss: 0.00001289
Iteration 131/1000 | Loss: 0.00001289
Iteration 132/1000 | Loss: 0.00001289
Iteration 133/1000 | Loss: 0.00001289
Iteration 134/1000 | Loss: 0.00001289
Iteration 135/1000 | Loss: 0.00001289
Iteration 136/1000 | Loss: 0.00001288
Iteration 137/1000 | Loss: 0.00001288
Iteration 138/1000 | Loss: 0.00001288
Iteration 139/1000 | Loss: 0.00001288
Iteration 140/1000 | Loss: 0.00001288
Iteration 141/1000 | Loss: 0.00001288
Iteration 142/1000 | Loss: 0.00001288
Iteration 143/1000 | Loss: 0.00001288
Iteration 144/1000 | Loss: 0.00001288
Iteration 145/1000 | Loss: 0.00001287
Iteration 146/1000 | Loss: 0.00001287
Iteration 147/1000 | Loss: 0.00001287
Iteration 148/1000 | Loss: 0.00001287
Iteration 149/1000 | Loss: 0.00001287
Iteration 150/1000 | Loss: 0.00001287
Iteration 151/1000 | Loss: 0.00001287
Iteration 152/1000 | Loss: 0.00001287
Iteration 153/1000 | Loss: 0.00001287
Iteration 154/1000 | Loss: 0.00001287
Iteration 155/1000 | Loss: 0.00001287
Iteration 156/1000 | Loss: 0.00001287
Iteration 157/1000 | Loss: 0.00001287
Iteration 158/1000 | Loss: 0.00001287
Iteration 159/1000 | Loss: 0.00001287
Iteration 160/1000 | Loss: 0.00001287
Iteration 161/1000 | Loss: 0.00001287
Iteration 162/1000 | Loss: 0.00001287
Iteration 163/1000 | Loss: 0.00001287
Iteration 164/1000 | Loss: 0.00001287
Iteration 165/1000 | Loss: 0.00001287
Iteration 166/1000 | Loss: 0.00001287
Iteration 167/1000 | Loss: 0.00001287
Iteration 168/1000 | Loss: 0.00001287
Iteration 169/1000 | Loss: 0.00001287
Iteration 170/1000 | Loss: 0.00001287
Iteration 171/1000 | Loss: 0.00001287
Iteration 172/1000 | Loss: 0.00001287
Iteration 173/1000 | Loss: 0.00001287
Iteration 174/1000 | Loss: 0.00001287
Iteration 175/1000 | Loss: 0.00001287
Iteration 176/1000 | Loss: 0.00001287
Iteration 177/1000 | Loss: 0.00001287
Iteration 178/1000 | Loss: 0.00001287
Iteration 179/1000 | Loss: 0.00001287
Iteration 180/1000 | Loss: 0.00001287
Iteration 181/1000 | Loss: 0.00001287
Iteration 182/1000 | Loss: 0.00001287
Iteration 183/1000 | Loss: 0.00001287
Iteration 184/1000 | Loss: 0.00001287
Iteration 185/1000 | Loss: 0.00001287
Iteration 186/1000 | Loss: 0.00001287
Iteration 187/1000 | Loss: 0.00001287
Iteration 188/1000 | Loss: 0.00001287
Iteration 189/1000 | Loss: 0.00001287
Iteration 190/1000 | Loss: 0.00001287
Iteration 191/1000 | Loss: 0.00001287
Iteration 192/1000 | Loss: 0.00001287
Iteration 193/1000 | Loss: 0.00001287
Iteration 194/1000 | Loss: 0.00001287
Iteration 195/1000 | Loss: 0.00001287
Iteration 196/1000 | Loss: 0.00001287
Iteration 197/1000 | Loss: 0.00001287
Iteration 198/1000 | Loss: 0.00001287
Iteration 199/1000 | Loss: 0.00001287
Iteration 200/1000 | Loss: 0.00001287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.2866574252257124e-05, 1.2866574252257124e-05, 1.2866574252257124e-05, 1.2866574252257124e-05, 1.2866574252257124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2866574252257124e-05

Optimization complete. Final v2v error: 2.942209243774414 mm

Highest mean error: 3.5243897438049316 mm for frame 15

Lowest mean error: 2.3339200019836426 mm for frame 108

Saving results

Total time: 40.53578066825867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804503
Iteration 2/25 | Loss: 0.00120244
Iteration 3/25 | Loss: 0.00112202
Iteration 4/25 | Loss: 0.00110000
Iteration 5/25 | Loss: 0.00109352
Iteration 6/25 | Loss: 0.00109152
Iteration 7/25 | Loss: 0.00109050
Iteration 8/25 | Loss: 0.00109050
Iteration 9/25 | Loss: 0.00109050
Iteration 10/25 | Loss: 0.00109050
Iteration 11/25 | Loss: 0.00109050
Iteration 12/25 | Loss: 0.00109050
Iteration 13/25 | Loss: 0.00109050
Iteration 14/25 | Loss: 0.00109050
Iteration 15/25 | Loss: 0.00109050
Iteration 16/25 | Loss: 0.00109050
Iteration 17/25 | Loss: 0.00109050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001090502250008285, 0.001090502250008285, 0.001090502250008285, 0.001090502250008285, 0.001090502250008285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001090502250008285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47652090
Iteration 2/25 | Loss: 0.00094080
Iteration 3/25 | Loss: 0.00094080
Iteration 4/25 | Loss: 0.00094080
Iteration 5/25 | Loss: 0.00094080
Iteration 6/25 | Loss: 0.00094080
Iteration 7/25 | Loss: 0.00094079
Iteration 8/25 | Loss: 0.00094079
Iteration 9/25 | Loss: 0.00094079
Iteration 10/25 | Loss: 0.00094079
Iteration 11/25 | Loss: 0.00094079
Iteration 12/25 | Loss: 0.00094079
Iteration 13/25 | Loss: 0.00094079
Iteration 14/25 | Loss: 0.00094079
Iteration 15/25 | Loss: 0.00094079
Iteration 16/25 | Loss: 0.00094079
Iteration 17/25 | Loss: 0.00094079
Iteration 18/25 | Loss: 0.00094079
Iteration 19/25 | Loss: 0.00094079
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009407937759533525, 0.0009407937759533525, 0.0009407937759533525, 0.0009407937759533525, 0.0009407937759533525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009407937759533525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094079
Iteration 2/1000 | Loss: 0.00005310
Iteration 3/1000 | Loss: 0.00003789
Iteration 4/1000 | Loss: 0.00002796
Iteration 5/1000 | Loss: 0.00002520
Iteration 6/1000 | Loss: 0.00002391
Iteration 7/1000 | Loss: 0.00002272
Iteration 8/1000 | Loss: 0.00002202
Iteration 9/1000 | Loss: 0.00002153
Iteration 10/1000 | Loss: 0.00002112
Iteration 11/1000 | Loss: 0.00002082
Iteration 12/1000 | Loss: 0.00002066
Iteration 13/1000 | Loss: 0.00002052
Iteration 14/1000 | Loss: 0.00002041
Iteration 15/1000 | Loss: 0.00002040
Iteration 16/1000 | Loss: 0.00002039
Iteration 17/1000 | Loss: 0.00002038
Iteration 18/1000 | Loss: 0.00002036
Iteration 19/1000 | Loss: 0.00002034
Iteration 20/1000 | Loss: 0.00002026
Iteration 21/1000 | Loss: 0.00002020
Iteration 22/1000 | Loss: 0.00002020
Iteration 23/1000 | Loss: 0.00002014
Iteration 24/1000 | Loss: 0.00002013
Iteration 25/1000 | Loss: 0.00002011
Iteration 26/1000 | Loss: 0.00002010
Iteration 27/1000 | Loss: 0.00002010
Iteration 28/1000 | Loss: 0.00002010
Iteration 29/1000 | Loss: 0.00002008
Iteration 30/1000 | Loss: 0.00002007
Iteration 31/1000 | Loss: 0.00002006
Iteration 32/1000 | Loss: 0.00002005
Iteration 33/1000 | Loss: 0.00002005
Iteration 34/1000 | Loss: 0.00002005
Iteration 35/1000 | Loss: 0.00002004
Iteration 36/1000 | Loss: 0.00002004
Iteration 37/1000 | Loss: 0.00002003
Iteration 38/1000 | Loss: 0.00002001
Iteration 39/1000 | Loss: 0.00002001
Iteration 40/1000 | Loss: 0.00002000
Iteration 41/1000 | Loss: 0.00001999
Iteration 42/1000 | Loss: 0.00001998
Iteration 43/1000 | Loss: 0.00001998
Iteration 44/1000 | Loss: 0.00001997
Iteration 45/1000 | Loss: 0.00001996
Iteration 46/1000 | Loss: 0.00001995
Iteration 47/1000 | Loss: 0.00001995
Iteration 48/1000 | Loss: 0.00001994
Iteration 49/1000 | Loss: 0.00001993
Iteration 50/1000 | Loss: 0.00001993
Iteration 51/1000 | Loss: 0.00001992
Iteration 52/1000 | Loss: 0.00001990
Iteration 53/1000 | Loss: 0.00001990
Iteration 54/1000 | Loss: 0.00001989
Iteration 55/1000 | Loss: 0.00001989
Iteration 56/1000 | Loss: 0.00001989
Iteration 57/1000 | Loss: 0.00001988
Iteration 58/1000 | Loss: 0.00001988
Iteration 59/1000 | Loss: 0.00001988
Iteration 60/1000 | Loss: 0.00001987
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00001987
Iteration 63/1000 | Loss: 0.00001986
Iteration 64/1000 | Loss: 0.00001986
Iteration 65/1000 | Loss: 0.00001985
Iteration 66/1000 | Loss: 0.00001985
Iteration 67/1000 | Loss: 0.00001984
Iteration 68/1000 | Loss: 0.00001983
Iteration 69/1000 | Loss: 0.00001983
Iteration 70/1000 | Loss: 0.00001983
Iteration 71/1000 | Loss: 0.00001982
Iteration 72/1000 | Loss: 0.00001982
Iteration 73/1000 | Loss: 0.00001982
Iteration 74/1000 | Loss: 0.00001981
Iteration 75/1000 | Loss: 0.00001981
Iteration 76/1000 | Loss: 0.00001981
Iteration 77/1000 | Loss: 0.00001980
Iteration 78/1000 | Loss: 0.00001980
Iteration 79/1000 | Loss: 0.00001980
Iteration 80/1000 | Loss: 0.00001980
Iteration 81/1000 | Loss: 0.00001980
Iteration 82/1000 | Loss: 0.00001979
Iteration 83/1000 | Loss: 0.00001979
Iteration 84/1000 | Loss: 0.00001979
Iteration 85/1000 | Loss: 0.00001978
Iteration 86/1000 | Loss: 0.00001978
Iteration 87/1000 | Loss: 0.00001978
Iteration 88/1000 | Loss: 0.00001977
Iteration 89/1000 | Loss: 0.00001977
Iteration 90/1000 | Loss: 0.00001977
Iteration 91/1000 | Loss: 0.00001977
Iteration 92/1000 | Loss: 0.00001976
Iteration 93/1000 | Loss: 0.00001976
Iteration 94/1000 | Loss: 0.00001976
Iteration 95/1000 | Loss: 0.00001976
Iteration 96/1000 | Loss: 0.00001976
Iteration 97/1000 | Loss: 0.00001975
Iteration 98/1000 | Loss: 0.00001975
Iteration 99/1000 | Loss: 0.00001975
Iteration 100/1000 | Loss: 0.00001975
Iteration 101/1000 | Loss: 0.00001975
Iteration 102/1000 | Loss: 0.00001974
Iteration 103/1000 | Loss: 0.00001974
Iteration 104/1000 | Loss: 0.00001974
Iteration 105/1000 | Loss: 0.00001974
Iteration 106/1000 | Loss: 0.00001974
Iteration 107/1000 | Loss: 0.00001973
Iteration 108/1000 | Loss: 0.00001973
Iteration 109/1000 | Loss: 0.00001973
Iteration 110/1000 | Loss: 0.00001972
Iteration 111/1000 | Loss: 0.00001972
Iteration 112/1000 | Loss: 0.00001972
Iteration 113/1000 | Loss: 0.00001972
Iteration 114/1000 | Loss: 0.00001972
Iteration 115/1000 | Loss: 0.00001971
Iteration 116/1000 | Loss: 0.00001971
Iteration 117/1000 | Loss: 0.00001971
Iteration 118/1000 | Loss: 0.00001971
Iteration 119/1000 | Loss: 0.00001971
Iteration 120/1000 | Loss: 0.00001971
Iteration 121/1000 | Loss: 0.00001971
Iteration 122/1000 | Loss: 0.00001970
Iteration 123/1000 | Loss: 0.00001970
Iteration 124/1000 | Loss: 0.00001970
Iteration 125/1000 | Loss: 0.00001970
Iteration 126/1000 | Loss: 0.00001970
Iteration 127/1000 | Loss: 0.00001970
Iteration 128/1000 | Loss: 0.00001970
Iteration 129/1000 | Loss: 0.00001969
Iteration 130/1000 | Loss: 0.00001969
Iteration 131/1000 | Loss: 0.00001969
Iteration 132/1000 | Loss: 0.00001969
Iteration 133/1000 | Loss: 0.00001969
Iteration 134/1000 | Loss: 0.00001969
Iteration 135/1000 | Loss: 0.00001969
Iteration 136/1000 | Loss: 0.00001969
Iteration 137/1000 | Loss: 0.00001969
Iteration 138/1000 | Loss: 0.00001969
Iteration 139/1000 | Loss: 0.00001969
Iteration 140/1000 | Loss: 0.00001969
Iteration 141/1000 | Loss: 0.00001968
Iteration 142/1000 | Loss: 0.00001968
Iteration 143/1000 | Loss: 0.00001968
Iteration 144/1000 | Loss: 0.00001968
Iteration 145/1000 | Loss: 0.00001968
Iteration 146/1000 | Loss: 0.00001968
Iteration 147/1000 | Loss: 0.00001968
Iteration 148/1000 | Loss: 0.00001968
Iteration 149/1000 | Loss: 0.00001968
Iteration 150/1000 | Loss: 0.00001968
Iteration 151/1000 | Loss: 0.00001968
Iteration 152/1000 | Loss: 0.00001968
Iteration 153/1000 | Loss: 0.00001968
Iteration 154/1000 | Loss: 0.00001968
Iteration 155/1000 | Loss: 0.00001968
Iteration 156/1000 | Loss: 0.00001968
Iteration 157/1000 | Loss: 0.00001968
Iteration 158/1000 | Loss: 0.00001968
Iteration 159/1000 | Loss: 0.00001968
Iteration 160/1000 | Loss: 0.00001968
Iteration 161/1000 | Loss: 0.00001968
Iteration 162/1000 | Loss: 0.00001968
Iteration 163/1000 | Loss: 0.00001968
Iteration 164/1000 | Loss: 0.00001968
Iteration 165/1000 | Loss: 0.00001968
Iteration 166/1000 | Loss: 0.00001968
Iteration 167/1000 | Loss: 0.00001968
Iteration 168/1000 | Loss: 0.00001968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.968106698768679e-05, 1.968106698768679e-05, 1.968106698768679e-05, 1.968106698768679e-05, 1.968106698768679e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.968106698768679e-05

Optimization complete. Final v2v error: 3.643500804901123 mm

Highest mean error: 5.4243950843811035 mm for frame 128

Lowest mean error: 2.661221981048584 mm for frame 44

Saving results

Total time: 43.544241189956665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821404
Iteration 2/25 | Loss: 0.00118007
Iteration 3/25 | Loss: 0.00106979
Iteration 4/25 | Loss: 0.00105901
Iteration 5/25 | Loss: 0.00105727
Iteration 6/25 | Loss: 0.00105699
Iteration 7/25 | Loss: 0.00105699
Iteration 8/25 | Loss: 0.00105699
Iteration 9/25 | Loss: 0.00105699
Iteration 10/25 | Loss: 0.00105699
Iteration 11/25 | Loss: 0.00105699
Iteration 12/25 | Loss: 0.00105699
Iteration 13/25 | Loss: 0.00105699
Iteration 14/25 | Loss: 0.00105697
Iteration 15/25 | Loss: 0.00105697
Iteration 16/25 | Loss: 0.00105697
Iteration 17/25 | Loss: 0.00105697
Iteration 18/25 | Loss: 0.00105697
Iteration 19/25 | Loss: 0.00105697
Iteration 20/25 | Loss: 0.00105697
Iteration 21/25 | Loss: 0.00105697
Iteration 22/25 | Loss: 0.00105697
Iteration 23/25 | Loss: 0.00105697
Iteration 24/25 | Loss: 0.00105697
Iteration 25/25 | Loss: 0.00105697

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34726906
Iteration 2/25 | Loss: 0.00069687
Iteration 3/25 | Loss: 0.00069684
Iteration 4/25 | Loss: 0.00069684
Iteration 5/25 | Loss: 0.00069684
Iteration 6/25 | Loss: 0.00069684
Iteration 7/25 | Loss: 0.00069684
Iteration 8/25 | Loss: 0.00069684
Iteration 9/25 | Loss: 0.00069684
Iteration 10/25 | Loss: 0.00069684
Iteration 11/25 | Loss: 0.00069684
Iteration 12/25 | Loss: 0.00069684
Iteration 13/25 | Loss: 0.00069684
Iteration 14/25 | Loss: 0.00069684
Iteration 15/25 | Loss: 0.00069684
Iteration 16/25 | Loss: 0.00069684
Iteration 17/25 | Loss: 0.00069684
Iteration 18/25 | Loss: 0.00069684
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006968406378291547, 0.0006968406378291547, 0.0006968406378291547, 0.0006968406378291547, 0.0006968406378291547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006968406378291547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069684
Iteration 2/1000 | Loss: 0.00001830
Iteration 3/1000 | Loss: 0.00001161
Iteration 4/1000 | Loss: 0.00001032
Iteration 5/1000 | Loss: 0.00000934
Iteration 6/1000 | Loss: 0.00000887
Iteration 7/1000 | Loss: 0.00000855
Iteration 8/1000 | Loss: 0.00000827
Iteration 9/1000 | Loss: 0.00000827
Iteration 10/1000 | Loss: 0.00000823
Iteration 11/1000 | Loss: 0.00000822
Iteration 12/1000 | Loss: 0.00000818
Iteration 13/1000 | Loss: 0.00000818
Iteration 14/1000 | Loss: 0.00000817
Iteration 15/1000 | Loss: 0.00000816
Iteration 16/1000 | Loss: 0.00000810
Iteration 17/1000 | Loss: 0.00000807
Iteration 18/1000 | Loss: 0.00000798
Iteration 19/1000 | Loss: 0.00000793
Iteration 20/1000 | Loss: 0.00000792
Iteration 21/1000 | Loss: 0.00000791
Iteration 22/1000 | Loss: 0.00000791
Iteration 23/1000 | Loss: 0.00000786
Iteration 24/1000 | Loss: 0.00000786
Iteration 25/1000 | Loss: 0.00000786
Iteration 26/1000 | Loss: 0.00000786
Iteration 27/1000 | Loss: 0.00000786
Iteration 28/1000 | Loss: 0.00000786
Iteration 29/1000 | Loss: 0.00000786
Iteration 30/1000 | Loss: 0.00000786
Iteration 31/1000 | Loss: 0.00000786
Iteration 32/1000 | Loss: 0.00000785
Iteration 33/1000 | Loss: 0.00000785
Iteration 34/1000 | Loss: 0.00000780
Iteration 35/1000 | Loss: 0.00000780
Iteration 36/1000 | Loss: 0.00000779
Iteration 37/1000 | Loss: 0.00000779
Iteration 38/1000 | Loss: 0.00000778
Iteration 39/1000 | Loss: 0.00000778
Iteration 40/1000 | Loss: 0.00000777
Iteration 41/1000 | Loss: 0.00000777
Iteration 42/1000 | Loss: 0.00000777
Iteration 43/1000 | Loss: 0.00000777
Iteration 44/1000 | Loss: 0.00000777
Iteration 45/1000 | Loss: 0.00000776
Iteration 46/1000 | Loss: 0.00000776
Iteration 47/1000 | Loss: 0.00000775
Iteration 48/1000 | Loss: 0.00000775
Iteration 49/1000 | Loss: 0.00000774
Iteration 50/1000 | Loss: 0.00000773
Iteration 51/1000 | Loss: 0.00000771
Iteration 52/1000 | Loss: 0.00000770
Iteration 53/1000 | Loss: 0.00000770
Iteration 54/1000 | Loss: 0.00000769
Iteration 55/1000 | Loss: 0.00000769
Iteration 56/1000 | Loss: 0.00000769
Iteration 57/1000 | Loss: 0.00000768
Iteration 58/1000 | Loss: 0.00000768
Iteration 59/1000 | Loss: 0.00000768
Iteration 60/1000 | Loss: 0.00000768
Iteration 61/1000 | Loss: 0.00000768
Iteration 62/1000 | Loss: 0.00000768
Iteration 63/1000 | Loss: 0.00000768
Iteration 64/1000 | Loss: 0.00000766
Iteration 65/1000 | Loss: 0.00000766
Iteration 66/1000 | Loss: 0.00000765
Iteration 67/1000 | Loss: 0.00000764
Iteration 68/1000 | Loss: 0.00000764
Iteration 69/1000 | Loss: 0.00000764
Iteration 70/1000 | Loss: 0.00000764
Iteration 71/1000 | Loss: 0.00000764
Iteration 72/1000 | Loss: 0.00000764
Iteration 73/1000 | Loss: 0.00000764
Iteration 74/1000 | Loss: 0.00000764
Iteration 75/1000 | Loss: 0.00000764
Iteration 76/1000 | Loss: 0.00000764
Iteration 77/1000 | Loss: 0.00000763
Iteration 78/1000 | Loss: 0.00000763
Iteration 79/1000 | Loss: 0.00000763
Iteration 80/1000 | Loss: 0.00000762
Iteration 81/1000 | Loss: 0.00000762
Iteration 82/1000 | Loss: 0.00000762
Iteration 83/1000 | Loss: 0.00000762
Iteration 84/1000 | Loss: 0.00000762
Iteration 85/1000 | Loss: 0.00000761
Iteration 86/1000 | Loss: 0.00000761
Iteration 87/1000 | Loss: 0.00000761
Iteration 88/1000 | Loss: 0.00000761
Iteration 89/1000 | Loss: 0.00000761
Iteration 90/1000 | Loss: 0.00000761
Iteration 91/1000 | Loss: 0.00000761
Iteration 92/1000 | Loss: 0.00000761
Iteration 93/1000 | Loss: 0.00000761
Iteration 94/1000 | Loss: 0.00000761
Iteration 95/1000 | Loss: 0.00000761
Iteration 96/1000 | Loss: 0.00000761
Iteration 97/1000 | Loss: 0.00000760
Iteration 98/1000 | Loss: 0.00000760
Iteration 99/1000 | Loss: 0.00000760
Iteration 100/1000 | Loss: 0.00000760
Iteration 101/1000 | Loss: 0.00000760
Iteration 102/1000 | Loss: 0.00000760
Iteration 103/1000 | Loss: 0.00000760
Iteration 104/1000 | Loss: 0.00000760
Iteration 105/1000 | Loss: 0.00000760
Iteration 106/1000 | Loss: 0.00000760
Iteration 107/1000 | Loss: 0.00000760
Iteration 108/1000 | Loss: 0.00000760
Iteration 109/1000 | Loss: 0.00000760
Iteration 110/1000 | Loss: 0.00000760
Iteration 111/1000 | Loss: 0.00000759
Iteration 112/1000 | Loss: 0.00000759
Iteration 113/1000 | Loss: 0.00000759
Iteration 114/1000 | Loss: 0.00000759
Iteration 115/1000 | Loss: 0.00000759
Iteration 116/1000 | Loss: 0.00000759
Iteration 117/1000 | Loss: 0.00000759
Iteration 118/1000 | Loss: 0.00000759
Iteration 119/1000 | Loss: 0.00000759
Iteration 120/1000 | Loss: 0.00000759
Iteration 121/1000 | Loss: 0.00000759
Iteration 122/1000 | Loss: 0.00000759
Iteration 123/1000 | Loss: 0.00000759
Iteration 124/1000 | Loss: 0.00000759
Iteration 125/1000 | Loss: 0.00000759
Iteration 126/1000 | Loss: 0.00000759
Iteration 127/1000 | Loss: 0.00000758
Iteration 128/1000 | Loss: 0.00000758
Iteration 129/1000 | Loss: 0.00000758
Iteration 130/1000 | Loss: 0.00000758
Iteration 131/1000 | Loss: 0.00000758
Iteration 132/1000 | Loss: 0.00000758
Iteration 133/1000 | Loss: 0.00000758
Iteration 134/1000 | Loss: 0.00000758
Iteration 135/1000 | Loss: 0.00000758
Iteration 136/1000 | Loss: 0.00000758
Iteration 137/1000 | Loss: 0.00000758
Iteration 138/1000 | Loss: 0.00000758
Iteration 139/1000 | Loss: 0.00000757
Iteration 140/1000 | Loss: 0.00000757
Iteration 141/1000 | Loss: 0.00000757
Iteration 142/1000 | Loss: 0.00000757
Iteration 143/1000 | Loss: 0.00000757
Iteration 144/1000 | Loss: 0.00000757
Iteration 145/1000 | Loss: 0.00000757
Iteration 146/1000 | Loss: 0.00000757
Iteration 147/1000 | Loss: 0.00000757
Iteration 148/1000 | Loss: 0.00000757
Iteration 149/1000 | Loss: 0.00000757
Iteration 150/1000 | Loss: 0.00000757
Iteration 151/1000 | Loss: 0.00000757
Iteration 152/1000 | Loss: 0.00000756
Iteration 153/1000 | Loss: 0.00000756
Iteration 154/1000 | Loss: 0.00000756
Iteration 155/1000 | Loss: 0.00000756
Iteration 156/1000 | Loss: 0.00000756
Iteration 157/1000 | Loss: 0.00000756
Iteration 158/1000 | Loss: 0.00000756
Iteration 159/1000 | Loss: 0.00000756
Iteration 160/1000 | Loss: 0.00000756
Iteration 161/1000 | Loss: 0.00000755
Iteration 162/1000 | Loss: 0.00000755
Iteration 163/1000 | Loss: 0.00000755
Iteration 164/1000 | Loss: 0.00000755
Iteration 165/1000 | Loss: 0.00000755
Iteration 166/1000 | Loss: 0.00000755
Iteration 167/1000 | Loss: 0.00000755
Iteration 168/1000 | Loss: 0.00000755
Iteration 169/1000 | Loss: 0.00000755
Iteration 170/1000 | Loss: 0.00000755
Iteration 171/1000 | Loss: 0.00000755
Iteration 172/1000 | Loss: 0.00000755
Iteration 173/1000 | Loss: 0.00000755
Iteration 174/1000 | Loss: 0.00000755
Iteration 175/1000 | Loss: 0.00000754
Iteration 176/1000 | Loss: 0.00000754
Iteration 177/1000 | Loss: 0.00000754
Iteration 178/1000 | Loss: 0.00000754
Iteration 179/1000 | Loss: 0.00000754
Iteration 180/1000 | Loss: 0.00000754
Iteration 181/1000 | Loss: 0.00000754
Iteration 182/1000 | Loss: 0.00000753
Iteration 183/1000 | Loss: 0.00000753
Iteration 184/1000 | Loss: 0.00000753
Iteration 185/1000 | Loss: 0.00000753
Iteration 186/1000 | Loss: 0.00000752
Iteration 187/1000 | Loss: 0.00000752
Iteration 188/1000 | Loss: 0.00000752
Iteration 189/1000 | Loss: 0.00000752
Iteration 190/1000 | Loss: 0.00000752
Iteration 191/1000 | Loss: 0.00000752
Iteration 192/1000 | Loss: 0.00000752
Iteration 193/1000 | Loss: 0.00000752
Iteration 194/1000 | Loss: 0.00000752
Iteration 195/1000 | Loss: 0.00000752
Iteration 196/1000 | Loss: 0.00000752
Iteration 197/1000 | Loss: 0.00000752
Iteration 198/1000 | Loss: 0.00000752
Iteration 199/1000 | Loss: 0.00000752
Iteration 200/1000 | Loss: 0.00000752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [7.5229113463137764e-06, 7.5229113463137764e-06, 7.5229113463137764e-06, 7.5229113463137764e-06, 7.5229113463137764e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.5229113463137764e-06

Optimization complete. Final v2v error: 2.368488311767578 mm

Highest mean error: 2.495358943939209 mm for frame 19

Lowest mean error: 2.286290168762207 mm for frame 39

Saving results

Total time: 35.338557720184326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063128
Iteration 2/25 | Loss: 0.00488566
Iteration 3/25 | Loss: 0.00468906
Iteration 4/25 | Loss: 0.00267337
Iteration 5/25 | Loss: 0.00210643
Iteration 6/25 | Loss: 0.00194418
Iteration 7/25 | Loss: 0.00182053
Iteration 8/25 | Loss: 0.00180790
Iteration 9/25 | Loss: 0.00175168
Iteration 10/25 | Loss: 0.00168403
Iteration 11/25 | Loss: 0.00166304
Iteration 12/25 | Loss: 0.00161621
Iteration 13/25 | Loss: 0.00153176
Iteration 14/25 | Loss: 0.00148966
Iteration 15/25 | Loss: 0.00144052
Iteration 16/25 | Loss: 0.00140054
Iteration 17/25 | Loss: 0.00138265
Iteration 18/25 | Loss: 0.00137064
Iteration 19/25 | Loss: 0.00136662
Iteration 20/25 | Loss: 0.00135783
Iteration 21/25 | Loss: 0.00135384
Iteration 22/25 | Loss: 0.00135529
Iteration 23/25 | Loss: 0.00135262
Iteration 24/25 | Loss: 0.00134557
Iteration 25/25 | Loss: 0.00134301

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.57343829
Iteration 2/25 | Loss: 0.00239599
Iteration 3/25 | Loss: 0.00239599
Iteration 4/25 | Loss: 0.00239599
Iteration 5/25 | Loss: 0.00239599
Iteration 6/25 | Loss: 0.00239599
Iteration 7/25 | Loss: 0.00239599
Iteration 8/25 | Loss: 0.00239599
Iteration 9/25 | Loss: 0.00239599
Iteration 10/25 | Loss: 0.00239599
Iteration 11/25 | Loss: 0.00239599
Iteration 12/25 | Loss: 0.00239599
Iteration 13/25 | Loss: 0.00239599
Iteration 14/25 | Loss: 0.00239599
Iteration 15/25 | Loss: 0.00239599
Iteration 16/25 | Loss: 0.00239599
Iteration 17/25 | Loss: 0.00239599
Iteration 18/25 | Loss: 0.00239599
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002395988442003727, 0.002395988442003727, 0.002395988442003727, 0.002395988442003727, 0.002395988442003727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002395988442003727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239599
Iteration 2/1000 | Loss: 0.00042249
Iteration 3/1000 | Loss: 0.00018309
Iteration 4/1000 | Loss: 0.00015148
Iteration 5/1000 | Loss: 0.00012765
Iteration 6/1000 | Loss: 0.00043567
Iteration 7/1000 | Loss: 0.00014905
Iteration 8/1000 | Loss: 0.00044735
Iteration 9/1000 | Loss: 0.00041336
Iteration 10/1000 | Loss: 0.00039389
Iteration 11/1000 | Loss: 0.00011121
Iteration 12/1000 | Loss: 0.00009614
Iteration 13/1000 | Loss: 0.00009002
Iteration 14/1000 | Loss: 0.00008560
Iteration 15/1000 | Loss: 0.00008172
Iteration 16/1000 | Loss: 0.00007992
Iteration 17/1000 | Loss: 0.00007831
Iteration 18/1000 | Loss: 0.00024217
Iteration 19/1000 | Loss: 0.00008085
Iteration 20/1000 | Loss: 0.00007615
Iteration 21/1000 | Loss: 0.00007476
Iteration 22/1000 | Loss: 0.00007392
Iteration 23/1000 | Loss: 0.00007330
Iteration 24/1000 | Loss: 0.00007274
Iteration 25/1000 | Loss: 0.00007224
Iteration 26/1000 | Loss: 0.00007187
Iteration 27/1000 | Loss: 0.00016244
Iteration 28/1000 | Loss: 0.00043435
Iteration 29/1000 | Loss: 0.00023879
Iteration 30/1000 | Loss: 0.00007302
Iteration 31/1000 | Loss: 0.00015462
Iteration 32/1000 | Loss: 0.00007521
Iteration 33/1000 | Loss: 0.00022905
Iteration 34/1000 | Loss: 0.00009622
Iteration 35/1000 | Loss: 0.00020423
Iteration 36/1000 | Loss: 0.00014231
Iteration 37/1000 | Loss: 0.00007627
Iteration 38/1000 | Loss: 0.00006893
Iteration 39/1000 | Loss: 0.00006482
Iteration 40/1000 | Loss: 0.00006155
Iteration 41/1000 | Loss: 0.00005973
Iteration 42/1000 | Loss: 0.00005864
Iteration 43/1000 | Loss: 0.00005783
Iteration 44/1000 | Loss: 0.00005721
Iteration 45/1000 | Loss: 0.00005679
Iteration 46/1000 | Loss: 0.00005633
Iteration 47/1000 | Loss: 0.00005603
Iteration 48/1000 | Loss: 0.00005559
Iteration 49/1000 | Loss: 0.00005506
Iteration 50/1000 | Loss: 0.00016467
Iteration 51/1000 | Loss: 0.00017566
Iteration 52/1000 | Loss: 0.00005772
Iteration 53/1000 | Loss: 0.00005304
Iteration 54/1000 | Loss: 0.00005070
Iteration 55/1000 | Loss: 0.00004788
Iteration 56/1000 | Loss: 0.00004649
Iteration 57/1000 | Loss: 0.00004566
Iteration 58/1000 | Loss: 0.00004509
Iteration 59/1000 | Loss: 0.00004462
Iteration 60/1000 | Loss: 0.00004414
Iteration 61/1000 | Loss: 0.00004383
Iteration 62/1000 | Loss: 0.00004358
Iteration 63/1000 | Loss: 0.00004338
Iteration 64/1000 | Loss: 0.00004327
Iteration 65/1000 | Loss: 0.00004320
Iteration 66/1000 | Loss: 0.00004317
Iteration 67/1000 | Loss: 0.00004317
Iteration 68/1000 | Loss: 0.00004316
Iteration 69/1000 | Loss: 0.00004315
Iteration 70/1000 | Loss: 0.00004314
Iteration 71/1000 | Loss: 0.00013588
Iteration 72/1000 | Loss: 0.00074768
Iteration 73/1000 | Loss: 0.00019578
Iteration 74/1000 | Loss: 0.00013825
Iteration 75/1000 | Loss: 0.00006151
Iteration 76/1000 | Loss: 0.00004977
Iteration 77/1000 | Loss: 0.00003904
Iteration 78/1000 | Loss: 0.00003050
Iteration 79/1000 | Loss: 0.00002698
Iteration 80/1000 | Loss: 0.00002466
Iteration 81/1000 | Loss: 0.00002331
Iteration 82/1000 | Loss: 0.00002253
Iteration 83/1000 | Loss: 0.00002184
Iteration 84/1000 | Loss: 0.00002130
Iteration 85/1000 | Loss: 0.00002094
Iteration 86/1000 | Loss: 0.00002071
Iteration 87/1000 | Loss: 0.00002059
Iteration 88/1000 | Loss: 0.00002059
Iteration 89/1000 | Loss: 0.00002056
Iteration 90/1000 | Loss: 0.00002054
Iteration 91/1000 | Loss: 0.00002051
Iteration 92/1000 | Loss: 0.00002051
Iteration 93/1000 | Loss: 0.00002051
Iteration 94/1000 | Loss: 0.00002051
Iteration 95/1000 | Loss: 0.00002051
Iteration 96/1000 | Loss: 0.00002050
Iteration 97/1000 | Loss: 0.00002050
Iteration 98/1000 | Loss: 0.00002050
Iteration 99/1000 | Loss: 0.00002049
Iteration 100/1000 | Loss: 0.00002049
Iteration 101/1000 | Loss: 0.00002049
Iteration 102/1000 | Loss: 0.00002049
Iteration 103/1000 | Loss: 0.00002049
Iteration 104/1000 | Loss: 0.00002049
Iteration 105/1000 | Loss: 0.00002049
Iteration 106/1000 | Loss: 0.00002049
Iteration 107/1000 | Loss: 0.00002049
Iteration 108/1000 | Loss: 0.00002049
Iteration 109/1000 | Loss: 0.00002049
Iteration 110/1000 | Loss: 0.00002048
Iteration 111/1000 | Loss: 0.00002048
Iteration 112/1000 | Loss: 0.00002048
Iteration 113/1000 | Loss: 0.00002048
Iteration 114/1000 | Loss: 0.00002048
Iteration 115/1000 | Loss: 0.00002048
Iteration 116/1000 | Loss: 0.00002048
Iteration 117/1000 | Loss: 0.00002048
Iteration 118/1000 | Loss: 0.00002048
Iteration 119/1000 | Loss: 0.00002048
Iteration 120/1000 | Loss: 0.00002048
Iteration 121/1000 | Loss: 0.00002048
Iteration 122/1000 | Loss: 0.00002048
Iteration 123/1000 | Loss: 0.00002048
Iteration 124/1000 | Loss: 0.00002048
Iteration 125/1000 | Loss: 0.00002048
Iteration 126/1000 | Loss: 0.00002048
Iteration 127/1000 | Loss: 0.00002048
Iteration 128/1000 | Loss: 0.00002048
Iteration 129/1000 | Loss: 0.00002048
Iteration 130/1000 | Loss: 0.00002048
Iteration 131/1000 | Loss: 0.00002048
Iteration 132/1000 | Loss: 0.00002048
Iteration 133/1000 | Loss: 0.00002048
Iteration 134/1000 | Loss: 0.00002048
Iteration 135/1000 | Loss: 0.00002048
Iteration 136/1000 | Loss: 0.00002048
Iteration 137/1000 | Loss: 0.00002048
Iteration 138/1000 | Loss: 0.00002048
Iteration 139/1000 | Loss: 0.00002048
Iteration 140/1000 | Loss: 0.00002048
Iteration 141/1000 | Loss: 0.00002048
Iteration 142/1000 | Loss: 0.00002048
Iteration 143/1000 | Loss: 0.00002048
Iteration 144/1000 | Loss: 0.00002048
Iteration 145/1000 | Loss: 0.00002048
Iteration 146/1000 | Loss: 0.00002048
Iteration 147/1000 | Loss: 0.00002048
Iteration 148/1000 | Loss: 0.00002048
Iteration 149/1000 | Loss: 0.00002048
Iteration 150/1000 | Loss: 0.00002048
Iteration 151/1000 | Loss: 0.00002048
Iteration 152/1000 | Loss: 0.00002048
Iteration 153/1000 | Loss: 0.00002048
Iteration 154/1000 | Loss: 0.00002048
Iteration 155/1000 | Loss: 0.00002048
Iteration 156/1000 | Loss: 0.00002048
Iteration 157/1000 | Loss: 0.00002048
Iteration 158/1000 | Loss: 0.00002048
Iteration 159/1000 | Loss: 0.00002048
Iteration 160/1000 | Loss: 0.00002048
Iteration 161/1000 | Loss: 0.00002048
Iteration 162/1000 | Loss: 0.00002048
Iteration 163/1000 | Loss: 0.00002048
Iteration 164/1000 | Loss: 0.00002048
Iteration 165/1000 | Loss: 0.00002048
Iteration 166/1000 | Loss: 0.00002048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.0475183191592805e-05, 2.0475183191592805e-05, 2.0475183191592805e-05, 2.0475183191592805e-05, 2.0475183191592805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0475183191592805e-05

Optimization complete. Final v2v error: 3.8296046257019043 mm

Highest mean error: 4.16591739654541 mm for frame 86

Lowest mean error: 3.623971700668335 mm for frame 57

Saving results

Total time: 167.83995532989502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408578
Iteration 2/25 | Loss: 0.00113740
Iteration 3/25 | Loss: 0.00107559
Iteration 4/25 | Loss: 0.00106689
Iteration 5/25 | Loss: 0.00106454
Iteration 6/25 | Loss: 0.00106414
Iteration 7/25 | Loss: 0.00106414
Iteration 8/25 | Loss: 0.00106414
Iteration 9/25 | Loss: 0.00106414
Iteration 10/25 | Loss: 0.00106414
Iteration 11/25 | Loss: 0.00106414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010641369735822082, 0.0010641369735822082, 0.0010641369735822082, 0.0010641369735822082, 0.0010641369735822082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010641369735822082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.44838047
Iteration 2/25 | Loss: 0.00077818
Iteration 3/25 | Loss: 0.00077818
Iteration 4/25 | Loss: 0.00077818
Iteration 5/25 | Loss: 0.00077818
Iteration 6/25 | Loss: 0.00077818
Iteration 7/25 | Loss: 0.00077818
Iteration 8/25 | Loss: 0.00077818
Iteration 9/25 | Loss: 0.00077818
Iteration 10/25 | Loss: 0.00077818
Iteration 11/25 | Loss: 0.00077818
Iteration 12/25 | Loss: 0.00077818
Iteration 13/25 | Loss: 0.00077817
Iteration 14/25 | Loss: 0.00077817
Iteration 15/25 | Loss: 0.00077817
Iteration 16/25 | Loss: 0.00077817
Iteration 17/25 | Loss: 0.00077817
Iteration 18/25 | Loss: 0.00077817
Iteration 19/25 | Loss: 0.00077817
Iteration 20/25 | Loss: 0.00077817
Iteration 21/25 | Loss: 0.00077817
Iteration 22/25 | Loss: 0.00077817
Iteration 23/25 | Loss: 0.00077817
Iteration 24/25 | Loss: 0.00077817
Iteration 25/25 | Loss: 0.00077817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077817
Iteration 2/1000 | Loss: 0.00001991
Iteration 3/1000 | Loss: 0.00001297
Iteration 4/1000 | Loss: 0.00001179
Iteration 5/1000 | Loss: 0.00001131
Iteration 6/1000 | Loss: 0.00001102
Iteration 7/1000 | Loss: 0.00001077
Iteration 8/1000 | Loss: 0.00001050
Iteration 9/1000 | Loss: 0.00001036
Iteration 10/1000 | Loss: 0.00001016
Iteration 11/1000 | Loss: 0.00001013
Iteration 12/1000 | Loss: 0.00001011
Iteration 13/1000 | Loss: 0.00001009
Iteration 14/1000 | Loss: 0.00001008
Iteration 15/1000 | Loss: 0.00001007
Iteration 16/1000 | Loss: 0.00001007
Iteration 17/1000 | Loss: 0.00001004
Iteration 18/1000 | Loss: 0.00000997
Iteration 19/1000 | Loss: 0.00000992
Iteration 20/1000 | Loss: 0.00000983
Iteration 21/1000 | Loss: 0.00000981
Iteration 22/1000 | Loss: 0.00000978
Iteration 23/1000 | Loss: 0.00000977
Iteration 24/1000 | Loss: 0.00000977
Iteration 25/1000 | Loss: 0.00000976
Iteration 26/1000 | Loss: 0.00000975
Iteration 27/1000 | Loss: 0.00000974
Iteration 28/1000 | Loss: 0.00000973
Iteration 29/1000 | Loss: 0.00000972
Iteration 30/1000 | Loss: 0.00000969
Iteration 31/1000 | Loss: 0.00000969
Iteration 32/1000 | Loss: 0.00000966
Iteration 33/1000 | Loss: 0.00000966
Iteration 34/1000 | Loss: 0.00000965
Iteration 35/1000 | Loss: 0.00000965
Iteration 36/1000 | Loss: 0.00000965
Iteration 37/1000 | Loss: 0.00000965
Iteration 38/1000 | Loss: 0.00000964
Iteration 39/1000 | Loss: 0.00000964
Iteration 40/1000 | Loss: 0.00000964
Iteration 41/1000 | Loss: 0.00000963
Iteration 42/1000 | Loss: 0.00000963
Iteration 43/1000 | Loss: 0.00000962
Iteration 44/1000 | Loss: 0.00000962
Iteration 45/1000 | Loss: 0.00000962
Iteration 46/1000 | Loss: 0.00000961
Iteration 47/1000 | Loss: 0.00000961
Iteration 48/1000 | Loss: 0.00000961
Iteration 49/1000 | Loss: 0.00000961
Iteration 50/1000 | Loss: 0.00000961
Iteration 51/1000 | Loss: 0.00000960
Iteration 52/1000 | Loss: 0.00000960
Iteration 53/1000 | Loss: 0.00000959
Iteration 54/1000 | Loss: 0.00000959
Iteration 55/1000 | Loss: 0.00000959
Iteration 56/1000 | Loss: 0.00000959
Iteration 57/1000 | Loss: 0.00000959
Iteration 58/1000 | Loss: 0.00000958
Iteration 59/1000 | Loss: 0.00000958
Iteration 60/1000 | Loss: 0.00000957
Iteration 61/1000 | Loss: 0.00000957
Iteration 62/1000 | Loss: 0.00000957
Iteration 63/1000 | Loss: 0.00000956
Iteration 64/1000 | Loss: 0.00000956
Iteration 65/1000 | Loss: 0.00000955
Iteration 66/1000 | Loss: 0.00000955
Iteration 67/1000 | Loss: 0.00000955
Iteration 68/1000 | Loss: 0.00000955
Iteration 69/1000 | Loss: 0.00000955
Iteration 70/1000 | Loss: 0.00000954
Iteration 71/1000 | Loss: 0.00000954
Iteration 72/1000 | Loss: 0.00000954
Iteration 73/1000 | Loss: 0.00000954
Iteration 74/1000 | Loss: 0.00000953
Iteration 75/1000 | Loss: 0.00000953
Iteration 76/1000 | Loss: 0.00000952
Iteration 77/1000 | Loss: 0.00000952
Iteration 78/1000 | Loss: 0.00000952
Iteration 79/1000 | Loss: 0.00000952
Iteration 80/1000 | Loss: 0.00000952
Iteration 81/1000 | Loss: 0.00000952
Iteration 82/1000 | Loss: 0.00000951
Iteration 83/1000 | Loss: 0.00000951
Iteration 84/1000 | Loss: 0.00000951
Iteration 85/1000 | Loss: 0.00000950
Iteration 86/1000 | Loss: 0.00000949
Iteration 87/1000 | Loss: 0.00000949
Iteration 88/1000 | Loss: 0.00000949
Iteration 89/1000 | Loss: 0.00000949
Iteration 90/1000 | Loss: 0.00000949
Iteration 91/1000 | Loss: 0.00000949
Iteration 92/1000 | Loss: 0.00000949
Iteration 93/1000 | Loss: 0.00000949
Iteration 94/1000 | Loss: 0.00000948
Iteration 95/1000 | Loss: 0.00000948
Iteration 96/1000 | Loss: 0.00000948
Iteration 97/1000 | Loss: 0.00000948
Iteration 98/1000 | Loss: 0.00000948
Iteration 99/1000 | Loss: 0.00000948
Iteration 100/1000 | Loss: 0.00000948
Iteration 101/1000 | Loss: 0.00000948
Iteration 102/1000 | Loss: 0.00000947
Iteration 103/1000 | Loss: 0.00000947
Iteration 104/1000 | Loss: 0.00000947
Iteration 105/1000 | Loss: 0.00000946
Iteration 106/1000 | Loss: 0.00000946
Iteration 107/1000 | Loss: 0.00000946
Iteration 108/1000 | Loss: 0.00000945
Iteration 109/1000 | Loss: 0.00000945
Iteration 110/1000 | Loss: 0.00000945
Iteration 111/1000 | Loss: 0.00000945
Iteration 112/1000 | Loss: 0.00000945
Iteration 113/1000 | Loss: 0.00000945
Iteration 114/1000 | Loss: 0.00000945
Iteration 115/1000 | Loss: 0.00000945
Iteration 116/1000 | Loss: 0.00000945
Iteration 117/1000 | Loss: 0.00000945
Iteration 118/1000 | Loss: 0.00000945
Iteration 119/1000 | Loss: 0.00000944
Iteration 120/1000 | Loss: 0.00000944
Iteration 121/1000 | Loss: 0.00000944
Iteration 122/1000 | Loss: 0.00000944
Iteration 123/1000 | Loss: 0.00000944
Iteration 124/1000 | Loss: 0.00000944
Iteration 125/1000 | Loss: 0.00000944
Iteration 126/1000 | Loss: 0.00000944
Iteration 127/1000 | Loss: 0.00000943
Iteration 128/1000 | Loss: 0.00000943
Iteration 129/1000 | Loss: 0.00000943
Iteration 130/1000 | Loss: 0.00000943
Iteration 131/1000 | Loss: 0.00000943
Iteration 132/1000 | Loss: 0.00000943
Iteration 133/1000 | Loss: 0.00000943
Iteration 134/1000 | Loss: 0.00000943
Iteration 135/1000 | Loss: 0.00000943
Iteration 136/1000 | Loss: 0.00000943
Iteration 137/1000 | Loss: 0.00000943
Iteration 138/1000 | Loss: 0.00000943
Iteration 139/1000 | Loss: 0.00000943
Iteration 140/1000 | Loss: 0.00000943
Iteration 141/1000 | Loss: 0.00000943
Iteration 142/1000 | Loss: 0.00000943
Iteration 143/1000 | Loss: 0.00000942
Iteration 144/1000 | Loss: 0.00000942
Iteration 145/1000 | Loss: 0.00000942
Iteration 146/1000 | Loss: 0.00000942
Iteration 147/1000 | Loss: 0.00000941
Iteration 148/1000 | Loss: 0.00000941
Iteration 149/1000 | Loss: 0.00000941
Iteration 150/1000 | Loss: 0.00000941
Iteration 151/1000 | Loss: 0.00000941
Iteration 152/1000 | Loss: 0.00000941
Iteration 153/1000 | Loss: 0.00000941
Iteration 154/1000 | Loss: 0.00000941
Iteration 155/1000 | Loss: 0.00000941
Iteration 156/1000 | Loss: 0.00000941
Iteration 157/1000 | Loss: 0.00000940
Iteration 158/1000 | Loss: 0.00000940
Iteration 159/1000 | Loss: 0.00000940
Iteration 160/1000 | Loss: 0.00000940
Iteration 161/1000 | Loss: 0.00000940
Iteration 162/1000 | Loss: 0.00000939
Iteration 163/1000 | Loss: 0.00000939
Iteration 164/1000 | Loss: 0.00000939
Iteration 165/1000 | Loss: 0.00000939
Iteration 166/1000 | Loss: 0.00000939
Iteration 167/1000 | Loss: 0.00000939
Iteration 168/1000 | Loss: 0.00000939
Iteration 169/1000 | Loss: 0.00000939
Iteration 170/1000 | Loss: 0.00000938
Iteration 171/1000 | Loss: 0.00000938
Iteration 172/1000 | Loss: 0.00000938
Iteration 173/1000 | Loss: 0.00000938
Iteration 174/1000 | Loss: 0.00000938
Iteration 175/1000 | Loss: 0.00000938
Iteration 176/1000 | Loss: 0.00000938
Iteration 177/1000 | Loss: 0.00000938
Iteration 178/1000 | Loss: 0.00000938
Iteration 179/1000 | Loss: 0.00000938
Iteration 180/1000 | Loss: 0.00000938
Iteration 181/1000 | Loss: 0.00000938
Iteration 182/1000 | Loss: 0.00000938
Iteration 183/1000 | Loss: 0.00000938
Iteration 184/1000 | Loss: 0.00000938
Iteration 185/1000 | Loss: 0.00000938
Iteration 186/1000 | Loss: 0.00000938
Iteration 187/1000 | Loss: 0.00000937
Iteration 188/1000 | Loss: 0.00000937
Iteration 189/1000 | Loss: 0.00000937
Iteration 190/1000 | Loss: 0.00000937
Iteration 191/1000 | Loss: 0.00000937
Iteration 192/1000 | Loss: 0.00000937
Iteration 193/1000 | Loss: 0.00000937
Iteration 194/1000 | Loss: 0.00000937
Iteration 195/1000 | Loss: 0.00000937
Iteration 196/1000 | Loss: 0.00000937
Iteration 197/1000 | Loss: 0.00000937
Iteration 198/1000 | Loss: 0.00000937
Iteration 199/1000 | Loss: 0.00000937
Iteration 200/1000 | Loss: 0.00000937
Iteration 201/1000 | Loss: 0.00000937
Iteration 202/1000 | Loss: 0.00000937
Iteration 203/1000 | Loss: 0.00000937
Iteration 204/1000 | Loss: 0.00000937
Iteration 205/1000 | Loss: 0.00000937
Iteration 206/1000 | Loss: 0.00000937
Iteration 207/1000 | Loss: 0.00000937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [9.367627171741333e-06, 9.367627171741333e-06, 9.367627171741333e-06, 9.367627171741333e-06, 9.367627171741333e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.367627171741333e-06

Optimization complete. Final v2v error: 2.6307928562164307 mm

Highest mean error: 3.03572154045105 mm for frame 57

Lowest mean error: 2.374805450439453 mm for frame 42

Saving results

Total time: 39.860896587371826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060999
Iteration 2/25 | Loss: 0.00523297
Iteration 3/25 | Loss: 0.00335413
Iteration 4/25 | Loss: 0.00265047
Iteration 5/25 | Loss: 0.00232854
Iteration 6/25 | Loss: 0.00225237
Iteration 7/25 | Loss: 0.00219071
Iteration 8/25 | Loss: 0.00187211
Iteration 9/25 | Loss: 0.00176432
Iteration 10/25 | Loss: 0.00160959
Iteration 11/25 | Loss: 0.00149862
Iteration 12/25 | Loss: 0.00148148
Iteration 13/25 | Loss: 0.00147812
Iteration 14/25 | Loss: 0.00145554
Iteration 15/25 | Loss: 0.00142156
Iteration 16/25 | Loss: 0.00140687
Iteration 17/25 | Loss: 0.00137790
Iteration 18/25 | Loss: 0.00136178
Iteration 19/25 | Loss: 0.00136913
Iteration 20/25 | Loss: 0.00138806
Iteration 21/25 | Loss: 0.00136352
Iteration 22/25 | Loss: 0.00133718
Iteration 23/25 | Loss: 0.00132554
Iteration 24/25 | Loss: 0.00132328
Iteration 25/25 | Loss: 0.00132181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.57412863
Iteration 2/25 | Loss: 0.00083380
Iteration 3/25 | Loss: 0.00081117
Iteration 4/25 | Loss: 0.00079781
Iteration 5/25 | Loss: 0.00079781
Iteration 6/25 | Loss: 0.00079781
Iteration 7/25 | Loss: 0.00079781
Iteration 8/25 | Loss: 0.00079781
Iteration 9/25 | Loss: 0.00079781
Iteration 10/25 | Loss: 0.00079781
Iteration 11/25 | Loss: 0.00079781
Iteration 12/25 | Loss: 0.00079781
Iteration 13/25 | Loss: 0.00079781
Iteration 14/25 | Loss: 0.00079781
Iteration 15/25 | Loss: 0.00079781
Iteration 16/25 | Loss: 0.00079781
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007978116045705974, 0.0007978116045705974, 0.0007978116045705974, 0.0007978116045705974, 0.0007978116045705974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007978116045705974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079781
Iteration 2/1000 | Loss: 0.00060281
Iteration 3/1000 | Loss: 0.00059263
Iteration 4/1000 | Loss: 0.00079769
Iteration 5/1000 | Loss: 0.00012478
Iteration 6/1000 | Loss: 0.00003798
Iteration 7/1000 | Loss: 0.00003555
Iteration 8/1000 | Loss: 0.00016087
Iteration 9/1000 | Loss: 0.00012815
Iteration 10/1000 | Loss: 0.00008499
Iteration 11/1000 | Loss: 0.00003522
Iteration 12/1000 | Loss: 0.00003094
Iteration 13/1000 | Loss: 0.00012532
Iteration 14/1000 | Loss: 0.00002781
Iteration 15/1000 | Loss: 0.00002681
Iteration 16/1000 | Loss: 0.00031344
Iteration 17/1000 | Loss: 0.00025640
Iteration 18/1000 | Loss: 0.00002559
Iteration 19/1000 | Loss: 0.00002507
Iteration 20/1000 | Loss: 0.00002449
Iteration 21/1000 | Loss: 0.00028756
Iteration 22/1000 | Loss: 0.00027638
Iteration 23/1000 | Loss: 0.00028598
Iteration 24/1000 | Loss: 0.00033969
Iteration 25/1000 | Loss: 0.00028237
Iteration 26/1000 | Loss: 0.00006690
Iteration 27/1000 | Loss: 0.00026091
Iteration 28/1000 | Loss: 0.00002444
Iteration 29/1000 | Loss: 0.00033355
Iteration 30/1000 | Loss: 0.00019753
Iteration 31/1000 | Loss: 0.00002453
Iteration 32/1000 | Loss: 0.00034412
Iteration 33/1000 | Loss: 0.00022978
Iteration 34/1000 | Loss: 0.00002629
Iteration 35/1000 | Loss: 0.00002446
Iteration 36/1000 | Loss: 0.00002403
Iteration 37/1000 | Loss: 0.00033064
Iteration 38/1000 | Loss: 0.00002799
Iteration 39/1000 | Loss: 0.00002610
Iteration 40/1000 | Loss: 0.00002482
Iteration 41/1000 | Loss: 0.00002426
Iteration 42/1000 | Loss: 0.00019676
Iteration 43/1000 | Loss: 0.00002863
Iteration 44/1000 | Loss: 0.00002613
Iteration 45/1000 | Loss: 0.00002529
Iteration 46/1000 | Loss: 0.00002489
Iteration 47/1000 | Loss: 0.00002459
Iteration 48/1000 | Loss: 0.00027121
Iteration 49/1000 | Loss: 0.00014945
Iteration 50/1000 | Loss: 0.00018044
Iteration 51/1000 | Loss: 0.00027726
Iteration 52/1000 | Loss: 0.00010490
Iteration 53/1000 | Loss: 0.00016576
Iteration 54/1000 | Loss: 0.00033981
Iteration 55/1000 | Loss: 0.00011306
Iteration 56/1000 | Loss: 0.00013439
Iteration 57/1000 | Loss: 0.00004722
Iteration 58/1000 | Loss: 0.00016229
Iteration 59/1000 | Loss: 0.00003137
Iteration 60/1000 | Loss: 0.00002868
Iteration 61/1000 | Loss: 0.00002754
Iteration 62/1000 | Loss: 0.00002652
Iteration 63/1000 | Loss: 0.00003097
Iteration 64/1000 | Loss: 0.00002478
Iteration 65/1000 | Loss: 0.00002367
Iteration 66/1000 | Loss: 0.00002312
Iteration 67/1000 | Loss: 0.00002293
Iteration 68/1000 | Loss: 0.00002282
Iteration 69/1000 | Loss: 0.00002276
Iteration 70/1000 | Loss: 0.00002269
Iteration 71/1000 | Loss: 0.00002266
Iteration 72/1000 | Loss: 0.00002265
Iteration 73/1000 | Loss: 0.00002265
Iteration 74/1000 | Loss: 0.00002265
Iteration 75/1000 | Loss: 0.00002264
Iteration 76/1000 | Loss: 0.00002264
Iteration 77/1000 | Loss: 0.00002263
Iteration 78/1000 | Loss: 0.00002263
Iteration 79/1000 | Loss: 0.00002263
Iteration 80/1000 | Loss: 0.00002261
Iteration 81/1000 | Loss: 0.00002261
Iteration 82/1000 | Loss: 0.00002260
Iteration 83/1000 | Loss: 0.00002258
Iteration 84/1000 | Loss: 0.00002258
Iteration 85/1000 | Loss: 0.00002257
Iteration 86/1000 | Loss: 0.00002257
Iteration 87/1000 | Loss: 0.00002256
Iteration 88/1000 | Loss: 0.00002255
Iteration 89/1000 | Loss: 0.00002254
Iteration 90/1000 | Loss: 0.00002253
Iteration 91/1000 | Loss: 0.00002253
Iteration 92/1000 | Loss: 0.00002252
Iteration 93/1000 | Loss: 0.00002252
Iteration 94/1000 | Loss: 0.00002252
Iteration 95/1000 | Loss: 0.00002251
Iteration 96/1000 | Loss: 0.00002251
Iteration 97/1000 | Loss: 0.00002250
Iteration 98/1000 | Loss: 0.00002250
Iteration 99/1000 | Loss: 0.00002250
Iteration 100/1000 | Loss: 0.00002250
Iteration 101/1000 | Loss: 0.00002250
Iteration 102/1000 | Loss: 0.00002250
Iteration 103/1000 | Loss: 0.00002250
Iteration 104/1000 | Loss: 0.00002249
Iteration 105/1000 | Loss: 0.00002249
Iteration 106/1000 | Loss: 0.00002249
Iteration 107/1000 | Loss: 0.00002249
Iteration 108/1000 | Loss: 0.00002249
Iteration 109/1000 | Loss: 0.00002249
Iteration 110/1000 | Loss: 0.00002249
Iteration 111/1000 | Loss: 0.00002249
Iteration 112/1000 | Loss: 0.00002249
Iteration 113/1000 | Loss: 0.00002248
Iteration 114/1000 | Loss: 0.00002248
Iteration 115/1000 | Loss: 0.00002244
Iteration 116/1000 | Loss: 0.00002244
Iteration 117/1000 | Loss: 0.00002243
Iteration 118/1000 | Loss: 0.00002243
Iteration 119/1000 | Loss: 0.00002243
Iteration 120/1000 | Loss: 0.00002242
Iteration 121/1000 | Loss: 0.00002242
Iteration 122/1000 | Loss: 0.00002242
Iteration 123/1000 | Loss: 0.00002242
Iteration 124/1000 | Loss: 0.00002242
Iteration 125/1000 | Loss: 0.00002242
Iteration 126/1000 | Loss: 0.00002242
Iteration 127/1000 | Loss: 0.00002242
Iteration 128/1000 | Loss: 0.00002241
Iteration 129/1000 | Loss: 0.00002241
Iteration 130/1000 | Loss: 0.00002241
Iteration 131/1000 | Loss: 0.00002241
Iteration 132/1000 | Loss: 0.00002241
Iteration 133/1000 | Loss: 0.00002241
Iteration 134/1000 | Loss: 0.00002240
Iteration 135/1000 | Loss: 0.00002240
Iteration 136/1000 | Loss: 0.00002240
Iteration 137/1000 | Loss: 0.00002240
Iteration 138/1000 | Loss: 0.00002240
Iteration 139/1000 | Loss: 0.00002240
Iteration 140/1000 | Loss: 0.00002240
Iteration 141/1000 | Loss: 0.00002240
Iteration 142/1000 | Loss: 0.00002240
Iteration 143/1000 | Loss: 0.00002240
Iteration 144/1000 | Loss: 0.00002239
Iteration 145/1000 | Loss: 0.00002239
Iteration 146/1000 | Loss: 0.00002239
Iteration 147/1000 | Loss: 0.00002239
Iteration 148/1000 | Loss: 0.00002239
Iteration 149/1000 | Loss: 0.00002239
Iteration 150/1000 | Loss: 0.00002239
Iteration 151/1000 | Loss: 0.00002239
Iteration 152/1000 | Loss: 0.00002238
Iteration 153/1000 | Loss: 0.00002238
Iteration 154/1000 | Loss: 0.00002238
Iteration 155/1000 | Loss: 0.00002238
Iteration 156/1000 | Loss: 0.00002238
Iteration 157/1000 | Loss: 0.00002237
Iteration 158/1000 | Loss: 0.00002237
Iteration 159/1000 | Loss: 0.00002237
Iteration 160/1000 | Loss: 0.00002237
Iteration 161/1000 | Loss: 0.00002237
Iteration 162/1000 | Loss: 0.00002237
Iteration 163/1000 | Loss: 0.00002237
Iteration 164/1000 | Loss: 0.00002237
Iteration 165/1000 | Loss: 0.00002237
Iteration 166/1000 | Loss: 0.00002237
Iteration 167/1000 | Loss: 0.00002237
Iteration 168/1000 | Loss: 0.00002237
Iteration 169/1000 | Loss: 0.00002236
Iteration 170/1000 | Loss: 0.00002236
Iteration 171/1000 | Loss: 0.00002236
Iteration 172/1000 | Loss: 0.00002236
Iteration 173/1000 | Loss: 0.00002236
Iteration 174/1000 | Loss: 0.00002236
Iteration 175/1000 | Loss: 0.00002236
Iteration 176/1000 | Loss: 0.00002236
Iteration 177/1000 | Loss: 0.00002236
Iteration 178/1000 | Loss: 0.00002236
Iteration 179/1000 | Loss: 0.00002236
Iteration 180/1000 | Loss: 0.00002236
Iteration 181/1000 | Loss: 0.00002236
Iteration 182/1000 | Loss: 0.00002236
Iteration 183/1000 | Loss: 0.00002236
Iteration 184/1000 | Loss: 0.00002236
Iteration 185/1000 | Loss: 0.00002236
Iteration 186/1000 | Loss: 0.00002236
Iteration 187/1000 | Loss: 0.00002236
Iteration 188/1000 | Loss: 0.00002236
Iteration 189/1000 | Loss: 0.00002236
Iteration 190/1000 | Loss: 0.00002236
Iteration 191/1000 | Loss: 0.00002236
Iteration 192/1000 | Loss: 0.00002236
Iteration 193/1000 | Loss: 0.00002236
Iteration 194/1000 | Loss: 0.00002236
Iteration 195/1000 | Loss: 0.00002236
Iteration 196/1000 | Loss: 0.00002236
Iteration 197/1000 | Loss: 0.00002236
Iteration 198/1000 | Loss: 0.00002236
Iteration 199/1000 | Loss: 0.00002236
Iteration 200/1000 | Loss: 0.00002236
Iteration 201/1000 | Loss: 0.00002236
Iteration 202/1000 | Loss: 0.00002236
Iteration 203/1000 | Loss: 0.00002236
Iteration 204/1000 | Loss: 0.00002236
Iteration 205/1000 | Loss: 0.00002236
Iteration 206/1000 | Loss: 0.00002236
Iteration 207/1000 | Loss: 0.00002236
Iteration 208/1000 | Loss: 0.00002236
Iteration 209/1000 | Loss: 0.00002236
Iteration 210/1000 | Loss: 0.00002236
Iteration 211/1000 | Loss: 0.00002236
Iteration 212/1000 | Loss: 0.00002236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.236489854112733e-05, 2.236489854112733e-05, 2.236489854112733e-05, 2.236489854112733e-05, 2.236489854112733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.236489854112733e-05

Optimization complete. Final v2v error: 4.09130334854126 mm

Highest mean error: 4.836750030517578 mm for frame 7

Lowest mean error: 3.8573758602142334 mm for frame 10

Saving results

Total time: 149.83197689056396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00744228
Iteration 2/25 | Loss: 0.00117144
Iteration 3/25 | Loss: 0.00110097
Iteration 4/25 | Loss: 0.00109186
Iteration 5/25 | Loss: 0.00108836
Iteration 6/25 | Loss: 0.00108818
Iteration 7/25 | Loss: 0.00108818
Iteration 8/25 | Loss: 0.00108818
Iteration 9/25 | Loss: 0.00108818
Iteration 10/25 | Loss: 0.00108818
Iteration 11/25 | Loss: 0.00108818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010881838388741016, 0.0010881838388741016, 0.0010881838388741016, 0.0010881838388741016, 0.0010881838388741016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010881838388741016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25348616
Iteration 2/25 | Loss: 0.00074561
Iteration 3/25 | Loss: 0.00074561
Iteration 4/25 | Loss: 0.00074561
Iteration 5/25 | Loss: 0.00074561
Iteration 6/25 | Loss: 0.00074561
Iteration 7/25 | Loss: 0.00074561
Iteration 8/25 | Loss: 0.00074561
Iteration 9/25 | Loss: 0.00074561
Iteration 10/25 | Loss: 0.00074561
Iteration 11/25 | Loss: 0.00074561
Iteration 12/25 | Loss: 0.00074561
Iteration 13/25 | Loss: 0.00074561
Iteration 14/25 | Loss: 0.00074561
Iteration 15/25 | Loss: 0.00074561
Iteration 16/25 | Loss: 0.00074561
Iteration 17/25 | Loss: 0.00074561
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000745610857848078, 0.000745610857848078, 0.000745610857848078, 0.000745610857848078, 0.000745610857848078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000745610857848078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074561
Iteration 2/1000 | Loss: 0.00003006
Iteration 3/1000 | Loss: 0.00001656
Iteration 4/1000 | Loss: 0.00001455
Iteration 5/1000 | Loss: 0.00001374
Iteration 6/1000 | Loss: 0.00001317
Iteration 7/1000 | Loss: 0.00001272
Iteration 8/1000 | Loss: 0.00001244
Iteration 9/1000 | Loss: 0.00001214
Iteration 10/1000 | Loss: 0.00001192
Iteration 11/1000 | Loss: 0.00001191
Iteration 12/1000 | Loss: 0.00001189
Iteration 13/1000 | Loss: 0.00001188
Iteration 14/1000 | Loss: 0.00001180
Iteration 15/1000 | Loss: 0.00001176
Iteration 16/1000 | Loss: 0.00001166
Iteration 17/1000 | Loss: 0.00001153
Iteration 18/1000 | Loss: 0.00001150
Iteration 19/1000 | Loss: 0.00001149
Iteration 20/1000 | Loss: 0.00001146
Iteration 21/1000 | Loss: 0.00001145
Iteration 22/1000 | Loss: 0.00001145
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001140
Iteration 25/1000 | Loss: 0.00001139
Iteration 26/1000 | Loss: 0.00001134
Iteration 27/1000 | Loss: 0.00001133
Iteration 28/1000 | Loss: 0.00001130
Iteration 29/1000 | Loss: 0.00001129
Iteration 30/1000 | Loss: 0.00001126
Iteration 31/1000 | Loss: 0.00001126
Iteration 32/1000 | Loss: 0.00001126
Iteration 33/1000 | Loss: 0.00001126
Iteration 34/1000 | Loss: 0.00001126
Iteration 35/1000 | Loss: 0.00001126
Iteration 36/1000 | Loss: 0.00001126
Iteration 37/1000 | Loss: 0.00001126
Iteration 38/1000 | Loss: 0.00001126
Iteration 39/1000 | Loss: 0.00001125
Iteration 40/1000 | Loss: 0.00001124
Iteration 41/1000 | Loss: 0.00001123
Iteration 42/1000 | Loss: 0.00001122
Iteration 43/1000 | Loss: 0.00001122
Iteration 44/1000 | Loss: 0.00001122
Iteration 45/1000 | Loss: 0.00001122
Iteration 46/1000 | Loss: 0.00001122
Iteration 47/1000 | Loss: 0.00001122
Iteration 48/1000 | Loss: 0.00001121
Iteration 49/1000 | Loss: 0.00001121
Iteration 50/1000 | Loss: 0.00001121
Iteration 51/1000 | Loss: 0.00001121
Iteration 52/1000 | Loss: 0.00001120
Iteration 53/1000 | Loss: 0.00001120
Iteration 54/1000 | Loss: 0.00001120
Iteration 55/1000 | Loss: 0.00001119
Iteration 56/1000 | Loss: 0.00001119
Iteration 57/1000 | Loss: 0.00001119
Iteration 58/1000 | Loss: 0.00001118
Iteration 59/1000 | Loss: 0.00001118
Iteration 60/1000 | Loss: 0.00001118
Iteration 61/1000 | Loss: 0.00001118
Iteration 62/1000 | Loss: 0.00001118
Iteration 63/1000 | Loss: 0.00001118
Iteration 64/1000 | Loss: 0.00001117
Iteration 65/1000 | Loss: 0.00001117
Iteration 66/1000 | Loss: 0.00001117
Iteration 67/1000 | Loss: 0.00001117
Iteration 68/1000 | Loss: 0.00001117
Iteration 69/1000 | Loss: 0.00001117
Iteration 70/1000 | Loss: 0.00001117
Iteration 71/1000 | Loss: 0.00001117
Iteration 72/1000 | Loss: 0.00001117
Iteration 73/1000 | Loss: 0.00001117
Iteration 74/1000 | Loss: 0.00001116
Iteration 75/1000 | Loss: 0.00001116
Iteration 76/1000 | Loss: 0.00001116
Iteration 77/1000 | Loss: 0.00001115
Iteration 78/1000 | Loss: 0.00001115
Iteration 79/1000 | Loss: 0.00001115
Iteration 80/1000 | Loss: 0.00001115
Iteration 81/1000 | Loss: 0.00001115
Iteration 82/1000 | Loss: 0.00001114
Iteration 83/1000 | Loss: 0.00001114
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001112
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001112
Iteration 89/1000 | Loss: 0.00001111
Iteration 90/1000 | Loss: 0.00001111
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001110
Iteration 93/1000 | Loss: 0.00001110
Iteration 94/1000 | Loss: 0.00001110
Iteration 95/1000 | Loss: 0.00001110
Iteration 96/1000 | Loss: 0.00001109
Iteration 97/1000 | Loss: 0.00001109
Iteration 98/1000 | Loss: 0.00001109
Iteration 99/1000 | Loss: 0.00001109
Iteration 100/1000 | Loss: 0.00001109
Iteration 101/1000 | Loss: 0.00001109
Iteration 102/1000 | Loss: 0.00001109
Iteration 103/1000 | Loss: 0.00001109
Iteration 104/1000 | Loss: 0.00001109
Iteration 105/1000 | Loss: 0.00001109
Iteration 106/1000 | Loss: 0.00001109
Iteration 107/1000 | Loss: 0.00001109
Iteration 108/1000 | Loss: 0.00001109
Iteration 109/1000 | Loss: 0.00001109
Iteration 110/1000 | Loss: 0.00001109
Iteration 111/1000 | Loss: 0.00001109
Iteration 112/1000 | Loss: 0.00001109
Iteration 113/1000 | Loss: 0.00001109
Iteration 114/1000 | Loss: 0.00001109
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001109
Iteration 118/1000 | Loss: 0.00001109
Iteration 119/1000 | Loss: 0.00001109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.1092429303971585e-05, 1.1092429303971585e-05, 1.1092429303971585e-05, 1.1092429303971585e-05, 1.1092429303971585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1092429303971585e-05

Optimization complete. Final v2v error: 2.8245229721069336 mm

Highest mean error: 3.4922077655792236 mm for frame 13

Lowest mean error: 2.6471285820007324 mm for frame 49

Saving results

Total time: 38.95181369781494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936264
Iteration 2/25 | Loss: 0.00193462
Iteration 3/25 | Loss: 0.00154328
Iteration 4/25 | Loss: 0.00146558
Iteration 5/25 | Loss: 0.00143425
Iteration 6/25 | Loss: 0.00142385
Iteration 7/25 | Loss: 0.00140793
Iteration 8/25 | Loss: 0.00140960
Iteration 9/25 | Loss: 0.00140617
Iteration 10/25 | Loss: 0.00139395
Iteration 11/25 | Loss: 0.00138568
Iteration 12/25 | Loss: 0.00138122
Iteration 13/25 | Loss: 0.00137947
Iteration 14/25 | Loss: 0.00137475
Iteration 15/25 | Loss: 0.00137830
Iteration 16/25 | Loss: 0.00137864
Iteration 17/25 | Loss: 0.00137649
Iteration 18/25 | Loss: 0.00137664
Iteration 19/25 | Loss: 0.00137928
Iteration 20/25 | Loss: 0.00137740
Iteration 21/25 | Loss: 0.00138072
Iteration 22/25 | Loss: 0.00137766
Iteration 23/25 | Loss: 0.00137997
Iteration 24/25 | Loss: 0.00137035
Iteration 25/25 | Loss: 0.00136589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.65289927
Iteration 2/25 | Loss: 0.00207085
Iteration 3/25 | Loss: 0.00207073
Iteration 4/25 | Loss: 0.00207072
Iteration 5/25 | Loss: 0.00207072
Iteration 6/25 | Loss: 0.00207072
Iteration 7/25 | Loss: 0.00207072
Iteration 8/25 | Loss: 0.00207072
Iteration 9/25 | Loss: 0.00207072
Iteration 10/25 | Loss: 0.00207072
Iteration 11/25 | Loss: 0.00207072
Iteration 12/25 | Loss: 0.00207072
Iteration 13/25 | Loss: 0.00207072
Iteration 14/25 | Loss: 0.00207072
Iteration 15/25 | Loss: 0.00207072
Iteration 16/25 | Loss: 0.00207072
Iteration 17/25 | Loss: 0.00207072
Iteration 18/25 | Loss: 0.00207072
Iteration 19/25 | Loss: 0.00207072
Iteration 20/25 | Loss: 0.00207072
Iteration 21/25 | Loss: 0.00207072
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002070721937343478, 0.002070721937343478, 0.002070721937343478, 0.002070721937343478, 0.002070721937343478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002070721937343478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207072
Iteration 2/1000 | Loss: 0.01684785
Iteration 3/1000 | Loss: 0.00114343
Iteration 4/1000 | Loss: 0.00120470
Iteration 5/1000 | Loss: 0.00055462
Iteration 6/1000 | Loss: 0.00032832
Iteration 7/1000 | Loss: 0.00012849
Iteration 8/1000 | Loss: 0.00167051
Iteration 9/1000 | Loss: 0.00038485
Iteration 10/1000 | Loss: 0.00019438
Iteration 11/1000 | Loss: 0.00020673
Iteration 12/1000 | Loss: 0.00022043
Iteration 13/1000 | Loss: 0.00179081
Iteration 14/1000 | Loss: 0.00084808
Iteration 15/1000 | Loss: 0.00030936
Iteration 16/1000 | Loss: 0.00027468
Iteration 17/1000 | Loss: 0.00030065
Iteration 18/1000 | Loss: 0.00024668
Iteration 19/1000 | Loss: 0.00018715
Iteration 20/1000 | Loss: 0.00029915
Iteration 21/1000 | Loss: 0.00036180
Iteration 22/1000 | Loss: 0.00028106
Iteration 23/1000 | Loss: 0.00037146
Iteration 24/1000 | Loss: 0.00189364
Iteration 25/1000 | Loss: 0.00038574
Iteration 26/1000 | Loss: 0.00022083
Iteration 27/1000 | Loss: 0.00041440
Iteration 28/1000 | Loss: 0.00040937
Iteration 29/1000 | Loss: 0.00039404
Iteration 30/1000 | Loss: 0.00030150
Iteration 31/1000 | Loss: 0.00047963
Iteration 32/1000 | Loss: 0.00029477
Iteration 33/1000 | Loss: 0.00060167
Iteration 34/1000 | Loss: 0.00025865
Iteration 35/1000 | Loss: 0.00066391
Iteration 36/1000 | Loss: 0.00027566
Iteration 37/1000 | Loss: 0.00063613
Iteration 38/1000 | Loss: 0.00033685
Iteration 39/1000 | Loss: 0.00024566
Iteration 40/1000 | Loss: 0.00016327
Iteration 41/1000 | Loss: 0.00024021
Iteration 42/1000 | Loss: 0.00024211
Iteration 43/1000 | Loss: 0.00023437
Iteration 44/1000 | Loss: 0.00022616
Iteration 45/1000 | Loss: 0.00024673
Iteration 46/1000 | Loss: 0.00020014
Iteration 47/1000 | Loss: 0.00028066
Iteration 48/1000 | Loss: 0.00015199
Iteration 49/1000 | Loss: 0.00007092
Iteration 50/1000 | Loss: 0.00025096
Iteration 51/1000 | Loss: 0.00021506
Iteration 52/1000 | Loss: 0.00025549
Iteration 53/1000 | Loss: 0.00007191
Iteration 54/1000 | Loss: 0.00007362
Iteration 55/1000 | Loss: 0.00006141
Iteration 56/1000 | Loss: 0.00005947
Iteration 57/1000 | Loss: 0.00058055
Iteration 58/1000 | Loss: 0.00111733
Iteration 59/1000 | Loss: 0.00070768
Iteration 60/1000 | Loss: 0.00064057
Iteration 61/1000 | Loss: 0.00080170
Iteration 62/1000 | Loss: 0.00037865
Iteration 63/1000 | Loss: 0.00043774
Iteration 64/1000 | Loss: 0.00011014
Iteration 65/1000 | Loss: 0.00013453
Iteration 66/1000 | Loss: 0.00032703
Iteration 67/1000 | Loss: 0.00042495
Iteration 68/1000 | Loss: 0.00016007
Iteration 69/1000 | Loss: 0.00011569
Iteration 70/1000 | Loss: 0.00009722
Iteration 71/1000 | Loss: 0.00019926
Iteration 72/1000 | Loss: 0.00032132
Iteration 73/1000 | Loss: 0.00015509
Iteration 74/1000 | Loss: 0.00041327
Iteration 75/1000 | Loss: 0.00015732
Iteration 76/1000 | Loss: 0.00008072
Iteration 77/1000 | Loss: 0.00013756
Iteration 78/1000 | Loss: 0.00010635
Iteration 79/1000 | Loss: 0.00018193
Iteration 80/1000 | Loss: 0.00020766
Iteration 81/1000 | Loss: 0.00017261
Iteration 82/1000 | Loss: 0.00005309
Iteration 83/1000 | Loss: 0.00005043
Iteration 84/1000 | Loss: 0.00004838
Iteration 85/1000 | Loss: 0.00004551
Iteration 86/1000 | Loss: 0.00004434
Iteration 87/1000 | Loss: 0.00004351
Iteration 88/1000 | Loss: 0.00004283
Iteration 89/1000 | Loss: 0.00004222
Iteration 90/1000 | Loss: 0.00004190
Iteration 91/1000 | Loss: 0.00004163
Iteration 92/1000 | Loss: 0.00004136
Iteration 93/1000 | Loss: 0.00004124
Iteration 94/1000 | Loss: 0.00004121
Iteration 95/1000 | Loss: 0.00004120
Iteration 96/1000 | Loss: 0.00004106
Iteration 97/1000 | Loss: 0.00004091
Iteration 98/1000 | Loss: 0.00004082
Iteration 99/1000 | Loss: 0.00004072
Iteration 100/1000 | Loss: 0.00004066
Iteration 101/1000 | Loss: 0.00004059
Iteration 102/1000 | Loss: 0.00004057
Iteration 103/1000 | Loss: 0.00004056
Iteration 104/1000 | Loss: 0.00004056
Iteration 105/1000 | Loss: 0.00004056
Iteration 106/1000 | Loss: 0.00004055
Iteration 107/1000 | Loss: 0.00004055
Iteration 108/1000 | Loss: 0.00004054
Iteration 109/1000 | Loss: 0.00004054
Iteration 110/1000 | Loss: 0.00004054
Iteration 111/1000 | Loss: 0.00004053
Iteration 112/1000 | Loss: 0.00004053
Iteration 113/1000 | Loss: 0.00004051
Iteration 114/1000 | Loss: 0.00004051
Iteration 115/1000 | Loss: 0.00004050
Iteration 116/1000 | Loss: 0.00004049
Iteration 117/1000 | Loss: 0.00004049
Iteration 118/1000 | Loss: 0.00004048
Iteration 119/1000 | Loss: 0.00004048
Iteration 120/1000 | Loss: 0.00004048
Iteration 121/1000 | Loss: 0.00004048
Iteration 122/1000 | Loss: 0.00004048
Iteration 123/1000 | Loss: 0.00004048
Iteration 124/1000 | Loss: 0.00004048
Iteration 125/1000 | Loss: 0.00004048
Iteration 126/1000 | Loss: 0.00004048
Iteration 127/1000 | Loss: 0.00004048
Iteration 128/1000 | Loss: 0.00004048
Iteration 129/1000 | Loss: 0.00004048
Iteration 130/1000 | Loss: 0.00004046
Iteration 131/1000 | Loss: 0.00004046
Iteration 132/1000 | Loss: 0.00004046
Iteration 133/1000 | Loss: 0.00004045
Iteration 134/1000 | Loss: 0.00004044
Iteration 135/1000 | Loss: 0.00004044
Iteration 136/1000 | Loss: 0.00004044
Iteration 137/1000 | Loss: 0.00004043
Iteration 138/1000 | Loss: 0.00004043
Iteration 139/1000 | Loss: 0.00004043
Iteration 140/1000 | Loss: 0.00004042
Iteration 141/1000 | Loss: 0.00004042
Iteration 142/1000 | Loss: 0.00004042
Iteration 143/1000 | Loss: 0.00004041
Iteration 144/1000 | Loss: 0.00004041
Iteration 145/1000 | Loss: 0.00004041
Iteration 146/1000 | Loss: 0.00004041
Iteration 147/1000 | Loss: 0.00004041
Iteration 148/1000 | Loss: 0.00004041
Iteration 149/1000 | Loss: 0.00004040
Iteration 150/1000 | Loss: 0.00004040
Iteration 151/1000 | Loss: 0.00004040
Iteration 152/1000 | Loss: 0.00004040
Iteration 153/1000 | Loss: 0.00004039
Iteration 154/1000 | Loss: 0.00004039
Iteration 155/1000 | Loss: 0.00004039
Iteration 156/1000 | Loss: 0.00004039
Iteration 157/1000 | Loss: 0.00004039
Iteration 158/1000 | Loss: 0.00004039
Iteration 159/1000 | Loss: 0.00004039
Iteration 160/1000 | Loss: 0.00004039
Iteration 161/1000 | Loss: 0.00004039
Iteration 162/1000 | Loss: 0.00004039
Iteration 163/1000 | Loss: 0.00004038
Iteration 164/1000 | Loss: 0.00004038
Iteration 165/1000 | Loss: 0.00004038
Iteration 166/1000 | Loss: 0.00004038
Iteration 167/1000 | Loss: 0.00004038
Iteration 168/1000 | Loss: 0.00004038
Iteration 169/1000 | Loss: 0.00004038
Iteration 170/1000 | Loss: 0.00004038
Iteration 171/1000 | Loss: 0.00004038
Iteration 172/1000 | Loss: 0.00004037
Iteration 173/1000 | Loss: 0.00004037
Iteration 174/1000 | Loss: 0.00004037
Iteration 175/1000 | Loss: 0.00004037
Iteration 176/1000 | Loss: 0.00004037
Iteration 177/1000 | Loss: 0.00004036
Iteration 178/1000 | Loss: 0.00004036
Iteration 179/1000 | Loss: 0.00004036
Iteration 180/1000 | Loss: 0.00004036
Iteration 181/1000 | Loss: 0.00004035
Iteration 182/1000 | Loss: 0.00004035
Iteration 183/1000 | Loss: 0.00004035
Iteration 184/1000 | Loss: 0.00004035
Iteration 185/1000 | Loss: 0.00004034
Iteration 186/1000 | Loss: 0.00004034
Iteration 187/1000 | Loss: 0.00004034
Iteration 188/1000 | Loss: 0.00004034
Iteration 189/1000 | Loss: 0.00004034
Iteration 190/1000 | Loss: 0.00004033
Iteration 191/1000 | Loss: 0.00004033
Iteration 192/1000 | Loss: 0.00004033
Iteration 193/1000 | Loss: 0.00004033
Iteration 194/1000 | Loss: 0.00004033
Iteration 195/1000 | Loss: 0.00004033
Iteration 196/1000 | Loss: 0.00004033
Iteration 197/1000 | Loss: 0.00004033
Iteration 198/1000 | Loss: 0.00004032
Iteration 199/1000 | Loss: 0.00004032
Iteration 200/1000 | Loss: 0.00004032
Iteration 201/1000 | Loss: 0.00004032
Iteration 202/1000 | Loss: 0.00004032
Iteration 203/1000 | Loss: 0.00004032
Iteration 204/1000 | Loss: 0.00004032
Iteration 205/1000 | Loss: 0.00004031
Iteration 206/1000 | Loss: 0.00004031
Iteration 207/1000 | Loss: 0.00004031
Iteration 208/1000 | Loss: 0.00004031
Iteration 209/1000 | Loss: 0.00004031
Iteration 210/1000 | Loss: 0.00004031
Iteration 211/1000 | Loss: 0.00004031
Iteration 212/1000 | Loss: 0.00004031
Iteration 213/1000 | Loss: 0.00004031
Iteration 214/1000 | Loss: 0.00004031
Iteration 215/1000 | Loss: 0.00004031
Iteration 216/1000 | Loss: 0.00004031
Iteration 217/1000 | Loss: 0.00004031
Iteration 218/1000 | Loss: 0.00004031
Iteration 219/1000 | Loss: 0.00004031
Iteration 220/1000 | Loss: 0.00004030
Iteration 221/1000 | Loss: 0.00004030
Iteration 222/1000 | Loss: 0.00004030
Iteration 223/1000 | Loss: 0.00004030
Iteration 224/1000 | Loss: 0.00004030
Iteration 225/1000 | Loss: 0.00004030
Iteration 226/1000 | Loss: 0.00004030
Iteration 227/1000 | Loss: 0.00004029
Iteration 228/1000 | Loss: 0.00004029
Iteration 229/1000 | Loss: 0.00004029
Iteration 230/1000 | Loss: 0.00004029
Iteration 231/1000 | Loss: 0.00004029
Iteration 232/1000 | Loss: 0.00004029
Iteration 233/1000 | Loss: 0.00004029
Iteration 234/1000 | Loss: 0.00004029
Iteration 235/1000 | Loss: 0.00004029
Iteration 236/1000 | Loss: 0.00004029
Iteration 237/1000 | Loss: 0.00004029
Iteration 238/1000 | Loss: 0.00004029
Iteration 239/1000 | Loss: 0.00004029
Iteration 240/1000 | Loss: 0.00004029
Iteration 241/1000 | Loss: 0.00004028
Iteration 242/1000 | Loss: 0.00004028
Iteration 243/1000 | Loss: 0.00004028
Iteration 244/1000 | Loss: 0.00004028
Iteration 245/1000 | Loss: 0.00004028
Iteration 246/1000 | Loss: 0.00004028
Iteration 247/1000 | Loss: 0.00004028
Iteration 248/1000 | Loss: 0.00004028
Iteration 249/1000 | Loss: 0.00004028
Iteration 250/1000 | Loss: 0.00004028
Iteration 251/1000 | Loss: 0.00004028
Iteration 252/1000 | Loss: 0.00004027
Iteration 253/1000 | Loss: 0.00004027
Iteration 254/1000 | Loss: 0.00004027
Iteration 255/1000 | Loss: 0.00004027
Iteration 256/1000 | Loss: 0.00004026
Iteration 257/1000 | Loss: 0.00004026
Iteration 258/1000 | Loss: 0.00004026
Iteration 259/1000 | Loss: 0.00004026
Iteration 260/1000 | Loss: 0.00004026
Iteration 261/1000 | Loss: 0.00004026
Iteration 262/1000 | Loss: 0.00004026
Iteration 263/1000 | Loss: 0.00004026
Iteration 264/1000 | Loss: 0.00004026
Iteration 265/1000 | Loss: 0.00004025
Iteration 266/1000 | Loss: 0.00004025
Iteration 267/1000 | Loss: 0.00004025
Iteration 268/1000 | Loss: 0.00004025
Iteration 269/1000 | Loss: 0.00004025
Iteration 270/1000 | Loss: 0.00004025
Iteration 271/1000 | Loss: 0.00004025
Iteration 272/1000 | Loss: 0.00004025
Iteration 273/1000 | Loss: 0.00004024
Iteration 274/1000 | Loss: 0.00004024
Iteration 275/1000 | Loss: 0.00004024
Iteration 276/1000 | Loss: 0.00004024
Iteration 277/1000 | Loss: 0.00004024
Iteration 278/1000 | Loss: 0.00004024
Iteration 279/1000 | Loss: 0.00004024
Iteration 280/1000 | Loss: 0.00004024
Iteration 281/1000 | Loss: 0.00004024
Iteration 282/1000 | Loss: 0.00004024
Iteration 283/1000 | Loss: 0.00004024
Iteration 284/1000 | Loss: 0.00004024
Iteration 285/1000 | Loss: 0.00004024
Iteration 286/1000 | Loss: 0.00004024
Iteration 287/1000 | Loss: 0.00004023
Iteration 288/1000 | Loss: 0.00004023
Iteration 289/1000 | Loss: 0.00004023
Iteration 290/1000 | Loss: 0.00004023
Iteration 291/1000 | Loss: 0.00004023
Iteration 292/1000 | Loss: 0.00004023
Iteration 293/1000 | Loss: 0.00004023
Iteration 294/1000 | Loss: 0.00004023
Iteration 295/1000 | Loss: 0.00004023
Iteration 296/1000 | Loss: 0.00004023
Iteration 297/1000 | Loss: 0.00004023
Iteration 298/1000 | Loss: 0.00004022
Iteration 299/1000 | Loss: 0.00004022
Iteration 300/1000 | Loss: 0.00004022
Iteration 301/1000 | Loss: 0.00004022
Iteration 302/1000 | Loss: 0.00004022
Iteration 303/1000 | Loss: 0.00004022
Iteration 304/1000 | Loss: 0.00004022
Iteration 305/1000 | Loss: 0.00004022
Iteration 306/1000 | Loss: 0.00004022
Iteration 307/1000 | Loss: 0.00004022
Iteration 308/1000 | Loss: 0.00004022
Iteration 309/1000 | Loss: 0.00004022
Iteration 310/1000 | Loss: 0.00004022
Iteration 311/1000 | Loss: 0.00004021
Iteration 312/1000 | Loss: 0.00004021
Iteration 313/1000 | Loss: 0.00004021
Iteration 314/1000 | Loss: 0.00004021
Iteration 315/1000 | Loss: 0.00004021
Iteration 316/1000 | Loss: 0.00004020
Iteration 317/1000 | Loss: 0.00004020
Iteration 318/1000 | Loss: 0.00004020
Iteration 319/1000 | Loss: 0.00004020
Iteration 320/1000 | Loss: 0.00004020
Iteration 321/1000 | Loss: 0.00004020
Iteration 322/1000 | Loss: 0.00004020
Iteration 323/1000 | Loss: 0.00004020
Iteration 324/1000 | Loss: 0.00004020
Iteration 325/1000 | Loss: 0.00004020
Iteration 326/1000 | Loss: 0.00004020
Iteration 327/1000 | Loss: 0.00004020
Iteration 328/1000 | Loss: 0.00004020
Iteration 329/1000 | Loss: 0.00004019
Iteration 330/1000 | Loss: 0.00004019
Iteration 331/1000 | Loss: 0.00004019
Iteration 332/1000 | Loss: 0.00004019
Iteration 333/1000 | Loss: 0.00004019
Iteration 334/1000 | Loss: 0.00004019
Iteration 335/1000 | Loss: 0.00004019
Iteration 336/1000 | Loss: 0.00004019
Iteration 337/1000 | Loss: 0.00004019
Iteration 338/1000 | Loss: 0.00004019
Iteration 339/1000 | Loss: 0.00004019
Iteration 340/1000 | Loss: 0.00004019
Iteration 341/1000 | Loss: 0.00004019
Iteration 342/1000 | Loss: 0.00004019
Iteration 343/1000 | Loss: 0.00004019
Iteration 344/1000 | Loss: 0.00004019
Iteration 345/1000 | Loss: 0.00004019
Iteration 346/1000 | Loss: 0.00004019
Iteration 347/1000 | Loss: 0.00004019
Iteration 348/1000 | Loss: 0.00004019
Iteration 349/1000 | Loss: 0.00004019
Iteration 350/1000 | Loss: 0.00004019
Iteration 351/1000 | Loss: 0.00004019
Iteration 352/1000 | Loss: 0.00004019
Iteration 353/1000 | Loss: 0.00004019
Iteration 354/1000 | Loss: 0.00004019
Iteration 355/1000 | Loss: 0.00004019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 355. Stopping optimization.
Last 5 losses: [4.018646359327249e-05, 4.018646359327249e-05, 4.018646359327249e-05, 4.018646359327249e-05, 4.018646359327249e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.018646359327249e-05

Optimization complete. Final v2v error: 4.603848934173584 mm

Highest mean error: 13.298523902893066 mm for frame 21

Lowest mean error: 3.110182523727417 mm for frame 150

Saving results

Total time: 228.10461282730103
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978941
Iteration 2/25 | Loss: 0.00165126
Iteration 3/25 | Loss: 0.00128123
Iteration 4/25 | Loss: 0.00126148
Iteration 5/25 | Loss: 0.00125421
Iteration 6/25 | Loss: 0.00125230
Iteration 7/25 | Loss: 0.00125230
Iteration 8/25 | Loss: 0.00125230
Iteration 9/25 | Loss: 0.00125230
Iteration 10/25 | Loss: 0.00125230
Iteration 11/25 | Loss: 0.00125230
Iteration 12/25 | Loss: 0.00125230
Iteration 13/25 | Loss: 0.00125230
Iteration 14/25 | Loss: 0.00125230
Iteration 15/25 | Loss: 0.00125230
Iteration 16/25 | Loss: 0.00125230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012523041805252433, 0.0012523041805252433, 0.0012523041805252433, 0.0012523041805252433, 0.0012523041805252433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012523041805252433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87878227
Iteration 2/25 | Loss: 0.00099184
Iteration 3/25 | Loss: 0.00099183
Iteration 4/25 | Loss: 0.00099183
Iteration 5/25 | Loss: 0.00099183
Iteration 6/25 | Loss: 0.00099183
Iteration 7/25 | Loss: 0.00099183
Iteration 8/25 | Loss: 0.00099183
Iteration 9/25 | Loss: 0.00099183
Iteration 10/25 | Loss: 0.00099183
Iteration 11/25 | Loss: 0.00099183
Iteration 12/25 | Loss: 0.00099183
Iteration 13/25 | Loss: 0.00099183
Iteration 14/25 | Loss: 0.00099183
Iteration 15/25 | Loss: 0.00099183
Iteration 16/25 | Loss: 0.00099183
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000991826062090695, 0.000991826062090695, 0.000991826062090695, 0.000991826062090695, 0.000991826062090695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000991826062090695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099183
Iteration 2/1000 | Loss: 0.00005876
Iteration 3/1000 | Loss: 0.00003900
Iteration 4/1000 | Loss: 0.00003068
Iteration 5/1000 | Loss: 0.00002888
Iteration 6/1000 | Loss: 0.00002763
Iteration 7/1000 | Loss: 0.00002701
Iteration 8/1000 | Loss: 0.00002630
Iteration 9/1000 | Loss: 0.00002590
Iteration 10/1000 | Loss: 0.00002549
Iteration 11/1000 | Loss: 0.00002522
Iteration 12/1000 | Loss: 0.00002499
Iteration 13/1000 | Loss: 0.00002479
Iteration 14/1000 | Loss: 0.00002458
Iteration 15/1000 | Loss: 0.00002443
Iteration 16/1000 | Loss: 0.00002427
Iteration 17/1000 | Loss: 0.00002412
Iteration 18/1000 | Loss: 0.00002406
Iteration 19/1000 | Loss: 0.00002405
Iteration 20/1000 | Loss: 0.00002397
Iteration 21/1000 | Loss: 0.00002392
Iteration 22/1000 | Loss: 0.00002391
Iteration 23/1000 | Loss: 0.00002391
Iteration 24/1000 | Loss: 0.00002391
Iteration 25/1000 | Loss: 0.00002390
Iteration 26/1000 | Loss: 0.00002390
Iteration 27/1000 | Loss: 0.00002388
Iteration 28/1000 | Loss: 0.00002388
Iteration 29/1000 | Loss: 0.00002387
Iteration 30/1000 | Loss: 0.00002387
Iteration 31/1000 | Loss: 0.00002387
Iteration 32/1000 | Loss: 0.00002387
Iteration 33/1000 | Loss: 0.00002385
Iteration 34/1000 | Loss: 0.00002385
Iteration 35/1000 | Loss: 0.00002385
Iteration 36/1000 | Loss: 0.00002385
Iteration 37/1000 | Loss: 0.00002385
Iteration 38/1000 | Loss: 0.00002385
Iteration 39/1000 | Loss: 0.00002384
Iteration 40/1000 | Loss: 0.00002384
Iteration 41/1000 | Loss: 0.00002384
Iteration 42/1000 | Loss: 0.00002384
Iteration 43/1000 | Loss: 0.00002381
Iteration 44/1000 | Loss: 0.00002380
Iteration 45/1000 | Loss: 0.00002380
Iteration 46/1000 | Loss: 0.00002378
Iteration 47/1000 | Loss: 0.00002377
Iteration 48/1000 | Loss: 0.00002377
Iteration 49/1000 | Loss: 0.00002374
Iteration 50/1000 | Loss: 0.00002372
Iteration 51/1000 | Loss: 0.00002372
Iteration 52/1000 | Loss: 0.00002372
Iteration 53/1000 | Loss: 0.00002371
Iteration 54/1000 | Loss: 0.00002368
Iteration 55/1000 | Loss: 0.00002368
Iteration 56/1000 | Loss: 0.00002368
Iteration 57/1000 | Loss: 0.00002367
Iteration 58/1000 | Loss: 0.00002366
Iteration 59/1000 | Loss: 0.00002366
Iteration 60/1000 | Loss: 0.00002366
Iteration 61/1000 | Loss: 0.00002365
Iteration 62/1000 | Loss: 0.00002365
Iteration 63/1000 | Loss: 0.00002365
Iteration 64/1000 | Loss: 0.00002364
Iteration 65/1000 | Loss: 0.00002364
Iteration 66/1000 | Loss: 0.00002364
Iteration 67/1000 | Loss: 0.00002364
Iteration 68/1000 | Loss: 0.00002364
Iteration 69/1000 | Loss: 0.00002364
Iteration 70/1000 | Loss: 0.00002364
Iteration 71/1000 | Loss: 0.00002364
Iteration 72/1000 | Loss: 0.00002364
Iteration 73/1000 | Loss: 0.00002364
Iteration 74/1000 | Loss: 0.00002364
Iteration 75/1000 | Loss: 0.00002363
Iteration 76/1000 | Loss: 0.00002363
Iteration 77/1000 | Loss: 0.00002363
Iteration 78/1000 | Loss: 0.00002363
Iteration 79/1000 | Loss: 0.00002363
Iteration 80/1000 | Loss: 0.00002363
Iteration 81/1000 | Loss: 0.00002363
Iteration 82/1000 | Loss: 0.00002363
Iteration 83/1000 | Loss: 0.00002362
Iteration 84/1000 | Loss: 0.00002362
Iteration 85/1000 | Loss: 0.00002362
Iteration 86/1000 | Loss: 0.00002362
Iteration 87/1000 | Loss: 0.00002362
Iteration 88/1000 | Loss: 0.00002362
Iteration 89/1000 | Loss: 0.00002362
Iteration 90/1000 | Loss: 0.00002362
Iteration 91/1000 | Loss: 0.00002362
Iteration 92/1000 | Loss: 0.00002362
Iteration 93/1000 | Loss: 0.00002362
Iteration 94/1000 | Loss: 0.00002362
Iteration 95/1000 | Loss: 0.00002362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.362056875426788e-05, 2.362056875426788e-05, 2.362056875426788e-05, 2.362056875426788e-05, 2.362056875426788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.362056875426788e-05

Optimization complete. Final v2v error: 4.008831024169922 mm

Highest mean error: 4.934919834136963 mm for frame 108

Lowest mean error: 3.1490392684936523 mm for frame 52

Saving results

Total time: 48.59209966659546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454884
Iteration 2/25 | Loss: 0.00133614
Iteration 3/25 | Loss: 0.00115408
Iteration 4/25 | Loss: 0.00113722
Iteration 5/25 | Loss: 0.00113369
Iteration 6/25 | Loss: 0.00113278
Iteration 7/25 | Loss: 0.00113278
Iteration 8/25 | Loss: 0.00113278
Iteration 9/25 | Loss: 0.00113278
Iteration 10/25 | Loss: 0.00113278
Iteration 11/25 | Loss: 0.00113278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011327795218676329, 0.0011327795218676329, 0.0011327795218676329, 0.0011327795218676329, 0.0011327795218676329]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011327795218676329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46420276
Iteration 2/25 | Loss: 0.00067550
Iteration 3/25 | Loss: 0.00067550
Iteration 4/25 | Loss: 0.00067550
Iteration 5/25 | Loss: 0.00067550
Iteration 6/25 | Loss: 0.00067549
Iteration 7/25 | Loss: 0.00067549
Iteration 8/25 | Loss: 0.00067549
Iteration 9/25 | Loss: 0.00067549
Iteration 10/25 | Loss: 0.00067549
Iteration 11/25 | Loss: 0.00067549
Iteration 12/25 | Loss: 0.00067549
Iteration 13/25 | Loss: 0.00067549
Iteration 14/25 | Loss: 0.00067549
Iteration 15/25 | Loss: 0.00067549
Iteration 16/25 | Loss: 0.00067549
Iteration 17/25 | Loss: 0.00067549
Iteration 18/25 | Loss: 0.00067549
Iteration 19/25 | Loss: 0.00067549
Iteration 20/25 | Loss: 0.00067549
Iteration 21/25 | Loss: 0.00067549
Iteration 22/25 | Loss: 0.00067549
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006754930363968015, 0.0006754930363968015, 0.0006754930363968015, 0.0006754930363968015, 0.0006754930363968015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006754930363968015

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067549
Iteration 2/1000 | Loss: 0.00003640
Iteration 3/1000 | Loss: 0.00002108
Iteration 4/1000 | Loss: 0.00001755
Iteration 5/1000 | Loss: 0.00001626
Iteration 6/1000 | Loss: 0.00001550
Iteration 7/1000 | Loss: 0.00001515
Iteration 8/1000 | Loss: 0.00001471
Iteration 9/1000 | Loss: 0.00001442
Iteration 10/1000 | Loss: 0.00001419
Iteration 11/1000 | Loss: 0.00001402
Iteration 12/1000 | Loss: 0.00001397
Iteration 13/1000 | Loss: 0.00001393
Iteration 14/1000 | Loss: 0.00001379
Iteration 15/1000 | Loss: 0.00001376
Iteration 16/1000 | Loss: 0.00001374
Iteration 17/1000 | Loss: 0.00001373
Iteration 18/1000 | Loss: 0.00001373
Iteration 19/1000 | Loss: 0.00001372
Iteration 20/1000 | Loss: 0.00001372
Iteration 21/1000 | Loss: 0.00001371
Iteration 22/1000 | Loss: 0.00001371
Iteration 23/1000 | Loss: 0.00001369
Iteration 24/1000 | Loss: 0.00001368
Iteration 25/1000 | Loss: 0.00001367
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001364
Iteration 28/1000 | Loss: 0.00001363
Iteration 29/1000 | Loss: 0.00001363
Iteration 30/1000 | Loss: 0.00001362
Iteration 31/1000 | Loss: 0.00001357
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001353
Iteration 34/1000 | Loss: 0.00001353
Iteration 35/1000 | Loss: 0.00001351
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001351
Iteration 38/1000 | Loss: 0.00001350
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001346
Iteration 41/1000 | Loss: 0.00001345
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001343
Iteration 45/1000 | Loss: 0.00001343
Iteration 46/1000 | Loss: 0.00001343
Iteration 47/1000 | Loss: 0.00001343
Iteration 48/1000 | Loss: 0.00001343
Iteration 49/1000 | Loss: 0.00001343
Iteration 50/1000 | Loss: 0.00001343
Iteration 51/1000 | Loss: 0.00001343
Iteration 52/1000 | Loss: 0.00001343
Iteration 53/1000 | Loss: 0.00001342
Iteration 54/1000 | Loss: 0.00001342
Iteration 55/1000 | Loss: 0.00001342
Iteration 56/1000 | Loss: 0.00001342
Iteration 57/1000 | Loss: 0.00001342
Iteration 58/1000 | Loss: 0.00001342
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001341
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001341
Iteration 68/1000 | Loss: 0.00001341
Iteration 69/1000 | Loss: 0.00001341
Iteration 70/1000 | Loss: 0.00001341
Iteration 71/1000 | Loss: 0.00001341
Iteration 72/1000 | Loss: 0.00001341
Iteration 73/1000 | Loss: 0.00001341
Iteration 74/1000 | Loss: 0.00001341
Iteration 75/1000 | Loss: 0.00001341
Iteration 76/1000 | Loss: 0.00001340
Iteration 77/1000 | Loss: 0.00001340
Iteration 78/1000 | Loss: 0.00001340
Iteration 79/1000 | Loss: 0.00001339
Iteration 80/1000 | Loss: 0.00001339
Iteration 81/1000 | Loss: 0.00001339
Iteration 82/1000 | Loss: 0.00001339
Iteration 83/1000 | Loss: 0.00001338
Iteration 84/1000 | Loss: 0.00001338
Iteration 85/1000 | Loss: 0.00001338
Iteration 86/1000 | Loss: 0.00001337
Iteration 87/1000 | Loss: 0.00001337
Iteration 88/1000 | Loss: 0.00001337
Iteration 89/1000 | Loss: 0.00001337
Iteration 90/1000 | Loss: 0.00001336
Iteration 91/1000 | Loss: 0.00001336
Iteration 92/1000 | Loss: 0.00001336
Iteration 93/1000 | Loss: 0.00001336
Iteration 94/1000 | Loss: 0.00001335
Iteration 95/1000 | Loss: 0.00001335
Iteration 96/1000 | Loss: 0.00001335
Iteration 97/1000 | Loss: 0.00001335
Iteration 98/1000 | Loss: 0.00001335
Iteration 99/1000 | Loss: 0.00001334
Iteration 100/1000 | Loss: 0.00001334
Iteration 101/1000 | Loss: 0.00001333
Iteration 102/1000 | Loss: 0.00001333
Iteration 103/1000 | Loss: 0.00001333
Iteration 104/1000 | Loss: 0.00001332
Iteration 105/1000 | Loss: 0.00001332
Iteration 106/1000 | Loss: 0.00001332
Iteration 107/1000 | Loss: 0.00001332
Iteration 108/1000 | Loss: 0.00001332
Iteration 109/1000 | Loss: 0.00001332
Iteration 110/1000 | Loss: 0.00001332
Iteration 111/1000 | Loss: 0.00001332
Iteration 112/1000 | Loss: 0.00001331
Iteration 113/1000 | Loss: 0.00001331
Iteration 114/1000 | Loss: 0.00001331
Iteration 115/1000 | Loss: 0.00001331
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001330
Iteration 118/1000 | Loss: 0.00001330
Iteration 119/1000 | Loss: 0.00001329
Iteration 120/1000 | Loss: 0.00001329
Iteration 121/1000 | Loss: 0.00001329
Iteration 122/1000 | Loss: 0.00001329
Iteration 123/1000 | Loss: 0.00001329
Iteration 124/1000 | Loss: 0.00001329
Iteration 125/1000 | Loss: 0.00001329
Iteration 126/1000 | Loss: 0.00001329
Iteration 127/1000 | Loss: 0.00001328
Iteration 128/1000 | Loss: 0.00001328
Iteration 129/1000 | Loss: 0.00001328
Iteration 130/1000 | Loss: 0.00001328
Iteration 131/1000 | Loss: 0.00001328
Iteration 132/1000 | Loss: 0.00001328
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001326
Iteration 141/1000 | Loss: 0.00001326
Iteration 142/1000 | Loss: 0.00001326
Iteration 143/1000 | Loss: 0.00001326
Iteration 144/1000 | Loss: 0.00001326
Iteration 145/1000 | Loss: 0.00001326
Iteration 146/1000 | Loss: 0.00001326
Iteration 147/1000 | Loss: 0.00001326
Iteration 148/1000 | Loss: 0.00001326
Iteration 149/1000 | Loss: 0.00001326
Iteration 150/1000 | Loss: 0.00001326
Iteration 151/1000 | Loss: 0.00001325
Iteration 152/1000 | Loss: 0.00001325
Iteration 153/1000 | Loss: 0.00001325
Iteration 154/1000 | Loss: 0.00001325
Iteration 155/1000 | Loss: 0.00001325
Iteration 156/1000 | Loss: 0.00001324
Iteration 157/1000 | Loss: 0.00001324
Iteration 158/1000 | Loss: 0.00001324
Iteration 159/1000 | Loss: 0.00001324
Iteration 160/1000 | Loss: 0.00001324
Iteration 161/1000 | Loss: 0.00001324
Iteration 162/1000 | Loss: 0.00001324
Iteration 163/1000 | Loss: 0.00001324
Iteration 164/1000 | Loss: 0.00001324
Iteration 165/1000 | Loss: 0.00001324
Iteration 166/1000 | Loss: 0.00001324
Iteration 167/1000 | Loss: 0.00001324
Iteration 168/1000 | Loss: 0.00001324
Iteration 169/1000 | Loss: 0.00001323
Iteration 170/1000 | Loss: 0.00001323
Iteration 171/1000 | Loss: 0.00001323
Iteration 172/1000 | Loss: 0.00001323
Iteration 173/1000 | Loss: 0.00001323
Iteration 174/1000 | Loss: 0.00001322
Iteration 175/1000 | Loss: 0.00001322
Iteration 176/1000 | Loss: 0.00001322
Iteration 177/1000 | Loss: 0.00001322
Iteration 178/1000 | Loss: 0.00001322
Iteration 179/1000 | Loss: 0.00001322
Iteration 180/1000 | Loss: 0.00001322
Iteration 181/1000 | Loss: 0.00001321
Iteration 182/1000 | Loss: 0.00001321
Iteration 183/1000 | Loss: 0.00001321
Iteration 184/1000 | Loss: 0.00001321
Iteration 185/1000 | Loss: 0.00001321
Iteration 186/1000 | Loss: 0.00001321
Iteration 187/1000 | Loss: 0.00001321
Iteration 188/1000 | Loss: 0.00001321
Iteration 189/1000 | Loss: 0.00001321
Iteration 190/1000 | Loss: 0.00001320
Iteration 191/1000 | Loss: 0.00001320
Iteration 192/1000 | Loss: 0.00001320
Iteration 193/1000 | Loss: 0.00001320
Iteration 194/1000 | Loss: 0.00001320
Iteration 195/1000 | Loss: 0.00001319
Iteration 196/1000 | Loss: 0.00001319
Iteration 197/1000 | Loss: 0.00001319
Iteration 198/1000 | Loss: 0.00001319
Iteration 199/1000 | Loss: 0.00001319
Iteration 200/1000 | Loss: 0.00001319
Iteration 201/1000 | Loss: 0.00001319
Iteration 202/1000 | Loss: 0.00001319
Iteration 203/1000 | Loss: 0.00001319
Iteration 204/1000 | Loss: 0.00001319
Iteration 205/1000 | Loss: 0.00001319
Iteration 206/1000 | Loss: 0.00001319
Iteration 207/1000 | Loss: 0.00001319
Iteration 208/1000 | Loss: 0.00001318
Iteration 209/1000 | Loss: 0.00001318
Iteration 210/1000 | Loss: 0.00001318
Iteration 211/1000 | Loss: 0.00001318
Iteration 212/1000 | Loss: 0.00001318
Iteration 213/1000 | Loss: 0.00001318
Iteration 214/1000 | Loss: 0.00001318
Iteration 215/1000 | Loss: 0.00001318
Iteration 216/1000 | Loss: 0.00001318
Iteration 217/1000 | Loss: 0.00001318
Iteration 218/1000 | Loss: 0.00001318
Iteration 219/1000 | Loss: 0.00001318
Iteration 220/1000 | Loss: 0.00001318
Iteration 221/1000 | Loss: 0.00001318
Iteration 222/1000 | Loss: 0.00001318
Iteration 223/1000 | Loss: 0.00001318
Iteration 224/1000 | Loss: 0.00001318
Iteration 225/1000 | Loss: 0.00001318
Iteration 226/1000 | Loss: 0.00001318
Iteration 227/1000 | Loss: 0.00001318
Iteration 228/1000 | Loss: 0.00001318
Iteration 229/1000 | Loss: 0.00001318
Iteration 230/1000 | Loss: 0.00001318
Iteration 231/1000 | Loss: 0.00001318
Iteration 232/1000 | Loss: 0.00001318
Iteration 233/1000 | Loss: 0.00001318
Iteration 234/1000 | Loss: 0.00001318
Iteration 235/1000 | Loss: 0.00001318
Iteration 236/1000 | Loss: 0.00001318
Iteration 237/1000 | Loss: 0.00001318
Iteration 238/1000 | Loss: 0.00001318
Iteration 239/1000 | Loss: 0.00001318
Iteration 240/1000 | Loss: 0.00001318
Iteration 241/1000 | Loss: 0.00001318
Iteration 242/1000 | Loss: 0.00001318
Iteration 243/1000 | Loss: 0.00001318
Iteration 244/1000 | Loss: 0.00001318
Iteration 245/1000 | Loss: 0.00001318
Iteration 246/1000 | Loss: 0.00001318
Iteration 247/1000 | Loss: 0.00001318
Iteration 248/1000 | Loss: 0.00001318
Iteration 249/1000 | Loss: 0.00001318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.3176945685700048e-05, 1.3176945685700048e-05, 1.3176945685700048e-05, 1.3176945685700048e-05, 1.3176945685700048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3176945685700048e-05

Optimization complete. Final v2v error: 3.0276236534118652 mm

Highest mean error: 4.344733238220215 mm for frame 67

Lowest mean error: 2.522479295730591 mm for frame 27

Saving results

Total time: 42.71842980384827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00557691
Iteration 2/25 | Loss: 0.00113886
Iteration 3/25 | Loss: 0.00107127
Iteration 4/25 | Loss: 0.00106261
Iteration 5/25 | Loss: 0.00105956
Iteration 6/25 | Loss: 0.00105899
Iteration 7/25 | Loss: 0.00105899
Iteration 8/25 | Loss: 0.00105899
Iteration 9/25 | Loss: 0.00105899
Iteration 10/25 | Loss: 0.00105899
Iteration 11/25 | Loss: 0.00105899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001058990485034883, 0.001058990485034883, 0.001058990485034883, 0.001058990485034883, 0.001058990485034883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001058990485034883

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.71955442
Iteration 2/25 | Loss: 0.00079946
Iteration 3/25 | Loss: 0.00079946
Iteration 4/25 | Loss: 0.00079946
Iteration 5/25 | Loss: 0.00079946
Iteration 6/25 | Loss: 0.00079946
Iteration 7/25 | Loss: 0.00079946
Iteration 8/25 | Loss: 0.00079946
Iteration 9/25 | Loss: 0.00079946
Iteration 10/25 | Loss: 0.00079946
Iteration 11/25 | Loss: 0.00079946
Iteration 12/25 | Loss: 0.00079945
Iteration 13/25 | Loss: 0.00079945
Iteration 14/25 | Loss: 0.00079945
Iteration 15/25 | Loss: 0.00079945
Iteration 16/25 | Loss: 0.00079945
Iteration 17/25 | Loss: 0.00079945
Iteration 18/25 | Loss: 0.00079945
Iteration 19/25 | Loss: 0.00079945
Iteration 20/25 | Loss: 0.00079945
Iteration 21/25 | Loss: 0.00079945
Iteration 22/25 | Loss: 0.00079945
Iteration 23/25 | Loss: 0.00079945
Iteration 24/25 | Loss: 0.00079945
Iteration 25/25 | Loss: 0.00079945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079945
Iteration 2/1000 | Loss: 0.00001816
Iteration 3/1000 | Loss: 0.00001209
Iteration 4/1000 | Loss: 0.00001084
Iteration 5/1000 | Loss: 0.00001027
Iteration 6/1000 | Loss: 0.00000995
Iteration 7/1000 | Loss: 0.00000961
Iteration 8/1000 | Loss: 0.00000960
Iteration 9/1000 | Loss: 0.00000958
Iteration 10/1000 | Loss: 0.00000958
Iteration 11/1000 | Loss: 0.00000942
Iteration 12/1000 | Loss: 0.00000939
Iteration 13/1000 | Loss: 0.00000929
Iteration 14/1000 | Loss: 0.00000917
Iteration 15/1000 | Loss: 0.00000911
Iteration 16/1000 | Loss: 0.00000910
Iteration 17/1000 | Loss: 0.00000910
Iteration 18/1000 | Loss: 0.00000910
Iteration 19/1000 | Loss: 0.00000909
Iteration 20/1000 | Loss: 0.00000906
Iteration 21/1000 | Loss: 0.00000904
Iteration 22/1000 | Loss: 0.00000903
Iteration 23/1000 | Loss: 0.00000902
Iteration 24/1000 | Loss: 0.00000899
Iteration 25/1000 | Loss: 0.00000895
Iteration 26/1000 | Loss: 0.00000892
Iteration 27/1000 | Loss: 0.00000892
Iteration 28/1000 | Loss: 0.00000891
Iteration 29/1000 | Loss: 0.00000889
Iteration 30/1000 | Loss: 0.00000888
Iteration 31/1000 | Loss: 0.00000887
Iteration 32/1000 | Loss: 0.00000887
Iteration 33/1000 | Loss: 0.00000886
Iteration 34/1000 | Loss: 0.00000886
Iteration 35/1000 | Loss: 0.00000886
Iteration 36/1000 | Loss: 0.00000885
Iteration 37/1000 | Loss: 0.00000885
Iteration 38/1000 | Loss: 0.00000879
Iteration 39/1000 | Loss: 0.00000878
Iteration 40/1000 | Loss: 0.00000877
Iteration 41/1000 | Loss: 0.00000877
Iteration 42/1000 | Loss: 0.00000876
Iteration 43/1000 | Loss: 0.00000876
Iteration 44/1000 | Loss: 0.00000876
Iteration 45/1000 | Loss: 0.00000876
Iteration 46/1000 | Loss: 0.00000876
Iteration 47/1000 | Loss: 0.00000876
Iteration 48/1000 | Loss: 0.00000876
Iteration 49/1000 | Loss: 0.00000876
Iteration 50/1000 | Loss: 0.00000876
Iteration 51/1000 | Loss: 0.00000876
Iteration 52/1000 | Loss: 0.00000875
Iteration 53/1000 | Loss: 0.00000875
Iteration 54/1000 | Loss: 0.00000875
Iteration 55/1000 | Loss: 0.00000875
Iteration 56/1000 | Loss: 0.00000875
Iteration 57/1000 | Loss: 0.00000875
Iteration 58/1000 | Loss: 0.00000875
Iteration 59/1000 | Loss: 0.00000874
Iteration 60/1000 | Loss: 0.00000873
Iteration 61/1000 | Loss: 0.00000872
Iteration 62/1000 | Loss: 0.00000872
Iteration 63/1000 | Loss: 0.00000871
Iteration 64/1000 | Loss: 0.00000871
Iteration 65/1000 | Loss: 0.00000871
Iteration 66/1000 | Loss: 0.00000871
Iteration 67/1000 | Loss: 0.00000871
Iteration 68/1000 | Loss: 0.00000871
Iteration 69/1000 | Loss: 0.00000870
Iteration 70/1000 | Loss: 0.00000870
Iteration 71/1000 | Loss: 0.00000870
Iteration 72/1000 | Loss: 0.00000870
Iteration 73/1000 | Loss: 0.00000870
Iteration 74/1000 | Loss: 0.00000869
Iteration 75/1000 | Loss: 0.00000869
Iteration 76/1000 | Loss: 0.00000868
Iteration 77/1000 | Loss: 0.00000868
Iteration 78/1000 | Loss: 0.00000868
Iteration 79/1000 | Loss: 0.00000867
Iteration 80/1000 | Loss: 0.00000867
Iteration 81/1000 | Loss: 0.00000866
Iteration 82/1000 | Loss: 0.00000865
Iteration 83/1000 | Loss: 0.00000865
Iteration 84/1000 | Loss: 0.00000865
Iteration 85/1000 | Loss: 0.00000865
Iteration 86/1000 | Loss: 0.00000864
Iteration 87/1000 | Loss: 0.00000864
Iteration 88/1000 | Loss: 0.00000864
Iteration 89/1000 | Loss: 0.00000864
Iteration 90/1000 | Loss: 0.00000864
Iteration 91/1000 | Loss: 0.00000864
Iteration 92/1000 | Loss: 0.00000864
Iteration 93/1000 | Loss: 0.00000864
Iteration 94/1000 | Loss: 0.00000864
Iteration 95/1000 | Loss: 0.00000863
Iteration 96/1000 | Loss: 0.00000863
Iteration 97/1000 | Loss: 0.00000863
Iteration 98/1000 | Loss: 0.00000862
Iteration 99/1000 | Loss: 0.00000862
Iteration 100/1000 | Loss: 0.00000862
Iteration 101/1000 | Loss: 0.00000862
Iteration 102/1000 | Loss: 0.00000862
Iteration 103/1000 | Loss: 0.00000862
Iteration 104/1000 | Loss: 0.00000862
Iteration 105/1000 | Loss: 0.00000861
Iteration 106/1000 | Loss: 0.00000861
Iteration 107/1000 | Loss: 0.00000861
Iteration 108/1000 | Loss: 0.00000861
Iteration 109/1000 | Loss: 0.00000861
Iteration 110/1000 | Loss: 0.00000861
Iteration 111/1000 | Loss: 0.00000861
Iteration 112/1000 | Loss: 0.00000860
Iteration 113/1000 | Loss: 0.00000860
Iteration 114/1000 | Loss: 0.00000860
Iteration 115/1000 | Loss: 0.00000859
Iteration 116/1000 | Loss: 0.00000859
Iteration 117/1000 | Loss: 0.00000859
Iteration 118/1000 | Loss: 0.00000859
Iteration 119/1000 | Loss: 0.00000859
Iteration 120/1000 | Loss: 0.00000859
Iteration 121/1000 | Loss: 0.00000859
Iteration 122/1000 | Loss: 0.00000858
Iteration 123/1000 | Loss: 0.00000858
Iteration 124/1000 | Loss: 0.00000858
Iteration 125/1000 | Loss: 0.00000858
Iteration 126/1000 | Loss: 0.00000858
Iteration 127/1000 | Loss: 0.00000858
Iteration 128/1000 | Loss: 0.00000858
Iteration 129/1000 | Loss: 0.00000858
Iteration 130/1000 | Loss: 0.00000857
Iteration 131/1000 | Loss: 0.00000857
Iteration 132/1000 | Loss: 0.00000857
Iteration 133/1000 | Loss: 0.00000857
Iteration 134/1000 | Loss: 0.00000857
Iteration 135/1000 | Loss: 0.00000857
Iteration 136/1000 | Loss: 0.00000857
Iteration 137/1000 | Loss: 0.00000857
Iteration 138/1000 | Loss: 0.00000857
Iteration 139/1000 | Loss: 0.00000857
Iteration 140/1000 | Loss: 0.00000857
Iteration 141/1000 | Loss: 0.00000857
Iteration 142/1000 | Loss: 0.00000857
Iteration 143/1000 | Loss: 0.00000857
Iteration 144/1000 | Loss: 0.00000857
Iteration 145/1000 | Loss: 0.00000857
Iteration 146/1000 | Loss: 0.00000857
Iteration 147/1000 | Loss: 0.00000857
Iteration 148/1000 | Loss: 0.00000857
Iteration 149/1000 | Loss: 0.00000856
Iteration 150/1000 | Loss: 0.00000856
Iteration 151/1000 | Loss: 0.00000856
Iteration 152/1000 | Loss: 0.00000856
Iteration 153/1000 | Loss: 0.00000856
Iteration 154/1000 | Loss: 0.00000856
Iteration 155/1000 | Loss: 0.00000856
Iteration 156/1000 | Loss: 0.00000856
Iteration 157/1000 | Loss: 0.00000856
Iteration 158/1000 | Loss: 0.00000856
Iteration 159/1000 | Loss: 0.00000855
Iteration 160/1000 | Loss: 0.00000855
Iteration 161/1000 | Loss: 0.00000855
Iteration 162/1000 | Loss: 0.00000855
Iteration 163/1000 | Loss: 0.00000855
Iteration 164/1000 | Loss: 0.00000855
Iteration 165/1000 | Loss: 0.00000855
Iteration 166/1000 | Loss: 0.00000855
Iteration 167/1000 | Loss: 0.00000855
Iteration 168/1000 | Loss: 0.00000855
Iteration 169/1000 | Loss: 0.00000855
Iteration 170/1000 | Loss: 0.00000855
Iteration 171/1000 | Loss: 0.00000855
Iteration 172/1000 | Loss: 0.00000855
Iteration 173/1000 | Loss: 0.00000855
Iteration 174/1000 | Loss: 0.00000855
Iteration 175/1000 | Loss: 0.00000855
Iteration 176/1000 | Loss: 0.00000855
Iteration 177/1000 | Loss: 0.00000855
Iteration 178/1000 | Loss: 0.00000855
Iteration 179/1000 | Loss: 0.00000855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [8.546491699235048e-06, 8.546491699235048e-06, 8.546491699235048e-06, 8.546491699235048e-06, 8.546491699235048e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.546491699235048e-06

Optimization complete. Final v2v error: 2.5281853675842285 mm

Highest mean error: 2.903994560241699 mm for frame 112

Lowest mean error: 2.3675222396850586 mm for frame 40

Saving results

Total time: 36.371646881103516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569308
Iteration 2/25 | Loss: 0.00149057
Iteration 3/25 | Loss: 0.00124454
Iteration 4/25 | Loss: 0.00122412
Iteration 5/25 | Loss: 0.00121711
Iteration 6/25 | Loss: 0.00121617
Iteration 7/25 | Loss: 0.00121617
Iteration 8/25 | Loss: 0.00121617
Iteration 9/25 | Loss: 0.00121617
Iteration 10/25 | Loss: 0.00121617
Iteration 11/25 | Loss: 0.00121617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00121616933029145, 0.00121616933029145, 0.00121616933029145, 0.00121616933029145, 0.00121616933029145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00121616933029145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89270139
Iteration 2/25 | Loss: 0.00065570
Iteration 3/25 | Loss: 0.00065566
Iteration 4/25 | Loss: 0.00065566
Iteration 5/25 | Loss: 0.00065566
Iteration 6/25 | Loss: 0.00065566
Iteration 7/25 | Loss: 0.00065566
Iteration 8/25 | Loss: 0.00065566
Iteration 9/25 | Loss: 0.00065566
Iteration 10/25 | Loss: 0.00065566
Iteration 11/25 | Loss: 0.00065566
Iteration 12/25 | Loss: 0.00065566
Iteration 13/25 | Loss: 0.00065566
Iteration 14/25 | Loss: 0.00065566
Iteration 15/25 | Loss: 0.00065566
Iteration 16/25 | Loss: 0.00065566
Iteration 17/25 | Loss: 0.00065566
Iteration 18/25 | Loss: 0.00065566
Iteration 19/25 | Loss: 0.00065566
Iteration 20/25 | Loss: 0.00065566
Iteration 21/25 | Loss: 0.00065566
Iteration 22/25 | Loss: 0.00065566
Iteration 23/25 | Loss: 0.00065566
Iteration 24/25 | Loss: 0.00065566
Iteration 25/25 | Loss: 0.00065566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065566
Iteration 2/1000 | Loss: 0.00005049
Iteration 3/1000 | Loss: 0.00003037
Iteration 4/1000 | Loss: 0.00002654
Iteration 5/1000 | Loss: 0.00002507
Iteration 6/1000 | Loss: 0.00002396
Iteration 7/1000 | Loss: 0.00002335
Iteration 8/1000 | Loss: 0.00002306
Iteration 9/1000 | Loss: 0.00002280
Iteration 10/1000 | Loss: 0.00002272
Iteration 11/1000 | Loss: 0.00002252
Iteration 12/1000 | Loss: 0.00002244
Iteration 13/1000 | Loss: 0.00002244
Iteration 14/1000 | Loss: 0.00002244
Iteration 15/1000 | Loss: 0.00002243
Iteration 16/1000 | Loss: 0.00002243
Iteration 17/1000 | Loss: 0.00002239
Iteration 18/1000 | Loss: 0.00002239
Iteration 19/1000 | Loss: 0.00002238
Iteration 20/1000 | Loss: 0.00002238
Iteration 21/1000 | Loss: 0.00002238
Iteration 22/1000 | Loss: 0.00002238
Iteration 23/1000 | Loss: 0.00002238
Iteration 24/1000 | Loss: 0.00002238
Iteration 25/1000 | Loss: 0.00002238
Iteration 26/1000 | Loss: 0.00002237
Iteration 27/1000 | Loss: 0.00002237
Iteration 28/1000 | Loss: 0.00002234
Iteration 29/1000 | Loss: 0.00002234
Iteration 30/1000 | Loss: 0.00002234
Iteration 31/1000 | Loss: 0.00002233
Iteration 32/1000 | Loss: 0.00002232
Iteration 33/1000 | Loss: 0.00002232
Iteration 34/1000 | Loss: 0.00002232
Iteration 35/1000 | Loss: 0.00002232
Iteration 36/1000 | Loss: 0.00002232
Iteration 37/1000 | Loss: 0.00002231
Iteration 38/1000 | Loss: 0.00002231
Iteration 39/1000 | Loss: 0.00002231
Iteration 40/1000 | Loss: 0.00002231
Iteration 41/1000 | Loss: 0.00002231
Iteration 42/1000 | Loss: 0.00002230
Iteration 43/1000 | Loss: 0.00002229
Iteration 44/1000 | Loss: 0.00002229
Iteration 45/1000 | Loss: 0.00002229
Iteration 46/1000 | Loss: 0.00002229
Iteration 47/1000 | Loss: 0.00002229
Iteration 48/1000 | Loss: 0.00002229
Iteration 49/1000 | Loss: 0.00002229
Iteration 50/1000 | Loss: 0.00002229
Iteration 51/1000 | Loss: 0.00002229
Iteration 52/1000 | Loss: 0.00002229
Iteration 53/1000 | Loss: 0.00002228
Iteration 54/1000 | Loss: 0.00002228
Iteration 55/1000 | Loss: 0.00002228
Iteration 56/1000 | Loss: 0.00002227
Iteration 57/1000 | Loss: 0.00002227
Iteration 58/1000 | Loss: 0.00002226
Iteration 59/1000 | Loss: 0.00002225
Iteration 60/1000 | Loss: 0.00002225
Iteration 61/1000 | Loss: 0.00002225
Iteration 62/1000 | Loss: 0.00002225
Iteration 63/1000 | Loss: 0.00002225
Iteration 64/1000 | Loss: 0.00002225
Iteration 65/1000 | Loss: 0.00002225
Iteration 66/1000 | Loss: 0.00002225
Iteration 67/1000 | Loss: 0.00002224
Iteration 68/1000 | Loss: 0.00002224
Iteration 69/1000 | Loss: 0.00002224
Iteration 70/1000 | Loss: 0.00002224
Iteration 71/1000 | Loss: 0.00002223
Iteration 72/1000 | Loss: 0.00002223
Iteration 73/1000 | Loss: 0.00002222
Iteration 74/1000 | Loss: 0.00002222
Iteration 75/1000 | Loss: 0.00002222
Iteration 76/1000 | Loss: 0.00002222
Iteration 77/1000 | Loss: 0.00002222
Iteration 78/1000 | Loss: 0.00002222
Iteration 79/1000 | Loss: 0.00002222
Iteration 80/1000 | Loss: 0.00002222
Iteration 81/1000 | Loss: 0.00002222
Iteration 82/1000 | Loss: 0.00002222
Iteration 83/1000 | Loss: 0.00002222
Iteration 84/1000 | Loss: 0.00002222
Iteration 85/1000 | Loss: 0.00002222
Iteration 86/1000 | Loss: 0.00002222
Iteration 87/1000 | Loss: 0.00002221
Iteration 88/1000 | Loss: 0.00002221
Iteration 89/1000 | Loss: 0.00002221
Iteration 90/1000 | Loss: 0.00002221
Iteration 91/1000 | Loss: 0.00002221
Iteration 92/1000 | Loss: 0.00002221
Iteration 93/1000 | Loss: 0.00002221
Iteration 94/1000 | Loss: 0.00002221
Iteration 95/1000 | Loss: 0.00002221
Iteration 96/1000 | Loss: 0.00002221
Iteration 97/1000 | Loss: 0.00002221
Iteration 98/1000 | Loss: 0.00002221
Iteration 99/1000 | Loss: 0.00002221
Iteration 100/1000 | Loss: 0.00002221
Iteration 101/1000 | Loss: 0.00002221
Iteration 102/1000 | Loss: 0.00002221
Iteration 103/1000 | Loss: 0.00002221
Iteration 104/1000 | Loss: 0.00002221
Iteration 105/1000 | Loss: 0.00002221
Iteration 106/1000 | Loss: 0.00002221
Iteration 107/1000 | Loss: 0.00002221
Iteration 108/1000 | Loss: 0.00002221
Iteration 109/1000 | Loss: 0.00002221
Iteration 110/1000 | Loss: 0.00002221
Iteration 111/1000 | Loss: 0.00002221
Iteration 112/1000 | Loss: 0.00002221
Iteration 113/1000 | Loss: 0.00002221
Iteration 114/1000 | Loss: 0.00002221
Iteration 115/1000 | Loss: 0.00002221
Iteration 116/1000 | Loss: 0.00002221
Iteration 117/1000 | Loss: 0.00002221
Iteration 118/1000 | Loss: 0.00002221
Iteration 119/1000 | Loss: 0.00002221
Iteration 120/1000 | Loss: 0.00002221
Iteration 121/1000 | Loss: 0.00002221
Iteration 122/1000 | Loss: 0.00002221
Iteration 123/1000 | Loss: 0.00002221
Iteration 124/1000 | Loss: 0.00002221
Iteration 125/1000 | Loss: 0.00002221
Iteration 126/1000 | Loss: 0.00002221
Iteration 127/1000 | Loss: 0.00002221
Iteration 128/1000 | Loss: 0.00002221
Iteration 129/1000 | Loss: 0.00002221
Iteration 130/1000 | Loss: 0.00002221
Iteration 131/1000 | Loss: 0.00002221
Iteration 132/1000 | Loss: 0.00002221
Iteration 133/1000 | Loss: 0.00002221
Iteration 134/1000 | Loss: 0.00002221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.220619899162557e-05, 2.220619899162557e-05, 2.220619899162557e-05, 2.220619899162557e-05, 2.220619899162557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.220619899162557e-05

Optimization complete. Final v2v error: 3.943608045578003 mm

Highest mean error: 4.319416522979736 mm for frame 113

Lowest mean error: 3.6912872791290283 mm for frame 6

Saving results

Total time: 31.941161155700684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00528659
Iteration 2/25 | Loss: 0.00132981
Iteration 3/25 | Loss: 0.00120154
Iteration 4/25 | Loss: 0.00118664
Iteration 5/25 | Loss: 0.00118221
Iteration 6/25 | Loss: 0.00118203
Iteration 7/25 | Loss: 0.00118203
Iteration 8/25 | Loss: 0.00118203
Iteration 9/25 | Loss: 0.00118203
Iteration 10/25 | Loss: 0.00118203
Iteration 11/25 | Loss: 0.00118203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011820251820608974, 0.0011820251820608974, 0.0011820251820608974, 0.0011820251820608974, 0.0011820251820608974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011820251820608974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.20679474
Iteration 2/25 | Loss: 0.00074639
Iteration 3/25 | Loss: 0.00074637
Iteration 4/25 | Loss: 0.00074637
Iteration 5/25 | Loss: 0.00074637
Iteration 6/25 | Loss: 0.00074637
Iteration 7/25 | Loss: 0.00074637
Iteration 8/25 | Loss: 0.00074637
Iteration 9/25 | Loss: 0.00074637
Iteration 10/25 | Loss: 0.00074637
Iteration 11/25 | Loss: 0.00074637
Iteration 12/25 | Loss: 0.00074637
Iteration 13/25 | Loss: 0.00074637
Iteration 14/25 | Loss: 0.00074637
Iteration 15/25 | Loss: 0.00074637
Iteration 16/25 | Loss: 0.00074637
Iteration 17/25 | Loss: 0.00074637
Iteration 18/25 | Loss: 0.00074637
Iteration 19/25 | Loss: 0.00074637
Iteration 20/25 | Loss: 0.00074637
Iteration 21/25 | Loss: 0.00074637
Iteration 22/25 | Loss: 0.00074637
Iteration 23/25 | Loss: 0.00074637
Iteration 24/25 | Loss: 0.00074637
Iteration 25/25 | Loss: 0.00074637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074637
Iteration 2/1000 | Loss: 0.00004114
Iteration 3/1000 | Loss: 0.00002300
Iteration 4/1000 | Loss: 0.00002026
Iteration 5/1000 | Loss: 0.00001931
Iteration 6/1000 | Loss: 0.00001841
Iteration 7/1000 | Loss: 0.00001786
Iteration 8/1000 | Loss: 0.00001752
Iteration 9/1000 | Loss: 0.00001720
Iteration 10/1000 | Loss: 0.00001695
Iteration 11/1000 | Loss: 0.00001675
Iteration 12/1000 | Loss: 0.00001670
Iteration 13/1000 | Loss: 0.00001663
Iteration 14/1000 | Loss: 0.00001663
Iteration 15/1000 | Loss: 0.00001660
Iteration 16/1000 | Loss: 0.00001654
Iteration 17/1000 | Loss: 0.00001646
Iteration 18/1000 | Loss: 0.00001643
Iteration 19/1000 | Loss: 0.00001642
Iteration 20/1000 | Loss: 0.00001642
Iteration 21/1000 | Loss: 0.00001641
Iteration 22/1000 | Loss: 0.00001640
Iteration 23/1000 | Loss: 0.00001639
Iteration 24/1000 | Loss: 0.00001639
Iteration 25/1000 | Loss: 0.00001638
Iteration 26/1000 | Loss: 0.00001638
Iteration 27/1000 | Loss: 0.00001638
Iteration 28/1000 | Loss: 0.00001638
Iteration 29/1000 | Loss: 0.00001637
Iteration 30/1000 | Loss: 0.00001637
Iteration 31/1000 | Loss: 0.00001637
Iteration 32/1000 | Loss: 0.00001636
Iteration 33/1000 | Loss: 0.00001635
Iteration 34/1000 | Loss: 0.00001634
Iteration 35/1000 | Loss: 0.00001634
Iteration 36/1000 | Loss: 0.00001633
Iteration 37/1000 | Loss: 0.00001633
Iteration 38/1000 | Loss: 0.00001633
Iteration 39/1000 | Loss: 0.00001633
Iteration 40/1000 | Loss: 0.00001632
Iteration 41/1000 | Loss: 0.00001632
Iteration 42/1000 | Loss: 0.00001632
Iteration 43/1000 | Loss: 0.00001632
Iteration 44/1000 | Loss: 0.00001631
Iteration 45/1000 | Loss: 0.00001631
Iteration 46/1000 | Loss: 0.00001630
Iteration 47/1000 | Loss: 0.00001630
Iteration 48/1000 | Loss: 0.00001630
Iteration 49/1000 | Loss: 0.00001630
Iteration 50/1000 | Loss: 0.00001630
Iteration 51/1000 | Loss: 0.00001629
Iteration 52/1000 | Loss: 0.00001629
Iteration 53/1000 | Loss: 0.00001629
Iteration 54/1000 | Loss: 0.00001628
Iteration 55/1000 | Loss: 0.00001628
Iteration 56/1000 | Loss: 0.00001628
Iteration 57/1000 | Loss: 0.00001627
Iteration 58/1000 | Loss: 0.00001627
Iteration 59/1000 | Loss: 0.00001627
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001626
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001625
Iteration 64/1000 | Loss: 0.00001625
Iteration 65/1000 | Loss: 0.00001625
Iteration 66/1000 | Loss: 0.00001624
Iteration 67/1000 | Loss: 0.00001624
Iteration 68/1000 | Loss: 0.00001623
Iteration 69/1000 | Loss: 0.00001623
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001623
Iteration 73/1000 | Loss: 0.00001622
Iteration 74/1000 | Loss: 0.00001622
Iteration 75/1000 | Loss: 0.00001622
Iteration 76/1000 | Loss: 0.00001622
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001622
Iteration 79/1000 | Loss: 0.00001622
Iteration 80/1000 | Loss: 0.00001622
Iteration 81/1000 | Loss: 0.00001621
Iteration 82/1000 | Loss: 0.00001621
Iteration 83/1000 | Loss: 0.00001621
Iteration 84/1000 | Loss: 0.00001620
Iteration 85/1000 | Loss: 0.00001620
Iteration 86/1000 | Loss: 0.00001620
Iteration 87/1000 | Loss: 0.00001620
Iteration 88/1000 | Loss: 0.00001620
Iteration 89/1000 | Loss: 0.00001620
Iteration 90/1000 | Loss: 0.00001620
Iteration 91/1000 | Loss: 0.00001620
Iteration 92/1000 | Loss: 0.00001619
Iteration 93/1000 | Loss: 0.00001619
Iteration 94/1000 | Loss: 0.00001619
Iteration 95/1000 | Loss: 0.00001618
Iteration 96/1000 | Loss: 0.00001618
Iteration 97/1000 | Loss: 0.00001618
Iteration 98/1000 | Loss: 0.00001617
Iteration 99/1000 | Loss: 0.00001617
Iteration 100/1000 | Loss: 0.00001617
Iteration 101/1000 | Loss: 0.00001617
Iteration 102/1000 | Loss: 0.00001617
Iteration 103/1000 | Loss: 0.00001617
Iteration 104/1000 | Loss: 0.00001617
Iteration 105/1000 | Loss: 0.00001617
Iteration 106/1000 | Loss: 0.00001616
Iteration 107/1000 | Loss: 0.00001616
Iteration 108/1000 | Loss: 0.00001616
Iteration 109/1000 | Loss: 0.00001615
Iteration 110/1000 | Loss: 0.00001615
Iteration 111/1000 | Loss: 0.00001615
Iteration 112/1000 | Loss: 0.00001615
Iteration 113/1000 | Loss: 0.00001615
Iteration 114/1000 | Loss: 0.00001615
Iteration 115/1000 | Loss: 0.00001615
Iteration 116/1000 | Loss: 0.00001615
Iteration 117/1000 | Loss: 0.00001615
Iteration 118/1000 | Loss: 0.00001615
Iteration 119/1000 | Loss: 0.00001615
Iteration 120/1000 | Loss: 0.00001615
Iteration 121/1000 | Loss: 0.00001615
Iteration 122/1000 | Loss: 0.00001615
Iteration 123/1000 | Loss: 0.00001615
Iteration 124/1000 | Loss: 0.00001615
Iteration 125/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.6148578652064316e-05, 1.6148578652064316e-05, 1.6148578652064316e-05, 1.6148578652064316e-05, 1.6148578652064316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6148578652064316e-05

Optimization complete. Final v2v error: 3.383401393890381 mm

Highest mean error: 3.75950026512146 mm for frame 123

Lowest mean error: 3.1359753608703613 mm for frame 150

Saving results

Total time: 40.40842032432556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920802
Iteration 2/25 | Loss: 0.00136496
Iteration 3/25 | Loss: 0.00124028
Iteration 4/25 | Loss: 0.00122979
Iteration 5/25 | Loss: 0.00122711
Iteration 6/25 | Loss: 0.00122691
Iteration 7/25 | Loss: 0.00122691
Iteration 8/25 | Loss: 0.00122691
Iteration 9/25 | Loss: 0.00122691
Iteration 10/25 | Loss: 0.00122691
Iteration 11/25 | Loss: 0.00122691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012269096914678812, 0.0012269096914678812, 0.0012269096914678812, 0.0012269096914678812, 0.0012269096914678812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012269096914678812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98593521
Iteration 2/25 | Loss: 0.00097687
Iteration 3/25 | Loss: 0.00097687
Iteration 4/25 | Loss: 0.00097687
Iteration 5/25 | Loss: 0.00097687
Iteration 6/25 | Loss: 0.00097687
Iteration 7/25 | Loss: 0.00097687
Iteration 8/25 | Loss: 0.00097687
Iteration 9/25 | Loss: 0.00097687
Iteration 10/25 | Loss: 0.00097687
Iteration 11/25 | Loss: 0.00097687
Iteration 12/25 | Loss: 0.00097687
Iteration 13/25 | Loss: 0.00097687
Iteration 14/25 | Loss: 0.00097687
Iteration 15/25 | Loss: 0.00097687
Iteration 16/25 | Loss: 0.00097687
Iteration 17/25 | Loss: 0.00097687
Iteration 18/25 | Loss: 0.00097687
Iteration 19/25 | Loss: 0.00097687
Iteration 20/25 | Loss: 0.00097687
Iteration 21/25 | Loss: 0.00097687
Iteration 22/25 | Loss: 0.00097687
Iteration 23/25 | Loss: 0.00097687
Iteration 24/25 | Loss: 0.00097687
Iteration 25/25 | Loss: 0.00097687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097687
Iteration 2/1000 | Loss: 0.00005651
Iteration 3/1000 | Loss: 0.00003380
Iteration 4/1000 | Loss: 0.00002591
Iteration 5/1000 | Loss: 0.00002380
Iteration 6/1000 | Loss: 0.00002283
Iteration 7/1000 | Loss: 0.00002233
Iteration 8/1000 | Loss: 0.00002194
Iteration 9/1000 | Loss: 0.00002168
Iteration 10/1000 | Loss: 0.00002142
Iteration 11/1000 | Loss: 0.00002117
Iteration 12/1000 | Loss: 0.00002098
Iteration 13/1000 | Loss: 0.00002096
Iteration 14/1000 | Loss: 0.00002089
Iteration 15/1000 | Loss: 0.00002082
Iteration 16/1000 | Loss: 0.00002070
Iteration 17/1000 | Loss: 0.00002065
Iteration 18/1000 | Loss: 0.00002065
Iteration 19/1000 | Loss: 0.00002065
Iteration 20/1000 | Loss: 0.00002065
Iteration 21/1000 | Loss: 0.00002064
Iteration 22/1000 | Loss: 0.00002064
Iteration 23/1000 | Loss: 0.00002063
Iteration 24/1000 | Loss: 0.00002063
Iteration 25/1000 | Loss: 0.00002062
Iteration 26/1000 | Loss: 0.00002061
Iteration 27/1000 | Loss: 0.00002060
Iteration 28/1000 | Loss: 0.00002059
Iteration 29/1000 | Loss: 0.00002058
Iteration 30/1000 | Loss: 0.00002058
Iteration 31/1000 | Loss: 0.00002058
Iteration 32/1000 | Loss: 0.00002058
Iteration 33/1000 | Loss: 0.00002058
Iteration 34/1000 | Loss: 0.00002058
Iteration 35/1000 | Loss: 0.00002058
Iteration 36/1000 | Loss: 0.00002058
Iteration 37/1000 | Loss: 0.00002057
Iteration 38/1000 | Loss: 0.00002057
Iteration 39/1000 | Loss: 0.00002057
Iteration 40/1000 | Loss: 0.00002057
Iteration 41/1000 | Loss: 0.00002056
Iteration 42/1000 | Loss: 0.00002055
Iteration 43/1000 | Loss: 0.00002055
Iteration 44/1000 | Loss: 0.00002054
Iteration 45/1000 | Loss: 0.00002054
Iteration 46/1000 | Loss: 0.00002054
Iteration 47/1000 | Loss: 0.00002054
Iteration 48/1000 | Loss: 0.00002054
Iteration 49/1000 | Loss: 0.00002054
Iteration 50/1000 | Loss: 0.00002054
Iteration 51/1000 | Loss: 0.00002054
Iteration 52/1000 | Loss: 0.00002054
Iteration 53/1000 | Loss: 0.00002053
Iteration 54/1000 | Loss: 0.00002052
Iteration 55/1000 | Loss: 0.00002052
Iteration 56/1000 | Loss: 0.00002052
Iteration 57/1000 | Loss: 0.00002052
Iteration 58/1000 | Loss: 0.00002051
Iteration 59/1000 | Loss: 0.00002051
Iteration 60/1000 | Loss: 0.00002051
Iteration 61/1000 | Loss: 0.00002051
Iteration 62/1000 | Loss: 0.00002051
Iteration 63/1000 | Loss: 0.00002051
Iteration 64/1000 | Loss: 0.00002051
Iteration 65/1000 | Loss: 0.00002051
Iteration 66/1000 | Loss: 0.00002051
Iteration 67/1000 | Loss: 0.00002051
Iteration 68/1000 | Loss: 0.00002051
Iteration 69/1000 | Loss: 0.00002050
Iteration 70/1000 | Loss: 0.00002050
Iteration 71/1000 | Loss: 0.00002050
Iteration 72/1000 | Loss: 0.00002048
Iteration 73/1000 | Loss: 0.00002048
Iteration 74/1000 | Loss: 0.00002047
Iteration 75/1000 | Loss: 0.00002047
Iteration 76/1000 | Loss: 0.00002046
Iteration 77/1000 | Loss: 0.00002046
Iteration 78/1000 | Loss: 0.00002045
Iteration 79/1000 | Loss: 0.00002045
Iteration 80/1000 | Loss: 0.00002045
Iteration 81/1000 | Loss: 0.00002045
Iteration 82/1000 | Loss: 0.00002045
Iteration 83/1000 | Loss: 0.00002045
Iteration 84/1000 | Loss: 0.00002045
Iteration 85/1000 | Loss: 0.00002045
Iteration 86/1000 | Loss: 0.00002045
Iteration 87/1000 | Loss: 0.00002045
Iteration 88/1000 | Loss: 0.00002045
Iteration 89/1000 | Loss: 0.00002044
Iteration 90/1000 | Loss: 0.00002044
Iteration 91/1000 | Loss: 0.00002044
Iteration 92/1000 | Loss: 0.00002044
Iteration 93/1000 | Loss: 0.00002043
Iteration 94/1000 | Loss: 0.00002043
Iteration 95/1000 | Loss: 0.00002043
Iteration 96/1000 | Loss: 0.00002043
Iteration 97/1000 | Loss: 0.00002043
Iteration 98/1000 | Loss: 0.00002042
Iteration 99/1000 | Loss: 0.00002042
Iteration 100/1000 | Loss: 0.00002042
Iteration 101/1000 | Loss: 0.00002042
Iteration 102/1000 | Loss: 0.00002042
Iteration 103/1000 | Loss: 0.00002042
Iteration 104/1000 | Loss: 0.00002042
Iteration 105/1000 | Loss: 0.00002041
Iteration 106/1000 | Loss: 0.00002041
Iteration 107/1000 | Loss: 0.00002041
Iteration 108/1000 | Loss: 0.00002041
Iteration 109/1000 | Loss: 0.00002041
Iteration 110/1000 | Loss: 0.00002041
Iteration 111/1000 | Loss: 0.00002041
Iteration 112/1000 | Loss: 0.00002040
Iteration 113/1000 | Loss: 0.00002040
Iteration 114/1000 | Loss: 0.00002040
Iteration 115/1000 | Loss: 0.00002040
Iteration 116/1000 | Loss: 0.00002040
Iteration 117/1000 | Loss: 0.00002040
Iteration 118/1000 | Loss: 0.00002039
Iteration 119/1000 | Loss: 0.00002039
Iteration 120/1000 | Loss: 0.00002039
Iteration 121/1000 | Loss: 0.00002038
Iteration 122/1000 | Loss: 0.00002038
Iteration 123/1000 | Loss: 0.00002038
Iteration 124/1000 | Loss: 0.00002038
Iteration 125/1000 | Loss: 0.00002038
Iteration 126/1000 | Loss: 0.00002038
Iteration 127/1000 | Loss: 0.00002037
Iteration 128/1000 | Loss: 0.00002037
Iteration 129/1000 | Loss: 0.00002036
Iteration 130/1000 | Loss: 0.00002036
Iteration 131/1000 | Loss: 0.00002036
Iteration 132/1000 | Loss: 0.00002035
Iteration 133/1000 | Loss: 0.00002035
Iteration 134/1000 | Loss: 0.00002035
Iteration 135/1000 | Loss: 0.00002035
Iteration 136/1000 | Loss: 0.00002035
Iteration 137/1000 | Loss: 0.00002035
Iteration 138/1000 | Loss: 0.00002034
Iteration 139/1000 | Loss: 0.00002034
Iteration 140/1000 | Loss: 0.00002034
Iteration 141/1000 | Loss: 0.00002033
Iteration 142/1000 | Loss: 0.00002033
Iteration 143/1000 | Loss: 0.00002033
Iteration 144/1000 | Loss: 0.00002033
Iteration 145/1000 | Loss: 0.00002032
Iteration 146/1000 | Loss: 0.00002032
Iteration 147/1000 | Loss: 0.00002032
Iteration 148/1000 | Loss: 0.00002031
Iteration 149/1000 | Loss: 0.00002031
Iteration 150/1000 | Loss: 0.00002031
Iteration 151/1000 | Loss: 0.00002031
Iteration 152/1000 | Loss: 0.00002031
Iteration 153/1000 | Loss: 0.00002031
Iteration 154/1000 | Loss: 0.00002030
Iteration 155/1000 | Loss: 0.00002030
Iteration 156/1000 | Loss: 0.00002030
Iteration 157/1000 | Loss: 0.00002030
Iteration 158/1000 | Loss: 0.00002030
Iteration 159/1000 | Loss: 0.00002030
Iteration 160/1000 | Loss: 0.00002030
Iteration 161/1000 | Loss: 0.00002030
Iteration 162/1000 | Loss: 0.00002030
Iteration 163/1000 | Loss: 0.00002030
Iteration 164/1000 | Loss: 0.00002030
Iteration 165/1000 | Loss: 0.00002030
Iteration 166/1000 | Loss: 0.00002030
Iteration 167/1000 | Loss: 0.00002030
Iteration 168/1000 | Loss: 0.00002030
Iteration 169/1000 | Loss: 0.00002030
Iteration 170/1000 | Loss: 0.00002030
Iteration 171/1000 | Loss: 0.00002029
Iteration 172/1000 | Loss: 0.00002029
Iteration 173/1000 | Loss: 0.00002029
Iteration 174/1000 | Loss: 0.00002029
Iteration 175/1000 | Loss: 0.00002029
Iteration 176/1000 | Loss: 0.00002029
Iteration 177/1000 | Loss: 0.00002029
Iteration 178/1000 | Loss: 0.00002029
Iteration 179/1000 | Loss: 0.00002029
Iteration 180/1000 | Loss: 0.00002029
Iteration 181/1000 | Loss: 0.00002029
Iteration 182/1000 | Loss: 0.00002029
Iteration 183/1000 | Loss: 0.00002029
Iteration 184/1000 | Loss: 0.00002029
Iteration 185/1000 | Loss: 0.00002029
Iteration 186/1000 | Loss: 0.00002029
Iteration 187/1000 | Loss: 0.00002029
Iteration 188/1000 | Loss: 0.00002029
Iteration 189/1000 | Loss: 0.00002029
Iteration 190/1000 | Loss: 0.00002029
Iteration 191/1000 | Loss: 0.00002029
Iteration 192/1000 | Loss: 0.00002029
Iteration 193/1000 | Loss: 0.00002029
Iteration 194/1000 | Loss: 0.00002029
Iteration 195/1000 | Loss: 0.00002029
Iteration 196/1000 | Loss: 0.00002029
Iteration 197/1000 | Loss: 0.00002029
Iteration 198/1000 | Loss: 0.00002029
Iteration 199/1000 | Loss: 0.00002029
Iteration 200/1000 | Loss: 0.00002029
Iteration 201/1000 | Loss: 0.00002029
Iteration 202/1000 | Loss: 0.00002029
Iteration 203/1000 | Loss: 0.00002029
Iteration 204/1000 | Loss: 0.00002029
Iteration 205/1000 | Loss: 0.00002029
Iteration 206/1000 | Loss: 0.00002029
Iteration 207/1000 | Loss: 0.00002029
Iteration 208/1000 | Loss: 0.00002029
Iteration 209/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.029017014137935e-05, 2.029017014137935e-05, 2.029017014137935e-05, 2.029017014137935e-05, 2.029017014137935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.029017014137935e-05

Optimization complete. Final v2v error: 3.686546802520752 mm

Highest mean error: 4.411361217498779 mm for frame 81

Lowest mean error: 3.176976203918457 mm for frame 0

Saving results

Total time: 41.747870445251465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752267
Iteration 2/25 | Loss: 0.00137023
Iteration 3/25 | Loss: 0.00120222
Iteration 4/25 | Loss: 0.00116483
Iteration 5/25 | Loss: 0.00115527
Iteration 6/25 | Loss: 0.00114976
Iteration 7/25 | Loss: 0.00116272
Iteration 8/25 | Loss: 0.00114207
Iteration 9/25 | Loss: 0.00114058
Iteration 10/25 | Loss: 0.00113561
Iteration 11/25 | Loss: 0.00113919
Iteration 12/25 | Loss: 0.00113504
Iteration 13/25 | Loss: 0.00113493
Iteration 14/25 | Loss: 0.00113488
Iteration 15/25 | Loss: 0.00113488
Iteration 16/25 | Loss: 0.00113487
Iteration 17/25 | Loss: 0.00113487
Iteration 18/25 | Loss: 0.00113487
Iteration 19/25 | Loss: 0.00113487
Iteration 20/25 | Loss: 0.00113487
Iteration 21/25 | Loss: 0.00113487
Iteration 22/25 | Loss: 0.00113487
Iteration 23/25 | Loss: 0.00113487
Iteration 24/25 | Loss: 0.00113486
Iteration 25/25 | Loss: 0.00113486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.96265793
Iteration 2/25 | Loss: 0.00096109
Iteration 3/25 | Loss: 0.00096102
Iteration 4/25 | Loss: 0.00096102
Iteration 5/25 | Loss: 0.00096102
Iteration 6/25 | Loss: 0.00096102
Iteration 7/25 | Loss: 0.00096102
Iteration 8/25 | Loss: 0.00096102
Iteration 9/25 | Loss: 0.00096102
Iteration 10/25 | Loss: 0.00096102
Iteration 11/25 | Loss: 0.00096102
Iteration 12/25 | Loss: 0.00096102
Iteration 13/25 | Loss: 0.00096102
Iteration 14/25 | Loss: 0.00096102
Iteration 15/25 | Loss: 0.00096102
Iteration 16/25 | Loss: 0.00096102
Iteration 17/25 | Loss: 0.00096102
Iteration 18/25 | Loss: 0.00096102
Iteration 19/25 | Loss: 0.00096102
Iteration 20/25 | Loss: 0.00096102
Iteration 21/25 | Loss: 0.00096102
Iteration 22/25 | Loss: 0.00096102
Iteration 23/25 | Loss: 0.00096102
Iteration 24/25 | Loss: 0.00096102
Iteration 25/25 | Loss: 0.00096102

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096102
Iteration 2/1000 | Loss: 0.00003135
Iteration 3/1000 | Loss: 0.00002158
Iteration 4/1000 | Loss: 0.00001923
Iteration 5/1000 | Loss: 0.00004614
Iteration 6/1000 | Loss: 0.00003175
Iteration 7/1000 | Loss: 0.00001802
Iteration 8/1000 | Loss: 0.00001678
Iteration 9/1000 | Loss: 0.00001641
Iteration 10/1000 | Loss: 0.00001604
Iteration 11/1000 | Loss: 0.00003814
Iteration 12/1000 | Loss: 0.00001566
Iteration 13/1000 | Loss: 0.00001563
Iteration 14/1000 | Loss: 0.00003065
Iteration 15/1000 | Loss: 0.00001550
Iteration 16/1000 | Loss: 0.00001549
Iteration 17/1000 | Loss: 0.00001548
Iteration 18/1000 | Loss: 0.00001531
Iteration 19/1000 | Loss: 0.00001528
Iteration 20/1000 | Loss: 0.00001526
Iteration 21/1000 | Loss: 0.00001526
Iteration 22/1000 | Loss: 0.00001525
Iteration 23/1000 | Loss: 0.00001524
Iteration 24/1000 | Loss: 0.00001523
Iteration 25/1000 | Loss: 0.00001522
Iteration 26/1000 | Loss: 0.00001521
Iteration 27/1000 | Loss: 0.00004262
Iteration 28/1000 | Loss: 0.00001688
Iteration 29/1000 | Loss: 0.00001568
Iteration 30/1000 | Loss: 0.00001510
Iteration 31/1000 | Loss: 0.00001507
Iteration 32/1000 | Loss: 0.00001506
Iteration 33/1000 | Loss: 0.00001505
Iteration 34/1000 | Loss: 0.00001505
Iteration 35/1000 | Loss: 0.00001504
Iteration 36/1000 | Loss: 0.00001504
Iteration 37/1000 | Loss: 0.00001504
Iteration 38/1000 | Loss: 0.00001504
Iteration 39/1000 | Loss: 0.00001504
Iteration 40/1000 | Loss: 0.00001504
Iteration 41/1000 | Loss: 0.00001504
Iteration 42/1000 | Loss: 0.00001503
Iteration 43/1000 | Loss: 0.00001503
Iteration 44/1000 | Loss: 0.00001503
Iteration 45/1000 | Loss: 0.00001503
Iteration 46/1000 | Loss: 0.00001503
Iteration 47/1000 | Loss: 0.00001503
Iteration 48/1000 | Loss: 0.00001502
Iteration 49/1000 | Loss: 0.00001502
Iteration 50/1000 | Loss: 0.00001502
Iteration 51/1000 | Loss: 0.00001502
Iteration 52/1000 | Loss: 0.00001502
Iteration 53/1000 | Loss: 0.00001502
Iteration 54/1000 | Loss: 0.00001501
Iteration 55/1000 | Loss: 0.00001501
Iteration 56/1000 | Loss: 0.00005250
Iteration 57/1000 | Loss: 0.00002042
Iteration 58/1000 | Loss: 0.00001501
Iteration 59/1000 | Loss: 0.00005035
Iteration 60/1000 | Loss: 0.00001813
Iteration 61/1000 | Loss: 0.00001678
Iteration 62/1000 | Loss: 0.00001498
Iteration 63/1000 | Loss: 0.00001496
Iteration 64/1000 | Loss: 0.00001496
Iteration 65/1000 | Loss: 0.00001496
Iteration 66/1000 | Loss: 0.00001495
Iteration 67/1000 | Loss: 0.00001495
Iteration 68/1000 | Loss: 0.00001495
Iteration 69/1000 | Loss: 0.00001494
Iteration 70/1000 | Loss: 0.00001494
Iteration 71/1000 | Loss: 0.00001494
Iteration 72/1000 | Loss: 0.00001493
Iteration 73/1000 | Loss: 0.00001493
Iteration 74/1000 | Loss: 0.00001493
Iteration 75/1000 | Loss: 0.00001493
Iteration 76/1000 | Loss: 0.00001493
Iteration 77/1000 | Loss: 0.00001493
Iteration 78/1000 | Loss: 0.00001493
Iteration 79/1000 | Loss: 0.00001493
Iteration 80/1000 | Loss: 0.00001493
Iteration 81/1000 | Loss: 0.00001492
Iteration 82/1000 | Loss: 0.00001492
Iteration 83/1000 | Loss: 0.00001492
Iteration 84/1000 | Loss: 0.00001492
Iteration 85/1000 | Loss: 0.00001492
Iteration 86/1000 | Loss: 0.00001492
Iteration 87/1000 | Loss: 0.00001491
Iteration 88/1000 | Loss: 0.00001491
Iteration 89/1000 | Loss: 0.00001491
Iteration 90/1000 | Loss: 0.00001491
Iteration 91/1000 | Loss: 0.00001491
Iteration 92/1000 | Loss: 0.00001491
Iteration 93/1000 | Loss: 0.00001490
Iteration 94/1000 | Loss: 0.00001490
Iteration 95/1000 | Loss: 0.00001490
Iteration 96/1000 | Loss: 0.00001490
Iteration 97/1000 | Loss: 0.00001490
Iteration 98/1000 | Loss: 0.00001490
Iteration 99/1000 | Loss: 0.00001490
Iteration 100/1000 | Loss: 0.00001490
Iteration 101/1000 | Loss: 0.00001490
Iteration 102/1000 | Loss: 0.00001490
Iteration 103/1000 | Loss: 0.00001489
Iteration 104/1000 | Loss: 0.00001489
Iteration 105/1000 | Loss: 0.00001489
Iteration 106/1000 | Loss: 0.00001489
Iteration 107/1000 | Loss: 0.00001489
Iteration 108/1000 | Loss: 0.00001489
Iteration 109/1000 | Loss: 0.00001489
Iteration 110/1000 | Loss: 0.00001489
Iteration 111/1000 | Loss: 0.00001489
Iteration 112/1000 | Loss: 0.00001489
Iteration 113/1000 | Loss: 0.00001489
Iteration 114/1000 | Loss: 0.00001489
Iteration 115/1000 | Loss: 0.00001489
Iteration 116/1000 | Loss: 0.00001489
Iteration 117/1000 | Loss: 0.00001488
Iteration 118/1000 | Loss: 0.00001488
Iteration 119/1000 | Loss: 0.00001488
Iteration 120/1000 | Loss: 0.00001487
Iteration 121/1000 | Loss: 0.00001487
Iteration 122/1000 | Loss: 0.00001487
Iteration 123/1000 | Loss: 0.00001486
Iteration 124/1000 | Loss: 0.00001486
Iteration 125/1000 | Loss: 0.00001486
Iteration 126/1000 | Loss: 0.00001485
Iteration 127/1000 | Loss: 0.00001484
Iteration 128/1000 | Loss: 0.00001484
Iteration 129/1000 | Loss: 0.00001484
Iteration 130/1000 | Loss: 0.00001484
Iteration 131/1000 | Loss: 0.00001484
Iteration 132/1000 | Loss: 0.00001484
Iteration 133/1000 | Loss: 0.00001484
Iteration 134/1000 | Loss: 0.00001483
Iteration 135/1000 | Loss: 0.00001483
Iteration 136/1000 | Loss: 0.00001483
Iteration 137/1000 | Loss: 0.00001483
Iteration 138/1000 | Loss: 0.00001483
Iteration 139/1000 | Loss: 0.00001483
Iteration 140/1000 | Loss: 0.00001483
Iteration 141/1000 | Loss: 0.00001483
Iteration 142/1000 | Loss: 0.00001483
Iteration 143/1000 | Loss: 0.00001483
Iteration 144/1000 | Loss: 0.00001482
Iteration 145/1000 | Loss: 0.00001482
Iteration 146/1000 | Loss: 0.00001482
Iteration 147/1000 | Loss: 0.00001481
Iteration 148/1000 | Loss: 0.00001481
Iteration 149/1000 | Loss: 0.00001481
Iteration 150/1000 | Loss: 0.00001481
Iteration 151/1000 | Loss: 0.00001481
Iteration 152/1000 | Loss: 0.00001480
Iteration 153/1000 | Loss: 0.00001480
Iteration 154/1000 | Loss: 0.00001480
Iteration 155/1000 | Loss: 0.00001480
Iteration 156/1000 | Loss: 0.00001480
Iteration 157/1000 | Loss: 0.00001480
Iteration 158/1000 | Loss: 0.00001480
Iteration 159/1000 | Loss: 0.00001480
Iteration 160/1000 | Loss: 0.00001480
Iteration 161/1000 | Loss: 0.00001480
Iteration 162/1000 | Loss: 0.00001480
Iteration 163/1000 | Loss: 0.00001480
Iteration 164/1000 | Loss: 0.00001480
Iteration 165/1000 | Loss: 0.00001480
Iteration 166/1000 | Loss: 0.00001480
Iteration 167/1000 | Loss: 0.00001480
Iteration 168/1000 | Loss: 0.00001479
Iteration 169/1000 | Loss: 0.00001479
Iteration 170/1000 | Loss: 0.00001479
Iteration 171/1000 | Loss: 0.00001479
Iteration 172/1000 | Loss: 0.00001479
Iteration 173/1000 | Loss: 0.00001479
Iteration 174/1000 | Loss: 0.00001479
Iteration 175/1000 | Loss: 0.00001479
Iteration 176/1000 | Loss: 0.00001479
Iteration 177/1000 | Loss: 0.00001479
Iteration 178/1000 | Loss: 0.00001479
Iteration 179/1000 | Loss: 0.00001479
Iteration 180/1000 | Loss: 0.00001479
Iteration 181/1000 | Loss: 0.00001479
Iteration 182/1000 | Loss: 0.00001479
Iteration 183/1000 | Loss: 0.00001479
Iteration 184/1000 | Loss: 0.00001478
Iteration 185/1000 | Loss: 0.00001478
Iteration 186/1000 | Loss: 0.00001478
Iteration 187/1000 | Loss: 0.00001478
Iteration 188/1000 | Loss: 0.00001478
Iteration 189/1000 | Loss: 0.00001478
Iteration 190/1000 | Loss: 0.00001478
Iteration 191/1000 | Loss: 0.00001478
Iteration 192/1000 | Loss: 0.00001478
Iteration 193/1000 | Loss: 0.00001478
Iteration 194/1000 | Loss: 0.00001478
Iteration 195/1000 | Loss: 0.00001478
Iteration 196/1000 | Loss: 0.00001478
Iteration 197/1000 | Loss: 0.00001478
Iteration 198/1000 | Loss: 0.00001478
Iteration 199/1000 | Loss: 0.00001478
Iteration 200/1000 | Loss: 0.00001478
Iteration 201/1000 | Loss: 0.00001478
Iteration 202/1000 | Loss: 0.00001477
Iteration 203/1000 | Loss: 0.00001477
Iteration 204/1000 | Loss: 0.00001477
Iteration 205/1000 | Loss: 0.00001477
Iteration 206/1000 | Loss: 0.00001477
Iteration 207/1000 | Loss: 0.00001477
Iteration 208/1000 | Loss: 0.00001477
Iteration 209/1000 | Loss: 0.00001477
Iteration 210/1000 | Loss: 0.00001477
Iteration 211/1000 | Loss: 0.00001477
Iteration 212/1000 | Loss: 0.00001477
Iteration 213/1000 | Loss: 0.00001477
Iteration 214/1000 | Loss: 0.00001477
Iteration 215/1000 | Loss: 0.00001477
Iteration 216/1000 | Loss: 0.00001477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.4768936125619803e-05, 1.4768936125619803e-05, 1.4768936125619803e-05, 1.4768936125619803e-05, 1.4768936125619803e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4768936125619803e-05

Optimization complete. Final v2v error: 3.1905593872070312 mm

Highest mean error: 4.095396041870117 mm for frame 138

Lowest mean error: 2.6660547256469727 mm for frame 191

Saving results

Total time: 80.97507905960083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037724
Iteration 2/25 | Loss: 0.00176494
Iteration 3/25 | Loss: 0.00145771
Iteration 4/25 | Loss: 0.00137412
Iteration 5/25 | Loss: 0.00140549
Iteration 6/25 | Loss: 0.00137129
Iteration 7/25 | Loss: 0.00133340
Iteration 8/25 | Loss: 0.00131876
Iteration 9/25 | Loss: 0.00129934
Iteration 10/25 | Loss: 0.00128500
Iteration 11/25 | Loss: 0.00127461
Iteration 12/25 | Loss: 0.00126635
Iteration 13/25 | Loss: 0.00126175
Iteration 14/25 | Loss: 0.00125492
Iteration 15/25 | Loss: 0.00125602
Iteration 16/25 | Loss: 0.00125162
Iteration 17/25 | Loss: 0.00125833
Iteration 18/25 | Loss: 0.00125593
Iteration 19/25 | Loss: 0.00125690
Iteration 20/25 | Loss: 0.00125458
Iteration 21/25 | Loss: 0.00125508
Iteration 22/25 | Loss: 0.00125440
Iteration 23/25 | Loss: 0.00125073
Iteration 24/25 | Loss: 0.00124639
Iteration 25/25 | Loss: 0.00125091

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63322461
Iteration 2/25 | Loss: 0.00150781
Iteration 3/25 | Loss: 0.00139391
Iteration 4/25 | Loss: 0.00139391
Iteration 5/25 | Loss: 0.00139390
Iteration 6/25 | Loss: 0.00139390
Iteration 7/25 | Loss: 0.00139390
Iteration 8/25 | Loss: 0.00139390
Iteration 9/25 | Loss: 0.00139390
Iteration 10/25 | Loss: 0.00139390
Iteration 11/25 | Loss: 0.00139390
Iteration 12/25 | Loss: 0.00139390
Iteration 13/25 | Loss: 0.00139390
Iteration 14/25 | Loss: 0.00139390
Iteration 15/25 | Loss: 0.00139390
Iteration 16/25 | Loss: 0.00139390
Iteration 17/25 | Loss: 0.00139390
Iteration 18/25 | Loss: 0.00139390
Iteration 19/25 | Loss: 0.00139390
Iteration 20/25 | Loss: 0.00139390
Iteration 21/25 | Loss: 0.00139390
Iteration 22/25 | Loss: 0.00139390
Iteration 23/25 | Loss: 0.00139390
Iteration 24/25 | Loss: 0.00139390
Iteration 25/25 | Loss: 0.00139390

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139390
Iteration 2/1000 | Loss: 0.00073480
Iteration 3/1000 | Loss: 0.00054879
Iteration 4/1000 | Loss: 0.00035755
Iteration 5/1000 | Loss: 0.00041032
Iteration 6/1000 | Loss: 0.00030678
Iteration 7/1000 | Loss: 0.00046003
Iteration 8/1000 | Loss: 0.00054083
Iteration 9/1000 | Loss: 0.00049743
Iteration 10/1000 | Loss: 0.00037684
Iteration 11/1000 | Loss: 0.00045653
Iteration 12/1000 | Loss: 0.00060348
Iteration 13/1000 | Loss: 0.00029515
Iteration 14/1000 | Loss: 0.00050506
Iteration 15/1000 | Loss: 0.00044874
Iteration 16/1000 | Loss: 0.00063857
Iteration 17/1000 | Loss: 0.00064218
Iteration 18/1000 | Loss: 0.00032448
Iteration 19/1000 | Loss: 0.00034844
Iteration 20/1000 | Loss: 0.00054941
Iteration 21/1000 | Loss: 0.00019460
Iteration 22/1000 | Loss: 0.00028674
Iteration 23/1000 | Loss: 0.00015885
Iteration 24/1000 | Loss: 0.00039451
Iteration 25/1000 | Loss: 0.00068528
Iteration 26/1000 | Loss: 0.00067623
Iteration 27/1000 | Loss: 0.00042711
Iteration 28/1000 | Loss: 0.00057125
Iteration 29/1000 | Loss: 0.00067756
Iteration 30/1000 | Loss: 0.00036019
Iteration 31/1000 | Loss: 0.00062811
Iteration 32/1000 | Loss: 0.00016840
Iteration 33/1000 | Loss: 0.00011498
Iteration 34/1000 | Loss: 0.00016658
Iteration 35/1000 | Loss: 0.00008887
Iteration 36/1000 | Loss: 0.00005916
Iteration 37/1000 | Loss: 0.00005614
Iteration 38/1000 | Loss: 0.00005821
Iteration 39/1000 | Loss: 0.00006061
Iteration 40/1000 | Loss: 0.00005364
Iteration 41/1000 | Loss: 0.00007126
Iteration 42/1000 | Loss: 0.00006014
Iteration 43/1000 | Loss: 0.00012761
Iteration 44/1000 | Loss: 0.00009224
Iteration 45/1000 | Loss: 0.00011508
Iteration 46/1000 | Loss: 0.00009416
Iteration 47/1000 | Loss: 0.00011360
Iteration 48/1000 | Loss: 0.00003243
Iteration 49/1000 | Loss: 0.00002828
Iteration 50/1000 | Loss: 0.00002656
Iteration 51/1000 | Loss: 0.00002563
Iteration 52/1000 | Loss: 0.00002476
Iteration 53/1000 | Loss: 0.00002403
Iteration 54/1000 | Loss: 0.00014459
Iteration 55/1000 | Loss: 0.00012149
Iteration 56/1000 | Loss: 0.00017381
Iteration 57/1000 | Loss: 0.00010701
Iteration 58/1000 | Loss: 0.00032556
Iteration 59/1000 | Loss: 0.00005907
Iteration 60/1000 | Loss: 0.00002445
Iteration 61/1000 | Loss: 0.00005997
Iteration 62/1000 | Loss: 0.00007125
Iteration 63/1000 | Loss: 0.00002220
Iteration 64/1000 | Loss: 0.00002173
Iteration 65/1000 | Loss: 0.00002091
Iteration 66/1000 | Loss: 0.00002040
Iteration 67/1000 | Loss: 0.00002021
Iteration 68/1000 | Loss: 0.00001989
Iteration 69/1000 | Loss: 0.00001957
Iteration 70/1000 | Loss: 0.00001937
Iteration 71/1000 | Loss: 0.00001931
Iteration 72/1000 | Loss: 0.00001923
Iteration 73/1000 | Loss: 0.00001923
Iteration 74/1000 | Loss: 0.00001923
Iteration 75/1000 | Loss: 0.00001923
Iteration 76/1000 | Loss: 0.00001922
Iteration 77/1000 | Loss: 0.00001920
Iteration 78/1000 | Loss: 0.00001919
Iteration 79/1000 | Loss: 0.00001915
Iteration 80/1000 | Loss: 0.00001911
Iteration 81/1000 | Loss: 0.00001911
Iteration 82/1000 | Loss: 0.00001910
Iteration 83/1000 | Loss: 0.00001909
Iteration 84/1000 | Loss: 0.00001908
Iteration 85/1000 | Loss: 0.00001907
Iteration 86/1000 | Loss: 0.00001904
Iteration 87/1000 | Loss: 0.00001904
Iteration 88/1000 | Loss: 0.00001903
Iteration 89/1000 | Loss: 0.00001902
Iteration 90/1000 | Loss: 0.00001902
Iteration 91/1000 | Loss: 0.00001902
Iteration 92/1000 | Loss: 0.00001901
Iteration 93/1000 | Loss: 0.00001899
Iteration 94/1000 | Loss: 0.00001896
Iteration 95/1000 | Loss: 0.00001894
Iteration 96/1000 | Loss: 0.00001894
Iteration 97/1000 | Loss: 0.00001893
Iteration 98/1000 | Loss: 0.00001888
Iteration 99/1000 | Loss: 0.00014627
Iteration 100/1000 | Loss: 0.00003738
Iteration 101/1000 | Loss: 0.00002658
Iteration 102/1000 | Loss: 0.00002847
Iteration 103/1000 | Loss: 0.00002319
Iteration 104/1000 | Loss: 0.00001997
Iteration 105/1000 | Loss: 0.00002463
Iteration 106/1000 | Loss: 0.00002207
Iteration 107/1000 | Loss: 0.00002296
Iteration 108/1000 | Loss: 0.00001958
Iteration 109/1000 | Loss: 0.00001928
Iteration 110/1000 | Loss: 0.00001912
Iteration 111/1000 | Loss: 0.00001906
Iteration 112/1000 | Loss: 0.00001902
Iteration 113/1000 | Loss: 0.00001900
Iteration 114/1000 | Loss: 0.00001899
Iteration 115/1000 | Loss: 0.00001898
Iteration 116/1000 | Loss: 0.00001896
Iteration 117/1000 | Loss: 0.00001890
Iteration 118/1000 | Loss: 0.00001886
Iteration 119/1000 | Loss: 0.00008918
Iteration 120/1000 | Loss: 0.00012130
Iteration 121/1000 | Loss: 0.00014181
Iteration 122/1000 | Loss: 0.00010478
Iteration 123/1000 | Loss: 0.00011506
Iteration 124/1000 | Loss: 0.00014246
Iteration 125/1000 | Loss: 0.00012741
Iteration 126/1000 | Loss: 0.00012002
Iteration 127/1000 | Loss: 0.00002345
Iteration 128/1000 | Loss: 0.00014962
Iteration 129/1000 | Loss: 0.00005800
Iteration 130/1000 | Loss: 0.00011074
Iteration 131/1000 | Loss: 0.00002398
Iteration 132/1000 | Loss: 0.00002083
Iteration 133/1000 | Loss: 0.00002006
Iteration 134/1000 | Loss: 0.00001929
Iteration 135/1000 | Loss: 0.00001890
Iteration 136/1000 | Loss: 0.00001863
Iteration 137/1000 | Loss: 0.00001851
Iteration 138/1000 | Loss: 0.00001847
Iteration 139/1000 | Loss: 0.00001845
Iteration 140/1000 | Loss: 0.00001845
Iteration 141/1000 | Loss: 0.00001845
Iteration 142/1000 | Loss: 0.00001844
Iteration 143/1000 | Loss: 0.00001844
Iteration 144/1000 | Loss: 0.00001844
Iteration 145/1000 | Loss: 0.00001844
Iteration 146/1000 | Loss: 0.00001843
Iteration 147/1000 | Loss: 0.00001842
Iteration 148/1000 | Loss: 0.00001837
Iteration 149/1000 | Loss: 0.00001835
Iteration 150/1000 | Loss: 0.00001835
Iteration 151/1000 | Loss: 0.00001834
Iteration 152/1000 | Loss: 0.00001834
Iteration 153/1000 | Loss: 0.00001834
Iteration 154/1000 | Loss: 0.00001834
Iteration 155/1000 | Loss: 0.00001834
Iteration 156/1000 | Loss: 0.00001834
Iteration 157/1000 | Loss: 0.00001834
Iteration 158/1000 | Loss: 0.00001834
Iteration 159/1000 | Loss: 0.00001833
Iteration 160/1000 | Loss: 0.00001833
Iteration 161/1000 | Loss: 0.00001833
Iteration 162/1000 | Loss: 0.00001833
Iteration 163/1000 | Loss: 0.00001833
Iteration 164/1000 | Loss: 0.00001833
Iteration 165/1000 | Loss: 0.00001833
Iteration 166/1000 | Loss: 0.00001832
Iteration 167/1000 | Loss: 0.00001832
Iteration 168/1000 | Loss: 0.00001832
Iteration 169/1000 | Loss: 0.00001831
Iteration 170/1000 | Loss: 0.00001829
Iteration 171/1000 | Loss: 0.00001829
Iteration 172/1000 | Loss: 0.00001828
Iteration 173/1000 | Loss: 0.00001828
Iteration 174/1000 | Loss: 0.00001828
Iteration 175/1000 | Loss: 0.00001827
Iteration 176/1000 | Loss: 0.00001827
Iteration 177/1000 | Loss: 0.00001827
Iteration 178/1000 | Loss: 0.00001826
Iteration 179/1000 | Loss: 0.00001826
Iteration 180/1000 | Loss: 0.00001825
Iteration 181/1000 | Loss: 0.00001825
Iteration 182/1000 | Loss: 0.00001825
Iteration 183/1000 | Loss: 0.00001825
Iteration 184/1000 | Loss: 0.00001825
Iteration 185/1000 | Loss: 0.00001825
Iteration 186/1000 | Loss: 0.00001825
Iteration 187/1000 | Loss: 0.00001825
Iteration 188/1000 | Loss: 0.00001825
Iteration 189/1000 | Loss: 0.00001824
Iteration 190/1000 | Loss: 0.00001824
Iteration 191/1000 | Loss: 0.00001824
Iteration 192/1000 | Loss: 0.00001824
Iteration 193/1000 | Loss: 0.00001824
Iteration 194/1000 | Loss: 0.00001824
Iteration 195/1000 | Loss: 0.00001824
Iteration 196/1000 | Loss: 0.00001824
Iteration 197/1000 | Loss: 0.00001824
Iteration 198/1000 | Loss: 0.00001823
Iteration 199/1000 | Loss: 0.00001823
Iteration 200/1000 | Loss: 0.00001823
Iteration 201/1000 | Loss: 0.00001822
Iteration 202/1000 | Loss: 0.00001821
Iteration 203/1000 | Loss: 0.00001821
Iteration 204/1000 | Loss: 0.00001821
Iteration 205/1000 | Loss: 0.00001821
Iteration 206/1000 | Loss: 0.00001821
Iteration 207/1000 | Loss: 0.00001821
Iteration 208/1000 | Loss: 0.00001820
Iteration 209/1000 | Loss: 0.00001820
Iteration 210/1000 | Loss: 0.00001820
Iteration 211/1000 | Loss: 0.00001820
Iteration 212/1000 | Loss: 0.00002821
Iteration 213/1000 | Loss: 0.00001908
Iteration 214/1000 | Loss: 0.00001867
Iteration 215/1000 | Loss: 0.00001834
Iteration 216/1000 | Loss: 0.00001813
Iteration 217/1000 | Loss: 0.00001812
Iteration 218/1000 | Loss: 0.00001811
Iteration 219/1000 | Loss: 0.00001811
Iteration 220/1000 | Loss: 0.00001811
Iteration 221/1000 | Loss: 0.00001811
Iteration 222/1000 | Loss: 0.00001810
Iteration 223/1000 | Loss: 0.00001810
Iteration 224/1000 | Loss: 0.00001810
Iteration 225/1000 | Loss: 0.00001809
Iteration 226/1000 | Loss: 0.00001809
Iteration 227/1000 | Loss: 0.00001809
Iteration 228/1000 | Loss: 0.00001808
Iteration 229/1000 | Loss: 0.00001808
Iteration 230/1000 | Loss: 0.00001808
Iteration 231/1000 | Loss: 0.00001808
Iteration 232/1000 | Loss: 0.00001807
Iteration 233/1000 | Loss: 0.00001807
Iteration 234/1000 | Loss: 0.00001806
Iteration 235/1000 | Loss: 0.00001806
Iteration 236/1000 | Loss: 0.00001806
Iteration 237/1000 | Loss: 0.00001806
Iteration 238/1000 | Loss: 0.00001806
Iteration 239/1000 | Loss: 0.00001806
Iteration 240/1000 | Loss: 0.00001806
Iteration 241/1000 | Loss: 0.00001805
Iteration 242/1000 | Loss: 0.00001805
Iteration 243/1000 | Loss: 0.00001805
Iteration 244/1000 | Loss: 0.00001805
Iteration 245/1000 | Loss: 0.00001804
Iteration 246/1000 | Loss: 0.00001804
Iteration 247/1000 | Loss: 0.00001804
Iteration 248/1000 | Loss: 0.00001803
Iteration 249/1000 | Loss: 0.00001803
Iteration 250/1000 | Loss: 0.00001803
Iteration 251/1000 | Loss: 0.00001803
Iteration 252/1000 | Loss: 0.00001803
Iteration 253/1000 | Loss: 0.00001803
Iteration 254/1000 | Loss: 0.00001803
Iteration 255/1000 | Loss: 0.00001803
Iteration 256/1000 | Loss: 0.00001803
Iteration 257/1000 | Loss: 0.00001803
Iteration 258/1000 | Loss: 0.00001802
Iteration 259/1000 | Loss: 0.00001802
Iteration 260/1000 | Loss: 0.00001802
Iteration 261/1000 | Loss: 0.00001802
Iteration 262/1000 | Loss: 0.00001802
Iteration 263/1000 | Loss: 0.00001802
Iteration 264/1000 | Loss: 0.00001802
Iteration 265/1000 | Loss: 0.00001802
Iteration 266/1000 | Loss: 0.00001802
Iteration 267/1000 | Loss: 0.00001802
Iteration 268/1000 | Loss: 0.00001802
Iteration 269/1000 | Loss: 0.00001802
Iteration 270/1000 | Loss: 0.00001802
Iteration 271/1000 | Loss: 0.00001802
Iteration 272/1000 | Loss: 0.00001802
Iteration 273/1000 | Loss: 0.00001802
Iteration 274/1000 | Loss: 0.00001802
Iteration 275/1000 | Loss: 0.00001802
Iteration 276/1000 | Loss: 0.00001802
Iteration 277/1000 | Loss: 0.00001802
Iteration 278/1000 | Loss: 0.00001802
Iteration 279/1000 | Loss: 0.00001802
Iteration 280/1000 | Loss: 0.00001802
Iteration 281/1000 | Loss: 0.00001802
Iteration 282/1000 | Loss: 0.00001801
Iteration 283/1000 | Loss: 0.00001801
Iteration 284/1000 | Loss: 0.00001801
Iteration 285/1000 | Loss: 0.00001801
Iteration 286/1000 | Loss: 0.00001801
Iteration 287/1000 | Loss: 0.00001801
Iteration 288/1000 | Loss: 0.00001801
Iteration 289/1000 | Loss: 0.00001801
Iteration 290/1000 | Loss: 0.00001801
Iteration 291/1000 | Loss: 0.00001801
Iteration 292/1000 | Loss: 0.00001801
Iteration 293/1000 | Loss: 0.00001801
Iteration 294/1000 | Loss: 0.00001801
Iteration 295/1000 | Loss: 0.00001801
Iteration 296/1000 | Loss: 0.00001801
Iteration 297/1000 | Loss: 0.00001801
Iteration 298/1000 | Loss: 0.00001801
Iteration 299/1000 | Loss: 0.00001801
Iteration 300/1000 | Loss: 0.00001801
Iteration 301/1000 | Loss: 0.00001801
Iteration 302/1000 | Loss: 0.00001801
Iteration 303/1000 | Loss: 0.00001801
Iteration 304/1000 | Loss: 0.00001801
Iteration 305/1000 | Loss: 0.00001801
Iteration 306/1000 | Loss: 0.00001801
Iteration 307/1000 | Loss: 0.00001801
Iteration 308/1000 | Loss: 0.00001801
Iteration 309/1000 | Loss: 0.00001801
Iteration 310/1000 | Loss: 0.00001801
Iteration 311/1000 | Loss: 0.00001801
Iteration 312/1000 | Loss: 0.00001801
Iteration 313/1000 | Loss: 0.00001801
Iteration 314/1000 | Loss: 0.00001801
Iteration 315/1000 | Loss: 0.00001801
Iteration 316/1000 | Loss: 0.00001801
Iteration 317/1000 | Loss: 0.00001801
Iteration 318/1000 | Loss: 0.00001801
Iteration 319/1000 | Loss: 0.00001801
Iteration 320/1000 | Loss: 0.00001801
Iteration 321/1000 | Loss: 0.00001801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 321. Stopping optimization.
Last 5 losses: [1.8006119717028923e-05, 1.8006119717028923e-05, 1.8006119717028923e-05, 1.8006119717028923e-05, 1.8006119717028923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8006119717028923e-05

Optimization complete. Final v2v error: 3.614060878753662 mm

Highest mean error: 5.970485687255859 mm for frame 3

Lowest mean error: 3.333409547805786 mm for frame 62

Saving results

Total time: 243.7793185710907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483153
Iteration 2/25 | Loss: 0.00121366
Iteration 3/25 | Loss: 0.00113419
Iteration 4/25 | Loss: 0.00112036
Iteration 5/25 | Loss: 0.00111644
Iteration 6/25 | Loss: 0.00111530
Iteration 7/25 | Loss: 0.00111483
Iteration 8/25 | Loss: 0.00111483
Iteration 9/25 | Loss: 0.00111483
Iteration 10/25 | Loss: 0.00111483
Iteration 11/25 | Loss: 0.00111483
Iteration 12/25 | Loss: 0.00111483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011148251360282302, 0.0011148251360282302, 0.0011148251360282302, 0.0011148251360282302, 0.0011148251360282302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011148251360282302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.27455759
Iteration 2/25 | Loss: 0.00096394
Iteration 3/25 | Loss: 0.00096393
Iteration 4/25 | Loss: 0.00096393
Iteration 5/25 | Loss: 0.00096393
Iteration 6/25 | Loss: 0.00096393
Iteration 7/25 | Loss: 0.00096393
Iteration 8/25 | Loss: 0.00096393
Iteration 9/25 | Loss: 0.00096393
Iteration 10/25 | Loss: 0.00096393
Iteration 11/25 | Loss: 0.00096393
Iteration 12/25 | Loss: 0.00096393
Iteration 13/25 | Loss: 0.00096393
Iteration 14/25 | Loss: 0.00096393
Iteration 15/25 | Loss: 0.00096393
Iteration 16/25 | Loss: 0.00096393
Iteration 17/25 | Loss: 0.00096393
Iteration 18/25 | Loss: 0.00096393
Iteration 19/25 | Loss: 0.00096393
Iteration 20/25 | Loss: 0.00096393
Iteration 21/25 | Loss: 0.00096393
Iteration 22/25 | Loss: 0.00096393
Iteration 23/25 | Loss: 0.00096393
Iteration 24/25 | Loss: 0.00096393
Iteration 25/25 | Loss: 0.00096393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096393
Iteration 2/1000 | Loss: 0.00003186
Iteration 3/1000 | Loss: 0.00002221
Iteration 4/1000 | Loss: 0.00002042
Iteration 5/1000 | Loss: 0.00001986
Iteration 6/1000 | Loss: 0.00001929
Iteration 7/1000 | Loss: 0.00001897
Iteration 8/1000 | Loss: 0.00001862
Iteration 9/1000 | Loss: 0.00001840
Iteration 10/1000 | Loss: 0.00001830
Iteration 11/1000 | Loss: 0.00001814
Iteration 12/1000 | Loss: 0.00001799
Iteration 13/1000 | Loss: 0.00001798
Iteration 14/1000 | Loss: 0.00001788
Iteration 15/1000 | Loss: 0.00001786
Iteration 16/1000 | Loss: 0.00001785
Iteration 17/1000 | Loss: 0.00001782
Iteration 18/1000 | Loss: 0.00001779
Iteration 19/1000 | Loss: 0.00001769
Iteration 20/1000 | Loss: 0.00001764
Iteration 21/1000 | Loss: 0.00001754
Iteration 22/1000 | Loss: 0.00001753
Iteration 23/1000 | Loss: 0.00001751
Iteration 24/1000 | Loss: 0.00001750
Iteration 25/1000 | Loss: 0.00001750
Iteration 26/1000 | Loss: 0.00001749
Iteration 27/1000 | Loss: 0.00001749
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001748
Iteration 30/1000 | Loss: 0.00001747
Iteration 31/1000 | Loss: 0.00001746
Iteration 32/1000 | Loss: 0.00001742
Iteration 33/1000 | Loss: 0.00001737
Iteration 34/1000 | Loss: 0.00001737
Iteration 35/1000 | Loss: 0.00001736
Iteration 36/1000 | Loss: 0.00001736
Iteration 37/1000 | Loss: 0.00001734
Iteration 38/1000 | Loss: 0.00001733
Iteration 39/1000 | Loss: 0.00001733
Iteration 40/1000 | Loss: 0.00001732
Iteration 41/1000 | Loss: 0.00001732
Iteration 42/1000 | Loss: 0.00001732
Iteration 43/1000 | Loss: 0.00001731
Iteration 44/1000 | Loss: 0.00001730
Iteration 45/1000 | Loss: 0.00001730
Iteration 46/1000 | Loss: 0.00001730
Iteration 47/1000 | Loss: 0.00001730
Iteration 48/1000 | Loss: 0.00001729
Iteration 49/1000 | Loss: 0.00001729
Iteration 50/1000 | Loss: 0.00001729
Iteration 51/1000 | Loss: 0.00001729
Iteration 52/1000 | Loss: 0.00001729
Iteration 53/1000 | Loss: 0.00001729
Iteration 54/1000 | Loss: 0.00001728
Iteration 55/1000 | Loss: 0.00001728
Iteration 56/1000 | Loss: 0.00001727
Iteration 57/1000 | Loss: 0.00001727
Iteration 58/1000 | Loss: 0.00001727
Iteration 59/1000 | Loss: 0.00001727
Iteration 60/1000 | Loss: 0.00001727
Iteration 61/1000 | Loss: 0.00001727
Iteration 62/1000 | Loss: 0.00001726
Iteration 63/1000 | Loss: 0.00001726
Iteration 64/1000 | Loss: 0.00001726
Iteration 65/1000 | Loss: 0.00001726
Iteration 66/1000 | Loss: 0.00001725
Iteration 67/1000 | Loss: 0.00001725
Iteration 68/1000 | Loss: 0.00001725
Iteration 69/1000 | Loss: 0.00001725
Iteration 70/1000 | Loss: 0.00001724
Iteration 71/1000 | Loss: 0.00001724
Iteration 72/1000 | Loss: 0.00001724
Iteration 73/1000 | Loss: 0.00001724
Iteration 74/1000 | Loss: 0.00001724
Iteration 75/1000 | Loss: 0.00001723
Iteration 76/1000 | Loss: 0.00001723
Iteration 77/1000 | Loss: 0.00001723
Iteration 78/1000 | Loss: 0.00001723
Iteration 79/1000 | Loss: 0.00001723
Iteration 80/1000 | Loss: 0.00001723
Iteration 81/1000 | Loss: 0.00001723
Iteration 82/1000 | Loss: 0.00001723
Iteration 83/1000 | Loss: 0.00001723
Iteration 84/1000 | Loss: 0.00001723
Iteration 85/1000 | Loss: 0.00001723
Iteration 86/1000 | Loss: 0.00001722
Iteration 87/1000 | Loss: 0.00001722
Iteration 88/1000 | Loss: 0.00001722
Iteration 89/1000 | Loss: 0.00001722
Iteration 90/1000 | Loss: 0.00001722
Iteration 91/1000 | Loss: 0.00001722
Iteration 92/1000 | Loss: 0.00001721
Iteration 93/1000 | Loss: 0.00001721
Iteration 94/1000 | Loss: 0.00001721
Iteration 95/1000 | Loss: 0.00001721
Iteration 96/1000 | Loss: 0.00001721
Iteration 97/1000 | Loss: 0.00001721
Iteration 98/1000 | Loss: 0.00001721
Iteration 99/1000 | Loss: 0.00001721
Iteration 100/1000 | Loss: 0.00001721
Iteration 101/1000 | Loss: 0.00001720
Iteration 102/1000 | Loss: 0.00001720
Iteration 103/1000 | Loss: 0.00001720
Iteration 104/1000 | Loss: 0.00001719
Iteration 105/1000 | Loss: 0.00001719
Iteration 106/1000 | Loss: 0.00001719
Iteration 107/1000 | Loss: 0.00001719
Iteration 108/1000 | Loss: 0.00001719
Iteration 109/1000 | Loss: 0.00001719
Iteration 110/1000 | Loss: 0.00001719
Iteration 111/1000 | Loss: 0.00001719
Iteration 112/1000 | Loss: 0.00001719
Iteration 113/1000 | Loss: 0.00001719
Iteration 114/1000 | Loss: 0.00001719
Iteration 115/1000 | Loss: 0.00001719
Iteration 116/1000 | Loss: 0.00001718
Iteration 117/1000 | Loss: 0.00001718
Iteration 118/1000 | Loss: 0.00001718
Iteration 119/1000 | Loss: 0.00001718
Iteration 120/1000 | Loss: 0.00001718
Iteration 121/1000 | Loss: 0.00001718
Iteration 122/1000 | Loss: 0.00001718
Iteration 123/1000 | Loss: 0.00001718
Iteration 124/1000 | Loss: 0.00001718
Iteration 125/1000 | Loss: 0.00001718
Iteration 126/1000 | Loss: 0.00001718
Iteration 127/1000 | Loss: 0.00001718
Iteration 128/1000 | Loss: 0.00001718
Iteration 129/1000 | Loss: 0.00001718
Iteration 130/1000 | Loss: 0.00001718
Iteration 131/1000 | Loss: 0.00001718
Iteration 132/1000 | Loss: 0.00001718
Iteration 133/1000 | Loss: 0.00001718
Iteration 134/1000 | Loss: 0.00001718
Iteration 135/1000 | Loss: 0.00001717
Iteration 136/1000 | Loss: 0.00001717
Iteration 137/1000 | Loss: 0.00001717
Iteration 138/1000 | Loss: 0.00001717
Iteration 139/1000 | Loss: 0.00001717
Iteration 140/1000 | Loss: 0.00001717
Iteration 141/1000 | Loss: 0.00001717
Iteration 142/1000 | Loss: 0.00001716
Iteration 143/1000 | Loss: 0.00001716
Iteration 144/1000 | Loss: 0.00001716
Iteration 145/1000 | Loss: 0.00001716
Iteration 146/1000 | Loss: 0.00001716
Iteration 147/1000 | Loss: 0.00001716
Iteration 148/1000 | Loss: 0.00001716
Iteration 149/1000 | Loss: 0.00001716
Iteration 150/1000 | Loss: 0.00001716
Iteration 151/1000 | Loss: 0.00001716
Iteration 152/1000 | Loss: 0.00001716
Iteration 153/1000 | Loss: 0.00001716
Iteration 154/1000 | Loss: 0.00001716
Iteration 155/1000 | Loss: 0.00001716
Iteration 156/1000 | Loss: 0.00001716
Iteration 157/1000 | Loss: 0.00001716
Iteration 158/1000 | Loss: 0.00001715
Iteration 159/1000 | Loss: 0.00001715
Iteration 160/1000 | Loss: 0.00001715
Iteration 161/1000 | Loss: 0.00001715
Iteration 162/1000 | Loss: 0.00001715
Iteration 163/1000 | Loss: 0.00001715
Iteration 164/1000 | Loss: 0.00001715
Iteration 165/1000 | Loss: 0.00001715
Iteration 166/1000 | Loss: 0.00001715
Iteration 167/1000 | Loss: 0.00001715
Iteration 168/1000 | Loss: 0.00001715
Iteration 169/1000 | Loss: 0.00001715
Iteration 170/1000 | Loss: 0.00001715
Iteration 171/1000 | Loss: 0.00001715
Iteration 172/1000 | Loss: 0.00001715
Iteration 173/1000 | Loss: 0.00001715
Iteration 174/1000 | Loss: 0.00001715
Iteration 175/1000 | Loss: 0.00001714
Iteration 176/1000 | Loss: 0.00001714
Iteration 177/1000 | Loss: 0.00001714
Iteration 178/1000 | Loss: 0.00001714
Iteration 179/1000 | Loss: 0.00001714
Iteration 180/1000 | Loss: 0.00001714
Iteration 181/1000 | Loss: 0.00001714
Iteration 182/1000 | Loss: 0.00001714
Iteration 183/1000 | Loss: 0.00001714
Iteration 184/1000 | Loss: 0.00001714
Iteration 185/1000 | Loss: 0.00001714
Iteration 186/1000 | Loss: 0.00001714
Iteration 187/1000 | Loss: 0.00001714
Iteration 188/1000 | Loss: 0.00001714
Iteration 189/1000 | Loss: 0.00001714
Iteration 190/1000 | Loss: 0.00001714
Iteration 191/1000 | Loss: 0.00001714
Iteration 192/1000 | Loss: 0.00001714
Iteration 193/1000 | Loss: 0.00001714
Iteration 194/1000 | Loss: 0.00001714
Iteration 195/1000 | Loss: 0.00001714
Iteration 196/1000 | Loss: 0.00001714
Iteration 197/1000 | Loss: 0.00001714
Iteration 198/1000 | Loss: 0.00001714
Iteration 199/1000 | Loss: 0.00001714
Iteration 200/1000 | Loss: 0.00001714
Iteration 201/1000 | Loss: 0.00001714
Iteration 202/1000 | Loss: 0.00001714
Iteration 203/1000 | Loss: 0.00001714
Iteration 204/1000 | Loss: 0.00001714
Iteration 205/1000 | Loss: 0.00001714
Iteration 206/1000 | Loss: 0.00001714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.7135498637799174e-05, 1.7135498637799174e-05, 1.7135498637799174e-05, 1.7135498637799174e-05, 1.7135498637799174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7135498637799174e-05

Optimization complete. Final v2v error: 3.4777023792266846 mm

Highest mean error: 4.129843235015869 mm for frame 17

Lowest mean error: 2.736396312713623 mm for frame 0

Saving results

Total time: 44.22795057296753
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490828
Iteration 2/25 | Loss: 0.00135316
Iteration 3/25 | Loss: 0.00120465
Iteration 4/25 | Loss: 0.00119641
Iteration 5/25 | Loss: 0.00119605
Iteration 6/25 | Loss: 0.00119605
Iteration 7/25 | Loss: 0.00119605
Iteration 8/25 | Loss: 0.00119605
Iteration 9/25 | Loss: 0.00119605
Iteration 10/25 | Loss: 0.00119605
Iteration 11/25 | Loss: 0.00119605
Iteration 12/25 | Loss: 0.00119605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011960483388975263, 0.0011960483388975263, 0.0011960483388975263, 0.0011960483388975263, 0.0011960483388975263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011960483388975263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35763538
Iteration 2/25 | Loss: 0.00086619
Iteration 3/25 | Loss: 0.00086619
Iteration 4/25 | Loss: 0.00086619
Iteration 5/25 | Loss: 0.00086619
Iteration 6/25 | Loss: 0.00086619
Iteration 7/25 | Loss: 0.00086619
Iteration 8/25 | Loss: 0.00086619
Iteration 9/25 | Loss: 0.00086619
Iteration 10/25 | Loss: 0.00086619
Iteration 11/25 | Loss: 0.00086619
Iteration 12/25 | Loss: 0.00086619
Iteration 13/25 | Loss: 0.00086619
Iteration 14/25 | Loss: 0.00086619
Iteration 15/25 | Loss: 0.00086619
Iteration 16/25 | Loss: 0.00086619
Iteration 17/25 | Loss: 0.00086619
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008661883766762912, 0.0008661883766762912, 0.0008661883766762912, 0.0008661883766762912, 0.0008661883766762912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008661883766762912

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086619
Iteration 2/1000 | Loss: 0.00003297
Iteration 3/1000 | Loss: 0.00002472
Iteration 4/1000 | Loss: 0.00002317
Iteration 5/1000 | Loss: 0.00002239
Iteration 6/1000 | Loss: 0.00002186
Iteration 7/1000 | Loss: 0.00002151
Iteration 8/1000 | Loss: 0.00002129
Iteration 9/1000 | Loss: 0.00002111
Iteration 10/1000 | Loss: 0.00002104
Iteration 11/1000 | Loss: 0.00002089
Iteration 12/1000 | Loss: 0.00002075
Iteration 13/1000 | Loss: 0.00002067
Iteration 14/1000 | Loss: 0.00002060
Iteration 15/1000 | Loss: 0.00002055
Iteration 16/1000 | Loss: 0.00002046
Iteration 17/1000 | Loss: 0.00002046
Iteration 18/1000 | Loss: 0.00002044
Iteration 19/1000 | Loss: 0.00002044
Iteration 20/1000 | Loss: 0.00002043
Iteration 21/1000 | Loss: 0.00002043
Iteration 22/1000 | Loss: 0.00002041
Iteration 23/1000 | Loss: 0.00002032
Iteration 24/1000 | Loss: 0.00002032
Iteration 25/1000 | Loss: 0.00002023
Iteration 26/1000 | Loss: 0.00002021
Iteration 27/1000 | Loss: 0.00002021
Iteration 28/1000 | Loss: 0.00002020
Iteration 29/1000 | Loss: 0.00002020
Iteration 30/1000 | Loss: 0.00002019
Iteration 31/1000 | Loss: 0.00002017
Iteration 32/1000 | Loss: 0.00002016
Iteration 33/1000 | Loss: 0.00002016
Iteration 34/1000 | Loss: 0.00002016
Iteration 35/1000 | Loss: 0.00002016
Iteration 36/1000 | Loss: 0.00002013
Iteration 37/1000 | Loss: 0.00002012
Iteration 38/1000 | Loss: 0.00002012
Iteration 39/1000 | Loss: 0.00002011
Iteration 40/1000 | Loss: 0.00002007
Iteration 41/1000 | Loss: 0.00002007
Iteration 42/1000 | Loss: 0.00002006
Iteration 43/1000 | Loss: 0.00002004
Iteration 44/1000 | Loss: 0.00002003
Iteration 45/1000 | Loss: 0.00002003
Iteration 46/1000 | Loss: 0.00002003
Iteration 47/1000 | Loss: 0.00002003
Iteration 48/1000 | Loss: 0.00002002
Iteration 49/1000 | Loss: 0.00002002
Iteration 50/1000 | Loss: 0.00002002
Iteration 51/1000 | Loss: 0.00002002
Iteration 52/1000 | Loss: 0.00002002
Iteration 53/1000 | Loss: 0.00002002
Iteration 54/1000 | Loss: 0.00002002
Iteration 55/1000 | Loss: 0.00002002
Iteration 56/1000 | Loss: 0.00002001
Iteration 57/1000 | Loss: 0.00002001
Iteration 58/1000 | Loss: 0.00002000
Iteration 59/1000 | Loss: 0.00002000
Iteration 60/1000 | Loss: 0.00002000
Iteration 61/1000 | Loss: 0.00002000
Iteration 62/1000 | Loss: 0.00002000
Iteration 63/1000 | Loss: 0.00002000
Iteration 64/1000 | Loss: 0.00002000
Iteration 65/1000 | Loss: 0.00002000
Iteration 66/1000 | Loss: 0.00002000
Iteration 67/1000 | Loss: 0.00001999
Iteration 68/1000 | Loss: 0.00001999
Iteration 69/1000 | Loss: 0.00001999
Iteration 70/1000 | Loss: 0.00001999
Iteration 71/1000 | Loss: 0.00001999
Iteration 72/1000 | Loss: 0.00001998
Iteration 73/1000 | Loss: 0.00001998
Iteration 74/1000 | Loss: 0.00001998
Iteration 75/1000 | Loss: 0.00001997
Iteration 76/1000 | Loss: 0.00001997
Iteration 77/1000 | Loss: 0.00001997
Iteration 78/1000 | Loss: 0.00001997
Iteration 79/1000 | Loss: 0.00001997
Iteration 80/1000 | Loss: 0.00001996
Iteration 81/1000 | Loss: 0.00001996
Iteration 82/1000 | Loss: 0.00001996
Iteration 83/1000 | Loss: 0.00001995
Iteration 84/1000 | Loss: 0.00001995
Iteration 85/1000 | Loss: 0.00001994
Iteration 86/1000 | Loss: 0.00001994
Iteration 87/1000 | Loss: 0.00001994
Iteration 88/1000 | Loss: 0.00001993
Iteration 89/1000 | Loss: 0.00001993
Iteration 90/1000 | Loss: 0.00001993
Iteration 91/1000 | Loss: 0.00001993
Iteration 92/1000 | Loss: 0.00001992
Iteration 93/1000 | Loss: 0.00001992
Iteration 94/1000 | Loss: 0.00001992
Iteration 95/1000 | Loss: 0.00001992
Iteration 96/1000 | Loss: 0.00001991
Iteration 97/1000 | Loss: 0.00001991
Iteration 98/1000 | Loss: 0.00001991
Iteration 99/1000 | Loss: 0.00001991
Iteration 100/1000 | Loss: 0.00001991
Iteration 101/1000 | Loss: 0.00001990
Iteration 102/1000 | Loss: 0.00001990
Iteration 103/1000 | Loss: 0.00001990
Iteration 104/1000 | Loss: 0.00001990
Iteration 105/1000 | Loss: 0.00001990
Iteration 106/1000 | Loss: 0.00001990
Iteration 107/1000 | Loss: 0.00001990
Iteration 108/1000 | Loss: 0.00001990
Iteration 109/1000 | Loss: 0.00001990
Iteration 110/1000 | Loss: 0.00001990
Iteration 111/1000 | Loss: 0.00001990
Iteration 112/1000 | Loss: 0.00001990
Iteration 113/1000 | Loss: 0.00001990
Iteration 114/1000 | Loss: 0.00001990
Iteration 115/1000 | Loss: 0.00001990
Iteration 116/1000 | Loss: 0.00001990
Iteration 117/1000 | Loss: 0.00001990
Iteration 118/1000 | Loss: 0.00001990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.990435521292966e-05, 1.990435521292966e-05, 1.990435521292966e-05, 1.990435521292966e-05, 1.990435521292966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.990435521292966e-05

Optimization complete. Final v2v error: 3.6148080825805664 mm

Highest mean error: 4.1158881187438965 mm for frame 157

Lowest mean error: 3.264070749282837 mm for frame 5

Saving results

Total time: 41.19492840766907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00529548
Iteration 2/25 | Loss: 0.00149769
Iteration 3/25 | Loss: 0.00122888
Iteration 4/25 | Loss: 0.00120216
Iteration 5/25 | Loss: 0.00119563
Iteration 6/25 | Loss: 0.00119394
Iteration 7/25 | Loss: 0.00119394
Iteration 8/25 | Loss: 0.00119394
Iteration 9/25 | Loss: 0.00119394
Iteration 10/25 | Loss: 0.00119394
Iteration 11/25 | Loss: 0.00119394
Iteration 12/25 | Loss: 0.00119394
Iteration 13/25 | Loss: 0.00119394
Iteration 14/25 | Loss: 0.00119394
Iteration 15/25 | Loss: 0.00119394
Iteration 16/25 | Loss: 0.00119394
Iteration 17/25 | Loss: 0.00119394
Iteration 18/25 | Loss: 0.00119394
Iteration 19/25 | Loss: 0.00119394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001193941105157137, 0.001193941105157137, 0.001193941105157137, 0.001193941105157137, 0.001193941105157137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001193941105157137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90120155
Iteration 2/25 | Loss: 0.00098086
Iteration 3/25 | Loss: 0.00098086
Iteration 4/25 | Loss: 0.00098085
Iteration 5/25 | Loss: 0.00098085
Iteration 6/25 | Loss: 0.00098085
Iteration 7/25 | Loss: 0.00098085
Iteration 8/25 | Loss: 0.00098085
Iteration 9/25 | Loss: 0.00098085
Iteration 10/25 | Loss: 0.00098085
Iteration 11/25 | Loss: 0.00098085
Iteration 12/25 | Loss: 0.00098085
Iteration 13/25 | Loss: 0.00098085
Iteration 14/25 | Loss: 0.00098085
Iteration 15/25 | Loss: 0.00098085
Iteration 16/25 | Loss: 0.00098085
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009808526374399662, 0.0009808526374399662, 0.0009808526374399662, 0.0009808526374399662, 0.0009808526374399662]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009808526374399662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098085
Iteration 2/1000 | Loss: 0.00006097
Iteration 3/1000 | Loss: 0.00003943
Iteration 4/1000 | Loss: 0.00003335
Iteration 5/1000 | Loss: 0.00003129
Iteration 6/1000 | Loss: 0.00002975
Iteration 7/1000 | Loss: 0.00002901
Iteration 8/1000 | Loss: 0.00002815
Iteration 9/1000 | Loss: 0.00002758
Iteration 10/1000 | Loss: 0.00002715
Iteration 11/1000 | Loss: 0.00002676
Iteration 12/1000 | Loss: 0.00002645
Iteration 13/1000 | Loss: 0.00002624
Iteration 14/1000 | Loss: 0.00002605
Iteration 15/1000 | Loss: 0.00002585
Iteration 16/1000 | Loss: 0.00002569
Iteration 17/1000 | Loss: 0.00002558
Iteration 18/1000 | Loss: 0.00002558
Iteration 19/1000 | Loss: 0.00002549
Iteration 20/1000 | Loss: 0.00002539
Iteration 21/1000 | Loss: 0.00002534
Iteration 22/1000 | Loss: 0.00002534
Iteration 23/1000 | Loss: 0.00002533
Iteration 24/1000 | Loss: 0.00002533
Iteration 25/1000 | Loss: 0.00002533
Iteration 26/1000 | Loss: 0.00002532
Iteration 27/1000 | Loss: 0.00002532
Iteration 28/1000 | Loss: 0.00002531
Iteration 29/1000 | Loss: 0.00002531
Iteration 30/1000 | Loss: 0.00002531
Iteration 31/1000 | Loss: 0.00002530
Iteration 32/1000 | Loss: 0.00002530
Iteration 33/1000 | Loss: 0.00002529
Iteration 34/1000 | Loss: 0.00002529
Iteration 35/1000 | Loss: 0.00002529
Iteration 36/1000 | Loss: 0.00002529
Iteration 37/1000 | Loss: 0.00002528
Iteration 38/1000 | Loss: 0.00002528
Iteration 39/1000 | Loss: 0.00002528
Iteration 40/1000 | Loss: 0.00002528
Iteration 41/1000 | Loss: 0.00002528
Iteration 42/1000 | Loss: 0.00002528
Iteration 43/1000 | Loss: 0.00002527
Iteration 44/1000 | Loss: 0.00002527
Iteration 45/1000 | Loss: 0.00002527
Iteration 46/1000 | Loss: 0.00002526
Iteration 47/1000 | Loss: 0.00002526
Iteration 48/1000 | Loss: 0.00002525
Iteration 49/1000 | Loss: 0.00002525
Iteration 50/1000 | Loss: 0.00002524
Iteration 51/1000 | Loss: 0.00002523
Iteration 52/1000 | Loss: 0.00002523
Iteration 53/1000 | Loss: 0.00002523
Iteration 54/1000 | Loss: 0.00002520
Iteration 55/1000 | Loss: 0.00002520
Iteration 56/1000 | Loss: 0.00002520
Iteration 57/1000 | Loss: 0.00002519
Iteration 58/1000 | Loss: 0.00002518
Iteration 59/1000 | Loss: 0.00002518
Iteration 60/1000 | Loss: 0.00002517
Iteration 61/1000 | Loss: 0.00002517
Iteration 62/1000 | Loss: 0.00002517
Iteration 63/1000 | Loss: 0.00002517
Iteration 64/1000 | Loss: 0.00002517
Iteration 65/1000 | Loss: 0.00002517
Iteration 66/1000 | Loss: 0.00002516
Iteration 67/1000 | Loss: 0.00002516
Iteration 68/1000 | Loss: 0.00002516
Iteration 69/1000 | Loss: 0.00002516
Iteration 70/1000 | Loss: 0.00002516
Iteration 71/1000 | Loss: 0.00002516
Iteration 72/1000 | Loss: 0.00002515
Iteration 73/1000 | Loss: 0.00002515
Iteration 74/1000 | Loss: 0.00002515
Iteration 75/1000 | Loss: 0.00002515
Iteration 76/1000 | Loss: 0.00002515
Iteration 77/1000 | Loss: 0.00002515
Iteration 78/1000 | Loss: 0.00002515
Iteration 79/1000 | Loss: 0.00002514
Iteration 80/1000 | Loss: 0.00002514
Iteration 81/1000 | Loss: 0.00002514
Iteration 82/1000 | Loss: 0.00002513
Iteration 83/1000 | Loss: 0.00002513
Iteration 84/1000 | Loss: 0.00002513
Iteration 85/1000 | Loss: 0.00002513
Iteration 86/1000 | Loss: 0.00002513
Iteration 87/1000 | Loss: 0.00002513
Iteration 88/1000 | Loss: 0.00002513
Iteration 89/1000 | Loss: 0.00002513
Iteration 90/1000 | Loss: 0.00002513
Iteration 91/1000 | Loss: 0.00002513
Iteration 92/1000 | Loss: 0.00002513
Iteration 93/1000 | Loss: 0.00002513
Iteration 94/1000 | Loss: 0.00002513
Iteration 95/1000 | Loss: 0.00002513
Iteration 96/1000 | Loss: 0.00002513
Iteration 97/1000 | Loss: 0.00002512
Iteration 98/1000 | Loss: 0.00002512
Iteration 99/1000 | Loss: 0.00002512
Iteration 100/1000 | Loss: 0.00002512
Iteration 101/1000 | Loss: 0.00002511
Iteration 102/1000 | Loss: 0.00002511
Iteration 103/1000 | Loss: 0.00002511
Iteration 104/1000 | Loss: 0.00002511
Iteration 105/1000 | Loss: 0.00002511
Iteration 106/1000 | Loss: 0.00002510
Iteration 107/1000 | Loss: 0.00002510
Iteration 108/1000 | Loss: 0.00002510
Iteration 109/1000 | Loss: 0.00002510
Iteration 110/1000 | Loss: 0.00002510
Iteration 111/1000 | Loss: 0.00002510
Iteration 112/1000 | Loss: 0.00002510
Iteration 113/1000 | Loss: 0.00002510
Iteration 114/1000 | Loss: 0.00002510
Iteration 115/1000 | Loss: 0.00002510
Iteration 116/1000 | Loss: 0.00002509
Iteration 117/1000 | Loss: 0.00002509
Iteration 118/1000 | Loss: 0.00002509
Iteration 119/1000 | Loss: 0.00002509
Iteration 120/1000 | Loss: 0.00002509
Iteration 121/1000 | Loss: 0.00002509
Iteration 122/1000 | Loss: 0.00002509
Iteration 123/1000 | Loss: 0.00002509
Iteration 124/1000 | Loss: 0.00002509
Iteration 125/1000 | Loss: 0.00002509
Iteration 126/1000 | Loss: 0.00002508
Iteration 127/1000 | Loss: 0.00002508
Iteration 128/1000 | Loss: 0.00002508
Iteration 129/1000 | Loss: 0.00002508
Iteration 130/1000 | Loss: 0.00002508
Iteration 131/1000 | Loss: 0.00002507
Iteration 132/1000 | Loss: 0.00002507
Iteration 133/1000 | Loss: 0.00002507
Iteration 134/1000 | Loss: 0.00002507
Iteration 135/1000 | Loss: 0.00002507
Iteration 136/1000 | Loss: 0.00002507
Iteration 137/1000 | Loss: 0.00002507
Iteration 138/1000 | Loss: 0.00002507
Iteration 139/1000 | Loss: 0.00002507
Iteration 140/1000 | Loss: 0.00002507
Iteration 141/1000 | Loss: 0.00002506
Iteration 142/1000 | Loss: 0.00002506
Iteration 143/1000 | Loss: 0.00002506
Iteration 144/1000 | Loss: 0.00002506
Iteration 145/1000 | Loss: 0.00002506
Iteration 146/1000 | Loss: 0.00002506
Iteration 147/1000 | Loss: 0.00002506
Iteration 148/1000 | Loss: 0.00002506
Iteration 149/1000 | Loss: 0.00002506
Iteration 150/1000 | Loss: 0.00002506
Iteration 151/1000 | Loss: 0.00002505
Iteration 152/1000 | Loss: 0.00002505
Iteration 153/1000 | Loss: 0.00002505
Iteration 154/1000 | Loss: 0.00002505
Iteration 155/1000 | Loss: 0.00002505
Iteration 156/1000 | Loss: 0.00002505
Iteration 157/1000 | Loss: 0.00002505
Iteration 158/1000 | Loss: 0.00002505
Iteration 159/1000 | Loss: 0.00002504
Iteration 160/1000 | Loss: 0.00002504
Iteration 161/1000 | Loss: 0.00002504
Iteration 162/1000 | Loss: 0.00002504
Iteration 163/1000 | Loss: 0.00002504
Iteration 164/1000 | Loss: 0.00002504
Iteration 165/1000 | Loss: 0.00002504
Iteration 166/1000 | Loss: 0.00002504
Iteration 167/1000 | Loss: 0.00002504
Iteration 168/1000 | Loss: 0.00002504
Iteration 169/1000 | Loss: 0.00002503
Iteration 170/1000 | Loss: 0.00002503
Iteration 171/1000 | Loss: 0.00002503
Iteration 172/1000 | Loss: 0.00002503
Iteration 173/1000 | Loss: 0.00002503
Iteration 174/1000 | Loss: 0.00002503
Iteration 175/1000 | Loss: 0.00002503
Iteration 176/1000 | Loss: 0.00002503
Iteration 177/1000 | Loss: 0.00002503
Iteration 178/1000 | Loss: 0.00002503
Iteration 179/1000 | Loss: 0.00002503
Iteration 180/1000 | Loss: 0.00002503
Iteration 181/1000 | Loss: 0.00002503
Iteration 182/1000 | Loss: 0.00002503
Iteration 183/1000 | Loss: 0.00002503
Iteration 184/1000 | Loss: 0.00002503
Iteration 185/1000 | Loss: 0.00002503
Iteration 186/1000 | Loss: 0.00002503
Iteration 187/1000 | Loss: 0.00002502
Iteration 188/1000 | Loss: 0.00002502
Iteration 189/1000 | Loss: 0.00002502
Iteration 190/1000 | Loss: 0.00002502
Iteration 191/1000 | Loss: 0.00002502
Iteration 192/1000 | Loss: 0.00002502
Iteration 193/1000 | Loss: 0.00002502
Iteration 194/1000 | Loss: 0.00002502
Iteration 195/1000 | Loss: 0.00002502
Iteration 196/1000 | Loss: 0.00002502
Iteration 197/1000 | Loss: 0.00002502
Iteration 198/1000 | Loss: 0.00002502
Iteration 199/1000 | Loss: 0.00002502
Iteration 200/1000 | Loss: 0.00002502
Iteration 201/1000 | Loss: 0.00002502
Iteration 202/1000 | Loss: 0.00002502
Iteration 203/1000 | Loss: 0.00002502
Iteration 204/1000 | Loss: 0.00002502
Iteration 205/1000 | Loss: 0.00002502
Iteration 206/1000 | Loss: 0.00002502
Iteration 207/1000 | Loss: 0.00002502
Iteration 208/1000 | Loss: 0.00002502
Iteration 209/1000 | Loss: 0.00002502
Iteration 210/1000 | Loss: 0.00002502
Iteration 211/1000 | Loss: 0.00002502
Iteration 212/1000 | Loss: 0.00002502
Iteration 213/1000 | Loss: 0.00002502
Iteration 214/1000 | Loss: 0.00002502
Iteration 215/1000 | Loss: 0.00002502
Iteration 216/1000 | Loss: 0.00002502
Iteration 217/1000 | Loss: 0.00002502
Iteration 218/1000 | Loss: 0.00002502
Iteration 219/1000 | Loss: 0.00002502
Iteration 220/1000 | Loss: 0.00002502
Iteration 221/1000 | Loss: 0.00002502
Iteration 222/1000 | Loss: 0.00002502
Iteration 223/1000 | Loss: 0.00002502
Iteration 224/1000 | Loss: 0.00002502
Iteration 225/1000 | Loss: 0.00002502
Iteration 226/1000 | Loss: 0.00002502
Iteration 227/1000 | Loss: 0.00002502
Iteration 228/1000 | Loss: 0.00002502
Iteration 229/1000 | Loss: 0.00002502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [2.5016741346917115e-05, 2.5016741346917115e-05, 2.5016741346917115e-05, 2.5016741346917115e-05, 2.5016741346917115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5016741346917115e-05

Optimization complete. Final v2v error: 4.106320381164551 mm

Highest mean error: 4.996743679046631 mm for frame 63

Lowest mean error: 3.3775477409362793 mm for frame 173

Saving results

Total time: 49.53173065185547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00566857
Iteration 2/25 | Loss: 0.00139412
Iteration 3/25 | Loss: 0.00119056
Iteration 4/25 | Loss: 0.00109554
Iteration 5/25 | Loss: 0.00108783
Iteration 6/25 | Loss: 0.00108817
Iteration 7/25 | Loss: 0.00108612
Iteration 8/25 | Loss: 0.00108359
Iteration 9/25 | Loss: 0.00108267
Iteration 10/25 | Loss: 0.00108226
Iteration 11/25 | Loss: 0.00108208
Iteration 12/25 | Loss: 0.00108198
Iteration 13/25 | Loss: 0.00108197
Iteration 14/25 | Loss: 0.00108197
Iteration 15/25 | Loss: 0.00108197
Iteration 16/25 | Loss: 0.00108196
Iteration 17/25 | Loss: 0.00108196
Iteration 18/25 | Loss: 0.00108196
Iteration 19/25 | Loss: 0.00108196
Iteration 20/25 | Loss: 0.00108196
Iteration 21/25 | Loss: 0.00108196
Iteration 22/25 | Loss: 0.00108196
Iteration 23/25 | Loss: 0.00108196
Iteration 24/25 | Loss: 0.00108195
Iteration 25/25 | Loss: 0.00108195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.17096901
Iteration 2/25 | Loss: 0.00077070
Iteration 3/25 | Loss: 0.00077069
Iteration 4/25 | Loss: 0.00077069
Iteration 5/25 | Loss: 0.00077069
Iteration 6/25 | Loss: 0.00077069
Iteration 7/25 | Loss: 0.00077069
Iteration 8/25 | Loss: 0.00077069
Iteration 9/25 | Loss: 0.00077069
Iteration 10/25 | Loss: 0.00077069
Iteration 11/25 | Loss: 0.00077069
Iteration 12/25 | Loss: 0.00077069
Iteration 13/25 | Loss: 0.00077069
Iteration 14/25 | Loss: 0.00077069
Iteration 15/25 | Loss: 0.00077069
Iteration 16/25 | Loss: 0.00077069
Iteration 17/25 | Loss: 0.00077069
Iteration 18/25 | Loss: 0.00077068
Iteration 19/25 | Loss: 0.00077069
Iteration 20/25 | Loss: 0.00077068
Iteration 21/25 | Loss: 0.00077068
Iteration 22/25 | Loss: 0.00077068
Iteration 23/25 | Loss: 0.00077068
Iteration 24/25 | Loss: 0.00077068
Iteration 25/25 | Loss: 0.00077068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007706849719397724, 0.0007706849719397724, 0.0007706849719397724, 0.0007706849719397724, 0.0007706849719397724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007706849719397724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077068
Iteration 2/1000 | Loss: 0.00001951
Iteration 3/1000 | Loss: 0.00001461
Iteration 4/1000 | Loss: 0.00001346
Iteration 5/1000 | Loss: 0.00001267
Iteration 6/1000 | Loss: 0.00004638
Iteration 7/1000 | Loss: 0.00001695
Iteration 8/1000 | Loss: 0.00001216
Iteration 9/1000 | Loss: 0.00001330
Iteration 10/1000 | Loss: 0.00001176
Iteration 11/1000 | Loss: 0.00001169
Iteration 12/1000 | Loss: 0.00001145
Iteration 13/1000 | Loss: 0.00001141
Iteration 14/1000 | Loss: 0.00001133
Iteration 15/1000 | Loss: 0.00001128
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001126
Iteration 18/1000 | Loss: 0.00001124
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001122
Iteration 21/1000 | Loss: 0.00001121
Iteration 22/1000 | Loss: 0.00001120
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001117
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001106
Iteration 28/1000 | Loss: 0.00001105
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001103
Iteration 31/1000 | Loss: 0.00001103
Iteration 32/1000 | Loss: 0.00001103
Iteration 33/1000 | Loss: 0.00001103
Iteration 34/1000 | Loss: 0.00001103
Iteration 35/1000 | Loss: 0.00001103
Iteration 36/1000 | Loss: 0.00001102
Iteration 37/1000 | Loss: 0.00006733
Iteration 38/1000 | Loss: 0.00001105
Iteration 39/1000 | Loss: 0.00001099
Iteration 40/1000 | Loss: 0.00001095
Iteration 41/1000 | Loss: 0.00001093
Iteration 42/1000 | Loss: 0.00001092
Iteration 43/1000 | Loss: 0.00001092
Iteration 44/1000 | Loss: 0.00001092
Iteration 45/1000 | Loss: 0.00001091
Iteration 46/1000 | Loss: 0.00001091
Iteration 47/1000 | Loss: 0.00001091
Iteration 48/1000 | Loss: 0.00001091
Iteration 49/1000 | Loss: 0.00001091
Iteration 50/1000 | Loss: 0.00001090
Iteration 51/1000 | Loss: 0.00001090
Iteration 52/1000 | Loss: 0.00001090
Iteration 53/1000 | Loss: 0.00001090
Iteration 54/1000 | Loss: 0.00001090
Iteration 55/1000 | Loss: 0.00001089
Iteration 56/1000 | Loss: 0.00001089
Iteration 57/1000 | Loss: 0.00001089
Iteration 58/1000 | Loss: 0.00001089
Iteration 59/1000 | Loss: 0.00001089
Iteration 60/1000 | Loss: 0.00001089
Iteration 61/1000 | Loss: 0.00001088
Iteration 62/1000 | Loss: 0.00001088
Iteration 63/1000 | Loss: 0.00001088
Iteration 64/1000 | Loss: 0.00001088
Iteration 65/1000 | Loss: 0.00001087
Iteration 66/1000 | Loss: 0.00001087
Iteration 67/1000 | Loss: 0.00001086
Iteration 68/1000 | Loss: 0.00001085
Iteration 69/1000 | Loss: 0.00001085
Iteration 70/1000 | Loss: 0.00001085
Iteration 71/1000 | Loss: 0.00001085
Iteration 72/1000 | Loss: 0.00001084
Iteration 73/1000 | Loss: 0.00001084
Iteration 74/1000 | Loss: 0.00001084
Iteration 75/1000 | Loss: 0.00001083
Iteration 76/1000 | Loss: 0.00001083
Iteration 77/1000 | Loss: 0.00001082
Iteration 78/1000 | Loss: 0.00001082
Iteration 79/1000 | Loss: 0.00001082
Iteration 80/1000 | Loss: 0.00001082
Iteration 81/1000 | Loss: 0.00001082
Iteration 82/1000 | Loss: 0.00001082
Iteration 83/1000 | Loss: 0.00001082
Iteration 84/1000 | Loss: 0.00001082
Iteration 85/1000 | Loss: 0.00001082
Iteration 86/1000 | Loss: 0.00001082
Iteration 87/1000 | Loss: 0.00001082
Iteration 88/1000 | Loss: 0.00001082
Iteration 89/1000 | Loss: 0.00001082
Iteration 90/1000 | Loss: 0.00001081
Iteration 91/1000 | Loss: 0.00001081
Iteration 92/1000 | Loss: 0.00001081
Iteration 93/1000 | Loss: 0.00001081
Iteration 94/1000 | Loss: 0.00001081
Iteration 95/1000 | Loss: 0.00001080
Iteration 96/1000 | Loss: 0.00001080
Iteration 97/1000 | Loss: 0.00001080
Iteration 98/1000 | Loss: 0.00001080
Iteration 99/1000 | Loss: 0.00001079
Iteration 100/1000 | Loss: 0.00001079
Iteration 101/1000 | Loss: 0.00001078
Iteration 102/1000 | Loss: 0.00001078
Iteration 103/1000 | Loss: 0.00001078
Iteration 104/1000 | Loss: 0.00001078
Iteration 105/1000 | Loss: 0.00001077
Iteration 106/1000 | Loss: 0.00001077
Iteration 107/1000 | Loss: 0.00001077
Iteration 108/1000 | Loss: 0.00001077
Iteration 109/1000 | Loss: 0.00001077
Iteration 110/1000 | Loss: 0.00001077
Iteration 111/1000 | Loss: 0.00001077
Iteration 112/1000 | Loss: 0.00001077
Iteration 113/1000 | Loss: 0.00001077
Iteration 114/1000 | Loss: 0.00001077
Iteration 115/1000 | Loss: 0.00001076
Iteration 116/1000 | Loss: 0.00001076
Iteration 117/1000 | Loss: 0.00001076
Iteration 118/1000 | Loss: 0.00001076
Iteration 119/1000 | Loss: 0.00001076
Iteration 120/1000 | Loss: 0.00001076
Iteration 121/1000 | Loss: 0.00001076
Iteration 122/1000 | Loss: 0.00001076
Iteration 123/1000 | Loss: 0.00001076
Iteration 124/1000 | Loss: 0.00001076
Iteration 125/1000 | Loss: 0.00001076
Iteration 126/1000 | Loss: 0.00001076
Iteration 127/1000 | Loss: 0.00001076
Iteration 128/1000 | Loss: 0.00001076
Iteration 129/1000 | Loss: 0.00001076
Iteration 130/1000 | Loss: 0.00001075
Iteration 131/1000 | Loss: 0.00001075
Iteration 132/1000 | Loss: 0.00001075
Iteration 133/1000 | Loss: 0.00001075
Iteration 134/1000 | Loss: 0.00001075
Iteration 135/1000 | Loss: 0.00001075
Iteration 136/1000 | Loss: 0.00001075
Iteration 137/1000 | Loss: 0.00001075
Iteration 138/1000 | Loss: 0.00001074
Iteration 139/1000 | Loss: 0.00001074
Iteration 140/1000 | Loss: 0.00001074
Iteration 141/1000 | Loss: 0.00001074
Iteration 142/1000 | Loss: 0.00001074
Iteration 143/1000 | Loss: 0.00001074
Iteration 144/1000 | Loss: 0.00001074
Iteration 145/1000 | Loss: 0.00001074
Iteration 146/1000 | Loss: 0.00001074
Iteration 147/1000 | Loss: 0.00001074
Iteration 148/1000 | Loss: 0.00001073
Iteration 149/1000 | Loss: 0.00001073
Iteration 150/1000 | Loss: 0.00001073
Iteration 151/1000 | Loss: 0.00001073
Iteration 152/1000 | Loss: 0.00001073
Iteration 153/1000 | Loss: 0.00001073
Iteration 154/1000 | Loss: 0.00001073
Iteration 155/1000 | Loss: 0.00001073
Iteration 156/1000 | Loss: 0.00001072
Iteration 157/1000 | Loss: 0.00001072
Iteration 158/1000 | Loss: 0.00001072
Iteration 159/1000 | Loss: 0.00001072
Iteration 160/1000 | Loss: 0.00001072
Iteration 161/1000 | Loss: 0.00001072
Iteration 162/1000 | Loss: 0.00001072
Iteration 163/1000 | Loss: 0.00001072
Iteration 164/1000 | Loss: 0.00001072
Iteration 165/1000 | Loss: 0.00001071
Iteration 166/1000 | Loss: 0.00001071
Iteration 167/1000 | Loss: 0.00001071
Iteration 168/1000 | Loss: 0.00001071
Iteration 169/1000 | Loss: 0.00001071
Iteration 170/1000 | Loss: 0.00001071
Iteration 171/1000 | Loss: 0.00001071
Iteration 172/1000 | Loss: 0.00001071
Iteration 173/1000 | Loss: 0.00001071
Iteration 174/1000 | Loss: 0.00001070
Iteration 175/1000 | Loss: 0.00001070
Iteration 176/1000 | Loss: 0.00001070
Iteration 177/1000 | Loss: 0.00001070
Iteration 178/1000 | Loss: 0.00001070
Iteration 179/1000 | Loss: 0.00001070
Iteration 180/1000 | Loss: 0.00001070
Iteration 181/1000 | Loss: 0.00001069
Iteration 182/1000 | Loss: 0.00004809
Iteration 183/1000 | Loss: 0.00001920
Iteration 184/1000 | Loss: 0.00001072
Iteration 185/1000 | Loss: 0.00001069
Iteration 186/1000 | Loss: 0.00001068
Iteration 187/1000 | Loss: 0.00001068
Iteration 188/1000 | Loss: 0.00001068
Iteration 189/1000 | Loss: 0.00001068
Iteration 190/1000 | Loss: 0.00001068
Iteration 191/1000 | Loss: 0.00001068
Iteration 192/1000 | Loss: 0.00001068
Iteration 193/1000 | Loss: 0.00001068
Iteration 194/1000 | Loss: 0.00001068
Iteration 195/1000 | Loss: 0.00001068
Iteration 196/1000 | Loss: 0.00001068
Iteration 197/1000 | Loss: 0.00001068
Iteration 198/1000 | Loss: 0.00001068
Iteration 199/1000 | Loss: 0.00001068
Iteration 200/1000 | Loss: 0.00001067
Iteration 201/1000 | Loss: 0.00001067
Iteration 202/1000 | Loss: 0.00001067
Iteration 203/1000 | Loss: 0.00001067
Iteration 204/1000 | Loss: 0.00001067
Iteration 205/1000 | Loss: 0.00001067
Iteration 206/1000 | Loss: 0.00001067
Iteration 207/1000 | Loss: 0.00001067
Iteration 208/1000 | Loss: 0.00001067
Iteration 209/1000 | Loss: 0.00001067
Iteration 210/1000 | Loss: 0.00001067
Iteration 211/1000 | Loss: 0.00001067
Iteration 212/1000 | Loss: 0.00001067
Iteration 213/1000 | Loss: 0.00001067
Iteration 214/1000 | Loss: 0.00001067
Iteration 215/1000 | Loss: 0.00001067
Iteration 216/1000 | Loss: 0.00001067
Iteration 217/1000 | Loss: 0.00001067
Iteration 218/1000 | Loss: 0.00001067
Iteration 219/1000 | Loss: 0.00001067
Iteration 220/1000 | Loss: 0.00001067
Iteration 221/1000 | Loss: 0.00001067
Iteration 222/1000 | Loss: 0.00001067
Iteration 223/1000 | Loss: 0.00001067
Iteration 224/1000 | Loss: 0.00001067
Iteration 225/1000 | Loss: 0.00001067
Iteration 226/1000 | Loss: 0.00001067
Iteration 227/1000 | Loss: 0.00001067
Iteration 228/1000 | Loss: 0.00001067
Iteration 229/1000 | Loss: 0.00001067
Iteration 230/1000 | Loss: 0.00001067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.0668265531421639e-05, 1.0668265531421639e-05, 1.0668265531421639e-05, 1.0668265531421639e-05, 1.0668265531421639e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0668265531421639e-05

Optimization complete. Final v2v error: 2.7708141803741455 mm

Highest mean error: 3.458341121673584 mm for frame 171

Lowest mean error: 2.530897617340088 mm for frame 31

Saving results

Total time: 66.82053923606873
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010775
Iteration 2/25 | Loss: 0.00258739
Iteration 3/25 | Loss: 0.00248728
Iteration 4/25 | Loss: 0.00213034
Iteration 5/25 | Loss: 0.00187826
Iteration 6/25 | Loss: 0.00188471
Iteration 7/25 | Loss: 0.00148398
Iteration 8/25 | Loss: 0.00138431
Iteration 9/25 | Loss: 0.00140686
Iteration 10/25 | Loss: 0.00132681
Iteration 11/25 | Loss: 0.00130076
Iteration 12/25 | Loss: 0.00129565
Iteration 13/25 | Loss: 0.00129236
Iteration 14/25 | Loss: 0.00128023
Iteration 15/25 | Loss: 0.00127664
Iteration 16/25 | Loss: 0.00127884
Iteration 17/25 | Loss: 0.00128776
Iteration 18/25 | Loss: 0.00127795
Iteration 19/25 | Loss: 0.00125593
Iteration 20/25 | Loss: 0.00125119
Iteration 21/25 | Loss: 0.00125007
Iteration 22/25 | Loss: 0.00124973
Iteration 23/25 | Loss: 0.00124965
Iteration 24/25 | Loss: 0.00124965
Iteration 25/25 | Loss: 0.00124965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37223113
Iteration 2/25 | Loss: 0.00085089
Iteration 3/25 | Loss: 0.00085089
Iteration 4/25 | Loss: 0.00085088
Iteration 5/25 | Loss: 0.00085088
Iteration 6/25 | Loss: 0.00085088
Iteration 7/25 | Loss: 0.00085088
Iteration 8/25 | Loss: 0.00085088
Iteration 9/25 | Loss: 0.00085088
Iteration 10/25 | Loss: 0.00085088
Iteration 11/25 | Loss: 0.00085088
Iteration 12/25 | Loss: 0.00085088
Iteration 13/25 | Loss: 0.00085088
Iteration 14/25 | Loss: 0.00085088
Iteration 15/25 | Loss: 0.00085088
Iteration 16/25 | Loss: 0.00085088
Iteration 17/25 | Loss: 0.00085088
Iteration 18/25 | Loss: 0.00085088
Iteration 19/25 | Loss: 0.00085088
Iteration 20/25 | Loss: 0.00085088
Iteration 21/25 | Loss: 0.00085088
Iteration 22/25 | Loss: 0.00085088
Iteration 23/25 | Loss: 0.00085088
Iteration 24/25 | Loss: 0.00085088
Iteration 25/25 | Loss: 0.00085088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085088
Iteration 2/1000 | Loss: 0.00083479
Iteration 3/1000 | Loss: 0.00013340
Iteration 4/1000 | Loss: 0.00009917
Iteration 5/1000 | Loss: 0.00007064
Iteration 6/1000 | Loss: 0.00006188
Iteration 7/1000 | Loss: 0.00005432
Iteration 8/1000 | Loss: 0.00011514
Iteration 9/1000 | Loss: 0.00005839
Iteration 10/1000 | Loss: 0.00004149
Iteration 11/1000 | Loss: 0.00003959
Iteration 12/1000 | Loss: 0.00006439
Iteration 13/1000 | Loss: 0.00003747
Iteration 14/1000 | Loss: 0.00003626
Iteration 15/1000 | Loss: 0.00003545
Iteration 16/1000 | Loss: 0.00006364
Iteration 17/1000 | Loss: 0.00003478
Iteration 18/1000 | Loss: 0.00019882
Iteration 19/1000 | Loss: 0.00091241
Iteration 20/1000 | Loss: 0.00024554
Iteration 21/1000 | Loss: 0.00013722
Iteration 22/1000 | Loss: 0.00059958
Iteration 23/1000 | Loss: 0.00004428
Iteration 24/1000 | Loss: 0.00006101
Iteration 25/1000 | Loss: 0.00003186
Iteration 26/1000 | Loss: 0.00013332
Iteration 27/1000 | Loss: 0.00002526
Iteration 28/1000 | Loss: 0.00002342
Iteration 29/1000 | Loss: 0.00002233
Iteration 30/1000 | Loss: 0.00002154
Iteration 31/1000 | Loss: 0.00004559
Iteration 32/1000 | Loss: 0.00002085
Iteration 33/1000 | Loss: 0.00010877
Iteration 34/1000 | Loss: 0.00005115
Iteration 35/1000 | Loss: 0.00002039
Iteration 36/1000 | Loss: 0.00002019
Iteration 37/1000 | Loss: 0.00002016
Iteration 38/1000 | Loss: 0.00002000
Iteration 39/1000 | Loss: 0.00001999
Iteration 40/1000 | Loss: 0.00005633
Iteration 41/1000 | Loss: 0.00002927
Iteration 42/1000 | Loss: 0.00001985
Iteration 43/1000 | Loss: 0.00001984
Iteration 44/1000 | Loss: 0.00001984
Iteration 45/1000 | Loss: 0.00001984
Iteration 46/1000 | Loss: 0.00001984
Iteration 47/1000 | Loss: 0.00001984
Iteration 48/1000 | Loss: 0.00001983
Iteration 49/1000 | Loss: 0.00001983
Iteration 50/1000 | Loss: 0.00001983
Iteration 51/1000 | Loss: 0.00001983
Iteration 52/1000 | Loss: 0.00001983
Iteration 53/1000 | Loss: 0.00001983
Iteration 54/1000 | Loss: 0.00001983
Iteration 55/1000 | Loss: 0.00001983
Iteration 56/1000 | Loss: 0.00001983
Iteration 57/1000 | Loss: 0.00001983
Iteration 58/1000 | Loss: 0.00001982
Iteration 59/1000 | Loss: 0.00001982
Iteration 60/1000 | Loss: 0.00004426
Iteration 61/1000 | Loss: 0.00001978
Iteration 62/1000 | Loss: 0.00001978
Iteration 63/1000 | Loss: 0.00001978
Iteration 64/1000 | Loss: 0.00001978
Iteration 65/1000 | Loss: 0.00001978
Iteration 66/1000 | Loss: 0.00001978
Iteration 67/1000 | Loss: 0.00001978
Iteration 68/1000 | Loss: 0.00001977
Iteration 69/1000 | Loss: 0.00001977
Iteration 70/1000 | Loss: 0.00001977
Iteration 71/1000 | Loss: 0.00001977
Iteration 72/1000 | Loss: 0.00001977
Iteration 73/1000 | Loss: 0.00001977
Iteration 74/1000 | Loss: 0.00001977
Iteration 75/1000 | Loss: 0.00001977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.977373358386103e-05, 1.977373358386103e-05, 1.977373358386103e-05, 1.977373358386103e-05, 1.977373358386103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.977373358386103e-05

Optimization complete. Final v2v error: 3.703078269958496 mm

Highest mean error: 4.055284023284912 mm for frame 212

Lowest mean error: 3.3766818046569824 mm for frame 12

Saving results

Total time: 105.80929112434387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444941
Iteration 2/25 | Loss: 0.00134738
Iteration 3/25 | Loss: 0.00117770
Iteration 4/25 | Loss: 0.00115041
Iteration 5/25 | Loss: 0.00114199
Iteration 6/25 | Loss: 0.00114056
Iteration 7/25 | Loss: 0.00113979
Iteration 8/25 | Loss: 0.00113979
Iteration 9/25 | Loss: 0.00113979
Iteration 10/25 | Loss: 0.00113979
Iteration 11/25 | Loss: 0.00113979
Iteration 12/25 | Loss: 0.00113979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011397876078262925, 0.0011397876078262925, 0.0011397876078262925, 0.0011397876078262925, 0.0011397876078262925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011397876078262925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18673193
Iteration 2/25 | Loss: 0.00102473
Iteration 3/25 | Loss: 0.00102471
Iteration 4/25 | Loss: 0.00102471
Iteration 5/25 | Loss: 0.00102471
Iteration 6/25 | Loss: 0.00102471
Iteration 7/25 | Loss: 0.00102471
Iteration 8/25 | Loss: 0.00102471
Iteration 9/25 | Loss: 0.00102471
Iteration 10/25 | Loss: 0.00102471
Iteration 11/25 | Loss: 0.00102471
Iteration 12/25 | Loss: 0.00102470
Iteration 13/25 | Loss: 0.00102470
Iteration 14/25 | Loss: 0.00102470
Iteration 15/25 | Loss: 0.00102470
Iteration 16/25 | Loss: 0.00102470
Iteration 17/25 | Loss: 0.00102470
Iteration 18/25 | Loss: 0.00102470
Iteration 19/25 | Loss: 0.00102470
Iteration 20/25 | Loss: 0.00102470
Iteration 21/25 | Loss: 0.00102470
Iteration 22/25 | Loss: 0.00102470
Iteration 23/25 | Loss: 0.00102470
Iteration 24/25 | Loss: 0.00102470
Iteration 25/25 | Loss: 0.00102470
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001024704659357667, 0.001024704659357667, 0.001024704659357667, 0.001024704659357667, 0.001024704659357667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001024704659357667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102470
Iteration 2/1000 | Loss: 0.00006574
Iteration 3/1000 | Loss: 0.00003797
Iteration 4/1000 | Loss: 0.00003009
Iteration 5/1000 | Loss: 0.00002794
Iteration 6/1000 | Loss: 0.00002651
Iteration 7/1000 | Loss: 0.00002546
Iteration 8/1000 | Loss: 0.00002478
Iteration 9/1000 | Loss: 0.00002423
Iteration 10/1000 | Loss: 0.00002375
Iteration 11/1000 | Loss: 0.00002345
Iteration 12/1000 | Loss: 0.00002326
Iteration 13/1000 | Loss: 0.00002321
Iteration 14/1000 | Loss: 0.00002302
Iteration 15/1000 | Loss: 0.00002291
Iteration 16/1000 | Loss: 0.00002285
Iteration 17/1000 | Loss: 0.00002278
Iteration 18/1000 | Loss: 0.00002274
Iteration 19/1000 | Loss: 0.00002273
Iteration 20/1000 | Loss: 0.00002273
Iteration 21/1000 | Loss: 0.00002271
Iteration 22/1000 | Loss: 0.00002266
Iteration 23/1000 | Loss: 0.00002262
Iteration 24/1000 | Loss: 0.00002250
Iteration 25/1000 | Loss: 0.00002250
Iteration 26/1000 | Loss: 0.00002249
Iteration 27/1000 | Loss: 0.00002248
Iteration 28/1000 | Loss: 0.00002247
Iteration 29/1000 | Loss: 0.00002247
Iteration 30/1000 | Loss: 0.00002246
Iteration 31/1000 | Loss: 0.00002246
Iteration 32/1000 | Loss: 0.00002242
Iteration 33/1000 | Loss: 0.00002242
Iteration 34/1000 | Loss: 0.00002242
Iteration 35/1000 | Loss: 0.00002242
Iteration 36/1000 | Loss: 0.00002242
Iteration 37/1000 | Loss: 0.00002242
Iteration 38/1000 | Loss: 0.00002242
Iteration 39/1000 | Loss: 0.00002242
Iteration 40/1000 | Loss: 0.00002242
Iteration 41/1000 | Loss: 0.00002242
Iteration 42/1000 | Loss: 0.00002242
Iteration 43/1000 | Loss: 0.00002242
Iteration 44/1000 | Loss: 0.00002241
Iteration 45/1000 | Loss: 0.00002241
Iteration 46/1000 | Loss: 0.00002241
Iteration 47/1000 | Loss: 0.00002240
Iteration 48/1000 | Loss: 0.00002239
Iteration 49/1000 | Loss: 0.00002239
Iteration 50/1000 | Loss: 0.00002239
Iteration 51/1000 | Loss: 0.00002238
Iteration 52/1000 | Loss: 0.00002238
Iteration 53/1000 | Loss: 0.00002238
Iteration 54/1000 | Loss: 0.00002237
Iteration 55/1000 | Loss: 0.00002237
Iteration 56/1000 | Loss: 0.00002234
Iteration 57/1000 | Loss: 0.00002234
Iteration 58/1000 | Loss: 0.00002233
Iteration 59/1000 | Loss: 0.00002233
Iteration 60/1000 | Loss: 0.00002233
Iteration 61/1000 | Loss: 0.00002232
Iteration 62/1000 | Loss: 0.00002232
Iteration 63/1000 | Loss: 0.00002232
Iteration 64/1000 | Loss: 0.00002231
Iteration 65/1000 | Loss: 0.00002230
Iteration 66/1000 | Loss: 0.00002230
Iteration 67/1000 | Loss: 0.00002230
Iteration 68/1000 | Loss: 0.00002229
Iteration 69/1000 | Loss: 0.00002229
Iteration 70/1000 | Loss: 0.00002227
Iteration 71/1000 | Loss: 0.00002227
Iteration 72/1000 | Loss: 0.00002227
Iteration 73/1000 | Loss: 0.00002227
Iteration 74/1000 | Loss: 0.00002227
Iteration 75/1000 | Loss: 0.00002227
Iteration 76/1000 | Loss: 0.00002227
Iteration 77/1000 | Loss: 0.00002226
Iteration 78/1000 | Loss: 0.00002226
Iteration 79/1000 | Loss: 0.00002225
Iteration 80/1000 | Loss: 0.00002225
Iteration 81/1000 | Loss: 0.00002225
Iteration 82/1000 | Loss: 0.00002224
Iteration 83/1000 | Loss: 0.00002224
Iteration 84/1000 | Loss: 0.00002224
Iteration 85/1000 | Loss: 0.00002224
Iteration 86/1000 | Loss: 0.00002223
Iteration 87/1000 | Loss: 0.00002223
Iteration 88/1000 | Loss: 0.00002223
Iteration 89/1000 | Loss: 0.00002223
Iteration 90/1000 | Loss: 0.00002223
Iteration 91/1000 | Loss: 0.00002222
Iteration 92/1000 | Loss: 0.00002222
Iteration 93/1000 | Loss: 0.00002222
Iteration 94/1000 | Loss: 0.00002221
Iteration 95/1000 | Loss: 0.00002221
Iteration 96/1000 | Loss: 0.00002221
Iteration 97/1000 | Loss: 0.00002221
Iteration 98/1000 | Loss: 0.00002220
Iteration 99/1000 | Loss: 0.00002220
Iteration 100/1000 | Loss: 0.00002220
Iteration 101/1000 | Loss: 0.00002220
Iteration 102/1000 | Loss: 0.00002220
Iteration 103/1000 | Loss: 0.00002220
Iteration 104/1000 | Loss: 0.00002220
Iteration 105/1000 | Loss: 0.00002220
Iteration 106/1000 | Loss: 0.00002220
Iteration 107/1000 | Loss: 0.00002220
Iteration 108/1000 | Loss: 0.00002220
Iteration 109/1000 | Loss: 0.00002219
Iteration 110/1000 | Loss: 0.00002219
Iteration 111/1000 | Loss: 0.00002219
Iteration 112/1000 | Loss: 0.00002219
Iteration 113/1000 | Loss: 0.00002219
Iteration 114/1000 | Loss: 0.00002219
Iteration 115/1000 | Loss: 0.00002219
Iteration 116/1000 | Loss: 0.00002219
Iteration 117/1000 | Loss: 0.00002218
Iteration 118/1000 | Loss: 0.00002218
Iteration 119/1000 | Loss: 0.00002218
Iteration 120/1000 | Loss: 0.00002218
Iteration 121/1000 | Loss: 0.00002218
Iteration 122/1000 | Loss: 0.00002217
Iteration 123/1000 | Loss: 0.00002217
Iteration 124/1000 | Loss: 0.00002217
Iteration 125/1000 | Loss: 0.00002217
Iteration 126/1000 | Loss: 0.00002216
Iteration 127/1000 | Loss: 0.00002216
Iteration 128/1000 | Loss: 0.00002216
Iteration 129/1000 | Loss: 0.00002216
Iteration 130/1000 | Loss: 0.00002216
Iteration 131/1000 | Loss: 0.00002215
Iteration 132/1000 | Loss: 0.00002215
Iteration 133/1000 | Loss: 0.00002215
Iteration 134/1000 | Loss: 0.00002215
Iteration 135/1000 | Loss: 0.00002215
Iteration 136/1000 | Loss: 0.00002214
Iteration 137/1000 | Loss: 0.00002214
Iteration 138/1000 | Loss: 0.00002214
Iteration 139/1000 | Loss: 0.00002214
Iteration 140/1000 | Loss: 0.00002214
Iteration 141/1000 | Loss: 0.00002214
Iteration 142/1000 | Loss: 0.00002213
Iteration 143/1000 | Loss: 0.00002213
Iteration 144/1000 | Loss: 0.00002213
Iteration 145/1000 | Loss: 0.00002213
Iteration 146/1000 | Loss: 0.00002213
Iteration 147/1000 | Loss: 0.00002213
Iteration 148/1000 | Loss: 0.00002212
Iteration 149/1000 | Loss: 0.00002212
Iteration 150/1000 | Loss: 0.00002212
Iteration 151/1000 | Loss: 0.00002212
Iteration 152/1000 | Loss: 0.00002212
Iteration 153/1000 | Loss: 0.00002212
Iteration 154/1000 | Loss: 0.00002212
Iteration 155/1000 | Loss: 0.00002211
Iteration 156/1000 | Loss: 0.00002211
Iteration 157/1000 | Loss: 0.00002211
Iteration 158/1000 | Loss: 0.00002211
Iteration 159/1000 | Loss: 0.00002211
Iteration 160/1000 | Loss: 0.00002211
Iteration 161/1000 | Loss: 0.00002211
Iteration 162/1000 | Loss: 0.00002211
Iteration 163/1000 | Loss: 0.00002211
Iteration 164/1000 | Loss: 0.00002211
Iteration 165/1000 | Loss: 0.00002211
Iteration 166/1000 | Loss: 0.00002211
Iteration 167/1000 | Loss: 0.00002211
Iteration 168/1000 | Loss: 0.00002211
Iteration 169/1000 | Loss: 0.00002211
Iteration 170/1000 | Loss: 0.00002211
Iteration 171/1000 | Loss: 0.00002211
Iteration 172/1000 | Loss: 0.00002210
Iteration 173/1000 | Loss: 0.00002210
Iteration 174/1000 | Loss: 0.00002210
Iteration 175/1000 | Loss: 0.00002210
Iteration 176/1000 | Loss: 0.00002210
Iteration 177/1000 | Loss: 0.00002210
Iteration 178/1000 | Loss: 0.00002210
Iteration 179/1000 | Loss: 0.00002210
Iteration 180/1000 | Loss: 0.00002210
Iteration 181/1000 | Loss: 0.00002210
Iteration 182/1000 | Loss: 0.00002210
Iteration 183/1000 | Loss: 0.00002210
Iteration 184/1000 | Loss: 0.00002210
Iteration 185/1000 | Loss: 0.00002210
Iteration 186/1000 | Loss: 0.00002210
Iteration 187/1000 | Loss: 0.00002210
Iteration 188/1000 | Loss: 0.00002210
Iteration 189/1000 | Loss: 0.00002210
Iteration 190/1000 | Loss: 0.00002210
Iteration 191/1000 | Loss: 0.00002210
Iteration 192/1000 | Loss: 0.00002210
Iteration 193/1000 | Loss: 0.00002210
Iteration 194/1000 | Loss: 0.00002210
Iteration 195/1000 | Loss: 0.00002210
Iteration 196/1000 | Loss: 0.00002210
Iteration 197/1000 | Loss: 0.00002210
Iteration 198/1000 | Loss: 0.00002210
Iteration 199/1000 | Loss: 0.00002210
Iteration 200/1000 | Loss: 0.00002210
Iteration 201/1000 | Loss: 0.00002210
Iteration 202/1000 | Loss: 0.00002210
Iteration 203/1000 | Loss: 0.00002210
Iteration 204/1000 | Loss: 0.00002210
Iteration 205/1000 | Loss: 0.00002210
Iteration 206/1000 | Loss: 0.00002210
Iteration 207/1000 | Loss: 0.00002210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [2.2098463887232356e-05, 2.2098463887232356e-05, 2.2098463887232356e-05, 2.2098463887232356e-05, 2.2098463887232356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2098463887232356e-05

Optimization complete. Final v2v error: 3.756702184677124 mm

Highest mean error: 5.607699871063232 mm for frame 82

Lowest mean error: 2.8886022567749023 mm for frame 50

Saving results

Total time: 47.44656801223755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421976
Iteration 2/25 | Loss: 0.00115643
Iteration 3/25 | Loss: 0.00109979
Iteration 4/25 | Loss: 0.00108631
Iteration 5/25 | Loss: 0.00108191
Iteration 6/25 | Loss: 0.00108093
Iteration 7/25 | Loss: 0.00108090
Iteration 8/25 | Loss: 0.00108090
Iteration 9/25 | Loss: 0.00108090
Iteration 10/25 | Loss: 0.00108090
Iteration 11/25 | Loss: 0.00108090
Iteration 12/25 | Loss: 0.00108090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010809027589857578, 0.0010809027589857578, 0.0010809027589857578, 0.0010809027589857578, 0.0010809027589857578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010809027589857578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62226379
Iteration 2/25 | Loss: 0.00082562
Iteration 3/25 | Loss: 0.00082562
Iteration 4/25 | Loss: 0.00082562
Iteration 5/25 | Loss: 0.00082562
Iteration 6/25 | Loss: 0.00082561
Iteration 7/25 | Loss: 0.00082561
Iteration 8/25 | Loss: 0.00082561
Iteration 9/25 | Loss: 0.00082561
Iteration 10/25 | Loss: 0.00082561
Iteration 11/25 | Loss: 0.00082561
Iteration 12/25 | Loss: 0.00082561
Iteration 13/25 | Loss: 0.00082561
Iteration 14/25 | Loss: 0.00082561
Iteration 15/25 | Loss: 0.00082561
Iteration 16/25 | Loss: 0.00082561
Iteration 17/25 | Loss: 0.00082561
Iteration 18/25 | Loss: 0.00082561
Iteration 19/25 | Loss: 0.00082561
Iteration 20/25 | Loss: 0.00082561
Iteration 21/25 | Loss: 0.00082561
Iteration 22/25 | Loss: 0.00082561
Iteration 23/25 | Loss: 0.00082561
Iteration 24/25 | Loss: 0.00082561
Iteration 25/25 | Loss: 0.00082561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082561
Iteration 2/1000 | Loss: 0.00001987
Iteration 3/1000 | Loss: 0.00001421
Iteration 4/1000 | Loss: 0.00001321
Iteration 5/1000 | Loss: 0.00001275
Iteration 6/1000 | Loss: 0.00001248
Iteration 7/1000 | Loss: 0.00001213
Iteration 8/1000 | Loss: 0.00001199
Iteration 9/1000 | Loss: 0.00001199
Iteration 10/1000 | Loss: 0.00001181
Iteration 11/1000 | Loss: 0.00001159
Iteration 12/1000 | Loss: 0.00001159
Iteration 13/1000 | Loss: 0.00001153
Iteration 14/1000 | Loss: 0.00001153
Iteration 15/1000 | Loss: 0.00001152
Iteration 16/1000 | Loss: 0.00001150
Iteration 17/1000 | Loss: 0.00001147
Iteration 18/1000 | Loss: 0.00001146
Iteration 19/1000 | Loss: 0.00001140
Iteration 20/1000 | Loss: 0.00001138
Iteration 21/1000 | Loss: 0.00001138
Iteration 22/1000 | Loss: 0.00001137
Iteration 23/1000 | Loss: 0.00001136
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001131
Iteration 27/1000 | Loss: 0.00001131
Iteration 28/1000 | Loss: 0.00001130
Iteration 29/1000 | Loss: 0.00001129
Iteration 30/1000 | Loss: 0.00001129
Iteration 31/1000 | Loss: 0.00001128
Iteration 32/1000 | Loss: 0.00001127
Iteration 33/1000 | Loss: 0.00001125
Iteration 34/1000 | Loss: 0.00001125
Iteration 35/1000 | Loss: 0.00001124
Iteration 36/1000 | Loss: 0.00001123
Iteration 37/1000 | Loss: 0.00001123
Iteration 38/1000 | Loss: 0.00001123
Iteration 39/1000 | Loss: 0.00001121
Iteration 40/1000 | Loss: 0.00001121
Iteration 41/1000 | Loss: 0.00001121
Iteration 42/1000 | Loss: 0.00001121
Iteration 43/1000 | Loss: 0.00001120
Iteration 44/1000 | Loss: 0.00001120
Iteration 45/1000 | Loss: 0.00001120
Iteration 46/1000 | Loss: 0.00001120
Iteration 47/1000 | Loss: 0.00001120
Iteration 48/1000 | Loss: 0.00001120
Iteration 49/1000 | Loss: 0.00001120
Iteration 50/1000 | Loss: 0.00001120
Iteration 51/1000 | Loss: 0.00001120
Iteration 52/1000 | Loss: 0.00001120
Iteration 53/1000 | Loss: 0.00001120
Iteration 54/1000 | Loss: 0.00001119
Iteration 55/1000 | Loss: 0.00001118
Iteration 56/1000 | Loss: 0.00001118
Iteration 57/1000 | Loss: 0.00001118
Iteration 58/1000 | Loss: 0.00001116
Iteration 59/1000 | Loss: 0.00001115
Iteration 60/1000 | Loss: 0.00001115
Iteration 61/1000 | Loss: 0.00001115
Iteration 62/1000 | Loss: 0.00001114
Iteration 63/1000 | Loss: 0.00001114
Iteration 64/1000 | Loss: 0.00001114
Iteration 65/1000 | Loss: 0.00001113
Iteration 66/1000 | Loss: 0.00001113
Iteration 67/1000 | Loss: 0.00001113
Iteration 68/1000 | Loss: 0.00001112
Iteration 69/1000 | Loss: 0.00001112
Iteration 70/1000 | Loss: 0.00001112
Iteration 71/1000 | Loss: 0.00001112
Iteration 72/1000 | Loss: 0.00001112
Iteration 73/1000 | Loss: 0.00001112
Iteration 74/1000 | Loss: 0.00001112
Iteration 75/1000 | Loss: 0.00001111
Iteration 76/1000 | Loss: 0.00001111
Iteration 77/1000 | Loss: 0.00001111
Iteration 78/1000 | Loss: 0.00001111
Iteration 79/1000 | Loss: 0.00001111
Iteration 80/1000 | Loss: 0.00001111
Iteration 81/1000 | Loss: 0.00001111
Iteration 82/1000 | Loss: 0.00001111
Iteration 83/1000 | Loss: 0.00001111
Iteration 84/1000 | Loss: 0.00001111
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001109
Iteration 92/1000 | Loss: 0.00001109
Iteration 93/1000 | Loss: 0.00001109
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001108
Iteration 96/1000 | Loss: 0.00001108
Iteration 97/1000 | Loss: 0.00001108
Iteration 98/1000 | Loss: 0.00001108
Iteration 99/1000 | Loss: 0.00001107
Iteration 100/1000 | Loss: 0.00001107
Iteration 101/1000 | Loss: 0.00001106
Iteration 102/1000 | Loss: 0.00001106
Iteration 103/1000 | Loss: 0.00001106
Iteration 104/1000 | Loss: 0.00001105
Iteration 105/1000 | Loss: 0.00001105
Iteration 106/1000 | Loss: 0.00001105
Iteration 107/1000 | Loss: 0.00001104
Iteration 108/1000 | Loss: 0.00001104
Iteration 109/1000 | Loss: 0.00001104
Iteration 110/1000 | Loss: 0.00001104
Iteration 111/1000 | Loss: 0.00001104
Iteration 112/1000 | Loss: 0.00001104
Iteration 113/1000 | Loss: 0.00001104
Iteration 114/1000 | Loss: 0.00001104
Iteration 115/1000 | Loss: 0.00001104
Iteration 116/1000 | Loss: 0.00001104
Iteration 117/1000 | Loss: 0.00001104
Iteration 118/1000 | Loss: 0.00001104
Iteration 119/1000 | Loss: 0.00001103
Iteration 120/1000 | Loss: 0.00001103
Iteration 121/1000 | Loss: 0.00001103
Iteration 122/1000 | Loss: 0.00001102
Iteration 123/1000 | Loss: 0.00001102
Iteration 124/1000 | Loss: 0.00001102
Iteration 125/1000 | Loss: 0.00001102
Iteration 126/1000 | Loss: 0.00001101
Iteration 127/1000 | Loss: 0.00001101
Iteration 128/1000 | Loss: 0.00001101
Iteration 129/1000 | Loss: 0.00001101
Iteration 130/1000 | Loss: 0.00001100
Iteration 131/1000 | Loss: 0.00001100
Iteration 132/1000 | Loss: 0.00001100
Iteration 133/1000 | Loss: 0.00001100
Iteration 134/1000 | Loss: 0.00001100
Iteration 135/1000 | Loss: 0.00001100
Iteration 136/1000 | Loss: 0.00001100
Iteration 137/1000 | Loss: 0.00001100
Iteration 138/1000 | Loss: 0.00001100
Iteration 139/1000 | Loss: 0.00001100
Iteration 140/1000 | Loss: 0.00001100
Iteration 141/1000 | Loss: 0.00001100
Iteration 142/1000 | Loss: 0.00001100
Iteration 143/1000 | Loss: 0.00001100
Iteration 144/1000 | Loss: 0.00001100
Iteration 145/1000 | Loss: 0.00001100
Iteration 146/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.0995673619618174e-05, 1.0995673619618174e-05, 1.0995673619618174e-05, 1.0995673619618174e-05, 1.0995673619618174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0995673619618174e-05

Optimization complete. Final v2v error: 2.8600120544433594 mm

Highest mean error: 3.3656258583068848 mm for frame 62

Lowest mean error: 2.6555557250976562 mm for frame 85

Saving results

Total time: 34.02477526664734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00685149
Iteration 2/25 | Loss: 0.00142725
Iteration 3/25 | Loss: 0.00119468
Iteration 4/25 | Loss: 0.00117920
Iteration 5/25 | Loss: 0.00116133
Iteration 6/25 | Loss: 0.00115947
Iteration 7/25 | Loss: 0.00114925
Iteration 8/25 | Loss: 0.00114537
Iteration 9/25 | Loss: 0.00114287
Iteration 10/25 | Loss: 0.00113843
Iteration 11/25 | Loss: 0.00113775
Iteration 12/25 | Loss: 0.00113772
Iteration 13/25 | Loss: 0.00113772
Iteration 14/25 | Loss: 0.00113771
Iteration 15/25 | Loss: 0.00113771
Iteration 16/25 | Loss: 0.00113771
Iteration 17/25 | Loss: 0.00113771
Iteration 18/25 | Loss: 0.00113771
Iteration 19/25 | Loss: 0.00113771
Iteration 20/25 | Loss: 0.00113771
Iteration 21/25 | Loss: 0.00113771
Iteration 22/25 | Loss: 0.00113771
Iteration 23/25 | Loss: 0.00113771
Iteration 24/25 | Loss: 0.00113771
Iteration 25/25 | Loss: 0.00113771

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06920123
Iteration 2/25 | Loss: 0.00099886
Iteration 3/25 | Loss: 0.00099857
Iteration 4/25 | Loss: 0.00099857
Iteration 5/25 | Loss: 0.00099857
Iteration 6/25 | Loss: 0.00099857
Iteration 7/25 | Loss: 0.00099857
Iteration 8/25 | Loss: 0.00099857
Iteration 9/25 | Loss: 0.00099857
Iteration 10/25 | Loss: 0.00099857
Iteration 11/25 | Loss: 0.00099857
Iteration 12/25 | Loss: 0.00099857
Iteration 13/25 | Loss: 0.00099857
Iteration 14/25 | Loss: 0.00099857
Iteration 15/25 | Loss: 0.00099857
Iteration 16/25 | Loss: 0.00099857
Iteration 17/25 | Loss: 0.00099857
Iteration 18/25 | Loss: 0.00099857
Iteration 19/25 | Loss: 0.00099857
Iteration 20/25 | Loss: 0.00099857
Iteration 21/25 | Loss: 0.00099857
Iteration 22/25 | Loss: 0.00099857
Iteration 23/25 | Loss: 0.00099857
Iteration 24/25 | Loss: 0.00099857
Iteration 25/25 | Loss: 0.00099857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099857
Iteration 2/1000 | Loss: 0.00005779
Iteration 3/1000 | Loss: 0.00003627
Iteration 4/1000 | Loss: 0.00002963
Iteration 5/1000 | Loss: 0.00002728
Iteration 6/1000 | Loss: 0.00002574
Iteration 7/1000 | Loss: 0.00002470
Iteration 8/1000 | Loss: 0.00002393
Iteration 9/1000 | Loss: 0.00002325
Iteration 10/1000 | Loss: 0.00002281
Iteration 11/1000 | Loss: 0.00002240
Iteration 12/1000 | Loss: 0.00002209
Iteration 13/1000 | Loss: 0.00002186
Iteration 14/1000 | Loss: 0.00002166
Iteration 15/1000 | Loss: 0.00002148
Iteration 16/1000 | Loss: 0.00002133
Iteration 17/1000 | Loss: 0.00002129
Iteration 18/1000 | Loss: 0.00002127
Iteration 19/1000 | Loss: 0.00002122
Iteration 20/1000 | Loss: 0.00002121
Iteration 21/1000 | Loss: 0.00002121
Iteration 22/1000 | Loss: 0.00002120
Iteration 23/1000 | Loss: 0.00002118
Iteration 24/1000 | Loss: 0.00002118
Iteration 25/1000 | Loss: 0.00002111
Iteration 26/1000 | Loss: 0.00002107
Iteration 27/1000 | Loss: 0.00002106
Iteration 28/1000 | Loss: 0.00002105
Iteration 29/1000 | Loss: 0.00002105
Iteration 30/1000 | Loss: 0.00002104
Iteration 31/1000 | Loss: 0.00002103
Iteration 32/1000 | Loss: 0.00002102
Iteration 33/1000 | Loss: 0.00002101
Iteration 34/1000 | Loss: 0.00002100
Iteration 35/1000 | Loss: 0.00002099
Iteration 36/1000 | Loss: 0.00002097
Iteration 37/1000 | Loss: 0.00002096
Iteration 38/1000 | Loss: 0.00002095
Iteration 39/1000 | Loss: 0.00002094
Iteration 40/1000 | Loss: 0.00002093
Iteration 41/1000 | Loss: 0.00002088
Iteration 42/1000 | Loss: 0.00002088
Iteration 43/1000 | Loss: 0.00002086
Iteration 44/1000 | Loss: 0.00002086
Iteration 45/1000 | Loss: 0.00002085
Iteration 46/1000 | Loss: 0.00002085
Iteration 47/1000 | Loss: 0.00002085
Iteration 48/1000 | Loss: 0.00002084
Iteration 49/1000 | Loss: 0.00002083
Iteration 50/1000 | Loss: 0.00002082
Iteration 51/1000 | Loss: 0.00002082
Iteration 52/1000 | Loss: 0.00002082
Iteration 53/1000 | Loss: 0.00002082
Iteration 54/1000 | Loss: 0.00002082
Iteration 55/1000 | Loss: 0.00002082
Iteration 56/1000 | Loss: 0.00002082
Iteration 57/1000 | Loss: 0.00002080
Iteration 58/1000 | Loss: 0.00002079
Iteration 59/1000 | Loss: 0.00002078
Iteration 60/1000 | Loss: 0.00002078
Iteration 61/1000 | Loss: 0.00002078
Iteration 62/1000 | Loss: 0.00002078
Iteration 63/1000 | Loss: 0.00002077
Iteration 64/1000 | Loss: 0.00002077
Iteration 65/1000 | Loss: 0.00002077
Iteration 66/1000 | Loss: 0.00002077
Iteration 67/1000 | Loss: 0.00002077
Iteration 68/1000 | Loss: 0.00002077
Iteration 69/1000 | Loss: 0.00002077
Iteration 70/1000 | Loss: 0.00002077
Iteration 71/1000 | Loss: 0.00002077
Iteration 72/1000 | Loss: 0.00002076
Iteration 73/1000 | Loss: 0.00002076
Iteration 74/1000 | Loss: 0.00002075
Iteration 75/1000 | Loss: 0.00002075
Iteration 76/1000 | Loss: 0.00002075
Iteration 77/1000 | Loss: 0.00002074
Iteration 78/1000 | Loss: 0.00002074
Iteration 79/1000 | Loss: 0.00002073
Iteration 80/1000 | Loss: 0.00002073
Iteration 81/1000 | Loss: 0.00002073
Iteration 82/1000 | Loss: 0.00002073
Iteration 83/1000 | Loss: 0.00002073
Iteration 84/1000 | Loss: 0.00002073
Iteration 85/1000 | Loss: 0.00002073
Iteration 86/1000 | Loss: 0.00002071
Iteration 87/1000 | Loss: 0.00002071
Iteration 88/1000 | Loss: 0.00002071
Iteration 89/1000 | Loss: 0.00002070
Iteration 90/1000 | Loss: 0.00002070
Iteration 91/1000 | Loss: 0.00002070
Iteration 92/1000 | Loss: 0.00002070
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002069
Iteration 97/1000 | Loss: 0.00002068
Iteration 98/1000 | Loss: 0.00002068
Iteration 99/1000 | Loss: 0.00002068
Iteration 100/1000 | Loss: 0.00002067
Iteration 101/1000 | Loss: 0.00002067
Iteration 102/1000 | Loss: 0.00002067
Iteration 103/1000 | Loss: 0.00002067
Iteration 104/1000 | Loss: 0.00002066
Iteration 105/1000 | Loss: 0.00002066
Iteration 106/1000 | Loss: 0.00002066
Iteration 107/1000 | Loss: 0.00002066
Iteration 108/1000 | Loss: 0.00002066
Iteration 109/1000 | Loss: 0.00002065
Iteration 110/1000 | Loss: 0.00002065
Iteration 111/1000 | Loss: 0.00002065
Iteration 112/1000 | Loss: 0.00002064
Iteration 113/1000 | Loss: 0.00002063
Iteration 114/1000 | Loss: 0.00002063
Iteration 115/1000 | Loss: 0.00002063
Iteration 116/1000 | Loss: 0.00002063
Iteration 117/1000 | Loss: 0.00002063
Iteration 118/1000 | Loss: 0.00002063
Iteration 119/1000 | Loss: 0.00002063
Iteration 120/1000 | Loss: 0.00002063
Iteration 121/1000 | Loss: 0.00002062
Iteration 122/1000 | Loss: 0.00002062
Iteration 123/1000 | Loss: 0.00002062
Iteration 124/1000 | Loss: 0.00002061
Iteration 125/1000 | Loss: 0.00002061
Iteration 126/1000 | Loss: 0.00002060
Iteration 127/1000 | Loss: 0.00002060
Iteration 128/1000 | Loss: 0.00002059
Iteration 129/1000 | Loss: 0.00002059
Iteration 130/1000 | Loss: 0.00002059
Iteration 131/1000 | Loss: 0.00002059
Iteration 132/1000 | Loss: 0.00002058
Iteration 133/1000 | Loss: 0.00002058
Iteration 134/1000 | Loss: 0.00002058
Iteration 135/1000 | Loss: 0.00002058
Iteration 136/1000 | Loss: 0.00002058
Iteration 137/1000 | Loss: 0.00002058
Iteration 138/1000 | Loss: 0.00002058
Iteration 139/1000 | Loss: 0.00002058
Iteration 140/1000 | Loss: 0.00002057
Iteration 141/1000 | Loss: 0.00002057
Iteration 142/1000 | Loss: 0.00002057
Iteration 143/1000 | Loss: 0.00002057
Iteration 144/1000 | Loss: 0.00002057
Iteration 145/1000 | Loss: 0.00002056
Iteration 146/1000 | Loss: 0.00002056
Iteration 147/1000 | Loss: 0.00002056
Iteration 148/1000 | Loss: 0.00002056
Iteration 149/1000 | Loss: 0.00002055
Iteration 150/1000 | Loss: 0.00002055
Iteration 151/1000 | Loss: 0.00002055
Iteration 152/1000 | Loss: 0.00002054
Iteration 153/1000 | Loss: 0.00002054
Iteration 154/1000 | Loss: 0.00002054
Iteration 155/1000 | Loss: 0.00002054
Iteration 156/1000 | Loss: 0.00002054
Iteration 157/1000 | Loss: 0.00002054
Iteration 158/1000 | Loss: 0.00002054
Iteration 159/1000 | Loss: 0.00002054
Iteration 160/1000 | Loss: 0.00002053
Iteration 161/1000 | Loss: 0.00002053
Iteration 162/1000 | Loss: 0.00002053
Iteration 163/1000 | Loss: 0.00002053
Iteration 164/1000 | Loss: 0.00002053
Iteration 165/1000 | Loss: 0.00002052
Iteration 166/1000 | Loss: 0.00002052
Iteration 167/1000 | Loss: 0.00002052
Iteration 168/1000 | Loss: 0.00002052
Iteration 169/1000 | Loss: 0.00002052
Iteration 170/1000 | Loss: 0.00002051
Iteration 171/1000 | Loss: 0.00002051
Iteration 172/1000 | Loss: 0.00002051
Iteration 173/1000 | Loss: 0.00002051
Iteration 174/1000 | Loss: 0.00002051
Iteration 175/1000 | Loss: 0.00002051
Iteration 176/1000 | Loss: 0.00002051
Iteration 177/1000 | Loss: 0.00002051
Iteration 178/1000 | Loss: 0.00002051
Iteration 179/1000 | Loss: 0.00002051
Iteration 180/1000 | Loss: 0.00002051
Iteration 181/1000 | Loss: 0.00002051
Iteration 182/1000 | Loss: 0.00002051
Iteration 183/1000 | Loss: 0.00002051
Iteration 184/1000 | Loss: 0.00002051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.051471710728947e-05, 2.051471710728947e-05, 2.051471710728947e-05, 2.051471710728947e-05, 2.051471710728947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.051471710728947e-05

Optimization complete. Final v2v error: 3.626209259033203 mm

Highest mean error: 5.884881973266602 mm for frame 131

Lowest mean error: 2.725105047225952 mm for frame 190

Saving results

Total time: 66.70942282676697
