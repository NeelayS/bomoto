Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=125, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7000-7055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049172
Iteration 2/25 | Loss: 0.00301674
Iteration 3/25 | Loss: 0.00191174
Iteration 4/25 | Loss: 0.00182157
Iteration 5/25 | Loss: 0.00144188
Iteration 6/25 | Loss: 0.00135953
Iteration 7/25 | Loss: 0.00128686
Iteration 8/25 | Loss: 0.00125033
Iteration 9/25 | Loss: 0.00121063
Iteration 10/25 | Loss: 0.00116210
Iteration 11/25 | Loss: 0.00114227
Iteration 12/25 | Loss: 0.00113619
Iteration 13/25 | Loss: 0.00111242
Iteration 14/25 | Loss: 0.00108849
Iteration 15/25 | Loss: 0.00107147
Iteration 16/25 | Loss: 0.00106005
Iteration 17/25 | Loss: 0.00105411
Iteration 18/25 | Loss: 0.00105018
Iteration 19/25 | Loss: 0.00104898
Iteration 20/25 | Loss: 0.00104749
Iteration 21/25 | Loss: 0.00104444
Iteration 22/25 | Loss: 0.00104352
Iteration 23/25 | Loss: 0.00104548
Iteration 24/25 | Loss: 0.00104460
Iteration 25/25 | Loss: 0.00104387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50115502
Iteration 2/25 | Loss: 0.00261585
Iteration 3/25 | Loss: 0.00261585
Iteration 4/25 | Loss: 0.00261585
Iteration 5/25 | Loss: 0.00261585
Iteration 6/25 | Loss: 0.00261584
Iteration 7/25 | Loss: 0.00261584
Iteration 8/25 | Loss: 0.00261584
Iteration 9/25 | Loss: 0.00261584
Iteration 10/25 | Loss: 0.00261584
Iteration 11/25 | Loss: 0.00261584
Iteration 12/25 | Loss: 0.00261584
Iteration 13/25 | Loss: 0.00261584
Iteration 14/25 | Loss: 0.00261584
Iteration 15/25 | Loss: 0.00261584
Iteration 16/25 | Loss: 0.00261584
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0026158434338867664, 0.0026158434338867664, 0.0026158434338867664, 0.0026158434338867664, 0.0026158434338867664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026158434338867664

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261584
Iteration 2/1000 | Loss: 0.00045728
Iteration 3/1000 | Loss: 0.00027708
Iteration 4/1000 | Loss: 0.00049345
Iteration 5/1000 | Loss: 0.00034185
Iteration 6/1000 | Loss: 0.00026688
Iteration 7/1000 | Loss: 0.00021512
Iteration 8/1000 | Loss: 0.00020125
Iteration 9/1000 | Loss: 0.00033327
Iteration 10/1000 | Loss: 0.00018406
Iteration 11/1000 | Loss: 0.00036371
Iteration 12/1000 | Loss: 0.00067387
Iteration 13/1000 | Loss: 0.00017435
Iteration 14/1000 | Loss: 0.00015874
Iteration 15/1000 | Loss: 0.00015203
Iteration 16/1000 | Loss: 0.00014528
Iteration 17/1000 | Loss: 0.00098751
Iteration 18/1000 | Loss: 0.00082460
Iteration 19/1000 | Loss: 0.00082627
Iteration 20/1000 | Loss: 0.00063068
Iteration 21/1000 | Loss: 0.00053322
Iteration 22/1000 | Loss: 0.00057075
Iteration 23/1000 | Loss: 0.00021430
Iteration 24/1000 | Loss: 0.00049881
Iteration 25/1000 | Loss: 0.00029874
Iteration 26/1000 | Loss: 0.00013834
Iteration 27/1000 | Loss: 0.00013970
Iteration 28/1000 | Loss: 0.00031412
Iteration 29/1000 | Loss: 0.00015158
Iteration 30/1000 | Loss: 0.00012924
Iteration 31/1000 | Loss: 0.00012645
Iteration 32/1000 | Loss: 0.00011844
Iteration 33/1000 | Loss: 0.00026799
Iteration 34/1000 | Loss: 0.00016116
Iteration 35/1000 | Loss: 0.00024921
Iteration 36/1000 | Loss: 0.00013035
Iteration 37/1000 | Loss: 0.00012762
Iteration 38/1000 | Loss: 0.00012929
Iteration 39/1000 | Loss: 0.00012677
Iteration 40/1000 | Loss: 0.00012844
Iteration 41/1000 | Loss: 0.00012384
Iteration 42/1000 | Loss: 0.00012385
Iteration 43/1000 | Loss: 0.00012190
Iteration 44/1000 | Loss: 0.00012023
Iteration 45/1000 | Loss: 0.00012051
Iteration 46/1000 | Loss: 0.00011983
Iteration 47/1000 | Loss: 0.00012045
Iteration 48/1000 | Loss: 0.00011164
Iteration 49/1000 | Loss: 0.00011224
Iteration 50/1000 | Loss: 0.00012133
Iteration 51/1000 | Loss: 0.00011915
Iteration 52/1000 | Loss: 0.00011842
Iteration 53/1000 | Loss: 0.00012348
Iteration 54/1000 | Loss: 0.00011993
Iteration 55/1000 | Loss: 0.00012194
Iteration 56/1000 | Loss: 0.00011416
Iteration 57/1000 | Loss: 0.00013042
Iteration 58/1000 | Loss: 0.00012599
Iteration 59/1000 | Loss: 0.00011422
Iteration 60/1000 | Loss: 0.00011241
Iteration 61/1000 | Loss: 0.00012045
Iteration 62/1000 | Loss: 0.00012902
Iteration 63/1000 | Loss: 0.00011396
Iteration 64/1000 | Loss: 0.00010676
Iteration 65/1000 | Loss: 0.00011498
Iteration 66/1000 | Loss: 0.00012338
Iteration 67/1000 | Loss: 0.00012483
Iteration 68/1000 | Loss: 0.00046860
Iteration 69/1000 | Loss: 0.00103082
Iteration 70/1000 | Loss: 0.00055278
Iteration 71/1000 | Loss: 0.00015691
Iteration 72/1000 | Loss: 0.00013064
Iteration 73/1000 | Loss: 0.00011777
Iteration 74/1000 | Loss: 0.00069556
Iteration 75/1000 | Loss: 0.00036207
Iteration 76/1000 | Loss: 0.00011709
Iteration 77/1000 | Loss: 0.00051076
Iteration 78/1000 | Loss: 0.00043189
Iteration 79/1000 | Loss: 0.00048619
Iteration 80/1000 | Loss: 0.00033787
Iteration 81/1000 | Loss: 0.00035979
Iteration 82/1000 | Loss: 0.00011660
Iteration 83/1000 | Loss: 0.00011972
Iteration 84/1000 | Loss: 0.00011573
Iteration 85/1000 | Loss: 0.00011342
Iteration 86/1000 | Loss: 0.00011352
Iteration 87/1000 | Loss: 0.00064307
Iteration 88/1000 | Loss: 0.00043054
Iteration 89/1000 | Loss: 0.00031244
Iteration 90/1000 | Loss: 0.00030059
Iteration 91/1000 | Loss: 0.00032853
Iteration 92/1000 | Loss: 0.00011517
Iteration 93/1000 | Loss: 0.00012057
Iteration 94/1000 | Loss: 0.00011181
Iteration 95/1000 | Loss: 0.00011785
Iteration 96/1000 | Loss: 0.00011419
Iteration 97/1000 | Loss: 0.00010792
Iteration 98/1000 | Loss: 0.00012159
Iteration 99/1000 | Loss: 0.00011505
Iteration 100/1000 | Loss: 0.00011600
Iteration 101/1000 | Loss: 0.00011679
Iteration 102/1000 | Loss: 0.00011783
Iteration 103/1000 | Loss: 0.00011658
Iteration 104/1000 | Loss: 0.00011889
Iteration 105/1000 | Loss: 0.00011662
Iteration 106/1000 | Loss: 0.00011698
Iteration 107/1000 | Loss: 0.00031337
Iteration 108/1000 | Loss: 0.00016664
Iteration 109/1000 | Loss: 0.00065950
Iteration 110/1000 | Loss: 0.00029190
Iteration 111/1000 | Loss: 0.00021517
Iteration 112/1000 | Loss: 0.00032085
Iteration 113/1000 | Loss: 0.00011140
Iteration 114/1000 | Loss: 0.00010388
Iteration 115/1000 | Loss: 0.00010188
Iteration 116/1000 | Loss: 0.00010051
Iteration 117/1000 | Loss: 0.00023005
Iteration 118/1000 | Loss: 0.00011788
Iteration 119/1000 | Loss: 0.00010671
Iteration 120/1000 | Loss: 0.00045070
Iteration 121/1000 | Loss: 0.00031064
Iteration 122/1000 | Loss: 0.00010340
Iteration 123/1000 | Loss: 0.00010010
Iteration 124/1000 | Loss: 0.00044923
Iteration 125/1000 | Loss: 0.00031494
Iteration 126/1000 | Loss: 0.00115233
Iteration 127/1000 | Loss: 0.00015624
Iteration 128/1000 | Loss: 0.00011080
Iteration 129/1000 | Loss: 0.00058555
Iteration 130/1000 | Loss: 0.00040706
Iteration 131/1000 | Loss: 0.00011143
Iteration 132/1000 | Loss: 0.00010167
Iteration 133/1000 | Loss: 0.00010848
Iteration 134/1000 | Loss: 0.00009544
Iteration 135/1000 | Loss: 0.00009227
Iteration 136/1000 | Loss: 0.00035264
Iteration 137/1000 | Loss: 0.00009107
Iteration 138/1000 | Loss: 0.00008907
Iteration 139/1000 | Loss: 0.00008817
Iteration 140/1000 | Loss: 0.00008723
Iteration 141/1000 | Loss: 0.00008640
Iteration 142/1000 | Loss: 0.00008614
Iteration 143/1000 | Loss: 0.00008586
Iteration 144/1000 | Loss: 0.00032008
Iteration 145/1000 | Loss: 0.00031576
Iteration 146/1000 | Loss: 0.00023002
Iteration 147/1000 | Loss: 0.00011075
Iteration 148/1000 | Loss: 0.00009259
Iteration 149/1000 | Loss: 0.00008826
Iteration 150/1000 | Loss: 0.00028556
Iteration 151/1000 | Loss: 0.00008970
Iteration 152/1000 | Loss: 0.00008431
Iteration 153/1000 | Loss: 0.00008294
Iteration 154/1000 | Loss: 0.00008253
Iteration 155/1000 | Loss: 0.00008206
Iteration 156/1000 | Loss: 0.00008160
Iteration 157/1000 | Loss: 0.00008132
Iteration 158/1000 | Loss: 0.00008103
Iteration 159/1000 | Loss: 0.00008094
Iteration 160/1000 | Loss: 0.00008088
Iteration 161/1000 | Loss: 0.00008087
Iteration 162/1000 | Loss: 0.00008086
Iteration 163/1000 | Loss: 0.00008084
Iteration 164/1000 | Loss: 0.00008084
Iteration 165/1000 | Loss: 0.00008084
Iteration 166/1000 | Loss: 0.00008084
Iteration 167/1000 | Loss: 0.00008084
Iteration 168/1000 | Loss: 0.00008084
Iteration 169/1000 | Loss: 0.00008084
Iteration 170/1000 | Loss: 0.00008084
Iteration 171/1000 | Loss: 0.00008082
Iteration 172/1000 | Loss: 0.00008082
Iteration 173/1000 | Loss: 0.00008081
Iteration 174/1000 | Loss: 0.00008081
Iteration 175/1000 | Loss: 0.00008081
Iteration 176/1000 | Loss: 0.00008080
Iteration 177/1000 | Loss: 0.00008078
Iteration 178/1000 | Loss: 0.00008078
Iteration 179/1000 | Loss: 0.00008077
Iteration 180/1000 | Loss: 0.00008077
Iteration 181/1000 | Loss: 0.00008077
Iteration 182/1000 | Loss: 0.00008076
Iteration 183/1000 | Loss: 0.00008075
Iteration 184/1000 | Loss: 0.00008075
Iteration 185/1000 | Loss: 0.00008075
Iteration 186/1000 | Loss: 0.00008075
Iteration 187/1000 | Loss: 0.00008075
Iteration 188/1000 | Loss: 0.00008075
Iteration 189/1000 | Loss: 0.00008075
Iteration 190/1000 | Loss: 0.00008074
Iteration 191/1000 | Loss: 0.00008074
Iteration 192/1000 | Loss: 0.00008074
Iteration 193/1000 | Loss: 0.00008073
Iteration 194/1000 | Loss: 0.00008070
Iteration 195/1000 | Loss: 0.00008067
Iteration 196/1000 | Loss: 0.00008067
Iteration 197/1000 | Loss: 0.00008067
Iteration 198/1000 | Loss: 0.00008067
Iteration 199/1000 | Loss: 0.00008067
Iteration 200/1000 | Loss: 0.00008067
Iteration 201/1000 | Loss: 0.00008067
Iteration 202/1000 | Loss: 0.00008066
Iteration 203/1000 | Loss: 0.00008066
Iteration 204/1000 | Loss: 0.00008066
Iteration 205/1000 | Loss: 0.00008066
Iteration 206/1000 | Loss: 0.00008066
Iteration 207/1000 | Loss: 0.00008066
Iteration 208/1000 | Loss: 0.00008066
Iteration 209/1000 | Loss: 0.00008066
Iteration 210/1000 | Loss: 0.00008066
Iteration 211/1000 | Loss: 0.00008065
Iteration 212/1000 | Loss: 0.00008065
Iteration 213/1000 | Loss: 0.00008064
Iteration 214/1000 | Loss: 0.00008064
Iteration 215/1000 | Loss: 0.00008064
Iteration 216/1000 | Loss: 0.00008064
Iteration 217/1000 | Loss: 0.00008064
Iteration 218/1000 | Loss: 0.00008064
Iteration 219/1000 | Loss: 0.00008064
Iteration 220/1000 | Loss: 0.00008063
Iteration 221/1000 | Loss: 0.00008063
Iteration 222/1000 | Loss: 0.00008063
Iteration 223/1000 | Loss: 0.00008063
Iteration 224/1000 | Loss: 0.00008063
Iteration 225/1000 | Loss: 0.00008063
Iteration 226/1000 | Loss: 0.00008063
Iteration 227/1000 | Loss: 0.00008063
Iteration 228/1000 | Loss: 0.00008063
Iteration 229/1000 | Loss: 0.00008063
Iteration 230/1000 | Loss: 0.00008063
Iteration 231/1000 | Loss: 0.00008062
Iteration 232/1000 | Loss: 0.00008062
Iteration 233/1000 | Loss: 0.00008062
Iteration 234/1000 | Loss: 0.00008061
Iteration 235/1000 | Loss: 0.00008061
Iteration 236/1000 | Loss: 0.00008061
Iteration 237/1000 | Loss: 0.00008061
Iteration 238/1000 | Loss: 0.00008061
Iteration 239/1000 | Loss: 0.00008060
Iteration 240/1000 | Loss: 0.00008060
Iteration 241/1000 | Loss: 0.00008060
Iteration 242/1000 | Loss: 0.00008060
Iteration 243/1000 | Loss: 0.00008060
Iteration 244/1000 | Loss: 0.00008060
Iteration 245/1000 | Loss: 0.00008060
Iteration 246/1000 | Loss: 0.00008060
Iteration 247/1000 | Loss: 0.00008060
Iteration 248/1000 | Loss: 0.00008059
Iteration 249/1000 | Loss: 0.00008059
Iteration 250/1000 | Loss: 0.00008059
Iteration 251/1000 | Loss: 0.00008059
Iteration 252/1000 | Loss: 0.00008059
Iteration 253/1000 | Loss: 0.00008059
Iteration 254/1000 | Loss: 0.00008059
Iteration 255/1000 | Loss: 0.00008059
Iteration 256/1000 | Loss: 0.00008059
Iteration 257/1000 | Loss: 0.00008059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [8.059431274887174e-05, 8.059431274887174e-05, 8.059431274887174e-05, 8.059431274887174e-05, 8.059431274887174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.059431274887174e-05

Optimization complete. Final v2v error: 4.974064350128174 mm

Highest mean error: 12.116379737854004 mm for frame 19

Lowest mean error: 3.3273963928222656 mm for frame 6

Saving results

Total time: 319.58086705207825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403694
Iteration 2/25 | Loss: 0.00086204
Iteration 3/25 | Loss: 0.00068217
Iteration 4/25 | Loss: 0.00065490
Iteration 5/25 | Loss: 0.00064664
Iteration 6/25 | Loss: 0.00064425
Iteration 7/25 | Loss: 0.00064368
Iteration 8/25 | Loss: 0.00064368
Iteration 9/25 | Loss: 0.00064368
Iteration 10/25 | Loss: 0.00064368
Iteration 11/25 | Loss: 0.00064368
Iteration 12/25 | Loss: 0.00064368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006436801631934941, 0.0006436801631934941, 0.0006436801631934941, 0.0006436801631934941, 0.0006436801631934941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006436801631934941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58560681
Iteration 2/25 | Loss: 0.00030931
Iteration 3/25 | Loss: 0.00030931
Iteration 4/25 | Loss: 0.00030931
Iteration 5/25 | Loss: 0.00030931
Iteration 6/25 | Loss: 0.00030931
Iteration 7/25 | Loss: 0.00030931
Iteration 8/25 | Loss: 0.00030931
Iteration 9/25 | Loss: 0.00030931
Iteration 10/25 | Loss: 0.00030931
Iteration 11/25 | Loss: 0.00030931
Iteration 12/25 | Loss: 0.00030931
Iteration 13/25 | Loss: 0.00030931
Iteration 14/25 | Loss: 0.00030931
Iteration 15/25 | Loss: 0.00030931
Iteration 16/25 | Loss: 0.00030931
Iteration 17/25 | Loss: 0.00030931
Iteration 18/25 | Loss: 0.00030931
Iteration 19/25 | Loss: 0.00030931
Iteration 20/25 | Loss: 0.00030931
Iteration 21/25 | Loss: 0.00030931
Iteration 22/25 | Loss: 0.00030931
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003093066916335374, 0.0003093066916335374, 0.0003093066916335374, 0.0003093066916335374, 0.0003093066916335374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003093066916335374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030931
Iteration 2/1000 | Loss: 0.00002278
Iteration 3/1000 | Loss: 0.00001475
Iteration 4/1000 | Loss: 0.00001304
Iteration 5/1000 | Loss: 0.00001235
Iteration 6/1000 | Loss: 0.00001166
Iteration 7/1000 | Loss: 0.00001139
Iteration 8/1000 | Loss: 0.00001137
Iteration 9/1000 | Loss: 0.00001113
Iteration 10/1000 | Loss: 0.00001097
Iteration 11/1000 | Loss: 0.00001097
Iteration 12/1000 | Loss: 0.00001093
Iteration 13/1000 | Loss: 0.00001089
Iteration 14/1000 | Loss: 0.00001084
Iteration 15/1000 | Loss: 0.00001081
Iteration 16/1000 | Loss: 0.00001074
Iteration 17/1000 | Loss: 0.00001069
Iteration 18/1000 | Loss: 0.00001067
Iteration 19/1000 | Loss: 0.00001067
Iteration 20/1000 | Loss: 0.00001066
Iteration 21/1000 | Loss: 0.00001066
Iteration 22/1000 | Loss: 0.00001066
Iteration 23/1000 | Loss: 0.00001065
Iteration 24/1000 | Loss: 0.00001065
Iteration 25/1000 | Loss: 0.00001065
Iteration 26/1000 | Loss: 0.00001065
Iteration 27/1000 | Loss: 0.00001064
Iteration 28/1000 | Loss: 0.00001064
Iteration 29/1000 | Loss: 0.00001064
Iteration 30/1000 | Loss: 0.00001064
Iteration 31/1000 | Loss: 0.00001064
Iteration 32/1000 | Loss: 0.00001063
Iteration 33/1000 | Loss: 0.00001063
Iteration 34/1000 | Loss: 0.00001063
Iteration 35/1000 | Loss: 0.00001062
Iteration 36/1000 | Loss: 0.00001062
Iteration 37/1000 | Loss: 0.00001062
Iteration 38/1000 | Loss: 0.00001062
Iteration 39/1000 | Loss: 0.00001062
Iteration 40/1000 | Loss: 0.00001062
Iteration 41/1000 | Loss: 0.00001061
Iteration 42/1000 | Loss: 0.00001061
Iteration 43/1000 | Loss: 0.00001061
Iteration 44/1000 | Loss: 0.00001061
Iteration 45/1000 | Loss: 0.00001061
Iteration 46/1000 | Loss: 0.00001061
Iteration 47/1000 | Loss: 0.00001060
Iteration 48/1000 | Loss: 0.00001060
Iteration 49/1000 | Loss: 0.00001060
Iteration 50/1000 | Loss: 0.00001060
Iteration 51/1000 | Loss: 0.00001060
Iteration 52/1000 | Loss: 0.00001059
Iteration 53/1000 | Loss: 0.00001058
Iteration 54/1000 | Loss: 0.00001058
Iteration 55/1000 | Loss: 0.00001058
Iteration 56/1000 | Loss: 0.00001057
Iteration 57/1000 | Loss: 0.00001057
Iteration 58/1000 | Loss: 0.00001057
Iteration 59/1000 | Loss: 0.00001057
Iteration 60/1000 | Loss: 0.00001056
Iteration 61/1000 | Loss: 0.00001056
Iteration 62/1000 | Loss: 0.00001056
Iteration 63/1000 | Loss: 0.00001056
Iteration 64/1000 | Loss: 0.00001055
Iteration 65/1000 | Loss: 0.00001055
Iteration 66/1000 | Loss: 0.00001055
Iteration 67/1000 | Loss: 0.00001055
Iteration 68/1000 | Loss: 0.00001055
Iteration 69/1000 | Loss: 0.00001055
Iteration 70/1000 | Loss: 0.00001055
Iteration 71/1000 | Loss: 0.00001055
Iteration 72/1000 | Loss: 0.00001055
Iteration 73/1000 | Loss: 0.00001054
Iteration 74/1000 | Loss: 0.00001054
Iteration 75/1000 | Loss: 0.00001054
Iteration 76/1000 | Loss: 0.00001054
Iteration 77/1000 | Loss: 0.00001054
Iteration 78/1000 | Loss: 0.00001054
Iteration 79/1000 | Loss: 0.00001053
Iteration 80/1000 | Loss: 0.00001053
Iteration 81/1000 | Loss: 0.00001053
Iteration 82/1000 | Loss: 0.00001053
Iteration 83/1000 | Loss: 0.00001053
Iteration 84/1000 | Loss: 0.00001052
Iteration 85/1000 | Loss: 0.00001052
Iteration 86/1000 | Loss: 0.00001052
Iteration 87/1000 | Loss: 0.00001052
Iteration 88/1000 | Loss: 0.00001052
Iteration 89/1000 | Loss: 0.00001052
Iteration 90/1000 | Loss: 0.00001051
Iteration 91/1000 | Loss: 0.00001051
Iteration 92/1000 | Loss: 0.00001051
Iteration 93/1000 | Loss: 0.00001051
Iteration 94/1000 | Loss: 0.00001051
Iteration 95/1000 | Loss: 0.00001051
Iteration 96/1000 | Loss: 0.00001050
Iteration 97/1000 | Loss: 0.00001050
Iteration 98/1000 | Loss: 0.00001050
Iteration 99/1000 | Loss: 0.00001049
Iteration 100/1000 | Loss: 0.00001049
Iteration 101/1000 | Loss: 0.00001049
Iteration 102/1000 | Loss: 0.00001049
Iteration 103/1000 | Loss: 0.00001049
Iteration 104/1000 | Loss: 0.00001049
Iteration 105/1000 | Loss: 0.00001049
Iteration 106/1000 | Loss: 0.00001049
Iteration 107/1000 | Loss: 0.00001049
Iteration 108/1000 | Loss: 0.00001049
Iteration 109/1000 | Loss: 0.00001048
Iteration 110/1000 | Loss: 0.00001048
Iteration 111/1000 | Loss: 0.00001048
Iteration 112/1000 | Loss: 0.00001048
Iteration 113/1000 | Loss: 0.00001048
Iteration 114/1000 | Loss: 0.00001047
Iteration 115/1000 | Loss: 0.00001047
Iteration 116/1000 | Loss: 0.00001047
Iteration 117/1000 | Loss: 0.00001047
Iteration 118/1000 | Loss: 0.00001047
Iteration 119/1000 | Loss: 0.00001047
Iteration 120/1000 | Loss: 0.00001047
Iteration 121/1000 | Loss: 0.00001047
Iteration 122/1000 | Loss: 0.00001047
Iteration 123/1000 | Loss: 0.00001047
Iteration 124/1000 | Loss: 0.00001047
Iteration 125/1000 | Loss: 0.00001047
Iteration 126/1000 | Loss: 0.00001047
Iteration 127/1000 | Loss: 0.00001046
Iteration 128/1000 | Loss: 0.00001046
Iteration 129/1000 | Loss: 0.00001046
Iteration 130/1000 | Loss: 0.00001046
Iteration 131/1000 | Loss: 0.00001046
Iteration 132/1000 | Loss: 0.00001046
Iteration 133/1000 | Loss: 0.00001046
Iteration 134/1000 | Loss: 0.00001046
Iteration 135/1000 | Loss: 0.00001046
Iteration 136/1000 | Loss: 0.00001046
Iteration 137/1000 | Loss: 0.00001046
Iteration 138/1000 | Loss: 0.00001046
Iteration 139/1000 | Loss: 0.00001046
Iteration 140/1000 | Loss: 0.00001046
Iteration 141/1000 | Loss: 0.00001046
Iteration 142/1000 | Loss: 0.00001045
Iteration 143/1000 | Loss: 0.00001045
Iteration 144/1000 | Loss: 0.00001045
Iteration 145/1000 | Loss: 0.00001045
Iteration 146/1000 | Loss: 0.00001045
Iteration 147/1000 | Loss: 0.00001045
Iteration 148/1000 | Loss: 0.00001045
Iteration 149/1000 | Loss: 0.00001045
Iteration 150/1000 | Loss: 0.00001045
Iteration 151/1000 | Loss: 0.00001045
Iteration 152/1000 | Loss: 0.00001045
Iteration 153/1000 | Loss: 0.00001045
Iteration 154/1000 | Loss: 0.00001045
Iteration 155/1000 | Loss: 0.00001045
Iteration 156/1000 | Loss: 0.00001044
Iteration 157/1000 | Loss: 0.00001044
Iteration 158/1000 | Loss: 0.00001044
Iteration 159/1000 | Loss: 0.00001044
Iteration 160/1000 | Loss: 0.00001044
Iteration 161/1000 | Loss: 0.00001043
Iteration 162/1000 | Loss: 0.00001043
Iteration 163/1000 | Loss: 0.00001043
Iteration 164/1000 | Loss: 0.00001043
Iteration 165/1000 | Loss: 0.00001043
Iteration 166/1000 | Loss: 0.00001043
Iteration 167/1000 | Loss: 0.00001043
Iteration 168/1000 | Loss: 0.00001043
Iteration 169/1000 | Loss: 0.00001043
Iteration 170/1000 | Loss: 0.00001043
Iteration 171/1000 | Loss: 0.00001042
Iteration 172/1000 | Loss: 0.00001042
Iteration 173/1000 | Loss: 0.00001042
Iteration 174/1000 | Loss: 0.00001042
Iteration 175/1000 | Loss: 0.00001042
Iteration 176/1000 | Loss: 0.00001042
Iteration 177/1000 | Loss: 0.00001042
Iteration 178/1000 | Loss: 0.00001041
Iteration 179/1000 | Loss: 0.00001041
Iteration 180/1000 | Loss: 0.00001041
Iteration 181/1000 | Loss: 0.00001041
Iteration 182/1000 | Loss: 0.00001041
Iteration 183/1000 | Loss: 0.00001041
Iteration 184/1000 | Loss: 0.00001041
Iteration 185/1000 | Loss: 0.00001040
Iteration 186/1000 | Loss: 0.00001040
Iteration 187/1000 | Loss: 0.00001040
Iteration 188/1000 | Loss: 0.00001040
Iteration 189/1000 | Loss: 0.00001040
Iteration 190/1000 | Loss: 0.00001040
Iteration 191/1000 | Loss: 0.00001040
Iteration 192/1000 | Loss: 0.00001040
Iteration 193/1000 | Loss: 0.00001040
Iteration 194/1000 | Loss: 0.00001040
Iteration 195/1000 | Loss: 0.00001039
Iteration 196/1000 | Loss: 0.00001039
Iteration 197/1000 | Loss: 0.00001039
Iteration 198/1000 | Loss: 0.00001039
Iteration 199/1000 | Loss: 0.00001039
Iteration 200/1000 | Loss: 0.00001039
Iteration 201/1000 | Loss: 0.00001039
Iteration 202/1000 | Loss: 0.00001039
Iteration 203/1000 | Loss: 0.00001038
Iteration 204/1000 | Loss: 0.00001038
Iteration 205/1000 | Loss: 0.00001038
Iteration 206/1000 | Loss: 0.00001038
Iteration 207/1000 | Loss: 0.00001038
Iteration 208/1000 | Loss: 0.00001037
Iteration 209/1000 | Loss: 0.00001037
Iteration 210/1000 | Loss: 0.00001037
Iteration 211/1000 | Loss: 0.00001037
Iteration 212/1000 | Loss: 0.00001036
Iteration 213/1000 | Loss: 0.00001036
Iteration 214/1000 | Loss: 0.00001036
Iteration 215/1000 | Loss: 0.00001036
Iteration 216/1000 | Loss: 0.00001036
Iteration 217/1000 | Loss: 0.00001036
Iteration 218/1000 | Loss: 0.00001036
Iteration 219/1000 | Loss: 0.00001035
Iteration 220/1000 | Loss: 0.00001035
Iteration 221/1000 | Loss: 0.00001035
Iteration 222/1000 | Loss: 0.00001035
Iteration 223/1000 | Loss: 0.00001035
Iteration 224/1000 | Loss: 0.00001035
Iteration 225/1000 | Loss: 0.00001034
Iteration 226/1000 | Loss: 0.00001034
Iteration 227/1000 | Loss: 0.00001034
Iteration 228/1000 | Loss: 0.00001034
Iteration 229/1000 | Loss: 0.00001034
Iteration 230/1000 | Loss: 0.00001034
Iteration 231/1000 | Loss: 0.00001034
Iteration 232/1000 | Loss: 0.00001034
Iteration 233/1000 | Loss: 0.00001034
Iteration 234/1000 | Loss: 0.00001033
Iteration 235/1000 | Loss: 0.00001033
Iteration 236/1000 | Loss: 0.00001033
Iteration 237/1000 | Loss: 0.00001033
Iteration 238/1000 | Loss: 0.00001032
Iteration 239/1000 | Loss: 0.00001032
Iteration 240/1000 | Loss: 0.00001032
Iteration 241/1000 | Loss: 0.00001032
Iteration 242/1000 | Loss: 0.00001032
Iteration 243/1000 | Loss: 0.00001032
Iteration 244/1000 | Loss: 0.00001032
Iteration 245/1000 | Loss: 0.00001032
Iteration 246/1000 | Loss: 0.00001032
Iteration 247/1000 | Loss: 0.00001032
Iteration 248/1000 | Loss: 0.00001032
Iteration 249/1000 | Loss: 0.00001032
Iteration 250/1000 | Loss: 0.00001032
Iteration 251/1000 | Loss: 0.00001032
Iteration 252/1000 | Loss: 0.00001032
Iteration 253/1000 | Loss: 0.00001032
Iteration 254/1000 | Loss: 0.00001032
Iteration 255/1000 | Loss: 0.00001032
Iteration 256/1000 | Loss: 0.00001032
Iteration 257/1000 | Loss: 0.00001032
Iteration 258/1000 | Loss: 0.00001032
Iteration 259/1000 | Loss: 0.00001032
Iteration 260/1000 | Loss: 0.00001032
Iteration 261/1000 | Loss: 0.00001032
Iteration 262/1000 | Loss: 0.00001032
Iteration 263/1000 | Loss: 0.00001032
Iteration 264/1000 | Loss: 0.00001032
Iteration 265/1000 | Loss: 0.00001032
Iteration 266/1000 | Loss: 0.00001032
Iteration 267/1000 | Loss: 0.00001032
Iteration 268/1000 | Loss: 0.00001032
Iteration 269/1000 | Loss: 0.00001032
Iteration 270/1000 | Loss: 0.00001032
Iteration 271/1000 | Loss: 0.00001032
Iteration 272/1000 | Loss: 0.00001032
Iteration 273/1000 | Loss: 0.00001032
Iteration 274/1000 | Loss: 0.00001032
Iteration 275/1000 | Loss: 0.00001032
Iteration 276/1000 | Loss: 0.00001032
Iteration 277/1000 | Loss: 0.00001032
Iteration 278/1000 | Loss: 0.00001032
Iteration 279/1000 | Loss: 0.00001032
Iteration 280/1000 | Loss: 0.00001032
Iteration 281/1000 | Loss: 0.00001032
Iteration 282/1000 | Loss: 0.00001032
Iteration 283/1000 | Loss: 0.00001032
Iteration 284/1000 | Loss: 0.00001032
Iteration 285/1000 | Loss: 0.00001032
Iteration 286/1000 | Loss: 0.00001032
Iteration 287/1000 | Loss: 0.00001032
Iteration 288/1000 | Loss: 0.00001032
Iteration 289/1000 | Loss: 0.00001032
Iteration 290/1000 | Loss: 0.00001032
Iteration 291/1000 | Loss: 0.00001032
Iteration 292/1000 | Loss: 0.00001032
Iteration 293/1000 | Loss: 0.00001032
Iteration 294/1000 | Loss: 0.00001032
Iteration 295/1000 | Loss: 0.00001032
Iteration 296/1000 | Loss: 0.00001032
Iteration 297/1000 | Loss: 0.00001032
Iteration 298/1000 | Loss: 0.00001032
Iteration 299/1000 | Loss: 0.00001032
Iteration 300/1000 | Loss: 0.00001032
Iteration 301/1000 | Loss: 0.00001032
Iteration 302/1000 | Loss: 0.00001032
Iteration 303/1000 | Loss: 0.00001032
Iteration 304/1000 | Loss: 0.00001032
Iteration 305/1000 | Loss: 0.00001032
Iteration 306/1000 | Loss: 0.00001032
Iteration 307/1000 | Loss: 0.00001032
Iteration 308/1000 | Loss: 0.00001032
Iteration 309/1000 | Loss: 0.00001032
Iteration 310/1000 | Loss: 0.00001032
Iteration 311/1000 | Loss: 0.00001032
Iteration 312/1000 | Loss: 0.00001032
Iteration 313/1000 | Loss: 0.00001032
Iteration 314/1000 | Loss: 0.00001032
Iteration 315/1000 | Loss: 0.00001032
Iteration 316/1000 | Loss: 0.00001032
Iteration 317/1000 | Loss: 0.00001032
Iteration 318/1000 | Loss: 0.00001032
Iteration 319/1000 | Loss: 0.00001032
Iteration 320/1000 | Loss: 0.00001032
Iteration 321/1000 | Loss: 0.00001032
Iteration 322/1000 | Loss: 0.00001032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 322. Stopping optimization.
Last 5 losses: [1.031747797242133e-05, 1.031747797242133e-05, 1.031747797242133e-05, 1.031747797242133e-05, 1.031747797242133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.031747797242133e-05

Optimization complete. Final v2v error: 2.69260573387146 mm

Highest mean error: 3.7667198181152344 mm for frame 169

Lowest mean error: 2.4367928504943848 mm for frame 94

Saving results

Total time: 51.14033007621765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009842
Iteration 2/25 | Loss: 0.00175149
Iteration 3/25 | Loss: 0.00118409
Iteration 4/25 | Loss: 0.00104904
Iteration 5/25 | Loss: 0.00101289
Iteration 6/25 | Loss: 0.00102747
Iteration 7/25 | Loss: 0.00197892
Iteration 8/25 | Loss: 0.00092791
Iteration 9/25 | Loss: 0.00073161
Iteration 10/25 | Loss: 0.00069878
Iteration 11/25 | Loss: 0.00068875
Iteration 12/25 | Loss: 0.00068318
Iteration 13/25 | Loss: 0.00068106
Iteration 14/25 | Loss: 0.00068003
Iteration 15/25 | Loss: 0.00067961
Iteration 16/25 | Loss: 0.00067935
Iteration 17/25 | Loss: 0.00067913
Iteration 18/25 | Loss: 0.00067908
Iteration 19/25 | Loss: 0.00067907
Iteration 20/25 | Loss: 0.00067907
Iteration 21/25 | Loss: 0.00067907
Iteration 22/25 | Loss: 0.00067907
Iteration 23/25 | Loss: 0.00067907
Iteration 24/25 | Loss: 0.00067907
Iteration 25/25 | Loss: 0.00067907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45226228
Iteration 2/25 | Loss: 0.00032089
Iteration 3/25 | Loss: 0.00032089
Iteration 4/25 | Loss: 0.00032089
Iteration 5/25 | Loss: 0.00032089
Iteration 6/25 | Loss: 0.00032089
Iteration 7/25 | Loss: 0.00032089
Iteration 8/25 | Loss: 0.00032089
Iteration 9/25 | Loss: 0.00032089
Iteration 10/25 | Loss: 0.00032088
Iteration 11/25 | Loss: 0.00032088
Iteration 12/25 | Loss: 0.00032088
Iteration 13/25 | Loss: 0.00032088
Iteration 14/25 | Loss: 0.00032088
Iteration 15/25 | Loss: 0.00032088
Iteration 16/25 | Loss: 0.00032088
Iteration 17/25 | Loss: 0.00032088
Iteration 18/25 | Loss: 0.00032088
Iteration 19/25 | Loss: 0.00032088
Iteration 20/25 | Loss: 0.00032088
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003208848647773266, 0.0003208848647773266, 0.0003208848647773266, 0.0003208848647773266, 0.0003208848647773266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003208848647773266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032088
Iteration 2/1000 | Loss: 0.00001744
Iteration 3/1000 | Loss: 0.00001493
Iteration 4/1000 | Loss: 0.00001346
Iteration 5/1000 | Loss: 0.00001281
Iteration 6/1000 | Loss: 0.00001237
Iteration 7/1000 | Loss: 0.00001218
Iteration 8/1000 | Loss: 0.00001206
Iteration 9/1000 | Loss: 0.00001204
Iteration 10/1000 | Loss: 0.00001195
Iteration 11/1000 | Loss: 0.00001195
Iteration 12/1000 | Loss: 0.00001194
Iteration 13/1000 | Loss: 0.00001193
Iteration 14/1000 | Loss: 0.00001192
Iteration 15/1000 | Loss: 0.00001192
Iteration 16/1000 | Loss: 0.00001192
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001191
Iteration 19/1000 | Loss: 0.00001190
Iteration 20/1000 | Loss: 0.00001190
Iteration 21/1000 | Loss: 0.00001189
Iteration 22/1000 | Loss: 0.00001189
Iteration 23/1000 | Loss: 0.00001189
Iteration 24/1000 | Loss: 0.00001188
Iteration 25/1000 | Loss: 0.00001188
Iteration 26/1000 | Loss: 0.00001188
Iteration 27/1000 | Loss: 0.00001188
Iteration 28/1000 | Loss: 0.00001187
Iteration 29/1000 | Loss: 0.00001187
Iteration 30/1000 | Loss: 0.00001187
Iteration 31/1000 | Loss: 0.00001187
Iteration 32/1000 | Loss: 0.00001187
Iteration 33/1000 | Loss: 0.00001187
Iteration 34/1000 | Loss: 0.00001187
Iteration 35/1000 | Loss: 0.00001186
Iteration 36/1000 | Loss: 0.00001186
Iteration 37/1000 | Loss: 0.00001186
Iteration 38/1000 | Loss: 0.00001186
Iteration 39/1000 | Loss: 0.00001186
Iteration 40/1000 | Loss: 0.00001185
Iteration 41/1000 | Loss: 0.00001185
Iteration 42/1000 | Loss: 0.00001185
Iteration 43/1000 | Loss: 0.00001185
Iteration 44/1000 | Loss: 0.00001185
Iteration 45/1000 | Loss: 0.00001184
Iteration 46/1000 | Loss: 0.00001184
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001184
Iteration 49/1000 | Loss: 0.00001184
Iteration 50/1000 | Loss: 0.00001184
Iteration 51/1000 | Loss: 0.00001184
Iteration 52/1000 | Loss: 0.00001184
Iteration 53/1000 | Loss: 0.00001184
Iteration 54/1000 | Loss: 0.00001183
Iteration 55/1000 | Loss: 0.00001183
Iteration 56/1000 | Loss: 0.00001183
Iteration 57/1000 | Loss: 0.00001183
Iteration 58/1000 | Loss: 0.00001183
Iteration 59/1000 | Loss: 0.00001183
Iteration 60/1000 | Loss: 0.00001183
Iteration 61/1000 | Loss: 0.00001183
Iteration 62/1000 | Loss: 0.00001183
Iteration 63/1000 | Loss: 0.00001183
Iteration 64/1000 | Loss: 0.00001183
Iteration 65/1000 | Loss: 0.00001183
Iteration 66/1000 | Loss: 0.00001183
Iteration 67/1000 | Loss: 0.00001183
Iteration 68/1000 | Loss: 0.00001183
Iteration 69/1000 | Loss: 0.00001182
Iteration 70/1000 | Loss: 0.00001182
Iteration 71/1000 | Loss: 0.00001182
Iteration 72/1000 | Loss: 0.00001182
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001182
Iteration 77/1000 | Loss: 0.00001182
Iteration 78/1000 | Loss: 0.00001182
Iteration 79/1000 | Loss: 0.00001182
Iteration 80/1000 | Loss: 0.00001182
Iteration 81/1000 | Loss: 0.00001181
Iteration 82/1000 | Loss: 0.00001181
Iteration 83/1000 | Loss: 0.00001181
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001181
Iteration 86/1000 | Loss: 0.00001181
Iteration 87/1000 | Loss: 0.00001181
Iteration 88/1000 | Loss: 0.00001181
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001181
Iteration 92/1000 | Loss: 0.00001181
Iteration 93/1000 | Loss: 0.00001181
Iteration 94/1000 | Loss: 0.00001181
Iteration 95/1000 | Loss: 0.00001181
Iteration 96/1000 | Loss: 0.00001181
Iteration 97/1000 | Loss: 0.00001181
Iteration 98/1000 | Loss: 0.00001181
Iteration 99/1000 | Loss: 0.00001181
Iteration 100/1000 | Loss: 0.00001181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.1805976100731641e-05, 1.1805976100731641e-05, 1.1805976100731641e-05, 1.1805976100731641e-05, 1.1805976100731641e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1805976100731641e-05

Optimization complete. Final v2v error: 2.8913140296936035 mm

Highest mean error: 2.9533395767211914 mm for frame 165

Lowest mean error: 2.843149185180664 mm for frame 109

Saving results

Total time: 50.420161485672
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849279
Iteration 2/25 | Loss: 0.00111462
Iteration 3/25 | Loss: 0.00080313
Iteration 4/25 | Loss: 0.00077844
Iteration 5/25 | Loss: 0.00077307
Iteration 6/25 | Loss: 0.00077173
Iteration 7/25 | Loss: 0.00077150
Iteration 8/25 | Loss: 0.00077150
Iteration 9/25 | Loss: 0.00077150
Iteration 10/25 | Loss: 0.00077150
Iteration 11/25 | Loss: 0.00077150
Iteration 12/25 | Loss: 0.00077150
Iteration 13/25 | Loss: 0.00077150
Iteration 14/25 | Loss: 0.00077150
Iteration 15/25 | Loss: 0.00077150
Iteration 16/25 | Loss: 0.00077150
Iteration 17/25 | Loss: 0.00077150
Iteration 18/25 | Loss: 0.00077150
Iteration 19/25 | Loss: 0.00077150
Iteration 20/25 | Loss: 0.00077150
Iteration 21/25 | Loss: 0.00077150
Iteration 22/25 | Loss: 0.00077150
Iteration 23/25 | Loss: 0.00077150
Iteration 24/25 | Loss: 0.00077150
Iteration 25/25 | Loss: 0.00077150

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05776978
Iteration 2/25 | Loss: 0.00034094
Iteration 3/25 | Loss: 0.00034093
Iteration 4/25 | Loss: 0.00034093
Iteration 5/25 | Loss: 0.00034093
Iteration 6/25 | Loss: 0.00034093
Iteration 7/25 | Loss: 0.00034093
Iteration 8/25 | Loss: 0.00034093
Iteration 9/25 | Loss: 0.00034093
Iteration 10/25 | Loss: 0.00034093
Iteration 11/25 | Loss: 0.00034093
Iteration 12/25 | Loss: 0.00034093
Iteration 13/25 | Loss: 0.00034093
Iteration 14/25 | Loss: 0.00034093
Iteration 15/25 | Loss: 0.00034093
Iteration 16/25 | Loss: 0.00034093
Iteration 17/25 | Loss: 0.00034093
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003409291966818273, 0.0003409291966818273, 0.0003409291966818273, 0.0003409291966818273, 0.0003409291966818273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003409291966818273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034093
Iteration 2/1000 | Loss: 0.00003144
Iteration 3/1000 | Loss: 0.00002617
Iteration 4/1000 | Loss: 0.00002451
Iteration 5/1000 | Loss: 0.00002289
Iteration 6/1000 | Loss: 0.00002223
Iteration 7/1000 | Loss: 0.00002158
Iteration 8/1000 | Loss: 0.00002130
Iteration 9/1000 | Loss: 0.00002106
Iteration 10/1000 | Loss: 0.00002105
Iteration 11/1000 | Loss: 0.00002088
Iteration 12/1000 | Loss: 0.00002083
Iteration 13/1000 | Loss: 0.00002083
Iteration 14/1000 | Loss: 0.00002079
Iteration 15/1000 | Loss: 0.00002079
Iteration 16/1000 | Loss: 0.00002079
Iteration 17/1000 | Loss: 0.00002077
Iteration 18/1000 | Loss: 0.00002076
Iteration 19/1000 | Loss: 0.00002073
Iteration 20/1000 | Loss: 0.00002073
Iteration 21/1000 | Loss: 0.00002073
Iteration 22/1000 | Loss: 0.00002073
Iteration 23/1000 | Loss: 0.00002073
Iteration 24/1000 | Loss: 0.00002072
Iteration 25/1000 | Loss: 0.00002072
Iteration 26/1000 | Loss: 0.00002072
Iteration 27/1000 | Loss: 0.00002072
Iteration 28/1000 | Loss: 0.00002072
Iteration 29/1000 | Loss: 0.00002072
Iteration 30/1000 | Loss: 0.00002071
Iteration 31/1000 | Loss: 0.00002071
Iteration 32/1000 | Loss: 0.00002071
Iteration 33/1000 | Loss: 0.00002070
Iteration 34/1000 | Loss: 0.00002070
Iteration 35/1000 | Loss: 0.00002070
Iteration 36/1000 | Loss: 0.00002070
Iteration 37/1000 | Loss: 0.00002070
Iteration 38/1000 | Loss: 0.00002070
Iteration 39/1000 | Loss: 0.00002070
Iteration 40/1000 | Loss: 0.00002070
Iteration 41/1000 | Loss: 0.00002070
Iteration 42/1000 | Loss: 0.00002069
Iteration 43/1000 | Loss: 0.00002069
Iteration 44/1000 | Loss: 0.00002068
Iteration 45/1000 | Loss: 0.00002068
Iteration 46/1000 | Loss: 0.00002068
Iteration 47/1000 | Loss: 0.00002067
Iteration 48/1000 | Loss: 0.00002067
Iteration 49/1000 | Loss: 0.00002065
Iteration 50/1000 | Loss: 0.00002065
Iteration 51/1000 | Loss: 0.00002064
Iteration 52/1000 | Loss: 0.00002064
Iteration 53/1000 | Loss: 0.00002063
Iteration 54/1000 | Loss: 0.00002063
Iteration 55/1000 | Loss: 0.00002063
Iteration 56/1000 | Loss: 0.00002063
Iteration 57/1000 | Loss: 0.00002063
Iteration 58/1000 | Loss: 0.00002063
Iteration 59/1000 | Loss: 0.00002063
Iteration 60/1000 | Loss: 0.00002063
Iteration 61/1000 | Loss: 0.00002062
Iteration 62/1000 | Loss: 0.00002062
Iteration 63/1000 | Loss: 0.00002062
Iteration 64/1000 | Loss: 0.00002062
Iteration 65/1000 | Loss: 0.00002061
Iteration 66/1000 | Loss: 0.00002061
Iteration 67/1000 | Loss: 0.00002061
Iteration 68/1000 | Loss: 0.00002061
Iteration 69/1000 | Loss: 0.00002061
Iteration 70/1000 | Loss: 0.00002061
Iteration 71/1000 | Loss: 0.00002061
Iteration 72/1000 | Loss: 0.00002061
Iteration 73/1000 | Loss: 0.00002061
Iteration 74/1000 | Loss: 0.00002061
Iteration 75/1000 | Loss: 0.00002060
Iteration 76/1000 | Loss: 0.00002060
Iteration 77/1000 | Loss: 0.00002060
Iteration 78/1000 | Loss: 0.00002060
Iteration 79/1000 | Loss: 0.00002059
Iteration 80/1000 | Loss: 0.00002059
Iteration 81/1000 | Loss: 0.00002058
Iteration 82/1000 | Loss: 0.00002058
Iteration 83/1000 | Loss: 0.00002058
Iteration 84/1000 | Loss: 0.00002058
Iteration 85/1000 | Loss: 0.00002058
Iteration 86/1000 | Loss: 0.00002058
Iteration 87/1000 | Loss: 0.00002058
Iteration 88/1000 | Loss: 0.00002058
Iteration 89/1000 | Loss: 0.00002057
Iteration 90/1000 | Loss: 0.00002057
Iteration 91/1000 | Loss: 0.00002057
Iteration 92/1000 | Loss: 0.00002057
Iteration 93/1000 | Loss: 0.00002057
Iteration 94/1000 | Loss: 0.00002057
Iteration 95/1000 | Loss: 0.00002057
Iteration 96/1000 | Loss: 0.00002057
Iteration 97/1000 | Loss: 0.00002057
Iteration 98/1000 | Loss: 0.00002057
Iteration 99/1000 | Loss: 0.00002057
Iteration 100/1000 | Loss: 0.00002056
Iteration 101/1000 | Loss: 0.00002056
Iteration 102/1000 | Loss: 0.00002056
Iteration 103/1000 | Loss: 0.00002056
Iteration 104/1000 | Loss: 0.00002056
Iteration 105/1000 | Loss: 0.00002056
Iteration 106/1000 | Loss: 0.00002056
Iteration 107/1000 | Loss: 0.00002056
Iteration 108/1000 | Loss: 0.00002056
Iteration 109/1000 | Loss: 0.00002056
Iteration 110/1000 | Loss: 0.00002056
Iteration 111/1000 | Loss: 0.00002056
Iteration 112/1000 | Loss: 0.00002056
Iteration 113/1000 | Loss: 0.00002056
Iteration 114/1000 | Loss: 0.00002056
Iteration 115/1000 | Loss: 0.00002056
Iteration 116/1000 | Loss: 0.00002056
Iteration 117/1000 | Loss: 0.00002056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.0564557416946627e-05, 2.0564557416946627e-05, 2.0564557416946627e-05, 2.0564557416946627e-05, 2.0564557416946627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0564557416946627e-05

Optimization complete. Final v2v error: 3.862654685974121 mm

Highest mean error: 4.058605194091797 mm for frame 12

Lowest mean error: 3.5690295696258545 mm for frame 44

Saving results

Total time: 32.92262601852417
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478705
Iteration 2/25 | Loss: 0.00086554
Iteration 3/25 | Loss: 0.00072206
Iteration 4/25 | Loss: 0.00069617
Iteration 5/25 | Loss: 0.00068904
Iteration 6/25 | Loss: 0.00068800
Iteration 7/25 | Loss: 0.00068800
Iteration 8/25 | Loss: 0.00068800
Iteration 9/25 | Loss: 0.00068800
Iteration 10/25 | Loss: 0.00068800
Iteration 11/25 | Loss: 0.00068800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006880038999952376, 0.0006880038999952376, 0.0006880038999952376, 0.0006880038999952376, 0.0006880038999952376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006880038999952376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.01443291
Iteration 2/25 | Loss: 0.00030859
Iteration 3/25 | Loss: 0.00030859
Iteration 4/25 | Loss: 0.00030859
Iteration 5/25 | Loss: 0.00030859
Iteration 6/25 | Loss: 0.00030859
Iteration 7/25 | Loss: 0.00030859
Iteration 8/25 | Loss: 0.00030859
Iteration 9/25 | Loss: 0.00030859
Iteration 10/25 | Loss: 0.00030859
Iteration 11/25 | Loss: 0.00030859
Iteration 12/25 | Loss: 0.00030859
Iteration 13/25 | Loss: 0.00030859
Iteration 14/25 | Loss: 0.00030859
Iteration 15/25 | Loss: 0.00030859
Iteration 16/25 | Loss: 0.00030859
Iteration 17/25 | Loss: 0.00030859
Iteration 18/25 | Loss: 0.00030859
Iteration 19/25 | Loss: 0.00030859
Iteration 20/25 | Loss: 0.00030859
Iteration 21/25 | Loss: 0.00030859
Iteration 22/25 | Loss: 0.00030859
Iteration 23/25 | Loss: 0.00030859
Iteration 24/25 | Loss: 0.00030859
Iteration 25/25 | Loss: 0.00030859

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030859
Iteration 2/1000 | Loss: 0.00002102
Iteration 3/1000 | Loss: 0.00001736
Iteration 4/1000 | Loss: 0.00001621
Iteration 5/1000 | Loss: 0.00001538
Iteration 6/1000 | Loss: 0.00001497
Iteration 7/1000 | Loss: 0.00001465
Iteration 8/1000 | Loss: 0.00001449
Iteration 9/1000 | Loss: 0.00001426
Iteration 10/1000 | Loss: 0.00001423
Iteration 11/1000 | Loss: 0.00001419
Iteration 12/1000 | Loss: 0.00001418
Iteration 13/1000 | Loss: 0.00001413
Iteration 14/1000 | Loss: 0.00001408
Iteration 15/1000 | Loss: 0.00001402
Iteration 16/1000 | Loss: 0.00001400
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001399
Iteration 19/1000 | Loss: 0.00001390
Iteration 20/1000 | Loss: 0.00001385
Iteration 21/1000 | Loss: 0.00001385
Iteration 22/1000 | Loss: 0.00001383
Iteration 23/1000 | Loss: 0.00001383
Iteration 24/1000 | Loss: 0.00001374
Iteration 25/1000 | Loss: 0.00001366
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001364
Iteration 28/1000 | Loss: 0.00001363
Iteration 29/1000 | Loss: 0.00001362
Iteration 30/1000 | Loss: 0.00001362
Iteration 31/1000 | Loss: 0.00001362
Iteration 32/1000 | Loss: 0.00001361
Iteration 33/1000 | Loss: 0.00001361
Iteration 34/1000 | Loss: 0.00001361
Iteration 35/1000 | Loss: 0.00001361
Iteration 36/1000 | Loss: 0.00001361
Iteration 37/1000 | Loss: 0.00001360
Iteration 38/1000 | Loss: 0.00001360
Iteration 39/1000 | Loss: 0.00001360
Iteration 40/1000 | Loss: 0.00001359
Iteration 41/1000 | Loss: 0.00001359
Iteration 42/1000 | Loss: 0.00001359
Iteration 43/1000 | Loss: 0.00001359
Iteration 44/1000 | Loss: 0.00001359
Iteration 45/1000 | Loss: 0.00001359
Iteration 46/1000 | Loss: 0.00001359
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001358
Iteration 49/1000 | Loss: 0.00001358
Iteration 50/1000 | Loss: 0.00001358
Iteration 51/1000 | Loss: 0.00001358
Iteration 52/1000 | Loss: 0.00001357
Iteration 53/1000 | Loss: 0.00001357
Iteration 54/1000 | Loss: 0.00001357
Iteration 55/1000 | Loss: 0.00001357
Iteration 56/1000 | Loss: 0.00001357
Iteration 57/1000 | Loss: 0.00001357
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001356
Iteration 60/1000 | Loss: 0.00001356
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001356
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001356
Iteration 67/1000 | Loss: 0.00001356
Iteration 68/1000 | Loss: 0.00001356
Iteration 69/1000 | Loss: 0.00001356
Iteration 70/1000 | Loss: 0.00001356
Iteration 71/1000 | Loss: 0.00001356
Iteration 72/1000 | Loss: 0.00001356
Iteration 73/1000 | Loss: 0.00001355
Iteration 74/1000 | Loss: 0.00001355
Iteration 75/1000 | Loss: 0.00001355
Iteration 76/1000 | Loss: 0.00001354
Iteration 77/1000 | Loss: 0.00001354
Iteration 78/1000 | Loss: 0.00001354
Iteration 79/1000 | Loss: 0.00001354
Iteration 80/1000 | Loss: 0.00001354
Iteration 81/1000 | Loss: 0.00001354
Iteration 82/1000 | Loss: 0.00001354
Iteration 83/1000 | Loss: 0.00001353
Iteration 84/1000 | Loss: 0.00001353
Iteration 85/1000 | Loss: 0.00001353
Iteration 86/1000 | Loss: 0.00001353
Iteration 87/1000 | Loss: 0.00001353
Iteration 88/1000 | Loss: 0.00001353
Iteration 89/1000 | Loss: 0.00001353
Iteration 90/1000 | Loss: 0.00001353
Iteration 91/1000 | Loss: 0.00001353
Iteration 92/1000 | Loss: 0.00001353
Iteration 93/1000 | Loss: 0.00001353
Iteration 94/1000 | Loss: 0.00001352
Iteration 95/1000 | Loss: 0.00001352
Iteration 96/1000 | Loss: 0.00001352
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001352
Iteration 100/1000 | Loss: 0.00001352
Iteration 101/1000 | Loss: 0.00001352
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001352
Iteration 104/1000 | Loss: 0.00001352
Iteration 105/1000 | Loss: 0.00001352
Iteration 106/1000 | Loss: 0.00001352
Iteration 107/1000 | Loss: 0.00001352
Iteration 108/1000 | Loss: 0.00001352
Iteration 109/1000 | Loss: 0.00001352
Iteration 110/1000 | Loss: 0.00001352
Iteration 111/1000 | Loss: 0.00001352
Iteration 112/1000 | Loss: 0.00001352
Iteration 113/1000 | Loss: 0.00001352
Iteration 114/1000 | Loss: 0.00001352
Iteration 115/1000 | Loss: 0.00001352
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001352
Iteration 118/1000 | Loss: 0.00001352
Iteration 119/1000 | Loss: 0.00001352
Iteration 120/1000 | Loss: 0.00001352
Iteration 121/1000 | Loss: 0.00001352
Iteration 122/1000 | Loss: 0.00001352
Iteration 123/1000 | Loss: 0.00001352
Iteration 124/1000 | Loss: 0.00001352
Iteration 125/1000 | Loss: 0.00001352
Iteration 126/1000 | Loss: 0.00001352
Iteration 127/1000 | Loss: 0.00001352
Iteration 128/1000 | Loss: 0.00001352
Iteration 129/1000 | Loss: 0.00001352
Iteration 130/1000 | Loss: 0.00001352
Iteration 131/1000 | Loss: 0.00001352
Iteration 132/1000 | Loss: 0.00001352
Iteration 133/1000 | Loss: 0.00001352
Iteration 134/1000 | Loss: 0.00001352
Iteration 135/1000 | Loss: 0.00001352
Iteration 136/1000 | Loss: 0.00001352
Iteration 137/1000 | Loss: 0.00001352
Iteration 138/1000 | Loss: 0.00001352
Iteration 139/1000 | Loss: 0.00001352
Iteration 140/1000 | Loss: 0.00001352
Iteration 141/1000 | Loss: 0.00001352
Iteration 142/1000 | Loss: 0.00001352
Iteration 143/1000 | Loss: 0.00001352
Iteration 144/1000 | Loss: 0.00001352
Iteration 145/1000 | Loss: 0.00001352
Iteration 146/1000 | Loss: 0.00001352
Iteration 147/1000 | Loss: 0.00001352
Iteration 148/1000 | Loss: 0.00001352
Iteration 149/1000 | Loss: 0.00001352
Iteration 150/1000 | Loss: 0.00001352
Iteration 151/1000 | Loss: 0.00001352
Iteration 152/1000 | Loss: 0.00001352
Iteration 153/1000 | Loss: 0.00001352
Iteration 154/1000 | Loss: 0.00001352
Iteration 155/1000 | Loss: 0.00001352
Iteration 156/1000 | Loss: 0.00001352
Iteration 157/1000 | Loss: 0.00001352
Iteration 158/1000 | Loss: 0.00001352
Iteration 159/1000 | Loss: 0.00001352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.3522493645723443e-05, 1.3522493645723443e-05, 1.3522493645723443e-05, 1.3522493645723443e-05, 1.3522493645723443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3522493645723443e-05

Optimization complete. Final v2v error: 3.118574380874634 mm

Highest mean error: 3.36340069770813 mm for frame 80

Lowest mean error: 2.972736120223999 mm for frame 256

Saving results

Total time: 41.38686561584473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071123
Iteration 2/25 | Loss: 0.00097702
Iteration 3/25 | Loss: 0.00072417
Iteration 4/25 | Loss: 0.00068549
Iteration 5/25 | Loss: 0.00067966
Iteration 6/25 | Loss: 0.00067775
Iteration 7/25 | Loss: 0.00067743
Iteration 8/25 | Loss: 0.00067743
Iteration 9/25 | Loss: 0.00067743
Iteration 10/25 | Loss: 0.00067743
Iteration 11/25 | Loss: 0.00067743
Iteration 12/25 | Loss: 0.00067743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006774300709366798, 0.0006774300709366798, 0.0006774300709366798, 0.0006774300709366798, 0.0006774300709366798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006774300709366798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99726009
Iteration 2/25 | Loss: 0.00038471
Iteration 3/25 | Loss: 0.00038470
Iteration 4/25 | Loss: 0.00038470
Iteration 5/25 | Loss: 0.00038470
Iteration 6/25 | Loss: 0.00038470
Iteration 7/25 | Loss: 0.00038470
Iteration 8/25 | Loss: 0.00038470
Iteration 9/25 | Loss: 0.00038470
Iteration 10/25 | Loss: 0.00038470
Iteration 11/25 | Loss: 0.00038470
Iteration 12/25 | Loss: 0.00038470
Iteration 13/25 | Loss: 0.00038470
Iteration 14/25 | Loss: 0.00038470
Iteration 15/25 | Loss: 0.00038470
Iteration 16/25 | Loss: 0.00038470
Iteration 17/25 | Loss: 0.00038470
Iteration 18/25 | Loss: 0.00038470
Iteration 19/25 | Loss: 0.00038470
Iteration 20/25 | Loss: 0.00038470
Iteration 21/25 | Loss: 0.00038470
Iteration 22/25 | Loss: 0.00038470
Iteration 23/25 | Loss: 0.00038470
Iteration 24/25 | Loss: 0.00038470
Iteration 25/25 | Loss: 0.00038470

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038470
Iteration 2/1000 | Loss: 0.00002314
Iteration 3/1000 | Loss: 0.00001702
Iteration 4/1000 | Loss: 0.00001481
Iteration 5/1000 | Loss: 0.00001396
Iteration 6/1000 | Loss: 0.00001344
Iteration 7/1000 | Loss: 0.00001298
Iteration 8/1000 | Loss: 0.00001286
Iteration 9/1000 | Loss: 0.00001267
Iteration 10/1000 | Loss: 0.00001260
Iteration 11/1000 | Loss: 0.00001240
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001226
Iteration 14/1000 | Loss: 0.00001222
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001217
Iteration 17/1000 | Loss: 0.00001217
Iteration 18/1000 | Loss: 0.00001217
Iteration 19/1000 | Loss: 0.00001217
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001215
Iteration 22/1000 | Loss: 0.00001215
Iteration 23/1000 | Loss: 0.00001215
Iteration 24/1000 | Loss: 0.00001214
Iteration 25/1000 | Loss: 0.00001214
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001211
Iteration 28/1000 | Loss: 0.00001211
Iteration 29/1000 | Loss: 0.00001211
Iteration 30/1000 | Loss: 0.00001210
Iteration 31/1000 | Loss: 0.00001210
Iteration 32/1000 | Loss: 0.00001209
Iteration 33/1000 | Loss: 0.00001209
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001209
Iteration 36/1000 | Loss: 0.00001209
Iteration 37/1000 | Loss: 0.00001208
Iteration 38/1000 | Loss: 0.00001208
Iteration 39/1000 | Loss: 0.00001208
Iteration 40/1000 | Loss: 0.00001208
Iteration 41/1000 | Loss: 0.00001208
Iteration 42/1000 | Loss: 0.00001208
Iteration 43/1000 | Loss: 0.00001207
Iteration 44/1000 | Loss: 0.00001206
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001204
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001203
Iteration 50/1000 | Loss: 0.00001203
Iteration 51/1000 | Loss: 0.00001203
Iteration 52/1000 | Loss: 0.00001203
Iteration 53/1000 | Loss: 0.00001203
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001202
Iteration 56/1000 | Loss: 0.00001202
Iteration 57/1000 | Loss: 0.00001202
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001201
Iteration 60/1000 | Loss: 0.00001201
Iteration 61/1000 | Loss: 0.00001201
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001201
Iteration 64/1000 | Loss: 0.00001201
Iteration 65/1000 | Loss: 0.00001201
Iteration 66/1000 | Loss: 0.00001201
Iteration 67/1000 | Loss: 0.00001201
Iteration 68/1000 | Loss: 0.00001201
Iteration 69/1000 | Loss: 0.00001201
Iteration 70/1000 | Loss: 0.00001201
Iteration 71/1000 | Loss: 0.00001201
Iteration 72/1000 | Loss: 0.00001201
Iteration 73/1000 | Loss: 0.00001201
Iteration 74/1000 | Loss: 0.00001201
Iteration 75/1000 | Loss: 0.00001201
Iteration 76/1000 | Loss: 0.00001201
Iteration 77/1000 | Loss: 0.00001201
Iteration 78/1000 | Loss: 0.00001201
Iteration 79/1000 | Loss: 0.00001201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.2006215001747478e-05, 1.2006215001747478e-05, 1.2006215001747478e-05, 1.2006215001747478e-05, 1.2006215001747478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2006215001747478e-05

Optimization complete. Final v2v error: 2.8615920543670654 mm

Highest mean error: 3.635087251663208 mm for frame 0

Lowest mean error: 2.5438995361328125 mm for frame 113

Saving results

Total time: 31.290310859680176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797181
Iteration 2/25 | Loss: 0.00210291
Iteration 3/25 | Loss: 0.00092066
Iteration 4/25 | Loss: 0.00080587
Iteration 5/25 | Loss: 0.00081757
Iteration 6/25 | Loss: 0.00071633
Iteration 7/25 | Loss: 0.00069862
Iteration 8/25 | Loss: 0.00069662
Iteration 9/25 | Loss: 0.00066912
Iteration 10/25 | Loss: 0.00066752
Iteration 11/25 | Loss: 0.00067513
Iteration 12/25 | Loss: 0.00066112
Iteration 13/25 | Loss: 0.00066233
Iteration 14/25 | Loss: 0.00065626
Iteration 15/25 | Loss: 0.00065846
Iteration 16/25 | Loss: 0.00065090
Iteration 17/25 | Loss: 0.00065081
Iteration 18/25 | Loss: 0.00065081
Iteration 19/25 | Loss: 0.00065081
Iteration 20/25 | Loss: 0.00065080
Iteration 21/25 | Loss: 0.00065080
Iteration 22/25 | Loss: 0.00065080
Iteration 23/25 | Loss: 0.00065080
Iteration 24/25 | Loss: 0.00065080
Iteration 25/25 | Loss: 0.00065080

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53104937
Iteration 2/25 | Loss: 0.00029493
Iteration 3/25 | Loss: 0.00029492
Iteration 4/25 | Loss: 0.00029492
Iteration 5/25 | Loss: 0.00029492
Iteration 6/25 | Loss: 0.00029492
Iteration 7/25 | Loss: 0.00029492
Iteration 8/25 | Loss: 0.00029492
Iteration 9/25 | Loss: 0.00029492
Iteration 10/25 | Loss: 0.00029492
Iteration 11/25 | Loss: 0.00029492
Iteration 12/25 | Loss: 0.00029492
Iteration 13/25 | Loss: 0.00029492
Iteration 14/25 | Loss: 0.00029492
Iteration 15/25 | Loss: 0.00029492
Iteration 16/25 | Loss: 0.00029492
Iteration 17/25 | Loss: 0.00029492
Iteration 18/25 | Loss: 0.00029492
Iteration 19/25 | Loss: 0.00029492
Iteration 20/25 | Loss: 0.00029492
Iteration 21/25 | Loss: 0.00029492
Iteration 22/25 | Loss: 0.00029492
Iteration 23/25 | Loss: 0.00029492
Iteration 24/25 | Loss: 0.00029492
Iteration 25/25 | Loss: 0.00029492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029492
Iteration 2/1000 | Loss: 0.00002444
Iteration 3/1000 | Loss: 0.00001527
Iteration 4/1000 | Loss: 0.00001396
Iteration 5/1000 | Loss: 0.00001306
Iteration 6/1000 | Loss: 0.00001261
Iteration 7/1000 | Loss: 0.00001228
Iteration 8/1000 | Loss: 0.00001218
Iteration 9/1000 | Loss: 0.00001209
Iteration 10/1000 | Loss: 0.00001205
Iteration 11/1000 | Loss: 0.00001203
Iteration 12/1000 | Loss: 0.00001200
Iteration 13/1000 | Loss: 0.00001195
Iteration 14/1000 | Loss: 0.00001192
Iteration 15/1000 | Loss: 0.00001190
Iteration 16/1000 | Loss: 0.00001183
Iteration 17/1000 | Loss: 0.00001183
Iteration 18/1000 | Loss: 0.00001181
Iteration 19/1000 | Loss: 0.00001173
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001157
Iteration 23/1000 | Loss: 0.00001152
Iteration 24/1000 | Loss: 0.00001151
Iteration 25/1000 | Loss: 0.00001150
Iteration 26/1000 | Loss: 0.00001150
Iteration 27/1000 | Loss: 0.00001149
Iteration 28/1000 | Loss: 0.00001148
Iteration 29/1000 | Loss: 0.00001145
Iteration 30/1000 | Loss: 0.00001145
Iteration 31/1000 | Loss: 0.00001145
Iteration 32/1000 | Loss: 0.00001145
Iteration 33/1000 | Loss: 0.00001145
Iteration 34/1000 | Loss: 0.00001145
Iteration 35/1000 | Loss: 0.00001145
Iteration 36/1000 | Loss: 0.00001145
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001144
Iteration 40/1000 | Loss: 0.00001143
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001143
Iteration 43/1000 | Loss: 0.00001143
Iteration 44/1000 | Loss: 0.00001143
Iteration 45/1000 | Loss: 0.00001142
Iteration 46/1000 | Loss: 0.00001142
Iteration 47/1000 | Loss: 0.00001141
Iteration 48/1000 | Loss: 0.00001140
Iteration 49/1000 | Loss: 0.00001140
Iteration 50/1000 | Loss: 0.00001140
Iteration 51/1000 | Loss: 0.00001140
Iteration 52/1000 | Loss: 0.00001140
Iteration 53/1000 | Loss: 0.00001140
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001140
Iteration 56/1000 | Loss: 0.00001139
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001139
Iteration 59/1000 | Loss: 0.00001139
Iteration 60/1000 | Loss: 0.00001138
Iteration 61/1000 | Loss: 0.00001137
Iteration 62/1000 | Loss: 0.00001137
Iteration 63/1000 | Loss: 0.00001137
Iteration 64/1000 | Loss: 0.00001136
Iteration 65/1000 | Loss: 0.00001136
Iteration 66/1000 | Loss: 0.00001136
Iteration 67/1000 | Loss: 0.00001135
Iteration 68/1000 | Loss: 0.00001134
Iteration 69/1000 | Loss: 0.00001134
Iteration 70/1000 | Loss: 0.00001133
Iteration 71/1000 | Loss: 0.00001133
Iteration 72/1000 | Loss: 0.00001132
Iteration 73/1000 | Loss: 0.00001132
Iteration 74/1000 | Loss: 0.00001131
Iteration 75/1000 | Loss: 0.00001131
Iteration 76/1000 | Loss: 0.00001131
Iteration 77/1000 | Loss: 0.00001130
Iteration 78/1000 | Loss: 0.00001130
Iteration 79/1000 | Loss: 0.00001130
Iteration 80/1000 | Loss: 0.00001129
Iteration 81/1000 | Loss: 0.00001129
Iteration 82/1000 | Loss: 0.00001128
Iteration 83/1000 | Loss: 0.00001128
Iteration 84/1000 | Loss: 0.00001127
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001127
Iteration 87/1000 | Loss: 0.00001127
Iteration 88/1000 | Loss: 0.00001127
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001127
Iteration 92/1000 | Loss: 0.00001126
Iteration 93/1000 | Loss: 0.00001126
Iteration 94/1000 | Loss: 0.00001126
Iteration 95/1000 | Loss: 0.00001126
Iteration 96/1000 | Loss: 0.00001126
Iteration 97/1000 | Loss: 0.00001126
Iteration 98/1000 | Loss: 0.00001126
Iteration 99/1000 | Loss: 0.00001126
Iteration 100/1000 | Loss: 0.00001126
Iteration 101/1000 | Loss: 0.00001126
Iteration 102/1000 | Loss: 0.00001126
Iteration 103/1000 | Loss: 0.00001125
Iteration 104/1000 | Loss: 0.00001125
Iteration 105/1000 | Loss: 0.00001125
Iteration 106/1000 | Loss: 0.00001125
Iteration 107/1000 | Loss: 0.00001125
Iteration 108/1000 | Loss: 0.00001125
Iteration 109/1000 | Loss: 0.00001125
Iteration 110/1000 | Loss: 0.00001125
Iteration 111/1000 | Loss: 0.00001125
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001125
Iteration 115/1000 | Loss: 0.00001125
Iteration 116/1000 | Loss: 0.00001124
Iteration 117/1000 | Loss: 0.00001124
Iteration 118/1000 | Loss: 0.00001124
Iteration 119/1000 | Loss: 0.00001124
Iteration 120/1000 | Loss: 0.00001124
Iteration 121/1000 | Loss: 0.00001124
Iteration 122/1000 | Loss: 0.00001124
Iteration 123/1000 | Loss: 0.00001124
Iteration 124/1000 | Loss: 0.00001124
Iteration 125/1000 | Loss: 0.00001124
Iteration 126/1000 | Loss: 0.00001124
Iteration 127/1000 | Loss: 0.00001124
Iteration 128/1000 | Loss: 0.00001124
Iteration 129/1000 | Loss: 0.00001124
Iteration 130/1000 | Loss: 0.00001123
Iteration 131/1000 | Loss: 0.00001123
Iteration 132/1000 | Loss: 0.00001123
Iteration 133/1000 | Loss: 0.00001123
Iteration 134/1000 | Loss: 0.00001123
Iteration 135/1000 | Loss: 0.00001123
Iteration 136/1000 | Loss: 0.00001123
Iteration 137/1000 | Loss: 0.00001123
Iteration 138/1000 | Loss: 0.00001123
Iteration 139/1000 | Loss: 0.00001123
Iteration 140/1000 | Loss: 0.00001123
Iteration 141/1000 | Loss: 0.00001123
Iteration 142/1000 | Loss: 0.00001123
Iteration 143/1000 | Loss: 0.00001123
Iteration 144/1000 | Loss: 0.00001123
Iteration 145/1000 | Loss: 0.00001123
Iteration 146/1000 | Loss: 0.00001123
Iteration 147/1000 | Loss: 0.00001123
Iteration 148/1000 | Loss: 0.00001123
Iteration 149/1000 | Loss: 0.00001123
Iteration 150/1000 | Loss: 0.00001123
Iteration 151/1000 | Loss: 0.00001123
Iteration 152/1000 | Loss: 0.00001123
Iteration 153/1000 | Loss: 0.00001123
Iteration 154/1000 | Loss: 0.00001123
Iteration 155/1000 | Loss: 0.00001123
Iteration 156/1000 | Loss: 0.00001123
Iteration 157/1000 | Loss: 0.00001123
Iteration 158/1000 | Loss: 0.00001123
Iteration 159/1000 | Loss: 0.00001123
Iteration 160/1000 | Loss: 0.00001123
Iteration 161/1000 | Loss: 0.00001123
Iteration 162/1000 | Loss: 0.00001123
Iteration 163/1000 | Loss: 0.00001123
Iteration 164/1000 | Loss: 0.00001123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.1226024071220309e-05, 1.1226024071220309e-05, 1.1226024071220309e-05, 1.1226024071220309e-05, 1.1226024071220309e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1226024071220309e-05

Optimization complete. Final v2v error: 2.887371063232422 mm

Highest mean error: 3.2407164573669434 mm for frame 57

Lowest mean error: 2.690340042114258 mm for frame 143

Saving results

Total time: 57.92564368247986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878846
Iteration 2/25 | Loss: 0.00149480
Iteration 3/25 | Loss: 0.00104226
Iteration 4/25 | Loss: 0.00095489
Iteration 5/25 | Loss: 0.00092575
Iteration 6/25 | Loss: 0.00089435
Iteration 7/25 | Loss: 0.00088621
Iteration 8/25 | Loss: 0.00088127
Iteration 9/25 | Loss: 0.00086776
Iteration 10/25 | Loss: 0.00085906
Iteration 11/25 | Loss: 0.00085646
Iteration 12/25 | Loss: 0.00086417
Iteration 13/25 | Loss: 0.00084421
Iteration 14/25 | Loss: 0.00084051
Iteration 15/25 | Loss: 0.00083749
Iteration 16/25 | Loss: 0.00083486
Iteration 17/25 | Loss: 0.00083814
Iteration 18/25 | Loss: 0.00083845
Iteration 19/25 | Loss: 0.00083769
Iteration 20/25 | Loss: 0.00083382
Iteration 21/25 | Loss: 0.00083364
Iteration 22/25 | Loss: 0.00083195
Iteration 23/25 | Loss: 0.00083578
Iteration 24/25 | Loss: 0.00082950
Iteration 25/25 | Loss: 0.00082539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90900970
Iteration 2/25 | Loss: 0.00057954
Iteration 3/25 | Loss: 0.00057954
Iteration 4/25 | Loss: 0.00057954
Iteration 5/25 | Loss: 0.00057954
Iteration 6/25 | Loss: 0.00057953
Iteration 7/25 | Loss: 0.00057953
Iteration 8/25 | Loss: 0.00057953
Iteration 9/25 | Loss: 0.00057953
Iteration 10/25 | Loss: 0.00057953
Iteration 11/25 | Loss: 0.00057953
Iteration 12/25 | Loss: 0.00057953
Iteration 13/25 | Loss: 0.00057953
Iteration 14/25 | Loss: 0.00057953
Iteration 15/25 | Loss: 0.00057953
Iteration 16/25 | Loss: 0.00057953
Iteration 17/25 | Loss: 0.00057953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005795340985059738, 0.0005795340985059738, 0.0005795340985059738, 0.0005795340985059738, 0.0005795340985059738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005795340985059738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057953
Iteration 2/1000 | Loss: 0.00012211
Iteration 3/1000 | Loss: 0.00007668
Iteration 4/1000 | Loss: 0.00004770
Iteration 5/1000 | Loss: 0.00047893
Iteration 6/1000 | Loss: 0.00033701
Iteration 7/1000 | Loss: 0.00034099
Iteration 8/1000 | Loss: 0.00015538
Iteration 9/1000 | Loss: 0.00014905
Iteration 10/1000 | Loss: 0.00014509
Iteration 11/1000 | Loss: 0.00013541
Iteration 12/1000 | Loss: 0.00015745
Iteration 13/1000 | Loss: 0.00014733
Iteration 14/1000 | Loss: 0.00015137
Iteration 15/1000 | Loss: 0.00015059
Iteration 16/1000 | Loss: 0.00013824
Iteration 17/1000 | Loss: 0.00003967
Iteration 18/1000 | Loss: 0.00009973
Iteration 19/1000 | Loss: 0.00012553
Iteration 20/1000 | Loss: 0.00013575
Iteration 21/1000 | Loss: 0.00011930
Iteration 22/1000 | Loss: 0.00011769
Iteration 23/1000 | Loss: 0.00010239
Iteration 24/1000 | Loss: 0.00010658
Iteration 25/1000 | Loss: 0.00016747
Iteration 26/1000 | Loss: 0.00009638
Iteration 27/1000 | Loss: 0.00009126
Iteration 28/1000 | Loss: 0.00076692
Iteration 29/1000 | Loss: 0.00054742
Iteration 30/1000 | Loss: 0.00055025
Iteration 31/1000 | Loss: 0.00051592
Iteration 32/1000 | Loss: 0.00019527
Iteration 33/1000 | Loss: 0.00024072
Iteration 34/1000 | Loss: 0.00003988
Iteration 35/1000 | Loss: 0.00003520
Iteration 36/1000 | Loss: 0.00003357
Iteration 37/1000 | Loss: 0.00003214
Iteration 38/1000 | Loss: 0.00003089
Iteration 39/1000 | Loss: 0.00003023
Iteration 40/1000 | Loss: 0.00002969
Iteration 41/1000 | Loss: 0.00002927
Iteration 42/1000 | Loss: 0.00002892
Iteration 43/1000 | Loss: 0.00002856
Iteration 44/1000 | Loss: 0.00002820
Iteration 45/1000 | Loss: 0.00002793
Iteration 46/1000 | Loss: 0.00002765
Iteration 47/1000 | Loss: 0.00002756
Iteration 48/1000 | Loss: 0.00002756
Iteration 49/1000 | Loss: 0.00002756
Iteration 50/1000 | Loss: 0.00002745
Iteration 51/1000 | Loss: 0.00002745
Iteration 52/1000 | Loss: 0.00002732
Iteration 53/1000 | Loss: 0.00002732
Iteration 54/1000 | Loss: 0.00002731
Iteration 55/1000 | Loss: 0.00002731
Iteration 56/1000 | Loss: 0.00002730
Iteration 57/1000 | Loss: 0.00002730
Iteration 58/1000 | Loss: 0.00002730
Iteration 59/1000 | Loss: 0.00002729
Iteration 60/1000 | Loss: 0.00002726
Iteration 61/1000 | Loss: 0.00002725
Iteration 62/1000 | Loss: 0.00002725
Iteration 63/1000 | Loss: 0.00002724
Iteration 64/1000 | Loss: 0.00002722
Iteration 65/1000 | Loss: 0.00002718
Iteration 66/1000 | Loss: 0.00002717
Iteration 67/1000 | Loss: 0.00002717
Iteration 68/1000 | Loss: 0.00002717
Iteration 69/1000 | Loss: 0.00002716
Iteration 70/1000 | Loss: 0.00002716
Iteration 71/1000 | Loss: 0.00002716
Iteration 72/1000 | Loss: 0.00002715
Iteration 73/1000 | Loss: 0.00002715
Iteration 74/1000 | Loss: 0.00002715
Iteration 75/1000 | Loss: 0.00002715
Iteration 76/1000 | Loss: 0.00002715
Iteration 77/1000 | Loss: 0.00002714
Iteration 78/1000 | Loss: 0.00002714
Iteration 79/1000 | Loss: 0.00002714
Iteration 80/1000 | Loss: 0.00002714
Iteration 81/1000 | Loss: 0.00002714
Iteration 82/1000 | Loss: 0.00002714
Iteration 83/1000 | Loss: 0.00002714
Iteration 84/1000 | Loss: 0.00002714
Iteration 85/1000 | Loss: 0.00002713
Iteration 86/1000 | Loss: 0.00002713
Iteration 87/1000 | Loss: 0.00002713
Iteration 88/1000 | Loss: 0.00002713
Iteration 89/1000 | Loss: 0.00002713
Iteration 90/1000 | Loss: 0.00002713
Iteration 91/1000 | Loss: 0.00002713
Iteration 92/1000 | Loss: 0.00002712
Iteration 93/1000 | Loss: 0.00002711
Iteration 94/1000 | Loss: 0.00002711
Iteration 95/1000 | Loss: 0.00002711
Iteration 96/1000 | Loss: 0.00002710
Iteration 97/1000 | Loss: 0.00002710
Iteration 98/1000 | Loss: 0.00002710
Iteration 99/1000 | Loss: 0.00002709
Iteration 100/1000 | Loss: 0.00002709
Iteration 101/1000 | Loss: 0.00002708
Iteration 102/1000 | Loss: 0.00002708
Iteration 103/1000 | Loss: 0.00002708
Iteration 104/1000 | Loss: 0.00002708
Iteration 105/1000 | Loss: 0.00002708
Iteration 106/1000 | Loss: 0.00002707
Iteration 107/1000 | Loss: 0.00002707
Iteration 108/1000 | Loss: 0.00002707
Iteration 109/1000 | Loss: 0.00002706
Iteration 110/1000 | Loss: 0.00002706
Iteration 111/1000 | Loss: 0.00002706
Iteration 112/1000 | Loss: 0.00002706
Iteration 113/1000 | Loss: 0.00002706
Iteration 114/1000 | Loss: 0.00002705
Iteration 115/1000 | Loss: 0.00002705
Iteration 116/1000 | Loss: 0.00002705
Iteration 117/1000 | Loss: 0.00002705
Iteration 118/1000 | Loss: 0.00002705
Iteration 119/1000 | Loss: 0.00002705
Iteration 120/1000 | Loss: 0.00002705
Iteration 121/1000 | Loss: 0.00002705
Iteration 122/1000 | Loss: 0.00002704
Iteration 123/1000 | Loss: 0.00002704
Iteration 124/1000 | Loss: 0.00002704
Iteration 125/1000 | Loss: 0.00002704
Iteration 126/1000 | Loss: 0.00002704
Iteration 127/1000 | Loss: 0.00002704
Iteration 128/1000 | Loss: 0.00002704
Iteration 129/1000 | Loss: 0.00002703
Iteration 130/1000 | Loss: 0.00002703
Iteration 131/1000 | Loss: 0.00002703
Iteration 132/1000 | Loss: 0.00002703
Iteration 133/1000 | Loss: 0.00002703
Iteration 134/1000 | Loss: 0.00002702
Iteration 135/1000 | Loss: 0.00002702
Iteration 136/1000 | Loss: 0.00002702
Iteration 137/1000 | Loss: 0.00002702
Iteration 138/1000 | Loss: 0.00002702
Iteration 139/1000 | Loss: 0.00002701
Iteration 140/1000 | Loss: 0.00002701
Iteration 141/1000 | Loss: 0.00002701
Iteration 142/1000 | Loss: 0.00002701
Iteration 143/1000 | Loss: 0.00002701
Iteration 144/1000 | Loss: 0.00002701
Iteration 145/1000 | Loss: 0.00002701
Iteration 146/1000 | Loss: 0.00002701
Iteration 147/1000 | Loss: 0.00002701
Iteration 148/1000 | Loss: 0.00002701
Iteration 149/1000 | Loss: 0.00002701
Iteration 150/1000 | Loss: 0.00002701
Iteration 151/1000 | Loss: 0.00002701
Iteration 152/1000 | Loss: 0.00002701
Iteration 153/1000 | Loss: 0.00002701
Iteration 154/1000 | Loss: 0.00002701
Iteration 155/1000 | Loss: 0.00002701
Iteration 156/1000 | Loss: 0.00002701
Iteration 157/1000 | Loss: 0.00002701
Iteration 158/1000 | Loss: 0.00002701
Iteration 159/1000 | Loss: 0.00002701
Iteration 160/1000 | Loss: 0.00002701
Iteration 161/1000 | Loss: 0.00002701
Iteration 162/1000 | Loss: 0.00002701
Iteration 163/1000 | Loss: 0.00002701
Iteration 164/1000 | Loss: 0.00002701
Iteration 165/1000 | Loss: 0.00002701
Iteration 166/1000 | Loss: 0.00002701
Iteration 167/1000 | Loss: 0.00002701
Iteration 168/1000 | Loss: 0.00002701
Iteration 169/1000 | Loss: 0.00002701
Iteration 170/1000 | Loss: 0.00002701
Iteration 171/1000 | Loss: 0.00002701
Iteration 172/1000 | Loss: 0.00002701
Iteration 173/1000 | Loss: 0.00002701
Iteration 174/1000 | Loss: 0.00002701
Iteration 175/1000 | Loss: 0.00002701
Iteration 176/1000 | Loss: 0.00002701
Iteration 177/1000 | Loss: 0.00002701
Iteration 178/1000 | Loss: 0.00002701
Iteration 179/1000 | Loss: 0.00002701
Iteration 180/1000 | Loss: 0.00002701
Iteration 181/1000 | Loss: 0.00002701
Iteration 182/1000 | Loss: 0.00002701
Iteration 183/1000 | Loss: 0.00002701
Iteration 184/1000 | Loss: 0.00002701
Iteration 185/1000 | Loss: 0.00002701
Iteration 186/1000 | Loss: 0.00002701
Iteration 187/1000 | Loss: 0.00002701
Iteration 188/1000 | Loss: 0.00002701
Iteration 189/1000 | Loss: 0.00002701
Iteration 190/1000 | Loss: 0.00002701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.7006715754396282e-05, 2.7006715754396282e-05, 2.7006715754396282e-05, 2.7006715754396282e-05, 2.7006715754396282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7006715754396282e-05

Optimization complete. Final v2v error: 4.307766437530518 mm

Highest mean error: 6.603090286254883 mm for frame 143

Lowest mean error: 3.3292505741119385 mm for frame 0

Saving results

Total time: 143.86319875717163
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00640407
Iteration 2/25 | Loss: 0.00112319
Iteration 3/25 | Loss: 0.00089387
Iteration 4/25 | Loss: 0.00085350
Iteration 5/25 | Loss: 0.00084094
Iteration 6/25 | Loss: 0.00083916
Iteration 7/25 | Loss: 0.00083907
Iteration 8/25 | Loss: 0.00083907
Iteration 9/25 | Loss: 0.00083907
Iteration 10/25 | Loss: 0.00083907
Iteration 11/25 | Loss: 0.00083907
Iteration 12/25 | Loss: 0.00083907
Iteration 13/25 | Loss: 0.00083907
Iteration 14/25 | Loss: 0.00083907
Iteration 15/25 | Loss: 0.00083907
Iteration 16/25 | Loss: 0.00083907
Iteration 17/25 | Loss: 0.00083907
Iteration 18/25 | Loss: 0.00083907
Iteration 19/25 | Loss: 0.00083907
Iteration 20/25 | Loss: 0.00083907
Iteration 21/25 | Loss: 0.00083907
Iteration 22/25 | Loss: 0.00083907
Iteration 23/25 | Loss: 0.00083907
Iteration 24/25 | Loss: 0.00083907
Iteration 25/25 | Loss: 0.00083907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.86412668
Iteration 2/25 | Loss: 0.00037901
Iteration 3/25 | Loss: 0.00037901
Iteration 4/25 | Loss: 0.00037901
Iteration 5/25 | Loss: 0.00037901
Iteration 6/25 | Loss: 0.00037901
Iteration 7/25 | Loss: 0.00037901
Iteration 8/25 | Loss: 0.00037901
Iteration 9/25 | Loss: 0.00037901
Iteration 10/25 | Loss: 0.00037901
Iteration 11/25 | Loss: 0.00037901
Iteration 12/25 | Loss: 0.00037901
Iteration 13/25 | Loss: 0.00037901
Iteration 14/25 | Loss: 0.00037901
Iteration 15/25 | Loss: 0.00037901
Iteration 16/25 | Loss: 0.00037901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000379008095478639, 0.000379008095478639, 0.000379008095478639, 0.000379008095478639, 0.000379008095478639]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000379008095478639

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037901
Iteration 2/1000 | Loss: 0.00003083
Iteration 3/1000 | Loss: 0.00002461
Iteration 4/1000 | Loss: 0.00002294
Iteration 5/1000 | Loss: 0.00002220
Iteration 6/1000 | Loss: 0.00002185
Iteration 7/1000 | Loss: 0.00002162
Iteration 8/1000 | Loss: 0.00002137
Iteration 9/1000 | Loss: 0.00002111
Iteration 10/1000 | Loss: 0.00002101
Iteration 11/1000 | Loss: 0.00002095
Iteration 12/1000 | Loss: 0.00002090
Iteration 13/1000 | Loss: 0.00002084
Iteration 14/1000 | Loss: 0.00002082
Iteration 15/1000 | Loss: 0.00002082
Iteration 16/1000 | Loss: 0.00002081
Iteration 17/1000 | Loss: 0.00002081
Iteration 18/1000 | Loss: 0.00002081
Iteration 19/1000 | Loss: 0.00002080
Iteration 20/1000 | Loss: 0.00002080
Iteration 21/1000 | Loss: 0.00002080
Iteration 22/1000 | Loss: 0.00002080
Iteration 23/1000 | Loss: 0.00002079
Iteration 24/1000 | Loss: 0.00002078
Iteration 25/1000 | Loss: 0.00002076
Iteration 26/1000 | Loss: 0.00002076
Iteration 27/1000 | Loss: 0.00002075
Iteration 28/1000 | Loss: 0.00002075
Iteration 29/1000 | Loss: 0.00002073
Iteration 30/1000 | Loss: 0.00002068
Iteration 31/1000 | Loss: 0.00002068
Iteration 32/1000 | Loss: 0.00002068
Iteration 33/1000 | Loss: 0.00002068
Iteration 34/1000 | Loss: 0.00002068
Iteration 35/1000 | Loss: 0.00002068
Iteration 36/1000 | Loss: 0.00002068
Iteration 37/1000 | Loss: 0.00002068
Iteration 38/1000 | Loss: 0.00002068
Iteration 39/1000 | Loss: 0.00002067
Iteration 40/1000 | Loss: 0.00002067
Iteration 41/1000 | Loss: 0.00002067
Iteration 42/1000 | Loss: 0.00002067
Iteration 43/1000 | Loss: 0.00002066
Iteration 44/1000 | Loss: 0.00002066
Iteration 45/1000 | Loss: 0.00002066
Iteration 46/1000 | Loss: 0.00002065
Iteration 47/1000 | Loss: 0.00002065
Iteration 48/1000 | Loss: 0.00002065
Iteration 49/1000 | Loss: 0.00002065
Iteration 50/1000 | Loss: 0.00002065
Iteration 51/1000 | Loss: 0.00002065
Iteration 52/1000 | Loss: 0.00002065
Iteration 53/1000 | Loss: 0.00002065
Iteration 54/1000 | Loss: 0.00002065
Iteration 55/1000 | Loss: 0.00002065
Iteration 56/1000 | Loss: 0.00002065
Iteration 57/1000 | Loss: 0.00002065
Iteration 58/1000 | Loss: 0.00002065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [2.0650375518016517e-05, 2.0650375518016517e-05, 2.0650375518016517e-05, 2.0650375518016517e-05, 2.0650375518016517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0650375518016517e-05

Optimization complete. Final v2v error: 3.779691457748413 mm

Highest mean error: 4.091580390930176 mm for frame 164

Lowest mean error: 3.327402114868164 mm for frame 214

Saving results

Total time: 34.19548416137695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383073
Iteration 2/25 | Loss: 0.00091978
Iteration 3/25 | Loss: 0.00072393
Iteration 4/25 | Loss: 0.00069084
Iteration 5/25 | Loss: 0.00068279
Iteration 6/25 | Loss: 0.00068178
Iteration 7/25 | Loss: 0.00068178
Iteration 8/25 | Loss: 0.00068178
Iteration 9/25 | Loss: 0.00068178
Iteration 10/25 | Loss: 0.00068178
Iteration 11/25 | Loss: 0.00068178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006817816756665707, 0.0006817816756665707, 0.0006817816756665707, 0.0006817816756665707, 0.0006817816756665707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006817816756665707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47797048
Iteration 2/25 | Loss: 0.00033249
Iteration 3/25 | Loss: 0.00033249
Iteration 4/25 | Loss: 0.00033249
Iteration 5/25 | Loss: 0.00033249
Iteration 6/25 | Loss: 0.00033249
Iteration 7/25 | Loss: 0.00033249
Iteration 8/25 | Loss: 0.00033249
Iteration 9/25 | Loss: 0.00033249
Iteration 10/25 | Loss: 0.00033249
Iteration 11/25 | Loss: 0.00033249
Iteration 12/25 | Loss: 0.00033249
Iteration 13/25 | Loss: 0.00033249
Iteration 14/25 | Loss: 0.00033249
Iteration 15/25 | Loss: 0.00033249
Iteration 16/25 | Loss: 0.00033249
Iteration 17/25 | Loss: 0.00033249
Iteration 18/25 | Loss: 0.00033249
Iteration 19/25 | Loss: 0.00033249
Iteration 20/25 | Loss: 0.00033249
Iteration 21/25 | Loss: 0.00033249
Iteration 22/25 | Loss: 0.00033249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003324855351820588, 0.0003324855351820588, 0.0003324855351820588, 0.0003324855351820588, 0.0003324855351820588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003324855351820588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033249
Iteration 2/1000 | Loss: 0.00002574
Iteration 3/1000 | Loss: 0.00001639
Iteration 4/1000 | Loss: 0.00001469
Iteration 5/1000 | Loss: 0.00001398
Iteration 6/1000 | Loss: 0.00001342
Iteration 7/1000 | Loss: 0.00001310
Iteration 8/1000 | Loss: 0.00001284
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001263
Iteration 11/1000 | Loss: 0.00001259
Iteration 12/1000 | Loss: 0.00001256
Iteration 13/1000 | Loss: 0.00001255
Iteration 14/1000 | Loss: 0.00001242
Iteration 15/1000 | Loss: 0.00001241
Iteration 16/1000 | Loss: 0.00001238
Iteration 17/1000 | Loss: 0.00001237
Iteration 18/1000 | Loss: 0.00001235
Iteration 19/1000 | Loss: 0.00001234
Iteration 20/1000 | Loss: 0.00001233
Iteration 21/1000 | Loss: 0.00001232
Iteration 22/1000 | Loss: 0.00001231
Iteration 23/1000 | Loss: 0.00001227
Iteration 24/1000 | Loss: 0.00001227
Iteration 25/1000 | Loss: 0.00001226
Iteration 26/1000 | Loss: 0.00001226
Iteration 27/1000 | Loss: 0.00001226
Iteration 28/1000 | Loss: 0.00001224
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001222
Iteration 31/1000 | Loss: 0.00001222
Iteration 32/1000 | Loss: 0.00001221
Iteration 33/1000 | Loss: 0.00001219
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001218
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001217
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001216
Iteration 42/1000 | Loss: 0.00001216
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001216
Iteration 45/1000 | Loss: 0.00001216
Iteration 46/1000 | Loss: 0.00001216
Iteration 47/1000 | Loss: 0.00001216
Iteration 48/1000 | Loss: 0.00001215
Iteration 49/1000 | Loss: 0.00001215
Iteration 50/1000 | Loss: 0.00001215
Iteration 51/1000 | Loss: 0.00001215
Iteration 52/1000 | Loss: 0.00001215
Iteration 53/1000 | Loss: 0.00001215
Iteration 54/1000 | Loss: 0.00001215
Iteration 55/1000 | Loss: 0.00001215
Iteration 56/1000 | Loss: 0.00001215
Iteration 57/1000 | Loss: 0.00001215
Iteration 58/1000 | Loss: 0.00001215
Iteration 59/1000 | Loss: 0.00001215
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001215
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001214
Iteration 70/1000 | Loss: 0.00001214
Iteration 71/1000 | Loss: 0.00001214
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001214
Iteration 75/1000 | Loss: 0.00001214
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001214
Iteration 79/1000 | Loss: 0.00001214
Iteration 80/1000 | Loss: 0.00001214
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001214
Iteration 86/1000 | Loss: 0.00001214
Iteration 87/1000 | Loss: 0.00001214
Iteration 88/1000 | Loss: 0.00001214
Iteration 89/1000 | Loss: 0.00001214
Iteration 90/1000 | Loss: 0.00001214
Iteration 91/1000 | Loss: 0.00001214
Iteration 92/1000 | Loss: 0.00001214
Iteration 93/1000 | Loss: 0.00001214
Iteration 94/1000 | Loss: 0.00001214
Iteration 95/1000 | Loss: 0.00001214
Iteration 96/1000 | Loss: 0.00001214
Iteration 97/1000 | Loss: 0.00001214
Iteration 98/1000 | Loss: 0.00001214
Iteration 99/1000 | Loss: 0.00001214
Iteration 100/1000 | Loss: 0.00001214
Iteration 101/1000 | Loss: 0.00001214
Iteration 102/1000 | Loss: 0.00001214
Iteration 103/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.214019357576035e-05, 1.214019357576035e-05, 1.214019357576035e-05, 1.214019357576035e-05, 1.214019357576035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.214019357576035e-05

Optimization complete. Final v2v error: 2.895857572555542 mm

Highest mean error: 3.158639669418335 mm for frame 17

Lowest mean error: 2.739037275314331 mm for frame 143

Saving results

Total time: 31.39588952064514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381708
Iteration 2/25 | Loss: 0.00079629
Iteration 3/25 | Loss: 0.00066015
Iteration 4/25 | Loss: 0.00064213
Iteration 5/25 | Loss: 0.00063738
Iteration 6/25 | Loss: 0.00063585
Iteration 7/25 | Loss: 0.00063555
Iteration 8/25 | Loss: 0.00063555
Iteration 9/25 | Loss: 0.00063555
Iteration 10/25 | Loss: 0.00063555
Iteration 11/25 | Loss: 0.00063555
Iteration 12/25 | Loss: 0.00063555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006355479708872736, 0.0006355479708872736, 0.0006355479708872736, 0.0006355479708872736, 0.0006355479708872736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006355479708872736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48748946
Iteration 2/25 | Loss: 0.00026932
Iteration 3/25 | Loss: 0.00026932
Iteration 4/25 | Loss: 0.00026931
Iteration 5/25 | Loss: 0.00026931
Iteration 6/25 | Loss: 0.00026931
Iteration 7/25 | Loss: 0.00026931
Iteration 8/25 | Loss: 0.00026931
Iteration 9/25 | Loss: 0.00026931
Iteration 10/25 | Loss: 0.00026931
Iteration 11/25 | Loss: 0.00026931
Iteration 12/25 | Loss: 0.00026931
Iteration 13/25 | Loss: 0.00026931
Iteration 14/25 | Loss: 0.00026931
Iteration 15/25 | Loss: 0.00026931
Iteration 16/25 | Loss: 0.00026931
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002693123242352158, 0.0002693123242352158, 0.0002693123242352158, 0.0002693123242352158, 0.0002693123242352158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002693123242352158

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026931
Iteration 2/1000 | Loss: 0.00002124
Iteration 3/1000 | Loss: 0.00001213
Iteration 4/1000 | Loss: 0.00001113
Iteration 5/1000 | Loss: 0.00001049
Iteration 6/1000 | Loss: 0.00001012
Iteration 7/1000 | Loss: 0.00000985
Iteration 8/1000 | Loss: 0.00000974
Iteration 9/1000 | Loss: 0.00000974
Iteration 10/1000 | Loss: 0.00000974
Iteration 11/1000 | Loss: 0.00000974
Iteration 12/1000 | Loss: 0.00000971
Iteration 13/1000 | Loss: 0.00000970
Iteration 14/1000 | Loss: 0.00000969
Iteration 15/1000 | Loss: 0.00000969
Iteration 16/1000 | Loss: 0.00000968
Iteration 17/1000 | Loss: 0.00000968
Iteration 18/1000 | Loss: 0.00000966
Iteration 19/1000 | Loss: 0.00000966
Iteration 20/1000 | Loss: 0.00000965
Iteration 21/1000 | Loss: 0.00000965
Iteration 22/1000 | Loss: 0.00000963
Iteration 23/1000 | Loss: 0.00000963
Iteration 24/1000 | Loss: 0.00000962
Iteration 25/1000 | Loss: 0.00000961
Iteration 26/1000 | Loss: 0.00000961
Iteration 27/1000 | Loss: 0.00000960
Iteration 28/1000 | Loss: 0.00000959
Iteration 29/1000 | Loss: 0.00000959
Iteration 30/1000 | Loss: 0.00000958
Iteration 31/1000 | Loss: 0.00000954
Iteration 32/1000 | Loss: 0.00000954
Iteration 33/1000 | Loss: 0.00000954
Iteration 34/1000 | Loss: 0.00000950
Iteration 35/1000 | Loss: 0.00000950
Iteration 36/1000 | Loss: 0.00000950
Iteration 37/1000 | Loss: 0.00000950
Iteration 38/1000 | Loss: 0.00000950
Iteration 39/1000 | Loss: 0.00000949
Iteration 40/1000 | Loss: 0.00000949
Iteration 41/1000 | Loss: 0.00000949
Iteration 42/1000 | Loss: 0.00000949
Iteration 43/1000 | Loss: 0.00000949
Iteration 44/1000 | Loss: 0.00000949
Iteration 45/1000 | Loss: 0.00000949
Iteration 46/1000 | Loss: 0.00000948
Iteration 47/1000 | Loss: 0.00000947
Iteration 48/1000 | Loss: 0.00000946
Iteration 49/1000 | Loss: 0.00000946
Iteration 50/1000 | Loss: 0.00000946
Iteration 51/1000 | Loss: 0.00000946
Iteration 52/1000 | Loss: 0.00000946
Iteration 53/1000 | Loss: 0.00000946
Iteration 54/1000 | Loss: 0.00000946
Iteration 55/1000 | Loss: 0.00000946
Iteration 56/1000 | Loss: 0.00000946
Iteration 57/1000 | Loss: 0.00000946
Iteration 58/1000 | Loss: 0.00000946
Iteration 59/1000 | Loss: 0.00000946
Iteration 60/1000 | Loss: 0.00000945
Iteration 61/1000 | Loss: 0.00000945
Iteration 62/1000 | Loss: 0.00000943
Iteration 63/1000 | Loss: 0.00000943
Iteration 64/1000 | Loss: 0.00000943
Iteration 65/1000 | Loss: 0.00000943
Iteration 66/1000 | Loss: 0.00000943
Iteration 67/1000 | Loss: 0.00000943
Iteration 68/1000 | Loss: 0.00000943
Iteration 69/1000 | Loss: 0.00000943
Iteration 70/1000 | Loss: 0.00000942
Iteration 71/1000 | Loss: 0.00000942
Iteration 72/1000 | Loss: 0.00000941
Iteration 73/1000 | Loss: 0.00000941
Iteration 74/1000 | Loss: 0.00000941
Iteration 75/1000 | Loss: 0.00000940
Iteration 76/1000 | Loss: 0.00000939
Iteration 77/1000 | Loss: 0.00000939
Iteration 78/1000 | Loss: 0.00000939
Iteration 79/1000 | Loss: 0.00000939
Iteration 80/1000 | Loss: 0.00000939
Iteration 81/1000 | Loss: 0.00000939
Iteration 82/1000 | Loss: 0.00000938
Iteration 83/1000 | Loss: 0.00000938
Iteration 84/1000 | Loss: 0.00000938
Iteration 85/1000 | Loss: 0.00000938
Iteration 86/1000 | Loss: 0.00000938
Iteration 87/1000 | Loss: 0.00000938
Iteration 88/1000 | Loss: 0.00000938
Iteration 89/1000 | Loss: 0.00000938
Iteration 90/1000 | Loss: 0.00000938
Iteration 91/1000 | Loss: 0.00000937
Iteration 92/1000 | Loss: 0.00000937
Iteration 93/1000 | Loss: 0.00000937
Iteration 94/1000 | Loss: 0.00000936
Iteration 95/1000 | Loss: 0.00000936
Iteration 96/1000 | Loss: 0.00000936
Iteration 97/1000 | Loss: 0.00000935
Iteration 98/1000 | Loss: 0.00000935
Iteration 99/1000 | Loss: 0.00000935
Iteration 100/1000 | Loss: 0.00000934
Iteration 101/1000 | Loss: 0.00000934
Iteration 102/1000 | Loss: 0.00000934
Iteration 103/1000 | Loss: 0.00000934
Iteration 104/1000 | Loss: 0.00000933
Iteration 105/1000 | Loss: 0.00000933
Iteration 106/1000 | Loss: 0.00000933
Iteration 107/1000 | Loss: 0.00000933
Iteration 108/1000 | Loss: 0.00000933
Iteration 109/1000 | Loss: 0.00000933
Iteration 110/1000 | Loss: 0.00000932
Iteration 111/1000 | Loss: 0.00000932
Iteration 112/1000 | Loss: 0.00000932
Iteration 113/1000 | Loss: 0.00000932
Iteration 114/1000 | Loss: 0.00000932
Iteration 115/1000 | Loss: 0.00000932
Iteration 116/1000 | Loss: 0.00000932
Iteration 117/1000 | Loss: 0.00000932
Iteration 118/1000 | Loss: 0.00000932
Iteration 119/1000 | Loss: 0.00000932
Iteration 120/1000 | Loss: 0.00000932
Iteration 121/1000 | Loss: 0.00000932
Iteration 122/1000 | Loss: 0.00000932
Iteration 123/1000 | Loss: 0.00000932
Iteration 124/1000 | Loss: 0.00000932
Iteration 125/1000 | Loss: 0.00000931
Iteration 126/1000 | Loss: 0.00000931
Iteration 127/1000 | Loss: 0.00000931
Iteration 128/1000 | Loss: 0.00000930
Iteration 129/1000 | Loss: 0.00000930
Iteration 130/1000 | Loss: 0.00000930
Iteration 131/1000 | Loss: 0.00000930
Iteration 132/1000 | Loss: 0.00000930
Iteration 133/1000 | Loss: 0.00000929
Iteration 134/1000 | Loss: 0.00000929
Iteration 135/1000 | Loss: 0.00000929
Iteration 136/1000 | Loss: 0.00000929
Iteration 137/1000 | Loss: 0.00000928
Iteration 138/1000 | Loss: 0.00000928
Iteration 139/1000 | Loss: 0.00000928
Iteration 140/1000 | Loss: 0.00000928
Iteration 141/1000 | Loss: 0.00000927
Iteration 142/1000 | Loss: 0.00000927
Iteration 143/1000 | Loss: 0.00000926
Iteration 144/1000 | Loss: 0.00000926
Iteration 145/1000 | Loss: 0.00000926
Iteration 146/1000 | Loss: 0.00000926
Iteration 147/1000 | Loss: 0.00000926
Iteration 148/1000 | Loss: 0.00000925
Iteration 149/1000 | Loss: 0.00000925
Iteration 150/1000 | Loss: 0.00000925
Iteration 151/1000 | Loss: 0.00000925
Iteration 152/1000 | Loss: 0.00000924
Iteration 153/1000 | Loss: 0.00000924
Iteration 154/1000 | Loss: 0.00000924
Iteration 155/1000 | Loss: 0.00000924
Iteration 156/1000 | Loss: 0.00000923
Iteration 157/1000 | Loss: 0.00000923
Iteration 158/1000 | Loss: 0.00000923
Iteration 159/1000 | Loss: 0.00000922
Iteration 160/1000 | Loss: 0.00000922
Iteration 161/1000 | Loss: 0.00000922
Iteration 162/1000 | Loss: 0.00000921
Iteration 163/1000 | Loss: 0.00000921
Iteration 164/1000 | Loss: 0.00000921
Iteration 165/1000 | Loss: 0.00000921
Iteration 166/1000 | Loss: 0.00000921
Iteration 167/1000 | Loss: 0.00000921
Iteration 168/1000 | Loss: 0.00000921
Iteration 169/1000 | Loss: 0.00000921
Iteration 170/1000 | Loss: 0.00000921
Iteration 171/1000 | Loss: 0.00000921
Iteration 172/1000 | Loss: 0.00000921
Iteration 173/1000 | Loss: 0.00000921
Iteration 174/1000 | Loss: 0.00000921
Iteration 175/1000 | Loss: 0.00000920
Iteration 176/1000 | Loss: 0.00000920
Iteration 177/1000 | Loss: 0.00000920
Iteration 178/1000 | Loss: 0.00000920
Iteration 179/1000 | Loss: 0.00000920
Iteration 180/1000 | Loss: 0.00000920
Iteration 181/1000 | Loss: 0.00000920
Iteration 182/1000 | Loss: 0.00000920
Iteration 183/1000 | Loss: 0.00000920
Iteration 184/1000 | Loss: 0.00000920
Iteration 185/1000 | Loss: 0.00000920
Iteration 186/1000 | Loss: 0.00000920
Iteration 187/1000 | Loss: 0.00000920
Iteration 188/1000 | Loss: 0.00000920
Iteration 189/1000 | Loss: 0.00000920
Iteration 190/1000 | Loss: 0.00000920
Iteration 191/1000 | Loss: 0.00000919
Iteration 192/1000 | Loss: 0.00000919
Iteration 193/1000 | Loss: 0.00000919
Iteration 194/1000 | Loss: 0.00000919
Iteration 195/1000 | Loss: 0.00000919
Iteration 196/1000 | Loss: 0.00000919
Iteration 197/1000 | Loss: 0.00000919
Iteration 198/1000 | Loss: 0.00000919
Iteration 199/1000 | Loss: 0.00000919
Iteration 200/1000 | Loss: 0.00000919
Iteration 201/1000 | Loss: 0.00000919
Iteration 202/1000 | Loss: 0.00000919
Iteration 203/1000 | Loss: 0.00000919
Iteration 204/1000 | Loss: 0.00000919
Iteration 205/1000 | Loss: 0.00000919
Iteration 206/1000 | Loss: 0.00000919
Iteration 207/1000 | Loss: 0.00000919
Iteration 208/1000 | Loss: 0.00000919
Iteration 209/1000 | Loss: 0.00000919
Iteration 210/1000 | Loss: 0.00000919
Iteration 211/1000 | Loss: 0.00000918
Iteration 212/1000 | Loss: 0.00000918
Iteration 213/1000 | Loss: 0.00000918
Iteration 214/1000 | Loss: 0.00000918
Iteration 215/1000 | Loss: 0.00000918
Iteration 216/1000 | Loss: 0.00000918
Iteration 217/1000 | Loss: 0.00000918
Iteration 218/1000 | Loss: 0.00000918
Iteration 219/1000 | Loss: 0.00000918
Iteration 220/1000 | Loss: 0.00000918
Iteration 221/1000 | Loss: 0.00000918
Iteration 222/1000 | Loss: 0.00000918
Iteration 223/1000 | Loss: 0.00000918
Iteration 224/1000 | Loss: 0.00000918
Iteration 225/1000 | Loss: 0.00000918
Iteration 226/1000 | Loss: 0.00000918
Iteration 227/1000 | Loss: 0.00000918
Iteration 228/1000 | Loss: 0.00000918
Iteration 229/1000 | Loss: 0.00000918
Iteration 230/1000 | Loss: 0.00000918
Iteration 231/1000 | Loss: 0.00000918
Iteration 232/1000 | Loss: 0.00000918
Iteration 233/1000 | Loss: 0.00000917
Iteration 234/1000 | Loss: 0.00000917
Iteration 235/1000 | Loss: 0.00000917
Iteration 236/1000 | Loss: 0.00000917
Iteration 237/1000 | Loss: 0.00000917
Iteration 238/1000 | Loss: 0.00000917
Iteration 239/1000 | Loss: 0.00000917
Iteration 240/1000 | Loss: 0.00000917
Iteration 241/1000 | Loss: 0.00000917
Iteration 242/1000 | Loss: 0.00000917
Iteration 243/1000 | Loss: 0.00000917
Iteration 244/1000 | Loss: 0.00000917
Iteration 245/1000 | Loss: 0.00000917
Iteration 246/1000 | Loss: 0.00000917
Iteration 247/1000 | Loss: 0.00000917
Iteration 248/1000 | Loss: 0.00000917
Iteration 249/1000 | Loss: 0.00000917
Iteration 250/1000 | Loss: 0.00000917
Iteration 251/1000 | Loss: 0.00000917
Iteration 252/1000 | Loss: 0.00000917
Iteration 253/1000 | Loss: 0.00000917
Iteration 254/1000 | Loss: 0.00000917
Iteration 255/1000 | Loss: 0.00000917
Iteration 256/1000 | Loss: 0.00000917
Iteration 257/1000 | Loss: 0.00000917
Iteration 258/1000 | Loss: 0.00000917
Iteration 259/1000 | Loss: 0.00000917
Iteration 260/1000 | Loss: 0.00000917
Iteration 261/1000 | Loss: 0.00000916
Iteration 262/1000 | Loss: 0.00000916
Iteration 263/1000 | Loss: 0.00000916
Iteration 264/1000 | Loss: 0.00000916
Iteration 265/1000 | Loss: 0.00000916
Iteration 266/1000 | Loss: 0.00000916
Iteration 267/1000 | Loss: 0.00000916
Iteration 268/1000 | Loss: 0.00000916
Iteration 269/1000 | Loss: 0.00000916
Iteration 270/1000 | Loss: 0.00000916
Iteration 271/1000 | Loss: 0.00000916
Iteration 272/1000 | Loss: 0.00000916
Iteration 273/1000 | Loss: 0.00000916
Iteration 274/1000 | Loss: 0.00000916
Iteration 275/1000 | Loss: 0.00000916
Iteration 276/1000 | Loss: 0.00000916
Iteration 277/1000 | Loss: 0.00000916
Iteration 278/1000 | Loss: 0.00000916
Iteration 279/1000 | Loss: 0.00000916
Iteration 280/1000 | Loss: 0.00000916
Iteration 281/1000 | Loss: 0.00000916
Iteration 282/1000 | Loss: 0.00000916
Iteration 283/1000 | Loss: 0.00000916
Iteration 284/1000 | Loss: 0.00000916
Iteration 285/1000 | Loss: 0.00000916
Iteration 286/1000 | Loss: 0.00000916
Iteration 287/1000 | Loss: 0.00000916
Iteration 288/1000 | Loss: 0.00000916
Iteration 289/1000 | Loss: 0.00000916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 289. Stopping optimization.
Last 5 losses: [9.164173206954729e-06, 9.164173206954729e-06, 9.164173206954729e-06, 9.164173206954729e-06, 9.164173206954729e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.164173206954729e-06

Optimization complete. Final v2v error: 2.571265697479248 mm

Highest mean error: 3.3757588863372803 mm for frame 64

Lowest mean error: 2.395580530166626 mm for frame 90

Saving results

Total time: 41.183156967163086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021645
Iteration 2/25 | Loss: 0.00177926
Iteration 3/25 | Loss: 0.00104855
Iteration 4/25 | Loss: 0.00088734
Iteration 5/25 | Loss: 0.00083962
Iteration 6/25 | Loss: 0.00081839
Iteration 7/25 | Loss: 0.00079011
Iteration 8/25 | Loss: 0.00076783
Iteration 9/25 | Loss: 0.00076742
Iteration 10/25 | Loss: 0.00074775
Iteration 11/25 | Loss: 0.00073910
Iteration 12/25 | Loss: 0.00074407
Iteration 13/25 | Loss: 0.00074085
Iteration 14/25 | Loss: 0.00073779
Iteration 15/25 | Loss: 0.00073259
Iteration 16/25 | Loss: 0.00073661
Iteration 17/25 | Loss: 0.00073750
Iteration 18/25 | Loss: 0.00073140
Iteration 19/25 | Loss: 0.00073021
Iteration 20/25 | Loss: 0.00072786
Iteration 21/25 | Loss: 0.00072906
Iteration 22/25 | Loss: 0.00072709
Iteration 23/25 | Loss: 0.00072679
Iteration 24/25 | Loss: 0.00072676
Iteration 25/25 | Loss: 0.00072676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51133037
Iteration 2/25 | Loss: 0.00041421
Iteration 3/25 | Loss: 0.00041421
Iteration 4/25 | Loss: 0.00041421
Iteration 5/25 | Loss: 0.00041421
Iteration 6/25 | Loss: 0.00041421
Iteration 7/25 | Loss: 0.00041421
Iteration 8/25 | Loss: 0.00041421
Iteration 9/25 | Loss: 0.00041421
Iteration 10/25 | Loss: 0.00041421
Iteration 11/25 | Loss: 0.00041421
Iteration 12/25 | Loss: 0.00041421
Iteration 13/25 | Loss: 0.00041421
Iteration 14/25 | Loss: 0.00041421
Iteration 15/25 | Loss: 0.00041421
Iteration 16/25 | Loss: 0.00041421
Iteration 17/25 | Loss: 0.00041421
Iteration 18/25 | Loss: 0.00041421
Iteration 19/25 | Loss: 0.00041421
Iteration 20/25 | Loss: 0.00041421
Iteration 21/25 | Loss: 0.00041421
Iteration 22/25 | Loss: 0.00041421
Iteration 23/25 | Loss: 0.00041421
Iteration 24/25 | Loss: 0.00041421
Iteration 25/25 | Loss: 0.00041421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041421
Iteration 2/1000 | Loss: 0.00022549
Iteration 3/1000 | Loss: 0.00003104
Iteration 4/1000 | Loss: 0.00002352
Iteration 5/1000 | Loss: 0.00001972
Iteration 6/1000 | Loss: 0.00001773
Iteration 7/1000 | Loss: 0.00001681
Iteration 8/1000 | Loss: 0.00001632
Iteration 9/1000 | Loss: 0.00001586
Iteration 10/1000 | Loss: 0.00001559
Iteration 11/1000 | Loss: 0.00019956
Iteration 12/1000 | Loss: 0.00011686
Iteration 13/1000 | Loss: 0.00003489
Iteration 14/1000 | Loss: 0.00009638
Iteration 15/1000 | Loss: 0.00015543
Iteration 16/1000 | Loss: 0.00020118
Iteration 17/1000 | Loss: 0.00066394
Iteration 18/1000 | Loss: 0.00019886
Iteration 19/1000 | Loss: 0.00018718
Iteration 20/1000 | Loss: 0.00022594
Iteration 21/1000 | Loss: 0.00044286
Iteration 22/1000 | Loss: 0.00006828
Iteration 23/1000 | Loss: 0.00005764
Iteration 24/1000 | Loss: 0.00002290
Iteration 25/1000 | Loss: 0.00001928
Iteration 26/1000 | Loss: 0.00001785
Iteration 27/1000 | Loss: 0.00001732
Iteration 28/1000 | Loss: 0.00004144
Iteration 29/1000 | Loss: 0.00001665
Iteration 30/1000 | Loss: 0.00017971
Iteration 31/1000 | Loss: 0.00019144
Iteration 32/1000 | Loss: 0.00024316
Iteration 33/1000 | Loss: 0.00023781
Iteration 34/1000 | Loss: 0.00008994
Iteration 35/1000 | Loss: 0.00013883
Iteration 36/1000 | Loss: 0.00005945
Iteration 37/1000 | Loss: 0.00004718
Iteration 38/1000 | Loss: 0.00003322
Iteration 39/1000 | Loss: 0.00001603
Iteration 40/1000 | Loss: 0.00002768
Iteration 41/1000 | Loss: 0.00005287
Iteration 42/1000 | Loss: 0.00003071
Iteration 43/1000 | Loss: 0.00001735
Iteration 44/1000 | Loss: 0.00020230
Iteration 45/1000 | Loss: 0.00005724
Iteration 46/1000 | Loss: 0.00017303
Iteration 47/1000 | Loss: 0.00014563
Iteration 48/1000 | Loss: 0.00011944
Iteration 49/1000 | Loss: 0.00002264
Iteration 50/1000 | Loss: 0.00002071
Iteration 51/1000 | Loss: 0.00001493
Iteration 52/1000 | Loss: 0.00002691
Iteration 53/1000 | Loss: 0.00001613
Iteration 54/1000 | Loss: 0.00001466
Iteration 55/1000 | Loss: 0.00001466
Iteration 56/1000 | Loss: 0.00002059
Iteration 57/1000 | Loss: 0.00001461
Iteration 58/1000 | Loss: 0.00001459
Iteration 59/1000 | Loss: 0.00001457
Iteration 60/1000 | Loss: 0.00001457
Iteration 61/1000 | Loss: 0.00001456
Iteration 62/1000 | Loss: 0.00001456
Iteration 63/1000 | Loss: 0.00001456
Iteration 64/1000 | Loss: 0.00001455
Iteration 65/1000 | Loss: 0.00001455
Iteration 66/1000 | Loss: 0.00001455
Iteration 67/1000 | Loss: 0.00001455
Iteration 68/1000 | Loss: 0.00001455
Iteration 69/1000 | Loss: 0.00001455
Iteration 70/1000 | Loss: 0.00001454
Iteration 71/1000 | Loss: 0.00001454
Iteration 72/1000 | Loss: 0.00001454
Iteration 73/1000 | Loss: 0.00001454
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001453
Iteration 76/1000 | Loss: 0.00001453
Iteration 77/1000 | Loss: 0.00001453
Iteration 78/1000 | Loss: 0.00001452
Iteration 79/1000 | Loss: 0.00001452
Iteration 80/1000 | Loss: 0.00001452
Iteration 81/1000 | Loss: 0.00001451
Iteration 82/1000 | Loss: 0.00001451
Iteration 83/1000 | Loss: 0.00001451
Iteration 84/1000 | Loss: 0.00001451
Iteration 85/1000 | Loss: 0.00001451
Iteration 86/1000 | Loss: 0.00001451
Iteration 87/1000 | Loss: 0.00001450
Iteration 88/1000 | Loss: 0.00001450
Iteration 89/1000 | Loss: 0.00001450
Iteration 90/1000 | Loss: 0.00001450
Iteration 91/1000 | Loss: 0.00001450
Iteration 92/1000 | Loss: 0.00001449
Iteration 93/1000 | Loss: 0.00001449
Iteration 94/1000 | Loss: 0.00002934
Iteration 95/1000 | Loss: 0.00001453
Iteration 96/1000 | Loss: 0.00001447
Iteration 97/1000 | Loss: 0.00001447
Iteration 98/1000 | Loss: 0.00001447
Iteration 99/1000 | Loss: 0.00001446
Iteration 100/1000 | Loss: 0.00001446
Iteration 101/1000 | Loss: 0.00001446
Iteration 102/1000 | Loss: 0.00001445
Iteration 103/1000 | Loss: 0.00001445
Iteration 104/1000 | Loss: 0.00001445
Iteration 105/1000 | Loss: 0.00001445
Iteration 106/1000 | Loss: 0.00001445
Iteration 107/1000 | Loss: 0.00001444
Iteration 108/1000 | Loss: 0.00001444
Iteration 109/1000 | Loss: 0.00001444
Iteration 110/1000 | Loss: 0.00001444
Iteration 111/1000 | Loss: 0.00001444
Iteration 112/1000 | Loss: 0.00001444
Iteration 113/1000 | Loss: 0.00001444
Iteration 114/1000 | Loss: 0.00001444
Iteration 115/1000 | Loss: 0.00001444
Iteration 116/1000 | Loss: 0.00001444
Iteration 117/1000 | Loss: 0.00001444
Iteration 118/1000 | Loss: 0.00001444
Iteration 119/1000 | Loss: 0.00001444
Iteration 120/1000 | Loss: 0.00001444
Iteration 121/1000 | Loss: 0.00001444
Iteration 122/1000 | Loss: 0.00001444
Iteration 123/1000 | Loss: 0.00001444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.4442605788644869e-05, 1.4442605788644869e-05, 1.4442605788644869e-05, 1.4442605788644869e-05, 1.4442605788644869e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4442605788644869e-05

Optimization complete. Final v2v error: 3.1237564086914062 mm

Highest mean error: 6.652127265930176 mm for frame 147

Lowest mean error: 2.5637893676757812 mm for frame 235

Saving results

Total time: 144.92289972305298
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831273
Iteration 2/25 | Loss: 0.00136291
Iteration 3/25 | Loss: 0.00089839
Iteration 4/25 | Loss: 0.00084352
Iteration 5/25 | Loss: 0.00082487
Iteration 6/25 | Loss: 0.00081984
Iteration 7/25 | Loss: 0.00081834
Iteration 8/25 | Loss: 0.00081793
Iteration 9/25 | Loss: 0.00081793
Iteration 10/25 | Loss: 0.00081793
Iteration 11/25 | Loss: 0.00081793
Iteration 12/25 | Loss: 0.00081793
Iteration 13/25 | Loss: 0.00081793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008179345168173313, 0.0008179345168173313, 0.0008179345168173313, 0.0008179345168173313, 0.0008179345168173313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008179345168173313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11744428
Iteration 2/25 | Loss: 0.00036611
Iteration 3/25 | Loss: 0.00036611
Iteration 4/25 | Loss: 0.00036611
Iteration 5/25 | Loss: 0.00036611
Iteration 6/25 | Loss: 0.00036611
Iteration 7/25 | Loss: 0.00036611
Iteration 8/25 | Loss: 0.00036611
Iteration 9/25 | Loss: 0.00036611
Iteration 10/25 | Loss: 0.00036611
Iteration 11/25 | Loss: 0.00036611
Iteration 12/25 | Loss: 0.00036611
Iteration 13/25 | Loss: 0.00036611
Iteration 14/25 | Loss: 0.00036611
Iteration 15/25 | Loss: 0.00036611
Iteration 16/25 | Loss: 0.00036611
Iteration 17/25 | Loss: 0.00036611
Iteration 18/25 | Loss: 0.00036611
Iteration 19/25 | Loss: 0.00036611
Iteration 20/25 | Loss: 0.00036611
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003661065420601517, 0.0003661065420601517, 0.0003661065420601517, 0.0003661065420601517, 0.0003661065420601517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003661065420601517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036611
Iteration 2/1000 | Loss: 0.00005018
Iteration 3/1000 | Loss: 0.00003890
Iteration 4/1000 | Loss: 0.00003519
Iteration 5/1000 | Loss: 0.00003374
Iteration 6/1000 | Loss: 0.00003265
Iteration 7/1000 | Loss: 0.00003204
Iteration 8/1000 | Loss: 0.00003126
Iteration 9/1000 | Loss: 0.00003068
Iteration 10/1000 | Loss: 0.00003036
Iteration 11/1000 | Loss: 0.00002999
Iteration 12/1000 | Loss: 0.00002965
Iteration 13/1000 | Loss: 0.00002932
Iteration 14/1000 | Loss: 0.00002910
Iteration 15/1000 | Loss: 0.00002887
Iteration 16/1000 | Loss: 0.00002871
Iteration 17/1000 | Loss: 0.00002868
Iteration 18/1000 | Loss: 0.00002859
Iteration 19/1000 | Loss: 0.00002858
Iteration 20/1000 | Loss: 0.00002853
Iteration 21/1000 | Loss: 0.00002842
Iteration 22/1000 | Loss: 0.00002835
Iteration 23/1000 | Loss: 0.00002835
Iteration 24/1000 | Loss: 0.00002833
Iteration 25/1000 | Loss: 0.00002833
Iteration 26/1000 | Loss: 0.00002833
Iteration 27/1000 | Loss: 0.00002833
Iteration 28/1000 | Loss: 0.00002832
Iteration 29/1000 | Loss: 0.00002832
Iteration 30/1000 | Loss: 0.00002832
Iteration 31/1000 | Loss: 0.00002831
Iteration 32/1000 | Loss: 0.00002831
Iteration 33/1000 | Loss: 0.00002830
Iteration 34/1000 | Loss: 0.00002830
Iteration 35/1000 | Loss: 0.00002830
Iteration 36/1000 | Loss: 0.00002830
Iteration 37/1000 | Loss: 0.00002829
Iteration 38/1000 | Loss: 0.00002829
Iteration 39/1000 | Loss: 0.00002829
Iteration 40/1000 | Loss: 0.00002829
Iteration 41/1000 | Loss: 0.00002828
Iteration 42/1000 | Loss: 0.00002828
Iteration 43/1000 | Loss: 0.00002828
Iteration 44/1000 | Loss: 0.00002828
Iteration 45/1000 | Loss: 0.00002828
Iteration 46/1000 | Loss: 0.00002827
Iteration 47/1000 | Loss: 0.00002827
Iteration 48/1000 | Loss: 0.00002827
Iteration 49/1000 | Loss: 0.00002827
Iteration 50/1000 | Loss: 0.00002827
Iteration 51/1000 | Loss: 0.00002827
Iteration 52/1000 | Loss: 0.00002826
Iteration 53/1000 | Loss: 0.00002826
Iteration 54/1000 | Loss: 0.00002826
Iteration 55/1000 | Loss: 0.00002825
Iteration 56/1000 | Loss: 0.00002825
Iteration 57/1000 | Loss: 0.00002825
Iteration 58/1000 | Loss: 0.00002824
Iteration 59/1000 | Loss: 0.00002824
Iteration 60/1000 | Loss: 0.00002824
Iteration 61/1000 | Loss: 0.00002824
Iteration 62/1000 | Loss: 0.00002824
Iteration 63/1000 | Loss: 0.00002824
Iteration 64/1000 | Loss: 0.00002824
Iteration 65/1000 | Loss: 0.00002824
Iteration 66/1000 | Loss: 0.00002824
Iteration 67/1000 | Loss: 0.00002824
Iteration 68/1000 | Loss: 0.00002823
Iteration 69/1000 | Loss: 0.00002823
Iteration 70/1000 | Loss: 0.00002823
Iteration 71/1000 | Loss: 0.00002822
Iteration 72/1000 | Loss: 0.00002822
Iteration 73/1000 | Loss: 0.00002822
Iteration 74/1000 | Loss: 0.00002822
Iteration 75/1000 | Loss: 0.00002822
Iteration 76/1000 | Loss: 0.00002822
Iteration 77/1000 | Loss: 0.00002822
Iteration 78/1000 | Loss: 0.00002822
Iteration 79/1000 | Loss: 0.00002822
Iteration 80/1000 | Loss: 0.00002822
Iteration 81/1000 | Loss: 0.00002821
Iteration 82/1000 | Loss: 0.00002821
Iteration 83/1000 | Loss: 0.00002821
Iteration 84/1000 | Loss: 0.00002821
Iteration 85/1000 | Loss: 0.00002821
Iteration 86/1000 | Loss: 0.00002821
Iteration 87/1000 | Loss: 0.00002821
Iteration 88/1000 | Loss: 0.00002821
Iteration 89/1000 | Loss: 0.00002820
Iteration 90/1000 | Loss: 0.00002820
Iteration 91/1000 | Loss: 0.00002820
Iteration 92/1000 | Loss: 0.00002820
Iteration 93/1000 | Loss: 0.00002820
Iteration 94/1000 | Loss: 0.00002820
Iteration 95/1000 | Loss: 0.00002819
Iteration 96/1000 | Loss: 0.00002819
Iteration 97/1000 | Loss: 0.00002819
Iteration 98/1000 | Loss: 0.00002819
Iteration 99/1000 | Loss: 0.00002818
Iteration 100/1000 | Loss: 0.00002818
Iteration 101/1000 | Loss: 0.00002818
Iteration 102/1000 | Loss: 0.00002818
Iteration 103/1000 | Loss: 0.00002818
Iteration 104/1000 | Loss: 0.00002818
Iteration 105/1000 | Loss: 0.00002817
Iteration 106/1000 | Loss: 0.00002817
Iteration 107/1000 | Loss: 0.00002817
Iteration 108/1000 | Loss: 0.00002816
Iteration 109/1000 | Loss: 0.00002816
Iteration 110/1000 | Loss: 0.00002816
Iteration 111/1000 | Loss: 0.00002816
Iteration 112/1000 | Loss: 0.00002815
Iteration 113/1000 | Loss: 0.00002815
Iteration 114/1000 | Loss: 0.00002815
Iteration 115/1000 | Loss: 0.00002815
Iteration 116/1000 | Loss: 0.00002815
Iteration 117/1000 | Loss: 0.00002815
Iteration 118/1000 | Loss: 0.00002815
Iteration 119/1000 | Loss: 0.00002815
Iteration 120/1000 | Loss: 0.00002815
Iteration 121/1000 | Loss: 0.00002815
Iteration 122/1000 | Loss: 0.00002814
Iteration 123/1000 | Loss: 0.00002814
Iteration 124/1000 | Loss: 0.00002814
Iteration 125/1000 | Loss: 0.00002814
Iteration 126/1000 | Loss: 0.00002814
Iteration 127/1000 | Loss: 0.00002814
Iteration 128/1000 | Loss: 0.00002814
Iteration 129/1000 | Loss: 0.00002814
Iteration 130/1000 | Loss: 0.00002813
Iteration 131/1000 | Loss: 0.00002813
Iteration 132/1000 | Loss: 0.00002813
Iteration 133/1000 | Loss: 0.00002813
Iteration 134/1000 | Loss: 0.00002813
Iteration 135/1000 | Loss: 0.00002813
Iteration 136/1000 | Loss: 0.00002812
Iteration 137/1000 | Loss: 0.00002812
Iteration 138/1000 | Loss: 0.00002812
Iteration 139/1000 | Loss: 0.00002812
Iteration 140/1000 | Loss: 0.00002812
Iteration 141/1000 | Loss: 0.00002812
Iteration 142/1000 | Loss: 0.00002811
Iteration 143/1000 | Loss: 0.00002811
Iteration 144/1000 | Loss: 0.00002811
Iteration 145/1000 | Loss: 0.00002811
Iteration 146/1000 | Loss: 0.00002811
Iteration 147/1000 | Loss: 0.00002811
Iteration 148/1000 | Loss: 0.00002811
Iteration 149/1000 | Loss: 0.00002811
Iteration 150/1000 | Loss: 0.00002811
Iteration 151/1000 | Loss: 0.00002810
Iteration 152/1000 | Loss: 0.00002810
Iteration 153/1000 | Loss: 0.00002810
Iteration 154/1000 | Loss: 0.00002810
Iteration 155/1000 | Loss: 0.00002810
Iteration 156/1000 | Loss: 0.00002810
Iteration 157/1000 | Loss: 0.00002810
Iteration 158/1000 | Loss: 0.00002810
Iteration 159/1000 | Loss: 0.00002810
Iteration 160/1000 | Loss: 0.00002810
Iteration 161/1000 | Loss: 0.00002810
Iteration 162/1000 | Loss: 0.00002810
Iteration 163/1000 | Loss: 0.00002810
Iteration 164/1000 | Loss: 0.00002810
Iteration 165/1000 | Loss: 0.00002810
Iteration 166/1000 | Loss: 0.00002810
Iteration 167/1000 | Loss: 0.00002810
Iteration 168/1000 | Loss: 0.00002810
Iteration 169/1000 | Loss: 0.00002809
Iteration 170/1000 | Loss: 0.00002809
Iteration 171/1000 | Loss: 0.00002809
Iteration 172/1000 | Loss: 0.00002809
Iteration 173/1000 | Loss: 0.00002809
Iteration 174/1000 | Loss: 0.00002809
Iteration 175/1000 | Loss: 0.00002809
Iteration 176/1000 | Loss: 0.00002809
Iteration 177/1000 | Loss: 0.00002809
Iteration 178/1000 | Loss: 0.00002809
Iteration 179/1000 | Loss: 0.00002808
Iteration 180/1000 | Loss: 0.00002808
Iteration 181/1000 | Loss: 0.00002808
Iteration 182/1000 | Loss: 0.00002808
Iteration 183/1000 | Loss: 0.00002808
Iteration 184/1000 | Loss: 0.00002807
Iteration 185/1000 | Loss: 0.00002807
Iteration 186/1000 | Loss: 0.00002807
Iteration 187/1000 | Loss: 0.00002807
Iteration 188/1000 | Loss: 0.00002807
Iteration 189/1000 | Loss: 0.00002807
Iteration 190/1000 | Loss: 0.00002807
Iteration 191/1000 | Loss: 0.00002806
Iteration 192/1000 | Loss: 0.00002806
Iteration 193/1000 | Loss: 0.00002806
Iteration 194/1000 | Loss: 0.00002806
Iteration 195/1000 | Loss: 0.00002806
Iteration 196/1000 | Loss: 0.00002806
Iteration 197/1000 | Loss: 0.00002806
Iteration 198/1000 | Loss: 0.00002806
Iteration 199/1000 | Loss: 0.00002806
Iteration 200/1000 | Loss: 0.00002806
Iteration 201/1000 | Loss: 0.00002806
Iteration 202/1000 | Loss: 0.00002806
Iteration 203/1000 | Loss: 0.00002806
Iteration 204/1000 | Loss: 0.00002806
Iteration 205/1000 | Loss: 0.00002806
Iteration 206/1000 | Loss: 0.00002806
Iteration 207/1000 | Loss: 0.00002806
Iteration 208/1000 | Loss: 0.00002806
Iteration 209/1000 | Loss: 0.00002805
Iteration 210/1000 | Loss: 0.00002805
Iteration 211/1000 | Loss: 0.00002805
Iteration 212/1000 | Loss: 0.00002805
Iteration 213/1000 | Loss: 0.00002805
Iteration 214/1000 | Loss: 0.00002805
Iteration 215/1000 | Loss: 0.00002805
Iteration 216/1000 | Loss: 0.00002805
Iteration 217/1000 | Loss: 0.00002805
Iteration 218/1000 | Loss: 0.00002805
Iteration 219/1000 | Loss: 0.00002805
Iteration 220/1000 | Loss: 0.00002805
Iteration 221/1000 | Loss: 0.00002805
Iteration 222/1000 | Loss: 0.00002805
Iteration 223/1000 | Loss: 0.00002805
Iteration 224/1000 | Loss: 0.00002805
Iteration 225/1000 | Loss: 0.00002805
Iteration 226/1000 | Loss: 0.00002805
Iteration 227/1000 | Loss: 0.00002805
Iteration 228/1000 | Loss: 0.00002805
Iteration 229/1000 | Loss: 0.00002805
Iteration 230/1000 | Loss: 0.00002805
Iteration 231/1000 | Loss: 0.00002805
Iteration 232/1000 | Loss: 0.00002805
Iteration 233/1000 | Loss: 0.00002805
Iteration 234/1000 | Loss: 0.00002805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [2.804529322020244e-05, 2.804529322020244e-05, 2.804529322020244e-05, 2.804529322020244e-05, 2.804529322020244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.804529322020244e-05

Optimization complete. Final v2v error: 4.269447326660156 mm

Highest mean error: 5.371847629547119 mm for frame 59

Lowest mean error: 3.5311601161956787 mm for frame 130

Saving results

Total time: 53.45971202850342
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792567
Iteration 2/25 | Loss: 0.00140286
Iteration 3/25 | Loss: 0.00078624
Iteration 4/25 | Loss: 0.00072999
Iteration 5/25 | Loss: 0.00070935
Iteration 6/25 | Loss: 0.00070472
Iteration 7/25 | Loss: 0.00070481
Iteration 8/25 | Loss: 0.00070387
Iteration 9/25 | Loss: 0.00070342
Iteration 10/25 | Loss: 0.00070336
Iteration 11/25 | Loss: 0.00070336
Iteration 12/25 | Loss: 0.00070336
Iteration 13/25 | Loss: 0.00070336
Iteration 14/25 | Loss: 0.00070336
Iteration 15/25 | Loss: 0.00070335
Iteration 16/25 | Loss: 0.00070335
Iteration 17/25 | Loss: 0.00070335
Iteration 18/25 | Loss: 0.00070335
Iteration 19/25 | Loss: 0.00070335
Iteration 20/25 | Loss: 0.00070335
Iteration 21/25 | Loss: 0.00070335
Iteration 22/25 | Loss: 0.00070335
Iteration 23/25 | Loss: 0.00070335
Iteration 24/25 | Loss: 0.00070335
Iteration 25/25 | Loss: 0.00070335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.80965829
Iteration 2/25 | Loss: 0.00036237
Iteration 3/25 | Loss: 0.00036235
Iteration 4/25 | Loss: 0.00036235
Iteration 5/25 | Loss: 0.00036235
Iteration 6/25 | Loss: 0.00036235
Iteration 7/25 | Loss: 0.00036235
Iteration 8/25 | Loss: 0.00036235
Iteration 9/25 | Loss: 0.00036235
Iteration 10/25 | Loss: 0.00036235
Iteration 11/25 | Loss: 0.00036235
Iteration 12/25 | Loss: 0.00036235
Iteration 13/25 | Loss: 0.00036235
Iteration 14/25 | Loss: 0.00036235
Iteration 15/25 | Loss: 0.00036235
Iteration 16/25 | Loss: 0.00036235
Iteration 17/25 | Loss: 0.00036235
Iteration 18/25 | Loss: 0.00036235
Iteration 19/25 | Loss: 0.00036235
Iteration 20/25 | Loss: 0.00036235
Iteration 21/25 | Loss: 0.00036235
Iteration 22/25 | Loss: 0.00036235
Iteration 23/25 | Loss: 0.00036235
Iteration 24/25 | Loss: 0.00036235
Iteration 25/25 | Loss: 0.00036235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036235
Iteration 2/1000 | Loss: 0.00002563
Iteration 3/1000 | Loss: 0.00003017
Iteration 4/1000 | Loss: 0.00001686
Iteration 5/1000 | Loss: 0.00002248
Iteration 6/1000 | Loss: 0.00001551
Iteration 7/1000 | Loss: 0.00001781
Iteration 8/1000 | Loss: 0.00001487
Iteration 9/1000 | Loss: 0.00001484
Iteration 10/1000 | Loss: 0.00002006
Iteration 11/1000 | Loss: 0.00001445
Iteration 12/1000 | Loss: 0.00002009
Iteration 13/1000 | Loss: 0.00001421
Iteration 14/1000 | Loss: 0.00001416
Iteration 15/1000 | Loss: 0.00001521
Iteration 16/1000 | Loss: 0.00001407
Iteration 17/1000 | Loss: 0.00001405
Iteration 18/1000 | Loss: 0.00001404
Iteration 19/1000 | Loss: 0.00001404
Iteration 20/1000 | Loss: 0.00001404
Iteration 21/1000 | Loss: 0.00001404
Iteration 22/1000 | Loss: 0.00001404
Iteration 23/1000 | Loss: 0.00001403
Iteration 24/1000 | Loss: 0.00001403
Iteration 25/1000 | Loss: 0.00001402
Iteration 26/1000 | Loss: 0.00001401
Iteration 27/1000 | Loss: 0.00001398
Iteration 28/1000 | Loss: 0.00001398
Iteration 29/1000 | Loss: 0.00001396
Iteration 30/1000 | Loss: 0.00001395
Iteration 31/1000 | Loss: 0.00001395
Iteration 32/1000 | Loss: 0.00001394
Iteration 33/1000 | Loss: 0.00001394
Iteration 34/1000 | Loss: 0.00001394
Iteration 35/1000 | Loss: 0.00001394
Iteration 36/1000 | Loss: 0.00001394
Iteration 37/1000 | Loss: 0.00001394
Iteration 38/1000 | Loss: 0.00001394
Iteration 39/1000 | Loss: 0.00001393
Iteration 40/1000 | Loss: 0.00001393
Iteration 41/1000 | Loss: 0.00001393
Iteration 42/1000 | Loss: 0.00001393
Iteration 43/1000 | Loss: 0.00002604
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001389
Iteration 46/1000 | Loss: 0.00001389
Iteration 47/1000 | Loss: 0.00001388
Iteration 48/1000 | Loss: 0.00001388
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001388
Iteration 51/1000 | Loss: 0.00001388
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001388
Iteration 55/1000 | Loss: 0.00001387
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001387
Iteration 58/1000 | Loss: 0.00001387
Iteration 59/1000 | Loss: 0.00001387
Iteration 60/1000 | Loss: 0.00001387
Iteration 61/1000 | Loss: 0.00001387
Iteration 62/1000 | Loss: 0.00001387
Iteration 63/1000 | Loss: 0.00001387
Iteration 64/1000 | Loss: 0.00001387
Iteration 65/1000 | Loss: 0.00001386
Iteration 66/1000 | Loss: 0.00001386
Iteration 67/1000 | Loss: 0.00001386
Iteration 68/1000 | Loss: 0.00001386
Iteration 69/1000 | Loss: 0.00001386
Iteration 70/1000 | Loss: 0.00001386
Iteration 71/1000 | Loss: 0.00001386
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001386
Iteration 74/1000 | Loss: 0.00001386
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001385
Iteration 79/1000 | Loss: 0.00001384
Iteration 80/1000 | Loss: 0.00001384
Iteration 81/1000 | Loss: 0.00001384
Iteration 82/1000 | Loss: 0.00001384
Iteration 83/1000 | Loss: 0.00001384
Iteration 84/1000 | Loss: 0.00001384
Iteration 85/1000 | Loss: 0.00001384
Iteration 86/1000 | Loss: 0.00001384
Iteration 87/1000 | Loss: 0.00001384
Iteration 88/1000 | Loss: 0.00001383
Iteration 89/1000 | Loss: 0.00001383
Iteration 90/1000 | Loss: 0.00001383
Iteration 91/1000 | Loss: 0.00001383
Iteration 92/1000 | Loss: 0.00001383
Iteration 93/1000 | Loss: 0.00001383
Iteration 94/1000 | Loss: 0.00001382
Iteration 95/1000 | Loss: 0.00001382
Iteration 96/1000 | Loss: 0.00001382
Iteration 97/1000 | Loss: 0.00001382
Iteration 98/1000 | Loss: 0.00001382
Iteration 99/1000 | Loss: 0.00001381
Iteration 100/1000 | Loss: 0.00001381
Iteration 101/1000 | Loss: 0.00001381
Iteration 102/1000 | Loss: 0.00001381
Iteration 103/1000 | Loss: 0.00001725
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001380
Iteration 106/1000 | Loss: 0.00001380
Iteration 107/1000 | Loss: 0.00001380
Iteration 108/1000 | Loss: 0.00001379
Iteration 109/1000 | Loss: 0.00001379
Iteration 110/1000 | Loss: 0.00001379
Iteration 111/1000 | Loss: 0.00001378
Iteration 112/1000 | Loss: 0.00001378
Iteration 113/1000 | Loss: 0.00001378
Iteration 114/1000 | Loss: 0.00001378
Iteration 115/1000 | Loss: 0.00001378
Iteration 116/1000 | Loss: 0.00001377
Iteration 117/1000 | Loss: 0.00001377
Iteration 118/1000 | Loss: 0.00001377
Iteration 119/1000 | Loss: 0.00001377
Iteration 120/1000 | Loss: 0.00001377
Iteration 121/1000 | Loss: 0.00001377
Iteration 122/1000 | Loss: 0.00001377
Iteration 123/1000 | Loss: 0.00001377
Iteration 124/1000 | Loss: 0.00001377
Iteration 125/1000 | Loss: 0.00001377
Iteration 126/1000 | Loss: 0.00001377
Iteration 127/1000 | Loss: 0.00001377
Iteration 128/1000 | Loss: 0.00001376
Iteration 129/1000 | Loss: 0.00001376
Iteration 130/1000 | Loss: 0.00001376
Iteration 131/1000 | Loss: 0.00001376
Iteration 132/1000 | Loss: 0.00001376
Iteration 133/1000 | Loss: 0.00001376
Iteration 134/1000 | Loss: 0.00001376
Iteration 135/1000 | Loss: 0.00001376
Iteration 136/1000 | Loss: 0.00001376
Iteration 137/1000 | Loss: 0.00001376
Iteration 138/1000 | Loss: 0.00001376
Iteration 139/1000 | Loss: 0.00001376
Iteration 140/1000 | Loss: 0.00001376
Iteration 141/1000 | Loss: 0.00001376
Iteration 142/1000 | Loss: 0.00001376
Iteration 143/1000 | Loss: 0.00001376
Iteration 144/1000 | Loss: 0.00001376
Iteration 145/1000 | Loss: 0.00001376
Iteration 146/1000 | Loss: 0.00001375
Iteration 147/1000 | Loss: 0.00001375
Iteration 148/1000 | Loss: 0.00001375
Iteration 149/1000 | Loss: 0.00001375
Iteration 150/1000 | Loss: 0.00001375
Iteration 151/1000 | Loss: 0.00001375
Iteration 152/1000 | Loss: 0.00001375
Iteration 153/1000 | Loss: 0.00001375
Iteration 154/1000 | Loss: 0.00001375
Iteration 155/1000 | Loss: 0.00001375
Iteration 156/1000 | Loss: 0.00001375
Iteration 157/1000 | Loss: 0.00001374
Iteration 158/1000 | Loss: 0.00001374
Iteration 159/1000 | Loss: 0.00001374
Iteration 160/1000 | Loss: 0.00001374
Iteration 161/1000 | Loss: 0.00001373
Iteration 162/1000 | Loss: 0.00001373
Iteration 163/1000 | Loss: 0.00001373
Iteration 164/1000 | Loss: 0.00001373
Iteration 165/1000 | Loss: 0.00001373
Iteration 166/1000 | Loss: 0.00001373
Iteration 167/1000 | Loss: 0.00001373
Iteration 168/1000 | Loss: 0.00001373
Iteration 169/1000 | Loss: 0.00001373
Iteration 170/1000 | Loss: 0.00001372
Iteration 171/1000 | Loss: 0.00001372
Iteration 172/1000 | Loss: 0.00001372
Iteration 173/1000 | Loss: 0.00001372
Iteration 174/1000 | Loss: 0.00001372
Iteration 175/1000 | Loss: 0.00001372
Iteration 176/1000 | Loss: 0.00001372
Iteration 177/1000 | Loss: 0.00001372
Iteration 178/1000 | Loss: 0.00001372
Iteration 179/1000 | Loss: 0.00001372
Iteration 180/1000 | Loss: 0.00001372
Iteration 181/1000 | Loss: 0.00001372
Iteration 182/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.3721197319682688e-05, 1.3721197319682688e-05, 1.3721197319682688e-05, 1.3721197319682688e-05, 1.3721197319682688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3721197319682688e-05

Optimization complete. Final v2v error: 3.1250476837158203 mm

Highest mean error: 3.510122299194336 mm for frame 175

Lowest mean error: 2.8090384006500244 mm for frame 35

Saving results

Total time: 56.269991397857666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079978
Iteration 2/25 | Loss: 0.01079978
Iteration 3/25 | Loss: 0.01079978
Iteration 4/25 | Loss: 0.01079978
Iteration 5/25 | Loss: 0.01079977
Iteration 6/25 | Loss: 0.01079977
Iteration 7/25 | Loss: 0.01079977
Iteration 8/25 | Loss: 0.01079977
Iteration 9/25 | Loss: 0.01079977
Iteration 10/25 | Loss: 0.01079977
Iteration 11/25 | Loss: 0.01079977
Iteration 12/25 | Loss: 0.01079977
Iteration 13/25 | Loss: 0.01079977
Iteration 14/25 | Loss: 0.01079977
Iteration 15/25 | Loss: 0.01079977
Iteration 16/25 | Loss: 0.01079977
Iteration 17/25 | Loss: 0.01079977
Iteration 18/25 | Loss: 0.01079977
Iteration 19/25 | Loss: 0.01079977
Iteration 20/25 | Loss: 0.01079976
Iteration 21/25 | Loss: 0.01079976
Iteration 22/25 | Loss: 0.01079976
Iteration 23/25 | Loss: 0.01079976
Iteration 24/25 | Loss: 0.01079976
Iteration 25/25 | Loss: 0.01079976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59346187
Iteration 2/25 | Loss: 0.12105748
Iteration 3/25 | Loss: 0.12105732
Iteration 4/25 | Loss: 0.12104616
Iteration 5/25 | Loss: 0.12104616
Iteration 6/25 | Loss: 0.12104616
Iteration 7/25 | Loss: 0.12104616
Iteration 8/25 | Loss: 0.12104616
Iteration 9/25 | Loss: 0.12104616
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.12104616314172745, 0.12104616314172745, 0.12104616314172745, 0.12104616314172745, 0.12104616314172745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12104616314172745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12104616
Iteration 2/1000 | Loss: 0.00145481
Iteration 3/1000 | Loss: 0.00060376
Iteration 4/1000 | Loss: 0.00030651
Iteration 5/1000 | Loss: 0.00028288
Iteration 6/1000 | Loss: 0.00030412
Iteration 7/1000 | Loss: 0.00005126
Iteration 8/1000 | Loss: 0.00008400
Iteration 9/1000 | Loss: 0.00017876
Iteration 10/1000 | Loss: 0.00029928
Iteration 11/1000 | Loss: 0.00057599
Iteration 12/1000 | Loss: 0.00009229
Iteration 13/1000 | Loss: 0.00007548
Iteration 14/1000 | Loss: 0.00024446
Iteration 15/1000 | Loss: 0.00005375
Iteration 16/1000 | Loss: 0.00040966
Iteration 17/1000 | Loss: 0.00005397
Iteration 18/1000 | Loss: 0.00008428
Iteration 19/1000 | Loss: 0.00002250
Iteration 20/1000 | Loss: 0.00002708
Iteration 21/1000 | Loss: 0.00012989
Iteration 22/1000 | Loss: 0.00003423
Iteration 23/1000 | Loss: 0.00007751
Iteration 24/1000 | Loss: 0.00001683
Iteration 25/1000 | Loss: 0.00002387
Iteration 26/1000 | Loss: 0.00003539
Iteration 27/1000 | Loss: 0.00001494
Iteration 28/1000 | Loss: 0.00002537
Iteration 29/1000 | Loss: 0.00002673
Iteration 30/1000 | Loss: 0.00001363
Iteration 31/1000 | Loss: 0.00003100
Iteration 32/1000 | Loss: 0.00003161
Iteration 33/1000 | Loss: 0.00002268
Iteration 34/1000 | Loss: 0.00001484
Iteration 35/1000 | Loss: 0.00001234
Iteration 36/1000 | Loss: 0.00006790
Iteration 37/1000 | Loss: 0.00014162
Iteration 38/1000 | Loss: 0.00006452
Iteration 39/1000 | Loss: 0.00010570
Iteration 40/1000 | Loss: 0.00004090
Iteration 41/1000 | Loss: 0.00003655
Iteration 42/1000 | Loss: 0.00002949
Iteration 43/1000 | Loss: 0.00003326
Iteration 44/1000 | Loss: 0.00002690
Iteration 45/1000 | Loss: 0.00003372
Iteration 46/1000 | Loss: 0.00049210
Iteration 47/1000 | Loss: 0.00002250
Iteration 48/1000 | Loss: 0.00001454
Iteration 49/1000 | Loss: 0.00001507
Iteration 50/1000 | Loss: 0.00002828
Iteration 51/1000 | Loss: 0.00001170
Iteration 52/1000 | Loss: 0.00001516
Iteration 53/1000 | Loss: 0.00001631
Iteration 54/1000 | Loss: 0.00003707
Iteration 55/1000 | Loss: 0.00001259
Iteration 56/1000 | Loss: 0.00001096
Iteration 57/1000 | Loss: 0.00001094
Iteration 58/1000 | Loss: 0.00001093
Iteration 59/1000 | Loss: 0.00001092
Iteration 60/1000 | Loss: 0.00001090
Iteration 61/1000 | Loss: 0.00001089
Iteration 62/1000 | Loss: 0.00001089
Iteration 63/1000 | Loss: 0.00001088
Iteration 64/1000 | Loss: 0.00001088
Iteration 65/1000 | Loss: 0.00001088
Iteration 66/1000 | Loss: 0.00001088
Iteration 67/1000 | Loss: 0.00001088
Iteration 68/1000 | Loss: 0.00001087
Iteration 69/1000 | Loss: 0.00001086
Iteration 70/1000 | Loss: 0.00001086
Iteration 71/1000 | Loss: 0.00001086
Iteration 72/1000 | Loss: 0.00001342
Iteration 73/1000 | Loss: 0.00001342
Iteration 74/1000 | Loss: 0.00001204
Iteration 75/1000 | Loss: 0.00001081
Iteration 76/1000 | Loss: 0.00001081
Iteration 77/1000 | Loss: 0.00001080
Iteration 78/1000 | Loss: 0.00001075
Iteration 79/1000 | Loss: 0.00001075
Iteration 80/1000 | Loss: 0.00001075
Iteration 81/1000 | Loss: 0.00001075
Iteration 82/1000 | Loss: 0.00001075
Iteration 83/1000 | Loss: 0.00001075
Iteration 84/1000 | Loss: 0.00001075
Iteration 85/1000 | Loss: 0.00001075
Iteration 86/1000 | Loss: 0.00001075
Iteration 87/1000 | Loss: 0.00001075
Iteration 88/1000 | Loss: 0.00001075
Iteration 89/1000 | Loss: 0.00001075
Iteration 90/1000 | Loss: 0.00001075
Iteration 91/1000 | Loss: 0.00001075
Iteration 92/1000 | Loss: 0.00001075
Iteration 93/1000 | Loss: 0.00001075
Iteration 94/1000 | Loss: 0.00001075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.0749491593742277e-05, 1.0749491593742277e-05, 1.0749491593742277e-05, 1.0749491593742277e-05, 1.0749491593742277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0749491593742277e-05

Optimization complete. Final v2v error: 2.7175443172454834 mm

Highest mean error: 9.724498748779297 mm for frame 177

Lowest mean error: 2.2466516494750977 mm for frame 218

Saving results

Total time: 100.43253564834595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00539104
Iteration 2/25 | Loss: 0.00106767
Iteration 3/25 | Loss: 0.00081684
Iteration 4/25 | Loss: 0.00077696
Iteration 5/25 | Loss: 0.00076777
Iteration 6/25 | Loss: 0.00076591
Iteration 7/25 | Loss: 0.00076570
Iteration 8/25 | Loss: 0.00076570
Iteration 9/25 | Loss: 0.00076570
Iteration 10/25 | Loss: 0.00076570
Iteration 11/25 | Loss: 0.00076570
Iteration 12/25 | Loss: 0.00076570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007657008245587349, 0.0007657008245587349, 0.0007657008245587349, 0.0007657008245587349, 0.0007657008245587349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007657008245587349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46770012
Iteration 2/25 | Loss: 0.00034362
Iteration 3/25 | Loss: 0.00034360
Iteration 4/25 | Loss: 0.00034360
Iteration 5/25 | Loss: 0.00034360
Iteration 6/25 | Loss: 0.00034360
Iteration 7/25 | Loss: 0.00034360
Iteration 8/25 | Loss: 0.00034360
Iteration 9/25 | Loss: 0.00034360
Iteration 10/25 | Loss: 0.00034360
Iteration 11/25 | Loss: 0.00034360
Iteration 12/25 | Loss: 0.00034360
Iteration 13/25 | Loss: 0.00034360
Iteration 14/25 | Loss: 0.00034360
Iteration 15/25 | Loss: 0.00034360
Iteration 16/25 | Loss: 0.00034360
Iteration 17/25 | Loss: 0.00034360
Iteration 18/25 | Loss: 0.00034360
Iteration 19/25 | Loss: 0.00034360
Iteration 20/25 | Loss: 0.00034360
Iteration 21/25 | Loss: 0.00034360
Iteration 22/25 | Loss: 0.00034360
Iteration 23/25 | Loss: 0.00034360
Iteration 24/25 | Loss: 0.00034360
Iteration 25/25 | Loss: 0.00034360

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034360
Iteration 2/1000 | Loss: 0.00005273
Iteration 3/1000 | Loss: 0.00003571
Iteration 4/1000 | Loss: 0.00003182
Iteration 5/1000 | Loss: 0.00002930
Iteration 6/1000 | Loss: 0.00002813
Iteration 7/1000 | Loss: 0.00002684
Iteration 8/1000 | Loss: 0.00002586
Iteration 9/1000 | Loss: 0.00002550
Iteration 10/1000 | Loss: 0.00002516
Iteration 11/1000 | Loss: 0.00002486
Iteration 12/1000 | Loss: 0.00002461
Iteration 13/1000 | Loss: 0.00002459
Iteration 14/1000 | Loss: 0.00002439
Iteration 15/1000 | Loss: 0.00002435
Iteration 16/1000 | Loss: 0.00002433
Iteration 17/1000 | Loss: 0.00002432
Iteration 18/1000 | Loss: 0.00002432
Iteration 19/1000 | Loss: 0.00002432
Iteration 20/1000 | Loss: 0.00002430
Iteration 21/1000 | Loss: 0.00002430
Iteration 22/1000 | Loss: 0.00002429
Iteration 23/1000 | Loss: 0.00002429
Iteration 24/1000 | Loss: 0.00002429
Iteration 25/1000 | Loss: 0.00002429
Iteration 26/1000 | Loss: 0.00002429
Iteration 27/1000 | Loss: 0.00002429
Iteration 28/1000 | Loss: 0.00002429
Iteration 29/1000 | Loss: 0.00002429
Iteration 30/1000 | Loss: 0.00002428
Iteration 31/1000 | Loss: 0.00002428
Iteration 32/1000 | Loss: 0.00002428
Iteration 33/1000 | Loss: 0.00002428
Iteration 34/1000 | Loss: 0.00002428
Iteration 35/1000 | Loss: 0.00002428
Iteration 36/1000 | Loss: 0.00002428
Iteration 37/1000 | Loss: 0.00002428
Iteration 38/1000 | Loss: 0.00002428
Iteration 39/1000 | Loss: 0.00002427
Iteration 40/1000 | Loss: 0.00002427
Iteration 41/1000 | Loss: 0.00002427
Iteration 42/1000 | Loss: 0.00002427
Iteration 43/1000 | Loss: 0.00002427
Iteration 44/1000 | Loss: 0.00002427
Iteration 45/1000 | Loss: 0.00002427
Iteration 46/1000 | Loss: 0.00002427
Iteration 47/1000 | Loss: 0.00002427
Iteration 48/1000 | Loss: 0.00002427
Iteration 49/1000 | Loss: 0.00002427
Iteration 50/1000 | Loss: 0.00002426
Iteration 51/1000 | Loss: 0.00002426
Iteration 52/1000 | Loss: 0.00002426
Iteration 53/1000 | Loss: 0.00002426
Iteration 54/1000 | Loss: 0.00002426
Iteration 55/1000 | Loss: 0.00002425
Iteration 56/1000 | Loss: 0.00002425
Iteration 57/1000 | Loss: 0.00002425
Iteration 58/1000 | Loss: 0.00002425
Iteration 59/1000 | Loss: 0.00002425
Iteration 60/1000 | Loss: 0.00002424
Iteration 61/1000 | Loss: 0.00002424
Iteration 62/1000 | Loss: 0.00002424
Iteration 63/1000 | Loss: 0.00002424
Iteration 64/1000 | Loss: 0.00002424
Iteration 65/1000 | Loss: 0.00002424
Iteration 66/1000 | Loss: 0.00002424
Iteration 67/1000 | Loss: 0.00002424
Iteration 68/1000 | Loss: 0.00002424
Iteration 69/1000 | Loss: 0.00002424
Iteration 70/1000 | Loss: 0.00002423
Iteration 71/1000 | Loss: 0.00002423
Iteration 72/1000 | Loss: 0.00002423
Iteration 73/1000 | Loss: 0.00002422
Iteration 74/1000 | Loss: 0.00002422
Iteration 75/1000 | Loss: 0.00002421
Iteration 76/1000 | Loss: 0.00002421
Iteration 77/1000 | Loss: 0.00002421
Iteration 78/1000 | Loss: 0.00002421
Iteration 79/1000 | Loss: 0.00002421
Iteration 80/1000 | Loss: 0.00002421
Iteration 81/1000 | Loss: 0.00002421
Iteration 82/1000 | Loss: 0.00002420
Iteration 83/1000 | Loss: 0.00002420
Iteration 84/1000 | Loss: 0.00002420
Iteration 85/1000 | Loss: 0.00002420
Iteration 86/1000 | Loss: 0.00002420
Iteration 87/1000 | Loss: 0.00002420
Iteration 88/1000 | Loss: 0.00002420
Iteration 89/1000 | Loss: 0.00002420
Iteration 90/1000 | Loss: 0.00002420
Iteration 91/1000 | Loss: 0.00002420
Iteration 92/1000 | Loss: 0.00002420
Iteration 93/1000 | Loss: 0.00002420
Iteration 94/1000 | Loss: 0.00002420
Iteration 95/1000 | Loss: 0.00002419
Iteration 96/1000 | Loss: 0.00002419
Iteration 97/1000 | Loss: 0.00002419
Iteration 98/1000 | Loss: 0.00002418
Iteration 99/1000 | Loss: 0.00002418
Iteration 100/1000 | Loss: 0.00002418
Iteration 101/1000 | Loss: 0.00002418
Iteration 102/1000 | Loss: 0.00002418
Iteration 103/1000 | Loss: 0.00002418
Iteration 104/1000 | Loss: 0.00002418
Iteration 105/1000 | Loss: 0.00002417
Iteration 106/1000 | Loss: 0.00002417
Iteration 107/1000 | Loss: 0.00002417
Iteration 108/1000 | Loss: 0.00002417
Iteration 109/1000 | Loss: 0.00002417
Iteration 110/1000 | Loss: 0.00002417
Iteration 111/1000 | Loss: 0.00002416
Iteration 112/1000 | Loss: 0.00002416
Iteration 113/1000 | Loss: 0.00002416
Iteration 114/1000 | Loss: 0.00002416
Iteration 115/1000 | Loss: 0.00002416
Iteration 116/1000 | Loss: 0.00002416
Iteration 117/1000 | Loss: 0.00002415
Iteration 118/1000 | Loss: 0.00002415
Iteration 119/1000 | Loss: 0.00002415
Iteration 120/1000 | Loss: 0.00002413
Iteration 121/1000 | Loss: 0.00002413
Iteration 122/1000 | Loss: 0.00002413
Iteration 123/1000 | Loss: 0.00002413
Iteration 124/1000 | Loss: 0.00002413
Iteration 125/1000 | Loss: 0.00002413
Iteration 126/1000 | Loss: 0.00002413
Iteration 127/1000 | Loss: 0.00002413
Iteration 128/1000 | Loss: 0.00002413
Iteration 129/1000 | Loss: 0.00002413
Iteration 130/1000 | Loss: 0.00002412
Iteration 131/1000 | Loss: 0.00002412
Iteration 132/1000 | Loss: 0.00002412
Iteration 133/1000 | Loss: 0.00002412
Iteration 134/1000 | Loss: 0.00002412
Iteration 135/1000 | Loss: 0.00002412
Iteration 136/1000 | Loss: 0.00002411
Iteration 137/1000 | Loss: 0.00002411
Iteration 138/1000 | Loss: 0.00002411
Iteration 139/1000 | Loss: 0.00002411
Iteration 140/1000 | Loss: 0.00002411
Iteration 141/1000 | Loss: 0.00002411
Iteration 142/1000 | Loss: 0.00002411
Iteration 143/1000 | Loss: 0.00002411
Iteration 144/1000 | Loss: 0.00002410
Iteration 145/1000 | Loss: 0.00002410
Iteration 146/1000 | Loss: 0.00002410
Iteration 147/1000 | Loss: 0.00002410
Iteration 148/1000 | Loss: 0.00002410
Iteration 149/1000 | Loss: 0.00002410
Iteration 150/1000 | Loss: 0.00002410
Iteration 151/1000 | Loss: 0.00002410
Iteration 152/1000 | Loss: 0.00002409
Iteration 153/1000 | Loss: 0.00002409
Iteration 154/1000 | Loss: 0.00002409
Iteration 155/1000 | Loss: 0.00002409
Iteration 156/1000 | Loss: 0.00002409
Iteration 157/1000 | Loss: 0.00002409
Iteration 158/1000 | Loss: 0.00002409
Iteration 159/1000 | Loss: 0.00002409
Iteration 160/1000 | Loss: 0.00002409
Iteration 161/1000 | Loss: 0.00002409
Iteration 162/1000 | Loss: 0.00002409
Iteration 163/1000 | Loss: 0.00002409
Iteration 164/1000 | Loss: 0.00002409
Iteration 165/1000 | Loss: 0.00002409
Iteration 166/1000 | Loss: 0.00002409
Iteration 167/1000 | Loss: 0.00002409
Iteration 168/1000 | Loss: 0.00002409
Iteration 169/1000 | Loss: 0.00002409
Iteration 170/1000 | Loss: 0.00002409
Iteration 171/1000 | Loss: 0.00002409
Iteration 172/1000 | Loss: 0.00002409
Iteration 173/1000 | Loss: 0.00002409
Iteration 174/1000 | Loss: 0.00002409
Iteration 175/1000 | Loss: 0.00002409
Iteration 176/1000 | Loss: 0.00002409
Iteration 177/1000 | Loss: 0.00002409
Iteration 178/1000 | Loss: 0.00002409
Iteration 179/1000 | Loss: 0.00002409
Iteration 180/1000 | Loss: 0.00002409
Iteration 181/1000 | Loss: 0.00002409
Iteration 182/1000 | Loss: 0.00002409
Iteration 183/1000 | Loss: 0.00002409
Iteration 184/1000 | Loss: 0.00002409
Iteration 185/1000 | Loss: 0.00002409
Iteration 186/1000 | Loss: 0.00002409
Iteration 187/1000 | Loss: 0.00002409
Iteration 188/1000 | Loss: 0.00002409
Iteration 189/1000 | Loss: 0.00002409
Iteration 190/1000 | Loss: 0.00002409
Iteration 191/1000 | Loss: 0.00002409
Iteration 192/1000 | Loss: 0.00002409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.4086180928861722e-05, 2.4086180928861722e-05, 2.4086180928861722e-05, 2.4086180928861722e-05, 2.4086180928861722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4086180928861722e-05

Optimization complete. Final v2v error: 4.093122482299805 mm

Highest mean error: 4.347877502441406 mm for frame 49

Lowest mean error: 3.8091893196105957 mm for frame 130

Saving results

Total time: 39.724185943603516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544225
Iteration 2/25 | Loss: 0.00100714
Iteration 3/25 | Loss: 0.00076037
Iteration 4/25 | Loss: 0.00072038
Iteration 5/25 | Loss: 0.00071097
Iteration 6/25 | Loss: 0.00070843
Iteration 7/25 | Loss: 0.00070804
Iteration 8/25 | Loss: 0.00070804
Iteration 9/25 | Loss: 0.00070804
Iteration 10/25 | Loss: 0.00070804
Iteration 11/25 | Loss: 0.00070804
Iteration 12/25 | Loss: 0.00070804
Iteration 13/25 | Loss: 0.00070804
Iteration 14/25 | Loss: 0.00070804
Iteration 15/25 | Loss: 0.00070804
Iteration 16/25 | Loss: 0.00070804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007080361247062683, 0.0007080361247062683, 0.0007080361247062683, 0.0007080361247062683, 0.0007080361247062683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007080361247062683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.55127478
Iteration 2/25 | Loss: 0.00031143
Iteration 3/25 | Loss: 0.00031142
Iteration 4/25 | Loss: 0.00031142
Iteration 5/25 | Loss: 0.00031142
Iteration 6/25 | Loss: 0.00031142
Iteration 7/25 | Loss: 0.00031142
Iteration 8/25 | Loss: 0.00031142
Iteration 9/25 | Loss: 0.00031141
Iteration 10/25 | Loss: 0.00031141
Iteration 11/25 | Loss: 0.00031141
Iteration 12/25 | Loss: 0.00031141
Iteration 13/25 | Loss: 0.00031141
Iteration 14/25 | Loss: 0.00031141
Iteration 15/25 | Loss: 0.00031141
Iteration 16/25 | Loss: 0.00031141
Iteration 17/25 | Loss: 0.00031141
Iteration 18/25 | Loss: 0.00031141
Iteration 19/25 | Loss: 0.00031141
Iteration 20/25 | Loss: 0.00031141
Iteration 21/25 | Loss: 0.00031141
Iteration 22/25 | Loss: 0.00031141
Iteration 23/25 | Loss: 0.00031141
Iteration 24/25 | Loss: 0.00031141
Iteration 25/25 | Loss: 0.00031141

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031141
Iteration 2/1000 | Loss: 0.00003161
Iteration 3/1000 | Loss: 0.00002226
Iteration 4/1000 | Loss: 0.00001968
Iteration 5/1000 | Loss: 0.00001886
Iteration 6/1000 | Loss: 0.00001807
Iteration 7/1000 | Loss: 0.00001760
Iteration 8/1000 | Loss: 0.00001731
Iteration 9/1000 | Loss: 0.00001724
Iteration 10/1000 | Loss: 0.00001706
Iteration 11/1000 | Loss: 0.00001698
Iteration 12/1000 | Loss: 0.00001682
Iteration 13/1000 | Loss: 0.00001677
Iteration 14/1000 | Loss: 0.00001670
Iteration 15/1000 | Loss: 0.00001661
Iteration 16/1000 | Loss: 0.00001654
Iteration 17/1000 | Loss: 0.00001649
Iteration 18/1000 | Loss: 0.00001645
Iteration 19/1000 | Loss: 0.00001645
Iteration 20/1000 | Loss: 0.00001644
Iteration 21/1000 | Loss: 0.00001640
Iteration 22/1000 | Loss: 0.00001640
Iteration 23/1000 | Loss: 0.00001639
Iteration 24/1000 | Loss: 0.00001638
Iteration 25/1000 | Loss: 0.00001638
Iteration 26/1000 | Loss: 0.00001637
Iteration 27/1000 | Loss: 0.00001637
Iteration 28/1000 | Loss: 0.00001636
Iteration 29/1000 | Loss: 0.00001636
Iteration 30/1000 | Loss: 0.00001636
Iteration 31/1000 | Loss: 0.00001635
Iteration 32/1000 | Loss: 0.00001635
Iteration 33/1000 | Loss: 0.00001634
Iteration 34/1000 | Loss: 0.00001633
Iteration 35/1000 | Loss: 0.00001633
Iteration 36/1000 | Loss: 0.00001632
Iteration 37/1000 | Loss: 0.00001632
Iteration 38/1000 | Loss: 0.00001632
Iteration 39/1000 | Loss: 0.00001632
Iteration 40/1000 | Loss: 0.00001632
Iteration 41/1000 | Loss: 0.00001631
Iteration 42/1000 | Loss: 0.00001631
Iteration 43/1000 | Loss: 0.00001630
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001629
Iteration 46/1000 | Loss: 0.00001629
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001628
Iteration 50/1000 | Loss: 0.00001628
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001627
Iteration 53/1000 | Loss: 0.00001626
Iteration 54/1000 | Loss: 0.00001626
Iteration 55/1000 | Loss: 0.00001626
Iteration 56/1000 | Loss: 0.00001626
Iteration 57/1000 | Loss: 0.00001626
Iteration 58/1000 | Loss: 0.00001626
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001625
Iteration 61/1000 | Loss: 0.00001625
Iteration 62/1000 | Loss: 0.00001625
Iteration 63/1000 | Loss: 0.00001625
Iteration 64/1000 | Loss: 0.00001625
Iteration 65/1000 | Loss: 0.00001625
Iteration 66/1000 | Loss: 0.00001625
Iteration 67/1000 | Loss: 0.00001625
Iteration 68/1000 | Loss: 0.00001625
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001622
Iteration 73/1000 | Loss: 0.00001622
Iteration 74/1000 | Loss: 0.00001622
Iteration 75/1000 | Loss: 0.00001622
Iteration 76/1000 | Loss: 0.00001622
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001622
Iteration 79/1000 | Loss: 0.00001621
Iteration 80/1000 | Loss: 0.00001621
Iteration 81/1000 | Loss: 0.00001621
Iteration 82/1000 | Loss: 0.00001621
Iteration 83/1000 | Loss: 0.00001621
Iteration 84/1000 | Loss: 0.00001621
Iteration 85/1000 | Loss: 0.00001621
Iteration 86/1000 | Loss: 0.00001621
Iteration 87/1000 | Loss: 0.00001621
Iteration 88/1000 | Loss: 0.00001621
Iteration 89/1000 | Loss: 0.00001620
Iteration 90/1000 | Loss: 0.00001620
Iteration 91/1000 | Loss: 0.00001620
Iteration 92/1000 | Loss: 0.00001620
Iteration 93/1000 | Loss: 0.00001620
Iteration 94/1000 | Loss: 0.00001620
Iteration 95/1000 | Loss: 0.00001619
Iteration 96/1000 | Loss: 0.00001619
Iteration 97/1000 | Loss: 0.00001619
Iteration 98/1000 | Loss: 0.00001619
Iteration 99/1000 | Loss: 0.00001619
Iteration 100/1000 | Loss: 0.00001619
Iteration 101/1000 | Loss: 0.00001619
Iteration 102/1000 | Loss: 0.00001619
Iteration 103/1000 | Loss: 0.00001619
Iteration 104/1000 | Loss: 0.00001619
Iteration 105/1000 | Loss: 0.00001619
Iteration 106/1000 | Loss: 0.00001619
Iteration 107/1000 | Loss: 0.00001619
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001619
Iteration 111/1000 | Loss: 0.00001618
Iteration 112/1000 | Loss: 0.00001618
Iteration 113/1000 | Loss: 0.00001618
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001618
Iteration 117/1000 | Loss: 0.00001618
Iteration 118/1000 | Loss: 0.00001618
Iteration 119/1000 | Loss: 0.00001618
Iteration 120/1000 | Loss: 0.00001618
Iteration 121/1000 | Loss: 0.00001617
Iteration 122/1000 | Loss: 0.00001617
Iteration 123/1000 | Loss: 0.00001617
Iteration 124/1000 | Loss: 0.00001617
Iteration 125/1000 | Loss: 0.00001617
Iteration 126/1000 | Loss: 0.00001617
Iteration 127/1000 | Loss: 0.00001617
Iteration 128/1000 | Loss: 0.00001617
Iteration 129/1000 | Loss: 0.00001617
Iteration 130/1000 | Loss: 0.00001617
Iteration 131/1000 | Loss: 0.00001616
Iteration 132/1000 | Loss: 0.00001616
Iteration 133/1000 | Loss: 0.00001616
Iteration 134/1000 | Loss: 0.00001616
Iteration 135/1000 | Loss: 0.00001616
Iteration 136/1000 | Loss: 0.00001616
Iteration 137/1000 | Loss: 0.00001616
Iteration 138/1000 | Loss: 0.00001616
Iteration 139/1000 | Loss: 0.00001616
Iteration 140/1000 | Loss: 0.00001616
Iteration 141/1000 | Loss: 0.00001616
Iteration 142/1000 | Loss: 0.00001615
Iteration 143/1000 | Loss: 0.00001615
Iteration 144/1000 | Loss: 0.00001615
Iteration 145/1000 | Loss: 0.00001615
Iteration 146/1000 | Loss: 0.00001615
Iteration 147/1000 | Loss: 0.00001615
Iteration 148/1000 | Loss: 0.00001615
Iteration 149/1000 | Loss: 0.00001615
Iteration 150/1000 | Loss: 0.00001615
Iteration 151/1000 | Loss: 0.00001615
Iteration 152/1000 | Loss: 0.00001615
Iteration 153/1000 | Loss: 0.00001615
Iteration 154/1000 | Loss: 0.00001615
Iteration 155/1000 | Loss: 0.00001615
Iteration 156/1000 | Loss: 0.00001615
Iteration 157/1000 | Loss: 0.00001615
Iteration 158/1000 | Loss: 0.00001615
Iteration 159/1000 | Loss: 0.00001615
Iteration 160/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.6152744137798436e-05, 1.6152744137798436e-05, 1.6152744137798436e-05, 1.6152744137798436e-05, 1.6152744137798436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6152744137798436e-05

Optimization complete. Final v2v error: 3.407726287841797 mm

Highest mean error: 3.729583740234375 mm for frame 0

Lowest mean error: 2.9612209796905518 mm for frame 93

Saving results

Total time: 39.55412149429321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00606768
Iteration 2/25 | Loss: 0.00080226
Iteration 3/25 | Loss: 0.00068209
Iteration 4/25 | Loss: 0.00065545
Iteration 5/25 | Loss: 0.00064629
Iteration 6/25 | Loss: 0.00064426
Iteration 7/25 | Loss: 0.00064354
Iteration 8/25 | Loss: 0.00064354
Iteration 9/25 | Loss: 0.00064354
Iteration 10/25 | Loss: 0.00064354
Iteration 11/25 | Loss: 0.00064354
Iteration 12/25 | Loss: 0.00064354
Iteration 13/25 | Loss: 0.00064354
Iteration 14/25 | Loss: 0.00064354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000643538951408118, 0.000643538951408118, 0.000643538951408118, 0.000643538951408118, 0.000643538951408118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000643538951408118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.02599573
Iteration 2/25 | Loss: 0.00033920
Iteration 3/25 | Loss: 0.00033920
Iteration 4/25 | Loss: 0.00033919
Iteration 5/25 | Loss: 0.00033919
Iteration 6/25 | Loss: 0.00033919
Iteration 7/25 | Loss: 0.00033919
Iteration 8/25 | Loss: 0.00033919
Iteration 9/25 | Loss: 0.00033919
Iteration 10/25 | Loss: 0.00033919
Iteration 11/25 | Loss: 0.00033919
Iteration 12/25 | Loss: 0.00033919
Iteration 13/25 | Loss: 0.00033919
Iteration 14/25 | Loss: 0.00033919
Iteration 15/25 | Loss: 0.00033919
Iteration 16/25 | Loss: 0.00033919
Iteration 17/25 | Loss: 0.00033919
Iteration 18/25 | Loss: 0.00033919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003391928621567786, 0.0003391928621567786, 0.0003391928621567786, 0.0003391928621567786, 0.0003391928621567786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003391928621567786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033919
Iteration 2/1000 | Loss: 0.00002042
Iteration 3/1000 | Loss: 0.00001497
Iteration 4/1000 | Loss: 0.00001389
Iteration 5/1000 | Loss: 0.00001307
Iteration 6/1000 | Loss: 0.00001271
Iteration 7/1000 | Loss: 0.00001243
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001217
Iteration 11/1000 | Loss: 0.00001212
Iteration 12/1000 | Loss: 0.00001205
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001196
Iteration 15/1000 | Loss: 0.00001196
Iteration 16/1000 | Loss: 0.00001191
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001190
Iteration 19/1000 | Loss: 0.00001190
Iteration 20/1000 | Loss: 0.00001189
Iteration 21/1000 | Loss: 0.00001189
Iteration 22/1000 | Loss: 0.00001188
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001181
Iteration 25/1000 | Loss: 0.00001178
Iteration 26/1000 | Loss: 0.00001178
Iteration 27/1000 | Loss: 0.00001177
Iteration 28/1000 | Loss: 0.00001177
Iteration 29/1000 | Loss: 0.00001176
Iteration 30/1000 | Loss: 0.00001176
Iteration 31/1000 | Loss: 0.00001175
Iteration 32/1000 | Loss: 0.00001175
Iteration 33/1000 | Loss: 0.00001174
Iteration 34/1000 | Loss: 0.00001174
Iteration 35/1000 | Loss: 0.00001173
Iteration 36/1000 | Loss: 0.00001173
Iteration 37/1000 | Loss: 0.00001173
Iteration 38/1000 | Loss: 0.00001173
Iteration 39/1000 | Loss: 0.00001173
Iteration 40/1000 | Loss: 0.00001173
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001173
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001171
Iteration 46/1000 | Loss: 0.00001170
Iteration 47/1000 | Loss: 0.00001170
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001169
Iteration 50/1000 | Loss: 0.00001169
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001169
Iteration 53/1000 | Loss: 0.00001166
Iteration 54/1000 | Loss: 0.00001164
Iteration 55/1000 | Loss: 0.00001164
Iteration 56/1000 | Loss: 0.00001164
Iteration 57/1000 | Loss: 0.00001164
Iteration 58/1000 | Loss: 0.00001164
Iteration 59/1000 | Loss: 0.00001164
Iteration 60/1000 | Loss: 0.00001164
Iteration 61/1000 | Loss: 0.00001164
Iteration 62/1000 | Loss: 0.00001164
Iteration 63/1000 | Loss: 0.00001164
Iteration 64/1000 | Loss: 0.00001164
Iteration 65/1000 | Loss: 0.00001162
Iteration 66/1000 | Loss: 0.00001162
Iteration 67/1000 | Loss: 0.00001160
Iteration 68/1000 | Loss: 0.00001160
Iteration 69/1000 | Loss: 0.00001159
Iteration 70/1000 | Loss: 0.00001158
Iteration 71/1000 | Loss: 0.00001158
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001157
Iteration 74/1000 | Loss: 0.00001157
Iteration 75/1000 | Loss: 0.00001156
Iteration 76/1000 | Loss: 0.00001156
Iteration 77/1000 | Loss: 0.00001156
Iteration 78/1000 | Loss: 0.00001156
Iteration 79/1000 | Loss: 0.00001156
Iteration 80/1000 | Loss: 0.00001156
Iteration 81/1000 | Loss: 0.00001156
Iteration 82/1000 | Loss: 0.00001156
Iteration 83/1000 | Loss: 0.00001155
Iteration 84/1000 | Loss: 0.00001154
Iteration 85/1000 | Loss: 0.00001154
Iteration 86/1000 | Loss: 0.00001154
Iteration 87/1000 | Loss: 0.00001154
Iteration 88/1000 | Loss: 0.00001154
Iteration 89/1000 | Loss: 0.00001154
Iteration 90/1000 | Loss: 0.00001154
Iteration 91/1000 | Loss: 0.00001154
Iteration 92/1000 | Loss: 0.00001154
Iteration 93/1000 | Loss: 0.00001154
Iteration 94/1000 | Loss: 0.00001153
Iteration 95/1000 | Loss: 0.00001153
Iteration 96/1000 | Loss: 0.00001153
Iteration 97/1000 | Loss: 0.00001153
Iteration 98/1000 | Loss: 0.00001153
Iteration 99/1000 | Loss: 0.00001153
Iteration 100/1000 | Loss: 0.00001153
Iteration 101/1000 | Loss: 0.00001153
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001153
Iteration 104/1000 | Loss: 0.00001153
Iteration 105/1000 | Loss: 0.00001153
Iteration 106/1000 | Loss: 0.00001153
Iteration 107/1000 | Loss: 0.00001152
Iteration 108/1000 | Loss: 0.00001152
Iteration 109/1000 | Loss: 0.00001152
Iteration 110/1000 | Loss: 0.00001152
Iteration 111/1000 | Loss: 0.00001152
Iteration 112/1000 | Loss: 0.00001152
Iteration 113/1000 | Loss: 0.00001152
Iteration 114/1000 | Loss: 0.00001152
Iteration 115/1000 | Loss: 0.00001152
Iteration 116/1000 | Loss: 0.00001152
Iteration 117/1000 | Loss: 0.00001152
Iteration 118/1000 | Loss: 0.00001152
Iteration 119/1000 | Loss: 0.00001152
Iteration 120/1000 | Loss: 0.00001151
Iteration 121/1000 | Loss: 0.00001151
Iteration 122/1000 | Loss: 0.00001151
Iteration 123/1000 | Loss: 0.00001151
Iteration 124/1000 | Loss: 0.00001151
Iteration 125/1000 | Loss: 0.00001151
Iteration 126/1000 | Loss: 0.00001151
Iteration 127/1000 | Loss: 0.00001151
Iteration 128/1000 | Loss: 0.00001151
Iteration 129/1000 | Loss: 0.00001151
Iteration 130/1000 | Loss: 0.00001151
Iteration 131/1000 | Loss: 0.00001151
Iteration 132/1000 | Loss: 0.00001151
Iteration 133/1000 | Loss: 0.00001151
Iteration 134/1000 | Loss: 0.00001151
Iteration 135/1000 | Loss: 0.00001151
Iteration 136/1000 | Loss: 0.00001151
Iteration 137/1000 | Loss: 0.00001151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.1510725016705692e-05, 1.1510725016705692e-05, 1.1510725016705692e-05, 1.1510725016705692e-05, 1.1510725016705692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1510725016705692e-05

Optimization complete. Final v2v error: 2.9068169593811035 mm

Highest mean error: 3.186067819595337 mm for frame 141

Lowest mean error: 2.7159547805786133 mm for frame 165

Saving results

Total time: 36.318166971206665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819319
Iteration 2/25 | Loss: 0.00126875
Iteration 3/25 | Loss: 0.00084182
Iteration 4/25 | Loss: 0.00072579
Iteration 5/25 | Loss: 0.00069955
Iteration 6/25 | Loss: 0.00069339
Iteration 7/25 | Loss: 0.00069260
Iteration 8/25 | Loss: 0.00069260
Iteration 9/25 | Loss: 0.00069260
Iteration 10/25 | Loss: 0.00069260
Iteration 11/25 | Loss: 0.00069260
Iteration 12/25 | Loss: 0.00069260
Iteration 13/25 | Loss: 0.00069260
Iteration 14/25 | Loss: 0.00069260
Iteration 15/25 | Loss: 0.00069260
Iteration 16/25 | Loss: 0.00069260
Iteration 17/25 | Loss: 0.00069260
Iteration 18/25 | Loss: 0.00069260
Iteration 19/25 | Loss: 0.00069260
Iteration 20/25 | Loss: 0.00069260
Iteration 21/25 | Loss: 0.00069260
Iteration 22/25 | Loss: 0.00069260
Iteration 23/25 | Loss: 0.00069260
Iteration 24/25 | Loss: 0.00069260
Iteration 25/25 | Loss: 0.00069260

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48654413
Iteration 2/25 | Loss: 0.00035416
Iteration 3/25 | Loss: 0.00035416
Iteration 4/25 | Loss: 0.00035416
Iteration 5/25 | Loss: 0.00035416
Iteration 6/25 | Loss: 0.00035416
Iteration 7/25 | Loss: 0.00035416
Iteration 8/25 | Loss: 0.00035416
Iteration 9/25 | Loss: 0.00035416
Iteration 10/25 | Loss: 0.00035416
Iteration 11/25 | Loss: 0.00035416
Iteration 12/25 | Loss: 0.00035416
Iteration 13/25 | Loss: 0.00035416
Iteration 14/25 | Loss: 0.00035416
Iteration 15/25 | Loss: 0.00035416
Iteration 16/25 | Loss: 0.00035416
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0003541559854056686, 0.0003541559854056686, 0.0003541559854056686, 0.0003541559854056686, 0.0003541559854056686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003541559854056686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035416
Iteration 2/1000 | Loss: 0.00002967
Iteration 3/1000 | Loss: 0.00002082
Iteration 4/1000 | Loss: 0.00001823
Iteration 5/1000 | Loss: 0.00001638
Iteration 6/1000 | Loss: 0.00001557
Iteration 7/1000 | Loss: 0.00001497
Iteration 8/1000 | Loss: 0.00001452
Iteration 9/1000 | Loss: 0.00001421
Iteration 10/1000 | Loss: 0.00001397
Iteration 11/1000 | Loss: 0.00001393
Iteration 12/1000 | Loss: 0.00001390
Iteration 13/1000 | Loss: 0.00001367
Iteration 14/1000 | Loss: 0.00001354
Iteration 15/1000 | Loss: 0.00001348
Iteration 16/1000 | Loss: 0.00001345
Iteration 17/1000 | Loss: 0.00001345
Iteration 18/1000 | Loss: 0.00001341
Iteration 19/1000 | Loss: 0.00001340
Iteration 20/1000 | Loss: 0.00001340
Iteration 21/1000 | Loss: 0.00001339
Iteration 22/1000 | Loss: 0.00001338
Iteration 23/1000 | Loss: 0.00001338
Iteration 24/1000 | Loss: 0.00001337
Iteration 25/1000 | Loss: 0.00001337
Iteration 26/1000 | Loss: 0.00001337
Iteration 27/1000 | Loss: 0.00001337
Iteration 28/1000 | Loss: 0.00001337
Iteration 29/1000 | Loss: 0.00001337
Iteration 30/1000 | Loss: 0.00001337
Iteration 31/1000 | Loss: 0.00001336
Iteration 32/1000 | Loss: 0.00001336
Iteration 33/1000 | Loss: 0.00001335
Iteration 34/1000 | Loss: 0.00001335
Iteration 35/1000 | Loss: 0.00001334
Iteration 36/1000 | Loss: 0.00001334
Iteration 37/1000 | Loss: 0.00001334
Iteration 38/1000 | Loss: 0.00001333
Iteration 39/1000 | Loss: 0.00001333
Iteration 40/1000 | Loss: 0.00001333
Iteration 41/1000 | Loss: 0.00001333
Iteration 42/1000 | Loss: 0.00001333
Iteration 43/1000 | Loss: 0.00001333
Iteration 44/1000 | Loss: 0.00001333
Iteration 45/1000 | Loss: 0.00001332
Iteration 46/1000 | Loss: 0.00001332
Iteration 47/1000 | Loss: 0.00001332
Iteration 48/1000 | Loss: 0.00001332
Iteration 49/1000 | Loss: 0.00001331
Iteration 50/1000 | Loss: 0.00001331
Iteration 51/1000 | Loss: 0.00001331
Iteration 52/1000 | Loss: 0.00001331
Iteration 53/1000 | Loss: 0.00001331
Iteration 54/1000 | Loss: 0.00001331
Iteration 55/1000 | Loss: 0.00001330
Iteration 56/1000 | Loss: 0.00001330
Iteration 57/1000 | Loss: 0.00001330
Iteration 58/1000 | Loss: 0.00001330
Iteration 59/1000 | Loss: 0.00001330
Iteration 60/1000 | Loss: 0.00001329
Iteration 61/1000 | Loss: 0.00001329
Iteration 62/1000 | Loss: 0.00001329
Iteration 63/1000 | Loss: 0.00001329
Iteration 64/1000 | Loss: 0.00001328
Iteration 65/1000 | Loss: 0.00001328
Iteration 66/1000 | Loss: 0.00001328
Iteration 67/1000 | Loss: 0.00001327
Iteration 68/1000 | Loss: 0.00001327
Iteration 69/1000 | Loss: 0.00001326
Iteration 70/1000 | Loss: 0.00001326
Iteration 71/1000 | Loss: 0.00001326
Iteration 72/1000 | Loss: 0.00001326
Iteration 73/1000 | Loss: 0.00001326
Iteration 74/1000 | Loss: 0.00001325
Iteration 75/1000 | Loss: 0.00001325
Iteration 76/1000 | Loss: 0.00001325
Iteration 77/1000 | Loss: 0.00001325
Iteration 78/1000 | Loss: 0.00001325
Iteration 79/1000 | Loss: 0.00001324
Iteration 80/1000 | Loss: 0.00001324
Iteration 81/1000 | Loss: 0.00001324
Iteration 82/1000 | Loss: 0.00001324
Iteration 83/1000 | Loss: 0.00001324
Iteration 84/1000 | Loss: 0.00001324
Iteration 85/1000 | Loss: 0.00001324
Iteration 86/1000 | Loss: 0.00001324
Iteration 87/1000 | Loss: 0.00001324
Iteration 88/1000 | Loss: 0.00001323
Iteration 89/1000 | Loss: 0.00001323
Iteration 90/1000 | Loss: 0.00001323
Iteration 91/1000 | Loss: 0.00001323
Iteration 92/1000 | Loss: 0.00001322
Iteration 93/1000 | Loss: 0.00001322
Iteration 94/1000 | Loss: 0.00001321
Iteration 95/1000 | Loss: 0.00001321
Iteration 96/1000 | Loss: 0.00001321
Iteration 97/1000 | Loss: 0.00001321
Iteration 98/1000 | Loss: 0.00001321
Iteration 99/1000 | Loss: 0.00001321
Iteration 100/1000 | Loss: 0.00001321
Iteration 101/1000 | Loss: 0.00001321
Iteration 102/1000 | Loss: 0.00001320
Iteration 103/1000 | Loss: 0.00001320
Iteration 104/1000 | Loss: 0.00001320
Iteration 105/1000 | Loss: 0.00001320
Iteration 106/1000 | Loss: 0.00001319
Iteration 107/1000 | Loss: 0.00001319
Iteration 108/1000 | Loss: 0.00001319
Iteration 109/1000 | Loss: 0.00001319
Iteration 110/1000 | Loss: 0.00001318
Iteration 111/1000 | Loss: 0.00001318
Iteration 112/1000 | Loss: 0.00001318
Iteration 113/1000 | Loss: 0.00001318
Iteration 114/1000 | Loss: 0.00001318
Iteration 115/1000 | Loss: 0.00001317
Iteration 116/1000 | Loss: 0.00001317
Iteration 117/1000 | Loss: 0.00001317
Iteration 118/1000 | Loss: 0.00001317
Iteration 119/1000 | Loss: 0.00001317
Iteration 120/1000 | Loss: 0.00001317
Iteration 121/1000 | Loss: 0.00001317
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001316
Iteration 124/1000 | Loss: 0.00001316
Iteration 125/1000 | Loss: 0.00001316
Iteration 126/1000 | Loss: 0.00001316
Iteration 127/1000 | Loss: 0.00001316
Iteration 128/1000 | Loss: 0.00001316
Iteration 129/1000 | Loss: 0.00001316
Iteration 130/1000 | Loss: 0.00001316
Iteration 131/1000 | Loss: 0.00001316
Iteration 132/1000 | Loss: 0.00001316
Iteration 133/1000 | Loss: 0.00001316
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001316
Iteration 136/1000 | Loss: 0.00001316
Iteration 137/1000 | Loss: 0.00001316
Iteration 138/1000 | Loss: 0.00001315
Iteration 139/1000 | Loss: 0.00001315
Iteration 140/1000 | Loss: 0.00001315
Iteration 141/1000 | Loss: 0.00001315
Iteration 142/1000 | Loss: 0.00001315
Iteration 143/1000 | Loss: 0.00001315
Iteration 144/1000 | Loss: 0.00001315
Iteration 145/1000 | Loss: 0.00001315
Iteration 146/1000 | Loss: 0.00001315
Iteration 147/1000 | Loss: 0.00001315
Iteration 148/1000 | Loss: 0.00001314
Iteration 149/1000 | Loss: 0.00001314
Iteration 150/1000 | Loss: 0.00001314
Iteration 151/1000 | Loss: 0.00001314
Iteration 152/1000 | Loss: 0.00001314
Iteration 153/1000 | Loss: 0.00001314
Iteration 154/1000 | Loss: 0.00001314
Iteration 155/1000 | Loss: 0.00001314
Iteration 156/1000 | Loss: 0.00001314
Iteration 157/1000 | Loss: 0.00001314
Iteration 158/1000 | Loss: 0.00001314
Iteration 159/1000 | Loss: 0.00001314
Iteration 160/1000 | Loss: 0.00001314
Iteration 161/1000 | Loss: 0.00001314
Iteration 162/1000 | Loss: 0.00001314
Iteration 163/1000 | Loss: 0.00001314
Iteration 164/1000 | Loss: 0.00001314
Iteration 165/1000 | Loss: 0.00001314
Iteration 166/1000 | Loss: 0.00001314
Iteration 167/1000 | Loss: 0.00001314
Iteration 168/1000 | Loss: 0.00001314
Iteration 169/1000 | Loss: 0.00001314
Iteration 170/1000 | Loss: 0.00001314
Iteration 171/1000 | Loss: 0.00001314
Iteration 172/1000 | Loss: 0.00001314
Iteration 173/1000 | Loss: 0.00001314
Iteration 174/1000 | Loss: 0.00001314
Iteration 175/1000 | Loss: 0.00001314
Iteration 176/1000 | Loss: 0.00001314
Iteration 177/1000 | Loss: 0.00001314
Iteration 178/1000 | Loss: 0.00001314
Iteration 179/1000 | Loss: 0.00001314
Iteration 180/1000 | Loss: 0.00001314
Iteration 181/1000 | Loss: 0.00001314
Iteration 182/1000 | Loss: 0.00001314
Iteration 183/1000 | Loss: 0.00001314
Iteration 184/1000 | Loss: 0.00001314
Iteration 185/1000 | Loss: 0.00001314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.3139451766619459e-05, 1.3139451766619459e-05, 1.3139451766619459e-05, 1.3139451766619459e-05, 1.3139451766619459e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3139451766619459e-05

Optimization complete. Final v2v error: 3.0741591453552246 mm

Highest mean error: 3.4999818801879883 mm for frame 195

Lowest mean error: 2.7889046669006348 mm for frame 123

Saving results

Total time: 46.75692844390869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083571
Iteration 2/25 | Loss: 0.01083571
Iteration 3/25 | Loss: 0.01083571
Iteration 4/25 | Loss: 0.01083570
Iteration 5/25 | Loss: 0.01083570
Iteration 6/25 | Loss: 0.01083570
Iteration 7/25 | Loss: 0.01083570
Iteration 8/25 | Loss: 0.01083570
Iteration 9/25 | Loss: 0.01083570
Iteration 10/25 | Loss: 0.01083570
Iteration 11/25 | Loss: 0.01083570
Iteration 12/25 | Loss: 0.01083569
Iteration 13/25 | Loss: 0.01083569
Iteration 14/25 | Loss: 0.01083569
Iteration 15/25 | Loss: 0.01083569
Iteration 16/25 | Loss: 0.01083569
Iteration 17/25 | Loss: 0.01083569
Iteration 18/25 | Loss: 0.01083569
Iteration 19/25 | Loss: 0.01083569
Iteration 20/25 | Loss: 0.01083568
Iteration 21/25 | Loss: 0.01083568
Iteration 22/25 | Loss: 0.01083568
Iteration 23/25 | Loss: 0.01083568
Iteration 24/25 | Loss: 0.01083568
Iteration 25/25 | Loss: 0.01083568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68125403
Iteration 2/25 | Loss: 0.09271766
Iteration 3/25 | Loss: 0.09260760
Iteration 4/25 | Loss: 0.09256250
Iteration 5/25 | Loss: 0.09256250
Iteration 6/25 | Loss: 0.09256250
Iteration 7/25 | Loss: 0.09256248
Iteration 8/25 | Loss: 0.09256248
Iteration 9/25 | Loss: 0.09256248
Iteration 10/25 | Loss: 0.09256248
Iteration 11/25 | Loss: 0.09256247
Iteration 12/25 | Loss: 0.09256247
Iteration 13/25 | Loss: 0.09256247
Iteration 14/25 | Loss: 0.09256247
Iteration 15/25 | Loss: 0.09256247
Iteration 16/25 | Loss: 0.09256247
Iteration 17/25 | Loss: 0.09256247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0925624668598175, 0.0925624668598175, 0.0925624668598175, 0.0925624668598175, 0.0925624668598175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0925624668598175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09256247
Iteration 2/1000 | Loss: 0.00071029
Iteration 3/1000 | Loss: 0.00038956
Iteration 4/1000 | Loss: 0.00597718
Iteration 5/1000 | Loss: 0.00024181
Iteration 6/1000 | Loss: 0.00075642
Iteration 7/1000 | Loss: 0.00004703
Iteration 8/1000 | Loss: 0.00003686
Iteration 9/1000 | Loss: 0.00003323
Iteration 10/1000 | Loss: 0.00002949
Iteration 11/1000 | Loss: 0.00002706
Iteration 12/1000 | Loss: 0.00002428
Iteration 13/1000 | Loss: 0.00002249
Iteration 14/1000 | Loss: 0.00002131
Iteration 15/1000 | Loss: 0.00002082
Iteration 16/1000 | Loss: 0.00001911
Iteration 17/1000 | Loss: 0.00001891
Iteration 18/1000 | Loss: 0.00001738
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001608
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001510
Iteration 23/1000 | Loss: 0.00001467
Iteration 24/1000 | Loss: 0.00001431
Iteration 25/1000 | Loss: 0.00001402
Iteration 26/1000 | Loss: 0.00001417
Iteration 27/1000 | Loss: 0.00001417
Iteration 28/1000 | Loss: 0.00001388
Iteration 29/1000 | Loss: 0.00001386
Iteration 30/1000 | Loss: 0.00001367
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001360
Iteration 33/1000 | Loss: 0.00001358
Iteration 34/1000 | Loss: 0.00001358
Iteration 35/1000 | Loss: 0.00001357
Iteration 36/1000 | Loss: 0.00001355
Iteration 37/1000 | Loss: 0.00001355
Iteration 38/1000 | Loss: 0.00001358
Iteration 39/1000 | Loss: 0.00001358
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001333
Iteration 42/1000 | Loss: 0.00001331
Iteration 43/1000 | Loss: 0.00001330
Iteration 44/1000 | Loss: 0.00001330
Iteration 45/1000 | Loss: 0.00001329
Iteration 46/1000 | Loss: 0.00001327
Iteration 47/1000 | Loss: 0.00001326
Iteration 48/1000 | Loss: 0.00001325
Iteration 49/1000 | Loss: 0.00001324
Iteration 50/1000 | Loss: 0.00001323
Iteration 51/1000 | Loss: 0.00001323
Iteration 52/1000 | Loss: 0.00001323
Iteration 53/1000 | Loss: 0.00001322
Iteration 54/1000 | Loss: 0.00001321
Iteration 55/1000 | Loss: 0.00001321
Iteration 56/1000 | Loss: 0.00001320
Iteration 57/1000 | Loss: 0.00001320
Iteration 58/1000 | Loss: 0.00001320
Iteration 59/1000 | Loss: 0.00001320
Iteration 60/1000 | Loss: 0.00001319
Iteration 61/1000 | Loss: 0.00001319
Iteration 62/1000 | Loss: 0.00001318
Iteration 63/1000 | Loss: 0.00001318
Iteration 64/1000 | Loss: 0.00001318
Iteration 65/1000 | Loss: 0.00001317
Iteration 66/1000 | Loss: 0.00001317
Iteration 67/1000 | Loss: 0.00001317
Iteration 68/1000 | Loss: 0.00001317
Iteration 69/1000 | Loss: 0.00001317
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001316
Iteration 72/1000 | Loss: 0.00001316
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001316
Iteration 76/1000 | Loss: 0.00001315
Iteration 77/1000 | Loss: 0.00001315
Iteration 78/1000 | Loss: 0.00001315
Iteration 79/1000 | Loss: 0.00001315
Iteration 80/1000 | Loss: 0.00001315
Iteration 81/1000 | Loss: 0.00001315
Iteration 82/1000 | Loss: 0.00001315
Iteration 83/1000 | Loss: 0.00001315
Iteration 84/1000 | Loss: 0.00001315
Iteration 85/1000 | Loss: 0.00001315
Iteration 86/1000 | Loss: 0.00001315
Iteration 87/1000 | Loss: 0.00001315
Iteration 88/1000 | Loss: 0.00001315
Iteration 89/1000 | Loss: 0.00001314
Iteration 90/1000 | Loss: 0.00001314
Iteration 91/1000 | Loss: 0.00001314
Iteration 92/1000 | Loss: 0.00001314
Iteration 93/1000 | Loss: 0.00001314
Iteration 94/1000 | Loss: 0.00001314
Iteration 95/1000 | Loss: 0.00001314
Iteration 96/1000 | Loss: 0.00001314
Iteration 97/1000 | Loss: 0.00001314
Iteration 98/1000 | Loss: 0.00001314
Iteration 99/1000 | Loss: 0.00001314
Iteration 100/1000 | Loss: 0.00001314
Iteration 101/1000 | Loss: 0.00001313
Iteration 102/1000 | Loss: 0.00001313
Iteration 103/1000 | Loss: 0.00001313
Iteration 104/1000 | Loss: 0.00001313
Iteration 105/1000 | Loss: 0.00001313
Iteration 106/1000 | Loss: 0.00001313
Iteration 107/1000 | Loss: 0.00001313
Iteration 108/1000 | Loss: 0.00001313
Iteration 109/1000 | Loss: 0.00001313
Iteration 110/1000 | Loss: 0.00001313
Iteration 111/1000 | Loss: 0.00001313
Iteration 112/1000 | Loss: 0.00001313
Iteration 113/1000 | Loss: 0.00001313
Iteration 114/1000 | Loss: 0.00001313
Iteration 115/1000 | Loss: 0.00001312
Iteration 116/1000 | Loss: 0.00001312
Iteration 117/1000 | Loss: 0.00001312
Iteration 118/1000 | Loss: 0.00001312
Iteration 119/1000 | Loss: 0.00001312
Iteration 120/1000 | Loss: 0.00001312
Iteration 121/1000 | Loss: 0.00001312
Iteration 122/1000 | Loss: 0.00001312
Iteration 123/1000 | Loss: 0.00001312
Iteration 124/1000 | Loss: 0.00001312
Iteration 125/1000 | Loss: 0.00001312
Iteration 126/1000 | Loss: 0.00001312
Iteration 127/1000 | Loss: 0.00001312
Iteration 128/1000 | Loss: 0.00001312
Iteration 129/1000 | Loss: 0.00001312
Iteration 130/1000 | Loss: 0.00001312
Iteration 131/1000 | Loss: 0.00001312
Iteration 132/1000 | Loss: 0.00001312
Iteration 133/1000 | Loss: 0.00001312
Iteration 134/1000 | Loss: 0.00001312
Iteration 135/1000 | Loss: 0.00001312
Iteration 136/1000 | Loss: 0.00001312
Iteration 137/1000 | Loss: 0.00001312
Iteration 138/1000 | Loss: 0.00001312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.3123928511049598e-05, 1.3123928511049598e-05, 1.3123928511049598e-05, 1.3123928511049598e-05, 1.3123928511049598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3123928511049598e-05

Optimization complete. Final v2v error: 3.053948402404785 mm

Highest mean error: 9.997967720031738 mm for frame 78

Lowest mean error: 2.851976156234741 mm for frame 135

Saving results

Total time: 68.20414423942566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492380
Iteration 2/25 | Loss: 0.00097953
Iteration 3/25 | Loss: 0.00072234
Iteration 4/25 | Loss: 0.00069921
Iteration 5/25 | Loss: 0.00069018
Iteration 6/25 | Loss: 0.00068799
Iteration 7/25 | Loss: 0.00068734
Iteration 8/25 | Loss: 0.00068734
Iteration 9/25 | Loss: 0.00068734
Iteration 10/25 | Loss: 0.00068734
Iteration 11/25 | Loss: 0.00068734
Iteration 12/25 | Loss: 0.00068734
Iteration 13/25 | Loss: 0.00068734
Iteration 14/25 | Loss: 0.00068734
Iteration 15/25 | Loss: 0.00068734
Iteration 16/25 | Loss: 0.00068734
Iteration 17/25 | Loss: 0.00068734
Iteration 18/25 | Loss: 0.00068734
Iteration 19/25 | Loss: 0.00068734
Iteration 20/25 | Loss: 0.00068734
Iteration 21/25 | Loss: 0.00068734
Iteration 22/25 | Loss: 0.00068734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006873367237858474, 0.0006873367237858474, 0.0006873367237858474, 0.0006873367237858474, 0.0006873367237858474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006873367237858474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59848225
Iteration 2/25 | Loss: 0.00029169
Iteration 3/25 | Loss: 0.00029167
Iteration 4/25 | Loss: 0.00029167
Iteration 5/25 | Loss: 0.00029167
Iteration 6/25 | Loss: 0.00029167
Iteration 7/25 | Loss: 0.00029167
Iteration 8/25 | Loss: 0.00029167
Iteration 9/25 | Loss: 0.00029167
Iteration 10/25 | Loss: 0.00029167
Iteration 11/25 | Loss: 0.00029167
Iteration 12/25 | Loss: 0.00029167
Iteration 13/25 | Loss: 0.00029167
Iteration 14/25 | Loss: 0.00029167
Iteration 15/25 | Loss: 0.00029167
Iteration 16/25 | Loss: 0.00029167
Iteration 17/25 | Loss: 0.00029167
Iteration 18/25 | Loss: 0.00029167
Iteration 19/25 | Loss: 0.00029167
Iteration 20/25 | Loss: 0.00029167
Iteration 21/25 | Loss: 0.00029167
Iteration 22/25 | Loss: 0.00029167
Iteration 23/25 | Loss: 0.00029167
Iteration 24/25 | Loss: 0.00029167
Iteration 25/25 | Loss: 0.00029167

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029167
Iteration 2/1000 | Loss: 0.00002551
Iteration 3/1000 | Loss: 0.00001785
Iteration 4/1000 | Loss: 0.00001632
Iteration 5/1000 | Loss: 0.00001560
Iteration 6/1000 | Loss: 0.00001506
Iteration 7/1000 | Loss: 0.00001469
Iteration 8/1000 | Loss: 0.00001449
Iteration 9/1000 | Loss: 0.00001446
Iteration 10/1000 | Loss: 0.00001442
Iteration 11/1000 | Loss: 0.00001432
Iteration 12/1000 | Loss: 0.00001432
Iteration 13/1000 | Loss: 0.00001430
Iteration 14/1000 | Loss: 0.00001425
Iteration 15/1000 | Loss: 0.00001418
Iteration 16/1000 | Loss: 0.00001414
Iteration 17/1000 | Loss: 0.00001413
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001412
Iteration 20/1000 | Loss: 0.00001412
Iteration 21/1000 | Loss: 0.00001411
Iteration 22/1000 | Loss: 0.00001411
Iteration 23/1000 | Loss: 0.00001410
Iteration 24/1000 | Loss: 0.00001409
Iteration 25/1000 | Loss: 0.00001409
Iteration 26/1000 | Loss: 0.00001408
Iteration 27/1000 | Loss: 0.00001408
Iteration 28/1000 | Loss: 0.00001405
Iteration 29/1000 | Loss: 0.00001405
Iteration 30/1000 | Loss: 0.00001405
Iteration 31/1000 | Loss: 0.00001404
Iteration 32/1000 | Loss: 0.00001404
Iteration 33/1000 | Loss: 0.00001403
Iteration 34/1000 | Loss: 0.00001403
Iteration 35/1000 | Loss: 0.00001403
Iteration 36/1000 | Loss: 0.00001402
Iteration 37/1000 | Loss: 0.00001402
Iteration 38/1000 | Loss: 0.00001402
Iteration 39/1000 | Loss: 0.00001402
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001401
Iteration 42/1000 | Loss: 0.00001401
Iteration 43/1000 | Loss: 0.00001400
Iteration 44/1000 | Loss: 0.00001400
Iteration 45/1000 | Loss: 0.00001399
Iteration 46/1000 | Loss: 0.00001399
Iteration 47/1000 | Loss: 0.00001399
Iteration 48/1000 | Loss: 0.00001399
Iteration 49/1000 | Loss: 0.00001399
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001398
Iteration 52/1000 | Loss: 0.00001397
Iteration 53/1000 | Loss: 0.00001397
Iteration 54/1000 | Loss: 0.00001397
Iteration 55/1000 | Loss: 0.00001397
Iteration 56/1000 | Loss: 0.00001397
Iteration 57/1000 | Loss: 0.00001397
Iteration 58/1000 | Loss: 0.00001397
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001396
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001396
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001396
Iteration 66/1000 | Loss: 0.00001396
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001396
Iteration 69/1000 | Loss: 0.00001396
Iteration 70/1000 | Loss: 0.00001396
Iteration 71/1000 | Loss: 0.00001396
Iteration 72/1000 | Loss: 0.00001395
Iteration 73/1000 | Loss: 0.00001395
Iteration 74/1000 | Loss: 0.00001395
Iteration 75/1000 | Loss: 0.00001395
Iteration 76/1000 | Loss: 0.00001395
Iteration 77/1000 | Loss: 0.00001395
Iteration 78/1000 | Loss: 0.00001395
Iteration 79/1000 | Loss: 0.00001395
Iteration 80/1000 | Loss: 0.00001395
Iteration 81/1000 | Loss: 0.00001395
Iteration 82/1000 | Loss: 0.00001395
Iteration 83/1000 | Loss: 0.00001395
Iteration 84/1000 | Loss: 0.00001395
Iteration 85/1000 | Loss: 0.00001395
Iteration 86/1000 | Loss: 0.00001395
Iteration 87/1000 | Loss: 0.00001395
Iteration 88/1000 | Loss: 0.00001395
Iteration 89/1000 | Loss: 0.00001395
Iteration 90/1000 | Loss: 0.00001395
Iteration 91/1000 | Loss: 0.00001394
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001394
Iteration 94/1000 | Loss: 0.00001394
Iteration 95/1000 | Loss: 0.00001394
Iteration 96/1000 | Loss: 0.00001394
Iteration 97/1000 | Loss: 0.00001394
Iteration 98/1000 | Loss: 0.00001394
Iteration 99/1000 | Loss: 0.00001394
Iteration 100/1000 | Loss: 0.00001394
Iteration 101/1000 | Loss: 0.00001394
Iteration 102/1000 | Loss: 0.00001394
Iteration 103/1000 | Loss: 0.00001394
Iteration 104/1000 | Loss: 0.00001394
Iteration 105/1000 | Loss: 0.00001394
Iteration 106/1000 | Loss: 0.00001394
Iteration 107/1000 | Loss: 0.00001394
Iteration 108/1000 | Loss: 0.00001394
Iteration 109/1000 | Loss: 0.00001394
Iteration 110/1000 | Loss: 0.00001394
Iteration 111/1000 | Loss: 0.00001394
Iteration 112/1000 | Loss: 0.00001394
Iteration 113/1000 | Loss: 0.00001394
Iteration 114/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.3939744349045213e-05, 1.3939744349045213e-05, 1.3939744349045213e-05, 1.3939744349045213e-05, 1.3939744349045213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3939744349045213e-05

Optimization complete. Final v2v error: 3.088245391845703 mm

Highest mean error: 4.218522071838379 mm for frame 110

Lowest mean error: 2.5364692211151123 mm for frame 17

Saving results

Total time: 34.18744778633118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036535
Iteration 2/25 | Loss: 0.00223746
Iteration 3/25 | Loss: 0.00150821
Iteration 4/25 | Loss: 0.00135659
Iteration 5/25 | Loss: 0.00132772
Iteration 6/25 | Loss: 0.00127379
Iteration 7/25 | Loss: 0.00125901
Iteration 8/25 | Loss: 0.00123479
Iteration 9/25 | Loss: 0.00121781
Iteration 10/25 | Loss: 0.00121798
Iteration 11/25 | Loss: 0.00120415
Iteration 12/25 | Loss: 0.00119438
Iteration 13/25 | Loss: 0.00119236
Iteration 14/25 | Loss: 0.00119188
Iteration 15/25 | Loss: 0.00119561
Iteration 16/25 | Loss: 0.00119059
Iteration 17/25 | Loss: 0.00118936
Iteration 18/25 | Loss: 0.00118917
Iteration 19/25 | Loss: 0.00118915
Iteration 20/25 | Loss: 0.00118915
Iteration 21/25 | Loss: 0.00118910
Iteration 22/25 | Loss: 0.00118909
Iteration 23/25 | Loss: 0.00118909
Iteration 24/25 | Loss: 0.00118908
Iteration 25/25 | Loss: 0.00118908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45724297
Iteration 2/25 | Loss: 0.00348169
Iteration 3/25 | Loss: 0.00348169
Iteration 4/25 | Loss: 0.00348169
Iteration 5/25 | Loss: 0.00348169
Iteration 6/25 | Loss: 0.00348169
Iteration 7/25 | Loss: 0.00348169
Iteration 8/25 | Loss: 0.00348169
Iteration 9/25 | Loss: 0.00348169
Iteration 10/25 | Loss: 0.00348169
Iteration 11/25 | Loss: 0.00348169
Iteration 12/25 | Loss: 0.00348169
Iteration 13/25 | Loss: 0.00348169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0034816903062164783, 0.0034816903062164783, 0.0034816903062164783, 0.0034816903062164783, 0.0034816903062164783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034816903062164783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00348169
Iteration 2/1000 | Loss: 0.00080545
Iteration 3/1000 | Loss: 0.00143805
Iteration 4/1000 | Loss: 0.00052070
Iteration 5/1000 | Loss: 0.00044969
Iteration 6/1000 | Loss: 0.00036587
Iteration 7/1000 | Loss: 0.00028675
Iteration 8/1000 | Loss: 0.00051991
Iteration 9/1000 | Loss: 0.00036812
Iteration 10/1000 | Loss: 0.00022944
Iteration 11/1000 | Loss: 0.00020314
Iteration 12/1000 | Loss: 0.00018798
Iteration 13/1000 | Loss: 0.00020530
Iteration 14/1000 | Loss: 0.00038906
Iteration 15/1000 | Loss: 0.00038019
Iteration 16/1000 | Loss: 0.00069211
Iteration 17/1000 | Loss: 0.00069542
Iteration 18/1000 | Loss: 0.00601038
Iteration 19/1000 | Loss: 0.01235934
Iteration 20/1000 | Loss: 0.00206202
Iteration 21/1000 | Loss: 0.00091547
Iteration 22/1000 | Loss: 0.00090201
Iteration 23/1000 | Loss: 0.00082124
Iteration 24/1000 | Loss: 0.00079992
Iteration 25/1000 | Loss: 0.00069231
Iteration 26/1000 | Loss: 0.00058472
Iteration 27/1000 | Loss: 0.00086119
Iteration 28/1000 | Loss: 0.00022952
Iteration 29/1000 | Loss: 0.00097006
Iteration 30/1000 | Loss: 0.00060262
Iteration 31/1000 | Loss: 0.00026031
Iteration 32/1000 | Loss: 0.00066190
Iteration 33/1000 | Loss: 0.00066984
Iteration 34/1000 | Loss: 0.00010250
Iteration 35/1000 | Loss: 0.00006719
Iteration 36/1000 | Loss: 0.00005011
Iteration 37/1000 | Loss: 0.00004163
Iteration 38/1000 | Loss: 0.00003423
Iteration 39/1000 | Loss: 0.00021965
Iteration 40/1000 | Loss: 0.00003270
Iteration 41/1000 | Loss: 0.00002728
Iteration 42/1000 | Loss: 0.00021755
Iteration 43/1000 | Loss: 0.00005515
Iteration 44/1000 | Loss: 0.00004284
Iteration 45/1000 | Loss: 0.00002660
Iteration 46/1000 | Loss: 0.00002082
Iteration 47/1000 | Loss: 0.00003439
Iteration 48/1000 | Loss: 0.00002102
Iteration 49/1000 | Loss: 0.00001927
Iteration 50/1000 | Loss: 0.00020049
Iteration 51/1000 | Loss: 0.00015975
Iteration 52/1000 | Loss: 0.00019782
Iteration 53/1000 | Loss: 0.00020485
Iteration 54/1000 | Loss: 0.00001763
Iteration 55/1000 | Loss: 0.00020556
Iteration 56/1000 | Loss: 0.00002261
Iteration 57/1000 | Loss: 0.00001597
Iteration 58/1000 | Loss: 0.00001437
Iteration 59/1000 | Loss: 0.00001386
Iteration 60/1000 | Loss: 0.00001361
Iteration 61/1000 | Loss: 0.00001347
Iteration 62/1000 | Loss: 0.00001340
Iteration 63/1000 | Loss: 0.00001333
Iteration 64/1000 | Loss: 0.00001323
Iteration 65/1000 | Loss: 0.00001319
Iteration 66/1000 | Loss: 0.00001316
Iteration 67/1000 | Loss: 0.00001316
Iteration 68/1000 | Loss: 0.00001312
Iteration 69/1000 | Loss: 0.00001312
Iteration 70/1000 | Loss: 0.00001310
Iteration 71/1000 | Loss: 0.00001309
Iteration 72/1000 | Loss: 0.00001309
Iteration 73/1000 | Loss: 0.00001308
Iteration 74/1000 | Loss: 0.00001308
Iteration 75/1000 | Loss: 0.00001308
Iteration 76/1000 | Loss: 0.00001307
Iteration 77/1000 | Loss: 0.00001307
Iteration 78/1000 | Loss: 0.00001307
Iteration 79/1000 | Loss: 0.00001307
Iteration 80/1000 | Loss: 0.00001307
Iteration 81/1000 | Loss: 0.00001307
Iteration 82/1000 | Loss: 0.00001307
Iteration 83/1000 | Loss: 0.00001307
Iteration 84/1000 | Loss: 0.00001307
Iteration 85/1000 | Loss: 0.00001306
Iteration 86/1000 | Loss: 0.00001305
Iteration 87/1000 | Loss: 0.00001305
Iteration 88/1000 | Loss: 0.00001305
Iteration 89/1000 | Loss: 0.00001304
Iteration 90/1000 | Loss: 0.00001304
Iteration 91/1000 | Loss: 0.00001303
Iteration 92/1000 | Loss: 0.00001303
Iteration 93/1000 | Loss: 0.00001303
Iteration 94/1000 | Loss: 0.00001303
Iteration 95/1000 | Loss: 0.00001303
Iteration 96/1000 | Loss: 0.00001303
Iteration 97/1000 | Loss: 0.00001302
Iteration 98/1000 | Loss: 0.00001302
Iteration 99/1000 | Loss: 0.00001302
Iteration 100/1000 | Loss: 0.00001302
Iteration 101/1000 | Loss: 0.00001302
Iteration 102/1000 | Loss: 0.00001302
Iteration 103/1000 | Loss: 0.00001302
Iteration 104/1000 | Loss: 0.00001302
Iteration 105/1000 | Loss: 0.00001301
Iteration 106/1000 | Loss: 0.00001301
Iteration 107/1000 | Loss: 0.00001301
Iteration 108/1000 | Loss: 0.00001301
Iteration 109/1000 | Loss: 0.00001301
Iteration 110/1000 | Loss: 0.00001301
Iteration 111/1000 | Loss: 0.00001301
Iteration 112/1000 | Loss: 0.00001301
Iteration 113/1000 | Loss: 0.00001300
Iteration 114/1000 | Loss: 0.00001300
Iteration 115/1000 | Loss: 0.00001300
Iteration 116/1000 | Loss: 0.00001300
Iteration 117/1000 | Loss: 0.00001300
Iteration 118/1000 | Loss: 0.00001300
Iteration 119/1000 | Loss: 0.00001300
Iteration 120/1000 | Loss: 0.00001299
Iteration 121/1000 | Loss: 0.00001299
Iteration 122/1000 | Loss: 0.00001299
Iteration 123/1000 | Loss: 0.00001299
Iteration 124/1000 | Loss: 0.00001299
Iteration 125/1000 | Loss: 0.00001299
Iteration 126/1000 | Loss: 0.00001299
Iteration 127/1000 | Loss: 0.00001299
Iteration 128/1000 | Loss: 0.00001299
Iteration 129/1000 | Loss: 0.00001298
Iteration 130/1000 | Loss: 0.00001298
Iteration 131/1000 | Loss: 0.00001298
Iteration 132/1000 | Loss: 0.00001298
Iteration 133/1000 | Loss: 0.00001298
Iteration 134/1000 | Loss: 0.00001298
Iteration 135/1000 | Loss: 0.00001298
Iteration 136/1000 | Loss: 0.00001298
Iteration 137/1000 | Loss: 0.00001298
Iteration 138/1000 | Loss: 0.00001298
Iteration 139/1000 | Loss: 0.00001298
Iteration 140/1000 | Loss: 0.00001298
Iteration 141/1000 | Loss: 0.00001298
Iteration 142/1000 | Loss: 0.00001298
Iteration 143/1000 | Loss: 0.00001298
Iteration 144/1000 | Loss: 0.00001298
Iteration 145/1000 | Loss: 0.00001298
Iteration 146/1000 | Loss: 0.00001298
Iteration 147/1000 | Loss: 0.00001298
Iteration 148/1000 | Loss: 0.00001298
Iteration 149/1000 | Loss: 0.00001297
Iteration 150/1000 | Loss: 0.00001297
Iteration 151/1000 | Loss: 0.00001297
Iteration 152/1000 | Loss: 0.00001297
Iteration 153/1000 | Loss: 0.00001297
Iteration 154/1000 | Loss: 0.00001297
Iteration 155/1000 | Loss: 0.00001297
Iteration 156/1000 | Loss: 0.00001297
Iteration 157/1000 | Loss: 0.00001297
Iteration 158/1000 | Loss: 0.00001297
Iteration 159/1000 | Loss: 0.00001297
Iteration 160/1000 | Loss: 0.00001297
Iteration 161/1000 | Loss: 0.00001297
Iteration 162/1000 | Loss: 0.00001297
Iteration 163/1000 | Loss: 0.00001297
Iteration 164/1000 | Loss: 0.00001297
Iteration 165/1000 | Loss: 0.00001297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.2973509001312777e-05, 1.2973509001312777e-05, 1.2973509001312777e-05, 1.2973509001312777e-05, 1.2973509001312777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2973509001312777e-05

Optimization complete. Final v2v error: 3.05942440032959 mm

Highest mean error: 3.854404926300049 mm for frame 13

Lowest mean error: 2.866285562515259 mm for frame 47

Saving results

Total time: 146.98392248153687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066707
Iteration 2/25 | Loss: 0.00248558
Iteration 3/25 | Loss: 0.00151717
Iteration 4/25 | Loss: 0.00130251
Iteration 5/25 | Loss: 0.00129035
Iteration 6/25 | Loss: 0.00110496
Iteration 7/25 | Loss: 0.00105730
Iteration 8/25 | Loss: 0.00095635
Iteration 9/25 | Loss: 0.00088376
Iteration 10/25 | Loss: 0.00088297
Iteration 11/25 | Loss: 0.00087188
Iteration 12/25 | Loss: 0.00084406
Iteration 13/25 | Loss: 0.00084672
Iteration 14/25 | Loss: 0.00083681
Iteration 15/25 | Loss: 0.00084808
Iteration 16/25 | Loss: 0.00084910
Iteration 17/25 | Loss: 0.00084320
Iteration 18/25 | Loss: 0.00082961
Iteration 19/25 | Loss: 0.00082747
Iteration 20/25 | Loss: 0.00083708
Iteration 21/25 | Loss: 0.00084773
Iteration 22/25 | Loss: 0.00086476
Iteration 23/25 | Loss: 0.00082718
Iteration 24/25 | Loss: 0.00082244
Iteration 25/25 | Loss: 0.00082121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51226640
Iteration 2/25 | Loss: 0.00100706
Iteration 3/25 | Loss: 0.00100706
Iteration 4/25 | Loss: 0.00100706
Iteration 5/25 | Loss: 0.00100706
Iteration 6/25 | Loss: 0.00100706
Iteration 7/25 | Loss: 0.00100706
Iteration 8/25 | Loss: 0.00100706
Iteration 9/25 | Loss: 0.00100706
Iteration 10/25 | Loss: 0.00100706
Iteration 11/25 | Loss: 0.00100706
Iteration 12/25 | Loss: 0.00100706
Iteration 13/25 | Loss: 0.00100706
Iteration 14/25 | Loss: 0.00100706
Iteration 15/25 | Loss: 0.00100706
Iteration 16/25 | Loss: 0.00100706
Iteration 17/25 | Loss: 0.00100706
Iteration 18/25 | Loss: 0.00100706
Iteration 19/25 | Loss: 0.00100706
Iteration 20/25 | Loss: 0.00100706
Iteration 21/25 | Loss: 0.00100706
Iteration 22/25 | Loss: 0.00100706
Iteration 23/25 | Loss: 0.00100706
Iteration 24/25 | Loss: 0.00100706
Iteration 25/25 | Loss: 0.00100706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100706
Iteration 2/1000 | Loss: 0.00039934
Iteration 3/1000 | Loss: 0.00035320
Iteration 4/1000 | Loss: 0.00037439
Iteration 5/1000 | Loss: 0.00035791
Iteration 6/1000 | Loss: 0.00449837
Iteration 7/1000 | Loss: 0.00286013
Iteration 8/1000 | Loss: 0.00412109
Iteration 9/1000 | Loss: 0.00226684
Iteration 10/1000 | Loss: 0.00397374
Iteration 11/1000 | Loss: 0.00319943
Iteration 12/1000 | Loss: 0.00270314
Iteration 13/1000 | Loss: 0.00320171
Iteration 14/1000 | Loss: 0.00411316
Iteration 15/1000 | Loss: 0.00271961
Iteration 16/1000 | Loss: 0.00122792
Iteration 17/1000 | Loss: 0.00138231
Iteration 18/1000 | Loss: 0.00117248
Iteration 19/1000 | Loss: 0.00227092
Iteration 20/1000 | Loss: 0.00175447
Iteration 21/1000 | Loss: 0.00586561
Iteration 22/1000 | Loss: 0.00191827
Iteration 23/1000 | Loss: 0.00257337
Iteration 24/1000 | Loss: 0.00203036
Iteration 25/1000 | Loss: 0.00222244
Iteration 26/1000 | Loss: 0.00120516
Iteration 27/1000 | Loss: 0.00163771
Iteration 28/1000 | Loss: 0.00084698
Iteration 29/1000 | Loss: 0.00266307
Iteration 30/1000 | Loss: 0.00080717
Iteration 31/1000 | Loss: 0.00238319
Iteration 32/1000 | Loss: 0.00048638
Iteration 33/1000 | Loss: 0.00032474
Iteration 34/1000 | Loss: 0.00053889
Iteration 35/1000 | Loss: 0.00036265
Iteration 36/1000 | Loss: 0.00048533
Iteration 37/1000 | Loss: 0.00031151
Iteration 38/1000 | Loss: 0.00057523
Iteration 39/1000 | Loss: 0.00096278
Iteration 40/1000 | Loss: 0.00064333
Iteration 41/1000 | Loss: 0.00027846
Iteration 42/1000 | Loss: 0.00033281
Iteration 43/1000 | Loss: 0.00068580
Iteration 44/1000 | Loss: 0.00043776
Iteration 45/1000 | Loss: 0.00027732
Iteration 46/1000 | Loss: 0.00033438
Iteration 47/1000 | Loss: 0.00034564
Iteration 48/1000 | Loss: 0.00031320
Iteration 49/1000 | Loss: 0.00031983
Iteration 50/1000 | Loss: 0.00030734
Iteration 51/1000 | Loss: 0.00029731
Iteration 52/1000 | Loss: 0.00071453
Iteration 53/1000 | Loss: 0.00075352
Iteration 54/1000 | Loss: 0.00047975
Iteration 55/1000 | Loss: 0.00029815
Iteration 56/1000 | Loss: 0.00062293
Iteration 57/1000 | Loss: 0.00028223
Iteration 58/1000 | Loss: 0.00047198
Iteration 59/1000 | Loss: 0.00069145
Iteration 60/1000 | Loss: 0.00030143
Iteration 61/1000 | Loss: 0.00055066
Iteration 62/1000 | Loss: 0.00014869
Iteration 63/1000 | Loss: 0.00063686
Iteration 64/1000 | Loss: 0.00052481
Iteration 65/1000 | Loss: 0.00029322
Iteration 66/1000 | Loss: 0.00028397
Iteration 67/1000 | Loss: 0.00035457
Iteration 68/1000 | Loss: 0.00025888
Iteration 69/1000 | Loss: 0.00062899
Iteration 70/1000 | Loss: 0.00046225
Iteration 71/1000 | Loss: 0.00039768
Iteration 72/1000 | Loss: 0.00095243
Iteration 73/1000 | Loss: 0.00020703
Iteration 74/1000 | Loss: 0.00018906
Iteration 75/1000 | Loss: 0.00167176
Iteration 76/1000 | Loss: 0.00066605
Iteration 77/1000 | Loss: 0.00062000
Iteration 78/1000 | Loss: 0.00005617
Iteration 79/1000 | Loss: 0.00171471
Iteration 80/1000 | Loss: 0.00086834
Iteration 81/1000 | Loss: 0.00165338
Iteration 82/1000 | Loss: 0.00019796
Iteration 83/1000 | Loss: 0.00401998
Iteration 84/1000 | Loss: 0.00419459
Iteration 85/1000 | Loss: 0.00204619
Iteration 86/1000 | Loss: 0.00017744
Iteration 87/1000 | Loss: 0.00187240
Iteration 88/1000 | Loss: 0.00004712
Iteration 89/1000 | Loss: 0.00003681
Iteration 90/1000 | Loss: 0.00043462
Iteration 91/1000 | Loss: 0.00002856
Iteration 92/1000 | Loss: 0.00002202
Iteration 93/1000 | Loss: 0.00001971
Iteration 94/1000 | Loss: 0.00001781
Iteration 95/1000 | Loss: 0.00001637
Iteration 96/1000 | Loss: 0.00001527
Iteration 97/1000 | Loss: 0.00001433
Iteration 98/1000 | Loss: 0.00001377
Iteration 99/1000 | Loss: 0.00001343
Iteration 100/1000 | Loss: 0.00001312
Iteration 101/1000 | Loss: 0.00001312
Iteration 102/1000 | Loss: 0.00001292
Iteration 103/1000 | Loss: 0.00001276
Iteration 104/1000 | Loss: 0.00001273
Iteration 105/1000 | Loss: 0.00001269
Iteration 106/1000 | Loss: 0.00001264
Iteration 107/1000 | Loss: 0.00001263
Iteration 108/1000 | Loss: 0.00001262
Iteration 109/1000 | Loss: 0.00001257
Iteration 110/1000 | Loss: 0.00001256
Iteration 111/1000 | Loss: 0.00001256
Iteration 112/1000 | Loss: 0.00001256
Iteration 113/1000 | Loss: 0.00001255
Iteration 114/1000 | Loss: 0.00001254
Iteration 115/1000 | Loss: 0.00001254
Iteration 116/1000 | Loss: 0.00001253
Iteration 117/1000 | Loss: 0.00001252
Iteration 118/1000 | Loss: 0.00001251
Iteration 119/1000 | Loss: 0.00001251
Iteration 120/1000 | Loss: 0.00001251
Iteration 121/1000 | Loss: 0.00001251
Iteration 122/1000 | Loss: 0.00001251
Iteration 123/1000 | Loss: 0.00001251
Iteration 124/1000 | Loss: 0.00001251
Iteration 125/1000 | Loss: 0.00001251
Iteration 126/1000 | Loss: 0.00001250
Iteration 127/1000 | Loss: 0.00001250
Iteration 128/1000 | Loss: 0.00001248
Iteration 129/1000 | Loss: 0.00001247
Iteration 130/1000 | Loss: 0.00001247
Iteration 131/1000 | Loss: 0.00001247
Iteration 132/1000 | Loss: 0.00001247
Iteration 133/1000 | Loss: 0.00001246
Iteration 134/1000 | Loss: 0.00001246
Iteration 135/1000 | Loss: 0.00001245
Iteration 136/1000 | Loss: 0.00001245
Iteration 137/1000 | Loss: 0.00001244
Iteration 138/1000 | Loss: 0.00001243
Iteration 139/1000 | Loss: 0.00001242
Iteration 140/1000 | Loss: 0.00001242
Iteration 141/1000 | Loss: 0.00001241
Iteration 142/1000 | Loss: 0.00001241
Iteration 143/1000 | Loss: 0.00001241
Iteration 144/1000 | Loss: 0.00001241
Iteration 145/1000 | Loss: 0.00001241
Iteration 146/1000 | Loss: 0.00001240
Iteration 147/1000 | Loss: 0.00001240
Iteration 148/1000 | Loss: 0.00001240
Iteration 149/1000 | Loss: 0.00001239
Iteration 150/1000 | Loss: 0.00001239
Iteration 151/1000 | Loss: 0.00001239
Iteration 152/1000 | Loss: 0.00001238
Iteration 153/1000 | Loss: 0.00001238
Iteration 154/1000 | Loss: 0.00001238
Iteration 155/1000 | Loss: 0.00001238
Iteration 156/1000 | Loss: 0.00001238
Iteration 157/1000 | Loss: 0.00001238
Iteration 158/1000 | Loss: 0.00001237
Iteration 159/1000 | Loss: 0.00001237
Iteration 160/1000 | Loss: 0.00001237
Iteration 161/1000 | Loss: 0.00001237
Iteration 162/1000 | Loss: 0.00001237
Iteration 163/1000 | Loss: 0.00001237
Iteration 164/1000 | Loss: 0.00001236
Iteration 165/1000 | Loss: 0.00001236
Iteration 166/1000 | Loss: 0.00001236
Iteration 167/1000 | Loss: 0.00001235
Iteration 168/1000 | Loss: 0.00001235
Iteration 169/1000 | Loss: 0.00001235
Iteration 170/1000 | Loss: 0.00001235
Iteration 171/1000 | Loss: 0.00001235
Iteration 172/1000 | Loss: 0.00001235
Iteration 173/1000 | Loss: 0.00001235
Iteration 174/1000 | Loss: 0.00001235
Iteration 175/1000 | Loss: 0.00001235
Iteration 176/1000 | Loss: 0.00001235
Iteration 177/1000 | Loss: 0.00001234
Iteration 178/1000 | Loss: 0.00001234
Iteration 179/1000 | Loss: 0.00001234
Iteration 180/1000 | Loss: 0.00001234
Iteration 181/1000 | Loss: 0.00001234
Iteration 182/1000 | Loss: 0.00001233
Iteration 183/1000 | Loss: 0.00001233
Iteration 184/1000 | Loss: 0.00001233
Iteration 185/1000 | Loss: 0.00001233
Iteration 186/1000 | Loss: 0.00001233
Iteration 187/1000 | Loss: 0.00001233
Iteration 188/1000 | Loss: 0.00001233
Iteration 189/1000 | Loss: 0.00001232
Iteration 190/1000 | Loss: 0.00001232
Iteration 191/1000 | Loss: 0.00001232
Iteration 192/1000 | Loss: 0.00001232
Iteration 193/1000 | Loss: 0.00001232
Iteration 194/1000 | Loss: 0.00001232
Iteration 195/1000 | Loss: 0.00001232
Iteration 196/1000 | Loss: 0.00001232
Iteration 197/1000 | Loss: 0.00001232
Iteration 198/1000 | Loss: 0.00001232
Iteration 199/1000 | Loss: 0.00001232
Iteration 200/1000 | Loss: 0.00001231
Iteration 201/1000 | Loss: 0.00001231
Iteration 202/1000 | Loss: 0.00001231
Iteration 203/1000 | Loss: 0.00001231
Iteration 204/1000 | Loss: 0.00001230
Iteration 205/1000 | Loss: 0.00001230
Iteration 206/1000 | Loss: 0.00001230
Iteration 207/1000 | Loss: 0.00001230
Iteration 208/1000 | Loss: 0.00001230
Iteration 209/1000 | Loss: 0.00001230
Iteration 210/1000 | Loss: 0.00001230
Iteration 211/1000 | Loss: 0.00001230
Iteration 212/1000 | Loss: 0.00001230
Iteration 213/1000 | Loss: 0.00001229
Iteration 214/1000 | Loss: 0.00001229
Iteration 215/1000 | Loss: 0.00001229
Iteration 216/1000 | Loss: 0.00001229
Iteration 217/1000 | Loss: 0.00001228
Iteration 218/1000 | Loss: 0.00001228
Iteration 219/1000 | Loss: 0.00001228
Iteration 220/1000 | Loss: 0.00001228
Iteration 221/1000 | Loss: 0.00001228
Iteration 222/1000 | Loss: 0.00001228
Iteration 223/1000 | Loss: 0.00001228
Iteration 224/1000 | Loss: 0.00001228
Iteration 225/1000 | Loss: 0.00001228
Iteration 226/1000 | Loss: 0.00001228
Iteration 227/1000 | Loss: 0.00001228
Iteration 228/1000 | Loss: 0.00001227
Iteration 229/1000 | Loss: 0.00001227
Iteration 230/1000 | Loss: 0.00001227
Iteration 231/1000 | Loss: 0.00001227
Iteration 232/1000 | Loss: 0.00001227
Iteration 233/1000 | Loss: 0.00001227
Iteration 234/1000 | Loss: 0.00001227
Iteration 235/1000 | Loss: 0.00001227
Iteration 236/1000 | Loss: 0.00001226
Iteration 237/1000 | Loss: 0.00001226
Iteration 238/1000 | Loss: 0.00001226
Iteration 239/1000 | Loss: 0.00001226
Iteration 240/1000 | Loss: 0.00001226
Iteration 241/1000 | Loss: 0.00001226
Iteration 242/1000 | Loss: 0.00001226
Iteration 243/1000 | Loss: 0.00001226
Iteration 244/1000 | Loss: 0.00001226
Iteration 245/1000 | Loss: 0.00001226
Iteration 246/1000 | Loss: 0.00001226
Iteration 247/1000 | Loss: 0.00001226
Iteration 248/1000 | Loss: 0.00001226
Iteration 249/1000 | Loss: 0.00001226
Iteration 250/1000 | Loss: 0.00001226
Iteration 251/1000 | Loss: 0.00001226
Iteration 252/1000 | Loss: 0.00001226
Iteration 253/1000 | Loss: 0.00001226
Iteration 254/1000 | Loss: 0.00001226
Iteration 255/1000 | Loss: 0.00001226
Iteration 256/1000 | Loss: 0.00001226
Iteration 257/1000 | Loss: 0.00001226
Iteration 258/1000 | Loss: 0.00001226
Iteration 259/1000 | Loss: 0.00001226
Iteration 260/1000 | Loss: 0.00001226
Iteration 261/1000 | Loss: 0.00001226
Iteration 262/1000 | Loss: 0.00001226
Iteration 263/1000 | Loss: 0.00001226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.2256604350113776e-05, 1.2256604350113776e-05, 1.2256604350113776e-05, 1.2256604350113776e-05, 1.2256604350113776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2256604350113776e-05

Optimization complete. Final v2v error: 2.924740791320801 mm

Highest mean error: 3.9336960315704346 mm for frame 57

Lowest mean error: 2.513198137283325 mm for frame 6

Saving results

Total time: 203.1149492263794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075552
Iteration 2/25 | Loss: 0.00243825
Iteration 3/25 | Loss: 0.00177361
Iteration 4/25 | Loss: 0.00137577
Iteration 5/25 | Loss: 0.00127172
Iteration 6/25 | Loss: 0.00145224
Iteration 7/25 | Loss: 0.00118801
Iteration 8/25 | Loss: 0.00117465
Iteration 9/25 | Loss: 0.00108367
Iteration 10/25 | Loss: 0.00090978
Iteration 11/25 | Loss: 0.00087917
Iteration 12/25 | Loss: 0.00092292
Iteration 13/25 | Loss: 0.00085183
Iteration 14/25 | Loss: 0.00082739
Iteration 15/25 | Loss: 0.00082799
Iteration 16/25 | Loss: 0.00082067
Iteration 17/25 | Loss: 0.00083328
Iteration 18/25 | Loss: 0.00087061
Iteration 19/25 | Loss: 0.00082394
Iteration 20/25 | Loss: 0.00083319
Iteration 21/25 | Loss: 0.00083169
Iteration 22/25 | Loss: 0.00080561
Iteration 23/25 | Loss: 0.00079696
Iteration 24/25 | Loss: 0.00088255
Iteration 25/25 | Loss: 0.00081578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49595964
Iteration 2/25 | Loss: 0.00136805
Iteration 3/25 | Loss: 0.00122001
Iteration 4/25 | Loss: 0.00122001
Iteration 5/25 | Loss: 0.00122001
Iteration 6/25 | Loss: 0.00122001
Iteration 7/25 | Loss: 0.00122001
Iteration 8/25 | Loss: 0.00122001
Iteration 9/25 | Loss: 0.00122001
Iteration 10/25 | Loss: 0.00122001
Iteration 11/25 | Loss: 0.00122001
Iteration 12/25 | Loss: 0.00122001
Iteration 13/25 | Loss: 0.00122001
Iteration 14/25 | Loss: 0.00122001
Iteration 15/25 | Loss: 0.00122001
Iteration 16/25 | Loss: 0.00122001
Iteration 17/25 | Loss: 0.00122001
Iteration 18/25 | Loss: 0.00122001
Iteration 19/25 | Loss: 0.00122001
Iteration 20/25 | Loss: 0.00122001
Iteration 21/25 | Loss: 0.00122001
Iteration 22/25 | Loss: 0.00122001
Iteration 23/25 | Loss: 0.00122001
Iteration 24/25 | Loss: 0.00122001
Iteration 25/25 | Loss: 0.00122001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122001
Iteration 2/1000 | Loss: 0.00122863
Iteration 3/1000 | Loss: 0.00127316
Iteration 4/1000 | Loss: 0.00090606
Iteration 5/1000 | Loss: 0.00076680
Iteration 6/1000 | Loss: 0.00045238
Iteration 7/1000 | Loss: 0.00025164
Iteration 8/1000 | Loss: 0.00004496
Iteration 9/1000 | Loss: 0.00013157
Iteration 10/1000 | Loss: 0.00003823
Iteration 11/1000 | Loss: 0.00043197
Iteration 12/1000 | Loss: 0.00004128
Iteration 13/1000 | Loss: 0.00015433
Iteration 14/1000 | Loss: 0.00022370
Iteration 15/1000 | Loss: 0.00062420
Iteration 16/1000 | Loss: 0.00004556
Iteration 17/1000 | Loss: 0.00004887
Iteration 18/1000 | Loss: 0.00162556
Iteration 19/1000 | Loss: 0.00137726
Iteration 20/1000 | Loss: 0.00341846
Iteration 21/1000 | Loss: 0.00120723
Iteration 22/1000 | Loss: 0.00077657
Iteration 23/1000 | Loss: 0.00027649
Iteration 24/1000 | Loss: 0.00004566
Iteration 25/1000 | Loss: 0.00015692
Iteration 26/1000 | Loss: 0.00005258
Iteration 27/1000 | Loss: 0.00007850
Iteration 28/1000 | Loss: 0.00003309
Iteration 29/1000 | Loss: 0.00013342
Iteration 30/1000 | Loss: 0.00003148
Iteration 31/1000 | Loss: 0.00073288
Iteration 32/1000 | Loss: 0.00155146
Iteration 33/1000 | Loss: 0.00124038
Iteration 34/1000 | Loss: 0.00029638
Iteration 35/1000 | Loss: 0.00106684
Iteration 36/1000 | Loss: 0.00020371
Iteration 37/1000 | Loss: 0.00004983
Iteration 38/1000 | Loss: 0.00101986
Iteration 39/1000 | Loss: 0.00083802
Iteration 40/1000 | Loss: 0.00083709
Iteration 41/1000 | Loss: 0.00010098
Iteration 42/1000 | Loss: 0.00084034
Iteration 43/1000 | Loss: 0.00013911
Iteration 44/1000 | Loss: 0.00044645
Iteration 45/1000 | Loss: 0.00106038
Iteration 46/1000 | Loss: 0.00072632
Iteration 47/1000 | Loss: 0.00073044
Iteration 48/1000 | Loss: 0.00124727
Iteration 49/1000 | Loss: 0.00075729
Iteration 50/1000 | Loss: 0.00100138
Iteration 51/1000 | Loss: 0.00059812
Iteration 52/1000 | Loss: 0.00130049
Iteration 53/1000 | Loss: 0.00074258
Iteration 54/1000 | Loss: 0.00121986
Iteration 55/1000 | Loss: 0.00074453
Iteration 56/1000 | Loss: 0.00053082
Iteration 57/1000 | Loss: 0.00006965
Iteration 58/1000 | Loss: 0.00003910
Iteration 59/1000 | Loss: 0.00003347
Iteration 60/1000 | Loss: 0.00093128
Iteration 61/1000 | Loss: 0.00058037
Iteration 62/1000 | Loss: 0.00005372
Iteration 63/1000 | Loss: 0.00011043
Iteration 64/1000 | Loss: 0.00130681
Iteration 65/1000 | Loss: 0.00100308
Iteration 66/1000 | Loss: 0.00127432
Iteration 67/1000 | Loss: 0.00114130
Iteration 68/1000 | Loss: 0.00017959
Iteration 69/1000 | Loss: 0.00003265
Iteration 70/1000 | Loss: 0.00103379
Iteration 71/1000 | Loss: 0.00019884
Iteration 72/1000 | Loss: 0.00020933
Iteration 73/1000 | Loss: 0.00021418
Iteration 74/1000 | Loss: 0.00025361
Iteration 75/1000 | Loss: 0.00011902
Iteration 76/1000 | Loss: 0.00002686
Iteration 77/1000 | Loss: 0.00002098
Iteration 78/1000 | Loss: 0.00001946
Iteration 79/1000 | Loss: 0.00001879
Iteration 80/1000 | Loss: 0.00023006
Iteration 81/1000 | Loss: 0.00001887
Iteration 82/1000 | Loss: 0.00029311
Iteration 83/1000 | Loss: 0.00014183
Iteration 84/1000 | Loss: 0.00007673
Iteration 85/1000 | Loss: 0.00001988
Iteration 86/1000 | Loss: 0.00019487
Iteration 87/1000 | Loss: 0.00012466
Iteration 88/1000 | Loss: 0.00009729
Iteration 89/1000 | Loss: 0.00001903
Iteration 90/1000 | Loss: 0.00004277
Iteration 91/1000 | Loss: 0.00001743
Iteration 92/1000 | Loss: 0.00001676
Iteration 93/1000 | Loss: 0.00017299
Iteration 94/1000 | Loss: 0.00001709
Iteration 95/1000 | Loss: 0.00001624
Iteration 96/1000 | Loss: 0.00001611
Iteration 97/1000 | Loss: 0.00001598
Iteration 98/1000 | Loss: 0.00001597
Iteration 99/1000 | Loss: 0.00001597
Iteration 100/1000 | Loss: 0.00001593
Iteration 101/1000 | Loss: 0.00001591
Iteration 102/1000 | Loss: 0.00001590
Iteration 103/1000 | Loss: 0.00001590
Iteration 104/1000 | Loss: 0.00001589
Iteration 105/1000 | Loss: 0.00001589
Iteration 106/1000 | Loss: 0.00001589
Iteration 107/1000 | Loss: 0.00001588
Iteration 108/1000 | Loss: 0.00001585
Iteration 109/1000 | Loss: 0.00001584
Iteration 110/1000 | Loss: 0.00001583
Iteration 111/1000 | Loss: 0.00001583
Iteration 112/1000 | Loss: 0.00001580
Iteration 113/1000 | Loss: 0.00001578
Iteration 114/1000 | Loss: 0.00001577
Iteration 115/1000 | Loss: 0.00001572
Iteration 116/1000 | Loss: 0.00001569
Iteration 117/1000 | Loss: 0.00001569
Iteration 118/1000 | Loss: 0.00001568
Iteration 119/1000 | Loss: 0.00001568
Iteration 120/1000 | Loss: 0.00001568
Iteration 121/1000 | Loss: 0.00001567
Iteration 122/1000 | Loss: 0.00001566
Iteration 123/1000 | Loss: 0.00001566
Iteration 124/1000 | Loss: 0.00001566
Iteration 125/1000 | Loss: 0.00001565
Iteration 126/1000 | Loss: 0.00001565
Iteration 127/1000 | Loss: 0.00001565
Iteration 128/1000 | Loss: 0.00001565
Iteration 129/1000 | Loss: 0.00001565
Iteration 130/1000 | Loss: 0.00001565
Iteration 131/1000 | Loss: 0.00001565
Iteration 132/1000 | Loss: 0.00001564
Iteration 133/1000 | Loss: 0.00001564
Iteration 134/1000 | Loss: 0.00001564
Iteration 135/1000 | Loss: 0.00001564
Iteration 136/1000 | Loss: 0.00001564
Iteration 137/1000 | Loss: 0.00001564
Iteration 138/1000 | Loss: 0.00001564
Iteration 139/1000 | Loss: 0.00001564
Iteration 140/1000 | Loss: 0.00001564
Iteration 141/1000 | Loss: 0.00001564
Iteration 142/1000 | Loss: 0.00001564
Iteration 143/1000 | Loss: 0.00001564
Iteration 144/1000 | Loss: 0.00001564
Iteration 145/1000 | Loss: 0.00001563
Iteration 146/1000 | Loss: 0.00001563
Iteration 147/1000 | Loss: 0.00001563
Iteration 148/1000 | Loss: 0.00001563
Iteration 149/1000 | Loss: 0.00001563
Iteration 150/1000 | Loss: 0.00001563
Iteration 151/1000 | Loss: 0.00001563
Iteration 152/1000 | Loss: 0.00001563
Iteration 153/1000 | Loss: 0.00001563
Iteration 154/1000 | Loss: 0.00001563
Iteration 155/1000 | Loss: 0.00001563
Iteration 156/1000 | Loss: 0.00001563
Iteration 157/1000 | Loss: 0.00001563
Iteration 158/1000 | Loss: 0.00001563
Iteration 159/1000 | Loss: 0.00001563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.5628236724296585e-05, 1.5628236724296585e-05, 1.5628236724296585e-05, 1.5628236724296585e-05, 1.5628236724296585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5628236724296585e-05

Optimization complete. Final v2v error: 3.336796998977661 mm

Highest mean error: 9.037949562072754 mm for frame 230

Lowest mean error: 2.923457622528076 mm for frame 147

Saving results

Total time: 218.7700114250183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062368
Iteration 2/25 | Loss: 0.00393836
Iteration 3/25 | Loss: 0.00253387
Iteration 4/25 | Loss: 0.00219738
Iteration 5/25 | Loss: 0.00183099
Iteration 6/25 | Loss: 0.00151884
Iteration 7/25 | Loss: 0.00136707
Iteration 8/25 | Loss: 0.00128450
Iteration 9/25 | Loss: 0.00117028
Iteration 10/25 | Loss: 0.00111436
Iteration 11/25 | Loss: 0.00111151
Iteration 12/25 | Loss: 0.00108601
Iteration 13/25 | Loss: 0.00106272
Iteration 14/25 | Loss: 0.00106033
Iteration 15/25 | Loss: 0.00106418
Iteration 16/25 | Loss: 0.00105493
Iteration 17/25 | Loss: 0.00101172
Iteration 18/25 | Loss: 0.00100056
Iteration 19/25 | Loss: 0.00098880
Iteration 20/25 | Loss: 0.00099132
Iteration 21/25 | Loss: 0.00098806
Iteration 22/25 | Loss: 0.00098438
Iteration 23/25 | Loss: 0.00098168
Iteration 24/25 | Loss: 0.00098063
Iteration 25/25 | Loss: 0.00098053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48113811
Iteration 2/25 | Loss: 0.00216581
Iteration 3/25 | Loss: 0.00205338
Iteration 4/25 | Loss: 0.00205338
Iteration 5/25 | Loss: 0.00205338
Iteration 6/25 | Loss: 0.00205338
Iteration 7/25 | Loss: 0.00205338
Iteration 8/25 | Loss: 0.00205337
Iteration 9/25 | Loss: 0.00205337
Iteration 10/25 | Loss: 0.00205337
Iteration 11/25 | Loss: 0.00205337
Iteration 12/25 | Loss: 0.00205337
Iteration 13/25 | Loss: 0.00205337
Iteration 14/25 | Loss: 0.00205337
Iteration 15/25 | Loss: 0.00205337
Iteration 16/25 | Loss: 0.00205337
Iteration 17/25 | Loss: 0.00205337
Iteration 18/25 | Loss: 0.00205337
Iteration 19/25 | Loss: 0.00205337
Iteration 20/25 | Loss: 0.00205337
Iteration 21/25 | Loss: 0.00205337
Iteration 22/25 | Loss: 0.00205337
Iteration 23/25 | Loss: 0.00205337
Iteration 24/25 | Loss: 0.00205337
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002053373958915472, 0.002053373958915472, 0.002053373958915472, 0.002053373958915472, 0.002053373958915472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002053373958915472

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205337
Iteration 2/1000 | Loss: 0.00466700
Iteration 3/1000 | Loss: 0.00050543
Iteration 4/1000 | Loss: 0.00041149
Iteration 5/1000 | Loss: 0.00047023
Iteration 6/1000 | Loss: 0.00022413
Iteration 7/1000 | Loss: 0.00101827
Iteration 8/1000 | Loss: 0.00020385
Iteration 9/1000 | Loss: 0.00023787
Iteration 10/1000 | Loss: 0.00012255
Iteration 11/1000 | Loss: 0.00010156
Iteration 12/1000 | Loss: 0.00037664
Iteration 13/1000 | Loss: 0.00017947
Iteration 14/1000 | Loss: 0.00086744
Iteration 15/1000 | Loss: 0.00043749
Iteration 16/1000 | Loss: 0.00005980
Iteration 17/1000 | Loss: 0.00015334
Iteration 18/1000 | Loss: 0.00014408
Iteration 19/1000 | Loss: 0.00008825
Iteration 20/1000 | Loss: 0.00014273
Iteration 21/1000 | Loss: 0.00053483
Iteration 22/1000 | Loss: 0.00028706
Iteration 23/1000 | Loss: 0.00036178
Iteration 24/1000 | Loss: 0.00003890
Iteration 25/1000 | Loss: 0.00002583
Iteration 26/1000 | Loss: 0.00009169
Iteration 27/1000 | Loss: 0.00003562
Iteration 28/1000 | Loss: 0.00002267
Iteration 29/1000 | Loss: 0.00002158
Iteration 30/1000 | Loss: 0.00002132
Iteration 31/1000 | Loss: 0.00002280
Iteration 32/1000 | Loss: 0.00002056
Iteration 33/1000 | Loss: 0.00002085
Iteration 34/1000 | Loss: 0.00002085
Iteration 35/1000 | Loss: 0.00002030
Iteration 36/1000 | Loss: 0.00002029
Iteration 37/1000 | Loss: 0.00002091
Iteration 38/1000 | Loss: 0.00002010
Iteration 39/1000 | Loss: 0.00002010
Iteration 40/1000 | Loss: 0.00002013
Iteration 41/1000 | Loss: 0.00002005
Iteration 42/1000 | Loss: 0.00002004
Iteration 43/1000 | Loss: 0.00002014
Iteration 44/1000 | Loss: 0.00002026
Iteration 45/1000 | Loss: 0.00001998
Iteration 46/1000 | Loss: 0.00001998
Iteration 47/1000 | Loss: 0.00001997
Iteration 48/1000 | Loss: 0.00001997
Iteration 49/1000 | Loss: 0.00001997
Iteration 50/1000 | Loss: 0.00001997
Iteration 51/1000 | Loss: 0.00001997
Iteration 52/1000 | Loss: 0.00001997
Iteration 53/1000 | Loss: 0.00001997
Iteration 54/1000 | Loss: 0.00001996
Iteration 55/1000 | Loss: 0.00001996
Iteration 56/1000 | Loss: 0.00001996
Iteration 57/1000 | Loss: 0.00001996
Iteration 58/1000 | Loss: 0.00001996
Iteration 59/1000 | Loss: 0.00001996
Iteration 60/1000 | Loss: 0.00001996
Iteration 61/1000 | Loss: 0.00001996
Iteration 62/1000 | Loss: 0.00001996
Iteration 63/1000 | Loss: 0.00001995
Iteration 64/1000 | Loss: 0.00001995
Iteration 65/1000 | Loss: 0.00002107
Iteration 66/1000 | Loss: 0.00001993
Iteration 67/1000 | Loss: 0.00001993
Iteration 68/1000 | Loss: 0.00001993
Iteration 69/1000 | Loss: 0.00001993
Iteration 70/1000 | Loss: 0.00001993
Iteration 71/1000 | Loss: 0.00001993
Iteration 72/1000 | Loss: 0.00001993
Iteration 73/1000 | Loss: 0.00001993
Iteration 74/1000 | Loss: 0.00001992
Iteration 75/1000 | Loss: 0.00001992
Iteration 76/1000 | Loss: 0.00001992
Iteration 77/1000 | Loss: 0.00001992
Iteration 78/1000 | Loss: 0.00001992
Iteration 79/1000 | Loss: 0.00001992
Iteration 80/1000 | Loss: 0.00001992
Iteration 81/1000 | Loss: 0.00001992
Iteration 82/1000 | Loss: 0.00001992
Iteration 83/1000 | Loss: 0.00001992
Iteration 84/1000 | Loss: 0.00001992
Iteration 85/1000 | Loss: 0.00001992
Iteration 86/1000 | Loss: 0.00001992
Iteration 87/1000 | Loss: 0.00001992
Iteration 88/1000 | Loss: 0.00001992
Iteration 89/1000 | Loss: 0.00001991
Iteration 90/1000 | Loss: 0.00001991
Iteration 91/1000 | Loss: 0.00001991
Iteration 92/1000 | Loss: 0.00001991
Iteration 93/1000 | Loss: 0.00001991
Iteration 94/1000 | Loss: 0.00001991
Iteration 95/1000 | Loss: 0.00001991
Iteration 96/1000 | Loss: 0.00001991
Iteration 97/1000 | Loss: 0.00001991
Iteration 98/1000 | Loss: 0.00001991
Iteration 99/1000 | Loss: 0.00001990
Iteration 100/1000 | Loss: 0.00001990
Iteration 101/1000 | Loss: 0.00001990
Iteration 102/1000 | Loss: 0.00001989
Iteration 103/1000 | Loss: 0.00001989
Iteration 104/1000 | Loss: 0.00001989
Iteration 105/1000 | Loss: 0.00001989
Iteration 106/1000 | Loss: 0.00001989
Iteration 107/1000 | Loss: 0.00001989
Iteration 108/1000 | Loss: 0.00001988
Iteration 109/1000 | Loss: 0.00001988
Iteration 110/1000 | Loss: 0.00001988
Iteration 111/1000 | Loss: 0.00001988
Iteration 112/1000 | Loss: 0.00001988
Iteration 113/1000 | Loss: 0.00001988
Iteration 114/1000 | Loss: 0.00001988
Iteration 115/1000 | Loss: 0.00001988
Iteration 116/1000 | Loss: 0.00001988
Iteration 117/1000 | Loss: 0.00001987
Iteration 118/1000 | Loss: 0.00001987
Iteration 119/1000 | Loss: 0.00001987
Iteration 120/1000 | Loss: 0.00002010
Iteration 121/1000 | Loss: 0.00002010
Iteration 122/1000 | Loss: 0.00001985
Iteration 123/1000 | Loss: 0.00001985
Iteration 124/1000 | Loss: 0.00001985
Iteration 125/1000 | Loss: 0.00001985
Iteration 126/1000 | Loss: 0.00001985
Iteration 127/1000 | Loss: 0.00001984
Iteration 128/1000 | Loss: 0.00001984
Iteration 129/1000 | Loss: 0.00001984
Iteration 130/1000 | Loss: 0.00001984
Iteration 131/1000 | Loss: 0.00001984
Iteration 132/1000 | Loss: 0.00001984
Iteration 133/1000 | Loss: 0.00001984
Iteration 134/1000 | Loss: 0.00001984
Iteration 135/1000 | Loss: 0.00001989
Iteration 136/1000 | Loss: 0.00001984
Iteration 137/1000 | Loss: 0.00001984
Iteration 138/1000 | Loss: 0.00001984
Iteration 139/1000 | Loss: 0.00001984
Iteration 140/1000 | Loss: 0.00001984
Iteration 141/1000 | Loss: 0.00001984
Iteration 142/1000 | Loss: 0.00001984
Iteration 143/1000 | Loss: 0.00001984
Iteration 144/1000 | Loss: 0.00001984
Iteration 145/1000 | Loss: 0.00001984
Iteration 146/1000 | Loss: 0.00001984
Iteration 147/1000 | Loss: 0.00001984
Iteration 148/1000 | Loss: 0.00001984
Iteration 149/1000 | Loss: 0.00001984
Iteration 150/1000 | Loss: 0.00001984
Iteration 151/1000 | Loss: 0.00001984
Iteration 152/1000 | Loss: 0.00001984
Iteration 153/1000 | Loss: 0.00001984
Iteration 154/1000 | Loss: 0.00001984
Iteration 155/1000 | Loss: 0.00001984
Iteration 156/1000 | Loss: 0.00001984
Iteration 157/1000 | Loss: 0.00001984
Iteration 158/1000 | Loss: 0.00001984
Iteration 159/1000 | Loss: 0.00001984
Iteration 160/1000 | Loss: 0.00001984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.9835299099213444e-05, 1.9835299099213444e-05, 1.9835299099213444e-05, 1.9835299099213444e-05, 1.9835299099213444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9835299099213444e-05

Optimization complete. Final v2v error: 3.720797300338745 mm

Highest mean error: 4.296582221984863 mm for frame 92

Lowest mean error: 3.3085367679595947 mm for frame 112

Saving results

Total time: 120.13821983337402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773381
Iteration 2/25 | Loss: 0.00084119
Iteration 3/25 | Loss: 0.00073279
Iteration 4/25 | Loss: 0.00069635
Iteration 5/25 | Loss: 0.00068592
Iteration 6/25 | Loss: 0.00068367
Iteration 7/25 | Loss: 0.00068351
Iteration 8/25 | Loss: 0.00068351
Iteration 9/25 | Loss: 0.00068351
Iteration 10/25 | Loss: 0.00068351
Iteration 11/25 | Loss: 0.00068351
Iteration 12/25 | Loss: 0.00068351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006835114909335971, 0.0006835114909335971, 0.0006835114909335971, 0.0006835114909335971, 0.0006835114909335971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006835114909335971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33761239
Iteration 2/25 | Loss: 0.00029715
Iteration 3/25 | Loss: 0.00029708
Iteration 4/25 | Loss: 0.00029708
Iteration 5/25 | Loss: 0.00029708
Iteration 6/25 | Loss: 0.00029708
Iteration 7/25 | Loss: 0.00029708
Iteration 8/25 | Loss: 0.00029708
Iteration 9/25 | Loss: 0.00029708
Iteration 10/25 | Loss: 0.00029708
Iteration 11/25 | Loss: 0.00029708
Iteration 12/25 | Loss: 0.00029708
Iteration 13/25 | Loss: 0.00029708
Iteration 14/25 | Loss: 0.00029708
Iteration 15/25 | Loss: 0.00029708
Iteration 16/25 | Loss: 0.00029708
Iteration 17/25 | Loss: 0.00029708
Iteration 18/25 | Loss: 0.00029708
Iteration 19/25 | Loss: 0.00029708
Iteration 20/25 | Loss: 0.00029708
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002970802306663245, 0.0002970802306663245, 0.0002970802306663245, 0.0002970802306663245, 0.0002970802306663245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002970802306663245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029708
Iteration 2/1000 | Loss: 0.00003316
Iteration 3/1000 | Loss: 0.00002166
Iteration 4/1000 | Loss: 0.00001946
Iteration 5/1000 | Loss: 0.00001797
Iteration 6/1000 | Loss: 0.00001737
Iteration 7/1000 | Loss: 0.00001688
Iteration 8/1000 | Loss: 0.00001655
Iteration 9/1000 | Loss: 0.00001650
Iteration 10/1000 | Loss: 0.00001626
Iteration 11/1000 | Loss: 0.00001618
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001599
Iteration 14/1000 | Loss: 0.00001598
Iteration 15/1000 | Loss: 0.00001598
Iteration 16/1000 | Loss: 0.00001592
Iteration 17/1000 | Loss: 0.00001592
Iteration 18/1000 | Loss: 0.00001590
Iteration 19/1000 | Loss: 0.00001590
Iteration 20/1000 | Loss: 0.00001590
Iteration 21/1000 | Loss: 0.00001587
Iteration 22/1000 | Loss: 0.00001578
Iteration 23/1000 | Loss: 0.00001578
Iteration 24/1000 | Loss: 0.00001577
Iteration 25/1000 | Loss: 0.00001577
Iteration 26/1000 | Loss: 0.00001577
Iteration 27/1000 | Loss: 0.00001576
Iteration 28/1000 | Loss: 0.00001576
Iteration 29/1000 | Loss: 0.00001576
Iteration 30/1000 | Loss: 0.00001576
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00001575
Iteration 33/1000 | Loss: 0.00001575
Iteration 34/1000 | Loss: 0.00001575
Iteration 35/1000 | Loss: 0.00001575
Iteration 36/1000 | Loss: 0.00001575
Iteration 37/1000 | Loss: 0.00001575
Iteration 38/1000 | Loss: 0.00001575
Iteration 39/1000 | Loss: 0.00001575
Iteration 40/1000 | Loss: 0.00001575
Iteration 41/1000 | Loss: 0.00001575
Iteration 42/1000 | Loss: 0.00001575
Iteration 43/1000 | Loss: 0.00001574
Iteration 44/1000 | Loss: 0.00001574
Iteration 45/1000 | Loss: 0.00001573
Iteration 46/1000 | Loss: 0.00001573
Iteration 47/1000 | Loss: 0.00001573
Iteration 48/1000 | Loss: 0.00001573
Iteration 49/1000 | Loss: 0.00001573
Iteration 50/1000 | Loss: 0.00001572
Iteration 51/1000 | Loss: 0.00001572
Iteration 52/1000 | Loss: 0.00001572
Iteration 53/1000 | Loss: 0.00001572
Iteration 54/1000 | Loss: 0.00001571
Iteration 55/1000 | Loss: 0.00001571
Iteration 56/1000 | Loss: 0.00001571
Iteration 57/1000 | Loss: 0.00001570
Iteration 58/1000 | Loss: 0.00001570
Iteration 59/1000 | Loss: 0.00001570
Iteration 60/1000 | Loss: 0.00001570
Iteration 61/1000 | Loss: 0.00001569
Iteration 62/1000 | Loss: 0.00001569
Iteration 63/1000 | Loss: 0.00001569
Iteration 64/1000 | Loss: 0.00001568
Iteration 65/1000 | Loss: 0.00001568
Iteration 66/1000 | Loss: 0.00001568
Iteration 67/1000 | Loss: 0.00001568
Iteration 68/1000 | Loss: 0.00001568
Iteration 69/1000 | Loss: 0.00001567
Iteration 70/1000 | Loss: 0.00001567
Iteration 71/1000 | Loss: 0.00001567
Iteration 72/1000 | Loss: 0.00001567
Iteration 73/1000 | Loss: 0.00001567
Iteration 74/1000 | Loss: 0.00001567
Iteration 75/1000 | Loss: 0.00001567
Iteration 76/1000 | Loss: 0.00001567
Iteration 77/1000 | Loss: 0.00001567
Iteration 78/1000 | Loss: 0.00001566
Iteration 79/1000 | Loss: 0.00001566
Iteration 80/1000 | Loss: 0.00001566
Iteration 81/1000 | Loss: 0.00001566
Iteration 82/1000 | Loss: 0.00001566
Iteration 83/1000 | Loss: 0.00001566
Iteration 84/1000 | Loss: 0.00001565
Iteration 85/1000 | Loss: 0.00001565
Iteration 86/1000 | Loss: 0.00001565
Iteration 87/1000 | Loss: 0.00001565
Iteration 88/1000 | Loss: 0.00001564
Iteration 89/1000 | Loss: 0.00001564
Iteration 90/1000 | Loss: 0.00001564
Iteration 91/1000 | Loss: 0.00001563
Iteration 92/1000 | Loss: 0.00001563
Iteration 93/1000 | Loss: 0.00001563
Iteration 94/1000 | Loss: 0.00001563
Iteration 95/1000 | Loss: 0.00001562
Iteration 96/1000 | Loss: 0.00001562
Iteration 97/1000 | Loss: 0.00001562
Iteration 98/1000 | Loss: 0.00001562
Iteration 99/1000 | Loss: 0.00001561
Iteration 100/1000 | Loss: 0.00001561
Iteration 101/1000 | Loss: 0.00001561
Iteration 102/1000 | Loss: 0.00001561
Iteration 103/1000 | Loss: 0.00001561
Iteration 104/1000 | Loss: 0.00001560
Iteration 105/1000 | Loss: 0.00001560
Iteration 106/1000 | Loss: 0.00001560
Iteration 107/1000 | Loss: 0.00001560
Iteration 108/1000 | Loss: 0.00001560
Iteration 109/1000 | Loss: 0.00001560
Iteration 110/1000 | Loss: 0.00001560
Iteration 111/1000 | Loss: 0.00001559
Iteration 112/1000 | Loss: 0.00001559
Iteration 113/1000 | Loss: 0.00001559
Iteration 114/1000 | Loss: 0.00001559
Iteration 115/1000 | Loss: 0.00001559
Iteration 116/1000 | Loss: 0.00001559
Iteration 117/1000 | Loss: 0.00001558
Iteration 118/1000 | Loss: 0.00001558
Iteration 119/1000 | Loss: 0.00001558
Iteration 120/1000 | Loss: 0.00001558
Iteration 121/1000 | Loss: 0.00001558
Iteration 122/1000 | Loss: 0.00001558
Iteration 123/1000 | Loss: 0.00001558
Iteration 124/1000 | Loss: 0.00001558
Iteration 125/1000 | Loss: 0.00001558
Iteration 126/1000 | Loss: 0.00001558
Iteration 127/1000 | Loss: 0.00001558
Iteration 128/1000 | Loss: 0.00001558
Iteration 129/1000 | Loss: 0.00001558
Iteration 130/1000 | Loss: 0.00001558
Iteration 131/1000 | Loss: 0.00001558
Iteration 132/1000 | Loss: 0.00001558
Iteration 133/1000 | Loss: 0.00001558
Iteration 134/1000 | Loss: 0.00001558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.5582359992549755e-05, 1.5582359992549755e-05, 1.5582359992549755e-05, 1.5582359992549755e-05, 1.5582359992549755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5582359992549755e-05

Optimization complete. Final v2v error: 3.333946704864502 mm

Highest mean error: 4.451146602630615 mm for frame 5

Lowest mean error: 2.965153694152832 mm for frame 100

Saving results

Total time: 40.96897220611572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419678
Iteration 2/25 | Loss: 0.00079053
Iteration 3/25 | Loss: 0.00069459
Iteration 4/25 | Loss: 0.00067653
Iteration 5/25 | Loss: 0.00066988
Iteration 6/25 | Loss: 0.00066865
Iteration 7/25 | Loss: 0.00066844
Iteration 8/25 | Loss: 0.00066844
Iteration 9/25 | Loss: 0.00066844
Iteration 10/25 | Loss: 0.00066844
Iteration 11/25 | Loss: 0.00066844
Iteration 12/25 | Loss: 0.00066844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006684408872388303, 0.0006684408872388303, 0.0006684408872388303, 0.0006684408872388303, 0.0006684408872388303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006684408872388303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48109245
Iteration 2/25 | Loss: 0.00034759
Iteration 3/25 | Loss: 0.00034759
Iteration 4/25 | Loss: 0.00034759
Iteration 5/25 | Loss: 0.00034759
Iteration 6/25 | Loss: 0.00034758
Iteration 7/25 | Loss: 0.00034758
Iteration 8/25 | Loss: 0.00034758
Iteration 9/25 | Loss: 0.00034758
Iteration 10/25 | Loss: 0.00034758
Iteration 11/25 | Loss: 0.00034758
Iteration 12/25 | Loss: 0.00034758
Iteration 13/25 | Loss: 0.00034758
Iteration 14/25 | Loss: 0.00034758
Iteration 15/25 | Loss: 0.00034758
Iteration 16/25 | Loss: 0.00034758
Iteration 17/25 | Loss: 0.00034758
Iteration 18/25 | Loss: 0.00034758
Iteration 19/25 | Loss: 0.00034758
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00034758378751575947, 0.00034758378751575947, 0.00034758378751575947, 0.00034758378751575947, 0.00034758378751575947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034758378751575947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034758
Iteration 2/1000 | Loss: 0.00002041
Iteration 3/1000 | Loss: 0.00001614
Iteration 4/1000 | Loss: 0.00001493
Iteration 5/1000 | Loss: 0.00001410
Iteration 6/1000 | Loss: 0.00001364
Iteration 7/1000 | Loss: 0.00001335
Iteration 8/1000 | Loss: 0.00001321
Iteration 9/1000 | Loss: 0.00001310
Iteration 10/1000 | Loss: 0.00001303
Iteration 11/1000 | Loss: 0.00001297
Iteration 12/1000 | Loss: 0.00001297
Iteration 13/1000 | Loss: 0.00001296
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001294
Iteration 16/1000 | Loss: 0.00001293
Iteration 17/1000 | Loss: 0.00001293
Iteration 18/1000 | Loss: 0.00001293
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001292
Iteration 22/1000 | Loss: 0.00001291
Iteration 23/1000 | Loss: 0.00001291
Iteration 24/1000 | Loss: 0.00001291
Iteration 25/1000 | Loss: 0.00001291
Iteration 26/1000 | Loss: 0.00001291
Iteration 27/1000 | Loss: 0.00001291
Iteration 28/1000 | Loss: 0.00001291
Iteration 29/1000 | Loss: 0.00001290
Iteration 30/1000 | Loss: 0.00001290
Iteration 31/1000 | Loss: 0.00001290
Iteration 32/1000 | Loss: 0.00001290
Iteration 33/1000 | Loss: 0.00001289
Iteration 34/1000 | Loss: 0.00001289
Iteration 35/1000 | Loss: 0.00001289
Iteration 36/1000 | Loss: 0.00001289
Iteration 37/1000 | Loss: 0.00001289
Iteration 38/1000 | Loss: 0.00001289
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001289
Iteration 41/1000 | Loss: 0.00001289
Iteration 42/1000 | Loss: 0.00001288
Iteration 43/1000 | Loss: 0.00001288
Iteration 44/1000 | Loss: 0.00001288
Iteration 45/1000 | Loss: 0.00001288
Iteration 46/1000 | Loss: 0.00001287
Iteration 47/1000 | Loss: 0.00001287
Iteration 48/1000 | Loss: 0.00001287
Iteration 49/1000 | Loss: 0.00001287
Iteration 50/1000 | Loss: 0.00001287
Iteration 51/1000 | Loss: 0.00001287
Iteration 52/1000 | Loss: 0.00001287
Iteration 53/1000 | Loss: 0.00001286
Iteration 54/1000 | Loss: 0.00001286
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001286
Iteration 57/1000 | Loss: 0.00001286
Iteration 58/1000 | Loss: 0.00001286
Iteration 59/1000 | Loss: 0.00001286
Iteration 60/1000 | Loss: 0.00001286
Iteration 61/1000 | Loss: 0.00001286
Iteration 62/1000 | Loss: 0.00001286
Iteration 63/1000 | Loss: 0.00001285
Iteration 64/1000 | Loss: 0.00001285
Iteration 65/1000 | Loss: 0.00001285
Iteration 66/1000 | Loss: 0.00001285
Iteration 67/1000 | Loss: 0.00001285
Iteration 68/1000 | Loss: 0.00001285
Iteration 69/1000 | Loss: 0.00001285
Iteration 70/1000 | Loss: 0.00001285
Iteration 71/1000 | Loss: 0.00001285
Iteration 72/1000 | Loss: 0.00001285
Iteration 73/1000 | Loss: 0.00001285
Iteration 74/1000 | Loss: 0.00001285
Iteration 75/1000 | Loss: 0.00001285
Iteration 76/1000 | Loss: 0.00001285
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001285
Iteration 82/1000 | Loss: 0.00001285
Iteration 83/1000 | Loss: 0.00001285
Iteration 84/1000 | Loss: 0.00001285
Iteration 85/1000 | Loss: 0.00001285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.2849852282670327e-05, 1.2849852282670327e-05, 1.2849852282670327e-05, 1.2849852282670327e-05, 1.2849852282670327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2849852282670327e-05

Optimization complete. Final v2v error: 3.0366122722625732 mm

Highest mean error: 3.208765983581543 mm for frame 183

Lowest mean error: 2.787665367126465 mm for frame 0

Saving results

Total time: 29.189194440841675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526879
Iteration 2/25 | Loss: 0.00135741
Iteration 3/25 | Loss: 0.00086711
Iteration 4/25 | Loss: 0.00078439
Iteration 5/25 | Loss: 0.00076246
Iteration 6/25 | Loss: 0.00075709
Iteration 7/25 | Loss: 0.00075567
Iteration 8/25 | Loss: 0.00075515
Iteration 9/25 | Loss: 0.00075514
Iteration 10/25 | Loss: 0.00075514
Iteration 11/25 | Loss: 0.00075514
Iteration 12/25 | Loss: 0.00075514
Iteration 13/25 | Loss: 0.00075514
Iteration 14/25 | Loss: 0.00075514
Iteration 15/25 | Loss: 0.00075514
Iteration 16/25 | Loss: 0.00075514
Iteration 17/25 | Loss: 0.00075514
Iteration 18/25 | Loss: 0.00075514
Iteration 19/25 | Loss: 0.00075514
Iteration 20/25 | Loss: 0.00075514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007551400340162218, 0.0007551400340162218, 0.0007551400340162218, 0.0007551400340162218, 0.0007551400340162218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007551400340162218

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41403532
Iteration 2/25 | Loss: 0.00032445
Iteration 3/25 | Loss: 0.00032443
Iteration 4/25 | Loss: 0.00032443
Iteration 5/25 | Loss: 0.00032443
Iteration 6/25 | Loss: 0.00032443
Iteration 7/25 | Loss: 0.00032442
Iteration 8/25 | Loss: 0.00032442
Iteration 9/25 | Loss: 0.00032442
Iteration 10/25 | Loss: 0.00032442
Iteration 11/25 | Loss: 0.00032442
Iteration 12/25 | Loss: 0.00032442
Iteration 13/25 | Loss: 0.00032442
Iteration 14/25 | Loss: 0.00032442
Iteration 15/25 | Loss: 0.00032442
Iteration 16/25 | Loss: 0.00032442
Iteration 17/25 | Loss: 0.00032442
Iteration 18/25 | Loss: 0.00032442
Iteration 19/25 | Loss: 0.00032442
Iteration 20/25 | Loss: 0.00032442
Iteration 21/25 | Loss: 0.00032442
Iteration 22/25 | Loss: 0.00032442
Iteration 23/25 | Loss: 0.00032442
Iteration 24/25 | Loss: 0.00032442
Iteration 25/25 | Loss: 0.00032442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032442
Iteration 2/1000 | Loss: 0.00004196
Iteration 3/1000 | Loss: 0.00002903
Iteration 4/1000 | Loss: 0.00002612
Iteration 5/1000 | Loss: 0.00002481
Iteration 6/1000 | Loss: 0.00002398
Iteration 7/1000 | Loss: 0.00002341
Iteration 8/1000 | Loss: 0.00002301
Iteration 9/1000 | Loss: 0.00002267
Iteration 10/1000 | Loss: 0.00002246
Iteration 11/1000 | Loss: 0.00002227
Iteration 12/1000 | Loss: 0.00002211
Iteration 13/1000 | Loss: 0.00002198
Iteration 14/1000 | Loss: 0.00002192
Iteration 15/1000 | Loss: 0.00002190
Iteration 16/1000 | Loss: 0.00002190
Iteration 17/1000 | Loss: 0.00002188
Iteration 18/1000 | Loss: 0.00002188
Iteration 19/1000 | Loss: 0.00002187
Iteration 20/1000 | Loss: 0.00002185
Iteration 21/1000 | Loss: 0.00002185
Iteration 22/1000 | Loss: 0.00002184
Iteration 23/1000 | Loss: 0.00002184
Iteration 24/1000 | Loss: 0.00002184
Iteration 25/1000 | Loss: 0.00002184
Iteration 26/1000 | Loss: 0.00002184
Iteration 27/1000 | Loss: 0.00002183
Iteration 28/1000 | Loss: 0.00002183
Iteration 29/1000 | Loss: 0.00002183
Iteration 30/1000 | Loss: 0.00002181
Iteration 31/1000 | Loss: 0.00002181
Iteration 32/1000 | Loss: 0.00002180
Iteration 33/1000 | Loss: 0.00002180
Iteration 34/1000 | Loss: 0.00002180
Iteration 35/1000 | Loss: 0.00002178
Iteration 36/1000 | Loss: 0.00002177
Iteration 37/1000 | Loss: 0.00002176
Iteration 38/1000 | Loss: 0.00002175
Iteration 39/1000 | Loss: 0.00002175
Iteration 40/1000 | Loss: 0.00002174
Iteration 41/1000 | Loss: 0.00002174
Iteration 42/1000 | Loss: 0.00002174
Iteration 43/1000 | Loss: 0.00002174
Iteration 44/1000 | Loss: 0.00002173
Iteration 45/1000 | Loss: 0.00002173
Iteration 46/1000 | Loss: 0.00002173
Iteration 47/1000 | Loss: 0.00002172
Iteration 48/1000 | Loss: 0.00002172
Iteration 49/1000 | Loss: 0.00002172
Iteration 50/1000 | Loss: 0.00002171
Iteration 51/1000 | Loss: 0.00002171
Iteration 52/1000 | Loss: 0.00002170
Iteration 53/1000 | Loss: 0.00002170
Iteration 54/1000 | Loss: 0.00002170
Iteration 55/1000 | Loss: 0.00002169
Iteration 56/1000 | Loss: 0.00002168
Iteration 57/1000 | Loss: 0.00002168
Iteration 58/1000 | Loss: 0.00002166
Iteration 59/1000 | Loss: 0.00002165
Iteration 60/1000 | Loss: 0.00002165
Iteration 61/1000 | Loss: 0.00002164
Iteration 62/1000 | Loss: 0.00002163
Iteration 63/1000 | Loss: 0.00002163
Iteration 64/1000 | Loss: 0.00002163
Iteration 65/1000 | Loss: 0.00002163
Iteration 66/1000 | Loss: 0.00002163
Iteration 67/1000 | Loss: 0.00002162
Iteration 68/1000 | Loss: 0.00002162
Iteration 69/1000 | Loss: 0.00002162
Iteration 70/1000 | Loss: 0.00002162
Iteration 71/1000 | Loss: 0.00002162
Iteration 72/1000 | Loss: 0.00002162
Iteration 73/1000 | Loss: 0.00002162
Iteration 74/1000 | Loss: 0.00002162
Iteration 75/1000 | Loss: 0.00002162
Iteration 76/1000 | Loss: 0.00002161
Iteration 77/1000 | Loss: 0.00002161
Iteration 78/1000 | Loss: 0.00002161
Iteration 79/1000 | Loss: 0.00002161
Iteration 80/1000 | Loss: 0.00002160
Iteration 81/1000 | Loss: 0.00002160
Iteration 82/1000 | Loss: 0.00002160
Iteration 83/1000 | Loss: 0.00002160
Iteration 84/1000 | Loss: 0.00002160
Iteration 85/1000 | Loss: 0.00002160
Iteration 86/1000 | Loss: 0.00002159
Iteration 87/1000 | Loss: 0.00002159
Iteration 88/1000 | Loss: 0.00002159
Iteration 89/1000 | Loss: 0.00002159
Iteration 90/1000 | Loss: 0.00002159
Iteration 91/1000 | Loss: 0.00002158
Iteration 92/1000 | Loss: 0.00002158
Iteration 93/1000 | Loss: 0.00002158
Iteration 94/1000 | Loss: 0.00002158
Iteration 95/1000 | Loss: 0.00002158
Iteration 96/1000 | Loss: 0.00002158
Iteration 97/1000 | Loss: 0.00002158
Iteration 98/1000 | Loss: 0.00002158
Iteration 99/1000 | Loss: 0.00002158
Iteration 100/1000 | Loss: 0.00002158
Iteration 101/1000 | Loss: 0.00002158
Iteration 102/1000 | Loss: 0.00002158
Iteration 103/1000 | Loss: 0.00002157
Iteration 104/1000 | Loss: 0.00002157
Iteration 105/1000 | Loss: 0.00002157
Iteration 106/1000 | Loss: 0.00002157
Iteration 107/1000 | Loss: 0.00002156
Iteration 108/1000 | Loss: 0.00002156
Iteration 109/1000 | Loss: 0.00002156
Iteration 110/1000 | Loss: 0.00002156
Iteration 111/1000 | Loss: 0.00002156
Iteration 112/1000 | Loss: 0.00002156
Iteration 113/1000 | Loss: 0.00002156
Iteration 114/1000 | Loss: 0.00002156
Iteration 115/1000 | Loss: 0.00002156
Iteration 116/1000 | Loss: 0.00002156
Iteration 117/1000 | Loss: 0.00002156
Iteration 118/1000 | Loss: 0.00002156
Iteration 119/1000 | Loss: 0.00002155
Iteration 120/1000 | Loss: 0.00002155
Iteration 121/1000 | Loss: 0.00002155
Iteration 122/1000 | Loss: 0.00002155
Iteration 123/1000 | Loss: 0.00002155
Iteration 124/1000 | Loss: 0.00002155
Iteration 125/1000 | Loss: 0.00002155
Iteration 126/1000 | Loss: 0.00002155
Iteration 127/1000 | Loss: 0.00002155
Iteration 128/1000 | Loss: 0.00002155
Iteration 129/1000 | Loss: 0.00002154
Iteration 130/1000 | Loss: 0.00002154
Iteration 131/1000 | Loss: 0.00002154
Iteration 132/1000 | Loss: 0.00002154
Iteration 133/1000 | Loss: 0.00002154
Iteration 134/1000 | Loss: 0.00002154
Iteration 135/1000 | Loss: 0.00002154
Iteration 136/1000 | Loss: 0.00002154
Iteration 137/1000 | Loss: 0.00002154
Iteration 138/1000 | Loss: 0.00002154
Iteration 139/1000 | Loss: 0.00002154
Iteration 140/1000 | Loss: 0.00002154
Iteration 141/1000 | Loss: 0.00002153
Iteration 142/1000 | Loss: 0.00002153
Iteration 143/1000 | Loss: 0.00002153
Iteration 144/1000 | Loss: 0.00002153
Iteration 145/1000 | Loss: 0.00002153
Iteration 146/1000 | Loss: 0.00002153
Iteration 147/1000 | Loss: 0.00002153
Iteration 148/1000 | Loss: 0.00002153
Iteration 149/1000 | Loss: 0.00002153
Iteration 150/1000 | Loss: 0.00002153
Iteration 151/1000 | Loss: 0.00002153
Iteration 152/1000 | Loss: 0.00002153
Iteration 153/1000 | Loss: 0.00002153
Iteration 154/1000 | Loss: 0.00002152
Iteration 155/1000 | Loss: 0.00002152
Iteration 156/1000 | Loss: 0.00002152
Iteration 157/1000 | Loss: 0.00002152
Iteration 158/1000 | Loss: 0.00002152
Iteration 159/1000 | Loss: 0.00002151
Iteration 160/1000 | Loss: 0.00002151
Iteration 161/1000 | Loss: 0.00002151
Iteration 162/1000 | Loss: 0.00002151
Iteration 163/1000 | Loss: 0.00002151
Iteration 164/1000 | Loss: 0.00002151
Iteration 165/1000 | Loss: 0.00002151
Iteration 166/1000 | Loss: 0.00002151
Iteration 167/1000 | Loss: 0.00002151
Iteration 168/1000 | Loss: 0.00002151
Iteration 169/1000 | Loss: 0.00002151
Iteration 170/1000 | Loss: 0.00002151
Iteration 171/1000 | Loss: 0.00002151
Iteration 172/1000 | Loss: 0.00002150
Iteration 173/1000 | Loss: 0.00002150
Iteration 174/1000 | Loss: 0.00002150
Iteration 175/1000 | Loss: 0.00002150
Iteration 176/1000 | Loss: 0.00002150
Iteration 177/1000 | Loss: 0.00002150
Iteration 178/1000 | Loss: 0.00002150
Iteration 179/1000 | Loss: 0.00002150
Iteration 180/1000 | Loss: 0.00002149
Iteration 181/1000 | Loss: 0.00002149
Iteration 182/1000 | Loss: 0.00002149
Iteration 183/1000 | Loss: 0.00002149
Iteration 184/1000 | Loss: 0.00002149
Iteration 185/1000 | Loss: 0.00002149
Iteration 186/1000 | Loss: 0.00002149
Iteration 187/1000 | Loss: 0.00002149
Iteration 188/1000 | Loss: 0.00002149
Iteration 189/1000 | Loss: 0.00002149
Iteration 190/1000 | Loss: 0.00002148
Iteration 191/1000 | Loss: 0.00002148
Iteration 192/1000 | Loss: 0.00002148
Iteration 193/1000 | Loss: 0.00002148
Iteration 194/1000 | Loss: 0.00002148
Iteration 195/1000 | Loss: 0.00002148
Iteration 196/1000 | Loss: 0.00002147
Iteration 197/1000 | Loss: 0.00002147
Iteration 198/1000 | Loss: 0.00002147
Iteration 199/1000 | Loss: 0.00002147
Iteration 200/1000 | Loss: 0.00002147
Iteration 201/1000 | Loss: 0.00002147
Iteration 202/1000 | Loss: 0.00002147
Iteration 203/1000 | Loss: 0.00002146
Iteration 204/1000 | Loss: 0.00002146
Iteration 205/1000 | Loss: 0.00002146
Iteration 206/1000 | Loss: 0.00002146
Iteration 207/1000 | Loss: 0.00002146
Iteration 208/1000 | Loss: 0.00002146
Iteration 209/1000 | Loss: 0.00002146
Iteration 210/1000 | Loss: 0.00002146
Iteration 211/1000 | Loss: 0.00002146
Iteration 212/1000 | Loss: 0.00002145
Iteration 213/1000 | Loss: 0.00002145
Iteration 214/1000 | Loss: 0.00002145
Iteration 215/1000 | Loss: 0.00002145
Iteration 216/1000 | Loss: 0.00002145
Iteration 217/1000 | Loss: 0.00002145
Iteration 218/1000 | Loss: 0.00002145
Iteration 219/1000 | Loss: 0.00002145
Iteration 220/1000 | Loss: 0.00002145
Iteration 221/1000 | Loss: 0.00002145
Iteration 222/1000 | Loss: 0.00002145
Iteration 223/1000 | Loss: 0.00002145
Iteration 224/1000 | Loss: 0.00002144
Iteration 225/1000 | Loss: 0.00002144
Iteration 226/1000 | Loss: 0.00002144
Iteration 227/1000 | Loss: 0.00002144
Iteration 228/1000 | Loss: 0.00002144
Iteration 229/1000 | Loss: 0.00002144
Iteration 230/1000 | Loss: 0.00002144
Iteration 231/1000 | Loss: 0.00002144
Iteration 232/1000 | Loss: 0.00002144
Iteration 233/1000 | Loss: 0.00002144
Iteration 234/1000 | Loss: 0.00002144
Iteration 235/1000 | Loss: 0.00002144
Iteration 236/1000 | Loss: 0.00002144
Iteration 237/1000 | Loss: 0.00002144
Iteration 238/1000 | Loss: 0.00002143
Iteration 239/1000 | Loss: 0.00002143
Iteration 240/1000 | Loss: 0.00002143
Iteration 241/1000 | Loss: 0.00002143
Iteration 242/1000 | Loss: 0.00002143
Iteration 243/1000 | Loss: 0.00002143
Iteration 244/1000 | Loss: 0.00002143
Iteration 245/1000 | Loss: 0.00002143
Iteration 246/1000 | Loss: 0.00002143
Iteration 247/1000 | Loss: 0.00002143
Iteration 248/1000 | Loss: 0.00002143
Iteration 249/1000 | Loss: 0.00002143
Iteration 250/1000 | Loss: 0.00002143
Iteration 251/1000 | Loss: 0.00002143
Iteration 252/1000 | Loss: 0.00002143
Iteration 253/1000 | Loss: 0.00002143
Iteration 254/1000 | Loss: 0.00002143
Iteration 255/1000 | Loss: 0.00002143
Iteration 256/1000 | Loss: 0.00002143
Iteration 257/1000 | Loss: 0.00002143
Iteration 258/1000 | Loss: 0.00002143
Iteration 259/1000 | Loss: 0.00002143
Iteration 260/1000 | Loss: 0.00002143
Iteration 261/1000 | Loss: 0.00002143
Iteration 262/1000 | Loss: 0.00002143
Iteration 263/1000 | Loss: 0.00002143
Iteration 264/1000 | Loss: 0.00002143
Iteration 265/1000 | Loss: 0.00002143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [2.1429765183711424e-05, 2.1429765183711424e-05, 2.1429765183711424e-05, 2.1429765183711424e-05, 2.1429765183711424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1429765183711424e-05

Optimization complete. Final v2v error: 3.7425119876861572 mm

Highest mean error: 5.698704719543457 mm for frame 57

Lowest mean error: 2.738621950149536 mm for frame 124

Saving results

Total time: 51.40522384643555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922116
Iteration 2/25 | Loss: 0.00155517
Iteration 3/25 | Loss: 0.00097593
Iteration 4/25 | Loss: 0.00087984
Iteration 5/25 | Loss: 0.00082529
Iteration 6/25 | Loss: 0.00085715
Iteration 7/25 | Loss: 0.00082275
Iteration 8/25 | Loss: 0.00078443
Iteration 9/25 | Loss: 0.00074315
Iteration 10/25 | Loss: 0.00072280
Iteration 11/25 | Loss: 0.00071346
Iteration 12/25 | Loss: 0.00071032
Iteration 13/25 | Loss: 0.00070946
Iteration 14/25 | Loss: 0.00070911
Iteration 15/25 | Loss: 0.00070899
Iteration 16/25 | Loss: 0.00070896
Iteration 17/25 | Loss: 0.00070896
Iteration 18/25 | Loss: 0.00070896
Iteration 19/25 | Loss: 0.00070896
Iteration 20/25 | Loss: 0.00070896
Iteration 21/25 | Loss: 0.00070896
Iteration 22/25 | Loss: 0.00070896
Iteration 23/25 | Loss: 0.00070896
Iteration 24/25 | Loss: 0.00070895
Iteration 25/25 | Loss: 0.00070895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51608121
Iteration 2/25 | Loss: 0.00042474
Iteration 3/25 | Loss: 0.00038041
Iteration 4/25 | Loss: 0.00038041
Iteration 5/25 | Loss: 0.00038041
Iteration 6/25 | Loss: 0.00038041
Iteration 7/25 | Loss: 0.00038041
Iteration 8/25 | Loss: 0.00038041
Iteration 9/25 | Loss: 0.00038041
Iteration 10/25 | Loss: 0.00038041
Iteration 11/25 | Loss: 0.00038041
Iteration 12/25 | Loss: 0.00038041
Iteration 13/25 | Loss: 0.00038041
Iteration 14/25 | Loss: 0.00038041
Iteration 15/25 | Loss: 0.00038041
Iteration 16/25 | Loss: 0.00038041
Iteration 17/25 | Loss: 0.00038041
Iteration 18/25 | Loss: 0.00038041
Iteration 19/25 | Loss: 0.00038041
Iteration 20/25 | Loss: 0.00038041
Iteration 21/25 | Loss: 0.00038041
Iteration 22/25 | Loss: 0.00038041
Iteration 23/25 | Loss: 0.00038041
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00038041011430323124, 0.00038041011430323124, 0.00038041011430323124, 0.00038041011430323124, 0.00038041011430323124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038041011430323124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038041
Iteration 2/1000 | Loss: 0.00004763
Iteration 3/1000 | Loss: 0.00008351
Iteration 4/1000 | Loss: 0.00003894
Iteration 5/1000 | Loss: 0.00002833
Iteration 6/1000 | Loss: 0.00002636
Iteration 7/1000 | Loss: 0.00022182
Iteration 8/1000 | Loss: 0.00002526
Iteration 9/1000 | Loss: 0.00002445
Iteration 10/1000 | Loss: 0.00125092
Iteration 11/1000 | Loss: 0.00005856
Iteration 12/1000 | Loss: 0.00002495
Iteration 13/1000 | Loss: 0.00002166
Iteration 14/1000 | Loss: 0.00001986
Iteration 15/1000 | Loss: 0.00001876
Iteration 16/1000 | Loss: 0.00001818
Iteration 17/1000 | Loss: 0.00007383
Iteration 18/1000 | Loss: 0.00001833
Iteration 19/1000 | Loss: 0.00001755
Iteration 20/1000 | Loss: 0.00001752
Iteration 21/1000 | Loss: 0.00001752
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001729
Iteration 25/1000 | Loss: 0.00007382
Iteration 26/1000 | Loss: 0.00001712
Iteration 27/1000 | Loss: 0.00008214
Iteration 28/1000 | Loss: 0.00001733
Iteration 29/1000 | Loss: 0.00001697
Iteration 30/1000 | Loss: 0.00001695
Iteration 31/1000 | Loss: 0.00001695
Iteration 32/1000 | Loss: 0.00001695
Iteration 33/1000 | Loss: 0.00001695
Iteration 34/1000 | Loss: 0.00001695
Iteration 35/1000 | Loss: 0.00001695
Iteration 36/1000 | Loss: 0.00001695
Iteration 37/1000 | Loss: 0.00001694
Iteration 38/1000 | Loss: 0.00001694
Iteration 39/1000 | Loss: 0.00001694
Iteration 40/1000 | Loss: 0.00001694
Iteration 41/1000 | Loss: 0.00001694
Iteration 42/1000 | Loss: 0.00001694
Iteration 43/1000 | Loss: 0.00001694
Iteration 44/1000 | Loss: 0.00001694
Iteration 45/1000 | Loss: 0.00001694
Iteration 46/1000 | Loss: 0.00001694
Iteration 47/1000 | Loss: 0.00001694
Iteration 48/1000 | Loss: 0.00001693
Iteration 49/1000 | Loss: 0.00001692
Iteration 50/1000 | Loss: 0.00001692
Iteration 51/1000 | Loss: 0.00001692
Iteration 52/1000 | Loss: 0.00001692
Iteration 53/1000 | Loss: 0.00001692
Iteration 54/1000 | Loss: 0.00001692
Iteration 55/1000 | Loss: 0.00001692
Iteration 56/1000 | Loss: 0.00001692
Iteration 57/1000 | Loss: 0.00001692
Iteration 58/1000 | Loss: 0.00001692
Iteration 59/1000 | Loss: 0.00001692
Iteration 60/1000 | Loss: 0.00001692
Iteration 61/1000 | Loss: 0.00001692
Iteration 62/1000 | Loss: 0.00001692
Iteration 63/1000 | Loss: 0.00001692
Iteration 64/1000 | Loss: 0.00001692
Iteration 65/1000 | Loss: 0.00001692
Iteration 66/1000 | Loss: 0.00001692
Iteration 67/1000 | Loss: 0.00001692
Iteration 68/1000 | Loss: 0.00001692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.691781471890863e-05, 1.691781471890863e-05, 1.691781471890863e-05, 1.691781471890863e-05, 1.691781471890863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.691781471890863e-05

Optimization complete. Final v2v error: 3.499117612838745 mm

Highest mean error: 4.324944019317627 mm for frame 75

Lowest mean error: 3.1104133129119873 mm for frame 105

Saving results

Total time: 65.35774183273315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999833
Iteration 2/25 | Loss: 0.00229463
Iteration 3/25 | Loss: 0.00171098
Iteration 4/25 | Loss: 0.00163014
Iteration 5/25 | Loss: 0.00156545
Iteration 6/25 | Loss: 0.00144709
Iteration 7/25 | Loss: 0.00135646
Iteration 8/25 | Loss: 0.00127129
Iteration 9/25 | Loss: 0.00125150
Iteration 10/25 | Loss: 0.00121326
Iteration 11/25 | Loss: 0.00118530
Iteration 12/25 | Loss: 0.00116085
Iteration 13/25 | Loss: 0.00112121
Iteration 14/25 | Loss: 0.00111231
Iteration 15/25 | Loss: 0.00112585
Iteration 16/25 | Loss: 0.00110997
Iteration 17/25 | Loss: 0.00109716
Iteration 18/25 | Loss: 0.00109356
Iteration 19/25 | Loss: 0.00109272
Iteration 20/25 | Loss: 0.00109728
Iteration 21/25 | Loss: 0.00108818
Iteration 22/25 | Loss: 0.00108665
Iteration 23/25 | Loss: 0.00108526
Iteration 24/25 | Loss: 0.00109211
Iteration 25/25 | Loss: 0.00108154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48522663
Iteration 2/25 | Loss: 0.00215972
Iteration 3/25 | Loss: 0.00215972
Iteration 4/25 | Loss: 0.00215972
Iteration 5/25 | Loss: 0.00215972
Iteration 6/25 | Loss: 0.00215972
Iteration 7/25 | Loss: 0.00215972
Iteration 8/25 | Loss: 0.00215972
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.0021597244776785374, 0.0021597244776785374, 0.0021597244776785374, 0.0021597244776785374, 0.0021597244776785374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021597244776785374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215972
Iteration 2/1000 | Loss: 0.00040131
Iteration 3/1000 | Loss: 0.00029065
Iteration 4/1000 | Loss: 0.00023595
Iteration 5/1000 | Loss: 0.00020954
Iteration 6/1000 | Loss: 0.00019459
Iteration 7/1000 | Loss: 0.00083227
Iteration 8/1000 | Loss: 0.00137815
Iteration 9/1000 | Loss: 0.00845653
Iteration 10/1000 | Loss: 0.00027202
Iteration 11/1000 | Loss: 0.00015820
Iteration 12/1000 | Loss: 0.00009812
Iteration 13/1000 | Loss: 0.00006658
Iteration 14/1000 | Loss: 0.00004868
Iteration 15/1000 | Loss: 0.00003521
Iteration 16/1000 | Loss: 0.00002895
Iteration 17/1000 | Loss: 0.00002418
Iteration 18/1000 | Loss: 0.00002081
Iteration 19/1000 | Loss: 0.00001827
Iteration 20/1000 | Loss: 0.00001642
Iteration 21/1000 | Loss: 0.00001503
Iteration 22/1000 | Loss: 0.00001391
Iteration 23/1000 | Loss: 0.00001338
Iteration 24/1000 | Loss: 0.00001289
Iteration 25/1000 | Loss: 0.00001253
Iteration 26/1000 | Loss: 0.00001225
Iteration 27/1000 | Loss: 0.00001211
Iteration 28/1000 | Loss: 0.00001200
Iteration 29/1000 | Loss: 0.00001199
Iteration 30/1000 | Loss: 0.00001198
Iteration 31/1000 | Loss: 0.00001197
Iteration 32/1000 | Loss: 0.00001197
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001195
Iteration 36/1000 | Loss: 0.00001195
Iteration 37/1000 | Loss: 0.00001195
Iteration 38/1000 | Loss: 0.00001195
Iteration 39/1000 | Loss: 0.00001194
Iteration 40/1000 | Loss: 0.00001194
Iteration 41/1000 | Loss: 0.00001194
Iteration 42/1000 | Loss: 0.00001193
Iteration 43/1000 | Loss: 0.00001192
Iteration 44/1000 | Loss: 0.00001191
Iteration 45/1000 | Loss: 0.00001191
Iteration 46/1000 | Loss: 0.00001184
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001183
Iteration 49/1000 | Loss: 0.00001183
Iteration 50/1000 | Loss: 0.00001183
Iteration 51/1000 | Loss: 0.00001182
Iteration 52/1000 | Loss: 0.00001182
Iteration 53/1000 | Loss: 0.00001182
Iteration 54/1000 | Loss: 0.00001181
Iteration 55/1000 | Loss: 0.00001181
Iteration 56/1000 | Loss: 0.00001181
Iteration 57/1000 | Loss: 0.00001181
Iteration 58/1000 | Loss: 0.00001180
Iteration 59/1000 | Loss: 0.00001179
Iteration 60/1000 | Loss: 0.00001179
Iteration 61/1000 | Loss: 0.00001179
Iteration 62/1000 | Loss: 0.00001178
Iteration 63/1000 | Loss: 0.00001178
Iteration 64/1000 | Loss: 0.00001177
Iteration 65/1000 | Loss: 0.00001177
Iteration 66/1000 | Loss: 0.00001177
Iteration 67/1000 | Loss: 0.00001177
Iteration 68/1000 | Loss: 0.00001177
Iteration 69/1000 | Loss: 0.00001176
Iteration 70/1000 | Loss: 0.00001176
Iteration 71/1000 | Loss: 0.00001176
Iteration 72/1000 | Loss: 0.00001176
Iteration 73/1000 | Loss: 0.00001176
Iteration 74/1000 | Loss: 0.00001175
Iteration 75/1000 | Loss: 0.00001175
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001175
Iteration 78/1000 | Loss: 0.00001175
Iteration 79/1000 | Loss: 0.00001175
Iteration 80/1000 | Loss: 0.00001175
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001174
Iteration 83/1000 | Loss: 0.00001174
Iteration 84/1000 | Loss: 0.00001174
Iteration 85/1000 | Loss: 0.00001173
Iteration 86/1000 | Loss: 0.00001173
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001171
Iteration 100/1000 | Loss: 0.00001171
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001170
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001169
Iteration 112/1000 | Loss: 0.00001169
Iteration 113/1000 | Loss: 0.00001169
Iteration 114/1000 | Loss: 0.00001168
Iteration 115/1000 | Loss: 0.00001168
Iteration 116/1000 | Loss: 0.00001168
Iteration 117/1000 | Loss: 0.00001168
Iteration 118/1000 | Loss: 0.00001168
Iteration 119/1000 | Loss: 0.00001168
Iteration 120/1000 | Loss: 0.00001167
Iteration 121/1000 | Loss: 0.00001167
Iteration 122/1000 | Loss: 0.00001167
Iteration 123/1000 | Loss: 0.00001167
Iteration 124/1000 | Loss: 0.00001167
Iteration 125/1000 | Loss: 0.00001167
Iteration 126/1000 | Loss: 0.00001167
Iteration 127/1000 | Loss: 0.00001167
Iteration 128/1000 | Loss: 0.00001167
Iteration 129/1000 | Loss: 0.00001167
Iteration 130/1000 | Loss: 0.00001167
Iteration 131/1000 | Loss: 0.00001167
Iteration 132/1000 | Loss: 0.00001167
Iteration 133/1000 | Loss: 0.00001167
Iteration 134/1000 | Loss: 0.00001166
Iteration 135/1000 | Loss: 0.00001166
Iteration 136/1000 | Loss: 0.00001166
Iteration 137/1000 | Loss: 0.00001166
Iteration 138/1000 | Loss: 0.00001166
Iteration 139/1000 | Loss: 0.00001166
Iteration 140/1000 | Loss: 0.00001166
Iteration 141/1000 | Loss: 0.00001166
Iteration 142/1000 | Loss: 0.00001166
Iteration 143/1000 | Loss: 0.00001166
Iteration 144/1000 | Loss: 0.00001166
Iteration 145/1000 | Loss: 0.00001166
Iteration 146/1000 | Loss: 0.00001166
Iteration 147/1000 | Loss: 0.00001166
Iteration 148/1000 | Loss: 0.00001166
Iteration 149/1000 | Loss: 0.00001166
Iteration 150/1000 | Loss: 0.00001166
Iteration 151/1000 | Loss: 0.00001166
Iteration 152/1000 | Loss: 0.00001166
Iteration 153/1000 | Loss: 0.00001166
Iteration 154/1000 | Loss: 0.00001166
Iteration 155/1000 | Loss: 0.00001166
Iteration 156/1000 | Loss: 0.00001166
Iteration 157/1000 | Loss: 0.00001166
Iteration 158/1000 | Loss: 0.00001166
Iteration 159/1000 | Loss: 0.00001166
Iteration 160/1000 | Loss: 0.00001166
Iteration 161/1000 | Loss: 0.00001166
Iteration 162/1000 | Loss: 0.00001166
Iteration 163/1000 | Loss: 0.00001166
Iteration 164/1000 | Loss: 0.00001166
Iteration 165/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.1664405064948369e-05, 1.1664405064948369e-05, 1.1664405064948369e-05, 1.1664405064948369e-05, 1.1664405064948369e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1664405064948369e-05

Optimization complete. Final v2v error: 2.9004220962524414 mm

Highest mean error: 3.065173625946045 mm for frame 45

Lowest mean error: 2.700921058654785 mm for frame 1

Saving results

Total time: 93.2027587890625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485242
Iteration 2/25 | Loss: 0.00083265
Iteration 3/25 | Loss: 0.00070447
Iteration 4/25 | Loss: 0.00067354
Iteration 5/25 | Loss: 0.00066429
Iteration 6/25 | Loss: 0.00066214
Iteration 7/25 | Loss: 0.00066167
Iteration 8/25 | Loss: 0.00066167
Iteration 9/25 | Loss: 0.00066167
Iteration 10/25 | Loss: 0.00066167
Iteration 11/25 | Loss: 0.00066167
Iteration 12/25 | Loss: 0.00066167
Iteration 13/25 | Loss: 0.00066167
Iteration 14/25 | Loss: 0.00066167
Iteration 15/25 | Loss: 0.00066167
Iteration 16/25 | Loss: 0.00066167
Iteration 17/25 | Loss: 0.00066167
Iteration 18/25 | Loss: 0.00066167
Iteration 19/25 | Loss: 0.00066167
Iteration 20/25 | Loss: 0.00066167
Iteration 21/25 | Loss: 0.00066167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000661666679661721, 0.000661666679661721, 0.000661666679661721, 0.000661666679661721, 0.000661666679661721]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000661666679661721

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37541330
Iteration 2/25 | Loss: 0.00026598
Iteration 3/25 | Loss: 0.00026595
Iteration 4/25 | Loss: 0.00026595
Iteration 5/25 | Loss: 0.00026595
Iteration 6/25 | Loss: 0.00026595
Iteration 7/25 | Loss: 0.00026595
Iteration 8/25 | Loss: 0.00026595
Iteration 9/25 | Loss: 0.00026595
Iteration 10/25 | Loss: 0.00026595
Iteration 11/25 | Loss: 0.00026595
Iteration 12/25 | Loss: 0.00026595
Iteration 13/25 | Loss: 0.00026595
Iteration 14/25 | Loss: 0.00026595
Iteration 15/25 | Loss: 0.00026595
Iteration 16/25 | Loss: 0.00026595
Iteration 17/25 | Loss: 0.00026595
Iteration 18/25 | Loss: 0.00026595
Iteration 19/25 | Loss: 0.00026595
Iteration 20/25 | Loss: 0.00026595
Iteration 21/25 | Loss: 0.00026595
Iteration 22/25 | Loss: 0.00026595
Iteration 23/25 | Loss: 0.00026595
Iteration 24/25 | Loss: 0.00026595
Iteration 25/25 | Loss: 0.00026595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026595
Iteration 2/1000 | Loss: 0.00002798
Iteration 3/1000 | Loss: 0.00001664
Iteration 4/1000 | Loss: 0.00001474
Iteration 5/1000 | Loss: 0.00001403
Iteration 6/1000 | Loss: 0.00001356
Iteration 7/1000 | Loss: 0.00001354
Iteration 8/1000 | Loss: 0.00001325
Iteration 9/1000 | Loss: 0.00001302
Iteration 10/1000 | Loss: 0.00001288
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001283
Iteration 13/1000 | Loss: 0.00001281
Iteration 14/1000 | Loss: 0.00001280
Iteration 15/1000 | Loss: 0.00001279
Iteration 16/1000 | Loss: 0.00001278
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001277
Iteration 19/1000 | Loss: 0.00001276
Iteration 20/1000 | Loss: 0.00001276
Iteration 21/1000 | Loss: 0.00001276
Iteration 22/1000 | Loss: 0.00001272
Iteration 23/1000 | Loss: 0.00001271
Iteration 24/1000 | Loss: 0.00001270
Iteration 25/1000 | Loss: 0.00001269
Iteration 26/1000 | Loss: 0.00001269
Iteration 27/1000 | Loss: 0.00001266
Iteration 28/1000 | Loss: 0.00001265
Iteration 29/1000 | Loss: 0.00001264
Iteration 30/1000 | Loss: 0.00001264
Iteration 31/1000 | Loss: 0.00001263
Iteration 32/1000 | Loss: 0.00001263
Iteration 33/1000 | Loss: 0.00001263
Iteration 34/1000 | Loss: 0.00001262
Iteration 35/1000 | Loss: 0.00001262
Iteration 36/1000 | Loss: 0.00001261
Iteration 37/1000 | Loss: 0.00001261
Iteration 38/1000 | Loss: 0.00001260
Iteration 39/1000 | Loss: 0.00001260
Iteration 40/1000 | Loss: 0.00001260
Iteration 41/1000 | Loss: 0.00001260
Iteration 42/1000 | Loss: 0.00001260
Iteration 43/1000 | Loss: 0.00001260
Iteration 44/1000 | Loss: 0.00001260
Iteration 45/1000 | Loss: 0.00001260
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001259
Iteration 48/1000 | Loss: 0.00001259
Iteration 49/1000 | Loss: 0.00001259
Iteration 50/1000 | Loss: 0.00001259
Iteration 51/1000 | Loss: 0.00001259
Iteration 52/1000 | Loss: 0.00001258
Iteration 53/1000 | Loss: 0.00001258
Iteration 54/1000 | Loss: 0.00001257
Iteration 55/1000 | Loss: 0.00001257
Iteration 56/1000 | Loss: 0.00001257
Iteration 57/1000 | Loss: 0.00001256
Iteration 58/1000 | Loss: 0.00001256
Iteration 59/1000 | Loss: 0.00001256
Iteration 60/1000 | Loss: 0.00001256
Iteration 61/1000 | Loss: 0.00001256
Iteration 62/1000 | Loss: 0.00001256
Iteration 63/1000 | Loss: 0.00001255
Iteration 64/1000 | Loss: 0.00001255
Iteration 65/1000 | Loss: 0.00001255
Iteration 66/1000 | Loss: 0.00001254
Iteration 67/1000 | Loss: 0.00001254
Iteration 68/1000 | Loss: 0.00001253
Iteration 69/1000 | Loss: 0.00001253
Iteration 70/1000 | Loss: 0.00001253
Iteration 71/1000 | Loss: 0.00001252
Iteration 72/1000 | Loss: 0.00001252
Iteration 73/1000 | Loss: 0.00001252
Iteration 74/1000 | Loss: 0.00001252
Iteration 75/1000 | Loss: 0.00001252
Iteration 76/1000 | Loss: 0.00001251
Iteration 77/1000 | Loss: 0.00001251
Iteration 78/1000 | Loss: 0.00001251
Iteration 79/1000 | Loss: 0.00001250
Iteration 80/1000 | Loss: 0.00001250
Iteration 81/1000 | Loss: 0.00001249
Iteration 82/1000 | Loss: 0.00001249
Iteration 83/1000 | Loss: 0.00001249
Iteration 84/1000 | Loss: 0.00001249
Iteration 85/1000 | Loss: 0.00001249
Iteration 86/1000 | Loss: 0.00001249
Iteration 87/1000 | Loss: 0.00001249
Iteration 88/1000 | Loss: 0.00001249
Iteration 89/1000 | Loss: 0.00001249
Iteration 90/1000 | Loss: 0.00001249
Iteration 91/1000 | Loss: 0.00001249
Iteration 92/1000 | Loss: 0.00001249
Iteration 93/1000 | Loss: 0.00001249
Iteration 94/1000 | Loss: 0.00001249
Iteration 95/1000 | Loss: 0.00001249
Iteration 96/1000 | Loss: 0.00001248
Iteration 97/1000 | Loss: 0.00001248
Iteration 98/1000 | Loss: 0.00001248
Iteration 99/1000 | Loss: 0.00001248
Iteration 100/1000 | Loss: 0.00001248
Iteration 101/1000 | Loss: 0.00001248
Iteration 102/1000 | Loss: 0.00001248
Iteration 103/1000 | Loss: 0.00001248
Iteration 104/1000 | Loss: 0.00001248
Iteration 105/1000 | Loss: 0.00001248
Iteration 106/1000 | Loss: 0.00001248
Iteration 107/1000 | Loss: 0.00001248
Iteration 108/1000 | Loss: 0.00001247
Iteration 109/1000 | Loss: 0.00001247
Iteration 110/1000 | Loss: 0.00001247
Iteration 111/1000 | Loss: 0.00001247
Iteration 112/1000 | Loss: 0.00001247
Iteration 113/1000 | Loss: 0.00001247
Iteration 114/1000 | Loss: 0.00001247
Iteration 115/1000 | Loss: 0.00001247
Iteration 116/1000 | Loss: 0.00001247
Iteration 117/1000 | Loss: 0.00001247
Iteration 118/1000 | Loss: 0.00001247
Iteration 119/1000 | Loss: 0.00001247
Iteration 120/1000 | Loss: 0.00001247
Iteration 121/1000 | Loss: 0.00001247
Iteration 122/1000 | Loss: 0.00001247
Iteration 123/1000 | Loss: 0.00001247
Iteration 124/1000 | Loss: 0.00001246
Iteration 125/1000 | Loss: 0.00001246
Iteration 126/1000 | Loss: 0.00001246
Iteration 127/1000 | Loss: 0.00001246
Iteration 128/1000 | Loss: 0.00001246
Iteration 129/1000 | Loss: 0.00001246
Iteration 130/1000 | Loss: 0.00001246
Iteration 131/1000 | Loss: 0.00001246
Iteration 132/1000 | Loss: 0.00001246
Iteration 133/1000 | Loss: 0.00001246
Iteration 134/1000 | Loss: 0.00001246
Iteration 135/1000 | Loss: 0.00001246
Iteration 136/1000 | Loss: 0.00001246
Iteration 137/1000 | Loss: 0.00001246
Iteration 138/1000 | Loss: 0.00001246
Iteration 139/1000 | Loss: 0.00001245
Iteration 140/1000 | Loss: 0.00001245
Iteration 141/1000 | Loss: 0.00001245
Iteration 142/1000 | Loss: 0.00001245
Iteration 143/1000 | Loss: 0.00001245
Iteration 144/1000 | Loss: 0.00001245
Iteration 145/1000 | Loss: 0.00001245
Iteration 146/1000 | Loss: 0.00001245
Iteration 147/1000 | Loss: 0.00001245
Iteration 148/1000 | Loss: 0.00001245
Iteration 149/1000 | Loss: 0.00001245
Iteration 150/1000 | Loss: 0.00001245
Iteration 151/1000 | Loss: 0.00001245
Iteration 152/1000 | Loss: 0.00001245
Iteration 153/1000 | Loss: 0.00001245
Iteration 154/1000 | Loss: 0.00001245
Iteration 155/1000 | Loss: 0.00001245
Iteration 156/1000 | Loss: 0.00001245
Iteration 157/1000 | Loss: 0.00001244
Iteration 158/1000 | Loss: 0.00001244
Iteration 159/1000 | Loss: 0.00001244
Iteration 160/1000 | Loss: 0.00001244
Iteration 161/1000 | Loss: 0.00001244
Iteration 162/1000 | Loss: 0.00001243
Iteration 163/1000 | Loss: 0.00001243
Iteration 164/1000 | Loss: 0.00001243
Iteration 165/1000 | Loss: 0.00001243
Iteration 166/1000 | Loss: 0.00001242
Iteration 167/1000 | Loss: 0.00001242
Iteration 168/1000 | Loss: 0.00001242
Iteration 169/1000 | Loss: 0.00001242
Iteration 170/1000 | Loss: 0.00001242
Iteration 171/1000 | Loss: 0.00001242
Iteration 172/1000 | Loss: 0.00001242
Iteration 173/1000 | Loss: 0.00001242
Iteration 174/1000 | Loss: 0.00001242
Iteration 175/1000 | Loss: 0.00001242
Iteration 176/1000 | Loss: 0.00001242
Iteration 177/1000 | Loss: 0.00001241
Iteration 178/1000 | Loss: 0.00001241
Iteration 179/1000 | Loss: 0.00001241
Iteration 180/1000 | Loss: 0.00001241
Iteration 181/1000 | Loss: 0.00001241
Iteration 182/1000 | Loss: 0.00001241
Iteration 183/1000 | Loss: 0.00001241
Iteration 184/1000 | Loss: 0.00001241
Iteration 185/1000 | Loss: 0.00001240
Iteration 186/1000 | Loss: 0.00001240
Iteration 187/1000 | Loss: 0.00001240
Iteration 188/1000 | Loss: 0.00001240
Iteration 189/1000 | Loss: 0.00001240
Iteration 190/1000 | Loss: 0.00001240
Iteration 191/1000 | Loss: 0.00001240
Iteration 192/1000 | Loss: 0.00001240
Iteration 193/1000 | Loss: 0.00001240
Iteration 194/1000 | Loss: 0.00001240
Iteration 195/1000 | Loss: 0.00001240
Iteration 196/1000 | Loss: 0.00001240
Iteration 197/1000 | Loss: 0.00001240
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.240098754351493e-05, 1.240098754351493e-05, 1.240098754351493e-05, 1.240098754351493e-05, 1.240098754351493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.240098754351493e-05

Optimization complete. Final v2v error: 2.9772069454193115 mm

Highest mean error: 3.3455584049224854 mm for frame 64

Lowest mean error: 2.5433168411254883 mm for frame 48

Saving results

Total time: 37.95229387283325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819205
Iteration 2/25 | Loss: 0.00174188
Iteration 3/25 | Loss: 0.00085622
Iteration 4/25 | Loss: 0.00075943
Iteration 5/25 | Loss: 0.00071683
Iteration 6/25 | Loss: 0.00070185
Iteration 7/25 | Loss: 0.00068816
Iteration 8/25 | Loss: 0.00068887
Iteration 9/25 | Loss: 0.00069339
Iteration 10/25 | Loss: 0.00068217
Iteration 11/25 | Loss: 0.00068191
Iteration 12/25 | Loss: 0.00067533
Iteration 13/25 | Loss: 0.00067602
Iteration 14/25 | Loss: 0.00067260
Iteration 15/25 | Loss: 0.00067218
Iteration 16/25 | Loss: 0.00067199
Iteration 17/25 | Loss: 0.00067187
Iteration 18/25 | Loss: 0.00067185
Iteration 19/25 | Loss: 0.00067185
Iteration 20/25 | Loss: 0.00067185
Iteration 21/25 | Loss: 0.00067185
Iteration 22/25 | Loss: 0.00067185
Iteration 23/25 | Loss: 0.00067184
Iteration 24/25 | Loss: 0.00067184
Iteration 25/25 | Loss: 0.00067184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03169417
Iteration 2/25 | Loss: 0.00032424
Iteration 3/25 | Loss: 0.00032424
Iteration 4/25 | Loss: 0.00032424
Iteration 5/25 | Loss: 0.00032424
Iteration 6/25 | Loss: 0.00032424
Iteration 7/25 | Loss: 0.00032424
Iteration 8/25 | Loss: 0.00032424
Iteration 9/25 | Loss: 0.00032424
Iteration 10/25 | Loss: 0.00032424
Iteration 11/25 | Loss: 0.00032424
Iteration 12/25 | Loss: 0.00032424
Iteration 13/25 | Loss: 0.00032424
Iteration 14/25 | Loss: 0.00032424
Iteration 15/25 | Loss: 0.00032424
Iteration 16/25 | Loss: 0.00032424
Iteration 17/25 | Loss: 0.00032424
Iteration 18/25 | Loss: 0.00032424
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00032423590891994536, 0.00032423590891994536, 0.00032423590891994536, 0.00032423590891994536, 0.00032423590891994536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032423590891994536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032424
Iteration 2/1000 | Loss: 0.00002725
Iteration 3/1000 | Loss: 0.00001804
Iteration 4/1000 | Loss: 0.00001705
Iteration 5/1000 | Loss: 0.00001618
Iteration 6/1000 | Loss: 0.00001563
Iteration 7/1000 | Loss: 0.00001520
Iteration 8/1000 | Loss: 0.00001493
Iteration 9/1000 | Loss: 0.00001475
Iteration 10/1000 | Loss: 0.00001473
Iteration 11/1000 | Loss: 0.00001464
Iteration 12/1000 | Loss: 0.00001455
Iteration 13/1000 | Loss: 0.00001455
Iteration 14/1000 | Loss: 0.00001454
Iteration 15/1000 | Loss: 0.00001452
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001450
Iteration 18/1000 | Loss: 0.00001447
Iteration 19/1000 | Loss: 0.00001446
Iteration 20/1000 | Loss: 0.00001441
Iteration 21/1000 | Loss: 0.00001439
Iteration 22/1000 | Loss: 0.00001439
Iteration 23/1000 | Loss: 0.00001434
Iteration 24/1000 | Loss: 0.00001433
Iteration 25/1000 | Loss: 0.00001430
Iteration 26/1000 | Loss: 0.00001430
Iteration 27/1000 | Loss: 0.00001430
Iteration 28/1000 | Loss: 0.00001430
Iteration 29/1000 | Loss: 0.00001429
Iteration 30/1000 | Loss: 0.00001429
Iteration 31/1000 | Loss: 0.00001429
Iteration 32/1000 | Loss: 0.00001429
Iteration 33/1000 | Loss: 0.00001428
Iteration 34/1000 | Loss: 0.00001428
Iteration 35/1000 | Loss: 0.00001428
Iteration 36/1000 | Loss: 0.00001427
Iteration 37/1000 | Loss: 0.00001426
Iteration 38/1000 | Loss: 0.00001426
Iteration 39/1000 | Loss: 0.00001426
Iteration 40/1000 | Loss: 0.00001425
Iteration 41/1000 | Loss: 0.00001424
Iteration 42/1000 | Loss: 0.00001423
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001423
Iteration 45/1000 | Loss: 0.00001422
Iteration 46/1000 | Loss: 0.00001422
Iteration 47/1000 | Loss: 0.00001422
Iteration 48/1000 | Loss: 0.00001422
Iteration 49/1000 | Loss: 0.00001421
Iteration 50/1000 | Loss: 0.00001421
Iteration 51/1000 | Loss: 0.00001420
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001419
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001418
Iteration 56/1000 | Loss: 0.00001418
Iteration 57/1000 | Loss: 0.00001417
Iteration 58/1000 | Loss: 0.00001417
Iteration 59/1000 | Loss: 0.00001415
Iteration 60/1000 | Loss: 0.00001414
Iteration 61/1000 | Loss: 0.00001414
Iteration 62/1000 | Loss: 0.00001414
Iteration 63/1000 | Loss: 0.00001414
Iteration 64/1000 | Loss: 0.00001414
Iteration 65/1000 | Loss: 0.00001413
Iteration 66/1000 | Loss: 0.00001413
Iteration 67/1000 | Loss: 0.00001413
Iteration 68/1000 | Loss: 0.00001413
Iteration 69/1000 | Loss: 0.00001413
Iteration 70/1000 | Loss: 0.00001413
Iteration 71/1000 | Loss: 0.00001413
Iteration 72/1000 | Loss: 0.00001413
Iteration 73/1000 | Loss: 0.00001413
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001413
Iteration 76/1000 | Loss: 0.00001413
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001412
Iteration 80/1000 | Loss: 0.00001412
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001410
Iteration 83/1000 | Loss: 0.00001410
Iteration 84/1000 | Loss: 0.00001410
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001409
Iteration 89/1000 | Loss: 0.00001409
Iteration 90/1000 | Loss: 0.00001409
Iteration 91/1000 | Loss: 0.00001409
Iteration 92/1000 | Loss: 0.00001409
Iteration 93/1000 | Loss: 0.00001408
Iteration 94/1000 | Loss: 0.00001408
Iteration 95/1000 | Loss: 0.00001408
Iteration 96/1000 | Loss: 0.00001408
Iteration 97/1000 | Loss: 0.00001408
Iteration 98/1000 | Loss: 0.00001408
Iteration 99/1000 | Loss: 0.00001408
Iteration 100/1000 | Loss: 0.00001408
Iteration 101/1000 | Loss: 0.00001408
Iteration 102/1000 | Loss: 0.00001408
Iteration 103/1000 | Loss: 0.00001408
Iteration 104/1000 | Loss: 0.00001408
Iteration 105/1000 | Loss: 0.00001408
Iteration 106/1000 | Loss: 0.00001408
Iteration 107/1000 | Loss: 0.00001408
Iteration 108/1000 | Loss: 0.00001408
Iteration 109/1000 | Loss: 0.00001408
Iteration 110/1000 | Loss: 0.00001408
Iteration 111/1000 | Loss: 0.00001407
Iteration 112/1000 | Loss: 0.00001407
Iteration 113/1000 | Loss: 0.00001407
Iteration 114/1000 | Loss: 0.00001407
Iteration 115/1000 | Loss: 0.00001407
Iteration 116/1000 | Loss: 0.00001407
Iteration 117/1000 | Loss: 0.00001407
Iteration 118/1000 | Loss: 0.00001407
Iteration 119/1000 | Loss: 0.00001406
Iteration 120/1000 | Loss: 0.00001406
Iteration 121/1000 | Loss: 0.00001406
Iteration 122/1000 | Loss: 0.00001406
Iteration 123/1000 | Loss: 0.00001406
Iteration 124/1000 | Loss: 0.00001406
Iteration 125/1000 | Loss: 0.00001406
Iteration 126/1000 | Loss: 0.00001406
Iteration 127/1000 | Loss: 0.00001406
Iteration 128/1000 | Loss: 0.00001406
Iteration 129/1000 | Loss: 0.00001406
Iteration 130/1000 | Loss: 0.00001406
Iteration 131/1000 | Loss: 0.00001406
Iteration 132/1000 | Loss: 0.00001406
Iteration 133/1000 | Loss: 0.00001406
Iteration 134/1000 | Loss: 0.00001406
Iteration 135/1000 | Loss: 0.00001406
Iteration 136/1000 | Loss: 0.00001406
Iteration 137/1000 | Loss: 0.00001405
Iteration 138/1000 | Loss: 0.00001405
Iteration 139/1000 | Loss: 0.00001405
Iteration 140/1000 | Loss: 0.00001405
Iteration 141/1000 | Loss: 0.00001405
Iteration 142/1000 | Loss: 0.00001405
Iteration 143/1000 | Loss: 0.00001405
Iteration 144/1000 | Loss: 0.00001405
Iteration 145/1000 | Loss: 0.00001405
Iteration 146/1000 | Loss: 0.00001405
Iteration 147/1000 | Loss: 0.00001405
Iteration 148/1000 | Loss: 0.00001405
Iteration 149/1000 | Loss: 0.00001405
Iteration 150/1000 | Loss: 0.00001405
Iteration 151/1000 | Loss: 0.00001405
Iteration 152/1000 | Loss: 0.00001405
Iteration 153/1000 | Loss: 0.00001405
Iteration 154/1000 | Loss: 0.00001405
Iteration 155/1000 | Loss: 0.00001405
Iteration 156/1000 | Loss: 0.00001405
Iteration 157/1000 | Loss: 0.00001405
Iteration 158/1000 | Loss: 0.00001405
Iteration 159/1000 | Loss: 0.00001405
Iteration 160/1000 | Loss: 0.00001405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.4049423953110818e-05, 1.4049423953110818e-05, 1.4049423953110818e-05, 1.4049423953110818e-05, 1.4049423953110818e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4049423953110818e-05

Optimization complete. Final v2v error: 3.183908700942993 mm

Highest mean error: 3.5550355911254883 mm for frame 115

Lowest mean error: 3.0030226707458496 mm for frame 77

Saving results

Total time: 59.59662389755249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824472
Iteration 2/25 | Loss: 0.00117635
Iteration 3/25 | Loss: 0.00084318
Iteration 4/25 | Loss: 0.00079428
Iteration 5/25 | Loss: 0.00077431
Iteration 6/25 | Loss: 0.00077076
Iteration 7/25 | Loss: 0.00077005
Iteration 8/25 | Loss: 0.00077003
Iteration 9/25 | Loss: 0.00077003
Iteration 10/25 | Loss: 0.00077003
Iteration 11/25 | Loss: 0.00077003
Iteration 12/25 | Loss: 0.00077003
Iteration 13/25 | Loss: 0.00077003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007700270507484674, 0.0007700270507484674, 0.0007700270507484674, 0.0007700270507484674, 0.0007700270507484674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007700270507484674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35392535
Iteration 2/25 | Loss: 0.00037835
Iteration 3/25 | Loss: 0.00037829
Iteration 4/25 | Loss: 0.00037829
Iteration 5/25 | Loss: 0.00037829
Iteration 6/25 | Loss: 0.00037829
Iteration 7/25 | Loss: 0.00037829
Iteration 8/25 | Loss: 0.00037829
Iteration 9/25 | Loss: 0.00037829
Iteration 10/25 | Loss: 0.00037829
Iteration 11/25 | Loss: 0.00037829
Iteration 12/25 | Loss: 0.00037829
Iteration 13/25 | Loss: 0.00037829
Iteration 14/25 | Loss: 0.00037829
Iteration 15/25 | Loss: 0.00037829
Iteration 16/25 | Loss: 0.00037829
Iteration 17/25 | Loss: 0.00037829
Iteration 18/25 | Loss: 0.00037829
Iteration 19/25 | Loss: 0.00037829
Iteration 20/25 | Loss: 0.00037829
Iteration 21/25 | Loss: 0.00037829
Iteration 22/25 | Loss: 0.00037829
Iteration 23/25 | Loss: 0.00037829
Iteration 24/25 | Loss: 0.00037829
Iteration 25/25 | Loss: 0.00037829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037829
Iteration 2/1000 | Loss: 0.00004023
Iteration 3/1000 | Loss: 0.00003083
Iteration 4/1000 | Loss: 0.00002727
Iteration 5/1000 | Loss: 0.00002599
Iteration 6/1000 | Loss: 0.00002452
Iteration 7/1000 | Loss: 0.00002364
Iteration 8/1000 | Loss: 0.00002309
Iteration 9/1000 | Loss: 0.00002266
Iteration 10/1000 | Loss: 0.00002242
Iteration 11/1000 | Loss: 0.00002229
Iteration 12/1000 | Loss: 0.00002220
Iteration 13/1000 | Loss: 0.00002204
Iteration 14/1000 | Loss: 0.00002199
Iteration 15/1000 | Loss: 0.00002198
Iteration 16/1000 | Loss: 0.00002198
Iteration 17/1000 | Loss: 0.00002196
Iteration 18/1000 | Loss: 0.00002196
Iteration 19/1000 | Loss: 0.00002195
Iteration 20/1000 | Loss: 0.00002191
Iteration 21/1000 | Loss: 0.00002189
Iteration 22/1000 | Loss: 0.00002189
Iteration 23/1000 | Loss: 0.00002188
Iteration 24/1000 | Loss: 0.00002188
Iteration 25/1000 | Loss: 0.00002188
Iteration 26/1000 | Loss: 0.00002187
Iteration 27/1000 | Loss: 0.00002187
Iteration 28/1000 | Loss: 0.00002186
Iteration 29/1000 | Loss: 0.00002186
Iteration 30/1000 | Loss: 0.00002186
Iteration 31/1000 | Loss: 0.00002185
Iteration 32/1000 | Loss: 0.00002185
Iteration 33/1000 | Loss: 0.00002184
Iteration 34/1000 | Loss: 0.00002184
Iteration 35/1000 | Loss: 0.00002184
Iteration 36/1000 | Loss: 0.00002183
Iteration 37/1000 | Loss: 0.00002183
Iteration 38/1000 | Loss: 0.00002182
Iteration 39/1000 | Loss: 0.00002182
Iteration 40/1000 | Loss: 0.00002182
Iteration 41/1000 | Loss: 0.00002182
Iteration 42/1000 | Loss: 0.00002181
Iteration 43/1000 | Loss: 0.00002181
Iteration 44/1000 | Loss: 0.00002181
Iteration 45/1000 | Loss: 0.00002181
Iteration 46/1000 | Loss: 0.00002180
Iteration 47/1000 | Loss: 0.00002180
Iteration 48/1000 | Loss: 0.00002180
Iteration 49/1000 | Loss: 0.00002180
Iteration 50/1000 | Loss: 0.00002180
Iteration 51/1000 | Loss: 0.00002180
Iteration 52/1000 | Loss: 0.00002179
Iteration 53/1000 | Loss: 0.00002179
Iteration 54/1000 | Loss: 0.00002179
Iteration 55/1000 | Loss: 0.00002179
Iteration 56/1000 | Loss: 0.00002179
Iteration 57/1000 | Loss: 0.00002179
Iteration 58/1000 | Loss: 0.00002179
Iteration 59/1000 | Loss: 0.00002178
Iteration 60/1000 | Loss: 0.00002178
Iteration 61/1000 | Loss: 0.00002178
Iteration 62/1000 | Loss: 0.00002178
Iteration 63/1000 | Loss: 0.00002178
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00002178
Iteration 66/1000 | Loss: 0.00002178
Iteration 67/1000 | Loss: 0.00002178
Iteration 68/1000 | Loss: 0.00002178
Iteration 69/1000 | Loss: 0.00002177
Iteration 70/1000 | Loss: 0.00002177
Iteration 71/1000 | Loss: 0.00002177
Iteration 72/1000 | Loss: 0.00002177
Iteration 73/1000 | Loss: 0.00002177
Iteration 74/1000 | Loss: 0.00002177
Iteration 75/1000 | Loss: 0.00002177
Iteration 76/1000 | Loss: 0.00002177
Iteration 77/1000 | Loss: 0.00002176
Iteration 78/1000 | Loss: 0.00002176
Iteration 79/1000 | Loss: 0.00002176
Iteration 80/1000 | Loss: 0.00002176
Iteration 81/1000 | Loss: 0.00002176
Iteration 82/1000 | Loss: 0.00002176
Iteration 83/1000 | Loss: 0.00002176
Iteration 84/1000 | Loss: 0.00002175
Iteration 85/1000 | Loss: 0.00002175
Iteration 86/1000 | Loss: 0.00002175
Iteration 87/1000 | Loss: 0.00002175
Iteration 88/1000 | Loss: 0.00002175
Iteration 89/1000 | Loss: 0.00002175
Iteration 90/1000 | Loss: 0.00002175
Iteration 91/1000 | Loss: 0.00002174
Iteration 92/1000 | Loss: 0.00002174
Iteration 93/1000 | Loss: 0.00002174
Iteration 94/1000 | Loss: 0.00002174
Iteration 95/1000 | Loss: 0.00002173
Iteration 96/1000 | Loss: 0.00002173
Iteration 97/1000 | Loss: 0.00002173
Iteration 98/1000 | Loss: 0.00002173
Iteration 99/1000 | Loss: 0.00002173
Iteration 100/1000 | Loss: 0.00002173
Iteration 101/1000 | Loss: 0.00002173
Iteration 102/1000 | Loss: 0.00002173
Iteration 103/1000 | Loss: 0.00002173
Iteration 104/1000 | Loss: 0.00002172
Iteration 105/1000 | Loss: 0.00002172
Iteration 106/1000 | Loss: 0.00002172
Iteration 107/1000 | Loss: 0.00002172
Iteration 108/1000 | Loss: 0.00002172
Iteration 109/1000 | Loss: 0.00002172
Iteration 110/1000 | Loss: 0.00002172
Iteration 111/1000 | Loss: 0.00002172
Iteration 112/1000 | Loss: 0.00002171
Iteration 113/1000 | Loss: 0.00002171
Iteration 114/1000 | Loss: 0.00002171
Iteration 115/1000 | Loss: 0.00002171
Iteration 116/1000 | Loss: 0.00002171
Iteration 117/1000 | Loss: 0.00002171
Iteration 118/1000 | Loss: 0.00002171
Iteration 119/1000 | Loss: 0.00002171
Iteration 120/1000 | Loss: 0.00002171
Iteration 121/1000 | Loss: 0.00002170
Iteration 122/1000 | Loss: 0.00002170
Iteration 123/1000 | Loss: 0.00002170
Iteration 124/1000 | Loss: 0.00002170
Iteration 125/1000 | Loss: 0.00002170
Iteration 126/1000 | Loss: 0.00002170
Iteration 127/1000 | Loss: 0.00002170
Iteration 128/1000 | Loss: 0.00002170
Iteration 129/1000 | Loss: 0.00002170
Iteration 130/1000 | Loss: 0.00002170
Iteration 131/1000 | Loss: 0.00002170
Iteration 132/1000 | Loss: 0.00002170
Iteration 133/1000 | Loss: 0.00002170
Iteration 134/1000 | Loss: 0.00002170
Iteration 135/1000 | Loss: 0.00002170
Iteration 136/1000 | Loss: 0.00002170
Iteration 137/1000 | Loss: 0.00002170
Iteration 138/1000 | Loss: 0.00002170
Iteration 139/1000 | Loss: 0.00002170
Iteration 140/1000 | Loss: 0.00002170
Iteration 141/1000 | Loss: 0.00002170
Iteration 142/1000 | Loss: 0.00002170
Iteration 143/1000 | Loss: 0.00002170
Iteration 144/1000 | Loss: 0.00002170
Iteration 145/1000 | Loss: 0.00002170
Iteration 146/1000 | Loss: 0.00002170
Iteration 147/1000 | Loss: 0.00002170
Iteration 148/1000 | Loss: 0.00002170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.1701829609810375e-05, 2.1701829609810375e-05, 2.1701829609810375e-05, 2.1701829609810375e-05, 2.1701829609810375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1701829609810375e-05

Optimization complete. Final v2v error: 3.9033312797546387 mm

Highest mean error: 5.316985607147217 mm for frame 53

Lowest mean error: 3.1311793327331543 mm for frame 81

Saving results

Total time: 38.34620666503906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788155
Iteration 2/25 | Loss: 0.00096372
Iteration 3/25 | Loss: 0.00076729
Iteration 4/25 | Loss: 0.00072869
Iteration 5/25 | Loss: 0.00072168
Iteration 6/25 | Loss: 0.00071982
Iteration 7/25 | Loss: 0.00071961
Iteration 8/25 | Loss: 0.00071961
Iteration 9/25 | Loss: 0.00071961
Iteration 10/25 | Loss: 0.00071961
Iteration 11/25 | Loss: 0.00071961
Iteration 12/25 | Loss: 0.00071961
Iteration 13/25 | Loss: 0.00071961
Iteration 14/25 | Loss: 0.00071961
Iteration 15/25 | Loss: 0.00071961
Iteration 16/25 | Loss: 0.00071961
Iteration 17/25 | Loss: 0.00071961
Iteration 18/25 | Loss: 0.00071961
Iteration 19/25 | Loss: 0.00071961
Iteration 20/25 | Loss: 0.00071961
Iteration 21/25 | Loss: 0.00071961
Iteration 22/25 | Loss: 0.00071961
Iteration 23/25 | Loss: 0.00071961
Iteration 24/25 | Loss: 0.00071961
Iteration 25/25 | Loss: 0.00071961

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.74185276
Iteration 2/25 | Loss: 0.00036843
Iteration 3/25 | Loss: 0.00036839
Iteration 4/25 | Loss: 0.00036839
Iteration 5/25 | Loss: 0.00036839
Iteration 6/25 | Loss: 0.00036839
Iteration 7/25 | Loss: 0.00036839
Iteration 8/25 | Loss: 0.00036839
Iteration 9/25 | Loss: 0.00036839
Iteration 10/25 | Loss: 0.00036839
Iteration 11/25 | Loss: 0.00036839
Iteration 12/25 | Loss: 0.00036839
Iteration 13/25 | Loss: 0.00036839
Iteration 14/25 | Loss: 0.00036839
Iteration 15/25 | Loss: 0.00036839
Iteration 16/25 | Loss: 0.00036839
Iteration 17/25 | Loss: 0.00036839
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003683902614284307, 0.0003683902614284307, 0.0003683902614284307, 0.0003683902614284307, 0.0003683902614284307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003683902614284307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036839
Iteration 2/1000 | Loss: 0.00002553
Iteration 3/1000 | Loss: 0.00001784
Iteration 4/1000 | Loss: 0.00001638
Iteration 5/1000 | Loss: 0.00001555
Iteration 6/1000 | Loss: 0.00001505
Iteration 7/1000 | Loss: 0.00001481
Iteration 8/1000 | Loss: 0.00001460
Iteration 9/1000 | Loss: 0.00001458
Iteration 10/1000 | Loss: 0.00001450
Iteration 11/1000 | Loss: 0.00001450
Iteration 12/1000 | Loss: 0.00001444
Iteration 13/1000 | Loss: 0.00001440
Iteration 14/1000 | Loss: 0.00001439
Iteration 15/1000 | Loss: 0.00001438
Iteration 16/1000 | Loss: 0.00001433
Iteration 17/1000 | Loss: 0.00001433
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001432
Iteration 20/1000 | Loss: 0.00001429
Iteration 21/1000 | Loss: 0.00001428
Iteration 22/1000 | Loss: 0.00001427
Iteration 23/1000 | Loss: 0.00001427
Iteration 24/1000 | Loss: 0.00001427
Iteration 25/1000 | Loss: 0.00001427
Iteration 26/1000 | Loss: 0.00001426
Iteration 27/1000 | Loss: 0.00001426
Iteration 28/1000 | Loss: 0.00001425
Iteration 29/1000 | Loss: 0.00001425
Iteration 30/1000 | Loss: 0.00001425
Iteration 31/1000 | Loss: 0.00001424
Iteration 32/1000 | Loss: 0.00001424
Iteration 33/1000 | Loss: 0.00001424
Iteration 34/1000 | Loss: 0.00001424
Iteration 35/1000 | Loss: 0.00001424
Iteration 36/1000 | Loss: 0.00001424
Iteration 37/1000 | Loss: 0.00001423
Iteration 38/1000 | Loss: 0.00001423
Iteration 39/1000 | Loss: 0.00001423
Iteration 40/1000 | Loss: 0.00001423
Iteration 41/1000 | Loss: 0.00001423
Iteration 42/1000 | Loss: 0.00001423
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001423
Iteration 45/1000 | Loss: 0.00001422
Iteration 46/1000 | Loss: 0.00001422
Iteration 47/1000 | Loss: 0.00001422
Iteration 48/1000 | Loss: 0.00001422
Iteration 49/1000 | Loss: 0.00001422
Iteration 50/1000 | Loss: 0.00001422
Iteration 51/1000 | Loss: 0.00001422
Iteration 52/1000 | Loss: 0.00001422
Iteration 53/1000 | Loss: 0.00001421
Iteration 54/1000 | Loss: 0.00001421
Iteration 55/1000 | Loss: 0.00001421
Iteration 56/1000 | Loss: 0.00001421
Iteration 57/1000 | Loss: 0.00001421
Iteration 58/1000 | Loss: 0.00001420
Iteration 59/1000 | Loss: 0.00001420
Iteration 60/1000 | Loss: 0.00001420
Iteration 61/1000 | Loss: 0.00001420
Iteration 62/1000 | Loss: 0.00001420
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001419
Iteration 67/1000 | Loss: 0.00001419
Iteration 68/1000 | Loss: 0.00001419
Iteration 69/1000 | Loss: 0.00001418
Iteration 70/1000 | Loss: 0.00001418
Iteration 71/1000 | Loss: 0.00001418
Iteration 72/1000 | Loss: 0.00001418
Iteration 73/1000 | Loss: 0.00001418
Iteration 74/1000 | Loss: 0.00001418
Iteration 75/1000 | Loss: 0.00001418
Iteration 76/1000 | Loss: 0.00001417
Iteration 77/1000 | Loss: 0.00001417
Iteration 78/1000 | Loss: 0.00001417
Iteration 79/1000 | Loss: 0.00001417
Iteration 80/1000 | Loss: 0.00001417
Iteration 81/1000 | Loss: 0.00001417
Iteration 82/1000 | Loss: 0.00001416
Iteration 83/1000 | Loss: 0.00001416
Iteration 84/1000 | Loss: 0.00001416
Iteration 85/1000 | Loss: 0.00001416
Iteration 86/1000 | Loss: 0.00001416
Iteration 87/1000 | Loss: 0.00001415
Iteration 88/1000 | Loss: 0.00001415
Iteration 89/1000 | Loss: 0.00001415
Iteration 90/1000 | Loss: 0.00001415
Iteration 91/1000 | Loss: 0.00001414
Iteration 92/1000 | Loss: 0.00001414
Iteration 93/1000 | Loss: 0.00001414
Iteration 94/1000 | Loss: 0.00001414
Iteration 95/1000 | Loss: 0.00001414
Iteration 96/1000 | Loss: 0.00001414
Iteration 97/1000 | Loss: 0.00001414
Iteration 98/1000 | Loss: 0.00001413
Iteration 99/1000 | Loss: 0.00001413
Iteration 100/1000 | Loss: 0.00001413
Iteration 101/1000 | Loss: 0.00001413
Iteration 102/1000 | Loss: 0.00001413
Iteration 103/1000 | Loss: 0.00001413
Iteration 104/1000 | Loss: 0.00001413
Iteration 105/1000 | Loss: 0.00001412
Iteration 106/1000 | Loss: 0.00001412
Iteration 107/1000 | Loss: 0.00001412
Iteration 108/1000 | Loss: 0.00001412
Iteration 109/1000 | Loss: 0.00001412
Iteration 110/1000 | Loss: 0.00001412
Iteration 111/1000 | Loss: 0.00001412
Iteration 112/1000 | Loss: 0.00001412
Iteration 113/1000 | Loss: 0.00001412
Iteration 114/1000 | Loss: 0.00001412
Iteration 115/1000 | Loss: 0.00001412
Iteration 116/1000 | Loss: 0.00001412
Iteration 117/1000 | Loss: 0.00001412
Iteration 118/1000 | Loss: 0.00001412
Iteration 119/1000 | Loss: 0.00001412
Iteration 120/1000 | Loss: 0.00001412
Iteration 121/1000 | Loss: 0.00001412
Iteration 122/1000 | Loss: 0.00001412
Iteration 123/1000 | Loss: 0.00001412
Iteration 124/1000 | Loss: 0.00001412
Iteration 125/1000 | Loss: 0.00001412
Iteration 126/1000 | Loss: 0.00001412
Iteration 127/1000 | Loss: 0.00001412
Iteration 128/1000 | Loss: 0.00001412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.4115279554971494e-05, 1.4115279554971494e-05, 1.4115279554971494e-05, 1.4115279554971494e-05, 1.4115279554971494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4115279554971494e-05

Optimization complete. Final v2v error: 3.2025363445281982 mm

Highest mean error: 3.5467991828918457 mm for frame 99

Lowest mean error: 2.9008915424346924 mm for frame 207

Saving results

Total time: 36.91665959358215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771792
Iteration 2/25 | Loss: 0.00138034
Iteration 3/25 | Loss: 0.00084803
Iteration 4/25 | Loss: 0.00083575
Iteration 5/25 | Loss: 0.00072052
Iteration 6/25 | Loss: 0.00069848
Iteration 7/25 | Loss: 0.00068673
Iteration 8/25 | Loss: 0.00069136
Iteration 9/25 | Loss: 0.00068316
Iteration 10/25 | Loss: 0.00068258
Iteration 11/25 | Loss: 0.00069613
Iteration 12/25 | Loss: 0.00068210
Iteration 13/25 | Loss: 0.00068127
Iteration 14/25 | Loss: 0.00068078
Iteration 15/25 | Loss: 0.00068056
Iteration 16/25 | Loss: 0.00068045
Iteration 17/25 | Loss: 0.00068044
Iteration 18/25 | Loss: 0.00068044
Iteration 19/25 | Loss: 0.00068044
Iteration 20/25 | Loss: 0.00068044
Iteration 21/25 | Loss: 0.00068044
Iteration 22/25 | Loss: 0.00068044
Iteration 23/25 | Loss: 0.00068043
Iteration 24/25 | Loss: 0.00068043
Iteration 25/25 | Loss: 0.00068043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97276235
Iteration 2/25 | Loss: 0.00037313
Iteration 3/25 | Loss: 0.00037313
Iteration 4/25 | Loss: 0.00037313
Iteration 5/25 | Loss: 0.00037313
Iteration 6/25 | Loss: 0.00037313
Iteration 7/25 | Loss: 0.00037313
Iteration 8/25 | Loss: 0.00037313
Iteration 9/25 | Loss: 0.00037313
Iteration 10/25 | Loss: 0.00037313
Iteration 11/25 | Loss: 0.00037313
Iteration 12/25 | Loss: 0.00037313
Iteration 13/25 | Loss: 0.00037313
Iteration 14/25 | Loss: 0.00037313
Iteration 15/25 | Loss: 0.00037313
Iteration 16/25 | Loss: 0.00037313
Iteration 17/25 | Loss: 0.00037313
Iteration 18/25 | Loss: 0.00037313
Iteration 19/25 | Loss: 0.00037313
Iteration 20/25 | Loss: 0.00037313
Iteration 21/25 | Loss: 0.00037313
Iteration 22/25 | Loss: 0.00037313
Iteration 23/25 | Loss: 0.00037313
Iteration 24/25 | Loss: 0.00037313
Iteration 25/25 | Loss: 0.00037313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037313
Iteration 2/1000 | Loss: 0.00002147
Iteration 3/1000 | Loss: 0.00001626
Iteration 4/1000 | Loss: 0.00001541
Iteration 5/1000 | Loss: 0.00001479
Iteration 6/1000 | Loss: 0.00001448
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001407
Iteration 9/1000 | Loss: 0.00001387
Iteration 10/1000 | Loss: 0.00001377
Iteration 11/1000 | Loss: 0.00001367
Iteration 12/1000 | Loss: 0.00001365
Iteration 13/1000 | Loss: 0.00001365
Iteration 14/1000 | Loss: 0.00001364
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001355
Iteration 17/1000 | Loss: 0.00001354
Iteration 18/1000 | Loss: 0.00001354
Iteration 19/1000 | Loss: 0.00001353
Iteration 20/1000 | Loss: 0.00001353
Iteration 21/1000 | Loss: 0.00001352
Iteration 22/1000 | Loss: 0.00001352
Iteration 23/1000 | Loss: 0.00001352
Iteration 24/1000 | Loss: 0.00001351
Iteration 25/1000 | Loss: 0.00001351
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001350
Iteration 28/1000 | Loss: 0.00001350
Iteration 29/1000 | Loss: 0.00001350
Iteration 30/1000 | Loss: 0.00001349
Iteration 31/1000 | Loss: 0.00001348
Iteration 32/1000 | Loss: 0.00001348
Iteration 33/1000 | Loss: 0.00001348
Iteration 34/1000 | Loss: 0.00001348
Iteration 35/1000 | Loss: 0.00001347
Iteration 36/1000 | Loss: 0.00001347
Iteration 37/1000 | Loss: 0.00001346
Iteration 38/1000 | Loss: 0.00001346
Iteration 39/1000 | Loss: 0.00001346
Iteration 40/1000 | Loss: 0.00001345
Iteration 41/1000 | Loss: 0.00001344
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001342
Iteration 48/1000 | Loss: 0.00001342
Iteration 49/1000 | Loss: 0.00001342
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001341
Iteration 52/1000 | Loss: 0.00001341
Iteration 53/1000 | Loss: 0.00001340
Iteration 54/1000 | Loss: 0.00001340
Iteration 55/1000 | Loss: 0.00001339
Iteration 56/1000 | Loss: 0.00001339
Iteration 57/1000 | Loss: 0.00001339
Iteration 58/1000 | Loss: 0.00001339
Iteration 59/1000 | Loss: 0.00001338
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001337
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001336
Iteration 65/1000 | Loss: 0.00001336
Iteration 66/1000 | Loss: 0.00001336
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001335
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001332
Iteration 77/1000 | Loss: 0.00001332
Iteration 78/1000 | Loss: 0.00001332
Iteration 79/1000 | Loss: 0.00001332
Iteration 80/1000 | Loss: 0.00001332
Iteration 81/1000 | Loss: 0.00001332
Iteration 82/1000 | Loss: 0.00001332
Iteration 83/1000 | Loss: 0.00001332
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001331
Iteration 89/1000 | Loss: 0.00001331
Iteration 90/1000 | Loss: 0.00001331
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001330
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00001328
Iteration 98/1000 | Loss: 0.00001328
Iteration 99/1000 | Loss: 0.00001328
Iteration 100/1000 | Loss: 0.00001328
Iteration 101/1000 | Loss: 0.00001328
Iteration 102/1000 | Loss: 0.00001328
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001327
Iteration 105/1000 | Loss: 0.00001327
Iteration 106/1000 | Loss: 0.00001327
Iteration 107/1000 | Loss: 0.00001327
Iteration 108/1000 | Loss: 0.00001327
Iteration 109/1000 | Loss: 0.00001327
Iteration 110/1000 | Loss: 0.00001326
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001326
Iteration 114/1000 | Loss: 0.00001326
Iteration 115/1000 | Loss: 0.00001326
Iteration 116/1000 | Loss: 0.00001326
Iteration 117/1000 | Loss: 0.00001326
Iteration 118/1000 | Loss: 0.00001326
Iteration 119/1000 | Loss: 0.00001326
Iteration 120/1000 | Loss: 0.00001326
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001325
Iteration 125/1000 | Loss: 0.00001325
Iteration 126/1000 | Loss: 0.00001325
Iteration 127/1000 | Loss: 0.00001325
Iteration 128/1000 | Loss: 0.00001325
Iteration 129/1000 | Loss: 0.00001324
Iteration 130/1000 | Loss: 0.00001324
Iteration 131/1000 | Loss: 0.00001324
Iteration 132/1000 | Loss: 0.00001324
Iteration 133/1000 | Loss: 0.00001324
Iteration 134/1000 | Loss: 0.00001324
Iteration 135/1000 | Loss: 0.00001324
Iteration 136/1000 | Loss: 0.00001324
Iteration 137/1000 | Loss: 0.00001324
Iteration 138/1000 | Loss: 0.00001324
Iteration 139/1000 | Loss: 0.00001324
Iteration 140/1000 | Loss: 0.00001324
Iteration 141/1000 | Loss: 0.00001324
Iteration 142/1000 | Loss: 0.00001324
Iteration 143/1000 | Loss: 0.00001324
Iteration 144/1000 | Loss: 0.00001324
Iteration 145/1000 | Loss: 0.00001324
Iteration 146/1000 | Loss: 0.00001324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.3237257917353418e-05, 1.3237257917353418e-05, 1.3237257917353418e-05, 1.3237257917353418e-05, 1.3237257917353418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3237257917353418e-05

Optimization complete. Final v2v error: 3.0419459342956543 mm

Highest mean error: 3.431347131729126 mm for frame 63

Lowest mean error: 2.757112741470337 mm for frame 187

Saving results

Total time: 56.222756147384644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431563
Iteration 2/25 | Loss: 0.00091828
Iteration 3/25 | Loss: 0.00078083
Iteration 4/25 | Loss: 0.00074064
Iteration 5/25 | Loss: 0.00073125
Iteration 6/25 | Loss: 0.00072932
Iteration 7/25 | Loss: 0.00072898
Iteration 8/25 | Loss: 0.00072898
Iteration 9/25 | Loss: 0.00072898
Iteration 10/25 | Loss: 0.00072898
Iteration 11/25 | Loss: 0.00072898
Iteration 12/25 | Loss: 0.00072898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007289783679880202, 0.0007289783679880202, 0.0007289783679880202, 0.0007289783679880202, 0.0007289783679880202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007289783679880202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49767804
Iteration 2/25 | Loss: 0.00041358
Iteration 3/25 | Loss: 0.00041357
Iteration 4/25 | Loss: 0.00041357
Iteration 5/25 | Loss: 0.00041357
Iteration 6/25 | Loss: 0.00041357
Iteration 7/25 | Loss: 0.00041357
Iteration 8/25 | Loss: 0.00041357
Iteration 9/25 | Loss: 0.00041357
Iteration 10/25 | Loss: 0.00041357
Iteration 11/25 | Loss: 0.00041357
Iteration 12/25 | Loss: 0.00041357
Iteration 13/25 | Loss: 0.00041357
Iteration 14/25 | Loss: 0.00041357
Iteration 15/25 | Loss: 0.00041357
Iteration 16/25 | Loss: 0.00041357
Iteration 17/25 | Loss: 0.00041357
Iteration 18/25 | Loss: 0.00041357
Iteration 19/25 | Loss: 0.00041357
Iteration 20/25 | Loss: 0.00041357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00041356784640811384, 0.00041356784640811384, 0.00041356784640811384, 0.00041356784640811384, 0.00041356784640811384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041356784640811384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041357
Iteration 2/1000 | Loss: 0.00004367
Iteration 3/1000 | Loss: 0.00002905
Iteration 4/1000 | Loss: 0.00002506
Iteration 5/1000 | Loss: 0.00002385
Iteration 6/1000 | Loss: 0.00002275
Iteration 7/1000 | Loss: 0.00002208
Iteration 8/1000 | Loss: 0.00002168
Iteration 9/1000 | Loss: 0.00002130
Iteration 10/1000 | Loss: 0.00002107
Iteration 11/1000 | Loss: 0.00002099
Iteration 12/1000 | Loss: 0.00002079
Iteration 13/1000 | Loss: 0.00002065
Iteration 14/1000 | Loss: 0.00002063
Iteration 15/1000 | Loss: 0.00002062
Iteration 16/1000 | Loss: 0.00002055
Iteration 17/1000 | Loss: 0.00002048
Iteration 18/1000 | Loss: 0.00002042
Iteration 19/1000 | Loss: 0.00002036
Iteration 20/1000 | Loss: 0.00002036
Iteration 21/1000 | Loss: 0.00002035
Iteration 22/1000 | Loss: 0.00002033
Iteration 23/1000 | Loss: 0.00002033
Iteration 24/1000 | Loss: 0.00002032
Iteration 25/1000 | Loss: 0.00002032
Iteration 26/1000 | Loss: 0.00002031
Iteration 27/1000 | Loss: 0.00002031
Iteration 28/1000 | Loss: 0.00002030
Iteration 29/1000 | Loss: 0.00002030
Iteration 30/1000 | Loss: 0.00002029
Iteration 31/1000 | Loss: 0.00002029
Iteration 32/1000 | Loss: 0.00002029
Iteration 33/1000 | Loss: 0.00002028
Iteration 34/1000 | Loss: 0.00002027
Iteration 35/1000 | Loss: 0.00002027
Iteration 36/1000 | Loss: 0.00002027
Iteration 37/1000 | Loss: 0.00002026
Iteration 38/1000 | Loss: 0.00002025
Iteration 39/1000 | Loss: 0.00002025
Iteration 40/1000 | Loss: 0.00002024
Iteration 41/1000 | Loss: 0.00002024
Iteration 42/1000 | Loss: 0.00002024
Iteration 43/1000 | Loss: 0.00002023
Iteration 44/1000 | Loss: 0.00002023
Iteration 45/1000 | Loss: 0.00002023
Iteration 46/1000 | Loss: 0.00002023
Iteration 47/1000 | Loss: 0.00002022
Iteration 48/1000 | Loss: 0.00002022
Iteration 49/1000 | Loss: 0.00002022
Iteration 50/1000 | Loss: 0.00002021
Iteration 51/1000 | Loss: 0.00002021
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00002020
Iteration 54/1000 | Loss: 0.00002020
Iteration 55/1000 | Loss: 0.00002020
Iteration 56/1000 | Loss: 0.00002019
Iteration 57/1000 | Loss: 0.00002019
Iteration 58/1000 | Loss: 0.00002019
Iteration 59/1000 | Loss: 0.00002018
Iteration 60/1000 | Loss: 0.00002018
Iteration 61/1000 | Loss: 0.00002018
Iteration 62/1000 | Loss: 0.00002018
Iteration 63/1000 | Loss: 0.00002018
Iteration 64/1000 | Loss: 0.00002017
Iteration 65/1000 | Loss: 0.00002017
Iteration 66/1000 | Loss: 0.00002017
Iteration 67/1000 | Loss: 0.00002017
Iteration 68/1000 | Loss: 0.00002016
Iteration 69/1000 | Loss: 0.00002016
Iteration 70/1000 | Loss: 0.00002016
Iteration 71/1000 | Loss: 0.00002015
Iteration 72/1000 | Loss: 0.00002015
Iteration 73/1000 | Loss: 0.00002015
Iteration 74/1000 | Loss: 0.00002015
Iteration 75/1000 | Loss: 0.00002014
Iteration 76/1000 | Loss: 0.00002014
Iteration 77/1000 | Loss: 0.00002014
Iteration 78/1000 | Loss: 0.00002014
Iteration 79/1000 | Loss: 0.00002014
Iteration 80/1000 | Loss: 0.00002014
Iteration 81/1000 | Loss: 0.00002014
Iteration 82/1000 | Loss: 0.00002013
Iteration 83/1000 | Loss: 0.00002013
Iteration 84/1000 | Loss: 0.00002013
Iteration 85/1000 | Loss: 0.00002013
Iteration 86/1000 | Loss: 0.00002013
Iteration 87/1000 | Loss: 0.00002012
Iteration 88/1000 | Loss: 0.00002012
Iteration 89/1000 | Loss: 0.00002012
Iteration 90/1000 | Loss: 0.00002011
Iteration 91/1000 | Loss: 0.00002011
Iteration 92/1000 | Loss: 0.00002011
Iteration 93/1000 | Loss: 0.00002011
Iteration 94/1000 | Loss: 0.00002011
Iteration 95/1000 | Loss: 0.00002010
Iteration 96/1000 | Loss: 0.00002010
Iteration 97/1000 | Loss: 0.00002010
Iteration 98/1000 | Loss: 0.00002010
Iteration 99/1000 | Loss: 0.00002010
Iteration 100/1000 | Loss: 0.00002010
Iteration 101/1000 | Loss: 0.00002009
Iteration 102/1000 | Loss: 0.00002009
Iteration 103/1000 | Loss: 0.00002009
Iteration 104/1000 | Loss: 0.00002009
Iteration 105/1000 | Loss: 0.00002009
Iteration 106/1000 | Loss: 0.00002009
Iteration 107/1000 | Loss: 0.00002009
Iteration 108/1000 | Loss: 0.00002009
Iteration 109/1000 | Loss: 0.00002009
Iteration 110/1000 | Loss: 0.00002009
Iteration 111/1000 | Loss: 0.00002009
Iteration 112/1000 | Loss: 0.00002009
Iteration 113/1000 | Loss: 0.00002009
Iteration 114/1000 | Loss: 0.00002009
Iteration 115/1000 | Loss: 0.00002009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.0087851225980558e-05, 2.0087851225980558e-05, 2.0087851225980558e-05, 2.0087851225980558e-05, 2.0087851225980558e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0087851225980558e-05

Optimization complete. Final v2v error: 3.63275146484375 mm

Highest mean error: 4.4818315505981445 mm for frame 181

Lowest mean error: 3.110111951828003 mm for frame 229

Saving results

Total time: 44.64837694168091
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041233
Iteration 2/25 | Loss: 0.00345020
Iteration 3/25 | Loss: 0.00181908
Iteration 4/25 | Loss: 0.00185503
Iteration 5/25 | Loss: 0.00190939
Iteration 6/25 | Loss: 0.00156441
Iteration 7/25 | Loss: 0.00154002
Iteration 8/25 | Loss: 0.00151186
Iteration 9/25 | Loss: 0.00146554
Iteration 10/25 | Loss: 0.00143460
Iteration 11/25 | Loss: 0.00141824
Iteration 12/25 | Loss: 0.00139275
Iteration 13/25 | Loss: 0.00141715
Iteration 14/25 | Loss: 0.00155663
Iteration 15/25 | Loss: 0.00136706
Iteration 16/25 | Loss: 0.00121331
Iteration 17/25 | Loss: 0.00115634
Iteration 18/25 | Loss: 0.00113053
Iteration 19/25 | Loss: 0.00111494
Iteration 20/25 | Loss: 0.00111282
Iteration 21/25 | Loss: 0.00111109
Iteration 22/25 | Loss: 0.00110574
Iteration 23/25 | Loss: 0.00110249
Iteration 24/25 | Loss: 0.00110562
Iteration 25/25 | Loss: 0.00110332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48509753
Iteration 2/25 | Loss: 0.00680273
Iteration 3/25 | Loss: 0.00175597
Iteration 4/25 | Loss: 0.00175593
Iteration 5/25 | Loss: 0.00175593
Iteration 6/25 | Loss: 0.00175593
Iteration 7/25 | Loss: 0.00175593
Iteration 8/25 | Loss: 0.00175593
Iteration 9/25 | Loss: 0.00175593
Iteration 10/25 | Loss: 0.00175593
Iteration 11/25 | Loss: 0.00175593
Iteration 12/25 | Loss: 0.00175593
Iteration 13/25 | Loss: 0.00175593
Iteration 14/25 | Loss: 0.00175593
Iteration 15/25 | Loss: 0.00175593
Iteration 16/25 | Loss: 0.00175593
Iteration 17/25 | Loss: 0.00175593
Iteration 18/25 | Loss: 0.00175593
Iteration 19/25 | Loss: 0.00175593
Iteration 20/25 | Loss: 0.00175593
Iteration 21/25 | Loss: 0.00175593
Iteration 22/25 | Loss: 0.00175593
Iteration 23/25 | Loss: 0.00175593
Iteration 24/25 | Loss: 0.00175593
Iteration 25/25 | Loss: 0.00175593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175593
Iteration 2/1000 | Loss: 0.00235889
Iteration 3/1000 | Loss: 0.00759432
Iteration 4/1000 | Loss: 0.00176984
Iteration 5/1000 | Loss: 0.00232241
Iteration 6/1000 | Loss: 0.00344591
Iteration 7/1000 | Loss: 0.00277140
Iteration 8/1000 | Loss: 0.00274087
Iteration 9/1000 | Loss: 0.00390859
Iteration 10/1000 | Loss: 0.00438395
Iteration 11/1000 | Loss: 0.00156040
Iteration 12/1000 | Loss: 0.00132325
Iteration 13/1000 | Loss: 0.00115772
Iteration 14/1000 | Loss: 0.00078507
Iteration 15/1000 | Loss: 0.00063092
Iteration 16/1000 | Loss: 0.00541428
Iteration 17/1000 | Loss: 0.00071111
Iteration 18/1000 | Loss: 0.00038459
Iteration 19/1000 | Loss: 0.00033895
Iteration 20/1000 | Loss: 0.00027566
Iteration 21/1000 | Loss: 0.00078884
Iteration 22/1000 | Loss: 0.00011704
Iteration 23/1000 | Loss: 0.00285744
Iteration 24/1000 | Loss: 0.00510530
Iteration 25/1000 | Loss: 0.00503219
Iteration 26/1000 | Loss: 0.00018845
Iteration 27/1000 | Loss: 0.00063167
Iteration 28/1000 | Loss: 0.00006019
Iteration 29/1000 | Loss: 0.00034452
Iteration 30/1000 | Loss: 0.00021782
Iteration 31/1000 | Loss: 0.00459324
Iteration 32/1000 | Loss: 0.00075768
Iteration 33/1000 | Loss: 0.00152378
Iteration 34/1000 | Loss: 0.00014515
Iteration 35/1000 | Loss: 0.00008195
Iteration 36/1000 | Loss: 0.00010738
Iteration 37/1000 | Loss: 0.00017674
Iteration 38/1000 | Loss: 0.00076962
Iteration 39/1000 | Loss: 0.00009496
Iteration 40/1000 | Loss: 0.00013307
Iteration 41/1000 | Loss: 0.00006779
Iteration 42/1000 | Loss: 0.00053520
Iteration 43/1000 | Loss: 0.00047474
Iteration 44/1000 | Loss: 0.00038626
Iteration 45/1000 | Loss: 0.00020922
Iteration 46/1000 | Loss: 0.00017198
Iteration 47/1000 | Loss: 0.00015374
Iteration 48/1000 | Loss: 0.00013582
Iteration 49/1000 | Loss: 0.00004428
Iteration 50/1000 | Loss: 0.00009129
Iteration 51/1000 | Loss: 0.00021004
Iteration 52/1000 | Loss: 0.00045564
Iteration 53/1000 | Loss: 0.00085785
Iteration 54/1000 | Loss: 0.00045427
Iteration 55/1000 | Loss: 0.00015943
Iteration 56/1000 | Loss: 0.00008906
Iteration 57/1000 | Loss: 0.00003581
Iteration 58/1000 | Loss: 0.00044657
Iteration 59/1000 | Loss: 0.00012740
Iteration 60/1000 | Loss: 0.00011996
Iteration 61/1000 | Loss: 0.00008463
Iteration 62/1000 | Loss: 0.00004227
Iteration 63/1000 | Loss: 0.00003428
Iteration 64/1000 | Loss: 0.00016187
Iteration 65/1000 | Loss: 0.00009434
Iteration 66/1000 | Loss: 0.00016475
Iteration 67/1000 | Loss: 0.00019708
Iteration 68/1000 | Loss: 0.00049973
Iteration 69/1000 | Loss: 0.00011210
Iteration 70/1000 | Loss: 0.00075362
Iteration 71/1000 | Loss: 0.00042918
Iteration 72/1000 | Loss: 0.00117066
Iteration 73/1000 | Loss: 0.00157931
Iteration 74/1000 | Loss: 0.00060267
Iteration 75/1000 | Loss: 0.00005484
Iteration 76/1000 | Loss: 0.00016458
Iteration 77/1000 | Loss: 0.00055509
Iteration 78/1000 | Loss: 0.00022155
Iteration 79/1000 | Loss: 0.00003981
Iteration 80/1000 | Loss: 0.00004426
Iteration 81/1000 | Loss: 0.00003605
Iteration 82/1000 | Loss: 0.00003616
Iteration 83/1000 | Loss: 0.00003177
Iteration 84/1000 | Loss: 0.00004839
Iteration 85/1000 | Loss: 0.00002714
Iteration 86/1000 | Loss: 0.00002144
Iteration 87/1000 | Loss: 0.00002667
Iteration 88/1000 | Loss: 0.00005438
Iteration 89/1000 | Loss: 0.00002829
Iteration 90/1000 | Loss: 0.00002724
Iteration 91/1000 | Loss: 0.00002619
Iteration 92/1000 | Loss: 0.00002716
Iteration 93/1000 | Loss: 0.00002715
Iteration 94/1000 | Loss: 0.00002100
Iteration 95/1000 | Loss: 0.00003364
Iteration 96/1000 | Loss: 0.00010198
Iteration 97/1000 | Loss: 0.00004659
Iteration 98/1000 | Loss: 0.00002105
Iteration 99/1000 | Loss: 0.00001894
Iteration 100/1000 | Loss: 0.00021091
Iteration 101/1000 | Loss: 0.00001789
Iteration 102/1000 | Loss: 0.00001695
Iteration 103/1000 | Loss: 0.00017262
Iteration 104/1000 | Loss: 0.00001710
Iteration 105/1000 | Loss: 0.00001619
Iteration 106/1000 | Loss: 0.00001595
Iteration 107/1000 | Loss: 0.00001588
Iteration 108/1000 | Loss: 0.00001587
Iteration 109/1000 | Loss: 0.00001587
Iteration 110/1000 | Loss: 0.00001587
Iteration 111/1000 | Loss: 0.00001586
Iteration 112/1000 | Loss: 0.00001584
Iteration 113/1000 | Loss: 0.00001584
Iteration 114/1000 | Loss: 0.00001583
Iteration 115/1000 | Loss: 0.00001583
Iteration 116/1000 | Loss: 0.00001577
Iteration 117/1000 | Loss: 0.00001576
Iteration 118/1000 | Loss: 0.00001576
Iteration 119/1000 | Loss: 0.00001570
Iteration 120/1000 | Loss: 0.00001569
Iteration 121/1000 | Loss: 0.00001568
Iteration 122/1000 | Loss: 0.00001568
Iteration 123/1000 | Loss: 0.00001567
Iteration 124/1000 | Loss: 0.00001567
Iteration 125/1000 | Loss: 0.00001564
Iteration 126/1000 | Loss: 0.00001564
Iteration 127/1000 | Loss: 0.00001563
Iteration 128/1000 | Loss: 0.00001561
Iteration 129/1000 | Loss: 0.00001560
Iteration 130/1000 | Loss: 0.00001560
Iteration 131/1000 | Loss: 0.00001560
Iteration 132/1000 | Loss: 0.00001560
Iteration 133/1000 | Loss: 0.00001560
Iteration 134/1000 | Loss: 0.00001560
Iteration 135/1000 | Loss: 0.00001560
Iteration 136/1000 | Loss: 0.00001560
Iteration 137/1000 | Loss: 0.00001559
Iteration 138/1000 | Loss: 0.00001559
Iteration 139/1000 | Loss: 0.00001559
Iteration 140/1000 | Loss: 0.00001559
Iteration 141/1000 | Loss: 0.00001559
Iteration 142/1000 | Loss: 0.00001558
Iteration 143/1000 | Loss: 0.00001558
Iteration 144/1000 | Loss: 0.00001557
Iteration 145/1000 | Loss: 0.00001557
Iteration 146/1000 | Loss: 0.00001557
Iteration 147/1000 | Loss: 0.00001556
Iteration 148/1000 | Loss: 0.00001556
Iteration 149/1000 | Loss: 0.00001555
Iteration 150/1000 | Loss: 0.00001550
Iteration 151/1000 | Loss: 0.00001549
Iteration 152/1000 | Loss: 0.00001549
Iteration 153/1000 | Loss: 0.00001549
Iteration 154/1000 | Loss: 0.00001549
Iteration 155/1000 | Loss: 0.00001548
Iteration 156/1000 | Loss: 0.00001548
Iteration 157/1000 | Loss: 0.00001548
Iteration 158/1000 | Loss: 0.00001547
Iteration 159/1000 | Loss: 0.00001547
Iteration 160/1000 | Loss: 0.00001547
Iteration 161/1000 | Loss: 0.00001546
Iteration 162/1000 | Loss: 0.00001546
Iteration 163/1000 | Loss: 0.00001545
Iteration 164/1000 | Loss: 0.00001545
Iteration 165/1000 | Loss: 0.00001545
Iteration 166/1000 | Loss: 0.00001544
Iteration 167/1000 | Loss: 0.00001544
Iteration 168/1000 | Loss: 0.00001544
Iteration 169/1000 | Loss: 0.00001544
Iteration 170/1000 | Loss: 0.00001543
Iteration 171/1000 | Loss: 0.00001543
Iteration 172/1000 | Loss: 0.00001543
Iteration 173/1000 | Loss: 0.00001543
Iteration 174/1000 | Loss: 0.00001542
Iteration 175/1000 | Loss: 0.00001542
Iteration 176/1000 | Loss: 0.00001542
Iteration 177/1000 | Loss: 0.00001542
Iteration 178/1000 | Loss: 0.00001542
Iteration 179/1000 | Loss: 0.00001542
Iteration 180/1000 | Loss: 0.00001542
Iteration 181/1000 | Loss: 0.00001542
Iteration 182/1000 | Loss: 0.00001542
Iteration 183/1000 | Loss: 0.00001542
Iteration 184/1000 | Loss: 0.00001542
Iteration 185/1000 | Loss: 0.00001542
Iteration 186/1000 | Loss: 0.00001542
Iteration 187/1000 | Loss: 0.00001542
Iteration 188/1000 | Loss: 0.00001542
Iteration 189/1000 | Loss: 0.00001542
Iteration 190/1000 | Loss: 0.00001542
Iteration 191/1000 | Loss: 0.00001541
Iteration 192/1000 | Loss: 0.00001541
Iteration 193/1000 | Loss: 0.00001541
Iteration 194/1000 | Loss: 0.00001541
Iteration 195/1000 | Loss: 0.00001541
Iteration 196/1000 | Loss: 0.00001541
Iteration 197/1000 | Loss: 0.00001541
Iteration 198/1000 | Loss: 0.00001541
Iteration 199/1000 | Loss: 0.00001541
Iteration 200/1000 | Loss: 0.00001541
Iteration 201/1000 | Loss: 0.00001541
Iteration 202/1000 | Loss: 0.00001541
Iteration 203/1000 | Loss: 0.00001541
Iteration 204/1000 | Loss: 0.00001541
Iteration 205/1000 | Loss: 0.00001541
Iteration 206/1000 | Loss: 0.00001540
Iteration 207/1000 | Loss: 0.00001540
Iteration 208/1000 | Loss: 0.00001540
Iteration 209/1000 | Loss: 0.00001540
Iteration 210/1000 | Loss: 0.00001540
Iteration 211/1000 | Loss: 0.00001540
Iteration 212/1000 | Loss: 0.00001540
Iteration 213/1000 | Loss: 0.00001540
Iteration 214/1000 | Loss: 0.00001540
Iteration 215/1000 | Loss: 0.00001540
Iteration 216/1000 | Loss: 0.00001540
Iteration 217/1000 | Loss: 0.00001540
Iteration 218/1000 | Loss: 0.00001540
Iteration 219/1000 | Loss: 0.00001540
Iteration 220/1000 | Loss: 0.00001540
Iteration 221/1000 | Loss: 0.00001540
Iteration 222/1000 | Loss: 0.00001540
Iteration 223/1000 | Loss: 0.00001540
Iteration 224/1000 | Loss: 0.00001540
Iteration 225/1000 | Loss: 0.00001540
Iteration 226/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.5398083633044735e-05, 1.5398083633044735e-05, 1.5398083633044735e-05, 1.5398083633044735e-05, 1.5398083633044735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5398083633044735e-05

Optimization complete. Final v2v error: 3.3157424926757812 mm

Highest mean error: 4.785236358642578 mm for frame 1

Lowest mean error: 2.931448221206665 mm for frame 228

Saving results

Total time: 237.85108137130737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00660320
Iteration 2/25 | Loss: 0.00093234
Iteration 3/25 | Loss: 0.00073775
Iteration 4/25 | Loss: 0.00071177
Iteration 5/25 | Loss: 0.00070482
Iteration 6/25 | Loss: 0.00070339
Iteration 7/25 | Loss: 0.00070330
Iteration 8/25 | Loss: 0.00070330
Iteration 9/25 | Loss: 0.00070330
Iteration 10/25 | Loss: 0.00070330
Iteration 11/25 | Loss: 0.00070330
Iteration 12/25 | Loss: 0.00070330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007032976718619466, 0.0007032976718619466, 0.0007032976718619466, 0.0007032976718619466, 0.0007032976718619466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007032976718619466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.20052958
Iteration 2/25 | Loss: 0.00032669
Iteration 3/25 | Loss: 0.00032660
Iteration 4/25 | Loss: 0.00032660
Iteration 5/25 | Loss: 0.00032660
Iteration 6/25 | Loss: 0.00032660
Iteration 7/25 | Loss: 0.00032660
Iteration 8/25 | Loss: 0.00032660
Iteration 9/25 | Loss: 0.00032660
Iteration 10/25 | Loss: 0.00032660
Iteration 11/25 | Loss: 0.00032660
Iteration 12/25 | Loss: 0.00032660
Iteration 13/25 | Loss: 0.00032660
Iteration 14/25 | Loss: 0.00032660
Iteration 15/25 | Loss: 0.00032660
Iteration 16/25 | Loss: 0.00032660
Iteration 17/25 | Loss: 0.00032660
Iteration 18/25 | Loss: 0.00032660
Iteration 19/25 | Loss: 0.00032660
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003265982959419489, 0.0003265982959419489, 0.0003265982959419489, 0.0003265982959419489, 0.0003265982959419489]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003265982959419489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032660
Iteration 2/1000 | Loss: 0.00002648
Iteration 3/1000 | Loss: 0.00002082
Iteration 4/1000 | Loss: 0.00001932
Iteration 5/1000 | Loss: 0.00001833
Iteration 6/1000 | Loss: 0.00001777
Iteration 7/1000 | Loss: 0.00001733
Iteration 8/1000 | Loss: 0.00001704
Iteration 9/1000 | Loss: 0.00001681
Iteration 10/1000 | Loss: 0.00001666
Iteration 11/1000 | Loss: 0.00001659
Iteration 12/1000 | Loss: 0.00001656
Iteration 13/1000 | Loss: 0.00001653
Iteration 14/1000 | Loss: 0.00001652
Iteration 15/1000 | Loss: 0.00001647
Iteration 16/1000 | Loss: 0.00001647
Iteration 17/1000 | Loss: 0.00001645
Iteration 18/1000 | Loss: 0.00001644
Iteration 19/1000 | Loss: 0.00001644
Iteration 20/1000 | Loss: 0.00001644
Iteration 21/1000 | Loss: 0.00001642
Iteration 22/1000 | Loss: 0.00001642
Iteration 23/1000 | Loss: 0.00001642
Iteration 24/1000 | Loss: 0.00001642
Iteration 25/1000 | Loss: 0.00001642
Iteration 26/1000 | Loss: 0.00001642
Iteration 27/1000 | Loss: 0.00001642
Iteration 28/1000 | Loss: 0.00001642
Iteration 29/1000 | Loss: 0.00001642
Iteration 30/1000 | Loss: 0.00001642
Iteration 31/1000 | Loss: 0.00001642
Iteration 32/1000 | Loss: 0.00001642
Iteration 33/1000 | Loss: 0.00001641
Iteration 34/1000 | Loss: 0.00001641
Iteration 35/1000 | Loss: 0.00001641
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001639
Iteration 38/1000 | Loss: 0.00001639
Iteration 39/1000 | Loss: 0.00001638
Iteration 40/1000 | Loss: 0.00001638
Iteration 41/1000 | Loss: 0.00001637
Iteration 42/1000 | Loss: 0.00001637
Iteration 43/1000 | Loss: 0.00001636
Iteration 44/1000 | Loss: 0.00001636
Iteration 45/1000 | Loss: 0.00001636
Iteration 46/1000 | Loss: 0.00001635
Iteration 47/1000 | Loss: 0.00001635
Iteration 48/1000 | Loss: 0.00001635
Iteration 49/1000 | Loss: 0.00001634
Iteration 50/1000 | Loss: 0.00001634
Iteration 51/1000 | Loss: 0.00001634
Iteration 52/1000 | Loss: 0.00001633
Iteration 53/1000 | Loss: 0.00001633
Iteration 54/1000 | Loss: 0.00001633
Iteration 55/1000 | Loss: 0.00001632
Iteration 56/1000 | Loss: 0.00001632
Iteration 57/1000 | Loss: 0.00001631
Iteration 58/1000 | Loss: 0.00001627
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001625
Iteration 61/1000 | Loss: 0.00001623
Iteration 62/1000 | Loss: 0.00001622
Iteration 63/1000 | Loss: 0.00001622
Iteration 64/1000 | Loss: 0.00001622
Iteration 65/1000 | Loss: 0.00001622
Iteration 66/1000 | Loss: 0.00001622
Iteration 67/1000 | Loss: 0.00001621
Iteration 68/1000 | Loss: 0.00001621
Iteration 69/1000 | Loss: 0.00001621
Iteration 70/1000 | Loss: 0.00001621
Iteration 71/1000 | Loss: 0.00001621
Iteration 72/1000 | Loss: 0.00001621
Iteration 73/1000 | Loss: 0.00001621
Iteration 74/1000 | Loss: 0.00001621
Iteration 75/1000 | Loss: 0.00001621
Iteration 76/1000 | Loss: 0.00001621
Iteration 77/1000 | Loss: 0.00001621
Iteration 78/1000 | Loss: 0.00001621
Iteration 79/1000 | Loss: 0.00001620
Iteration 80/1000 | Loss: 0.00001620
Iteration 81/1000 | Loss: 0.00001620
Iteration 82/1000 | Loss: 0.00001620
Iteration 83/1000 | Loss: 0.00001620
Iteration 84/1000 | Loss: 0.00001620
Iteration 85/1000 | Loss: 0.00001620
Iteration 86/1000 | Loss: 0.00001620
Iteration 87/1000 | Loss: 0.00001620
Iteration 88/1000 | Loss: 0.00001620
Iteration 89/1000 | Loss: 0.00001619
Iteration 90/1000 | Loss: 0.00001619
Iteration 91/1000 | Loss: 0.00001619
Iteration 92/1000 | Loss: 0.00001619
Iteration 93/1000 | Loss: 0.00001619
Iteration 94/1000 | Loss: 0.00001618
Iteration 95/1000 | Loss: 0.00001618
Iteration 96/1000 | Loss: 0.00001618
Iteration 97/1000 | Loss: 0.00001618
Iteration 98/1000 | Loss: 0.00001618
Iteration 99/1000 | Loss: 0.00001618
Iteration 100/1000 | Loss: 0.00001618
Iteration 101/1000 | Loss: 0.00001617
Iteration 102/1000 | Loss: 0.00001617
Iteration 103/1000 | Loss: 0.00001617
Iteration 104/1000 | Loss: 0.00001617
Iteration 105/1000 | Loss: 0.00001617
Iteration 106/1000 | Loss: 0.00001617
Iteration 107/1000 | Loss: 0.00001617
Iteration 108/1000 | Loss: 0.00001617
Iteration 109/1000 | Loss: 0.00001617
Iteration 110/1000 | Loss: 0.00001617
Iteration 111/1000 | Loss: 0.00001616
Iteration 112/1000 | Loss: 0.00001616
Iteration 113/1000 | Loss: 0.00001616
Iteration 114/1000 | Loss: 0.00001616
Iteration 115/1000 | Loss: 0.00001616
Iteration 116/1000 | Loss: 0.00001616
Iteration 117/1000 | Loss: 0.00001616
Iteration 118/1000 | Loss: 0.00001616
Iteration 119/1000 | Loss: 0.00001616
Iteration 120/1000 | Loss: 0.00001616
Iteration 121/1000 | Loss: 0.00001616
Iteration 122/1000 | Loss: 0.00001616
Iteration 123/1000 | Loss: 0.00001616
Iteration 124/1000 | Loss: 0.00001616
Iteration 125/1000 | Loss: 0.00001616
Iteration 126/1000 | Loss: 0.00001616
Iteration 127/1000 | Loss: 0.00001616
Iteration 128/1000 | Loss: 0.00001615
Iteration 129/1000 | Loss: 0.00001615
Iteration 130/1000 | Loss: 0.00001615
Iteration 131/1000 | Loss: 0.00001615
Iteration 132/1000 | Loss: 0.00001615
Iteration 133/1000 | Loss: 0.00001615
Iteration 134/1000 | Loss: 0.00001615
Iteration 135/1000 | Loss: 0.00001615
Iteration 136/1000 | Loss: 0.00001615
Iteration 137/1000 | Loss: 0.00001615
Iteration 138/1000 | Loss: 0.00001615
Iteration 139/1000 | Loss: 0.00001615
Iteration 140/1000 | Loss: 0.00001615
Iteration 141/1000 | Loss: 0.00001615
Iteration 142/1000 | Loss: 0.00001615
Iteration 143/1000 | Loss: 0.00001615
Iteration 144/1000 | Loss: 0.00001615
Iteration 145/1000 | Loss: 0.00001614
Iteration 146/1000 | Loss: 0.00001614
Iteration 147/1000 | Loss: 0.00001614
Iteration 148/1000 | Loss: 0.00001614
Iteration 149/1000 | Loss: 0.00001614
Iteration 150/1000 | Loss: 0.00001614
Iteration 151/1000 | Loss: 0.00001614
Iteration 152/1000 | Loss: 0.00001614
Iteration 153/1000 | Loss: 0.00001614
Iteration 154/1000 | Loss: 0.00001614
Iteration 155/1000 | Loss: 0.00001614
Iteration 156/1000 | Loss: 0.00001614
Iteration 157/1000 | Loss: 0.00001614
Iteration 158/1000 | Loss: 0.00001614
Iteration 159/1000 | Loss: 0.00001614
Iteration 160/1000 | Loss: 0.00001614
Iteration 161/1000 | Loss: 0.00001614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.61406187544344e-05, 1.61406187544344e-05, 1.61406187544344e-05, 1.61406187544344e-05, 1.61406187544344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.61406187544344e-05

Optimization complete. Final v2v error: 3.3889319896698 mm

Highest mean error: 3.7616305351257324 mm for frame 150

Lowest mean error: 3.138948678970337 mm for frame 100

Saving results

Total time: 36.957175493240356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832521
Iteration 2/25 | Loss: 0.00097664
Iteration 3/25 | Loss: 0.00074101
Iteration 4/25 | Loss: 0.00069220
Iteration 5/25 | Loss: 0.00068283
Iteration 6/25 | Loss: 0.00069612
Iteration 7/25 | Loss: 0.00067999
Iteration 8/25 | Loss: 0.00067949
Iteration 9/25 | Loss: 0.00068074
Iteration 10/25 | Loss: 0.00067553
Iteration 11/25 | Loss: 0.00067404
Iteration 12/25 | Loss: 0.00067124
Iteration 13/25 | Loss: 0.00067088
Iteration 14/25 | Loss: 0.00067058
Iteration 15/25 | Loss: 0.00067056
Iteration 16/25 | Loss: 0.00067056
Iteration 17/25 | Loss: 0.00067056
Iteration 18/25 | Loss: 0.00067056
Iteration 19/25 | Loss: 0.00067056
Iteration 20/25 | Loss: 0.00067056
Iteration 21/25 | Loss: 0.00067055
Iteration 22/25 | Loss: 0.00067055
Iteration 23/25 | Loss: 0.00067055
Iteration 24/25 | Loss: 0.00067055
Iteration 25/25 | Loss: 0.00067055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56234682
Iteration 2/25 | Loss: 0.00031781
Iteration 3/25 | Loss: 0.00031780
Iteration 4/25 | Loss: 0.00031780
Iteration 5/25 | Loss: 0.00031780
Iteration 6/25 | Loss: 0.00031780
Iteration 7/25 | Loss: 0.00031780
Iteration 8/25 | Loss: 0.00031780
Iteration 9/25 | Loss: 0.00031780
Iteration 10/25 | Loss: 0.00031780
Iteration 11/25 | Loss: 0.00031780
Iteration 12/25 | Loss: 0.00031780
Iteration 13/25 | Loss: 0.00031780
Iteration 14/25 | Loss: 0.00031780
Iteration 15/25 | Loss: 0.00031780
Iteration 16/25 | Loss: 0.00031780
Iteration 17/25 | Loss: 0.00031780
Iteration 18/25 | Loss: 0.00031780
Iteration 19/25 | Loss: 0.00031780
Iteration 20/25 | Loss: 0.00031780
Iteration 21/25 | Loss: 0.00031779
Iteration 22/25 | Loss: 0.00031779
Iteration 23/25 | Loss: 0.00031779
Iteration 24/25 | Loss: 0.00031779
Iteration 25/25 | Loss: 0.00031779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031779
Iteration 2/1000 | Loss: 0.00002373
Iteration 3/1000 | Loss: 0.00001573
Iteration 4/1000 | Loss: 0.00001418
Iteration 5/1000 | Loss: 0.00001347
Iteration 6/1000 | Loss: 0.00001299
Iteration 7/1000 | Loss: 0.00001271
Iteration 8/1000 | Loss: 0.00001250
Iteration 9/1000 | Loss: 0.00001242
Iteration 10/1000 | Loss: 0.00001234
Iteration 11/1000 | Loss: 0.00001219
Iteration 12/1000 | Loss: 0.00001219
Iteration 13/1000 | Loss: 0.00001213
Iteration 14/1000 | Loss: 0.00001210
Iteration 15/1000 | Loss: 0.00001210
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001209
Iteration 18/1000 | Loss: 0.00001208
Iteration 19/1000 | Loss: 0.00001202
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001196
Iteration 24/1000 | Loss: 0.00001196
Iteration 25/1000 | Loss: 0.00001195
Iteration 26/1000 | Loss: 0.00001195
Iteration 27/1000 | Loss: 0.00001193
Iteration 28/1000 | Loss: 0.00001193
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001193
Iteration 31/1000 | Loss: 0.00001193
Iteration 32/1000 | Loss: 0.00001192
Iteration 33/1000 | Loss: 0.00001192
Iteration 34/1000 | Loss: 0.00001192
Iteration 35/1000 | Loss: 0.00001192
Iteration 36/1000 | Loss: 0.00001192
Iteration 37/1000 | Loss: 0.00001191
Iteration 38/1000 | Loss: 0.00001191
Iteration 39/1000 | Loss: 0.00001191
Iteration 40/1000 | Loss: 0.00001189
Iteration 41/1000 | Loss: 0.00001189
Iteration 42/1000 | Loss: 0.00001188
Iteration 43/1000 | Loss: 0.00001188
Iteration 44/1000 | Loss: 0.00001187
Iteration 45/1000 | Loss: 0.00001187
Iteration 46/1000 | Loss: 0.00001187
Iteration 47/1000 | Loss: 0.00001187
Iteration 48/1000 | Loss: 0.00001187
Iteration 49/1000 | Loss: 0.00001186
Iteration 50/1000 | Loss: 0.00001186
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001186
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001186
Iteration 55/1000 | Loss: 0.00001185
Iteration 56/1000 | Loss: 0.00001185
Iteration 57/1000 | Loss: 0.00001185
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001185
Iteration 60/1000 | Loss: 0.00001185
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001184
Iteration 63/1000 | Loss: 0.00001184
Iteration 64/1000 | Loss: 0.00001184
Iteration 65/1000 | Loss: 0.00001184
Iteration 66/1000 | Loss: 0.00001184
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001184
Iteration 69/1000 | Loss: 0.00001183
Iteration 70/1000 | Loss: 0.00001183
Iteration 71/1000 | Loss: 0.00001183
Iteration 72/1000 | Loss: 0.00001183
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001182
Iteration 77/1000 | Loss: 0.00001181
Iteration 78/1000 | Loss: 0.00001181
Iteration 79/1000 | Loss: 0.00001181
Iteration 80/1000 | Loss: 0.00001181
Iteration 81/1000 | Loss: 0.00001180
Iteration 82/1000 | Loss: 0.00001180
Iteration 83/1000 | Loss: 0.00001179
Iteration 84/1000 | Loss: 0.00001179
Iteration 85/1000 | Loss: 0.00001179
Iteration 86/1000 | Loss: 0.00001179
Iteration 87/1000 | Loss: 0.00001179
Iteration 88/1000 | Loss: 0.00001178
Iteration 89/1000 | Loss: 0.00001178
Iteration 90/1000 | Loss: 0.00001178
Iteration 91/1000 | Loss: 0.00001178
Iteration 92/1000 | Loss: 0.00001178
Iteration 93/1000 | Loss: 0.00001178
Iteration 94/1000 | Loss: 0.00001178
Iteration 95/1000 | Loss: 0.00001178
Iteration 96/1000 | Loss: 0.00001178
Iteration 97/1000 | Loss: 0.00001178
Iteration 98/1000 | Loss: 0.00001178
Iteration 99/1000 | Loss: 0.00001178
Iteration 100/1000 | Loss: 0.00001178
Iteration 101/1000 | Loss: 0.00001178
Iteration 102/1000 | Loss: 0.00001178
Iteration 103/1000 | Loss: 0.00001178
Iteration 104/1000 | Loss: 0.00001178
Iteration 105/1000 | Loss: 0.00001178
Iteration 106/1000 | Loss: 0.00001178
Iteration 107/1000 | Loss: 0.00001178
Iteration 108/1000 | Loss: 0.00001178
Iteration 109/1000 | Loss: 0.00001178
Iteration 110/1000 | Loss: 0.00001178
Iteration 111/1000 | Loss: 0.00001178
Iteration 112/1000 | Loss: 0.00001178
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001178
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001178
Iteration 118/1000 | Loss: 0.00001178
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001178
Iteration 123/1000 | Loss: 0.00001178
Iteration 124/1000 | Loss: 0.00001178
Iteration 125/1000 | Loss: 0.00001178
Iteration 126/1000 | Loss: 0.00001178
Iteration 127/1000 | Loss: 0.00001178
Iteration 128/1000 | Loss: 0.00001178
Iteration 129/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.1776256542361807e-05, 1.1776256542361807e-05, 1.1776256542361807e-05, 1.1776256542361807e-05, 1.1776256542361807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1776256542361807e-05

Optimization complete. Final v2v error: 2.8848085403442383 mm

Highest mean error: 3.6155858039855957 mm for frame 56

Lowest mean error: 2.6080827713012695 mm for frame 125

Saving results

Total time: 47.92531394958496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402022
Iteration 2/25 | Loss: 0.00089603
Iteration 3/25 | Loss: 0.00071441
Iteration 4/25 | Loss: 0.00069945
Iteration 5/25 | Loss: 0.00069373
Iteration 6/25 | Loss: 0.00069319
Iteration 7/25 | Loss: 0.00069319
Iteration 8/25 | Loss: 0.00069319
Iteration 9/25 | Loss: 0.00069319
Iteration 10/25 | Loss: 0.00069319
Iteration 11/25 | Loss: 0.00069319
Iteration 12/25 | Loss: 0.00069319
Iteration 13/25 | Loss: 0.00069319
Iteration 14/25 | Loss: 0.00069319
Iteration 15/25 | Loss: 0.00069319
Iteration 16/25 | Loss: 0.00069319
Iteration 17/25 | Loss: 0.00069319
Iteration 18/25 | Loss: 0.00069319
Iteration 19/25 | Loss: 0.00069319
Iteration 20/25 | Loss: 0.00069319
Iteration 21/25 | Loss: 0.00069319
Iteration 22/25 | Loss: 0.00069319
Iteration 23/25 | Loss: 0.00069319
Iteration 24/25 | Loss: 0.00069319
Iteration 25/25 | Loss: 0.00069319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74527740
Iteration 2/25 | Loss: 0.00037168
Iteration 3/25 | Loss: 0.00037168
Iteration 4/25 | Loss: 0.00037168
Iteration 5/25 | Loss: 0.00037168
Iteration 6/25 | Loss: 0.00037168
Iteration 7/25 | Loss: 0.00037168
Iteration 8/25 | Loss: 0.00037168
Iteration 9/25 | Loss: 0.00037168
Iteration 10/25 | Loss: 0.00037168
Iteration 11/25 | Loss: 0.00037168
Iteration 12/25 | Loss: 0.00037168
Iteration 13/25 | Loss: 0.00037168
Iteration 14/25 | Loss: 0.00037168
Iteration 15/25 | Loss: 0.00037168
Iteration 16/25 | Loss: 0.00037168
Iteration 17/25 | Loss: 0.00037168
Iteration 18/25 | Loss: 0.00037168
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003716767532750964, 0.0003716767532750964, 0.0003716767532750964, 0.0003716767532750964, 0.0003716767532750964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003716767532750964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037168
Iteration 2/1000 | Loss: 0.00002417
Iteration 3/1000 | Loss: 0.00001566
Iteration 4/1000 | Loss: 0.00001417
Iteration 5/1000 | Loss: 0.00001324
Iteration 6/1000 | Loss: 0.00001286
Iteration 7/1000 | Loss: 0.00001258
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001227
Iteration 10/1000 | Loss: 0.00001220
Iteration 11/1000 | Loss: 0.00001202
Iteration 12/1000 | Loss: 0.00001198
Iteration 13/1000 | Loss: 0.00001198
Iteration 14/1000 | Loss: 0.00001195
Iteration 15/1000 | Loss: 0.00001187
Iteration 16/1000 | Loss: 0.00001182
Iteration 17/1000 | Loss: 0.00001182
Iteration 18/1000 | Loss: 0.00001178
Iteration 19/1000 | Loss: 0.00001178
Iteration 20/1000 | Loss: 0.00001178
Iteration 21/1000 | Loss: 0.00001178
Iteration 22/1000 | Loss: 0.00001177
Iteration 23/1000 | Loss: 0.00001176
Iteration 24/1000 | Loss: 0.00001176
Iteration 25/1000 | Loss: 0.00001175
Iteration 26/1000 | Loss: 0.00001175
Iteration 27/1000 | Loss: 0.00001175
Iteration 28/1000 | Loss: 0.00001174
Iteration 29/1000 | Loss: 0.00001174
Iteration 30/1000 | Loss: 0.00001174
Iteration 31/1000 | Loss: 0.00001171
Iteration 32/1000 | Loss: 0.00001170
Iteration 33/1000 | Loss: 0.00001170
Iteration 34/1000 | Loss: 0.00001170
Iteration 35/1000 | Loss: 0.00001170
Iteration 36/1000 | Loss: 0.00001170
Iteration 37/1000 | Loss: 0.00001169
Iteration 38/1000 | Loss: 0.00001169
Iteration 39/1000 | Loss: 0.00001169
Iteration 40/1000 | Loss: 0.00001169
Iteration 41/1000 | Loss: 0.00001169
Iteration 42/1000 | Loss: 0.00001169
Iteration 43/1000 | Loss: 0.00001169
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001169
Iteration 46/1000 | Loss: 0.00001169
Iteration 47/1000 | Loss: 0.00001169
Iteration 48/1000 | Loss: 0.00001169
Iteration 49/1000 | Loss: 0.00001169
Iteration 50/1000 | Loss: 0.00001169
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001169
Iteration 53/1000 | Loss: 0.00001169
Iteration 54/1000 | Loss: 0.00001169
Iteration 55/1000 | Loss: 0.00001169
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001169
Iteration 61/1000 | Loss: 0.00001169
Iteration 62/1000 | Loss: 0.00001169
Iteration 63/1000 | Loss: 0.00001169
Iteration 64/1000 | Loss: 0.00001169
Iteration 65/1000 | Loss: 0.00001169
Iteration 66/1000 | Loss: 0.00001169
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001169
Iteration 70/1000 | Loss: 0.00001169
Iteration 71/1000 | Loss: 0.00001169
Iteration 72/1000 | Loss: 0.00001169
Iteration 73/1000 | Loss: 0.00001169
Iteration 74/1000 | Loss: 0.00001169
Iteration 75/1000 | Loss: 0.00001169
Iteration 76/1000 | Loss: 0.00001169
Iteration 77/1000 | Loss: 0.00001169
Iteration 78/1000 | Loss: 0.00001169
Iteration 79/1000 | Loss: 0.00001169
Iteration 80/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.1689607163134497e-05, 1.1689607163134497e-05, 1.1689607163134497e-05, 1.1689607163134497e-05, 1.1689607163134497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1689607163134497e-05

Optimization complete. Final v2v error: 2.9129703044891357 mm

Highest mean error: 3.0443952083587646 mm for frame 70

Lowest mean error: 2.7679145336151123 mm for frame 145

Saving results

Total time: 32.75835299491882
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788324
Iteration 2/25 | Loss: 0.00111381
Iteration 3/25 | Loss: 0.00082183
Iteration 4/25 | Loss: 0.00076790
Iteration 5/25 | Loss: 0.00074198
Iteration 6/25 | Loss: 0.00073426
Iteration 7/25 | Loss: 0.00073087
Iteration 8/25 | Loss: 0.00072768
Iteration 9/25 | Loss: 0.00072737
Iteration 10/25 | Loss: 0.00072728
Iteration 11/25 | Loss: 0.00072728
Iteration 12/25 | Loss: 0.00072728
Iteration 13/25 | Loss: 0.00072728
Iteration 14/25 | Loss: 0.00072728
Iteration 15/25 | Loss: 0.00072728
Iteration 16/25 | Loss: 0.00072728
Iteration 17/25 | Loss: 0.00072728
Iteration 18/25 | Loss: 0.00072728
Iteration 19/25 | Loss: 0.00072728
Iteration 20/25 | Loss: 0.00072727
Iteration 21/25 | Loss: 0.00072727
Iteration 22/25 | Loss: 0.00072727
Iteration 23/25 | Loss: 0.00072727
Iteration 24/25 | Loss: 0.00072727
Iteration 25/25 | Loss: 0.00072727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.28725839
Iteration 2/25 | Loss: 0.00039709
Iteration 3/25 | Loss: 0.00039177
Iteration 4/25 | Loss: 0.00039177
Iteration 5/25 | Loss: 0.00039177
Iteration 6/25 | Loss: 0.00039177
Iteration 7/25 | Loss: 0.00039177
Iteration 8/25 | Loss: 0.00039177
Iteration 9/25 | Loss: 0.00039177
Iteration 10/25 | Loss: 0.00039177
Iteration 11/25 | Loss: 0.00039177
Iteration 12/25 | Loss: 0.00039177
Iteration 13/25 | Loss: 0.00039177
Iteration 14/25 | Loss: 0.00039177
Iteration 15/25 | Loss: 0.00039177
Iteration 16/25 | Loss: 0.00039177
Iteration 17/25 | Loss: 0.00039177
Iteration 18/25 | Loss: 0.00039177
Iteration 19/25 | Loss: 0.00039177
Iteration 20/25 | Loss: 0.00039177
Iteration 21/25 | Loss: 0.00039177
Iteration 22/25 | Loss: 0.00039177
Iteration 23/25 | Loss: 0.00039177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003917713765986264, 0.0003917713765986264, 0.0003917713765986264, 0.0003917713765986264, 0.0003917713765986264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003917713765986264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039177
Iteration 2/1000 | Loss: 0.00003837
Iteration 3/1000 | Loss: 0.00002093
Iteration 4/1000 | Loss: 0.00001865
Iteration 5/1000 | Loss: 0.00001759
Iteration 6/1000 | Loss: 0.00001724
Iteration 7/1000 | Loss: 0.00001698
Iteration 8/1000 | Loss: 0.00001683
Iteration 9/1000 | Loss: 0.00001674
Iteration 10/1000 | Loss: 0.00001662
Iteration 11/1000 | Loss: 0.00001658
Iteration 12/1000 | Loss: 0.00001654
Iteration 13/1000 | Loss: 0.00001654
Iteration 14/1000 | Loss: 0.00001653
Iteration 15/1000 | Loss: 0.00001646
Iteration 16/1000 | Loss: 0.00001645
Iteration 17/1000 | Loss: 0.00001644
Iteration 18/1000 | Loss: 0.00001644
Iteration 19/1000 | Loss: 0.00001644
Iteration 20/1000 | Loss: 0.00001643
Iteration 21/1000 | Loss: 0.00001641
Iteration 22/1000 | Loss: 0.00001640
Iteration 23/1000 | Loss: 0.00001640
Iteration 24/1000 | Loss: 0.00001640
Iteration 25/1000 | Loss: 0.00001640
Iteration 26/1000 | Loss: 0.00001640
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001639
Iteration 30/1000 | Loss: 0.00001638
Iteration 31/1000 | Loss: 0.00001638
Iteration 32/1000 | Loss: 0.00001637
Iteration 33/1000 | Loss: 0.00001637
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001636
Iteration 36/1000 | Loss: 0.00001635
Iteration 37/1000 | Loss: 0.00001635
Iteration 38/1000 | Loss: 0.00001634
Iteration 39/1000 | Loss: 0.00001634
Iteration 40/1000 | Loss: 0.00001634
Iteration 41/1000 | Loss: 0.00001634
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001631
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001630
Iteration 46/1000 | Loss: 0.00001630
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001627
Iteration 54/1000 | Loss: 0.00001627
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00001627
Iteration 58/1000 | Loss: 0.00001627
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001626
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001625
Iteration 65/1000 | Loss: 0.00001625
Iteration 66/1000 | Loss: 0.00001625
Iteration 67/1000 | Loss: 0.00001625
Iteration 68/1000 | Loss: 0.00001625
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001624
Iteration 71/1000 | Loss: 0.00001624
Iteration 72/1000 | Loss: 0.00001623
Iteration 73/1000 | Loss: 0.00001623
Iteration 74/1000 | Loss: 0.00001623
Iteration 75/1000 | Loss: 0.00001623
Iteration 76/1000 | Loss: 0.00001623
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001623
Iteration 79/1000 | Loss: 0.00001623
Iteration 80/1000 | Loss: 0.00001623
Iteration 81/1000 | Loss: 0.00001623
Iteration 82/1000 | Loss: 0.00001623
Iteration 83/1000 | Loss: 0.00001623
Iteration 84/1000 | Loss: 0.00001623
Iteration 85/1000 | Loss: 0.00001623
Iteration 86/1000 | Loss: 0.00001623
Iteration 87/1000 | Loss: 0.00001623
Iteration 88/1000 | Loss: 0.00001623
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001622
Iteration 94/1000 | Loss: 0.00001622
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001621
Iteration 97/1000 | Loss: 0.00001621
Iteration 98/1000 | Loss: 0.00001621
Iteration 99/1000 | Loss: 0.00001621
Iteration 100/1000 | Loss: 0.00001620
Iteration 101/1000 | Loss: 0.00001620
Iteration 102/1000 | Loss: 0.00001620
Iteration 103/1000 | Loss: 0.00001620
Iteration 104/1000 | Loss: 0.00001620
Iteration 105/1000 | Loss: 0.00001620
Iteration 106/1000 | Loss: 0.00001620
Iteration 107/1000 | Loss: 0.00001620
Iteration 108/1000 | Loss: 0.00001620
Iteration 109/1000 | Loss: 0.00001620
Iteration 110/1000 | Loss: 0.00001619
Iteration 111/1000 | Loss: 0.00001619
Iteration 112/1000 | Loss: 0.00001619
Iteration 113/1000 | Loss: 0.00001619
Iteration 114/1000 | Loss: 0.00001619
Iteration 115/1000 | Loss: 0.00001619
Iteration 116/1000 | Loss: 0.00001619
Iteration 117/1000 | Loss: 0.00001619
Iteration 118/1000 | Loss: 0.00001619
Iteration 119/1000 | Loss: 0.00001619
Iteration 120/1000 | Loss: 0.00001619
Iteration 121/1000 | Loss: 0.00001618
Iteration 122/1000 | Loss: 0.00001618
Iteration 123/1000 | Loss: 0.00001618
Iteration 124/1000 | Loss: 0.00001618
Iteration 125/1000 | Loss: 0.00001618
Iteration 126/1000 | Loss: 0.00001618
Iteration 127/1000 | Loss: 0.00001618
Iteration 128/1000 | Loss: 0.00001618
Iteration 129/1000 | Loss: 0.00001618
Iteration 130/1000 | Loss: 0.00001618
Iteration 131/1000 | Loss: 0.00001618
Iteration 132/1000 | Loss: 0.00001618
Iteration 133/1000 | Loss: 0.00001618
Iteration 134/1000 | Loss: 0.00001618
Iteration 135/1000 | Loss: 0.00001618
Iteration 136/1000 | Loss: 0.00001618
Iteration 137/1000 | Loss: 0.00001618
Iteration 138/1000 | Loss: 0.00001618
Iteration 139/1000 | Loss: 0.00001617
Iteration 140/1000 | Loss: 0.00001617
Iteration 141/1000 | Loss: 0.00001617
Iteration 142/1000 | Loss: 0.00001617
Iteration 143/1000 | Loss: 0.00001617
Iteration 144/1000 | Loss: 0.00001617
Iteration 145/1000 | Loss: 0.00001617
Iteration 146/1000 | Loss: 0.00001617
Iteration 147/1000 | Loss: 0.00001617
Iteration 148/1000 | Loss: 0.00001617
Iteration 149/1000 | Loss: 0.00001617
Iteration 150/1000 | Loss: 0.00001617
Iteration 151/1000 | Loss: 0.00001617
Iteration 152/1000 | Loss: 0.00001617
Iteration 153/1000 | Loss: 0.00001617
Iteration 154/1000 | Loss: 0.00001616
Iteration 155/1000 | Loss: 0.00001616
Iteration 156/1000 | Loss: 0.00001616
Iteration 157/1000 | Loss: 0.00001616
Iteration 158/1000 | Loss: 0.00001616
Iteration 159/1000 | Loss: 0.00001616
Iteration 160/1000 | Loss: 0.00001616
Iteration 161/1000 | Loss: 0.00001616
Iteration 162/1000 | Loss: 0.00001616
Iteration 163/1000 | Loss: 0.00001616
Iteration 164/1000 | Loss: 0.00001616
Iteration 165/1000 | Loss: 0.00001616
Iteration 166/1000 | Loss: 0.00001616
Iteration 167/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.6157671780092642e-05, 1.6157671780092642e-05, 1.6157671780092642e-05, 1.6157671780092642e-05, 1.6157671780092642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6157671780092642e-05

Optimization complete. Final v2v error: 3.38785457611084 mm

Highest mean error: 3.782766342163086 mm for frame 186

Lowest mean error: 3.15968918800354 mm for frame 12

Saving results

Total time: 44.19517946243286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853019
Iteration 2/25 | Loss: 0.00087259
Iteration 3/25 | Loss: 0.00069689
Iteration 4/25 | Loss: 0.00066324
Iteration 5/25 | Loss: 0.00065285
Iteration 6/25 | Loss: 0.00065075
Iteration 7/25 | Loss: 0.00065017
Iteration 8/25 | Loss: 0.00065017
Iteration 9/25 | Loss: 0.00065017
Iteration 10/25 | Loss: 0.00065017
Iteration 11/25 | Loss: 0.00065017
Iteration 12/25 | Loss: 0.00065017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006501720054075122, 0.0006501720054075122, 0.0006501720054075122, 0.0006501720054075122, 0.0006501720054075122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006501720054075122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.29748797
Iteration 2/25 | Loss: 0.00029416
Iteration 3/25 | Loss: 0.00029416
Iteration 4/25 | Loss: 0.00029416
Iteration 5/25 | Loss: 0.00029416
Iteration 6/25 | Loss: 0.00029416
Iteration 7/25 | Loss: 0.00029416
Iteration 8/25 | Loss: 0.00029416
Iteration 9/25 | Loss: 0.00029416
Iteration 10/25 | Loss: 0.00029416
Iteration 11/25 | Loss: 0.00029416
Iteration 12/25 | Loss: 0.00029416
Iteration 13/25 | Loss: 0.00029416
Iteration 14/25 | Loss: 0.00029416
Iteration 15/25 | Loss: 0.00029416
Iteration 16/25 | Loss: 0.00029416
Iteration 17/25 | Loss: 0.00029416
Iteration 18/25 | Loss: 0.00029416
Iteration 19/25 | Loss: 0.00029416
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000294161174679175, 0.000294161174679175, 0.000294161174679175, 0.000294161174679175, 0.000294161174679175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000294161174679175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029416
Iteration 2/1000 | Loss: 0.00002625
Iteration 3/1000 | Loss: 0.00001651
Iteration 4/1000 | Loss: 0.00001533
Iteration 5/1000 | Loss: 0.00001445
Iteration 6/1000 | Loss: 0.00001401
Iteration 7/1000 | Loss: 0.00001367
Iteration 8/1000 | Loss: 0.00001348
Iteration 9/1000 | Loss: 0.00001346
Iteration 10/1000 | Loss: 0.00001339
Iteration 11/1000 | Loss: 0.00001337
Iteration 12/1000 | Loss: 0.00001337
Iteration 13/1000 | Loss: 0.00001337
Iteration 14/1000 | Loss: 0.00001327
Iteration 15/1000 | Loss: 0.00001325
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001315
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001313
Iteration 20/1000 | Loss: 0.00001313
Iteration 21/1000 | Loss: 0.00001312
Iteration 22/1000 | Loss: 0.00001312
Iteration 23/1000 | Loss: 0.00001311
Iteration 24/1000 | Loss: 0.00001311
Iteration 25/1000 | Loss: 0.00001310
Iteration 26/1000 | Loss: 0.00001310
Iteration 27/1000 | Loss: 0.00001309
Iteration 28/1000 | Loss: 0.00001309
Iteration 29/1000 | Loss: 0.00001309
Iteration 30/1000 | Loss: 0.00001308
Iteration 31/1000 | Loss: 0.00001308
Iteration 32/1000 | Loss: 0.00001308
Iteration 33/1000 | Loss: 0.00001307
Iteration 34/1000 | Loss: 0.00001307
Iteration 35/1000 | Loss: 0.00001307
Iteration 36/1000 | Loss: 0.00001306
Iteration 37/1000 | Loss: 0.00001306
Iteration 38/1000 | Loss: 0.00001305
Iteration 39/1000 | Loss: 0.00001305
Iteration 40/1000 | Loss: 0.00001304
Iteration 41/1000 | Loss: 0.00001304
Iteration 42/1000 | Loss: 0.00001303
Iteration 43/1000 | Loss: 0.00001302
Iteration 44/1000 | Loss: 0.00001302
Iteration 45/1000 | Loss: 0.00001302
Iteration 46/1000 | Loss: 0.00001302
Iteration 47/1000 | Loss: 0.00001302
Iteration 48/1000 | Loss: 0.00001302
Iteration 49/1000 | Loss: 0.00001302
Iteration 50/1000 | Loss: 0.00001302
Iteration 51/1000 | Loss: 0.00001302
Iteration 52/1000 | Loss: 0.00001302
Iteration 53/1000 | Loss: 0.00001302
Iteration 54/1000 | Loss: 0.00001301
Iteration 55/1000 | Loss: 0.00001301
Iteration 56/1000 | Loss: 0.00001301
Iteration 57/1000 | Loss: 0.00001301
Iteration 58/1000 | Loss: 0.00001301
Iteration 59/1000 | Loss: 0.00001301
Iteration 60/1000 | Loss: 0.00001301
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001301
Iteration 63/1000 | Loss: 0.00001301
Iteration 64/1000 | Loss: 0.00001301
Iteration 65/1000 | Loss: 0.00001300
Iteration 66/1000 | Loss: 0.00001300
Iteration 67/1000 | Loss: 0.00001300
Iteration 68/1000 | Loss: 0.00001299
Iteration 69/1000 | Loss: 0.00001299
Iteration 70/1000 | Loss: 0.00001298
Iteration 71/1000 | Loss: 0.00001298
Iteration 72/1000 | Loss: 0.00001298
Iteration 73/1000 | Loss: 0.00001298
Iteration 74/1000 | Loss: 0.00001298
Iteration 75/1000 | Loss: 0.00001298
Iteration 76/1000 | Loss: 0.00001297
Iteration 77/1000 | Loss: 0.00001297
Iteration 78/1000 | Loss: 0.00001297
Iteration 79/1000 | Loss: 0.00001297
Iteration 80/1000 | Loss: 0.00001297
Iteration 81/1000 | Loss: 0.00001297
Iteration 82/1000 | Loss: 0.00001297
Iteration 83/1000 | Loss: 0.00001296
Iteration 84/1000 | Loss: 0.00001296
Iteration 85/1000 | Loss: 0.00001296
Iteration 86/1000 | Loss: 0.00001296
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001295
Iteration 89/1000 | Loss: 0.00001295
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001295
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001294
Iteration 95/1000 | Loss: 0.00001294
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001293
Iteration 100/1000 | Loss: 0.00001293
Iteration 101/1000 | Loss: 0.00001293
Iteration 102/1000 | Loss: 0.00001293
Iteration 103/1000 | Loss: 0.00001293
Iteration 104/1000 | Loss: 0.00001293
Iteration 105/1000 | Loss: 0.00001293
Iteration 106/1000 | Loss: 0.00001293
Iteration 107/1000 | Loss: 0.00001293
Iteration 108/1000 | Loss: 0.00001293
Iteration 109/1000 | Loss: 0.00001293
Iteration 110/1000 | Loss: 0.00001293
Iteration 111/1000 | Loss: 0.00001293
Iteration 112/1000 | Loss: 0.00001292
Iteration 113/1000 | Loss: 0.00001292
Iteration 114/1000 | Loss: 0.00001292
Iteration 115/1000 | Loss: 0.00001292
Iteration 116/1000 | Loss: 0.00001292
Iteration 117/1000 | Loss: 0.00001291
Iteration 118/1000 | Loss: 0.00001291
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001291
Iteration 121/1000 | Loss: 0.00001291
Iteration 122/1000 | Loss: 0.00001291
Iteration 123/1000 | Loss: 0.00001290
Iteration 124/1000 | Loss: 0.00001290
Iteration 125/1000 | Loss: 0.00001290
Iteration 126/1000 | Loss: 0.00001290
Iteration 127/1000 | Loss: 0.00001290
Iteration 128/1000 | Loss: 0.00001289
Iteration 129/1000 | Loss: 0.00001289
Iteration 130/1000 | Loss: 0.00001289
Iteration 131/1000 | Loss: 0.00001287
Iteration 132/1000 | Loss: 0.00001287
Iteration 133/1000 | Loss: 0.00001287
Iteration 134/1000 | Loss: 0.00001287
Iteration 135/1000 | Loss: 0.00001287
Iteration 136/1000 | Loss: 0.00001287
Iteration 137/1000 | Loss: 0.00001287
Iteration 138/1000 | Loss: 0.00001287
Iteration 139/1000 | Loss: 0.00001287
Iteration 140/1000 | Loss: 0.00001286
Iteration 141/1000 | Loss: 0.00001286
Iteration 142/1000 | Loss: 0.00001286
Iteration 143/1000 | Loss: 0.00001286
Iteration 144/1000 | Loss: 0.00001285
Iteration 145/1000 | Loss: 0.00001285
Iteration 146/1000 | Loss: 0.00001285
Iteration 147/1000 | Loss: 0.00001285
Iteration 148/1000 | Loss: 0.00001285
Iteration 149/1000 | Loss: 0.00001285
Iteration 150/1000 | Loss: 0.00001284
Iteration 151/1000 | Loss: 0.00001284
Iteration 152/1000 | Loss: 0.00001284
Iteration 153/1000 | Loss: 0.00001284
Iteration 154/1000 | Loss: 0.00001284
Iteration 155/1000 | Loss: 0.00001284
Iteration 156/1000 | Loss: 0.00001284
Iteration 157/1000 | Loss: 0.00001284
Iteration 158/1000 | Loss: 0.00001284
Iteration 159/1000 | Loss: 0.00001284
Iteration 160/1000 | Loss: 0.00001284
Iteration 161/1000 | Loss: 0.00001283
Iteration 162/1000 | Loss: 0.00001283
Iteration 163/1000 | Loss: 0.00001283
Iteration 164/1000 | Loss: 0.00001283
Iteration 165/1000 | Loss: 0.00001283
Iteration 166/1000 | Loss: 0.00001283
Iteration 167/1000 | Loss: 0.00001283
Iteration 168/1000 | Loss: 0.00001283
Iteration 169/1000 | Loss: 0.00001283
Iteration 170/1000 | Loss: 0.00001283
Iteration 171/1000 | Loss: 0.00001283
Iteration 172/1000 | Loss: 0.00001283
Iteration 173/1000 | Loss: 0.00001283
Iteration 174/1000 | Loss: 0.00001283
Iteration 175/1000 | Loss: 0.00001283
Iteration 176/1000 | Loss: 0.00001283
Iteration 177/1000 | Loss: 0.00001283
Iteration 178/1000 | Loss: 0.00001283
Iteration 179/1000 | Loss: 0.00001283
Iteration 180/1000 | Loss: 0.00001283
Iteration 181/1000 | Loss: 0.00001283
Iteration 182/1000 | Loss: 0.00001283
Iteration 183/1000 | Loss: 0.00001283
Iteration 184/1000 | Loss: 0.00001283
Iteration 185/1000 | Loss: 0.00001283
Iteration 186/1000 | Loss: 0.00001283
Iteration 187/1000 | Loss: 0.00001283
Iteration 188/1000 | Loss: 0.00001283
Iteration 189/1000 | Loss: 0.00001283
Iteration 190/1000 | Loss: 0.00001283
Iteration 191/1000 | Loss: 0.00001283
Iteration 192/1000 | Loss: 0.00001283
Iteration 193/1000 | Loss: 0.00001283
Iteration 194/1000 | Loss: 0.00001283
Iteration 195/1000 | Loss: 0.00001283
Iteration 196/1000 | Loss: 0.00001283
Iteration 197/1000 | Loss: 0.00001283
Iteration 198/1000 | Loss: 0.00001283
Iteration 199/1000 | Loss: 0.00001283
Iteration 200/1000 | Loss: 0.00001283
Iteration 201/1000 | Loss: 0.00001283
Iteration 202/1000 | Loss: 0.00001283
Iteration 203/1000 | Loss: 0.00001283
Iteration 204/1000 | Loss: 0.00001283
Iteration 205/1000 | Loss: 0.00001283
Iteration 206/1000 | Loss: 0.00001283
Iteration 207/1000 | Loss: 0.00001283
Iteration 208/1000 | Loss: 0.00001283
Iteration 209/1000 | Loss: 0.00001283
Iteration 210/1000 | Loss: 0.00001283
Iteration 211/1000 | Loss: 0.00001283
Iteration 212/1000 | Loss: 0.00001283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.2833947948820423e-05, 1.2833947948820423e-05, 1.2833947948820423e-05, 1.2833947948820423e-05, 1.2833947948820423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2833947948820423e-05

Optimization complete. Final v2v error: 3.0044000148773193 mm

Highest mean error: 3.8288493156433105 mm for frame 56

Lowest mean error: 2.6768789291381836 mm for frame 99

Saving results

Total time: 37.63984417915344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868914
Iteration 2/25 | Loss: 0.00104610
Iteration 3/25 | Loss: 0.00074902
Iteration 4/25 | Loss: 0.00070250
Iteration 5/25 | Loss: 0.00069153
Iteration 6/25 | Loss: 0.00068848
Iteration 7/25 | Loss: 0.00068783
Iteration 8/25 | Loss: 0.00068783
Iteration 9/25 | Loss: 0.00068783
Iteration 10/25 | Loss: 0.00068783
Iteration 11/25 | Loss: 0.00068783
Iteration 12/25 | Loss: 0.00068783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000687833467964083, 0.000687833467964083, 0.000687833467964083, 0.000687833467964083, 0.000687833467964083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000687833467964083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50252140
Iteration 2/25 | Loss: 0.00031722
Iteration 3/25 | Loss: 0.00031722
Iteration 4/25 | Loss: 0.00031722
Iteration 5/25 | Loss: 0.00031722
Iteration 6/25 | Loss: 0.00031722
Iteration 7/25 | Loss: 0.00031722
Iteration 8/25 | Loss: 0.00031722
Iteration 9/25 | Loss: 0.00031722
Iteration 10/25 | Loss: 0.00031722
Iteration 11/25 | Loss: 0.00031722
Iteration 12/25 | Loss: 0.00031722
Iteration 13/25 | Loss: 0.00031722
Iteration 14/25 | Loss: 0.00031722
Iteration 15/25 | Loss: 0.00031722
Iteration 16/25 | Loss: 0.00031722
Iteration 17/25 | Loss: 0.00031722
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00031721603590995073, 0.00031721603590995073, 0.00031721603590995073, 0.00031721603590995073, 0.00031721603590995073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031721603590995073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031722
Iteration 2/1000 | Loss: 0.00002203
Iteration 3/1000 | Loss: 0.00001467
Iteration 4/1000 | Loss: 0.00001349
Iteration 5/1000 | Loss: 0.00001285
Iteration 6/1000 | Loss: 0.00001220
Iteration 7/1000 | Loss: 0.00001190
Iteration 8/1000 | Loss: 0.00001185
Iteration 9/1000 | Loss: 0.00001165
Iteration 10/1000 | Loss: 0.00001157
Iteration 11/1000 | Loss: 0.00001140
Iteration 12/1000 | Loss: 0.00001136
Iteration 13/1000 | Loss: 0.00001131
Iteration 14/1000 | Loss: 0.00001129
Iteration 15/1000 | Loss: 0.00001129
Iteration 16/1000 | Loss: 0.00001128
Iteration 17/1000 | Loss: 0.00001128
Iteration 18/1000 | Loss: 0.00001128
Iteration 19/1000 | Loss: 0.00001127
Iteration 20/1000 | Loss: 0.00001123
Iteration 21/1000 | Loss: 0.00001122
Iteration 22/1000 | Loss: 0.00001120
Iteration 23/1000 | Loss: 0.00001120
Iteration 24/1000 | Loss: 0.00001119
Iteration 25/1000 | Loss: 0.00001119
Iteration 26/1000 | Loss: 0.00001118
Iteration 27/1000 | Loss: 0.00001118
Iteration 28/1000 | Loss: 0.00001118
Iteration 29/1000 | Loss: 0.00001118
Iteration 30/1000 | Loss: 0.00001118
Iteration 31/1000 | Loss: 0.00001118
Iteration 32/1000 | Loss: 0.00001118
Iteration 33/1000 | Loss: 0.00001118
Iteration 34/1000 | Loss: 0.00001118
Iteration 35/1000 | Loss: 0.00001118
Iteration 36/1000 | Loss: 0.00001117
Iteration 37/1000 | Loss: 0.00001117
Iteration 38/1000 | Loss: 0.00001117
Iteration 39/1000 | Loss: 0.00001116
Iteration 40/1000 | Loss: 0.00001116
Iteration 41/1000 | Loss: 0.00001116
Iteration 42/1000 | Loss: 0.00001115
Iteration 43/1000 | Loss: 0.00001115
Iteration 44/1000 | Loss: 0.00001115
Iteration 45/1000 | Loss: 0.00001115
Iteration 46/1000 | Loss: 0.00001115
Iteration 47/1000 | Loss: 0.00001115
Iteration 48/1000 | Loss: 0.00001115
Iteration 49/1000 | Loss: 0.00001115
Iteration 50/1000 | Loss: 0.00001115
Iteration 51/1000 | Loss: 0.00001115
Iteration 52/1000 | Loss: 0.00001115
Iteration 53/1000 | Loss: 0.00001115
Iteration 54/1000 | Loss: 0.00001114
Iteration 55/1000 | Loss: 0.00001114
Iteration 56/1000 | Loss: 0.00001113
Iteration 57/1000 | Loss: 0.00001113
Iteration 58/1000 | Loss: 0.00001113
Iteration 59/1000 | Loss: 0.00001113
Iteration 60/1000 | Loss: 0.00001112
Iteration 61/1000 | Loss: 0.00001112
Iteration 62/1000 | Loss: 0.00001112
Iteration 63/1000 | Loss: 0.00001111
Iteration 64/1000 | Loss: 0.00001111
Iteration 65/1000 | Loss: 0.00001111
Iteration 66/1000 | Loss: 0.00001111
Iteration 67/1000 | Loss: 0.00001110
Iteration 68/1000 | Loss: 0.00001110
Iteration 69/1000 | Loss: 0.00001110
Iteration 70/1000 | Loss: 0.00001110
Iteration 71/1000 | Loss: 0.00001110
Iteration 72/1000 | Loss: 0.00001110
Iteration 73/1000 | Loss: 0.00001110
Iteration 74/1000 | Loss: 0.00001110
Iteration 75/1000 | Loss: 0.00001110
Iteration 76/1000 | Loss: 0.00001110
Iteration 77/1000 | Loss: 0.00001110
Iteration 78/1000 | Loss: 0.00001110
Iteration 79/1000 | Loss: 0.00001110
Iteration 80/1000 | Loss: 0.00001110
Iteration 81/1000 | Loss: 0.00001110
Iteration 82/1000 | Loss: 0.00001110
Iteration 83/1000 | Loss: 0.00001110
Iteration 84/1000 | Loss: 0.00001110
Iteration 85/1000 | Loss: 0.00001110
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.110014727601083e-05, 1.110014727601083e-05, 1.110014727601083e-05, 1.110014727601083e-05, 1.110014727601083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.110014727601083e-05

Optimization complete. Final v2v error: 2.8558197021484375 mm

Highest mean error: 3.2541873455047607 mm for frame 133

Lowest mean error: 2.6435647010803223 mm for frame 30

Saving results

Total time: 34.98044729232788
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864951
Iteration 2/25 | Loss: 0.00096717
Iteration 3/25 | Loss: 0.00078157
Iteration 4/25 | Loss: 0.00074546
Iteration 5/25 | Loss: 0.00073442
Iteration 6/25 | Loss: 0.00073081
Iteration 7/25 | Loss: 0.00072968
Iteration 8/25 | Loss: 0.00072917
Iteration 9/25 | Loss: 0.00073115
Iteration 10/25 | Loss: 0.00073100
Iteration 11/25 | Loss: 0.00072864
Iteration 12/25 | Loss: 0.00073080
Iteration 13/25 | Loss: 0.00073081
Iteration 14/25 | Loss: 0.00073105
Iteration 15/25 | Loss: 0.00073051
Iteration 16/25 | Loss: 0.00073346
Iteration 17/25 | Loss: 0.00073202
Iteration 18/25 | Loss: 0.00072982
Iteration 19/25 | Loss: 0.00073026
Iteration 20/25 | Loss: 0.00072612
Iteration 21/25 | Loss: 0.00072409
Iteration 22/25 | Loss: 0.00072292
Iteration 23/25 | Loss: 0.00072242
Iteration 24/25 | Loss: 0.00072224
Iteration 25/25 | Loss: 0.00072205

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48665476
Iteration 2/25 | Loss: 0.00106599
Iteration 3/25 | Loss: 0.00106598
Iteration 4/25 | Loss: 0.00106598
Iteration 5/25 | Loss: 0.00106598
Iteration 6/25 | Loss: 0.00106598
Iteration 7/25 | Loss: 0.00106598
Iteration 8/25 | Loss: 0.00106598
Iteration 9/25 | Loss: 0.00106598
Iteration 10/25 | Loss: 0.00106598
Iteration 11/25 | Loss: 0.00106598
Iteration 12/25 | Loss: 0.00106598
Iteration 13/25 | Loss: 0.00106598
Iteration 14/25 | Loss: 0.00106598
Iteration 15/25 | Loss: 0.00106598
Iteration 16/25 | Loss: 0.00106598
Iteration 17/25 | Loss: 0.00106598
Iteration 18/25 | Loss: 0.00106598
Iteration 19/25 | Loss: 0.00106598
Iteration 20/25 | Loss: 0.00106598
Iteration 21/25 | Loss: 0.00106598
Iteration 22/25 | Loss: 0.00106598
Iteration 23/25 | Loss: 0.00106598
Iteration 24/25 | Loss: 0.00106598
Iteration 25/25 | Loss: 0.00106598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106598
Iteration 2/1000 | Loss: 0.00011687
Iteration 3/1000 | Loss: 0.00007896
Iteration 4/1000 | Loss: 0.00006815
Iteration 5/1000 | Loss: 0.00006170
Iteration 6/1000 | Loss: 0.00005832
Iteration 7/1000 | Loss: 0.00005670
Iteration 8/1000 | Loss: 0.00005493
Iteration 9/1000 | Loss: 0.00005295
Iteration 10/1000 | Loss: 0.00005175
Iteration 11/1000 | Loss: 0.00005111
Iteration 12/1000 | Loss: 0.00005057
Iteration 13/1000 | Loss: 0.00004996
Iteration 14/1000 | Loss: 0.00004928
Iteration 15/1000 | Loss: 0.00004887
Iteration 16/1000 | Loss: 0.00004845
Iteration 17/1000 | Loss: 0.00035298
Iteration 18/1000 | Loss: 0.00014077
Iteration 19/1000 | Loss: 0.00004839
Iteration 20/1000 | Loss: 0.00004775
Iteration 21/1000 | Loss: 0.00105632
Iteration 22/1000 | Loss: 0.00048569
Iteration 23/1000 | Loss: 0.00004963
Iteration 24/1000 | Loss: 0.00004772
Iteration 25/1000 | Loss: 0.00095002
Iteration 26/1000 | Loss: 0.00030537
Iteration 27/1000 | Loss: 0.00004936
Iteration 28/1000 | Loss: 0.00078823
Iteration 29/1000 | Loss: 0.00018986
Iteration 30/1000 | Loss: 0.00005949
Iteration 31/1000 | Loss: 0.00005285
Iteration 32/1000 | Loss: 0.00004940
Iteration 33/1000 | Loss: 0.00088221
Iteration 34/1000 | Loss: 0.00029876
Iteration 35/1000 | Loss: 0.00004949
Iteration 36/1000 | Loss: 0.00029245
Iteration 37/1000 | Loss: 0.00005172
Iteration 38/1000 | Loss: 0.00087110
Iteration 39/1000 | Loss: 0.00032296
Iteration 40/1000 | Loss: 0.00012553
Iteration 41/1000 | Loss: 0.00004680
Iteration 42/1000 | Loss: 0.00091705
Iteration 43/1000 | Loss: 0.00038548
Iteration 44/1000 | Loss: 0.00008051
Iteration 45/1000 | Loss: 0.00075146
Iteration 46/1000 | Loss: 0.00031186
Iteration 47/1000 | Loss: 0.00009003
Iteration 48/1000 | Loss: 0.00005429
Iteration 49/1000 | Loss: 0.00004922
Iteration 50/1000 | Loss: 0.00004650
Iteration 51/1000 | Loss: 0.00004427
Iteration 52/1000 | Loss: 0.00097447
Iteration 53/1000 | Loss: 0.00019416
Iteration 54/1000 | Loss: 0.00033962
Iteration 55/1000 | Loss: 0.00004384
Iteration 56/1000 | Loss: 0.00032324
Iteration 57/1000 | Loss: 0.00014227
Iteration 58/1000 | Loss: 0.00023020
Iteration 59/1000 | Loss: 0.00004493
Iteration 60/1000 | Loss: 0.00004165
Iteration 61/1000 | Loss: 0.00004050
Iteration 62/1000 | Loss: 0.00003966
Iteration 63/1000 | Loss: 0.00046112
Iteration 64/1000 | Loss: 0.00004176
Iteration 65/1000 | Loss: 0.00003833
Iteration 66/1000 | Loss: 0.00003772
Iteration 67/1000 | Loss: 0.00003720
Iteration 68/1000 | Loss: 0.00003661
Iteration 69/1000 | Loss: 0.00003627
Iteration 70/1000 | Loss: 0.00003606
Iteration 71/1000 | Loss: 0.00003600
Iteration 72/1000 | Loss: 0.00003590
Iteration 73/1000 | Loss: 0.00003589
Iteration 74/1000 | Loss: 0.00003589
Iteration 75/1000 | Loss: 0.00003588
Iteration 76/1000 | Loss: 0.00003582
Iteration 77/1000 | Loss: 0.00003581
Iteration 78/1000 | Loss: 0.00003581
Iteration 79/1000 | Loss: 0.00003578
Iteration 80/1000 | Loss: 0.00003575
Iteration 81/1000 | Loss: 0.00003574
Iteration 82/1000 | Loss: 0.00003573
Iteration 83/1000 | Loss: 0.00003573
Iteration 84/1000 | Loss: 0.00003572
Iteration 85/1000 | Loss: 0.00003571
Iteration 86/1000 | Loss: 0.00003569
Iteration 87/1000 | Loss: 0.00003569
Iteration 88/1000 | Loss: 0.00003568
Iteration 89/1000 | Loss: 0.00003566
Iteration 90/1000 | Loss: 0.00003565
Iteration 91/1000 | Loss: 0.00003564
Iteration 92/1000 | Loss: 0.00003563
Iteration 93/1000 | Loss: 0.00003563
Iteration 94/1000 | Loss: 0.00003563
Iteration 95/1000 | Loss: 0.00003563
Iteration 96/1000 | Loss: 0.00003562
Iteration 97/1000 | Loss: 0.00003562
Iteration 98/1000 | Loss: 0.00003561
Iteration 99/1000 | Loss: 0.00003561
Iteration 100/1000 | Loss: 0.00003560
Iteration 101/1000 | Loss: 0.00003560
Iteration 102/1000 | Loss: 0.00003560
Iteration 103/1000 | Loss: 0.00003559
Iteration 104/1000 | Loss: 0.00003552
Iteration 105/1000 | Loss: 0.00003552
Iteration 106/1000 | Loss: 0.00003551
Iteration 107/1000 | Loss: 0.00003551
Iteration 108/1000 | Loss: 0.00003551
Iteration 109/1000 | Loss: 0.00003550
Iteration 110/1000 | Loss: 0.00003550
Iteration 111/1000 | Loss: 0.00003550
Iteration 112/1000 | Loss: 0.00003549
Iteration 113/1000 | Loss: 0.00003549
Iteration 114/1000 | Loss: 0.00003549
Iteration 115/1000 | Loss: 0.00003548
Iteration 116/1000 | Loss: 0.00003548
Iteration 117/1000 | Loss: 0.00003548
Iteration 118/1000 | Loss: 0.00003548
Iteration 119/1000 | Loss: 0.00003548
Iteration 120/1000 | Loss: 0.00003548
Iteration 121/1000 | Loss: 0.00003547
Iteration 122/1000 | Loss: 0.00003547
Iteration 123/1000 | Loss: 0.00003547
Iteration 124/1000 | Loss: 0.00003547
Iteration 125/1000 | Loss: 0.00003547
Iteration 126/1000 | Loss: 0.00003547
Iteration 127/1000 | Loss: 0.00003547
Iteration 128/1000 | Loss: 0.00003546
Iteration 129/1000 | Loss: 0.00003546
Iteration 130/1000 | Loss: 0.00003546
Iteration 131/1000 | Loss: 0.00003546
Iteration 132/1000 | Loss: 0.00003546
Iteration 133/1000 | Loss: 0.00003546
Iteration 134/1000 | Loss: 0.00003546
Iteration 135/1000 | Loss: 0.00003546
Iteration 136/1000 | Loss: 0.00003546
Iteration 137/1000 | Loss: 0.00003546
Iteration 138/1000 | Loss: 0.00003546
Iteration 139/1000 | Loss: 0.00003546
Iteration 140/1000 | Loss: 0.00003546
Iteration 141/1000 | Loss: 0.00003545
Iteration 142/1000 | Loss: 0.00003545
Iteration 143/1000 | Loss: 0.00003545
Iteration 144/1000 | Loss: 0.00003545
Iteration 145/1000 | Loss: 0.00003545
Iteration 146/1000 | Loss: 0.00003545
Iteration 147/1000 | Loss: 0.00003545
Iteration 148/1000 | Loss: 0.00003545
Iteration 149/1000 | Loss: 0.00003545
Iteration 150/1000 | Loss: 0.00003545
Iteration 151/1000 | Loss: 0.00003545
Iteration 152/1000 | Loss: 0.00003545
Iteration 153/1000 | Loss: 0.00003545
Iteration 154/1000 | Loss: 0.00003545
Iteration 155/1000 | Loss: 0.00003545
Iteration 156/1000 | Loss: 0.00003545
Iteration 157/1000 | Loss: 0.00003545
Iteration 158/1000 | Loss: 0.00003545
Iteration 159/1000 | Loss: 0.00003545
Iteration 160/1000 | Loss: 0.00003545
Iteration 161/1000 | Loss: 0.00003544
Iteration 162/1000 | Loss: 0.00003544
Iteration 163/1000 | Loss: 0.00003544
Iteration 164/1000 | Loss: 0.00003544
Iteration 165/1000 | Loss: 0.00003544
Iteration 166/1000 | Loss: 0.00003544
Iteration 167/1000 | Loss: 0.00003544
Iteration 168/1000 | Loss: 0.00003544
Iteration 169/1000 | Loss: 0.00003544
Iteration 170/1000 | Loss: 0.00003544
Iteration 171/1000 | Loss: 0.00003544
Iteration 172/1000 | Loss: 0.00003544
Iteration 173/1000 | Loss: 0.00003544
Iteration 174/1000 | Loss: 0.00003544
Iteration 175/1000 | Loss: 0.00003544
Iteration 176/1000 | Loss: 0.00003544
Iteration 177/1000 | Loss: 0.00003544
Iteration 178/1000 | Loss: 0.00003544
Iteration 179/1000 | Loss: 0.00003544
Iteration 180/1000 | Loss: 0.00003544
Iteration 181/1000 | Loss: 0.00003544
Iteration 182/1000 | Loss: 0.00003544
Iteration 183/1000 | Loss: 0.00003544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [3.5444671084405854e-05, 3.5444671084405854e-05, 3.5444671084405854e-05, 3.5444671084405854e-05, 3.5444671084405854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5444671084405854e-05

Optimization complete. Final v2v error: 3.0678389072418213 mm

Highest mean error: 11.097085952758789 mm for frame 125

Lowest mean error: 2.398116111755371 mm for frame 84

Saving results

Total time: 182.19409084320068
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864078
Iteration 2/25 | Loss: 0.00103383
Iteration 3/25 | Loss: 0.00083623
Iteration 4/25 | Loss: 0.00078628
Iteration 5/25 | Loss: 0.00076012
Iteration 6/25 | Loss: 0.00075394
Iteration 7/25 | Loss: 0.00075157
Iteration 8/25 | Loss: 0.00075055
Iteration 9/25 | Loss: 0.00075055
Iteration 10/25 | Loss: 0.00075055
Iteration 11/25 | Loss: 0.00075055
Iteration 12/25 | Loss: 0.00075055
Iteration 13/25 | Loss: 0.00075055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007505473913624883, 0.0007505473913624883, 0.0007505473913624883, 0.0007505473913624883, 0.0007505473913624883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007505473913624883

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64364040
Iteration 2/25 | Loss: 0.00051799
Iteration 3/25 | Loss: 0.00051799
Iteration 4/25 | Loss: 0.00051799
Iteration 5/25 | Loss: 0.00051799
Iteration 6/25 | Loss: 0.00051799
Iteration 7/25 | Loss: 0.00051799
Iteration 8/25 | Loss: 0.00051799
Iteration 9/25 | Loss: 0.00051799
Iteration 10/25 | Loss: 0.00051799
Iteration 11/25 | Loss: 0.00051799
Iteration 12/25 | Loss: 0.00051799
Iteration 13/25 | Loss: 0.00051799
Iteration 14/25 | Loss: 0.00051799
Iteration 15/25 | Loss: 0.00051799
Iteration 16/25 | Loss: 0.00051799
Iteration 17/25 | Loss: 0.00051799
Iteration 18/25 | Loss: 0.00051799
Iteration 19/25 | Loss: 0.00051799
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005179857835173607, 0.0005179857835173607, 0.0005179857835173607, 0.0005179857835173607, 0.0005179857835173607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005179857835173607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051799
Iteration 2/1000 | Loss: 0.00004938
Iteration 3/1000 | Loss: 0.00003725
Iteration 4/1000 | Loss: 0.00003187
Iteration 5/1000 | Loss: 0.00002966
Iteration 6/1000 | Loss: 0.00002858
Iteration 7/1000 | Loss: 0.00002766
Iteration 8/1000 | Loss: 0.00002707
Iteration 9/1000 | Loss: 0.00002636
Iteration 10/1000 | Loss: 0.00002587
Iteration 11/1000 | Loss: 0.00002557
Iteration 12/1000 | Loss: 0.00002533
Iteration 13/1000 | Loss: 0.00002513
Iteration 14/1000 | Loss: 0.00002495
Iteration 15/1000 | Loss: 0.00002487
Iteration 16/1000 | Loss: 0.00002486
Iteration 17/1000 | Loss: 0.00002485
Iteration 18/1000 | Loss: 0.00002484
Iteration 19/1000 | Loss: 0.00002484
Iteration 20/1000 | Loss: 0.00002484
Iteration 21/1000 | Loss: 0.00002483
Iteration 22/1000 | Loss: 0.00002483
Iteration 23/1000 | Loss: 0.00002483
Iteration 24/1000 | Loss: 0.00002483
Iteration 25/1000 | Loss: 0.00002483
Iteration 26/1000 | Loss: 0.00002483
Iteration 27/1000 | Loss: 0.00002482
Iteration 28/1000 | Loss: 0.00002482
Iteration 29/1000 | Loss: 0.00002482
Iteration 30/1000 | Loss: 0.00002482
Iteration 31/1000 | Loss: 0.00002482
Iteration 32/1000 | Loss: 0.00002482
Iteration 33/1000 | Loss: 0.00002482
Iteration 34/1000 | Loss: 0.00002482
Iteration 35/1000 | Loss: 0.00002481
Iteration 36/1000 | Loss: 0.00002481
Iteration 37/1000 | Loss: 0.00002481
Iteration 38/1000 | Loss: 0.00002480
Iteration 39/1000 | Loss: 0.00002480
Iteration 40/1000 | Loss: 0.00002479
Iteration 41/1000 | Loss: 0.00002478
Iteration 42/1000 | Loss: 0.00002478
Iteration 43/1000 | Loss: 0.00002477
Iteration 44/1000 | Loss: 0.00002477
Iteration 45/1000 | Loss: 0.00002477
Iteration 46/1000 | Loss: 0.00002476
Iteration 47/1000 | Loss: 0.00002476
Iteration 48/1000 | Loss: 0.00002476
Iteration 49/1000 | Loss: 0.00002475
Iteration 50/1000 | Loss: 0.00002475
Iteration 51/1000 | Loss: 0.00002475
Iteration 52/1000 | Loss: 0.00002474
Iteration 53/1000 | Loss: 0.00002474
Iteration 54/1000 | Loss: 0.00002473
Iteration 55/1000 | Loss: 0.00002473
Iteration 56/1000 | Loss: 0.00002472
Iteration 57/1000 | Loss: 0.00002472
Iteration 58/1000 | Loss: 0.00002472
Iteration 59/1000 | Loss: 0.00002472
Iteration 60/1000 | Loss: 0.00002472
Iteration 61/1000 | Loss: 0.00002472
Iteration 62/1000 | Loss: 0.00002471
Iteration 63/1000 | Loss: 0.00002471
Iteration 64/1000 | Loss: 0.00002471
Iteration 65/1000 | Loss: 0.00002471
Iteration 66/1000 | Loss: 0.00002471
Iteration 67/1000 | Loss: 0.00002470
Iteration 68/1000 | Loss: 0.00002470
Iteration 69/1000 | Loss: 0.00002470
Iteration 70/1000 | Loss: 0.00002470
Iteration 71/1000 | Loss: 0.00002469
Iteration 72/1000 | Loss: 0.00002469
Iteration 73/1000 | Loss: 0.00002469
Iteration 74/1000 | Loss: 0.00002468
Iteration 75/1000 | Loss: 0.00002468
Iteration 76/1000 | Loss: 0.00002468
Iteration 77/1000 | Loss: 0.00002468
Iteration 78/1000 | Loss: 0.00002467
Iteration 79/1000 | Loss: 0.00002467
Iteration 80/1000 | Loss: 0.00002467
Iteration 81/1000 | Loss: 0.00002467
Iteration 82/1000 | Loss: 0.00002466
Iteration 83/1000 | Loss: 0.00002466
Iteration 84/1000 | Loss: 0.00002466
Iteration 85/1000 | Loss: 0.00002465
Iteration 86/1000 | Loss: 0.00002465
Iteration 87/1000 | Loss: 0.00002465
Iteration 88/1000 | Loss: 0.00002465
Iteration 89/1000 | Loss: 0.00002465
Iteration 90/1000 | Loss: 0.00002465
Iteration 91/1000 | Loss: 0.00002464
Iteration 92/1000 | Loss: 0.00002464
Iteration 93/1000 | Loss: 0.00002464
Iteration 94/1000 | Loss: 0.00002464
Iteration 95/1000 | Loss: 0.00002464
Iteration 96/1000 | Loss: 0.00002464
Iteration 97/1000 | Loss: 0.00002464
Iteration 98/1000 | Loss: 0.00002464
Iteration 99/1000 | Loss: 0.00002463
Iteration 100/1000 | Loss: 0.00002463
Iteration 101/1000 | Loss: 0.00002463
Iteration 102/1000 | Loss: 0.00002463
Iteration 103/1000 | Loss: 0.00002463
Iteration 104/1000 | Loss: 0.00002463
Iteration 105/1000 | Loss: 0.00002463
Iteration 106/1000 | Loss: 0.00002463
Iteration 107/1000 | Loss: 0.00002462
Iteration 108/1000 | Loss: 0.00002462
Iteration 109/1000 | Loss: 0.00002462
Iteration 110/1000 | Loss: 0.00002462
Iteration 111/1000 | Loss: 0.00002462
Iteration 112/1000 | Loss: 0.00002462
Iteration 113/1000 | Loss: 0.00002462
Iteration 114/1000 | Loss: 0.00002462
Iteration 115/1000 | Loss: 0.00002462
Iteration 116/1000 | Loss: 0.00002462
Iteration 117/1000 | Loss: 0.00002461
Iteration 118/1000 | Loss: 0.00002461
Iteration 119/1000 | Loss: 0.00002461
Iteration 120/1000 | Loss: 0.00002461
Iteration 121/1000 | Loss: 0.00002461
Iteration 122/1000 | Loss: 0.00002460
Iteration 123/1000 | Loss: 0.00002460
Iteration 124/1000 | Loss: 0.00002460
Iteration 125/1000 | Loss: 0.00002460
Iteration 126/1000 | Loss: 0.00002459
Iteration 127/1000 | Loss: 0.00002459
Iteration 128/1000 | Loss: 0.00002459
Iteration 129/1000 | Loss: 0.00002459
Iteration 130/1000 | Loss: 0.00002459
Iteration 131/1000 | Loss: 0.00002458
Iteration 132/1000 | Loss: 0.00002458
Iteration 133/1000 | Loss: 0.00002458
Iteration 134/1000 | Loss: 0.00002458
Iteration 135/1000 | Loss: 0.00002458
Iteration 136/1000 | Loss: 0.00002457
Iteration 137/1000 | Loss: 0.00002457
Iteration 138/1000 | Loss: 0.00002457
Iteration 139/1000 | Loss: 0.00002457
Iteration 140/1000 | Loss: 0.00002457
Iteration 141/1000 | Loss: 0.00002456
Iteration 142/1000 | Loss: 0.00002456
Iteration 143/1000 | Loss: 0.00002456
Iteration 144/1000 | Loss: 0.00002456
Iteration 145/1000 | Loss: 0.00002456
Iteration 146/1000 | Loss: 0.00002456
Iteration 147/1000 | Loss: 0.00002456
Iteration 148/1000 | Loss: 0.00002456
Iteration 149/1000 | Loss: 0.00002456
Iteration 150/1000 | Loss: 0.00002456
Iteration 151/1000 | Loss: 0.00002455
Iteration 152/1000 | Loss: 0.00002455
Iteration 153/1000 | Loss: 0.00002455
Iteration 154/1000 | Loss: 0.00002455
Iteration 155/1000 | Loss: 0.00002455
Iteration 156/1000 | Loss: 0.00002455
Iteration 157/1000 | Loss: 0.00002455
Iteration 158/1000 | Loss: 0.00002455
Iteration 159/1000 | Loss: 0.00002455
Iteration 160/1000 | Loss: 0.00002455
Iteration 161/1000 | Loss: 0.00002455
Iteration 162/1000 | Loss: 0.00002455
Iteration 163/1000 | Loss: 0.00002455
Iteration 164/1000 | Loss: 0.00002455
Iteration 165/1000 | Loss: 0.00002455
Iteration 166/1000 | Loss: 0.00002454
Iteration 167/1000 | Loss: 0.00002454
Iteration 168/1000 | Loss: 0.00002454
Iteration 169/1000 | Loss: 0.00002454
Iteration 170/1000 | Loss: 0.00002454
Iteration 171/1000 | Loss: 0.00002454
Iteration 172/1000 | Loss: 0.00002454
Iteration 173/1000 | Loss: 0.00002454
Iteration 174/1000 | Loss: 0.00002454
Iteration 175/1000 | Loss: 0.00002454
Iteration 176/1000 | Loss: 0.00002454
Iteration 177/1000 | Loss: 0.00002454
Iteration 178/1000 | Loss: 0.00002454
Iteration 179/1000 | Loss: 0.00002454
Iteration 180/1000 | Loss: 0.00002454
Iteration 181/1000 | Loss: 0.00002454
Iteration 182/1000 | Loss: 0.00002453
Iteration 183/1000 | Loss: 0.00002453
Iteration 184/1000 | Loss: 0.00002453
Iteration 185/1000 | Loss: 0.00002453
Iteration 186/1000 | Loss: 0.00002453
Iteration 187/1000 | Loss: 0.00002453
Iteration 188/1000 | Loss: 0.00002453
Iteration 189/1000 | Loss: 0.00002453
Iteration 190/1000 | Loss: 0.00002453
Iteration 191/1000 | Loss: 0.00002453
Iteration 192/1000 | Loss: 0.00002453
Iteration 193/1000 | Loss: 0.00002453
Iteration 194/1000 | Loss: 0.00002453
Iteration 195/1000 | Loss: 0.00002453
Iteration 196/1000 | Loss: 0.00002453
Iteration 197/1000 | Loss: 0.00002453
Iteration 198/1000 | Loss: 0.00002453
Iteration 199/1000 | Loss: 0.00002453
Iteration 200/1000 | Loss: 0.00002453
Iteration 201/1000 | Loss: 0.00002453
Iteration 202/1000 | Loss: 0.00002453
Iteration 203/1000 | Loss: 0.00002453
Iteration 204/1000 | Loss: 0.00002453
Iteration 205/1000 | Loss: 0.00002453
Iteration 206/1000 | Loss: 0.00002452
Iteration 207/1000 | Loss: 0.00002452
Iteration 208/1000 | Loss: 0.00002452
Iteration 209/1000 | Loss: 0.00002452
Iteration 210/1000 | Loss: 0.00002452
Iteration 211/1000 | Loss: 0.00002452
Iteration 212/1000 | Loss: 0.00002452
Iteration 213/1000 | Loss: 0.00002452
Iteration 214/1000 | Loss: 0.00002452
Iteration 215/1000 | Loss: 0.00002452
Iteration 216/1000 | Loss: 0.00002452
Iteration 217/1000 | Loss: 0.00002452
Iteration 218/1000 | Loss: 0.00002452
Iteration 219/1000 | Loss: 0.00002452
Iteration 220/1000 | Loss: 0.00002452
Iteration 221/1000 | Loss: 0.00002452
Iteration 222/1000 | Loss: 0.00002452
Iteration 223/1000 | Loss: 0.00002452
Iteration 224/1000 | Loss: 0.00002452
Iteration 225/1000 | Loss: 0.00002452
Iteration 226/1000 | Loss: 0.00002452
Iteration 227/1000 | Loss: 0.00002452
Iteration 228/1000 | Loss: 0.00002452
Iteration 229/1000 | Loss: 0.00002451
Iteration 230/1000 | Loss: 0.00002451
Iteration 231/1000 | Loss: 0.00002451
Iteration 232/1000 | Loss: 0.00002451
Iteration 233/1000 | Loss: 0.00002451
Iteration 234/1000 | Loss: 0.00002451
Iteration 235/1000 | Loss: 0.00002451
Iteration 236/1000 | Loss: 0.00002451
Iteration 237/1000 | Loss: 0.00002451
Iteration 238/1000 | Loss: 0.00002451
Iteration 239/1000 | Loss: 0.00002451
Iteration 240/1000 | Loss: 0.00002451
Iteration 241/1000 | Loss: 0.00002451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [2.451459840813186e-05, 2.451459840813186e-05, 2.451459840813186e-05, 2.451459840813186e-05, 2.451459840813186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.451459840813186e-05

Optimization complete. Final v2v error: 4.050748825073242 mm

Highest mean error: 5.929833889007568 mm for frame 100

Lowest mean error: 2.865546941757202 mm for frame 15

Saving results

Total time: 48.81313514709473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397053
Iteration 2/25 | Loss: 0.00093438
Iteration 3/25 | Loss: 0.00071059
Iteration 4/25 | Loss: 0.00069481
Iteration 5/25 | Loss: 0.00068714
Iteration 6/25 | Loss: 0.00068475
Iteration 7/25 | Loss: 0.00068418
Iteration 8/25 | Loss: 0.00068418
Iteration 9/25 | Loss: 0.00068418
Iteration 10/25 | Loss: 0.00068418
Iteration 11/25 | Loss: 0.00068418
Iteration 12/25 | Loss: 0.00068418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000684184196870774, 0.000684184196870774, 0.000684184196870774, 0.000684184196870774, 0.000684184196870774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000684184196870774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65943670
Iteration 2/25 | Loss: 0.00035884
Iteration 3/25 | Loss: 0.00035884
Iteration 4/25 | Loss: 0.00035884
Iteration 5/25 | Loss: 0.00035884
Iteration 6/25 | Loss: 0.00035884
Iteration 7/25 | Loss: 0.00035884
Iteration 8/25 | Loss: 0.00035884
Iteration 9/25 | Loss: 0.00035884
Iteration 10/25 | Loss: 0.00035884
Iteration 11/25 | Loss: 0.00035884
Iteration 12/25 | Loss: 0.00035884
Iteration 13/25 | Loss: 0.00035884
Iteration 14/25 | Loss: 0.00035884
Iteration 15/25 | Loss: 0.00035884
Iteration 16/25 | Loss: 0.00035884
Iteration 17/25 | Loss: 0.00035884
Iteration 18/25 | Loss: 0.00035884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003588388208299875, 0.0003588388208299875, 0.0003588388208299875, 0.0003588388208299875, 0.0003588388208299875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003588388208299875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035884
Iteration 2/1000 | Loss: 0.00002430
Iteration 3/1000 | Loss: 0.00001512
Iteration 4/1000 | Loss: 0.00001388
Iteration 5/1000 | Loss: 0.00001294
Iteration 6/1000 | Loss: 0.00001249
Iteration 7/1000 | Loss: 0.00001223
Iteration 8/1000 | Loss: 0.00001203
Iteration 9/1000 | Loss: 0.00001186
Iteration 10/1000 | Loss: 0.00001185
Iteration 11/1000 | Loss: 0.00001185
Iteration 12/1000 | Loss: 0.00001184
Iteration 13/1000 | Loss: 0.00001182
Iteration 14/1000 | Loss: 0.00001171
Iteration 15/1000 | Loss: 0.00001165
Iteration 16/1000 | Loss: 0.00001164
Iteration 17/1000 | Loss: 0.00001162
Iteration 18/1000 | Loss: 0.00001159
Iteration 19/1000 | Loss: 0.00001158
Iteration 20/1000 | Loss: 0.00001158
Iteration 21/1000 | Loss: 0.00001157
Iteration 22/1000 | Loss: 0.00001156
Iteration 23/1000 | Loss: 0.00001152
Iteration 24/1000 | Loss: 0.00001151
Iteration 25/1000 | Loss: 0.00001150
Iteration 26/1000 | Loss: 0.00001149
Iteration 27/1000 | Loss: 0.00001149
Iteration 28/1000 | Loss: 0.00001148
Iteration 29/1000 | Loss: 0.00001147
Iteration 30/1000 | Loss: 0.00001146
Iteration 31/1000 | Loss: 0.00001146
Iteration 32/1000 | Loss: 0.00001146
Iteration 33/1000 | Loss: 0.00001145
Iteration 34/1000 | Loss: 0.00001145
Iteration 35/1000 | Loss: 0.00001144
Iteration 36/1000 | Loss: 0.00001144
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001143
Iteration 39/1000 | Loss: 0.00001143
Iteration 40/1000 | Loss: 0.00001143
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001143
Iteration 43/1000 | Loss: 0.00001143
Iteration 44/1000 | Loss: 0.00001143
Iteration 45/1000 | Loss: 0.00001143
Iteration 46/1000 | Loss: 0.00001143
Iteration 47/1000 | Loss: 0.00001143
Iteration 48/1000 | Loss: 0.00001143
Iteration 49/1000 | Loss: 0.00001143
Iteration 50/1000 | Loss: 0.00001142
Iteration 51/1000 | Loss: 0.00001142
Iteration 52/1000 | Loss: 0.00001141
Iteration 53/1000 | Loss: 0.00001141
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001140
Iteration 56/1000 | Loss: 0.00001137
Iteration 57/1000 | Loss: 0.00001136
Iteration 58/1000 | Loss: 0.00001136
Iteration 59/1000 | Loss: 0.00001135
Iteration 60/1000 | Loss: 0.00001135
Iteration 61/1000 | Loss: 0.00001135
Iteration 62/1000 | Loss: 0.00001135
Iteration 63/1000 | Loss: 0.00001134
Iteration 64/1000 | Loss: 0.00001134
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001134
Iteration 67/1000 | Loss: 0.00001134
Iteration 68/1000 | Loss: 0.00001134
Iteration 69/1000 | Loss: 0.00001134
Iteration 70/1000 | Loss: 0.00001133
Iteration 71/1000 | Loss: 0.00001133
Iteration 72/1000 | Loss: 0.00001133
Iteration 73/1000 | Loss: 0.00001133
Iteration 74/1000 | Loss: 0.00001133
Iteration 75/1000 | Loss: 0.00001133
Iteration 76/1000 | Loss: 0.00001132
Iteration 77/1000 | Loss: 0.00001132
Iteration 78/1000 | Loss: 0.00001132
Iteration 79/1000 | Loss: 0.00001132
Iteration 80/1000 | Loss: 0.00001132
Iteration 81/1000 | Loss: 0.00001132
Iteration 82/1000 | Loss: 0.00001132
Iteration 83/1000 | Loss: 0.00001132
Iteration 84/1000 | Loss: 0.00001132
Iteration 85/1000 | Loss: 0.00001132
Iteration 86/1000 | Loss: 0.00001132
Iteration 87/1000 | Loss: 0.00001132
Iteration 88/1000 | Loss: 0.00001132
Iteration 89/1000 | Loss: 0.00001132
Iteration 90/1000 | Loss: 0.00001132
Iteration 91/1000 | Loss: 0.00001132
Iteration 92/1000 | Loss: 0.00001131
Iteration 93/1000 | Loss: 0.00001131
Iteration 94/1000 | Loss: 0.00001131
Iteration 95/1000 | Loss: 0.00001131
Iteration 96/1000 | Loss: 0.00001131
Iteration 97/1000 | Loss: 0.00001131
Iteration 98/1000 | Loss: 0.00001131
Iteration 99/1000 | Loss: 0.00001131
Iteration 100/1000 | Loss: 0.00001131
Iteration 101/1000 | Loss: 0.00001131
Iteration 102/1000 | Loss: 0.00001131
Iteration 103/1000 | Loss: 0.00001131
Iteration 104/1000 | Loss: 0.00001130
Iteration 105/1000 | Loss: 0.00001130
Iteration 106/1000 | Loss: 0.00001130
Iteration 107/1000 | Loss: 0.00001130
Iteration 108/1000 | Loss: 0.00001130
Iteration 109/1000 | Loss: 0.00001130
Iteration 110/1000 | Loss: 0.00001130
Iteration 111/1000 | Loss: 0.00001130
Iteration 112/1000 | Loss: 0.00001129
Iteration 113/1000 | Loss: 0.00001129
Iteration 114/1000 | Loss: 0.00001129
Iteration 115/1000 | Loss: 0.00001129
Iteration 116/1000 | Loss: 0.00001129
Iteration 117/1000 | Loss: 0.00001129
Iteration 118/1000 | Loss: 0.00001129
Iteration 119/1000 | Loss: 0.00001128
Iteration 120/1000 | Loss: 0.00001128
Iteration 121/1000 | Loss: 0.00001128
Iteration 122/1000 | Loss: 0.00001128
Iteration 123/1000 | Loss: 0.00001128
Iteration 124/1000 | Loss: 0.00001128
Iteration 125/1000 | Loss: 0.00001128
Iteration 126/1000 | Loss: 0.00001128
Iteration 127/1000 | Loss: 0.00001128
Iteration 128/1000 | Loss: 0.00001128
Iteration 129/1000 | Loss: 0.00001128
Iteration 130/1000 | Loss: 0.00001128
Iteration 131/1000 | Loss: 0.00001128
Iteration 132/1000 | Loss: 0.00001128
Iteration 133/1000 | Loss: 0.00001128
Iteration 134/1000 | Loss: 0.00001128
Iteration 135/1000 | Loss: 0.00001128
Iteration 136/1000 | Loss: 0.00001128
Iteration 137/1000 | Loss: 0.00001127
Iteration 138/1000 | Loss: 0.00001127
Iteration 139/1000 | Loss: 0.00001127
Iteration 140/1000 | Loss: 0.00001127
Iteration 141/1000 | Loss: 0.00001127
Iteration 142/1000 | Loss: 0.00001127
Iteration 143/1000 | Loss: 0.00001127
Iteration 144/1000 | Loss: 0.00001127
Iteration 145/1000 | Loss: 0.00001127
Iteration 146/1000 | Loss: 0.00001127
Iteration 147/1000 | Loss: 0.00001127
Iteration 148/1000 | Loss: 0.00001127
Iteration 149/1000 | Loss: 0.00001126
Iteration 150/1000 | Loss: 0.00001126
Iteration 151/1000 | Loss: 0.00001126
Iteration 152/1000 | Loss: 0.00001126
Iteration 153/1000 | Loss: 0.00001126
Iteration 154/1000 | Loss: 0.00001126
Iteration 155/1000 | Loss: 0.00001126
Iteration 156/1000 | Loss: 0.00001126
Iteration 157/1000 | Loss: 0.00001126
Iteration 158/1000 | Loss: 0.00001126
Iteration 159/1000 | Loss: 0.00001126
Iteration 160/1000 | Loss: 0.00001126
Iteration 161/1000 | Loss: 0.00001126
Iteration 162/1000 | Loss: 0.00001126
Iteration 163/1000 | Loss: 0.00001126
Iteration 164/1000 | Loss: 0.00001126
Iteration 165/1000 | Loss: 0.00001126
Iteration 166/1000 | Loss: 0.00001126
Iteration 167/1000 | Loss: 0.00001126
Iteration 168/1000 | Loss: 0.00001126
Iteration 169/1000 | Loss: 0.00001126
Iteration 170/1000 | Loss: 0.00001126
Iteration 171/1000 | Loss: 0.00001126
Iteration 172/1000 | Loss: 0.00001126
Iteration 173/1000 | Loss: 0.00001126
Iteration 174/1000 | Loss: 0.00001126
Iteration 175/1000 | Loss: 0.00001126
Iteration 176/1000 | Loss: 0.00001126
Iteration 177/1000 | Loss: 0.00001126
Iteration 178/1000 | Loss: 0.00001126
Iteration 179/1000 | Loss: 0.00001126
Iteration 180/1000 | Loss: 0.00001126
Iteration 181/1000 | Loss: 0.00001126
Iteration 182/1000 | Loss: 0.00001126
Iteration 183/1000 | Loss: 0.00001126
Iteration 184/1000 | Loss: 0.00001126
Iteration 185/1000 | Loss: 0.00001126
Iteration 186/1000 | Loss: 0.00001126
Iteration 187/1000 | Loss: 0.00001126
Iteration 188/1000 | Loss: 0.00001126
Iteration 189/1000 | Loss: 0.00001126
Iteration 190/1000 | Loss: 0.00001126
Iteration 191/1000 | Loss: 0.00001126
Iteration 192/1000 | Loss: 0.00001126
Iteration 193/1000 | Loss: 0.00001126
Iteration 194/1000 | Loss: 0.00001126
Iteration 195/1000 | Loss: 0.00001126
Iteration 196/1000 | Loss: 0.00001126
Iteration 197/1000 | Loss: 0.00001126
Iteration 198/1000 | Loss: 0.00001126
Iteration 199/1000 | Loss: 0.00001126
Iteration 200/1000 | Loss: 0.00001126
Iteration 201/1000 | Loss: 0.00001126
Iteration 202/1000 | Loss: 0.00001126
Iteration 203/1000 | Loss: 0.00001126
Iteration 204/1000 | Loss: 0.00001126
Iteration 205/1000 | Loss: 0.00001126
Iteration 206/1000 | Loss: 0.00001126
Iteration 207/1000 | Loss: 0.00001126
Iteration 208/1000 | Loss: 0.00001126
Iteration 209/1000 | Loss: 0.00001126
Iteration 210/1000 | Loss: 0.00001126
Iteration 211/1000 | Loss: 0.00001126
Iteration 212/1000 | Loss: 0.00001126
Iteration 213/1000 | Loss: 0.00001126
Iteration 214/1000 | Loss: 0.00001126
Iteration 215/1000 | Loss: 0.00001126
Iteration 216/1000 | Loss: 0.00001126
Iteration 217/1000 | Loss: 0.00001126
Iteration 218/1000 | Loss: 0.00001126
Iteration 219/1000 | Loss: 0.00001126
Iteration 220/1000 | Loss: 0.00001126
Iteration 221/1000 | Loss: 0.00001126
Iteration 222/1000 | Loss: 0.00001126
Iteration 223/1000 | Loss: 0.00001126
Iteration 224/1000 | Loss: 0.00001126
Iteration 225/1000 | Loss: 0.00001126
Iteration 226/1000 | Loss: 0.00001126
Iteration 227/1000 | Loss: 0.00001126
Iteration 228/1000 | Loss: 0.00001126
Iteration 229/1000 | Loss: 0.00001126
Iteration 230/1000 | Loss: 0.00001126
Iteration 231/1000 | Loss: 0.00001126
Iteration 232/1000 | Loss: 0.00001126
Iteration 233/1000 | Loss: 0.00001126
Iteration 234/1000 | Loss: 0.00001126
Iteration 235/1000 | Loss: 0.00001126
Iteration 236/1000 | Loss: 0.00001126
Iteration 237/1000 | Loss: 0.00001126
Iteration 238/1000 | Loss: 0.00001126
Iteration 239/1000 | Loss: 0.00001126
Iteration 240/1000 | Loss: 0.00001126
Iteration 241/1000 | Loss: 0.00001126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.1263657142990269e-05, 1.1263657142990269e-05, 1.1263657142990269e-05, 1.1263657142990269e-05, 1.1263657142990269e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1263657142990269e-05

Optimization complete. Final v2v error: 2.8501224517822266 mm

Highest mean error: 3.010284185409546 mm for frame 31

Lowest mean error: 2.6850056648254395 mm for frame 228

Saving results

Total time: 43.650887966156006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794937
Iteration 2/25 | Loss: 0.00143183
Iteration 3/25 | Loss: 0.00092756
Iteration 4/25 | Loss: 0.00088022
Iteration 5/25 | Loss: 0.00092838
Iteration 6/25 | Loss: 0.00087783
Iteration 7/25 | Loss: 0.00079999
Iteration 8/25 | Loss: 0.00078644
Iteration 9/25 | Loss: 0.00075774
Iteration 10/25 | Loss: 0.00074886
Iteration 11/25 | Loss: 0.00074168
Iteration 12/25 | Loss: 0.00073574
Iteration 13/25 | Loss: 0.00072925
Iteration 14/25 | Loss: 0.00073137
Iteration 15/25 | Loss: 0.00073048
Iteration 16/25 | Loss: 0.00072960
Iteration 17/25 | Loss: 0.00073341
Iteration 18/25 | Loss: 0.00073018
Iteration 19/25 | Loss: 0.00073249
Iteration 20/25 | Loss: 0.00072949
Iteration 21/25 | Loss: 0.00072814
Iteration 22/25 | Loss: 0.00072637
Iteration 23/25 | Loss: 0.00072360
Iteration 24/25 | Loss: 0.00072819
Iteration 25/25 | Loss: 0.00072438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73305750
Iteration 2/25 | Loss: 0.00097165
Iteration 3/25 | Loss: 0.00097165
Iteration 4/25 | Loss: 0.00097165
Iteration 5/25 | Loss: 0.00097165
Iteration 6/25 | Loss: 0.00097165
Iteration 7/25 | Loss: 0.00097165
Iteration 8/25 | Loss: 0.00097164
Iteration 9/25 | Loss: 0.00097164
Iteration 10/25 | Loss: 0.00097164
Iteration 11/25 | Loss: 0.00097164
Iteration 12/25 | Loss: 0.00097164
Iteration 13/25 | Loss: 0.00097164
Iteration 14/25 | Loss: 0.00097164
Iteration 15/25 | Loss: 0.00097164
Iteration 16/25 | Loss: 0.00097164
Iteration 17/25 | Loss: 0.00097164
Iteration 18/25 | Loss: 0.00097164
Iteration 19/25 | Loss: 0.00097164
Iteration 20/25 | Loss: 0.00097164
Iteration 21/25 | Loss: 0.00097164
Iteration 22/25 | Loss: 0.00097164
Iteration 23/25 | Loss: 0.00097164
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009716442436911166, 0.0009716442436911166, 0.0009716442436911166, 0.0009716442436911166, 0.0009716442436911166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009716442436911166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097164
Iteration 2/1000 | Loss: 0.00350430
Iteration 3/1000 | Loss: 0.00412034
Iteration 4/1000 | Loss: 0.00062090
Iteration 5/1000 | Loss: 0.00084410
Iteration 6/1000 | Loss: 0.00046739
Iteration 7/1000 | Loss: 0.00049016
Iteration 8/1000 | Loss: 0.00082683
Iteration 9/1000 | Loss: 0.00067872
Iteration 10/1000 | Loss: 0.00084253
Iteration 11/1000 | Loss: 0.00458161
Iteration 12/1000 | Loss: 0.00030649
Iteration 13/1000 | Loss: 0.00021521
Iteration 14/1000 | Loss: 0.00073402
Iteration 15/1000 | Loss: 0.00045779
Iteration 16/1000 | Loss: 0.00071231
Iteration 17/1000 | Loss: 0.00058325
Iteration 18/1000 | Loss: 0.00080899
Iteration 19/1000 | Loss: 0.00039230
Iteration 20/1000 | Loss: 0.00058453
Iteration 21/1000 | Loss: 0.00057640
Iteration 22/1000 | Loss: 0.00054025
Iteration 23/1000 | Loss: 0.00049648
Iteration 24/1000 | Loss: 0.00074052
Iteration 25/1000 | Loss: 0.00071833
Iteration 26/1000 | Loss: 0.00083129
Iteration 27/1000 | Loss: 0.00071006
Iteration 28/1000 | Loss: 0.00069452
Iteration 29/1000 | Loss: 0.00070248
Iteration 30/1000 | Loss: 0.00064415
Iteration 31/1000 | Loss: 0.00078872
Iteration 32/1000 | Loss: 0.00085962
Iteration 33/1000 | Loss: 0.00062100
Iteration 34/1000 | Loss: 0.00044476
Iteration 35/1000 | Loss: 0.00069750
Iteration 36/1000 | Loss: 0.00026857
Iteration 37/1000 | Loss: 0.00085010
Iteration 38/1000 | Loss: 0.00088001
Iteration 39/1000 | Loss: 0.00044586
Iteration 40/1000 | Loss: 0.00048996
Iteration 41/1000 | Loss: 0.00017093
Iteration 42/1000 | Loss: 0.00004270
Iteration 43/1000 | Loss: 0.00011454
Iteration 44/1000 | Loss: 0.00026914
Iteration 45/1000 | Loss: 0.00018817
Iteration 46/1000 | Loss: 0.00015873
Iteration 47/1000 | Loss: 0.00013654
Iteration 48/1000 | Loss: 0.00012188
Iteration 49/1000 | Loss: 0.00010947
Iteration 50/1000 | Loss: 0.00003323
Iteration 51/1000 | Loss: 0.00002743
Iteration 52/1000 | Loss: 0.00014413
Iteration 53/1000 | Loss: 0.00011815
Iteration 54/1000 | Loss: 0.00011568
Iteration 55/1000 | Loss: 0.00007062
Iteration 56/1000 | Loss: 0.00004237
Iteration 57/1000 | Loss: 0.00002231
Iteration 58/1000 | Loss: 0.00002013
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001832
Iteration 61/1000 | Loss: 0.00001783
Iteration 62/1000 | Loss: 0.00001747
Iteration 63/1000 | Loss: 0.00001694
Iteration 64/1000 | Loss: 0.00001659
Iteration 65/1000 | Loss: 0.00001632
Iteration 66/1000 | Loss: 0.00001631
Iteration 67/1000 | Loss: 0.00001629
Iteration 68/1000 | Loss: 0.00001622
Iteration 69/1000 | Loss: 0.00001618
Iteration 70/1000 | Loss: 0.00001612
Iteration 71/1000 | Loss: 0.00001612
Iteration 72/1000 | Loss: 0.00001611
Iteration 73/1000 | Loss: 0.00001601
Iteration 74/1000 | Loss: 0.00001601
Iteration 75/1000 | Loss: 0.00001601
Iteration 76/1000 | Loss: 0.00001599
Iteration 77/1000 | Loss: 0.00001594
Iteration 78/1000 | Loss: 0.00001585
Iteration 79/1000 | Loss: 0.00001585
Iteration 80/1000 | Loss: 0.00001583
Iteration 81/1000 | Loss: 0.00001583
Iteration 82/1000 | Loss: 0.00001582
Iteration 83/1000 | Loss: 0.00001582
Iteration 84/1000 | Loss: 0.00001581
Iteration 85/1000 | Loss: 0.00001580
Iteration 86/1000 | Loss: 0.00001579
Iteration 87/1000 | Loss: 0.00001579
Iteration 88/1000 | Loss: 0.00001578
Iteration 89/1000 | Loss: 0.00001576
Iteration 90/1000 | Loss: 0.00001573
Iteration 91/1000 | Loss: 0.00001573
Iteration 92/1000 | Loss: 0.00001573
Iteration 93/1000 | Loss: 0.00001572
Iteration 94/1000 | Loss: 0.00001571
Iteration 95/1000 | Loss: 0.00001571
Iteration 96/1000 | Loss: 0.00001571
Iteration 97/1000 | Loss: 0.00001571
Iteration 98/1000 | Loss: 0.00001570
Iteration 99/1000 | Loss: 0.00001570
Iteration 100/1000 | Loss: 0.00001569
Iteration 101/1000 | Loss: 0.00001568
Iteration 102/1000 | Loss: 0.00001568
Iteration 103/1000 | Loss: 0.00001568
Iteration 104/1000 | Loss: 0.00001568
Iteration 105/1000 | Loss: 0.00001568
Iteration 106/1000 | Loss: 0.00001568
Iteration 107/1000 | Loss: 0.00001568
Iteration 108/1000 | Loss: 0.00001568
Iteration 109/1000 | Loss: 0.00001567
Iteration 110/1000 | Loss: 0.00001567
Iteration 111/1000 | Loss: 0.00001566
Iteration 112/1000 | Loss: 0.00001566
Iteration 113/1000 | Loss: 0.00001566
Iteration 114/1000 | Loss: 0.00001565
Iteration 115/1000 | Loss: 0.00001565
Iteration 116/1000 | Loss: 0.00001564
Iteration 117/1000 | Loss: 0.00001564
Iteration 118/1000 | Loss: 0.00001564
Iteration 119/1000 | Loss: 0.00001563
Iteration 120/1000 | Loss: 0.00001563
Iteration 121/1000 | Loss: 0.00001563
Iteration 122/1000 | Loss: 0.00001563
Iteration 123/1000 | Loss: 0.00001563
Iteration 124/1000 | Loss: 0.00001563
Iteration 125/1000 | Loss: 0.00001563
Iteration 126/1000 | Loss: 0.00001562
Iteration 127/1000 | Loss: 0.00001562
Iteration 128/1000 | Loss: 0.00001562
Iteration 129/1000 | Loss: 0.00001562
Iteration 130/1000 | Loss: 0.00001561
Iteration 131/1000 | Loss: 0.00001561
Iteration 132/1000 | Loss: 0.00001561
Iteration 133/1000 | Loss: 0.00001560
Iteration 134/1000 | Loss: 0.00001560
Iteration 135/1000 | Loss: 0.00001559
Iteration 136/1000 | Loss: 0.00001559
Iteration 137/1000 | Loss: 0.00001559
Iteration 138/1000 | Loss: 0.00001559
Iteration 139/1000 | Loss: 0.00001559
Iteration 140/1000 | Loss: 0.00042357
Iteration 141/1000 | Loss: 0.00001955
Iteration 142/1000 | Loss: 0.00001730
Iteration 143/1000 | Loss: 0.00001615
Iteration 144/1000 | Loss: 0.00001507
Iteration 145/1000 | Loss: 0.00001442
Iteration 146/1000 | Loss: 0.00001412
Iteration 147/1000 | Loss: 0.00001402
Iteration 148/1000 | Loss: 0.00001396
Iteration 149/1000 | Loss: 0.00001394
Iteration 150/1000 | Loss: 0.00001394
Iteration 151/1000 | Loss: 0.00001389
Iteration 152/1000 | Loss: 0.00001388
Iteration 153/1000 | Loss: 0.00001388
Iteration 154/1000 | Loss: 0.00001387
Iteration 155/1000 | Loss: 0.00001387
Iteration 156/1000 | Loss: 0.00001386
Iteration 157/1000 | Loss: 0.00001386
Iteration 158/1000 | Loss: 0.00001385
Iteration 159/1000 | Loss: 0.00001385
Iteration 160/1000 | Loss: 0.00001385
Iteration 161/1000 | Loss: 0.00001384
Iteration 162/1000 | Loss: 0.00001384
Iteration 163/1000 | Loss: 0.00001384
Iteration 164/1000 | Loss: 0.00001383
Iteration 165/1000 | Loss: 0.00001383
Iteration 166/1000 | Loss: 0.00001383
Iteration 167/1000 | Loss: 0.00001382
Iteration 168/1000 | Loss: 0.00001382
Iteration 169/1000 | Loss: 0.00001382
Iteration 170/1000 | Loss: 0.00001381
Iteration 171/1000 | Loss: 0.00001381
Iteration 172/1000 | Loss: 0.00001381
Iteration 173/1000 | Loss: 0.00001381
Iteration 174/1000 | Loss: 0.00001381
Iteration 175/1000 | Loss: 0.00001380
Iteration 176/1000 | Loss: 0.00001380
Iteration 177/1000 | Loss: 0.00001380
Iteration 178/1000 | Loss: 0.00001379
Iteration 179/1000 | Loss: 0.00001378
Iteration 180/1000 | Loss: 0.00001378
Iteration 181/1000 | Loss: 0.00001378
Iteration 182/1000 | Loss: 0.00001378
Iteration 183/1000 | Loss: 0.00001378
Iteration 184/1000 | Loss: 0.00001378
Iteration 185/1000 | Loss: 0.00001378
Iteration 186/1000 | Loss: 0.00001378
Iteration 187/1000 | Loss: 0.00001378
Iteration 188/1000 | Loss: 0.00001377
Iteration 189/1000 | Loss: 0.00001377
Iteration 190/1000 | Loss: 0.00001377
Iteration 191/1000 | Loss: 0.00001376
Iteration 192/1000 | Loss: 0.00001376
Iteration 193/1000 | Loss: 0.00001376
Iteration 194/1000 | Loss: 0.00001376
Iteration 195/1000 | Loss: 0.00001376
Iteration 196/1000 | Loss: 0.00001376
Iteration 197/1000 | Loss: 0.00001376
Iteration 198/1000 | Loss: 0.00001375
Iteration 199/1000 | Loss: 0.00001375
Iteration 200/1000 | Loss: 0.00001375
Iteration 201/1000 | Loss: 0.00001375
Iteration 202/1000 | Loss: 0.00001375
Iteration 203/1000 | Loss: 0.00001375
Iteration 204/1000 | Loss: 0.00001375
Iteration 205/1000 | Loss: 0.00001375
Iteration 206/1000 | Loss: 0.00001375
Iteration 207/1000 | Loss: 0.00001375
Iteration 208/1000 | Loss: 0.00001374
Iteration 209/1000 | Loss: 0.00001374
Iteration 210/1000 | Loss: 0.00001374
Iteration 211/1000 | Loss: 0.00001374
Iteration 212/1000 | Loss: 0.00001374
Iteration 213/1000 | Loss: 0.00001374
Iteration 214/1000 | Loss: 0.00001374
Iteration 215/1000 | Loss: 0.00001374
Iteration 216/1000 | Loss: 0.00001374
Iteration 217/1000 | Loss: 0.00001374
Iteration 218/1000 | Loss: 0.00001374
Iteration 219/1000 | Loss: 0.00001373
Iteration 220/1000 | Loss: 0.00001373
Iteration 221/1000 | Loss: 0.00001373
Iteration 222/1000 | Loss: 0.00001373
Iteration 223/1000 | Loss: 0.00001373
Iteration 224/1000 | Loss: 0.00001373
Iteration 225/1000 | Loss: 0.00001373
Iteration 226/1000 | Loss: 0.00001373
Iteration 227/1000 | Loss: 0.00001373
Iteration 228/1000 | Loss: 0.00001373
Iteration 229/1000 | Loss: 0.00001373
Iteration 230/1000 | Loss: 0.00001373
Iteration 231/1000 | Loss: 0.00001373
Iteration 232/1000 | Loss: 0.00001373
Iteration 233/1000 | Loss: 0.00001373
Iteration 234/1000 | Loss: 0.00001373
Iteration 235/1000 | Loss: 0.00001373
Iteration 236/1000 | Loss: 0.00001373
Iteration 237/1000 | Loss: 0.00001373
Iteration 238/1000 | Loss: 0.00001372
Iteration 239/1000 | Loss: 0.00001372
Iteration 240/1000 | Loss: 0.00001372
Iteration 241/1000 | Loss: 0.00001372
Iteration 242/1000 | Loss: 0.00001372
Iteration 243/1000 | Loss: 0.00001372
Iteration 244/1000 | Loss: 0.00001372
Iteration 245/1000 | Loss: 0.00001372
Iteration 246/1000 | Loss: 0.00001372
Iteration 247/1000 | Loss: 0.00001372
Iteration 248/1000 | Loss: 0.00001372
Iteration 249/1000 | Loss: 0.00001372
Iteration 250/1000 | Loss: 0.00001372
Iteration 251/1000 | Loss: 0.00001371
Iteration 252/1000 | Loss: 0.00001371
Iteration 253/1000 | Loss: 0.00001371
Iteration 254/1000 | Loss: 0.00001371
Iteration 255/1000 | Loss: 0.00001371
Iteration 256/1000 | Loss: 0.00001371
Iteration 257/1000 | Loss: 0.00001371
Iteration 258/1000 | Loss: 0.00001371
Iteration 259/1000 | Loss: 0.00001371
Iteration 260/1000 | Loss: 0.00001370
Iteration 261/1000 | Loss: 0.00001370
Iteration 262/1000 | Loss: 0.00001370
Iteration 263/1000 | Loss: 0.00001370
Iteration 264/1000 | Loss: 0.00001370
Iteration 265/1000 | Loss: 0.00001370
Iteration 266/1000 | Loss: 0.00001369
Iteration 267/1000 | Loss: 0.00001369
Iteration 268/1000 | Loss: 0.00001369
Iteration 269/1000 | Loss: 0.00001369
Iteration 270/1000 | Loss: 0.00001369
Iteration 271/1000 | Loss: 0.00001369
Iteration 272/1000 | Loss: 0.00001369
Iteration 273/1000 | Loss: 0.00001369
Iteration 274/1000 | Loss: 0.00001369
Iteration 275/1000 | Loss: 0.00001369
Iteration 276/1000 | Loss: 0.00001369
Iteration 277/1000 | Loss: 0.00001368
Iteration 278/1000 | Loss: 0.00001368
Iteration 279/1000 | Loss: 0.00001368
Iteration 280/1000 | Loss: 0.00001368
Iteration 281/1000 | Loss: 0.00001368
Iteration 282/1000 | Loss: 0.00001368
Iteration 283/1000 | Loss: 0.00001368
Iteration 284/1000 | Loss: 0.00001368
Iteration 285/1000 | Loss: 0.00001368
Iteration 286/1000 | Loss: 0.00001368
Iteration 287/1000 | Loss: 0.00001368
Iteration 288/1000 | Loss: 0.00001368
Iteration 289/1000 | Loss: 0.00001368
Iteration 290/1000 | Loss: 0.00001368
Iteration 291/1000 | Loss: 0.00001368
Iteration 292/1000 | Loss: 0.00001368
Iteration 293/1000 | Loss: 0.00001368
Iteration 294/1000 | Loss: 0.00001368
Iteration 295/1000 | Loss: 0.00001368
Iteration 296/1000 | Loss: 0.00001368
Iteration 297/1000 | Loss: 0.00001368
Iteration 298/1000 | Loss: 0.00001368
Iteration 299/1000 | Loss: 0.00001368
Iteration 300/1000 | Loss: 0.00001367
Iteration 301/1000 | Loss: 0.00001367
Iteration 302/1000 | Loss: 0.00001367
Iteration 303/1000 | Loss: 0.00001367
Iteration 304/1000 | Loss: 0.00001367
Iteration 305/1000 | Loss: 0.00001367
Iteration 306/1000 | Loss: 0.00001367
Iteration 307/1000 | Loss: 0.00001367
Iteration 308/1000 | Loss: 0.00001367
Iteration 309/1000 | Loss: 0.00001367
Iteration 310/1000 | Loss: 0.00001367
Iteration 311/1000 | Loss: 0.00001367
Iteration 312/1000 | Loss: 0.00001367
Iteration 313/1000 | Loss: 0.00001367
Iteration 314/1000 | Loss: 0.00001367
Iteration 315/1000 | Loss: 0.00001367
Iteration 316/1000 | Loss: 0.00001367
Iteration 317/1000 | Loss: 0.00001367
Iteration 318/1000 | Loss: 0.00001367
Iteration 319/1000 | Loss: 0.00001367
Iteration 320/1000 | Loss: 0.00001367
Iteration 321/1000 | Loss: 0.00001367
Iteration 322/1000 | Loss: 0.00001367
Iteration 323/1000 | Loss: 0.00001367
Iteration 324/1000 | Loss: 0.00001367
Iteration 325/1000 | Loss: 0.00001367
Iteration 326/1000 | Loss: 0.00001367
Iteration 327/1000 | Loss: 0.00001367
Iteration 328/1000 | Loss: 0.00001366
Iteration 329/1000 | Loss: 0.00001366
Iteration 330/1000 | Loss: 0.00001366
Iteration 331/1000 | Loss: 0.00001366
Iteration 332/1000 | Loss: 0.00001366
Iteration 333/1000 | Loss: 0.00001366
Iteration 334/1000 | Loss: 0.00001366
Iteration 335/1000 | Loss: 0.00001366
Iteration 336/1000 | Loss: 0.00001366
Iteration 337/1000 | Loss: 0.00001366
Iteration 338/1000 | Loss: 0.00001366
Iteration 339/1000 | Loss: 0.00001366
Iteration 340/1000 | Loss: 0.00001366
Iteration 341/1000 | Loss: 0.00001366
Iteration 342/1000 | Loss: 0.00001366
Iteration 343/1000 | Loss: 0.00001366
Iteration 344/1000 | Loss: 0.00001366
Iteration 345/1000 | Loss: 0.00001366
Iteration 346/1000 | Loss: 0.00001366
Iteration 347/1000 | Loss: 0.00001366
Iteration 348/1000 | Loss: 0.00001366
Iteration 349/1000 | Loss: 0.00001366
Iteration 350/1000 | Loss: 0.00001366
Iteration 351/1000 | Loss: 0.00001366
Iteration 352/1000 | Loss: 0.00001366
Iteration 353/1000 | Loss: 0.00001366
Iteration 354/1000 | Loss: 0.00001366
Iteration 355/1000 | Loss: 0.00001366
Iteration 356/1000 | Loss: 0.00001366
Iteration 357/1000 | Loss: 0.00001366
Iteration 358/1000 | Loss: 0.00001366
Iteration 359/1000 | Loss: 0.00001366
Iteration 360/1000 | Loss: 0.00001366
Iteration 361/1000 | Loss: 0.00001366
Iteration 362/1000 | Loss: 0.00001366
Iteration 363/1000 | Loss: 0.00001366
Iteration 364/1000 | Loss: 0.00001366
Iteration 365/1000 | Loss: 0.00001366
Iteration 366/1000 | Loss: 0.00001366
Iteration 367/1000 | Loss: 0.00001366
Iteration 368/1000 | Loss: 0.00001366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 368. Stopping optimization.
Last 5 losses: [1.3664573998539709e-05, 1.3664573998539709e-05, 1.3664573998539709e-05, 1.3664573998539709e-05, 1.3664573998539709e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3664573998539709e-05

Optimization complete. Final v2v error: 3.051293134689331 mm

Highest mean error: 4.475637435913086 mm for frame 72

Lowest mean error: 2.4083094596862793 mm for frame 235

Saving results

Total time: 198.64074397087097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00367248
Iteration 2/25 | Loss: 0.00074308
Iteration 3/25 | Loss: 0.00064107
Iteration 4/25 | Loss: 0.00062628
Iteration 5/25 | Loss: 0.00062167
Iteration 6/25 | Loss: 0.00062005
Iteration 7/25 | Loss: 0.00061965
Iteration 8/25 | Loss: 0.00061965
Iteration 9/25 | Loss: 0.00061965
Iteration 10/25 | Loss: 0.00061965
Iteration 11/25 | Loss: 0.00061965
Iteration 12/25 | Loss: 0.00061965
Iteration 13/25 | Loss: 0.00061965
Iteration 14/25 | Loss: 0.00061965
Iteration 15/25 | Loss: 0.00061965
Iteration 16/25 | Loss: 0.00061965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006196493632160127, 0.0006196493632160127, 0.0006196493632160127, 0.0006196493632160127, 0.0006196493632160127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006196493632160127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48529720
Iteration 2/25 | Loss: 0.00026478
Iteration 3/25 | Loss: 0.00026477
Iteration 4/25 | Loss: 0.00026477
Iteration 5/25 | Loss: 0.00026477
Iteration 6/25 | Loss: 0.00026477
Iteration 7/25 | Loss: 0.00026477
Iteration 8/25 | Loss: 0.00026477
Iteration 9/25 | Loss: 0.00026477
Iteration 10/25 | Loss: 0.00026477
Iteration 11/25 | Loss: 0.00026477
Iteration 12/25 | Loss: 0.00026477
Iteration 13/25 | Loss: 0.00026477
Iteration 14/25 | Loss: 0.00026477
Iteration 15/25 | Loss: 0.00026477
Iteration 16/25 | Loss: 0.00026477
Iteration 17/25 | Loss: 0.00026477
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00026476895436644554, 0.00026476895436644554, 0.00026476895436644554, 0.00026476895436644554, 0.00026476895436644554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026476895436644554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026477
Iteration 2/1000 | Loss: 0.00001878
Iteration 3/1000 | Loss: 0.00001092
Iteration 4/1000 | Loss: 0.00000995
Iteration 5/1000 | Loss: 0.00000935
Iteration 6/1000 | Loss: 0.00000907
Iteration 7/1000 | Loss: 0.00000891
Iteration 8/1000 | Loss: 0.00000888
Iteration 9/1000 | Loss: 0.00000887
Iteration 10/1000 | Loss: 0.00000884
Iteration 11/1000 | Loss: 0.00000884
Iteration 12/1000 | Loss: 0.00000883
Iteration 13/1000 | Loss: 0.00000883
Iteration 14/1000 | Loss: 0.00000882
Iteration 15/1000 | Loss: 0.00000878
Iteration 16/1000 | Loss: 0.00000878
Iteration 17/1000 | Loss: 0.00000878
Iteration 18/1000 | Loss: 0.00000878
Iteration 19/1000 | Loss: 0.00000878
Iteration 20/1000 | Loss: 0.00000878
Iteration 21/1000 | Loss: 0.00000878
Iteration 22/1000 | Loss: 0.00000878
Iteration 23/1000 | Loss: 0.00000877
Iteration 24/1000 | Loss: 0.00000877
Iteration 25/1000 | Loss: 0.00000877
Iteration 26/1000 | Loss: 0.00000875
Iteration 27/1000 | Loss: 0.00000875
Iteration 28/1000 | Loss: 0.00000875
Iteration 29/1000 | Loss: 0.00000875
Iteration 30/1000 | Loss: 0.00000875
Iteration 31/1000 | Loss: 0.00000875
Iteration 32/1000 | Loss: 0.00000875
Iteration 33/1000 | Loss: 0.00000875
Iteration 34/1000 | Loss: 0.00000875
Iteration 35/1000 | Loss: 0.00000874
Iteration 36/1000 | Loss: 0.00000872
Iteration 37/1000 | Loss: 0.00000870
Iteration 38/1000 | Loss: 0.00000870
Iteration 39/1000 | Loss: 0.00000869
Iteration 40/1000 | Loss: 0.00000869
Iteration 41/1000 | Loss: 0.00000869
Iteration 42/1000 | Loss: 0.00000869
Iteration 43/1000 | Loss: 0.00000868
Iteration 44/1000 | Loss: 0.00000868
Iteration 45/1000 | Loss: 0.00000868
Iteration 46/1000 | Loss: 0.00000868
Iteration 47/1000 | Loss: 0.00000868
Iteration 48/1000 | Loss: 0.00000867
Iteration 49/1000 | Loss: 0.00000867
Iteration 50/1000 | Loss: 0.00000867
Iteration 51/1000 | Loss: 0.00000866
Iteration 52/1000 | Loss: 0.00000866
Iteration 53/1000 | Loss: 0.00000865
Iteration 54/1000 | Loss: 0.00000865
Iteration 55/1000 | Loss: 0.00000863
Iteration 56/1000 | Loss: 0.00000863
Iteration 57/1000 | Loss: 0.00000862
Iteration 58/1000 | Loss: 0.00000862
Iteration 59/1000 | Loss: 0.00000862
Iteration 60/1000 | Loss: 0.00000861
Iteration 61/1000 | Loss: 0.00000861
Iteration 62/1000 | Loss: 0.00000861
Iteration 63/1000 | Loss: 0.00000860
Iteration 64/1000 | Loss: 0.00000860
Iteration 65/1000 | Loss: 0.00000859
Iteration 66/1000 | Loss: 0.00000859
Iteration 67/1000 | Loss: 0.00000858
Iteration 68/1000 | Loss: 0.00000858
Iteration 69/1000 | Loss: 0.00000858
Iteration 70/1000 | Loss: 0.00000858
Iteration 71/1000 | Loss: 0.00000857
Iteration 72/1000 | Loss: 0.00000857
Iteration 73/1000 | Loss: 0.00000857
Iteration 74/1000 | Loss: 0.00000857
Iteration 75/1000 | Loss: 0.00000856
Iteration 76/1000 | Loss: 0.00000856
Iteration 77/1000 | Loss: 0.00000856
Iteration 78/1000 | Loss: 0.00000856
Iteration 79/1000 | Loss: 0.00000855
Iteration 80/1000 | Loss: 0.00000855
Iteration 81/1000 | Loss: 0.00000855
Iteration 82/1000 | Loss: 0.00000855
Iteration 83/1000 | Loss: 0.00000854
Iteration 84/1000 | Loss: 0.00000854
Iteration 85/1000 | Loss: 0.00000854
Iteration 86/1000 | Loss: 0.00000853
Iteration 87/1000 | Loss: 0.00000853
Iteration 88/1000 | Loss: 0.00000852
Iteration 89/1000 | Loss: 0.00000852
Iteration 90/1000 | Loss: 0.00000852
Iteration 91/1000 | Loss: 0.00000851
Iteration 92/1000 | Loss: 0.00000851
Iteration 93/1000 | Loss: 0.00000851
Iteration 94/1000 | Loss: 0.00000850
Iteration 95/1000 | Loss: 0.00000850
Iteration 96/1000 | Loss: 0.00000849
Iteration 97/1000 | Loss: 0.00000849
Iteration 98/1000 | Loss: 0.00000849
Iteration 99/1000 | Loss: 0.00000848
Iteration 100/1000 | Loss: 0.00000848
Iteration 101/1000 | Loss: 0.00000848
Iteration 102/1000 | Loss: 0.00000848
Iteration 103/1000 | Loss: 0.00000847
Iteration 104/1000 | Loss: 0.00000847
Iteration 105/1000 | Loss: 0.00000847
Iteration 106/1000 | Loss: 0.00000847
Iteration 107/1000 | Loss: 0.00000846
Iteration 108/1000 | Loss: 0.00000846
Iteration 109/1000 | Loss: 0.00000846
Iteration 110/1000 | Loss: 0.00000846
Iteration 111/1000 | Loss: 0.00000845
Iteration 112/1000 | Loss: 0.00000845
Iteration 113/1000 | Loss: 0.00000845
Iteration 114/1000 | Loss: 0.00000844
Iteration 115/1000 | Loss: 0.00000844
Iteration 116/1000 | Loss: 0.00000844
Iteration 117/1000 | Loss: 0.00000844
Iteration 118/1000 | Loss: 0.00000844
Iteration 119/1000 | Loss: 0.00000844
Iteration 120/1000 | Loss: 0.00000844
Iteration 121/1000 | Loss: 0.00000843
Iteration 122/1000 | Loss: 0.00000843
Iteration 123/1000 | Loss: 0.00000843
Iteration 124/1000 | Loss: 0.00000843
Iteration 125/1000 | Loss: 0.00000843
Iteration 126/1000 | Loss: 0.00000843
Iteration 127/1000 | Loss: 0.00000843
Iteration 128/1000 | Loss: 0.00000843
Iteration 129/1000 | Loss: 0.00000843
Iteration 130/1000 | Loss: 0.00000843
Iteration 131/1000 | Loss: 0.00000843
Iteration 132/1000 | Loss: 0.00000842
Iteration 133/1000 | Loss: 0.00000842
Iteration 134/1000 | Loss: 0.00000842
Iteration 135/1000 | Loss: 0.00000842
Iteration 136/1000 | Loss: 0.00000842
Iteration 137/1000 | Loss: 0.00000842
Iteration 138/1000 | Loss: 0.00000842
Iteration 139/1000 | Loss: 0.00000842
Iteration 140/1000 | Loss: 0.00000842
Iteration 141/1000 | Loss: 0.00000842
Iteration 142/1000 | Loss: 0.00000842
Iteration 143/1000 | Loss: 0.00000842
Iteration 144/1000 | Loss: 0.00000842
Iteration 145/1000 | Loss: 0.00000841
Iteration 146/1000 | Loss: 0.00000841
Iteration 147/1000 | Loss: 0.00000841
Iteration 148/1000 | Loss: 0.00000841
Iteration 149/1000 | Loss: 0.00000841
Iteration 150/1000 | Loss: 0.00000841
Iteration 151/1000 | Loss: 0.00000841
Iteration 152/1000 | Loss: 0.00000841
Iteration 153/1000 | Loss: 0.00000841
Iteration 154/1000 | Loss: 0.00000841
Iteration 155/1000 | Loss: 0.00000841
Iteration 156/1000 | Loss: 0.00000841
Iteration 157/1000 | Loss: 0.00000841
Iteration 158/1000 | Loss: 0.00000840
Iteration 159/1000 | Loss: 0.00000840
Iteration 160/1000 | Loss: 0.00000840
Iteration 161/1000 | Loss: 0.00000840
Iteration 162/1000 | Loss: 0.00000840
Iteration 163/1000 | Loss: 0.00000840
Iteration 164/1000 | Loss: 0.00000840
Iteration 165/1000 | Loss: 0.00000840
Iteration 166/1000 | Loss: 0.00000840
Iteration 167/1000 | Loss: 0.00000840
Iteration 168/1000 | Loss: 0.00000840
Iteration 169/1000 | Loss: 0.00000840
Iteration 170/1000 | Loss: 0.00000839
Iteration 171/1000 | Loss: 0.00000839
Iteration 172/1000 | Loss: 0.00000839
Iteration 173/1000 | Loss: 0.00000839
Iteration 174/1000 | Loss: 0.00000839
Iteration 175/1000 | Loss: 0.00000839
Iteration 176/1000 | Loss: 0.00000839
Iteration 177/1000 | Loss: 0.00000839
Iteration 178/1000 | Loss: 0.00000839
Iteration 179/1000 | Loss: 0.00000838
Iteration 180/1000 | Loss: 0.00000838
Iteration 181/1000 | Loss: 0.00000838
Iteration 182/1000 | Loss: 0.00000838
Iteration 183/1000 | Loss: 0.00000837
Iteration 184/1000 | Loss: 0.00000837
Iteration 185/1000 | Loss: 0.00000837
Iteration 186/1000 | Loss: 0.00000837
Iteration 187/1000 | Loss: 0.00000837
Iteration 188/1000 | Loss: 0.00000837
Iteration 189/1000 | Loss: 0.00000837
Iteration 190/1000 | Loss: 0.00000837
Iteration 191/1000 | Loss: 0.00000837
Iteration 192/1000 | Loss: 0.00000837
Iteration 193/1000 | Loss: 0.00000837
Iteration 194/1000 | Loss: 0.00000837
Iteration 195/1000 | Loss: 0.00000837
Iteration 196/1000 | Loss: 0.00000837
Iteration 197/1000 | Loss: 0.00000837
Iteration 198/1000 | Loss: 0.00000837
Iteration 199/1000 | Loss: 0.00000837
Iteration 200/1000 | Loss: 0.00000837
Iteration 201/1000 | Loss: 0.00000837
Iteration 202/1000 | Loss: 0.00000836
Iteration 203/1000 | Loss: 0.00000836
Iteration 204/1000 | Loss: 0.00000836
Iteration 205/1000 | Loss: 0.00000836
Iteration 206/1000 | Loss: 0.00000836
Iteration 207/1000 | Loss: 0.00000836
Iteration 208/1000 | Loss: 0.00000836
Iteration 209/1000 | Loss: 0.00000836
Iteration 210/1000 | Loss: 0.00000835
Iteration 211/1000 | Loss: 0.00000835
Iteration 212/1000 | Loss: 0.00000835
Iteration 213/1000 | Loss: 0.00000835
Iteration 214/1000 | Loss: 0.00000835
Iteration 215/1000 | Loss: 0.00000835
Iteration 216/1000 | Loss: 0.00000835
Iteration 217/1000 | Loss: 0.00000835
Iteration 218/1000 | Loss: 0.00000835
Iteration 219/1000 | Loss: 0.00000835
Iteration 220/1000 | Loss: 0.00000835
Iteration 221/1000 | Loss: 0.00000835
Iteration 222/1000 | Loss: 0.00000835
Iteration 223/1000 | Loss: 0.00000835
Iteration 224/1000 | Loss: 0.00000835
Iteration 225/1000 | Loss: 0.00000835
Iteration 226/1000 | Loss: 0.00000835
Iteration 227/1000 | Loss: 0.00000835
Iteration 228/1000 | Loss: 0.00000835
Iteration 229/1000 | Loss: 0.00000834
Iteration 230/1000 | Loss: 0.00000834
Iteration 231/1000 | Loss: 0.00000834
Iteration 232/1000 | Loss: 0.00000834
Iteration 233/1000 | Loss: 0.00000834
Iteration 234/1000 | Loss: 0.00000834
Iteration 235/1000 | Loss: 0.00000834
Iteration 236/1000 | Loss: 0.00000834
Iteration 237/1000 | Loss: 0.00000834
Iteration 238/1000 | Loss: 0.00000834
Iteration 239/1000 | Loss: 0.00000834
Iteration 240/1000 | Loss: 0.00000834
Iteration 241/1000 | Loss: 0.00000834
Iteration 242/1000 | Loss: 0.00000834
Iteration 243/1000 | Loss: 0.00000834
Iteration 244/1000 | Loss: 0.00000834
Iteration 245/1000 | Loss: 0.00000833
Iteration 246/1000 | Loss: 0.00000833
Iteration 247/1000 | Loss: 0.00000833
Iteration 248/1000 | Loss: 0.00000833
Iteration 249/1000 | Loss: 0.00000833
Iteration 250/1000 | Loss: 0.00000833
Iteration 251/1000 | Loss: 0.00000833
Iteration 252/1000 | Loss: 0.00000833
Iteration 253/1000 | Loss: 0.00000833
Iteration 254/1000 | Loss: 0.00000833
Iteration 255/1000 | Loss: 0.00000833
Iteration 256/1000 | Loss: 0.00000833
Iteration 257/1000 | Loss: 0.00000833
Iteration 258/1000 | Loss: 0.00000833
Iteration 259/1000 | Loss: 0.00000833
Iteration 260/1000 | Loss: 0.00000833
Iteration 261/1000 | Loss: 0.00000833
Iteration 262/1000 | Loss: 0.00000833
Iteration 263/1000 | Loss: 0.00000833
Iteration 264/1000 | Loss: 0.00000833
Iteration 265/1000 | Loss: 0.00000833
Iteration 266/1000 | Loss: 0.00000833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [8.329257980221882e-06, 8.329257980221882e-06, 8.329257980221882e-06, 8.329257980221882e-06, 8.329257980221882e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.329257980221882e-06

Optimization complete. Final v2v error: 2.4791324138641357 mm

Highest mean error: 2.702685832977295 mm for frame 78

Lowest mean error: 2.408062219619751 mm for frame 108

Saving results

Total time: 38.92583632469177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00536377
Iteration 2/25 | Loss: 0.00093768
Iteration 3/25 | Loss: 0.00076636
Iteration 4/25 | Loss: 0.00074270
Iteration 5/25 | Loss: 0.00073679
Iteration 6/25 | Loss: 0.00073491
Iteration 7/25 | Loss: 0.00073471
Iteration 8/25 | Loss: 0.00073471
Iteration 9/25 | Loss: 0.00073471
Iteration 10/25 | Loss: 0.00073471
Iteration 11/25 | Loss: 0.00073471
Iteration 12/25 | Loss: 0.00073471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007347064092755318, 0.0007347064092755318, 0.0007347064092755318, 0.0007347064092755318, 0.0007347064092755318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007347064092755318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47126281
Iteration 2/25 | Loss: 0.00035313
Iteration 3/25 | Loss: 0.00035310
Iteration 4/25 | Loss: 0.00035310
Iteration 5/25 | Loss: 0.00035310
Iteration 6/25 | Loss: 0.00035310
Iteration 7/25 | Loss: 0.00035310
Iteration 8/25 | Loss: 0.00035310
Iteration 9/25 | Loss: 0.00035309
Iteration 10/25 | Loss: 0.00035309
Iteration 11/25 | Loss: 0.00035309
Iteration 12/25 | Loss: 0.00035309
Iteration 13/25 | Loss: 0.00035309
Iteration 14/25 | Loss: 0.00035309
Iteration 15/25 | Loss: 0.00035309
Iteration 16/25 | Loss: 0.00035309
Iteration 17/25 | Loss: 0.00035309
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003530945105012506, 0.0003530945105012506, 0.0003530945105012506, 0.0003530945105012506, 0.0003530945105012506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003530945105012506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035309
Iteration 2/1000 | Loss: 0.00003818
Iteration 3/1000 | Loss: 0.00002356
Iteration 4/1000 | Loss: 0.00002122
Iteration 5/1000 | Loss: 0.00002004
Iteration 6/1000 | Loss: 0.00001949
Iteration 7/1000 | Loss: 0.00001907
Iteration 8/1000 | Loss: 0.00001870
Iteration 9/1000 | Loss: 0.00001843
Iteration 10/1000 | Loss: 0.00001830
Iteration 11/1000 | Loss: 0.00001818
Iteration 12/1000 | Loss: 0.00001801
Iteration 13/1000 | Loss: 0.00001800
Iteration 14/1000 | Loss: 0.00001794
Iteration 15/1000 | Loss: 0.00001792
Iteration 16/1000 | Loss: 0.00001791
Iteration 17/1000 | Loss: 0.00001785
Iteration 18/1000 | Loss: 0.00001785
Iteration 19/1000 | Loss: 0.00001781
Iteration 20/1000 | Loss: 0.00001780
Iteration 21/1000 | Loss: 0.00001779
Iteration 22/1000 | Loss: 0.00001779
Iteration 23/1000 | Loss: 0.00001778
Iteration 24/1000 | Loss: 0.00001777
Iteration 25/1000 | Loss: 0.00001777
Iteration 26/1000 | Loss: 0.00001777
Iteration 27/1000 | Loss: 0.00001776
Iteration 28/1000 | Loss: 0.00001775
Iteration 29/1000 | Loss: 0.00001775
Iteration 30/1000 | Loss: 0.00001775
Iteration 31/1000 | Loss: 0.00001774
Iteration 32/1000 | Loss: 0.00001774
Iteration 33/1000 | Loss: 0.00001773
Iteration 34/1000 | Loss: 0.00001773
Iteration 35/1000 | Loss: 0.00001771
Iteration 36/1000 | Loss: 0.00001771
Iteration 37/1000 | Loss: 0.00001770
Iteration 38/1000 | Loss: 0.00001770
Iteration 39/1000 | Loss: 0.00001769
Iteration 40/1000 | Loss: 0.00001769
Iteration 41/1000 | Loss: 0.00001769
Iteration 42/1000 | Loss: 0.00001769
Iteration 43/1000 | Loss: 0.00001769
Iteration 44/1000 | Loss: 0.00001769
Iteration 45/1000 | Loss: 0.00001769
Iteration 46/1000 | Loss: 0.00001769
Iteration 47/1000 | Loss: 0.00001769
Iteration 48/1000 | Loss: 0.00001768
Iteration 49/1000 | Loss: 0.00001768
Iteration 50/1000 | Loss: 0.00001768
Iteration 51/1000 | Loss: 0.00001768
Iteration 52/1000 | Loss: 0.00001765
Iteration 53/1000 | Loss: 0.00001765
Iteration 54/1000 | Loss: 0.00001765
Iteration 55/1000 | Loss: 0.00001765
Iteration 56/1000 | Loss: 0.00001765
Iteration 57/1000 | Loss: 0.00001765
Iteration 58/1000 | Loss: 0.00001765
Iteration 59/1000 | Loss: 0.00001765
Iteration 60/1000 | Loss: 0.00001765
Iteration 61/1000 | Loss: 0.00001764
Iteration 62/1000 | Loss: 0.00001764
Iteration 63/1000 | Loss: 0.00001763
Iteration 64/1000 | Loss: 0.00001762
Iteration 65/1000 | Loss: 0.00001762
Iteration 66/1000 | Loss: 0.00001762
Iteration 67/1000 | Loss: 0.00001761
Iteration 68/1000 | Loss: 0.00001761
Iteration 69/1000 | Loss: 0.00001761
Iteration 70/1000 | Loss: 0.00001761
Iteration 71/1000 | Loss: 0.00001761
Iteration 72/1000 | Loss: 0.00001761
Iteration 73/1000 | Loss: 0.00001760
Iteration 74/1000 | Loss: 0.00001760
Iteration 75/1000 | Loss: 0.00001760
Iteration 76/1000 | Loss: 0.00001760
Iteration 77/1000 | Loss: 0.00001760
Iteration 78/1000 | Loss: 0.00001760
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001759
Iteration 81/1000 | Loss: 0.00001759
Iteration 82/1000 | Loss: 0.00001758
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001758
Iteration 86/1000 | Loss: 0.00001758
Iteration 87/1000 | Loss: 0.00001758
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00001757
Iteration 90/1000 | Loss: 0.00001757
Iteration 91/1000 | Loss: 0.00001757
Iteration 92/1000 | Loss: 0.00001757
Iteration 93/1000 | Loss: 0.00001757
Iteration 94/1000 | Loss: 0.00001757
Iteration 95/1000 | Loss: 0.00001757
Iteration 96/1000 | Loss: 0.00001756
Iteration 97/1000 | Loss: 0.00001756
Iteration 98/1000 | Loss: 0.00001756
Iteration 99/1000 | Loss: 0.00001756
Iteration 100/1000 | Loss: 0.00001756
Iteration 101/1000 | Loss: 0.00001756
Iteration 102/1000 | Loss: 0.00001755
Iteration 103/1000 | Loss: 0.00001755
Iteration 104/1000 | Loss: 0.00001755
Iteration 105/1000 | Loss: 0.00001755
Iteration 106/1000 | Loss: 0.00001755
Iteration 107/1000 | Loss: 0.00001755
Iteration 108/1000 | Loss: 0.00001755
Iteration 109/1000 | Loss: 0.00001755
Iteration 110/1000 | Loss: 0.00001755
Iteration 111/1000 | Loss: 0.00001755
Iteration 112/1000 | Loss: 0.00001754
Iteration 113/1000 | Loss: 0.00001754
Iteration 114/1000 | Loss: 0.00001754
Iteration 115/1000 | Loss: 0.00001754
Iteration 116/1000 | Loss: 0.00001754
Iteration 117/1000 | Loss: 0.00001754
Iteration 118/1000 | Loss: 0.00001754
Iteration 119/1000 | Loss: 0.00001754
Iteration 120/1000 | Loss: 0.00001754
Iteration 121/1000 | Loss: 0.00001754
Iteration 122/1000 | Loss: 0.00001754
Iteration 123/1000 | Loss: 0.00001754
Iteration 124/1000 | Loss: 0.00001754
Iteration 125/1000 | Loss: 0.00001754
Iteration 126/1000 | Loss: 0.00001753
Iteration 127/1000 | Loss: 0.00001753
Iteration 128/1000 | Loss: 0.00001753
Iteration 129/1000 | Loss: 0.00001753
Iteration 130/1000 | Loss: 0.00001753
Iteration 131/1000 | Loss: 0.00001753
Iteration 132/1000 | Loss: 0.00001753
Iteration 133/1000 | Loss: 0.00001753
Iteration 134/1000 | Loss: 0.00001753
Iteration 135/1000 | Loss: 0.00001753
Iteration 136/1000 | Loss: 0.00001753
Iteration 137/1000 | Loss: 0.00001753
Iteration 138/1000 | Loss: 0.00001753
Iteration 139/1000 | Loss: 0.00001753
Iteration 140/1000 | Loss: 0.00001753
Iteration 141/1000 | Loss: 0.00001753
Iteration 142/1000 | Loss: 0.00001753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.7528220269014128e-05, 1.7528220269014128e-05, 1.7528220269014128e-05, 1.7528220269014128e-05, 1.7528220269014128e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7528220269014128e-05

Optimization complete. Final v2v error: 3.5285134315490723 mm

Highest mean error: 3.9485785961151123 mm for frame 101

Lowest mean error: 2.990220546722412 mm for frame 36

Saving results

Total time: 43.14202952384949
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058334
Iteration 2/25 | Loss: 0.00213552
Iteration 3/25 | Loss: 0.00150521
Iteration 4/25 | Loss: 0.00126302
Iteration 5/25 | Loss: 0.00114061
Iteration 6/25 | Loss: 0.00112664
Iteration 7/25 | Loss: 0.00108129
Iteration 8/25 | Loss: 0.00107509
Iteration 9/25 | Loss: 0.00103999
Iteration 10/25 | Loss: 0.00102890
Iteration 11/25 | Loss: 0.00103414
Iteration 12/25 | Loss: 0.00102704
Iteration 13/25 | Loss: 0.00103603
Iteration 14/25 | Loss: 0.00100872
Iteration 15/25 | Loss: 0.00099364
Iteration 16/25 | Loss: 0.00098017
Iteration 17/25 | Loss: 0.00095690
Iteration 18/25 | Loss: 0.00093895
Iteration 19/25 | Loss: 0.00093890
Iteration 20/25 | Loss: 0.00092587
Iteration 21/25 | Loss: 0.00092378
Iteration 22/25 | Loss: 0.00091681
Iteration 23/25 | Loss: 0.00091303
Iteration 24/25 | Loss: 0.00090814
Iteration 25/25 | Loss: 0.00091065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52467012
Iteration 2/25 | Loss: 0.00089992
Iteration 3/25 | Loss: 0.00063052
Iteration 4/25 | Loss: 0.00063052
Iteration 5/25 | Loss: 0.00063051
Iteration 6/25 | Loss: 0.00063051
Iteration 7/25 | Loss: 0.00063051
Iteration 8/25 | Loss: 0.00063051
Iteration 9/25 | Loss: 0.00063051
Iteration 10/25 | Loss: 0.00063051
Iteration 11/25 | Loss: 0.00063051
Iteration 12/25 | Loss: 0.00063051
Iteration 13/25 | Loss: 0.00063051
Iteration 14/25 | Loss: 0.00063051
Iteration 15/25 | Loss: 0.00063051
Iteration 16/25 | Loss: 0.00063051
Iteration 17/25 | Loss: 0.00063051
Iteration 18/25 | Loss: 0.00063051
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000630513415671885, 0.000630513415671885, 0.000630513415671885, 0.000630513415671885, 0.000630513415671885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000630513415671885

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063051
Iteration 2/1000 | Loss: 0.00029685
Iteration 3/1000 | Loss: 0.00021197
Iteration 4/1000 | Loss: 0.00034715
Iteration 5/1000 | Loss: 0.00017893
Iteration 6/1000 | Loss: 0.00069753
Iteration 7/1000 | Loss: 0.00015462
Iteration 8/1000 | Loss: 0.00013861
Iteration 9/1000 | Loss: 0.00012659
Iteration 10/1000 | Loss: 0.00019665
Iteration 11/1000 | Loss: 0.00014973
Iteration 12/1000 | Loss: 0.00015069
Iteration 13/1000 | Loss: 0.00014068
Iteration 14/1000 | Loss: 0.00009287
Iteration 15/1000 | Loss: 0.00010787
Iteration 16/1000 | Loss: 0.00027047
Iteration 17/1000 | Loss: 0.00039381
Iteration 18/1000 | Loss: 0.00010812
Iteration 19/1000 | Loss: 0.00016374
Iteration 20/1000 | Loss: 0.00015194
Iteration 21/1000 | Loss: 0.00010014
Iteration 22/1000 | Loss: 0.00009810
Iteration 23/1000 | Loss: 0.00014706
Iteration 24/1000 | Loss: 0.00013070
Iteration 25/1000 | Loss: 0.00005751
Iteration 26/1000 | Loss: 0.00019858
Iteration 27/1000 | Loss: 0.00011588
Iteration 28/1000 | Loss: 0.00036492
Iteration 29/1000 | Loss: 0.00015472
Iteration 30/1000 | Loss: 0.00012611
Iteration 31/1000 | Loss: 0.00016956
Iteration 32/1000 | Loss: 0.00015120
Iteration 33/1000 | Loss: 0.00017359
Iteration 34/1000 | Loss: 0.00072777
Iteration 35/1000 | Loss: 0.00040236
Iteration 36/1000 | Loss: 0.00006637
Iteration 37/1000 | Loss: 0.00009129
Iteration 38/1000 | Loss: 0.00037704
Iteration 39/1000 | Loss: 0.00040341
Iteration 40/1000 | Loss: 0.00028382
Iteration 41/1000 | Loss: 0.00023020
Iteration 42/1000 | Loss: 0.00030494
Iteration 43/1000 | Loss: 0.00020542
Iteration 44/1000 | Loss: 0.00024480
Iteration 45/1000 | Loss: 0.00017635
Iteration 46/1000 | Loss: 0.00021343
Iteration 47/1000 | Loss: 0.00026356
Iteration 48/1000 | Loss: 0.00038172
Iteration 49/1000 | Loss: 0.00018999
Iteration 50/1000 | Loss: 0.00074464
Iteration 51/1000 | Loss: 0.00052249
Iteration 52/1000 | Loss: 0.00066862
Iteration 53/1000 | Loss: 0.00069049
Iteration 54/1000 | Loss: 0.00024074
Iteration 55/1000 | Loss: 0.00032309
Iteration 56/1000 | Loss: 0.00029398
Iteration 57/1000 | Loss: 0.00008853
Iteration 58/1000 | Loss: 0.00004168
Iteration 59/1000 | Loss: 0.00011491
Iteration 60/1000 | Loss: 0.00014586
Iteration 61/1000 | Loss: 0.00012027
Iteration 62/1000 | Loss: 0.00010238
Iteration 63/1000 | Loss: 0.00011991
Iteration 64/1000 | Loss: 0.00012024
Iteration 65/1000 | Loss: 0.00012178
Iteration 66/1000 | Loss: 0.00006237
Iteration 67/1000 | Loss: 0.00014520
Iteration 68/1000 | Loss: 0.00022334
Iteration 69/1000 | Loss: 0.00029763
Iteration 70/1000 | Loss: 0.00014543
Iteration 71/1000 | Loss: 0.00025099
Iteration 72/1000 | Loss: 0.00021087
Iteration 73/1000 | Loss: 0.00036436
Iteration 74/1000 | Loss: 0.00012939
Iteration 75/1000 | Loss: 0.00030867
Iteration 76/1000 | Loss: 0.00022167
Iteration 77/1000 | Loss: 0.00006388
Iteration 78/1000 | Loss: 0.00011977
Iteration 79/1000 | Loss: 0.00046491
Iteration 80/1000 | Loss: 0.00008071
Iteration 81/1000 | Loss: 0.00018532
Iteration 82/1000 | Loss: 0.00014223
Iteration 83/1000 | Loss: 0.00025011
Iteration 84/1000 | Loss: 0.00026666
Iteration 85/1000 | Loss: 0.00060462
Iteration 86/1000 | Loss: 0.00011598
Iteration 87/1000 | Loss: 0.00023307
Iteration 88/1000 | Loss: 0.00004463
Iteration 89/1000 | Loss: 0.00004037
Iteration 90/1000 | Loss: 0.00003853
Iteration 91/1000 | Loss: 0.00003747
Iteration 92/1000 | Loss: 0.00003717
Iteration 93/1000 | Loss: 0.00003709
Iteration 94/1000 | Loss: 0.00003687
Iteration 95/1000 | Loss: 0.00012887
Iteration 96/1000 | Loss: 0.00012887
Iteration 97/1000 | Loss: 0.00042648
Iteration 98/1000 | Loss: 0.00004669
Iteration 99/1000 | Loss: 0.00003758
Iteration 100/1000 | Loss: 0.00008580
Iteration 101/1000 | Loss: 0.00003667
Iteration 102/1000 | Loss: 0.00003617
Iteration 103/1000 | Loss: 0.00003596
Iteration 104/1000 | Loss: 0.00003589
Iteration 105/1000 | Loss: 0.00003585
Iteration 106/1000 | Loss: 0.00003564
Iteration 107/1000 | Loss: 0.00003560
Iteration 108/1000 | Loss: 0.00024520
Iteration 109/1000 | Loss: 0.00087113
Iteration 110/1000 | Loss: 0.00007768
Iteration 111/1000 | Loss: 0.00005092
Iteration 112/1000 | Loss: 0.00003711
Iteration 113/1000 | Loss: 0.00003521
Iteration 114/1000 | Loss: 0.00003399
Iteration 115/1000 | Loss: 0.00003349
Iteration 116/1000 | Loss: 0.00003306
Iteration 117/1000 | Loss: 0.00011286
Iteration 118/1000 | Loss: 0.00003278
Iteration 119/1000 | Loss: 0.00003264
Iteration 120/1000 | Loss: 0.00003251
Iteration 121/1000 | Loss: 0.00003248
Iteration 122/1000 | Loss: 0.00003248
Iteration 123/1000 | Loss: 0.00008796
Iteration 124/1000 | Loss: 0.00005991
Iteration 125/1000 | Loss: 0.00003240
Iteration 126/1000 | Loss: 0.00003239
Iteration 127/1000 | Loss: 0.00003239
Iteration 128/1000 | Loss: 0.00007741
Iteration 129/1000 | Loss: 0.00005413
Iteration 130/1000 | Loss: 0.00003242
Iteration 131/1000 | Loss: 0.00003237
Iteration 132/1000 | Loss: 0.00003236
Iteration 133/1000 | Loss: 0.00003236
Iteration 134/1000 | Loss: 0.00003235
Iteration 135/1000 | Loss: 0.00003235
Iteration 136/1000 | Loss: 0.00003235
Iteration 137/1000 | Loss: 0.00004889
Iteration 138/1000 | Loss: 0.00027109
Iteration 139/1000 | Loss: 0.00015449
Iteration 140/1000 | Loss: 0.00008967
Iteration 141/1000 | Loss: 0.00005556
Iteration 142/1000 | Loss: 0.00003622
Iteration 143/1000 | Loss: 0.00003514
Iteration 144/1000 | Loss: 0.00003853
Iteration 145/1000 | Loss: 0.00003230
Iteration 146/1000 | Loss: 0.00003230
Iteration 147/1000 | Loss: 0.00003230
Iteration 148/1000 | Loss: 0.00003229
Iteration 149/1000 | Loss: 0.00003229
Iteration 150/1000 | Loss: 0.00003229
Iteration 151/1000 | Loss: 0.00003229
Iteration 152/1000 | Loss: 0.00003229
Iteration 153/1000 | Loss: 0.00003229
Iteration 154/1000 | Loss: 0.00003229
Iteration 155/1000 | Loss: 0.00003228
Iteration 156/1000 | Loss: 0.00003228
Iteration 157/1000 | Loss: 0.00003228
Iteration 158/1000 | Loss: 0.00003228
Iteration 159/1000 | Loss: 0.00003228
Iteration 160/1000 | Loss: 0.00003228
Iteration 161/1000 | Loss: 0.00003228
Iteration 162/1000 | Loss: 0.00003228
Iteration 163/1000 | Loss: 0.00003228
Iteration 164/1000 | Loss: 0.00003227
Iteration 165/1000 | Loss: 0.00003227
Iteration 166/1000 | Loss: 0.00003227
Iteration 167/1000 | Loss: 0.00003226
Iteration 168/1000 | Loss: 0.00003226
Iteration 169/1000 | Loss: 0.00003226
Iteration 170/1000 | Loss: 0.00003225
Iteration 171/1000 | Loss: 0.00003225
Iteration 172/1000 | Loss: 0.00003224
Iteration 173/1000 | Loss: 0.00003778
Iteration 174/1000 | Loss: 0.00036362
Iteration 175/1000 | Loss: 0.00003263
Iteration 176/1000 | Loss: 0.00003180
Iteration 177/1000 | Loss: 0.00003128
Iteration 178/1000 | Loss: 0.00009194
Iteration 179/1000 | Loss: 0.00003373
Iteration 180/1000 | Loss: 0.00003032
Iteration 181/1000 | Loss: 0.00005648
Iteration 182/1000 | Loss: 0.00008873
Iteration 183/1000 | Loss: 0.00002998
Iteration 184/1000 | Loss: 0.00002989
Iteration 185/1000 | Loss: 0.00002979
Iteration 186/1000 | Loss: 0.00003101
Iteration 187/1000 | Loss: 0.00003100
Iteration 188/1000 | Loss: 0.00013368
Iteration 189/1000 | Loss: 0.00003137
Iteration 190/1000 | Loss: 0.00002953
Iteration 191/1000 | Loss: 0.00002953
Iteration 192/1000 | Loss: 0.00002946
Iteration 193/1000 | Loss: 0.00002938
Iteration 194/1000 | Loss: 0.00002934
Iteration 195/1000 | Loss: 0.00002932
Iteration 196/1000 | Loss: 0.00002930
Iteration 197/1000 | Loss: 0.00002929
Iteration 198/1000 | Loss: 0.00002928
Iteration 199/1000 | Loss: 0.00002928
Iteration 200/1000 | Loss: 0.00002928
Iteration 201/1000 | Loss: 0.00002927
Iteration 202/1000 | Loss: 0.00002927
Iteration 203/1000 | Loss: 0.00002927
Iteration 204/1000 | Loss: 0.00002927
Iteration 205/1000 | Loss: 0.00002926
Iteration 206/1000 | Loss: 0.00002926
Iteration 207/1000 | Loss: 0.00002925
Iteration 208/1000 | Loss: 0.00002925
Iteration 209/1000 | Loss: 0.00002925
Iteration 210/1000 | Loss: 0.00002924
Iteration 211/1000 | Loss: 0.00002924
Iteration 212/1000 | Loss: 0.00002923
Iteration 213/1000 | Loss: 0.00002923
Iteration 214/1000 | Loss: 0.00002922
Iteration 215/1000 | Loss: 0.00002922
Iteration 216/1000 | Loss: 0.00002922
Iteration 217/1000 | Loss: 0.00002922
Iteration 218/1000 | Loss: 0.00002921
Iteration 219/1000 | Loss: 0.00002921
Iteration 220/1000 | Loss: 0.00002921
Iteration 221/1000 | Loss: 0.00002921
Iteration 222/1000 | Loss: 0.00002921
Iteration 223/1000 | Loss: 0.00002921
Iteration 224/1000 | Loss: 0.00002920
Iteration 225/1000 | Loss: 0.00002920
Iteration 226/1000 | Loss: 0.00002920
Iteration 227/1000 | Loss: 0.00002920
Iteration 228/1000 | Loss: 0.00002920
Iteration 229/1000 | Loss: 0.00002920
Iteration 230/1000 | Loss: 0.00002920
Iteration 231/1000 | Loss: 0.00002920
Iteration 232/1000 | Loss: 0.00002920
Iteration 233/1000 | Loss: 0.00002920
Iteration 234/1000 | Loss: 0.00002919
Iteration 235/1000 | Loss: 0.00002919
Iteration 236/1000 | Loss: 0.00002919
Iteration 237/1000 | Loss: 0.00002918
Iteration 238/1000 | Loss: 0.00002918
Iteration 239/1000 | Loss: 0.00002918
Iteration 240/1000 | Loss: 0.00002918
Iteration 241/1000 | Loss: 0.00002918
Iteration 242/1000 | Loss: 0.00002918
Iteration 243/1000 | Loss: 0.00002918
Iteration 244/1000 | Loss: 0.00002918
Iteration 245/1000 | Loss: 0.00002918
Iteration 246/1000 | Loss: 0.00002918
Iteration 247/1000 | Loss: 0.00002917
Iteration 248/1000 | Loss: 0.00002917
Iteration 249/1000 | Loss: 0.00002917
Iteration 250/1000 | Loss: 0.00002917
Iteration 251/1000 | Loss: 0.00002917
Iteration 252/1000 | Loss: 0.00002917
Iteration 253/1000 | Loss: 0.00002917
Iteration 254/1000 | Loss: 0.00002916
Iteration 255/1000 | Loss: 0.00002916
Iteration 256/1000 | Loss: 0.00002916
Iteration 257/1000 | Loss: 0.00002916
Iteration 258/1000 | Loss: 0.00002916
Iteration 259/1000 | Loss: 0.00002916
Iteration 260/1000 | Loss: 0.00002916
Iteration 261/1000 | Loss: 0.00002915
Iteration 262/1000 | Loss: 0.00002915
Iteration 263/1000 | Loss: 0.00002915
Iteration 264/1000 | Loss: 0.00002915
Iteration 265/1000 | Loss: 0.00002914
Iteration 266/1000 | Loss: 0.00002914
Iteration 267/1000 | Loss: 0.00002914
Iteration 268/1000 | Loss: 0.00002914
Iteration 269/1000 | Loss: 0.00002914
Iteration 270/1000 | Loss: 0.00002914
Iteration 271/1000 | Loss: 0.00002914
Iteration 272/1000 | Loss: 0.00002914
Iteration 273/1000 | Loss: 0.00002914
Iteration 274/1000 | Loss: 0.00002914
Iteration 275/1000 | Loss: 0.00002914
Iteration 276/1000 | Loss: 0.00002914
Iteration 277/1000 | Loss: 0.00002914
Iteration 278/1000 | Loss: 0.00002914
Iteration 279/1000 | Loss: 0.00002914
Iteration 280/1000 | Loss: 0.00002914
Iteration 281/1000 | Loss: 0.00002914
Iteration 282/1000 | Loss: 0.00002914
Iteration 283/1000 | Loss: 0.00002914
Iteration 284/1000 | Loss: 0.00002914
Iteration 285/1000 | Loss: 0.00002914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [2.9137181627447717e-05, 2.9137181627447717e-05, 2.9137181627447717e-05, 2.9137181627447717e-05, 2.9137181627447717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9137181627447717e-05

Optimization complete. Final v2v error: 4.242924213409424 mm

Highest mean error: 15.655914306640625 mm for frame 91

Lowest mean error: 3.6172595024108887 mm for frame 195

Saving results

Total time: 278.6236298084259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01120289
Iteration 2/25 | Loss: 0.00485864
Iteration 3/25 | Loss: 0.00560686
Iteration 4/25 | Loss: 0.00240556
Iteration 5/25 | Loss: 0.00216662
Iteration 6/25 | Loss: 0.00183146
Iteration 7/25 | Loss: 0.00180480
Iteration 8/25 | Loss: 0.00176289
Iteration 9/25 | Loss: 0.00170706
Iteration 10/25 | Loss: 0.00169126
Iteration 11/25 | Loss: 0.00167480
Iteration 12/25 | Loss: 0.00165115
Iteration 13/25 | Loss: 0.00164128
Iteration 14/25 | Loss: 0.00164509
Iteration 15/25 | Loss: 0.00164363
Iteration 16/25 | Loss: 0.00163972
Iteration 17/25 | Loss: 0.00162764
Iteration 18/25 | Loss: 0.00162125
Iteration 19/25 | Loss: 0.00161755
Iteration 20/25 | Loss: 0.00161825
Iteration 21/25 | Loss: 0.00161942
Iteration 22/25 | Loss: 0.00162011
Iteration 23/25 | Loss: 0.00161359
Iteration 24/25 | Loss: 0.00161678
Iteration 25/25 | Loss: 0.00161563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.55142021
Iteration 2/25 | Loss: 0.00657905
Iteration 3/25 | Loss: 0.00657905
Iteration 4/25 | Loss: 0.00657905
Iteration 5/25 | Loss: 0.00657905
Iteration 6/25 | Loss: 0.00657904
Iteration 7/25 | Loss: 0.00657904
Iteration 8/25 | Loss: 0.00657904
Iteration 9/25 | Loss: 0.00657904
Iteration 10/25 | Loss: 0.00657904
Iteration 11/25 | Loss: 0.00657904
Iteration 12/25 | Loss: 0.00657904
Iteration 13/25 | Loss: 0.00657904
Iteration 14/25 | Loss: 0.00657904
Iteration 15/25 | Loss: 0.00657904
Iteration 16/25 | Loss: 0.00657904
Iteration 17/25 | Loss: 0.00657904
Iteration 18/25 | Loss: 0.00657904
Iteration 19/25 | Loss: 0.00657904
Iteration 20/25 | Loss: 0.00657904
Iteration 21/25 | Loss: 0.00657904
Iteration 22/25 | Loss: 0.00657904
Iteration 23/25 | Loss: 0.00657904
Iteration 24/25 | Loss: 0.00657904
Iteration 25/25 | Loss: 0.00657904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00657904
Iteration 2/1000 | Loss: 0.00077311
Iteration 3/1000 | Loss: 0.00056136
Iteration 4/1000 | Loss: 0.00046037
Iteration 5/1000 | Loss: 0.00039507
Iteration 6/1000 | Loss: 0.00037168
Iteration 7/1000 | Loss: 0.00035555
Iteration 8/1000 | Loss: 0.00034140
Iteration 9/1000 | Loss: 0.00032562
Iteration 10/1000 | Loss: 0.00031954
Iteration 11/1000 | Loss: 0.00033299
Iteration 12/1000 | Loss: 0.00033033
Iteration 13/1000 | Loss: 0.00032490
Iteration 14/1000 | Loss: 0.00032295
Iteration 15/1000 | Loss: 0.00032156
Iteration 16/1000 | Loss: 0.00031893
Iteration 17/1000 | Loss: 0.00031764
Iteration 18/1000 | Loss: 0.00032520
Iteration 19/1000 | Loss: 0.00031221
Iteration 20/1000 | Loss: 0.00031948
Iteration 21/1000 | Loss: 0.00032771
Iteration 22/1000 | Loss: 0.00032865
Iteration 23/1000 | Loss: 0.00031907
Iteration 24/1000 | Loss: 0.00032720
Iteration 25/1000 | Loss: 0.00032783
Iteration 26/1000 | Loss: 0.00032475
Iteration 27/1000 | Loss: 0.00062159
Iteration 28/1000 | Loss: 0.00033989
Iteration 29/1000 | Loss: 0.00033907
Iteration 30/1000 | Loss: 0.00033166
Iteration 31/1000 | Loss: 0.00032573
Iteration 32/1000 | Loss: 0.00032774
Iteration 33/1000 | Loss: 0.00032225
Iteration 34/1000 | Loss: 0.00032278
Iteration 35/1000 | Loss: 0.00032400
Iteration 36/1000 | Loss: 0.00031872
Iteration 37/1000 | Loss: 0.00031530
Iteration 38/1000 | Loss: 0.00031452
Iteration 39/1000 | Loss: 0.00032075
Iteration 40/1000 | Loss: 0.00031865
Iteration 41/1000 | Loss: 0.00032625
Iteration 42/1000 | Loss: 0.00032470
Iteration 43/1000 | Loss: 0.00032218
Iteration 44/1000 | Loss: 0.00047520
Iteration 45/1000 | Loss: 0.00066107
Iteration 46/1000 | Loss: 0.00050846
Iteration 47/1000 | Loss: 0.00058386
Iteration 48/1000 | Loss: 0.00051264
Iteration 49/1000 | Loss: 0.00051489
Iteration 50/1000 | Loss: 0.00063741
Iteration 51/1000 | Loss: 0.00035138
Iteration 52/1000 | Loss: 0.00042445
Iteration 53/1000 | Loss: 0.00033265
Iteration 54/1000 | Loss: 0.00033021
Iteration 55/1000 | Loss: 0.00031959
Iteration 56/1000 | Loss: 0.00031604
Iteration 57/1000 | Loss: 0.00031471
Iteration 58/1000 | Loss: 0.00031266
Iteration 59/1000 | Loss: 0.00033400
Iteration 60/1000 | Loss: 0.00032129
Iteration 61/1000 | Loss: 0.00031830
Iteration 62/1000 | Loss: 0.00061103
Iteration 63/1000 | Loss: 0.00032486
Iteration 64/1000 | Loss: 0.00032672
Iteration 65/1000 | Loss: 0.00064064
Iteration 66/1000 | Loss: 0.00056507
Iteration 67/1000 | Loss: 0.00034928
Iteration 68/1000 | Loss: 0.00032698
Iteration 69/1000 | Loss: 0.00031061
Iteration 70/1000 | Loss: 0.00030898
Iteration 71/1000 | Loss: 0.00030803
Iteration 72/1000 | Loss: 0.00061995
Iteration 73/1000 | Loss: 0.00037334
Iteration 74/1000 | Loss: 0.00031968
Iteration 75/1000 | Loss: 0.00031416
Iteration 76/1000 | Loss: 0.00030831
Iteration 77/1000 | Loss: 0.00030681
Iteration 78/1000 | Loss: 0.00059140
Iteration 79/1000 | Loss: 0.00035178
Iteration 80/1000 | Loss: 0.00032293
Iteration 81/1000 | Loss: 0.00031237
Iteration 82/1000 | Loss: 0.00030938
Iteration 83/1000 | Loss: 0.00032026
Iteration 84/1000 | Loss: 0.00057163
Iteration 85/1000 | Loss: 0.00033964
Iteration 86/1000 | Loss: 0.00033332
Iteration 87/1000 | Loss: 0.00049852
Iteration 88/1000 | Loss: 0.00061728
Iteration 89/1000 | Loss: 0.00046639
Iteration 90/1000 | Loss: 0.00033188
Iteration 91/1000 | Loss: 0.00032162
Iteration 92/1000 | Loss: 0.00031303
Iteration 93/1000 | Loss: 0.00031016
Iteration 94/1000 | Loss: 0.00030879
Iteration 95/1000 | Loss: 0.00030832
Iteration 96/1000 | Loss: 0.00030781
Iteration 97/1000 | Loss: 0.00030742
Iteration 98/1000 | Loss: 0.00039655
Iteration 99/1000 | Loss: 0.00041683
Iteration 100/1000 | Loss: 0.00038870
Iteration 101/1000 | Loss: 0.00031070
Iteration 102/1000 | Loss: 0.00030739
Iteration 103/1000 | Loss: 0.00030594
Iteration 104/1000 | Loss: 0.00030506
Iteration 105/1000 | Loss: 0.00030460
Iteration 106/1000 | Loss: 0.00030426
Iteration 107/1000 | Loss: 0.00030414
Iteration 108/1000 | Loss: 0.00030413
Iteration 109/1000 | Loss: 0.00030412
Iteration 110/1000 | Loss: 0.00030407
Iteration 111/1000 | Loss: 0.00030407
Iteration 112/1000 | Loss: 0.00030405
Iteration 113/1000 | Loss: 0.00030405
Iteration 114/1000 | Loss: 0.00030404
Iteration 115/1000 | Loss: 0.00030404
Iteration 116/1000 | Loss: 0.00030404
Iteration 117/1000 | Loss: 0.00030404
Iteration 118/1000 | Loss: 0.00030402
Iteration 119/1000 | Loss: 0.00030402
Iteration 120/1000 | Loss: 0.00030402
Iteration 121/1000 | Loss: 0.00030402
Iteration 122/1000 | Loss: 0.00030402
Iteration 123/1000 | Loss: 0.00030402
Iteration 124/1000 | Loss: 0.00030402
Iteration 125/1000 | Loss: 0.00030402
Iteration 126/1000 | Loss: 0.00030401
Iteration 127/1000 | Loss: 0.00030400
Iteration 128/1000 | Loss: 0.00030400
Iteration 129/1000 | Loss: 0.00030399
Iteration 130/1000 | Loss: 0.00030399
Iteration 131/1000 | Loss: 0.00030398
Iteration 132/1000 | Loss: 0.00030398
Iteration 133/1000 | Loss: 0.00030398
Iteration 134/1000 | Loss: 0.00030398
Iteration 135/1000 | Loss: 0.00030398
Iteration 136/1000 | Loss: 0.00030398
Iteration 137/1000 | Loss: 0.00030398
Iteration 138/1000 | Loss: 0.00030397
Iteration 139/1000 | Loss: 0.00030397
Iteration 140/1000 | Loss: 0.00030397
Iteration 141/1000 | Loss: 0.00030397
Iteration 142/1000 | Loss: 0.00030397
Iteration 143/1000 | Loss: 0.00030396
Iteration 144/1000 | Loss: 0.00030396
Iteration 145/1000 | Loss: 0.00030396
Iteration 146/1000 | Loss: 0.00030396
Iteration 147/1000 | Loss: 0.00030395
Iteration 148/1000 | Loss: 0.00030395
Iteration 149/1000 | Loss: 0.00030395
Iteration 150/1000 | Loss: 0.00030395
Iteration 151/1000 | Loss: 0.00030395
Iteration 152/1000 | Loss: 0.00030395
Iteration 153/1000 | Loss: 0.00030395
Iteration 154/1000 | Loss: 0.00030395
Iteration 155/1000 | Loss: 0.00030395
Iteration 156/1000 | Loss: 0.00030395
Iteration 157/1000 | Loss: 0.00030395
Iteration 158/1000 | Loss: 0.00030395
Iteration 159/1000 | Loss: 0.00030395
Iteration 160/1000 | Loss: 0.00030395
Iteration 161/1000 | Loss: 0.00030395
Iteration 162/1000 | Loss: 0.00030395
Iteration 163/1000 | Loss: 0.00030395
Iteration 164/1000 | Loss: 0.00030395
Iteration 165/1000 | Loss: 0.00030394
Iteration 166/1000 | Loss: 0.00030394
Iteration 167/1000 | Loss: 0.00030394
Iteration 168/1000 | Loss: 0.00030394
Iteration 169/1000 | Loss: 0.00030394
Iteration 170/1000 | Loss: 0.00030394
Iteration 171/1000 | Loss: 0.00030394
Iteration 172/1000 | Loss: 0.00030394
Iteration 173/1000 | Loss: 0.00030394
Iteration 174/1000 | Loss: 0.00030394
Iteration 175/1000 | Loss: 0.00030394
Iteration 176/1000 | Loss: 0.00030394
Iteration 177/1000 | Loss: 0.00030394
Iteration 178/1000 | Loss: 0.00030394
Iteration 179/1000 | Loss: 0.00030394
Iteration 180/1000 | Loss: 0.00030394
Iteration 181/1000 | Loss: 0.00030394
Iteration 182/1000 | Loss: 0.00030393
Iteration 183/1000 | Loss: 0.00030393
Iteration 184/1000 | Loss: 0.00030393
Iteration 185/1000 | Loss: 0.00030393
Iteration 186/1000 | Loss: 0.00030393
Iteration 187/1000 | Loss: 0.00030393
Iteration 188/1000 | Loss: 0.00030393
Iteration 189/1000 | Loss: 0.00030393
Iteration 190/1000 | Loss: 0.00030393
Iteration 191/1000 | Loss: 0.00030393
Iteration 192/1000 | Loss: 0.00030393
Iteration 193/1000 | Loss: 0.00030393
Iteration 194/1000 | Loss: 0.00030393
Iteration 195/1000 | Loss: 0.00030393
Iteration 196/1000 | Loss: 0.00030393
Iteration 197/1000 | Loss: 0.00030393
Iteration 198/1000 | Loss: 0.00030393
Iteration 199/1000 | Loss: 0.00030393
Iteration 200/1000 | Loss: 0.00030393
Iteration 201/1000 | Loss: 0.00030392
Iteration 202/1000 | Loss: 0.00030392
Iteration 203/1000 | Loss: 0.00030392
Iteration 204/1000 | Loss: 0.00030392
Iteration 205/1000 | Loss: 0.00030392
Iteration 206/1000 | Loss: 0.00030392
Iteration 207/1000 | Loss: 0.00030392
Iteration 208/1000 | Loss: 0.00030392
Iteration 209/1000 | Loss: 0.00030391
Iteration 210/1000 | Loss: 0.00030391
Iteration 211/1000 | Loss: 0.00030391
Iteration 212/1000 | Loss: 0.00030391
Iteration 213/1000 | Loss: 0.00030391
Iteration 214/1000 | Loss: 0.00030391
Iteration 215/1000 | Loss: 0.00030391
Iteration 216/1000 | Loss: 0.00030391
Iteration 217/1000 | Loss: 0.00030391
Iteration 218/1000 | Loss: 0.00030391
Iteration 219/1000 | Loss: 0.00030391
Iteration 220/1000 | Loss: 0.00030391
Iteration 221/1000 | Loss: 0.00030391
Iteration 222/1000 | Loss: 0.00030391
Iteration 223/1000 | Loss: 0.00030391
Iteration 224/1000 | Loss: 0.00030390
Iteration 225/1000 | Loss: 0.00030390
Iteration 226/1000 | Loss: 0.00030390
Iteration 227/1000 | Loss: 0.00030390
Iteration 228/1000 | Loss: 0.00030390
Iteration 229/1000 | Loss: 0.00030390
Iteration 230/1000 | Loss: 0.00030390
Iteration 231/1000 | Loss: 0.00030390
Iteration 232/1000 | Loss: 0.00030390
Iteration 233/1000 | Loss: 0.00030390
Iteration 234/1000 | Loss: 0.00030390
Iteration 235/1000 | Loss: 0.00030390
Iteration 236/1000 | Loss: 0.00030390
Iteration 237/1000 | Loss: 0.00030390
Iteration 238/1000 | Loss: 0.00030390
Iteration 239/1000 | Loss: 0.00030390
Iteration 240/1000 | Loss: 0.00030390
Iteration 241/1000 | Loss: 0.00030390
Iteration 242/1000 | Loss: 0.00030390
Iteration 243/1000 | Loss: 0.00030390
Iteration 244/1000 | Loss: 0.00030389
Iteration 245/1000 | Loss: 0.00030389
Iteration 246/1000 | Loss: 0.00030389
Iteration 247/1000 | Loss: 0.00030389
Iteration 248/1000 | Loss: 0.00030389
Iteration 249/1000 | Loss: 0.00030389
Iteration 250/1000 | Loss: 0.00030389
Iteration 251/1000 | Loss: 0.00030389
Iteration 252/1000 | Loss: 0.00030389
Iteration 253/1000 | Loss: 0.00030389
Iteration 254/1000 | Loss: 0.00030389
Iteration 255/1000 | Loss: 0.00030389
Iteration 256/1000 | Loss: 0.00030388
Iteration 257/1000 | Loss: 0.00030388
Iteration 258/1000 | Loss: 0.00030388
Iteration 259/1000 | Loss: 0.00030388
Iteration 260/1000 | Loss: 0.00030388
Iteration 261/1000 | Loss: 0.00030388
Iteration 262/1000 | Loss: 0.00030388
Iteration 263/1000 | Loss: 0.00030387
Iteration 264/1000 | Loss: 0.00030387
Iteration 265/1000 | Loss: 0.00030387
Iteration 266/1000 | Loss: 0.00030387
Iteration 267/1000 | Loss: 0.00030387
Iteration 268/1000 | Loss: 0.00030387
Iteration 269/1000 | Loss: 0.00030387
Iteration 270/1000 | Loss: 0.00030387
Iteration 271/1000 | Loss: 0.00030387
Iteration 272/1000 | Loss: 0.00030387
Iteration 273/1000 | Loss: 0.00030386
Iteration 274/1000 | Loss: 0.00030386
Iteration 275/1000 | Loss: 0.00030386
Iteration 276/1000 | Loss: 0.00030386
Iteration 277/1000 | Loss: 0.00030386
Iteration 278/1000 | Loss: 0.00030386
Iteration 279/1000 | Loss: 0.00030386
Iteration 280/1000 | Loss: 0.00030386
Iteration 281/1000 | Loss: 0.00030386
Iteration 282/1000 | Loss: 0.00030386
Iteration 283/1000 | Loss: 0.00030386
Iteration 284/1000 | Loss: 0.00030386
Iteration 285/1000 | Loss: 0.00030386
Iteration 286/1000 | Loss: 0.00030386
Iteration 287/1000 | Loss: 0.00030386
Iteration 288/1000 | Loss: 0.00030386
Iteration 289/1000 | Loss: 0.00030386
Iteration 290/1000 | Loss: 0.00030385
Iteration 291/1000 | Loss: 0.00030385
Iteration 292/1000 | Loss: 0.00030385
Iteration 293/1000 | Loss: 0.00030385
Iteration 294/1000 | Loss: 0.00030385
Iteration 295/1000 | Loss: 0.00030385
Iteration 296/1000 | Loss: 0.00030385
Iteration 297/1000 | Loss: 0.00030385
Iteration 298/1000 | Loss: 0.00030385
Iteration 299/1000 | Loss: 0.00030385
Iteration 300/1000 | Loss: 0.00030385
Iteration 301/1000 | Loss: 0.00030385
Iteration 302/1000 | Loss: 0.00030385
Iteration 303/1000 | Loss: 0.00030385
Iteration 304/1000 | Loss: 0.00030385
Iteration 305/1000 | Loss: 0.00030385
Iteration 306/1000 | Loss: 0.00030385
Iteration 307/1000 | Loss: 0.00030385
Iteration 308/1000 | Loss: 0.00030385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 308. Stopping optimization.
Last 5 losses: [0.0003038517606910318, 0.0003038517606910318, 0.0003038517606910318, 0.0003038517606910318, 0.0003038517606910318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003038517606910318

Optimization complete. Final v2v error: 9.273908615112305 mm

Highest mean error: 13.351921081542969 mm for frame 150

Lowest mean error: 4.802761077880859 mm for frame 57

Saving results

Total time: 218.3746576309204
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073834
Iteration 2/25 | Loss: 0.01073834
Iteration 3/25 | Loss: 0.00211173
Iteration 4/25 | Loss: 0.00139971
Iteration 5/25 | Loss: 0.00117808
Iteration 6/25 | Loss: 0.00113854
Iteration 7/25 | Loss: 0.00105328
Iteration 8/25 | Loss: 0.00102838
Iteration 9/25 | Loss: 0.00100424
Iteration 10/25 | Loss: 0.00099881
Iteration 11/25 | Loss: 0.00099108
Iteration 12/25 | Loss: 0.00098895
Iteration 13/25 | Loss: 0.00099000
Iteration 14/25 | Loss: 0.00099101
Iteration 15/25 | Loss: 0.00098942
Iteration 16/25 | Loss: 0.00099127
Iteration 17/25 | Loss: 0.00098898
Iteration 18/25 | Loss: 0.00098553
Iteration 19/25 | Loss: 0.00098533
Iteration 20/25 | Loss: 0.00098643
Iteration 21/25 | Loss: 0.00098611
Iteration 22/25 | Loss: 0.00098617
Iteration 23/25 | Loss: 0.00098618
Iteration 24/25 | Loss: 0.00098701
Iteration 25/25 | Loss: 0.00098578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38950336
Iteration 2/25 | Loss: 0.00114982
Iteration 3/25 | Loss: 0.00104695
Iteration 4/25 | Loss: 0.00104695
Iteration 5/25 | Loss: 0.00104695
Iteration 6/25 | Loss: 0.00104695
Iteration 7/25 | Loss: 0.00104695
Iteration 8/25 | Loss: 0.00104695
Iteration 9/25 | Loss: 0.00104695
Iteration 10/25 | Loss: 0.00104695
Iteration 11/25 | Loss: 0.00104695
Iteration 12/25 | Loss: 0.00104695
Iteration 13/25 | Loss: 0.00104695
Iteration 14/25 | Loss: 0.00104695
Iteration 15/25 | Loss: 0.00104695
Iteration 16/25 | Loss: 0.00104695
Iteration 17/25 | Loss: 0.00104695
Iteration 18/25 | Loss: 0.00104695
Iteration 19/25 | Loss: 0.00104695
Iteration 20/25 | Loss: 0.00104695
Iteration 21/25 | Loss: 0.00104695
Iteration 22/25 | Loss: 0.00104695
Iteration 23/25 | Loss: 0.00104695
Iteration 24/25 | Loss: 0.00104695
Iteration 25/25 | Loss: 0.00104695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104695
Iteration 2/1000 | Loss: 0.00005660
Iteration 3/1000 | Loss: 0.00005433
Iteration 4/1000 | Loss: 0.00008309
Iteration 5/1000 | Loss: 0.00037845
Iteration 6/1000 | Loss: 0.00002695
Iteration 7/1000 | Loss: 0.00002565
Iteration 8/1000 | Loss: 0.00003261
Iteration 9/1000 | Loss: 0.00005253
Iteration 10/1000 | Loss: 0.00004086
Iteration 11/1000 | Loss: 0.00002430
Iteration 12/1000 | Loss: 0.00005470
Iteration 13/1000 | Loss: 0.00008453
Iteration 14/1000 | Loss: 0.00003647
Iteration 15/1000 | Loss: 0.00003578
Iteration 16/1000 | Loss: 0.00002432
Iteration 17/1000 | Loss: 0.00002315
Iteration 18/1000 | Loss: 0.00002408
Iteration 19/1000 | Loss: 0.00002315
Iteration 20/1000 | Loss: 0.00002796
Iteration 21/1000 | Loss: 0.00059571
Iteration 22/1000 | Loss: 0.00025402
Iteration 23/1000 | Loss: 0.00002896
Iteration 24/1000 | Loss: 0.00003463
Iteration 25/1000 | Loss: 0.00002268
Iteration 26/1000 | Loss: 0.00002833
Iteration 27/1000 | Loss: 0.00002497
Iteration 28/1000 | Loss: 0.00002116
Iteration 29/1000 | Loss: 0.00002330
Iteration 30/1000 | Loss: 0.00003606
Iteration 31/1000 | Loss: 0.00003218
Iteration 32/1000 | Loss: 0.00002155
Iteration 33/1000 | Loss: 0.00002028
Iteration 34/1000 | Loss: 0.00002071
Iteration 35/1000 | Loss: 0.00002150
Iteration 36/1000 | Loss: 0.00002057
Iteration 37/1000 | Loss: 0.00002215
Iteration 38/1000 | Loss: 0.00002042
Iteration 39/1000 | Loss: 0.00002056
Iteration 40/1000 | Loss: 0.00002056
Iteration 41/1000 | Loss: 0.00002158
Iteration 42/1000 | Loss: 0.00002117
Iteration 43/1000 | Loss: 0.00003293
Iteration 44/1000 | Loss: 0.00002195
Iteration 45/1000 | Loss: 0.00005318
Iteration 46/1000 | Loss: 0.00002458
Iteration 47/1000 | Loss: 0.00002459
Iteration 48/1000 | Loss: 0.00003286
Iteration 49/1000 | Loss: 0.00008313
Iteration 50/1000 | Loss: 0.00002039
Iteration 51/1000 | Loss: 0.00002099
Iteration 52/1000 | Loss: 0.00002118
Iteration 53/1000 | Loss: 0.00003937
Iteration 54/1000 | Loss: 0.00002352
Iteration 55/1000 | Loss: 0.00002797
Iteration 56/1000 | Loss: 0.00002565
Iteration 57/1000 | Loss: 0.00002432
Iteration 58/1000 | Loss: 0.00003889
Iteration 59/1000 | Loss: 0.00002382
Iteration 60/1000 | Loss: 0.00004594
Iteration 61/1000 | Loss: 0.00002184
Iteration 62/1000 | Loss: 0.00002165
Iteration 63/1000 | Loss: 0.00002078
Iteration 64/1000 | Loss: 0.00001988
Iteration 65/1000 | Loss: 0.00001988
Iteration 66/1000 | Loss: 0.00001987
Iteration 67/1000 | Loss: 0.00001987
Iteration 68/1000 | Loss: 0.00001987
Iteration 69/1000 | Loss: 0.00002185
Iteration 70/1000 | Loss: 0.00002050
Iteration 71/1000 | Loss: 0.00002213
Iteration 72/1000 | Loss: 0.00002168
Iteration 73/1000 | Loss: 0.00002247
Iteration 74/1000 | Loss: 0.00002149
Iteration 75/1000 | Loss: 0.00002093
Iteration 76/1000 | Loss: 0.00002204
Iteration 77/1000 | Loss: 0.00002245
Iteration 78/1000 | Loss: 0.00002621
Iteration 79/1000 | Loss: 0.00002372
Iteration 80/1000 | Loss: 0.00002316
Iteration 81/1000 | Loss: 0.00005893
Iteration 82/1000 | Loss: 0.00002073
Iteration 83/1000 | Loss: 0.00002105
Iteration 84/1000 | Loss: 0.00002063
Iteration 85/1000 | Loss: 0.00002060
Iteration 86/1000 | Loss: 0.00002142
Iteration 87/1000 | Loss: 0.00002267
Iteration 88/1000 | Loss: 0.00002050
Iteration 89/1000 | Loss: 0.00002663
Iteration 90/1000 | Loss: 0.00002328
Iteration 91/1000 | Loss: 0.00002344
Iteration 92/1000 | Loss: 0.00002088
Iteration 93/1000 | Loss: 0.00002342
Iteration 94/1000 | Loss: 0.00002217
Iteration 95/1000 | Loss: 0.00002262
Iteration 96/1000 | Loss: 0.00002711
Iteration 97/1000 | Loss: 0.00002413
Iteration 98/1000 | Loss: 0.00002063
Iteration 99/1000 | Loss: 0.00002287
Iteration 100/1000 | Loss: 0.00002284
Iteration 101/1000 | Loss: 0.00003725
Iteration 102/1000 | Loss: 0.00002184
Iteration 103/1000 | Loss: 0.00002243
Iteration 104/1000 | Loss: 0.00002460
Iteration 105/1000 | Loss: 0.00002067
Iteration 106/1000 | Loss: 0.00001991
Iteration 107/1000 | Loss: 0.00001972
Iteration 108/1000 | Loss: 0.00001972
Iteration 109/1000 | Loss: 0.00002019
Iteration 110/1000 | Loss: 0.00002040
Iteration 111/1000 | Loss: 0.00001970
Iteration 112/1000 | Loss: 0.00001970
Iteration 113/1000 | Loss: 0.00001970
Iteration 114/1000 | Loss: 0.00001970
Iteration 115/1000 | Loss: 0.00001969
Iteration 116/1000 | Loss: 0.00001969
Iteration 117/1000 | Loss: 0.00001969
Iteration 118/1000 | Loss: 0.00001968
Iteration 119/1000 | Loss: 0.00001982
Iteration 120/1000 | Loss: 0.00001981
Iteration 121/1000 | Loss: 0.00001973
Iteration 122/1000 | Loss: 0.00001970
Iteration 123/1000 | Loss: 0.00001968
Iteration 124/1000 | Loss: 0.00001983
Iteration 125/1000 | Loss: 0.00001970
Iteration 126/1000 | Loss: 0.00002040
Iteration 127/1000 | Loss: 0.00002053
Iteration 128/1000 | Loss: 0.00002408
Iteration 129/1000 | Loss: 0.00003252
Iteration 130/1000 | Loss: 0.00003041
Iteration 131/1000 | Loss: 0.00002536
Iteration 132/1000 | Loss: 0.00002071
Iteration 133/1000 | Loss: 0.00002427
Iteration 134/1000 | Loss: 0.00002263
Iteration 135/1000 | Loss: 0.00002338
Iteration 136/1000 | Loss: 0.00002338
Iteration 137/1000 | Loss: 0.00002372
Iteration 138/1000 | Loss: 0.00007492
Iteration 139/1000 | Loss: 0.00002110
Iteration 140/1000 | Loss: 0.00002552
Iteration 141/1000 | Loss: 0.00001964
Iteration 142/1000 | Loss: 0.00001964
Iteration 143/1000 | Loss: 0.00001964
Iteration 144/1000 | Loss: 0.00001964
Iteration 145/1000 | Loss: 0.00001964
Iteration 146/1000 | Loss: 0.00001964
Iteration 147/1000 | Loss: 0.00001964
Iteration 148/1000 | Loss: 0.00001964
Iteration 149/1000 | Loss: 0.00001964
Iteration 150/1000 | Loss: 0.00001964
Iteration 151/1000 | Loss: 0.00001964
Iteration 152/1000 | Loss: 0.00001964
Iteration 153/1000 | Loss: 0.00001964
Iteration 154/1000 | Loss: 0.00001964
Iteration 155/1000 | Loss: 0.00001964
Iteration 156/1000 | Loss: 0.00001964
Iteration 157/1000 | Loss: 0.00001964
Iteration 158/1000 | Loss: 0.00001964
Iteration 159/1000 | Loss: 0.00001964
Iteration 160/1000 | Loss: 0.00001964
Iteration 161/1000 | Loss: 0.00001964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.9639364836621098e-05, 1.9639364836621098e-05, 1.9639364836621098e-05, 1.9639364836621098e-05, 1.9639364836621098e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9639364836621098e-05

Optimization complete. Final v2v error: 3.2298412322998047 mm

Highest mean error: 10.28812026977539 mm for frame 108

Lowest mean error: 2.5075924396514893 mm for frame 127

Saving results

Total time: 209.82745337486267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01149741
Iteration 2/25 | Loss: 0.01149741
Iteration 3/25 | Loss: 0.01149741
Iteration 4/25 | Loss: 0.00224505
Iteration 5/25 | Loss: 0.00152026
Iteration 6/25 | Loss: 0.00142383
Iteration 7/25 | Loss: 0.00134214
Iteration 8/25 | Loss: 0.00132244
Iteration 9/25 | Loss: 0.00132081
Iteration 10/25 | Loss: 0.00132567
Iteration 11/25 | Loss: 0.00130923
Iteration 12/25 | Loss: 0.00130496
Iteration 13/25 | Loss: 0.00130750
Iteration 14/25 | Loss: 0.00130312
Iteration 15/25 | Loss: 0.00130054
Iteration 16/25 | Loss: 0.00130076
Iteration 17/25 | Loss: 0.00129992
Iteration 18/25 | Loss: 0.00129142
Iteration 19/25 | Loss: 0.00128501
Iteration 20/25 | Loss: 0.00128467
Iteration 21/25 | Loss: 0.00128233
Iteration 22/25 | Loss: 0.00128403
Iteration 23/25 | Loss: 0.00128265
Iteration 24/25 | Loss: 0.00128725
Iteration 25/25 | Loss: 0.00128348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48111963
Iteration 2/25 | Loss: 0.00096030
Iteration 3/25 | Loss: 0.00093945
Iteration 4/25 | Loss: 0.00093945
Iteration 5/25 | Loss: 0.00093945
Iteration 6/25 | Loss: 0.00093945
Iteration 7/25 | Loss: 0.00093945
Iteration 8/25 | Loss: 0.00093945
Iteration 9/25 | Loss: 0.00093945
Iteration 10/25 | Loss: 0.00093945
Iteration 11/25 | Loss: 0.00093945
Iteration 12/25 | Loss: 0.00093945
Iteration 13/25 | Loss: 0.00093945
Iteration 14/25 | Loss: 0.00093945
Iteration 15/25 | Loss: 0.00093945
Iteration 16/25 | Loss: 0.00093945
Iteration 17/25 | Loss: 0.00093945
Iteration 18/25 | Loss: 0.00093945
Iteration 19/25 | Loss: 0.00093945
Iteration 20/25 | Loss: 0.00093945
Iteration 21/25 | Loss: 0.00093945
Iteration 22/25 | Loss: 0.00093945
Iteration 23/25 | Loss: 0.00093945
Iteration 24/25 | Loss: 0.00093945
Iteration 25/25 | Loss: 0.00093945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093945
Iteration 2/1000 | Loss: 0.00012636
Iteration 3/1000 | Loss: 0.00009923
Iteration 4/1000 | Loss: 0.00007256
Iteration 5/1000 | Loss: 0.00010110
Iteration 6/1000 | Loss: 0.00009697
Iteration 7/1000 | Loss: 0.00007103
Iteration 8/1000 | Loss: 0.00006578
Iteration 9/1000 | Loss: 0.00021713
Iteration 10/1000 | Loss: 0.00035086
Iteration 11/1000 | Loss: 0.00009400
Iteration 12/1000 | Loss: 0.00006771
Iteration 13/1000 | Loss: 0.00007367
Iteration 14/1000 | Loss: 0.00006578
Iteration 15/1000 | Loss: 0.00007645
Iteration 16/1000 | Loss: 0.00006822
Iteration 17/1000 | Loss: 0.00006722
Iteration 18/1000 | Loss: 0.00007936
Iteration 19/1000 | Loss: 0.00006085
Iteration 20/1000 | Loss: 0.00006317
Iteration 21/1000 | Loss: 0.00008801
Iteration 22/1000 | Loss: 0.00007156
Iteration 23/1000 | Loss: 0.00007057
Iteration 24/1000 | Loss: 0.00006236
Iteration 25/1000 | Loss: 0.00008424
Iteration 26/1000 | Loss: 0.00007746
Iteration 27/1000 | Loss: 0.00008389
Iteration 28/1000 | Loss: 0.00007856
Iteration 29/1000 | Loss: 0.00009111
Iteration 30/1000 | Loss: 0.00007342
Iteration 31/1000 | Loss: 0.00008033
Iteration 32/1000 | Loss: 0.00003801
Iteration 33/1000 | Loss: 0.00005562
Iteration 34/1000 | Loss: 0.00006041
Iteration 35/1000 | Loss: 0.00006972
Iteration 36/1000 | Loss: 0.00006566
Iteration 37/1000 | Loss: 0.00006380
Iteration 38/1000 | Loss: 0.00006614
Iteration 39/1000 | Loss: 0.00006416
Iteration 40/1000 | Loss: 0.00005853
Iteration 41/1000 | Loss: 0.00022830
Iteration 42/1000 | Loss: 0.00005809
Iteration 43/1000 | Loss: 0.00014055
Iteration 44/1000 | Loss: 0.00004461
Iteration 45/1000 | Loss: 0.00006928
Iteration 46/1000 | Loss: 0.00006444
Iteration 47/1000 | Loss: 0.00006193
Iteration 48/1000 | Loss: 0.00007004
Iteration 49/1000 | Loss: 0.00005788
Iteration 50/1000 | Loss: 0.00003424
Iteration 51/1000 | Loss: 0.00004192
Iteration 52/1000 | Loss: 0.00005925
Iteration 53/1000 | Loss: 0.00004186
Iteration 54/1000 | Loss: 0.00007539
Iteration 55/1000 | Loss: 0.00006389
Iteration 56/1000 | Loss: 0.00016860
Iteration 57/1000 | Loss: 0.00005867
Iteration 58/1000 | Loss: 0.00008357
Iteration 59/1000 | Loss: 0.00006930
Iteration 60/1000 | Loss: 0.00003789
Iteration 61/1000 | Loss: 0.00005536
Iteration 62/1000 | Loss: 0.00006480
Iteration 63/1000 | Loss: 0.00006723
Iteration 64/1000 | Loss: 0.00004324
Iteration 65/1000 | Loss: 0.00004639
Iteration 66/1000 | Loss: 0.00008793
Iteration 67/1000 | Loss: 0.00003975
Iteration 68/1000 | Loss: 0.00004169
Iteration 69/1000 | Loss: 0.00004316
Iteration 70/1000 | Loss: 0.00007009
Iteration 71/1000 | Loss: 0.00004666
Iteration 72/1000 | Loss: 0.00004286
Iteration 73/1000 | Loss: 0.00004318
Iteration 74/1000 | Loss: 0.00005463
Iteration 75/1000 | Loss: 0.00026685
Iteration 76/1000 | Loss: 0.00015396
Iteration 77/1000 | Loss: 0.00004275
Iteration 78/1000 | Loss: 0.00004289
Iteration 79/1000 | Loss: 0.00003970
Iteration 80/1000 | Loss: 0.00004283
Iteration 81/1000 | Loss: 0.00006387
Iteration 82/1000 | Loss: 0.00071672
Iteration 83/1000 | Loss: 0.00007608
Iteration 84/1000 | Loss: 0.00006893
Iteration 85/1000 | Loss: 0.00037881
Iteration 86/1000 | Loss: 0.00014428
Iteration 87/1000 | Loss: 0.00006145
Iteration 88/1000 | Loss: 0.00006854
Iteration 89/1000 | Loss: 0.00023417
Iteration 90/1000 | Loss: 0.00018811
Iteration 91/1000 | Loss: 0.00006206
Iteration 92/1000 | Loss: 0.00006880
Iteration 93/1000 | Loss: 0.00006179
Iteration 94/1000 | Loss: 0.00006882
Iteration 95/1000 | Loss: 0.00005380
Iteration 96/1000 | Loss: 0.00006481
Iteration 97/1000 | Loss: 0.00006058
Iteration 98/1000 | Loss: 0.00006816
Iteration 99/1000 | Loss: 0.00006458
Iteration 100/1000 | Loss: 0.00005057
Iteration 101/1000 | Loss: 0.00007036
Iteration 102/1000 | Loss: 0.00004875
Iteration 103/1000 | Loss: 0.00006267
Iteration 104/1000 | Loss: 0.00005746
Iteration 105/1000 | Loss: 0.00006240
Iteration 106/1000 | Loss: 0.00005676
Iteration 107/1000 | Loss: 0.00006401
Iteration 108/1000 | Loss: 0.00006797
Iteration 109/1000 | Loss: 0.00017926
Iteration 110/1000 | Loss: 0.00007259
Iteration 111/1000 | Loss: 0.00011733
Iteration 112/1000 | Loss: 0.00006510
Iteration 113/1000 | Loss: 0.00011006
Iteration 114/1000 | Loss: 0.00006944
Iteration 115/1000 | Loss: 0.00006315
Iteration 116/1000 | Loss: 0.00008667
Iteration 117/1000 | Loss: 0.00004125
Iteration 118/1000 | Loss: 0.00008558
Iteration 119/1000 | Loss: 0.00004085
Iteration 120/1000 | Loss: 0.00008672
Iteration 121/1000 | Loss: 0.00004075
Iteration 122/1000 | Loss: 0.00008346
Iteration 123/1000 | Loss: 0.00005181
Iteration 124/1000 | Loss: 0.00007983
Iteration 125/1000 | Loss: 0.00004674
Iteration 126/1000 | Loss: 0.00007329
Iteration 127/1000 | Loss: 0.00006124
Iteration 128/1000 | Loss: 0.00006666
Iteration 129/1000 | Loss: 0.00004628
Iteration 130/1000 | Loss: 0.00006020
Iteration 131/1000 | Loss: 0.00005659
Iteration 132/1000 | Loss: 0.00004587
Iteration 133/1000 | Loss: 0.00005967
Iteration 134/1000 | Loss: 0.00004151
Iteration 135/1000 | Loss: 0.00006970
Iteration 136/1000 | Loss: 0.00004055
Iteration 137/1000 | Loss: 0.00005504
Iteration 138/1000 | Loss: 0.00004093
Iteration 139/1000 | Loss: 0.00003656
Iteration 140/1000 | Loss: 0.00005698
Iteration 141/1000 | Loss: 0.00006465
Iteration 142/1000 | Loss: 0.00027576
Iteration 143/1000 | Loss: 0.00027093
Iteration 144/1000 | Loss: 0.00031833
Iteration 145/1000 | Loss: 0.00020005
Iteration 146/1000 | Loss: 0.00002854
Iteration 147/1000 | Loss: 0.00002525
Iteration 148/1000 | Loss: 0.00002350
Iteration 149/1000 | Loss: 0.00002242
Iteration 150/1000 | Loss: 0.00002180
Iteration 151/1000 | Loss: 0.00025150
Iteration 152/1000 | Loss: 0.00008786
Iteration 153/1000 | Loss: 0.00002185
Iteration 154/1000 | Loss: 0.00025587
Iteration 155/1000 | Loss: 0.00056249
Iteration 156/1000 | Loss: 0.00004621
Iteration 157/1000 | Loss: 0.00004233
Iteration 158/1000 | Loss: 0.00004492
Iteration 159/1000 | Loss: 0.00002285
Iteration 160/1000 | Loss: 0.00002587
Iteration 161/1000 | Loss: 0.00002394
Iteration 162/1000 | Loss: 0.00002463
Iteration 163/1000 | Loss: 0.00001991
Iteration 164/1000 | Loss: 0.00001937
Iteration 165/1000 | Loss: 0.00001936
Iteration 166/1000 | Loss: 0.00001976
Iteration 167/1000 | Loss: 0.00001920
Iteration 168/1000 | Loss: 0.00001919
Iteration 169/1000 | Loss: 0.00001997
Iteration 170/1000 | Loss: 0.00001908
Iteration 171/1000 | Loss: 0.00001925
Iteration 172/1000 | Loss: 0.00001902
Iteration 173/1000 | Loss: 0.00001902
Iteration 174/1000 | Loss: 0.00001899
Iteration 175/1000 | Loss: 0.00001897
Iteration 176/1000 | Loss: 0.00001897
Iteration 177/1000 | Loss: 0.00001896
Iteration 178/1000 | Loss: 0.00001896
Iteration 179/1000 | Loss: 0.00001895
Iteration 180/1000 | Loss: 0.00001925
Iteration 181/1000 | Loss: 0.00001984
Iteration 182/1000 | Loss: 0.00001892
Iteration 183/1000 | Loss: 0.00001886
Iteration 184/1000 | Loss: 0.00001885
Iteration 185/1000 | Loss: 0.00001882
Iteration 186/1000 | Loss: 0.00001882
Iteration 187/1000 | Loss: 0.00001881
Iteration 188/1000 | Loss: 0.00001881
Iteration 189/1000 | Loss: 0.00001881
Iteration 190/1000 | Loss: 0.00001881
Iteration 191/1000 | Loss: 0.00001881
Iteration 192/1000 | Loss: 0.00001881
Iteration 193/1000 | Loss: 0.00001881
Iteration 194/1000 | Loss: 0.00001881
Iteration 195/1000 | Loss: 0.00001881
Iteration 196/1000 | Loss: 0.00001881
Iteration 197/1000 | Loss: 0.00001880
Iteration 198/1000 | Loss: 0.00001880
Iteration 199/1000 | Loss: 0.00001880
Iteration 200/1000 | Loss: 0.00001880
Iteration 201/1000 | Loss: 0.00001880
Iteration 202/1000 | Loss: 0.00001880
Iteration 203/1000 | Loss: 0.00001880
Iteration 204/1000 | Loss: 0.00001880
Iteration 205/1000 | Loss: 0.00001880
Iteration 206/1000 | Loss: 0.00001880
Iteration 207/1000 | Loss: 0.00001880
Iteration 208/1000 | Loss: 0.00001879
Iteration 209/1000 | Loss: 0.00001879
Iteration 210/1000 | Loss: 0.00001887
Iteration 211/1000 | Loss: 0.00001881
Iteration 212/1000 | Loss: 0.00001880
Iteration 213/1000 | Loss: 0.00001879
Iteration 214/1000 | Loss: 0.00001879
Iteration 215/1000 | Loss: 0.00001878
Iteration 216/1000 | Loss: 0.00001878
Iteration 217/1000 | Loss: 0.00001878
Iteration 218/1000 | Loss: 0.00001877
Iteration 219/1000 | Loss: 0.00001877
Iteration 220/1000 | Loss: 0.00001877
Iteration 221/1000 | Loss: 0.00001876
Iteration 222/1000 | Loss: 0.00001876
Iteration 223/1000 | Loss: 0.00001876
Iteration 224/1000 | Loss: 0.00001876
Iteration 225/1000 | Loss: 0.00001876
Iteration 226/1000 | Loss: 0.00001876
Iteration 227/1000 | Loss: 0.00001876
Iteration 228/1000 | Loss: 0.00001876
Iteration 229/1000 | Loss: 0.00001875
Iteration 230/1000 | Loss: 0.00001875
Iteration 231/1000 | Loss: 0.00001875
Iteration 232/1000 | Loss: 0.00001875
Iteration 233/1000 | Loss: 0.00001875
Iteration 234/1000 | Loss: 0.00001875
Iteration 235/1000 | Loss: 0.00001875
Iteration 236/1000 | Loss: 0.00001875
Iteration 237/1000 | Loss: 0.00001875
Iteration 238/1000 | Loss: 0.00001875
Iteration 239/1000 | Loss: 0.00001874
Iteration 240/1000 | Loss: 0.00001874
Iteration 241/1000 | Loss: 0.00001874
Iteration 242/1000 | Loss: 0.00001874
Iteration 243/1000 | Loss: 0.00001899
Iteration 244/1000 | Loss: 0.00001880
Iteration 245/1000 | Loss: 0.00001892
Iteration 246/1000 | Loss: 0.00001904
Iteration 247/1000 | Loss: 0.00001889
Iteration 248/1000 | Loss: 0.00001903
Iteration 249/1000 | Loss: 0.00001894
Iteration 250/1000 | Loss: 0.00001898
Iteration 251/1000 | Loss: 0.00001900
Iteration 252/1000 | Loss: 0.00001953
Iteration 253/1000 | Loss: 0.00001884
Iteration 254/1000 | Loss: 0.00001884
Iteration 255/1000 | Loss: 0.00001882
Iteration 256/1000 | Loss: 0.00001881
Iteration 257/1000 | Loss: 0.00001881
Iteration 258/1000 | Loss: 0.00001881
Iteration 259/1000 | Loss: 0.00001880
Iteration 260/1000 | Loss: 0.00001880
Iteration 261/1000 | Loss: 0.00001886
Iteration 262/1000 | Loss: 0.00001878
Iteration 263/1000 | Loss: 0.00001895
Iteration 264/1000 | Loss: 0.00001894
Iteration 265/1000 | Loss: 0.00001912
Iteration 266/1000 | Loss: 0.00001931
Iteration 267/1000 | Loss: 0.00001903
Iteration 268/1000 | Loss: 0.00001906
Iteration 269/1000 | Loss: 0.00001905
Iteration 270/1000 | Loss: 0.00001910
Iteration 271/1000 | Loss: 0.00001895
Iteration 272/1000 | Loss: 0.00001950
Iteration 273/1000 | Loss: 0.00001905
Iteration 274/1000 | Loss: 0.00001875
Iteration 275/1000 | Loss: 0.00001908
Iteration 276/1000 | Loss: 0.00001914
Iteration 277/1000 | Loss: 0.00001909
Iteration 278/1000 | Loss: 0.00001904
Iteration 279/1000 | Loss: 0.00001908
Iteration 280/1000 | Loss: 0.00001908
Iteration 281/1000 | Loss: 0.00001907
Iteration 282/1000 | Loss: 0.00001967
Iteration 283/1000 | Loss: 0.00001911
Iteration 284/1000 | Loss: 0.00001894
Iteration 285/1000 | Loss: 0.00001868
Iteration 286/1000 | Loss: 0.00001868
Iteration 287/1000 | Loss: 0.00001867
Iteration 288/1000 | Loss: 0.00001876
Iteration 289/1000 | Loss: 0.00001870
Iteration 290/1000 | Loss: 0.00001870
Iteration 291/1000 | Loss: 0.00001869
Iteration 292/1000 | Loss: 0.00001869
Iteration 293/1000 | Loss: 0.00001869
Iteration 294/1000 | Loss: 0.00001869
Iteration 295/1000 | Loss: 0.00001868
Iteration 296/1000 | Loss: 0.00001868
Iteration 297/1000 | Loss: 0.00001868
Iteration 298/1000 | Loss: 0.00001868
Iteration 299/1000 | Loss: 0.00001868
Iteration 300/1000 | Loss: 0.00001868
Iteration 301/1000 | Loss: 0.00001868
Iteration 302/1000 | Loss: 0.00001868
Iteration 303/1000 | Loss: 0.00001873
Iteration 304/1000 | Loss: 0.00001872
Iteration 305/1000 | Loss: 0.00001872
Iteration 306/1000 | Loss: 0.00001871
Iteration 307/1000 | Loss: 0.00001870
Iteration 308/1000 | Loss: 0.00001869
Iteration 309/1000 | Loss: 0.00001869
Iteration 310/1000 | Loss: 0.00001868
Iteration 311/1000 | Loss: 0.00001867
Iteration 312/1000 | Loss: 0.00001872
Iteration 313/1000 | Loss: 0.00001868
Iteration 314/1000 | Loss: 0.00001868
Iteration 315/1000 | Loss: 0.00001867
Iteration 316/1000 | Loss: 0.00001867
Iteration 317/1000 | Loss: 0.00001867
Iteration 318/1000 | Loss: 0.00001867
Iteration 319/1000 | Loss: 0.00001867
Iteration 320/1000 | Loss: 0.00001867
Iteration 321/1000 | Loss: 0.00001867
Iteration 322/1000 | Loss: 0.00001867
Iteration 323/1000 | Loss: 0.00001867
Iteration 324/1000 | Loss: 0.00001867
Iteration 325/1000 | Loss: 0.00001867
Iteration 326/1000 | Loss: 0.00001867
Iteration 327/1000 | Loss: 0.00001867
Iteration 328/1000 | Loss: 0.00001867
Iteration 329/1000 | Loss: 0.00001867
Iteration 330/1000 | Loss: 0.00001867
Iteration 331/1000 | Loss: 0.00001867
Iteration 332/1000 | Loss: 0.00001866
Iteration 333/1000 | Loss: 0.00001866
Iteration 334/1000 | Loss: 0.00001866
Iteration 335/1000 | Loss: 0.00001876
Iteration 336/1000 | Loss: 0.00001876
Iteration 337/1000 | Loss: 0.00001875
Iteration 338/1000 | Loss: 0.00001875
Iteration 339/1000 | Loss: 0.00001895
Iteration 340/1000 | Loss: 0.00001914
Iteration 341/1000 | Loss: 0.00001919
Iteration 342/1000 | Loss: 0.00001921
Iteration 343/1000 | Loss: 0.00001909
Iteration 344/1000 | Loss: 0.00001917
Iteration 345/1000 | Loss: 0.00001921
Iteration 346/1000 | Loss: 0.00001921
Iteration 347/1000 | Loss: 0.00002081
Iteration 348/1000 | Loss: 0.00001921
Iteration 349/1000 | Loss: 0.00001923
Iteration 350/1000 | Loss: 0.00001923
Iteration 351/1000 | Loss: 0.00001912
Iteration 352/1000 | Loss: 0.00001918
Iteration 353/1000 | Loss: 0.00001922
Iteration 354/1000 | Loss: 0.00001895
Iteration 355/1000 | Loss: 0.00001894
Iteration 356/1000 | Loss: 0.00001958
Iteration 357/1000 | Loss: 0.00001954
Iteration 358/1000 | Loss: 0.00002251
Iteration 359/1000 | Loss: 0.00003539
Iteration 360/1000 | Loss: 0.00001958
Iteration 361/1000 | Loss: 0.00001966
Iteration 362/1000 | Loss: 0.00001949
Iteration 363/1000 | Loss: 0.00001949
Iteration 364/1000 | Loss: 0.00001942
Iteration 365/1000 | Loss: 0.00001876
Iteration 366/1000 | Loss: 0.00001930
Iteration 367/1000 | Loss: 0.00001924
Iteration 368/1000 | Loss: 0.00002056
Iteration 369/1000 | Loss: 0.00001948
Iteration 370/1000 | Loss: 0.00001866
Iteration 371/1000 | Loss: 0.00001866
Iteration 372/1000 | Loss: 0.00001866
Iteration 373/1000 | Loss: 0.00001866
Iteration 374/1000 | Loss: 0.00001866
Iteration 375/1000 | Loss: 0.00001866
Iteration 376/1000 | Loss: 0.00001865
Iteration 377/1000 | Loss: 0.00001865
Iteration 378/1000 | Loss: 0.00001865
Iteration 379/1000 | Loss: 0.00001865
Iteration 380/1000 | Loss: 0.00001865
Iteration 381/1000 | Loss: 0.00001864
Iteration 382/1000 | Loss: 0.00001864
Iteration 383/1000 | Loss: 0.00001864
Iteration 384/1000 | Loss: 0.00001864
Iteration 385/1000 | Loss: 0.00001864
Iteration 386/1000 | Loss: 0.00001864
Iteration 387/1000 | Loss: 0.00001864
Iteration 388/1000 | Loss: 0.00001864
Iteration 389/1000 | Loss: 0.00001864
Iteration 390/1000 | Loss: 0.00001864
Iteration 391/1000 | Loss: 0.00001864
Iteration 392/1000 | Loss: 0.00001864
Iteration 393/1000 | Loss: 0.00001864
Iteration 394/1000 | Loss: 0.00001864
Iteration 395/1000 | Loss: 0.00001864
Iteration 396/1000 | Loss: 0.00001864
Iteration 397/1000 | Loss: 0.00001864
Iteration 398/1000 | Loss: 0.00001864
Iteration 399/1000 | Loss: 0.00001864
Iteration 400/1000 | Loss: 0.00001864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 400. Stopping optimization.
Last 5 losses: [1.8637765606399626e-05, 1.8637765606399626e-05, 1.8637765606399626e-05, 1.8637765606399626e-05, 1.8637765606399626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8637765606399626e-05

Optimization complete. Final v2v error: 3.5224459171295166 mm

Highest mean error: 10.774598121643066 mm for frame 213

Lowest mean error: 3.0637733936309814 mm for frame 161

Saving results

Total time: 409.87669825553894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841158
Iteration 2/25 | Loss: 0.00189932
Iteration 3/25 | Loss: 0.00155835
Iteration 4/25 | Loss: 0.00165177
Iteration 5/25 | Loss: 0.00152951
Iteration 6/25 | Loss: 0.00147883
Iteration 7/25 | Loss: 0.00146883
Iteration 8/25 | Loss: 0.00146392
Iteration 9/25 | Loss: 0.00146073
Iteration 10/25 | Loss: 0.00145817
Iteration 11/25 | Loss: 0.00145765
Iteration 12/25 | Loss: 0.00145734
Iteration 13/25 | Loss: 0.00145714
Iteration 14/25 | Loss: 0.00146115
Iteration 15/25 | Loss: 0.00145913
Iteration 16/25 | Loss: 0.00145615
Iteration 17/25 | Loss: 0.00145507
Iteration 18/25 | Loss: 0.00145475
Iteration 19/25 | Loss: 0.00145469
Iteration 20/25 | Loss: 0.00145468
Iteration 21/25 | Loss: 0.00145468
Iteration 22/25 | Loss: 0.00145468
Iteration 23/25 | Loss: 0.00145467
Iteration 24/25 | Loss: 0.00145467
Iteration 25/25 | Loss: 0.00145467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.47468424
Iteration 2/25 | Loss: 0.00073064
Iteration 3/25 | Loss: 0.00073062
Iteration 4/25 | Loss: 0.00073062
Iteration 5/25 | Loss: 0.00073062
Iteration 6/25 | Loss: 0.00073062
Iteration 7/25 | Loss: 0.00073062
Iteration 8/25 | Loss: 0.00073061
Iteration 9/25 | Loss: 0.00073061
Iteration 10/25 | Loss: 0.00073061
Iteration 11/25 | Loss: 0.00073061
Iteration 12/25 | Loss: 0.00073061
Iteration 13/25 | Loss: 0.00073061
Iteration 14/25 | Loss: 0.00073061
Iteration 15/25 | Loss: 0.00073061
Iteration 16/25 | Loss: 0.00073061
Iteration 17/25 | Loss: 0.00073061
Iteration 18/25 | Loss: 0.00073061
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007306142942979932, 0.0007306142942979932, 0.0007306142942979932, 0.0007306142942979932, 0.0007306142942979932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007306142942979932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073061
Iteration 2/1000 | Loss: 0.00005106
Iteration 3/1000 | Loss: 0.00003762
Iteration 4/1000 | Loss: 0.00003420
Iteration 5/1000 | Loss: 0.00003200
Iteration 6/1000 | Loss: 0.00003098
Iteration 7/1000 | Loss: 0.00003038
Iteration 8/1000 | Loss: 0.00010218
Iteration 9/1000 | Loss: 0.00022456
Iteration 10/1000 | Loss: 0.00024278
Iteration 11/1000 | Loss: 0.00013215
Iteration 12/1000 | Loss: 0.00003079
Iteration 13/1000 | Loss: 0.00002895
Iteration 14/1000 | Loss: 0.00002755
Iteration 15/1000 | Loss: 0.00002642
Iteration 16/1000 | Loss: 0.00002596
Iteration 17/1000 | Loss: 0.00002571
Iteration 18/1000 | Loss: 0.00002563
Iteration 19/1000 | Loss: 0.00002552
Iteration 20/1000 | Loss: 0.00002548
Iteration 21/1000 | Loss: 0.00002548
Iteration 22/1000 | Loss: 0.00002547
Iteration 23/1000 | Loss: 0.00002547
Iteration 24/1000 | Loss: 0.00002547
Iteration 25/1000 | Loss: 0.00002547
Iteration 26/1000 | Loss: 0.00002546
Iteration 27/1000 | Loss: 0.00002546
Iteration 28/1000 | Loss: 0.00002545
Iteration 29/1000 | Loss: 0.00002545
Iteration 30/1000 | Loss: 0.00002545
Iteration 31/1000 | Loss: 0.00002545
Iteration 32/1000 | Loss: 0.00002545
Iteration 33/1000 | Loss: 0.00002545
Iteration 34/1000 | Loss: 0.00002544
Iteration 35/1000 | Loss: 0.00002544
Iteration 36/1000 | Loss: 0.00002544
Iteration 37/1000 | Loss: 0.00002543
Iteration 38/1000 | Loss: 0.00002543
Iteration 39/1000 | Loss: 0.00002543
Iteration 40/1000 | Loss: 0.00002543
Iteration 41/1000 | Loss: 0.00002542
Iteration 42/1000 | Loss: 0.00002542
Iteration 43/1000 | Loss: 0.00002542
Iteration 44/1000 | Loss: 0.00002542
Iteration 45/1000 | Loss: 0.00002542
Iteration 46/1000 | Loss: 0.00002542
Iteration 47/1000 | Loss: 0.00002542
Iteration 48/1000 | Loss: 0.00002541
Iteration 49/1000 | Loss: 0.00002541
Iteration 50/1000 | Loss: 0.00002541
Iteration 51/1000 | Loss: 0.00002541
Iteration 52/1000 | Loss: 0.00002540
Iteration 53/1000 | Loss: 0.00002540
Iteration 54/1000 | Loss: 0.00002540
Iteration 55/1000 | Loss: 0.00002539
Iteration 56/1000 | Loss: 0.00002539
Iteration 57/1000 | Loss: 0.00002539
Iteration 58/1000 | Loss: 0.00002538
Iteration 59/1000 | Loss: 0.00002538
Iteration 60/1000 | Loss: 0.00002538
Iteration 61/1000 | Loss: 0.00002538
Iteration 62/1000 | Loss: 0.00002538
Iteration 63/1000 | Loss: 0.00002537
Iteration 64/1000 | Loss: 0.00002537
Iteration 65/1000 | Loss: 0.00002536
Iteration 66/1000 | Loss: 0.00002536
Iteration 67/1000 | Loss: 0.00002535
Iteration 68/1000 | Loss: 0.00002535
Iteration 69/1000 | Loss: 0.00002535
Iteration 70/1000 | Loss: 0.00002535
Iteration 71/1000 | Loss: 0.00002535
Iteration 72/1000 | Loss: 0.00002535
Iteration 73/1000 | Loss: 0.00002535
Iteration 74/1000 | Loss: 0.00002535
Iteration 75/1000 | Loss: 0.00002535
Iteration 76/1000 | Loss: 0.00002534
Iteration 77/1000 | Loss: 0.00002534
Iteration 78/1000 | Loss: 0.00002534
Iteration 79/1000 | Loss: 0.00002534
Iteration 80/1000 | Loss: 0.00002534
Iteration 81/1000 | Loss: 0.00002533
Iteration 82/1000 | Loss: 0.00002533
Iteration 83/1000 | Loss: 0.00002532
Iteration 84/1000 | Loss: 0.00002532
Iteration 85/1000 | Loss: 0.00002532
Iteration 86/1000 | Loss: 0.00002530
Iteration 87/1000 | Loss: 0.00002530
Iteration 88/1000 | Loss: 0.00002529
Iteration 89/1000 | Loss: 0.00002528
Iteration 90/1000 | Loss: 0.00002523
Iteration 91/1000 | Loss: 0.00002520
Iteration 92/1000 | Loss: 0.00002519
Iteration 93/1000 | Loss: 0.00002519
Iteration 94/1000 | Loss: 0.00002519
Iteration 95/1000 | Loss: 0.00002519
Iteration 96/1000 | Loss: 0.00002518
Iteration 97/1000 | Loss: 0.00002518
Iteration 98/1000 | Loss: 0.00002518
Iteration 99/1000 | Loss: 0.00002518
Iteration 100/1000 | Loss: 0.00002518
Iteration 101/1000 | Loss: 0.00002518
Iteration 102/1000 | Loss: 0.00002517
Iteration 103/1000 | Loss: 0.00002517
Iteration 104/1000 | Loss: 0.00002517
Iteration 105/1000 | Loss: 0.00002517
Iteration 106/1000 | Loss: 0.00002517
Iteration 107/1000 | Loss: 0.00002517
Iteration 108/1000 | Loss: 0.00002517
Iteration 109/1000 | Loss: 0.00002517
Iteration 110/1000 | Loss: 0.00002516
Iteration 111/1000 | Loss: 0.00002516
Iteration 112/1000 | Loss: 0.00002516
Iteration 113/1000 | Loss: 0.00002516
Iteration 114/1000 | Loss: 0.00002516
Iteration 115/1000 | Loss: 0.00002516
Iteration 116/1000 | Loss: 0.00002516
Iteration 117/1000 | Loss: 0.00002516
Iteration 118/1000 | Loss: 0.00002516
Iteration 119/1000 | Loss: 0.00002516
Iteration 120/1000 | Loss: 0.00002516
Iteration 121/1000 | Loss: 0.00002516
Iteration 122/1000 | Loss: 0.00002516
Iteration 123/1000 | Loss: 0.00002516
Iteration 124/1000 | Loss: 0.00002516
Iteration 125/1000 | Loss: 0.00002516
Iteration 126/1000 | Loss: 0.00002515
Iteration 127/1000 | Loss: 0.00002515
Iteration 128/1000 | Loss: 0.00002515
Iteration 129/1000 | Loss: 0.00002515
Iteration 130/1000 | Loss: 0.00002515
Iteration 131/1000 | Loss: 0.00002515
Iteration 132/1000 | Loss: 0.00002514
Iteration 133/1000 | Loss: 0.00002514
Iteration 134/1000 | Loss: 0.00002514
Iteration 135/1000 | Loss: 0.00002514
Iteration 136/1000 | Loss: 0.00002514
Iteration 137/1000 | Loss: 0.00002514
Iteration 138/1000 | Loss: 0.00002514
Iteration 139/1000 | Loss: 0.00002514
Iteration 140/1000 | Loss: 0.00002514
Iteration 141/1000 | Loss: 0.00002513
Iteration 142/1000 | Loss: 0.00002513
Iteration 143/1000 | Loss: 0.00002513
Iteration 144/1000 | Loss: 0.00002513
Iteration 145/1000 | Loss: 0.00002512
Iteration 146/1000 | Loss: 0.00002512
Iteration 147/1000 | Loss: 0.00002512
Iteration 148/1000 | Loss: 0.00002512
Iteration 149/1000 | Loss: 0.00002512
Iteration 150/1000 | Loss: 0.00002512
Iteration 151/1000 | Loss: 0.00002512
Iteration 152/1000 | Loss: 0.00002512
Iteration 153/1000 | Loss: 0.00002512
Iteration 154/1000 | Loss: 0.00002512
Iteration 155/1000 | Loss: 0.00002512
Iteration 156/1000 | Loss: 0.00002512
Iteration 157/1000 | Loss: 0.00002511
Iteration 158/1000 | Loss: 0.00002511
Iteration 159/1000 | Loss: 0.00002511
Iteration 160/1000 | Loss: 0.00002511
Iteration 161/1000 | Loss: 0.00002511
Iteration 162/1000 | Loss: 0.00002511
Iteration 163/1000 | Loss: 0.00002511
Iteration 164/1000 | Loss: 0.00002511
Iteration 165/1000 | Loss: 0.00002511
Iteration 166/1000 | Loss: 0.00002511
Iteration 167/1000 | Loss: 0.00002511
Iteration 168/1000 | Loss: 0.00002510
Iteration 169/1000 | Loss: 0.00002510
Iteration 170/1000 | Loss: 0.00002510
Iteration 171/1000 | Loss: 0.00002510
Iteration 172/1000 | Loss: 0.00002510
Iteration 173/1000 | Loss: 0.00002510
Iteration 174/1000 | Loss: 0.00002510
Iteration 175/1000 | Loss: 0.00002510
Iteration 176/1000 | Loss: 0.00002510
Iteration 177/1000 | Loss: 0.00002510
Iteration 178/1000 | Loss: 0.00002510
Iteration 179/1000 | Loss: 0.00002510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.5100680431933142e-05, 2.5100680431933142e-05, 2.5100680431933142e-05, 2.5100680431933142e-05, 2.5100680431933142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5100680431933142e-05

Optimization complete. Final v2v error: 4.150009632110596 mm

Highest mean error: 5.385846138000488 mm for frame 237

Lowest mean error: 3.706122636795044 mm for frame 102

Saving results

Total time: 81.8204300403595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902943
Iteration 2/25 | Loss: 0.00173122
Iteration 3/25 | Loss: 0.00146203
Iteration 4/25 | Loss: 0.00142718
Iteration 5/25 | Loss: 0.00142071
Iteration 6/25 | Loss: 0.00141954
Iteration 7/25 | Loss: 0.00141952
Iteration 8/25 | Loss: 0.00141952
Iteration 9/25 | Loss: 0.00141952
Iteration 10/25 | Loss: 0.00141952
Iteration 11/25 | Loss: 0.00141952
Iteration 12/25 | Loss: 0.00141952
Iteration 13/25 | Loss: 0.00141952
Iteration 14/25 | Loss: 0.00141952
Iteration 15/25 | Loss: 0.00141952
Iteration 16/25 | Loss: 0.00141952
Iteration 17/25 | Loss: 0.00141952
Iteration 18/25 | Loss: 0.00141952
Iteration 19/25 | Loss: 0.00141952
Iteration 20/25 | Loss: 0.00141952
Iteration 21/25 | Loss: 0.00141952
Iteration 22/25 | Loss: 0.00141952
Iteration 23/25 | Loss: 0.00141952
Iteration 24/25 | Loss: 0.00141952
Iteration 25/25 | Loss: 0.00141952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28497493
Iteration 2/25 | Loss: 0.00128521
Iteration 3/25 | Loss: 0.00128520
Iteration 4/25 | Loss: 0.00128520
Iteration 5/25 | Loss: 0.00128520
Iteration 6/25 | Loss: 0.00128520
Iteration 7/25 | Loss: 0.00128520
Iteration 8/25 | Loss: 0.00128520
Iteration 9/25 | Loss: 0.00128520
Iteration 10/25 | Loss: 0.00128520
Iteration 11/25 | Loss: 0.00128520
Iteration 12/25 | Loss: 0.00128520
Iteration 13/25 | Loss: 0.00128520
Iteration 14/25 | Loss: 0.00128520
Iteration 15/25 | Loss: 0.00128520
Iteration 16/25 | Loss: 0.00128520
Iteration 17/25 | Loss: 0.00128520
Iteration 18/25 | Loss: 0.00128520
Iteration 19/25 | Loss: 0.00128520
Iteration 20/25 | Loss: 0.00128520
Iteration 21/25 | Loss: 0.00128520
Iteration 22/25 | Loss: 0.00128520
Iteration 23/25 | Loss: 0.00128520
Iteration 24/25 | Loss: 0.00128520
Iteration 25/25 | Loss: 0.00128520

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128520
Iteration 2/1000 | Loss: 0.00003712
Iteration 3/1000 | Loss: 0.00002525
Iteration 4/1000 | Loss: 0.00002226
Iteration 5/1000 | Loss: 0.00002077
Iteration 6/1000 | Loss: 0.00002017
Iteration 7/1000 | Loss: 0.00001979
Iteration 8/1000 | Loss: 0.00001948
Iteration 9/1000 | Loss: 0.00001920
Iteration 10/1000 | Loss: 0.00001900
Iteration 11/1000 | Loss: 0.00001881
Iteration 12/1000 | Loss: 0.00001874
Iteration 13/1000 | Loss: 0.00001871
Iteration 14/1000 | Loss: 0.00001871
Iteration 15/1000 | Loss: 0.00001871
Iteration 16/1000 | Loss: 0.00001870
Iteration 17/1000 | Loss: 0.00001869
Iteration 18/1000 | Loss: 0.00001869
Iteration 19/1000 | Loss: 0.00001869
Iteration 20/1000 | Loss: 0.00001869
Iteration 21/1000 | Loss: 0.00001869
Iteration 22/1000 | Loss: 0.00001869
Iteration 23/1000 | Loss: 0.00001869
Iteration 24/1000 | Loss: 0.00001869
Iteration 25/1000 | Loss: 0.00001868
Iteration 26/1000 | Loss: 0.00001867
Iteration 27/1000 | Loss: 0.00001867
Iteration 28/1000 | Loss: 0.00001867
Iteration 29/1000 | Loss: 0.00001867
Iteration 30/1000 | Loss: 0.00001867
Iteration 31/1000 | Loss: 0.00001867
Iteration 32/1000 | Loss: 0.00001867
Iteration 33/1000 | Loss: 0.00001867
Iteration 34/1000 | Loss: 0.00001867
Iteration 35/1000 | Loss: 0.00001867
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001866
Iteration 40/1000 | Loss: 0.00001865
Iteration 41/1000 | Loss: 0.00001865
Iteration 42/1000 | Loss: 0.00001865
Iteration 43/1000 | Loss: 0.00001865
Iteration 44/1000 | Loss: 0.00001865
Iteration 45/1000 | Loss: 0.00001864
Iteration 46/1000 | Loss: 0.00001864
Iteration 47/1000 | Loss: 0.00001864
Iteration 48/1000 | Loss: 0.00001864
Iteration 49/1000 | Loss: 0.00001864
Iteration 50/1000 | Loss: 0.00001864
Iteration 51/1000 | Loss: 0.00001864
Iteration 52/1000 | Loss: 0.00001864
Iteration 53/1000 | Loss: 0.00001864
Iteration 54/1000 | Loss: 0.00001864
Iteration 55/1000 | Loss: 0.00001863
Iteration 56/1000 | Loss: 0.00001863
Iteration 57/1000 | Loss: 0.00001863
Iteration 58/1000 | Loss: 0.00001863
Iteration 59/1000 | Loss: 0.00001863
Iteration 60/1000 | Loss: 0.00001862
Iteration 61/1000 | Loss: 0.00001862
Iteration 62/1000 | Loss: 0.00001862
Iteration 63/1000 | Loss: 0.00001862
Iteration 64/1000 | Loss: 0.00001862
Iteration 65/1000 | Loss: 0.00001862
Iteration 66/1000 | Loss: 0.00001862
Iteration 67/1000 | Loss: 0.00001862
Iteration 68/1000 | Loss: 0.00001862
Iteration 69/1000 | Loss: 0.00001862
Iteration 70/1000 | Loss: 0.00001862
Iteration 71/1000 | Loss: 0.00001862
Iteration 72/1000 | Loss: 0.00001862
Iteration 73/1000 | Loss: 0.00001862
Iteration 74/1000 | Loss: 0.00001862
Iteration 75/1000 | Loss: 0.00001862
Iteration 76/1000 | Loss: 0.00001862
Iteration 77/1000 | Loss: 0.00001861
Iteration 78/1000 | Loss: 0.00001861
Iteration 79/1000 | Loss: 0.00001861
Iteration 80/1000 | Loss: 0.00001861
Iteration 81/1000 | Loss: 0.00001861
Iteration 82/1000 | Loss: 0.00001861
Iteration 83/1000 | Loss: 0.00001861
Iteration 84/1000 | Loss: 0.00001861
Iteration 85/1000 | Loss: 0.00001861
Iteration 86/1000 | Loss: 0.00001861
Iteration 87/1000 | Loss: 0.00001861
Iteration 88/1000 | Loss: 0.00001861
Iteration 89/1000 | Loss: 0.00001861
Iteration 90/1000 | Loss: 0.00001861
Iteration 91/1000 | Loss: 0.00001861
Iteration 92/1000 | Loss: 0.00001861
Iteration 93/1000 | Loss: 0.00001861
Iteration 94/1000 | Loss: 0.00001861
Iteration 95/1000 | Loss: 0.00001861
Iteration 96/1000 | Loss: 0.00001861
Iteration 97/1000 | Loss: 0.00001861
Iteration 98/1000 | Loss: 0.00001861
Iteration 99/1000 | Loss: 0.00001861
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001861
Iteration 102/1000 | Loss: 0.00001861
Iteration 103/1000 | Loss: 0.00001861
Iteration 104/1000 | Loss: 0.00001861
Iteration 105/1000 | Loss: 0.00001861
Iteration 106/1000 | Loss: 0.00001861
Iteration 107/1000 | Loss: 0.00001861
Iteration 108/1000 | Loss: 0.00001861
Iteration 109/1000 | Loss: 0.00001861
Iteration 110/1000 | Loss: 0.00001861
Iteration 111/1000 | Loss: 0.00001861
Iteration 112/1000 | Loss: 0.00001861
Iteration 113/1000 | Loss: 0.00001861
Iteration 114/1000 | Loss: 0.00001861
Iteration 115/1000 | Loss: 0.00001861
Iteration 116/1000 | Loss: 0.00001861
Iteration 117/1000 | Loss: 0.00001861
Iteration 118/1000 | Loss: 0.00001861
Iteration 119/1000 | Loss: 0.00001861
Iteration 120/1000 | Loss: 0.00001861
Iteration 121/1000 | Loss: 0.00001861
Iteration 122/1000 | Loss: 0.00001861
Iteration 123/1000 | Loss: 0.00001861
Iteration 124/1000 | Loss: 0.00001861
Iteration 125/1000 | Loss: 0.00001861
Iteration 126/1000 | Loss: 0.00001861
Iteration 127/1000 | Loss: 0.00001861
Iteration 128/1000 | Loss: 0.00001861
Iteration 129/1000 | Loss: 0.00001861
Iteration 130/1000 | Loss: 0.00001861
Iteration 131/1000 | Loss: 0.00001861
Iteration 132/1000 | Loss: 0.00001861
Iteration 133/1000 | Loss: 0.00001861
Iteration 134/1000 | Loss: 0.00001861
Iteration 135/1000 | Loss: 0.00001861
Iteration 136/1000 | Loss: 0.00001861
Iteration 137/1000 | Loss: 0.00001861
Iteration 138/1000 | Loss: 0.00001861
Iteration 139/1000 | Loss: 0.00001861
Iteration 140/1000 | Loss: 0.00001861
Iteration 141/1000 | Loss: 0.00001861
Iteration 142/1000 | Loss: 0.00001861
Iteration 143/1000 | Loss: 0.00001861
Iteration 144/1000 | Loss: 0.00001861
Iteration 145/1000 | Loss: 0.00001861
Iteration 146/1000 | Loss: 0.00001861
Iteration 147/1000 | Loss: 0.00001861
Iteration 148/1000 | Loss: 0.00001861
Iteration 149/1000 | Loss: 0.00001861
Iteration 150/1000 | Loss: 0.00001861
Iteration 151/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.8609289327287115e-05, 1.8609289327287115e-05, 1.8609289327287115e-05, 1.8609289327287115e-05, 1.8609289327287115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8609289327287115e-05

Optimization complete. Final v2v error: 3.6526107788085938 mm

Highest mean error: 4.0741496086120605 mm for frame 1

Lowest mean error: 3.462646722793579 mm for frame 86

Saving results

Total time: 33.18627572059631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2381/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2381/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01184995
Iteration 2/25 | Loss: 0.00389419
Iteration 3/25 | Loss: 0.00259365
Iteration 4/25 | Loss: 0.00240151
Iteration 5/25 | Loss: 0.00269113
Iteration 6/25 | Loss: 0.00280426
Iteration 7/25 | Loss: 0.00252124
Iteration 8/25 | Loss: 0.00266347
Iteration 9/25 | Loss: 0.00246978
Iteration 10/25 | Loss: 0.00225688
Iteration 11/25 | Loss: 0.00217096
Iteration 12/25 | Loss: 0.00207596
Iteration 13/25 | Loss: 0.00199110
Iteration 14/25 | Loss: 0.00194617
Iteration 15/25 | Loss: 0.00190844
Iteration 16/25 | Loss: 0.00186450
Iteration 17/25 | Loss: 0.00184571
Iteration 18/25 | Loss: 0.00182149
Iteration 19/25 | Loss: 0.00181338
Iteration 20/25 | Loss: 0.00179221
Iteration 21/25 | Loss: 0.00179163
Iteration 22/25 | Loss: 0.00178767
Iteration 23/25 | Loss: 0.00177621
Iteration 24/25 | Loss: 0.00177881
Iteration 25/25 | Loss: 0.00177901

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62840182
Iteration 2/25 | Loss: 0.00151597
Iteration 3/25 | Loss: 0.00151597
Iteration 4/25 | Loss: 0.00151597
Iteration 5/25 | Loss: 0.00151597
Iteration 6/25 | Loss: 0.00151597
Iteration 7/25 | Loss: 0.00151597
Iteration 8/25 | Loss: 0.00151597
Iteration 9/25 | Loss: 0.00151597
Iteration 10/25 | Loss: 0.00151597
Iteration 11/25 | Loss: 0.00151597
Iteration 12/25 | Loss: 0.00151597
Iteration 13/25 | Loss: 0.00151597
Iteration 14/25 | Loss: 0.00151597
Iteration 15/25 | Loss: 0.00151597
Iteration 16/25 | Loss: 0.00151597
Iteration 17/25 | Loss: 0.00151597
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015159697504714131, 0.0015159697504714131, 0.0015159697504714131, 0.0015159697504714131, 0.0015159697504714131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015159697504714131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151597
Iteration 2/1000 | Loss: 0.00050241
Iteration 3/1000 | Loss: 0.00059562
Iteration 4/1000 | Loss: 0.00028341
Iteration 5/1000 | Loss: 0.00057573
Iteration 6/1000 | Loss: 0.00040932
Iteration 7/1000 | Loss: 0.00030465
Iteration 8/1000 | Loss: 0.00063430
Iteration 9/1000 | Loss: 0.00056714
Iteration 10/1000 | Loss: 0.00066845
Iteration 11/1000 | Loss: 0.00050702
Iteration 12/1000 | Loss: 0.00046094
Iteration 13/1000 | Loss: 0.00057049
Iteration 14/1000 | Loss: 0.00044026
Iteration 15/1000 | Loss: 0.00047340
Iteration 16/1000 | Loss: 0.00050976
Iteration 17/1000 | Loss: 0.00045297
Iteration 18/1000 | Loss: 0.00049299
Iteration 19/1000 | Loss: 0.00049651
Iteration 20/1000 | Loss: 0.00051324
Iteration 21/1000 | Loss: 0.00055723
Iteration 22/1000 | Loss: 0.00055709
Iteration 23/1000 | Loss: 0.00048495
Iteration 24/1000 | Loss: 0.00048919
Iteration 25/1000 | Loss: 0.00033892
Iteration 26/1000 | Loss: 0.00033534
Iteration 27/1000 | Loss: 0.00046722
Iteration 28/1000 | Loss: 0.00048132
Iteration 29/1000 | Loss: 0.00047549
Iteration 30/1000 | Loss: 0.00054972
Iteration 31/1000 | Loss: 0.00044457
Iteration 32/1000 | Loss: 0.00050728
Iteration 33/1000 | Loss: 0.00059037
Iteration 34/1000 | Loss: 0.00042582
Iteration 35/1000 | Loss: 0.00060800
Iteration 36/1000 | Loss: 0.00056983
Iteration 37/1000 | Loss: 0.00062444
Iteration 38/1000 | Loss: 0.00065031
Iteration 39/1000 | Loss: 0.00078259
Iteration 40/1000 | Loss: 0.00072594
Iteration 41/1000 | Loss: 0.00048659
Iteration 42/1000 | Loss: 0.00065254
Iteration 43/1000 | Loss: 0.00061740
Iteration 44/1000 | Loss: 0.00051752
Iteration 45/1000 | Loss: 0.00051898
Iteration 46/1000 | Loss: 0.00047176
Iteration 47/1000 | Loss: 0.00052474
Iteration 48/1000 | Loss: 0.00067276
Iteration 49/1000 | Loss: 0.00051981
Iteration 50/1000 | Loss: 0.00035456
Iteration 51/1000 | Loss: 0.00042938
Iteration 52/1000 | Loss: 0.00052206
Iteration 53/1000 | Loss: 0.00047868
Iteration 54/1000 | Loss: 0.00035021
Iteration 55/1000 | Loss: 0.00036125
Iteration 56/1000 | Loss: 0.00052136
Iteration 57/1000 | Loss: 0.00052200
Iteration 58/1000 | Loss: 0.00059225
Iteration 59/1000 | Loss: 0.00052525
Iteration 60/1000 | Loss: 0.00054947
Iteration 61/1000 | Loss: 0.00047788
Iteration 62/1000 | Loss: 0.00048222
Iteration 63/1000 | Loss: 0.00050466
Iteration 64/1000 | Loss: 0.00047399
Iteration 65/1000 | Loss: 0.00047057
Iteration 66/1000 | Loss: 0.00055068
Iteration 67/1000 | Loss: 0.00060696
Iteration 68/1000 | Loss: 0.00035245
Iteration 69/1000 | Loss: 0.00032806
Iteration 70/1000 | Loss: 0.00040004
Iteration 71/1000 | Loss: 0.00051433
Iteration 72/1000 | Loss: 0.00047582
Iteration 73/1000 | Loss: 0.00052585
Iteration 74/1000 | Loss: 0.00047718
Iteration 75/1000 | Loss: 0.00042942
Iteration 76/1000 | Loss: 0.00054036
Iteration 77/1000 | Loss: 0.00051970
Iteration 78/1000 | Loss: 0.00048054
Iteration 79/1000 | Loss: 0.00049029
Iteration 80/1000 | Loss: 0.00049893
Iteration 81/1000 | Loss: 0.00039666
Iteration 82/1000 | Loss: 0.00037841
Iteration 83/1000 | Loss: 0.00054895
Iteration 84/1000 | Loss: 0.00048067
Iteration 85/1000 | Loss: 0.00039739
Iteration 86/1000 | Loss: 0.00044941
Iteration 87/1000 | Loss: 0.00043007
Iteration 88/1000 | Loss: 0.00057787
Iteration 89/1000 | Loss: 0.00051709
Iteration 90/1000 | Loss: 0.00039986
Iteration 91/1000 | Loss: 0.00031731
Iteration 92/1000 | Loss: 0.00037031
Iteration 93/1000 | Loss: 0.00043722
Iteration 94/1000 | Loss: 0.00041925
Iteration 95/1000 | Loss: 0.00056000
Iteration 96/1000 | Loss: 0.00051599
Iteration 97/1000 | Loss: 0.00042451
Iteration 98/1000 | Loss: 0.00043650
Iteration 99/1000 | Loss: 0.00089016
Iteration 100/1000 | Loss: 0.00059368
Iteration 101/1000 | Loss: 0.00053714
Iteration 102/1000 | Loss: 0.00077482
Iteration 103/1000 | Loss: 0.00058516
Iteration 104/1000 | Loss: 0.00084896
Iteration 105/1000 | Loss: 0.00079737
Iteration 106/1000 | Loss: 0.00064609
Iteration 107/1000 | Loss: 0.00058110
Iteration 108/1000 | Loss: 0.00176094
Iteration 109/1000 | Loss: 0.00048764
Iteration 110/1000 | Loss: 0.00076223
Iteration 111/1000 | Loss: 0.00074860
Iteration 112/1000 | Loss: 0.00052112
Iteration 113/1000 | Loss: 0.00059162
Iteration 114/1000 | Loss: 0.00049044
Iteration 115/1000 | Loss: 0.00050230
Iteration 116/1000 | Loss: 0.00069251
Iteration 117/1000 | Loss: 0.00165808
Iteration 118/1000 | Loss: 0.00042461
Iteration 119/1000 | Loss: 0.00035507
Iteration 120/1000 | Loss: 0.00038987
Iteration 121/1000 | Loss: 0.00044225
Iteration 122/1000 | Loss: 0.00046516
Iteration 123/1000 | Loss: 0.00044546
Iteration 124/1000 | Loss: 0.00038587
Iteration 125/1000 | Loss: 0.00043795
Iteration 126/1000 | Loss: 0.00032169
Iteration 127/1000 | Loss: 0.00026582
Iteration 128/1000 | Loss: 0.00048883
Iteration 129/1000 | Loss: 0.00034371
Iteration 130/1000 | Loss: 0.00037535
Iteration 131/1000 | Loss: 0.00032104
Iteration 132/1000 | Loss: 0.00033406
Iteration 133/1000 | Loss: 0.00032021
Iteration 134/1000 | Loss: 0.00029137
Iteration 135/1000 | Loss: 0.00034747
Iteration 136/1000 | Loss: 0.00027834
Iteration 137/1000 | Loss: 0.00033251
Iteration 138/1000 | Loss: 0.00038259
Iteration 139/1000 | Loss: 0.00033650
Iteration 140/1000 | Loss: 0.00035889
Iteration 141/1000 | Loss: 0.00034031
Iteration 142/1000 | Loss: 0.00034618
Iteration 143/1000 | Loss: 0.00034156
Iteration 144/1000 | Loss: 0.00033896
Iteration 145/1000 | Loss: 0.00031839
Iteration 146/1000 | Loss: 0.00032934
Iteration 147/1000 | Loss: 0.00033829
Iteration 148/1000 | Loss: 0.00033223
Iteration 149/1000 | Loss: 0.00033516
Iteration 150/1000 | Loss: 0.00035681
Iteration 151/1000 | Loss: 0.00038826
Iteration 152/1000 | Loss: 0.00035411
Iteration 153/1000 | Loss: 0.00036374
Iteration 154/1000 | Loss: 0.00028883
Iteration 155/1000 | Loss: 0.00028751
Iteration 156/1000 | Loss: 0.00026280
Iteration 157/1000 | Loss: 0.00019959
Iteration 158/1000 | Loss: 0.00029156
Iteration 159/1000 | Loss: 0.00034357
Iteration 160/1000 | Loss: 0.00034102
Iteration 161/1000 | Loss: 0.00031770
Iteration 162/1000 | Loss: 0.00027329
Iteration 163/1000 | Loss: 0.00021500
Iteration 164/1000 | Loss: 0.00022266
Iteration 165/1000 | Loss: 0.00031890
Iteration 166/1000 | Loss: 0.00024807
Iteration 167/1000 | Loss: 0.00030588
Iteration 168/1000 | Loss: 0.00028745
Iteration 169/1000 | Loss: 0.00029378
Iteration 170/1000 | Loss: 0.00034373
Iteration 171/1000 | Loss: 0.00027275
Iteration 172/1000 | Loss: 0.00026969
Iteration 173/1000 | Loss: 0.00030183
Iteration 174/1000 | Loss: 0.00030421
Iteration 175/1000 | Loss: 0.00035939
Iteration 176/1000 | Loss: 0.00036219
Iteration 177/1000 | Loss: 0.00036859
Iteration 178/1000 | Loss: 0.00036987
Iteration 179/1000 | Loss: 0.00037254
Iteration 180/1000 | Loss: 0.00035549
Iteration 181/1000 | Loss: 0.00034439
Iteration 182/1000 | Loss: 0.00032514
Iteration 183/1000 | Loss: 0.00032813
Iteration 184/1000 | Loss: 0.00025730
Iteration 185/1000 | Loss: 0.00023211
Iteration 186/1000 | Loss: 0.00036742
Iteration 187/1000 | Loss: 0.00025941
Iteration 188/1000 | Loss: 0.00024003
Iteration 189/1000 | Loss: 0.00024483
Iteration 190/1000 | Loss: 0.00065479
Iteration 191/1000 | Loss: 0.00029265
Iteration 192/1000 | Loss: 0.00027477
Iteration 193/1000 | Loss: 0.00030094
Iteration 194/1000 | Loss: 0.00028686
Iteration 195/1000 | Loss: 0.00027192
Iteration 196/1000 | Loss: 0.00026889
Iteration 197/1000 | Loss: 0.00026976
Iteration 198/1000 | Loss: 0.00032012
Iteration 199/1000 | Loss: 0.00031734
Iteration 200/1000 | Loss: 0.00030066
Iteration 201/1000 | Loss: 0.00032400
Iteration 202/1000 | Loss: 0.00030875
Iteration 203/1000 | Loss: 0.00030293
Iteration 204/1000 | Loss: 0.00029909
Iteration 205/1000 | Loss: 0.00030062
Iteration 206/1000 | Loss: 0.00033231
Iteration 207/1000 | Loss: 0.00031559
Iteration 208/1000 | Loss: 0.00030426
Iteration 209/1000 | Loss: 0.00033874
Iteration 210/1000 | Loss: 0.00034818
Iteration 211/1000 | Loss: 0.00026515
Iteration 212/1000 | Loss: 0.00030287
Iteration 213/1000 | Loss: 0.00027929
Iteration 214/1000 | Loss: 0.00020848
Iteration 215/1000 | Loss: 0.00020961
Iteration 216/1000 | Loss: 0.00023221
Iteration 217/1000 | Loss: 0.00025720
Iteration 218/1000 | Loss: 0.00027902
Iteration 219/1000 | Loss: 0.00022500
Iteration 220/1000 | Loss: 0.00030815
Iteration 221/1000 | Loss: 0.00042043
Iteration 222/1000 | Loss: 0.00036905
Iteration 223/1000 | Loss: 0.00027434
Iteration 224/1000 | Loss: 0.00027350
Iteration 225/1000 | Loss: 0.00020351
Iteration 226/1000 | Loss: 0.00024006
Iteration 227/1000 | Loss: 0.00021872
Iteration 228/1000 | Loss: 0.00027836
Iteration 229/1000 | Loss: 0.00021979
Iteration 230/1000 | Loss: 0.00023590
Iteration 231/1000 | Loss: 0.00027135
Iteration 232/1000 | Loss: 0.00023512
Iteration 233/1000 | Loss: 0.00022467
Iteration 234/1000 | Loss: 0.00031151
Iteration 235/1000 | Loss: 0.00024300
Iteration 236/1000 | Loss: 0.00024568
Iteration 237/1000 | Loss: 0.00024272
Iteration 238/1000 | Loss: 0.00027146
Iteration 239/1000 | Loss: 0.00023710
Iteration 240/1000 | Loss: 0.00028285
Iteration 241/1000 | Loss: 0.00023996
Iteration 242/1000 | Loss: 0.00027533
Iteration 243/1000 | Loss: 0.00025250
Iteration 244/1000 | Loss: 0.00025076
Iteration 245/1000 | Loss: 0.00022279
Iteration 246/1000 | Loss: 0.00028301
Iteration 247/1000 | Loss: 0.00027332
Iteration 248/1000 | Loss: 0.00026989
Iteration 249/1000 | Loss: 0.00029697
Iteration 250/1000 | Loss: 0.00029434
Iteration 251/1000 | Loss: 0.00029687
Iteration 252/1000 | Loss: 0.00026482
Iteration 253/1000 | Loss: 0.00023000
Iteration 254/1000 | Loss: 0.00023679
Iteration 255/1000 | Loss: 0.00027304
Iteration 256/1000 | Loss: 0.00032937
Iteration 257/1000 | Loss: 0.00027757
Iteration 258/1000 | Loss: 0.00028428
Iteration 259/1000 | Loss: 0.00024482
Iteration 260/1000 | Loss: 0.00028256
Iteration 261/1000 | Loss: 0.00024111
Iteration 262/1000 | Loss: 0.00023124
Iteration 263/1000 | Loss: 0.00026820
Iteration 264/1000 | Loss: 0.00024654
Iteration 265/1000 | Loss: 0.00022664
Iteration 266/1000 | Loss: 0.00026434
Iteration 267/1000 | Loss: 0.00024739
Iteration 268/1000 | Loss: 0.00026402
Iteration 269/1000 | Loss: 0.00022560
Iteration 270/1000 | Loss: 0.00022719
Iteration 271/1000 | Loss: 0.00033261
Iteration 272/1000 | Loss: 0.00025505
Iteration 273/1000 | Loss: 0.00020310
Iteration 274/1000 | Loss: 0.00021756
Iteration 275/1000 | Loss: 0.00027295
Iteration 276/1000 | Loss: 0.00025420
Iteration 277/1000 | Loss: 0.00023050
Iteration 278/1000 | Loss: 0.00022458
Iteration 279/1000 | Loss: 0.00024793
Iteration 280/1000 | Loss: 0.00024236
Iteration 281/1000 | Loss: 0.00023707
Iteration 282/1000 | Loss: 0.00028488
Iteration 283/1000 | Loss: 0.00022536
Iteration 284/1000 | Loss: 0.00023172
Iteration 285/1000 | Loss: 0.00028363
Iteration 286/1000 | Loss: 0.00027658
Iteration 287/1000 | Loss: 0.00030041
Iteration 288/1000 | Loss: 0.00030909
Iteration 289/1000 | Loss: 0.00026309
Iteration 290/1000 | Loss: 0.00022246
Iteration 291/1000 | Loss: 0.00022207
Iteration 292/1000 | Loss: 0.00025131
Iteration 293/1000 | Loss: 0.00026781
Iteration 294/1000 | Loss: 0.00019197
Iteration 295/1000 | Loss: 0.00017679
Iteration 296/1000 | Loss: 0.00020471
Iteration 297/1000 | Loss: 0.00020407
Iteration 298/1000 | Loss: 0.00025427
Iteration 299/1000 | Loss: 0.00020813
Iteration 300/1000 | Loss: 0.00024444
Iteration 301/1000 | Loss: 0.00023842
Iteration 302/1000 | Loss: 0.00022408
Iteration 303/1000 | Loss: 0.00021389
Iteration 304/1000 | Loss: 0.00037782
Iteration 305/1000 | Loss: 0.00027898
Iteration 306/1000 | Loss: 0.00032143
Iteration 307/1000 | Loss: 0.00027372
Iteration 308/1000 | Loss: 0.00029370
Iteration 309/1000 | Loss: 0.00027891
Iteration 310/1000 | Loss: 0.00029776
Iteration 311/1000 | Loss: 0.00031507
Iteration 312/1000 | Loss: 0.00042874
Iteration 313/1000 | Loss: 0.00021902
Iteration 314/1000 | Loss: 0.00020109
Iteration 315/1000 | Loss: 0.00018397
Iteration 316/1000 | Loss: 0.00019855
Iteration 317/1000 | Loss: 0.00022024
Iteration 318/1000 | Loss: 0.00022355
Iteration 319/1000 | Loss: 0.00020498
Iteration 320/1000 | Loss: 0.00018671
Iteration 321/1000 | Loss: 0.00023644
Iteration 322/1000 | Loss: 0.00021158
Iteration 323/1000 | Loss: 0.00020796
Iteration 324/1000 | Loss: 0.00017013
Iteration 325/1000 | Loss: 0.00020635
Iteration 326/1000 | Loss: 0.00016170
Iteration 327/1000 | Loss: 0.00019178
Iteration 328/1000 | Loss: 0.00018109
Iteration 329/1000 | Loss: 0.00019290
Iteration 330/1000 | Loss: 0.00019827
Iteration 331/1000 | Loss: 0.00017672
Iteration 332/1000 | Loss: 0.00018447
Iteration 333/1000 | Loss: 0.00022813
Iteration 334/1000 | Loss: 0.00021885
Iteration 335/1000 | Loss: 0.00020480
Iteration 336/1000 | Loss: 0.00018944
Iteration 337/1000 | Loss: 0.00023850
Iteration 338/1000 | Loss: 0.00023151
Iteration 339/1000 | Loss: 0.00022349
Iteration 340/1000 | Loss: 0.00021685
Iteration 341/1000 | Loss: 0.00021260
Iteration 342/1000 | Loss: 0.00022375
Iteration 343/1000 | Loss: 0.00023644
Iteration 344/1000 | Loss: 0.00023947
Iteration 345/1000 | Loss: 0.00024232
Iteration 346/1000 | Loss: 0.00025519
Iteration 347/1000 | Loss: 0.00019919
Iteration 348/1000 | Loss: 0.00020911
Iteration 349/1000 | Loss: 0.00023360
Iteration 350/1000 | Loss: 0.00025694
Iteration 351/1000 | Loss: 0.00025175
Iteration 352/1000 | Loss: 0.00023798
Iteration 353/1000 | Loss: 0.00023821
Iteration 354/1000 | Loss: 0.00021677
Iteration 355/1000 | Loss: 0.00021943
Iteration 356/1000 | Loss: 0.00033160
Iteration 357/1000 | Loss: 0.00031149
Iteration 358/1000 | Loss: 0.00017807
Iteration 359/1000 | Loss: 0.00022922
Iteration 360/1000 | Loss: 0.00026513
Iteration 361/1000 | Loss: 0.00017849
Iteration 362/1000 | Loss: 0.00025418
Iteration 363/1000 | Loss: 0.00029743
Iteration 364/1000 | Loss: 0.00023397
Iteration 365/1000 | Loss: 0.00023124
Iteration 366/1000 | Loss: 0.00018237
Iteration 367/1000 | Loss: 0.00018364
Iteration 368/1000 | Loss: 0.00021430
Iteration 369/1000 | Loss: 0.00021867
Iteration 370/1000 | Loss: 0.00052612
Iteration 371/1000 | Loss: 0.00022948
Iteration 372/1000 | Loss: 0.00019612
Iteration 373/1000 | Loss: 0.00020307
Iteration 374/1000 | Loss: 0.00019067
Iteration 375/1000 | Loss: 0.00016185
Iteration 376/1000 | Loss: 0.00016221
Iteration 377/1000 | Loss: 0.00020880
Iteration 378/1000 | Loss: 0.00021439
Iteration 379/1000 | Loss: 0.00017355
Iteration 380/1000 | Loss: 0.00016021
Iteration 381/1000 | Loss: 0.00020754
Iteration 382/1000 | Loss: 0.00021344
Iteration 383/1000 | Loss: 0.00022784
Iteration 384/1000 | Loss: 0.00021808
Iteration 385/1000 | Loss: 0.00017715
Iteration 386/1000 | Loss: 0.00020778
Iteration 387/1000 | Loss: 0.00021301
Iteration 388/1000 | Loss: 0.00020594
Iteration 389/1000 | Loss: 0.00021870
Iteration 390/1000 | Loss: 0.00022176
Iteration 391/1000 | Loss: 0.00021657
Iteration 392/1000 | Loss: 0.00019624
Iteration 393/1000 | Loss: 0.00016327
Iteration 394/1000 | Loss: 0.00014788
Iteration 395/1000 | Loss: 0.00017555
Iteration 396/1000 | Loss: 0.00019638
Iteration 397/1000 | Loss: 0.00020798
Iteration 398/1000 | Loss: 0.00017019
Iteration 399/1000 | Loss: 0.00019342
Iteration 400/1000 | Loss: 0.00017966
Iteration 401/1000 | Loss: 0.00020763
Iteration 402/1000 | Loss: 0.00021315
Iteration 403/1000 | Loss: 0.00013976
Iteration 404/1000 | Loss: 0.00021290
Iteration 405/1000 | Loss: 0.00021217
Iteration 406/1000 | Loss: 0.00020380
Iteration 407/1000 | Loss: 0.00015691
Iteration 408/1000 | Loss: 0.00020094
Iteration 409/1000 | Loss: 0.00020368
Iteration 410/1000 | Loss: 0.00021039
Iteration 411/1000 | Loss: 0.00020623
Iteration 412/1000 | Loss: 0.00020941
Iteration 413/1000 | Loss: 0.00020401
Iteration 414/1000 | Loss: 0.00021114
Iteration 415/1000 | Loss: 0.00020440
Iteration 416/1000 | Loss: 0.00015287
Iteration 417/1000 | Loss: 0.00021448
Iteration 418/1000 | Loss: 0.00018823
Iteration 419/1000 | Loss: 0.00021554
Iteration 420/1000 | Loss: 0.00016648
Iteration 421/1000 | Loss: 0.00019041
Iteration 422/1000 | Loss: 0.00021597
Iteration 423/1000 | Loss: 0.00021344
Iteration 424/1000 | Loss: 0.00017033
Iteration 425/1000 | Loss: 0.00019031
Iteration 426/1000 | Loss: 0.00017505
Iteration 427/1000 | Loss: 0.00020501
Iteration 428/1000 | Loss: 0.00020437
Iteration 429/1000 | Loss: 0.00023257
Iteration 430/1000 | Loss: 0.00017288
Iteration 431/1000 | Loss: 0.00022668
Iteration 432/1000 | Loss: 0.00017919
Iteration 433/1000 | Loss: 0.00014378
Iteration 434/1000 | Loss: 0.00019165
Iteration 435/1000 | Loss: 0.00019979
Iteration 436/1000 | Loss: 0.00019500
Iteration 437/1000 | Loss: 0.00015325
Iteration 438/1000 | Loss: 0.00020363
Iteration 439/1000 | Loss: 0.00022434
Iteration 440/1000 | Loss: 0.00017155
Iteration 441/1000 | Loss: 0.00019529
Iteration 442/1000 | Loss: 0.00018125
Iteration 443/1000 | Loss: 0.00019986
Iteration 444/1000 | Loss: 0.00021209
Iteration 445/1000 | Loss: 0.00021115
Iteration 446/1000 | Loss: 0.00022193
Iteration 447/1000 | Loss: 0.00021521
Iteration 448/1000 | Loss: 0.00021980
Iteration 449/1000 | Loss: 0.00022089
Iteration 450/1000 | Loss: 0.00018243
Iteration 451/1000 | Loss: 0.00015347
Iteration 452/1000 | Loss: 0.00019091
Iteration 453/1000 | Loss: 0.00019024
Iteration 454/1000 | Loss: 0.00016039
Iteration 455/1000 | Loss: 0.00018489
Iteration 456/1000 | Loss: 0.00014016
Iteration 457/1000 | Loss: 0.00018964
Iteration 458/1000 | Loss: 0.00017288
Iteration 459/1000 | Loss: 0.00018257
Iteration 460/1000 | Loss: 0.00020182
Iteration 461/1000 | Loss: 0.00019758
Iteration 462/1000 | Loss: 0.00016328
Iteration 463/1000 | Loss: 0.00017141
Iteration 464/1000 | Loss: 0.00016520
Iteration 465/1000 | Loss: 0.00016820
Iteration 466/1000 | Loss: 0.00018295
Iteration 467/1000 | Loss: 0.00020652
Iteration 468/1000 | Loss: 0.00020290
Iteration 469/1000 | Loss: 0.00018664
Iteration 470/1000 | Loss: 0.00020333
Iteration 471/1000 | Loss: 0.00021459
Iteration 472/1000 | Loss: 0.00020823
Iteration 473/1000 | Loss: 0.00020413
Iteration 474/1000 | Loss: 0.00021574
Iteration 475/1000 | Loss: 0.00015380
Iteration 476/1000 | Loss: 0.00020772
Iteration 477/1000 | Loss: 0.00019965
Iteration 478/1000 | Loss: 0.00019873
Iteration 479/1000 | Loss: 0.00018343
Iteration 480/1000 | Loss: 0.00016245
Iteration 481/1000 | Loss: 0.00018985
Iteration 482/1000 | Loss: 0.00018506
Iteration 483/1000 | Loss: 0.00016703
Iteration 484/1000 | Loss: 0.00015256
Iteration 485/1000 | Loss: 0.00016496
Iteration 486/1000 | Loss: 0.00018005
Iteration 487/1000 | Loss: 0.00021597
Iteration 488/1000 | Loss: 0.00018415
Iteration 489/1000 | Loss: 0.00014835
Iteration 490/1000 | Loss: 0.00018071
Iteration 491/1000 | Loss: 0.00019742
Iteration 492/1000 | Loss: 0.00021653
Iteration 493/1000 | Loss: 0.00017819
Iteration 494/1000 | Loss: 0.00015394
Iteration 495/1000 | Loss: 0.00014958
Iteration 496/1000 | Loss: 0.00017818
Iteration 497/1000 | Loss: 0.00016430
Iteration 498/1000 | Loss: 0.00017152
Iteration 499/1000 | Loss: 0.00016137
Iteration 500/1000 | Loss: 0.00016558
Iteration 501/1000 | Loss: 0.00016847
Iteration 502/1000 | Loss: 0.00018896
Iteration 503/1000 | Loss: 0.00019011
Iteration 504/1000 | Loss: 0.00018272
Iteration 505/1000 | Loss: 0.00018255
Iteration 506/1000 | Loss: 0.00018394
Iteration 507/1000 | Loss: 0.00017204
Iteration 508/1000 | Loss: 0.00017518
Iteration 509/1000 | Loss: 0.00014381
Iteration 510/1000 | Loss: 0.00014405
Iteration 511/1000 | Loss: 0.00017173
Iteration 512/1000 | Loss: 0.00016729
Iteration 513/1000 | Loss: 0.00020152
Iteration 514/1000 | Loss: 0.00016573
Iteration 515/1000 | Loss: 0.00015028
Iteration 516/1000 | Loss: 0.00017874
Iteration 517/1000 | Loss: 0.00016798
Iteration 518/1000 | Loss: 0.00016660
Iteration 519/1000 | Loss: 0.00018001
Iteration 520/1000 | Loss: 0.00015189
Iteration 521/1000 | Loss: 0.00020954
Iteration 522/1000 | Loss: 0.00019514
Iteration 523/1000 | Loss: 0.00020877
Iteration 524/1000 | Loss: 0.00019761
Iteration 525/1000 | Loss: 0.00019385
Iteration 526/1000 | Loss: 0.00016847
Iteration 527/1000 | Loss: 0.00021300
Iteration 528/1000 | Loss: 0.00019360
Iteration 529/1000 | Loss: 0.00019956
Iteration 530/1000 | Loss: 0.00018182
Iteration 531/1000 | Loss: 0.00018215
Iteration 532/1000 | Loss: 0.00018789
Iteration 533/1000 | Loss: 0.00019096
Iteration 534/1000 | Loss: 0.00019452
Iteration 535/1000 | Loss: 0.00017503
Iteration 536/1000 | Loss: 0.00018486
Iteration 537/1000 | Loss: 0.00016541
Iteration 538/1000 | Loss: 0.00017462
Iteration 539/1000 | Loss: 0.00018713
Iteration 540/1000 | Loss: 0.00019626
Iteration 541/1000 | Loss: 0.00019144
Iteration 542/1000 | Loss: 0.00020146
Iteration 543/1000 | Loss: 0.00018040
Iteration 544/1000 | Loss: 0.00020772
Iteration 545/1000 | Loss: 0.00021613
Iteration 546/1000 | Loss: 0.00018931
Iteration 547/1000 | Loss: 0.00018395
Iteration 548/1000 | Loss: 0.00016580
Iteration 549/1000 | Loss: 0.00017931
Iteration 550/1000 | Loss: 0.00017789
Iteration 551/1000 | Loss: 0.00017800
Iteration 552/1000 | Loss: 0.00019399
Iteration 553/1000 | Loss: 0.00016502
Iteration 554/1000 | Loss: 0.00016949
Iteration 555/1000 | Loss: 0.00017675
Iteration 556/1000 | Loss: 0.00019462
Iteration 557/1000 | Loss: 0.00017555
Iteration 558/1000 | Loss: 0.00018964
Iteration 559/1000 | Loss: 0.00016620
Iteration 560/1000 | Loss: 0.00020358
Iteration 561/1000 | Loss: 0.00016647
Iteration 562/1000 | Loss: 0.00016868
Iteration 563/1000 | Loss: 0.00013351
Iteration 564/1000 | Loss: 0.00019578
Iteration 565/1000 | Loss: 0.00016868
Iteration 566/1000 | Loss: 0.00016752
Iteration 567/1000 | Loss: 0.00016916
Iteration 568/1000 | Loss: 0.00017607
Iteration 569/1000 | Loss: 0.00019464
Iteration 570/1000 | Loss: 0.00021509
Iteration 571/1000 | Loss: 0.00021778
Iteration 572/1000 | Loss: 0.00020039
Iteration 573/1000 | Loss: 0.00019242
Iteration 574/1000 | Loss: 0.00016771
Iteration 575/1000 | Loss: 0.00023583
Iteration 576/1000 | Loss: 0.00020342
Iteration 577/1000 | Loss: 0.00018144
Iteration 578/1000 | Loss: 0.00016103
Iteration 579/1000 | Loss: 0.00016210
Iteration 580/1000 | Loss: 0.00020856
Iteration 581/1000 | Loss: 0.00020280
Iteration 582/1000 | Loss: 0.00017906
Iteration 583/1000 | Loss: 0.00017498
Iteration 584/1000 | Loss: 0.00017603
Iteration 585/1000 | Loss: 0.00017941
Iteration 586/1000 | Loss: 0.00020171
Iteration 587/1000 | Loss: 0.00017111
Iteration 588/1000 | Loss: 0.00016196
Iteration 589/1000 | Loss: 0.00017796
Iteration 590/1000 | Loss: 0.00013775
Iteration 591/1000 | Loss: 0.00014681
Iteration 592/1000 | Loss: 0.00019766
Iteration 593/1000 | Loss: 0.00018996
Iteration 594/1000 | Loss: 0.00018442
Iteration 595/1000 | Loss: 0.00016864
Iteration 596/1000 | Loss: 0.00020641
Iteration 597/1000 | Loss: 0.00018229
Iteration 598/1000 | Loss: 0.00019015
Iteration 599/1000 | Loss: 0.00018730
Iteration 600/1000 | Loss: 0.00016644
Iteration 601/1000 | Loss: 0.00017983
Iteration 602/1000 | Loss: 0.00020095
Iteration 603/1000 | Loss: 0.00015880
Iteration 604/1000 | Loss: 0.00013474
Iteration 605/1000 | Loss: 0.00013454
Iteration 606/1000 | Loss: 0.00012472
Iteration 607/1000 | Loss: 0.00014507
Iteration 608/1000 | Loss: 0.00016951
Iteration 609/1000 | Loss: 0.00015891
Iteration 610/1000 | Loss: 0.00013245
Iteration 611/1000 | Loss: 0.00013078
Iteration 612/1000 | Loss: 0.00015509
Iteration 613/1000 | Loss: 0.00014688
Iteration 614/1000 | Loss: 0.00015663
Iteration 615/1000 | Loss: 0.00017683
Iteration 616/1000 | Loss: 0.00015528
Iteration 617/1000 | Loss: 0.00014728
Iteration 618/1000 | Loss: 0.00017084
Iteration 619/1000 | Loss: 0.00016802
Iteration 620/1000 | Loss: 0.00016340
Iteration 621/1000 | Loss: 0.00016480
Iteration 622/1000 | Loss: 0.00016389
Iteration 623/1000 | Loss: 0.00016131
Iteration 624/1000 | Loss: 0.00015572
Iteration 625/1000 | Loss: 0.00016693
Iteration 626/1000 | Loss: 0.00016779
Iteration 627/1000 | Loss: 0.00017721
Iteration 628/1000 | Loss: 0.00017235
Iteration 629/1000 | Loss: 0.00016671
Iteration 630/1000 | Loss: 0.00014365
Iteration 631/1000 | Loss: 0.00014022
Iteration 632/1000 | Loss: 0.00014793
Iteration 633/1000 | Loss: 0.00013709
Iteration 634/1000 | Loss: 0.00012002
Iteration 635/1000 | Loss: 0.00014607
Iteration 636/1000 | Loss: 0.00017005
Iteration 637/1000 | Loss: 0.00014264
Iteration 638/1000 | Loss: 0.00013933
Iteration 639/1000 | Loss: 0.00013399
Iteration 640/1000 | Loss: 0.00015462
Iteration 641/1000 | Loss: 0.00013396
Iteration 642/1000 | Loss: 0.00015816
Iteration 643/1000 | Loss: 0.00017658
Iteration 644/1000 | Loss: 0.00016928
Iteration 645/1000 | Loss: 0.00014925
Iteration 646/1000 | Loss: 0.00016083
Iteration 647/1000 | Loss: 0.00017856
Iteration 648/1000 | Loss: 0.00017452
Iteration 649/1000 | Loss: 0.00017641
Iteration 650/1000 | Loss: 0.00018544
Iteration 651/1000 | Loss: 0.00018127
Iteration 652/1000 | Loss: 0.00018837
Iteration 653/1000 | Loss: 0.00018774
Iteration 654/1000 | Loss: 0.00012618
Iteration 655/1000 | Loss: 0.00015985
Iteration 656/1000 | Loss: 0.00016472
Iteration 657/1000 | Loss: 0.00017542
Iteration 658/1000 | Loss: 0.00013934
Iteration 659/1000 | Loss: 0.00015356
Iteration 660/1000 | Loss: 0.00017015
Iteration 661/1000 | Loss: 0.00017784
Iteration 662/1000 | Loss: 0.00018866
Iteration 663/1000 | Loss: 0.00017769
Iteration 664/1000 | Loss: 0.00018432
Iteration 665/1000 | Loss: 0.00017188
Iteration 666/1000 | Loss: 0.00014286
Iteration 667/1000 | Loss: 0.00015242
Iteration 668/1000 | Loss: 0.00014631
Iteration 669/1000 | Loss: 0.00016901
Iteration 670/1000 | Loss: 0.00014420
Iteration 671/1000 | Loss: 0.00014623
Iteration 672/1000 | Loss: 0.00012646
Iteration 673/1000 | Loss: 0.00017362
Iteration 674/1000 | Loss: 0.00018970
Iteration 675/1000 | Loss: 0.00018218
Iteration 676/1000 | Loss: 0.00016740
Iteration 677/1000 | Loss: 0.00012091
Iteration 678/1000 | Loss: 0.00015567
Iteration 679/1000 | Loss: 0.00016066
Iteration 680/1000 | Loss: 0.00016347
Iteration 681/1000 | Loss: 0.00015688
Iteration 682/1000 | Loss: 0.00016522
Iteration 683/1000 | Loss: 0.00016073
Iteration 684/1000 | Loss: 0.00015169
Iteration 685/1000 | Loss: 0.00016480
Iteration 686/1000 | Loss: 0.00015657
Iteration 687/1000 | Loss: 0.00015967
Iteration 688/1000 | Loss: 0.00016664
Iteration 689/1000 | Loss: 0.00015863
Iteration 690/1000 | Loss: 0.00015945
Iteration 691/1000 | Loss: 0.00016467
Iteration 692/1000 | Loss: 0.00015365
Iteration 693/1000 | Loss: 0.00016200
Iteration 694/1000 | Loss: 0.00016996
Iteration 695/1000 | Loss: 0.00016034
Iteration 696/1000 | Loss: 0.00015961
Iteration 697/1000 | Loss: 0.00016149
Iteration 698/1000 | Loss: 0.00016078
Iteration 699/1000 | Loss: 0.00015803
Iteration 700/1000 | Loss: 0.00014024
Iteration 701/1000 | Loss: 0.00012956
Iteration 702/1000 | Loss: 0.00016224
Iteration 703/1000 | Loss: 0.00016053
Iteration 704/1000 | Loss: 0.00015788
Iteration 705/1000 | Loss: 0.00015858
Iteration 706/1000 | Loss: 0.00017142
Iteration 707/1000 | Loss: 0.00015766
Iteration 708/1000 | Loss: 0.00016817
Iteration 709/1000 | Loss: 0.00015811
Iteration 710/1000 | Loss: 0.00014946
Iteration 711/1000 | Loss: 0.00016429
Iteration 712/1000 | Loss: 0.00016428
Iteration 713/1000 | Loss: 0.00015931
Iteration 714/1000 | Loss: 0.00016402
Iteration 715/1000 | Loss: 0.00013136
Iteration 716/1000 | Loss: 0.00012650
Iteration 717/1000 | Loss: 0.00012951
Iteration 718/1000 | Loss: 0.00019371
Iteration 719/1000 | Loss: 0.00016088
Iteration 720/1000 | Loss: 0.00015113
Iteration 721/1000 | Loss: 0.00014779
Iteration 722/1000 | Loss: 0.00014853
Iteration 723/1000 | Loss: 0.00015594
Iteration 724/1000 | Loss: 0.00016291
Iteration 725/1000 | Loss: 0.00016415
Iteration 726/1000 | Loss: 0.00017025
Iteration 727/1000 | Loss: 0.00014815
Iteration 728/1000 | Loss: 0.00014230
Iteration 729/1000 | Loss: 0.00014530
Iteration 730/1000 | Loss: 0.00017146
Iteration 731/1000 | Loss: 0.00015693
Iteration 732/1000 | Loss: 0.00017878
Iteration 733/1000 | Loss: 0.00015103
Iteration 734/1000 | Loss: 0.00014225
Iteration 735/1000 | Loss: 0.00014580
Iteration 736/1000 | Loss: 0.00014977
Iteration 737/1000 | Loss: 0.00014664
Iteration 738/1000 | Loss: 0.00016963
Iteration 739/1000 | Loss: 0.00016511
Iteration 740/1000 | Loss: 0.00016309
Iteration 741/1000 | Loss: 0.00016040
Iteration 742/1000 | Loss: 0.00016221
Iteration 743/1000 | Loss: 0.00014425
Iteration 744/1000 | Loss: 0.00016795
Iteration 745/1000 | Loss: 0.00014041
Iteration 746/1000 | Loss: 0.00014283
Iteration 747/1000 | Loss: 0.00013367
Iteration 748/1000 | Loss: 0.00011607
Iteration 749/1000 | Loss: 0.00015143
Iteration 750/1000 | Loss: 0.00014805
Iteration 751/1000 | Loss: 0.00014834
Iteration 752/1000 | Loss: 0.00014515
Iteration 753/1000 | Loss: 0.00014404
Iteration 754/1000 | Loss: 0.00014523
Iteration 755/1000 | Loss: 0.00015343
Iteration 756/1000 | Loss: 0.00015246
Iteration 757/1000 | Loss: 0.00015483
Iteration 758/1000 | Loss: 0.00016836
Iteration 759/1000 | Loss: 0.00016829
Iteration 760/1000 | Loss: 0.00014407
Iteration 761/1000 | Loss: 0.00014920
Iteration 762/1000 | Loss: 0.00016768
Iteration 763/1000 | Loss: 0.00013113
Iteration 764/1000 | Loss: 0.00014502
Iteration 765/1000 | Loss: 0.00013733
Iteration 766/1000 | Loss: 0.00014122
Iteration 767/1000 | Loss: 0.00016729
Iteration 768/1000 | Loss: 0.00014295
Iteration 769/1000 | Loss: 0.00018635
Iteration 770/1000 | Loss: 0.00014559
Iteration 771/1000 | Loss: 0.00012428
Iteration 772/1000 | Loss: 0.00012375
Iteration 773/1000 | Loss: 0.00011285
Iteration 774/1000 | Loss: 0.00014628
Iteration 775/1000 | Loss: 0.00013403
Iteration 776/1000 | Loss: 0.00012941
Iteration 777/1000 | Loss: 0.00012330
Iteration 778/1000 | Loss: 0.00012667
Iteration 779/1000 | Loss: 0.00012215
Iteration 780/1000 | Loss: 0.00011382
Iteration 781/1000 | Loss: 0.00011869
Iteration 782/1000 | Loss: 0.00015154
Iteration 783/1000 | Loss: 0.00013063
Iteration 784/1000 | Loss: 0.00011818
Iteration 785/1000 | Loss: 0.00009642
Iteration 786/1000 | Loss: 0.00009554
Iteration 787/1000 | Loss: 0.00009355
Iteration 788/1000 | Loss: 0.00009219
Iteration 789/1000 | Loss: 0.00009637
Iteration 790/1000 | Loss: 0.00009175
Iteration 791/1000 | Loss: 0.00009350
Iteration 792/1000 | Loss: 0.00009194
Iteration 793/1000 | Loss: 0.00009358
Iteration 794/1000 | Loss: 0.00009516
Iteration 795/1000 | Loss: 0.00009736
Iteration 796/1000 | Loss: 0.00008622
Iteration 797/1000 | Loss: 0.00009050
Iteration 798/1000 | Loss: 0.00008855
Iteration 799/1000 | Loss: 0.00008692
Iteration 800/1000 | Loss: 0.00009151
Iteration 801/1000 | Loss: 0.00009422
Iteration 802/1000 | Loss: 0.00009892
Iteration 803/1000 | Loss: 0.00009963
Iteration 804/1000 | Loss: 0.00008916
Iteration 805/1000 | Loss: 0.00009445
Iteration 806/1000 | Loss: 0.00009195
Iteration 807/1000 | Loss: 0.00008383
Iteration 808/1000 | Loss: 0.00008830
Iteration 809/1000 | Loss: 0.00009264
Iteration 810/1000 | Loss: 0.00008815
Iteration 811/1000 | Loss: 0.00009340
Iteration 812/1000 | Loss: 0.00008792
Iteration 813/1000 | Loss: 0.00009525
Iteration 814/1000 | Loss: 0.00009299
Iteration 815/1000 | Loss: 0.00008987
Iteration 816/1000 | Loss: 0.00009259
Iteration 817/1000 | Loss: 0.00009030
Iteration 818/1000 | Loss: 0.00009171
Iteration 819/1000 | Loss: 0.00009148
Iteration 820/1000 | Loss: 0.00009183
Iteration 821/1000 | Loss: 0.00009218
Iteration 822/1000 | Loss: 0.00008710
Iteration 823/1000 | Loss: 0.00008441
Iteration 824/1000 | Loss: 0.00008349
Iteration 825/1000 | Loss: 0.00008278
Iteration 826/1000 | Loss: 0.00008243
Iteration 827/1000 | Loss: 0.00008225
Iteration 828/1000 | Loss: 0.00008224
Iteration 829/1000 | Loss: 0.00008216
Iteration 830/1000 | Loss: 0.00008214
Iteration 831/1000 | Loss: 0.00008213
Iteration 832/1000 | Loss: 0.00008212
Iteration 833/1000 | Loss: 0.00008212
Iteration 834/1000 | Loss: 0.00008211
Iteration 835/1000 | Loss: 0.00008211
Iteration 836/1000 | Loss: 0.00008208
Iteration 837/1000 | Loss: 0.00008197
Iteration 838/1000 | Loss: 0.00008194
Iteration 839/1000 | Loss: 0.00008191
Iteration 840/1000 | Loss: 0.00008189
Iteration 841/1000 | Loss: 0.00008187
Iteration 842/1000 | Loss: 0.00008186
Iteration 843/1000 | Loss: 0.00008186
Iteration 844/1000 | Loss: 0.00008186
Iteration 845/1000 | Loss: 0.00008185
Iteration 846/1000 | Loss: 0.00008184
Iteration 847/1000 | Loss: 0.00008184
Iteration 848/1000 | Loss: 0.00008184
Iteration 849/1000 | Loss: 0.00008184
Iteration 850/1000 | Loss: 0.00008184
Iteration 851/1000 | Loss: 0.00008183
Iteration 852/1000 | Loss: 0.00008183
Iteration 853/1000 | Loss: 0.00008183
Iteration 854/1000 | Loss: 0.00008183
Iteration 855/1000 | Loss: 0.00008183
Iteration 856/1000 | Loss: 0.00008183
Iteration 857/1000 | Loss: 0.00008183
Iteration 858/1000 | Loss: 0.00008183
Iteration 859/1000 | Loss: 0.00008183
Iteration 860/1000 | Loss: 0.00008183
Iteration 861/1000 | Loss: 0.00008183
Iteration 862/1000 | Loss: 0.00008183
Iteration 863/1000 | Loss: 0.00008183
Iteration 864/1000 | Loss: 0.00008183
Iteration 865/1000 | Loss: 0.00008183
Iteration 866/1000 | Loss: 0.00008183
Iteration 867/1000 | Loss: 0.00008183
Iteration 868/1000 | Loss: 0.00008183
Iteration 869/1000 | Loss: 0.00008183
Iteration 870/1000 | Loss: 0.00008183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 870. Stopping optimization.
Last 5 losses: [8.182994497474283e-05, 8.182994497474283e-05, 8.182994497474283e-05, 8.182994497474283e-05, 8.182994497474283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.182994497474283e-05

Optimization complete. Final v2v error: 6.932333469390869 mm

Highest mean error: 8.332733154296875 mm for frame 101

Lowest mean error: 4.78074312210083 mm for frame 60

Saving results

Total time: 1413.4745798110962
