Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=152, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8512-8567
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935019
Iteration 2/25 | Loss: 0.00190186
Iteration 3/25 | Loss: 0.00166918
Iteration 4/25 | Loss: 0.00159679
Iteration 5/25 | Loss: 0.00160366
Iteration 6/25 | Loss: 0.00155690
Iteration 7/25 | Loss: 0.00154296
Iteration 8/25 | Loss: 0.00154395
Iteration 9/25 | Loss: 0.00154071
Iteration 10/25 | Loss: 0.00153688
Iteration 11/25 | Loss: 0.00153366
Iteration 12/25 | Loss: 0.00152931
Iteration 13/25 | Loss: 0.00152820
Iteration 14/25 | Loss: 0.00153412
Iteration 15/25 | Loss: 0.00154256
Iteration 16/25 | Loss: 0.00153291
Iteration 17/25 | Loss: 0.00152642
Iteration 18/25 | Loss: 0.00152618
Iteration 19/25 | Loss: 0.00152626
Iteration 20/25 | Loss: 0.00152542
Iteration 21/25 | Loss: 0.00152549
Iteration 22/25 | Loss: 0.00152967
Iteration 23/25 | Loss: 0.00152821
Iteration 24/25 | Loss: 0.00152504
Iteration 25/25 | Loss: 0.00152098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94580114
Iteration 2/25 | Loss: 0.00288131
Iteration 3/25 | Loss: 0.00233180
Iteration 4/25 | Loss: 0.00233180
Iteration 5/25 | Loss: 0.00233180
Iteration 6/25 | Loss: 0.00233180
Iteration 7/25 | Loss: 0.00233180
Iteration 8/25 | Loss: 0.00233180
Iteration 9/25 | Loss: 0.00233180
Iteration 10/25 | Loss: 0.00233180
Iteration 11/25 | Loss: 0.00233180
Iteration 12/25 | Loss: 0.00233180
Iteration 13/25 | Loss: 0.00233180
Iteration 14/25 | Loss: 0.00233180
Iteration 15/25 | Loss: 0.00233180
Iteration 16/25 | Loss: 0.00233180
Iteration 17/25 | Loss: 0.00233180
Iteration 18/25 | Loss: 0.00233180
Iteration 19/25 | Loss: 0.00233180
Iteration 20/25 | Loss: 0.00233180
Iteration 21/25 | Loss: 0.00233180
Iteration 22/25 | Loss: 0.00233180
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0023317974992096424, 0.0023317974992096424, 0.0023317974992096424, 0.0023317974992096424, 0.0023317974992096424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023317974992096424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233180
Iteration 2/1000 | Loss: 0.00046626
Iteration 3/1000 | Loss: 0.00246720
Iteration 4/1000 | Loss: 0.00149714
Iteration 5/1000 | Loss: 0.00159949
Iteration 6/1000 | Loss: 0.00029407
Iteration 7/1000 | Loss: 0.00095594
Iteration 8/1000 | Loss: 0.00123722
Iteration 9/1000 | Loss: 0.00078794
Iteration 10/1000 | Loss: 0.00020476
Iteration 11/1000 | Loss: 0.00009676
Iteration 12/1000 | Loss: 0.00014476
Iteration 13/1000 | Loss: 0.00007560
Iteration 14/1000 | Loss: 0.00025715
Iteration 15/1000 | Loss: 0.00006969
Iteration 16/1000 | Loss: 0.00006330
Iteration 17/1000 | Loss: 0.00054440
Iteration 18/1000 | Loss: 0.00070628
Iteration 19/1000 | Loss: 0.00037042
Iteration 20/1000 | Loss: 0.00072661
Iteration 21/1000 | Loss: 0.00037460
Iteration 22/1000 | Loss: 0.00047651
Iteration 23/1000 | Loss: 0.00006234
Iteration 24/1000 | Loss: 0.00005310
Iteration 25/1000 | Loss: 0.00005083
Iteration 26/1000 | Loss: 0.00024715
Iteration 27/1000 | Loss: 0.00041997
Iteration 28/1000 | Loss: 0.00005231
Iteration 29/1000 | Loss: 0.00004637
Iteration 30/1000 | Loss: 0.00004369
Iteration 31/1000 | Loss: 0.00004255
Iteration 32/1000 | Loss: 0.00004178
Iteration 33/1000 | Loss: 0.00004111
Iteration 34/1000 | Loss: 0.00029286
Iteration 35/1000 | Loss: 0.00034114
Iteration 36/1000 | Loss: 0.00014273
Iteration 37/1000 | Loss: 0.00011068
Iteration 38/1000 | Loss: 0.00008221
Iteration 39/1000 | Loss: 0.00006758
Iteration 40/1000 | Loss: 0.00004087
Iteration 41/1000 | Loss: 0.00003602
Iteration 42/1000 | Loss: 0.00003539
Iteration 43/1000 | Loss: 0.00004247
Iteration 44/1000 | Loss: 0.00003938
Iteration 45/1000 | Loss: 0.00003985
Iteration 46/1000 | Loss: 0.00003671
Iteration 47/1000 | Loss: 0.00003410
Iteration 48/1000 | Loss: 0.00003404
Iteration 49/1000 | Loss: 0.00003392
Iteration 50/1000 | Loss: 0.00003372
Iteration 51/1000 | Loss: 0.00003351
Iteration 52/1000 | Loss: 0.00003330
Iteration 53/1000 | Loss: 0.00003319
Iteration 54/1000 | Loss: 0.00003314
Iteration 55/1000 | Loss: 0.00003308
Iteration 56/1000 | Loss: 0.00003308
Iteration 57/1000 | Loss: 0.00003307
Iteration 58/1000 | Loss: 0.00003307
Iteration 59/1000 | Loss: 0.00003306
Iteration 60/1000 | Loss: 0.00003306
Iteration 61/1000 | Loss: 0.00003304
Iteration 62/1000 | Loss: 0.00003303
Iteration 63/1000 | Loss: 0.00003303
Iteration 64/1000 | Loss: 0.00003301
Iteration 65/1000 | Loss: 0.00003301
Iteration 66/1000 | Loss: 0.00003300
Iteration 67/1000 | Loss: 0.00003300
Iteration 68/1000 | Loss: 0.00003299
Iteration 69/1000 | Loss: 0.00003299
Iteration 70/1000 | Loss: 0.00003296
Iteration 71/1000 | Loss: 0.00003296
Iteration 72/1000 | Loss: 0.00003290
Iteration 73/1000 | Loss: 0.00015119
Iteration 74/1000 | Loss: 0.00027062
Iteration 75/1000 | Loss: 0.00021347
Iteration 76/1000 | Loss: 0.00017597
Iteration 77/1000 | Loss: 0.00014917
Iteration 78/1000 | Loss: 0.00003730
Iteration 79/1000 | Loss: 0.00005892
Iteration 80/1000 | Loss: 0.00003336
Iteration 81/1000 | Loss: 0.00013571
Iteration 82/1000 | Loss: 0.00004868
Iteration 83/1000 | Loss: 0.00003336
Iteration 84/1000 | Loss: 0.00013150
Iteration 85/1000 | Loss: 0.00004778
Iteration 86/1000 | Loss: 0.00003397
Iteration 87/1000 | Loss: 0.00013240
Iteration 88/1000 | Loss: 0.00023743
Iteration 89/1000 | Loss: 0.00003566
Iteration 90/1000 | Loss: 0.00003400
Iteration 91/1000 | Loss: 0.00003335
Iteration 92/1000 | Loss: 0.00003276
Iteration 93/1000 | Loss: 0.00003231
Iteration 94/1000 | Loss: 0.00003210
Iteration 95/1000 | Loss: 0.00003184
Iteration 96/1000 | Loss: 0.00003183
Iteration 97/1000 | Loss: 0.00003180
Iteration 98/1000 | Loss: 0.00003180
Iteration 99/1000 | Loss: 0.00003176
Iteration 100/1000 | Loss: 0.00003160
Iteration 101/1000 | Loss: 0.00003155
Iteration 102/1000 | Loss: 0.00003154
Iteration 103/1000 | Loss: 0.00003154
Iteration 104/1000 | Loss: 0.00003150
Iteration 105/1000 | Loss: 0.00003150
Iteration 106/1000 | Loss: 0.00003149
Iteration 107/1000 | Loss: 0.00003149
Iteration 108/1000 | Loss: 0.00003148
Iteration 109/1000 | Loss: 0.00003148
Iteration 110/1000 | Loss: 0.00003147
Iteration 111/1000 | Loss: 0.00003147
Iteration 112/1000 | Loss: 0.00003146
Iteration 113/1000 | Loss: 0.00003146
Iteration 114/1000 | Loss: 0.00003146
Iteration 115/1000 | Loss: 0.00003145
Iteration 116/1000 | Loss: 0.00003143
Iteration 117/1000 | Loss: 0.00003135
Iteration 118/1000 | Loss: 0.00003135
Iteration 119/1000 | Loss: 0.00003134
Iteration 120/1000 | Loss: 0.00003129
Iteration 121/1000 | Loss: 0.00003128
Iteration 122/1000 | Loss: 0.00003128
Iteration 123/1000 | Loss: 0.00003127
Iteration 124/1000 | Loss: 0.00003127
Iteration 125/1000 | Loss: 0.00003126
Iteration 126/1000 | Loss: 0.00003125
Iteration 127/1000 | Loss: 0.00003125
Iteration 128/1000 | Loss: 0.00003124
Iteration 129/1000 | Loss: 0.00003123
Iteration 130/1000 | Loss: 0.00003123
Iteration 131/1000 | Loss: 0.00003122
Iteration 132/1000 | Loss: 0.00003122
Iteration 133/1000 | Loss: 0.00003122
Iteration 134/1000 | Loss: 0.00003121
Iteration 135/1000 | Loss: 0.00003121
Iteration 136/1000 | Loss: 0.00003121
Iteration 137/1000 | Loss: 0.00003121
Iteration 138/1000 | Loss: 0.00003121
Iteration 139/1000 | Loss: 0.00003121
Iteration 140/1000 | Loss: 0.00003120
Iteration 141/1000 | Loss: 0.00003120
Iteration 142/1000 | Loss: 0.00003119
Iteration 143/1000 | Loss: 0.00003119
Iteration 144/1000 | Loss: 0.00003119
Iteration 145/1000 | Loss: 0.00003119
Iteration 146/1000 | Loss: 0.00003119
Iteration 147/1000 | Loss: 0.00003119
Iteration 148/1000 | Loss: 0.00003118
Iteration 149/1000 | Loss: 0.00003118
Iteration 150/1000 | Loss: 0.00003118
Iteration 151/1000 | Loss: 0.00003118
Iteration 152/1000 | Loss: 0.00003118
Iteration 153/1000 | Loss: 0.00003118
Iteration 154/1000 | Loss: 0.00003118
Iteration 155/1000 | Loss: 0.00003118
Iteration 156/1000 | Loss: 0.00003118
Iteration 157/1000 | Loss: 0.00003118
Iteration 158/1000 | Loss: 0.00003118
Iteration 159/1000 | Loss: 0.00003118
Iteration 160/1000 | Loss: 0.00003117
Iteration 161/1000 | Loss: 0.00003117
Iteration 162/1000 | Loss: 0.00003117
Iteration 163/1000 | Loss: 0.00003117
Iteration 164/1000 | Loss: 0.00003117
Iteration 165/1000 | Loss: 0.00003117
Iteration 166/1000 | Loss: 0.00003117
Iteration 167/1000 | Loss: 0.00003117
Iteration 168/1000 | Loss: 0.00003117
Iteration 169/1000 | Loss: 0.00003117
Iteration 170/1000 | Loss: 0.00003117
Iteration 171/1000 | Loss: 0.00003117
Iteration 172/1000 | Loss: 0.00003117
Iteration 173/1000 | Loss: 0.00003117
Iteration 174/1000 | Loss: 0.00003117
Iteration 175/1000 | Loss: 0.00003117
Iteration 176/1000 | Loss: 0.00003116
Iteration 177/1000 | Loss: 0.00003116
Iteration 178/1000 | Loss: 0.00003116
Iteration 179/1000 | Loss: 0.00003116
Iteration 180/1000 | Loss: 0.00003116
Iteration 181/1000 | Loss: 0.00003116
Iteration 182/1000 | Loss: 0.00003116
Iteration 183/1000 | Loss: 0.00003116
Iteration 184/1000 | Loss: 0.00003116
Iteration 185/1000 | Loss: 0.00003116
Iteration 186/1000 | Loss: 0.00003116
Iteration 187/1000 | Loss: 0.00003116
Iteration 188/1000 | Loss: 0.00003116
Iteration 189/1000 | Loss: 0.00003116
Iteration 190/1000 | Loss: 0.00003116
Iteration 191/1000 | Loss: 0.00003116
Iteration 192/1000 | Loss: 0.00003116
Iteration 193/1000 | Loss: 0.00003116
Iteration 194/1000 | Loss: 0.00003115
Iteration 195/1000 | Loss: 0.00003115
Iteration 196/1000 | Loss: 0.00003115
Iteration 197/1000 | Loss: 0.00003115
Iteration 198/1000 | Loss: 0.00003115
Iteration 199/1000 | Loss: 0.00003115
Iteration 200/1000 | Loss: 0.00003115
Iteration 201/1000 | Loss: 0.00003115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [3.115466461167671e-05, 3.115466461167671e-05, 3.115466461167671e-05, 3.115466461167671e-05, 3.115466461167671e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.115466461167671e-05

Optimization complete. Final v2v error: 4.602547645568848 mm

Highest mean error: 5.511102199554443 mm for frame 9

Lowest mean error: 3.574685573577881 mm for frame 208

Saving results

Total time: 193.24069046974182
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999089
Iteration 2/25 | Loss: 0.00999089
Iteration 3/25 | Loss: 0.00999089
Iteration 4/25 | Loss: 0.00345691
Iteration 5/25 | Loss: 0.00206435
Iteration 6/25 | Loss: 0.00191354
Iteration 7/25 | Loss: 0.00183664
Iteration 8/25 | Loss: 0.00177836
Iteration 9/25 | Loss: 0.00171155
Iteration 10/25 | Loss: 0.00168001
Iteration 11/25 | Loss: 0.00166147
Iteration 12/25 | Loss: 0.00163680
Iteration 13/25 | Loss: 0.00162533
Iteration 14/25 | Loss: 0.00161056
Iteration 15/25 | Loss: 0.00159884
Iteration 16/25 | Loss: 0.00158651
Iteration 17/25 | Loss: 0.00157655
Iteration 18/25 | Loss: 0.00157872
Iteration 19/25 | Loss: 0.00156922
Iteration 20/25 | Loss: 0.00156922
Iteration 21/25 | Loss: 0.00157678
Iteration 22/25 | Loss: 0.00158130
Iteration 23/25 | Loss: 0.00156225
Iteration 24/25 | Loss: 0.00155883
Iteration 25/25 | Loss: 0.00155086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43426037
Iteration 2/25 | Loss: 0.00553346
Iteration 3/25 | Loss: 0.00352726
Iteration 4/25 | Loss: 0.00352726
Iteration 5/25 | Loss: 0.00352726
Iteration 6/25 | Loss: 0.00352725
Iteration 7/25 | Loss: 0.00352725
Iteration 8/25 | Loss: 0.00352725
Iteration 9/25 | Loss: 0.00352725
Iteration 10/25 | Loss: 0.00352725
Iteration 11/25 | Loss: 0.00352725
Iteration 12/25 | Loss: 0.00352725
Iteration 13/25 | Loss: 0.00352725
Iteration 14/25 | Loss: 0.00352725
Iteration 15/25 | Loss: 0.00352725
Iteration 16/25 | Loss: 0.00352725
Iteration 17/25 | Loss: 0.00352725
Iteration 18/25 | Loss: 0.00352725
Iteration 19/25 | Loss: 0.00352725
Iteration 20/25 | Loss: 0.00352725
Iteration 21/25 | Loss: 0.00352725
Iteration 22/25 | Loss: 0.00352725
Iteration 23/25 | Loss: 0.00352725
Iteration 24/25 | Loss: 0.00352725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.003527252236381173, 0.003527252236381173, 0.003527252236381173, 0.003527252236381173, 0.003527252236381173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003527252236381173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00352725
Iteration 2/1000 | Loss: 0.00242882
Iteration 3/1000 | Loss: 0.00152659
Iteration 4/1000 | Loss: 0.00169994
Iteration 5/1000 | Loss: 0.00078623
Iteration 6/1000 | Loss: 0.00037912
Iteration 7/1000 | Loss: 0.00029306
Iteration 8/1000 | Loss: 0.00132450
Iteration 9/1000 | Loss: 0.00089686
Iteration 10/1000 | Loss: 0.00167632
Iteration 11/1000 | Loss: 0.00037573
Iteration 12/1000 | Loss: 0.00042941
Iteration 13/1000 | Loss: 0.00096175
Iteration 14/1000 | Loss: 0.00027486
Iteration 15/1000 | Loss: 0.00132085
Iteration 16/1000 | Loss: 0.00133733
Iteration 17/1000 | Loss: 0.00066859
Iteration 18/1000 | Loss: 0.00018732
Iteration 19/1000 | Loss: 0.00021068
Iteration 20/1000 | Loss: 0.00022218
Iteration 21/1000 | Loss: 0.00058938
Iteration 22/1000 | Loss: 0.00380641
Iteration 23/1000 | Loss: 0.00041069
Iteration 24/1000 | Loss: 0.00021660
Iteration 25/1000 | Loss: 0.00016596
Iteration 26/1000 | Loss: 0.00129684
Iteration 27/1000 | Loss: 0.00048732
Iteration 28/1000 | Loss: 0.00075067
Iteration 29/1000 | Loss: 0.00012400
Iteration 30/1000 | Loss: 0.00125962
Iteration 31/1000 | Loss: 0.00196790
Iteration 32/1000 | Loss: 0.00084790
Iteration 33/1000 | Loss: 0.00025068
Iteration 34/1000 | Loss: 0.00012335
Iteration 35/1000 | Loss: 0.00024781
Iteration 36/1000 | Loss: 0.00019715
Iteration 37/1000 | Loss: 0.00094510
Iteration 38/1000 | Loss: 0.00024635
Iteration 39/1000 | Loss: 0.00053287
Iteration 40/1000 | Loss: 0.00046199
Iteration 41/1000 | Loss: 0.00050376
Iteration 42/1000 | Loss: 0.00010760
Iteration 43/1000 | Loss: 0.00046976
Iteration 44/1000 | Loss: 0.00058762
Iteration 45/1000 | Loss: 0.00050032
Iteration 46/1000 | Loss: 0.00076349
Iteration 47/1000 | Loss: 0.00110105
Iteration 48/1000 | Loss: 0.00132753
Iteration 49/1000 | Loss: 0.00087307
Iteration 50/1000 | Loss: 0.00108279
Iteration 51/1000 | Loss: 0.00018271
Iteration 52/1000 | Loss: 0.00017283
Iteration 53/1000 | Loss: 0.00011664
Iteration 54/1000 | Loss: 0.00018887
Iteration 55/1000 | Loss: 0.00012979
Iteration 56/1000 | Loss: 0.00011462
Iteration 57/1000 | Loss: 0.00057048
Iteration 58/1000 | Loss: 0.00014370
Iteration 59/1000 | Loss: 0.00016857
Iteration 60/1000 | Loss: 0.00041370
Iteration 61/1000 | Loss: 0.00015091
Iteration 62/1000 | Loss: 0.00021443
Iteration 63/1000 | Loss: 0.00023300
Iteration 64/1000 | Loss: 0.00011585
Iteration 65/1000 | Loss: 0.00018739
Iteration 66/1000 | Loss: 0.00011379
Iteration 67/1000 | Loss: 0.00026957
Iteration 68/1000 | Loss: 0.00230577
Iteration 69/1000 | Loss: 0.00010676
Iteration 70/1000 | Loss: 0.00008199
Iteration 71/1000 | Loss: 0.00030793
Iteration 72/1000 | Loss: 0.00007307
Iteration 73/1000 | Loss: 0.00006984
Iteration 74/1000 | Loss: 0.00007164
Iteration 75/1000 | Loss: 0.00005415
Iteration 76/1000 | Loss: 0.00007292
Iteration 77/1000 | Loss: 0.00008163
Iteration 78/1000 | Loss: 0.00007374
Iteration 79/1000 | Loss: 0.00007096
Iteration 80/1000 | Loss: 0.00007658
Iteration 81/1000 | Loss: 0.00005957
Iteration 82/1000 | Loss: 0.00006881
Iteration 83/1000 | Loss: 0.00040243
Iteration 84/1000 | Loss: 0.00022819
Iteration 85/1000 | Loss: 0.00005578
Iteration 86/1000 | Loss: 0.00027741
Iteration 87/1000 | Loss: 0.00005523
Iteration 88/1000 | Loss: 0.00004940
Iteration 89/1000 | Loss: 0.00004778
Iteration 90/1000 | Loss: 0.00015180
Iteration 91/1000 | Loss: 0.00010593
Iteration 92/1000 | Loss: 0.00004781
Iteration 93/1000 | Loss: 0.00025738
Iteration 94/1000 | Loss: 0.00005873
Iteration 95/1000 | Loss: 0.00004477
Iteration 96/1000 | Loss: 0.00016416
Iteration 97/1000 | Loss: 0.00004949
Iteration 98/1000 | Loss: 0.00005244
Iteration 99/1000 | Loss: 0.00004372
Iteration 100/1000 | Loss: 0.00009831
Iteration 101/1000 | Loss: 0.00005622
Iteration 102/1000 | Loss: 0.00008250
Iteration 103/1000 | Loss: 0.00007102
Iteration 104/1000 | Loss: 0.00011549
Iteration 105/1000 | Loss: 0.00008993
Iteration 106/1000 | Loss: 0.00007010
Iteration 107/1000 | Loss: 0.00004974
Iteration 108/1000 | Loss: 0.00004243
Iteration 109/1000 | Loss: 0.00011987
Iteration 110/1000 | Loss: 0.00007615
Iteration 111/1000 | Loss: 0.00003960
Iteration 112/1000 | Loss: 0.00030508
Iteration 113/1000 | Loss: 0.00008165
Iteration 114/1000 | Loss: 0.00004838
Iteration 115/1000 | Loss: 0.00003916
Iteration 116/1000 | Loss: 0.00007580
Iteration 117/1000 | Loss: 0.00003797
Iteration 118/1000 | Loss: 0.00003760
Iteration 119/1000 | Loss: 0.00007988
Iteration 120/1000 | Loss: 0.00004265
Iteration 121/1000 | Loss: 0.00005392
Iteration 122/1000 | Loss: 0.00003723
Iteration 123/1000 | Loss: 0.00003715
Iteration 124/1000 | Loss: 0.00003712
Iteration 125/1000 | Loss: 0.00003712
Iteration 126/1000 | Loss: 0.00003711
Iteration 127/1000 | Loss: 0.00003710
Iteration 128/1000 | Loss: 0.00003710
Iteration 129/1000 | Loss: 0.00003710
Iteration 130/1000 | Loss: 0.00004777
Iteration 131/1000 | Loss: 0.00007822
Iteration 132/1000 | Loss: 0.00004480
Iteration 133/1000 | Loss: 0.00003704
Iteration 134/1000 | Loss: 0.00018753
Iteration 135/1000 | Loss: 0.00005420
Iteration 136/1000 | Loss: 0.00004530
Iteration 137/1000 | Loss: 0.00004284
Iteration 138/1000 | Loss: 0.00009580
Iteration 139/1000 | Loss: 0.00014009
Iteration 140/1000 | Loss: 0.00010118
Iteration 141/1000 | Loss: 0.00042230
Iteration 142/1000 | Loss: 0.00043631
Iteration 143/1000 | Loss: 0.00009131
Iteration 144/1000 | Loss: 0.00007467
Iteration 145/1000 | Loss: 0.00005976
Iteration 146/1000 | Loss: 0.00004821
Iteration 147/1000 | Loss: 0.00004253
Iteration 148/1000 | Loss: 0.00004466
Iteration 149/1000 | Loss: 0.00004013
Iteration 150/1000 | Loss: 0.00003994
Iteration 151/1000 | Loss: 0.00008558
Iteration 152/1000 | Loss: 0.00009664
Iteration 153/1000 | Loss: 0.00007613
Iteration 154/1000 | Loss: 0.00006684
Iteration 155/1000 | Loss: 0.00008075
Iteration 156/1000 | Loss: 0.00011712
Iteration 157/1000 | Loss: 0.00003893
Iteration 158/1000 | Loss: 0.00003805
Iteration 159/1000 | Loss: 0.00008530
Iteration 160/1000 | Loss: 0.00003670
Iteration 161/1000 | Loss: 0.00009481
Iteration 162/1000 | Loss: 0.00003776
Iteration 163/1000 | Loss: 0.00003715
Iteration 164/1000 | Loss: 0.00003578
Iteration 165/1000 | Loss: 0.00003575
Iteration 166/1000 | Loss: 0.00003565
Iteration 167/1000 | Loss: 0.00004353
Iteration 168/1000 | Loss: 0.00009811
Iteration 169/1000 | Loss: 0.00004344
Iteration 170/1000 | Loss: 0.00010284
Iteration 171/1000 | Loss: 0.00003699
Iteration 172/1000 | Loss: 0.00009738
Iteration 173/1000 | Loss: 0.00013655
Iteration 174/1000 | Loss: 0.00009739
Iteration 175/1000 | Loss: 0.00013834
Iteration 176/1000 | Loss: 0.00003982
Iteration 177/1000 | Loss: 0.00008909
Iteration 178/1000 | Loss: 0.00005347
Iteration 179/1000 | Loss: 0.00010374
Iteration 180/1000 | Loss: 0.00019661
Iteration 181/1000 | Loss: 0.00011324
Iteration 182/1000 | Loss: 0.00006748
Iteration 183/1000 | Loss: 0.00004227
Iteration 184/1000 | Loss: 0.00003641
Iteration 185/1000 | Loss: 0.00005259
Iteration 186/1000 | Loss: 0.00005745
Iteration 187/1000 | Loss: 0.00004906
Iteration 188/1000 | Loss: 0.00003353
Iteration 189/1000 | Loss: 0.00005069
Iteration 190/1000 | Loss: 0.00003277
Iteration 191/1000 | Loss: 0.00004976
Iteration 192/1000 | Loss: 0.00003235
Iteration 193/1000 | Loss: 0.00009741
Iteration 194/1000 | Loss: 0.00003849
Iteration 195/1000 | Loss: 0.00003212
Iteration 196/1000 | Loss: 0.00003196
Iteration 197/1000 | Loss: 0.00003195
Iteration 198/1000 | Loss: 0.00003697
Iteration 199/1000 | Loss: 0.00003697
Iteration 200/1000 | Loss: 0.00071160
Iteration 201/1000 | Loss: 0.00004121
Iteration 202/1000 | Loss: 0.00005876
Iteration 203/1000 | Loss: 0.00006648
Iteration 204/1000 | Loss: 0.00003487
Iteration 205/1000 | Loss: 0.00009069
Iteration 206/1000 | Loss: 0.00008906
Iteration 207/1000 | Loss: 0.00003694
Iteration 208/1000 | Loss: 0.00024819
Iteration 209/1000 | Loss: 0.00003063
Iteration 210/1000 | Loss: 0.00004414
Iteration 211/1000 | Loss: 0.00003020
Iteration 212/1000 | Loss: 0.00003005
Iteration 213/1000 | Loss: 0.00005063
Iteration 214/1000 | Loss: 0.00002994
Iteration 215/1000 | Loss: 0.00002992
Iteration 216/1000 | Loss: 0.00002991
Iteration 217/1000 | Loss: 0.00002990
Iteration 218/1000 | Loss: 0.00002990
Iteration 219/1000 | Loss: 0.00002990
Iteration 220/1000 | Loss: 0.00002989
Iteration 221/1000 | Loss: 0.00002989
Iteration 222/1000 | Loss: 0.00002989
Iteration 223/1000 | Loss: 0.00002988
Iteration 224/1000 | Loss: 0.00002988
Iteration 225/1000 | Loss: 0.00002988
Iteration 226/1000 | Loss: 0.00002987
Iteration 227/1000 | Loss: 0.00004644
Iteration 228/1000 | Loss: 0.00002986
Iteration 229/1000 | Loss: 0.00002983
Iteration 230/1000 | Loss: 0.00002983
Iteration 231/1000 | Loss: 0.00002982
Iteration 232/1000 | Loss: 0.00002980
Iteration 233/1000 | Loss: 0.00002980
Iteration 234/1000 | Loss: 0.00002980
Iteration 235/1000 | Loss: 0.00002980
Iteration 236/1000 | Loss: 0.00002980
Iteration 237/1000 | Loss: 0.00002980
Iteration 238/1000 | Loss: 0.00002980
Iteration 239/1000 | Loss: 0.00002980
Iteration 240/1000 | Loss: 0.00005597
Iteration 241/1000 | Loss: 0.00005597
Iteration 242/1000 | Loss: 0.00005416
Iteration 243/1000 | Loss: 0.00002979
Iteration 244/1000 | Loss: 0.00002979
Iteration 245/1000 | Loss: 0.00002979
Iteration 246/1000 | Loss: 0.00002979
Iteration 247/1000 | Loss: 0.00002979
Iteration 248/1000 | Loss: 0.00002979
Iteration 249/1000 | Loss: 0.00002979
Iteration 250/1000 | Loss: 0.00002979
Iteration 251/1000 | Loss: 0.00002979
Iteration 252/1000 | Loss: 0.00002979
Iteration 253/1000 | Loss: 0.00002978
Iteration 254/1000 | Loss: 0.00002978
Iteration 255/1000 | Loss: 0.00002978
Iteration 256/1000 | Loss: 0.00002978
Iteration 257/1000 | Loss: 0.00002978
Iteration 258/1000 | Loss: 0.00002978
Iteration 259/1000 | Loss: 0.00002978
Iteration 260/1000 | Loss: 0.00002978
Iteration 261/1000 | Loss: 0.00002978
Iteration 262/1000 | Loss: 0.00002978
Iteration 263/1000 | Loss: 0.00002977
Iteration 264/1000 | Loss: 0.00002977
Iteration 265/1000 | Loss: 0.00002976
Iteration 266/1000 | Loss: 0.00002975
Iteration 267/1000 | Loss: 0.00002975
Iteration 268/1000 | Loss: 0.00002975
Iteration 269/1000 | Loss: 0.00002974
Iteration 270/1000 | Loss: 0.00002974
Iteration 271/1000 | Loss: 0.00002974
Iteration 272/1000 | Loss: 0.00002973
Iteration 273/1000 | Loss: 0.00002973
Iteration 274/1000 | Loss: 0.00002973
Iteration 275/1000 | Loss: 0.00002973
Iteration 276/1000 | Loss: 0.00002972
Iteration 277/1000 | Loss: 0.00002972
Iteration 278/1000 | Loss: 0.00002971
Iteration 279/1000 | Loss: 0.00002971
Iteration 280/1000 | Loss: 0.00002971
Iteration 281/1000 | Loss: 0.00002971
Iteration 282/1000 | Loss: 0.00002971
Iteration 283/1000 | Loss: 0.00002971
Iteration 284/1000 | Loss: 0.00002971
Iteration 285/1000 | Loss: 0.00002970
Iteration 286/1000 | Loss: 0.00002970
Iteration 287/1000 | Loss: 0.00002970
Iteration 288/1000 | Loss: 0.00002970
Iteration 289/1000 | Loss: 0.00002970
Iteration 290/1000 | Loss: 0.00002970
Iteration 291/1000 | Loss: 0.00002970
Iteration 292/1000 | Loss: 0.00002969
Iteration 293/1000 | Loss: 0.00002969
Iteration 294/1000 | Loss: 0.00002969
Iteration 295/1000 | Loss: 0.00002969
Iteration 296/1000 | Loss: 0.00002969
Iteration 297/1000 | Loss: 0.00002969
Iteration 298/1000 | Loss: 0.00002969
Iteration 299/1000 | Loss: 0.00002969
Iteration 300/1000 | Loss: 0.00002968
Iteration 301/1000 | Loss: 0.00002968
Iteration 302/1000 | Loss: 0.00002968
Iteration 303/1000 | Loss: 0.00002968
Iteration 304/1000 | Loss: 0.00004213
Iteration 305/1000 | Loss: 0.00007634
Iteration 306/1000 | Loss: 0.00009881
Iteration 307/1000 | Loss: 0.00004170
Iteration 308/1000 | Loss: 0.00003387
Iteration 309/1000 | Loss: 0.00002974
Iteration 310/1000 | Loss: 0.00002965
Iteration 311/1000 | Loss: 0.00002965
Iteration 312/1000 | Loss: 0.00002964
Iteration 313/1000 | Loss: 0.00002964
Iteration 314/1000 | Loss: 0.00002963
Iteration 315/1000 | Loss: 0.00002963
Iteration 316/1000 | Loss: 0.00002962
Iteration 317/1000 | Loss: 0.00002962
Iteration 318/1000 | Loss: 0.00002962
Iteration 319/1000 | Loss: 0.00002962
Iteration 320/1000 | Loss: 0.00002962
Iteration 321/1000 | Loss: 0.00002962
Iteration 322/1000 | Loss: 0.00002961
Iteration 323/1000 | Loss: 0.00002961
Iteration 324/1000 | Loss: 0.00006680
Iteration 325/1000 | Loss: 0.00021297
Iteration 326/1000 | Loss: 0.00004367
Iteration 327/1000 | Loss: 0.00011704
Iteration 328/1000 | Loss: 0.00007406
Iteration 329/1000 | Loss: 0.00003809
Iteration 330/1000 | Loss: 0.00003482
Iteration 331/1000 | Loss: 0.00002965
Iteration 332/1000 | Loss: 0.00002965
Iteration 333/1000 | Loss: 0.00002964
Iteration 334/1000 | Loss: 0.00002964
Iteration 335/1000 | Loss: 0.00002963
Iteration 336/1000 | Loss: 0.00002963
Iteration 337/1000 | Loss: 0.00002963
Iteration 338/1000 | Loss: 0.00002962
Iteration 339/1000 | Loss: 0.00002962
Iteration 340/1000 | Loss: 0.00002962
Iteration 341/1000 | Loss: 0.00002961
Iteration 342/1000 | Loss: 0.00002961
Iteration 343/1000 | Loss: 0.00002961
Iteration 344/1000 | Loss: 0.00002961
Iteration 345/1000 | Loss: 0.00002960
Iteration 346/1000 | Loss: 0.00002960
Iteration 347/1000 | Loss: 0.00002960
Iteration 348/1000 | Loss: 0.00002959
Iteration 349/1000 | Loss: 0.00002959
Iteration 350/1000 | Loss: 0.00002959
Iteration 351/1000 | Loss: 0.00002959
Iteration 352/1000 | Loss: 0.00002959
Iteration 353/1000 | Loss: 0.00002959
Iteration 354/1000 | Loss: 0.00002959
Iteration 355/1000 | Loss: 0.00002959
Iteration 356/1000 | Loss: 0.00002959
Iteration 357/1000 | Loss: 0.00002959
Iteration 358/1000 | Loss: 0.00002959
Iteration 359/1000 | Loss: 0.00002959
Iteration 360/1000 | Loss: 0.00002959
Iteration 361/1000 | Loss: 0.00002959
Iteration 362/1000 | Loss: 0.00002959
Iteration 363/1000 | Loss: 0.00002959
Iteration 364/1000 | Loss: 0.00002959
Iteration 365/1000 | Loss: 0.00002959
Iteration 366/1000 | Loss: 0.00002959
Iteration 367/1000 | Loss: 0.00002959
Iteration 368/1000 | Loss: 0.00002959
Iteration 369/1000 | Loss: 0.00002959
Iteration 370/1000 | Loss: 0.00002959
Iteration 371/1000 | Loss: 0.00002959
Iteration 372/1000 | Loss: 0.00002959
Iteration 373/1000 | Loss: 0.00002959
Iteration 374/1000 | Loss: 0.00002959
Iteration 375/1000 | Loss: 0.00002959
Iteration 376/1000 | Loss: 0.00002959
Iteration 377/1000 | Loss: 0.00002959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 377. Stopping optimization.
Last 5 losses: [2.958889672299847e-05, 2.958889672299847e-05, 2.958889672299847e-05, 2.958889672299847e-05, 2.958889672299847e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.958889672299847e-05

Optimization complete. Final v2v error: 3.744459629058838 mm

Highest mean error: 10.926589965820312 mm for frame 144

Lowest mean error: 2.9706199169158936 mm for frame 227

Saving results

Total time: 410.6433656215668
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377416
Iteration 2/25 | Loss: 0.00136587
Iteration 3/25 | Loss: 0.00123046
Iteration 4/25 | Loss: 0.00121398
Iteration 5/25 | Loss: 0.00120945
Iteration 6/25 | Loss: 0.00120877
Iteration 7/25 | Loss: 0.00120877
Iteration 8/25 | Loss: 0.00120877
Iteration 9/25 | Loss: 0.00120877
Iteration 10/25 | Loss: 0.00120877
Iteration 11/25 | Loss: 0.00120877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001208769273944199, 0.001208769273944199, 0.001208769273944199, 0.001208769273944199, 0.001208769273944199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001208769273944199

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86410141
Iteration 2/25 | Loss: 0.00080608
Iteration 3/25 | Loss: 0.00080607
Iteration 4/25 | Loss: 0.00080607
Iteration 5/25 | Loss: 0.00080607
Iteration 6/25 | Loss: 0.00080607
Iteration 7/25 | Loss: 0.00080607
Iteration 8/25 | Loss: 0.00080607
Iteration 9/25 | Loss: 0.00080607
Iteration 10/25 | Loss: 0.00080607
Iteration 11/25 | Loss: 0.00080607
Iteration 12/25 | Loss: 0.00080607
Iteration 13/25 | Loss: 0.00080607
Iteration 14/25 | Loss: 0.00080607
Iteration 15/25 | Loss: 0.00080607
Iteration 16/25 | Loss: 0.00080607
Iteration 17/25 | Loss: 0.00080607
Iteration 18/25 | Loss: 0.00080607
Iteration 19/25 | Loss: 0.00080607
Iteration 20/25 | Loss: 0.00080607
Iteration 21/25 | Loss: 0.00080607
Iteration 22/25 | Loss: 0.00080607
Iteration 23/25 | Loss: 0.00080607
Iteration 24/25 | Loss: 0.00080607
Iteration 25/25 | Loss: 0.00080607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080607
Iteration 2/1000 | Loss: 0.00002716
Iteration 3/1000 | Loss: 0.00001920
Iteration 4/1000 | Loss: 0.00001746
Iteration 5/1000 | Loss: 0.00001643
Iteration 6/1000 | Loss: 0.00001546
Iteration 7/1000 | Loss: 0.00001485
Iteration 8/1000 | Loss: 0.00001455
Iteration 9/1000 | Loss: 0.00001417
Iteration 10/1000 | Loss: 0.00001383
Iteration 11/1000 | Loss: 0.00001359
Iteration 12/1000 | Loss: 0.00001340
Iteration 13/1000 | Loss: 0.00001322
Iteration 14/1000 | Loss: 0.00001320
Iteration 15/1000 | Loss: 0.00001320
Iteration 16/1000 | Loss: 0.00001317
Iteration 17/1000 | Loss: 0.00001317
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001313
Iteration 20/1000 | Loss: 0.00001313
Iteration 21/1000 | Loss: 0.00001312
Iteration 22/1000 | Loss: 0.00001311
Iteration 23/1000 | Loss: 0.00001311
Iteration 24/1000 | Loss: 0.00001310
Iteration 25/1000 | Loss: 0.00001310
Iteration 26/1000 | Loss: 0.00001308
Iteration 27/1000 | Loss: 0.00001308
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001300
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001298
Iteration 33/1000 | Loss: 0.00001298
Iteration 34/1000 | Loss: 0.00001297
Iteration 35/1000 | Loss: 0.00001297
Iteration 36/1000 | Loss: 0.00001297
Iteration 37/1000 | Loss: 0.00001296
Iteration 38/1000 | Loss: 0.00001296
Iteration 39/1000 | Loss: 0.00001295
Iteration 40/1000 | Loss: 0.00001295
Iteration 41/1000 | Loss: 0.00001294
Iteration 42/1000 | Loss: 0.00001294
Iteration 43/1000 | Loss: 0.00001294
Iteration 44/1000 | Loss: 0.00001294
Iteration 45/1000 | Loss: 0.00001294
Iteration 46/1000 | Loss: 0.00001294
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001293
Iteration 49/1000 | Loss: 0.00001293
Iteration 50/1000 | Loss: 0.00001293
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001292
Iteration 53/1000 | Loss: 0.00001292
Iteration 54/1000 | Loss: 0.00001292
Iteration 55/1000 | Loss: 0.00001291
Iteration 56/1000 | Loss: 0.00001291
Iteration 57/1000 | Loss: 0.00001291
Iteration 58/1000 | Loss: 0.00001291
Iteration 59/1000 | Loss: 0.00001291
Iteration 60/1000 | Loss: 0.00001291
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001290
Iteration 63/1000 | Loss: 0.00001290
Iteration 64/1000 | Loss: 0.00001290
Iteration 65/1000 | Loss: 0.00001290
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001290
Iteration 68/1000 | Loss: 0.00001290
Iteration 69/1000 | Loss: 0.00001290
Iteration 70/1000 | Loss: 0.00001290
Iteration 71/1000 | Loss: 0.00001289
Iteration 72/1000 | Loss: 0.00001289
Iteration 73/1000 | Loss: 0.00001289
Iteration 74/1000 | Loss: 0.00001289
Iteration 75/1000 | Loss: 0.00001289
Iteration 76/1000 | Loss: 0.00001288
Iteration 77/1000 | Loss: 0.00001288
Iteration 78/1000 | Loss: 0.00001288
Iteration 79/1000 | Loss: 0.00001288
Iteration 80/1000 | Loss: 0.00001288
Iteration 81/1000 | Loss: 0.00001288
Iteration 82/1000 | Loss: 0.00001287
Iteration 83/1000 | Loss: 0.00001287
Iteration 84/1000 | Loss: 0.00001287
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001286
Iteration 88/1000 | Loss: 0.00001286
Iteration 89/1000 | Loss: 0.00001285
Iteration 90/1000 | Loss: 0.00001285
Iteration 91/1000 | Loss: 0.00001285
Iteration 92/1000 | Loss: 0.00001284
Iteration 93/1000 | Loss: 0.00001284
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001283
Iteration 96/1000 | Loss: 0.00001283
Iteration 97/1000 | Loss: 0.00001283
Iteration 98/1000 | Loss: 0.00001282
Iteration 99/1000 | Loss: 0.00001282
Iteration 100/1000 | Loss: 0.00001281
Iteration 101/1000 | Loss: 0.00001281
Iteration 102/1000 | Loss: 0.00001281
Iteration 103/1000 | Loss: 0.00001281
Iteration 104/1000 | Loss: 0.00001280
Iteration 105/1000 | Loss: 0.00001280
Iteration 106/1000 | Loss: 0.00001280
Iteration 107/1000 | Loss: 0.00001279
Iteration 108/1000 | Loss: 0.00001279
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001278
Iteration 111/1000 | Loss: 0.00001278
Iteration 112/1000 | Loss: 0.00001278
Iteration 113/1000 | Loss: 0.00001278
Iteration 114/1000 | Loss: 0.00001278
Iteration 115/1000 | Loss: 0.00001277
Iteration 116/1000 | Loss: 0.00001277
Iteration 117/1000 | Loss: 0.00001277
Iteration 118/1000 | Loss: 0.00001277
Iteration 119/1000 | Loss: 0.00001277
Iteration 120/1000 | Loss: 0.00001277
Iteration 121/1000 | Loss: 0.00001277
Iteration 122/1000 | Loss: 0.00001277
Iteration 123/1000 | Loss: 0.00001277
Iteration 124/1000 | Loss: 0.00001277
Iteration 125/1000 | Loss: 0.00001276
Iteration 126/1000 | Loss: 0.00001276
Iteration 127/1000 | Loss: 0.00001276
Iteration 128/1000 | Loss: 0.00001276
Iteration 129/1000 | Loss: 0.00001276
Iteration 130/1000 | Loss: 0.00001276
Iteration 131/1000 | Loss: 0.00001276
Iteration 132/1000 | Loss: 0.00001276
Iteration 133/1000 | Loss: 0.00001276
Iteration 134/1000 | Loss: 0.00001275
Iteration 135/1000 | Loss: 0.00001275
Iteration 136/1000 | Loss: 0.00001275
Iteration 137/1000 | Loss: 0.00001275
Iteration 138/1000 | Loss: 0.00001275
Iteration 139/1000 | Loss: 0.00001275
Iteration 140/1000 | Loss: 0.00001275
Iteration 141/1000 | Loss: 0.00001275
Iteration 142/1000 | Loss: 0.00001275
Iteration 143/1000 | Loss: 0.00001275
Iteration 144/1000 | Loss: 0.00001275
Iteration 145/1000 | Loss: 0.00001275
Iteration 146/1000 | Loss: 0.00001275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.2753864211845212e-05, 1.2753864211845212e-05, 1.2753864211845212e-05, 1.2753864211845212e-05, 1.2753864211845212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2753864211845212e-05

Optimization complete. Final v2v error: 3.077044725418091 mm

Highest mean error: 3.5051138401031494 mm for frame 75

Lowest mean error: 2.6487948894500732 mm for frame 12

Saving results

Total time: 43.87676811218262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824294
Iteration 2/25 | Loss: 0.00236401
Iteration 3/25 | Loss: 0.00165939
Iteration 4/25 | Loss: 0.00152967
Iteration 5/25 | Loss: 0.00151169
Iteration 6/25 | Loss: 0.00146216
Iteration 7/25 | Loss: 0.00145210
Iteration 8/25 | Loss: 0.00145046
Iteration 9/25 | Loss: 0.00144906
Iteration 10/25 | Loss: 0.00144687
Iteration 11/25 | Loss: 0.00144395
Iteration 12/25 | Loss: 0.00144436
Iteration 13/25 | Loss: 0.00144418
Iteration 14/25 | Loss: 0.00144481
Iteration 15/25 | Loss: 0.00144462
Iteration 16/25 | Loss: 0.00144379
Iteration 17/25 | Loss: 0.00144397
Iteration 18/25 | Loss: 0.00144431
Iteration 19/25 | Loss: 0.00144402
Iteration 20/25 | Loss: 0.00144397
Iteration 21/25 | Loss: 0.00144379
Iteration 22/25 | Loss: 0.00144474
Iteration 23/25 | Loss: 0.00144412
Iteration 24/25 | Loss: 0.00144384
Iteration 25/25 | Loss: 0.00144417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27697504
Iteration 2/25 | Loss: 0.00093995
Iteration 3/25 | Loss: 0.00093992
Iteration 4/25 | Loss: 0.00093992
Iteration 5/25 | Loss: 0.00093992
Iteration 6/25 | Loss: 0.00093992
Iteration 7/25 | Loss: 0.00093992
Iteration 8/25 | Loss: 0.00093992
Iteration 9/25 | Loss: 0.00093992
Iteration 10/25 | Loss: 0.00093992
Iteration 11/25 | Loss: 0.00093992
Iteration 12/25 | Loss: 0.00093992
Iteration 13/25 | Loss: 0.00093992
Iteration 14/25 | Loss: 0.00093992
Iteration 15/25 | Loss: 0.00093992
Iteration 16/25 | Loss: 0.00093992
Iteration 17/25 | Loss: 0.00093992
Iteration 18/25 | Loss: 0.00093992
Iteration 19/25 | Loss: 0.00093992
Iteration 20/25 | Loss: 0.00093992
Iteration 21/25 | Loss: 0.00093992
Iteration 22/25 | Loss: 0.00093992
Iteration 23/25 | Loss: 0.00093992
Iteration 24/25 | Loss: 0.00093992
Iteration 25/25 | Loss: 0.00093992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093992
Iteration 2/1000 | Loss: 0.00006553
Iteration 3/1000 | Loss: 0.00004502
Iteration 4/1000 | Loss: 0.00003676
Iteration 5/1000 | Loss: 0.00004516
Iteration 6/1000 | Loss: 0.00003709
Iteration 7/1000 | Loss: 0.00003748
Iteration 8/1000 | Loss: 0.00004254
Iteration 9/1000 | Loss: 0.00004413
Iteration 10/1000 | Loss: 0.00004474
Iteration 11/1000 | Loss: 0.00004042
Iteration 12/1000 | Loss: 0.00004391
Iteration 13/1000 | Loss: 0.00003962
Iteration 14/1000 | Loss: 0.00004338
Iteration 15/1000 | Loss: 0.00004279
Iteration 16/1000 | Loss: 0.00004046
Iteration 17/1000 | Loss: 0.00004099
Iteration 18/1000 | Loss: 0.00003538
Iteration 19/1000 | Loss: 0.00003264
Iteration 20/1000 | Loss: 0.00003128
Iteration 21/1000 | Loss: 0.00003044
Iteration 22/1000 | Loss: 0.00002984
Iteration 23/1000 | Loss: 0.00002956
Iteration 24/1000 | Loss: 0.00002951
Iteration 25/1000 | Loss: 0.00002947
Iteration 26/1000 | Loss: 0.00002946
Iteration 27/1000 | Loss: 0.00002941
Iteration 28/1000 | Loss: 0.00002940
Iteration 29/1000 | Loss: 0.00002940
Iteration 30/1000 | Loss: 0.00002939
Iteration 31/1000 | Loss: 0.00002939
Iteration 32/1000 | Loss: 0.00002939
Iteration 33/1000 | Loss: 0.00002938
Iteration 34/1000 | Loss: 0.00002938
Iteration 35/1000 | Loss: 0.00002938
Iteration 36/1000 | Loss: 0.00002937
Iteration 37/1000 | Loss: 0.00002937
Iteration 38/1000 | Loss: 0.00002936
Iteration 39/1000 | Loss: 0.00002936
Iteration 40/1000 | Loss: 0.00002936
Iteration 41/1000 | Loss: 0.00002936
Iteration 42/1000 | Loss: 0.00002936
Iteration 43/1000 | Loss: 0.00002936
Iteration 44/1000 | Loss: 0.00002936
Iteration 45/1000 | Loss: 0.00002936
Iteration 46/1000 | Loss: 0.00002935
Iteration 47/1000 | Loss: 0.00002935
Iteration 48/1000 | Loss: 0.00002935
Iteration 49/1000 | Loss: 0.00002935
Iteration 50/1000 | Loss: 0.00002935
Iteration 51/1000 | Loss: 0.00002935
Iteration 52/1000 | Loss: 0.00002935
Iteration 53/1000 | Loss: 0.00002935
Iteration 54/1000 | Loss: 0.00002935
Iteration 55/1000 | Loss: 0.00002935
Iteration 56/1000 | Loss: 0.00002934
Iteration 57/1000 | Loss: 0.00002934
Iteration 58/1000 | Loss: 0.00002934
Iteration 59/1000 | Loss: 0.00002934
Iteration 60/1000 | Loss: 0.00002934
Iteration 61/1000 | Loss: 0.00002934
Iteration 62/1000 | Loss: 0.00002933
Iteration 63/1000 | Loss: 0.00002933
Iteration 64/1000 | Loss: 0.00002933
Iteration 65/1000 | Loss: 0.00002933
Iteration 66/1000 | Loss: 0.00002932
Iteration 67/1000 | Loss: 0.00002932
Iteration 68/1000 | Loss: 0.00002932
Iteration 69/1000 | Loss: 0.00002932
Iteration 70/1000 | Loss: 0.00002932
Iteration 71/1000 | Loss: 0.00002932
Iteration 72/1000 | Loss: 0.00002932
Iteration 73/1000 | Loss: 0.00002931
Iteration 74/1000 | Loss: 0.00002931
Iteration 75/1000 | Loss: 0.00002931
Iteration 76/1000 | Loss: 0.00002930
Iteration 77/1000 | Loss: 0.00002930
Iteration 78/1000 | Loss: 0.00002930
Iteration 79/1000 | Loss: 0.00002930
Iteration 80/1000 | Loss: 0.00002930
Iteration 81/1000 | Loss: 0.00002929
Iteration 82/1000 | Loss: 0.00002929
Iteration 83/1000 | Loss: 0.00002929
Iteration 84/1000 | Loss: 0.00002929
Iteration 85/1000 | Loss: 0.00002929
Iteration 86/1000 | Loss: 0.00002929
Iteration 87/1000 | Loss: 0.00002929
Iteration 88/1000 | Loss: 0.00002929
Iteration 89/1000 | Loss: 0.00002929
Iteration 90/1000 | Loss: 0.00002929
Iteration 91/1000 | Loss: 0.00002929
Iteration 92/1000 | Loss: 0.00002929
Iteration 93/1000 | Loss: 0.00002929
Iteration 94/1000 | Loss: 0.00002928
Iteration 95/1000 | Loss: 0.00002928
Iteration 96/1000 | Loss: 0.00002928
Iteration 97/1000 | Loss: 0.00002928
Iteration 98/1000 | Loss: 0.00002928
Iteration 99/1000 | Loss: 0.00002928
Iteration 100/1000 | Loss: 0.00002928
Iteration 101/1000 | Loss: 0.00002928
Iteration 102/1000 | Loss: 0.00002928
Iteration 103/1000 | Loss: 0.00002928
Iteration 104/1000 | Loss: 0.00002928
Iteration 105/1000 | Loss: 0.00002928
Iteration 106/1000 | Loss: 0.00002928
Iteration 107/1000 | Loss: 0.00002928
Iteration 108/1000 | Loss: 0.00002928
Iteration 109/1000 | Loss: 0.00002928
Iteration 110/1000 | Loss: 0.00002928
Iteration 111/1000 | Loss: 0.00002928
Iteration 112/1000 | Loss: 0.00002928
Iteration 113/1000 | Loss: 0.00002928
Iteration 114/1000 | Loss: 0.00002928
Iteration 115/1000 | Loss: 0.00002928
Iteration 116/1000 | Loss: 0.00002928
Iteration 117/1000 | Loss: 0.00002928
Iteration 118/1000 | Loss: 0.00002928
Iteration 119/1000 | Loss: 0.00002928
Iteration 120/1000 | Loss: 0.00002928
Iteration 121/1000 | Loss: 0.00002928
Iteration 122/1000 | Loss: 0.00002928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.9279282898642123e-05, 2.9279282898642123e-05, 2.9279282898642123e-05, 2.9279282898642123e-05, 2.9279282898642123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9279282898642123e-05

Optimization complete. Final v2v error: 4.386488914489746 mm

Highest mean error: 6.113495826721191 mm for frame 173

Lowest mean error: 4.219550609588623 mm for frame 53

Saving results

Total time: 94.49066948890686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453875
Iteration 2/25 | Loss: 0.00134182
Iteration 3/25 | Loss: 0.00124775
Iteration 4/25 | Loss: 0.00122873
Iteration 5/25 | Loss: 0.00122458
Iteration 6/25 | Loss: 0.00122423
Iteration 7/25 | Loss: 0.00122423
Iteration 8/25 | Loss: 0.00122423
Iteration 9/25 | Loss: 0.00122423
Iteration 10/25 | Loss: 0.00122423
Iteration 11/25 | Loss: 0.00122423
Iteration 12/25 | Loss: 0.00122423
Iteration 13/25 | Loss: 0.00122423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012242321390658617, 0.0012242321390658617, 0.0012242321390658617, 0.0012242321390658617, 0.0012242321390658617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012242321390658617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.27515459
Iteration 2/25 | Loss: 0.00075336
Iteration 3/25 | Loss: 0.00075334
Iteration 4/25 | Loss: 0.00075333
Iteration 5/25 | Loss: 0.00075333
Iteration 6/25 | Loss: 0.00075333
Iteration 7/25 | Loss: 0.00075333
Iteration 8/25 | Loss: 0.00075333
Iteration 9/25 | Loss: 0.00075333
Iteration 10/25 | Loss: 0.00075333
Iteration 11/25 | Loss: 0.00075333
Iteration 12/25 | Loss: 0.00075333
Iteration 13/25 | Loss: 0.00075333
Iteration 14/25 | Loss: 0.00075333
Iteration 15/25 | Loss: 0.00075333
Iteration 16/25 | Loss: 0.00075333
Iteration 17/25 | Loss: 0.00075333
Iteration 18/25 | Loss: 0.00075333
Iteration 19/25 | Loss: 0.00075333
Iteration 20/25 | Loss: 0.00075333
Iteration 21/25 | Loss: 0.00075333
Iteration 22/25 | Loss: 0.00075333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007533320458605886, 0.0007533320458605886, 0.0007533320458605886, 0.0007533320458605886, 0.0007533320458605886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007533320458605886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075333
Iteration 2/1000 | Loss: 0.00003179
Iteration 3/1000 | Loss: 0.00001973
Iteration 4/1000 | Loss: 0.00001783
Iteration 5/1000 | Loss: 0.00001688
Iteration 6/1000 | Loss: 0.00001615
Iteration 7/1000 | Loss: 0.00001581
Iteration 8/1000 | Loss: 0.00001550
Iteration 9/1000 | Loss: 0.00001525
Iteration 10/1000 | Loss: 0.00001503
Iteration 11/1000 | Loss: 0.00001503
Iteration 12/1000 | Loss: 0.00001493
Iteration 13/1000 | Loss: 0.00001478
Iteration 14/1000 | Loss: 0.00001477
Iteration 15/1000 | Loss: 0.00001476
Iteration 16/1000 | Loss: 0.00001476
Iteration 17/1000 | Loss: 0.00001475
Iteration 18/1000 | Loss: 0.00001474
Iteration 19/1000 | Loss: 0.00001473
Iteration 20/1000 | Loss: 0.00001472
Iteration 21/1000 | Loss: 0.00001472
Iteration 22/1000 | Loss: 0.00001472
Iteration 23/1000 | Loss: 0.00001471
Iteration 24/1000 | Loss: 0.00001470
Iteration 25/1000 | Loss: 0.00001470
Iteration 26/1000 | Loss: 0.00001470
Iteration 27/1000 | Loss: 0.00001469
Iteration 28/1000 | Loss: 0.00001468
Iteration 29/1000 | Loss: 0.00001468
Iteration 30/1000 | Loss: 0.00001467
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001464
Iteration 35/1000 | Loss: 0.00001464
Iteration 36/1000 | Loss: 0.00001463
Iteration 37/1000 | Loss: 0.00001463
Iteration 38/1000 | Loss: 0.00001463
Iteration 39/1000 | Loss: 0.00001458
Iteration 40/1000 | Loss: 0.00001456
Iteration 41/1000 | Loss: 0.00001456
Iteration 42/1000 | Loss: 0.00001456
Iteration 43/1000 | Loss: 0.00001455
Iteration 44/1000 | Loss: 0.00001455
Iteration 45/1000 | Loss: 0.00001454
Iteration 46/1000 | Loss: 0.00001454
Iteration 47/1000 | Loss: 0.00001453
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001452
Iteration 50/1000 | Loss: 0.00001452
Iteration 51/1000 | Loss: 0.00001452
Iteration 52/1000 | Loss: 0.00001451
Iteration 53/1000 | Loss: 0.00001451
Iteration 54/1000 | Loss: 0.00001451
Iteration 55/1000 | Loss: 0.00001451
Iteration 56/1000 | Loss: 0.00001451
Iteration 57/1000 | Loss: 0.00001451
Iteration 58/1000 | Loss: 0.00001450
Iteration 59/1000 | Loss: 0.00001450
Iteration 60/1000 | Loss: 0.00001450
Iteration 61/1000 | Loss: 0.00001450
Iteration 62/1000 | Loss: 0.00001450
Iteration 63/1000 | Loss: 0.00001450
Iteration 64/1000 | Loss: 0.00001450
Iteration 65/1000 | Loss: 0.00001450
Iteration 66/1000 | Loss: 0.00001449
Iteration 67/1000 | Loss: 0.00001449
Iteration 68/1000 | Loss: 0.00001449
Iteration 69/1000 | Loss: 0.00001449
Iteration 70/1000 | Loss: 0.00001449
Iteration 71/1000 | Loss: 0.00001449
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00001448
Iteration 74/1000 | Loss: 0.00001448
Iteration 75/1000 | Loss: 0.00001448
Iteration 76/1000 | Loss: 0.00001448
Iteration 77/1000 | Loss: 0.00001448
Iteration 78/1000 | Loss: 0.00001448
Iteration 79/1000 | Loss: 0.00001448
Iteration 80/1000 | Loss: 0.00001447
Iteration 81/1000 | Loss: 0.00001447
Iteration 82/1000 | Loss: 0.00001447
Iteration 83/1000 | Loss: 0.00001447
Iteration 84/1000 | Loss: 0.00001447
Iteration 85/1000 | Loss: 0.00001446
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001445
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001445
Iteration 93/1000 | Loss: 0.00001444
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001443
Iteration 97/1000 | Loss: 0.00001443
Iteration 98/1000 | Loss: 0.00001443
Iteration 99/1000 | Loss: 0.00001443
Iteration 100/1000 | Loss: 0.00001442
Iteration 101/1000 | Loss: 0.00001442
Iteration 102/1000 | Loss: 0.00001442
Iteration 103/1000 | Loss: 0.00001442
Iteration 104/1000 | Loss: 0.00001442
Iteration 105/1000 | Loss: 0.00001441
Iteration 106/1000 | Loss: 0.00001441
Iteration 107/1000 | Loss: 0.00001441
Iteration 108/1000 | Loss: 0.00001441
Iteration 109/1000 | Loss: 0.00001441
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001440
Iteration 113/1000 | Loss: 0.00001440
Iteration 114/1000 | Loss: 0.00001440
Iteration 115/1000 | Loss: 0.00001440
Iteration 116/1000 | Loss: 0.00001440
Iteration 117/1000 | Loss: 0.00001439
Iteration 118/1000 | Loss: 0.00001439
Iteration 119/1000 | Loss: 0.00001439
Iteration 120/1000 | Loss: 0.00001439
Iteration 121/1000 | Loss: 0.00001439
Iteration 122/1000 | Loss: 0.00001439
Iteration 123/1000 | Loss: 0.00001439
Iteration 124/1000 | Loss: 0.00001438
Iteration 125/1000 | Loss: 0.00001438
Iteration 126/1000 | Loss: 0.00001438
Iteration 127/1000 | Loss: 0.00001437
Iteration 128/1000 | Loss: 0.00001437
Iteration 129/1000 | Loss: 0.00001437
Iteration 130/1000 | Loss: 0.00001437
Iteration 131/1000 | Loss: 0.00001437
Iteration 132/1000 | Loss: 0.00001436
Iteration 133/1000 | Loss: 0.00001436
Iteration 134/1000 | Loss: 0.00001436
Iteration 135/1000 | Loss: 0.00001435
Iteration 136/1000 | Loss: 0.00001435
Iteration 137/1000 | Loss: 0.00001435
Iteration 138/1000 | Loss: 0.00001435
Iteration 139/1000 | Loss: 0.00001435
Iteration 140/1000 | Loss: 0.00001435
Iteration 141/1000 | Loss: 0.00001435
Iteration 142/1000 | Loss: 0.00001435
Iteration 143/1000 | Loss: 0.00001435
Iteration 144/1000 | Loss: 0.00001434
Iteration 145/1000 | Loss: 0.00001434
Iteration 146/1000 | Loss: 0.00001434
Iteration 147/1000 | Loss: 0.00001433
Iteration 148/1000 | Loss: 0.00001433
Iteration 149/1000 | Loss: 0.00001433
Iteration 150/1000 | Loss: 0.00001432
Iteration 151/1000 | Loss: 0.00001432
Iteration 152/1000 | Loss: 0.00001432
Iteration 153/1000 | Loss: 0.00001432
Iteration 154/1000 | Loss: 0.00001432
Iteration 155/1000 | Loss: 0.00001432
Iteration 156/1000 | Loss: 0.00001432
Iteration 157/1000 | Loss: 0.00001431
Iteration 158/1000 | Loss: 0.00001431
Iteration 159/1000 | Loss: 0.00001431
Iteration 160/1000 | Loss: 0.00001431
Iteration 161/1000 | Loss: 0.00001431
Iteration 162/1000 | Loss: 0.00001431
Iteration 163/1000 | Loss: 0.00001431
Iteration 164/1000 | Loss: 0.00001430
Iteration 165/1000 | Loss: 0.00001430
Iteration 166/1000 | Loss: 0.00001430
Iteration 167/1000 | Loss: 0.00001430
Iteration 168/1000 | Loss: 0.00001430
Iteration 169/1000 | Loss: 0.00001430
Iteration 170/1000 | Loss: 0.00001430
Iteration 171/1000 | Loss: 0.00001430
Iteration 172/1000 | Loss: 0.00001430
Iteration 173/1000 | Loss: 0.00001430
Iteration 174/1000 | Loss: 0.00001430
Iteration 175/1000 | Loss: 0.00001429
Iteration 176/1000 | Loss: 0.00001429
Iteration 177/1000 | Loss: 0.00001429
Iteration 178/1000 | Loss: 0.00001429
Iteration 179/1000 | Loss: 0.00001429
Iteration 180/1000 | Loss: 0.00001429
Iteration 181/1000 | Loss: 0.00001429
Iteration 182/1000 | Loss: 0.00001429
Iteration 183/1000 | Loss: 0.00001429
Iteration 184/1000 | Loss: 0.00001429
Iteration 185/1000 | Loss: 0.00001429
Iteration 186/1000 | Loss: 0.00001429
Iteration 187/1000 | Loss: 0.00001429
Iteration 188/1000 | Loss: 0.00001429
Iteration 189/1000 | Loss: 0.00001429
Iteration 190/1000 | Loss: 0.00001429
Iteration 191/1000 | Loss: 0.00001429
Iteration 192/1000 | Loss: 0.00001428
Iteration 193/1000 | Loss: 0.00001428
Iteration 194/1000 | Loss: 0.00001428
Iteration 195/1000 | Loss: 0.00001428
Iteration 196/1000 | Loss: 0.00001428
Iteration 197/1000 | Loss: 0.00001428
Iteration 198/1000 | Loss: 0.00001428
Iteration 199/1000 | Loss: 0.00001428
Iteration 200/1000 | Loss: 0.00001428
Iteration 201/1000 | Loss: 0.00001428
Iteration 202/1000 | Loss: 0.00001428
Iteration 203/1000 | Loss: 0.00001428
Iteration 204/1000 | Loss: 0.00001428
Iteration 205/1000 | Loss: 0.00001428
Iteration 206/1000 | Loss: 0.00001428
Iteration 207/1000 | Loss: 0.00001428
Iteration 208/1000 | Loss: 0.00001428
Iteration 209/1000 | Loss: 0.00001428
Iteration 210/1000 | Loss: 0.00001428
Iteration 211/1000 | Loss: 0.00001428
Iteration 212/1000 | Loss: 0.00001428
Iteration 213/1000 | Loss: 0.00001428
Iteration 214/1000 | Loss: 0.00001428
Iteration 215/1000 | Loss: 0.00001428
Iteration 216/1000 | Loss: 0.00001428
Iteration 217/1000 | Loss: 0.00001428
Iteration 218/1000 | Loss: 0.00001428
Iteration 219/1000 | Loss: 0.00001428
Iteration 220/1000 | Loss: 0.00001428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.4280130926636048e-05, 1.4280130926636048e-05, 1.4280130926636048e-05, 1.4280130926636048e-05, 1.4280130926636048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4280130926636048e-05

Optimization complete. Final v2v error: 3.2162957191467285 mm

Highest mean error: 3.610687732696533 mm for frame 83

Lowest mean error: 2.8723814487457275 mm for frame 31

Saving results

Total time: 45.95914149284363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475322
Iteration 2/25 | Loss: 0.00131057
Iteration 3/25 | Loss: 0.00124146
Iteration 4/25 | Loss: 0.00123347
Iteration 5/25 | Loss: 0.00123087
Iteration 6/25 | Loss: 0.00123071
Iteration 7/25 | Loss: 0.00123071
Iteration 8/25 | Loss: 0.00123071
Iteration 9/25 | Loss: 0.00123071
Iteration 10/25 | Loss: 0.00123071
Iteration 11/25 | Loss: 0.00123071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012307054130360484, 0.0012307054130360484, 0.0012307054130360484, 0.0012307054130360484, 0.0012307054130360484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012307054130360484

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.19790888
Iteration 2/25 | Loss: 0.00076550
Iteration 3/25 | Loss: 0.00076549
Iteration 4/25 | Loss: 0.00076549
Iteration 5/25 | Loss: 0.00076549
Iteration 6/25 | Loss: 0.00076549
Iteration 7/25 | Loss: 0.00076549
Iteration 8/25 | Loss: 0.00076549
Iteration 9/25 | Loss: 0.00076549
Iteration 10/25 | Loss: 0.00076549
Iteration 11/25 | Loss: 0.00076549
Iteration 12/25 | Loss: 0.00076549
Iteration 13/25 | Loss: 0.00076549
Iteration 14/25 | Loss: 0.00076549
Iteration 15/25 | Loss: 0.00076549
Iteration 16/25 | Loss: 0.00076549
Iteration 17/25 | Loss: 0.00076549
Iteration 18/25 | Loss: 0.00076549
Iteration 19/25 | Loss: 0.00076549
Iteration 20/25 | Loss: 0.00076549
Iteration 21/25 | Loss: 0.00076548
Iteration 22/25 | Loss: 0.00076548
Iteration 23/25 | Loss: 0.00076549
Iteration 24/25 | Loss: 0.00076549
Iteration 25/25 | Loss: 0.00076548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076548
Iteration 2/1000 | Loss: 0.00002839
Iteration 3/1000 | Loss: 0.00002026
Iteration 4/1000 | Loss: 0.00001845
Iteration 5/1000 | Loss: 0.00001754
Iteration 6/1000 | Loss: 0.00001686
Iteration 7/1000 | Loss: 0.00001629
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001571
Iteration 10/1000 | Loss: 0.00001540
Iteration 11/1000 | Loss: 0.00001535
Iteration 12/1000 | Loss: 0.00001525
Iteration 13/1000 | Loss: 0.00001520
Iteration 14/1000 | Loss: 0.00001517
Iteration 15/1000 | Loss: 0.00001515
Iteration 16/1000 | Loss: 0.00001512
Iteration 17/1000 | Loss: 0.00001508
Iteration 18/1000 | Loss: 0.00001507
Iteration 19/1000 | Loss: 0.00001506
Iteration 20/1000 | Loss: 0.00001505
Iteration 21/1000 | Loss: 0.00001504
Iteration 22/1000 | Loss: 0.00001504
Iteration 23/1000 | Loss: 0.00001503
Iteration 24/1000 | Loss: 0.00001502
Iteration 25/1000 | Loss: 0.00001502
Iteration 26/1000 | Loss: 0.00001500
Iteration 27/1000 | Loss: 0.00001500
Iteration 28/1000 | Loss: 0.00001499
Iteration 29/1000 | Loss: 0.00001499
Iteration 30/1000 | Loss: 0.00001498
Iteration 31/1000 | Loss: 0.00001498
Iteration 32/1000 | Loss: 0.00001497
Iteration 33/1000 | Loss: 0.00001497
Iteration 34/1000 | Loss: 0.00001497
Iteration 35/1000 | Loss: 0.00001496
Iteration 36/1000 | Loss: 0.00001496
Iteration 37/1000 | Loss: 0.00001496
Iteration 38/1000 | Loss: 0.00001495
Iteration 39/1000 | Loss: 0.00001495
Iteration 40/1000 | Loss: 0.00001493
Iteration 41/1000 | Loss: 0.00001493
Iteration 42/1000 | Loss: 0.00001493
Iteration 43/1000 | Loss: 0.00001493
Iteration 44/1000 | Loss: 0.00001493
Iteration 45/1000 | Loss: 0.00001493
Iteration 46/1000 | Loss: 0.00001493
Iteration 47/1000 | Loss: 0.00001493
Iteration 48/1000 | Loss: 0.00001493
Iteration 49/1000 | Loss: 0.00001492
Iteration 50/1000 | Loss: 0.00001492
Iteration 51/1000 | Loss: 0.00001492
Iteration 52/1000 | Loss: 0.00001492
Iteration 53/1000 | Loss: 0.00001492
Iteration 54/1000 | Loss: 0.00001492
Iteration 55/1000 | Loss: 0.00001490
Iteration 56/1000 | Loss: 0.00001489
Iteration 57/1000 | Loss: 0.00001489
Iteration 58/1000 | Loss: 0.00001489
Iteration 59/1000 | Loss: 0.00001489
Iteration 60/1000 | Loss: 0.00001489
Iteration 61/1000 | Loss: 0.00001489
Iteration 62/1000 | Loss: 0.00001488
Iteration 63/1000 | Loss: 0.00001488
Iteration 64/1000 | Loss: 0.00001488
Iteration 65/1000 | Loss: 0.00001487
Iteration 66/1000 | Loss: 0.00001487
Iteration 67/1000 | Loss: 0.00001486
Iteration 68/1000 | Loss: 0.00001486
Iteration 69/1000 | Loss: 0.00001486
Iteration 70/1000 | Loss: 0.00001485
Iteration 71/1000 | Loss: 0.00001485
Iteration 72/1000 | Loss: 0.00001484
Iteration 73/1000 | Loss: 0.00001484
Iteration 74/1000 | Loss: 0.00001484
Iteration 75/1000 | Loss: 0.00001484
Iteration 76/1000 | Loss: 0.00001484
Iteration 77/1000 | Loss: 0.00001484
Iteration 78/1000 | Loss: 0.00001483
Iteration 79/1000 | Loss: 0.00001483
Iteration 80/1000 | Loss: 0.00001483
Iteration 81/1000 | Loss: 0.00001482
Iteration 82/1000 | Loss: 0.00001482
Iteration 83/1000 | Loss: 0.00001482
Iteration 84/1000 | Loss: 0.00001482
Iteration 85/1000 | Loss: 0.00001481
Iteration 86/1000 | Loss: 0.00001481
Iteration 87/1000 | Loss: 0.00001480
Iteration 88/1000 | Loss: 0.00001479
Iteration 89/1000 | Loss: 0.00001479
Iteration 90/1000 | Loss: 0.00001479
Iteration 91/1000 | Loss: 0.00001478
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001478
Iteration 94/1000 | Loss: 0.00001478
Iteration 95/1000 | Loss: 0.00001477
Iteration 96/1000 | Loss: 0.00001477
Iteration 97/1000 | Loss: 0.00001477
Iteration 98/1000 | Loss: 0.00001477
Iteration 99/1000 | Loss: 0.00001476
Iteration 100/1000 | Loss: 0.00001476
Iteration 101/1000 | Loss: 0.00001476
Iteration 102/1000 | Loss: 0.00001475
Iteration 103/1000 | Loss: 0.00001475
Iteration 104/1000 | Loss: 0.00001475
Iteration 105/1000 | Loss: 0.00001475
Iteration 106/1000 | Loss: 0.00001475
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001474
Iteration 109/1000 | Loss: 0.00001474
Iteration 110/1000 | Loss: 0.00001473
Iteration 111/1000 | Loss: 0.00001473
Iteration 112/1000 | Loss: 0.00001472
Iteration 113/1000 | Loss: 0.00001472
Iteration 114/1000 | Loss: 0.00001472
Iteration 115/1000 | Loss: 0.00001472
Iteration 116/1000 | Loss: 0.00001472
Iteration 117/1000 | Loss: 0.00001471
Iteration 118/1000 | Loss: 0.00001471
Iteration 119/1000 | Loss: 0.00001470
Iteration 120/1000 | Loss: 0.00001470
Iteration 121/1000 | Loss: 0.00001470
Iteration 122/1000 | Loss: 0.00001469
Iteration 123/1000 | Loss: 0.00001469
Iteration 124/1000 | Loss: 0.00001469
Iteration 125/1000 | Loss: 0.00001468
Iteration 126/1000 | Loss: 0.00001468
Iteration 127/1000 | Loss: 0.00001468
Iteration 128/1000 | Loss: 0.00001468
Iteration 129/1000 | Loss: 0.00001467
Iteration 130/1000 | Loss: 0.00001467
Iteration 131/1000 | Loss: 0.00001467
Iteration 132/1000 | Loss: 0.00001467
Iteration 133/1000 | Loss: 0.00001466
Iteration 134/1000 | Loss: 0.00001466
Iteration 135/1000 | Loss: 0.00001466
Iteration 136/1000 | Loss: 0.00001465
Iteration 137/1000 | Loss: 0.00001465
Iteration 138/1000 | Loss: 0.00001464
Iteration 139/1000 | Loss: 0.00001464
Iteration 140/1000 | Loss: 0.00001464
Iteration 141/1000 | Loss: 0.00001464
Iteration 142/1000 | Loss: 0.00001464
Iteration 143/1000 | Loss: 0.00001464
Iteration 144/1000 | Loss: 0.00001464
Iteration 145/1000 | Loss: 0.00001464
Iteration 146/1000 | Loss: 0.00001464
Iteration 147/1000 | Loss: 0.00001463
Iteration 148/1000 | Loss: 0.00001463
Iteration 149/1000 | Loss: 0.00001463
Iteration 150/1000 | Loss: 0.00001463
Iteration 151/1000 | Loss: 0.00001463
Iteration 152/1000 | Loss: 0.00001463
Iteration 153/1000 | Loss: 0.00001462
Iteration 154/1000 | Loss: 0.00001462
Iteration 155/1000 | Loss: 0.00001462
Iteration 156/1000 | Loss: 0.00001462
Iteration 157/1000 | Loss: 0.00001462
Iteration 158/1000 | Loss: 0.00001462
Iteration 159/1000 | Loss: 0.00001461
Iteration 160/1000 | Loss: 0.00001461
Iteration 161/1000 | Loss: 0.00001461
Iteration 162/1000 | Loss: 0.00001461
Iteration 163/1000 | Loss: 0.00001461
Iteration 164/1000 | Loss: 0.00001461
Iteration 165/1000 | Loss: 0.00001461
Iteration 166/1000 | Loss: 0.00001461
Iteration 167/1000 | Loss: 0.00001461
Iteration 168/1000 | Loss: 0.00001461
Iteration 169/1000 | Loss: 0.00001460
Iteration 170/1000 | Loss: 0.00001460
Iteration 171/1000 | Loss: 0.00001460
Iteration 172/1000 | Loss: 0.00001460
Iteration 173/1000 | Loss: 0.00001460
Iteration 174/1000 | Loss: 0.00001460
Iteration 175/1000 | Loss: 0.00001460
Iteration 176/1000 | Loss: 0.00001460
Iteration 177/1000 | Loss: 0.00001460
Iteration 178/1000 | Loss: 0.00001460
Iteration 179/1000 | Loss: 0.00001460
Iteration 180/1000 | Loss: 0.00001460
Iteration 181/1000 | Loss: 0.00001460
Iteration 182/1000 | Loss: 0.00001460
Iteration 183/1000 | Loss: 0.00001460
Iteration 184/1000 | Loss: 0.00001460
Iteration 185/1000 | Loss: 0.00001460
Iteration 186/1000 | Loss: 0.00001459
Iteration 187/1000 | Loss: 0.00001459
Iteration 188/1000 | Loss: 0.00001459
Iteration 189/1000 | Loss: 0.00001459
Iteration 190/1000 | Loss: 0.00001459
Iteration 191/1000 | Loss: 0.00001459
Iteration 192/1000 | Loss: 0.00001459
Iteration 193/1000 | Loss: 0.00001459
Iteration 194/1000 | Loss: 0.00001459
Iteration 195/1000 | Loss: 0.00001459
Iteration 196/1000 | Loss: 0.00001459
Iteration 197/1000 | Loss: 0.00001459
Iteration 198/1000 | Loss: 0.00001458
Iteration 199/1000 | Loss: 0.00001458
Iteration 200/1000 | Loss: 0.00001458
Iteration 201/1000 | Loss: 0.00001458
Iteration 202/1000 | Loss: 0.00001458
Iteration 203/1000 | Loss: 0.00001458
Iteration 204/1000 | Loss: 0.00001458
Iteration 205/1000 | Loss: 0.00001458
Iteration 206/1000 | Loss: 0.00001458
Iteration 207/1000 | Loss: 0.00001458
Iteration 208/1000 | Loss: 0.00001458
Iteration 209/1000 | Loss: 0.00001458
Iteration 210/1000 | Loss: 0.00001458
Iteration 211/1000 | Loss: 0.00001458
Iteration 212/1000 | Loss: 0.00001458
Iteration 213/1000 | Loss: 0.00001458
Iteration 214/1000 | Loss: 0.00001458
Iteration 215/1000 | Loss: 0.00001458
Iteration 216/1000 | Loss: 0.00001458
Iteration 217/1000 | Loss: 0.00001457
Iteration 218/1000 | Loss: 0.00001457
Iteration 219/1000 | Loss: 0.00001457
Iteration 220/1000 | Loss: 0.00001457
Iteration 221/1000 | Loss: 0.00001457
Iteration 222/1000 | Loss: 0.00001457
Iteration 223/1000 | Loss: 0.00001457
Iteration 224/1000 | Loss: 0.00001457
Iteration 225/1000 | Loss: 0.00001457
Iteration 226/1000 | Loss: 0.00001457
Iteration 227/1000 | Loss: 0.00001457
Iteration 228/1000 | Loss: 0.00001456
Iteration 229/1000 | Loss: 0.00001456
Iteration 230/1000 | Loss: 0.00001456
Iteration 231/1000 | Loss: 0.00001456
Iteration 232/1000 | Loss: 0.00001456
Iteration 233/1000 | Loss: 0.00001456
Iteration 234/1000 | Loss: 0.00001456
Iteration 235/1000 | Loss: 0.00001456
Iteration 236/1000 | Loss: 0.00001456
Iteration 237/1000 | Loss: 0.00001456
Iteration 238/1000 | Loss: 0.00001456
Iteration 239/1000 | Loss: 0.00001456
Iteration 240/1000 | Loss: 0.00001456
Iteration 241/1000 | Loss: 0.00001456
Iteration 242/1000 | Loss: 0.00001456
Iteration 243/1000 | Loss: 0.00001456
Iteration 244/1000 | Loss: 0.00001456
Iteration 245/1000 | Loss: 0.00001456
Iteration 246/1000 | Loss: 0.00001456
Iteration 247/1000 | Loss: 0.00001455
Iteration 248/1000 | Loss: 0.00001455
Iteration 249/1000 | Loss: 0.00001455
Iteration 250/1000 | Loss: 0.00001455
Iteration 251/1000 | Loss: 0.00001455
Iteration 252/1000 | Loss: 0.00001455
Iteration 253/1000 | Loss: 0.00001455
Iteration 254/1000 | Loss: 0.00001455
Iteration 255/1000 | Loss: 0.00001455
Iteration 256/1000 | Loss: 0.00001455
Iteration 257/1000 | Loss: 0.00001455
Iteration 258/1000 | Loss: 0.00001455
Iteration 259/1000 | Loss: 0.00001455
Iteration 260/1000 | Loss: 0.00001455
Iteration 261/1000 | Loss: 0.00001455
Iteration 262/1000 | Loss: 0.00001455
Iteration 263/1000 | Loss: 0.00001455
Iteration 264/1000 | Loss: 0.00001455
Iteration 265/1000 | Loss: 0.00001455
Iteration 266/1000 | Loss: 0.00001455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.4551323147315998e-05, 1.4551323147315998e-05, 1.4551323147315998e-05, 1.4551323147315998e-05, 1.4551323147315998e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4551323147315998e-05

Optimization complete. Final v2v error: 3.2231149673461914 mm

Highest mean error: 3.494647979736328 mm for frame 79

Lowest mean error: 2.9943039417266846 mm for frame 153

Saving results

Total time: 43.60888886451721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956877
Iteration 2/25 | Loss: 0.00956876
Iteration 3/25 | Loss: 0.00956876
Iteration 4/25 | Loss: 0.00250890
Iteration 5/25 | Loss: 0.00164210
Iteration 6/25 | Loss: 0.00154309
Iteration 7/25 | Loss: 0.00155199
Iteration 8/25 | Loss: 0.00142735
Iteration 9/25 | Loss: 0.00135839
Iteration 10/25 | Loss: 0.00134012
Iteration 11/25 | Loss: 0.00132021
Iteration 12/25 | Loss: 0.00132430
Iteration 13/25 | Loss: 0.00130840
Iteration 14/25 | Loss: 0.00130304
Iteration 15/25 | Loss: 0.00129993
Iteration 16/25 | Loss: 0.00129926
Iteration 17/25 | Loss: 0.00129880
Iteration 18/25 | Loss: 0.00129882
Iteration 19/25 | Loss: 0.00129623
Iteration 20/25 | Loss: 0.00129572
Iteration 21/25 | Loss: 0.00129701
Iteration 22/25 | Loss: 0.00129444
Iteration 23/25 | Loss: 0.00129360
Iteration 24/25 | Loss: 0.00129325
Iteration 25/25 | Loss: 0.00129312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42363656
Iteration 2/25 | Loss: 0.00073291
Iteration 3/25 | Loss: 0.00073291
Iteration 4/25 | Loss: 0.00073291
Iteration 5/25 | Loss: 0.00073291
Iteration 6/25 | Loss: 0.00073291
Iteration 7/25 | Loss: 0.00073291
Iteration 8/25 | Loss: 0.00073291
Iteration 9/25 | Loss: 0.00073290
Iteration 10/25 | Loss: 0.00073290
Iteration 11/25 | Loss: 0.00073290
Iteration 12/25 | Loss: 0.00073290
Iteration 13/25 | Loss: 0.00073290
Iteration 14/25 | Loss: 0.00073290
Iteration 15/25 | Loss: 0.00073290
Iteration 16/25 | Loss: 0.00073290
Iteration 17/25 | Loss: 0.00073290
Iteration 18/25 | Loss: 0.00073290
Iteration 19/25 | Loss: 0.00073290
Iteration 20/25 | Loss: 0.00073290
Iteration 21/25 | Loss: 0.00073290
Iteration 22/25 | Loss: 0.00073290
Iteration 23/25 | Loss: 0.00073290
Iteration 24/25 | Loss: 0.00073290
Iteration 25/25 | Loss: 0.00073290

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073290
Iteration 2/1000 | Loss: 0.00003816
Iteration 3/1000 | Loss: 0.00002598
Iteration 4/1000 | Loss: 0.00002338
Iteration 5/1000 | Loss: 0.00002227
Iteration 6/1000 | Loss: 0.00002137
Iteration 7/1000 | Loss: 0.00002078
Iteration 8/1000 | Loss: 0.00002045
Iteration 9/1000 | Loss: 0.00002000
Iteration 10/1000 | Loss: 0.00001971
Iteration 11/1000 | Loss: 0.00001951
Iteration 12/1000 | Loss: 0.00001938
Iteration 13/1000 | Loss: 0.00001931
Iteration 14/1000 | Loss: 0.00001924
Iteration 15/1000 | Loss: 0.00001922
Iteration 16/1000 | Loss: 0.00001920
Iteration 17/1000 | Loss: 0.00001919
Iteration 18/1000 | Loss: 0.00001918
Iteration 19/1000 | Loss: 0.00001917
Iteration 20/1000 | Loss: 0.00001917
Iteration 21/1000 | Loss: 0.00001915
Iteration 22/1000 | Loss: 0.00001908
Iteration 23/1000 | Loss: 0.00001907
Iteration 24/1000 | Loss: 0.00001906
Iteration 25/1000 | Loss: 0.00001904
Iteration 26/1000 | Loss: 0.00001902
Iteration 27/1000 | Loss: 0.00001900
Iteration 28/1000 | Loss: 0.00001900
Iteration 29/1000 | Loss: 0.00001900
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001900
Iteration 32/1000 | Loss: 0.00001898
Iteration 33/1000 | Loss: 0.00001898
Iteration 34/1000 | Loss: 0.00001896
Iteration 35/1000 | Loss: 0.00001895
Iteration 36/1000 | Loss: 0.00001895
Iteration 37/1000 | Loss: 0.00001895
Iteration 38/1000 | Loss: 0.00001895
Iteration 39/1000 | Loss: 0.00001893
Iteration 40/1000 | Loss: 0.00001893
Iteration 41/1000 | Loss: 0.00001891
Iteration 42/1000 | Loss: 0.00001890
Iteration 43/1000 | Loss: 0.00001889
Iteration 44/1000 | Loss: 0.00001886
Iteration 45/1000 | Loss: 0.00001885
Iteration 46/1000 | Loss: 0.00001882
Iteration 47/1000 | Loss: 0.00001881
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001879
Iteration 50/1000 | Loss: 0.00001877
Iteration 51/1000 | Loss: 0.00001877
Iteration 52/1000 | Loss: 0.00001876
Iteration 53/1000 | Loss: 0.00001876
Iteration 54/1000 | Loss: 0.00001875
Iteration 55/1000 | Loss: 0.00001875
Iteration 56/1000 | Loss: 0.00001875
Iteration 57/1000 | Loss: 0.00001874
Iteration 58/1000 | Loss: 0.00001874
Iteration 59/1000 | Loss: 0.00001873
Iteration 60/1000 | Loss: 0.00001872
Iteration 61/1000 | Loss: 0.00039122
Iteration 62/1000 | Loss: 0.00010496
Iteration 63/1000 | Loss: 0.00001961
Iteration 64/1000 | Loss: 0.00001882
Iteration 65/1000 | Loss: 0.00001875
Iteration 66/1000 | Loss: 0.00001874
Iteration 67/1000 | Loss: 0.00001874
Iteration 68/1000 | Loss: 0.00036465
Iteration 69/1000 | Loss: 0.00002788
Iteration 70/1000 | Loss: 0.00002002
Iteration 71/1000 | Loss: 0.00001845
Iteration 72/1000 | Loss: 0.00001801
Iteration 73/1000 | Loss: 0.00001793
Iteration 74/1000 | Loss: 0.00001784
Iteration 75/1000 | Loss: 0.00001783
Iteration 76/1000 | Loss: 0.00001777
Iteration 77/1000 | Loss: 0.00001776
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001761
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001759
Iteration 82/1000 | Loss: 0.00001759
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001756
Iteration 85/1000 | Loss: 0.00001756
Iteration 86/1000 | Loss: 0.00001755
Iteration 87/1000 | Loss: 0.00001754
Iteration 88/1000 | Loss: 0.00001754
Iteration 89/1000 | Loss: 0.00001753
Iteration 90/1000 | Loss: 0.00001753
Iteration 91/1000 | Loss: 0.00001752
Iteration 92/1000 | Loss: 0.00001752
Iteration 93/1000 | Loss: 0.00001752
Iteration 94/1000 | Loss: 0.00001751
Iteration 95/1000 | Loss: 0.00001751
Iteration 96/1000 | Loss: 0.00001750
Iteration 97/1000 | Loss: 0.00001750
Iteration 98/1000 | Loss: 0.00001749
Iteration 99/1000 | Loss: 0.00001749
Iteration 100/1000 | Loss: 0.00001747
Iteration 101/1000 | Loss: 0.00001747
Iteration 102/1000 | Loss: 0.00001746
Iteration 103/1000 | Loss: 0.00001746
Iteration 104/1000 | Loss: 0.00001745
Iteration 105/1000 | Loss: 0.00001745
Iteration 106/1000 | Loss: 0.00001744
Iteration 107/1000 | Loss: 0.00001744
Iteration 108/1000 | Loss: 0.00001743
Iteration 109/1000 | Loss: 0.00001743
Iteration 110/1000 | Loss: 0.00001743
Iteration 111/1000 | Loss: 0.00001743
Iteration 112/1000 | Loss: 0.00001743
Iteration 113/1000 | Loss: 0.00001742
Iteration 114/1000 | Loss: 0.00001742
Iteration 115/1000 | Loss: 0.00001742
Iteration 116/1000 | Loss: 0.00001742
Iteration 117/1000 | Loss: 0.00001741
Iteration 118/1000 | Loss: 0.00001741
Iteration 119/1000 | Loss: 0.00001740
Iteration 120/1000 | Loss: 0.00001740
Iteration 121/1000 | Loss: 0.00001740
Iteration 122/1000 | Loss: 0.00001739
Iteration 123/1000 | Loss: 0.00001739
Iteration 124/1000 | Loss: 0.00001739
Iteration 125/1000 | Loss: 0.00001739
Iteration 126/1000 | Loss: 0.00001738
Iteration 127/1000 | Loss: 0.00001738
Iteration 128/1000 | Loss: 0.00001737
Iteration 129/1000 | Loss: 0.00001737
Iteration 130/1000 | Loss: 0.00001736
Iteration 131/1000 | Loss: 0.00001736
Iteration 132/1000 | Loss: 0.00001735
Iteration 133/1000 | Loss: 0.00001735
Iteration 134/1000 | Loss: 0.00001731
Iteration 135/1000 | Loss: 0.00001729
Iteration 136/1000 | Loss: 0.00001729
Iteration 137/1000 | Loss: 0.00001728
Iteration 138/1000 | Loss: 0.00001728
Iteration 139/1000 | Loss: 0.00001728
Iteration 140/1000 | Loss: 0.00001727
Iteration 141/1000 | Loss: 0.00001727
Iteration 142/1000 | Loss: 0.00001726
Iteration 143/1000 | Loss: 0.00001726
Iteration 144/1000 | Loss: 0.00001726
Iteration 145/1000 | Loss: 0.00001725
Iteration 146/1000 | Loss: 0.00001725
Iteration 147/1000 | Loss: 0.00001725
Iteration 148/1000 | Loss: 0.00001724
Iteration 149/1000 | Loss: 0.00001724
Iteration 150/1000 | Loss: 0.00001724
Iteration 151/1000 | Loss: 0.00001723
Iteration 152/1000 | Loss: 0.00001723
Iteration 153/1000 | Loss: 0.00001723
Iteration 154/1000 | Loss: 0.00001722
Iteration 155/1000 | Loss: 0.00001721
Iteration 156/1000 | Loss: 0.00001721
Iteration 157/1000 | Loss: 0.00001721
Iteration 158/1000 | Loss: 0.00001721
Iteration 159/1000 | Loss: 0.00001720
Iteration 160/1000 | Loss: 0.00001720
Iteration 161/1000 | Loss: 0.00001720
Iteration 162/1000 | Loss: 0.00001720
Iteration 163/1000 | Loss: 0.00001720
Iteration 164/1000 | Loss: 0.00001720
Iteration 165/1000 | Loss: 0.00001719
Iteration 166/1000 | Loss: 0.00001719
Iteration 167/1000 | Loss: 0.00001719
Iteration 168/1000 | Loss: 0.00001719
Iteration 169/1000 | Loss: 0.00001718
Iteration 170/1000 | Loss: 0.00001718
Iteration 171/1000 | Loss: 0.00001718
Iteration 172/1000 | Loss: 0.00001718
Iteration 173/1000 | Loss: 0.00001718
Iteration 174/1000 | Loss: 0.00001718
Iteration 175/1000 | Loss: 0.00001718
Iteration 176/1000 | Loss: 0.00001717
Iteration 177/1000 | Loss: 0.00001717
Iteration 178/1000 | Loss: 0.00001717
Iteration 179/1000 | Loss: 0.00001717
Iteration 180/1000 | Loss: 0.00001717
Iteration 181/1000 | Loss: 0.00001717
Iteration 182/1000 | Loss: 0.00001717
Iteration 183/1000 | Loss: 0.00001717
Iteration 184/1000 | Loss: 0.00001717
Iteration 185/1000 | Loss: 0.00001717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.717302257020492e-05, 1.717302257020492e-05, 1.717302257020492e-05, 1.717302257020492e-05, 1.717302257020492e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.717302257020492e-05

Optimization complete. Final v2v error: 3.44577956199646 mm

Highest mean error: 5.204982280731201 mm for frame 13

Lowest mean error: 2.9937236309051514 mm for frame 229

Saving results

Total time: 107.08482050895691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515092
Iteration 2/25 | Loss: 0.00158495
Iteration 3/25 | Loss: 0.00132095
Iteration 4/25 | Loss: 0.00129704
Iteration 5/25 | Loss: 0.00129296
Iteration 6/25 | Loss: 0.00129072
Iteration 7/25 | Loss: 0.00129072
Iteration 8/25 | Loss: 0.00129072
Iteration 9/25 | Loss: 0.00129072
Iteration 10/25 | Loss: 0.00129072
Iteration 11/25 | Loss: 0.00129072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012907176278531551, 0.0012907176278531551, 0.0012907176278531551, 0.0012907176278531551, 0.0012907176278531551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012907176278531551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25794637
Iteration 2/25 | Loss: 0.00083215
Iteration 3/25 | Loss: 0.00083211
Iteration 4/25 | Loss: 0.00083211
Iteration 5/25 | Loss: 0.00083211
Iteration 6/25 | Loss: 0.00083211
Iteration 7/25 | Loss: 0.00083211
Iteration 8/25 | Loss: 0.00083211
Iteration 9/25 | Loss: 0.00083211
Iteration 10/25 | Loss: 0.00083211
Iteration 11/25 | Loss: 0.00083211
Iteration 12/25 | Loss: 0.00083211
Iteration 13/25 | Loss: 0.00083211
Iteration 14/25 | Loss: 0.00083211
Iteration 15/25 | Loss: 0.00083211
Iteration 16/25 | Loss: 0.00083211
Iteration 17/25 | Loss: 0.00083211
Iteration 18/25 | Loss: 0.00083211
Iteration 19/25 | Loss: 0.00083211
Iteration 20/25 | Loss: 0.00083211
Iteration 21/25 | Loss: 0.00083211
Iteration 22/25 | Loss: 0.00083211
Iteration 23/25 | Loss: 0.00083211
Iteration 24/25 | Loss: 0.00083211
Iteration 25/25 | Loss: 0.00083211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083211
Iteration 2/1000 | Loss: 0.00004531
Iteration 3/1000 | Loss: 0.00003112
Iteration 4/1000 | Loss: 0.00002749
Iteration 5/1000 | Loss: 0.00002592
Iteration 6/1000 | Loss: 0.00002504
Iteration 7/1000 | Loss: 0.00002432
Iteration 8/1000 | Loss: 0.00002366
Iteration 9/1000 | Loss: 0.00002320
Iteration 10/1000 | Loss: 0.00002286
Iteration 11/1000 | Loss: 0.00002249
Iteration 12/1000 | Loss: 0.00002227
Iteration 13/1000 | Loss: 0.00002220
Iteration 14/1000 | Loss: 0.00002196
Iteration 15/1000 | Loss: 0.00002185
Iteration 16/1000 | Loss: 0.00002168
Iteration 17/1000 | Loss: 0.00002153
Iteration 18/1000 | Loss: 0.00002153
Iteration 19/1000 | Loss: 0.00002152
Iteration 20/1000 | Loss: 0.00002151
Iteration 21/1000 | Loss: 0.00002151
Iteration 22/1000 | Loss: 0.00002150
Iteration 23/1000 | Loss: 0.00002150
Iteration 24/1000 | Loss: 0.00002146
Iteration 25/1000 | Loss: 0.00002136
Iteration 26/1000 | Loss: 0.00002134
Iteration 27/1000 | Loss: 0.00002131
Iteration 28/1000 | Loss: 0.00002129
Iteration 29/1000 | Loss: 0.00002125
Iteration 30/1000 | Loss: 0.00002125
Iteration 31/1000 | Loss: 0.00002122
Iteration 32/1000 | Loss: 0.00002121
Iteration 33/1000 | Loss: 0.00002119
Iteration 34/1000 | Loss: 0.00002119
Iteration 35/1000 | Loss: 0.00002118
Iteration 36/1000 | Loss: 0.00002118
Iteration 37/1000 | Loss: 0.00002118
Iteration 38/1000 | Loss: 0.00002118
Iteration 39/1000 | Loss: 0.00002118
Iteration 40/1000 | Loss: 0.00002118
Iteration 41/1000 | Loss: 0.00002118
Iteration 42/1000 | Loss: 0.00002118
Iteration 43/1000 | Loss: 0.00002118
Iteration 44/1000 | Loss: 0.00002118
Iteration 45/1000 | Loss: 0.00002117
Iteration 46/1000 | Loss: 0.00002117
Iteration 47/1000 | Loss: 0.00002117
Iteration 48/1000 | Loss: 0.00002116
Iteration 49/1000 | Loss: 0.00002115
Iteration 50/1000 | Loss: 0.00002115
Iteration 51/1000 | Loss: 0.00002114
Iteration 52/1000 | Loss: 0.00002114
Iteration 53/1000 | Loss: 0.00002114
Iteration 54/1000 | Loss: 0.00002113
Iteration 55/1000 | Loss: 0.00002113
Iteration 56/1000 | Loss: 0.00002113
Iteration 57/1000 | Loss: 0.00002113
Iteration 58/1000 | Loss: 0.00002112
Iteration 59/1000 | Loss: 0.00002112
Iteration 60/1000 | Loss: 0.00002112
Iteration 61/1000 | Loss: 0.00002112
Iteration 62/1000 | Loss: 0.00002111
Iteration 63/1000 | Loss: 0.00002111
Iteration 64/1000 | Loss: 0.00002111
Iteration 65/1000 | Loss: 0.00002111
Iteration 66/1000 | Loss: 0.00002111
Iteration 67/1000 | Loss: 0.00002111
Iteration 68/1000 | Loss: 0.00002111
Iteration 69/1000 | Loss: 0.00002111
Iteration 70/1000 | Loss: 0.00002111
Iteration 71/1000 | Loss: 0.00002111
Iteration 72/1000 | Loss: 0.00002110
Iteration 73/1000 | Loss: 0.00002110
Iteration 74/1000 | Loss: 0.00002110
Iteration 75/1000 | Loss: 0.00002110
Iteration 76/1000 | Loss: 0.00002110
Iteration 77/1000 | Loss: 0.00002109
Iteration 78/1000 | Loss: 0.00002109
Iteration 79/1000 | Loss: 0.00002109
Iteration 80/1000 | Loss: 0.00002109
Iteration 81/1000 | Loss: 0.00002109
Iteration 82/1000 | Loss: 0.00002109
Iteration 83/1000 | Loss: 0.00002108
Iteration 84/1000 | Loss: 0.00002108
Iteration 85/1000 | Loss: 0.00002108
Iteration 86/1000 | Loss: 0.00002108
Iteration 87/1000 | Loss: 0.00002108
Iteration 88/1000 | Loss: 0.00002108
Iteration 89/1000 | Loss: 0.00002107
Iteration 90/1000 | Loss: 0.00002107
Iteration 91/1000 | Loss: 0.00002107
Iteration 92/1000 | Loss: 0.00002107
Iteration 93/1000 | Loss: 0.00002107
Iteration 94/1000 | Loss: 0.00002106
Iteration 95/1000 | Loss: 0.00002106
Iteration 96/1000 | Loss: 0.00002106
Iteration 97/1000 | Loss: 0.00002106
Iteration 98/1000 | Loss: 0.00002106
Iteration 99/1000 | Loss: 0.00002106
Iteration 100/1000 | Loss: 0.00002106
Iteration 101/1000 | Loss: 0.00002105
Iteration 102/1000 | Loss: 0.00002105
Iteration 103/1000 | Loss: 0.00002105
Iteration 104/1000 | Loss: 0.00002105
Iteration 105/1000 | Loss: 0.00002105
Iteration 106/1000 | Loss: 0.00002105
Iteration 107/1000 | Loss: 0.00002105
Iteration 108/1000 | Loss: 0.00002105
Iteration 109/1000 | Loss: 0.00002104
Iteration 110/1000 | Loss: 0.00002104
Iteration 111/1000 | Loss: 0.00002104
Iteration 112/1000 | Loss: 0.00002104
Iteration 113/1000 | Loss: 0.00002103
Iteration 114/1000 | Loss: 0.00002103
Iteration 115/1000 | Loss: 0.00002103
Iteration 116/1000 | Loss: 0.00002103
Iteration 117/1000 | Loss: 0.00002103
Iteration 118/1000 | Loss: 0.00002102
Iteration 119/1000 | Loss: 0.00002102
Iteration 120/1000 | Loss: 0.00002102
Iteration 121/1000 | Loss: 0.00002102
Iteration 122/1000 | Loss: 0.00002102
Iteration 123/1000 | Loss: 0.00002102
Iteration 124/1000 | Loss: 0.00002102
Iteration 125/1000 | Loss: 0.00002102
Iteration 126/1000 | Loss: 0.00002102
Iteration 127/1000 | Loss: 0.00002102
Iteration 128/1000 | Loss: 0.00002102
Iteration 129/1000 | Loss: 0.00002102
Iteration 130/1000 | Loss: 0.00002102
Iteration 131/1000 | Loss: 0.00002102
Iteration 132/1000 | Loss: 0.00002101
Iteration 133/1000 | Loss: 0.00002101
Iteration 134/1000 | Loss: 0.00002101
Iteration 135/1000 | Loss: 0.00002101
Iteration 136/1000 | Loss: 0.00002101
Iteration 137/1000 | Loss: 0.00002101
Iteration 138/1000 | Loss: 0.00002101
Iteration 139/1000 | Loss: 0.00002101
Iteration 140/1000 | Loss: 0.00002100
Iteration 141/1000 | Loss: 0.00002100
Iteration 142/1000 | Loss: 0.00002100
Iteration 143/1000 | Loss: 0.00002100
Iteration 144/1000 | Loss: 0.00002100
Iteration 145/1000 | Loss: 0.00002099
Iteration 146/1000 | Loss: 0.00002099
Iteration 147/1000 | Loss: 0.00002099
Iteration 148/1000 | Loss: 0.00002099
Iteration 149/1000 | Loss: 0.00002099
Iteration 150/1000 | Loss: 0.00002099
Iteration 151/1000 | Loss: 0.00002099
Iteration 152/1000 | Loss: 0.00002099
Iteration 153/1000 | Loss: 0.00002098
Iteration 154/1000 | Loss: 0.00002098
Iteration 155/1000 | Loss: 0.00002098
Iteration 156/1000 | Loss: 0.00002098
Iteration 157/1000 | Loss: 0.00002098
Iteration 158/1000 | Loss: 0.00002098
Iteration 159/1000 | Loss: 0.00002098
Iteration 160/1000 | Loss: 0.00002098
Iteration 161/1000 | Loss: 0.00002098
Iteration 162/1000 | Loss: 0.00002098
Iteration 163/1000 | Loss: 0.00002097
Iteration 164/1000 | Loss: 0.00002097
Iteration 165/1000 | Loss: 0.00002097
Iteration 166/1000 | Loss: 0.00002097
Iteration 167/1000 | Loss: 0.00002097
Iteration 168/1000 | Loss: 0.00002097
Iteration 169/1000 | Loss: 0.00002097
Iteration 170/1000 | Loss: 0.00002097
Iteration 171/1000 | Loss: 0.00002097
Iteration 172/1000 | Loss: 0.00002097
Iteration 173/1000 | Loss: 0.00002097
Iteration 174/1000 | Loss: 0.00002097
Iteration 175/1000 | Loss: 0.00002096
Iteration 176/1000 | Loss: 0.00002096
Iteration 177/1000 | Loss: 0.00002096
Iteration 178/1000 | Loss: 0.00002096
Iteration 179/1000 | Loss: 0.00002096
Iteration 180/1000 | Loss: 0.00002096
Iteration 181/1000 | Loss: 0.00002096
Iteration 182/1000 | Loss: 0.00002096
Iteration 183/1000 | Loss: 0.00002096
Iteration 184/1000 | Loss: 0.00002096
Iteration 185/1000 | Loss: 0.00002096
Iteration 186/1000 | Loss: 0.00002096
Iteration 187/1000 | Loss: 0.00002096
Iteration 188/1000 | Loss: 0.00002096
Iteration 189/1000 | Loss: 0.00002096
Iteration 190/1000 | Loss: 0.00002096
Iteration 191/1000 | Loss: 0.00002095
Iteration 192/1000 | Loss: 0.00002095
Iteration 193/1000 | Loss: 0.00002095
Iteration 194/1000 | Loss: 0.00002095
Iteration 195/1000 | Loss: 0.00002095
Iteration 196/1000 | Loss: 0.00002095
Iteration 197/1000 | Loss: 0.00002095
Iteration 198/1000 | Loss: 0.00002095
Iteration 199/1000 | Loss: 0.00002095
Iteration 200/1000 | Loss: 0.00002095
Iteration 201/1000 | Loss: 0.00002095
Iteration 202/1000 | Loss: 0.00002095
Iteration 203/1000 | Loss: 0.00002095
Iteration 204/1000 | Loss: 0.00002095
Iteration 205/1000 | Loss: 0.00002095
Iteration 206/1000 | Loss: 0.00002095
Iteration 207/1000 | Loss: 0.00002095
Iteration 208/1000 | Loss: 0.00002095
Iteration 209/1000 | Loss: 0.00002095
Iteration 210/1000 | Loss: 0.00002095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.095064519380685e-05, 2.095064519380685e-05, 2.095064519380685e-05, 2.095064519380685e-05, 2.095064519380685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.095064519380685e-05

Optimization complete. Final v2v error: 3.620824098587036 mm

Highest mean error: 5.672270774841309 mm for frame 80

Lowest mean error: 2.892822742462158 mm for frame 131

Saving results

Total time: 49.150492429733276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016107
Iteration 2/25 | Loss: 0.00246170
Iteration 3/25 | Loss: 0.00201318
Iteration 4/25 | Loss: 0.00177250
Iteration 5/25 | Loss: 0.00166480
Iteration 6/25 | Loss: 0.00154977
Iteration 7/25 | Loss: 0.00156052
Iteration 8/25 | Loss: 0.00140576
Iteration 9/25 | Loss: 0.00135658
Iteration 10/25 | Loss: 0.00133508
Iteration 11/25 | Loss: 0.00132747
Iteration 12/25 | Loss: 0.00132468
Iteration 13/25 | Loss: 0.00132728
Iteration 14/25 | Loss: 0.00132686
Iteration 15/25 | Loss: 0.00132748
Iteration 16/25 | Loss: 0.00132446
Iteration 17/25 | Loss: 0.00132168
Iteration 18/25 | Loss: 0.00131685
Iteration 19/25 | Loss: 0.00131661
Iteration 20/25 | Loss: 0.00131655
Iteration 21/25 | Loss: 0.00131655
Iteration 22/25 | Loss: 0.00131655
Iteration 23/25 | Loss: 0.00131655
Iteration 24/25 | Loss: 0.00131655
Iteration 25/25 | Loss: 0.00131655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46978772
Iteration 2/25 | Loss: 0.00070203
Iteration 3/25 | Loss: 0.00064175
Iteration 4/25 | Loss: 0.00064175
Iteration 5/25 | Loss: 0.00064175
Iteration 6/25 | Loss: 0.00064175
Iteration 7/25 | Loss: 0.00064175
Iteration 8/25 | Loss: 0.00064175
Iteration 9/25 | Loss: 0.00064175
Iteration 10/25 | Loss: 0.00064175
Iteration 11/25 | Loss: 0.00064175
Iteration 12/25 | Loss: 0.00064175
Iteration 13/25 | Loss: 0.00064175
Iteration 14/25 | Loss: 0.00064175
Iteration 15/25 | Loss: 0.00064175
Iteration 16/25 | Loss: 0.00064175
Iteration 17/25 | Loss: 0.00064175
Iteration 18/25 | Loss: 0.00064175
Iteration 19/25 | Loss: 0.00064175
Iteration 20/25 | Loss: 0.00064175
Iteration 21/25 | Loss: 0.00064175
Iteration 22/25 | Loss: 0.00064175
Iteration 23/25 | Loss: 0.00064175
Iteration 24/25 | Loss: 0.00064175
Iteration 25/25 | Loss: 0.00064175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064175
Iteration 2/1000 | Loss: 0.00005159
Iteration 3/1000 | Loss: 0.00013229
Iteration 4/1000 | Loss: 0.00003354
Iteration 5/1000 | Loss: 0.00002969
Iteration 6/1000 | Loss: 0.00022206
Iteration 7/1000 | Loss: 0.00002881
Iteration 8/1000 | Loss: 0.00002775
Iteration 9/1000 | Loss: 0.00002713
Iteration 10/1000 | Loss: 0.00017244
Iteration 11/1000 | Loss: 0.00002761
Iteration 12/1000 | Loss: 0.00002650
Iteration 13/1000 | Loss: 0.00002632
Iteration 14/1000 | Loss: 0.00013468
Iteration 15/1000 | Loss: 0.00004707
Iteration 16/1000 | Loss: 0.00006539
Iteration 17/1000 | Loss: 0.00002591
Iteration 18/1000 | Loss: 0.00002590
Iteration 19/1000 | Loss: 0.00002590
Iteration 20/1000 | Loss: 0.00002590
Iteration 21/1000 | Loss: 0.00002590
Iteration 22/1000 | Loss: 0.00002590
Iteration 23/1000 | Loss: 0.00002590
Iteration 24/1000 | Loss: 0.00002586
Iteration 25/1000 | Loss: 0.00002586
Iteration 26/1000 | Loss: 0.00002576
Iteration 27/1000 | Loss: 0.00002576
Iteration 28/1000 | Loss: 0.00002572
Iteration 29/1000 | Loss: 0.00002571
Iteration 30/1000 | Loss: 0.00002571
Iteration 31/1000 | Loss: 0.00002569
Iteration 32/1000 | Loss: 0.00002569
Iteration 33/1000 | Loss: 0.00002566
Iteration 34/1000 | Loss: 0.00002566
Iteration 35/1000 | Loss: 0.00002565
Iteration 36/1000 | Loss: 0.00002564
Iteration 37/1000 | Loss: 0.00002563
Iteration 38/1000 | Loss: 0.00002563
Iteration 39/1000 | Loss: 0.00002563
Iteration 40/1000 | Loss: 0.00002562
Iteration 41/1000 | Loss: 0.00002562
Iteration 42/1000 | Loss: 0.00002562
Iteration 43/1000 | Loss: 0.00002561
Iteration 44/1000 | Loss: 0.00002561
Iteration 45/1000 | Loss: 0.00002561
Iteration 46/1000 | Loss: 0.00002560
Iteration 47/1000 | Loss: 0.00002560
Iteration 48/1000 | Loss: 0.00002560
Iteration 49/1000 | Loss: 0.00002560
Iteration 50/1000 | Loss: 0.00002560
Iteration 51/1000 | Loss: 0.00002560
Iteration 52/1000 | Loss: 0.00002560
Iteration 53/1000 | Loss: 0.00002560
Iteration 54/1000 | Loss: 0.00002559
Iteration 55/1000 | Loss: 0.00002559
Iteration 56/1000 | Loss: 0.00002558
Iteration 57/1000 | Loss: 0.00002558
Iteration 58/1000 | Loss: 0.00002557
Iteration 59/1000 | Loss: 0.00002557
Iteration 60/1000 | Loss: 0.00002557
Iteration 61/1000 | Loss: 0.00002556
Iteration 62/1000 | Loss: 0.00002556
Iteration 63/1000 | Loss: 0.00002555
Iteration 64/1000 | Loss: 0.00002555
Iteration 65/1000 | Loss: 0.00002555
Iteration 66/1000 | Loss: 0.00002555
Iteration 67/1000 | Loss: 0.00002555
Iteration 68/1000 | Loss: 0.00002554
Iteration 69/1000 | Loss: 0.00002554
Iteration 70/1000 | Loss: 0.00002554
Iteration 71/1000 | Loss: 0.00002554
Iteration 72/1000 | Loss: 0.00002554
Iteration 73/1000 | Loss: 0.00002554
Iteration 74/1000 | Loss: 0.00002554
Iteration 75/1000 | Loss: 0.00002554
Iteration 76/1000 | Loss: 0.00002554
Iteration 77/1000 | Loss: 0.00002554
Iteration 78/1000 | Loss: 0.00002554
Iteration 79/1000 | Loss: 0.00002553
Iteration 80/1000 | Loss: 0.00002553
Iteration 81/1000 | Loss: 0.00002552
Iteration 82/1000 | Loss: 0.00002551
Iteration 83/1000 | Loss: 0.00002551
Iteration 84/1000 | Loss: 0.00002550
Iteration 85/1000 | Loss: 0.00002550
Iteration 86/1000 | Loss: 0.00002550
Iteration 87/1000 | Loss: 0.00002550
Iteration 88/1000 | Loss: 0.00002550
Iteration 89/1000 | Loss: 0.00002550
Iteration 90/1000 | Loss: 0.00002550
Iteration 91/1000 | Loss: 0.00002550
Iteration 92/1000 | Loss: 0.00002550
Iteration 93/1000 | Loss: 0.00002549
Iteration 94/1000 | Loss: 0.00002549
Iteration 95/1000 | Loss: 0.00002549
Iteration 96/1000 | Loss: 0.00002548
Iteration 97/1000 | Loss: 0.00002548
Iteration 98/1000 | Loss: 0.00002548
Iteration 99/1000 | Loss: 0.00002548
Iteration 100/1000 | Loss: 0.00002548
Iteration 101/1000 | Loss: 0.00002547
Iteration 102/1000 | Loss: 0.00002547
Iteration 103/1000 | Loss: 0.00002547
Iteration 104/1000 | Loss: 0.00002547
Iteration 105/1000 | Loss: 0.00002547
Iteration 106/1000 | Loss: 0.00002547
Iteration 107/1000 | Loss: 0.00002546
Iteration 108/1000 | Loss: 0.00002546
Iteration 109/1000 | Loss: 0.00002546
Iteration 110/1000 | Loss: 0.00002545
Iteration 111/1000 | Loss: 0.00002545
Iteration 112/1000 | Loss: 0.00002545
Iteration 113/1000 | Loss: 0.00002545
Iteration 114/1000 | Loss: 0.00002544
Iteration 115/1000 | Loss: 0.00002544
Iteration 116/1000 | Loss: 0.00002544
Iteration 117/1000 | Loss: 0.00002544
Iteration 118/1000 | Loss: 0.00002543
Iteration 119/1000 | Loss: 0.00002543
Iteration 120/1000 | Loss: 0.00002543
Iteration 121/1000 | Loss: 0.00002543
Iteration 122/1000 | Loss: 0.00002543
Iteration 123/1000 | Loss: 0.00002543
Iteration 124/1000 | Loss: 0.00002543
Iteration 125/1000 | Loss: 0.00002543
Iteration 126/1000 | Loss: 0.00002543
Iteration 127/1000 | Loss: 0.00002543
Iteration 128/1000 | Loss: 0.00002543
Iteration 129/1000 | Loss: 0.00002543
Iteration 130/1000 | Loss: 0.00002543
Iteration 131/1000 | Loss: 0.00002543
Iteration 132/1000 | Loss: 0.00002543
Iteration 133/1000 | Loss: 0.00002543
Iteration 134/1000 | Loss: 0.00002543
Iteration 135/1000 | Loss: 0.00002543
Iteration 136/1000 | Loss: 0.00002543
Iteration 137/1000 | Loss: 0.00002543
Iteration 138/1000 | Loss: 0.00002543
Iteration 139/1000 | Loss: 0.00002543
Iteration 140/1000 | Loss: 0.00002543
Iteration 141/1000 | Loss: 0.00002543
Iteration 142/1000 | Loss: 0.00002543
Iteration 143/1000 | Loss: 0.00002543
Iteration 144/1000 | Loss: 0.00002543
Iteration 145/1000 | Loss: 0.00002543
Iteration 146/1000 | Loss: 0.00002543
Iteration 147/1000 | Loss: 0.00002543
Iteration 148/1000 | Loss: 0.00002543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.542651600379031e-05, 2.542651600379031e-05, 2.542651600379031e-05, 2.542651600379031e-05, 2.542651600379031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.542651600379031e-05

Optimization complete. Final v2v error: 4.216462135314941 mm

Highest mean error: 4.563601970672607 mm for frame 47

Lowest mean error: 3.8166017532348633 mm for frame 58

Saving results

Total time: 78.8410656452179
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067574
Iteration 2/25 | Loss: 0.00308908
Iteration 3/25 | Loss: 0.00237449
Iteration 4/25 | Loss: 0.00204205
Iteration 5/25 | Loss: 0.00193438
Iteration 6/25 | Loss: 0.00185682
Iteration 7/25 | Loss: 0.00183801
Iteration 8/25 | Loss: 0.00178002
Iteration 9/25 | Loss: 0.00170552
Iteration 10/25 | Loss: 0.00163056
Iteration 11/25 | Loss: 0.00159813
Iteration 12/25 | Loss: 0.00157217
Iteration 13/25 | Loss: 0.00156622
Iteration 14/25 | Loss: 0.00156190
Iteration 15/25 | Loss: 0.00155644
Iteration 16/25 | Loss: 0.00154922
Iteration 17/25 | Loss: 0.00159127
Iteration 18/25 | Loss: 0.00157474
Iteration 19/25 | Loss: 0.00156576
Iteration 20/25 | Loss: 0.00153781
Iteration 21/25 | Loss: 0.00151914
Iteration 22/25 | Loss: 0.00151787
Iteration 23/25 | Loss: 0.00151730
Iteration 24/25 | Loss: 0.00151975
Iteration 25/25 | Loss: 0.00151932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13654637
Iteration 2/25 | Loss: 0.00159515
Iteration 3/25 | Loss: 0.00125633
Iteration 4/25 | Loss: 0.00125633
Iteration 5/25 | Loss: 0.00125632
Iteration 6/25 | Loss: 0.00125632
Iteration 7/25 | Loss: 0.00125632
Iteration 8/25 | Loss: 0.00125632
Iteration 9/25 | Loss: 0.00125632
Iteration 10/25 | Loss: 0.00125632
Iteration 11/25 | Loss: 0.00125632
Iteration 12/25 | Loss: 0.00125632
Iteration 13/25 | Loss: 0.00125632
Iteration 14/25 | Loss: 0.00125632
Iteration 15/25 | Loss: 0.00125632
Iteration 16/25 | Loss: 0.00125632
Iteration 17/25 | Loss: 0.00125632
Iteration 18/25 | Loss: 0.00125632
Iteration 19/25 | Loss: 0.00125632
Iteration 20/25 | Loss: 0.00125632
Iteration 21/25 | Loss: 0.00125632
Iteration 22/25 | Loss: 0.00125632
Iteration 23/25 | Loss: 0.00125632
Iteration 24/25 | Loss: 0.00125632
Iteration 25/25 | Loss: 0.00125632

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125632
Iteration 2/1000 | Loss: 0.00016783
Iteration 3/1000 | Loss: 0.00041073
Iteration 4/1000 | Loss: 0.00007442
Iteration 5/1000 | Loss: 0.00027274
Iteration 6/1000 | Loss: 0.00010401
Iteration 7/1000 | Loss: 0.00036861
Iteration 8/1000 | Loss: 0.00022884
Iteration 9/1000 | Loss: 0.00011676
Iteration 10/1000 | Loss: 0.00009519
Iteration 11/1000 | Loss: 0.00009536
Iteration 12/1000 | Loss: 0.00019122
Iteration 13/1000 | Loss: 0.00261203
Iteration 14/1000 | Loss: 0.00119366
Iteration 15/1000 | Loss: 0.00291775
Iteration 16/1000 | Loss: 0.00087930
Iteration 17/1000 | Loss: 0.00078385
Iteration 18/1000 | Loss: 0.00020427
Iteration 19/1000 | Loss: 0.00030370
Iteration 20/1000 | Loss: 0.00029991
Iteration 21/1000 | Loss: 0.00011558
Iteration 22/1000 | Loss: 0.00013217
Iteration 23/1000 | Loss: 0.00011504
Iteration 24/1000 | Loss: 0.00014054
Iteration 25/1000 | Loss: 0.00048661
Iteration 26/1000 | Loss: 0.00063174
Iteration 27/1000 | Loss: 0.00104534
Iteration 28/1000 | Loss: 0.00026939
Iteration 29/1000 | Loss: 0.00061992
Iteration 30/1000 | Loss: 0.00012441
Iteration 31/1000 | Loss: 0.00009964
Iteration 32/1000 | Loss: 0.00054701
Iteration 33/1000 | Loss: 0.00282707
Iteration 34/1000 | Loss: 0.00021899
Iteration 35/1000 | Loss: 0.00020729
Iteration 36/1000 | Loss: 0.00024477
Iteration 37/1000 | Loss: 0.00046759
Iteration 38/1000 | Loss: 0.00321528
Iteration 39/1000 | Loss: 0.00259244
Iteration 40/1000 | Loss: 0.00097559
Iteration 41/1000 | Loss: 0.00012851
Iteration 42/1000 | Loss: 0.00019092
Iteration 43/1000 | Loss: 0.00044219
Iteration 44/1000 | Loss: 0.00113172
Iteration 45/1000 | Loss: 0.00100311
Iteration 46/1000 | Loss: 0.00110844
Iteration 47/1000 | Loss: 0.00028172
Iteration 48/1000 | Loss: 0.00031532
Iteration 49/1000 | Loss: 0.00030502
Iteration 50/1000 | Loss: 0.00022879
Iteration 51/1000 | Loss: 0.00027934
Iteration 52/1000 | Loss: 0.00019376
Iteration 53/1000 | Loss: 0.00007715
Iteration 54/1000 | Loss: 0.00025051
Iteration 55/1000 | Loss: 0.00012770
Iteration 56/1000 | Loss: 0.00014383
Iteration 57/1000 | Loss: 0.00006679
Iteration 58/1000 | Loss: 0.00021459
Iteration 59/1000 | Loss: 0.00019918
Iteration 60/1000 | Loss: 0.00029224
Iteration 61/1000 | Loss: 0.00018339
Iteration 62/1000 | Loss: 0.00031481
Iteration 63/1000 | Loss: 0.00005647
Iteration 64/1000 | Loss: 0.00005229
Iteration 65/1000 | Loss: 0.00004973
Iteration 66/1000 | Loss: 0.00014277
Iteration 67/1000 | Loss: 0.00024201
Iteration 68/1000 | Loss: 0.00021149
Iteration 69/1000 | Loss: 0.00028570
Iteration 70/1000 | Loss: 0.00008458
Iteration 71/1000 | Loss: 0.00006905
Iteration 72/1000 | Loss: 0.00032422
Iteration 73/1000 | Loss: 0.00022267
Iteration 74/1000 | Loss: 0.00022316
Iteration 75/1000 | Loss: 0.00030486
Iteration 76/1000 | Loss: 0.00008811
Iteration 77/1000 | Loss: 0.00004529
Iteration 78/1000 | Loss: 0.00014095
Iteration 79/1000 | Loss: 0.00008324
Iteration 80/1000 | Loss: 0.00038460
Iteration 81/1000 | Loss: 0.00005352
Iteration 82/1000 | Loss: 0.00004409
Iteration 83/1000 | Loss: 0.00004233
Iteration 84/1000 | Loss: 0.00009376
Iteration 85/1000 | Loss: 0.00013366
Iteration 86/1000 | Loss: 0.00006061
Iteration 87/1000 | Loss: 0.00004010
Iteration 88/1000 | Loss: 0.00007048
Iteration 89/1000 | Loss: 0.00008740
Iteration 90/1000 | Loss: 0.00006491
Iteration 91/1000 | Loss: 0.00016982
Iteration 92/1000 | Loss: 0.00006668
Iteration 93/1000 | Loss: 0.00003948
Iteration 94/1000 | Loss: 0.00003925
Iteration 95/1000 | Loss: 0.00006526
Iteration 96/1000 | Loss: 0.00007852
Iteration 97/1000 | Loss: 0.00004207
Iteration 98/1000 | Loss: 0.00003897
Iteration 99/1000 | Loss: 0.00003894
Iteration 100/1000 | Loss: 0.00005876
Iteration 101/1000 | Loss: 0.00006257
Iteration 102/1000 | Loss: 0.00004360
Iteration 103/1000 | Loss: 0.00005833
Iteration 104/1000 | Loss: 0.00005832
Iteration 105/1000 | Loss: 0.00018846
Iteration 106/1000 | Loss: 0.00005165
Iteration 107/1000 | Loss: 0.00004223
Iteration 108/1000 | Loss: 0.00005035
Iteration 109/1000 | Loss: 0.00005223
Iteration 110/1000 | Loss: 0.00003890
Iteration 111/1000 | Loss: 0.00007305
Iteration 112/1000 | Loss: 0.00005147
Iteration 113/1000 | Loss: 0.00005684
Iteration 114/1000 | Loss: 0.00007714
Iteration 115/1000 | Loss: 0.00004312
Iteration 116/1000 | Loss: 0.00008778
Iteration 117/1000 | Loss: 0.00009339
Iteration 118/1000 | Loss: 0.00029462
Iteration 119/1000 | Loss: 0.00004902
Iteration 120/1000 | Loss: 0.00005555
Iteration 121/1000 | Loss: 0.00003938
Iteration 122/1000 | Loss: 0.00004241
Iteration 123/1000 | Loss: 0.00005095
Iteration 124/1000 | Loss: 0.00004988
Iteration 125/1000 | Loss: 0.00006161
Iteration 126/1000 | Loss: 0.00005159
Iteration 127/1000 | Loss: 0.00005972
Iteration 128/1000 | Loss: 0.00010164
Iteration 129/1000 | Loss: 0.00004618
Iteration 130/1000 | Loss: 0.00016670
Iteration 131/1000 | Loss: 0.00007462
Iteration 132/1000 | Loss: 0.00005761
Iteration 133/1000 | Loss: 0.00013801
Iteration 134/1000 | Loss: 0.00011091
Iteration 135/1000 | Loss: 0.00008241
Iteration 136/1000 | Loss: 0.00012380
Iteration 137/1000 | Loss: 0.00005251
Iteration 138/1000 | Loss: 0.00006131
Iteration 139/1000 | Loss: 0.00007250
Iteration 140/1000 | Loss: 0.00007834
Iteration 141/1000 | Loss: 0.00006287
Iteration 142/1000 | Loss: 0.00006359
Iteration 143/1000 | Loss: 0.00005410
Iteration 144/1000 | Loss: 0.00006834
Iteration 145/1000 | Loss: 0.00007385
Iteration 146/1000 | Loss: 0.00006974
Iteration 147/1000 | Loss: 0.00013303
Iteration 148/1000 | Loss: 0.00007984
Iteration 149/1000 | Loss: 0.00007434
Iteration 150/1000 | Loss: 0.00006738
Iteration 151/1000 | Loss: 0.00008370
Iteration 152/1000 | Loss: 0.00009204
Iteration 153/1000 | Loss: 0.00006900
Iteration 154/1000 | Loss: 0.00006530
Iteration 155/1000 | Loss: 0.00008168
Iteration 156/1000 | Loss: 0.00006707
Iteration 157/1000 | Loss: 0.00007720
Iteration 158/1000 | Loss: 0.00008749
Iteration 159/1000 | Loss: 0.00007989
Iteration 160/1000 | Loss: 0.00008113
Iteration 161/1000 | Loss: 0.00004704
Iteration 162/1000 | Loss: 0.00004341
Iteration 163/1000 | Loss: 0.00004124
Iteration 164/1000 | Loss: 0.00005409
Iteration 165/1000 | Loss: 0.00008331
Iteration 166/1000 | Loss: 0.00003921
Iteration 167/1000 | Loss: 0.00003879
Iteration 168/1000 | Loss: 0.00003866
Iteration 169/1000 | Loss: 0.00003855
Iteration 170/1000 | Loss: 0.00003855
Iteration 171/1000 | Loss: 0.00003854
Iteration 172/1000 | Loss: 0.00003854
Iteration 173/1000 | Loss: 0.00003853
Iteration 174/1000 | Loss: 0.00006716
Iteration 175/1000 | Loss: 0.00003856
Iteration 176/1000 | Loss: 0.00003848
Iteration 177/1000 | Loss: 0.00003847
Iteration 178/1000 | Loss: 0.00003847
Iteration 179/1000 | Loss: 0.00003847
Iteration 180/1000 | Loss: 0.00008091
Iteration 181/1000 | Loss: 0.00003881
Iteration 182/1000 | Loss: 0.00003839
Iteration 183/1000 | Loss: 0.00003838
Iteration 184/1000 | Loss: 0.00003837
Iteration 185/1000 | Loss: 0.00003837
Iteration 186/1000 | Loss: 0.00003837
Iteration 187/1000 | Loss: 0.00003837
Iteration 188/1000 | Loss: 0.00003837
Iteration 189/1000 | Loss: 0.00003837
Iteration 190/1000 | Loss: 0.00003837
Iteration 191/1000 | Loss: 0.00003837
Iteration 192/1000 | Loss: 0.00003837
Iteration 193/1000 | Loss: 0.00003837
Iteration 194/1000 | Loss: 0.00003837
Iteration 195/1000 | Loss: 0.00003836
Iteration 196/1000 | Loss: 0.00003836
Iteration 197/1000 | Loss: 0.00003835
Iteration 198/1000 | Loss: 0.00003835
Iteration 199/1000 | Loss: 0.00003835
Iteration 200/1000 | Loss: 0.00003835
Iteration 201/1000 | Loss: 0.00003835
Iteration 202/1000 | Loss: 0.00003834
Iteration 203/1000 | Loss: 0.00003834
Iteration 204/1000 | Loss: 0.00003834
Iteration 205/1000 | Loss: 0.00003834
Iteration 206/1000 | Loss: 0.00003834
Iteration 207/1000 | Loss: 0.00006573
Iteration 208/1000 | Loss: 0.00005950
Iteration 209/1000 | Loss: 0.00006256
Iteration 210/1000 | Loss: 0.00009487
Iteration 211/1000 | Loss: 0.00004289
Iteration 212/1000 | Loss: 0.00004430
Iteration 213/1000 | Loss: 0.00004260
Iteration 214/1000 | Loss: 0.00003839
Iteration 215/1000 | Loss: 0.00004575
Iteration 216/1000 | Loss: 0.00003938
Iteration 217/1000 | Loss: 0.00004016
Iteration 218/1000 | Loss: 0.00003832
Iteration 219/1000 | Loss: 0.00003832
Iteration 220/1000 | Loss: 0.00003832
Iteration 221/1000 | Loss: 0.00003832
Iteration 222/1000 | Loss: 0.00003831
Iteration 223/1000 | Loss: 0.00003831
Iteration 224/1000 | Loss: 0.00003831
Iteration 225/1000 | Loss: 0.00003831
Iteration 226/1000 | Loss: 0.00003831
Iteration 227/1000 | Loss: 0.00003831
Iteration 228/1000 | Loss: 0.00003831
Iteration 229/1000 | Loss: 0.00003831
Iteration 230/1000 | Loss: 0.00003831
Iteration 231/1000 | Loss: 0.00003831
Iteration 232/1000 | Loss: 0.00003830
Iteration 233/1000 | Loss: 0.00003827
Iteration 234/1000 | Loss: 0.00003827
Iteration 235/1000 | Loss: 0.00003827
Iteration 236/1000 | Loss: 0.00003827
Iteration 237/1000 | Loss: 0.00003826
Iteration 238/1000 | Loss: 0.00003826
Iteration 239/1000 | Loss: 0.00003826
Iteration 240/1000 | Loss: 0.00003826
Iteration 241/1000 | Loss: 0.00003826
Iteration 242/1000 | Loss: 0.00003826
Iteration 243/1000 | Loss: 0.00003826
Iteration 244/1000 | Loss: 0.00003826
Iteration 245/1000 | Loss: 0.00003826
Iteration 246/1000 | Loss: 0.00003826
Iteration 247/1000 | Loss: 0.00003826
Iteration 248/1000 | Loss: 0.00003825
Iteration 249/1000 | Loss: 0.00003825
Iteration 250/1000 | Loss: 0.00003825
Iteration 251/1000 | Loss: 0.00003825
Iteration 252/1000 | Loss: 0.00003825
Iteration 253/1000 | Loss: 0.00003825
Iteration 254/1000 | Loss: 0.00003825
Iteration 255/1000 | Loss: 0.00003825
Iteration 256/1000 | Loss: 0.00003824
Iteration 257/1000 | Loss: 0.00003824
Iteration 258/1000 | Loss: 0.00003824
Iteration 259/1000 | Loss: 0.00003824
Iteration 260/1000 | Loss: 0.00003824
Iteration 261/1000 | Loss: 0.00003824
Iteration 262/1000 | Loss: 0.00003824
Iteration 263/1000 | Loss: 0.00003824
Iteration 264/1000 | Loss: 0.00003824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [3.8242935261223465e-05, 3.8242935261223465e-05, 3.8242935261223465e-05, 3.8242935261223465e-05, 3.8242935261223465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8242935261223465e-05

Optimization complete. Final v2v error: 4.913896560668945 mm

Highest mean error: 12.017565727233887 mm for frame 91

Lowest mean error: 3.9370172023773193 mm for frame 0

Saving results

Total time: 341.4389214515686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00572587
Iteration 2/25 | Loss: 0.00158148
Iteration 3/25 | Loss: 0.00135025
Iteration 4/25 | Loss: 0.00132141
Iteration 5/25 | Loss: 0.00131516
Iteration 6/25 | Loss: 0.00131329
Iteration 7/25 | Loss: 0.00131221
Iteration 8/25 | Loss: 0.00131138
Iteration 9/25 | Loss: 0.00131536
Iteration 10/25 | Loss: 0.00131661
Iteration 11/25 | Loss: 0.00131787
Iteration 12/25 | Loss: 0.00131186
Iteration 13/25 | Loss: 0.00130944
Iteration 14/25 | Loss: 0.00130570
Iteration 15/25 | Loss: 0.00130401
Iteration 16/25 | Loss: 0.00130327
Iteration 17/25 | Loss: 0.00130315
Iteration 18/25 | Loss: 0.00130451
Iteration 19/25 | Loss: 0.00130451
Iteration 20/25 | Loss: 0.00130451
Iteration 21/25 | Loss: 0.00130451
Iteration 22/25 | Loss: 0.00130451
Iteration 23/25 | Loss: 0.00130451
Iteration 24/25 | Loss: 0.00130451
Iteration 25/25 | Loss: 0.00130451

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53263628
Iteration 2/25 | Loss: 0.00077423
Iteration 3/25 | Loss: 0.00077422
Iteration 4/25 | Loss: 0.00077422
Iteration 5/25 | Loss: 0.00077422
Iteration 6/25 | Loss: 0.00077422
Iteration 7/25 | Loss: 0.00077422
Iteration 8/25 | Loss: 0.00077422
Iteration 9/25 | Loss: 0.00077422
Iteration 10/25 | Loss: 0.00077422
Iteration 11/25 | Loss: 0.00077422
Iteration 12/25 | Loss: 0.00077422
Iteration 13/25 | Loss: 0.00077422
Iteration 14/25 | Loss: 0.00077422
Iteration 15/25 | Loss: 0.00077422
Iteration 16/25 | Loss: 0.00077422
Iteration 17/25 | Loss: 0.00077422
Iteration 18/25 | Loss: 0.00077422
Iteration 19/25 | Loss: 0.00077422
Iteration 20/25 | Loss: 0.00077422
Iteration 21/25 | Loss: 0.00077422
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007742164889350533, 0.0007742164889350533, 0.0007742164889350533, 0.0007742164889350533, 0.0007742164889350533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007742164889350533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077422
Iteration 2/1000 | Loss: 0.00005678
Iteration 3/1000 | Loss: 0.00004253
Iteration 4/1000 | Loss: 0.00003054
Iteration 5/1000 | Loss: 0.00002821
Iteration 6/1000 | Loss: 0.00002692
Iteration 7/1000 | Loss: 0.00002541
Iteration 8/1000 | Loss: 0.00002507
Iteration 9/1000 | Loss: 0.00002388
Iteration 10/1000 | Loss: 0.00002344
Iteration 11/1000 | Loss: 0.00002290
Iteration 12/1000 | Loss: 0.00002250
Iteration 13/1000 | Loss: 0.00002217
Iteration 14/1000 | Loss: 0.00002188
Iteration 15/1000 | Loss: 0.00002187
Iteration 16/1000 | Loss: 0.00002167
Iteration 17/1000 | Loss: 0.00002162
Iteration 18/1000 | Loss: 0.00002150
Iteration 19/1000 | Loss: 0.00002146
Iteration 20/1000 | Loss: 0.00002143
Iteration 21/1000 | Loss: 0.00002130
Iteration 22/1000 | Loss: 0.00002127
Iteration 23/1000 | Loss: 0.00002121
Iteration 24/1000 | Loss: 0.00002117
Iteration 25/1000 | Loss: 0.00002117
Iteration 26/1000 | Loss: 0.00002116
Iteration 27/1000 | Loss: 0.00002116
Iteration 28/1000 | Loss: 0.00002115
Iteration 29/1000 | Loss: 0.00002115
Iteration 30/1000 | Loss: 0.00002114
Iteration 31/1000 | Loss: 0.00002113
Iteration 32/1000 | Loss: 0.00002113
Iteration 33/1000 | Loss: 0.00002112
Iteration 34/1000 | Loss: 0.00002112
Iteration 35/1000 | Loss: 0.00002111
Iteration 36/1000 | Loss: 0.00002111
Iteration 37/1000 | Loss: 0.00002111
Iteration 38/1000 | Loss: 0.00002110
Iteration 39/1000 | Loss: 0.00002110
Iteration 40/1000 | Loss: 0.00002109
Iteration 41/1000 | Loss: 0.00002108
Iteration 42/1000 | Loss: 0.00002107
Iteration 43/1000 | Loss: 0.00002107
Iteration 44/1000 | Loss: 0.00002106
Iteration 45/1000 | Loss: 0.00002105
Iteration 46/1000 | Loss: 0.00002104
Iteration 47/1000 | Loss: 0.00002103
Iteration 48/1000 | Loss: 0.00002102
Iteration 49/1000 | Loss: 0.00002102
Iteration 50/1000 | Loss: 0.00002101
Iteration 51/1000 | Loss: 0.00002101
Iteration 52/1000 | Loss: 0.00002101
Iteration 53/1000 | Loss: 0.00002101
Iteration 54/1000 | Loss: 0.00002101
Iteration 55/1000 | Loss: 0.00002101
Iteration 56/1000 | Loss: 0.00002100
Iteration 57/1000 | Loss: 0.00002100
Iteration 58/1000 | Loss: 0.00002100
Iteration 59/1000 | Loss: 0.00002100
Iteration 60/1000 | Loss: 0.00002100
Iteration 61/1000 | Loss: 0.00002100
Iteration 62/1000 | Loss: 0.00002100
Iteration 63/1000 | Loss: 0.00002100
Iteration 64/1000 | Loss: 0.00002099
Iteration 65/1000 | Loss: 0.00002099
Iteration 66/1000 | Loss: 0.00002099
Iteration 67/1000 | Loss: 0.00002099
Iteration 68/1000 | Loss: 0.00002099
Iteration 69/1000 | Loss: 0.00002098
Iteration 70/1000 | Loss: 0.00002098
Iteration 71/1000 | Loss: 0.00002098
Iteration 72/1000 | Loss: 0.00002098
Iteration 73/1000 | Loss: 0.00002098
Iteration 74/1000 | Loss: 0.00002097
Iteration 75/1000 | Loss: 0.00002097
Iteration 76/1000 | Loss: 0.00002097
Iteration 77/1000 | Loss: 0.00002097
Iteration 78/1000 | Loss: 0.00002097
Iteration 79/1000 | Loss: 0.00002097
Iteration 80/1000 | Loss: 0.00002096
Iteration 81/1000 | Loss: 0.00002096
Iteration 82/1000 | Loss: 0.00002096
Iteration 83/1000 | Loss: 0.00002096
Iteration 84/1000 | Loss: 0.00002095
Iteration 85/1000 | Loss: 0.00002094
Iteration 86/1000 | Loss: 0.00002094
Iteration 87/1000 | Loss: 0.00002094
Iteration 88/1000 | Loss: 0.00002093
Iteration 89/1000 | Loss: 0.00002093
Iteration 90/1000 | Loss: 0.00002093
Iteration 91/1000 | Loss: 0.00002093
Iteration 92/1000 | Loss: 0.00002092
Iteration 93/1000 | Loss: 0.00002092
Iteration 94/1000 | Loss: 0.00002092
Iteration 95/1000 | Loss: 0.00002092
Iteration 96/1000 | Loss: 0.00002092
Iteration 97/1000 | Loss: 0.00002092
Iteration 98/1000 | Loss: 0.00002092
Iteration 99/1000 | Loss: 0.00002092
Iteration 100/1000 | Loss: 0.00002091
Iteration 101/1000 | Loss: 0.00002091
Iteration 102/1000 | Loss: 0.00002091
Iteration 103/1000 | Loss: 0.00002090
Iteration 104/1000 | Loss: 0.00002090
Iteration 105/1000 | Loss: 0.00002090
Iteration 106/1000 | Loss: 0.00002089
Iteration 107/1000 | Loss: 0.00002089
Iteration 108/1000 | Loss: 0.00002089
Iteration 109/1000 | Loss: 0.00002089
Iteration 110/1000 | Loss: 0.00002088
Iteration 111/1000 | Loss: 0.00002088
Iteration 112/1000 | Loss: 0.00002088
Iteration 113/1000 | Loss: 0.00002088
Iteration 114/1000 | Loss: 0.00002088
Iteration 115/1000 | Loss: 0.00002088
Iteration 116/1000 | Loss: 0.00002088
Iteration 117/1000 | Loss: 0.00002088
Iteration 118/1000 | Loss: 0.00002088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.0877956558251753e-05, 2.0877956558251753e-05, 2.0877956558251753e-05, 2.0877956558251753e-05, 2.0877956558251753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0877956558251753e-05

Optimization complete. Final v2v error: 3.823124885559082 mm

Highest mean error: 4.730928897857666 mm for frame 133

Lowest mean error: 3.006781816482544 mm for frame 74

Saving results

Total time: 73.99970555305481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00511364
Iteration 2/25 | Loss: 0.00137750
Iteration 3/25 | Loss: 0.00126782
Iteration 4/25 | Loss: 0.00125155
Iteration 5/25 | Loss: 0.00124581
Iteration 6/25 | Loss: 0.00124466
Iteration 7/25 | Loss: 0.00124457
Iteration 8/25 | Loss: 0.00124457
Iteration 9/25 | Loss: 0.00124457
Iteration 10/25 | Loss: 0.00124457
Iteration 11/25 | Loss: 0.00124457
Iteration 12/25 | Loss: 0.00124457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00124457199126482, 0.00124457199126482, 0.00124457199126482, 0.00124457199126482, 0.00124457199126482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00124457199126482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.43652582
Iteration 2/25 | Loss: 0.00087491
Iteration 3/25 | Loss: 0.00087490
Iteration 4/25 | Loss: 0.00087490
Iteration 5/25 | Loss: 0.00087490
Iteration 6/25 | Loss: 0.00087490
Iteration 7/25 | Loss: 0.00087490
Iteration 8/25 | Loss: 0.00087490
Iteration 9/25 | Loss: 0.00087490
Iteration 10/25 | Loss: 0.00087490
Iteration 11/25 | Loss: 0.00087490
Iteration 12/25 | Loss: 0.00087490
Iteration 13/25 | Loss: 0.00087490
Iteration 14/25 | Loss: 0.00087490
Iteration 15/25 | Loss: 0.00087490
Iteration 16/25 | Loss: 0.00087490
Iteration 17/25 | Loss: 0.00087490
Iteration 18/25 | Loss: 0.00087490
Iteration 19/25 | Loss: 0.00087490
Iteration 20/25 | Loss: 0.00087490
Iteration 21/25 | Loss: 0.00087490
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008749015396460891, 0.0008749015396460891, 0.0008749015396460891, 0.0008749015396460891, 0.0008749015396460891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008749015396460891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087490
Iteration 2/1000 | Loss: 0.00004084
Iteration 3/1000 | Loss: 0.00002610
Iteration 4/1000 | Loss: 0.00002140
Iteration 5/1000 | Loss: 0.00001998
Iteration 6/1000 | Loss: 0.00001898
Iteration 7/1000 | Loss: 0.00001836
Iteration 8/1000 | Loss: 0.00001783
Iteration 9/1000 | Loss: 0.00001743
Iteration 10/1000 | Loss: 0.00001713
Iteration 11/1000 | Loss: 0.00001692
Iteration 12/1000 | Loss: 0.00001678
Iteration 13/1000 | Loss: 0.00001677
Iteration 14/1000 | Loss: 0.00001668
Iteration 15/1000 | Loss: 0.00001664
Iteration 16/1000 | Loss: 0.00001664
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001662
Iteration 20/1000 | Loss: 0.00001661
Iteration 21/1000 | Loss: 0.00001660
Iteration 22/1000 | Loss: 0.00001659
Iteration 23/1000 | Loss: 0.00001659
Iteration 24/1000 | Loss: 0.00001659
Iteration 25/1000 | Loss: 0.00001659
Iteration 26/1000 | Loss: 0.00001658
Iteration 27/1000 | Loss: 0.00001658
Iteration 28/1000 | Loss: 0.00001657
Iteration 29/1000 | Loss: 0.00001656
Iteration 30/1000 | Loss: 0.00001656
Iteration 31/1000 | Loss: 0.00001655
Iteration 32/1000 | Loss: 0.00001655
Iteration 33/1000 | Loss: 0.00001655
Iteration 34/1000 | Loss: 0.00001654
Iteration 35/1000 | Loss: 0.00001653
Iteration 36/1000 | Loss: 0.00001653
Iteration 37/1000 | Loss: 0.00001652
Iteration 38/1000 | Loss: 0.00001652
Iteration 39/1000 | Loss: 0.00001652
Iteration 40/1000 | Loss: 0.00001650
Iteration 41/1000 | Loss: 0.00001649
Iteration 42/1000 | Loss: 0.00001649
Iteration 43/1000 | Loss: 0.00001648
Iteration 44/1000 | Loss: 0.00001648
Iteration 45/1000 | Loss: 0.00001647
Iteration 46/1000 | Loss: 0.00001647
Iteration 47/1000 | Loss: 0.00001646
Iteration 48/1000 | Loss: 0.00001646
Iteration 49/1000 | Loss: 0.00001646
Iteration 50/1000 | Loss: 0.00001644
Iteration 51/1000 | Loss: 0.00001644
Iteration 52/1000 | Loss: 0.00001643
Iteration 53/1000 | Loss: 0.00001643
Iteration 54/1000 | Loss: 0.00001643
Iteration 55/1000 | Loss: 0.00001642
Iteration 56/1000 | Loss: 0.00001642
Iteration 57/1000 | Loss: 0.00001642
Iteration 58/1000 | Loss: 0.00001642
Iteration 59/1000 | Loss: 0.00001642
Iteration 60/1000 | Loss: 0.00001642
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001641
Iteration 63/1000 | Loss: 0.00001641
Iteration 64/1000 | Loss: 0.00001641
Iteration 65/1000 | Loss: 0.00001641
Iteration 66/1000 | Loss: 0.00001641
Iteration 67/1000 | Loss: 0.00001641
Iteration 68/1000 | Loss: 0.00001641
Iteration 69/1000 | Loss: 0.00001640
Iteration 70/1000 | Loss: 0.00001640
Iteration 71/1000 | Loss: 0.00001640
Iteration 72/1000 | Loss: 0.00001640
Iteration 73/1000 | Loss: 0.00001640
Iteration 74/1000 | Loss: 0.00001639
Iteration 75/1000 | Loss: 0.00001639
Iteration 76/1000 | Loss: 0.00001639
Iteration 77/1000 | Loss: 0.00001639
Iteration 78/1000 | Loss: 0.00001639
Iteration 79/1000 | Loss: 0.00001638
Iteration 80/1000 | Loss: 0.00001638
Iteration 81/1000 | Loss: 0.00001638
Iteration 82/1000 | Loss: 0.00001638
Iteration 83/1000 | Loss: 0.00001638
Iteration 84/1000 | Loss: 0.00001638
Iteration 85/1000 | Loss: 0.00001638
Iteration 86/1000 | Loss: 0.00001638
Iteration 87/1000 | Loss: 0.00001637
Iteration 88/1000 | Loss: 0.00001637
Iteration 89/1000 | Loss: 0.00001637
Iteration 90/1000 | Loss: 0.00001637
Iteration 91/1000 | Loss: 0.00001637
Iteration 92/1000 | Loss: 0.00001637
Iteration 93/1000 | Loss: 0.00001636
Iteration 94/1000 | Loss: 0.00001636
Iteration 95/1000 | Loss: 0.00001636
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001636
Iteration 98/1000 | Loss: 0.00001636
Iteration 99/1000 | Loss: 0.00001636
Iteration 100/1000 | Loss: 0.00001636
Iteration 101/1000 | Loss: 0.00001636
Iteration 102/1000 | Loss: 0.00001636
Iteration 103/1000 | Loss: 0.00001636
Iteration 104/1000 | Loss: 0.00001636
Iteration 105/1000 | Loss: 0.00001636
Iteration 106/1000 | Loss: 0.00001636
Iteration 107/1000 | Loss: 0.00001636
Iteration 108/1000 | Loss: 0.00001635
Iteration 109/1000 | Loss: 0.00001635
Iteration 110/1000 | Loss: 0.00001635
Iteration 111/1000 | Loss: 0.00001635
Iteration 112/1000 | Loss: 0.00001635
Iteration 113/1000 | Loss: 0.00001635
Iteration 114/1000 | Loss: 0.00001635
Iteration 115/1000 | Loss: 0.00001635
Iteration 116/1000 | Loss: 0.00001635
Iteration 117/1000 | Loss: 0.00001635
Iteration 118/1000 | Loss: 0.00001635
Iteration 119/1000 | Loss: 0.00001635
Iteration 120/1000 | Loss: 0.00001635
Iteration 121/1000 | Loss: 0.00001635
Iteration 122/1000 | Loss: 0.00001635
Iteration 123/1000 | Loss: 0.00001635
Iteration 124/1000 | Loss: 0.00001635
Iteration 125/1000 | Loss: 0.00001635
Iteration 126/1000 | Loss: 0.00001635
Iteration 127/1000 | Loss: 0.00001635
Iteration 128/1000 | Loss: 0.00001635
Iteration 129/1000 | Loss: 0.00001635
Iteration 130/1000 | Loss: 0.00001635
Iteration 131/1000 | Loss: 0.00001635
Iteration 132/1000 | Loss: 0.00001635
Iteration 133/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.6346397387678735e-05, 1.6346397387678735e-05, 1.6346397387678735e-05, 1.6346397387678735e-05, 1.6346397387678735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6346397387678735e-05

Optimization complete. Final v2v error: 3.393122434616089 mm

Highest mean error: 3.8069217205047607 mm for frame 107

Lowest mean error: 3.079866409301758 mm for frame 35

Saving results

Total time: 35.79898977279663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408278
Iteration 2/25 | Loss: 0.00131732
Iteration 3/25 | Loss: 0.00122756
Iteration 4/25 | Loss: 0.00122110
Iteration 5/25 | Loss: 0.00121877
Iteration 6/25 | Loss: 0.00121877
Iteration 7/25 | Loss: 0.00121877
Iteration 8/25 | Loss: 0.00121877
Iteration 9/25 | Loss: 0.00121877
Iteration 10/25 | Loss: 0.00121877
Iteration 11/25 | Loss: 0.00121877
Iteration 12/25 | Loss: 0.00121877
Iteration 13/25 | Loss: 0.00121877
Iteration 14/25 | Loss: 0.00121877
Iteration 15/25 | Loss: 0.00121877
Iteration 16/25 | Loss: 0.00121877
Iteration 17/25 | Loss: 0.00121877
Iteration 18/25 | Loss: 0.00121877
Iteration 19/25 | Loss: 0.00121877
Iteration 20/25 | Loss: 0.00121877
Iteration 21/25 | Loss: 0.00121877
Iteration 22/25 | Loss: 0.00121877
Iteration 23/25 | Loss: 0.00121877
Iteration 24/25 | Loss: 0.00121877
Iteration 25/25 | Loss: 0.00121877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74037063
Iteration 2/25 | Loss: 0.00070593
Iteration 3/25 | Loss: 0.00070593
Iteration 4/25 | Loss: 0.00070593
Iteration 5/25 | Loss: 0.00070593
Iteration 6/25 | Loss: 0.00070593
Iteration 7/25 | Loss: 0.00070593
Iteration 8/25 | Loss: 0.00070593
Iteration 9/25 | Loss: 0.00070593
Iteration 10/25 | Loss: 0.00070593
Iteration 11/25 | Loss: 0.00070593
Iteration 12/25 | Loss: 0.00070593
Iteration 13/25 | Loss: 0.00070593
Iteration 14/25 | Loss: 0.00070593
Iteration 15/25 | Loss: 0.00070593
Iteration 16/25 | Loss: 0.00070593
Iteration 17/25 | Loss: 0.00070593
Iteration 18/25 | Loss: 0.00070593
Iteration 19/25 | Loss: 0.00070593
Iteration 20/25 | Loss: 0.00070593
Iteration 21/25 | Loss: 0.00070593
Iteration 22/25 | Loss: 0.00070593
Iteration 23/25 | Loss: 0.00070593
Iteration 24/25 | Loss: 0.00070593
Iteration 25/25 | Loss: 0.00070593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070593
Iteration 2/1000 | Loss: 0.00003045
Iteration 3/1000 | Loss: 0.00001915
Iteration 4/1000 | Loss: 0.00001609
Iteration 5/1000 | Loss: 0.00001516
Iteration 6/1000 | Loss: 0.00001422
Iteration 7/1000 | Loss: 0.00001364
Iteration 8/1000 | Loss: 0.00001334
Iteration 9/1000 | Loss: 0.00001322
Iteration 10/1000 | Loss: 0.00001278
Iteration 11/1000 | Loss: 0.00001254
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00001216
Iteration 15/1000 | Loss: 0.00001216
Iteration 16/1000 | Loss: 0.00001215
Iteration 17/1000 | Loss: 0.00001215
Iteration 18/1000 | Loss: 0.00001214
Iteration 19/1000 | Loss: 0.00001212
Iteration 20/1000 | Loss: 0.00001212
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001211
Iteration 23/1000 | Loss: 0.00001210
Iteration 24/1000 | Loss: 0.00001210
Iteration 25/1000 | Loss: 0.00001209
Iteration 26/1000 | Loss: 0.00001209
Iteration 27/1000 | Loss: 0.00001208
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001206
Iteration 31/1000 | Loss: 0.00001206
Iteration 32/1000 | Loss: 0.00001205
Iteration 33/1000 | Loss: 0.00001205
Iteration 34/1000 | Loss: 0.00001205
Iteration 35/1000 | Loss: 0.00001204
Iteration 36/1000 | Loss: 0.00001204
Iteration 37/1000 | Loss: 0.00001204
Iteration 38/1000 | Loss: 0.00001203
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001201
Iteration 44/1000 | Loss: 0.00001200
Iteration 45/1000 | Loss: 0.00001200
Iteration 46/1000 | Loss: 0.00001200
Iteration 47/1000 | Loss: 0.00001200
Iteration 48/1000 | Loss: 0.00001200
Iteration 49/1000 | Loss: 0.00001199
Iteration 50/1000 | Loss: 0.00001199
Iteration 51/1000 | Loss: 0.00001199
Iteration 52/1000 | Loss: 0.00001199
Iteration 53/1000 | Loss: 0.00001199
Iteration 54/1000 | Loss: 0.00001198
Iteration 55/1000 | Loss: 0.00001198
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001195
Iteration 58/1000 | Loss: 0.00001195
Iteration 59/1000 | Loss: 0.00001195
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001194
Iteration 62/1000 | Loss: 0.00001194
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00001193
Iteration 65/1000 | Loss: 0.00001193
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001192
Iteration 68/1000 | Loss: 0.00001192
Iteration 69/1000 | Loss: 0.00001192
Iteration 70/1000 | Loss: 0.00001192
Iteration 71/1000 | Loss: 0.00001192
Iteration 72/1000 | Loss: 0.00001191
Iteration 73/1000 | Loss: 0.00001191
Iteration 74/1000 | Loss: 0.00001191
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001189
Iteration 84/1000 | Loss: 0.00001189
Iteration 85/1000 | Loss: 0.00001188
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001187
Iteration 88/1000 | Loss: 0.00001187
Iteration 89/1000 | Loss: 0.00001186
Iteration 90/1000 | Loss: 0.00001186
Iteration 91/1000 | Loss: 0.00001186
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001186
Iteration 94/1000 | Loss: 0.00001185
Iteration 95/1000 | Loss: 0.00001185
Iteration 96/1000 | Loss: 0.00001185
Iteration 97/1000 | Loss: 0.00001185
Iteration 98/1000 | Loss: 0.00001184
Iteration 99/1000 | Loss: 0.00001184
Iteration 100/1000 | Loss: 0.00001184
Iteration 101/1000 | Loss: 0.00001183
Iteration 102/1000 | Loss: 0.00001183
Iteration 103/1000 | Loss: 0.00001183
Iteration 104/1000 | Loss: 0.00001183
Iteration 105/1000 | Loss: 0.00001183
Iteration 106/1000 | Loss: 0.00001183
Iteration 107/1000 | Loss: 0.00001182
Iteration 108/1000 | Loss: 0.00001182
Iteration 109/1000 | Loss: 0.00001182
Iteration 110/1000 | Loss: 0.00001182
Iteration 111/1000 | Loss: 0.00001182
Iteration 112/1000 | Loss: 0.00001182
Iteration 113/1000 | Loss: 0.00001182
Iteration 114/1000 | Loss: 0.00001182
Iteration 115/1000 | Loss: 0.00001182
Iteration 116/1000 | Loss: 0.00001182
Iteration 117/1000 | Loss: 0.00001181
Iteration 118/1000 | Loss: 0.00001181
Iteration 119/1000 | Loss: 0.00001181
Iteration 120/1000 | Loss: 0.00001181
Iteration 121/1000 | Loss: 0.00001181
Iteration 122/1000 | Loss: 0.00001181
Iteration 123/1000 | Loss: 0.00001180
Iteration 124/1000 | Loss: 0.00001180
Iteration 125/1000 | Loss: 0.00001180
Iteration 126/1000 | Loss: 0.00001180
Iteration 127/1000 | Loss: 0.00001180
Iteration 128/1000 | Loss: 0.00001180
Iteration 129/1000 | Loss: 0.00001179
Iteration 130/1000 | Loss: 0.00001179
Iteration 131/1000 | Loss: 0.00001179
Iteration 132/1000 | Loss: 0.00001179
Iteration 133/1000 | Loss: 0.00001179
Iteration 134/1000 | Loss: 0.00001179
Iteration 135/1000 | Loss: 0.00001179
Iteration 136/1000 | Loss: 0.00001178
Iteration 137/1000 | Loss: 0.00001178
Iteration 138/1000 | Loss: 0.00001178
Iteration 139/1000 | Loss: 0.00001178
Iteration 140/1000 | Loss: 0.00001177
Iteration 141/1000 | Loss: 0.00001177
Iteration 142/1000 | Loss: 0.00001177
Iteration 143/1000 | Loss: 0.00001177
Iteration 144/1000 | Loss: 0.00001177
Iteration 145/1000 | Loss: 0.00001177
Iteration 146/1000 | Loss: 0.00001176
Iteration 147/1000 | Loss: 0.00001176
Iteration 148/1000 | Loss: 0.00001176
Iteration 149/1000 | Loss: 0.00001176
Iteration 150/1000 | Loss: 0.00001175
Iteration 151/1000 | Loss: 0.00001175
Iteration 152/1000 | Loss: 0.00001175
Iteration 153/1000 | Loss: 0.00001175
Iteration 154/1000 | Loss: 0.00001175
Iteration 155/1000 | Loss: 0.00001175
Iteration 156/1000 | Loss: 0.00001175
Iteration 157/1000 | Loss: 0.00001174
Iteration 158/1000 | Loss: 0.00001174
Iteration 159/1000 | Loss: 0.00001174
Iteration 160/1000 | Loss: 0.00001174
Iteration 161/1000 | Loss: 0.00001174
Iteration 162/1000 | Loss: 0.00001174
Iteration 163/1000 | Loss: 0.00001174
Iteration 164/1000 | Loss: 0.00001174
Iteration 165/1000 | Loss: 0.00001174
Iteration 166/1000 | Loss: 0.00001174
Iteration 167/1000 | Loss: 0.00001174
Iteration 168/1000 | Loss: 0.00001174
Iteration 169/1000 | Loss: 0.00001174
Iteration 170/1000 | Loss: 0.00001174
Iteration 171/1000 | Loss: 0.00001174
Iteration 172/1000 | Loss: 0.00001174
Iteration 173/1000 | Loss: 0.00001174
Iteration 174/1000 | Loss: 0.00001174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.1743782124540303e-05, 1.1743782124540303e-05, 1.1743782124540303e-05, 1.1743782124540303e-05, 1.1743782124540303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1743782124540303e-05

Optimization complete. Final v2v error: 2.949941396713257 mm

Highest mean error: 3.1371233463287354 mm for frame 18

Lowest mean error: 2.7896382808685303 mm for frame 13

Saving results

Total time: 44.0971782207489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783440
Iteration 2/25 | Loss: 0.00157215
Iteration 3/25 | Loss: 0.00129883
Iteration 4/25 | Loss: 0.00126625
Iteration 5/25 | Loss: 0.00125730
Iteration 6/25 | Loss: 0.00125389
Iteration 7/25 | Loss: 0.00125332
Iteration 8/25 | Loss: 0.00125316
Iteration 9/25 | Loss: 0.00125304
Iteration 10/25 | Loss: 0.00125303
Iteration 11/25 | Loss: 0.00125303
Iteration 12/25 | Loss: 0.00125303
Iteration 13/25 | Loss: 0.00125303
Iteration 14/25 | Loss: 0.00125303
Iteration 15/25 | Loss: 0.00125303
Iteration 16/25 | Loss: 0.00125303
Iteration 17/25 | Loss: 0.00125303
Iteration 18/25 | Loss: 0.00125303
Iteration 19/25 | Loss: 0.00125303
Iteration 20/25 | Loss: 0.00125303
Iteration 21/25 | Loss: 0.00125303
Iteration 22/25 | Loss: 0.00125303
Iteration 23/25 | Loss: 0.00125303
Iteration 24/25 | Loss: 0.00125303
Iteration 25/25 | Loss: 0.00125303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23231816
Iteration 2/25 | Loss: 0.00088099
Iteration 3/25 | Loss: 0.00088099
Iteration 4/25 | Loss: 0.00088098
Iteration 5/25 | Loss: 0.00088098
Iteration 6/25 | Loss: 0.00088098
Iteration 7/25 | Loss: 0.00088098
Iteration 8/25 | Loss: 0.00088098
Iteration 9/25 | Loss: 0.00088098
Iteration 10/25 | Loss: 0.00088098
Iteration 11/25 | Loss: 0.00088098
Iteration 12/25 | Loss: 0.00088098
Iteration 13/25 | Loss: 0.00088098
Iteration 14/25 | Loss: 0.00088098
Iteration 15/25 | Loss: 0.00088098
Iteration 16/25 | Loss: 0.00088098
Iteration 17/25 | Loss: 0.00088098
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008809828432276845, 0.0008809828432276845, 0.0008809828432276845, 0.0008809828432276845, 0.0008809828432276845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008809828432276845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088098
Iteration 2/1000 | Loss: 0.00002838
Iteration 3/1000 | Loss: 0.00002197
Iteration 4/1000 | Loss: 0.00002419
Iteration 5/1000 | Loss: 0.00002189
Iteration 6/1000 | Loss: 0.00001957
Iteration 7/1000 | Loss: 0.00001898
Iteration 8/1000 | Loss: 0.00001857
Iteration 9/1000 | Loss: 0.00001816
Iteration 10/1000 | Loss: 0.00001787
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00001752
Iteration 13/1000 | Loss: 0.00001751
Iteration 14/1000 | Loss: 0.00001748
Iteration 15/1000 | Loss: 0.00001746
Iteration 16/1000 | Loss: 0.00001745
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001743
Iteration 19/1000 | Loss: 0.00001740
Iteration 20/1000 | Loss: 0.00001738
Iteration 21/1000 | Loss: 0.00001737
Iteration 22/1000 | Loss: 0.00001736
Iteration 23/1000 | Loss: 0.00001735
Iteration 24/1000 | Loss: 0.00001735
Iteration 25/1000 | Loss: 0.00001734
Iteration 26/1000 | Loss: 0.00001732
Iteration 27/1000 | Loss: 0.00001731
Iteration 28/1000 | Loss: 0.00001731
Iteration 29/1000 | Loss: 0.00001731
Iteration 30/1000 | Loss: 0.00001731
Iteration 31/1000 | Loss: 0.00001731
Iteration 32/1000 | Loss: 0.00001730
Iteration 33/1000 | Loss: 0.00001730
Iteration 34/1000 | Loss: 0.00001730
Iteration 35/1000 | Loss: 0.00001729
Iteration 36/1000 | Loss: 0.00001729
Iteration 37/1000 | Loss: 0.00001729
Iteration 38/1000 | Loss: 0.00001728
Iteration 39/1000 | Loss: 0.00001728
Iteration 40/1000 | Loss: 0.00001727
Iteration 41/1000 | Loss: 0.00001727
Iteration 42/1000 | Loss: 0.00001727
Iteration 43/1000 | Loss: 0.00001727
Iteration 44/1000 | Loss: 0.00001726
Iteration 45/1000 | Loss: 0.00001726
Iteration 46/1000 | Loss: 0.00001726
Iteration 47/1000 | Loss: 0.00001725
Iteration 48/1000 | Loss: 0.00001724
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001723
Iteration 53/1000 | Loss: 0.00001723
Iteration 54/1000 | Loss: 0.00001723
Iteration 55/1000 | Loss: 0.00001723
Iteration 56/1000 | Loss: 0.00001723
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001722
Iteration 60/1000 | Loss: 0.00001722
Iteration 61/1000 | Loss: 0.00001722
Iteration 62/1000 | Loss: 0.00001722
Iteration 63/1000 | Loss: 0.00001722
Iteration 64/1000 | Loss: 0.00001721
Iteration 65/1000 | Loss: 0.00001721
Iteration 66/1000 | Loss: 0.00001721
Iteration 67/1000 | Loss: 0.00001721
Iteration 68/1000 | Loss: 0.00001721
Iteration 69/1000 | Loss: 0.00001721
Iteration 70/1000 | Loss: 0.00001720
Iteration 71/1000 | Loss: 0.00001720
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001920
Iteration 75/1000 | Loss: 0.00001719
Iteration 76/1000 | Loss: 0.00001718
Iteration 77/1000 | Loss: 0.00001718
Iteration 78/1000 | Loss: 0.00001717
Iteration 79/1000 | Loss: 0.00001717
Iteration 80/1000 | Loss: 0.00001717
Iteration 81/1000 | Loss: 0.00001716
Iteration 82/1000 | Loss: 0.00001716
Iteration 83/1000 | Loss: 0.00001716
Iteration 84/1000 | Loss: 0.00001716
Iteration 85/1000 | Loss: 0.00001716
Iteration 86/1000 | Loss: 0.00001716
Iteration 87/1000 | Loss: 0.00001715
Iteration 88/1000 | Loss: 0.00001715
Iteration 89/1000 | Loss: 0.00001715
Iteration 90/1000 | Loss: 0.00001715
Iteration 91/1000 | Loss: 0.00001715
Iteration 92/1000 | Loss: 0.00001715
Iteration 93/1000 | Loss: 0.00001715
Iteration 94/1000 | Loss: 0.00001715
Iteration 95/1000 | Loss: 0.00001715
Iteration 96/1000 | Loss: 0.00001714
Iteration 97/1000 | Loss: 0.00001714
Iteration 98/1000 | Loss: 0.00001714
Iteration 99/1000 | Loss: 0.00001714
Iteration 100/1000 | Loss: 0.00001714
Iteration 101/1000 | Loss: 0.00001714
Iteration 102/1000 | Loss: 0.00001714
Iteration 103/1000 | Loss: 0.00001714
Iteration 104/1000 | Loss: 0.00001713
Iteration 105/1000 | Loss: 0.00001713
Iteration 106/1000 | Loss: 0.00001713
Iteration 107/1000 | Loss: 0.00001713
Iteration 108/1000 | Loss: 0.00001713
Iteration 109/1000 | Loss: 0.00001713
Iteration 110/1000 | Loss: 0.00001713
Iteration 111/1000 | Loss: 0.00001713
Iteration 112/1000 | Loss: 0.00001712
Iteration 113/1000 | Loss: 0.00001712
Iteration 114/1000 | Loss: 0.00001712
Iteration 115/1000 | Loss: 0.00001712
Iteration 116/1000 | Loss: 0.00001712
Iteration 117/1000 | Loss: 0.00001712
Iteration 118/1000 | Loss: 0.00001711
Iteration 119/1000 | Loss: 0.00001711
Iteration 120/1000 | Loss: 0.00001711
Iteration 121/1000 | Loss: 0.00001711
Iteration 122/1000 | Loss: 0.00001711
Iteration 123/1000 | Loss: 0.00001711
Iteration 124/1000 | Loss: 0.00001711
Iteration 125/1000 | Loss: 0.00001711
Iteration 126/1000 | Loss: 0.00001711
Iteration 127/1000 | Loss: 0.00001711
Iteration 128/1000 | Loss: 0.00001711
Iteration 129/1000 | Loss: 0.00001710
Iteration 130/1000 | Loss: 0.00001710
Iteration 131/1000 | Loss: 0.00001710
Iteration 132/1000 | Loss: 0.00001710
Iteration 133/1000 | Loss: 0.00001710
Iteration 134/1000 | Loss: 0.00001710
Iteration 135/1000 | Loss: 0.00001710
Iteration 136/1000 | Loss: 0.00001710
Iteration 137/1000 | Loss: 0.00001710
Iteration 138/1000 | Loss: 0.00001709
Iteration 139/1000 | Loss: 0.00001709
Iteration 140/1000 | Loss: 0.00001709
Iteration 141/1000 | Loss: 0.00001709
Iteration 142/1000 | Loss: 0.00001709
Iteration 143/1000 | Loss: 0.00001709
Iteration 144/1000 | Loss: 0.00001708
Iteration 145/1000 | Loss: 0.00001708
Iteration 146/1000 | Loss: 0.00001708
Iteration 147/1000 | Loss: 0.00001708
Iteration 148/1000 | Loss: 0.00001708
Iteration 149/1000 | Loss: 0.00001708
Iteration 150/1000 | Loss: 0.00001708
Iteration 151/1000 | Loss: 0.00001708
Iteration 152/1000 | Loss: 0.00001707
Iteration 153/1000 | Loss: 0.00001707
Iteration 154/1000 | Loss: 0.00001707
Iteration 155/1000 | Loss: 0.00001707
Iteration 156/1000 | Loss: 0.00001707
Iteration 157/1000 | Loss: 0.00001707
Iteration 158/1000 | Loss: 0.00001707
Iteration 159/1000 | Loss: 0.00001707
Iteration 160/1000 | Loss: 0.00001707
Iteration 161/1000 | Loss: 0.00001707
Iteration 162/1000 | Loss: 0.00001707
Iteration 163/1000 | Loss: 0.00001707
Iteration 164/1000 | Loss: 0.00001707
Iteration 165/1000 | Loss: 0.00001706
Iteration 166/1000 | Loss: 0.00001706
Iteration 167/1000 | Loss: 0.00001706
Iteration 168/1000 | Loss: 0.00001706
Iteration 169/1000 | Loss: 0.00001706
Iteration 170/1000 | Loss: 0.00001706
Iteration 171/1000 | Loss: 0.00001706
Iteration 172/1000 | Loss: 0.00001706
Iteration 173/1000 | Loss: 0.00001706
Iteration 174/1000 | Loss: 0.00001706
Iteration 175/1000 | Loss: 0.00001706
Iteration 176/1000 | Loss: 0.00001706
Iteration 177/1000 | Loss: 0.00001706
Iteration 178/1000 | Loss: 0.00001705
Iteration 179/1000 | Loss: 0.00001705
Iteration 180/1000 | Loss: 0.00001705
Iteration 181/1000 | Loss: 0.00001705
Iteration 182/1000 | Loss: 0.00001705
Iteration 183/1000 | Loss: 0.00001705
Iteration 184/1000 | Loss: 0.00001705
Iteration 185/1000 | Loss: 0.00001704
Iteration 186/1000 | Loss: 0.00001704
Iteration 187/1000 | Loss: 0.00001704
Iteration 188/1000 | Loss: 0.00001704
Iteration 189/1000 | Loss: 0.00001704
Iteration 190/1000 | Loss: 0.00001704
Iteration 191/1000 | Loss: 0.00001704
Iteration 192/1000 | Loss: 0.00001704
Iteration 193/1000 | Loss: 0.00001704
Iteration 194/1000 | Loss: 0.00001704
Iteration 195/1000 | Loss: 0.00001704
Iteration 196/1000 | Loss: 0.00001704
Iteration 197/1000 | Loss: 0.00001704
Iteration 198/1000 | Loss: 0.00001704
Iteration 199/1000 | Loss: 0.00001704
Iteration 200/1000 | Loss: 0.00001704
Iteration 201/1000 | Loss: 0.00001704
Iteration 202/1000 | Loss: 0.00001703
Iteration 203/1000 | Loss: 0.00001703
Iteration 204/1000 | Loss: 0.00001703
Iteration 205/1000 | Loss: 0.00001703
Iteration 206/1000 | Loss: 0.00001703
Iteration 207/1000 | Loss: 0.00001703
Iteration 208/1000 | Loss: 0.00001703
Iteration 209/1000 | Loss: 0.00001703
Iteration 210/1000 | Loss: 0.00001703
Iteration 211/1000 | Loss: 0.00001703
Iteration 212/1000 | Loss: 0.00001703
Iteration 213/1000 | Loss: 0.00001703
Iteration 214/1000 | Loss: 0.00001703
Iteration 215/1000 | Loss: 0.00001703
Iteration 216/1000 | Loss: 0.00001702
Iteration 217/1000 | Loss: 0.00001702
Iteration 218/1000 | Loss: 0.00001702
Iteration 219/1000 | Loss: 0.00001702
Iteration 220/1000 | Loss: 0.00001702
Iteration 221/1000 | Loss: 0.00001941
Iteration 222/1000 | Loss: 0.00001941
Iteration 223/1000 | Loss: 0.00001701
Iteration 224/1000 | Loss: 0.00001701
Iteration 225/1000 | Loss: 0.00001701
Iteration 226/1000 | Loss: 0.00001701
Iteration 227/1000 | Loss: 0.00001701
Iteration 228/1000 | Loss: 0.00001701
Iteration 229/1000 | Loss: 0.00001701
Iteration 230/1000 | Loss: 0.00001700
Iteration 231/1000 | Loss: 0.00001700
Iteration 232/1000 | Loss: 0.00001700
Iteration 233/1000 | Loss: 0.00001700
Iteration 234/1000 | Loss: 0.00001700
Iteration 235/1000 | Loss: 0.00001700
Iteration 236/1000 | Loss: 0.00001700
Iteration 237/1000 | Loss: 0.00001700
Iteration 238/1000 | Loss: 0.00001700
Iteration 239/1000 | Loss: 0.00001700
Iteration 240/1000 | Loss: 0.00001700
Iteration 241/1000 | Loss: 0.00001700
Iteration 242/1000 | Loss: 0.00001700
Iteration 243/1000 | Loss: 0.00001700
Iteration 244/1000 | Loss: 0.00001700
Iteration 245/1000 | Loss: 0.00001700
Iteration 246/1000 | Loss: 0.00001700
Iteration 247/1000 | Loss: 0.00001700
Iteration 248/1000 | Loss: 0.00001700
Iteration 249/1000 | Loss: 0.00001700
Iteration 250/1000 | Loss: 0.00001700
Iteration 251/1000 | Loss: 0.00001700
Iteration 252/1000 | Loss: 0.00001700
Iteration 253/1000 | Loss: 0.00001700
Iteration 254/1000 | Loss: 0.00001700
Iteration 255/1000 | Loss: 0.00001700
Iteration 256/1000 | Loss: 0.00001700
Iteration 257/1000 | Loss: 0.00001700
Iteration 258/1000 | Loss: 0.00001700
Iteration 259/1000 | Loss: 0.00001700
Iteration 260/1000 | Loss: 0.00001700
Iteration 261/1000 | Loss: 0.00001700
Iteration 262/1000 | Loss: 0.00001700
Iteration 263/1000 | Loss: 0.00001700
Iteration 264/1000 | Loss: 0.00001700
Iteration 265/1000 | Loss: 0.00001700
Iteration 266/1000 | Loss: 0.00001700
Iteration 267/1000 | Loss: 0.00001700
Iteration 268/1000 | Loss: 0.00001700
Iteration 269/1000 | Loss: 0.00001700
Iteration 270/1000 | Loss: 0.00001700
Iteration 271/1000 | Loss: 0.00001700
Iteration 272/1000 | Loss: 0.00001700
Iteration 273/1000 | Loss: 0.00001700
Iteration 274/1000 | Loss: 0.00001700
Iteration 275/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.699877975624986e-05, 1.699877975624986e-05, 1.699877975624986e-05, 1.699877975624986e-05, 1.699877975624986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.699877975624986e-05

Optimization complete. Final v2v error: 3.401336193084717 mm

Highest mean error: 5.458523750305176 mm for frame 48

Lowest mean error: 3.0559096336364746 mm for frame 151

Saving results

Total time: 59.138319969177246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791282
Iteration 2/25 | Loss: 0.00192128
Iteration 3/25 | Loss: 0.00161474
Iteration 4/25 | Loss: 0.00140025
Iteration 5/25 | Loss: 0.00138784
Iteration 6/25 | Loss: 0.00135798
Iteration 7/25 | Loss: 0.00132461
Iteration 8/25 | Loss: 0.00131782
Iteration 9/25 | Loss: 0.00132457
Iteration 10/25 | Loss: 0.00132573
Iteration 11/25 | Loss: 0.00132316
Iteration 12/25 | Loss: 0.00131346
Iteration 13/25 | Loss: 0.00130747
Iteration 14/25 | Loss: 0.00130622
Iteration 15/25 | Loss: 0.00130585
Iteration 16/25 | Loss: 0.00130571
Iteration 17/25 | Loss: 0.00130565
Iteration 18/25 | Loss: 0.00130564
Iteration 19/25 | Loss: 0.00130564
Iteration 20/25 | Loss: 0.00130564
Iteration 21/25 | Loss: 0.00130564
Iteration 22/25 | Loss: 0.00130564
Iteration 23/25 | Loss: 0.00130564
Iteration 24/25 | Loss: 0.00130564
Iteration 25/25 | Loss: 0.00130564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.04605293
Iteration 2/25 | Loss: 0.00094247
Iteration 3/25 | Loss: 0.00077021
Iteration 4/25 | Loss: 0.00077021
Iteration 5/25 | Loss: 0.00077021
Iteration 6/25 | Loss: 0.00077021
Iteration 7/25 | Loss: 0.00077021
Iteration 8/25 | Loss: 0.00077021
Iteration 9/25 | Loss: 0.00077021
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0007702102302573621, 0.0007702102302573621, 0.0007702102302573621, 0.0007702102302573621, 0.0007702102302573621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007702102302573621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077021
Iteration 2/1000 | Loss: 0.00005668
Iteration 3/1000 | Loss: 0.00003978
Iteration 4/1000 | Loss: 0.00003564
Iteration 5/1000 | Loss: 0.00003370
Iteration 6/1000 | Loss: 0.00003150
Iteration 7/1000 | Loss: 0.00003043
Iteration 8/1000 | Loss: 0.00002969
Iteration 9/1000 | Loss: 0.00002926
Iteration 10/1000 | Loss: 0.00002887
Iteration 11/1000 | Loss: 0.00002853
Iteration 12/1000 | Loss: 0.00002851
Iteration 13/1000 | Loss: 0.00002824
Iteration 14/1000 | Loss: 0.00002801
Iteration 15/1000 | Loss: 0.00002790
Iteration 16/1000 | Loss: 0.00002785
Iteration 17/1000 | Loss: 0.00002784
Iteration 18/1000 | Loss: 0.00002780
Iteration 19/1000 | Loss: 0.00002780
Iteration 20/1000 | Loss: 0.00002779
Iteration 21/1000 | Loss: 0.00002778
Iteration 22/1000 | Loss: 0.00002777
Iteration 23/1000 | Loss: 0.00002776
Iteration 24/1000 | Loss: 0.00002775
Iteration 25/1000 | Loss: 0.00002775
Iteration 26/1000 | Loss: 0.00002775
Iteration 27/1000 | Loss: 0.00002775
Iteration 28/1000 | Loss: 0.00002774
Iteration 29/1000 | Loss: 0.00002774
Iteration 30/1000 | Loss: 0.00002774
Iteration 31/1000 | Loss: 0.00002774
Iteration 32/1000 | Loss: 0.00002773
Iteration 33/1000 | Loss: 0.00002772
Iteration 34/1000 | Loss: 0.00002771
Iteration 35/1000 | Loss: 0.00002770
Iteration 36/1000 | Loss: 0.00002770
Iteration 37/1000 | Loss: 0.00002769
Iteration 38/1000 | Loss: 0.00002766
Iteration 39/1000 | Loss: 0.00002765
Iteration 40/1000 | Loss: 0.00002764
Iteration 41/1000 | Loss: 0.00002764
Iteration 42/1000 | Loss: 0.00002764
Iteration 43/1000 | Loss: 0.00002763
Iteration 44/1000 | Loss: 0.00002763
Iteration 45/1000 | Loss: 0.00002763
Iteration 46/1000 | Loss: 0.00002763
Iteration 47/1000 | Loss: 0.00002762
Iteration 48/1000 | Loss: 0.00002762
Iteration 49/1000 | Loss: 0.00002762
Iteration 50/1000 | Loss: 0.00002761
Iteration 51/1000 | Loss: 0.00002761
Iteration 52/1000 | Loss: 0.00002761
Iteration 53/1000 | Loss: 0.00002761
Iteration 54/1000 | Loss: 0.00002761
Iteration 55/1000 | Loss: 0.00002761
Iteration 56/1000 | Loss: 0.00002761
Iteration 57/1000 | Loss: 0.00002761
Iteration 58/1000 | Loss: 0.00002761
Iteration 59/1000 | Loss: 0.00002760
Iteration 60/1000 | Loss: 0.00002760
Iteration 61/1000 | Loss: 0.00002760
Iteration 62/1000 | Loss: 0.00002760
Iteration 63/1000 | Loss: 0.00002759
Iteration 64/1000 | Loss: 0.00002759
Iteration 65/1000 | Loss: 0.00002759
Iteration 66/1000 | Loss: 0.00002759
Iteration 67/1000 | Loss: 0.00002758
Iteration 68/1000 | Loss: 0.00002758
Iteration 69/1000 | Loss: 0.00002757
Iteration 70/1000 | Loss: 0.00002757
Iteration 71/1000 | Loss: 0.00002757
Iteration 72/1000 | Loss: 0.00002757
Iteration 73/1000 | Loss: 0.00002757
Iteration 74/1000 | Loss: 0.00002757
Iteration 75/1000 | Loss: 0.00002757
Iteration 76/1000 | Loss: 0.00002757
Iteration 77/1000 | Loss: 0.00002757
Iteration 78/1000 | Loss: 0.00002757
Iteration 79/1000 | Loss: 0.00002757
Iteration 80/1000 | Loss: 0.00002757
Iteration 81/1000 | Loss: 0.00002757
Iteration 82/1000 | Loss: 0.00002756
Iteration 83/1000 | Loss: 0.00002756
Iteration 84/1000 | Loss: 0.00002756
Iteration 85/1000 | Loss: 0.00002756
Iteration 86/1000 | Loss: 0.00002756
Iteration 87/1000 | Loss: 0.00002755
Iteration 88/1000 | Loss: 0.00002755
Iteration 89/1000 | Loss: 0.00002755
Iteration 90/1000 | Loss: 0.00002755
Iteration 91/1000 | Loss: 0.00002754
Iteration 92/1000 | Loss: 0.00002754
Iteration 93/1000 | Loss: 0.00002754
Iteration 94/1000 | Loss: 0.00002754
Iteration 95/1000 | Loss: 0.00002754
Iteration 96/1000 | Loss: 0.00002753
Iteration 97/1000 | Loss: 0.00002753
Iteration 98/1000 | Loss: 0.00002753
Iteration 99/1000 | Loss: 0.00002753
Iteration 100/1000 | Loss: 0.00002753
Iteration 101/1000 | Loss: 0.00002753
Iteration 102/1000 | Loss: 0.00002753
Iteration 103/1000 | Loss: 0.00002752
Iteration 104/1000 | Loss: 0.00002752
Iteration 105/1000 | Loss: 0.00002752
Iteration 106/1000 | Loss: 0.00002752
Iteration 107/1000 | Loss: 0.00002752
Iteration 108/1000 | Loss: 0.00002752
Iteration 109/1000 | Loss: 0.00002751
Iteration 110/1000 | Loss: 0.00002751
Iteration 111/1000 | Loss: 0.00002751
Iteration 112/1000 | Loss: 0.00002751
Iteration 113/1000 | Loss: 0.00002751
Iteration 114/1000 | Loss: 0.00002751
Iteration 115/1000 | Loss: 0.00002751
Iteration 116/1000 | Loss: 0.00002751
Iteration 117/1000 | Loss: 0.00002750
Iteration 118/1000 | Loss: 0.00002750
Iteration 119/1000 | Loss: 0.00002750
Iteration 120/1000 | Loss: 0.00002750
Iteration 121/1000 | Loss: 0.00002750
Iteration 122/1000 | Loss: 0.00002750
Iteration 123/1000 | Loss: 0.00002750
Iteration 124/1000 | Loss: 0.00002750
Iteration 125/1000 | Loss: 0.00002750
Iteration 126/1000 | Loss: 0.00002750
Iteration 127/1000 | Loss: 0.00002750
Iteration 128/1000 | Loss: 0.00002750
Iteration 129/1000 | Loss: 0.00002749
Iteration 130/1000 | Loss: 0.00002749
Iteration 131/1000 | Loss: 0.00002749
Iteration 132/1000 | Loss: 0.00002749
Iteration 133/1000 | Loss: 0.00002749
Iteration 134/1000 | Loss: 0.00002748
Iteration 135/1000 | Loss: 0.00002748
Iteration 136/1000 | Loss: 0.00002748
Iteration 137/1000 | Loss: 0.00002747
Iteration 138/1000 | Loss: 0.00002747
Iteration 139/1000 | Loss: 0.00002747
Iteration 140/1000 | Loss: 0.00002747
Iteration 141/1000 | Loss: 0.00002747
Iteration 142/1000 | Loss: 0.00002747
Iteration 143/1000 | Loss: 0.00002747
Iteration 144/1000 | Loss: 0.00002746
Iteration 145/1000 | Loss: 0.00002746
Iteration 146/1000 | Loss: 0.00002746
Iteration 147/1000 | Loss: 0.00002746
Iteration 148/1000 | Loss: 0.00002746
Iteration 149/1000 | Loss: 0.00002746
Iteration 150/1000 | Loss: 0.00002746
Iteration 151/1000 | Loss: 0.00002746
Iteration 152/1000 | Loss: 0.00002746
Iteration 153/1000 | Loss: 0.00002746
Iteration 154/1000 | Loss: 0.00002746
Iteration 155/1000 | Loss: 0.00002746
Iteration 156/1000 | Loss: 0.00002745
Iteration 157/1000 | Loss: 0.00002745
Iteration 158/1000 | Loss: 0.00002745
Iteration 159/1000 | Loss: 0.00002745
Iteration 160/1000 | Loss: 0.00002745
Iteration 161/1000 | Loss: 0.00002745
Iteration 162/1000 | Loss: 0.00002745
Iteration 163/1000 | Loss: 0.00002745
Iteration 164/1000 | Loss: 0.00002745
Iteration 165/1000 | Loss: 0.00002745
Iteration 166/1000 | Loss: 0.00002745
Iteration 167/1000 | Loss: 0.00002745
Iteration 168/1000 | Loss: 0.00002745
Iteration 169/1000 | Loss: 0.00002745
Iteration 170/1000 | Loss: 0.00002745
Iteration 171/1000 | Loss: 0.00002744
Iteration 172/1000 | Loss: 0.00002744
Iteration 173/1000 | Loss: 0.00002744
Iteration 174/1000 | Loss: 0.00002744
Iteration 175/1000 | Loss: 0.00002744
Iteration 176/1000 | Loss: 0.00002744
Iteration 177/1000 | Loss: 0.00002744
Iteration 178/1000 | Loss: 0.00002743
Iteration 179/1000 | Loss: 0.00002743
Iteration 180/1000 | Loss: 0.00002743
Iteration 181/1000 | Loss: 0.00002743
Iteration 182/1000 | Loss: 0.00002743
Iteration 183/1000 | Loss: 0.00002743
Iteration 184/1000 | Loss: 0.00002743
Iteration 185/1000 | Loss: 0.00002743
Iteration 186/1000 | Loss: 0.00002743
Iteration 187/1000 | Loss: 0.00002743
Iteration 188/1000 | Loss: 0.00002743
Iteration 189/1000 | Loss: 0.00002743
Iteration 190/1000 | Loss: 0.00002743
Iteration 191/1000 | Loss: 0.00002743
Iteration 192/1000 | Loss: 0.00002743
Iteration 193/1000 | Loss: 0.00002743
Iteration 194/1000 | Loss: 0.00002743
Iteration 195/1000 | Loss: 0.00002743
Iteration 196/1000 | Loss: 0.00002743
Iteration 197/1000 | Loss: 0.00002743
Iteration 198/1000 | Loss: 0.00002743
Iteration 199/1000 | Loss: 0.00002743
Iteration 200/1000 | Loss: 0.00002743
Iteration 201/1000 | Loss: 0.00002743
Iteration 202/1000 | Loss: 0.00002743
Iteration 203/1000 | Loss: 0.00002743
Iteration 204/1000 | Loss: 0.00002743
Iteration 205/1000 | Loss: 0.00002743
Iteration 206/1000 | Loss: 0.00002743
Iteration 207/1000 | Loss: 0.00002743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [2.7430938644101843e-05, 2.7430938644101843e-05, 2.7430938644101843e-05, 2.7430938644101843e-05, 2.7430938644101843e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7430938644101843e-05

Optimization complete. Final v2v error: 4.094939231872559 mm

Highest mean error: 11.570638656616211 mm for frame 214

Lowest mean error: 3.7688145637512207 mm for frame 150

Saving results

Total time: 71.44102883338928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412956
Iteration 2/25 | Loss: 0.00131910
Iteration 3/25 | Loss: 0.00122327
Iteration 4/25 | Loss: 0.00121227
Iteration 5/25 | Loss: 0.00120792
Iteration 6/25 | Loss: 0.00120709
Iteration 7/25 | Loss: 0.00120709
Iteration 8/25 | Loss: 0.00120709
Iteration 9/25 | Loss: 0.00120709
Iteration 10/25 | Loss: 0.00120709
Iteration 11/25 | Loss: 0.00120709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012070864904671907, 0.0012070864904671907, 0.0012070864904671907, 0.0012070864904671907, 0.0012070864904671907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012070864904671907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13680482
Iteration 2/25 | Loss: 0.00079517
Iteration 3/25 | Loss: 0.00079515
Iteration 4/25 | Loss: 0.00079515
Iteration 5/25 | Loss: 0.00079515
Iteration 6/25 | Loss: 0.00079515
Iteration 7/25 | Loss: 0.00079515
Iteration 8/25 | Loss: 0.00079515
Iteration 9/25 | Loss: 0.00079515
Iteration 10/25 | Loss: 0.00079515
Iteration 11/25 | Loss: 0.00079515
Iteration 12/25 | Loss: 0.00079515
Iteration 13/25 | Loss: 0.00079515
Iteration 14/25 | Loss: 0.00079515
Iteration 15/25 | Loss: 0.00079515
Iteration 16/25 | Loss: 0.00079515
Iteration 17/25 | Loss: 0.00079515
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007951481966301799, 0.0007951481966301799, 0.0007951481966301799, 0.0007951481966301799, 0.0007951481966301799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007951481966301799

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079515
Iteration 2/1000 | Loss: 0.00002916
Iteration 3/1000 | Loss: 0.00002063
Iteration 4/1000 | Loss: 0.00001777
Iteration 5/1000 | Loss: 0.00001683
Iteration 6/1000 | Loss: 0.00001604
Iteration 7/1000 | Loss: 0.00001552
Iteration 8/1000 | Loss: 0.00001507
Iteration 9/1000 | Loss: 0.00001477
Iteration 10/1000 | Loss: 0.00001473
Iteration 11/1000 | Loss: 0.00001467
Iteration 12/1000 | Loss: 0.00001450
Iteration 13/1000 | Loss: 0.00001433
Iteration 14/1000 | Loss: 0.00001428
Iteration 15/1000 | Loss: 0.00001421
Iteration 16/1000 | Loss: 0.00001417
Iteration 17/1000 | Loss: 0.00001412
Iteration 18/1000 | Loss: 0.00001407
Iteration 19/1000 | Loss: 0.00001407
Iteration 20/1000 | Loss: 0.00001401
Iteration 21/1000 | Loss: 0.00001393
Iteration 22/1000 | Loss: 0.00001388
Iteration 23/1000 | Loss: 0.00001383
Iteration 24/1000 | Loss: 0.00001383
Iteration 25/1000 | Loss: 0.00001381
Iteration 26/1000 | Loss: 0.00001380
Iteration 27/1000 | Loss: 0.00001379
Iteration 28/1000 | Loss: 0.00001379
Iteration 29/1000 | Loss: 0.00001378
Iteration 30/1000 | Loss: 0.00001378
Iteration 31/1000 | Loss: 0.00001378
Iteration 32/1000 | Loss: 0.00001377
Iteration 33/1000 | Loss: 0.00001377
Iteration 34/1000 | Loss: 0.00001377
Iteration 35/1000 | Loss: 0.00001377
Iteration 36/1000 | Loss: 0.00001376
Iteration 37/1000 | Loss: 0.00001376
Iteration 38/1000 | Loss: 0.00001376
Iteration 39/1000 | Loss: 0.00001376
Iteration 40/1000 | Loss: 0.00001375
Iteration 41/1000 | Loss: 0.00001375
Iteration 42/1000 | Loss: 0.00001374
Iteration 43/1000 | Loss: 0.00001374
Iteration 44/1000 | Loss: 0.00001374
Iteration 45/1000 | Loss: 0.00001374
Iteration 46/1000 | Loss: 0.00001373
Iteration 47/1000 | Loss: 0.00001373
Iteration 48/1000 | Loss: 0.00001372
Iteration 49/1000 | Loss: 0.00001372
Iteration 50/1000 | Loss: 0.00001372
Iteration 51/1000 | Loss: 0.00001372
Iteration 52/1000 | Loss: 0.00001372
Iteration 53/1000 | Loss: 0.00001371
Iteration 54/1000 | Loss: 0.00001371
Iteration 55/1000 | Loss: 0.00001371
Iteration 56/1000 | Loss: 0.00001371
Iteration 57/1000 | Loss: 0.00001371
Iteration 58/1000 | Loss: 0.00001371
Iteration 59/1000 | Loss: 0.00001370
Iteration 60/1000 | Loss: 0.00001370
Iteration 61/1000 | Loss: 0.00001369
Iteration 62/1000 | Loss: 0.00001369
Iteration 63/1000 | Loss: 0.00001369
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001367
Iteration 68/1000 | Loss: 0.00001367
Iteration 69/1000 | Loss: 0.00001367
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001367
Iteration 73/1000 | Loss: 0.00001367
Iteration 74/1000 | Loss: 0.00001367
Iteration 75/1000 | Loss: 0.00001366
Iteration 76/1000 | Loss: 0.00001366
Iteration 77/1000 | Loss: 0.00001366
Iteration 78/1000 | Loss: 0.00001366
Iteration 79/1000 | Loss: 0.00001366
Iteration 80/1000 | Loss: 0.00001365
Iteration 81/1000 | Loss: 0.00001365
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001365
Iteration 84/1000 | Loss: 0.00001364
Iteration 85/1000 | Loss: 0.00001364
Iteration 86/1000 | Loss: 0.00001363
Iteration 87/1000 | Loss: 0.00001363
Iteration 88/1000 | Loss: 0.00001363
Iteration 89/1000 | Loss: 0.00001362
Iteration 90/1000 | Loss: 0.00001362
Iteration 91/1000 | Loss: 0.00001362
Iteration 92/1000 | Loss: 0.00001362
Iteration 93/1000 | Loss: 0.00001362
Iteration 94/1000 | Loss: 0.00001362
Iteration 95/1000 | Loss: 0.00001362
Iteration 96/1000 | Loss: 0.00001361
Iteration 97/1000 | Loss: 0.00001361
Iteration 98/1000 | Loss: 0.00001361
Iteration 99/1000 | Loss: 0.00001361
Iteration 100/1000 | Loss: 0.00001361
Iteration 101/1000 | Loss: 0.00001360
Iteration 102/1000 | Loss: 0.00001360
Iteration 103/1000 | Loss: 0.00001360
Iteration 104/1000 | Loss: 0.00001360
Iteration 105/1000 | Loss: 0.00001359
Iteration 106/1000 | Loss: 0.00001359
Iteration 107/1000 | Loss: 0.00001359
Iteration 108/1000 | Loss: 0.00001359
Iteration 109/1000 | Loss: 0.00001359
Iteration 110/1000 | Loss: 0.00001359
Iteration 111/1000 | Loss: 0.00001359
Iteration 112/1000 | Loss: 0.00001359
Iteration 113/1000 | Loss: 0.00001358
Iteration 114/1000 | Loss: 0.00001358
Iteration 115/1000 | Loss: 0.00001358
Iteration 116/1000 | Loss: 0.00001358
Iteration 117/1000 | Loss: 0.00001358
Iteration 118/1000 | Loss: 0.00001358
Iteration 119/1000 | Loss: 0.00001357
Iteration 120/1000 | Loss: 0.00001357
Iteration 121/1000 | Loss: 0.00001357
Iteration 122/1000 | Loss: 0.00001357
Iteration 123/1000 | Loss: 0.00001357
Iteration 124/1000 | Loss: 0.00001356
Iteration 125/1000 | Loss: 0.00001356
Iteration 126/1000 | Loss: 0.00001356
Iteration 127/1000 | Loss: 0.00001356
Iteration 128/1000 | Loss: 0.00001356
Iteration 129/1000 | Loss: 0.00001355
Iteration 130/1000 | Loss: 0.00001355
Iteration 131/1000 | Loss: 0.00001355
Iteration 132/1000 | Loss: 0.00001355
Iteration 133/1000 | Loss: 0.00001355
Iteration 134/1000 | Loss: 0.00001354
Iteration 135/1000 | Loss: 0.00001354
Iteration 136/1000 | Loss: 0.00001354
Iteration 137/1000 | Loss: 0.00001354
Iteration 138/1000 | Loss: 0.00001354
Iteration 139/1000 | Loss: 0.00001354
Iteration 140/1000 | Loss: 0.00001354
Iteration 141/1000 | Loss: 0.00001353
Iteration 142/1000 | Loss: 0.00001353
Iteration 143/1000 | Loss: 0.00001353
Iteration 144/1000 | Loss: 0.00001353
Iteration 145/1000 | Loss: 0.00001352
Iteration 146/1000 | Loss: 0.00001352
Iteration 147/1000 | Loss: 0.00001352
Iteration 148/1000 | Loss: 0.00001352
Iteration 149/1000 | Loss: 0.00001352
Iteration 150/1000 | Loss: 0.00001352
Iteration 151/1000 | Loss: 0.00001351
Iteration 152/1000 | Loss: 0.00001351
Iteration 153/1000 | Loss: 0.00001351
Iteration 154/1000 | Loss: 0.00001351
Iteration 155/1000 | Loss: 0.00001351
Iteration 156/1000 | Loss: 0.00001351
Iteration 157/1000 | Loss: 0.00001351
Iteration 158/1000 | Loss: 0.00001351
Iteration 159/1000 | Loss: 0.00001350
Iteration 160/1000 | Loss: 0.00001350
Iteration 161/1000 | Loss: 0.00001350
Iteration 162/1000 | Loss: 0.00001350
Iteration 163/1000 | Loss: 0.00001350
Iteration 164/1000 | Loss: 0.00001349
Iteration 165/1000 | Loss: 0.00001349
Iteration 166/1000 | Loss: 0.00001349
Iteration 167/1000 | Loss: 0.00001349
Iteration 168/1000 | Loss: 0.00001349
Iteration 169/1000 | Loss: 0.00001349
Iteration 170/1000 | Loss: 0.00001349
Iteration 171/1000 | Loss: 0.00001349
Iteration 172/1000 | Loss: 0.00001348
Iteration 173/1000 | Loss: 0.00001348
Iteration 174/1000 | Loss: 0.00001348
Iteration 175/1000 | Loss: 0.00001348
Iteration 176/1000 | Loss: 0.00001348
Iteration 177/1000 | Loss: 0.00001348
Iteration 178/1000 | Loss: 0.00001348
Iteration 179/1000 | Loss: 0.00001348
Iteration 180/1000 | Loss: 0.00001348
Iteration 181/1000 | Loss: 0.00001348
Iteration 182/1000 | Loss: 0.00001348
Iteration 183/1000 | Loss: 0.00001347
Iteration 184/1000 | Loss: 0.00001347
Iteration 185/1000 | Loss: 0.00001347
Iteration 186/1000 | Loss: 0.00001347
Iteration 187/1000 | Loss: 0.00001347
Iteration 188/1000 | Loss: 0.00001347
Iteration 189/1000 | Loss: 0.00001347
Iteration 190/1000 | Loss: 0.00001347
Iteration 191/1000 | Loss: 0.00001347
Iteration 192/1000 | Loss: 0.00001347
Iteration 193/1000 | Loss: 0.00001347
Iteration 194/1000 | Loss: 0.00001347
Iteration 195/1000 | Loss: 0.00001347
Iteration 196/1000 | Loss: 0.00001347
Iteration 197/1000 | Loss: 0.00001347
Iteration 198/1000 | Loss: 0.00001347
Iteration 199/1000 | Loss: 0.00001346
Iteration 200/1000 | Loss: 0.00001346
Iteration 201/1000 | Loss: 0.00001346
Iteration 202/1000 | Loss: 0.00001346
Iteration 203/1000 | Loss: 0.00001346
Iteration 204/1000 | Loss: 0.00001346
Iteration 205/1000 | Loss: 0.00001346
Iteration 206/1000 | Loss: 0.00001346
Iteration 207/1000 | Loss: 0.00001346
Iteration 208/1000 | Loss: 0.00001346
Iteration 209/1000 | Loss: 0.00001346
Iteration 210/1000 | Loss: 0.00001346
Iteration 211/1000 | Loss: 0.00001346
Iteration 212/1000 | Loss: 0.00001346
Iteration 213/1000 | Loss: 0.00001346
Iteration 214/1000 | Loss: 0.00001346
Iteration 215/1000 | Loss: 0.00001346
Iteration 216/1000 | Loss: 0.00001346
Iteration 217/1000 | Loss: 0.00001346
Iteration 218/1000 | Loss: 0.00001346
Iteration 219/1000 | Loss: 0.00001346
Iteration 220/1000 | Loss: 0.00001346
Iteration 221/1000 | Loss: 0.00001346
Iteration 222/1000 | Loss: 0.00001346
Iteration 223/1000 | Loss: 0.00001346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.3457002751238178e-05, 1.3457002751238178e-05, 1.3457002751238178e-05, 1.3457002751238178e-05, 1.3457002751238178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3457002751238178e-05

Optimization complete. Final v2v error: 3.14009690284729 mm

Highest mean error: 3.626952648162842 mm for frame 37

Lowest mean error: 2.870316982269287 mm for frame 26

Saving results

Total time: 45.60856246948242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413028
Iteration 2/25 | Loss: 0.00136203
Iteration 3/25 | Loss: 0.00126030
Iteration 4/25 | Loss: 0.00123850
Iteration 5/25 | Loss: 0.00122976
Iteration 6/25 | Loss: 0.00122786
Iteration 7/25 | Loss: 0.00122786
Iteration 8/25 | Loss: 0.00122786
Iteration 9/25 | Loss: 0.00122786
Iteration 10/25 | Loss: 0.00122786
Iteration 11/25 | Loss: 0.00122786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001227857661433518, 0.001227857661433518, 0.001227857661433518, 0.001227857661433518, 0.001227857661433518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001227857661433518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42728198
Iteration 2/25 | Loss: 0.00085084
Iteration 3/25 | Loss: 0.00085084
Iteration 4/25 | Loss: 0.00085084
Iteration 5/25 | Loss: 0.00085084
Iteration 6/25 | Loss: 0.00085084
Iteration 7/25 | Loss: 0.00085084
Iteration 8/25 | Loss: 0.00085084
Iteration 9/25 | Loss: 0.00085084
Iteration 10/25 | Loss: 0.00085084
Iteration 11/25 | Loss: 0.00085084
Iteration 12/25 | Loss: 0.00085084
Iteration 13/25 | Loss: 0.00085084
Iteration 14/25 | Loss: 0.00085084
Iteration 15/25 | Loss: 0.00085084
Iteration 16/25 | Loss: 0.00085084
Iteration 17/25 | Loss: 0.00085084
Iteration 18/25 | Loss: 0.00085084
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008508366881869733, 0.0008508366881869733, 0.0008508366881869733, 0.0008508366881869733, 0.0008508366881869733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008508366881869733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085084
Iteration 2/1000 | Loss: 0.00005103
Iteration 3/1000 | Loss: 0.00003316
Iteration 4/1000 | Loss: 0.00002596
Iteration 5/1000 | Loss: 0.00002364
Iteration 6/1000 | Loss: 0.00002194
Iteration 7/1000 | Loss: 0.00002107
Iteration 8/1000 | Loss: 0.00002056
Iteration 9/1000 | Loss: 0.00002030
Iteration 10/1000 | Loss: 0.00002005
Iteration 11/1000 | Loss: 0.00001982
Iteration 12/1000 | Loss: 0.00001979
Iteration 13/1000 | Loss: 0.00001972
Iteration 14/1000 | Loss: 0.00001962
Iteration 15/1000 | Loss: 0.00001950
Iteration 16/1000 | Loss: 0.00001950
Iteration 17/1000 | Loss: 0.00001949
Iteration 18/1000 | Loss: 0.00001949
Iteration 19/1000 | Loss: 0.00001947
Iteration 20/1000 | Loss: 0.00001945
Iteration 21/1000 | Loss: 0.00001945
Iteration 22/1000 | Loss: 0.00001940
Iteration 23/1000 | Loss: 0.00001940
Iteration 24/1000 | Loss: 0.00001940
Iteration 25/1000 | Loss: 0.00001940
Iteration 26/1000 | Loss: 0.00001939
Iteration 27/1000 | Loss: 0.00001939
Iteration 28/1000 | Loss: 0.00001939
Iteration 29/1000 | Loss: 0.00001939
Iteration 30/1000 | Loss: 0.00001938
Iteration 31/1000 | Loss: 0.00001938
Iteration 32/1000 | Loss: 0.00001938
Iteration 33/1000 | Loss: 0.00001937
Iteration 34/1000 | Loss: 0.00001936
Iteration 35/1000 | Loss: 0.00001936
Iteration 36/1000 | Loss: 0.00001936
Iteration 37/1000 | Loss: 0.00001936
Iteration 38/1000 | Loss: 0.00001935
Iteration 39/1000 | Loss: 0.00001935
Iteration 40/1000 | Loss: 0.00001935
Iteration 41/1000 | Loss: 0.00001935
Iteration 42/1000 | Loss: 0.00001935
Iteration 43/1000 | Loss: 0.00001935
Iteration 44/1000 | Loss: 0.00001935
Iteration 45/1000 | Loss: 0.00001935
Iteration 46/1000 | Loss: 0.00001935
Iteration 47/1000 | Loss: 0.00001935
Iteration 48/1000 | Loss: 0.00001935
Iteration 49/1000 | Loss: 0.00001934
Iteration 50/1000 | Loss: 0.00001934
Iteration 51/1000 | Loss: 0.00001934
Iteration 52/1000 | Loss: 0.00001933
Iteration 53/1000 | Loss: 0.00001933
Iteration 54/1000 | Loss: 0.00001932
Iteration 55/1000 | Loss: 0.00001932
Iteration 56/1000 | Loss: 0.00001932
Iteration 57/1000 | Loss: 0.00001932
Iteration 58/1000 | Loss: 0.00001932
Iteration 59/1000 | Loss: 0.00001932
Iteration 60/1000 | Loss: 0.00001932
Iteration 61/1000 | Loss: 0.00001932
Iteration 62/1000 | Loss: 0.00001932
Iteration 63/1000 | Loss: 0.00001932
Iteration 64/1000 | Loss: 0.00001931
Iteration 65/1000 | Loss: 0.00001931
Iteration 66/1000 | Loss: 0.00001931
Iteration 67/1000 | Loss: 0.00001931
Iteration 68/1000 | Loss: 0.00001931
Iteration 69/1000 | Loss: 0.00001931
Iteration 70/1000 | Loss: 0.00001930
Iteration 71/1000 | Loss: 0.00001930
Iteration 72/1000 | Loss: 0.00001929
Iteration 73/1000 | Loss: 0.00001928
Iteration 74/1000 | Loss: 0.00001928
Iteration 75/1000 | Loss: 0.00001928
Iteration 76/1000 | Loss: 0.00001927
Iteration 77/1000 | Loss: 0.00001927
Iteration 78/1000 | Loss: 0.00001927
Iteration 79/1000 | Loss: 0.00001926
Iteration 80/1000 | Loss: 0.00001926
Iteration 81/1000 | Loss: 0.00001926
Iteration 82/1000 | Loss: 0.00001926
Iteration 83/1000 | Loss: 0.00001925
Iteration 84/1000 | Loss: 0.00001925
Iteration 85/1000 | Loss: 0.00001925
Iteration 86/1000 | Loss: 0.00001924
Iteration 87/1000 | Loss: 0.00001924
Iteration 88/1000 | Loss: 0.00001924
Iteration 89/1000 | Loss: 0.00001924
Iteration 90/1000 | Loss: 0.00001923
Iteration 91/1000 | Loss: 0.00001923
Iteration 92/1000 | Loss: 0.00001923
Iteration 93/1000 | Loss: 0.00001922
Iteration 94/1000 | Loss: 0.00001922
Iteration 95/1000 | Loss: 0.00001922
Iteration 96/1000 | Loss: 0.00001922
Iteration 97/1000 | Loss: 0.00001921
Iteration 98/1000 | Loss: 0.00001921
Iteration 99/1000 | Loss: 0.00001921
Iteration 100/1000 | Loss: 0.00001920
Iteration 101/1000 | Loss: 0.00001920
Iteration 102/1000 | Loss: 0.00001920
Iteration 103/1000 | Loss: 0.00001920
Iteration 104/1000 | Loss: 0.00001920
Iteration 105/1000 | Loss: 0.00001920
Iteration 106/1000 | Loss: 0.00001920
Iteration 107/1000 | Loss: 0.00001919
Iteration 108/1000 | Loss: 0.00001919
Iteration 109/1000 | Loss: 0.00001919
Iteration 110/1000 | Loss: 0.00001919
Iteration 111/1000 | Loss: 0.00001919
Iteration 112/1000 | Loss: 0.00001919
Iteration 113/1000 | Loss: 0.00001919
Iteration 114/1000 | Loss: 0.00001918
Iteration 115/1000 | Loss: 0.00001918
Iteration 116/1000 | Loss: 0.00001918
Iteration 117/1000 | Loss: 0.00001918
Iteration 118/1000 | Loss: 0.00001918
Iteration 119/1000 | Loss: 0.00001918
Iteration 120/1000 | Loss: 0.00001918
Iteration 121/1000 | Loss: 0.00001917
Iteration 122/1000 | Loss: 0.00001917
Iteration 123/1000 | Loss: 0.00001917
Iteration 124/1000 | Loss: 0.00001917
Iteration 125/1000 | Loss: 0.00001917
Iteration 126/1000 | Loss: 0.00001917
Iteration 127/1000 | Loss: 0.00001917
Iteration 128/1000 | Loss: 0.00001917
Iteration 129/1000 | Loss: 0.00001917
Iteration 130/1000 | Loss: 0.00001916
Iteration 131/1000 | Loss: 0.00001916
Iteration 132/1000 | Loss: 0.00001916
Iteration 133/1000 | Loss: 0.00001916
Iteration 134/1000 | Loss: 0.00001916
Iteration 135/1000 | Loss: 0.00001916
Iteration 136/1000 | Loss: 0.00001916
Iteration 137/1000 | Loss: 0.00001916
Iteration 138/1000 | Loss: 0.00001915
Iteration 139/1000 | Loss: 0.00001915
Iteration 140/1000 | Loss: 0.00001915
Iteration 141/1000 | Loss: 0.00001915
Iteration 142/1000 | Loss: 0.00001915
Iteration 143/1000 | Loss: 0.00001915
Iteration 144/1000 | Loss: 0.00001915
Iteration 145/1000 | Loss: 0.00001915
Iteration 146/1000 | Loss: 0.00001915
Iteration 147/1000 | Loss: 0.00001915
Iteration 148/1000 | Loss: 0.00001915
Iteration 149/1000 | Loss: 0.00001915
Iteration 150/1000 | Loss: 0.00001915
Iteration 151/1000 | Loss: 0.00001915
Iteration 152/1000 | Loss: 0.00001914
Iteration 153/1000 | Loss: 0.00001914
Iteration 154/1000 | Loss: 0.00001914
Iteration 155/1000 | Loss: 0.00001914
Iteration 156/1000 | Loss: 0.00001914
Iteration 157/1000 | Loss: 0.00001913
Iteration 158/1000 | Loss: 0.00001913
Iteration 159/1000 | Loss: 0.00001913
Iteration 160/1000 | Loss: 0.00001913
Iteration 161/1000 | Loss: 0.00001913
Iteration 162/1000 | Loss: 0.00001913
Iteration 163/1000 | Loss: 0.00001913
Iteration 164/1000 | Loss: 0.00001913
Iteration 165/1000 | Loss: 0.00001913
Iteration 166/1000 | Loss: 0.00001912
Iteration 167/1000 | Loss: 0.00001912
Iteration 168/1000 | Loss: 0.00001912
Iteration 169/1000 | Loss: 0.00001912
Iteration 170/1000 | Loss: 0.00001912
Iteration 171/1000 | Loss: 0.00001912
Iteration 172/1000 | Loss: 0.00001912
Iteration 173/1000 | Loss: 0.00001912
Iteration 174/1000 | Loss: 0.00001912
Iteration 175/1000 | Loss: 0.00001912
Iteration 176/1000 | Loss: 0.00001911
Iteration 177/1000 | Loss: 0.00001911
Iteration 178/1000 | Loss: 0.00001911
Iteration 179/1000 | Loss: 0.00001911
Iteration 180/1000 | Loss: 0.00001911
Iteration 181/1000 | Loss: 0.00001911
Iteration 182/1000 | Loss: 0.00001911
Iteration 183/1000 | Loss: 0.00001911
Iteration 184/1000 | Loss: 0.00001911
Iteration 185/1000 | Loss: 0.00001911
Iteration 186/1000 | Loss: 0.00001911
Iteration 187/1000 | Loss: 0.00001911
Iteration 188/1000 | Loss: 0.00001911
Iteration 189/1000 | Loss: 0.00001911
Iteration 190/1000 | Loss: 0.00001911
Iteration 191/1000 | Loss: 0.00001911
Iteration 192/1000 | Loss: 0.00001911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.911310391733423e-05, 1.911310391733423e-05, 1.911310391733423e-05, 1.911310391733423e-05, 1.911310391733423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.911310391733423e-05

Optimization complete. Final v2v error: 3.6236727237701416 mm

Highest mean error: 4.106290340423584 mm for frame 238

Lowest mean error: 3.2196109294891357 mm for frame 53

Saving results

Total time: 44.51449394226074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399537
Iteration 2/25 | Loss: 0.00128476
Iteration 3/25 | Loss: 0.00120619
Iteration 4/25 | Loss: 0.00119578
Iteration 5/25 | Loss: 0.00119249
Iteration 6/25 | Loss: 0.00119189
Iteration 7/25 | Loss: 0.00119189
Iteration 8/25 | Loss: 0.00119189
Iteration 9/25 | Loss: 0.00119189
Iteration 10/25 | Loss: 0.00119189
Iteration 11/25 | Loss: 0.00119189
Iteration 12/25 | Loss: 0.00119189
Iteration 13/25 | Loss: 0.00119189
Iteration 14/25 | Loss: 0.00119189
Iteration 15/25 | Loss: 0.00119189
Iteration 16/25 | Loss: 0.00119189
Iteration 17/25 | Loss: 0.00119189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011918926611542702, 0.0011918926611542702, 0.0011918926611542702, 0.0011918926611542702, 0.0011918926611542702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011918926611542702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44594383
Iteration 2/25 | Loss: 0.00075846
Iteration 3/25 | Loss: 0.00075846
Iteration 4/25 | Loss: 0.00075846
Iteration 5/25 | Loss: 0.00075846
Iteration 6/25 | Loss: 0.00075846
Iteration 7/25 | Loss: 0.00075846
Iteration 8/25 | Loss: 0.00075846
Iteration 9/25 | Loss: 0.00075846
Iteration 10/25 | Loss: 0.00075846
Iteration 11/25 | Loss: 0.00075846
Iteration 12/25 | Loss: 0.00075846
Iteration 13/25 | Loss: 0.00075846
Iteration 14/25 | Loss: 0.00075846
Iteration 15/25 | Loss: 0.00075846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007584575214423239, 0.0007584575214423239, 0.0007584575214423239, 0.0007584575214423239, 0.0007584575214423239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007584575214423239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075846
Iteration 2/1000 | Loss: 0.00003133
Iteration 3/1000 | Loss: 0.00002089
Iteration 4/1000 | Loss: 0.00001654
Iteration 5/1000 | Loss: 0.00001522
Iteration 6/1000 | Loss: 0.00001435
Iteration 7/1000 | Loss: 0.00001368
Iteration 8/1000 | Loss: 0.00001322
Iteration 9/1000 | Loss: 0.00001296
Iteration 10/1000 | Loss: 0.00001277
Iteration 11/1000 | Loss: 0.00001263
Iteration 12/1000 | Loss: 0.00001259
Iteration 13/1000 | Loss: 0.00001258
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001255
Iteration 17/1000 | Loss: 0.00001251
Iteration 18/1000 | Loss: 0.00001251
Iteration 19/1000 | Loss: 0.00001250
Iteration 20/1000 | Loss: 0.00001250
Iteration 21/1000 | Loss: 0.00001250
Iteration 22/1000 | Loss: 0.00001248
Iteration 23/1000 | Loss: 0.00001247
Iteration 24/1000 | Loss: 0.00001247
Iteration 25/1000 | Loss: 0.00001246
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001243
Iteration 28/1000 | Loss: 0.00001243
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001243
Iteration 32/1000 | Loss: 0.00001243
Iteration 33/1000 | Loss: 0.00001242
Iteration 34/1000 | Loss: 0.00001242
Iteration 35/1000 | Loss: 0.00001242
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001242
Iteration 39/1000 | Loss: 0.00001240
Iteration 40/1000 | Loss: 0.00001239
Iteration 41/1000 | Loss: 0.00001239
Iteration 42/1000 | Loss: 0.00001238
Iteration 43/1000 | Loss: 0.00001238
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001237
Iteration 46/1000 | Loss: 0.00001237
Iteration 47/1000 | Loss: 0.00001237
Iteration 48/1000 | Loss: 0.00001236
Iteration 49/1000 | Loss: 0.00001236
Iteration 50/1000 | Loss: 0.00001236
Iteration 51/1000 | Loss: 0.00001234
Iteration 52/1000 | Loss: 0.00001234
Iteration 53/1000 | Loss: 0.00001234
Iteration 54/1000 | Loss: 0.00001233
Iteration 55/1000 | Loss: 0.00001233
Iteration 56/1000 | Loss: 0.00001233
Iteration 57/1000 | Loss: 0.00001232
Iteration 58/1000 | Loss: 0.00001232
Iteration 59/1000 | Loss: 0.00001231
Iteration 60/1000 | Loss: 0.00001231
Iteration 61/1000 | Loss: 0.00001231
Iteration 62/1000 | Loss: 0.00001231
Iteration 63/1000 | Loss: 0.00001231
Iteration 64/1000 | Loss: 0.00001230
Iteration 65/1000 | Loss: 0.00001230
Iteration 66/1000 | Loss: 0.00001230
Iteration 67/1000 | Loss: 0.00001230
Iteration 68/1000 | Loss: 0.00001229
Iteration 69/1000 | Loss: 0.00001229
Iteration 70/1000 | Loss: 0.00001229
Iteration 71/1000 | Loss: 0.00001228
Iteration 72/1000 | Loss: 0.00001228
Iteration 73/1000 | Loss: 0.00001228
Iteration 74/1000 | Loss: 0.00001228
Iteration 75/1000 | Loss: 0.00001228
Iteration 76/1000 | Loss: 0.00001227
Iteration 77/1000 | Loss: 0.00001227
Iteration 78/1000 | Loss: 0.00001227
Iteration 79/1000 | Loss: 0.00001227
Iteration 80/1000 | Loss: 0.00001227
Iteration 81/1000 | Loss: 0.00001226
Iteration 82/1000 | Loss: 0.00001226
Iteration 83/1000 | Loss: 0.00001226
Iteration 84/1000 | Loss: 0.00001226
Iteration 85/1000 | Loss: 0.00001226
Iteration 86/1000 | Loss: 0.00001226
Iteration 87/1000 | Loss: 0.00001226
Iteration 88/1000 | Loss: 0.00001226
Iteration 89/1000 | Loss: 0.00001226
Iteration 90/1000 | Loss: 0.00001226
Iteration 91/1000 | Loss: 0.00001226
Iteration 92/1000 | Loss: 0.00001225
Iteration 93/1000 | Loss: 0.00001225
Iteration 94/1000 | Loss: 0.00001225
Iteration 95/1000 | Loss: 0.00001225
Iteration 96/1000 | Loss: 0.00001225
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001224
Iteration 99/1000 | Loss: 0.00001223
Iteration 100/1000 | Loss: 0.00001223
Iteration 101/1000 | Loss: 0.00001223
Iteration 102/1000 | Loss: 0.00001223
Iteration 103/1000 | Loss: 0.00001223
Iteration 104/1000 | Loss: 0.00001223
Iteration 105/1000 | Loss: 0.00001223
Iteration 106/1000 | Loss: 0.00001222
Iteration 107/1000 | Loss: 0.00001222
Iteration 108/1000 | Loss: 0.00001222
Iteration 109/1000 | Loss: 0.00001222
Iteration 110/1000 | Loss: 0.00001222
Iteration 111/1000 | Loss: 0.00001222
Iteration 112/1000 | Loss: 0.00001222
Iteration 113/1000 | Loss: 0.00001221
Iteration 114/1000 | Loss: 0.00001221
Iteration 115/1000 | Loss: 0.00001221
Iteration 116/1000 | Loss: 0.00001220
Iteration 117/1000 | Loss: 0.00001220
Iteration 118/1000 | Loss: 0.00001220
Iteration 119/1000 | Loss: 0.00001220
Iteration 120/1000 | Loss: 0.00001220
Iteration 121/1000 | Loss: 0.00001220
Iteration 122/1000 | Loss: 0.00001220
Iteration 123/1000 | Loss: 0.00001220
Iteration 124/1000 | Loss: 0.00001220
Iteration 125/1000 | Loss: 0.00001220
Iteration 126/1000 | Loss: 0.00001220
Iteration 127/1000 | Loss: 0.00001220
Iteration 128/1000 | Loss: 0.00001220
Iteration 129/1000 | Loss: 0.00001220
Iteration 130/1000 | Loss: 0.00001220
Iteration 131/1000 | Loss: 0.00001220
Iteration 132/1000 | Loss: 0.00001220
Iteration 133/1000 | Loss: 0.00001220
Iteration 134/1000 | Loss: 0.00001219
Iteration 135/1000 | Loss: 0.00001219
Iteration 136/1000 | Loss: 0.00001219
Iteration 137/1000 | Loss: 0.00001218
Iteration 138/1000 | Loss: 0.00001218
Iteration 139/1000 | Loss: 0.00001218
Iteration 140/1000 | Loss: 0.00001217
Iteration 141/1000 | Loss: 0.00001217
Iteration 142/1000 | Loss: 0.00001217
Iteration 143/1000 | Loss: 0.00001216
Iteration 144/1000 | Loss: 0.00001216
Iteration 145/1000 | Loss: 0.00001216
Iteration 146/1000 | Loss: 0.00001215
Iteration 147/1000 | Loss: 0.00001215
Iteration 148/1000 | Loss: 0.00001215
Iteration 149/1000 | Loss: 0.00001215
Iteration 150/1000 | Loss: 0.00001215
Iteration 151/1000 | Loss: 0.00001214
Iteration 152/1000 | Loss: 0.00001214
Iteration 153/1000 | Loss: 0.00001214
Iteration 154/1000 | Loss: 0.00001214
Iteration 155/1000 | Loss: 0.00001214
Iteration 156/1000 | Loss: 0.00001214
Iteration 157/1000 | Loss: 0.00001214
Iteration 158/1000 | Loss: 0.00001214
Iteration 159/1000 | Loss: 0.00001213
Iteration 160/1000 | Loss: 0.00001213
Iteration 161/1000 | Loss: 0.00001213
Iteration 162/1000 | Loss: 0.00001213
Iteration 163/1000 | Loss: 0.00001213
Iteration 164/1000 | Loss: 0.00001213
Iteration 165/1000 | Loss: 0.00001212
Iteration 166/1000 | Loss: 0.00001212
Iteration 167/1000 | Loss: 0.00001212
Iteration 168/1000 | Loss: 0.00001212
Iteration 169/1000 | Loss: 0.00001212
Iteration 170/1000 | Loss: 0.00001212
Iteration 171/1000 | Loss: 0.00001212
Iteration 172/1000 | Loss: 0.00001212
Iteration 173/1000 | Loss: 0.00001212
Iteration 174/1000 | Loss: 0.00001211
Iteration 175/1000 | Loss: 0.00001211
Iteration 176/1000 | Loss: 0.00001211
Iteration 177/1000 | Loss: 0.00001211
Iteration 178/1000 | Loss: 0.00001211
Iteration 179/1000 | Loss: 0.00001210
Iteration 180/1000 | Loss: 0.00001210
Iteration 181/1000 | Loss: 0.00001210
Iteration 182/1000 | Loss: 0.00001210
Iteration 183/1000 | Loss: 0.00001210
Iteration 184/1000 | Loss: 0.00001210
Iteration 185/1000 | Loss: 0.00001210
Iteration 186/1000 | Loss: 0.00001210
Iteration 187/1000 | Loss: 0.00001209
Iteration 188/1000 | Loss: 0.00001209
Iteration 189/1000 | Loss: 0.00001209
Iteration 190/1000 | Loss: 0.00001209
Iteration 191/1000 | Loss: 0.00001208
Iteration 192/1000 | Loss: 0.00001208
Iteration 193/1000 | Loss: 0.00001208
Iteration 194/1000 | Loss: 0.00001208
Iteration 195/1000 | Loss: 0.00001207
Iteration 196/1000 | Loss: 0.00001207
Iteration 197/1000 | Loss: 0.00001207
Iteration 198/1000 | Loss: 0.00001207
Iteration 199/1000 | Loss: 0.00001207
Iteration 200/1000 | Loss: 0.00001207
Iteration 201/1000 | Loss: 0.00001207
Iteration 202/1000 | Loss: 0.00001206
Iteration 203/1000 | Loss: 0.00001206
Iteration 204/1000 | Loss: 0.00001206
Iteration 205/1000 | Loss: 0.00001206
Iteration 206/1000 | Loss: 0.00001206
Iteration 207/1000 | Loss: 0.00001206
Iteration 208/1000 | Loss: 0.00001205
Iteration 209/1000 | Loss: 0.00001205
Iteration 210/1000 | Loss: 0.00001205
Iteration 211/1000 | Loss: 0.00001205
Iteration 212/1000 | Loss: 0.00001205
Iteration 213/1000 | Loss: 0.00001205
Iteration 214/1000 | Loss: 0.00001205
Iteration 215/1000 | Loss: 0.00001205
Iteration 216/1000 | Loss: 0.00001205
Iteration 217/1000 | Loss: 0.00001205
Iteration 218/1000 | Loss: 0.00001205
Iteration 219/1000 | Loss: 0.00001205
Iteration 220/1000 | Loss: 0.00001204
Iteration 221/1000 | Loss: 0.00001204
Iteration 222/1000 | Loss: 0.00001204
Iteration 223/1000 | Loss: 0.00001204
Iteration 224/1000 | Loss: 0.00001204
Iteration 225/1000 | Loss: 0.00001204
Iteration 226/1000 | Loss: 0.00001204
Iteration 227/1000 | Loss: 0.00001204
Iteration 228/1000 | Loss: 0.00001204
Iteration 229/1000 | Loss: 0.00001204
Iteration 230/1000 | Loss: 0.00001204
Iteration 231/1000 | Loss: 0.00001204
Iteration 232/1000 | Loss: 0.00001204
Iteration 233/1000 | Loss: 0.00001204
Iteration 234/1000 | Loss: 0.00001204
Iteration 235/1000 | Loss: 0.00001204
Iteration 236/1000 | Loss: 0.00001203
Iteration 237/1000 | Loss: 0.00001203
Iteration 238/1000 | Loss: 0.00001203
Iteration 239/1000 | Loss: 0.00001203
Iteration 240/1000 | Loss: 0.00001203
Iteration 241/1000 | Loss: 0.00001203
Iteration 242/1000 | Loss: 0.00001203
Iteration 243/1000 | Loss: 0.00001203
Iteration 244/1000 | Loss: 0.00001203
Iteration 245/1000 | Loss: 0.00001203
Iteration 246/1000 | Loss: 0.00001203
Iteration 247/1000 | Loss: 0.00001203
Iteration 248/1000 | Loss: 0.00001203
Iteration 249/1000 | Loss: 0.00001202
Iteration 250/1000 | Loss: 0.00001202
Iteration 251/1000 | Loss: 0.00001202
Iteration 252/1000 | Loss: 0.00001202
Iteration 253/1000 | Loss: 0.00001202
Iteration 254/1000 | Loss: 0.00001202
Iteration 255/1000 | Loss: 0.00001202
Iteration 256/1000 | Loss: 0.00001202
Iteration 257/1000 | Loss: 0.00001202
Iteration 258/1000 | Loss: 0.00001202
Iteration 259/1000 | Loss: 0.00001202
Iteration 260/1000 | Loss: 0.00001202
Iteration 261/1000 | Loss: 0.00001202
Iteration 262/1000 | Loss: 0.00001202
Iteration 263/1000 | Loss: 0.00001202
Iteration 264/1000 | Loss: 0.00001202
Iteration 265/1000 | Loss: 0.00001202
Iteration 266/1000 | Loss: 0.00001202
Iteration 267/1000 | Loss: 0.00001201
Iteration 268/1000 | Loss: 0.00001201
Iteration 269/1000 | Loss: 0.00001201
Iteration 270/1000 | Loss: 0.00001201
Iteration 271/1000 | Loss: 0.00001201
Iteration 272/1000 | Loss: 0.00001201
Iteration 273/1000 | Loss: 0.00001201
Iteration 274/1000 | Loss: 0.00001201
Iteration 275/1000 | Loss: 0.00001201
Iteration 276/1000 | Loss: 0.00001201
Iteration 277/1000 | Loss: 0.00001201
Iteration 278/1000 | Loss: 0.00001201
Iteration 279/1000 | Loss: 0.00001201
Iteration 280/1000 | Loss: 0.00001201
Iteration 281/1000 | Loss: 0.00001201
Iteration 282/1000 | Loss: 0.00001201
Iteration 283/1000 | Loss: 0.00001201
Iteration 284/1000 | Loss: 0.00001201
Iteration 285/1000 | Loss: 0.00001201
Iteration 286/1000 | Loss: 0.00001201
Iteration 287/1000 | Loss: 0.00001201
Iteration 288/1000 | Loss: 0.00001201
Iteration 289/1000 | Loss: 0.00001201
Iteration 290/1000 | Loss: 0.00001201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [1.2010300451947842e-05, 1.2010300451947842e-05, 1.2010300451947842e-05, 1.2010300451947842e-05, 1.2010300451947842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2010300451947842e-05

Optimization complete. Final v2v error: 2.9448797702789307 mm

Highest mean error: 3.9359004497528076 mm for frame 87

Lowest mean error: 2.703179121017456 mm for frame 182

Saving results

Total time: 45.65919899940491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512195
Iteration 2/25 | Loss: 0.00155262
Iteration 3/25 | Loss: 0.00132729
Iteration 4/25 | Loss: 0.00130466
Iteration 5/25 | Loss: 0.00129736
Iteration 6/25 | Loss: 0.00129491
Iteration 7/25 | Loss: 0.00129460
Iteration 8/25 | Loss: 0.00129460
Iteration 9/25 | Loss: 0.00129460
Iteration 10/25 | Loss: 0.00129460
Iteration 11/25 | Loss: 0.00129460
Iteration 12/25 | Loss: 0.00129460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012946001952514052, 0.0012946001952514052, 0.0012946001952514052, 0.0012946001952514052, 0.0012946001952514052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012946001952514052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14522851
Iteration 2/25 | Loss: 0.00092209
Iteration 3/25 | Loss: 0.00092207
Iteration 4/25 | Loss: 0.00092207
Iteration 5/25 | Loss: 0.00092207
Iteration 6/25 | Loss: 0.00092207
Iteration 7/25 | Loss: 0.00092207
Iteration 8/25 | Loss: 0.00092207
Iteration 9/25 | Loss: 0.00092207
Iteration 10/25 | Loss: 0.00092207
Iteration 11/25 | Loss: 0.00092207
Iteration 12/25 | Loss: 0.00092207
Iteration 13/25 | Loss: 0.00092207
Iteration 14/25 | Loss: 0.00092207
Iteration 15/25 | Loss: 0.00092207
Iteration 16/25 | Loss: 0.00092207
Iteration 17/25 | Loss: 0.00092207
Iteration 18/25 | Loss: 0.00092207
Iteration 19/25 | Loss: 0.00092207
Iteration 20/25 | Loss: 0.00092207
Iteration 21/25 | Loss: 0.00092207
Iteration 22/25 | Loss: 0.00092207
Iteration 23/25 | Loss: 0.00092207
Iteration 24/25 | Loss: 0.00092207
Iteration 25/25 | Loss: 0.00092207
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009220711071975529, 0.0009220711071975529, 0.0009220711071975529, 0.0009220711071975529, 0.0009220711071975529]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009220711071975529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092207
Iteration 2/1000 | Loss: 0.00006853
Iteration 3/1000 | Loss: 0.00004164
Iteration 4/1000 | Loss: 0.00003320
Iteration 5/1000 | Loss: 0.00003102
Iteration 6/1000 | Loss: 0.00002941
Iteration 7/1000 | Loss: 0.00002834
Iteration 8/1000 | Loss: 0.00002747
Iteration 9/1000 | Loss: 0.00002703
Iteration 10/1000 | Loss: 0.00002664
Iteration 11/1000 | Loss: 0.00002619
Iteration 12/1000 | Loss: 0.00002583
Iteration 13/1000 | Loss: 0.00002561
Iteration 14/1000 | Loss: 0.00002537
Iteration 15/1000 | Loss: 0.00002515
Iteration 16/1000 | Loss: 0.00002509
Iteration 17/1000 | Loss: 0.00002505
Iteration 18/1000 | Loss: 0.00002496
Iteration 19/1000 | Loss: 0.00002491
Iteration 20/1000 | Loss: 0.00002483
Iteration 21/1000 | Loss: 0.00002479
Iteration 22/1000 | Loss: 0.00002479
Iteration 23/1000 | Loss: 0.00002477
Iteration 24/1000 | Loss: 0.00002476
Iteration 25/1000 | Loss: 0.00002470
Iteration 26/1000 | Loss: 0.00002469
Iteration 27/1000 | Loss: 0.00002468
Iteration 28/1000 | Loss: 0.00002467
Iteration 29/1000 | Loss: 0.00002467
Iteration 30/1000 | Loss: 0.00002466
Iteration 31/1000 | Loss: 0.00002466
Iteration 32/1000 | Loss: 0.00002465
Iteration 33/1000 | Loss: 0.00002464
Iteration 34/1000 | Loss: 0.00002463
Iteration 35/1000 | Loss: 0.00002463
Iteration 36/1000 | Loss: 0.00002463
Iteration 37/1000 | Loss: 0.00002463
Iteration 38/1000 | Loss: 0.00002462
Iteration 39/1000 | Loss: 0.00002462
Iteration 40/1000 | Loss: 0.00002462
Iteration 41/1000 | Loss: 0.00002461
Iteration 42/1000 | Loss: 0.00002461
Iteration 43/1000 | Loss: 0.00002461
Iteration 44/1000 | Loss: 0.00002461
Iteration 45/1000 | Loss: 0.00002460
Iteration 46/1000 | Loss: 0.00002460
Iteration 47/1000 | Loss: 0.00002460
Iteration 48/1000 | Loss: 0.00002460
Iteration 49/1000 | Loss: 0.00002459
Iteration 50/1000 | Loss: 0.00002459
Iteration 51/1000 | Loss: 0.00002459
Iteration 52/1000 | Loss: 0.00002458
Iteration 53/1000 | Loss: 0.00002458
Iteration 54/1000 | Loss: 0.00002457
Iteration 55/1000 | Loss: 0.00002457
Iteration 56/1000 | Loss: 0.00002457
Iteration 57/1000 | Loss: 0.00002457
Iteration 58/1000 | Loss: 0.00002457
Iteration 59/1000 | Loss: 0.00002457
Iteration 60/1000 | Loss: 0.00002456
Iteration 61/1000 | Loss: 0.00002456
Iteration 62/1000 | Loss: 0.00002456
Iteration 63/1000 | Loss: 0.00002456
Iteration 64/1000 | Loss: 0.00002456
Iteration 65/1000 | Loss: 0.00002456
Iteration 66/1000 | Loss: 0.00002456
Iteration 67/1000 | Loss: 0.00002456
Iteration 68/1000 | Loss: 0.00002455
Iteration 69/1000 | Loss: 0.00002455
Iteration 70/1000 | Loss: 0.00002455
Iteration 71/1000 | Loss: 0.00002455
Iteration 72/1000 | Loss: 0.00002455
Iteration 73/1000 | Loss: 0.00002455
Iteration 74/1000 | Loss: 0.00002455
Iteration 75/1000 | Loss: 0.00002455
Iteration 76/1000 | Loss: 0.00002454
Iteration 77/1000 | Loss: 0.00002454
Iteration 78/1000 | Loss: 0.00002454
Iteration 79/1000 | Loss: 0.00002454
Iteration 80/1000 | Loss: 0.00002453
Iteration 81/1000 | Loss: 0.00002453
Iteration 82/1000 | Loss: 0.00002453
Iteration 83/1000 | Loss: 0.00002453
Iteration 84/1000 | Loss: 0.00002452
Iteration 85/1000 | Loss: 0.00002452
Iteration 86/1000 | Loss: 0.00002452
Iteration 87/1000 | Loss: 0.00002452
Iteration 88/1000 | Loss: 0.00002452
Iteration 89/1000 | Loss: 0.00002451
Iteration 90/1000 | Loss: 0.00002451
Iteration 91/1000 | Loss: 0.00002451
Iteration 92/1000 | Loss: 0.00002451
Iteration 93/1000 | Loss: 0.00002451
Iteration 94/1000 | Loss: 0.00002451
Iteration 95/1000 | Loss: 0.00002451
Iteration 96/1000 | Loss: 0.00002451
Iteration 97/1000 | Loss: 0.00002451
Iteration 98/1000 | Loss: 0.00002451
Iteration 99/1000 | Loss: 0.00002451
Iteration 100/1000 | Loss: 0.00002451
Iteration 101/1000 | Loss: 0.00002451
Iteration 102/1000 | Loss: 0.00002451
Iteration 103/1000 | Loss: 0.00002451
Iteration 104/1000 | Loss: 0.00002451
Iteration 105/1000 | Loss: 0.00002451
Iteration 106/1000 | Loss: 0.00002451
Iteration 107/1000 | Loss: 0.00002451
Iteration 108/1000 | Loss: 0.00002451
Iteration 109/1000 | Loss: 0.00002451
Iteration 110/1000 | Loss: 0.00002451
Iteration 111/1000 | Loss: 0.00002451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.4511671654181555e-05, 2.4511671654181555e-05, 2.4511671654181555e-05, 2.4511671654181555e-05, 2.4511671654181555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4511671654181555e-05

Optimization complete. Final v2v error: 4.067589282989502 mm

Highest mean error: 5.219595909118652 mm for frame 103

Lowest mean error: 2.9439868927001953 mm for frame 71

Saving results

Total time: 48.62955141067505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985084
Iteration 2/25 | Loss: 0.00250270
Iteration 3/25 | Loss: 0.00182474
Iteration 4/25 | Loss: 0.00168143
Iteration 5/25 | Loss: 0.00174407
Iteration 6/25 | Loss: 0.00162357
Iteration 7/25 | Loss: 0.00157396
Iteration 8/25 | Loss: 0.00143346
Iteration 9/25 | Loss: 0.00141431
Iteration 10/25 | Loss: 0.00147697
Iteration 11/25 | Loss: 0.00137123
Iteration 12/25 | Loss: 0.00133057
Iteration 13/25 | Loss: 0.00133521
Iteration 14/25 | Loss: 0.00132586
Iteration 15/25 | Loss: 0.00129845
Iteration 16/25 | Loss: 0.00128696
Iteration 17/25 | Loss: 0.00128083
Iteration 18/25 | Loss: 0.00127612
Iteration 19/25 | Loss: 0.00128597
Iteration 20/25 | Loss: 0.00128268
Iteration 21/25 | Loss: 0.00127372
Iteration 22/25 | Loss: 0.00126663
Iteration 23/25 | Loss: 0.00126710
Iteration 24/25 | Loss: 0.00127002
Iteration 25/25 | Loss: 0.00127489

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82164383
Iteration 2/25 | Loss: 0.00231160
Iteration 3/25 | Loss: 0.00129508
Iteration 4/25 | Loss: 0.00129508
Iteration 5/25 | Loss: 0.00129508
Iteration 6/25 | Loss: 0.00129508
Iteration 7/25 | Loss: 0.00129508
Iteration 8/25 | Loss: 0.00129508
Iteration 9/25 | Loss: 0.00129508
Iteration 10/25 | Loss: 0.00129508
Iteration 11/25 | Loss: 0.00129508
Iteration 12/25 | Loss: 0.00129508
Iteration 13/25 | Loss: 0.00129508
Iteration 14/25 | Loss: 0.00129508
Iteration 15/25 | Loss: 0.00129508
Iteration 16/25 | Loss: 0.00129508
Iteration 17/25 | Loss: 0.00129508
Iteration 18/25 | Loss: 0.00129508
Iteration 19/25 | Loss: 0.00129508
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012950774980708957, 0.0012950774980708957, 0.0012950774980708957, 0.0012950774980708957, 0.0012950774980708957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012950774980708957

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129508
Iteration 2/1000 | Loss: 0.00202330
Iteration 3/1000 | Loss: 0.00145006
Iteration 4/1000 | Loss: 0.00045270
Iteration 5/1000 | Loss: 0.00066460
Iteration 6/1000 | Loss: 0.00040011
Iteration 7/1000 | Loss: 0.00380320
Iteration 8/1000 | Loss: 0.00181968
Iteration 9/1000 | Loss: 0.00282689
Iteration 10/1000 | Loss: 0.00136458
Iteration 11/1000 | Loss: 0.00222177
Iteration 12/1000 | Loss: 0.00154457
Iteration 13/1000 | Loss: 0.00243769
Iteration 14/1000 | Loss: 0.00147810
Iteration 15/1000 | Loss: 0.00018895
Iteration 16/1000 | Loss: 0.00148102
Iteration 17/1000 | Loss: 0.00123771
Iteration 18/1000 | Loss: 0.00420028
Iteration 19/1000 | Loss: 0.00049033
Iteration 20/1000 | Loss: 0.00029757
Iteration 21/1000 | Loss: 0.00013332
Iteration 22/1000 | Loss: 0.00093096
Iteration 23/1000 | Loss: 0.00017080
Iteration 24/1000 | Loss: 0.00031612
Iteration 25/1000 | Loss: 0.00043751
Iteration 26/1000 | Loss: 0.00154659
Iteration 27/1000 | Loss: 0.00077673
Iteration 28/1000 | Loss: 0.00151116
Iteration 29/1000 | Loss: 0.00063188
Iteration 30/1000 | Loss: 0.00137199
Iteration 31/1000 | Loss: 0.00038043
Iteration 32/1000 | Loss: 0.00039241
Iteration 33/1000 | Loss: 0.00062993
Iteration 34/1000 | Loss: 0.00056123
Iteration 35/1000 | Loss: 0.00024127
Iteration 36/1000 | Loss: 0.00005086
Iteration 37/1000 | Loss: 0.00004292
Iteration 38/1000 | Loss: 0.00006474
Iteration 39/1000 | Loss: 0.00009539
Iteration 40/1000 | Loss: 0.00066329
Iteration 41/1000 | Loss: 0.00025836
Iteration 42/1000 | Loss: 0.00009773
Iteration 43/1000 | Loss: 0.00003535
Iteration 44/1000 | Loss: 0.00003413
Iteration 45/1000 | Loss: 0.00003345
Iteration 46/1000 | Loss: 0.00003274
Iteration 47/1000 | Loss: 0.00072577
Iteration 48/1000 | Loss: 0.00183210
Iteration 49/1000 | Loss: 0.00145478
Iteration 50/1000 | Loss: 0.00123605
Iteration 51/1000 | Loss: 0.00080869
Iteration 52/1000 | Loss: 0.00044511
Iteration 53/1000 | Loss: 0.00109240
Iteration 54/1000 | Loss: 0.00066407
Iteration 55/1000 | Loss: 0.00120859
Iteration 56/1000 | Loss: 0.00085931
Iteration 57/1000 | Loss: 0.00120669
Iteration 58/1000 | Loss: 0.00077414
Iteration 59/1000 | Loss: 0.00086316
Iteration 60/1000 | Loss: 0.00061800
Iteration 61/1000 | Loss: 0.00035748
Iteration 62/1000 | Loss: 0.00040658
Iteration 63/1000 | Loss: 0.00079762
Iteration 64/1000 | Loss: 0.00082747
Iteration 65/1000 | Loss: 0.00110916
Iteration 66/1000 | Loss: 0.00106935
Iteration 67/1000 | Loss: 0.00081530
Iteration 68/1000 | Loss: 0.00112287
Iteration 69/1000 | Loss: 0.00075930
Iteration 70/1000 | Loss: 0.00041146
Iteration 71/1000 | Loss: 0.00010468
Iteration 72/1000 | Loss: 0.00037652
Iteration 73/1000 | Loss: 0.00051224
Iteration 74/1000 | Loss: 0.00040924
Iteration 75/1000 | Loss: 0.00003439
Iteration 76/1000 | Loss: 0.00042110
Iteration 77/1000 | Loss: 0.00037889
Iteration 78/1000 | Loss: 0.00074903
Iteration 79/1000 | Loss: 0.00064053
Iteration 80/1000 | Loss: 0.00060419
Iteration 81/1000 | Loss: 0.00101173
Iteration 82/1000 | Loss: 0.00031618
Iteration 83/1000 | Loss: 0.00004239
Iteration 84/1000 | Loss: 0.00054198
Iteration 85/1000 | Loss: 0.00023131
Iteration 86/1000 | Loss: 0.00114065
Iteration 87/1000 | Loss: 0.00083820
Iteration 88/1000 | Loss: 0.00053405
Iteration 89/1000 | Loss: 0.00026531
Iteration 90/1000 | Loss: 0.00002434
Iteration 91/1000 | Loss: 0.00002081
Iteration 92/1000 | Loss: 0.00001857
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001615
Iteration 95/1000 | Loss: 0.00001564
Iteration 96/1000 | Loss: 0.00001523
Iteration 97/1000 | Loss: 0.00001495
Iteration 98/1000 | Loss: 0.00001478
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001444
Iteration 101/1000 | Loss: 0.00001441
Iteration 102/1000 | Loss: 0.00001441
Iteration 103/1000 | Loss: 0.00001441
Iteration 104/1000 | Loss: 0.00001440
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001438
Iteration 108/1000 | Loss: 0.00001438
Iteration 109/1000 | Loss: 0.00001438
Iteration 110/1000 | Loss: 0.00001437
Iteration 111/1000 | Loss: 0.00001437
Iteration 112/1000 | Loss: 0.00001436
Iteration 113/1000 | Loss: 0.00001436
Iteration 114/1000 | Loss: 0.00001435
Iteration 115/1000 | Loss: 0.00001435
Iteration 116/1000 | Loss: 0.00001433
Iteration 117/1000 | Loss: 0.00001433
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001431
Iteration 121/1000 | Loss: 0.00001431
Iteration 122/1000 | Loss: 0.00001431
Iteration 123/1000 | Loss: 0.00001431
Iteration 124/1000 | Loss: 0.00001430
Iteration 125/1000 | Loss: 0.00001430
Iteration 126/1000 | Loss: 0.00001430
Iteration 127/1000 | Loss: 0.00001430
Iteration 128/1000 | Loss: 0.00001429
Iteration 129/1000 | Loss: 0.00001429
Iteration 130/1000 | Loss: 0.00001429
Iteration 131/1000 | Loss: 0.00001428
Iteration 132/1000 | Loss: 0.00001428
Iteration 133/1000 | Loss: 0.00001427
Iteration 134/1000 | Loss: 0.00001426
Iteration 135/1000 | Loss: 0.00001425
Iteration 136/1000 | Loss: 0.00001422
Iteration 137/1000 | Loss: 0.00001422
Iteration 138/1000 | Loss: 0.00001421
Iteration 139/1000 | Loss: 0.00001421
Iteration 140/1000 | Loss: 0.00001421
Iteration 141/1000 | Loss: 0.00001421
Iteration 142/1000 | Loss: 0.00001420
Iteration 143/1000 | Loss: 0.00001420
Iteration 144/1000 | Loss: 0.00001420
Iteration 145/1000 | Loss: 0.00001419
Iteration 146/1000 | Loss: 0.00001419
Iteration 147/1000 | Loss: 0.00001419
Iteration 148/1000 | Loss: 0.00001419
Iteration 149/1000 | Loss: 0.00001419
Iteration 150/1000 | Loss: 0.00001419
Iteration 151/1000 | Loss: 0.00001419
Iteration 152/1000 | Loss: 0.00001418
Iteration 153/1000 | Loss: 0.00001418
Iteration 154/1000 | Loss: 0.00001418
Iteration 155/1000 | Loss: 0.00001417
Iteration 156/1000 | Loss: 0.00001417
Iteration 157/1000 | Loss: 0.00001416
Iteration 158/1000 | Loss: 0.00001416
Iteration 159/1000 | Loss: 0.00001416
Iteration 160/1000 | Loss: 0.00001416
Iteration 161/1000 | Loss: 0.00001415
Iteration 162/1000 | Loss: 0.00001415
Iteration 163/1000 | Loss: 0.00001415
Iteration 164/1000 | Loss: 0.00001415
Iteration 165/1000 | Loss: 0.00001414
Iteration 166/1000 | Loss: 0.00001414
Iteration 167/1000 | Loss: 0.00001414
Iteration 168/1000 | Loss: 0.00001414
Iteration 169/1000 | Loss: 0.00001413
Iteration 170/1000 | Loss: 0.00001413
Iteration 171/1000 | Loss: 0.00001413
Iteration 172/1000 | Loss: 0.00001413
Iteration 173/1000 | Loss: 0.00001413
Iteration 174/1000 | Loss: 0.00001413
Iteration 175/1000 | Loss: 0.00001412
Iteration 176/1000 | Loss: 0.00001412
Iteration 177/1000 | Loss: 0.00001412
Iteration 178/1000 | Loss: 0.00001412
Iteration 179/1000 | Loss: 0.00001412
Iteration 180/1000 | Loss: 0.00001412
Iteration 181/1000 | Loss: 0.00001412
Iteration 182/1000 | Loss: 0.00001412
Iteration 183/1000 | Loss: 0.00001412
Iteration 184/1000 | Loss: 0.00001412
Iteration 185/1000 | Loss: 0.00001411
Iteration 186/1000 | Loss: 0.00001411
Iteration 187/1000 | Loss: 0.00001411
Iteration 188/1000 | Loss: 0.00001411
Iteration 189/1000 | Loss: 0.00001411
Iteration 190/1000 | Loss: 0.00001411
Iteration 191/1000 | Loss: 0.00001411
Iteration 192/1000 | Loss: 0.00001411
Iteration 193/1000 | Loss: 0.00001411
Iteration 194/1000 | Loss: 0.00001411
Iteration 195/1000 | Loss: 0.00001411
Iteration 196/1000 | Loss: 0.00001411
Iteration 197/1000 | Loss: 0.00001411
Iteration 198/1000 | Loss: 0.00001411
Iteration 199/1000 | Loss: 0.00001411
Iteration 200/1000 | Loss: 0.00001411
Iteration 201/1000 | Loss: 0.00001411
Iteration 202/1000 | Loss: 0.00001411
Iteration 203/1000 | Loss: 0.00001411
Iteration 204/1000 | Loss: 0.00001411
Iteration 205/1000 | Loss: 0.00001411
Iteration 206/1000 | Loss: 0.00001411
Iteration 207/1000 | Loss: 0.00001411
Iteration 208/1000 | Loss: 0.00001411
Iteration 209/1000 | Loss: 0.00001411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.410562344972277e-05, 1.410562344972277e-05, 1.410562344972277e-05, 1.410562344972277e-05, 1.410562344972277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.410562344972277e-05

Optimization complete. Final v2v error: 3.194958209991455 mm

Highest mean error: 4.416710376739502 mm for frame 59

Lowest mean error: 2.82546067237854 mm for frame 91

Saving results

Total time: 189.13842821121216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815975
Iteration 2/25 | Loss: 0.00142139
Iteration 3/25 | Loss: 0.00128803
Iteration 4/25 | Loss: 0.00127278
Iteration 5/25 | Loss: 0.00126850
Iteration 6/25 | Loss: 0.00126793
Iteration 7/25 | Loss: 0.00126793
Iteration 8/25 | Loss: 0.00126793
Iteration 9/25 | Loss: 0.00126793
Iteration 10/25 | Loss: 0.00126793
Iteration 11/25 | Loss: 0.00126793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001267933170311153, 0.001267933170311153, 0.001267933170311153, 0.001267933170311153, 0.001267933170311153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001267933170311153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60658813
Iteration 2/25 | Loss: 0.00078478
Iteration 3/25 | Loss: 0.00078478
Iteration 4/25 | Loss: 0.00078477
Iteration 5/25 | Loss: 0.00078477
Iteration 6/25 | Loss: 0.00078477
Iteration 7/25 | Loss: 0.00078477
Iteration 8/25 | Loss: 0.00078477
Iteration 9/25 | Loss: 0.00078477
Iteration 10/25 | Loss: 0.00078477
Iteration 11/25 | Loss: 0.00078477
Iteration 12/25 | Loss: 0.00078477
Iteration 13/25 | Loss: 0.00078477
Iteration 14/25 | Loss: 0.00078477
Iteration 15/25 | Loss: 0.00078477
Iteration 16/25 | Loss: 0.00078477
Iteration 17/25 | Loss: 0.00078477
Iteration 18/25 | Loss: 0.00078477
Iteration 19/25 | Loss: 0.00078477
Iteration 20/25 | Loss: 0.00078477
Iteration 21/25 | Loss: 0.00078477
Iteration 22/25 | Loss: 0.00078477
Iteration 23/25 | Loss: 0.00078477
Iteration 24/25 | Loss: 0.00078477
Iteration 25/25 | Loss: 0.00078477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078477
Iteration 2/1000 | Loss: 0.00004217
Iteration 3/1000 | Loss: 0.00002700
Iteration 4/1000 | Loss: 0.00002355
Iteration 5/1000 | Loss: 0.00002206
Iteration 6/1000 | Loss: 0.00002113
Iteration 7/1000 | Loss: 0.00002047
Iteration 8/1000 | Loss: 0.00001995
Iteration 9/1000 | Loss: 0.00001961
Iteration 10/1000 | Loss: 0.00001929
Iteration 11/1000 | Loss: 0.00001902
Iteration 12/1000 | Loss: 0.00001882
Iteration 13/1000 | Loss: 0.00001867
Iteration 14/1000 | Loss: 0.00001861
Iteration 15/1000 | Loss: 0.00001848
Iteration 16/1000 | Loss: 0.00001844
Iteration 17/1000 | Loss: 0.00001838
Iteration 18/1000 | Loss: 0.00001834
Iteration 19/1000 | Loss: 0.00001833
Iteration 20/1000 | Loss: 0.00001832
Iteration 21/1000 | Loss: 0.00001832
Iteration 22/1000 | Loss: 0.00001832
Iteration 23/1000 | Loss: 0.00001831
Iteration 24/1000 | Loss: 0.00001830
Iteration 25/1000 | Loss: 0.00001830
Iteration 26/1000 | Loss: 0.00001829
Iteration 27/1000 | Loss: 0.00001828
Iteration 28/1000 | Loss: 0.00001827
Iteration 29/1000 | Loss: 0.00001827
Iteration 30/1000 | Loss: 0.00001826
Iteration 31/1000 | Loss: 0.00001825
Iteration 32/1000 | Loss: 0.00001824
Iteration 33/1000 | Loss: 0.00001824
Iteration 34/1000 | Loss: 0.00001822
Iteration 35/1000 | Loss: 0.00001819
Iteration 36/1000 | Loss: 0.00001818
Iteration 37/1000 | Loss: 0.00001818
Iteration 38/1000 | Loss: 0.00001817
Iteration 39/1000 | Loss: 0.00001816
Iteration 40/1000 | Loss: 0.00001815
Iteration 41/1000 | Loss: 0.00001815
Iteration 42/1000 | Loss: 0.00001814
Iteration 43/1000 | Loss: 0.00001813
Iteration 44/1000 | Loss: 0.00001813
Iteration 45/1000 | Loss: 0.00001812
Iteration 46/1000 | Loss: 0.00001812
Iteration 47/1000 | Loss: 0.00001812
Iteration 48/1000 | Loss: 0.00001812
Iteration 49/1000 | Loss: 0.00001811
Iteration 50/1000 | Loss: 0.00001811
Iteration 51/1000 | Loss: 0.00001810
Iteration 52/1000 | Loss: 0.00001810
Iteration 53/1000 | Loss: 0.00001809
Iteration 54/1000 | Loss: 0.00001809
Iteration 55/1000 | Loss: 0.00001808
Iteration 56/1000 | Loss: 0.00001808
Iteration 57/1000 | Loss: 0.00001808
Iteration 58/1000 | Loss: 0.00001806
Iteration 59/1000 | Loss: 0.00001806
Iteration 60/1000 | Loss: 0.00001806
Iteration 61/1000 | Loss: 0.00001806
Iteration 62/1000 | Loss: 0.00001806
Iteration 63/1000 | Loss: 0.00001806
Iteration 64/1000 | Loss: 0.00001806
Iteration 65/1000 | Loss: 0.00001806
Iteration 66/1000 | Loss: 0.00001806
Iteration 67/1000 | Loss: 0.00001805
Iteration 68/1000 | Loss: 0.00001805
Iteration 69/1000 | Loss: 0.00001804
Iteration 70/1000 | Loss: 0.00001804
Iteration 71/1000 | Loss: 0.00001803
Iteration 72/1000 | Loss: 0.00001803
Iteration 73/1000 | Loss: 0.00001802
Iteration 74/1000 | Loss: 0.00001802
Iteration 75/1000 | Loss: 0.00001801
Iteration 76/1000 | Loss: 0.00001801
Iteration 77/1000 | Loss: 0.00001801
Iteration 78/1000 | Loss: 0.00001800
Iteration 79/1000 | Loss: 0.00001800
Iteration 80/1000 | Loss: 0.00001800
Iteration 81/1000 | Loss: 0.00001799
Iteration 82/1000 | Loss: 0.00001799
Iteration 83/1000 | Loss: 0.00001799
Iteration 84/1000 | Loss: 0.00001799
Iteration 85/1000 | Loss: 0.00001799
Iteration 86/1000 | Loss: 0.00001798
Iteration 87/1000 | Loss: 0.00001798
Iteration 88/1000 | Loss: 0.00001798
Iteration 89/1000 | Loss: 0.00001797
Iteration 90/1000 | Loss: 0.00001797
Iteration 91/1000 | Loss: 0.00001797
Iteration 92/1000 | Loss: 0.00001797
Iteration 93/1000 | Loss: 0.00001797
Iteration 94/1000 | Loss: 0.00001797
Iteration 95/1000 | Loss: 0.00001797
Iteration 96/1000 | Loss: 0.00001797
Iteration 97/1000 | Loss: 0.00001796
Iteration 98/1000 | Loss: 0.00001796
Iteration 99/1000 | Loss: 0.00001795
Iteration 100/1000 | Loss: 0.00001795
Iteration 101/1000 | Loss: 0.00001794
Iteration 102/1000 | Loss: 0.00001794
Iteration 103/1000 | Loss: 0.00001794
Iteration 104/1000 | Loss: 0.00001794
Iteration 105/1000 | Loss: 0.00001794
Iteration 106/1000 | Loss: 0.00001794
Iteration 107/1000 | Loss: 0.00001794
Iteration 108/1000 | Loss: 0.00001794
Iteration 109/1000 | Loss: 0.00001793
Iteration 110/1000 | Loss: 0.00001793
Iteration 111/1000 | Loss: 0.00001793
Iteration 112/1000 | Loss: 0.00001793
Iteration 113/1000 | Loss: 0.00001793
Iteration 114/1000 | Loss: 0.00001793
Iteration 115/1000 | Loss: 0.00001792
Iteration 116/1000 | Loss: 0.00001792
Iteration 117/1000 | Loss: 0.00001792
Iteration 118/1000 | Loss: 0.00001792
Iteration 119/1000 | Loss: 0.00001792
Iteration 120/1000 | Loss: 0.00001792
Iteration 121/1000 | Loss: 0.00001792
Iteration 122/1000 | Loss: 0.00001792
Iteration 123/1000 | Loss: 0.00001792
Iteration 124/1000 | Loss: 0.00001792
Iteration 125/1000 | Loss: 0.00001792
Iteration 126/1000 | Loss: 0.00001791
Iteration 127/1000 | Loss: 0.00001791
Iteration 128/1000 | Loss: 0.00001791
Iteration 129/1000 | Loss: 0.00001791
Iteration 130/1000 | Loss: 0.00001791
Iteration 131/1000 | Loss: 0.00001791
Iteration 132/1000 | Loss: 0.00001791
Iteration 133/1000 | Loss: 0.00001791
Iteration 134/1000 | Loss: 0.00001791
Iteration 135/1000 | Loss: 0.00001791
Iteration 136/1000 | Loss: 0.00001791
Iteration 137/1000 | Loss: 0.00001790
Iteration 138/1000 | Loss: 0.00001790
Iteration 139/1000 | Loss: 0.00001790
Iteration 140/1000 | Loss: 0.00001790
Iteration 141/1000 | Loss: 0.00001790
Iteration 142/1000 | Loss: 0.00001790
Iteration 143/1000 | Loss: 0.00001789
Iteration 144/1000 | Loss: 0.00001789
Iteration 145/1000 | Loss: 0.00001789
Iteration 146/1000 | Loss: 0.00001789
Iteration 147/1000 | Loss: 0.00001789
Iteration 148/1000 | Loss: 0.00001789
Iteration 149/1000 | Loss: 0.00001789
Iteration 150/1000 | Loss: 0.00001788
Iteration 151/1000 | Loss: 0.00001788
Iteration 152/1000 | Loss: 0.00001788
Iteration 153/1000 | Loss: 0.00001788
Iteration 154/1000 | Loss: 0.00001788
Iteration 155/1000 | Loss: 0.00001788
Iteration 156/1000 | Loss: 0.00001788
Iteration 157/1000 | Loss: 0.00001788
Iteration 158/1000 | Loss: 0.00001787
Iteration 159/1000 | Loss: 0.00001787
Iteration 160/1000 | Loss: 0.00001787
Iteration 161/1000 | Loss: 0.00001787
Iteration 162/1000 | Loss: 0.00001787
Iteration 163/1000 | Loss: 0.00001787
Iteration 164/1000 | Loss: 0.00001787
Iteration 165/1000 | Loss: 0.00001787
Iteration 166/1000 | Loss: 0.00001787
Iteration 167/1000 | Loss: 0.00001786
Iteration 168/1000 | Loss: 0.00001786
Iteration 169/1000 | Loss: 0.00001786
Iteration 170/1000 | Loss: 0.00001786
Iteration 171/1000 | Loss: 0.00001786
Iteration 172/1000 | Loss: 0.00001786
Iteration 173/1000 | Loss: 0.00001786
Iteration 174/1000 | Loss: 0.00001786
Iteration 175/1000 | Loss: 0.00001785
Iteration 176/1000 | Loss: 0.00001785
Iteration 177/1000 | Loss: 0.00001785
Iteration 178/1000 | Loss: 0.00001785
Iteration 179/1000 | Loss: 0.00001785
Iteration 180/1000 | Loss: 0.00001785
Iteration 181/1000 | Loss: 0.00001785
Iteration 182/1000 | Loss: 0.00001785
Iteration 183/1000 | Loss: 0.00001785
Iteration 184/1000 | Loss: 0.00001785
Iteration 185/1000 | Loss: 0.00001785
Iteration 186/1000 | Loss: 0.00001785
Iteration 187/1000 | Loss: 0.00001785
Iteration 188/1000 | Loss: 0.00001785
Iteration 189/1000 | Loss: 0.00001785
Iteration 190/1000 | Loss: 0.00001785
Iteration 191/1000 | Loss: 0.00001784
Iteration 192/1000 | Loss: 0.00001784
Iteration 193/1000 | Loss: 0.00001784
Iteration 194/1000 | Loss: 0.00001784
Iteration 195/1000 | Loss: 0.00001784
Iteration 196/1000 | Loss: 0.00001784
Iteration 197/1000 | Loss: 0.00001784
Iteration 198/1000 | Loss: 0.00001784
Iteration 199/1000 | Loss: 0.00001784
Iteration 200/1000 | Loss: 0.00001784
Iteration 201/1000 | Loss: 0.00001784
Iteration 202/1000 | Loss: 0.00001784
Iteration 203/1000 | Loss: 0.00001784
Iteration 204/1000 | Loss: 0.00001783
Iteration 205/1000 | Loss: 0.00001783
Iteration 206/1000 | Loss: 0.00001783
Iteration 207/1000 | Loss: 0.00001783
Iteration 208/1000 | Loss: 0.00001783
Iteration 209/1000 | Loss: 0.00001783
Iteration 210/1000 | Loss: 0.00001783
Iteration 211/1000 | Loss: 0.00001783
Iteration 212/1000 | Loss: 0.00001783
Iteration 213/1000 | Loss: 0.00001783
Iteration 214/1000 | Loss: 0.00001783
Iteration 215/1000 | Loss: 0.00001783
Iteration 216/1000 | Loss: 0.00001783
Iteration 217/1000 | Loss: 0.00001783
Iteration 218/1000 | Loss: 0.00001783
Iteration 219/1000 | Loss: 0.00001783
Iteration 220/1000 | Loss: 0.00001782
Iteration 221/1000 | Loss: 0.00001782
Iteration 222/1000 | Loss: 0.00001782
Iteration 223/1000 | Loss: 0.00001782
Iteration 224/1000 | Loss: 0.00001782
Iteration 225/1000 | Loss: 0.00001782
Iteration 226/1000 | Loss: 0.00001782
Iteration 227/1000 | Loss: 0.00001782
Iteration 228/1000 | Loss: 0.00001782
Iteration 229/1000 | Loss: 0.00001782
Iteration 230/1000 | Loss: 0.00001782
Iteration 231/1000 | Loss: 0.00001781
Iteration 232/1000 | Loss: 0.00001781
Iteration 233/1000 | Loss: 0.00001781
Iteration 234/1000 | Loss: 0.00001781
Iteration 235/1000 | Loss: 0.00001781
Iteration 236/1000 | Loss: 0.00001781
Iteration 237/1000 | Loss: 0.00001781
Iteration 238/1000 | Loss: 0.00001781
Iteration 239/1000 | Loss: 0.00001781
Iteration 240/1000 | Loss: 0.00001781
Iteration 241/1000 | Loss: 0.00001781
Iteration 242/1000 | Loss: 0.00001781
Iteration 243/1000 | Loss: 0.00001781
Iteration 244/1000 | Loss: 0.00001781
Iteration 245/1000 | Loss: 0.00001781
Iteration 246/1000 | Loss: 0.00001781
Iteration 247/1000 | Loss: 0.00001781
Iteration 248/1000 | Loss: 0.00001780
Iteration 249/1000 | Loss: 0.00001780
Iteration 250/1000 | Loss: 0.00001780
Iteration 251/1000 | Loss: 0.00001780
Iteration 252/1000 | Loss: 0.00001780
Iteration 253/1000 | Loss: 0.00001780
Iteration 254/1000 | Loss: 0.00001780
Iteration 255/1000 | Loss: 0.00001780
Iteration 256/1000 | Loss: 0.00001780
Iteration 257/1000 | Loss: 0.00001780
Iteration 258/1000 | Loss: 0.00001780
Iteration 259/1000 | Loss: 0.00001780
Iteration 260/1000 | Loss: 0.00001780
Iteration 261/1000 | Loss: 0.00001780
Iteration 262/1000 | Loss: 0.00001780
Iteration 263/1000 | Loss: 0.00001780
Iteration 264/1000 | Loss: 0.00001780
Iteration 265/1000 | Loss: 0.00001780
Iteration 266/1000 | Loss: 0.00001780
Iteration 267/1000 | Loss: 0.00001780
Iteration 268/1000 | Loss: 0.00001780
Iteration 269/1000 | Loss: 0.00001780
Iteration 270/1000 | Loss: 0.00001780
Iteration 271/1000 | Loss: 0.00001780
Iteration 272/1000 | Loss: 0.00001780
Iteration 273/1000 | Loss: 0.00001780
Iteration 274/1000 | Loss: 0.00001780
Iteration 275/1000 | Loss: 0.00001780
Iteration 276/1000 | Loss: 0.00001780
Iteration 277/1000 | Loss: 0.00001780
Iteration 278/1000 | Loss: 0.00001780
Iteration 279/1000 | Loss: 0.00001780
Iteration 280/1000 | Loss: 0.00001780
Iteration 281/1000 | Loss: 0.00001780
Iteration 282/1000 | Loss: 0.00001780
Iteration 283/1000 | Loss: 0.00001780
Iteration 284/1000 | Loss: 0.00001780
Iteration 285/1000 | Loss: 0.00001780
Iteration 286/1000 | Loss: 0.00001780
Iteration 287/1000 | Loss: 0.00001780
Iteration 288/1000 | Loss: 0.00001780
Iteration 289/1000 | Loss: 0.00001780
Iteration 290/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [1.780042475729715e-05, 1.780042475729715e-05, 1.780042475729715e-05, 1.780042475729715e-05, 1.780042475729715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.780042475729715e-05

Optimization complete. Final v2v error: 3.5483667850494385 mm

Highest mean error: 4.4569621086120605 mm for frame 86

Lowest mean error: 3.059495687484741 mm for frame 60

Saving results

Total time: 48.407665967941284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991414
Iteration 2/25 | Loss: 0.00183063
Iteration 3/25 | Loss: 0.00145745
Iteration 4/25 | Loss: 0.00141802
Iteration 5/25 | Loss: 0.00141109
Iteration 6/25 | Loss: 0.00141035
Iteration 7/25 | Loss: 0.00141035
Iteration 8/25 | Loss: 0.00141035
Iteration 9/25 | Loss: 0.00141035
Iteration 10/25 | Loss: 0.00141035
Iteration 11/25 | Loss: 0.00141035
Iteration 12/25 | Loss: 0.00141035
Iteration 13/25 | Loss: 0.00141035
Iteration 14/25 | Loss: 0.00141035
Iteration 15/25 | Loss: 0.00141035
Iteration 16/25 | Loss: 0.00141035
Iteration 17/25 | Loss: 0.00141035
Iteration 18/25 | Loss: 0.00141035
Iteration 19/25 | Loss: 0.00141035
Iteration 20/25 | Loss: 0.00141035
Iteration 21/25 | Loss: 0.00141035
Iteration 22/25 | Loss: 0.00141035
Iteration 23/25 | Loss: 0.00141035
Iteration 24/25 | Loss: 0.00141035
Iteration 25/25 | Loss: 0.00141035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.84711647
Iteration 2/25 | Loss: 0.00075810
Iteration 3/25 | Loss: 0.00075806
Iteration 4/25 | Loss: 0.00075806
Iteration 5/25 | Loss: 0.00075806
Iteration 6/25 | Loss: 0.00075806
Iteration 7/25 | Loss: 0.00075806
Iteration 8/25 | Loss: 0.00075806
Iteration 9/25 | Loss: 0.00075806
Iteration 10/25 | Loss: 0.00075806
Iteration 11/25 | Loss: 0.00075806
Iteration 12/25 | Loss: 0.00075806
Iteration 13/25 | Loss: 0.00075806
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007580573437735438, 0.0007580573437735438, 0.0007580573437735438, 0.0007580573437735438, 0.0007580573437735438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007580573437735438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075806
Iteration 2/1000 | Loss: 0.00006960
Iteration 3/1000 | Loss: 0.00004491
Iteration 4/1000 | Loss: 0.00004059
Iteration 5/1000 | Loss: 0.00003880
Iteration 6/1000 | Loss: 0.00003738
Iteration 7/1000 | Loss: 0.00003657
Iteration 8/1000 | Loss: 0.00003602
Iteration 9/1000 | Loss: 0.00003548
Iteration 10/1000 | Loss: 0.00003508
Iteration 11/1000 | Loss: 0.00003484
Iteration 12/1000 | Loss: 0.00003461
Iteration 13/1000 | Loss: 0.00003441
Iteration 14/1000 | Loss: 0.00003422
Iteration 15/1000 | Loss: 0.00003407
Iteration 16/1000 | Loss: 0.00003404
Iteration 17/1000 | Loss: 0.00003397
Iteration 18/1000 | Loss: 0.00003390
Iteration 19/1000 | Loss: 0.00003389
Iteration 20/1000 | Loss: 0.00003387
Iteration 21/1000 | Loss: 0.00003386
Iteration 22/1000 | Loss: 0.00003385
Iteration 23/1000 | Loss: 0.00003382
Iteration 24/1000 | Loss: 0.00003380
Iteration 25/1000 | Loss: 0.00003375
Iteration 26/1000 | Loss: 0.00003374
Iteration 27/1000 | Loss: 0.00003371
Iteration 28/1000 | Loss: 0.00003371
Iteration 29/1000 | Loss: 0.00003370
Iteration 30/1000 | Loss: 0.00003369
Iteration 31/1000 | Loss: 0.00003368
Iteration 32/1000 | Loss: 0.00003368
Iteration 33/1000 | Loss: 0.00003367
Iteration 34/1000 | Loss: 0.00003364
Iteration 35/1000 | Loss: 0.00003364
Iteration 36/1000 | Loss: 0.00003363
Iteration 37/1000 | Loss: 0.00003363
Iteration 38/1000 | Loss: 0.00003360
Iteration 39/1000 | Loss: 0.00003359
Iteration 40/1000 | Loss: 0.00003358
Iteration 41/1000 | Loss: 0.00003358
Iteration 42/1000 | Loss: 0.00003358
Iteration 43/1000 | Loss: 0.00003357
Iteration 44/1000 | Loss: 0.00003357
Iteration 45/1000 | Loss: 0.00003357
Iteration 46/1000 | Loss: 0.00003355
Iteration 47/1000 | Loss: 0.00003355
Iteration 48/1000 | Loss: 0.00003355
Iteration 49/1000 | Loss: 0.00003354
Iteration 50/1000 | Loss: 0.00003354
Iteration 51/1000 | Loss: 0.00003354
Iteration 52/1000 | Loss: 0.00003354
Iteration 53/1000 | Loss: 0.00003354
Iteration 54/1000 | Loss: 0.00003354
Iteration 55/1000 | Loss: 0.00003354
Iteration 56/1000 | Loss: 0.00003353
Iteration 57/1000 | Loss: 0.00003352
Iteration 58/1000 | Loss: 0.00003352
Iteration 59/1000 | Loss: 0.00003352
Iteration 60/1000 | Loss: 0.00003352
Iteration 61/1000 | Loss: 0.00003352
Iteration 62/1000 | Loss: 0.00003351
Iteration 63/1000 | Loss: 0.00003351
Iteration 64/1000 | Loss: 0.00003351
Iteration 65/1000 | Loss: 0.00003351
Iteration 66/1000 | Loss: 0.00003351
Iteration 67/1000 | Loss: 0.00003351
Iteration 68/1000 | Loss: 0.00003351
Iteration 69/1000 | Loss: 0.00003351
Iteration 70/1000 | Loss: 0.00003351
Iteration 71/1000 | Loss: 0.00003351
Iteration 72/1000 | Loss: 0.00003351
Iteration 73/1000 | Loss: 0.00003351
Iteration 74/1000 | Loss: 0.00003351
Iteration 75/1000 | Loss: 0.00003350
Iteration 76/1000 | Loss: 0.00003350
Iteration 77/1000 | Loss: 0.00003349
Iteration 78/1000 | Loss: 0.00003349
Iteration 79/1000 | Loss: 0.00003349
Iteration 80/1000 | Loss: 0.00003349
Iteration 81/1000 | Loss: 0.00003349
Iteration 82/1000 | Loss: 0.00003349
Iteration 83/1000 | Loss: 0.00003349
Iteration 84/1000 | Loss: 0.00003349
Iteration 85/1000 | Loss: 0.00003349
Iteration 86/1000 | Loss: 0.00003349
Iteration 87/1000 | Loss: 0.00003349
Iteration 88/1000 | Loss: 0.00003349
Iteration 89/1000 | Loss: 0.00003348
Iteration 90/1000 | Loss: 0.00003348
Iteration 91/1000 | Loss: 0.00003348
Iteration 92/1000 | Loss: 0.00003348
Iteration 93/1000 | Loss: 0.00003348
Iteration 94/1000 | Loss: 0.00003348
Iteration 95/1000 | Loss: 0.00003348
Iteration 96/1000 | Loss: 0.00003347
Iteration 97/1000 | Loss: 0.00003347
Iteration 98/1000 | Loss: 0.00003347
Iteration 99/1000 | Loss: 0.00003347
Iteration 100/1000 | Loss: 0.00003346
Iteration 101/1000 | Loss: 0.00003346
Iteration 102/1000 | Loss: 0.00003346
Iteration 103/1000 | Loss: 0.00003346
Iteration 104/1000 | Loss: 0.00003345
Iteration 105/1000 | Loss: 0.00003345
Iteration 106/1000 | Loss: 0.00003345
Iteration 107/1000 | Loss: 0.00003345
Iteration 108/1000 | Loss: 0.00003345
Iteration 109/1000 | Loss: 0.00003345
Iteration 110/1000 | Loss: 0.00003345
Iteration 111/1000 | Loss: 0.00003345
Iteration 112/1000 | Loss: 0.00003345
Iteration 113/1000 | Loss: 0.00003345
Iteration 114/1000 | Loss: 0.00003345
Iteration 115/1000 | Loss: 0.00003345
Iteration 116/1000 | Loss: 0.00003345
Iteration 117/1000 | Loss: 0.00003345
Iteration 118/1000 | Loss: 0.00003345
Iteration 119/1000 | Loss: 0.00003345
Iteration 120/1000 | Loss: 0.00003345
Iteration 121/1000 | Loss: 0.00003345
Iteration 122/1000 | Loss: 0.00003345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [3.34503420162946e-05, 3.34503420162946e-05, 3.34503420162946e-05, 3.34503420162946e-05, 3.34503420162946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.34503420162946e-05

Optimization complete. Final v2v error: 4.720360279083252 mm

Highest mean error: 5.639626502990723 mm for frame 0

Lowest mean error: 4.084111213684082 mm for frame 195

Saving results

Total time: 46.53342580795288
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00610668
Iteration 2/25 | Loss: 0.00167884
Iteration 3/25 | Loss: 0.00151520
Iteration 4/25 | Loss: 0.00148063
Iteration 5/25 | Loss: 0.00148854
Iteration 6/25 | Loss: 0.00142084
Iteration 7/25 | Loss: 0.00138198
Iteration 8/25 | Loss: 0.00137909
Iteration 9/25 | Loss: 0.00138047
Iteration 10/25 | Loss: 0.00137035
Iteration 11/25 | Loss: 0.00136927
Iteration 12/25 | Loss: 0.00136294
Iteration 13/25 | Loss: 0.00136053
Iteration 14/25 | Loss: 0.00135985
Iteration 15/25 | Loss: 0.00135957
Iteration 16/25 | Loss: 0.00135937
Iteration 17/25 | Loss: 0.00135936
Iteration 18/25 | Loss: 0.00135936
Iteration 19/25 | Loss: 0.00135936
Iteration 20/25 | Loss: 0.00135935
Iteration 21/25 | Loss: 0.00135935
Iteration 22/25 | Loss: 0.00135935
Iteration 23/25 | Loss: 0.00135935
Iteration 24/25 | Loss: 0.00135935
Iteration 25/25 | Loss: 0.00135935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32786191
Iteration 2/25 | Loss: 0.00166371
Iteration 3/25 | Loss: 0.00166364
Iteration 4/25 | Loss: 0.00166364
Iteration 5/25 | Loss: 0.00166364
Iteration 6/25 | Loss: 0.00166364
Iteration 7/25 | Loss: 0.00166364
Iteration 8/25 | Loss: 0.00166364
Iteration 9/25 | Loss: 0.00166364
Iteration 10/25 | Loss: 0.00166364
Iteration 11/25 | Loss: 0.00166364
Iteration 12/25 | Loss: 0.00166364
Iteration 13/25 | Loss: 0.00166364
Iteration 14/25 | Loss: 0.00166364
Iteration 15/25 | Loss: 0.00166364
Iteration 16/25 | Loss: 0.00166364
Iteration 17/25 | Loss: 0.00166364
Iteration 18/25 | Loss: 0.00166364
Iteration 19/25 | Loss: 0.00166364
Iteration 20/25 | Loss: 0.00166364
Iteration 21/25 | Loss: 0.00166364
Iteration 22/25 | Loss: 0.00166364
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0016636357177048922, 0.0016636357177048922, 0.0016636357177048922, 0.0016636357177048922, 0.0016636357177048922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016636357177048922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166364
Iteration 2/1000 | Loss: 0.00028115
Iteration 3/1000 | Loss: 0.00013705
Iteration 4/1000 | Loss: 0.00007032
Iteration 5/1000 | Loss: 0.00005110
Iteration 6/1000 | Loss: 0.00004363
Iteration 7/1000 | Loss: 0.00004000
Iteration 8/1000 | Loss: 0.00003730
Iteration 9/1000 | Loss: 0.00003539
Iteration 10/1000 | Loss: 0.00003397
Iteration 11/1000 | Loss: 0.00003291
Iteration 12/1000 | Loss: 0.00003219
Iteration 13/1000 | Loss: 0.00003161
Iteration 14/1000 | Loss: 0.00003126
Iteration 15/1000 | Loss: 0.00003082
Iteration 16/1000 | Loss: 0.00003052
Iteration 17/1000 | Loss: 0.00003023
Iteration 18/1000 | Loss: 0.00003002
Iteration 19/1000 | Loss: 0.00002990
Iteration 20/1000 | Loss: 0.00002988
Iteration 21/1000 | Loss: 0.00002986
Iteration 22/1000 | Loss: 0.00002986
Iteration 23/1000 | Loss: 0.00002985
Iteration 24/1000 | Loss: 0.00002984
Iteration 25/1000 | Loss: 0.00002976
Iteration 26/1000 | Loss: 0.00002972
Iteration 27/1000 | Loss: 0.00002972
Iteration 28/1000 | Loss: 0.00002971
Iteration 29/1000 | Loss: 0.00002970
Iteration 30/1000 | Loss: 0.00002968
Iteration 31/1000 | Loss: 0.00002967
Iteration 32/1000 | Loss: 0.00002966
Iteration 33/1000 | Loss: 0.00002951
Iteration 34/1000 | Loss: 0.00002951
Iteration 35/1000 | Loss: 0.00002951
Iteration 36/1000 | Loss: 0.00002950
Iteration 37/1000 | Loss: 0.00002948
Iteration 38/1000 | Loss: 0.00002948
Iteration 39/1000 | Loss: 0.00002947
Iteration 40/1000 | Loss: 0.00002947
Iteration 41/1000 | Loss: 0.00002946
Iteration 42/1000 | Loss: 0.00002941
Iteration 43/1000 | Loss: 0.00002937
Iteration 44/1000 | Loss: 0.00002934
Iteration 45/1000 | Loss: 0.00002934
Iteration 46/1000 | Loss: 0.00002933
Iteration 47/1000 | Loss: 0.00002931
Iteration 48/1000 | Loss: 0.00002930
Iteration 49/1000 | Loss: 0.00002930
Iteration 50/1000 | Loss: 0.00002929
Iteration 51/1000 | Loss: 0.00002927
Iteration 52/1000 | Loss: 0.00002926
Iteration 53/1000 | Loss: 0.00002925
Iteration 54/1000 | Loss: 0.00002925
Iteration 55/1000 | Loss: 0.00002924
Iteration 56/1000 | Loss: 0.00002921
Iteration 57/1000 | Loss: 0.00002921
Iteration 58/1000 | Loss: 0.00002920
Iteration 59/1000 | Loss: 0.00002920
Iteration 60/1000 | Loss: 0.00002919
Iteration 61/1000 | Loss: 0.00002919
Iteration 62/1000 | Loss: 0.00002918
Iteration 63/1000 | Loss: 0.00002918
Iteration 64/1000 | Loss: 0.00002918
Iteration 65/1000 | Loss: 0.00002918
Iteration 66/1000 | Loss: 0.00002918
Iteration 67/1000 | Loss: 0.00002918
Iteration 68/1000 | Loss: 0.00002917
Iteration 69/1000 | Loss: 0.00002917
Iteration 70/1000 | Loss: 0.00002917
Iteration 71/1000 | Loss: 0.00002916
Iteration 72/1000 | Loss: 0.00002916
Iteration 73/1000 | Loss: 0.00002916
Iteration 74/1000 | Loss: 0.00002915
Iteration 75/1000 | Loss: 0.00002915
Iteration 76/1000 | Loss: 0.00002915
Iteration 77/1000 | Loss: 0.00002914
Iteration 78/1000 | Loss: 0.00002914
Iteration 79/1000 | Loss: 0.00002913
Iteration 80/1000 | Loss: 0.00002913
Iteration 81/1000 | Loss: 0.00002910
Iteration 82/1000 | Loss: 0.00002910
Iteration 83/1000 | Loss: 0.00002909
Iteration 84/1000 | Loss: 0.00002909
Iteration 85/1000 | Loss: 0.00002909
Iteration 86/1000 | Loss: 0.00002908
Iteration 87/1000 | Loss: 0.00002908
Iteration 88/1000 | Loss: 0.00002907
Iteration 89/1000 | Loss: 0.00002907
Iteration 90/1000 | Loss: 0.00002906
Iteration 91/1000 | Loss: 0.00002906
Iteration 92/1000 | Loss: 0.00002905
Iteration 93/1000 | Loss: 0.00002905
Iteration 94/1000 | Loss: 0.00002904
Iteration 95/1000 | Loss: 0.00002904
Iteration 96/1000 | Loss: 0.00002904
Iteration 97/1000 | Loss: 0.00002903
Iteration 98/1000 | Loss: 0.00002903
Iteration 99/1000 | Loss: 0.00002903
Iteration 100/1000 | Loss: 0.00002902
Iteration 101/1000 | Loss: 0.00002902
Iteration 102/1000 | Loss: 0.00002902
Iteration 103/1000 | Loss: 0.00002901
Iteration 104/1000 | Loss: 0.00002901
Iteration 105/1000 | Loss: 0.00002901
Iteration 106/1000 | Loss: 0.00002901
Iteration 107/1000 | Loss: 0.00002901
Iteration 108/1000 | Loss: 0.00002901
Iteration 109/1000 | Loss: 0.00002901
Iteration 110/1000 | Loss: 0.00002901
Iteration 111/1000 | Loss: 0.00002901
Iteration 112/1000 | Loss: 0.00002901
Iteration 113/1000 | Loss: 0.00002900
Iteration 114/1000 | Loss: 0.00002900
Iteration 115/1000 | Loss: 0.00002900
Iteration 116/1000 | Loss: 0.00002900
Iteration 117/1000 | Loss: 0.00002900
Iteration 118/1000 | Loss: 0.00002900
Iteration 119/1000 | Loss: 0.00002900
Iteration 120/1000 | Loss: 0.00002900
Iteration 121/1000 | Loss: 0.00002900
Iteration 122/1000 | Loss: 0.00002900
Iteration 123/1000 | Loss: 0.00002900
Iteration 124/1000 | Loss: 0.00002900
Iteration 125/1000 | Loss: 0.00002899
Iteration 126/1000 | Loss: 0.00002899
Iteration 127/1000 | Loss: 0.00002899
Iteration 128/1000 | Loss: 0.00002899
Iteration 129/1000 | Loss: 0.00002899
Iteration 130/1000 | Loss: 0.00002899
Iteration 131/1000 | Loss: 0.00002899
Iteration 132/1000 | Loss: 0.00002899
Iteration 133/1000 | Loss: 0.00002899
Iteration 134/1000 | Loss: 0.00002898
Iteration 135/1000 | Loss: 0.00002898
Iteration 136/1000 | Loss: 0.00002898
Iteration 137/1000 | Loss: 0.00002898
Iteration 138/1000 | Loss: 0.00002898
Iteration 139/1000 | Loss: 0.00002898
Iteration 140/1000 | Loss: 0.00002898
Iteration 141/1000 | Loss: 0.00002898
Iteration 142/1000 | Loss: 0.00002898
Iteration 143/1000 | Loss: 0.00002898
Iteration 144/1000 | Loss: 0.00002898
Iteration 145/1000 | Loss: 0.00002898
Iteration 146/1000 | Loss: 0.00002898
Iteration 147/1000 | Loss: 0.00002898
Iteration 148/1000 | Loss: 0.00002898
Iteration 149/1000 | Loss: 0.00002898
Iteration 150/1000 | Loss: 0.00002898
Iteration 151/1000 | Loss: 0.00002898
Iteration 152/1000 | Loss: 0.00002898
Iteration 153/1000 | Loss: 0.00002898
Iteration 154/1000 | Loss: 0.00002897
Iteration 155/1000 | Loss: 0.00002897
Iteration 156/1000 | Loss: 0.00002897
Iteration 157/1000 | Loss: 0.00002897
Iteration 158/1000 | Loss: 0.00002897
Iteration 159/1000 | Loss: 0.00002897
Iteration 160/1000 | Loss: 0.00002897
Iteration 161/1000 | Loss: 0.00002897
Iteration 162/1000 | Loss: 0.00002897
Iteration 163/1000 | Loss: 0.00002897
Iteration 164/1000 | Loss: 0.00002897
Iteration 165/1000 | Loss: 0.00002897
Iteration 166/1000 | Loss: 0.00002897
Iteration 167/1000 | Loss: 0.00002897
Iteration 168/1000 | Loss: 0.00002897
Iteration 169/1000 | Loss: 0.00002897
Iteration 170/1000 | Loss: 0.00002897
Iteration 171/1000 | Loss: 0.00002897
Iteration 172/1000 | Loss: 0.00002897
Iteration 173/1000 | Loss: 0.00002896
Iteration 174/1000 | Loss: 0.00002896
Iteration 175/1000 | Loss: 0.00002896
Iteration 176/1000 | Loss: 0.00002896
Iteration 177/1000 | Loss: 0.00002896
Iteration 178/1000 | Loss: 0.00002896
Iteration 179/1000 | Loss: 0.00002896
Iteration 180/1000 | Loss: 0.00002896
Iteration 181/1000 | Loss: 0.00002896
Iteration 182/1000 | Loss: 0.00002896
Iteration 183/1000 | Loss: 0.00002896
Iteration 184/1000 | Loss: 0.00002896
Iteration 185/1000 | Loss: 0.00002896
Iteration 186/1000 | Loss: 0.00002896
Iteration 187/1000 | Loss: 0.00002896
Iteration 188/1000 | Loss: 0.00002896
Iteration 189/1000 | Loss: 0.00002896
Iteration 190/1000 | Loss: 0.00002896
Iteration 191/1000 | Loss: 0.00002896
Iteration 192/1000 | Loss: 0.00002896
Iteration 193/1000 | Loss: 0.00002896
Iteration 194/1000 | Loss: 0.00002896
Iteration 195/1000 | Loss: 0.00002896
Iteration 196/1000 | Loss: 0.00002896
Iteration 197/1000 | Loss: 0.00002896
Iteration 198/1000 | Loss: 0.00002896
Iteration 199/1000 | Loss: 0.00002896
Iteration 200/1000 | Loss: 0.00002896
Iteration 201/1000 | Loss: 0.00002896
Iteration 202/1000 | Loss: 0.00002896
Iteration 203/1000 | Loss: 0.00002896
Iteration 204/1000 | Loss: 0.00002896
Iteration 205/1000 | Loss: 0.00002896
Iteration 206/1000 | Loss: 0.00002896
Iteration 207/1000 | Loss: 0.00002896
Iteration 208/1000 | Loss: 0.00002896
Iteration 209/1000 | Loss: 0.00002896
Iteration 210/1000 | Loss: 0.00002896
Iteration 211/1000 | Loss: 0.00002896
Iteration 212/1000 | Loss: 0.00002896
Iteration 213/1000 | Loss: 0.00002896
Iteration 214/1000 | Loss: 0.00002896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.8955129891983233e-05, 2.8955129891983233e-05, 2.8955129891983233e-05, 2.8955129891983233e-05, 2.8955129891983233e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8955129891983233e-05

Optimization complete. Final v2v error: 4.017513751983643 mm

Highest mean error: 11.217225074768066 mm for frame 137

Lowest mean error: 3.1724841594696045 mm for frame 76

Saving results

Total time: 72.98640704154968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00583696
Iteration 2/25 | Loss: 0.00154370
Iteration 3/25 | Loss: 0.00130191
Iteration 4/25 | Loss: 0.00127222
Iteration 5/25 | Loss: 0.00126859
Iteration 6/25 | Loss: 0.00126767
Iteration 7/25 | Loss: 0.00126766
Iteration 8/25 | Loss: 0.00126766
Iteration 9/25 | Loss: 0.00126766
Iteration 10/25 | Loss: 0.00126766
Iteration 11/25 | Loss: 0.00126766
Iteration 12/25 | Loss: 0.00126766
Iteration 13/25 | Loss: 0.00126766
Iteration 14/25 | Loss: 0.00126766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012676554033532739, 0.0012676554033532739, 0.0012676554033532739, 0.0012676554033532739, 0.0012676554033532739]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012676554033532739

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40161180
Iteration 2/25 | Loss: 0.00061769
Iteration 3/25 | Loss: 0.00061765
Iteration 4/25 | Loss: 0.00061765
Iteration 5/25 | Loss: 0.00061765
Iteration 6/25 | Loss: 0.00061765
Iteration 7/25 | Loss: 0.00061765
Iteration 8/25 | Loss: 0.00061765
Iteration 9/25 | Loss: 0.00061765
Iteration 10/25 | Loss: 0.00061765
Iteration 11/25 | Loss: 0.00061765
Iteration 12/25 | Loss: 0.00061765
Iteration 13/25 | Loss: 0.00061765
Iteration 14/25 | Loss: 0.00061765
Iteration 15/25 | Loss: 0.00061765
Iteration 16/25 | Loss: 0.00061765
Iteration 17/25 | Loss: 0.00061765
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006176457391120493, 0.0006176457391120493, 0.0006176457391120493, 0.0006176457391120493, 0.0006176457391120493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006176457391120493

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061765
Iteration 2/1000 | Loss: 0.00004612
Iteration 3/1000 | Loss: 0.00003356
Iteration 4/1000 | Loss: 0.00002569
Iteration 5/1000 | Loss: 0.00002362
Iteration 6/1000 | Loss: 0.00002235
Iteration 7/1000 | Loss: 0.00002148
Iteration 8/1000 | Loss: 0.00002065
Iteration 9/1000 | Loss: 0.00002017
Iteration 10/1000 | Loss: 0.00001979
Iteration 11/1000 | Loss: 0.00001962
Iteration 12/1000 | Loss: 0.00001937
Iteration 13/1000 | Loss: 0.00001915
Iteration 14/1000 | Loss: 0.00001895
Iteration 15/1000 | Loss: 0.00001873
Iteration 16/1000 | Loss: 0.00001863
Iteration 17/1000 | Loss: 0.00001860
Iteration 18/1000 | Loss: 0.00001858
Iteration 19/1000 | Loss: 0.00001858
Iteration 20/1000 | Loss: 0.00001856
Iteration 21/1000 | Loss: 0.00001854
Iteration 22/1000 | Loss: 0.00001853
Iteration 23/1000 | Loss: 0.00001852
Iteration 24/1000 | Loss: 0.00001851
Iteration 25/1000 | Loss: 0.00001850
Iteration 26/1000 | Loss: 0.00001849
Iteration 27/1000 | Loss: 0.00001845
Iteration 28/1000 | Loss: 0.00001844
Iteration 29/1000 | Loss: 0.00001841
Iteration 30/1000 | Loss: 0.00001840
Iteration 31/1000 | Loss: 0.00001838
Iteration 32/1000 | Loss: 0.00001837
Iteration 33/1000 | Loss: 0.00001834
Iteration 34/1000 | Loss: 0.00001833
Iteration 35/1000 | Loss: 0.00001833
Iteration 36/1000 | Loss: 0.00001830
Iteration 37/1000 | Loss: 0.00001829
Iteration 38/1000 | Loss: 0.00001828
Iteration 39/1000 | Loss: 0.00001828
Iteration 40/1000 | Loss: 0.00001828
Iteration 41/1000 | Loss: 0.00001827
Iteration 42/1000 | Loss: 0.00001827
Iteration 43/1000 | Loss: 0.00001827
Iteration 44/1000 | Loss: 0.00001827
Iteration 45/1000 | Loss: 0.00001826
Iteration 46/1000 | Loss: 0.00001826
Iteration 47/1000 | Loss: 0.00001826
Iteration 48/1000 | Loss: 0.00001825
Iteration 49/1000 | Loss: 0.00001825
Iteration 50/1000 | Loss: 0.00001825
Iteration 51/1000 | Loss: 0.00001824
Iteration 52/1000 | Loss: 0.00001824
Iteration 53/1000 | Loss: 0.00001824
Iteration 54/1000 | Loss: 0.00001823
Iteration 55/1000 | Loss: 0.00001823
Iteration 56/1000 | Loss: 0.00001823
Iteration 57/1000 | Loss: 0.00001823
Iteration 58/1000 | Loss: 0.00001822
Iteration 59/1000 | Loss: 0.00001822
Iteration 60/1000 | Loss: 0.00001822
Iteration 61/1000 | Loss: 0.00001822
Iteration 62/1000 | Loss: 0.00001822
Iteration 63/1000 | Loss: 0.00001822
Iteration 64/1000 | Loss: 0.00001822
Iteration 65/1000 | Loss: 0.00001822
Iteration 66/1000 | Loss: 0.00001822
Iteration 67/1000 | Loss: 0.00001821
Iteration 68/1000 | Loss: 0.00001821
Iteration 69/1000 | Loss: 0.00001821
Iteration 70/1000 | Loss: 0.00001821
Iteration 71/1000 | Loss: 0.00001820
Iteration 72/1000 | Loss: 0.00001819
Iteration 73/1000 | Loss: 0.00001819
Iteration 74/1000 | Loss: 0.00001819
Iteration 75/1000 | Loss: 0.00001819
Iteration 76/1000 | Loss: 0.00001819
Iteration 77/1000 | Loss: 0.00001818
Iteration 78/1000 | Loss: 0.00001818
Iteration 79/1000 | Loss: 0.00001818
Iteration 80/1000 | Loss: 0.00001817
Iteration 81/1000 | Loss: 0.00001817
Iteration 82/1000 | Loss: 0.00001817
Iteration 83/1000 | Loss: 0.00001817
Iteration 84/1000 | Loss: 0.00001817
Iteration 85/1000 | Loss: 0.00001816
Iteration 86/1000 | Loss: 0.00001816
Iteration 87/1000 | Loss: 0.00001816
Iteration 88/1000 | Loss: 0.00001816
Iteration 89/1000 | Loss: 0.00001816
Iteration 90/1000 | Loss: 0.00001816
Iteration 91/1000 | Loss: 0.00001815
Iteration 92/1000 | Loss: 0.00001815
Iteration 93/1000 | Loss: 0.00001815
Iteration 94/1000 | Loss: 0.00001815
Iteration 95/1000 | Loss: 0.00001815
Iteration 96/1000 | Loss: 0.00001815
Iteration 97/1000 | Loss: 0.00001815
Iteration 98/1000 | Loss: 0.00001814
Iteration 99/1000 | Loss: 0.00001814
Iteration 100/1000 | Loss: 0.00001814
Iteration 101/1000 | Loss: 0.00001814
Iteration 102/1000 | Loss: 0.00001814
Iteration 103/1000 | Loss: 0.00001814
Iteration 104/1000 | Loss: 0.00001814
Iteration 105/1000 | Loss: 0.00001814
Iteration 106/1000 | Loss: 0.00001814
Iteration 107/1000 | Loss: 0.00001814
Iteration 108/1000 | Loss: 0.00001814
Iteration 109/1000 | Loss: 0.00001813
Iteration 110/1000 | Loss: 0.00001813
Iteration 111/1000 | Loss: 0.00001813
Iteration 112/1000 | Loss: 0.00001813
Iteration 113/1000 | Loss: 0.00001813
Iteration 114/1000 | Loss: 0.00001813
Iteration 115/1000 | Loss: 0.00001813
Iteration 116/1000 | Loss: 0.00001813
Iteration 117/1000 | Loss: 0.00001813
Iteration 118/1000 | Loss: 0.00001813
Iteration 119/1000 | Loss: 0.00001813
Iteration 120/1000 | Loss: 0.00001813
Iteration 121/1000 | Loss: 0.00001813
Iteration 122/1000 | Loss: 0.00001813
Iteration 123/1000 | Loss: 0.00001813
Iteration 124/1000 | Loss: 0.00001813
Iteration 125/1000 | Loss: 0.00001813
Iteration 126/1000 | Loss: 0.00001813
Iteration 127/1000 | Loss: 0.00001813
Iteration 128/1000 | Loss: 0.00001813
Iteration 129/1000 | Loss: 0.00001813
Iteration 130/1000 | Loss: 0.00001813
Iteration 131/1000 | Loss: 0.00001813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.812808113754727e-05, 1.812808113754727e-05, 1.812808113754727e-05, 1.812808113754727e-05, 1.812808113754727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.812808113754727e-05

Optimization complete. Final v2v error: 3.585906505584717 mm

Highest mean error: 3.866117000579834 mm for frame 67

Lowest mean error: 3.261758804321289 mm for frame 16

Saving results

Total time: 40.619667291641235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00538221
Iteration 2/25 | Loss: 0.00155221
Iteration 3/25 | Loss: 0.00134798
Iteration 4/25 | Loss: 0.00132716
Iteration 5/25 | Loss: 0.00131967
Iteration 6/25 | Loss: 0.00131744
Iteration 7/25 | Loss: 0.00131714
Iteration 8/25 | Loss: 0.00131714
Iteration 9/25 | Loss: 0.00131714
Iteration 10/25 | Loss: 0.00131714
Iteration 11/25 | Loss: 0.00131714
Iteration 12/25 | Loss: 0.00131714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001317142858169973, 0.001317142858169973, 0.001317142858169973, 0.001317142858169973, 0.001317142858169973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001317142858169973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97387934
Iteration 2/25 | Loss: 0.00092592
Iteration 3/25 | Loss: 0.00092592
Iteration 4/25 | Loss: 0.00092592
Iteration 5/25 | Loss: 0.00092592
Iteration 6/25 | Loss: 0.00092592
Iteration 7/25 | Loss: 0.00092592
Iteration 8/25 | Loss: 0.00092592
Iteration 9/25 | Loss: 0.00092592
Iteration 10/25 | Loss: 0.00092592
Iteration 11/25 | Loss: 0.00092591
Iteration 12/25 | Loss: 0.00092591
Iteration 13/25 | Loss: 0.00092591
Iteration 14/25 | Loss: 0.00092591
Iteration 15/25 | Loss: 0.00092591
Iteration 16/25 | Loss: 0.00092591
Iteration 17/25 | Loss: 0.00092591
Iteration 18/25 | Loss: 0.00092591
Iteration 19/25 | Loss: 0.00092591
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009259147918783128, 0.0009259147918783128, 0.0009259147918783128, 0.0009259147918783128, 0.0009259147918783128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009259147918783128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092591
Iteration 2/1000 | Loss: 0.00008735
Iteration 3/1000 | Loss: 0.00005060
Iteration 4/1000 | Loss: 0.00004177
Iteration 5/1000 | Loss: 0.00003719
Iteration 6/1000 | Loss: 0.00003512
Iteration 7/1000 | Loss: 0.00003393
Iteration 8/1000 | Loss: 0.00003283
Iteration 9/1000 | Loss: 0.00003187
Iteration 10/1000 | Loss: 0.00003133
Iteration 11/1000 | Loss: 0.00003091
Iteration 12/1000 | Loss: 0.00003056
Iteration 13/1000 | Loss: 0.00003022
Iteration 14/1000 | Loss: 0.00002995
Iteration 15/1000 | Loss: 0.00002977
Iteration 16/1000 | Loss: 0.00002955
Iteration 17/1000 | Loss: 0.00002939
Iteration 18/1000 | Loss: 0.00002935
Iteration 19/1000 | Loss: 0.00002931
Iteration 20/1000 | Loss: 0.00002926
Iteration 21/1000 | Loss: 0.00002922
Iteration 22/1000 | Loss: 0.00002914
Iteration 23/1000 | Loss: 0.00002906
Iteration 24/1000 | Loss: 0.00002900
Iteration 25/1000 | Loss: 0.00002900
Iteration 26/1000 | Loss: 0.00002900
Iteration 27/1000 | Loss: 0.00002898
Iteration 28/1000 | Loss: 0.00002898
Iteration 29/1000 | Loss: 0.00002898
Iteration 30/1000 | Loss: 0.00002898
Iteration 31/1000 | Loss: 0.00002898
Iteration 32/1000 | Loss: 0.00002898
Iteration 33/1000 | Loss: 0.00002898
Iteration 34/1000 | Loss: 0.00002897
Iteration 35/1000 | Loss: 0.00002897
Iteration 36/1000 | Loss: 0.00002897
Iteration 37/1000 | Loss: 0.00002897
Iteration 38/1000 | Loss: 0.00002897
Iteration 39/1000 | Loss: 0.00002897
Iteration 40/1000 | Loss: 0.00002896
Iteration 41/1000 | Loss: 0.00002896
Iteration 42/1000 | Loss: 0.00002896
Iteration 43/1000 | Loss: 0.00002895
Iteration 44/1000 | Loss: 0.00002895
Iteration 45/1000 | Loss: 0.00002895
Iteration 46/1000 | Loss: 0.00002894
Iteration 47/1000 | Loss: 0.00002894
Iteration 48/1000 | Loss: 0.00002893
Iteration 49/1000 | Loss: 0.00002893
Iteration 50/1000 | Loss: 0.00002893
Iteration 51/1000 | Loss: 0.00002892
Iteration 52/1000 | Loss: 0.00002892
Iteration 53/1000 | Loss: 0.00002892
Iteration 54/1000 | Loss: 0.00002891
Iteration 55/1000 | Loss: 0.00002891
Iteration 56/1000 | Loss: 0.00002890
Iteration 57/1000 | Loss: 0.00002890
Iteration 58/1000 | Loss: 0.00002889
Iteration 59/1000 | Loss: 0.00002888
Iteration 60/1000 | Loss: 0.00002887
Iteration 61/1000 | Loss: 0.00002887
Iteration 62/1000 | Loss: 0.00002886
Iteration 63/1000 | Loss: 0.00002884
Iteration 64/1000 | Loss: 0.00002884
Iteration 65/1000 | Loss: 0.00002884
Iteration 66/1000 | Loss: 0.00002883
Iteration 67/1000 | Loss: 0.00002883
Iteration 68/1000 | Loss: 0.00002883
Iteration 69/1000 | Loss: 0.00002882
Iteration 70/1000 | Loss: 0.00002882
Iteration 71/1000 | Loss: 0.00002881
Iteration 72/1000 | Loss: 0.00002881
Iteration 73/1000 | Loss: 0.00002881
Iteration 74/1000 | Loss: 0.00002881
Iteration 75/1000 | Loss: 0.00002881
Iteration 76/1000 | Loss: 0.00002881
Iteration 77/1000 | Loss: 0.00002881
Iteration 78/1000 | Loss: 0.00002880
Iteration 79/1000 | Loss: 0.00002880
Iteration 80/1000 | Loss: 0.00002880
Iteration 81/1000 | Loss: 0.00002880
Iteration 82/1000 | Loss: 0.00002880
Iteration 83/1000 | Loss: 0.00002880
Iteration 84/1000 | Loss: 0.00002880
Iteration 85/1000 | Loss: 0.00002879
Iteration 86/1000 | Loss: 0.00002879
Iteration 87/1000 | Loss: 0.00002879
Iteration 88/1000 | Loss: 0.00002879
Iteration 89/1000 | Loss: 0.00002879
Iteration 90/1000 | Loss: 0.00002878
Iteration 91/1000 | Loss: 0.00002878
Iteration 92/1000 | Loss: 0.00002878
Iteration 93/1000 | Loss: 0.00002878
Iteration 94/1000 | Loss: 0.00002878
Iteration 95/1000 | Loss: 0.00002877
Iteration 96/1000 | Loss: 0.00002877
Iteration 97/1000 | Loss: 0.00002877
Iteration 98/1000 | Loss: 0.00002877
Iteration 99/1000 | Loss: 0.00002876
Iteration 100/1000 | Loss: 0.00002876
Iteration 101/1000 | Loss: 0.00002876
Iteration 102/1000 | Loss: 0.00002876
Iteration 103/1000 | Loss: 0.00002876
Iteration 104/1000 | Loss: 0.00002876
Iteration 105/1000 | Loss: 0.00002876
Iteration 106/1000 | Loss: 0.00002876
Iteration 107/1000 | Loss: 0.00002876
Iteration 108/1000 | Loss: 0.00002875
Iteration 109/1000 | Loss: 0.00002875
Iteration 110/1000 | Loss: 0.00002875
Iteration 111/1000 | Loss: 0.00002875
Iteration 112/1000 | Loss: 0.00002875
Iteration 113/1000 | Loss: 0.00002875
Iteration 114/1000 | Loss: 0.00002874
Iteration 115/1000 | Loss: 0.00002874
Iteration 116/1000 | Loss: 0.00002874
Iteration 117/1000 | Loss: 0.00002874
Iteration 118/1000 | Loss: 0.00002874
Iteration 119/1000 | Loss: 0.00002874
Iteration 120/1000 | Loss: 0.00002874
Iteration 121/1000 | Loss: 0.00002874
Iteration 122/1000 | Loss: 0.00002874
Iteration 123/1000 | Loss: 0.00002874
Iteration 124/1000 | Loss: 0.00002874
Iteration 125/1000 | Loss: 0.00002873
Iteration 126/1000 | Loss: 0.00002873
Iteration 127/1000 | Loss: 0.00002873
Iteration 128/1000 | Loss: 0.00002873
Iteration 129/1000 | Loss: 0.00002873
Iteration 130/1000 | Loss: 0.00002873
Iteration 131/1000 | Loss: 0.00002873
Iteration 132/1000 | Loss: 0.00002873
Iteration 133/1000 | Loss: 0.00002872
Iteration 134/1000 | Loss: 0.00002872
Iteration 135/1000 | Loss: 0.00002872
Iteration 136/1000 | Loss: 0.00002872
Iteration 137/1000 | Loss: 0.00002872
Iteration 138/1000 | Loss: 0.00002872
Iteration 139/1000 | Loss: 0.00002872
Iteration 140/1000 | Loss: 0.00002872
Iteration 141/1000 | Loss: 0.00002871
Iteration 142/1000 | Loss: 0.00002871
Iteration 143/1000 | Loss: 0.00002871
Iteration 144/1000 | Loss: 0.00002871
Iteration 145/1000 | Loss: 0.00002871
Iteration 146/1000 | Loss: 0.00002871
Iteration 147/1000 | Loss: 0.00002871
Iteration 148/1000 | Loss: 0.00002871
Iteration 149/1000 | Loss: 0.00002871
Iteration 150/1000 | Loss: 0.00002871
Iteration 151/1000 | Loss: 0.00002871
Iteration 152/1000 | Loss: 0.00002871
Iteration 153/1000 | Loss: 0.00002871
Iteration 154/1000 | Loss: 0.00002871
Iteration 155/1000 | Loss: 0.00002871
Iteration 156/1000 | Loss: 0.00002871
Iteration 157/1000 | Loss: 0.00002871
Iteration 158/1000 | Loss: 0.00002871
Iteration 159/1000 | Loss: 0.00002871
Iteration 160/1000 | Loss: 0.00002871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.8707439923891798e-05, 2.8707439923891798e-05, 2.8707439923891798e-05, 2.8707439923891798e-05, 2.8707439923891798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8707439923891798e-05

Optimization complete. Final v2v error: 4.437452793121338 mm

Highest mean error: 5.238436698913574 mm for frame 63

Lowest mean error: 3.796915292739868 mm for frame 173

Saving results

Total time: 48.123313426971436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016018
Iteration 2/25 | Loss: 0.00294251
Iteration 3/25 | Loss: 0.00180841
Iteration 4/25 | Loss: 0.00164335
Iteration 5/25 | Loss: 0.00152239
Iteration 6/25 | Loss: 0.00151256
Iteration 7/25 | Loss: 0.00147283
Iteration 8/25 | Loss: 0.00144269
Iteration 9/25 | Loss: 0.00144100
Iteration 10/25 | Loss: 0.00142439
Iteration 11/25 | Loss: 0.00139972
Iteration 12/25 | Loss: 0.00140085
Iteration 13/25 | Loss: 0.00139451
Iteration 14/25 | Loss: 0.00138012
Iteration 15/25 | Loss: 0.00138684
Iteration 16/25 | Loss: 0.00137610
Iteration 17/25 | Loss: 0.00137715
Iteration 18/25 | Loss: 0.00137528
Iteration 19/25 | Loss: 0.00137195
Iteration 20/25 | Loss: 0.00137001
Iteration 21/25 | Loss: 0.00136937
Iteration 22/25 | Loss: 0.00136970
Iteration 23/25 | Loss: 0.00136891
Iteration 24/25 | Loss: 0.00136934
Iteration 25/25 | Loss: 0.00137559

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39106941
Iteration 2/25 | Loss: 0.00267982
Iteration 3/25 | Loss: 0.00130346
Iteration 4/25 | Loss: 0.00130346
Iteration 5/25 | Loss: 0.00130346
Iteration 6/25 | Loss: 0.00130346
Iteration 7/25 | Loss: 0.00130346
Iteration 8/25 | Loss: 0.00130346
Iteration 9/25 | Loss: 0.00130346
Iteration 10/25 | Loss: 0.00130346
Iteration 11/25 | Loss: 0.00130346
Iteration 12/25 | Loss: 0.00130346
Iteration 13/25 | Loss: 0.00130346
Iteration 14/25 | Loss: 0.00130346
Iteration 15/25 | Loss: 0.00130346
Iteration 16/25 | Loss: 0.00130346
Iteration 17/25 | Loss: 0.00130346
Iteration 18/25 | Loss: 0.00130346
Iteration 19/25 | Loss: 0.00130346
Iteration 20/25 | Loss: 0.00130346
Iteration 21/25 | Loss: 0.00130346
Iteration 22/25 | Loss: 0.00130346
Iteration 23/25 | Loss: 0.00130346
Iteration 24/25 | Loss: 0.00130346
Iteration 25/25 | Loss: 0.00130346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130346
Iteration 2/1000 | Loss: 0.00033141
Iteration 3/1000 | Loss: 0.00116814
Iteration 4/1000 | Loss: 0.00028640
Iteration 5/1000 | Loss: 0.00147435
Iteration 6/1000 | Loss: 0.00080622
Iteration 7/1000 | Loss: 0.00418782
Iteration 8/1000 | Loss: 0.00184003
Iteration 9/1000 | Loss: 0.00187955
Iteration 10/1000 | Loss: 0.00375852
Iteration 11/1000 | Loss: 0.00137555
Iteration 12/1000 | Loss: 0.00022149
Iteration 13/1000 | Loss: 0.00015898
Iteration 14/1000 | Loss: 0.00051141
Iteration 15/1000 | Loss: 0.00170321
Iteration 16/1000 | Loss: 0.00030116
Iteration 17/1000 | Loss: 0.00033460
Iteration 18/1000 | Loss: 0.00032074
Iteration 19/1000 | Loss: 0.00027648
Iteration 20/1000 | Loss: 0.00015911
Iteration 21/1000 | Loss: 0.00022212
Iteration 22/1000 | Loss: 0.00157637
Iteration 23/1000 | Loss: 0.00032821
Iteration 24/1000 | Loss: 0.00175331
Iteration 25/1000 | Loss: 0.00006932
Iteration 26/1000 | Loss: 0.00040057
Iteration 27/1000 | Loss: 0.00038533
Iteration 28/1000 | Loss: 0.00026353
Iteration 29/1000 | Loss: 0.00060051
Iteration 30/1000 | Loss: 0.00024305
Iteration 31/1000 | Loss: 0.00034546
Iteration 32/1000 | Loss: 0.00018980
Iteration 33/1000 | Loss: 0.00005780
Iteration 34/1000 | Loss: 0.00030249
Iteration 35/1000 | Loss: 0.00028499
Iteration 36/1000 | Loss: 0.00006020
Iteration 37/1000 | Loss: 0.00005370
Iteration 38/1000 | Loss: 0.00062165
Iteration 39/1000 | Loss: 0.00119518
Iteration 40/1000 | Loss: 0.00396491
Iteration 41/1000 | Loss: 0.00363739
Iteration 42/1000 | Loss: 0.00132580
Iteration 43/1000 | Loss: 0.00025003
Iteration 44/1000 | Loss: 0.00022650
Iteration 45/1000 | Loss: 0.00015684
Iteration 46/1000 | Loss: 0.00004531
Iteration 47/1000 | Loss: 0.00112819
Iteration 48/1000 | Loss: 0.00005305
Iteration 49/1000 | Loss: 0.00004908
Iteration 50/1000 | Loss: 0.00003864
Iteration 51/1000 | Loss: 0.00003458
Iteration 52/1000 | Loss: 0.00050798
Iteration 53/1000 | Loss: 0.00052688
Iteration 54/1000 | Loss: 0.00002814
Iteration 55/1000 | Loss: 0.00002550
Iteration 56/1000 | Loss: 0.00002659
Iteration 57/1000 | Loss: 0.00002321
Iteration 58/1000 | Loss: 0.00002227
Iteration 59/1000 | Loss: 0.00002984
Iteration 60/1000 | Loss: 0.00011713
Iteration 61/1000 | Loss: 0.00006822
Iteration 62/1000 | Loss: 0.00008178
Iteration 63/1000 | Loss: 0.00020193
Iteration 64/1000 | Loss: 0.00004089
Iteration 65/1000 | Loss: 0.00045011
Iteration 66/1000 | Loss: 0.00023882
Iteration 67/1000 | Loss: 0.00002889
Iteration 68/1000 | Loss: 0.00002555
Iteration 69/1000 | Loss: 0.00026875
Iteration 70/1000 | Loss: 0.00023017
Iteration 71/1000 | Loss: 0.00003859
Iteration 72/1000 | Loss: 0.00002533
Iteration 73/1000 | Loss: 0.00002441
Iteration 74/1000 | Loss: 0.00002147
Iteration 75/1000 | Loss: 0.00018046
Iteration 76/1000 | Loss: 0.00002294
Iteration 77/1000 | Loss: 0.00002068
Iteration 78/1000 | Loss: 0.00002399
Iteration 79/1000 | Loss: 0.00028950
Iteration 80/1000 | Loss: 0.00002380
Iteration 81/1000 | Loss: 0.00007866
Iteration 82/1000 | Loss: 0.00003162
Iteration 83/1000 | Loss: 0.00001969
Iteration 84/1000 | Loss: 0.00009109
Iteration 85/1000 | Loss: 0.00003741
Iteration 86/1000 | Loss: 0.00003398
Iteration 87/1000 | Loss: 0.00001913
Iteration 88/1000 | Loss: 0.00001882
Iteration 89/1000 | Loss: 0.00001864
Iteration 90/1000 | Loss: 0.00001861
Iteration 91/1000 | Loss: 0.00002840
Iteration 92/1000 | Loss: 0.00001878
Iteration 93/1000 | Loss: 0.00001843
Iteration 94/1000 | Loss: 0.00001840
Iteration 95/1000 | Loss: 0.00001840
Iteration 96/1000 | Loss: 0.00001840
Iteration 97/1000 | Loss: 0.00001840
Iteration 98/1000 | Loss: 0.00001840
Iteration 99/1000 | Loss: 0.00001840
Iteration 100/1000 | Loss: 0.00001840
Iteration 101/1000 | Loss: 0.00001840
Iteration 102/1000 | Loss: 0.00001839
Iteration 103/1000 | Loss: 0.00001839
Iteration 104/1000 | Loss: 0.00001838
Iteration 105/1000 | Loss: 0.00001838
Iteration 106/1000 | Loss: 0.00001837
Iteration 107/1000 | Loss: 0.00001837
Iteration 108/1000 | Loss: 0.00001837
Iteration 109/1000 | Loss: 0.00001837
Iteration 110/1000 | Loss: 0.00001836
Iteration 111/1000 | Loss: 0.00001836
Iteration 112/1000 | Loss: 0.00001833
Iteration 113/1000 | Loss: 0.00001831
Iteration 114/1000 | Loss: 0.00001831
Iteration 115/1000 | Loss: 0.00001830
Iteration 116/1000 | Loss: 0.00001930
Iteration 117/1000 | Loss: 0.00001930
Iteration 118/1000 | Loss: 0.00001827
Iteration 119/1000 | Loss: 0.00001820
Iteration 120/1000 | Loss: 0.00001820
Iteration 121/1000 | Loss: 0.00001820
Iteration 122/1000 | Loss: 0.00001820
Iteration 123/1000 | Loss: 0.00001820
Iteration 124/1000 | Loss: 0.00001820
Iteration 125/1000 | Loss: 0.00001820
Iteration 126/1000 | Loss: 0.00001819
Iteration 127/1000 | Loss: 0.00001819
Iteration 128/1000 | Loss: 0.00001819
Iteration 129/1000 | Loss: 0.00001819
Iteration 130/1000 | Loss: 0.00001819
Iteration 131/1000 | Loss: 0.00001819
Iteration 132/1000 | Loss: 0.00001819
Iteration 133/1000 | Loss: 0.00001819
Iteration 134/1000 | Loss: 0.00001819
Iteration 135/1000 | Loss: 0.00001819
Iteration 136/1000 | Loss: 0.00001819
Iteration 137/1000 | Loss: 0.00001819
Iteration 138/1000 | Loss: 0.00001819
Iteration 139/1000 | Loss: 0.00001819
Iteration 140/1000 | Loss: 0.00001819
Iteration 141/1000 | Loss: 0.00001819
Iteration 142/1000 | Loss: 0.00001819
Iteration 143/1000 | Loss: 0.00001819
Iteration 144/1000 | Loss: 0.00001819
Iteration 145/1000 | Loss: 0.00001819
Iteration 146/1000 | Loss: 0.00001819
Iteration 147/1000 | Loss: 0.00001819
Iteration 148/1000 | Loss: 0.00001819
Iteration 149/1000 | Loss: 0.00001819
Iteration 150/1000 | Loss: 0.00001819
Iteration 151/1000 | Loss: 0.00001819
Iteration 152/1000 | Loss: 0.00001819
Iteration 153/1000 | Loss: 0.00001819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.8188751710113138e-05, 1.8188751710113138e-05, 1.8188751710113138e-05, 1.8188751710113138e-05, 1.8188751710113138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8188751710113138e-05

Optimization complete. Final v2v error: 3.567110538482666 mm

Highest mean error: 4.999894142150879 mm for frame 43

Lowest mean error: 2.953416585922241 mm for frame 24

Saving results

Total time: 205.7055094242096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784026
Iteration 2/25 | Loss: 0.00154640
Iteration 3/25 | Loss: 0.00127836
Iteration 4/25 | Loss: 0.00124470
Iteration 5/25 | Loss: 0.00123661
Iteration 6/25 | Loss: 0.00123529
Iteration 7/25 | Loss: 0.00123529
Iteration 8/25 | Loss: 0.00123529
Iteration 9/25 | Loss: 0.00123529
Iteration 10/25 | Loss: 0.00123529
Iteration 11/25 | Loss: 0.00123529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012352861231192946, 0.0012352861231192946, 0.0012352861231192946, 0.0012352861231192946, 0.0012352861231192946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012352861231192946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40230918
Iteration 2/25 | Loss: 0.00061674
Iteration 3/25 | Loss: 0.00061673
Iteration 4/25 | Loss: 0.00061673
Iteration 5/25 | Loss: 0.00061673
Iteration 6/25 | Loss: 0.00061673
Iteration 7/25 | Loss: 0.00061673
Iteration 8/25 | Loss: 0.00061673
Iteration 9/25 | Loss: 0.00061673
Iteration 10/25 | Loss: 0.00061673
Iteration 11/25 | Loss: 0.00061673
Iteration 12/25 | Loss: 0.00061673
Iteration 13/25 | Loss: 0.00061673
Iteration 14/25 | Loss: 0.00061673
Iteration 15/25 | Loss: 0.00061673
Iteration 16/25 | Loss: 0.00061673
Iteration 17/25 | Loss: 0.00061673
Iteration 18/25 | Loss: 0.00061673
Iteration 19/25 | Loss: 0.00061673
Iteration 20/25 | Loss: 0.00061673
Iteration 21/25 | Loss: 0.00061673
Iteration 22/25 | Loss: 0.00061673
Iteration 23/25 | Loss: 0.00061673
Iteration 24/25 | Loss: 0.00061673
Iteration 25/25 | Loss: 0.00061673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061673
Iteration 2/1000 | Loss: 0.00004770
Iteration 3/1000 | Loss: 0.00003262
Iteration 4/1000 | Loss: 0.00002617
Iteration 5/1000 | Loss: 0.00002436
Iteration 6/1000 | Loss: 0.00002313
Iteration 7/1000 | Loss: 0.00002218
Iteration 8/1000 | Loss: 0.00002164
Iteration 9/1000 | Loss: 0.00002124
Iteration 10/1000 | Loss: 0.00002093
Iteration 11/1000 | Loss: 0.00002060
Iteration 12/1000 | Loss: 0.00002032
Iteration 13/1000 | Loss: 0.00002022
Iteration 14/1000 | Loss: 0.00002006
Iteration 15/1000 | Loss: 0.00002002
Iteration 16/1000 | Loss: 0.00002001
Iteration 17/1000 | Loss: 0.00001999
Iteration 18/1000 | Loss: 0.00001987
Iteration 19/1000 | Loss: 0.00001983
Iteration 20/1000 | Loss: 0.00001980
Iteration 21/1000 | Loss: 0.00001979
Iteration 22/1000 | Loss: 0.00001977
Iteration 23/1000 | Loss: 0.00001977
Iteration 24/1000 | Loss: 0.00001976
Iteration 25/1000 | Loss: 0.00001976
Iteration 26/1000 | Loss: 0.00001975
Iteration 27/1000 | Loss: 0.00001975
Iteration 28/1000 | Loss: 0.00001974
Iteration 29/1000 | Loss: 0.00001974
Iteration 30/1000 | Loss: 0.00001973
Iteration 31/1000 | Loss: 0.00001973
Iteration 32/1000 | Loss: 0.00001973
Iteration 33/1000 | Loss: 0.00001972
Iteration 34/1000 | Loss: 0.00001972
Iteration 35/1000 | Loss: 0.00001971
Iteration 36/1000 | Loss: 0.00001971
Iteration 37/1000 | Loss: 0.00001970
Iteration 38/1000 | Loss: 0.00001970
Iteration 39/1000 | Loss: 0.00001970
Iteration 40/1000 | Loss: 0.00001969
Iteration 41/1000 | Loss: 0.00001969
Iteration 42/1000 | Loss: 0.00001968
Iteration 43/1000 | Loss: 0.00001967
Iteration 44/1000 | Loss: 0.00001967
Iteration 45/1000 | Loss: 0.00001967
Iteration 46/1000 | Loss: 0.00001967
Iteration 47/1000 | Loss: 0.00001967
Iteration 48/1000 | Loss: 0.00001966
Iteration 49/1000 | Loss: 0.00001966
Iteration 50/1000 | Loss: 0.00001966
Iteration 51/1000 | Loss: 0.00001966
Iteration 52/1000 | Loss: 0.00001966
Iteration 53/1000 | Loss: 0.00001966
Iteration 54/1000 | Loss: 0.00001966
Iteration 55/1000 | Loss: 0.00001966
Iteration 56/1000 | Loss: 0.00001965
Iteration 57/1000 | Loss: 0.00001965
Iteration 58/1000 | Loss: 0.00001964
Iteration 59/1000 | Loss: 0.00001964
Iteration 60/1000 | Loss: 0.00001963
Iteration 61/1000 | Loss: 0.00001963
Iteration 62/1000 | Loss: 0.00001963
Iteration 63/1000 | Loss: 0.00001963
Iteration 64/1000 | Loss: 0.00001962
Iteration 65/1000 | Loss: 0.00001962
Iteration 66/1000 | Loss: 0.00001962
Iteration 67/1000 | Loss: 0.00001962
Iteration 68/1000 | Loss: 0.00001961
Iteration 69/1000 | Loss: 0.00001961
Iteration 70/1000 | Loss: 0.00001961
Iteration 71/1000 | Loss: 0.00001961
Iteration 72/1000 | Loss: 0.00001961
Iteration 73/1000 | Loss: 0.00001961
Iteration 74/1000 | Loss: 0.00001961
Iteration 75/1000 | Loss: 0.00001960
Iteration 76/1000 | Loss: 0.00001960
Iteration 77/1000 | Loss: 0.00001960
Iteration 78/1000 | Loss: 0.00001960
Iteration 79/1000 | Loss: 0.00001959
Iteration 80/1000 | Loss: 0.00001959
Iteration 81/1000 | Loss: 0.00001958
Iteration 82/1000 | Loss: 0.00001958
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001958
Iteration 85/1000 | Loss: 0.00001958
Iteration 86/1000 | Loss: 0.00001958
Iteration 87/1000 | Loss: 0.00001958
Iteration 88/1000 | Loss: 0.00001958
Iteration 89/1000 | Loss: 0.00001957
Iteration 90/1000 | Loss: 0.00001957
Iteration 91/1000 | Loss: 0.00001957
Iteration 92/1000 | Loss: 0.00001957
Iteration 93/1000 | Loss: 0.00001957
Iteration 94/1000 | Loss: 0.00001957
Iteration 95/1000 | Loss: 0.00001957
Iteration 96/1000 | Loss: 0.00001956
Iteration 97/1000 | Loss: 0.00001956
Iteration 98/1000 | Loss: 0.00001956
Iteration 99/1000 | Loss: 0.00001956
Iteration 100/1000 | Loss: 0.00001956
Iteration 101/1000 | Loss: 0.00001956
Iteration 102/1000 | Loss: 0.00001956
Iteration 103/1000 | Loss: 0.00001956
Iteration 104/1000 | Loss: 0.00001956
Iteration 105/1000 | Loss: 0.00001955
Iteration 106/1000 | Loss: 0.00001955
Iteration 107/1000 | Loss: 0.00001955
Iteration 108/1000 | Loss: 0.00001955
Iteration 109/1000 | Loss: 0.00001954
Iteration 110/1000 | Loss: 0.00001954
Iteration 111/1000 | Loss: 0.00001954
Iteration 112/1000 | Loss: 0.00001954
Iteration 113/1000 | Loss: 0.00001954
Iteration 114/1000 | Loss: 0.00001954
Iteration 115/1000 | Loss: 0.00001954
Iteration 116/1000 | Loss: 0.00001954
Iteration 117/1000 | Loss: 0.00001954
Iteration 118/1000 | Loss: 0.00001954
Iteration 119/1000 | Loss: 0.00001953
Iteration 120/1000 | Loss: 0.00001953
Iteration 121/1000 | Loss: 0.00001953
Iteration 122/1000 | Loss: 0.00001953
Iteration 123/1000 | Loss: 0.00001952
Iteration 124/1000 | Loss: 0.00001952
Iteration 125/1000 | Loss: 0.00001952
Iteration 126/1000 | Loss: 0.00001952
Iteration 127/1000 | Loss: 0.00001952
Iteration 128/1000 | Loss: 0.00001952
Iteration 129/1000 | Loss: 0.00001952
Iteration 130/1000 | Loss: 0.00001952
Iteration 131/1000 | Loss: 0.00001952
Iteration 132/1000 | Loss: 0.00001952
Iteration 133/1000 | Loss: 0.00001952
Iteration 134/1000 | Loss: 0.00001952
Iteration 135/1000 | Loss: 0.00001952
Iteration 136/1000 | Loss: 0.00001951
Iteration 137/1000 | Loss: 0.00001951
Iteration 138/1000 | Loss: 0.00001951
Iteration 139/1000 | Loss: 0.00001951
Iteration 140/1000 | Loss: 0.00001951
Iteration 141/1000 | Loss: 0.00001951
Iteration 142/1000 | Loss: 0.00001951
Iteration 143/1000 | Loss: 0.00001951
Iteration 144/1000 | Loss: 0.00001951
Iteration 145/1000 | Loss: 0.00001951
Iteration 146/1000 | Loss: 0.00001951
Iteration 147/1000 | Loss: 0.00001951
Iteration 148/1000 | Loss: 0.00001951
Iteration 149/1000 | Loss: 0.00001951
Iteration 150/1000 | Loss: 0.00001950
Iteration 151/1000 | Loss: 0.00001950
Iteration 152/1000 | Loss: 0.00001950
Iteration 153/1000 | Loss: 0.00001950
Iteration 154/1000 | Loss: 0.00001950
Iteration 155/1000 | Loss: 0.00001950
Iteration 156/1000 | Loss: 0.00001950
Iteration 157/1000 | Loss: 0.00001950
Iteration 158/1000 | Loss: 0.00001950
Iteration 159/1000 | Loss: 0.00001950
Iteration 160/1000 | Loss: 0.00001950
Iteration 161/1000 | Loss: 0.00001950
Iteration 162/1000 | Loss: 0.00001950
Iteration 163/1000 | Loss: 0.00001950
Iteration 164/1000 | Loss: 0.00001950
Iteration 165/1000 | Loss: 0.00001950
Iteration 166/1000 | Loss: 0.00001950
Iteration 167/1000 | Loss: 0.00001949
Iteration 168/1000 | Loss: 0.00001949
Iteration 169/1000 | Loss: 0.00001949
Iteration 170/1000 | Loss: 0.00001949
Iteration 171/1000 | Loss: 0.00001949
Iteration 172/1000 | Loss: 0.00001949
Iteration 173/1000 | Loss: 0.00001949
Iteration 174/1000 | Loss: 0.00001949
Iteration 175/1000 | Loss: 0.00001949
Iteration 176/1000 | Loss: 0.00001949
Iteration 177/1000 | Loss: 0.00001949
Iteration 178/1000 | Loss: 0.00001949
Iteration 179/1000 | Loss: 0.00001948
Iteration 180/1000 | Loss: 0.00001948
Iteration 181/1000 | Loss: 0.00001948
Iteration 182/1000 | Loss: 0.00001948
Iteration 183/1000 | Loss: 0.00001948
Iteration 184/1000 | Loss: 0.00001947
Iteration 185/1000 | Loss: 0.00001947
Iteration 186/1000 | Loss: 0.00001947
Iteration 187/1000 | Loss: 0.00001947
Iteration 188/1000 | Loss: 0.00001947
Iteration 189/1000 | Loss: 0.00001947
Iteration 190/1000 | Loss: 0.00001947
Iteration 191/1000 | Loss: 0.00001947
Iteration 192/1000 | Loss: 0.00001947
Iteration 193/1000 | Loss: 0.00001947
Iteration 194/1000 | Loss: 0.00001947
Iteration 195/1000 | Loss: 0.00001947
Iteration 196/1000 | Loss: 0.00001947
Iteration 197/1000 | Loss: 0.00001947
Iteration 198/1000 | Loss: 0.00001947
Iteration 199/1000 | Loss: 0.00001947
Iteration 200/1000 | Loss: 0.00001947
Iteration 201/1000 | Loss: 0.00001947
Iteration 202/1000 | Loss: 0.00001947
Iteration 203/1000 | Loss: 0.00001947
Iteration 204/1000 | Loss: 0.00001947
Iteration 205/1000 | Loss: 0.00001947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.947179225680884e-05, 1.947179225680884e-05, 1.947179225680884e-05, 1.947179225680884e-05, 1.947179225680884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.947179225680884e-05

Optimization complete. Final v2v error: 3.654046058654785 mm

Highest mean error: 4.604503631591797 mm for frame 163

Lowest mean error: 3.1190974712371826 mm for frame 217

Saving results

Total time: 48.41423964500427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453706
Iteration 2/25 | Loss: 0.00148756
Iteration 3/25 | Loss: 0.00126285
Iteration 4/25 | Loss: 0.00123365
Iteration 5/25 | Loss: 0.00122886
Iteration 6/25 | Loss: 0.00122780
Iteration 7/25 | Loss: 0.00122780
Iteration 8/25 | Loss: 0.00122780
Iteration 9/25 | Loss: 0.00122780
Iteration 10/25 | Loss: 0.00122780
Iteration 11/25 | Loss: 0.00122780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001227803761139512, 0.001227803761139512, 0.001227803761139512, 0.001227803761139512, 0.001227803761139512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001227803761139512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55096138
Iteration 2/25 | Loss: 0.00073257
Iteration 3/25 | Loss: 0.00073257
Iteration 4/25 | Loss: 0.00073257
Iteration 5/25 | Loss: 0.00073257
Iteration 6/25 | Loss: 0.00073257
Iteration 7/25 | Loss: 0.00073257
Iteration 8/25 | Loss: 0.00073257
Iteration 9/25 | Loss: 0.00073257
Iteration 10/25 | Loss: 0.00073257
Iteration 11/25 | Loss: 0.00073257
Iteration 12/25 | Loss: 0.00073257
Iteration 13/25 | Loss: 0.00073257
Iteration 14/25 | Loss: 0.00073257
Iteration 15/25 | Loss: 0.00073257
Iteration 16/25 | Loss: 0.00073257
Iteration 17/25 | Loss: 0.00073257
Iteration 18/25 | Loss: 0.00073257
Iteration 19/25 | Loss: 0.00073257
Iteration 20/25 | Loss: 0.00073257
Iteration 21/25 | Loss: 0.00073257
Iteration 22/25 | Loss: 0.00073257
Iteration 23/25 | Loss: 0.00073257
Iteration 24/25 | Loss: 0.00073257
Iteration 25/25 | Loss: 0.00073257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073257
Iteration 2/1000 | Loss: 0.00003270
Iteration 3/1000 | Loss: 0.00002090
Iteration 4/1000 | Loss: 0.00001856
Iteration 5/1000 | Loss: 0.00001761
Iteration 6/1000 | Loss: 0.00001683
Iteration 7/1000 | Loss: 0.00001633
Iteration 8/1000 | Loss: 0.00001600
Iteration 9/1000 | Loss: 0.00001580
Iteration 10/1000 | Loss: 0.00001550
Iteration 11/1000 | Loss: 0.00001533
Iteration 12/1000 | Loss: 0.00001529
Iteration 13/1000 | Loss: 0.00001528
Iteration 14/1000 | Loss: 0.00001528
Iteration 15/1000 | Loss: 0.00001527
Iteration 16/1000 | Loss: 0.00001519
Iteration 17/1000 | Loss: 0.00001508
Iteration 18/1000 | Loss: 0.00001505
Iteration 19/1000 | Loss: 0.00001502
Iteration 20/1000 | Loss: 0.00001501
Iteration 21/1000 | Loss: 0.00001501
Iteration 22/1000 | Loss: 0.00001501
Iteration 23/1000 | Loss: 0.00001501
Iteration 24/1000 | Loss: 0.00001501
Iteration 25/1000 | Loss: 0.00001500
Iteration 26/1000 | Loss: 0.00001500
Iteration 27/1000 | Loss: 0.00001499
Iteration 28/1000 | Loss: 0.00001498
Iteration 29/1000 | Loss: 0.00001498
Iteration 30/1000 | Loss: 0.00001498
Iteration 31/1000 | Loss: 0.00001497
Iteration 32/1000 | Loss: 0.00001497
Iteration 33/1000 | Loss: 0.00001496
Iteration 34/1000 | Loss: 0.00001496
Iteration 35/1000 | Loss: 0.00001496
Iteration 36/1000 | Loss: 0.00001495
Iteration 37/1000 | Loss: 0.00001495
Iteration 38/1000 | Loss: 0.00001494
Iteration 39/1000 | Loss: 0.00001494
Iteration 40/1000 | Loss: 0.00001493
Iteration 41/1000 | Loss: 0.00001493
Iteration 42/1000 | Loss: 0.00001493
Iteration 43/1000 | Loss: 0.00001492
Iteration 44/1000 | Loss: 0.00001492
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001490
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001489
Iteration 50/1000 | Loss: 0.00001489
Iteration 51/1000 | Loss: 0.00001488
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001486
Iteration 58/1000 | Loss: 0.00001486
Iteration 59/1000 | Loss: 0.00001484
Iteration 60/1000 | Loss: 0.00001484
Iteration 61/1000 | Loss: 0.00001483
Iteration 62/1000 | Loss: 0.00001482
Iteration 63/1000 | Loss: 0.00001482
Iteration 64/1000 | Loss: 0.00001482
Iteration 65/1000 | Loss: 0.00001481
Iteration 66/1000 | Loss: 0.00001481
Iteration 67/1000 | Loss: 0.00001481
Iteration 68/1000 | Loss: 0.00001481
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001479
Iteration 72/1000 | Loss: 0.00001479
Iteration 73/1000 | Loss: 0.00001478
Iteration 74/1000 | Loss: 0.00001478
Iteration 75/1000 | Loss: 0.00001478
Iteration 76/1000 | Loss: 0.00001478
Iteration 77/1000 | Loss: 0.00001478
Iteration 78/1000 | Loss: 0.00001478
Iteration 79/1000 | Loss: 0.00001477
Iteration 80/1000 | Loss: 0.00001477
Iteration 81/1000 | Loss: 0.00001477
Iteration 82/1000 | Loss: 0.00001477
Iteration 83/1000 | Loss: 0.00001477
Iteration 84/1000 | Loss: 0.00001477
Iteration 85/1000 | Loss: 0.00001476
Iteration 86/1000 | Loss: 0.00001476
Iteration 87/1000 | Loss: 0.00001476
Iteration 88/1000 | Loss: 0.00001476
Iteration 89/1000 | Loss: 0.00001476
Iteration 90/1000 | Loss: 0.00001475
Iteration 91/1000 | Loss: 0.00001475
Iteration 92/1000 | Loss: 0.00001475
Iteration 93/1000 | Loss: 0.00001475
Iteration 94/1000 | Loss: 0.00001474
Iteration 95/1000 | Loss: 0.00001474
Iteration 96/1000 | Loss: 0.00001474
Iteration 97/1000 | Loss: 0.00001474
Iteration 98/1000 | Loss: 0.00001474
Iteration 99/1000 | Loss: 0.00001474
Iteration 100/1000 | Loss: 0.00001474
Iteration 101/1000 | Loss: 0.00001474
Iteration 102/1000 | Loss: 0.00001474
Iteration 103/1000 | Loss: 0.00001473
Iteration 104/1000 | Loss: 0.00001473
Iteration 105/1000 | Loss: 0.00001473
Iteration 106/1000 | Loss: 0.00001472
Iteration 107/1000 | Loss: 0.00001472
Iteration 108/1000 | Loss: 0.00001472
Iteration 109/1000 | Loss: 0.00001472
Iteration 110/1000 | Loss: 0.00001472
Iteration 111/1000 | Loss: 0.00001472
Iteration 112/1000 | Loss: 0.00001472
Iteration 113/1000 | Loss: 0.00001472
Iteration 114/1000 | Loss: 0.00001472
Iteration 115/1000 | Loss: 0.00001472
Iteration 116/1000 | Loss: 0.00001471
Iteration 117/1000 | Loss: 0.00001471
Iteration 118/1000 | Loss: 0.00001471
Iteration 119/1000 | Loss: 0.00001471
Iteration 120/1000 | Loss: 0.00001471
Iteration 121/1000 | Loss: 0.00001470
Iteration 122/1000 | Loss: 0.00001470
Iteration 123/1000 | Loss: 0.00001470
Iteration 124/1000 | Loss: 0.00001470
Iteration 125/1000 | Loss: 0.00001470
Iteration 126/1000 | Loss: 0.00001470
Iteration 127/1000 | Loss: 0.00001470
Iteration 128/1000 | Loss: 0.00001469
Iteration 129/1000 | Loss: 0.00001469
Iteration 130/1000 | Loss: 0.00001469
Iteration 131/1000 | Loss: 0.00001469
Iteration 132/1000 | Loss: 0.00001469
Iteration 133/1000 | Loss: 0.00001469
Iteration 134/1000 | Loss: 0.00001469
Iteration 135/1000 | Loss: 0.00001469
Iteration 136/1000 | Loss: 0.00001469
Iteration 137/1000 | Loss: 0.00001468
Iteration 138/1000 | Loss: 0.00001468
Iteration 139/1000 | Loss: 0.00001468
Iteration 140/1000 | Loss: 0.00001467
Iteration 141/1000 | Loss: 0.00001467
Iteration 142/1000 | Loss: 0.00001467
Iteration 143/1000 | Loss: 0.00001466
Iteration 144/1000 | Loss: 0.00001466
Iteration 145/1000 | Loss: 0.00001466
Iteration 146/1000 | Loss: 0.00001465
Iteration 147/1000 | Loss: 0.00001465
Iteration 148/1000 | Loss: 0.00001464
Iteration 149/1000 | Loss: 0.00001464
Iteration 150/1000 | Loss: 0.00001464
Iteration 151/1000 | Loss: 0.00001464
Iteration 152/1000 | Loss: 0.00001463
Iteration 153/1000 | Loss: 0.00001463
Iteration 154/1000 | Loss: 0.00001463
Iteration 155/1000 | Loss: 0.00001462
Iteration 156/1000 | Loss: 0.00001462
Iteration 157/1000 | Loss: 0.00001462
Iteration 158/1000 | Loss: 0.00001461
Iteration 159/1000 | Loss: 0.00001461
Iteration 160/1000 | Loss: 0.00001461
Iteration 161/1000 | Loss: 0.00001461
Iteration 162/1000 | Loss: 0.00001461
Iteration 163/1000 | Loss: 0.00001461
Iteration 164/1000 | Loss: 0.00001461
Iteration 165/1000 | Loss: 0.00001461
Iteration 166/1000 | Loss: 0.00001460
Iteration 167/1000 | Loss: 0.00001460
Iteration 168/1000 | Loss: 0.00001460
Iteration 169/1000 | Loss: 0.00001460
Iteration 170/1000 | Loss: 0.00001460
Iteration 171/1000 | Loss: 0.00001459
Iteration 172/1000 | Loss: 0.00001459
Iteration 173/1000 | Loss: 0.00001459
Iteration 174/1000 | Loss: 0.00001459
Iteration 175/1000 | Loss: 0.00001459
Iteration 176/1000 | Loss: 0.00001459
Iteration 177/1000 | Loss: 0.00001458
Iteration 178/1000 | Loss: 0.00001458
Iteration 179/1000 | Loss: 0.00001458
Iteration 180/1000 | Loss: 0.00001458
Iteration 181/1000 | Loss: 0.00001458
Iteration 182/1000 | Loss: 0.00001458
Iteration 183/1000 | Loss: 0.00001458
Iteration 184/1000 | Loss: 0.00001458
Iteration 185/1000 | Loss: 0.00001458
Iteration 186/1000 | Loss: 0.00001458
Iteration 187/1000 | Loss: 0.00001458
Iteration 188/1000 | Loss: 0.00001458
Iteration 189/1000 | Loss: 0.00001458
Iteration 190/1000 | Loss: 0.00001458
Iteration 191/1000 | Loss: 0.00001458
Iteration 192/1000 | Loss: 0.00001458
Iteration 193/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.4582414223696105e-05, 1.4582414223696105e-05, 1.4582414223696105e-05, 1.4582414223696105e-05, 1.4582414223696105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4582414223696105e-05

Optimization complete. Final v2v error: 3.2089455127716064 mm

Highest mean error: 4.257877349853516 mm for frame 105

Lowest mean error: 2.851672410964966 mm for frame 159

Saving results

Total time: 45.606783628463745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827098
Iteration 2/25 | Loss: 0.00146138
Iteration 3/25 | Loss: 0.00132458
Iteration 4/25 | Loss: 0.00130630
Iteration 5/25 | Loss: 0.00130176
Iteration 6/25 | Loss: 0.00130143
Iteration 7/25 | Loss: 0.00130143
Iteration 8/25 | Loss: 0.00130143
Iteration 9/25 | Loss: 0.00130143
Iteration 10/25 | Loss: 0.00130143
Iteration 11/25 | Loss: 0.00130143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001301425858400762, 0.001301425858400762, 0.001301425858400762, 0.001301425858400762, 0.001301425858400762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001301425858400762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40053487
Iteration 2/25 | Loss: 0.00076333
Iteration 3/25 | Loss: 0.00076327
Iteration 4/25 | Loss: 0.00076327
Iteration 5/25 | Loss: 0.00076327
Iteration 6/25 | Loss: 0.00076326
Iteration 7/25 | Loss: 0.00076326
Iteration 8/25 | Loss: 0.00076326
Iteration 9/25 | Loss: 0.00076326
Iteration 10/25 | Loss: 0.00076326
Iteration 11/25 | Loss: 0.00076326
Iteration 12/25 | Loss: 0.00076326
Iteration 13/25 | Loss: 0.00076326
Iteration 14/25 | Loss: 0.00076326
Iteration 15/25 | Loss: 0.00076326
Iteration 16/25 | Loss: 0.00076326
Iteration 17/25 | Loss: 0.00076326
Iteration 18/25 | Loss: 0.00076326
Iteration 19/25 | Loss: 0.00076326
Iteration 20/25 | Loss: 0.00076326
Iteration 21/25 | Loss: 0.00076326
Iteration 22/25 | Loss: 0.00076326
Iteration 23/25 | Loss: 0.00076326
Iteration 24/25 | Loss: 0.00076326
Iteration 25/25 | Loss: 0.00076326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076326
Iteration 2/1000 | Loss: 0.00004987
Iteration 3/1000 | Loss: 0.00002853
Iteration 4/1000 | Loss: 0.00002458
Iteration 5/1000 | Loss: 0.00002299
Iteration 6/1000 | Loss: 0.00002183
Iteration 7/1000 | Loss: 0.00002098
Iteration 8/1000 | Loss: 0.00002045
Iteration 9/1000 | Loss: 0.00002007
Iteration 10/1000 | Loss: 0.00001977
Iteration 11/1000 | Loss: 0.00001949
Iteration 12/1000 | Loss: 0.00001930
Iteration 13/1000 | Loss: 0.00001919
Iteration 14/1000 | Loss: 0.00001906
Iteration 15/1000 | Loss: 0.00001905
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00001896
Iteration 18/1000 | Loss: 0.00001893
Iteration 19/1000 | Loss: 0.00001887
Iteration 20/1000 | Loss: 0.00001886
Iteration 21/1000 | Loss: 0.00001885
Iteration 22/1000 | Loss: 0.00001885
Iteration 23/1000 | Loss: 0.00001884
Iteration 24/1000 | Loss: 0.00001884
Iteration 25/1000 | Loss: 0.00001883
Iteration 26/1000 | Loss: 0.00001881
Iteration 27/1000 | Loss: 0.00001880
Iteration 28/1000 | Loss: 0.00001880
Iteration 29/1000 | Loss: 0.00001879
Iteration 30/1000 | Loss: 0.00001879
Iteration 31/1000 | Loss: 0.00001878
Iteration 32/1000 | Loss: 0.00001878
Iteration 33/1000 | Loss: 0.00001878
Iteration 34/1000 | Loss: 0.00001876
Iteration 35/1000 | Loss: 0.00001876
Iteration 36/1000 | Loss: 0.00001876
Iteration 37/1000 | Loss: 0.00001876
Iteration 38/1000 | Loss: 0.00001876
Iteration 39/1000 | Loss: 0.00001876
Iteration 40/1000 | Loss: 0.00001876
Iteration 41/1000 | Loss: 0.00001875
Iteration 42/1000 | Loss: 0.00001875
Iteration 43/1000 | Loss: 0.00001875
Iteration 44/1000 | Loss: 0.00001875
Iteration 45/1000 | Loss: 0.00001875
Iteration 46/1000 | Loss: 0.00001875
Iteration 47/1000 | Loss: 0.00001875
Iteration 48/1000 | Loss: 0.00001875
Iteration 49/1000 | Loss: 0.00001875
Iteration 50/1000 | Loss: 0.00001874
Iteration 51/1000 | Loss: 0.00001874
Iteration 52/1000 | Loss: 0.00001873
Iteration 53/1000 | Loss: 0.00001872
Iteration 54/1000 | Loss: 0.00001872
Iteration 55/1000 | Loss: 0.00001872
Iteration 56/1000 | Loss: 0.00001871
Iteration 57/1000 | Loss: 0.00001871
Iteration 58/1000 | Loss: 0.00001871
Iteration 59/1000 | Loss: 0.00001871
Iteration 60/1000 | Loss: 0.00001870
Iteration 61/1000 | Loss: 0.00001870
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001868
Iteration 64/1000 | Loss: 0.00001868
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001868
Iteration 67/1000 | Loss: 0.00001868
Iteration 68/1000 | Loss: 0.00001868
Iteration 69/1000 | Loss: 0.00001867
Iteration 70/1000 | Loss: 0.00001867
Iteration 71/1000 | Loss: 0.00001867
Iteration 72/1000 | Loss: 0.00001866
Iteration 73/1000 | Loss: 0.00001865
Iteration 74/1000 | Loss: 0.00001865
Iteration 75/1000 | Loss: 0.00001865
Iteration 76/1000 | Loss: 0.00001865
Iteration 77/1000 | Loss: 0.00001865
Iteration 78/1000 | Loss: 0.00001864
Iteration 79/1000 | Loss: 0.00001864
Iteration 80/1000 | Loss: 0.00001864
Iteration 81/1000 | Loss: 0.00001864
Iteration 82/1000 | Loss: 0.00001864
Iteration 83/1000 | Loss: 0.00001864
Iteration 84/1000 | Loss: 0.00001864
Iteration 85/1000 | Loss: 0.00001863
Iteration 86/1000 | Loss: 0.00001863
Iteration 87/1000 | Loss: 0.00001863
Iteration 88/1000 | Loss: 0.00001863
Iteration 89/1000 | Loss: 0.00001863
Iteration 90/1000 | Loss: 0.00001862
Iteration 91/1000 | Loss: 0.00001861
Iteration 92/1000 | Loss: 0.00001860
Iteration 93/1000 | Loss: 0.00001860
Iteration 94/1000 | Loss: 0.00001860
Iteration 95/1000 | Loss: 0.00001860
Iteration 96/1000 | Loss: 0.00001860
Iteration 97/1000 | Loss: 0.00001859
Iteration 98/1000 | Loss: 0.00001859
Iteration 99/1000 | Loss: 0.00001859
Iteration 100/1000 | Loss: 0.00001859
Iteration 101/1000 | Loss: 0.00001858
Iteration 102/1000 | Loss: 0.00001858
Iteration 103/1000 | Loss: 0.00001858
Iteration 104/1000 | Loss: 0.00001858
Iteration 105/1000 | Loss: 0.00001858
Iteration 106/1000 | Loss: 0.00001858
Iteration 107/1000 | Loss: 0.00001858
Iteration 108/1000 | Loss: 0.00001858
Iteration 109/1000 | Loss: 0.00001858
Iteration 110/1000 | Loss: 0.00001858
Iteration 111/1000 | Loss: 0.00001858
Iteration 112/1000 | Loss: 0.00001857
Iteration 113/1000 | Loss: 0.00001857
Iteration 114/1000 | Loss: 0.00001857
Iteration 115/1000 | Loss: 0.00001857
Iteration 116/1000 | Loss: 0.00001857
Iteration 117/1000 | Loss: 0.00001857
Iteration 118/1000 | Loss: 0.00001856
Iteration 119/1000 | Loss: 0.00001856
Iteration 120/1000 | Loss: 0.00001855
Iteration 121/1000 | Loss: 0.00001855
Iteration 122/1000 | Loss: 0.00001854
Iteration 123/1000 | Loss: 0.00001854
Iteration 124/1000 | Loss: 0.00001854
Iteration 125/1000 | Loss: 0.00001854
Iteration 126/1000 | Loss: 0.00001854
Iteration 127/1000 | Loss: 0.00001854
Iteration 128/1000 | Loss: 0.00001853
Iteration 129/1000 | Loss: 0.00001853
Iteration 130/1000 | Loss: 0.00001853
Iteration 131/1000 | Loss: 0.00001853
Iteration 132/1000 | Loss: 0.00001853
Iteration 133/1000 | Loss: 0.00001853
Iteration 134/1000 | Loss: 0.00001853
Iteration 135/1000 | Loss: 0.00001853
Iteration 136/1000 | Loss: 0.00001852
Iteration 137/1000 | Loss: 0.00001852
Iteration 138/1000 | Loss: 0.00001852
Iteration 139/1000 | Loss: 0.00001852
Iteration 140/1000 | Loss: 0.00001852
Iteration 141/1000 | Loss: 0.00001852
Iteration 142/1000 | Loss: 0.00001851
Iteration 143/1000 | Loss: 0.00001851
Iteration 144/1000 | Loss: 0.00001851
Iteration 145/1000 | Loss: 0.00001851
Iteration 146/1000 | Loss: 0.00001851
Iteration 147/1000 | Loss: 0.00001851
Iteration 148/1000 | Loss: 0.00001851
Iteration 149/1000 | Loss: 0.00001851
Iteration 150/1000 | Loss: 0.00001851
Iteration 151/1000 | Loss: 0.00001850
Iteration 152/1000 | Loss: 0.00001850
Iteration 153/1000 | Loss: 0.00001850
Iteration 154/1000 | Loss: 0.00001850
Iteration 155/1000 | Loss: 0.00001850
Iteration 156/1000 | Loss: 0.00001850
Iteration 157/1000 | Loss: 0.00001850
Iteration 158/1000 | Loss: 0.00001849
Iteration 159/1000 | Loss: 0.00001849
Iteration 160/1000 | Loss: 0.00001849
Iteration 161/1000 | Loss: 0.00001849
Iteration 162/1000 | Loss: 0.00001849
Iteration 163/1000 | Loss: 0.00001849
Iteration 164/1000 | Loss: 0.00001849
Iteration 165/1000 | Loss: 0.00001849
Iteration 166/1000 | Loss: 0.00001849
Iteration 167/1000 | Loss: 0.00001849
Iteration 168/1000 | Loss: 0.00001848
Iteration 169/1000 | Loss: 0.00001848
Iteration 170/1000 | Loss: 0.00001848
Iteration 171/1000 | Loss: 0.00001848
Iteration 172/1000 | Loss: 0.00001848
Iteration 173/1000 | Loss: 0.00001848
Iteration 174/1000 | Loss: 0.00001848
Iteration 175/1000 | Loss: 0.00001848
Iteration 176/1000 | Loss: 0.00001848
Iteration 177/1000 | Loss: 0.00001848
Iteration 178/1000 | Loss: 0.00001848
Iteration 179/1000 | Loss: 0.00001848
Iteration 180/1000 | Loss: 0.00001848
Iteration 181/1000 | Loss: 0.00001848
Iteration 182/1000 | Loss: 0.00001848
Iteration 183/1000 | Loss: 0.00001847
Iteration 184/1000 | Loss: 0.00001847
Iteration 185/1000 | Loss: 0.00001847
Iteration 186/1000 | Loss: 0.00001847
Iteration 187/1000 | Loss: 0.00001847
Iteration 188/1000 | Loss: 0.00001847
Iteration 189/1000 | Loss: 0.00001847
Iteration 190/1000 | Loss: 0.00001847
Iteration 191/1000 | Loss: 0.00001847
Iteration 192/1000 | Loss: 0.00001847
Iteration 193/1000 | Loss: 0.00001847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.8466713299858384e-05, 1.8466713299858384e-05, 1.8466713299858384e-05, 1.8466713299858384e-05, 1.8466713299858384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8466713299858384e-05

Optimization complete. Final v2v error: 3.6175899505615234 mm

Highest mean error: 4.38842248916626 mm for frame 1

Lowest mean error: 3.2021005153656006 mm for frame 186

Saving results

Total time: 47.43992829322815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917790
Iteration 2/25 | Loss: 0.00151658
Iteration 3/25 | Loss: 0.00139054
Iteration 4/25 | Loss: 0.00137680
Iteration 5/25 | Loss: 0.00137260
Iteration 6/25 | Loss: 0.00137257
Iteration 7/25 | Loss: 0.00137257
Iteration 8/25 | Loss: 0.00137257
Iteration 9/25 | Loss: 0.00137257
Iteration 10/25 | Loss: 0.00137257
Iteration 11/25 | Loss: 0.00137257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013725715689361095, 0.0013725715689361095, 0.0013725715689361095, 0.0013725715689361095, 0.0013725715689361095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013725715689361095

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04884219
Iteration 2/25 | Loss: 0.00097819
Iteration 3/25 | Loss: 0.00097819
Iteration 4/25 | Loss: 0.00097819
Iteration 5/25 | Loss: 0.00097819
Iteration 6/25 | Loss: 0.00097819
Iteration 7/25 | Loss: 0.00097819
Iteration 8/25 | Loss: 0.00097819
Iteration 9/25 | Loss: 0.00097819
Iteration 10/25 | Loss: 0.00097819
Iteration 11/25 | Loss: 0.00097819
Iteration 12/25 | Loss: 0.00097819
Iteration 13/25 | Loss: 0.00097819
Iteration 14/25 | Loss: 0.00097819
Iteration 15/25 | Loss: 0.00097819
Iteration 16/25 | Loss: 0.00097819
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009781877743080258, 0.0009781877743080258, 0.0009781877743080258, 0.0009781877743080258, 0.0009781877743080258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009781877743080258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097819
Iteration 2/1000 | Loss: 0.00005463
Iteration 3/1000 | Loss: 0.00003652
Iteration 4/1000 | Loss: 0.00002934
Iteration 5/1000 | Loss: 0.00002773
Iteration 6/1000 | Loss: 0.00002663
Iteration 7/1000 | Loss: 0.00002609
Iteration 8/1000 | Loss: 0.00002549
Iteration 9/1000 | Loss: 0.00002517
Iteration 10/1000 | Loss: 0.00002485
Iteration 11/1000 | Loss: 0.00002453
Iteration 12/1000 | Loss: 0.00002436
Iteration 13/1000 | Loss: 0.00002418
Iteration 14/1000 | Loss: 0.00002402
Iteration 15/1000 | Loss: 0.00002399
Iteration 16/1000 | Loss: 0.00002395
Iteration 17/1000 | Loss: 0.00002395
Iteration 18/1000 | Loss: 0.00002393
Iteration 19/1000 | Loss: 0.00002392
Iteration 20/1000 | Loss: 0.00002391
Iteration 21/1000 | Loss: 0.00002391
Iteration 22/1000 | Loss: 0.00002390
Iteration 23/1000 | Loss: 0.00002390
Iteration 24/1000 | Loss: 0.00002390
Iteration 25/1000 | Loss: 0.00002388
Iteration 26/1000 | Loss: 0.00002388
Iteration 27/1000 | Loss: 0.00002388
Iteration 28/1000 | Loss: 0.00002387
Iteration 29/1000 | Loss: 0.00002387
Iteration 30/1000 | Loss: 0.00002386
Iteration 31/1000 | Loss: 0.00002386
Iteration 32/1000 | Loss: 0.00002385
Iteration 33/1000 | Loss: 0.00002383
Iteration 34/1000 | Loss: 0.00002383
Iteration 35/1000 | Loss: 0.00002383
Iteration 36/1000 | Loss: 0.00002383
Iteration 37/1000 | Loss: 0.00002383
Iteration 38/1000 | Loss: 0.00002383
Iteration 39/1000 | Loss: 0.00002383
Iteration 40/1000 | Loss: 0.00002382
Iteration 41/1000 | Loss: 0.00002382
Iteration 42/1000 | Loss: 0.00002382
Iteration 43/1000 | Loss: 0.00002382
Iteration 44/1000 | Loss: 0.00002382
Iteration 45/1000 | Loss: 0.00002382
Iteration 46/1000 | Loss: 0.00002382
Iteration 47/1000 | Loss: 0.00002382
Iteration 48/1000 | Loss: 0.00002382
Iteration 49/1000 | Loss: 0.00002380
Iteration 50/1000 | Loss: 0.00002380
Iteration 51/1000 | Loss: 0.00002379
Iteration 52/1000 | Loss: 0.00002376
Iteration 53/1000 | Loss: 0.00002376
Iteration 54/1000 | Loss: 0.00002373
Iteration 55/1000 | Loss: 0.00002372
Iteration 56/1000 | Loss: 0.00002372
Iteration 57/1000 | Loss: 0.00002372
Iteration 58/1000 | Loss: 0.00002371
Iteration 59/1000 | Loss: 0.00002371
Iteration 60/1000 | Loss: 0.00002370
Iteration 61/1000 | Loss: 0.00002370
Iteration 62/1000 | Loss: 0.00002370
Iteration 63/1000 | Loss: 0.00002370
Iteration 64/1000 | Loss: 0.00002370
Iteration 65/1000 | Loss: 0.00002370
Iteration 66/1000 | Loss: 0.00002370
Iteration 67/1000 | Loss: 0.00002370
Iteration 68/1000 | Loss: 0.00002370
Iteration 69/1000 | Loss: 0.00002370
Iteration 70/1000 | Loss: 0.00002369
Iteration 71/1000 | Loss: 0.00002369
Iteration 72/1000 | Loss: 0.00002369
Iteration 73/1000 | Loss: 0.00002369
Iteration 74/1000 | Loss: 0.00002368
Iteration 75/1000 | Loss: 0.00002368
Iteration 76/1000 | Loss: 0.00002367
Iteration 77/1000 | Loss: 0.00002367
Iteration 78/1000 | Loss: 0.00002367
Iteration 79/1000 | Loss: 0.00002367
Iteration 80/1000 | Loss: 0.00002366
Iteration 81/1000 | Loss: 0.00002366
Iteration 82/1000 | Loss: 0.00002366
Iteration 83/1000 | Loss: 0.00002365
Iteration 84/1000 | Loss: 0.00002363
Iteration 85/1000 | Loss: 0.00002363
Iteration 86/1000 | Loss: 0.00002363
Iteration 87/1000 | Loss: 0.00002363
Iteration 88/1000 | Loss: 0.00002362
Iteration 89/1000 | Loss: 0.00002362
Iteration 90/1000 | Loss: 0.00002361
Iteration 91/1000 | Loss: 0.00002360
Iteration 92/1000 | Loss: 0.00002360
Iteration 93/1000 | Loss: 0.00002359
Iteration 94/1000 | Loss: 0.00002358
Iteration 95/1000 | Loss: 0.00002358
Iteration 96/1000 | Loss: 0.00002358
Iteration 97/1000 | Loss: 0.00002357
Iteration 98/1000 | Loss: 0.00002357
Iteration 99/1000 | Loss: 0.00002357
Iteration 100/1000 | Loss: 0.00002357
Iteration 101/1000 | Loss: 0.00002357
Iteration 102/1000 | Loss: 0.00002357
Iteration 103/1000 | Loss: 0.00002357
Iteration 104/1000 | Loss: 0.00002356
Iteration 105/1000 | Loss: 0.00002356
Iteration 106/1000 | Loss: 0.00002356
Iteration 107/1000 | Loss: 0.00002356
Iteration 108/1000 | Loss: 0.00002356
Iteration 109/1000 | Loss: 0.00002356
Iteration 110/1000 | Loss: 0.00002355
Iteration 111/1000 | Loss: 0.00002355
Iteration 112/1000 | Loss: 0.00002355
Iteration 113/1000 | Loss: 0.00002355
Iteration 114/1000 | Loss: 0.00002355
Iteration 115/1000 | Loss: 0.00002355
Iteration 116/1000 | Loss: 0.00002354
Iteration 117/1000 | Loss: 0.00002354
Iteration 118/1000 | Loss: 0.00002354
Iteration 119/1000 | Loss: 0.00002354
Iteration 120/1000 | Loss: 0.00002354
Iteration 121/1000 | Loss: 0.00002354
Iteration 122/1000 | Loss: 0.00002354
Iteration 123/1000 | Loss: 0.00002354
Iteration 124/1000 | Loss: 0.00002354
Iteration 125/1000 | Loss: 0.00002354
Iteration 126/1000 | Loss: 0.00002353
Iteration 127/1000 | Loss: 0.00002353
Iteration 128/1000 | Loss: 0.00002353
Iteration 129/1000 | Loss: 0.00002353
Iteration 130/1000 | Loss: 0.00002352
Iteration 131/1000 | Loss: 0.00002352
Iteration 132/1000 | Loss: 0.00002352
Iteration 133/1000 | Loss: 0.00002352
Iteration 134/1000 | Loss: 0.00002352
Iteration 135/1000 | Loss: 0.00002352
Iteration 136/1000 | Loss: 0.00002352
Iteration 137/1000 | Loss: 0.00002352
Iteration 138/1000 | Loss: 0.00002352
Iteration 139/1000 | Loss: 0.00002352
Iteration 140/1000 | Loss: 0.00002351
Iteration 141/1000 | Loss: 0.00002351
Iteration 142/1000 | Loss: 0.00002351
Iteration 143/1000 | Loss: 0.00002351
Iteration 144/1000 | Loss: 0.00002351
Iteration 145/1000 | Loss: 0.00002351
Iteration 146/1000 | Loss: 0.00002350
Iteration 147/1000 | Loss: 0.00002350
Iteration 148/1000 | Loss: 0.00002350
Iteration 149/1000 | Loss: 0.00002350
Iteration 150/1000 | Loss: 0.00002350
Iteration 151/1000 | Loss: 0.00002350
Iteration 152/1000 | Loss: 0.00002349
Iteration 153/1000 | Loss: 0.00002349
Iteration 154/1000 | Loss: 0.00002349
Iteration 155/1000 | Loss: 0.00002349
Iteration 156/1000 | Loss: 0.00002349
Iteration 157/1000 | Loss: 0.00002349
Iteration 158/1000 | Loss: 0.00002349
Iteration 159/1000 | Loss: 0.00002349
Iteration 160/1000 | Loss: 0.00002349
Iteration 161/1000 | Loss: 0.00002349
Iteration 162/1000 | Loss: 0.00002349
Iteration 163/1000 | Loss: 0.00002349
Iteration 164/1000 | Loss: 0.00002349
Iteration 165/1000 | Loss: 0.00002349
Iteration 166/1000 | Loss: 0.00002349
Iteration 167/1000 | Loss: 0.00002349
Iteration 168/1000 | Loss: 0.00002349
Iteration 169/1000 | Loss: 0.00002349
Iteration 170/1000 | Loss: 0.00002349
Iteration 171/1000 | Loss: 0.00002349
Iteration 172/1000 | Loss: 0.00002349
Iteration 173/1000 | Loss: 0.00002349
Iteration 174/1000 | Loss: 0.00002349
Iteration 175/1000 | Loss: 0.00002349
Iteration 176/1000 | Loss: 0.00002349
Iteration 177/1000 | Loss: 0.00002349
Iteration 178/1000 | Loss: 0.00002349
Iteration 179/1000 | Loss: 0.00002349
Iteration 180/1000 | Loss: 0.00002349
Iteration 181/1000 | Loss: 0.00002349
Iteration 182/1000 | Loss: 0.00002349
Iteration 183/1000 | Loss: 0.00002349
Iteration 184/1000 | Loss: 0.00002349
Iteration 185/1000 | Loss: 0.00002349
Iteration 186/1000 | Loss: 0.00002349
Iteration 187/1000 | Loss: 0.00002349
Iteration 188/1000 | Loss: 0.00002349
Iteration 189/1000 | Loss: 0.00002349
Iteration 190/1000 | Loss: 0.00002349
Iteration 191/1000 | Loss: 0.00002349
Iteration 192/1000 | Loss: 0.00002349
Iteration 193/1000 | Loss: 0.00002349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.3490416424465366e-05, 2.3490416424465366e-05, 2.3490416424465366e-05, 2.3490416424465366e-05, 2.3490416424465366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3490416424465366e-05

Optimization complete. Final v2v error: 3.9852182865142822 mm

Highest mean error: 4.623359680175781 mm for frame 81

Lowest mean error: 3.5426666736602783 mm for frame 0

Saving results

Total time: 40.52430772781372
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815540
Iteration 2/25 | Loss: 0.00148334
Iteration 3/25 | Loss: 0.00129484
Iteration 4/25 | Loss: 0.00127868
Iteration 5/25 | Loss: 0.00127729
Iteration 6/25 | Loss: 0.00127729
Iteration 7/25 | Loss: 0.00127729
Iteration 8/25 | Loss: 0.00127729
Iteration 9/25 | Loss: 0.00127729
Iteration 10/25 | Loss: 0.00127729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012772921472787857, 0.0012772921472787857, 0.0012772921472787857, 0.0012772921472787857, 0.0012772921472787857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012772921472787857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04046595
Iteration 2/25 | Loss: 0.00057742
Iteration 3/25 | Loss: 0.00057741
Iteration 4/25 | Loss: 0.00057741
Iteration 5/25 | Loss: 0.00057741
Iteration 6/25 | Loss: 0.00057741
Iteration 7/25 | Loss: 0.00057741
Iteration 8/25 | Loss: 0.00057741
Iteration 9/25 | Loss: 0.00057741
Iteration 10/25 | Loss: 0.00057741
Iteration 11/25 | Loss: 0.00057741
Iteration 12/25 | Loss: 0.00057741
Iteration 13/25 | Loss: 0.00057741
Iteration 14/25 | Loss: 0.00057741
Iteration 15/25 | Loss: 0.00057741
Iteration 16/25 | Loss: 0.00057741
Iteration 17/25 | Loss: 0.00057741
Iteration 18/25 | Loss: 0.00057741
Iteration 19/25 | Loss: 0.00057741
Iteration 20/25 | Loss: 0.00057741
Iteration 21/25 | Loss: 0.00057741
Iteration 22/25 | Loss: 0.00057741
Iteration 23/25 | Loss: 0.00057741
Iteration 24/25 | Loss: 0.00057741
Iteration 25/25 | Loss: 0.00057741
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005774118471890688, 0.0005774118471890688, 0.0005774118471890688, 0.0005774118471890688, 0.0005774118471890688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005774118471890688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057741
Iteration 2/1000 | Loss: 0.00003581
Iteration 3/1000 | Loss: 0.00002750
Iteration 4/1000 | Loss: 0.00002544
Iteration 5/1000 | Loss: 0.00002438
Iteration 6/1000 | Loss: 0.00002375
Iteration 7/1000 | Loss: 0.00002324
Iteration 8/1000 | Loss: 0.00002292
Iteration 9/1000 | Loss: 0.00002264
Iteration 10/1000 | Loss: 0.00002242
Iteration 11/1000 | Loss: 0.00002237
Iteration 12/1000 | Loss: 0.00002223
Iteration 13/1000 | Loss: 0.00002223
Iteration 14/1000 | Loss: 0.00002212
Iteration 15/1000 | Loss: 0.00002199
Iteration 16/1000 | Loss: 0.00002194
Iteration 17/1000 | Loss: 0.00002183
Iteration 18/1000 | Loss: 0.00002180
Iteration 19/1000 | Loss: 0.00002178
Iteration 20/1000 | Loss: 0.00002177
Iteration 21/1000 | Loss: 0.00002177
Iteration 22/1000 | Loss: 0.00002177
Iteration 23/1000 | Loss: 0.00002176
Iteration 24/1000 | Loss: 0.00002176
Iteration 25/1000 | Loss: 0.00002176
Iteration 26/1000 | Loss: 0.00002176
Iteration 27/1000 | Loss: 0.00002176
Iteration 28/1000 | Loss: 0.00002176
Iteration 29/1000 | Loss: 0.00002175
Iteration 30/1000 | Loss: 0.00002175
Iteration 31/1000 | Loss: 0.00002174
Iteration 32/1000 | Loss: 0.00002174
Iteration 33/1000 | Loss: 0.00002174
Iteration 34/1000 | Loss: 0.00002174
Iteration 35/1000 | Loss: 0.00002173
Iteration 36/1000 | Loss: 0.00002167
Iteration 37/1000 | Loss: 0.00002165
Iteration 38/1000 | Loss: 0.00002165
Iteration 39/1000 | Loss: 0.00002165
Iteration 40/1000 | Loss: 0.00002164
Iteration 41/1000 | Loss: 0.00002164
Iteration 42/1000 | Loss: 0.00002164
Iteration 43/1000 | Loss: 0.00002164
Iteration 44/1000 | Loss: 0.00002164
Iteration 45/1000 | Loss: 0.00002164
Iteration 46/1000 | Loss: 0.00002164
Iteration 47/1000 | Loss: 0.00002164
Iteration 48/1000 | Loss: 0.00002164
Iteration 49/1000 | Loss: 0.00002164
Iteration 50/1000 | Loss: 0.00002163
Iteration 51/1000 | Loss: 0.00002163
Iteration 52/1000 | Loss: 0.00002163
Iteration 53/1000 | Loss: 0.00002163
Iteration 54/1000 | Loss: 0.00002163
Iteration 55/1000 | Loss: 0.00002163
Iteration 56/1000 | Loss: 0.00002163
Iteration 57/1000 | Loss: 0.00002162
Iteration 58/1000 | Loss: 0.00002162
Iteration 59/1000 | Loss: 0.00002161
Iteration 60/1000 | Loss: 0.00002161
Iteration 61/1000 | Loss: 0.00002161
Iteration 62/1000 | Loss: 0.00002161
Iteration 63/1000 | Loss: 0.00002160
Iteration 64/1000 | Loss: 0.00002160
Iteration 65/1000 | Loss: 0.00002159
Iteration 66/1000 | Loss: 0.00002159
Iteration 67/1000 | Loss: 0.00002159
Iteration 68/1000 | Loss: 0.00002159
Iteration 69/1000 | Loss: 0.00002159
Iteration 70/1000 | Loss: 0.00002159
Iteration 71/1000 | Loss: 0.00002158
Iteration 72/1000 | Loss: 0.00002158
Iteration 73/1000 | Loss: 0.00002158
Iteration 74/1000 | Loss: 0.00002158
Iteration 75/1000 | Loss: 0.00002157
Iteration 76/1000 | Loss: 0.00002157
Iteration 77/1000 | Loss: 0.00002156
Iteration 78/1000 | Loss: 0.00002156
Iteration 79/1000 | Loss: 0.00002156
Iteration 80/1000 | Loss: 0.00002156
Iteration 81/1000 | Loss: 0.00002155
Iteration 82/1000 | Loss: 0.00002155
Iteration 83/1000 | Loss: 0.00002155
Iteration 84/1000 | Loss: 0.00002155
Iteration 85/1000 | Loss: 0.00002155
Iteration 86/1000 | Loss: 0.00002155
Iteration 87/1000 | Loss: 0.00002155
Iteration 88/1000 | Loss: 0.00002154
Iteration 89/1000 | Loss: 0.00002154
Iteration 90/1000 | Loss: 0.00002154
Iteration 91/1000 | Loss: 0.00002154
Iteration 92/1000 | Loss: 0.00002154
Iteration 93/1000 | Loss: 0.00002154
Iteration 94/1000 | Loss: 0.00002154
Iteration 95/1000 | Loss: 0.00002154
Iteration 96/1000 | Loss: 0.00002153
Iteration 97/1000 | Loss: 0.00002153
Iteration 98/1000 | Loss: 0.00002153
Iteration 99/1000 | Loss: 0.00002153
Iteration 100/1000 | Loss: 0.00002153
Iteration 101/1000 | Loss: 0.00002153
Iteration 102/1000 | Loss: 0.00002153
Iteration 103/1000 | Loss: 0.00002152
Iteration 104/1000 | Loss: 0.00002152
Iteration 105/1000 | Loss: 0.00002152
Iteration 106/1000 | Loss: 0.00002152
Iteration 107/1000 | Loss: 0.00002152
Iteration 108/1000 | Loss: 0.00002152
Iteration 109/1000 | Loss: 0.00002152
Iteration 110/1000 | Loss: 0.00002152
Iteration 111/1000 | Loss: 0.00002152
Iteration 112/1000 | Loss: 0.00002152
Iteration 113/1000 | Loss: 0.00002152
Iteration 114/1000 | Loss: 0.00002152
Iteration 115/1000 | Loss: 0.00002152
Iteration 116/1000 | Loss: 0.00002152
Iteration 117/1000 | Loss: 0.00002152
Iteration 118/1000 | Loss: 0.00002152
Iteration 119/1000 | Loss: 0.00002152
Iteration 120/1000 | Loss: 0.00002152
Iteration 121/1000 | Loss: 0.00002152
Iteration 122/1000 | Loss: 0.00002152
Iteration 123/1000 | Loss: 0.00002152
Iteration 124/1000 | Loss: 0.00002151
Iteration 125/1000 | Loss: 0.00002151
Iteration 126/1000 | Loss: 0.00002151
Iteration 127/1000 | Loss: 0.00002151
Iteration 128/1000 | Loss: 0.00002151
Iteration 129/1000 | Loss: 0.00002151
Iteration 130/1000 | Loss: 0.00002151
Iteration 131/1000 | Loss: 0.00002151
Iteration 132/1000 | Loss: 0.00002151
Iteration 133/1000 | Loss: 0.00002151
Iteration 134/1000 | Loss: 0.00002151
Iteration 135/1000 | Loss: 0.00002151
Iteration 136/1000 | Loss: 0.00002151
Iteration 137/1000 | Loss: 0.00002151
Iteration 138/1000 | Loss: 0.00002151
Iteration 139/1000 | Loss: 0.00002151
Iteration 140/1000 | Loss: 0.00002151
Iteration 141/1000 | Loss: 0.00002151
Iteration 142/1000 | Loss: 0.00002151
Iteration 143/1000 | Loss: 0.00002151
Iteration 144/1000 | Loss: 0.00002151
Iteration 145/1000 | Loss: 0.00002151
Iteration 146/1000 | Loss: 0.00002151
Iteration 147/1000 | Loss: 0.00002151
Iteration 148/1000 | Loss: 0.00002151
Iteration 149/1000 | Loss: 0.00002151
Iteration 150/1000 | Loss: 0.00002151
Iteration 151/1000 | Loss: 0.00002151
Iteration 152/1000 | Loss: 0.00002151
Iteration 153/1000 | Loss: 0.00002151
Iteration 154/1000 | Loss: 0.00002151
Iteration 155/1000 | Loss: 0.00002151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.1509391444851644e-05, 2.1509391444851644e-05, 2.1509391444851644e-05, 2.1509391444851644e-05, 2.1509391444851644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1509391444851644e-05

Optimization complete. Final v2v error: 3.8015859127044678 mm

Highest mean error: 3.937074899673462 mm for frame 107

Lowest mean error: 3.6425578594207764 mm for frame 36

Saving results

Total time: 36.69712209701538
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018939
Iteration 2/25 | Loss: 0.00293773
Iteration 3/25 | Loss: 0.00255017
Iteration 4/25 | Loss: 0.00199742
Iteration 5/25 | Loss: 0.00187092
Iteration 6/25 | Loss: 0.00185950
Iteration 7/25 | Loss: 0.00180262
Iteration 8/25 | Loss: 0.00211002
Iteration 9/25 | Loss: 0.00186587
Iteration 10/25 | Loss: 0.00159964
Iteration 11/25 | Loss: 0.00150671
Iteration 12/25 | Loss: 0.00146125
Iteration 13/25 | Loss: 0.00143968
Iteration 14/25 | Loss: 0.00143338
Iteration 15/25 | Loss: 0.00141750
Iteration 16/25 | Loss: 0.00140668
Iteration 17/25 | Loss: 0.00135765
Iteration 18/25 | Loss: 0.00132441
Iteration 19/25 | Loss: 0.00129763
Iteration 20/25 | Loss: 0.00128030
Iteration 21/25 | Loss: 0.00126834
Iteration 22/25 | Loss: 0.00126518
Iteration 23/25 | Loss: 0.00125537
Iteration 24/25 | Loss: 0.00125170
Iteration 25/25 | Loss: 0.00124551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64262640
Iteration 2/25 | Loss: 0.00070085
Iteration 3/25 | Loss: 0.00070085
Iteration 4/25 | Loss: 0.00070085
Iteration 5/25 | Loss: 0.00070085
Iteration 6/25 | Loss: 0.00070085
Iteration 7/25 | Loss: 0.00070085
Iteration 8/25 | Loss: 0.00070085
Iteration 9/25 | Loss: 0.00070085
Iteration 10/25 | Loss: 0.00070085
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.000700850912835449, 0.000700850912835449, 0.000700850912835449, 0.000700850912835449, 0.000700850912835449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000700850912835449

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070085
Iteration 2/1000 | Loss: 0.00006735
Iteration 3/1000 | Loss: 0.00003670
Iteration 4/1000 | Loss: 0.00003010
Iteration 5/1000 | Loss: 0.00002591
Iteration 6/1000 | Loss: 0.00008132
Iteration 7/1000 | Loss: 0.00002418
Iteration 8/1000 | Loss: 0.00039069
Iteration 9/1000 | Loss: 0.00021480
Iteration 10/1000 | Loss: 0.00015384
Iteration 11/1000 | Loss: 0.00002338
Iteration 12/1000 | Loss: 0.00002146
Iteration 13/1000 | Loss: 0.00002053
Iteration 14/1000 | Loss: 0.00001989
Iteration 15/1000 | Loss: 0.00001937
Iteration 16/1000 | Loss: 0.00001907
Iteration 17/1000 | Loss: 0.00002313
Iteration 18/1000 | Loss: 0.00001858
Iteration 19/1000 | Loss: 0.00001828
Iteration 20/1000 | Loss: 0.00002299
Iteration 21/1000 | Loss: 0.00002165
Iteration 22/1000 | Loss: 0.00001916
Iteration 23/1000 | Loss: 0.00001772
Iteration 24/1000 | Loss: 0.00001769
Iteration 25/1000 | Loss: 0.00001769
Iteration 26/1000 | Loss: 0.00001768
Iteration 27/1000 | Loss: 0.00001768
Iteration 28/1000 | Loss: 0.00001768
Iteration 29/1000 | Loss: 0.00001767
Iteration 30/1000 | Loss: 0.00001765
Iteration 31/1000 | Loss: 0.00001765
Iteration 32/1000 | Loss: 0.00001764
Iteration 33/1000 | Loss: 0.00001764
Iteration 34/1000 | Loss: 0.00001763
Iteration 35/1000 | Loss: 0.00001751
Iteration 36/1000 | Loss: 0.00004428
Iteration 37/1000 | Loss: 0.00001750
Iteration 38/1000 | Loss: 0.00001741
Iteration 39/1000 | Loss: 0.00001736
Iteration 40/1000 | Loss: 0.00001732
Iteration 41/1000 | Loss: 0.00001731
Iteration 42/1000 | Loss: 0.00001727
Iteration 43/1000 | Loss: 0.00001710
Iteration 44/1000 | Loss: 0.00001679
Iteration 45/1000 | Loss: 0.00015762
Iteration 46/1000 | Loss: 0.00015762
Iteration 47/1000 | Loss: 0.00025370
Iteration 48/1000 | Loss: 0.00002669
Iteration 49/1000 | Loss: 0.00001798
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001915
Iteration 52/1000 | Loss: 0.00001716
Iteration 53/1000 | Loss: 0.00001677
Iteration 54/1000 | Loss: 0.00001621
Iteration 55/1000 | Loss: 0.00001617
Iteration 56/1000 | Loss: 0.00001616
Iteration 57/1000 | Loss: 0.00001616
Iteration 58/1000 | Loss: 0.00001616
Iteration 59/1000 | Loss: 0.00001615
Iteration 60/1000 | Loss: 0.00001614
Iteration 61/1000 | Loss: 0.00001613
Iteration 62/1000 | Loss: 0.00001612
Iteration 63/1000 | Loss: 0.00001611
Iteration 64/1000 | Loss: 0.00001609
Iteration 65/1000 | Loss: 0.00001598
Iteration 66/1000 | Loss: 0.00001598
Iteration 67/1000 | Loss: 0.00001598
Iteration 68/1000 | Loss: 0.00001598
Iteration 69/1000 | Loss: 0.00001597
Iteration 70/1000 | Loss: 0.00001597
Iteration 71/1000 | Loss: 0.00001597
Iteration 72/1000 | Loss: 0.00001596
Iteration 73/1000 | Loss: 0.00001596
Iteration 74/1000 | Loss: 0.00001594
Iteration 75/1000 | Loss: 0.00001594
Iteration 76/1000 | Loss: 0.00001594
Iteration 77/1000 | Loss: 0.00001594
Iteration 78/1000 | Loss: 0.00001594
Iteration 79/1000 | Loss: 0.00001593
Iteration 80/1000 | Loss: 0.00001593
Iteration 81/1000 | Loss: 0.00001593
Iteration 82/1000 | Loss: 0.00001593
Iteration 83/1000 | Loss: 0.00001593
Iteration 84/1000 | Loss: 0.00001593
Iteration 85/1000 | Loss: 0.00001592
Iteration 86/1000 | Loss: 0.00001592
Iteration 87/1000 | Loss: 0.00001591
Iteration 88/1000 | Loss: 0.00001591
Iteration 89/1000 | Loss: 0.00001591
Iteration 90/1000 | Loss: 0.00001591
Iteration 91/1000 | Loss: 0.00001591
Iteration 92/1000 | Loss: 0.00001591
Iteration 93/1000 | Loss: 0.00001590
Iteration 94/1000 | Loss: 0.00001590
Iteration 95/1000 | Loss: 0.00001590
Iteration 96/1000 | Loss: 0.00001590
Iteration 97/1000 | Loss: 0.00001590
Iteration 98/1000 | Loss: 0.00001590
Iteration 99/1000 | Loss: 0.00001590
Iteration 100/1000 | Loss: 0.00001589
Iteration 101/1000 | Loss: 0.00001589
Iteration 102/1000 | Loss: 0.00001589
Iteration 103/1000 | Loss: 0.00001589
Iteration 104/1000 | Loss: 0.00001589
Iteration 105/1000 | Loss: 0.00001589
Iteration 106/1000 | Loss: 0.00001589
Iteration 107/1000 | Loss: 0.00002470
Iteration 108/1000 | Loss: 0.00001588
Iteration 109/1000 | Loss: 0.00001586
Iteration 110/1000 | Loss: 0.00001585
Iteration 111/1000 | Loss: 0.00001585
Iteration 112/1000 | Loss: 0.00001585
Iteration 113/1000 | Loss: 0.00001585
Iteration 114/1000 | Loss: 0.00001584
Iteration 115/1000 | Loss: 0.00001584
Iteration 116/1000 | Loss: 0.00001584
Iteration 117/1000 | Loss: 0.00001584
Iteration 118/1000 | Loss: 0.00001584
Iteration 119/1000 | Loss: 0.00001584
Iteration 120/1000 | Loss: 0.00001584
Iteration 121/1000 | Loss: 0.00001584
Iteration 122/1000 | Loss: 0.00001584
Iteration 123/1000 | Loss: 0.00001584
Iteration 124/1000 | Loss: 0.00001584
Iteration 125/1000 | Loss: 0.00001584
Iteration 126/1000 | Loss: 0.00001584
Iteration 127/1000 | Loss: 0.00001584
Iteration 128/1000 | Loss: 0.00001583
Iteration 129/1000 | Loss: 0.00001583
Iteration 130/1000 | Loss: 0.00001583
Iteration 131/1000 | Loss: 0.00001583
Iteration 132/1000 | Loss: 0.00001583
Iteration 133/1000 | Loss: 0.00001583
Iteration 134/1000 | Loss: 0.00001583
Iteration 135/1000 | Loss: 0.00001583
Iteration 136/1000 | Loss: 0.00001583
Iteration 137/1000 | Loss: 0.00001583
Iteration 138/1000 | Loss: 0.00001583
Iteration 139/1000 | Loss: 0.00001583
Iteration 140/1000 | Loss: 0.00001583
Iteration 141/1000 | Loss: 0.00001583
Iteration 142/1000 | Loss: 0.00001583
Iteration 143/1000 | Loss: 0.00001583
Iteration 144/1000 | Loss: 0.00001583
Iteration 145/1000 | Loss: 0.00001583
Iteration 146/1000 | Loss: 0.00001583
Iteration 147/1000 | Loss: 0.00001583
Iteration 148/1000 | Loss: 0.00001583
Iteration 149/1000 | Loss: 0.00001583
Iteration 150/1000 | Loss: 0.00001583
Iteration 151/1000 | Loss: 0.00001583
Iteration 152/1000 | Loss: 0.00001583
Iteration 153/1000 | Loss: 0.00001583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.58340044436045e-05, 1.58340044436045e-05, 1.58340044436045e-05, 1.58340044436045e-05, 1.58340044436045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.58340044436045e-05

Optimization complete. Final v2v error: 3.4003090858459473 mm

Highest mean error: 4.37388277053833 mm for frame 90

Lowest mean error: 3.2535948753356934 mm for frame 239

Saving results

Total time: 123.07526540756226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435620
Iteration 2/25 | Loss: 0.00133996
Iteration 3/25 | Loss: 0.00126276
Iteration 4/25 | Loss: 0.00124972
Iteration 5/25 | Loss: 0.00124517
Iteration 6/25 | Loss: 0.00124491
Iteration 7/25 | Loss: 0.00124491
Iteration 8/25 | Loss: 0.00124491
Iteration 9/25 | Loss: 0.00124491
Iteration 10/25 | Loss: 0.00124491
Iteration 11/25 | Loss: 0.00124491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012449094792827964, 0.0012449094792827964, 0.0012449094792827964, 0.0012449094792827964, 0.0012449094792827964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012449094792827964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85683620
Iteration 2/25 | Loss: 0.00080816
Iteration 3/25 | Loss: 0.00080816
Iteration 4/25 | Loss: 0.00080816
Iteration 5/25 | Loss: 0.00080815
Iteration 6/25 | Loss: 0.00080815
Iteration 7/25 | Loss: 0.00080815
Iteration 8/25 | Loss: 0.00080815
Iteration 9/25 | Loss: 0.00080815
Iteration 10/25 | Loss: 0.00080815
Iteration 11/25 | Loss: 0.00080815
Iteration 12/25 | Loss: 0.00080815
Iteration 13/25 | Loss: 0.00080815
Iteration 14/25 | Loss: 0.00080815
Iteration 15/25 | Loss: 0.00080815
Iteration 16/25 | Loss: 0.00080815
Iteration 17/25 | Loss: 0.00080815
Iteration 18/25 | Loss: 0.00080815
Iteration 19/25 | Loss: 0.00080815
Iteration 20/25 | Loss: 0.00080815
Iteration 21/25 | Loss: 0.00080815
Iteration 22/25 | Loss: 0.00080815
Iteration 23/25 | Loss: 0.00080815
Iteration 24/25 | Loss: 0.00080815
Iteration 25/25 | Loss: 0.00080815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080815
Iteration 2/1000 | Loss: 0.00003156
Iteration 3/1000 | Loss: 0.00002458
Iteration 4/1000 | Loss: 0.00002216
Iteration 5/1000 | Loss: 0.00002132
Iteration 6/1000 | Loss: 0.00002066
Iteration 7/1000 | Loss: 0.00002029
Iteration 8/1000 | Loss: 0.00001985
Iteration 9/1000 | Loss: 0.00001957
Iteration 10/1000 | Loss: 0.00001928
Iteration 11/1000 | Loss: 0.00001903
Iteration 12/1000 | Loss: 0.00001900
Iteration 13/1000 | Loss: 0.00001880
Iteration 14/1000 | Loss: 0.00001867
Iteration 15/1000 | Loss: 0.00001867
Iteration 16/1000 | Loss: 0.00001866
Iteration 17/1000 | Loss: 0.00001860
Iteration 18/1000 | Loss: 0.00001859
Iteration 19/1000 | Loss: 0.00001857
Iteration 20/1000 | Loss: 0.00001854
Iteration 21/1000 | Loss: 0.00001853
Iteration 22/1000 | Loss: 0.00001851
Iteration 23/1000 | Loss: 0.00001850
Iteration 24/1000 | Loss: 0.00001849
Iteration 25/1000 | Loss: 0.00001848
Iteration 26/1000 | Loss: 0.00001848
Iteration 27/1000 | Loss: 0.00001847
Iteration 28/1000 | Loss: 0.00001843
Iteration 29/1000 | Loss: 0.00001842
Iteration 30/1000 | Loss: 0.00001841
Iteration 31/1000 | Loss: 0.00001841
Iteration 32/1000 | Loss: 0.00001840
Iteration 33/1000 | Loss: 0.00001835
Iteration 34/1000 | Loss: 0.00001834
Iteration 35/1000 | Loss: 0.00001834
Iteration 36/1000 | Loss: 0.00001833
Iteration 37/1000 | Loss: 0.00001832
Iteration 38/1000 | Loss: 0.00001829
Iteration 39/1000 | Loss: 0.00001828
Iteration 40/1000 | Loss: 0.00001828
Iteration 41/1000 | Loss: 0.00001828
Iteration 42/1000 | Loss: 0.00001826
Iteration 43/1000 | Loss: 0.00001826
Iteration 44/1000 | Loss: 0.00001826
Iteration 45/1000 | Loss: 0.00001824
Iteration 46/1000 | Loss: 0.00001824
Iteration 47/1000 | Loss: 0.00001824
Iteration 48/1000 | Loss: 0.00001824
Iteration 49/1000 | Loss: 0.00001823
Iteration 50/1000 | Loss: 0.00001823
Iteration 51/1000 | Loss: 0.00001822
Iteration 52/1000 | Loss: 0.00001822
Iteration 53/1000 | Loss: 0.00001822
Iteration 54/1000 | Loss: 0.00001821
Iteration 55/1000 | Loss: 0.00001821
Iteration 56/1000 | Loss: 0.00001821
Iteration 57/1000 | Loss: 0.00001821
Iteration 58/1000 | Loss: 0.00001820
Iteration 59/1000 | Loss: 0.00001820
Iteration 60/1000 | Loss: 0.00001820
Iteration 61/1000 | Loss: 0.00001820
Iteration 62/1000 | Loss: 0.00001820
Iteration 63/1000 | Loss: 0.00001819
Iteration 64/1000 | Loss: 0.00001819
Iteration 65/1000 | Loss: 0.00001819
Iteration 66/1000 | Loss: 0.00001819
Iteration 67/1000 | Loss: 0.00001818
Iteration 68/1000 | Loss: 0.00001818
Iteration 69/1000 | Loss: 0.00001818
Iteration 70/1000 | Loss: 0.00001818
Iteration 71/1000 | Loss: 0.00001817
Iteration 72/1000 | Loss: 0.00001817
Iteration 73/1000 | Loss: 0.00001817
Iteration 74/1000 | Loss: 0.00001817
Iteration 75/1000 | Loss: 0.00001817
Iteration 76/1000 | Loss: 0.00001816
Iteration 77/1000 | Loss: 0.00001816
Iteration 78/1000 | Loss: 0.00001816
Iteration 79/1000 | Loss: 0.00001815
Iteration 80/1000 | Loss: 0.00001815
Iteration 81/1000 | Loss: 0.00001815
Iteration 82/1000 | Loss: 0.00001815
Iteration 83/1000 | Loss: 0.00001815
Iteration 84/1000 | Loss: 0.00001815
Iteration 85/1000 | Loss: 0.00001815
Iteration 86/1000 | Loss: 0.00001815
Iteration 87/1000 | Loss: 0.00001815
Iteration 88/1000 | Loss: 0.00001815
Iteration 89/1000 | Loss: 0.00001815
Iteration 90/1000 | Loss: 0.00001815
Iteration 91/1000 | Loss: 0.00001815
Iteration 92/1000 | Loss: 0.00001814
Iteration 93/1000 | Loss: 0.00001814
Iteration 94/1000 | Loss: 0.00001814
Iteration 95/1000 | Loss: 0.00001814
Iteration 96/1000 | Loss: 0.00001814
Iteration 97/1000 | Loss: 0.00001814
Iteration 98/1000 | Loss: 0.00001814
Iteration 99/1000 | Loss: 0.00001814
Iteration 100/1000 | Loss: 0.00001814
Iteration 101/1000 | Loss: 0.00001814
Iteration 102/1000 | Loss: 0.00001813
Iteration 103/1000 | Loss: 0.00001813
Iteration 104/1000 | Loss: 0.00001813
Iteration 105/1000 | Loss: 0.00001813
Iteration 106/1000 | Loss: 0.00001813
Iteration 107/1000 | Loss: 0.00001813
Iteration 108/1000 | Loss: 0.00001813
Iteration 109/1000 | Loss: 0.00001813
Iteration 110/1000 | Loss: 0.00001813
Iteration 111/1000 | Loss: 0.00001813
Iteration 112/1000 | Loss: 0.00001813
Iteration 113/1000 | Loss: 0.00001812
Iteration 114/1000 | Loss: 0.00001812
Iteration 115/1000 | Loss: 0.00001812
Iteration 116/1000 | Loss: 0.00001812
Iteration 117/1000 | Loss: 0.00001812
Iteration 118/1000 | Loss: 0.00001812
Iteration 119/1000 | Loss: 0.00001812
Iteration 120/1000 | Loss: 0.00001812
Iteration 121/1000 | Loss: 0.00001812
Iteration 122/1000 | Loss: 0.00001812
Iteration 123/1000 | Loss: 0.00001812
Iteration 124/1000 | Loss: 0.00001812
Iteration 125/1000 | Loss: 0.00001812
Iteration 126/1000 | Loss: 0.00001812
Iteration 127/1000 | Loss: 0.00001812
Iteration 128/1000 | Loss: 0.00001811
Iteration 129/1000 | Loss: 0.00001811
Iteration 130/1000 | Loss: 0.00001811
Iteration 131/1000 | Loss: 0.00001811
Iteration 132/1000 | Loss: 0.00001811
Iteration 133/1000 | Loss: 0.00001811
Iteration 134/1000 | Loss: 0.00001811
Iteration 135/1000 | Loss: 0.00001811
Iteration 136/1000 | Loss: 0.00001811
Iteration 137/1000 | Loss: 0.00001811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.811466609069612e-05, 1.811466609069612e-05, 1.811466609069612e-05, 1.811466609069612e-05, 1.811466609069612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.811466609069612e-05

Optimization complete. Final v2v error: 3.6091480255126953 mm

Highest mean error: 3.885967254638672 mm for frame 31

Lowest mean error: 3.3610012531280518 mm for frame 48

Saving results

Total time: 37.24766564369202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957662
Iteration 2/25 | Loss: 0.00178824
Iteration 3/25 | Loss: 0.00147073
Iteration 4/25 | Loss: 0.00136983
Iteration 5/25 | Loss: 0.00132853
Iteration 6/25 | Loss: 0.00131159
Iteration 7/25 | Loss: 0.00130520
Iteration 8/25 | Loss: 0.00130407
Iteration 9/25 | Loss: 0.00130386
Iteration 10/25 | Loss: 0.00130383
Iteration 11/25 | Loss: 0.00130383
Iteration 12/25 | Loss: 0.00130383
Iteration 13/25 | Loss: 0.00130383
Iteration 14/25 | Loss: 0.00130383
Iteration 15/25 | Loss: 0.00130383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013038282049819827, 0.0013038282049819827, 0.0013038282049819827, 0.0013038282049819827, 0.0013038282049819827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013038282049819827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59631586
Iteration 2/25 | Loss: 0.00086166
Iteration 3/25 | Loss: 0.00086164
Iteration 4/25 | Loss: 0.00086163
Iteration 5/25 | Loss: 0.00086163
Iteration 6/25 | Loss: 0.00086163
Iteration 7/25 | Loss: 0.00086163
Iteration 8/25 | Loss: 0.00086163
Iteration 9/25 | Loss: 0.00086163
Iteration 10/25 | Loss: 0.00086163
Iteration 11/25 | Loss: 0.00086163
Iteration 12/25 | Loss: 0.00086163
Iteration 13/25 | Loss: 0.00086163
Iteration 14/25 | Loss: 0.00086163
Iteration 15/25 | Loss: 0.00086163
Iteration 16/25 | Loss: 0.00086163
Iteration 17/25 | Loss: 0.00086163
Iteration 18/25 | Loss: 0.00086163
Iteration 19/25 | Loss: 0.00086163
Iteration 20/25 | Loss: 0.00086163
Iteration 21/25 | Loss: 0.00086163
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008616331615485251, 0.0008616331615485251, 0.0008616331615485251, 0.0008616331615485251, 0.0008616331615485251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008616331615485251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086163
Iteration 2/1000 | Loss: 0.00005111
Iteration 3/1000 | Loss: 0.00003369
Iteration 4/1000 | Loss: 0.00002885
Iteration 5/1000 | Loss: 0.00002694
Iteration 6/1000 | Loss: 0.00002526
Iteration 7/1000 | Loss: 0.00002406
Iteration 8/1000 | Loss: 0.00002340
Iteration 9/1000 | Loss: 0.00002289
Iteration 10/1000 | Loss: 0.00002239
Iteration 11/1000 | Loss: 0.00002207
Iteration 12/1000 | Loss: 0.00002184
Iteration 13/1000 | Loss: 0.00002163
Iteration 14/1000 | Loss: 0.00002151
Iteration 15/1000 | Loss: 0.00002147
Iteration 16/1000 | Loss: 0.00002146
Iteration 17/1000 | Loss: 0.00002144
Iteration 18/1000 | Loss: 0.00002144
Iteration 19/1000 | Loss: 0.00002143
Iteration 20/1000 | Loss: 0.00002143
Iteration 21/1000 | Loss: 0.00002142
Iteration 22/1000 | Loss: 0.00002141
Iteration 23/1000 | Loss: 0.00002141
Iteration 24/1000 | Loss: 0.00002140
Iteration 25/1000 | Loss: 0.00002140
Iteration 26/1000 | Loss: 0.00002139
Iteration 27/1000 | Loss: 0.00002139
Iteration 28/1000 | Loss: 0.00002138
Iteration 29/1000 | Loss: 0.00002138
Iteration 30/1000 | Loss: 0.00002133
Iteration 31/1000 | Loss: 0.00002133
Iteration 32/1000 | Loss: 0.00002132
Iteration 33/1000 | Loss: 0.00002132
Iteration 34/1000 | Loss: 0.00002132
Iteration 35/1000 | Loss: 0.00002132
Iteration 36/1000 | Loss: 0.00002132
Iteration 37/1000 | Loss: 0.00002131
Iteration 38/1000 | Loss: 0.00002131
Iteration 39/1000 | Loss: 0.00002131
Iteration 40/1000 | Loss: 0.00002130
Iteration 41/1000 | Loss: 0.00002130
Iteration 42/1000 | Loss: 0.00002130
Iteration 43/1000 | Loss: 0.00002130
Iteration 44/1000 | Loss: 0.00002129
Iteration 45/1000 | Loss: 0.00002129
Iteration 46/1000 | Loss: 0.00002129
Iteration 47/1000 | Loss: 0.00002129
Iteration 48/1000 | Loss: 0.00002128
Iteration 49/1000 | Loss: 0.00002128
Iteration 50/1000 | Loss: 0.00002128
Iteration 51/1000 | Loss: 0.00002128
Iteration 52/1000 | Loss: 0.00002127
Iteration 53/1000 | Loss: 0.00002127
Iteration 54/1000 | Loss: 0.00002127
Iteration 55/1000 | Loss: 0.00002127
Iteration 56/1000 | Loss: 0.00002126
Iteration 57/1000 | Loss: 0.00002126
Iteration 58/1000 | Loss: 0.00002126
Iteration 59/1000 | Loss: 0.00002125
Iteration 60/1000 | Loss: 0.00002125
Iteration 61/1000 | Loss: 0.00002125
Iteration 62/1000 | Loss: 0.00002124
Iteration 63/1000 | Loss: 0.00002124
Iteration 64/1000 | Loss: 0.00002124
Iteration 65/1000 | Loss: 0.00002123
Iteration 66/1000 | Loss: 0.00002123
Iteration 67/1000 | Loss: 0.00002123
Iteration 68/1000 | Loss: 0.00002122
Iteration 69/1000 | Loss: 0.00002122
Iteration 70/1000 | Loss: 0.00002122
Iteration 71/1000 | Loss: 0.00002122
Iteration 72/1000 | Loss: 0.00002121
Iteration 73/1000 | Loss: 0.00002121
Iteration 74/1000 | Loss: 0.00002121
Iteration 75/1000 | Loss: 0.00002121
Iteration 76/1000 | Loss: 0.00002121
Iteration 77/1000 | Loss: 0.00002120
Iteration 78/1000 | Loss: 0.00002120
Iteration 79/1000 | Loss: 0.00002120
Iteration 80/1000 | Loss: 0.00002119
Iteration 81/1000 | Loss: 0.00002119
Iteration 82/1000 | Loss: 0.00002119
Iteration 83/1000 | Loss: 0.00002119
Iteration 84/1000 | Loss: 0.00002118
Iteration 85/1000 | Loss: 0.00002118
Iteration 86/1000 | Loss: 0.00002118
Iteration 87/1000 | Loss: 0.00002118
Iteration 88/1000 | Loss: 0.00002118
Iteration 89/1000 | Loss: 0.00002117
Iteration 90/1000 | Loss: 0.00002117
Iteration 91/1000 | Loss: 0.00002117
Iteration 92/1000 | Loss: 0.00002117
Iteration 93/1000 | Loss: 0.00002117
Iteration 94/1000 | Loss: 0.00002117
Iteration 95/1000 | Loss: 0.00002117
Iteration 96/1000 | Loss: 0.00002116
Iteration 97/1000 | Loss: 0.00002116
Iteration 98/1000 | Loss: 0.00002115
Iteration 99/1000 | Loss: 0.00002115
Iteration 100/1000 | Loss: 0.00002115
Iteration 101/1000 | Loss: 0.00002114
Iteration 102/1000 | Loss: 0.00002114
Iteration 103/1000 | Loss: 0.00002114
Iteration 104/1000 | Loss: 0.00002113
Iteration 105/1000 | Loss: 0.00002113
Iteration 106/1000 | Loss: 0.00002113
Iteration 107/1000 | Loss: 0.00002113
Iteration 108/1000 | Loss: 0.00002113
Iteration 109/1000 | Loss: 0.00002112
Iteration 110/1000 | Loss: 0.00002112
Iteration 111/1000 | Loss: 0.00002112
Iteration 112/1000 | Loss: 0.00002112
Iteration 113/1000 | Loss: 0.00002112
Iteration 114/1000 | Loss: 0.00002112
Iteration 115/1000 | Loss: 0.00002111
Iteration 116/1000 | Loss: 0.00002111
Iteration 117/1000 | Loss: 0.00002111
Iteration 118/1000 | Loss: 0.00002111
Iteration 119/1000 | Loss: 0.00002111
Iteration 120/1000 | Loss: 0.00002110
Iteration 121/1000 | Loss: 0.00002110
Iteration 122/1000 | Loss: 0.00002110
Iteration 123/1000 | Loss: 0.00002110
Iteration 124/1000 | Loss: 0.00002110
Iteration 125/1000 | Loss: 0.00002110
Iteration 126/1000 | Loss: 0.00002110
Iteration 127/1000 | Loss: 0.00002110
Iteration 128/1000 | Loss: 0.00002110
Iteration 129/1000 | Loss: 0.00002110
Iteration 130/1000 | Loss: 0.00002110
Iteration 131/1000 | Loss: 0.00002110
Iteration 132/1000 | Loss: 0.00002109
Iteration 133/1000 | Loss: 0.00002109
Iteration 134/1000 | Loss: 0.00002109
Iteration 135/1000 | Loss: 0.00002109
Iteration 136/1000 | Loss: 0.00002109
Iteration 137/1000 | Loss: 0.00002109
Iteration 138/1000 | Loss: 0.00002109
Iteration 139/1000 | Loss: 0.00002109
Iteration 140/1000 | Loss: 0.00002109
Iteration 141/1000 | Loss: 0.00002109
Iteration 142/1000 | Loss: 0.00002109
Iteration 143/1000 | Loss: 0.00002109
Iteration 144/1000 | Loss: 0.00002108
Iteration 145/1000 | Loss: 0.00002108
Iteration 146/1000 | Loss: 0.00002108
Iteration 147/1000 | Loss: 0.00002108
Iteration 148/1000 | Loss: 0.00002108
Iteration 149/1000 | Loss: 0.00002108
Iteration 150/1000 | Loss: 0.00002107
Iteration 151/1000 | Loss: 0.00002107
Iteration 152/1000 | Loss: 0.00002107
Iteration 153/1000 | Loss: 0.00002107
Iteration 154/1000 | Loss: 0.00002107
Iteration 155/1000 | Loss: 0.00002107
Iteration 156/1000 | Loss: 0.00002107
Iteration 157/1000 | Loss: 0.00002107
Iteration 158/1000 | Loss: 0.00002107
Iteration 159/1000 | Loss: 0.00002107
Iteration 160/1000 | Loss: 0.00002107
Iteration 161/1000 | Loss: 0.00002107
Iteration 162/1000 | Loss: 0.00002107
Iteration 163/1000 | Loss: 0.00002107
Iteration 164/1000 | Loss: 0.00002107
Iteration 165/1000 | Loss: 0.00002107
Iteration 166/1000 | Loss: 0.00002107
Iteration 167/1000 | Loss: 0.00002107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.1069627109682187e-05, 2.1069627109682187e-05, 2.1069627109682187e-05, 2.1069627109682187e-05, 2.1069627109682187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1069627109682187e-05

Optimization complete. Final v2v error: 3.8498380184173584 mm

Highest mean error: 4.8912529945373535 mm for frame 165

Lowest mean error: 3.427252769470215 mm for frame 139

Saving results

Total time: 54.481157302856445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769146
Iteration 2/25 | Loss: 0.00192226
Iteration 3/25 | Loss: 0.00143852
Iteration 4/25 | Loss: 0.00135538
Iteration 5/25 | Loss: 0.00127147
Iteration 6/25 | Loss: 0.00127336
Iteration 7/25 | Loss: 0.00123794
Iteration 8/25 | Loss: 0.00123812
Iteration 9/25 | Loss: 0.00121538
Iteration 10/25 | Loss: 0.00120755
Iteration 11/25 | Loss: 0.00120915
Iteration 12/25 | Loss: 0.00119309
Iteration 13/25 | Loss: 0.00119595
Iteration 14/25 | Loss: 0.00119046
Iteration 15/25 | Loss: 0.00119021
Iteration 16/25 | Loss: 0.00119021
Iteration 17/25 | Loss: 0.00119021
Iteration 18/25 | Loss: 0.00119021
Iteration 19/25 | Loss: 0.00119021
Iteration 20/25 | Loss: 0.00119021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011902120895683765, 0.0011902120895683765, 0.0011902120895683765, 0.0011902120895683765, 0.0011902120895683765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011902120895683765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11750078
Iteration 2/25 | Loss: 0.00087496
Iteration 3/25 | Loss: 0.00087495
Iteration 4/25 | Loss: 0.00081720
Iteration 5/25 | Loss: 0.00081720
Iteration 6/25 | Loss: 0.00081720
Iteration 7/25 | Loss: 0.00081720
Iteration 8/25 | Loss: 0.00081720
Iteration 9/25 | Loss: 0.00081720
Iteration 10/25 | Loss: 0.00081719
Iteration 11/25 | Loss: 0.00081719
Iteration 12/25 | Loss: 0.00081719
Iteration 13/25 | Loss: 0.00081719
Iteration 14/25 | Loss: 0.00081719
Iteration 15/25 | Loss: 0.00081719
Iteration 16/25 | Loss: 0.00081719
Iteration 17/25 | Loss: 0.00081719
Iteration 18/25 | Loss: 0.00081719
Iteration 19/25 | Loss: 0.00081719
Iteration 20/25 | Loss: 0.00081719
Iteration 21/25 | Loss: 0.00081719
Iteration 22/25 | Loss: 0.00081719
Iteration 23/25 | Loss: 0.00081719
Iteration 24/25 | Loss: 0.00081719
Iteration 25/25 | Loss: 0.00081719

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081719
Iteration 2/1000 | Loss: 0.00009768
Iteration 3/1000 | Loss: 0.00008671
Iteration 4/1000 | Loss: 0.00025265
Iteration 5/1000 | Loss: 0.00001453
Iteration 6/1000 | Loss: 0.00001383
Iteration 7/1000 | Loss: 0.00021268
Iteration 8/1000 | Loss: 0.00001319
Iteration 9/1000 | Loss: 0.00004916
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001260
Iteration 12/1000 | Loss: 0.00003475
Iteration 13/1000 | Loss: 0.00002995
Iteration 14/1000 | Loss: 0.00001245
Iteration 15/1000 | Loss: 0.00001224
Iteration 16/1000 | Loss: 0.00001221
Iteration 17/1000 | Loss: 0.00003596
Iteration 18/1000 | Loss: 0.00003596
Iteration 19/1000 | Loss: 0.00011715
Iteration 20/1000 | Loss: 0.00011975
Iteration 21/1000 | Loss: 0.00001921
Iteration 22/1000 | Loss: 0.00002378
Iteration 23/1000 | Loss: 0.00001232
Iteration 24/1000 | Loss: 0.00001344
Iteration 25/1000 | Loss: 0.00001961
Iteration 26/1000 | Loss: 0.00001313
Iteration 27/1000 | Loss: 0.00001322
Iteration 28/1000 | Loss: 0.00001235
Iteration 29/1000 | Loss: 0.00001886
Iteration 30/1000 | Loss: 0.00001441
Iteration 31/1000 | Loss: 0.00014355
Iteration 32/1000 | Loss: 0.00002412
Iteration 33/1000 | Loss: 0.00003248
Iteration 34/1000 | Loss: 0.00001539
Iteration 35/1000 | Loss: 0.00001199
Iteration 36/1000 | Loss: 0.00001210
Iteration 37/1000 | Loss: 0.00001891
Iteration 38/1000 | Loss: 0.00004395
Iteration 39/1000 | Loss: 0.00002438
Iteration 40/1000 | Loss: 0.00001412
Iteration 41/1000 | Loss: 0.00002027
Iteration 42/1000 | Loss: 0.00001325
Iteration 43/1000 | Loss: 0.00001471
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001158
Iteration 46/1000 | Loss: 0.00001157
Iteration 47/1000 | Loss: 0.00001157
Iteration 48/1000 | Loss: 0.00001157
Iteration 49/1000 | Loss: 0.00001157
Iteration 50/1000 | Loss: 0.00001157
Iteration 51/1000 | Loss: 0.00001157
Iteration 52/1000 | Loss: 0.00001157
Iteration 53/1000 | Loss: 0.00001157
Iteration 54/1000 | Loss: 0.00001156
Iteration 55/1000 | Loss: 0.00001155
Iteration 56/1000 | Loss: 0.00001155
Iteration 57/1000 | Loss: 0.00001153
Iteration 58/1000 | Loss: 0.00001153
Iteration 59/1000 | Loss: 0.00001153
Iteration 60/1000 | Loss: 0.00001153
Iteration 61/1000 | Loss: 0.00001153
Iteration 62/1000 | Loss: 0.00001153
Iteration 63/1000 | Loss: 0.00001153
Iteration 64/1000 | Loss: 0.00001152
Iteration 65/1000 | Loss: 0.00001152
Iteration 66/1000 | Loss: 0.00001152
Iteration 67/1000 | Loss: 0.00001152
Iteration 68/1000 | Loss: 0.00001152
Iteration 69/1000 | Loss: 0.00001152
Iteration 70/1000 | Loss: 0.00001152
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001280
Iteration 73/1000 | Loss: 0.00004584
Iteration 74/1000 | Loss: 0.00001452
Iteration 75/1000 | Loss: 0.00001681
Iteration 76/1000 | Loss: 0.00001274
Iteration 77/1000 | Loss: 0.00002461
Iteration 78/1000 | Loss: 0.00002433
Iteration 79/1000 | Loss: 0.00001274
Iteration 80/1000 | Loss: 0.00001146
Iteration 81/1000 | Loss: 0.00001142
Iteration 82/1000 | Loss: 0.00001142
Iteration 83/1000 | Loss: 0.00001142
Iteration 84/1000 | Loss: 0.00001142
Iteration 85/1000 | Loss: 0.00001142
Iteration 86/1000 | Loss: 0.00001142
Iteration 87/1000 | Loss: 0.00001142
Iteration 88/1000 | Loss: 0.00001142
Iteration 89/1000 | Loss: 0.00001142
Iteration 90/1000 | Loss: 0.00001142
Iteration 91/1000 | Loss: 0.00001142
Iteration 92/1000 | Loss: 0.00001142
Iteration 93/1000 | Loss: 0.00001270
Iteration 94/1000 | Loss: 0.00002698
Iteration 95/1000 | Loss: 0.00001141
Iteration 96/1000 | Loss: 0.00002705
Iteration 97/1000 | Loss: 0.00012201
Iteration 98/1000 | Loss: 0.00027338
Iteration 99/1000 | Loss: 0.00001716
Iteration 100/1000 | Loss: 0.00001275
Iteration 101/1000 | Loss: 0.00001625
Iteration 102/1000 | Loss: 0.00002762
Iteration 103/1000 | Loss: 0.00002061
Iteration 104/1000 | Loss: 0.00005427
Iteration 105/1000 | Loss: 0.00001422
Iteration 106/1000 | Loss: 0.00002093
Iteration 107/1000 | Loss: 0.00001998
Iteration 108/1000 | Loss: 0.00009664
Iteration 109/1000 | Loss: 0.00007091
Iteration 110/1000 | Loss: 0.00001254
Iteration 111/1000 | Loss: 0.00001606
Iteration 112/1000 | Loss: 0.00001168
Iteration 113/1000 | Loss: 0.00001146
Iteration 114/1000 | Loss: 0.00001145
Iteration 115/1000 | Loss: 0.00001142
Iteration 116/1000 | Loss: 0.00001142
Iteration 117/1000 | Loss: 0.00001141
Iteration 118/1000 | Loss: 0.00001141
Iteration 119/1000 | Loss: 0.00001141
Iteration 120/1000 | Loss: 0.00001141
Iteration 121/1000 | Loss: 0.00001141
Iteration 122/1000 | Loss: 0.00001141
Iteration 123/1000 | Loss: 0.00001141
Iteration 124/1000 | Loss: 0.00001140
Iteration 125/1000 | Loss: 0.00001140
Iteration 126/1000 | Loss: 0.00002817
Iteration 127/1000 | Loss: 0.00003181
Iteration 128/1000 | Loss: 0.00005902
Iteration 129/1000 | Loss: 0.00041600
Iteration 130/1000 | Loss: 0.00002481
Iteration 131/1000 | Loss: 0.00006764
Iteration 132/1000 | Loss: 0.00002057
Iteration 133/1000 | Loss: 0.00002291
Iteration 134/1000 | Loss: 0.00005109
Iteration 135/1000 | Loss: 0.00001234
Iteration 136/1000 | Loss: 0.00001368
Iteration 137/1000 | Loss: 0.00001180
Iteration 138/1000 | Loss: 0.00002573
Iteration 139/1000 | Loss: 0.00013110
Iteration 140/1000 | Loss: 0.00001544
Iteration 141/1000 | Loss: 0.00001152
Iteration 142/1000 | Loss: 0.00001137
Iteration 143/1000 | Loss: 0.00001137
Iteration 144/1000 | Loss: 0.00001137
Iteration 145/1000 | Loss: 0.00001136
Iteration 146/1000 | Loss: 0.00001136
Iteration 147/1000 | Loss: 0.00001136
Iteration 148/1000 | Loss: 0.00001136
Iteration 149/1000 | Loss: 0.00001136
Iteration 150/1000 | Loss: 0.00001136
Iteration 151/1000 | Loss: 0.00001136
Iteration 152/1000 | Loss: 0.00001136
Iteration 153/1000 | Loss: 0.00002488
Iteration 154/1000 | Loss: 0.00001965
Iteration 155/1000 | Loss: 0.00001968
Iteration 156/1000 | Loss: 0.00001264
Iteration 157/1000 | Loss: 0.00001136
Iteration 158/1000 | Loss: 0.00001136
Iteration 159/1000 | Loss: 0.00001135
Iteration 160/1000 | Loss: 0.00001135
Iteration 161/1000 | Loss: 0.00001135
Iteration 162/1000 | Loss: 0.00001135
Iteration 163/1000 | Loss: 0.00001135
Iteration 164/1000 | Loss: 0.00001135
Iteration 165/1000 | Loss: 0.00001135
Iteration 166/1000 | Loss: 0.00001135
Iteration 167/1000 | Loss: 0.00001135
Iteration 168/1000 | Loss: 0.00001135
Iteration 169/1000 | Loss: 0.00001135
Iteration 170/1000 | Loss: 0.00001135
Iteration 171/1000 | Loss: 0.00001135
Iteration 172/1000 | Loss: 0.00001135
Iteration 173/1000 | Loss: 0.00001135
Iteration 174/1000 | Loss: 0.00001135
Iteration 175/1000 | Loss: 0.00001135
Iteration 176/1000 | Loss: 0.00001135
Iteration 177/1000 | Loss: 0.00001135
Iteration 178/1000 | Loss: 0.00001135
Iteration 179/1000 | Loss: 0.00001135
Iteration 180/1000 | Loss: 0.00001135
Iteration 181/1000 | Loss: 0.00001135
Iteration 182/1000 | Loss: 0.00001135
Iteration 183/1000 | Loss: 0.00001135
Iteration 184/1000 | Loss: 0.00001135
Iteration 185/1000 | Loss: 0.00001135
Iteration 186/1000 | Loss: 0.00001135
Iteration 187/1000 | Loss: 0.00001135
Iteration 188/1000 | Loss: 0.00001135
Iteration 189/1000 | Loss: 0.00001135
Iteration 190/1000 | Loss: 0.00001135
Iteration 191/1000 | Loss: 0.00001135
Iteration 192/1000 | Loss: 0.00001135
Iteration 193/1000 | Loss: 0.00001135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.1351606190146413e-05, 1.1351606190146413e-05, 1.1351606190146413e-05, 1.1351606190146413e-05, 1.1351606190146413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1351606190146413e-05

Optimization complete. Final v2v error: 2.8957159519195557 mm

Highest mean error: 3.2270588874816895 mm for frame 60

Lowest mean error: 2.7249677181243896 mm for frame 148

Saving results

Total time: 144.98308801651
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877976
Iteration 2/25 | Loss: 0.00152494
Iteration 3/25 | Loss: 0.00133167
Iteration 4/25 | Loss: 0.00130046
Iteration 5/25 | Loss: 0.00129261
Iteration 6/25 | Loss: 0.00129137
Iteration 7/25 | Loss: 0.00129137
Iteration 8/25 | Loss: 0.00129137
Iteration 9/25 | Loss: 0.00129137
Iteration 10/25 | Loss: 0.00129137
Iteration 11/25 | Loss: 0.00129137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012913683895021677, 0.0012913683895021677, 0.0012913683895021677, 0.0012913683895021677, 0.0012913683895021677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012913683895021677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26787305
Iteration 2/25 | Loss: 0.00077262
Iteration 3/25 | Loss: 0.00077262
Iteration 4/25 | Loss: 0.00077262
Iteration 5/25 | Loss: 0.00077262
Iteration 6/25 | Loss: 0.00077262
Iteration 7/25 | Loss: 0.00077262
Iteration 8/25 | Loss: 0.00077262
Iteration 9/25 | Loss: 0.00077262
Iteration 10/25 | Loss: 0.00077262
Iteration 11/25 | Loss: 0.00077262
Iteration 12/25 | Loss: 0.00077262
Iteration 13/25 | Loss: 0.00077262
Iteration 14/25 | Loss: 0.00077262
Iteration 15/25 | Loss: 0.00077262
Iteration 16/25 | Loss: 0.00077262
Iteration 17/25 | Loss: 0.00077262
Iteration 18/25 | Loss: 0.00077262
Iteration 19/25 | Loss: 0.00077262
Iteration 20/25 | Loss: 0.00077262
Iteration 21/25 | Loss: 0.00077262
Iteration 22/25 | Loss: 0.00077262
Iteration 23/25 | Loss: 0.00077262
Iteration 24/25 | Loss: 0.00077262
Iteration 25/25 | Loss: 0.00077262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077262
Iteration 2/1000 | Loss: 0.00006477
Iteration 3/1000 | Loss: 0.00004086
Iteration 4/1000 | Loss: 0.00003427
Iteration 5/1000 | Loss: 0.00003196
Iteration 6/1000 | Loss: 0.00003004
Iteration 7/1000 | Loss: 0.00002845
Iteration 8/1000 | Loss: 0.00002746
Iteration 9/1000 | Loss: 0.00002680
Iteration 10/1000 | Loss: 0.00002633
Iteration 11/1000 | Loss: 0.00002600
Iteration 12/1000 | Loss: 0.00002574
Iteration 13/1000 | Loss: 0.00002568
Iteration 14/1000 | Loss: 0.00002547
Iteration 15/1000 | Loss: 0.00002531
Iteration 16/1000 | Loss: 0.00002525
Iteration 17/1000 | Loss: 0.00002523
Iteration 18/1000 | Loss: 0.00002522
Iteration 19/1000 | Loss: 0.00002520
Iteration 20/1000 | Loss: 0.00002517
Iteration 21/1000 | Loss: 0.00002515
Iteration 22/1000 | Loss: 0.00002510
Iteration 23/1000 | Loss: 0.00002508
Iteration 24/1000 | Loss: 0.00002507
Iteration 25/1000 | Loss: 0.00002506
Iteration 26/1000 | Loss: 0.00002506
Iteration 27/1000 | Loss: 0.00002505
Iteration 28/1000 | Loss: 0.00002504
Iteration 29/1000 | Loss: 0.00002504
Iteration 30/1000 | Loss: 0.00002503
Iteration 31/1000 | Loss: 0.00002503
Iteration 32/1000 | Loss: 0.00002502
Iteration 33/1000 | Loss: 0.00002502
Iteration 34/1000 | Loss: 0.00002501
Iteration 35/1000 | Loss: 0.00002500
Iteration 36/1000 | Loss: 0.00002500
Iteration 37/1000 | Loss: 0.00002498
Iteration 38/1000 | Loss: 0.00002497
Iteration 39/1000 | Loss: 0.00002497
Iteration 40/1000 | Loss: 0.00002496
Iteration 41/1000 | Loss: 0.00002495
Iteration 42/1000 | Loss: 0.00002495
Iteration 43/1000 | Loss: 0.00002495
Iteration 44/1000 | Loss: 0.00002494
Iteration 45/1000 | Loss: 0.00002494
Iteration 46/1000 | Loss: 0.00002493
Iteration 47/1000 | Loss: 0.00002493
Iteration 48/1000 | Loss: 0.00002492
Iteration 49/1000 | Loss: 0.00002492
Iteration 50/1000 | Loss: 0.00002491
Iteration 51/1000 | Loss: 0.00002491
Iteration 52/1000 | Loss: 0.00002491
Iteration 53/1000 | Loss: 0.00002490
Iteration 54/1000 | Loss: 0.00002490
Iteration 55/1000 | Loss: 0.00002489
Iteration 56/1000 | Loss: 0.00002489
Iteration 57/1000 | Loss: 0.00002489
Iteration 58/1000 | Loss: 0.00002488
Iteration 59/1000 | Loss: 0.00002488
Iteration 60/1000 | Loss: 0.00002487
Iteration 61/1000 | Loss: 0.00002487
Iteration 62/1000 | Loss: 0.00002487
Iteration 63/1000 | Loss: 0.00002486
Iteration 64/1000 | Loss: 0.00002485
Iteration 65/1000 | Loss: 0.00002484
Iteration 66/1000 | Loss: 0.00002484
Iteration 67/1000 | Loss: 0.00002484
Iteration 68/1000 | Loss: 0.00002483
Iteration 69/1000 | Loss: 0.00002483
Iteration 70/1000 | Loss: 0.00002483
Iteration 71/1000 | Loss: 0.00002483
Iteration 72/1000 | Loss: 0.00002483
Iteration 73/1000 | Loss: 0.00002483
Iteration 74/1000 | Loss: 0.00002482
Iteration 75/1000 | Loss: 0.00002482
Iteration 76/1000 | Loss: 0.00002482
Iteration 77/1000 | Loss: 0.00002481
Iteration 78/1000 | Loss: 0.00002480
Iteration 79/1000 | Loss: 0.00002480
Iteration 80/1000 | Loss: 0.00002480
Iteration 81/1000 | Loss: 0.00002479
Iteration 82/1000 | Loss: 0.00002479
Iteration 83/1000 | Loss: 0.00002479
Iteration 84/1000 | Loss: 0.00002478
Iteration 85/1000 | Loss: 0.00002478
Iteration 86/1000 | Loss: 0.00002478
Iteration 87/1000 | Loss: 0.00002478
Iteration 88/1000 | Loss: 0.00002477
Iteration 89/1000 | Loss: 0.00002477
Iteration 90/1000 | Loss: 0.00002477
Iteration 91/1000 | Loss: 0.00002476
Iteration 92/1000 | Loss: 0.00002476
Iteration 93/1000 | Loss: 0.00002476
Iteration 94/1000 | Loss: 0.00002476
Iteration 95/1000 | Loss: 0.00002476
Iteration 96/1000 | Loss: 0.00002476
Iteration 97/1000 | Loss: 0.00002476
Iteration 98/1000 | Loss: 0.00002475
Iteration 99/1000 | Loss: 0.00002475
Iteration 100/1000 | Loss: 0.00002475
Iteration 101/1000 | Loss: 0.00002475
Iteration 102/1000 | Loss: 0.00002475
Iteration 103/1000 | Loss: 0.00002474
Iteration 104/1000 | Loss: 0.00002474
Iteration 105/1000 | Loss: 0.00002474
Iteration 106/1000 | Loss: 0.00002474
Iteration 107/1000 | Loss: 0.00002473
Iteration 108/1000 | Loss: 0.00002473
Iteration 109/1000 | Loss: 0.00002473
Iteration 110/1000 | Loss: 0.00002473
Iteration 111/1000 | Loss: 0.00002473
Iteration 112/1000 | Loss: 0.00002472
Iteration 113/1000 | Loss: 0.00002472
Iteration 114/1000 | Loss: 0.00002472
Iteration 115/1000 | Loss: 0.00002472
Iteration 116/1000 | Loss: 0.00002472
Iteration 117/1000 | Loss: 0.00002472
Iteration 118/1000 | Loss: 0.00002472
Iteration 119/1000 | Loss: 0.00002472
Iteration 120/1000 | Loss: 0.00002472
Iteration 121/1000 | Loss: 0.00002472
Iteration 122/1000 | Loss: 0.00002471
Iteration 123/1000 | Loss: 0.00002471
Iteration 124/1000 | Loss: 0.00002471
Iteration 125/1000 | Loss: 0.00002471
Iteration 126/1000 | Loss: 0.00002471
Iteration 127/1000 | Loss: 0.00002471
Iteration 128/1000 | Loss: 0.00002471
Iteration 129/1000 | Loss: 0.00002471
Iteration 130/1000 | Loss: 0.00002471
Iteration 131/1000 | Loss: 0.00002471
Iteration 132/1000 | Loss: 0.00002471
Iteration 133/1000 | Loss: 0.00002471
Iteration 134/1000 | Loss: 0.00002471
Iteration 135/1000 | Loss: 0.00002471
Iteration 136/1000 | Loss: 0.00002470
Iteration 137/1000 | Loss: 0.00002470
Iteration 138/1000 | Loss: 0.00002470
Iteration 139/1000 | Loss: 0.00002470
Iteration 140/1000 | Loss: 0.00002470
Iteration 141/1000 | Loss: 0.00002470
Iteration 142/1000 | Loss: 0.00002470
Iteration 143/1000 | Loss: 0.00002470
Iteration 144/1000 | Loss: 0.00002470
Iteration 145/1000 | Loss: 0.00002470
Iteration 146/1000 | Loss: 0.00002470
Iteration 147/1000 | Loss: 0.00002470
Iteration 148/1000 | Loss: 0.00002470
Iteration 149/1000 | Loss: 0.00002470
Iteration 150/1000 | Loss: 0.00002470
Iteration 151/1000 | Loss: 0.00002470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.4702756491024047e-05, 2.4702756491024047e-05, 2.4702756491024047e-05, 2.4702756491024047e-05, 2.4702756491024047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4702756491024047e-05

Optimization complete. Final v2v error: 4.105238914489746 mm

Highest mean error: 4.457196235656738 mm for frame 189

Lowest mean error: 3.3214914798736572 mm for frame 237

Saving results

Total time: 47.25835561752319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057638
Iteration 2/25 | Loss: 0.00212095
Iteration 3/25 | Loss: 0.00173000
Iteration 4/25 | Loss: 0.00132028
Iteration 5/25 | Loss: 0.00132577
Iteration 6/25 | Loss: 0.00126824
Iteration 7/25 | Loss: 0.00126070
Iteration 8/25 | Loss: 0.00124897
Iteration 9/25 | Loss: 0.00124362
Iteration 10/25 | Loss: 0.00124544
Iteration 11/25 | Loss: 0.00124186
Iteration 12/25 | Loss: 0.00124155
Iteration 13/25 | Loss: 0.00124207
Iteration 14/25 | Loss: 0.00124138
Iteration 15/25 | Loss: 0.00124208
Iteration 16/25 | Loss: 0.00124136
Iteration 17/25 | Loss: 0.00124125
Iteration 18/25 | Loss: 0.00124122
Iteration 19/25 | Loss: 0.00124121
Iteration 20/25 | Loss: 0.00124121
Iteration 21/25 | Loss: 0.00124121
Iteration 22/25 | Loss: 0.00124121
Iteration 23/25 | Loss: 0.00124121
Iteration 24/25 | Loss: 0.00124121
Iteration 25/25 | Loss: 0.00124121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.95973635
Iteration 2/25 | Loss: 0.00093080
Iteration 3/25 | Loss: 0.00084515
Iteration 4/25 | Loss: 0.00084515
Iteration 5/25 | Loss: 0.00084515
Iteration 6/25 | Loss: 0.00084515
Iteration 7/25 | Loss: 0.00084515
Iteration 8/25 | Loss: 0.00084515
Iteration 9/25 | Loss: 0.00084515
Iteration 10/25 | Loss: 0.00084515
Iteration 11/25 | Loss: 0.00084515
Iteration 12/25 | Loss: 0.00084515
Iteration 13/25 | Loss: 0.00084515
Iteration 14/25 | Loss: 0.00084515
Iteration 15/25 | Loss: 0.00084515
Iteration 16/25 | Loss: 0.00084515
Iteration 17/25 | Loss: 0.00084515
Iteration 18/25 | Loss: 0.00084515
Iteration 19/25 | Loss: 0.00084515
Iteration 20/25 | Loss: 0.00084515
Iteration 21/25 | Loss: 0.00084515
Iteration 22/25 | Loss: 0.00084515
Iteration 23/25 | Loss: 0.00084515
Iteration 24/25 | Loss: 0.00084515
Iteration 25/25 | Loss: 0.00084515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084515
Iteration 2/1000 | Loss: 0.00009641
Iteration 3/1000 | Loss: 0.00008538
Iteration 4/1000 | Loss: 0.00013464
Iteration 5/1000 | Loss: 0.00003162
Iteration 6/1000 | Loss: 0.00004717
Iteration 7/1000 | Loss: 0.00003000
Iteration 8/1000 | Loss: 0.00003167
Iteration 9/1000 | Loss: 0.00010719
Iteration 10/1000 | Loss: 0.00004416
Iteration 11/1000 | Loss: 0.00002747
Iteration 12/1000 | Loss: 0.00002710
Iteration 13/1000 | Loss: 0.00002659
Iteration 14/1000 | Loss: 0.00037250
Iteration 15/1000 | Loss: 0.00019981
Iteration 16/1000 | Loss: 0.00003583
Iteration 17/1000 | Loss: 0.00005383
Iteration 18/1000 | Loss: 0.00002550
Iteration 19/1000 | Loss: 0.00007186
Iteration 20/1000 | Loss: 0.00002636
Iteration 21/1000 | Loss: 0.00005523
Iteration 22/1000 | Loss: 0.00002378
Iteration 23/1000 | Loss: 0.00002351
Iteration 24/1000 | Loss: 0.00002334
Iteration 25/1000 | Loss: 0.00002325
Iteration 26/1000 | Loss: 0.00002319
Iteration 27/1000 | Loss: 0.00002303
Iteration 28/1000 | Loss: 0.00002302
Iteration 29/1000 | Loss: 0.00006517
Iteration 30/1000 | Loss: 0.00002290
Iteration 31/1000 | Loss: 0.00003801
Iteration 32/1000 | Loss: 0.00002659
Iteration 33/1000 | Loss: 0.00003847
Iteration 34/1000 | Loss: 0.00002287
Iteration 35/1000 | Loss: 0.00002276
Iteration 36/1000 | Loss: 0.00003502
Iteration 37/1000 | Loss: 0.00002447
Iteration 38/1000 | Loss: 0.00002274
Iteration 39/1000 | Loss: 0.00002504
Iteration 40/1000 | Loss: 0.00002613
Iteration 41/1000 | Loss: 0.00002272
Iteration 42/1000 | Loss: 0.00002272
Iteration 43/1000 | Loss: 0.00002272
Iteration 44/1000 | Loss: 0.00002272
Iteration 45/1000 | Loss: 0.00002272
Iteration 46/1000 | Loss: 0.00002272
Iteration 47/1000 | Loss: 0.00002272
Iteration 48/1000 | Loss: 0.00002457
Iteration 49/1000 | Loss: 0.00002657
Iteration 50/1000 | Loss: 0.00002453
Iteration 51/1000 | Loss: 0.00002264
Iteration 52/1000 | Loss: 0.00002264
Iteration 53/1000 | Loss: 0.00002264
Iteration 54/1000 | Loss: 0.00002264
Iteration 55/1000 | Loss: 0.00002264
Iteration 56/1000 | Loss: 0.00002264
Iteration 57/1000 | Loss: 0.00002264
Iteration 58/1000 | Loss: 0.00002264
Iteration 59/1000 | Loss: 0.00002264
Iteration 60/1000 | Loss: 0.00002264
Iteration 61/1000 | Loss: 0.00002263
Iteration 62/1000 | Loss: 0.00002263
Iteration 63/1000 | Loss: 0.00002263
Iteration 64/1000 | Loss: 0.00002263
Iteration 65/1000 | Loss: 0.00002262
Iteration 66/1000 | Loss: 0.00002262
Iteration 67/1000 | Loss: 0.00002262
Iteration 68/1000 | Loss: 0.00002262
Iteration 69/1000 | Loss: 0.00002262
Iteration 70/1000 | Loss: 0.00002262
Iteration 71/1000 | Loss: 0.00002262
Iteration 72/1000 | Loss: 0.00002262
Iteration 73/1000 | Loss: 0.00002262
Iteration 74/1000 | Loss: 0.00002262
Iteration 75/1000 | Loss: 0.00002262
Iteration 76/1000 | Loss: 0.00002262
Iteration 77/1000 | Loss: 0.00002262
Iteration 78/1000 | Loss: 0.00002262
Iteration 79/1000 | Loss: 0.00002262
Iteration 80/1000 | Loss: 0.00002262
Iteration 81/1000 | Loss: 0.00002262
Iteration 82/1000 | Loss: 0.00002262
Iteration 83/1000 | Loss: 0.00002262
Iteration 84/1000 | Loss: 0.00002262
Iteration 85/1000 | Loss: 0.00002262
Iteration 86/1000 | Loss: 0.00002262
Iteration 87/1000 | Loss: 0.00002262
Iteration 88/1000 | Loss: 0.00002262
Iteration 89/1000 | Loss: 0.00002262
Iteration 90/1000 | Loss: 0.00002262
Iteration 91/1000 | Loss: 0.00002262
Iteration 92/1000 | Loss: 0.00002262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.2620821255259216e-05, 2.2620821255259216e-05, 2.2620821255259216e-05, 2.2620821255259216e-05, 2.2620821255259216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2620821255259216e-05

Optimization complete. Final v2v error: 4.067995071411133 mm

Highest mean error: 4.591644763946533 mm for frame 92

Lowest mean error: 3.8427538871765137 mm for frame 109

Saving results

Total time: 84.2875862121582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841833
Iteration 2/25 | Loss: 0.00132646
Iteration 3/25 | Loss: 0.00122682
Iteration 4/25 | Loss: 0.00121563
Iteration 5/25 | Loss: 0.00121317
Iteration 6/25 | Loss: 0.00121317
Iteration 7/25 | Loss: 0.00121317
Iteration 8/25 | Loss: 0.00121317
Iteration 9/25 | Loss: 0.00121317
Iteration 10/25 | Loss: 0.00121317
Iteration 11/25 | Loss: 0.00121317
Iteration 12/25 | Loss: 0.00121317
Iteration 13/25 | Loss: 0.00121317
Iteration 14/25 | Loss: 0.00121317
Iteration 15/25 | Loss: 0.00121317
Iteration 16/25 | Loss: 0.00121317
Iteration 17/25 | Loss: 0.00121317
Iteration 18/25 | Loss: 0.00121317
Iteration 19/25 | Loss: 0.00121317
Iteration 20/25 | Loss: 0.00121317
Iteration 21/25 | Loss: 0.00121317
Iteration 22/25 | Loss: 0.00121317
Iteration 23/25 | Loss: 0.00121317
Iteration 24/25 | Loss: 0.00121317
Iteration 25/25 | Loss: 0.00121317

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.55351877
Iteration 2/25 | Loss: 0.00075219
Iteration 3/25 | Loss: 0.00075217
Iteration 4/25 | Loss: 0.00075217
Iteration 5/25 | Loss: 0.00075217
Iteration 6/25 | Loss: 0.00075217
Iteration 7/25 | Loss: 0.00075217
Iteration 8/25 | Loss: 0.00075217
Iteration 9/25 | Loss: 0.00075217
Iteration 10/25 | Loss: 0.00075217
Iteration 11/25 | Loss: 0.00075217
Iteration 12/25 | Loss: 0.00075217
Iteration 13/25 | Loss: 0.00075217
Iteration 14/25 | Loss: 0.00075217
Iteration 15/25 | Loss: 0.00075217
Iteration 16/25 | Loss: 0.00075217
Iteration 17/25 | Loss: 0.00075217
Iteration 18/25 | Loss: 0.00075217
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007521689403802156, 0.0007521689403802156, 0.0007521689403802156, 0.0007521689403802156, 0.0007521689403802156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007521689403802156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075217
Iteration 2/1000 | Loss: 0.00003061
Iteration 3/1000 | Loss: 0.00002329
Iteration 4/1000 | Loss: 0.00002041
Iteration 5/1000 | Loss: 0.00001909
Iteration 6/1000 | Loss: 0.00001814
Iteration 7/1000 | Loss: 0.00001750
Iteration 8/1000 | Loss: 0.00001712
Iteration 9/1000 | Loss: 0.00001676
Iteration 10/1000 | Loss: 0.00001652
Iteration 11/1000 | Loss: 0.00001630
Iteration 12/1000 | Loss: 0.00001626
Iteration 13/1000 | Loss: 0.00001611
Iteration 14/1000 | Loss: 0.00001609
Iteration 15/1000 | Loss: 0.00001603
Iteration 16/1000 | Loss: 0.00001603
Iteration 17/1000 | Loss: 0.00001598
Iteration 18/1000 | Loss: 0.00001598
Iteration 19/1000 | Loss: 0.00001597
Iteration 20/1000 | Loss: 0.00001597
Iteration 21/1000 | Loss: 0.00001595
Iteration 22/1000 | Loss: 0.00001595
Iteration 23/1000 | Loss: 0.00001594
Iteration 24/1000 | Loss: 0.00001594
Iteration 25/1000 | Loss: 0.00001591
Iteration 26/1000 | Loss: 0.00001588
Iteration 27/1000 | Loss: 0.00001587
Iteration 28/1000 | Loss: 0.00001586
Iteration 29/1000 | Loss: 0.00001585
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001581
Iteration 32/1000 | Loss: 0.00001575
Iteration 33/1000 | Loss: 0.00001575
Iteration 34/1000 | Loss: 0.00001574
Iteration 35/1000 | Loss: 0.00001573
Iteration 36/1000 | Loss: 0.00001573
Iteration 37/1000 | Loss: 0.00001572
Iteration 38/1000 | Loss: 0.00001572
Iteration 39/1000 | Loss: 0.00001572
Iteration 40/1000 | Loss: 0.00001572
Iteration 41/1000 | Loss: 0.00001572
Iteration 42/1000 | Loss: 0.00001572
Iteration 43/1000 | Loss: 0.00001572
Iteration 44/1000 | Loss: 0.00001572
Iteration 45/1000 | Loss: 0.00001572
Iteration 46/1000 | Loss: 0.00001572
Iteration 47/1000 | Loss: 0.00001572
Iteration 48/1000 | Loss: 0.00001572
Iteration 49/1000 | Loss: 0.00001572
Iteration 50/1000 | Loss: 0.00001571
Iteration 51/1000 | Loss: 0.00001571
Iteration 52/1000 | Loss: 0.00001571
Iteration 53/1000 | Loss: 0.00001571
Iteration 54/1000 | Loss: 0.00001571
Iteration 55/1000 | Loss: 0.00001571
Iteration 56/1000 | Loss: 0.00001571
Iteration 57/1000 | Loss: 0.00001570
Iteration 58/1000 | Loss: 0.00001570
Iteration 59/1000 | Loss: 0.00001569
Iteration 60/1000 | Loss: 0.00001568
Iteration 61/1000 | Loss: 0.00001568
Iteration 62/1000 | Loss: 0.00001567
Iteration 63/1000 | Loss: 0.00001566
Iteration 64/1000 | Loss: 0.00001566
Iteration 65/1000 | Loss: 0.00001565
Iteration 66/1000 | Loss: 0.00001564
Iteration 67/1000 | Loss: 0.00001563
Iteration 68/1000 | Loss: 0.00001563
Iteration 69/1000 | Loss: 0.00001563
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001562
Iteration 72/1000 | Loss: 0.00001560
Iteration 73/1000 | Loss: 0.00001560
Iteration 74/1000 | Loss: 0.00001558
Iteration 75/1000 | Loss: 0.00001558
Iteration 76/1000 | Loss: 0.00001557
Iteration 77/1000 | Loss: 0.00001557
Iteration 78/1000 | Loss: 0.00001556
Iteration 79/1000 | Loss: 0.00001556
Iteration 80/1000 | Loss: 0.00001556
Iteration 81/1000 | Loss: 0.00001556
Iteration 82/1000 | Loss: 0.00001555
Iteration 83/1000 | Loss: 0.00001554
Iteration 84/1000 | Loss: 0.00001554
Iteration 85/1000 | Loss: 0.00001554
Iteration 86/1000 | Loss: 0.00001554
Iteration 87/1000 | Loss: 0.00001554
Iteration 88/1000 | Loss: 0.00001554
Iteration 89/1000 | Loss: 0.00001554
Iteration 90/1000 | Loss: 0.00001554
Iteration 91/1000 | Loss: 0.00001554
Iteration 92/1000 | Loss: 0.00001554
Iteration 93/1000 | Loss: 0.00001553
Iteration 94/1000 | Loss: 0.00001553
Iteration 95/1000 | Loss: 0.00001553
Iteration 96/1000 | Loss: 0.00001553
Iteration 97/1000 | Loss: 0.00001553
Iteration 98/1000 | Loss: 0.00001552
Iteration 99/1000 | Loss: 0.00001552
Iteration 100/1000 | Loss: 0.00001551
Iteration 101/1000 | Loss: 0.00001551
Iteration 102/1000 | Loss: 0.00001551
Iteration 103/1000 | Loss: 0.00001550
Iteration 104/1000 | Loss: 0.00001549
Iteration 105/1000 | Loss: 0.00001549
Iteration 106/1000 | Loss: 0.00001549
Iteration 107/1000 | Loss: 0.00001549
Iteration 108/1000 | Loss: 0.00001549
Iteration 109/1000 | Loss: 0.00001549
Iteration 110/1000 | Loss: 0.00001549
Iteration 111/1000 | Loss: 0.00001549
Iteration 112/1000 | Loss: 0.00001549
Iteration 113/1000 | Loss: 0.00001549
Iteration 114/1000 | Loss: 0.00001549
Iteration 115/1000 | Loss: 0.00001548
Iteration 116/1000 | Loss: 0.00001548
Iteration 117/1000 | Loss: 0.00001548
Iteration 118/1000 | Loss: 0.00001548
Iteration 119/1000 | Loss: 0.00001548
Iteration 120/1000 | Loss: 0.00001547
Iteration 121/1000 | Loss: 0.00001547
Iteration 122/1000 | Loss: 0.00001547
Iteration 123/1000 | Loss: 0.00001547
Iteration 124/1000 | Loss: 0.00001547
Iteration 125/1000 | Loss: 0.00001546
Iteration 126/1000 | Loss: 0.00001546
Iteration 127/1000 | Loss: 0.00001546
Iteration 128/1000 | Loss: 0.00001546
Iteration 129/1000 | Loss: 0.00001546
Iteration 130/1000 | Loss: 0.00001546
Iteration 131/1000 | Loss: 0.00001546
Iteration 132/1000 | Loss: 0.00001546
Iteration 133/1000 | Loss: 0.00001546
Iteration 134/1000 | Loss: 0.00001546
Iteration 135/1000 | Loss: 0.00001546
Iteration 136/1000 | Loss: 0.00001546
Iteration 137/1000 | Loss: 0.00001545
Iteration 138/1000 | Loss: 0.00001545
Iteration 139/1000 | Loss: 0.00001545
Iteration 140/1000 | Loss: 0.00001545
Iteration 141/1000 | Loss: 0.00001545
Iteration 142/1000 | Loss: 0.00001544
Iteration 143/1000 | Loss: 0.00001544
Iteration 144/1000 | Loss: 0.00001544
Iteration 145/1000 | Loss: 0.00001544
Iteration 146/1000 | Loss: 0.00001544
Iteration 147/1000 | Loss: 0.00001544
Iteration 148/1000 | Loss: 0.00001544
Iteration 149/1000 | Loss: 0.00001543
Iteration 150/1000 | Loss: 0.00001543
Iteration 151/1000 | Loss: 0.00001543
Iteration 152/1000 | Loss: 0.00001543
Iteration 153/1000 | Loss: 0.00001543
Iteration 154/1000 | Loss: 0.00001543
Iteration 155/1000 | Loss: 0.00001543
Iteration 156/1000 | Loss: 0.00001543
Iteration 157/1000 | Loss: 0.00001543
Iteration 158/1000 | Loss: 0.00001543
Iteration 159/1000 | Loss: 0.00001543
Iteration 160/1000 | Loss: 0.00001543
Iteration 161/1000 | Loss: 0.00001543
Iteration 162/1000 | Loss: 0.00001542
Iteration 163/1000 | Loss: 0.00001542
Iteration 164/1000 | Loss: 0.00001542
Iteration 165/1000 | Loss: 0.00001542
Iteration 166/1000 | Loss: 0.00001542
Iteration 167/1000 | Loss: 0.00001542
Iteration 168/1000 | Loss: 0.00001542
Iteration 169/1000 | Loss: 0.00001542
Iteration 170/1000 | Loss: 0.00001542
Iteration 171/1000 | Loss: 0.00001542
Iteration 172/1000 | Loss: 0.00001542
Iteration 173/1000 | Loss: 0.00001542
Iteration 174/1000 | Loss: 0.00001542
Iteration 175/1000 | Loss: 0.00001541
Iteration 176/1000 | Loss: 0.00001541
Iteration 177/1000 | Loss: 0.00001541
Iteration 178/1000 | Loss: 0.00001541
Iteration 179/1000 | Loss: 0.00001541
Iteration 180/1000 | Loss: 0.00001541
Iteration 181/1000 | Loss: 0.00001541
Iteration 182/1000 | Loss: 0.00001541
Iteration 183/1000 | Loss: 0.00001541
Iteration 184/1000 | Loss: 0.00001541
Iteration 185/1000 | Loss: 0.00001541
Iteration 186/1000 | Loss: 0.00001541
Iteration 187/1000 | Loss: 0.00001541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.541425626783166e-05, 1.541425626783166e-05, 1.541425626783166e-05, 1.541425626783166e-05, 1.541425626783166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.541425626783166e-05

Optimization complete. Final v2v error: 3.3339877128601074 mm

Highest mean error: 3.699446439743042 mm for frame 111

Lowest mean error: 2.9545044898986816 mm for frame 167

Saving results

Total time: 41.28749227523804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771098
Iteration 2/25 | Loss: 0.00147568
Iteration 3/25 | Loss: 0.00124984
Iteration 4/25 | Loss: 0.00123310
Iteration 5/25 | Loss: 0.00122980
Iteration 6/25 | Loss: 0.00122977
Iteration 7/25 | Loss: 0.00122977
Iteration 8/25 | Loss: 0.00122977
Iteration 9/25 | Loss: 0.00122977
Iteration 10/25 | Loss: 0.00122977
Iteration 11/25 | Loss: 0.00122977
Iteration 12/25 | Loss: 0.00122977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012297721114009619, 0.0012297721114009619, 0.0012297721114009619, 0.0012297721114009619, 0.0012297721114009619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012297721114009619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41992724
Iteration 2/25 | Loss: 0.00054253
Iteration 3/25 | Loss: 0.00054252
Iteration 4/25 | Loss: 0.00054252
Iteration 5/25 | Loss: 0.00054252
Iteration 6/25 | Loss: 0.00054252
Iteration 7/25 | Loss: 0.00054252
Iteration 8/25 | Loss: 0.00054252
Iteration 9/25 | Loss: 0.00054252
Iteration 10/25 | Loss: 0.00054252
Iteration 11/25 | Loss: 0.00054252
Iteration 12/25 | Loss: 0.00054252
Iteration 13/25 | Loss: 0.00054252
Iteration 14/25 | Loss: 0.00054252
Iteration 15/25 | Loss: 0.00054252
Iteration 16/25 | Loss: 0.00054252
Iteration 17/25 | Loss: 0.00054252
Iteration 18/25 | Loss: 0.00054252
Iteration 19/25 | Loss: 0.00054252
Iteration 20/25 | Loss: 0.00054252
Iteration 21/25 | Loss: 0.00054252
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005425206036306918, 0.0005425206036306918, 0.0005425206036306918, 0.0005425206036306918, 0.0005425206036306918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005425206036306918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054252
Iteration 2/1000 | Loss: 0.00003292
Iteration 3/1000 | Loss: 0.00002485
Iteration 4/1000 | Loss: 0.00002174
Iteration 5/1000 | Loss: 0.00002012
Iteration 6/1000 | Loss: 0.00001899
Iteration 7/1000 | Loss: 0.00001821
Iteration 8/1000 | Loss: 0.00001763
Iteration 9/1000 | Loss: 0.00001707
Iteration 10/1000 | Loss: 0.00001678
Iteration 11/1000 | Loss: 0.00001659
Iteration 12/1000 | Loss: 0.00001639
Iteration 13/1000 | Loss: 0.00001637
Iteration 14/1000 | Loss: 0.00001623
Iteration 15/1000 | Loss: 0.00001618
Iteration 16/1000 | Loss: 0.00001613
Iteration 17/1000 | Loss: 0.00001606
Iteration 18/1000 | Loss: 0.00001604
Iteration 19/1000 | Loss: 0.00001604
Iteration 20/1000 | Loss: 0.00001604
Iteration 21/1000 | Loss: 0.00001602
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001597
Iteration 24/1000 | Loss: 0.00001593
Iteration 25/1000 | Loss: 0.00001592
Iteration 26/1000 | Loss: 0.00001590
Iteration 27/1000 | Loss: 0.00001589
Iteration 28/1000 | Loss: 0.00001588
Iteration 29/1000 | Loss: 0.00001587
Iteration 30/1000 | Loss: 0.00001585
Iteration 31/1000 | Loss: 0.00001584
Iteration 32/1000 | Loss: 0.00001584
Iteration 33/1000 | Loss: 0.00001584
Iteration 34/1000 | Loss: 0.00001583
Iteration 35/1000 | Loss: 0.00001583
Iteration 36/1000 | Loss: 0.00001583
Iteration 37/1000 | Loss: 0.00001582
Iteration 38/1000 | Loss: 0.00001578
Iteration 39/1000 | Loss: 0.00001578
Iteration 40/1000 | Loss: 0.00001578
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001577
Iteration 45/1000 | Loss: 0.00001576
Iteration 46/1000 | Loss: 0.00001575
Iteration 47/1000 | Loss: 0.00001575
Iteration 48/1000 | Loss: 0.00001575
Iteration 49/1000 | Loss: 0.00001575
Iteration 50/1000 | Loss: 0.00001575
Iteration 51/1000 | Loss: 0.00001575
Iteration 52/1000 | Loss: 0.00001575
Iteration 53/1000 | Loss: 0.00001575
Iteration 54/1000 | Loss: 0.00001574
Iteration 55/1000 | Loss: 0.00001574
Iteration 56/1000 | Loss: 0.00001572
Iteration 57/1000 | Loss: 0.00001572
Iteration 58/1000 | Loss: 0.00001571
Iteration 59/1000 | Loss: 0.00001571
Iteration 60/1000 | Loss: 0.00001570
Iteration 61/1000 | Loss: 0.00001570
Iteration 62/1000 | Loss: 0.00001569
Iteration 63/1000 | Loss: 0.00001569
Iteration 64/1000 | Loss: 0.00001569
Iteration 65/1000 | Loss: 0.00001567
Iteration 66/1000 | Loss: 0.00001567
Iteration 67/1000 | Loss: 0.00001567
Iteration 68/1000 | Loss: 0.00001567
Iteration 69/1000 | Loss: 0.00001567
Iteration 70/1000 | Loss: 0.00001567
Iteration 71/1000 | Loss: 0.00001566
Iteration 72/1000 | Loss: 0.00001566
Iteration 73/1000 | Loss: 0.00001565
Iteration 74/1000 | Loss: 0.00001565
Iteration 75/1000 | Loss: 0.00001565
Iteration 76/1000 | Loss: 0.00001564
Iteration 77/1000 | Loss: 0.00001564
Iteration 78/1000 | Loss: 0.00001564
Iteration 79/1000 | Loss: 0.00001564
Iteration 80/1000 | Loss: 0.00001563
Iteration 81/1000 | Loss: 0.00001563
Iteration 82/1000 | Loss: 0.00001563
Iteration 83/1000 | Loss: 0.00001563
Iteration 84/1000 | Loss: 0.00001563
Iteration 85/1000 | Loss: 0.00001563
Iteration 86/1000 | Loss: 0.00001562
Iteration 87/1000 | Loss: 0.00001562
Iteration 88/1000 | Loss: 0.00001562
Iteration 89/1000 | Loss: 0.00001562
Iteration 90/1000 | Loss: 0.00001562
Iteration 91/1000 | Loss: 0.00001562
Iteration 92/1000 | Loss: 0.00001562
Iteration 93/1000 | Loss: 0.00001561
Iteration 94/1000 | Loss: 0.00001561
Iteration 95/1000 | Loss: 0.00001561
Iteration 96/1000 | Loss: 0.00001561
Iteration 97/1000 | Loss: 0.00001560
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001560
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001560
Iteration 102/1000 | Loss: 0.00001560
Iteration 103/1000 | Loss: 0.00001560
Iteration 104/1000 | Loss: 0.00001560
Iteration 105/1000 | Loss: 0.00001560
Iteration 106/1000 | Loss: 0.00001560
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001559
Iteration 111/1000 | Loss: 0.00001559
Iteration 112/1000 | Loss: 0.00001559
Iteration 113/1000 | Loss: 0.00001559
Iteration 114/1000 | Loss: 0.00001559
Iteration 115/1000 | Loss: 0.00001559
Iteration 116/1000 | Loss: 0.00001559
Iteration 117/1000 | Loss: 0.00001559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.5587191228405572e-05, 1.5587191228405572e-05, 1.5587191228405572e-05, 1.5587191228405572e-05, 1.5587191228405572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5587191228405572e-05

Optimization complete. Final v2v error: 3.336803913116455 mm

Highest mean error: 3.676940441131592 mm for frame 8

Lowest mean error: 3.218026638031006 mm for frame 121

Saving results

Total time: 42.99265646934509
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815507
Iteration 2/25 | Loss: 0.00156638
Iteration 3/25 | Loss: 0.00127301
Iteration 4/25 | Loss: 0.00125339
Iteration 5/25 | Loss: 0.00125099
Iteration 6/25 | Loss: 0.00125089
Iteration 7/25 | Loss: 0.00125089
Iteration 8/25 | Loss: 0.00125089
Iteration 9/25 | Loss: 0.00125089
Iteration 10/25 | Loss: 0.00125089
Iteration 11/25 | Loss: 0.00125089
Iteration 12/25 | Loss: 0.00125089
Iteration 13/25 | Loss: 0.00125089
Iteration 14/25 | Loss: 0.00125089
Iteration 15/25 | Loss: 0.00125089
Iteration 16/25 | Loss: 0.00125089
Iteration 17/25 | Loss: 0.00125089
Iteration 18/25 | Loss: 0.00125089
Iteration 19/25 | Loss: 0.00125089
Iteration 20/25 | Loss: 0.00125089
Iteration 21/25 | Loss: 0.00125089
Iteration 22/25 | Loss: 0.00125089
Iteration 23/25 | Loss: 0.00125089
Iteration 24/25 | Loss: 0.00125089
Iteration 25/25 | Loss: 0.00125089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40664482
Iteration 2/25 | Loss: 0.00056885
Iteration 3/25 | Loss: 0.00056884
Iteration 4/25 | Loss: 0.00056884
Iteration 5/25 | Loss: 0.00056884
Iteration 6/25 | Loss: 0.00056884
Iteration 7/25 | Loss: 0.00056884
Iteration 8/25 | Loss: 0.00056884
Iteration 9/25 | Loss: 0.00056884
Iteration 10/25 | Loss: 0.00056884
Iteration 11/25 | Loss: 0.00056884
Iteration 12/25 | Loss: 0.00056884
Iteration 13/25 | Loss: 0.00056884
Iteration 14/25 | Loss: 0.00056884
Iteration 15/25 | Loss: 0.00056884
Iteration 16/25 | Loss: 0.00056884
Iteration 17/25 | Loss: 0.00056884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005688394303433597, 0.0005688394303433597, 0.0005688394303433597, 0.0005688394303433597, 0.0005688394303433597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005688394303433597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056884
Iteration 2/1000 | Loss: 0.00003019
Iteration 3/1000 | Loss: 0.00002243
Iteration 4/1000 | Loss: 0.00002010
Iteration 5/1000 | Loss: 0.00001910
Iteration 6/1000 | Loss: 0.00001851
Iteration 7/1000 | Loss: 0.00001777
Iteration 8/1000 | Loss: 0.00001749
Iteration 9/1000 | Loss: 0.00001712
Iteration 10/1000 | Loss: 0.00001680
Iteration 11/1000 | Loss: 0.00001653
Iteration 12/1000 | Loss: 0.00001629
Iteration 13/1000 | Loss: 0.00001621
Iteration 14/1000 | Loss: 0.00001619
Iteration 15/1000 | Loss: 0.00001618
Iteration 16/1000 | Loss: 0.00001618
Iteration 17/1000 | Loss: 0.00001605
Iteration 18/1000 | Loss: 0.00001599
Iteration 19/1000 | Loss: 0.00001598
Iteration 20/1000 | Loss: 0.00001598
Iteration 21/1000 | Loss: 0.00001597
Iteration 22/1000 | Loss: 0.00001597
Iteration 23/1000 | Loss: 0.00001595
Iteration 24/1000 | Loss: 0.00001594
Iteration 25/1000 | Loss: 0.00001594
Iteration 26/1000 | Loss: 0.00001593
Iteration 27/1000 | Loss: 0.00001587
Iteration 28/1000 | Loss: 0.00001587
Iteration 29/1000 | Loss: 0.00001582
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001580
Iteration 32/1000 | Loss: 0.00001577
Iteration 33/1000 | Loss: 0.00001576
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001573
Iteration 36/1000 | Loss: 0.00001571
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001569
Iteration 39/1000 | Loss: 0.00001569
Iteration 40/1000 | Loss: 0.00001568
Iteration 41/1000 | Loss: 0.00001568
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001567
Iteration 44/1000 | Loss: 0.00001567
Iteration 45/1000 | Loss: 0.00001567
Iteration 46/1000 | Loss: 0.00001567
Iteration 47/1000 | Loss: 0.00001567
Iteration 48/1000 | Loss: 0.00001567
Iteration 49/1000 | Loss: 0.00001567
Iteration 50/1000 | Loss: 0.00001567
Iteration 51/1000 | Loss: 0.00001567
Iteration 52/1000 | Loss: 0.00001566
Iteration 53/1000 | Loss: 0.00001566
Iteration 54/1000 | Loss: 0.00001566
Iteration 55/1000 | Loss: 0.00001566
Iteration 56/1000 | Loss: 0.00001566
Iteration 57/1000 | Loss: 0.00001566
Iteration 58/1000 | Loss: 0.00001566
Iteration 59/1000 | Loss: 0.00001565
Iteration 60/1000 | Loss: 0.00001565
Iteration 61/1000 | Loss: 0.00001565
Iteration 62/1000 | Loss: 0.00001565
Iteration 63/1000 | Loss: 0.00001565
Iteration 64/1000 | Loss: 0.00001565
Iteration 65/1000 | Loss: 0.00001565
Iteration 66/1000 | Loss: 0.00001565
Iteration 67/1000 | Loss: 0.00001565
Iteration 68/1000 | Loss: 0.00001565
Iteration 69/1000 | Loss: 0.00001565
Iteration 70/1000 | Loss: 0.00001565
Iteration 71/1000 | Loss: 0.00001565
Iteration 72/1000 | Loss: 0.00001565
Iteration 73/1000 | Loss: 0.00001564
Iteration 74/1000 | Loss: 0.00001564
Iteration 75/1000 | Loss: 0.00001564
Iteration 76/1000 | Loss: 0.00001564
Iteration 77/1000 | Loss: 0.00001564
Iteration 78/1000 | Loss: 0.00001564
Iteration 79/1000 | Loss: 0.00001564
Iteration 80/1000 | Loss: 0.00001564
Iteration 81/1000 | Loss: 0.00001564
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001564
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001564
Iteration 86/1000 | Loss: 0.00001564
Iteration 87/1000 | Loss: 0.00001563
Iteration 88/1000 | Loss: 0.00001563
Iteration 89/1000 | Loss: 0.00001563
Iteration 90/1000 | Loss: 0.00001563
Iteration 91/1000 | Loss: 0.00001563
Iteration 92/1000 | Loss: 0.00001563
Iteration 93/1000 | Loss: 0.00001563
Iteration 94/1000 | Loss: 0.00001563
Iteration 95/1000 | Loss: 0.00001563
Iteration 96/1000 | Loss: 0.00001563
Iteration 97/1000 | Loss: 0.00001563
Iteration 98/1000 | Loss: 0.00001563
Iteration 99/1000 | Loss: 0.00001563
Iteration 100/1000 | Loss: 0.00001563
Iteration 101/1000 | Loss: 0.00001563
Iteration 102/1000 | Loss: 0.00001563
Iteration 103/1000 | Loss: 0.00001563
Iteration 104/1000 | Loss: 0.00001563
Iteration 105/1000 | Loss: 0.00001563
Iteration 106/1000 | Loss: 0.00001563
Iteration 107/1000 | Loss: 0.00001563
Iteration 108/1000 | Loss: 0.00001563
Iteration 109/1000 | Loss: 0.00001562
Iteration 110/1000 | Loss: 0.00001562
Iteration 111/1000 | Loss: 0.00001562
Iteration 112/1000 | Loss: 0.00001562
Iteration 113/1000 | Loss: 0.00001562
Iteration 114/1000 | Loss: 0.00001562
Iteration 115/1000 | Loss: 0.00001562
Iteration 116/1000 | Loss: 0.00001562
Iteration 117/1000 | Loss: 0.00001562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.562310353619978e-05, 1.562310353619978e-05, 1.562310353619978e-05, 1.562310353619978e-05, 1.562310353619978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.562310353619978e-05

Optimization complete. Final v2v error: 3.3234686851501465 mm

Highest mean error: 3.3815598487854004 mm for frame 99

Lowest mean error: 3.2713818550109863 mm for frame 141

Saving results

Total time: 35.11213994026184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813571
Iteration 2/25 | Loss: 0.00131131
Iteration 3/25 | Loss: 0.00120294
Iteration 4/25 | Loss: 0.00118731
Iteration 5/25 | Loss: 0.00118402
Iteration 6/25 | Loss: 0.00118394
Iteration 7/25 | Loss: 0.00118394
Iteration 8/25 | Loss: 0.00118394
Iteration 9/25 | Loss: 0.00118394
Iteration 10/25 | Loss: 0.00118394
Iteration 11/25 | Loss: 0.00118394
Iteration 12/25 | Loss: 0.00118394
Iteration 13/25 | Loss: 0.00118394
Iteration 14/25 | Loss: 0.00118394
Iteration 15/25 | Loss: 0.00118394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011839409125968814, 0.0011839409125968814, 0.0011839409125968814, 0.0011839409125968814, 0.0011839409125968814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011839409125968814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43713832
Iteration 2/25 | Loss: 0.00066796
Iteration 3/25 | Loss: 0.00066796
Iteration 4/25 | Loss: 0.00066795
Iteration 5/25 | Loss: 0.00066795
Iteration 6/25 | Loss: 0.00066795
Iteration 7/25 | Loss: 0.00066795
Iteration 8/25 | Loss: 0.00066795
Iteration 9/25 | Loss: 0.00066795
Iteration 10/25 | Loss: 0.00066795
Iteration 11/25 | Loss: 0.00066795
Iteration 12/25 | Loss: 0.00066795
Iteration 13/25 | Loss: 0.00066795
Iteration 14/25 | Loss: 0.00066795
Iteration 15/25 | Loss: 0.00066795
Iteration 16/25 | Loss: 0.00066795
Iteration 17/25 | Loss: 0.00066795
Iteration 18/25 | Loss: 0.00066795
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006679504294879735, 0.0006679504294879735, 0.0006679504294879735, 0.0006679504294879735, 0.0006679504294879735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006679504294879735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066795
Iteration 2/1000 | Loss: 0.00002659
Iteration 3/1000 | Loss: 0.00001834
Iteration 4/1000 | Loss: 0.00001555
Iteration 5/1000 | Loss: 0.00001433
Iteration 6/1000 | Loss: 0.00001325
Iteration 7/1000 | Loss: 0.00001270
Iteration 8/1000 | Loss: 0.00001241
Iteration 9/1000 | Loss: 0.00001214
Iteration 10/1000 | Loss: 0.00001205
Iteration 11/1000 | Loss: 0.00001200
Iteration 12/1000 | Loss: 0.00001195
Iteration 13/1000 | Loss: 0.00001190
Iteration 14/1000 | Loss: 0.00001190
Iteration 15/1000 | Loss: 0.00001190
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001179
Iteration 18/1000 | Loss: 0.00001176
Iteration 19/1000 | Loss: 0.00001176
Iteration 20/1000 | Loss: 0.00001174
Iteration 21/1000 | Loss: 0.00001173
Iteration 22/1000 | Loss: 0.00001173
Iteration 23/1000 | Loss: 0.00001172
Iteration 24/1000 | Loss: 0.00001171
Iteration 25/1000 | Loss: 0.00001171
Iteration 26/1000 | Loss: 0.00001171
Iteration 27/1000 | Loss: 0.00001170
Iteration 28/1000 | Loss: 0.00001169
Iteration 29/1000 | Loss: 0.00001169
Iteration 30/1000 | Loss: 0.00001168
Iteration 31/1000 | Loss: 0.00001167
Iteration 32/1000 | Loss: 0.00001167
Iteration 33/1000 | Loss: 0.00001167
Iteration 34/1000 | Loss: 0.00001167
Iteration 35/1000 | Loss: 0.00001167
Iteration 36/1000 | Loss: 0.00001166
Iteration 37/1000 | Loss: 0.00001166
Iteration 38/1000 | Loss: 0.00001166
Iteration 39/1000 | Loss: 0.00001166
Iteration 40/1000 | Loss: 0.00001166
Iteration 41/1000 | Loss: 0.00001166
Iteration 42/1000 | Loss: 0.00001164
Iteration 43/1000 | Loss: 0.00001164
Iteration 44/1000 | Loss: 0.00001163
Iteration 45/1000 | Loss: 0.00001163
Iteration 46/1000 | Loss: 0.00001163
Iteration 47/1000 | Loss: 0.00001162
Iteration 48/1000 | Loss: 0.00001162
Iteration 49/1000 | Loss: 0.00001161
Iteration 50/1000 | Loss: 0.00001161
Iteration 51/1000 | Loss: 0.00001161
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00001160
Iteration 54/1000 | Loss: 0.00001160
Iteration 55/1000 | Loss: 0.00001159
Iteration 56/1000 | Loss: 0.00001159
Iteration 57/1000 | Loss: 0.00001158
Iteration 58/1000 | Loss: 0.00001158
Iteration 59/1000 | Loss: 0.00001157
Iteration 60/1000 | Loss: 0.00001156
Iteration 61/1000 | Loss: 0.00001156
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001156
Iteration 64/1000 | Loss: 0.00001156
Iteration 65/1000 | Loss: 0.00001155
Iteration 66/1000 | Loss: 0.00001154
Iteration 67/1000 | Loss: 0.00001153
Iteration 68/1000 | Loss: 0.00001152
Iteration 69/1000 | Loss: 0.00001152
Iteration 70/1000 | Loss: 0.00001151
Iteration 71/1000 | Loss: 0.00001151
Iteration 72/1000 | Loss: 0.00001150
Iteration 73/1000 | Loss: 0.00001150
Iteration 74/1000 | Loss: 0.00001149
Iteration 75/1000 | Loss: 0.00001149
Iteration 76/1000 | Loss: 0.00001148
Iteration 77/1000 | Loss: 0.00001148
Iteration 78/1000 | Loss: 0.00001147
Iteration 79/1000 | Loss: 0.00001147
Iteration 80/1000 | Loss: 0.00001147
Iteration 81/1000 | Loss: 0.00001147
Iteration 82/1000 | Loss: 0.00001146
Iteration 83/1000 | Loss: 0.00001143
Iteration 84/1000 | Loss: 0.00001143
Iteration 85/1000 | Loss: 0.00001143
Iteration 86/1000 | Loss: 0.00001142
Iteration 87/1000 | Loss: 0.00001142
Iteration 88/1000 | Loss: 0.00001142
Iteration 89/1000 | Loss: 0.00001142
Iteration 90/1000 | Loss: 0.00001142
Iteration 91/1000 | Loss: 0.00001141
Iteration 92/1000 | Loss: 0.00001141
Iteration 93/1000 | Loss: 0.00001140
Iteration 94/1000 | Loss: 0.00001140
Iteration 95/1000 | Loss: 0.00001139
Iteration 96/1000 | Loss: 0.00001139
Iteration 97/1000 | Loss: 0.00001139
Iteration 98/1000 | Loss: 0.00001138
Iteration 99/1000 | Loss: 0.00001138
Iteration 100/1000 | Loss: 0.00001137
Iteration 101/1000 | Loss: 0.00001137
Iteration 102/1000 | Loss: 0.00001136
Iteration 103/1000 | Loss: 0.00001136
Iteration 104/1000 | Loss: 0.00001136
Iteration 105/1000 | Loss: 0.00001135
Iteration 106/1000 | Loss: 0.00001134
Iteration 107/1000 | Loss: 0.00001134
Iteration 108/1000 | Loss: 0.00001134
Iteration 109/1000 | Loss: 0.00001134
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001132
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001131
Iteration 115/1000 | Loss: 0.00001131
Iteration 116/1000 | Loss: 0.00001130
Iteration 117/1000 | Loss: 0.00001129
Iteration 118/1000 | Loss: 0.00001129
Iteration 119/1000 | Loss: 0.00001129
Iteration 120/1000 | Loss: 0.00001128
Iteration 121/1000 | Loss: 0.00001128
Iteration 122/1000 | Loss: 0.00001128
Iteration 123/1000 | Loss: 0.00001127
Iteration 124/1000 | Loss: 0.00001127
Iteration 125/1000 | Loss: 0.00001127
Iteration 126/1000 | Loss: 0.00001127
Iteration 127/1000 | Loss: 0.00001127
Iteration 128/1000 | Loss: 0.00001127
Iteration 129/1000 | Loss: 0.00001127
Iteration 130/1000 | Loss: 0.00001127
Iteration 131/1000 | Loss: 0.00001126
Iteration 132/1000 | Loss: 0.00001126
Iteration 133/1000 | Loss: 0.00001126
Iteration 134/1000 | Loss: 0.00001126
Iteration 135/1000 | Loss: 0.00001126
Iteration 136/1000 | Loss: 0.00001126
Iteration 137/1000 | Loss: 0.00001126
Iteration 138/1000 | Loss: 0.00001126
Iteration 139/1000 | Loss: 0.00001126
Iteration 140/1000 | Loss: 0.00001126
Iteration 141/1000 | Loss: 0.00001126
Iteration 142/1000 | Loss: 0.00001126
Iteration 143/1000 | Loss: 0.00001126
Iteration 144/1000 | Loss: 0.00001126
Iteration 145/1000 | Loss: 0.00001126
Iteration 146/1000 | Loss: 0.00001126
Iteration 147/1000 | Loss: 0.00001126
Iteration 148/1000 | Loss: 0.00001126
Iteration 149/1000 | Loss: 0.00001126
Iteration 150/1000 | Loss: 0.00001126
Iteration 151/1000 | Loss: 0.00001126
Iteration 152/1000 | Loss: 0.00001126
Iteration 153/1000 | Loss: 0.00001126
Iteration 154/1000 | Loss: 0.00001126
Iteration 155/1000 | Loss: 0.00001126
Iteration 156/1000 | Loss: 0.00001126
Iteration 157/1000 | Loss: 0.00001126
Iteration 158/1000 | Loss: 0.00001126
Iteration 159/1000 | Loss: 0.00001126
Iteration 160/1000 | Loss: 0.00001126
Iteration 161/1000 | Loss: 0.00001126
Iteration 162/1000 | Loss: 0.00001126
Iteration 163/1000 | Loss: 0.00001126
Iteration 164/1000 | Loss: 0.00001126
Iteration 165/1000 | Loss: 0.00001126
Iteration 166/1000 | Loss: 0.00001126
Iteration 167/1000 | Loss: 0.00001126
Iteration 168/1000 | Loss: 0.00001126
Iteration 169/1000 | Loss: 0.00001126
Iteration 170/1000 | Loss: 0.00001126
Iteration 171/1000 | Loss: 0.00001126
Iteration 172/1000 | Loss: 0.00001126
Iteration 173/1000 | Loss: 0.00001126
Iteration 174/1000 | Loss: 0.00001126
Iteration 175/1000 | Loss: 0.00001126
Iteration 176/1000 | Loss: 0.00001126
Iteration 177/1000 | Loss: 0.00001126
Iteration 178/1000 | Loss: 0.00001126
Iteration 179/1000 | Loss: 0.00001126
Iteration 180/1000 | Loss: 0.00001126
Iteration 181/1000 | Loss: 0.00001126
Iteration 182/1000 | Loss: 0.00001126
Iteration 183/1000 | Loss: 0.00001126
Iteration 184/1000 | Loss: 0.00001126
Iteration 185/1000 | Loss: 0.00001126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.1256563993811142e-05, 1.1256563993811142e-05, 1.1256563993811142e-05, 1.1256563993811142e-05, 1.1256563993811142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1256563993811142e-05

Optimization complete. Final v2v error: 2.8843142986297607 mm

Highest mean error: 3.1365132331848145 mm for frame 59

Lowest mean error: 2.673362970352173 mm for frame 206

Saving results

Total time: 41.482431173324585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820024
Iteration 2/25 | Loss: 0.00125450
Iteration 3/25 | Loss: 0.00118690
Iteration 4/25 | Loss: 0.00116707
Iteration 5/25 | Loss: 0.00116141
Iteration 6/25 | Loss: 0.00116078
Iteration 7/25 | Loss: 0.00116078
Iteration 8/25 | Loss: 0.00116078
Iteration 9/25 | Loss: 0.00116078
Iteration 10/25 | Loss: 0.00116078
Iteration 11/25 | Loss: 0.00116078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011607793858274817, 0.0011607793858274817, 0.0011607793858274817, 0.0011607793858274817, 0.0011607793858274817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011607793858274817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.34442616
Iteration 2/25 | Loss: 0.00084115
Iteration 3/25 | Loss: 0.00084115
Iteration 4/25 | Loss: 0.00084115
Iteration 5/25 | Loss: 0.00084115
Iteration 6/25 | Loss: 0.00084115
Iteration 7/25 | Loss: 0.00084115
Iteration 8/25 | Loss: 0.00084115
Iteration 9/25 | Loss: 0.00084115
Iteration 10/25 | Loss: 0.00084115
Iteration 11/25 | Loss: 0.00084115
Iteration 12/25 | Loss: 0.00084115
Iteration 13/25 | Loss: 0.00084115
Iteration 14/25 | Loss: 0.00084115
Iteration 15/25 | Loss: 0.00084115
Iteration 16/25 | Loss: 0.00084115
Iteration 17/25 | Loss: 0.00084115
Iteration 18/25 | Loss: 0.00084115
Iteration 19/25 | Loss: 0.00084115
Iteration 20/25 | Loss: 0.00084115
Iteration 21/25 | Loss: 0.00084115
Iteration 22/25 | Loss: 0.00084115
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008411513990722597, 0.0008411513990722597, 0.0008411513990722597, 0.0008411513990722597, 0.0008411513990722597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008411513990722597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084115
Iteration 2/1000 | Loss: 0.00003017
Iteration 3/1000 | Loss: 0.00001912
Iteration 4/1000 | Loss: 0.00001705
Iteration 5/1000 | Loss: 0.00001585
Iteration 6/1000 | Loss: 0.00001497
Iteration 7/1000 | Loss: 0.00001439
Iteration 8/1000 | Loss: 0.00001407
Iteration 9/1000 | Loss: 0.00001366
Iteration 10/1000 | Loss: 0.00001345
Iteration 11/1000 | Loss: 0.00001343
Iteration 12/1000 | Loss: 0.00001342
Iteration 13/1000 | Loss: 0.00001341
Iteration 14/1000 | Loss: 0.00001340
Iteration 15/1000 | Loss: 0.00001323
Iteration 16/1000 | Loss: 0.00001310
Iteration 17/1000 | Loss: 0.00001306
Iteration 18/1000 | Loss: 0.00001303
Iteration 19/1000 | Loss: 0.00001303
Iteration 20/1000 | Loss: 0.00001302
Iteration 21/1000 | Loss: 0.00001302
Iteration 22/1000 | Loss: 0.00001300
Iteration 23/1000 | Loss: 0.00001299
Iteration 24/1000 | Loss: 0.00001299
Iteration 25/1000 | Loss: 0.00001299
Iteration 26/1000 | Loss: 0.00001299
Iteration 27/1000 | Loss: 0.00001298
Iteration 28/1000 | Loss: 0.00001298
Iteration 29/1000 | Loss: 0.00001298
Iteration 30/1000 | Loss: 0.00001297
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001296
Iteration 33/1000 | Loss: 0.00001295
Iteration 34/1000 | Loss: 0.00001295
Iteration 35/1000 | Loss: 0.00001295
Iteration 36/1000 | Loss: 0.00001294
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001293
Iteration 41/1000 | Loss: 0.00001293
Iteration 42/1000 | Loss: 0.00001293
Iteration 43/1000 | Loss: 0.00001292
Iteration 44/1000 | Loss: 0.00001292
Iteration 45/1000 | Loss: 0.00001292
Iteration 46/1000 | Loss: 0.00001292
Iteration 47/1000 | Loss: 0.00001292
Iteration 48/1000 | Loss: 0.00001292
Iteration 49/1000 | Loss: 0.00001292
Iteration 50/1000 | Loss: 0.00001292
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001292
Iteration 53/1000 | Loss: 0.00001291
Iteration 54/1000 | Loss: 0.00001291
Iteration 55/1000 | Loss: 0.00001291
Iteration 56/1000 | Loss: 0.00001289
Iteration 57/1000 | Loss: 0.00001288
Iteration 58/1000 | Loss: 0.00001288
Iteration 59/1000 | Loss: 0.00001288
Iteration 60/1000 | Loss: 0.00001288
Iteration 61/1000 | Loss: 0.00001287
Iteration 62/1000 | Loss: 0.00001287
Iteration 63/1000 | Loss: 0.00001287
Iteration 64/1000 | Loss: 0.00001287
Iteration 65/1000 | Loss: 0.00001287
Iteration 66/1000 | Loss: 0.00001287
Iteration 67/1000 | Loss: 0.00001286
Iteration 68/1000 | Loss: 0.00001286
Iteration 69/1000 | Loss: 0.00001286
Iteration 70/1000 | Loss: 0.00001285
Iteration 71/1000 | Loss: 0.00001285
Iteration 72/1000 | Loss: 0.00001285
Iteration 73/1000 | Loss: 0.00001285
Iteration 74/1000 | Loss: 0.00001285
Iteration 75/1000 | Loss: 0.00001285
Iteration 76/1000 | Loss: 0.00001284
Iteration 77/1000 | Loss: 0.00001284
Iteration 78/1000 | Loss: 0.00001284
Iteration 79/1000 | Loss: 0.00001284
Iteration 80/1000 | Loss: 0.00001283
Iteration 81/1000 | Loss: 0.00001283
Iteration 82/1000 | Loss: 0.00001283
Iteration 83/1000 | Loss: 0.00001283
Iteration 84/1000 | Loss: 0.00001282
Iteration 85/1000 | Loss: 0.00001282
Iteration 86/1000 | Loss: 0.00001282
Iteration 87/1000 | Loss: 0.00001282
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001281
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001281
Iteration 93/1000 | Loss: 0.00001280
Iteration 94/1000 | Loss: 0.00001280
Iteration 95/1000 | Loss: 0.00001280
Iteration 96/1000 | Loss: 0.00001280
Iteration 97/1000 | Loss: 0.00001279
Iteration 98/1000 | Loss: 0.00001279
Iteration 99/1000 | Loss: 0.00001279
Iteration 100/1000 | Loss: 0.00001279
Iteration 101/1000 | Loss: 0.00001279
Iteration 102/1000 | Loss: 0.00001279
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001278
Iteration 105/1000 | Loss: 0.00001278
Iteration 106/1000 | Loss: 0.00001278
Iteration 107/1000 | Loss: 0.00001278
Iteration 108/1000 | Loss: 0.00001278
Iteration 109/1000 | Loss: 0.00001278
Iteration 110/1000 | Loss: 0.00001278
Iteration 111/1000 | Loss: 0.00001278
Iteration 112/1000 | Loss: 0.00001278
Iteration 113/1000 | Loss: 0.00001278
Iteration 114/1000 | Loss: 0.00001278
Iteration 115/1000 | Loss: 0.00001278
Iteration 116/1000 | Loss: 0.00001278
Iteration 117/1000 | Loss: 0.00001278
Iteration 118/1000 | Loss: 0.00001278
Iteration 119/1000 | Loss: 0.00001278
Iteration 120/1000 | Loss: 0.00001278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.2779999451595359e-05, 1.2779999451595359e-05, 1.2779999451595359e-05, 1.2779999451595359e-05, 1.2779999451595359e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2779999451595359e-05

Optimization complete. Final v2v error: 3.0524611473083496 mm

Highest mean error: 3.26859974861145 mm for frame 239

Lowest mean error: 2.8088479042053223 mm for frame 49

Saving results

Total time: 37.14191675186157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430691
Iteration 2/25 | Loss: 0.00131591
Iteration 3/25 | Loss: 0.00123453
Iteration 4/25 | Loss: 0.00121933
Iteration 5/25 | Loss: 0.00121552
Iteration 6/25 | Loss: 0.00121532
Iteration 7/25 | Loss: 0.00121532
Iteration 8/25 | Loss: 0.00121532
Iteration 9/25 | Loss: 0.00121532
Iteration 10/25 | Loss: 0.00121532
Iteration 11/25 | Loss: 0.00121532
Iteration 12/25 | Loss: 0.00121532
Iteration 13/25 | Loss: 0.00121532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012153221759945154, 0.0012153221759945154, 0.0012153221759945154, 0.0012153221759945154, 0.0012153221759945154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012153221759945154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.18132448
Iteration 2/25 | Loss: 0.00067993
Iteration 3/25 | Loss: 0.00067991
Iteration 4/25 | Loss: 0.00067991
Iteration 5/25 | Loss: 0.00067991
Iteration 6/25 | Loss: 0.00067991
Iteration 7/25 | Loss: 0.00067991
Iteration 8/25 | Loss: 0.00067991
Iteration 9/25 | Loss: 0.00067991
Iteration 10/25 | Loss: 0.00067991
Iteration 11/25 | Loss: 0.00067991
Iteration 12/25 | Loss: 0.00067991
Iteration 13/25 | Loss: 0.00067991
Iteration 14/25 | Loss: 0.00067991
Iteration 15/25 | Loss: 0.00067991
Iteration 16/25 | Loss: 0.00067991
Iteration 17/25 | Loss: 0.00067991
Iteration 18/25 | Loss: 0.00067991
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000679911463521421, 0.000679911463521421, 0.000679911463521421, 0.000679911463521421, 0.000679911463521421]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000679911463521421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067991
Iteration 2/1000 | Loss: 0.00002688
Iteration 3/1000 | Loss: 0.00002098
Iteration 4/1000 | Loss: 0.00001962
Iteration 5/1000 | Loss: 0.00001863
Iteration 6/1000 | Loss: 0.00001802
Iteration 7/1000 | Loss: 0.00001757
Iteration 8/1000 | Loss: 0.00001723
Iteration 9/1000 | Loss: 0.00001686
Iteration 10/1000 | Loss: 0.00001669
Iteration 11/1000 | Loss: 0.00001648
Iteration 12/1000 | Loss: 0.00001643
Iteration 13/1000 | Loss: 0.00001631
Iteration 14/1000 | Loss: 0.00001624
Iteration 15/1000 | Loss: 0.00001621
Iteration 16/1000 | Loss: 0.00001620
Iteration 17/1000 | Loss: 0.00001619
Iteration 18/1000 | Loss: 0.00001619
Iteration 19/1000 | Loss: 0.00001615
Iteration 20/1000 | Loss: 0.00001612
Iteration 21/1000 | Loss: 0.00001611
Iteration 22/1000 | Loss: 0.00001611
Iteration 23/1000 | Loss: 0.00001611
Iteration 24/1000 | Loss: 0.00001605
Iteration 25/1000 | Loss: 0.00001604
Iteration 26/1000 | Loss: 0.00001603
Iteration 27/1000 | Loss: 0.00001602
Iteration 28/1000 | Loss: 0.00001600
Iteration 29/1000 | Loss: 0.00001599
Iteration 30/1000 | Loss: 0.00001598
Iteration 31/1000 | Loss: 0.00001598
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001594
Iteration 34/1000 | Loss: 0.00001594
Iteration 35/1000 | Loss: 0.00001593
Iteration 36/1000 | Loss: 0.00001593
Iteration 37/1000 | Loss: 0.00001593
Iteration 38/1000 | Loss: 0.00001592
Iteration 39/1000 | Loss: 0.00001592
Iteration 40/1000 | Loss: 0.00001591
Iteration 41/1000 | Loss: 0.00001590
Iteration 42/1000 | Loss: 0.00001590
Iteration 43/1000 | Loss: 0.00001590
Iteration 44/1000 | Loss: 0.00001589
Iteration 45/1000 | Loss: 0.00001589
Iteration 46/1000 | Loss: 0.00001589
Iteration 47/1000 | Loss: 0.00001589
Iteration 48/1000 | Loss: 0.00001589
Iteration 49/1000 | Loss: 0.00001589
Iteration 50/1000 | Loss: 0.00001589
Iteration 51/1000 | Loss: 0.00001589
Iteration 52/1000 | Loss: 0.00001589
Iteration 53/1000 | Loss: 0.00001588
Iteration 54/1000 | Loss: 0.00001586
Iteration 55/1000 | Loss: 0.00001586
Iteration 56/1000 | Loss: 0.00001585
Iteration 57/1000 | Loss: 0.00001585
Iteration 58/1000 | Loss: 0.00001585
Iteration 59/1000 | Loss: 0.00001585
Iteration 60/1000 | Loss: 0.00001581
Iteration 61/1000 | Loss: 0.00001581
Iteration 62/1000 | Loss: 0.00001581
Iteration 63/1000 | Loss: 0.00001579
Iteration 64/1000 | Loss: 0.00001578
Iteration 65/1000 | Loss: 0.00001578
Iteration 66/1000 | Loss: 0.00001577
Iteration 67/1000 | Loss: 0.00001577
Iteration 68/1000 | Loss: 0.00001577
Iteration 69/1000 | Loss: 0.00001576
Iteration 70/1000 | Loss: 0.00001576
Iteration 71/1000 | Loss: 0.00001575
Iteration 72/1000 | Loss: 0.00001574
Iteration 73/1000 | Loss: 0.00001574
Iteration 74/1000 | Loss: 0.00001573
Iteration 75/1000 | Loss: 0.00001573
Iteration 76/1000 | Loss: 0.00001573
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001571
Iteration 80/1000 | Loss: 0.00001571
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001570
Iteration 83/1000 | Loss: 0.00001570
Iteration 84/1000 | Loss: 0.00001570
Iteration 85/1000 | Loss: 0.00001570
Iteration 86/1000 | Loss: 0.00001570
Iteration 87/1000 | Loss: 0.00001570
Iteration 88/1000 | Loss: 0.00001569
Iteration 89/1000 | Loss: 0.00001569
Iteration 90/1000 | Loss: 0.00001569
Iteration 91/1000 | Loss: 0.00001569
Iteration 92/1000 | Loss: 0.00001569
Iteration 93/1000 | Loss: 0.00001569
Iteration 94/1000 | Loss: 0.00001568
Iteration 95/1000 | Loss: 0.00001568
Iteration 96/1000 | Loss: 0.00001568
Iteration 97/1000 | Loss: 0.00001568
Iteration 98/1000 | Loss: 0.00001568
Iteration 99/1000 | Loss: 0.00001568
Iteration 100/1000 | Loss: 0.00001568
Iteration 101/1000 | Loss: 0.00001568
Iteration 102/1000 | Loss: 0.00001568
Iteration 103/1000 | Loss: 0.00001568
Iteration 104/1000 | Loss: 0.00001568
Iteration 105/1000 | Loss: 0.00001567
Iteration 106/1000 | Loss: 0.00001567
Iteration 107/1000 | Loss: 0.00001567
Iteration 108/1000 | Loss: 0.00001567
Iteration 109/1000 | Loss: 0.00001567
Iteration 110/1000 | Loss: 0.00001567
Iteration 111/1000 | Loss: 0.00001567
Iteration 112/1000 | Loss: 0.00001567
Iteration 113/1000 | Loss: 0.00001567
Iteration 114/1000 | Loss: 0.00001567
Iteration 115/1000 | Loss: 0.00001567
Iteration 116/1000 | Loss: 0.00001567
Iteration 117/1000 | Loss: 0.00001567
Iteration 118/1000 | Loss: 0.00001567
Iteration 119/1000 | Loss: 0.00001567
Iteration 120/1000 | Loss: 0.00001567
Iteration 121/1000 | Loss: 0.00001567
Iteration 122/1000 | Loss: 0.00001567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.56660007633036e-05, 1.56660007633036e-05, 1.56660007633036e-05, 1.56660007633036e-05, 1.56660007633036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.56660007633036e-05

Optimization complete. Final v2v error: 3.3394830226898193 mm

Highest mean error: 4.129978656768799 mm for frame 239

Lowest mean error: 3.0983636379241943 mm for frame 10

Saving results

Total time: 41.286078453063965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534161
Iteration 2/25 | Loss: 0.00147990
Iteration 3/25 | Loss: 0.00132045
Iteration 4/25 | Loss: 0.00130521
Iteration 5/25 | Loss: 0.00130122
Iteration 6/25 | Loss: 0.00130107
Iteration 7/25 | Loss: 0.00130107
Iteration 8/25 | Loss: 0.00130107
Iteration 9/25 | Loss: 0.00130107
Iteration 10/25 | Loss: 0.00130107
Iteration 11/25 | Loss: 0.00130107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013010691618546844, 0.0013010691618546844, 0.0013010691618546844, 0.0013010691618546844, 0.0013010691618546844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013010691618546844

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81902695
Iteration 2/25 | Loss: 0.00081701
Iteration 3/25 | Loss: 0.00081700
Iteration 4/25 | Loss: 0.00081700
Iteration 5/25 | Loss: 0.00081700
Iteration 6/25 | Loss: 0.00081700
Iteration 7/25 | Loss: 0.00081700
Iteration 8/25 | Loss: 0.00081700
Iteration 9/25 | Loss: 0.00081700
Iteration 10/25 | Loss: 0.00081700
Iteration 11/25 | Loss: 0.00081700
Iteration 12/25 | Loss: 0.00081700
Iteration 13/25 | Loss: 0.00081700
Iteration 14/25 | Loss: 0.00081700
Iteration 15/25 | Loss: 0.00081700
Iteration 16/25 | Loss: 0.00081700
Iteration 17/25 | Loss: 0.00081700
Iteration 18/25 | Loss: 0.00081700
Iteration 19/25 | Loss: 0.00081700
Iteration 20/25 | Loss: 0.00081700
Iteration 21/25 | Loss: 0.00081700
Iteration 22/25 | Loss: 0.00081700
Iteration 23/25 | Loss: 0.00081700
Iteration 24/25 | Loss: 0.00081700
Iteration 25/25 | Loss: 0.00081700

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081700
Iteration 2/1000 | Loss: 0.00004860
Iteration 3/1000 | Loss: 0.00002592
Iteration 4/1000 | Loss: 0.00002146
Iteration 5/1000 | Loss: 0.00002006
Iteration 6/1000 | Loss: 0.00001907
Iteration 7/1000 | Loss: 0.00001836
Iteration 8/1000 | Loss: 0.00001793
Iteration 9/1000 | Loss: 0.00001756
Iteration 10/1000 | Loss: 0.00001717
Iteration 11/1000 | Loss: 0.00001690
Iteration 12/1000 | Loss: 0.00001669
Iteration 13/1000 | Loss: 0.00001642
Iteration 14/1000 | Loss: 0.00001626
Iteration 15/1000 | Loss: 0.00001618
Iteration 16/1000 | Loss: 0.00001617
Iteration 17/1000 | Loss: 0.00001600
Iteration 18/1000 | Loss: 0.00001582
Iteration 19/1000 | Loss: 0.00001568
Iteration 20/1000 | Loss: 0.00001562
Iteration 21/1000 | Loss: 0.00001556
Iteration 22/1000 | Loss: 0.00001551
Iteration 23/1000 | Loss: 0.00001551
Iteration 24/1000 | Loss: 0.00001547
Iteration 25/1000 | Loss: 0.00001547
Iteration 26/1000 | Loss: 0.00001545
Iteration 27/1000 | Loss: 0.00001544
Iteration 28/1000 | Loss: 0.00001540
Iteration 29/1000 | Loss: 0.00001540
Iteration 30/1000 | Loss: 0.00001539
Iteration 31/1000 | Loss: 0.00001538
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001536
Iteration 35/1000 | Loss: 0.00001535
Iteration 36/1000 | Loss: 0.00001535
Iteration 37/1000 | Loss: 0.00001533
Iteration 38/1000 | Loss: 0.00001533
Iteration 39/1000 | Loss: 0.00001533
Iteration 40/1000 | Loss: 0.00001533
Iteration 41/1000 | Loss: 0.00001533
Iteration 42/1000 | Loss: 0.00001533
Iteration 43/1000 | Loss: 0.00001532
Iteration 44/1000 | Loss: 0.00001532
Iteration 45/1000 | Loss: 0.00001532
Iteration 46/1000 | Loss: 0.00001532
Iteration 47/1000 | Loss: 0.00001531
Iteration 48/1000 | Loss: 0.00001531
Iteration 49/1000 | Loss: 0.00001530
Iteration 50/1000 | Loss: 0.00001530
Iteration 51/1000 | Loss: 0.00001530
Iteration 52/1000 | Loss: 0.00001530
Iteration 53/1000 | Loss: 0.00001529
Iteration 54/1000 | Loss: 0.00001529
Iteration 55/1000 | Loss: 0.00001529
Iteration 56/1000 | Loss: 0.00001529
Iteration 57/1000 | Loss: 0.00001529
Iteration 58/1000 | Loss: 0.00001529
Iteration 59/1000 | Loss: 0.00001529
Iteration 60/1000 | Loss: 0.00001528
Iteration 61/1000 | Loss: 0.00001528
Iteration 62/1000 | Loss: 0.00001528
Iteration 63/1000 | Loss: 0.00001528
Iteration 64/1000 | Loss: 0.00001528
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001526
Iteration 70/1000 | Loss: 0.00001526
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001525
Iteration 73/1000 | Loss: 0.00001525
Iteration 74/1000 | Loss: 0.00001525
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001524
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001522
Iteration 82/1000 | Loss: 0.00001522
Iteration 83/1000 | Loss: 0.00001522
Iteration 84/1000 | Loss: 0.00001522
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001521
Iteration 88/1000 | Loss: 0.00001521
Iteration 89/1000 | Loss: 0.00001521
Iteration 90/1000 | Loss: 0.00001521
Iteration 91/1000 | Loss: 0.00001521
Iteration 92/1000 | Loss: 0.00001521
Iteration 93/1000 | Loss: 0.00001520
Iteration 94/1000 | Loss: 0.00001520
Iteration 95/1000 | Loss: 0.00001520
Iteration 96/1000 | Loss: 0.00001520
Iteration 97/1000 | Loss: 0.00001519
Iteration 98/1000 | Loss: 0.00001519
Iteration 99/1000 | Loss: 0.00001519
Iteration 100/1000 | Loss: 0.00001519
Iteration 101/1000 | Loss: 0.00001519
Iteration 102/1000 | Loss: 0.00001519
Iteration 103/1000 | Loss: 0.00001518
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001518
Iteration 106/1000 | Loss: 0.00001518
Iteration 107/1000 | Loss: 0.00001518
Iteration 108/1000 | Loss: 0.00001518
Iteration 109/1000 | Loss: 0.00001518
Iteration 110/1000 | Loss: 0.00001517
Iteration 111/1000 | Loss: 0.00001517
Iteration 112/1000 | Loss: 0.00001517
Iteration 113/1000 | Loss: 0.00001516
Iteration 114/1000 | Loss: 0.00001516
Iteration 115/1000 | Loss: 0.00001516
Iteration 116/1000 | Loss: 0.00001516
Iteration 117/1000 | Loss: 0.00001516
Iteration 118/1000 | Loss: 0.00001516
Iteration 119/1000 | Loss: 0.00001516
Iteration 120/1000 | Loss: 0.00001515
Iteration 121/1000 | Loss: 0.00001515
Iteration 122/1000 | Loss: 0.00001515
Iteration 123/1000 | Loss: 0.00001515
Iteration 124/1000 | Loss: 0.00001515
Iteration 125/1000 | Loss: 0.00001515
Iteration 126/1000 | Loss: 0.00001514
Iteration 127/1000 | Loss: 0.00001514
Iteration 128/1000 | Loss: 0.00001514
Iteration 129/1000 | Loss: 0.00001513
Iteration 130/1000 | Loss: 0.00001511
Iteration 131/1000 | Loss: 0.00001511
Iteration 132/1000 | Loss: 0.00001510
Iteration 133/1000 | Loss: 0.00001510
Iteration 134/1000 | Loss: 0.00001509
Iteration 135/1000 | Loss: 0.00001509
Iteration 136/1000 | Loss: 0.00001508
Iteration 137/1000 | Loss: 0.00001508
Iteration 138/1000 | Loss: 0.00001508
Iteration 139/1000 | Loss: 0.00001508
Iteration 140/1000 | Loss: 0.00001508
Iteration 141/1000 | Loss: 0.00001508
Iteration 142/1000 | Loss: 0.00001508
Iteration 143/1000 | Loss: 0.00001508
Iteration 144/1000 | Loss: 0.00001508
Iteration 145/1000 | Loss: 0.00001508
Iteration 146/1000 | Loss: 0.00001508
Iteration 147/1000 | Loss: 0.00001507
Iteration 148/1000 | Loss: 0.00001507
Iteration 149/1000 | Loss: 0.00001507
Iteration 150/1000 | Loss: 0.00001507
Iteration 151/1000 | Loss: 0.00001507
Iteration 152/1000 | Loss: 0.00001507
Iteration 153/1000 | Loss: 0.00001507
Iteration 154/1000 | Loss: 0.00001506
Iteration 155/1000 | Loss: 0.00001506
Iteration 156/1000 | Loss: 0.00001506
Iteration 157/1000 | Loss: 0.00001506
Iteration 158/1000 | Loss: 0.00001506
Iteration 159/1000 | Loss: 0.00001506
Iteration 160/1000 | Loss: 0.00001506
Iteration 161/1000 | Loss: 0.00001506
Iteration 162/1000 | Loss: 0.00001506
Iteration 163/1000 | Loss: 0.00001505
Iteration 164/1000 | Loss: 0.00001505
Iteration 165/1000 | Loss: 0.00001505
Iteration 166/1000 | Loss: 0.00001505
Iteration 167/1000 | Loss: 0.00001505
Iteration 168/1000 | Loss: 0.00001505
Iteration 169/1000 | Loss: 0.00001505
Iteration 170/1000 | Loss: 0.00001505
Iteration 171/1000 | Loss: 0.00001504
Iteration 172/1000 | Loss: 0.00001504
Iteration 173/1000 | Loss: 0.00001504
Iteration 174/1000 | Loss: 0.00001504
Iteration 175/1000 | Loss: 0.00001504
Iteration 176/1000 | Loss: 0.00001504
Iteration 177/1000 | Loss: 0.00001504
Iteration 178/1000 | Loss: 0.00001504
Iteration 179/1000 | Loss: 0.00001504
Iteration 180/1000 | Loss: 0.00001503
Iteration 181/1000 | Loss: 0.00001503
Iteration 182/1000 | Loss: 0.00001503
Iteration 183/1000 | Loss: 0.00001503
Iteration 184/1000 | Loss: 0.00001503
Iteration 185/1000 | Loss: 0.00001503
Iteration 186/1000 | Loss: 0.00001503
Iteration 187/1000 | Loss: 0.00001503
Iteration 188/1000 | Loss: 0.00001503
Iteration 189/1000 | Loss: 0.00001503
Iteration 190/1000 | Loss: 0.00001503
Iteration 191/1000 | Loss: 0.00001503
Iteration 192/1000 | Loss: 0.00001503
Iteration 193/1000 | Loss: 0.00001503
Iteration 194/1000 | Loss: 0.00001502
Iteration 195/1000 | Loss: 0.00001502
Iteration 196/1000 | Loss: 0.00001502
Iteration 197/1000 | Loss: 0.00001502
Iteration 198/1000 | Loss: 0.00001502
Iteration 199/1000 | Loss: 0.00001502
Iteration 200/1000 | Loss: 0.00001502
Iteration 201/1000 | Loss: 0.00001502
Iteration 202/1000 | Loss: 0.00001502
Iteration 203/1000 | Loss: 0.00001502
Iteration 204/1000 | Loss: 0.00001502
Iteration 205/1000 | Loss: 0.00001502
Iteration 206/1000 | Loss: 0.00001502
Iteration 207/1000 | Loss: 0.00001502
Iteration 208/1000 | Loss: 0.00001502
Iteration 209/1000 | Loss: 0.00001502
Iteration 210/1000 | Loss: 0.00001502
Iteration 211/1000 | Loss: 0.00001502
Iteration 212/1000 | Loss: 0.00001502
Iteration 213/1000 | Loss: 0.00001502
Iteration 214/1000 | Loss: 0.00001502
Iteration 215/1000 | Loss: 0.00001502
Iteration 216/1000 | Loss: 0.00001502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.5022744264570065e-05, 1.5022744264570065e-05, 1.5022744264570065e-05, 1.5022744264570065e-05, 1.5022744264570065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5022744264570065e-05

Optimization complete. Final v2v error: 3.29119610786438 mm

Highest mean error: 3.8393495082855225 mm for frame 239

Lowest mean error: 3.23270583152771 mm for frame 147

Saving results

Total time: 56.591909885406494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771221
Iteration 2/25 | Loss: 0.00140580
Iteration 3/25 | Loss: 0.00124670
Iteration 4/25 | Loss: 0.00122608
Iteration 5/25 | Loss: 0.00122135
Iteration 6/25 | Loss: 0.00122066
Iteration 7/25 | Loss: 0.00122066
Iteration 8/25 | Loss: 0.00122066
Iteration 9/25 | Loss: 0.00122066
Iteration 10/25 | Loss: 0.00122066
Iteration 11/25 | Loss: 0.00122066
Iteration 12/25 | Loss: 0.00122066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001220657373778522, 0.001220657373778522, 0.001220657373778522, 0.001220657373778522, 0.001220657373778522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001220657373778522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39501965
Iteration 2/25 | Loss: 0.00054445
Iteration 3/25 | Loss: 0.00054441
Iteration 4/25 | Loss: 0.00054441
Iteration 5/25 | Loss: 0.00054441
Iteration 6/25 | Loss: 0.00054441
Iteration 7/25 | Loss: 0.00054441
Iteration 8/25 | Loss: 0.00054441
Iteration 9/25 | Loss: 0.00054441
Iteration 10/25 | Loss: 0.00054441
Iteration 11/25 | Loss: 0.00054441
Iteration 12/25 | Loss: 0.00054441
Iteration 13/25 | Loss: 0.00054441
Iteration 14/25 | Loss: 0.00054441
Iteration 15/25 | Loss: 0.00054441
Iteration 16/25 | Loss: 0.00054441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005444050766527653, 0.0005444050766527653, 0.0005444050766527653, 0.0005444050766527653, 0.0005444050766527653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005444050766527653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054441
Iteration 2/1000 | Loss: 0.00003497
Iteration 3/1000 | Loss: 0.00002673
Iteration 4/1000 | Loss: 0.00002171
Iteration 5/1000 | Loss: 0.00001997
Iteration 6/1000 | Loss: 0.00001880
Iteration 7/1000 | Loss: 0.00001802
Iteration 8/1000 | Loss: 0.00001737
Iteration 9/1000 | Loss: 0.00001698
Iteration 10/1000 | Loss: 0.00001668
Iteration 11/1000 | Loss: 0.00001646
Iteration 12/1000 | Loss: 0.00001628
Iteration 13/1000 | Loss: 0.00001616
Iteration 14/1000 | Loss: 0.00001610
Iteration 15/1000 | Loss: 0.00001602
Iteration 16/1000 | Loss: 0.00001601
Iteration 17/1000 | Loss: 0.00001588
Iteration 18/1000 | Loss: 0.00001585
Iteration 19/1000 | Loss: 0.00001583
Iteration 20/1000 | Loss: 0.00001582
Iteration 21/1000 | Loss: 0.00001581
Iteration 22/1000 | Loss: 0.00001581
Iteration 23/1000 | Loss: 0.00001581
Iteration 24/1000 | Loss: 0.00001580
Iteration 25/1000 | Loss: 0.00001578
Iteration 26/1000 | Loss: 0.00001577
Iteration 27/1000 | Loss: 0.00001577
Iteration 28/1000 | Loss: 0.00001576
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001571
Iteration 34/1000 | Loss: 0.00001570
Iteration 35/1000 | Loss: 0.00001570
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001570
Iteration 39/1000 | Loss: 0.00001570
Iteration 40/1000 | Loss: 0.00001570
Iteration 41/1000 | Loss: 0.00001570
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001567
Iteration 44/1000 | Loss: 0.00001567
Iteration 45/1000 | Loss: 0.00001566
Iteration 46/1000 | Loss: 0.00001566
Iteration 47/1000 | Loss: 0.00001566
Iteration 48/1000 | Loss: 0.00001565
Iteration 49/1000 | Loss: 0.00001565
Iteration 50/1000 | Loss: 0.00001564
Iteration 51/1000 | Loss: 0.00001564
Iteration 52/1000 | Loss: 0.00001563
Iteration 53/1000 | Loss: 0.00001563
Iteration 54/1000 | Loss: 0.00001562
Iteration 55/1000 | Loss: 0.00001562
Iteration 56/1000 | Loss: 0.00001562
Iteration 57/1000 | Loss: 0.00001562
Iteration 58/1000 | Loss: 0.00001562
Iteration 59/1000 | Loss: 0.00001561
Iteration 60/1000 | Loss: 0.00001561
Iteration 61/1000 | Loss: 0.00001561
Iteration 62/1000 | Loss: 0.00001561
Iteration 63/1000 | Loss: 0.00001560
Iteration 64/1000 | Loss: 0.00001560
Iteration 65/1000 | Loss: 0.00001560
Iteration 66/1000 | Loss: 0.00001560
Iteration 67/1000 | Loss: 0.00001560
Iteration 68/1000 | Loss: 0.00001560
Iteration 69/1000 | Loss: 0.00001559
Iteration 70/1000 | Loss: 0.00001559
Iteration 71/1000 | Loss: 0.00001559
Iteration 72/1000 | Loss: 0.00001559
Iteration 73/1000 | Loss: 0.00001559
Iteration 74/1000 | Loss: 0.00001558
Iteration 75/1000 | Loss: 0.00001558
Iteration 76/1000 | Loss: 0.00001558
Iteration 77/1000 | Loss: 0.00001558
Iteration 78/1000 | Loss: 0.00001558
Iteration 79/1000 | Loss: 0.00001557
Iteration 80/1000 | Loss: 0.00001556
Iteration 81/1000 | Loss: 0.00001556
Iteration 82/1000 | Loss: 0.00001556
Iteration 83/1000 | Loss: 0.00001556
Iteration 84/1000 | Loss: 0.00001556
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001555
Iteration 87/1000 | Loss: 0.00001555
Iteration 88/1000 | Loss: 0.00001554
Iteration 89/1000 | Loss: 0.00001554
Iteration 90/1000 | Loss: 0.00001554
Iteration 91/1000 | Loss: 0.00001554
Iteration 92/1000 | Loss: 0.00001554
Iteration 93/1000 | Loss: 0.00001554
Iteration 94/1000 | Loss: 0.00001553
Iteration 95/1000 | Loss: 0.00001553
Iteration 96/1000 | Loss: 0.00001553
Iteration 97/1000 | Loss: 0.00001553
Iteration 98/1000 | Loss: 0.00001553
Iteration 99/1000 | Loss: 0.00001553
Iteration 100/1000 | Loss: 0.00001553
Iteration 101/1000 | Loss: 0.00001553
Iteration 102/1000 | Loss: 0.00001552
Iteration 103/1000 | Loss: 0.00001552
Iteration 104/1000 | Loss: 0.00001552
Iteration 105/1000 | Loss: 0.00001552
Iteration 106/1000 | Loss: 0.00001552
Iteration 107/1000 | Loss: 0.00001552
Iteration 108/1000 | Loss: 0.00001552
Iteration 109/1000 | Loss: 0.00001552
Iteration 110/1000 | Loss: 0.00001552
Iteration 111/1000 | Loss: 0.00001552
Iteration 112/1000 | Loss: 0.00001552
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001551
Iteration 115/1000 | Loss: 0.00001551
Iteration 116/1000 | Loss: 0.00001551
Iteration 117/1000 | Loss: 0.00001551
Iteration 118/1000 | Loss: 0.00001551
Iteration 119/1000 | Loss: 0.00001551
Iteration 120/1000 | Loss: 0.00001551
Iteration 121/1000 | Loss: 0.00001551
Iteration 122/1000 | Loss: 0.00001551
Iteration 123/1000 | Loss: 0.00001551
Iteration 124/1000 | Loss: 0.00001551
Iteration 125/1000 | Loss: 0.00001551
Iteration 126/1000 | Loss: 0.00001551
Iteration 127/1000 | Loss: 0.00001551
Iteration 128/1000 | Loss: 0.00001551
Iteration 129/1000 | Loss: 0.00001551
Iteration 130/1000 | Loss: 0.00001551
Iteration 131/1000 | Loss: 0.00001551
Iteration 132/1000 | Loss: 0.00001551
Iteration 133/1000 | Loss: 0.00001551
Iteration 134/1000 | Loss: 0.00001551
Iteration 135/1000 | Loss: 0.00001551
Iteration 136/1000 | Loss: 0.00001551
Iteration 137/1000 | Loss: 0.00001551
Iteration 138/1000 | Loss: 0.00001551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.551109380670823e-05, 1.551109380670823e-05, 1.551109380670823e-05, 1.551109380670823e-05, 1.551109380670823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.551109380670823e-05

Optimization complete. Final v2v error: 3.3228812217712402 mm

Highest mean error: 3.6791069507598877 mm for frame 112

Lowest mean error: 3.0433335304260254 mm for frame 23

Saving results

Total time: 44.08126091957092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494188
Iteration 2/25 | Loss: 0.00142826
Iteration 3/25 | Loss: 0.00127593
Iteration 4/25 | Loss: 0.00126231
Iteration 5/25 | Loss: 0.00125869
Iteration 6/25 | Loss: 0.00125814
Iteration 7/25 | Loss: 0.00125814
Iteration 8/25 | Loss: 0.00125814
Iteration 9/25 | Loss: 0.00125814
Iteration 10/25 | Loss: 0.00125814
Iteration 11/25 | Loss: 0.00125814
Iteration 12/25 | Loss: 0.00125814
Iteration 13/25 | Loss: 0.00125814
Iteration 14/25 | Loss: 0.00125814
Iteration 15/25 | Loss: 0.00125814
Iteration 16/25 | Loss: 0.00125814
Iteration 17/25 | Loss: 0.00125814
Iteration 18/25 | Loss: 0.00125814
Iteration 19/25 | Loss: 0.00125814
Iteration 20/25 | Loss: 0.00125814
Iteration 21/25 | Loss: 0.00125814
Iteration 22/25 | Loss: 0.00125814
Iteration 23/25 | Loss: 0.00125814
Iteration 24/25 | Loss: 0.00125814
Iteration 25/25 | Loss: 0.00125814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54142237
Iteration 2/25 | Loss: 0.00074021
Iteration 3/25 | Loss: 0.00074020
Iteration 4/25 | Loss: 0.00074020
Iteration 5/25 | Loss: 0.00074020
Iteration 6/25 | Loss: 0.00074020
Iteration 7/25 | Loss: 0.00074020
Iteration 8/25 | Loss: 0.00074020
Iteration 9/25 | Loss: 0.00074020
Iteration 10/25 | Loss: 0.00074020
Iteration 11/25 | Loss: 0.00074020
Iteration 12/25 | Loss: 0.00074020
Iteration 13/25 | Loss: 0.00074019
Iteration 14/25 | Loss: 0.00074019
Iteration 15/25 | Loss: 0.00074020
Iteration 16/25 | Loss: 0.00074019
Iteration 17/25 | Loss: 0.00074019
Iteration 18/25 | Loss: 0.00074019
Iteration 19/25 | Loss: 0.00074019
Iteration 20/25 | Loss: 0.00074019
Iteration 21/25 | Loss: 0.00074019
Iteration 22/25 | Loss: 0.00074019
Iteration 23/25 | Loss: 0.00074019
Iteration 24/25 | Loss: 0.00074019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007401949260383844, 0.0007401949260383844, 0.0007401949260383844, 0.0007401949260383844, 0.0007401949260383844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007401949260383844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074019
Iteration 2/1000 | Loss: 0.00003387
Iteration 3/1000 | Loss: 0.00002396
Iteration 4/1000 | Loss: 0.00002141
Iteration 5/1000 | Loss: 0.00002032
Iteration 6/1000 | Loss: 0.00001943
Iteration 7/1000 | Loss: 0.00001874
Iteration 8/1000 | Loss: 0.00001823
Iteration 9/1000 | Loss: 0.00001792
Iteration 10/1000 | Loss: 0.00001760
Iteration 11/1000 | Loss: 0.00001738
Iteration 12/1000 | Loss: 0.00001734
Iteration 13/1000 | Loss: 0.00001717
Iteration 14/1000 | Loss: 0.00001713
Iteration 15/1000 | Loss: 0.00001709
Iteration 16/1000 | Loss: 0.00001701
Iteration 17/1000 | Loss: 0.00001700
Iteration 18/1000 | Loss: 0.00001698
Iteration 19/1000 | Loss: 0.00001695
Iteration 20/1000 | Loss: 0.00001695
Iteration 21/1000 | Loss: 0.00001694
Iteration 22/1000 | Loss: 0.00001694
Iteration 23/1000 | Loss: 0.00001692
Iteration 24/1000 | Loss: 0.00001692
Iteration 25/1000 | Loss: 0.00001692
Iteration 26/1000 | Loss: 0.00001691
Iteration 27/1000 | Loss: 0.00001691
Iteration 28/1000 | Loss: 0.00001691
Iteration 29/1000 | Loss: 0.00001690
Iteration 30/1000 | Loss: 0.00001689
Iteration 31/1000 | Loss: 0.00001689
Iteration 32/1000 | Loss: 0.00001688
Iteration 33/1000 | Loss: 0.00001688
Iteration 34/1000 | Loss: 0.00001688
Iteration 35/1000 | Loss: 0.00001687
Iteration 36/1000 | Loss: 0.00001687
Iteration 37/1000 | Loss: 0.00001687
Iteration 38/1000 | Loss: 0.00001686
Iteration 39/1000 | Loss: 0.00001686
Iteration 40/1000 | Loss: 0.00001685
Iteration 41/1000 | Loss: 0.00001685
Iteration 42/1000 | Loss: 0.00001685
Iteration 43/1000 | Loss: 0.00001684
Iteration 44/1000 | Loss: 0.00001684
Iteration 45/1000 | Loss: 0.00001683
Iteration 46/1000 | Loss: 0.00001683
Iteration 47/1000 | Loss: 0.00001683
Iteration 48/1000 | Loss: 0.00001682
Iteration 49/1000 | Loss: 0.00001682
Iteration 50/1000 | Loss: 0.00001682
Iteration 51/1000 | Loss: 0.00001682
Iteration 52/1000 | Loss: 0.00001681
Iteration 53/1000 | Loss: 0.00001681
Iteration 54/1000 | Loss: 0.00001680
Iteration 55/1000 | Loss: 0.00001680
Iteration 56/1000 | Loss: 0.00001679
Iteration 57/1000 | Loss: 0.00001679
Iteration 58/1000 | Loss: 0.00001679
Iteration 59/1000 | Loss: 0.00001678
Iteration 60/1000 | Loss: 0.00001678
Iteration 61/1000 | Loss: 0.00001678
Iteration 62/1000 | Loss: 0.00001677
Iteration 63/1000 | Loss: 0.00001677
Iteration 64/1000 | Loss: 0.00001676
Iteration 65/1000 | Loss: 0.00001675
Iteration 66/1000 | Loss: 0.00001675
Iteration 67/1000 | Loss: 0.00001675
Iteration 68/1000 | Loss: 0.00001674
Iteration 69/1000 | Loss: 0.00001674
Iteration 70/1000 | Loss: 0.00001672
Iteration 71/1000 | Loss: 0.00001672
Iteration 72/1000 | Loss: 0.00001672
Iteration 73/1000 | Loss: 0.00001672
Iteration 74/1000 | Loss: 0.00001672
Iteration 75/1000 | Loss: 0.00001672
Iteration 76/1000 | Loss: 0.00001672
Iteration 77/1000 | Loss: 0.00001671
Iteration 78/1000 | Loss: 0.00001670
Iteration 79/1000 | Loss: 0.00001670
Iteration 80/1000 | Loss: 0.00001669
Iteration 81/1000 | Loss: 0.00001669
Iteration 82/1000 | Loss: 0.00001668
Iteration 83/1000 | Loss: 0.00001668
Iteration 84/1000 | Loss: 0.00001668
Iteration 85/1000 | Loss: 0.00001668
Iteration 86/1000 | Loss: 0.00001667
Iteration 87/1000 | Loss: 0.00001667
Iteration 88/1000 | Loss: 0.00001666
Iteration 89/1000 | Loss: 0.00001666
Iteration 90/1000 | Loss: 0.00001666
Iteration 91/1000 | Loss: 0.00001665
Iteration 92/1000 | Loss: 0.00001665
Iteration 93/1000 | Loss: 0.00001665
Iteration 94/1000 | Loss: 0.00001665
Iteration 95/1000 | Loss: 0.00001665
Iteration 96/1000 | Loss: 0.00001665
Iteration 97/1000 | Loss: 0.00001665
Iteration 98/1000 | Loss: 0.00001664
Iteration 99/1000 | Loss: 0.00001664
Iteration 100/1000 | Loss: 0.00001664
Iteration 101/1000 | Loss: 0.00001663
Iteration 102/1000 | Loss: 0.00001663
Iteration 103/1000 | Loss: 0.00001663
Iteration 104/1000 | Loss: 0.00001663
Iteration 105/1000 | Loss: 0.00001663
Iteration 106/1000 | Loss: 0.00001663
Iteration 107/1000 | Loss: 0.00001663
Iteration 108/1000 | Loss: 0.00001662
Iteration 109/1000 | Loss: 0.00001662
Iteration 110/1000 | Loss: 0.00001662
Iteration 111/1000 | Loss: 0.00001661
Iteration 112/1000 | Loss: 0.00001661
Iteration 113/1000 | Loss: 0.00001661
Iteration 114/1000 | Loss: 0.00001661
Iteration 115/1000 | Loss: 0.00001661
Iteration 116/1000 | Loss: 0.00001661
Iteration 117/1000 | Loss: 0.00001661
Iteration 118/1000 | Loss: 0.00001661
Iteration 119/1000 | Loss: 0.00001661
Iteration 120/1000 | Loss: 0.00001661
Iteration 121/1000 | Loss: 0.00001661
Iteration 122/1000 | Loss: 0.00001661
Iteration 123/1000 | Loss: 0.00001661
Iteration 124/1000 | Loss: 0.00001660
Iteration 125/1000 | Loss: 0.00001660
Iteration 126/1000 | Loss: 0.00001660
Iteration 127/1000 | Loss: 0.00001660
Iteration 128/1000 | Loss: 0.00001660
Iteration 129/1000 | Loss: 0.00001660
Iteration 130/1000 | Loss: 0.00001660
Iteration 131/1000 | Loss: 0.00001660
Iteration 132/1000 | Loss: 0.00001660
Iteration 133/1000 | Loss: 0.00001659
Iteration 134/1000 | Loss: 0.00001659
Iteration 135/1000 | Loss: 0.00001659
Iteration 136/1000 | Loss: 0.00001659
Iteration 137/1000 | Loss: 0.00001659
Iteration 138/1000 | Loss: 0.00001659
Iteration 139/1000 | Loss: 0.00001659
Iteration 140/1000 | Loss: 0.00001659
Iteration 141/1000 | Loss: 0.00001659
Iteration 142/1000 | Loss: 0.00001659
Iteration 143/1000 | Loss: 0.00001659
Iteration 144/1000 | Loss: 0.00001659
Iteration 145/1000 | Loss: 0.00001659
Iteration 146/1000 | Loss: 0.00001659
Iteration 147/1000 | Loss: 0.00001659
Iteration 148/1000 | Loss: 0.00001659
Iteration 149/1000 | Loss: 0.00001659
Iteration 150/1000 | Loss: 0.00001659
Iteration 151/1000 | Loss: 0.00001659
Iteration 152/1000 | Loss: 0.00001659
Iteration 153/1000 | Loss: 0.00001659
Iteration 154/1000 | Loss: 0.00001659
Iteration 155/1000 | Loss: 0.00001659
Iteration 156/1000 | Loss: 0.00001659
Iteration 157/1000 | Loss: 0.00001659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.6590365703450516e-05, 1.6590365703450516e-05, 1.6590365703450516e-05, 1.6590365703450516e-05, 1.6590365703450516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6590365703450516e-05

Optimization complete. Final v2v error: 3.3561131954193115 mm

Highest mean error: 4.490373611450195 mm for frame 110

Lowest mean error: 2.753693103790283 mm for frame 166

Saving results

Total time: 39.5073881149292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818339
Iteration 2/25 | Loss: 0.00128094
Iteration 3/25 | Loss: 0.00120066
Iteration 4/25 | Loss: 0.00119337
Iteration 5/25 | Loss: 0.00119197
Iteration 6/25 | Loss: 0.00119183
Iteration 7/25 | Loss: 0.00119183
Iteration 8/25 | Loss: 0.00119183
Iteration 9/25 | Loss: 0.00119183
Iteration 10/25 | Loss: 0.00119183
Iteration 11/25 | Loss: 0.00119183
Iteration 12/25 | Loss: 0.00119183
Iteration 13/25 | Loss: 0.00119183
Iteration 14/25 | Loss: 0.00119183
Iteration 15/25 | Loss: 0.00119183
Iteration 16/25 | Loss: 0.00119183
Iteration 17/25 | Loss: 0.00119183
Iteration 18/25 | Loss: 0.00119183
Iteration 19/25 | Loss: 0.00119183
Iteration 20/25 | Loss: 0.00119183
Iteration 21/25 | Loss: 0.00119183
Iteration 22/25 | Loss: 0.00119183
Iteration 23/25 | Loss: 0.00119183
Iteration 24/25 | Loss: 0.00119183
Iteration 25/25 | Loss: 0.00119183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43091500
Iteration 2/25 | Loss: 0.00068108
Iteration 3/25 | Loss: 0.00068106
Iteration 4/25 | Loss: 0.00068105
Iteration 5/25 | Loss: 0.00068105
Iteration 6/25 | Loss: 0.00068105
Iteration 7/25 | Loss: 0.00068105
Iteration 8/25 | Loss: 0.00068105
Iteration 9/25 | Loss: 0.00068105
Iteration 10/25 | Loss: 0.00068105
Iteration 11/25 | Loss: 0.00068105
Iteration 12/25 | Loss: 0.00068105
Iteration 13/25 | Loss: 0.00068105
Iteration 14/25 | Loss: 0.00068105
Iteration 15/25 | Loss: 0.00068105
Iteration 16/25 | Loss: 0.00068105
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000681052973959595, 0.000681052973959595, 0.000681052973959595, 0.000681052973959595, 0.000681052973959595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000681052973959595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068105
Iteration 2/1000 | Loss: 0.00002349
Iteration 3/1000 | Loss: 0.00001506
Iteration 4/1000 | Loss: 0.00001307
Iteration 5/1000 | Loss: 0.00001201
Iteration 6/1000 | Loss: 0.00001141
Iteration 7/1000 | Loss: 0.00001099
Iteration 8/1000 | Loss: 0.00001073
Iteration 9/1000 | Loss: 0.00001069
Iteration 10/1000 | Loss: 0.00001061
Iteration 11/1000 | Loss: 0.00001061
Iteration 12/1000 | Loss: 0.00001056
Iteration 13/1000 | Loss: 0.00001055
Iteration 14/1000 | Loss: 0.00001054
Iteration 15/1000 | Loss: 0.00001045
Iteration 16/1000 | Loss: 0.00001043
Iteration 17/1000 | Loss: 0.00001043
Iteration 18/1000 | Loss: 0.00001041
Iteration 19/1000 | Loss: 0.00001037
Iteration 20/1000 | Loss: 0.00001035
Iteration 21/1000 | Loss: 0.00001032
Iteration 22/1000 | Loss: 0.00001032
Iteration 23/1000 | Loss: 0.00001031
Iteration 24/1000 | Loss: 0.00001030
Iteration 25/1000 | Loss: 0.00001029
Iteration 26/1000 | Loss: 0.00001024
Iteration 27/1000 | Loss: 0.00001022
Iteration 28/1000 | Loss: 0.00001021
Iteration 29/1000 | Loss: 0.00001021
Iteration 30/1000 | Loss: 0.00001021
Iteration 31/1000 | Loss: 0.00001021
Iteration 32/1000 | Loss: 0.00001019
Iteration 33/1000 | Loss: 0.00001019
Iteration 34/1000 | Loss: 0.00001019
Iteration 35/1000 | Loss: 0.00001018
Iteration 36/1000 | Loss: 0.00001017
Iteration 37/1000 | Loss: 0.00001016
Iteration 38/1000 | Loss: 0.00001016
Iteration 39/1000 | Loss: 0.00001016
Iteration 40/1000 | Loss: 0.00001016
Iteration 41/1000 | Loss: 0.00001016
Iteration 42/1000 | Loss: 0.00001013
Iteration 43/1000 | Loss: 0.00001013
Iteration 44/1000 | Loss: 0.00001012
Iteration 45/1000 | Loss: 0.00001012
Iteration 46/1000 | Loss: 0.00001012
Iteration 47/1000 | Loss: 0.00001011
Iteration 48/1000 | Loss: 0.00001010
Iteration 49/1000 | Loss: 0.00001010
Iteration 50/1000 | Loss: 0.00001010
Iteration 51/1000 | Loss: 0.00001010
Iteration 52/1000 | Loss: 0.00001010
Iteration 53/1000 | Loss: 0.00001010
Iteration 54/1000 | Loss: 0.00001009
Iteration 55/1000 | Loss: 0.00001009
Iteration 56/1000 | Loss: 0.00001009
Iteration 57/1000 | Loss: 0.00001008
Iteration 58/1000 | Loss: 0.00001008
Iteration 59/1000 | Loss: 0.00001008
Iteration 60/1000 | Loss: 0.00001008
Iteration 61/1000 | Loss: 0.00001008
Iteration 62/1000 | Loss: 0.00001007
Iteration 63/1000 | Loss: 0.00001007
Iteration 64/1000 | Loss: 0.00001007
Iteration 65/1000 | Loss: 0.00001007
Iteration 66/1000 | Loss: 0.00001007
Iteration 67/1000 | Loss: 0.00001007
Iteration 68/1000 | Loss: 0.00001007
Iteration 69/1000 | Loss: 0.00001007
Iteration 70/1000 | Loss: 0.00001006
Iteration 71/1000 | Loss: 0.00001006
Iteration 72/1000 | Loss: 0.00001006
Iteration 73/1000 | Loss: 0.00001006
Iteration 74/1000 | Loss: 0.00001006
Iteration 75/1000 | Loss: 0.00001005
Iteration 76/1000 | Loss: 0.00001005
Iteration 77/1000 | Loss: 0.00001005
Iteration 78/1000 | Loss: 0.00001005
Iteration 79/1000 | Loss: 0.00001005
Iteration 80/1000 | Loss: 0.00001005
Iteration 81/1000 | Loss: 0.00001005
Iteration 82/1000 | Loss: 0.00001005
Iteration 83/1000 | Loss: 0.00001005
Iteration 84/1000 | Loss: 0.00001005
Iteration 85/1000 | Loss: 0.00001005
Iteration 86/1000 | Loss: 0.00001005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.0045293493021745e-05, 1.0045293493021745e-05, 1.0045293493021745e-05, 1.0045293493021745e-05, 1.0045293493021745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0045293493021745e-05

Optimization complete. Final v2v error: 2.708202362060547 mm

Highest mean error: 2.8767952919006348 mm for frame 0

Lowest mean error: 2.61684513092041 mm for frame 94

Saving results

Total time: 29.352970600128174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486908
Iteration 2/25 | Loss: 0.00158473
Iteration 3/25 | Loss: 0.00130211
Iteration 4/25 | Loss: 0.00126769
Iteration 5/25 | Loss: 0.00126281
Iteration 6/25 | Loss: 0.00126195
Iteration 7/25 | Loss: 0.00126195
Iteration 8/25 | Loss: 0.00126195
Iteration 9/25 | Loss: 0.00126195
Iteration 10/25 | Loss: 0.00126195
Iteration 11/25 | Loss: 0.00126195
Iteration 12/25 | Loss: 0.00126195
Iteration 13/25 | Loss: 0.00126195
Iteration 14/25 | Loss: 0.00126195
Iteration 15/25 | Loss: 0.00126195
Iteration 16/25 | Loss: 0.00126195
Iteration 17/25 | Loss: 0.00126195
Iteration 18/25 | Loss: 0.00126195
Iteration 19/25 | Loss: 0.00126195
Iteration 20/25 | Loss: 0.00126195
Iteration 21/25 | Loss: 0.00126195
Iteration 22/25 | Loss: 0.00126195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001261953730136156, 0.001261953730136156, 0.001261953730136156, 0.001261953730136156, 0.001261953730136156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001261953730136156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49820328
Iteration 2/25 | Loss: 0.00058248
Iteration 3/25 | Loss: 0.00058246
Iteration 4/25 | Loss: 0.00058246
Iteration 5/25 | Loss: 0.00058246
Iteration 6/25 | Loss: 0.00058246
Iteration 7/25 | Loss: 0.00058246
Iteration 8/25 | Loss: 0.00058246
Iteration 9/25 | Loss: 0.00058246
Iteration 10/25 | Loss: 0.00058246
Iteration 11/25 | Loss: 0.00058246
Iteration 12/25 | Loss: 0.00058246
Iteration 13/25 | Loss: 0.00058246
Iteration 14/25 | Loss: 0.00058246
Iteration 15/25 | Loss: 0.00058246
Iteration 16/25 | Loss: 0.00058246
Iteration 17/25 | Loss: 0.00058246
Iteration 18/25 | Loss: 0.00058246
Iteration 19/25 | Loss: 0.00058246
Iteration 20/25 | Loss: 0.00058246
Iteration 21/25 | Loss: 0.00058246
Iteration 22/25 | Loss: 0.00058246
Iteration 23/25 | Loss: 0.00058246
Iteration 24/25 | Loss: 0.00058246
Iteration 25/25 | Loss: 0.00058246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058246
Iteration 2/1000 | Loss: 0.00003907
Iteration 3/1000 | Loss: 0.00002576
Iteration 4/1000 | Loss: 0.00002301
Iteration 5/1000 | Loss: 0.00002175
Iteration 6/1000 | Loss: 0.00002071
Iteration 7/1000 | Loss: 0.00002020
Iteration 8/1000 | Loss: 0.00001961
Iteration 9/1000 | Loss: 0.00001917
Iteration 10/1000 | Loss: 0.00001882
Iteration 11/1000 | Loss: 0.00001865
Iteration 12/1000 | Loss: 0.00001847
Iteration 13/1000 | Loss: 0.00001826
Iteration 14/1000 | Loss: 0.00001820
Iteration 15/1000 | Loss: 0.00001818
Iteration 16/1000 | Loss: 0.00001817
Iteration 17/1000 | Loss: 0.00001812
Iteration 18/1000 | Loss: 0.00001810
Iteration 19/1000 | Loss: 0.00001808
Iteration 20/1000 | Loss: 0.00001807
Iteration 21/1000 | Loss: 0.00001805
Iteration 22/1000 | Loss: 0.00001805
Iteration 23/1000 | Loss: 0.00001804
Iteration 24/1000 | Loss: 0.00001804
Iteration 25/1000 | Loss: 0.00001800
Iteration 26/1000 | Loss: 0.00001799
Iteration 27/1000 | Loss: 0.00001798
Iteration 28/1000 | Loss: 0.00001798
Iteration 29/1000 | Loss: 0.00001797
Iteration 30/1000 | Loss: 0.00001793
Iteration 31/1000 | Loss: 0.00001793
Iteration 32/1000 | Loss: 0.00001792
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001790
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001789
Iteration 37/1000 | Loss: 0.00001789
Iteration 38/1000 | Loss: 0.00001789
Iteration 39/1000 | Loss: 0.00001789
Iteration 40/1000 | Loss: 0.00001788
Iteration 41/1000 | Loss: 0.00001788
Iteration 42/1000 | Loss: 0.00001788
Iteration 43/1000 | Loss: 0.00001788
Iteration 44/1000 | Loss: 0.00001787
Iteration 45/1000 | Loss: 0.00001787
Iteration 46/1000 | Loss: 0.00001787
Iteration 47/1000 | Loss: 0.00001787
Iteration 48/1000 | Loss: 0.00001787
Iteration 49/1000 | Loss: 0.00001787
Iteration 50/1000 | Loss: 0.00001787
Iteration 51/1000 | Loss: 0.00001786
Iteration 52/1000 | Loss: 0.00001786
Iteration 53/1000 | Loss: 0.00001786
Iteration 54/1000 | Loss: 0.00001786
Iteration 55/1000 | Loss: 0.00001786
Iteration 56/1000 | Loss: 0.00001785
Iteration 57/1000 | Loss: 0.00001785
Iteration 58/1000 | Loss: 0.00001784
Iteration 59/1000 | Loss: 0.00001783
Iteration 60/1000 | Loss: 0.00001783
Iteration 61/1000 | Loss: 0.00001783
Iteration 62/1000 | Loss: 0.00001783
Iteration 63/1000 | Loss: 0.00001783
Iteration 64/1000 | Loss: 0.00001782
Iteration 65/1000 | Loss: 0.00001782
Iteration 66/1000 | Loss: 0.00001782
Iteration 67/1000 | Loss: 0.00001782
Iteration 68/1000 | Loss: 0.00001782
Iteration 69/1000 | Loss: 0.00001782
Iteration 70/1000 | Loss: 0.00001781
Iteration 71/1000 | Loss: 0.00001781
Iteration 72/1000 | Loss: 0.00001780
Iteration 73/1000 | Loss: 0.00001780
Iteration 74/1000 | Loss: 0.00001780
Iteration 75/1000 | Loss: 0.00001780
Iteration 76/1000 | Loss: 0.00001780
Iteration 77/1000 | Loss: 0.00001780
Iteration 78/1000 | Loss: 0.00001780
Iteration 79/1000 | Loss: 0.00001780
Iteration 80/1000 | Loss: 0.00001780
Iteration 81/1000 | Loss: 0.00001780
Iteration 82/1000 | Loss: 0.00001780
Iteration 83/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.7798949556890875e-05, 1.7798949556890875e-05, 1.7798949556890875e-05, 1.7798949556890875e-05, 1.7798949556890875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7798949556890875e-05

Optimization complete. Final v2v error: 3.5555922985076904 mm

Highest mean error: 4.075982570648193 mm for frame 133

Lowest mean error: 3.1939220428466797 mm for frame 235

Saving results

Total time: 39.351951122283936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00580379
Iteration 2/25 | Loss: 0.00148758
Iteration 3/25 | Loss: 0.00134746
Iteration 4/25 | Loss: 0.00131484
Iteration 5/25 | Loss: 0.00130531
Iteration 6/25 | Loss: 0.00130327
Iteration 7/25 | Loss: 0.00130236
Iteration 8/25 | Loss: 0.00130190
Iteration 9/25 | Loss: 0.00130755
Iteration 10/25 | Loss: 0.00130279
Iteration 11/25 | Loss: 0.00130178
Iteration 12/25 | Loss: 0.00130149
Iteration 13/25 | Loss: 0.00130058
Iteration 14/25 | Loss: 0.00130017
Iteration 15/25 | Loss: 0.00129988
Iteration 16/25 | Loss: 0.00129975
Iteration 17/25 | Loss: 0.00129961
Iteration 18/25 | Loss: 0.00129940
Iteration 19/25 | Loss: 0.00130313
Iteration 20/25 | Loss: 0.00130379
Iteration 21/25 | Loss: 0.00130422
Iteration 22/25 | Loss: 0.00130302
Iteration 23/25 | Loss: 0.00130220
Iteration 24/25 | Loss: 0.00130298
Iteration 25/25 | Loss: 0.00130171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23846269
Iteration 2/25 | Loss: 0.00093578
Iteration 3/25 | Loss: 0.00093571
Iteration 4/25 | Loss: 0.00093571
Iteration 5/25 | Loss: 0.00093571
Iteration 6/25 | Loss: 0.00093571
Iteration 7/25 | Loss: 0.00093571
Iteration 8/25 | Loss: 0.00093571
Iteration 9/25 | Loss: 0.00093571
Iteration 10/25 | Loss: 0.00093571
Iteration 11/25 | Loss: 0.00093571
Iteration 12/25 | Loss: 0.00093571
Iteration 13/25 | Loss: 0.00093571
Iteration 14/25 | Loss: 0.00093571
Iteration 15/25 | Loss: 0.00093571
Iteration 16/25 | Loss: 0.00093571
Iteration 17/25 | Loss: 0.00093571
Iteration 18/25 | Loss: 0.00093571
Iteration 19/25 | Loss: 0.00093571
Iteration 20/25 | Loss: 0.00093571
Iteration 21/25 | Loss: 0.00093571
Iteration 22/25 | Loss: 0.00093571
Iteration 23/25 | Loss: 0.00093571
Iteration 24/25 | Loss: 0.00093571
Iteration 25/25 | Loss: 0.00093571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093571
Iteration 2/1000 | Loss: 0.00022173
Iteration 3/1000 | Loss: 0.00004528
Iteration 4/1000 | Loss: 0.00026575
Iteration 5/1000 | Loss: 0.00003901
Iteration 6/1000 | Loss: 0.00003467
Iteration 7/1000 | Loss: 0.00003288
Iteration 8/1000 | Loss: 0.00003130
Iteration 9/1000 | Loss: 0.00003061
Iteration 10/1000 | Loss: 0.00022776
Iteration 11/1000 | Loss: 0.00009352
Iteration 12/1000 | Loss: 0.00002992
Iteration 13/1000 | Loss: 0.00034853
Iteration 14/1000 | Loss: 0.00004009
Iteration 15/1000 | Loss: 0.00003359
Iteration 16/1000 | Loss: 0.00003111
Iteration 17/1000 | Loss: 0.00003016
Iteration 18/1000 | Loss: 0.00002923
Iteration 19/1000 | Loss: 0.00002875
Iteration 20/1000 | Loss: 0.00002834
Iteration 21/1000 | Loss: 0.00002806
Iteration 22/1000 | Loss: 0.00002755
Iteration 23/1000 | Loss: 0.00002706
Iteration 24/1000 | Loss: 0.00030927
Iteration 25/1000 | Loss: 0.00015457
Iteration 26/1000 | Loss: 0.00003156
Iteration 27/1000 | Loss: 0.00002957
Iteration 28/1000 | Loss: 0.00002824
Iteration 29/1000 | Loss: 0.00002761
Iteration 30/1000 | Loss: 0.00002666
Iteration 31/1000 | Loss: 0.00002593
Iteration 32/1000 | Loss: 0.00002533
Iteration 33/1000 | Loss: 0.00002502
Iteration 34/1000 | Loss: 0.00002494
Iteration 35/1000 | Loss: 0.00002493
Iteration 36/1000 | Loss: 0.00002493
Iteration 37/1000 | Loss: 0.00002493
Iteration 38/1000 | Loss: 0.00002493
Iteration 39/1000 | Loss: 0.00002492
Iteration 40/1000 | Loss: 0.00002487
Iteration 41/1000 | Loss: 0.00002481
Iteration 42/1000 | Loss: 0.00002479
Iteration 43/1000 | Loss: 0.00002479
Iteration 44/1000 | Loss: 0.00002479
Iteration 45/1000 | Loss: 0.00002478
Iteration 46/1000 | Loss: 0.00002478
Iteration 47/1000 | Loss: 0.00002478
Iteration 48/1000 | Loss: 0.00002476
Iteration 49/1000 | Loss: 0.00002476
Iteration 50/1000 | Loss: 0.00002476
Iteration 51/1000 | Loss: 0.00002475
Iteration 52/1000 | Loss: 0.00002475
Iteration 53/1000 | Loss: 0.00002472
Iteration 54/1000 | Loss: 0.00002472
Iteration 55/1000 | Loss: 0.00002470
Iteration 56/1000 | Loss: 0.00002469
Iteration 57/1000 | Loss: 0.00002468
Iteration 58/1000 | Loss: 0.00002468
Iteration 59/1000 | Loss: 0.00002468
Iteration 60/1000 | Loss: 0.00002467
Iteration 61/1000 | Loss: 0.00002467
Iteration 62/1000 | Loss: 0.00002467
Iteration 63/1000 | Loss: 0.00002467
Iteration 64/1000 | Loss: 0.00002467
Iteration 65/1000 | Loss: 0.00002467
Iteration 66/1000 | Loss: 0.00002467
Iteration 67/1000 | Loss: 0.00002467
Iteration 68/1000 | Loss: 0.00002467
Iteration 69/1000 | Loss: 0.00002467
Iteration 70/1000 | Loss: 0.00002467
Iteration 71/1000 | Loss: 0.00002466
Iteration 72/1000 | Loss: 0.00002466
Iteration 73/1000 | Loss: 0.00002466
Iteration 74/1000 | Loss: 0.00002466
Iteration 75/1000 | Loss: 0.00002464
Iteration 76/1000 | Loss: 0.00002463
Iteration 77/1000 | Loss: 0.00002463
Iteration 78/1000 | Loss: 0.00002463
Iteration 79/1000 | Loss: 0.00002462
Iteration 80/1000 | Loss: 0.00002462
Iteration 81/1000 | Loss: 0.00002461
Iteration 82/1000 | Loss: 0.00002461
Iteration 83/1000 | Loss: 0.00002461
Iteration 84/1000 | Loss: 0.00002460
Iteration 85/1000 | Loss: 0.00002460
Iteration 86/1000 | Loss: 0.00002460
Iteration 87/1000 | Loss: 0.00002459
Iteration 88/1000 | Loss: 0.00002459
Iteration 89/1000 | Loss: 0.00002458
Iteration 90/1000 | Loss: 0.00002458
Iteration 91/1000 | Loss: 0.00002458
Iteration 92/1000 | Loss: 0.00002456
Iteration 93/1000 | Loss: 0.00002455
Iteration 94/1000 | Loss: 0.00002455
Iteration 95/1000 | Loss: 0.00002455
Iteration 96/1000 | Loss: 0.00002454
Iteration 97/1000 | Loss: 0.00002454
Iteration 98/1000 | Loss: 0.00002454
Iteration 99/1000 | Loss: 0.00002453
Iteration 100/1000 | Loss: 0.00002453
Iteration 101/1000 | Loss: 0.00002453
Iteration 102/1000 | Loss: 0.00002453
Iteration 103/1000 | Loss: 0.00002453
Iteration 104/1000 | Loss: 0.00002452
Iteration 105/1000 | Loss: 0.00002452
Iteration 106/1000 | Loss: 0.00002452
Iteration 107/1000 | Loss: 0.00002452
Iteration 108/1000 | Loss: 0.00002452
Iteration 109/1000 | Loss: 0.00002451
Iteration 110/1000 | Loss: 0.00002451
Iteration 111/1000 | Loss: 0.00002451
Iteration 112/1000 | Loss: 0.00002451
Iteration 113/1000 | Loss: 0.00002451
Iteration 114/1000 | Loss: 0.00002450
Iteration 115/1000 | Loss: 0.00002450
Iteration 116/1000 | Loss: 0.00002450
Iteration 117/1000 | Loss: 0.00002450
Iteration 118/1000 | Loss: 0.00002450
Iteration 119/1000 | Loss: 0.00002450
Iteration 120/1000 | Loss: 0.00002450
Iteration 121/1000 | Loss: 0.00002450
Iteration 122/1000 | Loss: 0.00002450
Iteration 123/1000 | Loss: 0.00002450
Iteration 124/1000 | Loss: 0.00002450
Iteration 125/1000 | Loss: 0.00002449
Iteration 126/1000 | Loss: 0.00002449
Iteration 127/1000 | Loss: 0.00002449
Iteration 128/1000 | Loss: 0.00002449
Iteration 129/1000 | Loss: 0.00002449
Iteration 130/1000 | Loss: 0.00002449
Iteration 131/1000 | Loss: 0.00002449
Iteration 132/1000 | Loss: 0.00002449
Iteration 133/1000 | Loss: 0.00002449
Iteration 134/1000 | Loss: 0.00002449
Iteration 135/1000 | Loss: 0.00002449
Iteration 136/1000 | Loss: 0.00002449
Iteration 137/1000 | Loss: 0.00002449
Iteration 138/1000 | Loss: 0.00002449
Iteration 139/1000 | Loss: 0.00002449
Iteration 140/1000 | Loss: 0.00002449
Iteration 141/1000 | Loss: 0.00002449
Iteration 142/1000 | Loss: 0.00002449
Iteration 143/1000 | Loss: 0.00002449
Iteration 144/1000 | Loss: 0.00002449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.4494060198776424e-05, 2.4494060198776424e-05, 2.4494060198776424e-05, 2.4494060198776424e-05, 2.4494060198776424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4494060198776424e-05

Optimization complete. Final v2v error: 4.04066276550293 mm

Highest mean error: 6.133086681365967 mm for frame 109

Lowest mean error: 3.387617588043213 mm for frame 0

Saving results

Total time: 102.26457667350769
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435304
Iteration 2/25 | Loss: 0.00127968
Iteration 3/25 | Loss: 0.00122038
Iteration 4/25 | Loss: 0.00121139
Iteration 5/25 | Loss: 0.00120862
Iteration 6/25 | Loss: 0.00120857
Iteration 7/25 | Loss: 0.00120857
Iteration 8/25 | Loss: 0.00120857
Iteration 9/25 | Loss: 0.00120857
Iteration 10/25 | Loss: 0.00120841
Iteration 11/25 | Loss: 0.00120841
Iteration 12/25 | Loss: 0.00120841
Iteration 13/25 | Loss: 0.00120841
Iteration 14/25 | Loss: 0.00120841
Iteration 15/25 | Loss: 0.00120841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012084066402167082, 0.0012084066402167082, 0.0012084066402167082, 0.0012084066402167082, 0.0012084066402167082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012084066402167082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44194865
Iteration 2/25 | Loss: 0.00073035
Iteration 3/25 | Loss: 0.00073035
Iteration 4/25 | Loss: 0.00073035
Iteration 5/25 | Loss: 0.00073035
Iteration 6/25 | Loss: 0.00073035
Iteration 7/25 | Loss: 0.00073035
Iteration 8/25 | Loss: 0.00073035
Iteration 9/25 | Loss: 0.00073035
Iteration 10/25 | Loss: 0.00073034
Iteration 11/25 | Loss: 0.00073034
Iteration 12/25 | Loss: 0.00073034
Iteration 13/25 | Loss: 0.00073034
Iteration 14/25 | Loss: 0.00073034
Iteration 15/25 | Loss: 0.00073034
Iteration 16/25 | Loss: 0.00073034
Iteration 17/25 | Loss: 0.00073034
Iteration 18/25 | Loss: 0.00073034
Iteration 19/25 | Loss: 0.00073034
Iteration 20/25 | Loss: 0.00073034
Iteration 21/25 | Loss: 0.00073034
Iteration 22/25 | Loss: 0.00073034
Iteration 23/25 | Loss: 0.00073034
Iteration 24/25 | Loss: 0.00073034
Iteration 25/25 | Loss: 0.00073034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073034
Iteration 2/1000 | Loss: 0.00002452
Iteration 3/1000 | Loss: 0.00001729
Iteration 4/1000 | Loss: 0.00001547
Iteration 5/1000 | Loss: 0.00001460
Iteration 6/1000 | Loss: 0.00001409
Iteration 7/1000 | Loss: 0.00001383
Iteration 8/1000 | Loss: 0.00001360
Iteration 9/1000 | Loss: 0.00001354
Iteration 10/1000 | Loss: 0.00001349
Iteration 11/1000 | Loss: 0.00001347
Iteration 12/1000 | Loss: 0.00001346
Iteration 13/1000 | Loss: 0.00001336
Iteration 14/1000 | Loss: 0.00001329
Iteration 15/1000 | Loss: 0.00001327
Iteration 16/1000 | Loss: 0.00001327
Iteration 17/1000 | Loss: 0.00001326
Iteration 18/1000 | Loss: 0.00001326
Iteration 19/1000 | Loss: 0.00001326
Iteration 20/1000 | Loss: 0.00001326
Iteration 21/1000 | Loss: 0.00001323
Iteration 22/1000 | Loss: 0.00001316
Iteration 23/1000 | Loss: 0.00001314
Iteration 24/1000 | Loss: 0.00001314
Iteration 25/1000 | Loss: 0.00001313
Iteration 26/1000 | Loss: 0.00001313
Iteration 27/1000 | Loss: 0.00001312
Iteration 28/1000 | Loss: 0.00001312
Iteration 29/1000 | Loss: 0.00001311
Iteration 30/1000 | Loss: 0.00001310
Iteration 31/1000 | Loss: 0.00001309
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001308
Iteration 34/1000 | Loss: 0.00001307
Iteration 35/1000 | Loss: 0.00001307
Iteration 36/1000 | Loss: 0.00001306
Iteration 37/1000 | Loss: 0.00001306
Iteration 38/1000 | Loss: 0.00001303
Iteration 39/1000 | Loss: 0.00001303
Iteration 40/1000 | Loss: 0.00001303
Iteration 41/1000 | Loss: 0.00001303
Iteration 42/1000 | Loss: 0.00001303
Iteration 43/1000 | Loss: 0.00001303
Iteration 44/1000 | Loss: 0.00001303
Iteration 45/1000 | Loss: 0.00001303
Iteration 46/1000 | Loss: 0.00001302
Iteration 47/1000 | Loss: 0.00001302
Iteration 48/1000 | Loss: 0.00001302
Iteration 49/1000 | Loss: 0.00001301
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001298
Iteration 53/1000 | Loss: 0.00001298
Iteration 54/1000 | Loss: 0.00001298
Iteration 55/1000 | Loss: 0.00001298
Iteration 56/1000 | Loss: 0.00001297
Iteration 57/1000 | Loss: 0.00001297
Iteration 58/1000 | Loss: 0.00001296
Iteration 59/1000 | Loss: 0.00001296
Iteration 60/1000 | Loss: 0.00001295
Iteration 61/1000 | Loss: 0.00001295
Iteration 62/1000 | Loss: 0.00001295
Iteration 63/1000 | Loss: 0.00001294
Iteration 64/1000 | Loss: 0.00001294
Iteration 65/1000 | Loss: 0.00001293
Iteration 66/1000 | Loss: 0.00001291
Iteration 67/1000 | Loss: 0.00001291
Iteration 68/1000 | Loss: 0.00001291
Iteration 69/1000 | Loss: 0.00001291
Iteration 70/1000 | Loss: 0.00001291
Iteration 71/1000 | Loss: 0.00001291
Iteration 72/1000 | Loss: 0.00001291
Iteration 73/1000 | Loss: 0.00001291
Iteration 74/1000 | Loss: 0.00001291
Iteration 75/1000 | Loss: 0.00001291
Iteration 76/1000 | Loss: 0.00001291
Iteration 77/1000 | Loss: 0.00001291
Iteration 78/1000 | Loss: 0.00001291
Iteration 79/1000 | Loss: 0.00001291
Iteration 80/1000 | Loss: 0.00001291
Iteration 81/1000 | Loss: 0.00001291
Iteration 82/1000 | Loss: 0.00001291
Iteration 83/1000 | Loss: 0.00001291
Iteration 84/1000 | Loss: 0.00001291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.2908360076835379e-05, 1.2908360076835379e-05, 1.2908360076835379e-05, 1.2908360076835379e-05, 1.2908360076835379e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2908360076835379e-05

Optimization complete. Final v2v error: 3.0461487770080566 mm

Highest mean error: 3.207305431365967 mm for frame 181

Lowest mean error: 2.8841519355773926 mm for frame 151

Saving results

Total time: 31.4458270072937
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413356
Iteration 2/25 | Loss: 0.00127106
Iteration 3/25 | Loss: 0.00122830
Iteration 4/25 | Loss: 0.00122017
Iteration 5/25 | Loss: 0.00121824
Iteration 6/25 | Loss: 0.00121811
Iteration 7/25 | Loss: 0.00121811
Iteration 8/25 | Loss: 0.00121811
Iteration 9/25 | Loss: 0.00121811
Iteration 10/25 | Loss: 0.00121811
Iteration 11/25 | Loss: 0.00121811
Iteration 12/25 | Loss: 0.00121811
Iteration 13/25 | Loss: 0.00121811
Iteration 14/25 | Loss: 0.00121811
Iteration 15/25 | Loss: 0.00121811
Iteration 16/25 | Loss: 0.00121811
Iteration 17/25 | Loss: 0.00121811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012181091587990522, 0.0012181091587990522, 0.0012181091587990522, 0.0012181091587990522, 0.0012181091587990522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012181091587990522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48633361
Iteration 2/25 | Loss: 0.00095410
Iteration 3/25 | Loss: 0.00095410
Iteration 4/25 | Loss: 0.00095410
Iteration 5/25 | Loss: 0.00095410
Iteration 6/25 | Loss: 0.00095410
Iteration 7/25 | Loss: 0.00095410
Iteration 8/25 | Loss: 0.00095410
Iteration 9/25 | Loss: 0.00095410
Iteration 10/25 | Loss: 0.00095410
Iteration 11/25 | Loss: 0.00095410
Iteration 12/25 | Loss: 0.00095410
Iteration 13/25 | Loss: 0.00095410
Iteration 14/25 | Loss: 0.00095410
Iteration 15/25 | Loss: 0.00095410
Iteration 16/25 | Loss: 0.00095410
Iteration 17/25 | Loss: 0.00095410
Iteration 18/25 | Loss: 0.00095410
Iteration 19/25 | Loss: 0.00095410
Iteration 20/25 | Loss: 0.00095410
Iteration 21/25 | Loss: 0.00095410
Iteration 22/25 | Loss: 0.00095410
Iteration 23/25 | Loss: 0.00095410
Iteration 24/25 | Loss: 0.00095410
Iteration 25/25 | Loss: 0.00095410

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095410
Iteration 2/1000 | Loss: 0.00002851
Iteration 3/1000 | Loss: 0.00001967
Iteration 4/1000 | Loss: 0.00001673
Iteration 5/1000 | Loss: 0.00001577
Iteration 6/1000 | Loss: 0.00001518
Iteration 7/1000 | Loss: 0.00001494
Iteration 8/1000 | Loss: 0.00001458
Iteration 9/1000 | Loss: 0.00001451
Iteration 10/1000 | Loss: 0.00001439
Iteration 11/1000 | Loss: 0.00001430
Iteration 12/1000 | Loss: 0.00001409
Iteration 13/1000 | Loss: 0.00001395
Iteration 14/1000 | Loss: 0.00001392
Iteration 15/1000 | Loss: 0.00001392
Iteration 16/1000 | Loss: 0.00001390
Iteration 17/1000 | Loss: 0.00001390
Iteration 18/1000 | Loss: 0.00001389
Iteration 19/1000 | Loss: 0.00001389
Iteration 20/1000 | Loss: 0.00001389
Iteration 21/1000 | Loss: 0.00001388
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001385
Iteration 24/1000 | Loss: 0.00001384
Iteration 25/1000 | Loss: 0.00001383
Iteration 26/1000 | Loss: 0.00001383
Iteration 27/1000 | Loss: 0.00001381
Iteration 28/1000 | Loss: 0.00001380
Iteration 29/1000 | Loss: 0.00001379
Iteration 30/1000 | Loss: 0.00001378
Iteration 31/1000 | Loss: 0.00001377
Iteration 32/1000 | Loss: 0.00001376
Iteration 33/1000 | Loss: 0.00001376
Iteration 34/1000 | Loss: 0.00001374
Iteration 35/1000 | Loss: 0.00001374
Iteration 36/1000 | Loss: 0.00001373
Iteration 37/1000 | Loss: 0.00001373
Iteration 38/1000 | Loss: 0.00001373
Iteration 39/1000 | Loss: 0.00001373
Iteration 40/1000 | Loss: 0.00001373
Iteration 41/1000 | Loss: 0.00001373
Iteration 42/1000 | Loss: 0.00001373
Iteration 43/1000 | Loss: 0.00001373
Iteration 44/1000 | Loss: 0.00001373
Iteration 45/1000 | Loss: 0.00001373
Iteration 46/1000 | Loss: 0.00001373
Iteration 47/1000 | Loss: 0.00001373
Iteration 48/1000 | Loss: 0.00001373
Iteration 49/1000 | Loss: 0.00001373
Iteration 50/1000 | Loss: 0.00001373
Iteration 51/1000 | Loss: 0.00001373
Iteration 52/1000 | Loss: 0.00001373
Iteration 53/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 53. Stopping optimization.
Last 5 losses: [1.3726405995839741e-05, 1.3726405995839741e-05, 1.3726405995839741e-05, 1.3726405995839741e-05, 1.3726405995839741e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3726405995839741e-05

Optimization complete. Final v2v error: 3.1481709480285645 mm

Highest mean error: 3.2557389736175537 mm for frame 9

Lowest mean error: 2.9724597930908203 mm for frame 100

Saving results

Total time: 26.689920902252197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903298
Iteration 2/25 | Loss: 0.00182670
Iteration 3/25 | Loss: 0.00150018
Iteration 4/25 | Loss: 0.00144923
Iteration 5/25 | Loss: 0.00146424
Iteration 6/25 | Loss: 0.00147484
Iteration 7/25 | Loss: 0.00142759
Iteration 8/25 | Loss: 0.00141365
Iteration 9/25 | Loss: 0.00140785
Iteration 10/25 | Loss: 0.00140980
Iteration 11/25 | Loss: 0.00143226
Iteration 12/25 | Loss: 0.00141743
Iteration 13/25 | Loss: 0.00140652
Iteration 14/25 | Loss: 0.00139564
Iteration 15/25 | Loss: 0.00138736
Iteration 16/25 | Loss: 0.00138109
Iteration 17/25 | Loss: 0.00138326
Iteration 18/25 | Loss: 0.00137549
Iteration 19/25 | Loss: 0.00137669
Iteration 20/25 | Loss: 0.00137132
Iteration 21/25 | Loss: 0.00136911
Iteration 22/25 | Loss: 0.00137446
Iteration 23/25 | Loss: 0.00137375
Iteration 24/25 | Loss: 0.00137393
Iteration 25/25 | Loss: 0.00138300

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37625837
Iteration 2/25 | Loss: 0.00179929
Iteration 3/25 | Loss: 0.00179894
Iteration 4/25 | Loss: 0.00179894
Iteration 5/25 | Loss: 0.00179894
Iteration 6/25 | Loss: 0.00179894
Iteration 7/25 | Loss: 0.00179894
Iteration 8/25 | Loss: 0.00179894
Iteration 9/25 | Loss: 0.00179894
Iteration 10/25 | Loss: 0.00179894
Iteration 11/25 | Loss: 0.00179894
Iteration 12/25 | Loss: 0.00179894
Iteration 13/25 | Loss: 0.00179894
Iteration 14/25 | Loss: 0.00179894
Iteration 15/25 | Loss: 0.00179894
Iteration 16/25 | Loss: 0.00179894
Iteration 17/25 | Loss: 0.00179894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017989350017160177, 0.0017989350017160177, 0.0017989350017160177, 0.0017989350017160177, 0.0017989350017160177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017989350017160177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179894
Iteration 2/1000 | Loss: 0.00283304
Iteration 3/1000 | Loss: 0.00171628
Iteration 4/1000 | Loss: 0.00015158
Iteration 5/1000 | Loss: 0.00402177
Iteration 6/1000 | Loss: 0.00019925
Iteration 7/1000 | Loss: 0.00012243
Iteration 8/1000 | Loss: 0.00006268
Iteration 9/1000 | Loss: 0.00023780
Iteration 10/1000 | Loss: 0.00019198
Iteration 11/1000 | Loss: 0.00020669
Iteration 12/1000 | Loss: 0.00020561
Iteration 13/1000 | Loss: 0.00005527
Iteration 14/1000 | Loss: 0.00017241
Iteration 15/1000 | Loss: 0.00013127
Iteration 16/1000 | Loss: 0.00016534
Iteration 17/1000 | Loss: 0.00012911
Iteration 18/1000 | Loss: 0.00012825
Iteration 19/1000 | Loss: 0.00006704
Iteration 20/1000 | Loss: 0.00034041
Iteration 21/1000 | Loss: 0.00014072
Iteration 22/1000 | Loss: 0.00030308
Iteration 23/1000 | Loss: 0.00005660
Iteration 24/1000 | Loss: 0.00004155
Iteration 25/1000 | Loss: 0.00003725
Iteration 26/1000 | Loss: 0.00003532
Iteration 27/1000 | Loss: 0.00003427
Iteration 28/1000 | Loss: 0.00051129
Iteration 29/1000 | Loss: 0.00003254
Iteration 30/1000 | Loss: 0.00003095
Iteration 31/1000 | Loss: 0.00003006
Iteration 32/1000 | Loss: 0.00002928
Iteration 33/1000 | Loss: 0.00002869
Iteration 34/1000 | Loss: 0.00002831
Iteration 35/1000 | Loss: 0.00002793
Iteration 36/1000 | Loss: 0.00002758
Iteration 37/1000 | Loss: 0.00002738
Iteration 38/1000 | Loss: 0.00002718
Iteration 39/1000 | Loss: 0.00002710
Iteration 40/1000 | Loss: 0.00002701
Iteration 41/1000 | Loss: 0.00002700
Iteration 42/1000 | Loss: 0.00002698
Iteration 43/1000 | Loss: 0.00002697
Iteration 44/1000 | Loss: 0.00002692
Iteration 45/1000 | Loss: 0.00002689
Iteration 46/1000 | Loss: 0.00002689
Iteration 47/1000 | Loss: 0.00002688
Iteration 48/1000 | Loss: 0.00002688
Iteration 49/1000 | Loss: 0.00002688
Iteration 50/1000 | Loss: 0.00002687
Iteration 51/1000 | Loss: 0.00002687
Iteration 52/1000 | Loss: 0.00002687
Iteration 53/1000 | Loss: 0.00002686
Iteration 54/1000 | Loss: 0.00002684
Iteration 55/1000 | Loss: 0.00002681
Iteration 56/1000 | Loss: 0.00002680
Iteration 57/1000 | Loss: 0.00002679
Iteration 58/1000 | Loss: 0.00002678
Iteration 59/1000 | Loss: 0.00002678
Iteration 60/1000 | Loss: 0.00002678
Iteration 61/1000 | Loss: 0.00002677
Iteration 62/1000 | Loss: 0.00002677
Iteration 63/1000 | Loss: 0.00002676
Iteration 64/1000 | Loss: 0.00002674
Iteration 65/1000 | Loss: 0.00002674
Iteration 66/1000 | Loss: 0.00002670
Iteration 67/1000 | Loss: 0.00002670
Iteration 68/1000 | Loss: 0.00002667
Iteration 69/1000 | Loss: 0.00002667
Iteration 70/1000 | Loss: 0.00002667
Iteration 71/1000 | Loss: 0.00002667
Iteration 72/1000 | Loss: 0.00002667
Iteration 73/1000 | Loss: 0.00002667
Iteration 74/1000 | Loss: 0.00002667
Iteration 75/1000 | Loss: 0.00002667
Iteration 76/1000 | Loss: 0.00002666
Iteration 77/1000 | Loss: 0.00002666
Iteration 78/1000 | Loss: 0.00002666
Iteration 79/1000 | Loss: 0.00002665
Iteration 80/1000 | Loss: 0.00002665
Iteration 81/1000 | Loss: 0.00002665
Iteration 82/1000 | Loss: 0.00002665
Iteration 83/1000 | Loss: 0.00002664
Iteration 84/1000 | Loss: 0.00002664
Iteration 85/1000 | Loss: 0.00002664
Iteration 86/1000 | Loss: 0.00002663
Iteration 87/1000 | Loss: 0.00002663
Iteration 88/1000 | Loss: 0.00002663
Iteration 89/1000 | Loss: 0.00002663
Iteration 90/1000 | Loss: 0.00002663
Iteration 91/1000 | Loss: 0.00002663
Iteration 92/1000 | Loss: 0.00002663
Iteration 93/1000 | Loss: 0.00002662
Iteration 94/1000 | Loss: 0.00002662
Iteration 95/1000 | Loss: 0.00002662
Iteration 96/1000 | Loss: 0.00002661
Iteration 97/1000 | Loss: 0.00002661
Iteration 98/1000 | Loss: 0.00002661
Iteration 99/1000 | Loss: 0.00002661
Iteration 100/1000 | Loss: 0.00002661
Iteration 101/1000 | Loss: 0.00002660
Iteration 102/1000 | Loss: 0.00002660
Iteration 103/1000 | Loss: 0.00002660
Iteration 104/1000 | Loss: 0.00002660
Iteration 105/1000 | Loss: 0.00002659
Iteration 106/1000 | Loss: 0.00002659
Iteration 107/1000 | Loss: 0.00002659
Iteration 108/1000 | Loss: 0.00002659
Iteration 109/1000 | Loss: 0.00002658
Iteration 110/1000 | Loss: 0.00002658
Iteration 111/1000 | Loss: 0.00002658
Iteration 112/1000 | Loss: 0.00002658
Iteration 113/1000 | Loss: 0.00002657
Iteration 114/1000 | Loss: 0.00002657
Iteration 115/1000 | Loss: 0.00002657
Iteration 116/1000 | Loss: 0.00002657
Iteration 117/1000 | Loss: 0.00002657
Iteration 118/1000 | Loss: 0.00002657
Iteration 119/1000 | Loss: 0.00002656
Iteration 120/1000 | Loss: 0.00002656
Iteration 121/1000 | Loss: 0.00002656
Iteration 122/1000 | Loss: 0.00002656
Iteration 123/1000 | Loss: 0.00002656
Iteration 124/1000 | Loss: 0.00002656
Iteration 125/1000 | Loss: 0.00002655
Iteration 126/1000 | Loss: 0.00002655
Iteration 127/1000 | Loss: 0.00002655
Iteration 128/1000 | Loss: 0.00002654
Iteration 129/1000 | Loss: 0.00002654
Iteration 130/1000 | Loss: 0.00002654
Iteration 131/1000 | Loss: 0.00002653
Iteration 132/1000 | Loss: 0.00002653
Iteration 133/1000 | Loss: 0.00002653
Iteration 134/1000 | Loss: 0.00002652
Iteration 135/1000 | Loss: 0.00002652
Iteration 136/1000 | Loss: 0.00002652
Iteration 137/1000 | Loss: 0.00002652
Iteration 138/1000 | Loss: 0.00002651
Iteration 139/1000 | Loss: 0.00002651
Iteration 140/1000 | Loss: 0.00002651
Iteration 141/1000 | Loss: 0.00002650
Iteration 142/1000 | Loss: 0.00002650
Iteration 143/1000 | Loss: 0.00002650
Iteration 144/1000 | Loss: 0.00002650
Iteration 145/1000 | Loss: 0.00002649
Iteration 146/1000 | Loss: 0.00002649
Iteration 147/1000 | Loss: 0.00002649
Iteration 148/1000 | Loss: 0.00002649
Iteration 149/1000 | Loss: 0.00002649
Iteration 150/1000 | Loss: 0.00002648
Iteration 151/1000 | Loss: 0.00002648
Iteration 152/1000 | Loss: 0.00002648
Iteration 153/1000 | Loss: 0.00002648
Iteration 154/1000 | Loss: 0.00002647
Iteration 155/1000 | Loss: 0.00002647
Iteration 156/1000 | Loss: 0.00002647
Iteration 157/1000 | Loss: 0.00002647
Iteration 158/1000 | Loss: 0.00002646
Iteration 159/1000 | Loss: 0.00002646
Iteration 160/1000 | Loss: 0.00002646
Iteration 161/1000 | Loss: 0.00002646
Iteration 162/1000 | Loss: 0.00002646
Iteration 163/1000 | Loss: 0.00002646
Iteration 164/1000 | Loss: 0.00002646
Iteration 165/1000 | Loss: 0.00002646
Iteration 166/1000 | Loss: 0.00002645
Iteration 167/1000 | Loss: 0.00002645
Iteration 168/1000 | Loss: 0.00002645
Iteration 169/1000 | Loss: 0.00002645
Iteration 170/1000 | Loss: 0.00002645
Iteration 171/1000 | Loss: 0.00002645
Iteration 172/1000 | Loss: 0.00002645
Iteration 173/1000 | Loss: 0.00002645
Iteration 174/1000 | Loss: 0.00002645
Iteration 175/1000 | Loss: 0.00002645
Iteration 176/1000 | Loss: 0.00002644
Iteration 177/1000 | Loss: 0.00002644
Iteration 178/1000 | Loss: 0.00002644
Iteration 179/1000 | Loss: 0.00002644
Iteration 180/1000 | Loss: 0.00002644
Iteration 181/1000 | Loss: 0.00002644
Iteration 182/1000 | Loss: 0.00002644
Iteration 183/1000 | Loss: 0.00002644
Iteration 184/1000 | Loss: 0.00002644
Iteration 185/1000 | Loss: 0.00002644
Iteration 186/1000 | Loss: 0.00002644
Iteration 187/1000 | Loss: 0.00002644
Iteration 188/1000 | Loss: 0.00002644
Iteration 189/1000 | Loss: 0.00002644
Iteration 190/1000 | Loss: 0.00002644
Iteration 191/1000 | Loss: 0.00002644
Iteration 192/1000 | Loss: 0.00002644
Iteration 193/1000 | Loss: 0.00002644
Iteration 194/1000 | Loss: 0.00002644
Iteration 195/1000 | Loss: 0.00002644
Iteration 196/1000 | Loss: 0.00002644
Iteration 197/1000 | Loss: 0.00002644
Iteration 198/1000 | Loss: 0.00002644
Iteration 199/1000 | Loss: 0.00002644
Iteration 200/1000 | Loss: 0.00002644
Iteration 201/1000 | Loss: 0.00002644
Iteration 202/1000 | Loss: 0.00002644
Iteration 203/1000 | Loss: 0.00002644
Iteration 204/1000 | Loss: 0.00002644
Iteration 205/1000 | Loss: 0.00002644
Iteration 206/1000 | Loss: 0.00002644
Iteration 207/1000 | Loss: 0.00002644
Iteration 208/1000 | Loss: 0.00002644
Iteration 209/1000 | Loss: 0.00002644
Iteration 210/1000 | Loss: 0.00002644
Iteration 211/1000 | Loss: 0.00002644
Iteration 212/1000 | Loss: 0.00002644
Iteration 213/1000 | Loss: 0.00002644
Iteration 214/1000 | Loss: 0.00002644
Iteration 215/1000 | Loss: 0.00002644
Iteration 216/1000 | Loss: 0.00002644
Iteration 217/1000 | Loss: 0.00002644
Iteration 218/1000 | Loss: 0.00002644
Iteration 219/1000 | Loss: 0.00002644
Iteration 220/1000 | Loss: 0.00002644
Iteration 221/1000 | Loss: 0.00002644
Iteration 222/1000 | Loss: 0.00002644
Iteration 223/1000 | Loss: 0.00002644
Iteration 224/1000 | Loss: 0.00002644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [2.6443680326337926e-05, 2.6443680326337926e-05, 2.6443680326337926e-05, 2.6443680326337926e-05, 2.6443680326337926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6443680326337926e-05

Optimization complete. Final v2v error: 4.255790710449219 mm

Highest mean error: 6.03452730178833 mm for frame 97

Lowest mean error: 3.298837423324585 mm for frame 144

Saving results

Total time: 115.86318445205688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773841
Iteration 2/25 | Loss: 0.00161372
Iteration 3/25 | Loss: 0.00138339
Iteration 4/25 | Loss: 0.00136899
Iteration 5/25 | Loss: 0.00136589
Iteration 6/25 | Loss: 0.00136548
Iteration 7/25 | Loss: 0.00136548
Iteration 8/25 | Loss: 0.00136548
Iteration 9/25 | Loss: 0.00136548
Iteration 10/25 | Loss: 0.00136548
Iteration 11/25 | Loss: 0.00136548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001365481410175562, 0.001365481410175562, 0.001365481410175562, 0.001365481410175562, 0.001365481410175562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001365481410175562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42508459
Iteration 2/25 | Loss: 0.00082149
Iteration 3/25 | Loss: 0.00082145
Iteration 4/25 | Loss: 0.00082145
Iteration 5/25 | Loss: 0.00082145
Iteration 6/25 | Loss: 0.00082145
Iteration 7/25 | Loss: 0.00082145
Iteration 8/25 | Loss: 0.00082145
Iteration 9/25 | Loss: 0.00082145
Iteration 10/25 | Loss: 0.00082145
Iteration 11/25 | Loss: 0.00082145
Iteration 12/25 | Loss: 0.00082145
Iteration 13/25 | Loss: 0.00082145
Iteration 14/25 | Loss: 0.00082145
Iteration 15/25 | Loss: 0.00082145
Iteration 16/25 | Loss: 0.00082145
Iteration 17/25 | Loss: 0.00082145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008214482804760337, 0.0008214482804760337, 0.0008214482804760337, 0.0008214482804760337, 0.0008214482804760337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008214482804760337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082145
Iteration 2/1000 | Loss: 0.00004730
Iteration 3/1000 | Loss: 0.00003555
Iteration 4/1000 | Loss: 0.00003195
Iteration 5/1000 | Loss: 0.00003022
Iteration 6/1000 | Loss: 0.00002937
Iteration 7/1000 | Loss: 0.00002882
Iteration 8/1000 | Loss: 0.00002825
Iteration 9/1000 | Loss: 0.00002787
Iteration 10/1000 | Loss: 0.00002766
Iteration 11/1000 | Loss: 0.00002736
Iteration 12/1000 | Loss: 0.00002707
Iteration 13/1000 | Loss: 0.00002679
Iteration 14/1000 | Loss: 0.00002659
Iteration 15/1000 | Loss: 0.00002658
Iteration 16/1000 | Loss: 0.00002653
Iteration 17/1000 | Loss: 0.00002636
Iteration 18/1000 | Loss: 0.00002633
Iteration 19/1000 | Loss: 0.00002633
Iteration 20/1000 | Loss: 0.00002631
Iteration 21/1000 | Loss: 0.00002630
Iteration 22/1000 | Loss: 0.00002630
Iteration 23/1000 | Loss: 0.00002629
Iteration 24/1000 | Loss: 0.00002628
Iteration 25/1000 | Loss: 0.00002628
Iteration 26/1000 | Loss: 0.00002628
Iteration 27/1000 | Loss: 0.00002628
Iteration 28/1000 | Loss: 0.00002627
Iteration 29/1000 | Loss: 0.00002627
Iteration 30/1000 | Loss: 0.00002627
Iteration 31/1000 | Loss: 0.00002627
Iteration 32/1000 | Loss: 0.00002627
Iteration 33/1000 | Loss: 0.00002627
Iteration 34/1000 | Loss: 0.00002627
Iteration 35/1000 | Loss: 0.00002627
Iteration 36/1000 | Loss: 0.00002627
Iteration 37/1000 | Loss: 0.00002626
Iteration 38/1000 | Loss: 0.00002626
Iteration 39/1000 | Loss: 0.00002626
Iteration 40/1000 | Loss: 0.00002626
Iteration 41/1000 | Loss: 0.00002626
Iteration 42/1000 | Loss: 0.00002625
Iteration 43/1000 | Loss: 0.00002625
Iteration 44/1000 | Loss: 0.00002625
Iteration 45/1000 | Loss: 0.00002624
Iteration 46/1000 | Loss: 0.00002623
Iteration 47/1000 | Loss: 0.00002623
Iteration 48/1000 | Loss: 0.00002623
Iteration 49/1000 | Loss: 0.00002623
Iteration 50/1000 | Loss: 0.00002623
Iteration 51/1000 | Loss: 0.00002623
Iteration 52/1000 | Loss: 0.00002623
Iteration 53/1000 | Loss: 0.00002623
Iteration 54/1000 | Loss: 0.00002623
Iteration 55/1000 | Loss: 0.00002623
Iteration 56/1000 | Loss: 0.00002622
Iteration 57/1000 | Loss: 0.00002621
Iteration 58/1000 | Loss: 0.00002621
Iteration 59/1000 | Loss: 0.00002620
Iteration 60/1000 | Loss: 0.00002620
Iteration 61/1000 | Loss: 0.00002619
Iteration 62/1000 | Loss: 0.00002619
Iteration 63/1000 | Loss: 0.00002618
Iteration 64/1000 | Loss: 0.00002618
Iteration 65/1000 | Loss: 0.00002618
Iteration 66/1000 | Loss: 0.00002617
Iteration 67/1000 | Loss: 0.00002616
Iteration 68/1000 | Loss: 0.00002616
Iteration 69/1000 | Loss: 0.00002616
Iteration 70/1000 | Loss: 0.00002616
Iteration 71/1000 | Loss: 0.00002616
Iteration 72/1000 | Loss: 0.00002616
Iteration 73/1000 | Loss: 0.00002616
Iteration 74/1000 | Loss: 0.00002616
Iteration 75/1000 | Loss: 0.00002616
Iteration 76/1000 | Loss: 0.00002616
Iteration 77/1000 | Loss: 0.00002616
Iteration 78/1000 | Loss: 0.00002615
Iteration 79/1000 | Loss: 0.00002615
Iteration 80/1000 | Loss: 0.00002615
Iteration 81/1000 | Loss: 0.00002614
Iteration 82/1000 | Loss: 0.00002613
Iteration 83/1000 | Loss: 0.00002613
Iteration 84/1000 | Loss: 0.00002613
Iteration 85/1000 | Loss: 0.00002613
Iteration 86/1000 | Loss: 0.00002613
Iteration 87/1000 | Loss: 0.00002613
Iteration 88/1000 | Loss: 0.00002613
Iteration 89/1000 | Loss: 0.00002613
Iteration 90/1000 | Loss: 0.00002613
Iteration 91/1000 | Loss: 0.00002612
Iteration 92/1000 | Loss: 0.00002612
Iteration 93/1000 | Loss: 0.00002612
Iteration 94/1000 | Loss: 0.00002612
Iteration 95/1000 | Loss: 0.00002612
Iteration 96/1000 | Loss: 0.00002611
Iteration 97/1000 | Loss: 0.00002611
Iteration 98/1000 | Loss: 0.00002611
Iteration 99/1000 | Loss: 0.00002610
Iteration 100/1000 | Loss: 0.00002610
Iteration 101/1000 | Loss: 0.00002610
Iteration 102/1000 | Loss: 0.00002610
Iteration 103/1000 | Loss: 0.00002610
Iteration 104/1000 | Loss: 0.00002609
Iteration 105/1000 | Loss: 0.00002609
Iteration 106/1000 | Loss: 0.00002609
Iteration 107/1000 | Loss: 0.00002608
Iteration 108/1000 | Loss: 0.00002608
Iteration 109/1000 | Loss: 0.00002607
Iteration 110/1000 | Loss: 0.00002607
Iteration 111/1000 | Loss: 0.00002607
Iteration 112/1000 | Loss: 0.00002607
Iteration 113/1000 | Loss: 0.00002607
Iteration 114/1000 | Loss: 0.00002607
Iteration 115/1000 | Loss: 0.00002607
Iteration 116/1000 | Loss: 0.00002607
Iteration 117/1000 | Loss: 0.00002607
Iteration 118/1000 | Loss: 0.00002606
Iteration 119/1000 | Loss: 0.00002606
Iteration 120/1000 | Loss: 0.00002606
Iteration 121/1000 | Loss: 0.00002606
Iteration 122/1000 | Loss: 0.00002606
Iteration 123/1000 | Loss: 0.00002606
Iteration 124/1000 | Loss: 0.00002606
Iteration 125/1000 | Loss: 0.00002606
Iteration 126/1000 | Loss: 0.00002606
Iteration 127/1000 | Loss: 0.00002606
Iteration 128/1000 | Loss: 0.00002606
Iteration 129/1000 | Loss: 0.00002606
Iteration 130/1000 | Loss: 0.00002606
Iteration 131/1000 | Loss: 0.00002606
Iteration 132/1000 | Loss: 0.00002606
Iteration 133/1000 | Loss: 0.00002606
Iteration 134/1000 | Loss: 0.00002606
Iteration 135/1000 | Loss: 0.00002606
Iteration 136/1000 | Loss: 0.00002606
Iteration 137/1000 | Loss: 0.00002606
Iteration 138/1000 | Loss: 0.00002606
Iteration 139/1000 | Loss: 0.00002606
Iteration 140/1000 | Loss: 0.00002606
Iteration 141/1000 | Loss: 0.00002606
Iteration 142/1000 | Loss: 0.00002606
Iteration 143/1000 | Loss: 0.00002606
Iteration 144/1000 | Loss: 0.00002606
Iteration 145/1000 | Loss: 0.00002606
Iteration 146/1000 | Loss: 0.00002606
Iteration 147/1000 | Loss: 0.00002606
Iteration 148/1000 | Loss: 0.00002606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.6061179596581496e-05, 2.6061179596581496e-05, 2.6061179596581496e-05, 2.6061179596581496e-05, 2.6061179596581496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6061179596581496e-05

Optimization complete. Final v2v error: 4.254567623138428 mm

Highest mean error: 4.501626968383789 mm for frame 75

Lowest mean error: 4.112520694732666 mm for frame 5

Saving results

Total time: 38.849427461624146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408636
Iteration 2/25 | Loss: 0.00126010
Iteration 3/25 | Loss: 0.00120164
Iteration 4/25 | Loss: 0.00119173
Iteration 5/25 | Loss: 0.00118805
Iteration 6/25 | Loss: 0.00118755
Iteration 7/25 | Loss: 0.00118755
Iteration 8/25 | Loss: 0.00118755
Iteration 9/25 | Loss: 0.00118755
Iteration 10/25 | Loss: 0.00118755
Iteration 11/25 | Loss: 0.00118755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011875474592670798, 0.0011875474592670798, 0.0011875474592670798, 0.0011875474592670798, 0.0011875474592670798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011875474592670798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.91312838
Iteration 2/25 | Loss: 0.00076769
Iteration 3/25 | Loss: 0.00076768
Iteration 4/25 | Loss: 0.00076768
Iteration 5/25 | Loss: 0.00076768
Iteration 6/25 | Loss: 0.00076768
Iteration 7/25 | Loss: 0.00076768
Iteration 8/25 | Loss: 0.00076768
Iteration 9/25 | Loss: 0.00076768
Iteration 10/25 | Loss: 0.00076768
Iteration 11/25 | Loss: 0.00076768
Iteration 12/25 | Loss: 0.00076768
Iteration 13/25 | Loss: 0.00076768
Iteration 14/25 | Loss: 0.00076768
Iteration 15/25 | Loss: 0.00076768
Iteration 16/25 | Loss: 0.00076768
Iteration 17/25 | Loss: 0.00076768
Iteration 18/25 | Loss: 0.00076768
Iteration 19/25 | Loss: 0.00076768
Iteration 20/25 | Loss: 0.00076768
Iteration 21/25 | Loss: 0.00076768
Iteration 22/25 | Loss: 0.00076768
Iteration 23/25 | Loss: 0.00076768
Iteration 24/25 | Loss: 0.00076768
Iteration 25/25 | Loss: 0.00076768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076768
Iteration 2/1000 | Loss: 0.00002679
Iteration 3/1000 | Loss: 0.00001879
Iteration 4/1000 | Loss: 0.00001562
Iteration 5/1000 | Loss: 0.00001469
Iteration 6/1000 | Loss: 0.00001401
Iteration 7/1000 | Loss: 0.00001364
Iteration 8/1000 | Loss: 0.00001331
Iteration 9/1000 | Loss: 0.00001324
Iteration 10/1000 | Loss: 0.00001318
Iteration 11/1000 | Loss: 0.00001298
Iteration 12/1000 | Loss: 0.00001293
Iteration 13/1000 | Loss: 0.00001281
Iteration 14/1000 | Loss: 0.00001278
Iteration 15/1000 | Loss: 0.00001278
Iteration 16/1000 | Loss: 0.00001277
Iteration 17/1000 | Loss: 0.00001274
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001259
Iteration 20/1000 | Loss: 0.00001257
Iteration 21/1000 | Loss: 0.00001256
Iteration 22/1000 | Loss: 0.00001256
Iteration 23/1000 | Loss: 0.00001254
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001254
Iteration 27/1000 | Loss: 0.00001253
Iteration 28/1000 | Loss: 0.00001253
Iteration 29/1000 | Loss: 0.00001253
Iteration 30/1000 | Loss: 0.00001252
Iteration 31/1000 | Loss: 0.00001251
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001249
Iteration 34/1000 | Loss: 0.00001249
Iteration 35/1000 | Loss: 0.00001249
Iteration 36/1000 | Loss: 0.00001249
Iteration 37/1000 | Loss: 0.00001249
Iteration 38/1000 | Loss: 0.00001248
Iteration 39/1000 | Loss: 0.00001248
Iteration 40/1000 | Loss: 0.00001248
Iteration 41/1000 | Loss: 0.00001248
Iteration 42/1000 | Loss: 0.00001248
Iteration 43/1000 | Loss: 0.00001247
Iteration 44/1000 | Loss: 0.00001247
Iteration 45/1000 | Loss: 0.00001245
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001245
Iteration 48/1000 | Loss: 0.00001245
Iteration 49/1000 | Loss: 0.00001245
Iteration 50/1000 | Loss: 0.00001245
Iteration 51/1000 | Loss: 0.00001245
Iteration 52/1000 | Loss: 0.00001245
Iteration 53/1000 | Loss: 0.00001245
Iteration 54/1000 | Loss: 0.00001244
Iteration 55/1000 | Loss: 0.00001244
Iteration 56/1000 | Loss: 0.00001244
Iteration 57/1000 | Loss: 0.00001244
Iteration 58/1000 | Loss: 0.00001244
Iteration 59/1000 | Loss: 0.00001243
Iteration 60/1000 | Loss: 0.00001242
Iteration 61/1000 | Loss: 0.00001242
Iteration 62/1000 | Loss: 0.00001242
Iteration 63/1000 | Loss: 0.00001241
Iteration 64/1000 | Loss: 0.00001241
Iteration 65/1000 | Loss: 0.00001241
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001239
Iteration 69/1000 | Loss: 0.00001239
Iteration 70/1000 | Loss: 0.00001239
Iteration 71/1000 | Loss: 0.00001238
Iteration 72/1000 | Loss: 0.00001238
Iteration 73/1000 | Loss: 0.00001238
Iteration 74/1000 | Loss: 0.00001237
Iteration 75/1000 | Loss: 0.00001237
Iteration 76/1000 | Loss: 0.00001237
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001236
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001235
Iteration 84/1000 | Loss: 0.00001235
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001233
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001232
Iteration 91/1000 | Loss: 0.00001232
Iteration 92/1000 | Loss: 0.00001231
Iteration 93/1000 | Loss: 0.00001231
Iteration 94/1000 | Loss: 0.00001230
Iteration 95/1000 | Loss: 0.00001230
Iteration 96/1000 | Loss: 0.00001230
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001228
Iteration 101/1000 | Loss: 0.00001228
Iteration 102/1000 | Loss: 0.00001228
Iteration 103/1000 | Loss: 0.00001227
Iteration 104/1000 | Loss: 0.00001227
Iteration 105/1000 | Loss: 0.00001227
Iteration 106/1000 | Loss: 0.00001227
Iteration 107/1000 | Loss: 0.00001226
Iteration 108/1000 | Loss: 0.00001226
Iteration 109/1000 | Loss: 0.00001226
Iteration 110/1000 | Loss: 0.00001226
Iteration 111/1000 | Loss: 0.00001225
Iteration 112/1000 | Loss: 0.00001225
Iteration 113/1000 | Loss: 0.00001224
Iteration 114/1000 | Loss: 0.00001224
Iteration 115/1000 | Loss: 0.00001224
Iteration 116/1000 | Loss: 0.00001224
Iteration 117/1000 | Loss: 0.00001223
Iteration 118/1000 | Loss: 0.00001223
Iteration 119/1000 | Loss: 0.00001223
Iteration 120/1000 | Loss: 0.00001223
Iteration 121/1000 | Loss: 0.00001223
Iteration 122/1000 | Loss: 0.00001223
Iteration 123/1000 | Loss: 0.00001223
Iteration 124/1000 | Loss: 0.00001223
Iteration 125/1000 | Loss: 0.00001223
Iteration 126/1000 | Loss: 0.00001222
Iteration 127/1000 | Loss: 0.00001222
Iteration 128/1000 | Loss: 0.00001222
Iteration 129/1000 | Loss: 0.00001222
Iteration 130/1000 | Loss: 0.00001222
Iteration 131/1000 | Loss: 0.00001222
Iteration 132/1000 | Loss: 0.00001222
Iteration 133/1000 | Loss: 0.00001222
Iteration 134/1000 | Loss: 0.00001222
Iteration 135/1000 | Loss: 0.00001222
Iteration 136/1000 | Loss: 0.00001222
Iteration 137/1000 | Loss: 0.00001222
Iteration 138/1000 | Loss: 0.00001222
Iteration 139/1000 | Loss: 0.00001222
Iteration 140/1000 | Loss: 0.00001222
Iteration 141/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.2223709745740052e-05, 1.2223709745740052e-05, 1.2223709745740052e-05, 1.2223709745740052e-05, 1.2223709745740052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2223709745740052e-05

Optimization complete. Final v2v error: 2.99259614944458 mm

Highest mean error: 3.3546059131622314 mm for frame 82

Lowest mean error: 2.774221658706665 mm for frame 27

Saving results

Total time: 35.47557997703552
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489733
Iteration 2/25 | Loss: 0.00135071
Iteration 3/25 | Loss: 0.00124748
Iteration 4/25 | Loss: 0.00123403
Iteration 5/25 | Loss: 0.00123079
Iteration 6/25 | Loss: 0.00123079
Iteration 7/25 | Loss: 0.00123079
Iteration 8/25 | Loss: 0.00123079
Iteration 9/25 | Loss: 0.00123079
Iteration 10/25 | Loss: 0.00123079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012307889992371202, 0.0012307889992371202, 0.0012307889992371202, 0.0012307889992371202, 0.0012307889992371202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012307889992371202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.75025797
Iteration 2/25 | Loss: 0.00079586
Iteration 3/25 | Loss: 0.00079585
Iteration 4/25 | Loss: 0.00079585
Iteration 5/25 | Loss: 0.00079585
Iteration 6/25 | Loss: 0.00079585
Iteration 7/25 | Loss: 0.00079585
Iteration 8/25 | Loss: 0.00079585
Iteration 9/25 | Loss: 0.00079585
Iteration 10/25 | Loss: 0.00079585
Iteration 11/25 | Loss: 0.00079585
Iteration 12/25 | Loss: 0.00079585
Iteration 13/25 | Loss: 0.00079585
Iteration 14/25 | Loss: 0.00079585
Iteration 15/25 | Loss: 0.00079585
Iteration 16/25 | Loss: 0.00079585
Iteration 17/25 | Loss: 0.00079585
Iteration 18/25 | Loss: 0.00079585
Iteration 19/25 | Loss: 0.00079585
Iteration 20/25 | Loss: 0.00079585
Iteration 21/25 | Loss: 0.00079585
Iteration 22/25 | Loss: 0.00079585
Iteration 23/25 | Loss: 0.00079585
Iteration 24/25 | Loss: 0.00079585
Iteration 25/25 | Loss: 0.00079585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079585
Iteration 2/1000 | Loss: 0.00002453
Iteration 3/1000 | Loss: 0.00001884
Iteration 4/1000 | Loss: 0.00001725
Iteration 5/1000 | Loss: 0.00001628
Iteration 6/1000 | Loss: 0.00001572
Iteration 7/1000 | Loss: 0.00001534
Iteration 8/1000 | Loss: 0.00001505
Iteration 9/1000 | Loss: 0.00001479
Iteration 10/1000 | Loss: 0.00001461
Iteration 11/1000 | Loss: 0.00001460
Iteration 12/1000 | Loss: 0.00001448
Iteration 13/1000 | Loss: 0.00001446
Iteration 14/1000 | Loss: 0.00001440
Iteration 15/1000 | Loss: 0.00001437
Iteration 16/1000 | Loss: 0.00001435
Iteration 17/1000 | Loss: 0.00001435
Iteration 18/1000 | Loss: 0.00001434
Iteration 19/1000 | Loss: 0.00001432
Iteration 20/1000 | Loss: 0.00001432
Iteration 21/1000 | Loss: 0.00001431
Iteration 22/1000 | Loss: 0.00001429
Iteration 23/1000 | Loss: 0.00001428
Iteration 24/1000 | Loss: 0.00001428
Iteration 25/1000 | Loss: 0.00001427
Iteration 26/1000 | Loss: 0.00001426
Iteration 27/1000 | Loss: 0.00001425
Iteration 28/1000 | Loss: 0.00001425
Iteration 29/1000 | Loss: 0.00001424
Iteration 30/1000 | Loss: 0.00001419
Iteration 31/1000 | Loss: 0.00001419
Iteration 32/1000 | Loss: 0.00001414
Iteration 33/1000 | Loss: 0.00001413
Iteration 34/1000 | Loss: 0.00001412
Iteration 35/1000 | Loss: 0.00001412
Iteration 36/1000 | Loss: 0.00001411
Iteration 37/1000 | Loss: 0.00001411
Iteration 38/1000 | Loss: 0.00001410
Iteration 39/1000 | Loss: 0.00001406
Iteration 40/1000 | Loss: 0.00001405
Iteration 41/1000 | Loss: 0.00001404
Iteration 42/1000 | Loss: 0.00001404
Iteration 43/1000 | Loss: 0.00001404
Iteration 44/1000 | Loss: 0.00001404
Iteration 45/1000 | Loss: 0.00001404
Iteration 46/1000 | Loss: 0.00001403
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001402
Iteration 49/1000 | Loss: 0.00001402
Iteration 50/1000 | Loss: 0.00001401
Iteration 51/1000 | Loss: 0.00001401
Iteration 52/1000 | Loss: 0.00001401
Iteration 53/1000 | Loss: 0.00001401
Iteration 54/1000 | Loss: 0.00001400
Iteration 55/1000 | Loss: 0.00001400
Iteration 56/1000 | Loss: 0.00001400
Iteration 57/1000 | Loss: 0.00001400
Iteration 58/1000 | Loss: 0.00001399
Iteration 59/1000 | Loss: 0.00001399
Iteration 60/1000 | Loss: 0.00001399
Iteration 61/1000 | Loss: 0.00001399
Iteration 62/1000 | Loss: 0.00001398
Iteration 63/1000 | Loss: 0.00001398
Iteration 64/1000 | Loss: 0.00001398
Iteration 65/1000 | Loss: 0.00001398
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001397
Iteration 68/1000 | Loss: 0.00001397
Iteration 69/1000 | Loss: 0.00001396
Iteration 70/1000 | Loss: 0.00001396
Iteration 71/1000 | Loss: 0.00001396
Iteration 72/1000 | Loss: 0.00001396
Iteration 73/1000 | Loss: 0.00001395
Iteration 74/1000 | Loss: 0.00001395
Iteration 75/1000 | Loss: 0.00001394
Iteration 76/1000 | Loss: 0.00001394
Iteration 77/1000 | Loss: 0.00001394
Iteration 78/1000 | Loss: 0.00001393
Iteration 79/1000 | Loss: 0.00001393
Iteration 80/1000 | Loss: 0.00001393
Iteration 81/1000 | Loss: 0.00001393
Iteration 82/1000 | Loss: 0.00001393
Iteration 83/1000 | Loss: 0.00001393
Iteration 84/1000 | Loss: 0.00001392
Iteration 85/1000 | Loss: 0.00001392
Iteration 86/1000 | Loss: 0.00001392
Iteration 87/1000 | Loss: 0.00001392
Iteration 88/1000 | Loss: 0.00001391
Iteration 89/1000 | Loss: 0.00001391
Iteration 90/1000 | Loss: 0.00001390
Iteration 91/1000 | Loss: 0.00001390
Iteration 92/1000 | Loss: 0.00001389
Iteration 93/1000 | Loss: 0.00001389
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001388
Iteration 97/1000 | Loss: 0.00001388
Iteration 98/1000 | Loss: 0.00001387
Iteration 99/1000 | Loss: 0.00001387
Iteration 100/1000 | Loss: 0.00001386
Iteration 101/1000 | Loss: 0.00001386
Iteration 102/1000 | Loss: 0.00001386
Iteration 103/1000 | Loss: 0.00001386
Iteration 104/1000 | Loss: 0.00001386
Iteration 105/1000 | Loss: 0.00001386
Iteration 106/1000 | Loss: 0.00001386
Iteration 107/1000 | Loss: 0.00001386
Iteration 108/1000 | Loss: 0.00001386
Iteration 109/1000 | Loss: 0.00001386
Iteration 110/1000 | Loss: 0.00001386
Iteration 111/1000 | Loss: 0.00001386
Iteration 112/1000 | Loss: 0.00001386
Iteration 113/1000 | Loss: 0.00001385
Iteration 114/1000 | Loss: 0.00001385
Iteration 115/1000 | Loss: 0.00001384
Iteration 116/1000 | Loss: 0.00001384
Iteration 117/1000 | Loss: 0.00001384
Iteration 118/1000 | Loss: 0.00001383
Iteration 119/1000 | Loss: 0.00001383
Iteration 120/1000 | Loss: 0.00001383
Iteration 121/1000 | Loss: 0.00001383
Iteration 122/1000 | Loss: 0.00001383
Iteration 123/1000 | Loss: 0.00001382
Iteration 124/1000 | Loss: 0.00001382
Iteration 125/1000 | Loss: 0.00001382
Iteration 126/1000 | Loss: 0.00001382
Iteration 127/1000 | Loss: 0.00001382
Iteration 128/1000 | Loss: 0.00001382
Iteration 129/1000 | Loss: 0.00001382
Iteration 130/1000 | Loss: 0.00001382
Iteration 131/1000 | Loss: 0.00001382
Iteration 132/1000 | Loss: 0.00001382
Iteration 133/1000 | Loss: 0.00001382
Iteration 134/1000 | Loss: 0.00001382
Iteration 135/1000 | Loss: 0.00001381
Iteration 136/1000 | Loss: 0.00001381
Iteration 137/1000 | Loss: 0.00001381
Iteration 138/1000 | Loss: 0.00001381
Iteration 139/1000 | Loss: 0.00001381
Iteration 140/1000 | Loss: 0.00001381
Iteration 141/1000 | Loss: 0.00001381
Iteration 142/1000 | Loss: 0.00001381
Iteration 143/1000 | Loss: 0.00001381
Iteration 144/1000 | Loss: 0.00001381
Iteration 145/1000 | Loss: 0.00001381
Iteration 146/1000 | Loss: 0.00001380
Iteration 147/1000 | Loss: 0.00001380
Iteration 148/1000 | Loss: 0.00001380
Iteration 149/1000 | Loss: 0.00001380
Iteration 150/1000 | Loss: 0.00001380
Iteration 151/1000 | Loss: 0.00001380
Iteration 152/1000 | Loss: 0.00001380
Iteration 153/1000 | Loss: 0.00001380
Iteration 154/1000 | Loss: 0.00001380
Iteration 155/1000 | Loss: 0.00001380
Iteration 156/1000 | Loss: 0.00001380
Iteration 157/1000 | Loss: 0.00001379
Iteration 158/1000 | Loss: 0.00001379
Iteration 159/1000 | Loss: 0.00001379
Iteration 160/1000 | Loss: 0.00001379
Iteration 161/1000 | Loss: 0.00001379
Iteration 162/1000 | Loss: 0.00001379
Iteration 163/1000 | Loss: 0.00001379
Iteration 164/1000 | Loss: 0.00001379
Iteration 165/1000 | Loss: 0.00001379
Iteration 166/1000 | Loss: 0.00001379
Iteration 167/1000 | Loss: 0.00001378
Iteration 168/1000 | Loss: 0.00001378
Iteration 169/1000 | Loss: 0.00001378
Iteration 170/1000 | Loss: 0.00001377
Iteration 171/1000 | Loss: 0.00001377
Iteration 172/1000 | Loss: 0.00001377
Iteration 173/1000 | Loss: 0.00001377
Iteration 174/1000 | Loss: 0.00001377
Iteration 175/1000 | Loss: 0.00001377
Iteration 176/1000 | Loss: 0.00001377
Iteration 177/1000 | Loss: 0.00001377
Iteration 178/1000 | Loss: 0.00001377
Iteration 179/1000 | Loss: 0.00001376
Iteration 180/1000 | Loss: 0.00001376
Iteration 181/1000 | Loss: 0.00001376
Iteration 182/1000 | Loss: 0.00001376
Iteration 183/1000 | Loss: 0.00001376
Iteration 184/1000 | Loss: 0.00001376
Iteration 185/1000 | Loss: 0.00001376
Iteration 186/1000 | Loss: 0.00001376
Iteration 187/1000 | Loss: 0.00001376
Iteration 188/1000 | Loss: 0.00001376
Iteration 189/1000 | Loss: 0.00001375
Iteration 190/1000 | Loss: 0.00001375
Iteration 191/1000 | Loss: 0.00001375
Iteration 192/1000 | Loss: 0.00001375
Iteration 193/1000 | Loss: 0.00001375
Iteration 194/1000 | Loss: 0.00001375
Iteration 195/1000 | Loss: 0.00001375
Iteration 196/1000 | Loss: 0.00001375
Iteration 197/1000 | Loss: 0.00001375
Iteration 198/1000 | Loss: 0.00001375
Iteration 199/1000 | Loss: 0.00001375
Iteration 200/1000 | Loss: 0.00001375
Iteration 201/1000 | Loss: 0.00001375
Iteration 202/1000 | Loss: 0.00001375
Iteration 203/1000 | Loss: 0.00001375
Iteration 204/1000 | Loss: 0.00001375
Iteration 205/1000 | Loss: 0.00001375
Iteration 206/1000 | Loss: 0.00001375
Iteration 207/1000 | Loss: 0.00001375
Iteration 208/1000 | Loss: 0.00001375
Iteration 209/1000 | Loss: 0.00001375
Iteration 210/1000 | Loss: 0.00001375
Iteration 211/1000 | Loss: 0.00001375
Iteration 212/1000 | Loss: 0.00001375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.3747600860369857e-05, 1.3747600860369857e-05, 1.3747600860369857e-05, 1.3747600860369857e-05, 1.3747600860369857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3747600860369857e-05

Optimization complete. Final v2v error: 3.1544547080993652 mm

Highest mean error: 3.4413976669311523 mm for frame 213

Lowest mean error: 2.895968437194824 mm for frame 118

Saving results

Total time: 46.44860553741455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805756
Iteration 2/25 | Loss: 0.00148466
Iteration 3/25 | Loss: 0.00126748
Iteration 4/25 | Loss: 0.00124144
Iteration 5/25 | Loss: 0.00123931
Iteration 6/25 | Loss: 0.00125924
Iteration 7/25 | Loss: 0.00125312
Iteration 8/25 | Loss: 0.00123793
Iteration 9/25 | Loss: 0.00123176
Iteration 10/25 | Loss: 0.00122755
Iteration 11/25 | Loss: 0.00122493
Iteration 12/25 | Loss: 0.00122380
Iteration 13/25 | Loss: 0.00122456
Iteration 14/25 | Loss: 0.00122285
Iteration 15/25 | Loss: 0.00122043
Iteration 16/25 | Loss: 0.00121915
Iteration 17/25 | Loss: 0.00122025
Iteration 18/25 | Loss: 0.00121865
Iteration 19/25 | Loss: 0.00121711
Iteration 20/25 | Loss: 0.00121658
Iteration 21/25 | Loss: 0.00121636
Iteration 22/25 | Loss: 0.00121622
Iteration 23/25 | Loss: 0.00121619
Iteration 24/25 | Loss: 0.00121618
Iteration 25/25 | Loss: 0.00121618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58855927
Iteration 2/25 | Loss: 0.00061981
Iteration 3/25 | Loss: 0.00061980
Iteration 4/25 | Loss: 0.00061980
Iteration 5/25 | Loss: 0.00061980
Iteration 6/25 | Loss: 0.00061980
Iteration 7/25 | Loss: 0.00061980
Iteration 8/25 | Loss: 0.00061980
Iteration 9/25 | Loss: 0.00061980
Iteration 10/25 | Loss: 0.00061980
Iteration 11/25 | Loss: 0.00061980
Iteration 12/25 | Loss: 0.00061980
Iteration 13/25 | Loss: 0.00061980
Iteration 14/25 | Loss: 0.00061980
Iteration 15/25 | Loss: 0.00061980
Iteration 16/25 | Loss: 0.00061980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006197952898219228, 0.0006197952898219228, 0.0006197952898219228, 0.0006197952898219228, 0.0006197952898219228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006197952898219228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061980
Iteration 2/1000 | Loss: 0.00003131
Iteration 3/1000 | Loss: 0.00002495
Iteration 4/1000 | Loss: 0.00002248
Iteration 5/1000 | Loss: 0.00002142
Iteration 6/1000 | Loss: 0.00002030
Iteration 7/1000 | Loss: 0.00012332
Iteration 8/1000 | Loss: 0.00002037
Iteration 9/1000 | Loss: 0.00001880
Iteration 10/1000 | Loss: 0.00001783
Iteration 11/1000 | Loss: 0.00001727
Iteration 12/1000 | Loss: 0.00001683
Iteration 13/1000 | Loss: 0.00001663
Iteration 14/1000 | Loss: 0.00001661
Iteration 15/1000 | Loss: 0.00001661
Iteration 16/1000 | Loss: 0.00001661
Iteration 17/1000 | Loss: 0.00001642
Iteration 18/1000 | Loss: 0.00001620
Iteration 19/1000 | Loss: 0.00001615
Iteration 20/1000 | Loss: 0.00001605
Iteration 21/1000 | Loss: 0.00001605
Iteration 22/1000 | Loss: 0.00001602
Iteration 23/1000 | Loss: 0.00001602
Iteration 24/1000 | Loss: 0.00001601
Iteration 25/1000 | Loss: 0.00001600
Iteration 26/1000 | Loss: 0.00001599
Iteration 27/1000 | Loss: 0.00001594
Iteration 28/1000 | Loss: 0.00001593
Iteration 29/1000 | Loss: 0.00001593
Iteration 30/1000 | Loss: 0.00001593
Iteration 31/1000 | Loss: 0.00001593
Iteration 32/1000 | Loss: 0.00001593
Iteration 33/1000 | Loss: 0.00001593
Iteration 34/1000 | Loss: 0.00001593
Iteration 35/1000 | Loss: 0.00001593
Iteration 36/1000 | Loss: 0.00001593
Iteration 37/1000 | Loss: 0.00001593
Iteration 38/1000 | Loss: 0.00001592
Iteration 39/1000 | Loss: 0.00001591
Iteration 40/1000 | Loss: 0.00001591
Iteration 41/1000 | Loss: 0.00001590
Iteration 42/1000 | Loss: 0.00001590
Iteration 43/1000 | Loss: 0.00001590
Iteration 44/1000 | Loss: 0.00001589
Iteration 45/1000 | Loss: 0.00001589
Iteration 46/1000 | Loss: 0.00001589
Iteration 47/1000 | Loss: 0.00001589
Iteration 48/1000 | Loss: 0.00001589
Iteration 49/1000 | Loss: 0.00001588
Iteration 50/1000 | Loss: 0.00001588
Iteration 51/1000 | Loss: 0.00001588
Iteration 52/1000 | Loss: 0.00001588
Iteration 53/1000 | Loss: 0.00001587
Iteration 54/1000 | Loss: 0.00001587
Iteration 55/1000 | Loss: 0.00001587
Iteration 56/1000 | Loss: 0.00001587
Iteration 57/1000 | Loss: 0.00001587
Iteration 58/1000 | Loss: 0.00001586
Iteration 59/1000 | Loss: 0.00001586
Iteration 60/1000 | Loss: 0.00001586
Iteration 61/1000 | Loss: 0.00001585
Iteration 62/1000 | Loss: 0.00001585
Iteration 63/1000 | Loss: 0.00001585
Iteration 64/1000 | Loss: 0.00001585
Iteration 65/1000 | Loss: 0.00001585
Iteration 66/1000 | Loss: 0.00001585
Iteration 67/1000 | Loss: 0.00001585
Iteration 68/1000 | Loss: 0.00001585
Iteration 69/1000 | Loss: 0.00001585
Iteration 70/1000 | Loss: 0.00001584
Iteration 71/1000 | Loss: 0.00001584
Iteration 72/1000 | Loss: 0.00001584
Iteration 73/1000 | Loss: 0.00001584
Iteration 74/1000 | Loss: 0.00001584
Iteration 75/1000 | Loss: 0.00001583
Iteration 76/1000 | Loss: 0.00001583
Iteration 77/1000 | Loss: 0.00001583
Iteration 78/1000 | Loss: 0.00001583
Iteration 79/1000 | Loss: 0.00001582
Iteration 80/1000 | Loss: 0.00001582
Iteration 81/1000 | Loss: 0.00001582
Iteration 82/1000 | Loss: 0.00001582
Iteration 83/1000 | Loss: 0.00001582
Iteration 84/1000 | Loss: 0.00001582
Iteration 85/1000 | Loss: 0.00001582
Iteration 86/1000 | Loss: 0.00001581
Iteration 87/1000 | Loss: 0.00001581
Iteration 88/1000 | Loss: 0.00001581
Iteration 89/1000 | Loss: 0.00001581
Iteration 90/1000 | Loss: 0.00001581
Iteration 91/1000 | Loss: 0.00001581
Iteration 92/1000 | Loss: 0.00001581
Iteration 93/1000 | Loss: 0.00001581
Iteration 94/1000 | Loss: 0.00001581
Iteration 95/1000 | Loss: 0.00001581
Iteration 96/1000 | Loss: 0.00001581
Iteration 97/1000 | Loss: 0.00001581
Iteration 98/1000 | Loss: 0.00001581
Iteration 99/1000 | Loss: 0.00001581
Iteration 100/1000 | Loss: 0.00001581
Iteration 101/1000 | Loss: 0.00001581
Iteration 102/1000 | Loss: 0.00001581
Iteration 103/1000 | Loss: 0.00001581
Iteration 104/1000 | Loss: 0.00001581
Iteration 105/1000 | Loss: 0.00001581
Iteration 106/1000 | Loss: 0.00001580
Iteration 107/1000 | Loss: 0.00001580
Iteration 108/1000 | Loss: 0.00001580
Iteration 109/1000 | Loss: 0.00001580
Iteration 110/1000 | Loss: 0.00001580
Iteration 111/1000 | Loss: 0.00001580
Iteration 112/1000 | Loss: 0.00001580
Iteration 113/1000 | Loss: 0.00001580
Iteration 114/1000 | Loss: 0.00001580
Iteration 115/1000 | Loss: 0.00001579
Iteration 116/1000 | Loss: 0.00001579
Iteration 117/1000 | Loss: 0.00001579
Iteration 118/1000 | Loss: 0.00001579
Iteration 119/1000 | Loss: 0.00001579
Iteration 120/1000 | Loss: 0.00001579
Iteration 121/1000 | Loss: 0.00001579
Iteration 122/1000 | Loss: 0.00001578
Iteration 123/1000 | Loss: 0.00001578
Iteration 124/1000 | Loss: 0.00001578
Iteration 125/1000 | Loss: 0.00001578
Iteration 126/1000 | Loss: 0.00001578
Iteration 127/1000 | Loss: 0.00001578
Iteration 128/1000 | Loss: 0.00001578
Iteration 129/1000 | Loss: 0.00001578
Iteration 130/1000 | Loss: 0.00001578
Iteration 131/1000 | Loss: 0.00001578
Iteration 132/1000 | Loss: 0.00001578
Iteration 133/1000 | Loss: 0.00001578
Iteration 134/1000 | Loss: 0.00001578
Iteration 135/1000 | Loss: 0.00001578
Iteration 136/1000 | Loss: 0.00001578
Iteration 137/1000 | Loss: 0.00001577
Iteration 138/1000 | Loss: 0.00001577
Iteration 139/1000 | Loss: 0.00001577
Iteration 140/1000 | Loss: 0.00001577
Iteration 141/1000 | Loss: 0.00001577
Iteration 142/1000 | Loss: 0.00001577
Iteration 143/1000 | Loss: 0.00001577
Iteration 144/1000 | Loss: 0.00001577
Iteration 145/1000 | Loss: 0.00001577
Iteration 146/1000 | Loss: 0.00001577
Iteration 147/1000 | Loss: 0.00001577
Iteration 148/1000 | Loss: 0.00001577
Iteration 149/1000 | Loss: 0.00001577
Iteration 150/1000 | Loss: 0.00001577
Iteration 151/1000 | Loss: 0.00001577
Iteration 152/1000 | Loss: 0.00001576
Iteration 153/1000 | Loss: 0.00001576
Iteration 154/1000 | Loss: 0.00001576
Iteration 155/1000 | Loss: 0.00001576
Iteration 156/1000 | Loss: 0.00001576
Iteration 157/1000 | Loss: 0.00001576
Iteration 158/1000 | Loss: 0.00001576
Iteration 159/1000 | Loss: 0.00001576
Iteration 160/1000 | Loss: 0.00001576
Iteration 161/1000 | Loss: 0.00001576
Iteration 162/1000 | Loss: 0.00001575
Iteration 163/1000 | Loss: 0.00001575
Iteration 164/1000 | Loss: 0.00001575
Iteration 165/1000 | Loss: 0.00001575
Iteration 166/1000 | Loss: 0.00001575
Iteration 167/1000 | Loss: 0.00001575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.5754894775454886e-05, 1.5754894775454886e-05, 1.5754894775454886e-05, 1.5754894775454886e-05, 1.5754894775454886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5754894775454886e-05

Optimization complete. Final v2v error: 3.378310203552246 mm

Highest mean error: 3.9599924087524414 mm for frame 131

Lowest mean error: 2.8844664096832275 mm for frame 77

Saving results

Total time: 79.78180646896362
