Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=108, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6048-6103
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438432
Iteration 2/25 | Loss: 0.00138703
Iteration 3/25 | Loss: 0.00126755
Iteration 4/25 | Loss: 0.00124953
Iteration 5/25 | Loss: 0.00124430
Iteration 6/25 | Loss: 0.00124337
Iteration 7/25 | Loss: 0.00124337
Iteration 8/25 | Loss: 0.00124337
Iteration 9/25 | Loss: 0.00124337
Iteration 10/25 | Loss: 0.00124337
Iteration 11/25 | Loss: 0.00124337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012433743104338646, 0.0012433743104338646, 0.0012433743104338646, 0.0012433743104338646, 0.0012433743104338646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012433743104338646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39620602
Iteration 2/25 | Loss: 0.00079219
Iteration 3/25 | Loss: 0.00079219
Iteration 4/25 | Loss: 0.00079219
Iteration 5/25 | Loss: 0.00079219
Iteration 6/25 | Loss: 0.00079219
Iteration 7/25 | Loss: 0.00079219
Iteration 8/25 | Loss: 0.00079219
Iteration 9/25 | Loss: 0.00079219
Iteration 10/25 | Loss: 0.00079219
Iteration 11/25 | Loss: 0.00079219
Iteration 12/25 | Loss: 0.00079219
Iteration 13/25 | Loss: 0.00079219
Iteration 14/25 | Loss: 0.00079219
Iteration 15/25 | Loss: 0.00079219
Iteration 16/25 | Loss: 0.00079219
Iteration 17/25 | Loss: 0.00079219
Iteration 18/25 | Loss: 0.00079219
Iteration 19/25 | Loss: 0.00079219
Iteration 20/25 | Loss: 0.00079219
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007921877549961209, 0.0007921877549961209, 0.0007921877549961209, 0.0007921877549961209, 0.0007921877549961209]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007921877549961209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079219
Iteration 2/1000 | Loss: 0.00004044
Iteration 3/1000 | Loss: 0.00002808
Iteration 4/1000 | Loss: 0.00002341
Iteration 5/1000 | Loss: 0.00002169
Iteration 6/1000 | Loss: 0.00002092
Iteration 7/1000 | Loss: 0.00002030
Iteration 8/1000 | Loss: 0.00001978
Iteration 9/1000 | Loss: 0.00001933
Iteration 10/1000 | Loss: 0.00001904
Iteration 11/1000 | Loss: 0.00001883
Iteration 12/1000 | Loss: 0.00001876
Iteration 13/1000 | Loss: 0.00001855
Iteration 14/1000 | Loss: 0.00001851
Iteration 15/1000 | Loss: 0.00001847
Iteration 16/1000 | Loss: 0.00001842
Iteration 17/1000 | Loss: 0.00001840
Iteration 18/1000 | Loss: 0.00001833
Iteration 19/1000 | Loss: 0.00001831
Iteration 20/1000 | Loss: 0.00001830
Iteration 21/1000 | Loss: 0.00001829
Iteration 22/1000 | Loss: 0.00001825
Iteration 23/1000 | Loss: 0.00001824
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001822
Iteration 26/1000 | Loss: 0.00001819
Iteration 27/1000 | Loss: 0.00001819
Iteration 28/1000 | Loss: 0.00001819
Iteration 29/1000 | Loss: 0.00001818
Iteration 30/1000 | Loss: 0.00001817
Iteration 31/1000 | Loss: 0.00001817
Iteration 32/1000 | Loss: 0.00001816
Iteration 33/1000 | Loss: 0.00001815
Iteration 34/1000 | Loss: 0.00001814
Iteration 35/1000 | Loss: 0.00001814
Iteration 36/1000 | Loss: 0.00001814
Iteration 37/1000 | Loss: 0.00001812
Iteration 38/1000 | Loss: 0.00001812
Iteration 39/1000 | Loss: 0.00001811
Iteration 40/1000 | Loss: 0.00001811
Iteration 41/1000 | Loss: 0.00001811
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001810
Iteration 44/1000 | Loss: 0.00001810
Iteration 45/1000 | Loss: 0.00001809
Iteration 46/1000 | Loss: 0.00001809
Iteration 47/1000 | Loss: 0.00001808
Iteration 48/1000 | Loss: 0.00001808
Iteration 49/1000 | Loss: 0.00001808
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001807
Iteration 52/1000 | Loss: 0.00001807
Iteration 53/1000 | Loss: 0.00001807
Iteration 54/1000 | Loss: 0.00001807
Iteration 55/1000 | Loss: 0.00001806
Iteration 56/1000 | Loss: 0.00001806
Iteration 57/1000 | Loss: 0.00001805
Iteration 58/1000 | Loss: 0.00001805
Iteration 59/1000 | Loss: 0.00001805
Iteration 60/1000 | Loss: 0.00001804
Iteration 61/1000 | Loss: 0.00001804
Iteration 62/1000 | Loss: 0.00001804
Iteration 63/1000 | Loss: 0.00001804
Iteration 64/1000 | Loss: 0.00001803
Iteration 65/1000 | Loss: 0.00001803
Iteration 66/1000 | Loss: 0.00001802
Iteration 67/1000 | Loss: 0.00001801
Iteration 68/1000 | Loss: 0.00001801
Iteration 69/1000 | Loss: 0.00001800
Iteration 70/1000 | Loss: 0.00001800
Iteration 71/1000 | Loss: 0.00001800
Iteration 72/1000 | Loss: 0.00001798
Iteration 73/1000 | Loss: 0.00001798
Iteration 74/1000 | Loss: 0.00001797
Iteration 75/1000 | Loss: 0.00001796
Iteration 76/1000 | Loss: 0.00001796
Iteration 77/1000 | Loss: 0.00001795
Iteration 78/1000 | Loss: 0.00001795
Iteration 79/1000 | Loss: 0.00001795
Iteration 80/1000 | Loss: 0.00001794
Iteration 81/1000 | Loss: 0.00001794
Iteration 82/1000 | Loss: 0.00001794
Iteration 83/1000 | Loss: 0.00001793
Iteration 84/1000 | Loss: 0.00001793
Iteration 85/1000 | Loss: 0.00001793
Iteration 86/1000 | Loss: 0.00001792
Iteration 87/1000 | Loss: 0.00001792
Iteration 88/1000 | Loss: 0.00001792
Iteration 89/1000 | Loss: 0.00001791
Iteration 90/1000 | Loss: 0.00001791
Iteration 91/1000 | Loss: 0.00001791
Iteration 92/1000 | Loss: 0.00001791
Iteration 93/1000 | Loss: 0.00001790
Iteration 94/1000 | Loss: 0.00001790
Iteration 95/1000 | Loss: 0.00001790
Iteration 96/1000 | Loss: 0.00001790
Iteration 97/1000 | Loss: 0.00001789
Iteration 98/1000 | Loss: 0.00001789
Iteration 99/1000 | Loss: 0.00001789
Iteration 100/1000 | Loss: 0.00001789
Iteration 101/1000 | Loss: 0.00001788
Iteration 102/1000 | Loss: 0.00001788
Iteration 103/1000 | Loss: 0.00001788
Iteration 104/1000 | Loss: 0.00001788
Iteration 105/1000 | Loss: 0.00001788
Iteration 106/1000 | Loss: 0.00001788
Iteration 107/1000 | Loss: 0.00001787
Iteration 108/1000 | Loss: 0.00001787
Iteration 109/1000 | Loss: 0.00001786
Iteration 110/1000 | Loss: 0.00001786
Iteration 111/1000 | Loss: 0.00001786
Iteration 112/1000 | Loss: 0.00001786
Iteration 113/1000 | Loss: 0.00001786
Iteration 114/1000 | Loss: 0.00001786
Iteration 115/1000 | Loss: 0.00001786
Iteration 116/1000 | Loss: 0.00001786
Iteration 117/1000 | Loss: 0.00001786
Iteration 118/1000 | Loss: 0.00001786
Iteration 119/1000 | Loss: 0.00001785
Iteration 120/1000 | Loss: 0.00001785
Iteration 121/1000 | Loss: 0.00001785
Iteration 122/1000 | Loss: 0.00001785
Iteration 123/1000 | Loss: 0.00001785
Iteration 124/1000 | Loss: 0.00001785
Iteration 125/1000 | Loss: 0.00001785
Iteration 126/1000 | Loss: 0.00001785
Iteration 127/1000 | Loss: 0.00001785
Iteration 128/1000 | Loss: 0.00001784
Iteration 129/1000 | Loss: 0.00001784
Iteration 130/1000 | Loss: 0.00001784
Iteration 131/1000 | Loss: 0.00001784
Iteration 132/1000 | Loss: 0.00001783
Iteration 133/1000 | Loss: 0.00001783
Iteration 134/1000 | Loss: 0.00001783
Iteration 135/1000 | Loss: 0.00001783
Iteration 136/1000 | Loss: 0.00001783
Iteration 137/1000 | Loss: 0.00001783
Iteration 138/1000 | Loss: 0.00001783
Iteration 139/1000 | Loss: 0.00001782
Iteration 140/1000 | Loss: 0.00001782
Iteration 141/1000 | Loss: 0.00001782
Iteration 142/1000 | Loss: 0.00001782
Iteration 143/1000 | Loss: 0.00001781
Iteration 144/1000 | Loss: 0.00001781
Iteration 145/1000 | Loss: 0.00001781
Iteration 146/1000 | Loss: 0.00001781
Iteration 147/1000 | Loss: 0.00001781
Iteration 148/1000 | Loss: 0.00001780
Iteration 149/1000 | Loss: 0.00001780
Iteration 150/1000 | Loss: 0.00001780
Iteration 151/1000 | Loss: 0.00001780
Iteration 152/1000 | Loss: 0.00001780
Iteration 153/1000 | Loss: 0.00001780
Iteration 154/1000 | Loss: 0.00001780
Iteration 155/1000 | Loss: 0.00001780
Iteration 156/1000 | Loss: 0.00001780
Iteration 157/1000 | Loss: 0.00001780
Iteration 158/1000 | Loss: 0.00001780
Iteration 159/1000 | Loss: 0.00001780
Iteration 160/1000 | Loss: 0.00001780
Iteration 161/1000 | Loss: 0.00001780
Iteration 162/1000 | Loss: 0.00001780
Iteration 163/1000 | Loss: 0.00001780
Iteration 164/1000 | Loss: 0.00001779
Iteration 165/1000 | Loss: 0.00001779
Iteration 166/1000 | Loss: 0.00001779
Iteration 167/1000 | Loss: 0.00001779
Iteration 168/1000 | Loss: 0.00001779
Iteration 169/1000 | Loss: 0.00001779
Iteration 170/1000 | Loss: 0.00001779
Iteration 171/1000 | Loss: 0.00001779
Iteration 172/1000 | Loss: 0.00001779
Iteration 173/1000 | Loss: 0.00001779
Iteration 174/1000 | Loss: 0.00001779
Iteration 175/1000 | Loss: 0.00001779
Iteration 176/1000 | Loss: 0.00001779
Iteration 177/1000 | Loss: 0.00001779
Iteration 178/1000 | Loss: 0.00001779
Iteration 179/1000 | Loss: 0.00001779
Iteration 180/1000 | Loss: 0.00001778
Iteration 181/1000 | Loss: 0.00001778
Iteration 182/1000 | Loss: 0.00001778
Iteration 183/1000 | Loss: 0.00001778
Iteration 184/1000 | Loss: 0.00001778
Iteration 185/1000 | Loss: 0.00001778
Iteration 186/1000 | Loss: 0.00001778
Iteration 187/1000 | Loss: 0.00001778
Iteration 188/1000 | Loss: 0.00001778
Iteration 189/1000 | Loss: 0.00001778
Iteration 190/1000 | Loss: 0.00001778
Iteration 191/1000 | Loss: 0.00001778
Iteration 192/1000 | Loss: 0.00001778
Iteration 193/1000 | Loss: 0.00001777
Iteration 194/1000 | Loss: 0.00001777
Iteration 195/1000 | Loss: 0.00001777
Iteration 196/1000 | Loss: 0.00001777
Iteration 197/1000 | Loss: 0.00001777
Iteration 198/1000 | Loss: 0.00001777
Iteration 199/1000 | Loss: 0.00001777
Iteration 200/1000 | Loss: 0.00001776
Iteration 201/1000 | Loss: 0.00001776
Iteration 202/1000 | Loss: 0.00001776
Iteration 203/1000 | Loss: 0.00001776
Iteration 204/1000 | Loss: 0.00001776
Iteration 205/1000 | Loss: 0.00001776
Iteration 206/1000 | Loss: 0.00001776
Iteration 207/1000 | Loss: 0.00001776
Iteration 208/1000 | Loss: 0.00001776
Iteration 209/1000 | Loss: 0.00001776
Iteration 210/1000 | Loss: 0.00001776
Iteration 211/1000 | Loss: 0.00001775
Iteration 212/1000 | Loss: 0.00001775
Iteration 213/1000 | Loss: 0.00001775
Iteration 214/1000 | Loss: 0.00001775
Iteration 215/1000 | Loss: 0.00001775
Iteration 216/1000 | Loss: 0.00001775
Iteration 217/1000 | Loss: 0.00001775
Iteration 218/1000 | Loss: 0.00001775
Iteration 219/1000 | Loss: 0.00001775
Iteration 220/1000 | Loss: 0.00001775
Iteration 221/1000 | Loss: 0.00001775
Iteration 222/1000 | Loss: 0.00001775
Iteration 223/1000 | Loss: 0.00001775
Iteration 224/1000 | Loss: 0.00001775
Iteration 225/1000 | Loss: 0.00001775
Iteration 226/1000 | Loss: 0.00001775
Iteration 227/1000 | Loss: 0.00001775
Iteration 228/1000 | Loss: 0.00001775
Iteration 229/1000 | Loss: 0.00001775
Iteration 230/1000 | Loss: 0.00001775
Iteration 231/1000 | Loss: 0.00001775
Iteration 232/1000 | Loss: 0.00001775
Iteration 233/1000 | Loss: 0.00001775
Iteration 234/1000 | Loss: 0.00001775
Iteration 235/1000 | Loss: 0.00001775
Iteration 236/1000 | Loss: 0.00001775
Iteration 237/1000 | Loss: 0.00001775
Iteration 238/1000 | Loss: 0.00001775
Iteration 239/1000 | Loss: 0.00001775
Iteration 240/1000 | Loss: 0.00001775
Iteration 241/1000 | Loss: 0.00001775
Iteration 242/1000 | Loss: 0.00001775
Iteration 243/1000 | Loss: 0.00001775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.7746491721482016e-05, 1.7746491721482016e-05, 1.7746491721482016e-05, 1.7746491721482016e-05, 1.7746491721482016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7746491721482016e-05

Optimization complete. Final v2v error: 3.5276622772216797 mm

Highest mean error: 3.954857587814331 mm for frame 70

Lowest mean error: 3.239119529724121 mm for frame 22

Saving results

Total time: 46.589699506759644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486103
Iteration 2/25 | Loss: 0.00135865
Iteration 3/25 | Loss: 0.00127395
Iteration 4/25 | Loss: 0.00126427
Iteration 5/25 | Loss: 0.00126047
Iteration 6/25 | Loss: 0.00126047
Iteration 7/25 | Loss: 0.00126047
Iteration 8/25 | Loss: 0.00126047
Iteration 9/25 | Loss: 0.00126047
Iteration 10/25 | Loss: 0.00126047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001260470482520759, 0.001260470482520759, 0.001260470482520759, 0.001260470482520759, 0.001260470482520759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001260470482520759

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89973760
Iteration 2/25 | Loss: 0.00073347
Iteration 3/25 | Loss: 0.00073347
Iteration 4/25 | Loss: 0.00073347
Iteration 5/25 | Loss: 0.00073347
Iteration 6/25 | Loss: 0.00073347
Iteration 7/25 | Loss: 0.00073347
Iteration 8/25 | Loss: 0.00073347
Iteration 9/25 | Loss: 0.00073347
Iteration 10/25 | Loss: 0.00073346
Iteration 11/25 | Loss: 0.00073346
Iteration 12/25 | Loss: 0.00073346
Iteration 13/25 | Loss: 0.00073346
Iteration 14/25 | Loss: 0.00073346
Iteration 15/25 | Loss: 0.00073346
Iteration 16/25 | Loss: 0.00073346
Iteration 17/25 | Loss: 0.00073346
Iteration 18/25 | Loss: 0.00073346
Iteration 19/25 | Loss: 0.00073346
Iteration 20/25 | Loss: 0.00073346
Iteration 21/25 | Loss: 0.00073346
Iteration 22/25 | Loss: 0.00073346
Iteration 23/25 | Loss: 0.00073346
Iteration 24/25 | Loss: 0.00073346
Iteration 25/25 | Loss: 0.00073346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073346
Iteration 2/1000 | Loss: 0.00003422
Iteration 3/1000 | Loss: 0.00002486
Iteration 4/1000 | Loss: 0.00002198
Iteration 5/1000 | Loss: 0.00002056
Iteration 6/1000 | Loss: 0.00001948
Iteration 7/1000 | Loss: 0.00001881
Iteration 8/1000 | Loss: 0.00001844
Iteration 9/1000 | Loss: 0.00001802
Iteration 10/1000 | Loss: 0.00001773
Iteration 11/1000 | Loss: 0.00001746
Iteration 12/1000 | Loss: 0.00001722
Iteration 13/1000 | Loss: 0.00001704
Iteration 14/1000 | Loss: 0.00001694
Iteration 15/1000 | Loss: 0.00001693
Iteration 16/1000 | Loss: 0.00001678
Iteration 17/1000 | Loss: 0.00001664
Iteration 18/1000 | Loss: 0.00001658
Iteration 19/1000 | Loss: 0.00001658
Iteration 20/1000 | Loss: 0.00001657
Iteration 21/1000 | Loss: 0.00001656
Iteration 22/1000 | Loss: 0.00001655
Iteration 23/1000 | Loss: 0.00001650
Iteration 24/1000 | Loss: 0.00001649
Iteration 25/1000 | Loss: 0.00001648
Iteration 26/1000 | Loss: 0.00001648
Iteration 27/1000 | Loss: 0.00001642
Iteration 28/1000 | Loss: 0.00001638
Iteration 29/1000 | Loss: 0.00001638
Iteration 30/1000 | Loss: 0.00001632
Iteration 31/1000 | Loss: 0.00001632
Iteration 32/1000 | Loss: 0.00001631
Iteration 33/1000 | Loss: 0.00001630
Iteration 34/1000 | Loss: 0.00001629
Iteration 35/1000 | Loss: 0.00001629
Iteration 36/1000 | Loss: 0.00001628
Iteration 37/1000 | Loss: 0.00001626
Iteration 38/1000 | Loss: 0.00001625
Iteration 39/1000 | Loss: 0.00001625
Iteration 40/1000 | Loss: 0.00001624
Iteration 41/1000 | Loss: 0.00001624
Iteration 42/1000 | Loss: 0.00001623
Iteration 43/1000 | Loss: 0.00001623
Iteration 44/1000 | Loss: 0.00001622
Iteration 45/1000 | Loss: 0.00001622
Iteration 46/1000 | Loss: 0.00001622
Iteration 47/1000 | Loss: 0.00001621
Iteration 48/1000 | Loss: 0.00001621
Iteration 49/1000 | Loss: 0.00001621
Iteration 50/1000 | Loss: 0.00001620
Iteration 51/1000 | Loss: 0.00001620
Iteration 52/1000 | Loss: 0.00001620
Iteration 53/1000 | Loss: 0.00001619
Iteration 54/1000 | Loss: 0.00001619
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001619
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001618
Iteration 59/1000 | Loss: 0.00001618
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001617
Iteration 62/1000 | Loss: 0.00001617
Iteration 63/1000 | Loss: 0.00001617
Iteration 64/1000 | Loss: 0.00001616
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001616
Iteration 67/1000 | Loss: 0.00001615
Iteration 68/1000 | Loss: 0.00001615
Iteration 69/1000 | Loss: 0.00001615
Iteration 70/1000 | Loss: 0.00001614
Iteration 71/1000 | Loss: 0.00001614
Iteration 72/1000 | Loss: 0.00001614
Iteration 73/1000 | Loss: 0.00001614
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00001613
Iteration 76/1000 | Loss: 0.00001613
Iteration 77/1000 | Loss: 0.00001613
Iteration 78/1000 | Loss: 0.00001613
Iteration 79/1000 | Loss: 0.00001613
Iteration 80/1000 | Loss: 0.00001613
Iteration 81/1000 | Loss: 0.00001612
Iteration 82/1000 | Loss: 0.00001612
Iteration 83/1000 | Loss: 0.00001612
Iteration 84/1000 | Loss: 0.00001612
Iteration 85/1000 | Loss: 0.00001612
Iteration 86/1000 | Loss: 0.00001612
Iteration 87/1000 | Loss: 0.00001612
Iteration 88/1000 | Loss: 0.00001612
Iteration 89/1000 | Loss: 0.00001611
Iteration 90/1000 | Loss: 0.00001611
Iteration 91/1000 | Loss: 0.00001611
Iteration 92/1000 | Loss: 0.00001611
Iteration 93/1000 | Loss: 0.00001610
Iteration 94/1000 | Loss: 0.00001610
Iteration 95/1000 | Loss: 0.00001610
Iteration 96/1000 | Loss: 0.00001610
Iteration 97/1000 | Loss: 0.00001610
Iteration 98/1000 | Loss: 0.00001610
Iteration 99/1000 | Loss: 0.00001609
Iteration 100/1000 | Loss: 0.00001609
Iteration 101/1000 | Loss: 0.00001609
Iteration 102/1000 | Loss: 0.00001609
Iteration 103/1000 | Loss: 0.00001609
Iteration 104/1000 | Loss: 0.00001609
Iteration 105/1000 | Loss: 0.00001609
Iteration 106/1000 | Loss: 0.00001609
Iteration 107/1000 | Loss: 0.00001608
Iteration 108/1000 | Loss: 0.00001608
Iteration 109/1000 | Loss: 0.00001608
Iteration 110/1000 | Loss: 0.00001608
Iteration 111/1000 | Loss: 0.00001608
Iteration 112/1000 | Loss: 0.00001608
Iteration 113/1000 | Loss: 0.00001608
Iteration 114/1000 | Loss: 0.00001608
Iteration 115/1000 | Loss: 0.00001608
Iteration 116/1000 | Loss: 0.00001608
Iteration 117/1000 | Loss: 0.00001608
Iteration 118/1000 | Loss: 0.00001607
Iteration 119/1000 | Loss: 0.00001607
Iteration 120/1000 | Loss: 0.00001607
Iteration 121/1000 | Loss: 0.00001607
Iteration 122/1000 | Loss: 0.00001607
Iteration 123/1000 | Loss: 0.00001607
Iteration 124/1000 | Loss: 0.00001606
Iteration 125/1000 | Loss: 0.00001606
Iteration 126/1000 | Loss: 0.00001606
Iteration 127/1000 | Loss: 0.00001606
Iteration 128/1000 | Loss: 0.00001606
Iteration 129/1000 | Loss: 0.00001605
Iteration 130/1000 | Loss: 0.00001605
Iteration 131/1000 | Loss: 0.00001605
Iteration 132/1000 | Loss: 0.00001604
Iteration 133/1000 | Loss: 0.00001604
Iteration 134/1000 | Loss: 0.00001604
Iteration 135/1000 | Loss: 0.00001604
Iteration 136/1000 | Loss: 0.00001603
Iteration 137/1000 | Loss: 0.00001603
Iteration 138/1000 | Loss: 0.00001603
Iteration 139/1000 | Loss: 0.00001603
Iteration 140/1000 | Loss: 0.00001603
Iteration 141/1000 | Loss: 0.00001603
Iteration 142/1000 | Loss: 0.00001602
Iteration 143/1000 | Loss: 0.00001602
Iteration 144/1000 | Loss: 0.00001602
Iteration 145/1000 | Loss: 0.00001602
Iteration 146/1000 | Loss: 0.00001602
Iteration 147/1000 | Loss: 0.00001602
Iteration 148/1000 | Loss: 0.00001602
Iteration 149/1000 | Loss: 0.00001602
Iteration 150/1000 | Loss: 0.00001602
Iteration 151/1000 | Loss: 0.00001602
Iteration 152/1000 | Loss: 0.00001602
Iteration 153/1000 | Loss: 0.00001602
Iteration 154/1000 | Loss: 0.00001602
Iteration 155/1000 | Loss: 0.00001602
Iteration 156/1000 | Loss: 0.00001602
Iteration 157/1000 | Loss: 0.00001601
Iteration 158/1000 | Loss: 0.00001601
Iteration 159/1000 | Loss: 0.00001601
Iteration 160/1000 | Loss: 0.00001601
Iteration 161/1000 | Loss: 0.00001601
Iteration 162/1000 | Loss: 0.00001601
Iteration 163/1000 | Loss: 0.00001601
Iteration 164/1000 | Loss: 0.00001601
Iteration 165/1000 | Loss: 0.00001601
Iteration 166/1000 | Loss: 0.00001601
Iteration 167/1000 | Loss: 0.00001600
Iteration 168/1000 | Loss: 0.00001600
Iteration 169/1000 | Loss: 0.00001600
Iteration 170/1000 | Loss: 0.00001600
Iteration 171/1000 | Loss: 0.00001600
Iteration 172/1000 | Loss: 0.00001600
Iteration 173/1000 | Loss: 0.00001600
Iteration 174/1000 | Loss: 0.00001600
Iteration 175/1000 | Loss: 0.00001600
Iteration 176/1000 | Loss: 0.00001600
Iteration 177/1000 | Loss: 0.00001600
Iteration 178/1000 | Loss: 0.00001600
Iteration 179/1000 | Loss: 0.00001600
Iteration 180/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.6004105418687686e-05, 1.6004105418687686e-05, 1.6004105418687686e-05, 1.6004105418687686e-05, 1.6004105418687686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6004105418687686e-05

Optimization complete. Final v2v error: 3.3845999240875244 mm

Highest mean error: 3.694274425506592 mm for frame 0

Lowest mean error: 3.2075276374816895 mm for frame 31

Saving results

Total time: 52.11902689933777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569352
Iteration 2/25 | Loss: 0.00147278
Iteration 3/25 | Loss: 0.00125427
Iteration 4/25 | Loss: 0.00122314
Iteration 5/25 | Loss: 0.00121282
Iteration 6/25 | Loss: 0.00121091
Iteration 7/25 | Loss: 0.00121084
Iteration 8/25 | Loss: 0.00121084
Iteration 9/25 | Loss: 0.00121084
Iteration 10/25 | Loss: 0.00121084
Iteration 11/25 | Loss: 0.00121084
Iteration 12/25 | Loss: 0.00121084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012108394876122475, 0.0012108394876122475, 0.0012108394876122475, 0.0012108394876122475, 0.0012108394876122475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012108394876122475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89354134
Iteration 2/25 | Loss: 0.00064583
Iteration 3/25 | Loss: 0.00064583
Iteration 4/25 | Loss: 0.00064583
Iteration 5/25 | Loss: 0.00064583
Iteration 6/25 | Loss: 0.00064583
Iteration 7/25 | Loss: 0.00064583
Iteration 8/25 | Loss: 0.00064583
Iteration 9/25 | Loss: 0.00064583
Iteration 10/25 | Loss: 0.00064583
Iteration 11/25 | Loss: 0.00064583
Iteration 12/25 | Loss: 0.00064583
Iteration 13/25 | Loss: 0.00064583
Iteration 14/25 | Loss: 0.00064583
Iteration 15/25 | Loss: 0.00064583
Iteration 16/25 | Loss: 0.00064583
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006458277930505574, 0.0006458277930505574, 0.0006458277930505574, 0.0006458277930505574, 0.0006458277930505574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006458277930505574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064583
Iteration 2/1000 | Loss: 0.00004895
Iteration 3/1000 | Loss: 0.00003075
Iteration 4/1000 | Loss: 0.00002735
Iteration 5/1000 | Loss: 0.00002548
Iteration 6/1000 | Loss: 0.00002420
Iteration 7/1000 | Loss: 0.00002349
Iteration 8/1000 | Loss: 0.00002314
Iteration 9/1000 | Loss: 0.00002280
Iteration 10/1000 | Loss: 0.00002259
Iteration 11/1000 | Loss: 0.00002241
Iteration 12/1000 | Loss: 0.00002232
Iteration 13/1000 | Loss: 0.00002227
Iteration 14/1000 | Loss: 0.00002225
Iteration 15/1000 | Loss: 0.00002225
Iteration 16/1000 | Loss: 0.00002224
Iteration 17/1000 | Loss: 0.00002224
Iteration 18/1000 | Loss: 0.00002223
Iteration 19/1000 | Loss: 0.00002223
Iteration 20/1000 | Loss: 0.00002223
Iteration 21/1000 | Loss: 0.00002223
Iteration 22/1000 | Loss: 0.00002223
Iteration 23/1000 | Loss: 0.00002222
Iteration 24/1000 | Loss: 0.00002222
Iteration 25/1000 | Loss: 0.00002222
Iteration 26/1000 | Loss: 0.00002222
Iteration 27/1000 | Loss: 0.00002221
Iteration 28/1000 | Loss: 0.00002221
Iteration 29/1000 | Loss: 0.00002221
Iteration 30/1000 | Loss: 0.00002221
Iteration 31/1000 | Loss: 0.00002220
Iteration 32/1000 | Loss: 0.00002220
Iteration 33/1000 | Loss: 0.00002220
Iteration 34/1000 | Loss: 0.00002220
Iteration 35/1000 | Loss: 0.00002220
Iteration 36/1000 | Loss: 0.00002219
Iteration 37/1000 | Loss: 0.00002219
Iteration 38/1000 | Loss: 0.00002218
Iteration 39/1000 | Loss: 0.00002218
Iteration 40/1000 | Loss: 0.00002218
Iteration 41/1000 | Loss: 0.00002218
Iteration 42/1000 | Loss: 0.00002217
Iteration 43/1000 | Loss: 0.00002217
Iteration 44/1000 | Loss: 0.00002217
Iteration 45/1000 | Loss: 0.00002217
Iteration 46/1000 | Loss: 0.00002216
Iteration 47/1000 | Loss: 0.00002216
Iteration 48/1000 | Loss: 0.00002216
Iteration 49/1000 | Loss: 0.00002215
Iteration 50/1000 | Loss: 0.00002215
Iteration 51/1000 | Loss: 0.00002215
Iteration 52/1000 | Loss: 0.00002214
Iteration 53/1000 | Loss: 0.00002213
Iteration 54/1000 | Loss: 0.00002212
Iteration 55/1000 | Loss: 0.00002212
Iteration 56/1000 | Loss: 0.00002212
Iteration 57/1000 | Loss: 0.00002212
Iteration 58/1000 | Loss: 0.00002212
Iteration 59/1000 | Loss: 0.00002212
Iteration 60/1000 | Loss: 0.00002212
Iteration 61/1000 | Loss: 0.00002211
Iteration 62/1000 | Loss: 0.00002211
Iteration 63/1000 | Loss: 0.00002211
Iteration 64/1000 | Loss: 0.00002211
Iteration 65/1000 | Loss: 0.00002211
Iteration 66/1000 | Loss: 0.00002211
Iteration 67/1000 | Loss: 0.00002211
Iteration 68/1000 | Loss: 0.00002211
Iteration 69/1000 | Loss: 0.00002211
Iteration 70/1000 | Loss: 0.00002211
Iteration 71/1000 | Loss: 0.00002210
Iteration 72/1000 | Loss: 0.00002210
Iteration 73/1000 | Loss: 0.00002210
Iteration 74/1000 | Loss: 0.00002210
Iteration 75/1000 | Loss: 0.00002209
Iteration 76/1000 | Loss: 0.00002209
Iteration 77/1000 | Loss: 0.00002209
Iteration 78/1000 | Loss: 0.00002209
Iteration 79/1000 | Loss: 0.00002209
Iteration 80/1000 | Loss: 0.00002209
Iteration 81/1000 | Loss: 0.00002209
Iteration 82/1000 | Loss: 0.00002209
Iteration 83/1000 | Loss: 0.00002209
Iteration 84/1000 | Loss: 0.00002209
Iteration 85/1000 | Loss: 0.00002208
Iteration 86/1000 | Loss: 0.00002208
Iteration 87/1000 | Loss: 0.00002208
Iteration 88/1000 | Loss: 0.00002208
Iteration 89/1000 | Loss: 0.00002208
Iteration 90/1000 | Loss: 0.00002208
Iteration 91/1000 | Loss: 0.00002208
Iteration 92/1000 | Loss: 0.00002208
Iteration 93/1000 | Loss: 0.00002208
Iteration 94/1000 | Loss: 0.00002208
Iteration 95/1000 | Loss: 0.00002208
Iteration 96/1000 | Loss: 0.00002208
Iteration 97/1000 | Loss: 0.00002208
Iteration 98/1000 | Loss: 0.00002208
Iteration 99/1000 | Loss: 0.00002208
Iteration 100/1000 | Loss: 0.00002208
Iteration 101/1000 | Loss: 0.00002208
Iteration 102/1000 | Loss: 0.00002208
Iteration 103/1000 | Loss: 0.00002208
Iteration 104/1000 | Loss: 0.00002208
Iteration 105/1000 | Loss: 0.00002208
Iteration 106/1000 | Loss: 0.00002208
Iteration 107/1000 | Loss: 0.00002208
Iteration 108/1000 | Loss: 0.00002208
Iteration 109/1000 | Loss: 0.00002208
Iteration 110/1000 | Loss: 0.00002208
Iteration 111/1000 | Loss: 0.00002208
Iteration 112/1000 | Loss: 0.00002208
Iteration 113/1000 | Loss: 0.00002208
Iteration 114/1000 | Loss: 0.00002208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.2081258066464216e-05, 2.2081258066464216e-05, 2.2081258066464216e-05, 2.2081258066464216e-05, 2.2081258066464216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2081258066464216e-05

Optimization complete. Final v2v error: 3.931565761566162 mm

Highest mean error: 4.282677173614502 mm for frame 113

Lowest mean error: 3.679593324661255 mm for frame 8

Saving results

Total time: 32.56401991844177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412864
Iteration 2/25 | Loss: 0.00115019
Iteration 3/25 | Loss: 0.00109398
Iteration 4/25 | Loss: 0.00108771
Iteration 5/25 | Loss: 0.00108580
Iteration 6/25 | Loss: 0.00108559
Iteration 7/25 | Loss: 0.00108559
Iteration 8/25 | Loss: 0.00108559
Iteration 9/25 | Loss: 0.00108559
Iteration 10/25 | Loss: 0.00108559
Iteration 11/25 | Loss: 0.00108559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010855855653062463, 0.0010855855653062463, 0.0010855855653062463, 0.0010855855653062463, 0.0010855855653062463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010855855653062463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36856246
Iteration 2/25 | Loss: 0.00076842
Iteration 3/25 | Loss: 0.00076842
Iteration 4/25 | Loss: 0.00076842
Iteration 5/25 | Loss: 0.00076841
Iteration 6/25 | Loss: 0.00076841
Iteration 7/25 | Loss: 0.00076841
Iteration 8/25 | Loss: 0.00076841
Iteration 9/25 | Loss: 0.00076841
Iteration 10/25 | Loss: 0.00076841
Iteration 11/25 | Loss: 0.00076841
Iteration 12/25 | Loss: 0.00076841
Iteration 13/25 | Loss: 0.00076841
Iteration 14/25 | Loss: 0.00076841
Iteration 15/25 | Loss: 0.00076841
Iteration 16/25 | Loss: 0.00076841
Iteration 17/25 | Loss: 0.00076841
Iteration 18/25 | Loss: 0.00076841
Iteration 19/25 | Loss: 0.00076841
Iteration 20/25 | Loss: 0.00076841
Iteration 21/25 | Loss: 0.00076841
Iteration 22/25 | Loss: 0.00076841
Iteration 23/25 | Loss: 0.00076841
Iteration 24/25 | Loss: 0.00076841
Iteration 25/25 | Loss: 0.00076841

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076841
Iteration 2/1000 | Loss: 0.00002328
Iteration 3/1000 | Loss: 0.00001365
Iteration 4/1000 | Loss: 0.00001211
Iteration 5/1000 | Loss: 0.00001158
Iteration 6/1000 | Loss: 0.00001145
Iteration 7/1000 | Loss: 0.00001109
Iteration 8/1000 | Loss: 0.00001082
Iteration 9/1000 | Loss: 0.00001055
Iteration 10/1000 | Loss: 0.00001053
Iteration 11/1000 | Loss: 0.00001041
Iteration 12/1000 | Loss: 0.00001022
Iteration 13/1000 | Loss: 0.00001021
Iteration 14/1000 | Loss: 0.00001018
Iteration 15/1000 | Loss: 0.00001011
Iteration 16/1000 | Loss: 0.00001011
Iteration 17/1000 | Loss: 0.00001010
Iteration 18/1000 | Loss: 0.00001009
Iteration 19/1000 | Loss: 0.00001009
Iteration 20/1000 | Loss: 0.00001008
Iteration 21/1000 | Loss: 0.00001007
Iteration 22/1000 | Loss: 0.00001006
Iteration 23/1000 | Loss: 0.00001001
Iteration 24/1000 | Loss: 0.00001001
Iteration 25/1000 | Loss: 0.00001000
Iteration 26/1000 | Loss: 0.00001000
Iteration 27/1000 | Loss: 0.00001000
Iteration 28/1000 | Loss: 0.00001000
Iteration 29/1000 | Loss: 0.00001000
Iteration 30/1000 | Loss: 0.00001000
Iteration 31/1000 | Loss: 0.00000999
Iteration 32/1000 | Loss: 0.00000998
Iteration 33/1000 | Loss: 0.00000998
Iteration 34/1000 | Loss: 0.00000997
Iteration 35/1000 | Loss: 0.00000997
Iteration 36/1000 | Loss: 0.00000996
Iteration 37/1000 | Loss: 0.00000996
Iteration 38/1000 | Loss: 0.00000995
Iteration 39/1000 | Loss: 0.00000995
Iteration 40/1000 | Loss: 0.00000994
Iteration 41/1000 | Loss: 0.00000994
Iteration 42/1000 | Loss: 0.00000991
Iteration 43/1000 | Loss: 0.00000991
Iteration 44/1000 | Loss: 0.00000991
Iteration 45/1000 | Loss: 0.00000991
Iteration 46/1000 | Loss: 0.00000991
Iteration 47/1000 | Loss: 0.00000991
Iteration 48/1000 | Loss: 0.00000991
Iteration 49/1000 | Loss: 0.00000991
Iteration 50/1000 | Loss: 0.00000990
Iteration 51/1000 | Loss: 0.00000988
Iteration 52/1000 | Loss: 0.00000988
Iteration 53/1000 | Loss: 0.00000988
Iteration 54/1000 | Loss: 0.00000988
Iteration 55/1000 | Loss: 0.00000988
Iteration 56/1000 | Loss: 0.00000988
Iteration 57/1000 | Loss: 0.00000988
Iteration 58/1000 | Loss: 0.00000988
Iteration 59/1000 | Loss: 0.00000987
Iteration 60/1000 | Loss: 0.00000987
Iteration 61/1000 | Loss: 0.00000987
Iteration 62/1000 | Loss: 0.00000987
Iteration 63/1000 | Loss: 0.00000987
Iteration 64/1000 | Loss: 0.00000987
Iteration 65/1000 | Loss: 0.00000987
Iteration 66/1000 | Loss: 0.00000987
Iteration 67/1000 | Loss: 0.00000987
Iteration 68/1000 | Loss: 0.00000987
Iteration 69/1000 | Loss: 0.00000987
Iteration 70/1000 | Loss: 0.00000987
Iteration 71/1000 | Loss: 0.00000987
Iteration 72/1000 | Loss: 0.00000987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [9.874467650661245e-06, 9.874467650661245e-06, 9.874467650661245e-06, 9.874467650661245e-06, 9.874467650661245e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.874467650661245e-06

Optimization complete. Final v2v error: 2.7215282917022705 mm

Highest mean error: 2.797191619873047 mm for frame 39

Lowest mean error: 2.6102733612060547 mm for frame 126

Saving results

Total time: 27.298994064331055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808257
Iteration 2/25 | Loss: 0.00137639
Iteration 3/25 | Loss: 0.00112487
Iteration 4/25 | Loss: 0.00109409
Iteration 5/25 | Loss: 0.00108667
Iteration 6/25 | Loss: 0.00109080
Iteration 7/25 | Loss: 0.00109360
Iteration 8/25 | Loss: 0.00108723
Iteration 9/25 | Loss: 0.00108382
Iteration 10/25 | Loss: 0.00107711
Iteration 11/25 | Loss: 0.00107434
Iteration 12/25 | Loss: 0.00107340
Iteration 13/25 | Loss: 0.00107285
Iteration 14/25 | Loss: 0.00107266
Iteration 15/25 | Loss: 0.00107260
Iteration 16/25 | Loss: 0.00107259
Iteration 17/25 | Loss: 0.00107259
Iteration 18/25 | Loss: 0.00107257
Iteration 19/25 | Loss: 0.00107257
Iteration 20/25 | Loss: 0.00107256
Iteration 21/25 | Loss: 0.00107256
Iteration 22/25 | Loss: 0.00107256
Iteration 23/25 | Loss: 0.00107256
Iteration 24/25 | Loss: 0.00107256
Iteration 25/25 | Loss: 0.00107256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49734104
Iteration 2/25 | Loss: 0.00069362
Iteration 3/25 | Loss: 0.00068366
Iteration 4/25 | Loss: 0.00068366
Iteration 5/25 | Loss: 0.00068366
Iteration 6/25 | Loss: 0.00068366
Iteration 7/25 | Loss: 0.00068366
Iteration 8/25 | Loss: 0.00068366
Iteration 9/25 | Loss: 0.00068366
Iteration 10/25 | Loss: 0.00068366
Iteration 11/25 | Loss: 0.00068366
Iteration 12/25 | Loss: 0.00068366
Iteration 13/25 | Loss: 0.00068366
Iteration 14/25 | Loss: 0.00068366
Iteration 15/25 | Loss: 0.00068366
Iteration 16/25 | Loss: 0.00068366
Iteration 17/25 | Loss: 0.00068366
Iteration 18/25 | Loss: 0.00068366
Iteration 19/25 | Loss: 0.00068366
Iteration 20/25 | Loss: 0.00068366
Iteration 21/25 | Loss: 0.00068366
Iteration 22/25 | Loss: 0.00068366
Iteration 23/25 | Loss: 0.00068366
Iteration 24/25 | Loss: 0.00068366
Iteration 25/25 | Loss: 0.00068366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006836570100858808, 0.0006836570100858808, 0.0006836570100858808, 0.0006836570100858808, 0.0006836570100858808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006836570100858808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068366
Iteration 2/1000 | Loss: 0.00003385
Iteration 3/1000 | Loss: 0.00002002
Iteration 4/1000 | Loss: 0.00001816
Iteration 5/1000 | Loss: 0.00001721
Iteration 6/1000 | Loss: 0.00001629
Iteration 7/1000 | Loss: 0.00012574
Iteration 8/1000 | Loss: 0.00001694
Iteration 9/1000 | Loss: 0.00001485
Iteration 10/1000 | Loss: 0.00001402
Iteration 11/1000 | Loss: 0.00001366
Iteration 12/1000 | Loss: 0.00001318
Iteration 13/1000 | Loss: 0.00001297
Iteration 14/1000 | Loss: 0.00001279
Iteration 15/1000 | Loss: 0.00001263
Iteration 16/1000 | Loss: 0.00001254
Iteration 17/1000 | Loss: 0.00001253
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001253
Iteration 20/1000 | Loss: 0.00001253
Iteration 21/1000 | Loss: 0.00001252
Iteration 22/1000 | Loss: 0.00001252
Iteration 23/1000 | Loss: 0.00001251
Iteration 24/1000 | Loss: 0.00001249
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001247
Iteration 27/1000 | Loss: 0.00001247
Iteration 28/1000 | Loss: 0.00001247
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001246
Iteration 31/1000 | Loss: 0.00001246
Iteration 32/1000 | Loss: 0.00001246
Iteration 33/1000 | Loss: 0.00001246
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001243
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001242
Iteration 42/1000 | Loss: 0.00001242
Iteration 43/1000 | Loss: 0.00001242
Iteration 44/1000 | Loss: 0.00001242
Iteration 45/1000 | Loss: 0.00001242
Iteration 46/1000 | Loss: 0.00001242
Iteration 47/1000 | Loss: 0.00001242
Iteration 48/1000 | Loss: 0.00001242
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001241
Iteration 51/1000 | Loss: 0.00001240
Iteration 52/1000 | Loss: 0.00001239
Iteration 53/1000 | Loss: 0.00001239
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001238
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001236
Iteration 59/1000 | Loss: 0.00001236
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001236
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001236
Iteration 65/1000 | Loss: 0.00001236
Iteration 66/1000 | Loss: 0.00001236
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001235
Iteration 69/1000 | Loss: 0.00001235
Iteration 70/1000 | Loss: 0.00001235
Iteration 71/1000 | Loss: 0.00001234
Iteration 72/1000 | Loss: 0.00001234
Iteration 73/1000 | Loss: 0.00001234
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001233
Iteration 76/1000 | Loss: 0.00001233
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001231
Iteration 81/1000 | Loss: 0.00001231
Iteration 82/1000 | Loss: 0.00001231
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001230
Iteration 85/1000 | Loss: 0.00001230
Iteration 86/1000 | Loss: 0.00001230
Iteration 87/1000 | Loss: 0.00001229
Iteration 88/1000 | Loss: 0.00001229
Iteration 89/1000 | Loss: 0.00001229
Iteration 90/1000 | Loss: 0.00001229
Iteration 91/1000 | Loss: 0.00001229
Iteration 92/1000 | Loss: 0.00001229
Iteration 93/1000 | Loss: 0.00001229
Iteration 94/1000 | Loss: 0.00001229
Iteration 95/1000 | Loss: 0.00001229
Iteration 96/1000 | Loss: 0.00001229
Iteration 97/1000 | Loss: 0.00001228
Iteration 98/1000 | Loss: 0.00001228
Iteration 99/1000 | Loss: 0.00001228
Iteration 100/1000 | Loss: 0.00001227
Iteration 101/1000 | Loss: 0.00001227
Iteration 102/1000 | Loss: 0.00001227
Iteration 103/1000 | Loss: 0.00001227
Iteration 104/1000 | Loss: 0.00001227
Iteration 105/1000 | Loss: 0.00001227
Iteration 106/1000 | Loss: 0.00001227
Iteration 107/1000 | Loss: 0.00001227
Iteration 108/1000 | Loss: 0.00001227
Iteration 109/1000 | Loss: 0.00001227
Iteration 110/1000 | Loss: 0.00001227
Iteration 111/1000 | Loss: 0.00001226
Iteration 112/1000 | Loss: 0.00001226
Iteration 113/1000 | Loss: 0.00001226
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001226
Iteration 116/1000 | Loss: 0.00001226
Iteration 117/1000 | Loss: 0.00001226
Iteration 118/1000 | Loss: 0.00001226
Iteration 119/1000 | Loss: 0.00001226
Iteration 120/1000 | Loss: 0.00001226
Iteration 121/1000 | Loss: 0.00001226
Iteration 122/1000 | Loss: 0.00001226
Iteration 123/1000 | Loss: 0.00001226
Iteration 124/1000 | Loss: 0.00001226
Iteration 125/1000 | Loss: 0.00001225
Iteration 126/1000 | Loss: 0.00001225
Iteration 127/1000 | Loss: 0.00001225
Iteration 128/1000 | Loss: 0.00001225
Iteration 129/1000 | Loss: 0.00001225
Iteration 130/1000 | Loss: 0.00001225
Iteration 131/1000 | Loss: 0.00001225
Iteration 132/1000 | Loss: 0.00001224
Iteration 133/1000 | Loss: 0.00001224
Iteration 134/1000 | Loss: 0.00001224
Iteration 135/1000 | Loss: 0.00001224
Iteration 136/1000 | Loss: 0.00001224
Iteration 137/1000 | Loss: 0.00001224
Iteration 138/1000 | Loss: 0.00001224
Iteration 139/1000 | Loss: 0.00001224
Iteration 140/1000 | Loss: 0.00001224
Iteration 141/1000 | Loss: 0.00001224
Iteration 142/1000 | Loss: 0.00001224
Iteration 143/1000 | Loss: 0.00001224
Iteration 144/1000 | Loss: 0.00001224
Iteration 145/1000 | Loss: 0.00001224
Iteration 146/1000 | Loss: 0.00001224
Iteration 147/1000 | Loss: 0.00001224
Iteration 148/1000 | Loss: 0.00001223
Iteration 149/1000 | Loss: 0.00001223
Iteration 150/1000 | Loss: 0.00001223
Iteration 151/1000 | Loss: 0.00001223
Iteration 152/1000 | Loss: 0.00001223
Iteration 153/1000 | Loss: 0.00001223
Iteration 154/1000 | Loss: 0.00001223
Iteration 155/1000 | Loss: 0.00001223
Iteration 156/1000 | Loss: 0.00001223
Iteration 157/1000 | Loss: 0.00001223
Iteration 158/1000 | Loss: 0.00001222
Iteration 159/1000 | Loss: 0.00001222
Iteration 160/1000 | Loss: 0.00001222
Iteration 161/1000 | Loss: 0.00001222
Iteration 162/1000 | Loss: 0.00001222
Iteration 163/1000 | Loss: 0.00001222
Iteration 164/1000 | Loss: 0.00001222
Iteration 165/1000 | Loss: 0.00001222
Iteration 166/1000 | Loss: 0.00001222
Iteration 167/1000 | Loss: 0.00001222
Iteration 168/1000 | Loss: 0.00001222
Iteration 169/1000 | Loss: 0.00001222
Iteration 170/1000 | Loss: 0.00001221
Iteration 171/1000 | Loss: 0.00001221
Iteration 172/1000 | Loss: 0.00001221
Iteration 173/1000 | Loss: 0.00001221
Iteration 174/1000 | Loss: 0.00001221
Iteration 175/1000 | Loss: 0.00001221
Iteration 176/1000 | Loss: 0.00001221
Iteration 177/1000 | Loss: 0.00001221
Iteration 178/1000 | Loss: 0.00001221
Iteration 179/1000 | Loss: 0.00001221
Iteration 180/1000 | Loss: 0.00001221
Iteration 181/1000 | Loss: 0.00001221
Iteration 182/1000 | Loss: 0.00001221
Iteration 183/1000 | Loss: 0.00001221
Iteration 184/1000 | Loss: 0.00001221
Iteration 185/1000 | Loss: 0.00001221
Iteration 186/1000 | Loss: 0.00001221
Iteration 187/1000 | Loss: 0.00001221
Iteration 188/1000 | Loss: 0.00001221
Iteration 189/1000 | Loss: 0.00001221
Iteration 190/1000 | Loss: 0.00001221
Iteration 191/1000 | Loss: 0.00001221
Iteration 192/1000 | Loss: 0.00001221
Iteration 193/1000 | Loss: 0.00001221
Iteration 194/1000 | Loss: 0.00001221
Iteration 195/1000 | Loss: 0.00001221
Iteration 196/1000 | Loss: 0.00001221
Iteration 197/1000 | Loss: 0.00001221
Iteration 198/1000 | Loss: 0.00001221
Iteration 199/1000 | Loss: 0.00001221
Iteration 200/1000 | Loss: 0.00001221
Iteration 201/1000 | Loss: 0.00001221
Iteration 202/1000 | Loss: 0.00001221
Iteration 203/1000 | Loss: 0.00001221
Iteration 204/1000 | Loss: 0.00001221
Iteration 205/1000 | Loss: 0.00001221
Iteration 206/1000 | Loss: 0.00001221
Iteration 207/1000 | Loss: 0.00001221
Iteration 208/1000 | Loss: 0.00001221
Iteration 209/1000 | Loss: 0.00001221
Iteration 210/1000 | Loss: 0.00001221
Iteration 211/1000 | Loss: 0.00001221
Iteration 212/1000 | Loss: 0.00001221
Iteration 213/1000 | Loss: 0.00001221
Iteration 214/1000 | Loss: 0.00001221
Iteration 215/1000 | Loss: 0.00001221
Iteration 216/1000 | Loss: 0.00001221
Iteration 217/1000 | Loss: 0.00001221
Iteration 218/1000 | Loss: 0.00001221
Iteration 219/1000 | Loss: 0.00001221
Iteration 220/1000 | Loss: 0.00001221
Iteration 221/1000 | Loss: 0.00001221
Iteration 222/1000 | Loss: 0.00001221
Iteration 223/1000 | Loss: 0.00001221
Iteration 224/1000 | Loss: 0.00001221
Iteration 225/1000 | Loss: 0.00001221
Iteration 226/1000 | Loss: 0.00001221
Iteration 227/1000 | Loss: 0.00001221
Iteration 228/1000 | Loss: 0.00001221
Iteration 229/1000 | Loss: 0.00001221
Iteration 230/1000 | Loss: 0.00001221
Iteration 231/1000 | Loss: 0.00001221
Iteration 232/1000 | Loss: 0.00001221
Iteration 233/1000 | Loss: 0.00001221
Iteration 234/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.2211287867103238e-05, 1.2211287867103238e-05, 1.2211287867103238e-05, 1.2211287867103238e-05, 1.2211287867103238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2211287867103238e-05

Optimization complete. Final v2v error: 2.996415138244629 mm

Highest mean error: 3.6600077152252197 mm for frame 108

Lowest mean error: 2.4793803691864014 mm for frame 75

Saving results

Total time: 70.64390921592712
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021601
Iteration 2/25 | Loss: 0.00258152
Iteration 3/25 | Loss: 0.00159893
Iteration 4/25 | Loss: 0.00145226
Iteration 5/25 | Loss: 0.00138727
Iteration 6/25 | Loss: 0.00138911
Iteration 7/25 | Loss: 0.00135193
Iteration 8/25 | Loss: 0.00132939
Iteration 9/25 | Loss: 0.00131213
Iteration 10/25 | Loss: 0.00129186
Iteration 11/25 | Loss: 0.00127537
Iteration 12/25 | Loss: 0.00127472
Iteration 13/25 | Loss: 0.00126923
Iteration 14/25 | Loss: 0.00126803
Iteration 15/25 | Loss: 0.00126398
Iteration 16/25 | Loss: 0.00125610
Iteration 17/25 | Loss: 0.00126226
Iteration 18/25 | Loss: 0.00125272
Iteration 19/25 | Loss: 0.00124803
Iteration 20/25 | Loss: 0.00124297
Iteration 21/25 | Loss: 0.00124244
Iteration 22/25 | Loss: 0.00124526
Iteration 23/25 | Loss: 0.00124495
Iteration 24/25 | Loss: 0.00123251
Iteration 25/25 | Loss: 0.00123470

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31038344
Iteration 2/25 | Loss: 0.00228583
Iteration 3/25 | Loss: 0.00144916
Iteration 4/25 | Loss: 0.00144916
Iteration 5/25 | Loss: 0.00144916
Iteration 6/25 | Loss: 0.00144916
Iteration 7/25 | Loss: 0.00144916
Iteration 8/25 | Loss: 0.00144915
Iteration 9/25 | Loss: 0.00144915
Iteration 10/25 | Loss: 0.00144915
Iteration 11/25 | Loss: 0.00144915
Iteration 12/25 | Loss: 0.00144915
Iteration 13/25 | Loss: 0.00144915
Iteration 14/25 | Loss: 0.00144915
Iteration 15/25 | Loss: 0.00144915
Iteration 16/25 | Loss: 0.00144915
Iteration 17/25 | Loss: 0.00144915
Iteration 18/25 | Loss: 0.00144915
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014491538750007749, 0.0014491538750007749, 0.0014491538750007749, 0.0014491538750007749, 0.0014491538750007749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014491538750007749

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144915
Iteration 2/1000 | Loss: 0.00035324
Iteration 3/1000 | Loss: 0.00062728
Iteration 4/1000 | Loss: 0.00081259
Iteration 5/1000 | Loss: 0.00067871
Iteration 6/1000 | Loss: 0.00019584
Iteration 7/1000 | Loss: 0.00013531
Iteration 8/1000 | Loss: 0.00078071
Iteration 9/1000 | Loss: 0.00079943
Iteration 10/1000 | Loss: 0.00104776
Iteration 11/1000 | Loss: 0.00080510
Iteration 12/1000 | Loss: 0.00160482
Iteration 13/1000 | Loss: 0.00216361
Iteration 14/1000 | Loss: 0.00208748
Iteration 15/1000 | Loss: 0.00126839
Iteration 16/1000 | Loss: 0.00121507
Iteration 17/1000 | Loss: 0.00137471
Iteration 18/1000 | Loss: 0.00089133
Iteration 19/1000 | Loss: 0.00097778
Iteration 20/1000 | Loss: 0.00115126
Iteration 21/1000 | Loss: 0.00035075
Iteration 22/1000 | Loss: 0.00031207
Iteration 23/1000 | Loss: 0.00014573
Iteration 24/1000 | Loss: 0.00013145
Iteration 25/1000 | Loss: 0.00070745
Iteration 26/1000 | Loss: 0.00032955
Iteration 27/1000 | Loss: 0.00039203
Iteration 28/1000 | Loss: 0.00226945
Iteration 29/1000 | Loss: 0.00030354
Iteration 30/1000 | Loss: 0.00038078
Iteration 31/1000 | Loss: 0.00047727
Iteration 32/1000 | Loss: 0.00022054
Iteration 33/1000 | Loss: 0.00083439
Iteration 34/1000 | Loss: 0.00171106
Iteration 35/1000 | Loss: 0.00106407
Iteration 36/1000 | Loss: 0.00094500
Iteration 37/1000 | Loss: 0.00110041
Iteration 38/1000 | Loss: 0.00006271
Iteration 39/1000 | Loss: 0.00005592
Iteration 40/1000 | Loss: 0.00005204
Iteration 41/1000 | Loss: 0.00004977
Iteration 42/1000 | Loss: 0.00004719
Iteration 43/1000 | Loss: 0.00004530
Iteration 44/1000 | Loss: 0.00005571
Iteration 45/1000 | Loss: 0.00436275
Iteration 46/1000 | Loss: 0.00107977
Iteration 47/1000 | Loss: 0.00004753
Iteration 48/1000 | Loss: 0.00010235
Iteration 49/1000 | Loss: 0.00158846
Iteration 50/1000 | Loss: 0.00003430
Iteration 51/1000 | Loss: 0.00009821
Iteration 52/1000 | Loss: 0.00002491
Iteration 53/1000 | Loss: 0.00002175
Iteration 54/1000 | Loss: 0.00002001
Iteration 55/1000 | Loss: 0.00001844
Iteration 56/1000 | Loss: 0.00001751
Iteration 57/1000 | Loss: 0.00001677
Iteration 58/1000 | Loss: 0.00001619
Iteration 59/1000 | Loss: 0.00001576
Iteration 60/1000 | Loss: 0.00007952
Iteration 61/1000 | Loss: 0.00016056
Iteration 62/1000 | Loss: 0.00007065
Iteration 63/1000 | Loss: 0.00001551
Iteration 64/1000 | Loss: 0.00001517
Iteration 65/1000 | Loss: 0.00001509
Iteration 66/1000 | Loss: 0.00001569
Iteration 67/1000 | Loss: 0.00001498
Iteration 68/1000 | Loss: 0.00001492
Iteration 69/1000 | Loss: 0.00001491
Iteration 70/1000 | Loss: 0.00001490
Iteration 71/1000 | Loss: 0.00001481
Iteration 72/1000 | Loss: 0.00001477
Iteration 73/1000 | Loss: 0.00001515
Iteration 74/1000 | Loss: 0.00001469
Iteration 75/1000 | Loss: 0.00001469
Iteration 76/1000 | Loss: 0.00001468
Iteration 77/1000 | Loss: 0.00001468
Iteration 78/1000 | Loss: 0.00001468
Iteration 79/1000 | Loss: 0.00001468
Iteration 80/1000 | Loss: 0.00001468
Iteration 81/1000 | Loss: 0.00001467
Iteration 82/1000 | Loss: 0.00001467
Iteration 83/1000 | Loss: 0.00001467
Iteration 84/1000 | Loss: 0.00001467
Iteration 85/1000 | Loss: 0.00001467
Iteration 86/1000 | Loss: 0.00001467
Iteration 87/1000 | Loss: 0.00001467
Iteration 88/1000 | Loss: 0.00001467
Iteration 89/1000 | Loss: 0.00001467
Iteration 90/1000 | Loss: 0.00001466
Iteration 91/1000 | Loss: 0.00001466
Iteration 92/1000 | Loss: 0.00001466
Iteration 93/1000 | Loss: 0.00001465
Iteration 94/1000 | Loss: 0.00001465
Iteration 95/1000 | Loss: 0.00001475
Iteration 96/1000 | Loss: 0.00008380
Iteration 97/1000 | Loss: 0.00002078
Iteration 98/1000 | Loss: 0.00002322
Iteration 99/1000 | Loss: 0.00001676
Iteration 100/1000 | Loss: 0.00006942
Iteration 101/1000 | Loss: 0.00001847
Iteration 102/1000 | Loss: 0.00001461
Iteration 103/1000 | Loss: 0.00001460
Iteration 104/1000 | Loss: 0.00001460
Iteration 105/1000 | Loss: 0.00001460
Iteration 106/1000 | Loss: 0.00001460
Iteration 107/1000 | Loss: 0.00001460
Iteration 108/1000 | Loss: 0.00001460
Iteration 109/1000 | Loss: 0.00001460
Iteration 110/1000 | Loss: 0.00001460
Iteration 111/1000 | Loss: 0.00001460
Iteration 112/1000 | Loss: 0.00001460
Iteration 113/1000 | Loss: 0.00001460
Iteration 114/1000 | Loss: 0.00001460
Iteration 115/1000 | Loss: 0.00001460
Iteration 116/1000 | Loss: 0.00001459
Iteration 117/1000 | Loss: 0.00001459
Iteration 118/1000 | Loss: 0.00001459
Iteration 119/1000 | Loss: 0.00001458
Iteration 120/1000 | Loss: 0.00001458
Iteration 121/1000 | Loss: 0.00001457
Iteration 122/1000 | Loss: 0.00001457
Iteration 123/1000 | Loss: 0.00001457
Iteration 124/1000 | Loss: 0.00001457
Iteration 125/1000 | Loss: 0.00001457
Iteration 126/1000 | Loss: 0.00001457
Iteration 127/1000 | Loss: 0.00001457
Iteration 128/1000 | Loss: 0.00001457
Iteration 129/1000 | Loss: 0.00001457
Iteration 130/1000 | Loss: 0.00001457
Iteration 131/1000 | Loss: 0.00001457
Iteration 132/1000 | Loss: 0.00001456
Iteration 133/1000 | Loss: 0.00001456
Iteration 134/1000 | Loss: 0.00001456
Iteration 135/1000 | Loss: 0.00006763
Iteration 136/1000 | Loss: 0.00005216
Iteration 137/1000 | Loss: 0.00001809
Iteration 138/1000 | Loss: 0.00001521
Iteration 139/1000 | Loss: 0.00001526
Iteration 140/1000 | Loss: 0.00001486
Iteration 141/1000 | Loss: 0.00001458
Iteration 142/1000 | Loss: 0.00001458
Iteration 143/1000 | Loss: 0.00001457
Iteration 144/1000 | Loss: 0.00001457
Iteration 145/1000 | Loss: 0.00001456
Iteration 146/1000 | Loss: 0.00001456
Iteration 147/1000 | Loss: 0.00001456
Iteration 148/1000 | Loss: 0.00001456
Iteration 149/1000 | Loss: 0.00001456
Iteration 150/1000 | Loss: 0.00001456
Iteration 151/1000 | Loss: 0.00001456
Iteration 152/1000 | Loss: 0.00001456
Iteration 153/1000 | Loss: 0.00001456
Iteration 154/1000 | Loss: 0.00001456
Iteration 155/1000 | Loss: 0.00001456
Iteration 156/1000 | Loss: 0.00001456
Iteration 157/1000 | Loss: 0.00001456
Iteration 158/1000 | Loss: 0.00001456
Iteration 159/1000 | Loss: 0.00001455
Iteration 160/1000 | Loss: 0.00001455
Iteration 161/1000 | Loss: 0.00001455
Iteration 162/1000 | Loss: 0.00001455
Iteration 163/1000 | Loss: 0.00001455
Iteration 164/1000 | Loss: 0.00001454
Iteration 165/1000 | Loss: 0.00001454
Iteration 166/1000 | Loss: 0.00001454
Iteration 167/1000 | Loss: 0.00001454
Iteration 168/1000 | Loss: 0.00001453
Iteration 169/1000 | Loss: 0.00001453
Iteration 170/1000 | Loss: 0.00001453
Iteration 171/1000 | Loss: 0.00001453
Iteration 172/1000 | Loss: 0.00001453
Iteration 173/1000 | Loss: 0.00001453
Iteration 174/1000 | Loss: 0.00001453
Iteration 175/1000 | Loss: 0.00001453
Iteration 176/1000 | Loss: 0.00001452
Iteration 177/1000 | Loss: 0.00001452
Iteration 178/1000 | Loss: 0.00001452
Iteration 179/1000 | Loss: 0.00001452
Iteration 180/1000 | Loss: 0.00001452
Iteration 181/1000 | Loss: 0.00003707
Iteration 182/1000 | Loss: 0.00001544
Iteration 183/1000 | Loss: 0.00001485
Iteration 184/1000 | Loss: 0.00001454
Iteration 185/1000 | Loss: 0.00001454
Iteration 186/1000 | Loss: 0.00001454
Iteration 187/1000 | Loss: 0.00001454
Iteration 188/1000 | Loss: 0.00001454
Iteration 189/1000 | Loss: 0.00001454
Iteration 190/1000 | Loss: 0.00001454
Iteration 191/1000 | Loss: 0.00001454
Iteration 192/1000 | Loss: 0.00001454
Iteration 193/1000 | Loss: 0.00001453
Iteration 194/1000 | Loss: 0.00001453
Iteration 195/1000 | Loss: 0.00001453
Iteration 196/1000 | Loss: 0.00001453
Iteration 197/1000 | Loss: 0.00001453
Iteration 198/1000 | Loss: 0.00001453
Iteration 199/1000 | Loss: 0.00001453
Iteration 200/1000 | Loss: 0.00001453
Iteration 201/1000 | Loss: 0.00001453
Iteration 202/1000 | Loss: 0.00001453
Iteration 203/1000 | Loss: 0.00001453
Iteration 204/1000 | Loss: 0.00001453
Iteration 205/1000 | Loss: 0.00001453
Iteration 206/1000 | Loss: 0.00001453
Iteration 207/1000 | Loss: 0.00001453
Iteration 208/1000 | Loss: 0.00001453
Iteration 209/1000 | Loss: 0.00001453
Iteration 210/1000 | Loss: 0.00001453
Iteration 211/1000 | Loss: 0.00001453
Iteration 212/1000 | Loss: 0.00001453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.4529878171742894e-05, 1.4529878171742894e-05, 1.4529878171742894e-05, 1.4529878171742894e-05, 1.4529878171742894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4529878171742894e-05

Optimization complete. Final v2v error: 3.184006690979004 mm

Highest mean error: 4.869724273681641 mm for frame 43

Lowest mean error: 2.565634250640869 mm for frame 61

Saving results

Total time: 195.0149440765381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831805
Iteration 2/25 | Loss: 0.00129413
Iteration 3/25 | Loss: 0.00114274
Iteration 4/25 | Loss: 0.00109164
Iteration 5/25 | Loss: 0.00108646
Iteration 6/25 | Loss: 0.00108212
Iteration 7/25 | Loss: 0.00108133
Iteration 8/25 | Loss: 0.00107974
Iteration 9/25 | Loss: 0.00107972
Iteration 10/25 | Loss: 0.00108284
Iteration 11/25 | Loss: 0.00108222
Iteration 12/25 | Loss: 0.00107997
Iteration 13/25 | Loss: 0.00107963
Iteration 14/25 | Loss: 0.00107901
Iteration 15/25 | Loss: 0.00107878
Iteration 16/25 | Loss: 0.00107882
Iteration 17/25 | Loss: 0.00107868
Iteration 18/25 | Loss: 0.00107856
Iteration 19/25 | Loss: 0.00107856
Iteration 20/25 | Loss: 0.00107856
Iteration 21/25 | Loss: 0.00107856
Iteration 22/25 | Loss: 0.00107856
Iteration 23/25 | Loss: 0.00107860
Iteration 24/25 | Loss: 0.00107856
Iteration 25/25 | Loss: 0.00107856

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69402409
Iteration 2/25 | Loss: 0.00085683
Iteration 3/25 | Loss: 0.00085683
Iteration 4/25 | Loss: 0.00085682
Iteration 5/25 | Loss: 0.00085682
Iteration 6/25 | Loss: 0.00085682
Iteration 7/25 | Loss: 0.00085682
Iteration 8/25 | Loss: 0.00085682
Iteration 9/25 | Loss: 0.00085682
Iteration 10/25 | Loss: 0.00085682
Iteration 11/25 | Loss: 0.00085682
Iteration 12/25 | Loss: 0.00085682
Iteration 13/25 | Loss: 0.00085682
Iteration 14/25 | Loss: 0.00085682
Iteration 15/25 | Loss: 0.00085682
Iteration 16/25 | Loss: 0.00085682
Iteration 17/25 | Loss: 0.00085682
Iteration 18/25 | Loss: 0.00085682
Iteration 19/25 | Loss: 0.00085682
Iteration 20/25 | Loss: 0.00085682
Iteration 21/25 | Loss: 0.00085682
Iteration 22/25 | Loss: 0.00085682
Iteration 23/25 | Loss: 0.00085682
Iteration 24/25 | Loss: 0.00085682
Iteration 25/25 | Loss: 0.00085682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085682
Iteration 2/1000 | Loss: 0.00002074
Iteration 3/1000 | Loss: 0.00001332
Iteration 4/1000 | Loss: 0.00001199
Iteration 5/1000 | Loss: 0.00001540
Iteration 6/1000 | Loss: 0.00001145
Iteration 7/1000 | Loss: 0.00001088
Iteration 8/1000 | Loss: 0.00001084
Iteration 9/1000 | Loss: 0.00001043
Iteration 10/1000 | Loss: 0.00001033
Iteration 11/1000 | Loss: 0.00001036
Iteration 12/1000 | Loss: 0.00001014
Iteration 13/1000 | Loss: 0.00001014
Iteration 14/1000 | Loss: 0.00001003
Iteration 15/1000 | Loss: 0.00001003
Iteration 16/1000 | Loss: 0.00001003
Iteration 17/1000 | Loss: 0.00001003
Iteration 18/1000 | Loss: 0.00001001
Iteration 19/1000 | Loss: 0.00001000
Iteration 20/1000 | Loss: 0.00000999
Iteration 21/1000 | Loss: 0.00000999
Iteration 22/1000 | Loss: 0.00000999
Iteration 23/1000 | Loss: 0.00000999
Iteration 24/1000 | Loss: 0.00000999
Iteration 25/1000 | Loss: 0.00000999
Iteration 26/1000 | Loss: 0.00000999
Iteration 27/1000 | Loss: 0.00000999
Iteration 28/1000 | Loss: 0.00001077
Iteration 29/1000 | Loss: 0.00001077
Iteration 30/1000 | Loss: 0.00000992
Iteration 31/1000 | Loss: 0.00000990
Iteration 32/1000 | Loss: 0.00000989
Iteration 33/1000 | Loss: 0.00000989
Iteration 34/1000 | Loss: 0.00000987
Iteration 35/1000 | Loss: 0.00000990
Iteration 36/1000 | Loss: 0.00000982
Iteration 37/1000 | Loss: 0.00000982
Iteration 38/1000 | Loss: 0.00000981
Iteration 39/1000 | Loss: 0.00001009
Iteration 40/1000 | Loss: 0.00000977
Iteration 41/1000 | Loss: 0.00000977
Iteration 42/1000 | Loss: 0.00000977
Iteration 43/1000 | Loss: 0.00000977
Iteration 44/1000 | Loss: 0.00000977
Iteration 45/1000 | Loss: 0.00000977
Iteration 46/1000 | Loss: 0.00000977
Iteration 47/1000 | Loss: 0.00000977
Iteration 48/1000 | Loss: 0.00000977
Iteration 49/1000 | Loss: 0.00000977
Iteration 50/1000 | Loss: 0.00000997
Iteration 51/1000 | Loss: 0.00000974
Iteration 52/1000 | Loss: 0.00000973
Iteration 53/1000 | Loss: 0.00000971
Iteration 54/1000 | Loss: 0.00000971
Iteration 55/1000 | Loss: 0.00000988
Iteration 56/1000 | Loss: 0.00000974
Iteration 57/1000 | Loss: 0.00000966
Iteration 58/1000 | Loss: 0.00000966
Iteration 59/1000 | Loss: 0.00000966
Iteration 60/1000 | Loss: 0.00000966
Iteration 61/1000 | Loss: 0.00000966
Iteration 62/1000 | Loss: 0.00000966
Iteration 63/1000 | Loss: 0.00000966
Iteration 64/1000 | Loss: 0.00000966
Iteration 65/1000 | Loss: 0.00000966
Iteration 66/1000 | Loss: 0.00000966
Iteration 67/1000 | Loss: 0.00000966
Iteration 68/1000 | Loss: 0.00000966
Iteration 69/1000 | Loss: 0.00000966
Iteration 70/1000 | Loss: 0.00000965
Iteration 71/1000 | Loss: 0.00000965
Iteration 72/1000 | Loss: 0.00000965
Iteration 73/1000 | Loss: 0.00000964
Iteration 74/1000 | Loss: 0.00000968
Iteration 75/1000 | Loss: 0.00000960
Iteration 76/1000 | Loss: 0.00000960
Iteration 77/1000 | Loss: 0.00000960
Iteration 78/1000 | Loss: 0.00000960
Iteration 79/1000 | Loss: 0.00000960
Iteration 80/1000 | Loss: 0.00000960
Iteration 81/1000 | Loss: 0.00000960
Iteration 82/1000 | Loss: 0.00000960
Iteration 83/1000 | Loss: 0.00000960
Iteration 84/1000 | Loss: 0.00000960
Iteration 85/1000 | Loss: 0.00000960
Iteration 86/1000 | Loss: 0.00000960
Iteration 87/1000 | Loss: 0.00000960
Iteration 88/1000 | Loss: 0.00000960
Iteration 89/1000 | Loss: 0.00000960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [9.595913070370443e-06, 9.595913070370443e-06, 9.595913070370443e-06, 9.595913070370443e-06, 9.595913070370443e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.595913070370443e-06

Optimization complete. Final v2v error: 2.6495120525360107 mm

Highest mean error: 8.475011825561523 mm for frame 25

Lowest mean error: 2.427218198776245 mm for frame 229

Saving results

Total time: 66.36817336082458
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010813
Iteration 2/25 | Loss: 0.00182154
Iteration 3/25 | Loss: 0.00157009
Iteration 4/25 | Loss: 0.00147227
Iteration 5/25 | Loss: 0.00142368
Iteration 6/25 | Loss: 0.00140341
Iteration 7/25 | Loss: 0.00124942
Iteration 8/25 | Loss: 0.00121238
Iteration 9/25 | Loss: 0.00120245
Iteration 10/25 | Loss: 0.00120388
Iteration 11/25 | Loss: 0.00120460
Iteration 12/25 | Loss: 0.00117306
Iteration 13/25 | Loss: 0.00116454
Iteration 14/25 | Loss: 0.00116137
Iteration 15/25 | Loss: 0.00115970
Iteration 16/25 | Loss: 0.00115739
Iteration 17/25 | Loss: 0.00115644
Iteration 18/25 | Loss: 0.00115595
Iteration 19/25 | Loss: 0.00115606
Iteration 20/25 | Loss: 0.00115583
Iteration 21/25 | Loss: 0.00115601
Iteration 22/25 | Loss: 0.00115586
Iteration 23/25 | Loss: 0.00115597
Iteration 24/25 | Loss: 0.00115587
Iteration 25/25 | Loss: 0.00115592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41654599
Iteration 2/25 | Loss: 0.00104815
Iteration 3/25 | Loss: 0.00093773
Iteration 4/25 | Loss: 0.00093772
Iteration 5/25 | Loss: 0.00093772
Iteration 6/25 | Loss: 0.00093772
Iteration 7/25 | Loss: 0.00093772
Iteration 8/25 | Loss: 0.00093772
Iteration 9/25 | Loss: 0.00093772
Iteration 10/25 | Loss: 0.00093772
Iteration 11/25 | Loss: 0.00093772
Iteration 12/25 | Loss: 0.00093772
Iteration 13/25 | Loss: 0.00093772
Iteration 14/25 | Loss: 0.00093772
Iteration 15/25 | Loss: 0.00093772
Iteration 16/25 | Loss: 0.00093772
Iteration 17/25 | Loss: 0.00093772
Iteration 18/25 | Loss: 0.00093772
Iteration 19/25 | Loss: 0.00093772
Iteration 20/25 | Loss: 0.00093772
Iteration 21/25 | Loss: 0.00093772
Iteration 22/25 | Loss: 0.00093772
Iteration 23/25 | Loss: 0.00093772
Iteration 24/25 | Loss: 0.00093772
Iteration 25/25 | Loss: 0.00093772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093772
Iteration 2/1000 | Loss: 0.00016226
Iteration 3/1000 | Loss: 0.00056435
Iteration 4/1000 | Loss: 0.00008496
Iteration 5/1000 | Loss: 0.00001618
Iteration 6/1000 | Loss: 0.00016183
Iteration 7/1000 | Loss: 0.00001551
Iteration 8/1000 | Loss: 0.00001487
Iteration 9/1000 | Loss: 0.00016657
Iteration 10/1000 | Loss: 0.00031466
Iteration 11/1000 | Loss: 0.00003505
Iteration 12/1000 | Loss: 0.00002827
Iteration 13/1000 | Loss: 0.00003373
Iteration 14/1000 | Loss: 0.00006686
Iteration 15/1000 | Loss: 0.00001413
Iteration 16/1000 | Loss: 0.00001413
Iteration 17/1000 | Loss: 0.00001413
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001413
Iteration 20/1000 | Loss: 0.00001413
Iteration 21/1000 | Loss: 0.00001413
Iteration 22/1000 | Loss: 0.00001413
Iteration 23/1000 | Loss: 0.00001413
Iteration 24/1000 | Loss: 0.00001412
Iteration 25/1000 | Loss: 0.00003593
Iteration 26/1000 | Loss: 0.00001386
Iteration 27/1000 | Loss: 0.00001376
Iteration 28/1000 | Loss: 0.00001374
Iteration 29/1000 | Loss: 0.00001373
Iteration 30/1000 | Loss: 0.00001365
Iteration 31/1000 | Loss: 0.00001351
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001337
Iteration 34/1000 | Loss: 0.00001335
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001332
Iteration 37/1000 | Loss: 0.00022967
Iteration 38/1000 | Loss: 0.00016383
Iteration 39/1000 | Loss: 0.00002593
Iteration 40/1000 | Loss: 0.00002094
Iteration 41/1000 | Loss: 0.00005536
Iteration 42/1000 | Loss: 0.00001354
Iteration 43/1000 | Loss: 0.00001319
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001331
Iteration 46/1000 | Loss: 0.00001301
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001279
Iteration 49/1000 | Loss: 0.00001277
Iteration 50/1000 | Loss: 0.00001276
Iteration 51/1000 | Loss: 0.00001264
Iteration 52/1000 | Loss: 0.00001250
Iteration 53/1000 | Loss: 0.00001249
Iteration 54/1000 | Loss: 0.00001249
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001248
Iteration 57/1000 | Loss: 0.00001248
Iteration 58/1000 | Loss: 0.00001248
Iteration 59/1000 | Loss: 0.00001248
Iteration 60/1000 | Loss: 0.00001247
Iteration 61/1000 | Loss: 0.00001247
Iteration 62/1000 | Loss: 0.00001247
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001246
Iteration 65/1000 | Loss: 0.00001245
Iteration 66/1000 | Loss: 0.00001245
Iteration 67/1000 | Loss: 0.00001245
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001243
Iteration 72/1000 | Loss: 0.00001243
Iteration 73/1000 | Loss: 0.00001243
Iteration 74/1000 | Loss: 0.00001243
Iteration 75/1000 | Loss: 0.00001243
Iteration 76/1000 | Loss: 0.00001243
Iteration 77/1000 | Loss: 0.00001242
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00001241
Iteration 80/1000 | Loss: 0.00001241
Iteration 81/1000 | Loss: 0.00001241
Iteration 82/1000 | Loss: 0.00001241
Iteration 83/1000 | Loss: 0.00001240
Iteration 84/1000 | Loss: 0.00001240
Iteration 85/1000 | Loss: 0.00001240
Iteration 86/1000 | Loss: 0.00001240
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001240
Iteration 91/1000 | Loss: 0.00001240
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001239
Iteration 94/1000 | Loss: 0.00001239
Iteration 95/1000 | Loss: 0.00001239
Iteration 96/1000 | Loss: 0.00001239
Iteration 97/1000 | Loss: 0.00001239
Iteration 98/1000 | Loss: 0.00001239
Iteration 99/1000 | Loss: 0.00001239
Iteration 100/1000 | Loss: 0.00001239
Iteration 101/1000 | Loss: 0.00001239
Iteration 102/1000 | Loss: 0.00001239
Iteration 103/1000 | Loss: 0.00001239
Iteration 104/1000 | Loss: 0.00001239
Iteration 105/1000 | Loss: 0.00001239
Iteration 106/1000 | Loss: 0.00001239
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001238
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001238
Iteration 112/1000 | Loss: 0.00001238
Iteration 113/1000 | Loss: 0.00001238
Iteration 114/1000 | Loss: 0.00001238
Iteration 115/1000 | Loss: 0.00018509
Iteration 116/1000 | Loss: 0.00002026
Iteration 117/1000 | Loss: 0.00001873
Iteration 118/1000 | Loss: 0.00001249
Iteration 119/1000 | Loss: 0.00001239
Iteration 120/1000 | Loss: 0.00001238
Iteration 121/1000 | Loss: 0.00001238
Iteration 122/1000 | Loss: 0.00001238
Iteration 123/1000 | Loss: 0.00001238
Iteration 124/1000 | Loss: 0.00001237
Iteration 125/1000 | Loss: 0.00001237
Iteration 126/1000 | Loss: 0.00001237
Iteration 127/1000 | Loss: 0.00001236
Iteration 128/1000 | Loss: 0.00001235
Iteration 129/1000 | Loss: 0.00001235
Iteration 130/1000 | Loss: 0.00001235
Iteration 131/1000 | Loss: 0.00001234
Iteration 132/1000 | Loss: 0.00001234
Iteration 133/1000 | Loss: 0.00001233
Iteration 134/1000 | Loss: 0.00001233
Iteration 135/1000 | Loss: 0.00001233
Iteration 136/1000 | Loss: 0.00001233
Iteration 137/1000 | Loss: 0.00001233
Iteration 138/1000 | Loss: 0.00001233
Iteration 139/1000 | Loss: 0.00001233
Iteration 140/1000 | Loss: 0.00001233
Iteration 141/1000 | Loss: 0.00001233
Iteration 142/1000 | Loss: 0.00001232
Iteration 143/1000 | Loss: 0.00001232
Iteration 144/1000 | Loss: 0.00001232
Iteration 145/1000 | Loss: 0.00001232
Iteration 146/1000 | Loss: 0.00001232
Iteration 147/1000 | Loss: 0.00001232
Iteration 148/1000 | Loss: 0.00001232
Iteration 149/1000 | Loss: 0.00001232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.2323492228460964e-05, 1.2323492228460964e-05, 1.2323492228460964e-05, 1.2323492228460964e-05, 1.2323492228460964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2323492228460964e-05

Optimization complete. Final v2v error: 2.91192364692688 mm

Highest mean error: 8.378937721252441 mm for frame 109

Lowest mean error: 2.592043399810791 mm for frame 20

Saving results

Total time: 105.85899043083191
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064840
Iteration 2/25 | Loss: 0.00272963
Iteration 3/25 | Loss: 0.00149435
Iteration 4/25 | Loss: 0.00132803
Iteration 5/25 | Loss: 0.00131843
Iteration 6/25 | Loss: 0.00126951
Iteration 7/25 | Loss: 0.00118972
Iteration 8/25 | Loss: 0.00115862
Iteration 9/25 | Loss: 0.00114226
Iteration 10/25 | Loss: 0.00113687
Iteration 11/25 | Loss: 0.00113078
Iteration 12/25 | Loss: 0.00112864
Iteration 13/25 | Loss: 0.00112306
Iteration 14/25 | Loss: 0.00112052
Iteration 15/25 | Loss: 0.00111568
Iteration 16/25 | Loss: 0.00111544
Iteration 17/25 | Loss: 0.00111934
Iteration 18/25 | Loss: 0.00112584
Iteration 19/25 | Loss: 0.00111877
Iteration 20/25 | Loss: 0.00111540
Iteration 21/25 | Loss: 0.00110702
Iteration 22/25 | Loss: 0.00110460
Iteration 23/25 | Loss: 0.00110430
Iteration 24/25 | Loss: 0.00110243
Iteration 25/25 | Loss: 0.00110067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36376524
Iteration 2/25 | Loss: 0.00085593
Iteration 3/25 | Loss: 0.00085593
Iteration 4/25 | Loss: 0.00085593
Iteration 5/25 | Loss: 0.00085593
Iteration 6/25 | Loss: 0.00085593
Iteration 7/25 | Loss: 0.00085593
Iteration 8/25 | Loss: 0.00085593
Iteration 9/25 | Loss: 0.00085593
Iteration 10/25 | Loss: 0.00085593
Iteration 11/25 | Loss: 0.00085593
Iteration 12/25 | Loss: 0.00085593
Iteration 13/25 | Loss: 0.00085593
Iteration 14/25 | Loss: 0.00085593
Iteration 15/25 | Loss: 0.00085593
Iteration 16/25 | Loss: 0.00085593
Iteration 17/25 | Loss: 0.00085593
Iteration 18/25 | Loss: 0.00085593
Iteration 19/25 | Loss: 0.00085593
Iteration 20/25 | Loss: 0.00085593
Iteration 21/25 | Loss: 0.00085593
Iteration 22/25 | Loss: 0.00085593
Iteration 23/25 | Loss: 0.00085593
Iteration 24/25 | Loss: 0.00085593
Iteration 25/25 | Loss: 0.00085593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085593
Iteration 2/1000 | Loss: 0.00025437
Iteration 3/1000 | Loss: 0.00015438
Iteration 4/1000 | Loss: 0.00004126
Iteration 5/1000 | Loss: 0.00004038
Iteration 6/1000 | Loss: 0.00004190
Iteration 7/1000 | Loss: 0.00003062
Iteration 8/1000 | Loss: 0.00014525
Iteration 9/1000 | Loss: 0.00035118
Iteration 10/1000 | Loss: 0.00022704
Iteration 11/1000 | Loss: 0.00043094
Iteration 12/1000 | Loss: 0.00018672
Iteration 13/1000 | Loss: 0.00004940
Iteration 14/1000 | Loss: 0.00005197
Iteration 15/1000 | Loss: 0.00004056
Iteration 16/1000 | Loss: 0.00003247
Iteration 17/1000 | Loss: 0.00003905
Iteration 18/1000 | Loss: 0.00013691
Iteration 19/1000 | Loss: 0.00003649
Iteration 20/1000 | Loss: 0.00004254
Iteration 21/1000 | Loss: 0.00002732
Iteration 22/1000 | Loss: 0.00024283
Iteration 23/1000 | Loss: 0.00025405
Iteration 24/1000 | Loss: 0.00002739
Iteration 25/1000 | Loss: 0.00023510
Iteration 26/1000 | Loss: 0.00004411
Iteration 27/1000 | Loss: 0.00004711
Iteration 28/1000 | Loss: 0.00003914
Iteration 29/1000 | Loss: 0.00003467
Iteration 30/1000 | Loss: 0.00003439
Iteration 31/1000 | Loss: 0.00002986
Iteration 32/1000 | Loss: 0.00003780
Iteration 33/1000 | Loss: 0.00003655
Iteration 34/1000 | Loss: 0.00003672
Iteration 35/1000 | Loss: 0.00003596
Iteration 36/1000 | Loss: 0.00003240
Iteration 37/1000 | Loss: 0.00003777
Iteration 38/1000 | Loss: 0.00003570
Iteration 39/1000 | Loss: 0.00003574
Iteration 40/1000 | Loss: 0.00003401
Iteration 41/1000 | Loss: 0.00003394
Iteration 42/1000 | Loss: 0.00003482
Iteration 43/1000 | Loss: 0.00002670
Iteration 44/1000 | Loss: 0.00003863
Iteration 45/1000 | Loss: 0.00003519
Iteration 46/1000 | Loss: 0.00003561
Iteration 47/1000 | Loss: 0.00003059
Iteration 48/1000 | Loss: 0.00002696
Iteration 49/1000 | Loss: 0.00003107
Iteration 50/1000 | Loss: 0.00003936
Iteration 51/1000 | Loss: 0.00004085
Iteration 52/1000 | Loss: 0.00003730
Iteration 53/1000 | Loss: 0.00003335
Iteration 54/1000 | Loss: 0.00003471
Iteration 55/1000 | Loss: 0.00003058
Iteration 56/1000 | Loss: 0.00003832
Iteration 57/1000 | Loss: 0.00003085
Iteration 58/1000 | Loss: 0.00003523
Iteration 59/1000 | Loss: 0.00002921
Iteration 60/1000 | Loss: 0.00004339
Iteration 61/1000 | Loss: 0.00002044
Iteration 62/1000 | Loss: 0.00001683
Iteration 63/1000 | Loss: 0.00001641
Iteration 64/1000 | Loss: 0.00001617
Iteration 65/1000 | Loss: 0.00001597
Iteration 66/1000 | Loss: 0.00001591
Iteration 67/1000 | Loss: 0.00001588
Iteration 68/1000 | Loss: 0.00001587
Iteration 69/1000 | Loss: 0.00001582
Iteration 70/1000 | Loss: 0.00001579
Iteration 71/1000 | Loss: 0.00001578
Iteration 72/1000 | Loss: 0.00001575
Iteration 73/1000 | Loss: 0.00001727
Iteration 74/1000 | Loss: 0.00001649
Iteration 75/1000 | Loss: 0.00001575
Iteration 76/1000 | Loss: 0.00001537
Iteration 77/1000 | Loss: 0.00001520
Iteration 78/1000 | Loss: 0.00001518
Iteration 79/1000 | Loss: 0.00001511
Iteration 80/1000 | Loss: 0.00001510
Iteration 81/1000 | Loss: 0.00001508
Iteration 82/1000 | Loss: 0.00001507
Iteration 83/1000 | Loss: 0.00001507
Iteration 84/1000 | Loss: 0.00001506
Iteration 85/1000 | Loss: 0.00001506
Iteration 86/1000 | Loss: 0.00001506
Iteration 87/1000 | Loss: 0.00001505
Iteration 88/1000 | Loss: 0.00001505
Iteration 89/1000 | Loss: 0.00001505
Iteration 90/1000 | Loss: 0.00001504
Iteration 91/1000 | Loss: 0.00001504
Iteration 92/1000 | Loss: 0.00001503
Iteration 93/1000 | Loss: 0.00001503
Iteration 94/1000 | Loss: 0.00001501
Iteration 95/1000 | Loss: 0.00001500
Iteration 96/1000 | Loss: 0.00001500
Iteration 97/1000 | Loss: 0.00001500
Iteration 98/1000 | Loss: 0.00001499
Iteration 99/1000 | Loss: 0.00001499
Iteration 100/1000 | Loss: 0.00001498
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001497
Iteration 103/1000 | Loss: 0.00001497
Iteration 104/1000 | Loss: 0.00001497
Iteration 105/1000 | Loss: 0.00001497
Iteration 106/1000 | Loss: 0.00001497
Iteration 107/1000 | Loss: 0.00001497
Iteration 108/1000 | Loss: 0.00001497
Iteration 109/1000 | Loss: 0.00001497
Iteration 110/1000 | Loss: 0.00001497
Iteration 111/1000 | Loss: 0.00001497
Iteration 112/1000 | Loss: 0.00001496
Iteration 113/1000 | Loss: 0.00001496
Iteration 114/1000 | Loss: 0.00001496
Iteration 115/1000 | Loss: 0.00001495
Iteration 116/1000 | Loss: 0.00001494
Iteration 117/1000 | Loss: 0.00001493
Iteration 118/1000 | Loss: 0.00001493
Iteration 119/1000 | Loss: 0.00001493
Iteration 120/1000 | Loss: 0.00001493
Iteration 121/1000 | Loss: 0.00001492
Iteration 122/1000 | Loss: 0.00001492
Iteration 123/1000 | Loss: 0.00001492
Iteration 124/1000 | Loss: 0.00001492
Iteration 125/1000 | Loss: 0.00001491
Iteration 126/1000 | Loss: 0.00001491
Iteration 127/1000 | Loss: 0.00001491
Iteration 128/1000 | Loss: 0.00001491
Iteration 129/1000 | Loss: 0.00001490
Iteration 130/1000 | Loss: 0.00001490
Iteration 131/1000 | Loss: 0.00001490
Iteration 132/1000 | Loss: 0.00001490
Iteration 133/1000 | Loss: 0.00001488
Iteration 134/1000 | Loss: 0.00001488
Iteration 135/1000 | Loss: 0.00001488
Iteration 136/1000 | Loss: 0.00001488
Iteration 137/1000 | Loss: 0.00001487
Iteration 138/1000 | Loss: 0.00001487
Iteration 139/1000 | Loss: 0.00001487
Iteration 140/1000 | Loss: 0.00001487
Iteration 141/1000 | Loss: 0.00001487
Iteration 142/1000 | Loss: 0.00001487
Iteration 143/1000 | Loss: 0.00001487
Iteration 144/1000 | Loss: 0.00001487
Iteration 145/1000 | Loss: 0.00001487
Iteration 146/1000 | Loss: 0.00001487
Iteration 147/1000 | Loss: 0.00001487
Iteration 148/1000 | Loss: 0.00001487
Iteration 149/1000 | Loss: 0.00001487
Iteration 150/1000 | Loss: 0.00001487
Iteration 151/1000 | Loss: 0.00001487
Iteration 152/1000 | Loss: 0.00001487
Iteration 153/1000 | Loss: 0.00001487
Iteration 154/1000 | Loss: 0.00001487
Iteration 155/1000 | Loss: 0.00001487
Iteration 156/1000 | Loss: 0.00001487
Iteration 157/1000 | Loss: 0.00001487
Iteration 158/1000 | Loss: 0.00001487
Iteration 159/1000 | Loss: 0.00001487
Iteration 160/1000 | Loss: 0.00001487
Iteration 161/1000 | Loss: 0.00001487
Iteration 162/1000 | Loss: 0.00001487
Iteration 163/1000 | Loss: 0.00001487
Iteration 164/1000 | Loss: 0.00001487
Iteration 165/1000 | Loss: 0.00001487
Iteration 166/1000 | Loss: 0.00001487
Iteration 167/1000 | Loss: 0.00001487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.4870566701574717e-05, 1.4870566701574717e-05, 1.4870566701574717e-05, 1.4870566701574717e-05, 1.4870566701574717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4870566701574717e-05

Optimization complete. Final v2v error: 3.181135654449463 mm

Highest mean error: 6.040988922119141 mm for frame 80

Lowest mean error: 2.9109699726104736 mm for frame 36

Saving results

Total time: 158.79358315467834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00606431
Iteration 2/25 | Loss: 0.00154943
Iteration 3/25 | Loss: 0.00138372
Iteration 4/25 | Loss: 0.00134736
Iteration 5/25 | Loss: 0.00134510
Iteration 6/25 | Loss: 0.00129321
Iteration 7/25 | Loss: 0.00126112
Iteration 8/25 | Loss: 0.00124010
Iteration 9/25 | Loss: 0.00123708
Iteration 10/25 | Loss: 0.00123589
Iteration 11/25 | Loss: 0.00123050
Iteration 12/25 | Loss: 0.00123077
Iteration 13/25 | Loss: 0.00122846
Iteration 14/25 | Loss: 0.00122596
Iteration 15/25 | Loss: 0.00122710
Iteration 16/25 | Loss: 0.00122594
Iteration 17/25 | Loss: 0.00122044
Iteration 18/25 | Loss: 0.00121928
Iteration 19/25 | Loss: 0.00121885
Iteration 20/25 | Loss: 0.00121916
Iteration 21/25 | Loss: 0.00121672
Iteration 22/25 | Loss: 0.00121590
Iteration 23/25 | Loss: 0.00121568
Iteration 24/25 | Loss: 0.00121567
Iteration 25/25 | Loss: 0.00121567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25023317
Iteration 2/25 | Loss: 0.00126607
Iteration 3/25 | Loss: 0.00126599
Iteration 4/25 | Loss: 0.00126599
Iteration 5/25 | Loss: 0.00126599
Iteration 6/25 | Loss: 0.00126599
Iteration 7/25 | Loss: 0.00126599
Iteration 8/25 | Loss: 0.00126599
Iteration 9/25 | Loss: 0.00126598
Iteration 10/25 | Loss: 0.00126598
Iteration 11/25 | Loss: 0.00126598
Iteration 12/25 | Loss: 0.00126598
Iteration 13/25 | Loss: 0.00126598
Iteration 14/25 | Loss: 0.00126598
Iteration 15/25 | Loss: 0.00126598
Iteration 16/25 | Loss: 0.00126598
Iteration 17/25 | Loss: 0.00126598
Iteration 18/25 | Loss: 0.00126598
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012659840285778046, 0.0012659840285778046, 0.0012659840285778046, 0.0012659840285778046, 0.0012659840285778046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012659840285778046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126598
Iteration 2/1000 | Loss: 0.00013440
Iteration 3/1000 | Loss: 0.00005635
Iteration 4/1000 | Loss: 0.00004255
Iteration 5/1000 | Loss: 0.00003751
Iteration 6/1000 | Loss: 0.00003522
Iteration 7/1000 | Loss: 0.00003302
Iteration 8/1000 | Loss: 0.00003149
Iteration 9/1000 | Loss: 0.00003048
Iteration 10/1000 | Loss: 0.00002967
Iteration 11/1000 | Loss: 0.00002907
Iteration 12/1000 | Loss: 0.00002864
Iteration 13/1000 | Loss: 0.00002839
Iteration 14/1000 | Loss: 0.00002810
Iteration 15/1000 | Loss: 0.00002788
Iteration 16/1000 | Loss: 0.00002766
Iteration 17/1000 | Loss: 0.00002753
Iteration 18/1000 | Loss: 0.00002747
Iteration 19/1000 | Loss: 0.00002744
Iteration 20/1000 | Loss: 0.00002738
Iteration 21/1000 | Loss: 0.00002734
Iteration 22/1000 | Loss: 0.00002730
Iteration 23/1000 | Loss: 0.00002726
Iteration 24/1000 | Loss: 0.00002721
Iteration 25/1000 | Loss: 0.00002717
Iteration 26/1000 | Loss: 0.00002717
Iteration 27/1000 | Loss: 0.00002716
Iteration 28/1000 | Loss: 0.00002715
Iteration 29/1000 | Loss: 0.00002715
Iteration 30/1000 | Loss: 0.00002715
Iteration 31/1000 | Loss: 0.00002715
Iteration 32/1000 | Loss: 0.00002714
Iteration 33/1000 | Loss: 0.00002714
Iteration 34/1000 | Loss: 0.00002714
Iteration 35/1000 | Loss: 0.00002713
Iteration 36/1000 | Loss: 0.00002712
Iteration 37/1000 | Loss: 0.00002712
Iteration 38/1000 | Loss: 0.00002711
Iteration 39/1000 | Loss: 0.00002711
Iteration 40/1000 | Loss: 0.00002711
Iteration 41/1000 | Loss: 0.00002710
Iteration 42/1000 | Loss: 0.00002710
Iteration 43/1000 | Loss: 0.00002709
Iteration 44/1000 | Loss: 0.00002708
Iteration 45/1000 | Loss: 0.00002708
Iteration 46/1000 | Loss: 0.00002706
Iteration 47/1000 | Loss: 0.00002706
Iteration 48/1000 | Loss: 0.00002706
Iteration 49/1000 | Loss: 0.00002705
Iteration 50/1000 | Loss: 0.00002705
Iteration 51/1000 | Loss: 0.00002704
Iteration 52/1000 | Loss: 0.00002704
Iteration 53/1000 | Loss: 0.00002704
Iteration 54/1000 | Loss: 0.00002704
Iteration 55/1000 | Loss: 0.00002703
Iteration 56/1000 | Loss: 0.00002703
Iteration 57/1000 | Loss: 0.00002702
Iteration 58/1000 | Loss: 0.00002702
Iteration 59/1000 | Loss: 0.00002702
Iteration 60/1000 | Loss: 0.00002701
Iteration 61/1000 | Loss: 0.00002700
Iteration 62/1000 | Loss: 0.00002699
Iteration 63/1000 | Loss: 0.00002699
Iteration 64/1000 | Loss: 0.00002699
Iteration 65/1000 | Loss: 0.00002696
Iteration 66/1000 | Loss: 0.00002694
Iteration 67/1000 | Loss: 0.00002694
Iteration 68/1000 | Loss: 0.00002693
Iteration 69/1000 | Loss: 0.00002693
Iteration 70/1000 | Loss: 0.00002692
Iteration 71/1000 | Loss: 0.00002690
Iteration 72/1000 | Loss: 0.00002690
Iteration 73/1000 | Loss: 0.00002690
Iteration 74/1000 | Loss: 0.00002690
Iteration 75/1000 | Loss: 0.00002689
Iteration 76/1000 | Loss: 0.00002689
Iteration 77/1000 | Loss: 0.00002689
Iteration 78/1000 | Loss: 0.00002689
Iteration 79/1000 | Loss: 0.00002688
Iteration 80/1000 | Loss: 0.00002688
Iteration 81/1000 | Loss: 0.00002688
Iteration 82/1000 | Loss: 0.00002687
Iteration 83/1000 | Loss: 0.00002687
Iteration 84/1000 | Loss: 0.00002687
Iteration 85/1000 | Loss: 0.00002686
Iteration 86/1000 | Loss: 0.00002686
Iteration 87/1000 | Loss: 0.00002686
Iteration 88/1000 | Loss: 0.00002686
Iteration 89/1000 | Loss: 0.00002685
Iteration 90/1000 | Loss: 0.00002685
Iteration 91/1000 | Loss: 0.00002685
Iteration 92/1000 | Loss: 0.00002685
Iteration 93/1000 | Loss: 0.00002684
Iteration 94/1000 | Loss: 0.00002684
Iteration 95/1000 | Loss: 0.00002684
Iteration 96/1000 | Loss: 0.00002684
Iteration 97/1000 | Loss: 0.00002684
Iteration 98/1000 | Loss: 0.00002683
Iteration 99/1000 | Loss: 0.00002683
Iteration 100/1000 | Loss: 0.00002683
Iteration 101/1000 | Loss: 0.00002683
Iteration 102/1000 | Loss: 0.00002683
Iteration 103/1000 | Loss: 0.00002683
Iteration 104/1000 | Loss: 0.00002683
Iteration 105/1000 | Loss: 0.00002682
Iteration 106/1000 | Loss: 0.00002682
Iteration 107/1000 | Loss: 0.00002682
Iteration 108/1000 | Loss: 0.00002682
Iteration 109/1000 | Loss: 0.00002682
Iteration 110/1000 | Loss: 0.00002681
Iteration 111/1000 | Loss: 0.00002681
Iteration 112/1000 | Loss: 0.00002681
Iteration 113/1000 | Loss: 0.00002681
Iteration 114/1000 | Loss: 0.00002680
Iteration 115/1000 | Loss: 0.00002680
Iteration 116/1000 | Loss: 0.00002679
Iteration 117/1000 | Loss: 0.00002679
Iteration 118/1000 | Loss: 0.00002679
Iteration 119/1000 | Loss: 0.00002678
Iteration 120/1000 | Loss: 0.00002678
Iteration 121/1000 | Loss: 0.00002677
Iteration 122/1000 | Loss: 0.00002677
Iteration 123/1000 | Loss: 0.00002677
Iteration 124/1000 | Loss: 0.00002677
Iteration 125/1000 | Loss: 0.00002677
Iteration 126/1000 | Loss: 0.00002677
Iteration 127/1000 | Loss: 0.00002676
Iteration 128/1000 | Loss: 0.00002676
Iteration 129/1000 | Loss: 0.00002676
Iteration 130/1000 | Loss: 0.00002675
Iteration 131/1000 | Loss: 0.00002675
Iteration 132/1000 | Loss: 0.00002675
Iteration 133/1000 | Loss: 0.00002675
Iteration 134/1000 | Loss: 0.00002674
Iteration 135/1000 | Loss: 0.00002674
Iteration 136/1000 | Loss: 0.00002674
Iteration 137/1000 | Loss: 0.00002673
Iteration 138/1000 | Loss: 0.00002673
Iteration 139/1000 | Loss: 0.00002673
Iteration 140/1000 | Loss: 0.00002672
Iteration 141/1000 | Loss: 0.00002672
Iteration 142/1000 | Loss: 0.00002672
Iteration 143/1000 | Loss: 0.00002671
Iteration 144/1000 | Loss: 0.00002671
Iteration 145/1000 | Loss: 0.00002671
Iteration 146/1000 | Loss: 0.00002671
Iteration 147/1000 | Loss: 0.00002670
Iteration 148/1000 | Loss: 0.00002670
Iteration 149/1000 | Loss: 0.00002670
Iteration 150/1000 | Loss: 0.00002670
Iteration 151/1000 | Loss: 0.00002670
Iteration 152/1000 | Loss: 0.00002670
Iteration 153/1000 | Loss: 0.00002670
Iteration 154/1000 | Loss: 0.00002670
Iteration 155/1000 | Loss: 0.00002669
Iteration 156/1000 | Loss: 0.00002669
Iteration 157/1000 | Loss: 0.00002669
Iteration 158/1000 | Loss: 0.00002669
Iteration 159/1000 | Loss: 0.00002669
Iteration 160/1000 | Loss: 0.00002669
Iteration 161/1000 | Loss: 0.00002669
Iteration 162/1000 | Loss: 0.00002669
Iteration 163/1000 | Loss: 0.00002669
Iteration 164/1000 | Loss: 0.00002669
Iteration 165/1000 | Loss: 0.00002669
Iteration 166/1000 | Loss: 0.00002669
Iteration 167/1000 | Loss: 0.00002669
Iteration 168/1000 | Loss: 0.00002669
Iteration 169/1000 | Loss: 0.00002669
Iteration 170/1000 | Loss: 0.00002668
Iteration 171/1000 | Loss: 0.00002668
Iteration 172/1000 | Loss: 0.00002668
Iteration 173/1000 | Loss: 0.00002668
Iteration 174/1000 | Loss: 0.00002668
Iteration 175/1000 | Loss: 0.00002668
Iteration 176/1000 | Loss: 0.00002668
Iteration 177/1000 | Loss: 0.00002668
Iteration 178/1000 | Loss: 0.00002668
Iteration 179/1000 | Loss: 0.00002668
Iteration 180/1000 | Loss: 0.00002668
Iteration 181/1000 | Loss: 0.00002668
Iteration 182/1000 | Loss: 0.00002668
Iteration 183/1000 | Loss: 0.00002668
Iteration 184/1000 | Loss: 0.00002668
Iteration 185/1000 | Loss: 0.00002668
Iteration 186/1000 | Loss: 0.00002668
Iteration 187/1000 | Loss: 0.00002668
Iteration 188/1000 | Loss: 0.00002667
Iteration 189/1000 | Loss: 0.00002667
Iteration 190/1000 | Loss: 0.00002667
Iteration 191/1000 | Loss: 0.00002667
Iteration 192/1000 | Loss: 0.00002667
Iteration 193/1000 | Loss: 0.00002667
Iteration 194/1000 | Loss: 0.00002667
Iteration 195/1000 | Loss: 0.00002667
Iteration 196/1000 | Loss: 0.00002667
Iteration 197/1000 | Loss: 0.00002667
Iteration 198/1000 | Loss: 0.00002666
Iteration 199/1000 | Loss: 0.00002666
Iteration 200/1000 | Loss: 0.00002666
Iteration 201/1000 | Loss: 0.00002666
Iteration 202/1000 | Loss: 0.00002666
Iteration 203/1000 | Loss: 0.00002666
Iteration 204/1000 | Loss: 0.00002666
Iteration 205/1000 | Loss: 0.00002666
Iteration 206/1000 | Loss: 0.00002666
Iteration 207/1000 | Loss: 0.00002666
Iteration 208/1000 | Loss: 0.00002666
Iteration 209/1000 | Loss: 0.00002666
Iteration 210/1000 | Loss: 0.00002666
Iteration 211/1000 | Loss: 0.00002666
Iteration 212/1000 | Loss: 0.00002666
Iteration 213/1000 | Loss: 0.00002665
Iteration 214/1000 | Loss: 0.00002665
Iteration 215/1000 | Loss: 0.00002665
Iteration 216/1000 | Loss: 0.00002665
Iteration 217/1000 | Loss: 0.00002665
Iteration 218/1000 | Loss: 0.00002665
Iteration 219/1000 | Loss: 0.00002665
Iteration 220/1000 | Loss: 0.00002665
Iteration 221/1000 | Loss: 0.00002665
Iteration 222/1000 | Loss: 0.00002665
Iteration 223/1000 | Loss: 0.00002665
Iteration 224/1000 | Loss: 0.00002665
Iteration 225/1000 | Loss: 0.00002665
Iteration 226/1000 | Loss: 0.00002665
Iteration 227/1000 | Loss: 0.00002665
Iteration 228/1000 | Loss: 0.00002665
Iteration 229/1000 | Loss: 0.00002665
Iteration 230/1000 | Loss: 0.00002665
Iteration 231/1000 | Loss: 0.00002665
Iteration 232/1000 | Loss: 0.00002665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [2.6649784558685496e-05, 2.6649784558685496e-05, 2.6649784558685496e-05, 2.6649784558685496e-05, 2.6649784558685496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6649784558685496e-05

Optimization complete. Final v2v error: 3.7747602462768555 mm

Highest mean error: 11.122956275939941 mm for frame 137

Lowest mean error: 2.9378669261932373 mm for frame 0

Saving results

Total time: 86.78892397880554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794587
Iteration 2/25 | Loss: 0.00164661
Iteration 3/25 | Loss: 0.00133066
Iteration 4/25 | Loss: 0.00128389
Iteration 5/25 | Loss: 0.00126231
Iteration 6/25 | Loss: 0.00125320
Iteration 7/25 | Loss: 0.00127396
Iteration 8/25 | Loss: 0.00125548
Iteration 9/25 | Loss: 0.00124218
Iteration 10/25 | Loss: 0.00123695
Iteration 11/25 | Loss: 0.00123120
Iteration 12/25 | Loss: 0.00123052
Iteration 13/25 | Loss: 0.00122560
Iteration 14/25 | Loss: 0.00122498
Iteration 15/25 | Loss: 0.00122480
Iteration 16/25 | Loss: 0.00122522
Iteration 17/25 | Loss: 0.00122389
Iteration 18/25 | Loss: 0.00122342
Iteration 19/25 | Loss: 0.00122329
Iteration 20/25 | Loss: 0.00122328
Iteration 21/25 | Loss: 0.00122328
Iteration 22/25 | Loss: 0.00122325
Iteration 23/25 | Loss: 0.00122325
Iteration 24/25 | Loss: 0.00122325
Iteration 25/25 | Loss: 0.00122325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.46107531
Iteration 2/25 | Loss: 0.00073812
Iteration 3/25 | Loss: 0.00073809
Iteration 4/25 | Loss: 0.00073809
Iteration 5/25 | Loss: 0.00073809
Iteration 6/25 | Loss: 0.00073809
Iteration 7/25 | Loss: 0.00073809
Iteration 8/25 | Loss: 0.00073809
Iteration 9/25 | Loss: 0.00073809
Iteration 10/25 | Loss: 0.00073809
Iteration 11/25 | Loss: 0.00073809
Iteration 12/25 | Loss: 0.00073809
Iteration 13/25 | Loss: 0.00073809
Iteration 14/25 | Loss: 0.00073809
Iteration 15/25 | Loss: 0.00073809
Iteration 16/25 | Loss: 0.00073809
Iteration 17/25 | Loss: 0.00073809
Iteration 18/25 | Loss: 0.00073808
Iteration 19/25 | Loss: 0.00073808
Iteration 20/25 | Loss: 0.00073808
Iteration 21/25 | Loss: 0.00073808
Iteration 22/25 | Loss: 0.00073808
Iteration 23/25 | Loss: 0.00073808
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007380849565379322, 0.0007380849565379322, 0.0007380849565379322, 0.0007380849565379322, 0.0007380849565379322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007380849565379322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073808
Iteration 2/1000 | Loss: 0.00004062
Iteration 3/1000 | Loss: 0.00002893
Iteration 4/1000 | Loss: 0.00002679
Iteration 5/1000 | Loss: 0.00002573
Iteration 6/1000 | Loss: 0.00002473
Iteration 7/1000 | Loss: 0.00002423
Iteration 8/1000 | Loss: 0.00002372
Iteration 9/1000 | Loss: 0.00002332
Iteration 10/1000 | Loss: 0.00002305
Iteration 11/1000 | Loss: 0.00002279
Iteration 12/1000 | Loss: 0.00002262
Iteration 13/1000 | Loss: 0.00002259
Iteration 14/1000 | Loss: 0.00002252
Iteration 15/1000 | Loss: 0.00002230
Iteration 16/1000 | Loss: 0.00002224
Iteration 17/1000 | Loss: 0.00002220
Iteration 18/1000 | Loss: 0.00002220
Iteration 19/1000 | Loss: 0.00002219
Iteration 20/1000 | Loss: 0.00002218
Iteration 21/1000 | Loss: 0.00002215
Iteration 22/1000 | Loss: 0.00002215
Iteration 23/1000 | Loss: 0.00002215
Iteration 24/1000 | Loss: 0.00002215
Iteration 25/1000 | Loss: 0.00002215
Iteration 26/1000 | Loss: 0.00002215
Iteration 27/1000 | Loss: 0.00002215
Iteration 28/1000 | Loss: 0.00002211
Iteration 29/1000 | Loss: 0.00002211
Iteration 30/1000 | Loss: 0.00002211
Iteration 31/1000 | Loss: 0.00002211
Iteration 32/1000 | Loss: 0.00002210
Iteration 33/1000 | Loss: 0.00002210
Iteration 34/1000 | Loss: 0.00002210
Iteration 35/1000 | Loss: 0.00002210
Iteration 36/1000 | Loss: 0.00002210
Iteration 37/1000 | Loss: 0.00002210
Iteration 38/1000 | Loss: 0.00002210
Iteration 39/1000 | Loss: 0.00002210
Iteration 40/1000 | Loss: 0.00002210
Iteration 41/1000 | Loss: 0.00002210
Iteration 42/1000 | Loss: 0.00002209
Iteration 43/1000 | Loss: 0.00002209
Iteration 44/1000 | Loss: 0.00002209
Iteration 45/1000 | Loss: 0.00002209
Iteration 46/1000 | Loss: 0.00002209
Iteration 47/1000 | Loss: 0.00002207
Iteration 48/1000 | Loss: 0.00002207
Iteration 49/1000 | Loss: 0.00002207
Iteration 50/1000 | Loss: 0.00002206
Iteration 51/1000 | Loss: 0.00002206
Iteration 52/1000 | Loss: 0.00002206
Iteration 53/1000 | Loss: 0.00002206
Iteration 54/1000 | Loss: 0.00002206
Iteration 55/1000 | Loss: 0.00002206
Iteration 56/1000 | Loss: 0.00002206
Iteration 57/1000 | Loss: 0.00002205
Iteration 58/1000 | Loss: 0.00002205
Iteration 59/1000 | Loss: 0.00002204
Iteration 60/1000 | Loss: 0.00002204
Iteration 61/1000 | Loss: 0.00002204
Iteration 62/1000 | Loss: 0.00002204
Iteration 63/1000 | Loss: 0.00002204
Iteration 64/1000 | Loss: 0.00002203
Iteration 65/1000 | Loss: 0.00002203
Iteration 66/1000 | Loss: 0.00002202
Iteration 67/1000 | Loss: 0.00002201
Iteration 68/1000 | Loss: 0.00002201
Iteration 69/1000 | Loss: 0.00002201
Iteration 70/1000 | Loss: 0.00002201
Iteration 71/1000 | Loss: 0.00002200
Iteration 72/1000 | Loss: 0.00002200
Iteration 73/1000 | Loss: 0.00002200
Iteration 74/1000 | Loss: 0.00002200
Iteration 75/1000 | Loss: 0.00002200
Iteration 76/1000 | Loss: 0.00002200
Iteration 77/1000 | Loss: 0.00002200
Iteration 78/1000 | Loss: 0.00002200
Iteration 79/1000 | Loss: 0.00002200
Iteration 80/1000 | Loss: 0.00002200
Iteration 81/1000 | Loss: 0.00002200
Iteration 82/1000 | Loss: 0.00002200
Iteration 83/1000 | Loss: 0.00002200
Iteration 84/1000 | Loss: 0.00002200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.199610389652662e-05, 2.199610389652662e-05, 2.199610389652662e-05, 2.199610389652662e-05, 2.199610389652662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.199610389652662e-05

Optimization complete. Final v2v error: 3.6972525119781494 mm

Highest mean error: 11.785687446594238 mm for frame 103

Lowest mean error: 3.008089303970337 mm for frame 0

Saving results

Total time: 69.19611954689026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772185
Iteration 2/25 | Loss: 0.00192584
Iteration 3/25 | Loss: 0.00132102
Iteration 4/25 | Loss: 0.00128131
Iteration 5/25 | Loss: 0.00128551
Iteration 6/25 | Loss: 0.00128952
Iteration 7/25 | Loss: 0.00121816
Iteration 8/25 | Loss: 0.00120067
Iteration 9/25 | Loss: 0.00119227
Iteration 10/25 | Loss: 0.00119038
Iteration 11/25 | Loss: 0.00119003
Iteration 12/25 | Loss: 0.00118994
Iteration 13/25 | Loss: 0.00118991
Iteration 14/25 | Loss: 0.00118991
Iteration 15/25 | Loss: 0.00118991
Iteration 16/25 | Loss: 0.00118990
Iteration 17/25 | Loss: 0.00118990
Iteration 18/25 | Loss: 0.00118990
Iteration 19/25 | Loss: 0.00118990
Iteration 20/25 | Loss: 0.00118990
Iteration 21/25 | Loss: 0.00118990
Iteration 22/25 | Loss: 0.00118990
Iteration 23/25 | Loss: 0.00118990
Iteration 24/25 | Loss: 0.00118990
Iteration 25/25 | Loss: 0.00118990

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.53103399
Iteration 2/25 | Loss: 0.00059043
Iteration 3/25 | Loss: 0.00059038
Iteration 4/25 | Loss: 0.00059037
Iteration 5/25 | Loss: 0.00059037
Iteration 6/25 | Loss: 0.00059037
Iteration 7/25 | Loss: 0.00059037
Iteration 8/25 | Loss: 0.00059037
Iteration 9/25 | Loss: 0.00059037
Iteration 10/25 | Loss: 0.00059037
Iteration 11/25 | Loss: 0.00059037
Iteration 12/25 | Loss: 0.00059037
Iteration 13/25 | Loss: 0.00059037
Iteration 14/25 | Loss: 0.00059037
Iteration 15/25 | Loss: 0.00059037
Iteration 16/25 | Loss: 0.00059037
Iteration 17/25 | Loss: 0.00059037
Iteration 18/25 | Loss: 0.00059037
Iteration 19/25 | Loss: 0.00059037
Iteration 20/25 | Loss: 0.00059037
Iteration 21/25 | Loss: 0.00059037
Iteration 22/25 | Loss: 0.00059037
Iteration 23/25 | Loss: 0.00059037
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005903723649680614, 0.0005903723649680614, 0.0005903723649680614, 0.0005903723649680614, 0.0005903723649680614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005903723649680614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059037
Iteration 2/1000 | Loss: 0.00002766
Iteration 3/1000 | Loss: 0.00002104
Iteration 4/1000 | Loss: 0.00001974
Iteration 5/1000 | Loss: 0.00001901
Iteration 6/1000 | Loss: 0.00001879
Iteration 7/1000 | Loss: 0.00001835
Iteration 8/1000 | Loss: 0.00001796
Iteration 9/1000 | Loss: 0.00001773
Iteration 10/1000 | Loss: 0.00001752
Iteration 11/1000 | Loss: 0.00001747
Iteration 12/1000 | Loss: 0.00017017
Iteration 13/1000 | Loss: 0.00002138
Iteration 14/1000 | Loss: 0.00001988
Iteration 15/1000 | Loss: 0.00001922
Iteration 16/1000 | Loss: 0.00001886
Iteration 17/1000 | Loss: 0.00001857
Iteration 18/1000 | Loss: 0.00001837
Iteration 19/1000 | Loss: 0.00001834
Iteration 20/1000 | Loss: 0.00001822
Iteration 21/1000 | Loss: 0.00001817
Iteration 22/1000 | Loss: 0.00001817
Iteration 23/1000 | Loss: 0.00001816
Iteration 24/1000 | Loss: 0.00001813
Iteration 25/1000 | Loss: 0.00001811
Iteration 26/1000 | Loss: 0.00001770
Iteration 27/1000 | Loss: 0.00001726
Iteration 28/1000 | Loss: 0.00001704
Iteration 29/1000 | Loss: 0.00001702
Iteration 30/1000 | Loss: 0.00001701
Iteration 31/1000 | Loss: 0.00001694
Iteration 32/1000 | Loss: 0.00003404
Iteration 33/1000 | Loss: 0.00002370
Iteration 34/1000 | Loss: 0.00003490
Iteration 35/1000 | Loss: 0.00002628
Iteration 36/1000 | Loss: 0.00003452
Iteration 37/1000 | Loss: 0.00020614
Iteration 38/1000 | Loss: 0.00004998
Iteration 39/1000 | Loss: 0.00006514
Iteration 40/1000 | Loss: 0.00003569
Iteration 41/1000 | Loss: 0.00005479
Iteration 42/1000 | Loss: 0.00006783
Iteration 43/1000 | Loss: 0.00002544
Iteration 44/1000 | Loss: 0.00009359
Iteration 45/1000 | Loss: 0.00031811
Iteration 46/1000 | Loss: 0.00008597
Iteration 47/1000 | Loss: 0.00009580
Iteration 48/1000 | Loss: 0.00006737
Iteration 49/1000 | Loss: 0.00001783
Iteration 50/1000 | Loss: 0.00001708
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001691
Iteration 53/1000 | Loss: 0.00001690
Iteration 54/1000 | Loss: 0.00001690
Iteration 55/1000 | Loss: 0.00001690
Iteration 56/1000 | Loss: 0.00001689
Iteration 57/1000 | Loss: 0.00001689
Iteration 58/1000 | Loss: 0.00001688
Iteration 59/1000 | Loss: 0.00001688
Iteration 60/1000 | Loss: 0.00001686
Iteration 61/1000 | Loss: 0.00001685
Iteration 62/1000 | Loss: 0.00001685
Iteration 63/1000 | Loss: 0.00001685
Iteration 64/1000 | Loss: 0.00001685
Iteration 65/1000 | Loss: 0.00001685
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001685
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001684
Iteration 73/1000 | Loss: 0.00001684
Iteration 74/1000 | Loss: 0.00001684
Iteration 75/1000 | Loss: 0.00001684
Iteration 76/1000 | Loss: 0.00001684
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001684
Iteration 79/1000 | Loss: 0.00001684
Iteration 80/1000 | Loss: 0.00001684
Iteration 81/1000 | Loss: 0.00001684
Iteration 82/1000 | Loss: 0.00001684
Iteration 83/1000 | Loss: 0.00001684
Iteration 84/1000 | Loss: 0.00001684
Iteration 85/1000 | Loss: 0.00001684
Iteration 86/1000 | Loss: 0.00001684
Iteration 87/1000 | Loss: 0.00001684
Iteration 88/1000 | Loss: 0.00001684
Iteration 89/1000 | Loss: 0.00001684
Iteration 90/1000 | Loss: 0.00001684
Iteration 91/1000 | Loss: 0.00001684
Iteration 92/1000 | Loss: 0.00001684
Iteration 93/1000 | Loss: 0.00001684
Iteration 94/1000 | Loss: 0.00001684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.684385097178165e-05, 1.684385097178165e-05, 1.684385097178165e-05, 1.684385097178165e-05, 1.684385097178165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.684385097178165e-05

Optimization complete. Final v2v error: 3.3917770385742188 mm

Highest mean error: 6.435089588165283 mm for frame 155

Lowest mean error: 3.103919267654419 mm for frame 134

Saving results

Total time: 91.98518705368042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00517195
Iteration 2/25 | Loss: 0.00128528
Iteration 3/25 | Loss: 0.00115918
Iteration 4/25 | Loss: 0.00114903
Iteration 5/25 | Loss: 0.00114693
Iteration 6/25 | Loss: 0.00114693
Iteration 7/25 | Loss: 0.00114693
Iteration 8/25 | Loss: 0.00114693
Iteration 9/25 | Loss: 0.00114693
Iteration 10/25 | Loss: 0.00114693
Iteration 11/25 | Loss: 0.00114693
Iteration 12/25 | Loss: 0.00114693
Iteration 13/25 | Loss: 0.00114693
Iteration 14/25 | Loss: 0.00114693
Iteration 15/25 | Loss: 0.00114693
Iteration 16/25 | Loss: 0.00114693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011469317832961679, 0.0011469317832961679, 0.0011469317832961679, 0.0011469317832961679, 0.0011469317832961679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011469317832961679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81998211
Iteration 2/25 | Loss: 0.00056156
Iteration 3/25 | Loss: 0.00056156
Iteration 4/25 | Loss: 0.00056156
Iteration 5/25 | Loss: 0.00056156
Iteration 6/25 | Loss: 0.00056156
Iteration 7/25 | Loss: 0.00056156
Iteration 8/25 | Loss: 0.00056156
Iteration 9/25 | Loss: 0.00056156
Iteration 10/25 | Loss: 0.00056156
Iteration 11/25 | Loss: 0.00056156
Iteration 12/25 | Loss: 0.00056156
Iteration 13/25 | Loss: 0.00056156
Iteration 14/25 | Loss: 0.00056156
Iteration 15/25 | Loss: 0.00056156
Iteration 16/25 | Loss: 0.00056156
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005615578265860677, 0.0005615578265860677, 0.0005615578265860677, 0.0005615578265860677, 0.0005615578265860677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005615578265860677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056156
Iteration 2/1000 | Loss: 0.00002216
Iteration 3/1000 | Loss: 0.00001600
Iteration 4/1000 | Loss: 0.00001470
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001394
Iteration 7/1000 | Loss: 0.00001361
Iteration 8/1000 | Loss: 0.00001328
Iteration 9/1000 | Loss: 0.00001313
Iteration 10/1000 | Loss: 0.00001306
Iteration 11/1000 | Loss: 0.00001289
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00001278
Iteration 14/1000 | Loss: 0.00001278
Iteration 15/1000 | Loss: 0.00001278
Iteration 16/1000 | Loss: 0.00001278
Iteration 17/1000 | Loss: 0.00001278
Iteration 18/1000 | Loss: 0.00001275
Iteration 19/1000 | Loss: 0.00001274
Iteration 20/1000 | Loss: 0.00001273
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001260
Iteration 23/1000 | Loss: 0.00001260
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001256
Iteration 27/1000 | Loss: 0.00001256
Iteration 28/1000 | Loss: 0.00001256
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001255
Iteration 32/1000 | Loss: 0.00001255
Iteration 33/1000 | Loss: 0.00001255
Iteration 34/1000 | Loss: 0.00001255
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001255
Iteration 37/1000 | Loss: 0.00001254
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001252
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001252
Iteration 44/1000 | Loss: 0.00001251
Iteration 45/1000 | Loss: 0.00001250
Iteration 46/1000 | Loss: 0.00001246
Iteration 47/1000 | Loss: 0.00001246
Iteration 48/1000 | Loss: 0.00001246
Iteration 49/1000 | Loss: 0.00001246
Iteration 50/1000 | Loss: 0.00001246
Iteration 51/1000 | Loss: 0.00001246
Iteration 52/1000 | Loss: 0.00001245
Iteration 53/1000 | Loss: 0.00001245
Iteration 54/1000 | Loss: 0.00001245
Iteration 55/1000 | Loss: 0.00001244
Iteration 56/1000 | Loss: 0.00001244
Iteration 57/1000 | Loss: 0.00001244
Iteration 58/1000 | Loss: 0.00001244
Iteration 59/1000 | Loss: 0.00001243
Iteration 60/1000 | Loss: 0.00001243
Iteration 61/1000 | Loss: 0.00001243
Iteration 62/1000 | Loss: 0.00001242
Iteration 63/1000 | Loss: 0.00001242
Iteration 64/1000 | Loss: 0.00001242
Iteration 65/1000 | Loss: 0.00001242
Iteration 66/1000 | Loss: 0.00001242
Iteration 67/1000 | Loss: 0.00001242
Iteration 68/1000 | Loss: 0.00001242
Iteration 69/1000 | Loss: 0.00001242
Iteration 70/1000 | Loss: 0.00001242
Iteration 71/1000 | Loss: 0.00001242
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001241
Iteration 74/1000 | Loss: 0.00001241
Iteration 75/1000 | Loss: 0.00001240
Iteration 76/1000 | Loss: 0.00001240
Iteration 77/1000 | Loss: 0.00001240
Iteration 78/1000 | Loss: 0.00001240
Iteration 79/1000 | Loss: 0.00001239
Iteration 80/1000 | Loss: 0.00001239
Iteration 81/1000 | Loss: 0.00001238
Iteration 82/1000 | Loss: 0.00001238
Iteration 83/1000 | Loss: 0.00001238
Iteration 84/1000 | Loss: 0.00001237
Iteration 85/1000 | Loss: 0.00001237
Iteration 86/1000 | Loss: 0.00001237
Iteration 87/1000 | Loss: 0.00001237
Iteration 88/1000 | Loss: 0.00001237
Iteration 89/1000 | Loss: 0.00001237
Iteration 90/1000 | Loss: 0.00001237
Iteration 91/1000 | Loss: 0.00001237
Iteration 92/1000 | Loss: 0.00001237
Iteration 93/1000 | Loss: 0.00001236
Iteration 94/1000 | Loss: 0.00001236
Iteration 95/1000 | Loss: 0.00001236
Iteration 96/1000 | Loss: 0.00001236
Iteration 97/1000 | Loss: 0.00001236
Iteration 98/1000 | Loss: 0.00001235
Iteration 99/1000 | Loss: 0.00001235
Iteration 100/1000 | Loss: 0.00001235
Iteration 101/1000 | Loss: 0.00001235
Iteration 102/1000 | Loss: 0.00001235
Iteration 103/1000 | Loss: 0.00001235
Iteration 104/1000 | Loss: 0.00001235
Iteration 105/1000 | Loss: 0.00001235
Iteration 106/1000 | Loss: 0.00001235
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001234
Iteration 110/1000 | Loss: 0.00001234
Iteration 111/1000 | Loss: 0.00001234
Iteration 112/1000 | Loss: 0.00001234
Iteration 113/1000 | Loss: 0.00001234
Iteration 114/1000 | Loss: 0.00001234
Iteration 115/1000 | Loss: 0.00001233
Iteration 116/1000 | Loss: 0.00001233
Iteration 117/1000 | Loss: 0.00001233
Iteration 118/1000 | Loss: 0.00001233
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001231
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001231
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001231
Iteration 128/1000 | Loss: 0.00001231
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001230
Iteration 132/1000 | Loss: 0.00001230
Iteration 133/1000 | Loss: 0.00001230
Iteration 134/1000 | Loss: 0.00001230
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001229
Iteration 137/1000 | Loss: 0.00001229
Iteration 138/1000 | Loss: 0.00001229
Iteration 139/1000 | Loss: 0.00001229
Iteration 140/1000 | Loss: 0.00001228
Iteration 141/1000 | Loss: 0.00001228
Iteration 142/1000 | Loss: 0.00001228
Iteration 143/1000 | Loss: 0.00001228
Iteration 144/1000 | Loss: 0.00001228
Iteration 145/1000 | Loss: 0.00001227
Iteration 146/1000 | Loss: 0.00001227
Iteration 147/1000 | Loss: 0.00001227
Iteration 148/1000 | Loss: 0.00001227
Iteration 149/1000 | Loss: 0.00001227
Iteration 150/1000 | Loss: 0.00001227
Iteration 151/1000 | Loss: 0.00001227
Iteration 152/1000 | Loss: 0.00001226
Iteration 153/1000 | Loss: 0.00001226
Iteration 154/1000 | Loss: 0.00001226
Iteration 155/1000 | Loss: 0.00001226
Iteration 156/1000 | Loss: 0.00001226
Iteration 157/1000 | Loss: 0.00001226
Iteration 158/1000 | Loss: 0.00001226
Iteration 159/1000 | Loss: 0.00001226
Iteration 160/1000 | Loss: 0.00001226
Iteration 161/1000 | Loss: 0.00001226
Iteration 162/1000 | Loss: 0.00001226
Iteration 163/1000 | Loss: 0.00001226
Iteration 164/1000 | Loss: 0.00001226
Iteration 165/1000 | Loss: 0.00001226
Iteration 166/1000 | Loss: 0.00001226
Iteration 167/1000 | Loss: 0.00001225
Iteration 168/1000 | Loss: 0.00001225
Iteration 169/1000 | Loss: 0.00001225
Iteration 170/1000 | Loss: 0.00001225
Iteration 171/1000 | Loss: 0.00001225
Iteration 172/1000 | Loss: 0.00001225
Iteration 173/1000 | Loss: 0.00001225
Iteration 174/1000 | Loss: 0.00001225
Iteration 175/1000 | Loss: 0.00001225
Iteration 176/1000 | Loss: 0.00001225
Iteration 177/1000 | Loss: 0.00001225
Iteration 178/1000 | Loss: 0.00001225
Iteration 179/1000 | Loss: 0.00001225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.225195410370361e-05, 1.225195410370361e-05, 1.225195410370361e-05, 1.225195410370361e-05, 1.225195410370361e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.225195410370361e-05

Optimization complete. Final v2v error: 2.959411859512329 mm

Highest mean error: 3.0887627601623535 mm for frame 6

Lowest mean error: 2.838818311691284 mm for frame 168

Saving results

Total time: 36.322139501571655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841859
Iteration 2/25 | Loss: 0.00118702
Iteration 3/25 | Loss: 0.00108488
Iteration 4/25 | Loss: 0.00107306
Iteration 5/25 | Loss: 0.00107007
Iteration 6/25 | Loss: 0.00106999
Iteration 7/25 | Loss: 0.00106999
Iteration 8/25 | Loss: 0.00106999
Iteration 9/25 | Loss: 0.00106999
Iteration 10/25 | Loss: 0.00106999
Iteration 11/25 | Loss: 0.00106999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001069985912181437, 0.001069985912181437, 0.001069985912181437, 0.001069985912181437, 0.001069985912181437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001069985912181437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.79269218
Iteration 2/25 | Loss: 0.00081529
Iteration 3/25 | Loss: 0.00081529
Iteration 4/25 | Loss: 0.00081529
Iteration 5/25 | Loss: 0.00081529
Iteration 6/25 | Loss: 0.00081529
Iteration 7/25 | Loss: 0.00081529
Iteration 8/25 | Loss: 0.00081529
Iteration 9/25 | Loss: 0.00081529
Iteration 10/25 | Loss: 0.00081529
Iteration 11/25 | Loss: 0.00081529
Iteration 12/25 | Loss: 0.00081529
Iteration 13/25 | Loss: 0.00081529
Iteration 14/25 | Loss: 0.00081529
Iteration 15/25 | Loss: 0.00081529
Iteration 16/25 | Loss: 0.00081529
Iteration 17/25 | Loss: 0.00081529
Iteration 18/25 | Loss: 0.00081529
Iteration 19/25 | Loss: 0.00081529
Iteration 20/25 | Loss: 0.00081529
Iteration 21/25 | Loss: 0.00081529
Iteration 22/25 | Loss: 0.00081529
Iteration 23/25 | Loss: 0.00081529
Iteration 24/25 | Loss: 0.00081529
Iteration 25/25 | Loss: 0.00081529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081529
Iteration 2/1000 | Loss: 0.00001548
Iteration 3/1000 | Loss: 0.00001162
Iteration 4/1000 | Loss: 0.00001069
Iteration 5/1000 | Loss: 0.00000999
Iteration 6/1000 | Loss: 0.00000959
Iteration 7/1000 | Loss: 0.00000939
Iteration 8/1000 | Loss: 0.00000927
Iteration 9/1000 | Loss: 0.00000904
Iteration 10/1000 | Loss: 0.00000897
Iteration 11/1000 | Loss: 0.00000890
Iteration 12/1000 | Loss: 0.00000890
Iteration 13/1000 | Loss: 0.00000886
Iteration 14/1000 | Loss: 0.00000886
Iteration 15/1000 | Loss: 0.00000884
Iteration 16/1000 | Loss: 0.00000883
Iteration 17/1000 | Loss: 0.00000883
Iteration 18/1000 | Loss: 0.00000883
Iteration 19/1000 | Loss: 0.00000882
Iteration 20/1000 | Loss: 0.00000882
Iteration 21/1000 | Loss: 0.00000880
Iteration 22/1000 | Loss: 0.00000877
Iteration 23/1000 | Loss: 0.00000877
Iteration 24/1000 | Loss: 0.00000876
Iteration 25/1000 | Loss: 0.00000875
Iteration 26/1000 | Loss: 0.00000875
Iteration 27/1000 | Loss: 0.00000874
Iteration 28/1000 | Loss: 0.00000872
Iteration 29/1000 | Loss: 0.00000871
Iteration 30/1000 | Loss: 0.00000870
Iteration 31/1000 | Loss: 0.00000870
Iteration 32/1000 | Loss: 0.00000869
Iteration 33/1000 | Loss: 0.00000865
Iteration 34/1000 | Loss: 0.00000865
Iteration 35/1000 | Loss: 0.00000864
Iteration 36/1000 | Loss: 0.00000864
Iteration 37/1000 | Loss: 0.00000864
Iteration 38/1000 | Loss: 0.00000862
Iteration 39/1000 | Loss: 0.00000862
Iteration 40/1000 | Loss: 0.00000861
Iteration 41/1000 | Loss: 0.00000861
Iteration 42/1000 | Loss: 0.00000860
Iteration 43/1000 | Loss: 0.00000859
Iteration 44/1000 | Loss: 0.00000858
Iteration 45/1000 | Loss: 0.00000858
Iteration 46/1000 | Loss: 0.00000857
Iteration 47/1000 | Loss: 0.00000853
Iteration 48/1000 | Loss: 0.00000851
Iteration 49/1000 | Loss: 0.00000851
Iteration 50/1000 | Loss: 0.00000850
Iteration 51/1000 | Loss: 0.00000849
Iteration 52/1000 | Loss: 0.00000849
Iteration 53/1000 | Loss: 0.00000847
Iteration 54/1000 | Loss: 0.00000847
Iteration 55/1000 | Loss: 0.00000847
Iteration 56/1000 | Loss: 0.00000847
Iteration 57/1000 | Loss: 0.00000847
Iteration 58/1000 | Loss: 0.00000847
Iteration 59/1000 | Loss: 0.00000847
Iteration 60/1000 | Loss: 0.00000847
Iteration 61/1000 | Loss: 0.00000847
Iteration 62/1000 | Loss: 0.00000846
Iteration 63/1000 | Loss: 0.00000846
Iteration 64/1000 | Loss: 0.00000846
Iteration 65/1000 | Loss: 0.00000846
Iteration 66/1000 | Loss: 0.00000846
Iteration 67/1000 | Loss: 0.00000845
Iteration 68/1000 | Loss: 0.00000844
Iteration 69/1000 | Loss: 0.00000844
Iteration 70/1000 | Loss: 0.00000844
Iteration 71/1000 | Loss: 0.00000843
Iteration 72/1000 | Loss: 0.00000843
Iteration 73/1000 | Loss: 0.00000843
Iteration 74/1000 | Loss: 0.00000843
Iteration 75/1000 | Loss: 0.00000843
Iteration 76/1000 | Loss: 0.00000843
Iteration 77/1000 | Loss: 0.00000843
Iteration 78/1000 | Loss: 0.00000843
Iteration 79/1000 | Loss: 0.00000843
Iteration 80/1000 | Loss: 0.00000843
Iteration 81/1000 | Loss: 0.00000843
Iteration 82/1000 | Loss: 0.00000843
Iteration 83/1000 | Loss: 0.00000843
Iteration 84/1000 | Loss: 0.00000843
Iteration 85/1000 | Loss: 0.00000843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [8.431597052549478e-06, 8.431597052549478e-06, 8.431597052549478e-06, 8.431597052549478e-06, 8.431597052549478e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.431597052549478e-06

Optimization complete. Final v2v error: 2.517124891281128 mm

Highest mean error: 2.7601795196533203 mm for frame 238

Lowest mean error: 2.322493076324463 mm for frame 5

Saving results

Total time: 33.9354944229126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397754
Iteration 2/25 | Loss: 0.00119558
Iteration 3/25 | Loss: 0.00109347
Iteration 4/25 | Loss: 0.00108616
Iteration 5/25 | Loss: 0.00108363
Iteration 6/25 | Loss: 0.00108363
Iteration 7/25 | Loss: 0.00108363
Iteration 8/25 | Loss: 0.00108363
Iteration 9/25 | Loss: 0.00108363
Iteration 10/25 | Loss: 0.00108363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001083632931113243, 0.001083632931113243, 0.001083632931113243, 0.001083632931113243, 0.001083632931113243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001083632931113243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61519301
Iteration 2/25 | Loss: 0.00069294
Iteration 3/25 | Loss: 0.00069294
Iteration 4/25 | Loss: 0.00069294
Iteration 5/25 | Loss: 0.00069294
Iteration 6/25 | Loss: 0.00069294
Iteration 7/25 | Loss: 0.00069294
Iteration 8/25 | Loss: 0.00069294
Iteration 9/25 | Loss: 0.00069294
Iteration 10/25 | Loss: 0.00069294
Iteration 11/25 | Loss: 0.00069294
Iteration 12/25 | Loss: 0.00069294
Iteration 13/25 | Loss: 0.00069294
Iteration 14/25 | Loss: 0.00069294
Iteration 15/25 | Loss: 0.00069294
Iteration 16/25 | Loss: 0.00069294
Iteration 17/25 | Loss: 0.00069294
Iteration 18/25 | Loss: 0.00069294
Iteration 19/25 | Loss: 0.00069294
Iteration 20/25 | Loss: 0.00069294
Iteration 21/25 | Loss: 0.00069294
Iteration 22/25 | Loss: 0.00069294
Iteration 23/25 | Loss: 0.00069294
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006929385708644986, 0.0006929385708644986, 0.0006929385708644986, 0.0006929385708644986, 0.0006929385708644986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006929385708644986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069294
Iteration 2/1000 | Loss: 0.00002407
Iteration 3/1000 | Loss: 0.00001347
Iteration 4/1000 | Loss: 0.00001179
Iteration 5/1000 | Loss: 0.00001091
Iteration 6/1000 | Loss: 0.00001042
Iteration 7/1000 | Loss: 0.00001037
Iteration 8/1000 | Loss: 0.00001036
Iteration 9/1000 | Loss: 0.00001022
Iteration 10/1000 | Loss: 0.00000995
Iteration 11/1000 | Loss: 0.00000964
Iteration 12/1000 | Loss: 0.00000959
Iteration 13/1000 | Loss: 0.00000956
Iteration 14/1000 | Loss: 0.00000947
Iteration 15/1000 | Loss: 0.00000941
Iteration 16/1000 | Loss: 0.00000941
Iteration 17/1000 | Loss: 0.00000927
Iteration 18/1000 | Loss: 0.00000924
Iteration 19/1000 | Loss: 0.00000923
Iteration 20/1000 | Loss: 0.00000922
Iteration 21/1000 | Loss: 0.00000921
Iteration 22/1000 | Loss: 0.00000920
Iteration 23/1000 | Loss: 0.00000920
Iteration 24/1000 | Loss: 0.00000919
Iteration 25/1000 | Loss: 0.00000917
Iteration 26/1000 | Loss: 0.00000916
Iteration 27/1000 | Loss: 0.00000916
Iteration 28/1000 | Loss: 0.00000916
Iteration 29/1000 | Loss: 0.00000916
Iteration 30/1000 | Loss: 0.00000916
Iteration 31/1000 | Loss: 0.00000915
Iteration 32/1000 | Loss: 0.00000915
Iteration 33/1000 | Loss: 0.00000914
Iteration 34/1000 | Loss: 0.00000914
Iteration 35/1000 | Loss: 0.00000914
Iteration 36/1000 | Loss: 0.00000913
Iteration 37/1000 | Loss: 0.00000913
Iteration 38/1000 | Loss: 0.00000913
Iteration 39/1000 | Loss: 0.00000912
Iteration 40/1000 | Loss: 0.00000912
Iteration 41/1000 | Loss: 0.00000912
Iteration 42/1000 | Loss: 0.00000912
Iteration 43/1000 | Loss: 0.00000911
Iteration 44/1000 | Loss: 0.00000911
Iteration 45/1000 | Loss: 0.00000911
Iteration 46/1000 | Loss: 0.00000910
Iteration 47/1000 | Loss: 0.00000910
Iteration 48/1000 | Loss: 0.00000910
Iteration 49/1000 | Loss: 0.00000909
Iteration 50/1000 | Loss: 0.00000909
Iteration 51/1000 | Loss: 0.00000908
Iteration 52/1000 | Loss: 0.00000908
Iteration 53/1000 | Loss: 0.00000908
Iteration 54/1000 | Loss: 0.00000907
Iteration 55/1000 | Loss: 0.00000907
Iteration 56/1000 | Loss: 0.00000906
Iteration 57/1000 | Loss: 0.00000906
Iteration 58/1000 | Loss: 0.00000906
Iteration 59/1000 | Loss: 0.00000906
Iteration 60/1000 | Loss: 0.00000905
Iteration 61/1000 | Loss: 0.00000905
Iteration 62/1000 | Loss: 0.00000904
Iteration 63/1000 | Loss: 0.00000904
Iteration 64/1000 | Loss: 0.00000904
Iteration 65/1000 | Loss: 0.00000904
Iteration 66/1000 | Loss: 0.00000904
Iteration 67/1000 | Loss: 0.00000903
Iteration 68/1000 | Loss: 0.00000903
Iteration 69/1000 | Loss: 0.00000903
Iteration 70/1000 | Loss: 0.00000903
Iteration 71/1000 | Loss: 0.00000903
Iteration 72/1000 | Loss: 0.00000903
Iteration 73/1000 | Loss: 0.00000902
Iteration 74/1000 | Loss: 0.00000902
Iteration 75/1000 | Loss: 0.00000902
Iteration 76/1000 | Loss: 0.00000901
Iteration 77/1000 | Loss: 0.00000901
Iteration 78/1000 | Loss: 0.00000901
Iteration 79/1000 | Loss: 0.00000901
Iteration 80/1000 | Loss: 0.00000901
Iteration 81/1000 | Loss: 0.00000901
Iteration 82/1000 | Loss: 0.00000901
Iteration 83/1000 | Loss: 0.00000901
Iteration 84/1000 | Loss: 0.00000901
Iteration 85/1000 | Loss: 0.00000901
Iteration 86/1000 | Loss: 0.00000901
Iteration 87/1000 | Loss: 0.00000900
Iteration 88/1000 | Loss: 0.00000900
Iteration 89/1000 | Loss: 0.00000900
Iteration 90/1000 | Loss: 0.00000900
Iteration 91/1000 | Loss: 0.00000900
Iteration 92/1000 | Loss: 0.00000899
Iteration 93/1000 | Loss: 0.00000899
Iteration 94/1000 | Loss: 0.00000899
Iteration 95/1000 | Loss: 0.00000899
Iteration 96/1000 | Loss: 0.00000899
Iteration 97/1000 | Loss: 0.00000899
Iteration 98/1000 | Loss: 0.00000899
Iteration 99/1000 | Loss: 0.00000899
Iteration 100/1000 | Loss: 0.00000899
Iteration 101/1000 | Loss: 0.00000899
Iteration 102/1000 | Loss: 0.00000899
Iteration 103/1000 | Loss: 0.00000899
Iteration 104/1000 | Loss: 0.00000899
Iteration 105/1000 | Loss: 0.00000899
Iteration 106/1000 | Loss: 0.00000898
Iteration 107/1000 | Loss: 0.00000898
Iteration 108/1000 | Loss: 0.00000898
Iteration 109/1000 | Loss: 0.00000898
Iteration 110/1000 | Loss: 0.00000898
Iteration 111/1000 | Loss: 0.00000898
Iteration 112/1000 | Loss: 0.00000897
Iteration 113/1000 | Loss: 0.00000897
Iteration 114/1000 | Loss: 0.00000897
Iteration 115/1000 | Loss: 0.00000896
Iteration 116/1000 | Loss: 0.00000896
Iteration 117/1000 | Loss: 0.00000896
Iteration 118/1000 | Loss: 0.00000896
Iteration 119/1000 | Loss: 0.00000896
Iteration 120/1000 | Loss: 0.00000896
Iteration 121/1000 | Loss: 0.00000896
Iteration 122/1000 | Loss: 0.00000896
Iteration 123/1000 | Loss: 0.00000896
Iteration 124/1000 | Loss: 0.00000896
Iteration 125/1000 | Loss: 0.00000895
Iteration 126/1000 | Loss: 0.00000895
Iteration 127/1000 | Loss: 0.00000895
Iteration 128/1000 | Loss: 0.00000895
Iteration 129/1000 | Loss: 0.00000895
Iteration 130/1000 | Loss: 0.00000895
Iteration 131/1000 | Loss: 0.00000895
Iteration 132/1000 | Loss: 0.00000895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [8.953941687650513e-06, 8.953941687650513e-06, 8.953941687650513e-06, 8.953941687650513e-06, 8.953941687650513e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.953941687650513e-06

Optimization complete. Final v2v error: 2.5816197395324707 mm

Highest mean error: 2.7373807430267334 mm for frame 132

Lowest mean error: 2.435054063796997 mm for frame 34

Saving results

Total time: 37.80647850036621
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787453
Iteration 2/25 | Loss: 0.00138201
Iteration 3/25 | Loss: 0.00118408
Iteration 4/25 | Loss: 0.00117274
Iteration 5/25 | Loss: 0.00117192
Iteration 6/25 | Loss: 0.00117192
Iteration 7/25 | Loss: 0.00117192
Iteration 8/25 | Loss: 0.00117192
Iteration 9/25 | Loss: 0.00117192
Iteration 10/25 | Loss: 0.00117192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011719204485416412, 0.0011719204485416412, 0.0011719204485416412, 0.0011719204485416412, 0.0011719204485416412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011719204485416412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32796621
Iteration 2/25 | Loss: 0.00055357
Iteration 3/25 | Loss: 0.00055357
Iteration 4/25 | Loss: 0.00055357
Iteration 5/25 | Loss: 0.00055357
Iteration 6/25 | Loss: 0.00055357
Iteration 7/25 | Loss: 0.00055357
Iteration 8/25 | Loss: 0.00055357
Iteration 9/25 | Loss: 0.00055357
Iteration 10/25 | Loss: 0.00055357
Iteration 11/25 | Loss: 0.00055357
Iteration 12/25 | Loss: 0.00055357
Iteration 13/25 | Loss: 0.00055357
Iteration 14/25 | Loss: 0.00055357
Iteration 15/25 | Loss: 0.00055357
Iteration 16/25 | Loss: 0.00055357
Iteration 17/25 | Loss: 0.00055357
Iteration 18/25 | Loss: 0.00055357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005535668460652232, 0.0005535668460652232, 0.0005535668460652232, 0.0005535668460652232, 0.0005535668460652232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005535668460652232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055357
Iteration 2/1000 | Loss: 0.00002724
Iteration 3/1000 | Loss: 0.00002233
Iteration 4/1000 | Loss: 0.00002080
Iteration 5/1000 | Loss: 0.00002009
Iteration 6/1000 | Loss: 0.00001942
Iteration 7/1000 | Loss: 0.00001891
Iteration 8/1000 | Loss: 0.00001839
Iteration 9/1000 | Loss: 0.00001810
Iteration 10/1000 | Loss: 0.00001808
Iteration 11/1000 | Loss: 0.00001792
Iteration 12/1000 | Loss: 0.00001785
Iteration 13/1000 | Loss: 0.00001781
Iteration 14/1000 | Loss: 0.00001781
Iteration 15/1000 | Loss: 0.00001780
Iteration 16/1000 | Loss: 0.00001780
Iteration 17/1000 | Loss: 0.00001780
Iteration 18/1000 | Loss: 0.00001779
Iteration 19/1000 | Loss: 0.00001779
Iteration 20/1000 | Loss: 0.00001779
Iteration 21/1000 | Loss: 0.00001779
Iteration 22/1000 | Loss: 0.00001779
Iteration 23/1000 | Loss: 0.00001779
Iteration 24/1000 | Loss: 0.00001778
Iteration 25/1000 | Loss: 0.00001778
Iteration 26/1000 | Loss: 0.00001778
Iteration 27/1000 | Loss: 0.00001777
Iteration 28/1000 | Loss: 0.00001773
Iteration 29/1000 | Loss: 0.00001772
Iteration 30/1000 | Loss: 0.00001772
Iteration 31/1000 | Loss: 0.00001769
Iteration 32/1000 | Loss: 0.00001769
Iteration 33/1000 | Loss: 0.00001769
Iteration 34/1000 | Loss: 0.00001769
Iteration 35/1000 | Loss: 0.00001768
Iteration 36/1000 | Loss: 0.00001768
Iteration 37/1000 | Loss: 0.00001768
Iteration 38/1000 | Loss: 0.00001768
Iteration 39/1000 | Loss: 0.00001768
Iteration 40/1000 | Loss: 0.00001768
Iteration 41/1000 | Loss: 0.00001767
Iteration 42/1000 | Loss: 0.00001767
Iteration 43/1000 | Loss: 0.00001767
Iteration 44/1000 | Loss: 0.00001766
Iteration 45/1000 | Loss: 0.00001766
Iteration 46/1000 | Loss: 0.00001766
Iteration 47/1000 | Loss: 0.00001766
Iteration 48/1000 | Loss: 0.00001766
Iteration 49/1000 | Loss: 0.00001766
Iteration 50/1000 | Loss: 0.00001765
Iteration 51/1000 | Loss: 0.00001765
Iteration 52/1000 | Loss: 0.00001765
Iteration 53/1000 | Loss: 0.00001765
Iteration 54/1000 | Loss: 0.00001764
Iteration 55/1000 | Loss: 0.00001764
Iteration 56/1000 | Loss: 0.00001764
Iteration 57/1000 | Loss: 0.00001764
Iteration 58/1000 | Loss: 0.00001764
Iteration 59/1000 | Loss: 0.00001764
Iteration 60/1000 | Loss: 0.00001764
Iteration 61/1000 | Loss: 0.00001764
Iteration 62/1000 | Loss: 0.00001764
Iteration 63/1000 | Loss: 0.00001764
Iteration 64/1000 | Loss: 0.00001763
Iteration 65/1000 | Loss: 0.00001763
Iteration 66/1000 | Loss: 0.00001763
Iteration 67/1000 | Loss: 0.00001763
Iteration 68/1000 | Loss: 0.00001763
Iteration 69/1000 | Loss: 0.00001763
Iteration 70/1000 | Loss: 0.00001763
Iteration 71/1000 | Loss: 0.00001763
Iteration 72/1000 | Loss: 0.00001763
Iteration 73/1000 | Loss: 0.00001763
Iteration 74/1000 | Loss: 0.00001762
Iteration 75/1000 | Loss: 0.00001761
Iteration 76/1000 | Loss: 0.00001761
Iteration 77/1000 | Loss: 0.00001761
Iteration 78/1000 | Loss: 0.00001760
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001759
Iteration 81/1000 | Loss: 0.00001759
Iteration 82/1000 | Loss: 0.00001759
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001757
Iteration 86/1000 | Loss: 0.00001757
Iteration 87/1000 | Loss: 0.00001757
Iteration 88/1000 | Loss: 0.00001756
Iteration 89/1000 | Loss: 0.00001756
Iteration 90/1000 | Loss: 0.00001756
Iteration 91/1000 | Loss: 0.00001756
Iteration 92/1000 | Loss: 0.00001756
Iteration 93/1000 | Loss: 0.00001756
Iteration 94/1000 | Loss: 0.00001756
Iteration 95/1000 | Loss: 0.00001755
Iteration 96/1000 | Loss: 0.00001755
Iteration 97/1000 | Loss: 0.00001755
Iteration 98/1000 | Loss: 0.00001755
Iteration 99/1000 | Loss: 0.00001755
Iteration 100/1000 | Loss: 0.00001754
Iteration 101/1000 | Loss: 0.00001754
Iteration 102/1000 | Loss: 0.00001754
Iteration 103/1000 | Loss: 0.00001754
Iteration 104/1000 | Loss: 0.00001754
Iteration 105/1000 | Loss: 0.00001754
Iteration 106/1000 | Loss: 0.00001754
Iteration 107/1000 | Loss: 0.00001754
Iteration 108/1000 | Loss: 0.00001754
Iteration 109/1000 | Loss: 0.00001754
Iteration 110/1000 | Loss: 0.00001754
Iteration 111/1000 | Loss: 0.00001754
Iteration 112/1000 | Loss: 0.00001754
Iteration 113/1000 | Loss: 0.00001754
Iteration 114/1000 | Loss: 0.00001754
Iteration 115/1000 | Loss: 0.00001754
Iteration 116/1000 | Loss: 0.00001754
Iteration 117/1000 | Loss: 0.00001754
Iteration 118/1000 | Loss: 0.00001754
Iteration 119/1000 | Loss: 0.00001754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.754386721586343e-05, 1.754386721586343e-05, 1.754386721586343e-05, 1.754386721586343e-05, 1.754386721586343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.754386721586343e-05

Optimization complete. Final v2v error: 3.491482973098755 mm

Highest mean error: 3.7368390560150146 mm for frame 54

Lowest mean error: 3.2727468013763428 mm for frame 119

Saving results

Total time: 33.794063568115234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531843
Iteration 2/25 | Loss: 0.00147987
Iteration 3/25 | Loss: 0.00117648
Iteration 4/25 | Loss: 0.00114642
Iteration 5/25 | Loss: 0.00113787
Iteration 6/25 | Loss: 0.00113674
Iteration 7/25 | Loss: 0.00112638
Iteration 8/25 | Loss: 0.00112237
Iteration 9/25 | Loss: 0.00112082
Iteration 10/25 | Loss: 0.00111984
Iteration 11/25 | Loss: 0.00112377
Iteration 12/25 | Loss: 0.00112263
Iteration 13/25 | Loss: 0.00111878
Iteration 14/25 | Loss: 0.00111754
Iteration 15/25 | Loss: 0.00111676
Iteration 16/25 | Loss: 0.00111663
Iteration 17/25 | Loss: 0.00111661
Iteration 18/25 | Loss: 0.00111661
Iteration 19/25 | Loss: 0.00111661
Iteration 20/25 | Loss: 0.00111661
Iteration 21/25 | Loss: 0.00111661
Iteration 22/25 | Loss: 0.00111661
Iteration 23/25 | Loss: 0.00111661
Iteration 24/25 | Loss: 0.00111661
Iteration 25/25 | Loss: 0.00111661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57808995
Iteration 2/25 | Loss: 0.00090711
Iteration 3/25 | Loss: 0.00090710
Iteration 4/25 | Loss: 0.00090710
Iteration 5/25 | Loss: 0.00090710
Iteration 6/25 | Loss: 0.00090710
Iteration 7/25 | Loss: 0.00090710
Iteration 8/25 | Loss: 0.00090710
Iteration 9/25 | Loss: 0.00090710
Iteration 10/25 | Loss: 0.00090710
Iteration 11/25 | Loss: 0.00090710
Iteration 12/25 | Loss: 0.00090710
Iteration 13/25 | Loss: 0.00090710
Iteration 14/25 | Loss: 0.00090710
Iteration 15/25 | Loss: 0.00090710
Iteration 16/25 | Loss: 0.00090710
Iteration 17/25 | Loss: 0.00090710
Iteration 18/25 | Loss: 0.00090710
Iteration 19/25 | Loss: 0.00090710
Iteration 20/25 | Loss: 0.00090710
Iteration 21/25 | Loss: 0.00090710
Iteration 22/25 | Loss: 0.00090710
Iteration 23/25 | Loss: 0.00090710
Iteration 24/25 | Loss: 0.00090710
Iteration 25/25 | Loss: 0.00090710

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090710
Iteration 2/1000 | Loss: 0.00002645
Iteration 3/1000 | Loss: 0.00001818
Iteration 4/1000 | Loss: 0.00001635
Iteration 5/1000 | Loss: 0.00001542
Iteration 6/1000 | Loss: 0.00001489
Iteration 7/1000 | Loss: 0.00001448
Iteration 8/1000 | Loss: 0.00001415
Iteration 9/1000 | Loss: 0.00001394
Iteration 10/1000 | Loss: 0.00001364
Iteration 11/1000 | Loss: 0.00001350
Iteration 12/1000 | Loss: 0.00001337
Iteration 13/1000 | Loss: 0.00001331
Iteration 14/1000 | Loss: 0.00001331
Iteration 15/1000 | Loss: 0.00001328
Iteration 16/1000 | Loss: 0.00001327
Iteration 17/1000 | Loss: 0.00001326
Iteration 18/1000 | Loss: 0.00001326
Iteration 19/1000 | Loss: 0.00001325
Iteration 20/1000 | Loss: 0.00001325
Iteration 21/1000 | Loss: 0.00001324
Iteration 22/1000 | Loss: 0.00001324
Iteration 23/1000 | Loss: 0.00001324
Iteration 24/1000 | Loss: 0.00001323
Iteration 25/1000 | Loss: 0.00001323
Iteration 26/1000 | Loss: 0.00001323
Iteration 27/1000 | Loss: 0.00001322
Iteration 28/1000 | Loss: 0.00001322
Iteration 29/1000 | Loss: 0.00001321
Iteration 30/1000 | Loss: 0.00001321
Iteration 31/1000 | Loss: 0.00001320
Iteration 32/1000 | Loss: 0.00001320
Iteration 33/1000 | Loss: 0.00001320
Iteration 34/1000 | Loss: 0.00001320
Iteration 35/1000 | Loss: 0.00001320
Iteration 36/1000 | Loss: 0.00001319
Iteration 37/1000 | Loss: 0.00001319
Iteration 38/1000 | Loss: 0.00001319
Iteration 39/1000 | Loss: 0.00001319
Iteration 40/1000 | Loss: 0.00001319
Iteration 41/1000 | Loss: 0.00001319
Iteration 42/1000 | Loss: 0.00001318
Iteration 43/1000 | Loss: 0.00001318
Iteration 44/1000 | Loss: 0.00001318
Iteration 45/1000 | Loss: 0.00001318
Iteration 46/1000 | Loss: 0.00001318
Iteration 47/1000 | Loss: 0.00001317
Iteration 48/1000 | Loss: 0.00001317
Iteration 49/1000 | Loss: 0.00001317
Iteration 50/1000 | Loss: 0.00001317
Iteration 51/1000 | Loss: 0.00001317
Iteration 52/1000 | Loss: 0.00001317
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001317
Iteration 55/1000 | Loss: 0.00001317
Iteration 56/1000 | Loss: 0.00001316
Iteration 57/1000 | Loss: 0.00001316
Iteration 58/1000 | Loss: 0.00001316
Iteration 59/1000 | Loss: 0.00001316
Iteration 60/1000 | Loss: 0.00001316
Iteration 61/1000 | Loss: 0.00001315
Iteration 62/1000 | Loss: 0.00001315
Iteration 63/1000 | Loss: 0.00001315
Iteration 64/1000 | Loss: 0.00001315
Iteration 65/1000 | Loss: 0.00001315
Iteration 66/1000 | Loss: 0.00001315
Iteration 67/1000 | Loss: 0.00001314
Iteration 68/1000 | Loss: 0.00001314
Iteration 69/1000 | Loss: 0.00001314
Iteration 70/1000 | Loss: 0.00001313
Iteration 71/1000 | Loss: 0.00001313
Iteration 72/1000 | Loss: 0.00001313
Iteration 73/1000 | Loss: 0.00001313
Iteration 74/1000 | Loss: 0.00001313
Iteration 75/1000 | Loss: 0.00001313
Iteration 76/1000 | Loss: 0.00001313
Iteration 77/1000 | Loss: 0.00001313
Iteration 78/1000 | Loss: 0.00001313
Iteration 79/1000 | Loss: 0.00001312
Iteration 80/1000 | Loss: 0.00001312
Iteration 81/1000 | Loss: 0.00001312
Iteration 82/1000 | Loss: 0.00001312
Iteration 83/1000 | Loss: 0.00001312
Iteration 84/1000 | Loss: 0.00001312
Iteration 85/1000 | Loss: 0.00001311
Iteration 86/1000 | Loss: 0.00001311
Iteration 87/1000 | Loss: 0.00001311
Iteration 88/1000 | Loss: 0.00001311
Iteration 89/1000 | Loss: 0.00001311
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001311
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001311
Iteration 94/1000 | Loss: 0.00001311
Iteration 95/1000 | Loss: 0.00001311
Iteration 96/1000 | Loss: 0.00001311
Iteration 97/1000 | Loss: 0.00001311
Iteration 98/1000 | Loss: 0.00001311
Iteration 99/1000 | Loss: 0.00001311
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001311
Iteration 102/1000 | Loss: 0.00001311
Iteration 103/1000 | Loss: 0.00001311
Iteration 104/1000 | Loss: 0.00001311
Iteration 105/1000 | Loss: 0.00001311
Iteration 106/1000 | Loss: 0.00001311
Iteration 107/1000 | Loss: 0.00001311
Iteration 108/1000 | Loss: 0.00001311
Iteration 109/1000 | Loss: 0.00001311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.310763127548853e-05, 1.310763127548853e-05, 1.310763127548853e-05, 1.310763127548853e-05, 1.310763127548853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.310763127548853e-05

Optimization complete. Final v2v error: 3.0661110877990723 mm

Highest mean error: 3.85426664352417 mm for frame 67

Lowest mean error: 2.7779479026794434 mm for frame 239

Saving results

Total time: 59.61324691772461
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792788
Iteration 2/25 | Loss: 0.00130395
Iteration 3/25 | Loss: 0.00109403
Iteration 4/25 | Loss: 0.00107895
Iteration 5/25 | Loss: 0.00107364
Iteration 6/25 | Loss: 0.00107190
Iteration 7/25 | Loss: 0.00107183
Iteration 8/25 | Loss: 0.00107183
Iteration 9/25 | Loss: 0.00107183
Iteration 10/25 | Loss: 0.00107183
Iteration 11/25 | Loss: 0.00107183
Iteration 12/25 | Loss: 0.00107183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001071828417479992, 0.001071828417479992, 0.001071828417479992, 0.001071828417479992, 0.001071828417479992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001071828417479992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16276968
Iteration 2/25 | Loss: 0.00088240
Iteration 3/25 | Loss: 0.00088240
Iteration 4/25 | Loss: 0.00088240
Iteration 5/25 | Loss: 0.00088239
Iteration 6/25 | Loss: 0.00088239
Iteration 7/25 | Loss: 0.00088239
Iteration 8/25 | Loss: 0.00088239
Iteration 9/25 | Loss: 0.00088239
Iteration 10/25 | Loss: 0.00088239
Iteration 11/25 | Loss: 0.00088239
Iteration 12/25 | Loss: 0.00088239
Iteration 13/25 | Loss: 0.00088239
Iteration 14/25 | Loss: 0.00088239
Iteration 15/25 | Loss: 0.00088239
Iteration 16/25 | Loss: 0.00088239
Iteration 17/25 | Loss: 0.00088239
Iteration 18/25 | Loss: 0.00088239
Iteration 19/25 | Loss: 0.00088239
Iteration 20/25 | Loss: 0.00088239
Iteration 21/25 | Loss: 0.00088239
Iteration 22/25 | Loss: 0.00088239
Iteration 23/25 | Loss: 0.00088239
Iteration 24/25 | Loss: 0.00088239
Iteration 25/25 | Loss: 0.00088239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088239
Iteration 2/1000 | Loss: 0.00004485
Iteration 3/1000 | Loss: 0.00002721
Iteration 4/1000 | Loss: 0.00001983
Iteration 5/1000 | Loss: 0.00001720
Iteration 6/1000 | Loss: 0.00001598
Iteration 7/1000 | Loss: 0.00001504
Iteration 8/1000 | Loss: 0.00001451
Iteration 9/1000 | Loss: 0.00001399
Iteration 10/1000 | Loss: 0.00001365
Iteration 11/1000 | Loss: 0.00001346
Iteration 12/1000 | Loss: 0.00001321
Iteration 13/1000 | Loss: 0.00001308
Iteration 14/1000 | Loss: 0.00001306
Iteration 15/1000 | Loss: 0.00001296
Iteration 16/1000 | Loss: 0.00001296
Iteration 17/1000 | Loss: 0.00001289
Iteration 18/1000 | Loss: 0.00001289
Iteration 19/1000 | Loss: 0.00001288
Iteration 20/1000 | Loss: 0.00001288
Iteration 21/1000 | Loss: 0.00001287
Iteration 22/1000 | Loss: 0.00001286
Iteration 23/1000 | Loss: 0.00001284
Iteration 24/1000 | Loss: 0.00001282
Iteration 25/1000 | Loss: 0.00001281
Iteration 26/1000 | Loss: 0.00001278
Iteration 27/1000 | Loss: 0.00001276
Iteration 28/1000 | Loss: 0.00001276
Iteration 29/1000 | Loss: 0.00001273
Iteration 30/1000 | Loss: 0.00001269
Iteration 31/1000 | Loss: 0.00001264
Iteration 32/1000 | Loss: 0.00001263
Iteration 33/1000 | Loss: 0.00001263
Iteration 34/1000 | Loss: 0.00001263
Iteration 35/1000 | Loss: 0.00001262
Iteration 36/1000 | Loss: 0.00001262
Iteration 37/1000 | Loss: 0.00001257
Iteration 38/1000 | Loss: 0.00001256
Iteration 39/1000 | Loss: 0.00001256
Iteration 40/1000 | Loss: 0.00001256
Iteration 41/1000 | Loss: 0.00001256
Iteration 42/1000 | Loss: 0.00001256
Iteration 43/1000 | Loss: 0.00001256
Iteration 44/1000 | Loss: 0.00001256
Iteration 45/1000 | Loss: 0.00001256
Iteration 46/1000 | Loss: 0.00001255
Iteration 47/1000 | Loss: 0.00001255
Iteration 48/1000 | Loss: 0.00001255
Iteration 49/1000 | Loss: 0.00001255
Iteration 50/1000 | Loss: 0.00001255
Iteration 51/1000 | Loss: 0.00001255
Iteration 52/1000 | Loss: 0.00001255
Iteration 53/1000 | Loss: 0.00001255
Iteration 54/1000 | Loss: 0.00001255
Iteration 55/1000 | Loss: 0.00001255
Iteration 56/1000 | Loss: 0.00001254
Iteration 57/1000 | Loss: 0.00001253
Iteration 58/1000 | Loss: 0.00001252
Iteration 59/1000 | Loss: 0.00001252
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001251
Iteration 63/1000 | Loss: 0.00001249
Iteration 64/1000 | Loss: 0.00001249
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001246
Iteration 67/1000 | Loss: 0.00001245
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001244
Iteration 72/1000 | Loss: 0.00001244
Iteration 73/1000 | Loss: 0.00001244
Iteration 74/1000 | Loss: 0.00001244
Iteration 75/1000 | Loss: 0.00001244
Iteration 76/1000 | Loss: 0.00001244
Iteration 77/1000 | Loss: 0.00001243
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00001242
Iteration 80/1000 | Loss: 0.00001241
Iteration 81/1000 | Loss: 0.00001241
Iteration 82/1000 | Loss: 0.00001241
Iteration 83/1000 | Loss: 0.00001241
Iteration 84/1000 | Loss: 0.00001240
Iteration 85/1000 | Loss: 0.00001240
Iteration 86/1000 | Loss: 0.00001239
Iteration 87/1000 | Loss: 0.00001239
Iteration 88/1000 | Loss: 0.00001239
Iteration 89/1000 | Loss: 0.00001238
Iteration 90/1000 | Loss: 0.00001237
Iteration 91/1000 | Loss: 0.00001237
Iteration 92/1000 | Loss: 0.00001236
Iteration 93/1000 | Loss: 0.00001236
Iteration 94/1000 | Loss: 0.00001236
Iteration 95/1000 | Loss: 0.00001235
Iteration 96/1000 | Loss: 0.00001235
Iteration 97/1000 | Loss: 0.00001235
Iteration 98/1000 | Loss: 0.00001235
Iteration 99/1000 | Loss: 0.00001235
Iteration 100/1000 | Loss: 0.00001235
Iteration 101/1000 | Loss: 0.00001235
Iteration 102/1000 | Loss: 0.00001235
Iteration 103/1000 | Loss: 0.00001235
Iteration 104/1000 | Loss: 0.00001235
Iteration 105/1000 | Loss: 0.00001235
Iteration 106/1000 | Loss: 0.00001235
Iteration 107/1000 | Loss: 0.00001235
Iteration 108/1000 | Loss: 0.00001235
Iteration 109/1000 | Loss: 0.00001235
Iteration 110/1000 | Loss: 0.00001235
Iteration 111/1000 | Loss: 0.00001235
Iteration 112/1000 | Loss: 0.00001235
Iteration 113/1000 | Loss: 0.00001235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.2350254110060632e-05, 1.2350254110060632e-05, 1.2350254110060632e-05, 1.2350254110060632e-05, 1.2350254110060632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2350254110060632e-05

Optimization complete. Final v2v error: 2.882652997970581 mm

Highest mean error: 4.211095809936523 mm for frame 70

Lowest mean error: 2.3360657691955566 mm for frame 102

Saving results

Total time: 38.06130123138428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966458
Iteration 2/25 | Loss: 0.00313330
Iteration 3/25 | Loss: 0.00211015
Iteration 4/25 | Loss: 0.00197865
Iteration 5/25 | Loss: 0.00184085
Iteration 6/25 | Loss: 0.00180966
Iteration 7/25 | Loss: 0.00186978
Iteration 8/25 | Loss: 0.00176288
Iteration 9/25 | Loss: 0.00168505
Iteration 10/25 | Loss: 0.00152961
Iteration 11/25 | Loss: 0.00145565
Iteration 12/25 | Loss: 0.00139871
Iteration 13/25 | Loss: 0.00135350
Iteration 14/25 | Loss: 0.00131994
Iteration 15/25 | Loss: 0.00131424
Iteration 16/25 | Loss: 0.00130992
Iteration 17/25 | Loss: 0.00128448
Iteration 18/25 | Loss: 0.00128708
Iteration 19/25 | Loss: 0.00127926
Iteration 20/25 | Loss: 0.00127335
Iteration 21/25 | Loss: 0.00126871
Iteration 22/25 | Loss: 0.00126113
Iteration 23/25 | Loss: 0.00125757
Iteration 24/25 | Loss: 0.00125249
Iteration 25/25 | Loss: 0.00125093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32636964
Iteration 2/25 | Loss: 0.00150665
Iteration 3/25 | Loss: 0.00144523
Iteration 4/25 | Loss: 0.00144523
Iteration 5/25 | Loss: 0.00144523
Iteration 6/25 | Loss: 0.00144523
Iteration 7/25 | Loss: 0.00144523
Iteration 8/25 | Loss: 0.00144523
Iteration 9/25 | Loss: 0.00144523
Iteration 10/25 | Loss: 0.00144523
Iteration 11/25 | Loss: 0.00144523
Iteration 12/25 | Loss: 0.00144523
Iteration 13/25 | Loss: 0.00144523
Iteration 14/25 | Loss: 0.00144523
Iteration 15/25 | Loss: 0.00144523
Iteration 16/25 | Loss: 0.00144523
Iteration 17/25 | Loss: 0.00144523
Iteration 18/25 | Loss: 0.00144523
Iteration 19/25 | Loss: 0.00144523
Iteration 20/25 | Loss: 0.00144523
Iteration 21/25 | Loss: 0.00144523
Iteration 22/25 | Loss: 0.00144523
Iteration 23/25 | Loss: 0.00144523
Iteration 24/25 | Loss: 0.00144523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014452282339334488, 0.0014452282339334488, 0.0014452282339334488, 0.0014452282339334488, 0.0014452282339334488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014452282339334488

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144523
Iteration 2/1000 | Loss: 0.00024031
Iteration 3/1000 | Loss: 0.00030388
Iteration 4/1000 | Loss: 0.00011441
Iteration 5/1000 | Loss: 0.00029909
Iteration 6/1000 | Loss: 0.00010107
Iteration 7/1000 | Loss: 0.00013657
Iteration 8/1000 | Loss: 0.00032808
Iteration 9/1000 | Loss: 0.00016808
Iteration 10/1000 | Loss: 0.00041993
Iteration 11/1000 | Loss: 0.00010537
Iteration 12/1000 | Loss: 0.00008471
Iteration 13/1000 | Loss: 0.00026706
Iteration 14/1000 | Loss: 0.00009674
Iteration 15/1000 | Loss: 0.00009035
Iteration 16/1000 | Loss: 0.00009018
Iteration 17/1000 | Loss: 0.00024262
Iteration 18/1000 | Loss: 0.00013865
Iteration 19/1000 | Loss: 0.00066554
Iteration 20/1000 | Loss: 0.00510522
Iteration 21/1000 | Loss: 0.00323845
Iteration 22/1000 | Loss: 0.00243125
Iteration 23/1000 | Loss: 0.00097000
Iteration 24/1000 | Loss: 0.00018558
Iteration 25/1000 | Loss: 0.00013296
Iteration 26/1000 | Loss: 0.00009604
Iteration 27/1000 | Loss: 0.00005888
Iteration 28/1000 | Loss: 0.00021299
Iteration 29/1000 | Loss: 0.00006518
Iteration 30/1000 | Loss: 0.00019429
Iteration 31/1000 | Loss: 0.00004696
Iteration 32/1000 | Loss: 0.00006335
Iteration 33/1000 | Loss: 0.00003837
Iteration 34/1000 | Loss: 0.00006758
Iteration 35/1000 | Loss: 0.00002837
Iteration 36/1000 | Loss: 0.00002510
Iteration 37/1000 | Loss: 0.00002359
Iteration 38/1000 | Loss: 0.00002231
Iteration 39/1000 | Loss: 0.00002123
Iteration 40/1000 | Loss: 0.00002046
Iteration 41/1000 | Loss: 0.00001977
Iteration 42/1000 | Loss: 0.00010318
Iteration 43/1000 | Loss: 0.00001974
Iteration 44/1000 | Loss: 0.00001918
Iteration 45/1000 | Loss: 0.00001896
Iteration 46/1000 | Loss: 0.00006463
Iteration 47/1000 | Loss: 0.00002307
Iteration 48/1000 | Loss: 0.00004199
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001876
Iteration 51/1000 | Loss: 0.00001869
Iteration 52/1000 | Loss: 0.00001869
Iteration 53/1000 | Loss: 0.00001867
Iteration 54/1000 | Loss: 0.00001865
Iteration 55/1000 | Loss: 0.00001863
Iteration 56/1000 | Loss: 0.00001863
Iteration 57/1000 | Loss: 0.00001862
Iteration 58/1000 | Loss: 0.00001862
Iteration 59/1000 | Loss: 0.00001861
Iteration 60/1000 | Loss: 0.00001861
Iteration 61/1000 | Loss: 0.00001861
Iteration 62/1000 | Loss: 0.00001860
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001858
Iteration 65/1000 | Loss: 0.00001858
Iteration 66/1000 | Loss: 0.00001858
Iteration 67/1000 | Loss: 0.00001857
Iteration 68/1000 | Loss: 0.00001857
Iteration 69/1000 | Loss: 0.00001857
Iteration 70/1000 | Loss: 0.00001854
Iteration 71/1000 | Loss: 0.00001854
Iteration 72/1000 | Loss: 0.00001850
Iteration 73/1000 | Loss: 0.00001850
Iteration 74/1000 | Loss: 0.00001850
Iteration 75/1000 | Loss: 0.00001850
Iteration 76/1000 | Loss: 0.00001849
Iteration 77/1000 | Loss: 0.00001849
Iteration 78/1000 | Loss: 0.00001849
Iteration 79/1000 | Loss: 0.00001849
Iteration 80/1000 | Loss: 0.00001848
Iteration 81/1000 | Loss: 0.00001847
Iteration 82/1000 | Loss: 0.00001847
Iteration 83/1000 | Loss: 0.00001846
Iteration 84/1000 | Loss: 0.00001846
Iteration 85/1000 | Loss: 0.00001846
Iteration 86/1000 | Loss: 0.00001846
Iteration 87/1000 | Loss: 0.00001846
Iteration 88/1000 | Loss: 0.00001845
Iteration 89/1000 | Loss: 0.00001845
Iteration 90/1000 | Loss: 0.00001845
Iteration 91/1000 | Loss: 0.00001845
Iteration 92/1000 | Loss: 0.00001845
Iteration 93/1000 | Loss: 0.00001844
Iteration 94/1000 | Loss: 0.00001842
Iteration 95/1000 | Loss: 0.00001842
Iteration 96/1000 | Loss: 0.00001842
Iteration 97/1000 | Loss: 0.00001842
Iteration 98/1000 | Loss: 0.00001842
Iteration 99/1000 | Loss: 0.00001842
Iteration 100/1000 | Loss: 0.00001842
Iteration 101/1000 | Loss: 0.00001842
Iteration 102/1000 | Loss: 0.00001842
Iteration 103/1000 | Loss: 0.00001842
Iteration 104/1000 | Loss: 0.00001841
Iteration 105/1000 | Loss: 0.00009750
Iteration 106/1000 | Loss: 0.00001873
Iteration 107/1000 | Loss: 0.00001840
Iteration 108/1000 | Loss: 0.00001840
Iteration 109/1000 | Loss: 0.00001839
Iteration 110/1000 | Loss: 0.00001839
Iteration 111/1000 | Loss: 0.00001836
Iteration 112/1000 | Loss: 0.00001836
Iteration 113/1000 | Loss: 0.00001836
Iteration 114/1000 | Loss: 0.00001836
Iteration 115/1000 | Loss: 0.00001836
Iteration 116/1000 | Loss: 0.00001836
Iteration 117/1000 | Loss: 0.00001836
Iteration 118/1000 | Loss: 0.00001836
Iteration 119/1000 | Loss: 0.00001836
Iteration 120/1000 | Loss: 0.00001836
Iteration 121/1000 | Loss: 0.00001835
Iteration 122/1000 | Loss: 0.00001835
Iteration 123/1000 | Loss: 0.00001835
Iteration 124/1000 | Loss: 0.00001835
Iteration 125/1000 | Loss: 0.00001835
Iteration 126/1000 | Loss: 0.00001834
Iteration 127/1000 | Loss: 0.00001834
Iteration 128/1000 | Loss: 0.00001834
Iteration 129/1000 | Loss: 0.00001834
Iteration 130/1000 | Loss: 0.00001834
Iteration 131/1000 | Loss: 0.00001833
Iteration 132/1000 | Loss: 0.00001833
Iteration 133/1000 | Loss: 0.00001833
Iteration 134/1000 | Loss: 0.00001833
Iteration 135/1000 | Loss: 0.00001833
Iteration 136/1000 | Loss: 0.00001833
Iteration 137/1000 | Loss: 0.00001833
Iteration 138/1000 | Loss: 0.00001833
Iteration 139/1000 | Loss: 0.00001833
Iteration 140/1000 | Loss: 0.00001832
Iteration 141/1000 | Loss: 0.00001832
Iteration 142/1000 | Loss: 0.00001832
Iteration 143/1000 | Loss: 0.00001832
Iteration 144/1000 | Loss: 0.00001832
Iteration 145/1000 | Loss: 0.00001832
Iteration 146/1000 | Loss: 0.00001832
Iteration 147/1000 | Loss: 0.00001832
Iteration 148/1000 | Loss: 0.00001832
Iteration 149/1000 | Loss: 0.00001832
Iteration 150/1000 | Loss: 0.00001832
Iteration 151/1000 | Loss: 0.00001831
Iteration 152/1000 | Loss: 0.00001831
Iteration 153/1000 | Loss: 0.00001831
Iteration 154/1000 | Loss: 0.00001831
Iteration 155/1000 | Loss: 0.00001831
Iteration 156/1000 | Loss: 0.00001831
Iteration 157/1000 | Loss: 0.00001831
Iteration 158/1000 | Loss: 0.00001831
Iteration 159/1000 | Loss: 0.00001831
Iteration 160/1000 | Loss: 0.00001831
Iteration 161/1000 | Loss: 0.00001831
Iteration 162/1000 | Loss: 0.00001831
Iteration 163/1000 | Loss: 0.00001831
Iteration 164/1000 | Loss: 0.00001831
Iteration 165/1000 | Loss: 0.00001830
Iteration 166/1000 | Loss: 0.00001830
Iteration 167/1000 | Loss: 0.00001830
Iteration 168/1000 | Loss: 0.00001830
Iteration 169/1000 | Loss: 0.00001830
Iteration 170/1000 | Loss: 0.00001830
Iteration 171/1000 | Loss: 0.00001830
Iteration 172/1000 | Loss: 0.00001830
Iteration 173/1000 | Loss: 0.00001830
Iteration 174/1000 | Loss: 0.00001830
Iteration 175/1000 | Loss: 0.00001830
Iteration 176/1000 | Loss: 0.00001830
Iteration 177/1000 | Loss: 0.00001830
Iteration 178/1000 | Loss: 0.00001830
Iteration 179/1000 | Loss: 0.00001830
Iteration 180/1000 | Loss: 0.00001830
Iteration 181/1000 | Loss: 0.00001830
Iteration 182/1000 | Loss: 0.00001830
Iteration 183/1000 | Loss: 0.00001830
Iteration 184/1000 | Loss: 0.00001830
Iteration 185/1000 | Loss: 0.00001830
Iteration 186/1000 | Loss: 0.00001829
Iteration 187/1000 | Loss: 0.00001829
Iteration 188/1000 | Loss: 0.00001829
Iteration 189/1000 | Loss: 0.00001829
Iteration 190/1000 | Loss: 0.00001829
Iteration 191/1000 | Loss: 0.00001829
Iteration 192/1000 | Loss: 0.00001829
Iteration 193/1000 | Loss: 0.00001829
Iteration 194/1000 | Loss: 0.00001829
Iteration 195/1000 | Loss: 0.00001829
Iteration 196/1000 | Loss: 0.00001829
Iteration 197/1000 | Loss: 0.00001829
Iteration 198/1000 | Loss: 0.00001829
Iteration 199/1000 | Loss: 0.00001829
Iteration 200/1000 | Loss: 0.00001829
Iteration 201/1000 | Loss: 0.00001829
Iteration 202/1000 | Loss: 0.00001828
Iteration 203/1000 | Loss: 0.00001828
Iteration 204/1000 | Loss: 0.00001828
Iteration 205/1000 | Loss: 0.00001828
Iteration 206/1000 | Loss: 0.00001828
Iteration 207/1000 | Loss: 0.00001828
Iteration 208/1000 | Loss: 0.00001828
Iteration 209/1000 | Loss: 0.00001828
Iteration 210/1000 | Loss: 0.00001828
Iteration 211/1000 | Loss: 0.00001828
Iteration 212/1000 | Loss: 0.00001828
Iteration 213/1000 | Loss: 0.00001828
Iteration 214/1000 | Loss: 0.00001828
Iteration 215/1000 | Loss: 0.00001828
Iteration 216/1000 | Loss: 0.00001827
Iteration 217/1000 | Loss: 0.00001827
Iteration 218/1000 | Loss: 0.00001827
Iteration 219/1000 | Loss: 0.00001827
Iteration 220/1000 | Loss: 0.00001827
Iteration 221/1000 | Loss: 0.00001827
Iteration 222/1000 | Loss: 0.00001827
Iteration 223/1000 | Loss: 0.00001827
Iteration 224/1000 | Loss: 0.00001827
Iteration 225/1000 | Loss: 0.00001827
Iteration 226/1000 | Loss: 0.00001827
Iteration 227/1000 | Loss: 0.00001827
Iteration 228/1000 | Loss: 0.00001827
Iteration 229/1000 | Loss: 0.00001827
Iteration 230/1000 | Loss: 0.00001827
Iteration 231/1000 | Loss: 0.00001827
Iteration 232/1000 | Loss: 0.00001827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.826826701289974e-05, 1.826826701289974e-05, 1.826826701289974e-05, 1.826826701289974e-05, 1.826826701289974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.826826701289974e-05

Optimization complete. Final v2v error: 3.3448634147644043 mm

Highest mean error: 11.81035327911377 mm for frame 45

Lowest mean error: 2.935453414916992 mm for frame 2

Saving results

Total time: 130.21975302696228
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998520
Iteration 2/25 | Loss: 0.00289923
Iteration 3/25 | Loss: 0.00213311
Iteration 4/25 | Loss: 0.00185616
Iteration 5/25 | Loss: 0.00168055
Iteration 6/25 | Loss: 0.00163768
Iteration 7/25 | Loss: 0.00159495
Iteration 8/25 | Loss: 0.00153140
Iteration 9/25 | Loss: 0.00147724
Iteration 10/25 | Loss: 0.00144535
Iteration 11/25 | Loss: 0.00144319
Iteration 12/25 | Loss: 0.00142262
Iteration 13/25 | Loss: 0.00138934
Iteration 14/25 | Loss: 0.00137779
Iteration 15/25 | Loss: 0.00136416
Iteration 16/25 | Loss: 0.00136489
Iteration 17/25 | Loss: 0.00136395
Iteration 18/25 | Loss: 0.00136084
Iteration 19/25 | Loss: 0.00135762
Iteration 20/25 | Loss: 0.00135178
Iteration 21/25 | Loss: 0.00135292
Iteration 22/25 | Loss: 0.00134342
Iteration 23/25 | Loss: 0.00134245
Iteration 24/25 | Loss: 0.00134848
Iteration 25/25 | Loss: 0.00134414

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34869885
Iteration 2/25 | Loss: 0.00392942
Iteration 3/25 | Loss: 0.00314465
Iteration 4/25 | Loss: 0.00314463
Iteration 5/25 | Loss: 0.00314462
Iteration 6/25 | Loss: 0.00314462
Iteration 7/25 | Loss: 0.00314462
Iteration 8/25 | Loss: 0.00314462
Iteration 9/25 | Loss: 0.00314462
Iteration 10/25 | Loss: 0.00314462
Iteration 11/25 | Loss: 0.00314462
Iteration 12/25 | Loss: 0.00314462
Iteration 13/25 | Loss: 0.00314462
Iteration 14/25 | Loss: 0.00314462
Iteration 15/25 | Loss: 0.00314462
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003144621616229415, 0.003144621616229415, 0.003144621616229415, 0.003144621616229415, 0.003144621616229415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003144621616229415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00314462
Iteration 2/1000 | Loss: 0.00054228
Iteration 3/1000 | Loss: 0.00067649
Iteration 4/1000 | Loss: 0.00097896
Iteration 5/1000 | Loss: 0.00108381
Iteration 6/1000 | Loss: 0.00042082
Iteration 7/1000 | Loss: 0.00037553
Iteration 8/1000 | Loss: 0.00065617
Iteration 9/1000 | Loss: 0.00026807
Iteration 10/1000 | Loss: 0.00018937
Iteration 11/1000 | Loss: 0.00016462
Iteration 12/1000 | Loss: 0.00019395
Iteration 13/1000 | Loss: 0.00026897
Iteration 14/1000 | Loss: 0.00098912
Iteration 15/1000 | Loss: 0.00022629
Iteration 16/1000 | Loss: 0.00063784
Iteration 17/1000 | Loss: 0.00046511
Iteration 18/1000 | Loss: 0.00014855
Iteration 19/1000 | Loss: 0.00013830
Iteration 20/1000 | Loss: 0.00015816
Iteration 21/1000 | Loss: 0.00012609
Iteration 22/1000 | Loss: 0.00031564
Iteration 23/1000 | Loss: 0.00026708
Iteration 24/1000 | Loss: 0.00020865
Iteration 25/1000 | Loss: 0.00012605
Iteration 26/1000 | Loss: 0.00012057
Iteration 27/1000 | Loss: 0.00033440
Iteration 28/1000 | Loss: 0.00038861
Iteration 29/1000 | Loss: 0.00036460
Iteration 30/1000 | Loss: 0.00024546
Iteration 31/1000 | Loss: 0.00011954
Iteration 32/1000 | Loss: 0.00011841
Iteration 33/1000 | Loss: 0.00033220
Iteration 34/1000 | Loss: 0.00012468
Iteration 35/1000 | Loss: 0.00012126
Iteration 36/1000 | Loss: 0.00013704
Iteration 37/1000 | Loss: 0.00013860
Iteration 38/1000 | Loss: 0.00012589
Iteration 39/1000 | Loss: 0.00013923
Iteration 40/1000 | Loss: 0.00012253
Iteration 41/1000 | Loss: 0.00013020
Iteration 42/1000 | Loss: 0.00032038
Iteration 43/1000 | Loss: 0.00022422
Iteration 44/1000 | Loss: 0.00014168
Iteration 45/1000 | Loss: 0.00012634
Iteration 46/1000 | Loss: 0.00013476
Iteration 47/1000 | Loss: 0.00012287
Iteration 48/1000 | Loss: 0.00013678
Iteration 49/1000 | Loss: 0.00013317
Iteration 50/1000 | Loss: 0.00023807
Iteration 51/1000 | Loss: 0.00011104
Iteration 52/1000 | Loss: 0.00010214
Iteration 53/1000 | Loss: 0.00011815
Iteration 54/1000 | Loss: 0.00010250
Iteration 55/1000 | Loss: 0.00009882
Iteration 56/1000 | Loss: 0.00010760
Iteration 57/1000 | Loss: 0.00023967
Iteration 58/1000 | Loss: 0.00011788
Iteration 59/1000 | Loss: 0.00010214
Iteration 60/1000 | Loss: 0.00009955
Iteration 61/1000 | Loss: 0.00010372
Iteration 62/1000 | Loss: 0.00009341
Iteration 63/1000 | Loss: 0.00011065
Iteration 64/1000 | Loss: 0.00009208
Iteration 65/1000 | Loss: 0.00010397
Iteration 66/1000 | Loss: 0.00009256
Iteration 67/1000 | Loss: 0.00029668
Iteration 68/1000 | Loss: 0.00027846
Iteration 69/1000 | Loss: 0.00023213
Iteration 70/1000 | Loss: 0.00016851
Iteration 71/1000 | Loss: 0.00017224
Iteration 72/1000 | Loss: 0.00021544
Iteration 73/1000 | Loss: 0.00037372
Iteration 74/1000 | Loss: 0.00022362
Iteration 75/1000 | Loss: 0.00026492
Iteration 76/1000 | Loss: 0.00018643
Iteration 77/1000 | Loss: 0.00010717
Iteration 78/1000 | Loss: 0.00010214
Iteration 79/1000 | Loss: 0.00029119
Iteration 80/1000 | Loss: 0.00047572
Iteration 81/1000 | Loss: 0.00015531
Iteration 82/1000 | Loss: 0.00009567
Iteration 83/1000 | Loss: 0.00010598
Iteration 84/1000 | Loss: 0.00019899
Iteration 85/1000 | Loss: 0.00019431
Iteration 86/1000 | Loss: 0.00129165
Iteration 87/1000 | Loss: 0.00041207
Iteration 88/1000 | Loss: 0.00029386
Iteration 89/1000 | Loss: 0.00015716
Iteration 90/1000 | Loss: 0.00011547
Iteration 91/1000 | Loss: 0.00011725
Iteration 92/1000 | Loss: 0.00015332
Iteration 93/1000 | Loss: 0.00016407
Iteration 94/1000 | Loss: 0.00009243
Iteration 95/1000 | Loss: 0.00009363
Iteration 96/1000 | Loss: 0.00009205
Iteration 97/1000 | Loss: 0.00009790
Iteration 98/1000 | Loss: 0.00010454
Iteration 99/1000 | Loss: 0.00013506
Iteration 100/1000 | Loss: 0.00008318
Iteration 101/1000 | Loss: 0.00010172
Iteration 102/1000 | Loss: 0.00008206
Iteration 103/1000 | Loss: 0.00045129
Iteration 104/1000 | Loss: 0.00083256
Iteration 105/1000 | Loss: 0.00029554
Iteration 106/1000 | Loss: 0.00021168
Iteration 107/1000 | Loss: 0.00008958
Iteration 108/1000 | Loss: 0.00008661
Iteration 109/1000 | Loss: 0.00023639
Iteration 110/1000 | Loss: 0.00013908
Iteration 111/1000 | Loss: 0.00008429
Iteration 112/1000 | Loss: 0.00009337
Iteration 113/1000 | Loss: 0.00008059
Iteration 114/1000 | Loss: 0.00008590
Iteration 115/1000 | Loss: 0.00008796
Iteration 116/1000 | Loss: 0.00008003
Iteration 117/1000 | Loss: 0.00029854
Iteration 118/1000 | Loss: 0.00019129
Iteration 119/1000 | Loss: 0.00027257
Iteration 120/1000 | Loss: 0.00026860
Iteration 121/1000 | Loss: 0.00031201
Iteration 122/1000 | Loss: 0.00043726
Iteration 123/1000 | Loss: 0.00018514
Iteration 124/1000 | Loss: 0.00009131
Iteration 125/1000 | Loss: 0.00008599
Iteration 126/1000 | Loss: 0.00010852
Iteration 127/1000 | Loss: 0.00008055
Iteration 128/1000 | Loss: 0.00017802
Iteration 129/1000 | Loss: 0.00015877
Iteration 130/1000 | Loss: 0.00018709
Iteration 131/1000 | Loss: 0.00023826
Iteration 132/1000 | Loss: 0.00025077
Iteration 133/1000 | Loss: 0.00018845
Iteration 134/1000 | Loss: 0.00011335
Iteration 135/1000 | Loss: 0.00007925
Iteration 136/1000 | Loss: 0.00007742
Iteration 137/1000 | Loss: 0.00007668
Iteration 138/1000 | Loss: 0.00007633
Iteration 139/1000 | Loss: 0.00008315
Iteration 140/1000 | Loss: 0.00007699
Iteration 141/1000 | Loss: 0.00007595
Iteration 142/1000 | Loss: 0.00007595
Iteration 143/1000 | Loss: 0.00007594
Iteration 144/1000 | Loss: 0.00007584
Iteration 145/1000 | Loss: 0.00007790
Iteration 146/1000 | Loss: 0.00007536
Iteration 147/1000 | Loss: 0.00007491
Iteration 148/1000 | Loss: 0.00030994
Iteration 149/1000 | Loss: 0.00008454
Iteration 150/1000 | Loss: 0.00008431
Iteration 151/1000 | Loss: 0.00009966
Iteration 152/1000 | Loss: 0.00007864
Iteration 153/1000 | Loss: 0.00007817
Iteration 154/1000 | Loss: 0.00009271
Iteration 155/1000 | Loss: 0.00009544
Iteration 156/1000 | Loss: 0.00008481
Iteration 157/1000 | Loss: 0.00008958
Iteration 158/1000 | Loss: 0.00008556
Iteration 159/1000 | Loss: 0.00008902
Iteration 160/1000 | Loss: 0.00007600
Iteration 161/1000 | Loss: 0.00007518
Iteration 162/1000 | Loss: 0.00007480
Iteration 163/1000 | Loss: 0.00007442
Iteration 164/1000 | Loss: 0.00007434
Iteration 165/1000 | Loss: 0.00007410
Iteration 166/1000 | Loss: 0.00007409
Iteration 167/1000 | Loss: 0.00007403
Iteration 168/1000 | Loss: 0.00007386
Iteration 169/1000 | Loss: 0.00008038
Iteration 170/1000 | Loss: 0.00007374
Iteration 171/1000 | Loss: 0.00007372
Iteration 172/1000 | Loss: 0.00008144
Iteration 173/1000 | Loss: 0.00008191
Iteration 174/1000 | Loss: 0.00007361
Iteration 175/1000 | Loss: 0.00007352
Iteration 176/1000 | Loss: 0.00007351
Iteration 177/1000 | Loss: 0.00007351
Iteration 178/1000 | Loss: 0.00007351
Iteration 179/1000 | Loss: 0.00007351
Iteration 180/1000 | Loss: 0.00007351
Iteration 181/1000 | Loss: 0.00007351
Iteration 182/1000 | Loss: 0.00007351
Iteration 183/1000 | Loss: 0.00007351
Iteration 184/1000 | Loss: 0.00007351
Iteration 185/1000 | Loss: 0.00007351
Iteration 186/1000 | Loss: 0.00007350
Iteration 187/1000 | Loss: 0.00007350
Iteration 188/1000 | Loss: 0.00007350
Iteration 189/1000 | Loss: 0.00007349
Iteration 190/1000 | Loss: 0.00007349
Iteration 191/1000 | Loss: 0.00007349
Iteration 192/1000 | Loss: 0.00007348
Iteration 193/1000 | Loss: 0.00007346
Iteration 194/1000 | Loss: 0.00007345
Iteration 195/1000 | Loss: 0.00007343
Iteration 196/1000 | Loss: 0.00007342
Iteration 197/1000 | Loss: 0.00007342
Iteration 198/1000 | Loss: 0.00007341
Iteration 199/1000 | Loss: 0.00007341
Iteration 200/1000 | Loss: 0.00007340
Iteration 201/1000 | Loss: 0.00007339
Iteration 202/1000 | Loss: 0.00007337
Iteration 203/1000 | Loss: 0.00007335
Iteration 204/1000 | Loss: 0.00007334
Iteration 205/1000 | Loss: 0.00007334
Iteration 206/1000 | Loss: 0.00007334
Iteration 207/1000 | Loss: 0.00007333
Iteration 208/1000 | Loss: 0.00007333
Iteration 209/1000 | Loss: 0.00007333
Iteration 210/1000 | Loss: 0.00007331
Iteration 211/1000 | Loss: 0.00007328
Iteration 212/1000 | Loss: 0.00007325
Iteration 213/1000 | Loss: 0.00007325
Iteration 214/1000 | Loss: 0.00007325
Iteration 215/1000 | Loss: 0.00007325
Iteration 216/1000 | Loss: 0.00007325
Iteration 217/1000 | Loss: 0.00007324
Iteration 218/1000 | Loss: 0.00007324
Iteration 219/1000 | Loss: 0.00007324
Iteration 220/1000 | Loss: 0.00007324
Iteration 221/1000 | Loss: 0.00007324
Iteration 222/1000 | Loss: 0.00007324
Iteration 223/1000 | Loss: 0.00007324
Iteration 224/1000 | Loss: 0.00007324
Iteration 225/1000 | Loss: 0.00007324
Iteration 226/1000 | Loss: 0.00007323
Iteration 227/1000 | Loss: 0.00007323
Iteration 228/1000 | Loss: 0.00007323
Iteration 229/1000 | Loss: 0.00007323
Iteration 230/1000 | Loss: 0.00007322
Iteration 231/1000 | Loss: 0.00007322
Iteration 232/1000 | Loss: 0.00007322
Iteration 233/1000 | Loss: 0.00007321
Iteration 234/1000 | Loss: 0.00007321
Iteration 235/1000 | Loss: 0.00007321
Iteration 236/1000 | Loss: 0.00007321
Iteration 237/1000 | Loss: 0.00007321
Iteration 238/1000 | Loss: 0.00007321
Iteration 239/1000 | Loss: 0.00007320
Iteration 240/1000 | Loss: 0.00007320
Iteration 241/1000 | Loss: 0.00007320
Iteration 242/1000 | Loss: 0.00007320
Iteration 243/1000 | Loss: 0.00007319
Iteration 244/1000 | Loss: 0.00007319
Iteration 245/1000 | Loss: 0.00007319
Iteration 246/1000 | Loss: 0.00007319
Iteration 247/1000 | Loss: 0.00007319
Iteration 248/1000 | Loss: 0.00007319
Iteration 249/1000 | Loss: 0.00007319
Iteration 250/1000 | Loss: 0.00007319
Iteration 251/1000 | Loss: 0.00007319
Iteration 252/1000 | Loss: 0.00007319
Iteration 253/1000 | Loss: 0.00007318
Iteration 254/1000 | Loss: 0.00007318
Iteration 255/1000 | Loss: 0.00007318
Iteration 256/1000 | Loss: 0.00007318
Iteration 257/1000 | Loss: 0.00007318
Iteration 258/1000 | Loss: 0.00007318
Iteration 259/1000 | Loss: 0.00007318
Iteration 260/1000 | Loss: 0.00007318
Iteration 261/1000 | Loss: 0.00007318
Iteration 262/1000 | Loss: 0.00007318
Iteration 263/1000 | Loss: 0.00007318
Iteration 264/1000 | Loss: 0.00007318
Iteration 265/1000 | Loss: 0.00007318
Iteration 266/1000 | Loss: 0.00007317
Iteration 267/1000 | Loss: 0.00007317
Iteration 268/1000 | Loss: 0.00007317
Iteration 269/1000 | Loss: 0.00007317
Iteration 270/1000 | Loss: 0.00007317
Iteration 271/1000 | Loss: 0.00007317
Iteration 272/1000 | Loss: 0.00007317
Iteration 273/1000 | Loss: 0.00007317
Iteration 274/1000 | Loss: 0.00007317
Iteration 275/1000 | Loss: 0.00007317
Iteration 276/1000 | Loss: 0.00007317
Iteration 277/1000 | Loss: 0.00007317
Iteration 278/1000 | Loss: 0.00007317
Iteration 279/1000 | Loss: 0.00007317
Iteration 280/1000 | Loss: 0.00007317
Iteration 281/1000 | Loss: 0.00007317
Iteration 282/1000 | Loss: 0.00007317
Iteration 283/1000 | Loss: 0.00007316
Iteration 284/1000 | Loss: 0.00007316
Iteration 285/1000 | Loss: 0.00007316
Iteration 286/1000 | Loss: 0.00007316
Iteration 287/1000 | Loss: 0.00007316
Iteration 288/1000 | Loss: 0.00007316
Iteration 289/1000 | Loss: 0.00007316
Iteration 290/1000 | Loss: 0.00007316
Iteration 291/1000 | Loss: 0.00007316
Iteration 292/1000 | Loss: 0.00007316
Iteration 293/1000 | Loss: 0.00007316
Iteration 294/1000 | Loss: 0.00007316
Iteration 295/1000 | Loss: 0.00007315
Iteration 296/1000 | Loss: 0.00007315
Iteration 297/1000 | Loss: 0.00007315
Iteration 298/1000 | Loss: 0.00007315
Iteration 299/1000 | Loss: 0.00007315
Iteration 300/1000 | Loss: 0.00007315
Iteration 301/1000 | Loss: 0.00007315
Iteration 302/1000 | Loss: 0.00007315
Iteration 303/1000 | Loss: 0.00007315
Iteration 304/1000 | Loss: 0.00007314
Iteration 305/1000 | Loss: 0.00007314
Iteration 306/1000 | Loss: 0.00007314
Iteration 307/1000 | Loss: 0.00007314
Iteration 308/1000 | Loss: 0.00007314
Iteration 309/1000 | Loss: 0.00007314
Iteration 310/1000 | Loss: 0.00007314
Iteration 311/1000 | Loss: 0.00007313
Iteration 312/1000 | Loss: 0.00007313
Iteration 313/1000 | Loss: 0.00007313
Iteration 314/1000 | Loss: 0.00007313
Iteration 315/1000 | Loss: 0.00007313
Iteration 316/1000 | Loss: 0.00007313
Iteration 317/1000 | Loss: 0.00007313
Iteration 318/1000 | Loss: 0.00007312
Iteration 319/1000 | Loss: 0.00007312
Iteration 320/1000 | Loss: 0.00007312
Iteration 321/1000 | Loss: 0.00007312
Iteration 322/1000 | Loss: 0.00007312
Iteration 323/1000 | Loss: 0.00007311
Iteration 324/1000 | Loss: 0.00007311
Iteration 325/1000 | Loss: 0.00007311
Iteration 326/1000 | Loss: 0.00007311
Iteration 327/1000 | Loss: 0.00007311
Iteration 328/1000 | Loss: 0.00007311
Iteration 329/1000 | Loss: 0.00007311
Iteration 330/1000 | Loss: 0.00007311
Iteration 331/1000 | Loss: 0.00007311
Iteration 332/1000 | Loss: 0.00007311
Iteration 333/1000 | Loss: 0.00007311
Iteration 334/1000 | Loss: 0.00007311
Iteration 335/1000 | Loss: 0.00007311
Iteration 336/1000 | Loss: 0.00007311
Iteration 337/1000 | Loss: 0.00007311
Iteration 338/1000 | Loss: 0.00007311
Iteration 339/1000 | Loss: 0.00007311
Iteration 340/1000 | Loss: 0.00007311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 340. Stopping optimization.
Last 5 losses: [7.310658838832751e-05, 7.310658838832751e-05, 7.310658838832751e-05, 7.310658838832751e-05, 7.310658838832751e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.310658838832751e-05

Optimization complete. Final v2v error: 4.273995399475098 mm

Highest mean error: 12.74412727355957 mm for frame 185

Lowest mean error: 2.378359079360962 mm for frame 161

Saving results

Total time: 339.9864139556885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011459
Iteration 2/25 | Loss: 0.00214964
Iteration 3/25 | Loss: 0.00225220
Iteration 4/25 | Loss: 0.00154389
Iteration 5/25 | Loss: 0.00145643
Iteration 6/25 | Loss: 0.00161591
Iteration 7/25 | Loss: 0.00137612
Iteration 8/25 | Loss: 0.00130373
Iteration 9/25 | Loss: 0.00125301
Iteration 10/25 | Loss: 0.00125384
Iteration 11/25 | Loss: 0.00122621
Iteration 12/25 | Loss: 0.00120596
Iteration 13/25 | Loss: 0.00118936
Iteration 14/25 | Loss: 0.00119350
Iteration 15/25 | Loss: 0.00118235
Iteration 16/25 | Loss: 0.00117896
Iteration 17/25 | Loss: 0.00117062
Iteration 18/25 | Loss: 0.00116396
Iteration 19/25 | Loss: 0.00116142
Iteration 20/25 | Loss: 0.00116082
Iteration 21/25 | Loss: 0.00116054
Iteration 22/25 | Loss: 0.00116042
Iteration 23/25 | Loss: 0.00116034
Iteration 24/25 | Loss: 0.00116029
Iteration 25/25 | Loss: 0.00116029

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38581586
Iteration 2/25 | Loss: 0.00104420
Iteration 3/25 | Loss: 0.00104419
Iteration 4/25 | Loss: 0.00104419
Iteration 5/25 | Loss: 0.00104419
Iteration 6/25 | Loss: 0.00104419
Iteration 7/25 | Loss: 0.00104419
Iteration 8/25 | Loss: 0.00104419
Iteration 9/25 | Loss: 0.00104419
Iteration 10/25 | Loss: 0.00104419
Iteration 11/25 | Loss: 0.00104419
Iteration 12/25 | Loss: 0.00104419
Iteration 13/25 | Loss: 0.00104419
Iteration 14/25 | Loss: 0.00104419
Iteration 15/25 | Loss: 0.00104419
Iteration 16/25 | Loss: 0.00104419
Iteration 17/25 | Loss: 0.00104419
Iteration 18/25 | Loss: 0.00104419
Iteration 19/25 | Loss: 0.00104419
Iteration 20/25 | Loss: 0.00104419
Iteration 21/25 | Loss: 0.00104419
Iteration 22/25 | Loss: 0.00104419
Iteration 23/25 | Loss: 0.00104419
Iteration 24/25 | Loss: 0.00104419
Iteration 25/25 | Loss: 0.00104419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104419
Iteration 2/1000 | Loss: 0.00004687
Iteration 3/1000 | Loss: 0.00003590
Iteration 4/1000 | Loss: 0.00003044
Iteration 5/1000 | Loss: 0.00002868
Iteration 6/1000 | Loss: 0.00019010
Iteration 7/1000 | Loss: 0.00004876
Iteration 8/1000 | Loss: 0.00003213
Iteration 9/1000 | Loss: 0.00002846
Iteration 10/1000 | Loss: 0.00002649
Iteration 11/1000 | Loss: 0.00002454
Iteration 12/1000 | Loss: 0.00002364
Iteration 13/1000 | Loss: 0.00002302
Iteration 14/1000 | Loss: 0.00002272
Iteration 15/1000 | Loss: 0.00002242
Iteration 16/1000 | Loss: 0.00008336
Iteration 17/1000 | Loss: 0.00002598
Iteration 18/1000 | Loss: 0.00002302
Iteration 19/1000 | Loss: 0.00002156
Iteration 20/1000 | Loss: 0.00002040
Iteration 21/1000 | Loss: 0.00001946
Iteration 22/1000 | Loss: 0.00001889
Iteration 23/1000 | Loss: 0.00001860
Iteration 24/1000 | Loss: 0.00001845
Iteration 25/1000 | Loss: 0.00001841
Iteration 26/1000 | Loss: 0.00001840
Iteration 27/1000 | Loss: 0.00001839
Iteration 28/1000 | Loss: 0.00001838
Iteration 29/1000 | Loss: 0.00001837
Iteration 30/1000 | Loss: 0.00056527
Iteration 31/1000 | Loss: 0.00039151
Iteration 32/1000 | Loss: 0.00002503
Iteration 33/1000 | Loss: 0.00002113
Iteration 34/1000 | Loss: 0.00001938
Iteration 35/1000 | Loss: 0.00001854
Iteration 36/1000 | Loss: 0.00001835
Iteration 37/1000 | Loss: 0.00001831
Iteration 38/1000 | Loss: 0.00050350
Iteration 39/1000 | Loss: 0.00002711
Iteration 40/1000 | Loss: 0.00001992
Iteration 41/1000 | Loss: 0.00001823
Iteration 42/1000 | Loss: 0.00001750
Iteration 43/1000 | Loss: 0.00001690
Iteration 44/1000 | Loss: 0.00001654
Iteration 45/1000 | Loss: 0.00001638
Iteration 46/1000 | Loss: 0.00001624
Iteration 47/1000 | Loss: 0.00001622
Iteration 48/1000 | Loss: 0.00001617
Iteration 49/1000 | Loss: 0.00001610
Iteration 50/1000 | Loss: 0.00001595
Iteration 51/1000 | Loss: 0.00001594
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001585
Iteration 54/1000 | Loss: 0.00001575
Iteration 55/1000 | Loss: 0.00001574
Iteration 56/1000 | Loss: 0.00001573
Iteration 57/1000 | Loss: 0.00001571
Iteration 58/1000 | Loss: 0.00001570
Iteration 59/1000 | Loss: 0.00001566
Iteration 60/1000 | Loss: 0.00001565
Iteration 61/1000 | Loss: 0.00001564
Iteration 62/1000 | Loss: 0.00001564
Iteration 63/1000 | Loss: 0.00001563
Iteration 64/1000 | Loss: 0.00001562
Iteration 65/1000 | Loss: 0.00001562
Iteration 66/1000 | Loss: 0.00001562
Iteration 67/1000 | Loss: 0.00001561
Iteration 68/1000 | Loss: 0.00001561
Iteration 69/1000 | Loss: 0.00001561
Iteration 70/1000 | Loss: 0.00001561
Iteration 71/1000 | Loss: 0.00001561
Iteration 72/1000 | Loss: 0.00001561
Iteration 73/1000 | Loss: 0.00001561
Iteration 74/1000 | Loss: 0.00001561
Iteration 75/1000 | Loss: 0.00001560
Iteration 76/1000 | Loss: 0.00001560
Iteration 77/1000 | Loss: 0.00001560
Iteration 78/1000 | Loss: 0.00001560
Iteration 79/1000 | Loss: 0.00001560
Iteration 80/1000 | Loss: 0.00001560
Iteration 81/1000 | Loss: 0.00001560
Iteration 82/1000 | Loss: 0.00001560
Iteration 83/1000 | Loss: 0.00001560
Iteration 84/1000 | Loss: 0.00001560
Iteration 85/1000 | Loss: 0.00001560
Iteration 86/1000 | Loss: 0.00001559
Iteration 87/1000 | Loss: 0.00001559
Iteration 88/1000 | Loss: 0.00001559
Iteration 89/1000 | Loss: 0.00001559
Iteration 90/1000 | Loss: 0.00001559
Iteration 91/1000 | Loss: 0.00001559
Iteration 92/1000 | Loss: 0.00001559
Iteration 93/1000 | Loss: 0.00001559
Iteration 94/1000 | Loss: 0.00001559
Iteration 95/1000 | Loss: 0.00001559
Iteration 96/1000 | Loss: 0.00001559
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001559
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001558
Iteration 103/1000 | Loss: 0.00001558
Iteration 104/1000 | Loss: 0.00001558
Iteration 105/1000 | Loss: 0.00001558
Iteration 106/1000 | Loss: 0.00001558
Iteration 107/1000 | Loss: 0.00001558
Iteration 108/1000 | Loss: 0.00001557
Iteration 109/1000 | Loss: 0.00001557
Iteration 110/1000 | Loss: 0.00001557
Iteration 111/1000 | Loss: 0.00001557
Iteration 112/1000 | Loss: 0.00001557
Iteration 113/1000 | Loss: 0.00001557
Iteration 114/1000 | Loss: 0.00001557
Iteration 115/1000 | Loss: 0.00001557
Iteration 116/1000 | Loss: 0.00001557
Iteration 117/1000 | Loss: 0.00001557
Iteration 118/1000 | Loss: 0.00001556
Iteration 119/1000 | Loss: 0.00001556
Iteration 120/1000 | Loss: 0.00001556
Iteration 121/1000 | Loss: 0.00001556
Iteration 122/1000 | Loss: 0.00001556
Iteration 123/1000 | Loss: 0.00001556
Iteration 124/1000 | Loss: 0.00001556
Iteration 125/1000 | Loss: 0.00001556
Iteration 126/1000 | Loss: 0.00001556
Iteration 127/1000 | Loss: 0.00001555
Iteration 128/1000 | Loss: 0.00001555
Iteration 129/1000 | Loss: 0.00001555
Iteration 130/1000 | Loss: 0.00001555
Iteration 131/1000 | Loss: 0.00001555
Iteration 132/1000 | Loss: 0.00001555
Iteration 133/1000 | Loss: 0.00001555
Iteration 134/1000 | Loss: 0.00001555
Iteration 135/1000 | Loss: 0.00001555
Iteration 136/1000 | Loss: 0.00001555
Iteration 137/1000 | Loss: 0.00001555
Iteration 138/1000 | Loss: 0.00001555
Iteration 139/1000 | Loss: 0.00001555
Iteration 140/1000 | Loss: 0.00001555
Iteration 141/1000 | Loss: 0.00001554
Iteration 142/1000 | Loss: 0.00001554
Iteration 143/1000 | Loss: 0.00001554
Iteration 144/1000 | Loss: 0.00001554
Iteration 145/1000 | Loss: 0.00001554
Iteration 146/1000 | Loss: 0.00001554
Iteration 147/1000 | Loss: 0.00001554
Iteration 148/1000 | Loss: 0.00001554
Iteration 149/1000 | Loss: 0.00001554
Iteration 150/1000 | Loss: 0.00001554
Iteration 151/1000 | Loss: 0.00001554
Iteration 152/1000 | Loss: 0.00001554
Iteration 153/1000 | Loss: 0.00001554
Iteration 154/1000 | Loss: 0.00001554
Iteration 155/1000 | Loss: 0.00001554
Iteration 156/1000 | Loss: 0.00001553
Iteration 157/1000 | Loss: 0.00001553
Iteration 158/1000 | Loss: 0.00001553
Iteration 159/1000 | Loss: 0.00001553
Iteration 160/1000 | Loss: 0.00001553
Iteration 161/1000 | Loss: 0.00001553
Iteration 162/1000 | Loss: 0.00001553
Iteration 163/1000 | Loss: 0.00001553
Iteration 164/1000 | Loss: 0.00001553
Iteration 165/1000 | Loss: 0.00001553
Iteration 166/1000 | Loss: 0.00001553
Iteration 167/1000 | Loss: 0.00001553
Iteration 168/1000 | Loss: 0.00001553
Iteration 169/1000 | Loss: 0.00001553
Iteration 170/1000 | Loss: 0.00001553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.553265428810846e-05, 1.553265428810846e-05, 1.553265428810846e-05, 1.553265428810846e-05, 1.553265428810846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.553265428810846e-05

Optimization complete. Final v2v error: 3.3092942237854004 mm

Highest mean error: 4.118910312652588 mm for frame 61

Lowest mean error: 2.662726879119873 mm for frame 43

Saving results

Total time: 109.52397298812866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803875
Iteration 2/25 | Loss: 0.00159594
Iteration 3/25 | Loss: 0.00124761
Iteration 4/25 | Loss: 0.00121339
Iteration 5/25 | Loss: 0.00120397
Iteration 6/25 | Loss: 0.00120096
Iteration 7/25 | Loss: 0.00120049
Iteration 8/25 | Loss: 0.00120049
Iteration 9/25 | Loss: 0.00120049
Iteration 10/25 | Loss: 0.00120049
Iteration 11/25 | Loss: 0.00120049
Iteration 12/25 | Loss: 0.00120049
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012004912132397294, 0.0012004912132397294, 0.0012004912132397294, 0.0012004912132397294, 0.0012004912132397294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012004912132397294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09047735
Iteration 2/25 | Loss: 0.00100866
Iteration 3/25 | Loss: 0.00100866
Iteration 4/25 | Loss: 0.00100866
Iteration 5/25 | Loss: 0.00100866
Iteration 6/25 | Loss: 0.00100866
Iteration 7/25 | Loss: 0.00100866
Iteration 8/25 | Loss: 0.00100866
Iteration 9/25 | Loss: 0.00100866
Iteration 10/25 | Loss: 0.00100865
Iteration 11/25 | Loss: 0.00100865
Iteration 12/25 | Loss: 0.00100865
Iteration 13/25 | Loss: 0.00100865
Iteration 14/25 | Loss: 0.00100865
Iteration 15/25 | Loss: 0.00100865
Iteration 16/25 | Loss: 0.00100865
Iteration 17/25 | Loss: 0.00100865
Iteration 18/25 | Loss: 0.00100865
Iteration 19/25 | Loss: 0.00100865
Iteration 20/25 | Loss: 0.00100865
Iteration 21/25 | Loss: 0.00100865
Iteration 22/25 | Loss: 0.00100865
Iteration 23/25 | Loss: 0.00100865
Iteration 24/25 | Loss: 0.00100865
Iteration 25/25 | Loss: 0.00100865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100865
Iteration 2/1000 | Loss: 0.00008888
Iteration 3/1000 | Loss: 0.00005032
Iteration 4/1000 | Loss: 0.00003934
Iteration 5/1000 | Loss: 0.00003671
Iteration 6/1000 | Loss: 0.00003505
Iteration 7/1000 | Loss: 0.00003404
Iteration 8/1000 | Loss: 0.00003322
Iteration 9/1000 | Loss: 0.00003248
Iteration 10/1000 | Loss: 0.00003203
Iteration 11/1000 | Loss: 0.00003162
Iteration 12/1000 | Loss: 0.00003123
Iteration 13/1000 | Loss: 0.00003094
Iteration 14/1000 | Loss: 0.00003075
Iteration 15/1000 | Loss: 0.00003055
Iteration 16/1000 | Loss: 0.00003038
Iteration 17/1000 | Loss: 0.00003031
Iteration 18/1000 | Loss: 0.00003029
Iteration 19/1000 | Loss: 0.00003029
Iteration 20/1000 | Loss: 0.00003023
Iteration 21/1000 | Loss: 0.00003019
Iteration 22/1000 | Loss: 0.00003019
Iteration 23/1000 | Loss: 0.00003018
Iteration 24/1000 | Loss: 0.00003017
Iteration 25/1000 | Loss: 0.00003014
Iteration 26/1000 | Loss: 0.00003014
Iteration 27/1000 | Loss: 0.00003014
Iteration 28/1000 | Loss: 0.00003013
Iteration 29/1000 | Loss: 0.00003010
Iteration 30/1000 | Loss: 0.00003007
Iteration 31/1000 | Loss: 0.00003007
Iteration 32/1000 | Loss: 0.00003006
Iteration 33/1000 | Loss: 0.00003005
Iteration 34/1000 | Loss: 0.00003004
Iteration 35/1000 | Loss: 0.00003004
Iteration 36/1000 | Loss: 0.00003004
Iteration 37/1000 | Loss: 0.00003004
Iteration 38/1000 | Loss: 0.00003004
Iteration 39/1000 | Loss: 0.00003004
Iteration 40/1000 | Loss: 0.00003004
Iteration 41/1000 | Loss: 0.00003003
Iteration 42/1000 | Loss: 0.00003003
Iteration 43/1000 | Loss: 0.00003003
Iteration 44/1000 | Loss: 0.00003003
Iteration 45/1000 | Loss: 0.00003003
Iteration 46/1000 | Loss: 0.00003003
Iteration 47/1000 | Loss: 0.00003002
Iteration 48/1000 | Loss: 0.00003002
Iteration 49/1000 | Loss: 0.00003002
Iteration 50/1000 | Loss: 0.00003001
Iteration 51/1000 | Loss: 0.00003001
Iteration 52/1000 | Loss: 0.00003001
Iteration 53/1000 | Loss: 0.00003001
Iteration 54/1000 | Loss: 0.00003001
Iteration 55/1000 | Loss: 0.00003001
Iteration 56/1000 | Loss: 0.00003001
Iteration 57/1000 | Loss: 0.00003001
Iteration 58/1000 | Loss: 0.00003001
Iteration 59/1000 | Loss: 0.00003000
Iteration 60/1000 | Loss: 0.00003000
Iteration 61/1000 | Loss: 0.00003000
Iteration 62/1000 | Loss: 0.00003000
Iteration 63/1000 | Loss: 0.00003000
Iteration 64/1000 | Loss: 0.00003000
Iteration 65/1000 | Loss: 0.00003000
Iteration 66/1000 | Loss: 0.00003000
Iteration 67/1000 | Loss: 0.00002999
Iteration 68/1000 | Loss: 0.00002999
Iteration 69/1000 | Loss: 0.00002999
Iteration 70/1000 | Loss: 0.00002999
Iteration 71/1000 | Loss: 0.00002999
Iteration 72/1000 | Loss: 0.00002999
Iteration 73/1000 | Loss: 0.00002999
Iteration 74/1000 | Loss: 0.00002998
Iteration 75/1000 | Loss: 0.00002998
Iteration 76/1000 | Loss: 0.00002998
Iteration 77/1000 | Loss: 0.00002998
Iteration 78/1000 | Loss: 0.00002997
Iteration 79/1000 | Loss: 0.00002997
Iteration 80/1000 | Loss: 0.00002997
Iteration 81/1000 | Loss: 0.00002997
Iteration 82/1000 | Loss: 0.00002997
Iteration 83/1000 | Loss: 0.00002996
Iteration 84/1000 | Loss: 0.00002996
Iteration 85/1000 | Loss: 0.00002996
Iteration 86/1000 | Loss: 0.00002996
Iteration 87/1000 | Loss: 0.00002996
Iteration 88/1000 | Loss: 0.00002996
Iteration 89/1000 | Loss: 0.00002995
Iteration 90/1000 | Loss: 0.00002995
Iteration 91/1000 | Loss: 0.00002995
Iteration 92/1000 | Loss: 0.00002995
Iteration 93/1000 | Loss: 0.00002995
Iteration 94/1000 | Loss: 0.00002994
Iteration 95/1000 | Loss: 0.00002994
Iteration 96/1000 | Loss: 0.00002994
Iteration 97/1000 | Loss: 0.00002993
Iteration 98/1000 | Loss: 0.00002993
Iteration 99/1000 | Loss: 0.00002993
Iteration 100/1000 | Loss: 0.00002993
Iteration 101/1000 | Loss: 0.00002992
Iteration 102/1000 | Loss: 0.00002992
Iteration 103/1000 | Loss: 0.00002992
Iteration 104/1000 | Loss: 0.00002992
Iteration 105/1000 | Loss: 0.00002992
Iteration 106/1000 | Loss: 0.00002991
Iteration 107/1000 | Loss: 0.00002991
Iteration 108/1000 | Loss: 0.00002991
Iteration 109/1000 | Loss: 0.00002991
Iteration 110/1000 | Loss: 0.00002990
Iteration 111/1000 | Loss: 0.00002990
Iteration 112/1000 | Loss: 0.00002990
Iteration 113/1000 | Loss: 0.00002989
Iteration 114/1000 | Loss: 0.00002989
Iteration 115/1000 | Loss: 0.00002989
Iteration 116/1000 | Loss: 0.00002989
Iteration 117/1000 | Loss: 0.00002989
Iteration 118/1000 | Loss: 0.00002988
Iteration 119/1000 | Loss: 0.00002988
Iteration 120/1000 | Loss: 0.00002988
Iteration 121/1000 | Loss: 0.00002988
Iteration 122/1000 | Loss: 0.00002988
Iteration 123/1000 | Loss: 0.00002988
Iteration 124/1000 | Loss: 0.00002988
Iteration 125/1000 | Loss: 0.00002987
Iteration 126/1000 | Loss: 0.00002987
Iteration 127/1000 | Loss: 0.00002987
Iteration 128/1000 | Loss: 0.00002987
Iteration 129/1000 | Loss: 0.00002987
Iteration 130/1000 | Loss: 0.00002987
Iteration 131/1000 | Loss: 0.00002987
Iteration 132/1000 | Loss: 0.00002986
Iteration 133/1000 | Loss: 0.00002986
Iteration 134/1000 | Loss: 0.00002986
Iteration 135/1000 | Loss: 0.00002986
Iteration 136/1000 | Loss: 0.00002986
Iteration 137/1000 | Loss: 0.00002986
Iteration 138/1000 | Loss: 0.00002986
Iteration 139/1000 | Loss: 0.00002986
Iteration 140/1000 | Loss: 0.00002986
Iteration 141/1000 | Loss: 0.00002986
Iteration 142/1000 | Loss: 0.00002986
Iteration 143/1000 | Loss: 0.00002986
Iteration 144/1000 | Loss: 0.00002986
Iteration 145/1000 | Loss: 0.00002985
Iteration 146/1000 | Loss: 0.00002985
Iteration 147/1000 | Loss: 0.00002985
Iteration 148/1000 | Loss: 0.00002985
Iteration 149/1000 | Loss: 0.00002985
Iteration 150/1000 | Loss: 0.00002985
Iteration 151/1000 | Loss: 0.00002985
Iteration 152/1000 | Loss: 0.00002985
Iteration 153/1000 | Loss: 0.00002985
Iteration 154/1000 | Loss: 0.00002984
Iteration 155/1000 | Loss: 0.00002984
Iteration 156/1000 | Loss: 0.00002984
Iteration 157/1000 | Loss: 0.00002984
Iteration 158/1000 | Loss: 0.00002984
Iteration 159/1000 | Loss: 0.00002984
Iteration 160/1000 | Loss: 0.00002984
Iteration 161/1000 | Loss: 0.00002984
Iteration 162/1000 | Loss: 0.00002984
Iteration 163/1000 | Loss: 0.00002984
Iteration 164/1000 | Loss: 0.00002983
Iteration 165/1000 | Loss: 0.00002983
Iteration 166/1000 | Loss: 0.00002983
Iteration 167/1000 | Loss: 0.00002983
Iteration 168/1000 | Loss: 0.00002983
Iteration 169/1000 | Loss: 0.00002983
Iteration 170/1000 | Loss: 0.00002983
Iteration 171/1000 | Loss: 0.00002983
Iteration 172/1000 | Loss: 0.00002983
Iteration 173/1000 | Loss: 0.00002983
Iteration 174/1000 | Loss: 0.00002983
Iteration 175/1000 | Loss: 0.00002983
Iteration 176/1000 | Loss: 0.00002983
Iteration 177/1000 | Loss: 0.00002983
Iteration 178/1000 | Loss: 0.00002983
Iteration 179/1000 | Loss: 0.00002983
Iteration 180/1000 | Loss: 0.00002983
Iteration 181/1000 | Loss: 0.00002983
Iteration 182/1000 | Loss: 0.00002983
Iteration 183/1000 | Loss: 0.00002983
Iteration 184/1000 | Loss: 0.00002983
Iteration 185/1000 | Loss: 0.00002983
Iteration 186/1000 | Loss: 0.00002983
Iteration 187/1000 | Loss: 0.00002983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.983075683005154e-05, 2.983075683005154e-05, 2.983075683005154e-05, 2.983075683005154e-05, 2.983075683005154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.983075683005154e-05

Optimization complete. Final v2v error: 4.3793158531188965 mm

Highest mean error: 5.080985069274902 mm for frame 146

Lowest mean error: 3.7713193893432617 mm for frame 123

Saving results

Total time: 46.56358623504639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00710594
Iteration 2/25 | Loss: 0.00218396
Iteration 3/25 | Loss: 0.00149495
Iteration 4/25 | Loss: 0.00135114
Iteration 5/25 | Loss: 0.00125409
Iteration 6/25 | Loss: 0.00125211
Iteration 7/25 | Loss: 0.00118565
Iteration 8/25 | Loss: 0.00113542
Iteration 9/25 | Loss: 0.00112578
Iteration 10/25 | Loss: 0.00109736
Iteration 11/25 | Loss: 0.00110123
Iteration 12/25 | Loss: 0.00107784
Iteration 13/25 | Loss: 0.00108223
Iteration 14/25 | Loss: 0.00108664
Iteration 15/25 | Loss: 0.00107412
Iteration 16/25 | Loss: 0.00107790
Iteration 17/25 | Loss: 0.00107362
Iteration 18/25 | Loss: 0.00107349
Iteration 19/25 | Loss: 0.00107335
Iteration 20/25 | Loss: 0.00107320
Iteration 21/25 | Loss: 0.00107310
Iteration 22/25 | Loss: 0.00107303
Iteration 23/25 | Loss: 0.00107302
Iteration 24/25 | Loss: 0.00107302
Iteration 25/25 | Loss: 0.00107302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67613029
Iteration 2/25 | Loss: 0.00084534
Iteration 3/25 | Loss: 0.00084534
Iteration 4/25 | Loss: 0.00084534
Iteration 5/25 | Loss: 0.00084534
Iteration 6/25 | Loss: 0.00084534
Iteration 7/25 | Loss: 0.00084534
Iteration 8/25 | Loss: 0.00084534
Iteration 9/25 | Loss: 0.00084534
Iteration 10/25 | Loss: 0.00084534
Iteration 11/25 | Loss: 0.00084533
Iteration 12/25 | Loss: 0.00084533
Iteration 13/25 | Loss: 0.00084533
Iteration 14/25 | Loss: 0.00084533
Iteration 15/25 | Loss: 0.00084533
Iteration 16/25 | Loss: 0.00084533
Iteration 17/25 | Loss: 0.00084533
Iteration 18/25 | Loss: 0.00084533
Iteration 19/25 | Loss: 0.00084533
Iteration 20/25 | Loss: 0.00084533
Iteration 21/25 | Loss: 0.00084533
Iteration 22/25 | Loss: 0.00084533
Iteration 23/25 | Loss: 0.00084533
Iteration 24/25 | Loss: 0.00084533
Iteration 25/25 | Loss: 0.00084533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084533
Iteration 2/1000 | Loss: 0.00002764
Iteration 3/1000 | Loss: 0.00004209
Iteration 4/1000 | Loss: 0.00001822
Iteration 5/1000 | Loss: 0.00003423
Iteration 6/1000 | Loss: 0.00001693
Iteration 7/1000 | Loss: 0.00001646
Iteration 8/1000 | Loss: 0.00001623
Iteration 9/1000 | Loss: 0.00001599
Iteration 10/1000 | Loss: 0.00001582
Iteration 11/1000 | Loss: 0.00001564
Iteration 12/1000 | Loss: 0.00046119
Iteration 13/1000 | Loss: 0.00002130
Iteration 14/1000 | Loss: 0.00001682
Iteration 15/1000 | Loss: 0.00001535
Iteration 16/1000 | Loss: 0.00001483
Iteration 17/1000 | Loss: 0.00001440
Iteration 18/1000 | Loss: 0.00001408
Iteration 19/1000 | Loss: 0.00001408
Iteration 20/1000 | Loss: 0.00001399
Iteration 21/1000 | Loss: 0.00001394
Iteration 22/1000 | Loss: 0.00001393
Iteration 23/1000 | Loss: 0.00001390
Iteration 24/1000 | Loss: 0.00001390
Iteration 25/1000 | Loss: 0.00001389
Iteration 26/1000 | Loss: 0.00001388
Iteration 27/1000 | Loss: 0.00001387
Iteration 28/1000 | Loss: 0.00001385
Iteration 29/1000 | Loss: 0.00001384
Iteration 30/1000 | Loss: 0.00001384
Iteration 31/1000 | Loss: 0.00001381
Iteration 32/1000 | Loss: 0.00001380
Iteration 33/1000 | Loss: 0.00001380
Iteration 34/1000 | Loss: 0.00001380
Iteration 35/1000 | Loss: 0.00001379
Iteration 36/1000 | Loss: 0.00001379
Iteration 37/1000 | Loss: 0.00001378
Iteration 38/1000 | Loss: 0.00001377
Iteration 39/1000 | Loss: 0.00001377
Iteration 40/1000 | Loss: 0.00001375
Iteration 41/1000 | Loss: 0.00001374
Iteration 42/1000 | Loss: 0.00001373
Iteration 43/1000 | Loss: 0.00001373
Iteration 44/1000 | Loss: 0.00001372
Iteration 45/1000 | Loss: 0.00001367
Iteration 46/1000 | Loss: 0.00001367
Iteration 47/1000 | Loss: 0.00001362
Iteration 48/1000 | Loss: 0.00001361
Iteration 49/1000 | Loss: 0.00001361
Iteration 50/1000 | Loss: 0.00001361
Iteration 51/1000 | Loss: 0.00001361
Iteration 52/1000 | Loss: 0.00001361
Iteration 53/1000 | Loss: 0.00001361
Iteration 54/1000 | Loss: 0.00001361
Iteration 55/1000 | Loss: 0.00001360
Iteration 56/1000 | Loss: 0.00001360
Iteration 57/1000 | Loss: 0.00001359
Iteration 58/1000 | Loss: 0.00001359
Iteration 59/1000 | Loss: 0.00001359
Iteration 60/1000 | Loss: 0.00001358
Iteration 61/1000 | Loss: 0.00001358
Iteration 62/1000 | Loss: 0.00001358
Iteration 63/1000 | Loss: 0.00001358
Iteration 64/1000 | Loss: 0.00001358
Iteration 65/1000 | Loss: 0.00001358
Iteration 66/1000 | Loss: 0.00001358
Iteration 67/1000 | Loss: 0.00001357
Iteration 68/1000 | Loss: 0.00001357
Iteration 69/1000 | Loss: 0.00001357
Iteration 70/1000 | Loss: 0.00001357
Iteration 71/1000 | Loss: 0.00001357
Iteration 72/1000 | Loss: 0.00001356
Iteration 73/1000 | Loss: 0.00001356
Iteration 74/1000 | Loss: 0.00001356
Iteration 75/1000 | Loss: 0.00001356
Iteration 76/1000 | Loss: 0.00001355
Iteration 77/1000 | Loss: 0.00001355
Iteration 78/1000 | Loss: 0.00001355
Iteration 79/1000 | Loss: 0.00001354
Iteration 80/1000 | Loss: 0.00001354
Iteration 81/1000 | Loss: 0.00001354
Iteration 82/1000 | Loss: 0.00001354
Iteration 83/1000 | Loss: 0.00001353
Iteration 84/1000 | Loss: 0.00001353
Iteration 85/1000 | Loss: 0.00001353
Iteration 86/1000 | Loss: 0.00001353
Iteration 87/1000 | Loss: 0.00001353
Iteration 88/1000 | Loss: 0.00001353
Iteration 89/1000 | Loss: 0.00001353
Iteration 90/1000 | Loss: 0.00001353
Iteration 91/1000 | Loss: 0.00001353
Iteration 92/1000 | Loss: 0.00001353
Iteration 93/1000 | Loss: 0.00001353
Iteration 94/1000 | Loss: 0.00001353
Iteration 95/1000 | Loss: 0.00001353
Iteration 96/1000 | Loss: 0.00001353
Iteration 97/1000 | Loss: 0.00001353
Iteration 98/1000 | Loss: 0.00001353
Iteration 99/1000 | Loss: 0.00001353
Iteration 100/1000 | Loss: 0.00001353
Iteration 101/1000 | Loss: 0.00001353
Iteration 102/1000 | Loss: 0.00001353
Iteration 103/1000 | Loss: 0.00001353
Iteration 104/1000 | Loss: 0.00001353
Iteration 105/1000 | Loss: 0.00001353
Iteration 106/1000 | Loss: 0.00001353
Iteration 107/1000 | Loss: 0.00001353
Iteration 108/1000 | Loss: 0.00001353
Iteration 109/1000 | Loss: 0.00001353
Iteration 110/1000 | Loss: 0.00001353
Iteration 111/1000 | Loss: 0.00001353
Iteration 112/1000 | Loss: 0.00001353
Iteration 113/1000 | Loss: 0.00001353
Iteration 114/1000 | Loss: 0.00001353
Iteration 115/1000 | Loss: 0.00001353
Iteration 116/1000 | Loss: 0.00001353
Iteration 117/1000 | Loss: 0.00001353
Iteration 118/1000 | Loss: 0.00001353
Iteration 119/1000 | Loss: 0.00001353
Iteration 120/1000 | Loss: 0.00001353
Iteration 121/1000 | Loss: 0.00001353
Iteration 122/1000 | Loss: 0.00001353
Iteration 123/1000 | Loss: 0.00001353
Iteration 124/1000 | Loss: 0.00001353
Iteration 125/1000 | Loss: 0.00001353
Iteration 126/1000 | Loss: 0.00001353
Iteration 127/1000 | Loss: 0.00001353
Iteration 128/1000 | Loss: 0.00001353
Iteration 129/1000 | Loss: 0.00001353
Iteration 130/1000 | Loss: 0.00001353
Iteration 131/1000 | Loss: 0.00001353
Iteration 132/1000 | Loss: 0.00001353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.3530523574445397e-05, 1.3530523574445397e-05, 1.3530523574445397e-05, 1.3530523574445397e-05, 1.3530523574445397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3530523574445397e-05

Optimization complete. Final v2v error: 2.6987855434417725 mm

Highest mean error: 20.06805419921875 mm for frame 132

Lowest mean error: 2.3839023113250732 mm for frame 180

Saving results

Total time: 76.44434714317322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764559
Iteration 2/25 | Loss: 0.00141864
Iteration 3/25 | Loss: 0.00122664
Iteration 4/25 | Loss: 0.00120652
Iteration 5/25 | Loss: 0.00120228
Iteration 6/25 | Loss: 0.00120213
Iteration 7/25 | Loss: 0.00120213
Iteration 8/25 | Loss: 0.00120204
Iteration 9/25 | Loss: 0.00120204
Iteration 10/25 | Loss: 0.00120204
Iteration 11/25 | Loss: 0.00120204
Iteration 12/25 | Loss: 0.00120204
Iteration 13/25 | Loss: 0.00120204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012020365102216601, 0.0012020365102216601, 0.0012020365102216601, 0.0012020365102216601, 0.0012020365102216601]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012020365102216601

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21433055
Iteration 2/25 | Loss: 0.00068250
Iteration 3/25 | Loss: 0.00068247
Iteration 4/25 | Loss: 0.00068247
Iteration 5/25 | Loss: 0.00068247
Iteration 6/25 | Loss: 0.00068247
Iteration 7/25 | Loss: 0.00068247
Iteration 8/25 | Loss: 0.00068247
Iteration 9/25 | Loss: 0.00068247
Iteration 10/25 | Loss: 0.00068247
Iteration 11/25 | Loss: 0.00068247
Iteration 12/25 | Loss: 0.00068247
Iteration 13/25 | Loss: 0.00068247
Iteration 14/25 | Loss: 0.00068247
Iteration 15/25 | Loss: 0.00068247
Iteration 16/25 | Loss: 0.00068247
Iteration 17/25 | Loss: 0.00068247
Iteration 18/25 | Loss: 0.00068247
Iteration 19/25 | Loss: 0.00068247
Iteration 20/25 | Loss: 0.00068247
Iteration 21/25 | Loss: 0.00068247
Iteration 22/25 | Loss: 0.00068247
Iteration 23/25 | Loss: 0.00068247
Iteration 24/25 | Loss: 0.00068247
Iteration 25/25 | Loss: 0.00068247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068247
Iteration 2/1000 | Loss: 0.00003734
Iteration 3/1000 | Loss: 0.00002412
Iteration 4/1000 | Loss: 0.00002127
Iteration 5/1000 | Loss: 0.00002002
Iteration 6/1000 | Loss: 0.00001903
Iteration 7/1000 | Loss: 0.00001839
Iteration 8/1000 | Loss: 0.00001801
Iteration 9/1000 | Loss: 0.00001771
Iteration 10/1000 | Loss: 0.00001744
Iteration 11/1000 | Loss: 0.00001729
Iteration 12/1000 | Loss: 0.00001722
Iteration 13/1000 | Loss: 0.00001713
Iteration 14/1000 | Loss: 0.00001704
Iteration 15/1000 | Loss: 0.00001702
Iteration 16/1000 | Loss: 0.00001702
Iteration 17/1000 | Loss: 0.00001701
Iteration 18/1000 | Loss: 0.00001700
Iteration 19/1000 | Loss: 0.00001699
Iteration 20/1000 | Loss: 0.00001699
Iteration 21/1000 | Loss: 0.00001698
Iteration 22/1000 | Loss: 0.00001698
Iteration 23/1000 | Loss: 0.00001697
Iteration 24/1000 | Loss: 0.00001696
Iteration 25/1000 | Loss: 0.00001696
Iteration 26/1000 | Loss: 0.00001695
Iteration 27/1000 | Loss: 0.00001695
Iteration 28/1000 | Loss: 0.00001695
Iteration 29/1000 | Loss: 0.00001694
Iteration 30/1000 | Loss: 0.00001694
Iteration 31/1000 | Loss: 0.00001694
Iteration 32/1000 | Loss: 0.00001693
Iteration 33/1000 | Loss: 0.00001693
Iteration 34/1000 | Loss: 0.00001693
Iteration 35/1000 | Loss: 0.00001693
Iteration 36/1000 | Loss: 0.00001692
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001691
Iteration 40/1000 | Loss: 0.00001691
Iteration 41/1000 | Loss: 0.00001691
Iteration 42/1000 | Loss: 0.00001691
Iteration 43/1000 | Loss: 0.00001690
Iteration 44/1000 | Loss: 0.00001690
Iteration 45/1000 | Loss: 0.00001690
Iteration 46/1000 | Loss: 0.00001690
Iteration 47/1000 | Loss: 0.00001690
Iteration 48/1000 | Loss: 0.00001690
Iteration 49/1000 | Loss: 0.00001690
Iteration 50/1000 | Loss: 0.00001687
Iteration 51/1000 | Loss: 0.00001684
Iteration 52/1000 | Loss: 0.00001682
Iteration 53/1000 | Loss: 0.00001681
Iteration 54/1000 | Loss: 0.00001681
Iteration 55/1000 | Loss: 0.00001681
Iteration 56/1000 | Loss: 0.00001681
Iteration 57/1000 | Loss: 0.00001680
Iteration 58/1000 | Loss: 0.00001680
Iteration 59/1000 | Loss: 0.00001680
Iteration 60/1000 | Loss: 0.00001678
Iteration 61/1000 | Loss: 0.00001677
Iteration 62/1000 | Loss: 0.00001676
Iteration 63/1000 | Loss: 0.00001675
Iteration 64/1000 | Loss: 0.00001675
Iteration 65/1000 | Loss: 0.00001675
Iteration 66/1000 | Loss: 0.00001675
Iteration 67/1000 | Loss: 0.00001674
Iteration 68/1000 | Loss: 0.00001674
Iteration 69/1000 | Loss: 0.00001674
Iteration 70/1000 | Loss: 0.00001674
Iteration 71/1000 | Loss: 0.00001674
Iteration 72/1000 | Loss: 0.00001674
Iteration 73/1000 | Loss: 0.00001674
Iteration 74/1000 | Loss: 0.00001674
Iteration 75/1000 | Loss: 0.00001674
Iteration 76/1000 | Loss: 0.00001674
Iteration 77/1000 | Loss: 0.00001674
Iteration 78/1000 | Loss: 0.00001673
Iteration 79/1000 | Loss: 0.00001673
Iteration 80/1000 | Loss: 0.00001672
Iteration 81/1000 | Loss: 0.00001671
Iteration 82/1000 | Loss: 0.00001670
Iteration 83/1000 | Loss: 0.00001670
Iteration 84/1000 | Loss: 0.00001669
Iteration 85/1000 | Loss: 0.00001669
Iteration 86/1000 | Loss: 0.00001669
Iteration 87/1000 | Loss: 0.00001669
Iteration 88/1000 | Loss: 0.00001669
Iteration 89/1000 | Loss: 0.00001669
Iteration 90/1000 | Loss: 0.00001668
Iteration 91/1000 | Loss: 0.00001668
Iteration 92/1000 | Loss: 0.00001667
Iteration 93/1000 | Loss: 0.00001667
Iteration 94/1000 | Loss: 0.00001667
Iteration 95/1000 | Loss: 0.00001667
Iteration 96/1000 | Loss: 0.00001666
Iteration 97/1000 | Loss: 0.00001666
Iteration 98/1000 | Loss: 0.00001665
Iteration 99/1000 | Loss: 0.00001663
Iteration 100/1000 | Loss: 0.00001663
Iteration 101/1000 | Loss: 0.00001663
Iteration 102/1000 | Loss: 0.00001663
Iteration 103/1000 | Loss: 0.00001662
Iteration 104/1000 | Loss: 0.00001662
Iteration 105/1000 | Loss: 0.00001662
Iteration 106/1000 | Loss: 0.00001662
Iteration 107/1000 | Loss: 0.00001661
Iteration 108/1000 | Loss: 0.00001661
Iteration 109/1000 | Loss: 0.00001660
Iteration 110/1000 | Loss: 0.00001660
Iteration 111/1000 | Loss: 0.00001659
Iteration 112/1000 | Loss: 0.00001659
Iteration 113/1000 | Loss: 0.00001659
Iteration 114/1000 | Loss: 0.00001659
Iteration 115/1000 | Loss: 0.00001658
Iteration 116/1000 | Loss: 0.00001658
Iteration 117/1000 | Loss: 0.00001658
Iteration 118/1000 | Loss: 0.00001658
Iteration 119/1000 | Loss: 0.00001658
Iteration 120/1000 | Loss: 0.00001658
Iteration 121/1000 | Loss: 0.00001658
Iteration 122/1000 | Loss: 0.00001658
Iteration 123/1000 | Loss: 0.00001658
Iteration 124/1000 | Loss: 0.00001658
Iteration 125/1000 | Loss: 0.00001658
Iteration 126/1000 | Loss: 0.00001658
Iteration 127/1000 | Loss: 0.00001657
Iteration 128/1000 | Loss: 0.00001657
Iteration 129/1000 | Loss: 0.00001657
Iteration 130/1000 | Loss: 0.00001657
Iteration 131/1000 | Loss: 0.00001657
Iteration 132/1000 | Loss: 0.00001657
Iteration 133/1000 | Loss: 0.00001656
Iteration 134/1000 | Loss: 0.00001656
Iteration 135/1000 | Loss: 0.00001656
Iteration 136/1000 | Loss: 0.00001656
Iteration 137/1000 | Loss: 0.00001656
Iteration 138/1000 | Loss: 0.00001656
Iteration 139/1000 | Loss: 0.00001656
Iteration 140/1000 | Loss: 0.00001656
Iteration 141/1000 | Loss: 0.00001656
Iteration 142/1000 | Loss: 0.00001656
Iteration 143/1000 | Loss: 0.00001655
Iteration 144/1000 | Loss: 0.00001655
Iteration 145/1000 | Loss: 0.00001655
Iteration 146/1000 | Loss: 0.00001655
Iteration 147/1000 | Loss: 0.00001655
Iteration 148/1000 | Loss: 0.00001655
Iteration 149/1000 | Loss: 0.00001655
Iteration 150/1000 | Loss: 0.00001655
Iteration 151/1000 | Loss: 0.00001655
Iteration 152/1000 | Loss: 0.00001654
Iteration 153/1000 | Loss: 0.00001654
Iteration 154/1000 | Loss: 0.00001654
Iteration 155/1000 | Loss: 0.00001654
Iteration 156/1000 | Loss: 0.00001653
Iteration 157/1000 | Loss: 0.00001653
Iteration 158/1000 | Loss: 0.00001653
Iteration 159/1000 | Loss: 0.00001653
Iteration 160/1000 | Loss: 0.00001652
Iteration 161/1000 | Loss: 0.00001652
Iteration 162/1000 | Loss: 0.00001652
Iteration 163/1000 | Loss: 0.00001652
Iteration 164/1000 | Loss: 0.00001652
Iteration 165/1000 | Loss: 0.00001652
Iteration 166/1000 | Loss: 0.00001652
Iteration 167/1000 | Loss: 0.00001652
Iteration 168/1000 | Loss: 0.00001652
Iteration 169/1000 | Loss: 0.00001652
Iteration 170/1000 | Loss: 0.00001652
Iteration 171/1000 | Loss: 0.00001652
Iteration 172/1000 | Loss: 0.00001652
Iteration 173/1000 | Loss: 0.00001652
Iteration 174/1000 | Loss: 0.00001651
Iteration 175/1000 | Loss: 0.00001651
Iteration 176/1000 | Loss: 0.00001651
Iteration 177/1000 | Loss: 0.00001651
Iteration 178/1000 | Loss: 0.00001651
Iteration 179/1000 | Loss: 0.00001651
Iteration 180/1000 | Loss: 0.00001651
Iteration 181/1000 | Loss: 0.00001651
Iteration 182/1000 | Loss: 0.00001651
Iteration 183/1000 | Loss: 0.00001651
Iteration 184/1000 | Loss: 0.00001651
Iteration 185/1000 | Loss: 0.00001651
Iteration 186/1000 | Loss: 0.00001651
Iteration 187/1000 | Loss: 0.00001651
Iteration 188/1000 | Loss: 0.00001651
Iteration 189/1000 | Loss: 0.00001651
Iteration 190/1000 | Loss: 0.00001651
Iteration 191/1000 | Loss: 0.00001651
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.6510964996996336e-05, 1.6510964996996336e-05, 1.6510964996996336e-05, 1.6510964996996336e-05, 1.6510964996996336e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6510964996996336e-05

Optimization complete. Final v2v error: 3.3129818439483643 mm

Highest mean error: 4.349239349365234 mm for frame 221

Lowest mean error: 2.789949417114258 mm for frame 102

Saving results

Total time: 45.53179931640625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434348
Iteration 2/25 | Loss: 0.00115477
Iteration 3/25 | Loss: 0.00109946
Iteration 4/25 | Loss: 0.00109400
Iteration 5/25 | Loss: 0.00109186
Iteration 6/25 | Loss: 0.00109182
Iteration 7/25 | Loss: 0.00109182
Iteration 8/25 | Loss: 0.00109182
Iteration 9/25 | Loss: 0.00109182
Iteration 10/25 | Loss: 0.00109182
Iteration 11/25 | Loss: 0.00109182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010918228654190898, 0.0010918228654190898, 0.0010918228654190898, 0.0010918228654190898, 0.0010918228654190898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010918228654190898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99867177
Iteration 2/25 | Loss: 0.00057170
Iteration 3/25 | Loss: 0.00057169
Iteration 4/25 | Loss: 0.00057169
Iteration 5/25 | Loss: 0.00057169
Iteration 6/25 | Loss: 0.00057169
Iteration 7/25 | Loss: 0.00057169
Iteration 8/25 | Loss: 0.00057169
Iteration 9/25 | Loss: 0.00057169
Iteration 10/25 | Loss: 0.00057169
Iteration 11/25 | Loss: 0.00057169
Iteration 12/25 | Loss: 0.00057169
Iteration 13/25 | Loss: 0.00057169
Iteration 14/25 | Loss: 0.00057169
Iteration 15/25 | Loss: 0.00057169
Iteration 16/25 | Loss: 0.00057169
Iteration 17/25 | Loss: 0.00057169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005716896266676486, 0.0005716896266676486, 0.0005716896266676486, 0.0005716896266676486, 0.0005716896266676486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005716896266676486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057169
Iteration 2/1000 | Loss: 0.00003647
Iteration 3/1000 | Loss: 0.00002079
Iteration 4/1000 | Loss: 0.00001668
Iteration 5/1000 | Loss: 0.00001549
Iteration 6/1000 | Loss: 0.00001499
Iteration 7/1000 | Loss: 0.00001421
Iteration 8/1000 | Loss: 0.00001380
Iteration 9/1000 | Loss: 0.00001336
Iteration 10/1000 | Loss: 0.00001329
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001290
Iteration 13/1000 | Loss: 0.00001278
Iteration 14/1000 | Loss: 0.00001278
Iteration 15/1000 | Loss: 0.00001268
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001268
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001268
Iteration 20/1000 | Loss: 0.00001268
Iteration 21/1000 | Loss: 0.00001268
Iteration 22/1000 | Loss: 0.00001268
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00001268
Iteration 25/1000 | Loss: 0.00001250
Iteration 26/1000 | Loss: 0.00001242
Iteration 27/1000 | Loss: 0.00001242
Iteration 28/1000 | Loss: 0.00001242
Iteration 29/1000 | Loss: 0.00001241
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001241
Iteration 32/1000 | Loss: 0.00001241
Iteration 33/1000 | Loss: 0.00001241
Iteration 34/1000 | Loss: 0.00001241
Iteration 35/1000 | Loss: 0.00001241
Iteration 36/1000 | Loss: 0.00001241
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001241
Iteration 39/1000 | Loss: 0.00001239
Iteration 40/1000 | Loss: 0.00001239
Iteration 41/1000 | Loss: 0.00001239
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001237
Iteration 47/1000 | Loss: 0.00001237
Iteration 48/1000 | Loss: 0.00001237
Iteration 49/1000 | Loss: 0.00001237
Iteration 50/1000 | Loss: 0.00001236
Iteration 51/1000 | Loss: 0.00001236
Iteration 52/1000 | Loss: 0.00001235
Iteration 53/1000 | Loss: 0.00001235
Iteration 54/1000 | Loss: 0.00001235
Iteration 55/1000 | Loss: 0.00001234
Iteration 56/1000 | Loss: 0.00001234
Iteration 57/1000 | Loss: 0.00001234
Iteration 58/1000 | Loss: 0.00001234
Iteration 59/1000 | Loss: 0.00001234
Iteration 60/1000 | Loss: 0.00001234
Iteration 61/1000 | Loss: 0.00001233
Iteration 62/1000 | Loss: 0.00001233
Iteration 63/1000 | Loss: 0.00001233
Iteration 64/1000 | Loss: 0.00001233
Iteration 65/1000 | Loss: 0.00001233
Iteration 66/1000 | Loss: 0.00001233
Iteration 67/1000 | Loss: 0.00001232
Iteration 68/1000 | Loss: 0.00001232
Iteration 69/1000 | Loss: 0.00001232
Iteration 70/1000 | Loss: 0.00001232
Iteration 71/1000 | Loss: 0.00001232
Iteration 72/1000 | Loss: 0.00001232
Iteration 73/1000 | Loss: 0.00001232
Iteration 74/1000 | Loss: 0.00001232
Iteration 75/1000 | Loss: 0.00001231
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001231
Iteration 78/1000 | Loss: 0.00001230
Iteration 79/1000 | Loss: 0.00001230
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001230
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001230
Iteration 85/1000 | Loss: 0.00001230
Iteration 86/1000 | Loss: 0.00001230
Iteration 87/1000 | Loss: 0.00001230
Iteration 88/1000 | Loss: 0.00001230
Iteration 89/1000 | Loss: 0.00001230
Iteration 90/1000 | Loss: 0.00001230
Iteration 91/1000 | Loss: 0.00001230
Iteration 92/1000 | Loss: 0.00001230
Iteration 93/1000 | Loss: 0.00001230
Iteration 94/1000 | Loss: 0.00001229
Iteration 95/1000 | Loss: 0.00001229
Iteration 96/1000 | Loss: 0.00001228
Iteration 97/1000 | Loss: 0.00001228
Iteration 98/1000 | Loss: 0.00001228
Iteration 99/1000 | Loss: 0.00001228
Iteration 100/1000 | Loss: 0.00001228
Iteration 101/1000 | Loss: 0.00001228
Iteration 102/1000 | Loss: 0.00001228
Iteration 103/1000 | Loss: 0.00001228
Iteration 104/1000 | Loss: 0.00001227
Iteration 105/1000 | Loss: 0.00001227
Iteration 106/1000 | Loss: 0.00001226
Iteration 107/1000 | Loss: 0.00001226
Iteration 108/1000 | Loss: 0.00001226
Iteration 109/1000 | Loss: 0.00001225
Iteration 110/1000 | Loss: 0.00001225
Iteration 111/1000 | Loss: 0.00001224
Iteration 112/1000 | Loss: 0.00001224
Iteration 113/1000 | Loss: 0.00001223
Iteration 114/1000 | Loss: 0.00001223
Iteration 115/1000 | Loss: 0.00001222
Iteration 116/1000 | Loss: 0.00001222
Iteration 117/1000 | Loss: 0.00001222
Iteration 118/1000 | Loss: 0.00001222
Iteration 119/1000 | Loss: 0.00001222
Iteration 120/1000 | Loss: 0.00001222
Iteration 121/1000 | Loss: 0.00001222
Iteration 122/1000 | Loss: 0.00001222
Iteration 123/1000 | Loss: 0.00001222
Iteration 124/1000 | Loss: 0.00001222
Iteration 125/1000 | Loss: 0.00001222
Iteration 126/1000 | Loss: 0.00001222
Iteration 127/1000 | Loss: 0.00001221
Iteration 128/1000 | Loss: 0.00001221
Iteration 129/1000 | Loss: 0.00001221
Iteration 130/1000 | Loss: 0.00001221
Iteration 131/1000 | Loss: 0.00001221
Iteration 132/1000 | Loss: 0.00001221
Iteration 133/1000 | Loss: 0.00001221
Iteration 134/1000 | Loss: 0.00001221
Iteration 135/1000 | Loss: 0.00001221
Iteration 136/1000 | Loss: 0.00001221
Iteration 137/1000 | Loss: 0.00001221
Iteration 138/1000 | Loss: 0.00001221
Iteration 139/1000 | Loss: 0.00001221
Iteration 140/1000 | Loss: 0.00001221
Iteration 141/1000 | Loss: 0.00001221
Iteration 142/1000 | Loss: 0.00001221
Iteration 143/1000 | Loss: 0.00001221
Iteration 144/1000 | Loss: 0.00001221
Iteration 145/1000 | Loss: 0.00001221
Iteration 146/1000 | Loss: 0.00001221
Iteration 147/1000 | Loss: 0.00001221
Iteration 148/1000 | Loss: 0.00001221
Iteration 149/1000 | Loss: 0.00001221
Iteration 150/1000 | Loss: 0.00001221
Iteration 151/1000 | Loss: 0.00001221
Iteration 152/1000 | Loss: 0.00001221
Iteration 153/1000 | Loss: 0.00001221
Iteration 154/1000 | Loss: 0.00001221
Iteration 155/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.2212502042530105e-05, 1.2212502042530105e-05, 1.2212502042530105e-05, 1.2212502042530105e-05, 1.2212502042530105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2212502042530105e-05

Optimization complete. Final v2v error: 3.0058133602142334 mm

Highest mean error: 3.095637321472168 mm for frame 10

Lowest mean error: 2.9703903198242188 mm for frame 72

Saving results

Total time: 34.627285957336426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005534
Iteration 2/25 | Loss: 0.01005534
Iteration 3/25 | Loss: 0.00341271
Iteration 4/25 | Loss: 0.00228901
Iteration 5/25 | Loss: 0.00189340
Iteration 6/25 | Loss: 0.00160474
Iteration 7/25 | Loss: 0.00139345
Iteration 8/25 | Loss: 0.00130395
Iteration 9/25 | Loss: 0.00126297
Iteration 10/25 | Loss: 0.00124770
Iteration 11/25 | Loss: 0.00122818
Iteration 12/25 | Loss: 0.00121204
Iteration 13/25 | Loss: 0.00120466
Iteration 14/25 | Loss: 0.00120205
Iteration 15/25 | Loss: 0.00120124
Iteration 16/25 | Loss: 0.00120096
Iteration 17/25 | Loss: 0.00120089
Iteration 18/25 | Loss: 0.00120089
Iteration 19/25 | Loss: 0.00120088
Iteration 20/25 | Loss: 0.00120088
Iteration 21/25 | Loss: 0.00120088
Iteration 22/25 | Loss: 0.00120088
Iteration 23/25 | Loss: 0.00120088
Iteration 24/25 | Loss: 0.00120088
Iteration 25/25 | Loss: 0.00120088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31785381
Iteration 2/25 | Loss: 0.00102180
Iteration 3/25 | Loss: 0.00102180
Iteration 4/25 | Loss: 0.00102180
Iteration 5/25 | Loss: 0.00102180
Iteration 6/25 | Loss: 0.00102180
Iteration 7/25 | Loss: 0.00102180
Iteration 8/25 | Loss: 0.00102180
Iteration 9/25 | Loss: 0.00102180
Iteration 10/25 | Loss: 0.00102180
Iteration 11/25 | Loss: 0.00102180
Iteration 12/25 | Loss: 0.00102180
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010218005627393723, 0.0010218005627393723, 0.0010218005627393723, 0.0010218005627393723, 0.0010218005627393723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010218005627393723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102180
Iteration 2/1000 | Loss: 0.00031036
Iteration 3/1000 | Loss: 0.00004686
Iteration 4/1000 | Loss: 0.00003885
Iteration 5/1000 | Loss: 0.00003230
Iteration 6/1000 | Loss: 0.00002908
Iteration 7/1000 | Loss: 0.00002775
Iteration 8/1000 | Loss: 0.00002671
Iteration 9/1000 | Loss: 0.00002595
Iteration 10/1000 | Loss: 0.00002515
Iteration 11/1000 | Loss: 0.00002445
Iteration 12/1000 | Loss: 0.00002399
Iteration 13/1000 | Loss: 0.00002369
Iteration 14/1000 | Loss: 0.00002349
Iteration 15/1000 | Loss: 0.00002347
Iteration 16/1000 | Loss: 0.00002343
Iteration 17/1000 | Loss: 0.00002343
Iteration 18/1000 | Loss: 0.00002339
Iteration 19/1000 | Loss: 0.00002330
Iteration 20/1000 | Loss: 0.00002327
Iteration 21/1000 | Loss: 0.00002327
Iteration 22/1000 | Loss: 0.00002326
Iteration 23/1000 | Loss: 0.00002326
Iteration 24/1000 | Loss: 0.00002324
Iteration 25/1000 | Loss: 0.00002323
Iteration 26/1000 | Loss: 0.00002322
Iteration 27/1000 | Loss: 0.00002322
Iteration 28/1000 | Loss: 0.00002321
Iteration 29/1000 | Loss: 0.00002319
Iteration 30/1000 | Loss: 0.00002319
Iteration 31/1000 | Loss: 0.00002318
Iteration 32/1000 | Loss: 0.00002318
Iteration 33/1000 | Loss: 0.00002318
Iteration 34/1000 | Loss: 0.00002317
Iteration 35/1000 | Loss: 0.00002317
Iteration 36/1000 | Loss: 0.00002317
Iteration 37/1000 | Loss: 0.00002317
Iteration 38/1000 | Loss: 0.00002316
Iteration 39/1000 | Loss: 0.00002316
Iteration 40/1000 | Loss: 0.00002316
Iteration 41/1000 | Loss: 0.00002316
Iteration 42/1000 | Loss: 0.00002316
Iteration 43/1000 | Loss: 0.00002315
Iteration 44/1000 | Loss: 0.00002315
Iteration 45/1000 | Loss: 0.00002315
Iteration 46/1000 | Loss: 0.00002314
Iteration 47/1000 | Loss: 0.00002314
Iteration 48/1000 | Loss: 0.00002313
Iteration 49/1000 | Loss: 0.00002313
Iteration 50/1000 | Loss: 0.00002313
Iteration 51/1000 | Loss: 0.00002312
Iteration 52/1000 | Loss: 0.00002311
Iteration 53/1000 | Loss: 0.00002311
Iteration 54/1000 | Loss: 0.00002310
Iteration 55/1000 | Loss: 0.00002310
Iteration 56/1000 | Loss: 0.00002310
Iteration 57/1000 | Loss: 0.00002309
Iteration 58/1000 | Loss: 0.00002309
Iteration 59/1000 | Loss: 0.00002309
Iteration 60/1000 | Loss: 0.00002308
Iteration 61/1000 | Loss: 0.00002308
Iteration 62/1000 | Loss: 0.00002308
Iteration 63/1000 | Loss: 0.00002307
Iteration 64/1000 | Loss: 0.00002307
Iteration 65/1000 | Loss: 0.00002307
Iteration 66/1000 | Loss: 0.00002307
Iteration 67/1000 | Loss: 0.00002306
Iteration 68/1000 | Loss: 0.00002306
Iteration 69/1000 | Loss: 0.00002306
Iteration 70/1000 | Loss: 0.00002305
Iteration 71/1000 | Loss: 0.00002305
Iteration 72/1000 | Loss: 0.00002305
Iteration 73/1000 | Loss: 0.00002305
Iteration 74/1000 | Loss: 0.00002305
Iteration 75/1000 | Loss: 0.00002304
Iteration 76/1000 | Loss: 0.00002304
Iteration 77/1000 | Loss: 0.00002304
Iteration 78/1000 | Loss: 0.00002304
Iteration 79/1000 | Loss: 0.00002304
Iteration 80/1000 | Loss: 0.00002304
Iteration 81/1000 | Loss: 0.00002303
Iteration 82/1000 | Loss: 0.00002303
Iteration 83/1000 | Loss: 0.00002302
Iteration 84/1000 | Loss: 0.00002301
Iteration 85/1000 | Loss: 0.00002300
Iteration 86/1000 | Loss: 0.00002300
Iteration 87/1000 | Loss: 0.00002300
Iteration 88/1000 | Loss: 0.00002300
Iteration 89/1000 | Loss: 0.00002300
Iteration 90/1000 | Loss: 0.00002299
Iteration 91/1000 | Loss: 0.00002299
Iteration 92/1000 | Loss: 0.00002298
Iteration 93/1000 | Loss: 0.00002298
Iteration 94/1000 | Loss: 0.00002298
Iteration 95/1000 | Loss: 0.00002297
Iteration 96/1000 | Loss: 0.00002297
Iteration 97/1000 | Loss: 0.00002296
Iteration 98/1000 | Loss: 0.00002296
Iteration 99/1000 | Loss: 0.00002296
Iteration 100/1000 | Loss: 0.00002295
Iteration 101/1000 | Loss: 0.00002295
Iteration 102/1000 | Loss: 0.00002294
Iteration 103/1000 | Loss: 0.00002294
Iteration 104/1000 | Loss: 0.00002294
Iteration 105/1000 | Loss: 0.00002293
Iteration 106/1000 | Loss: 0.00002293
Iteration 107/1000 | Loss: 0.00002293
Iteration 108/1000 | Loss: 0.00002292
Iteration 109/1000 | Loss: 0.00002292
Iteration 110/1000 | Loss: 0.00002292
Iteration 111/1000 | Loss: 0.00002292
Iteration 112/1000 | Loss: 0.00002291
Iteration 113/1000 | Loss: 0.00002291
Iteration 114/1000 | Loss: 0.00002290
Iteration 115/1000 | Loss: 0.00002289
Iteration 116/1000 | Loss: 0.00002289
Iteration 117/1000 | Loss: 0.00002288
Iteration 118/1000 | Loss: 0.00002288
Iteration 119/1000 | Loss: 0.00002287
Iteration 120/1000 | Loss: 0.00002287
Iteration 121/1000 | Loss: 0.00002287
Iteration 122/1000 | Loss: 0.00002286
Iteration 123/1000 | Loss: 0.00002286
Iteration 124/1000 | Loss: 0.00002286
Iteration 125/1000 | Loss: 0.00002285
Iteration 126/1000 | Loss: 0.00002285
Iteration 127/1000 | Loss: 0.00002285
Iteration 128/1000 | Loss: 0.00002285
Iteration 129/1000 | Loss: 0.00002285
Iteration 130/1000 | Loss: 0.00002285
Iteration 131/1000 | Loss: 0.00002285
Iteration 132/1000 | Loss: 0.00002285
Iteration 133/1000 | Loss: 0.00002284
Iteration 134/1000 | Loss: 0.00002284
Iteration 135/1000 | Loss: 0.00002284
Iteration 136/1000 | Loss: 0.00002284
Iteration 137/1000 | Loss: 0.00002284
Iteration 138/1000 | Loss: 0.00002283
Iteration 139/1000 | Loss: 0.00002283
Iteration 140/1000 | Loss: 0.00002283
Iteration 141/1000 | Loss: 0.00002283
Iteration 142/1000 | Loss: 0.00002282
Iteration 143/1000 | Loss: 0.00002282
Iteration 144/1000 | Loss: 0.00002282
Iteration 145/1000 | Loss: 0.00002282
Iteration 146/1000 | Loss: 0.00002282
Iteration 147/1000 | Loss: 0.00002282
Iteration 148/1000 | Loss: 0.00002282
Iteration 149/1000 | Loss: 0.00002281
Iteration 150/1000 | Loss: 0.00002281
Iteration 151/1000 | Loss: 0.00002281
Iteration 152/1000 | Loss: 0.00002281
Iteration 153/1000 | Loss: 0.00002281
Iteration 154/1000 | Loss: 0.00002281
Iteration 155/1000 | Loss: 0.00002281
Iteration 156/1000 | Loss: 0.00002281
Iteration 157/1000 | Loss: 0.00002281
Iteration 158/1000 | Loss: 0.00002281
Iteration 159/1000 | Loss: 0.00002281
Iteration 160/1000 | Loss: 0.00002281
Iteration 161/1000 | Loss: 0.00002281
Iteration 162/1000 | Loss: 0.00002280
Iteration 163/1000 | Loss: 0.00002280
Iteration 164/1000 | Loss: 0.00002280
Iteration 165/1000 | Loss: 0.00002280
Iteration 166/1000 | Loss: 0.00002280
Iteration 167/1000 | Loss: 0.00002280
Iteration 168/1000 | Loss: 0.00002280
Iteration 169/1000 | Loss: 0.00002280
Iteration 170/1000 | Loss: 0.00002280
Iteration 171/1000 | Loss: 0.00002280
Iteration 172/1000 | Loss: 0.00002280
Iteration 173/1000 | Loss: 0.00002280
Iteration 174/1000 | Loss: 0.00002280
Iteration 175/1000 | Loss: 0.00002280
Iteration 176/1000 | Loss: 0.00002280
Iteration 177/1000 | Loss: 0.00002280
Iteration 178/1000 | Loss: 0.00002279
Iteration 179/1000 | Loss: 0.00002279
Iteration 180/1000 | Loss: 0.00002279
Iteration 181/1000 | Loss: 0.00002279
Iteration 182/1000 | Loss: 0.00002279
Iteration 183/1000 | Loss: 0.00002279
Iteration 184/1000 | Loss: 0.00002279
Iteration 185/1000 | Loss: 0.00002279
Iteration 186/1000 | Loss: 0.00002279
Iteration 187/1000 | Loss: 0.00002279
Iteration 188/1000 | Loss: 0.00002279
Iteration 189/1000 | Loss: 0.00002279
Iteration 190/1000 | Loss: 0.00002279
Iteration 191/1000 | Loss: 0.00002279
Iteration 192/1000 | Loss: 0.00002279
Iteration 193/1000 | Loss: 0.00002279
Iteration 194/1000 | Loss: 0.00002279
Iteration 195/1000 | Loss: 0.00002278
Iteration 196/1000 | Loss: 0.00002278
Iteration 197/1000 | Loss: 0.00002278
Iteration 198/1000 | Loss: 0.00002278
Iteration 199/1000 | Loss: 0.00002278
Iteration 200/1000 | Loss: 0.00002278
Iteration 201/1000 | Loss: 0.00002278
Iteration 202/1000 | Loss: 0.00002278
Iteration 203/1000 | Loss: 0.00002278
Iteration 204/1000 | Loss: 0.00002278
Iteration 205/1000 | Loss: 0.00002278
Iteration 206/1000 | Loss: 0.00002278
Iteration 207/1000 | Loss: 0.00002278
Iteration 208/1000 | Loss: 0.00002277
Iteration 209/1000 | Loss: 0.00002277
Iteration 210/1000 | Loss: 0.00002277
Iteration 211/1000 | Loss: 0.00002277
Iteration 212/1000 | Loss: 0.00002277
Iteration 213/1000 | Loss: 0.00002277
Iteration 214/1000 | Loss: 0.00002277
Iteration 215/1000 | Loss: 0.00002277
Iteration 216/1000 | Loss: 0.00002277
Iteration 217/1000 | Loss: 0.00002277
Iteration 218/1000 | Loss: 0.00002277
Iteration 219/1000 | Loss: 0.00002276
Iteration 220/1000 | Loss: 0.00002276
Iteration 221/1000 | Loss: 0.00002276
Iteration 222/1000 | Loss: 0.00002276
Iteration 223/1000 | Loss: 0.00002276
Iteration 224/1000 | Loss: 0.00002276
Iteration 225/1000 | Loss: 0.00002276
Iteration 226/1000 | Loss: 0.00002276
Iteration 227/1000 | Loss: 0.00002276
Iteration 228/1000 | Loss: 0.00002276
Iteration 229/1000 | Loss: 0.00002276
Iteration 230/1000 | Loss: 0.00002276
Iteration 231/1000 | Loss: 0.00002276
Iteration 232/1000 | Loss: 0.00002276
Iteration 233/1000 | Loss: 0.00002276
Iteration 234/1000 | Loss: 0.00002276
Iteration 235/1000 | Loss: 0.00002276
Iteration 236/1000 | Loss: 0.00002276
Iteration 237/1000 | Loss: 0.00002276
Iteration 238/1000 | Loss: 0.00002276
Iteration 239/1000 | Loss: 0.00002276
Iteration 240/1000 | Loss: 0.00002276
Iteration 241/1000 | Loss: 0.00002276
Iteration 242/1000 | Loss: 0.00002276
Iteration 243/1000 | Loss: 0.00002276
Iteration 244/1000 | Loss: 0.00002276
Iteration 245/1000 | Loss: 0.00002276
Iteration 246/1000 | Loss: 0.00002276
Iteration 247/1000 | Loss: 0.00002276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [2.276193117722869e-05, 2.276193117722869e-05, 2.276193117722869e-05, 2.276193117722869e-05, 2.276193117722869e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.276193117722869e-05

Optimization complete. Final v2v error: 4.097709655761719 mm

Highest mean error: 4.465198993682861 mm for frame 153

Lowest mean error: 3.6927998065948486 mm for frame 148

Saving results

Total time: 75.64517879486084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006686
Iteration 2/25 | Loss: 0.01006686
Iteration 3/25 | Loss: 0.00389459
Iteration 4/25 | Loss: 0.00218186
Iteration 5/25 | Loss: 0.00181664
Iteration 6/25 | Loss: 0.00164908
Iteration 7/25 | Loss: 0.00152571
Iteration 8/25 | Loss: 0.00143958
Iteration 9/25 | Loss: 0.00136391
Iteration 10/25 | Loss: 0.00131694
Iteration 11/25 | Loss: 0.00130371
Iteration 12/25 | Loss: 0.00129921
Iteration 13/25 | Loss: 0.00128809
Iteration 14/25 | Loss: 0.00128917
Iteration 15/25 | Loss: 0.00127350
Iteration 16/25 | Loss: 0.00127000
Iteration 17/25 | Loss: 0.00127034
Iteration 18/25 | Loss: 0.00126849
Iteration 19/25 | Loss: 0.00126407
Iteration 20/25 | Loss: 0.00126181
Iteration 21/25 | Loss: 0.00125972
Iteration 22/25 | Loss: 0.00126216
Iteration 23/25 | Loss: 0.00126174
Iteration 24/25 | Loss: 0.00126165
Iteration 25/25 | Loss: 0.00125900

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33499026
Iteration 2/25 | Loss: 0.00198549
Iteration 3/25 | Loss: 0.00198549
Iteration 4/25 | Loss: 0.00171040
Iteration 5/25 | Loss: 0.00171039
Iteration 6/25 | Loss: 0.00171039
Iteration 7/25 | Loss: 0.00171039
Iteration 8/25 | Loss: 0.00171039
Iteration 9/25 | Loss: 0.00171039
Iteration 10/25 | Loss: 0.00171039
Iteration 11/25 | Loss: 0.00171039
Iteration 12/25 | Loss: 0.00171039
Iteration 13/25 | Loss: 0.00171039
Iteration 14/25 | Loss: 0.00171039
Iteration 15/25 | Loss: 0.00171039
Iteration 16/25 | Loss: 0.00171039
Iteration 17/25 | Loss: 0.00171039
Iteration 18/25 | Loss: 0.00171039
Iteration 19/25 | Loss: 0.00171039
Iteration 20/25 | Loss: 0.00171039
Iteration 21/25 | Loss: 0.00171039
Iteration 22/25 | Loss: 0.00171039
Iteration 23/25 | Loss: 0.00171039
Iteration 24/25 | Loss: 0.00171039
Iteration 25/25 | Loss: 0.00171039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171039
Iteration 2/1000 | Loss: 0.00050029
Iteration 3/1000 | Loss: 0.00021610
Iteration 4/1000 | Loss: 0.00013678
Iteration 5/1000 | Loss: 0.00020011
Iteration 6/1000 | Loss: 0.00025377
Iteration 7/1000 | Loss: 0.00011603
Iteration 8/1000 | Loss: 0.00011140
Iteration 9/1000 | Loss: 0.00013723
Iteration 10/1000 | Loss: 0.00054395
Iteration 11/1000 | Loss: 0.00012321
Iteration 12/1000 | Loss: 0.00014806
Iteration 13/1000 | Loss: 0.00016788
Iteration 14/1000 | Loss: 0.00024272
Iteration 15/1000 | Loss: 0.00011923
Iteration 16/1000 | Loss: 0.00014829
Iteration 17/1000 | Loss: 0.00015607
Iteration 18/1000 | Loss: 0.00012292
Iteration 19/1000 | Loss: 0.00083954
Iteration 20/1000 | Loss: 0.00237461
Iteration 21/1000 | Loss: 0.00260552
Iteration 22/1000 | Loss: 0.00111056
Iteration 23/1000 | Loss: 0.00291515
Iteration 24/1000 | Loss: 0.00067866
Iteration 25/1000 | Loss: 0.00016240
Iteration 26/1000 | Loss: 0.00019407
Iteration 27/1000 | Loss: 0.00024366
Iteration 28/1000 | Loss: 0.00007828
Iteration 29/1000 | Loss: 0.00005052
Iteration 30/1000 | Loss: 0.00003943
Iteration 31/1000 | Loss: 0.00003436
Iteration 32/1000 | Loss: 0.00002865
Iteration 33/1000 | Loss: 0.00015112
Iteration 34/1000 | Loss: 0.00002283
Iteration 35/1000 | Loss: 0.00004000
Iteration 36/1000 | Loss: 0.00004866
Iteration 37/1000 | Loss: 0.00001922
Iteration 38/1000 | Loss: 0.00002645
Iteration 39/1000 | Loss: 0.00001687
Iteration 40/1000 | Loss: 0.00002834
Iteration 41/1000 | Loss: 0.00002839
Iteration 42/1000 | Loss: 0.00039392
Iteration 43/1000 | Loss: 0.00002178
Iteration 44/1000 | Loss: 0.00007969
Iteration 45/1000 | Loss: 0.00001581
Iteration 46/1000 | Loss: 0.00001925
Iteration 47/1000 | Loss: 0.00004093
Iteration 48/1000 | Loss: 0.00002342
Iteration 49/1000 | Loss: 0.00001634
Iteration 50/1000 | Loss: 0.00001743
Iteration 51/1000 | Loss: 0.00001416
Iteration 52/1000 | Loss: 0.00001641
Iteration 53/1000 | Loss: 0.00001876
Iteration 54/1000 | Loss: 0.00002778
Iteration 55/1000 | Loss: 0.00002778
Iteration 56/1000 | Loss: 0.00002146
Iteration 57/1000 | Loss: 0.00001500
Iteration 58/1000 | Loss: 0.00001419
Iteration 59/1000 | Loss: 0.00001386
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001385
Iteration 63/1000 | Loss: 0.00001385
Iteration 64/1000 | Loss: 0.00001385
Iteration 65/1000 | Loss: 0.00001385
Iteration 66/1000 | Loss: 0.00001385
Iteration 67/1000 | Loss: 0.00001385
Iteration 68/1000 | Loss: 0.00001385
Iteration 69/1000 | Loss: 0.00001385
Iteration 70/1000 | Loss: 0.00001385
Iteration 71/1000 | Loss: 0.00001389
Iteration 72/1000 | Loss: 0.00001389
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001383
Iteration 75/1000 | Loss: 0.00001383
Iteration 76/1000 | Loss: 0.00001383
Iteration 77/1000 | Loss: 0.00001383
Iteration 78/1000 | Loss: 0.00001383
Iteration 79/1000 | Loss: 0.00001383
Iteration 80/1000 | Loss: 0.00001383
Iteration 81/1000 | Loss: 0.00001383
Iteration 82/1000 | Loss: 0.00001383
Iteration 83/1000 | Loss: 0.00001383
Iteration 84/1000 | Loss: 0.00001383
Iteration 85/1000 | Loss: 0.00001382
Iteration 86/1000 | Loss: 0.00001382
Iteration 87/1000 | Loss: 0.00001382
Iteration 88/1000 | Loss: 0.00001382
Iteration 89/1000 | Loss: 0.00001382
Iteration 90/1000 | Loss: 0.00001382
Iteration 91/1000 | Loss: 0.00001381
Iteration 92/1000 | Loss: 0.00001381
Iteration 93/1000 | Loss: 0.00001381
Iteration 94/1000 | Loss: 0.00001400
Iteration 95/1000 | Loss: 0.00001380
Iteration 96/1000 | Loss: 0.00001380
Iteration 97/1000 | Loss: 0.00001379
Iteration 98/1000 | Loss: 0.00001379
Iteration 99/1000 | Loss: 0.00001379
Iteration 100/1000 | Loss: 0.00001379
Iteration 101/1000 | Loss: 0.00001379
Iteration 102/1000 | Loss: 0.00001379
Iteration 103/1000 | Loss: 0.00001379
Iteration 104/1000 | Loss: 0.00001379
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001378
Iteration 107/1000 | Loss: 0.00001378
Iteration 108/1000 | Loss: 0.00001576
Iteration 109/1000 | Loss: 0.00003016
Iteration 110/1000 | Loss: 0.00002436
Iteration 111/1000 | Loss: 0.00002817
Iteration 112/1000 | Loss: 0.00002375
Iteration 113/1000 | Loss: 0.00002784
Iteration 114/1000 | Loss: 0.00002394
Iteration 115/1000 | Loss: 0.00002621
Iteration 116/1000 | Loss: 0.00003635
Iteration 117/1000 | Loss: 0.00002218
Iteration 118/1000 | Loss: 0.00002930
Iteration 119/1000 | Loss: 0.00002672
Iteration 120/1000 | Loss: 0.00002111
Iteration 121/1000 | Loss: 0.00002721
Iteration 122/1000 | Loss: 0.00001826
Iteration 123/1000 | Loss: 0.00002771
Iteration 124/1000 | Loss: 0.00001533
Iteration 125/1000 | Loss: 0.00002668
Iteration 126/1000 | Loss: 0.00001564
Iteration 127/1000 | Loss: 0.00002407
Iteration 128/1000 | Loss: 0.00002217
Iteration 129/1000 | Loss: 0.00002290
Iteration 130/1000 | Loss: 0.00002094
Iteration 131/1000 | Loss: 0.00002094
Iteration 132/1000 | Loss: 0.00001553
Iteration 133/1000 | Loss: 0.00001414
Iteration 134/1000 | Loss: 0.00002087
Iteration 135/1000 | Loss: 0.00001524
Iteration 136/1000 | Loss: 0.00008098
Iteration 137/1000 | Loss: 0.00001706
Iteration 138/1000 | Loss: 0.00001395
Iteration 139/1000 | Loss: 0.00001777
Iteration 140/1000 | Loss: 0.00001364
Iteration 141/1000 | Loss: 0.00001364
Iteration 142/1000 | Loss: 0.00001363
Iteration 143/1000 | Loss: 0.00001363
Iteration 144/1000 | Loss: 0.00001363
Iteration 145/1000 | Loss: 0.00001363
Iteration 146/1000 | Loss: 0.00001363
Iteration 147/1000 | Loss: 0.00001363
Iteration 148/1000 | Loss: 0.00001363
Iteration 149/1000 | Loss: 0.00001363
Iteration 150/1000 | Loss: 0.00001363
Iteration 151/1000 | Loss: 0.00001363
Iteration 152/1000 | Loss: 0.00001363
Iteration 153/1000 | Loss: 0.00001363
Iteration 154/1000 | Loss: 0.00001363
Iteration 155/1000 | Loss: 0.00001363
Iteration 156/1000 | Loss: 0.00001362
Iteration 157/1000 | Loss: 0.00001362
Iteration 158/1000 | Loss: 0.00001362
Iteration 159/1000 | Loss: 0.00001362
Iteration 160/1000 | Loss: 0.00001362
Iteration 161/1000 | Loss: 0.00001362
Iteration 162/1000 | Loss: 0.00001362
Iteration 163/1000 | Loss: 0.00001362
Iteration 164/1000 | Loss: 0.00001361
Iteration 165/1000 | Loss: 0.00001361
Iteration 166/1000 | Loss: 0.00001361
Iteration 167/1000 | Loss: 0.00001361
Iteration 168/1000 | Loss: 0.00001361
Iteration 169/1000 | Loss: 0.00001361
Iteration 170/1000 | Loss: 0.00001360
Iteration 171/1000 | Loss: 0.00001360
Iteration 172/1000 | Loss: 0.00001392
Iteration 173/1000 | Loss: 0.00001596
Iteration 174/1000 | Loss: 0.00001358
Iteration 175/1000 | Loss: 0.00001350
Iteration 176/1000 | Loss: 0.00001350
Iteration 177/1000 | Loss: 0.00001350
Iteration 178/1000 | Loss: 0.00001350
Iteration 179/1000 | Loss: 0.00001350
Iteration 180/1000 | Loss: 0.00001350
Iteration 181/1000 | Loss: 0.00001350
Iteration 182/1000 | Loss: 0.00001350
Iteration 183/1000 | Loss: 0.00001350
Iteration 184/1000 | Loss: 0.00001349
Iteration 185/1000 | Loss: 0.00001349
Iteration 186/1000 | Loss: 0.00001414
Iteration 187/1000 | Loss: 0.00001472
Iteration 188/1000 | Loss: 0.00001721
Iteration 189/1000 | Loss: 0.00004232
Iteration 190/1000 | Loss: 0.00001372
Iteration 191/1000 | Loss: 0.00001689
Iteration 192/1000 | Loss: 0.00001689
Iteration 193/1000 | Loss: 0.00004768
Iteration 194/1000 | Loss: 0.00001678
Iteration 195/1000 | Loss: 0.00001355
Iteration 196/1000 | Loss: 0.00001355
Iteration 197/1000 | Loss: 0.00001425
Iteration 198/1000 | Loss: 0.00001358
Iteration 199/1000 | Loss: 0.00001358
Iteration 200/1000 | Loss: 0.00001378
Iteration 201/1000 | Loss: 0.00001434
Iteration 202/1000 | Loss: 0.00001366
Iteration 203/1000 | Loss: 0.00001436
Iteration 204/1000 | Loss: 0.00001339
Iteration 205/1000 | Loss: 0.00001319
Iteration 206/1000 | Loss: 0.00001319
Iteration 207/1000 | Loss: 0.00001319
Iteration 208/1000 | Loss: 0.00001319
Iteration 209/1000 | Loss: 0.00001319
Iteration 210/1000 | Loss: 0.00001319
Iteration 211/1000 | Loss: 0.00001319
Iteration 212/1000 | Loss: 0.00001319
Iteration 213/1000 | Loss: 0.00001319
Iteration 214/1000 | Loss: 0.00001319
Iteration 215/1000 | Loss: 0.00001319
Iteration 216/1000 | Loss: 0.00001319
Iteration 217/1000 | Loss: 0.00001319
Iteration 218/1000 | Loss: 0.00001319
Iteration 219/1000 | Loss: 0.00001319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.3190960089559667e-05, 1.3190960089559667e-05, 1.3190960089559667e-05, 1.3190960089559667e-05, 1.3190960089559667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3190960089559667e-05

Optimization complete. Final v2v error: 3.0383830070495605 mm

Highest mean error: 4.365315914154053 mm for frame 33

Lowest mean error: 2.663088798522949 mm for frame 169

Saving results

Total time: 220.63827753067017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00698489
Iteration 2/25 | Loss: 0.00138430
Iteration 3/25 | Loss: 0.00113689
Iteration 4/25 | Loss: 0.00110650
Iteration 5/25 | Loss: 0.00110170
Iteration 6/25 | Loss: 0.00110081
Iteration 7/25 | Loss: 0.00110081
Iteration 8/25 | Loss: 0.00110081
Iteration 9/25 | Loss: 0.00110081
Iteration 10/25 | Loss: 0.00110081
Iteration 11/25 | Loss: 0.00110081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011008087312802672, 0.0011008087312802672, 0.0011008087312802672, 0.0011008087312802672, 0.0011008087312802672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011008087312802672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36426020
Iteration 2/25 | Loss: 0.00076436
Iteration 3/25 | Loss: 0.00076435
Iteration 4/25 | Loss: 0.00076435
Iteration 5/25 | Loss: 0.00076435
Iteration 6/25 | Loss: 0.00076435
Iteration 7/25 | Loss: 0.00076435
Iteration 8/25 | Loss: 0.00076435
Iteration 9/25 | Loss: 0.00076435
Iteration 10/25 | Loss: 0.00076435
Iteration 11/25 | Loss: 0.00076435
Iteration 12/25 | Loss: 0.00076435
Iteration 13/25 | Loss: 0.00076435
Iteration 14/25 | Loss: 0.00076435
Iteration 15/25 | Loss: 0.00076435
Iteration 16/25 | Loss: 0.00076435
Iteration 17/25 | Loss: 0.00076435
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000764346681535244, 0.000764346681535244, 0.000764346681535244, 0.000764346681535244, 0.000764346681535244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000764346681535244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076435
Iteration 2/1000 | Loss: 0.00002863
Iteration 3/1000 | Loss: 0.00001838
Iteration 4/1000 | Loss: 0.00001661
Iteration 5/1000 | Loss: 0.00001550
Iteration 6/1000 | Loss: 0.00001456
Iteration 7/1000 | Loss: 0.00001410
Iteration 8/1000 | Loss: 0.00001375
Iteration 9/1000 | Loss: 0.00001343
Iteration 10/1000 | Loss: 0.00001317
Iteration 11/1000 | Loss: 0.00001303
Iteration 12/1000 | Loss: 0.00001299
Iteration 13/1000 | Loss: 0.00001293
Iteration 14/1000 | Loss: 0.00001284
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001276
Iteration 17/1000 | Loss: 0.00001276
Iteration 18/1000 | Loss: 0.00001275
Iteration 19/1000 | Loss: 0.00001274
Iteration 20/1000 | Loss: 0.00001273
Iteration 21/1000 | Loss: 0.00001269
Iteration 22/1000 | Loss: 0.00001267
Iteration 23/1000 | Loss: 0.00001266
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001263
Iteration 26/1000 | Loss: 0.00001262
Iteration 27/1000 | Loss: 0.00001255
Iteration 28/1000 | Loss: 0.00001253
Iteration 29/1000 | Loss: 0.00001253
Iteration 30/1000 | Loss: 0.00001252
Iteration 31/1000 | Loss: 0.00001252
Iteration 32/1000 | Loss: 0.00001251
Iteration 33/1000 | Loss: 0.00001251
Iteration 34/1000 | Loss: 0.00001249
Iteration 35/1000 | Loss: 0.00001248
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001241
Iteration 39/1000 | Loss: 0.00001241
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001237
Iteration 46/1000 | Loss: 0.00001237
Iteration 47/1000 | Loss: 0.00001236
Iteration 48/1000 | Loss: 0.00001236
Iteration 49/1000 | Loss: 0.00001235
Iteration 50/1000 | Loss: 0.00001235
Iteration 51/1000 | Loss: 0.00001234
Iteration 52/1000 | Loss: 0.00001234
Iteration 53/1000 | Loss: 0.00001234
Iteration 54/1000 | Loss: 0.00001233
Iteration 55/1000 | Loss: 0.00001233
Iteration 56/1000 | Loss: 0.00001232
Iteration 57/1000 | Loss: 0.00001232
Iteration 58/1000 | Loss: 0.00001231
Iteration 59/1000 | Loss: 0.00001231
Iteration 60/1000 | Loss: 0.00001231
Iteration 61/1000 | Loss: 0.00001230
Iteration 62/1000 | Loss: 0.00001230
Iteration 63/1000 | Loss: 0.00001230
Iteration 64/1000 | Loss: 0.00001229
Iteration 65/1000 | Loss: 0.00001229
Iteration 66/1000 | Loss: 0.00001229
Iteration 67/1000 | Loss: 0.00001228
Iteration 68/1000 | Loss: 0.00001227
Iteration 69/1000 | Loss: 0.00001227
Iteration 70/1000 | Loss: 0.00001227
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001226
Iteration 74/1000 | Loss: 0.00001225
Iteration 75/1000 | Loss: 0.00001225
Iteration 76/1000 | Loss: 0.00001225
Iteration 77/1000 | Loss: 0.00001225
Iteration 78/1000 | Loss: 0.00001224
Iteration 79/1000 | Loss: 0.00001223
Iteration 80/1000 | Loss: 0.00001223
Iteration 81/1000 | Loss: 0.00001223
Iteration 82/1000 | Loss: 0.00001223
Iteration 83/1000 | Loss: 0.00001223
Iteration 84/1000 | Loss: 0.00001223
Iteration 85/1000 | Loss: 0.00001223
Iteration 86/1000 | Loss: 0.00001223
Iteration 87/1000 | Loss: 0.00001223
Iteration 88/1000 | Loss: 0.00001223
Iteration 89/1000 | Loss: 0.00001223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.2230982065375429e-05, 1.2230982065375429e-05, 1.2230982065375429e-05, 1.2230982065375429e-05, 1.2230982065375429e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2230982065375429e-05

Optimization complete. Final v2v error: 3.0038225650787354 mm

Highest mean error: 3.530036687850952 mm for frame 235

Lowest mean error: 2.661369562149048 mm for frame 165

Saving results

Total time: 41.17958617210388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485134
Iteration 2/25 | Loss: 0.00135479
Iteration 3/25 | Loss: 0.00118053
Iteration 4/25 | Loss: 0.00116365
Iteration 5/25 | Loss: 0.00116079
Iteration 6/25 | Loss: 0.00116005
Iteration 7/25 | Loss: 0.00116005
Iteration 8/25 | Loss: 0.00116005
Iteration 9/25 | Loss: 0.00116005
Iteration 10/25 | Loss: 0.00116005
Iteration 11/25 | Loss: 0.00116005
Iteration 12/25 | Loss: 0.00116005
Iteration 13/25 | Loss: 0.00116005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011600509751588106, 0.0011600509751588106, 0.0011600509751588106, 0.0011600509751588106, 0.0011600509751588106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011600509751588106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46029544
Iteration 2/25 | Loss: 0.00086504
Iteration 3/25 | Loss: 0.00086501
Iteration 4/25 | Loss: 0.00086501
Iteration 5/25 | Loss: 0.00086501
Iteration 6/25 | Loss: 0.00086501
Iteration 7/25 | Loss: 0.00086501
Iteration 8/25 | Loss: 0.00086501
Iteration 9/25 | Loss: 0.00086501
Iteration 10/25 | Loss: 0.00086501
Iteration 11/25 | Loss: 0.00086501
Iteration 12/25 | Loss: 0.00086501
Iteration 13/25 | Loss: 0.00086501
Iteration 14/25 | Loss: 0.00086501
Iteration 15/25 | Loss: 0.00086501
Iteration 16/25 | Loss: 0.00086501
Iteration 17/25 | Loss: 0.00086501
Iteration 18/25 | Loss: 0.00086501
Iteration 19/25 | Loss: 0.00086501
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008650050149299204, 0.0008650050149299204, 0.0008650050149299204, 0.0008650050149299204, 0.0008650050149299204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008650050149299204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086501
Iteration 2/1000 | Loss: 0.00003411
Iteration 3/1000 | Loss: 0.00002316
Iteration 4/1000 | Loss: 0.00001877
Iteration 5/1000 | Loss: 0.00001763
Iteration 6/1000 | Loss: 0.00001665
Iteration 7/1000 | Loss: 0.00001615
Iteration 8/1000 | Loss: 0.00001566
Iteration 9/1000 | Loss: 0.00001525
Iteration 10/1000 | Loss: 0.00001491
Iteration 11/1000 | Loss: 0.00001471
Iteration 12/1000 | Loss: 0.00001449
Iteration 13/1000 | Loss: 0.00001439
Iteration 14/1000 | Loss: 0.00001430
Iteration 15/1000 | Loss: 0.00001425
Iteration 16/1000 | Loss: 0.00001420
Iteration 17/1000 | Loss: 0.00001418
Iteration 18/1000 | Loss: 0.00001417
Iteration 19/1000 | Loss: 0.00001417
Iteration 20/1000 | Loss: 0.00001416
Iteration 21/1000 | Loss: 0.00001416
Iteration 22/1000 | Loss: 0.00001416
Iteration 23/1000 | Loss: 0.00001415
Iteration 24/1000 | Loss: 0.00001415
Iteration 25/1000 | Loss: 0.00001415
Iteration 26/1000 | Loss: 0.00001415
Iteration 27/1000 | Loss: 0.00001414
Iteration 28/1000 | Loss: 0.00001414
Iteration 29/1000 | Loss: 0.00001414
Iteration 30/1000 | Loss: 0.00001413
Iteration 31/1000 | Loss: 0.00001413
Iteration 32/1000 | Loss: 0.00001413
Iteration 33/1000 | Loss: 0.00001412
Iteration 34/1000 | Loss: 0.00001412
Iteration 35/1000 | Loss: 0.00001412
Iteration 36/1000 | Loss: 0.00001412
Iteration 37/1000 | Loss: 0.00001412
Iteration 38/1000 | Loss: 0.00001412
Iteration 39/1000 | Loss: 0.00001411
Iteration 40/1000 | Loss: 0.00001411
Iteration 41/1000 | Loss: 0.00001411
Iteration 42/1000 | Loss: 0.00001411
Iteration 43/1000 | Loss: 0.00001411
Iteration 44/1000 | Loss: 0.00001410
Iteration 45/1000 | Loss: 0.00001410
Iteration 46/1000 | Loss: 0.00001410
Iteration 47/1000 | Loss: 0.00001410
Iteration 48/1000 | Loss: 0.00001410
Iteration 49/1000 | Loss: 0.00001409
Iteration 50/1000 | Loss: 0.00001409
Iteration 51/1000 | Loss: 0.00001409
Iteration 52/1000 | Loss: 0.00001409
Iteration 53/1000 | Loss: 0.00001409
Iteration 54/1000 | Loss: 0.00001409
Iteration 55/1000 | Loss: 0.00001409
Iteration 56/1000 | Loss: 0.00001409
Iteration 57/1000 | Loss: 0.00001409
Iteration 58/1000 | Loss: 0.00001409
Iteration 59/1000 | Loss: 0.00001409
Iteration 60/1000 | Loss: 0.00001409
Iteration 61/1000 | Loss: 0.00001408
Iteration 62/1000 | Loss: 0.00001408
Iteration 63/1000 | Loss: 0.00001408
Iteration 64/1000 | Loss: 0.00001408
Iteration 65/1000 | Loss: 0.00001408
Iteration 66/1000 | Loss: 0.00001407
Iteration 67/1000 | Loss: 0.00001407
Iteration 68/1000 | Loss: 0.00001407
Iteration 69/1000 | Loss: 0.00001407
Iteration 70/1000 | Loss: 0.00001407
Iteration 71/1000 | Loss: 0.00001406
Iteration 72/1000 | Loss: 0.00001406
Iteration 73/1000 | Loss: 0.00001406
Iteration 74/1000 | Loss: 0.00001406
Iteration 75/1000 | Loss: 0.00001406
Iteration 76/1000 | Loss: 0.00001406
Iteration 77/1000 | Loss: 0.00001406
Iteration 78/1000 | Loss: 0.00001406
Iteration 79/1000 | Loss: 0.00001406
Iteration 80/1000 | Loss: 0.00001405
Iteration 81/1000 | Loss: 0.00001405
Iteration 82/1000 | Loss: 0.00001405
Iteration 83/1000 | Loss: 0.00001405
Iteration 84/1000 | Loss: 0.00001405
Iteration 85/1000 | Loss: 0.00001405
Iteration 86/1000 | Loss: 0.00001405
Iteration 87/1000 | Loss: 0.00001405
Iteration 88/1000 | Loss: 0.00001405
Iteration 89/1000 | Loss: 0.00001405
Iteration 90/1000 | Loss: 0.00001404
Iteration 91/1000 | Loss: 0.00001404
Iteration 92/1000 | Loss: 0.00001404
Iteration 93/1000 | Loss: 0.00001404
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001403
Iteration 96/1000 | Loss: 0.00001403
Iteration 97/1000 | Loss: 0.00001403
Iteration 98/1000 | Loss: 0.00001403
Iteration 99/1000 | Loss: 0.00001403
Iteration 100/1000 | Loss: 0.00001403
Iteration 101/1000 | Loss: 0.00001403
Iteration 102/1000 | Loss: 0.00001403
Iteration 103/1000 | Loss: 0.00001402
Iteration 104/1000 | Loss: 0.00001402
Iteration 105/1000 | Loss: 0.00001402
Iteration 106/1000 | Loss: 0.00001402
Iteration 107/1000 | Loss: 0.00001402
Iteration 108/1000 | Loss: 0.00001401
Iteration 109/1000 | Loss: 0.00001401
Iteration 110/1000 | Loss: 0.00001401
Iteration 111/1000 | Loss: 0.00001401
Iteration 112/1000 | Loss: 0.00001401
Iteration 113/1000 | Loss: 0.00001401
Iteration 114/1000 | Loss: 0.00001401
Iteration 115/1000 | Loss: 0.00001401
Iteration 116/1000 | Loss: 0.00001401
Iteration 117/1000 | Loss: 0.00001401
Iteration 118/1000 | Loss: 0.00001401
Iteration 119/1000 | Loss: 0.00001400
Iteration 120/1000 | Loss: 0.00001400
Iteration 121/1000 | Loss: 0.00001400
Iteration 122/1000 | Loss: 0.00001400
Iteration 123/1000 | Loss: 0.00001400
Iteration 124/1000 | Loss: 0.00001400
Iteration 125/1000 | Loss: 0.00001400
Iteration 126/1000 | Loss: 0.00001399
Iteration 127/1000 | Loss: 0.00001399
Iteration 128/1000 | Loss: 0.00001399
Iteration 129/1000 | Loss: 0.00001399
Iteration 130/1000 | Loss: 0.00001399
Iteration 131/1000 | Loss: 0.00001399
Iteration 132/1000 | Loss: 0.00001399
Iteration 133/1000 | Loss: 0.00001399
Iteration 134/1000 | Loss: 0.00001399
Iteration 135/1000 | Loss: 0.00001399
Iteration 136/1000 | Loss: 0.00001399
Iteration 137/1000 | Loss: 0.00001399
Iteration 138/1000 | Loss: 0.00001399
Iteration 139/1000 | Loss: 0.00001399
Iteration 140/1000 | Loss: 0.00001398
Iteration 141/1000 | Loss: 0.00001398
Iteration 142/1000 | Loss: 0.00001398
Iteration 143/1000 | Loss: 0.00001398
Iteration 144/1000 | Loss: 0.00001398
Iteration 145/1000 | Loss: 0.00001398
Iteration 146/1000 | Loss: 0.00001398
Iteration 147/1000 | Loss: 0.00001398
Iteration 148/1000 | Loss: 0.00001398
Iteration 149/1000 | Loss: 0.00001398
Iteration 150/1000 | Loss: 0.00001398
Iteration 151/1000 | Loss: 0.00001398
Iteration 152/1000 | Loss: 0.00001398
Iteration 153/1000 | Loss: 0.00001398
Iteration 154/1000 | Loss: 0.00001397
Iteration 155/1000 | Loss: 0.00001397
Iteration 156/1000 | Loss: 0.00001397
Iteration 157/1000 | Loss: 0.00001397
Iteration 158/1000 | Loss: 0.00001397
Iteration 159/1000 | Loss: 0.00001397
Iteration 160/1000 | Loss: 0.00001397
Iteration 161/1000 | Loss: 0.00001397
Iteration 162/1000 | Loss: 0.00001397
Iteration 163/1000 | Loss: 0.00001397
Iteration 164/1000 | Loss: 0.00001397
Iteration 165/1000 | Loss: 0.00001397
Iteration 166/1000 | Loss: 0.00001397
Iteration 167/1000 | Loss: 0.00001397
Iteration 168/1000 | Loss: 0.00001396
Iteration 169/1000 | Loss: 0.00001396
Iteration 170/1000 | Loss: 0.00001396
Iteration 171/1000 | Loss: 0.00001396
Iteration 172/1000 | Loss: 0.00001396
Iteration 173/1000 | Loss: 0.00001396
Iteration 174/1000 | Loss: 0.00001396
Iteration 175/1000 | Loss: 0.00001396
Iteration 176/1000 | Loss: 0.00001396
Iteration 177/1000 | Loss: 0.00001396
Iteration 178/1000 | Loss: 0.00001396
Iteration 179/1000 | Loss: 0.00001396
Iteration 180/1000 | Loss: 0.00001396
Iteration 181/1000 | Loss: 0.00001396
Iteration 182/1000 | Loss: 0.00001396
Iteration 183/1000 | Loss: 0.00001396
Iteration 184/1000 | Loss: 0.00001396
Iteration 185/1000 | Loss: 0.00001396
Iteration 186/1000 | Loss: 0.00001396
Iteration 187/1000 | Loss: 0.00001396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.3961838703835383e-05, 1.3961838703835383e-05, 1.3961838703835383e-05, 1.3961838703835383e-05, 1.3961838703835383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3961838703835383e-05

Optimization complete. Final v2v error: 3.1331121921539307 mm

Highest mean error: 3.706423759460449 mm for frame 69

Lowest mean error: 2.617774248123169 mm for frame 93

Saving results

Total time: 40.08442687988281
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396264
Iteration 2/25 | Loss: 0.00113484
Iteration 3/25 | Loss: 0.00106270
Iteration 4/25 | Loss: 0.00105273
Iteration 5/25 | Loss: 0.00104927
Iteration 6/25 | Loss: 0.00104886
Iteration 7/25 | Loss: 0.00104886
Iteration 8/25 | Loss: 0.00104886
Iteration 9/25 | Loss: 0.00104886
Iteration 10/25 | Loss: 0.00104886
Iteration 11/25 | Loss: 0.00104886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010488632833585143, 0.0010488632833585143, 0.0010488632833585143, 0.0010488632833585143, 0.0010488632833585143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010488632833585143

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77828968
Iteration 2/25 | Loss: 0.00081868
Iteration 3/25 | Loss: 0.00081868
Iteration 4/25 | Loss: 0.00081868
Iteration 5/25 | Loss: 0.00081868
Iteration 6/25 | Loss: 0.00081868
Iteration 7/25 | Loss: 0.00081868
Iteration 8/25 | Loss: 0.00081868
Iteration 9/25 | Loss: 0.00081868
Iteration 10/25 | Loss: 0.00081868
Iteration 11/25 | Loss: 0.00081868
Iteration 12/25 | Loss: 0.00081868
Iteration 13/25 | Loss: 0.00081868
Iteration 14/25 | Loss: 0.00081868
Iteration 15/25 | Loss: 0.00081868
Iteration 16/25 | Loss: 0.00081868
Iteration 17/25 | Loss: 0.00081868
Iteration 18/25 | Loss: 0.00081868
Iteration 19/25 | Loss: 0.00081868
Iteration 20/25 | Loss: 0.00081868
Iteration 21/25 | Loss: 0.00081868
Iteration 22/25 | Loss: 0.00081868
Iteration 23/25 | Loss: 0.00081868
Iteration 24/25 | Loss: 0.00081868
Iteration 25/25 | Loss: 0.00081868

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081868
Iteration 2/1000 | Loss: 0.00001365
Iteration 3/1000 | Loss: 0.00001074
Iteration 4/1000 | Loss: 0.00001005
Iteration 5/1000 | Loss: 0.00000955
Iteration 6/1000 | Loss: 0.00000922
Iteration 7/1000 | Loss: 0.00000915
Iteration 8/1000 | Loss: 0.00000907
Iteration 9/1000 | Loss: 0.00000884
Iteration 10/1000 | Loss: 0.00000865
Iteration 11/1000 | Loss: 0.00000864
Iteration 12/1000 | Loss: 0.00000862
Iteration 13/1000 | Loss: 0.00000859
Iteration 14/1000 | Loss: 0.00000858
Iteration 15/1000 | Loss: 0.00000857
Iteration 16/1000 | Loss: 0.00000857
Iteration 17/1000 | Loss: 0.00000854
Iteration 18/1000 | Loss: 0.00000853
Iteration 19/1000 | Loss: 0.00000852
Iteration 20/1000 | Loss: 0.00000851
Iteration 21/1000 | Loss: 0.00000851
Iteration 22/1000 | Loss: 0.00000848
Iteration 23/1000 | Loss: 0.00000848
Iteration 24/1000 | Loss: 0.00000847
Iteration 25/1000 | Loss: 0.00000843
Iteration 26/1000 | Loss: 0.00000840
Iteration 27/1000 | Loss: 0.00000839
Iteration 28/1000 | Loss: 0.00000839
Iteration 29/1000 | Loss: 0.00000838
Iteration 30/1000 | Loss: 0.00000836
Iteration 31/1000 | Loss: 0.00000836
Iteration 32/1000 | Loss: 0.00000835
Iteration 33/1000 | Loss: 0.00000835
Iteration 34/1000 | Loss: 0.00000835
Iteration 35/1000 | Loss: 0.00000835
Iteration 36/1000 | Loss: 0.00000834
Iteration 37/1000 | Loss: 0.00000834
Iteration 38/1000 | Loss: 0.00000832
Iteration 39/1000 | Loss: 0.00000832
Iteration 40/1000 | Loss: 0.00000831
Iteration 41/1000 | Loss: 0.00000831
Iteration 42/1000 | Loss: 0.00000830
Iteration 43/1000 | Loss: 0.00000830
Iteration 44/1000 | Loss: 0.00000830
Iteration 45/1000 | Loss: 0.00000830
Iteration 46/1000 | Loss: 0.00000829
Iteration 47/1000 | Loss: 0.00000829
Iteration 48/1000 | Loss: 0.00000829
Iteration 49/1000 | Loss: 0.00000829
Iteration 50/1000 | Loss: 0.00000828
Iteration 51/1000 | Loss: 0.00000827
Iteration 52/1000 | Loss: 0.00000827
Iteration 53/1000 | Loss: 0.00000827
Iteration 54/1000 | Loss: 0.00000827
Iteration 55/1000 | Loss: 0.00000827
Iteration 56/1000 | Loss: 0.00000827
Iteration 57/1000 | Loss: 0.00000826
Iteration 58/1000 | Loss: 0.00000826
Iteration 59/1000 | Loss: 0.00000826
Iteration 60/1000 | Loss: 0.00000826
Iteration 61/1000 | Loss: 0.00000826
Iteration 62/1000 | Loss: 0.00000825
Iteration 63/1000 | Loss: 0.00000825
Iteration 64/1000 | Loss: 0.00000824
Iteration 65/1000 | Loss: 0.00000824
Iteration 66/1000 | Loss: 0.00000823
Iteration 67/1000 | Loss: 0.00000823
Iteration 68/1000 | Loss: 0.00000823
Iteration 69/1000 | Loss: 0.00000823
Iteration 70/1000 | Loss: 0.00000823
Iteration 71/1000 | Loss: 0.00000823
Iteration 72/1000 | Loss: 0.00000822
Iteration 73/1000 | Loss: 0.00000822
Iteration 74/1000 | Loss: 0.00000822
Iteration 75/1000 | Loss: 0.00000821
Iteration 76/1000 | Loss: 0.00000821
Iteration 77/1000 | Loss: 0.00000820
Iteration 78/1000 | Loss: 0.00000820
Iteration 79/1000 | Loss: 0.00000819
Iteration 80/1000 | Loss: 0.00000818
Iteration 81/1000 | Loss: 0.00000818
Iteration 82/1000 | Loss: 0.00000817
Iteration 83/1000 | Loss: 0.00000817
Iteration 84/1000 | Loss: 0.00000816
Iteration 85/1000 | Loss: 0.00000816
Iteration 86/1000 | Loss: 0.00000816
Iteration 87/1000 | Loss: 0.00000815
Iteration 88/1000 | Loss: 0.00000815
Iteration 89/1000 | Loss: 0.00000814
Iteration 90/1000 | Loss: 0.00000814
Iteration 91/1000 | Loss: 0.00000814
Iteration 92/1000 | Loss: 0.00000814
Iteration 93/1000 | Loss: 0.00000813
Iteration 94/1000 | Loss: 0.00000813
Iteration 95/1000 | Loss: 0.00000812
Iteration 96/1000 | Loss: 0.00000812
Iteration 97/1000 | Loss: 0.00000812
Iteration 98/1000 | Loss: 0.00000812
Iteration 99/1000 | Loss: 0.00000812
Iteration 100/1000 | Loss: 0.00000812
Iteration 101/1000 | Loss: 0.00000812
Iteration 102/1000 | Loss: 0.00000812
Iteration 103/1000 | Loss: 0.00000811
Iteration 104/1000 | Loss: 0.00000811
Iteration 105/1000 | Loss: 0.00000811
Iteration 106/1000 | Loss: 0.00000811
Iteration 107/1000 | Loss: 0.00000811
Iteration 108/1000 | Loss: 0.00000811
Iteration 109/1000 | Loss: 0.00000811
Iteration 110/1000 | Loss: 0.00000811
Iteration 111/1000 | Loss: 0.00000811
Iteration 112/1000 | Loss: 0.00000811
Iteration 113/1000 | Loss: 0.00000811
Iteration 114/1000 | Loss: 0.00000811
Iteration 115/1000 | Loss: 0.00000810
Iteration 116/1000 | Loss: 0.00000810
Iteration 117/1000 | Loss: 0.00000810
Iteration 118/1000 | Loss: 0.00000810
Iteration 119/1000 | Loss: 0.00000810
Iteration 120/1000 | Loss: 0.00000810
Iteration 121/1000 | Loss: 0.00000810
Iteration 122/1000 | Loss: 0.00000810
Iteration 123/1000 | Loss: 0.00000810
Iteration 124/1000 | Loss: 0.00000810
Iteration 125/1000 | Loss: 0.00000810
Iteration 126/1000 | Loss: 0.00000810
Iteration 127/1000 | Loss: 0.00000810
Iteration 128/1000 | Loss: 0.00000810
Iteration 129/1000 | Loss: 0.00000810
Iteration 130/1000 | Loss: 0.00000810
Iteration 131/1000 | Loss: 0.00000810
Iteration 132/1000 | Loss: 0.00000810
Iteration 133/1000 | Loss: 0.00000809
Iteration 134/1000 | Loss: 0.00000809
Iteration 135/1000 | Loss: 0.00000809
Iteration 136/1000 | Loss: 0.00000809
Iteration 137/1000 | Loss: 0.00000809
Iteration 138/1000 | Loss: 0.00000809
Iteration 139/1000 | Loss: 0.00000809
Iteration 140/1000 | Loss: 0.00000809
Iteration 141/1000 | Loss: 0.00000809
Iteration 142/1000 | Loss: 0.00000808
Iteration 143/1000 | Loss: 0.00000808
Iteration 144/1000 | Loss: 0.00000808
Iteration 145/1000 | Loss: 0.00000808
Iteration 146/1000 | Loss: 0.00000808
Iteration 147/1000 | Loss: 0.00000807
Iteration 148/1000 | Loss: 0.00000807
Iteration 149/1000 | Loss: 0.00000807
Iteration 150/1000 | Loss: 0.00000807
Iteration 151/1000 | Loss: 0.00000807
Iteration 152/1000 | Loss: 0.00000807
Iteration 153/1000 | Loss: 0.00000807
Iteration 154/1000 | Loss: 0.00000806
Iteration 155/1000 | Loss: 0.00000806
Iteration 156/1000 | Loss: 0.00000806
Iteration 157/1000 | Loss: 0.00000806
Iteration 158/1000 | Loss: 0.00000806
Iteration 159/1000 | Loss: 0.00000806
Iteration 160/1000 | Loss: 0.00000806
Iteration 161/1000 | Loss: 0.00000806
Iteration 162/1000 | Loss: 0.00000806
Iteration 163/1000 | Loss: 0.00000806
Iteration 164/1000 | Loss: 0.00000805
Iteration 165/1000 | Loss: 0.00000805
Iteration 166/1000 | Loss: 0.00000805
Iteration 167/1000 | Loss: 0.00000805
Iteration 168/1000 | Loss: 0.00000805
Iteration 169/1000 | Loss: 0.00000805
Iteration 170/1000 | Loss: 0.00000805
Iteration 171/1000 | Loss: 0.00000805
Iteration 172/1000 | Loss: 0.00000805
Iteration 173/1000 | Loss: 0.00000805
Iteration 174/1000 | Loss: 0.00000805
Iteration 175/1000 | Loss: 0.00000804
Iteration 176/1000 | Loss: 0.00000803
Iteration 177/1000 | Loss: 0.00000803
Iteration 178/1000 | Loss: 0.00000803
Iteration 179/1000 | Loss: 0.00000802
Iteration 180/1000 | Loss: 0.00000802
Iteration 181/1000 | Loss: 0.00000802
Iteration 182/1000 | Loss: 0.00000802
Iteration 183/1000 | Loss: 0.00000801
Iteration 184/1000 | Loss: 0.00000801
Iteration 185/1000 | Loss: 0.00000801
Iteration 186/1000 | Loss: 0.00000801
Iteration 187/1000 | Loss: 0.00000801
Iteration 188/1000 | Loss: 0.00000801
Iteration 189/1000 | Loss: 0.00000801
Iteration 190/1000 | Loss: 0.00000800
Iteration 191/1000 | Loss: 0.00000800
Iteration 192/1000 | Loss: 0.00000800
Iteration 193/1000 | Loss: 0.00000800
Iteration 194/1000 | Loss: 0.00000800
Iteration 195/1000 | Loss: 0.00000800
Iteration 196/1000 | Loss: 0.00000800
Iteration 197/1000 | Loss: 0.00000800
Iteration 198/1000 | Loss: 0.00000800
Iteration 199/1000 | Loss: 0.00000800
Iteration 200/1000 | Loss: 0.00000800
Iteration 201/1000 | Loss: 0.00000800
Iteration 202/1000 | Loss: 0.00000800
Iteration 203/1000 | Loss: 0.00000800
Iteration 204/1000 | Loss: 0.00000800
Iteration 205/1000 | Loss: 0.00000800
Iteration 206/1000 | Loss: 0.00000800
Iteration 207/1000 | Loss: 0.00000800
Iteration 208/1000 | Loss: 0.00000800
Iteration 209/1000 | Loss: 0.00000800
Iteration 210/1000 | Loss: 0.00000800
Iteration 211/1000 | Loss: 0.00000800
Iteration 212/1000 | Loss: 0.00000800
Iteration 213/1000 | Loss: 0.00000800
Iteration 214/1000 | Loss: 0.00000800
Iteration 215/1000 | Loss: 0.00000800
Iteration 216/1000 | Loss: 0.00000800
Iteration 217/1000 | Loss: 0.00000800
Iteration 218/1000 | Loss: 0.00000800
Iteration 219/1000 | Loss: 0.00000800
Iteration 220/1000 | Loss: 0.00000800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [7.995832675078418e-06, 7.995832675078418e-06, 7.995832675078418e-06, 7.995832675078418e-06, 7.995832675078418e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.995832675078418e-06

Optimization complete. Final v2v error: 2.436367988586426 mm

Highest mean error: 2.8710358142852783 mm for frame 87

Lowest mean error: 2.3341102600097656 mm for frame 142

Saving results

Total time: 42.06281566619873
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792784
Iteration 2/25 | Loss: 0.00130588
Iteration 3/25 | Loss: 0.00109474
Iteration 4/25 | Loss: 0.00107931
Iteration 5/25 | Loss: 0.00107398
Iteration 6/25 | Loss: 0.00107226
Iteration 7/25 | Loss: 0.00107218
Iteration 8/25 | Loss: 0.00107218
Iteration 9/25 | Loss: 0.00107218
Iteration 10/25 | Loss: 0.00107218
Iteration 11/25 | Loss: 0.00107218
Iteration 12/25 | Loss: 0.00107218
Iteration 13/25 | Loss: 0.00107218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010721771977841854, 0.0010721771977841854, 0.0010721771977841854, 0.0010721771977841854, 0.0010721771977841854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010721771977841854

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16287053
Iteration 2/25 | Loss: 0.00088302
Iteration 3/25 | Loss: 0.00088302
Iteration 4/25 | Loss: 0.00088302
Iteration 5/25 | Loss: 0.00088302
Iteration 6/25 | Loss: 0.00088302
Iteration 7/25 | Loss: 0.00088302
Iteration 8/25 | Loss: 0.00088302
Iteration 9/25 | Loss: 0.00088302
Iteration 10/25 | Loss: 0.00088302
Iteration 11/25 | Loss: 0.00088302
Iteration 12/25 | Loss: 0.00088302
Iteration 13/25 | Loss: 0.00088302
Iteration 14/25 | Loss: 0.00088302
Iteration 15/25 | Loss: 0.00088302
Iteration 16/25 | Loss: 0.00088302
Iteration 17/25 | Loss: 0.00088302
Iteration 18/25 | Loss: 0.00088302
Iteration 19/25 | Loss: 0.00088302
Iteration 20/25 | Loss: 0.00088302
Iteration 21/25 | Loss: 0.00088302
Iteration 22/25 | Loss: 0.00088302
Iteration 23/25 | Loss: 0.00088302
Iteration 24/25 | Loss: 0.00088302
Iteration 25/25 | Loss: 0.00088302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088302
Iteration 2/1000 | Loss: 0.00004499
Iteration 3/1000 | Loss: 0.00002721
Iteration 4/1000 | Loss: 0.00001999
Iteration 5/1000 | Loss: 0.00001758
Iteration 6/1000 | Loss: 0.00001627
Iteration 7/1000 | Loss: 0.00001511
Iteration 8/1000 | Loss: 0.00001468
Iteration 9/1000 | Loss: 0.00001411
Iteration 10/1000 | Loss: 0.00001365
Iteration 11/1000 | Loss: 0.00001343
Iteration 12/1000 | Loss: 0.00001317
Iteration 13/1000 | Loss: 0.00001309
Iteration 14/1000 | Loss: 0.00001298
Iteration 15/1000 | Loss: 0.00001294
Iteration 16/1000 | Loss: 0.00001291
Iteration 17/1000 | Loss: 0.00001290
Iteration 18/1000 | Loss: 0.00001289
Iteration 19/1000 | Loss: 0.00001288
Iteration 20/1000 | Loss: 0.00001285
Iteration 21/1000 | Loss: 0.00001283
Iteration 22/1000 | Loss: 0.00001283
Iteration 23/1000 | Loss: 0.00001282
Iteration 24/1000 | Loss: 0.00001282
Iteration 25/1000 | Loss: 0.00001280
Iteration 26/1000 | Loss: 0.00001280
Iteration 27/1000 | Loss: 0.00001279
Iteration 28/1000 | Loss: 0.00001279
Iteration 29/1000 | Loss: 0.00001271
Iteration 30/1000 | Loss: 0.00001263
Iteration 31/1000 | Loss: 0.00001255
Iteration 32/1000 | Loss: 0.00001255
Iteration 33/1000 | Loss: 0.00001255
Iteration 34/1000 | Loss: 0.00001255
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001251
Iteration 37/1000 | Loss: 0.00001251
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001251
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001248
Iteration 42/1000 | Loss: 0.00001247
Iteration 43/1000 | Loss: 0.00001246
Iteration 44/1000 | Loss: 0.00001246
Iteration 45/1000 | Loss: 0.00001246
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001245
Iteration 48/1000 | Loss: 0.00001244
Iteration 49/1000 | Loss: 0.00001244
Iteration 50/1000 | Loss: 0.00001244
Iteration 51/1000 | Loss: 0.00001244
Iteration 52/1000 | Loss: 0.00001243
Iteration 53/1000 | Loss: 0.00001243
Iteration 54/1000 | Loss: 0.00001243
Iteration 55/1000 | Loss: 0.00001243
Iteration 56/1000 | Loss: 0.00001243
Iteration 57/1000 | Loss: 0.00001243
Iteration 58/1000 | Loss: 0.00001242
Iteration 59/1000 | Loss: 0.00001241
Iteration 60/1000 | Loss: 0.00001241
Iteration 61/1000 | Loss: 0.00001240
Iteration 62/1000 | Loss: 0.00001240
Iteration 63/1000 | Loss: 0.00001240
Iteration 64/1000 | Loss: 0.00001240
Iteration 65/1000 | Loss: 0.00001239
Iteration 66/1000 | Loss: 0.00001239
Iteration 67/1000 | Loss: 0.00001239
Iteration 68/1000 | Loss: 0.00001238
Iteration 69/1000 | Loss: 0.00001238
Iteration 70/1000 | Loss: 0.00001238
Iteration 71/1000 | Loss: 0.00001237
Iteration 72/1000 | Loss: 0.00001237
Iteration 73/1000 | Loss: 0.00001237
Iteration 74/1000 | Loss: 0.00001237
Iteration 75/1000 | Loss: 0.00001237
Iteration 76/1000 | Loss: 0.00001237
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001236
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001235
Iteration 85/1000 | Loss: 0.00001235
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001235
Iteration 88/1000 | Loss: 0.00001235
Iteration 89/1000 | Loss: 0.00001235
Iteration 90/1000 | Loss: 0.00001235
Iteration 91/1000 | Loss: 0.00001235
Iteration 92/1000 | Loss: 0.00001235
Iteration 93/1000 | Loss: 0.00001235
Iteration 94/1000 | Loss: 0.00001234
Iteration 95/1000 | Loss: 0.00001234
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001234
Iteration 98/1000 | Loss: 0.00001234
Iteration 99/1000 | Loss: 0.00001234
Iteration 100/1000 | Loss: 0.00001234
Iteration 101/1000 | Loss: 0.00001234
Iteration 102/1000 | Loss: 0.00001233
Iteration 103/1000 | Loss: 0.00001233
Iteration 104/1000 | Loss: 0.00001233
Iteration 105/1000 | Loss: 0.00001233
Iteration 106/1000 | Loss: 0.00001233
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001232
Iteration 109/1000 | Loss: 0.00001232
Iteration 110/1000 | Loss: 0.00001232
Iteration 111/1000 | Loss: 0.00001232
Iteration 112/1000 | Loss: 0.00001232
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001231
Iteration 119/1000 | Loss: 0.00001231
Iteration 120/1000 | Loss: 0.00001231
Iteration 121/1000 | Loss: 0.00001231
Iteration 122/1000 | Loss: 0.00001231
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001231
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001231
Iteration 128/1000 | Loss: 0.00001231
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001230
Iteration 132/1000 | Loss: 0.00001230
Iteration 133/1000 | Loss: 0.00001229
Iteration 134/1000 | Loss: 0.00001229
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001229
Iteration 137/1000 | Loss: 0.00001229
Iteration 138/1000 | Loss: 0.00001229
Iteration 139/1000 | Loss: 0.00001229
Iteration 140/1000 | Loss: 0.00001229
Iteration 141/1000 | Loss: 0.00001229
Iteration 142/1000 | Loss: 0.00001229
Iteration 143/1000 | Loss: 0.00001228
Iteration 144/1000 | Loss: 0.00001228
Iteration 145/1000 | Loss: 0.00001228
Iteration 146/1000 | Loss: 0.00001228
Iteration 147/1000 | Loss: 0.00001228
Iteration 148/1000 | Loss: 0.00001228
Iteration 149/1000 | Loss: 0.00001228
Iteration 150/1000 | Loss: 0.00001227
Iteration 151/1000 | Loss: 0.00001227
Iteration 152/1000 | Loss: 0.00001227
Iteration 153/1000 | Loss: 0.00001227
Iteration 154/1000 | Loss: 0.00001227
Iteration 155/1000 | Loss: 0.00001226
Iteration 156/1000 | Loss: 0.00001226
Iteration 157/1000 | Loss: 0.00001226
Iteration 158/1000 | Loss: 0.00001226
Iteration 159/1000 | Loss: 0.00001226
Iteration 160/1000 | Loss: 0.00001226
Iteration 161/1000 | Loss: 0.00001226
Iteration 162/1000 | Loss: 0.00001226
Iteration 163/1000 | Loss: 0.00001226
Iteration 164/1000 | Loss: 0.00001226
Iteration 165/1000 | Loss: 0.00001225
Iteration 166/1000 | Loss: 0.00001225
Iteration 167/1000 | Loss: 0.00001225
Iteration 168/1000 | Loss: 0.00001225
Iteration 169/1000 | Loss: 0.00001225
Iteration 170/1000 | Loss: 0.00001225
Iteration 171/1000 | Loss: 0.00001225
Iteration 172/1000 | Loss: 0.00001225
Iteration 173/1000 | Loss: 0.00001224
Iteration 174/1000 | Loss: 0.00001224
Iteration 175/1000 | Loss: 0.00001224
Iteration 176/1000 | Loss: 0.00001224
Iteration 177/1000 | Loss: 0.00001224
Iteration 178/1000 | Loss: 0.00001224
Iteration 179/1000 | Loss: 0.00001223
Iteration 180/1000 | Loss: 0.00001223
Iteration 181/1000 | Loss: 0.00001223
Iteration 182/1000 | Loss: 0.00001223
Iteration 183/1000 | Loss: 0.00001223
Iteration 184/1000 | Loss: 0.00001223
Iteration 185/1000 | Loss: 0.00001223
Iteration 186/1000 | Loss: 0.00001223
Iteration 187/1000 | Loss: 0.00001222
Iteration 188/1000 | Loss: 0.00001222
Iteration 189/1000 | Loss: 0.00001222
Iteration 190/1000 | Loss: 0.00001222
Iteration 191/1000 | Loss: 0.00001222
Iteration 192/1000 | Loss: 0.00001222
Iteration 193/1000 | Loss: 0.00001222
Iteration 194/1000 | Loss: 0.00001222
Iteration 195/1000 | Loss: 0.00001222
Iteration 196/1000 | Loss: 0.00001222
Iteration 197/1000 | Loss: 0.00001222
Iteration 198/1000 | Loss: 0.00001222
Iteration 199/1000 | Loss: 0.00001222
Iteration 200/1000 | Loss: 0.00001222
Iteration 201/1000 | Loss: 0.00001222
Iteration 202/1000 | Loss: 0.00001222
Iteration 203/1000 | Loss: 0.00001222
Iteration 204/1000 | Loss: 0.00001222
Iteration 205/1000 | Loss: 0.00001222
Iteration 206/1000 | Loss: 0.00001222
Iteration 207/1000 | Loss: 0.00001222
Iteration 208/1000 | Loss: 0.00001222
Iteration 209/1000 | Loss: 0.00001222
Iteration 210/1000 | Loss: 0.00001222
Iteration 211/1000 | Loss: 0.00001222
Iteration 212/1000 | Loss: 0.00001222
Iteration 213/1000 | Loss: 0.00001222
Iteration 214/1000 | Loss: 0.00001222
Iteration 215/1000 | Loss: 0.00001222
Iteration 216/1000 | Loss: 0.00001222
Iteration 217/1000 | Loss: 0.00001222
Iteration 218/1000 | Loss: 0.00001222
Iteration 219/1000 | Loss: 0.00001222
Iteration 220/1000 | Loss: 0.00001222
Iteration 221/1000 | Loss: 0.00001222
Iteration 222/1000 | Loss: 0.00001222
Iteration 223/1000 | Loss: 0.00001222
Iteration 224/1000 | Loss: 0.00001222
Iteration 225/1000 | Loss: 0.00001222
Iteration 226/1000 | Loss: 0.00001222
Iteration 227/1000 | Loss: 0.00001222
Iteration 228/1000 | Loss: 0.00001222
Iteration 229/1000 | Loss: 0.00001222
Iteration 230/1000 | Loss: 0.00001222
Iteration 231/1000 | Loss: 0.00001222
Iteration 232/1000 | Loss: 0.00001222
Iteration 233/1000 | Loss: 0.00001222
Iteration 234/1000 | Loss: 0.00001222
Iteration 235/1000 | Loss: 0.00001222
Iteration 236/1000 | Loss: 0.00001222
Iteration 237/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.2218541087349877e-05, 1.2218541087349877e-05, 1.2218541087349877e-05, 1.2218541087349877e-05, 1.2218541087349877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2218541087349877e-05

Optimization complete. Final v2v error: 2.8623571395874023 mm

Highest mean error: 4.199602127075195 mm for frame 68

Lowest mean error: 2.315669059753418 mm for frame 102

Saving results

Total time: 47.0574369430542
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787506
Iteration 2/25 | Loss: 0.00215906
Iteration 3/25 | Loss: 0.00164334
Iteration 4/25 | Loss: 0.00152486
Iteration 5/25 | Loss: 0.00142087
Iteration 6/25 | Loss: 0.00134418
Iteration 7/25 | Loss: 0.00131988
Iteration 8/25 | Loss: 0.00130957
Iteration 9/25 | Loss: 0.00130234
Iteration 10/25 | Loss: 0.00131702
Iteration 11/25 | Loss: 0.00128320
Iteration 12/25 | Loss: 0.00129211
Iteration 13/25 | Loss: 0.00127430
Iteration 14/25 | Loss: 0.00127063
Iteration 15/25 | Loss: 0.00126959
Iteration 16/25 | Loss: 0.00126938
Iteration 17/25 | Loss: 0.00126931
Iteration 18/25 | Loss: 0.00126931
Iteration 19/25 | Loss: 0.00126931
Iteration 20/25 | Loss: 0.00126931
Iteration 21/25 | Loss: 0.00126930
Iteration 22/25 | Loss: 0.00126930
Iteration 23/25 | Loss: 0.00126930
Iteration 24/25 | Loss: 0.00126930
Iteration 25/25 | Loss: 0.00126930

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34722972
Iteration 2/25 | Loss: 0.00062222
Iteration 3/25 | Loss: 0.00062220
Iteration 4/25 | Loss: 0.00062220
Iteration 5/25 | Loss: 0.00062220
Iteration 6/25 | Loss: 0.00062220
Iteration 7/25 | Loss: 0.00062220
Iteration 8/25 | Loss: 0.00062220
Iteration 9/25 | Loss: 0.00062220
Iteration 10/25 | Loss: 0.00062220
Iteration 11/25 | Loss: 0.00062220
Iteration 12/25 | Loss: 0.00062220
Iteration 13/25 | Loss: 0.00062220
Iteration 14/25 | Loss: 0.00062220
Iteration 15/25 | Loss: 0.00062220
Iteration 16/25 | Loss: 0.00062220
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006221962976269424, 0.0006221962976269424, 0.0006221962976269424, 0.0006221962976269424, 0.0006221962976269424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006221962976269424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062220
Iteration 2/1000 | Loss: 0.00003760
Iteration 3/1000 | Loss: 0.00002893
Iteration 4/1000 | Loss: 0.00002694
Iteration 5/1000 | Loss: 0.00002602
Iteration 6/1000 | Loss: 0.00002552
Iteration 7/1000 | Loss: 0.00002521
Iteration 8/1000 | Loss: 0.00002497
Iteration 9/1000 | Loss: 0.00002486
Iteration 10/1000 | Loss: 0.00002469
Iteration 11/1000 | Loss: 0.00002454
Iteration 12/1000 | Loss: 0.00002451
Iteration 13/1000 | Loss: 0.00002444
Iteration 14/1000 | Loss: 0.00002444
Iteration 15/1000 | Loss: 0.00002443
Iteration 16/1000 | Loss: 0.00002441
Iteration 17/1000 | Loss: 0.00002441
Iteration 18/1000 | Loss: 0.00002440
Iteration 19/1000 | Loss: 0.00002440
Iteration 20/1000 | Loss: 0.00002436
Iteration 21/1000 | Loss: 0.00002435
Iteration 22/1000 | Loss: 0.00002435
Iteration 23/1000 | Loss: 0.00002432
Iteration 24/1000 | Loss: 0.00002431
Iteration 25/1000 | Loss: 0.00002426
Iteration 26/1000 | Loss: 0.00002426
Iteration 27/1000 | Loss: 0.00002421
Iteration 28/1000 | Loss: 0.00002420
Iteration 29/1000 | Loss: 0.00002419
Iteration 30/1000 | Loss: 0.00002419
Iteration 31/1000 | Loss: 0.00002419
Iteration 32/1000 | Loss: 0.00002419
Iteration 33/1000 | Loss: 0.00002419
Iteration 34/1000 | Loss: 0.00002419
Iteration 35/1000 | Loss: 0.00002418
Iteration 36/1000 | Loss: 0.00002417
Iteration 37/1000 | Loss: 0.00002417
Iteration 38/1000 | Loss: 0.00002416
Iteration 39/1000 | Loss: 0.00002416
Iteration 40/1000 | Loss: 0.00002416
Iteration 41/1000 | Loss: 0.00002415
Iteration 42/1000 | Loss: 0.00002415
Iteration 43/1000 | Loss: 0.00002415
Iteration 44/1000 | Loss: 0.00002415
Iteration 45/1000 | Loss: 0.00002415
Iteration 46/1000 | Loss: 0.00002414
Iteration 47/1000 | Loss: 0.00002414
Iteration 48/1000 | Loss: 0.00002414
Iteration 49/1000 | Loss: 0.00002414
Iteration 50/1000 | Loss: 0.00002413
Iteration 51/1000 | Loss: 0.00002413
Iteration 52/1000 | Loss: 0.00002413
Iteration 53/1000 | Loss: 0.00002413
Iteration 54/1000 | Loss: 0.00002412
Iteration 55/1000 | Loss: 0.00002412
Iteration 56/1000 | Loss: 0.00002412
Iteration 57/1000 | Loss: 0.00002412
Iteration 58/1000 | Loss: 0.00002412
Iteration 59/1000 | Loss: 0.00002412
Iteration 60/1000 | Loss: 0.00002411
Iteration 61/1000 | Loss: 0.00002411
Iteration 62/1000 | Loss: 0.00002411
Iteration 63/1000 | Loss: 0.00002411
Iteration 64/1000 | Loss: 0.00002411
Iteration 65/1000 | Loss: 0.00002411
Iteration 66/1000 | Loss: 0.00002411
Iteration 67/1000 | Loss: 0.00002410
Iteration 68/1000 | Loss: 0.00002410
Iteration 69/1000 | Loss: 0.00002410
Iteration 70/1000 | Loss: 0.00002410
Iteration 71/1000 | Loss: 0.00002410
Iteration 72/1000 | Loss: 0.00002410
Iteration 73/1000 | Loss: 0.00002410
Iteration 74/1000 | Loss: 0.00002410
Iteration 75/1000 | Loss: 0.00002410
Iteration 76/1000 | Loss: 0.00002410
Iteration 77/1000 | Loss: 0.00002410
Iteration 78/1000 | Loss: 0.00002410
Iteration 79/1000 | Loss: 0.00002410
Iteration 80/1000 | Loss: 0.00002410
Iteration 81/1000 | Loss: 0.00002410
Iteration 82/1000 | Loss: 0.00002410
Iteration 83/1000 | Loss: 0.00002410
Iteration 84/1000 | Loss: 0.00002410
Iteration 85/1000 | Loss: 0.00002410
Iteration 86/1000 | Loss: 0.00002410
Iteration 87/1000 | Loss: 0.00002410
Iteration 88/1000 | Loss: 0.00002410
Iteration 89/1000 | Loss: 0.00002410
Iteration 90/1000 | Loss: 0.00002410
Iteration 91/1000 | Loss: 0.00002410
Iteration 92/1000 | Loss: 0.00002410
Iteration 93/1000 | Loss: 0.00002410
Iteration 94/1000 | Loss: 0.00002410
Iteration 95/1000 | Loss: 0.00002410
Iteration 96/1000 | Loss: 0.00002410
Iteration 97/1000 | Loss: 0.00002410
Iteration 98/1000 | Loss: 0.00002410
Iteration 99/1000 | Loss: 0.00002410
Iteration 100/1000 | Loss: 0.00002410
Iteration 101/1000 | Loss: 0.00002410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.409523403912317e-05, 2.409523403912317e-05, 2.409523403912317e-05, 2.409523403912317e-05, 2.409523403912317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.409523403912317e-05

Optimization complete. Final v2v error: 4.102169990539551 mm

Highest mean error: 4.351435661315918 mm for frame 6

Lowest mean error: 3.8084070682525635 mm for frame 154

Saving results

Total time: 53.8631227016449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593028
Iteration 2/25 | Loss: 0.00157134
Iteration 3/25 | Loss: 0.00121235
Iteration 4/25 | Loss: 0.00117221
Iteration 5/25 | Loss: 0.00116756
Iteration 6/25 | Loss: 0.00116613
Iteration 7/25 | Loss: 0.00116586
Iteration 8/25 | Loss: 0.00116586
Iteration 9/25 | Loss: 0.00116586
Iteration 10/25 | Loss: 0.00116586
Iteration 11/25 | Loss: 0.00116586
Iteration 12/25 | Loss: 0.00116586
Iteration 13/25 | Loss: 0.00116586
Iteration 14/25 | Loss: 0.00116586
Iteration 15/25 | Loss: 0.00116586
Iteration 16/25 | Loss: 0.00116586
Iteration 17/25 | Loss: 0.00116586
Iteration 18/25 | Loss: 0.00116586
Iteration 19/25 | Loss: 0.00116586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011658591683954, 0.0011658591683954, 0.0011658591683954, 0.0011658591683954, 0.0011658591683954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011658591683954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17902279
Iteration 2/25 | Loss: 0.00065408
Iteration 3/25 | Loss: 0.00065407
Iteration 4/25 | Loss: 0.00065407
Iteration 5/25 | Loss: 0.00065407
Iteration 6/25 | Loss: 0.00065407
Iteration 7/25 | Loss: 0.00065407
Iteration 8/25 | Loss: 0.00065407
Iteration 9/25 | Loss: 0.00065407
Iteration 10/25 | Loss: 0.00065407
Iteration 11/25 | Loss: 0.00065407
Iteration 12/25 | Loss: 0.00065407
Iteration 13/25 | Loss: 0.00065407
Iteration 14/25 | Loss: 0.00065407
Iteration 15/25 | Loss: 0.00065407
Iteration 16/25 | Loss: 0.00065407
Iteration 17/25 | Loss: 0.00065407
Iteration 18/25 | Loss: 0.00065407
Iteration 19/25 | Loss: 0.00065407
Iteration 20/25 | Loss: 0.00065407
Iteration 21/25 | Loss: 0.00065407
Iteration 22/25 | Loss: 0.00065407
Iteration 23/25 | Loss: 0.00065407
Iteration 24/25 | Loss: 0.00065407
Iteration 25/25 | Loss: 0.00065407

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065407
Iteration 2/1000 | Loss: 0.00004015
Iteration 3/1000 | Loss: 0.00002829
Iteration 4/1000 | Loss: 0.00002089
Iteration 5/1000 | Loss: 0.00001952
Iteration 6/1000 | Loss: 0.00001865
Iteration 7/1000 | Loss: 0.00001809
Iteration 8/1000 | Loss: 0.00001774
Iteration 9/1000 | Loss: 0.00001735
Iteration 10/1000 | Loss: 0.00001709
Iteration 11/1000 | Loss: 0.00001695
Iteration 12/1000 | Loss: 0.00001687
Iteration 13/1000 | Loss: 0.00001682
Iteration 14/1000 | Loss: 0.00001681
Iteration 15/1000 | Loss: 0.00001681
Iteration 16/1000 | Loss: 0.00001681
Iteration 17/1000 | Loss: 0.00001681
Iteration 18/1000 | Loss: 0.00001680
Iteration 19/1000 | Loss: 0.00001680
Iteration 20/1000 | Loss: 0.00001680
Iteration 21/1000 | Loss: 0.00001679
Iteration 22/1000 | Loss: 0.00001678
Iteration 23/1000 | Loss: 0.00001678
Iteration 24/1000 | Loss: 0.00001678
Iteration 25/1000 | Loss: 0.00001678
Iteration 26/1000 | Loss: 0.00001678
Iteration 27/1000 | Loss: 0.00001677
Iteration 28/1000 | Loss: 0.00001677
Iteration 29/1000 | Loss: 0.00001675
Iteration 30/1000 | Loss: 0.00001675
Iteration 31/1000 | Loss: 0.00001675
Iteration 32/1000 | Loss: 0.00001674
Iteration 33/1000 | Loss: 0.00001674
Iteration 34/1000 | Loss: 0.00001674
Iteration 35/1000 | Loss: 0.00001673
Iteration 36/1000 | Loss: 0.00001669
Iteration 37/1000 | Loss: 0.00001665
Iteration 38/1000 | Loss: 0.00001665
Iteration 39/1000 | Loss: 0.00001664
Iteration 40/1000 | Loss: 0.00001663
Iteration 41/1000 | Loss: 0.00001662
Iteration 42/1000 | Loss: 0.00001662
Iteration 43/1000 | Loss: 0.00001661
Iteration 44/1000 | Loss: 0.00001661
Iteration 45/1000 | Loss: 0.00001660
Iteration 46/1000 | Loss: 0.00001660
Iteration 47/1000 | Loss: 0.00001660
Iteration 48/1000 | Loss: 0.00001659
Iteration 49/1000 | Loss: 0.00001659
Iteration 50/1000 | Loss: 0.00001659
Iteration 51/1000 | Loss: 0.00001659
Iteration 52/1000 | Loss: 0.00001658
Iteration 53/1000 | Loss: 0.00001658
Iteration 54/1000 | Loss: 0.00001658
Iteration 55/1000 | Loss: 0.00001657
Iteration 56/1000 | Loss: 0.00001657
Iteration 57/1000 | Loss: 0.00001656
Iteration 58/1000 | Loss: 0.00001655
Iteration 59/1000 | Loss: 0.00001654
Iteration 60/1000 | Loss: 0.00001654
Iteration 61/1000 | Loss: 0.00001654
Iteration 62/1000 | Loss: 0.00001654
Iteration 63/1000 | Loss: 0.00001654
Iteration 64/1000 | Loss: 0.00001654
Iteration 65/1000 | Loss: 0.00001654
Iteration 66/1000 | Loss: 0.00001653
Iteration 67/1000 | Loss: 0.00001653
Iteration 68/1000 | Loss: 0.00001653
Iteration 69/1000 | Loss: 0.00001651
Iteration 70/1000 | Loss: 0.00001651
Iteration 71/1000 | Loss: 0.00001651
Iteration 72/1000 | Loss: 0.00001650
Iteration 73/1000 | Loss: 0.00001650
Iteration 74/1000 | Loss: 0.00001650
Iteration 75/1000 | Loss: 0.00001650
Iteration 76/1000 | Loss: 0.00001649
Iteration 77/1000 | Loss: 0.00001649
Iteration 78/1000 | Loss: 0.00001649
Iteration 79/1000 | Loss: 0.00001649
Iteration 80/1000 | Loss: 0.00001649
Iteration 81/1000 | Loss: 0.00001649
Iteration 82/1000 | Loss: 0.00001649
Iteration 83/1000 | Loss: 0.00001649
Iteration 84/1000 | Loss: 0.00001649
Iteration 85/1000 | Loss: 0.00001649
Iteration 86/1000 | Loss: 0.00001648
Iteration 87/1000 | Loss: 0.00001648
Iteration 88/1000 | Loss: 0.00001648
Iteration 89/1000 | Loss: 0.00001647
Iteration 90/1000 | Loss: 0.00001647
Iteration 91/1000 | Loss: 0.00001647
Iteration 92/1000 | Loss: 0.00001647
Iteration 93/1000 | Loss: 0.00001647
Iteration 94/1000 | Loss: 0.00001647
Iteration 95/1000 | Loss: 0.00001647
Iteration 96/1000 | Loss: 0.00001647
Iteration 97/1000 | Loss: 0.00001647
Iteration 98/1000 | Loss: 0.00001646
Iteration 99/1000 | Loss: 0.00001646
Iteration 100/1000 | Loss: 0.00001646
Iteration 101/1000 | Loss: 0.00001646
Iteration 102/1000 | Loss: 0.00001646
Iteration 103/1000 | Loss: 0.00001645
Iteration 104/1000 | Loss: 0.00001645
Iteration 105/1000 | Loss: 0.00001644
Iteration 106/1000 | Loss: 0.00001644
Iteration 107/1000 | Loss: 0.00001644
Iteration 108/1000 | Loss: 0.00001644
Iteration 109/1000 | Loss: 0.00001643
Iteration 110/1000 | Loss: 0.00001643
Iteration 111/1000 | Loss: 0.00001643
Iteration 112/1000 | Loss: 0.00001643
Iteration 113/1000 | Loss: 0.00001643
Iteration 114/1000 | Loss: 0.00001642
Iteration 115/1000 | Loss: 0.00001642
Iteration 116/1000 | Loss: 0.00001642
Iteration 117/1000 | Loss: 0.00001642
Iteration 118/1000 | Loss: 0.00001642
Iteration 119/1000 | Loss: 0.00001642
Iteration 120/1000 | Loss: 0.00001642
Iteration 121/1000 | Loss: 0.00001641
Iteration 122/1000 | Loss: 0.00001641
Iteration 123/1000 | Loss: 0.00001641
Iteration 124/1000 | Loss: 0.00001641
Iteration 125/1000 | Loss: 0.00001641
Iteration 126/1000 | Loss: 0.00001641
Iteration 127/1000 | Loss: 0.00001641
Iteration 128/1000 | Loss: 0.00001641
Iteration 129/1000 | Loss: 0.00001641
Iteration 130/1000 | Loss: 0.00001641
Iteration 131/1000 | Loss: 0.00001641
Iteration 132/1000 | Loss: 0.00001640
Iteration 133/1000 | Loss: 0.00001640
Iteration 134/1000 | Loss: 0.00001640
Iteration 135/1000 | Loss: 0.00001640
Iteration 136/1000 | Loss: 0.00001639
Iteration 137/1000 | Loss: 0.00001639
Iteration 138/1000 | Loss: 0.00001639
Iteration 139/1000 | Loss: 0.00001639
Iteration 140/1000 | Loss: 0.00001639
Iteration 141/1000 | Loss: 0.00001639
Iteration 142/1000 | Loss: 0.00001639
Iteration 143/1000 | Loss: 0.00001639
Iteration 144/1000 | Loss: 0.00001639
Iteration 145/1000 | Loss: 0.00001639
Iteration 146/1000 | Loss: 0.00001639
Iteration 147/1000 | Loss: 0.00001639
Iteration 148/1000 | Loss: 0.00001639
Iteration 149/1000 | Loss: 0.00001639
Iteration 150/1000 | Loss: 0.00001639
Iteration 151/1000 | Loss: 0.00001639
Iteration 152/1000 | Loss: 0.00001639
Iteration 153/1000 | Loss: 0.00001639
Iteration 154/1000 | Loss: 0.00001639
Iteration 155/1000 | Loss: 0.00001639
Iteration 156/1000 | Loss: 0.00001639
Iteration 157/1000 | Loss: 0.00001639
Iteration 158/1000 | Loss: 0.00001639
Iteration 159/1000 | Loss: 0.00001638
Iteration 160/1000 | Loss: 0.00001638
Iteration 161/1000 | Loss: 0.00001638
Iteration 162/1000 | Loss: 0.00001638
Iteration 163/1000 | Loss: 0.00001638
Iteration 164/1000 | Loss: 0.00001638
Iteration 165/1000 | Loss: 0.00001638
Iteration 166/1000 | Loss: 0.00001638
Iteration 167/1000 | Loss: 0.00001638
Iteration 168/1000 | Loss: 0.00001638
Iteration 169/1000 | Loss: 0.00001638
Iteration 170/1000 | Loss: 0.00001638
Iteration 171/1000 | Loss: 0.00001638
Iteration 172/1000 | Loss: 0.00001638
Iteration 173/1000 | Loss: 0.00001638
Iteration 174/1000 | Loss: 0.00001637
Iteration 175/1000 | Loss: 0.00001637
Iteration 176/1000 | Loss: 0.00001637
Iteration 177/1000 | Loss: 0.00001637
Iteration 178/1000 | Loss: 0.00001637
Iteration 179/1000 | Loss: 0.00001637
Iteration 180/1000 | Loss: 0.00001637
Iteration 181/1000 | Loss: 0.00001637
Iteration 182/1000 | Loss: 0.00001637
Iteration 183/1000 | Loss: 0.00001637
Iteration 184/1000 | Loss: 0.00001637
Iteration 185/1000 | Loss: 0.00001637
Iteration 186/1000 | Loss: 0.00001637
Iteration 187/1000 | Loss: 0.00001637
Iteration 188/1000 | Loss: 0.00001637
Iteration 189/1000 | Loss: 0.00001637
Iteration 190/1000 | Loss: 0.00001637
Iteration 191/1000 | Loss: 0.00001637
Iteration 192/1000 | Loss: 0.00001637
Iteration 193/1000 | Loss: 0.00001637
Iteration 194/1000 | Loss: 0.00001637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.6374740880564786e-05, 1.6374740880564786e-05, 1.6374740880564786e-05, 1.6374740880564786e-05, 1.6374740880564786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6374740880564786e-05

Optimization complete. Final v2v error: 3.3076090812683105 mm

Highest mean error: 5.105842113494873 mm for frame 58

Lowest mean error: 2.8538246154785156 mm for frame 135

Saving results

Total time: 40.76409316062927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061725
Iteration 2/25 | Loss: 0.01061725
Iteration 3/25 | Loss: 0.01061725
Iteration 4/25 | Loss: 0.01061724
Iteration 5/25 | Loss: 0.01061724
Iteration 6/25 | Loss: 0.01061724
Iteration 7/25 | Loss: 0.01061724
Iteration 8/25 | Loss: 0.01061724
Iteration 9/25 | Loss: 0.01061724
Iteration 10/25 | Loss: 0.01061724
Iteration 11/25 | Loss: 0.01061724
Iteration 12/25 | Loss: 0.01061723
Iteration 13/25 | Loss: 0.00242975
Iteration 14/25 | Loss: 0.00172713
Iteration 15/25 | Loss: 0.00130450
Iteration 16/25 | Loss: 0.00124208
Iteration 17/25 | Loss: 0.00121157
Iteration 18/25 | Loss: 0.00120054
Iteration 19/25 | Loss: 0.00118647
Iteration 20/25 | Loss: 0.00118379
Iteration 21/25 | Loss: 0.00118754
Iteration 22/25 | Loss: 0.00118419
Iteration 23/25 | Loss: 0.00118093
Iteration 24/25 | Loss: 0.00117981
Iteration 25/25 | Loss: 0.00117960

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34025121
Iteration 2/25 | Loss: 0.00118673
Iteration 3/25 | Loss: 0.00118673
Iteration 4/25 | Loss: 0.00118672
Iteration 5/25 | Loss: 0.00118672
Iteration 6/25 | Loss: 0.00118672
Iteration 7/25 | Loss: 0.00118672
Iteration 8/25 | Loss: 0.00118672
Iteration 9/25 | Loss: 0.00118672
Iteration 10/25 | Loss: 0.00118672
Iteration 11/25 | Loss: 0.00118672
Iteration 12/25 | Loss: 0.00118672
Iteration 13/25 | Loss: 0.00118672
Iteration 14/25 | Loss: 0.00118672
Iteration 15/25 | Loss: 0.00118672
Iteration 16/25 | Loss: 0.00118672
Iteration 17/25 | Loss: 0.00118672
Iteration 18/25 | Loss: 0.00118672
Iteration 19/25 | Loss: 0.00118672
Iteration 20/25 | Loss: 0.00118672
Iteration 21/25 | Loss: 0.00118672
Iteration 22/25 | Loss: 0.00118672
Iteration 23/25 | Loss: 0.00118672
Iteration 24/25 | Loss: 0.00118672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011867217253893614, 0.0011867217253893614, 0.0011867217253893614, 0.0011867217253893614, 0.0011867217253893614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011867217253893614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118672
Iteration 2/1000 | Loss: 0.00009708
Iteration 3/1000 | Loss: 0.00007060
Iteration 4/1000 | Loss: 0.00003563
Iteration 5/1000 | Loss: 0.00005422
Iteration 6/1000 | Loss: 0.00003014
Iteration 7/1000 | Loss: 0.00002907
Iteration 8/1000 | Loss: 0.00003439
Iteration 9/1000 | Loss: 0.00002952
Iteration 10/1000 | Loss: 0.00002755
Iteration 11/1000 | Loss: 0.00002731
Iteration 12/1000 | Loss: 0.00020315
Iteration 13/1000 | Loss: 0.00009204
Iteration 14/1000 | Loss: 0.00002884
Iteration 15/1000 | Loss: 0.00015402
Iteration 16/1000 | Loss: 0.00006047
Iteration 17/1000 | Loss: 0.00004757
Iteration 18/1000 | Loss: 0.00002672
Iteration 19/1000 | Loss: 0.00002658
Iteration 20/1000 | Loss: 0.00002642
Iteration 21/1000 | Loss: 0.00002639
Iteration 22/1000 | Loss: 0.00002635
Iteration 23/1000 | Loss: 0.00002635
Iteration 24/1000 | Loss: 0.00002634
Iteration 25/1000 | Loss: 0.00002631
Iteration 26/1000 | Loss: 0.00002631
Iteration 27/1000 | Loss: 0.00002631
Iteration 28/1000 | Loss: 0.00002630
Iteration 29/1000 | Loss: 0.00002630
Iteration 30/1000 | Loss: 0.00002629
Iteration 31/1000 | Loss: 0.00002628
Iteration 32/1000 | Loss: 0.00002628
Iteration 33/1000 | Loss: 0.00002628
Iteration 34/1000 | Loss: 0.00002628
Iteration 35/1000 | Loss: 0.00002627
Iteration 36/1000 | Loss: 0.00002627
Iteration 37/1000 | Loss: 0.00002627
Iteration 38/1000 | Loss: 0.00002627
Iteration 39/1000 | Loss: 0.00002627
Iteration 40/1000 | Loss: 0.00002627
Iteration 41/1000 | Loss: 0.00002626
Iteration 42/1000 | Loss: 0.00002626
Iteration 43/1000 | Loss: 0.00002626
Iteration 44/1000 | Loss: 0.00002625
Iteration 45/1000 | Loss: 0.00002625
Iteration 46/1000 | Loss: 0.00002624
Iteration 47/1000 | Loss: 0.00002622
Iteration 48/1000 | Loss: 0.00002621
Iteration 49/1000 | Loss: 0.00002619
Iteration 50/1000 | Loss: 0.00002618
Iteration 51/1000 | Loss: 0.00002617
Iteration 52/1000 | Loss: 0.00002616
Iteration 53/1000 | Loss: 0.00002615
Iteration 54/1000 | Loss: 0.00002615
Iteration 55/1000 | Loss: 0.00002614
Iteration 56/1000 | Loss: 0.00002610
Iteration 57/1000 | Loss: 0.00002609
Iteration 58/1000 | Loss: 0.00002609
Iteration 59/1000 | Loss: 0.00002608
Iteration 60/1000 | Loss: 0.00002608
Iteration 61/1000 | Loss: 0.00002607
Iteration 62/1000 | Loss: 0.00002607
Iteration 63/1000 | Loss: 0.00002606
Iteration 64/1000 | Loss: 0.00002606
Iteration 65/1000 | Loss: 0.00002606
Iteration 66/1000 | Loss: 0.00002604
Iteration 67/1000 | Loss: 0.00002604
Iteration 68/1000 | Loss: 0.00002604
Iteration 69/1000 | Loss: 0.00002603
Iteration 70/1000 | Loss: 0.00002603
Iteration 71/1000 | Loss: 0.00002602
Iteration 72/1000 | Loss: 0.00021588
Iteration 73/1000 | Loss: 0.00003354
Iteration 74/1000 | Loss: 0.00002849
Iteration 75/1000 | Loss: 0.00002730
Iteration 76/1000 | Loss: 0.00008756
Iteration 77/1000 | Loss: 0.00002851
Iteration 78/1000 | Loss: 0.00003454
Iteration 79/1000 | Loss: 0.00002607
Iteration 80/1000 | Loss: 0.00002603
Iteration 81/1000 | Loss: 0.00002603
Iteration 82/1000 | Loss: 0.00002603
Iteration 83/1000 | Loss: 0.00002602
Iteration 84/1000 | Loss: 0.00002600
Iteration 85/1000 | Loss: 0.00002600
Iteration 86/1000 | Loss: 0.00002598
Iteration 87/1000 | Loss: 0.00002598
Iteration 88/1000 | Loss: 0.00002598
Iteration 89/1000 | Loss: 0.00002597
Iteration 90/1000 | Loss: 0.00002597
Iteration 91/1000 | Loss: 0.00002597
Iteration 92/1000 | Loss: 0.00002597
Iteration 93/1000 | Loss: 0.00002596
Iteration 94/1000 | Loss: 0.00002596
Iteration 95/1000 | Loss: 0.00002596
Iteration 96/1000 | Loss: 0.00002596
Iteration 97/1000 | Loss: 0.00002596
Iteration 98/1000 | Loss: 0.00002596
Iteration 99/1000 | Loss: 0.00002595
Iteration 100/1000 | Loss: 0.00002595
Iteration 101/1000 | Loss: 0.00002595
Iteration 102/1000 | Loss: 0.00002595
Iteration 103/1000 | Loss: 0.00002595
Iteration 104/1000 | Loss: 0.00002594
Iteration 105/1000 | Loss: 0.00002594
Iteration 106/1000 | Loss: 0.00002594
Iteration 107/1000 | Loss: 0.00002594
Iteration 108/1000 | Loss: 0.00002593
Iteration 109/1000 | Loss: 0.00002593
Iteration 110/1000 | Loss: 0.00002593
Iteration 111/1000 | Loss: 0.00002593
Iteration 112/1000 | Loss: 0.00002592
Iteration 113/1000 | Loss: 0.00002592
Iteration 114/1000 | Loss: 0.00002592
Iteration 115/1000 | Loss: 0.00002592
Iteration 116/1000 | Loss: 0.00002592
Iteration 117/1000 | Loss: 0.00002591
Iteration 118/1000 | Loss: 0.00002591
Iteration 119/1000 | Loss: 0.00002591
Iteration 120/1000 | Loss: 0.00002591
Iteration 121/1000 | Loss: 0.00002591
Iteration 122/1000 | Loss: 0.00002591
Iteration 123/1000 | Loss: 0.00002591
Iteration 124/1000 | Loss: 0.00002591
Iteration 125/1000 | Loss: 0.00002591
Iteration 126/1000 | Loss: 0.00002591
Iteration 127/1000 | Loss: 0.00002591
Iteration 128/1000 | Loss: 0.00002591
Iteration 129/1000 | Loss: 0.00002591
Iteration 130/1000 | Loss: 0.00002590
Iteration 131/1000 | Loss: 0.00002590
Iteration 132/1000 | Loss: 0.00002590
Iteration 133/1000 | Loss: 0.00002590
Iteration 134/1000 | Loss: 0.00002590
Iteration 135/1000 | Loss: 0.00002590
Iteration 136/1000 | Loss: 0.00002590
Iteration 137/1000 | Loss: 0.00002590
Iteration 138/1000 | Loss: 0.00002590
Iteration 139/1000 | Loss: 0.00002590
Iteration 140/1000 | Loss: 0.00002590
Iteration 141/1000 | Loss: 0.00002590
Iteration 142/1000 | Loss: 0.00002590
Iteration 143/1000 | Loss: 0.00002590
Iteration 144/1000 | Loss: 0.00002590
Iteration 145/1000 | Loss: 0.00002590
Iteration 146/1000 | Loss: 0.00002590
Iteration 147/1000 | Loss: 0.00002590
Iteration 148/1000 | Loss: 0.00002590
Iteration 149/1000 | Loss: 0.00002590
Iteration 150/1000 | Loss: 0.00002590
Iteration 151/1000 | Loss: 0.00002590
Iteration 152/1000 | Loss: 0.00002590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.5896115403156728e-05, 2.5896115403156728e-05, 2.5896115403156728e-05, 2.5896115403156728e-05, 2.5896115403156728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5896115403156728e-05

Optimization complete. Final v2v error: 4.322001934051514 mm

Highest mean error: 4.657478332519531 mm for frame 8

Lowest mean error: 3.2463901042938232 mm for frame 0

Saving results

Total time: 82.73687243461609
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847165
Iteration 2/25 | Loss: 0.00115160
Iteration 3/25 | Loss: 0.00107130
Iteration 4/25 | Loss: 0.00106164
Iteration 5/25 | Loss: 0.00105815
Iteration 6/25 | Loss: 0.00105730
Iteration 7/25 | Loss: 0.00105730
Iteration 8/25 | Loss: 0.00105730
Iteration 9/25 | Loss: 0.00105730
Iteration 10/25 | Loss: 0.00105730
Iteration 11/25 | Loss: 0.00105730
Iteration 12/25 | Loss: 0.00105730
Iteration 13/25 | Loss: 0.00105730
Iteration 14/25 | Loss: 0.00105730
Iteration 15/25 | Loss: 0.00105730
Iteration 16/25 | Loss: 0.00105730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010573030449450016, 0.0010573030449450016, 0.0010573030449450016, 0.0010573030449450016, 0.0010573030449450016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010573030449450016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68160772
Iteration 2/25 | Loss: 0.00084780
Iteration 3/25 | Loss: 0.00084779
Iteration 4/25 | Loss: 0.00084779
Iteration 5/25 | Loss: 0.00084779
Iteration 6/25 | Loss: 0.00084779
Iteration 7/25 | Loss: 0.00084779
Iteration 8/25 | Loss: 0.00084779
Iteration 9/25 | Loss: 0.00084779
Iteration 10/25 | Loss: 0.00084779
Iteration 11/25 | Loss: 0.00084779
Iteration 12/25 | Loss: 0.00084779
Iteration 13/25 | Loss: 0.00084779
Iteration 14/25 | Loss: 0.00084779
Iteration 15/25 | Loss: 0.00084779
Iteration 16/25 | Loss: 0.00084779
Iteration 17/25 | Loss: 0.00084779
Iteration 18/25 | Loss: 0.00084779
Iteration 19/25 | Loss: 0.00084779
Iteration 20/25 | Loss: 0.00084779
Iteration 21/25 | Loss: 0.00084779
Iteration 22/25 | Loss: 0.00084779
Iteration 23/25 | Loss: 0.00084779
Iteration 24/25 | Loss: 0.00084779
Iteration 25/25 | Loss: 0.00084779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084779
Iteration 2/1000 | Loss: 0.00002064
Iteration 3/1000 | Loss: 0.00001384
Iteration 4/1000 | Loss: 0.00001112
Iteration 5/1000 | Loss: 0.00001040
Iteration 6/1000 | Loss: 0.00000994
Iteration 7/1000 | Loss: 0.00000979
Iteration 8/1000 | Loss: 0.00000960
Iteration 9/1000 | Loss: 0.00000935
Iteration 10/1000 | Loss: 0.00000934
Iteration 11/1000 | Loss: 0.00000934
Iteration 12/1000 | Loss: 0.00000932
Iteration 13/1000 | Loss: 0.00000916
Iteration 14/1000 | Loss: 0.00000903
Iteration 15/1000 | Loss: 0.00000896
Iteration 16/1000 | Loss: 0.00000896
Iteration 17/1000 | Loss: 0.00000895
Iteration 18/1000 | Loss: 0.00000895
Iteration 19/1000 | Loss: 0.00000890
Iteration 20/1000 | Loss: 0.00000889
Iteration 21/1000 | Loss: 0.00000889
Iteration 22/1000 | Loss: 0.00000888
Iteration 23/1000 | Loss: 0.00000888
Iteration 24/1000 | Loss: 0.00000888
Iteration 25/1000 | Loss: 0.00000888
Iteration 26/1000 | Loss: 0.00000886
Iteration 27/1000 | Loss: 0.00000885
Iteration 28/1000 | Loss: 0.00000885
Iteration 29/1000 | Loss: 0.00000884
Iteration 30/1000 | Loss: 0.00000884
Iteration 31/1000 | Loss: 0.00000883
Iteration 32/1000 | Loss: 0.00000882
Iteration 33/1000 | Loss: 0.00000880
Iteration 34/1000 | Loss: 0.00000879
Iteration 35/1000 | Loss: 0.00000879
Iteration 36/1000 | Loss: 0.00000878
Iteration 37/1000 | Loss: 0.00000878
Iteration 38/1000 | Loss: 0.00000877
Iteration 39/1000 | Loss: 0.00000877
Iteration 40/1000 | Loss: 0.00000876
Iteration 41/1000 | Loss: 0.00000875
Iteration 42/1000 | Loss: 0.00000875
Iteration 43/1000 | Loss: 0.00000874
Iteration 44/1000 | Loss: 0.00000874
Iteration 45/1000 | Loss: 0.00000874
Iteration 46/1000 | Loss: 0.00000874
Iteration 47/1000 | Loss: 0.00000873
Iteration 48/1000 | Loss: 0.00000873
Iteration 49/1000 | Loss: 0.00000873
Iteration 50/1000 | Loss: 0.00000873
Iteration 51/1000 | Loss: 0.00000872
Iteration 52/1000 | Loss: 0.00000872
Iteration 53/1000 | Loss: 0.00000871
Iteration 54/1000 | Loss: 0.00000871
Iteration 55/1000 | Loss: 0.00000871
Iteration 56/1000 | Loss: 0.00000870
Iteration 57/1000 | Loss: 0.00000870
Iteration 58/1000 | Loss: 0.00000870
Iteration 59/1000 | Loss: 0.00000869
Iteration 60/1000 | Loss: 0.00000869
Iteration 61/1000 | Loss: 0.00000869
Iteration 62/1000 | Loss: 0.00000868
Iteration 63/1000 | Loss: 0.00000868
Iteration 64/1000 | Loss: 0.00000867
Iteration 65/1000 | Loss: 0.00000867
Iteration 66/1000 | Loss: 0.00000866
Iteration 67/1000 | Loss: 0.00000866
Iteration 68/1000 | Loss: 0.00000866
Iteration 69/1000 | Loss: 0.00000866
Iteration 70/1000 | Loss: 0.00000866
Iteration 71/1000 | Loss: 0.00000866
Iteration 72/1000 | Loss: 0.00000866
Iteration 73/1000 | Loss: 0.00000865
Iteration 74/1000 | Loss: 0.00000865
Iteration 75/1000 | Loss: 0.00000864
Iteration 76/1000 | Loss: 0.00000864
Iteration 77/1000 | Loss: 0.00000864
Iteration 78/1000 | Loss: 0.00000863
Iteration 79/1000 | Loss: 0.00000863
Iteration 80/1000 | Loss: 0.00000863
Iteration 81/1000 | Loss: 0.00000863
Iteration 82/1000 | Loss: 0.00000863
Iteration 83/1000 | Loss: 0.00000863
Iteration 84/1000 | Loss: 0.00000863
Iteration 85/1000 | Loss: 0.00000863
Iteration 86/1000 | Loss: 0.00000863
Iteration 87/1000 | Loss: 0.00000862
Iteration 88/1000 | Loss: 0.00000862
Iteration 89/1000 | Loss: 0.00000862
Iteration 90/1000 | Loss: 0.00000861
Iteration 91/1000 | Loss: 0.00000861
Iteration 92/1000 | Loss: 0.00000861
Iteration 93/1000 | Loss: 0.00000861
Iteration 94/1000 | Loss: 0.00000860
Iteration 95/1000 | Loss: 0.00000860
Iteration 96/1000 | Loss: 0.00000860
Iteration 97/1000 | Loss: 0.00000860
Iteration 98/1000 | Loss: 0.00000860
Iteration 99/1000 | Loss: 0.00000860
Iteration 100/1000 | Loss: 0.00000860
Iteration 101/1000 | Loss: 0.00000860
Iteration 102/1000 | Loss: 0.00000860
Iteration 103/1000 | Loss: 0.00000860
Iteration 104/1000 | Loss: 0.00000859
Iteration 105/1000 | Loss: 0.00000859
Iteration 106/1000 | Loss: 0.00000859
Iteration 107/1000 | Loss: 0.00000859
Iteration 108/1000 | Loss: 0.00000858
Iteration 109/1000 | Loss: 0.00000858
Iteration 110/1000 | Loss: 0.00000858
Iteration 111/1000 | Loss: 0.00000858
Iteration 112/1000 | Loss: 0.00000858
Iteration 113/1000 | Loss: 0.00000858
Iteration 114/1000 | Loss: 0.00000858
Iteration 115/1000 | Loss: 0.00000858
Iteration 116/1000 | Loss: 0.00000858
Iteration 117/1000 | Loss: 0.00000858
Iteration 118/1000 | Loss: 0.00000857
Iteration 119/1000 | Loss: 0.00000857
Iteration 120/1000 | Loss: 0.00000857
Iteration 121/1000 | Loss: 0.00000857
Iteration 122/1000 | Loss: 0.00000857
Iteration 123/1000 | Loss: 0.00000857
Iteration 124/1000 | Loss: 0.00000857
Iteration 125/1000 | Loss: 0.00000857
Iteration 126/1000 | Loss: 0.00000857
Iteration 127/1000 | Loss: 0.00000857
Iteration 128/1000 | Loss: 0.00000857
Iteration 129/1000 | Loss: 0.00000857
Iteration 130/1000 | Loss: 0.00000857
Iteration 131/1000 | Loss: 0.00000857
Iteration 132/1000 | Loss: 0.00000856
Iteration 133/1000 | Loss: 0.00000856
Iteration 134/1000 | Loss: 0.00000856
Iteration 135/1000 | Loss: 0.00000856
Iteration 136/1000 | Loss: 0.00000856
Iteration 137/1000 | Loss: 0.00000856
Iteration 138/1000 | Loss: 0.00000856
Iteration 139/1000 | Loss: 0.00000856
Iteration 140/1000 | Loss: 0.00000856
Iteration 141/1000 | Loss: 0.00000856
Iteration 142/1000 | Loss: 0.00000856
Iteration 143/1000 | Loss: 0.00000855
Iteration 144/1000 | Loss: 0.00000855
Iteration 145/1000 | Loss: 0.00000855
Iteration 146/1000 | Loss: 0.00000855
Iteration 147/1000 | Loss: 0.00000855
Iteration 148/1000 | Loss: 0.00000855
Iteration 149/1000 | Loss: 0.00000855
Iteration 150/1000 | Loss: 0.00000854
Iteration 151/1000 | Loss: 0.00000854
Iteration 152/1000 | Loss: 0.00000854
Iteration 153/1000 | Loss: 0.00000854
Iteration 154/1000 | Loss: 0.00000854
Iteration 155/1000 | Loss: 0.00000854
Iteration 156/1000 | Loss: 0.00000854
Iteration 157/1000 | Loss: 0.00000854
Iteration 158/1000 | Loss: 0.00000854
Iteration 159/1000 | Loss: 0.00000854
Iteration 160/1000 | Loss: 0.00000853
Iteration 161/1000 | Loss: 0.00000853
Iteration 162/1000 | Loss: 0.00000853
Iteration 163/1000 | Loss: 0.00000852
Iteration 164/1000 | Loss: 0.00000852
Iteration 165/1000 | Loss: 0.00000852
Iteration 166/1000 | Loss: 0.00000852
Iteration 167/1000 | Loss: 0.00000852
Iteration 168/1000 | Loss: 0.00000852
Iteration 169/1000 | Loss: 0.00000852
Iteration 170/1000 | Loss: 0.00000852
Iteration 171/1000 | Loss: 0.00000852
Iteration 172/1000 | Loss: 0.00000851
Iteration 173/1000 | Loss: 0.00000851
Iteration 174/1000 | Loss: 0.00000851
Iteration 175/1000 | Loss: 0.00000851
Iteration 176/1000 | Loss: 0.00000851
Iteration 177/1000 | Loss: 0.00000851
Iteration 178/1000 | Loss: 0.00000851
Iteration 179/1000 | Loss: 0.00000851
Iteration 180/1000 | Loss: 0.00000850
Iteration 181/1000 | Loss: 0.00000850
Iteration 182/1000 | Loss: 0.00000850
Iteration 183/1000 | Loss: 0.00000849
Iteration 184/1000 | Loss: 0.00000849
Iteration 185/1000 | Loss: 0.00000849
Iteration 186/1000 | Loss: 0.00000849
Iteration 187/1000 | Loss: 0.00000849
Iteration 188/1000 | Loss: 0.00000849
Iteration 189/1000 | Loss: 0.00000849
Iteration 190/1000 | Loss: 0.00000849
Iteration 191/1000 | Loss: 0.00000849
Iteration 192/1000 | Loss: 0.00000849
Iteration 193/1000 | Loss: 0.00000849
Iteration 194/1000 | Loss: 0.00000848
Iteration 195/1000 | Loss: 0.00000848
Iteration 196/1000 | Loss: 0.00000848
Iteration 197/1000 | Loss: 0.00000848
Iteration 198/1000 | Loss: 0.00000848
Iteration 199/1000 | Loss: 0.00000848
Iteration 200/1000 | Loss: 0.00000848
Iteration 201/1000 | Loss: 0.00000848
Iteration 202/1000 | Loss: 0.00000848
Iteration 203/1000 | Loss: 0.00000848
Iteration 204/1000 | Loss: 0.00000848
Iteration 205/1000 | Loss: 0.00000847
Iteration 206/1000 | Loss: 0.00000847
Iteration 207/1000 | Loss: 0.00000847
Iteration 208/1000 | Loss: 0.00000847
Iteration 209/1000 | Loss: 0.00000847
Iteration 210/1000 | Loss: 0.00000847
Iteration 211/1000 | Loss: 0.00000847
Iteration 212/1000 | Loss: 0.00000847
Iteration 213/1000 | Loss: 0.00000847
Iteration 214/1000 | Loss: 0.00000847
Iteration 215/1000 | Loss: 0.00000847
Iteration 216/1000 | Loss: 0.00000847
Iteration 217/1000 | Loss: 0.00000847
Iteration 218/1000 | Loss: 0.00000847
Iteration 219/1000 | Loss: 0.00000847
Iteration 220/1000 | Loss: 0.00000847
Iteration 221/1000 | Loss: 0.00000847
Iteration 222/1000 | Loss: 0.00000847
Iteration 223/1000 | Loss: 0.00000847
Iteration 224/1000 | Loss: 0.00000847
Iteration 225/1000 | Loss: 0.00000847
Iteration 226/1000 | Loss: 0.00000847
Iteration 227/1000 | Loss: 0.00000847
Iteration 228/1000 | Loss: 0.00000847
Iteration 229/1000 | Loss: 0.00000847
Iteration 230/1000 | Loss: 0.00000847
Iteration 231/1000 | Loss: 0.00000847
Iteration 232/1000 | Loss: 0.00000847
Iteration 233/1000 | Loss: 0.00000847
Iteration 234/1000 | Loss: 0.00000847
Iteration 235/1000 | Loss: 0.00000847
Iteration 236/1000 | Loss: 0.00000847
Iteration 237/1000 | Loss: 0.00000847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [8.466873623547144e-06, 8.466873623547144e-06, 8.466873623547144e-06, 8.466873623547144e-06, 8.466873623547144e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.466873623547144e-06

Optimization complete. Final v2v error: 2.493687629699707 mm

Highest mean error: 2.8721115589141846 mm for frame 57

Lowest mean error: 2.300525665283203 mm for frame 129

Saving results

Total time: 39.216134786605835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438973
Iteration 2/25 | Loss: 0.00116660
Iteration 3/25 | Loss: 0.00110517
Iteration 4/25 | Loss: 0.00109125
Iteration 5/25 | Loss: 0.00108693
Iteration 6/25 | Loss: 0.00108635
Iteration 7/25 | Loss: 0.00108635
Iteration 8/25 | Loss: 0.00108635
Iteration 9/25 | Loss: 0.00108635
Iteration 10/25 | Loss: 0.00108635
Iteration 11/25 | Loss: 0.00108635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010863466886803508, 0.0010863466886803508, 0.0010863466886803508, 0.0010863466886803508, 0.0010863466886803508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010863466886803508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36510515
Iteration 2/25 | Loss: 0.00082029
Iteration 3/25 | Loss: 0.00082029
Iteration 4/25 | Loss: 0.00082029
Iteration 5/25 | Loss: 0.00082029
Iteration 6/25 | Loss: 0.00082029
Iteration 7/25 | Loss: 0.00082029
Iteration 8/25 | Loss: 0.00082029
Iteration 9/25 | Loss: 0.00082029
Iteration 10/25 | Loss: 0.00082029
Iteration 11/25 | Loss: 0.00082029
Iteration 12/25 | Loss: 0.00082029
Iteration 13/25 | Loss: 0.00082029
Iteration 14/25 | Loss: 0.00082029
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000820290413685143, 0.000820290413685143, 0.000820290413685143, 0.000820290413685143, 0.000820290413685143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000820290413685143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082029
Iteration 2/1000 | Loss: 0.00002488
Iteration 3/1000 | Loss: 0.00001804
Iteration 4/1000 | Loss: 0.00001650
Iteration 5/1000 | Loss: 0.00001568
Iteration 6/1000 | Loss: 0.00001508
Iteration 7/1000 | Loss: 0.00001463
Iteration 8/1000 | Loss: 0.00001429
Iteration 9/1000 | Loss: 0.00001395
Iteration 10/1000 | Loss: 0.00001375
Iteration 11/1000 | Loss: 0.00001368
Iteration 12/1000 | Loss: 0.00001364
Iteration 13/1000 | Loss: 0.00001361
Iteration 14/1000 | Loss: 0.00001360
Iteration 15/1000 | Loss: 0.00001360
Iteration 16/1000 | Loss: 0.00001358
Iteration 17/1000 | Loss: 0.00001352
Iteration 18/1000 | Loss: 0.00001348
Iteration 19/1000 | Loss: 0.00001340
Iteration 20/1000 | Loss: 0.00001339
Iteration 21/1000 | Loss: 0.00001335
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001335
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001335
Iteration 26/1000 | Loss: 0.00001335
Iteration 27/1000 | Loss: 0.00001335
Iteration 28/1000 | Loss: 0.00001335
Iteration 29/1000 | Loss: 0.00001335
Iteration 30/1000 | Loss: 0.00001335
Iteration 31/1000 | Loss: 0.00001335
Iteration 32/1000 | Loss: 0.00001335
Iteration 33/1000 | Loss: 0.00001334
Iteration 34/1000 | Loss: 0.00001334
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001332
Iteration 37/1000 | Loss: 0.00001331
Iteration 38/1000 | Loss: 0.00001331
Iteration 39/1000 | Loss: 0.00001330
Iteration 40/1000 | Loss: 0.00001328
Iteration 41/1000 | Loss: 0.00001327
Iteration 42/1000 | Loss: 0.00001327
Iteration 43/1000 | Loss: 0.00001326
Iteration 44/1000 | Loss: 0.00001324
Iteration 45/1000 | Loss: 0.00001323
Iteration 46/1000 | Loss: 0.00001323
Iteration 47/1000 | Loss: 0.00001322
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001319
Iteration 51/1000 | Loss: 0.00001319
Iteration 52/1000 | Loss: 0.00001319
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001318
Iteration 55/1000 | Loss: 0.00001318
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001317
Iteration 58/1000 | Loss: 0.00001316
Iteration 59/1000 | Loss: 0.00001316
Iteration 60/1000 | Loss: 0.00001315
Iteration 61/1000 | Loss: 0.00001315
Iteration 62/1000 | Loss: 0.00001314
Iteration 63/1000 | Loss: 0.00001314
Iteration 64/1000 | Loss: 0.00001314
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001310
Iteration 68/1000 | Loss: 0.00001309
Iteration 69/1000 | Loss: 0.00001308
Iteration 70/1000 | Loss: 0.00001304
Iteration 71/1000 | Loss: 0.00001304
Iteration 72/1000 | Loss: 0.00001303
Iteration 73/1000 | Loss: 0.00001303
Iteration 74/1000 | Loss: 0.00001303
Iteration 75/1000 | Loss: 0.00001303
Iteration 76/1000 | Loss: 0.00001302
Iteration 77/1000 | Loss: 0.00001302
Iteration 78/1000 | Loss: 0.00001299
Iteration 79/1000 | Loss: 0.00001299
Iteration 80/1000 | Loss: 0.00001298
Iteration 81/1000 | Loss: 0.00001297
Iteration 82/1000 | Loss: 0.00001297
Iteration 83/1000 | Loss: 0.00001297
Iteration 84/1000 | Loss: 0.00001296
Iteration 85/1000 | Loss: 0.00001296
Iteration 86/1000 | Loss: 0.00001295
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001295
Iteration 89/1000 | Loss: 0.00001295
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001295
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001294
Iteration 95/1000 | Loss: 0.00001294
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001294
Iteration 101/1000 | Loss: 0.00001294
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001293
Iteration 106/1000 | Loss: 0.00001293
Iteration 107/1000 | Loss: 0.00001293
Iteration 108/1000 | Loss: 0.00001292
Iteration 109/1000 | Loss: 0.00001292
Iteration 110/1000 | Loss: 0.00001292
Iteration 111/1000 | Loss: 0.00001292
Iteration 112/1000 | Loss: 0.00001292
Iteration 113/1000 | Loss: 0.00001292
Iteration 114/1000 | Loss: 0.00001292
Iteration 115/1000 | Loss: 0.00001292
Iteration 116/1000 | Loss: 0.00001292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.2920273547933903e-05, 1.2920273547933903e-05, 1.2920273547933903e-05, 1.2920273547933903e-05, 1.2920273547933903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2920273547933903e-05

Optimization complete. Final v2v error: 3.0671446323394775 mm

Highest mean error: 3.2153401374816895 mm for frame 140

Lowest mean error: 2.9250996112823486 mm for frame 19

Saving results

Total time: 36.33981657028198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397736
Iteration 2/25 | Loss: 0.00114180
Iteration 3/25 | Loss: 0.00106961
Iteration 4/25 | Loss: 0.00105874
Iteration 5/25 | Loss: 0.00105507
Iteration 6/25 | Loss: 0.00105427
Iteration 7/25 | Loss: 0.00105427
Iteration 8/25 | Loss: 0.00105427
Iteration 9/25 | Loss: 0.00105427
Iteration 10/25 | Loss: 0.00105427
Iteration 11/25 | Loss: 0.00105427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010542693780735135, 0.0010542693780735135, 0.0010542693780735135, 0.0010542693780735135, 0.0010542693780735135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010542693780735135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86253572
Iteration 2/25 | Loss: 0.00081153
Iteration 3/25 | Loss: 0.00081153
Iteration 4/25 | Loss: 0.00081153
Iteration 5/25 | Loss: 0.00081153
Iteration 6/25 | Loss: 0.00081153
Iteration 7/25 | Loss: 0.00081153
Iteration 8/25 | Loss: 0.00081153
Iteration 9/25 | Loss: 0.00081153
Iteration 10/25 | Loss: 0.00081153
Iteration 11/25 | Loss: 0.00081153
Iteration 12/25 | Loss: 0.00081153
Iteration 13/25 | Loss: 0.00081153
Iteration 14/25 | Loss: 0.00081153
Iteration 15/25 | Loss: 0.00081153
Iteration 16/25 | Loss: 0.00081153
Iteration 17/25 | Loss: 0.00081153
Iteration 18/25 | Loss: 0.00081153
Iteration 19/25 | Loss: 0.00081153
Iteration 20/25 | Loss: 0.00081153
Iteration 21/25 | Loss: 0.00081153
Iteration 22/25 | Loss: 0.00081153
Iteration 23/25 | Loss: 0.00081153
Iteration 24/25 | Loss: 0.00081153
Iteration 25/25 | Loss: 0.00081153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081153
Iteration 2/1000 | Loss: 0.00002330
Iteration 3/1000 | Loss: 0.00001501
Iteration 4/1000 | Loss: 0.00001240
Iteration 5/1000 | Loss: 0.00001168
Iteration 6/1000 | Loss: 0.00001114
Iteration 7/1000 | Loss: 0.00001082
Iteration 8/1000 | Loss: 0.00001048
Iteration 9/1000 | Loss: 0.00001040
Iteration 10/1000 | Loss: 0.00001031
Iteration 11/1000 | Loss: 0.00001029
Iteration 12/1000 | Loss: 0.00001028
Iteration 13/1000 | Loss: 0.00001027
Iteration 14/1000 | Loss: 0.00001018
Iteration 15/1000 | Loss: 0.00001004
Iteration 16/1000 | Loss: 0.00000993
Iteration 17/1000 | Loss: 0.00000987
Iteration 18/1000 | Loss: 0.00000987
Iteration 19/1000 | Loss: 0.00000982
Iteration 20/1000 | Loss: 0.00000979
Iteration 21/1000 | Loss: 0.00000979
Iteration 22/1000 | Loss: 0.00000978
Iteration 23/1000 | Loss: 0.00000972
Iteration 24/1000 | Loss: 0.00000971
Iteration 25/1000 | Loss: 0.00000969
Iteration 26/1000 | Loss: 0.00000969
Iteration 27/1000 | Loss: 0.00000968
Iteration 28/1000 | Loss: 0.00000968
Iteration 29/1000 | Loss: 0.00000967
Iteration 30/1000 | Loss: 0.00000967
Iteration 31/1000 | Loss: 0.00000967
Iteration 32/1000 | Loss: 0.00000967
Iteration 33/1000 | Loss: 0.00000967
Iteration 34/1000 | Loss: 0.00000967
Iteration 35/1000 | Loss: 0.00000966
Iteration 36/1000 | Loss: 0.00000966
Iteration 37/1000 | Loss: 0.00000965
Iteration 38/1000 | Loss: 0.00000965
Iteration 39/1000 | Loss: 0.00000965
Iteration 40/1000 | Loss: 0.00000965
Iteration 41/1000 | Loss: 0.00000964
Iteration 42/1000 | Loss: 0.00000963
Iteration 43/1000 | Loss: 0.00000962
Iteration 44/1000 | Loss: 0.00000962
Iteration 45/1000 | Loss: 0.00000962
Iteration 46/1000 | Loss: 0.00000962
Iteration 47/1000 | Loss: 0.00000962
Iteration 48/1000 | Loss: 0.00000962
Iteration 49/1000 | Loss: 0.00000961
Iteration 50/1000 | Loss: 0.00000961
Iteration 51/1000 | Loss: 0.00000961
Iteration 52/1000 | Loss: 0.00000960
Iteration 53/1000 | Loss: 0.00000960
Iteration 54/1000 | Loss: 0.00000960
Iteration 55/1000 | Loss: 0.00000959
Iteration 56/1000 | Loss: 0.00000958
Iteration 57/1000 | Loss: 0.00000958
Iteration 58/1000 | Loss: 0.00000958
Iteration 59/1000 | Loss: 0.00000957
Iteration 60/1000 | Loss: 0.00000957
Iteration 61/1000 | Loss: 0.00000957
Iteration 62/1000 | Loss: 0.00000957
Iteration 63/1000 | Loss: 0.00000957
Iteration 64/1000 | Loss: 0.00000957
Iteration 65/1000 | Loss: 0.00000956
Iteration 66/1000 | Loss: 0.00000956
Iteration 67/1000 | Loss: 0.00000956
Iteration 68/1000 | Loss: 0.00000956
Iteration 69/1000 | Loss: 0.00000956
Iteration 70/1000 | Loss: 0.00000956
Iteration 71/1000 | Loss: 0.00000956
Iteration 72/1000 | Loss: 0.00000956
Iteration 73/1000 | Loss: 0.00000956
Iteration 74/1000 | Loss: 0.00000956
Iteration 75/1000 | Loss: 0.00000956
Iteration 76/1000 | Loss: 0.00000956
Iteration 77/1000 | Loss: 0.00000956
Iteration 78/1000 | Loss: 0.00000956
Iteration 79/1000 | Loss: 0.00000956
Iteration 80/1000 | Loss: 0.00000956
Iteration 81/1000 | Loss: 0.00000956
Iteration 82/1000 | Loss: 0.00000956
Iteration 83/1000 | Loss: 0.00000956
Iteration 84/1000 | Loss: 0.00000956
Iteration 85/1000 | Loss: 0.00000956
Iteration 86/1000 | Loss: 0.00000956
Iteration 87/1000 | Loss: 0.00000956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [9.56153598963283e-06, 9.56153598963283e-06, 9.56153598963283e-06, 9.56153598963283e-06, 9.56153598963283e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.56153598963283e-06

Optimization complete. Final v2v error: 2.6536381244659424 mm

Highest mean error: 3.1464741230010986 mm for frame 55

Lowest mean error: 2.4725213050842285 mm for frame 89

Saving results

Total time: 31.176145553588867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054542
Iteration 2/25 | Loss: 0.00148536
Iteration 3/25 | Loss: 0.00122337
Iteration 4/25 | Loss: 0.00119504
Iteration 5/25 | Loss: 0.00118741
Iteration 6/25 | Loss: 0.00118532
Iteration 7/25 | Loss: 0.00118475
Iteration 8/25 | Loss: 0.00118475
Iteration 9/25 | Loss: 0.00118475
Iteration 10/25 | Loss: 0.00118475
Iteration 11/25 | Loss: 0.00118475
Iteration 12/25 | Loss: 0.00118475
Iteration 13/25 | Loss: 0.00118475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011847511632367969, 0.0011847511632367969, 0.0011847511632367969, 0.0011847511632367969, 0.0011847511632367969]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011847511632367969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09237349
Iteration 2/25 | Loss: 0.00100155
Iteration 3/25 | Loss: 0.00100154
Iteration 4/25 | Loss: 0.00100154
Iteration 5/25 | Loss: 0.00100154
Iteration 6/25 | Loss: 0.00100154
Iteration 7/25 | Loss: 0.00100154
Iteration 8/25 | Loss: 0.00100154
Iteration 9/25 | Loss: 0.00100154
Iteration 10/25 | Loss: 0.00100154
Iteration 11/25 | Loss: 0.00100154
Iteration 12/25 | Loss: 0.00100154
Iteration 13/25 | Loss: 0.00100154
Iteration 14/25 | Loss: 0.00100154
Iteration 15/25 | Loss: 0.00100154
Iteration 16/25 | Loss: 0.00100154
Iteration 17/25 | Loss: 0.00100154
Iteration 18/25 | Loss: 0.00100154
Iteration 19/25 | Loss: 0.00100154
Iteration 20/25 | Loss: 0.00100154
Iteration 21/25 | Loss: 0.00100154
Iteration 22/25 | Loss: 0.00100154
Iteration 23/25 | Loss: 0.00100154
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001001537311822176, 0.001001537311822176, 0.001001537311822176, 0.001001537311822176, 0.001001537311822176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001001537311822176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100154
Iteration 2/1000 | Loss: 0.00005088
Iteration 3/1000 | Loss: 0.00003063
Iteration 4/1000 | Loss: 0.00002680
Iteration 5/1000 | Loss: 0.00002508
Iteration 6/1000 | Loss: 0.00002394
Iteration 7/1000 | Loss: 0.00002328
Iteration 8/1000 | Loss: 0.00002272
Iteration 9/1000 | Loss: 0.00002235
Iteration 10/1000 | Loss: 0.00002206
Iteration 11/1000 | Loss: 0.00002187
Iteration 12/1000 | Loss: 0.00002179
Iteration 13/1000 | Loss: 0.00002179
Iteration 14/1000 | Loss: 0.00002178
Iteration 15/1000 | Loss: 0.00002174
Iteration 16/1000 | Loss: 0.00002160
Iteration 17/1000 | Loss: 0.00002158
Iteration 18/1000 | Loss: 0.00002157
Iteration 19/1000 | Loss: 0.00002155
Iteration 20/1000 | Loss: 0.00002151
Iteration 21/1000 | Loss: 0.00002138
Iteration 22/1000 | Loss: 0.00002135
Iteration 23/1000 | Loss: 0.00002134
Iteration 24/1000 | Loss: 0.00002131
Iteration 25/1000 | Loss: 0.00002130
Iteration 26/1000 | Loss: 0.00002130
Iteration 27/1000 | Loss: 0.00002126
Iteration 28/1000 | Loss: 0.00002121
Iteration 29/1000 | Loss: 0.00002121
Iteration 30/1000 | Loss: 0.00002120
Iteration 31/1000 | Loss: 0.00002115
Iteration 32/1000 | Loss: 0.00002114
Iteration 33/1000 | Loss: 0.00002113
Iteration 34/1000 | Loss: 0.00002107
Iteration 35/1000 | Loss: 0.00002106
Iteration 36/1000 | Loss: 0.00002106
Iteration 37/1000 | Loss: 0.00002105
Iteration 38/1000 | Loss: 0.00002105
Iteration 39/1000 | Loss: 0.00002104
Iteration 40/1000 | Loss: 0.00002104
Iteration 41/1000 | Loss: 0.00002104
Iteration 42/1000 | Loss: 0.00002104
Iteration 43/1000 | Loss: 0.00002103
Iteration 44/1000 | Loss: 0.00002103
Iteration 45/1000 | Loss: 0.00002103
Iteration 46/1000 | Loss: 0.00002103
Iteration 47/1000 | Loss: 0.00002103
Iteration 48/1000 | Loss: 0.00002103
Iteration 49/1000 | Loss: 0.00002103
Iteration 50/1000 | Loss: 0.00002103
Iteration 51/1000 | Loss: 0.00002103
Iteration 52/1000 | Loss: 0.00002103
Iteration 53/1000 | Loss: 0.00002102
Iteration 54/1000 | Loss: 0.00002102
Iteration 55/1000 | Loss: 0.00002102
Iteration 56/1000 | Loss: 0.00002102
Iteration 57/1000 | Loss: 0.00002102
Iteration 58/1000 | Loss: 0.00002101
Iteration 59/1000 | Loss: 0.00002101
Iteration 60/1000 | Loss: 0.00002099
Iteration 61/1000 | Loss: 0.00002099
Iteration 62/1000 | Loss: 0.00002099
Iteration 63/1000 | Loss: 0.00002099
Iteration 64/1000 | Loss: 0.00002099
Iteration 65/1000 | Loss: 0.00002099
Iteration 66/1000 | Loss: 0.00002099
Iteration 67/1000 | Loss: 0.00002099
Iteration 68/1000 | Loss: 0.00002099
Iteration 69/1000 | Loss: 0.00002099
Iteration 70/1000 | Loss: 0.00002099
Iteration 71/1000 | Loss: 0.00002099
Iteration 72/1000 | Loss: 0.00002098
Iteration 73/1000 | Loss: 0.00002098
Iteration 74/1000 | Loss: 0.00002097
Iteration 75/1000 | Loss: 0.00002097
Iteration 76/1000 | Loss: 0.00002097
Iteration 77/1000 | Loss: 0.00002097
Iteration 78/1000 | Loss: 0.00002097
Iteration 79/1000 | Loss: 0.00002096
Iteration 80/1000 | Loss: 0.00002096
Iteration 81/1000 | Loss: 0.00002096
Iteration 82/1000 | Loss: 0.00002096
Iteration 83/1000 | Loss: 0.00002096
Iteration 84/1000 | Loss: 0.00002096
Iteration 85/1000 | Loss: 0.00002096
Iteration 86/1000 | Loss: 0.00002096
Iteration 87/1000 | Loss: 0.00002096
Iteration 88/1000 | Loss: 0.00002095
Iteration 89/1000 | Loss: 0.00002095
Iteration 90/1000 | Loss: 0.00002095
Iteration 91/1000 | Loss: 0.00002094
Iteration 92/1000 | Loss: 0.00002094
Iteration 93/1000 | Loss: 0.00002094
Iteration 94/1000 | Loss: 0.00002094
Iteration 95/1000 | Loss: 0.00002093
Iteration 96/1000 | Loss: 0.00002093
Iteration 97/1000 | Loss: 0.00002093
Iteration 98/1000 | Loss: 0.00002092
Iteration 99/1000 | Loss: 0.00002091
Iteration 100/1000 | Loss: 0.00002091
Iteration 101/1000 | Loss: 0.00002091
Iteration 102/1000 | Loss: 0.00002091
Iteration 103/1000 | Loss: 0.00002090
Iteration 104/1000 | Loss: 0.00002090
Iteration 105/1000 | Loss: 0.00002090
Iteration 106/1000 | Loss: 0.00002090
Iteration 107/1000 | Loss: 0.00002090
Iteration 108/1000 | Loss: 0.00002089
Iteration 109/1000 | Loss: 0.00002089
Iteration 110/1000 | Loss: 0.00002089
Iteration 111/1000 | Loss: 0.00002089
Iteration 112/1000 | Loss: 0.00002089
Iteration 113/1000 | Loss: 0.00002089
Iteration 114/1000 | Loss: 0.00002089
Iteration 115/1000 | Loss: 0.00002088
Iteration 116/1000 | Loss: 0.00002088
Iteration 117/1000 | Loss: 0.00002088
Iteration 118/1000 | Loss: 0.00002088
Iteration 119/1000 | Loss: 0.00002088
Iteration 120/1000 | Loss: 0.00002088
Iteration 121/1000 | Loss: 0.00002088
Iteration 122/1000 | Loss: 0.00002088
Iteration 123/1000 | Loss: 0.00002087
Iteration 124/1000 | Loss: 0.00002087
Iteration 125/1000 | Loss: 0.00002087
Iteration 126/1000 | Loss: 0.00002087
Iteration 127/1000 | Loss: 0.00002087
Iteration 128/1000 | Loss: 0.00002087
Iteration 129/1000 | Loss: 0.00002087
Iteration 130/1000 | Loss: 0.00002086
Iteration 131/1000 | Loss: 0.00002086
Iteration 132/1000 | Loss: 0.00002086
Iteration 133/1000 | Loss: 0.00002086
Iteration 134/1000 | Loss: 0.00002086
Iteration 135/1000 | Loss: 0.00002085
Iteration 136/1000 | Loss: 0.00002085
Iteration 137/1000 | Loss: 0.00002085
Iteration 138/1000 | Loss: 0.00002084
Iteration 139/1000 | Loss: 0.00002084
Iteration 140/1000 | Loss: 0.00002084
Iteration 141/1000 | Loss: 0.00002083
Iteration 142/1000 | Loss: 0.00002083
Iteration 143/1000 | Loss: 0.00002083
Iteration 144/1000 | Loss: 0.00002083
Iteration 145/1000 | Loss: 0.00002083
Iteration 146/1000 | Loss: 0.00002083
Iteration 147/1000 | Loss: 0.00002083
Iteration 148/1000 | Loss: 0.00002082
Iteration 149/1000 | Loss: 0.00002082
Iteration 150/1000 | Loss: 0.00002082
Iteration 151/1000 | Loss: 0.00002082
Iteration 152/1000 | Loss: 0.00002081
Iteration 153/1000 | Loss: 0.00002081
Iteration 154/1000 | Loss: 0.00002081
Iteration 155/1000 | Loss: 0.00002081
Iteration 156/1000 | Loss: 0.00002081
Iteration 157/1000 | Loss: 0.00002081
Iteration 158/1000 | Loss: 0.00002081
Iteration 159/1000 | Loss: 0.00002081
Iteration 160/1000 | Loss: 0.00002081
Iteration 161/1000 | Loss: 0.00002081
Iteration 162/1000 | Loss: 0.00002081
Iteration 163/1000 | Loss: 0.00002080
Iteration 164/1000 | Loss: 0.00002080
Iteration 165/1000 | Loss: 0.00002080
Iteration 166/1000 | Loss: 0.00002080
Iteration 167/1000 | Loss: 0.00002080
Iteration 168/1000 | Loss: 0.00002080
Iteration 169/1000 | Loss: 0.00002080
Iteration 170/1000 | Loss: 0.00002080
Iteration 171/1000 | Loss: 0.00002080
Iteration 172/1000 | Loss: 0.00002080
Iteration 173/1000 | Loss: 0.00002080
Iteration 174/1000 | Loss: 0.00002080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.080467857012991e-05, 2.080467857012991e-05, 2.080467857012991e-05, 2.080467857012991e-05, 2.080467857012991e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.080467857012991e-05

Optimization complete. Final v2v error: 3.71657657623291 mm

Highest mean error: 5.1843085289001465 mm for frame 47

Lowest mean error: 3.2331087589263916 mm for frame 150

Saving results

Total time: 48.023372650146484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478331
Iteration 2/25 | Loss: 0.00118350
Iteration 3/25 | Loss: 0.00109499
Iteration 4/25 | Loss: 0.00108506
Iteration 5/25 | Loss: 0.00108259
Iteration 6/25 | Loss: 0.00108259
Iteration 7/25 | Loss: 0.00108259
Iteration 8/25 | Loss: 0.00108259
Iteration 9/25 | Loss: 0.00108259
Iteration 10/25 | Loss: 0.00108259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010825907811522484, 0.0010825907811522484, 0.0010825907811522484, 0.0010825907811522484, 0.0010825907811522484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010825907811522484

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.72814512
Iteration 2/25 | Loss: 0.00074899
Iteration 3/25 | Loss: 0.00074898
Iteration 4/25 | Loss: 0.00074898
Iteration 5/25 | Loss: 0.00074898
Iteration 6/25 | Loss: 0.00074898
Iteration 7/25 | Loss: 0.00074898
Iteration 8/25 | Loss: 0.00074898
Iteration 9/25 | Loss: 0.00074898
Iteration 10/25 | Loss: 0.00074898
Iteration 11/25 | Loss: 0.00074898
Iteration 12/25 | Loss: 0.00074898
Iteration 13/25 | Loss: 0.00074898
Iteration 14/25 | Loss: 0.00074898
Iteration 15/25 | Loss: 0.00074898
Iteration 16/25 | Loss: 0.00074898
Iteration 17/25 | Loss: 0.00074898
Iteration 18/25 | Loss: 0.00074898
Iteration 19/25 | Loss: 0.00074898
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007489754352718592, 0.0007489754352718592, 0.0007489754352718592, 0.0007489754352718592, 0.0007489754352718592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007489754352718592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074898
Iteration 2/1000 | Loss: 0.00001873
Iteration 3/1000 | Loss: 0.00001516
Iteration 4/1000 | Loss: 0.00001391
Iteration 5/1000 | Loss: 0.00001321
Iteration 6/1000 | Loss: 0.00001274
Iteration 7/1000 | Loss: 0.00001250
Iteration 8/1000 | Loss: 0.00001218
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001191
Iteration 11/1000 | Loss: 0.00001190
Iteration 12/1000 | Loss: 0.00001190
Iteration 13/1000 | Loss: 0.00001174
Iteration 14/1000 | Loss: 0.00001169
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001164
Iteration 17/1000 | Loss: 0.00001162
Iteration 18/1000 | Loss: 0.00001157
Iteration 19/1000 | Loss: 0.00001155
Iteration 20/1000 | Loss: 0.00001151
Iteration 21/1000 | Loss: 0.00001151
Iteration 22/1000 | Loss: 0.00001150
Iteration 23/1000 | Loss: 0.00001150
Iteration 24/1000 | Loss: 0.00001149
Iteration 25/1000 | Loss: 0.00001148
Iteration 26/1000 | Loss: 0.00001147
Iteration 27/1000 | Loss: 0.00001146
Iteration 28/1000 | Loss: 0.00001145
Iteration 29/1000 | Loss: 0.00001145
Iteration 30/1000 | Loss: 0.00001145
Iteration 31/1000 | Loss: 0.00001145
Iteration 32/1000 | Loss: 0.00001145
Iteration 33/1000 | Loss: 0.00001145
Iteration 34/1000 | Loss: 0.00001143
Iteration 35/1000 | Loss: 0.00001143
Iteration 36/1000 | Loss: 0.00001142
Iteration 37/1000 | Loss: 0.00001142
Iteration 38/1000 | Loss: 0.00001141
Iteration 39/1000 | Loss: 0.00001141
Iteration 40/1000 | Loss: 0.00001140
Iteration 41/1000 | Loss: 0.00001140
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001138
Iteration 44/1000 | Loss: 0.00001138
Iteration 45/1000 | Loss: 0.00001137
Iteration 46/1000 | Loss: 0.00001136
Iteration 47/1000 | Loss: 0.00001136
Iteration 48/1000 | Loss: 0.00001136
Iteration 49/1000 | Loss: 0.00001136
Iteration 50/1000 | Loss: 0.00001135
Iteration 51/1000 | Loss: 0.00001135
Iteration 52/1000 | Loss: 0.00001135
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001133
Iteration 55/1000 | Loss: 0.00001133
Iteration 56/1000 | Loss: 0.00001132
Iteration 57/1000 | Loss: 0.00001132
Iteration 58/1000 | Loss: 0.00001132
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001131
Iteration 61/1000 | Loss: 0.00001131
Iteration 62/1000 | Loss: 0.00001131
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001130
Iteration 69/1000 | Loss: 0.00001129
Iteration 70/1000 | Loss: 0.00001129
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001129
Iteration 73/1000 | Loss: 0.00001128
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001128
Iteration 77/1000 | Loss: 0.00001128
Iteration 78/1000 | Loss: 0.00001128
Iteration 79/1000 | Loss: 0.00001128
Iteration 80/1000 | Loss: 0.00001128
Iteration 81/1000 | Loss: 0.00001128
Iteration 82/1000 | Loss: 0.00001128
Iteration 83/1000 | Loss: 0.00001127
Iteration 84/1000 | Loss: 0.00001127
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001126
Iteration 87/1000 | Loss: 0.00001125
Iteration 88/1000 | Loss: 0.00001125
Iteration 89/1000 | Loss: 0.00001125
Iteration 90/1000 | Loss: 0.00001124
Iteration 91/1000 | Loss: 0.00001124
Iteration 92/1000 | Loss: 0.00001124
Iteration 93/1000 | Loss: 0.00001123
Iteration 94/1000 | Loss: 0.00001123
Iteration 95/1000 | Loss: 0.00001123
Iteration 96/1000 | Loss: 0.00001123
Iteration 97/1000 | Loss: 0.00001122
Iteration 98/1000 | Loss: 0.00001122
Iteration 99/1000 | Loss: 0.00001122
Iteration 100/1000 | Loss: 0.00001122
Iteration 101/1000 | Loss: 0.00001122
Iteration 102/1000 | Loss: 0.00001122
Iteration 103/1000 | Loss: 0.00001122
Iteration 104/1000 | Loss: 0.00001122
Iteration 105/1000 | Loss: 0.00001122
Iteration 106/1000 | Loss: 0.00001122
Iteration 107/1000 | Loss: 0.00001122
Iteration 108/1000 | Loss: 0.00001122
Iteration 109/1000 | Loss: 0.00001122
Iteration 110/1000 | Loss: 0.00001122
Iteration 111/1000 | Loss: 0.00001122
Iteration 112/1000 | Loss: 0.00001122
Iteration 113/1000 | Loss: 0.00001122
Iteration 114/1000 | Loss: 0.00001122
Iteration 115/1000 | Loss: 0.00001122
Iteration 116/1000 | Loss: 0.00001122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.1224197805859149e-05, 1.1224197805859149e-05, 1.1224197805859149e-05, 1.1224197805859149e-05, 1.1224197805859149e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1224197805859149e-05

Optimization complete. Final v2v error: 2.857787609100342 mm

Highest mean error: 3.2974741458892822 mm for frame 192

Lowest mean error: 2.605431079864502 mm for frame 215

Saving results

Total time: 36.72994041442871
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994785
Iteration 2/25 | Loss: 0.00239959
Iteration 3/25 | Loss: 0.00185571
Iteration 4/25 | Loss: 0.00173092
Iteration 5/25 | Loss: 0.00155590
Iteration 6/25 | Loss: 0.00138610
Iteration 7/25 | Loss: 0.00129627
Iteration 8/25 | Loss: 0.00125268
Iteration 9/25 | Loss: 0.00121876
Iteration 10/25 | Loss: 0.00119045
Iteration 11/25 | Loss: 0.00118183
Iteration 12/25 | Loss: 0.00118016
Iteration 13/25 | Loss: 0.00117977
Iteration 14/25 | Loss: 0.00117962
Iteration 15/25 | Loss: 0.00117958
Iteration 16/25 | Loss: 0.00117958
Iteration 17/25 | Loss: 0.00117958
Iteration 18/25 | Loss: 0.00117958
Iteration 19/25 | Loss: 0.00117958
Iteration 20/25 | Loss: 0.00117957
Iteration 21/25 | Loss: 0.00117957
Iteration 22/25 | Loss: 0.00117957
Iteration 23/25 | Loss: 0.00117957
Iteration 24/25 | Loss: 0.00117957
Iteration 25/25 | Loss: 0.00117957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25324476
Iteration 2/25 | Loss: 0.00054145
Iteration 3/25 | Loss: 0.00054144
Iteration 4/25 | Loss: 0.00054144
Iteration 5/25 | Loss: 0.00054144
Iteration 6/25 | Loss: 0.00054144
Iteration 7/25 | Loss: 0.00054144
Iteration 8/25 | Loss: 0.00054144
Iteration 9/25 | Loss: 0.00054144
Iteration 10/25 | Loss: 0.00054143
Iteration 11/25 | Loss: 0.00054143
Iteration 12/25 | Loss: 0.00054143
Iteration 13/25 | Loss: 0.00054143
Iteration 14/25 | Loss: 0.00054143
Iteration 15/25 | Loss: 0.00054143
Iteration 16/25 | Loss: 0.00054143
Iteration 17/25 | Loss: 0.00054143
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000541434739716351, 0.000541434739716351, 0.000541434739716351, 0.000541434739716351, 0.000541434739716351]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000541434739716351

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054143
Iteration 2/1000 | Loss: 0.00005330
Iteration 3/1000 | Loss: 0.00002955
Iteration 4/1000 | Loss: 0.00002276
Iteration 5/1000 | Loss: 0.00002031
Iteration 6/1000 | Loss: 0.00001914
Iteration 7/1000 | Loss: 0.00001847
Iteration 8/1000 | Loss: 0.00001776
Iteration 9/1000 | Loss: 0.00001742
Iteration 10/1000 | Loss: 0.00001710
Iteration 11/1000 | Loss: 0.00001693
Iteration 12/1000 | Loss: 0.00001679
Iteration 13/1000 | Loss: 0.00001677
Iteration 14/1000 | Loss: 0.00001671
Iteration 15/1000 | Loss: 0.00001670
Iteration 16/1000 | Loss: 0.00001665
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001661
Iteration 19/1000 | Loss: 0.00001660
Iteration 20/1000 | Loss: 0.00001654
Iteration 21/1000 | Loss: 0.00001649
Iteration 22/1000 | Loss: 0.00001649
Iteration 23/1000 | Loss: 0.00001649
Iteration 24/1000 | Loss: 0.00001648
Iteration 25/1000 | Loss: 0.00001648
Iteration 26/1000 | Loss: 0.00001648
Iteration 27/1000 | Loss: 0.00001648
Iteration 28/1000 | Loss: 0.00001648
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001648
Iteration 31/1000 | Loss: 0.00001648
Iteration 32/1000 | Loss: 0.00001648
Iteration 33/1000 | Loss: 0.00001647
Iteration 34/1000 | Loss: 0.00001647
Iteration 35/1000 | Loss: 0.00001644
Iteration 36/1000 | Loss: 0.00001644
Iteration 37/1000 | Loss: 0.00001643
Iteration 38/1000 | Loss: 0.00001643
Iteration 39/1000 | Loss: 0.00001643
Iteration 40/1000 | Loss: 0.00001643
Iteration 41/1000 | Loss: 0.00001643
Iteration 42/1000 | Loss: 0.00001643
Iteration 43/1000 | Loss: 0.00001643
Iteration 44/1000 | Loss: 0.00001641
Iteration 45/1000 | Loss: 0.00001641
Iteration 46/1000 | Loss: 0.00001640
Iteration 47/1000 | Loss: 0.00001636
Iteration 48/1000 | Loss: 0.00001636
Iteration 49/1000 | Loss: 0.00001635
Iteration 50/1000 | Loss: 0.00001630
Iteration 51/1000 | Loss: 0.00001630
Iteration 52/1000 | Loss: 0.00001629
Iteration 53/1000 | Loss: 0.00001629
Iteration 54/1000 | Loss: 0.00001629
Iteration 55/1000 | Loss: 0.00001628
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00001627
Iteration 58/1000 | Loss: 0.00001626
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001625
Iteration 62/1000 | Loss: 0.00001625
Iteration 63/1000 | Loss: 0.00001625
Iteration 64/1000 | Loss: 0.00001624
Iteration 65/1000 | Loss: 0.00001624
Iteration 66/1000 | Loss: 0.00001624
Iteration 67/1000 | Loss: 0.00001624
Iteration 68/1000 | Loss: 0.00001624
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001623
Iteration 73/1000 | Loss: 0.00001623
Iteration 74/1000 | Loss: 0.00001623
Iteration 75/1000 | Loss: 0.00001623
Iteration 76/1000 | Loss: 0.00001623
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001623
Iteration 79/1000 | Loss: 0.00001622
Iteration 80/1000 | Loss: 0.00001622
Iteration 81/1000 | Loss: 0.00001622
Iteration 82/1000 | Loss: 0.00001622
Iteration 83/1000 | Loss: 0.00001622
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001621
Iteration 90/1000 | Loss: 0.00001621
Iteration 91/1000 | Loss: 0.00001621
Iteration 92/1000 | Loss: 0.00001621
Iteration 93/1000 | Loss: 0.00001621
Iteration 94/1000 | Loss: 0.00001621
Iteration 95/1000 | Loss: 0.00001621
Iteration 96/1000 | Loss: 0.00001621
Iteration 97/1000 | Loss: 0.00001621
Iteration 98/1000 | Loss: 0.00001621
Iteration 99/1000 | Loss: 0.00001620
Iteration 100/1000 | Loss: 0.00001620
Iteration 101/1000 | Loss: 0.00001620
Iteration 102/1000 | Loss: 0.00001620
Iteration 103/1000 | Loss: 0.00001620
Iteration 104/1000 | Loss: 0.00001620
Iteration 105/1000 | Loss: 0.00001620
Iteration 106/1000 | Loss: 0.00001620
Iteration 107/1000 | Loss: 0.00001619
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001618
Iteration 111/1000 | Loss: 0.00001618
Iteration 112/1000 | Loss: 0.00001618
Iteration 113/1000 | Loss: 0.00001618
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001618
Iteration 117/1000 | Loss: 0.00001618
Iteration 118/1000 | Loss: 0.00001617
Iteration 119/1000 | Loss: 0.00001617
Iteration 120/1000 | Loss: 0.00001617
Iteration 121/1000 | Loss: 0.00001617
Iteration 122/1000 | Loss: 0.00001617
Iteration 123/1000 | Loss: 0.00001617
Iteration 124/1000 | Loss: 0.00001616
Iteration 125/1000 | Loss: 0.00001616
Iteration 126/1000 | Loss: 0.00001616
Iteration 127/1000 | Loss: 0.00001616
Iteration 128/1000 | Loss: 0.00001615
Iteration 129/1000 | Loss: 0.00001615
Iteration 130/1000 | Loss: 0.00001615
Iteration 131/1000 | Loss: 0.00001615
Iteration 132/1000 | Loss: 0.00001615
Iteration 133/1000 | Loss: 0.00001615
Iteration 134/1000 | Loss: 0.00001615
Iteration 135/1000 | Loss: 0.00001615
Iteration 136/1000 | Loss: 0.00001615
Iteration 137/1000 | Loss: 0.00001614
Iteration 138/1000 | Loss: 0.00001614
Iteration 139/1000 | Loss: 0.00001614
Iteration 140/1000 | Loss: 0.00001614
Iteration 141/1000 | Loss: 0.00001614
Iteration 142/1000 | Loss: 0.00001613
Iteration 143/1000 | Loss: 0.00001613
Iteration 144/1000 | Loss: 0.00001613
Iteration 145/1000 | Loss: 0.00001613
Iteration 146/1000 | Loss: 0.00001613
Iteration 147/1000 | Loss: 0.00001613
Iteration 148/1000 | Loss: 0.00001613
Iteration 149/1000 | Loss: 0.00001613
Iteration 150/1000 | Loss: 0.00001613
Iteration 151/1000 | Loss: 0.00001613
Iteration 152/1000 | Loss: 0.00001613
Iteration 153/1000 | Loss: 0.00001613
Iteration 154/1000 | Loss: 0.00001613
Iteration 155/1000 | Loss: 0.00001613
Iteration 156/1000 | Loss: 0.00001613
Iteration 157/1000 | Loss: 0.00001613
Iteration 158/1000 | Loss: 0.00001613
Iteration 159/1000 | Loss: 0.00001613
Iteration 160/1000 | Loss: 0.00001613
Iteration 161/1000 | Loss: 0.00001613
Iteration 162/1000 | Loss: 0.00001613
Iteration 163/1000 | Loss: 0.00001613
Iteration 164/1000 | Loss: 0.00001613
Iteration 165/1000 | Loss: 0.00001613
Iteration 166/1000 | Loss: 0.00001613
Iteration 167/1000 | Loss: 0.00001613
Iteration 168/1000 | Loss: 0.00001613
Iteration 169/1000 | Loss: 0.00001613
Iteration 170/1000 | Loss: 0.00001613
Iteration 171/1000 | Loss: 0.00001613
Iteration 172/1000 | Loss: 0.00001613
Iteration 173/1000 | Loss: 0.00001613
Iteration 174/1000 | Loss: 0.00001613
Iteration 175/1000 | Loss: 0.00001613
Iteration 176/1000 | Loss: 0.00001613
Iteration 177/1000 | Loss: 0.00001612
Iteration 178/1000 | Loss: 0.00001612
Iteration 179/1000 | Loss: 0.00001612
Iteration 180/1000 | Loss: 0.00001612
Iteration 181/1000 | Loss: 0.00001612
Iteration 182/1000 | Loss: 0.00001612
Iteration 183/1000 | Loss: 0.00001612
Iteration 184/1000 | Loss: 0.00001612
Iteration 185/1000 | Loss: 0.00001612
Iteration 186/1000 | Loss: 0.00001612
Iteration 187/1000 | Loss: 0.00001612
Iteration 188/1000 | Loss: 0.00001612
Iteration 189/1000 | Loss: 0.00001612
Iteration 190/1000 | Loss: 0.00001612
Iteration 191/1000 | Loss: 0.00001612
Iteration 192/1000 | Loss: 0.00001612
Iteration 193/1000 | Loss: 0.00001612
Iteration 194/1000 | Loss: 0.00001612
Iteration 195/1000 | Loss: 0.00001612
Iteration 196/1000 | Loss: 0.00001612
Iteration 197/1000 | Loss: 0.00001612
Iteration 198/1000 | Loss: 0.00001612
Iteration 199/1000 | Loss: 0.00001612
Iteration 200/1000 | Loss: 0.00001612
Iteration 201/1000 | Loss: 0.00001612
Iteration 202/1000 | Loss: 0.00001612
Iteration 203/1000 | Loss: 0.00001612
Iteration 204/1000 | Loss: 0.00001612
Iteration 205/1000 | Loss: 0.00001612
Iteration 206/1000 | Loss: 0.00001612
Iteration 207/1000 | Loss: 0.00001612
Iteration 208/1000 | Loss: 0.00001612
Iteration 209/1000 | Loss: 0.00001612
Iteration 210/1000 | Loss: 0.00001612
Iteration 211/1000 | Loss: 0.00001612
Iteration 212/1000 | Loss: 0.00001612
Iteration 213/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.6124158719321713e-05, 1.6124158719321713e-05, 1.6124158719321713e-05, 1.6124158719321713e-05, 1.6124158719321713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6124158719321713e-05

Optimization complete. Final v2v error: 3.344679832458496 mm

Highest mean error: 5.923544883728027 mm for frame 1

Lowest mean error: 2.9936106204986572 mm for frame 108

Saving results

Total time: 57.4655339717865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00643156
Iteration 2/25 | Loss: 0.00124995
Iteration 3/25 | Loss: 0.00114865
Iteration 4/25 | Loss: 0.00113744
Iteration 5/25 | Loss: 0.00113467
Iteration 6/25 | Loss: 0.00113439
Iteration 7/25 | Loss: 0.00113439
Iteration 8/25 | Loss: 0.00113439
Iteration 9/25 | Loss: 0.00113439
Iteration 10/25 | Loss: 0.00113439
Iteration 11/25 | Loss: 0.00113439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011343860533088446, 0.0011343860533088446, 0.0011343860533088446, 0.0011343860533088446, 0.0011343860533088446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011343860533088446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.06794453
Iteration 2/25 | Loss: 0.00075855
Iteration 3/25 | Loss: 0.00075849
Iteration 4/25 | Loss: 0.00075848
Iteration 5/25 | Loss: 0.00075848
Iteration 6/25 | Loss: 0.00075848
Iteration 7/25 | Loss: 0.00075848
Iteration 8/25 | Loss: 0.00075848
Iteration 9/25 | Loss: 0.00075848
Iteration 10/25 | Loss: 0.00075848
Iteration 11/25 | Loss: 0.00075848
Iteration 12/25 | Loss: 0.00075848
Iteration 13/25 | Loss: 0.00075848
Iteration 14/25 | Loss: 0.00075848
Iteration 15/25 | Loss: 0.00075848
Iteration 16/25 | Loss: 0.00075848
Iteration 17/25 | Loss: 0.00075848
Iteration 18/25 | Loss: 0.00075848
Iteration 19/25 | Loss: 0.00075848
Iteration 20/25 | Loss: 0.00075848
Iteration 21/25 | Loss: 0.00075848
Iteration 22/25 | Loss: 0.00075848
Iteration 23/25 | Loss: 0.00075848
Iteration 24/25 | Loss: 0.00075848
Iteration 25/25 | Loss: 0.00075848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075848
Iteration 2/1000 | Loss: 0.00002207
Iteration 3/1000 | Loss: 0.00001673
Iteration 4/1000 | Loss: 0.00001564
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00001468
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001403
Iteration 9/1000 | Loss: 0.00001370
Iteration 10/1000 | Loss: 0.00001348
Iteration 11/1000 | Loss: 0.00001337
Iteration 12/1000 | Loss: 0.00001332
Iteration 13/1000 | Loss: 0.00001331
Iteration 14/1000 | Loss: 0.00001331
Iteration 15/1000 | Loss: 0.00001328
Iteration 16/1000 | Loss: 0.00001323
Iteration 17/1000 | Loss: 0.00001323
Iteration 18/1000 | Loss: 0.00001313
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001306
Iteration 21/1000 | Loss: 0.00001306
Iteration 22/1000 | Loss: 0.00001302
Iteration 23/1000 | Loss: 0.00001302
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001301
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001300
Iteration 28/1000 | Loss: 0.00001299
Iteration 29/1000 | Loss: 0.00001298
Iteration 30/1000 | Loss: 0.00001297
Iteration 31/1000 | Loss: 0.00001296
Iteration 32/1000 | Loss: 0.00001296
Iteration 33/1000 | Loss: 0.00001291
Iteration 34/1000 | Loss: 0.00001291
Iteration 35/1000 | Loss: 0.00001290
Iteration 36/1000 | Loss: 0.00001290
Iteration 37/1000 | Loss: 0.00001289
Iteration 38/1000 | Loss: 0.00001285
Iteration 39/1000 | Loss: 0.00001284
Iteration 40/1000 | Loss: 0.00001284
Iteration 41/1000 | Loss: 0.00001282
Iteration 42/1000 | Loss: 0.00001281
Iteration 43/1000 | Loss: 0.00001281
Iteration 44/1000 | Loss: 0.00001281
Iteration 45/1000 | Loss: 0.00001280
Iteration 46/1000 | Loss: 0.00001280
Iteration 47/1000 | Loss: 0.00001280
Iteration 48/1000 | Loss: 0.00001279
Iteration 49/1000 | Loss: 0.00001279
Iteration 50/1000 | Loss: 0.00001279
Iteration 51/1000 | Loss: 0.00001278
Iteration 52/1000 | Loss: 0.00001278
Iteration 53/1000 | Loss: 0.00001278
Iteration 54/1000 | Loss: 0.00001278
Iteration 55/1000 | Loss: 0.00001278
Iteration 56/1000 | Loss: 0.00001278
Iteration 57/1000 | Loss: 0.00001277
Iteration 58/1000 | Loss: 0.00001277
Iteration 59/1000 | Loss: 0.00001277
Iteration 60/1000 | Loss: 0.00001277
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00001276
Iteration 63/1000 | Loss: 0.00001276
Iteration 64/1000 | Loss: 0.00001275
Iteration 65/1000 | Loss: 0.00001275
Iteration 66/1000 | Loss: 0.00001275
Iteration 67/1000 | Loss: 0.00001274
Iteration 68/1000 | Loss: 0.00001274
Iteration 69/1000 | Loss: 0.00001274
Iteration 70/1000 | Loss: 0.00001272
Iteration 71/1000 | Loss: 0.00001272
Iteration 72/1000 | Loss: 0.00001272
Iteration 73/1000 | Loss: 0.00001272
Iteration 74/1000 | Loss: 0.00001272
Iteration 75/1000 | Loss: 0.00001271
Iteration 76/1000 | Loss: 0.00001271
Iteration 77/1000 | Loss: 0.00001271
Iteration 78/1000 | Loss: 0.00001271
Iteration 79/1000 | Loss: 0.00001271
Iteration 80/1000 | Loss: 0.00001271
Iteration 81/1000 | Loss: 0.00001271
Iteration 82/1000 | Loss: 0.00001271
Iteration 83/1000 | Loss: 0.00001270
Iteration 84/1000 | Loss: 0.00001270
Iteration 85/1000 | Loss: 0.00001269
Iteration 86/1000 | Loss: 0.00001269
Iteration 87/1000 | Loss: 0.00001269
Iteration 88/1000 | Loss: 0.00001269
Iteration 89/1000 | Loss: 0.00001268
Iteration 90/1000 | Loss: 0.00001268
Iteration 91/1000 | Loss: 0.00001268
Iteration 92/1000 | Loss: 0.00001268
Iteration 93/1000 | Loss: 0.00001268
Iteration 94/1000 | Loss: 0.00001268
Iteration 95/1000 | Loss: 0.00001267
Iteration 96/1000 | Loss: 0.00001267
Iteration 97/1000 | Loss: 0.00001267
Iteration 98/1000 | Loss: 0.00001266
Iteration 99/1000 | Loss: 0.00001266
Iteration 100/1000 | Loss: 0.00001266
Iteration 101/1000 | Loss: 0.00001266
Iteration 102/1000 | Loss: 0.00001266
Iteration 103/1000 | Loss: 0.00001266
Iteration 104/1000 | Loss: 0.00001266
Iteration 105/1000 | Loss: 0.00001265
Iteration 106/1000 | Loss: 0.00001265
Iteration 107/1000 | Loss: 0.00001265
Iteration 108/1000 | Loss: 0.00001265
Iteration 109/1000 | Loss: 0.00001265
Iteration 110/1000 | Loss: 0.00001265
Iteration 111/1000 | Loss: 0.00001264
Iteration 112/1000 | Loss: 0.00001264
Iteration 113/1000 | Loss: 0.00001264
Iteration 114/1000 | Loss: 0.00001264
Iteration 115/1000 | Loss: 0.00001264
Iteration 116/1000 | Loss: 0.00001264
Iteration 117/1000 | Loss: 0.00001264
Iteration 118/1000 | Loss: 0.00001264
Iteration 119/1000 | Loss: 0.00001263
Iteration 120/1000 | Loss: 0.00001263
Iteration 121/1000 | Loss: 0.00001263
Iteration 122/1000 | Loss: 0.00001263
Iteration 123/1000 | Loss: 0.00001263
Iteration 124/1000 | Loss: 0.00001263
Iteration 125/1000 | Loss: 0.00001263
Iteration 126/1000 | Loss: 0.00001262
Iteration 127/1000 | Loss: 0.00001262
Iteration 128/1000 | Loss: 0.00001262
Iteration 129/1000 | Loss: 0.00001262
Iteration 130/1000 | Loss: 0.00001262
Iteration 131/1000 | Loss: 0.00001262
Iteration 132/1000 | Loss: 0.00001262
Iteration 133/1000 | Loss: 0.00001262
Iteration 134/1000 | Loss: 0.00001261
Iteration 135/1000 | Loss: 0.00001261
Iteration 136/1000 | Loss: 0.00001261
Iteration 137/1000 | Loss: 0.00001261
Iteration 138/1000 | Loss: 0.00001261
Iteration 139/1000 | Loss: 0.00001261
Iteration 140/1000 | Loss: 0.00001261
Iteration 141/1000 | Loss: 0.00001261
Iteration 142/1000 | Loss: 0.00001261
Iteration 143/1000 | Loss: 0.00001261
Iteration 144/1000 | Loss: 0.00001261
Iteration 145/1000 | Loss: 0.00001261
Iteration 146/1000 | Loss: 0.00001261
Iteration 147/1000 | Loss: 0.00001261
Iteration 148/1000 | Loss: 0.00001261
Iteration 149/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.2608610632014461e-05, 1.2608610632014461e-05, 1.2608610632014461e-05, 1.2608610632014461e-05, 1.2608610632014461e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2608610632014461e-05

Optimization complete. Final v2v error: 3.0007216930389404 mm

Highest mean error: 3.5916552543640137 mm for frame 140

Lowest mean error: 2.774320125579834 mm for frame 9

Saving results

Total time: 39.51628375053406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878706
Iteration 2/25 | Loss: 0.00266383
Iteration 3/25 | Loss: 0.00180495
Iteration 4/25 | Loss: 0.00155052
Iteration 5/25 | Loss: 0.00146244
Iteration 6/25 | Loss: 0.00148497
Iteration 7/25 | Loss: 0.00145005
Iteration 8/25 | Loss: 0.00144358
Iteration 9/25 | Loss: 0.00143727
Iteration 10/25 | Loss: 0.00142832
Iteration 11/25 | Loss: 0.00142825
Iteration 12/25 | Loss: 0.00141713
Iteration 13/25 | Loss: 0.00141762
Iteration 14/25 | Loss: 0.00140932
Iteration 15/25 | Loss: 0.00142181
Iteration 16/25 | Loss: 0.00141940
Iteration 17/25 | Loss: 0.00140644
Iteration 18/25 | Loss: 0.00140537
Iteration 19/25 | Loss: 0.00140181
Iteration 20/25 | Loss: 0.00140089
Iteration 21/25 | Loss: 0.00139293
Iteration 22/25 | Loss: 0.00139334
Iteration 23/25 | Loss: 0.00141152
Iteration 24/25 | Loss: 0.00139781
Iteration 25/25 | Loss: 0.00139634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.03259897
Iteration 2/25 | Loss: 0.00417579
Iteration 3/25 | Loss: 0.00405488
Iteration 4/25 | Loss: 0.00405480
Iteration 5/25 | Loss: 0.00405480
Iteration 6/25 | Loss: 0.00405480
Iteration 7/25 | Loss: 0.00405480
Iteration 8/25 | Loss: 0.00405480
Iteration 9/25 | Loss: 0.00405480
Iteration 10/25 | Loss: 0.00405480
Iteration 11/25 | Loss: 0.00405480
Iteration 12/25 | Loss: 0.00405479
Iteration 13/25 | Loss: 0.00405479
Iteration 14/25 | Loss: 0.00405479
Iteration 15/25 | Loss: 0.00405479
Iteration 16/25 | Loss: 0.00405479
Iteration 17/25 | Loss: 0.00405479
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00405479408800602, 0.00405479408800602, 0.00405479408800602, 0.00405479408800602, 0.00405479408800602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00405479408800602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00405479
Iteration 2/1000 | Loss: 0.00062040
Iteration 3/1000 | Loss: 0.00110133
Iteration 4/1000 | Loss: 0.00083133
Iteration 5/1000 | Loss: 0.00318525
Iteration 6/1000 | Loss: 0.00320311
Iteration 7/1000 | Loss: 0.00107614
Iteration 8/1000 | Loss: 0.00048320
Iteration 9/1000 | Loss: 0.00140718
Iteration 10/1000 | Loss: 0.00062951
Iteration 11/1000 | Loss: 0.00036787
Iteration 12/1000 | Loss: 0.00074433
Iteration 13/1000 | Loss: 0.00026146
Iteration 14/1000 | Loss: 0.00184387
Iteration 15/1000 | Loss: 0.00164666
Iteration 16/1000 | Loss: 0.00078757
Iteration 17/1000 | Loss: 0.00033816
Iteration 18/1000 | Loss: 0.00010064
Iteration 19/1000 | Loss: 0.00020985
Iteration 20/1000 | Loss: 0.00009434
Iteration 21/1000 | Loss: 0.00020521
Iteration 22/1000 | Loss: 0.00013860
Iteration 23/1000 | Loss: 0.00008529
Iteration 24/1000 | Loss: 0.00021730
Iteration 25/1000 | Loss: 0.00015384
Iteration 26/1000 | Loss: 0.00022363
Iteration 27/1000 | Loss: 0.00082366
Iteration 28/1000 | Loss: 0.00025605
Iteration 29/1000 | Loss: 0.00007760
Iteration 30/1000 | Loss: 0.00006449
Iteration 31/1000 | Loss: 0.00149205
Iteration 32/1000 | Loss: 0.00099479
Iteration 33/1000 | Loss: 0.00048582
Iteration 34/1000 | Loss: 0.00087090
Iteration 35/1000 | Loss: 0.00040882
Iteration 36/1000 | Loss: 0.00011760
Iteration 37/1000 | Loss: 0.00007353
Iteration 38/1000 | Loss: 0.00006474
Iteration 39/1000 | Loss: 0.00067358
Iteration 40/1000 | Loss: 0.00010684
Iteration 41/1000 | Loss: 0.00005925
Iteration 42/1000 | Loss: 0.00010477
Iteration 43/1000 | Loss: 0.00049246
Iteration 44/1000 | Loss: 0.00230345
Iteration 45/1000 | Loss: 0.00096829
Iteration 46/1000 | Loss: 0.00010525
Iteration 47/1000 | Loss: 0.00014980
Iteration 48/1000 | Loss: 0.00006514
Iteration 49/1000 | Loss: 0.00010140
Iteration 50/1000 | Loss: 0.00009962
Iteration 51/1000 | Loss: 0.00005740
Iteration 52/1000 | Loss: 0.00005624
Iteration 53/1000 | Loss: 0.00014651
Iteration 54/1000 | Loss: 0.00040928
Iteration 55/1000 | Loss: 0.00016907
Iteration 56/1000 | Loss: 0.00018388
Iteration 57/1000 | Loss: 0.00032354
Iteration 58/1000 | Loss: 0.00029559
Iteration 59/1000 | Loss: 0.00044594
Iteration 60/1000 | Loss: 0.00020973
Iteration 61/1000 | Loss: 0.00006303
Iteration 62/1000 | Loss: 0.00070752
Iteration 63/1000 | Loss: 0.00028630
Iteration 64/1000 | Loss: 0.00007695
Iteration 65/1000 | Loss: 0.00005744
Iteration 66/1000 | Loss: 0.00023493
Iteration 67/1000 | Loss: 0.00022782
Iteration 68/1000 | Loss: 0.00004969
Iteration 69/1000 | Loss: 0.00010525
Iteration 70/1000 | Loss: 0.00004706
Iteration 71/1000 | Loss: 0.00016584
Iteration 72/1000 | Loss: 0.00004531
Iteration 73/1000 | Loss: 0.00004389
Iteration 74/1000 | Loss: 0.00004615
Iteration 75/1000 | Loss: 0.00004227
Iteration 76/1000 | Loss: 0.00004174
Iteration 77/1000 | Loss: 0.00013512
Iteration 78/1000 | Loss: 0.00004285
Iteration 79/1000 | Loss: 0.00004093
Iteration 80/1000 | Loss: 0.00010348
Iteration 81/1000 | Loss: 0.00004056
Iteration 82/1000 | Loss: 0.00004024
Iteration 83/1000 | Loss: 0.00011455
Iteration 84/1000 | Loss: 0.00004021
Iteration 85/1000 | Loss: 0.00021296
Iteration 86/1000 | Loss: 0.00020139
Iteration 87/1000 | Loss: 0.00023401
Iteration 88/1000 | Loss: 0.00022190
Iteration 89/1000 | Loss: 0.00004684
Iteration 90/1000 | Loss: 0.00004379
Iteration 91/1000 | Loss: 0.00004233
Iteration 92/1000 | Loss: 0.00014113
Iteration 93/1000 | Loss: 0.00008361
Iteration 94/1000 | Loss: 0.00005262
Iteration 95/1000 | Loss: 0.00004064
Iteration 96/1000 | Loss: 0.00004022
Iteration 97/1000 | Loss: 0.00009991
Iteration 98/1000 | Loss: 0.00006846
Iteration 99/1000 | Loss: 0.00005117
Iteration 100/1000 | Loss: 0.00042878
Iteration 101/1000 | Loss: 0.00011703
Iteration 102/1000 | Loss: 0.00040138
Iteration 103/1000 | Loss: 0.00004282
Iteration 104/1000 | Loss: 0.00024523
Iteration 105/1000 | Loss: 0.00031096
Iteration 106/1000 | Loss: 0.00021798
Iteration 107/1000 | Loss: 0.00006590
Iteration 108/1000 | Loss: 0.00004905
Iteration 109/1000 | Loss: 0.00004032
Iteration 110/1000 | Loss: 0.00003943
Iteration 111/1000 | Loss: 0.00024441
Iteration 112/1000 | Loss: 0.00095018
Iteration 113/1000 | Loss: 0.00006028
Iteration 114/1000 | Loss: 0.00003899
Iteration 115/1000 | Loss: 0.00003894
Iteration 116/1000 | Loss: 0.00003894
Iteration 117/1000 | Loss: 0.00003894
Iteration 118/1000 | Loss: 0.00003893
Iteration 119/1000 | Loss: 0.00003893
Iteration 120/1000 | Loss: 0.00003893
Iteration 121/1000 | Loss: 0.00003893
Iteration 122/1000 | Loss: 0.00003893
Iteration 123/1000 | Loss: 0.00003893
Iteration 124/1000 | Loss: 0.00003893
Iteration 125/1000 | Loss: 0.00003893
Iteration 126/1000 | Loss: 0.00003893
Iteration 127/1000 | Loss: 0.00003892
Iteration 128/1000 | Loss: 0.00003892
Iteration 129/1000 | Loss: 0.00003892
Iteration 130/1000 | Loss: 0.00003892
Iteration 131/1000 | Loss: 0.00003892
Iteration 132/1000 | Loss: 0.00003891
Iteration 133/1000 | Loss: 0.00003891
Iteration 134/1000 | Loss: 0.00003891
Iteration 135/1000 | Loss: 0.00003891
Iteration 136/1000 | Loss: 0.00003890
Iteration 137/1000 | Loss: 0.00003889
Iteration 138/1000 | Loss: 0.00003889
Iteration 139/1000 | Loss: 0.00003888
Iteration 140/1000 | Loss: 0.00003887
Iteration 141/1000 | Loss: 0.00003887
Iteration 142/1000 | Loss: 0.00003887
Iteration 143/1000 | Loss: 0.00003887
Iteration 144/1000 | Loss: 0.00003887
Iteration 145/1000 | Loss: 0.00003886
Iteration 146/1000 | Loss: 0.00003885
Iteration 147/1000 | Loss: 0.00008939
Iteration 148/1000 | Loss: 0.00003885
Iteration 149/1000 | Loss: 0.00003878
Iteration 150/1000 | Loss: 0.00003878
Iteration 151/1000 | Loss: 0.00003878
Iteration 152/1000 | Loss: 0.00003878
Iteration 153/1000 | Loss: 0.00003878
Iteration 154/1000 | Loss: 0.00003877
Iteration 155/1000 | Loss: 0.00003877
Iteration 156/1000 | Loss: 0.00003871
Iteration 157/1000 | Loss: 0.00003871
Iteration 158/1000 | Loss: 0.00003870
Iteration 159/1000 | Loss: 0.00003870
Iteration 160/1000 | Loss: 0.00003869
Iteration 161/1000 | Loss: 0.00003869
Iteration 162/1000 | Loss: 0.00003868
Iteration 163/1000 | Loss: 0.00003868
Iteration 164/1000 | Loss: 0.00003868
Iteration 165/1000 | Loss: 0.00003867
Iteration 166/1000 | Loss: 0.00003867
Iteration 167/1000 | Loss: 0.00003863
Iteration 168/1000 | Loss: 0.00003863
Iteration 169/1000 | Loss: 0.00003863
Iteration 170/1000 | Loss: 0.00003863
Iteration 171/1000 | Loss: 0.00003863
Iteration 172/1000 | Loss: 0.00003863
Iteration 173/1000 | Loss: 0.00003863
Iteration 174/1000 | Loss: 0.00003863
Iteration 175/1000 | Loss: 0.00003863
Iteration 176/1000 | Loss: 0.00003863
Iteration 177/1000 | Loss: 0.00003861
Iteration 178/1000 | Loss: 0.00003861
Iteration 179/1000 | Loss: 0.00009938
Iteration 180/1000 | Loss: 0.00008390
Iteration 181/1000 | Loss: 0.00003919
Iteration 182/1000 | Loss: 0.00006596
Iteration 183/1000 | Loss: 0.00003878
Iteration 184/1000 | Loss: 0.00003852
Iteration 185/1000 | Loss: 0.00003851
Iteration 186/1000 | Loss: 0.00003850
Iteration 187/1000 | Loss: 0.00003849
Iteration 188/1000 | Loss: 0.00003849
Iteration 189/1000 | Loss: 0.00003849
Iteration 190/1000 | Loss: 0.00003849
Iteration 191/1000 | Loss: 0.00003849
Iteration 192/1000 | Loss: 0.00003849
Iteration 193/1000 | Loss: 0.00003849
Iteration 194/1000 | Loss: 0.00003848
Iteration 195/1000 | Loss: 0.00003848
Iteration 196/1000 | Loss: 0.00003848
Iteration 197/1000 | Loss: 0.00003848
Iteration 198/1000 | Loss: 0.00003848
Iteration 199/1000 | Loss: 0.00003848
Iteration 200/1000 | Loss: 0.00003848
Iteration 201/1000 | Loss: 0.00003848
Iteration 202/1000 | Loss: 0.00003848
Iteration 203/1000 | Loss: 0.00003848
Iteration 204/1000 | Loss: 0.00003848
Iteration 205/1000 | Loss: 0.00003848
Iteration 206/1000 | Loss: 0.00003848
Iteration 207/1000 | Loss: 0.00003848
Iteration 208/1000 | Loss: 0.00003847
Iteration 209/1000 | Loss: 0.00003847
Iteration 210/1000 | Loss: 0.00003847
Iteration 211/1000 | Loss: 0.00003847
Iteration 212/1000 | Loss: 0.00003847
Iteration 213/1000 | Loss: 0.00003846
Iteration 214/1000 | Loss: 0.00003846
Iteration 215/1000 | Loss: 0.00003846
Iteration 216/1000 | Loss: 0.00003846
Iteration 217/1000 | Loss: 0.00003846
Iteration 218/1000 | Loss: 0.00003846
Iteration 219/1000 | Loss: 0.00003846
Iteration 220/1000 | Loss: 0.00003846
Iteration 221/1000 | Loss: 0.00003845
Iteration 222/1000 | Loss: 0.00003845
Iteration 223/1000 | Loss: 0.00003845
Iteration 224/1000 | Loss: 0.00003844
Iteration 225/1000 | Loss: 0.00003844
Iteration 226/1000 | Loss: 0.00003844
Iteration 227/1000 | Loss: 0.00003843
Iteration 228/1000 | Loss: 0.00003843
Iteration 229/1000 | Loss: 0.00003843
Iteration 230/1000 | Loss: 0.00003843
Iteration 231/1000 | Loss: 0.00003843
Iteration 232/1000 | Loss: 0.00003843
Iteration 233/1000 | Loss: 0.00003842
Iteration 234/1000 | Loss: 0.00003842
Iteration 235/1000 | Loss: 0.00003842
Iteration 236/1000 | Loss: 0.00003842
Iteration 237/1000 | Loss: 0.00003841
Iteration 238/1000 | Loss: 0.00003841
Iteration 239/1000 | Loss: 0.00003840
Iteration 240/1000 | Loss: 0.00003840
Iteration 241/1000 | Loss: 0.00003840
Iteration 242/1000 | Loss: 0.00003839
Iteration 243/1000 | Loss: 0.00003839
Iteration 244/1000 | Loss: 0.00003839
Iteration 245/1000 | Loss: 0.00003838
Iteration 246/1000 | Loss: 0.00003838
Iteration 247/1000 | Loss: 0.00003838
Iteration 248/1000 | Loss: 0.00003837
Iteration 249/1000 | Loss: 0.00003836
Iteration 250/1000 | Loss: 0.00003836
Iteration 251/1000 | Loss: 0.00003835
Iteration 252/1000 | Loss: 0.00003834
Iteration 253/1000 | Loss: 0.00003834
Iteration 254/1000 | Loss: 0.00003834
Iteration 255/1000 | Loss: 0.00009988
Iteration 256/1000 | Loss: 0.00049194
Iteration 257/1000 | Loss: 0.00004438
Iteration 258/1000 | Loss: 0.00004078
Iteration 259/1000 | Loss: 0.00003916
Iteration 260/1000 | Loss: 0.00008894
Iteration 261/1000 | Loss: 0.00003751
Iteration 262/1000 | Loss: 0.00003687
Iteration 263/1000 | Loss: 0.00003640
Iteration 264/1000 | Loss: 0.00003615
Iteration 265/1000 | Loss: 0.00003608
Iteration 266/1000 | Loss: 0.00003591
Iteration 267/1000 | Loss: 0.00003584
Iteration 268/1000 | Loss: 0.00009326
Iteration 269/1000 | Loss: 0.00003851
Iteration 270/1000 | Loss: 0.00003578
Iteration 271/1000 | Loss: 0.00003571
Iteration 272/1000 | Loss: 0.00003564
Iteration 273/1000 | Loss: 0.00003564
Iteration 274/1000 | Loss: 0.00003562
Iteration 275/1000 | Loss: 0.00003560
Iteration 276/1000 | Loss: 0.00003560
Iteration 277/1000 | Loss: 0.00003560
Iteration 278/1000 | Loss: 0.00003560
Iteration 279/1000 | Loss: 0.00003560
Iteration 280/1000 | Loss: 0.00003560
Iteration 281/1000 | Loss: 0.00003559
Iteration 282/1000 | Loss: 0.00003559
Iteration 283/1000 | Loss: 0.00010832
Iteration 284/1000 | Loss: 0.00006213
Iteration 285/1000 | Loss: 0.00067956
Iteration 286/1000 | Loss: 0.00017669
Iteration 287/1000 | Loss: 0.00040800
Iteration 288/1000 | Loss: 0.00003950
Iteration 289/1000 | Loss: 0.00005197
Iteration 290/1000 | Loss: 0.00003657
Iteration 291/1000 | Loss: 0.00003557
Iteration 292/1000 | Loss: 0.00003553
Iteration 293/1000 | Loss: 0.00003553
Iteration 294/1000 | Loss: 0.00003552
Iteration 295/1000 | Loss: 0.00003552
Iteration 296/1000 | Loss: 0.00003551
Iteration 297/1000 | Loss: 0.00003551
Iteration 298/1000 | Loss: 0.00003550
Iteration 299/1000 | Loss: 0.00003550
Iteration 300/1000 | Loss: 0.00003549
Iteration 301/1000 | Loss: 0.00003549
Iteration 302/1000 | Loss: 0.00003549
Iteration 303/1000 | Loss: 0.00003549
Iteration 304/1000 | Loss: 0.00003548
Iteration 305/1000 | Loss: 0.00003548
Iteration 306/1000 | Loss: 0.00003548
Iteration 307/1000 | Loss: 0.00003548
Iteration 308/1000 | Loss: 0.00003548
Iteration 309/1000 | Loss: 0.00003548
Iteration 310/1000 | Loss: 0.00003547
Iteration 311/1000 | Loss: 0.00003547
Iteration 312/1000 | Loss: 0.00003547
Iteration 313/1000 | Loss: 0.00003547
Iteration 314/1000 | Loss: 0.00003547
Iteration 315/1000 | Loss: 0.00003547
Iteration 316/1000 | Loss: 0.00003547
Iteration 317/1000 | Loss: 0.00003547
Iteration 318/1000 | Loss: 0.00003547
Iteration 319/1000 | Loss: 0.00003546
Iteration 320/1000 | Loss: 0.00003546
Iteration 321/1000 | Loss: 0.00003546
Iteration 322/1000 | Loss: 0.00003546
Iteration 323/1000 | Loss: 0.00003546
Iteration 324/1000 | Loss: 0.00003546
Iteration 325/1000 | Loss: 0.00003546
Iteration 326/1000 | Loss: 0.00003546
Iteration 327/1000 | Loss: 0.00003546
Iteration 328/1000 | Loss: 0.00003546
Iteration 329/1000 | Loss: 0.00003546
Iteration 330/1000 | Loss: 0.00003545
Iteration 331/1000 | Loss: 0.00003545
Iteration 332/1000 | Loss: 0.00003545
Iteration 333/1000 | Loss: 0.00003545
Iteration 334/1000 | Loss: 0.00003545
Iteration 335/1000 | Loss: 0.00003545
Iteration 336/1000 | Loss: 0.00003545
Iteration 337/1000 | Loss: 0.00003545
Iteration 338/1000 | Loss: 0.00003545
Iteration 339/1000 | Loss: 0.00003545
Iteration 340/1000 | Loss: 0.00003545
Iteration 341/1000 | Loss: 0.00003545
Iteration 342/1000 | Loss: 0.00003545
Iteration 343/1000 | Loss: 0.00003545
Iteration 344/1000 | Loss: 0.00003545
Iteration 345/1000 | Loss: 0.00003545
Iteration 346/1000 | Loss: 0.00003545
Iteration 347/1000 | Loss: 0.00003545
Iteration 348/1000 | Loss: 0.00003545
Iteration 349/1000 | Loss: 0.00003545
Iteration 350/1000 | Loss: 0.00003545
Iteration 351/1000 | Loss: 0.00003545
Iteration 352/1000 | Loss: 0.00003545
Iteration 353/1000 | Loss: 0.00003545
Iteration 354/1000 | Loss: 0.00003545
Iteration 355/1000 | Loss: 0.00003545
Iteration 356/1000 | Loss: 0.00003545
Iteration 357/1000 | Loss: 0.00003545
Iteration 358/1000 | Loss: 0.00003545
Iteration 359/1000 | Loss: 0.00003545
Iteration 360/1000 | Loss: 0.00003545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 360. Stopping optimization.
Last 5 losses: [3.544763239915483e-05, 3.544763239915483e-05, 3.544763239915483e-05, 3.544763239915483e-05, 3.544763239915483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.544763239915483e-05

Optimization complete. Final v2v error: 3.4471077919006348 mm

Highest mean error: 12.612553596496582 mm for frame 87

Lowest mean error: 2.471457004547119 mm for frame 112

Saving results

Total time: 274.6935839653015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00942481
Iteration 2/25 | Loss: 0.00237362
Iteration 3/25 | Loss: 0.00209369
Iteration 4/25 | Loss: 0.00158112
Iteration 5/25 | Loss: 0.00153225
Iteration 6/25 | Loss: 0.00149526
Iteration 7/25 | Loss: 0.00170808
Iteration 8/25 | Loss: 0.00174461
Iteration 9/25 | Loss: 0.00178433
Iteration 10/25 | Loss: 0.00157050
Iteration 11/25 | Loss: 0.00131444
Iteration 12/25 | Loss: 0.00125179
Iteration 13/25 | Loss: 0.00122901
Iteration 14/25 | Loss: 0.00122152
Iteration 15/25 | Loss: 0.00121579
Iteration 16/25 | Loss: 0.00122153
Iteration 17/25 | Loss: 0.00122577
Iteration 18/25 | Loss: 0.00121975
Iteration 19/25 | Loss: 0.00121278
Iteration 20/25 | Loss: 0.00120827
Iteration 21/25 | Loss: 0.00120447
Iteration 22/25 | Loss: 0.00120345
Iteration 23/25 | Loss: 0.00120309
Iteration 24/25 | Loss: 0.00120294
Iteration 25/25 | Loss: 0.00120291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30621707
Iteration 2/25 | Loss: 0.00039275
Iteration 3/25 | Loss: 0.00039275
Iteration 4/25 | Loss: 0.00039275
Iteration 5/25 | Loss: 0.00039275
Iteration 6/25 | Loss: 0.00039275
Iteration 7/25 | Loss: 0.00039275
Iteration 8/25 | Loss: 0.00039275
Iteration 9/25 | Loss: 0.00039275
Iteration 10/25 | Loss: 0.00039275
Iteration 11/25 | Loss: 0.00039275
Iteration 12/25 | Loss: 0.00039275
Iteration 13/25 | Loss: 0.00039275
Iteration 14/25 | Loss: 0.00039275
Iteration 15/25 | Loss: 0.00039275
Iteration 16/25 | Loss: 0.00039275
Iteration 17/25 | Loss: 0.00039275
Iteration 18/25 | Loss: 0.00039275
Iteration 19/25 | Loss: 0.00039275
Iteration 20/25 | Loss: 0.00039275
Iteration 21/25 | Loss: 0.00039275
Iteration 22/25 | Loss: 0.00039275
Iteration 23/25 | Loss: 0.00039275
Iteration 24/25 | Loss: 0.00039275
Iteration 25/25 | Loss: 0.00039275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039275
Iteration 2/1000 | Loss: 0.00003753
Iteration 3/1000 | Loss: 0.00003099
Iteration 4/1000 | Loss: 0.00002949
Iteration 5/1000 | Loss: 0.00003529
Iteration 6/1000 | Loss: 0.00002815
Iteration 7/1000 | Loss: 0.00002784
Iteration 8/1000 | Loss: 0.00002750
Iteration 9/1000 | Loss: 0.00002725
Iteration 10/1000 | Loss: 0.00002701
Iteration 11/1000 | Loss: 0.00002681
Iteration 12/1000 | Loss: 0.00002673
Iteration 13/1000 | Loss: 0.00002672
Iteration 14/1000 | Loss: 0.00003498
Iteration 15/1000 | Loss: 0.00002669
Iteration 16/1000 | Loss: 0.00002839
Iteration 17/1000 | Loss: 0.00002665
Iteration 18/1000 | Loss: 0.00002665
Iteration 19/1000 | Loss: 0.00002665
Iteration 20/1000 | Loss: 0.00002665
Iteration 21/1000 | Loss: 0.00002664
Iteration 22/1000 | Loss: 0.00002664
Iteration 23/1000 | Loss: 0.00002664
Iteration 24/1000 | Loss: 0.00002664
Iteration 25/1000 | Loss: 0.00002664
Iteration 26/1000 | Loss: 0.00002663
Iteration 27/1000 | Loss: 0.00002662
Iteration 28/1000 | Loss: 0.00002662
Iteration 29/1000 | Loss: 0.00002661
Iteration 30/1000 | Loss: 0.00002661
Iteration 31/1000 | Loss: 0.00002660
Iteration 32/1000 | Loss: 0.00002660
Iteration 33/1000 | Loss: 0.00002660
Iteration 34/1000 | Loss: 0.00002659
Iteration 35/1000 | Loss: 0.00002659
Iteration 36/1000 | Loss: 0.00002659
Iteration 37/1000 | Loss: 0.00002659
Iteration 38/1000 | Loss: 0.00002658
Iteration 39/1000 | Loss: 0.00002658
Iteration 40/1000 | Loss: 0.00002658
Iteration 41/1000 | Loss: 0.00002658
Iteration 42/1000 | Loss: 0.00002658
Iteration 43/1000 | Loss: 0.00002658
Iteration 44/1000 | Loss: 0.00002657
Iteration 45/1000 | Loss: 0.00002657
Iteration 46/1000 | Loss: 0.00002657
Iteration 47/1000 | Loss: 0.00002657
Iteration 48/1000 | Loss: 0.00002657
Iteration 49/1000 | Loss: 0.00002656
Iteration 50/1000 | Loss: 0.00002656
Iteration 51/1000 | Loss: 0.00002655
Iteration 52/1000 | Loss: 0.00002654
Iteration 53/1000 | Loss: 0.00002654
Iteration 54/1000 | Loss: 0.00002653
Iteration 55/1000 | Loss: 0.00002653
Iteration 56/1000 | Loss: 0.00002653
Iteration 57/1000 | Loss: 0.00002653
Iteration 58/1000 | Loss: 0.00002653
Iteration 59/1000 | Loss: 0.00002653
Iteration 60/1000 | Loss: 0.00002653
Iteration 61/1000 | Loss: 0.00002653
Iteration 62/1000 | Loss: 0.00002652
Iteration 63/1000 | Loss: 0.00002652
Iteration 64/1000 | Loss: 0.00002652
Iteration 65/1000 | Loss: 0.00002652
Iteration 66/1000 | Loss: 0.00002651
Iteration 67/1000 | Loss: 0.00002651
Iteration 68/1000 | Loss: 0.00002651
Iteration 69/1000 | Loss: 0.00002651
Iteration 70/1000 | Loss: 0.00002651
Iteration 71/1000 | Loss: 0.00002651
Iteration 72/1000 | Loss: 0.00002650
Iteration 73/1000 | Loss: 0.00002650
Iteration 74/1000 | Loss: 0.00002650
Iteration 75/1000 | Loss: 0.00002650
Iteration 76/1000 | Loss: 0.00002650
Iteration 77/1000 | Loss: 0.00002650
Iteration 78/1000 | Loss: 0.00002650
Iteration 79/1000 | Loss: 0.00002649
Iteration 80/1000 | Loss: 0.00002649
Iteration 81/1000 | Loss: 0.00002649
Iteration 82/1000 | Loss: 0.00002649
Iteration 83/1000 | Loss: 0.00002649
Iteration 84/1000 | Loss: 0.00002649
Iteration 85/1000 | Loss: 0.00002649
Iteration 86/1000 | Loss: 0.00002649
Iteration 87/1000 | Loss: 0.00002649
Iteration 88/1000 | Loss: 0.00002649
Iteration 89/1000 | Loss: 0.00002649
Iteration 90/1000 | Loss: 0.00002649
Iteration 91/1000 | Loss: 0.00002648
Iteration 92/1000 | Loss: 0.00002648
Iteration 93/1000 | Loss: 0.00002648
Iteration 94/1000 | Loss: 0.00002648
Iteration 95/1000 | Loss: 0.00002648
Iteration 96/1000 | Loss: 0.00002648
Iteration 97/1000 | Loss: 0.00002648
Iteration 98/1000 | Loss: 0.00002648
Iteration 99/1000 | Loss: 0.00002648
Iteration 100/1000 | Loss: 0.00002648
Iteration 101/1000 | Loss: 0.00002648
Iteration 102/1000 | Loss: 0.00002648
Iteration 103/1000 | Loss: 0.00002648
Iteration 104/1000 | Loss: 0.00002648
Iteration 105/1000 | Loss: 0.00002648
Iteration 106/1000 | Loss: 0.00002648
Iteration 107/1000 | Loss: 0.00002648
Iteration 108/1000 | Loss: 0.00002648
Iteration 109/1000 | Loss: 0.00002648
Iteration 110/1000 | Loss: 0.00002648
Iteration 111/1000 | Loss: 0.00002648
Iteration 112/1000 | Loss: 0.00002648
Iteration 113/1000 | Loss: 0.00002647
Iteration 114/1000 | Loss: 0.00002647
Iteration 115/1000 | Loss: 0.00002647
Iteration 116/1000 | Loss: 0.00002647
Iteration 117/1000 | Loss: 0.00002647
Iteration 118/1000 | Loss: 0.00002647
Iteration 119/1000 | Loss: 0.00002647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.64741902356036e-05, 2.64741902356036e-05, 2.64741902356036e-05, 2.64741902356036e-05, 2.64741902356036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.64741902356036e-05

Optimization complete. Final v2v error: 4.13396692276001 mm

Highest mean error: 5.566620349884033 mm for frame 174

Lowest mean error: 3.817025661468506 mm for frame 194

Saving results

Total time: 70.80936598777771
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474732
Iteration 2/25 | Loss: 0.00137296
Iteration 3/25 | Loss: 0.00113392
Iteration 4/25 | Loss: 0.00111307
Iteration 5/25 | Loss: 0.00111016
Iteration 6/25 | Loss: 0.00110911
Iteration 7/25 | Loss: 0.00110911
Iteration 8/25 | Loss: 0.00110911
Iteration 9/25 | Loss: 0.00110911
Iteration 10/25 | Loss: 0.00110911
Iteration 11/25 | Loss: 0.00110911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011091112392023206, 0.0011091112392023206, 0.0011091112392023206, 0.0011091112392023206, 0.0011091112392023206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011091112392023206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37052810
Iteration 2/25 | Loss: 0.00075837
Iteration 3/25 | Loss: 0.00075837
Iteration 4/25 | Loss: 0.00075837
Iteration 5/25 | Loss: 0.00075837
Iteration 6/25 | Loss: 0.00075837
Iteration 7/25 | Loss: 0.00075836
Iteration 8/25 | Loss: 0.00075836
Iteration 9/25 | Loss: 0.00075836
Iteration 10/25 | Loss: 0.00075836
Iteration 11/25 | Loss: 0.00075836
Iteration 12/25 | Loss: 0.00075836
Iteration 13/25 | Loss: 0.00075836
Iteration 14/25 | Loss: 0.00075836
Iteration 15/25 | Loss: 0.00075836
Iteration 16/25 | Loss: 0.00075836
Iteration 17/25 | Loss: 0.00075836
Iteration 18/25 | Loss: 0.00075836
Iteration 19/25 | Loss: 0.00075836
Iteration 20/25 | Loss: 0.00075836
Iteration 21/25 | Loss: 0.00075836
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007583638071082532, 0.0007583638071082532, 0.0007583638071082532, 0.0007583638071082532, 0.0007583638071082532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007583638071082532

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075836
Iteration 2/1000 | Loss: 0.00003342
Iteration 3/1000 | Loss: 0.00001972
Iteration 4/1000 | Loss: 0.00001787
Iteration 5/1000 | Loss: 0.00001663
Iteration 6/1000 | Loss: 0.00001602
Iteration 7/1000 | Loss: 0.00001537
Iteration 8/1000 | Loss: 0.00001495
Iteration 9/1000 | Loss: 0.00001464
Iteration 10/1000 | Loss: 0.00001438
Iteration 11/1000 | Loss: 0.00001425
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001416
Iteration 14/1000 | Loss: 0.00001413
Iteration 15/1000 | Loss: 0.00001411
Iteration 16/1000 | Loss: 0.00001411
Iteration 17/1000 | Loss: 0.00001408
Iteration 18/1000 | Loss: 0.00001408
Iteration 19/1000 | Loss: 0.00001406
Iteration 20/1000 | Loss: 0.00001405
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001401
Iteration 23/1000 | Loss: 0.00001401
Iteration 24/1000 | Loss: 0.00001399
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001397
Iteration 27/1000 | Loss: 0.00001393
Iteration 28/1000 | Loss: 0.00001391
Iteration 29/1000 | Loss: 0.00001390
Iteration 30/1000 | Loss: 0.00001389
Iteration 31/1000 | Loss: 0.00001389
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001389
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001388
Iteration 36/1000 | Loss: 0.00001388
Iteration 37/1000 | Loss: 0.00001388
Iteration 38/1000 | Loss: 0.00001388
Iteration 39/1000 | Loss: 0.00001388
Iteration 40/1000 | Loss: 0.00001388
Iteration 41/1000 | Loss: 0.00001388
Iteration 42/1000 | Loss: 0.00001387
Iteration 43/1000 | Loss: 0.00001386
Iteration 44/1000 | Loss: 0.00001386
Iteration 45/1000 | Loss: 0.00001385
Iteration 46/1000 | Loss: 0.00001385
Iteration 47/1000 | Loss: 0.00001385
Iteration 48/1000 | Loss: 0.00001384
Iteration 49/1000 | Loss: 0.00001384
Iteration 50/1000 | Loss: 0.00001384
Iteration 51/1000 | Loss: 0.00001383
Iteration 52/1000 | Loss: 0.00001382
Iteration 53/1000 | Loss: 0.00001382
Iteration 54/1000 | Loss: 0.00001382
Iteration 55/1000 | Loss: 0.00001381
Iteration 56/1000 | Loss: 0.00001381
Iteration 57/1000 | Loss: 0.00001381
Iteration 58/1000 | Loss: 0.00001381
Iteration 59/1000 | Loss: 0.00001380
Iteration 60/1000 | Loss: 0.00001380
Iteration 61/1000 | Loss: 0.00001380
Iteration 62/1000 | Loss: 0.00001379
Iteration 63/1000 | Loss: 0.00001379
Iteration 64/1000 | Loss: 0.00001379
Iteration 65/1000 | Loss: 0.00001379
Iteration 66/1000 | Loss: 0.00001379
Iteration 67/1000 | Loss: 0.00001378
Iteration 68/1000 | Loss: 0.00001378
Iteration 69/1000 | Loss: 0.00001378
Iteration 70/1000 | Loss: 0.00001378
Iteration 71/1000 | Loss: 0.00001378
Iteration 72/1000 | Loss: 0.00001378
Iteration 73/1000 | Loss: 0.00001377
Iteration 74/1000 | Loss: 0.00001377
Iteration 75/1000 | Loss: 0.00001377
Iteration 76/1000 | Loss: 0.00001377
Iteration 77/1000 | Loss: 0.00001377
Iteration 78/1000 | Loss: 0.00001376
Iteration 79/1000 | Loss: 0.00001376
Iteration 80/1000 | Loss: 0.00001375
Iteration 81/1000 | Loss: 0.00001375
Iteration 82/1000 | Loss: 0.00001375
Iteration 83/1000 | Loss: 0.00001375
Iteration 84/1000 | Loss: 0.00001374
Iteration 85/1000 | Loss: 0.00001374
Iteration 86/1000 | Loss: 0.00001374
Iteration 87/1000 | Loss: 0.00001373
Iteration 88/1000 | Loss: 0.00001373
Iteration 89/1000 | Loss: 0.00001373
Iteration 90/1000 | Loss: 0.00001372
Iteration 91/1000 | Loss: 0.00001372
Iteration 92/1000 | Loss: 0.00001372
Iteration 93/1000 | Loss: 0.00001372
Iteration 94/1000 | Loss: 0.00001371
Iteration 95/1000 | Loss: 0.00001371
Iteration 96/1000 | Loss: 0.00001371
Iteration 97/1000 | Loss: 0.00001371
Iteration 98/1000 | Loss: 0.00001371
Iteration 99/1000 | Loss: 0.00001370
Iteration 100/1000 | Loss: 0.00001370
Iteration 101/1000 | Loss: 0.00001370
Iteration 102/1000 | Loss: 0.00001369
Iteration 103/1000 | Loss: 0.00001369
Iteration 104/1000 | Loss: 0.00001369
Iteration 105/1000 | Loss: 0.00001369
Iteration 106/1000 | Loss: 0.00001368
Iteration 107/1000 | Loss: 0.00001368
Iteration 108/1000 | Loss: 0.00001368
Iteration 109/1000 | Loss: 0.00001368
Iteration 110/1000 | Loss: 0.00001367
Iteration 111/1000 | Loss: 0.00001367
Iteration 112/1000 | Loss: 0.00001367
Iteration 113/1000 | Loss: 0.00001367
Iteration 114/1000 | Loss: 0.00001367
Iteration 115/1000 | Loss: 0.00001366
Iteration 116/1000 | Loss: 0.00001366
Iteration 117/1000 | Loss: 0.00001366
Iteration 118/1000 | Loss: 0.00001366
Iteration 119/1000 | Loss: 0.00001365
Iteration 120/1000 | Loss: 0.00001365
Iteration 121/1000 | Loss: 0.00001365
Iteration 122/1000 | Loss: 0.00001365
Iteration 123/1000 | Loss: 0.00001365
Iteration 124/1000 | Loss: 0.00001364
Iteration 125/1000 | Loss: 0.00001364
Iteration 126/1000 | Loss: 0.00001364
Iteration 127/1000 | Loss: 0.00001364
Iteration 128/1000 | Loss: 0.00001364
Iteration 129/1000 | Loss: 0.00001364
Iteration 130/1000 | Loss: 0.00001364
Iteration 131/1000 | Loss: 0.00001364
Iteration 132/1000 | Loss: 0.00001363
Iteration 133/1000 | Loss: 0.00001363
Iteration 134/1000 | Loss: 0.00001363
Iteration 135/1000 | Loss: 0.00001362
Iteration 136/1000 | Loss: 0.00001362
Iteration 137/1000 | Loss: 0.00001362
Iteration 138/1000 | Loss: 0.00001361
Iteration 139/1000 | Loss: 0.00001361
Iteration 140/1000 | Loss: 0.00001361
Iteration 141/1000 | Loss: 0.00001361
Iteration 142/1000 | Loss: 0.00001361
Iteration 143/1000 | Loss: 0.00001360
Iteration 144/1000 | Loss: 0.00001360
Iteration 145/1000 | Loss: 0.00001360
Iteration 146/1000 | Loss: 0.00001360
Iteration 147/1000 | Loss: 0.00001359
Iteration 148/1000 | Loss: 0.00001359
Iteration 149/1000 | Loss: 0.00001359
Iteration 150/1000 | Loss: 0.00001359
Iteration 151/1000 | Loss: 0.00001359
Iteration 152/1000 | Loss: 0.00001359
Iteration 153/1000 | Loss: 0.00001359
Iteration 154/1000 | Loss: 0.00001359
Iteration 155/1000 | Loss: 0.00001358
Iteration 156/1000 | Loss: 0.00001358
Iteration 157/1000 | Loss: 0.00001358
Iteration 158/1000 | Loss: 0.00001358
Iteration 159/1000 | Loss: 0.00001357
Iteration 160/1000 | Loss: 0.00001357
Iteration 161/1000 | Loss: 0.00001357
Iteration 162/1000 | Loss: 0.00001357
Iteration 163/1000 | Loss: 0.00001357
Iteration 164/1000 | Loss: 0.00001356
Iteration 165/1000 | Loss: 0.00001356
Iteration 166/1000 | Loss: 0.00001356
Iteration 167/1000 | Loss: 0.00001356
Iteration 168/1000 | Loss: 0.00001356
Iteration 169/1000 | Loss: 0.00001356
Iteration 170/1000 | Loss: 0.00001356
Iteration 171/1000 | Loss: 0.00001356
Iteration 172/1000 | Loss: 0.00001355
Iteration 173/1000 | Loss: 0.00001355
Iteration 174/1000 | Loss: 0.00001355
Iteration 175/1000 | Loss: 0.00001355
Iteration 176/1000 | Loss: 0.00001355
Iteration 177/1000 | Loss: 0.00001355
Iteration 178/1000 | Loss: 0.00001355
Iteration 179/1000 | Loss: 0.00001355
Iteration 180/1000 | Loss: 0.00001355
Iteration 181/1000 | Loss: 0.00001355
Iteration 182/1000 | Loss: 0.00001354
Iteration 183/1000 | Loss: 0.00001354
Iteration 184/1000 | Loss: 0.00001354
Iteration 185/1000 | Loss: 0.00001354
Iteration 186/1000 | Loss: 0.00001354
Iteration 187/1000 | Loss: 0.00001354
Iteration 188/1000 | Loss: 0.00001354
Iteration 189/1000 | Loss: 0.00001354
Iteration 190/1000 | Loss: 0.00001354
Iteration 191/1000 | Loss: 0.00001354
Iteration 192/1000 | Loss: 0.00001354
Iteration 193/1000 | Loss: 0.00001354
Iteration 194/1000 | Loss: 0.00001354
Iteration 195/1000 | Loss: 0.00001354
Iteration 196/1000 | Loss: 0.00001354
Iteration 197/1000 | Loss: 0.00001354
Iteration 198/1000 | Loss: 0.00001354
Iteration 199/1000 | Loss: 0.00001354
Iteration 200/1000 | Loss: 0.00001354
Iteration 201/1000 | Loss: 0.00001354
Iteration 202/1000 | Loss: 0.00001354
Iteration 203/1000 | Loss: 0.00001354
Iteration 204/1000 | Loss: 0.00001354
Iteration 205/1000 | Loss: 0.00001354
Iteration 206/1000 | Loss: 0.00001354
Iteration 207/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.3536151527659968e-05, 1.3536151527659968e-05, 1.3536151527659968e-05, 1.3536151527659968e-05, 1.3536151527659968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3536151527659968e-05

Optimization complete. Final v2v error: 2.9341201782226562 mm

Highest mean error: 4.525991916656494 mm for frame 72

Lowest mean error: 2.4880363941192627 mm for frame 162

Saving results

Total time: 41.08553409576416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413523
Iteration 2/25 | Loss: 0.00113750
Iteration 3/25 | Loss: 0.00107876
Iteration 4/25 | Loss: 0.00107110
Iteration 5/25 | Loss: 0.00106883
Iteration 6/25 | Loss: 0.00106883
Iteration 7/25 | Loss: 0.00106883
Iteration 8/25 | Loss: 0.00106883
Iteration 9/25 | Loss: 0.00106883
Iteration 10/25 | Loss: 0.00106883
Iteration 11/25 | Loss: 0.00106883
Iteration 12/25 | Loss: 0.00106883
Iteration 13/25 | Loss: 0.00106883
Iteration 14/25 | Loss: 0.00106883
Iteration 15/25 | Loss: 0.00106883
Iteration 16/25 | Loss: 0.00106883
Iteration 17/25 | Loss: 0.00106883
Iteration 18/25 | Loss: 0.00106883
Iteration 19/25 | Loss: 0.00106883
Iteration 20/25 | Loss: 0.00106883
Iteration 21/25 | Loss: 0.00106883
Iteration 22/25 | Loss: 0.00106883
Iteration 23/25 | Loss: 0.00106883
Iteration 24/25 | Loss: 0.00106883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001068827579729259, 0.001068827579729259, 0.001068827579729259, 0.001068827579729259, 0.001068827579729259]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001068827579729259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.43897557
Iteration 2/25 | Loss: 0.00075836
Iteration 3/25 | Loss: 0.00075835
Iteration 4/25 | Loss: 0.00075835
Iteration 5/25 | Loss: 0.00075835
Iteration 6/25 | Loss: 0.00075835
Iteration 7/25 | Loss: 0.00075835
Iteration 8/25 | Loss: 0.00075835
Iteration 9/25 | Loss: 0.00075835
Iteration 10/25 | Loss: 0.00075835
Iteration 11/25 | Loss: 0.00075835
Iteration 12/25 | Loss: 0.00075835
Iteration 13/25 | Loss: 0.00075835
Iteration 14/25 | Loss: 0.00075835
Iteration 15/25 | Loss: 0.00075835
Iteration 16/25 | Loss: 0.00075835
Iteration 17/25 | Loss: 0.00075835
Iteration 18/25 | Loss: 0.00075835
Iteration 19/25 | Loss: 0.00075835
Iteration 20/25 | Loss: 0.00075835
Iteration 21/25 | Loss: 0.00075835
Iteration 22/25 | Loss: 0.00075835
Iteration 23/25 | Loss: 0.00075835
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007583467522636056, 0.0007583467522636056, 0.0007583467522636056, 0.0007583467522636056, 0.0007583467522636056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007583467522636056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075835
Iteration 2/1000 | Loss: 0.00001641
Iteration 3/1000 | Loss: 0.00001272
Iteration 4/1000 | Loss: 0.00001186
Iteration 5/1000 | Loss: 0.00001136
Iteration 6/1000 | Loss: 0.00001098
Iteration 7/1000 | Loss: 0.00001079
Iteration 8/1000 | Loss: 0.00001047
Iteration 9/1000 | Loss: 0.00001047
Iteration 10/1000 | Loss: 0.00001046
Iteration 11/1000 | Loss: 0.00001043
Iteration 12/1000 | Loss: 0.00001036
Iteration 13/1000 | Loss: 0.00001035
Iteration 14/1000 | Loss: 0.00001034
Iteration 15/1000 | Loss: 0.00001030
Iteration 16/1000 | Loss: 0.00001026
Iteration 17/1000 | Loss: 0.00001025
Iteration 18/1000 | Loss: 0.00001024
Iteration 19/1000 | Loss: 0.00001023
Iteration 20/1000 | Loss: 0.00001023
Iteration 21/1000 | Loss: 0.00001022
Iteration 22/1000 | Loss: 0.00001020
Iteration 23/1000 | Loss: 0.00001014
Iteration 24/1000 | Loss: 0.00001004
Iteration 25/1000 | Loss: 0.00001000
Iteration 26/1000 | Loss: 0.00001000
Iteration 27/1000 | Loss: 0.00000999
Iteration 28/1000 | Loss: 0.00000991
Iteration 29/1000 | Loss: 0.00000990
Iteration 30/1000 | Loss: 0.00000990
Iteration 31/1000 | Loss: 0.00000989
Iteration 32/1000 | Loss: 0.00000989
Iteration 33/1000 | Loss: 0.00000988
Iteration 34/1000 | Loss: 0.00000988
Iteration 35/1000 | Loss: 0.00000987
Iteration 36/1000 | Loss: 0.00000986
Iteration 37/1000 | Loss: 0.00000986
Iteration 38/1000 | Loss: 0.00000985
Iteration 39/1000 | Loss: 0.00000984
Iteration 40/1000 | Loss: 0.00000984
Iteration 41/1000 | Loss: 0.00000984
Iteration 42/1000 | Loss: 0.00000983
Iteration 43/1000 | Loss: 0.00000982
Iteration 44/1000 | Loss: 0.00000982
Iteration 45/1000 | Loss: 0.00000981
Iteration 46/1000 | Loss: 0.00000980
Iteration 47/1000 | Loss: 0.00000980
Iteration 48/1000 | Loss: 0.00000980
Iteration 49/1000 | Loss: 0.00000980
Iteration 50/1000 | Loss: 0.00000980
Iteration 51/1000 | Loss: 0.00000980
Iteration 52/1000 | Loss: 0.00000979
Iteration 53/1000 | Loss: 0.00000979
Iteration 54/1000 | Loss: 0.00000979
Iteration 55/1000 | Loss: 0.00000977
Iteration 56/1000 | Loss: 0.00000977
Iteration 57/1000 | Loss: 0.00000976
Iteration 58/1000 | Loss: 0.00000975
Iteration 59/1000 | Loss: 0.00000975
Iteration 60/1000 | Loss: 0.00000975
Iteration 61/1000 | Loss: 0.00000975
Iteration 62/1000 | Loss: 0.00000975
Iteration 63/1000 | Loss: 0.00000975
Iteration 64/1000 | Loss: 0.00000975
Iteration 65/1000 | Loss: 0.00000975
Iteration 66/1000 | Loss: 0.00000974
Iteration 67/1000 | Loss: 0.00000974
Iteration 68/1000 | Loss: 0.00000974
Iteration 69/1000 | Loss: 0.00000974
Iteration 70/1000 | Loss: 0.00000974
Iteration 71/1000 | Loss: 0.00000974
Iteration 72/1000 | Loss: 0.00000974
Iteration 73/1000 | Loss: 0.00000974
Iteration 74/1000 | Loss: 0.00000973
Iteration 75/1000 | Loss: 0.00000973
Iteration 76/1000 | Loss: 0.00000972
Iteration 77/1000 | Loss: 0.00000972
Iteration 78/1000 | Loss: 0.00000971
Iteration 79/1000 | Loss: 0.00000971
Iteration 80/1000 | Loss: 0.00000971
Iteration 81/1000 | Loss: 0.00000971
Iteration 82/1000 | Loss: 0.00000970
Iteration 83/1000 | Loss: 0.00000970
Iteration 84/1000 | Loss: 0.00000970
Iteration 85/1000 | Loss: 0.00000969
Iteration 86/1000 | Loss: 0.00000969
Iteration 87/1000 | Loss: 0.00000969
Iteration 88/1000 | Loss: 0.00000969
Iteration 89/1000 | Loss: 0.00000969
Iteration 90/1000 | Loss: 0.00000969
Iteration 91/1000 | Loss: 0.00000968
Iteration 92/1000 | Loss: 0.00000968
Iteration 93/1000 | Loss: 0.00000968
Iteration 94/1000 | Loss: 0.00000968
Iteration 95/1000 | Loss: 0.00000967
Iteration 96/1000 | Loss: 0.00000967
Iteration 97/1000 | Loss: 0.00000967
Iteration 98/1000 | Loss: 0.00000966
Iteration 99/1000 | Loss: 0.00000966
Iteration 100/1000 | Loss: 0.00000966
Iteration 101/1000 | Loss: 0.00000966
Iteration 102/1000 | Loss: 0.00000966
Iteration 103/1000 | Loss: 0.00000966
Iteration 104/1000 | Loss: 0.00000966
Iteration 105/1000 | Loss: 0.00000966
Iteration 106/1000 | Loss: 0.00000966
Iteration 107/1000 | Loss: 0.00000966
Iteration 108/1000 | Loss: 0.00000966
Iteration 109/1000 | Loss: 0.00000965
Iteration 110/1000 | Loss: 0.00000965
Iteration 111/1000 | Loss: 0.00000965
Iteration 112/1000 | Loss: 0.00000965
Iteration 113/1000 | Loss: 0.00000965
Iteration 114/1000 | Loss: 0.00000965
Iteration 115/1000 | Loss: 0.00000965
Iteration 116/1000 | Loss: 0.00000964
Iteration 117/1000 | Loss: 0.00000964
Iteration 118/1000 | Loss: 0.00000964
Iteration 119/1000 | Loss: 0.00000964
Iteration 120/1000 | Loss: 0.00000964
Iteration 121/1000 | Loss: 0.00000963
Iteration 122/1000 | Loss: 0.00000963
Iteration 123/1000 | Loss: 0.00000962
Iteration 124/1000 | Loss: 0.00000962
Iteration 125/1000 | Loss: 0.00000961
Iteration 126/1000 | Loss: 0.00000961
Iteration 127/1000 | Loss: 0.00000961
Iteration 128/1000 | Loss: 0.00000961
Iteration 129/1000 | Loss: 0.00000961
Iteration 130/1000 | Loss: 0.00000960
Iteration 131/1000 | Loss: 0.00000959
Iteration 132/1000 | Loss: 0.00000959
Iteration 133/1000 | Loss: 0.00000959
Iteration 134/1000 | Loss: 0.00000959
Iteration 135/1000 | Loss: 0.00000959
Iteration 136/1000 | Loss: 0.00000959
Iteration 137/1000 | Loss: 0.00000959
Iteration 138/1000 | Loss: 0.00000959
Iteration 139/1000 | Loss: 0.00000958
Iteration 140/1000 | Loss: 0.00000958
Iteration 141/1000 | Loss: 0.00000958
Iteration 142/1000 | Loss: 0.00000958
Iteration 143/1000 | Loss: 0.00000958
Iteration 144/1000 | Loss: 0.00000958
Iteration 145/1000 | Loss: 0.00000958
Iteration 146/1000 | Loss: 0.00000958
Iteration 147/1000 | Loss: 0.00000957
Iteration 148/1000 | Loss: 0.00000957
Iteration 149/1000 | Loss: 0.00000957
Iteration 150/1000 | Loss: 0.00000957
Iteration 151/1000 | Loss: 0.00000956
Iteration 152/1000 | Loss: 0.00000956
Iteration 153/1000 | Loss: 0.00000956
Iteration 154/1000 | Loss: 0.00000956
Iteration 155/1000 | Loss: 0.00000956
Iteration 156/1000 | Loss: 0.00000956
Iteration 157/1000 | Loss: 0.00000956
Iteration 158/1000 | Loss: 0.00000956
Iteration 159/1000 | Loss: 0.00000955
Iteration 160/1000 | Loss: 0.00000955
Iteration 161/1000 | Loss: 0.00000954
Iteration 162/1000 | Loss: 0.00000954
Iteration 163/1000 | Loss: 0.00000954
Iteration 164/1000 | Loss: 0.00000954
Iteration 165/1000 | Loss: 0.00000954
Iteration 166/1000 | Loss: 0.00000954
Iteration 167/1000 | Loss: 0.00000954
Iteration 168/1000 | Loss: 0.00000953
Iteration 169/1000 | Loss: 0.00000953
Iteration 170/1000 | Loss: 0.00000953
Iteration 171/1000 | Loss: 0.00000953
Iteration 172/1000 | Loss: 0.00000953
Iteration 173/1000 | Loss: 0.00000953
Iteration 174/1000 | Loss: 0.00000953
Iteration 175/1000 | Loss: 0.00000953
Iteration 176/1000 | Loss: 0.00000953
Iteration 177/1000 | Loss: 0.00000953
Iteration 178/1000 | Loss: 0.00000953
Iteration 179/1000 | Loss: 0.00000953
Iteration 180/1000 | Loss: 0.00000953
Iteration 181/1000 | Loss: 0.00000953
Iteration 182/1000 | Loss: 0.00000952
Iteration 183/1000 | Loss: 0.00000952
Iteration 184/1000 | Loss: 0.00000952
Iteration 185/1000 | Loss: 0.00000952
Iteration 186/1000 | Loss: 0.00000952
Iteration 187/1000 | Loss: 0.00000952
Iteration 188/1000 | Loss: 0.00000952
Iteration 189/1000 | Loss: 0.00000951
Iteration 190/1000 | Loss: 0.00000951
Iteration 191/1000 | Loss: 0.00000951
Iteration 192/1000 | Loss: 0.00000951
Iteration 193/1000 | Loss: 0.00000951
Iteration 194/1000 | Loss: 0.00000951
Iteration 195/1000 | Loss: 0.00000951
Iteration 196/1000 | Loss: 0.00000951
Iteration 197/1000 | Loss: 0.00000951
Iteration 198/1000 | Loss: 0.00000951
Iteration 199/1000 | Loss: 0.00000951
Iteration 200/1000 | Loss: 0.00000951
Iteration 201/1000 | Loss: 0.00000951
Iteration 202/1000 | Loss: 0.00000951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [9.514072189631406e-06, 9.514072189631406e-06, 9.514072189631406e-06, 9.514072189631406e-06, 9.514072189631406e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.514072189631406e-06

Optimization complete. Final v2v error: 2.6152567863464355 mm

Highest mean error: 2.982927083969116 mm for frame 157

Lowest mean error: 2.3935036659240723 mm for frame 132

Saving results

Total time: 43.67966628074646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414104
Iteration 2/25 | Loss: 0.00112992
Iteration 3/25 | Loss: 0.00107417
Iteration 4/25 | Loss: 0.00106732
Iteration 5/25 | Loss: 0.00106582
Iteration 6/25 | Loss: 0.00106582
Iteration 7/25 | Loss: 0.00106582
Iteration 8/25 | Loss: 0.00106582
Iteration 9/25 | Loss: 0.00106582
Iteration 10/25 | Loss: 0.00106582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001065818010829389, 0.001065818010829389, 0.001065818010829389, 0.001065818010829389, 0.001065818010829389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001065818010829389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.46659112
Iteration 2/25 | Loss: 0.00067920
Iteration 3/25 | Loss: 0.00067920
Iteration 4/25 | Loss: 0.00067920
Iteration 5/25 | Loss: 0.00067920
Iteration 6/25 | Loss: 0.00067920
Iteration 7/25 | Loss: 0.00067920
Iteration 8/25 | Loss: 0.00067920
Iteration 9/25 | Loss: 0.00067920
Iteration 10/25 | Loss: 0.00067920
Iteration 11/25 | Loss: 0.00067920
Iteration 12/25 | Loss: 0.00067920
Iteration 13/25 | Loss: 0.00067920
Iteration 14/25 | Loss: 0.00067920
Iteration 15/25 | Loss: 0.00067920
Iteration 16/25 | Loss: 0.00067920
Iteration 17/25 | Loss: 0.00067920
Iteration 18/25 | Loss: 0.00067920
Iteration 19/25 | Loss: 0.00067920
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000679196382407099, 0.000679196382407099, 0.000679196382407099, 0.000679196382407099, 0.000679196382407099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000679196382407099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067920
Iteration 2/1000 | Loss: 0.00002159
Iteration 3/1000 | Loss: 0.00001618
Iteration 4/1000 | Loss: 0.00001475
Iteration 5/1000 | Loss: 0.00001402
Iteration 6/1000 | Loss: 0.00001334
Iteration 7/1000 | Loss: 0.00001298
Iteration 8/1000 | Loss: 0.00001265
Iteration 9/1000 | Loss: 0.00001240
Iteration 10/1000 | Loss: 0.00001230
Iteration 11/1000 | Loss: 0.00001230
Iteration 12/1000 | Loss: 0.00001227
Iteration 13/1000 | Loss: 0.00001220
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001191
Iteration 16/1000 | Loss: 0.00001178
Iteration 17/1000 | Loss: 0.00001169
Iteration 18/1000 | Loss: 0.00001161
Iteration 19/1000 | Loss: 0.00001156
Iteration 20/1000 | Loss: 0.00001152
Iteration 21/1000 | Loss: 0.00001150
Iteration 22/1000 | Loss: 0.00001150
Iteration 23/1000 | Loss: 0.00001150
Iteration 24/1000 | Loss: 0.00001149
Iteration 25/1000 | Loss: 0.00001149
Iteration 26/1000 | Loss: 0.00001148
Iteration 27/1000 | Loss: 0.00001147
Iteration 28/1000 | Loss: 0.00001146
Iteration 29/1000 | Loss: 0.00001146
Iteration 30/1000 | Loss: 0.00001145
Iteration 31/1000 | Loss: 0.00001145
Iteration 32/1000 | Loss: 0.00001145
Iteration 33/1000 | Loss: 0.00001144
Iteration 34/1000 | Loss: 0.00001144
Iteration 35/1000 | Loss: 0.00001143
Iteration 36/1000 | Loss: 0.00001142
Iteration 37/1000 | Loss: 0.00001142
Iteration 38/1000 | Loss: 0.00001141
Iteration 39/1000 | Loss: 0.00001140
Iteration 40/1000 | Loss: 0.00001140
Iteration 41/1000 | Loss: 0.00001140
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001140
Iteration 44/1000 | Loss: 0.00001140
Iteration 45/1000 | Loss: 0.00001139
Iteration 46/1000 | Loss: 0.00001139
Iteration 47/1000 | Loss: 0.00001139
Iteration 48/1000 | Loss: 0.00001139
Iteration 49/1000 | Loss: 0.00001138
Iteration 50/1000 | Loss: 0.00001136
Iteration 51/1000 | Loss: 0.00001136
Iteration 52/1000 | Loss: 0.00001136
Iteration 53/1000 | Loss: 0.00001136
Iteration 54/1000 | Loss: 0.00001135
Iteration 55/1000 | Loss: 0.00001135
Iteration 56/1000 | Loss: 0.00001135
Iteration 57/1000 | Loss: 0.00001135
Iteration 58/1000 | Loss: 0.00001135
Iteration 59/1000 | Loss: 0.00001134
Iteration 60/1000 | Loss: 0.00001133
Iteration 61/1000 | Loss: 0.00001133
Iteration 62/1000 | Loss: 0.00001132
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001129
Iteration 67/1000 | Loss: 0.00001129
Iteration 68/1000 | Loss: 0.00001128
Iteration 69/1000 | Loss: 0.00001126
Iteration 70/1000 | Loss: 0.00001126
Iteration 71/1000 | Loss: 0.00001126
Iteration 72/1000 | Loss: 0.00001126
Iteration 73/1000 | Loss: 0.00001126
Iteration 74/1000 | Loss: 0.00001125
Iteration 75/1000 | Loss: 0.00001125
Iteration 76/1000 | Loss: 0.00001125
Iteration 77/1000 | Loss: 0.00001125
Iteration 78/1000 | Loss: 0.00001125
Iteration 79/1000 | Loss: 0.00001124
Iteration 80/1000 | Loss: 0.00001124
Iteration 81/1000 | Loss: 0.00001124
Iteration 82/1000 | Loss: 0.00001124
Iteration 83/1000 | Loss: 0.00001124
Iteration 84/1000 | Loss: 0.00001124
Iteration 85/1000 | Loss: 0.00001123
Iteration 86/1000 | Loss: 0.00001123
Iteration 87/1000 | Loss: 0.00001122
Iteration 88/1000 | Loss: 0.00001121
Iteration 89/1000 | Loss: 0.00001121
Iteration 90/1000 | Loss: 0.00001120
Iteration 91/1000 | Loss: 0.00001120
Iteration 92/1000 | Loss: 0.00001120
Iteration 93/1000 | Loss: 0.00001120
Iteration 94/1000 | Loss: 0.00001120
Iteration 95/1000 | Loss: 0.00001119
Iteration 96/1000 | Loss: 0.00001119
Iteration 97/1000 | Loss: 0.00001119
Iteration 98/1000 | Loss: 0.00001119
Iteration 99/1000 | Loss: 0.00001118
Iteration 100/1000 | Loss: 0.00001118
Iteration 101/1000 | Loss: 0.00001118
Iteration 102/1000 | Loss: 0.00001117
Iteration 103/1000 | Loss: 0.00001117
Iteration 104/1000 | Loss: 0.00001117
Iteration 105/1000 | Loss: 0.00001117
Iteration 106/1000 | Loss: 0.00001116
Iteration 107/1000 | Loss: 0.00001116
Iteration 108/1000 | Loss: 0.00001116
Iteration 109/1000 | Loss: 0.00001116
Iteration 110/1000 | Loss: 0.00001116
Iteration 111/1000 | Loss: 0.00001116
Iteration 112/1000 | Loss: 0.00001116
Iteration 113/1000 | Loss: 0.00001116
Iteration 114/1000 | Loss: 0.00001116
Iteration 115/1000 | Loss: 0.00001115
Iteration 116/1000 | Loss: 0.00001115
Iteration 117/1000 | Loss: 0.00001115
Iteration 118/1000 | Loss: 0.00001115
Iteration 119/1000 | Loss: 0.00001115
Iteration 120/1000 | Loss: 0.00001115
Iteration 121/1000 | Loss: 0.00001115
Iteration 122/1000 | Loss: 0.00001115
Iteration 123/1000 | Loss: 0.00001115
Iteration 124/1000 | Loss: 0.00001115
Iteration 125/1000 | Loss: 0.00001115
Iteration 126/1000 | Loss: 0.00001115
Iteration 127/1000 | Loss: 0.00001115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.1151279977639206e-05, 1.1151279977639206e-05, 1.1151279977639206e-05, 1.1151279977639206e-05, 1.1151279977639206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1151279977639206e-05

Optimization complete. Final v2v error: 2.889920949935913 mm

Highest mean error: 3.0667214393615723 mm for frame 70

Lowest mean error: 2.718996524810791 mm for frame 12

Saving results

Total time: 41.42753720283508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819702
Iteration 2/25 | Loss: 0.00133260
Iteration 3/25 | Loss: 0.00111863
Iteration 4/25 | Loss: 0.00109674
Iteration 5/25 | Loss: 0.00109356
Iteration 6/25 | Loss: 0.00109300
Iteration 7/25 | Loss: 0.00109300
Iteration 8/25 | Loss: 0.00109300
Iteration 9/25 | Loss: 0.00109300
Iteration 10/25 | Loss: 0.00109300
Iteration 11/25 | Loss: 0.00109300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010929954005405307, 0.0010929954005405307, 0.0010929954005405307, 0.0010929954005405307, 0.0010929954005405307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010929954005405307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33692837
Iteration 2/25 | Loss: 0.00056336
Iteration 3/25 | Loss: 0.00056332
Iteration 4/25 | Loss: 0.00056332
Iteration 5/25 | Loss: 0.00056332
Iteration 6/25 | Loss: 0.00056332
Iteration 7/25 | Loss: 0.00056332
Iteration 8/25 | Loss: 0.00056332
Iteration 9/25 | Loss: 0.00056332
Iteration 10/25 | Loss: 0.00056332
Iteration 11/25 | Loss: 0.00056332
Iteration 12/25 | Loss: 0.00056332
Iteration 13/25 | Loss: 0.00056332
Iteration 14/25 | Loss: 0.00056332
Iteration 15/25 | Loss: 0.00056332
Iteration 16/25 | Loss: 0.00056332
Iteration 17/25 | Loss: 0.00056332
Iteration 18/25 | Loss: 0.00056332
Iteration 19/25 | Loss: 0.00056332
Iteration 20/25 | Loss: 0.00056332
Iteration 21/25 | Loss: 0.00056332
Iteration 22/25 | Loss: 0.00056332
Iteration 23/25 | Loss: 0.00056332
Iteration 24/25 | Loss: 0.00056332
Iteration 25/25 | Loss: 0.00056332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056332
Iteration 2/1000 | Loss: 0.00002857
Iteration 3/1000 | Loss: 0.00002009
Iteration 4/1000 | Loss: 0.00001673
Iteration 5/1000 | Loss: 0.00001533
Iteration 6/1000 | Loss: 0.00001456
Iteration 7/1000 | Loss: 0.00001390
Iteration 8/1000 | Loss: 0.00001351
Iteration 9/1000 | Loss: 0.00001323
Iteration 10/1000 | Loss: 0.00001298
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001265
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001258
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001257
Iteration 17/1000 | Loss: 0.00001253
Iteration 18/1000 | Loss: 0.00001247
Iteration 19/1000 | Loss: 0.00001239
Iteration 20/1000 | Loss: 0.00001236
Iteration 21/1000 | Loss: 0.00001235
Iteration 22/1000 | Loss: 0.00001234
Iteration 23/1000 | Loss: 0.00001217
Iteration 24/1000 | Loss: 0.00001216
Iteration 25/1000 | Loss: 0.00001214
Iteration 26/1000 | Loss: 0.00001213
Iteration 27/1000 | Loss: 0.00001213
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001204
Iteration 31/1000 | Loss: 0.00001204
Iteration 32/1000 | Loss: 0.00001203
Iteration 33/1000 | Loss: 0.00001202
Iteration 34/1000 | Loss: 0.00001201
Iteration 35/1000 | Loss: 0.00001200
Iteration 36/1000 | Loss: 0.00001200
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001199
Iteration 39/1000 | Loss: 0.00001198
Iteration 40/1000 | Loss: 0.00001198
Iteration 41/1000 | Loss: 0.00001197
Iteration 42/1000 | Loss: 0.00001197
Iteration 43/1000 | Loss: 0.00001196
Iteration 44/1000 | Loss: 0.00001196
Iteration 45/1000 | Loss: 0.00001195
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001195
Iteration 49/1000 | Loss: 0.00001195
Iteration 50/1000 | Loss: 0.00001195
Iteration 51/1000 | Loss: 0.00001195
Iteration 52/1000 | Loss: 0.00001195
Iteration 53/1000 | Loss: 0.00001195
Iteration 54/1000 | Loss: 0.00001195
Iteration 55/1000 | Loss: 0.00001194
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001192
Iteration 59/1000 | Loss: 0.00001191
Iteration 60/1000 | Loss: 0.00001190
Iteration 61/1000 | Loss: 0.00001190
Iteration 62/1000 | Loss: 0.00001190
Iteration 63/1000 | Loss: 0.00001189
Iteration 64/1000 | Loss: 0.00001189
Iteration 65/1000 | Loss: 0.00001189
Iteration 66/1000 | Loss: 0.00001188
Iteration 67/1000 | Loss: 0.00001188
Iteration 68/1000 | Loss: 0.00001188
Iteration 69/1000 | Loss: 0.00001187
Iteration 70/1000 | Loss: 0.00001187
Iteration 71/1000 | Loss: 0.00001187
Iteration 72/1000 | Loss: 0.00001186
Iteration 73/1000 | Loss: 0.00001186
Iteration 74/1000 | Loss: 0.00001186
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001185
Iteration 77/1000 | Loss: 0.00001185
Iteration 78/1000 | Loss: 0.00001185
Iteration 79/1000 | Loss: 0.00001185
Iteration 80/1000 | Loss: 0.00001185
Iteration 81/1000 | Loss: 0.00001185
Iteration 82/1000 | Loss: 0.00001184
Iteration 83/1000 | Loss: 0.00001183
Iteration 84/1000 | Loss: 0.00001183
Iteration 85/1000 | Loss: 0.00001183
Iteration 86/1000 | Loss: 0.00001183
Iteration 87/1000 | Loss: 0.00001183
Iteration 88/1000 | Loss: 0.00001183
Iteration 89/1000 | Loss: 0.00001183
Iteration 90/1000 | Loss: 0.00001183
Iteration 91/1000 | Loss: 0.00001183
Iteration 92/1000 | Loss: 0.00001183
Iteration 93/1000 | Loss: 0.00001183
Iteration 94/1000 | Loss: 0.00001183
Iteration 95/1000 | Loss: 0.00001183
Iteration 96/1000 | Loss: 0.00001183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.1825810361187905e-05, 1.1825810361187905e-05, 1.1825810361187905e-05, 1.1825810361187905e-05, 1.1825810361187905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1825810361187905e-05

Optimization complete. Final v2v error: 2.92543363571167 mm

Highest mean error: 3.7972769737243652 mm for frame 36

Lowest mean error: 2.6527013778686523 mm for frame 8

Saving results

Total time: 36.32636475563049
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443984
Iteration 2/25 | Loss: 0.00117956
Iteration 3/25 | Loss: 0.00107791
Iteration 4/25 | Loss: 0.00105860
Iteration 5/25 | Loss: 0.00105213
Iteration 6/25 | Loss: 0.00105026
Iteration 7/25 | Loss: 0.00104985
Iteration 8/25 | Loss: 0.00104985
Iteration 9/25 | Loss: 0.00104985
Iteration 10/25 | Loss: 0.00104985
Iteration 11/25 | Loss: 0.00104985
Iteration 12/25 | Loss: 0.00104985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010498492047190666, 0.0010498492047190666, 0.0010498492047190666, 0.0010498492047190666, 0.0010498492047190666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010498492047190666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23600900
Iteration 2/25 | Loss: 0.00078309
Iteration 3/25 | Loss: 0.00078306
Iteration 4/25 | Loss: 0.00078306
Iteration 5/25 | Loss: 0.00078306
Iteration 6/25 | Loss: 0.00078306
Iteration 7/25 | Loss: 0.00078306
Iteration 8/25 | Loss: 0.00078306
Iteration 9/25 | Loss: 0.00078306
Iteration 10/25 | Loss: 0.00078306
Iteration 11/25 | Loss: 0.00078306
Iteration 12/25 | Loss: 0.00078306
Iteration 13/25 | Loss: 0.00078306
Iteration 14/25 | Loss: 0.00078306
Iteration 15/25 | Loss: 0.00078306
Iteration 16/25 | Loss: 0.00078306
Iteration 17/25 | Loss: 0.00078306
Iteration 18/25 | Loss: 0.00078306
Iteration 19/25 | Loss: 0.00078306
Iteration 20/25 | Loss: 0.00078306
Iteration 21/25 | Loss: 0.00078306
Iteration 22/25 | Loss: 0.00078306
Iteration 23/25 | Loss: 0.00078306
Iteration 24/25 | Loss: 0.00078306
Iteration 25/25 | Loss: 0.00078306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078306
Iteration 2/1000 | Loss: 0.00003702
Iteration 3/1000 | Loss: 0.00002179
Iteration 4/1000 | Loss: 0.00001601
Iteration 5/1000 | Loss: 0.00001424
Iteration 6/1000 | Loss: 0.00001328
Iteration 7/1000 | Loss: 0.00001255
Iteration 8/1000 | Loss: 0.00001208
Iteration 9/1000 | Loss: 0.00001167
Iteration 10/1000 | Loss: 0.00001143
Iteration 11/1000 | Loss: 0.00001137
Iteration 12/1000 | Loss: 0.00001132
Iteration 13/1000 | Loss: 0.00001131
Iteration 14/1000 | Loss: 0.00001130
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001125
Iteration 17/1000 | Loss: 0.00001111
Iteration 18/1000 | Loss: 0.00001107
Iteration 19/1000 | Loss: 0.00001106
Iteration 20/1000 | Loss: 0.00001106
Iteration 21/1000 | Loss: 0.00001105
Iteration 22/1000 | Loss: 0.00001104
Iteration 23/1000 | Loss: 0.00001103
Iteration 24/1000 | Loss: 0.00001103
Iteration 25/1000 | Loss: 0.00001102
Iteration 26/1000 | Loss: 0.00001097
Iteration 27/1000 | Loss: 0.00001095
Iteration 28/1000 | Loss: 0.00001095
Iteration 29/1000 | Loss: 0.00001094
Iteration 30/1000 | Loss: 0.00001093
Iteration 31/1000 | Loss: 0.00001093
Iteration 32/1000 | Loss: 0.00001092
Iteration 33/1000 | Loss: 0.00001091
Iteration 34/1000 | Loss: 0.00001091
Iteration 35/1000 | Loss: 0.00001091
Iteration 36/1000 | Loss: 0.00001090
Iteration 37/1000 | Loss: 0.00001088
Iteration 38/1000 | Loss: 0.00001087
Iteration 39/1000 | Loss: 0.00001086
Iteration 40/1000 | Loss: 0.00001086
Iteration 41/1000 | Loss: 0.00001086
Iteration 42/1000 | Loss: 0.00001085
Iteration 43/1000 | Loss: 0.00001084
Iteration 44/1000 | Loss: 0.00001081
Iteration 45/1000 | Loss: 0.00001076
Iteration 46/1000 | Loss: 0.00001076
Iteration 47/1000 | Loss: 0.00001073
Iteration 48/1000 | Loss: 0.00001073
Iteration 49/1000 | Loss: 0.00001072
Iteration 50/1000 | Loss: 0.00001072
Iteration 51/1000 | Loss: 0.00001072
Iteration 52/1000 | Loss: 0.00001072
Iteration 53/1000 | Loss: 0.00001071
Iteration 54/1000 | Loss: 0.00001071
Iteration 55/1000 | Loss: 0.00001071
Iteration 56/1000 | Loss: 0.00001070
Iteration 57/1000 | Loss: 0.00001070
Iteration 58/1000 | Loss: 0.00001070
Iteration 59/1000 | Loss: 0.00001069
Iteration 60/1000 | Loss: 0.00001069
Iteration 61/1000 | Loss: 0.00001069
Iteration 62/1000 | Loss: 0.00001069
Iteration 63/1000 | Loss: 0.00001068
Iteration 64/1000 | Loss: 0.00001068
Iteration 65/1000 | Loss: 0.00001068
Iteration 66/1000 | Loss: 0.00001068
Iteration 67/1000 | Loss: 0.00001068
Iteration 68/1000 | Loss: 0.00001068
Iteration 69/1000 | Loss: 0.00001067
Iteration 70/1000 | Loss: 0.00001067
Iteration 71/1000 | Loss: 0.00001067
Iteration 72/1000 | Loss: 0.00001066
Iteration 73/1000 | Loss: 0.00001066
Iteration 74/1000 | Loss: 0.00001066
Iteration 75/1000 | Loss: 0.00001065
Iteration 76/1000 | Loss: 0.00001065
Iteration 77/1000 | Loss: 0.00001065
Iteration 78/1000 | Loss: 0.00001065
Iteration 79/1000 | Loss: 0.00001065
Iteration 80/1000 | Loss: 0.00001065
Iteration 81/1000 | Loss: 0.00001065
Iteration 82/1000 | Loss: 0.00001065
Iteration 83/1000 | Loss: 0.00001065
Iteration 84/1000 | Loss: 0.00001065
Iteration 85/1000 | Loss: 0.00001065
Iteration 86/1000 | Loss: 0.00001065
Iteration 87/1000 | Loss: 0.00001065
Iteration 88/1000 | Loss: 0.00001065
Iteration 89/1000 | Loss: 0.00001065
Iteration 90/1000 | Loss: 0.00001065
Iteration 91/1000 | Loss: 0.00001064
Iteration 92/1000 | Loss: 0.00001064
Iteration 93/1000 | Loss: 0.00001064
Iteration 94/1000 | Loss: 0.00001064
Iteration 95/1000 | Loss: 0.00001064
Iteration 96/1000 | Loss: 0.00001064
Iteration 97/1000 | Loss: 0.00001064
Iteration 98/1000 | Loss: 0.00001064
Iteration 99/1000 | Loss: 0.00001064
Iteration 100/1000 | Loss: 0.00001064
Iteration 101/1000 | Loss: 0.00001063
Iteration 102/1000 | Loss: 0.00001063
Iteration 103/1000 | Loss: 0.00001063
Iteration 104/1000 | Loss: 0.00001063
Iteration 105/1000 | Loss: 0.00001063
Iteration 106/1000 | Loss: 0.00001062
Iteration 107/1000 | Loss: 0.00001062
Iteration 108/1000 | Loss: 0.00001062
Iteration 109/1000 | Loss: 0.00001062
Iteration 110/1000 | Loss: 0.00001062
Iteration 111/1000 | Loss: 0.00001062
Iteration 112/1000 | Loss: 0.00001062
Iteration 113/1000 | Loss: 0.00001062
Iteration 114/1000 | Loss: 0.00001062
Iteration 115/1000 | Loss: 0.00001062
Iteration 116/1000 | Loss: 0.00001062
Iteration 117/1000 | Loss: 0.00001062
Iteration 118/1000 | Loss: 0.00001061
Iteration 119/1000 | Loss: 0.00001061
Iteration 120/1000 | Loss: 0.00001061
Iteration 121/1000 | Loss: 0.00001061
Iteration 122/1000 | Loss: 0.00001061
Iteration 123/1000 | Loss: 0.00001061
Iteration 124/1000 | Loss: 0.00001061
Iteration 125/1000 | Loss: 0.00001061
Iteration 126/1000 | Loss: 0.00001061
Iteration 127/1000 | Loss: 0.00001061
Iteration 128/1000 | Loss: 0.00001061
Iteration 129/1000 | Loss: 0.00001061
Iteration 130/1000 | Loss: 0.00001061
Iteration 131/1000 | Loss: 0.00001060
Iteration 132/1000 | Loss: 0.00001060
Iteration 133/1000 | Loss: 0.00001060
Iteration 134/1000 | Loss: 0.00001060
Iteration 135/1000 | Loss: 0.00001060
Iteration 136/1000 | Loss: 0.00001060
Iteration 137/1000 | Loss: 0.00001060
Iteration 138/1000 | Loss: 0.00001060
Iteration 139/1000 | Loss: 0.00001059
Iteration 140/1000 | Loss: 0.00001059
Iteration 141/1000 | Loss: 0.00001059
Iteration 142/1000 | Loss: 0.00001058
Iteration 143/1000 | Loss: 0.00001058
Iteration 144/1000 | Loss: 0.00001057
Iteration 145/1000 | Loss: 0.00001057
Iteration 146/1000 | Loss: 0.00001057
Iteration 147/1000 | Loss: 0.00001057
Iteration 148/1000 | Loss: 0.00001057
Iteration 149/1000 | Loss: 0.00001057
Iteration 150/1000 | Loss: 0.00001057
Iteration 151/1000 | Loss: 0.00001057
Iteration 152/1000 | Loss: 0.00001057
Iteration 153/1000 | Loss: 0.00001057
Iteration 154/1000 | Loss: 0.00001057
Iteration 155/1000 | Loss: 0.00001057
Iteration 156/1000 | Loss: 0.00001057
Iteration 157/1000 | Loss: 0.00001057
Iteration 158/1000 | Loss: 0.00001057
Iteration 159/1000 | Loss: 0.00001057
Iteration 160/1000 | Loss: 0.00001056
Iteration 161/1000 | Loss: 0.00001056
Iteration 162/1000 | Loss: 0.00001056
Iteration 163/1000 | Loss: 0.00001056
Iteration 164/1000 | Loss: 0.00001056
Iteration 165/1000 | Loss: 0.00001055
Iteration 166/1000 | Loss: 0.00001055
Iteration 167/1000 | Loss: 0.00001055
Iteration 168/1000 | Loss: 0.00001055
Iteration 169/1000 | Loss: 0.00001055
Iteration 170/1000 | Loss: 0.00001055
Iteration 171/1000 | Loss: 0.00001055
Iteration 172/1000 | Loss: 0.00001055
Iteration 173/1000 | Loss: 0.00001055
Iteration 174/1000 | Loss: 0.00001054
Iteration 175/1000 | Loss: 0.00001054
Iteration 176/1000 | Loss: 0.00001054
Iteration 177/1000 | Loss: 0.00001054
Iteration 178/1000 | Loss: 0.00001054
Iteration 179/1000 | Loss: 0.00001054
Iteration 180/1000 | Loss: 0.00001054
Iteration 181/1000 | Loss: 0.00001054
Iteration 182/1000 | Loss: 0.00001054
Iteration 183/1000 | Loss: 0.00001054
Iteration 184/1000 | Loss: 0.00001054
Iteration 185/1000 | Loss: 0.00001054
Iteration 186/1000 | Loss: 0.00001054
Iteration 187/1000 | Loss: 0.00001054
Iteration 188/1000 | Loss: 0.00001054
Iteration 189/1000 | Loss: 0.00001054
Iteration 190/1000 | Loss: 0.00001054
Iteration 191/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.054145013767993e-05, 1.054145013767993e-05, 1.054145013767993e-05, 1.054145013767993e-05, 1.054145013767993e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.054145013767993e-05

Optimization complete. Final v2v error: 2.7961883544921875 mm

Highest mean error: 3.2856485843658447 mm for frame 57

Lowest mean error: 2.4898054599761963 mm for frame 16

Saving results

Total time: 41.553138256073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420150
Iteration 2/25 | Loss: 0.00119074
Iteration 3/25 | Loss: 0.00110399
Iteration 4/25 | Loss: 0.00108762
Iteration 5/25 | Loss: 0.00108350
Iteration 6/25 | Loss: 0.00108311
Iteration 7/25 | Loss: 0.00108311
Iteration 8/25 | Loss: 0.00108311
Iteration 9/25 | Loss: 0.00108311
Iteration 10/25 | Loss: 0.00108311
Iteration 11/25 | Loss: 0.00108311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010831108083948493, 0.0010831108083948493, 0.0010831108083948493, 0.0010831108083948493, 0.0010831108083948493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010831108083948493

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.10462189
Iteration 2/25 | Loss: 0.00072585
Iteration 3/25 | Loss: 0.00072584
Iteration 4/25 | Loss: 0.00072584
Iteration 5/25 | Loss: 0.00072584
Iteration 6/25 | Loss: 0.00072584
Iteration 7/25 | Loss: 0.00072584
Iteration 8/25 | Loss: 0.00072584
Iteration 9/25 | Loss: 0.00072584
Iteration 10/25 | Loss: 0.00072584
Iteration 11/25 | Loss: 0.00072584
Iteration 12/25 | Loss: 0.00072584
Iteration 13/25 | Loss: 0.00072584
Iteration 14/25 | Loss: 0.00072584
Iteration 15/25 | Loss: 0.00072584
Iteration 16/25 | Loss: 0.00072584
Iteration 17/25 | Loss: 0.00072584
Iteration 18/25 | Loss: 0.00072584
Iteration 19/25 | Loss: 0.00072584
Iteration 20/25 | Loss: 0.00072584
Iteration 21/25 | Loss: 0.00072584
Iteration 22/25 | Loss: 0.00072584
Iteration 23/25 | Loss: 0.00072584
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007258389377966523, 0.0007258389377966523, 0.0007258389377966523, 0.0007258389377966523, 0.0007258389377966523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007258389377966523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072584
Iteration 2/1000 | Loss: 0.00002106
Iteration 3/1000 | Loss: 0.00001687
Iteration 4/1000 | Loss: 0.00001577
Iteration 5/1000 | Loss: 0.00001495
Iteration 6/1000 | Loss: 0.00001442
Iteration 7/1000 | Loss: 0.00001405
Iteration 8/1000 | Loss: 0.00001373
Iteration 9/1000 | Loss: 0.00001353
Iteration 10/1000 | Loss: 0.00001329
Iteration 11/1000 | Loss: 0.00001328
Iteration 12/1000 | Loss: 0.00001309
Iteration 13/1000 | Loss: 0.00001301
Iteration 14/1000 | Loss: 0.00001288
Iteration 15/1000 | Loss: 0.00001287
Iteration 16/1000 | Loss: 0.00001286
Iteration 17/1000 | Loss: 0.00001285
Iteration 18/1000 | Loss: 0.00001280
Iteration 19/1000 | Loss: 0.00001280
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001278
Iteration 23/1000 | Loss: 0.00001277
Iteration 24/1000 | Loss: 0.00001277
Iteration 25/1000 | Loss: 0.00001277
Iteration 26/1000 | Loss: 0.00001272
Iteration 27/1000 | Loss: 0.00001267
Iteration 28/1000 | Loss: 0.00001267
Iteration 29/1000 | Loss: 0.00001264
Iteration 30/1000 | Loss: 0.00001263
Iteration 31/1000 | Loss: 0.00001259
Iteration 32/1000 | Loss: 0.00001259
Iteration 33/1000 | Loss: 0.00001258
Iteration 34/1000 | Loss: 0.00001258
Iteration 35/1000 | Loss: 0.00001257
Iteration 36/1000 | Loss: 0.00001257
Iteration 37/1000 | Loss: 0.00001257
Iteration 38/1000 | Loss: 0.00001255
Iteration 39/1000 | Loss: 0.00001255
Iteration 40/1000 | Loss: 0.00001255
Iteration 41/1000 | Loss: 0.00001254
Iteration 42/1000 | Loss: 0.00001253
Iteration 43/1000 | Loss: 0.00001252
Iteration 44/1000 | Loss: 0.00001252
Iteration 45/1000 | Loss: 0.00001252
Iteration 46/1000 | Loss: 0.00001252
Iteration 47/1000 | Loss: 0.00001252
Iteration 48/1000 | Loss: 0.00001252
Iteration 49/1000 | Loss: 0.00001251
Iteration 50/1000 | Loss: 0.00001251
Iteration 51/1000 | Loss: 0.00001251
Iteration 52/1000 | Loss: 0.00001250
Iteration 53/1000 | Loss: 0.00001250
Iteration 54/1000 | Loss: 0.00001249
Iteration 55/1000 | Loss: 0.00001249
Iteration 56/1000 | Loss: 0.00001249
Iteration 57/1000 | Loss: 0.00001249
Iteration 58/1000 | Loss: 0.00001249
Iteration 59/1000 | Loss: 0.00001248
Iteration 60/1000 | Loss: 0.00001248
Iteration 61/1000 | Loss: 0.00001248
Iteration 62/1000 | Loss: 0.00001247
Iteration 63/1000 | Loss: 0.00001247
Iteration 64/1000 | Loss: 0.00001246
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001245
Iteration 67/1000 | Loss: 0.00001244
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001242
Iteration 70/1000 | Loss: 0.00001241
Iteration 71/1000 | Loss: 0.00001241
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00001240
Iteration 74/1000 | Loss: 0.00001240
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001238
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001237
Iteration 82/1000 | Loss: 0.00001237
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001236
Iteration 87/1000 | Loss: 0.00001236
Iteration 88/1000 | Loss: 0.00001236
Iteration 89/1000 | Loss: 0.00001236
Iteration 90/1000 | Loss: 0.00001236
Iteration 91/1000 | Loss: 0.00001236
Iteration 92/1000 | Loss: 0.00001236
Iteration 93/1000 | Loss: 0.00001235
Iteration 94/1000 | Loss: 0.00001235
Iteration 95/1000 | Loss: 0.00001235
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001234
Iteration 98/1000 | Loss: 0.00001234
Iteration 99/1000 | Loss: 0.00001234
Iteration 100/1000 | Loss: 0.00001234
Iteration 101/1000 | Loss: 0.00001234
Iteration 102/1000 | Loss: 0.00001234
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001233
Iteration 106/1000 | Loss: 0.00001233
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001233
Iteration 109/1000 | Loss: 0.00001233
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001233
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001232
Iteration 124/1000 | Loss: 0.00001232
Iteration 125/1000 | Loss: 0.00001232
Iteration 126/1000 | Loss: 0.00001232
Iteration 127/1000 | Loss: 0.00001232
Iteration 128/1000 | Loss: 0.00001232
Iteration 129/1000 | Loss: 0.00001232
Iteration 130/1000 | Loss: 0.00001232
Iteration 131/1000 | Loss: 0.00001231
Iteration 132/1000 | Loss: 0.00001231
Iteration 133/1000 | Loss: 0.00001231
Iteration 134/1000 | Loss: 0.00001231
Iteration 135/1000 | Loss: 0.00001231
Iteration 136/1000 | Loss: 0.00001230
Iteration 137/1000 | Loss: 0.00001230
Iteration 138/1000 | Loss: 0.00001230
Iteration 139/1000 | Loss: 0.00001230
Iteration 140/1000 | Loss: 0.00001230
Iteration 141/1000 | Loss: 0.00001230
Iteration 142/1000 | Loss: 0.00001230
Iteration 143/1000 | Loss: 0.00001230
Iteration 144/1000 | Loss: 0.00001230
Iteration 145/1000 | Loss: 0.00001230
Iteration 146/1000 | Loss: 0.00001230
Iteration 147/1000 | Loss: 0.00001230
Iteration 148/1000 | Loss: 0.00001230
Iteration 149/1000 | Loss: 0.00001230
Iteration 150/1000 | Loss: 0.00001230
Iteration 151/1000 | Loss: 0.00001230
Iteration 152/1000 | Loss: 0.00001230
Iteration 153/1000 | Loss: 0.00001230
Iteration 154/1000 | Loss: 0.00001230
Iteration 155/1000 | Loss: 0.00001230
Iteration 156/1000 | Loss: 0.00001230
Iteration 157/1000 | Loss: 0.00001230
Iteration 158/1000 | Loss: 0.00001230
Iteration 159/1000 | Loss: 0.00001230
Iteration 160/1000 | Loss: 0.00001230
Iteration 161/1000 | Loss: 0.00001230
Iteration 162/1000 | Loss: 0.00001230
Iteration 163/1000 | Loss: 0.00001230
Iteration 164/1000 | Loss: 0.00001230
Iteration 165/1000 | Loss: 0.00001230
Iteration 166/1000 | Loss: 0.00001230
Iteration 167/1000 | Loss: 0.00001230
Iteration 168/1000 | Loss: 0.00001230
Iteration 169/1000 | Loss: 0.00001230
Iteration 170/1000 | Loss: 0.00001230
Iteration 171/1000 | Loss: 0.00001230
Iteration 172/1000 | Loss: 0.00001230
Iteration 173/1000 | Loss: 0.00001230
Iteration 174/1000 | Loss: 0.00001230
Iteration 175/1000 | Loss: 0.00001230
Iteration 176/1000 | Loss: 0.00001230
Iteration 177/1000 | Loss: 0.00001230
Iteration 178/1000 | Loss: 0.00001230
Iteration 179/1000 | Loss: 0.00001230
Iteration 180/1000 | Loss: 0.00001230
Iteration 181/1000 | Loss: 0.00001230
Iteration 182/1000 | Loss: 0.00001230
Iteration 183/1000 | Loss: 0.00001230
Iteration 184/1000 | Loss: 0.00001230
Iteration 185/1000 | Loss: 0.00001230
Iteration 186/1000 | Loss: 0.00001230
Iteration 187/1000 | Loss: 0.00001230
Iteration 188/1000 | Loss: 0.00001230
Iteration 189/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.229813278769143e-05, 1.229813278769143e-05, 1.229813278769143e-05, 1.229813278769143e-05, 1.229813278769143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.229813278769143e-05

Optimization complete. Final v2v error: 2.986926317214966 mm

Highest mean error: 3.7934556007385254 mm for frame 236

Lowest mean error: 2.8285834789276123 mm for frame 10

Saving results

Total time: 44.917062759399414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768477
Iteration 2/25 | Loss: 0.00142789
Iteration 3/25 | Loss: 0.00114088
Iteration 4/25 | Loss: 0.00110753
Iteration 5/25 | Loss: 0.00110397
Iteration 6/25 | Loss: 0.00110386
Iteration 7/25 | Loss: 0.00110386
Iteration 8/25 | Loss: 0.00110386
Iteration 9/25 | Loss: 0.00110386
Iteration 10/25 | Loss: 0.00110386
Iteration 11/25 | Loss: 0.00110386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011038585798814893, 0.0011038585798814893, 0.0011038585798814893, 0.0011038585798814893, 0.0011038585798814893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011038585798814893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31859195
Iteration 2/25 | Loss: 0.00068991
Iteration 3/25 | Loss: 0.00068988
Iteration 4/25 | Loss: 0.00068988
Iteration 5/25 | Loss: 0.00068988
Iteration 6/25 | Loss: 0.00068988
Iteration 7/25 | Loss: 0.00068988
Iteration 8/25 | Loss: 0.00068987
Iteration 9/25 | Loss: 0.00068987
Iteration 10/25 | Loss: 0.00068987
Iteration 11/25 | Loss: 0.00068987
Iteration 12/25 | Loss: 0.00068987
Iteration 13/25 | Loss: 0.00068987
Iteration 14/25 | Loss: 0.00068987
Iteration 15/25 | Loss: 0.00068987
Iteration 16/25 | Loss: 0.00068987
Iteration 17/25 | Loss: 0.00068987
Iteration 18/25 | Loss: 0.00068987
Iteration 19/25 | Loss: 0.00068987
Iteration 20/25 | Loss: 0.00068987
Iteration 21/25 | Loss: 0.00068987
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00068987428676337, 0.00068987428676337, 0.00068987428676337, 0.00068987428676337, 0.00068987428676337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00068987428676337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068987
Iteration 2/1000 | Loss: 0.00003105
Iteration 3/1000 | Loss: 0.00002158
Iteration 4/1000 | Loss: 0.00001703
Iteration 5/1000 | Loss: 0.00001588
Iteration 6/1000 | Loss: 0.00001504
Iteration 7/1000 | Loss: 0.00001441
Iteration 8/1000 | Loss: 0.00001392
Iteration 9/1000 | Loss: 0.00001354
Iteration 10/1000 | Loss: 0.00001309
Iteration 11/1000 | Loss: 0.00001285
Iteration 12/1000 | Loss: 0.00001281
Iteration 13/1000 | Loss: 0.00001277
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001247
Iteration 16/1000 | Loss: 0.00001243
Iteration 17/1000 | Loss: 0.00001237
Iteration 18/1000 | Loss: 0.00001236
Iteration 19/1000 | Loss: 0.00001235
Iteration 20/1000 | Loss: 0.00001234
Iteration 21/1000 | Loss: 0.00001234
Iteration 22/1000 | Loss: 0.00001229
Iteration 23/1000 | Loss: 0.00001226
Iteration 24/1000 | Loss: 0.00001226
Iteration 25/1000 | Loss: 0.00001225
Iteration 26/1000 | Loss: 0.00001225
Iteration 27/1000 | Loss: 0.00001224
Iteration 28/1000 | Loss: 0.00001223
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001223
Iteration 31/1000 | Loss: 0.00001222
Iteration 32/1000 | Loss: 0.00001222
Iteration 33/1000 | Loss: 0.00001221
Iteration 34/1000 | Loss: 0.00001221
Iteration 35/1000 | Loss: 0.00001221
Iteration 36/1000 | Loss: 0.00001220
Iteration 37/1000 | Loss: 0.00001220
Iteration 38/1000 | Loss: 0.00001220
Iteration 39/1000 | Loss: 0.00001220
Iteration 40/1000 | Loss: 0.00001219
Iteration 41/1000 | Loss: 0.00001219
Iteration 42/1000 | Loss: 0.00001219
Iteration 43/1000 | Loss: 0.00001219
Iteration 44/1000 | Loss: 0.00001218
Iteration 45/1000 | Loss: 0.00001218
Iteration 46/1000 | Loss: 0.00001217
Iteration 47/1000 | Loss: 0.00001217
Iteration 48/1000 | Loss: 0.00001216
Iteration 49/1000 | Loss: 0.00001216
Iteration 50/1000 | Loss: 0.00001216
Iteration 51/1000 | Loss: 0.00001215
Iteration 52/1000 | Loss: 0.00001214
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001213
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00001213
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001212
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001212
Iteration 63/1000 | Loss: 0.00001212
Iteration 64/1000 | Loss: 0.00001212
Iteration 65/1000 | Loss: 0.00001212
Iteration 66/1000 | Loss: 0.00001212
Iteration 67/1000 | Loss: 0.00001211
Iteration 68/1000 | Loss: 0.00001211
Iteration 69/1000 | Loss: 0.00001211
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001211
Iteration 74/1000 | Loss: 0.00001211
Iteration 75/1000 | Loss: 0.00001210
Iteration 76/1000 | Loss: 0.00001210
Iteration 77/1000 | Loss: 0.00001210
Iteration 78/1000 | Loss: 0.00001210
Iteration 79/1000 | Loss: 0.00001210
Iteration 80/1000 | Loss: 0.00001210
Iteration 81/1000 | Loss: 0.00001210
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001210
Iteration 84/1000 | Loss: 0.00001210
Iteration 85/1000 | Loss: 0.00001210
Iteration 86/1000 | Loss: 0.00001210
Iteration 87/1000 | Loss: 0.00001209
Iteration 88/1000 | Loss: 0.00001209
Iteration 89/1000 | Loss: 0.00001209
Iteration 90/1000 | Loss: 0.00001209
Iteration 91/1000 | Loss: 0.00001209
Iteration 92/1000 | Loss: 0.00001209
Iteration 93/1000 | Loss: 0.00001209
Iteration 94/1000 | Loss: 0.00001208
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001207
Iteration 97/1000 | Loss: 0.00001207
Iteration 98/1000 | Loss: 0.00001207
Iteration 99/1000 | Loss: 0.00001207
Iteration 100/1000 | Loss: 0.00001206
Iteration 101/1000 | Loss: 0.00001206
Iteration 102/1000 | Loss: 0.00001206
Iteration 103/1000 | Loss: 0.00001206
Iteration 104/1000 | Loss: 0.00001206
Iteration 105/1000 | Loss: 0.00001206
Iteration 106/1000 | Loss: 0.00001206
Iteration 107/1000 | Loss: 0.00001205
Iteration 108/1000 | Loss: 0.00001205
Iteration 109/1000 | Loss: 0.00001205
Iteration 110/1000 | Loss: 0.00001205
Iteration 111/1000 | Loss: 0.00001205
Iteration 112/1000 | Loss: 0.00001205
Iteration 113/1000 | Loss: 0.00001205
Iteration 114/1000 | Loss: 0.00001205
Iteration 115/1000 | Loss: 0.00001204
Iteration 116/1000 | Loss: 0.00001204
Iteration 117/1000 | Loss: 0.00001204
Iteration 118/1000 | Loss: 0.00001204
Iteration 119/1000 | Loss: 0.00001204
Iteration 120/1000 | Loss: 0.00001204
Iteration 121/1000 | Loss: 0.00001204
Iteration 122/1000 | Loss: 0.00001204
Iteration 123/1000 | Loss: 0.00001204
Iteration 124/1000 | Loss: 0.00001204
Iteration 125/1000 | Loss: 0.00001204
Iteration 126/1000 | Loss: 0.00001204
Iteration 127/1000 | Loss: 0.00001204
Iteration 128/1000 | Loss: 0.00001204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.2038977729389444e-05, 1.2038977729389444e-05, 1.2038977729389444e-05, 1.2038977729389444e-05, 1.2038977729389444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2038977729389444e-05

Optimization complete. Final v2v error: 2.9584362506866455 mm

Highest mean error: 3.608808994293213 mm for frame 22

Lowest mean error: 2.548705577850342 mm for frame 74

Saving results

Total time: 42.263919830322266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485925
Iteration 2/25 | Loss: 0.00128812
Iteration 3/25 | Loss: 0.00114342
Iteration 4/25 | Loss: 0.00113113
Iteration 5/25 | Loss: 0.00112806
Iteration 6/25 | Loss: 0.00112761
Iteration 7/25 | Loss: 0.00112761
Iteration 8/25 | Loss: 0.00112761
Iteration 9/25 | Loss: 0.00112761
Iteration 10/25 | Loss: 0.00112761
Iteration 11/25 | Loss: 0.00112761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011276068398728967, 0.0011276068398728967, 0.0011276068398728967, 0.0011276068398728967, 0.0011276068398728967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011276068398728967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45041990
Iteration 2/25 | Loss: 0.00079906
Iteration 3/25 | Loss: 0.00079904
Iteration 4/25 | Loss: 0.00079904
Iteration 5/25 | Loss: 0.00079904
Iteration 6/25 | Loss: 0.00079904
Iteration 7/25 | Loss: 0.00079904
Iteration 8/25 | Loss: 0.00079904
Iteration 9/25 | Loss: 0.00079904
Iteration 10/25 | Loss: 0.00079904
Iteration 11/25 | Loss: 0.00079904
Iteration 12/25 | Loss: 0.00079904
Iteration 13/25 | Loss: 0.00079904
Iteration 14/25 | Loss: 0.00079904
Iteration 15/25 | Loss: 0.00079904
Iteration 16/25 | Loss: 0.00079904
Iteration 17/25 | Loss: 0.00079904
Iteration 18/25 | Loss: 0.00079904
Iteration 19/25 | Loss: 0.00079904
Iteration 20/25 | Loss: 0.00079904
Iteration 21/25 | Loss: 0.00079904
Iteration 22/25 | Loss: 0.00079904
Iteration 23/25 | Loss: 0.00079904
Iteration 24/25 | Loss: 0.00079904
Iteration 25/25 | Loss: 0.00079904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079904
Iteration 2/1000 | Loss: 0.00002819
Iteration 3/1000 | Loss: 0.00001873
Iteration 4/1000 | Loss: 0.00001686
Iteration 5/1000 | Loss: 0.00001585
Iteration 6/1000 | Loss: 0.00001537
Iteration 7/1000 | Loss: 0.00001487
Iteration 8/1000 | Loss: 0.00001463
Iteration 9/1000 | Loss: 0.00001435
Iteration 10/1000 | Loss: 0.00001411
Iteration 11/1000 | Loss: 0.00001394
Iteration 12/1000 | Loss: 0.00001386
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001373
Iteration 16/1000 | Loss: 0.00001370
Iteration 17/1000 | Loss: 0.00001369
Iteration 18/1000 | Loss: 0.00001369
Iteration 19/1000 | Loss: 0.00001369
Iteration 20/1000 | Loss: 0.00001369
Iteration 21/1000 | Loss: 0.00001368
Iteration 22/1000 | Loss: 0.00001368
Iteration 23/1000 | Loss: 0.00001367
Iteration 24/1000 | Loss: 0.00001366
Iteration 25/1000 | Loss: 0.00001365
Iteration 26/1000 | Loss: 0.00001364
Iteration 27/1000 | Loss: 0.00001364
Iteration 28/1000 | Loss: 0.00001364
Iteration 29/1000 | Loss: 0.00001364
Iteration 30/1000 | Loss: 0.00001364
Iteration 31/1000 | Loss: 0.00001364
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001364
Iteration 35/1000 | Loss: 0.00001364
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001362
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001361
Iteration 40/1000 | Loss: 0.00001361
Iteration 41/1000 | Loss: 0.00001360
Iteration 42/1000 | Loss: 0.00001360
Iteration 43/1000 | Loss: 0.00001360
Iteration 44/1000 | Loss: 0.00001359
Iteration 45/1000 | Loss: 0.00001359
Iteration 46/1000 | Loss: 0.00001359
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001358
Iteration 49/1000 | Loss: 0.00001358
Iteration 50/1000 | Loss: 0.00001358
Iteration 51/1000 | Loss: 0.00001358
Iteration 52/1000 | Loss: 0.00001358
Iteration 53/1000 | Loss: 0.00001358
Iteration 54/1000 | Loss: 0.00001357
Iteration 55/1000 | Loss: 0.00001357
Iteration 56/1000 | Loss: 0.00001357
Iteration 57/1000 | Loss: 0.00001357
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001357
Iteration 62/1000 | Loss: 0.00001357
Iteration 63/1000 | Loss: 0.00001357
Iteration 64/1000 | Loss: 0.00001357
Iteration 65/1000 | Loss: 0.00001357
Iteration 66/1000 | Loss: 0.00001357
Iteration 67/1000 | Loss: 0.00001357
Iteration 68/1000 | Loss: 0.00001357
Iteration 69/1000 | Loss: 0.00001357
Iteration 70/1000 | Loss: 0.00001357
Iteration 71/1000 | Loss: 0.00001357
Iteration 72/1000 | Loss: 0.00001357
Iteration 73/1000 | Loss: 0.00001357
Iteration 74/1000 | Loss: 0.00001357
Iteration 75/1000 | Loss: 0.00001357
Iteration 76/1000 | Loss: 0.00001357
Iteration 77/1000 | Loss: 0.00001357
Iteration 78/1000 | Loss: 0.00001357
Iteration 79/1000 | Loss: 0.00001357
Iteration 80/1000 | Loss: 0.00001357
Iteration 81/1000 | Loss: 0.00001357
Iteration 82/1000 | Loss: 0.00001357
Iteration 83/1000 | Loss: 0.00001357
Iteration 84/1000 | Loss: 0.00001357
Iteration 85/1000 | Loss: 0.00001357
Iteration 86/1000 | Loss: 0.00001357
Iteration 87/1000 | Loss: 0.00001357
Iteration 88/1000 | Loss: 0.00001357
Iteration 89/1000 | Loss: 0.00001357
Iteration 90/1000 | Loss: 0.00001357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.3571007002610713e-05, 1.3571007002610713e-05, 1.3571007002610713e-05, 1.3571007002610713e-05, 1.3571007002610713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3571007002610713e-05

Optimization complete. Final v2v error: 3.0285098552703857 mm

Highest mean error: 4.176766872406006 mm for frame 110

Lowest mean error: 2.4392004013061523 mm for frame 10

Saving results

Total time: 31.64865207672119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490658
Iteration 2/25 | Loss: 0.00117011
Iteration 3/25 | Loss: 0.00108323
Iteration 4/25 | Loss: 0.00107152
Iteration 5/25 | Loss: 0.00106773
Iteration 6/25 | Loss: 0.00106718
Iteration 7/25 | Loss: 0.00106718
Iteration 8/25 | Loss: 0.00106718
Iteration 9/25 | Loss: 0.00106718
Iteration 10/25 | Loss: 0.00106718
Iteration 11/25 | Loss: 0.00106718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001067180885002017, 0.001067180885002017, 0.001067180885002017, 0.001067180885002017, 0.001067180885002017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001067180885002017

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24022722
Iteration 2/25 | Loss: 0.00081453
Iteration 3/25 | Loss: 0.00081452
Iteration 4/25 | Loss: 0.00081452
Iteration 5/25 | Loss: 0.00081451
Iteration 6/25 | Loss: 0.00081451
Iteration 7/25 | Loss: 0.00081451
Iteration 8/25 | Loss: 0.00081451
Iteration 9/25 | Loss: 0.00081451
Iteration 10/25 | Loss: 0.00081451
Iteration 11/25 | Loss: 0.00081451
Iteration 12/25 | Loss: 0.00081451
Iteration 13/25 | Loss: 0.00081451
Iteration 14/25 | Loss: 0.00081451
Iteration 15/25 | Loss: 0.00081451
Iteration 16/25 | Loss: 0.00081451
Iteration 17/25 | Loss: 0.00081451
Iteration 18/25 | Loss: 0.00081451
Iteration 19/25 | Loss: 0.00081451
Iteration 20/25 | Loss: 0.00081451
Iteration 21/25 | Loss: 0.00081451
Iteration 22/25 | Loss: 0.00081451
Iteration 23/25 | Loss: 0.00081451
Iteration 24/25 | Loss: 0.00081451
Iteration 25/25 | Loss: 0.00081451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081451
Iteration 2/1000 | Loss: 0.00001678
Iteration 3/1000 | Loss: 0.00001338
Iteration 4/1000 | Loss: 0.00001240
Iteration 5/1000 | Loss: 0.00001180
Iteration 6/1000 | Loss: 0.00001142
Iteration 7/1000 | Loss: 0.00001108
Iteration 8/1000 | Loss: 0.00001081
Iteration 9/1000 | Loss: 0.00001068
Iteration 10/1000 | Loss: 0.00001068
Iteration 11/1000 | Loss: 0.00001063
Iteration 12/1000 | Loss: 0.00001063
Iteration 13/1000 | Loss: 0.00001062
Iteration 14/1000 | Loss: 0.00001047
Iteration 15/1000 | Loss: 0.00001038
Iteration 16/1000 | Loss: 0.00001038
Iteration 17/1000 | Loss: 0.00001037
Iteration 18/1000 | Loss: 0.00001030
Iteration 19/1000 | Loss: 0.00001030
Iteration 20/1000 | Loss: 0.00001023
Iteration 21/1000 | Loss: 0.00001016
Iteration 22/1000 | Loss: 0.00001008
Iteration 23/1000 | Loss: 0.00001007
Iteration 24/1000 | Loss: 0.00001005
Iteration 25/1000 | Loss: 0.00001005
Iteration 26/1000 | Loss: 0.00001005
Iteration 27/1000 | Loss: 0.00001005
Iteration 28/1000 | Loss: 0.00001005
Iteration 29/1000 | Loss: 0.00001004
Iteration 30/1000 | Loss: 0.00001002
Iteration 31/1000 | Loss: 0.00001001
Iteration 32/1000 | Loss: 0.00001000
Iteration 33/1000 | Loss: 0.00001000
Iteration 34/1000 | Loss: 0.00001000
Iteration 35/1000 | Loss: 0.00000999
Iteration 36/1000 | Loss: 0.00000998
Iteration 37/1000 | Loss: 0.00000996
Iteration 38/1000 | Loss: 0.00000996
Iteration 39/1000 | Loss: 0.00000995
Iteration 40/1000 | Loss: 0.00000995
Iteration 41/1000 | Loss: 0.00000995
Iteration 42/1000 | Loss: 0.00000995
Iteration 43/1000 | Loss: 0.00000995
Iteration 44/1000 | Loss: 0.00000995
Iteration 45/1000 | Loss: 0.00000995
Iteration 46/1000 | Loss: 0.00000995
Iteration 47/1000 | Loss: 0.00000995
Iteration 48/1000 | Loss: 0.00000994
Iteration 49/1000 | Loss: 0.00000994
Iteration 50/1000 | Loss: 0.00000994
Iteration 51/1000 | Loss: 0.00000993
Iteration 52/1000 | Loss: 0.00000992
Iteration 53/1000 | Loss: 0.00000992
Iteration 54/1000 | Loss: 0.00000992
Iteration 55/1000 | Loss: 0.00000992
Iteration 56/1000 | Loss: 0.00000991
Iteration 57/1000 | Loss: 0.00000991
Iteration 58/1000 | Loss: 0.00000991
Iteration 59/1000 | Loss: 0.00000990
Iteration 60/1000 | Loss: 0.00000988
Iteration 61/1000 | Loss: 0.00000988
Iteration 62/1000 | Loss: 0.00000988
Iteration 63/1000 | Loss: 0.00000988
Iteration 64/1000 | Loss: 0.00000987
Iteration 65/1000 | Loss: 0.00000987
Iteration 66/1000 | Loss: 0.00000987
Iteration 67/1000 | Loss: 0.00000987
Iteration 68/1000 | Loss: 0.00000986
Iteration 69/1000 | Loss: 0.00000986
Iteration 70/1000 | Loss: 0.00000985
Iteration 71/1000 | Loss: 0.00000981
Iteration 72/1000 | Loss: 0.00000981
Iteration 73/1000 | Loss: 0.00000980
Iteration 74/1000 | Loss: 0.00000980
Iteration 75/1000 | Loss: 0.00000980
Iteration 76/1000 | Loss: 0.00000980
Iteration 77/1000 | Loss: 0.00000977
Iteration 78/1000 | Loss: 0.00000976
Iteration 79/1000 | Loss: 0.00000976
Iteration 80/1000 | Loss: 0.00000975
Iteration 81/1000 | Loss: 0.00000974
Iteration 82/1000 | Loss: 0.00000974
Iteration 83/1000 | Loss: 0.00000974
Iteration 84/1000 | Loss: 0.00000974
Iteration 85/1000 | Loss: 0.00000974
Iteration 86/1000 | Loss: 0.00000974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [9.744640919961967e-06, 9.744640919961967e-06, 9.744640919961967e-06, 9.744640919961967e-06, 9.744640919961967e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.744640919961967e-06

Optimization complete. Final v2v error: 2.7290215492248535 mm

Highest mean error: 3.084843397140503 mm for frame 215

Lowest mean error: 2.521754503250122 mm for frame 236

Saving results

Total time: 40.677398443222046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591364
Iteration 2/25 | Loss: 0.00117352
Iteration 3/25 | Loss: 0.00108384
Iteration 4/25 | Loss: 0.00106939
Iteration 5/25 | Loss: 0.00106443
Iteration 6/25 | Loss: 0.00106337
Iteration 7/25 | Loss: 0.00106337
Iteration 8/25 | Loss: 0.00106337
Iteration 9/25 | Loss: 0.00106337
Iteration 10/25 | Loss: 0.00106337
Iteration 11/25 | Loss: 0.00106337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001063368283212185, 0.001063368283212185, 0.001063368283212185, 0.001063368283212185, 0.001063368283212185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001063368283212185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.90742350
Iteration 2/25 | Loss: 0.00081208
Iteration 3/25 | Loss: 0.00081208
Iteration 4/25 | Loss: 0.00081208
Iteration 5/25 | Loss: 0.00081207
Iteration 6/25 | Loss: 0.00081207
Iteration 7/25 | Loss: 0.00081207
Iteration 8/25 | Loss: 0.00081207
Iteration 9/25 | Loss: 0.00081207
Iteration 10/25 | Loss: 0.00081207
Iteration 11/25 | Loss: 0.00081207
Iteration 12/25 | Loss: 0.00081207
Iteration 13/25 | Loss: 0.00081207
Iteration 14/25 | Loss: 0.00081207
Iteration 15/25 | Loss: 0.00081207
Iteration 16/25 | Loss: 0.00081207
Iteration 17/25 | Loss: 0.00081207
Iteration 18/25 | Loss: 0.00081207
Iteration 19/25 | Loss: 0.00081207
Iteration 20/25 | Loss: 0.00081207
Iteration 21/25 | Loss: 0.00081207
Iteration 22/25 | Loss: 0.00081207
Iteration 23/25 | Loss: 0.00081207
Iteration 24/25 | Loss: 0.00081207
Iteration 25/25 | Loss: 0.00081207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081207
Iteration 2/1000 | Loss: 0.00002050
Iteration 3/1000 | Loss: 0.00001400
Iteration 4/1000 | Loss: 0.00001207
Iteration 5/1000 | Loss: 0.00001152
Iteration 6/1000 | Loss: 0.00001108
Iteration 7/1000 | Loss: 0.00001073
Iteration 8/1000 | Loss: 0.00001054
Iteration 9/1000 | Loss: 0.00001025
Iteration 10/1000 | Loss: 0.00001005
Iteration 11/1000 | Loss: 0.00000995
Iteration 12/1000 | Loss: 0.00000993
Iteration 13/1000 | Loss: 0.00000980
Iteration 14/1000 | Loss: 0.00000975
Iteration 15/1000 | Loss: 0.00000974
Iteration 16/1000 | Loss: 0.00000972
Iteration 17/1000 | Loss: 0.00000966
Iteration 18/1000 | Loss: 0.00000962
Iteration 19/1000 | Loss: 0.00000961
Iteration 20/1000 | Loss: 0.00000961
Iteration 21/1000 | Loss: 0.00000960
Iteration 22/1000 | Loss: 0.00000960
Iteration 23/1000 | Loss: 0.00000959
Iteration 24/1000 | Loss: 0.00000958
Iteration 25/1000 | Loss: 0.00000958
Iteration 26/1000 | Loss: 0.00000957
Iteration 27/1000 | Loss: 0.00000956
Iteration 28/1000 | Loss: 0.00000956
Iteration 29/1000 | Loss: 0.00000955
Iteration 30/1000 | Loss: 0.00000955
Iteration 31/1000 | Loss: 0.00000954
Iteration 32/1000 | Loss: 0.00000953
Iteration 33/1000 | Loss: 0.00000952
Iteration 34/1000 | Loss: 0.00000952
Iteration 35/1000 | Loss: 0.00000951
Iteration 36/1000 | Loss: 0.00000951
Iteration 37/1000 | Loss: 0.00000950
Iteration 38/1000 | Loss: 0.00000950
Iteration 39/1000 | Loss: 0.00000949
Iteration 40/1000 | Loss: 0.00000949
Iteration 41/1000 | Loss: 0.00000949
Iteration 42/1000 | Loss: 0.00000948
Iteration 43/1000 | Loss: 0.00000948
Iteration 44/1000 | Loss: 0.00000947
Iteration 45/1000 | Loss: 0.00000947
Iteration 46/1000 | Loss: 0.00000947
Iteration 47/1000 | Loss: 0.00000947
Iteration 48/1000 | Loss: 0.00000946
Iteration 49/1000 | Loss: 0.00000946
Iteration 50/1000 | Loss: 0.00000946
Iteration 51/1000 | Loss: 0.00000946
Iteration 52/1000 | Loss: 0.00000946
Iteration 53/1000 | Loss: 0.00000946
Iteration 54/1000 | Loss: 0.00000946
Iteration 55/1000 | Loss: 0.00000946
Iteration 56/1000 | Loss: 0.00000946
Iteration 57/1000 | Loss: 0.00000946
Iteration 58/1000 | Loss: 0.00000946
Iteration 59/1000 | Loss: 0.00000945
Iteration 60/1000 | Loss: 0.00000943
Iteration 61/1000 | Loss: 0.00000942
Iteration 62/1000 | Loss: 0.00000942
Iteration 63/1000 | Loss: 0.00000941
Iteration 64/1000 | Loss: 0.00000941
Iteration 65/1000 | Loss: 0.00000940
Iteration 66/1000 | Loss: 0.00000939
Iteration 67/1000 | Loss: 0.00000937
Iteration 68/1000 | Loss: 0.00000937
Iteration 69/1000 | Loss: 0.00000937
Iteration 70/1000 | Loss: 0.00000937
Iteration 71/1000 | Loss: 0.00000937
Iteration 72/1000 | Loss: 0.00000937
Iteration 73/1000 | Loss: 0.00000937
Iteration 74/1000 | Loss: 0.00000936
Iteration 75/1000 | Loss: 0.00000936
Iteration 76/1000 | Loss: 0.00000935
Iteration 77/1000 | Loss: 0.00000935
Iteration 78/1000 | Loss: 0.00000935
Iteration 79/1000 | Loss: 0.00000934
Iteration 80/1000 | Loss: 0.00000934
Iteration 81/1000 | Loss: 0.00000933
Iteration 82/1000 | Loss: 0.00000933
Iteration 83/1000 | Loss: 0.00000933
Iteration 84/1000 | Loss: 0.00000933
Iteration 85/1000 | Loss: 0.00000932
Iteration 86/1000 | Loss: 0.00000932
Iteration 87/1000 | Loss: 0.00000932
Iteration 88/1000 | Loss: 0.00000932
Iteration 89/1000 | Loss: 0.00000932
Iteration 90/1000 | Loss: 0.00000932
Iteration 91/1000 | Loss: 0.00000932
Iteration 92/1000 | Loss: 0.00000932
Iteration 93/1000 | Loss: 0.00000932
Iteration 94/1000 | Loss: 0.00000931
Iteration 95/1000 | Loss: 0.00000931
Iteration 96/1000 | Loss: 0.00000931
Iteration 97/1000 | Loss: 0.00000930
Iteration 98/1000 | Loss: 0.00000930
Iteration 99/1000 | Loss: 0.00000930
Iteration 100/1000 | Loss: 0.00000930
Iteration 101/1000 | Loss: 0.00000930
Iteration 102/1000 | Loss: 0.00000930
Iteration 103/1000 | Loss: 0.00000929
Iteration 104/1000 | Loss: 0.00000929
Iteration 105/1000 | Loss: 0.00000929
Iteration 106/1000 | Loss: 0.00000929
Iteration 107/1000 | Loss: 0.00000929
Iteration 108/1000 | Loss: 0.00000929
Iteration 109/1000 | Loss: 0.00000929
Iteration 110/1000 | Loss: 0.00000929
Iteration 111/1000 | Loss: 0.00000929
Iteration 112/1000 | Loss: 0.00000929
Iteration 113/1000 | Loss: 0.00000929
Iteration 114/1000 | Loss: 0.00000929
Iteration 115/1000 | Loss: 0.00000928
Iteration 116/1000 | Loss: 0.00000928
Iteration 117/1000 | Loss: 0.00000928
Iteration 118/1000 | Loss: 0.00000927
Iteration 119/1000 | Loss: 0.00000927
Iteration 120/1000 | Loss: 0.00000927
Iteration 121/1000 | Loss: 0.00000927
Iteration 122/1000 | Loss: 0.00000927
Iteration 123/1000 | Loss: 0.00000926
Iteration 124/1000 | Loss: 0.00000926
Iteration 125/1000 | Loss: 0.00000926
Iteration 126/1000 | Loss: 0.00000926
Iteration 127/1000 | Loss: 0.00000926
Iteration 128/1000 | Loss: 0.00000926
Iteration 129/1000 | Loss: 0.00000926
Iteration 130/1000 | Loss: 0.00000926
Iteration 131/1000 | Loss: 0.00000925
Iteration 132/1000 | Loss: 0.00000925
Iteration 133/1000 | Loss: 0.00000925
Iteration 134/1000 | Loss: 0.00000924
Iteration 135/1000 | Loss: 0.00000924
Iteration 136/1000 | Loss: 0.00000924
Iteration 137/1000 | Loss: 0.00000924
Iteration 138/1000 | Loss: 0.00000924
Iteration 139/1000 | Loss: 0.00000923
Iteration 140/1000 | Loss: 0.00000923
Iteration 141/1000 | Loss: 0.00000923
Iteration 142/1000 | Loss: 0.00000923
Iteration 143/1000 | Loss: 0.00000923
Iteration 144/1000 | Loss: 0.00000922
Iteration 145/1000 | Loss: 0.00000922
Iteration 146/1000 | Loss: 0.00000922
Iteration 147/1000 | Loss: 0.00000922
Iteration 148/1000 | Loss: 0.00000922
Iteration 149/1000 | Loss: 0.00000922
Iteration 150/1000 | Loss: 0.00000922
Iteration 151/1000 | Loss: 0.00000922
Iteration 152/1000 | Loss: 0.00000922
Iteration 153/1000 | Loss: 0.00000922
Iteration 154/1000 | Loss: 0.00000922
Iteration 155/1000 | Loss: 0.00000921
Iteration 156/1000 | Loss: 0.00000921
Iteration 157/1000 | Loss: 0.00000921
Iteration 158/1000 | Loss: 0.00000921
Iteration 159/1000 | Loss: 0.00000921
Iteration 160/1000 | Loss: 0.00000921
Iteration 161/1000 | Loss: 0.00000921
Iteration 162/1000 | Loss: 0.00000921
Iteration 163/1000 | Loss: 0.00000921
Iteration 164/1000 | Loss: 0.00000921
Iteration 165/1000 | Loss: 0.00000921
Iteration 166/1000 | Loss: 0.00000921
Iteration 167/1000 | Loss: 0.00000921
Iteration 168/1000 | Loss: 0.00000921
Iteration 169/1000 | Loss: 0.00000921
Iteration 170/1000 | Loss: 0.00000920
Iteration 171/1000 | Loss: 0.00000920
Iteration 172/1000 | Loss: 0.00000920
Iteration 173/1000 | Loss: 0.00000920
Iteration 174/1000 | Loss: 0.00000920
Iteration 175/1000 | Loss: 0.00000920
Iteration 176/1000 | Loss: 0.00000920
Iteration 177/1000 | Loss: 0.00000920
Iteration 178/1000 | Loss: 0.00000920
Iteration 179/1000 | Loss: 0.00000920
Iteration 180/1000 | Loss: 0.00000920
Iteration 181/1000 | Loss: 0.00000920
Iteration 182/1000 | Loss: 0.00000920
Iteration 183/1000 | Loss: 0.00000920
Iteration 184/1000 | Loss: 0.00000920
Iteration 185/1000 | Loss: 0.00000920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [9.197531653626356e-06, 9.197531653626356e-06, 9.197531653626356e-06, 9.197531653626356e-06, 9.197531653626356e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.197531653626356e-06

Optimization complete. Final v2v error: 2.616471529006958 mm

Highest mean error: 2.8528332710266113 mm for frame 98

Lowest mean error: 2.47690486907959 mm for frame 162

Saving results

Total time: 40.24467420578003
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033190
Iteration 2/25 | Loss: 0.00171388
Iteration 3/25 | Loss: 0.00131970
Iteration 4/25 | Loss: 0.00128648
Iteration 5/25 | Loss: 0.00123204
Iteration 6/25 | Loss: 0.00122460
Iteration 7/25 | Loss: 0.00123615
Iteration 8/25 | Loss: 0.00116392
Iteration 9/25 | Loss: 0.00112623
Iteration 10/25 | Loss: 0.00110283
Iteration 11/25 | Loss: 0.00114642
Iteration 12/25 | Loss: 0.00110004
Iteration 13/25 | Loss: 0.00110184
Iteration 14/25 | Loss: 0.00109627
Iteration 15/25 | Loss: 0.00109130
Iteration 16/25 | Loss: 0.00108732
Iteration 17/25 | Loss: 0.00108553
Iteration 18/25 | Loss: 0.00108240
Iteration 19/25 | Loss: 0.00107985
Iteration 20/25 | Loss: 0.00107891
Iteration 21/25 | Loss: 0.00107846
Iteration 22/25 | Loss: 0.00107827
Iteration 23/25 | Loss: 0.00107809
Iteration 24/25 | Loss: 0.00107803
Iteration 25/25 | Loss: 0.00107797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52903426
Iteration 2/25 | Loss: 0.00090381
Iteration 3/25 | Loss: 0.00090381
Iteration 4/25 | Loss: 0.00090381
Iteration 5/25 | Loss: 0.00090381
Iteration 6/25 | Loss: 0.00090381
Iteration 7/25 | Loss: 0.00090381
Iteration 8/25 | Loss: 0.00090381
Iteration 9/25 | Loss: 0.00090381
Iteration 10/25 | Loss: 0.00090381
Iteration 11/25 | Loss: 0.00090381
Iteration 12/25 | Loss: 0.00090381
Iteration 13/25 | Loss: 0.00090381
Iteration 14/25 | Loss: 0.00090381
Iteration 15/25 | Loss: 0.00090381
Iteration 16/25 | Loss: 0.00090381
Iteration 17/25 | Loss: 0.00090381
Iteration 18/25 | Loss: 0.00090381
Iteration 19/25 | Loss: 0.00090381
Iteration 20/25 | Loss: 0.00090381
Iteration 21/25 | Loss: 0.00090381
Iteration 22/25 | Loss: 0.00090381
Iteration 23/25 | Loss: 0.00090381
Iteration 24/25 | Loss: 0.00090381
Iteration 25/25 | Loss: 0.00090381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090380
Iteration 2/1000 | Loss: 0.00002318
Iteration 3/1000 | Loss: 0.00001595
Iteration 4/1000 | Loss: 0.00001302
Iteration 5/1000 | Loss: 0.00001216
Iteration 6/1000 | Loss: 0.00030853
Iteration 7/1000 | Loss: 0.00057003
Iteration 8/1000 | Loss: 0.00003971
Iteration 9/1000 | Loss: 0.00002474
Iteration 10/1000 | Loss: 0.00046802
Iteration 11/1000 | Loss: 0.00001775
Iteration 12/1000 | Loss: 0.00001234
Iteration 13/1000 | Loss: 0.00005737
Iteration 14/1000 | Loss: 0.00001113
Iteration 15/1000 | Loss: 0.00001085
Iteration 16/1000 | Loss: 0.00001079
Iteration 17/1000 | Loss: 0.00001078
Iteration 18/1000 | Loss: 0.00001069
Iteration 19/1000 | Loss: 0.00001056
Iteration 20/1000 | Loss: 0.00001054
Iteration 21/1000 | Loss: 0.00001043
Iteration 22/1000 | Loss: 0.00001035
Iteration 23/1000 | Loss: 0.00001032
Iteration 24/1000 | Loss: 0.00001027
Iteration 25/1000 | Loss: 0.00001026
Iteration 26/1000 | Loss: 0.00001026
Iteration 27/1000 | Loss: 0.00001025
Iteration 28/1000 | Loss: 0.00001025
Iteration 29/1000 | Loss: 0.00001024
Iteration 30/1000 | Loss: 0.00001024
Iteration 31/1000 | Loss: 0.00001024
Iteration 32/1000 | Loss: 0.00001024
Iteration 33/1000 | Loss: 0.00001023
Iteration 34/1000 | Loss: 0.00001022
Iteration 35/1000 | Loss: 0.00001021
Iteration 36/1000 | Loss: 0.00001021
Iteration 37/1000 | Loss: 0.00001021
Iteration 38/1000 | Loss: 0.00001021
Iteration 39/1000 | Loss: 0.00001021
Iteration 40/1000 | Loss: 0.00001020
Iteration 41/1000 | Loss: 0.00001020
Iteration 42/1000 | Loss: 0.00001020
Iteration 43/1000 | Loss: 0.00001020
Iteration 44/1000 | Loss: 0.00001020
Iteration 45/1000 | Loss: 0.00001020
Iteration 46/1000 | Loss: 0.00001020
Iteration 47/1000 | Loss: 0.00001020
Iteration 48/1000 | Loss: 0.00001019
Iteration 49/1000 | Loss: 0.00001019
Iteration 50/1000 | Loss: 0.00001019
Iteration 51/1000 | Loss: 0.00001019
Iteration 52/1000 | Loss: 0.00001018
Iteration 53/1000 | Loss: 0.00001018
Iteration 54/1000 | Loss: 0.00001018
Iteration 55/1000 | Loss: 0.00001017
Iteration 56/1000 | Loss: 0.00001017
Iteration 57/1000 | Loss: 0.00001017
Iteration 58/1000 | Loss: 0.00001017
Iteration 59/1000 | Loss: 0.00001017
Iteration 60/1000 | Loss: 0.00001017
Iteration 61/1000 | Loss: 0.00001017
Iteration 62/1000 | Loss: 0.00001017
Iteration 63/1000 | Loss: 0.00001017
Iteration 64/1000 | Loss: 0.00001016
Iteration 65/1000 | Loss: 0.00001016
Iteration 66/1000 | Loss: 0.00001016
Iteration 67/1000 | Loss: 0.00001016
Iteration 68/1000 | Loss: 0.00001015
Iteration 69/1000 | Loss: 0.00001015
Iteration 70/1000 | Loss: 0.00001015
Iteration 71/1000 | Loss: 0.00001015
Iteration 72/1000 | Loss: 0.00001015
Iteration 73/1000 | Loss: 0.00001015
Iteration 74/1000 | Loss: 0.00001014
Iteration 75/1000 | Loss: 0.00001014
Iteration 76/1000 | Loss: 0.00001014
Iteration 77/1000 | Loss: 0.00001014
Iteration 78/1000 | Loss: 0.00001013
Iteration 79/1000 | Loss: 0.00001013
Iteration 80/1000 | Loss: 0.00001013
Iteration 81/1000 | Loss: 0.00001013
Iteration 82/1000 | Loss: 0.00001012
Iteration 83/1000 | Loss: 0.00001012
Iteration 84/1000 | Loss: 0.00001012
Iteration 85/1000 | Loss: 0.00001011
Iteration 86/1000 | Loss: 0.00001011
Iteration 87/1000 | Loss: 0.00001011
Iteration 88/1000 | Loss: 0.00001010
Iteration 89/1000 | Loss: 0.00001010
Iteration 90/1000 | Loss: 0.00001010
Iteration 91/1000 | Loss: 0.00001009
Iteration 92/1000 | Loss: 0.00001009
Iteration 93/1000 | Loss: 0.00001008
Iteration 94/1000 | Loss: 0.00001008
Iteration 95/1000 | Loss: 0.00001008
Iteration 96/1000 | Loss: 0.00001007
Iteration 97/1000 | Loss: 0.00001007
Iteration 98/1000 | Loss: 0.00001007
Iteration 99/1000 | Loss: 0.00001007
Iteration 100/1000 | Loss: 0.00001007
Iteration 101/1000 | Loss: 0.00001007
Iteration 102/1000 | Loss: 0.00001007
Iteration 103/1000 | Loss: 0.00001007
Iteration 104/1000 | Loss: 0.00001007
Iteration 105/1000 | Loss: 0.00001007
Iteration 106/1000 | Loss: 0.00001007
Iteration 107/1000 | Loss: 0.00001006
Iteration 108/1000 | Loss: 0.00001006
Iteration 109/1000 | Loss: 0.00001006
Iteration 110/1000 | Loss: 0.00001006
Iteration 111/1000 | Loss: 0.00001006
Iteration 112/1000 | Loss: 0.00001006
Iteration 113/1000 | Loss: 0.00001006
Iteration 114/1000 | Loss: 0.00001006
Iteration 115/1000 | Loss: 0.00001006
Iteration 116/1000 | Loss: 0.00001006
Iteration 117/1000 | Loss: 0.00001006
Iteration 118/1000 | Loss: 0.00001006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.0064443813462276e-05, 1.0064443813462276e-05, 1.0064443813462276e-05, 1.0064443813462276e-05, 1.0064443813462276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0064443813462276e-05

Optimization complete. Final v2v error: 2.6942999362945557 mm

Highest mean error: 3.3335585594177246 mm for frame 60

Lowest mean error: 2.446354866027832 mm for frame 134

Saving results

Total time: 76.77922773361206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477252
Iteration 2/25 | Loss: 0.00126011
Iteration 3/25 | Loss: 0.00113451
Iteration 4/25 | Loss: 0.00110885
Iteration 5/25 | Loss: 0.00110039
Iteration 6/25 | Loss: 0.00109924
Iteration 7/25 | Loss: 0.00109924
Iteration 8/25 | Loss: 0.00109924
Iteration 9/25 | Loss: 0.00109924
Iteration 10/25 | Loss: 0.00109924
Iteration 11/25 | Loss: 0.00109924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001099238870665431, 0.001099238870665431, 0.001099238870665431, 0.001099238870665431, 0.001099238870665431]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001099238870665431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30796802
Iteration 2/25 | Loss: 0.00092490
Iteration 3/25 | Loss: 0.00092490
Iteration 4/25 | Loss: 0.00092490
Iteration 5/25 | Loss: 0.00092490
Iteration 6/25 | Loss: 0.00092490
Iteration 7/25 | Loss: 0.00092490
Iteration 8/25 | Loss: 0.00092490
Iteration 9/25 | Loss: 0.00092490
Iteration 10/25 | Loss: 0.00092490
Iteration 11/25 | Loss: 0.00092490
Iteration 12/25 | Loss: 0.00092490
Iteration 13/25 | Loss: 0.00092490
Iteration 14/25 | Loss: 0.00092490
Iteration 15/25 | Loss: 0.00092490
Iteration 16/25 | Loss: 0.00092490
Iteration 17/25 | Loss: 0.00092490
Iteration 18/25 | Loss: 0.00092490
Iteration 19/25 | Loss: 0.00092490
Iteration 20/25 | Loss: 0.00092490
Iteration 21/25 | Loss: 0.00092490
Iteration 22/25 | Loss: 0.00092490
Iteration 23/25 | Loss: 0.00092490
Iteration 24/25 | Loss: 0.00092490
Iteration 25/25 | Loss: 0.00092490

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092490
Iteration 2/1000 | Loss: 0.00005381
Iteration 3/1000 | Loss: 0.00003460
Iteration 4/1000 | Loss: 0.00002908
Iteration 5/1000 | Loss: 0.00002756
Iteration 6/1000 | Loss: 0.00002683
Iteration 7/1000 | Loss: 0.00002615
Iteration 8/1000 | Loss: 0.00002575
Iteration 9/1000 | Loss: 0.00002557
Iteration 10/1000 | Loss: 0.00002556
Iteration 11/1000 | Loss: 0.00002555
Iteration 12/1000 | Loss: 0.00002545
Iteration 13/1000 | Loss: 0.00002533
Iteration 14/1000 | Loss: 0.00002518
Iteration 15/1000 | Loss: 0.00002509
Iteration 16/1000 | Loss: 0.00002508
Iteration 17/1000 | Loss: 0.00002508
Iteration 18/1000 | Loss: 0.00002507
Iteration 19/1000 | Loss: 0.00002498
Iteration 20/1000 | Loss: 0.00002487
Iteration 21/1000 | Loss: 0.00002487
Iteration 22/1000 | Loss: 0.00002487
Iteration 23/1000 | Loss: 0.00002487
Iteration 24/1000 | Loss: 0.00002487
Iteration 25/1000 | Loss: 0.00002487
Iteration 26/1000 | Loss: 0.00002487
Iteration 27/1000 | Loss: 0.00002487
Iteration 28/1000 | Loss: 0.00002487
Iteration 29/1000 | Loss: 0.00002487
Iteration 30/1000 | Loss: 0.00002486
Iteration 31/1000 | Loss: 0.00002486
Iteration 32/1000 | Loss: 0.00002485
Iteration 33/1000 | Loss: 0.00002477
Iteration 34/1000 | Loss: 0.00002473
Iteration 35/1000 | Loss: 0.00002470
Iteration 36/1000 | Loss: 0.00002468
Iteration 37/1000 | Loss: 0.00002463
Iteration 38/1000 | Loss: 0.00002460
Iteration 39/1000 | Loss: 0.00002459
Iteration 40/1000 | Loss: 0.00002457
Iteration 41/1000 | Loss: 0.00002457
Iteration 42/1000 | Loss: 0.00002457
Iteration 43/1000 | Loss: 0.00002457
Iteration 44/1000 | Loss: 0.00002456
Iteration 45/1000 | Loss: 0.00002456
Iteration 46/1000 | Loss: 0.00002456
Iteration 47/1000 | Loss: 0.00002456
Iteration 48/1000 | Loss: 0.00002456
Iteration 49/1000 | Loss: 0.00002455
Iteration 50/1000 | Loss: 0.00002455
Iteration 51/1000 | Loss: 0.00002454
Iteration 52/1000 | Loss: 0.00002454
Iteration 53/1000 | Loss: 0.00002454
Iteration 54/1000 | Loss: 0.00002454
Iteration 55/1000 | Loss: 0.00002453
Iteration 56/1000 | Loss: 0.00002453
Iteration 57/1000 | Loss: 0.00002453
Iteration 58/1000 | Loss: 0.00002452
Iteration 59/1000 | Loss: 0.00002452
Iteration 60/1000 | Loss: 0.00002452
Iteration 61/1000 | Loss: 0.00002452
Iteration 62/1000 | Loss: 0.00002452
Iteration 63/1000 | Loss: 0.00002452
Iteration 64/1000 | Loss: 0.00002451
Iteration 65/1000 | Loss: 0.00002451
Iteration 66/1000 | Loss: 0.00002451
Iteration 67/1000 | Loss: 0.00002451
Iteration 68/1000 | Loss: 0.00002451
Iteration 69/1000 | Loss: 0.00002450
Iteration 70/1000 | Loss: 0.00002450
Iteration 71/1000 | Loss: 0.00002449
Iteration 72/1000 | Loss: 0.00002449
Iteration 73/1000 | Loss: 0.00002449
Iteration 74/1000 | Loss: 0.00002449
Iteration 75/1000 | Loss: 0.00002449
Iteration 76/1000 | Loss: 0.00002448
Iteration 77/1000 | Loss: 0.00002448
Iteration 78/1000 | Loss: 0.00002448
Iteration 79/1000 | Loss: 0.00002448
Iteration 80/1000 | Loss: 0.00002447
Iteration 81/1000 | Loss: 0.00002447
Iteration 82/1000 | Loss: 0.00002447
Iteration 83/1000 | Loss: 0.00002447
Iteration 84/1000 | Loss: 0.00002446
Iteration 85/1000 | Loss: 0.00002446
Iteration 86/1000 | Loss: 0.00002445
Iteration 87/1000 | Loss: 0.00002445
Iteration 88/1000 | Loss: 0.00002445
Iteration 89/1000 | Loss: 0.00002445
Iteration 90/1000 | Loss: 0.00002444
Iteration 91/1000 | Loss: 0.00002444
Iteration 92/1000 | Loss: 0.00002444
Iteration 93/1000 | Loss: 0.00002444
Iteration 94/1000 | Loss: 0.00002444
Iteration 95/1000 | Loss: 0.00002444
Iteration 96/1000 | Loss: 0.00002444
Iteration 97/1000 | Loss: 0.00002444
Iteration 98/1000 | Loss: 0.00002444
Iteration 99/1000 | Loss: 0.00002444
Iteration 100/1000 | Loss: 0.00002444
Iteration 101/1000 | Loss: 0.00002444
Iteration 102/1000 | Loss: 0.00002444
Iteration 103/1000 | Loss: 0.00002443
Iteration 104/1000 | Loss: 0.00002443
Iteration 105/1000 | Loss: 0.00002443
Iteration 106/1000 | Loss: 0.00002443
Iteration 107/1000 | Loss: 0.00002443
Iteration 108/1000 | Loss: 0.00002443
Iteration 109/1000 | Loss: 0.00002443
Iteration 110/1000 | Loss: 0.00002443
Iteration 111/1000 | Loss: 0.00002443
Iteration 112/1000 | Loss: 0.00002443
Iteration 113/1000 | Loss: 0.00002443
Iteration 114/1000 | Loss: 0.00002443
Iteration 115/1000 | Loss: 0.00002442
Iteration 116/1000 | Loss: 0.00002442
Iteration 117/1000 | Loss: 0.00002442
Iteration 118/1000 | Loss: 0.00002441
Iteration 119/1000 | Loss: 0.00002441
Iteration 120/1000 | Loss: 0.00002441
Iteration 121/1000 | Loss: 0.00002441
Iteration 122/1000 | Loss: 0.00002440
Iteration 123/1000 | Loss: 0.00002440
Iteration 124/1000 | Loss: 0.00002440
Iteration 125/1000 | Loss: 0.00002440
Iteration 126/1000 | Loss: 0.00002439
Iteration 127/1000 | Loss: 0.00002439
Iteration 128/1000 | Loss: 0.00002439
Iteration 129/1000 | Loss: 0.00002439
Iteration 130/1000 | Loss: 0.00002438
Iteration 131/1000 | Loss: 0.00002438
Iteration 132/1000 | Loss: 0.00002438
Iteration 133/1000 | Loss: 0.00002438
Iteration 134/1000 | Loss: 0.00002438
Iteration 135/1000 | Loss: 0.00002438
Iteration 136/1000 | Loss: 0.00002438
Iteration 137/1000 | Loss: 0.00002438
Iteration 138/1000 | Loss: 0.00002437
Iteration 139/1000 | Loss: 0.00002437
Iteration 140/1000 | Loss: 0.00002437
Iteration 141/1000 | Loss: 0.00002437
Iteration 142/1000 | Loss: 0.00002437
Iteration 143/1000 | Loss: 0.00002437
Iteration 144/1000 | Loss: 0.00002436
Iteration 145/1000 | Loss: 0.00002436
Iteration 146/1000 | Loss: 0.00002436
Iteration 147/1000 | Loss: 0.00002436
Iteration 148/1000 | Loss: 0.00002436
Iteration 149/1000 | Loss: 0.00002436
Iteration 150/1000 | Loss: 0.00002436
Iteration 151/1000 | Loss: 0.00002436
Iteration 152/1000 | Loss: 0.00002436
Iteration 153/1000 | Loss: 0.00002436
Iteration 154/1000 | Loss: 0.00002436
Iteration 155/1000 | Loss: 0.00002435
Iteration 156/1000 | Loss: 0.00002435
Iteration 157/1000 | Loss: 0.00002435
Iteration 158/1000 | Loss: 0.00002435
Iteration 159/1000 | Loss: 0.00002435
Iteration 160/1000 | Loss: 0.00002435
Iteration 161/1000 | Loss: 0.00002435
Iteration 162/1000 | Loss: 0.00002435
Iteration 163/1000 | Loss: 0.00002435
Iteration 164/1000 | Loss: 0.00002435
Iteration 165/1000 | Loss: 0.00002435
Iteration 166/1000 | Loss: 0.00002435
Iteration 167/1000 | Loss: 0.00002435
Iteration 168/1000 | Loss: 0.00002435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.435074384266045e-05, 2.435074384266045e-05, 2.435074384266045e-05, 2.435074384266045e-05, 2.435074384266045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.435074384266045e-05

Optimization complete. Final v2v error: 3.894906759262085 mm

Highest mean error: 4.234407424926758 mm for frame 38

Lowest mean error: 3.59295916557312 mm for frame 169

Saving results

Total time: 41.32560968399048
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976067
Iteration 2/25 | Loss: 0.00299729
Iteration 3/25 | Loss: 0.00207963
Iteration 4/25 | Loss: 0.00195808
Iteration 5/25 | Loss: 0.00180935
Iteration 6/25 | Loss: 0.00174838
Iteration 7/25 | Loss: 0.00165162
Iteration 8/25 | Loss: 0.00159235
Iteration 9/25 | Loss: 0.00157519
Iteration 10/25 | Loss: 0.00157611
Iteration 11/25 | Loss: 0.00155704
Iteration 12/25 | Loss: 0.00154946
Iteration 13/25 | Loss: 0.00148546
Iteration 14/25 | Loss: 0.00147668
Iteration 15/25 | Loss: 0.00145986
Iteration 16/25 | Loss: 0.00146529
Iteration 17/25 | Loss: 0.00146078
Iteration 18/25 | Loss: 0.00145663
Iteration 19/25 | Loss: 0.00145126
Iteration 20/25 | Loss: 0.00145488
Iteration 21/25 | Loss: 0.00145333
Iteration 22/25 | Loss: 0.00144967
Iteration 23/25 | Loss: 0.00144331
Iteration 24/25 | Loss: 0.00144178
Iteration 25/25 | Loss: 0.00144109

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33522868
Iteration 2/25 | Loss: 0.00403230
Iteration 3/25 | Loss: 0.00307655
Iteration 4/25 | Loss: 0.00307655
Iteration 5/25 | Loss: 0.00307655
Iteration 6/25 | Loss: 0.00307655
Iteration 7/25 | Loss: 0.00307655
Iteration 8/25 | Loss: 0.00307655
Iteration 9/25 | Loss: 0.00307655
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.003076547523960471, 0.003076547523960471, 0.003076547523960471, 0.003076547523960471, 0.003076547523960471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003076547523960471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00307655
Iteration 2/1000 | Loss: 0.00233026
Iteration 3/1000 | Loss: 0.00254922
Iteration 4/1000 | Loss: 0.00234927
Iteration 5/1000 | Loss: 0.00133415
Iteration 6/1000 | Loss: 0.00140709
Iteration 7/1000 | Loss: 0.00077454
Iteration 8/1000 | Loss: 0.00090256
Iteration 9/1000 | Loss: 0.00062585
Iteration 10/1000 | Loss: 0.00077080
Iteration 11/1000 | Loss: 0.00071988
Iteration 12/1000 | Loss: 0.00086079
Iteration 13/1000 | Loss: 0.00125548
Iteration 14/1000 | Loss: 0.00074283
Iteration 15/1000 | Loss: 0.00193333
Iteration 16/1000 | Loss: 0.00077264
Iteration 17/1000 | Loss: 0.00085996
Iteration 18/1000 | Loss: 0.00067972
Iteration 19/1000 | Loss: 0.00146790
Iteration 20/1000 | Loss: 0.00089521
Iteration 21/1000 | Loss: 0.00046399
Iteration 22/1000 | Loss: 0.00040420
Iteration 23/1000 | Loss: 0.00050190
Iteration 24/1000 | Loss: 0.00051315
Iteration 25/1000 | Loss: 0.00104641
Iteration 26/1000 | Loss: 0.00133323
Iteration 27/1000 | Loss: 0.00087798
Iteration 28/1000 | Loss: 0.00095180
Iteration 29/1000 | Loss: 0.00077615
Iteration 30/1000 | Loss: 0.00066960
Iteration 31/1000 | Loss: 0.00067464
Iteration 32/1000 | Loss: 0.00050337
Iteration 33/1000 | Loss: 0.00043401
Iteration 34/1000 | Loss: 0.00130059
Iteration 35/1000 | Loss: 0.00079869
Iteration 36/1000 | Loss: 0.00068935
Iteration 37/1000 | Loss: 0.00070813
Iteration 38/1000 | Loss: 0.00057820
Iteration 39/1000 | Loss: 0.00062078
Iteration 40/1000 | Loss: 0.00058278
Iteration 41/1000 | Loss: 0.00044963
Iteration 42/1000 | Loss: 0.00072644
Iteration 43/1000 | Loss: 0.00111416
Iteration 44/1000 | Loss: 0.00086463
Iteration 45/1000 | Loss: 0.00077393
Iteration 46/1000 | Loss: 0.00058608
Iteration 47/1000 | Loss: 0.00047913
Iteration 48/1000 | Loss: 0.00058746
Iteration 49/1000 | Loss: 0.00063675
Iteration 50/1000 | Loss: 0.00052133
Iteration 51/1000 | Loss: 0.00042859
Iteration 52/1000 | Loss: 0.00056925
Iteration 53/1000 | Loss: 0.00082329
Iteration 54/1000 | Loss: 0.00076438
Iteration 55/1000 | Loss: 0.00080378
Iteration 56/1000 | Loss: 0.00101200
Iteration 57/1000 | Loss: 0.00053658
Iteration 58/1000 | Loss: 0.00069227
Iteration 59/1000 | Loss: 0.00134629
Iteration 60/1000 | Loss: 0.00065547
Iteration 61/1000 | Loss: 0.00020366
Iteration 62/1000 | Loss: 0.00047853
Iteration 63/1000 | Loss: 0.00047655
Iteration 64/1000 | Loss: 0.00047986
Iteration 65/1000 | Loss: 0.00038720
Iteration 66/1000 | Loss: 0.00055418
Iteration 67/1000 | Loss: 0.00028234
Iteration 68/1000 | Loss: 0.00040369
Iteration 69/1000 | Loss: 0.00021700
Iteration 70/1000 | Loss: 0.00033540
Iteration 71/1000 | Loss: 0.00040361
Iteration 72/1000 | Loss: 0.00048878
Iteration 73/1000 | Loss: 0.00042826
Iteration 74/1000 | Loss: 0.00043778
Iteration 75/1000 | Loss: 0.00044486
Iteration 76/1000 | Loss: 0.00039398
Iteration 77/1000 | Loss: 0.00066296
Iteration 78/1000 | Loss: 0.00040879
Iteration 79/1000 | Loss: 0.00033761
Iteration 80/1000 | Loss: 0.00043338
Iteration 81/1000 | Loss: 0.00038376
Iteration 82/1000 | Loss: 0.00026119
Iteration 83/1000 | Loss: 0.00036449
Iteration 84/1000 | Loss: 0.00048760
Iteration 85/1000 | Loss: 0.00056805
Iteration 86/1000 | Loss: 0.00069265
Iteration 87/1000 | Loss: 0.00050765
Iteration 88/1000 | Loss: 0.00042603
Iteration 89/1000 | Loss: 0.00048193
Iteration 90/1000 | Loss: 0.00055848
Iteration 91/1000 | Loss: 0.00059795
Iteration 92/1000 | Loss: 0.00053011
Iteration 93/1000 | Loss: 0.00044949
Iteration 94/1000 | Loss: 0.00050663
Iteration 95/1000 | Loss: 0.00055927
Iteration 96/1000 | Loss: 0.00045545
Iteration 97/1000 | Loss: 0.00041690
Iteration 98/1000 | Loss: 0.00045491
Iteration 99/1000 | Loss: 0.00039203
Iteration 100/1000 | Loss: 0.00049039
Iteration 101/1000 | Loss: 0.00047825
Iteration 102/1000 | Loss: 0.00050044
Iteration 103/1000 | Loss: 0.00050407
Iteration 104/1000 | Loss: 0.00046867
Iteration 105/1000 | Loss: 0.00046645
Iteration 106/1000 | Loss: 0.00051832
Iteration 107/1000 | Loss: 0.00044056
Iteration 108/1000 | Loss: 0.00050253
Iteration 109/1000 | Loss: 0.00050349
Iteration 110/1000 | Loss: 0.00050208
Iteration 111/1000 | Loss: 0.00039515
Iteration 112/1000 | Loss: 0.00045681
Iteration 113/1000 | Loss: 0.00045087
Iteration 114/1000 | Loss: 0.00015245
Iteration 115/1000 | Loss: 0.00029966
Iteration 116/1000 | Loss: 0.00022737
Iteration 117/1000 | Loss: 0.00027496
Iteration 118/1000 | Loss: 0.00032488
Iteration 119/1000 | Loss: 0.00039416
Iteration 120/1000 | Loss: 0.00044638
Iteration 121/1000 | Loss: 0.00060544
Iteration 122/1000 | Loss: 0.00083724
Iteration 123/1000 | Loss: 0.00040628
Iteration 124/1000 | Loss: 0.00029433
Iteration 125/1000 | Loss: 0.00031981
Iteration 126/1000 | Loss: 0.00014281
Iteration 127/1000 | Loss: 0.00021795
Iteration 128/1000 | Loss: 0.00043236
Iteration 129/1000 | Loss: 0.00066491
Iteration 130/1000 | Loss: 0.00031612
Iteration 131/1000 | Loss: 0.00035740
Iteration 132/1000 | Loss: 0.00045213
Iteration 133/1000 | Loss: 0.00066251
Iteration 134/1000 | Loss: 0.00038030
Iteration 135/1000 | Loss: 0.00034146
Iteration 136/1000 | Loss: 0.00042863
Iteration 137/1000 | Loss: 0.00049587
Iteration 138/1000 | Loss: 0.00049669
Iteration 139/1000 | Loss: 0.00037378
Iteration 140/1000 | Loss: 0.00037393
Iteration 141/1000 | Loss: 0.00033986
Iteration 142/1000 | Loss: 0.00028314
Iteration 143/1000 | Loss: 0.00033510
Iteration 144/1000 | Loss: 0.00027781
Iteration 145/1000 | Loss: 0.00028519
Iteration 146/1000 | Loss: 0.00047068
Iteration 147/1000 | Loss: 0.00049810
Iteration 148/1000 | Loss: 0.00043562
Iteration 149/1000 | Loss: 0.00059014
Iteration 150/1000 | Loss: 0.00041619
Iteration 151/1000 | Loss: 0.00042843
Iteration 152/1000 | Loss: 0.00045007
Iteration 153/1000 | Loss: 0.00018354
Iteration 154/1000 | Loss: 0.00037384
Iteration 155/1000 | Loss: 0.00022178
Iteration 156/1000 | Loss: 0.00021076
Iteration 157/1000 | Loss: 0.00037496
Iteration 158/1000 | Loss: 0.00026503
Iteration 159/1000 | Loss: 0.00019657
Iteration 160/1000 | Loss: 0.00015086
Iteration 161/1000 | Loss: 0.00032304
Iteration 162/1000 | Loss: 0.00021007
Iteration 163/1000 | Loss: 0.00027245
Iteration 164/1000 | Loss: 0.00047700
Iteration 165/1000 | Loss: 0.00034318
Iteration 166/1000 | Loss: 0.00033762
Iteration 167/1000 | Loss: 0.00026479
Iteration 168/1000 | Loss: 0.00020538
Iteration 169/1000 | Loss: 0.00017676
Iteration 170/1000 | Loss: 0.00012947
Iteration 171/1000 | Loss: 0.00022807
Iteration 172/1000 | Loss: 0.00024622
Iteration 173/1000 | Loss: 0.00026097
Iteration 174/1000 | Loss: 0.00031800
Iteration 175/1000 | Loss: 0.00020993
Iteration 176/1000 | Loss: 0.00018616
Iteration 177/1000 | Loss: 0.00038953
Iteration 178/1000 | Loss: 0.00016489
Iteration 179/1000 | Loss: 0.00019642
Iteration 180/1000 | Loss: 0.00014693
Iteration 181/1000 | Loss: 0.00009301
Iteration 182/1000 | Loss: 0.00025377
Iteration 183/1000 | Loss: 0.00026757
Iteration 184/1000 | Loss: 0.00027501
Iteration 185/1000 | Loss: 0.00019373
Iteration 186/1000 | Loss: 0.00025112
Iteration 187/1000 | Loss: 0.00015803
Iteration 188/1000 | Loss: 0.00017548
Iteration 189/1000 | Loss: 0.00075420
Iteration 190/1000 | Loss: 0.00050438
Iteration 191/1000 | Loss: 0.00061271
Iteration 192/1000 | Loss: 0.00026905
Iteration 193/1000 | Loss: 0.00035699
Iteration 194/1000 | Loss: 0.00037710
Iteration 195/1000 | Loss: 0.00017963
Iteration 196/1000 | Loss: 0.00020204
Iteration 197/1000 | Loss: 0.00021165
Iteration 198/1000 | Loss: 0.00024925
Iteration 199/1000 | Loss: 0.00018248
Iteration 200/1000 | Loss: 0.00024861
Iteration 201/1000 | Loss: 0.00034233
Iteration 202/1000 | Loss: 0.00029461
Iteration 203/1000 | Loss: 0.00056877
Iteration 204/1000 | Loss: 0.00088892
Iteration 205/1000 | Loss: 0.00037311
Iteration 206/1000 | Loss: 0.00043181
Iteration 207/1000 | Loss: 0.00055397
Iteration 208/1000 | Loss: 0.00034597
Iteration 209/1000 | Loss: 0.00035762
Iteration 210/1000 | Loss: 0.00037243
Iteration 211/1000 | Loss: 0.00046829
Iteration 212/1000 | Loss: 0.00037899
Iteration 213/1000 | Loss: 0.00035695
Iteration 214/1000 | Loss: 0.00032672
Iteration 215/1000 | Loss: 0.00040041
Iteration 216/1000 | Loss: 0.00027296
Iteration 217/1000 | Loss: 0.00026176
Iteration 218/1000 | Loss: 0.00031639
Iteration 219/1000 | Loss: 0.00035771
Iteration 220/1000 | Loss: 0.00031565
Iteration 221/1000 | Loss: 0.00039346
Iteration 222/1000 | Loss: 0.00034161
Iteration 223/1000 | Loss: 0.00025405
Iteration 224/1000 | Loss: 0.00012750
Iteration 225/1000 | Loss: 0.00043308
Iteration 226/1000 | Loss: 0.00045001
Iteration 227/1000 | Loss: 0.00041364
Iteration 228/1000 | Loss: 0.00015651
Iteration 229/1000 | Loss: 0.00014800
Iteration 230/1000 | Loss: 0.00019885
Iteration 231/1000 | Loss: 0.00009704
Iteration 232/1000 | Loss: 0.00046512
Iteration 233/1000 | Loss: 0.00040199
Iteration 234/1000 | Loss: 0.00030993
Iteration 235/1000 | Loss: 0.00033597
Iteration 236/1000 | Loss: 0.00093317
Iteration 237/1000 | Loss: 0.00040504
Iteration 238/1000 | Loss: 0.00035111
Iteration 239/1000 | Loss: 0.00039541
Iteration 240/1000 | Loss: 0.00021891
Iteration 241/1000 | Loss: 0.00187622
Iteration 242/1000 | Loss: 0.00035810
Iteration 243/1000 | Loss: 0.00034655
Iteration 244/1000 | Loss: 0.00040516
Iteration 245/1000 | Loss: 0.00103306
Iteration 246/1000 | Loss: 0.00038961
Iteration 247/1000 | Loss: 0.00036872
Iteration 248/1000 | Loss: 0.00029355
Iteration 249/1000 | Loss: 0.00033815
Iteration 250/1000 | Loss: 0.00055064
Iteration 251/1000 | Loss: 0.00027078
Iteration 252/1000 | Loss: 0.00051891
Iteration 253/1000 | Loss: 0.00027261
Iteration 254/1000 | Loss: 0.00026434
Iteration 255/1000 | Loss: 0.00027562
Iteration 256/1000 | Loss: 0.00052935
Iteration 257/1000 | Loss: 0.00021979
Iteration 258/1000 | Loss: 0.00041496
Iteration 259/1000 | Loss: 0.00013370
Iteration 260/1000 | Loss: 0.00023077
Iteration 261/1000 | Loss: 0.00038717
Iteration 262/1000 | Loss: 0.00019258
Iteration 263/1000 | Loss: 0.00029865
Iteration 264/1000 | Loss: 0.00022834
Iteration 265/1000 | Loss: 0.00013402
Iteration 266/1000 | Loss: 0.00010664
Iteration 267/1000 | Loss: 0.00007320
Iteration 268/1000 | Loss: 0.00017426
Iteration 269/1000 | Loss: 0.00028060
Iteration 270/1000 | Loss: 0.00015196
Iteration 271/1000 | Loss: 0.00018893
Iteration 272/1000 | Loss: 0.00009972
Iteration 273/1000 | Loss: 0.00028506
Iteration 274/1000 | Loss: 0.00014858
Iteration 275/1000 | Loss: 0.00011205
Iteration 276/1000 | Loss: 0.00011734
Iteration 277/1000 | Loss: 0.00028823
Iteration 278/1000 | Loss: 0.00034918
Iteration 279/1000 | Loss: 0.00020252
Iteration 280/1000 | Loss: 0.00009052
Iteration 281/1000 | Loss: 0.00033830
Iteration 282/1000 | Loss: 0.00026680
Iteration 283/1000 | Loss: 0.00027651
Iteration 284/1000 | Loss: 0.00011369
Iteration 285/1000 | Loss: 0.00027563
Iteration 286/1000 | Loss: 0.00036146
Iteration 287/1000 | Loss: 0.00027931
Iteration 288/1000 | Loss: 0.00007026
Iteration 289/1000 | Loss: 0.00047246
Iteration 290/1000 | Loss: 0.00022541
Iteration 291/1000 | Loss: 0.00027265
Iteration 292/1000 | Loss: 0.00108341
Iteration 293/1000 | Loss: 0.00091456
Iteration 294/1000 | Loss: 0.00031761
Iteration 295/1000 | Loss: 0.00012024
Iteration 296/1000 | Loss: 0.00015675
Iteration 297/1000 | Loss: 0.00019633
Iteration 298/1000 | Loss: 0.00021858
Iteration 299/1000 | Loss: 0.00083970
Iteration 300/1000 | Loss: 0.00052567
Iteration 301/1000 | Loss: 0.00022334
Iteration 302/1000 | Loss: 0.00086103
Iteration 303/1000 | Loss: 0.00042125
Iteration 304/1000 | Loss: 0.00016293
Iteration 305/1000 | Loss: 0.00006263
Iteration 306/1000 | Loss: 0.00075215
Iteration 307/1000 | Loss: 0.00053411
Iteration 308/1000 | Loss: 0.00020455
Iteration 309/1000 | Loss: 0.00024613
Iteration 310/1000 | Loss: 0.00020032
Iteration 311/1000 | Loss: 0.00007428
Iteration 312/1000 | Loss: 0.00005594
Iteration 313/1000 | Loss: 0.00010276
Iteration 314/1000 | Loss: 0.00014685
Iteration 315/1000 | Loss: 0.00015401
Iteration 316/1000 | Loss: 0.00007764
Iteration 317/1000 | Loss: 0.00005296
Iteration 318/1000 | Loss: 0.00005539
Iteration 319/1000 | Loss: 0.00004423
Iteration 320/1000 | Loss: 0.00005167
Iteration 321/1000 | Loss: 0.00011269
Iteration 322/1000 | Loss: 0.00010969
Iteration 323/1000 | Loss: 0.00031638
Iteration 324/1000 | Loss: 0.00008453
Iteration 325/1000 | Loss: 0.00009118
Iteration 326/1000 | Loss: 0.00005930
Iteration 327/1000 | Loss: 0.00009606
Iteration 328/1000 | Loss: 0.00012009
Iteration 329/1000 | Loss: 0.00017294
Iteration 330/1000 | Loss: 0.00008396
Iteration 331/1000 | Loss: 0.00006359
Iteration 332/1000 | Loss: 0.00012617
Iteration 333/1000 | Loss: 0.00011857
Iteration 334/1000 | Loss: 0.00014163
Iteration 335/1000 | Loss: 0.00011581
Iteration 336/1000 | Loss: 0.00012526
Iteration 337/1000 | Loss: 0.00011364
Iteration 338/1000 | Loss: 0.00009199
Iteration 339/1000 | Loss: 0.00008204
Iteration 340/1000 | Loss: 0.00010483
Iteration 341/1000 | Loss: 0.00012172
Iteration 342/1000 | Loss: 0.00015823
Iteration 343/1000 | Loss: 0.00011359
Iteration 344/1000 | Loss: 0.00013058
Iteration 345/1000 | Loss: 0.00011371
Iteration 346/1000 | Loss: 0.00008689
Iteration 347/1000 | Loss: 0.00009323
Iteration 348/1000 | Loss: 0.00013491
Iteration 349/1000 | Loss: 0.00013845
Iteration 350/1000 | Loss: 0.00012100
Iteration 351/1000 | Loss: 0.00012722
Iteration 352/1000 | Loss: 0.00012784
Iteration 353/1000 | Loss: 0.00012840
Iteration 354/1000 | Loss: 0.00014771
Iteration 355/1000 | Loss: 0.00004785
Iteration 356/1000 | Loss: 0.00008848
Iteration 357/1000 | Loss: 0.00006969
Iteration 358/1000 | Loss: 0.00013557
Iteration 359/1000 | Loss: 0.00012134
Iteration 360/1000 | Loss: 0.00009825
Iteration 361/1000 | Loss: 0.00011951
Iteration 362/1000 | Loss: 0.00011424
Iteration 363/1000 | Loss: 0.00012617
Iteration 364/1000 | Loss: 0.00010070
Iteration 365/1000 | Loss: 0.00010250
Iteration 366/1000 | Loss: 0.00008170
Iteration 367/1000 | Loss: 0.00009937
Iteration 368/1000 | Loss: 0.00007658
Iteration 369/1000 | Loss: 0.00012094
Iteration 370/1000 | Loss: 0.00010717
Iteration 371/1000 | Loss: 0.00012751
Iteration 372/1000 | Loss: 0.00009115
Iteration 373/1000 | Loss: 0.00009590
Iteration 374/1000 | Loss: 0.00008346
Iteration 375/1000 | Loss: 0.00015863
Iteration 376/1000 | Loss: 0.00019847
Iteration 377/1000 | Loss: 0.00007847
Iteration 378/1000 | Loss: 0.00017886
Iteration 379/1000 | Loss: 0.00019055
Iteration 380/1000 | Loss: 0.00021366
Iteration 381/1000 | Loss: 0.00017338
Iteration 382/1000 | Loss: 0.00013877
Iteration 383/1000 | Loss: 0.00017852
Iteration 384/1000 | Loss: 0.00006195
Iteration 385/1000 | Loss: 0.00011900
Iteration 386/1000 | Loss: 0.00014309
Iteration 387/1000 | Loss: 0.00021360
Iteration 388/1000 | Loss: 0.00015781
Iteration 389/1000 | Loss: 0.00004635
Iteration 390/1000 | Loss: 0.00011471
Iteration 391/1000 | Loss: 0.00009148
Iteration 392/1000 | Loss: 0.00009854
Iteration 393/1000 | Loss: 0.00007356
Iteration 394/1000 | Loss: 0.00006560
Iteration 395/1000 | Loss: 0.00004244
Iteration 396/1000 | Loss: 0.00011608
Iteration 397/1000 | Loss: 0.00014026
Iteration 398/1000 | Loss: 0.00009565
Iteration 399/1000 | Loss: 0.00013248
Iteration 400/1000 | Loss: 0.00014994
Iteration 401/1000 | Loss: 0.00004299
Iteration 402/1000 | Loss: 0.00015396
Iteration 403/1000 | Loss: 0.00014808
Iteration 404/1000 | Loss: 0.00003890
Iteration 405/1000 | Loss: 0.00016423
Iteration 406/1000 | Loss: 0.00014790
Iteration 407/1000 | Loss: 0.00010790
Iteration 408/1000 | Loss: 0.00014381
Iteration 409/1000 | Loss: 0.00011534
Iteration 410/1000 | Loss: 0.00014418
Iteration 411/1000 | Loss: 0.00009034
Iteration 412/1000 | Loss: 0.00009782
Iteration 413/1000 | Loss: 0.00013410
Iteration 414/1000 | Loss: 0.00009153
Iteration 415/1000 | Loss: 0.00014370
Iteration 416/1000 | Loss: 0.00006477
Iteration 417/1000 | Loss: 0.00005123
Iteration 418/1000 | Loss: 0.00018296
Iteration 419/1000 | Loss: 0.00010257
Iteration 420/1000 | Loss: 0.00008820
Iteration 421/1000 | Loss: 0.00016697
Iteration 422/1000 | Loss: 0.00013423
Iteration 423/1000 | Loss: 0.00005304
Iteration 424/1000 | Loss: 0.00010728
Iteration 425/1000 | Loss: 0.00017993
Iteration 426/1000 | Loss: 0.00014929
Iteration 427/1000 | Loss: 0.00012070
Iteration 428/1000 | Loss: 0.00010533
Iteration 429/1000 | Loss: 0.00010385
Iteration 430/1000 | Loss: 0.00016538
Iteration 431/1000 | Loss: 0.00012699
Iteration 432/1000 | Loss: 0.00008517
Iteration 433/1000 | Loss: 0.00011625
Iteration 434/1000 | Loss: 0.00012861
Iteration 435/1000 | Loss: 0.00012355
Iteration 436/1000 | Loss: 0.00019572
Iteration 437/1000 | Loss: 0.00014770
Iteration 438/1000 | Loss: 0.00013169
Iteration 439/1000 | Loss: 0.00014177
Iteration 440/1000 | Loss: 0.00011794
Iteration 441/1000 | Loss: 0.00014601
Iteration 442/1000 | Loss: 0.00011148
Iteration 443/1000 | Loss: 0.00010197
Iteration 444/1000 | Loss: 0.00014790
Iteration 445/1000 | Loss: 0.00013384
Iteration 446/1000 | Loss: 0.00014716
Iteration 447/1000 | Loss: 0.00017571
Iteration 448/1000 | Loss: 0.00016600
Iteration 449/1000 | Loss: 0.00020498
Iteration 450/1000 | Loss: 0.00006927
Iteration 451/1000 | Loss: 0.00013348
Iteration 452/1000 | Loss: 0.00014681
Iteration 453/1000 | Loss: 0.00013851
Iteration 454/1000 | Loss: 0.00010041
Iteration 455/1000 | Loss: 0.00008359
Iteration 456/1000 | Loss: 0.00015077
Iteration 457/1000 | Loss: 0.00004117
Iteration 458/1000 | Loss: 0.00005473
Iteration 459/1000 | Loss: 0.00004528
Iteration 460/1000 | Loss: 0.00011602
Iteration 461/1000 | Loss: 0.00006338
Iteration 462/1000 | Loss: 0.00004059
Iteration 463/1000 | Loss: 0.00015660
Iteration 464/1000 | Loss: 0.00012867
Iteration 465/1000 | Loss: 0.00010389
Iteration 466/1000 | Loss: 0.00004001
Iteration 467/1000 | Loss: 0.00005488
Iteration 468/1000 | Loss: 0.00020412
Iteration 469/1000 | Loss: 0.00007478
Iteration 470/1000 | Loss: 0.00009123
Iteration 471/1000 | Loss: 0.00010744
Iteration 472/1000 | Loss: 0.00010328
Iteration 473/1000 | Loss: 0.00014653
Iteration 474/1000 | Loss: 0.00011453
Iteration 475/1000 | Loss: 0.00014081
Iteration 476/1000 | Loss: 0.00012816
Iteration 477/1000 | Loss: 0.00007900
Iteration 478/1000 | Loss: 0.00010278
Iteration 479/1000 | Loss: 0.00012713
Iteration 480/1000 | Loss: 0.00010281
Iteration 481/1000 | Loss: 0.00012766
Iteration 482/1000 | Loss: 0.00011491
Iteration 483/1000 | Loss: 0.00012800
Iteration 484/1000 | Loss: 0.00011396
Iteration 485/1000 | Loss: 0.00013280
Iteration 486/1000 | Loss: 0.00014540
Iteration 487/1000 | Loss: 0.00013253
Iteration 488/1000 | Loss: 0.00012332
Iteration 489/1000 | Loss: 0.00012641
Iteration 490/1000 | Loss: 0.00011187
Iteration 491/1000 | Loss: 0.00013358
Iteration 492/1000 | Loss: 0.00011072
Iteration 493/1000 | Loss: 0.00012563
Iteration 494/1000 | Loss: 0.00011287
Iteration 495/1000 | Loss: 0.00012063
Iteration 496/1000 | Loss: 0.00011604
Iteration 497/1000 | Loss: 0.00012504
Iteration 498/1000 | Loss: 0.00010396
Iteration 499/1000 | Loss: 0.00004031
Iteration 500/1000 | Loss: 0.00008941
Iteration 501/1000 | Loss: 0.00013596
Iteration 502/1000 | Loss: 0.00015861
Iteration 503/1000 | Loss: 0.00013653
Iteration 504/1000 | Loss: 0.00009767
Iteration 505/1000 | Loss: 0.00013708
Iteration 506/1000 | Loss: 0.00016183
Iteration 507/1000 | Loss: 0.00008490
Iteration 508/1000 | Loss: 0.00003853
Iteration 509/1000 | Loss: 0.00007450
Iteration 510/1000 | Loss: 0.00005212
Iteration 511/1000 | Loss: 0.00004536
Iteration 512/1000 | Loss: 0.00002952
Iteration 513/1000 | Loss: 0.00004161
Iteration 514/1000 | Loss: 0.00003553
Iteration 515/1000 | Loss: 0.00004663
Iteration 516/1000 | Loss: 0.00004163
Iteration 517/1000 | Loss: 0.00005320
Iteration 518/1000 | Loss: 0.00004680
Iteration 519/1000 | Loss: 0.00005075
Iteration 520/1000 | Loss: 0.00004506
Iteration 521/1000 | Loss: 0.00005194
Iteration 522/1000 | Loss: 0.00004274
Iteration 523/1000 | Loss: 0.00018688
Iteration 524/1000 | Loss: 0.00017792
Iteration 525/1000 | Loss: 0.00005034
Iteration 526/1000 | Loss: 0.00018408
Iteration 527/1000 | Loss: 0.00014524
Iteration 528/1000 | Loss: 0.00017807
Iteration 529/1000 | Loss: 0.00018053
Iteration 530/1000 | Loss: 0.00003458
Iteration 531/1000 | Loss: 0.00003110
Iteration 532/1000 | Loss: 0.00004953
Iteration 533/1000 | Loss: 0.00004298
Iteration 534/1000 | Loss: 0.00004301
Iteration 535/1000 | Loss: 0.00005017
Iteration 536/1000 | Loss: 0.00005717
Iteration 537/1000 | Loss: 0.00003064
Iteration 538/1000 | Loss: 0.00004244
Iteration 539/1000 | Loss: 0.00004412
Iteration 540/1000 | Loss: 0.00003833
Iteration 541/1000 | Loss: 0.00004917
Iteration 542/1000 | Loss: 0.00004311
Iteration 543/1000 | Loss: 0.00005002
Iteration 544/1000 | Loss: 0.00004336
Iteration 545/1000 | Loss: 0.00003839
Iteration 546/1000 | Loss: 0.00003558
Iteration 547/1000 | Loss: 0.00004146
Iteration 548/1000 | Loss: 0.00004591
Iteration 549/1000 | Loss: 0.00004637
Iteration 550/1000 | Loss: 0.00004145
Iteration 551/1000 | Loss: 0.00004625
Iteration 552/1000 | Loss: 0.00003549
Iteration 553/1000 | Loss: 0.00003623
Iteration 554/1000 | Loss: 0.00003801
Iteration 555/1000 | Loss: 0.00004728
Iteration 556/1000 | Loss: 0.00003678
Iteration 557/1000 | Loss: 0.00003873
Iteration 558/1000 | Loss: 0.00003784
Iteration 559/1000 | Loss: 0.00003950
Iteration 560/1000 | Loss: 0.00003418
Iteration 561/1000 | Loss: 0.00003718
Iteration 562/1000 | Loss: 0.00003684
Iteration 563/1000 | Loss: 0.00004712
Iteration 564/1000 | Loss: 0.00004931
Iteration 565/1000 | Loss: 0.00003914
Iteration 566/1000 | Loss: 0.00004487
Iteration 567/1000 | Loss: 0.00003953
Iteration 568/1000 | Loss: 0.00003378
Iteration 569/1000 | Loss: 0.00002734
Iteration 570/1000 | Loss: 0.00005268
Iteration 571/1000 | Loss: 0.00004197
Iteration 572/1000 | Loss: 0.00004662
Iteration 573/1000 | Loss: 0.00004657
Iteration 574/1000 | Loss: 0.00003860
Iteration 575/1000 | Loss: 0.00003282
Iteration 576/1000 | Loss: 0.00002261
Iteration 577/1000 | Loss: 0.00003850
Iteration 578/1000 | Loss: 0.00004276
Iteration 579/1000 | Loss: 0.00003633
Iteration 580/1000 | Loss: 0.00004480
Iteration 581/1000 | Loss: 0.00004684
Iteration 582/1000 | Loss: 0.00004577
Iteration 583/1000 | Loss: 0.00002799
Iteration 584/1000 | Loss: 0.00002966
Iteration 585/1000 | Loss: 0.00004751
Iteration 586/1000 | Loss: 0.00004251
Iteration 587/1000 | Loss: 0.00004188
Iteration 588/1000 | Loss: 0.00005147
Iteration 589/1000 | Loss: 0.00005186
Iteration 590/1000 | Loss: 0.00004515
Iteration 591/1000 | Loss: 0.00004191
Iteration 592/1000 | Loss: 0.00004328
Iteration 593/1000 | Loss: 0.00004120
Iteration 594/1000 | Loss: 0.00004954
Iteration 595/1000 | Loss: 0.00002991
Iteration 596/1000 | Loss: 0.00002588
Iteration 597/1000 | Loss: 0.00004356
Iteration 598/1000 | Loss: 0.00004934
Iteration 599/1000 | Loss: 0.00004806
Iteration 600/1000 | Loss: 0.00005055
Iteration 601/1000 | Loss: 0.00005694
Iteration 602/1000 | Loss: 0.00005617
Iteration 603/1000 | Loss: 0.00004847
Iteration 604/1000 | Loss: 0.00005192
Iteration 605/1000 | Loss: 0.00004737
Iteration 606/1000 | Loss: 0.00004528
Iteration 607/1000 | Loss: 0.00004448
Iteration 608/1000 | Loss: 0.00004435
Iteration 609/1000 | Loss: 0.00005345
Iteration 610/1000 | Loss: 0.00004749
Iteration 611/1000 | Loss: 0.00003856
Iteration 612/1000 | Loss: 0.00004825
Iteration 613/1000 | Loss: 0.00004601
Iteration 614/1000 | Loss: 0.00004396
Iteration 615/1000 | Loss: 0.00003418
Iteration 616/1000 | Loss: 0.00004770
Iteration 617/1000 | Loss: 0.00005468
Iteration 618/1000 | Loss: 0.00005105
Iteration 619/1000 | Loss: 0.00005951
Iteration 620/1000 | Loss: 0.00006182
Iteration 621/1000 | Loss: 0.00006256
Iteration 622/1000 | Loss: 0.00005693
Iteration 623/1000 | Loss: 0.00005214
Iteration 624/1000 | Loss: 0.00005576
Iteration 625/1000 | Loss: 0.00003473
Iteration 626/1000 | Loss: 0.00003946
Iteration 627/1000 | Loss: 0.00003492
Iteration 628/1000 | Loss: 0.00003801
Iteration 629/1000 | Loss: 0.00004459
Iteration 630/1000 | Loss: 0.00004650
Iteration 631/1000 | Loss: 0.00003282
Iteration 632/1000 | Loss: 0.00005816
Iteration 633/1000 | Loss: 0.00003662
Iteration 634/1000 | Loss: 0.00004613
Iteration 635/1000 | Loss: 0.00003578
Iteration 636/1000 | Loss: 0.00003893
Iteration 637/1000 | Loss: 0.00004457
Iteration 638/1000 | Loss: 0.00004245
Iteration 639/1000 | Loss: 0.00004451
Iteration 640/1000 | Loss: 0.00004245
Iteration 641/1000 | Loss: 0.00004665
Iteration 642/1000 | Loss: 0.00004452
Iteration 643/1000 | Loss: 0.00004619
Iteration 644/1000 | Loss: 0.00004241
Iteration 645/1000 | Loss: 0.00003709
Iteration 646/1000 | Loss: 0.00004295
Iteration 647/1000 | Loss: 0.00004416
Iteration 648/1000 | Loss: 0.00004408
Iteration 649/1000 | Loss: 0.00004891
Iteration 650/1000 | Loss: 0.00005356
Iteration 651/1000 | Loss: 0.00003223
Iteration 652/1000 | Loss: 0.00004074
Iteration 653/1000 | Loss: 0.00004298
Iteration 654/1000 | Loss: 0.00003626
Iteration 655/1000 | Loss: 0.00004882
Iteration 656/1000 | Loss: 0.00004146
Iteration 657/1000 | Loss: 0.00005344
Iteration 658/1000 | Loss: 0.00004281
Iteration 659/1000 | Loss: 0.00004809
Iteration 660/1000 | Loss: 0.00004450
Iteration 661/1000 | Loss: 0.00004673
Iteration 662/1000 | Loss: 0.00003359
Iteration 663/1000 | Loss: 0.00002851
Iteration 664/1000 | Loss: 0.00004654
Iteration 665/1000 | Loss: 0.00004503
Iteration 666/1000 | Loss: 0.00004405
Iteration 667/1000 | Loss: 0.00004849
Iteration 668/1000 | Loss: 0.00004810
Iteration 669/1000 | Loss: 0.00003968
Iteration 670/1000 | Loss: 0.00005675
Iteration 671/1000 | Loss: 0.00004681
Iteration 672/1000 | Loss: 0.00003992
Iteration 673/1000 | Loss: 0.00004139
Iteration 674/1000 | Loss: 0.00005011
Iteration 675/1000 | Loss: 0.00005385
Iteration 676/1000 | Loss: 0.00004526
Iteration 677/1000 | Loss: 0.00005267
Iteration 678/1000 | Loss: 0.00004324
Iteration 679/1000 | Loss: 0.00004844
Iteration 680/1000 | Loss: 0.00005720
Iteration 681/1000 | Loss: 0.00002916
Iteration 682/1000 | Loss: 0.00002439
Iteration 683/1000 | Loss: 0.00002276
Iteration 684/1000 | Loss: 0.00002224
Iteration 685/1000 | Loss: 0.00002190
Iteration 686/1000 | Loss: 0.00002148
Iteration 687/1000 | Loss: 0.00002130
Iteration 688/1000 | Loss: 0.00002127
Iteration 689/1000 | Loss: 0.00002103
Iteration 690/1000 | Loss: 0.00002101
Iteration 691/1000 | Loss: 0.00002093
Iteration 692/1000 | Loss: 0.00002080
Iteration 693/1000 | Loss: 0.00002067
Iteration 694/1000 | Loss: 0.00002067
Iteration 695/1000 | Loss: 0.00002066
Iteration 696/1000 | Loss: 0.00002066
Iteration 697/1000 | Loss: 0.00002066
Iteration 698/1000 | Loss: 0.00002066
Iteration 699/1000 | Loss: 0.00002066
Iteration 700/1000 | Loss: 0.00002066
Iteration 701/1000 | Loss: 0.00002065
Iteration 702/1000 | Loss: 0.00002065
Iteration 703/1000 | Loss: 0.00002065
Iteration 704/1000 | Loss: 0.00002063
Iteration 705/1000 | Loss: 0.00002062
Iteration 706/1000 | Loss: 0.00002062
Iteration 707/1000 | Loss: 0.00002062
Iteration 708/1000 | Loss: 0.00002062
Iteration 709/1000 | Loss: 0.00002062
Iteration 710/1000 | Loss: 0.00002062
Iteration 711/1000 | Loss: 0.00002062
Iteration 712/1000 | Loss: 0.00002061
Iteration 713/1000 | Loss: 0.00002059
Iteration 714/1000 | Loss: 0.00002058
Iteration 715/1000 | Loss: 0.00002058
Iteration 716/1000 | Loss: 0.00002057
Iteration 717/1000 | Loss: 0.00002057
Iteration 718/1000 | Loss: 0.00002057
Iteration 719/1000 | Loss: 0.00002056
Iteration 720/1000 | Loss: 0.00002056
Iteration 721/1000 | Loss: 0.00002056
Iteration 722/1000 | Loss: 0.00002056
Iteration 723/1000 | Loss: 0.00002056
Iteration 724/1000 | Loss: 0.00002055
Iteration 725/1000 | Loss: 0.00002055
Iteration 726/1000 | Loss: 0.00002055
Iteration 727/1000 | Loss: 0.00002055
Iteration 728/1000 | Loss: 0.00002055
Iteration 729/1000 | Loss: 0.00002055
Iteration 730/1000 | Loss: 0.00002055
Iteration 731/1000 | Loss: 0.00002055
Iteration 732/1000 | Loss: 0.00002055
Iteration 733/1000 | Loss: 0.00002055
Iteration 734/1000 | Loss: 0.00002054
Iteration 735/1000 | Loss: 0.00002054
Iteration 736/1000 | Loss: 0.00002054
Iteration 737/1000 | Loss: 0.00002054
Iteration 738/1000 | Loss: 0.00002054
Iteration 739/1000 | Loss: 0.00002054
Iteration 740/1000 | Loss: 0.00002054
Iteration 741/1000 | Loss: 0.00002054
Iteration 742/1000 | Loss: 0.00002054
Iteration 743/1000 | Loss: 0.00002054
Iteration 744/1000 | Loss: 0.00002053
Iteration 745/1000 | Loss: 0.00002053
Iteration 746/1000 | Loss: 0.00002053
Iteration 747/1000 | Loss: 0.00002053
Iteration 748/1000 | Loss: 0.00002053
Iteration 749/1000 | Loss: 0.00002053
Iteration 750/1000 | Loss: 0.00002053
Iteration 751/1000 | Loss: 0.00002053
Iteration 752/1000 | Loss: 0.00002053
Iteration 753/1000 | Loss: 0.00002053
Iteration 754/1000 | Loss: 0.00002053
Iteration 755/1000 | Loss: 0.00002053
Iteration 756/1000 | Loss: 0.00002053
Iteration 757/1000 | Loss: 0.00002052
Iteration 758/1000 | Loss: 0.00002052
Iteration 759/1000 | Loss: 0.00002052
Iteration 760/1000 | Loss: 0.00002052
Iteration 761/1000 | Loss: 0.00002052
Iteration 762/1000 | Loss: 0.00002052
Iteration 763/1000 | Loss: 0.00002052
Iteration 764/1000 | Loss: 0.00002051
Iteration 765/1000 | Loss: 0.00002051
Iteration 766/1000 | Loss: 0.00002051
Iteration 767/1000 | Loss: 0.00002051
Iteration 768/1000 | Loss: 0.00002051
Iteration 769/1000 | Loss: 0.00002051
Iteration 770/1000 | Loss: 0.00002051
Iteration 771/1000 | Loss: 0.00002050
Iteration 772/1000 | Loss: 0.00002050
Iteration 773/1000 | Loss: 0.00002050
Iteration 774/1000 | Loss: 0.00002050
Iteration 775/1000 | Loss: 0.00002050
Iteration 776/1000 | Loss: 0.00002050
Iteration 777/1000 | Loss: 0.00002050
Iteration 778/1000 | Loss: 0.00002050
Iteration 779/1000 | Loss: 0.00002050
Iteration 780/1000 | Loss: 0.00002050
Iteration 781/1000 | Loss: 0.00002050
Iteration 782/1000 | Loss: 0.00002050
Iteration 783/1000 | Loss: 0.00002050
Iteration 784/1000 | Loss: 0.00002050
Iteration 785/1000 | Loss: 0.00002050
Iteration 786/1000 | Loss: 0.00002050
Iteration 787/1000 | Loss: 0.00002049
Iteration 788/1000 | Loss: 0.00002049
Iteration 789/1000 | Loss: 0.00002049
Iteration 790/1000 | Loss: 0.00002049
Iteration 791/1000 | Loss: 0.00002049
Iteration 792/1000 | Loss: 0.00002048
Iteration 793/1000 | Loss: 0.00002048
Iteration 794/1000 | Loss: 0.00002048
Iteration 795/1000 | Loss: 0.00002048
Iteration 796/1000 | Loss: 0.00002048
Iteration 797/1000 | Loss: 0.00002048
Iteration 798/1000 | Loss: 0.00002048
Iteration 799/1000 | Loss: 0.00002048
Iteration 800/1000 | Loss: 0.00002048
Iteration 801/1000 | Loss: 0.00002048
Iteration 802/1000 | Loss: 0.00002048
Iteration 803/1000 | Loss: 0.00002048
Iteration 804/1000 | Loss: 0.00002048
Iteration 805/1000 | Loss: 0.00002048
Iteration 806/1000 | Loss: 0.00002048
Iteration 807/1000 | Loss: 0.00002048
Iteration 808/1000 | Loss: 0.00002048
Iteration 809/1000 | Loss: 0.00002047
Iteration 810/1000 | Loss: 0.00002047
Iteration 811/1000 | Loss: 0.00002047
Iteration 812/1000 | Loss: 0.00002047
Iteration 813/1000 | Loss: 0.00002047
Iteration 814/1000 | Loss: 0.00002047
Iteration 815/1000 | Loss: 0.00002047
Iteration 816/1000 | Loss: 0.00002047
Iteration 817/1000 | Loss: 0.00002047
Iteration 818/1000 | Loss: 0.00002047
Iteration 819/1000 | Loss: 0.00002047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 819. Stopping optimization.
Last 5 losses: [2.0473409676924348e-05, 2.0473409676924348e-05, 2.0473409676924348e-05, 2.0473409676924348e-05, 2.0473409676924348e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0473409676924348e-05

Optimization complete. Final v2v error: 3.420900344848633 mm

Highest mean error: 11.711262702941895 mm for frame 45

Lowest mean error: 2.9860002994537354 mm for frame 129

Saving results

Total time: 1105.1512780189514
