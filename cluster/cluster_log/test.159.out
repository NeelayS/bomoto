Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=159, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8904-8959
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025431
Iteration 2/25 | Loss: 0.00235694
Iteration 3/25 | Loss: 0.00193282
Iteration 4/25 | Loss: 0.00184701
Iteration 5/25 | Loss: 0.00181843
Iteration 6/25 | Loss: 0.00180869
Iteration 7/25 | Loss: 0.00180683
Iteration 8/25 | Loss: 0.00180683
Iteration 9/25 | Loss: 0.00180683
Iteration 10/25 | Loss: 0.00180683
Iteration 11/25 | Loss: 0.00180683
Iteration 12/25 | Loss: 0.00180683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0018068301724269986, 0.0018068301724269986, 0.0018068301724269986, 0.0018068301724269986, 0.0018068301724269986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018068301724269986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25829172
Iteration 2/25 | Loss: 0.00774065
Iteration 3/25 | Loss: 0.00694424
Iteration 4/25 | Loss: 0.00694424
Iteration 5/25 | Loss: 0.00694424
Iteration 6/25 | Loss: 0.00694424
Iteration 7/25 | Loss: 0.00694424
Iteration 8/25 | Loss: 0.00694423
Iteration 9/25 | Loss: 0.00694423
Iteration 10/25 | Loss: 0.00694423
Iteration 11/25 | Loss: 0.00694423
Iteration 12/25 | Loss: 0.00694423
Iteration 13/25 | Loss: 0.00694423
Iteration 14/25 | Loss: 0.00694423
Iteration 15/25 | Loss: 0.00694423
Iteration 16/25 | Loss: 0.00694423
Iteration 17/25 | Loss: 0.00694423
Iteration 18/25 | Loss: 0.00694423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00694423308596015, 0.00694423308596015, 0.00694423308596015, 0.00694423308596015, 0.00694423308596015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00694423308596015

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00694423
Iteration 2/1000 | Loss: 0.00123747
Iteration 3/1000 | Loss: 0.00078483
Iteration 4/1000 | Loss: 0.00101269
Iteration 5/1000 | Loss: 0.00329625
Iteration 6/1000 | Loss: 0.00126453
Iteration 7/1000 | Loss: 0.00076006
Iteration 8/1000 | Loss: 0.00090334
Iteration 9/1000 | Loss: 0.00067003
Iteration 10/1000 | Loss: 0.00073423
Iteration 11/1000 | Loss: 0.00042304
Iteration 12/1000 | Loss: 0.00054466
Iteration 13/1000 | Loss: 0.00905643
Iteration 14/1000 | Loss: 0.00532739
Iteration 15/1000 | Loss: 0.00063628
Iteration 16/1000 | Loss: 0.00036972
Iteration 17/1000 | Loss: 0.00021063
Iteration 18/1000 | Loss: 0.00011405
Iteration 19/1000 | Loss: 0.00007845
Iteration 20/1000 | Loss: 0.00005906
Iteration 21/1000 | Loss: 0.00004512
Iteration 22/1000 | Loss: 0.00003414
Iteration 23/1000 | Loss: 0.00005744
Iteration 24/1000 | Loss: 0.00003798
Iteration 25/1000 | Loss: 0.00008979
Iteration 26/1000 | Loss: 0.00014711
Iteration 27/1000 | Loss: 0.00001962
Iteration 28/1000 | Loss: 0.00004462
Iteration 29/1000 | Loss: 0.00001602
Iteration 30/1000 | Loss: 0.00002612
Iteration 31/1000 | Loss: 0.00001420
Iteration 32/1000 | Loss: 0.00001349
Iteration 33/1000 | Loss: 0.00004327
Iteration 34/1000 | Loss: 0.00011765
Iteration 35/1000 | Loss: 0.00006918
Iteration 36/1000 | Loss: 0.00003556
Iteration 37/1000 | Loss: 0.00002579
Iteration 38/1000 | Loss: 0.00001285
Iteration 39/1000 | Loss: 0.00001496
Iteration 40/1000 | Loss: 0.00003920
Iteration 41/1000 | Loss: 0.00001553
Iteration 42/1000 | Loss: 0.00001293
Iteration 43/1000 | Loss: 0.00001257
Iteration 44/1000 | Loss: 0.00001257
Iteration 45/1000 | Loss: 0.00001257
Iteration 46/1000 | Loss: 0.00001257
Iteration 47/1000 | Loss: 0.00001257
Iteration 48/1000 | Loss: 0.00001257
Iteration 49/1000 | Loss: 0.00001256
Iteration 50/1000 | Loss: 0.00001256
Iteration 51/1000 | Loss: 0.00001513
Iteration 52/1000 | Loss: 0.00002774
Iteration 53/1000 | Loss: 0.00005020
Iteration 54/1000 | Loss: 0.00001259
Iteration 55/1000 | Loss: 0.00001314
Iteration 56/1000 | Loss: 0.00001251
Iteration 57/1000 | Loss: 0.00001250
Iteration 58/1000 | Loss: 0.00001248
Iteration 59/1000 | Loss: 0.00001247
Iteration 60/1000 | Loss: 0.00001247
Iteration 61/1000 | Loss: 0.00001247
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001245
Iteration 65/1000 | Loss: 0.00001245
Iteration 66/1000 | Loss: 0.00001245
Iteration 67/1000 | Loss: 0.00001245
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001244
Iteration 72/1000 | Loss: 0.00001244
Iteration 73/1000 | Loss: 0.00001244
Iteration 74/1000 | Loss: 0.00001244
Iteration 75/1000 | Loss: 0.00001244
Iteration 76/1000 | Loss: 0.00001244
Iteration 77/1000 | Loss: 0.00001244
Iteration 78/1000 | Loss: 0.00001244
Iteration 79/1000 | Loss: 0.00001244
Iteration 80/1000 | Loss: 0.00001243
Iteration 81/1000 | Loss: 0.00001243
Iteration 82/1000 | Loss: 0.00001243
Iteration 83/1000 | Loss: 0.00001243
Iteration 84/1000 | Loss: 0.00001243
Iteration 85/1000 | Loss: 0.00001243
Iteration 86/1000 | Loss: 0.00001243
Iteration 87/1000 | Loss: 0.00001243
Iteration 88/1000 | Loss: 0.00001243
Iteration 89/1000 | Loss: 0.00001242
Iteration 90/1000 | Loss: 0.00001242
Iteration 91/1000 | Loss: 0.00001242
Iteration 92/1000 | Loss: 0.00001242
Iteration 93/1000 | Loss: 0.00001242
Iteration 94/1000 | Loss: 0.00001242
Iteration 95/1000 | Loss: 0.00001242
Iteration 96/1000 | Loss: 0.00001242
Iteration 97/1000 | Loss: 0.00001241
Iteration 98/1000 | Loss: 0.00001241
Iteration 99/1000 | Loss: 0.00001241
Iteration 100/1000 | Loss: 0.00001241
Iteration 101/1000 | Loss: 0.00001241
Iteration 102/1000 | Loss: 0.00001241
Iteration 103/1000 | Loss: 0.00001241
Iteration 104/1000 | Loss: 0.00001241
Iteration 105/1000 | Loss: 0.00001241
Iteration 106/1000 | Loss: 0.00001240
Iteration 107/1000 | Loss: 0.00001240
Iteration 108/1000 | Loss: 0.00001240
Iteration 109/1000 | Loss: 0.00001240
Iteration 110/1000 | Loss: 0.00001240
Iteration 111/1000 | Loss: 0.00001240
Iteration 112/1000 | Loss: 0.00001240
Iteration 113/1000 | Loss: 0.00001240
Iteration 114/1000 | Loss: 0.00001240
Iteration 115/1000 | Loss: 0.00001240
Iteration 116/1000 | Loss: 0.00001240
Iteration 117/1000 | Loss: 0.00001240
Iteration 118/1000 | Loss: 0.00001240
Iteration 119/1000 | Loss: 0.00001240
Iteration 120/1000 | Loss: 0.00001240
Iteration 121/1000 | Loss: 0.00001239
Iteration 122/1000 | Loss: 0.00001239
Iteration 123/1000 | Loss: 0.00001239
Iteration 124/1000 | Loss: 0.00001239
Iteration 125/1000 | Loss: 0.00001239
Iteration 126/1000 | Loss: 0.00001239
Iteration 127/1000 | Loss: 0.00001239
Iteration 128/1000 | Loss: 0.00001239
Iteration 129/1000 | Loss: 0.00001239
Iteration 130/1000 | Loss: 0.00001239
Iteration 131/1000 | Loss: 0.00001239
Iteration 132/1000 | Loss: 0.00001239
Iteration 133/1000 | Loss: 0.00001239
Iteration 134/1000 | Loss: 0.00001239
Iteration 135/1000 | Loss: 0.00001239
Iteration 136/1000 | Loss: 0.00001239
Iteration 137/1000 | Loss: 0.00001239
Iteration 138/1000 | Loss: 0.00001239
Iteration 139/1000 | Loss: 0.00001239
Iteration 140/1000 | Loss: 0.00001239
Iteration 141/1000 | Loss: 0.00001239
Iteration 142/1000 | Loss: 0.00001239
Iteration 143/1000 | Loss: 0.00001239
Iteration 144/1000 | Loss: 0.00001239
Iteration 145/1000 | Loss: 0.00001239
Iteration 146/1000 | Loss: 0.00001239
Iteration 147/1000 | Loss: 0.00001239
Iteration 148/1000 | Loss: 0.00001239
Iteration 149/1000 | Loss: 0.00001239
Iteration 150/1000 | Loss: 0.00001239
Iteration 151/1000 | Loss: 0.00001239
Iteration 152/1000 | Loss: 0.00001239
Iteration 153/1000 | Loss: 0.00001239
Iteration 154/1000 | Loss: 0.00001239
Iteration 155/1000 | Loss: 0.00001239
Iteration 156/1000 | Loss: 0.00001239
Iteration 157/1000 | Loss: 0.00001239
Iteration 158/1000 | Loss: 0.00001239
Iteration 159/1000 | Loss: 0.00001239
Iteration 160/1000 | Loss: 0.00001239
Iteration 161/1000 | Loss: 0.00001239
Iteration 162/1000 | Loss: 0.00001239
Iteration 163/1000 | Loss: 0.00001239
Iteration 164/1000 | Loss: 0.00001239
Iteration 165/1000 | Loss: 0.00001239
Iteration 166/1000 | Loss: 0.00001239
Iteration 167/1000 | Loss: 0.00001239
Iteration 168/1000 | Loss: 0.00001239
Iteration 169/1000 | Loss: 0.00001239
Iteration 170/1000 | Loss: 0.00001239
Iteration 171/1000 | Loss: 0.00001239
Iteration 172/1000 | Loss: 0.00001239
Iteration 173/1000 | Loss: 0.00001239
Iteration 174/1000 | Loss: 0.00001239
Iteration 175/1000 | Loss: 0.00001239
Iteration 176/1000 | Loss: 0.00001239
Iteration 177/1000 | Loss: 0.00001239
Iteration 178/1000 | Loss: 0.00001239
Iteration 179/1000 | Loss: 0.00001239
Iteration 180/1000 | Loss: 0.00001239
Iteration 181/1000 | Loss: 0.00001239
Iteration 182/1000 | Loss: 0.00001239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.2385340596665628e-05, 1.2385340596665628e-05, 1.2385340596665628e-05, 1.2385340596665628e-05, 1.2385340596665628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2385340596665628e-05

Optimization complete. Final v2v error: 2.981879711151123 mm

Highest mean error: 3.1805243492126465 mm for frame 1

Lowest mean error: 2.8008666038513184 mm for frame 213

Saving results

Total time: 94.56236672401428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00910436
Iteration 2/25 | Loss: 0.00139527
Iteration 3/25 | Loss: 0.00108088
Iteration 4/25 | Loss: 0.00104665
Iteration 5/25 | Loss: 0.00104175
Iteration 6/25 | Loss: 0.00104089
Iteration 7/25 | Loss: 0.00104089
Iteration 8/25 | Loss: 0.00104089
Iteration 9/25 | Loss: 0.00104089
Iteration 10/25 | Loss: 0.00104089
Iteration 11/25 | Loss: 0.00104089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010408860398456454, 0.0010408860398456454, 0.0010408860398456454, 0.0010408860398456454, 0.0010408860398456454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010408860398456454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83694029
Iteration 2/25 | Loss: 0.00053691
Iteration 3/25 | Loss: 0.00053691
Iteration 4/25 | Loss: 0.00053691
Iteration 5/25 | Loss: 0.00053691
Iteration 6/25 | Loss: 0.00053691
Iteration 7/25 | Loss: 0.00053691
Iteration 8/25 | Loss: 0.00053691
Iteration 9/25 | Loss: 0.00053691
Iteration 10/25 | Loss: 0.00053691
Iteration 11/25 | Loss: 0.00053691
Iteration 12/25 | Loss: 0.00053691
Iteration 13/25 | Loss: 0.00053691
Iteration 14/25 | Loss: 0.00053691
Iteration 15/25 | Loss: 0.00053691
Iteration 16/25 | Loss: 0.00053691
Iteration 17/25 | Loss: 0.00053691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000536906358320266, 0.000536906358320266, 0.000536906358320266, 0.000536906358320266, 0.000536906358320266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000536906358320266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053691
Iteration 2/1000 | Loss: 0.00005156
Iteration 3/1000 | Loss: 0.00004017
Iteration 4/1000 | Loss: 0.00003612
Iteration 5/1000 | Loss: 0.00003415
Iteration 6/1000 | Loss: 0.00003263
Iteration 7/1000 | Loss: 0.00003169
Iteration 8/1000 | Loss: 0.00003125
Iteration 9/1000 | Loss: 0.00003092
Iteration 10/1000 | Loss: 0.00003068
Iteration 11/1000 | Loss: 0.00003050
Iteration 12/1000 | Loss: 0.00003032
Iteration 13/1000 | Loss: 0.00003029
Iteration 14/1000 | Loss: 0.00003029
Iteration 15/1000 | Loss: 0.00003029
Iteration 16/1000 | Loss: 0.00003029
Iteration 17/1000 | Loss: 0.00003029
Iteration 18/1000 | Loss: 0.00003029
Iteration 19/1000 | Loss: 0.00003029
Iteration 20/1000 | Loss: 0.00003029
Iteration 21/1000 | Loss: 0.00003028
Iteration 22/1000 | Loss: 0.00003028
Iteration 23/1000 | Loss: 0.00003026
Iteration 24/1000 | Loss: 0.00003026
Iteration 25/1000 | Loss: 0.00003026
Iteration 26/1000 | Loss: 0.00003026
Iteration 27/1000 | Loss: 0.00003026
Iteration 28/1000 | Loss: 0.00003025
Iteration 29/1000 | Loss: 0.00003021
Iteration 30/1000 | Loss: 0.00003020
Iteration 31/1000 | Loss: 0.00003020
Iteration 32/1000 | Loss: 0.00003020
Iteration 33/1000 | Loss: 0.00003020
Iteration 34/1000 | Loss: 0.00003020
Iteration 35/1000 | Loss: 0.00003020
Iteration 36/1000 | Loss: 0.00003019
Iteration 37/1000 | Loss: 0.00003019
Iteration 38/1000 | Loss: 0.00003019
Iteration 39/1000 | Loss: 0.00003019
Iteration 40/1000 | Loss: 0.00003019
Iteration 41/1000 | Loss: 0.00003019
Iteration 42/1000 | Loss: 0.00003019
Iteration 43/1000 | Loss: 0.00003019
Iteration 44/1000 | Loss: 0.00003019
Iteration 45/1000 | Loss: 0.00003019
Iteration 46/1000 | Loss: 0.00003018
Iteration 47/1000 | Loss: 0.00003018
Iteration 48/1000 | Loss: 0.00003018
Iteration 49/1000 | Loss: 0.00003018
Iteration 50/1000 | Loss: 0.00003018
Iteration 51/1000 | Loss: 0.00003018
Iteration 52/1000 | Loss: 0.00003018
Iteration 53/1000 | Loss: 0.00003018
Iteration 54/1000 | Loss: 0.00003018
Iteration 55/1000 | Loss: 0.00003017
Iteration 56/1000 | Loss: 0.00003017
Iteration 57/1000 | Loss: 0.00003017
Iteration 58/1000 | Loss: 0.00003017
Iteration 59/1000 | Loss: 0.00003017
Iteration 60/1000 | Loss: 0.00003017
Iteration 61/1000 | Loss: 0.00003017
Iteration 62/1000 | Loss: 0.00003017
Iteration 63/1000 | Loss: 0.00003016
Iteration 64/1000 | Loss: 0.00003016
Iteration 65/1000 | Loss: 0.00003016
Iteration 66/1000 | Loss: 0.00003016
Iteration 67/1000 | Loss: 0.00003016
Iteration 68/1000 | Loss: 0.00003016
Iteration 69/1000 | Loss: 0.00003015
Iteration 70/1000 | Loss: 0.00003015
Iteration 71/1000 | Loss: 0.00003015
Iteration 72/1000 | Loss: 0.00003015
Iteration 73/1000 | Loss: 0.00003015
Iteration 74/1000 | Loss: 0.00003015
Iteration 75/1000 | Loss: 0.00003015
Iteration 76/1000 | Loss: 0.00003015
Iteration 77/1000 | Loss: 0.00003015
Iteration 78/1000 | Loss: 0.00003015
Iteration 79/1000 | Loss: 0.00003015
Iteration 80/1000 | Loss: 0.00003015
Iteration 81/1000 | Loss: 0.00003015
Iteration 82/1000 | Loss: 0.00003015
Iteration 83/1000 | Loss: 0.00003015
Iteration 84/1000 | Loss: 0.00003015
Iteration 85/1000 | Loss: 0.00003015
Iteration 86/1000 | Loss: 0.00003014
Iteration 87/1000 | Loss: 0.00003014
Iteration 88/1000 | Loss: 0.00003014
Iteration 89/1000 | Loss: 0.00003014
Iteration 90/1000 | Loss: 0.00003014
Iteration 91/1000 | Loss: 0.00003014
Iteration 92/1000 | Loss: 0.00003014
Iteration 93/1000 | Loss: 0.00003014
Iteration 94/1000 | Loss: 0.00003014
Iteration 95/1000 | Loss: 0.00003014
Iteration 96/1000 | Loss: 0.00003014
Iteration 97/1000 | Loss: 0.00003014
Iteration 98/1000 | Loss: 0.00003014
Iteration 99/1000 | Loss: 0.00003014
Iteration 100/1000 | Loss: 0.00003013
Iteration 101/1000 | Loss: 0.00003013
Iteration 102/1000 | Loss: 0.00003013
Iteration 103/1000 | Loss: 0.00003013
Iteration 104/1000 | Loss: 0.00003013
Iteration 105/1000 | Loss: 0.00003013
Iteration 106/1000 | Loss: 0.00003013
Iteration 107/1000 | Loss: 0.00003013
Iteration 108/1000 | Loss: 0.00003013
Iteration 109/1000 | Loss: 0.00003012
Iteration 110/1000 | Loss: 0.00003012
Iteration 111/1000 | Loss: 0.00003011
Iteration 112/1000 | Loss: 0.00003011
Iteration 113/1000 | Loss: 0.00003011
Iteration 114/1000 | Loss: 0.00003010
Iteration 115/1000 | Loss: 0.00003010
Iteration 116/1000 | Loss: 0.00003010
Iteration 117/1000 | Loss: 0.00003010
Iteration 118/1000 | Loss: 0.00003010
Iteration 119/1000 | Loss: 0.00003010
Iteration 120/1000 | Loss: 0.00003010
Iteration 121/1000 | Loss: 0.00003010
Iteration 122/1000 | Loss: 0.00003009
Iteration 123/1000 | Loss: 0.00003009
Iteration 124/1000 | Loss: 0.00003009
Iteration 125/1000 | Loss: 0.00003009
Iteration 126/1000 | Loss: 0.00003009
Iteration 127/1000 | Loss: 0.00003009
Iteration 128/1000 | Loss: 0.00003009
Iteration 129/1000 | Loss: 0.00003009
Iteration 130/1000 | Loss: 0.00003009
Iteration 131/1000 | Loss: 0.00003008
Iteration 132/1000 | Loss: 0.00003008
Iteration 133/1000 | Loss: 0.00003008
Iteration 134/1000 | Loss: 0.00003008
Iteration 135/1000 | Loss: 0.00003008
Iteration 136/1000 | Loss: 0.00003008
Iteration 137/1000 | Loss: 0.00003008
Iteration 138/1000 | Loss: 0.00003008
Iteration 139/1000 | Loss: 0.00003008
Iteration 140/1000 | Loss: 0.00003008
Iteration 141/1000 | Loss: 0.00003008
Iteration 142/1000 | Loss: 0.00003008
Iteration 143/1000 | Loss: 0.00003008
Iteration 144/1000 | Loss: 0.00003008
Iteration 145/1000 | Loss: 0.00003008
Iteration 146/1000 | Loss: 0.00003008
Iteration 147/1000 | Loss: 0.00003008
Iteration 148/1000 | Loss: 0.00003008
Iteration 149/1000 | Loss: 0.00003008
Iteration 150/1000 | Loss: 0.00003008
Iteration 151/1000 | Loss: 0.00003008
Iteration 152/1000 | Loss: 0.00003008
Iteration 153/1000 | Loss: 0.00003008
Iteration 154/1000 | Loss: 0.00003008
Iteration 155/1000 | Loss: 0.00003008
Iteration 156/1000 | Loss: 0.00003008
Iteration 157/1000 | Loss: 0.00003008
Iteration 158/1000 | Loss: 0.00003008
Iteration 159/1000 | Loss: 0.00003008
Iteration 160/1000 | Loss: 0.00003008
Iteration 161/1000 | Loss: 0.00003008
Iteration 162/1000 | Loss: 0.00003008
Iteration 163/1000 | Loss: 0.00003008
Iteration 164/1000 | Loss: 0.00003008
Iteration 165/1000 | Loss: 0.00003008
Iteration 166/1000 | Loss: 0.00003008
Iteration 167/1000 | Loss: 0.00003008
Iteration 168/1000 | Loss: 0.00003008
Iteration 169/1000 | Loss: 0.00003008
Iteration 170/1000 | Loss: 0.00003008
Iteration 171/1000 | Loss: 0.00003008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [3.007565283041913e-05, 3.007565283041913e-05, 3.007565283041913e-05, 3.007565283041913e-05, 3.007565283041913e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.007565283041913e-05

Optimization complete. Final v2v error: 4.6262617111206055 mm

Highest mean error: 4.891608715057373 mm for frame 123

Lowest mean error: 4.31443977355957 mm for frame 38

Saving results

Total time: 34.516573429107666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00575115
Iteration 2/25 | Loss: 0.00100065
Iteration 3/25 | Loss: 0.00092484
Iteration 4/25 | Loss: 0.00091188
Iteration 5/25 | Loss: 0.00090780
Iteration 6/25 | Loss: 0.00090646
Iteration 7/25 | Loss: 0.00090646
Iteration 8/25 | Loss: 0.00090646
Iteration 9/25 | Loss: 0.00090646
Iteration 10/25 | Loss: 0.00090646
Iteration 11/25 | Loss: 0.00090646
Iteration 12/25 | Loss: 0.00090646
Iteration 13/25 | Loss: 0.00090646
Iteration 14/25 | Loss: 0.00090646
Iteration 15/25 | Loss: 0.00090646
Iteration 16/25 | Loss: 0.00090646
Iteration 17/25 | Loss: 0.00090646
Iteration 18/25 | Loss: 0.00090646
Iteration 19/25 | Loss: 0.00090646
Iteration 20/25 | Loss: 0.00090646
Iteration 21/25 | Loss: 0.00090646
Iteration 22/25 | Loss: 0.00090646
Iteration 23/25 | Loss: 0.00090646
Iteration 24/25 | Loss: 0.00090646
Iteration 25/25 | Loss: 0.00090646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69818568
Iteration 2/25 | Loss: 0.00126469
Iteration 3/25 | Loss: 0.00126469
Iteration 4/25 | Loss: 0.00126469
Iteration 5/25 | Loss: 0.00126469
Iteration 6/25 | Loss: 0.00126469
Iteration 7/25 | Loss: 0.00126469
Iteration 8/25 | Loss: 0.00126469
Iteration 9/25 | Loss: 0.00126469
Iteration 10/25 | Loss: 0.00126469
Iteration 11/25 | Loss: 0.00126469
Iteration 12/25 | Loss: 0.00126469
Iteration 13/25 | Loss: 0.00126469
Iteration 14/25 | Loss: 0.00126469
Iteration 15/25 | Loss: 0.00126469
Iteration 16/25 | Loss: 0.00126469
Iteration 17/25 | Loss: 0.00126469
Iteration 18/25 | Loss: 0.00126469
Iteration 19/25 | Loss: 0.00126469
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00126468890812248, 0.00126468890812248, 0.00126468890812248, 0.00126468890812248, 0.00126468890812248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00126468890812248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126469
Iteration 2/1000 | Loss: 0.00002375
Iteration 3/1000 | Loss: 0.00001147
Iteration 4/1000 | Loss: 0.00000950
Iteration 5/1000 | Loss: 0.00000867
Iteration 6/1000 | Loss: 0.00000815
Iteration 7/1000 | Loss: 0.00000803
Iteration 8/1000 | Loss: 0.00000785
Iteration 9/1000 | Loss: 0.00000770
Iteration 10/1000 | Loss: 0.00000763
Iteration 11/1000 | Loss: 0.00000763
Iteration 12/1000 | Loss: 0.00000763
Iteration 13/1000 | Loss: 0.00000763
Iteration 14/1000 | Loss: 0.00000762
Iteration 15/1000 | Loss: 0.00000762
Iteration 16/1000 | Loss: 0.00000762
Iteration 17/1000 | Loss: 0.00000762
Iteration 18/1000 | Loss: 0.00000762
Iteration 19/1000 | Loss: 0.00000761
Iteration 20/1000 | Loss: 0.00000761
Iteration 21/1000 | Loss: 0.00000760
Iteration 22/1000 | Loss: 0.00000759
Iteration 23/1000 | Loss: 0.00000759
Iteration 24/1000 | Loss: 0.00000758
Iteration 25/1000 | Loss: 0.00000758
Iteration 26/1000 | Loss: 0.00000758
Iteration 27/1000 | Loss: 0.00000758
Iteration 28/1000 | Loss: 0.00000757
Iteration 29/1000 | Loss: 0.00000757
Iteration 30/1000 | Loss: 0.00000757
Iteration 31/1000 | Loss: 0.00000757
Iteration 32/1000 | Loss: 0.00000757
Iteration 33/1000 | Loss: 0.00000757
Iteration 34/1000 | Loss: 0.00000757
Iteration 35/1000 | Loss: 0.00000757
Iteration 36/1000 | Loss: 0.00000757
Iteration 37/1000 | Loss: 0.00000756
Iteration 38/1000 | Loss: 0.00000755
Iteration 39/1000 | Loss: 0.00000755
Iteration 40/1000 | Loss: 0.00000754
Iteration 41/1000 | Loss: 0.00000754
Iteration 42/1000 | Loss: 0.00000754
Iteration 43/1000 | Loss: 0.00000753
Iteration 44/1000 | Loss: 0.00000753
Iteration 45/1000 | Loss: 0.00000752
Iteration 46/1000 | Loss: 0.00000752
Iteration 47/1000 | Loss: 0.00000751
Iteration 48/1000 | Loss: 0.00000751
Iteration 49/1000 | Loss: 0.00000750
Iteration 50/1000 | Loss: 0.00000750
Iteration 51/1000 | Loss: 0.00000750
Iteration 52/1000 | Loss: 0.00000749
Iteration 53/1000 | Loss: 0.00000749
Iteration 54/1000 | Loss: 0.00000749
Iteration 55/1000 | Loss: 0.00000749
Iteration 56/1000 | Loss: 0.00000749
Iteration 57/1000 | Loss: 0.00000749
Iteration 58/1000 | Loss: 0.00000749
Iteration 59/1000 | Loss: 0.00000748
Iteration 60/1000 | Loss: 0.00000748
Iteration 61/1000 | Loss: 0.00000748
Iteration 62/1000 | Loss: 0.00000748
Iteration 63/1000 | Loss: 0.00000748
Iteration 64/1000 | Loss: 0.00000748
Iteration 65/1000 | Loss: 0.00000748
Iteration 66/1000 | Loss: 0.00000748
Iteration 67/1000 | Loss: 0.00000748
Iteration 68/1000 | Loss: 0.00000747
Iteration 69/1000 | Loss: 0.00000747
Iteration 70/1000 | Loss: 0.00000747
Iteration 71/1000 | Loss: 0.00000747
Iteration 72/1000 | Loss: 0.00000747
Iteration 73/1000 | Loss: 0.00000747
Iteration 74/1000 | Loss: 0.00000747
Iteration 75/1000 | Loss: 0.00000747
Iteration 76/1000 | Loss: 0.00000747
Iteration 77/1000 | Loss: 0.00000747
Iteration 78/1000 | Loss: 0.00000747
Iteration 79/1000 | Loss: 0.00000747
Iteration 80/1000 | Loss: 0.00000747
Iteration 81/1000 | Loss: 0.00000747
Iteration 82/1000 | Loss: 0.00000747
Iteration 83/1000 | Loss: 0.00000746
Iteration 84/1000 | Loss: 0.00000746
Iteration 85/1000 | Loss: 0.00000746
Iteration 86/1000 | Loss: 0.00000746
Iteration 87/1000 | Loss: 0.00000746
Iteration 88/1000 | Loss: 0.00000746
Iteration 89/1000 | Loss: 0.00000746
Iteration 90/1000 | Loss: 0.00000746
Iteration 91/1000 | Loss: 0.00000746
Iteration 92/1000 | Loss: 0.00000746
Iteration 93/1000 | Loss: 0.00000746
Iteration 94/1000 | Loss: 0.00000745
Iteration 95/1000 | Loss: 0.00000745
Iteration 96/1000 | Loss: 0.00000745
Iteration 97/1000 | Loss: 0.00000745
Iteration 98/1000 | Loss: 0.00000745
Iteration 99/1000 | Loss: 0.00000745
Iteration 100/1000 | Loss: 0.00000745
Iteration 101/1000 | Loss: 0.00000745
Iteration 102/1000 | Loss: 0.00000745
Iteration 103/1000 | Loss: 0.00000745
Iteration 104/1000 | Loss: 0.00000745
Iteration 105/1000 | Loss: 0.00000745
Iteration 106/1000 | Loss: 0.00000745
Iteration 107/1000 | Loss: 0.00000745
Iteration 108/1000 | Loss: 0.00000745
Iteration 109/1000 | Loss: 0.00000745
Iteration 110/1000 | Loss: 0.00000744
Iteration 111/1000 | Loss: 0.00000744
Iteration 112/1000 | Loss: 0.00000744
Iteration 113/1000 | Loss: 0.00000744
Iteration 114/1000 | Loss: 0.00000744
Iteration 115/1000 | Loss: 0.00000744
Iteration 116/1000 | Loss: 0.00000744
Iteration 117/1000 | Loss: 0.00000744
Iteration 118/1000 | Loss: 0.00000744
Iteration 119/1000 | Loss: 0.00000744
Iteration 120/1000 | Loss: 0.00000744
Iteration 121/1000 | Loss: 0.00000744
Iteration 122/1000 | Loss: 0.00000744
Iteration 123/1000 | Loss: 0.00000743
Iteration 124/1000 | Loss: 0.00000743
Iteration 125/1000 | Loss: 0.00000743
Iteration 126/1000 | Loss: 0.00000743
Iteration 127/1000 | Loss: 0.00000743
Iteration 128/1000 | Loss: 0.00000743
Iteration 129/1000 | Loss: 0.00000742
Iteration 130/1000 | Loss: 0.00000742
Iteration 131/1000 | Loss: 0.00000742
Iteration 132/1000 | Loss: 0.00000742
Iteration 133/1000 | Loss: 0.00000742
Iteration 134/1000 | Loss: 0.00000742
Iteration 135/1000 | Loss: 0.00000742
Iteration 136/1000 | Loss: 0.00000742
Iteration 137/1000 | Loss: 0.00000742
Iteration 138/1000 | Loss: 0.00000741
Iteration 139/1000 | Loss: 0.00000741
Iteration 140/1000 | Loss: 0.00000741
Iteration 141/1000 | Loss: 0.00000741
Iteration 142/1000 | Loss: 0.00000741
Iteration 143/1000 | Loss: 0.00000741
Iteration 144/1000 | Loss: 0.00000741
Iteration 145/1000 | Loss: 0.00000741
Iteration 146/1000 | Loss: 0.00000741
Iteration 147/1000 | Loss: 0.00000741
Iteration 148/1000 | Loss: 0.00000741
Iteration 149/1000 | Loss: 0.00000741
Iteration 150/1000 | Loss: 0.00000740
Iteration 151/1000 | Loss: 0.00000740
Iteration 152/1000 | Loss: 0.00000740
Iteration 153/1000 | Loss: 0.00000740
Iteration 154/1000 | Loss: 0.00000739
Iteration 155/1000 | Loss: 0.00000739
Iteration 156/1000 | Loss: 0.00000739
Iteration 157/1000 | Loss: 0.00000739
Iteration 158/1000 | Loss: 0.00000739
Iteration 159/1000 | Loss: 0.00000739
Iteration 160/1000 | Loss: 0.00000739
Iteration 161/1000 | Loss: 0.00000738
Iteration 162/1000 | Loss: 0.00000738
Iteration 163/1000 | Loss: 0.00000738
Iteration 164/1000 | Loss: 0.00000738
Iteration 165/1000 | Loss: 0.00000738
Iteration 166/1000 | Loss: 0.00000738
Iteration 167/1000 | Loss: 0.00000738
Iteration 168/1000 | Loss: 0.00000738
Iteration 169/1000 | Loss: 0.00000738
Iteration 170/1000 | Loss: 0.00000738
Iteration 171/1000 | Loss: 0.00000738
Iteration 172/1000 | Loss: 0.00000738
Iteration 173/1000 | Loss: 0.00000738
Iteration 174/1000 | Loss: 0.00000738
Iteration 175/1000 | Loss: 0.00000738
Iteration 176/1000 | Loss: 0.00000737
Iteration 177/1000 | Loss: 0.00000737
Iteration 178/1000 | Loss: 0.00000737
Iteration 179/1000 | Loss: 0.00000737
Iteration 180/1000 | Loss: 0.00000737
Iteration 181/1000 | Loss: 0.00000737
Iteration 182/1000 | Loss: 0.00000737
Iteration 183/1000 | Loss: 0.00000737
Iteration 184/1000 | Loss: 0.00000737
Iteration 185/1000 | Loss: 0.00000737
Iteration 186/1000 | Loss: 0.00000737
Iteration 187/1000 | Loss: 0.00000737
Iteration 188/1000 | Loss: 0.00000736
Iteration 189/1000 | Loss: 0.00000736
Iteration 190/1000 | Loss: 0.00000736
Iteration 191/1000 | Loss: 0.00000736
Iteration 192/1000 | Loss: 0.00000736
Iteration 193/1000 | Loss: 0.00000736
Iteration 194/1000 | Loss: 0.00000736
Iteration 195/1000 | Loss: 0.00000735
Iteration 196/1000 | Loss: 0.00000735
Iteration 197/1000 | Loss: 0.00000735
Iteration 198/1000 | Loss: 0.00000735
Iteration 199/1000 | Loss: 0.00000735
Iteration 200/1000 | Loss: 0.00000735
Iteration 201/1000 | Loss: 0.00000735
Iteration 202/1000 | Loss: 0.00000735
Iteration 203/1000 | Loss: 0.00000735
Iteration 204/1000 | Loss: 0.00000735
Iteration 205/1000 | Loss: 0.00000734
Iteration 206/1000 | Loss: 0.00000734
Iteration 207/1000 | Loss: 0.00000734
Iteration 208/1000 | Loss: 0.00000734
Iteration 209/1000 | Loss: 0.00000734
Iteration 210/1000 | Loss: 0.00000734
Iteration 211/1000 | Loss: 0.00000734
Iteration 212/1000 | Loss: 0.00000734
Iteration 213/1000 | Loss: 0.00000734
Iteration 214/1000 | Loss: 0.00000734
Iteration 215/1000 | Loss: 0.00000734
Iteration 216/1000 | Loss: 0.00000734
Iteration 217/1000 | Loss: 0.00000734
Iteration 218/1000 | Loss: 0.00000734
Iteration 219/1000 | Loss: 0.00000734
Iteration 220/1000 | Loss: 0.00000734
Iteration 221/1000 | Loss: 0.00000733
Iteration 222/1000 | Loss: 0.00000733
Iteration 223/1000 | Loss: 0.00000733
Iteration 224/1000 | Loss: 0.00000733
Iteration 225/1000 | Loss: 0.00000733
Iteration 226/1000 | Loss: 0.00000733
Iteration 227/1000 | Loss: 0.00000733
Iteration 228/1000 | Loss: 0.00000733
Iteration 229/1000 | Loss: 0.00000733
Iteration 230/1000 | Loss: 0.00000733
Iteration 231/1000 | Loss: 0.00000733
Iteration 232/1000 | Loss: 0.00000733
Iteration 233/1000 | Loss: 0.00000733
Iteration 234/1000 | Loss: 0.00000733
Iteration 235/1000 | Loss: 0.00000733
Iteration 236/1000 | Loss: 0.00000733
Iteration 237/1000 | Loss: 0.00000733
Iteration 238/1000 | Loss: 0.00000732
Iteration 239/1000 | Loss: 0.00000732
Iteration 240/1000 | Loss: 0.00000732
Iteration 241/1000 | Loss: 0.00000732
Iteration 242/1000 | Loss: 0.00000732
Iteration 243/1000 | Loss: 0.00000732
Iteration 244/1000 | Loss: 0.00000732
Iteration 245/1000 | Loss: 0.00000732
Iteration 246/1000 | Loss: 0.00000732
Iteration 247/1000 | Loss: 0.00000732
Iteration 248/1000 | Loss: 0.00000732
Iteration 249/1000 | Loss: 0.00000732
Iteration 250/1000 | Loss: 0.00000731
Iteration 251/1000 | Loss: 0.00000731
Iteration 252/1000 | Loss: 0.00000731
Iteration 253/1000 | Loss: 0.00000731
Iteration 254/1000 | Loss: 0.00000731
Iteration 255/1000 | Loss: 0.00000731
Iteration 256/1000 | Loss: 0.00000731
Iteration 257/1000 | Loss: 0.00000731
Iteration 258/1000 | Loss: 0.00000731
Iteration 259/1000 | Loss: 0.00000731
Iteration 260/1000 | Loss: 0.00000731
Iteration 261/1000 | Loss: 0.00000731
Iteration 262/1000 | Loss: 0.00000731
Iteration 263/1000 | Loss: 0.00000731
Iteration 264/1000 | Loss: 0.00000731
Iteration 265/1000 | Loss: 0.00000731
Iteration 266/1000 | Loss: 0.00000731
Iteration 267/1000 | Loss: 0.00000731
Iteration 268/1000 | Loss: 0.00000731
Iteration 269/1000 | Loss: 0.00000731
Iteration 270/1000 | Loss: 0.00000731
Iteration 271/1000 | Loss: 0.00000731
Iteration 272/1000 | Loss: 0.00000731
Iteration 273/1000 | Loss: 0.00000731
Iteration 274/1000 | Loss: 0.00000731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [7.3058749876508955e-06, 7.3058749876508955e-06, 7.3058749876508955e-06, 7.3058749876508955e-06, 7.3058749876508955e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.3058749876508955e-06

Optimization complete. Final v2v error: 2.290058135986328 mm

Highest mean error: 2.6078221797943115 mm for frame 122

Lowest mean error: 2.0774033069610596 mm for frame 39

Saving results

Total time: 37.71608090400696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01091047
Iteration 2/25 | Loss: 0.00583550
Iteration 3/25 | Loss: 0.00376134
Iteration 4/25 | Loss: 0.00350017
Iteration 5/25 | Loss: 0.00219774
Iteration 6/25 | Loss: 0.00208602
Iteration 7/25 | Loss: 0.00199429
Iteration 8/25 | Loss: 0.00214950
Iteration 9/25 | Loss: 0.00197362
Iteration 10/25 | Loss: 0.00153272
Iteration 11/25 | Loss: 0.00137774
Iteration 12/25 | Loss: 0.00135000
Iteration 13/25 | Loss: 0.00130725
Iteration 14/25 | Loss: 0.00129083
Iteration 15/25 | Loss: 0.00126252
Iteration 16/25 | Loss: 0.00125888
Iteration 17/25 | Loss: 0.00125436
Iteration 18/25 | Loss: 0.00124998
Iteration 19/25 | Loss: 0.00124975
Iteration 20/25 | Loss: 0.00124965
Iteration 21/25 | Loss: 0.00124963
Iteration 22/25 | Loss: 0.00124963
Iteration 23/25 | Loss: 0.00124962
Iteration 24/25 | Loss: 0.00124962
Iteration 25/25 | Loss: 0.00124962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.41742930
Iteration 2/25 | Loss: 0.00076378
Iteration 3/25 | Loss: 0.00076378
Iteration 4/25 | Loss: 0.00070762
Iteration 5/25 | Loss: 0.00070761
Iteration 6/25 | Loss: 0.00070761
Iteration 7/25 | Loss: 0.00070761
Iteration 8/25 | Loss: 0.00070761
Iteration 9/25 | Loss: 0.00070761
Iteration 10/25 | Loss: 0.00070761
Iteration 11/25 | Loss: 0.00070761
Iteration 12/25 | Loss: 0.00070761
Iteration 13/25 | Loss: 0.00070761
Iteration 14/25 | Loss: 0.00070761
Iteration 15/25 | Loss: 0.00070761
Iteration 16/25 | Loss: 0.00070761
Iteration 17/25 | Loss: 0.00070761
Iteration 18/25 | Loss: 0.00070761
Iteration 19/25 | Loss: 0.00070761
Iteration 20/25 | Loss: 0.00070761
Iteration 21/25 | Loss: 0.00070761
Iteration 22/25 | Loss: 0.00070761
Iteration 23/25 | Loss: 0.00070761
Iteration 24/25 | Loss: 0.00070761
Iteration 25/25 | Loss: 0.00070761

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070761
Iteration 2/1000 | Loss: 0.00077084
Iteration 3/1000 | Loss: 0.00007183
Iteration 4/1000 | Loss: 0.00006046
Iteration 5/1000 | Loss: 0.00005494
Iteration 6/1000 | Loss: 0.00005458
Iteration 7/1000 | Loss: 0.00022800
Iteration 8/1000 | Loss: 0.00143442
Iteration 9/1000 | Loss: 0.00045227
Iteration 10/1000 | Loss: 0.00014226
Iteration 11/1000 | Loss: 0.00007323
Iteration 12/1000 | Loss: 0.00005527
Iteration 13/1000 | Loss: 0.00005040
Iteration 14/1000 | Loss: 0.00004565
Iteration 15/1000 | Loss: 0.00005182
Iteration 16/1000 | Loss: 0.00004106
Iteration 17/1000 | Loss: 0.00004192
Iteration 18/1000 | Loss: 0.00003821
Iteration 19/1000 | Loss: 0.00003822
Iteration 20/1000 | Loss: 0.00003663
Iteration 21/1000 | Loss: 0.00003714
Iteration 22/1000 | Loss: 0.00003610
Iteration 23/1000 | Loss: 0.00003569
Iteration 24/1000 | Loss: 0.00003675
Iteration 25/1000 | Loss: 0.00003519
Iteration 26/1000 | Loss: 0.00003722
Iteration 27/1000 | Loss: 0.00003594
Iteration 28/1000 | Loss: 0.00003488
Iteration 29/1000 | Loss: 0.00003484
Iteration 30/1000 | Loss: 0.00003483
Iteration 31/1000 | Loss: 0.00003483
Iteration 32/1000 | Loss: 0.00003530
Iteration 33/1000 | Loss: 0.00003506
Iteration 34/1000 | Loss: 0.00003471
Iteration 35/1000 | Loss: 0.00003465
Iteration 36/1000 | Loss: 0.00003464
Iteration 37/1000 | Loss: 0.00003464
Iteration 38/1000 | Loss: 0.00003464
Iteration 39/1000 | Loss: 0.00003464
Iteration 40/1000 | Loss: 0.00003502
Iteration 41/1000 | Loss: 0.00003448
Iteration 42/1000 | Loss: 0.00003448
Iteration 43/1000 | Loss: 0.00003448
Iteration 44/1000 | Loss: 0.00003447
Iteration 45/1000 | Loss: 0.00003436
Iteration 46/1000 | Loss: 0.00003723
Iteration 47/1000 | Loss: 0.00003539
Iteration 48/1000 | Loss: 0.00003428
Iteration 49/1000 | Loss: 0.00003428
Iteration 50/1000 | Loss: 0.00003697
Iteration 51/1000 | Loss: 0.00003697
Iteration 52/1000 | Loss: 0.00003646
Iteration 53/1000 | Loss: 0.00003514
Iteration 54/1000 | Loss: 0.00003443
Iteration 55/1000 | Loss: 0.00003401
Iteration 56/1000 | Loss: 0.00003401
Iteration 57/1000 | Loss: 0.00003400
Iteration 58/1000 | Loss: 0.00003400
Iteration 59/1000 | Loss: 0.00003400
Iteration 60/1000 | Loss: 0.00003400
Iteration 61/1000 | Loss: 0.00003400
Iteration 62/1000 | Loss: 0.00003400
Iteration 63/1000 | Loss: 0.00003400
Iteration 64/1000 | Loss: 0.00003400
Iteration 65/1000 | Loss: 0.00003400
Iteration 66/1000 | Loss: 0.00003400
Iteration 67/1000 | Loss: 0.00003400
Iteration 68/1000 | Loss: 0.00003410
Iteration 69/1000 | Loss: 0.00003496
Iteration 70/1000 | Loss: 0.00004065
Iteration 71/1000 | Loss: 0.00003610
Iteration 72/1000 | Loss: 0.00003623
Iteration 73/1000 | Loss: 0.00003380
Iteration 74/1000 | Loss: 0.00003405
Iteration 75/1000 | Loss: 0.00003371
Iteration 76/1000 | Loss: 0.00003371
Iteration 77/1000 | Loss: 0.00003371
Iteration 78/1000 | Loss: 0.00003371
Iteration 79/1000 | Loss: 0.00003371
Iteration 80/1000 | Loss: 0.00003370
Iteration 81/1000 | Loss: 0.00003370
Iteration 82/1000 | Loss: 0.00003370
Iteration 83/1000 | Loss: 0.00003370
Iteration 84/1000 | Loss: 0.00003370
Iteration 85/1000 | Loss: 0.00003370
Iteration 86/1000 | Loss: 0.00003370
Iteration 87/1000 | Loss: 0.00003370
Iteration 88/1000 | Loss: 0.00003370
Iteration 89/1000 | Loss: 0.00003370
Iteration 90/1000 | Loss: 0.00003370
Iteration 91/1000 | Loss: 0.00003369
Iteration 92/1000 | Loss: 0.00003360
Iteration 93/1000 | Loss: 0.00003612
Iteration 94/1000 | Loss: 0.00003389
Iteration 95/1000 | Loss: 0.00003390
Iteration 96/1000 | Loss: 0.00003357
Iteration 97/1000 | Loss: 0.00003357
Iteration 98/1000 | Loss: 0.00003357
Iteration 99/1000 | Loss: 0.00003357
Iteration 100/1000 | Loss: 0.00003357
Iteration 101/1000 | Loss: 0.00003356
Iteration 102/1000 | Loss: 0.00003356
Iteration 103/1000 | Loss: 0.00003355
Iteration 104/1000 | Loss: 0.00003355
Iteration 105/1000 | Loss: 0.00003355
Iteration 106/1000 | Loss: 0.00003354
Iteration 107/1000 | Loss: 0.00003354
Iteration 108/1000 | Loss: 0.00003354
Iteration 109/1000 | Loss: 0.00003354
Iteration 110/1000 | Loss: 0.00003354
Iteration 111/1000 | Loss: 0.00003354
Iteration 112/1000 | Loss: 0.00003354
Iteration 113/1000 | Loss: 0.00003354
Iteration 114/1000 | Loss: 0.00003354
Iteration 115/1000 | Loss: 0.00003354
Iteration 116/1000 | Loss: 0.00003354
Iteration 117/1000 | Loss: 0.00003354
Iteration 118/1000 | Loss: 0.00003354
Iteration 119/1000 | Loss: 0.00003354
Iteration 120/1000 | Loss: 0.00003354
Iteration 121/1000 | Loss: 0.00003354
Iteration 122/1000 | Loss: 0.00003354
Iteration 123/1000 | Loss: 0.00003354
Iteration 124/1000 | Loss: 0.00003354
Iteration 125/1000 | Loss: 0.00003354
Iteration 126/1000 | Loss: 0.00003354
Iteration 127/1000 | Loss: 0.00003354
Iteration 128/1000 | Loss: 0.00003353
Iteration 129/1000 | Loss: 0.00003353
Iteration 130/1000 | Loss: 0.00003353
Iteration 131/1000 | Loss: 0.00003353
Iteration 132/1000 | Loss: 0.00003353
Iteration 133/1000 | Loss: 0.00003353
Iteration 134/1000 | Loss: 0.00003353
Iteration 135/1000 | Loss: 0.00003352
Iteration 136/1000 | Loss: 0.00003352
Iteration 137/1000 | Loss: 0.00003352
Iteration 138/1000 | Loss: 0.00003692
Iteration 139/1000 | Loss: 0.00003350
Iteration 140/1000 | Loss: 0.00003437
Iteration 141/1000 | Loss: 0.00003350
Iteration 142/1000 | Loss: 0.00003350
Iteration 143/1000 | Loss: 0.00003350
Iteration 144/1000 | Loss: 0.00003350
Iteration 145/1000 | Loss: 0.00003349
Iteration 146/1000 | Loss: 0.00003349
Iteration 147/1000 | Loss: 0.00003348
Iteration 148/1000 | Loss: 0.00003348
Iteration 149/1000 | Loss: 0.00003348
Iteration 150/1000 | Loss: 0.00003348
Iteration 151/1000 | Loss: 0.00003348
Iteration 152/1000 | Loss: 0.00003348
Iteration 153/1000 | Loss: 0.00003348
Iteration 154/1000 | Loss: 0.00003348
Iteration 155/1000 | Loss: 0.00003348
Iteration 156/1000 | Loss: 0.00003348
Iteration 157/1000 | Loss: 0.00003348
Iteration 158/1000 | Loss: 0.00003348
Iteration 159/1000 | Loss: 0.00003348
Iteration 160/1000 | Loss: 0.00003348
Iteration 161/1000 | Loss: 0.00003348
Iteration 162/1000 | Loss: 0.00003347
Iteration 163/1000 | Loss: 0.00003347
Iteration 164/1000 | Loss: 0.00003347
Iteration 165/1000 | Loss: 0.00003347
Iteration 166/1000 | Loss: 0.00003347
Iteration 167/1000 | Loss: 0.00003347
Iteration 168/1000 | Loss: 0.00003347
Iteration 169/1000 | Loss: 0.00003347
Iteration 170/1000 | Loss: 0.00003347
Iteration 171/1000 | Loss: 0.00003347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [3.347371966810897e-05, 3.347371966810897e-05, 3.347371966810897e-05, 3.347371966810897e-05, 3.347371966810897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.347371966810897e-05

Optimization complete. Final v2v error: 4.648722171783447 mm

Highest mean error: 5.105724334716797 mm for frame 59

Lowest mean error: 4.014382362365723 mm for frame 9

Saving results

Total time: 114.2609052658081
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00620411
Iteration 2/25 | Loss: 0.00144418
Iteration 3/25 | Loss: 0.00127469
Iteration 4/25 | Loss: 0.00125484
Iteration 5/25 | Loss: 0.00119848
Iteration 6/25 | Loss: 0.00118279
Iteration 7/25 | Loss: 0.00115393
Iteration 8/25 | Loss: 0.00114279
Iteration 9/25 | Loss: 0.00113440
Iteration 10/25 | Loss: 0.00112674
Iteration 11/25 | Loss: 0.00112422
Iteration 12/25 | Loss: 0.00112335
Iteration 13/25 | Loss: 0.00112325
Iteration 14/25 | Loss: 0.00112323
Iteration 15/25 | Loss: 0.00112323
Iteration 16/25 | Loss: 0.00112323
Iteration 17/25 | Loss: 0.00112323
Iteration 18/25 | Loss: 0.00112323
Iteration 19/25 | Loss: 0.00112323
Iteration 20/25 | Loss: 0.00112323
Iteration 21/25 | Loss: 0.00112323
Iteration 22/25 | Loss: 0.00112323
Iteration 23/25 | Loss: 0.00112322
Iteration 24/25 | Loss: 0.00112322
Iteration 25/25 | Loss: 0.00112322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19726670
Iteration 2/25 | Loss: 0.00174097
Iteration 3/25 | Loss: 0.00174088
Iteration 4/25 | Loss: 0.00174088
Iteration 5/25 | Loss: 0.00174088
Iteration 6/25 | Loss: 0.00174088
Iteration 7/25 | Loss: 0.00174088
Iteration 8/25 | Loss: 0.00174088
Iteration 9/25 | Loss: 0.00174088
Iteration 10/25 | Loss: 0.00174088
Iteration 11/25 | Loss: 0.00174088
Iteration 12/25 | Loss: 0.00174088
Iteration 13/25 | Loss: 0.00174088
Iteration 14/25 | Loss: 0.00174088
Iteration 15/25 | Loss: 0.00174088
Iteration 16/25 | Loss: 0.00174088
Iteration 17/25 | Loss: 0.00174088
Iteration 18/25 | Loss: 0.00174088
Iteration 19/25 | Loss: 0.00174088
Iteration 20/25 | Loss: 0.00174088
Iteration 21/25 | Loss: 0.00174088
Iteration 22/25 | Loss: 0.00174088
Iteration 23/25 | Loss: 0.00174088
Iteration 24/25 | Loss: 0.00174088
Iteration 25/25 | Loss: 0.00174088
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001740877516567707, 0.001740877516567707, 0.001740877516567707, 0.001740877516567707, 0.001740877516567707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001740877516567707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174088
Iteration 2/1000 | Loss: 0.00015334
Iteration 3/1000 | Loss: 0.00007211
Iteration 4/1000 | Loss: 0.00004871
Iteration 5/1000 | Loss: 0.00003776
Iteration 6/1000 | Loss: 0.00003389
Iteration 7/1000 | Loss: 0.00003204
Iteration 8/1000 | Loss: 0.00003062
Iteration 9/1000 | Loss: 0.00002961
Iteration 10/1000 | Loss: 0.00002907
Iteration 11/1000 | Loss: 0.00002861
Iteration 12/1000 | Loss: 0.00002829
Iteration 13/1000 | Loss: 0.00002794
Iteration 14/1000 | Loss: 0.00002761
Iteration 15/1000 | Loss: 0.00002737
Iteration 16/1000 | Loss: 0.00002717
Iteration 17/1000 | Loss: 0.00002705
Iteration 18/1000 | Loss: 0.00002703
Iteration 19/1000 | Loss: 0.00002696
Iteration 20/1000 | Loss: 0.00002692
Iteration 21/1000 | Loss: 0.00002690
Iteration 22/1000 | Loss: 0.00002689
Iteration 23/1000 | Loss: 0.00002682
Iteration 24/1000 | Loss: 0.00002678
Iteration 25/1000 | Loss: 0.00002677
Iteration 26/1000 | Loss: 0.00002676
Iteration 27/1000 | Loss: 0.00002675
Iteration 28/1000 | Loss: 0.00002672
Iteration 29/1000 | Loss: 0.00002667
Iteration 30/1000 | Loss: 0.00002665
Iteration 31/1000 | Loss: 0.00002665
Iteration 32/1000 | Loss: 0.00002664
Iteration 33/1000 | Loss: 0.00002662
Iteration 34/1000 | Loss: 0.00002660
Iteration 35/1000 | Loss: 0.00002659
Iteration 36/1000 | Loss: 0.00002659
Iteration 37/1000 | Loss: 0.00002655
Iteration 38/1000 | Loss: 0.00002655
Iteration 39/1000 | Loss: 0.00002654
Iteration 40/1000 | Loss: 0.00002653
Iteration 41/1000 | Loss: 0.00002653
Iteration 42/1000 | Loss: 0.00002652
Iteration 43/1000 | Loss: 0.00002652
Iteration 44/1000 | Loss: 0.00002652
Iteration 45/1000 | Loss: 0.00002652
Iteration 46/1000 | Loss: 0.00002652
Iteration 47/1000 | Loss: 0.00002652
Iteration 48/1000 | Loss: 0.00002652
Iteration 49/1000 | Loss: 0.00002651
Iteration 50/1000 | Loss: 0.00002651
Iteration 51/1000 | Loss: 0.00002651
Iteration 52/1000 | Loss: 0.00002651
Iteration 53/1000 | Loss: 0.00002651
Iteration 54/1000 | Loss: 0.00002651
Iteration 55/1000 | Loss: 0.00002650
Iteration 56/1000 | Loss: 0.00002649
Iteration 57/1000 | Loss: 0.00002648
Iteration 58/1000 | Loss: 0.00002648
Iteration 59/1000 | Loss: 0.00002648
Iteration 60/1000 | Loss: 0.00002647
Iteration 61/1000 | Loss: 0.00002647
Iteration 62/1000 | Loss: 0.00002647
Iteration 63/1000 | Loss: 0.00002646
Iteration 64/1000 | Loss: 0.00002646
Iteration 65/1000 | Loss: 0.00002646
Iteration 66/1000 | Loss: 0.00002646
Iteration 67/1000 | Loss: 0.00002645
Iteration 68/1000 | Loss: 0.00002645
Iteration 69/1000 | Loss: 0.00002645
Iteration 70/1000 | Loss: 0.00002645
Iteration 71/1000 | Loss: 0.00002645
Iteration 72/1000 | Loss: 0.00002645
Iteration 73/1000 | Loss: 0.00002645
Iteration 74/1000 | Loss: 0.00002645
Iteration 75/1000 | Loss: 0.00002645
Iteration 76/1000 | Loss: 0.00002644
Iteration 77/1000 | Loss: 0.00002644
Iteration 78/1000 | Loss: 0.00002643
Iteration 79/1000 | Loss: 0.00002643
Iteration 80/1000 | Loss: 0.00002643
Iteration 81/1000 | Loss: 0.00002643
Iteration 82/1000 | Loss: 0.00002643
Iteration 83/1000 | Loss: 0.00002642
Iteration 84/1000 | Loss: 0.00002642
Iteration 85/1000 | Loss: 0.00002642
Iteration 86/1000 | Loss: 0.00002642
Iteration 87/1000 | Loss: 0.00002642
Iteration 88/1000 | Loss: 0.00002642
Iteration 89/1000 | Loss: 0.00002642
Iteration 90/1000 | Loss: 0.00002642
Iteration 91/1000 | Loss: 0.00002642
Iteration 92/1000 | Loss: 0.00002642
Iteration 93/1000 | Loss: 0.00002641
Iteration 94/1000 | Loss: 0.00002641
Iteration 95/1000 | Loss: 0.00002640
Iteration 96/1000 | Loss: 0.00002640
Iteration 97/1000 | Loss: 0.00002640
Iteration 98/1000 | Loss: 0.00002639
Iteration 99/1000 | Loss: 0.00002639
Iteration 100/1000 | Loss: 0.00002639
Iteration 101/1000 | Loss: 0.00002639
Iteration 102/1000 | Loss: 0.00002639
Iteration 103/1000 | Loss: 0.00002639
Iteration 104/1000 | Loss: 0.00002639
Iteration 105/1000 | Loss: 0.00002638
Iteration 106/1000 | Loss: 0.00002638
Iteration 107/1000 | Loss: 0.00002638
Iteration 108/1000 | Loss: 0.00002638
Iteration 109/1000 | Loss: 0.00002638
Iteration 110/1000 | Loss: 0.00002638
Iteration 111/1000 | Loss: 0.00002638
Iteration 112/1000 | Loss: 0.00002638
Iteration 113/1000 | Loss: 0.00002638
Iteration 114/1000 | Loss: 0.00002638
Iteration 115/1000 | Loss: 0.00002638
Iteration 116/1000 | Loss: 0.00002638
Iteration 117/1000 | Loss: 0.00002638
Iteration 118/1000 | Loss: 0.00002638
Iteration 119/1000 | Loss: 0.00002638
Iteration 120/1000 | Loss: 0.00002638
Iteration 121/1000 | Loss: 0.00002638
Iteration 122/1000 | Loss: 0.00002638
Iteration 123/1000 | Loss: 0.00002638
Iteration 124/1000 | Loss: 0.00002638
Iteration 125/1000 | Loss: 0.00002638
Iteration 126/1000 | Loss: 0.00002638
Iteration 127/1000 | Loss: 0.00002638
Iteration 128/1000 | Loss: 0.00002638
Iteration 129/1000 | Loss: 0.00002638
Iteration 130/1000 | Loss: 0.00002638
Iteration 131/1000 | Loss: 0.00002638
Iteration 132/1000 | Loss: 0.00002638
Iteration 133/1000 | Loss: 0.00002638
Iteration 134/1000 | Loss: 0.00002638
Iteration 135/1000 | Loss: 0.00002638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.6380126655567437e-05, 2.6380126655567437e-05, 2.6380126655567437e-05, 2.6380126655567437e-05, 2.6380126655567437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6380126655567437e-05

Optimization complete. Final v2v error: 4.081748008728027 mm

Highest mean error: 5.879377365112305 mm for frame 139

Lowest mean error: 3.280142307281494 mm for frame 150

Saving results

Total time: 59.55005884170532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858544
Iteration 2/25 | Loss: 0.00106615
Iteration 3/25 | Loss: 0.00094424
Iteration 4/25 | Loss: 0.00093435
Iteration 5/25 | Loss: 0.00093250
Iteration 6/25 | Loss: 0.00093208
Iteration 7/25 | Loss: 0.00093208
Iteration 8/25 | Loss: 0.00093208
Iteration 9/25 | Loss: 0.00093208
Iteration 10/25 | Loss: 0.00093208
Iteration 11/25 | Loss: 0.00093208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009320759563706815, 0.0009320759563706815, 0.0009320759563706815, 0.0009320759563706815, 0.0009320759563706815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009320759563706815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26983976
Iteration 2/25 | Loss: 0.00114549
Iteration 3/25 | Loss: 0.00114548
Iteration 4/25 | Loss: 0.00114548
Iteration 5/25 | Loss: 0.00114548
Iteration 6/25 | Loss: 0.00114548
Iteration 7/25 | Loss: 0.00114548
Iteration 8/25 | Loss: 0.00114547
Iteration 9/25 | Loss: 0.00114547
Iteration 10/25 | Loss: 0.00114547
Iteration 11/25 | Loss: 0.00114547
Iteration 12/25 | Loss: 0.00114547
Iteration 13/25 | Loss: 0.00114547
Iteration 14/25 | Loss: 0.00114547
Iteration 15/25 | Loss: 0.00114547
Iteration 16/25 | Loss: 0.00114547
Iteration 17/25 | Loss: 0.00114547
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001145474030636251, 0.001145474030636251, 0.001145474030636251, 0.001145474030636251, 0.001145474030636251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001145474030636251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114547
Iteration 2/1000 | Loss: 0.00002418
Iteration 3/1000 | Loss: 0.00001468
Iteration 4/1000 | Loss: 0.00001302
Iteration 5/1000 | Loss: 0.00001188
Iteration 6/1000 | Loss: 0.00001113
Iteration 7/1000 | Loss: 0.00001077
Iteration 8/1000 | Loss: 0.00001058
Iteration 9/1000 | Loss: 0.00001040
Iteration 10/1000 | Loss: 0.00001037
Iteration 11/1000 | Loss: 0.00001036
Iteration 12/1000 | Loss: 0.00001028
Iteration 13/1000 | Loss: 0.00001026
Iteration 14/1000 | Loss: 0.00001021
Iteration 15/1000 | Loss: 0.00001020
Iteration 16/1000 | Loss: 0.00001020
Iteration 17/1000 | Loss: 0.00001020
Iteration 18/1000 | Loss: 0.00001019
Iteration 19/1000 | Loss: 0.00001018
Iteration 20/1000 | Loss: 0.00001016
Iteration 21/1000 | Loss: 0.00001015
Iteration 22/1000 | Loss: 0.00001015
Iteration 23/1000 | Loss: 0.00001014
Iteration 24/1000 | Loss: 0.00001012
Iteration 25/1000 | Loss: 0.00001011
Iteration 26/1000 | Loss: 0.00001009
Iteration 27/1000 | Loss: 0.00000999
Iteration 28/1000 | Loss: 0.00000999
Iteration 29/1000 | Loss: 0.00000998
Iteration 30/1000 | Loss: 0.00000997
Iteration 31/1000 | Loss: 0.00000996
Iteration 32/1000 | Loss: 0.00000996
Iteration 33/1000 | Loss: 0.00000995
Iteration 34/1000 | Loss: 0.00000994
Iteration 35/1000 | Loss: 0.00000994
Iteration 36/1000 | Loss: 0.00000994
Iteration 37/1000 | Loss: 0.00000993
Iteration 38/1000 | Loss: 0.00000993
Iteration 39/1000 | Loss: 0.00000992
Iteration 40/1000 | Loss: 0.00000992
Iteration 41/1000 | Loss: 0.00000991
Iteration 42/1000 | Loss: 0.00000991
Iteration 43/1000 | Loss: 0.00000991
Iteration 44/1000 | Loss: 0.00000990
Iteration 45/1000 | Loss: 0.00000989
Iteration 46/1000 | Loss: 0.00000989
Iteration 47/1000 | Loss: 0.00000988
Iteration 48/1000 | Loss: 0.00000988
Iteration 49/1000 | Loss: 0.00000987
Iteration 50/1000 | Loss: 0.00000987
Iteration 51/1000 | Loss: 0.00000986
Iteration 52/1000 | Loss: 0.00000986
Iteration 53/1000 | Loss: 0.00000985
Iteration 54/1000 | Loss: 0.00000985
Iteration 55/1000 | Loss: 0.00000985
Iteration 56/1000 | Loss: 0.00000984
Iteration 57/1000 | Loss: 0.00000984
Iteration 58/1000 | Loss: 0.00000984
Iteration 59/1000 | Loss: 0.00000984
Iteration 60/1000 | Loss: 0.00000984
Iteration 61/1000 | Loss: 0.00000984
Iteration 62/1000 | Loss: 0.00000984
Iteration 63/1000 | Loss: 0.00000984
Iteration 64/1000 | Loss: 0.00000983
Iteration 65/1000 | Loss: 0.00000983
Iteration 66/1000 | Loss: 0.00000983
Iteration 67/1000 | Loss: 0.00000982
Iteration 68/1000 | Loss: 0.00000982
Iteration 69/1000 | Loss: 0.00000982
Iteration 70/1000 | Loss: 0.00000982
Iteration 71/1000 | Loss: 0.00000982
Iteration 72/1000 | Loss: 0.00000981
Iteration 73/1000 | Loss: 0.00000981
Iteration 74/1000 | Loss: 0.00000980
Iteration 75/1000 | Loss: 0.00000980
Iteration 76/1000 | Loss: 0.00000980
Iteration 77/1000 | Loss: 0.00000980
Iteration 78/1000 | Loss: 0.00000980
Iteration 79/1000 | Loss: 0.00000980
Iteration 80/1000 | Loss: 0.00000980
Iteration 81/1000 | Loss: 0.00000980
Iteration 82/1000 | Loss: 0.00000980
Iteration 83/1000 | Loss: 0.00000979
Iteration 84/1000 | Loss: 0.00000979
Iteration 85/1000 | Loss: 0.00000979
Iteration 86/1000 | Loss: 0.00000979
Iteration 87/1000 | Loss: 0.00000979
Iteration 88/1000 | Loss: 0.00000979
Iteration 89/1000 | Loss: 0.00000978
Iteration 90/1000 | Loss: 0.00000978
Iteration 91/1000 | Loss: 0.00000978
Iteration 92/1000 | Loss: 0.00000978
Iteration 93/1000 | Loss: 0.00000977
Iteration 94/1000 | Loss: 0.00000977
Iteration 95/1000 | Loss: 0.00000977
Iteration 96/1000 | Loss: 0.00000977
Iteration 97/1000 | Loss: 0.00000977
Iteration 98/1000 | Loss: 0.00000977
Iteration 99/1000 | Loss: 0.00000977
Iteration 100/1000 | Loss: 0.00000977
Iteration 101/1000 | Loss: 0.00000977
Iteration 102/1000 | Loss: 0.00000977
Iteration 103/1000 | Loss: 0.00000976
Iteration 104/1000 | Loss: 0.00000976
Iteration 105/1000 | Loss: 0.00000975
Iteration 106/1000 | Loss: 0.00000975
Iteration 107/1000 | Loss: 0.00000975
Iteration 108/1000 | Loss: 0.00000975
Iteration 109/1000 | Loss: 0.00000975
Iteration 110/1000 | Loss: 0.00000975
Iteration 111/1000 | Loss: 0.00000975
Iteration 112/1000 | Loss: 0.00000975
Iteration 113/1000 | Loss: 0.00000974
Iteration 114/1000 | Loss: 0.00000974
Iteration 115/1000 | Loss: 0.00000974
Iteration 116/1000 | Loss: 0.00000974
Iteration 117/1000 | Loss: 0.00000974
Iteration 118/1000 | Loss: 0.00000973
Iteration 119/1000 | Loss: 0.00000973
Iteration 120/1000 | Loss: 0.00000973
Iteration 121/1000 | Loss: 0.00000973
Iteration 122/1000 | Loss: 0.00000973
Iteration 123/1000 | Loss: 0.00000973
Iteration 124/1000 | Loss: 0.00000973
Iteration 125/1000 | Loss: 0.00000973
Iteration 126/1000 | Loss: 0.00000973
Iteration 127/1000 | Loss: 0.00000973
Iteration 128/1000 | Loss: 0.00000973
Iteration 129/1000 | Loss: 0.00000973
Iteration 130/1000 | Loss: 0.00000972
Iteration 131/1000 | Loss: 0.00000972
Iteration 132/1000 | Loss: 0.00000971
Iteration 133/1000 | Loss: 0.00000971
Iteration 134/1000 | Loss: 0.00000971
Iteration 135/1000 | Loss: 0.00000971
Iteration 136/1000 | Loss: 0.00000971
Iteration 137/1000 | Loss: 0.00000971
Iteration 138/1000 | Loss: 0.00000971
Iteration 139/1000 | Loss: 0.00000971
Iteration 140/1000 | Loss: 0.00000971
Iteration 141/1000 | Loss: 0.00000971
Iteration 142/1000 | Loss: 0.00000971
Iteration 143/1000 | Loss: 0.00000970
Iteration 144/1000 | Loss: 0.00000970
Iteration 145/1000 | Loss: 0.00000970
Iteration 146/1000 | Loss: 0.00000970
Iteration 147/1000 | Loss: 0.00000970
Iteration 148/1000 | Loss: 0.00000970
Iteration 149/1000 | Loss: 0.00000970
Iteration 150/1000 | Loss: 0.00000970
Iteration 151/1000 | Loss: 0.00000970
Iteration 152/1000 | Loss: 0.00000970
Iteration 153/1000 | Loss: 0.00000970
Iteration 154/1000 | Loss: 0.00000970
Iteration 155/1000 | Loss: 0.00000969
Iteration 156/1000 | Loss: 0.00000969
Iteration 157/1000 | Loss: 0.00000969
Iteration 158/1000 | Loss: 0.00000969
Iteration 159/1000 | Loss: 0.00000969
Iteration 160/1000 | Loss: 0.00000969
Iteration 161/1000 | Loss: 0.00000969
Iteration 162/1000 | Loss: 0.00000969
Iteration 163/1000 | Loss: 0.00000969
Iteration 164/1000 | Loss: 0.00000969
Iteration 165/1000 | Loss: 0.00000968
Iteration 166/1000 | Loss: 0.00000968
Iteration 167/1000 | Loss: 0.00000968
Iteration 168/1000 | Loss: 0.00000968
Iteration 169/1000 | Loss: 0.00000968
Iteration 170/1000 | Loss: 0.00000968
Iteration 171/1000 | Loss: 0.00000968
Iteration 172/1000 | Loss: 0.00000968
Iteration 173/1000 | Loss: 0.00000968
Iteration 174/1000 | Loss: 0.00000968
Iteration 175/1000 | Loss: 0.00000968
Iteration 176/1000 | Loss: 0.00000968
Iteration 177/1000 | Loss: 0.00000967
Iteration 178/1000 | Loss: 0.00000967
Iteration 179/1000 | Loss: 0.00000967
Iteration 180/1000 | Loss: 0.00000967
Iteration 181/1000 | Loss: 0.00000967
Iteration 182/1000 | Loss: 0.00000966
Iteration 183/1000 | Loss: 0.00000966
Iteration 184/1000 | Loss: 0.00000966
Iteration 185/1000 | Loss: 0.00000966
Iteration 186/1000 | Loss: 0.00000966
Iteration 187/1000 | Loss: 0.00000966
Iteration 188/1000 | Loss: 0.00000966
Iteration 189/1000 | Loss: 0.00000966
Iteration 190/1000 | Loss: 0.00000966
Iteration 191/1000 | Loss: 0.00000966
Iteration 192/1000 | Loss: 0.00000966
Iteration 193/1000 | Loss: 0.00000966
Iteration 194/1000 | Loss: 0.00000966
Iteration 195/1000 | Loss: 0.00000966
Iteration 196/1000 | Loss: 0.00000965
Iteration 197/1000 | Loss: 0.00000965
Iteration 198/1000 | Loss: 0.00000965
Iteration 199/1000 | Loss: 0.00000965
Iteration 200/1000 | Loss: 0.00000965
Iteration 201/1000 | Loss: 0.00000965
Iteration 202/1000 | Loss: 0.00000965
Iteration 203/1000 | Loss: 0.00000965
Iteration 204/1000 | Loss: 0.00000965
Iteration 205/1000 | Loss: 0.00000965
Iteration 206/1000 | Loss: 0.00000965
Iteration 207/1000 | Loss: 0.00000965
Iteration 208/1000 | Loss: 0.00000965
Iteration 209/1000 | Loss: 0.00000965
Iteration 210/1000 | Loss: 0.00000965
Iteration 211/1000 | Loss: 0.00000965
Iteration 212/1000 | Loss: 0.00000965
Iteration 213/1000 | Loss: 0.00000964
Iteration 214/1000 | Loss: 0.00000964
Iteration 215/1000 | Loss: 0.00000964
Iteration 216/1000 | Loss: 0.00000964
Iteration 217/1000 | Loss: 0.00000964
Iteration 218/1000 | Loss: 0.00000964
Iteration 219/1000 | Loss: 0.00000964
Iteration 220/1000 | Loss: 0.00000964
Iteration 221/1000 | Loss: 0.00000964
Iteration 222/1000 | Loss: 0.00000964
Iteration 223/1000 | Loss: 0.00000964
Iteration 224/1000 | Loss: 0.00000964
Iteration 225/1000 | Loss: 0.00000964
Iteration 226/1000 | Loss: 0.00000964
Iteration 227/1000 | Loss: 0.00000964
Iteration 228/1000 | Loss: 0.00000964
Iteration 229/1000 | Loss: 0.00000963
Iteration 230/1000 | Loss: 0.00000963
Iteration 231/1000 | Loss: 0.00000963
Iteration 232/1000 | Loss: 0.00000963
Iteration 233/1000 | Loss: 0.00000963
Iteration 234/1000 | Loss: 0.00000963
Iteration 235/1000 | Loss: 0.00000963
Iteration 236/1000 | Loss: 0.00000963
Iteration 237/1000 | Loss: 0.00000963
Iteration 238/1000 | Loss: 0.00000963
Iteration 239/1000 | Loss: 0.00000963
Iteration 240/1000 | Loss: 0.00000963
Iteration 241/1000 | Loss: 0.00000962
Iteration 242/1000 | Loss: 0.00000962
Iteration 243/1000 | Loss: 0.00000962
Iteration 244/1000 | Loss: 0.00000962
Iteration 245/1000 | Loss: 0.00000962
Iteration 246/1000 | Loss: 0.00000962
Iteration 247/1000 | Loss: 0.00000962
Iteration 248/1000 | Loss: 0.00000962
Iteration 249/1000 | Loss: 0.00000962
Iteration 250/1000 | Loss: 0.00000962
Iteration 251/1000 | Loss: 0.00000962
Iteration 252/1000 | Loss: 0.00000962
Iteration 253/1000 | Loss: 0.00000962
Iteration 254/1000 | Loss: 0.00000962
Iteration 255/1000 | Loss: 0.00000962
Iteration 256/1000 | Loss: 0.00000962
Iteration 257/1000 | Loss: 0.00000961
Iteration 258/1000 | Loss: 0.00000961
Iteration 259/1000 | Loss: 0.00000961
Iteration 260/1000 | Loss: 0.00000961
Iteration 261/1000 | Loss: 0.00000961
Iteration 262/1000 | Loss: 0.00000961
Iteration 263/1000 | Loss: 0.00000961
Iteration 264/1000 | Loss: 0.00000961
Iteration 265/1000 | Loss: 0.00000961
Iteration 266/1000 | Loss: 0.00000961
Iteration 267/1000 | Loss: 0.00000961
Iteration 268/1000 | Loss: 0.00000961
Iteration 269/1000 | Loss: 0.00000961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [9.610754204913974e-06, 9.610754204913974e-06, 9.610754204913974e-06, 9.610754204913974e-06, 9.610754204913974e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.610754204913974e-06

Optimization complete. Final v2v error: 2.622112989425659 mm

Highest mean error: 2.9050583839416504 mm for frame 30

Lowest mean error: 2.355224609375 mm for frame 82

Saving results

Total time: 40.0675265789032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894449
Iteration 2/25 | Loss: 0.00111106
Iteration 3/25 | Loss: 0.00101787
Iteration 4/25 | Loss: 0.00100271
Iteration 5/25 | Loss: 0.00099764
Iteration 6/25 | Loss: 0.00099636
Iteration 7/25 | Loss: 0.00099636
Iteration 8/25 | Loss: 0.00099636
Iteration 9/25 | Loss: 0.00099636
Iteration 10/25 | Loss: 0.00099636
Iteration 11/25 | Loss: 0.00099636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009963578777387738, 0.0009963578777387738, 0.0009963578777387738, 0.0009963578777387738, 0.0009963578777387738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009963578777387738

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25168455
Iteration 2/25 | Loss: 0.00146630
Iteration 3/25 | Loss: 0.00146628
Iteration 4/25 | Loss: 0.00146628
Iteration 5/25 | Loss: 0.00146628
Iteration 6/25 | Loss: 0.00146628
Iteration 7/25 | Loss: 0.00146628
Iteration 8/25 | Loss: 0.00146628
Iteration 9/25 | Loss: 0.00146628
Iteration 10/25 | Loss: 0.00146628
Iteration 11/25 | Loss: 0.00146628
Iteration 12/25 | Loss: 0.00146628
Iteration 13/25 | Loss: 0.00146628
Iteration 14/25 | Loss: 0.00146628
Iteration 15/25 | Loss: 0.00146628
Iteration 16/25 | Loss: 0.00146628
Iteration 17/25 | Loss: 0.00146628
Iteration 18/25 | Loss: 0.00146628
Iteration 19/25 | Loss: 0.00146628
Iteration 20/25 | Loss: 0.00146628
Iteration 21/25 | Loss: 0.00146628
Iteration 22/25 | Loss: 0.00146628
Iteration 23/25 | Loss: 0.00146628
Iteration 24/25 | Loss: 0.00146628
Iteration 25/25 | Loss: 0.00146628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146628
Iteration 2/1000 | Loss: 0.00004114
Iteration 3/1000 | Loss: 0.00002336
Iteration 4/1000 | Loss: 0.00001790
Iteration 5/1000 | Loss: 0.00001635
Iteration 6/1000 | Loss: 0.00001541
Iteration 7/1000 | Loss: 0.00001478
Iteration 8/1000 | Loss: 0.00001429
Iteration 9/1000 | Loss: 0.00001398
Iteration 10/1000 | Loss: 0.00001394
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001360
Iteration 13/1000 | Loss: 0.00001350
Iteration 14/1000 | Loss: 0.00001341
Iteration 15/1000 | Loss: 0.00001339
Iteration 16/1000 | Loss: 0.00001329
Iteration 17/1000 | Loss: 0.00001321
Iteration 18/1000 | Loss: 0.00001319
Iteration 19/1000 | Loss: 0.00001318
Iteration 20/1000 | Loss: 0.00001318
Iteration 21/1000 | Loss: 0.00001317
Iteration 22/1000 | Loss: 0.00001317
Iteration 23/1000 | Loss: 0.00001317
Iteration 24/1000 | Loss: 0.00001317
Iteration 25/1000 | Loss: 0.00001316
Iteration 26/1000 | Loss: 0.00001316
Iteration 27/1000 | Loss: 0.00001315
Iteration 28/1000 | Loss: 0.00001315
Iteration 29/1000 | Loss: 0.00001314
Iteration 30/1000 | Loss: 0.00001314
Iteration 31/1000 | Loss: 0.00001313
Iteration 32/1000 | Loss: 0.00001313
Iteration 33/1000 | Loss: 0.00001313
Iteration 34/1000 | Loss: 0.00001312
Iteration 35/1000 | Loss: 0.00001312
Iteration 36/1000 | Loss: 0.00001311
Iteration 37/1000 | Loss: 0.00001311
Iteration 38/1000 | Loss: 0.00001310
Iteration 39/1000 | Loss: 0.00001310
Iteration 40/1000 | Loss: 0.00001310
Iteration 41/1000 | Loss: 0.00001309
Iteration 42/1000 | Loss: 0.00001309
Iteration 43/1000 | Loss: 0.00001309
Iteration 44/1000 | Loss: 0.00001309
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001308
Iteration 48/1000 | Loss: 0.00001308
Iteration 49/1000 | Loss: 0.00001308
Iteration 50/1000 | Loss: 0.00001308
Iteration 51/1000 | Loss: 0.00001308
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001307
Iteration 55/1000 | Loss: 0.00001307
Iteration 56/1000 | Loss: 0.00001307
Iteration 57/1000 | Loss: 0.00001307
Iteration 58/1000 | Loss: 0.00001307
Iteration 59/1000 | Loss: 0.00001307
Iteration 60/1000 | Loss: 0.00001306
Iteration 61/1000 | Loss: 0.00001306
Iteration 62/1000 | Loss: 0.00001306
Iteration 63/1000 | Loss: 0.00001306
Iteration 64/1000 | Loss: 0.00001306
Iteration 65/1000 | Loss: 0.00001305
Iteration 66/1000 | Loss: 0.00001305
Iteration 67/1000 | Loss: 0.00001304
Iteration 68/1000 | Loss: 0.00001304
Iteration 69/1000 | Loss: 0.00001304
Iteration 70/1000 | Loss: 0.00001304
Iteration 71/1000 | Loss: 0.00001303
Iteration 72/1000 | Loss: 0.00001303
Iteration 73/1000 | Loss: 0.00001303
Iteration 74/1000 | Loss: 0.00001303
Iteration 75/1000 | Loss: 0.00001303
Iteration 76/1000 | Loss: 0.00001303
Iteration 77/1000 | Loss: 0.00001303
Iteration 78/1000 | Loss: 0.00001303
Iteration 79/1000 | Loss: 0.00001303
Iteration 80/1000 | Loss: 0.00001303
Iteration 81/1000 | Loss: 0.00001303
Iteration 82/1000 | Loss: 0.00001303
Iteration 83/1000 | Loss: 0.00001303
Iteration 84/1000 | Loss: 0.00001303
Iteration 85/1000 | Loss: 0.00001303
Iteration 86/1000 | Loss: 0.00001303
Iteration 87/1000 | Loss: 0.00001303
Iteration 88/1000 | Loss: 0.00001303
Iteration 89/1000 | Loss: 0.00001303
Iteration 90/1000 | Loss: 0.00001303
Iteration 91/1000 | Loss: 0.00001303
Iteration 92/1000 | Loss: 0.00001303
Iteration 93/1000 | Loss: 0.00001303
Iteration 94/1000 | Loss: 0.00001303
Iteration 95/1000 | Loss: 0.00001303
Iteration 96/1000 | Loss: 0.00001303
Iteration 97/1000 | Loss: 0.00001303
Iteration 98/1000 | Loss: 0.00001303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.3026121450820938e-05, 1.3026121450820938e-05, 1.3026121450820938e-05, 1.3026121450820938e-05, 1.3026121450820938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3026121450820938e-05

Optimization complete. Final v2v error: 2.944964647293091 mm

Highest mean error: 4.0956130027771 mm for frame 35

Lowest mean error: 2.3315932750701904 mm for frame 68

Saving results

Total time: 33.721800804138184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00517088
Iteration 2/25 | Loss: 0.00112905
Iteration 3/25 | Loss: 0.00099400
Iteration 4/25 | Loss: 0.00098186
Iteration 5/25 | Loss: 0.00098007
Iteration 6/25 | Loss: 0.00098001
Iteration 7/25 | Loss: 0.00098001
Iteration 8/25 | Loss: 0.00098001
Iteration 9/25 | Loss: 0.00098001
Iteration 10/25 | Loss: 0.00098001
Iteration 11/25 | Loss: 0.00098001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00098000792786479, 0.00098000792786479, 0.00098000792786479, 0.00098000792786479, 0.00098000792786479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00098000792786479

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75944567
Iteration 2/25 | Loss: 0.00051376
Iteration 3/25 | Loss: 0.00051375
Iteration 4/25 | Loss: 0.00051375
Iteration 5/25 | Loss: 0.00051375
Iteration 6/25 | Loss: 0.00051375
Iteration 7/25 | Loss: 0.00051375
Iteration 8/25 | Loss: 0.00051375
Iteration 9/25 | Loss: 0.00051375
Iteration 10/25 | Loss: 0.00051375
Iteration 11/25 | Loss: 0.00051375
Iteration 12/25 | Loss: 0.00051375
Iteration 13/25 | Loss: 0.00051375
Iteration 14/25 | Loss: 0.00051375
Iteration 15/25 | Loss: 0.00051375
Iteration 16/25 | Loss: 0.00051375
Iteration 17/25 | Loss: 0.00051375
Iteration 18/25 | Loss: 0.00051375
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005137475091032684, 0.0005137475091032684, 0.0005137475091032684, 0.0005137475091032684, 0.0005137475091032684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005137475091032684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051375
Iteration 2/1000 | Loss: 0.00002691
Iteration 3/1000 | Loss: 0.00001934
Iteration 4/1000 | Loss: 0.00001695
Iteration 5/1000 | Loss: 0.00001588
Iteration 6/1000 | Loss: 0.00001534
Iteration 7/1000 | Loss: 0.00001501
Iteration 8/1000 | Loss: 0.00001490
Iteration 9/1000 | Loss: 0.00001490
Iteration 10/1000 | Loss: 0.00001489
Iteration 11/1000 | Loss: 0.00001489
Iteration 12/1000 | Loss: 0.00001489
Iteration 13/1000 | Loss: 0.00001489
Iteration 14/1000 | Loss: 0.00001489
Iteration 15/1000 | Loss: 0.00001489
Iteration 16/1000 | Loss: 0.00001489
Iteration 17/1000 | Loss: 0.00001489
Iteration 18/1000 | Loss: 0.00001489
Iteration 19/1000 | Loss: 0.00001489
Iteration 20/1000 | Loss: 0.00001489
Iteration 21/1000 | Loss: 0.00001489
Iteration 22/1000 | Loss: 0.00001488
Iteration 23/1000 | Loss: 0.00001488
Iteration 24/1000 | Loss: 0.00001486
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001470
Iteration 27/1000 | Loss: 0.00001467
Iteration 28/1000 | Loss: 0.00001467
Iteration 29/1000 | Loss: 0.00001467
Iteration 30/1000 | Loss: 0.00001466
Iteration 31/1000 | Loss: 0.00001466
Iteration 32/1000 | Loss: 0.00001464
Iteration 33/1000 | Loss: 0.00001464
Iteration 34/1000 | Loss: 0.00001464
Iteration 35/1000 | Loss: 0.00001463
Iteration 36/1000 | Loss: 0.00001463
Iteration 37/1000 | Loss: 0.00001463
Iteration 38/1000 | Loss: 0.00001463
Iteration 39/1000 | Loss: 0.00001463
Iteration 40/1000 | Loss: 0.00001463
Iteration 41/1000 | Loss: 0.00001462
Iteration 42/1000 | Loss: 0.00001462
Iteration 43/1000 | Loss: 0.00001456
Iteration 44/1000 | Loss: 0.00001454
Iteration 45/1000 | Loss: 0.00001454
Iteration 46/1000 | Loss: 0.00001454
Iteration 47/1000 | Loss: 0.00001453
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001453
Iteration 50/1000 | Loss: 0.00001453
Iteration 51/1000 | Loss: 0.00001453
Iteration 52/1000 | Loss: 0.00001452
Iteration 53/1000 | Loss: 0.00001452
Iteration 54/1000 | Loss: 0.00001452
Iteration 55/1000 | Loss: 0.00001452
Iteration 56/1000 | Loss: 0.00001451
Iteration 57/1000 | Loss: 0.00001451
Iteration 58/1000 | Loss: 0.00001451
Iteration 59/1000 | Loss: 0.00001451
Iteration 60/1000 | Loss: 0.00001451
Iteration 61/1000 | Loss: 0.00001451
Iteration 62/1000 | Loss: 0.00001450
Iteration 63/1000 | Loss: 0.00001449
Iteration 64/1000 | Loss: 0.00001449
Iteration 65/1000 | Loss: 0.00001449
Iteration 66/1000 | Loss: 0.00001449
Iteration 67/1000 | Loss: 0.00001448
Iteration 68/1000 | Loss: 0.00001448
Iteration 69/1000 | Loss: 0.00001448
Iteration 70/1000 | Loss: 0.00001447
Iteration 71/1000 | Loss: 0.00001447
Iteration 72/1000 | Loss: 0.00001447
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001447
Iteration 75/1000 | Loss: 0.00001445
Iteration 76/1000 | Loss: 0.00001445
Iteration 77/1000 | Loss: 0.00001445
Iteration 78/1000 | Loss: 0.00001445
Iteration 79/1000 | Loss: 0.00001445
Iteration 80/1000 | Loss: 0.00001445
Iteration 81/1000 | Loss: 0.00001445
Iteration 82/1000 | Loss: 0.00001445
Iteration 83/1000 | Loss: 0.00001445
Iteration 84/1000 | Loss: 0.00001445
Iteration 85/1000 | Loss: 0.00001445
Iteration 86/1000 | Loss: 0.00001444
Iteration 87/1000 | Loss: 0.00001444
Iteration 88/1000 | Loss: 0.00001443
Iteration 89/1000 | Loss: 0.00001443
Iteration 90/1000 | Loss: 0.00001443
Iteration 91/1000 | Loss: 0.00001443
Iteration 92/1000 | Loss: 0.00001443
Iteration 93/1000 | Loss: 0.00001443
Iteration 94/1000 | Loss: 0.00001443
Iteration 95/1000 | Loss: 0.00001442
Iteration 96/1000 | Loss: 0.00001442
Iteration 97/1000 | Loss: 0.00001442
Iteration 98/1000 | Loss: 0.00001442
Iteration 99/1000 | Loss: 0.00001442
Iteration 100/1000 | Loss: 0.00001441
Iteration 101/1000 | Loss: 0.00001441
Iteration 102/1000 | Loss: 0.00001441
Iteration 103/1000 | Loss: 0.00001441
Iteration 104/1000 | Loss: 0.00001441
Iteration 105/1000 | Loss: 0.00001441
Iteration 106/1000 | Loss: 0.00001441
Iteration 107/1000 | Loss: 0.00001441
Iteration 108/1000 | Loss: 0.00001441
Iteration 109/1000 | Loss: 0.00001441
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001441
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001441
Iteration 115/1000 | Loss: 0.00001441
Iteration 116/1000 | Loss: 0.00001441
Iteration 117/1000 | Loss: 0.00001441
Iteration 118/1000 | Loss: 0.00001441
Iteration 119/1000 | Loss: 0.00001441
Iteration 120/1000 | Loss: 0.00001441
Iteration 121/1000 | Loss: 0.00001441
Iteration 122/1000 | Loss: 0.00001441
Iteration 123/1000 | Loss: 0.00001441
Iteration 124/1000 | Loss: 0.00001441
Iteration 125/1000 | Loss: 0.00001441
Iteration 126/1000 | Loss: 0.00001441
Iteration 127/1000 | Loss: 0.00001441
Iteration 128/1000 | Loss: 0.00001441
Iteration 129/1000 | Loss: 0.00001441
Iteration 130/1000 | Loss: 0.00001441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.4412888049264438e-05, 1.4412888049264438e-05, 1.4412888049264438e-05, 1.4412888049264438e-05, 1.4412888049264438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4412888049264438e-05

Optimization complete. Final v2v error: 3.1337385177612305 mm

Highest mean error: 3.339669942855835 mm for frame 46

Lowest mean error: 2.954209089279175 mm for frame 168

Saving results

Total time: 28.73373055458069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976239
Iteration 2/25 | Loss: 0.00179413
Iteration 3/25 | Loss: 0.00176124
Iteration 4/25 | Loss: 0.00121288
Iteration 5/25 | Loss: 0.00118769
Iteration 6/25 | Loss: 0.00118288
Iteration 7/25 | Loss: 0.00118159
Iteration 8/25 | Loss: 0.00118117
Iteration 9/25 | Loss: 0.00118100
Iteration 10/25 | Loss: 0.00118091
Iteration 11/25 | Loss: 0.00118090
Iteration 12/25 | Loss: 0.00118090
Iteration 13/25 | Loss: 0.00118090
Iteration 14/25 | Loss: 0.00118090
Iteration 15/25 | Loss: 0.00118090
Iteration 16/25 | Loss: 0.00118090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001180902007035911, 0.001180902007035911, 0.001180902007035911, 0.001180902007035911, 0.001180902007035911]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001180902007035911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16155732
Iteration 2/25 | Loss: 0.00094874
Iteration 3/25 | Loss: 0.00094874
Iteration 4/25 | Loss: 0.00094874
Iteration 5/25 | Loss: 0.00094873
Iteration 6/25 | Loss: 0.00094873
Iteration 7/25 | Loss: 0.00094873
Iteration 8/25 | Loss: 0.00094873
Iteration 9/25 | Loss: 0.00094873
Iteration 10/25 | Loss: 0.00094873
Iteration 11/25 | Loss: 0.00094873
Iteration 12/25 | Loss: 0.00094873
Iteration 13/25 | Loss: 0.00094873
Iteration 14/25 | Loss: 0.00094873
Iteration 15/25 | Loss: 0.00094873
Iteration 16/25 | Loss: 0.00094873
Iteration 17/25 | Loss: 0.00094873
Iteration 18/25 | Loss: 0.00094873
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009487331844866276, 0.0009487331844866276, 0.0009487331844866276, 0.0009487331844866276, 0.0009487331844866276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009487331844866276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094873
Iteration 2/1000 | Loss: 0.00007872
Iteration 3/1000 | Loss: 0.00005069
Iteration 4/1000 | Loss: 0.00003780
Iteration 5/1000 | Loss: 0.00036872
Iteration 6/1000 | Loss: 0.00003371
Iteration 7/1000 | Loss: 0.00003225
Iteration 8/1000 | Loss: 0.00003141
Iteration 9/1000 | Loss: 0.00003067
Iteration 10/1000 | Loss: 0.00003017
Iteration 11/1000 | Loss: 0.00002995
Iteration 12/1000 | Loss: 0.00002971
Iteration 13/1000 | Loss: 0.00002971
Iteration 14/1000 | Loss: 0.00002952
Iteration 15/1000 | Loss: 0.00002935
Iteration 16/1000 | Loss: 0.00002931
Iteration 17/1000 | Loss: 0.00029989
Iteration 18/1000 | Loss: 0.00092328
Iteration 19/1000 | Loss: 0.00004678
Iteration 20/1000 | Loss: 0.00002931
Iteration 21/1000 | Loss: 0.00002908
Iteration 22/1000 | Loss: 0.00002904
Iteration 23/1000 | Loss: 0.00002902
Iteration 24/1000 | Loss: 0.00002898
Iteration 25/1000 | Loss: 0.00002897
Iteration 26/1000 | Loss: 0.00002896
Iteration 27/1000 | Loss: 0.00002896
Iteration 28/1000 | Loss: 0.00030379
Iteration 29/1000 | Loss: 0.00042951
Iteration 30/1000 | Loss: 0.00005885
Iteration 31/1000 | Loss: 0.00031173
Iteration 32/1000 | Loss: 0.00002985
Iteration 33/1000 | Loss: 0.00002912
Iteration 34/1000 | Loss: 0.00002898
Iteration 35/1000 | Loss: 0.00002895
Iteration 36/1000 | Loss: 0.00002895
Iteration 37/1000 | Loss: 0.00002894
Iteration 38/1000 | Loss: 0.00002894
Iteration 39/1000 | Loss: 0.00002893
Iteration 40/1000 | Loss: 0.00002893
Iteration 41/1000 | Loss: 0.00002893
Iteration 42/1000 | Loss: 0.00002893
Iteration 43/1000 | Loss: 0.00002892
Iteration 44/1000 | Loss: 0.00002892
Iteration 45/1000 | Loss: 0.00002892
Iteration 46/1000 | Loss: 0.00002891
Iteration 47/1000 | Loss: 0.00002888
Iteration 48/1000 | Loss: 0.00002887
Iteration 49/1000 | Loss: 0.00002886
Iteration 50/1000 | Loss: 0.00002885
Iteration 51/1000 | Loss: 0.00002885
Iteration 52/1000 | Loss: 0.00002882
Iteration 53/1000 | Loss: 0.00002881
Iteration 54/1000 | Loss: 0.00002881
Iteration 55/1000 | Loss: 0.00002881
Iteration 56/1000 | Loss: 0.00002881
Iteration 57/1000 | Loss: 0.00002881
Iteration 58/1000 | Loss: 0.00002881
Iteration 59/1000 | Loss: 0.00002881
Iteration 60/1000 | Loss: 0.00002881
Iteration 61/1000 | Loss: 0.00002881
Iteration 62/1000 | Loss: 0.00002881
Iteration 63/1000 | Loss: 0.00002881
Iteration 64/1000 | Loss: 0.00002881
Iteration 65/1000 | Loss: 0.00002881
Iteration 66/1000 | Loss: 0.00002881
Iteration 67/1000 | Loss: 0.00002881
Iteration 68/1000 | Loss: 0.00002881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [2.8807004127884284e-05, 2.8807004127884284e-05, 2.8807004127884284e-05, 2.8807004127884284e-05, 2.8807004127884284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8807004127884284e-05

Optimization complete. Final v2v error: 4.520368576049805 mm

Highest mean error: 12.214776992797852 mm for frame 32

Lowest mean error: 3.982259511947632 mm for frame 227

Saving results

Total time: 64.41900873184204
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928089
Iteration 2/25 | Loss: 0.00146653
Iteration 3/25 | Loss: 0.00115726
Iteration 4/25 | Loss: 0.00112130
Iteration 5/25 | Loss: 0.00111748
Iteration 6/25 | Loss: 0.00110803
Iteration 7/25 | Loss: 0.00109771
Iteration 8/25 | Loss: 0.00108828
Iteration 9/25 | Loss: 0.00108306
Iteration 10/25 | Loss: 0.00108124
Iteration 11/25 | Loss: 0.00108080
Iteration 12/25 | Loss: 0.00108075
Iteration 13/25 | Loss: 0.00108075
Iteration 14/25 | Loss: 0.00108075
Iteration 15/25 | Loss: 0.00108074
Iteration 16/25 | Loss: 0.00108074
Iteration 17/25 | Loss: 0.00108074
Iteration 18/25 | Loss: 0.00108074
Iteration 19/25 | Loss: 0.00108074
Iteration 20/25 | Loss: 0.00108074
Iteration 21/25 | Loss: 0.00108074
Iteration 22/25 | Loss: 0.00108074
Iteration 23/25 | Loss: 0.00108074
Iteration 24/25 | Loss: 0.00108074
Iteration 25/25 | Loss: 0.00108074

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27276087
Iteration 2/25 | Loss: 0.00089071
Iteration 3/25 | Loss: 0.00089070
Iteration 4/25 | Loss: 0.00089070
Iteration 5/25 | Loss: 0.00089070
Iteration 6/25 | Loss: 0.00089070
Iteration 7/25 | Loss: 0.00089070
Iteration 8/25 | Loss: 0.00089070
Iteration 9/25 | Loss: 0.00089070
Iteration 10/25 | Loss: 0.00089070
Iteration 11/25 | Loss: 0.00089070
Iteration 12/25 | Loss: 0.00089070
Iteration 13/25 | Loss: 0.00089070
Iteration 14/25 | Loss: 0.00089070
Iteration 15/25 | Loss: 0.00089070
Iteration 16/25 | Loss: 0.00089070
Iteration 17/25 | Loss: 0.00089070
Iteration 18/25 | Loss: 0.00089070
Iteration 19/25 | Loss: 0.00089070
Iteration 20/25 | Loss: 0.00089070
Iteration 21/25 | Loss: 0.00089070
Iteration 22/25 | Loss: 0.00089070
Iteration 23/25 | Loss: 0.00089070
Iteration 24/25 | Loss: 0.00089070
Iteration 25/25 | Loss: 0.00089070

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089070
Iteration 2/1000 | Loss: 0.00006187
Iteration 3/1000 | Loss: 0.00018934
Iteration 4/1000 | Loss: 0.00004765
Iteration 5/1000 | Loss: 0.00003650
Iteration 6/1000 | Loss: 0.00003226
Iteration 7/1000 | Loss: 0.00002965
Iteration 8/1000 | Loss: 0.00034460
Iteration 9/1000 | Loss: 0.00003067
Iteration 10/1000 | Loss: 0.00002843
Iteration 11/1000 | Loss: 0.00002681
Iteration 12/1000 | Loss: 0.00002542
Iteration 13/1000 | Loss: 0.00002482
Iteration 14/1000 | Loss: 0.00002452
Iteration 15/1000 | Loss: 0.00002428
Iteration 16/1000 | Loss: 0.00002412
Iteration 17/1000 | Loss: 0.00002401
Iteration 18/1000 | Loss: 0.00002400
Iteration 19/1000 | Loss: 0.00002392
Iteration 20/1000 | Loss: 0.00002390
Iteration 21/1000 | Loss: 0.00002387
Iteration 22/1000 | Loss: 0.00002383
Iteration 23/1000 | Loss: 0.00002383
Iteration 24/1000 | Loss: 0.00002383
Iteration 25/1000 | Loss: 0.00002382
Iteration 26/1000 | Loss: 0.00002381
Iteration 27/1000 | Loss: 0.00002380
Iteration 28/1000 | Loss: 0.00002376
Iteration 29/1000 | Loss: 0.00002375
Iteration 30/1000 | Loss: 0.00002374
Iteration 31/1000 | Loss: 0.00002374
Iteration 32/1000 | Loss: 0.00002374
Iteration 33/1000 | Loss: 0.00002373
Iteration 34/1000 | Loss: 0.00002373
Iteration 35/1000 | Loss: 0.00002373
Iteration 36/1000 | Loss: 0.00002373
Iteration 37/1000 | Loss: 0.00002372
Iteration 38/1000 | Loss: 0.00002372
Iteration 39/1000 | Loss: 0.00002372
Iteration 40/1000 | Loss: 0.00002371
Iteration 41/1000 | Loss: 0.00002371
Iteration 42/1000 | Loss: 0.00002369
Iteration 43/1000 | Loss: 0.00002368
Iteration 44/1000 | Loss: 0.00002367
Iteration 45/1000 | Loss: 0.00002365
Iteration 46/1000 | Loss: 0.00002365
Iteration 47/1000 | Loss: 0.00002365
Iteration 48/1000 | Loss: 0.00002364
Iteration 49/1000 | Loss: 0.00002364
Iteration 50/1000 | Loss: 0.00002364
Iteration 51/1000 | Loss: 0.00002364
Iteration 52/1000 | Loss: 0.00002364
Iteration 53/1000 | Loss: 0.00002361
Iteration 54/1000 | Loss: 0.00002361
Iteration 55/1000 | Loss: 0.00002360
Iteration 56/1000 | Loss: 0.00002360
Iteration 57/1000 | Loss: 0.00002360
Iteration 58/1000 | Loss: 0.00002359
Iteration 59/1000 | Loss: 0.00002359
Iteration 60/1000 | Loss: 0.00002359
Iteration 61/1000 | Loss: 0.00002359
Iteration 62/1000 | Loss: 0.00002359
Iteration 63/1000 | Loss: 0.00002358
Iteration 64/1000 | Loss: 0.00002358
Iteration 65/1000 | Loss: 0.00002357
Iteration 66/1000 | Loss: 0.00002357
Iteration 67/1000 | Loss: 0.00002356
Iteration 68/1000 | Loss: 0.00002356
Iteration 69/1000 | Loss: 0.00002356
Iteration 70/1000 | Loss: 0.00002356
Iteration 71/1000 | Loss: 0.00002355
Iteration 72/1000 | Loss: 0.00002355
Iteration 73/1000 | Loss: 0.00002353
Iteration 74/1000 | Loss: 0.00002353
Iteration 75/1000 | Loss: 0.00002353
Iteration 76/1000 | Loss: 0.00002352
Iteration 77/1000 | Loss: 0.00002352
Iteration 78/1000 | Loss: 0.00002352
Iteration 79/1000 | Loss: 0.00002352
Iteration 80/1000 | Loss: 0.00002352
Iteration 81/1000 | Loss: 0.00002351
Iteration 82/1000 | Loss: 0.00002351
Iteration 83/1000 | Loss: 0.00002351
Iteration 84/1000 | Loss: 0.00002351
Iteration 85/1000 | Loss: 0.00002351
Iteration 86/1000 | Loss: 0.00002351
Iteration 87/1000 | Loss: 0.00002351
Iteration 88/1000 | Loss: 0.00002351
Iteration 89/1000 | Loss: 0.00002351
Iteration 90/1000 | Loss: 0.00002350
Iteration 91/1000 | Loss: 0.00002350
Iteration 92/1000 | Loss: 0.00002350
Iteration 93/1000 | Loss: 0.00002350
Iteration 94/1000 | Loss: 0.00002350
Iteration 95/1000 | Loss: 0.00002350
Iteration 96/1000 | Loss: 0.00002350
Iteration 97/1000 | Loss: 0.00002350
Iteration 98/1000 | Loss: 0.00002349
Iteration 99/1000 | Loss: 0.00002349
Iteration 100/1000 | Loss: 0.00002349
Iteration 101/1000 | Loss: 0.00002349
Iteration 102/1000 | Loss: 0.00002349
Iteration 103/1000 | Loss: 0.00002349
Iteration 104/1000 | Loss: 0.00002348
Iteration 105/1000 | Loss: 0.00002348
Iteration 106/1000 | Loss: 0.00002348
Iteration 107/1000 | Loss: 0.00002348
Iteration 108/1000 | Loss: 0.00002347
Iteration 109/1000 | Loss: 0.00002347
Iteration 110/1000 | Loss: 0.00002347
Iteration 111/1000 | Loss: 0.00002347
Iteration 112/1000 | Loss: 0.00002347
Iteration 113/1000 | Loss: 0.00002346
Iteration 114/1000 | Loss: 0.00002346
Iteration 115/1000 | Loss: 0.00002346
Iteration 116/1000 | Loss: 0.00002346
Iteration 117/1000 | Loss: 0.00002345
Iteration 118/1000 | Loss: 0.00002345
Iteration 119/1000 | Loss: 0.00002345
Iteration 120/1000 | Loss: 0.00002345
Iteration 121/1000 | Loss: 0.00002345
Iteration 122/1000 | Loss: 0.00002345
Iteration 123/1000 | Loss: 0.00002345
Iteration 124/1000 | Loss: 0.00002345
Iteration 125/1000 | Loss: 0.00002345
Iteration 126/1000 | Loss: 0.00002345
Iteration 127/1000 | Loss: 0.00002345
Iteration 128/1000 | Loss: 0.00002345
Iteration 129/1000 | Loss: 0.00002344
Iteration 130/1000 | Loss: 0.00002344
Iteration 131/1000 | Loss: 0.00002344
Iteration 132/1000 | Loss: 0.00002344
Iteration 133/1000 | Loss: 0.00002344
Iteration 134/1000 | Loss: 0.00002343
Iteration 135/1000 | Loss: 0.00002343
Iteration 136/1000 | Loss: 0.00002343
Iteration 137/1000 | Loss: 0.00002343
Iteration 138/1000 | Loss: 0.00002343
Iteration 139/1000 | Loss: 0.00002343
Iteration 140/1000 | Loss: 0.00002343
Iteration 141/1000 | Loss: 0.00002343
Iteration 142/1000 | Loss: 0.00002343
Iteration 143/1000 | Loss: 0.00002343
Iteration 144/1000 | Loss: 0.00002343
Iteration 145/1000 | Loss: 0.00002343
Iteration 146/1000 | Loss: 0.00002343
Iteration 147/1000 | Loss: 0.00002343
Iteration 148/1000 | Loss: 0.00002343
Iteration 149/1000 | Loss: 0.00002343
Iteration 150/1000 | Loss: 0.00002343
Iteration 151/1000 | Loss: 0.00002343
Iteration 152/1000 | Loss: 0.00002342
Iteration 153/1000 | Loss: 0.00002342
Iteration 154/1000 | Loss: 0.00002342
Iteration 155/1000 | Loss: 0.00002342
Iteration 156/1000 | Loss: 0.00002342
Iteration 157/1000 | Loss: 0.00002342
Iteration 158/1000 | Loss: 0.00002342
Iteration 159/1000 | Loss: 0.00002342
Iteration 160/1000 | Loss: 0.00002342
Iteration 161/1000 | Loss: 0.00002342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.3422702724928968e-05, 2.3422702724928968e-05, 2.3422702724928968e-05, 2.3422702724928968e-05, 2.3422702724928968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3422702724928968e-05

Optimization complete. Final v2v error: 4.027163982391357 mm

Highest mean error: 5.522981643676758 mm for frame 221

Lowest mean error: 3.2996621131896973 mm for frame 238

Saving results

Total time: 66.69225645065308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00730933
Iteration 2/25 | Loss: 0.00127751
Iteration 3/25 | Loss: 0.00110366
Iteration 4/25 | Loss: 0.00107654
Iteration 5/25 | Loss: 0.00106819
Iteration 6/25 | Loss: 0.00106589
Iteration 7/25 | Loss: 0.00106589
Iteration 8/25 | Loss: 0.00106589
Iteration 9/25 | Loss: 0.00106589
Iteration 10/25 | Loss: 0.00106589
Iteration 11/25 | Loss: 0.00106589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001065894146449864, 0.001065894146449864, 0.001065894146449864, 0.001065894146449864, 0.001065894146449864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001065894146449864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.00587678
Iteration 2/25 | Loss: 0.00134927
Iteration 3/25 | Loss: 0.00134927
Iteration 4/25 | Loss: 0.00134927
Iteration 5/25 | Loss: 0.00134927
Iteration 6/25 | Loss: 0.00134926
Iteration 7/25 | Loss: 0.00134926
Iteration 8/25 | Loss: 0.00134926
Iteration 9/25 | Loss: 0.00134926
Iteration 10/25 | Loss: 0.00134926
Iteration 11/25 | Loss: 0.00134926
Iteration 12/25 | Loss: 0.00134926
Iteration 13/25 | Loss: 0.00134926
Iteration 14/25 | Loss: 0.00134926
Iteration 15/25 | Loss: 0.00134926
Iteration 16/25 | Loss: 0.00134926
Iteration 17/25 | Loss: 0.00134926
Iteration 18/25 | Loss: 0.00134926
Iteration 19/25 | Loss: 0.00134926
Iteration 20/25 | Loss: 0.00134926
Iteration 21/25 | Loss: 0.00134926
Iteration 22/25 | Loss: 0.00134926
Iteration 23/25 | Loss: 0.00134926
Iteration 24/25 | Loss: 0.00134926
Iteration 25/25 | Loss: 0.00134926
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013492623111233115, 0.0013492623111233115, 0.0013492623111233115, 0.0013492623111233115, 0.0013492623111233115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013492623111233115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134926
Iteration 2/1000 | Loss: 0.00003055
Iteration 3/1000 | Loss: 0.00002272
Iteration 4/1000 | Loss: 0.00002055
Iteration 5/1000 | Loss: 0.00001967
Iteration 6/1000 | Loss: 0.00001914
Iteration 7/1000 | Loss: 0.00001881
Iteration 8/1000 | Loss: 0.00001848
Iteration 9/1000 | Loss: 0.00001825
Iteration 10/1000 | Loss: 0.00001806
Iteration 11/1000 | Loss: 0.00001802
Iteration 12/1000 | Loss: 0.00001798
Iteration 13/1000 | Loss: 0.00001797
Iteration 14/1000 | Loss: 0.00001796
Iteration 15/1000 | Loss: 0.00001795
Iteration 16/1000 | Loss: 0.00001795
Iteration 17/1000 | Loss: 0.00001794
Iteration 18/1000 | Loss: 0.00001783
Iteration 19/1000 | Loss: 0.00001776
Iteration 20/1000 | Loss: 0.00001765
Iteration 21/1000 | Loss: 0.00001764
Iteration 22/1000 | Loss: 0.00001763
Iteration 23/1000 | Loss: 0.00001762
Iteration 24/1000 | Loss: 0.00001762
Iteration 25/1000 | Loss: 0.00001760
Iteration 26/1000 | Loss: 0.00001759
Iteration 27/1000 | Loss: 0.00001759
Iteration 28/1000 | Loss: 0.00001757
Iteration 29/1000 | Loss: 0.00001754
Iteration 30/1000 | Loss: 0.00001754
Iteration 31/1000 | Loss: 0.00001754
Iteration 32/1000 | Loss: 0.00001753
Iteration 33/1000 | Loss: 0.00001753
Iteration 34/1000 | Loss: 0.00001753
Iteration 35/1000 | Loss: 0.00001753
Iteration 36/1000 | Loss: 0.00001752
Iteration 37/1000 | Loss: 0.00001752
Iteration 38/1000 | Loss: 0.00001752
Iteration 39/1000 | Loss: 0.00001752
Iteration 40/1000 | Loss: 0.00001751
Iteration 41/1000 | Loss: 0.00001750
Iteration 42/1000 | Loss: 0.00001750
Iteration 43/1000 | Loss: 0.00001750
Iteration 44/1000 | Loss: 0.00001750
Iteration 45/1000 | Loss: 0.00001749
Iteration 46/1000 | Loss: 0.00001749
Iteration 47/1000 | Loss: 0.00001749
Iteration 48/1000 | Loss: 0.00001749
Iteration 49/1000 | Loss: 0.00001749
Iteration 50/1000 | Loss: 0.00001748
Iteration 51/1000 | Loss: 0.00001748
Iteration 52/1000 | Loss: 0.00001747
Iteration 53/1000 | Loss: 0.00001747
Iteration 54/1000 | Loss: 0.00001746
Iteration 55/1000 | Loss: 0.00001746
Iteration 56/1000 | Loss: 0.00001746
Iteration 57/1000 | Loss: 0.00001746
Iteration 58/1000 | Loss: 0.00001746
Iteration 59/1000 | Loss: 0.00001745
Iteration 60/1000 | Loss: 0.00001745
Iteration 61/1000 | Loss: 0.00001745
Iteration 62/1000 | Loss: 0.00001744
Iteration 63/1000 | Loss: 0.00001744
Iteration 64/1000 | Loss: 0.00001744
Iteration 65/1000 | Loss: 0.00001743
Iteration 66/1000 | Loss: 0.00001743
Iteration 67/1000 | Loss: 0.00001743
Iteration 68/1000 | Loss: 0.00001742
Iteration 69/1000 | Loss: 0.00001742
Iteration 70/1000 | Loss: 0.00001742
Iteration 71/1000 | Loss: 0.00001742
Iteration 72/1000 | Loss: 0.00001742
Iteration 73/1000 | Loss: 0.00001742
Iteration 74/1000 | Loss: 0.00001742
Iteration 75/1000 | Loss: 0.00001742
Iteration 76/1000 | Loss: 0.00001742
Iteration 77/1000 | Loss: 0.00001742
Iteration 78/1000 | Loss: 0.00001741
Iteration 79/1000 | Loss: 0.00001741
Iteration 80/1000 | Loss: 0.00001741
Iteration 81/1000 | Loss: 0.00001741
Iteration 82/1000 | Loss: 0.00001741
Iteration 83/1000 | Loss: 0.00001741
Iteration 84/1000 | Loss: 0.00001740
Iteration 85/1000 | Loss: 0.00001740
Iteration 86/1000 | Loss: 0.00001740
Iteration 87/1000 | Loss: 0.00001740
Iteration 88/1000 | Loss: 0.00001740
Iteration 89/1000 | Loss: 0.00001740
Iteration 90/1000 | Loss: 0.00001740
Iteration 91/1000 | Loss: 0.00001740
Iteration 92/1000 | Loss: 0.00001740
Iteration 93/1000 | Loss: 0.00001740
Iteration 94/1000 | Loss: 0.00001740
Iteration 95/1000 | Loss: 0.00001740
Iteration 96/1000 | Loss: 0.00001740
Iteration 97/1000 | Loss: 0.00001739
Iteration 98/1000 | Loss: 0.00001739
Iteration 99/1000 | Loss: 0.00001739
Iteration 100/1000 | Loss: 0.00001739
Iteration 101/1000 | Loss: 0.00001739
Iteration 102/1000 | Loss: 0.00001739
Iteration 103/1000 | Loss: 0.00001739
Iteration 104/1000 | Loss: 0.00001738
Iteration 105/1000 | Loss: 0.00001738
Iteration 106/1000 | Loss: 0.00001738
Iteration 107/1000 | Loss: 0.00001738
Iteration 108/1000 | Loss: 0.00001738
Iteration 109/1000 | Loss: 0.00001738
Iteration 110/1000 | Loss: 0.00001738
Iteration 111/1000 | Loss: 0.00001738
Iteration 112/1000 | Loss: 0.00001738
Iteration 113/1000 | Loss: 0.00001738
Iteration 114/1000 | Loss: 0.00001738
Iteration 115/1000 | Loss: 0.00001738
Iteration 116/1000 | Loss: 0.00001738
Iteration 117/1000 | Loss: 0.00001738
Iteration 118/1000 | Loss: 0.00001738
Iteration 119/1000 | Loss: 0.00001738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.737632010190282e-05, 1.737632010190282e-05, 1.737632010190282e-05, 1.737632010190282e-05, 1.737632010190282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.737632010190282e-05

Optimization complete. Final v2v error: 3.5066707134246826 mm

Highest mean error: 4.1641950607299805 mm for frame 178

Lowest mean error: 2.5210111141204834 mm for frame 236

Saving results

Total time: 39.61417889595032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820897
Iteration 2/25 | Loss: 0.00161437
Iteration 3/25 | Loss: 0.00114093
Iteration 4/25 | Loss: 0.00110926
Iteration 5/25 | Loss: 0.00110237
Iteration 6/25 | Loss: 0.00110071
Iteration 7/25 | Loss: 0.00110039
Iteration 8/25 | Loss: 0.00110039
Iteration 9/25 | Loss: 0.00110039
Iteration 10/25 | Loss: 0.00110039
Iteration 11/25 | Loss: 0.00110039
Iteration 12/25 | Loss: 0.00110039
Iteration 13/25 | Loss: 0.00110039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001100394525565207, 0.001100394525565207, 0.001100394525565207, 0.001100394525565207, 0.001100394525565207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001100394525565207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.35914463
Iteration 2/25 | Loss: 0.00085133
Iteration 3/25 | Loss: 0.00085131
Iteration 4/25 | Loss: 0.00085131
Iteration 5/25 | Loss: 0.00085131
Iteration 6/25 | Loss: 0.00085131
Iteration 7/25 | Loss: 0.00085131
Iteration 8/25 | Loss: 0.00085130
Iteration 9/25 | Loss: 0.00085130
Iteration 10/25 | Loss: 0.00085130
Iteration 11/25 | Loss: 0.00085130
Iteration 12/25 | Loss: 0.00085130
Iteration 13/25 | Loss: 0.00085130
Iteration 14/25 | Loss: 0.00085130
Iteration 15/25 | Loss: 0.00085130
Iteration 16/25 | Loss: 0.00085130
Iteration 17/25 | Loss: 0.00085130
Iteration 18/25 | Loss: 0.00085130
Iteration 19/25 | Loss: 0.00085130
Iteration 20/25 | Loss: 0.00085130
Iteration 21/25 | Loss: 0.00085130
Iteration 22/25 | Loss: 0.00085130
Iteration 23/25 | Loss: 0.00085130
Iteration 24/25 | Loss: 0.00085130
Iteration 25/25 | Loss: 0.00085130

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085130
Iteration 2/1000 | Loss: 0.00009100
Iteration 3/1000 | Loss: 0.00006031
Iteration 4/1000 | Loss: 0.00004900
Iteration 5/1000 | Loss: 0.00004571
Iteration 6/1000 | Loss: 0.00004373
Iteration 7/1000 | Loss: 0.00004261
Iteration 8/1000 | Loss: 0.00004163
Iteration 9/1000 | Loss: 0.00004053
Iteration 10/1000 | Loss: 0.00003961
Iteration 11/1000 | Loss: 0.00003908
Iteration 12/1000 | Loss: 0.00003868
Iteration 13/1000 | Loss: 0.00003835
Iteration 14/1000 | Loss: 0.00003796
Iteration 15/1000 | Loss: 0.00003763
Iteration 16/1000 | Loss: 0.00003735
Iteration 17/1000 | Loss: 0.00003704
Iteration 18/1000 | Loss: 0.00003672
Iteration 19/1000 | Loss: 0.00003649
Iteration 20/1000 | Loss: 0.00003631
Iteration 21/1000 | Loss: 0.00003616
Iteration 22/1000 | Loss: 0.00003610
Iteration 23/1000 | Loss: 0.00003610
Iteration 24/1000 | Loss: 0.00003608
Iteration 25/1000 | Loss: 0.00003603
Iteration 26/1000 | Loss: 0.00003603
Iteration 27/1000 | Loss: 0.00003602
Iteration 28/1000 | Loss: 0.00003591
Iteration 29/1000 | Loss: 0.00003591
Iteration 30/1000 | Loss: 0.00003585
Iteration 31/1000 | Loss: 0.00003585
Iteration 32/1000 | Loss: 0.00003585
Iteration 33/1000 | Loss: 0.00003585
Iteration 34/1000 | Loss: 0.00003585
Iteration 35/1000 | Loss: 0.00003585
Iteration 36/1000 | Loss: 0.00003585
Iteration 37/1000 | Loss: 0.00003585
Iteration 38/1000 | Loss: 0.00003585
Iteration 39/1000 | Loss: 0.00003584
Iteration 40/1000 | Loss: 0.00003584
Iteration 41/1000 | Loss: 0.00003584
Iteration 42/1000 | Loss: 0.00003584
Iteration 43/1000 | Loss: 0.00003583
Iteration 44/1000 | Loss: 0.00003583
Iteration 45/1000 | Loss: 0.00003583
Iteration 46/1000 | Loss: 0.00003583
Iteration 47/1000 | Loss: 0.00003582
Iteration 48/1000 | Loss: 0.00003582
Iteration 49/1000 | Loss: 0.00003582
Iteration 50/1000 | Loss: 0.00003582
Iteration 51/1000 | Loss: 0.00003582
Iteration 52/1000 | Loss: 0.00003582
Iteration 53/1000 | Loss: 0.00003582
Iteration 54/1000 | Loss: 0.00003581
Iteration 55/1000 | Loss: 0.00003581
Iteration 56/1000 | Loss: 0.00003581
Iteration 57/1000 | Loss: 0.00003581
Iteration 58/1000 | Loss: 0.00003581
Iteration 59/1000 | Loss: 0.00003580
Iteration 60/1000 | Loss: 0.00003580
Iteration 61/1000 | Loss: 0.00003580
Iteration 62/1000 | Loss: 0.00003580
Iteration 63/1000 | Loss: 0.00003580
Iteration 64/1000 | Loss: 0.00003580
Iteration 65/1000 | Loss: 0.00003580
Iteration 66/1000 | Loss: 0.00003579
Iteration 67/1000 | Loss: 0.00003579
Iteration 68/1000 | Loss: 0.00003578
Iteration 69/1000 | Loss: 0.00003578
Iteration 70/1000 | Loss: 0.00003578
Iteration 71/1000 | Loss: 0.00003578
Iteration 72/1000 | Loss: 0.00003578
Iteration 73/1000 | Loss: 0.00003578
Iteration 74/1000 | Loss: 0.00003578
Iteration 75/1000 | Loss: 0.00003578
Iteration 76/1000 | Loss: 0.00003578
Iteration 77/1000 | Loss: 0.00003578
Iteration 78/1000 | Loss: 0.00003578
Iteration 79/1000 | Loss: 0.00003578
Iteration 80/1000 | Loss: 0.00003578
Iteration 81/1000 | Loss: 0.00003577
Iteration 82/1000 | Loss: 0.00003577
Iteration 83/1000 | Loss: 0.00003577
Iteration 84/1000 | Loss: 0.00003577
Iteration 85/1000 | Loss: 0.00003577
Iteration 86/1000 | Loss: 0.00003577
Iteration 87/1000 | Loss: 0.00003576
Iteration 88/1000 | Loss: 0.00003576
Iteration 89/1000 | Loss: 0.00003576
Iteration 90/1000 | Loss: 0.00003576
Iteration 91/1000 | Loss: 0.00003576
Iteration 92/1000 | Loss: 0.00003576
Iteration 93/1000 | Loss: 0.00003576
Iteration 94/1000 | Loss: 0.00003576
Iteration 95/1000 | Loss: 0.00003576
Iteration 96/1000 | Loss: 0.00003576
Iteration 97/1000 | Loss: 0.00003576
Iteration 98/1000 | Loss: 0.00003576
Iteration 99/1000 | Loss: 0.00003576
Iteration 100/1000 | Loss: 0.00003576
Iteration 101/1000 | Loss: 0.00003576
Iteration 102/1000 | Loss: 0.00003576
Iteration 103/1000 | Loss: 0.00003576
Iteration 104/1000 | Loss: 0.00003576
Iteration 105/1000 | Loss: 0.00003576
Iteration 106/1000 | Loss: 0.00003576
Iteration 107/1000 | Loss: 0.00003576
Iteration 108/1000 | Loss: 0.00003576
Iteration 109/1000 | Loss: 0.00003576
Iteration 110/1000 | Loss: 0.00003576
Iteration 111/1000 | Loss: 0.00003576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [3.5760836908593774e-05, 3.5760836908593774e-05, 3.5760836908593774e-05, 3.5760836908593774e-05, 3.5760836908593774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5760836908593774e-05

Optimization complete. Final v2v error: 4.5663018226623535 mm

Highest mean error: 6.3744587898254395 mm for frame 61

Lowest mean error: 2.8642241954803467 mm for frame 99

Saving results

Total time: 50.25423264503479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01096513
Iteration 2/25 | Loss: 0.00269300
Iteration 3/25 | Loss: 0.00170655
Iteration 4/25 | Loss: 0.00158081
Iteration 5/25 | Loss: 0.00151524
Iteration 6/25 | Loss: 0.00147987
Iteration 7/25 | Loss: 0.00139737
Iteration 8/25 | Loss: 0.00137879
Iteration 9/25 | Loss: 0.00135402
Iteration 10/25 | Loss: 0.00134743
Iteration 11/25 | Loss: 0.00133518
Iteration 12/25 | Loss: 0.00133545
Iteration 13/25 | Loss: 0.00133000
Iteration 14/25 | Loss: 0.00132597
Iteration 15/25 | Loss: 0.00132322
Iteration 16/25 | Loss: 0.00132030
Iteration 17/25 | Loss: 0.00132001
Iteration 18/25 | Loss: 0.00131712
Iteration 19/25 | Loss: 0.00131816
Iteration 20/25 | Loss: 0.00131675
Iteration 21/25 | Loss: 0.00131822
Iteration 22/25 | Loss: 0.00131905
Iteration 23/25 | Loss: 0.00131666
Iteration 24/25 | Loss: 0.00131817
Iteration 25/25 | Loss: 0.00131771

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11471772
Iteration 2/25 | Loss: 0.00337937
Iteration 3/25 | Loss: 0.00337937
Iteration 4/25 | Loss: 0.00337937
Iteration 5/25 | Loss: 0.00337937
Iteration 6/25 | Loss: 0.00337937
Iteration 7/25 | Loss: 0.00337936
Iteration 8/25 | Loss: 0.00337936
Iteration 9/25 | Loss: 0.00337936
Iteration 10/25 | Loss: 0.00337936
Iteration 11/25 | Loss: 0.00337936
Iteration 12/25 | Loss: 0.00337936
Iteration 13/25 | Loss: 0.00337936
Iteration 14/25 | Loss: 0.00337936
Iteration 15/25 | Loss: 0.00337936
Iteration 16/25 | Loss: 0.00337936
Iteration 17/25 | Loss: 0.00337936
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003379363100975752, 0.003379363100975752, 0.003379363100975752, 0.003379363100975752, 0.003379363100975752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003379363100975752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00337936
Iteration 2/1000 | Loss: 0.00059389
Iteration 3/1000 | Loss: 0.00244676
Iteration 4/1000 | Loss: 0.00188243
Iteration 5/1000 | Loss: 0.00121638
Iteration 6/1000 | Loss: 0.00048819
Iteration 7/1000 | Loss: 0.00038185
Iteration 8/1000 | Loss: 0.00056332
Iteration 9/1000 | Loss: 0.00043842
Iteration 10/1000 | Loss: 0.00154991
Iteration 11/1000 | Loss: 0.00045103
Iteration 12/1000 | Loss: 0.00034925
Iteration 13/1000 | Loss: 0.00334805
Iteration 14/1000 | Loss: 0.00343053
Iteration 15/1000 | Loss: 0.00126983
Iteration 16/1000 | Loss: 0.00103030
Iteration 17/1000 | Loss: 0.00152696
Iteration 18/1000 | Loss: 0.00163735
Iteration 19/1000 | Loss: 0.00040997
Iteration 20/1000 | Loss: 0.00029884
Iteration 21/1000 | Loss: 0.00093712
Iteration 22/1000 | Loss: 0.00178096
Iteration 23/1000 | Loss: 0.00157956
Iteration 24/1000 | Loss: 0.00159907
Iteration 25/1000 | Loss: 0.00144469
Iteration 26/1000 | Loss: 0.00111485
Iteration 27/1000 | Loss: 0.00069507
Iteration 28/1000 | Loss: 0.00092630
Iteration 29/1000 | Loss: 0.00075504
Iteration 30/1000 | Loss: 0.00106619
Iteration 31/1000 | Loss: 0.00088235
Iteration 32/1000 | Loss: 0.00064795
Iteration 33/1000 | Loss: 0.00046690
Iteration 34/1000 | Loss: 0.00066985
Iteration 35/1000 | Loss: 0.00102461
Iteration 36/1000 | Loss: 0.00087917
Iteration 37/1000 | Loss: 0.00103688
Iteration 38/1000 | Loss: 0.00090855
Iteration 39/1000 | Loss: 0.00116287
Iteration 40/1000 | Loss: 0.00143119
Iteration 41/1000 | Loss: 0.00107793
Iteration 42/1000 | Loss: 0.00125703
Iteration 43/1000 | Loss: 0.00101120
Iteration 44/1000 | Loss: 0.00096993
Iteration 45/1000 | Loss: 0.00108525
Iteration 46/1000 | Loss: 0.00114082
Iteration 47/1000 | Loss: 0.00144590
Iteration 48/1000 | Loss: 0.00106171
Iteration 49/1000 | Loss: 0.00072084
Iteration 50/1000 | Loss: 0.00071180
Iteration 51/1000 | Loss: 0.00106858
Iteration 52/1000 | Loss: 0.00089081
Iteration 53/1000 | Loss: 0.00120410
Iteration 54/1000 | Loss: 0.00121093
Iteration 55/1000 | Loss: 0.00109457
Iteration 56/1000 | Loss: 0.00073487
Iteration 57/1000 | Loss: 0.00049671
Iteration 58/1000 | Loss: 0.00044149
Iteration 59/1000 | Loss: 0.00095262
Iteration 60/1000 | Loss: 0.00066697
Iteration 61/1000 | Loss: 0.00027415
Iteration 62/1000 | Loss: 0.00038479
Iteration 63/1000 | Loss: 0.00055265
Iteration 64/1000 | Loss: 0.00040154
Iteration 65/1000 | Loss: 0.00052652
Iteration 66/1000 | Loss: 0.00056505
Iteration 67/1000 | Loss: 0.00085284
Iteration 68/1000 | Loss: 0.00054308
Iteration 69/1000 | Loss: 0.00026983
Iteration 70/1000 | Loss: 0.00042915
Iteration 71/1000 | Loss: 0.00070133
Iteration 72/1000 | Loss: 0.00051027
Iteration 73/1000 | Loss: 0.00097602
Iteration 74/1000 | Loss: 0.00115142
Iteration 75/1000 | Loss: 0.00190943
Iteration 76/1000 | Loss: 0.00131873
Iteration 77/1000 | Loss: 0.00059742
Iteration 78/1000 | Loss: 0.00052953
Iteration 79/1000 | Loss: 0.00276824
Iteration 80/1000 | Loss: 0.00227113
Iteration 81/1000 | Loss: 0.00175244
Iteration 82/1000 | Loss: 0.00133524
Iteration 83/1000 | Loss: 0.00303745
Iteration 84/1000 | Loss: 0.00239418
Iteration 85/1000 | Loss: 0.00245598
Iteration 86/1000 | Loss: 0.00048098
Iteration 87/1000 | Loss: 0.00076649
Iteration 88/1000 | Loss: 0.00164664
Iteration 89/1000 | Loss: 0.00070341
Iteration 90/1000 | Loss: 0.00060993
Iteration 91/1000 | Loss: 0.00096624
Iteration 92/1000 | Loss: 0.00086629
Iteration 93/1000 | Loss: 0.00049787
Iteration 94/1000 | Loss: 0.00086763
Iteration 95/1000 | Loss: 0.00117308
Iteration 96/1000 | Loss: 0.00059146
Iteration 97/1000 | Loss: 0.00039750
Iteration 98/1000 | Loss: 0.00013769
Iteration 99/1000 | Loss: 0.00010733
Iteration 100/1000 | Loss: 0.00008634
Iteration 101/1000 | Loss: 0.00035023
Iteration 102/1000 | Loss: 0.00044836
Iteration 103/1000 | Loss: 0.00050918
Iteration 104/1000 | Loss: 0.00031594
Iteration 105/1000 | Loss: 0.00019306
Iteration 106/1000 | Loss: 0.00068978
Iteration 107/1000 | Loss: 0.00063230
Iteration 108/1000 | Loss: 0.00068772
Iteration 109/1000 | Loss: 0.00154548
Iteration 110/1000 | Loss: 0.00090792
Iteration 111/1000 | Loss: 0.00068973
Iteration 112/1000 | Loss: 0.00026942
Iteration 113/1000 | Loss: 0.00013781
Iteration 114/1000 | Loss: 0.00133165
Iteration 115/1000 | Loss: 0.00129223
Iteration 116/1000 | Loss: 0.00110532
Iteration 117/1000 | Loss: 0.00037646
Iteration 118/1000 | Loss: 0.00141470
Iteration 119/1000 | Loss: 0.00039856
Iteration 120/1000 | Loss: 0.00028397
Iteration 121/1000 | Loss: 0.00069758
Iteration 122/1000 | Loss: 0.00057374
Iteration 123/1000 | Loss: 0.00007328
Iteration 124/1000 | Loss: 0.00005778
Iteration 125/1000 | Loss: 0.00005047
Iteration 126/1000 | Loss: 0.00004517
Iteration 127/1000 | Loss: 0.00004269
Iteration 128/1000 | Loss: 0.00004111
Iteration 129/1000 | Loss: 0.00004009
Iteration 130/1000 | Loss: 0.00003899
Iteration 131/1000 | Loss: 0.00003811
Iteration 132/1000 | Loss: 0.00093990
Iteration 133/1000 | Loss: 0.00042725
Iteration 134/1000 | Loss: 0.00061249
Iteration 135/1000 | Loss: 0.00020523
Iteration 136/1000 | Loss: 0.00004376
Iteration 137/1000 | Loss: 0.00003845
Iteration 138/1000 | Loss: 0.00045939
Iteration 139/1000 | Loss: 0.00014659
Iteration 140/1000 | Loss: 0.00037963
Iteration 141/1000 | Loss: 0.00017580
Iteration 142/1000 | Loss: 0.00034513
Iteration 143/1000 | Loss: 0.00014572
Iteration 144/1000 | Loss: 0.00033893
Iteration 145/1000 | Loss: 0.00080996
Iteration 146/1000 | Loss: 0.00005517
Iteration 147/1000 | Loss: 0.00004568
Iteration 148/1000 | Loss: 0.00018880
Iteration 149/1000 | Loss: 0.00022822
Iteration 150/1000 | Loss: 0.00016887
Iteration 151/1000 | Loss: 0.00021029
Iteration 152/1000 | Loss: 0.00014621
Iteration 153/1000 | Loss: 0.00049919
Iteration 154/1000 | Loss: 0.00017395
Iteration 155/1000 | Loss: 0.00011325
Iteration 156/1000 | Loss: 0.00019675
Iteration 157/1000 | Loss: 0.00038301
Iteration 158/1000 | Loss: 0.00027107
Iteration 159/1000 | Loss: 0.00020912
Iteration 160/1000 | Loss: 0.00034625
Iteration 161/1000 | Loss: 0.00017442
Iteration 162/1000 | Loss: 0.00026379
Iteration 163/1000 | Loss: 0.00006966
Iteration 164/1000 | Loss: 0.00003996
Iteration 165/1000 | Loss: 0.00020867
Iteration 166/1000 | Loss: 0.00021716
Iteration 167/1000 | Loss: 0.00004955
Iteration 168/1000 | Loss: 0.00032298
Iteration 169/1000 | Loss: 0.00004526
Iteration 170/1000 | Loss: 0.00003949
Iteration 171/1000 | Loss: 0.00003839
Iteration 172/1000 | Loss: 0.00047824
Iteration 173/1000 | Loss: 0.00024374
Iteration 174/1000 | Loss: 0.00049334
Iteration 175/1000 | Loss: 0.00007600
Iteration 176/1000 | Loss: 0.00004753
Iteration 177/1000 | Loss: 0.00004345
Iteration 178/1000 | Loss: 0.00004092
Iteration 179/1000 | Loss: 0.00003797
Iteration 180/1000 | Loss: 0.00087963
Iteration 181/1000 | Loss: 0.00030746
Iteration 182/1000 | Loss: 0.00003903
Iteration 183/1000 | Loss: 0.00070189
Iteration 184/1000 | Loss: 0.00039536
Iteration 185/1000 | Loss: 0.00003736
Iteration 186/1000 | Loss: 0.00003453
Iteration 187/1000 | Loss: 0.00062494
Iteration 188/1000 | Loss: 0.00034208
Iteration 189/1000 | Loss: 0.00004660
Iteration 190/1000 | Loss: 0.00004191
Iteration 191/1000 | Loss: 0.00019256
Iteration 192/1000 | Loss: 0.00019854
Iteration 193/1000 | Loss: 0.00042663
Iteration 194/1000 | Loss: 0.00025001
Iteration 195/1000 | Loss: 0.00022036
Iteration 196/1000 | Loss: 0.00019257
Iteration 197/1000 | Loss: 0.00003604
Iteration 198/1000 | Loss: 0.00003450
Iteration 199/1000 | Loss: 0.00003404
Iteration 200/1000 | Loss: 0.00003372
Iteration 201/1000 | Loss: 0.00003343
Iteration 202/1000 | Loss: 0.00003321
Iteration 203/1000 | Loss: 0.00003299
Iteration 204/1000 | Loss: 0.00079677
Iteration 205/1000 | Loss: 0.00039831
Iteration 206/1000 | Loss: 0.00102781
Iteration 207/1000 | Loss: 0.00038401
Iteration 208/1000 | Loss: 0.00005360
Iteration 209/1000 | Loss: 0.00003870
Iteration 210/1000 | Loss: 0.00090004
Iteration 211/1000 | Loss: 0.00004373
Iteration 212/1000 | Loss: 0.00003633
Iteration 213/1000 | Loss: 0.00003346
Iteration 214/1000 | Loss: 0.00003204
Iteration 215/1000 | Loss: 0.00003066
Iteration 216/1000 | Loss: 0.00002966
Iteration 217/1000 | Loss: 0.00002913
Iteration 218/1000 | Loss: 0.00002871
Iteration 219/1000 | Loss: 0.00002850
Iteration 220/1000 | Loss: 0.00002828
Iteration 221/1000 | Loss: 0.00002810
Iteration 222/1000 | Loss: 0.00002807
Iteration 223/1000 | Loss: 0.00002806
Iteration 224/1000 | Loss: 0.00002805
Iteration 225/1000 | Loss: 0.00002801
Iteration 226/1000 | Loss: 0.00002799
Iteration 227/1000 | Loss: 0.00002797
Iteration 228/1000 | Loss: 0.00002795
Iteration 229/1000 | Loss: 0.00002794
Iteration 230/1000 | Loss: 0.00002793
Iteration 231/1000 | Loss: 0.00002793
Iteration 232/1000 | Loss: 0.00002789
Iteration 233/1000 | Loss: 0.00002784
Iteration 234/1000 | Loss: 0.00002784
Iteration 235/1000 | Loss: 0.00002784
Iteration 236/1000 | Loss: 0.00002784
Iteration 237/1000 | Loss: 0.00002782
Iteration 238/1000 | Loss: 0.00002779
Iteration 239/1000 | Loss: 0.00002775
Iteration 240/1000 | Loss: 0.00002771
Iteration 241/1000 | Loss: 0.00002771
Iteration 242/1000 | Loss: 0.00002771
Iteration 243/1000 | Loss: 0.00002771
Iteration 244/1000 | Loss: 0.00002771
Iteration 245/1000 | Loss: 0.00002771
Iteration 246/1000 | Loss: 0.00002771
Iteration 247/1000 | Loss: 0.00002771
Iteration 248/1000 | Loss: 0.00002771
Iteration 249/1000 | Loss: 0.00002770
Iteration 250/1000 | Loss: 0.00002770
Iteration 251/1000 | Loss: 0.00002770
Iteration 252/1000 | Loss: 0.00002769
Iteration 253/1000 | Loss: 0.00002769
Iteration 254/1000 | Loss: 0.00002768
Iteration 255/1000 | Loss: 0.00002768
Iteration 256/1000 | Loss: 0.00002768
Iteration 257/1000 | Loss: 0.00002768
Iteration 258/1000 | Loss: 0.00002768
Iteration 259/1000 | Loss: 0.00002768
Iteration 260/1000 | Loss: 0.00002768
Iteration 261/1000 | Loss: 0.00002767
Iteration 262/1000 | Loss: 0.00002767
Iteration 263/1000 | Loss: 0.00002767
Iteration 264/1000 | Loss: 0.00002766
Iteration 265/1000 | Loss: 0.00002765
Iteration 266/1000 | Loss: 0.00002765
Iteration 267/1000 | Loss: 0.00002764
Iteration 268/1000 | Loss: 0.00002764
Iteration 269/1000 | Loss: 0.00002764
Iteration 270/1000 | Loss: 0.00002764
Iteration 271/1000 | Loss: 0.00002764
Iteration 272/1000 | Loss: 0.00002763
Iteration 273/1000 | Loss: 0.00002763
Iteration 274/1000 | Loss: 0.00002763
Iteration 275/1000 | Loss: 0.00002763
Iteration 276/1000 | Loss: 0.00002763
Iteration 277/1000 | Loss: 0.00002763
Iteration 278/1000 | Loss: 0.00002763
Iteration 279/1000 | Loss: 0.00002762
Iteration 280/1000 | Loss: 0.00002762
Iteration 281/1000 | Loss: 0.00002762
Iteration 282/1000 | Loss: 0.00002762
Iteration 283/1000 | Loss: 0.00002762
Iteration 284/1000 | Loss: 0.00002762
Iteration 285/1000 | Loss: 0.00002762
Iteration 286/1000 | Loss: 0.00002762
Iteration 287/1000 | Loss: 0.00002762
Iteration 288/1000 | Loss: 0.00002762
Iteration 289/1000 | Loss: 0.00002762
Iteration 290/1000 | Loss: 0.00002762
Iteration 291/1000 | Loss: 0.00002762
Iteration 292/1000 | Loss: 0.00002762
Iteration 293/1000 | Loss: 0.00002761
Iteration 294/1000 | Loss: 0.00002761
Iteration 295/1000 | Loss: 0.00002761
Iteration 296/1000 | Loss: 0.00002761
Iteration 297/1000 | Loss: 0.00002761
Iteration 298/1000 | Loss: 0.00002761
Iteration 299/1000 | Loss: 0.00002761
Iteration 300/1000 | Loss: 0.00002761
Iteration 301/1000 | Loss: 0.00002761
Iteration 302/1000 | Loss: 0.00002761
Iteration 303/1000 | Loss: 0.00002761
Iteration 304/1000 | Loss: 0.00002761
Iteration 305/1000 | Loss: 0.00002761
Iteration 306/1000 | Loss: 0.00002761
Iteration 307/1000 | Loss: 0.00002761
Iteration 308/1000 | Loss: 0.00002761
Iteration 309/1000 | Loss: 0.00002760
Iteration 310/1000 | Loss: 0.00002760
Iteration 311/1000 | Loss: 0.00002760
Iteration 312/1000 | Loss: 0.00002760
Iteration 313/1000 | Loss: 0.00002760
Iteration 314/1000 | Loss: 0.00002760
Iteration 315/1000 | Loss: 0.00002760
Iteration 316/1000 | Loss: 0.00002760
Iteration 317/1000 | Loss: 0.00002760
Iteration 318/1000 | Loss: 0.00002760
Iteration 319/1000 | Loss: 0.00002760
Iteration 320/1000 | Loss: 0.00002760
Iteration 321/1000 | Loss: 0.00002760
Iteration 322/1000 | Loss: 0.00002760
Iteration 323/1000 | Loss: 0.00002760
Iteration 324/1000 | Loss: 0.00002760
Iteration 325/1000 | Loss: 0.00002760
Iteration 326/1000 | Loss: 0.00002760
Iteration 327/1000 | Loss: 0.00002760
Iteration 328/1000 | Loss: 0.00002760
Iteration 329/1000 | Loss: 0.00002760
Iteration 330/1000 | Loss: 0.00002760
Iteration 331/1000 | Loss: 0.00002760
Iteration 332/1000 | Loss: 0.00002760
Iteration 333/1000 | Loss: 0.00002760
Iteration 334/1000 | Loss: 0.00002760
Iteration 335/1000 | Loss: 0.00002760
Iteration 336/1000 | Loss: 0.00002760
Iteration 337/1000 | Loss: 0.00002760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 337. Stopping optimization.
Last 5 losses: [2.7596272047958337e-05, 2.7596272047958337e-05, 2.7596272047958337e-05, 2.7596272047958337e-05, 2.7596272047958337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7596272047958337e-05

Optimization complete. Final v2v error: 3.6842761039733887 mm

Highest mean error: 11.936153411865234 mm for frame 67

Lowest mean error: 2.4748787879943848 mm for frame 3

Saving results

Total time: 363.8506488800049
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057017
Iteration 2/25 | Loss: 0.00234435
Iteration 3/25 | Loss: 0.00227061
Iteration 4/25 | Loss: 0.00126770
Iteration 5/25 | Loss: 0.00118531
Iteration 6/25 | Loss: 0.00110530
Iteration 7/25 | Loss: 0.00106323
Iteration 8/25 | Loss: 0.00105504
Iteration 9/25 | Loss: 0.00105378
Iteration 10/25 | Loss: 0.00105355
Iteration 11/25 | Loss: 0.00105355
Iteration 12/25 | Loss: 0.00105355
Iteration 13/25 | Loss: 0.00105355
Iteration 14/25 | Loss: 0.00105355
Iteration 15/25 | Loss: 0.00105355
Iteration 16/25 | Loss: 0.00105355
Iteration 17/25 | Loss: 0.00105355
Iteration 18/25 | Loss: 0.00105355
Iteration 19/25 | Loss: 0.00105355
Iteration 20/25 | Loss: 0.00105355
Iteration 21/25 | Loss: 0.00105355
Iteration 22/25 | Loss: 0.00105355
Iteration 23/25 | Loss: 0.00105355
Iteration 24/25 | Loss: 0.00105355
Iteration 25/25 | Loss: 0.00105355

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23311615
Iteration 2/25 | Loss: 0.00150454
Iteration 3/25 | Loss: 0.00098031
Iteration 4/25 | Loss: 0.00098031
Iteration 5/25 | Loss: 0.00098031
Iteration 6/25 | Loss: 0.00098031
Iteration 7/25 | Loss: 0.00098031
Iteration 8/25 | Loss: 0.00098031
Iteration 9/25 | Loss: 0.00098031
Iteration 10/25 | Loss: 0.00098031
Iteration 11/25 | Loss: 0.00098031
Iteration 12/25 | Loss: 0.00098031
Iteration 13/25 | Loss: 0.00098031
Iteration 14/25 | Loss: 0.00098031
Iteration 15/25 | Loss: 0.00098031
Iteration 16/25 | Loss: 0.00098031
Iteration 17/25 | Loss: 0.00098031
Iteration 18/25 | Loss: 0.00098031
Iteration 19/25 | Loss: 0.00098031
Iteration 20/25 | Loss: 0.00098031
Iteration 21/25 | Loss: 0.00098031
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009803090943023562, 0.0009803090943023562, 0.0009803090943023562, 0.0009803090943023562, 0.0009803090943023562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009803090943023562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098031
Iteration 2/1000 | Loss: 0.00003203
Iteration 3/1000 | Loss: 0.00002066
Iteration 4/1000 | Loss: 0.00022692
Iteration 5/1000 | Loss: 0.00001752
Iteration 6/1000 | Loss: 0.00001657
Iteration 7/1000 | Loss: 0.00001608
Iteration 8/1000 | Loss: 0.00001560
Iteration 9/1000 | Loss: 0.00001532
Iteration 10/1000 | Loss: 0.00001517
Iteration 11/1000 | Loss: 0.00001509
Iteration 12/1000 | Loss: 0.00001505
Iteration 13/1000 | Loss: 0.00001496
Iteration 14/1000 | Loss: 0.00001495
Iteration 15/1000 | Loss: 0.00001492
Iteration 16/1000 | Loss: 0.00001490
Iteration 17/1000 | Loss: 0.00001486
Iteration 18/1000 | Loss: 0.00001484
Iteration 19/1000 | Loss: 0.00001483
Iteration 20/1000 | Loss: 0.00001483
Iteration 21/1000 | Loss: 0.00001483
Iteration 22/1000 | Loss: 0.00001482
Iteration 23/1000 | Loss: 0.00001480
Iteration 24/1000 | Loss: 0.00001480
Iteration 25/1000 | Loss: 0.00001479
Iteration 26/1000 | Loss: 0.00001478
Iteration 27/1000 | Loss: 0.00001478
Iteration 28/1000 | Loss: 0.00001478
Iteration 29/1000 | Loss: 0.00001478
Iteration 30/1000 | Loss: 0.00001477
Iteration 31/1000 | Loss: 0.00001477
Iteration 32/1000 | Loss: 0.00001477
Iteration 33/1000 | Loss: 0.00001477
Iteration 34/1000 | Loss: 0.00001476
Iteration 35/1000 | Loss: 0.00001476
Iteration 36/1000 | Loss: 0.00001476
Iteration 37/1000 | Loss: 0.00001476
Iteration 38/1000 | Loss: 0.00001476
Iteration 39/1000 | Loss: 0.00001476
Iteration 40/1000 | Loss: 0.00001475
Iteration 41/1000 | Loss: 0.00001475
Iteration 42/1000 | Loss: 0.00001475
Iteration 43/1000 | Loss: 0.00001474
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001473
Iteration 46/1000 | Loss: 0.00001473
Iteration 47/1000 | Loss: 0.00001473
Iteration 48/1000 | Loss: 0.00001473
Iteration 49/1000 | Loss: 0.00001473
Iteration 50/1000 | Loss: 0.00001473
Iteration 51/1000 | Loss: 0.00001473
Iteration 52/1000 | Loss: 0.00001473
Iteration 53/1000 | Loss: 0.00001473
Iteration 54/1000 | Loss: 0.00001473
Iteration 55/1000 | Loss: 0.00001472
Iteration 56/1000 | Loss: 0.00001472
Iteration 57/1000 | Loss: 0.00001472
Iteration 58/1000 | Loss: 0.00001472
Iteration 59/1000 | Loss: 0.00001472
Iteration 60/1000 | Loss: 0.00001472
Iteration 61/1000 | Loss: 0.00001472
Iteration 62/1000 | Loss: 0.00001471
Iteration 63/1000 | Loss: 0.00001471
Iteration 64/1000 | Loss: 0.00001471
Iteration 65/1000 | Loss: 0.00001471
Iteration 66/1000 | Loss: 0.00001471
Iteration 67/1000 | Loss: 0.00001471
Iteration 68/1000 | Loss: 0.00001471
Iteration 69/1000 | Loss: 0.00001471
Iteration 70/1000 | Loss: 0.00001471
Iteration 71/1000 | Loss: 0.00001471
Iteration 72/1000 | Loss: 0.00001470
Iteration 73/1000 | Loss: 0.00001470
Iteration 74/1000 | Loss: 0.00001470
Iteration 75/1000 | Loss: 0.00001470
Iteration 76/1000 | Loss: 0.00001470
Iteration 77/1000 | Loss: 0.00001470
Iteration 78/1000 | Loss: 0.00001470
Iteration 79/1000 | Loss: 0.00001470
Iteration 80/1000 | Loss: 0.00001470
Iteration 81/1000 | Loss: 0.00001470
Iteration 82/1000 | Loss: 0.00001470
Iteration 83/1000 | Loss: 0.00001470
Iteration 84/1000 | Loss: 0.00001470
Iteration 85/1000 | Loss: 0.00001470
Iteration 86/1000 | Loss: 0.00001470
Iteration 87/1000 | Loss: 0.00001470
Iteration 88/1000 | Loss: 0.00001470
Iteration 89/1000 | Loss: 0.00001470
Iteration 90/1000 | Loss: 0.00001470
Iteration 91/1000 | Loss: 0.00001470
Iteration 92/1000 | Loss: 0.00001470
Iteration 93/1000 | Loss: 0.00001470
Iteration 94/1000 | Loss: 0.00001470
Iteration 95/1000 | Loss: 0.00001470
Iteration 96/1000 | Loss: 0.00001470
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001470
Iteration 99/1000 | Loss: 0.00001470
Iteration 100/1000 | Loss: 0.00001470
Iteration 101/1000 | Loss: 0.00001470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.47001428558724e-05, 1.47001428558724e-05, 1.47001428558724e-05, 1.47001428558724e-05, 1.47001428558724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.47001428558724e-05

Optimization complete. Final v2v error: 3.168823719024658 mm

Highest mean error: 3.282269239425659 mm for frame 5

Lowest mean error: 3.0736916065216064 mm for frame 24

Saving results

Total time: 39.36541676521301
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00684007
Iteration 2/25 | Loss: 0.00120867
Iteration 3/25 | Loss: 0.00105761
Iteration 4/25 | Loss: 0.00104786
Iteration 5/25 | Loss: 0.00104625
Iteration 6/25 | Loss: 0.00104625
Iteration 7/25 | Loss: 0.00104625
Iteration 8/25 | Loss: 0.00104625
Iteration 9/25 | Loss: 0.00104625
Iteration 10/25 | Loss: 0.00104625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001046250108629465, 0.001046250108629465, 0.001046250108629465, 0.001046250108629465, 0.001046250108629465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001046250108629465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.59357166
Iteration 2/25 | Loss: 0.00105821
Iteration 3/25 | Loss: 0.00105818
Iteration 4/25 | Loss: 0.00105818
Iteration 5/25 | Loss: 0.00105818
Iteration 6/25 | Loss: 0.00105818
Iteration 7/25 | Loss: 0.00105818
Iteration 8/25 | Loss: 0.00105818
Iteration 9/25 | Loss: 0.00105818
Iteration 10/25 | Loss: 0.00105818
Iteration 11/25 | Loss: 0.00105818
Iteration 12/25 | Loss: 0.00105818
Iteration 13/25 | Loss: 0.00105818
Iteration 14/25 | Loss: 0.00105818
Iteration 15/25 | Loss: 0.00105818
Iteration 16/25 | Loss: 0.00105818
Iteration 17/25 | Loss: 0.00105818
Iteration 18/25 | Loss: 0.00105818
Iteration 19/25 | Loss: 0.00105818
Iteration 20/25 | Loss: 0.00105818
Iteration 21/25 | Loss: 0.00105818
Iteration 22/25 | Loss: 0.00105818
Iteration 23/25 | Loss: 0.00105818
Iteration 24/25 | Loss: 0.00105818
Iteration 25/25 | Loss: 0.00105818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105818
Iteration 2/1000 | Loss: 0.00003705
Iteration 3/1000 | Loss: 0.00002050
Iteration 4/1000 | Loss: 0.00001701
Iteration 5/1000 | Loss: 0.00001575
Iteration 6/1000 | Loss: 0.00001535
Iteration 7/1000 | Loss: 0.00001498
Iteration 8/1000 | Loss: 0.00001475
Iteration 9/1000 | Loss: 0.00001457
Iteration 10/1000 | Loss: 0.00001448
Iteration 11/1000 | Loss: 0.00001431
Iteration 12/1000 | Loss: 0.00001429
Iteration 13/1000 | Loss: 0.00001428
Iteration 14/1000 | Loss: 0.00001423
Iteration 15/1000 | Loss: 0.00001420
Iteration 16/1000 | Loss: 0.00001420
Iteration 17/1000 | Loss: 0.00001420
Iteration 18/1000 | Loss: 0.00001419
Iteration 19/1000 | Loss: 0.00001419
Iteration 20/1000 | Loss: 0.00001418
Iteration 21/1000 | Loss: 0.00001418
Iteration 22/1000 | Loss: 0.00001418
Iteration 23/1000 | Loss: 0.00001417
Iteration 24/1000 | Loss: 0.00001417
Iteration 25/1000 | Loss: 0.00001416
Iteration 26/1000 | Loss: 0.00001416
Iteration 27/1000 | Loss: 0.00001416
Iteration 28/1000 | Loss: 0.00001416
Iteration 29/1000 | Loss: 0.00001415
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001414
Iteration 32/1000 | Loss: 0.00001414
Iteration 33/1000 | Loss: 0.00001414
Iteration 34/1000 | Loss: 0.00001413
Iteration 35/1000 | Loss: 0.00001413
Iteration 36/1000 | Loss: 0.00001412
Iteration 37/1000 | Loss: 0.00001412
Iteration 38/1000 | Loss: 0.00001412
Iteration 39/1000 | Loss: 0.00001411
Iteration 40/1000 | Loss: 0.00001411
Iteration 41/1000 | Loss: 0.00001411
Iteration 42/1000 | Loss: 0.00001411
Iteration 43/1000 | Loss: 0.00001411
Iteration 44/1000 | Loss: 0.00001410
Iteration 45/1000 | Loss: 0.00001409
Iteration 46/1000 | Loss: 0.00001408
Iteration 47/1000 | Loss: 0.00001408
Iteration 48/1000 | Loss: 0.00001407
Iteration 49/1000 | Loss: 0.00001407
Iteration 50/1000 | Loss: 0.00001406
Iteration 51/1000 | Loss: 0.00001405
Iteration 52/1000 | Loss: 0.00001405
Iteration 53/1000 | Loss: 0.00001405
Iteration 54/1000 | Loss: 0.00001405
Iteration 55/1000 | Loss: 0.00001405
Iteration 56/1000 | Loss: 0.00001405
Iteration 57/1000 | Loss: 0.00001405
Iteration 58/1000 | Loss: 0.00001405
Iteration 59/1000 | Loss: 0.00001404
Iteration 60/1000 | Loss: 0.00001404
Iteration 61/1000 | Loss: 0.00001404
Iteration 62/1000 | Loss: 0.00001404
Iteration 63/1000 | Loss: 0.00001404
Iteration 64/1000 | Loss: 0.00001403
Iteration 65/1000 | Loss: 0.00001403
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001403
Iteration 69/1000 | Loss: 0.00001403
Iteration 70/1000 | Loss: 0.00001403
Iteration 71/1000 | Loss: 0.00001403
Iteration 72/1000 | Loss: 0.00001403
Iteration 73/1000 | Loss: 0.00001403
Iteration 74/1000 | Loss: 0.00001403
Iteration 75/1000 | Loss: 0.00001403
Iteration 76/1000 | Loss: 0.00001403
Iteration 77/1000 | Loss: 0.00001403
Iteration 78/1000 | Loss: 0.00001402
Iteration 79/1000 | Loss: 0.00001402
Iteration 80/1000 | Loss: 0.00001401
Iteration 81/1000 | Loss: 0.00001401
Iteration 82/1000 | Loss: 0.00001401
Iteration 83/1000 | Loss: 0.00001401
Iteration 84/1000 | Loss: 0.00001400
Iteration 85/1000 | Loss: 0.00001400
Iteration 86/1000 | Loss: 0.00001400
Iteration 87/1000 | Loss: 0.00001399
Iteration 88/1000 | Loss: 0.00001399
Iteration 89/1000 | Loss: 0.00001399
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001398
Iteration 92/1000 | Loss: 0.00001398
Iteration 93/1000 | Loss: 0.00001398
Iteration 94/1000 | Loss: 0.00001398
Iteration 95/1000 | Loss: 0.00001398
Iteration 96/1000 | Loss: 0.00001397
Iteration 97/1000 | Loss: 0.00001397
Iteration 98/1000 | Loss: 0.00001397
Iteration 99/1000 | Loss: 0.00001397
Iteration 100/1000 | Loss: 0.00001396
Iteration 101/1000 | Loss: 0.00001396
Iteration 102/1000 | Loss: 0.00001396
Iteration 103/1000 | Loss: 0.00001396
Iteration 104/1000 | Loss: 0.00001396
Iteration 105/1000 | Loss: 0.00001395
Iteration 106/1000 | Loss: 0.00001395
Iteration 107/1000 | Loss: 0.00001395
Iteration 108/1000 | Loss: 0.00001395
Iteration 109/1000 | Loss: 0.00001395
Iteration 110/1000 | Loss: 0.00001395
Iteration 111/1000 | Loss: 0.00001395
Iteration 112/1000 | Loss: 0.00001394
Iteration 113/1000 | Loss: 0.00001394
Iteration 114/1000 | Loss: 0.00001394
Iteration 115/1000 | Loss: 0.00001394
Iteration 116/1000 | Loss: 0.00001394
Iteration 117/1000 | Loss: 0.00001393
Iteration 118/1000 | Loss: 0.00001393
Iteration 119/1000 | Loss: 0.00001393
Iteration 120/1000 | Loss: 0.00001393
Iteration 121/1000 | Loss: 0.00001393
Iteration 122/1000 | Loss: 0.00001393
Iteration 123/1000 | Loss: 0.00001393
Iteration 124/1000 | Loss: 0.00001393
Iteration 125/1000 | Loss: 0.00001393
Iteration 126/1000 | Loss: 0.00001392
Iteration 127/1000 | Loss: 0.00001392
Iteration 128/1000 | Loss: 0.00001392
Iteration 129/1000 | Loss: 0.00001391
Iteration 130/1000 | Loss: 0.00001391
Iteration 131/1000 | Loss: 0.00001391
Iteration 132/1000 | Loss: 0.00001391
Iteration 133/1000 | Loss: 0.00001391
Iteration 134/1000 | Loss: 0.00001391
Iteration 135/1000 | Loss: 0.00001390
Iteration 136/1000 | Loss: 0.00001390
Iteration 137/1000 | Loss: 0.00001390
Iteration 138/1000 | Loss: 0.00001390
Iteration 139/1000 | Loss: 0.00001390
Iteration 140/1000 | Loss: 0.00001390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.390366378473118e-05, 1.390366378473118e-05, 1.390366378473118e-05, 1.390366378473118e-05, 1.390366378473118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.390366378473118e-05

Optimization complete. Final v2v error: 3.101428747177124 mm

Highest mean error: 3.8972041606903076 mm for frame 176

Lowest mean error: 2.6353349685668945 mm for frame 188

Saving results

Total time: 34.11112141609192
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00505809
Iteration 2/25 | Loss: 0.00127781
Iteration 3/25 | Loss: 0.00107743
Iteration 4/25 | Loss: 0.00105646
Iteration 5/25 | Loss: 0.00104956
Iteration 6/25 | Loss: 0.00104731
Iteration 7/25 | Loss: 0.00104731
Iteration 8/25 | Loss: 0.00104731
Iteration 9/25 | Loss: 0.00104731
Iteration 10/25 | Loss: 0.00104731
Iteration 11/25 | Loss: 0.00104731
Iteration 12/25 | Loss: 0.00104731
Iteration 13/25 | Loss: 0.00104731
Iteration 14/25 | Loss: 0.00104731
Iteration 15/25 | Loss: 0.00104731
Iteration 16/25 | Loss: 0.00104731
Iteration 17/25 | Loss: 0.00104731
Iteration 18/25 | Loss: 0.00104731
Iteration 19/25 | Loss: 0.00104731
Iteration 20/25 | Loss: 0.00104731
Iteration 21/25 | Loss: 0.00104731
Iteration 22/25 | Loss: 0.00104731
Iteration 23/25 | Loss: 0.00104731
Iteration 24/25 | Loss: 0.00104731
Iteration 25/25 | Loss: 0.00104731

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.73969620
Iteration 2/25 | Loss: 0.00105808
Iteration 3/25 | Loss: 0.00105808
Iteration 4/25 | Loss: 0.00105808
Iteration 5/25 | Loss: 0.00105808
Iteration 6/25 | Loss: 0.00105808
Iteration 7/25 | Loss: 0.00105808
Iteration 8/25 | Loss: 0.00105808
Iteration 9/25 | Loss: 0.00105808
Iteration 10/25 | Loss: 0.00105808
Iteration 11/25 | Loss: 0.00105808
Iteration 12/25 | Loss: 0.00105808
Iteration 13/25 | Loss: 0.00105808
Iteration 14/25 | Loss: 0.00105808
Iteration 15/25 | Loss: 0.00105808
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001058077090419829, 0.001058077090419829, 0.001058077090419829, 0.001058077090419829, 0.001058077090419829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001058077090419829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105808
Iteration 2/1000 | Loss: 0.00006098
Iteration 3/1000 | Loss: 0.00003976
Iteration 4/1000 | Loss: 0.00003289
Iteration 5/1000 | Loss: 0.00003043
Iteration 6/1000 | Loss: 0.00002927
Iteration 7/1000 | Loss: 0.00002839
Iteration 8/1000 | Loss: 0.00002779
Iteration 9/1000 | Loss: 0.00002720
Iteration 10/1000 | Loss: 0.00002686
Iteration 11/1000 | Loss: 0.00002663
Iteration 12/1000 | Loss: 0.00002640
Iteration 13/1000 | Loss: 0.00002611
Iteration 14/1000 | Loss: 0.00002591
Iteration 15/1000 | Loss: 0.00002570
Iteration 16/1000 | Loss: 0.00002567
Iteration 17/1000 | Loss: 0.00002550
Iteration 18/1000 | Loss: 0.00002542
Iteration 19/1000 | Loss: 0.00002540
Iteration 20/1000 | Loss: 0.00002537
Iteration 21/1000 | Loss: 0.00002531
Iteration 22/1000 | Loss: 0.00002515
Iteration 23/1000 | Loss: 0.00002498
Iteration 24/1000 | Loss: 0.00002482
Iteration 25/1000 | Loss: 0.00002480
Iteration 26/1000 | Loss: 0.00002466
Iteration 27/1000 | Loss: 0.00002466
Iteration 28/1000 | Loss: 0.00002459
Iteration 29/1000 | Loss: 0.00002451
Iteration 30/1000 | Loss: 0.00002451
Iteration 31/1000 | Loss: 0.00002450
Iteration 32/1000 | Loss: 0.00002449
Iteration 33/1000 | Loss: 0.00002448
Iteration 34/1000 | Loss: 0.00002448
Iteration 35/1000 | Loss: 0.00002447
Iteration 36/1000 | Loss: 0.00002447
Iteration 37/1000 | Loss: 0.00002446
Iteration 38/1000 | Loss: 0.00002445
Iteration 39/1000 | Loss: 0.00002444
Iteration 40/1000 | Loss: 0.00002442
Iteration 41/1000 | Loss: 0.00002442
Iteration 42/1000 | Loss: 0.00002441
Iteration 43/1000 | Loss: 0.00002441
Iteration 44/1000 | Loss: 0.00002441
Iteration 45/1000 | Loss: 0.00002440
Iteration 46/1000 | Loss: 0.00002440
Iteration 47/1000 | Loss: 0.00002440
Iteration 48/1000 | Loss: 0.00002439
Iteration 49/1000 | Loss: 0.00002439
Iteration 50/1000 | Loss: 0.00002438
Iteration 51/1000 | Loss: 0.00002438
Iteration 52/1000 | Loss: 0.00002438
Iteration 53/1000 | Loss: 0.00002438
Iteration 54/1000 | Loss: 0.00002438
Iteration 55/1000 | Loss: 0.00002438
Iteration 56/1000 | Loss: 0.00002438
Iteration 57/1000 | Loss: 0.00002438
Iteration 58/1000 | Loss: 0.00002438
Iteration 59/1000 | Loss: 0.00002438
Iteration 60/1000 | Loss: 0.00002438
Iteration 61/1000 | Loss: 0.00002436
Iteration 62/1000 | Loss: 0.00002436
Iteration 63/1000 | Loss: 0.00002436
Iteration 64/1000 | Loss: 0.00002435
Iteration 65/1000 | Loss: 0.00002435
Iteration 66/1000 | Loss: 0.00002435
Iteration 67/1000 | Loss: 0.00002434
Iteration 68/1000 | Loss: 0.00002434
Iteration 69/1000 | Loss: 0.00002434
Iteration 70/1000 | Loss: 0.00002434
Iteration 71/1000 | Loss: 0.00002434
Iteration 72/1000 | Loss: 0.00002434
Iteration 73/1000 | Loss: 0.00002433
Iteration 74/1000 | Loss: 0.00002433
Iteration 75/1000 | Loss: 0.00002433
Iteration 76/1000 | Loss: 0.00002432
Iteration 77/1000 | Loss: 0.00002432
Iteration 78/1000 | Loss: 0.00002432
Iteration 79/1000 | Loss: 0.00002432
Iteration 80/1000 | Loss: 0.00002432
Iteration 81/1000 | Loss: 0.00002432
Iteration 82/1000 | Loss: 0.00002432
Iteration 83/1000 | Loss: 0.00002432
Iteration 84/1000 | Loss: 0.00002432
Iteration 85/1000 | Loss: 0.00002431
Iteration 86/1000 | Loss: 0.00002431
Iteration 87/1000 | Loss: 0.00002431
Iteration 88/1000 | Loss: 0.00002430
Iteration 89/1000 | Loss: 0.00002430
Iteration 90/1000 | Loss: 0.00002430
Iteration 91/1000 | Loss: 0.00002430
Iteration 92/1000 | Loss: 0.00002430
Iteration 93/1000 | Loss: 0.00002430
Iteration 94/1000 | Loss: 0.00002429
Iteration 95/1000 | Loss: 0.00002429
Iteration 96/1000 | Loss: 0.00002429
Iteration 97/1000 | Loss: 0.00002429
Iteration 98/1000 | Loss: 0.00002429
Iteration 99/1000 | Loss: 0.00002429
Iteration 100/1000 | Loss: 0.00002429
Iteration 101/1000 | Loss: 0.00002429
Iteration 102/1000 | Loss: 0.00002429
Iteration 103/1000 | Loss: 0.00002429
Iteration 104/1000 | Loss: 0.00002429
Iteration 105/1000 | Loss: 0.00002429
Iteration 106/1000 | Loss: 0.00002428
Iteration 107/1000 | Loss: 0.00002428
Iteration 108/1000 | Loss: 0.00002427
Iteration 109/1000 | Loss: 0.00002427
Iteration 110/1000 | Loss: 0.00002427
Iteration 111/1000 | Loss: 0.00002426
Iteration 112/1000 | Loss: 0.00002426
Iteration 113/1000 | Loss: 0.00002426
Iteration 114/1000 | Loss: 0.00002425
Iteration 115/1000 | Loss: 0.00002425
Iteration 116/1000 | Loss: 0.00002425
Iteration 117/1000 | Loss: 0.00002425
Iteration 118/1000 | Loss: 0.00002424
Iteration 119/1000 | Loss: 0.00002424
Iteration 120/1000 | Loss: 0.00002423
Iteration 121/1000 | Loss: 0.00002423
Iteration 122/1000 | Loss: 0.00002423
Iteration 123/1000 | Loss: 0.00002423
Iteration 124/1000 | Loss: 0.00002422
Iteration 125/1000 | Loss: 0.00002422
Iteration 126/1000 | Loss: 0.00002421
Iteration 127/1000 | Loss: 0.00002421
Iteration 128/1000 | Loss: 0.00002421
Iteration 129/1000 | Loss: 0.00002421
Iteration 130/1000 | Loss: 0.00002420
Iteration 131/1000 | Loss: 0.00002420
Iteration 132/1000 | Loss: 0.00002420
Iteration 133/1000 | Loss: 0.00002420
Iteration 134/1000 | Loss: 0.00002420
Iteration 135/1000 | Loss: 0.00002420
Iteration 136/1000 | Loss: 0.00002420
Iteration 137/1000 | Loss: 0.00002419
Iteration 138/1000 | Loss: 0.00002419
Iteration 139/1000 | Loss: 0.00002419
Iteration 140/1000 | Loss: 0.00002419
Iteration 141/1000 | Loss: 0.00002418
Iteration 142/1000 | Loss: 0.00002418
Iteration 143/1000 | Loss: 0.00002418
Iteration 144/1000 | Loss: 0.00002418
Iteration 145/1000 | Loss: 0.00002418
Iteration 146/1000 | Loss: 0.00002418
Iteration 147/1000 | Loss: 0.00002418
Iteration 148/1000 | Loss: 0.00002418
Iteration 149/1000 | Loss: 0.00002417
Iteration 150/1000 | Loss: 0.00002417
Iteration 151/1000 | Loss: 0.00002417
Iteration 152/1000 | Loss: 0.00002417
Iteration 153/1000 | Loss: 0.00002417
Iteration 154/1000 | Loss: 0.00002417
Iteration 155/1000 | Loss: 0.00002417
Iteration 156/1000 | Loss: 0.00002417
Iteration 157/1000 | Loss: 0.00002417
Iteration 158/1000 | Loss: 0.00002417
Iteration 159/1000 | Loss: 0.00002417
Iteration 160/1000 | Loss: 0.00002417
Iteration 161/1000 | Loss: 0.00002416
Iteration 162/1000 | Loss: 0.00002416
Iteration 163/1000 | Loss: 0.00002416
Iteration 164/1000 | Loss: 0.00002416
Iteration 165/1000 | Loss: 0.00002416
Iteration 166/1000 | Loss: 0.00002416
Iteration 167/1000 | Loss: 0.00002416
Iteration 168/1000 | Loss: 0.00002416
Iteration 169/1000 | Loss: 0.00002416
Iteration 170/1000 | Loss: 0.00002416
Iteration 171/1000 | Loss: 0.00002415
Iteration 172/1000 | Loss: 0.00002415
Iteration 173/1000 | Loss: 0.00002415
Iteration 174/1000 | Loss: 0.00002415
Iteration 175/1000 | Loss: 0.00002415
Iteration 176/1000 | Loss: 0.00002415
Iteration 177/1000 | Loss: 0.00002414
Iteration 178/1000 | Loss: 0.00002414
Iteration 179/1000 | Loss: 0.00002414
Iteration 180/1000 | Loss: 0.00002414
Iteration 181/1000 | Loss: 0.00002414
Iteration 182/1000 | Loss: 0.00002414
Iteration 183/1000 | Loss: 0.00002414
Iteration 184/1000 | Loss: 0.00002414
Iteration 185/1000 | Loss: 0.00002414
Iteration 186/1000 | Loss: 0.00002414
Iteration 187/1000 | Loss: 0.00002414
Iteration 188/1000 | Loss: 0.00002413
Iteration 189/1000 | Loss: 0.00002413
Iteration 190/1000 | Loss: 0.00002413
Iteration 191/1000 | Loss: 0.00002413
Iteration 192/1000 | Loss: 0.00002413
Iteration 193/1000 | Loss: 0.00002413
Iteration 194/1000 | Loss: 0.00002412
Iteration 195/1000 | Loss: 0.00002412
Iteration 196/1000 | Loss: 0.00002412
Iteration 197/1000 | Loss: 0.00002412
Iteration 198/1000 | Loss: 0.00002412
Iteration 199/1000 | Loss: 0.00002412
Iteration 200/1000 | Loss: 0.00002412
Iteration 201/1000 | Loss: 0.00002412
Iteration 202/1000 | Loss: 0.00002412
Iteration 203/1000 | Loss: 0.00002412
Iteration 204/1000 | Loss: 0.00002412
Iteration 205/1000 | Loss: 0.00002412
Iteration 206/1000 | Loss: 0.00002412
Iteration 207/1000 | Loss: 0.00002412
Iteration 208/1000 | Loss: 0.00002412
Iteration 209/1000 | Loss: 0.00002412
Iteration 210/1000 | Loss: 0.00002412
Iteration 211/1000 | Loss: 0.00002411
Iteration 212/1000 | Loss: 0.00002411
Iteration 213/1000 | Loss: 0.00002411
Iteration 214/1000 | Loss: 0.00002411
Iteration 215/1000 | Loss: 0.00002411
Iteration 216/1000 | Loss: 0.00002411
Iteration 217/1000 | Loss: 0.00002411
Iteration 218/1000 | Loss: 0.00002411
Iteration 219/1000 | Loss: 0.00002411
Iteration 220/1000 | Loss: 0.00002411
Iteration 221/1000 | Loss: 0.00002411
Iteration 222/1000 | Loss: 0.00002411
Iteration 223/1000 | Loss: 0.00002411
Iteration 224/1000 | Loss: 0.00002411
Iteration 225/1000 | Loss: 0.00002411
Iteration 226/1000 | Loss: 0.00002411
Iteration 227/1000 | Loss: 0.00002411
Iteration 228/1000 | Loss: 0.00002411
Iteration 229/1000 | Loss: 0.00002411
Iteration 230/1000 | Loss: 0.00002411
Iteration 231/1000 | Loss: 0.00002411
Iteration 232/1000 | Loss: 0.00002411
Iteration 233/1000 | Loss: 0.00002411
Iteration 234/1000 | Loss: 0.00002411
Iteration 235/1000 | Loss: 0.00002411
Iteration 236/1000 | Loss: 0.00002411
Iteration 237/1000 | Loss: 0.00002411
Iteration 238/1000 | Loss: 0.00002411
Iteration 239/1000 | Loss: 0.00002411
Iteration 240/1000 | Loss: 0.00002411
Iteration 241/1000 | Loss: 0.00002411
Iteration 242/1000 | Loss: 0.00002411
Iteration 243/1000 | Loss: 0.00002411
Iteration 244/1000 | Loss: 0.00002411
Iteration 245/1000 | Loss: 0.00002411
Iteration 246/1000 | Loss: 0.00002411
Iteration 247/1000 | Loss: 0.00002411
Iteration 248/1000 | Loss: 0.00002411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [2.4111048332997598e-05, 2.4111048332997598e-05, 2.4111048332997598e-05, 2.4111048332997598e-05, 2.4111048332997598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4111048332997598e-05

Optimization complete. Final v2v error: 4.124129295349121 mm

Highest mean error: 4.561922550201416 mm for frame 266

Lowest mean error: 3.6524100303649902 mm for frame 0

Saving results

Total time: 64.89946985244751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036562
Iteration 2/25 | Loss: 0.00146482
Iteration 3/25 | Loss: 0.00120088
Iteration 4/25 | Loss: 0.00115623
Iteration 5/25 | Loss: 0.00114061
Iteration 6/25 | Loss: 0.00113632
Iteration 7/25 | Loss: 0.00113570
Iteration 8/25 | Loss: 0.00113570
Iteration 9/25 | Loss: 0.00113570
Iteration 10/25 | Loss: 0.00113570
Iteration 11/25 | Loss: 0.00113570
Iteration 12/25 | Loss: 0.00113570
Iteration 13/25 | Loss: 0.00113570
Iteration 14/25 | Loss: 0.00113570
Iteration 15/25 | Loss: 0.00113570
Iteration 16/25 | Loss: 0.00113570
Iteration 17/25 | Loss: 0.00113570
Iteration 18/25 | Loss: 0.00113570
Iteration 19/25 | Loss: 0.00113570
Iteration 20/25 | Loss: 0.00113570
Iteration 21/25 | Loss: 0.00113570
Iteration 22/25 | Loss: 0.00113570
Iteration 23/25 | Loss: 0.00113570
Iteration 24/25 | Loss: 0.00113570
Iteration 25/25 | Loss: 0.00113570

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89784545
Iteration 2/25 | Loss: 0.00107940
Iteration 3/25 | Loss: 0.00107920
Iteration 4/25 | Loss: 0.00107920
Iteration 5/25 | Loss: 0.00107920
Iteration 6/25 | Loss: 0.00107920
Iteration 7/25 | Loss: 0.00107920
Iteration 8/25 | Loss: 0.00107920
Iteration 9/25 | Loss: 0.00107920
Iteration 10/25 | Loss: 0.00107920
Iteration 11/25 | Loss: 0.00107920
Iteration 12/25 | Loss: 0.00107920
Iteration 13/25 | Loss: 0.00107920
Iteration 14/25 | Loss: 0.00107920
Iteration 15/25 | Loss: 0.00107920
Iteration 16/25 | Loss: 0.00107920
Iteration 17/25 | Loss: 0.00107920
Iteration 18/25 | Loss: 0.00107920
Iteration 19/25 | Loss: 0.00107920
Iteration 20/25 | Loss: 0.00107920
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010791996028274298, 0.0010791996028274298, 0.0010791996028274298, 0.0010791996028274298, 0.0010791996028274298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010791996028274298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107920
Iteration 2/1000 | Loss: 0.00009245
Iteration 3/1000 | Loss: 0.00006585
Iteration 4/1000 | Loss: 0.00005656
Iteration 5/1000 | Loss: 0.00005347
Iteration 6/1000 | Loss: 0.00005169
Iteration 7/1000 | Loss: 0.00005009
Iteration 8/1000 | Loss: 0.00004902
Iteration 9/1000 | Loss: 0.00004809
Iteration 10/1000 | Loss: 0.00004713
Iteration 11/1000 | Loss: 0.00004647
Iteration 12/1000 | Loss: 0.00004602
Iteration 13/1000 | Loss: 0.00004566
Iteration 14/1000 | Loss: 0.00004534
Iteration 15/1000 | Loss: 0.00004512
Iteration 16/1000 | Loss: 0.00004489
Iteration 17/1000 | Loss: 0.00004470
Iteration 18/1000 | Loss: 0.00004459
Iteration 19/1000 | Loss: 0.00004456
Iteration 20/1000 | Loss: 0.00004437
Iteration 21/1000 | Loss: 0.00004420
Iteration 22/1000 | Loss: 0.00004406
Iteration 23/1000 | Loss: 0.00004396
Iteration 24/1000 | Loss: 0.00004393
Iteration 25/1000 | Loss: 0.00004393
Iteration 26/1000 | Loss: 0.00004392
Iteration 27/1000 | Loss: 0.00004392
Iteration 28/1000 | Loss: 0.00004392
Iteration 29/1000 | Loss: 0.00004391
Iteration 30/1000 | Loss: 0.00004391
Iteration 31/1000 | Loss: 0.00004390
Iteration 32/1000 | Loss: 0.00004390
Iteration 33/1000 | Loss: 0.00004389
Iteration 34/1000 | Loss: 0.00004388
Iteration 35/1000 | Loss: 0.00004388
Iteration 36/1000 | Loss: 0.00004387
Iteration 37/1000 | Loss: 0.00004387
Iteration 38/1000 | Loss: 0.00004387
Iteration 39/1000 | Loss: 0.00004386
Iteration 40/1000 | Loss: 0.00004386
Iteration 41/1000 | Loss: 0.00004386
Iteration 42/1000 | Loss: 0.00004385
Iteration 43/1000 | Loss: 0.00004385
Iteration 44/1000 | Loss: 0.00004385
Iteration 45/1000 | Loss: 0.00004385
Iteration 46/1000 | Loss: 0.00004382
Iteration 47/1000 | Loss: 0.00004382
Iteration 48/1000 | Loss: 0.00004382
Iteration 49/1000 | Loss: 0.00004382
Iteration 50/1000 | Loss: 0.00004382
Iteration 51/1000 | Loss: 0.00004382
Iteration 52/1000 | Loss: 0.00004382
Iteration 53/1000 | Loss: 0.00004382
Iteration 54/1000 | Loss: 0.00004382
Iteration 55/1000 | Loss: 0.00004382
Iteration 56/1000 | Loss: 0.00004381
Iteration 57/1000 | Loss: 0.00004380
Iteration 58/1000 | Loss: 0.00004379
Iteration 59/1000 | Loss: 0.00004379
Iteration 60/1000 | Loss: 0.00004378
Iteration 61/1000 | Loss: 0.00004378
Iteration 62/1000 | Loss: 0.00004378
Iteration 63/1000 | Loss: 0.00004378
Iteration 64/1000 | Loss: 0.00004377
Iteration 65/1000 | Loss: 0.00004377
Iteration 66/1000 | Loss: 0.00004376
Iteration 67/1000 | Loss: 0.00004374
Iteration 68/1000 | Loss: 0.00004374
Iteration 69/1000 | Loss: 0.00004374
Iteration 70/1000 | Loss: 0.00004373
Iteration 71/1000 | Loss: 0.00004372
Iteration 72/1000 | Loss: 0.00004372
Iteration 73/1000 | Loss: 0.00004372
Iteration 74/1000 | Loss: 0.00004372
Iteration 75/1000 | Loss: 0.00004371
Iteration 76/1000 | Loss: 0.00004371
Iteration 77/1000 | Loss: 0.00004371
Iteration 78/1000 | Loss: 0.00004371
Iteration 79/1000 | Loss: 0.00004371
Iteration 80/1000 | Loss: 0.00004371
Iteration 81/1000 | Loss: 0.00004371
Iteration 82/1000 | Loss: 0.00004371
Iteration 83/1000 | Loss: 0.00004371
Iteration 84/1000 | Loss: 0.00004371
Iteration 85/1000 | Loss: 0.00004371
Iteration 86/1000 | Loss: 0.00004371
Iteration 87/1000 | Loss: 0.00004370
Iteration 88/1000 | Loss: 0.00004369
Iteration 89/1000 | Loss: 0.00004369
Iteration 90/1000 | Loss: 0.00004368
Iteration 91/1000 | Loss: 0.00004368
Iteration 92/1000 | Loss: 0.00004368
Iteration 93/1000 | Loss: 0.00004368
Iteration 94/1000 | Loss: 0.00004368
Iteration 95/1000 | Loss: 0.00004367
Iteration 96/1000 | Loss: 0.00004367
Iteration 97/1000 | Loss: 0.00004367
Iteration 98/1000 | Loss: 0.00004367
Iteration 99/1000 | Loss: 0.00004367
Iteration 100/1000 | Loss: 0.00004367
Iteration 101/1000 | Loss: 0.00004367
Iteration 102/1000 | Loss: 0.00004367
Iteration 103/1000 | Loss: 0.00004367
Iteration 104/1000 | Loss: 0.00004367
Iteration 105/1000 | Loss: 0.00004367
Iteration 106/1000 | Loss: 0.00004366
Iteration 107/1000 | Loss: 0.00004366
Iteration 108/1000 | Loss: 0.00004366
Iteration 109/1000 | Loss: 0.00004366
Iteration 110/1000 | Loss: 0.00004365
Iteration 111/1000 | Loss: 0.00004365
Iteration 112/1000 | Loss: 0.00004365
Iteration 113/1000 | Loss: 0.00004364
Iteration 114/1000 | Loss: 0.00004364
Iteration 115/1000 | Loss: 0.00004364
Iteration 116/1000 | Loss: 0.00004364
Iteration 117/1000 | Loss: 0.00004364
Iteration 118/1000 | Loss: 0.00004364
Iteration 119/1000 | Loss: 0.00004363
Iteration 120/1000 | Loss: 0.00004363
Iteration 121/1000 | Loss: 0.00004363
Iteration 122/1000 | Loss: 0.00004363
Iteration 123/1000 | Loss: 0.00004363
Iteration 124/1000 | Loss: 0.00004363
Iteration 125/1000 | Loss: 0.00004363
Iteration 126/1000 | Loss: 0.00004363
Iteration 127/1000 | Loss: 0.00004362
Iteration 128/1000 | Loss: 0.00004362
Iteration 129/1000 | Loss: 0.00004362
Iteration 130/1000 | Loss: 0.00004362
Iteration 131/1000 | Loss: 0.00004362
Iteration 132/1000 | Loss: 0.00004362
Iteration 133/1000 | Loss: 0.00004361
Iteration 134/1000 | Loss: 0.00004361
Iteration 135/1000 | Loss: 0.00004361
Iteration 136/1000 | Loss: 0.00004361
Iteration 137/1000 | Loss: 0.00004360
Iteration 138/1000 | Loss: 0.00004360
Iteration 139/1000 | Loss: 0.00004360
Iteration 140/1000 | Loss: 0.00004360
Iteration 141/1000 | Loss: 0.00004360
Iteration 142/1000 | Loss: 0.00004360
Iteration 143/1000 | Loss: 0.00004360
Iteration 144/1000 | Loss: 0.00004360
Iteration 145/1000 | Loss: 0.00004360
Iteration 146/1000 | Loss: 0.00004360
Iteration 147/1000 | Loss: 0.00004360
Iteration 148/1000 | Loss: 0.00004360
Iteration 149/1000 | Loss: 0.00004360
Iteration 150/1000 | Loss: 0.00004360
Iteration 151/1000 | Loss: 0.00004359
Iteration 152/1000 | Loss: 0.00004359
Iteration 153/1000 | Loss: 0.00004359
Iteration 154/1000 | Loss: 0.00004359
Iteration 155/1000 | Loss: 0.00004359
Iteration 156/1000 | Loss: 0.00004359
Iteration 157/1000 | Loss: 0.00004359
Iteration 158/1000 | Loss: 0.00004359
Iteration 159/1000 | Loss: 0.00004359
Iteration 160/1000 | Loss: 0.00004358
Iteration 161/1000 | Loss: 0.00004358
Iteration 162/1000 | Loss: 0.00004358
Iteration 163/1000 | Loss: 0.00004358
Iteration 164/1000 | Loss: 0.00004358
Iteration 165/1000 | Loss: 0.00004358
Iteration 166/1000 | Loss: 0.00004358
Iteration 167/1000 | Loss: 0.00004358
Iteration 168/1000 | Loss: 0.00004358
Iteration 169/1000 | Loss: 0.00004358
Iteration 170/1000 | Loss: 0.00004358
Iteration 171/1000 | Loss: 0.00004358
Iteration 172/1000 | Loss: 0.00004358
Iteration 173/1000 | Loss: 0.00004357
Iteration 174/1000 | Loss: 0.00004357
Iteration 175/1000 | Loss: 0.00004357
Iteration 176/1000 | Loss: 0.00004357
Iteration 177/1000 | Loss: 0.00004357
Iteration 178/1000 | Loss: 0.00004357
Iteration 179/1000 | Loss: 0.00004357
Iteration 180/1000 | Loss: 0.00004357
Iteration 181/1000 | Loss: 0.00004357
Iteration 182/1000 | Loss: 0.00004357
Iteration 183/1000 | Loss: 0.00004357
Iteration 184/1000 | Loss: 0.00004357
Iteration 185/1000 | Loss: 0.00004357
Iteration 186/1000 | Loss: 0.00004357
Iteration 187/1000 | Loss: 0.00004357
Iteration 188/1000 | Loss: 0.00004357
Iteration 189/1000 | Loss: 0.00004357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [4.356827412266284e-05, 4.356827412266284e-05, 4.356827412266284e-05, 4.356827412266284e-05, 4.356827412266284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.356827412266284e-05

Optimization complete. Final v2v error: 5.333765029907227 mm

Highest mean error: 6.094878673553467 mm for frame 182

Lowest mean error: 4.671067237854004 mm for frame 151

Saving results

Total time: 61.44848942756653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039847
Iteration 2/25 | Loss: 0.00272019
Iteration 3/25 | Loss: 0.00199494
Iteration 4/25 | Loss: 0.00172584
Iteration 5/25 | Loss: 0.00166567
Iteration 6/25 | Loss: 0.00160911
Iteration 7/25 | Loss: 0.00157800
Iteration 8/25 | Loss: 0.00148901
Iteration 9/25 | Loss: 0.00143112
Iteration 10/25 | Loss: 0.00137118
Iteration 11/25 | Loss: 0.00132815
Iteration 12/25 | Loss: 0.00130767
Iteration 13/25 | Loss: 0.00130035
Iteration 14/25 | Loss: 0.00127966
Iteration 15/25 | Loss: 0.00127698
Iteration 16/25 | Loss: 0.00127872
Iteration 17/25 | Loss: 0.00126874
Iteration 18/25 | Loss: 0.00125788
Iteration 19/25 | Loss: 0.00125327
Iteration 20/25 | Loss: 0.00125209
Iteration 21/25 | Loss: 0.00125184
Iteration 22/25 | Loss: 0.00125178
Iteration 23/25 | Loss: 0.00125178
Iteration 24/25 | Loss: 0.00125178
Iteration 25/25 | Loss: 0.00125178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25713956
Iteration 2/25 | Loss: 0.00295068
Iteration 3/25 | Loss: 0.00245229
Iteration 4/25 | Loss: 0.00245229
Iteration 5/25 | Loss: 0.00245229
Iteration 6/25 | Loss: 0.00245229
Iteration 7/25 | Loss: 0.00245229
Iteration 8/25 | Loss: 0.00245229
Iteration 9/25 | Loss: 0.00245229
Iteration 10/25 | Loss: 0.00245229
Iteration 11/25 | Loss: 0.00245229
Iteration 12/25 | Loss: 0.00245229
Iteration 13/25 | Loss: 0.00245229
Iteration 14/25 | Loss: 0.00245229
Iteration 15/25 | Loss: 0.00245229
Iteration 16/25 | Loss: 0.00245229
Iteration 17/25 | Loss: 0.00245229
Iteration 18/25 | Loss: 0.00245229
Iteration 19/25 | Loss: 0.00245229
Iteration 20/25 | Loss: 0.00245229
Iteration 21/25 | Loss: 0.00245229
Iteration 22/25 | Loss: 0.00245229
Iteration 23/25 | Loss: 0.00245229
Iteration 24/25 | Loss: 0.00245229
Iteration 25/25 | Loss: 0.00245229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245229
Iteration 2/1000 | Loss: 0.00072011
Iteration 3/1000 | Loss: 0.00038479
Iteration 4/1000 | Loss: 0.00030914
Iteration 5/1000 | Loss: 0.00017719
Iteration 6/1000 | Loss: 0.00012640
Iteration 7/1000 | Loss: 0.00011608
Iteration 8/1000 | Loss: 0.00025085
Iteration 9/1000 | Loss: 0.00011420
Iteration 10/1000 | Loss: 0.00021704
Iteration 11/1000 | Loss: 0.00065577
Iteration 12/1000 | Loss: 0.00032399
Iteration 13/1000 | Loss: 0.00009902
Iteration 14/1000 | Loss: 0.00025210
Iteration 15/1000 | Loss: 0.00009670
Iteration 16/1000 | Loss: 0.00015376
Iteration 17/1000 | Loss: 0.00028599
Iteration 18/1000 | Loss: 0.00009356
Iteration 19/1000 | Loss: 0.00009129
Iteration 20/1000 | Loss: 0.00008957
Iteration 21/1000 | Loss: 0.00008754
Iteration 22/1000 | Loss: 0.00008607
Iteration 23/1000 | Loss: 0.00008525
Iteration 24/1000 | Loss: 0.00018882
Iteration 25/1000 | Loss: 0.00015794
Iteration 26/1000 | Loss: 0.00010141
Iteration 27/1000 | Loss: 0.00008412
Iteration 28/1000 | Loss: 0.00008374
Iteration 29/1000 | Loss: 0.00008343
Iteration 30/1000 | Loss: 0.00018167
Iteration 31/1000 | Loss: 0.00014911
Iteration 32/1000 | Loss: 0.00030884
Iteration 33/1000 | Loss: 0.00152680
Iteration 34/1000 | Loss: 0.00406518
Iteration 35/1000 | Loss: 0.00102202
Iteration 36/1000 | Loss: 0.00025574
Iteration 37/1000 | Loss: 0.00035649
Iteration 38/1000 | Loss: 0.00028062
Iteration 39/1000 | Loss: 0.00040265
Iteration 40/1000 | Loss: 0.00023784
Iteration 41/1000 | Loss: 0.00021445
Iteration 42/1000 | Loss: 0.00021685
Iteration 43/1000 | Loss: 0.00016320
Iteration 44/1000 | Loss: 0.00008804
Iteration 45/1000 | Loss: 0.00016853
Iteration 46/1000 | Loss: 0.00010266
Iteration 47/1000 | Loss: 0.00014406
Iteration 48/1000 | Loss: 0.00019423
Iteration 49/1000 | Loss: 0.00037803
Iteration 50/1000 | Loss: 0.00017526
Iteration 51/1000 | Loss: 0.00015754
Iteration 52/1000 | Loss: 0.00009340
Iteration 53/1000 | Loss: 0.00009877
Iteration 54/1000 | Loss: 0.00018244
Iteration 55/1000 | Loss: 0.00005537
Iteration 56/1000 | Loss: 0.00004123
Iteration 57/1000 | Loss: 0.00003980
Iteration 58/1000 | Loss: 0.00026941
Iteration 59/1000 | Loss: 0.00009933
Iteration 60/1000 | Loss: 0.00016818
Iteration 61/1000 | Loss: 0.00016771
Iteration 62/1000 | Loss: 0.00013679
Iteration 63/1000 | Loss: 0.00031639
Iteration 64/1000 | Loss: 0.00023058
Iteration 65/1000 | Loss: 0.00015101
Iteration 66/1000 | Loss: 0.00021461
Iteration 67/1000 | Loss: 0.00013593
Iteration 68/1000 | Loss: 0.00048482
Iteration 69/1000 | Loss: 0.00006591
Iteration 70/1000 | Loss: 0.00011037
Iteration 71/1000 | Loss: 0.00009180
Iteration 72/1000 | Loss: 0.00003574
Iteration 73/1000 | Loss: 0.00003320
Iteration 74/1000 | Loss: 0.00003259
Iteration 75/1000 | Loss: 0.00011535
Iteration 76/1000 | Loss: 0.00003197
Iteration 77/1000 | Loss: 0.00003145
Iteration 78/1000 | Loss: 0.00003114
Iteration 79/1000 | Loss: 0.00003080
Iteration 80/1000 | Loss: 0.00003048
Iteration 81/1000 | Loss: 0.00003037
Iteration 82/1000 | Loss: 0.00003033
Iteration 83/1000 | Loss: 0.00003027
Iteration 84/1000 | Loss: 0.00003024
Iteration 85/1000 | Loss: 0.00003024
Iteration 86/1000 | Loss: 0.00003023
Iteration 87/1000 | Loss: 0.00003022
Iteration 88/1000 | Loss: 0.00003022
Iteration 89/1000 | Loss: 0.00003021
Iteration 90/1000 | Loss: 0.00003021
Iteration 91/1000 | Loss: 0.00003021
Iteration 92/1000 | Loss: 0.00003020
Iteration 93/1000 | Loss: 0.00003019
Iteration 94/1000 | Loss: 0.00003019
Iteration 95/1000 | Loss: 0.00003018
Iteration 96/1000 | Loss: 0.00003018
Iteration 97/1000 | Loss: 0.00003018
Iteration 98/1000 | Loss: 0.00003017
Iteration 99/1000 | Loss: 0.00003017
Iteration 100/1000 | Loss: 0.00003017
Iteration 101/1000 | Loss: 0.00003017
Iteration 102/1000 | Loss: 0.00003017
Iteration 103/1000 | Loss: 0.00003015
Iteration 104/1000 | Loss: 0.00003015
Iteration 105/1000 | Loss: 0.00003015
Iteration 106/1000 | Loss: 0.00003014
Iteration 107/1000 | Loss: 0.00003014
Iteration 108/1000 | Loss: 0.00003014
Iteration 109/1000 | Loss: 0.00003014
Iteration 110/1000 | Loss: 0.00003014
Iteration 111/1000 | Loss: 0.00003014
Iteration 112/1000 | Loss: 0.00003014
Iteration 113/1000 | Loss: 0.00003014
Iteration 114/1000 | Loss: 0.00003013
Iteration 115/1000 | Loss: 0.00003013
Iteration 116/1000 | Loss: 0.00003013
Iteration 117/1000 | Loss: 0.00003013
Iteration 118/1000 | Loss: 0.00003013
Iteration 119/1000 | Loss: 0.00003013
Iteration 120/1000 | Loss: 0.00003012
Iteration 121/1000 | Loss: 0.00003012
Iteration 122/1000 | Loss: 0.00003012
Iteration 123/1000 | Loss: 0.00003012
Iteration 124/1000 | Loss: 0.00003011
Iteration 125/1000 | Loss: 0.00003011
Iteration 126/1000 | Loss: 0.00003011
Iteration 127/1000 | Loss: 0.00003011
Iteration 128/1000 | Loss: 0.00003011
Iteration 129/1000 | Loss: 0.00003011
Iteration 130/1000 | Loss: 0.00003011
Iteration 131/1000 | Loss: 0.00003010
Iteration 132/1000 | Loss: 0.00003010
Iteration 133/1000 | Loss: 0.00003010
Iteration 134/1000 | Loss: 0.00003010
Iteration 135/1000 | Loss: 0.00003010
Iteration 136/1000 | Loss: 0.00003009
Iteration 137/1000 | Loss: 0.00003009
Iteration 138/1000 | Loss: 0.00003009
Iteration 139/1000 | Loss: 0.00003009
Iteration 140/1000 | Loss: 0.00003009
Iteration 141/1000 | Loss: 0.00003009
Iteration 142/1000 | Loss: 0.00003009
Iteration 143/1000 | Loss: 0.00003009
Iteration 144/1000 | Loss: 0.00003009
Iteration 145/1000 | Loss: 0.00003009
Iteration 146/1000 | Loss: 0.00003009
Iteration 147/1000 | Loss: 0.00003009
Iteration 148/1000 | Loss: 0.00003009
Iteration 149/1000 | Loss: 0.00003008
Iteration 150/1000 | Loss: 0.00003008
Iteration 151/1000 | Loss: 0.00003008
Iteration 152/1000 | Loss: 0.00003008
Iteration 153/1000 | Loss: 0.00003008
Iteration 154/1000 | Loss: 0.00003008
Iteration 155/1000 | Loss: 0.00003008
Iteration 156/1000 | Loss: 0.00003008
Iteration 157/1000 | Loss: 0.00003008
Iteration 158/1000 | Loss: 0.00003008
Iteration 159/1000 | Loss: 0.00003008
Iteration 160/1000 | Loss: 0.00003008
Iteration 161/1000 | Loss: 0.00003008
Iteration 162/1000 | Loss: 0.00003008
Iteration 163/1000 | Loss: 0.00003007
Iteration 164/1000 | Loss: 0.00003007
Iteration 165/1000 | Loss: 0.00003007
Iteration 166/1000 | Loss: 0.00003007
Iteration 167/1000 | Loss: 0.00003007
Iteration 168/1000 | Loss: 0.00003007
Iteration 169/1000 | Loss: 0.00003007
Iteration 170/1000 | Loss: 0.00003007
Iteration 171/1000 | Loss: 0.00003007
Iteration 172/1000 | Loss: 0.00003007
Iteration 173/1000 | Loss: 0.00003007
Iteration 174/1000 | Loss: 0.00003007
Iteration 175/1000 | Loss: 0.00003007
Iteration 176/1000 | Loss: 0.00003007
Iteration 177/1000 | Loss: 0.00003007
Iteration 178/1000 | Loss: 0.00003007
Iteration 179/1000 | Loss: 0.00003007
Iteration 180/1000 | Loss: 0.00003006
Iteration 181/1000 | Loss: 0.00003006
Iteration 182/1000 | Loss: 0.00003006
Iteration 183/1000 | Loss: 0.00003006
Iteration 184/1000 | Loss: 0.00003006
Iteration 185/1000 | Loss: 0.00003006
Iteration 186/1000 | Loss: 0.00003006
Iteration 187/1000 | Loss: 0.00003006
Iteration 188/1000 | Loss: 0.00003006
Iteration 189/1000 | Loss: 0.00003006
Iteration 190/1000 | Loss: 0.00003006
Iteration 191/1000 | Loss: 0.00003006
Iteration 192/1000 | Loss: 0.00003006
Iteration 193/1000 | Loss: 0.00003006
Iteration 194/1000 | Loss: 0.00003006
Iteration 195/1000 | Loss: 0.00003006
Iteration 196/1000 | Loss: 0.00003006
Iteration 197/1000 | Loss: 0.00003006
Iteration 198/1000 | Loss: 0.00003006
Iteration 199/1000 | Loss: 0.00003006
Iteration 200/1000 | Loss: 0.00003006
Iteration 201/1000 | Loss: 0.00003006
Iteration 202/1000 | Loss: 0.00003006
Iteration 203/1000 | Loss: 0.00003006
Iteration 204/1000 | Loss: 0.00003006
Iteration 205/1000 | Loss: 0.00003006
Iteration 206/1000 | Loss: 0.00003006
Iteration 207/1000 | Loss: 0.00003006
Iteration 208/1000 | Loss: 0.00003006
Iteration 209/1000 | Loss: 0.00003006
Iteration 210/1000 | Loss: 0.00003006
Iteration 211/1000 | Loss: 0.00003006
Iteration 212/1000 | Loss: 0.00003006
Iteration 213/1000 | Loss: 0.00003006
Iteration 214/1000 | Loss: 0.00003006
Iteration 215/1000 | Loss: 0.00003006
Iteration 216/1000 | Loss: 0.00003006
Iteration 217/1000 | Loss: 0.00003006
Iteration 218/1000 | Loss: 0.00003006
Iteration 219/1000 | Loss: 0.00003006
Iteration 220/1000 | Loss: 0.00003006
Iteration 221/1000 | Loss: 0.00003006
Iteration 222/1000 | Loss: 0.00003006
Iteration 223/1000 | Loss: 0.00003006
Iteration 224/1000 | Loss: 0.00003006
Iteration 225/1000 | Loss: 0.00003006
Iteration 226/1000 | Loss: 0.00003006
Iteration 227/1000 | Loss: 0.00003006
Iteration 228/1000 | Loss: 0.00003006
Iteration 229/1000 | Loss: 0.00003006
Iteration 230/1000 | Loss: 0.00003006
Iteration 231/1000 | Loss: 0.00003006
Iteration 232/1000 | Loss: 0.00003006
Iteration 233/1000 | Loss: 0.00003006
Iteration 234/1000 | Loss: 0.00003006
Iteration 235/1000 | Loss: 0.00003006
Iteration 236/1000 | Loss: 0.00003006
Iteration 237/1000 | Loss: 0.00003006
Iteration 238/1000 | Loss: 0.00003006
Iteration 239/1000 | Loss: 0.00003006
Iteration 240/1000 | Loss: 0.00003006
Iteration 241/1000 | Loss: 0.00003006
Iteration 242/1000 | Loss: 0.00003006
Iteration 243/1000 | Loss: 0.00003006
Iteration 244/1000 | Loss: 0.00003006
Iteration 245/1000 | Loss: 0.00003006
Iteration 246/1000 | Loss: 0.00003006
Iteration 247/1000 | Loss: 0.00003006
Iteration 248/1000 | Loss: 0.00003006
Iteration 249/1000 | Loss: 0.00003006
Iteration 250/1000 | Loss: 0.00003006
Iteration 251/1000 | Loss: 0.00003006
Iteration 252/1000 | Loss: 0.00003006
Iteration 253/1000 | Loss: 0.00003006
Iteration 254/1000 | Loss: 0.00003006
Iteration 255/1000 | Loss: 0.00003006
Iteration 256/1000 | Loss: 0.00003006
Iteration 257/1000 | Loss: 0.00003006
Iteration 258/1000 | Loss: 0.00003006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [3.005503094755113e-05, 3.005503094755113e-05, 3.005503094755113e-05, 3.005503094755113e-05, 3.005503094755113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.005503094755113e-05

Optimization complete. Final v2v error: 3.467020034790039 mm

Highest mean error: 5.344142913818359 mm for frame 55

Lowest mean error: 2.2631561756134033 mm for frame 151

Saving results

Total time: 160.77713799476624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934381
Iteration 2/25 | Loss: 0.00280373
Iteration 3/25 | Loss: 0.00180163
Iteration 4/25 | Loss: 0.00175205
Iteration 5/25 | Loss: 0.00148776
Iteration 6/25 | Loss: 0.00139536
Iteration 7/25 | Loss: 0.00138770
Iteration 8/25 | Loss: 0.00136150
Iteration 9/25 | Loss: 0.00134892
Iteration 10/25 | Loss: 0.00134457
Iteration 11/25 | Loss: 0.00134090
Iteration 12/25 | Loss: 0.00134088
Iteration 13/25 | Loss: 0.00133730
Iteration 14/25 | Loss: 0.00133605
Iteration 15/25 | Loss: 0.00133366
Iteration 16/25 | Loss: 0.00134889
Iteration 17/25 | Loss: 0.00132398
Iteration 18/25 | Loss: 0.00132083
Iteration 19/25 | Loss: 0.00131811
Iteration 20/25 | Loss: 0.00131882
Iteration 21/25 | Loss: 0.00131794
Iteration 22/25 | Loss: 0.00131759
Iteration 23/25 | Loss: 0.00131850
Iteration 24/25 | Loss: 0.00131736
Iteration 25/25 | Loss: 0.00131727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.07107878
Iteration 2/25 | Loss: 0.00541569
Iteration 3/25 | Loss: 0.00310188
Iteration 4/25 | Loss: 0.00326860
Iteration 5/25 | Loss: 0.00326860
Iteration 6/25 | Loss: 0.00326860
Iteration 7/25 | Loss: 0.00327963
Iteration 8/25 | Loss: 0.00351416
Iteration 9/25 | Loss: 0.00310185
Iteration 10/25 | Loss: 0.00310185
Iteration 11/25 | Loss: 0.00310185
Iteration 12/25 | Loss: 0.00310185
Iteration 13/25 | Loss: 0.00310184
Iteration 14/25 | Loss: 0.00310184
Iteration 15/25 | Loss: 0.00310184
Iteration 16/25 | Loss: 0.00310184
Iteration 17/25 | Loss: 0.00310184
Iteration 18/25 | Loss: 0.00310184
Iteration 19/25 | Loss: 0.00310184
Iteration 20/25 | Loss: 0.00310184
Iteration 21/25 | Loss: 0.00310184
Iteration 22/25 | Loss: 0.00310184
Iteration 23/25 | Loss: 0.00310184
Iteration 24/25 | Loss: 0.00310184
Iteration 25/25 | Loss: 0.00310184
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.003101843176409602, 0.003101843176409602, 0.003101843176409602, 0.003101843176409602, 0.003101843176409602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003101843176409602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00310184
Iteration 2/1000 | Loss: 0.00653300
Iteration 3/1000 | Loss: 0.00392245
Iteration 4/1000 | Loss: 0.00144721
Iteration 5/1000 | Loss: 0.00141488
Iteration 6/1000 | Loss: 0.00283483
Iteration 7/1000 | Loss: 0.00114915
Iteration 8/1000 | Loss: 0.00286659
Iteration 9/1000 | Loss: 0.00066148
Iteration 10/1000 | Loss: 0.00132598
Iteration 11/1000 | Loss: 0.00106127
Iteration 12/1000 | Loss: 0.00202700
Iteration 13/1000 | Loss: 0.00132262
Iteration 14/1000 | Loss: 0.00059652
Iteration 15/1000 | Loss: 0.00098811
Iteration 16/1000 | Loss: 0.00148509
Iteration 17/1000 | Loss: 0.00131874
Iteration 18/1000 | Loss: 0.00145172
Iteration 19/1000 | Loss: 0.00154172
Iteration 20/1000 | Loss: 0.00148860
Iteration 21/1000 | Loss: 0.00097269
Iteration 22/1000 | Loss: 0.00090666
Iteration 23/1000 | Loss: 0.00081917
Iteration 24/1000 | Loss: 0.00063844
Iteration 25/1000 | Loss: 0.00027947
Iteration 26/1000 | Loss: 0.00055904
Iteration 27/1000 | Loss: 0.00323914
Iteration 28/1000 | Loss: 0.00382430
Iteration 29/1000 | Loss: 0.00483846
Iteration 30/1000 | Loss: 0.00723346
Iteration 31/1000 | Loss: 0.00350319
Iteration 32/1000 | Loss: 0.00079889
Iteration 33/1000 | Loss: 0.00235244
Iteration 34/1000 | Loss: 0.00032322
Iteration 35/1000 | Loss: 0.00105787
Iteration 36/1000 | Loss: 0.00101683
Iteration 37/1000 | Loss: 0.00304648
Iteration 38/1000 | Loss: 0.00052254
Iteration 39/1000 | Loss: 0.00065834
Iteration 40/1000 | Loss: 0.00213395
Iteration 41/1000 | Loss: 0.00044068
Iteration 42/1000 | Loss: 0.00025092
Iteration 43/1000 | Loss: 0.00010794
Iteration 44/1000 | Loss: 0.00032766
Iteration 45/1000 | Loss: 0.00176027
Iteration 46/1000 | Loss: 0.00063968
Iteration 47/1000 | Loss: 0.00038853
Iteration 48/1000 | Loss: 0.00019038
Iteration 49/1000 | Loss: 0.00037639
Iteration 50/1000 | Loss: 0.00024063
Iteration 51/1000 | Loss: 0.00022441
Iteration 52/1000 | Loss: 0.00015060
Iteration 53/1000 | Loss: 0.00004331
Iteration 54/1000 | Loss: 0.00044133
Iteration 55/1000 | Loss: 0.00021852
Iteration 56/1000 | Loss: 0.00025700
Iteration 57/1000 | Loss: 0.00023452
Iteration 58/1000 | Loss: 0.00046950
Iteration 59/1000 | Loss: 0.00004267
Iteration 60/1000 | Loss: 0.00104324
Iteration 61/1000 | Loss: 0.00166463
Iteration 62/1000 | Loss: 0.00020878
Iteration 63/1000 | Loss: 0.00008562
Iteration 64/1000 | Loss: 0.00004206
Iteration 65/1000 | Loss: 0.00049290
Iteration 66/1000 | Loss: 0.00004530
Iteration 67/1000 | Loss: 0.00004007
Iteration 68/1000 | Loss: 0.00003741
Iteration 69/1000 | Loss: 0.00003501
Iteration 70/1000 | Loss: 0.00034955
Iteration 71/1000 | Loss: 0.00012553
Iteration 72/1000 | Loss: 0.00014130
Iteration 73/1000 | Loss: 0.00004624
Iteration 74/1000 | Loss: 0.00101547
Iteration 75/1000 | Loss: 0.00042074
Iteration 76/1000 | Loss: 0.00010536
Iteration 77/1000 | Loss: 0.00003985
Iteration 78/1000 | Loss: 0.00023453
Iteration 79/1000 | Loss: 0.00003816
Iteration 80/1000 | Loss: 0.00003124
Iteration 81/1000 | Loss: 0.00002812
Iteration 82/1000 | Loss: 0.00002641
Iteration 83/1000 | Loss: 0.00045595
Iteration 84/1000 | Loss: 0.00007609
Iteration 85/1000 | Loss: 0.00002482
Iteration 86/1000 | Loss: 0.00011389
Iteration 87/1000 | Loss: 0.00017086
Iteration 88/1000 | Loss: 0.00005316
Iteration 89/1000 | Loss: 0.00025049
Iteration 90/1000 | Loss: 0.00006516
Iteration 91/1000 | Loss: 0.00018140
Iteration 92/1000 | Loss: 0.00032874
Iteration 93/1000 | Loss: 0.00019912
Iteration 94/1000 | Loss: 0.00029164
Iteration 95/1000 | Loss: 0.00046761
Iteration 96/1000 | Loss: 0.00028534
Iteration 97/1000 | Loss: 0.00037855
Iteration 98/1000 | Loss: 0.00017903
Iteration 99/1000 | Loss: 0.00020625
Iteration 100/1000 | Loss: 0.00002544
Iteration 101/1000 | Loss: 0.00029002
Iteration 102/1000 | Loss: 0.00038519
Iteration 103/1000 | Loss: 0.00003787
Iteration 104/1000 | Loss: 0.00004752
Iteration 105/1000 | Loss: 0.00034805
Iteration 106/1000 | Loss: 0.00043121
Iteration 107/1000 | Loss: 0.00019234
Iteration 108/1000 | Loss: 0.00037999
Iteration 109/1000 | Loss: 0.00092664
Iteration 110/1000 | Loss: 0.00031267
Iteration 111/1000 | Loss: 0.00064962
Iteration 112/1000 | Loss: 0.00029169
Iteration 113/1000 | Loss: 0.00022475
Iteration 114/1000 | Loss: 0.00002536
Iteration 115/1000 | Loss: 0.00002332
Iteration 116/1000 | Loss: 0.00002284
Iteration 117/1000 | Loss: 0.00002976
Iteration 118/1000 | Loss: 0.00023237
Iteration 119/1000 | Loss: 0.00009060
Iteration 120/1000 | Loss: 0.00033299
Iteration 121/1000 | Loss: 0.00076272
Iteration 122/1000 | Loss: 0.00018883
Iteration 123/1000 | Loss: 0.00004735
Iteration 124/1000 | Loss: 0.00034664
Iteration 125/1000 | Loss: 0.00004032
Iteration 126/1000 | Loss: 0.00002781
Iteration 127/1000 | Loss: 0.00067257
Iteration 128/1000 | Loss: 0.00017819
Iteration 129/1000 | Loss: 0.00026891
Iteration 130/1000 | Loss: 0.00005632
Iteration 131/1000 | Loss: 0.00002503
Iteration 132/1000 | Loss: 0.00002298
Iteration 133/1000 | Loss: 0.00002194
Iteration 134/1000 | Loss: 0.00002077
Iteration 135/1000 | Loss: 0.00002019
Iteration 136/1000 | Loss: 0.00001980
Iteration 137/1000 | Loss: 0.00001968
Iteration 138/1000 | Loss: 0.00001967
Iteration 139/1000 | Loss: 0.00001957
Iteration 140/1000 | Loss: 0.00001955
Iteration 141/1000 | Loss: 0.00001954
Iteration 142/1000 | Loss: 0.00001953
Iteration 143/1000 | Loss: 0.00001943
Iteration 144/1000 | Loss: 0.00001943
Iteration 145/1000 | Loss: 0.00001942
Iteration 146/1000 | Loss: 0.00001941
Iteration 147/1000 | Loss: 0.00001941
Iteration 148/1000 | Loss: 0.00001940
Iteration 149/1000 | Loss: 0.00001940
Iteration 150/1000 | Loss: 0.00001940
Iteration 151/1000 | Loss: 0.00001939
Iteration 152/1000 | Loss: 0.00001939
Iteration 153/1000 | Loss: 0.00001939
Iteration 154/1000 | Loss: 0.00001938
Iteration 155/1000 | Loss: 0.00001937
Iteration 156/1000 | Loss: 0.00001937
Iteration 157/1000 | Loss: 0.00001936
Iteration 158/1000 | Loss: 0.00001936
Iteration 159/1000 | Loss: 0.00001936
Iteration 160/1000 | Loss: 0.00001936
Iteration 161/1000 | Loss: 0.00001936
Iteration 162/1000 | Loss: 0.00001936
Iteration 163/1000 | Loss: 0.00001936
Iteration 164/1000 | Loss: 0.00001936
Iteration 165/1000 | Loss: 0.00001936
Iteration 166/1000 | Loss: 0.00001936
Iteration 167/1000 | Loss: 0.00001935
Iteration 168/1000 | Loss: 0.00001934
Iteration 169/1000 | Loss: 0.00001934
Iteration 170/1000 | Loss: 0.00001933
Iteration 171/1000 | Loss: 0.00001922
Iteration 172/1000 | Loss: 0.00001916
Iteration 173/1000 | Loss: 0.00001916
Iteration 174/1000 | Loss: 0.00001915
Iteration 175/1000 | Loss: 0.00001915
Iteration 176/1000 | Loss: 0.00001914
Iteration 177/1000 | Loss: 0.00001914
Iteration 178/1000 | Loss: 0.00001914
Iteration 179/1000 | Loss: 0.00001913
Iteration 180/1000 | Loss: 0.00001913
Iteration 181/1000 | Loss: 0.00001913
Iteration 182/1000 | Loss: 0.00001913
Iteration 183/1000 | Loss: 0.00001912
Iteration 184/1000 | Loss: 0.00001912
Iteration 185/1000 | Loss: 0.00001912
Iteration 186/1000 | Loss: 0.00001911
Iteration 187/1000 | Loss: 0.00001911
Iteration 188/1000 | Loss: 0.00001911
Iteration 189/1000 | Loss: 0.00001910
Iteration 190/1000 | Loss: 0.00001910
Iteration 191/1000 | Loss: 0.00001910
Iteration 192/1000 | Loss: 0.00001910
Iteration 193/1000 | Loss: 0.00001910
Iteration 194/1000 | Loss: 0.00001910
Iteration 195/1000 | Loss: 0.00001909
Iteration 196/1000 | Loss: 0.00001909
Iteration 197/1000 | Loss: 0.00001909
Iteration 198/1000 | Loss: 0.00001909
Iteration 199/1000 | Loss: 0.00001909
Iteration 200/1000 | Loss: 0.00001909
Iteration 201/1000 | Loss: 0.00001908
Iteration 202/1000 | Loss: 0.00001908
Iteration 203/1000 | Loss: 0.00001908
Iteration 204/1000 | Loss: 0.00001908
Iteration 205/1000 | Loss: 0.00001907
Iteration 206/1000 | Loss: 0.00001907
Iteration 207/1000 | Loss: 0.00001907
Iteration 208/1000 | Loss: 0.00001907
Iteration 209/1000 | Loss: 0.00001907
Iteration 210/1000 | Loss: 0.00001907
Iteration 211/1000 | Loss: 0.00001907
Iteration 212/1000 | Loss: 0.00001907
Iteration 213/1000 | Loss: 0.00001907
Iteration 214/1000 | Loss: 0.00001907
Iteration 215/1000 | Loss: 0.00001907
Iteration 216/1000 | Loss: 0.00001907
Iteration 217/1000 | Loss: 0.00001907
Iteration 218/1000 | Loss: 0.00001907
Iteration 219/1000 | Loss: 0.00001907
Iteration 220/1000 | Loss: 0.00001907
Iteration 221/1000 | Loss: 0.00001907
Iteration 222/1000 | Loss: 0.00001907
Iteration 223/1000 | Loss: 0.00001907
Iteration 224/1000 | Loss: 0.00001907
Iteration 225/1000 | Loss: 0.00001907
Iteration 226/1000 | Loss: 0.00001907
Iteration 227/1000 | Loss: 0.00001907
Iteration 228/1000 | Loss: 0.00001907
Iteration 229/1000 | Loss: 0.00001907
Iteration 230/1000 | Loss: 0.00001907
Iteration 231/1000 | Loss: 0.00001907
Iteration 232/1000 | Loss: 0.00001907
Iteration 233/1000 | Loss: 0.00001907
Iteration 234/1000 | Loss: 0.00001907
Iteration 235/1000 | Loss: 0.00001907
Iteration 236/1000 | Loss: 0.00001907
Iteration 237/1000 | Loss: 0.00001907
Iteration 238/1000 | Loss: 0.00001907
Iteration 239/1000 | Loss: 0.00001907
Iteration 240/1000 | Loss: 0.00001907
Iteration 241/1000 | Loss: 0.00001907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.9068194887950085e-05, 1.9068194887950085e-05, 1.9068194887950085e-05, 1.9068194887950085e-05, 1.9068194887950085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9068194887950085e-05

Optimization complete. Final v2v error: 3.2977349758148193 mm

Highest mean error: 8.701735496520996 mm for frame 50

Lowest mean error: 2.233161449432373 mm for frame 23

Saving results

Total time: 268.75977540016174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808484
Iteration 2/25 | Loss: 0.00126674
Iteration 3/25 | Loss: 0.00110468
Iteration 4/25 | Loss: 0.00108946
Iteration 5/25 | Loss: 0.00108625
Iteration 6/25 | Loss: 0.00108484
Iteration 7/25 | Loss: 0.00108472
Iteration 8/25 | Loss: 0.00108472
Iteration 9/25 | Loss: 0.00108472
Iteration 10/25 | Loss: 0.00108472
Iteration 11/25 | Loss: 0.00108472
Iteration 12/25 | Loss: 0.00108472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010847204830497503, 0.0010847204830497503, 0.0010847204830497503, 0.0010847204830497503, 0.0010847204830497503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010847204830497503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05591547
Iteration 2/25 | Loss: 0.00068287
Iteration 3/25 | Loss: 0.00068284
Iteration 4/25 | Loss: 0.00068284
Iteration 5/25 | Loss: 0.00068284
Iteration 6/25 | Loss: 0.00068284
Iteration 7/25 | Loss: 0.00068284
Iteration 8/25 | Loss: 0.00068284
Iteration 9/25 | Loss: 0.00068284
Iteration 10/25 | Loss: 0.00068284
Iteration 11/25 | Loss: 0.00068284
Iteration 12/25 | Loss: 0.00068284
Iteration 13/25 | Loss: 0.00068284
Iteration 14/25 | Loss: 0.00068284
Iteration 15/25 | Loss: 0.00068284
Iteration 16/25 | Loss: 0.00068284
Iteration 17/25 | Loss: 0.00068284
Iteration 18/25 | Loss: 0.00068284
Iteration 19/25 | Loss: 0.00068284
Iteration 20/25 | Loss: 0.00068284
Iteration 21/25 | Loss: 0.00068284
Iteration 22/25 | Loss: 0.00068284
Iteration 23/25 | Loss: 0.00068284
Iteration 24/25 | Loss: 0.00068284
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006828353507444263, 0.0006828353507444263, 0.0006828353507444263, 0.0006828353507444263, 0.0006828353507444263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006828353507444263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068284
Iteration 2/1000 | Loss: 0.00004184
Iteration 3/1000 | Loss: 0.00002780
Iteration 4/1000 | Loss: 0.00002491
Iteration 5/1000 | Loss: 0.00002390
Iteration 6/1000 | Loss: 0.00002337
Iteration 7/1000 | Loss: 0.00002285
Iteration 8/1000 | Loss: 0.00002238
Iteration 9/1000 | Loss: 0.00002201
Iteration 10/1000 | Loss: 0.00002183
Iteration 11/1000 | Loss: 0.00002170
Iteration 12/1000 | Loss: 0.00002166
Iteration 13/1000 | Loss: 0.00002162
Iteration 14/1000 | Loss: 0.00002162
Iteration 15/1000 | Loss: 0.00002161
Iteration 16/1000 | Loss: 0.00002160
Iteration 17/1000 | Loss: 0.00002154
Iteration 18/1000 | Loss: 0.00002149
Iteration 19/1000 | Loss: 0.00002140
Iteration 20/1000 | Loss: 0.00002139
Iteration 21/1000 | Loss: 0.00002138
Iteration 22/1000 | Loss: 0.00002138
Iteration 23/1000 | Loss: 0.00002137
Iteration 24/1000 | Loss: 0.00002136
Iteration 25/1000 | Loss: 0.00002133
Iteration 26/1000 | Loss: 0.00002132
Iteration 27/1000 | Loss: 0.00002132
Iteration 28/1000 | Loss: 0.00002132
Iteration 29/1000 | Loss: 0.00002131
Iteration 30/1000 | Loss: 0.00002131
Iteration 31/1000 | Loss: 0.00002131
Iteration 32/1000 | Loss: 0.00002130
Iteration 33/1000 | Loss: 0.00002130
Iteration 34/1000 | Loss: 0.00002130
Iteration 35/1000 | Loss: 0.00002130
Iteration 36/1000 | Loss: 0.00002129
Iteration 37/1000 | Loss: 0.00002129
Iteration 38/1000 | Loss: 0.00002129
Iteration 39/1000 | Loss: 0.00002128
Iteration 40/1000 | Loss: 0.00002128
Iteration 41/1000 | Loss: 0.00002128
Iteration 42/1000 | Loss: 0.00002128
Iteration 43/1000 | Loss: 0.00002127
Iteration 44/1000 | Loss: 0.00002127
Iteration 45/1000 | Loss: 0.00002127
Iteration 46/1000 | Loss: 0.00002127
Iteration 47/1000 | Loss: 0.00002126
Iteration 48/1000 | Loss: 0.00002126
Iteration 49/1000 | Loss: 0.00002125
Iteration 50/1000 | Loss: 0.00002125
Iteration 51/1000 | Loss: 0.00002125
Iteration 52/1000 | Loss: 0.00002125
Iteration 53/1000 | Loss: 0.00002125
Iteration 54/1000 | Loss: 0.00002125
Iteration 55/1000 | Loss: 0.00002125
Iteration 56/1000 | Loss: 0.00002125
Iteration 57/1000 | Loss: 0.00002125
Iteration 58/1000 | Loss: 0.00002125
Iteration 59/1000 | Loss: 0.00002123
Iteration 60/1000 | Loss: 0.00002123
Iteration 61/1000 | Loss: 0.00002123
Iteration 62/1000 | Loss: 0.00002123
Iteration 63/1000 | Loss: 0.00002123
Iteration 64/1000 | Loss: 0.00002123
Iteration 65/1000 | Loss: 0.00002123
Iteration 66/1000 | Loss: 0.00002123
Iteration 67/1000 | Loss: 0.00002122
Iteration 68/1000 | Loss: 0.00002122
Iteration 69/1000 | Loss: 0.00002122
Iteration 70/1000 | Loss: 0.00002121
Iteration 71/1000 | Loss: 0.00002121
Iteration 72/1000 | Loss: 0.00002120
Iteration 73/1000 | Loss: 0.00002120
Iteration 74/1000 | Loss: 0.00002120
Iteration 75/1000 | Loss: 0.00002120
Iteration 76/1000 | Loss: 0.00002120
Iteration 77/1000 | Loss: 0.00002119
Iteration 78/1000 | Loss: 0.00002119
Iteration 79/1000 | Loss: 0.00002119
Iteration 80/1000 | Loss: 0.00002119
Iteration 81/1000 | Loss: 0.00002119
Iteration 82/1000 | Loss: 0.00002119
Iteration 83/1000 | Loss: 0.00002119
Iteration 84/1000 | Loss: 0.00002118
Iteration 85/1000 | Loss: 0.00002118
Iteration 86/1000 | Loss: 0.00002118
Iteration 87/1000 | Loss: 0.00002118
Iteration 88/1000 | Loss: 0.00002118
Iteration 89/1000 | Loss: 0.00002118
Iteration 90/1000 | Loss: 0.00002118
Iteration 91/1000 | Loss: 0.00002118
Iteration 92/1000 | Loss: 0.00002118
Iteration 93/1000 | Loss: 0.00002118
Iteration 94/1000 | Loss: 0.00002118
Iteration 95/1000 | Loss: 0.00002117
Iteration 96/1000 | Loss: 0.00002117
Iteration 97/1000 | Loss: 0.00002117
Iteration 98/1000 | Loss: 0.00002117
Iteration 99/1000 | Loss: 0.00002116
Iteration 100/1000 | Loss: 0.00002116
Iteration 101/1000 | Loss: 0.00002116
Iteration 102/1000 | Loss: 0.00002116
Iteration 103/1000 | Loss: 0.00002115
Iteration 104/1000 | Loss: 0.00002115
Iteration 105/1000 | Loss: 0.00002115
Iteration 106/1000 | Loss: 0.00002114
Iteration 107/1000 | Loss: 0.00002114
Iteration 108/1000 | Loss: 0.00002114
Iteration 109/1000 | Loss: 0.00002113
Iteration 110/1000 | Loss: 0.00002113
Iteration 111/1000 | Loss: 0.00002112
Iteration 112/1000 | Loss: 0.00002112
Iteration 113/1000 | Loss: 0.00002112
Iteration 114/1000 | Loss: 0.00002112
Iteration 115/1000 | Loss: 0.00002112
Iteration 116/1000 | Loss: 0.00002112
Iteration 117/1000 | Loss: 0.00002112
Iteration 118/1000 | Loss: 0.00002112
Iteration 119/1000 | Loss: 0.00002112
Iteration 120/1000 | Loss: 0.00002112
Iteration 121/1000 | Loss: 0.00002112
Iteration 122/1000 | Loss: 0.00002111
Iteration 123/1000 | Loss: 0.00002111
Iteration 124/1000 | Loss: 0.00002111
Iteration 125/1000 | Loss: 0.00002111
Iteration 126/1000 | Loss: 0.00002111
Iteration 127/1000 | Loss: 0.00002111
Iteration 128/1000 | Loss: 0.00002111
Iteration 129/1000 | Loss: 0.00002111
Iteration 130/1000 | Loss: 0.00002111
Iteration 131/1000 | Loss: 0.00002111
Iteration 132/1000 | Loss: 0.00002111
Iteration 133/1000 | Loss: 0.00002110
Iteration 134/1000 | Loss: 0.00002110
Iteration 135/1000 | Loss: 0.00002110
Iteration 136/1000 | Loss: 0.00002110
Iteration 137/1000 | Loss: 0.00002110
Iteration 138/1000 | Loss: 0.00002110
Iteration 139/1000 | Loss: 0.00002110
Iteration 140/1000 | Loss: 0.00002109
Iteration 141/1000 | Loss: 0.00002109
Iteration 142/1000 | Loss: 0.00002109
Iteration 143/1000 | Loss: 0.00002109
Iteration 144/1000 | Loss: 0.00002109
Iteration 145/1000 | Loss: 0.00002109
Iteration 146/1000 | Loss: 0.00002109
Iteration 147/1000 | Loss: 0.00002109
Iteration 148/1000 | Loss: 0.00002109
Iteration 149/1000 | Loss: 0.00002109
Iteration 150/1000 | Loss: 0.00002109
Iteration 151/1000 | Loss: 0.00002109
Iteration 152/1000 | Loss: 0.00002108
Iteration 153/1000 | Loss: 0.00002108
Iteration 154/1000 | Loss: 0.00002108
Iteration 155/1000 | Loss: 0.00002108
Iteration 156/1000 | Loss: 0.00002108
Iteration 157/1000 | Loss: 0.00002108
Iteration 158/1000 | Loss: 0.00002108
Iteration 159/1000 | Loss: 0.00002108
Iteration 160/1000 | Loss: 0.00002108
Iteration 161/1000 | Loss: 0.00002108
Iteration 162/1000 | Loss: 0.00002108
Iteration 163/1000 | Loss: 0.00002108
Iteration 164/1000 | Loss: 0.00002108
Iteration 165/1000 | Loss: 0.00002108
Iteration 166/1000 | Loss: 0.00002108
Iteration 167/1000 | Loss: 0.00002108
Iteration 168/1000 | Loss: 0.00002108
Iteration 169/1000 | Loss: 0.00002108
Iteration 170/1000 | Loss: 0.00002108
Iteration 171/1000 | Loss: 0.00002108
Iteration 172/1000 | Loss: 0.00002108
Iteration 173/1000 | Loss: 0.00002108
Iteration 174/1000 | Loss: 0.00002108
Iteration 175/1000 | Loss: 0.00002108
Iteration 176/1000 | Loss: 0.00002108
Iteration 177/1000 | Loss: 0.00002108
Iteration 178/1000 | Loss: 0.00002108
Iteration 179/1000 | Loss: 0.00002108
Iteration 180/1000 | Loss: 0.00002108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.107770342263393e-05, 2.107770342263393e-05, 2.107770342263393e-05, 2.107770342263393e-05, 2.107770342263393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.107770342263393e-05

Optimization complete. Final v2v error: 3.7800521850585938 mm

Highest mean error: 4.071208953857422 mm for frame 56

Lowest mean error: 3.119873046875 mm for frame 1

Saving results

Total time: 39.79142618179321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389544
Iteration 2/25 | Loss: 0.00105696
Iteration 3/25 | Loss: 0.00094557
Iteration 4/25 | Loss: 0.00093660
Iteration 5/25 | Loss: 0.00093455
Iteration 6/25 | Loss: 0.00093409
Iteration 7/25 | Loss: 0.00093409
Iteration 8/25 | Loss: 0.00093409
Iteration 9/25 | Loss: 0.00093409
Iteration 10/25 | Loss: 0.00093409
Iteration 11/25 | Loss: 0.00093409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009340870892629027, 0.0009340870892629027, 0.0009340870892629027, 0.0009340870892629027, 0.0009340870892629027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009340870892629027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29123926
Iteration 2/25 | Loss: 0.00123784
Iteration 3/25 | Loss: 0.00123784
Iteration 4/25 | Loss: 0.00123784
Iteration 5/25 | Loss: 0.00123783
Iteration 6/25 | Loss: 0.00123783
Iteration 7/25 | Loss: 0.00123783
Iteration 8/25 | Loss: 0.00123783
Iteration 9/25 | Loss: 0.00123783
Iteration 10/25 | Loss: 0.00123783
Iteration 11/25 | Loss: 0.00123783
Iteration 12/25 | Loss: 0.00123783
Iteration 13/25 | Loss: 0.00123783
Iteration 14/25 | Loss: 0.00123783
Iteration 15/25 | Loss: 0.00123783
Iteration 16/25 | Loss: 0.00123783
Iteration 17/25 | Loss: 0.00123783
Iteration 18/25 | Loss: 0.00123783
Iteration 19/25 | Loss: 0.00123783
Iteration 20/25 | Loss: 0.00123783
Iteration 21/25 | Loss: 0.00123783
Iteration 22/25 | Loss: 0.00123783
Iteration 23/25 | Loss: 0.00123783
Iteration 24/25 | Loss: 0.00123783
Iteration 25/25 | Loss: 0.00123783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123783
Iteration 2/1000 | Loss: 0.00002941
Iteration 3/1000 | Loss: 0.00001333
Iteration 4/1000 | Loss: 0.00001072
Iteration 5/1000 | Loss: 0.00000973
Iteration 6/1000 | Loss: 0.00000923
Iteration 7/1000 | Loss: 0.00000888
Iteration 8/1000 | Loss: 0.00000864
Iteration 9/1000 | Loss: 0.00000855
Iteration 10/1000 | Loss: 0.00000853
Iteration 11/1000 | Loss: 0.00000852
Iteration 12/1000 | Loss: 0.00000851
Iteration 13/1000 | Loss: 0.00000849
Iteration 14/1000 | Loss: 0.00000847
Iteration 15/1000 | Loss: 0.00000847
Iteration 16/1000 | Loss: 0.00000847
Iteration 17/1000 | Loss: 0.00000846
Iteration 18/1000 | Loss: 0.00000845
Iteration 19/1000 | Loss: 0.00000845
Iteration 20/1000 | Loss: 0.00000844
Iteration 21/1000 | Loss: 0.00000843
Iteration 22/1000 | Loss: 0.00000842
Iteration 23/1000 | Loss: 0.00000842
Iteration 24/1000 | Loss: 0.00000842
Iteration 25/1000 | Loss: 0.00000841
Iteration 26/1000 | Loss: 0.00000839
Iteration 27/1000 | Loss: 0.00000836
Iteration 28/1000 | Loss: 0.00000835
Iteration 29/1000 | Loss: 0.00000835
Iteration 30/1000 | Loss: 0.00000835
Iteration 31/1000 | Loss: 0.00000832
Iteration 32/1000 | Loss: 0.00000832
Iteration 33/1000 | Loss: 0.00000830
Iteration 34/1000 | Loss: 0.00000829
Iteration 35/1000 | Loss: 0.00000829
Iteration 36/1000 | Loss: 0.00000829
Iteration 37/1000 | Loss: 0.00000828
Iteration 38/1000 | Loss: 0.00000828
Iteration 39/1000 | Loss: 0.00000827
Iteration 40/1000 | Loss: 0.00000827
Iteration 41/1000 | Loss: 0.00000826
Iteration 42/1000 | Loss: 0.00000826
Iteration 43/1000 | Loss: 0.00000826
Iteration 44/1000 | Loss: 0.00000825
Iteration 45/1000 | Loss: 0.00000825
Iteration 46/1000 | Loss: 0.00000825
Iteration 47/1000 | Loss: 0.00000824
Iteration 48/1000 | Loss: 0.00000824
Iteration 49/1000 | Loss: 0.00000824
Iteration 50/1000 | Loss: 0.00000824
Iteration 51/1000 | Loss: 0.00000824
Iteration 52/1000 | Loss: 0.00000824
Iteration 53/1000 | Loss: 0.00000823
Iteration 54/1000 | Loss: 0.00000823
Iteration 55/1000 | Loss: 0.00000823
Iteration 56/1000 | Loss: 0.00000823
Iteration 57/1000 | Loss: 0.00000822
Iteration 58/1000 | Loss: 0.00000822
Iteration 59/1000 | Loss: 0.00000822
Iteration 60/1000 | Loss: 0.00000822
Iteration 61/1000 | Loss: 0.00000822
Iteration 62/1000 | Loss: 0.00000822
Iteration 63/1000 | Loss: 0.00000822
Iteration 64/1000 | Loss: 0.00000822
Iteration 65/1000 | Loss: 0.00000821
Iteration 66/1000 | Loss: 0.00000821
Iteration 67/1000 | Loss: 0.00000821
Iteration 68/1000 | Loss: 0.00000821
Iteration 69/1000 | Loss: 0.00000821
Iteration 70/1000 | Loss: 0.00000821
Iteration 71/1000 | Loss: 0.00000820
Iteration 72/1000 | Loss: 0.00000820
Iteration 73/1000 | Loss: 0.00000820
Iteration 74/1000 | Loss: 0.00000820
Iteration 75/1000 | Loss: 0.00000820
Iteration 76/1000 | Loss: 0.00000820
Iteration 77/1000 | Loss: 0.00000820
Iteration 78/1000 | Loss: 0.00000820
Iteration 79/1000 | Loss: 0.00000820
Iteration 80/1000 | Loss: 0.00000820
Iteration 81/1000 | Loss: 0.00000820
Iteration 82/1000 | Loss: 0.00000820
Iteration 83/1000 | Loss: 0.00000819
Iteration 84/1000 | Loss: 0.00000819
Iteration 85/1000 | Loss: 0.00000819
Iteration 86/1000 | Loss: 0.00000819
Iteration 87/1000 | Loss: 0.00000819
Iteration 88/1000 | Loss: 0.00000819
Iteration 89/1000 | Loss: 0.00000819
Iteration 90/1000 | Loss: 0.00000818
Iteration 91/1000 | Loss: 0.00000818
Iteration 92/1000 | Loss: 0.00000818
Iteration 93/1000 | Loss: 0.00000818
Iteration 94/1000 | Loss: 0.00000818
Iteration 95/1000 | Loss: 0.00000818
Iteration 96/1000 | Loss: 0.00000818
Iteration 97/1000 | Loss: 0.00000818
Iteration 98/1000 | Loss: 0.00000817
Iteration 99/1000 | Loss: 0.00000817
Iteration 100/1000 | Loss: 0.00000817
Iteration 101/1000 | Loss: 0.00000817
Iteration 102/1000 | Loss: 0.00000817
Iteration 103/1000 | Loss: 0.00000817
Iteration 104/1000 | Loss: 0.00000817
Iteration 105/1000 | Loss: 0.00000817
Iteration 106/1000 | Loss: 0.00000817
Iteration 107/1000 | Loss: 0.00000817
Iteration 108/1000 | Loss: 0.00000817
Iteration 109/1000 | Loss: 0.00000817
Iteration 110/1000 | Loss: 0.00000817
Iteration 111/1000 | Loss: 0.00000817
Iteration 112/1000 | Loss: 0.00000817
Iteration 113/1000 | Loss: 0.00000817
Iteration 114/1000 | Loss: 0.00000817
Iteration 115/1000 | Loss: 0.00000817
Iteration 116/1000 | Loss: 0.00000816
Iteration 117/1000 | Loss: 0.00000816
Iteration 118/1000 | Loss: 0.00000816
Iteration 119/1000 | Loss: 0.00000816
Iteration 120/1000 | Loss: 0.00000816
Iteration 121/1000 | Loss: 0.00000815
Iteration 122/1000 | Loss: 0.00000815
Iteration 123/1000 | Loss: 0.00000815
Iteration 124/1000 | Loss: 0.00000815
Iteration 125/1000 | Loss: 0.00000815
Iteration 126/1000 | Loss: 0.00000814
Iteration 127/1000 | Loss: 0.00000814
Iteration 128/1000 | Loss: 0.00000814
Iteration 129/1000 | Loss: 0.00000813
Iteration 130/1000 | Loss: 0.00000813
Iteration 131/1000 | Loss: 0.00000813
Iteration 132/1000 | Loss: 0.00000812
Iteration 133/1000 | Loss: 0.00000812
Iteration 134/1000 | Loss: 0.00000812
Iteration 135/1000 | Loss: 0.00000812
Iteration 136/1000 | Loss: 0.00000812
Iteration 137/1000 | Loss: 0.00000812
Iteration 138/1000 | Loss: 0.00000812
Iteration 139/1000 | Loss: 0.00000812
Iteration 140/1000 | Loss: 0.00000811
Iteration 141/1000 | Loss: 0.00000811
Iteration 142/1000 | Loss: 0.00000811
Iteration 143/1000 | Loss: 0.00000811
Iteration 144/1000 | Loss: 0.00000811
Iteration 145/1000 | Loss: 0.00000810
Iteration 146/1000 | Loss: 0.00000810
Iteration 147/1000 | Loss: 0.00000810
Iteration 148/1000 | Loss: 0.00000810
Iteration 149/1000 | Loss: 0.00000810
Iteration 150/1000 | Loss: 0.00000810
Iteration 151/1000 | Loss: 0.00000810
Iteration 152/1000 | Loss: 0.00000810
Iteration 153/1000 | Loss: 0.00000810
Iteration 154/1000 | Loss: 0.00000810
Iteration 155/1000 | Loss: 0.00000810
Iteration 156/1000 | Loss: 0.00000809
Iteration 157/1000 | Loss: 0.00000809
Iteration 158/1000 | Loss: 0.00000809
Iteration 159/1000 | Loss: 0.00000809
Iteration 160/1000 | Loss: 0.00000809
Iteration 161/1000 | Loss: 0.00000809
Iteration 162/1000 | Loss: 0.00000809
Iteration 163/1000 | Loss: 0.00000809
Iteration 164/1000 | Loss: 0.00000809
Iteration 165/1000 | Loss: 0.00000808
Iteration 166/1000 | Loss: 0.00000808
Iteration 167/1000 | Loss: 0.00000808
Iteration 168/1000 | Loss: 0.00000808
Iteration 169/1000 | Loss: 0.00000808
Iteration 170/1000 | Loss: 0.00000808
Iteration 171/1000 | Loss: 0.00000807
Iteration 172/1000 | Loss: 0.00000807
Iteration 173/1000 | Loss: 0.00000807
Iteration 174/1000 | Loss: 0.00000807
Iteration 175/1000 | Loss: 0.00000806
Iteration 176/1000 | Loss: 0.00000806
Iteration 177/1000 | Loss: 0.00000806
Iteration 178/1000 | Loss: 0.00000806
Iteration 179/1000 | Loss: 0.00000806
Iteration 180/1000 | Loss: 0.00000806
Iteration 181/1000 | Loss: 0.00000806
Iteration 182/1000 | Loss: 0.00000805
Iteration 183/1000 | Loss: 0.00000805
Iteration 184/1000 | Loss: 0.00000805
Iteration 185/1000 | Loss: 0.00000804
Iteration 186/1000 | Loss: 0.00000804
Iteration 187/1000 | Loss: 0.00000804
Iteration 188/1000 | Loss: 0.00000804
Iteration 189/1000 | Loss: 0.00000804
Iteration 190/1000 | Loss: 0.00000804
Iteration 191/1000 | Loss: 0.00000804
Iteration 192/1000 | Loss: 0.00000803
Iteration 193/1000 | Loss: 0.00000803
Iteration 194/1000 | Loss: 0.00000803
Iteration 195/1000 | Loss: 0.00000803
Iteration 196/1000 | Loss: 0.00000803
Iteration 197/1000 | Loss: 0.00000803
Iteration 198/1000 | Loss: 0.00000802
Iteration 199/1000 | Loss: 0.00000802
Iteration 200/1000 | Loss: 0.00000802
Iteration 201/1000 | Loss: 0.00000802
Iteration 202/1000 | Loss: 0.00000802
Iteration 203/1000 | Loss: 0.00000802
Iteration 204/1000 | Loss: 0.00000802
Iteration 205/1000 | Loss: 0.00000802
Iteration 206/1000 | Loss: 0.00000802
Iteration 207/1000 | Loss: 0.00000802
Iteration 208/1000 | Loss: 0.00000802
Iteration 209/1000 | Loss: 0.00000801
Iteration 210/1000 | Loss: 0.00000801
Iteration 211/1000 | Loss: 0.00000801
Iteration 212/1000 | Loss: 0.00000801
Iteration 213/1000 | Loss: 0.00000801
Iteration 214/1000 | Loss: 0.00000801
Iteration 215/1000 | Loss: 0.00000801
Iteration 216/1000 | Loss: 0.00000801
Iteration 217/1000 | Loss: 0.00000801
Iteration 218/1000 | Loss: 0.00000801
Iteration 219/1000 | Loss: 0.00000801
Iteration 220/1000 | Loss: 0.00000801
Iteration 221/1000 | Loss: 0.00000801
Iteration 222/1000 | Loss: 0.00000801
Iteration 223/1000 | Loss: 0.00000801
Iteration 224/1000 | Loss: 0.00000801
Iteration 225/1000 | Loss: 0.00000801
Iteration 226/1000 | Loss: 0.00000800
Iteration 227/1000 | Loss: 0.00000800
Iteration 228/1000 | Loss: 0.00000800
Iteration 229/1000 | Loss: 0.00000800
Iteration 230/1000 | Loss: 0.00000800
Iteration 231/1000 | Loss: 0.00000800
Iteration 232/1000 | Loss: 0.00000800
Iteration 233/1000 | Loss: 0.00000800
Iteration 234/1000 | Loss: 0.00000800
Iteration 235/1000 | Loss: 0.00000800
Iteration 236/1000 | Loss: 0.00000800
Iteration 237/1000 | Loss: 0.00000800
Iteration 238/1000 | Loss: 0.00000800
Iteration 239/1000 | Loss: 0.00000800
Iteration 240/1000 | Loss: 0.00000800
Iteration 241/1000 | Loss: 0.00000800
Iteration 242/1000 | Loss: 0.00000800
Iteration 243/1000 | Loss: 0.00000800
Iteration 244/1000 | Loss: 0.00000800
Iteration 245/1000 | Loss: 0.00000800
Iteration 246/1000 | Loss: 0.00000800
Iteration 247/1000 | Loss: 0.00000800
Iteration 248/1000 | Loss: 0.00000800
Iteration 249/1000 | Loss: 0.00000800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [8.000731213542167e-06, 8.000731213542167e-06, 8.000731213542167e-06, 8.000731213542167e-06, 8.000731213542167e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.000731213542167e-06

Optimization complete. Final v2v error: 2.328895092010498 mm

Highest mean error: 3.3279621601104736 mm for frame 73

Lowest mean error: 1.9237167835235596 mm for frame 159

Saving results

Total time: 38.13665318489075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893517
Iteration 2/25 | Loss: 0.00210681
Iteration 3/25 | Loss: 0.00147150
Iteration 4/25 | Loss: 0.00140515
Iteration 5/25 | Loss: 0.00139930
Iteration 6/25 | Loss: 0.00139930
Iteration 7/25 | Loss: 0.00139930
Iteration 8/25 | Loss: 0.00139930
Iteration 9/25 | Loss: 0.00139930
Iteration 10/25 | Loss: 0.00139930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001399296335875988, 0.001399296335875988, 0.001399296335875988, 0.001399296335875988, 0.001399296335875988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001399296335875988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25608110
Iteration 2/25 | Loss: 0.00085851
Iteration 3/25 | Loss: 0.00085851
Iteration 4/25 | Loss: 0.00085851
Iteration 5/25 | Loss: 0.00085851
Iteration 6/25 | Loss: 0.00085851
Iteration 7/25 | Loss: 0.00085851
Iteration 8/25 | Loss: 0.00085851
Iteration 9/25 | Loss: 0.00085851
Iteration 10/25 | Loss: 0.00085851
Iteration 11/25 | Loss: 0.00085851
Iteration 12/25 | Loss: 0.00085851
Iteration 13/25 | Loss: 0.00085851
Iteration 14/25 | Loss: 0.00085851
Iteration 15/25 | Loss: 0.00085851
Iteration 16/25 | Loss: 0.00085851
Iteration 17/25 | Loss: 0.00085851
Iteration 18/25 | Loss: 0.00085851
Iteration 19/25 | Loss: 0.00085851
Iteration 20/25 | Loss: 0.00085851
Iteration 21/25 | Loss: 0.00085851
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008585110772401094, 0.0008585110772401094, 0.0008585110772401094, 0.0008585110772401094, 0.0008585110772401094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008585110772401094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085851
Iteration 2/1000 | Loss: 0.00010677
Iteration 3/1000 | Loss: 0.00005456
Iteration 4/1000 | Loss: 0.00004287
Iteration 5/1000 | Loss: 0.00003860
Iteration 6/1000 | Loss: 0.00003595
Iteration 7/1000 | Loss: 0.00003427
Iteration 8/1000 | Loss: 0.00003294
Iteration 9/1000 | Loss: 0.00003211
Iteration 10/1000 | Loss: 0.00003153
Iteration 11/1000 | Loss: 0.00003123
Iteration 12/1000 | Loss: 0.00003109
Iteration 13/1000 | Loss: 0.00003093
Iteration 14/1000 | Loss: 0.00003086
Iteration 15/1000 | Loss: 0.00003082
Iteration 16/1000 | Loss: 0.00003081
Iteration 17/1000 | Loss: 0.00003080
Iteration 18/1000 | Loss: 0.00003080
Iteration 19/1000 | Loss: 0.00003079
Iteration 20/1000 | Loss: 0.00003079
Iteration 21/1000 | Loss: 0.00003079
Iteration 22/1000 | Loss: 0.00003079
Iteration 23/1000 | Loss: 0.00003079
Iteration 24/1000 | Loss: 0.00003079
Iteration 25/1000 | Loss: 0.00003079
Iteration 26/1000 | Loss: 0.00003078
Iteration 27/1000 | Loss: 0.00003078
Iteration 28/1000 | Loss: 0.00003078
Iteration 29/1000 | Loss: 0.00003078
Iteration 30/1000 | Loss: 0.00003078
Iteration 31/1000 | Loss: 0.00003078
Iteration 32/1000 | Loss: 0.00003077
Iteration 33/1000 | Loss: 0.00003075
Iteration 34/1000 | Loss: 0.00003073
Iteration 35/1000 | Loss: 0.00003073
Iteration 36/1000 | Loss: 0.00003073
Iteration 37/1000 | Loss: 0.00003072
Iteration 38/1000 | Loss: 0.00003072
Iteration 39/1000 | Loss: 0.00003071
Iteration 40/1000 | Loss: 0.00003070
Iteration 41/1000 | Loss: 0.00003070
Iteration 42/1000 | Loss: 0.00003070
Iteration 43/1000 | Loss: 0.00003070
Iteration 44/1000 | Loss: 0.00003070
Iteration 45/1000 | Loss: 0.00003070
Iteration 46/1000 | Loss: 0.00003070
Iteration 47/1000 | Loss: 0.00003070
Iteration 48/1000 | Loss: 0.00003070
Iteration 49/1000 | Loss: 0.00003069
Iteration 50/1000 | Loss: 0.00003069
Iteration 51/1000 | Loss: 0.00003069
Iteration 52/1000 | Loss: 0.00003068
Iteration 53/1000 | Loss: 0.00003068
Iteration 54/1000 | Loss: 0.00003068
Iteration 55/1000 | Loss: 0.00003068
Iteration 56/1000 | Loss: 0.00003068
Iteration 57/1000 | Loss: 0.00003068
Iteration 58/1000 | Loss: 0.00003067
Iteration 59/1000 | Loss: 0.00003067
Iteration 60/1000 | Loss: 0.00003067
Iteration 61/1000 | Loss: 0.00003067
Iteration 62/1000 | Loss: 0.00003067
Iteration 63/1000 | Loss: 0.00003067
Iteration 64/1000 | Loss: 0.00003066
Iteration 65/1000 | Loss: 0.00003066
Iteration 66/1000 | Loss: 0.00003065
Iteration 67/1000 | Loss: 0.00003065
Iteration 68/1000 | Loss: 0.00003065
Iteration 69/1000 | Loss: 0.00003064
Iteration 70/1000 | Loss: 0.00003064
Iteration 71/1000 | Loss: 0.00003064
Iteration 72/1000 | Loss: 0.00003064
Iteration 73/1000 | Loss: 0.00003064
Iteration 74/1000 | Loss: 0.00003064
Iteration 75/1000 | Loss: 0.00003064
Iteration 76/1000 | Loss: 0.00003064
Iteration 77/1000 | Loss: 0.00003063
Iteration 78/1000 | Loss: 0.00003063
Iteration 79/1000 | Loss: 0.00003063
Iteration 80/1000 | Loss: 0.00003063
Iteration 81/1000 | Loss: 0.00003062
Iteration 82/1000 | Loss: 0.00003062
Iteration 83/1000 | Loss: 0.00003062
Iteration 84/1000 | Loss: 0.00003061
Iteration 85/1000 | Loss: 0.00003061
Iteration 86/1000 | Loss: 0.00003061
Iteration 87/1000 | Loss: 0.00003061
Iteration 88/1000 | Loss: 0.00003060
Iteration 89/1000 | Loss: 0.00003060
Iteration 90/1000 | Loss: 0.00003060
Iteration 91/1000 | Loss: 0.00003060
Iteration 92/1000 | Loss: 0.00003060
Iteration 93/1000 | Loss: 0.00003059
Iteration 94/1000 | Loss: 0.00003059
Iteration 95/1000 | Loss: 0.00003059
Iteration 96/1000 | Loss: 0.00003059
Iteration 97/1000 | Loss: 0.00003059
Iteration 98/1000 | Loss: 0.00003059
Iteration 99/1000 | Loss: 0.00003059
Iteration 100/1000 | Loss: 0.00003059
Iteration 101/1000 | Loss: 0.00003059
Iteration 102/1000 | Loss: 0.00003059
Iteration 103/1000 | Loss: 0.00003058
Iteration 104/1000 | Loss: 0.00003058
Iteration 105/1000 | Loss: 0.00003058
Iteration 106/1000 | Loss: 0.00003058
Iteration 107/1000 | Loss: 0.00003057
Iteration 108/1000 | Loss: 0.00003057
Iteration 109/1000 | Loss: 0.00003057
Iteration 110/1000 | Loss: 0.00003057
Iteration 111/1000 | Loss: 0.00003057
Iteration 112/1000 | Loss: 0.00003057
Iteration 113/1000 | Loss: 0.00003056
Iteration 114/1000 | Loss: 0.00003056
Iteration 115/1000 | Loss: 0.00003056
Iteration 116/1000 | Loss: 0.00003056
Iteration 117/1000 | Loss: 0.00003055
Iteration 118/1000 | Loss: 0.00003055
Iteration 119/1000 | Loss: 0.00003055
Iteration 120/1000 | Loss: 0.00003055
Iteration 121/1000 | Loss: 0.00003055
Iteration 122/1000 | Loss: 0.00003055
Iteration 123/1000 | Loss: 0.00003055
Iteration 124/1000 | Loss: 0.00003055
Iteration 125/1000 | Loss: 0.00003055
Iteration 126/1000 | Loss: 0.00003054
Iteration 127/1000 | Loss: 0.00003054
Iteration 128/1000 | Loss: 0.00003054
Iteration 129/1000 | Loss: 0.00003054
Iteration 130/1000 | Loss: 0.00003054
Iteration 131/1000 | Loss: 0.00003054
Iteration 132/1000 | Loss: 0.00003054
Iteration 133/1000 | Loss: 0.00003054
Iteration 134/1000 | Loss: 0.00003054
Iteration 135/1000 | Loss: 0.00003054
Iteration 136/1000 | Loss: 0.00003054
Iteration 137/1000 | Loss: 0.00003054
Iteration 138/1000 | Loss: 0.00003054
Iteration 139/1000 | Loss: 0.00003054
Iteration 140/1000 | Loss: 0.00003054
Iteration 141/1000 | Loss: 0.00003054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [3.053609179914929e-05, 3.053609179914929e-05, 3.053609179914929e-05, 3.053609179914929e-05, 3.053609179914929e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.053609179914929e-05

Optimization complete. Final v2v error: 4.761102676391602 mm

Highest mean error: 5.055727481842041 mm for frame 89

Lowest mean error: 4.455933094024658 mm for frame 47

Saving results

Total time: 39.53508281707764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01130476
Iteration 2/25 | Loss: 0.00238453
Iteration 3/25 | Loss: 0.00165500
Iteration 4/25 | Loss: 0.00154525
Iteration 5/25 | Loss: 0.00147623
Iteration 6/25 | Loss: 0.00147043
Iteration 7/25 | Loss: 0.00145619
Iteration 8/25 | Loss: 0.00146401
Iteration 9/25 | Loss: 0.00142856
Iteration 10/25 | Loss: 0.00139212
Iteration 11/25 | Loss: 0.00138444
Iteration 12/25 | Loss: 0.00135050
Iteration 13/25 | Loss: 0.00133898
Iteration 14/25 | Loss: 0.00133585
Iteration 15/25 | Loss: 0.00132812
Iteration 16/25 | Loss: 0.00132286
Iteration 17/25 | Loss: 0.00131750
Iteration 18/25 | Loss: 0.00131527
Iteration 19/25 | Loss: 0.00131303
Iteration 20/25 | Loss: 0.00131189
Iteration 21/25 | Loss: 0.00131041
Iteration 22/25 | Loss: 0.00131045
Iteration 23/25 | Loss: 0.00131196
Iteration 24/25 | Loss: 0.00130993
Iteration 25/25 | Loss: 0.00130921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21786344
Iteration 2/25 | Loss: 0.00121206
Iteration 3/25 | Loss: 0.00121202
Iteration 4/25 | Loss: 0.00121202
Iteration 5/25 | Loss: 0.00121202
Iteration 6/25 | Loss: 0.00121202
Iteration 7/25 | Loss: 0.00121202
Iteration 8/25 | Loss: 0.00121202
Iteration 9/25 | Loss: 0.00121202
Iteration 10/25 | Loss: 0.00121202
Iteration 11/25 | Loss: 0.00121202
Iteration 12/25 | Loss: 0.00121202
Iteration 13/25 | Loss: 0.00121202
Iteration 14/25 | Loss: 0.00121202
Iteration 15/25 | Loss: 0.00121202
Iteration 16/25 | Loss: 0.00121202
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012120157480239868, 0.0012120157480239868, 0.0012120157480239868, 0.0012120157480239868, 0.0012120157480239868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012120157480239868

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121202
Iteration 2/1000 | Loss: 0.00029596
Iteration 3/1000 | Loss: 0.00033405
Iteration 4/1000 | Loss: 0.00029872
Iteration 5/1000 | Loss: 0.00034036
Iteration 6/1000 | Loss: 0.00076913
Iteration 7/1000 | Loss: 0.00028993
Iteration 8/1000 | Loss: 0.00013106
Iteration 9/1000 | Loss: 0.00107231
Iteration 10/1000 | Loss: 0.00127571
Iteration 11/1000 | Loss: 0.00020823
Iteration 12/1000 | Loss: 0.00012008
Iteration 13/1000 | Loss: 0.00014964
Iteration 14/1000 | Loss: 0.00009491
Iteration 15/1000 | Loss: 0.00009537
Iteration 16/1000 | Loss: 0.00009499
Iteration 17/1000 | Loss: 0.00024449
Iteration 18/1000 | Loss: 0.00010450
Iteration 19/1000 | Loss: 0.00008554
Iteration 20/1000 | Loss: 0.00014661
Iteration 21/1000 | Loss: 0.00019920
Iteration 22/1000 | Loss: 0.00019045
Iteration 23/1000 | Loss: 0.00016982
Iteration 24/1000 | Loss: 0.00018168
Iteration 25/1000 | Loss: 0.00023077
Iteration 26/1000 | Loss: 0.00007623
Iteration 27/1000 | Loss: 0.00007437
Iteration 28/1000 | Loss: 0.00007366
Iteration 29/1000 | Loss: 0.00005822
Iteration 30/1000 | Loss: 0.00007664
Iteration 31/1000 | Loss: 0.00007226
Iteration 32/1000 | Loss: 0.00007082
Iteration 33/1000 | Loss: 0.00006508
Iteration 34/1000 | Loss: 0.00006430
Iteration 35/1000 | Loss: 0.00006413
Iteration 36/1000 | Loss: 0.00005782
Iteration 37/1000 | Loss: 0.00048949
Iteration 38/1000 | Loss: 0.00040917
Iteration 39/1000 | Loss: 0.00106227
Iteration 40/1000 | Loss: 0.00176092
Iteration 41/1000 | Loss: 0.00070161
Iteration 42/1000 | Loss: 0.00012340
Iteration 43/1000 | Loss: 0.00009138
Iteration 44/1000 | Loss: 0.00011801
Iteration 45/1000 | Loss: 0.00007138
Iteration 46/1000 | Loss: 0.00006116
Iteration 47/1000 | Loss: 0.00005571
Iteration 48/1000 | Loss: 0.00007321
Iteration 49/1000 | Loss: 0.00005609
Iteration 50/1000 | Loss: 0.00007197
Iteration 51/1000 | Loss: 0.00005127
Iteration 52/1000 | Loss: 0.00005396
Iteration 53/1000 | Loss: 0.00053612
Iteration 54/1000 | Loss: 0.00055048
Iteration 55/1000 | Loss: 0.00007190
Iteration 56/1000 | Loss: 0.00006014
Iteration 57/1000 | Loss: 0.00005890
Iteration 58/1000 | Loss: 0.00004895
Iteration 59/1000 | Loss: 0.00005249
Iteration 60/1000 | Loss: 0.00005241
Iteration 61/1000 | Loss: 0.00005234
Iteration 62/1000 | Loss: 0.00006261
Iteration 63/1000 | Loss: 0.00004819
Iteration 64/1000 | Loss: 0.00004938
Iteration 65/1000 | Loss: 0.00005876
Iteration 66/1000 | Loss: 0.00006568
Iteration 67/1000 | Loss: 0.00004818
Iteration 68/1000 | Loss: 0.00003854
Iteration 69/1000 | Loss: 0.00006621
Iteration 70/1000 | Loss: 0.00005290
Iteration 71/1000 | Loss: 0.00006069
Iteration 72/1000 | Loss: 0.00004907
Iteration 73/1000 | Loss: 0.00004003
Iteration 74/1000 | Loss: 0.00003943
Iteration 75/1000 | Loss: 0.00005367
Iteration 76/1000 | Loss: 0.00004998
Iteration 77/1000 | Loss: 0.00005457
Iteration 78/1000 | Loss: 0.00046437
Iteration 79/1000 | Loss: 0.00005791
Iteration 80/1000 | Loss: 0.00004361
Iteration 81/1000 | Loss: 0.00006764
Iteration 82/1000 | Loss: 0.00003937
Iteration 83/1000 | Loss: 0.00003419
Iteration 84/1000 | Loss: 0.00003290
Iteration 85/1000 | Loss: 0.00003211
Iteration 86/1000 | Loss: 0.00003532
Iteration 87/1000 | Loss: 0.00003235
Iteration 88/1000 | Loss: 0.00003279
Iteration 89/1000 | Loss: 0.00003636
Iteration 90/1000 | Loss: 0.00004055
Iteration 91/1000 | Loss: 0.00004266
Iteration 92/1000 | Loss: 0.00004721
Iteration 93/1000 | Loss: 0.00004635
Iteration 94/1000 | Loss: 0.00005006
Iteration 95/1000 | Loss: 0.00003609
Iteration 96/1000 | Loss: 0.00003502
Iteration 97/1000 | Loss: 0.00003233
Iteration 98/1000 | Loss: 0.00003143
Iteration 99/1000 | Loss: 0.00003107
Iteration 100/1000 | Loss: 0.00003071
Iteration 101/1000 | Loss: 0.00003053
Iteration 102/1000 | Loss: 0.00003050
Iteration 103/1000 | Loss: 0.00003041
Iteration 104/1000 | Loss: 0.00003035
Iteration 105/1000 | Loss: 0.00003032
Iteration 106/1000 | Loss: 0.00003032
Iteration 107/1000 | Loss: 0.00003032
Iteration 108/1000 | Loss: 0.00003032
Iteration 109/1000 | Loss: 0.00003032
Iteration 110/1000 | Loss: 0.00003031
Iteration 111/1000 | Loss: 0.00003031
Iteration 112/1000 | Loss: 0.00003031
Iteration 113/1000 | Loss: 0.00003030
Iteration 114/1000 | Loss: 0.00003030
Iteration 115/1000 | Loss: 0.00003029
Iteration 116/1000 | Loss: 0.00003028
Iteration 117/1000 | Loss: 0.00003028
Iteration 118/1000 | Loss: 0.00003028
Iteration 119/1000 | Loss: 0.00003027
Iteration 120/1000 | Loss: 0.00003027
Iteration 121/1000 | Loss: 0.00003027
Iteration 122/1000 | Loss: 0.00003027
Iteration 123/1000 | Loss: 0.00003026
Iteration 124/1000 | Loss: 0.00003026
Iteration 125/1000 | Loss: 0.00003026
Iteration 126/1000 | Loss: 0.00003025
Iteration 127/1000 | Loss: 0.00003025
Iteration 128/1000 | Loss: 0.00003024
Iteration 129/1000 | Loss: 0.00003024
Iteration 130/1000 | Loss: 0.00003024
Iteration 131/1000 | Loss: 0.00003023
Iteration 132/1000 | Loss: 0.00003023
Iteration 133/1000 | Loss: 0.00003022
Iteration 134/1000 | Loss: 0.00003022
Iteration 135/1000 | Loss: 0.00003021
Iteration 136/1000 | Loss: 0.00003021
Iteration 137/1000 | Loss: 0.00003021
Iteration 138/1000 | Loss: 0.00003020
Iteration 139/1000 | Loss: 0.00003020
Iteration 140/1000 | Loss: 0.00003019
Iteration 141/1000 | Loss: 0.00003019
Iteration 142/1000 | Loss: 0.00003019
Iteration 143/1000 | Loss: 0.00003018
Iteration 144/1000 | Loss: 0.00003018
Iteration 145/1000 | Loss: 0.00003017
Iteration 146/1000 | Loss: 0.00003017
Iteration 147/1000 | Loss: 0.00003017
Iteration 148/1000 | Loss: 0.00003017
Iteration 149/1000 | Loss: 0.00003016
Iteration 150/1000 | Loss: 0.00003016
Iteration 151/1000 | Loss: 0.00003016
Iteration 152/1000 | Loss: 0.00003015
Iteration 153/1000 | Loss: 0.00003011
Iteration 154/1000 | Loss: 0.00003011
Iteration 155/1000 | Loss: 0.00003010
Iteration 156/1000 | Loss: 0.00003010
Iteration 157/1000 | Loss: 0.00003009
Iteration 158/1000 | Loss: 0.00003009
Iteration 159/1000 | Loss: 0.00003009
Iteration 160/1000 | Loss: 0.00003009
Iteration 161/1000 | Loss: 0.00003009
Iteration 162/1000 | Loss: 0.00003008
Iteration 163/1000 | Loss: 0.00003008
Iteration 164/1000 | Loss: 0.00003008
Iteration 165/1000 | Loss: 0.00003008
Iteration 166/1000 | Loss: 0.00003008
Iteration 167/1000 | Loss: 0.00003008
Iteration 168/1000 | Loss: 0.00003008
Iteration 169/1000 | Loss: 0.00003008
Iteration 170/1000 | Loss: 0.00003008
Iteration 171/1000 | Loss: 0.00003007
Iteration 172/1000 | Loss: 0.00003007
Iteration 173/1000 | Loss: 0.00003007
Iteration 174/1000 | Loss: 0.00003007
Iteration 175/1000 | Loss: 0.00003007
Iteration 176/1000 | Loss: 0.00003007
Iteration 177/1000 | Loss: 0.00003007
Iteration 178/1000 | Loss: 0.00003007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [3.0073075322434306e-05, 3.0073075322434306e-05, 3.0073075322434306e-05, 3.0073075322434306e-05, 3.0073075322434306e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0073075322434306e-05

Optimization complete. Final v2v error: 4.393913745880127 mm

Highest mean error: 5.83071756362915 mm for frame 99

Lowest mean error: 3.7916100025177 mm for frame 24

Saving results

Total time: 224.90919494628906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403706
Iteration 2/25 | Loss: 0.00129869
Iteration 3/25 | Loss: 0.00119741
Iteration 4/25 | Loss: 0.00118593
Iteration 5/25 | Loss: 0.00118241
Iteration 6/25 | Loss: 0.00118154
Iteration 7/25 | Loss: 0.00118153
Iteration 8/25 | Loss: 0.00118153
Iteration 9/25 | Loss: 0.00118153
Iteration 10/25 | Loss: 0.00118153
Iteration 11/25 | Loss: 0.00118153
Iteration 12/25 | Loss: 0.00118153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011815321631729603, 0.0011815321631729603, 0.0011815321631729603, 0.0011815321631729603, 0.0011815321631729603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011815321631729603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28169608
Iteration 2/25 | Loss: 0.00094702
Iteration 3/25 | Loss: 0.00094702
Iteration 4/25 | Loss: 0.00094702
Iteration 5/25 | Loss: 0.00094702
Iteration 6/25 | Loss: 0.00094702
Iteration 7/25 | Loss: 0.00094702
Iteration 8/25 | Loss: 0.00094702
Iteration 9/25 | Loss: 0.00094702
Iteration 10/25 | Loss: 0.00094702
Iteration 11/25 | Loss: 0.00094702
Iteration 12/25 | Loss: 0.00094702
Iteration 13/25 | Loss: 0.00094702
Iteration 14/25 | Loss: 0.00094702
Iteration 15/25 | Loss: 0.00094702
Iteration 16/25 | Loss: 0.00094702
Iteration 17/25 | Loss: 0.00094702
Iteration 18/25 | Loss: 0.00094702
Iteration 19/25 | Loss: 0.00094702
Iteration 20/25 | Loss: 0.00094702
Iteration 21/25 | Loss: 0.00094702
Iteration 22/25 | Loss: 0.00094702
Iteration 23/25 | Loss: 0.00094702
Iteration 24/25 | Loss: 0.00094702
Iteration 25/25 | Loss: 0.00094702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094702
Iteration 2/1000 | Loss: 0.00008504
Iteration 3/1000 | Loss: 0.00004672
Iteration 4/1000 | Loss: 0.00003190
Iteration 5/1000 | Loss: 0.00002699
Iteration 6/1000 | Loss: 0.00002407
Iteration 7/1000 | Loss: 0.00002240
Iteration 8/1000 | Loss: 0.00002135
Iteration 9/1000 | Loss: 0.00002062
Iteration 10/1000 | Loss: 0.00001997
Iteration 11/1000 | Loss: 0.00001950
Iteration 12/1000 | Loss: 0.00001915
Iteration 13/1000 | Loss: 0.00001893
Iteration 14/1000 | Loss: 0.00001882
Iteration 15/1000 | Loss: 0.00001861
Iteration 16/1000 | Loss: 0.00001860
Iteration 17/1000 | Loss: 0.00001853
Iteration 18/1000 | Loss: 0.00001847
Iteration 19/1000 | Loss: 0.00001846
Iteration 20/1000 | Loss: 0.00001842
Iteration 21/1000 | Loss: 0.00001842
Iteration 22/1000 | Loss: 0.00001841
Iteration 23/1000 | Loss: 0.00001840
Iteration 24/1000 | Loss: 0.00001839
Iteration 25/1000 | Loss: 0.00001839
Iteration 26/1000 | Loss: 0.00001838
Iteration 27/1000 | Loss: 0.00001836
Iteration 28/1000 | Loss: 0.00001834
Iteration 29/1000 | Loss: 0.00001834
Iteration 30/1000 | Loss: 0.00001833
Iteration 31/1000 | Loss: 0.00001830
Iteration 32/1000 | Loss: 0.00001826
Iteration 33/1000 | Loss: 0.00001826
Iteration 34/1000 | Loss: 0.00001825
Iteration 35/1000 | Loss: 0.00001825
Iteration 36/1000 | Loss: 0.00001825
Iteration 37/1000 | Loss: 0.00001824
Iteration 38/1000 | Loss: 0.00001824
Iteration 39/1000 | Loss: 0.00001823
Iteration 40/1000 | Loss: 0.00001823
Iteration 41/1000 | Loss: 0.00001822
Iteration 42/1000 | Loss: 0.00001822
Iteration 43/1000 | Loss: 0.00001822
Iteration 44/1000 | Loss: 0.00001822
Iteration 45/1000 | Loss: 0.00001822
Iteration 46/1000 | Loss: 0.00001822
Iteration 47/1000 | Loss: 0.00001821
Iteration 48/1000 | Loss: 0.00001821
Iteration 49/1000 | Loss: 0.00001820
Iteration 50/1000 | Loss: 0.00001820
Iteration 51/1000 | Loss: 0.00001819
Iteration 52/1000 | Loss: 0.00001819
Iteration 53/1000 | Loss: 0.00001819
Iteration 54/1000 | Loss: 0.00001819
Iteration 55/1000 | Loss: 0.00001819
Iteration 56/1000 | Loss: 0.00001819
Iteration 57/1000 | Loss: 0.00001819
Iteration 58/1000 | Loss: 0.00001818
Iteration 59/1000 | Loss: 0.00001818
Iteration 60/1000 | Loss: 0.00001818
Iteration 61/1000 | Loss: 0.00001818
Iteration 62/1000 | Loss: 0.00001818
Iteration 63/1000 | Loss: 0.00001818
Iteration 64/1000 | Loss: 0.00001818
Iteration 65/1000 | Loss: 0.00001818
Iteration 66/1000 | Loss: 0.00001818
Iteration 67/1000 | Loss: 0.00001818
Iteration 68/1000 | Loss: 0.00001818
Iteration 69/1000 | Loss: 0.00001817
Iteration 70/1000 | Loss: 0.00001817
Iteration 71/1000 | Loss: 0.00001817
Iteration 72/1000 | Loss: 0.00001817
Iteration 73/1000 | Loss: 0.00001817
Iteration 74/1000 | Loss: 0.00001817
Iteration 75/1000 | Loss: 0.00001817
Iteration 76/1000 | Loss: 0.00001817
Iteration 77/1000 | Loss: 0.00001817
Iteration 78/1000 | Loss: 0.00001816
Iteration 79/1000 | Loss: 0.00001816
Iteration 80/1000 | Loss: 0.00001816
Iteration 81/1000 | Loss: 0.00001816
Iteration 82/1000 | Loss: 0.00001816
Iteration 83/1000 | Loss: 0.00001816
Iteration 84/1000 | Loss: 0.00001816
Iteration 85/1000 | Loss: 0.00001816
Iteration 86/1000 | Loss: 0.00001816
Iteration 87/1000 | Loss: 0.00001815
Iteration 88/1000 | Loss: 0.00001815
Iteration 89/1000 | Loss: 0.00001815
Iteration 90/1000 | Loss: 0.00001815
Iteration 91/1000 | Loss: 0.00001815
Iteration 92/1000 | Loss: 0.00001815
Iteration 93/1000 | Loss: 0.00001815
Iteration 94/1000 | Loss: 0.00001815
Iteration 95/1000 | Loss: 0.00001815
Iteration 96/1000 | Loss: 0.00001815
Iteration 97/1000 | Loss: 0.00001815
Iteration 98/1000 | Loss: 0.00001815
Iteration 99/1000 | Loss: 0.00001815
Iteration 100/1000 | Loss: 0.00001815
Iteration 101/1000 | Loss: 0.00001815
Iteration 102/1000 | Loss: 0.00001814
Iteration 103/1000 | Loss: 0.00001814
Iteration 104/1000 | Loss: 0.00001814
Iteration 105/1000 | Loss: 0.00001814
Iteration 106/1000 | Loss: 0.00001814
Iteration 107/1000 | Loss: 0.00001814
Iteration 108/1000 | Loss: 0.00001814
Iteration 109/1000 | Loss: 0.00001814
Iteration 110/1000 | Loss: 0.00001814
Iteration 111/1000 | Loss: 0.00001814
Iteration 112/1000 | Loss: 0.00001814
Iteration 113/1000 | Loss: 0.00001814
Iteration 114/1000 | Loss: 0.00001814
Iteration 115/1000 | Loss: 0.00001814
Iteration 116/1000 | Loss: 0.00001813
Iteration 117/1000 | Loss: 0.00001813
Iteration 118/1000 | Loss: 0.00001813
Iteration 119/1000 | Loss: 0.00001813
Iteration 120/1000 | Loss: 0.00001813
Iteration 121/1000 | Loss: 0.00001813
Iteration 122/1000 | Loss: 0.00001813
Iteration 123/1000 | Loss: 0.00001813
Iteration 124/1000 | Loss: 0.00001813
Iteration 125/1000 | Loss: 0.00001813
Iteration 126/1000 | Loss: 0.00001813
Iteration 127/1000 | Loss: 0.00001813
Iteration 128/1000 | Loss: 0.00001813
Iteration 129/1000 | Loss: 0.00001813
Iteration 130/1000 | Loss: 0.00001813
Iteration 131/1000 | Loss: 0.00001813
Iteration 132/1000 | Loss: 0.00001813
Iteration 133/1000 | Loss: 0.00001813
Iteration 134/1000 | Loss: 0.00001813
Iteration 135/1000 | Loss: 0.00001812
Iteration 136/1000 | Loss: 0.00001812
Iteration 137/1000 | Loss: 0.00001812
Iteration 138/1000 | Loss: 0.00001812
Iteration 139/1000 | Loss: 0.00001812
Iteration 140/1000 | Loss: 0.00001812
Iteration 141/1000 | Loss: 0.00001812
Iteration 142/1000 | Loss: 0.00001812
Iteration 143/1000 | Loss: 0.00001812
Iteration 144/1000 | Loss: 0.00001812
Iteration 145/1000 | Loss: 0.00001812
Iteration 146/1000 | Loss: 0.00001812
Iteration 147/1000 | Loss: 0.00001812
Iteration 148/1000 | Loss: 0.00001812
Iteration 149/1000 | Loss: 0.00001812
Iteration 150/1000 | Loss: 0.00001812
Iteration 151/1000 | Loss: 0.00001812
Iteration 152/1000 | Loss: 0.00001812
Iteration 153/1000 | Loss: 0.00001812
Iteration 154/1000 | Loss: 0.00001812
Iteration 155/1000 | Loss: 0.00001812
Iteration 156/1000 | Loss: 0.00001812
Iteration 157/1000 | Loss: 0.00001812
Iteration 158/1000 | Loss: 0.00001812
Iteration 159/1000 | Loss: 0.00001811
Iteration 160/1000 | Loss: 0.00001811
Iteration 161/1000 | Loss: 0.00001811
Iteration 162/1000 | Loss: 0.00001811
Iteration 163/1000 | Loss: 0.00001811
Iteration 164/1000 | Loss: 0.00001811
Iteration 165/1000 | Loss: 0.00001811
Iteration 166/1000 | Loss: 0.00001811
Iteration 167/1000 | Loss: 0.00001811
Iteration 168/1000 | Loss: 0.00001811
Iteration 169/1000 | Loss: 0.00001811
Iteration 170/1000 | Loss: 0.00001811
Iteration 171/1000 | Loss: 0.00001811
Iteration 172/1000 | Loss: 0.00001811
Iteration 173/1000 | Loss: 0.00001811
Iteration 174/1000 | Loss: 0.00001810
Iteration 175/1000 | Loss: 0.00001810
Iteration 176/1000 | Loss: 0.00001810
Iteration 177/1000 | Loss: 0.00001810
Iteration 178/1000 | Loss: 0.00001810
Iteration 179/1000 | Loss: 0.00001810
Iteration 180/1000 | Loss: 0.00001810
Iteration 181/1000 | Loss: 0.00001810
Iteration 182/1000 | Loss: 0.00001810
Iteration 183/1000 | Loss: 0.00001810
Iteration 184/1000 | Loss: 0.00001810
Iteration 185/1000 | Loss: 0.00001810
Iteration 186/1000 | Loss: 0.00001810
Iteration 187/1000 | Loss: 0.00001810
Iteration 188/1000 | Loss: 0.00001810
Iteration 189/1000 | Loss: 0.00001810
Iteration 190/1000 | Loss: 0.00001810
Iteration 191/1000 | Loss: 0.00001810
Iteration 192/1000 | Loss: 0.00001809
Iteration 193/1000 | Loss: 0.00001809
Iteration 194/1000 | Loss: 0.00001809
Iteration 195/1000 | Loss: 0.00001809
Iteration 196/1000 | Loss: 0.00001809
Iteration 197/1000 | Loss: 0.00001809
Iteration 198/1000 | Loss: 0.00001809
Iteration 199/1000 | Loss: 0.00001809
Iteration 200/1000 | Loss: 0.00001809
Iteration 201/1000 | Loss: 0.00001809
Iteration 202/1000 | Loss: 0.00001809
Iteration 203/1000 | Loss: 0.00001809
Iteration 204/1000 | Loss: 0.00001809
Iteration 205/1000 | Loss: 0.00001809
Iteration 206/1000 | Loss: 0.00001809
Iteration 207/1000 | Loss: 0.00001809
Iteration 208/1000 | Loss: 0.00001809
Iteration 209/1000 | Loss: 0.00001809
Iteration 210/1000 | Loss: 0.00001809
Iteration 211/1000 | Loss: 0.00001809
Iteration 212/1000 | Loss: 0.00001809
Iteration 213/1000 | Loss: 0.00001809
Iteration 214/1000 | Loss: 0.00001809
Iteration 215/1000 | Loss: 0.00001809
Iteration 216/1000 | Loss: 0.00001809
Iteration 217/1000 | Loss: 0.00001809
Iteration 218/1000 | Loss: 0.00001809
Iteration 219/1000 | Loss: 0.00001809
Iteration 220/1000 | Loss: 0.00001809
Iteration 221/1000 | Loss: 0.00001809
Iteration 222/1000 | Loss: 0.00001809
Iteration 223/1000 | Loss: 0.00001809
Iteration 224/1000 | Loss: 0.00001809
Iteration 225/1000 | Loss: 0.00001809
Iteration 226/1000 | Loss: 0.00001809
Iteration 227/1000 | Loss: 0.00001809
Iteration 228/1000 | Loss: 0.00001809
Iteration 229/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.808576053008437e-05, 1.808576053008437e-05, 1.808576053008437e-05, 1.808576053008437e-05, 1.808576053008437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.808576053008437e-05

Optimization complete. Final v2v error: 3.653437852859497 mm

Highest mean error: 4.366916656494141 mm for frame 70

Lowest mean error: 3.016655683517456 mm for frame 82

Saving results

Total time: 42.63781237602234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946601
Iteration 2/25 | Loss: 0.00149406
Iteration 3/25 | Loss: 0.00126552
Iteration 4/25 | Loss: 0.00124481
Iteration 5/25 | Loss: 0.00123972
Iteration 6/25 | Loss: 0.00123874
Iteration 7/25 | Loss: 0.00123874
Iteration 8/25 | Loss: 0.00123874
Iteration 9/25 | Loss: 0.00123874
Iteration 10/25 | Loss: 0.00123874
Iteration 11/25 | Loss: 0.00123874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012387408642098308, 0.0012387408642098308, 0.0012387408642098308, 0.0012387408642098308, 0.0012387408642098308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012387408642098308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15197682
Iteration 2/25 | Loss: 0.00124870
Iteration 3/25 | Loss: 0.00124870
Iteration 4/25 | Loss: 0.00124870
Iteration 5/25 | Loss: 0.00124870
Iteration 6/25 | Loss: 0.00124870
Iteration 7/25 | Loss: 0.00124870
Iteration 8/25 | Loss: 0.00124870
Iteration 9/25 | Loss: 0.00124870
Iteration 10/25 | Loss: 0.00124870
Iteration 11/25 | Loss: 0.00124870
Iteration 12/25 | Loss: 0.00124870
Iteration 13/25 | Loss: 0.00124870
Iteration 14/25 | Loss: 0.00124870
Iteration 15/25 | Loss: 0.00124870
Iteration 16/25 | Loss: 0.00124870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001248699496500194, 0.001248699496500194, 0.001248699496500194, 0.001248699496500194, 0.001248699496500194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001248699496500194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124870
Iteration 2/1000 | Loss: 0.00011499
Iteration 3/1000 | Loss: 0.00005866
Iteration 4/1000 | Loss: 0.00004026
Iteration 5/1000 | Loss: 0.00003570
Iteration 6/1000 | Loss: 0.00003235
Iteration 7/1000 | Loss: 0.00003061
Iteration 8/1000 | Loss: 0.00002967
Iteration 9/1000 | Loss: 0.00002907
Iteration 10/1000 | Loss: 0.00002841
Iteration 11/1000 | Loss: 0.00002787
Iteration 12/1000 | Loss: 0.00002750
Iteration 13/1000 | Loss: 0.00002714
Iteration 14/1000 | Loss: 0.00002687
Iteration 15/1000 | Loss: 0.00002667
Iteration 16/1000 | Loss: 0.00002657
Iteration 17/1000 | Loss: 0.00002645
Iteration 18/1000 | Loss: 0.00002644
Iteration 19/1000 | Loss: 0.00002638
Iteration 20/1000 | Loss: 0.00002638
Iteration 21/1000 | Loss: 0.00002626
Iteration 22/1000 | Loss: 0.00002625
Iteration 23/1000 | Loss: 0.00002624
Iteration 24/1000 | Loss: 0.00002623
Iteration 25/1000 | Loss: 0.00002621
Iteration 26/1000 | Loss: 0.00002615
Iteration 27/1000 | Loss: 0.00002612
Iteration 28/1000 | Loss: 0.00002612
Iteration 29/1000 | Loss: 0.00002611
Iteration 30/1000 | Loss: 0.00002611
Iteration 31/1000 | Loss: 0.00002610
Iteration 32/1000 | Loss: 0.00002609
Iteration 33/1000 | Loss: 0.00002609
Iteration 34/1000 | Loss: 0.00002608
Iteration 35/1000 | Loss: 0.00002608
Iteration 36/1000 | Loss: 0.00002607
Iteration 37/1000 | Loss: 0.00002606
Iteration 38/1000 | Loss: 0.00002606
Iteration 39/1000 | Loss: 0.00002606
Iteration 40/1000 | Loss: 0.00002606
Iteration 41/1000 | Loss: 0.00002606
Iteration 42/1000 | Loss: 0.00002606
Iteration 43/1000 | Loss: 0.00002606
Iteration 44/1000 | Loss: 0.00002606
Iteration 45/1000 | Loss: 0.00002606
Iteration 46/1000 | Loss: 0.00002606
Iteration 47/1000 | Loss: 0.00002606
Iteration 48/1000 | Loss: 0.00002606
Iteration 49/1000 | Loss: 0.00002605
Iteration 50/1000 | Loss: 0.00002605
Iteration 51/1000 | Loss: 0.00002605
Iteration 52/1000 | Loss: 0.00002605
Iteration 53/1000 | Loss: 0.00002605
Iteration 54/1000 | Loss: 0.00002605
Iteration 55/1000 | Loss: 0.00002605
Iteration 56/1000 | Loss: 0.00002605
Iteration 57/1000 | Loss: 0.00002605
Iteration 58/1000 | Loss: 0.00002604
Iteration 59/1000 | Loss: 0.00002604
Iteration 60/1000 | Loss: 0.00002603
Iteration 61/1000 | Loss: 0.00002603
Iteration 62/1000 | Loss: 0.00002602
Iteration 63/1000 | Loss: 0.00002602
Iteration 64/1000 | Loss: 0.00002602
Iteration 65/1000 | Loss: 0.00002601
Iteration 66/1000 | Loss: 0.00002601
Iteration 67/1000 | Loss: 0.00002601
Iteration 68/1000 | Loss: 0.00002601
Iteration 69/1000 | Loss: 0.00002600
Iteration 70/1000 | Loss: 0.00002600
Iteration 71/1000 | Loss: 0.00002600
Iteration 72/1000 | Loss: 0.00002600
Iteration 73/1000 | Loss: 0.00002600
Iteration 74/1000 | Loss: 0.00002599
Iteration 75/1000 | Loss: 0.00002599
Iteration 76/1000 | Loss: 0.00002599
Iteration 77/1000 | Loss: 0.00002599
Iteration 78/1000 | Loss: 0.00002599
Iteration 79/1000 | Loss: 0.00002599
Iteration 80/1000 | Loss: 0.00002599
Iteration 81/1000 | Loss: 0.00002599
Iteration 82/1000 | Loss: 0.00002599
Iteration 83/1000 | Loss: 0.00002599
Iteration 84/1000 | Loss: 0.00002598
Iteration 85/1000 | Loss: 0.00002598
Iteration 86/1000 | Loss: 0.00002598
Iteration 87/1000 | Loss: 0.00002598
Iteration 88/1000 | Loss: 0.00002598
Iteration 89/1000 | Loss: 0.00002598
Iteration 90/1000 | Loss: 0.00002598
Iteration 91/1000 | Loss: 0.00002598
Iteration 92/1000 | Loss: 0.00002598
Iteration 93/1000 | Loss: 0.00002598
Iteration 94/1000 | Loss: 0.00002598
Iteration 95/1000 | Loss: 0.00002598
Iteration 96/1000 | Loss: 0.00002597
Iteration 97/1000 | Loss: 0.00002597
Iteration 98/1000 | Loss: 0.00002597
Iteration 99/1000 | Loss: 0.00002597
Iteration 100/1000 | Loss: 0.00002597
Iteration 101/1000 | Loss: 0.00002597
Iteration 102/1000 | Loss: 0.00002597
Iteration 103/1000 | Loss: 0.00002597
Iteration 104/1000 | Loss: 0.00002597
Iteration 105/1000 | Loss: 0.00002597
Iteration 106/1000 | Loss: 0.00002597
Iteration 107/1000 | Loss: 0.00002596
Iteration 108/1000 | Loss: 0.00002596
Iteration 109/1000 | Loss: 0.00002596
Iteration 110/1000 | Loss: 0.00002596
Iteration 111/1000 | Loss: 0.00002596
Iteration 112/1000 | Loss: 0.00002596
Iteration 113/1000 | Loss: 0.00002596
Iteration 114/1000 | Loss: 0.00002596
Iteration 115/1000 | Loss: 0.00002595
Iteration 116/1000 | Loss: 0.00002595
Iteration 117/1000 | Loss: 0.00002595
Iteration 118/1000 | Loss: 0.00002595
Iteration 119/1000 | Loss: 0.00002595
Iteration 120/1000 | Loss: 0.00002595
Iteration 121/1000 | Loss: 0.00002595
Iteration 122/1000 | Loss: 0.00002595
Iteration 123/1000 | Loss: 0.00002595
Iteration 124/1000 | Loss: 0.00002595
Iteration 125/1000 | Loss: 0.00002595
Iteration 126/1000 | Loss: 0.00002595
Iteration 127/1000 | Loss: 0.00002595
Iteration 128/1000 | Loss: 0.00002595
Iteration 129/1000 | Loss: 0.00002595
Iteration 130/1000 | Loss: 0.00002595
Iteration 131/1000 | Loss: 0.00002595
Iteration 132/1000 | Loss: 0.00002595
Iteration 133/1000 | Loss: 0.00002595
Iteration 134/1000 | Loss: 0.00002595
Iteration 135/1000 | Loss: 0.00002595
Iteration 136/1000 | Loss: 0.00002595
Iteration 137/1000 | Loss: 0.00002595
Iteration 138/1000 | Loss: 0.00002595
Iteration 139/1000 | Loss: 0.00002595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.5949722839868627e-05, 2.5949722839868627e-05, 2.5949722839868627e-05, 2.5949722839868627e-05, 2.5949722839868627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5949722839868627e-05

Optimization complete. Final v2v error: 4.3903093338012695 mm

Highest mean error: 4.930215358734131 mm for frame 209

Lowest mean error: 3.9809513092041016 mm for frame 0

Saving results

Total time: 48.88713979721069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835463
Iteration 2/25 | Loss: 0.00151423
Iteration 3/25 | Loss: 0.00118662
Iteration 4/25 | Loss: 0.00116122
Iteration 5/25 | Loss: 0.00115612
Iteration 6/25 | Loss: 0.00115520
Iteration 7/25 | Loss: 0.00115520
Iteration 8/25 | Loss: 0.00115520
Iteration 9/25 | Loss: 0.00115520
Iteration 10/25 | Loss: 0.00115520
Iteration 11/25 | Loss: 0.00115520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011552036739885807, 0.0011552036739885807, 0.0011552036739885807, 0.0011552036739885807, 0.0011552036739885807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011552036739885807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31241643
Iteration 2/25 | Loss: 0.00094513
Iteration 3/25 | Loss: 0.00094513
Iteration 4/25 | Loss: 0.00094513
Iteration 5/25 | Loss: 0.00094512
Iteration 6/25 | Loss: 0.00094512
Iteration 7/25 | Loss: 0.00094512
Iteration 8/25 | Loss: 0.00094512
Iteration 9/25 | Loss: 0.00094512
Iteration 10/25 | Loss: 0.00094512
Iteration 11/25 | Loss: 0.00094512
Iteration 12/25 | Loss: 0.00094512
Iteration 13/25 | Loss: 0.00094512
Iteration 14/25 | Loss: 0.00094512
Iteration 15/25 | Loss: 0.00094512
Iteration 16/25 | Loss: 0.00094512
Iteration 17/25 | Loss: 0.00094512
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009451222722418606, 0.0009451222722418606, 0.0009451222722418606, 0.0009451222722418606, 0.0009451222722418606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009451222722418606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094512
Iteration 2/1000 | Loss: 0.00005129
Iteration 3/1000 | Loss: 0.00002487
Iteration 4/1000 | Loss: 0.00002071
Iteration 5/1000 | Loss: 0.00001818
Iteration 6/1000 | Loss: 0.00001677
Iteration 7/1000 | Loss: 0.00001595
Iteration 8/1000 | Loss: 0.00001539
Iteration 9/1000 | Loss: 0.00001488
Iteration 10/1000 | Loss: 0.00001474
Iteration 11/1000 | Loss: 0.00001464
Iteration 12/1000 | Loss: 0.00001461
Iteration 13/1000 | Loss: 0.00001448
Iteration 14/1000 | Loss: 0.00001429
Iteration 15/1000 | Loss: 0.00001421
Iteration 16/1000 | Loss: 0.00001421
Iteration 17/1000 | Loss: 0.00001420
Iteration 18/1000 | Loss: 0.00001419
Iteration 19/1000 | Loss: 0.00001418
Iteration 20/1000 | Loss: 0.00001417
Iteration 21/1000 | Loss: 0.00001417
Iteration 22/1000 | Loss: 0.00001416
Iteration 23/1000 | Loss: 0.00001416
Iteration 24/1000 | Loss: 0.00001415
Iteration 25/1000 | Loss: 0.00001414
Iteration 26/1000 | Loss: 0.00001411
Iteration 27/1000 | Loss: 0.00001411
Iteration 28/1000 | Loss: 0.00001411
Iteration 29/1000 | Loss: 0.00001411
Iteration 30/1000 | Loss: 0.00001410
Iteration 31/1000 | Loss: 0.00001410
Iteration 32/1000 | Loss: 0.00001410
Iteration 33/1000 | Loss: 0.00001409
Iteration 34/1000 | Loss: 0.00001409
Iteration 35/1000 | Loss: 0.00001409
Iteration 36/1000 | Loss: 0.00001408
Iteration 37/1000 | Loss: 0.00001408
Iteration 38/1000 | Loss: 0.00001408
Iteration 39/1000 | Loss: 0.00001408
Iteration 40/1000 | Loss: 0.00001407
Iteration 41/1000 | Loss: 0.00001407
Iteration 42/1000 | Loss: 0.00001406
Iteration 43/1000 | Loss: 0.00001406
Iteration 44/1000 | Loss: 0.00001405
Iteration 45/1000 | Loss: 0.00001404
Iteration 46/1000 | Loss: 0.00001403
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001402
Iteration 49/1000 | Loss: 0.00001402
Iteration 50/1000 | Loss: 0.00001401
Iteration 51/1000 | Loss: 0.00001401
Iteration 52/1000 | Loss: 0.00001401
Iteration 53/1000 | Loss: 0.00001401
Iteration 54/1000 | Loss: 0.00001400
Iteration 55/1000 | Loss: 0.00001400
Iteration 56/1000 | Loss: 0.00001399
Iteration 57/1000 | Loss: 0.00001398
Iteration 58/1000 | Loss: 0.00001398
Iteration 59/1000 | Loss: 0.00001398
Iteration 60/1000 | Loss: 0.00001398
Iteration 61/1000 | Loss: 0.00001398
Iteration 62/1000 | Loss: 0.00001398
Iteration 63/1000 | Loss: 0.00001398
Iteration 64/1000 | Loss: 0.00001398
Iteration 65/1000 | Loss: 0.00001398
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001398
Iteration 68/1000 | Loss: 0.00001397
Iteration 69/1000 | Loss: 0.00001397
Iteration 70/1000 | Loss: 0.00001397
Iteration 71/1000 | Loss: 0.00001397
Iteration 72/1000 | Loss: 0.00001397
Iteration 73/1000 | Loss: 0.00001397
Iteration 74/1000 | Loss: 0.00001396
Iteration 75/1000 | Loss: 0.00001396
Iteration 76/1000 | Loss: 0.00001396
Iteration 77/1000 | Loss: 0.00001396
Iteration 78/1000 | Loss: 0.00001396
Iteration 79/1000 | Loss: 0.00001396
Iteration 80/1000 | Loss: 0.00001396
Iteration 81/1000 | Loss: 0.00001396
Iteration 82/1000 | Loss: 0.00001395
Iteration 83/1000 | Loss: 0.00001395
Iteration 84/1000 | Loss: 0.00001395
Iteration 85/1000 | Loss: 0.00001395
Iteration 86/1000 | Loss: 0.00001395
Iteration 87/1000 | Loss: 0.00001395
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001394
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001394
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001394
Iteration 94/1000 | Loss: 0.00001394
Iteration 95/1000 | Loss: 0.00001394
Iteration 96/1000 | Loss: 0.00001393
Iteration 97/1000 | Loss: 0.00001393
Iteration 98/1000 | Loss: 0.00001393
Iteration 99/1000 | Loss: 0.00001393
Iteration 100/1000 | Loss: 0.00001393
Iteration 101/1000 | Loss: 0.00001393
Iteration 102/1000 | Loss: 0.00001393
Iteration 103/1000 | Loss: 0.00001393
Iteration 104/1000 | Loss: 0.00001393
Iteration 105/1000 | Loss: 0.00001393
Iteration 106/1000 | Loss: 0.00001393
Iteration 107/1000 | Loss: 0.00001393
Iteration 108/1000 | Loss: 0.00001393
Iteration 109/1000 | Loss: 0.00001393
Iteration 110/1000 | Loss: 0.00001393
Iteration 111/1000 | Loss: 0.00001393
Iteration 112/1000 | Loss: 0.00001393
Iteration 113/1000 | Loss: 0.00001393
Iteration 114/1000 | Loss: 0.00001393
Iteration 115/1000 | Loss: 0.00001393
Iteration 116/1000 | Loss: 0.00001393
Iteration 117/1000 | Loss: 0.00001393
Iteration 118/1000 | Loss: 0.00001393
Iteration 119/1000 | Loss: 0.00001393
Iteration 120/1000 | Loss: 0.00001393
Iteration 121/1000 | Loss: 0.00001393
Iteration 122/1000 | Loss: 0.00001393
Iteration 123/1000 | Loss: 0.00001393
Iteration 124/1000 | Loss: 0.00001393
Iteration 125/1000 | Loss: 0.00001393
Iteration 126/1000 | Loss: 0.00001393
Iteration 127/1000 | Loss: 0.00001393
Iteration 128/1000 | Loss: 0.00001393
Iteration 129/1000 | Loss: 0.00001393
Iteration 130/1000 | Loss: 0.00001393
Iteration 131/1000 | Loss: 0.00001393
Iteration 132/1000 | Loss: 0.00001393
Iteration 133/1000 | Loss: 0.00001393
Iteration 134/1000 | Loss: 0.00001393
Iteration 135/1000 | Loss: 0.00001393
Iteration 136/1000 | Loss: 0.00001393
Iteration 137/1000 | Loss: 0.00001393
Iteration 138/1000 | Loss: 0.00001393
Iteration 139/1000 | Loss: 0.00001393
Iteration 140/1000 | Loss: 0.00001393
Iteration 141/1000 | Loss: 0.00001393
Iteration 142/1000 | Loss: 0.00001393
Iteration 143/1000 | Loss: 0.00001393
Iteration 144/1000 | Loss: 0.00001393
Iteration 145/1000 | Loss: 0.00001393
Iteration 146/1000 | Loss: 0.00001393
Iteration 147/1000 | Loss: 0.00001393
Iteration 148/1000 | Loss: 0.00001393
Iteration 149/1000 | Loss: 0.00001393
Iteration 150/1000 | Loss: 0.00001393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.393073489452945e-05, 1.393073489452945e-05, 1.393073489452945e-05, 1.393073489452945e-05, 1.393073489452945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.393073489452945e-05

Optimization complete. Final v2v error: 3.2170066833496094 mm

Highest mean error: 3.8409342765808105 mm for frame 146

Lowest mean error: 2.7694602012634277 mm for frame 247

Saving results

Total time: 39.825844526290894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01120850
Iteration 2/25 | Loss: 0.01120850
Iteration 3/25 | Loss: 0.01120850
Iteration 4/25 | Loss: 0.01120850
Iteration 5/25 | Loss: 0.01120850
Iteration 6/25 | Loss: 0.01120850
Iteration 7/25 | Loss: 0.01120850
Iteration 8/25 | Loss: 0.01120850
Iteration 9/25 | Loss: 0.01120849
Iteration 10/25 | Loss: 0.01120849
Iteration 11/25 | Loss: 0.01120849
Iteration 12/25 | Loss: 0.01120849
Iteration 13/25 | Loss: 0.01120849
Iteration 14/25 | Loss: 0.01120849
Iteration 15/25 | Loss: 0.01120849
Iteration 16/25 | Loss: 0.01120849
Iteration 17/25 | Loss: 0.01120849
Iteration 18/25 | Loss: 0.01120849
Iteration 19/25 | Loss: 0.01120849
Iteration 20/25 | Loss: 0.01120848
Iteration 21/25 | Loss: 0.01120848
Iteration 22/25 | Loss: 0.01120848
Iteration 23/25 | Loss: 0.01120848
Iteration 24/25 | Loss: 0.01120848
Iteration 25/25 | Loss: 0.01120848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82177973
Iteration 2/25 | Loss: 0.06674604
Iteration 3/25 | Loss: 0.06669055
Iteration 4/25 | Loss: 0.06669055
Iteration 5/25 | Loss: 0.06669054
Iteration 6/25 | Loss: 0.06669053
Iteration 7/25 | Loss: 0.06669053
Iteration 8/25 | Loss: 0.06669053
Iteration 9/25 | Loss: 0.06669053
Iteration 10/25 | Loss: 0.06669053
Iteration 11/25 | Loss: 0.06669053
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.06669052690267563, 0.06669052690267563, 0.06669052690267563, 0.06669052690267563, 0.06669052690267563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06669052690267563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06669053
Iteration 2/1000 | Loss: 0.00625725
Iteration 3/1000 | Loss: 0.00205411
Iteration 4/1000 | Loss: 0.00116717
Iteration 5/1000 | Loss: 0.00953107
Iteration 6/1000 | Loss: 0.00986255
Iteration 7/1000 | Loss: 0.00691315
Iteration 8/1000 | Loss: 0.00025433
Iteration 9/1000 | Loss: 0.00096564
Iteration 10/1000 | Loss: 0.00040681
Iteration 11/1000 | Loss: 0.00044913
Iteration 12/1000 | Loss: 0.00035947
Iteration 13/1000 | Loss: 0.00011634
Iteration 14/1000 | Loss: 0.00017691
Iteration 15/1000 | Loss: 0.00006233
Iteration 16/1000 | Loss: 0.00019267
Iteration 17/1000 | Loss: 0.00029929
Iteration 18/1000 | Loss: 0.00010424
Iteration 19/1000 | Loss: 0.00011592
Iteration 20/1000 | Loss: 0.00004816
Iteration 21/1000 | Loss: 0.00004547
Iteration 22/1000 | Loss: 0.00029606
Iteration 23/1000 | Loss: 0.00221674
Iteration 24/1000 | Loss: 0.00006000
Iteration 25/1000 | Loss: 0.00009558
Iteration 26/1000 | Loss: 0.00004498
Iteration 27/1000 | Loss: 0.00005738
Iteration 28/1000 | Loss: 0.00003861
Iteration 29/1000 | Loss: 0.00003728
Iteration 30/1000 | Loss: 0.00035032
Iteration 31/1000 | Loss: 0.00017238
Iteration 32/1000 | Loss: 0.00028009
Iteration 33/1000 | Loss: 0.00008199
Iteration 34/1000 | Loss: 0.00034811
Iteration 35/1000 | Loss: 0.00004344
Iteration 36/1000 | Loss: 0.00012423
Iteration 37/1000 | Loss: 0.00025891
Iteration 38/1000 | Loss: 0.00030564
Iteration 39/1000 | Loss: 0.00013867
Iteration 40/1000 | Loss: 0.00034683
Iteration 41/1000 | Loss: 0.00006803
Iteration 42/1000 | Loss: 0.00029033
Iteration 43/1000 | Loss: 0.00003923
Iteration 44/1000 | Loss: 0.00003964
Iteration 45/1000 | Loss: 0.00004551
Iteration 46/1000 | Loss: 0.00028120
Iteration 47/1000 | Loss: 0.00003772
Iteration 48/1000 | Loss: 0.00005610
Iteration 49/1000 | Loss: 0.00006707
Iteration 50/1000 | Loss: 0.00003835
Iteration 51/1000 | Loss: 0.00003433
Iteration 52/1000 | Loss: 0.00003495
Iteration 53/1000 | Loss: 0.00003324
Iteration 54/1000 | Loss: 0.00003490
Iteration 55/1000 | Loss: 0.00009249
Iteration 56/1000 | Loss: 0.00063592
Iteration 57/1000 | Loss: 0.00048897
Iteration 58/1000 | Loss: 0.00007767
Iteration 59/1000 | Loss: 0.00004338
Iteration 60/1000 | Loss: 0.00003239
Iteration 61/1000 | Loss: 0.00020612
Iteration 62/1000 | Loss: 0.00015080
Iteration 63/1000 | Loss: 0.00031952
Iteration 64/1000 | Loss: 0.00004989
Iteration 65/1000 | Loss: 0.00003196
Iteration 66/1000 | Loss: 0.00004989
Iteration 67/1000 | Loss: 0.00022158
Iteration 68/1000 | Loss: 0.00042761
Iteration 69/1000 | Loss: 0.00006194
Iteration 70/1000 | Loss: 0.00016911
Iteration 71/1000 | Loss: 0.00040250
Iteration 72/1000 | Loss: 0.00027486
Iteration 73/1000 | Loss: 0.00018721
Iteration 74/1000 | Loss: 0.00004662
Iteration 75/1000 | Loss: 0.00003551
Iteration 76/1000 | Loss: 0.00025757
Iteration 77/1000 | Loss: 0.00004423
Iteration 78/1000 | Loss: 0.00006495
Iteration 79/1000 | Loss: 0.00003254
Iteration 80/1000 | Loss: 0.00004858
Iteration 81/1000 | Loss: 0.00020405
Iteration 82/1000 | Loss: 0.00029527
Iteration 83/1000 | Loss: 0.00021629
Iteration 84/1000 | Loss: 0.00005044
Iteration 85/1000 | Loss: 0.00004458
Iteration 86/1000 | Loss: 0.00004609
Iteration 87/1000 | Loss: 0.00004371
Iteration 88/1000 | Loss: 0.00003148
Iteration 89/1000 | Loss: 0.00004283
Iteration 90/1000 | Loss: 0.00003133
Iteration 91/1000 | Loss: 0.00003128
Iteration 92/1000 | Loss: 0.00003128
Iteration 93/1000 | Loss: 0.00003128
Iteration 94/1000 | Loss: 0.00003127
Iteration 95/1000 | Loss: 0.00003127
Iteration 96/1000 | Loss: 0.00003127
Iteration 97/1000 | Loss: 0.00003127
Iteration 98/1000 | Loss: 0.00003125
Iteration 99/1000 | Loss: 0.00003125
Iteration 100/1000 | Loss: 0.00003124
Iteration 101/1000 | Loss: 0.00005651
Iteration 102/1000 | Loss: 0.00003131
Iteration 103/1000 | Loss: 0.00003115
Iteration 104/1000 | Loss: 0.00003115
Iteration 105/1000 | Loss: 0.00004981
Iteration 106/1000 | Loss: 0.00005321
Iteration 107/1000 | Loss: 0.00003241
Iteration 108/1000 | Loss: 0.00003483
Iteration 109/1000 | Loss: 0.00003115
Iteration 110/1000 | Loss: 0.00003114
Iteration 111/1000 | Loss: 0.00003114
Iteration 112/1000 | Loss: 0.00003112
Iteration 113/1000 | Loss: 0.00003112
Iteration 114/1000 | Loss: 0.00003111
Iteration 115/1000 | Loss: 0.00003111
Iteration 116/1000 | Loss: 0.00003111
Iteration 117/1000 | Loss: 0.00003110
Iteration 118/1000 | Loss: 0.00003110
Iteration 119/1000 | Loss: 0.00003110
Iteration 120/1000 | Loss: 0.00003109
Iteration 121/1000 | Loss: 0.00003109
Iteration 122/1000 | Loss: 0.00003109
Iteration 123/1000 | Loss: 0.00003103
Iteration 124/1000 | Loss: 0.00003102
Iteration 125/1000 | Loss: 0.00003102
Iteration 126/1000 | Loss: 0.00006834
Iteration 127/1000 | Loss: 0.00003247
Iteration 128/1000 | Loss: 0.00006003
Iteration 129/1000 | Loss: 0.00003107
Iteration 130/1000 | Loss: 0.00010361
Iteration 131/1000 | Loss: 0.00005515
Iteration 132/1000 | Loss: 0.00004140
Iteration 133/1000 | Loss: 0.00011323
Iteration 134/1000 | Loss: 0.00003885
Iteration 135/1000 | Loss: 0.00003092
Iteration 136/1000 | Loss: 0.00003085
Iteration 137/1000 | Loss: 0.00003085
Iteration 138/1000 | Loss: 0.00003085
Iteration 139/1000 | Loss: 0.00003085
Iteration 140/1000 | Loss: 0.00003085
Iteration 141/1000 | Loss: 0.00003085
Iteration 142/1000 | Loss: 0.00003085
Iteration 143/1000 | Loss: 0.00003085
Iteration 144/1000 | Loss: 0.00003085
Iteration 145/1000 | Loss: 0.00003085
Iteration 146/1000 | Loss: 0.00003085
Iteration 147/1000 | Loss: 0.00003085
Iteration 148/1000 | Loss: 0.00003085
Iteration 149/1000 | Loss: 0.00004733
Iteration 150/1000 | Loss: 0.00005576
Iteration 151/1000 | Loss: 0.00003325
Iteration 152/1000 | Loss: 0.00003408
Iteration 153/1000 | Loss: 0.00003084
Iteration 154/1000 | Loss: 0.00003084
Iteration 155/1000 | Loss: 0.00003084
Iteration 156/1000 | Loss: 0.00003084
Iteration 157/1000 | Loss: 0.00003084
Iteration 158/1000 | Loss: 0.00003084
Iteration 159/1000 | Loss: 0.00003084
Iteration 160/1000 | Loss: 0.00003084
Iteration 161/1000 | Loss: 0.00003084
Iteration 162/1000 | Loss: 0.00003083
Iteration 163/1000 | Loss: 0.00003083
Iteration 164/1000 | Loss: 0.00003082
Iteration 165/1000 | Loss: 0.00003082
Iteration 166/1000 | Loss: 0.00003082
Iteration 167/1000 | Loss: 0.00003082
Iteration 168/1000 | Loss: 0.00003082
Iteration 169/1000 | Loss: 0.00003082
Iteration 170/1000 | Loss: 0.00003082
Iteration 171/1000 | Loss: 0.00003081
Iteration 172/1000 | Loss: 0.00003081
Iteration 173/1000 | Loss: 0.00003081
Iteration 174/1000 | Loss: 0.00003081
Iteration 175/1000 | Loss: 0.00003081
Iteration 176/1000 | Loss: 0.00003081
Iteration 177/1000 | Loss: 0.00003081
Iteration 178/1000 | Loss: 0.00003081
Iteration 179/1000 | Loss: 0.00003080
Iteration 180/1000 | Loss: 0.00003080
Iteration 181/1000 | Loss: 0.00003080
Iteration 182/1000 | Loss: 0.00003080
Iteration 183/1000 | Loss: 0.00003080
Iteration 184/1000 | Loss: 0.00003080
Iteration 185/1000 | Loss: 0.00003080
Iteration 186/1000 | Loss: 0.00003080
Iteration 187/1000 | Loss: 0.00003080
Iteration 188/1000 | Loss: 0.00003080
Iteration 189/1000 | Loss: 0.00003079
Iteration 190/1000 | Loss: 0.00003079
Iteration 191/1000 | Loss: 0.00003079
Iteration 192/1000 | Loss: 0.00003079
Iteration 193/1000 | Loss: 0.00003079
Iteration 194/1000 | Loss: 0.00003079
Iteration 195/1000 | Loss: 0.00003079
Iteration 196/1000 | Loss: 0.00003078
Iteration 197/1000 | Loss: 0.00003078
Iteration 198/1000 | Loss: 0.00003078
Iteration 199/1000 | Loss: 0.00003078
Iteration 200/1000 | Loss: 0.00003077
Iteration 201/1000 | Loss: 0.00003077
Iteration 202/1000 | Loss: 0.00003077
Iteration 203/1000 | Loss: 0.00003077
Iteration 204/1000 | Loss: 0.00003077
Iteration 205/1000 | Loss: 0.00003077
Iteration 206/1000 | Loss: 0.00003076
Iteration 207/1000 | Loss: 0.00003076
Iteration 208/1000 | Loss: 0.00003076
Iteration 209/1000 | Loss: 0.00003076
Iteration 210/1000 | Loss: 0.00003076
Iteration 211/1000 | Loss: 0.00003076
Iteration 212/1000 | Loss: 0.00003076
Iteration 213/1000 | Loss: 0.00003076
Iteration 214/1000 | Loss: 0.00003076
Iteration 215/1000 | Loss: 0.00003076
Iteration 216/1000 | Loss: 0.00003076
Iteration 217/1000 | Loss: 0.00003076
Iteration 218/1000 | Loss: 0.00003076
Iteration 219/1000 | Loss: 0.00003076
Iteration 220/1000 | Loss: 0.00003076
Iteration 221/1000 | Loss: 0.00003076
Iteration 222/1000 | Loss: 0.00003076
Iteration 223/1000 | Loss: 0.00003076
Iteration 224/1000 | Loss: 0.00003076
Iteration 225/1000 | Loss: 0.00003076
Iteration 226/1000 | Loss: 0.00003070
Iteration 227/1000 | Loss: 0.00003070
Iteration 228/1000 | Loss: 0.00003070
Iteration 229/1000 | Loss: 0.00003070
Iteration 230/1000 | Loss: 0.00003070
Iteration 231/1000 | Loss: 0.00003070
Iteration 232/1000 | Loss: 0.00003070
Iteration 233/1000 | Loss: 0.00003070
Iteration 234/1000 | Loss: 0.00003070
Iteration 235/1000 | Loss: 0.00003069
Iteration 236/1000 | Loss: 0.00003069
Iteration 237/1000 | Loss: 0.00003069
Iteration 238/1000 | Loss: 0.00003069
Iteration 239/1000 | Loss: 0.00003069
Iteration 240/1000 | Loss: 0.00003069
Iteration 241/1000 | Loss: 0.00003069
Iteration 242/1000 | Loss: 0.00003069
Iteration 243/1000 | Loss: 0.00003069
Iteration 244/1000 | Loss: 0.00003069
Iteration 245/1000 | Loss: 0.00003069
Iteration 246/1000 | Loss: 0.00003069
Iteration 247/1000 | Loss: 0.00003068
Iteration 248/1000 | Loss: 0.00003068
Iteration 249/1000 | Loss: 0.00003068
Iteration 250/1000 | Loss: 0.00003067
Iteration 251/1000 | Loss: 0.00003067
Iteration 252/1000 | Loss: 0.00003067
Iteration 253/1000 | Loss: 0.00003066
Iteration 254/1000 | Loss: 0.00003066
Iteration 255/1000 | Loss: 0.00003065
Iteration 256/1000 | Loss: 0.00006447
Iteration 257/1000 | Loss: 0.00003067
Iteration 258/1000 | Loss: 0.00003060
Iteration 259/1000 | Loss: 0.00003060
Iteration 260/1000 | Loss: 0.00003060
Iteration 261/1000 | Loss: 0.00003060
Iteration 262/1000 | Loss: 0.00003060
Iteration 263/1000 | Loss: 0.00003060
Iteration 264/1000 | Loss: 0.00003060
Iteration 265/1000 | Loss: 0.00003060
Iteration 266/1000 | Loss: 0.00003060
Iteration 267/1000 | Loss: 0.00003060
Iteration 268/1000 | Loss: 0.00003059
Iteration 269/1000 | Loss: 0.00003059
Iteration 270/1000 | Loss: 0.00003059
Iteration 271/1000 | Loss: 0.00003059
Iteration 272/1000 | Loss: 0.00003059
Iteration 273/1000 | Loss: 0.00003059
Iteration 274/1000 | Loss: 0.00003059
Iteration 275/1000 | Loss: 0.00003058
Iteration 276/1000 | Loss: 0.00003058
Iteration 277/1000 | Loss: 0.00003058
Iteration 278/1000 | Loss: 0.00020960
Iteration 279/1000 | Loss: 0.00023736
Iteration 280/1000 | Loss: 0.00004351
Iteration 281/1000 | Loss: 0.00013442
Iteration 282/1000 | Loss: 0.00003890
Iteration 283/1000 | Loss: 0.00016902
Iteration 284/1000 | Loss: 0.00003283
Iteration 285/1000 | Loss: 0.00024254
Iteration 286/1000 | Loss: 0.00010777
Iteration 287/1000 | Loss: 0.00016948
Iteration 288/1000 | Loss: 0.00004563
Iteration 289/1000 | Loss: 0.00011632
Iteration 290/1000 | Loss: 0.00044235
Iteration 291/1000 | Loss: 0.00053267
Iteration 292/1000 | Loss: 0.00074860
Iteration 293/1000 | Loss: 0.00003535
Iteration 294/1000 | Loss: 0.00005704
Iteration 295/1000 | Loss: 0.00004500
Iteration 296/1000 | Loss: 0.00016157
Iteration 297/1000 | Loss: 0.00003149
Iteration 298/1000 | Loss: 0.00003119
Iteration 299/1000 | Loss: 0.00003091
Iteration 300/1000 | Loss: 0.00005069
Iteration 301/1000 | Loss: 0.00009391
Iteration 302/1000 | Loss: 0.00027429
Iteration 303/1000 | Loss: 0.00003136
Iteration 304/1000 | Loss: 0.00004945
Iteration 305/1000 | Loss: 0.00003059
Iteration 306/1000 | Loss: 0.00004033
Iteration 307/1000 | Loss: 0.00003047
Iteration 308/1000 | Loss: 0.00003046
Iteration 309/1000 | Loss: 0.00003046
Iteration 310/1000 | Loss: 0.00003046
Iteration 311/1000 | Loss: 0.00003046
Iteration 312/1000 | Loss: 0.00003045
Iteration 313/1000 | Loss: 0.00003045
Iteration 314/1000 | Loss: 0.00003044
Iteration 315/1000 | Loss: 0.00003043
Iteration 316/1000 | Loss: 0.00003043
Iteration 317/1000 | Loss: 0.00003043
Iteration 318/1000 | Loss: 0.00003043
Iteration 319/1000 | Loss: 0.00003042
Iteration 320/1000 | Loss: 0.00003042
Iteration 321/1000 | Loss: 0.00003042
Iteration 322/1000 | Loss: 0.00003042
Iteration 323/1000 | Loss: 0.00003042
Iteration 324/1000 | Loss: 0.00003042
Iteration 325/1000 | Loss: 0.00003042
Iteration 326/1000 | Loss: 0.00003042
Iteration 327/1000 | Loss: 0.00003042
Iteration 328/1000 | Loss: 0.00003041
Iteration 329/1000 | Loss: 0.00003041
Iteration 330/1000 | Loss: 0.00003041
Iteration 331/1000 | Loss: 0.00003041
Iteration 332/1000 | Loss: 0.00003041
Iteration 333/1000 | Loss: 0.00003040
Iteration 334/1000 | Loss: 0.00003040
Iteration 335/1000 | Loss: 0.00003040
Iteration 336/1000 | Loss: 0.00003040
Iteration 337/1000 | Loss: 0.00003040
Iteration 338/1000 | Loss: 0.00003040
Iteration 339/1000 | Loss: 0.00003040
Iteration 340/1000 | Loss: 0.00003040
Iteration 341/1000 | Loss: 0.00003040
Iteration 342/1000 | Loss: 0.00003039
Iteration 343/1000 | Loss: 0.00003039
Iteration 344/1000 | Loss: 0.00003039
Iteration 345/1000 | Loss: 0.00003039
Iteration 346/1000 | Loss: 0.00003039
Iteration 347/1000 | Loss: 0.00003039
Iteration 348/1000 | Loss: 0.00003039
Iteration 349/1000 | Loss: 0.00003039
Iteration 350/1000 | Loss: 0.00003039
Iteration 351/1000 | Loss: 0.00003039
Iteration 352/1000 | Loss: 0.00003039
Iteration 353/1000 | Loss: 0.00003039
Iteration 354/1000 | Loss: 0.00003038
Iteration 355/1000 | Loss: 0.00003038
Iteration 356/1000 | Loss: 0.00003038
Iteration 357/1000 | Loss: 0.00003038
Iteration 358/1000 | Loss: 0.00003038
Iteration 359/1000 | Loss: 0.00003037
Iteration 360/1000 | Loss: 0.00005416
Iteration 361/1000 | Loss: 0.00003045
Iteration 362/1000 | Loss: 0.00003036
Iteration 363/1000 | Loss: 0.00003035
Iteration 364/1000 | Loss: 0.00003035
Iteration 365/1000 | Loss: 0.00003035
Iteration 366/1000 | Loss: 0.00003035
Iteration 367/1000 | Loss: 0.00003035
Iteration 368/1000 | Loss: 0.00003035
Iteration 369/1000 | Loss: 0.00005319
Iteration 370/1000 | Loss: 0.00005319
Iteration 371/1000 | Loss: 0.00005319
Iteration 372/1000 | Loss: 0.00009431
Iteration 373/1000 | Loss: 0.00003285
Iteration 374/1000 | Loss: 0.00008381
Iteration 375/1000 | Loss: 0.00007481
Iteration 376/1000 | Loss: 0.00003052
Iteration 377/1000 | Loss: 0.00005087
Iteration 378/1000 | Loss: 0.00003209
Iteration 379/1000 | Loss: 0.00003356
Iteration 380/1000 | Loss: 0.00003035
Iteration 381/1000 | Loss: 0.00003035
Iteration 382/1000 | Loss: 0.00003035
Iteration 383/1000 | Loss: 0.00003035
Iteration 384/1000 | Loss: 0.00003035
Iteration 385/1000 | Loss: 0.00003035
Iteration 386/1000 | Loss: 0.00003035
Iteration 387/1000 | Loss: 0.00003035
Iteration 388/1000 | Loss: 0.00003035
Iteration 389/1000 | Loss: 0.00003034
Iteration 390/1000 | Loss: 0.00003034
Iteration 391/1000 | Loss: 0.00003034
Iteration 392/1000 | Loss: 0.00003034
Iteration 393/1000 | Loss: 0.00003034
Iteration 394/1000 | Loss: 0.00003034
Iteration 395/1000 | Loss: 0.00003034
Iteration 396/1000 | Loss: 0.00003033
Iteration 397/1000 | Loss: 0.00003316
Iteration 398/1000 | Loss: 0.00008420
Iteration 399/1000 | Loss: 0.00003082
Iteration 400/1000 | Loss: 0.00004233
Iteration 401/1000 | Loss: 0.00003050
Iteration 402/1000 | Loss: 0.00004106
Iteration 403/1000 | Loss: 0.00003040
Iteration 404/1000 | Loss: 0.00003038
Iteration 405/1000 | Loss: 0.00004018
Iteration 406/1000 | Loss: 0.00004499
Iteration 407/1000 | Loss: 0.00003314
Iteration 408/1000 | Loss: 0.00003378
Iteration 409/1000 | Loss: 0.00003986
Iteration 410/1000 | Loss: 0.00003271
Iteration 411/1000 | Loss: 0.00003036
Iteration 412/1000 | Loss: 0.00003036
Iteration 413/1000 | Loss: 0.00003036
Iteration 414/1000 | Loss: 0.00003030
Iteration 415/1000 | Loss: 0.00003030
Iteration 416/1000 | Loss: 0.00003030
Iteration 417/1000 | Loss: 0.00003030
Iteration 418/1000 | Loss: 0.00003030
Iteration 419/1000 | Loss: 0.00003029
Iteration 420/1000 | Loss: 0.00003029
Iteration 421/1000 | Loss: 0.00003029
Iteration 422/1000 | Loss: 0.00003029
Iteration 423/1000 | Loss: 0.00003029
Iteration 424/1000 | Loss: 0.00003029
Iteration 425/1000 | Loss: 0.00003029
Iteration 426/1000 | Loss: 0.00003029
Iteration 427/1000 | Loss: 0.00003029
Iteration 428/1000 | Loss: 0.00003028
Iteration 429/1000 | Loss: 0.00003028
Iteration 430/1000 | Loss: 0.00003028
Iteration 431/1000 | Loss: 0.00003028
Iteration 432/1000 | Loss: 0.00003028
Iteration 433/1000 | Loss: 0.00003028
Iteration 434/1000 | Loss: 0.00003028
Iteration 435/1000 | Loss: 0.00003027
Iteration 436/1000 | Loss: 0.00003027
Iteration 437/1000 | Loss: 0.00003027
Iteration 438/1000 | Loss: 0.00003027
Iteration 439/1000 | Loss: 0.00003027
Iteration 440/1000 | Loss: 0.00003197
Iteration 441/1000 | Loss: 0.00003027
Iteration 442/1000 | Loss: 0.00003025
Iteration 443/1000 | Loss: 0.00003025
Iteration 444/1000 | Loss: 0.00003025
Iteration 445/1000 | Loss: 0.00003025
Iteration 446/1000 | Loss: 0.00003025
Iteration 447/1000 | Loss: 0.00003025
Iteration 448/1000 | Loss: 0.00003025
Iteration 449/1000 | Loss: 0.00003025
Iteration 450/1000 | Loss: 0.00003025
Iteration 451/1000 | Loss: 0.00003025
Iteration 452/1000 | Loss: 0.00003025
Iteration 453/1000 | Loss: 0.00003025
Iteration 454/1000 | Loss: 0.00003025
Iteration 455/1000 | Loss: 0.00003025
Iteration 456/1000 | Loss: 0.00003025
Iteration 457/1000 | Loss: 0.00003025
Iteration 458/1000 | Loss: 0.00003025
Iteration 459/1000 | Loss: 0.00003025
Iteration 460/1000 | Loss: 0.00003025
Iteration 461/1000 | Loss: 0.00003025
Iteration 462/1000 | Loss: 0.00003025
Iteration 463/1000 | Loss: 0.00003025
Iteration 464/1000 | Loss: 0.00003025
Iteration 465/1000 | Loss: 0.00003025
Iteration 466/1000 | Loss: 0.00003025
Iteration 467/1000 | Loss: 0.00003025
Iteration 468/1000 | Loss: 0.00003025
Iteration 469/1000 | Loss: 0.00003025
Iteration 470/1000 | Loss: 0.00003025
Iteration 471/1000 | Loss: 0.00003025
Iteration 472/1000 | Loss: 0.00003025
Iteration 473/1000 | Loss: 0.00003025
Iteration 474/1000 | Loss: 0.00003025
Iteration 475/1000 | Loss: 0.00003025
Iteration 476/1000 | Loss: 0.00003025
Iteration 477/1000 | Loss: 0.00003025
Iteration 478/1000 | Loss: 0.00003025
Iteration 479/1000 | Loss: 0.00003025
Iteration 480/1000 | Loss: 0.00003025
Iteration 481/1000 | Loss: 0.00003025
Iteration 482/1000 | Loss: 0.00003025
Iteration 483/1000 | Loss: 0.00003025
Iteration 484/1000 | Loss: 0.00003025
Iteration 485/1000 | Loss: 0.00003025
Iteration 486/1000 | Loss: 0.00003025
Iteration 487/1000 | Loss: 0.00003025
Iteration 488/1000 | Loss: 0.00003025
Iteration 489/1000 | Loss: 0.00003025
Iteration 490/1000 | Loss: 0.00003025
Iteration 491/1000 | Loss: 0.00003025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 491. Stopping optimization.
Last 5 losses: [3.0249153496697545e-05, 3.0249153496697545e-05, 3.0249153496697545e-05, 3.0249153496697545e-05, 3.0249153496697545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0249153496697545e-05

Optimization complete. Final v2v error: 4.179149150848389 mm

Highest mean error: 19.157209396362305 mm for frame 9

Lowest mean error: 3.1241703033447266 mm for frame 112

Saving results

Total time: 295.5286076068878
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844440
Iteration 2/25 | Loss: 0.00138085
Iteration 3/25 | Loss: 0.00117913
Iteration 4/25 | Loss: 0.00115384
Iteration 5/25 | Loss: 0.00115002
Iteration 6/25 | Loss: 0.00114929
Iteration 7/25 | Loss: 0.00114929
Iteration 8/25 | Loss: 0.00114929
Iteration 9/25 | Loss: 0.00114929
Iteration 10/25 | Loss: 0.00114929
Iteration 11/25 | Loss: 0.00114929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011492909397929907, 0.0011492909397929907, 0.0011492909397929907, 0.0011492909397929907, 0.0011492909397929907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011492909397929907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.39105535
Iteration 2/25 | Loss: 0.00083560
Iteration 3/25 | Loss: 0.00083560
Iteration 4/25 | Loss: 0.00083560
Iteration 5/25 | Loss: 0.00083560
Iteration 6/25 | Loss: 0.00083560
Iteration 7/25 | Loss: 0.00083560
Iteration 8/25 | Loss: 0.00083560
Iteration 9/25 | Loss: 0.00083560
Iteration 10/25 | Loss: 0.00083560
Iteration 11/25 | Loss: 0.00083560
Iteration 12/25 | Loss: 0.00083560
Iteration 13/25 | Loss: 0.00083560
Iteration 14/25 | Loss: 0.00083560
Iteration 15/25 | Loss: 0.00083560
Iteration 16/25 | Loss: 0.00083560
Iteration 17/25 | Loss: 0.00083560
Iteration 18/25 | Loss: 0.00083560
Iteration 19/25 | Loss: 0.00083560
Iteration 20/25 | Loss: 0.00083560
Iteration 21/25 | Loss: 0.00083560
Iteration 22/25 | Loss: 0.00083560
Iteration 23/25 | Loss: 0.00083560
Iteration 24/25 | Loss: 0.00083560
Iteration 25/25 | Loss: 0.00083560

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083560
Iteration 2/1000 | Loss: 0.00004744
Iteration 3/1000 | Loss: 0.00002564
Iteration 4/1000 | Loss: 0.00002071
Iteration 5/1000 | Loss: 0.00001853
Iteration 6/1000 | Loss: 0.00001761
Iteration 7/1000 | Loss: 0.00001663
Iteration 8/1000 | Loss: 0.00001608
Iteration 9/1000 | Loss: 0.00001561
Iteration 10/1000 | Loss: 0.00001526
Iteration 11/1000 | Loss: 0.00001506
Iteration 12/1000 | Loss: 0.00001506
Iteration 13/1000 | Loss: 0.00001505
Iteration 14/1000 | Loss: 0.00001501
Iteration 15/1000 | Loss: 0.00001500
Iteration 16/1000 | Loss: 0.00001496
Iteration 17/1000 | Loss: 0.00001492
Iteration 18/1000 | Loss: 0.00001492
Iteration 19/1000 | Loss: 0.00001492
Iteration 20/1000 | Loss: 0.00001492
Iteration 21/1000 | Loss: 0.00001492
Iteration 22/1000 | Loss: 0.00001492
Iteration 23/1000 | Loss: 0.00001492
Iteration 24/1000 | Loss: 0.00001492
Iteration 25/1000 | Loss: 0.00001491
Iteration 26/1000 | Loss: 0.00001491
Iteration 27/1000 | Loss: 0.00001490
Iteration 28/1000 | Loss: 0.00001490
Iteration 29/1000 | Loss: 0.00001486
Iteration 30/1000 | Loss: 0.00001486
Iteration 31/1000 | Loss: 0.00001486
Iteration 32/1000 | Loss: 0.00001486
Iteration 33/1000 | Loss: 0.00001486
Iteration 34/1000 | Loss: 0.00001486
Iteration 35/1000 | Loss: 0.00001486
Iteration 36/1000 | Loss: 0.00001486
Iteration 37/1000 | Loss: 0.00001486
Iteration 38/1000 | Loss: 0.00001486
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001485
Iteration 41/1000 | Loss: 0.00001485
Iteration 42/1000 | Loss: 0.00001485
Iteration 43/1000 | Loss: 0.00001483
Iteration 44/1000 | Loss: 0.00001482
Iteration 45/1000 | Loss: 0.00001482
Iteration 46/1000 | Loss: 0.00001482
Iteration 47/1000 | Loss: 0.00001482
Iteration 48/1000 | Loss: 0.00001482
Iteration 49/1000 | Loss: 0.00001481
Iteration 50/1000 | Loss: 0.00001481
Iteration 51/1000 | Loss: 0.00001481
Iteration 52/1000 | Loss: 0.00001481
Iteration 53/1000 | Loss: 0.00001480
Iteration 54/1000 | Loss: 0.00001480
Iteration 55/1000 | Loss: 0.00001480
Iteration 56/1000 | Loss: 0.00001479
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001478
Iteration 61/1000 | Loss: 0.00001478
Iteration 62/1000 | Loss: 0.00001478
Iteration 63/1000 | Loss: 0.00001477
Iteration 64/1000 | Loss: 0.00001477
Iteration 65/1000 | Loss: 0.00001477
Iteration 66/1000 | Loss: 0.00001476
Iteration 67/1000 | Loss: 0.00001476
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001475
Iteration 74/1000 | Loss: 0.00001475
Iteration 75/1000 | Loss: 0.00001475
Iteration 76/1000 | Loss: 0.00001475
Iteration 77/1000 | Loss: 0.00001475
Iteration 78/1000 | Loss: 0.00001474
Iteration 79/1000 | Loss: 0.00001474
Iteration 80/1000 | Loss: 0.00001474
Iteration 81/1000 | Loss: 0.00001474
Iteration 82/1000 | Loss: 0.00001474
Iteration 83/1000 | Loss: 0.00001474
Iteration 84/1000 | Loss: 0.00001474
Iteration 85/1000 | Loss: 0.00001473
Iteration 86/1000 | Loss: 0.00001473
Iteration 87/1000 | Loss: 0.00001473
Iteration 88/1000 | Loss: 0.00001472
Iteration 89/1000 | Loss: 0.00001472
Iteration 90/1000 | Loss: 0.00001472
Iteration 91/1000 | Loss: 0.00001472
Iteration 92/1000 | Loss: 0.00001471
Iteration 93/1000 | Loss: 0.00001471
Iteration 94/1000 | Loss: 0.00001471
Iteration 95/1000 | Loss: 0.00001470
Iteration 96/1000 | Loss: 0.00001470
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001470
Iteration 99/1000 | Loss: 0.00001470
Iteration 100/1000 | Loss: 0.00001470
Iteration 101/1000 | Loss: 0.00001470
Iteration 102/1000 | Loss: 0.00001469
Iteration 103/1000 | Loss: 0.00001469
Iteration 104/1000 | Loss: 0.00001469
Iteration 105/1000 | Loss: 0.00001469
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001469
Iteration 108/1000 | Loss: 0.00001469
Iteration 109/1000 | Loss: 0.00001469
Iteration 110/1000 | Loss: 0.00001469
Iteration 111/1000 | Loss: 0.00001468
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001468
Iteration 117/1000 | Loss: 0.00001468
Iteration 118/1000 | Loss: 0.00001467
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001467
Iteration 121/1000 | Loss: 0.00001467
Iteration 122/1000 | Loss: 0.00001467
Iteration 123/1000 | Loss: 0.00001467
Iteration 124/1000 | Loss: 0.00001467
Iteration 125/1000 | Loss: 0.00001467
Iteration 126/1000 | Loss: 0.00001467
Iteration 127/1000 | Loss: 0.00001467
Iteration 128/1000 | Loss: 0.00001467
Iteration 129/1000 | Loss: 0.00001466
Iteration 130/1000 | Loss: 0.00001466
Iteration 131/1000 | Loss: 0.00001466
Iteration 132/1000 | Loss: 0.00001466
Iteration 133/1000 | Loss: 0.00001466
Iteration 134/1000 | Loss: 0.00001465
Iteration 135/1000 | Loss: 0.00001465
Iteration 136/1000 | Loss: 0.00001465
Iteration 137/1000 | Loss: 0.00001464
Iteration 138/1000 | Loss: 0.00001464
Iteration 139/1000 | Loss: 0.00001464
Iteration 140/1000 | Loss: 0.00001464
Iteration 141/1000 | Loss: 0.00001464
Iteration 142/1000 | Loss: 0.00001464
Iteration 143/1000 | Loss: 0.00001463
Iteration 144/1000 | Loss: 0.00001463
Iteration 145/1000 | Loss: 0.00001463
Iteration 146/1000 | Loss: 0.00001463
Iteration 147/1000 | Loss: 0.00001463
Iteration 148/1000 | Loss: 0.00001463
Iteration 149/1000 | Loss: 0.00001463
Iteration 150/1000 | Loss: 0.00001463
Iteration 151/1000 | Loss: 0.00001463
Iteration 152/1000 | Loss: 0.00001462
Iteration 153/1000 | Loss: 0.00001462
Iteration 154/1000 | Loss: 0.00001462
Iteration 155/1000 | Loss: 0.00001462
Iteration 156/1000 | Loss: 0.00001462
Iteration 157/1000 | Loss: 0.00001462
Iteration 158/1000 | Loss: 0.00001462
Iteration 159/1000 | Loss: 0.00001462
Iteration 160/1000 | Loss: 0.00001461
Iteration 161/1000 | Loss: 0.00001461
Iteration 162/1000 | Loss: 0.00001461
Iteration 163/1000 | Loss: 0.00001461
Iteration 164/1000 | Loss: 0.00001461
Iteration 165/1000 | Loss: 0.00001461
Iteration 166/1000 | Loss: 0.00001461
Iteration 167/1000 | Loss: 0.00001461
Iteration 168/1000 | Loss: 0.00001461
Iteration 169/1000 | Loss: 0.00001461
Iteration 170/1000 | Loss: 0.00001461
Iteration 171/1000 | Loss: 0.00001461
Iteration 172/1000 | Loss: 0.00001461
Iteration 173/1000 | Loss: 0.00001461
Iteration 174/1000 | Loss: 0.00001461
Iteration 175/1000 | Loss: 0.00001461
Iteration 176/1000 | Loss: 0.00001461
Iteration 177/1000 | Loss: 0.00001461
Iteration 178/1000 | Loss: 0.00001461
Iteration 179/1000 | Loss: 0.00001461
Iteration 180/1000 | Loss: 0.00001461
Iteration 181/1000 | Loss: 0.00001461
Iteration 182/1000 | Loss: 0.00001461
Iteration 183/1000 | Loss: 0.00001461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.4605247088184115e-05, 1.4605247088184115e-05, 1.4605247088184115e-05, 1.4605247088184115e-05, 1.4605247088184115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4605247088184115e-05

Optimization complete. Final v2v error: 3.2396931648254395 mm

Highest mean error: 4.153592586517334 mm for frame 85

Lowest mean error: 2.6876213550567627 mm for frame 131

Saving results

Total time: 41.22791576385498
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854996
Iteration 2/25 | Loss: 0.00123842
Iteration 3/25 | Loss: 0.00114820
Iteration 4/25 | Loss: 0.00114055
Iteration 5/25 | Loss: 0.00113853
Iteration 6/25 | Loss: 0.00113804
Iteration 7/25 | Loss: 0.00113804
Iteration 8/25 | Loss: 0.00113804
Iteration 9/25 | Loss: 0.00113804
Iteration 10/25 | Loss: 0.00113804
Iteration 11/25 | Loss: 0.00113804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011380359064787626, 0.0011380359064787626, 0.0011380359064787626, 0.0011380359064787626, 0.0011380359064787626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011380359064787626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33426225
Iteration 2/25 | Loss: 0.00092393
Iteration 3/25 | Loss: 0.00092392
Iteration 4/25 | Loss: 0.00092392
Iteration 5/25 | Loss: 0.00092392
Iteration 6/25 | Loss: 0.00092392
Iteration 7/25 | Loss: 0.00092392
Iteration 8/25 | Loss: 0.00092392
Iteration 9/25 | Loss: 0.00092392
Iteration 10/25 | Loss: 0.00092392
Iteration 11/25 | Loss: 0.00092392
Iteration 12/25 | Loss: 0.00092392
Iteration 13/25 | Loss: 0.00092392
Iteration 14/25 | Loss: 0.00092392
Iteration 15/25 | Loss: 0.00092392
Iteration 16/25 | Loss: 0.00092392
Iteration 17/25 | Loss: 0.00092392
Iteration 18/25 | Loss: 0.00092392
Iteration 19/25 | Loss: 0.00092392
Iteration 20/25 | Loss: 0.00092392
Iteration 21/25 | Loss: 0.00092392
Iteration 22/25 | Loss: 0.00092392
Iteration 23/25 | Loss: 0.00092392
Iteration 24/25 | Loss: 0.00092392
Iteration 25/25 | Loss: 0.00092392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092392
Iteration 2/1000 | Loss: 0.00005188
Iteration 3/1000 | Loss: 0.00002352
Iteration 4/1000 | Loss: 0.00001887
Iteration 5/1000 | Loss: 0.00001592
Iteration 6/1000 | Loss: 0.00001470
Iteration 7/1000 | Loss: 0.00001399
Iteration 8/1000 | Loss: 0.00001341
Iteration 9/1000 | Loss: 0.00001298
Iteration 10/1000 | Loss: 0.00001267
Iteration 11/1000 | Loss: 0.00001243
Iteration 12/1000 | Loss: 0.00001221
Iteration 13/1000 | Loss: 0.00001214
Iteration 14/1000 | Loss: 0.00001212
Iteration 15/1000 | Loss: 0.00001206
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001205
Iteration 18/1000 | Loss: 0.00001204
Iteration 19/1000 | Loss: 0.00001204
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001197
Iteration 22/1000 | Loss: 0.00001194
Iteration 23/1000 | Loss: 0.00001194
Iteration 24/1000 | Loss: 0.00001193
Iteration 25/1000 | Loss: 0.00001192
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001192
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001191
Iteration 30/1000 | Loss: 0.00001191
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001190
Iteration 33/1000 | Loss: 0.00001189
Iteration 34/1000 | Loss: 0.00001188
Iteration 35/1000 | Loss: 0.00001188
Iteration 36/1000 | Loss: 0.00001188
Iteration 37/1000 | Loss: 0.00001188
Iteration 38/1000 | Loss: 0.00001188
Iteration 39/1000 | Loss: 0.00001188
Iteration 40/1000 | Loss: 0.00001187
Iteration 41/1000 | Loss: 0.00001187
Iteration 42/1000 | Loss: 0.00001187
Iteration 43/1000 | Loss: 0.00001187
Iteration 44/1000 | Loss: 0.00001187
Iteration 45/1000 | Loss: 0.00001186
Iteration 46/1000 | Loss: 0.00001186
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001185
Iteration 49/1000 | Loss: 0.00001185
Iteration 50/1000 | Loss: 0.00001184
Iteration 51/1000 | Loss: 0.00001183
Iteration 52/1000 | Loss: 0.00001183
Iteration 53/1000 | Loss: 0.00001183
Iteration 54/1000 | Loss: 0.00001183
Iteration 55/1000 | Loss: 0.00001183
Iteration 56/1000 | Loss: 0.00001183
Iteration 57/1000 | Loss: 0.00001183
Iteration 58/1000 | Loss: 0.00001183
Iteration 59/1000 | Loss: 0.00001183
Iteration 60/1000 | Loss: 0.00001182
Iteration 61/1000 | Loss: 0.00001181
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001179
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001177
Iteration 73/1000 | Loss: 0.00001177
Iteration 74/1000 | Loss: 0.00001177
Iteration 75/1000 | Loss: 0.00001177
Iteration 76/1000 | Loss: 0.00001177
Iteration 77/1000 | Loss: 0.00001176
Iteration 78/1000 | Loss: 0.00001176
Iteration 79/1000 | Loss: 0.00001176
Iteration 80/1000 | Loss: 0.00001175
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001174
Iteration 83/1000 | Loss: 0.00001174
Iteration 84/1000 | Loss: 0.00001174
Iteration 85/1000 | Loss: 0.00001173
Iteration 86/1000 | Loss: 0.00001173
Iteration 87/1000 | Loss: 0.00001173
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001173
Iteration 90/1000 | Loss: 0.00001173
Iteration 91/1000 | Loss: 0.00001173
Iteration 92/1000 | Loss: 0.00001173
Iteration 93/1000 | Loss: 0.00001173
Iteration 94/1000 | Loss: 0.00001173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.1727701348718256e-05, 1.1727701348718256e-05, 1.1727701348718256e-05, 1.1727701348718256e-05, 1.1727701348718256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1727701348718256e-05

Optimization complete. Final v2v error: 3.007375955581665 mm

Highest mean error: 3.203284978866577 mm for frame 37

Lowest mean error: 2.8052268028259277 mm for frame 99

Saving results

Total time: 32.979161977767944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801028
Iteration 2/25 | Loss: 0.00126571
Iteration 3/25 | Loss: 0.00117597
Iteration 4/25 | Loss: 0.00115629
Iteration 5/25 | Loss: 0.00114978
Iteration 6/25 | Loss: 0.00114872
Iteration 7/25 | Loss: 0.00114872
Iteration 8/25 | Loss: 0.00114872
Iteration 9/25 | Loss: 0.00114872
Iteration 10/25 | Loss: 0.00114872
Iteration 11/25 | Loss: 0.00114872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011487186420708895, 0.0011487186420708895, 0.0011487186420708895, 0.0011487186420708895, 0.0011487186420708895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011487186420708895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26608145
Iteration 2/25 | Loss: 0.00095145
Iteration 3/25 | Loss: 0.00095140
Iteration 4/25 | Loss: 0.00095140
Iteration 5/25 | Loss: 0.00095140
Iteration 6/25 | Loss: 0.00095140
Iteration 7/25 | Loss: 0.00095140
Iteration 8/25 | Loss: 0.00095140
Iteration 9/25 | Loss: 0.00095140
Iteration 10/25 | Loss: 0.00095139
Iteration 11/25 | Loss: 0.00095139
Iteration 12/25 | Loss: 0.00095139
Iteration 13/25 | Loss: 0.00095139
Iteration 14/25 | Loss: 0.00095139
Iteration 15/25 | Loss: 0.00095139
Iteration 16/25 | Loss: 0.00095139
Iteration 17/25 | Loss: 0.00095139
Iteration 18/25 | Loss: 0.00095139
Iteration 19/25 | Loss: 0.00095139
Iteration 20/25 | Loss: 0.00095139
Iteration 21/25 | Loss: 0.00095139
Iteration 22/25 | Loss: 0.00095139
Iteration 23/25 | Loss: 0.00095139
Iteration 24/25 | Loss: 0.00095139
Iteration 25/25 | Loss: 0.00095139

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095139
Iteration 2/1000 | Loss: 0.00003886
Iteration 3/1000 | Loss: 0.00002481
Iteration 4/1000 | Loss: 0.00002062
Iteration 5/1000 | Loss: 0.00001891
Iteration 6/1000 | Loss: 0.00001802
Iteration 7/1000 | Loss: 0.00001750
Iteration 8/1000 | Loss: 0.00001706
Iteration 9/1000 | Loss: 0.00001702
Iteration 10/1000 | Loss: 0.00001670
Iteration 11/1000 | Loss: 0.00001642
Iteration 12/1000 | Loss: 0.00001611
Iteration 13/1000 | Loss: 0.00001608
Iteration 14/1000 | Loss: 0.00001600
Iteration 15/1000 | Loss: 0.00001589
Iteration 16/1000 | Loss: 0.00001589
Iteration 17/1000 | Loss: 0.00001583
Iteration 18/1000 | Loss: 0.00001583
Iteration 19/1000 | Loss: 0.00001583
Iteration 20/1000 | Loss: 0.00001583
Iteration 21/1000 | Loss: 0.00001583
Iteration 22/1000 | Loss: 0.00001582
Iteration 23/1000 | Loss: 0.00001582
Iteration 24/1000 | Loss: 0.00001582
Iteration 25/1000 | Loss: 0.00001581
Iteration 26/1000 | Loss: 0.00001581
Iteration 27/1000 | Loss: 0.00001580
Iteration 28/1000 | Loss: 0.00001580
Iteration 29/1000 | Loss: 0.00001580
Iteration 30/1000 | Loss: 0.00001579
Iteration 31/1000 | Loss: 0.00001579
Iteration 32/1000 | Loss: 0.00001578
Iteration 33/1000 | Loss: 0.00001578
Iteration 34/1000 | Loss: 0.00001578
Iteration 35/1000 | Loss: 0.00001577
Iteration 36/1000 | Loss: 0.00001575
Iteration 37/1000 | Loss: 0.00001574
Iteration 38/1000 | Loss: 0.00001574
Iteration 39/1000 | Loss: 0.00001572
Iteration 40/1000 | Loss: 0.00001572
Iteration 41/1000 | Loss: 0.00001572
Iteration 42/1000 | Loss: 0.00001572
Iteration 43/1000 | Loss: 0.00001572
Iteration 44/1000 | Loss: 0.00001572
Iteration 45/1000 | Loss: 0.00001572
Iteration 46/1000 | Loss: 0.00001572
Iteration 47/1000 | Loss: 0.00001572
Iteration 48/1000 | Loss: 0.00001572
Iteration 49/1000 | Loss: 0.00001571
Iteration 50/1000 | Loss: 0.00001571
Iteration 51/1000 | Loss: 0.00001571
Iteration 52/1000 | Loss: 0.00001571
Iteration 53/1000 | Loss: 0.00001571
Iteration 54/1000 | Loss: 0.00001571
Iteration 55/1000 | Loss: 0.00001571
Iteration 56/1000 | Loss: 0.00001571
Iteration 57/1000 | Loss: 0.00001571
Iteration 58/1000 | Loss: 0.00001570
Iteration 59/1000 | Loss: 0.00001570
Iteration 60/1000 | Loss: 0.00001570
Iteration 61/1000 | Loss: 0.00001569
Iteration 62/1000 | Loss: 0.00001569
Iteration 63/1000 | Loss: 0.00001569
Iteration 64/1000 | Loss: 0.00001569
Iteration 65/1000 | Loss: 0.00001569
Iteration 66/1000 | Loss: 0.00001569
Iteration 67/1000 | Loss: 0.00001569
Iteration 68/1000 | Loss: 0.00001569
Iteration 69/1000 | Loss: 0.00001569
Iteration 70/1000 | Loss: 0.00001568
Iteration 71/1000 | Loss: 0.00001568
Iteration 72/1000 | Loss: 0.00001568
Iteration 73/1000 | Loss: 0.00001567
Iteration 74/1000 | Loss: 0.00001567
Iteration 75/1000 | Loss: 0.00001567
Iteration 76/1000 | Loss: 0.00001566
Iteration 77/1000 | Loss: 0.00001566
Iteration 78/1000 | Loss: 0.00001566
Iteration 79/1000 | Loss: 0.00001566
Iteration 80/1000 | Loss: 0.00001566
Iteration 81/1000 | Loss: 0.00001566
Iteration 82/1000 | Loss: 0.00001565
Iteration 83/1000 | Loss: 0.00001565
Iteration 84/1000 | Loss: 0.00001565
Iteration 85/1000 | Loss: 0.00001565
Iteration 86/1000 | Loss: 0.00001565
Iteration 87/1000 | Loss: 0.00001565
Iteration 88/1000 | Loss: 0.00001565
Iteration 89/1000 | Loss: 0.00001565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.565377169754356e-05, 1.565377169754356e-05, 1.565377169754356e-05, 1.565377169754356e-05, 1.565377169754356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.565377169754356e-05

Optimization complete. Final v2v error: 3.464646577835083 mm

Highest mean error: 4.0715107917785645 mm for frame 13

Lowest mean error: 3.2627270221710205 mm for frame 102

Saving results

Total time: 36.50740909576416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041578
Iteration 2/25 | Loss: 0.00160297
Iteration 3/25 | Loss: 0.00133742
Iteration 4/25 | Loss: 0.00128802
Iteration 5/25 | Loss: 0.00124337
Iteration 6/25 | Loss: 0.00122753
Iteration 7/25 | Loss: 0.00120777
Iteration 8/25 | Loss: 0.00120068
Iteration 9/25 | Loss: 0.00119829
Iteration 10/25 | Loss: 0.00119733
Iteration 11/25 | Loss: 0.00119647
Iteration 12/25 | Loss: 0.00119216
Iteration 13/25 | Loss: 0.00118819
Iteration 14/25 | Loss: 0.00118599
Iteration 15/25 | Loss: 0.00118568
Iteration 16/25 | Loss: 0.00118561
Iteration 17/25 | Loss: 0.00118560
Iteration 18/25 | Loss: 0.00118560
Iteration 19/25 | Loss: 0.00118560
Iteration 20/25 | Loss: 0.00118560
Iteration 21/25 | Loss: 0.00118560
Iteration 22/25 | Loss: 0.00118559
Iteration 23/25 | Loss: 0.00118559
Iteration 24/25 | Loss: 0.00118559
Iteration 25/25 | Loss: 0.00118559

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.19209027
Iteration 2/25 | Loss: 0.00091310
Iteration 3/25 | Loss: 0.00091309
Iteration 4/25 | Loss: 0.00091309
Iteration 5/25 | Loss: 0.00091309
Iteration 6/25 | Loss: 0.00091309
Iteration 7/25 | Loss: 0.00091309
Iteration 8/25 | Loss: 0.00091309
Iteration 9/25 | Loss: 0.00091309
Iteration 10/25 | Loss: 0.00091309
Iteration 11/25 | Loss: 0.00091309
Iteration 12/25 | Loss: 0.00091309
Iteration 13/25 | Loss: 0.00091309
Iteration 14/25 | Loss: 0.00091309
Iteration 15/25 | Loss: 0.00091309
Iteration 16/25 | Loss: 0.00091309
Iteration 17/25 | Loss: 0.00091309
Iteration 18/25 | Loss: 0.00091309
Iteration 19/25 | Loss: 0.00091309
Iteration 20/25 | Loss: 0.00091309
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009130897815339267, 0.0009130897815339267, 0.0009130897815339267, 0.0009130897815339267, 0.0009130897815339267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009130897815339267

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091309
Iteration 2/1000 | Loss: 0.00005948
Iteration 3/1000 | Loss: 0.00003279
Iteration 4/1000 | Loss: 0.00002723
Iteration 5/1000 | Loss: 0.00002526
Iteration 6/1000 | Loss: 0.00002372
Iteration 7/1000 | Loss: 0.00002278
Iteration 8/1000 | Loss: 0.00002175
Iteration 9/1000 | Loss: 0.00028367
Iteration 10/1000 | Loss: 0.00002418
Iteration 11/1000 | Loss: 0.00002120
Iteration 12/1000 | Loss: 0.00001983
Iteration 13/1000 | Loss: 0.00001904
Iteration 14/1000 | Loss: 0.00001865
Iteration 15/1000 | Loss: 0.00001849
Iteration 16/1000 | Loss: 0.00001838
Iteration 17/1000 | Loss: 0.00001833
Iteration 18/1000 | Loss: 0.00001832
Iteration 19/1000 | Loss: 0.00001831
Iteration 20/1000 | Loss: 0.00001831
Iteration 21/1000 | Loss: 0.00001830
Iteration 22/1000 | Loss: 0.00001829
Iteration 23/1000 | Loss: 0.00001829
Iteration 24/1000 | Loss: 0.00001827
Iteration 25/1000 | Loss: 0.00001826
Iteration 26/1000 | Loss: 0.00001825
Iteration 27/1000 | Loss: 0.00001824
Iteration 28/1000 | Loss: 0.00001824
Iteration 29/1000 | Loss: 0.00001824
Iteration 30/1000 | Loss: 0.00001824
Iteration 31/1000 | Loss: 0.00001824
Iteration 32/1000 | Loss: 0.00001823
Iteration 33/1000 | Loss: 0.00001823
Iteration 34/1000 | Loss: 0.00001822
Iteration 35/1000 | Loss: 0.00001819
Iteration 36/1000 | Loss: 0.00001813
Iteration 37/1000 | Loss: 0.00001813
Iteration 38/1000 | Loss: 0.00001812
Iteration 39/1000 | Loss: 0.00001812
Iteration 40/1000 | Loss: 0.00001811
Iteration 41/1000 | Loss: 0.00001810
Iteration 42/1000 | Loss: 0.00001807
Iteration 43/1000 | Loss: 0.00001807
Iteration 44/1000 | Loss: 0.00001806
Iteration 45/1000 | Loss: 0.00001806
Iteration 46/1000 | Loss: 0.00001806
Iteration 47/1000 | Loss: 0.00001805
Iteration 48/1000 | Loss: 0.00001805
Iteration 49/1000 | Loss: 0.00001804
Iteration 50/1000 | Loss: 0.00001804
Iteration 51/1000 | Loss: 0.00001804
Iteration 52/1000 | Loss: 0.00001803
Iteration 53/1000 | Loss: 0.00001803
Iteration 54/1000 | Loss: 0.00001803
Iteration 55/1000 | Loss: 0.00001803
Iteration 56/1000 | Loss: 0.00001803
Iteration 57/1000 | Loss: 0.00001802
Iteration 58/1000 | Loss: 0.00001802
Iteration 59/1000 | Loss: 0.00001802
Iteration 60/1000 | Loss: 0.00001802
Iteration 61/1000 | Loss: 0.00001801
Iteration 62/1000 | Loss: 0.00001801
Iteration 63/1000 | Loss: 0.00001800
Iteration 64/1000 | Loss: 0.00001800
Iteration 65/1000 | Loss: 0.00001800
Iteration 66/1000 | Loss: 0.00001799
Iteration 67/1000 | Loss: 0.00001799
Iteration 68/1000 | Loss: 0.00001798
Iteration 69/1000 | Loss: 0.00001798
Iteration 70/1000 | Loss: 0.00001797
Iteration 71/1000 | Loss: 0.00001797
Iteration 72/1000 | Loss: 0.00001797
Iteration 73/1000 | Loss: 0.00001796
Iteration 74/1000 | Loss: 0.00001796
Iteration 75/1000 | Loss: 0.00001796
Iteration 76/1000 | Loss: 0.00001796
Iteration 77/1000 | Loss: 0.00001796
Iteration 78/1000 | Loss: 0.00001796
Iteration 79/1000 | Loss: 0.00001796
Iteration 80/1000 | Loss: 0.00001796
Iteration 81/1000 | Loss: 0.00001796
Iteration 82/1000 | Loss: 0.00001795
Iteration 83/1000 | Loss: 0.00001795
Iteration 84/1000 | Loss: 0.00001795
Iteration 85/1000 | Loss: 0.00001795
Iteration 86/1000 | Loss: 0.00001794
Iteration 87/1000 | Loss: 0.00001794
Iteration 88/1000 | Loss: 0.00001794
Iteration 89/1000 | Loss: 0.00001793
Iteration 90/1000 | Loss: 0.00001793
Iteration 91/1000 | Loss: 0.00001792
Iteration 92/1000 | Loss: 0.00001792
Iteration 93/1000 | Loss: 0.00001792
Iteration 94/1000 | Loss: 0.00001791
Iteration 95/1000 | Loss: 0.00001789
Iteration 96/1000 | Loss: 0.00001788
Iteration 97/1000 | Loss: 0.00001787
Iteration 98/1000 | Loss: 0.00001787
Iteration 99/1000 | Loss: 0.00001787
Iteration 100/1000 | Loss: 0.00001786
Iteration 101/1000 | Loss: 0.00001786
Iteration 102/1000 | Loss: 0.00001786
Iteration 103/1000 | Loss: 0.00001786
Iteration 104/1000 | Loss: 0.00001786
Iteration 105/1000 | Loss: 0.00001786
Iteration 106/1000 | Loss: 0.00001786
Iteration 107/1000 | Loss: 0.00001786
Iteration 108/1000 | Loss: 0.00001786
Iteration 109/1000 | Loss: 0.00001786
Iteration 110/1000 | Loss: 0.00001786
Iteration 111/1000 | Loss: 0.00001785
Iteration 112/1000 | Loss: 0.00001785
Iteration 113/1000 | Loss: 0.00001785
Iteration 114/1000 | Loss: 0.00001784
Iteration 115/1000 | Loss: 0.00001784
Iteration 116/1000 | Loss: 0.00001784
Iteration 117/1000 | Loss: 0.00001784
Iteration 118/1000 | Loss: 0.00001783
Iteration 119/1000 | Loss: 0.00001783
Iteration 120/1000 | Loss: 0.00001783
Iteration 121/1000 | Loss: 0.00001783
Iteration 122/1000 | Loss: 0.00001783
Iteration 123/1000 | Loss: 0.00001783
Iteration 124/1000 | Loss: 0.00001783
Iteration 125/1000 | Loss: 0.00001783
Iteration 126/1000 | Loss: 0.00001783
Iteration 127/1000 | Loss: 0.00001783
Iteration 128/1000 | Loss: 0.00001783
Iteration 129/1000 | Loss: 0.00001782
Iteration 130/1000 | Loss: 0.00001782
Iteration 131/1000 | Loss: 0.00001782
Iteration 132/1000 | Loss: 0.00001782
Iteration 133/1000 | Loss: 0.00001782
Iteration 134/1000 | Loss: 0.00001782
Iteration 135/1000 | Loss: 0.00001782
Iteration 136/1000 | Loss: 0.00001782
Iteration 137/1000 | Loss: 0.00001782
Iteration 138/1000 | Loss: 0.00001782
Iteration 139/1000 | Loss: 0.00001782
Iteration 140/1000 | Loss: 0.00001781
Iteration 141/1000 | Loss: 0.00001781
Iteration 142/1000 | Loss: 0.00001781
Iteration 143/1000 | Loss: 0.00001781
Iteration 144/1000 | Loss: 0.00001781
Iteration 145/1000 | Loss: 0.00001780
Iteration 146/1000 | Loss: 0.00001780
Iteration 147/1000 | Loss: 0.00001780
Iteration 148/1000 | Loss: 0.00001780
Iteration 149/1000 | Loss: 0.00001780
Iteration 150/1000 | Loss: 0.00001780
Iteration 151/1000 | Loss: 0.00001780
Iteration 152/1000 | Loss: 0.00001779
Iteration 153/1000 | Loss: 0.00001779
Iteration 154/1000 | Loss: 0.00001779
Iteration 155/1000 | Loss: 0.00001779
Iteration 156/1000 | Loss: 0.00001779
Iteration 157/1000 | Loss: 0.00001779
Iteration 158/1000 | Loss: 0.00001779
Iteration 159/1000 | Loss: 0.00001779
Iteration 160/1000 | Loss: 0.00001779
Iteration 161/1000 | Loss: 0.00001779
Iteration 162/1000 | Loss: 0.00001779
Iteration 163/1000 | Loss: 0.00001779
Iteration 164/1000 | Loss: 0.00001779
Iteration 165/1000 | Loss: 0.00001779
Iteration 166/1000 | Loss: 0.00001779
Iteration 167/1000 | Loss: 0.00001779
Iteration 168/1000 | Loss: 0.00001779
Iteration 169/1000 | Loss: 0.00001779
Iteration 170/1000 | Loss: 0.00001778
Iteration 171/1000 | Loss: 0.00001778
Iteration 172/1000 | Loss: 0.00001778
Iteration 173/1000 | Loss: 0.00001778
Iteration 174/1000 | Loss: 0.00001778
Iteration 175/1000 | Loss: 0.00001778
Iteration 176/1000 | Loss: 0.00001778
Iteration 177/1000 | Loss: 0.00001778
Iteration 178/1000 | Loss: 0.00001778
Iteration 179/1000 | Loss: 0.00001778
Iteration 180/1000 | Loss: 0.00001778
Iteration 181/1000 | Loss: 0.00001778
Iteration 182/1000 | Loss: 0.00001778
Iteration 183/1000 | Loss: 0.00001778
Iteration 184/1000 | Loss: 0.00001778
Iteration 185/1000 | Loss: 0.00001778
Iteration 186/1000 | Loss: 0.00001778
Iteration 187/1000 | Loss: 0.00001778
Iteration 188/1000 | Loss: 0.00001778
Iteration 189/1000 | Loss: 0.00001778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.777805300662294e-05, 1.777805300662294e-05, 1.777805300662294e-05, 1.777805300662294e-05, 1.777805300662294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.777805300662294e-05

Optimization complete. Final v2v error: 3.6570322513580322 mm

Highest mean error: 4.285744667053223 mm for frame 0

Lowest mean error: 3.3321573734283447 mm for frame 127

Saving results

Total time: 73.60840368270874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032400
Iteration 2/25 | Loss: 0.01032400
Iteration 3/25 | Loss: 0.00314661
Iteration 4/25 | Loss: 0.00226450
Iteration 5/25 | Loss: 0.00212097
Iteration 6/25 | Loss: 0.00193520
Iteration 7/25 | Loss: 0.00167795
Iteration 8/25 | Loss: 0.00154562
Iteration 9/25 | Loss: 0.00145039
Iteration 10/25 | Loss: 0.00141504
Iteration 11/25 | Loss: 0.00138616
Iteration 12/25 | Loss: 0.00136748
Iteration 13/25 | Loss: 0.00135761
Iteration 14/25 | Loss: 0.00134646
Iteration 15/25 | Loss: 0.00134574
Iteration 16/25 | Loss: 0.00134114
Iteration 17/25 | Loss: 0.00133669
Iteration 18/25 | Loss: 0.00133917
Iteration 19/25 | Loss: 0.00133778
Iteration 20/25 | Loss: 0.00133488
Iteration 21/25 | Loss: 0.00133388
Iteration 22/25 | Loss: 0.00133300
Iteration 23/25 | Loss: 0.00133281
Iteration 24/25 | Loss: 0.00133281
Iteration 25/25 | Loss: 0.00133281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28391290
Iteration 2/25 | Loss: 0.00224564
Iteration 3/25 | Loss: 0.00211054
Iteration 4/25 | Loss: 0.00211054
Iteration 5/25 | Loss: 0.00211054
Iteration 6/25 | Loss: 0.00211054
Iteration 7/25 | Loss: 0.00211054
Iteration 8/25 | Loss: 0.00211054
Iteration 9/25 | Loss: 0.00211054
Iteration 10/25 | Loss: 0.00211054
Iteration 11/25 | Loss: 0.00211054
Iteration 12/25 | Loss: 0.00211054
Iteration 13/25 | Loss: 0.00211054
Iteration 14/25 | Loss: 0.00211054
Iteration 15/25 | Loss: 0.00211054
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0021105394698679447, 0.0021105394698679447, 0.0021105394698679447, 0.0021105394698679447, 0.0021105394698679447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021105394698679447

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211054
Iteration 2/1000 | Loss: 0.00066658
Iteration 3/1000 | Loss: 0.00092387
Iteration 4/1000 | Loss: 0.00025966
Iteration 5/1000 | Loss: 0.00034317
Iteration 6/1000 | Loss: 0.00020696
Iteration 7/1000 | Loss: 0.00039118
Iteration 8/1000 | Loss: 0.00021842
Iteration 9/1000 | Loss: 0.00027782
Iteration 10/1000 | Loss: 0.00026582
Iteration 11/1000 | Loss: 0.00038511
Iteration 12/1000 | Loss: 0.00015317
Iteration 13/1000 | Loss: 0.00036971
Iteration 14/1000 | Loss: 0.00053070
Iteration 15/1000 | Loss: 0.00108239
Iteration 16/1000 | Loss: 0.00030261
Iteration 17/1000 | Loss: 0.00021694
Iteration 18/1000 | Loss: 0.00016328
Iteration 19/1000 | Loss: 0.00015876
Iteration 20/1000 | Loss: 0.00023119
Iteration 21/1000 | Loss: 0.00021094
Iteration 22/1000 | Loss: 0.00012334
Iteration 23/1000 | Loss: 0.00022891
Iteration 24/1000 | Loss: 0.00011695
Iteration 25/1000 | Loss: 0.00012974
Iteration 26/1000 | Loss: 0.00011292
Iteration 27/1000 | Loss: 0.00018450
Iteration 28/1000 | Loss: 0.00019766
Iteration 29/1000 | Loss: 0.00018553
Iteration 30/1000 | Loss: 0.00038874
Iteration 31/1000 | Loss: 0.00035788
Iteration 32/1000 | Loss: 0.00058405
Iteration 33/1000 | Loss: 0.00040511
Iteration 34/1000 | Loss: 0.00017611
Iteration 35/1000 | Loss: 0.00015976
Iteration 36/1000 | Loss: 0.00018643
Iteration 37/1000 | Loss: 0.00036821
Iteration 38/1000 | Loss: 0.00017618
Iteration 39/1000 | Loss: 0.00013829
Iteration 40/1000 | Loss: 0.00021977
Iteration 41/1000 | Loss: 0.00012034
Iteration 42/1000 | Loss: 0.00024504
Iteration 43/1000 | Loss: 0.00030622
Iteration 44/1000 | Loss: 0.00024332
Iteration 45/1000 | Loss: 0.00012207
Iteration 46/1000 | Loss: 0.00011724
Iteration 47/1000 | Loss: 0.00011377
Iteration 48/1000 | Loss: 0.00025465
Iteration 49/1000 | Loss: 0.00010620
Iteration 50/1000 | Loss: 0.00010454
Iteration 51/1000 | Loss: 0.00009919
Iteration 52/1000 | Loss: 0.00010549
Iteration 53/1000 | Loss: 0.00011209
Iteration 54/1000 | Loss: 0.00009288
Iteration 55/1000 | Loss: 0.00018961
Iteration 56/1000 | Loss: 0.00011966
Iteration 57/1000 | Loss: 0.00041348
Iteration 58/1000 | Loss: 0.00009475
Iteration 59/1000 | Loss: 0.00010542
Iteration 60/1000 | Loss: 0.00009172
Iteration 61/1000 | Loss: 0.00013005
Iteration 62/1000 | Loss: 0.00035849
Iteration 63/1000 | Loss: 0.00014431
Iteration 64/1000 | Loss: 0.00026933
Iteration 65/1000 | Loss: 0.00015268
Iteration 66/1000 | Loss: 0.00009045
Iteration 67/1000 | Loss: 0.00009152
Iteration 68/1000 | Loss: 0.00009934
Iteration 69/1000 | Loss: 0.00009580
Iteration 70/1000 | Loss: 0.00009460
Iteration 71/1000 | Loss: 0.00008577
Iteration 72/1000 | Loss: 0.00025048
Iteration 73/1000 | Loss: 0.00013294
Iteration 74/1000 | Loss: 0.00008998
Iteration 75/1000 | Loss: 0.00008683
Iteration 76/1000 | Loss: 0.00008881
Iteration 77/1000 | Loss: 0.00008499
Iteration 78/1000 | Loss: 0.00008499
Iteration 79/1000 | Loss: 0.00008499
Iteration 80/1000 | Loss: 0.00008499
Iteration 81/1000 | Loss: 0.00008499
Iteration 82/1000 | Loss: 0.00008499
Iteration 83/1000 | Loss: 0.00008498
Iteration 84/1000 | Loss: 0.00008619
Iteration 85/1000 | Loss: 0.00008489
Iteration 86/1000 | Loss: 0.00008486
Iteration 87/1000 | Loss: 0.00017529
Iteration 88/1000 | Loss: 0.00014196
Iteration 89/1000 | Loss: 0.00009462
Iteration 90/1000 | Loss: 0.00014653
Iteration 91/1000 | Loss: 0.00008870
Iteration 92/1000 | Loss: 0.00011444
Iteration 93/1000 | Loss: 0.00010251
Iteration 94/1000 | Loss: 0.00015607
Iteration 95/1000 | Loss: 0.00011278
Iteration 96/1000 | Loss: 0.00009009
Iteration 97/1000 | Loss: 0.00009860
Iteration 98/1000 | Loss: 0.00008554
Iteration 99/1000 | Loss: 0.00009125
Iteration 100/1000 | Loss: 0.00008407
Iteration 101/1000 | Loss: 0.00008617
Iteration 102/1000 | Loss: 0.00008363
Iteration 103/1000 | Loss: 0.00008559
Iteration 104/1000 | Loss: 0.00008147
Iteration 105/1000 | Loss: 0.00008147
Iteration 106/1000 | Loss: 0.00008146
Iteration 107/1000 | Loss: 0.00008146
Iteration 108/1000 | Loss: 0.00008145
Iteration 109/1000 | Loss: 0.00008134
Iteration 110/1000 | Loss: 0.00008128
Iteration 111/1000 | Loss: 0.00008128
Iteration 112/1000 | Loss: 0.00008128
Iteration 113/1000 | Loss: 0.00008128
Iteration 114/1000 | Loss: 0.00008128
Iteration 115/1000 | Loss: 0.00008127
Iteration 116/1000 | Loss: 0.00008127
Iteration 117/1000 | Loss: 0.00008127
Iteration 118/1000 | Loss: 0.00008127
Iteration 119/1000 | Loss: 0.00017570
Iteration 120/1000 | Loss: 0.00009269
Iteration 121/1000 | Loss: 0.00008474
Iteration 122/1000 | Loss: 0.00008302
Iteration 123/1000 | Loss: 0.00008227
Iteration 124/1000 | Loss: 0.00008082
Iteration 125/1000 | Loss: 0.00008414
Iteration 126/1000 | Loss: 0.00008030
Iteration 127/1000 | Loss: 0.00007996
Iteration 128/1000 | Loss: 0.00007981
Iteration 129/1000 | Loss: 0.00007979
Iteration 130/1000 | Loss: 0.00008636
Iteration 131/1000 | Loss: 0.00008045
Iteration 132/1000 | Loss: 0.00007965
Iteration 133/1000 | Loss: 0.00007964
Iteration 134/1000 | Loss: 0.00007964
Iteration 135/1000 | Loss: 0.00007964
Iteration 136/1000 | Loss: 0.00007964
Iteration 137/1000 | Loss: 0.00007964
Iteration 138/1000 | Loss: 0.00007964
Iteration 139/1000 | Loss: 0.00007964
Iteration 140/1000 | Loss: 0.00007964
Iteration 141/1000 | Loss: 0.00007963
Iteration 142/1000 | Loss: 0.00007963
Iteration 143/1000 | Loss: 0.00007963
Iteration 144/1000 | Loss: 0.00007963
Iteration 145/1000 | Loss: 0.00007963
Iteration 146/1000 | Loss: 0.00007963
Iteration 147/1000 | Loss: 0.00007963
Iteration 148/1000 | Loss: 0.00007963
Iteration 149/1000 | Loss: 0.00008190
Iteration 150/1000 | Loss: 0.00007966
Iteration 151/1000 | Loss: 0.00007965
Iteration 152/1000 | Loss: 0.00007961
Iteration 153/1000 | Loss: 0.00007961
Iteration 154/1000 | Loss: 0.00007960
Iteration 155/1000 | Loss: 0.00007960
Iteration 156/1000 | Loss: 0.00007960
Iteration 157/1000 | Loss: 0.00007960
Iteration 158/1000 | Loss: 0.00007960
Iteration 159/1000 | Loss: 0.00007960
Iteration 160/1000 | Loss: 0.00007960
Iteration 161/1000 | Loss: 0.00007960
Iteration 162/1000 | Loss: 0.00007960
Iteration 163/1000 | Loss: 0.00007960
Iteration 164/1000 | Loss: 0.00007959
Iteration 165/1000 | Loss: 0.00007959
Iteration 166/1000 | Loss: 0.00007959
Iteration 167/1000 | Loss: 0.00007959
Iteration 168/1000 | Loss: 0.00007959
Iteration 169/1000 | Loss: 0.00007959
Iteration 170/1000 | Loss: 0.00007959
Iteration 171/1000 | Loss: 0.00007958
Iteration 172/1000 | Loss: 0.00007958
Iteration 173/1000 | Loss: 0.00007958
Iteration 174/1000 | Loss: 0.00007958
Iteration 175/1000 | Loss: 0.00007958
Iteration 176/1000 | Loss: 0.00007957
Iteration 177/1000 | Loss: 0.00007957
Iteration 178/1000 | Loss: 0.00007957
Iteration 179/1000 | Loss: 0.00007957
Iteration 180/1000 | Loss: 0.00007957
Iteration 181/1000 | Loss: 0.00008069
Iteration 182/1000 | Loss: 0.00007957
Iteration 183/1000 | Loss: 0.00007957
Iteration 184/1000 | Loss: 0.00007957
Iteration 185/1000 | Loss: 0.00007957
Iteration 186/1000 | Loss: 0.00007957
Iteration 187/1000 | Loss: 0.00007957
Iteration 188/1000 | Loss: 0.00007957
Iteration 189/1000 | Loss: 0.00007957
Iteration 190/1000 | Loss: 0.00007957
Iteration 191/1000 | Loss: 0.00007956
Iteration 192/1000 | Loss: 0.00007956
Iteration 193/1000 | Loss: 0.00007956
Iteration 194/1000 | Loss: 0.00007955
Iteration 195/1000 | Loss: 0.00007955
Iteration 196/1000 | Loss: 0.00007955
Iteration 197/1000 | Loss: 0.00007955
Iteration 198/1000 | Loss: 0.00007955
Iteration 199/1000 | Loss: 0.00007955
Iteration 200/1000 | Loss: 0.00007955
Iteration 201/1000 | Loss: 0.00007955
Iteration 202/1000 | Loss: 0.00007955
Iteration 203/1000 | Loss: 0.00007955
Iteration 204/1000 | Loss: 0.00007954
Iteration 205/1000 | Loss: 0.00007954
Iteration 206/1000 | Loss: 0.00007954
Iteration 207/1000 | Loss: 0.00007954
Iteration 208/1000 | Loss: 0.00007954
Iteration 209/1000 | Loss: 0.00007954
Iteration 210/1000 | Loss: 0.00007954
Iteration 211/1000 | Loss: 0.00007954
Iteration 212/1000 | Loss: 0.00007954
Iteration 213/1000 | Loss: 0.00007954
Iteration 214/1000 | Loss: 0.00007953
Iteration 215/1000 | Loss: 0.00007953
Iteration 216/1000 | Loss: 0.00007953
Iteration 217/1000 | Loss: 0.00007953
Iteration 218/1000 | Loss: 0.00007953
Iteration 219/1000 | Loss: 0.00007953
Iteration 220/1000 | Loss: 0.00007953
Iteration 221/1000 | Loss: 0.00007953
Iteration 222/1000 | Loss: 0.00007953
Iteration 223/1000 | Loss: 0.00007953
Iteration 224/1000 | Loss: 0.00007953
Iteration 225/1000 | Loss: 0.00007953
Iteration 226/1000 | Loss: 0.00007952
Iteration 227/1000 | Loss: 0.00007952
Iteration 228/1000 | Loss: 0.00007952
Iteration 229/1000 | Loss: 0.00007952
Iteration 230/1000 | Loss: 0.00008041
Iteration 231/1000 | Loss: 0.00007951
Iteration 232/1000 | Loss: 0.00007951
Iteration 233/1000 | Loss: 0.00007951
Iteration 234/1000 | Loss: 0.00007951
Iteration 235/1000 | Loss: 0.00007951
Iteration 236/1000 | Loss: 0.00007951
Iteration 237/1000 | Loss: 0.00007951
Iteration 238/1000 | Loss: 0.00007951
Iteration 239/1000 | Loss: 0.00007951
Iteration 240/1000 | Loss: 0.00007951
Iteration 241/1000 | Loss: 0.00007951
Iteration 242/1000 | Loss: 0.00007951
Iteration 243/1000 | Loss: 0.00007951
Iteration 244/1000 | Loss: 0.00007951
Iteration 245/1000 | Loss: 0.00007951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [7.950622239150107e-05, 7.950622239150107e-05, 7.950622239150107e-05, 7.950622239150107e-05, 7.950622239150107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.950622239150107e-05

Optimization complete. Final v2v error: 5.0737996101379395 mm

Highest mean error: 22.240222930908203 mm for frame 135

Lowest mean error: 3.580303430557251 mm for frame 59

Saving results

Total time: 231.6609718799591
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039222
Iteration 2/25 | Loss: 0.00288150
Iteration 3/25 | Loss: 0.00209155
Iteration 4/25 | Loss: 0.00199602
Iteration 5/25 | Loss: 0.00197416
Iteration 6/25 | Loss: 0.00195426
Iteration 7/25 | Loss: 0.00194920
Iteration 8/25 | Loss: 0.00194415
Iteration 9/25 | Loss: 0.00194415
Iteration 10/25 | Loss: 0.00193824
Iteration 11/25 | Loss: 0.00193565
Iteration 12/25 | Loss: 0.00193208
Iteration 13/25 | Loss: 0.00192886
Iteration 14/25 | Loss: 0.00192771
Iteration 15/25 | Loss: 0.00193102
Iteration 16/25 | Loss: 0.00193150
Iteration 17/25 | Loss: 0.00193124
Iteration 18/25 | Loss: 0.00193060
Iteration 19/25 | Loss: 0.00192991
Iteration 20/25 | Loss: 0.00192946
Iteration 21/25 | Loss: 0.00192988
Iteration 22/25 | Loss: 0.00192783
Iteration 23/25 | Loss: 0.00192752
Iteration 24/25 | Loss: 0.00192726
Iteration 25/25 | Loss: 0.00192708

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27595532
Iteration 2/25 | Loss: 0.00527773
Iteration 3/25 | Loss: 0.00527773
Iteration 4/25 | Loss: 0.00527772
Iteration 5/25 | Loss: 0.00527772
Iteration 6/25 | Loss: 0.00527772
Iteration 7/25 | Loss: 0.00527772
Iteration 8/25 | Loss: 0.00527772
Iteration 9/25 | Loss: 0.00527772
Iteration 10/25 | Loss: 0.00527772
Iteration 11/25 | Loss: 0.00527772
Iteration 12/25 | Loss: 0.00527772
Iteration 13/25 | Loss: 0.00527772
Iteration 14/25 | Loss: 0.00527772
Iteration 15/25 | Loss: 0.00527772
Iteration 16/25 | Loss: 0.00527772
Iteration 17/25 | Loss: 0.00527772
Iteration 18/25 | Loss: 0.00527772
Iteration 19/25 | Loss: 0.00527772
Iteration 20/25 | Loss: 0.00527772
Iteration 21/25 | Loss: 0.00527772
Iteration 22/25 | Loss: 0.00527772
Iteration 23/25 | Loss: 0.00527772
Iteration 24/25 | Loss: 0.00527772
Iteration 25/25 | Loss: 0.00527772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00527772
Iteration 2/1000 | Loss: 0.00076882
Iteration 3/1000 | Loss: 0.00055747
Iteration 4/1000 | Loss: 0.00049289
Iteration 5/1000 | Loss: 0.00045130
Iteration 6/1000 | Loss: 0.00041631
Iteration 7/1000 | Loss: 0.00039453
Iteration 8/1000 | Loss: 0.00037228
Iteration 9/1000 | Loss: 0.00357323
Iteration 10/1000 | Loss: 0.02208417
Iteration 11/1000 | Loss: 0.00058312
Iteration 12/1000 | Loss: 0.00035719
Iteration 13/1000 | Loss: 0.00022780
Iteration 14/1000 | Loss: 0.00015747
Iteration 15/1000 | Loss: 0.00011359
Iteration 16/1000 | Loss: 0.00007506
Iteration 17/1000 | Loss: 0.00005584
Iteration 18/1000 | Loss: 0.00004439
Iteration 19/1000 | Loss: 0.00003707
Iteration 20/1000 | Loss: 0.00003182
Iteration 21/1000 | Loss: 0.00002852
Iteration 22/1000 | Loss: 0.00002469
Iteration 23/1000 | Loss: 0.00002201
Iteration 24/1000 | Loss: 0.00001974
Iteration 25/1000 | Loss: 0.00001840
Iteration 26/1000 | Loss: 0.00001746
Iteration 27/1000 | Loss: 0.00001669
Iteration 28/1000 | Loss: 0.00001611
Iteration 29/1000 | Loss: 0.00001565
Iteration 30/1000 | Loss: 0.00001543
Iteration 31/1000 | Loss: 0.00001534
Iteration 32/1000 | Loss: 0.00001531
Iteration 33/1000 | Loss: 0.00001531
Iteration 34/1000 | Loss: 0.00001531
Iteration 35/1000 | Loss: 0.00001531
Iteration 36/1000 | Loss: 0.00001531
Iteration 37/1000 | Loss: 0.00001531
Iteration 38/1000 | Loss: 0.00001531
Iteration 39/1000 | Loss: 0.00001530
Iteration 40/1000 | Loss: 0.00001530
Iteration 41/1000 | Loss: 0.00001530
Iteration 42/1000 | Loss: 0.00001530
Iteration 43/1000 | Loss: 0.00001530
Iteration 44/1000 | Loss: 0.00001530
Iteration 45/1000 | Loss: 0.00001530
Iteration 46/1000 | Loss: 0.00001530
Iteration 47/1000 | Loss: 0.00001530
Iteration 48/1000 | Loss: 0.00001530
Iteration 49/1000 | Loss: 0.00001530
Iteration 50/1000 | Loss: 0.00001530
Iteration 51/1000 | Loss: 0.00001529
Iteration 52/1000 | Loss: 0.00001529
Iteration 53/1000 | Loss: 0.00001529
Iteration 54/1000 | Loss: 0.00001528
Iteration 55/1000 | Loss: 0.00001528
Iteration 56/1000 | Loss: 0.00001527
Iteration 57/1000 | Loss: 0.00001526
Iteration 58/1000 | Loss: 0.00001526
Iteration 59/1000 | Loss: 0.00001525
Iteration 60/1000 | Loss: 0.00001525
Iteration 61/1000 | Loss: 0.00001524
Iteration 62/1000 | Loss: 0.00001524
Iteration 63/1000 | Loss: 0.00001523
Iteration 64/1000 | Loss: 0.00001523
Iteration 65/1000 | Loss: 0.00001523
Iteration 66/1000 | Loss: 0.00001523
Iteration 67/1000 | Loss: 0.00001523
Iteration 68/1000 | Loss: 0.00001522
Iteration 69/1000 | Loss: 0.00001522
Iteration 70/1000 | Loss: 0.00001522
Iteration 71/1000 | Loss: 0.00001522
Iteration 72/1000 | Loss: 0.00001522
Iteration 73/1000 | Loss: 0.00001522
Iteration 74/1000 | Loss: 0.00001522
Iteration 75/1000 | Loss: 0.00001522
Iteration 76/1000 | Loss: 0.00001521
Iteration 77/1000 | Loss: 0.00001520
Iteration 78/1000 | Loss: 0.00001520
Iteration 79/1000 | Loss: 0.00001520
Iteration 80/1000 | Loss: 0.00001520
Iteration 81/1000 | Loss: 0.00001520
Iteration 82/1000 | Loss: 0.00001520
Iteration 83/1000 | Loss: 0.00001520
Iteration 84/1000 | Loss: 0.00001520
Iteration 85/1000 | Loss: 0.00001519
Iteration 86/1000 | Loss: 0.00001519
Iteration 87/1000 | Loss: 0.00001519
Iteration 88/1000 | Loss: 0.00001519
Iteration 89/1000 | Loss: 0.00001519
Iteration 90/1000 | Loss: 0.00001519
Iteration 91/1000 | Loss: 0.00001519
Iteration 92/1000 | Loss: 0.00001519
Iteration 93/1000 | Loss: 0.00001519
Iteration 94/1000 | Loss: 0.00001519
Iteration 95/1000 | Loss: 0.00001519
Iteration 96/1000 | Loss: 0.00001519
Iteration 97/1000 | Loss: 0.00001519
Iteration 98/1000 | Loss: 0.00001519
Iteration 99/1000 | Loss: 0.00001519
Iteration 100/1000 | Loss: 0.00001519
Iteration 101/1000 | Loss: 0.00001519
Iteration 102/1000 | Loss: 0.00001519
Iteration 103/1000 | Loss: 0.00001519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.5193251783784945e-05, 1.5193251783784945e-05, 1.5193251783784945e-05, 1.5193251783784945e-05, 1.5193251783784945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5193251783784945e-05

Optimization complete. Final v2v error: 3.439474105834961 mm

Highest mean error: 3.526313304901123 mm for frame 37

Lowest mean error: 3.3414804935455322 mm for frame 97

Saving results

Total time: 105.34959030151367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064446
Iteration 2/25 | Loss: 0.00276741
Iteration 3/25 | Loss: 0.00243927
Iteration 4/25 | Loss: 0.00237763
Iteration 5/25 | Loss: 0.00214449
Iteration 6/25 | Loss: 0.00199178
Iteration 7/25 | Loss: 0.00175618
Iteration 8/25 | Loss: 0.00154785
Iteration 9/25 | Loss: 0.00146080
Iteration 10/25 | Loss: 0.00140515
Iteration 11/25 | Loss: 0.00137820
Iteration 12/25 | Loss: 0.00137050
Iteration 13/25 | Loss: 0.00137392
Iteration 14/25 | Loss: 0.00139724
Iteration 15/25 | Loss: 0.00138693
Iteration 16/25 | Loss: 0.00136551
Iteration 17/25 | Loss: 0.00135335
Iteration 18/25 | Loss: 0.00134545
Iteration 19/25 | Loss: 0.00135227
Iteration 20/25 | Loss: 0.00134367
Iteration 21/25 | Loss: 0.00134271
Iteration 22/25 | Loss: 0.00134826
Iteration 23/25 | Loss: 0.00134755
Iteration 24/25 | Loss: 0.00134847
Iteration 25/25 | Loss: 0.00134459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34632313
Iteration 2/25 | Loss: 0.00507855
Iteration 3/25 | Loss: 0.00239608
Iteration 4/25 | Loss: 0.00208887
Iteration 5/25 | Loss: 0.00208887
Iteration 6/25 | Loss: 0.00208887
Iteration 7/25 | Loss: 0.00208887
Iteration 8/25 | Loss: 0.00208887
Iteration 9/25 | Loss: 0.00208887
Iteration 10/25 | Loss: 0.00208887
Iteration 11/25 | Loss: 0.00208887
Iteration 12/25 | Loss: 0.00208887
Iteration 13/25 | Loss: 0.00208887
Iteration 14/25 | Loss: 0.00208887
Iteration 15/25 | Loss: 0.00208887
Iteration 16/25 | Loss: 0.00208887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0020888657309114933, 0.0020888657309114933, 0.0020888657309114933, 0.0020888657309114933, 0.0020888657309114933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020888657309114933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208887
Iteration 2/1000 | Loss: 0.00237285
Iteration 3/1000 | Loss: 0.00031995
Iteration 4/1000 | Loss: 0.00103557
Iteration 5/1000 | Loss: 0.00041150
Iteration 6/1000 | Loss: 0.00123199
Iteration 7/1000 | Loss: 0.00042695
Iteration 8/1000 | Loss: 0.00130470
Iteration 9/1000 | Loss: 0.00045058
Iteration 10/1000 | Loss: 0.00017862
Iteration 11/1000 | Loss: 0.00016575
Iteration 12/1000 | Loss: 0.00096792
Iteration 13/1000 | Loss: 0.00137786
Iteration 14/1000 | Loss: 0.00196645
Iteration 15/1000 | Loss: 0.00097459
Iteration 16/1000 | Loss: 0.00026882
Iteration 17/1000 | Loss: 0.00032567
Iteration 18/1000 | Loss: 0.00029875
Iteration 19/1000 | Loss: 0.00015119
Iteration 20/1000 | Loss: 0.00086778
Iteration 21/1000 | Loss: 0.00158704
Iteration 22/1000 | Loss: 0.00034932
Iteration 23/1000 | Loss: 0.00036226
Iteration 24/1000 | Loss: 0.00038244
Iteration 25/1000 | Loss: 0.00149562
Iteration 26/1000 | Loss: 0.00014924
Iteration 27/1000 | Loss: 0.00077162
Iteration 28/1000 | Loss: 0.00014445
Iteration 29/1000 | Loss: 0.00014269
Iteration 30/1000 | Loss: 0.00013975
Iteration 31/1000 | Loss: 0.00089240
Iteration 32/1000 | Loss: 0.00325765
Iteration 33/1000 | Loss: 0.00378356
Iteration 34/1000 | Loss: 0.00304157
Iteration 35/1000 | Loss: 0.00025680
Iteration 36/1000 | Loss: 0.00075031
Iteration 37/1000 | Loss: 0.00084512
Iteration 38/1000 | Loss: 0.00044378
Iteration 39/1000 | Loss: 0.00018106
Iteration 40/1000 | Loss: 0.00037376
Iteration 41/1000 | Loss: 0.00067276
Iteration 42/1000 | Loss: 0.00054720
Iteration 43/1000 | Loss: 0.00020692
Iteration 44/1000 | Loss: 0.00044796
Iteration 45/1000 | Loss: 0.00044527
Iteration 46/1000 | Loss: 0.00014106
Iteration 47/1000 | Loss: 0.00013350
Iteration 48/1000 | Loss: 0.00023682
Iteration 49/1000 | Loss: 0.00039471
Iteration 50/1000 | Loss: 0.00035334
Iteration 51/1000 | Loss: 0.00018425
Iteration 52/1000 | Loss: 0.00019067
Iteration 53/1000 | Loss: 0.00011032
Iteration 54/1000 | Loss: 0.00010652
Iteration 55/1000 | Loss: 0.00009296
Iteration 56/1000 | Loss: 0.00008940
Iteration 57/1000 | Loss: 0.00008620
Iteration 58/1000 | Loss: 0.00008453
Iteration 59/1000 | Loss: 0.00008346
Iteration 60/1000 | Loss: 0.00013125
Iteration 61/1000 | Loss: 0.00062724
Iteration 62/1000 | Loss: 0.00009916
Iteration 63/1000 | Loss: 0.00014579
Iteration 64/1000 | Loss: 0.00008445
Iteration 65/1000 | Loss: 0.00008224
Iteration 66/1000 | Loss: 0.00008096
Iteration 67/1000 | Loss: 0.00008004
Iteration 68/1000 | Loss: 0.00007956
Iteration 69/1000 | Loss: 0.00007924
Iteration 70/1000 | Loss: 0.00018868
Iteration 71/1000 | Loss: 0.00008858
Iteration 72/1000 | Loss: 0.00008218
Iteration 73/1000 | Loss: 0.00007994
Iteration 74/1000 | Loss: 0.00007865
Iteration 75/1000 | Loss: 0.00007791
Iteration 76/1000 | Loss: 0.00007755
Iteration 77/1000 | Loss: 0.00007733
Iteration 78/1000 | Loss: 0.00007726
Iteration 79/1000 | Loss: 0.00019229
Iteration 80/1000 | Loss: 0.00019808
Iteration 81/1000 | Loss: 0.00010695
Iteration 82/1000 | Loss: 0.00011046
Iteration 83/1000 | Loss: 0.00015361
Iteration 84/1000 | Loss: 0.00008160
Iteration 85/1000 | Loss: 0.00007835
Iteration 86/1000 | Loss: 0.00007625
Iteration 87/1000 | Loss: 0.00007526
Iteration 88/1000 | Loss: 0.00007474
Iteration 89/1000 | Loss: 0.00007433
Iteration 90/1000 | Loss: 0.00007405
Iteration 91/1000 | Loss: 0.00007396
Iteration 92/1000 | Loss: 0.00007391
Iteration 93/1000 | Loss: 0.00007391
Iteration 94/1000 | Loss: 0.00007387
Iteration 95/1000 | Loss: 0.00007386
Iteration 96/1000 | Loss: 0.00007386
Iteration 97/1000 | Loss: 0.00007386
Iteration 98/1000 | Loss: 0.00007386
Iteration 99/1000 | Loss: 0.00007386
Iteration 100/1000 | Loss: 0.00007386
Iteration 101/1000 | Loss: 0.00007385
Iteration 102/1000 | Loss: 0.00007385
Iteration 103/1000 | Loss: 0.00007380
Iteration 104/1000 | Loss: 0.00007379
Iteration 105/1000 | Loss: 0.00007377
Iteration 106/1000 | Loss: 0.00007377
Iteration 107/1000 | Loss: 0.00007377
Iteration 108/1000 | Loss: 0.00007376
Iteration 109/1000 | Loss: 0.00007376
Iteration 110/1000 | Loss: 0.00007376
Iteration 111/1000 | Loss: 0.00007376
Iteration 112/1000 | Loss: 0.00007376
Iteration 113/1000 | Loss: 0.00007375
Iteration 114/1000 | Loss: 0.00007375
Iteration 115/1000 | Loss: 0.00007374
Iteration 116/1000 | Loss: 0.00007373
Iteration 117/1000 | Loss: 0.00007372
Iteration 118/1000 | Loss: 0.00007370
Iteration 119/1000 | Loss: 0.00007370
Iteration 120/1000 | Loss: 0.00007369
Iteration 121/1000 | Loss: 0.00007369
Iteration 122/1000 | Loss: 0.00007369
Iteration 123/1000 | Loss: 0.00007367
Iteration 124/1000 | Loss: 0.00007367
Iteration 125/1000 | Loss: 0.00007367
Iteration 126/1000 | Loss: 0.00007366
Iteration 127/1000 | Loss: 0.00007366
Iteration 128/1000 | Loss: 0.00007366
Iteration 129/1000 | Loss: 0.00007366
Iteration 130/1000 | Loss: 0.00007365
Iteration 131/1000 | Loss: 0.00007365
Iteration 132/1000 | Loss: 0.00007365
Iteration 133/1000 | Loss: 0.00007365
Iteration 134/1000 | Loss: 0.00007365
Iteration 135/1000 | Loss: 0.00007365
Iteration 136/1000 | Loss: 0.00007365
Iteration 137/1000 | Loss: 0.00007364
Iteration 138/1000 | Loss: 0.00007364
Iteration 139/1000 | Loss: 0.00007364
Iteration 140/1000 | Loss: 0.00007364
Iteration 141/1000 | Loss: 0.00007364
Iteration 142/1000 | Loss: 0.00007364
Iteration 143/1000 | Loss: 0.00007364
Iteration 144/1000 | Loss: 0.00007363
Iteration 145/1000 | Loss: 0.00007363
Iteration 146/1000 | Loss: 0.00007363
Iteration 147/1000 | Loss: 0.00007363
Iteration 148/1000 | Loss: 0.00018093
Iteration 149/1000 | Loss: 0.00008505
Iteration 150/1000 | Loss: 0.00007914
Iteration 151/1000 | Loss: 0.00007583
Iteration 152/1000 | Loss: 0.00007418
Iteration 153/1000 | Loss: 0.00007313
Iteration 154/1000 | Loss: 0.00007262
Iteration 155/1000 | Loss: 0.00007239
Iteration 156/1000 | Loss: 0.00007228
Iteration 157/1000 | Loss: 0.00007213
Iteration 158/1000 | Loss: 0.00007212
Iteration 159/1000 | Loss: 0.00007212
Iteration 160/1000 | Loss: 0.00007205
Iteration 161/1000 | Loss: 0.00007205
Iteration 162/1000 | Loss: 0.00007205
Iteration 163/1000 | Loss: 0.00007204
Iteration 164/1000 | Loss: 0.00007204
Iteration 165/1000 | Loss: 0.00007204
Iteration 166/1000 | Loss: 0.00007204
Iteration 167/1000 | Loss: 0.00007204
Iteration 168/1000 | Loss: 0.00007204
Iteration 169/1000 | Loss: 0.00007204
Iteration 170/1000 | Loss: 0.00007204
Iteration 171/1000 | Loss: 0.00007204
Iteration 172/1000 | Loss: 0.00007204
Iteration 173/1000 | Loss: 0.00007203
Iteration 174/1000 | Loss: 0.00007203
Iteration 175/1000 | Loss: 0.00007203
Iteration 176/1000 | Loss: 0.00007202
Iteration 177/1000 | Loss: 0.00007202
Iteration 178/1000 | Loss: 0.00007202
Iteration 179/1000 | Loss: 0.00007202
Iteration 180/1000 | Loss: 0.00007202
Iteration 181/1000 | Loss: 0.00007202
Iteration 182/1000 | Loss: 0.00007202
Iteration 183/1000 | Loss: 0.00007202
Iteration 184/1000 | Loss: 0.00007202
Iteration 185/1000 | Loss: 0.00007202
Iteration 186/1000 | Loss: 0.00007202
Iteration 187/1000 | Loss: 0.00007202
Iteration 188/1000 | Loss: 0.00007202
Iteration 189/1000 | Loss: 0.00007202
Iteration 190/1000 | Loss: 0.00007202
Iteration 191/1000 | Loss: 0.00007202
Iteration 192/1000 | Loss: 0.00007202
Iteration 193/1000 | Loss: 0.00007201
Iteration 194/1000 | Loss: 0.00007201
Iteration 195/1000 | Loss: 0.00007201
Iteration 196/1000 | Loss: 0.00007201
Iteration 197/1000 | Loss: 0.00007201
Iteration 198/1000 | Loss: 0.00007201
Iteration 199/1000 | Loss: 0.00007201
Iteration 200/1000 | Loss: 0.00007201
Iteration 201/1000 | Loss: 0.00007201
Iteration 202/1000 | Loss: 0.00007201
Iteration 203/1000 | Loss: 0.00007201
Iteration 204/1000 | Loss: 0.00007201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [7.201269909273833e-05, 7.201269909273833e-05, 7.201269909273833e-05, 7.201269909273833e-05, 7.201269909273833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.201269909273833e-05

Optimization complete. Final v2v error: 4.8562164306640625 mm

Highest mean error: 23.045753479003906 mm for frame 46

Lowest mean error: 3.057262897491455 mm for frame 6

Saving results

Total time: 209.31942486763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00563941
Iteration 2/25 | Loss: 0.00144436
Iteration 3/25 | Loss: 0.00120532
Iteration 4/25 | Loss: 0.00117684
Iteration 5/25 | Loss: 0.00117067
Iteration 6/25 | Loss: 0.00117360
Iteration 7/25 | Loss: 0.00117055
Iteration 8/25 | Loss: 0.00116720
Iteration 9/25 | Loss: 0.00116613
Iteration 10/25 | Loss: 0.00116585
Iteration 11/25 | Loss: 0.00116582
Iteration 12/25 | Loss: 0.00116581
Iteration 13/25 | Loss: 0.00116581
Iteration 14/25 | Loss: 0.00116579
Iteration 15/25 | Loss: 0.00116579
Iteration 16/25 | Loss: 0.00116569
Iteration 17/25 | Loss: 0.00116560
Iteration 18/25 | Loss: 0.00116558
Iteration 19/25 | Loss: 0.00116558
Iteration 20/25 | Loss: 0.00116558
Iteration 21/25 | Loss: 0.00116558
Iteration 22/25 | Loss: 0.00116556
Iteration 23/25 | Loss: 0.00116555
Iteration 24/25 | Loss: 0.00116555
Iteration 25/25 | Loss: 0.00116544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59741592
Iteration 2/25 | Loss: 0.00097719
Iteration 3/25 | Loss: 0.00097593
Iteration 4/25 | Loss: 0.00097593
Iteration 5/25 | Loss: 0.00097593
Iteration 6/25 | Loss: 0.00097593
Iteration 7/25 | Loss: 0.00097593
Iteration 8/25 | Loss: 0.00097593
Iteration 9/25 | Loss: 0.00097593
Iteration 10/25 | Loss: 0.00097593
Iteration 11/25 | Loss: 0.00097593
Iteration 12/25 | Loss: 0.00097593
Iteration 13/25 | Loss: 0.00097593
Iteration 14/25 | Loss: 0.00097593
Iteration 15/25 | Loss: 0.00097593
Iteration 16/25 | Loss: 0.00097593
Iteration 17/25 | Loss: 0.00097593
Iteration 18/25 | Loss: 0.00097593
Iteration 19/25 | Loss: 0.00097593
Iteration 20/25 | Loss: 0.00097593
Iteration 21/25 | Loss: 0.00097593
Iteration 22/25 | Loss: 0.00097593
Iteration 23/25 | Loss: 0.00097593
Iteration 24/25 | Loss: 0.00097593
Iteration 25/25 | Loss: 0.00097593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097593
Iteration 2/1000 | Loss: 0.00005511
Iteration 3/1000 | Loss: 0.00002687
Iteration 4/1000 | Loss: 0.00002251
Iteration 5/1000 | Loss: 0.00002067
Iteration 6/1000 | Loss: 0.00004697
Iteration 7/1000 | Loss: 0.00003029
Iteration 8/1000 | Loss: 0.00002663
Iteration 9/1000 | Loss: 0.00047484
Iteration 10/1000 | Loss: 0.00031702
Iteration 11/1000 | Loss: 0.00008184
Iteration 12/1000 | Loss: 0.00002137
Iteration 13/1000 | Loss: 0.00002010
Iteration 14/1000 | Loss: 0.00008041
Iteration 15/1000 | Loss: 0.00016756
Iteration 16/1000 | Loss: 0.00002361
Iteration 17/1000 | Loss: 0.00001950
Iteration 18/1000 | Loss: 0.00001807
Iteration 19/1000 | Loss: 0.00001733
Iteration 20/1000 | Loss: 0.00001692
Iteration 21/1000 | Loss: 0.00001660
Iteration 22/1000 | Loss: 0.00001618
Iteration 23/1000 | Loss: 0.00001946
Iteration 24/1000 | Loss: 0.00001795
Iteration 25/1000 | Loss: 0.00001765
Iteration 26/1000 | Loss: 0.00001602
Iteration 27/1000 | Loss: 0.00001563
Iteration 28/1000 | Loss: 0.00001547
Iteration 29/1000 | Loss: 0.00001546
Iteration 30/1000 | Loss: 0.00001544
Iteration 31/1000 | Loss: 0.00001541
Iteration 32/1000 | Loss: 0.00001576
Iteration 33/1000 | Loss: 0.00001506
Iteration 34/1000 | Loss: 0.00001498
Iteration 35/1000 | Loss: 0.00001522
Iteration 36/1000 | Loss: 0.00001466
Iteration 37/1000 | Loss: 0.00001462
Iteration 38/1000 | Loss: 0.00001928
Iteration 39/1000 | Loss: 0.00001497
Iteration 40/1000 | Loss: 0.00001442
Iteration 41/1000 | Loss: 0.00001441
Iteration 42/1000 | Loss: 0.00001439
Iteration 43/1000 | Loss: 0.00001439
Iteration 44/1000 | Loss: 0.00001438
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001433
Iteration 49/1000 | Loss: 0.00001433
Iteration 50/1000 | Loss: 0.00001433
Iteration 51/1000 | Loss: 0.00001433
Iteration 52/1000 | Loss: 0.00001433
Iteration 53/1000 | Loss: 0.00001433
Iteration 54/1000 | Loss: 0.00001433
Iteration 55/1000 | Loss: 0.00001432
Iteration 56/1000 | Loss: 0.00001432
Iteration 57/1000 | Loss: 0.00001432
Iteration 58/1000 | Loss: 0.00001431
Iteration 59/1000 | Loss: 0.00001430
Iteration 60/1000 | Loss: 0.00001430
Iteration 61/1000 | Loss: 0.00001430
Iteration 62/1000 | Loss: 0.00001430
Iteration 63/1000 | Loss: 0.00001429
Iteration 64/1000 | Loss: 0.00001430
Iteration 65/1000 | Loss: 0.00001429
Iteration 66/1000 | Loss: 0.00001429
Iteration 67/1000 | Loss: 0.00001428
Iteration 68/1000 | Loss: 0.00001428
Iteration 69/1000 | Loss: 0.00001428
Iteration 70/1000 | Loss: 0.00001428
Iteration 71/1000 | Loss: 0.00001428
Iteration 72/1000 | Loss: 0.00001428
Iteration 73/1000 | Loss: 0.00001428
Iteration 74/1000 | Loss: 0.00001428
Iteration 75/1000 | Loss: 0.00001428
Iteration 76/1000 | Loss: 0.00001428
Iteration 77/1000 | Loss: 0.00001428
Iteration 78/1000 | Loss: 0.00001428
Iteration 79/1000 | Loss: 0.00001428
Iteration 80/1000 | Loss: 0.00001427
Iteration 81/1000 | Loss: 0.00001427
Iteration 82/1000 | Loss: 0.00001427
Iteration 83/1000 | Loss: 0.00001427
Iteration 84/1000 | Loss: 0.00001427
Iteration 85/1000 | Loss: 0.00001427
Iteration 86/1000 | Loss: 0.00001427
Iteration 87/1000 | Loss: 0.00001427
Iteration 88/1000 | Loss: 0.00001427
Iteration 89/1000 | Loss: 0.00001427
Iteration 90/1000 | Loss: 0.00001427
Iteration 91/1000 | Loss: 0.00001426
Iteration 92/1000 | Loss: 0.00001426
Iteration 93/1000 | Loss: 0.00001426
Iteration 94/1000 | Loss: 0.00001426
Iteration 95/1000 | Loss: 0.00001426
Iteration 96/1000 | Loss: 0.00001426
Iteration 97/1000 | Loss: 0.00001426
Iteration 98/1000 | Loss: 0.00001426
Iteration 99/1000 | Loss: 0.00001426
Iteration 100/1000 | Loss: 0.00001426
Iteration 101/1000 | Loss: 0.00001426
Iteration 102/1000 | Loss: 0.00001425
Iteration 103/1000 | Loss: 0.00001425
Iteration 104/1000 | Loss: 0.00001425
Iteration 105/1000 | Loss: 0.00001425
Iteration 106/1000 | Loss: 0.00001425
Iteration 107/1000 | Loss: 0.00001425
Iteration 108/1000 | Loss: 0.00001425
Iteration 109/1000 | Loss: 0.00001425
Iteration 110/1000 | Loss: 0.00001425
Iteration 111/1000 | Loss: 0.00001425
Iteration 112/1000 | Loss: 0.00001425
Iteration 113/1000 | Loss: 0.00001425
Iteration 114/1000 | Loss: 0.00001425
Iteration 115/1000 | Loss: 0.00001425
Iteration 116/1000 | Loss: 0.00001425
Iteration 117/1000 | Loss: 0.00001425
Iteration 118/1000 | Loss: 0.00001425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.4253049812396057e-05, 1.4253049812396057e-05, 1.4253049812396057e-05, 1.4253049812396057e-05, 1.4253049812396057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4253049812396057e-05

Optimization complete. Final v2v error: 3.30576229095459 mm

Highest mean error: 4.6053972244262695 mm for frame 129

Lowest mean error: 2.860352039337158 mm for frame 1

Saving results

Total time: 92.81950092315674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965663
Iteration 2/25 | Loss: 0.00190810
Iteration 3/25 | Loss: 0.00151780
Iteration 4/25 | Loss: 0.00146965
Iteration 5/25 | Loss: 0.00143285
Iteration 6/25 | Loss: 0.00137648
Iteration 7/25 | Loss: 0.00135811
Iteration 8/25 | Loss: 0.00134181
Iteration 9/25 | Loss: 0.00132353
Iteration 10/25 | Loss: 0.00131838
Iteration 11/25 | Loss: 0.00131698
Iteration 12/25 | Loss: 0.00131668
Iteration 13/25 | Loss: 0.00131850
Iteration 14/25 | Loss: 0.00131113
Iteration 15/25 | Loss: 0.00130971
Iteration 16/25 | Loss: 0.00130898
Iteration 17/25 | Loss: 0.00130875
Iteration 18/25 | Loss: 0.00130803
Iteration 19/25 | Loss: 0.00130175
Iteration 20/25 | Loss: 0.00130114
Iteration 21/25 | Loss: 0.00130110
Iteration 22/25 | Loss: 0.00130110
Iteration 23/25 | Loss: 0.00130110
Iteration 24/25 | Loss: 0.00130110
Iteration 25/25 | Loss: 0.00130110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.32564068
Iteration 2/25 | Loss: 0.00101045
Iteration 3/25 | Loss: 0.00101039
Iteration 4/25 | Loss: 0.00101039
Iteration 5/25 | Loss: 0.00101039
Iteration 6/25 | Loss: 0.00101039
Iteration 7/25 | Loss: 0.00101039
Iteration 8/25 | Loss: 0.00101039
Iteration 9/25 | Loss: 0.00101039
Iteration 10/25 | Loss: 0.00101039
Iteration 11/25 | Loss: 0.00101039
Iteration 12/25 | Loss: 0.00101039
Iteration 13/25 | Loss: 0.00101039
Iteration 14/25 | Loss: 0.00101039
Iteration 15/25 | Loss: 0.00101039
Iteration 16/25 | Loss: 0.00101039
Iteration 17/25 | Loss: 0.00101039
Iteration 18/25 | Loss: 0.00101039
Iteration 19/25 | Loss: 0.00101039
Iteration 20/25 | Loss: 0.00101039
Iteration 21/25 | Loss: 0.00101039
Iteration 22/25 | Loss: 0.00101039
Iteration 23/25 | Loss: 0.00101039
Iteration 24/25 | Loss: 0.00101039
Iteration 25/25 | Loss: 0.00101039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101039
Iteration 2/1000 | Loss: 0.00011483
Iteration 3/1000 | Loss: 0.00018209
Iteration 4/1000 | Loss: 0.00011020
Iteration 5/1000 | Loss: 0.00008585
Iteration 6/1000 | Loss: 0.00007229
Iteration 7/1000 | Loss: 0.00006204
Iteration 8/1000 | Loss: 0.00005922
Iteration 9/1000 | Loss: 0.00008601
Iteration 10/1000 | Loss: 0.00006112
Iteration 11/1000 | Loss: 0.00005787
Iteration 12/1000 | Loss: 0.00005379
Iteration 13/1000 | Loss: 0.00008807
Iteration 14/1000 | Loss: 0.00005336
Iteration 15/1000 | Loss: 0.00004927
Iteration 16/1000 | Loss: 0.00012017
Iteration 17/1000 | Loss: 0.00005607
Iteration 18/1000 | Loss: 0.00008405
Iteration 19/1000 | Loss: 0.00005371
Iteration 20/1000 | Loss: 0.00004853
Iteration 21/1000 | Loss: 0.00004555
Iteration 22/1000 | Loss: 0.00004401
Iteration 23/1000 | Loss: 0.00004267
Iteration 24/1000 | Loss: 0.00004152
Iteration 25/1000 | Loss: 0.00004033
Iteration 26/1000 | Loss: 0.00003896
Iteration 27/1000 | Loss: 0.00003782
Iteration 28/1000 | Loss: 0.00004304
Iteration 29/1000 | Loss: 0.00004024
Iteration 30/1000 | Loss: 0.00003844
Iteration 31/1000 | Loss: 0.00003757
Iteration 32/1000 | Loss: 0.00003685
Iteration 33/1000 | Loss: 0.00003614
Iteration 34/1000 | Loss: 0.00003555
Iteration 35/1000 | Loss: 0.00003524
Iteration 36/1000 | Loss: 0.00003507
Iteration 37/1000 | Loss: 0.00003498
Iteration 38/1000 | Loss: 0.00003491
Iteration 39/1000 | Loss: 0.00003489
Iteration 40/1000 | Loss: 0.00003488
Iteration 41/1000 | Loss: 0.00003487
Iteration 42/1000 | Loss: 0.00003487
Iteration 43/1000 | Loss: 0.00003486
Iteration 44/1000 | Loss: 0.00003486
Iteration 45/1000 | Loss: 0.00003486
Iteration 46/1000 | Loss: 0.00003485
Iteration 47/1000 | Loss: 0.00003485
Iteration 48/1000 | Loss: 0.00003484
Iteration 49/1000 | Loss: 0.00003484
Iteration 50/1000 | Loss: 0.00003483
Iteration 51/1000 | Loss: 0.00003483
Iteration 52/1000 | Loss: 0.00003483
Iteration 53/1000 | Loss: 0.00003482
Iteration 54/1000 | Loss: 0.00003482
Iteration 55/1000 | Loss: 0.00003482
Iteration 56/1000 | Loss: 0.00003482
Iteration 57/1000 | Loss: 0.00003482
Iteration 58/1000 | Loss: 0.00003482
Iteration 59/1000 | Loss: 0.00003482
Iteration 60/1000 | Loss: 0.00003482
Iteration 61/1000 | Loss: 0.00003482
Iteration 62/1000 | Loss: 0.00003482
Iteration 63/1000 | Loss: 0.00003482
Iteration 64/1000 | Loss: 0.00003482
Iteration 65/1000 | Loss: 0.00003482
Iteration 66/1000 | Loss: 0.00003482
Iteration 67/1000 | Loss: 0.00003481
Iteration 68/1000 | Loss: 0.00003481
Iteration 69/1000 | Loss: 0.00003481
Iteration 70/1000 | Loss: 0.00003480
Iteration 71/1000 | Loss: 0.00003480
Iteration 72/1000 | Loss: 0.00003480
Iteration 73/1000 | Loss: 0.00003480
Iteration 74/1000 | Loss: 0.00003480
Iteration 75/1000 | Loss: 0.00003480
Iteration 76/1000 | Loss: 0.00003479
Iteration 77/1000 | Loss: 0.00003479
Iteration 78/1000 | Loss: 0.00003479
Iteration 79/1000 | Loss: 0.00003478
Iteration 80/1000 | Loss: 0.00003478
Iteration 81/1000 | Loss: 0.00003478
Iteration 82/1000 | Loss: 0.00003478
Iteration 83/1000 | Loss: 0.00003478
Iteration 84/1000 | Loss: 0.00003478
Iteration 85/1000 | Loss: 0.00003478
Iteration 86/1000 | Loss: 0.00003478
Iteration 87/1000 | Loss: 0.00003478
Iteration 88/1000 | Loss: 0.00003477
Iteration 89/1000 | Loss: 0.00003477
Iteration 90/1000 | Loss: 0.00003477
Iteration 91/1000 | Loss: 0.00003476
Iteration 92/1000 | Loss: 0.00003476
Iteration 93/1000 | Loss: 0.00003476
Iteration 94/1000 | Loss: 0.00003476
Iteration 95/1000 | Loss: 0.00003476
Iteration 96/1000 | Loss: 0.00003476
Iteration 97/1000 | Loss: 0.00003476
Iteration 98/1000 | Loss: 0.00003476
Iteration 99/1000 | Loss: 0.00003476
Iteration 100/1000 | Loss: 0.00003476
Iteration 101/1000 | Loss: 0.00003476
Iteration 102/1000 | Loss: 0.00003476
Iteration 103/1000 | Loss: 0.00003476
Iteration 104/1000 | Loss: 0.00003475
Iteration 105/1000 | Loss: 0.00003475
Iteration 106/1000 | Loss: 0.00003475
Iteration 107/1000 | Loss: 0.00003475
Iteration 108/1000 | Loss: 0.00003475
Iteration 109/1000 | Loss: 0.00003475
Iteration 110/1000 | Loss: 0.00003475
Iteration 111/1000 | Loss: 0.00003474
Iteration 112/1000 | Loss: 0.00003474
Iteration 113/1000 | Loss: 0.00003474
Iteration 114/1000 | Loss: 0.00003474
Iteration 115/1000 | Loss: 0.00003474
Iteration 116/1000 | Loss: 0.00003474
Iteration 117/1000 | Loss: 0.00003473
Iteration 118/1000 | Loss: 0.00003473
Iteration 119/1000 | Loss: 0.00003473
Iteration 120/1000 | Loss: 0.00003473
Iteration 121/1000 | Loss: 0.00003472
Iteration 122/1000 | Loss: 0.00003472
Iteration 123/1000 | Loss: 0.00003472
Iteration 124/1000 | Loss: 0.00003472
Iteration 125/1000 | Loss: 0.00003472
Iteration 126/1000 | Loss: 0.00003472
Iteration 127/1000 | Loss: 0.00003472
Iteration 128/1000 | Loss: 0.00003471
Iteration 129/1000 | Loss: 0.00003471
Iteration 130/1000 | Loss: 0.00003471
Iteration 131/1000 | Loss: 0.00003471
Iteration 132/1000 | Loss: 0.00003471
Iteration 133/1000 | Loss: 0.00003471
Iteration 134/1000 | Loss: 0.00003471
Iteration 135/1000 | Loss: 0.00003470
Iteration 136/1000 | Loss: 0.00003470
Iteration 137/1000 | Loss: 0.00003469
Iteration 138/1000 | Loss: 0.00003469
Iteration 139/1000 | Loss: 0.00003469
Iteration 140/1000 | Loss: 0.00003468
Iteration 141/1000 | Loss: 0.00003468
Iteration 142/1000 | Loss: 0.00003468
Iteration 143/1000 | Loss: 0.00003468
Iteration 144/1000 | Loss: 0.00003468
Iteration 145/1000 | Loss: 0.00003468
Iteration 146/1000 | Loss: 0.00003467
Iteration 147/1000 | Loss: 0.00003467
Iteration 148/1000 | Loss: 0.00003466
Iteration 149/1000 | Loss: 0.00003466
Iteration 150/1000 | Loss: 0.00003465
Iteration 151/1000 | Loss: 0.00003465
Iteration 152/1000 | Loss: 0.00003465
Iteration 153/1000 | Loss: 0.00003464
Iteration 154/1000 | Loss: 0.00003463
Iteration 155/1000 | Loss: 0.00003463
Iteration 156/1000 | Loss: 0.00003461
Iteration 157/1000 | Loss: 0.00003461
Iteration 158/1000 | Loss: 0.00003460
Iteration 159/1000 | Loss: 0.00003460
Iteration 160/1000 | Loss: 0.00003459
Iteration 161/1000 | Loss: 0.00003459
Iteration 162/1000 | Loss: 0.00003458
Iteration 163/1000 | Loss: 0.00003458
Iteration 164/1000 | Loss: 0.00003457
Iteration 165/1000 | Loss: 0.00003457
Iteration 166/1000 | Loss: 0.00003457
Iteration 167/1000 | Loss: 0.00003456
Iteration 168/1000 | Loss: 0.00003456
Iteration 169/1000 | Loss: 0.00003455
Iteration 170/1000 | Loss: 0.00003455
Iteration 171/1000 | Loss: 0.00003454
Iteration 172/1000 | Loss: 0.00003454
Iteration 173/1000 | Loss: 0.00003454
Iteration 174/1000 | Loss: 0.00003454
Iteration 175/1000 | Loss: 0.00003453
Iteration 176/1000 | Loss: 0.00003453
Iteration 177/1000 | Loss: 0.00003453
Iteration 178/1000 | Loss: 0.00003453
Iteration 179/1000 | Loss: 0.00003453
Iteration 180/1000 | Loss: 0.00003453
Iteration 181/1000 | Loss: 0.00003453
Iteration 182/1000 | Loss: 0.00003453
Iteration 183/1000 | Loss: 0.00003452
Iteration 184/1000 | Loss: 0.00003452
Iteration 185/1000 | Loss: 0.00003452
Iteration 186/1000 | Loss: 0.00003452
Iteration 187/1000 | Loss: 0.00003452
Iteration 188/1000 | Loss: 0.00003452
Iteration 189/1000 | Loss: 0.00003452
Iteration 190/1000 | Loss: 0.00003452
Iteration 191/1000 | Loss: 0.00003452
Iteration 192/1000 | Loss: 0.00003452
Iteration 193/1000 | Loss: 0.00003452
Iteration 194/1000 | Loss: 0.00003452
Iteration 195/1000 | Loss: 0.00003452
Iteration 196/1000 | Loss: 0.00003451
Iteration 197/1000 | Loss: 0.00003451
Iteration 198/1000 | Loss: 0.00003451
Iteration 199/1000 | Loss: 0.00003451
Iteration 200/1000 | Loss: 0.00003451
Iteration 201/1000 | Loss: 0.00003451
Iteration 202/1000 | Loss: 0.00003451
Iteration 203/1000 | Loss: 0.00003451
Iteration 204/1000 | Loss: 0.00003451
Iteration 205/1000 | Loss: 0.00003451
Iteration 206/1000 | Loss: 0.00003451
Iteration 207/1000 | Loss: 0.00003451
Iteration 208/1000 | Loss: 0.00003451
Iteration 209/1000 | Loss: 0.00003450
Iteration 210/1000 | Loss: 0.00003450
Iteration 211/1000 | Loss: 0.00003450
Iteration 212/1000 | Loss: 0.00003450
Iteration 213/1000 | Loss: 0.00003450
Iteration 214/1000 | Loss: 0.00003450
Iteration 215/1000 | Loss: 0.00003450
Iteration 216/1000 | Loss: 0.00003450
Iteration 217/1000 | Loss: 0.00003450
Iteration 218/1000 | Loss: 0.00003449
Iteration 219/1000 | Loss: 0.00003449
Iteration 220/1000 | Loss: 0.00003449
Iteration 221/1000 | Loss: 0.00003449
Iteration 222/1000 | Loss: 0.00003449
Iteration 223/1000 | Loss: 0.00003448
Iteration 224/1000 | Loss: 0.00003448
Iteration 225/1000 | Loss: 0.00003448
Iteration 226/1000 | Loss: 0.00003448
Iteration 227/1000 | Loss: 0.00003448
Iteration 228/1000 | Loss: 0.00003448
Iteration 229/1000 | Loss: 0.00003448
Iteration 230/1000 | Loss: 0.00003448
Iteration 231/1000 | Loss: 0.00003447
Iteration 232/1000 | Loss: 0.00003447
Iteration 233/1000 | Loss: 0.00003447
Iteration 234/1000 | Loss: 0.00003447
Iteration 235/1000 | Loss: 0.00003447
Iteration 236/1000 | Loss: 0.00003447
Iteration 237/1000 | Loss: 0.00003447
Iteration 238/1000 | Loss: 0.00003447
Iteration 239/1000 | Loss: 0.00003447
Iteration 240/1000 | Loss: 0.00003447
Iteration 241/1000 | Loss: 0.00003446
Iteration 242/1000 | Loss: 0.00003446
Iteration 243/1000 | Loss: 0.00003446
Iteration 244/1000 | Loss: 0.00003446
Iteration 245/1000 | Loss: 0.00003446
Iteration 246/1000 | Loss: 0.00003446
Iteration 247/1000 | Loss: 0.00003446
Iteration 248/1000 | Loss: 0.00003445
Iteration 249/1000 | Loss: 0.00003445
Iteration 250/1000 | Loss: 0.00003445
Iteration 251/1000 | Loss: 0.00003445
Iteration 252/1000 | Loss: 0.00003445
Iteration 253/1000 | Loss: 0.00003445
Iteration 254/1000 | Loss: 0.00003444
Iteration 255/1000 | Loss: 0.00003444
Iteration 256/1000 | Loss: 0.00003444
Iteration 257/1000 | Loss: 0.00003444
Iteration 258/1000 | Loss: 0.00003444
Iteration 259/1000 | Loss: 0.00003444
Iteration 260/1000 | Loss: 0.00003444
Iteration 261/1000 | Loss: 0.00003444
Iteration 262/1000 | Loss: 0.00003444
Iteration 263/1000 | Loss: 0.00003444
Iteration 264/1000 | Loss: 0.00003444
Iteration 265/1000 | Loss: 0.00003444
Iteration 266/1000 | Loss: 0.00003444
Iteration 267/1000 | Loss: 0.00003444
Iteration 268/1000 | Loss: 0.00003443
Iteration 269/1000 | Loss: 0.00003443
Iteration 270/1000 | Loss: 0.00003443
Iteration 271/1000 | Loss: 0.00003443
Iteration 272/1000 | Loss: 0.00003443
Iteration 273/1000 | Loss: 0.00003443
Iteration 274/1000 | Loss: 0.00003443
Iteration 275/1000 | Loss: 0.00003443
Iteration 276/1000 | Loss: 0.00003443
Iteration 277/1000 | Loss: 0.00003443
Iteration 278/1000 | Loss: 0.00003443
Iteration 279/1000 | Loss: 0.00003443
Iteration 280/1000 | Loss: 0.00003443
Iteration 281/1000 | Loss: 0.00003443
Iteration 282/1000 | Loss: 0.00003443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 282. Stopping optimization.
Last 5 losses: [3.4433578548487276e-05, 3.4433578548487276e-05, 3.4433578548487276e-05, 3.4433578548487276e-05, 3.4433578548487276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4433578548487276e-05

Optimization complete. Final v2v error: 4.620429039001465 mm

Highest mean error: 7.86731481552124 mm for frame 116

Lowest mean error: 3.5647857189178467 mm for frame 1

Saving results

Total time: 110.07336163520813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00709128
Iteration 2/25 | Loss: 0.00141412
Iteration 3/25 | Loss: 0.00122454
Iteration 4/25 | Loss: 0.00120834
Iteration 5/25 | Loss: 0.00120543
Iteration 6/25 | Loss: 0.00120502
Iteration 7/25 | Loss: 0.00120502
Iteration 8/25 | Loss: 0.00120502
Iteration 9/25 | Loss: 0.00120502
Iteration 10/25 | Loss: 0.00120502
Iteration 11/25 | Loss: 0.00120502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012050241930410266, 0.0012050241930410266, 0.0012050241930410266, 0.0012050241930410266, 0.0012050241930410266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012050241930410266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63825965
Iteration 2/25 | Loss: 0.00084430
Iteration 3/25 | Loss: 0.00084428
Iteration 4/25 | Loss: 0.00084427
Iteration 5/25 | Loss: 0.00084427
Iteration 6/25 | Loss: 0.00084427
Iteration 7/25 | Loss: 0.00084427
Iteration 8/25 | Loss: 0.00084427
Iteration 9/25 | Loss: 0.00084427
Iteration 10/25 | Loss: 0.00084427
Iteration 11/25 | Loss: 0.00084427
Iteration 12/25 | Loss: 0.00084427
Iteration 13/25 | Loss: 0.00084427
Iteration 14/25 | Loss: 0.00084427
Iteration 15/25 | Loss: 0.00084427
Iteration 16/25 | Loss: 0.00084427
Iteration 17/25 | Loss: 0.00084427
Iteration 18/25 | Loss: 0.00084427
Iteration 19/25 | Loss: 0.00084427
Iteration 20/25 | Loss: 0.00084427
Iteration 21/25 | Loss: 0.00084427
Iteration 22/25 | Loss: 0.00084427
Iteration 23/25 | Loss: 0.00084427
Iteration 24/25 | Loss: 0.00084427
Iteration 25/25 | Loss: 0.00084427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084427
Iteration 2/1000 | Loss: 0.00004945
Iteration 3/1000 | Loss: 0.00002887
Iteration 4/1000 | Loss: 0.00002456
Iteration 5/1000 | Loss: 0.00002270
Iteration 6/1000 | Loss: 0.00002178
Iteration 7/1000 | Loss: 0.00002104
Iteration 8/1000 | Loss: 0.00002045
Iteration 9/1000 | Loss: 0.00002000
Iteration 10/1000 | Loss: 0.00001965
Iteration 11/1000 | Loss: 0.00001938
Iteration 12/1000 | Loss: 0.00001933
Iteration 13/1000 | Loss: 0.00001929
Iteration 14/1000 | Loss: 0.00001928
Iteration 15/1000 | Loss: 0.00001926
Iteration 16/1000 | Loss: 0.00001926
Iteration 17/1000 | Loss: 0.00001925
Iteration 18/1000 | Loss: 0.00001925
Iteration 19/1000 | Loss: 0.00001924
Iteration 20/1000 | Loss: 0.00001924
Iteration 21/1000 | Loss: 0.00001924
Iteration 22/1000 | Loss: 0.00001922
Iteration 23/1000 | Loss: 0.00001921
Iteration 24/1000 | Loss: 0.00001919
Iteration 25/1000 | Loss: 0.00001918
Iteration 26/1000 | Loss: 0.00001918
Iteration 27/1000 | Loss: 0.00001913
Iteration 28/1000 | Loss: 0.00001913
Iteration 29/1000 | Loss: 0.00001912
Iteration 30/1000 | Loss: 0.00001910
Iteration 31/1000 | Loss: 0.00001907
Iteration 32/1000 | Loss: 0.00001906
Iteration 33/1000 | Loss: 0.00001905
Iteration 34/1000 | Loss: 0.00001905
Iteration 35/1000 | Loss: 0.00001904
Iteration 36/1000 | Loss: 0.00001903
Iteration 37/1000 | Loss: 0.00001903
Iteration 38/1000 | Loss: 0.00001902
Iteration 39/1000 | Loss: 0.00001902
Iteration 40/1000 | Loss: 0.00001902
Iteration 41/1000 | Loss: 0.00001901
Iteration 42/1000 | Loss: 0.00001901
Iteration 43/1000 | Loss: 0.00001901
Iteration 44/1000 | Loss: 0.00001900
Iteration 45/1000 | Loss: 0.00001900
Iteration 46/1000 | Loss: 0.00001899
Iteration 47/1000 | Loss: 0.00001898
Iteration 48/1000 | Loss: 0.00001898
Iteration 49/1000 | Loss: 0.00001898
Iteration 50/1000 | Loss: 0.00001896
Iteration 51/1000 | Loss: 0.00001896
Iteration 52/1000 | Loss: 0.00001896
Iteration 53/1000 | Loss: 0.00001896
Iteration 54/1000 | Loss: 0.00001895
Iteration 55/1000 | Loss: 0.00001893
Iteration 56/1000 | Loss: 0.00001893
Iteration 57/1000 | Loss: 0.00001893
Iteration 58/1000 | Loss: 0.00001893
Iteration 59/1000 | Loss: 0.00001893
Iteration 60/1000 | Loss: 0.00001893
Iteration 61/1000 | Loss: 0.00001892
Iteration 62/1000 | Loss: 0.00001892
Iteration 63/1000 | Loss: 0.00001892
Iteration 64/1000 | Loss: 0.00001892
Iteration 65/1000 | Loss: 0.00001892
Iteration 66/1000 | Loss: 0.00001892
Iteration 67/1000 | Loss: 0.00001892
Iteration 68/1000 | Loss: 0.00001892
Iteration 69/1000 | Loss: 0.00001891
Iteration 70/1000 | Loss: 0.00001890
Iteration 71/1000 | Loss: 0.00001890
Iteration 72/1000 | Loss: 0.00001890
Iteration 73/1000 | Loss: 0.00001889
Iteration 74/1000 | Loss: 0.00001889
Iteration 75/1000 | Loss: 0.00001889
Iteration 76/1000 | Loss: 0.00001889
Iteration 77/1000 | Loss: 0.00001888
Iteration 78/1000 | Loss: 0.00001888
Iteration 79/1000 | Loss: 0.00001888
Iteration 80/1000 | Loss: 0.00001887
Iteration 81/1000 | Loss: 0.00001887
Iteration 82/1000 | Loss: 0.00001887
Iteration 83/1000 | Loss: 0.00001887
Iteration 84/1000 | Loss: 0.00001887
Iteration 85/1000 | Loss: 0.00001887
Iteration 86/1000 | Loss: 0.00001886
Iteration 87/1000 | Loss: 0.00001886
Iteration 88/1000 | Loss: 0.00001886
Iteration 89/1000 | Loss: 0.00001886
Iteration 90/1000 | Loss: 0.00001886
Iteration 91/1000 | Loss: 0.00001886
Iteration 92/1000 | Loss: 0.00001885
Iteration 93/1000 | Loss: 0.00001885
Iteration 94/1000 | Loss: 0.00001885
Iteration 95/1000 | Loss: 0.00001885
Iteration 96/1000 | Loss: 0.00001885
Iteration 97/1000 | Loss: 0.00001885
Iteration 98/1000 | Loss: 0.00001884
Iteration 99/1000 | Loss: 0.00001884
Iteration 100/1000 | Loss: 0.00001884
Iteration 101/1000 | Loss: 0.00001884
Iteration 102/1000 | Loss: 0.00001884
Iteration 103/1000 | Loss: 0.00001884
Iteration 104/1000 | Loss: 0.00001884
Iteration 105/1000 | Loss: 0.00001883
Iteration 106/1000 | Loss: 0.00001883
Iteration 107/1000 | Loss: 0.00001883
Iteration 108/1000 | Loss: 0.00001883
Iteration 109/1000 | Loss: 0.00001883
Iteration 110/1000 | Loss: 0.00001883
Iteration 111/1000 | Loss: 0.00001883
Iteration 112/1000 | Loss: 0.00001883
Iteration 113/1000 | Loss: 0.00001883
Iteration 114/1000 | Loss: 0.00001883
Iteration 115/1000 | Loss: 0.00001883
Iteration 116/1000 | Loss: 0.00001883
Iteration 117/1000 | Loss: 0.00001883
Iteration 118/1000 | Loss: 0.00001883
Iteration 119/1000 | Loss: 0.00001883
Iteration 120/1000 | Loss: 0.00001883
Iteration 121/1000 | Loss: 0.00001883
Iteration 122/1000 | Loss: 0.00001882
Iteration 123/1000 | Loss: 0.00001882
Iteration 124/1000 | Loss: 0.00001882
Iteration 125/1000 | Loss: 0.00001882
Iteration 126/1000 | Loss: 0.00001882
Iteration 127/1000 | Loss: 0.00001882
Iteration 128/1000 | Loss: 0.00001882
Iteration 129/1000 | Loss: 0.00001882
Iteration 130/1000 | Loss: 0.00001882
Iteration 131/1000 | Loss: 0.00001882
Iteration 132/1000 | Loss: 0.00001882
Iteration 133/1000 | Loss: 0.00001882
Iteration 134/1000 | Loss: 0.00001882
Iteration 135/1000 | Loss: 0.00001882
Iteration 136/1000 | Loss: 0.00001882
Iteration 137/1000 | Loss: 0.00001881
Iteration 138/1000 | Loss: 0.00001881
Iteration 139/1000 | Loss: 0.00001881
Iteration 140/1000 | Loss: 0.00001881
Iteration 141/1000 | Loss: 0.00001881
Iteration 142/1000 | Loss: 0.00001881
Iteration 143/1000 | Loss: 0.00001881
Iteration 144/1000 | Loss: 0.00001881
Iteration 145/1000 | Loss: 0.00001881
Iteration 146/1000 | Loss: 0.00001881
Iteration 147/1000 | Loss: 0.00001881
Iteration 148/1000 | Loss: 0.00001881
Iteration 149/1000 | Loss: 0.00001881
Iteration 150/1000 | Loss: 0.00001881
Iteration 151/1000 | Loss: 0.00001881
Iteration 152/1000 | Loss: 0.00001881
Iteration 153/1000 | Loss: 0.00001881
Iteration 154/1000 | Loss: 0.00001881
Iteration 155/1000 | Loss: 0.00001881
Iteration 156/1000 | Loss: 0.00001881
Iteration 157/1000 | Loss: 0.00001881
Iteration 158/1000 | Loss: 0.00001881
Iteration 159/1000 | Loss: 0.00001881
Iteration 160/1000 | Loss: 0.00001881
Iteration 161/1000 | Loss: 0.00001881
Iteration 162/1000 | Loss: 0.00001881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.8810991605278105e-05, 1.8810991605278105e-05, 1.8810991605278105e-05, 1.8810991605278105e-05, 1.8810991605278105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8810991605278105e-05

Optimization complete. Final v2v error: 3.716994047164917 mm

Highest mean error: 4.242640495300293 mm for frame 197

Lowest mean error: 3.3629913330078125 mm for frame 4

Saving results

Total time: 38.600436210632324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108599
Iteration 2/25 | Loss: 0.01108599
Iteration 3/25 | Loss: 0.00185663
Iteration 4/25 | Loss: 0.00155333
Iteration 5/25 | Loss: 0.00139204
Iteration 6/25 | Loss: 0.00122316
Iteration 7/25 | Loss: 0.00118189
Iteration 8/25 | Loss: 0.00115335
Iteration 9/25 | Loss: 0.00113065
Iteration 10/25 | Loss: 0.00113429
Iteration 11/25 | Loss: 0.00115157
Iteration 12/25 | Loss: 0.00112272
Iteration 13/25 | Loss: 0.00113098
Iteration 14/25 | Loss: 0.00112893
Iteration 15/25 | Loss: 0.00112967
Iteration 16/25 | Loss: 0.00111779
Iteration 17/25 | Loss: 0.00111694
Iteration 18/25 | Loss: 0.00111688
Iteration 19/25 | Loss: 0.00111584
Iteration 20/25 | Loss: 0.00111583
Iteration 21/25 | Loss: 0.00111556
Iteration 22/25 | Loss: 0.00111522
Iteration 23/25 | Loss: 0.00111519
Iteration 24/25 | Loss: 0.00111519
Iteration 25/25 | Loss: 0.00111518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41159141
Iteration 2/25 | Loss: 0.00146895
Iteration 3/25 | Loss: 0.00146895
Iteration 4/25 | Loss: 0.00118918
Iteration 5/25 | Loss: 0.00118914
Iteration 6/25 | Loss: 0.00118914
Iteration 7/25 | Loss: 0.00118914
Iteration 8/25 | Loss: 0.00118914
Iteration 9/25 | Loss: 0.00118914
Iteration 10/25 | Loss: 0.00118914
Iteration 11/25 | Loss: 0.00118914
Iteration 12/25 | Loss: 0.00118914
Iteration 13/25 | Loss: 0.00118914
Iteration 14/25 | Loss: 0.00118914
Iteration 15/25 | Loss: 0.00118914
Iteration 16/25 | Loss: 0.00118914
Iteration 17/25 | Loss: 0.00118914
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011891431640833616, 0.0011891431640833616, 0.0011891431640833616, 0.0011891431640833616, 0.0011891431640833616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011891431640833616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118914
Iteration 2/1000 | Loss: 0.00031009
Iteration 3/1000 | Loss: 0.00005284
Iteration 4/1000 | Loss: 0.00014353
Iteration 5/1000 | Loss: 0.00004119
Iteration 6/1000 | Loss: 0.00003912
Iteration 7/1000 | Loss: 0.00003752
Iteration 8/1000 | Loss: 0.00011526
Iteration 9/1000 | Loss: 0.00017096
Iteration 10/1000 | Loss: 0.00004036
Iteration 11/1000 | Loss: 0.00009508
Iteration 12/1000 | Loss: 0.00008795
Iteration 13/1000 | Loss: 0.00029960
Iteration 14/1000 | Loss: 0.00003450
Iteration 15/1000 | Loss: 0.00003425
Iteration 16/1000 | Loss: 0.00003394
Iteration 17/1000 | Loss: 0.00061906
Iteration 18/1000 | Loss: 0.00052916
Iteration 19/1000 | Loss: 0.00004962
Iteration 20/1000 | Loss: 0.00003835
Iteration 21/1000 | Loss: 0.00016879
Iteration 22/1000 | Loss: 0.00005059
Iteration 23/1000 | Loss: 0.00004836
Iteration 24/1000 | Loss: 0.00003152
Iteration 25/1000 | Loss: 0.00003006
Iteration 26/1000 | Loss: 0.00002937
Iteration 27/1000 | Loss: 0.00003024
Iteration 28/1000 | Loss: 0.00010452
Iteration 29/1000 | Loss: 0.00014585
Iteration 30/1000 | Loss: 0.00009262
Iteration 31/1000 | Loss: 0.00002852
Iteration 32/1000 | Loss: 0.00002808
Iteration 33/1000 | Loss: 0.00002807
Iteration 34/1000 | Loss: 0.00021567
Iteration 35/1000 | Loss: 0.00003796
Iteration 36/1000 | Loss: 0.00006942
Iteration 37/1000 | Loss: 0.00006466
Iteration 38/1000 | Loss: 0.00003441
Iteration 39/1000 | Loss: 0.00003063
Iteration 40/1000 | Loss: 0.00006211
Iteration 41/1000 | Loss: 0.00003890
Iteration 42/1000 | Loss: 0.00002825
Iteration 43/1000 | Loss: 0.00002782
Iteration 44/1000 | Loss: 0.00002791
Iteration 45/1000 | Loss: 0.00002758
Iteration 46/1000 | Loss: 0.00002758
Iteration 47/1000 | Loss: 0.00002758
Iteration 48/1000 | Loss: 0.00002752
Iteration 49/1000 | Loss: 0.00002742
Iteration 50/1000 | Loss: 0.00002726
Iteration 51/1000 | Loss: 0.00002752
Iteration 52/1000 | Loss: 0.00014607
Iteration 53/1000 | Loss: 0.00007036
Iteration 54/1000 | Loss: 0.00002797
Iteration 55/1000 | Loss: 0.00002734
Iteration 56/1000 | Loss: 0.00002713
Iteration 57/1000 | Loss: 0.00002713
Iteration 58/1000 | Loss: 0.00002735
Iteration 59/1000 | Loss: 0.00002734
Iteration 60/1000 | Loss: 0.00002718
Iteration 61/1000 | Loss: 0.00002712
Iteration 62/1000 | Loss: 0.00002712
Iteration 63/1000 | Loss: 0.00002712
Iteration 64/1000 | Loss: 0.00002712
Iteration 65/1000 | Loss: 0.00002712
Iteration 66/1000 | Loss: 0.00002711
Iteration 67/1000 | Loss: 0.00002711
Iteration 68/1000 | Loss: 0.00002711
Iteration 69/1000 | Loss: 0.00002711
Iteration 70/1000 | Loss: 0.00002711
Iteration 71/1000 | Loss: 0.00002711
Iteration 72/1000 | Loss: 0.00002711
Iteration 73/1000 | Loss: 0.00002710
Iteration 74/1000 | Loss: 0.00002733
Iteration 75/1000 | Loss: 0.00002715
Iteration 76/1000 | Loss: 0.00002712
Iteration 77/1000 | Loss: 0.00002712
Iteration 78/1000 | Loss: 0.00002711
Iteration 79/1000 | Loss: 0.00002711
Iteration 80/1000 | Loss: 0.00002711
Iteration 81/1000 | Loss: 0.00002710
Iteration 82/1000 | Loss: 0.00002710
Iteration 83/1000 | Loss: 0.00002710
Iteration 84/1000 | Loss: 0.00002710
Iteration 85/1000 | Loss: 0.00002710
Iteration 86/1000 | Loss: 0.00002710
Iteration 87/1000 | Loss: 0.00002709
Iteration 88/1000 | Loss: 0.00002709
Iteration 89/1000 | Loss: 0.00002709
Iteration 90/1000 | Loss: 0.00002709
Iteration 91/1000 | Loss: 0.00002732
Iteration 92/1000 | Loss: 0.00002732
Iteration 93/1000 | Loss: 0.00002715
Iteration 94/1000 | Loss: 0.00002715
Iteration 95/1000 | Loss: 0.00002715
Iteration 96/1000 | Loss: 0.00002709
Iteration 97/1000 | Loss: 0.00002709
Iteration 98/1000 | Loss: 0.00002709
Iteration 99/1000 | Loss: 0.00002709
Iteration 100/1000 | Loss: 0.00002709
Iteration 101/1000 | Loss: 0.00002709
Iteration 102/1000 | Loss: 0.00002733
Iteration 103/1000 | Loss: 0.00002733
Iteration 104/1000 | Loss: 0.00002718
Iteration 105/1000 | Loss: 0.00002718
Iteration 106/1000 | Loss: 0.00002717
Iteration 107/1000 | Loss: 0.00002734
Iteration 108/1000 | Loss: 0.00002716
Iteration 109/1000 | Loss: 0.00002709
Iteration 110/1000 | Loss: 0.00002708
Iteration 111/1000 | Loss: 0.00002732
Iteration 112/1000 | Loss: 0.00002732
Iteration 113/1000 | Loss: 0.00002716
Iteration 114/1000 | Loss: 0.00002716
Iteration 115/1000 | Loss: 0.00002715
Iteration 116/1000 | Loss: 0.00002708
Iteration 117/1000 | Loss: 0.00002708
Iteration 118/1000 | Loss: 0.00002731
Iteration 119/1000 | Loss: 0.00002715
Iteration 120/1000 | Loss: 0.00002715
Iteration 121/1000 | Loss: 0.00002712
Iteration 122/1000 | Loss: 0.00002710
Iteration 123/1000 | Loss: 0.00002710
Iteration 124/1000 | Loss: 0.00002710
Iteration 125/1000 | Loss: 0.00002710
Iteration 126/1000 | Loss: 0.00002709
Iteration 127/1000 | Loss: 0.00002709
Iteration 128/1000 | Loss: 0.00002709
Iteration 129/1000 | Loss: 0.00002709
Iteration 130/1000 | Loss: 0.00002708
Iteration 131/1000 | Loss: 0.00002730
Iteration 132/1000 | Loss: 0.00002716
Iteration 133/1000 | Loss: 0.00002729
Iteration 134/1000 | Loss: 0.00002715
Iteration 135/1000 | Loss: 0.00002715
Iteration 136/1000 | Loss: 0.00002708
Iteration 137/1000 | Loss: 0.00002727
Iteration 138/1000 | Loss: 0.00002715
Iteration 139/1000 | Loss: 0.00002709
Iteration 140/1000 | Loss: 0.00002709
Iteration 141/1000 | Loss: 0.00002709
Iteration 142/1000 | Loss: 0.00002709
Iteration 143/1000 | Loss: 0.00002709
Iteration 144/1000 | Loss: 0.00002708
Iteration 145/1000 | Loss: 0.00002708
Iteration 146/1000 | Loss: 0.00002708
Iteration 147/1000 | Loss: 0.00002708
Iteration 148/1000 | Loss: 0.00002727
Iteration 149/1000 | Loss: 0.00002714
Iteration 150/1000 | Loss: 0.00002713
Iteration 151/1000 | Loss: 0.00002713
Iteration 152/1000 | Loss: 0.00002712
Iteration 153/1000 | Loss: 0.00002712
Iteration 154/1000 | Loss: 0.00002711
Iteration 155/1000 | Loss: 0.00002711
Iteration 156/1000 | Loss: 0.00002711
Iteration 157/1000 | Loss: 0.00002711
Iteration 158/1000 | Loss: 0.00002711
Iteration 159/1000 | Loss: 0.00002711
Iteration 160/1000 | Loss: 0.00002711
Iteration 161/1000 | Loss: 0.00002711
Iteration 162/1000 | Loss: 0.00002711
Iteration 163/1000 | Loss: 0.00002711
Iteration 164/1000 | Loss: 0.00002711
Iteration 165/1000 | Loss: 0.00002710
Iteration 166/1000 | Loss: 0.00002710
Iteration 167/1000 | Loss: 0.00002710
Iteration 168/1000 | Loss: 0.00002710
Iteration 169/1000 | Loss: 0.00002708
Iteration 170/1000 | Loss: 0.00002708
Iteration 171/1000 | Loss: 0.00002727
Iteration 172/1000 | Loss: 0.00002713
Iteration 173/1000 | Loss: 0.00002712
Iteration 174/1000 | Loss: 0.00002709
Iteration 175/1000 | Loss: 0.00002709
Iteration 176/1000 | Loss: 0.00002707
Iteration 177/1000 | Loss: 0.00002726
Iteration 178/1000 | Loss: 0.00002726
Iteration 179/1000 | Loss: 0.00002712
Iteration 180/1000 | Loss: 0.00002712
Iteration 181/1000 | Loss: 0.00002708
Iteration 182/1000 | Loss: 0.00002708
Iteration 183/1000 | Loss: 0.00002708
Iteration 184/1000 | Loss: 0.00002707
Iteration 185/1000 | Loss: 0.00002707
Iteration 186/1000 | Loss: 0.00002707
Iteration 187/1000 | Loss: 0.00002725
Iteration 188/1000 | Loss: 0.00002725
Iteration 189/1000 | Loss: 0.00002712
Iteration 190/1000 | Loss: 0.00002711
Iteration 191/1000 | Loss: 0.00002707
Iteration 192/1000 | Loss: 0.00002707
Iteration 193/1000 | Loss: 0.00002707
Iteration 194/1000 | Loss: 0.00002707
Iteration 195/1000 | Loss: 0.00002725
Iteration 196/1000 | Loss: 0.00002725
Iteration 197/1000 | Loss: 0.00002712
Iteration 198/1000 | Loss: 0.00002725
Iteration 199/1000 | Loss: 0.00002712
Iteration 200/1000 | Loss: 0.00002711
Iteration 201/1000 | Loss: 0.00002709
Iteration 202/1000 | Loss: 0.00002725
Iteration 203/1000 | Loss: 0.00002711
Iteration 204/1000 | Loss: 0.00002711
Iteration 205/1000 | Loss: 0.00002711
Iteration 206/1000 | Loss: 0.00002710
Iteration 207/1000 | Loss: 0.00002710
Iteration 208/1000 | Loss: 0.00002709
Iteration 209/1000 | Loss: 0.00002709
Iteration 210/1000 | Loss: 0.00002708
Iteration 211/1000 | Loss: 0.00002707
Iteration 212/1000 | Loss: 0.00002725
Iteration 213/1000 | Loss: 0.00002725
Iteration 214/1000 | Loss: 0.00002710
Iteration 215/1000 | Loss: 0.00002710
Iteration 216/1000 | Loss: 0.00002708
Iteration 217/1000 | Loss: 0.00002707
Iteration 218/1000 | Loss: 0.00002707
Iteration 219/1000 | Loss: 0.00002707
Iteration 220/1000 | Loss: 0.00002707
Iteration 221/1000 | Loss: 0.00002707
Iteration 222/1000 | Loss: 0.00002724
Iteration 223/1000 | Loss: 0.00002724
Iteration 224/1000 | Loss: 0.00002710
Iteration 225/1000 | Loss: 0.00002710
Iteration 226/1000 | Loss: 0.00002710
Iteration 227/1000 | Loss: 0.00002709
Iteration 228/1000 | Loss: 0.00002709
Iteration 229/1000 | Loss: 0.00002709
Iteration 230/1000 | Loss: 0.00002708
Iteration 231/1000 | Loss: 0.00002708
Iteration 232/1000 | Loss: 0.00002708
Iteration 233/1000 | Loss: 0.00002708
Iteration 234/1000 | Loss: 0.00002708
Iteration 235/1000 | Loss: 0.00002708
Iteration 236/1000 | Loss: 0.00002708
Iteration 237/1000 | Loss: 0.00002708
Iteration 238/1000 | Loss: 0.00002708
Iteration 239/1000 | Loss: 0.00002708
Iteration 240/1000 | Loss: 0.00002708
Iteration 241/1000 | Loss: 0.00002707
Iteration 242/1000 | Loss: 0.00002707
Iteration 243/1000 | Loss: 0.00002707
Iteration 244/1000 | Loss: 0.00002707
Iteration 245/1000 | Loss: 0.00002707
Iteration 246/1000 | Loss: 0.00002707
Iteration 247/1000 | Loss: 0.00002707
Iteration 248/1000 | Loss: 0.00002707
Iteration 249/1000 | Loss: 0.00002707
Iteration 250/1000 | Loss: 0.00002707
Iteration 251/1000 | Loss: 0.00002707
Iteration 252/1000 | Loss: 0.00002707
Iteration 253/1000 | Loss: 0.00002707
Iteration 254/1000 | Loss: 0.00002707
Iteration 255/1000 | Loss: 0.00002723
Iteration 256/1000 | Loss: 0.00002711
Iteration 257/1000 | Loss: 0.00002711
Iteration 258/1000 | Loss: 0.00002707
Iteration 259/1000 | Loss: 0.00002707
Iteration 260/1000 | Loss: 0.00002707
Iteration 261/1000 | Loss: 0.00002706
Iteration 262/1000 | Loss: 0.00002722
Iteration 263/1000 | Loss: 0.00002710
Iteration 264/1000 | Loss: 0.00002709
Iteration 265/1000 | Loss: 0.00002708
Iteration 266/1000 | Loss: 0.00002707
Iteration 267/1000 | Loss: 0.00002707
Iteration 268/1000 | Loss: 0.00002707
Iteration 269/1000 | Loss: 0.00002707
Iteration 270/1000 | Loss: 0.00002722
Iteration 271/1000 | Loss: 0.00002709
Iteration 272/1000 | Loss: 0.00002709
Iteration 273/1000 | Loss: 0.00002707
Iteration 274/1000 | Loss: 0.00002707
Iteration 275/1000 | Loss: 0.00002707
Iteration 276/1000 | Loss: 0.00002707
Iteration 277/1000 | Loss: 0.00002706
Iteration 278/1000 | Loss: 0.00002706
Iteration 279/1000 | Loss: 0.00002721
Iteration 280/1000 | Loss: 0.00002721
Iteration 281/1000 | Loss: 0.00002709
Iteration 282/1000 | Loss: 0.00002709
Iteration 283/1000 | Loss: 0.00002708
Iteration 284/1000 | Loss: 0.00002708
Iteration 285/1000 | Loss: 0.00002707
Iteration 286/1000 | Loss: 0.00002707
Iteration 287/1000 | Loss: 0.00002707
Iteration 288/1000 | Loss: 0.00002707
Iteration 289/1000 | Loss: 0.00002707
Iteration 290/1000 | Loss: 0.00002707
Iteration 291/1000 | Loss: 0.00002707
Iteration 292/1000 | Loss: 0.00002707
Iteration 293/1000 | Loss: 0.00002707
Iteration 294/1000 | Loss: 0.00002706
Iteration 295/1000 | Loss: 0.00002706
Iteration 296/1000 | Loss: 0.00002722
Iteration 297/1000 | Loss: 0.00002721
Iteration 298/1000 | Loss: 0.00002721
Iteration 299/1000 | Loss: 0.00002706
Iteration 300/1000 | Loss: 0.00002706
Iteration 301/1000 | Loss: 0.00002706
Iteration 302/1000 | Loss: 0.00002706
Iteration 303/1000 | Loss: 0.00002706
Iteration 304/1000 | Loss: 0.00002706
Iteration 305/1000 | Loss: 0.00002706
Iteration 306/1000 | Loss: 0.00002706
Iteration 307/1000 | Loss: 0.00002706
Iteration 308/1000 | Loss: 0.00002706
Iteration 309/1000 | Loss: 0.00002706
Iteration 310/1000 | Loss: 0.00002706
Iteration 311/1000 | Loss: 0.00002706
Iteration 312/1000 | Loss: 0.00002706
Iteration 313/1000 | Loss: 0.00002706
Iteration 314/1000 | Loss: 0.00002706
Iteration 315/1000 | Loss: 0.00002706
Iteration 316/1000 | Loss: 0.00002706
Iteration 317/1000 | Loss: 0.00002706
Iteration 318/1000 | Loss: 0.00002706
Iteration 319/1000 | Loss: 0.00002706
Iteration 320/1000 | Loss: 0.00002706
Iteration 321/1000 | Loss: 0.00002706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 321. Stopping optimization.
Last 5 losses: [2.7061641958425753e-05, 2.7061641958425753e-05, 2.7061641958425753e-05, 2.7061641958425753e-05, 2.7061641958425753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7061641958425753e-05

Optimization complete. Final v2v error: 3.634343147277832 mm

Highest mean error: 22.656330108642578 mm for frame 39

Lowest mean error: 3.05942964553833 mm for frame 44

Saving results

Total time: 145.8830451965332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035514
Iteration 2/25 | Loss: 0.00242521
Iteration 3/25 | Loss: 0.00153670
Iteration 4/25 | Loss: 0.00137818
Iteration 5/25 | Loss: 0.00130410
Iteration 6/25 | Loss: 0.00127090
Iteration 7/25 | Loss: 0.00122772
Iteration 8/25 | Loss: 0.00120423
Iteration 9/25 | Loss: 0.00120651
Iteration 10/25 | Loss: 0.00118798
Iteration 11/25 | Loss: 0.00118604
Iteration 12/25 | Loss: 0.00118577
Iteration 13/25 | Loss: 0.00118514
Iteration 14/25 | Loss: 0.00118495
Iteration 15/25 | Loss: 0.00118492
Iteration 16/25 | Loss: 0.00119149
Iteration 17/25 | Loss: 0.00118583
Iteration 18/25 | Loss: 0.00118396
Iteration 19/25 | Loss: 0.00118548
Iteration 20/25 | Loss: 0.00118375
Iteration 21/25 | Loss: 0.00118335
Iteration 22/25 | Loss: 0.00118253
Iteration 23/25 | Loss: 0.00118248
Iteration 24/25 | Loss: 0.00118226
Iteration 25/25 | Loss: 0.00118149

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44789517
Iteration 2/25 | Loss: 0.00115749
Iteration 3/25 | Loss: 0.00102830
Iteration 4/25 | Loss: 0.00102830
Iteration 5/25 | Loss: 0.00102830
Iteration 6/25 | Loss: 0.00102829
Iteration 7/25 | Loss: 0.00102829
Iteration 8/25 | Loss: 0.00102829
Iteration 9/25 | Loss: 0.00102829
Iteration 10/25 | Loss: 0.00102829
Iteration 11/25 | Loss: 0.00102829
Iteration 12/25 | Loss: 0.00102829
Iteration 13/25 | Loss: 0.00102829
Iteration 14/25 | Loss: 0.00102829
Iteration 15/25 | Loss: 0.00102829
Iteration 16/25 | Loss: 0.00102829
Iteration 17/25 | Loss: 0.00102829
Iteration 18/25 | Loss: 0.00102829
Iteration 19/25 | Loss: 0.00102829
Iteration 20/25 | Loss: 0.00102829
Iteration 21/25 | Loss: 0.00102829
Iteration 22/25 | Loss: 0.00102829
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010282930452376604, 0.0010282930452376604, 0.0010282930452376604, 0.0010282930452376604, 0.0010282930452376604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010282930452376604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102829
Iteration 2/1000 | Loss: 0.00039580
Iteration 3/1000 | Loss: 0.00027061
Iteration 4/1000 | Loss: 0.00008156
Iteration 5/1000 | Loss: 0.00023960
Iteration 6/1000 | Loss: 0.00004973
Iteration 7/1000 | Loss: 0.00003503
Iteration 8/1000 | Loss: 0.00005480
Iteration 9/1000 | Loss: 0.00004044
Iteration 10/1000 | Loss: 0.00027844
Iteration 11/1000 | Loss: 0.00034613
Iteration 12/1000 | Loss: 0.00033817
Iteration 13/1000 | Loss: 0.00029350
Iteration 14/1000 | Loss: 0.00007303
Iteration 15/1000 | Loss: 0.00004957
Iteration 16/1000 | Loss: 0.00002697
Iteration 17/1000 | Loss: 0.00007597
Iteration 18/1000 | Loss: 0.00002528
Iteration 19/1000 | Loss: 0.00002397
Iteration 20/1000 | Loss: 0.00006787
Iteration 21/1000 | Loss: 0.00002247
Iteration 22/1000 | Loss: 0.00003564
Iteration 23/1000 | Loss: 0.00007227
Iteration 24/1000 | Loss: 0.00002137
Iteration 25/1000 | Loss: 0.00002470
Iteration 26/1000 | Loss: 0.00002323
Iteration 27/1000 | Loss: 0.00002018
Iteration 28/1000 | Loss: 0.00001989
Iteration 29/1000 | Loss: 0.00001946
Iteration 30/1000 | Loss: 0.00001910
Iteration 31/1000 | Loss: 0.00001884
Iteration 32/1000 | Loss: 0.00002718
Iteration 33/1000 | Loss: 0.00002005
Iteration 34/1000 | Loss: 0.00001842
Iteration 35/1000 | Loss: 0.00001867
Iteration 36/1000 | Loss: 0.00001831
Iteration 37/1000 | Loss: 0.00001831
Iteration 38/1000 | Loss: 0.00001828
Iteration 39/1000 | Loss: 0.00001828
Iteration 40/1000 | Loss: 0.00001827
Iteration 41/1000 | Loss: 0.00001827
Iteration 42/1000 | Loss: 0.00001825
Iteration 43/1000 | Loss: 0.00001824
Iteration 44/1000 | Loss: 0.00001821
Iteration 45/1000 | Loss: 0.00001811
Iteration 46/1000 | Loss: 0.00001808
Iteration 47/1000 | Loss: 0.00001806
Iteration 48/1000 | Loss: 0.00001806
Iteration 49/1000 | Loss: 0.00001805
Iteration 50/1000 | Loss: 0.00001805
Iteration 51/1000 | Loss: 0.00001804
Iteration 52/1000 | Loss: 0.00001802
Iteration 53/1000 | Loss: 0.00001802
Iteration 54/1000 | Loss: 0.00001801
Iteration 55/1000 | Loss: 0.00001801
Iteration 56/1000 | Loss: 0.00001801
Iteration 57/1000 | Loss: 0.00001801
Iteration 58/1000 | Loss: 0.00001801
Iteration 59/1000 | Loss: 0.00001801
Iteration 60/1000 | Loss: 0.00001801
Iteration 61/1000 | Loss: 0.00001801
Iteration 62/1000 | Loss: 0.00001800
Iteration 63/1000 | Loss: 0.00001800
Iteration 64/1000 | Loss: 0.00001800
Iteration 65/1000 | Loss: 0.00001799
Iteration 66/1000 | Loss: 0.00001798
Iteration 67/1000 | Loss: 0.00001798
Iteration 68/1000 | Loss: 0.00001797
Iteration 69/1000 | Loss: 0.00001797
Iteration 70/1000 | Loss: 0.00001796
Iteration 71/1000 | Loss: 0.00001796
Iteration 72/1000 | Loss: 0.00001795
Iteration 73/1000 | Loss: 0.00001795
Iteration 74/1000 | Loss: 0.00001795
Iteration 75/1000 | Loss: 0.00001794
Iteration 76/1000 | Loss: 0.00001794
Iteration 77/1000 | Loss: 0.00001794
Iteration 78/1000 | Loss: 0.00001794
Iteration 79/1000 | Loss: 0.00001794
Iteration 80/1000 | Loss: 0.00001794
Iteration 81/1000 | Loss: 0.00001794
Iteration 82/1000 | Loss: 0.00001794
Iteration 83/1000 | Loss: 0.00001794
Iteration 84/1000 | Loss: 0.00003041
Iteration 85/1000 | Loss: 0.00003041
Iteration 86/1000 | Loss: 0.00001799
Iteration 87/1000 | Loss: 0.00001788
Iteration 88/1000 | Loss: 0.00001788
Iteration 89/1000 | Loss: 0.00001788
Iteration 90/1000 | Loss: 0.00001788
Iteration 91/1000 | Loss: 0.00001788
Iteration 92/1000 | Loss: 0.00001788
Iteration 93/1000 | Loss: 0.00001787
Iteration 94/1000 | Loss: 0.00001786
Iteration 95/1000 | Loss: 0.00001786
Iteration 96/1000 | Loss: 0.00001786
Iteration 97/1000 | Loss: 0.00001786
Iteration 98/1000 | Loss: 0.00001786
Iteration 99/1000 | Loss: 0.00001786
Iteration 100/1000 | Loss: 0.00001786
Iteration 101/1000 | Loss: 0.00001785
Iteration 102/1000 | Loss: 0.00001785
Iteration 103/1000 | Loss: 0.00001785
Iteration 104/1000 | Loss: 0.00001785
Iteration 105/1000 | Loss: 0.00001785
Iteration 106/1000 | Loss: 0.00001785
Iteration 107/1000 | Loss: 0.00001785
Iteration 108/1000 | Loss: 0.00001785
Iteration 109/1000 | Loss: 0.00001785
Iteration 110/1000 | Loss: 0.00001785
Iteration 111/1000 | Loss: 0.00001785
Iteration 112/1000 | Loss: 0.00001785
Iteration 113/1000 | Loss: 0.00001785
Iteration 114/1000 | Loss: 0.00001785
Iteration 115/1000 | Loss: 0.00001785
Iteration 116/1000 | Loss: 0.00001785
Iteration 117/1000 | Loss: 0.00001785
Iteration 118/1000 | Loss: 0.00001785
Iteration 119/1000 | Loss: 0.00001785
Iteration 120/1000 | Loss: 0.00001785
Iteration 121/1000 | Loss: 0.00001785
Iteration 122/1000 | Loss: 0.00001785
Iteration 123/1000 | Loss: 0.00001785
Iteration 124/1000 | Loss: 0.00001785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.784621963452082e-05, 1.784621963452082e-05, 1.784621963452082e-05, 1.784621963452082e-05, 1.784621963452082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.784621963452082e-05

Optimization complete. Final v2v error: 3.5374999046325684 mm

Highest mean error: 10.405466079711914 mm for frame 127

Lowest mean error: 3.0115411281585693 mm for frame 123

Saving results

Total time: 101.26326870918274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899270
Iteration 2/25 | Loss: 0.00154903
Iteration 3/25 | Loss: 0.00126009
Iteration 4/25 | Loss: 0.00121991
Iteration 5/25 | Loss: 0.00121568
Iteration 6/25 | Loss: 0.00121544
Iteration 7/25 | Loss: 0.00121544
Iteration 8/25 | Loss: 0.00121544
Iteration 9/25 | Loss: 0.00121544
Iteration 10/25 | Loss: 0.00121544
Iteration 11/25 | Loss: 0.00121544
Iteration 12/25 | Loss: 0.00121544
Iteration 13/25 | Loss: 0.00121544
Iteration 14/25 | Loss: 0.00121544
Iteration 15/25 | Loss: 0.00121544
Iteration 16/25 | Loss: 0.00121544
Iteration 17/25 | Loss: 0.00121544
Iteration 18/25 | Loss: 0.00121544
Iteration 19/25 | Loss: 0.00121544
Iteration 20/25 | Loss: 0.00121544
Iteration 21/25 | Loss: 0.00121544
Iteration 22/25 | Loss: 0.00121544
Iteration 23/25 | Loss: 0.00121544
Iteration 24/25 | Loss: 0.00121544
Iteration 25/25 | Loss: 0.00121544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88137776
Iteration 2/25 | Loss: 0.00063434
Iteration 3/25 | Loss: 0.00063433
Iteration 4/25 | Loss: 0.00063433
Iteration 5/25 | Loss: 0.00063433
Iteration 6/25 | Loss: 0.00063433
Iteration 7/25 | Loss: 0.00063433
Iteration 8/25 | Loss: 0.00063433
Iteration 9/25 | Loss: 0.00063433
Iteration 10/25 | Loss: 0.00063433
Iteration 11/25 | Loss: 0.00063433
Iteration 12/25 | Loss: 0.00063433
Iteration 13/25 | Loss: 0.00063433
Iteration 14/25 | Loss: 0.00063433
Iteration 15/25 | Loss: 0.00063433
Iteration 16/25 | Loss: 0.00063433
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006343271234072745, 0.0006343271234072745, 0.0006343271234072745, 0.0006343271234072745, 0.0006343271234072745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006343271234072745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063433
Iteration 2/1000 | Loss: 0.00004160
Iteration 3/1000 | Loss: 0.00002700
Iteration 4/1000 | Loss: 0.00002393
Iteration 5/1000 | Loss: 0.00002269
Iteration 6/1000 | Loss: 0.00002193
Iteration 7/1000 | Loss: 0.00002149
Iteration 8/1000 | Loss: 0.00002121
Iteration 9/1000 | Loss: 0.00002094
Iteration 10/1000 | Loss: 0.00002078
Iteration 11/1000 | Loss: 0.00002074
Iteration 12/1000 | Loss: 0.00002074
Iteration 13/1000 | Loss: 0.00002074
Iteration 14/1000 | Loss: 0.00002074
Iteration 15/1000 | Loss: 0.00002074
Iteration 16/1000 | Loss: 0.00002074
Iteration 17/1000 | Loss: 0.00002074
Iteration 18/1000 | Loss: 0.00002074
Iteration 19/1000 | Loss: 0.00002074
Iteration 20/1000 | Loss: 0.00002074
Iteration 21/1000 | Loss: 0.00002073
Iteration 22/1000 | Loss: 0.00002065
Iteration 23/1000 | Loss: 0.00002065
Iteration 24/1000 | Loss: 0.00002065
Iteration 25/1000 | Loss: 0.00002065
Iteration 26/1000 | Loss: 0.00002065
Iteration 27/1000 | Loss: 0.00002065
Iteration 28/1000 | Loss: 0.00002065
Iteration 29/1000 | Loss: 0.00002065
Iteration 30/1000 | Loss: 0.00002064
Iteration 31/1000 | Loss: 0.00002064
Iteration 32/1000 | Loss: 0.00002064
Iteration 33/1000 | Loss: 0.00002064
Iteration 34/1000 | Loss: 0.00002064
Iteration 35/1000 | Loss: 0.00002064
Iteration 36/1000 | Loss: 0.00002064
Iteration 37/1000 | Loss: 0.00002063
Iteration 38/1000 | Loss: 0.00002063
Iteration 39/1000 | Loss: 0.00002063
Iteration 40/1000 | Loss: 0.00002063
Iteration 41/1000 | Loss: 0.00002063
Iteration 42/1000 | Loss: 0.00002063
Iteration 43/1000 | Loss: 0.00002063
Iteration 44/1000 | Loss: 0.00002062
Iteration 45/1000 | Loss: 0.00002062
Iteration 46/1000 | Loss: 0.00002062
Iteration 47/1000 | Loss: 0.00002062
Iteration 48/1000 | Loss: 0.00002062
Iteration 49/1000 | Loss: 0.00002062
Iteration 50/1000 | Loss: 0.00002062
Iteration 51/1000 | Loss: 0.00002062
Iteration 52/1000 | Loss: 0.00002062
Iteration 53/1000 | Loss: 0.00002062
Iteration 54/1000 | Loss: 0.00002062
Iteration 55/1000 | Loss: 0.00002062
Iteration 56/1000 | Loss: 0.00002061
Iteration 57/1000 | Loss: 0.00002061
Iteration 58/1000 | Loss: 0.00002061
Iteration 59/1000 | Loss: 0.00002060
Iteration 60/1000 | Loss: 0.00002060
Iteration 61/1000 | Loss: 0.00002060
Iteration 62/1000 | Loss: 0.00002060
Iteration 63/1000 | Loss: 0.00002060
Iteration 64/1000 | Loss: 0.00002060
Iteration 65/1000 | Loss: 0.00002060
Iteration 66/1000 | Loss: 0.00002060
Iteration 67/1000 | Loss: 0.00002060
Iteration 68/1000 | Loss: 0.00002060
Iteration 69/1000 | Loss: 0.00002060
Iteration 70/1000 | Loss: 0.00002060
Iteration 71/1000 | Loss: 0.00002060
Iteration 72/1000 | Loss: 0.00002060
Iteration 73/1000 | Loss: 0.00002060
Iteration 74/1000 | Loss: 0.00002060
Iteration 75/1000 | Loss: 0.00002060
Iteration 76/1000 | Loss: 0.00002060
Iteration 77/1000 | Loss: 0.00002060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [2.0596684407792054e-05, 2.0596684407792054e-05, 2.0596684407792054e-05, 2.0596684407792054e-05, 2.0596684407792054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0596684407792054e-05

Optimization complete. Final v2v error: 3.8004066944122314 mm

Highest mean error: 3.8810274600982666 mm for frame 217

Lowest mean error: 3.7242114543914795 mm for frame 5

Saving results

Total time: 30.56938147544861
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027627
Iteration 2/25 | Loss: 0.00226843
Iteration 3/25 | Loss: 0.00157855
Iteration 4/25 | Loss: 0.00155034
Iteration 5/25 | Loss: 0.00145601
Iteration 6/25 | Loss: 0.00139968
Iteration 7/25 | Loss: 0.00132199
Iteration 8/25 | Loss: 0.00127920
Iteration 9/25 | Loss: 0.00127303
Iteration 10/25 | Loss: 0.00126959
Iteration 11/25 | Loss: 0.00126578
Iteration 12/25 | Loss: 0.00126479
Iteration 13/25 | Loss: 0.00126454
Iteration 14/25 | Loss: 0.00126449
Iteration 15/25 | Loss: 0.00126229
Iteration 16/25 | Loss: 0.00126187
Iteration 17/25 | Loss: 0.00126166
Iteration 18/25 | Loss: 0.00126157
Iteration 19/25 | Loss: 0.00126156
Iteration 20/25 | Loss: 0.00126156
Iteration 21/25 | Loss: 0.00126156
Iteration 22/25 | Loss: 0.00126155
Iteration 23/25 | Loss: 0.00126155
Iteration 24/25 | Loss: 0.00126155
Iteration 25/25 | Loss: 0.00126155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24011612
Iteration 2/25 | Loss: 0.00094097
Iteration 3/25 | Loss: 0.00094097
Iteration 4/25 | Loss: 0.00094097
Iteration 5/25 | Loss: 0.00094097
Iteration 6/25 | Loss: 0.00094097
Iteration 7/25 | Loss: 0.00094097
Iteration 8/25 | Loss: 0.00094096
Iteration 9/25 | Loss: 0.00094096
Iteration 10/25 | Loss: 0.00094096
Iteration 11/25 | Loss: 0.00094096
Iteration 12/25 | Loss: 0.00094096
Iteration 13/25 | Loss: 0.00094096
Iteration 14/25 | Loss: 0.00094096
Iteration 15/25 | Loss: 0.00094096
Iteration 16/25 | Loss: 0.00094096
Iteration 17/25 | Loss: 0.00094096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009409644990228117, 0.0009409644990228117, 0.0009409644990228117, 0.0009409644990228117, 0.0009409644990228117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009409644990228117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094096
Iteration 2/1000 | Loss: 0.00005888
Iteration 3/1000 | Loss: 0.00003431
Iteration 4/1000 | Loss: 0.00002941
Iteration 5/1000 | Loss: 0.00002716
Iteration 6/1000 | Loss: 0.00002585
Iteration 7/1000 | Loss: 0.00002501
Iteration 8/1000 | Loss: 0.00002433
Iteration 9/1000 | Loss: 0.00002378
Iteration 10/1000 | Loss: 0.00002344
Iteration 11/1000 | Loss: 0.00002320
Iteration 12/1000 | Loss: 0.00002295
Iteration 13/1000 | Loss: 0.00002294
Iteration 14/1000 | Loss: 0.00002281
Iteration 15/1000 | Loss: 0.00002279
Iteration 16/1000 | Loss: 0.00002276
Iteration 17/1000 | Loss: 0.00002275
Iteration 18/1000 | Loss: 0.00002274
Iteration 19/1000 | Loss: 0.00002270
Iteration 20/1000 | Loss: 0.00002269
Iteration 21/1000 | Loss: 0.00002269
Iteration 22/1000 | Loss: 0.00002269
Iteration 23/1000 | Loss: 0.00002269
Iteration 24/1000 | Loss: 0.00002269
Iteration 25/1000 | Loss: 0.00002269
Iteration 26/1000 | Loss: 0.00002269
Iteration 27/1000 | Loss: 0.00002268
Iteration 28/1000 | Loss: 0.00002266
Iteration 29/1000 | Loss: 0.00002266
Iteration 30/1000 | Loss: 0.00002265
Iteration 31/1000 | Loss: 0.00002265
Iteration 32/1000 | Loss: 0.00002265
Iteration 33/1000 | Loss: 0.00002265
Iteration 34/1000 | Loss: 0.00002265
Iteration 35/1000 | Loss: 0.00002265
Iteration 36/1000 | Loss: 0.00002265
Iteration 37/1000 | Loss: 0.00002265
Iteration 38/1000 | Loss: 0.00002265
Iteration 39/1000 | Loss: 0.00002265
Iteration 40/1000 | Loss: 0.00002264
Iteration 41/1000 | Loss: 0.00002264
Iteration 42/1000 | Loss: 0.00002264
Iteration 43/1000 | Loss: 0.00002264
Iteration 44/1000 | Loss: 0.00002263
Iteration 45/1000 | Loss: 0.00002261
Iteration 46/1000 | Loss: 0.00002260
Iteration 47/1000 | Loss: 0.00002260
Iteration 48/1000 | Loss: 0.00002260
Iteration 49/1000 | Loss: 0.00002259
Iteration 50/1000 | Loss: 0.00002259
Iteration 51/1000 | Loss: 0.00002259
Iteration 52/1000 | Loss: 0.00002259
Iteration 53/1000 | Loss: 0.00002259
Iteration 54/1000 | Loss: 0.00002258
Iteration 55/1000 | Loss: 0.00002258
Iteration 56/1000 | Loss: 0.00002258
Iteration 57/1000 | Loss: 0.00002258
Iteration 58/1000 | Loss: 0.00002258
Iteration 59/1000 | Loss: 0.00002258
Iteration 60/1000 | Loss: 0.00002258
Iteration 61/1000 | Loss: 0.00002258
Iteration 62/1000 | Loss: 0.00002257
Iteration 63/1000 | Loss: 0.00002257
Iteration 64/1000 | Loss: 0.00002257
Iteration 65/1000 | Loss: 0.00002257
Iteration 66/1000 | Loss: 0.00002257
Iteration 67/1000 | Loss: 0.00002257
Iteration 68/1000 | Loss: 0.00002257
Iteration 69/1000 | Loss: 0.00002257
Iteration 70/1000 | Loss: 0.00002257
Iteration 71/1000 | Loss: 0.00002256
Iteration 72/1000 | Loss: 0.00002256
Iteration 73/1000 | Loss: 0.00002255
Iteration 74/1000 | Loss: 0.00002255
Iteration 75/1000 | Loss: 0.00002255
Iteration 76/1000 | Loss: 0.00002255
Iteration 77/1000 | Loss: 0.00002255
Iteration 78/1000 | Loss: 0.00002255
Iteration 79/1000 | Loss: 0.00002255
Iteration 80/1000 | Loss: 0.00002255
Iteration 81/1000 | Loss: 0.00002255
Iteration 82/1000 | Loss: 0.00002255
Iteration 83/1000 | Loss: 0.00002255
Iteration 84/1000 | Loss: 0.00002255
Iteration 85/1000 | Loss: 0.00002255
Iteration 86/1000 | Loss: 0.00002255
Iteration 87/1000 | Loss: 0.00002255
Iteration 88/1000 | Loss: 0.00002255
Iteration 89/1000 | Loss: 0.00002255
Iteration 90/1000 | Loss: 0.00002255
Iteration 91/1000 | Loss: 0.00002255
Iteration 92/1000 | Loss: 0.00002255
Iteration 93/1000 | Loss: 0.00002255
Iteration 94/1000 | Loss: 0.00002255
Iteration 95/1000 | Loss: 0.00002255
Iteration 96/1000 | Loss: 0.00002255
Iteration 97/1000 | Loss: 0.00002255
Iteration 98/1000 | Loss: 0.00002255
Iteration 99/1000 | Loss: 0.00002255
Iteration 100/1000 | Loss: 0.00002255
Iteration 101/1000 | Loss: 0.00002255
Iteration 102/1000 | Loss: 0.00002255
Iteration 103/1000 | Loss: 0.00002255
Iteration 104/1000 | Loss: 0.00002255
Iteration 105/1000 | Loss: 0.00002255
Iteration 106/1000 | Loss: 0.00002255
Iteration 107/1000 | Loss: 0.00002255
Iteration 108/1000 | Loss: 0.00002255
Iteration 109/1000 | Loss: 0.00002255
Iteration 110/1000 | Loss: 0.00002255
Iteration 111/1000 | Loss: 0.00002255
Iteration 112/1000 | Loss: 0.00002255
Iteration 113/1000 | Loss: 0.00002255
Iteration 114/1000 | Loss: 0.00002255
Iteration 115/1000 | Loss: 0.00002254
Iteration 116/1000 | Loss: 0.00002254
Iteration 117/1000 | Loss: 0.00002254
Iteration 118/1000 | Loss: 0.00002254
Iteration 119/1000 | Loss: 0.00002254
Iteration 120/1000 | Loss: 0.00002254
Iteration 121/1000 | Loss: 0.00002254
Iteration 122/1000 | Loss: 0.00002254
Iteration 123/1000 | Loss: 0.00002254
Iteration 124/1000 | Loss: 0.00002254
Iteration 125/1000 | Loss: 0.00002254
Iteration 126/1000 | Loss: 0.00002254
Iteration 127/1000 | Loss: 0.00002254
Iteration 128/1000 | Loss: 0.00002254
Iteration 129/1000 | Loss: 0.00002254
Iteration 130/1000 | Loss: 0.00002254
Iteration 131/1000 | Loss: 0.00002254
Iteration 132/1000 | Loss: 0.00002254
Iteration 133/1000 | Loss: 0.00002254
Iteration 134/1000 | Loss: 0.00002254
Iteration 135/1000 | Loss: 0.00002254
Iteration 136/1000 | Loss: 0.00002254
Iteration 137/1000 | Loss: 0.00002254
Iteration 138/1000 | Loss: 0.00002254
Iteration 139/1000 | Loss: 0.00002254
Iteration 140/1000 | Loss: 0.00002254
Iteration 141/1000 | Loss: 0.00002254
Iteration 142/1000 | Loss: 0.00002254
Iteration 143/1000 | Loss: 0.00002254
Iteration 144/1000 | Loss: 0.00002254
Iteration 145/1000 | Loss: 0.00002254
Iteration 146/1000 | Loss: 0.00002254
Iteration 147/1000 | Loss: 0.00002254
Iteration 148/1000 | Loss: 0.00002254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.254302853543777e-05, 2.254302853543777e-05, 2.254302853543777e-05, 2.254302853543777e-05, 2.254302853543777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.254302853543777e-05

Optimization complete. Final v2v error: 3.7993383407592773 mm

Highest mean error: 12.225586891174316 mm for frame 41

Lowest mean error: 3.523346185684204 mm for frame 55

Saving results

Total time: 57.11998128890991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908986
Iteration 2/25 | Loss: 0.00249496
Iteration 3/25 | Loss: 0.00189589
Iteration 4/25 | Loss: 0.00180246
Iteration 5/25 | Loss: 0.00174074
Iteration 6/25 | Loss: 0.00168739
Iteration 7/25 | Loss: 0.00158169
Iteration 8/25 | Loss: 0.00147549
Iteration 9/25 | Loss: 0.00140416
Iteration 10/25 | Loss: 0.00137646
Iteration 11/25 | Loss: 0.00136136
Iteration 12/25 | Loss: 0.00132902
Iteration 13/25 | Loss: 0.00130767
Iteration 14/25 | Loss: 0.00129790
Iteration 15/25 | Loss: 0.00129013
Iteration 16/25 | Loss: 0.00128450
Iteration 17/25 | Loss: 0.00128019
Iteration 18/25 | Loss: 0.00128143
Iteration 19/25 | Loss: 0.00128254
Iteration 20/25 | Loss: 0.00127856
Iteration 21/25 | Loss: 0.00127591
Iteration 22/25 | Loss: 0.00127489
Iteration 23/25 | Loss: 0.00127300
Iteration 24/25 | Loss: 0.00127442
Iteration 25/25 | Loss: 0.00127404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30995870
Iteration 2/25 | Loss: 0.00078210
Iteration 3/25 | Loss: 0.00078208
Iteration 4/25 | Loss: 0.00078208
Iteration 5/25 | Loss: 0.00078208
Iteration 6/25 | Loss: 0.00078208
Iteration 7/25 | Loss: 0.00078208
Iteration 8/25 | Loss: 0.00078208
Iteration 9/25 | Loss: 0.00078208
Iteration 10/25 | Loss: 0.00078208
Iteration 11/25 | Loss: 0.00078208
Iteration 12/25 | Loss: 0.00078208
Iteration 13/25 | Loss: 0.00078208
Iteration 14/25 | Loss: 0.00078208
Iteration 15/25 | Loss: 0.00078208
Iteration 16/25 | Loss: 0.00078208
Iteration 17/25 | Loss: 0.00078208
Iteration 18/25 | Loss: 0.00078208
Iteration 19/25 | Loss: 0.00078208
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007820760365575552, 0.0007820760365575552, 0.0007820760365575552, 0.0007820760365575552, 0.0007820760365575552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007820760365575552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078208
Iteration 2/1000 | Loss: 0.00013479
Iteration 3/1000 | Loss: 0.00011169
Iteration 4/1000 | Loss: 0.00010375
Iteration 5/1000 | Loss: 0.00008307
Iteration 6/1000 | Loss: 0.00010422
Iteration 7/1000 | Loss: 0.00011591
Iteration 8/1000 | Loss: 0.00008194
Iteration 9/1000 | Loss: 0.00010167
Iteration 10/1000 | Loss: 0.00009530
Iteration 11/1000 | Loss: 0.00015015
Iteration 12/1000 | Loss: 0.00015609
Iteration 13/1000 | Loss: 0.00012636
Iteration 14/1000 | Loss: 0.00014550
Iteration 15/1000 | Loss: 0.00013510
Iteration 16/1000 | Loss: 0.00015396
Iteration 17/1000 | Loss: 0.00015852
Iteration 18/1000 | Loss: 0.00013389
Iteration 19/1000 | Loss: 0.00012429
Iteration 20/1000 | Loss: 0.00015887
Iteration 21/1000 | Loss: 0.00015612
Iteration 22/1000 | Loss: 0.00016193
Iteration 23/1000 | Loss: 0.00018281
Iteration 24/1000 | Loss: 0.00016264
Iteration 25/1000 | Loss: 0.00015729
Iteration 26/1000 | Loss: 0.00016012
Iteration 27/1000 | Loss: 0.00015145
Iteration 28/1000 | Loss: 0.00014562
Iteration 29/1000 | Loss: 0.00015628
Iteration 30/1000 | Loss: 0.00017185
Iteration 31/1000 | Loss: 0.00017139
Iteration 32/1000 | Loss: 0.00018741
Iteration 33/1000 | Loss: 0.00017022
Iteration 34/1000 | Loss: 0.00018117
Iteration 35/1000 | Loss: 0.00017856
Iteration 36/1000 | Loss: 0.00018183
Iteration 37/1000 | Loss: 0.00011887
Iteration 38/1000 | Loss: 0.00013064
Iteration 39/1000 | Loss: 0.00014505
Iteration 40/1000 | Loss: 0.00015020
Iteration 41/1000 | Loss: 0.00013071
Iteration 42/1000 | Loss: 0.00015342
Iteration 43/1000 | Loss: 0.00014778
Iteration 44/1000 | Loss: 0.00011942
Iteration 45/1000 | Loss: 0.00017633
Iteration 46/1000 | Loss: 0.00015862
Iteration 47/1000 | Loss: 0.00014203
Iteration 48/1000 | Loss: 0.00016372
Iteration 49/1000 | Loss: 0.00015689
Iteration 50/1000 | Loss: 0.00010149
Iteration 51/1000 | Loss: 0.00006722
Iteration 52/1000 | Loss: 0.00012346
Iteration 53/1000 | Loss: 0.00011191
Iteration 54/1000 | Loss: 0.00016920
Iteration 55/1000 | Loss: 0.00014072
Iteration 56/1000 | Loss: 0.00010926
Iteration 57/1000 | Loss: 0.00011275
Iteration 58/1000 | Loss: 0.00011836
Iteration 59/1000 | Loss: 0.00013928
Iteration 60/1000 | Loss: 0.00014634
Iteration 61/1000 | Loss: 0.00016601
Iteration 62/1000 | Loss: 0.00008662
Iteration 63/1000 | Loss: 0.00012524
Iteration 64/1000 | Loss: 0.00013336
Iteration 65/1000 | Loss: 0.00010840
Iteration 66/1000 | Loss: 0.00012608
Iteration 67/1000 | Loss: 0.00010585
Iteration 68/1000 | Loss: 0.00012761
Iteration 69/1000 | Loss: 0.00011206
Iteration 70/1000 | Loss: 0.00012283
Iteration 71/1000 | Loss: 0.00012452
Iteration 72/1000 | Loss: 0.00015218
Iteration 73/1000 | Loss: 0.00010460
Iteration 74/1000 | Loss: 0.00012629
Iteration 75/1000 | Loss: 0.00010843
Iteration 76/1000 | Loss: 0.00011054
Iteration 77/1000 | Loss: 0.00016484
Iteration 78/1000 | Loss: 0.00012291
Iteration 79/1000 | Loss: 0.00011180
Iteration 80/1000 | Loss: 0.00016157
Iteration 81/1000 | Loss: 0.00013800
Iteration 82/1000 | Loss: 0.00012733
Iteration 83/1000 | Loss: 0.00012895
Iteration 84/1000 | Loss: 0.00010226
Iteration 85/1000 | Loss: 0.00014919
Iteration 86/1000 | Loss: 0.00008232
Iteration 87/1000 | Loss: 0.00009437
Iteration 88/1000 | Loss: 0.00010946
Iteration 89/1000 | Loss: 0.00011454
Iteration 90/1000 | Loss: 0.00013147
Iteration 91/1000 | Loss: 0.00011063
Iteration 92/1000 | Loss: 0.00012411
Iteration 93/1000 | Loss: 0.00011842
Iteration 94/1000 | Loss: 0.00013703
Iteration 95/1000 | Loss: 0.00011877
Iteration 96/1000 | Loss: 0.00011956
Iteration 97/1000 | Loss: 0.00012337
Iteration 98/1000 | Loss: 0.00013954
Iteration 99/1000 | Loss: 0.00015343
Iteration 100/1000 | Loss: 0.00007994
Iteration 101/1000 | Loss: 0.00013707
Iteration 102/1000 | Loss: 0.00010905
Iteration 103/1000 | Loss: 0.00010001
Iteration 104/1000 | Loss: 0.00010123
Iteration 105/1000 | Loss: 0.00010163
Iteration 106/1000 | Loss: 0.00010523
Iteration 107/1000 | Loss: 0.00012476
Iteration 108/1000 | Loss: 0.00011477
Iteration 109/1000 | Loss: 0.00011729
Iteration 110/1000 | Loss: 0.00011965
Iteration 111/1000 | Loss: 0.00009637
Iteration 112/1000 | Loss: 0.00009992
Iteration 113/1000 | Loss: 0.00012384
Iteration 114/1000 | Loss: 0.00011314
Iteration 115/1000 | Loss: 0.00011690
Iteration 116/1000 | Loss: 0.00010331
Iteration 117/1000 | Loss: 0.00012671
Iteration 118/1000 | Loss: 0.00012511
Iteration 119/1000 | Loss: 0.00009035
Iteration 120/1000 | Loss: 0.00007904
Iteration 121/1000 | Loss: 0.00011418
Iteration 122/1000 | Loss: 0.00009220
Iteration 123/1000 | Loss: 0.00006584
Iteration 124/1000 | Loss: 0.00008704
Iteration 125/1000 | Loss: 0.00008041
Iteration 126/1000 | Loss: 0.00010269
Iteration 127/1000 | Loss: 0.00008435
Iteration 128/1000 | Loss: 0.00009242
Iteration 129/1000 | Loss: 0.00006225
Iteration 130/1000 | Loss: 0.00009556
Iteration 131/1000 | Loss: 0.00006597
Iteration 132/1000 | Loss: 0.00006343
Iteration 133/1000 | Loss: 0.00008800
Iteration 134/1000 | Loss: 0.00011409
Iteration 135/1000 | Loss: 0.00008074
Iteration 136/1000 | Loss: 0.00009498
Iteration 137/1000 | Loss: 0.00008677
Iteration 138/1000 | Loss: 0.00006503
Iteration 139/1000 | Loss: 0.00007114
Iteration 140/1000 | Loss: 0.00011732
Iteration 141/1000 | Loss: 0.00010409
Iteration 142/1000 | Loss: 0.00013201
Iteration 143/1000 | Loss: 0.00010215
Iteration 144/1000 | Loss: 0.00011180
Iteration 145/1000 | Loss: 0.00009764
Iteration 146/1000 | Loss: 0.00011302
Iteration 147/1000 | Loss: 0.00011121
Iteration 148/1000 | Loss: 0.00011896
Iteration 149/1000 | Loss: 0.00011492
Iteration 150/1000 | Loss: 0.00006583
Iteration 151/1000 | Loss: 0.00006952
Iteration 152/1000 | Loss: 0.00008629
Iteration 153/1000 | Loss: 0.00007726
Iteration 154/1000 | Loss: 0.00007717
Iteration 155/1000 | Loss: 0.00009394
Iteration 156/1000 | Loss: 0.00009073
Iteration 157/1000 | Loss: 0.00008986
Iteration 158/1000 | Loss: 0.00011075
Iteration 159/1000 | Loss: 0.00010354
Iteration 160/1000 | Loss: 0.00009758
Iteration 161/1000 | Loss: 0.00007167
Iteration 162/1000 | Loss: 0.00006480
Iteration 163/1000 | Loss: 0.00005880
Iteration 164/1000 | Loss: 0.00007675
Iteration 165/1000 | Loss: 0.00005618
Iteration 166/1000 | Loss: 0.00007987
Iteration 167/1000 | Loss: 0.00008192
Iteration 168/1000 | Loss: 0.00008289
Iteration 169/1000 | Loss: 0.00007915
Iteration 170/1000 | Loss: 0.00005486
Iteration 171/1000 | Loss: 0.00004534
Iteration 172/1000 | Loss: 0.00007011
Iteration 173/1000 | Loss: 0.00007602
Iteration 174/1000 | Loss: 0.00009053
Iteration 175/1000 | Loss: 0.00010004
Iteration 176/1000 | Loss: 0.00009722
Iteration 177/1000 | Loss: 0.00010177
Iteration 178/1000 | Loss: 0.00009746
Iteration 179/1000 | Loss: 0.00005273
Iteration 180/1000 | Loss: 0.00005434
Iteration 181/1000 | Loss: 0.00004812
Iteration 182/1000 | Loss: 0.00005532
Iteration 183/1000 | Loss: 0.00008369
Iteration 184/1000 | Loss: 0.00006856
Iteration 185/1000 | Loss: 0.00004543
Iteration 186/1000 | Loss: 0.00004115
Iteration 187/1000 | Loss: 0.00005205
Iteration 188/1000 | Loss: 0.00007020
Iteration 189/1000 | Loss: 0.00006739
Iteration 190/1000 | Loss: 0.00005106
Iteration 191/1000 | Loss: 0.00006083
Iteration 192/1000 | Loss: 0.00006469
Iteration 193/1000 | Loss: 0.00007184
Iteration 194/1000 | Loss: 0.00007252
Iteration 195/1000 | Loss: 0.00004730
Iteration 196/1000 | Loss: 0.00004989
Iteration 197/1000 | Loss: 0.00003733
Iteration 198/1000 | Loss: 0.00005509
Iteration 199/1000 | Loss: 0.00004645
Iteration 200/1000 | Loss: 0.00004881
Iteration 201/1000 | Loss: 0.00004888
Iteration 202/1000 | Loss: 0.00004170
Iteration 203/1000 | Loss: 0.00004999
Iteration 204/1000 | Loss: 0.00004173
Iteration 205/1000 | Loss: 0.00005215
Iteration 206/1000 | Loss: 0.00004961
Iteration 207/1000 | Loss: 0.00006208
Iteration 208/1000 | Loss: 0.00004773
Iteration 209/1000 | Loss: 0.00006423
Iteration 210/1000 | Loss: 0.00004015
Iteration 211/1000 | Loss: 0.00005406
Iteration 212/1000 | Loss: 0.00004203
Iteration 213/1000 | Loss: 0.00005555
Iteration 214/1000 | Loss: 0.00005523
Iteration 215/1000 | Loss: 0.00005430
Iteration 216/1000 | Loss: 0.00007175
Iteration 217/1000 | Loss: 0.00005635
Iteration 218/1000 | Loss: 0.00004323
Iteration 219/1000 | Loss: 0.00005979
Iteration 220/1000 | Loss: 0.00004617
Iteration 221/1000 | Loss: 0.00006341
Iteration 222/1000 | Loss: 0.00006010
Iteration 223/1000 | Loss: 0.00006270
Iteration 224/1000 | Loss: 0.00005872
Iteration 225/1000 | Loss: 0.00003530
Iteration 226/1000 | Loss: 0.00002854
Iteration 227/1000 | Loss: 0.00004767
Iteration 228/1000 | Loss: 0.00002789
Iteration 229/1000 | Loss: 0.00002454
Iteration 230/1000 | Loss: 0.00002316
Iteration 231/1000 | Loss: 0.00002268
Iteration 232/1000 | Loss: 0.00002243
Iteration 233/1000 | Loss: 0.00002202
Iteration 234/1000 | Loss: 0.00002153
Iteration 235/1000 | Loss: 0.00002118
Iteration 236/1000 | Loss: 0.00002093
Iteration 237/1000 | Loss: 0.00002079
Iteration 238/1000 | Loss: 0.00002070
Iteration 239/1000 | Loss: 0.00002069
Iteration 240/1000 | Loss: 0.00002069
Iteration 241/1000 | Loss: 0.00002064
Iteration 242/1000 | Loss: 0.00002057
Iteration 243/1000 | Loss: 0.00002057
Iteration 244/1000 | Loss: 0.00002056
Iteration 245/1000 | Loss: 0.00002055
Iteration 246/1000 | Loss: 0.00002055
Iteration 247/1000 | Loss: 0.00002055
Iteration 248/1000 | Loss: 0.00002055
Iteration 249/1000 | Loss: 0.00002054
Iteration 250/1000 | Loss: 0.00002054
Iteration 251/1000 | Loss: 0.00002054
Iteration 252/1000 | Loss: 0.00002053
Iteration 253/1000 | Loss: 0.00002053
Iteration 254/1000 | Loss: 0.00002053
Iteration 255/1000 | Loss: 0.00002053
Iteration 256/1000 | Loss: 0.00002053
Iteration 257/1000 | Loss: 0.00002053
Iteration 258/1000 | Loss: 0.00002053
Iteration 259/1000 | Loss: 0.00002053
Iteration 260/1000 | Loss: 0.00002053
Iteration 261/1000 | Loss: 0.00002053
Iteration 262/1000 | Loss: 0.00002053
Iteration 263/1000 | Loss: 0.00002053
Iteration 264/1000 | Loss: 0.00002053
Iteration 265/1000 | Loss: 0.00002053
Iteration 266/1000 | Loss: 0.00002053
Iteration 267/1000 | Loss: 0.00002052
Iteration 268/1000 | Loss: 0.00002052
Iteration 269/1000 | Loss: 0.00002052
Iteration 270/1000 | Loss: 0.00002052
Iteration 271/1000 | Loss: 0.00002052
Iteration 272/1000 | Loss: 0.00002052
Iteration 273/1000 | Loss: 0.00002052
Iteration 274/1000 | Loss: 0.00002052
Iteration 275/1000 | Loss: 0.00002052
Iteration 276/1000 | Loss: 0.00002052
Iteration 277/1000 | Loss: 0.00002052
Iteration 278/1000 | Loss: 0.00002052
Iteration 279/1000 | Loss: 0.00002052
Iteration 280/1000 | Loss: 0.00002052
Iteration 281/1000 | Loss: 0.00002052
Iteration 282/1000 | Loss: 0.00002051
Iteration 283/1000 | Loss: 0.00002051
Iteration 284/1000 | Loss: 0.00002051
Iteration 285/1000 | Loss: 0.00002051
Iteration 286/1000 | Loss: 0.00002051
Iteration 287/1000 | Loss: 0.00002051
Iteration 288/1000 | Loss: 0.00002051
Iteration 289/1000 | Loss: 0.00002051
Iteration 290/1000 | Loss: 0.00002051
Iteration 291/1000 | Loss: 0.00002051
Iteration 292/1000 | Loss: 0.00002051
Iteration 293/1000 | Loss: 0.00002051
Iteration 294/1000 | Loss: 0.00002051
Iteration 295/1000 | Loss: 0.00002051
Iteration 296/1000 | Loss: 0.00002051
Iteration 297/1000 | Loss: 0.00002051
Iteration 298/1000 | Loss: 0.00002051
Iteration 299/1000 | Loss: 0.00002051
Iteration 300/1000 | Loss: 0.00002051
Iteration 301/1000 | Loss: 0.00002051
Iteration 302/1000 | Loss: 0.00002051
Iteration 303/1000 | Loss: 0.00002051
Iteration 304/1000 | Loss: 0.00002051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 304. Stopping optimization.
Last 5 losses: [2.0507628505583853e-05, 2.0507628505583853e-05, 2.0507628505583853e-05, 2.0507628505583853e-05, 2.0507628505583853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0507628505583853e-05

Optimization complete. Final v2v error: 3.983516216278076 mm

Highest mean error: 5.707327365875244 mm for frame 26

Lowest mean error: 3.7322819232940674 mm for frame 42

Saving results

Total time: 437.09697461128235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521872
Iteration 2/25 | Loss: 0.00136051
Iteration 3/25 | Loss: 0.00122083
Iteration 4/25 | Loss: 0.00120460
Iteration 5/25 | Loss: 0.00120000
Iteration 6/25 | Loss: 0.00119894
Iteration 7/25 | Loss: 0.00119882
Iteration 8/25 | Loss: 0.00119882
Iteration 9/25 | Loss: 0.00119882
Iteration 10/25 | Loss: 0.00119882
Iteration 11/25 | Loss: 0.00119882
Iteration 12/25 | Loss: 0.00119882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011988175101578236, 0.0011988175101578236, 0.0011988175101578236, 0.0011988175101578236, 0.0011988175101578236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011988175101578236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39964926
Iteration 2/25 | Loss: 0.00086501
Iteration 3/25 | Loss: 0.00086498
Iteration 4/25 | Loss: 0.00086498
Iteration 5/25 | Loss: 0.00086498
Iteration 6/25 | Loss: 0.00086498
Iteration 7/25 | Loss: 0.00086498
Iteration 8/25 | Loss: 0.00086498
Iteration 9/25 | Loss: 0.00086498
Iteration 10/25 | Loss: 0.00086498
Iteration 11/25 | Loss: 0.00086498
Iteration 12/25 | Loss: 0.00086498
Iteration 13/25 | Loss: 0.00086498
Iteration 14/25 | Loss: 0.00086498
Iteration 15/25 | Loss: 0.00086498
Iteration 16/25 | Loss: 0.00086498
Iteration 17/25 | Loss: 0.00086498
Iteration 18/25 | Loss: 0.00086498
Iteration 19/25 | Loss: 0.00086498
Iteration 20/25 | Loss: 0.00086498
Iteration 21/25 | Loss: 0.00086498
Iteration 22/25 | Loss: 0.00086498
Iteration 23/25 | Loss: 0.00086498
Iteration 24/25 | Loss: 0.00086498
Iteration 25/25 | Loss: 0.00086498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086498
Iteration 2/1000 | Loss: 0.00009819
Iteration 3/1000 | Loss: 0.00005142
Iteration 4/1000 | Loss: 0.00003598
Iteration 5/1000 | Loss: 0.00003268
Iteration 6/1000 | Loss: 0.00003029
Iteration 7/1000 | Loss: 0.00002889
Iteration 8/1000 | Loss: 0.00002766
Iteration 9/1000 | Loss: 0.00002624
Iteration 10/1000 | Loss: 0.00002527
Iteration 11/1000 | Loss: 0.00002468
Iteration 12/1000 | Loss: 0.00002429
Iteration 13/1000 | Loss: 0.00002399
Iteration 14/1000 | Loss: 0.00002374
Iteration 15/1000 | Loss: 0.00002359
Iteration 16/1000 | Loss: 0.00002357
Iteration 17/1000 | Loss: 0.00002352
Iteration 18/1000 | Loss: 0.00002351
Iteration 19/1000 | Loss: 0.00002348
Iteration 20/1000 | Loss: 0.00002347
Iteration 21/1000 | Loss: 0.00002339
Iteration 22/1000 | Loss: 0.00002331
Iteration 23/1000 | Loss: 0.00002331
Iteration 24/1000 | Loss: 0.00002329
Iteration 25/1000 | Loss: 0.00002324
Iteration 26/1000 | Loss: 0.00002320
Iteration 27/1000 | Loss: 0.00002320
Iteration 28/1000 | Loss: 0.00002320
Iteration 29/1000 | Loss: 0.00002320
Iteration 30/1000 | Loss: 0.00002320
Iteration 31/1000 | Loss: 0.00002320
Iteration 32/1000 | Loss: 0.00002320
Iteration 33/1000 | Loss: 0.00002319
Iteration 34/1000 | Loss: 0.00002319
Iteration 35/1000 | Loss: 0.00002317
Iteration 36/1000 | Loss: 0.00002317
Iteration 37/1000 | Loss: 0.00002315
Iteration 38/1000 | Loss: 0.00002314
Iteration 39/1000 | Loss: 0.00002314
Iteration 40/1000 | Loss: 0.00002314
Iteration 41/1000 | Loss: 0.00002311
Iteration 42/1000 | Loss: 0.00002310
Iteration 43/1000 | Loss: 0.00002310
Iteration 44/1000 | Loss: 0.00002309
Iteration 45/1000 | Loss: 0.00002309
Iteration 46/1000 | Loss: 0.00002309
Iteration 47/1000 | Loss: 0.00002308
Iteration 48/1000 | Loss: 0.00002308
Iteration 49/1000 | Loss: 0.00002308
Iteration 50/1000 | Loss: 0.00002307
Iteration 51/1000 | Loss: 0.00002305
Iteration 52/1000 | Loss: 0.00002305
Iteration 53/1000 | Loss: 0.00002305
Iteration 54/1000 | Loss: 0.00002305
Iteration 55/1000 | Loss: 0.00002304
Iteration 56/1000 | Loss: 0.00002304
Iteration 57/1000 | Loss: 0.00002304
Iteration 58/1000 | Loss: 0.00002304
Iteration 59/1000 | Loss: 0.00002304
Iteration 60/1000 | Loss: 0.00002303
Iteration 61/1000 | Loss: 0.00002303
Iteration 62/1000 | Loss: 0.00002301
Iteration 63/1000 | Loss: 0.00002301
Iteration 64/1000 | Loss: 0.00002301
Iteration 65/1000 | Loss: 0.00002301
Iteration 66/1000 | Loss: 0.00002301
Iteration 67/1000 | Loss: 0.00002301
Iteration 68/1000 | Loss: 0.00002301
Iteration 69/1000 | Loss: 0.00002300
Iteration 70/1000 | Loss: 0.00002300
Iteration 71/1000 | Loss: 0.00002300
Iteration 72/1000 | Loss: 0.00002299
Iteration 73/1000 | Loss: 0.00002299
Iteration 74/1000 | Loss: 0.00002299
Iteration 75/1000 | Loss: 0.00002299
Iteration 76/1000 | Loss: 0.00002299
Iteration 77/1000 | Loss: 0.00002299
Iteration 78/1000 | Loss: 0.00002299
Iteration 79/1000 | Loss: 0.00002298
Iteration 80/1000 | Loss: 0.00002298
Iteration 81/1000 | Loss: 0.00002298
Iteration 82/1000 | Loss: 0.00002297
Iteration 83/1000 | Loss: 0.00002297
Iteration 84/1000 | Loss: 0.00002297
Iteration 85/1000 | Loss: 0.00002297
Iteration 86/1000 | Loss: 0.00002296
Iteration 87/1000 | Loss: 0.00002296
Iteration 88/1000 | Loss: 0.00002296
Iteration 89/1000 | Loss: 0.00002296
Iteration 90/1000 | Loss: 0.00002296
Iteration 91/1000 | Loss: 0.00002295
Iteration 92/1000 | Loss: 0.00002295
Iteration 93/1000 | Loss: 0.00002294
Iteration 94/1000 | Loss: 0.00002294
Iteration 95/1000 | Loss: 0.00002294
Iteration 96/1000 | Loss: 0.00002294
Iteration 97/1000 | Loss: 0.00002294
Iteration 98/1000 | Loss: 0.00002293
Iteration 99/1000 | Loss: 0.00002293
Iteration 100/1000 | Loss: 0.00002293
Iteration 101/1000 | Loss: 0.00002293
Iteration 102/1000 | Loss: 0.00002293
Iteration 103/1000 | Loss: 0.00002293
Iteration 104/1000 | Loss: 0.00002293
Iteration 105/1000 | Loss: 0.00002293
Iteration 106/1000 | Loss: 0.00002293
Iteration 107/1000 | Loss: 0.00002293
Iteration 108/1000 | Loss: 0.00002293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.293145917064976e-05, 2.293145917064976e-05, 2.293145917064976e-05, 2.293145917064976e-05, 2.293145917064976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.293145917064976e-05

Optimization complete. Final v2v error: 3.8670694828033447 mm

Highest mean error: 5.926932334899902 mm for frame 61

Lowest mean error: 3.210789680480957 mm for frame 87

Saving results

Total time: 41.562052965164185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886465
Iteration 2/25 | Loss: 0.00144397
Iteration 3/25 | Loss: 0.00127998
Iteration 4/25 | Loss: 0.00124307
Iteration 5/25 | Loss: 0.00123655
Iteration 6/25 | Loss: 0.00123496
Iteration 7/25 | Loss: 0.00123480
Iteration 8/25 | Loss: 0.00123480
Iteration 9/25 | Loss: 0.00123480
Iteration 10/25 | Loss: 0.00123480
Iteration 11/25 | Loss: 0.00123480
Iteration 12/25 | Loss: 0.00123480
Iteration 13/25 | Loss: 0.00123480
Iteration 14/25 | Loss: 0.00123480
Iteration 15/25 | Loss: 0.00123480
Iteration 16/25 | Loss: 0.00123480
Iteration 17/25 | Loss: 0.00123480
Iteration 18/25 | Loss: 0.00123480
Iteration 19/25 | Loss: 0.00123480
Iteration 20/25 | Loss: 0.00123480
Iteration 21/25 | Loss: 0.00123480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001234796829521656, 0.001234796829521656, 0.001234796829521656, 0.001234796829521656, 0.001234796829521656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001234796829521656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28008366
Iteration 2/25 | Loss: 0.00108405
Iteration 3/25 | Loss: 0.00108404
Iteration 4/25 | Loss: 0.00108404
Iteration 5/25 | Loss: 0.00108404
Iteration 6/25 | Loss: 0.00108404
Iteration 7/25 | Loss: 0.00108404
Iteration 8/25 | Loss: 0.00108404
Iteration 9/25 | Loss: 0.00108404
Iteration 10/25 | Loss: 0.00108404
Iteration 11/25 | Loss: 0.00108404
Iteration 12/25 | Loss: 0.00108404
Iteration 13/25 | Loss: 0.00108404
Iteration 14/25 | Loss: 0.00108404
Iteration 15/25 | Loss: 0.00108404
Iteration 16/25 | Loss: 0.00108404
Iteration 17/25 | Loss: 0.00108404
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010840424802154303, 0.0010840424802154303, 0.0010840424802154303, 0.0010840424802154303, 0.0010840424802154303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010840424802154303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108404
Iteration 2/1000 | Loss: 0.00008841
Iteration 3/1000 | Loss: 0.00003839
Iteration 4/1000 | Loss: 0.00002975
Iteration 5/1000 | Loss: 0.00002685
Iteration 6/1000 | Loss: 0.00002525
Iteration 7/1000 | Loss: 0.00002398
Iteration 8/1000 | Loss: 0.00002322
Iteration 9/1000 | Loss: 0.00002254
Iteration 10/1000 | Loss: 0.00002198
Iteration 11/1000 | Loss: 0.00002160
Iteration 12/1000 | Loss: 0.00002128
Iteration 13/1000 | Loss: 0.00002104
Iteration 14/1000 | Loss: 0.00002097
Iteration 15/1000 | Loss: 0.00002097
Iteration 16/1000 | Loss: 0.00002096
Iteration 17/1000 | Loss: 0.00002095
Iteration 18/1000 | Loss: 0.00002095
Iteration 19/1000 | Loss: 0.00002094
Iteration 20/1000 | Loss: 0.00002094
Iteration 21/1000 | Loss: 0.00002094
Iteration 22/1000 | Loss: 0.00002093
Iteration 23/1000 | Loss: 0.00002092
Iteration 24/1000 | Loss: 0.00002090
Iteration 25/1000 | Loss: 0.00002090
Iteration 26/1000 | Loss: 0.00002090
Iteration 27/1000 | Loss: 0.00002090
Iteration 28/1000 | Loss: 0.00002090
Iteration 29/1000 | Loss: 0.00002090
Iteration 30/1000 | Loss: 0.00002090
Iteration 31/1000 | Loss: 0.00002089
Iteration 32/1000 | Loss: 0.00002089
Iteration 33/1000 | Loss: 0.00002089
Iteration 34/1000 | Loss: 0.00002089
Iteration 35/1000 | Loss: 0.00002089
Iteration 36/1000 | Loss: 0.00002089
Iteration 37/1000 | Loss: 0.00002089
Iteration 38/1000 | Loss: 0.00002089
Iteration 39/1000 | Loss: 0.00002089
Iteration 40/1000 | Loss: 0.00002088
Iteration 41/1000 | Loss: 0.00002087
Iteration 42/1000 | Loss: 0.00002086
Iteration 43/1000 | Loss: 0.00002085
Iteration 44/1000 | Loss: 0.00002084
Iteration 45/1000 | Loss: 0.00002083
Iteration 46/1000 | Loss: 0.00002082
Iteration 47/1000 | Loss: 0.00002080
Iteration 48/1000 | Loss: 0.00002080
Iteration 49/1000 | Loss: 0.00002075
Iteration 50/1000 | Loss: 0.00002074
Iteration 51/1000 | Loss: 0.00002074
Iteration 52/1000 | Loss: 0.00002073
Iteration 53/1000 | Loss: 0.00002073
Iteration 54/1000 | Loss: 0.00002073
Iteration 55/1000 | Loss: 0.00002073
Iteration 56/1000 | Loss: 0.00002073
Iteration 57/1000 | Loss: 0.00002072
Iteration 58/1000 | Loss: 0.00002072
Iteration 59/1000 | Loss: 0.00002072
Iteration 60/1000 | Loss: 0.00002072
Iteration 61/1000 | Loss: 0.00002072
Iteration 62/1000 | Loss: 0.00002072
Iteration 63/1000 | Loss: 0.00002072
Iteration 64/1000 | Loss: 0.00002072
Iteration 65/1000 | Loss: 0.00002071
Iteration 66/1000 | Loss: 0.00002071
Iteration 67/1000 | Loss: 0.00002071
Iteration 68/1000 | Loss: 0.00002071
Iteration 69/1000 | Loss: 0.00002070
Iteration 70/1000 | Loss: 0.00002070
Iteration 71/1000 | Loss: 0.00002068
Iteration 72/1000 | Loss: 0.00002068
Iteration 73/1000 | Loss: 0.00002068
Iteration 74/1000 | Loss: 0.00002067
Iteration 75/1000 | Loss: 0.00002067
Iteration 76/1000 | Loss: 0.00002067
Iteration 77/1000 | Loss: 0.00002066
Iteration 78/1000 | Loss: 0.00002066
Iteration 79/1000 | Loss: 0.00002066
Iteration 80/1000 | Loss: 0.00002066
Iteration 81/1000 | Loss: 0.00002066
Iteration 82/1000 | Loss: 0.00002066
Iteration 83/1000 | Loss: 0.00002065
Iteration 84/1000 | Loss: 0.00002065
Iteration 85/1000 | Loss: 0.00002065
Iteration 86/1000 | Loss: 0.00002065
Iteration 87/1000 | Loss: 0.00002065
Iteration 88/1000 | Loss: 0.00002065
Iteration 89/1000 | Loss: 0.00002065
Iteration 90/1000 | Loss: 0.00002065
Iteration 91/1000 | Loss: 0.00002064
Iteration 92/1000 | Loss: 0.00002064
Iteration 93/1000 | Loss: 0.00002064
Iteration 94/1000 | Loss: 0.00002064
Iteration 95/1000 | Loss: 0.00002064
Iteration 96/1000 | Loss: 0.00002064
Iteration 97/1000 | Loss: 0.00002064
Iteration 98/1000 | Loss: 0.00002064
Iteration 99/1000 | Loss: 0.00002064
Iteration 100/1000 | Loss: 0.00002064
Iteration 101/1000 | Loss: 0.00002064
Iteration 102/1000 | Loss: 0.00002064
Iteration 103/1000 | Loss: 0.00002063
Iteration 104/1000 | Loss: 0.00002063
Iteration 105/1000 | Loss: 0.00002063
Iteration 106/1000 | Loss: 0.00002063
Iteration 107/1000 | Loss: 0.00002063
Iteration 108/1000 | Loss: 0.00002062
Iteration 109/1000 | Loss: 0.00002062
Iteration 110/1000 | Loss: 0.00002062
Iteration 111/1000 | Loss: 0.00002062
Iteration 112/1000 | Loss: 0.00002062
Iteration 113/1000 | Loss: 0.00002062
Iteration 114/1000 | Loss: 0.00002062
Iteration 115/1000 | Loss: 0.00002062
Iteration 116/1000 | Loss: 0.00002062
Iteration 117/1000 | Loss: 0.00002062
Iteration 118/1000 | Loss: 0.00002062
Iteration 119/1000 | Loss: 0.00002062
Iteration 120/1000 | Loss: 0.00002062
Iteration 121/1000 | Loss: 0.00002062
Iteration 122/1000 | Loss: 0.00002061
Iteration 123/1000 | Loss: 0.00002061
Iteration 124/1000 | Loss: 0.00002061
Iteration 125/1000 | Loss: 0.00002061
Iteration 126/1000 | Loss: 0.00002061
Iteration 127/1000 | Loss: 0.00002061
Iteration 128/1000 | Loss: 0.00002061
Iteration 129/1000 | Loss: 0.00002061
Iteration 130/1000 | Loss: 0.00002061
Iteration 131/1000 | Loss: 0.00002061
Iteration 132/1000 | Loss: 0.00002061
Iteration 133/1000 | Loss: 0.00002061
Iteration 134/1000 | Loss: 0.00002061
Iteration 135/1000 | Loss: 0.00002061
Iteration 136/1000 | Loss: 0.00002061
Iteration 137/1000 | Loss: 0.00002061
Iteration 138/1000 | Loss: 0.00002061
Iteration 139/1000 | Loss: 0.00002061
Iteration 140/1000 | Loss: 0.00002061
Iteration 141/1000 | Loss: 0.00002061
Iteration 142/1000 | Loss: 0.00002061
Iteration 143/1000 | Loss: 0.00002061
Iteration 144/1000 | Loss: 0.00002061
Iteration 145/1000 | Loss: 0.00002061
Iteration 146/1000 | Loss: 0.00002061
Iteration 147/1000 | Loss: 0.00002061
Iteration 148/1000 | Loss: 0.00002061
Iteration 149/1000 | Loss: 0.00002061
Iteration 150/1000 | Loss: 0.00002061
Iteration 151/1000 | Loss: 0.00002061
Iteration 152/1000 | Loss: 0.00002061
Iteration 153/1000 | Loss: 0.00002061
Iteration 154/1000 | Loss: 0.00002061
Iteration 155/1000 | Loss: 0.00002061
Iteration 156/1000 | Loss: 0.00002061
Iteration 157/1000 | Loss: 0.00002061
Iteration 158/1000 | Loss: 0.00002061
Iteration 159/1000 | Loss: 0.00002061
Iteration 160/1000 | Loss: 0.00002061
Iteration 161/1000 | Loss: 0.00002061
Iteration 162/1000 | Loss: 0.00002061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.0609228158718906e-05, 2.0609228158718906e-05, 2.0609228158718906e-05, 2.0609228158718906e-05, 2.0609228158718906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0609228158718906e-05

Optimization complete. Final v2v error: 3.8717434406280518 mm

Highest mean error: 4.011029243469238 mm for frame 69

Lowest mean error: 3.757227659225464 mm for frame 50

Saving results

Total time: 37.78051424026489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041764
Iteration 2/25 | Loss: 0.00182408
Iteration 3/25 | Loss: 0.00141890
Iteration 4/25 | Loss: 0.00138505
Iteration 5/25 | Loss: 0.00137374
Iteration 6/25 | Loss: 0.00137202
Iteration 7/25 | Loss: 0.00137202
Iteration 8/25 | Loss: 0.00137202
Iteration 9/25 | Loss: 0.00137202
Iteration 10/25 | Loss: 0.00137202
Iteration 11/25 | Loss: 0.00137202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013720174320042133, 0.0013720174320042133, 0.0013720174320042133, 0.0013720174320042133, 0.0013720174320042133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013720174320042133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.51442909
Iteration 2/25 | Loss: 0.00097462
Iteration 3/25 | Loss: 0.00097462
Iteration 4/25 | Loss: 0.00097462
Iteration 5/25 | Loss: 0.00097462
Iteration 6/25 | Loss: 0.00097462
Iteration 7/25 | Loss: 0.00097462
Iteration 8/25 | Loss: 0.00097462
Iteration 9/25 | Loss: 0.00097462
Iteration 10/25 | Loss: 0.00097462
Iteration 11/25 | Loss: 0.00097462
Iteration 12/25 | Loss: 0.00097462
Iteration 13/25 | Loss: 0.00097462
Iteration 14/25 | Loss: 0.00097462
Iteration 15/25 | Loss: 0.00097462
Iteration 16/25 | Loss: 0.00097462
Iteration 17/25 | Loss: 0.00097462
Iteration 18/25 | Loss: 0.00097462
Iteration 19/25 | Loss: 0.00097462
Iteration 20/25 | Loss: 0.00097462
Iteration 21/25 | Loss: 0.00097462
Iteration 22/25 | Loss: 0.00097462
Iteration 23/25 | Loss: 0.00097462
Iteration 24/25 | Loss: 0.00097462
Iteration 25/25 | Loss: 0.00097462

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097462
Iteration 2/1000 | Loss: 0.00008615
Iteration 3/1000 | Loss: 0.00005887
Iteration 4/1000 | Loss: 0.00005278
Iteration 5/1000 | Loss: 0.00004892
Iteration 6/1000 | Loss: 0.00004736
Iteration 7/1000 | Loss: 0.00004608
Iteration 8/1000 | Loss: 0.00004540
Iteration 9/1000 | Loss: 0.00004485
Iteration 10/1000 | Loss: 0.00004431
Iteration 11/1000 | Loss: 0.00004395
Iteration 12/1000 | Loss: 0.00004369
Iteration 13/1000 | Loss: 0.00004349
Iteration 14/1000 | Loss: 0.00004325
Iteration 15/1000 | Loss: 0.00004303
Iteration 16/1000 | Loss: 0.00004284
Iteration 17/1000 | Loss: 0.00004266
Iteration 18/1000 | Loss: 0.00004255
Iteration 19/1000 | Loss: 0.00004236
Iteration 20/1000 | Loss: 0.00004221
Iteration 21/1000 | Loss: 0.00004207
Iteration 22/1000 | Loss: 0.00004206
Iteration 23/1000 | Loss: 0.00004202
Iteration 24/1000 | Loss: 0.00004202
Iteration 25/1000 | Loss: 0.00004195
Iteration 26/1000 | Loss: 0.00004195
Iteration 27/1000 | Loss: 0.00004194
Iteration 28/1000 | Loss: 0.00004193
Iteration 29/1000 | Loss: 0.00004191
Iteration 30/1000 | Loss: 0.00004189
Iteration 31/1000 | Loss: 0.00004189
Iteration 32/1000 | Loss: 0.00004189
Iteration 33/1000 | Loss: 0.00004189
Iteration 34/1000 | Loss: 0.00004189
Iteration 35/1000 | Loss: 0.00004189
Iteration 36/1000 | Loss: 0.00004188
Iteration 37/1000 | Loss: 0.00004188
Iteration 38/1000 | Loss: 0.00004188
Iteration 39/1000 | Loss: 0.00004188
Iteration 40/1000 | Loss: 0.00004188
Iteration 41/1000 | Loss: 0.00004188
Iteration 42/1000 | Loss: 0.00004188
Iteration 43/1000 | Loss: 0.00004186
Iteration 44/1000 | Loss: 0.00004186
Iteration 45/1000 | Loss: 0.00004185
Iteration 46/1000 | Loss: 0.00004185
Iteration 47/1000 | Loss: 0.00004184
Iteration 48/1000 | Loss: 0.00004184
Iteration 49/1000 | Loss: 0.00004182
Iteration 50/1000 | Loss: 0.00004181
Iteration 51/1000 | Loss: 0.00004181
Iteration 52/1000 | Loss: 0.00004180
Iteration 53/1000 | Loss: 0.00004180
Iteration 54/1000 | Loss: 0.00004179
Iteration 55/1000 | Loss: 0.00004179
Iteration 56/1000 | Loss: 0.00004178
Iteration 57/1000 | Loss: 0.00004178
Iteration 58/1000 | Loss: 0.00004178
Iteration 59/1000 | Loss: 0.00004178
Iteration 60/1000 | Loss: 0.00004177
Iteration 61/1000 | Loss: 0.00004177
Iteration 62/1000 | Loss: 0.00004177
Iteration 63/1000 | Loss: 0.00004177
Iteration 64/1000 | Loss: 0.00004177
Iteration 65/1000 | Loss: 0.00004176
Iteration 66/1000 | Loss: 0.00004176
Iteration 67/1000 | Loss: 0.00004176
Iteration 68/1000 | Loss: 0.00004175
Iteration 69/1000 | Loss: 0.00004175
Iteration 70/1000 | Loss: 0.00004175
Iteration 71/1000 | Loss: 0.00004175
Iteration 72/1000 | Loss: 0.00004174
Iteration 73/1000 | Loss: 0.00004174
Iteration 74/1000 | Loss: 0.00004174
Iteration 75/1000 | Loss: 0.00004174
Iteration 76/1000 | Loss: 0.00004174
Iteration 77/1000 | Loss: 0.00004174
Iteration 78/1000 | Loss: 0.00004173
Iteration 79/1000 | Loss: 0.00004173
Iteration 80/1000 | Loss: 0.00004173
Iteration 81/1000 | Loss: 0.00004173
Iteration 82/1000 | Loss: 0.00004173
Iteration 83/1000 | Loss: 0.00004173
Iteration 84/1000 | Loss: 0.00004173
Iteration 85/1000 | Loss: 0.00004172
Iteration 86/1000 | Loss: 0.00004171
Iteration 87/1000 | Loss: 0.00004171
Iteration 88/1000 | Loss: 0.00004171
Iteration 89/1000 | Loss: 0.00004170
Iteration 90/1000 | Loss: 0.00004170
Iteration 91/1000 | Loss: 0.00004170
Iteration 92/1000 | Loss: 0.00004170
Iteration 93/1000 | Loss: 0.00004170
Iteration 94/1000 | Loss: 0.00004169
Iteration 95/1000 | Loss: 0.00004169
Iteration 96/1000 | Loss: 0.00004169
Iteration 97/1000 | Loss: 0.00004169
Iteration 98/1000 | Loss: 0.00004169
Iteration 99/1000 | Loss: 0.00004169
Iteration 100/1000 | Loss: 0.00004169
Iteration 101/1000 | Loss: 0.00004169
Iteration 102/1000 | Loss: 0.00004169
Iteration 103/1000 | Loss: 0.00004168
Iteration 104/1000 | Loss: 0.00004168
Iteration 105/1000 | Loss: 0.00004168
Iteration 106/1000 | Loss: 0.00004168
Iteration 107/1000 | Loss: 0.00004168
Iteration 108/1000 | Loss: 0.00004168
Iteration 109/1000 | Loss: 0.00004168
Iteration 110/1000 | Loss: 0.00004168
Iteration 111/1000 | Loss: 0.00004167
Iteration 112/1000 | Loss: 0.00004167
Iteration 113/1000 | Loss: 0.00004167
Iteration 114/1000 | Loss: 0.00004167
Iteration 115/1000 | Loss: 0.00004167
Iteration 116/1000 | Loss: 0.00004167
Iteration 117/1000 | Loss: 0.00004167
Iteration 118/1000 | Loss: 0.00004167
Iteration 119/1000 | Loss: 0.00004167
Iteration 120/1000 | Loss: 0.00004166
Iteration 121/1000 | Loss: 0.00004166
Iteration 122/1000 | Loss: 0.00004166
Iteration 123/1000 | Loss: 0.00004166
Iteration 124/1000 | Loss: 0.00004166
Iteration 125/1000 | Loss: 0.00004166
Iteration 126/1000 | Loss: 0.00004166
Iteration 127/1000 | Loss: 0.00004166
Iteration 128/1000 | Loss: 0.00004166
Iteration 129/1000 | Loss: 0.00004166
Iteration 130/1000 | Loss: 0.00004166
Iteration 131/1000 | Loss: 0.00004166
Iteration 132/1000 | Loss: 0.00004165
Iteration 133/1000 | Loss: 0.00004165
Iteration 134/1000 | Loss: 0.00004165
Iteration 135/1000 | Loss: 0.00004165
Iteration 136/1000 | Loss: 0.00004165
Iteration 137/1000 | Loss: 0.00004165
Iteration 138/1000 | Loss: 0.00004165
Iteration 139/1000 | Loss: 0.00004165
Iteration 140/1000 | Loss: 0.00004165
Iteration 141/1000 | Loss: 0.00004165
Iteration 142/1000 | Loss: 0.00004164
Iteration 143/1000 | Loss: 0.00004164
Iteration 144/1000 | Loss: 0.00004164
Iteration 145/1000 | Loss: 0.00004164
Iteration 146/1000 | Loss: 0.00004164
Iteration 147/1000 | Loss: 0.00004164
Iteration 148/1000 | Loss: 0.00004164
Iteration 149/1000 | Loss: 0.00004164
Iteration 150/1000 | Loss: 0.00004164
Iteration 151/1000 | Loss: 0.00004164
Iteration 152/1000 | Loss: 0.00004164
Iteration 153/1000 | Loss: 0.00004164
Iteration 154/1000 | Loss: 0.00004164
Iteration 155/1000 | Loss: 0.00004164
Iteration 156/1000 | Loss: 0.00004164
Iteration 157/1000 | Loss: 0.00004164
Iteration 158/1000 | Loss: 0.00004164
Iteration 159/1000 | Loss: 0.00004164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [4.163892663200386e-05, 4.163892663200386e-05, 4.163892663200386e-05, 4.163892663200386e-05, 4.163892663200386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.163892663200386e-05

Optimization complete. Final v2v error: 5.141737461090088 mm

Highest mean error: 6.491627216339111 mm for frame 14

Lowest mean error: 4.762840270996094 mm for frame 79

Saving results

Total time: 51.857181549072266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1431/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1431/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058019
Iteration 2/25 | Loss: 0.00261700
Iteration 3/25 | Loss: 0.00173578
Iteration 4/25 | Loss: 0.00159281
Iteration 5/25 | Loss: 0.00167725
Iteration 6/25 | Loss: 0.00136626
Iteration 7/25 | Loss: 0.00129223
Iteration 8/25 | Loss: 0.00122781
Iteration 9/25 | Loss: 0.00120420
Iteration 10/25 | Loss: 0.00119916
Iteration 11/25 | Loss: 0.00119864
Iteration 12/25 | Loss: 0.00119671
Iteration 13/25 | Loss: 0.00119648
Iteration 14/25 | Loss: 0.00120433
Iteration 15/25 | Loss: 0.00120218
Iteration 16/25 | Loss: 0.00119894
Iteration 17/25 | Loss: 0.00119327
Iteration 18/25 | Loss: 0.00119238
Iteration 19/25 | Loss: 0.00119292
Iteration 20/25 | Loss: 0.00119309
Iteration 21/25 | Loss: 0.00119617
Iteration 22/25 | Loss: 0.00119184
Iteration 23/25 | Loss: 0.00119177
Iteration 24/25 | Loss: 0.00119177
Iteration 25/25 | Loss: 0.00119177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32641757
Iteration 2/25 | Loss: 0.00095188
Iteration 3/25 | Loss: 0.00084263
Iteration 4/25 | Loss: 0.00084234
Iteration 5/25 | Loss: 0.00083873
Iteration 6/25 | Loss: 0.00083873
Iteration 7/25 | Loss: 0.00083873
Iteration 8/25 | Loss: 0.00083873
Iteration 9/25 | Loss: 0.00083873
Iteration 10/25 | Loss: 0.00083873
Iteration 11/25 | Loss: 0.00083873
Iteration 12/25 | Loss: 0.00083873
Iteration 13/25 | Loss: 0.00083873
Iteration 14/25 | Loss: 0.00083873
Iteration 15/25 | Loss: 0.00083873
Iteration 16/25 | Loss: 0.00083873
Iteration 17/25 | Loss: 0.00083873
Iteration 18/25 | Loss: 0.00083873
Iteration 19/25 | Loss: 0.00083873
Iteration 20/25 | Loss: 0.00083873
Iteration 21/25 | Loss: 0.00083873
Iteration 22/25 | Loss: 0.00083873
Iteration 23/25 | Loss: 0.00083873
Iteration 24/25 | Loss: 0.00083873
Iteration 25/25 | Loss: 0.00083873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083873
Iteration 2/1000 | Loss: 0.00012389
Iteration 3/1000 | Loss: 0.00003077
Iteration 4/1000 | Loss: 0.00007434
Iteration 5/1000 | Loss: 0.00010159
Iteration 6/1000 | Loss: 0.00005799
Iteration 7/1000 | Loss: 0.00016408
Iteration 8/1000 | Loss: 0.00002358
Iteration 9/1000 | Loss: 0.00012387
Iteration 10/1000 | Loss: 0.00065384
Iteration 11/1000 | Loss: 0.00005923
Iteration 12/1000 | Loss: 0.00021008
Iteration 13/1000 | Loss: 0.00029850
Iteration 14/1000 | Loss: 0.00002893
Iteration 15/1000 | Loss: 0.00003567
Iteration 16/1000 | Loss: 0.00002902
Iteration 17/1000 | Loss: 0.00003565
Iteration 18/1000 | Loss: 0.00003044
Iteration 19/1000 | Loss: 0.00001773
Iteration 20/1000 | Loss: 0.00002211
Iteration 21/1000 | Loss: 0.00002418
Iteration 22/1000 | Loss: 0.00003406
Iteration 23/1000 | Loss: 0.00001746
Iteration 24/1000 | Loss: 0.00001587
Iteration 25/1000 | Loss: 0.00002625
Iteration 26/1000 | Loss: 0.00003400
Iteration 27/1000 | Loss: 0.00001555
Iteration 28/1000 | Loss: 0.00001920
Iteration 29/1000 | Loss: 0.00001550
Iteration 30/1000 | Loss: 0.00001549
Iteration 31/1000 | Loss: 0.00001549
Iteration 32/1000 | Loss: 0.00001549
Iteration 33/1000 | Loss: 0.00001548
Iteration 34/1000 | Loss: 0.00001548
Iteration 35/1000 | Loss: 0.00001547
Iteration 36/1000 | Loss: 0.00001547
Iteration 37/1000 | Loss: 0.00001546
Iteration 38/1000 | Loss: 0.00001546
Iteration 39/1000 | Loss: 0.00001546
Iteration 40/1000 | Loss: 0.00001546
Iteration 41/1000 | Loss: 0.00001546
Iteration 42/1000 | Loss: 0.00001546
Iteration 43/1000 | Loss: 0.00001546
Iteration 44/1000 | Loss: 0.00001546
Iteration 45/1000 | Loss: 0.00001545
Iteration 46/1000 | Loss: 0.00001545
Iteration 47/1000 | Loss: 0.00001545
Iteration 48/1000 | Loss: 0.00001545
Iteration 49/1000 | Loss: 0.00001544
Iteration 50/1000 | Loss: 0.00001544
Iteration 51/1000 | Loss: 0.00001543
Iteration 52/1000 | Loss: 0.00002185
Iteration 53/1000 | Loss: 0.00003273
Iteration 54/1000 | Loss: 0.00004316
Iteration 55/1000 | Loss: 0.00001542
Iteration 56/1000 | Loss: 0.00002100
Iteration 57/1000 | Loss: 0.00004197
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001539
Iteration 60/1000 | Loss: 0.00001539
Iteration 61/1000 | Loss: 0.00001539
Iteration 62/1000 | Loss: 0.00001539
Iteration 63/1000 | Loss: 0.00001593
Iteration 64/1000 | Loss: 0.00001538
Iteration 65/1000 | Loss: 0.00001538
Iteration 66/1000 | Loss: 0.00001538
Iteration 67/1000 | Loss: 0.00001538
Iteration 68/1000 | Loss: 0.00001538
Iteration 69/1000 | Loss: 0.00001538
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001538
Iteration 73/1000 | Loss: 0.00001538
Iteration 74/1000 | Loss: 0.00001538
Iteration 75/1000 | Loss: 0.00001538
Iteration 76/1000 | Loss: 0.00001538
Iteration 77/1000 | Loss: 0.00001537
Iteration 78/1000 | Loss: 0.00001537
Iteration 79/1000 | Loss: 0.00001884
Iteration 80/1000 | Loss: 0.00001577
Iteration 81/1000 | Loss: 0.00001538
Iteration 82/1000 | Loss: 0.00001538
Iteration 83/1000 | Loss: 0.00001538
Iteration 84/1000 | Loss: 0.00001542
Iteration 85/1000 | Loss: 0.00001538
Iteration 86/1000 | Loss: 0.00001538
Iteration 87/1000 | Loss: 0.00001538
Iteration 88/1000 | Loss: 0.00001538
Iteration 89/1000 | Loss: 0.00001538
Iteration 90/1000 | Loss: 0.00001537
Iteration 91/1000 | Loss: 0.00001537
Iteration 92/1000 | Loss: 0.00001537
Iteration 93/1000 | Loss: 0.00001537
Iteration 94/1000 | Loss: 0.00001537
Iteration 95/1000 | Loss: 0.00001541
Iteration 96/1000 | Loss: 0.00001535
Iteration 97/1000 | Loss: 0.00001535
Iteration 98/1000 | Loss: 0.00001535
Iteration 99/1000 | Loss: 0.00001535
Iteration 100/1000 | Loss: 0.00001535
Iteration 101/1000 | Loss: 0.00001535
Iteration 102/1000 | Loss: 0.00001535
Iteration 103/1000 | Loss: 0.00001535
Iteration 104/1000 | Loss: 0.00001535
Iteration 105/1000 | Loss: 0.00001535
Iteration 106/1000 | Loss: 0.00001535
Iteration 107/1000 | Loss: 0.00001535
Iteration 108/1000 | Loss: 0.00001535
Iteration 109/1000 | Loss: 0.00001535
Iteration 110/1000 | Loss: 0.00001535
Iteration 111/1000 | Loss: 0.00001535
Iteration 112/1000 | Loss: 0.00001535
Iteration 113/1000 | Loss: 0.00001535
Iteration 114/1000 | Loss: 0.00001535
Iteration 115/1000 | Loss: 0.00001535
Iteration 116/1000 | Loss: 0.00001535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.534745388198644e-05, 1.534745388198644e-05, 1.534745388198644e-05, 1.534745388198644e-05, 1.534745388198644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.534745388198644e-05

Optimization complete. Final v2v error: 3.4975972175598145 mm

Highest mean error: 4.087059497833252 mm for frame 92

Lowest mean error: 3.067504405975342 mm for frame 57

Saving results

Total time: 92.99953365325928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913330
Iteration 2/25 | Loss: 0.00193833
Iteration 3/25 | Loss: 0.00131889
Iteration 4/25 | Loss: 0.00119267
Iteration 5/25 | Loss: 0.00116156
Iteration 6/25 | Loss: 0.00115296
Iteration 7/25 | Loss: 0.00116242
Iteration 8/25 | Loss: 0.00117230
Iteration 9/25 | Loss: 0.00116804
Iteration 10/25 | Loss: 0.00114713
Iteration 11/25 | Loss: 0.00112646
Iteration 12/25 | Loss: 0.00112079
Iteration 13/25 | Loss: 0.00112196
Iteration 14/25 | Loss: 0.00111204
Iteration 15/25 | Loss: 0.00111774
Iteration 16/25 | Loss: 0.00110661
Iteration 17/25 | Loss: 0.00110511
Iteration 18/25 | Loss: 0.00110904
Iteration 19/25 | Loss: 0.00109728
Iteration 20/25 | Loss: 0.00109472
Iteration 21/25 | Loss: 0.00109335
Iteration 22/25 | Loss: 0.00109349
Iteration 23/25 | Loss: 0.00109379
Iteration 24/25 | Loss: 0.00109166
Iteration 25/25 | Loss: 0.00109072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.42926311
Iteration 2/25 | Loss: 0.00033946
Iteration 3/25 | Loss: 0.00033943
Iteration 4/25 | Loss: 0.00033943
Iteration 5/25 | Loss: 0.00033943
Iteration 6/25 | Loss: 0.00033943
Iteration 7/25 | Loss: 0.00033943
Iteration 8/25 | Loss: 0.00033943
Iteration 9/25 | Loss: 0.00033943
Iteration 10/25 | Loss: 0.00033942
Iteration 11/25 | Loss: 0.00033942
Iteration 12/25 | Loss: 0.00033942
Iteration 13/25 | Loss: 0.00033942
Iteration 14/25 | Loss: 0.00033942
Iteration 15/25 | Loss: 0.00033942
Iteration 16/25 | Loss: 0.00033942
Iteration 17/25 | Loss: 0.00033942
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000339424645062536, 0.000339424645062536, 0.000339424645062536, 0.000339424645062536, 0.000339424645062536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000339424645062536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033942
Iteration 2/1000 | Loss: 0.00004194
Iteration 3/1000 | Loss: 0.00003022
Iteration 4/1000 | Loss: 0.00002772
Iteration 5/1000 | Loss: 0.00002647
Iteration 6/1000 | Loss: 0.00002546
Iteration 7/1000 | Loss: 0.00002499
Iteration 8/1000 | Loss: 0.00002470
Iteration 9/1000 | Loss: 0.00002443
Iteration 10/1000 | Loss: 0.00002427
Iteration 11/1000 | Loss: 0.00032337
Iteration 12/1000 | Loss: 0.00029458
Iteration 13/1000 | Loss: 0.00005218
Iteration 14/1000 | Loss: 0.00005486
Iteration 15/1000 | Loss: 0.00003177
Iteration 16/1000 | Loss: 0.00003756
Iteration 17/1000 | Loss: 0.00002767
Iteration 18/1000 | Loss: 0.00002685
Iteration 19/1000 | Loss: 0.00002627
Iteration 20/1000 | Loss: 0.00016798
Iteration 21/1000 | Loss: 0.00010137
Iteration 22/1000 | Loss: 0.00003001
Iteration 23/1000 | Loss: 0.00002668
Iteration 24/1000 | Loss: 0.00002568
Iteration 25/1000 | Loss: 0.00002545
Iteration 26/1000 | Loss: 0.00020572
Iteration 27/1000 | Loss: 0.00011825
Iteration 28/1000 | Loss: 0.00017953
Iteration 29/1000 | Loss: 0.00010917
Iteration 30/1000 | Loss: 0.00003569
Iteration 31/1000 | Loss: 0.00003632
Iteration 32/1000 | Loss: 0.00002807
Iteration 33/1000 | Loss: 0.00015363
Iteration 34/1000 | Loss: 0.00006805
Iteration 35/1000 | Loss: 0.00002606
Iteration 36/1000 | Loss: 0.00018955
Iteration 37/1000 | Loss: 0.00015458
Iteration 38/1000 | Loss: 0.00005758
Iteration 39/1000 | Loss: 0.00007165
Iteration 40/1000 | Loss: 0.00014983
Iteration 41/1000 | Loss: 0.00006016
Iteration 42/1000 | Loss: 0.00004830
Iteration 43/1000 | Loss: 0.00009752
Iteration 44/1000 | Loss: 0.00012277
Iteration 45/1000 | Loss: 0.00016179
Iteration 46/1000 | Loss: 0.00008864
Iteration 47/1000 | Loss: 0.00003014
Iteration 48/1000 | Loss: 0.00003758
Iteration 49/1000 | Loss: 0.00008982
Iteration 50/1000 | Loss: 0.00007256
Iteration 51/1000 | Loss: 0.00016052
Iteration 52/1000 | Loss: 0.00003403
Iteration 53/1000 | Loss: 0.00003725
Iteration 54/1000 | Loss: 0.00002672
Iteration 55/1000 | Loss: 0.00002589
Iteration 56/1000 | Loss: 0.00020640
Iteration 57/1000 | Loss: 0.00002896
Iteration 58/1000 | Loss: 0.00002668
Iteration 59/1000 | Loss: 0.00002596
Iteration 60/1000 | Loss: 0.00002550
Iteration 61/1000 | Loss: 0.00002529
Iteration 62/1000 | Loss: 0.00002506
Iteration 63/1000 | Loss: 0.00002504
Iteration 64/1000 | Loss: 0.00002496
Iteration 65/1000 | Loss: 0.00022482
Iteration 66/1000 | Loss: 0.00019880
Iteration 67/1000 | Loss: 0.00011657
Iteration 68/1000 | Loss: 0.00013383
Iteration 69/1000 | Loss: 0.00011263
Iteration 70/1000 | Loss: 0.00013048
Iteration 71/1000 | Loss: 0.00009822
Iteration 72/1000 | Loss: 0.00019785
Iteration 73/1000 | Loss: 0.00040145
Iteration 74/1000 | Loss: 0.00024276
Iteration 75/1000 | Loss: 0.00039934
Iteration 76/1000 | Loss: 0.00005294
Iteration 77/1000 | Loss: 0.00003684
Iteration 78/1000 | Loss: 0.00003173
Iteration 79/1000 | Loss: 0.00002904
Iteration 80/1000 | Loss: 0.00002781
Iteration 81/1000 | Loss: 0.00002689
Iteration 82/1000 | Loss: 0.00002649
Iteration 83/1000 | Loss: 0.00002650
Iteration 84/1000 | Loss: 0.00002596
Iteration 85/1000 | Loss: 0.00002577
Iteration 86/1000 | Loss: 0.00002575
Iteration 87/1000 | Loss: 0.00002551
Iteration 88/1000 | Loss: 0.00002533
Iteration 89/1000 | Loss: 0.00002531
Iteration 90/1000 | Loss: 0.00003234
Iteration 91/1000 | Loss: 0.00004389
Iteration 92/1000 | Loss: 0.00013340
Iteration 93/1000 | Loss: 0.00004440
Iteration 94/1000 | Loss: 0.00003934
Iteration 95/1000 | Loss: 0.00004179
Iteration 96/1000 | Loss: 0.00003385
Iteration 97/1000 | Loss: 0.00002940
Iteration 98/1000 | Loss: 0.00002795
Iteration 99/1000 | Loss: 0.00002598
Iteration 100/1000 | Loss: 0.00002470
Iteration 101/1000 | Loss: 0.00002391
Iteration 102/1000 | Loss: 0.00002333
Iteration 103/1000 | Loss: 0.00002310
Iteration 104/1000 | Loss: 0.00002298
Iteration 105/1000 | Loss: 0.00002284
Iteration 106/1000 | Loss: 0.00002281
Iteration 107/1000 | Loss: 0.00002280
Iteration 108/1000 | Loss: 0.00002280
Iteration 109/1000 | Loss: 0.00002280
Iteration 110/1000 | Loss: 0.00002280
Iteration 111/1000 | Loss: 0.00002279
Iteration 112/1000 | Loss: 0.00002279
Iteration 113/1000 | Loss: 0.00002278
Iteration 114/1000 | Loss: 0.00002276
Iteration 115/1000 | Loss: 0.00002276
Iteration 116/1000 | Loss: 0.00002275
Iteration 117/1000 | Loss: 0.00002275
Iteration 118/1000 | Loss: 0.00002275
Iteration 119/1000 | Loss: 0.00002275
Iteration 120/1000 | Loss: 0.00002275
Iteration 121/1000 | Loss: 0.00002275
Iteration 122/1000 | Loss: 0.00002275
Iteration 123/1000 | Loss: 0.00002275
Iteration 124/1000 | Loss: 0.00002274
Iteration 125/1000 | Loss: 0.00002274
Iteration 126/1000 | Loss: 0.00002274
Iteration 127/1000 | Loss: 0.00002274
Iteration 128/1000 | Loss: 0.00002274
Iteration 129/1000 | Loss: 0.00002274
Iteration 130/1000 | Loss: 0.00002274
Iteration 131/1000 | Loss: 0.00002274
Iteration 132/1000 | Loss: 0.00002273
Iteration 133/1000 | Loss: 0.00002273
Iteration 134/1000 | Loss: 0.00002273
Iteration 135/1000 | Loss: 0.00002273
Iteration 136/1000 | Loss: 0.00002273
Iteration 137/1000 | Loss: 0.00002273
Iteration 138/1000 | Loss: 0.00002273
Iteration 139/1000 | Loss: 0.00002273
Iteration 140/1000 | Loss: 0.00002273
Iteration 141/1000 | Loss: 0.00002273
Iteration 142/1000 | Loss: 0.00002273
Iteration 143/1000 | Loss: 0.00002272
Iteration 144/1000 | Loss: 0.00002272
Iteration 145/1000 | Loss: 0.00002272
Iteration 146/1000 | Loss: 0.00002272
Iteration 147/1000 | Loss: 0.00002272
Iteration 148/1000 | Loss: 0.00002271
Iteration 149/1000 | Loss: 0.00002271
Iteration 150/1000 | Loss: 0.00002271
Iteration 151/1000 | Loss: 0.00002271
Iteration 152/1000 | Loss: 0.00002271
Iteration 153/1000 | Loss: 0.00002271
Iteration 154/1000 | Loss: 0.00002271
Iteration 155/1000 | Loss: 0.00002271
Iteration 156/1000 | Loss: 0.00002270
Iteration 157/1000 | Loss: 0.00002270
Iteration 158/1000 | Loss: 0.00002270
Iteration 159/1000 | Loss: 0.00002270
Iteration 160/1000 | Loss: 0.00002270
Iteration 161/1000 | Loss: 0.00002269
Iteration 162/1000 | Loss: 0.00002269
Iteration 163/1000 | Loss: 0.00002269
Iteration 164/1000 | Loss: 0.00002268
Iteration 165/1000 | Loss: 0.00002268
Iteration 166/1000 | Loss: 0.00002268
Iteration 167/1000 | Loss: 0.00002268
Iteration 168/1000 | Loss: 0.00002268
Iteration 169/1000 | Loss: 0.00002268
Iteration 170/1000 | Loss: 0.00002268
Iteration 171/1000 | Loss: 0.00002268
Iteration 172/1000 | Loss: 0.00002268
Iteration 173/1000 | Loss: 0.00002267
Iteration 174/1000 | Loss: 0.00002267
Iteration 175/1000 | Loss: 0.00002267
Iteration 176/1000 | Loss: 0.00002267
Iteration 177/1000 | Loss: 0.00002267
Iteration 178/1000 | Loss: 0.00002266
Iteration 179/1000 | Loss: 0.00002266
Iteration 180/1000 | Loss: 0.00002265
Iteration 181/1000 | Loss: 0.00002265
Iteration 182/1000 | Loss: 0.00002264
Iteration 183/1000 | Loss: 0.00002264
Iteration 184/1000 | Loss: 0.00002264
Iteration 185/1000 | Loss: 0.00002264
Iteration 186/1000 | Loss: 0.00002264
Iteration 187/1000 | Loss: 0.00002264
Iteration 188/1000 | Loss: 0.00002264
Iteration 189/1000 | Loss: 0.00002263
Iteration 190/1000 | Loss: 0.00002263
Iteration 191/1000 | Loss: 0.00002263
Iteration 192/1000 | Loss: 0.00002263
Iteration 193/1000 | Loss: 0.00002263
Iteration 194/1000 | Loss: 0.00002263
Iteration 195/1000 | Loss: 0.00002263
Iteration 196/1000 | Loss: 0.00002263
Iteration 197/1000 | Loss: 0.00002263
Iteration 198/1000 | Loss: 0.00002262
Iteration 199/1000 | Loss: 0.00002262
Iteration 200/1000 | Loss: 0.00002262
Iteration 201/1000 | Loss: 0.00002262
Iteration 202/1000 | Loss: 0.00002261
Iteration 203/1000 | Loss: 0.00002261
Iteration 204/1000 | Loss: 0.00002261
Iteration 205/1000 | Loss: 0.00002261
Iteration 206/1000 | Loss: 0.00002261
Iteration 207/1000 | Loss: 0.00002261
Iteration 208/1000 | Loss: 0.00002261
Iteration 209/1000 | Loss: 0.00002261
Iteration 210/1000 | Loss: 0.00002261
Iteration 211/1000 | Loss: 0.00002261
Iteration 212/1000 | Loss: 0.00002261
Iteration 213/1000 | Loss: 0.00002261
Iteration 214/1000 | Loss: 0.00002261
Iteration 215/1000 | Loss: 0.00002261
Iteration 216/1000 | Loss: 0.00002261
Iteration 217/1000 | Loss: 0.00002261
Iteration 218/1000 | Loss: 0.00002261
Iteration 219/1000 | Loss: 0.00002260
Iteration 220/1000 | Loss: 0.00002260
Iteration 221/1000 | Loss: 0.00002260
Iteration 222/1000 | Loss: 0.00002260
Iteration 223/1000 | Loss: 0.00002260
Iteration 224/1000 | Loss: 0.00002260
Iteration 225/1000 | Loss: 0.00002260
Iteration 226/1000 | Loss: 0.00002260
Iteration 227/1000 | Loss: 0.00002260
Iteration 228/1000 | Loss: 0.00002259
Iteration 229/1000 | Loss: 0.00002259
Iteration 230/1000 | Loss: 0.00002259
Iteration 231/1000 | Loss: 0.00002259
Iteration 232/1000 | Loss: 0.00002259
Iteration 233/1000 | Loss: 0.00002259
Iteration 234/1000 | Loss: 0.00002259
Iteration 235/1000 | Loss: 0.00002259
Iteration 236/1000 | Loss: 0.00002259
Iteration 237/1000 | Loss: 0.00002259
Iteration 238/1000 | Loss: 0.00002259
Iteration 239/1000 | Loss: 0.00002259
Iteration 240/1000 | Loss: 0.00002259
Iteration 241/1000 | Loss: 0.00002259
Iteration 242/1000 | Loss: 0.00002259
Iteration 243/1000 | Loss: 0.00002258
Iteration 244/1000 | Loss: 0.00002258
Iteration 245/1000 | Loss: 0.00002258
Iteration 246/1000 | Loss: 0.00002258
Iteration 247/1000 | Loss: 0.00002258
Iteration 248/1000 | Loss: 0.00002258
Iteration 249/1000 | Loss: 0.00002258
Iteration 250/1000 | Loss: 0.00002258
Iteration 251/1000 | Loss: 0.00002258
Iteration 252/1000 | Loss: 0.00002258
Iteration 253/1000 | Loss: 0.00002258
Iteration 254/1000 | Loss: 0.00002258
Iteration 255/1000 | Loss: 0.00002258
Iteration 256/1000 | Loss: 0.00002258
Iteration 257/1000 | Loss: 0.00002258
Iteration 258/1000 | Loss: 0.00002258
Iteration 259/1000 | Loss: 0.00002258
Iteration 260/1000 | Loss: 0.00002258
Iteration 261/1000 | Loss: 0.00002258
Iteration 262/1000 | Loss: 0.00002258
Iteration 263/1000 | Loss: 0.00002258
Iteration 264/1000 | Loss: 0.00002258
Iteration 265/1000 | Loss: 0.00002257
Iteration 266/1000 | Loss: 0.00002257
Iteration 267/1000 | Loss: 0.00002257
Iteration 268/1000 | Loss: 0.00002257
Iteration 269/1000 | Loss: 0.00002257
Iteration 270/1000 | Loss: 0.00002257
Iteration 271/1000 | Loss: 0.00002257
Iteration 272/1000 | Loss: 0.00002257
Iteration 273/1000 | Loss: 0.00002257
Iteration 274/1000 | Loss: 0.00002257
Iteration 275/1000 | Loss: 0.00002257
Iteration 276/1000 | Loss: 0.00002257
Iteration 277/1000 | Loss: 0.00002257
Iteration 278/1000 | Loss: 0.00002257
Iteration 279/1000 | Loss: 0.00002257
Iteration 280/1000 | Loss: 0.00002257
Iteration 281/1000 | Loss: 0.00002257
Iteration 282/1000 | Loss: 0.00002257
Iteration 283/1000 | Loss: 0.00002257
Iteration 284/1000 | Loss: 0.00002257
Iteration 285/1000 | Loss: 0.00002257
Iteration 286/1000 | Loss: 0.00002257
Iteration 287/1000 | Loss: 0.00002257
Iteration 288/1000 | Loss: 0.00002257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [2.257445339637343e-05, 2.257445339637343e-05, 2.257445339637343e-05, 2.257445339637343e-05, 2.257445339637343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.257445339637343e-05

Optimization complete. Final v2v error: 3.8377785682678223 mm

Highest mean error: 8.329505920410156 mm for frame 9

Lowest mean error: 3.3017847537994385 mm for frame 203

Saving results

Total time: 225.41166710853577
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924638
Iteration 2/25 | Loss: 0.00144180
Iteration 3/25 | Loss: 0.00119941
Iteration 4/25 | Loss: 0.00116625
Iteration 5/25 | Loss: 0.00115911
Iteration 6/25 | Loss: 0.00115737
Iteration 7/25 | Loss: 0.00115709
Iteration 8/25 | Loss: 0.00115709
Iteration 9/25 | Loss: 0.00115709
Iteration 10/25 | Loss: 0.00115709
Iteration 11/25 | Loss: 0.00115709
Iteration 12/25 | Loss: 0.00115709
Iteration 13/25 | Loss: 0.00115709
Iteration 14/25 | Loss: 0.00115709
Iteration 15/25 | Loss: 0.00115709
Iteration 16/25 | Loss: 0.00115709
Iteration 17/25 | Loss: 0.00115709
Iteration 18/25 | Loss: 0.00115709
Iteration 19/25 | Loss: 0.00115706
Iteration 20/25 | Loss: 0.00115706
Iteration 21/25 | Loss: 0.00115706
Iteration 22/25 | Loss: 0.00115706
Iteration 23/25 | Loss: 0.00115706
Iteration 24/25 | Loss: 0.00115706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011570628266781569, 0.0011570628266781569, 0.0011570628266781569, 0.0011570628266781569, 0.0011570628266781569]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011570628266781569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29131758
Iteration 2/25 | Loss: 0.00053065
Iteration 3/25 | Loss: 0.00053065
Iteration 4/25 | Loss: 0.00053065
Iteration 5/25 | Loss: 0.00053065
Iteration 6/25 | Loss: 0.00053065
Iteration 7/25 | Loss: 0.00053065
Iteration 8/25 | Loss: 0.00053065
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.0005306453676894307, 0.0005306453676894307, 0.0005306453676894307, 0.0005306453676894307, 0.0005306453676894307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005306453676894307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053065
Iteration 2/1000 | Loss: 0.00005954
Iteration 3/1000 | Loss: 0.00004436
Iteration 4/1000 | Loss: 0.00003715
Iteration 5/1000 | Loss: 0.00003458
Iteration 6/1000 | Loss: 0.00003325
Iteration 7/1000 | Loss: 0.00003220
Iteration 8/1000 | Loss: 0.00003161
Iteration 9/1000 | Loss: 0.00003099
Iteration 10/1000 | Loss: 0.00003054
Iteration 11/1000 | Loss: 0.00003027
Iteration 12/1000 | Loss: 0.00003003
Iteration 13/1000 | Loss: 0.00002984
Iteration 14/1000 | Loss: 0.00002976
Iteration 15/1000 | Loss: 0.00002964
Iteration 16/1000 | Loss: 0.00002950
Iteration 17/1000 | Loss: 0.00002950
Iteration 18/1000 | Loss: 0.00002950
Iteration 19/1000 | Loss: 0.00002949
Iteration 20/1000 | Loss: 0.00002949
Iteration 21/1000 | Loss: 0.00002949
Iteration 22/1000 | Loss: 0.00002948
Iteration 23/1000 | Loss: 0.00002948
Iteration 24/1000 | Loss: 0.00002944
Iteration 25/1000 | Loss: 0.00002943
Iteration 26/1000 | Loss: 0.00002943
Iteration 27/1000 | Loss: 0.00002943
Iteration 28/1000 | Loss: 0.00002942
Iteration 29/1000 | Loss: 0.00002942
Iteration 30/1000 | Loss: 0.00002941
Iteration 31/1000 | Loss: 0.00002940
Iteration 32/1000 | Loss: 0.00002939
Iteration 33/1000 | Loss: 0.00002937
Iteration 34/1000 | Loss: 0.00002937
Iteration 35/1000 | Loss: 0.00002937
Iteration 36/1000 | Loss: 0.00002937
Iteration 37/1000 | Loss: 0.00002937
Iteration 38/1000 | Loss: 0.00002937
Iteration 39/1000 | Loss: 0.00002937
Iteration 40/1000 | Loss: 0.00002937
Iteration 41/1000 | Loss: 0.00002937
Iteration 42/1000 | Loss: 0.00002936
Iteration 43/1000 | Loss: 0.00002933
Iteration 44/1000 | Loss: 0.00002933
Iteration 45/1000 | Loss: 0.00002933
Iteration 46/1000 | Loss: 0.00002933
Iteration 47/1000 | Loss: 0.00002932
Iteration 48/1000 | Loss: 0.00002932
Iteration 49/1000 | Loss: 0.00002932
Iteration 50/1000 | Loss: 0.00002931
Iteration 51/1000 | Loss: 0.00002931
Iteration 52/1000 | Loss: 0.00002931
Iteration 53/1000 | Loss: 0.00002930
Iteration 54/1000 | Loss: 0.00002930
Iteration 55/1000 | Loss: 0.00002929
Iteration 56/1000 | Loss: 0.00002929
Iteration 57/1000 | Loss: 0.00002929
Iteration 58/1000 | Loss: 0.00002929
Iteration 59/1000 | Loss: 0.00002928
Iteration 60/1000 | Loss: 0.00002928
Iteration 61/1000 | Loss: 0.00002928
Iteration 62/1000 | Loss: 0.00002928
Iteration 63/1000 | Loss: 0.00002928
Iteration 64/1000 | Loss: 0.00002928
Iteration 65/1000 | Loss: 0.00002928
Iteration 66/1000 | Loss: 0.00002928
Iteration 67/1000 | Loss: 0.00002928
Iteration 68/1000 | Loss: 0.00002928
Iteration 69/1000 | Loss: 0.00002928
Iteration 70/1000 | Loss: 0.00002928
Iteration 71/1000 | Loss: 0.00002928
Iteration 72/1000 | Loss: 0.00002927
Iteration 73/1000 | Loss: 0.00002927
Iteration 74/1000 | Loss: 0.00002927
Iteration 75/1000 | Loss: 0.00002927
Iteration 76/1000 | Loss: 0.00002926
Iteration 77/1000 | Loss: 0.00002926
Iteration 78/1000 | Loss: 0.00002926
Iteration 79/1000 | Loss: 0.00002925
Iteration 80/1000 | Loss: 0.00002925
Iteration 81/1000 | Loss: 0.00002925
Iteration 82/1000 | Loss: 0.00002925
Iteration 83/1000 | Loss: 0.00002924
Iteration 84/1000 | Loss: 0.00002924
Iteration 85/1000 | Loss: 0.00002924
Iteration 86/1000 | Loss: 0.00002924
Iteration 87/1000 | Loss: 0.00002923
Iteration 88/1000 | Loss: 0.00002923
Iteration 89/1000 | Loss: 0.00002923
Iteration 90/1000 | Loss: 0.00002922
Iteration 91/1000 | Loss: 0.00002922
Iteration 92/1000 | Loss: 0.00002922
Iteration 93/1000 | Loss: 0.00002922
Iteration 94/1000 | Loss: 0.00002922
Iteration 95/1000 | Loss: 0.00002922
Iteration 96/1000 | Loss: 0.00002921
Iteration 97/1000 | Loss: 0.00002921
Iteration 98/1000 | Loss: 0.00002921
Iteration 99/1000 | Loss: 0.00002921
Iteration 100/1000 | Loss: 0.00002921
Iteration 101/1000 | Loss: 0.00002920
Iteration 102/1000 | Loss: 0.00002920
Iteration 103/1000 | Loss: 0.00002920
Iteration 104/1000 | Loss: 0.00002919
Iteration 105/1000 | Loss: 0.00002919
Iteration 106/1000 | Loss: 0.00002919
Iteration 107/1000 | Loss: 0.00002919
Iteration 108/1000 | Loss: 0.00002919
Iteration 109/1000 | Loss: 0.00002919
Iteration 110/1000 | Loss: 0.00002918
Iteration 111/1000 | Loss: 0.00002918
Iteration 112/1000 | Loss: 0.00002918
Iteration 113/1000 | Loss: 0.00002917
Iteration 114/1000 | Loss: 0.00002917
Iteration 115/1000 | Loss: 0.00002917
Iteration 116/1000 | Loss: 0.00002917
Iteration 117/1000 | Loss: 0.00002917
Iteration 118/1000 | Loss: 0.00002917
Iteration 119/1000 | Loss: 0.00002917
Iteration 120/1000 | Loss: 0.00002916
Iteration 121/1000 | Loss: 0.00002916
Iteration 122/1000 | Loss: 0.00002916
Iteration 123/1000 | Loss: 0.00002916
Iteration 124/1000 | Loss: 0.00002915
Iteration 125/1000 | Loss: 0.00002915
Iteration 126/1000 | Loss: 0.00002915
Iteration 127/1000 | Loss: 0.00002914
Iteration 128/1000 | Loss: 0.00002914
Iteration 129/1000 | Loss: 0.00002914
Iteration 130/1000 | Loss: 0.00002913
Iteration 131/1000 | Loss: 0.00002913
Iteration 132/1000 | Loss: 0.00002913
Iteration 133/1000 | Loss: 0.00002913
Iteration 134/1000 | Loss: 0.00002913
Iteration 135/1000 | Loss: 0.00002913
Iteration 136/1000 | Loss: 0.00002913
Iteration 137/1000 | Loss: 0.00002913
Iteration 138/1000 | Loss: 0.00002912
Iteration 139/1000 | Loss: 0.00002912
Iteration 140/1000 | Loss: 0.00002912
Iteration 141/1000 | Loss: 0.00002912
Iteration 142/1000 | Loss: 0.00002912
Iteration 143/1000 | Loss: 0.00002912
Iteration 144/1000 | Loss: 0.00002912
Iteration 145/1000 | Loss: 0.00002912
Iteration 146/1000 | Loss: 0.00002911
Iteration 147/1000 | Loss: 0.00002911
Iteration 148/1000 | Loss: 0.00002911
Iteration 149/1000 | Loss: 0.00002911
Iteration 150/1000 | Loss: 0.00002911
Iteration 151/1000 | Loss: 0.00002911
Iteration 152/1000 | Loss: 0.00002911
Iteration 153/1000 | Loss: 0.00002911
Iteration 154/1000 | Loss: 0.00002911
Iteration 155/1000 | Loss: 0.00002911
Iteration 156/1000 | Loss: 0.00002911
Iteration 157/1000 | Loss: 0.00002911
Iteration 158/1000 | Loss: 0.00002911
Iteration 159/1000 | Loss: 0.00002911
Iteration 160/1000 | Loss: 0.00002911
Iteration 161/1000 | Loss: 0.00002911
Iteration 162/1000 | Loss: 0.00002910
Iteration 163/1000 | Loss: 0.00002910
Iteration 164/1000 | Loss: 0.00002910
Iteration 165/1000 | Loss: 0.00002910
Iteration 166/1000 | Loss: 0.00002910
Iteration 167/1000 | Loss: 0.00002910
Iteration 168/1000 | Loss: 0.00002910
Iteration 169/1000 | Loss: 0.00002910
Iteration 170/1000 | Loss: 0.00002910
Iteration 171/1000 | Loss: 0.00002910
Iteration 172/1000 | Loss: 0.00002910
Iteration 173/1000 | Loss: 0.00002910
Iteration 174/1000 | Loss: 0.00002910
Iteration 175/1000 | Loss: 0.00002910
Iteration 176/1000 | Loss: 0.00002910
Iteration 177/1000 | Loss: 0.00002910
Iteration 178/1000 | Loss: 0.00002910
Iteration 179/1000 | Loss: 0.00002910
Iteration 180/1000 | Loss: 0.00002910
Iteration 181/1000 | Loss: 0.00002910
Iteration 182/1000 | Loss: 0.00002910
Iteration 183/1000 | Loss: 0.00002910
Iteration 184/1000 | Loss: 0.00002910
Iteration 185/1000 | Loss: 0.00002910
Iteration 186/1000 | Loss: 0.00002910
Iteration 187/1000 | Loss: 0.00002910
Iteration 188/1000 | Loss: 0.00002910
Iteration 189/1000 | Loss: 0.00002910
Iteration 190/1000 | Loss: 0.00002910
Iteration 191/1000 | Loss: 0.00002910
Iteration 192/1000 | Loss: 0.00002910
Iteration 193/1000 | Loss: 0.00002910
Iteration 194/1000 | Loss: 0.00002910
Iteration 195/1000 | Loss: 0.00002910
Iteration 196/1000 | Loss: 0.00002910
Iteration 197/1000 | Loss: 0.00002910
Iteration 198/1000 | Loss: 0.00002910
Iteration 199/1000 | Loss: 0.00002910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.910413058998529e-05, 2.910413058998529e-05, 2.910413058998529e-05, 2.910413058998529e-05, 2.910413058998529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.910413058998529e-05

Optimization complete. Final v2v error: 4.497048377990723 mm

Highest mean error: 4.928073406219482 mm for frame 25

Lowest mean error: 4.048843860626221 mm for frame 81

Saving results

Total time: 41.890504360198975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00505884
Iteration 2/25 | Loss: 0.00112257
Iteration 3/25 | Loss: 0.00101755
Iteration 4/25 | Loss: 0.00100584
Iteration 5/25 | Loss: 0.00100377
Iteration 6/25 | Loss: 0.00100312
Iteration 7/25 | Loss: 0.00100308
Iteration 8/25 | Loss: 0.00100308
Iteration 9/25 | Loss: 0.00100308
Iteration 10/25 | Loss: 0.00100308
Iteration 11/25 | Loss: 0.00100308
Iteration 12/25 | Loss: 0.00100308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010030822595581412, 0.0010030822595581412, 0.0010030822595581412, 0.0010030822595581412, 0.0010030822595581412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010030822595581412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24495423
Iteration 2/25 | Loss: 0.00042342
Iteration 3/25 | Loss: 0.00042339
Iteration 4/25 | Loss: 0.00042339
Iteration 5/25 | Loss: 0.00042339
Iteration 6/25 | Loss: 0.00042339
Iteration 7/25 | Loss: 0.00042339
Iteration 8/25 | Loss: 0.00042339
Iteration 9/25 | Loss: 0.00042339
Iteration 10/25 | Loss: 0.00042339
Iteration 11/25 | Loss: 0.00042339
Iteration 12/25 | Loss: 0.00042339
Iteration 13/25 | Loss: 0.00042339
Iteration 14/25 | Loss: 0.00042339
Iteration 15/25 | Loss: 0.00042339
Iteration 16/25 | Loss: 0.00042339
Iteration 17/25 | Loss: 0.00042339
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004233883519191295, 0.0004233883519191295, 0.0004233883519191295, 0.0004233883519191295, 0.0004233883519191295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004233883519191295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042339
Iteration 2/1000 | Loss: 0.00003541
Iteration 3/1000 | Loss: 0.00002362
Iteration 4/1000 | Loss: 0.00001921
Iteration 5/1000 | Loss: 0.00001785
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001647
Iteration 8/1000 | Loss: 0.00001605
Iteration 9/1000 | Loss: 0.00001575
Iteration 10/1000 | Loss: 0.00001556
Iteration 11/1000 | Loss: 0.00001543
Iteration 12/1000 | Loss: 0.00001520
Iteration 13/1000 | Loss: 0.00001506
Iteration 14/1000 | Loss: 0.00001495
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00001482
Iteration 17/1000 | Loss: 0.00001482
Iteration 18/1000 | Loss: 0.00001482
Iteration 19/1000 | Loss: 0.00001481
Iteration 20/1000 | Loss: 0.00001481
Iteration 21/1000 | Loss: 0.00001481
Iteration 22/1000 | Loss: 0.00001481
Iteration 23/1000 | Loss: 0.00001481
Iteration 24/1000 | Loss: 0.00001481
Iteration 25/1000 | Loss: 0.00001480
Iteration 26/1000 | Loss: 0.00001480
Iteration 27/1000 | Loss: 0.00001479
Iteration 28/1000 | Loss: 0.00001479
Iteration 29/1000 | Loss: 0.00001479
Iteration 30/1000 | Loss: 0.00001479
Iteration 31/1000 | Loss: 0.00001478
Iteration 32/1000 | Loss: 0.00001478
Iteration 33/1000 | Loss: 0.00001478
Iteration 34/1000 | Loss: 0.00001478
Iteration 35/1000 | Loss: 0.00001478
Iteration 36/1000 | Loss: 0.00001478
Iteration 37/1000 | Loss: 0.00001478
Iteration 38/1000 | Loss: 0.00001478
Iteration 39/1000 | Loss: 0.00001478
Iteration 40/1000 | Loss: 0.00001478
Iteration 41/1000 | Loss: 0.00001478
Iteration 42/1000 | Loss: 0.00001478
Iteration 43/1000 | Loss: 0.00001478
Iteration 44/1000 | Loss: 0.00001477
Iteration 45/1000 | Loss: 0.00001477
Iteration 46/1000 | Loss: 0.00001477
Iteration 47/1000 | Loss: 0.00001477
Iteration 48/1000 | Loss: 0.00001477
Iteration 49/1000 | Loss: 0.00001477
Iteration 50/1000 | Loss: 0.00001477
Iteration 51/1000 | Loss: 0.00001477
Iteration 52/1000 | Loss: 0.00001477
Iteration 53/1000 | Loss: 0.00001477
Iteration 54/1000 | Loss: 0.00001477
Iteration 55/1000 | Loss: 0.00001477
Iteration 56/1000 | Loss: 0.00001477
Iteration 57/1000 | Loss: 0.00001477
Iteration 58/1000 | Loss: 0.00001477
Iteration 59/1000 | Loss: 0.00001477
Iteration 60/1000 | Loss: 0.00001477
Iteration 61/1000 | Loss: 0.00001477
Iteration 62/1000 | Loss: 0.00001477
Iteration 63/1000 | Loss: 0.00001477
Iteration 64/1000 | Loss: 0.00001477
Iteration 65/1000 | Loss: 0.00001477
Iteration 66/1000 | Loss: 0.00001477
Iteration 67/1000 | Loss: 0.00001477
Iteration 68/1000 | Loss: 0.00001477
Iteration 69/1000 | Loss: 0.00001477
Iteration 70/1000 | Loss: 0.00001477
Iteration 71/1000 | Loss: 0.00001477
Iteration 72/1000 | Loss: 0.00001477
Iteration 73/1000 | Loss: 0.00001477
Iteration 74/1000 | Loss: 0.00001477
Iteration 75/1000 | Loss: 0.00001477
Iteration 76/1000 | Loss: 0.00001477
Iteration 77/1000 | Loss: 0.00001477
Iteration 78/1000 | Loss: 0.00001477
Iteration 79/1000 | Loss: 0.00001477
Iteration 80/1000 | Loss: 0.00001477
Iteration 81/1000 | Loss: 0.00001477
Iteration 82/1000 | Loss: 0.00001477
Iteration 83/1000 | Loss: 0.00001477
Iteration 84/1000 | Loss: 0.00001477
Iteration 85/1000 | Loss: 0.00001477
Iteration 86/1000 | Loss: 0.00001477
Iteration 87/1000 | Loss: 0.00001477
Iteration 88/1000 | Loss: 0.00001477
Iteration 89/1000 | Loss: 0.00001477
Iteration 90/1000 | Loss: 0.00001477
Iteration 91/1000 | Loss: 0.00001477
Iteration 92/1000 | Loss: 0.00001477
Iteration 93/1000 | Loss: 0.00001477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.4774992450838909e-05, 1.4774992450838909e-05, 1.4774992450838909e-05, 1.4774992450838909e-05, 1.4774992450838909e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4774992450838909e-05

Optimization complete. Final v2v error: 3.3223330974578857 mm

Highest mean error: 3.7945170402526855 mm for frame 109

Lowest mean error: 2.8359222412109375 mm for frame 134

Saving results

Total time: 31.59007430076599
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410003
Iteration 2/25 | Loss: 0.00112800
Iteration 3/25 | Loss: 0.00099521
Iteration 4/25 | Loss: 0.00098394
Iteration 5/25 | Loss: 0.00098153
Iteration 6/25 | Loss: 0.00098092
Iteration 7/25 | Loss: 0.00098092
Iteration 8/25 | Loss: 0.00098092
Iteration 9/25 | Loss: 0.00098092
Iteration 10/25 | Loss: 0.00098092
Iteration 11/25 | Loss: 0.00098092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009809177136048675, 0.0009809177136048675, 0.0009809177136048675, 0.0009809177136048675, 0.0009809177136048675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009809177136048675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31878269
Iteration 2/25 | Loss: 0.00048145
Iteration 3/25 | Loss: 0.00048145
Iteration 4/25 | Loss: 0.00048145
Iteration 5/25 | Loss: 0.00048145
Iteration 6/25 | Loss: 0.00048145
Iteration 7/25 | Loss: 0.00048145
Iteration 8/25 | Loss: 0.00048145
Iteration 9/25 | Loss: 0.00048145
Iteration 10/25 | Loss: 0.00048145
Iteration 11/25 | Loss: 0.00048145
Iteration 12/25 | Loss: 0.00048145
Iteration 13/25 | Loss: 0.00048145
Iteration 14/25 | Loss: 0.00048145
Iteration 15/25 | Loss: 0.00048145
Iteration 16/25 | Loss: 0.00048145
Iteration 17/25 | Loss: 0.00048145
Iteration 18/25 | Loss: 0.00048145
Iteration 19/25 | Loss: 0.00048145
Iteration 20/25 | Loss: 0.00048145
Iteration 21/25 | Loss: 0.00048145
Iteration 22/25 | Loss: 0.00048145
Iteration 23/25 | Loss: 0.00048145
Iteration 24/25 | Loss: 0.00048145
Iteration 25/25 | Loss: 0.00048145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048145
Iteration 2/1000 | Loss: 0.00002713
Iteration 3/1000 | Loss: 0.00001980
Iteration 4/1000 | Loss: 0.00001696
Iteration 5/1000 | Loss: 0.00001570
Iteration 6/1000 | Loss: 0.00001477
Iteration 7/1000 | Loss: 0.00001430
Iteration 8/1000 | Loss: 0.00001406
Iteration 9/1000 | Loss: 0.00001381
Iteration 10/1000 | Loss: 0.00001360
Iteration 11/1000 | Loss: 0.00001345
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001338
Iteration 14/1000 | Loss: 0.00001338
Iteration 15/1000 | Loss: 0.00001333
Iteration 16/1000 | Loss: 0.00001333
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001331
Iteration 19/1000 | Loss: 0.00001331
Iteration 20/1000 | Loss: 0.00001330
Iteration 21/1000 | Loss: 0.00001329
Iteration 22/1000 | Loss: 0.00001329
Iteration 23/1000 | Loss: 0.00001329
Iteration 24/1000 | Loss: 0.00001328
Iteration 25/1000 | Loss: 0.00001328
Iteration 26/1000 | Loss: 0.00001328
Iteration 27/1000 | Loss: 0.00001328
Iteration 28/1000 | Loss: 0.00001328
Iteration 29/1000 | Loss: 0.00001327
Iteration 30/1000 | Loss: 0.00001326
Iteration 31/1000 | Loss: 0.00001326
Iteration 32/1000 | Loss: 0.00001325
Iteration 33/1000 | Loss: 0.00001325
Iteration 34/1000 | Loss: 0.00001324
Iteration 35/1000 | Loss: 0.00001324
Iteration 36/1000 | Loss: 0.00001324
Iteration 37/1000 | Loss: 0.00001324
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001322
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001321
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001321
Iteration 50/1000 | Loss: 0.00001321
Iteration 51/1000 | Loss: 0.00001321
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001320
Iteration 54/1000 | Loss: 0.00001320
Iteration 55/1000 | Loss: 0.00001320
Iteration 56/1000 | Loss: 0.00001320
Iteration 57/1000 | Loss: 0.00001320
Iteration 58/1000 | Loss: 0.00001320
Iteration 59/1000 | Loss: 0.00001319
Iteration 60/1000 | Loss: 0.00001319
Iteration 61/1000 | Loss: 0.00001318
Iteration 62/1000 | Loss: 0.00001318
Iteration 63/1000 | Loss: 0.00001318
Iteration 64/1000 | Loss: 0.00001318
Iteration 65/1000 | Loss: 0.00001318
Iteration 66/1000 | Loss: 0.00001318
Iteration 67/1000 | Loss: 0.00001317
Iteration 68/1000 | Loss: 0.00001317
Iteration 69/1000 | Loss: 0.00001317
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001316
Iteration 72/1000 | Loss: 0.00001316
Iteration 73/1000 | Loss: 0.00001315
Iteration 74/1000 | Loss: 0.00001315
Iteration 75/1000 | Loss: 0.00001315
Iteration 76/1000 | Loss: 0.00001314
Iteration 77/1000 | Loss: 0.00001314
Iteration 78/1000 | Loss: 0.00001314
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001313
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001312
Iteration 84/1000 | Loss: 0.00001312
Iteration 85/1000 | Loss: 0.00001312
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001311
Iteration 88/1000 | Loss: 0.00001311
Iteration 89/1000 | Loss: 0.00001311
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001310
Iteration 92/1000 | Loss: 0.00001310
Iteration 93/1000 | Loss: 0.00001310
Iteration 94/1000 | Loss: 0.00001310
Iteration 95/1000 | Loss: 0.00001310
Iteration 96/1000 | Loss: 0.00001310
Iteration 97/1000 | Loss: 0.00001310
Iteration 98/1000 | Loss: 0.00001310
Iteration 99/1000 | Loss: 0.00001310
Iteration 100/1000 | Loss: 0.00001310
Iteration 101/1000 | Loss: 0.00001310
Iteration 102/1000 | Loss: 0.00001310
Iteration 103/1000 | Loss: 0.00001310
Iteration 104/1000 | Loss: 0.00001310
Iteration 105/1000 | Loss: 0.00001310
Iteration 106/1000 | Loss: 0.00001310
Iteration 107/1000 | Loss: 0.00001310
Iteration 108/1000 | Loss: 0.00001310
Iteration 109/1000 | Loss: 0.00001310
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001310
Iteration 114/1000 | Loss: 0.00001310
Iteration 115/1000 | Loss: 0.00001310
Iteration 116/1000 | Loss: 0.00001310
Iteration 117/1000 | Loss: 0.00001310
Iteration 118/1000 | Loss: 0.00001310
Iteration 119/1000 | Loss: 0.00001310
Iteration 120/1000 | Loss: 0.00001310
Iteration 121/1000 | Loss: 0.00001310
Iteration 122/1000 | Loss: 0.00001310
Iteration 123/1000 | Loss: 0.00001310
Iteration 124/1000 | Loss: 0.00001310
Iteration 125/1000 | Loss: 0.00001310
Iteration 126/1000 | Loss: 0.00001310
Iteration 127/1000 | Loss: 0.00001310
Iteration 128/1000 | Loss: 0.00001310
Iteration 129/1000 | Loss: 0.00001310
Iteration 130/1000 | Loss: 0.00001310
Iteration 131/1000 | Loss: 0.00001310
Iteration 132/1000 | Loss: 0.00001310
Iteration 133/1000 | Loss: 0.00001310
Iteration 134/1000 | Loss: 0.00001310
Iteration 135/1000 | Loss: 0.00001310
Iteration 136/1000 | Loss: 0.00001310
Iteration 137/1000 | Loss: 0.00001310
Iteration 138/1000 | Loss: 0.00001310
Iteration 139/1000 | Loss: 0.00001310
Iteration 140/1000 | Loss: 0.00001310
Iteration 141/1000 | Loss: 0.00001310
Iteration 142/1000 | Loss: 0.00001310
Iteration 143/1000 | Loss: 0.00001310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.3098020644974895e-05, 1.3098020644974895e-05, 1.3098020644974895e-05, 1.3098020644974895e-05, 1.3098020644974895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3098020644974895e-05

Optimization complete. Final v2v error: 3.093687057495117 mm

Highest mean error: 3.5374791622161865 mm for frame 123

Lowest mean error: 2.4912102222442627 mm for frame 21

Saving results

Total time: 35.414032220840454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088302
Iteration 2/25 | Loss: 0.00209474
Iteration 3/25 | Loss: 0.00169679
Iteration 4/25 | Loss: 0.00139436
Iteration 5/25 | Loss: 0.00121201
Iteration 6/25 | Loss: 0.00121035
Iteration 7/25 | Loss: 0.00115703
Iteration 8/25 | Loss: 0.00109203
Iteration 9/25 | Loss: 0.00104078
Iteration 10/25 | Loss: 0.00100853
Iteration 11/25 | Loss: 0.00102112
Iteration 12/25 | Loss: 0.00101800
Iteration 13/25 | Loss: 0.00100859
Iteration 14/25 | Loss: 0.00101303
Iteration 15/25 | Loss: 0.00100665
Iteration 16/25 | Loss: 0.00100287
Iteration 17/25 | Loss: 0.00099964
Iteration 18/25 | Loss: 0.00099926
Iteration 19/25 | Loss: 0.00098534
Iteration 20/25 | Loss: 0.00097408
Iteration 21/25 | Loss: 0.00096764
Iteration 22/25 | Loss: 0.00096376
Iteration 23/25 | Loss: 0.00096270
Iteration 24/25 | Loss: 0.00096240
Iteration 25/25 | Loss: 0.00096859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32570791
Iteration 2/25 | Loss: 0.00118206
Iteration 3/25 | Loss: 0.00095262
Iteration 4/25 | Loss: 0.00095262
Iteration 5/25 | Loss: 0.00095262
Iteration 6/25 | Loss: 0.00095262
Iteration 7/25 | Loss: 0.00095262
Iteration 8/25 | Loss: 0.00095262
Iteration 9/25 | Loss: 0.00095262
Iteration 10/25 | Loss: 0.00095262
Iteration 11/25 | Loss: 0.00095262
Iteration 12/25 | Loss: 0.00095262
Iteration 13/25 | Loss: 0.00095262
Iteration 14/25 | Loss: 0.00095262
Iteration 15/25 | Loss: 0.00095262
Iteration 16/25 | Loss: 0.00095262
Iteration 17/25 | Loss: 0.00095262
Iteration 18/25 | Loss: 0.00095262
Iteration 19/25 | Loss: 0.00095262
Iteration 20/25 | Loss: 0.00095262
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009526195353828371, 0.0009526195353828371, 0.0009526195353828371, 0.0009526195353828371, 0.0009526195353828371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009526195353828371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095262
Iteration 2/1000 | Loss: 0.00047499
Iteration 3/1000 | Loss: 0.00029082
Iteration 4/1000 | Loss: 0.00030595
Iteration 5/1000 | Loss: 0.00017227
Iteration 6/1000 | Loss: 0.00013898
Iteration 7/1000 | Loss: 0.00008854
Iteration 8/1000 | Loss: 0.00027685
Iteration 9/1000 | Loss: 0.00015165
Iteration 10/1000 | Loss: 0.00014369
Iteration 11/1000 | Loss: 0.00011027
Iteration 12/1000 | Loss: 0.00011279
Iteration 13/1000 | Loss: 0.00009514
Iteration 14/1000 | Loss: 0.00030811
Iteration 15/1000 | Loss: 0.00006231
Iteration 16/1000 | Loss: 0.00007681
Iteration 17/1000 | Loss: 0.00027065
Iteration 18/1000 | Loss: 0.00003599
Iteration 19/1000 | Loss: 0.00002795
Iteration 20/1000 | Loss: 0.00021876
Iteration 21/1000 | Loss: 0.00006495
Iteration 22/1000 | Loss: 0.00003560
Iteration 23/1000 | Loss: 0.00004544
Iteration 24/1000 | Loss: 0.00002584
Iteration 25/1000 | Loss: 0.00008440
Iteration 26/1000 | Loss: 0.00004964
Iteration 27/1000 | Loss: 0.00006181
Iteration 28/1000 | Loss: 0.00002732
Iteration 29/1000 | Loss: 0.00005930
Iteration 30/1000 | Loss: 0.00026707
Iteration 31/1000 | Loss: 0.00021683
Iteration 32/1000 | Loss: 0.00043683
Iteration 33/1000 | Loss: 0.00023000
Iteration 34/1000 | Loss: 0.00034635
Iteration 35/1000 | Loss: 0.00024849
Iteration 36/1000 | Loss: 0.00058645
Iteration 37/1000 | Loss: 0.00023173
Iteration 38/1000 | Loss: 0.00032347
Iteration 39/1000 | Loss: 0.00046903
Iteration 40/1000 | Loss: 0.00025610
Iteration 41/1000 | Loss: 0.00003810
Iteration 42/1000 | Loss: 0.00004396
Iteration 43/1000 | Loss: 0.00002942
Iteration 44/1000 | Loss: 0.00003027
Iteration 45/1000 | Loss: 0.00002493
Iteration 46/1000 | Loss: 0.00023925
Iteration 47/1000 | Loss: 0.00027360
Iteration 48/1000 | Loss: 0.00011235
Iteration 49/1000 | Loss: 0.00003271
Iteration 50/1000 | Loss: 0.00002771
Iteration 51/1000 | Loss: 0.00054074
Iteration 52/1000 | Loss: 0.00035302
Iteration 53/1000 | Loss: 0.00010134
Iteration 54/1000 | Loss: 0.00006204
Iteration 55/1000 | Loss: 0.00002617
Iteration 56/1000 | Loss: 0.00002697
Iteration 57/1000 | Loss: 0.00003383
Iteration 58/1000 | Loss: 0.00001773
Iteration 59/1000 | Loss: 0.00001670
Iteration 60/1000 | Loss: 0.00001608
Iteration 61/1000 | Loss: 0.00001564
Iteration 62/1000 | Loss: 0.00001534
Iteration 63/1000 | Loss: 0.00001515
Iteration 64/1000 | Loss: 0.00001494
Iteration 65/1000 | Loss: 0.00001482
Iteration 66/1000 | Loss: 0.00001466
Iteration 67/1000 | Loss: 0.00001464
Iteration 68/1000 | Loss: 0.00001464
Iteration 69/1000 | Loss: 0.00001453
Iteration 70/1000 | Loss: 0.00001448
Iteration 71/1000 | Loss: 0.00001926
Iteration 72/1000 | Loss: 0.00001455
Iteration 73/1000 | Loss: 0.00001403
Iteration 74/1000 | Loss: 0.00001372
Iteration 75/1000 | Loss: 0.00001368
Iteration 76/1000 | Loss: 0.00001356
Iteration 77/1000 | Loss: 0.00001355
Iteration 78/1000 | Loss: 0.00001354
Iteration 79/1000 | Loss: 0.00001351
Iteration 80/1000 | Loss: 0.00001350
Iteration 81/1000 | Loss: 0.00001343
Iteration 82/1000 | Loss: 0.00001340
Iteration 83/1000 | Loss: 0.00001340
Iteration 84/1000 | Loss: 0.00001339
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001330
Iteration 87/1000 | Loss: 0.00001328
Iteration 88/1000 | Loss: 0.00001328
Iteration 89/1000 | Loss: 0.00001328
Iteration 90/1000 | Loss: 0.00001328
Iteration 91/1000 | Loss: 0.00001327
Iteration 92/1000 | Loss: 0.00001327
Iteration 93/1000 | Loss: 0.00001327
Iteration 94/1000 | Loss: 0.00001327
Iteration 95/1000 | Loss: 0.00001327
Iteration 96/1000 | Loss: 0.00001327
Iteration 97/1000 | Loss: 0.00001327
Iteration 98/1000 | Loss: 0.00001326
Iteration 99/1000 | Loss: 0.00001325
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001325
Iteration 102/1000 | Loss: 0.00001325
Iteration 103/1000 | Loss: 0.00001324
Iteration 104/1000 | Loss: 0.00001324
Iteration 105/1000 | Loss: 0.00001324
Iteration 106/1000 | Loss: 0.00001324
Iteration 107/1000 | Loss: 0.00001324
Iteration 108/1000 | Loss: 0.00001324
Iteration 109/1000 | Loss: 0.00001323
Iteration 110/1000 | Loss: 0.00001323
Iteration 111/1000 | Loss: 0.00001323
Iteration 112/1000 | Loss: 0.00001323
Iteration 113/1000 | Loss: 0.00001323
Iteration 114/1000 | Loss: 0.00001323
Iteration 115/1000 | Loss: 0.00001323
Iteration 116/1000 | Loss: 0.00001323
Iteration 117/1000 | Loss: 0.00001323
Iteration 118/1000 | Loss: 0.00001322
Iteration 119/1000 | Loss: 0.00001322
Iteration 120/1000 | Loss: 0.00001322
Iteration 121/1000 | Loss: 0.00001322
Iteration 122/1000 | Loss: 0.00001321
Iteration 123/1000 | Loss: 0.00001321
Iteration 124/1000 | Loss: 0.00001321
Iteration 125/1000 | Loss: 0.00001321
Iteration 126/1000 | Loss: 0.00001321
Iteration 127/1000 | Loss: 0.00001321
Iteration 128/1000 | Loss: 0.00001321
Iteration 129/1000 | Loss: 0.00001321
Iteration 130/1000 | Loss: 0.00001321
Iteration 131/1000 | Loss: 0.00001321
Iteration 132/1000 | Loss: 0.00001321
Iteration 133/1000 | Loss: 0.00001321
Iteration 134/1000 | Loss: 0.00001321
Iteration 135/1000 | Loss: 0.00001321
Iteration 136/1000 | Loss: 0.00001321
Iteration 137/1000 | Loss: 0.00001321
Iteration 138/1000 | Loss: 0.00001320
Iteration 139/1000 | Loss: 0.00001320
Iteration 140/1000 | Loss: 0.00001320
Iteration 141/1000 | Loss: 0.00001320
Iteration 142/1000 | Loss: 0.00001320
Iteration 143/1000 | Loss: 0.00001320
Iteration 144/1000 | Loss: 0.00001320
Iteration 145/1000 | Loss: 0.00001320
Iteration 146/1000 | Loss: 0.00001320
Iteration 147/1000 | Loss: 0.00001320
Iteration 148/1000 | Loss: 0.00001320
Iteration 149/1000 | Loss: 0.00001320
Iteration 150/1000 | Loss: 0.00001320
Iteration 151/1000 | Loss: 0.00001320
Iteration 152/1000 | Loss: 0.00001320
Iteration 153/1000 | Loss: 0.00001319
Iteration 154/1000 | Loss: 0.00001319
Iteration 155/1000 | Loss: 0.00001319
Iteration 156/1000 | Loss: 0.00001319
Iteration 157/1000 | Loss: 0.00001319
Iteration 158/1000 | Loss: 0.00001319
Iteration 159/1000 | Loss: 0.00001319
Iteration 160/1000 | Loss: 0.00001319
Iteration 161/1000 | Loss: 0.00001319
Iteration 162/1000 | Loss: 0.00001319
Iteration 163/1000 | Loss: 0.00001319
Iteration 164/1000 | Loss: 0.00001319
Iteration 165/1000 | Loss: 0.00001319
Iteration 166/1000 | Loss: 0.00001319
Iteration 167/1000 | Loss: 0.00001319
Iteration 168/1000 | Loss: 0.00001319
Iteration 169/1000 | Loss: 0.00001319
Iteration 170/1000 | Loss: 0.00001319
Iteration 171/1000 | Loss: 0.00001318
Iteration 172/1000 | Loss: 0.00001318
Iteration 173/1000 | Loss: 0.00001318
Iteration 174/1000 | Loss: 0.00001318
Iteration 175/1000 | Loss: 0.00001318
Iteration 176/1000 | Loss: 0.00001318
Iteration 177/1000 | Loss: 0.00001317
Iteration 178/1000 | Loss: 0.00001317
Iteration 179/1000 | Loss: 0.00001317
Iteration 180/1000 | Loss: 0.00001317
Iteration 181/1000 | Loss: 0.00001317
Iteration 182/1000 | Loss: 0.00001317
Iteration 183/1000 | Loss: 0.00001317
Iteration 184/1000 | Loss: 0.00001317
Iteration 185/1000 | Loss: 0.00001317
Iteration 186/1000 | Loss: 0.00001317
Iteration 187/1000 | Loss: 0.00001317
Iteration 188/1000 | Loss: 0.00001317
Iteration 189/1000 | Loss: 0.00001317
Iteration 190/1000 | Loss: 0.00001317
Iteration 191/1000 | Loss: 0.00001317
Iteration 192/1000 | Loss: 0.00001316
Iteration 193/1000 | Loss: 0.00001316
Iteration 194/1000 | Loss: 0.00001316
Iteration 195/1000 | Loss: 0.00001316
Iteration 196/1000 | Loss: 0.00001316
Iteration 197/1000 | Loss: 0.00001316
Iteration 198/1000 | Loss: 0.00001316
Iteration 199/1000 | Loss: 0.00001316
Iteration 200/1000 | Loss: 0.00001316
Iteration 201/1000 | Loss: 0.00001316
Iteration 202/1000 | Loss: 0.00001316
Iteration 203/1000 | Loss: 0.00001316
Iteration 204/1000 | Loss: 0.00001315
Iteration 205/1000 | Loss: 0.00001315
Iteration 206/1000 | Loss: 0.00001315
Iteration 207/1000 | Loss: 0.00001315
Iteration 208/1000 | Loss: 0.00001315
Iteration 209/1000 | Loss: 0.00001315
Iteration 210/1000 | Loss: 0.00001315
Iteration 211/1000 | Loss: 0.00001315
Iteration 212/1000 | Loss: 0.00001315
Iteration 213/1000 | Loss: 0.00001315
Iteration 214/1000 | Loss: 0.00001315
Iteration 215/1000 | Loss: 0.00001315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.3153529835108202e-05, 1.3153529835108202e-05, 1.3153529835108202e-05, 1.3153529835108202e-05, 1.3153529835108202e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3153529835108202e-05

Optimization complete. Final v2v error: 3.118326425552368 mm

Highest mean error: 8.427507400512695 mm for frame 22

Lowest mean error: 2.8578643798828125 mm for frame 117

Saving results

Total time: 157.48166394233704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800760
Iteration 2/25 | Loss: 0.00119030
Iteration 3/25 | Loss: 0.00106207
Iteration 4/25 | Loss: 0.00104206
Iteration 5/25 | Loss: 0.00103663
Iteration 6/25 | Loss: 0.00103515
Iteration 7/25 | Loss: 0.00103493
Iteration 8/25 | Loss: 0.00103493
Iteration 9/25 | Loss: 0.00103493
Iteration 10/25 | Loss: 0.00103493
Iteration 11/25 | Loss: 0.00103493
Iteration 12/25 | Loss: 0.00103493
Iteration 13/25 | Loss: 0.00103493
Iteration 14/25 | Loss: 0.00103493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010349301155656576, 0.0010349301155656576, 0.0010349301155656576, 0.0010349301155656576, 0.0010349301155656576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010349301155656576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34293985
Iteration 2/25 | Loss: 0.00049539
Iteration 3/25 | Loss: 0.00049532
Iteration 4/25 | Loss: 0.00049532
Iteration 5/25 | Loss: 0.00049532
Iteration 6/25 | Loss: 0.00049531
Iteration 7/25 | Loss: 0.00049531
Iteration 8/25 | Loss: 0.00049531
Iteration 9/25 | Loss: 0.00049531
Iteration 10/25 | Loss: 0.00049531
Iteration 11/25 | Loss: 0.00049531
Iteration 12/25 | Loss: 0.00049531
Iteration 13/25 | Loss: 0.00049531
Iteration 14/25 | Loss: 0.00049531
Iteration 15/25 | Loss: 0.00049531
Iteration 16/25 | Loss: 0.00049531
Iteration 17/25 | Loss: 0.00049531
Iteration 18/25 | Loss: 0.00049531
Iteration 19/25 | Loss: 0.00049531
Iteration 20/25 | Loss: 0.00049531
Iteration 21/25 | Loss: 0.00049531
Iteration 22/25 | Loss: 0.00049531
Iteration 23/25 | Loss: 0.00049531
Iteration 24/25 | Loss: 0.00049531
Iteration 25/25 | Loss: 0.00049531

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049531
Iteration 2/1000 | Loss: 0.00004052
Iteration 3/1000 | Loss: 0.00002806
Iteration 4/1000 | Loss: 0.00002169
Iteration 5/1000 | Loss: 0.00002010
Iteration 6/1000 | Loss: 0.00001910
Iteration 7/1000 | Loss: 0.00001842
Iteration 8/1000 | Loss: 0.00001802
Iteration 9/1000 | Loss: 0.00001768
Iteration 10/1000 | Loss: 0.00001757
Iteration 11/1000 | Loss: 0.00001747
Iteration 12/1000 | Loss: 0.00001746
Iteration 13/1000 | Loss: 0.00001743
Iteration 14/1000 | Loss: 0.00001741
Iteration 15/1000 | Loss: 0.00001740
Iteration 16/1000 | Loss: 0.00001735
Iteration 17/1000 | Loss: 0.00001718
Iteration 18/1000 | Loss: 0.00001715
Iteration 19/1000 | Loss: 0.00001712
Iteration 20/1000 | Loss: 0.00001705
Iteration 21/1000 | Loss: 0.00001704
Iteration 22/1000 | Loss: 0.00001703
Iteration 23/1000 | Loss: 0.00001702
Iteration 24/1000 | Loss: 0.00001701
Iteration 25/1000 | Loss: 0.00001701
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001701
Iteration 29/1000 | Loss: 0.00001700
Iteration 30/1000 | Loss: 0.00001700
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001698
Iteration 34/1000 | Loss: 0.00001698
Iteration 35/1000 | Loss: 0.00001698
Iteration 36/1000 | Loss: 0.00001697
Iteration 37/1000 | Loss: 0.00001697
Iteration 38/1000 | Loss: 0.00001697
Iteration 39/1000 | Loss: 0.00001697
Iteration 40/1000 | Loss: 0.00001697
Iteration 41/1000 | Loss: 0.00001697
Iteration 42/1000 | Loss: 0.00001697
Iteration 43/1000 | Loss: 0.00001697
Iteration 44/1000 | Loss: 0.00001697
Iteration 45/1000 | Loss: 0.00001697
Iteration 46/1000 | Loss: 0.00001697
Iteration 47/1000 | Loss: 0.00001697
Iteration 48/1000 | Loss: 0.00001697
Iteration 49/1000 | Loss: 0.00001697
Iteration 50/1000 | Loss: 0.00001697
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001697
Iteration 53/1000 | Loss: 0.00001697
Iteration 54/1000 | Loss: 0.00001697
Iteration 55/1000 | Loss: 0.00001697
Iteration 56/1000 | Loss: 0.00001697
Iteration 57/1000 | Loss: 0.00001697
Iteration 58/1000 | Loss: 0.00001697
Iteration 59/1000 | Loss: 0.00001697
Iteration 60/1000 | Loss: 0.00001697
Iteration 61/1000 | Loss: 0.00001697
Iteration 62/1000 | Loss: 0.00001697
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001697
Iteration 65/1000 | Loss: 0.00001697
Iteration 66/1000 | Loss: 0.00001697
Iteration 67/1000 | Loss: 0.00001697
Iteration 68/1000 | Loss: 0.00001697
Iteration 69/1000 | Loss: 0.00001697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.6968811905826442e-05, 1.6968811905826442e-05, 1.6968811905826442e-05, 1.6968811905826442e-05, 1.6968811905826442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6968811905826442e-05

Optimization complete. Final v2v error: 3.5587174892425537 mm

Highest mean error: 3.9632985591888428 mm for frame 39

Lowest mean error: 3.3010058403015137 mm for frame 0

Saving results

Total time: 30.60162854194641
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01151204
Iteration 2/25 | Loss: 0.00410501
Iteration 3/25 | Loss: 0.00304107
Iteration 4/25 | Loss: 0.00289983
Iteration 5/25 | Loss: 0.00247285
Iteration 6/25 | Loss: 0.00229036
Iteration 7/25 | Loss: 0.00216266
Iteration 8/25 | Loss: 0.00196863
Iteration 9/25 | Loss: 0.00185110
Iteration 10/25 | Loss: 0.00179460
Iteration 11/25 | Loss: 0.00176330
Iteration 12/25 | Loss: 0.00173107
Iteration 13/25 | Loss: 0.00170080
Iteration 14/25 | Loss: 0.00168151
Iteration 15/25 | Loss: 0.00167513
Iteration 16/25 | Loss: 0.00165409
Iteration 17/25 | Loss: 0.00163943
Iteration 18/25 | Loss: 0.00162979
Iteration 19/25 | Loss: 0.00162339
Iteration 20/25 | Loss: 0.00161323
Iteration 21/25 | Loss: 0.00161157
Iteration 22/25 | Loss: 0.00160972
Iteration 23/25 | Loss: 0.00162033
Iteration 24/25 | Loss: 0.00161550
Iteration 25/25 | Loss: 0.00161824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13455927
Iteration 2/25 | Loss: 0.00724223
Iteration 3/25 | Loss: 0.00715445
Iteration 4/25 | Loss: 0.00715445
Iteration 5/25 | Loss: 0.00715444
Iteration 6/25 | Loss: 0.00715444
Iteration 7/25 | Loss: 0.00715444
Iteration 8/25 | Loss: 0.00715444
Iteration 9/25 | Loss: 0.00715444
Iteration 10/25 | Loss: 0.00715444
Iteration 11/25 | Loss: 0.00715444
Iteration 12/25 | Loss: 0.00715444
Iteration 13/25 | Loss: 0.00715444
Iteration 14/25 | Loss: 0.00715444
Iteration 15/25 | Loss: 0.00715444
Iteration 16/25 | Loss: 0.00715444
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.007154442835599184, 0.007154442835599184, 0.007154442835599184, 0.007154442835599184, 0.007154442835599184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007154442835599184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00715444
Iteration 2/1000 | Loss: 0.00182829
Iteration 3/1000 | Loss: 0.00128072
Iteration 4/1000 | Loss: 0.00111585
Iteration 5/1000 | Loss: 0.00109278
Iteration 6/1000 | Loss: 0.00097097
Iteration 7/1000 | Loss: 0.00121999
Iteration 8/1000 | Loss: 0.00113792
Iteration 9/1000 | Loss: 0.00091531
Iteration 10/1000 | Loss: 0.00081723
Iteration 11/1000 | Loss: 0.00101263
Iteration 12/1000 | Loss: 0.00106206
Iteration 13/1000 | Loss: 0.00059236
Iteration 14/1000 | Loss: 0.00066185
Iteration 15/1000 | Loss: 0.00091737
Iteration 16/1000 | Loss: 0.00064752
Iteration 17/1000 | Loss: 0.00069019
Iteration 18/1000 | Loss: 0.00034932
Iteration 19/1000 | Loss: 0.00044866
Iteration 20/1000 | Loss: 0.00050989
Iteration 21/1000 | Loss: 0.00141549
Iteration 22/1000 | Loss: 0.00137128
Iteration 23/1000 | Loss: 0.00080697
Iteration 24/1000 | Loss: 0.00038784
Iteration 25/1000 | Loss: 0.00055211
Iteration 26/1000 | Loss: 0.00022957
Iteration 27/1000 | Loss: 0.00107843
Iteration 28/1000 | Loss: 0.00266300
Iteration 29/1000 | Loss: 0.00081058
Iteration 30/1000 | Loss: 0.00032853
Iteration 31/1000 | Loss: 0.00028879
Iteration 32/1000 | Loss: 0.00039165
Iteration 33/1000 | Loss: 0.00137115
Iteration 34/1000 | Loss: 0.00035410
Iteration 35/1000 | Loss: 0.00039329
Iteration 36/1000 | Loss: 0.00157618
Iteration 37/1000 | Loss: 0.00223327
Iteration 38/1000 | Loss: 0.00503318
Iteration 39/1000 | Loss: 0.00080217
Iteration 40/1000 | Loss: 0.00109927
Iteration 41/1000 | Loss: 0.00067852
Iteration 42/1000 | Loss: 0.00036356
Iteration 43/1000 | Loss: 0.00121464
Iteration 44/1000 | Loss: 0.00052670
Iteration 45/1000 | Loss: 0.00034334
Iteration 46/1000 | Loss: 0.00031854
Iteration 47/1000 | Loss: 0.00127176
Iteration 48/1000 | Loss: 0.00046415
Iteration 49/1000 | Loss: 0.00092063
Iteration 50/1000 | Loss: 0.00079982
Iteration 51/1000 | Loss: 0.00079501
Iteration 52/1000 | Loss: 0.00066999
Iteration 53/1000 | Loss: 0.00053845
Iteration 54/1000 | Loss: 0.00048143
Iteration 55/1000 | Loss: 0.00082559
Iteration 56/1000 | Loss: 0.00168493
Iteration 57/1000 | Loss: 0.00099268
Iteration 58/1000 | Loss: 0.00103865
Iteration 59/1000 | Loss: 0.00039115
Iteration 60/1000 | Loss: 0.00082572
Iteration 61/1000 | Loss: 0.00059972
Iteration 62/1000 | Loss: 0.00056694
Iteration 63/1000 | Loss: 0.00028211
Iteration 64/1000 | Loss: 0.00071499
Iteration 65/1000 | Loss: 0.00041483
Iteration 66/1000 | Loss: 0.00130252
Iteration 67/1000 | Loss: 0.00071932
Iteration 68/1000 | Loss: 0.00129671
Iteration 69/1000 | Loss: 0.00054841
Iteration 70/1000 | Loss: 0.00060673
Iteration 71/1000 | Loss: 0.00089386
Iteration 72/1000 | Loss: 0.00106672
Iteration 73/1000 | Loss: 0.00015179
Iteration 74/1000 | Loss: 0.00058374
Iteration 75/1000 | Loss: 0.00032050
Iteration 76/1000 | Loss: 0.00043136
Iteration 77/1000 | Loss: 0.00111403
Iteration 78/1000 | Loss: 0.00086020
Iteration 79/1000 | Loss: 0.00066363
Iteration 80/1000 | Loss: 0.00186208
Iteration 81/1000 | Loss: 0.00065537
Iteration 82/1000 | Loss: 0.00060847
Iteration 83/1000 | Loss: 0.00034279
Iteration 84/1000 | Loss: 0.00029395
Iteration 85/1000 | Loss: 0.00073893
Iteration 86/1000 | Loss: 0.00066491
Iteration 87/1000 | Loss: 0.00041226
Iteration 88/1000 | Loss: 0.00009617
Iteration 89/1000 | Loss: 0.00071501
Iteration 90/1000 | Loss: 0.00176376
Iteration 91/1000 | Loss: 0.00109443
Iteration 92/1000 | Loss: 0.00077261
Iteration 93/1000 | Loss: 0.00015644
Iteration 94/1000 | Loss: 0.00011048
Iteration 95/1000 | Loss: 0.00070504
Iteration 96/1000 | Loss: 0.00052622
Iteration 97/1000 | Loss: 0.00182008
Iteration 98/1000 | Loss: 0.00110318
Iteration 99/1000 | Loss: 0.00096841
Iteration 100/1000 | Loss: 0.00113793
Iteration 101/1000 | Loss: 0.00055104
Iteration 102/1000 | Loss: 0.00102259
Iteration 103/1000 | Loss: 0.00071651
Iteration 104/1000 | Loss: 0.00067687
Iteration 105/1000 | Loss: 0.00008798
Iteration 106/1000 | Loss: 0.00006517
Iteration 107/1000 | Loss: 0.00059640
Iteration 108/1000 | Loss: 0.00045070
Iteration 109/1000 | Loss: 0.00016868
Iteration 110/1000 | Loss: 0.00035692
Iteration 111/1000 | Loss: 0.00048993
Iteration 112/1000 | Loss: 0.00022147
Iteration 113/1000 | Loss: 0.00017684
Iteration 114/1000 | Loss: 0.00035000
Iteration 115/1000 | Loss: 0.00005090
Iteration 116/1000 | Loss: 0.00034974
Iteration 117/1000 | Loss: 0.00019821
Iteration 118/1000 | Loss: 0.00029415
Iteration 119/1000 | Loss: 0.00013516
Iteration 120/1000 | Loss: 0.00010662
Iteration 121/1000 | Loss: 0.00059264
Iteration 122/1000 | Loss: 0.00054633
Iteration 123/1000 | Loss: 0.00052385
Iteration 124/1000 | Loss: 0.00003822
Iteration 125/1000 | Loss: 0.00003851
Iteration 126/1000 | Loss: 0.00002922
Iteration 127/1000 | Loss: 0.00002777
Iteration 128/1000 | Loss: 0.00003662
Iteration 129/1000 | Loss: 0.00002952
Iteration 130/1000 | Loss: 0.00003211
Iteration 131/1000 | Loss: 0.00002635
Iteration 132/1000 | Loss: 0.00002593
Iteration 133/1000 | Loss: 0.00003227
Iteration 134/1000 | Loss: 0.00002575
Iteration 135/1000 | Loss: 0.00002560
Iteration 136/1000 | Loss: 0.00002559
Iteration 137/1000 | Loss: 0.00002559
Iteration 138/1000 | Loss: 0.00002556
Iteration 139/1000 | Loss: 0.00002556
Iteration 140/1000 | Loss: 0.00003520
Iteration 141/1000 | Loss: 0.00002537
Iteration 142/1000 | Loss: 0.00002531
Iteration 143/1000 | Loss: 0.00002527
Iteration 144/1000 | Loss: 0.00003419
Iteration 145/1000 | Loss: 0.00002527
Iteration 146/1000 | Loss: 0.00002518
Iteration 147/1000 | Loss: 0.00002517
Iteration 148/1000 | Loss: 0.00003418
Iteration 149/1000 | Loss: 0.00002515
Iteration 150/1000 | Loss: 0.00002514
Iteration 151/1000 | Loss: 0.00002512
Iteration 152/1000 | Loss: 0.00002512
Iteration 153/1000 | Loss: 0.00002512
Iteration 154/1000 | Loss: 0.00002512
Iteration 155/1000 | Loss: 0.00002512
Iteration 156/1000 | Loss: 0.00002511
Iteration 157/1000 | Loss: 0.00002511
Iteration 158/1000 | Loss: 0.00002511
Iteration 159/1000 | Loss: 0.00002511
Iteration 160/1000 | Loss: 0.00002511
Iteration 161/1000 | Loss: 0.00002510
Iteration 162/1000 | Loss: 0.00002510
Iteration 163/1000 | Loss: 0.00002510
Iteration 164/1000 | Loss: 0.00002510
Iteration 165/1000 | Loss: 0.00002510
Iteration 166/1000 | Loss: 0.00002510
Iteration 167/1000 | Loss: 0.00002510
Iteration 168/1000 | Loss: 0.00002510
Iteration 169/1000 | Loss: 0.00002509
Iteration 170/1000 | Loss: 0.00002509
Iteration 171/1000 | Loss: 0.00002509
Iteration 172/1000 | Loss: 0.00002509
Iteration 173/1000 | Loss: 0.00002509
Iteration 174/1000 | Loss: 0.00002509
Iteration 175/1000 | Loss: 0.00002509
Iteration 176/1000 | Loss: 0.00002509
Iteration 177/1000 | Loss: 0.00002508
Iteration 178/1000 | Loss: 0.00002508
Iteration 179/1000 | Loss: 0.00002508
Iteration 180/1000 | Loss: 0.00002508
Iteration 181/1000 | Loss: 0.00002507
Iteration 182/1000 | Loss: 0.00002507
Iteration 183/1000 | Loss: 0.00002507
Iteration 184/1000 | Loss: 0.00002507
Iteration 185/1000 | Loss: 0.00002507
Iteration 186/1000 | Loss: 0.00002507
Iteration 187/1000 | Loss: 0.00002507
Iteration 188/1000 | Loss: 0.00002506
Iteration 189/1000 | Loss: 0.00002506
Iteration 190/1000 | Loss: 0.00002506
Iteration 191/1000 | Loss: 0.00002506
Iteration 192/1000 | Loss: 0.00002506
Iteration 193/1000 | Loss: 0.00002506
Iteration 194/1000 | Loss: 0.00002506
Iteration 195/1000 | Loss: 0.00002506
Iteration 196/1000 | Loss: 0.00002505
Iteration 197/1000 | Loss: 0.00002505
Iteration 198/1000 | Loss: 0.00002505
Iteration 199/1000 | Loss: 0.00002505
Iteration 200/1000 | Loss: 0.00002505
Iteration 201/1000 | Loss: 0.00002505
Iteration 202/1000 | Loss: 0.00002505
Iteration 203/1000 | Loss: 0.00002505
Iteration 204/1000 | Loss: 0.00002505
Iteration 205/1000 | Loss: 0.00002505
Iteration 206/1000 | Loss: 0.00002505
Iteration 207/1000 | Loss: 0.00002505
Iteration 208/1000 | Loss: 0.00002505
Iteration 209/1000 | Loss: 0.00002505
Iteration 210/1000 | Loss: 0.00002505
Iteration 211/1000 | Loss: 0.00002505
Iteration 212/1000 | Loss: 0.00002505
Iteration 213/1000 | Loss: 0.00002505
Iteration 214/1000 | Loss: 0.00002505
Iteration 215/1000 | Loss: 0.00002505
Iteration 216/1000 | Loss: 0.00002505
Iteration 217/1000 | Loss: 0.00002505
Iteration 218/1000 | Loss: 0.00002505
Iteration 219/1000 | Loss: 0.00002505
Iteration 220/1000 | Loss: 0.00002505
Iteration 221/1000 | Loss: 0.00002505
Iteration 222/1000 | Loss: 0.00002505
Iteration 223/1000 | Loss: 0.00002505
Iteration 224/1000 | Loss: 0.00002505
Iteration 225/1000 | Loss: 0.00002505
Iteration 226/1000 | Loss: 0.00002505
Iteration 227/1000 | Loss: 0.00002505
Iteration 228/1000 | Loss: 0.00002505
Iteration 229/1000 | Loss: 0.00002505
Iteration 230/1000 | Loss: 0.00002505
Iteration 231/1000 | Loss: 0.00002505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [2.5052317141671665e-05, 2.5052317141671665e-05, 2.5052317141671665e-05, 2.5052317141671665e-05, 2.5052317141671665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5052317141671665e-05

Optimization complete. Final v2v error: 4.1952738761901855 mm

Highest mean error: 4.780694961547852 mm for frame 172

Lowest mean error: 3.729423999786377 mm for frame 19

Saving results

Total time: 285.9849650859833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960985
Iteration 2/25 | Loss: 0.00119870
Iteration 3/25 | Loss: 0.00103569
Iteration 4/25 | Loss: 0.00101187
Iteration 5/25 | Loss: 0.00100242
Iteration 6/25 | Loss: 0.00100016
Iteration 7/25 | Loss: 0.00100008
Iteration 8/25 | Loss: 0.00100008
Iteration 9/25 | Loss: 0.00100008
Iteration 10/25 | Loss: 0.00100008
Iteration 11/25 | Loss: 0.00100008
Iteration 12/25 | Loss: 0.00100008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001000082353129983, 0.001000082353129983, 0.001000082353129983, 0.001000082353129983, 0.001000082353129983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001000082353129983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33162796
Iteration 2/25 | Loss: 0.00048622
Iteration 3/25 | Loss: 0.00048622
Iteration 4/25 | Loss: 0.00048621
Iteration 5/25 | Loss: 0.00048621
Iteration 6/25 | Loss: 0.00048621
Iteration 7/25 | Loss: 0.00048621
Iteration 8/25 | Loss: 0.00048621
Iteration 9/25 | Loss: 0.00048621
Iteration 10/25 | Loss: 0.00048621
Iteration 11/25 | Loss: 0.00048621
Iteration 12/25 | Loss: 0.00048621
Iteration 13/25 | Loss: 0.00048621
Iteration 14/25 | Loss: 0.00048621
Iteration 15/25 | Loss: 0.00048621
Iteration 16/25 | Loss: 0.00048621
Iteration 17/25 | Loss: 0.00048621
Iteration 18/25 | Loss: 0.00048621
Iteration 19/25 | Loss: 0.00048621
Iteration 20/25 | Loss: 0.00048621
Iteration 21/25 | Loss: 0.00048621
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00048621182213537395, 0.00048621182213537395, 0.00048621182213537395, 0.00048621182213537395, 0.00048621182213537395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00048621182213537395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048621
Iteration 2/1000 | Loss: 0.00003958
Iteration 3/1000 | Loss: 0.00002041
Iteration 4/1000 | Loss: 0.00001732
Iteration 5/1000 | Loss: 0.00001592
Iteration 6/1000 | Loss: 0.00001529
Iteration 7/1000 | Loss: 0.00001487
Iteration 8/1000 | Loss: 0.00001451
Iteration 9/1000 | Loss: 0.00001424
Iteration 10/1000 | Loss: 0.00001404
Iteration 11/1000 | Loss: 0.00001396
Iteration 12/1000 | Loss: 0.00001388
Iteration 13/1000 | Loss: 0.00001383
Iteration 14/1000 | Loss: 0.00001375
Iteration 15/1000 | Loss: 0.00001371
Iteration 16/1000 | Loss: 0.00001364
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001356
Iteration 19/1000 | Loss: 0.00001355
Iteration 20/1000 | Loss: 0.00001354
Iteration 21/1000 | Loss: 0.00001353
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001350
Iteration 24/1000 | Loss: 0.00001350
Iteration 25/1000 | Loss: 0.00001350
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001350
Iteration 28/1000 | Loss: 0.00001349
Iteration 29/1000 | Loss: 0.00001349
Iteration 30/1000 | Loss: 0.00001348
Iteration 31/1000 | Loss: 0.00001347
Iteration 32/1000 | Loss: 0.00001346
Iteration 33/1000 | Loss: 0.00001346
Iteration 34/1000 | Loss: 0.00001345
Iteration 35/1000 | Loss: 0.00001345
Iteration 36/1000 | Loss: 0.00001345
Iteration 37/1000 | Loss: 0.00001345
Iteration 38/1000 | Loss: 0.00001345
Iteration 39/1000 | Loss: 0.00001344
Iteration 40/1000 | Loss: 0.00001344
Iteration 41/1000 | Loss: 0.00001344
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001343
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001341
Iteration 49/1000 | Loss: 0.00001341
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001341
Iteration 52/1000 | Loss: 0.00001341
Iteration 53/1000 | Loss: 0.00001340
Iteration 54/1000 | Loss: 0.00001340
Iteration 55/1000 | Loss: 0.00001340
Iteration 56/1000 | Loss: 0.00001340
Iteration 57/1000 | Loss: 0.00001340
Iteration 58/1000 | Loss: 0.00001340
Iteration 59/1000 | Loss: 0.00001340
Iteration 60/1000 | Loss: 0.00001339
Iteration 61/1000 | Loss: 0.00001339
Iteration 62/1000 | Loss: 0.00001339
Iteration 63/1000 | Loss: 0.00001338
Iteration 64/1000 | Loss: 0.00001338
Iteration 65/1000 | Loss: 0.00001338
Iteration 66/1000 | Loss: 0.00001337
Iteration 67/1000 | Loss: 0.00001337
Iteration 68/1000 | Loss: 0.00001337
Iteration 69/1000 | Loss: 0.00001337
Iteration 70/1000 | Loss: 0.00001337
Iteration 71/1000 | Loss: 0.00001337
Iteration 72/1000 | Loss: 0.00001336
Iteration 73/1000 | Loss: 0.00001336
Iteration 74/1000 | Loss: 0.00001336
Iteration 75/1000 | Loss: 0.00001335
Iteration 76/1000 | Loss: 0.00001335
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001333
Iteration 84/1000 | Loss: 0.00001333
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001332
Iteration 87/1000 | Loss: 0.00001332
Iteration 88/1000 | Loss: 0.00001332
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001331
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001330
Iteration 93/1000 | Loss: 0.00001330
Iteration 94/1000 | Loss: 0.00001330
Iteration 95/1000 | Loss: 0.00001330
Iteration 96/1000 | Loss: 0.00001330
Iteration 97/1000 | Loss: 0.00001330
Iteration 98/1000 | Loss: 0.00001330
Iteration 99/1000 | Loss: 0.00001330
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001330
Iteration 104/1000 | Loss: 0.00001329
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001329
Iteration 111/1000 | Loss: 0.00001329
Iteration 112/1000 | Loss: 0.00001329
Iteration 113/1000 | Loss: 0.00001328
Iteration 114/1000 | Loss: 0.00001328
Iteration 115/1000 | Loss: 0.00001328
Iteration 116/1000 | Loss: 0.00001328
Iteration 117/1000 | Loss: 0.00001328
Iteration 118/1000 | Loss: 0.00001328
Iteration 119/1000 | Loss: 0.00001328
Iteration 120/1000 | Loss: 0.00001328
Iteration 121/1000 | Loss: 0.00001328
Iteration 122/1000 | Loss: 0.00001328
Iteration 123/1000 | Loss: 0.00001328
Iteration 124/1000 | Loss: 0.00001328
Iteration 125/1000 | Loss: 0.00001328
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001328
Iteration 128/1000 | Loss: 0.00001328
Iteration 129/1000 | Loss: 0.00001328
Iteration 130/1000 | Loss: 0.00001328
Iteration 131/1000 | Loss: 0.00001328
Iteration 132/1000 | Loss: 0.00001328
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001326
Iteration 144/1000 | Loss: 0.00001326
Iteration 145/1000 | Loss: 0.00001326
Iteration 146/1000 | Loss: 0.00001326
Iteration 147/1000 | Loss: 0.00001326
Iteration 148/1000 | Loss: 0.00001326
Iteration 149/1000 | Loss: 0.00001326
Iteration 150/1000 | Loss: 0.00001326
Iteration 151/1000 | Loss: 0.00001326
Iteration 152/1000 | Loss: 0.00001326
Iteration 153/1000 | Loss: 0.00001326
Iteration 154/1000 | Loss: 0.00001326
Iteration 155/1000 | Loss: 0.00001326
Iteration 156/1000 | Loss: 0.00001326
Iteration 157/1000 | Loss: 0.00001326
Iteration 158/1000 | Loss: 0.00001326
Iteration 159/1000 | Loss: 0.00001326
Iteration 160/1000 | Loss: 0.00001326
Iteration 161/1000 | Loss: 0.00001326
Iteration 162/1000 | Loss: 0.00001326
Iteration 163/1000 | Loss: 0.00001326
Iteration 164/1000 | Loss: 0.00001326
Iteration 165/1000 | Loss: 0.00001326
Iteration 166/1000 | Loss: 0.00001326
Iteration 167/1000 | Loss: 0.00001326
Iteration 168/1000 | Loss: 0.00001326
Iteration 169/1000 | Loss: 0.00001326
Iteration 170/1000 | Loss: 0.00001326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.325845096289413e-05, 1.325845096289413e-05, 1.325845096289413e-05, 1.325845096289413e-05, 1.325845096289413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.325845096289413e-05

Optimization complete. Final v2v error: 3.1812851428985596 mm

Highest mean error: 4.0127272605896 mm for frame 175

Lowest mean error: 2.56630277633667 mm for frame 224

Saving results

Total time: 43.879626989364624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101602
Iteration 2/25 | Loss: 0.00308674
Iteration 3/25 | Loss: 0.00167295
Iteration 4/25 | Loss: 0.00132825
Iteration 5/25 | Loss: 0.00123566
Iteration 6/25 | Loss: 0.00118575
Iteration 7/25 | Loss: 0.00110956
Iteration 8/25 | Loss: 0.00103959
Iteration 9/25 | Loss: 0.00100829
Iteration 10/25 | Loss: 0.00098321
Iteration 11/25 | Loss: 0.00096118
Iteration 12/25 | Loss: 0.00094228
Iteration 13/25 | Loss: 0.00093357
Iteration 14/25 | Loss: 0.00091830
Iteration 15/25 | Loss: 0.00091821
Iteration 16/25 | Loss: 0.00091161
Iteration 17/25 | Loss: 0.00091288
Iteration 18/25 | Loss: 0.00091242
Iteration 19/25 | Loss: 0.00091303
Iteration 20/25 | Loss: 0.00091083
Iteration 21/25 | Loss: 0.00091301
Iteration 22/25 | Loss: 0.00091179
Iteration 23/25 | Loss: 0.00091003
Iteration 24/25 | Loss: 0.00091237
Iteration 25/25 | Loss: 0.00091913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91327524
Iteration 2/25 | Loss: 0.00054509
Iteration 3/25 | Loss: 0.00054509
Iteration 4/25 | Loss: 0.00045249
Iteration 5/25 | Loss: 0.00045249
Iteration 6/25 | Loss: 0.00045249
Iteration 7/25 | Loss: 0.00045249
Iteration 8/25 | Loss: 0.00045249
Iteration 9/25 | Loss: 0.00045249
Iteration 10/25 | Loss: 0.00045249
Iteration 11/25 | Loss: 0.00045249
Iteration 12/25 | Loss: 0.00045249
Iteration 13/25 | Loss: 0.00045249
Iteration 14/25 | Loss: 0.00045249
Iteration 15/25 | Loss: 0.00045249
Iteration 16/25 | Loss: 0.00045249
Iteration 17/25 | Loss: 0.00045249
Iteration 18/25 | Loss: 0.00045249
Iteration 19/25 | Loss: 0.00045249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004524869145825505, 0.0004524869145825505, 0.0004524869145825505, 0.0004524869145825505, 0.0004524869145825505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004524869145825505

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045249
Iteration 2/1000 | Loss: 0.00014771
Iteration 3/1000 | Loss: 0.00008964
Iteration 4/1000 | Loss: 0.00023453
Iteration 5/1000 | Loss: 0.00096301
Iteration 6/1000 | Loss: 0.00003151
Iteration 7/1000 | Loss: 0.00003137
Iteration 8/1000 | Loss: 0.00005073
Iteration 9/1000 | Loss: 0.00012542
Iteration 10/1000 | Loss: 0.00006913
Iteration 11/1000 | Loss: 0.00009272
Iteration 12/1000 | Loss: 0.00003226
Iteration 13/1000 | Loss: 0.00003029
Iteration 14/1000 | Loss: 0.00005104
Iteration 15/1000 | Loss: 0.00003847
Iteration 16/1000 | Loss: 0.00010223
Iteration 17/1000 | Loss: 0.00002951
Iteration 18/1000 | Loss: 0.00002914
Iteration 19/1000 | Loss: 0.00004325
Iteration 20/1000 | Loss: 0.00003433
Iteration 21/1000 | Loss: 0.00004585
Iteration 22/1000 | Loss: 0.00002984
Iteration 23/1000 | Loss: 0.00003848
Iteration 24/1000 | Loss: 0.00003263
Iteration 25/1000 | Loss: 0.00002909
Iteration 26/1000 | Loss: 0.00010346
Iteration 27/1000 | Loss: 0.00003099
Iteration 28/1000 | Loss: 0.00003546
Iteration 29/1000 | Loss: 0.00003027
Iteration 30/1000 | Loss: 0.00057690
Iteration 31/1000 | Loss: 0.00038929
Iteration 32/1000 | Loss: 0.00052737
Iteration 33/1000 | Loss: 0.00011027
Iteration 34/1000 | Loss: 0.00006707
Iteration 35/1000 | Loss: 0.00010811
Iteration 36/1000 | Loss: 0.00005009
Iteration 37/1000 | Loss: 0.00004401
Iteration 38/1000 | Loss: 0.00007456
Iteration 39/1000 | Loss: 0.00004585
Iteration 40/1000 | Loss: 0.00004628
Iteration 41/1000 | Loss: 0.00002924
Iteration 42/1000 | Loss: 0.00010259
Iteration 43/1000 | Loss: 0.00005754
Iteration 44/1000 | Loss: 0.00002659
Iteration 45/1000 | Loss: 0.00008528
Iteration 46/1000 | Loss: 0.00003570
Iteration 47/1000 | Loss: 0.00003533
Iteration 48/1000 | Loss: 0.00004102
Iteration 49/1000 | Loss: 0.00003392
Iteration 50/1000 | Loss: 0.00008717
Iteration 51/1000 | Loss: 0.00003362
Iteration 52/1000 | Loss: 0.00002925
Iteration 53/1000 | Loss: 0.00049961
Iteration 54/1000 | Loss: 0.00026064
Iteration 55/1000 | Loss: 0.00049862
Iteration 56/1000 | Loss: 0.00028156
Iteration 57/1000 | Loss: 0.00007149
Iteration 58/1000 | Loss: 0.00002144
Iteration 59/1000 | Loss: 0.00021889
Iteration 60/1000 | Loss: 0.00004368
Iteration 61/1000 | Loss: 0.00003640
Iteration 62/1000 | Loss: 0.00003806
Iteration 63/1000 | Loss: 0.00003162
Iteration 64/1000 | Loss: 0.00007534
Iteration 65/1000 | Loss: 0.00019344
Iteration 66/1000 | Loss: 0.00004911
Iteration 67/1000 | Loss: 0.00003644
Iteration 68/1000 | Loss: 0.00004246
Iteration 69/1000 | Loss: 0.00002808
Iteration 70/1000 | Loss: 0.00003929
Iteration 71/1000 | Loss: 0.00002671
Iteration 72/1000 | Loss: 0.00009531
Iteration 73/1000 | Loss: 0.00005364
Iteration 74/1000 | Loss: 0.00002943
Iteration 75/1000 | Loss: 0.00003312
Iteration 76/1000 | Loss: 0.00002602
Iteration 77/1000 | Loss: 0.00004514
Iteration 78/1000 | Loss: 0.00002429
Iteration 79/1000 | Loss: 0.00007236
Iteration 80/1000 | Loss: 0.00002759
Iteration 81/1000 | Loss: 0.00006475
Iteration 82/1000 | Loss: 0.00003563
Iteration 83/1000 | Loss: 0.00012631
Iteration 84/1000 | Loss: 0.00005568
Iteration 85/1000 | Loss: 0.00002607
Iteration 86/1000 | Loss: 0.00007396
Iteration 87/1000 | Loss: 0.00003767
Iteration 88/1000 | Loss: 0.00009372
Iteration 89/1000 | Loss: 0.00001465
Iteration 90/1000 | Loss: 0.00004824
Iteration 91/1000 | Loss: 0.00001648
Iteration 92/1000 | Loss: 0.00001371
Iteration 93/1000 | Loss: 0.00001507
Iteration 94/1000 | Loss: 0.00001547
Iteration 95/1000 | Loss: 0.00001317
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00004194
Iteration 98/1000 | Loss: 0.00001607
Iteration 99/1000 | Loss: 0.00001296
Iteration 100/1000 | Loss: 0.00001296
Iteration 101/1000 | Loss: 0.00001295
Iteration 102/1000 | Loss: 0.00001325
Iteration 103/1000 | Loss: 0.00003538
Iteration 104/1000 | Loss: 0.00012717
Iteration 105/1000 | Loss: 0.00002669
Iteration 106/1000 | Loss: 0.00001295
Iteration 107/1000 | Loss: 0.00001292
Iteration 108/1000 | Loss: 0.00001291
Iteration 109/1000 | Loss: 0.00001291
Iteration 110/1000 | Loss: 0.00003594
Iteration 111/1000 | Loss: 0.00002448
Iteration 112/1000 | Loss: 0.00001290
Iteration 113/1000 | Loss: 0.00001290
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001289
Iteration 116/1000 | Loss: 0.00001289
Iteration 117/1000 | Loss: 0.00001289
Iteration 118/1000 | Loss: 0.00001289
Iteration 119/1000 | Loss: 0.00001289
Iteration 120/1000 | Loss: 0.00002722
Iteration 121/1000 | Loss: 0.00004173
Iteration 122/1000 | Loss: 0.00002218
Iteration 123/1000 | Loss: 0.00006307
Iteration 124/1000 | Loss: 0.00002058
Iteration 125/1000 | Loss: 0.00001288
Iteration 126/1000 | Loss: 0.00001288
Iteration 127/1000 | Loss: 0.00001288
Iteration 128/1000 | Loss: 0.00001288
Iteration 129/1000 | Loss: 0.00001288
Iteration 130/1000 | Loss: 0.00001287
Iteration 131/1000 | Loss: 0.00001287
Iteration 132/1000 | Loss: 0.00001287
Iteration 133/1000 | Loss: 0.00001287
Iteration 134/1000 | Loss: 0.00001287
Iteration 135/1000 | Loss: 0.00003540
Iteration 136/1000 | Loss: 0.00006212
Iteration 137/1000 | Loss: 0.00008550
Iteration 138/1000 | Loss: 0.00013578
Iteration 139/1000 | Loss: 0.00004641
Iteration 140/1000 | Loss: 0.00002489
Iteration 141/1000 | Loss: 0.00001292
Iteration 142/1000 | Loss: 0.00001291
Iteration 143/1000 | Loss: 0.00001290
Iteration 144/1000 | Loss: 0.00001290
Iteration 145/1000 | Loss: 0.00001289
Iteration 146/1000 | Loss: 0.00001777
Iteration 147/1000 | Loss: 0.00001289
Iteration 148/1000 | Loss: 0.00001289
Iteration 149/1000 | Loss: 0.00001288
Iteration 150/1000 | Loss: 0.00001526
Iteration 151/1000 | Loss: 0.00001354
Iteration 152/1000 | Loss: 0.00001290
Iteration 153/1000 | Loss: 0.00001289
Iteration 154/1000 | Loss: 0.00001289
Iteration 155/1000 | Loss: 0.00001289
Iteration 156/1000 | Loss: 0.00001289
Iteration 157/1000 | Loss: 0.00001328
Iteration 158/1000 | Loss: 0.00001289
Iteration 159/1000 | Loss: 0.00001289
Iteration 160/1000 | Loss: 0.00001289
Iteration 161/1000 | Loss: 0.00001289
Iteration 162/1000 | Loss: 0.00001289
Iteration 163/1000 | Loss: 0.00001289
Iteration 164/1000 | Loss: 0.00001289
Iteration 165/1000 | Loss: 0.00001289
Iteration 166/1000 | Loss: 0.00001289
Iteration 167/1000 | Loss: 0.00001289
Iteration 168/1000 | Loss: 0.00001289
Iteration 169/1000 | Loss: 0.00001289
Iteration 170/1000 | Loss: 0.00001288
Iteration 171/1000 | Loss: 0.00001288
Iteration 172/1000 | Loss: 0.00001288
Iteration 173/1000 | Loss: 0.00001288
Iteration 174/1000 | Loss: 0.00001288
Iteration 175/1000 | Loss: 0.00001288
Iteration 176/1000 | Loss: 0.00001287
Iteration 177/1000 | Loss: 0.00001287
Iteration 178/1000 | Loss: 0.00001287
Iteration 179/1000 | Loss: 0.00001286
Iteration 180/1000 | Loss: 0.00001286
Iteration 181/1000 | Loss: 0.00001286
Iteration 182/1000 | Loss: 0.00001286
Iteration 183/1000 | Loss: 0.00001285
Iteration 184/1000 | Loss: 0.00001285
Iteration 185/1000 | Loss: 0.00001285
Iteration 186/1000 | Loss: 0.00001284
Iteration 187/1000 | Loss: 0.00001284
Iteration 188/1000 | Loss: 0.00005655
Iteration 189/1000 | Loss: 0.00001395
Iteration 190/1000 | Loss: 0.00001449
Iteration 191/1000 | Loss: 0.00001290
Iteration 192/1000 | Loss: 0.00001290
Iteration 193/1000 | Loss: 0.00001289
Iteration 194/1000 | Loss: 0.00001289
Iteration 195/1000 | Loss: 0.00001289
Iteration 196/1000 | Loss: 0.00001858
Iteration 197/1000 | Loss: 0.00001286
Iteration 198/1000 | Loss: 0.00001286
Iteration 199/1000 | Loss: 0.00001286
Iteration 200/1000 | Loss: 0.00001286
Iteration 201/1000 | Loss: 0.00001286
Iteration 202/1000 | Loss: 0.00001286
Iteration 203/1000 | Loss: 0.00001286
Iteration 204/1000 | Loss: 0.00001285
Iteration 205/1000 | Loss: 0.00001858
Iteration 206/1000 | Loss: 0.00001284
Iteration 207/1000 | Loss: 0.00001284
Iteration 208/1000 | Loss: 0.00001284
Iteration 209/1000 | Loss: 0.00001283
Iteration 210/1000 | Loss: 0.00001283
Iteration 211/1000 | Loss: 0.00001283
Iteration 212/1000 | Loss: 0.00001283
Iteration 213/1000 | Loss: 0.00001283
Iteration 214/1000 | Loss: 0.00001283
Iteration 215/1000 | Loss: 0.00001283
Iteration 216/1000 | Loss: 0.00001283
Iteration 217/1000 | Loss: 0.00001283
Iteration 218/1000 | Loss: 0.00001283
Iteration 219/1000 | Loss: 0.00001283
Iteration 220/1000 | Loss: 0.00001283
Iteration 221/1000 | Loss: 0.00001282
Iteration 222/1000 | Loss: 0.00001282
Iteration 223/1000 | Loss: 0.00001282
Iteration 224/1000 | Loss: 0.00001282
Iteration 225/1000 | Loss: 0.00001282
Iteration 226/1000 | Loss: 0.00001282
Iteration 227/1000 | Loss: 0.00001282
Iteration 228/1000 | Loss: 0.00001282
Iteration 229/1000 | Loss: 0.00001282
Iteration 230/1000 | Loss: 0.00001282
Iteration 231/1000 | Loss: 0.00001282
Iteration 232/1000 | Loss: 0.00001282
Iteration 233/1000 | Loss: 0.00001282
Iteration 234/1000 | Loss: 0.00001282
Iteration 235/1000 | Loss: 0.00001281
Iteration 236/1000 | Loss: 0.00001281
Iteration 237/1000 | Loss: 0.00001281
Iteration 238/1000 | Loss: 0.00001281
Iteration 239/1000 | Loss: 0.00001281
Iteration 240/1000 | Loss: 0.00001281
Iteration 241/1000 | Loss: 0.00001281
Iteration 242/1000 | Loss: 0.00001281
Iteration 243/1000 | Loss: 0.00001281
Iteration 244/1000 | Loss: 0.00001281
Iteration 245/1000 | Loss: 0.00001281
Iteration 246/1000 | Loss: 0.00001281
Iteration 247/1000 | Loss: 0.00001281
Iteration 248/1000 | Loss: 0.00001281
Iteration 249/1000 | Loss: 0.00001281
Iteration 250/1000 | Loss: 0.00001281
Iteration 251/1000 | Loss: 0.00001281
Iteration 252/1000 | Loss: 0.00001281
Iteration 253/1000 | Loss: 0.00001281
Iteration 254/1000 | Loss: 0.00001281
Iteration 255/1000 | Loss: 0.00001281
Iteration 256/1000 | Loss: 0.00001281
Iteration 257/1000 | Loss: 0.00001280
Iteration 258/1000 | Loss: 0.00001280
Iteration 259/1000 | Loss: 0.00001280
Iteration 260/1000 | Loss: 0.00001280
Iteration 261/1000 | Loss: 0.00001280
Iteration 262/1000 | Loss: 0.00001280
Iteration 263/1000 | Loss: 0.00001280
Iteration 264/1000 | Loss: 0.00001280
Iteration 265/1000 | Loss: 0.00001280
Iteration 266/1000 | Loss: 0.00001280
Iteration 267/1000 | Loss: 0.00001280
Iteration 268/1000 | Loss: 0.00001279
Iteration 269/1000 | Loss: 0.00001279
Iteration 270/1000 | Loss: 0.00001279
Iteration 271/1000 | Loss: 0.00001279
Iteration 272/1000 | Loss: 0.00001279
Iteration 273/1000 | Loss: 0.00001279
Iteration 274/1000 | Loss: 0.00001279
Iteration 275/1000 | Loss: 0.00001279
Iteration 276/1000 | Loss: 0.00001278
Iteration 277/1000 | Loss: 0.00001278
Iteration 278/1000 | Loss: 0.00001278
Iteration 279/1000 | Loss: 0.00001278
Iteration 280/1000 | Loss: 0.00001278
Iteration 281/1000 | Loss: 0.00001278
Iteration 282/1000 | Loss: 0.00001278
Iteration 283/1000 | Loss: 0.00001278
Iteration 284/1000 | Loss: 0.00001278
Iteration 285/1000 | Loss: 0.00001278
Iteration 286/1000 | Loss: 0.00001278
Iteration 287/1000 | Loss: 0.00001278
Iteration 288/1000 | Loss: 0.00001278
Iteration 289/1000 | Loss: 0.00001278
Iteration 290/1000 | Loss: 0.00001278
Iteration 291/1000 | Loss: 0.00001278
Iteration 292/1000 | Loss: 0.00001278
Iteration 293/1000 | Loss: 0.00001278
Iteration 294/1000 | Loss: 0.00001278
Iteration 295/1000 | Loss: 0.00001278
Iteration 296/1000 | Loss: 0.00001278
Iteration 297/1000 | Loss: 0.00001278
Iteration 298/1000 | Loss: 0.00001277
Iteration 299/1000 | Loss: 0.00001277
Iteration 300/1000 | Loss: 0.00001277
Iteration 301/1000 | Loss: 0.00001277
Iteration 302/1000 | Loss: 0.00001277
Iteration 303/1000 | Loss: 0.00001277
Iteration 304/1000 | Loss: 0.00001277
Iteration 305/1000 | Loss: 0.00001277
Iteration 306/1000 | Loss: 0.00001277
Iteration 307/1000 | Loss: 0.00001277
Iteration 308/1000 | Loss: 0.00001277
Iteration 309/1000 | Loss: 0.00001277
Iteration 310/1000 | Loss: 0.00001277
Iteration 311/1000 | Loss: 0.00001277
Iteration 312/1000 | Loss: 0.00001277
Iteration 313/1000 | Loss: 0.00001277
Iteration 314/1000 | Loss: 0.00001277
Iteration 315/1000 | Loss: 0.00001277
Iteration 316/1000 | Loss: 0.00001277
Iteration 317/1000 | Loss: 0.00001277
Iteration 318/1000 | Loss: 0.00001277
Iteration 319/1000 | Loss: 0.00001277
Iteration 320/1000 | Loss: 0.00001277
Iteration 321/1000 | Loss: 0.00001277
Iteration 322/1000 | Loss: 0.00001277
Iteration 323/1000 | Loss: 0.00001277
Iteration 324/1000 | Loss: 0.00001277
Iteration 325/1000 | Loss: 0.00001277
Iteration 326/1000 | Loss: 0.00001277
Iteration 327/1000 | Loss: 0.00001277
Iteration 328/1000 | Loss: 0.00001277
Iteration 329/1000 | Loss: 0.00001277
Iteration 330/1000 | Loss: 0.00001277
Iteration 331/1000 | Loss: 0.00001277
Iteration 332/1000 | Loss: 0.00001277
Iteration 333/1000 | Loss: 0.00001277
Iteration 334/1000 | Loss: 0.00001277
Iteration 335/1000 | Loss: 0.00001277
Iteration 336/1000 | Loss: 0.00001277
Iteration 337/1000 | Loss: 0.00001277
Iteration 338/1000 | Loss: 0.00001277
Iteration 339/1000 | Loss: 0.00001277
Iteration 340/1000 | Loss: 0.00001277
Iteration 341/1000 | Loss: 0.00001277
Iteration 342/1000 | Loss: 0.00001277
Iteration 343/1000 | Loss: 0.00001277
Iteration 344/1000 | Loss: 0.00001277
Iteration 345/1000 | Loss: 0.00001277
Iteration 346/1000 | Loss: 0.00001277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 346. Stopping optimization.
Last 5 losses: [1.2769064596795943e-05, 1.2769064596795943e-05, 1.2769064596795943e-05, 1.2769064596795943e-05, 1.2769064596795943e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2769064596795943e-05

Optimization complete. Final v2v error: 2.9702653884887695 mm

Highest mean error: 8.355648040771484 mm for frame 24

Lowest mean error: 2.488335609436035 mm for frame 2

Saving results

Total time: 227.18474006652832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_us_1280/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_us_1280/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461544
Iteration 2/25 | Loss: 0.00104925
Iteration 3/25 | Loss: 0.00096319
Iteration 4/25 | Loss: 0.00095213
Iteration 5/25 | Loss: 0.00094826
Iteration 6/25 | Loss: 0.00094716
Iteration 7/25 | Loss: 0.00094689
Iteration 8/25 | Loss: 0.00094689
Iteration 9/25 | Loss: 0.00094689
Iteration 10/25 | Loss: 0.00094689
Iteration 11/25 | Loss: 0.00094689
Iteration 12/25 | Loss: 0.00094689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009468885255046189, 0.0009468885255046189, 0.0009468885255046189, 0.0009468885255046189, 0.0009468885255046189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009468885255046189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46494913
Iteration 2/25 | Loss: 0.00045929
Iteration 3/25 | Loss: 0.00045928
Iteration 4/25 | Loss: 0.00045928
Iteration 5/25 | Loss: 0.00045928
Iteration 6/25 | Loss: 0.00045928
Iteration 7/25 | Loss: 0.00045928
Iteration 8/25 | Loss: 0.00045928
Iteration 9/25 | Loss: 0.00045928
Iteration 10/25 | Loss: 0.00045928
Iteration 11/25 | Loss: 0.00045928
Iteration 12/25 | Loss: 0.00045928
Iteration 13/25 | Loss: 0.00045928
Iteration 14/25 | Loss: 0.00045928
Iteration 15/25 | Loss: 0.00045928
Iteration 16/25 | Loss: 0.00045928
Iteration 17/25 | Loss: 0.00045928
Iteration 18/25 | Loss: 0.00045928
Iteration 19/25 | Loss: 0.00045928
Iteration 20/25 | Loss: 0.00045928
Iteration 21/25 | Loss: 0.00045928
Iteration 22/25 | Loss: 0.00045928
Iteration 23/25 | Loss: 0.00045928
Iteration 24/25 | Loss: 0.00045928
Iteration 25/25 | Loss: 0.00045928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045928
Iteration 2/1000 | Loss: 0.00002325
Iteration 3/1000 | Loss: 0.00001589
Iteration 4/1000 | Loss: 0.00001400
Iteration 5/1000 | Loss: 0.00001308
Iteration 6/1000 | Loss: 0.00001257
Iteration 7/1000 | Loss: 0.00001208
Iteration 8/1000 | Loss: 0.00001186
Iteration 9/1000 | Loss: 0.00001185
Iteration 10/1000 | Loss: 0.00001180
Iteration 11/1000 | Loss: 0.00001180
Iteration 12/1000 | Loss: 0.00001178
Iteration 13/1000 | Loss: 0.00001178
Iteration 14/1000 | Loss: 0.00001177
Iteration 15/1000 | Loss: 0.00001175
Iteration 16/1000 | Loss: 0.00001173
Iteration 17/1000 | Loss: 0.00001172
Iteration 18/1000 | Loss: 0.00001171
Iteration 19/1000 | Loss: 0.00001170
Iteration 20/1000 | Loss: 0.00001169
Iteration 21/1000 | Loss: 0.00001169
Iteration 22/1000 | Loss: 0.00001169
Iteration 23/1000 | Loss: 0.00001165
Iteration 24/1000 | Loss: 0.00001165
Iteration 25/1000 | Loss: 0.00001164
Iteration 26/1000 | Loss: 0.00001164
Iteration 27/1000 | Loss: 0.00001164
Iteration 28/1000 | Loss: 0.00001163
Iteration 29/1000 | Loss: 0.00001161
Iteration 30/1000 | Loss: 0.00001158
Iteration 31/1000 | Loss: 0.00001154
Iteration 32/1000 | Loss: 0.00001153
Iteration 33/1000 | Loss: 0.00001152
Iteration 34/1000 | Loss: 0.00001152
Iteration 35/1000 | Loss: 0.00001152
Iteration 36/1000 | Loss: 0.00001151
Iteration 37/1000 | Loss: 0.00001151
Iteration 38/1000 | Loss: 0.00001150
Iteration 39/1000 | Loss: 0.00001145
Iteration 40/1000 | Loss: 0.00001142
Iteration 41/1000 | Loss: 0.00001142
Iteration 42/1000 | Loss: 0.00001142
Iteration 43/1000 | Loss: 0.00001141
Iteration 44/1000 | Loss: 0.00001141
Iteration 45/1000 | Loss: 0.00001141
Iteration 46/1000 | Loss: 0.00001141
Iteration 47/1000 | Loss: 0.00001139
Iteration 48/1000 | Loss: 0.00001138
Iteration 49/1000 | Loss: 0.00001138
Iteration 50/1000 | Loss: 0.00001138
Iteration 51/1000 | Loss: 0.00001138
Iteration 52/1000 | Loss: 0.00001137
Iteration 53/1000 | Loss: 0.00001137
Iteration 54/1000 | Loss: 0.00001137
Iteration 55/1000 | Loss: 0.00001136
Iteration 56/1000 | Loss: 0.00001136
Iteration 57/1000 | Loss: 0.00001136
Iteration 58/1000 | Loss: 0.00001135
Iteration 59/1000 | Loss: 0.00001135
Iteration 60/1000 | Loss: 0.00001135
Iteration 61/1000 | Loss: 0.00001135
Iteration 62/1000 | Loss: 0.00001135
Iteration 63/1000 | Loss: 0.00001135
Iteration 64/1000 | Loss: 0.00001135
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001134
Iteration 67/1000 | Loss: 0.00001134
Iteration 68/1000 | Loss: 0.00001134
Iteration 69/1000 | Loss: 0.00001134
Iteration 70/1000 | Loss: 0.00001134
Iteration 71/1000 | Loss: 0.00001133
Iteration 72/1000 | Loss: 0.00001133
Iteration 73/1000 | Loss: 0.00001133
Iteration 74/1000 | Loss: 0.00001133
Iteration 75/1000 | Loss: 0.00001133
Iteration 76/1000 | Loss: 0.00001133
Iteration 77/1000 | Loss: 0.00001133
Iteration 78/1000 | Loss: 0.00001133
Iteration 79/1000 | Loss: 0.00001133
Iteration 80/1000 | Loss: 0.00001133
Iteration 81/1000 | Loss: 0.00001132
Iteration 82/1000 | Loss: 0.00001132
Iteration 83/1000 | Loss: 0.00001132
Iteration 84/1000 | Loss: 0.00001132
Iteration 85/1000 | Loss: 0.00001132
Iteration 86/1000 | Loss: 0.00001132
Iteration 87/1000 | Loss: 0.00001132
Iteration 88/1000 | Loss: 0.00001132
Iteration 89/1000 | Loss: 0.00001132
Iteration 90/1000 | Loss: 0.00001132
Iteration 91/1000 | Loss: 0.00001132
Iteration 92/1000 | Loss: 0.00001131
Iteration 93/1000 | Loss: 0.00001131
Iteration 94/1000 | Loss: 0.00001131
Iteration 95/1000 | Loss: 0.00001131
Iteration 96/1000 | Loss: 0.00001131
Iteration 97/1000 | Loss: 0.00001131
Iteration 98/1000 | Loss: 0.00001131
Iteration 99/1000 | Loss: 0.00001130
Iteration 100/1000 | Loss: 0.00001130
Iteration 101/1000 | Loss: 0.00001130
Iteration 102/1000 | Loss: 0.00001130
Iteration 103/1000 | Loss: 0.00001130
Iteration 104/1000 | Loss: 0.00001130
Iteration 105/1000 | Loss: 0.00001130
Iteration 106/1000 | Loss: 0.00001130
Iteration 107/1000 | Loss: 0.00001130
Iteration 108/1000 | Loss: 0.00001130
Iteration 109/1000 | Loss: 0.00001130
Iteration 110/1000 | Loss: 0.00001129
Iteration 111/1000 | Loss: 0.00001129
Iteration 112/1000 | Loss: 0.00001129
Iteration 113/1000 | Loss: 0.00001129
Iteration 114/1000 | Loss: 0.00001129
Iteration 115/1000 | Loss: 0.00001129
Iteration 116/1000 | Loss: 0.00001129
Iteration 117/1000 | Loss: 0.00001129
Iteration 118/1000 | Loss: 0.00001129
Iteration 119/1000 | Loss: 0.00001129
Iteration 120/1000 | Loss: 0.00001129
Iteration 121/1000 | Loss: 0.00001129
Iteration 122/1000 | Loss: 0.00001129
Iteration 123/1000 | Loss: 0.00001129
Iteration 124/1000 | Loss: 0.00001129
Iteration 125/1000 | Loss: 0.00001129
Iteration 126/1000 | Loss: 0.00001129
Iteration 127/1000 | Loss: 0.00001129
Iteration 128/1000 | Loss: 0.00001129
Iteration 129/1000 | Loss: 0.00001129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.128565509134205e-05, 1.128565509134205e-05, 1.128565509134205e-05, 1.128565509134205e-05, 1.128565509134205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.128565509134205e-05

Optimization complete. Final v2v error: 2.930063486099243 mm

Highest mean error: 3.204831838607788 mm for frame 29

Lowest mean error: 2.6218087673187256 mm for frame 15

Saving results

Total time: 32.47143602371216
