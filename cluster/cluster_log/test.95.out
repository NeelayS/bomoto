Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=95, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5320-5375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988663
Iteration 2/25 | Loss: 0.00988663
Iteration 3/25 | Loss: 0.00306128
Iteration 4/25 | Loss: 0.00223895
Iteration 5/25 | Loss: 0.00211024
Iteration 6/25 | Loss: 0.00196457
Iteration 7/25 | Loss: 0.00189423
Iteration 8/25 | Loss: 0.00159080
Iteration 9/25 | Loss: 0.00132664
Iteration 10/25 | Loss: 0.00126733
Iteration 11/25 | Loss: 0.00125011
Iteration 12/25 | Loss: 0.00124620
Iteration 13/25 | Loss: 0.00123619
Iteration 14/25 | Loss: 0.00123362
Iteration 15/25 | Loss: 0.00123240
Iteration 16/25 | Loss: 0.00123673
Iteration 17/25 | Loss: 0.00123201
Iteration 18/25 | Loss: 0.00122927
Iteration 19/25 | Loss: 0.00122713
Iteration 20/25 | Loss: 0.00122562
Iteration 21/25 | Loss: 0.00122472
Iteration 22/25 | Loss: 0.00122435
Iteration 23/25 | Loss: 0.00122419
Iteration 24/25 | Loss: 0.00122418
Iteration 25/25 | Loss: 0.00122418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25615275
Iteration 2/25 | Loss: 0.00229979
Iteration 3/25 | Loss: 0.00229979
Iteration 4/25 | Loss: 0.00229979
Iteration 5/25 | Loss: 0.00229979
Iteration 6/25 | Loss: 0.00229979
Iteration 7/25 | Loss: 0.00229979
Iteration 8/25 | Loss: 0.00229978
Iteration 9/25 | Loss: 0.00229978
Iteration 10/25 | Loss: 0.00229978
Iteration 11/25 | Loss: 0.00229978
Iteration 12/25 | Loss: 0.00229978
Iteration 13/25 | Loss: 0.00229978
Iteration 14/25 | Loss: 0.00229978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0022997844498604536, 0.0022997844498604536, 0.0022997844498604536, 0.0022997844498604536, 0.0022997844498604536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022997844498604536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229978
Iteration 2/1000 | Loss: 0.00101374
Iteration 3/1000 | Loss: 0.00018400
Iteration 4/1000 | Loss: 0.00013560
Iteration 5/1000 | Loss: 0.00010841
Iteration 6/1000 | Loss: 0.00009365
Iteration 7/1000 | Loss: 0.00007884
Iteration 8/1000 | Loss: 0.00007213
Iteration 9/1000 | Loss: 0.00006614
Iteration 10/1000 | Loss: 0.00006274
Iteration 11/1000 | Loss: 0.00006066
Iteration 12/1000 | Loss: 0.00025228
Iteration 13/1000 | Loss: 0.00080211
Iteration 14/1000 | Loss: 0.00043962
Iteration 15/1000 | Loss: 0.00010869
Iteration 16/1000 | Loss: 0.00006670
Iteration 17/1000 | Loss: 0.00006104
Iteration 18/1000 | Loss: 0.00005390
Iteration 19/1000 | Loss: 0.00042140
Iteration 20/1000 | Loss: 0.00183048
Iteration 21/1000 | Loss: 0.00009465
Iteration 22/1000 | Loss: 0.00006464
Iteration 23/1000 | Loss: 0.00004774
Iteration 24/1000 | Loss: 0.00003693
Iteration 25/1000 | Loss: 0.00002832
Iteration 26/1000 | Loss: 0.00002494
Iteration 27/1000 | Loss: 0.00002298
Iteration 28/1000 | Loss: 0.00002170
Iteration 29/1000 | Loss: 0.00002081
Iteration 30/1000 | Loss: 0.00002022
Iteration 31/1000 | Loss: 0.00001973
Iteration 32/1000 | Loss: 0.00001953
Iteration 33/1000 | Loss: 0.00001945
Iteration 34/1000 | Loss: 0.00001937
Iteration 35/1000 | Loss: 0.00001937
Iteration 36/1000 | Loss: 0.00001937
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001928
Iteration 40/1000 | Loss: 0.00001920
Iteration 41/1000 | Loss: 0.00001920
Iteration 42/1000 | Loss: 0.00001920
Iteration 43/1000 | Loss: 0.00001920
Iteration 44/1000 | Loss: 0.00001920
Iteration 45/1000 | Loss: 0.00001920
Iteration 46/1000 | Loss: 0.00001920
Iteration 47/1000 | Loss: 0.00001920
Iteration 48/1000 | Loss: 0.00001920
Iteration 49/1000 | Loss: 0.00001919
Iteration 50/1000 | Loss: 0.00001919
Iteration 51/1000 | Loss: 0.00001919
Iteration 52/1000 | Loss: 0.00001918
Iteration 53/1000 | Loss: 0.00001918
Iteration 54/1000 | Loss: 0.00001917
Iteration 55/1000 | Loss: 0.00001917
Iteration 56/1000 | Loss: 0.00001917
Iteration 57/1000 | Loss: 0.00001917
Iteration 58/1000 | Loss: 0.00001917
Iteration 59/1000 | Loss: 0.00001917
Iteration 60/1000 | Loss: 0.00001917
Iteration 61/1000 | Loss: 0.00001916
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001914
Iteration 66/1000 | Loss: 0.00001914
Iteration 67/1000 | Loss: 0.00001914
Iteration 68/1000 | Loss: 0.00001914
Iteration 69/1000 | Loss: 0.00001914
Iteration 70/1000 | Loss: 0.00001914
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001914
Iteration 73/1000 | Loss: 0.00001913
Iteration 74/1000 | Loss: 0.00001913
Iteration 75/1000 | Loss: 0.00001913
Iteration 76/1000 | Loss: 0.00001913
Iteration 77/1000 | Loss: 0.00001913
Iteration 78/1000 | Loss: 0.00001913
Iteration 79/1000 | Loss: 0.00001913
Iteration 80/1000 | Loss: 0.00001913
Iteration 81/1000 | Loss: 0.00001913
Iteration 82/1000 | Loss: 0.00001913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.913376763695851e-05, 1.913376763695851e-05, 1.913376763695851e-05, 1.913376763695851e-05, 1.913376763695851e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.913376763695851e-05

Optimization complete. Final v2v error: 3.7254726886749268 mm

Highest mean error: 5.183455944061279 mm for frame 83

Lowest mean error: 3.016767978668213 mm for frame 163

Saving results

Total time: 96.11685800552368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891233
Iteration 2/25 | Loss: 0.00114734
Iteration 3/25 | Loss: 0.00103111
Iteration 4/25 | Loss: 0.00101008
Iteration 5/25 | Loss: 0.00100225
Iteration 6/25 | Loss: 0.00099995
Iteration 7/25 | Loss: 0.00099989
Iteration 8/25 | Loss: 0.00099989
Iteration 9/25 | Loss: 0.00099989
Iteration 10/25 | Loss: 0.00099989
Iteration 11/25 | Loss: 0.00099989
Iteration 12/25 | Loss: 0.00099989
Iteration 13/25 | Loss: 0.00099989
Iteration 14/25 | Loss: 0.00099989
Iteration 15/25 | Loss: 0.00099989
Iteration 16/25 | Loss: 0.00099989
Iteration 17/25 | Loss: 0.00099989
Iteration 18/25 | Loss: 0.00099989
Iteration 19/25 | Loss: 0.00099989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009998924797400832, 0.0009998924797400832, 0.0009998924797400832, 0.0009998924797400832, 0.0009998924797400832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009998924797400832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26693833
Iteration 2/25 | Loss: 0.00194173
Iteration 3/25 | Loss: 0.00194173
Iteration 4/25 | Loss: 0.00194173
Iteration 5/25 | Loss: 0.00194172
Iteration 6/25 | Loss: 0.00194172
Iteration 7/25 | Loss: 0.00194172
Iteration 8/25 | Loss: 0.00194172
Iteration 9/25 | Loss: 0.00194172
Iteration 10/25 | Loss: 0.00194172
Iteration 11/25 | Loss: 0.00194172
Iteration 12/25 | Loss: 0.00194172
Iteration 13/25 | Loss: 0.00194172
Iteration 14/25 | Loss: 0.00194172
Iteration 15/25 | Loss: 0.00194172
Iteration 16/25 | Loss: 0.00194172
Iteration 17/25 | Loss: 0.00194172
Iteration 18/25 | Loss: 0.00194172
Iteration 19/25 | Loss: 0.00194172
Iteration 20/25 | Loss: 0.00194172
Iteration 21/25 | Loss: 0.00194172
Iteration 22/25 | Loss: 0.00194172
Iteration 23/25 | Loss: 0.00194172
Iteration 24/25 | Loss: 0.00194172
Iteration 25/25 | Loss: 0.00194172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194172
Iteration 2/1000 | Loss: 0.00005567
Iteration 3/1000 | Loss: 0.00003233
Iteration 4/1000 | Loss: 0.00002405
Iteration 5/1000 | Loss: 0.00002228
Iteration 6/1000 | Loss: 0.00002113
Iteration 7/1000 | Loss: 0.00002050
Iteration 8/1000 | Loss: 0.00001989
Iteration 9/1000 | Loss: 0.00001955
Iteration 10/1000 | Loss: 0.00001930
Iteration 11/1000 | Loss: 0.00001912
Iteration 12/1000 | Loss: 0.00001894
Iteration 13/1000 | Loss: 0.00001892
Iteration 14/1000 | Loss: 0.00001877
Iteration 15/1000 | Loss: 0.00001873
Iteration 16/1000 | Loss: 0.00001865
Iteration 17/1000 | Loss: 0.00001856
Iteration 18/1000 | Loss: 0.00001856
Iteration 19/1000 | Loss: 0.00001856
Iteration 20/1000 | Loss: 0.00001856
Iteration 21/1000 | Loss: 0.00001855
Iteration 22/1000 | Loss: 0.00001854
Iteration 23/1000 | Loss: 0.00001852
Iteration 24/1000 | Loss: 0.00001852
Iteration 25/1000 | Loss: 0.00001852
Iteration 26/1000 | Loss: 0.00001851
Iteration 27/1000 | Loss: 0.00001851
Iteration 28/1000 | Loss: 0.00001851
Iteration 29/1000 | Loss: 0.00001851
Iteration 30/1000 | Loss: 0.00001851
Iteration 31/1000 | Loss: 0.00001851
Iteration 32/1000 | Loss: 0.00001851
Iteration 33/1000 | Loss: 0.00001851
Iteration 34/1000 | Loss: 0.00001850
Iteration 35/1000 | Loss: 0.00001850
Iteration 36/1000 | Loss: 0.00001850
Iteration 37/1000 | Loss: 0.00001849
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001849
Iteration 40/1000 | Loss: 0.00001848
Iteration 41/1000 | Loss: 0.00001847
Iteration 42/1000 | Loss: 0.00001847
Iteration 43/1000 | Loss: 0.00001846
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001845
Iteration 46/1000 | Loss: 0.00001845
Iteration 47/1000 | Loss: 0.00001845
Iteration 48/1000 | Loss: 0.00001844
Iteration 49/1000 | Loss: 0.00001844
Iteration 50/1000 | Loss: 0.00001844
Iteration 51/1000 | Loss: 0.00001844
Iteration 52/1000 | Loss: 0.00001843
Iteration 53/1000 | Loss: 0.00001843
Iteration 54/1000 | Loss: 0.00001843
Iteration 55/1000 | Loss: 0.00001843
Iteration 56/1000 | Loss: 0.00001842
Iteration 57/1000 | Loss: 0.00001842
Iteration 58/1000 | Loss: 0.00001842
Iteration 59/1000 | Loss: 0.00001841
Iteration 60/1000 | Loss: 0.00001841
Iteration 61/1000 | Loss: 0.00001841
Iteration 62/1000 | Loss: 0.00001840
Iteration 63/1000 | Loss: 0.00001840
Iteration 64/1000 | Loss: 0.00001840
Iteration 65/1000 | Loss: 0.00001840
Iteration 66/1000 | Loss: 0.00001840
Iteration 67/1000 | Loss: 0.00001839
Iteration 68/1000 | Loss: 0.00001839
Iteration 69/1000 | Loss: 0.00001838
Iteration 70/1000 | Loss: 0.00001838
Iteration 71/1000 | Loss: 0.00001837
Iteration 72/1000 | Loss: 0.00001837
Iteration 73/1000 | Loss: 0.00001837
Iteration 74/1000 | Loss: 0.00001836
Iteration 75/1000 | Loss: 0.00001836
Iteration 76/1000 | Loss: 0.00001835
Iteration 77/1000 | Loss: 0.00001835
Iteration 78/1000 | Loss: 0.00001835
Iteration 79/1000 | Loss: 0.00001834
Iteration 80/1000 | Loss: 0.00001834
Iteration 81/1000 | Loss: 0.00001833
Iteration 82/1000 | Loss: 0.00001831
Iteration 83/1000 | Loss: 0.00001831
Iteration 84/1000 | Loss: 0.00001831
Iteration 85/1000 | Loss: 0.00001830
Iteration 86/1000 | Loss: 0.00001830
Iteration 87/1000 | Loss: 0.00001830
Iteration 88/1000 | Loss: 0.00001830
Iteration 89/1000 | Loss: 0.00001829
Iteration 90/1000 | Loss: 0.00001829
Iteration 91/1000 | Loss: 0.00001829
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001829
Iteration 94/1000 | Loss: 0.00001828
Iteration 95/1000 | Loss: 0.00001828
Iteration 96/1000 | Loss: 0.00001828
Iteration 97/1000 | Loss: 0.00001828
Iteration 98/1000 | Loss: 0.00001828
Iteration 99/1000 | Loss: 0.00001828
Iteration 100/1000 | Loss: 0.00001828
Iteration 101/1000 | Loss: 0.00001828
Iteration 102/1000 | Loss: 0.00001828
Iteration 103/1000 | Loss: 0.00001828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.8281500160810538e-05, 1.8281500160810538e-05, 1.8281500160810538e-05, 1.8281500160810538e-05, 1.8281500160810538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8281500160810538e-05

Optimization complete. Final v2v error: 3.7221484184265137 mm

Highest mean error: 4.10616397857666 mm for frame 168

Lowest mean error: 3.390974521636963 mm for frame 2

Saving results

Total time: 37.934662103652954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864704
Iteration 2/25 | Loss: 0.00109718
Iteration 3/25 | Loss: 0.00096381
Iteration 4/25 | Loss: 0.00094483
Iteration 5/25 | Loss: 0.00093802
Iteration 6/25 | Loss: 0.00093605
Iteration 7/25 | Loss: 0.00093583
Iteration 8/25 | Loss: 0.00093583
Iteration 9/25 | Loss: 0.00093583
Iteration 10/25 | Loss: 0.00093583
Iteration 11/25 | Loss: 0.00093583
Iteration 12/25 | Loss: 0.00093583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009358258103020489, 0.0009358258103020489, 0.0009358258103020489, 0.0009358258103020489, 0.0009358258103020489]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009358258103020489

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.21261644
Iteration 2/25 | Loss: 0.00126264
Iteration 3/25 | Loss: 0.00126264
Iteration 4/25 | Loss: 0.00126264
Iteration 5/25 | Loss: 0.00126264
Iteration 6/25 | Loss: 0.00126264
Iteration 7/25 | Loss: 0.00126264
Iteration 8/25 | Loss: 0.00126264
Iteration 9/25 | Loss: 0.00126264
Iteration 10/25 | Loss: 0.00126264
Iteration 11/25 | Loss: 0.00126264
Iteration 12/25 | Loss: 0.00126264
Iteration 13/25 | Loss: 0.00126264
Iteration 14/25 | Loss: 0.00126264
Iteration 15/25 | Loss: 0.00126264
Iteration 16/25 | Loss: 0.00126264
Iteration 17/25 | Loss: 0.00126264
Iteration 18/25 | Loss: 0.00126264
Iteration 19/25 | Loss: 0.00126264
Iteration 20/25 | Loss: 0.00126264
Iteration 21/25 | Loss: 0.00126264
Iteration 22/25 | Loss: 0.00126264
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012626360403373837, 0.0012626360403373837, 0.0012626360403373837, 0.0012626360403373837, 0.0012626360403373837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012626360403373837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126264
Iteration 2/1000 | Loss: 0.00003007
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001608
Iteration 5/1000 | Loss: 0.00001483
Iteration 6/1000 | Loss: 0.00001407
Iteration 7/1000 | Loss: 0.00001358
Iteration 8/1000 | Loss: 0.00001331
Iteration 9/1000 | Loss: 0.00001322
Iteration 10/1000 | Loss: 0.00001322
Iteration 11/1000 | Loss: 0.00001318
Iteration 12/1000 | Loss: 0.00001318
Iteration 13/1000 | Loss: 0.00001311
Iteration 14/1000 | Loss: 0.00001310
Iteration 15/1000 | Loss: 0.00001309
Iteration 16/1000 | Loss: 0.00001309
Iteration 17/1000 | Loss: 0.00001308
Iteration 18/1000 | Loss: 0.00001307
Iteration 19/1000 | Loss: 0.00001307
Iteration 20/1000 | Loss: 0.00001306
Iteration 21/1000 | Loss: 0.00001304
Iteration 22/1000 | Loss: 0.00001304
Iteration 23/1000 | Loss: 0.00001304
Iteration 24/1000 | Loss: 0.00001303
Iteration 25/1000 | Loss: 0.00001302
Iteration 26/1000 | Loss: 0.00001302
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001299
Iteration 33/1000 | Loss: 0.00001299
Iteration 34/1000 | Loss: 0.00001299
Iteration 35/1000 | Loss: 0.00001299
Iteration 36/1000 | Loss: 0.00001299
Iteration 37/1000 | Loss: 0.00001299
Iteration 38/1000 | Loss: 0.00001298
Iteration 39/1000 | Loss: 0.00001298
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001295
Iteration 43/1000 | Loss: 0.00001290
Iteration 44/1000 | Loss: 0.00001289
Iteration 45/1000 | Loss: 0.00001289
Iteration 46/1000 | Loss: 0.00001288
Iteration 47/1000 | Loss: 0.00001288
Iteration 48/1000 | Loss: 0.00001288
Iteration 49/1000 | Loss: 0.00001288
Iteration 50/1000 | Loss: 0.00001287
Iteration 51/1000 | Loss: 0.00001287
Iteration 52/1000 | Loss: 0.00001287
Iteration 53/1000 | Loss: 0.00001287
Iteration 54/1000 | Loss: 0.00001287
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001286
Iteration 57/1000 | Loss: 0.00001286
Iteration 58/1000 | Loss: 0.00001286
Iteration 59/1000 | Loss: 0.00001286
Iteration 60/1000 | Loss: 0.00001285
Iteration 61/1000 | Loss: 0.00001285
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001285
Iteration 64/1000 | Loss: 0.00001285
Iteration 65/1000 | Loss: 0.00001285
Iteration 66/1000 | Loss: 0.00001285
Iteration 67/1000 | Loss: 0.00001285
Iteration 68/1000 | Loss: 0.00001285
Iteration 69/1000 | Loss: 0.00001285
Iteration 70/1000 | Loss: 0.00001285
Iteration 71/1000 | Loss: 0.00001284
Iteration 72/1000 | Loss: 0.00001284
Iteration 73/1000 | Loss: 0.00001283
Iteration 74/1000 | Loss: 0.00001283
Iteration 75/1000 | Loss: 0.00001283
Iteration 76/1000 | Loss: 0.00001283
Iteration 77/1000 | Loss: 0.00001283
Iteration 78/1000 | Loss: 0.00001283
Iteration 79/1000 | Loss: 0.00001283
Iteration 80/1000 | Loss: 0.00001282
Iteration 81/1000 | Loss: 0.00001282
Iteration 82/1000 | Loss: 0.00001282
Iteration 83/1000 | Loss: 0.00001282
Iteration 84/1000 | Loss: 0.00001282
Iteration 85/1000 | Loss: 0.00001282
Iteration 86/1000 | Loss: 0.00001282
Iteration 87/1000 | Loss: 0.00001282
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001282
Iteration 90/1000 | Loss: 0.00001282
Iteration 91/1000 | Loss: 0.00001282
Iteration 92/1000 | Loss: 0.00001282
Iteration 93/1000 | Loss: 0.00001281
Iteration 94/1000 | Loss: 0.00001281
Iteration 95/1000 | Loss: 0.00001281
Iteration 96/1000 | Loss: 0.00001281
Iteration 97/1000 | Loss: 0.00001281
Iteration 98/1000 | Loss: 0.00001281
Iteration 99/1000 | Loss: 0.00001280
Iteration 100/1000 | Loss: 0.00001280
Iteration 101/1000 | Loss: 0.00001280
Iteration 102/1000 | Loss: 0.00001280
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001279
Iteration 105/1000 | Loss: 0.00001279
Iteration 106/1000 | Loss: 0.00001279
Iteration 107/1000 | Loss: 0.00001278
Iteration 108/1000 | Loss: 0.00001278
Iteration 109/1000 | Loss: 0.00001278
Iteration 110/1000 | Loss: 0.00001278
Iteration 111/1000 | Loss: 0.00001278
Iteration 112/1000 | Loss: 0.00001278
Iteration 113/1000 | Loss: 0.00001278
Iteration 114/1000 | Loss: 0.00001277
Iteration 115/1000 | Loss: 0.00001277
Iteration 116/1000 | Loss: 0.00001277
Iteration 117/1000 | Loss: 0.00001277
Iteration 118/1000 | Loss: 0.00001277
Iteration 119/1000 | Loss: 0.00001277
Iteration 120/1000 | Loss: 0.00001277
Iteration 121/1000 | Loss: 0.00001277
Iteration 122/1000 | Loss: 0.00001276
Iteration 123/1000 | Loss: 0.00001276
Iteration 124/1000 | Loss: 0.00001276
Iteration 125/1000 | Loss: 0.00001276
Iteration 126/1000 | Loss: 0.00001276
Iteration 127/1000 | Loss: 0.00001276
Iteration 128/1000 | Loss: 0.00001276
Iteration 129/1000 | Loss: 0.00001275
Iteration 130/1000 | Loss: 0.00001275
Iteration 131/1000 | Loss: 0.00001275
Iteration 132/1000 | Loss: 0.00001275
Iteration 133/1000 | Loss: 0.00001275
Iteration 134/1000 | Loss: 0.00001275
Iteration 135/1000 | Loss: 0.00001275
Iteration 136/1000 | Loss: 0.00001275
Iteration 137/1000 | Loss: 0.00001275
Iteration 138/1000 | Loss: 0.00001275
Iteration 139/1000 | Loss: 0.00001275
Iteration 140/1000 | Loss: 0.00001275
Iteration 141/1000 | Loss: 0.00001275
Iteration 142/1000 | Loss: 0.00001275
Iteration 143/1000 | Loss: 0.00001274
Iteration 144/1000 | Loss: 0.00001274
Iteration 145/1000 | Loss: 0.00001274
Iteration 146/1000 | Loss: 0.00001274
Iteration 147/1000 | Loss: 0.00001274
Iteration 148/1000 | Loss: 0.00001274
Iteration 149/1000 | Loss: 0.00001274
Iteration 150/1000 | Loss: 0.00001274
Iteration 151/1000 | Loss: 0.00001274
Iteration 152/1000 | Loss: 0.00001274
Iteration 153/1000 | Loss: 0.00001274
Iteration 154/1000 | Loss: 0.00001274
Iteration 155/1000 | Loss: 0.00001274
Iteration 156/1000 | Loss: 0.00001273
Iteration 157/1000 | Loss: 0.00001273
Iteration 158/1000 | Loss: 0.00001273
Iteration 159/1000 | Loss: 0.00001273
Iteration 160/1000 | Loss: 0.00001273
Iteration 161/1000 | Loss: 0.00001273
Iteration 162/1000 | Loss: 0.00001273
Iteration 163/1000 | Loss: 0.00001273
Iteration 164/1000 | Loss: 0.00001273
Iteration 165/1000 | Loss: 0.00001273
Iteration 166/1000 | Loss: 0.00001273
Iteration 167/1000 | Loss: 0.00001273
Iteration 168/1000 | Loss: 0.00001273
Iteration 169/1000 | Loss: 0.00001273
Iteration 170/1000 | Loss: 0.00001273
Iteration 171/1000 | Loss: 0.00001273
Iteration 172/1000 | Loss: 0.00001272
Iteration 173/1000 | Loss: 0.00001272
Iteration 174/1000 | Loss: 0.00001272
Iteration 175/1000 | Loss: 0.00001272
Iteration 176/1000 | Loss: 0.00001272
Iteration 177/1000 | Loss: 0.00001272
Iteration 178/1000 | Loss: 0.00001272
Iteration 179/1000 | Loss: 0.00001272
Iteration 180/1000 | Loss: 0.00001272
Iteration 181/1000 | Loss: 0.00001272
Iteration 182/1000 | Loss: 0.00001272
Iteration 183/1000 | Loss: 0.00001272
Iteration 184/1000 | Loss: 0.00001272
Iteration 185/1000 | Loss: 0.00001271
Iteration 186/1000 | Loss: 0.00001271
Iteration 187/1000 | Loss: 0.00001271
Iteration 188/1000 | Loss: 0.00001271
Iteration 189/1000 | Loss: 0.00001271
Iteration 190/1000 | Loss: 0.00001271
Iteration 191/1000 | Loss: 0.00001271
Iteration 192/1000 | Loss: 0.00001271
Iteration 193/1000 | Loss: 0.00001271
Iteration 194/1000 | Loss: 0.00001271
Iteration 195/1000 | Loss: 0.00001271
Iteration 196/1000 | Loss: 0.00001271
Iteration 197/1000 | Loss: 0.00001271
Iteration 198/1000 | Loss: 0.00001271
Iteration 199/1000 | Loss: 0.00001270
Iteration 200/1000 | Loss: 0.00001270
Iteration 201/1000 | Loss: 0.00001270
Iteration 202/1000 | Loss: 0.00001270
Iteration 203/1000 | Loss: 0.00001270
Iteration 204/1000 | Loss: 0.00001270
Iteration 205/1000 | Loss: 0.00001270
Iteration 206/1000 | Loss: 0.00001270
Iteration 207/1000 | Loss: 0.00001270
Iteration 208/1000 | Loss: 0.00001270
Iteration 209/1000 | Loss: 0.00001270
Iteration 210/1000 | Loss: 0.00001270
Iteration 211/1000 | Loss: 0.00001270
Iteration 212/1000 | Loss: 0.00001270
Iteration 213/1000 | Loss: 0.00001270
Iteration 214/1000 | Loss: 0.00001270
Iteration 215/1000 | Loss: 0.00001270
Iteration 216/1000 | Loss: 0.00001270
Iteration 217/1000 | Loss: 0.00001270
Iteration 218/1000 | Loss: 0.00001270
Iteration 219/1000 | Loss: 0.00001270
Iteration 220/1000 | Loss: 0.00001270
Iteration 221/1000 | Loss: 0.00001270
Iteration 222/1000 | Loss: 0.00001269
Iteration 223/1000 | Loss: 0.00001269
Iteration 224/1000 | Loss: 0.00001269
Iteration 225/1000 | Loss: 0.00001269
Iteration 226/1000 | Loss: 0.00001269
Iteration 227/1000 | Loss: 0.00001269
Iteration 228/1000 | Loss: 0.00001269
Iteration 229/1000 | Loss: 0.00001269
Iteration 230/1000 | Loss: 0.00001269
Iteration 231/1000 | Loss: 0.00001269
Iteration 232/1000 | Loss: 0.00001269
Iteration 233/1000 | Loss: 0.00001269
Iteration 234/1000 | Loss: 0.00001269
Iteration 235/1000 | Loss: 0.00001269
Iteration 236/1000 | Loss: 0.00001269
Iteration 237/1000 | Loss: 0.00001268
Iteration 238/1000 | Loss: 0.00001268
Iteration 239/1000 | Loss: 0.00001268
Iteration 240/1000 | Loss: 0.00001268
Iteration 241/1000 | Loss: 0.00001268
Iteration 242/1000 | Loss: 0.00001268
Iteration 243/1000 | Loss: 0.00001268
Iteration 244/1000 | Loss: 0.00001268
Iteration 245/1000 | Loss: 0.00001268
Iteration 246/1000 | Loss: 0.00001268
Iteration 247/1000 | Loss: 0.00001268
Iteration 248/1000 | Loss: 0.00001268
Iteration 249/1000 | Loss: 0.00001268
Iteration 250/1000 | Loss: 0.00001268
Iteration 251/1000 | Loss: 0.00001268
Iteration 252/1000 | Loss: 0.00001268
Iteration 253/1000 | Loss: 0.00001267
Iteration 254/1000 | Loss: 0.00001267
Iteration 255/1000 | Loss: 0.00001267
Iteration 256/1000 | Loss: 0.00001267
Iteration 257/1000 | Loss: 0.00001267
Iteration 258/1000 | Loss: 0.00001267
Iteration 259/1000 | Loss: 0.00001267
Iteration 260/1000 | Loss: 0.00001267
Iteration 261/1000 | Loss: 0.00001267
Iteration 262/1000 | Loss: 0.00001267
Iteration 263/1000 | Loss: 0.00001267
Iteration 264/1000 | Loss: 0.00001267
Iteration 265/1000 | Loss: 0.00001267
Iteration 266/1000 | Loss: 0.00001267
Iteration 267/1000 | Loss: 0.00001267
Iteration 268/1000 | Loss: 0.00001267
Iteration 269/1000 | Loss: 0.00001267
Iteration 270/1000 | Loss: 0.00001267
Iteration 271/1000 | Loss: 0.00001267
Iteration 272/1000 | Loss: 0.00001267
Iteration 273/1000 | Loss: 0.00001267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [1.2672571756411344e-05, 1.2672571756411344e-05, 1.2672571756411344e-05, 1.2672571756411344e-05, 1.2672571756411344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2672571756411344e-05

Optimization complete. Final v2v error: 2.9433577060699463 mm

Highest mean error: 3.9958395957946777 mm for frame 45

Lowest mean error: 2.5019302368164062 mm for frame 108

Saving results

Total time: 39.96164345741272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965660
Iteration 2/25 | Loss: 0.00158121
Iteration 3/25 | Loss: 0.00113794
Iteration 4/25 | Loss: 0.00111144
Iteration 5/25 | Loss: 0.00110513
Iteration 6/25 | Loss: 0.00110310
Iteration 7/25 | Loss: 0.00110287
Iteration 8/25 | Loss: 0.00110287
Iteration 9/25 | Loss: 0.00110287
Iteration 10/25 | Loss: 0.00110287
Iteration 11/25 | Loss: 0.00110287
Iteration 12/25 | Loss: 0.00110287
Iteration 13/25 | Loss: 0.00110287
Iteration 14/25 | Loss: 0.00110287
Iteration 15/25 | Loss: 0.00110287
Iteration 16/25 | Loss: 0.00110287
Iteration 17/25 | Loss: 0.00110287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011028653243556619, 0.0011028653243556619, 0.0011028653243556619, 0.0011028653243556619, 0.0011028653243556619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011028653243556619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84424800
Iteration 2/25 | Loss: 0.00120459
Iteration 3/25 | Loss: 0.00120458
Iteration 4/25 | Loss: 0.00120457
Iteration 5/25 | Loss: 0.00120457
Iteration 6/25 | Loss: 0.00120457
Iteration 7/25 | Loss: 0.00120457
Iteration 8/25 | Loss: 0.00120457
Iteration 9/25 | Loss: 0.00120457
Iteration 10/25 | Loss: 0.00120457
Iteration 11/25 | Loss: 0.00120457
Iteration 12/25 | Loss: 0.00120457
Iteration 13/25 | Loss: 0.00120457
Iteration 14/25 | Loss: 0.00120457
Iteration 15/25 | Loss: 0.00120457
Iteration 16/25 | Loss: 0.00120457
Iteration 17/25 | Loss: 0.00120457
Iteration 18/25 | Loss: 0.00120457
Iteration 19/25 | Loss: 0.00120457
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012045729672536254, 0.0012045729672536254, 0.0012045729672536254, 0.0012045729672536254, 0.0012045729672536254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012045729672536254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120457
Iteration 2/1000 | Loss: 0.00007358
Iteration 3/1000 | Loss: 0.00005406
Iteration 4/1000 | Loss: 0.00004647
Iteration 5/1000 | Loss: 0.00004343
Iteration 6/1000 | Loss: 0.00004115
Iteration 7/1000 | Loss: 0.00003984
Iteration 8/1000 | Loss: 0.00003879
Iteration 9/1000 | Loss: 0.00003797
Iteration 10/1000 | Loss: 0.00003747
Iteration 11/1000 | Loss: 0.00003714
Iteration 12/1000 | Loss: 0.00003686
Iteration 13/1000 | Loss: 0.00003646
Iteration 14/1000 | Loss: 0.00003615
Iteration 15/1000 | Loss: 0.00003583
Iteration 16/1000 | Loss: 0.00003554
Iteration 17/1000 | Loss: 0.00003529
Iteration 18/1000 | Loss: 0.00003506
Iteration 19/1000 | Loss: 0.00003481
Iteration 20/1000 | Loss: 0.00003458
Iteration 21/1000 | Loss: 0.00003437
Iteration 22/1000 | Loss: 0.00003427
Iteration 23/1000 | Loss: 0.00003422
Iteration 24/1000 | Loss: 0.00003418
Iteration 25/1000 | Loss: 0.00003417
Iteration 26/1000 | Loss: 0.00003417
Iteration 27/1000 | Loss: 0.00003416
Iteration 28/1000 | Loss: 0.00003416
Iteration 29/1000 | Loss: 0.00003416
Iteration 30/1000 | Loss: 0.00003415
Iteration 31/1000 | Loss: 0.00003415
Iteration 32/1000 | Loss: 0.00003414
Iteration 33/1000 | Loss: 0.00003414
Iteration 34/1000 | Loss: 0.00003414
Iteration 35/1000 | Loss: 0.00003413
Iteration 36/1000 | Loss: 0.00003413
Iteration 37/1000 | Loss: 0.00003412
Iteration 38/1000 | Loss: 0.00003412
Iteration 39/1000 | Loss: 0.00003412
Iteration 40/1000 | Loss: 0.00003411
Iteration 41/1000 | Loss: 0.00003411
Iteration 42/1000 | Loss: 0.00003410
Iteration 43/1000 | Loss: 0.00003410
Iteration 44/1000 | Loss: 0.00003409
Iteration 45/1000 | Loss: 0.00003409
Iteration 46/1000 | Loss: 0.00003409
Iteration 47/1000 | Loss: 0.00003409
Iteration 48/1000 | Loss: 0.00003408
Iteration 49/1000 | Loss: 0.00003408
Iteration 50/1000 | Loss: 0.00003408
Iteration 51/1000 | Loss: 0.00003407
Iteration 52/1000 | Loss: 0.00003407
Iteration 53/1000 | Loss: 0.00003407
Iteration 54/1000 | Loss: 0.00003406
Iteration 55/1000 | Loss: 0.00003406
Iteration 56/1000 | Loss: 0.00003405
Iteration 57/1000 | Loss: 0.00003405
Iteration 58/1000 | Loss: 0.00003404
Iteration 59/1000 | Loss: 0.00003404
Iteration 60/1000 | Loss: 0.00003404
Iteration 61/1000 | Loss: 0.00003404
Iteration 62/1000 | Loss: 0.00003404
Iteration 63/1000 | Loss: 0.00003404
Iteration 64/1000 | Loss: 0.00003404
Iteration 65/1000 | Loss: 0.00003404
Iteration 66/1000 | Loss: 0.00003404
Iteration 67/1000 | Loss: 0.00003404
Iteration 68/1000 | Loss: 0.00003404
Iteration 69/1000 | Loss: 0.00003404
Iteration 70/1000 | Loss: 0.00003403
Iteration 71/1000 | Loss: 0.00003403
Iteration 72/1000 | Loss: 0.00003403
Iteration 73/1000 | Loss: 0.00003402
Iteration 74/1000 | Loss: 0.00003402
Iteration 75/1000 | Loss: 0.00003402
Iteration 76/1000 | Loss: 0.00003402
Iteration 77/1000 | Loss: 0.00003402
Iteration 78/1000 | Loss: 0.00003401
Iteration 79/1000 | Loss: 0.00003401
Iteration 80/1000 | Loss: 0.00003401
Iteration 81/1000 | Loss: 0.00003401
Iteration 82/1000 | Loss: 0.00003401
Iteration 83/1000 | Loss: 0.00003401
Iteration 84/1000 | Loss: 0.00003401
Iteration 85/1000 | Loss: 0.00003400
Iteration 86/1000 | Loss: 0.00003400
Iteration 87/1000 | Loss: 0.00003400
Iteration 88/1000 | Loss: 0.00003400
Iteration 89/1000 | Loss: 0.00003400
Iteration 90/1000 | Loss: 0.00003399
Iteration 91/1000 | Loss: 0.00003399
Iteration 92/1000 | Loss: 0.00003399
Iteration 93/1000 | Loss: 0.00003399
Iteration 94/1000 | Loss: 0.00003398
Iteration 95/1000 | Loss: 0.00003398
Iteration 96/1000 | Loss: 0.00003398
Iteration 97/1000 | Loss: 0.00003397
Iteration 98/1000 | Loss: 0.00003397
Iteration 99/1000 | Loss: 0.00003396
Iteration 100/1000 | Loss: 0.00003396
Iteration 101/1000 | Loss: 0.00003396
Iteration 102/1000 | Loss: 0.00003396
Iteration 103/1000 | Loss: 0.00003396
Iteration 104/1000 | Loss: 0.00003396
Iteration 105/1000 | Loss: 0.00003396
Iteration 106/1000 | Loss: 0.00003395
Iteration 107/1000 | Loss: 0.00003394
Iteration 108/1000 | Loss: 0.00003394
Iteration 109/1000 | Loss: 0.00003393
Iteration 110/1000 | Loss: 0.00003393
Iteration 111/1000 | Loss: 0.00003393
Iteration 112/1000 | Loss: 0.00003393
Iteration 113/1000 | Loss: 0.00003393
Iteration 114/1000 | Loss: 0.00003392
Iteration 115/1000 | Loss: 0.00003392
Iteration 116/1000 | Loss: 0.00003392
Iteration 117/1000 | Loss: 0.00003391
Iteration 118/1000 | Loss: 0.00003391
Iteration 119/1000 | Loss: 0.00003391
Iteration 120/1000 | Loss: 0.00003391
Iteration 121/1000 | Loss: 0.00003391
Iteration 122/1000 | Loss: 0.00003391
Iteration 123/1000 | Loss: 0.00003391
Iteration 124/1000 | Loss: 0.00003390
Iteration 125/1000 | Loss: 0.00003389
Iteration 126/1000 | Loss: 0.00003389
Iteration 127/1000 | Loss: 0.00003389
Iteration 128/1000 | Loss: 0.00003389
Iteration 129/1000 | Loss: 0.00003389
Iteration 130/1000 | Loss: 0.00003389
Iteration 131/1000 | Loss: 0.00003389
Iteration 132/1000 | Loss: 0.00003389
Iteration 133/1000 | Loss: 0.00003389
Iteration 134/1000 | Loss: 0.00003388
Iteration 135/1000 | Loss: 0.00003388
Iteration 136/1000 | Loss: 0.00003388
Iteration 137/1000 | Loss: 0.00003388
Iteration 138/1000 | Loss: 0.00003388
Iteration 139/1000 | Loss: 0.00003388
Iteration 140/1000 | Loss: 0.00003388
Iteration 141/1000 | Loss: 0.00003388
Iteration 142/1000 | Loss: 0.00003388
Iteration 143/1000 | Loss: 0.00003387
Iteration 144/1000 | Loss: 0.00003387
Iteration 145/1000 | Loss: 0.00003387
Iteration 146/1000 | Loss: 0.00003387
Iteration 147/1000 | Loss: 0.00003387
Iteration 148/1000 | Loss: 0.00003387
Iteration 149/1000 | Loss: 0.00003387
Iteration 150/1000 | Loss: 0.00003387
Iteration 151/1000 | Loss: 0.00003387
Iteration 152/1000 | Loss: 0.00003387
Iteration 153/1000 | Loss: 0.00003386
Iteration 154/1000 | Loss: 0.00003386
Iteration 155/1000 | Loss: 0.00003386
Iteration 156/1000 | Loss: 0.00003386
Iteration 157/1000 | Loss: 0.00003386
Iteration 158/1000 | Loss: 0.00003386
Iteration 159/1000 | Loss: 0.00003386
Iteration 160/1000 | Loss: 0.00003386
Iteration 161/1000 | Loss: 0.00003386
Iteration 162/1000 | Loss: 0.00003386
Iteration 163/1000 | Loss: 0.00003386
Iteration 164/1000 | Loss: 0.00003386
Iteration 165/1000 | Loss: 0.00003386
Iteration 166/1000 | Loss: 0.00003386
Iteration 167/1000 | Loss: 0.00003386
Iteration 168/1000 | Loss: 0.00003385
Iteration 169/1000 | Loss: 0.00003385
Iteration 170/1000 | Loss: 0.00003385
Iteration 171/1000 | Loss: 0.00003385
Iteration 172/1000 | Loss: 0.00003385
Iteration 173/1000 | Loss: 0.00003385
Iteration 174/1000 | Loss: 0.00003385
Iteration 175/1000 | Loss: 0.00003384
Iteration 176/1000 | Loss: 0.00003384
Iteration 177/1000 | Loss: 0.00003384
Iteration 178/1000 | Loss: 0.00003384
Iteration 179/1000 | Loss: 0.00003384
Iteration 180/1000 | Loss: 0.00003383
Iteration 181/1000 | Loss: 0.00003383
Iteration 182/1000 | Loss: 0.00003383
Iteration 183/1000 | Loss: 0.00003383
Iteration 184/1000 | Loss: 0.00003383
Iteration 185/1000 | Loss: 0.00003383
Iteration 186/1000 | Loss: 0.00003383
Iteration 187/1000 | Loss: 0.00003383
Iteration 188/1000 | Loss: 0.00003383
Iteration 189/1000 | Loss: 0.00003382
Iteration 190/1000 | Loss: 0.00003382
Iteration 191/1000 | Loss: 0.00003382
Iteration 192/1000 | Loss: 0.00003382
Iteration 193/1000 | Loss: 0.00003382
Iteration 194/1000 | Loss: 0.00003382
Iteration 195/1000 | Loss: 0.00003382
Iteration 196/1000 | Loss: 0.00003382
Iteration 197/1000 | Loss: 0.00003382
Iteration 198/1000 | Loss: 0.00003382
Iteration 199/1000 | Loss: 0.00003381
Iteration 200/1000 | Loss: 0.00003381
Iteration 201/1000 | Loss: 0.00003381
Iteration 202/1000 | Loss: 0.00003381
Iteration 203/1000 | Loss: 0.00003381
Iteration 204/1000 | Loss: 0.00003381
Iteration 205/1000 | Loss: 0.00003381
Iteration 206/1000 | Loss: 0.00003381
Iteration 207/1000 | Loss: 0.00003380
Iteration 208/1000 | Loss: 0.00003380
Iteration 209/1000 | Loss: 0.00003380
Iteration 210/1000 | Loss: 0.00003380
Iteration 211/1000 | Loss: 0.00003380
Iteration 212/1000 | Loss: 0.00003380
Iteration 213/1000 | Loss: 0.00003380
Iteration 214/1000 | Loss: 0.00003380
Iteration 215/1000 | Loss: 0.00003380
Iteration 216/1000 | Loss: 0.00003380
Iteration 217/1000 | Loss: 0.00003380
Iteration 218/1000 | Loss: 0.00003380
Iteration 219/1000 | Loss: 0.00003380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [3.380374619155191e-05, 3.380374619155191e-05, 3.380374619155191e-05, 3.380374619155191e-05, 3.380374619155191e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.380374619155191e-05

Optimization complete. Final v2v error: 4.731166362762451 mm

Highest mean error: 5.537900447845459 mm for frame 129

Lowest mean error: 3.738563299179077 mm for frame 28

Saving results

Total time: 56.1078827381134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00749576
Iteration 2/25 | Loss: 0.00161289
Iteration 3/25 | Loss: 0.00126620
Iteration 4/25 | Loss: 0.00104062
Iteration 5/25 | Loss: 0.00100140
Iteration 6/25 | Loss: 0.00097053
Iteration 7/25 | Loss: 0.00095648
Iteration 8/25 | Loss: 0.00095290
Iteration 9/25 | Loss: 0.00095725
Iteration 10/25 | Loss: 0.00095103
Iteration 11/25 | Loss: 0.00094838
Iteration 12/25 | Loss: 0.00094704
Iteration 13/25 | Loss: 0.00094695
Iteration 14/25 | Loss: 0.00094663
Iteration 15/25 | Loss: 0.00094669
Iteration 16/25 | Loss: 0.00094519
Iteration 17/25 | Loss: 0.00094409
Iteration 18/25 | Loss: 0.00094330
Iteration 19/25 | Loss: 0.00094288
Iteration 20/25 | Loss: 0.00094255
Iteration 21/25 | Loss: 0.00094238
Iteration 22/25 | Loss: 0.00094229
Iteration 23/25 | Loss: 0.00094229
Iteration 24/25 | Loss: 0.00094229
Iteration 25/25 | Loss: 0.00094229

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77847946
Iteration 2/25 | Loss: 0.00158449
Iteration 3/25 | Loss: 0.00158449
Iteration 4/25 | Loss: 0.00158449
Iteration 5/25 | Loss: 0.00158448
Iteration 6/25 | Loss: 0.00158448
Iteration 7/25 | Loss: 0.00158448
Iteration 8/25 | Loss: 0.00158448
Iteration 9/25 | Loss: 0.00158448
Iteration 10/25 | Loss: 0.00158448
Iteration 11/25 | Loss: 0.00158448
Iteration 12/25 | Loss: 0.00158448
Iteration 13/25 | Loss: 0.00158448
Iteration 14/25 | Loss: 0.00158448
Iteration 15/25 | Loss: 0.00158448
Iteration 16/25 | Loss: 0.00158448
Iteration 17/25 | Loss: 0.00158448
Iteration 18/25 | Loss: 0.00158448
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00158448307774961, 0.00158448307774961, 0.00158448307774961, 0.00158448307774961, 0.00158448307774961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00158448307774961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158448
Iteration 2/1000 | Loss: 0.00003616
Iteration 3/1000 | Loss: 0.00002304
Iteration 4/1000 | Loss: 0.00016263
Iteration 5/1000 | Loss: 0.00001762
Iteration 6/1000 | Loss: 0.00001701
Iteration 7/1000 | Loss: 0.00001656
Iteration 8/1000 | Loss: 0.00001612
Iteration 9/1000 | Loss: 0.00001583
Iteration 10/1000 | Loss: 0.00001577
Iteration 11/1000 | Loss: 0.00001555
Iteration 12/1000 | Loss: 0.00001548
Iteration 13/1000 | Loss: 0.00001546
Iteration 14/1000 | Loss: 0.00001546
Iteration 15/1000 | Loss: 0.00001544
Iteration 16/1000 | Loss: 0.00001541
Iteration 17/1000 | Loss: 0.00001541
Iteration 18/1000 | Loss: 0.00001539
Iteration 19/1000 | Loss: 0.00001538
Iteration 20/1000 | Loss: 0.00001538
Iteration 21/1000 | Loss: 0.00001536
Iteration 22/1000 | Loss: 0.00001534
Iteration 23/1000 | Loss: 0.00001533
Iteration 24/1000 | Loss: 0.00001532
Iteration 25/1000 | Loss: 0.00001531
Iteration 26/1000 | Loss: 0.00001531
Iteration 27/1000 | Loss: 0.00001525
Iteration 28/1000 | Loss: 0.00001525
Iteration 29/1000 | Loss: 0.00001525
Iteration 30/1000 | Loss: 0.00001525
Iteration 31/1000 | Loss: 0.00001525
Iteration 32/1000 | Loss: 0.00001525
Iteration 33/1000 | Loss: 0.00001525
Iteration 34/1000 | Loss: 0.00001525
Iteration 35/1000 | Loss: 0.00001525
Iteration 36/1000 | Loss: 0.00001524
Iteration 37/1000 | Loss: 0.00001524
Iteration 38/1000 | Loss: 0.00001524
Iteration 39/1000 | Loss: 0.00001523
Iteration 40/1000 | Loss: 0.00001522
Iteration 41/1000 | Loss: 0.00001521
Iteration 42/1000 | Loss: 0.00001521
Iteration 43/1000 | Loss: 0.00001521
Iteration 44/1000 | Loss: 0.00001520
Iteration 45/1000 | Loss: 0.00001520
Iteration 46/1000 | Loss: 0.00001519
Iteration 47/1000 | Loss: 0.00001518
Iteration 48/1000 | Loss: 0.00001518
Iteration 49/1000 | Loss: 0.00001517
Iteration 50/1000 | Loss: 0.00001517
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001515
Iteration 54/1000 | Loss: 0.00001515
Iteration 55/1000 | Loss: 0.00001515
Iteration 56/1000 | Loss: 0.00001515
Iteration 57/1000 | Loss: 0.00001515
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001515
Iteration 60/1000 | Loss: 0.00001515
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001514
Iteration 64/1000 | Loss: 0.00001514
Iteration 65/1000 | Loss: 0.00001514
Iteration 66/1000 | Loss: 0.00001514
Iteration 67/1000 | Loss: 0.00001513
Iteration 68/1000 | Loss: 0.00001513
Iteration 69/1000 | Loss: 0.00001513
Iteration 70/1000 | Loss: 0.00001513
Iteration 71/1000 | Loss: 0.00001512
Iteration 72/1000 | Loss: 0.00001512
Iteration 73/1000 | Loss: 0.00001511
Iteration 74/1000 | Loss: 0.00001511
Iteration 75/1000 | Loss: 0.00001511
Iteration 76/1000 | Loss: 0.00001511
Iteration 77/1000 | Loss: 0.00001511
Iteration 78/1000 | Loss: 0.00001511
Iteration 79/1000 | Loss: 0.00001511
Iteration 80/1000 | Loss: 0.00001510
Iteration 81/1000 | Loss: 0.00001510
Iteration 82/1000 | Loss: 0.00001510
Iteration 83/1000 | Loss: 0.00001510
Iteration 84/1000 | Loss: 0.00001510
Iteration 85/1000 | Loss: 0.00001510
Iteration 86/1000 | Loss: 0.00001510
Iteration 87/1000 | Loss: 0.00001509
Iteration 88/1000 | Loss: 0.00001509
Iteration 89/1000 | Loss: 0.00001508
Iteration 90/1000 | Loss: 0.00001508
Iteration 91/1000 | Loss: 0.00001508
Iteration 92/1000 | Loss: 0.00001507
Iteration 93/1000 | Loss: 0.00001507
Iteration 94/1000 | Loss: 0.00001507
Iteration 95/1000 | Loss: 0.00001507
Iteration 96/1000 | Loss: 0.00001507
Iteration 97/1000 | Loss: 0.00001507
Iteration 98/1000 | Loss: 0.00001506
Iteration 99/1000 | Loss: 0.00001506
Iteration 100/1000 | Loss: 0.00001506
Iteration 101/1000 | Loss: 0.00001506
Iteration 102/1000 | Loss: 0.00001505
Iteration 103/1000 | Loss: 0.00001505
Iteration 104/1000 | Loss: 0.00001504
Iteration 105/1000 | Loss: 0.00001504
Iteration 106/1000 | Loss: 0.00001504
Iteration 107/1000 | Loss: 0.00001504
Iteration 108/1000 | Loss: 0.00001503
Iteration 109/1000 | Loss: 0.00001503
Iteration 110/1000 | Loss: 0.00001503
Iteration 111/1000 | Loss: 0.00001503
Iteration 112/1000 | Loss: 0.00001502
Iteration 113/1000 | Loss: 0.00001502
Iteration 114/1000 | Loss: 0.00001501
Iteration 115/1000 | Loss: 0.00001501
Iteration 116/1000 | Loss: 0.00001501
Iteration 117/1000 | Loss: 0.00001501
Iteration 118/1000 | Loss: 0.00001501
Iteration 119/1000 | Loss: 0.00001501
Iteration 120/1000 | Loss: 0.00001501
Iteration 121/1000 | Loss: 0.00001501
Iteration 122/1000 | Loss: 0.00001501
Iteration 123/1000 | Loss: 0.00001501
Iteration 124/1000 | Loss: 0.00001500
Iteration 125/1000 | Loss: 0.00001500
Iteration 126/1000 | Loss: 0.00001500
Iteration 127/1000 | Loss: 0.00001500
Iteration 128/1000 | Loss: 0.00001500
Iteration 129/1000 | Loss: 0.00001500
Iteration 130/1000 | Loss: 0.00001500
Iteration 131/1000 | Loss: 0.00001500
Iteration 132/1000 | Loss: 0.00001500
Iteration 133/1000 | Loss: 0.00001500
Iteration 134/1000 | Loss: 0.00001500
Iteration 135/1000 | Loss: 0.00001499
Iteration 136/1000 | Loss: 0.00001499
Iteration 137/1000 | Loss: 0.00001499
Iteration 138/1000 | Loss: 0.00001499
Iteration 139/1000 | Loss: 0.00001499
Iteration 140/1000 | Loss: 0.00001499
Iteration 141/1000 | Loss: 0.00001499
Iteration 142/1000 | Loss: 0.00001499
Iteration 143/1000 | Loss: 0.00001499
Iteration 144/1000 | Loss: 0.00001499
Iteration 145/1000 | Loss: 0.00001499
Iteration 146/1000 | Loss: 0.00001499
Iteration 147/1000 | Loss: 0.00001498
Iteration 148/1000 | Loss: 0.00001498
Iteration 149/1000 | Loss: 0.00001498
Iteration 150/1000 | Loss: 0.00001498
Iteration 151/1000 | Loss: 0.00001498
Iteration 152/1000 | Loss: 0.00001498
Iteration 153/1000 | Loss: 0.00001498
Iteration 154/1000 | Loss: 0.00001498
Iteration 155/1000 | Loss: 0.00001498
Iteration 156/1000 | Loss: 0.00001498
Iteration 157/1000 | Loss: 0.00001498
Iteration 158/1000 | Loss: 0.00001498
Iteration 159/1000 | Loss: 0.00001498
Iteration 160/1000 | Loss: 0.00001498
Iteration 161/1000 | Loss: 0.00001498
Iteration 162/1000 | Loss: 0.00001498
Iteration 163/1000 | Loss: 0.00001498
Iteration 164/1000 | Loss: 0.00001498
Iteration 165/1000 | Loss: 0.00001497
Iteration 166/1000 | Loss: 0.00001497
Iteration 167/1000 | Loss: 0.00001497
Iteration 168/1000 | Loss: 0.00001497
Iteration 169/1000 | Loss: 0.00001497
Iteration 170/1000 | Loss: 0.00001497
Iteration 171/1000 | Loss: 0.00001497
Iteration 172/1000 | Loss: 0.00001497
Iteration 173/1000 | Loss: 0.00001497
Iteration 174/1000 | Loss: 0.00001497
Iteration 175/1000 | Loss: 0.00001497
Iteration 176/1000 | Loss: 0.00001497
Iteration 177/1000 | Loss: 0.00001497
Iteration 178/1000 | Loss: 0.00001497
Iteration 179/1000 | Loss: 0.00001497
Iteration 180/1000 | Loss: 0.00001497
Iteration 181/1000 | Loss: 0.00001497
Iteration 182/1000 | Loss: 0.00001497
Iteration 183/1000 | Loss: 0.00001497
Iteration 184/1000 | Loss: 0.00001497
Iteration 185/1000 | Loss: 0.00001497
Iteration 186/1000 | Loss: 0.00001497
Iteration 187/1000 | Loss: 0.00001497
Iteration 188/1000 | Loss: 0.00001497
Iteration 189/1000 | Loss: 0.00001497
Iteration 190/1000 | Loss: 0.00001497
Iteration 191/1000 | Loss: 0.00001497
Iteration 192/1000 | Loss: 0.00001497
Iteration 193/1000 | Loss: 0.00001497
Iteration 194/1000 | Loss: 0.00001497
Iteration 195/1000 | Loss: 0.00001497
Iteration 196/1000 | Loss: 0.00001497
Iteration 197/1000 | Loss: 0.00001497
Iteration 198/1000 | Loss: 0.00001497
Iteration 199/1000 | Loss: 0.00001497
Iteration 200/1000 | Loss: 0.00001497
Iteration 201/1000 | Loss: 0.00001497
Iteration 202/1000 | Loss: 0.00001497
Iteration 203/1000 | Loss: 0.00001497
Iteration 204/1000 | Loss: 0.00001497
Iteration 205/1000 | Loss: 0.00001497
Iteration 206/1000 | Loss: 0.00001497
Iteration 207/1000 | Loss: 0.00001497
Iteration 208/1000 | Loss: 0.00001497
Iteration 209/1000 | Loss: 0.00001497
Iteration 210/1000 | Loss: 0.00001497
Iteration 211/1000 | Loss: 0.00001497
Iteration 212/1000 | Loss: 0.00001497
Iteration 213/1000 | Loss: 0.00001497
Iteration 214/1000 | Loss: 0.00001497
Iteration 215/1000 | Loss: 0.00001497
Iteration 216/1000 | Loss: 0.00001497
Iteration 217/1000 | Loss: 0.00001497
Iteration 218/1000 | Loss: 0.00001497
Iteration 219/1000 | Loss: 0.00001497
Iteration 220/1000 | Loss: 0.00001497
Iteration 221/1000 | Loss: 0.00001497
Iteration 222/1000 | Loss: 0.00001497
Iteration 223/1000 | Loss: 0.00001497
Iteration 224/1000 | Loss: 0.00001497
Iteration 225/1000 | Loss: 0.00001497
Iteration 226/1000 | Loss: 0.00001497
Iteration 227/1000 | Loss: 0.00001497
Iteration 228/1000 | Loss: 0.00001497
Iteration 229/1000 | Loss: 0.00001497
Iteration 230/1000 | Loss: 0.00001497
Iteration 231/1000 | Loss: 0.00001497
Iteration 232/1000 | Loss: 0.00001497
Iteration 233/1000 | Loss: 0.00001497
Iteration 234/1000 | Loss: 0.00001497
Iteration 235/1000 | Loss: 0.00001497
Iteration 236/1000 | Loss: 0.00001497
Iteration 237/1000 | Loss: 0.00001497
Iteration 238/1000 | Loss: 0.00001497
Iteration 239/1000 | Loss: 0.00001497
Iteration 240/1000 | Loss: 0.00001497
Iteration 241/1000 | Loss: 0.00001497
Iteration 242/1000 | Loss: 0.00001497
Iteration 243/1000 | Loss: 0.00001497
Iteration 244/1000 | Loss: 0.00001497
Iteration 245/1000 | Loss: 0.00001497
Iteration 246/1000 | Loss: 0.00001497
Iteration 247/1000 | Loss: 0.00001497
Iteration 248/1000 | Loss: 0.00001497
Iteration 249/1000 | Loss: 0.00001497
Iteration 250/1000 | Loss: 0.00001497
Iteration 251/1000 | Loss: 0.00001497
Iteration 252/1000 | Loss: 0.00001497
Iteration 253/1000 | Loss: 0.00001497
Iteration 254/1000 | Loss: 0.00001497
Iteration 255/1000 | Loss: 0.00001497
Iteration 256/1000 | Loss: 0.00001497
Iteration 257/1000 | Loss: 0.00001497
Iteration 258/1000 | Loss: 0.00001497
Iteration 259/1000 | Loss: 0.00001497
Iteration 260/1000 | Loss: 0.00001497
Iteration 261/1000 | Loss: 0.00001497
Iteration 262/1000 | Loss: 0.00001497
Iteration 263/1000 | Loss: 0.00001497
Iteration 264/1000 | Loss: 0.00001497
Iteration 265/1000 | Loss: 0.00001497
Iteration 266/1000 | Loss: 0.00001497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.4965516129450407e-05, 1.4965516129450407e-05, 1.4965516129450407e-05, 1.4965516129450407e-05, 1.4965516129450407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4965516129450407e-05

Optimization complete. Final v2v error: 3.002249002456665 mm

Highest mean error: 8.707259178161621 mm for frame 180

Lowest mean error: 2.32672119140625 mm for frame 0

Saving results

Total time: 71.53856348991394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830589
Iteration 2/25 | Loss: 0.00138312
Iteration 3/25 | Loss: 0.00110998
Iteration 4/25 | Loss: 0.00108875
Iteration 5/25 | Loss: 0.00108580
Iteration 6/25 | Loss: 0.00108542
Iteration 7/25 | Loss: 0.00108542
Iteration 8/25 | Loss: 0.00108542
Iteration 9/25 | Loss: 0.00108542
Iteration 10/25 | Loss: 0.00108542
Iteration 11/25 | Loss: 0.00108542
Iteration 12/25 | Loss: 0.00108542
Iteration 13/25 | Loss: 0.00108542
Iteration 14/25 | Loss: 0.00108542
Iteration 15/25 | Loss: 0.00108542
Iteration 16/25 | Loss: 0.00108542
Iteration 17/25 | Loss: 0.00108542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010854247957468033, 0.0010854247957468033, 0.0010854247957468033, 0.0010854247957468033, 0.0010854247957468033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010854247957468033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80720860
Iteration 2/25 | Loss: 0.00111262
Iteration 3/25 | Loss: 0.00111262
Iteration 4/25 | Loss: 0.00111262
Iteration 5/25 | Loss: 0.00111261
Iteration 6/25 | Loss: 0.00111261
Iteration 7/25 | Loss: 0.00111261
Iteration 8/25 | Loss: 0.00111261
Iteration 9/25 | Loss: 0.00111261
Iteration 10/25 | Loss: 0.00111261
Iteration 11/25 | Loss: 0.00111261
Iteration 12/25 | Loss: 0.00111261
Iteration 13/25 | Loss: 0.00111261
Iteration 14/25 | Loss: 0.00111261
Iteration 15/25 | Loss: 0.00111261
Iteration 16/25 | Loss: 0.00111261
Iteration 17/25 | Loss: 0.00111261
Iteration 18/25 | Loss: 0.00111261
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011126133613288403, 0.0011126133613288403, 0.0011126133613288403, 0.0011126133613288403, 0.0011126133613288403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011126133613288403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111261
Iteration 2/1000 | Loss: 0.00003609
Iteration 3/1000 | Loss: 0.00002583
Iteration 4/1000 | Loss: 0.00002355
Iteration 5/1000 | Loss: 0.00002242
Iteration 6/1000 | Loss: 0.00002182
Iteration 7/1000 | Loss: 0.00002113
Iteration 8/1000 | Loss: 0.00002068
Iteration 9/1000 | Loss: 0.00002043
Iteration 10/1000 | Loss: 0.00002007
Iteration 11/1000 | Loss: 0.00001989
Iteration 12/1000 | Loss: 0.00001987
Iteration 13/1000 | Loss: 0.00001982
Iteration 14/1000 | Loss: 0.00001963
Iteration 15/1000 | Loss: 0.00001952
Iteration 16/1000 | Loss: 0.00001947
Iteration 17/1000 | Loss: 0.00001946
Iteration 18/1000 | Loss: 0.00001946
Iteration 19/1000 | Loss: 0.00001945
Iteration 20/1000 | Loss: 0.00001945
Iteration 21/1000 | Loss: 0.00001945
Iteration 22/1000 | Loss: 0.00001944
Iteration 23/1000 | Loss: 0.00001944
Iteration 24/1000 | Loss: 0.00001944
Iteration 25/1000 | Loss: 0.00001943
Iteration 26/1000 | Loss: 0.00001940
Iteration 27/1000 | Loss: 0.00001940
Iteration 28/1000 | Loss: 0.00001940
Iteration 29/1000 | Loss: 0.00001940
Iteration 30/1000 | Loss: 0.00001940
Iteration 31/1000 | Loss: 0.00001940
Iteration 32/1000 | Loss: 0.00001940
Iteration 33/1000 | Loss: 0.00001939
Iteration 34/1000 | Loss: 0.00001939
Iteration 35/1000 | Loss: 0.00001939
Iteration 36/1000 | Loss: 0.00001939
Iteration 37/1000 | Loss: 0.00001939
Iteration 38/1000 | Loss: 0.00001939
Iteration 39/1000 | Loss: 0.00001939
Iteration 40/1000 | Loss: 0.00001939
Iteration 41/1000 | Loss: 0.00001939
Iteration 42/1000 | Loss: 0.00001939
Iteration 43/1000 | Loss: 0.00001939
Iteration 44/1000 | Loss: 0.00001939
Iteration 45/1000 | Loss: 0.00001938
Iteration 46/1000 | Loss: 0.00001938
Iteration 47/1000 | Loss: 0.00001938
Iteration 48/1000 | Loss: 0.00001938
Iteration 49/1000 | Loss: 0.00001937
Iteration 50/1000 | Loss: 0.00001937
Iteration 51/1000 | Loss: 0.00001937
Iteration 52/1000 | Loss: 0.00001937
Iteration 53/1000 | Loss: 0.00001937
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001937
Iteration 56/1000 | Loss: 0.00001937
Iteration 57/1000 | Loss: 0.00001937
Iteration 58/1000 | Loss: 0.00001936
Iteration 59/1000 | Loss: 0.00001936
Iteration 60/1000 | Loss: 0.00001936
Iteration 61/1000 | Loss: 0.00001936
Iteration 62/1000 | Loss: 0.00001936
Iteration 63/1000 | Loss: 0.00001936
Iteration 64/1000 | Loss: 0.00001936
Iteration 65/1000 | Loss: 0.00001935
Iteration 66/1000 | Loss: 0.00001935
Iteration 67/1000 | Loss: 0.00001935
Iteration 68/1000 | Loss: 0.00001935
Iteration 69/1000 | Loss: 0.00001935
Iteration 70/1000 | Loss: 0.00001935
Iteration 71/1000 | Loss: 0.00001935
Iteration 72/1000 | Loss: 0.00001935
Iteration 73/1000 | Loss: 0.00001935
Iteration 74/1000 | Loss: 0.00001934
Iteration 75/1000 | Loss: 0.00001934
Iteration 76/1000 | Loss: 0.00001934
Iteration 77/1000 | Loss: 0.00001934
Iteration 78/1000 | Loss: 0.00001934
Iteration 79/1000 | Loss: 0.00001934
Iteration 80/1000 | Loss: 0.00001934
Iteration 81/1000 | Loss: 0.00001934
Iteration 82/1000 | Loss: 0.00001933
Iteration 83/1000 | Loss: 0.00001932
Iteration 84/1000 | Loss: 0.00001932
Iteration 85/1000 | Loss: 0.00001932
Iteration 86/1000 | Loss: 0.00001932
Iteration 87/1000 | Loss: 0.00001932
Iteration 88/1000 | Loss: 0.00001931
Iteration 89/1000 | Loss: 0.00001929
Iteration 90/1000 | Loss: 0.00001929
Iteration 91/1000 | Loss: 0.00001928
Iteration 92/1000 | Loss: 0.00001928
Iteration 93/1000 | Loss: 0.00001928
Iteration 94/1000 | Loss: 0.00001928
Iteration 95/1000 | Loss: 0.00001928
Iteration 96/1000 | Loss: 0.00001928
Iteration 97/1000 | Loss: 0.00001927
Iteration 98/1000 | Loss: 0.00001927
Iteration 99/1000 | Loss: 0.00001926
Iteration 100/1000 | Loss: 0.00001926
Iteration 101/1000 | Loss: 0.00001926
Iteration 102/1000 | Loss: 0.00001926
Iteration 103/1000 | Loss: 0.00001926
Iteration 104/1000 | Loss: 0.00001926
Iteration 105/1000 | Loss: 0.00001926
Iteration 106/1000 | Loss: 0.00001926
Iteration 107/1000 | Loss: 0.00001926
Iteration 108/1000 | Loss: 0.00001926
Iteration 109/1000 | Loss: 0.00001926
Iteration 110/1000 | Loss: 0.00001926
Iteration 111/1000 | Loss: 0.00001925
Iteration 112/1000 | Loss: 0.00001925
Iteration 113/1000 | Loss: 0.00001925
Iteration 114/1000 | Loss: 0.00001924
Iteration 115/1000 | Loss: 0.00001924
Iteration 116/1000 | Loss: 0.00001924
Iteration 117/1000 | Loss: 0.00001924
Iteration 118/1000 | Loss: 0.00001924
Iteration 119/1000 | Loss: 0.00001924
Iteration 120/1000 | Loss: 0.00001924
Iteration 121/1000 | Loss: 0.00001923
Iteration 122/1000 | Loss: 0.00001923
Iteration 123/1000 | Loss: 0.00001923
Iteration 124/1000 | Loss: 0.00001923
Iteration 125/1000 | Loss: 0.00001923
Iteration 126/1000 | Loss: 0.00001923
Iteration 127/1000 | Loss: 0.00001922
Iteration 128/1000 | Loss: 0.00001922
Iteration 129/1000 | Loss: 0.00001922
Iteration 130/1000 | Loss: 0.00001921
Iteration 131/1000 | Loss: 0.00001921
Iteration 132/1000 | Loss: 0.00001921
Iteration 133/1000 | Loss: 0.00001920
Iteration 134/1000 | Loss: 0.00001920
Iteration 135/1000 | Loss: 0.00001920
Iteration 136/1000 | Loss: 0.00001920
Iteration 137/1000 | Loss: 0.00001920
Iteration 138/1000 | Loss: 0.00001920
Iteration 139/1000 | Loss: 0.00001920
Iteration 140/1000 | Loss: 0.00001919
Iteration 141/1000 | Loss: 0.00001919
Iteration 142/1000 | Loss: 0.00001919
Iteration 143/1000 | Loss: 0.00001919
Iteration 144/1000 | Loss: 0.00001919
Iteration 145/1000 | Loss: 0.00001919
Iteration 146/1000 | Loss: 0.00001919
Iteration 147/1000 | Loss: 0.00001919
Iteration 148/1000 | Loss: 0.00001919
Iteration 149/1000 | Loss: 0.00001919
Iteration 150/1000 | Loss: 0.00001919
Iteration 151/1000 | Loss: 0.00001919
Iteration 152/1000 | Loss: 0.00001919
Iteration 153/1000 | Loss: 0.00001918
Iteration 154/1000 | Loss: 0.00001918
Iteration 155/1000 | Loss: 0.00001918
Iteration 156/1000 | Loss: 0.00001918
Iteration 157/1000 | Loss: 0.00001918
Iteration 158/1000 | Loss: 0.00001918
Iteration 159/1000 | Loss: 0.00001918
Iteration 160/1000 | Loss: 0.00001918
Iteration 161/1000 | Loss: 0.00001918
Iteration 162/1000 | Loss: 0.00001918
Iteration 163/1000 | Loss: 0.00001918
Iteration 164/1000 | Loss: 0.00001918
Iteration 165/1000 | Loss: 0.00001918
Iteration 166/1000 | Loss: 0.00001918
Iteration 167/1000 | Loss: 0.00001918
Iteration 168/1000 | Loss: 0.00001918
Iteration 169/1000 | Loss: 0.00001918
Iteration 170/1000 | Loss: 0.00001918
Iteration 171/1000 | Loss: 0.00001918
Iteration 172/1000 | Loss: 0.00001918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.9183700715075247e-05, 1.9183700715075247e-05, 1.9183700715075247e-05, 1.9183700715075247e-05, 1.9183700715075247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9183700715075247e-05

Optimization complete. Final v2v error: 3.7105934619903564 mm

Highest mean error: 3.8035964965820312 mm for frame 109

Lowest mean error: 3.5888400077819824 mm for frame 135

Saving results

Total time: 37.20330047607422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043883
Iteration 2/25 | Loss: 0.00187093
Iteration 3/25 | Loss: 0.00133896
Iteration 4/25 | Loss: 0.00128073
Iteration 5/25 | Loss: 0.00125422
Iteration 6/25 | Loss: 0.00122363
Iteration 7/25 | Loss: 0.00115968
Iteration 8/25 | Loss: 0.00113968
Iteration 9/25 | Loss: 0.00112287
Iteration 10/25 | Loss: 0.00112516
Iteration 11/25 | Loss: 0.00111178
Iteration 12/25 | Loss: 0.00111049
Iteration 13/25 | Loss: 0.00111149
Iteration 14/25 | Loss: 0.00111011
Iteration 15/25 | Loss: 0.00110836
Iteration 16/25 | Loss: 0.00110785
Iteration 17/25 | Loss: 0.00110770
Iteration 18/25 | Loss: 0.00110764
Iteration 19/25 | Loss: 0.00110764
Iteration 20/25 | Loss: 0.00110764
Iteration 21/25 | Loss: 0.00110764
Iteration 22/25 | Loss: 0.00110764
Iteration 23/25 | Loss: 0.00110764
Iteration 24/25 | Loss: 0.00110764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011076426599174738, 0.0011076426599174738, 0.0011076426599174738, 0.0011076426599174738, 0.0011076426599174738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011076426599174738

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21668720
Iteration 2/25 | Loss: 0.00233780
Iteration 3/25 | Loss: 0.00233779
Iteration 4/25 | Loss: 0.00233779
Iteration 5/25 | Loss: 0.00233779
Iteration 6/25 | Loss: 0.00233779
Iteration 7/25 | Loss: 0.00233779
Iteration 8/25 | Loss: 0.00233779
Iteration 9/25 | Loss: 0.00233779
Iteration 10/25 | Loss: 0.00233779
Iteration 11/25 | Loss: 0.00233779
Iteration 12/25 | Loss: 0.00233779
Iteration 13/25 | Loss: 0.00233779
Iteration 14/25 | Loss: 0.00233779
Iteration 15/25 | Loss: 0.00233779
Iteration 16/25 | Loss: 0.00233779
Iteration 17/25 | Loss: 0.00233779
Iteration 18/25 | Loss: 0.00233779
Iteration 19/25 | Loss: 0.00233779
Iteration 20/25 | Loss: 0.00233779
Iteration 21/25 | Loss: 0.00233779
Iteration 22/25 | Loss: 0.00233779
Iteration 23/25 | Loss: 0.00233779
Iteration 24/25 | Loss: 0.00233779
Iteration 25/25 | Loss: 0.00233779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233779
Iteration 2/1000 | Loss: 0.00004368
Iteration 3/1000 | Loss: 0.00002890
Iteration 4/1000 | Loss: 0.00002611
Iteration 5/1000 | Loss: 0.00002431
Iteration 6/1000 | Loss: 0.00002346
Iteration 7/1000 | Loss: 0.00002272
Iteration 8/1000 | Loss: 0.00002182
Iteration 9/1000 | Loss: 0.00002119
Iteration 10/1000 | Loss: 0.00117062
Iteration 11/1000 | Loss: 0.00025358
Iteration 12/1000 | Loss: 0.00036005
Iteration 13/1000 | Loss: 0.00001981
Iteration 14/1000 | Loss: 0.00001703
Iteration 15/1000 | Loss: 0.00001529
Iteration 16/1000 | Loss: 0.00001416
Iteration 17/1000 | Loss: 0.00001362
Iteration 18/1000 | Loss: 0.00026428
Iteration 19/1000 | Loss: 0.00002761
Iteration 20/1000 | Loss: 0.00004004
Iteration 21/1000 | Loss: 0.00001323
Iteration 22/1000 | Loss: 0.00001313
Iteration 23/1000 | Loss: 0.00001295
Iteration 24/1000 | Loss: 0.00001292
Iteration 25/1000 | Loss: 0.00016154
Iteration 26/1000 | Loss: 0.00001294
Iteration 27/1000 | Loss: 0.00001261
Iteration 28/1000 | Loss: 0.00001249
Iteration 29/1000 | Loss: 0.00001246
Iteration 30/1000 | Loss: 0.00018293
Iteration 31/1000 | Loss: 0.00001259
Iteration 32/1000 | Loss: 0.00001231
Iteration 33/1000 | Loss: 0.00001231
Iteration 34/1000 | Loss: 0.00001229
Iteration 35/1000 | Loss: 0.00001229
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001227
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001226
Iteration 40/1000 | Loss: 0.00001226
Iteration 41/1000 | Loss: 0.00001226
Iteration 42/1000 | Loss: 0.00001226
Iteration 43/1000 | Loss: 0.00001226
Iteration 44/1000 | Loss: 0.00001225
Iteration 45/1000 | Loss: 0.00001225
Iteration 46/1000 | Loss: 0.00001225
Iteration 47/1000 | Loss: 0.00001225
Iteration 48/1000 | Loss: 0.00001224
Iteration 49/1000 | Loss: 0.00001224
Iteration 50/1000 | Loss: 0.00001224
Iteration 51/1000 | Loss: 0.00001224
Iteration 52/1000 | Loss: 0.00001223
Iteration 53/1000 | Loss: 0.00001223
Iteration 54/1000 | Loss: 0.00001222
Iteration 55/1000 | Loss: 0.00001222
Iteration 56/1000 | Loss: 0.00001222
Iteration 57/1000 | Loss: 0.00001222
Iteration 58/1000 | Loss: 0.00001222
Iteration 59/1000 | Loss: 0.00001222
Iteration 60/1000 | Loss: 0.00001222
Iteration 61/1000 | Loss: 0.00001222
Iteration 62/1000 | Loss: 0.00001222
Iteration 63/1000 | Loss: 0.00001222
Iteration 64/1000 | Loss: 0.00001221
Iteration 65/1000 | Loss: 0.00001221
Iteration 66/1000 | Loss: 0.00001221
Iteration 67/1000 | Loss: 0.00001220
Iteration 68/1000 | Loss: 0.00001220
Iteration 69/1000 | Loss: 0.00001220
Iteration 70/1000 | Loss: 0.00001220
Iteration 71/1000 | Loss: 0.00001220
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001220
Iteration 74/1000 | Loss: 0.00001219
Iteration 75/1000 | Loss: 0.00001219
Iteration 76/1000 | Loss: 0.00001219
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001218
Iteration 79/1000 | Loss: 0.00001218
Iteration 80/1000 | Loss: 0.00001218
Iteration 81/1000 | Loss: 0.00001218
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001217
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001217
Iteration 95/1000 | Loss: 0.00001217
Iteration 96/1000 | Loss: 0.00001217
Iteration 97/1000 | Loss: 0.00001217
Iteration 98/1000 | Loss: 0.00001217
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001216
Iteration 101/1000 | Loss: 0.00001216
Iteration 102/1000 | Loss: 0.00001216
Iteration 103/1000 | Loss: 0.00001216
Iteration 104/1000 | Loss: 0.00001216
Iteration 105/1000 | Loss: 0.00001216
Iteration 106/1000 | Loss: 0.00001216
Iteration 107/1000 | Loss: 0.00001216
Iteration 108/1000 | Loss: 0.00001216
Iteration 109/1000 | Loss: 0.00001216
Iteration 110/1000 | Loss: 0.00001216
Iteration 111/1000 | Loss: 0.00001216
Iteration 112/1000 | Loss: 0.00001216
Iteration 113/1000 | Loss: 0.00001216
Iteration 114/1000 | Loss: 0.00001215
Iteration 115/1000 | Loss: 0.00001215
Iteration 116/1000 | Loss: 0.00001215
Iteration 117/1000 | Loss: 0.00001215
Iteration 118/1000 | Loss: 0.00001215
Iteration 119/1000 | Loss: 0.00001215
Iteration 120/1000 | Loss: 0.00001215
Iteration 121/1000 | Loss: 0.00001215
Iteration 122/1000 | Loss: 0.00001215
Iteration 123/1000 | Loss: 0.00001215
Iteration 124/1000 | Loss: 0.00001215
Iteration 125/1000 | Loss: 0.00001214
Iteration 126/1000 | Loss: 0.00001214
Iteration 127/1000 | Loss: 0.00001214
Iteration 128/1000 | Loss: 0.00001214
Iteration 129/1000 | Loss: 0.00001214
Iteration 130/1000 | Loss: 0.00001214
Iteration 131/1000 | Loss: 0.00001214
Iteration 132/1000 | Loss: 0.00001214
Iteration 133/1000 | Loss: 0.00001214
Iteration 134/1000 | Loss: 0.00001214
Iteration 135/1000 | Loss: 0.00001214
Iteration 136/1000 | Loss: 0.00001214
Iteration 137/1000 | Loss: 0.00001214
Iteration 138/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.2142178093199618e-05, 1.2142178093199618e-05, 1.2142178093199618e-05, 1.2142178093199618e-05, 1.2142178093199618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2142178093199618e-05

Optimization complete. Final v2v error: 2.996790647506714 mm

Highest mean error: 3.539849281311035 mm for frame 157

Lowest mean error: 2.7070534229278564 mm for frame 231

Saving results

Total time: 90.5961081981659
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797100
Iteration 2/25 | Loss: 0.00181342
Iteration 3/25 | Loss: 0.00121962
Iteration 4/25 | Loss: 0.00114171
Iteration 5/25 | Loss: 0.00113418
Iteration 6/25 | Loss: 0.00113274
Iteration 7/25 | Loss: 0.00113252
Iteration 8/25 | Loss: 0.00113252
Iteration 9/25 | Loss: 0.00113252
Iteration 10/25 | Loss: 0.00113252
Iteration 11/25 | Loss: 0.00113252
Iteration 12/25 | Loss: 0.00113252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011325246887281537, 0.0011325246887281537, 0.0011325246887281537, 0.0011325246887281537, 0.0011325246887281537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011325246887281537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21442163
Iteration 2/25 | Loss: 0.00180972
Iteration 3/25 | Loss: 0.00180971
Iteration 4/25 | Loss: 0.00180971
Iteration 5/25 | Loss: 0.00180970
Iteration 6/25 | Loss: 0.00180970
Iteration 7/25 | Loss: 0.00180970
Iteration 8/25 | Loss: 0.00180970
Iteration 9/25 | Loss: 0.00180970
Iteration 10/25 | Loss: 0.00180970
Iteration 11/25 | Loss: 0.00180970
Iteration 12/25 | Loss: 0.00180970
Iteration 13/25 | Loss: 0.00180970
Iteration 14/25 | Loss: 0.00180970
Iteration 15/25 | Loss: 0.00180970
Iteration 16/25 | Loss: 0.00180970
Iteration 17/25 | Loss: 0.00180970
Iteration 18/25 | Loss: 0.00180970
Iteration 19/25 | Loss: 0.00180970
Iteration 20/25 | Loss: 0.00180970
Iteration 21/25 | Loss: 0.00180970
Iteration 22/25 | Loss: 0.00180970
Iteration 23/25 | Loss: 0.00180970
Iteration 24/25 | Loss: 0.00180970
Iteration 25/25 | Loss: 0.00180970

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180970
Iteration 2/1000 | Loss: 0.00003451
Iteration 3/1000 | Loss: 0.00002292
Iteration 4/1000 | Loss: 0.00001957
Iteration 5/1000 | Loss: 0.00001844
Iteration 6/1000 | Loss: 0.00001793
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001712
Iteration 9/1000 | Loss: 0.00001681
Iteration 10/1000 | Loss: 0.00001659
Iteration 11/1000 | Loss: 0.00001638
Iteration 12/1000 | Loss: 0.00001625
Iteration 13/1000 | Loss: 0.00001619
Iteration 14/1000 | Loss: 0.00001617
Iteration 15/1000 | Loss: 0.00001612
Iteration 16/1000 | Loss: 0.00001611
Iteration 17/1000 | Loss: 0.00001610
Iteration 18/1000 | Loss: 0.00001609
Iteration 19/1000 | Loss: 0.00001608
Iteration 20/1000 | Loss: 0.00001608
Iteration 21/1000 | Loss: 0.00001608
Iteration 22/1000 | Loss: 0.00001608
Iteration 23/1000 | Loss: 0.00001603
Iteration 24/1000 | Loss: 0.00001600
Iteration 25/1000 | Loss: 0.00001598
Iteration 26/1000 | Loss: 0.00001598
Iteration 27/1000 | Loss: 0.00001598
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00001597
Iteration 30/1000 | Loss: 0.00001597
Iteration 31/1000 | Loss: 0.00001597
Iteration 32/1000 | Loss: 0.00001596
Iteration 33/1000 | Loss: 0.00001596
Iteration 34/1000 | Loss: 0.00001595
Iteration 35/1000 | Loss: 0.00001595
Iteration 36/1000 | Loss: 0.00001594
Iteration 37/1000 | Loss: 0.00001593
Iteration 38/1000 | Loss: 0.00001593
Iteration 39/1000 | Loss: 0.00001592
Iteration 40/1000 | Loss: 0.00001592
Iteration 41/1000 | Loss: 0.00001590
Iteration 42/1000 | Loss: 0.00001590
Iteration 43/1000 | Loss: 0.00001590
Iteration 44/1000 | Loss: 0.00001590
Iteration 45/1000 | Loss: 0.00001590
Iteration 46/1000 | Loss: 0.00001590
Iteration 47/1000 | Loss: 0.00001590
Iteration 48/1000 | Loss: 0.00001590
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001590
Iteration 51/1000 | Loss: 0.00001590
Iteration 52/1000 | Loss: 0.00001590
Iteration 53/1000 | Loss: 0.00001589
Iteration 54/1000 | Loss: 0.00001589
Iteration 55/1000 | Loss: 0.00001589
Iteration 56/1000 | Loss: 0.00001589
Iteration 57/1000 | Loss: 0.00001589
Iteration 58/1000 | Loss: 0.00001589
Iteration 59/1000 | Loss: 0.00001589
Iteration 60/1000 | Loss: 0.00001588
Iteration 61/1000 | Loss: 0.00001588
Iteration 62/1000 | Loss: 0.00001588
Iteration 63/1000 | Loss: 0.00001587
Iteration 64/1000 | Loss: 0.00001587
Iteration 65/1000 | Loss: 0.00001587
Iteration 66/1000 | Loss: 0.00001587
Iteration 67/1000 | Loss: 0.00001586
Iteration 68/1000 | Loss: 0.00001586
Iteration 69/1000 | Loss: 0.00001586
Iteration 70/1000 | Loss: 0.00001586
Iteration 71/1000 | Loss: 0.00001586
Iteration 72/1000 | Loss: 0.00001586
Iteration 73/1000 | Loss: 0.00001586
Iteration 74/1000 | Loss: 0.00001586
Iteration 75/1000 | Loss: 0.00001586
Iteration 76/1000 | Loss: 0.00001586
Iteration 77/1000 | Loss: 0.00001585
Iteration 78/1000 | Loss: 0.00001585
Iteration 79/1000 | Loss: 0.00001585
Iteration 80/1000 | Loss: 0.00001585
Iteration 81/1000 | Loss: 0.00001585
Iteration 82/1000 | Loss: 0.00001585
Iteration 83/1000 | Loss: 0.00001585
Iteration 84/1000 | Loss: 0.00001585
Iteration 85/1000 | Loss: 0.00001585
Iteration 86/1000 | Loss: 0.00001585
Iteration 87/1000 | Loss: 0.00001584
Iteration 88/1000 | Loss: 0.00001584
Iteration 89/1000 | Loss: 0.00001584
Iteration 90/1000 | Loss: 0.00001584
Iteration 91/1000 | Loss: 0.00001584
Iteration 92/1000 | Loss: 0.00001584
Iteration 93/1000 | Loss: 0.00001584
Iteration 94/1000 | Loss: 0.00001584
Iteration 95/1000 | Loss: 0.00001584
Iteration 96/1000 | Loss: 0.00001584
Iteration 97/1000 | Loss: 0.00001584
Iteration 98/1000 | Loss: 0.00001584
Iteration 99/1000 | Loss: 0.00001583
Iteration 100/1000 | Loss: 0.00001583
Iteration 101/1000 | Loss: 0.00001583
Iteration 102/1000 | Loss: 0.00001583
Iteration 103/1000 | Loss: 0.00001583
Iteration 104/1000 | Loss: 0.00001583
Iteration 105/1000 | Loss: 0.00001583
Iteration 106/1000 | Loss: 0.00001582
Iteration 107/1000 | Loss: 0.00001582
Iteration 108/1000 | Loss: 0.00001582
Iteration 109/1000 | Loss: 0.00001582
Iteration 110/1000 | Loss: 0.00001582
Iteration 111/1000 | Loss: 0.00001582
Iteration 112/1000 | Loss: 0.00001582
Iteration 113/1000 | Loss: 0.00001582
Iteration 114/1000 | Loss: 0.00001582
Iteration 115/1000 | Loss: 0.00001582
Iteration 116/1000 | Loss: 0.00001582
Iteration 117/1000 | Loss: 0.00001582
Iteration 118/1000 | Loss: 0.00001582
Iteration 119/1000 | Loss: 0.00001582
Iteration 120/1000 | Loss: 0.00001582
Iteration 121/1000 | Loss: 0.00001582
Iteration 122/1000 | Loss: 0.00001581
Iteration 123/1000 | Loss: 0.00001581
Iteration 124/1000 | Loss: 0.00001581
Iteration 125/1000 | Loss: 0.00001581
Iteration 126/1000 | Loss: 0.00001581
Iteration 127/1000 | Loss: 0.00001581
Iteration 128/1000 | Loss: 0.00001580
Iteration 129/1000 | Loss: 0.00001580
Iteration 130/1000 | Loss: 0.00001580
Iteration 131/1000 | Loss: 0.00001580
Iteration 132/1000 | Loss: 0.00001580
Iteration 133/1000 | Loss: 0.00001580
Iteration 134/1000 | Loss: 0.00001579
Iteration 135/1000 | Loss: 0.00001579
Iteration 136/1000 | Loss: 0.00001579
Iteration 137/1000 | Loss: 0.00001579
Iteration 138/1000 | Loss: 0.00001579
Iteration 139/1000 | Loss: 0.00001579
Iteration 140/1000 | Loss: 0.00001579
Iteration 141/1000 | Loss: 0.00001579
Iteration 142/1000 | Loss: 0.00001579
Iteration 143/1000 | Loss: 0.00001579
Iteration 144/1000 | Loss: 0.00001579
Iteration 145/1000 | Loss: 0.00001579
Iteration 146/1000 | Loss: 0.00001579
Iteration 147/1000 | Loss: 0.00001579
Iteration 148/1000 | Loss: 0.00001579
Iteration 149/1000 | Loss: 0.00001579
Iteration 150/1000 | Loss: 0.00001579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.578508090460673e-05, 1.578508090460673e-05, 1.578508090460673e-05, 1.578508090460673e-05, 1.578508090460673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.578508090460673e-05

Optimization complete. Final v2v error: 3.3923757076263428 mm

Highest mean error: 3.7838134765625 mm for frame 6

Lowest mean error: 3.0068910121917725 mm for frame 113

Saving results

Total time: 37.24413204193115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00723084
Iteration 2/25 | Loss: 0.00146107
Iteration 3/25 | Loss: 0.00121003
Iteration 4/25 | Loss: 0.00119142
Iteration 5/25 | Loss: 0.00118544
Iteration 6/25 | Loss: 0.00118375
Iteration 7/25 | Loss: 0.00118375
Iteration 8/25 | Loss: 0.00118375
Iteration 9/25 | Loss: 0.00118375
Iteration 10/25 | Loss: 0.00118375
Iteration 11/25 | Loss: 0.00118375
Iteration 12/25 | Loss: 0.00118375
Iteration 13/25 | Loss: 0.00118375
Iteration 14/25 | Loss: 0.00118375
Iteration 15/25 | Loss: 0.00118375
Iteration 16/25 | Loss: 0.00118375
Iteration 17/25 | Loss: 0.00118375
Iteration 18/25 | Loss: 0.00118375
Iteration 19/25 | Loss: 0.00118375
Iteration 20/25 | Loss: 0.00118375
Iteration 21/25 | Loss: 0.00118375
Iteration 22/25 | Loss: 0.00118375
Iteration 23/25 | Loss: 0.00118375
Iteration 24/25 | Loss: 0.00118375
Iteration 25/25 | Loss: 0.00118375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.73549473
Iteration 2/25 | Loss: 0.00165043
Iteration 3/25 | Loss: 0.00165038
Iteration 4/25 | Loss: 0.00165038
Iteration 5/25 | Loss: 0.00165038
Iteration 6/25 | Loss: 0.00165038
Iteration 7/25 | Loss: 0.00165038
Iteration 8/25 | Loss: 0.00165038
Iteration 9/25 | Loss: 0.00165038
Iteration 10/25 | Loss: 0.00165038
Iteration 11/25 | Loss: 0.00165038
Iteration 12/25 | Loss: 0.00165038
Iteration 13/25 | Loss: 0.00165038
Iteration 14/25 | Loss: 0.00165038
Iteration 15/25 | Loss: 0.00165038
Iteration 16/25 | Loss: 0.00165038
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00165038020350039, 0.00165038020350039, 0.00165038020350039, 0.00165038020350039, 0.00165038020350039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00165038020350039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165038
Iteration 2/1000 | Loss: 0.00007624
Iteration 3/1000 | Loss: 0.00004732
Iteration 4/1000 | Loss: 0.00003939
Iteration 5/1000 | Loss: 0.00003676
Iteration 6/1000 | Loss: 0.00003522
Iteration 7/1000 | Loss: 0.00003472
Iteration 8/1000 | Loss: 0.00003416
Iteration 9/1000 | Loss: 0.00003368
Iteration 10/1000 | Loss: 0.00003328
Iteration 11/1000 | Loss: 0.00003293
Iteration 12/1000 | Loss: 0.00003260
Iteration 13/1000 | Loss: 0.00003236
Iteration 14/1000 | Loss: 0.00003212
Iteration 15/1000 | Loss: 0.00003184
Iteration 16/1000 | Loss: 0.00003164
Iteration 17/1000 | Loss: 0.00003149
Iteration 18/1000 | Loss: 0.00003142
Iteration 19/1000 | Loss: 0.00003132
Iteration 20/1000 | Loss: 0.00003114
Iteration 21/1000 | Loss: 0.00003106
Iteration 22/1000 | Loss: 0.00003090
Iteration 23/1000 | Loss: 0.00003084
Iteration 24/1000 | Loss: 0.00003077
Iteration 25/1000 | Loss: 0.00003070
Iteration 26/1000 | Loss: 0.00003067
Iteration 27/1000 | Loss: 0.00003067
Iteration 28/1000 | Loss: 0.00003064
Iteration 29/1000 | Loss: 0.00003063
Iteration 30/1000 | Loss: 0.00003059
Iteration 31/1000 | Loss: 0.00003059
Iteration 32/1000 | Loss: 0.00003058
Iteration 33/1000 | Loss: 0.00003056
Iteration 34/1000 | Loss: 0.00003054
Iteration 35/1000 | Loss: 0.00003054
Iteration 36/1000 | Loss: 0.00003054
Iteration 37/1000 | Loss: 0.00003053
Iteration 38/1000 | Loss: 0.00003053
Iteration 39/1000 | Loss: 0.00003053
Iteration 40/1000 | Loss: 0.00003053
Iteration 41/1000 | Loss: 0.00003053
Iteration 42/1000 | Loss: 0.00003053
Iteration 43/1000 | Loss: 0.00003051
Iteration 44/1000 | Loss: 0.00003051
Iteration 45/1000 | Loss: 0.00003051
Iteration 46/1000 | Loss: 0.00003050
Iteration 47/1000 | Loss: 0.00003050
Iteration 48/1000 | Loss: 0.00003049
Iteration 49/1000 | Loss: 0.00003049
Iteration 50/1000 | Loss: 0.00003048
Iteration 51/1000 | Loss: 0.00003048
Iteration 52/1000 | Loss: 0.00003048
Iteration 53/1000 | Loss: 0.00003047
Iteration 54/1000 | Loss: 0.00003047
Iteration 55/1000 | Loss: 0.00003047
Iteration 56/1000 | Loss: 0.00003046
Iteration 57/1000 | Loss: 0.00003046
Iteration 58/1000 | Loss: 0.00003046
Iteration 59/1000 | Loss: 0.00003045
Iteration 60/1000 | Loss: 0.00003045
Iteration 61/1000 | Loss: 0.00003045
Iteration 62/1000 | Loss: 0.00003045
Iteration 63/1000 | Loss: 0.00003045
Iteration 64/1000 | Loss: 0.00003045
Iteration 65/1000 | Loss: 0.00003044
Iteration 66/1000 | Loss: 0.00003044
Iteration 67/1000 | Loss: 0.00003044
Iteration 68/1000 | Loss: 0.00003043
Iteration 69/1000 | Loss: 0.00003043
Iteration 70/1000 | Loss: 0.00003043
Iteration 71/1000 | Loss: 0.00003043
Iteration 72/1000 | Loss: 0.00003043
Iteration 73/1000 | Loss: 0.00003043
Iteration 74/1000 | Loss: 0.00003043
Iteration 75/1000 | Loss: 0.00003043
Iteration 76/1000 | Loss: 0.00003043
Iteration 77/1000 | Loss: 0.00003043
Iteration 78/1000 | Loss: 0.00003043
Iteration 79/1000 | Loss: 0.00003043
Iteration 80/1000 | Loss: 0.00003043
Iteration 81/1000 | Loss: 0.00003042
Iteration 82/1000 | Loss: 0.00003042
Iteration 83/1000 | Loss: 0.00003042
Iteration 84/1000 | Loss: 0.00003041
Iteration 85/1000 | Loss: 0.00003041
Iteration 86/1000 | Loss: 0.00003041
Iteration 87/1000 | Loss: 0.00003041
Iteration 88/1000 | Loss: 0.00003041
Iteration 89/1000 | Loss: 0.00003040
Iteration 90/1000 | Loss: 0.00003040
Iteration 91/1000 | Loss: 0.00003040
Iteration 92/1000 | Loss: 0.00003039
Iteration 93/1000 | Loss: 0.00003039
Iteration 94/1000 | Loss: 0.00003039
Iteration 95/1000 | Loss: 0.00003039
Iteration 96/1000 | Loss: 0.00003038
Iteration 97/1000 | Loss: 0.00003038
Iteration 98/1000 | Loss: 0.00003038
Iteration 99/1000 | Loss: 0.00003038
Iteration 100/1000 | Loss: 0.00003038
Iteration 101/1000 | Loss: 0.00003038
Iteration 102/1000 | Loss: 0.00003038
Iteration 103/1000 | Loss: 0.00003038
Iteration 104/1000 | Loss: 0.00003038
Iteration 105/1000 | Loss: 0.00003037
Iteration 106/1000 | Loss: 0.00003037
Iteration 107/1000 | Loss: 0.00003037
Iteration 108/1000 | Loss: 0.00003037
Iteration 109/1000 | Loss: 0.00003037
Iteration 110/1000 | Loss: 0.00003037
Iteration 111/1000 | Loss: 0.00003037
Iteration 112/1000 | Loss: 0.00003036
Iteration 113/1000 | Loss: 0.00003036
Iteration 114/1000 | Loss: 0.00003036
Iteration 115/1000 | Loss: 0.00003036
Iteration 116/1000 | Loss: 0.00003036
Iteration 117/1000 | Loss: 0.00003036
Iteration 118/1000 | Loss: 0.00003036
Iteration 119/1000 | Loss: 0.00003036
Iteration 120/1000 | Loss: 0.00003036
Iteration 121/1000 | Loss: 0.00003036
Iteration 122/1000 | Loss: 0.00003036
Iteration 123/1000 | Loss: 0.00003036
Iteration 124/1000 | Loss: 0.00003036
Iteration 125/1000 | Loss: 0.00003036
Iteration 126/1000 | Loss: 0.00003036
Iteration 127/1000 | Loss: 0.00003036
Iteration 128/1000 | Loss: 0.00003035
Iteration 129/1000 | Loss: 0.00003035
Iteration 130/1000 | Loss: 0.00003035
Iteration 131/1000 | Loss: 0.00003035
Iteration 132/1000 | Loss: 0.00003035
Iteration 133/1000 | Loss: 0.00003035
Iteration 134/1000 | Loss: 0.00003035
Iteration 135/1000 | Loss: 0.00003034
Iteration 136/1000 | Loss: 0.00003034
Iteration 137/1000 | Loss: 0.00003034
Iteration 138/1000 | Loss: 0.00003034
Iteration 139/1000 | Loss: 0.00003034
Iteration 140/1000 | Loss: 0.00003034
Iteration 141/1000 | Loss: 0.00003034
Iteration 142/1000 | Loss: 0.00003034
Iteration 143/1000 | Loss: 0.00003034
Iteration 144/1000 | Loss: 0.00003034
Iteration 145/1000 | Loss: 0.00003034
Iteration 146/1000 | Loss: 0.00003034
Iteration 147/1000 | Loss: 0.00003034
Iteration 148/1000 | Loss: 0.00003034
Iteration 149/1000 | Loss: 0.00003034
Iteration 150/1000 | Loss: 0.00003033
Iteration 151/1000 | Loss: 0.00003033
Iteration 152/1000 | Loss: 0.00003033
Iteration 153/1000 | Loss: 0.00003033
Iteration 154/1000 | Loss: 0.00003033
Iteration 155/1000 | Loss: 0.00003033
Iteration 156/1000 | Loss: 0.00003033
Iteration 157/1000 | Loss: 0.00003033
Iteration 158/1000 | Loss: 0.00003033
Iteration 159/1000 | Loss: 0.00003033
Iteration 160/1000 | Loss: 0.00003033
Iteration 161/1000 | Loss: 0.00003032
Iteration 162/1000 | Loss: 0.00003032
Iteration 163/1000 | Loss: 0.00003032
Iteration 164/1000 | Loss: 0.00003032
Iteration 165/1000 | Loss: 0.00003032
Iteration 166/1000 | Loss: 0.00003032
Iteration 167/1000 | Loss: 0.00003032
Iteration 168/1000 | Loss: 0.00003032
Iteration 169/1000 | Loss: 0.00003032
Iteration 170/1000 | Loss: 0.00003032
Iteration 171/1000 | Loss: 0.00003031
Iteration 172/1000 | Loss: 0.00003031
Iteration 173/1000 | Loss: 0.00003031
Iteration 174/1000 | Loss: 0.00003031
Iteration 175/1000 | Loss: 0.00003031
Iteration 176/1000 | Loss: 0.00003031
Iteration 177/1000 | Loss: 0.00003031
Iteration 178/1000 | Loss: 0.00003031
Iteration 179/1000 | Loss: 0.00003031
Iteration 180/1000 | Loss: 0.00003031
Iteration 181/1000 | Loss: 0.00003031
Iteration 182/1000 | Loss: 0.00003031
Iteration 183/1000 | Loss: 0.00003031
Iteration 184/1000 | Loss: 0.00003031
Iteration 185/1000 | Loss: 0.00003031
Iteration 186/1000 | Loss: 0.00003031
Iteration 187/1000 | Loss: 0.00003031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [3.0314067771541886e-05, 3.0314067771541886e-05, 3.0314067771541886e-05, 3.0314067771541886e-05, 3.0314067771541886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0314067771541886e-05

Optimization complete. Final v2v error: 4.400068759918213 mm

Highest mean error: 5.864218235015869 mm for frame 119

Lowest mean error: 3.4536261558532715 mm for frame 0

Saving results

Total time: 53.82578468322754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816760
Iteration 2/25 | Loss: 0.00144365
Iteration 3/25 | Loss: 0.00117141
Iteration 4/25 | Loss: 0.00115334
Iteration 5/25 | Loss: 0.00114928
Iteration 6/25 | Loss: 0.00114722
Iteration 7/25 | Loss: 0.00114693
Iteration 8/25 | Loss: 0.00114693
Iteration 9/25 | Loss: 0.00114693
Iteration 10/25 | Loss: 0.00114693
Iteration 11/25 | Loss: 0.00114693
Iteration 12/25 | Loss: 0.00114693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011469287564978004, 0.0011469287564978004, 0.0011469287564978004, 0.0011469287564978004, 0.0011469287564978004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011469287564978004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10096300
Iteration 2/25 | Loss: 0.00360008
Iteration 3/25 | Loss: 0.00360008
Iteration 4/25 | Loss: 0.00360008
Iteration 5/25 | Loss: 0.00360008
Iteration 6/25 | Loss: 0.00360008
Iteration 7/25 | Loss: 0.00360007
Iteration 8/25 | Loss: 0.00360007
Iteration 9/25 | Loss: 0.00360007
Iteration 10/25 | Loss: 0.00360007
Iteration 11/25 | Loss: 0.00360007
Iteration 12/25 | Loss: 0.00360007
Iteration 13/25 | Loss: 0.00360007
Iteration 14/25 | Loss: 0.00360007
Iteration 15/25 | Loss: 0.00360007
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0036000742111355066, 0.0036000742111355066, 0.0036000742111355066, 0.0036000742111355066, 0.0036000742111355066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036000742111355066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00360007
Iteration 2/1000 | Loss: 0.00007509
Iteration 3/1000 | Loss: 0.00004347
Iteration 4/1000 | Loss: 0.00003664
Iteration 5/1000 | Loss: 0.00003481
Iteration 6/1000 | Loss: 0.00003393
Iteration 7/1000 | Loss: 0.00003335
Iteration 8/1000 | Loss: 0.00003267
Iteration 9/1000 | Loss: 0.00003194
Iteration 10/1000 | Loss: 0.00003163
Iteration 11/1000 | Loss: 0.00003136
Iteration 12/1000 | Loss: 0.00003108
Iteration 13/1000 | Loss: 0.00003096
Iteration 14/1000 | Loss: 0.00003077
Iteration 15/1000 | Loss: 0.00003075
Iteration 16/1000 | Loss: 0.00003075
Iteration 17/1000 | Loss: 0.00003074
Iteration 18/1000 | Loss: 0.00003061
Iteration 19/1000 | Loss: 0.00003060
Iteration 20/1000 | Loss: 0.00003060
Iteration 21/1000 | Loss: 0.00003054
Iteration 22/1000 | Loss: 0.00003043
Iteration 23/1000 | Loss: 0.00003041
Iteration 24/1000 | Loss: 0.00003041
Iteration 25/1000 | Loss: 0.00003039
Iteration 26/1000 | Loss: 0.00003038
Iteration 27/1000 | Loss: 0.00003038
Iteration 28/1000 | Loss: 0.00003038
Iteration 29/1000 | Loss: 0.00003036
Iteration 30/1000 | Loss: 0.00003036
Iteration 31/1000 | Loss: 0.00003035
Iteration 32/1000 | Loss: 0.00003035
Iteration 33/1000 | Loss: 0.00003034
Iteration 34/1000 | Loss: 0.00003034
Iteration 35/1000 | Loss: 0.00003034
Iteration 36/1000 | Loss: 0.00003034
Iteration 37/1000 | Loss: 0.00003034
Iteration 38/1000 | Loss: 0.00003034
Iteration 39/1000 | Loss: 0.00003034
Iteration 40/1000 | Loss: 0.00003034
Iteration 41/1000 | Loss: 0.00003034
Iteration 42/1000 | Loss: 0.00003034
Iteration 43/1000 | Loss: 0.00003033
Iteration 44/1000 | Loss: 0.00003033
Iteration 45/1000 | Loss: 0.00003032
Iteration 46/1000 | Loss: 0.00003032
Iteration 47/1000 | Loss: 0.00003032
Iteration 48/1000 | Loss: 0.00003032
Iteration 49/1000 | Loss: 0.00003032
Iteration 50/1000 | Loss: 0.00003032
Iteration 51/1000 | Loss: 0.00003032
Iteration 52/1000 | Loss: 0.00003030
Iteration 53/1000 | Loss: 0.00003030
Iteration 54/1000 | Loss: 0.00003029
Iteration 55/1000 | Loss: 0.00003026
Iteration 56/1000 | Loss: 0.00003026
Iteration 57/1000 | Loss: 0.00003026
Iteration 58/1000 | Loss: 0.00003026
Iteration 59/1000 | Loss: 0.00003026
Iteration 60/1000 | Loss: 0.00003026
Iteration 61/1000 | Loss: 0.00003026
Iteration 62/1000 | Loss: 0.00003026
Iteration 63/1000 | Loss: 0.00003026
Iteration 64/1000 | Loss: 0.00003025
Iteration 65/1000 | Loss: 0.00003025
Iteration 66/1000 | Loss: 0.00003025
Iteration 67/1000 | Loss: 0.00003024
Iteration 68/1000 | Loss: 0.00003023
Iteration 69/1000 | Loss: 0.00003023
Iteration 70/1000 | Loss: 0.00003023
Iteration 71/1000 | Loss: 0.00003023
Iteration 72/1000 | Loss: 0.00003023
Iteration 73/1000 | Loss: 0.00003023
Iteration 74/1000 | Loss: 0.00003023
Iteration 75/1000 | Loss: 0.00003023
Iteration 76/1000 | Loss: 0.00003022
Iteration 77/1000 | Loss: 0.00003022
Iteration 78/1000 | Loss: 0.00003022
Iteration 79/1000 | Loss: 0.00003022
Iteration 80/1000 | Loss: 0.00003022
Iteration 81/1000 | Loss: 0.00003022
Iteration 82/1000 | Loss: 0.00003022
Iteration 83/1000 | Loss: 0.00003022
Iteration 84/1000 | Loss: 0.00003022
Iteration 85/1000 | Loss: 0.00003021
Iteration 86/1000 | Loss: 0.00003021
Iteration 87/1000 | Loss: 0.00003021
Iteration 88/1000 | Loss: 0.00003021
Iteration 89/1000 | Loss: 0.00003021
Iteration 90/1000 | Loss: 0.00003021
Iteration 91/1000 | Loss: 0.00003021
Iteration 92/1000 | Loss: 0.00003021
Iteration 93/1000 | Loss: 0.00003021
Iteration 94/1000 | Loss: 0.00003021
Iteration 95/1000 | Loss: 0.00003021
Iteration 96/1000 | Loss: 0.00003020
Iteration 97/1000 | Loss: 0.00003020
Iteration 98/1000 | Loss: 0.00003020
Iteration 99/1000 | Loss: 0.00003020
Iteration 100/1000 | Loss: 0.00003020
Iteration 101/1000 | Loss: 0.00003020
Iteration 102/1000 | Loss: 0.00003020
Iteration 103/1000 | Loss: 0.00003020
Iteration 104/1000 | Loss: 0.00003019
Iteration 105/1000 | Loss: 0.00003019
Iteration 106/1000 | Loss: 0.00003019
Iteration 107/1000 | Loss: 0.00003019
Iteration 108/1000 | Loss: 0.00003019
Iteration 109/1000 | Loss: 0.00003019
Iteration 110/1000 | Loss: 0.00003019
Iteration 111/1000 | Loss: 0.00003019
Iteration 112/1000 | Loss: 0.00003018
Iteration 113/1000 | Loss: 0.00003018
Iteration 114/1000 | Loss: 0.00003018
Iteration 115/1000 | Loss: 0.00003018
Iteration 116/1000 | Loss: 0.00003018
Iteration 117/1000 | Loss: 0.00003018
Iteration 118/1000 | Loss: 0.00003018
Iteration 119/1000 | Loss: 0.00003018
Iteration 120/1000 | Loss: 0.00003018
Iteration 121/1000 | Loss: 0.00003018
Iteration 122/1000 | Loss: 0.00003018
Iteration 123/1000 | Loss: 0.00003017
Iteration 124/1000 | Loss: 0.00003017
Iteration 125/1000 | Loss: 0.00003017
Iteration 126/1000 | Loss: 0.00003017
Iteration 127/1000 | Loss: 0.00003017
Iteration 128/1000 | Loss: 0.00003017
Iteration 129/1000 | Loss: 0.00003017
Iteration 130/1000 | Loss: 0.00003017
Iteration 131/1000 | Loss: 0.00003017
Iteration 132/1000 | Loss: 0.00003017
Iteration 133/1000 | Loss: 0.00003017
Iteration 134/1000 | Loss: 0.00003017
Iteration 135/1000 | Loss: 0.00003017
Iteration 136/1000 | Loss: 0.00003017
Iteration 137/1000 | Loss: 0.00003017
Iteration 138/1000 | Loss: 0.00003017
Iteration 139/1000 | Loss: 0.00003017
Iteration 140/1000 | Loss: 0.00003017
Iteration 141/1000 | Loss: 0.00003017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [3.0166391297825612e-05, 3.0166391297825612e-05, 3.0166391297825612e-05, 3.0166391297825612e-05, 3.0166391297825612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0166391297825612e-05

Optimization complete. Final v2v error: 4.299515247344971 mm

Highest mean error: 4.529751777648926 mm for frame 18

Lowest mean error: 4.059534549713135 mm for frame 0

Saving results

Total time: 39.83461093902588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407867
Iteration 2/25 | Loss: 0.00115095
Iteration 3/25 | Loss: 0.00108955
Iteration 4/25 | Loss: 0.00107794
Iteration 5/25 | Loss: 0.00107496
Iteration 6/25 | Loss: 0.00107446
Iteration 7/25 | Loss: 0.00107446
Iteration 8/25 | Loss: 0.00107446
Iteration 9/25 | Loss: 0.00107446
Iteration 10/25 | Loss: 0.00107446
Iteration 11/25 | Loss: 0.00107446
Iteration 12/25 | Loss: 0.00107446
Iteration 13/25 | Loss: 0.00107446
Iteration 14/25 | Loss: 0.00107446
Iteration 15/25 | Loss: 0.00107446
Iteration 16/25 | Loss: 0.00107446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010744608007371426, 0.0010744608007371426, 0.0010744608007371426, 0.0010744608007371426, 0.0010744608007371426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010744608007371426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23045051
Iteration 2/25 | Loss: 0.00186187
Iteration 3/25 | Loss: 0.00186187
Iteration 4/25 | Loss: 0.00186187
Iteration 5/25 | Loss: 0.00186187
Iteration 6/25 | Loss: 0.00186187
Iteration 7/25 | Loss: 0.00186187
Iteration 8/25 | Loss: 0.00186187
Iteration 9/25 | Loss: 0.00186187
Iteration 10/25 | Loss: 0.00186187
Iteration 11/25 | Loss: 0.00186187
Iteration 12/25 | Loss: 0.00186187
Iteration 13/25 | Loss: 0.00186187
Iteration 14/25 | Loss: 0.00186187
Iteration 15/25 | Loss: 0.00186187
Iteration 16/25 | Loss: 0.00186187
Iteration 17/25 | Loss: 0.00186187
Iteration 18/25 | Loss: 0.00186187
Iteration 19/25 | Loss: 0.00186187
Iteration 20/25 | Loss: 0.00186187
Iteration 21/25 | Loss: 0.00186187
Iteration 22/25 | Loss: 0.00186187
Iteration 23/25 | Loss: 0.00186187
Iteration 24/25 | Loss: 0.00186187
Iteration 25/25 | Loss: 0.00186187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186187
Iteration 2/1000 | Loss: 0.00003106
Iteration 3/1000 | Loss: 0.00001595
Iteration 4/1000 | Loss: 0.00001360
Iteration 5/1000 | Loss: 0.00001283
Iteration 6/1000 | Loss: 0.00001198
Iteration 7/1000 | Loss: 0.00001130
Iteration 8/1000 | Loss: 0.00001104
Iteration 9/1000 | Loss: 0.00001078
Iteration 10/1000 | Loss: 0.00001063
Iteration 11/1000 | Loss: 0.00001048
Iteration 12/1000 | Loss: 0.00001039
Iteration 13/1000 | Loss: 0.00001039
Iteration 14/1000 | Loss: 0.00001039
Iteration 15/1000 | Loss: 0.00001038
Iteration 16/1000 | Loss: 0.00001038
Iteration 17/1000 | Loss: 0.00001037
Iteration 18/1000 | Loss: 0.00001032
Iteration 19/1000 | Loss: 0.00001031
Iteration 20/1000 | Loss: 0.00001015
Iteration 21/1000 | Loss: 0.00001009
Iteration 22/1000 | Loss: 0.00001006
Iteration 23/1000 | Loss: 0.00001006
Iteration 24/1000 | Loss: 0.00001003
Iteration 25/1000 | Loss: 0.00001002
Iteration 26/1000 | Loss: 0.00000997
Iteration 27/1000 | Loss: 0.00000995
Iteration 28/1000 | Loss: 0.00000995
Iteration 29/1000 | Loss: 0.00000995
Iteration 30/1000 | Loss: 0.00000992
Iteration 31/1000 | Loss: 0.00000992
Iteration 32/1000 | Loss: 0.00000991
Iteration 33/1000 | Loss: 0.00000991
Iteration 34/1000 | Loss: 0.00000990
Iteration 35/1000 | Loss: 0.00000990
Iteration 36/1000 | Loss: 0.00000989
Iteration 37/1000 | Loss: 0.00000988
Iteration 38/1000 | Loss: 0.00000988
Iteration 39/1000 | Loss: 0.00000988
Iteration 40/1000 | Loss: 0.00000988
Iteration 41/1000 | Loss: 0.00000988
Iteration 42/1000 | Loss: 0.00000988
Iteration 43/1000 | Loss: 0.00000987
Iteration 44/1000 | Loss: 0.00000987
Iteration 45/1000 | Loss: 0.00000987
Iteration 46/1000 | Loss: 0.00000987
Iteration 47/1000 | Loss: 0.00000986
Iteration 48/1000 | Loss: 0.00000986
Iteration 49/1000 | Loss: 0.00000986
Iteration 50/1000 | Loss: 0.00000985
Iteration 51/1000 | Loss: 0.00000985
Iteration 52/1000 | Loss: 0.00000985
Iteration 53/1000 | Loss: 0.00000984
Iteration 54/1000 | Loss: 0.00000984
Iteration 55/1000 | Loss: 0.00000984
Iteration 56/1000 | Loss: 0.00000984
Iteration 57/1000 | Loss: 0.00000983
Iteration 58/1000 | Loss: 0.00000983
Iteration 59/1000 | Loss: 0.00000982
Iteration 60/1000 | Loss: 0.00000982
Iteration 61/1000 | Loss: 0.00000982
Iteration 62/1000 | Loss: 0.00000982
Iteration 63/1000 | Loss: 0.00000981
Iteration 64/1000 | Loss: 0.00000981
Iteration 65/1000 | Loss: 0.00000981
Iteration 66/1000 | Loss: 0.00000981
Iteration 67/1000 | Loss: 0.00000981
Iteration 68/1000 | Loss: 0.00000981
Iteration 69/1000 | Loss: 0.00000981
Iteration 70/1000 | Loss: 0.00000981
Iteration 71/1000 | Loss: 0.00000981
Iteration 72/1000 | Loss: 0.00000981
Iteration 73/1000 | Loss: 0.00000981
Iteration 74/1000 | Loss: 0.00000981
Iteration 75/1000 | Loss: 0.00000981
Iteration 76/1000 | Loss: 0.00000981
Iteration 77/1000 | Loss: 0.00000980
Iteration 78/1000 | Loss: 0.00000980
Iteration 79/1000 | Loss: 0.00000980
Iteration 80/1000 | Loss: 0.00000980
Iteration 81/1000 | Loss: 0.00000980
Iteration 82/1000 | Loss: 0.00000980
Iteration 83/1000 | Loss: 0.00000980
Iteration 84/1000 | Loss: 0.00000979
Iteration 85/1000 | Loss: 0.00000979
Iteration 86/1000 | Loss: 0.00000979
Iteration 87/1000 | Loss: 0.00000979
Iteration 88/1000 | Loss: 0.00000979
Iteration 89/1000 | Loss: 0.00000979
Iteration 90/1000 | Loss: 0.00000979
Iteration 91/1000 | Loss: 0.00000979
Iteration 92/1000 | Loss: 0.00000979
Iteration 93/1000 | Loss: 0.00000979
Iteration 94/1000 | Loss: 0.00000979
Iteration 95/1000 | Loss: 0.00000979
Iteration 96/1000 | Loss: 0.00000978
Iteration 97/1000 | Loss: 0.00000978
Iteration 98/1000 | Loss: 0.00000978
Iteration 99/1000 | Loss: 0.00000978
Iteration 100/1000 | Loss: 0.00000978
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000978
Iteration 103/1000 | Loss: 0.00000978
Iteration 104/1000 | Loss: 0.00000978
Iteration 105/1000 | Loss: 0.00000978
Iteration 106/1000 | Loss: 0.00000978
Iteration 107/1000 | Loss: 0.00000978
Iteration 108/1000 | Loss: 0.00000978
Iteration 109/1000 | Loss: 0.00000978
Iteration 110/1000 | Loss: 0.00000977
Iteration 111/1000 | Loss: 0.00000977
Iteration 112/1000 | Loss: 0.00000977
Iteration 113/1000 | Loss: 0.00000977
Iteration 114/1000 | Loss: 0.00000976
Iteration 115/1000 | Loss: 0.00000976
Iteration 116/1000 | Loss: 0.00000976
Iteration 117/1000 | Loss: 0.00000976
Iteration 118/1000 | Loss: 0.00000976
Iteration 119/1000 | Loss: 0.00000976
Iteration 120/1000 | Loss: 0.00000976
Iteration 121/1000 | Loss: 0.00000976
Iteration 122/1000 | Loss: 0.00000976
Iteration 123/1000 | Loss: 0.00000976
Iteration 124/1000 | Loss: 0.00000976
Iteration 125/1000 | Loss: 0.00000976
Iteration 126/1000 | Loss: 0.00000976
Iteration 127/1000 | Loss: 0.00000976
Iteration 128/1000 | Loss: 0.00000976
Iteration 129/1000 | Loss: 0.00000976
Iteration 130/1000 | Loss: 0.00000976
Iteration 131/1000 | Loss: 0.00000976
Iteration 132/1000 | Loss: 0.00000976
Iteration 133/1000 | Loss: 0.00000976
Iteration 134/1000 | Loss: 0.00000976
Iteration 135/1000 | Loss: 0.00000976
Iteration 136/1000 | Loss: 0.00000976
Iteration 137/1000 | Loss: 0.00000976
Iteration 138/1000 | Loss: 0.00000976
Iteration 139/1000 | Loss: 0.00000976
Iteration 140/1000 | Loss: 0.00000976
Iteration 141/1000 | Loss: 0.00000976
Iteration 142/1000 | Loss: 0.00000976
Iteration 143/1000 | Loss: 0.00000976
Iteration 144/1000 | Loss: 0.00000976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [9.757557563716546e-06, 9.757557563716546e-06, 9.757557563716546e-06, 9.757557563716546e-06, 9.757557563716546e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.757557563716546e-06

Optimization complete. Final v2v error: 2.7347960472106934 mm

Highest mean error: 3.0713205337524414 mm for frame 189

Lowest mean error: 2.2808234691619873 mm for frame 6

Saving results

Total time: 38.031846046447754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010714
Iteration 2/25 | Loss: 0.00165862
Iteration 3/25 | Loss: 0.00133168
Iteration 4/25 | Loss: 0.00122347
Iteration 5/25 | Loss: 0.00121857
Iteration 6/25 | Loss: 0.00117790
Iteration 7/25 | Loss: 0.00113665
Iteration 8/25 | Loss: 0.00110760
Iteration 9/25 | Loss: 0.00110037
Iteration 10/25 | Loss: 0.00110284
Iteration 11/25 | Loss: 0.00110071
Iteration 12/25 | Loss: 0.00109430
Iteration 13/25 | Loss: 0.00109716
Iteration 14/25 | Loss: 0.00110119
Iteration 15/25 | Loss: 0.00109549
Iteration 16/25 | Loss: 0.00109004
Iteration 17/25 | Loss: 0.00109652
Iteration 18/25 | Loss: 0.00108699
Iteration 19/25 | Loss: 0.00108021
Iteration 20/25 | Loss: 0.00107781
Iteration 21/25 | Loss: 0.00107602
Iteration 22/25 | Loss: 0.00107542
Iteration 23/25 | Loss: 0.00107520
Iteration 24/25 | Loss: 0.00107518
Iteration 25/25 | Loss: 0.00107517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21390629
Iteration 2/25 | Loss: 0.00230413
Iteration 3/25 | Loss: 0.00198651
Iteration 4/25 | Loss: 0.00198651
Iteration 5/25 | Loss: 0.00198651
Iteration 6/25 | Loss: 0.00198651
Iteration 7/25 | Loss: 0.00198651
Iteration 8/25 | Loss: 0.00198651
Iteration 9/25 | Loss: 0.00198651
Iteration 10/25 | Loss: 0.00198651
Iteration 11/25 | Loss: 0.00198651
Iteration 12/25 | Loss: 0.00198651
Iteration 13/25 | Loss: 0.00198651
Iteration 14/25 | Loss: 0.00198651
Iteration 15/25 | Loss: 0.00198651
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001986508024856448, 0.001986508024856448, 0.001986508024856448, 0.001986508024856448, 0.001986508024856448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001986508024856448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198651
Iteration 2/1000 | Loss: 0.00021567
Iteration 3/1000 | Loss: 0.00062319
Iteration 4/1000 | Loss: 0.00005821
Iteration 5/1000 | Loss: 0.00058570
Iteration 6/1000 | Loss: 0.00002349
Iteration 7/1000 | Loss: 0.00005925
Iteration 8/1000 | Loss: 0.00002092
Iteration 9/1000 | Loss: 0.00004102
Iteration 10/1000 | Loss: 0.00005331
Iteration 11/1000 | Loss: 0.00003199
Iteration 12/1000 | Loss: 0.00002841
Iteration 13/1000 | Loss: 0.00010373
Iteration 14/1000 | Loss: 0.00003168
Iteration 15/1000 | Loss: 0.00002961
Iteration 16/1000 | Loss: 0.00004224
Iteration 17/1000 | Loss: 0.00002867
Iteration 18/1000 | Loss: 0.00004354
Iteration 19/1000 | Loss: 0.00002494
Iteration 20/1000 | Loss: 0.00004008
Iteration 21/1000 | Loss: 0.00002224
Iteration 22/1000 | Loss: 0.00001769
Iteration 23/1000 | Loss: 0.00001999
Iteration 24/1000 | Loss: 0.00003551
Iteration 25/1000 | Loss: 0.00010316
Iteration 26/1000 | Loss: 0.00013437
Iteration 27/1000 | Loss: 0.00011309
Iteration 28/1000 | Loss: 0.00004929
Iteration 29/1000 | Loss: 0.00001835
Iteration 30/1000 | Loss: 0.00002790
Iteration 31/1000 | Loss: 0.00001656
Iteration 32/1000 | Loss: 0.00004537
Iteration 33/1000 | Loss: 0.00001306
Iteration 34/1000 | Loss: 0.00006044
Iteration 35/1000 | Loss: 0.00055438
Iteration 36/1000 | Loss: 0.00001472
Iteration 37/1000 | Loss: 0.00001213
Iteration 38/1000 | Loss: 0.00001148
Iteration 39/1000 | Loss: 0.00002934
Iteration 40/1000 | Loss: 0.00005257
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001107
Iteration 43/1000 | Loss: 0.00001098
Iteration 44/1000 | Loss: 0.00001095
Iteration 45/1000 | Loss: 0.00001095
Iteration 46/1000 | Loss: 0.00001095
Iteration 47/1000 | Loss: 0.00001095
Iteration 48/1000 | Loss: 0.00001094
Iteration 49/1000 | Loss: 0.00001094
Iteration 50/1000 | Loss: 0.00001094
Iteration 51/1000 | Loss: 0.00001094
Iteration 52/1000 | Loss: 0.00001094
Iteration 53/1000 | Loss: 0.00001094
Iteration 54/1000 | Loss: 0.00001094
Iteration 55/1000 | Loss: 0.00001092
Iteration 56/1000 | Loss: 0.00002676
Iteration 57/1000 | Loss: 0.00001089
Iteration 58/1000 | Loss: 0.00001082
Iteration 59/1000 | Loss: 0.00001082
Iteration 60/1000 | Loss: 0.00004683
Iteration 61/1000 | Loss: 0.00002264
Iteration 62/1000 | Loss: 0.00001446
Iteration 63/1000 | Loss: 0.00001079
Iteration 64/1000 | Loss: 0.00001077
Iteration 65/1000 | Loss: 0.00001077
Iteration 66/1000 | Loss: 0.00001074
Iteration 67/1000 | Loss: 0.00001074
Iteration 68/1000 | Loss: 0.00001074
Iteration 69/1000 | Loss: 0.00001074
Iteration 70/1000 | Loss: 0.00001073
Iteration 71/1000 | Loss: 0.00001073
Iteration 72/1000 | Loss: 0.00001072
Iteration 73/1000 | Loss: 0.00001072
Iteration 74/1000 | Loss: 0.00001072
Iteration 75/1000 | Loss: 0.00001071
Iteration 76/1000 | Loss: 0.00001071
Iteration 77/1000 | Loss: 0.00001071
Iteration 78/1000 | Loss: 0.00001071
Iteration 79/1000 | Loss: 0.00001071
Iteration 80/1000 | Loss: 0.00001071
Iteration 81/1000 | Loss: 0.00001071
Iteration 82/1000 | Loss: 0.00001070
Iteration 83/1000 | Loss: 0.00001070
Iteration 84/1000 | Loss: 0.00001070
Iteration 85/1000 | Loss: 0.00001070
Iteration 86/1000 | Loss: 0.00001070
Iteration 87/1000 | Loss: 0.00001070
Iteration 88/1000 | Loss: 0.00001070
Iteration 89/1000 | Loss: 0.00001070
Iteration 90/1000 | Loss: 0.00001070
Iteration 91/1000 | Loss: 0.00001069
Iteration 92/1000 | Loss: 0.00001069
Iteration 93/1000 | Loss: 0.00001069
Iteration 94/1000 | Loss: 0.00001069
Iteration 95/1000 | Loss: 0.00001069
Iteration 96/1000 | Loss: 0.00001068
Iteration 97/1000 | Loss: 0.00001068
Iteration 98/1000 | Loss: 0.00001068
Iteration 99/1000 | Loss: 0.00001068
Iteration 100/1000 | Loss: 0.00001067
Iteration 101/1000 | Loss: 0.00001067
Iteration 102/1000 | Loss: 0.00001067
Iteration 103/1000 | Loss: 0.00001067
Iteration 104/1000 | Loss: 0.00001067
Iteration 105/1000 | Loss: 0.00001066
Iteration 106/1000 | Loss: 0.00001066
Iteration 107/1000 | Loss: 0.00001065
Iteration 108/1000 | Loss: 0.00001064
Iteration 109/1000 | Loss: 0.00001064
Iteration 110/1000 | Loss: 0.00001064
Iteration 111/1000 | Loss: 0.00001064
Iteration 112/1000 | Loss: 0.00001063
Iteration 113/1000 | Loss: 0.00001063
Iteration 114/1000 | Loss: 0.00001063
Iteration 115/1000 | Loss: 0.00001063
Iteration 116/1000 | Loss: 0.00001063
Iteration 117/1000 | Loss: 0.00001063
Iteration 118/1000 | Loss: 0.00001063
Iteration 119/1000 | Loss: 0.00001063
Iteration 120/1000 | Loss: 0.00001063
Iteration 121/1000 | Loss: 0.00001063
Iteration 122/1000 | Loss: 0.00001063
Iteration 123/1000 | Loss: 0.00001063
Iteration 124/1000 | Loss: 0.00001063
Iteration 125/1000 | Loss: 0.00001062
Iteration 126/1000 | Loss: 0.00001062
Iteration 127/1000 | Loss: 0.00001062
Iteration 128/1000 | Loss: 0.00001062
Iteration 129/1000 | Loss: 0.00001062
Iteration 130/1000 | Loss: 0.00001062
Iteration 131/1000 | Loss: 0.00001062
Iteration 132/1000 | Loss: 0.00001062
Iteration 133/1000 | Loss: 0.00001061
Iteration 134/1000 | Loss: 0.00001061
Iteration 135/1000 | Loss: 0.00001061
Iteration 136/1000 | Loss: 0.00001061
Iteration 137/1000 | Loss: 0.00001061
Iteration 138/1000 | Loss: 0.00001061
Iteration 139/1000 | Loss: 0.00001061
Iteration 140/1000 | Loss: 0.00001061
Iteration 141/1000 | Loss: 0.00001061
Iteration 142/1000 | Loss: 0.00001061
Iteration 143/1000 | Loss: 0.00001061
Iteration 144/1000 | Loss: 0.00001061
Iteration 145/1000 | Loss: 0.00001061
Iteration 146/1000 | Loss: 0.00001061
Iteration 147/1000 | Loss: 0.00001061
Iteration 148/1000 | Loss: 0.00001061
Iteration 149/1000 | Loss: 0.00001061
Iteration 150/1000 | Loss: 0.00001061
Iteration 151/1000 | Loss: 0.00001061
Iteration 152/1000 | Loss: 0.00001061
Iteration 153/1000 | Loss: 0.00001061
Iteration 154/1000 | Loss: 0.00001061
Iteration 155/1000 | Loss: 0.00001061
Iteration 156/1000 | Loss: 0.00001061
Iteration 157/1000 | Loss: 0.00001061
Iteration 158/1000 | Loss: 0.00001061
Iteration 159/1000 | Loss: 0.00001061
Iteration 160/1000 | Loss: 0.00001061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.0610837307467591e-05, 1.0610837307467591e-05, 1.0610837307467591e-05, 1.0610837307467591e-05, 1.0610837307467591e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0610837307467591e-05

Optimization complete. Final v2v error: 2.8103575706481934 mm

Highest mean error: 3.5439271926879883 mm for frame 59

Lowest mean error: 2.4251821041107178 mm for frame 21

Saving results

Total time: 116.73992729187012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00372037
Iteration 2/25 | Loss: 0.00117090
Iteration 3/25 | Loss: 0.00105398
Iteration 4/25 | Loss: 0.00104197
Iteration 5/25 | Loss: 0.00103915
Iteration 6/25 | Loss: 0.00103814
Iteration 7/25 | Loss: 0.00103775
Iteration 8/25 | Loss: 0.00103775
Iteration 9/25 | Loss: 0.00103775
Iteration 10/25 | Loss: 0.00103775
Iteration 11/25 | Loss: 0.00103775
Iteration 12/25 | Loss: 0.00103775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001037747017107904, 0.001037747017107904, 0.001037747017107904, 0.001037747017107904, 0.001037747017107904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001037747017107904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33995044
Iteration 2/25 | Loss: 0.00202566
Iteration 3/25 | Loss: 0.00202566
Iteration 4/25 | Loss: 0.00202566
Iteration 5/25 | Loss: 0.00202566
Iteration 6/25 | Loss: 0.00202566
Iteration 7/25 | Loss: 0.00202566
Iteration 8/25 | Loss: 0.00202566
Iteration 9/25 | Loss: 0.00202566
Iteration 10/25 | Loss: 0.00202566
Iteration 11/25 | Loss: 0.00202566
Iteration 12/25 | Loss: 0.00202566
Iteration 13/25 | Loss: 0.00202566
Iteration 14/25 | Loss: 0.00202566
Iteration 15/25 | Loss: 0.00202566
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0020256557036191225, 0.0020256557036191225, 0.0020256557036191225, 0.0020256557036191225, 0.0020256557036191225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020256557036191225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202566
Iteration 2/1000 | Loss: 0.00002884
Iteration 3/1000 | Loss: 0.00001555
Iteration 4/1000 | Loss: 0.00001248
Iteration 5/1000 | Loss: 0.00001129
Iteration 6/1000 | Loss: 0.00001041
Iteration 7/1000 | Loss: 0.00000988
Iteration 8/1000 | Loss: 0.00000946
Iteration 9/1000 | Loss: 0.00000918
Iteration 10/1000 | Loss: 0.00000899
Iteration 11/1000 | Loss: 0.00000878
Iteration 12/1000 | Loss: 0.00000862
Iteration 13/1000 | Loss: 0.00000853
Iteration 14/1000 | Loss: 0.00000845
Iteration 15/1000 | Loss: 0.00000844
Iteration 16/1000 | Loss: 0.00000842
Iteration 17/1000 | Loss: 0.00000842
Iteration 18/1000 | Loss: 0.00000841
Iteration 19/1000 | Loss: 0.00000836
Iteration 20/1000 | Loss: 0.00000831
Iteration 21/1000 | Loss: 0.00000831
Iteration 22/1000 | Loss: 0.00000831
Iteration 23/1000 | Loss: 0.00000831
Iteration 24/1000 | Loss: 0.00000830
Iteration 25/1000 | Loss: 0.00000830
Iteration 26/1000 | Loss: 0.00000829
Iteration 27/1000 | Loss: 0.00000828
Iteration 28/1000 | Loss: 0.00000828
Iteration 29/1000 | Loss: 0.00000827
Iteration 30/1000 | Loss: 0.00000826
Iteration 31/1000 | Loss: 0.00000826
Iteration 32/1000 | Loss: 0.00000826
Iteration 33/1000 | Loss: 0.00000826
Iteration 34/1000 | Loss: 0.00000825
Iteration 35/1000 | Loss: 0.00000825
Iteration 36/1000 | Loss: 0.00000825
Iteration 37/1000 | Loss: 0.00000825
Iteration 38/1000 | Loss: 0.00000825
Iteration 39/1000 | Loss: 0.00000825
Iteration 40/1000 | Loss: 0.00000825
Iteration 41/1000 | Loss: 0.00000825
Iteration 42/1000 | Loss: 0.00000825
Iteration 43/1000 | Loss: 0.00000825
Iteration 44/1000 | Loss: 0.00000825
Iteration 45/1000 | Loss: 0.00000825
Iteration 46/1000 | Loss: 0.00000824
Iteration 47/1000 | Loss: 0.00000824
Iteration 48/1000 | Loss: 0.00000824
Iteration 49/1000 | Loss: 0.00000823
Iteration 50/1000 | Loss: 0.00000822
Iteration 51/1000 | Loss: 0.00000821
Iteration 52/1000 | Loss: 0.00000821
Iteration 53/1000 | Loss: 0.00000821
Iteration 54/1000 | Loss: 0.00000821
Iteration 55/1000 | Loss: 0.00000821
Iteration 56/1000 | Loss: 0.00000820
Iteration 57/1000 | Loss: 0.00000820
Iteration 58/1000 | Loss: 0.00000820
Iteration 59/1000 | Loss: 0.00000820
Iteration 60/1000 | Loss: 0.00000820
Iteration 61/1000 | Loss: 0.00000820
Iteration 62/1000 | Loss: 0.00000819
Iteration 63/1000 | Loss: 0.00000819
Iteration 64/1000 | Loss: 0.00000818
Iteration 65/1000 | Loss: 0.00000818
Iteration 66/1000 | Loss: 0.00000818
Iteration 67/1000 | Loss: 0.00000818
Iteration 68/1000 | Loss: 0.00000818
Iteration 69/1000 | Loss: 0.00000818
Iteration 70/1000 | Loss: 0.00000817
Iteration 71/1000 | Loss: 0.00000817
Iteration 72/1000 | Loss: 0.00000817
Iteration 73/1000 | Loss: 0.00000817
Iteration 74/1000 | Loss: 0.00000817
Iteration 75/1000 | Loss: 0.00000817
Iteration 76/1000 | Loss: 0.00000817
Iteration 77/1000 | Loss: 0.00000817
Iteration 78/1000 | Loss: 0.00000817
Iteration 79/1000 | Loss: 0.00000817
Iteration 80/1000 | Loss: 0.00000817
Iteration 81/1000 | Loss: 0.00000817
Iteration 82/1000 | Loss: 0.00000817
Iteration 83/1000 | Loss: 0.00000817
Iteration 84/1000 | Loss: 0.00000817
Iteration 85/1000 | Loss: 0.00000817
Iteration 86/1000 | Loss: 0.00000817
Iteration 87/1000 | Loss: 0.00000817
Iteration 88/1000 | Loss: 0.00000817
Iteration 89/1000 | Loss: 0.00000817
Iteration 90/1000 | Loss: 0.00000817
Iteration 91/1000 | Loss: 0.00000817
Iteration 92/1000 | Loss: 0.00000817
Iteration 93/1000 | Loss: 0.00000817
Iteration 94/1000 | Loss: 0.00000817
Iteration 95/1000 | Loss: 0.00000817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [8.171930858225096e-06, 8.171930858225096e-06, 8.171930858225096e-06, 8.171930858225096e-06, 8.171930858225096e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.171930858225096e-06

Optimization complete. Final v2v error: 2.3978826999664307 mm

Highest mean error: 3.464493751525879 mm for frame 59

Lowest mean error: 2.0046870708465576 mm for frame 31

Saving results

Total time: 35.79064393043518
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540667
Iteration 2/25 | Loss: 0.00139865
Iteration 3/25 | Loss: 0.00116570
Iteration 4/25 | Loss: 0.00112939
Iteration 5/25 | Loss: 0.00112472
Iteration 6/25 | Loss: 0.00112689
Iteration 7/25 | Loss: 0.00112448
Iteration 8/25 | Loss: 0.00110140
Iteration 9/25 | Loss: 0.00108552
Iteration 10/25 | Loss: 0.00108514
Iteration 11/25 | Loss: 0.00108051
Iteration 12/25 | Loss: 0.00107743
Iteration 13/25 | Loss: 0.00107311
Iteration 14/25 | Loss: 0.00107155
Iteration 15/25 | Loss: 0.00107106
Iteration 16/25 | Loss: 0.00107092
Iteration 17/25 | Loss: 0.00107086
Iteration 18/25 | Loss: 0.00107086
Iteration 19/25 | Loss: 0.00107086
Iteration 20/25 | Loss: 0.00107086
Iteration 21/25 | Loss: 0.00107086
Iteration 22/25 | Loss: 0.00107085
Iteration 23/25 | Loss: 0.00107085
Iteration 24/25 | Loss: 0.00107085
Iteration 25/25 | Loss: 0.00107085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00924385
Iteration 2/25 | Loss: 0.00126371
Iteration 3/25 | Loss: 0.00126369
Iteration 4/25 | Loss: 0.00126369
Iteration 5/25 | Loss: 0.00126369
Iteration 6/25 | Loss: 0.00126369
Iteration 7/25 | Loss: 0.00126369
Iteration 8/25 | Loss: 0.00126368
Iteration 9/25 | Loss: 0.00126368
Iteration 10/25 | Loss: 0.00126368
Iteration 11/25 | Loss: 0.00126368
Iteration 12/25 | Loss: 0.00126368
Iteration 13/25 | Loss: 0.00126368
Iteration 14/25 | Loss: 0.00126368
Iteration 15/25 | Loss: 0.00126368
Iteration 16/25 | Loss: 0.00126368
Iteration 17/25 | Loss: 0.00126368
Iteration 18/25 | Loss: 0.00126368
Iteration 19/25 | Loss: 0.00126368
Iteration 20/25 | Loss: 0.00126368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012636840110644698, 0.0012636840110644698, 0.0012636840110644698, 0.0012636840110644698, 0.0012636840110644698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012636840110644698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126368
Iteration 2/1000 | Loss: 0.00002851
Iteration 3/1000 | Loss: 0.00001976
Iteration 4/1000 | Loss: 0.00001785
Iteration 5/1000 | Loss: 0.00001706
Iteration 6/1000 | Loss: 0.00001663
Iteration 7/1000 | Loss: 0.00001616
Iteration 8/1000 | Loss: 0.00001576
Iteration 9/1000 | Loss: 0.00001550
Iteration 10/1000 | Loss: 0.00001529
Iteration 11/1000 | Loss: 0.00001520
Iteration 12/1000 | Loss: 0.00001515
Iteration 13/1000 | Loss: 0.00001513
Iteration 14/1000 | Loss: 0.00001508
Iteration 15/1000 | Loss: 0.00001495
Iteration 16/1000 | Loss: 0.00001492
Iteration 17/1000 | Loss: 0.00001489
Iteration 18/1000 | Loss: 0.00001480
Iteration 19/1000 | Loss: 0.00001478
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001476
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001475
Iteration 25/1000 | Loss: 0.00001474
Iteration 26/1000 | Loss: 0.00001474
Iteration 27/1000 | Loss: 0.00001474
Iteration 28/1000 | Loss: 0.00001472
Iteration 29/1000 | Loss: 0.00001472
Iteration 30/1000 | Loss: 0.00001472
Iteration 31/1000 | Loss: 0.00001472
Iteration 32/1000 | Loss: 0.00001472
Iteration 33/1000 | Loss: 0.00001472
Iteration 34/1000 | Loss: 0.00001472
Iteration 35/1000 | Loss: 0.00001472
Iteration 36/1000 | Loss: 0.00001472
Iteration 37/1000 | Loss: 0.00001472
Iteration 38/1000 | Loss: 0.00001472
Iteration 39/1000 | Loss: 0.00001472
Iteration 40/1000 | Loss: 0.00001471
Iteration 41/1000 | Loss: 0.00001471
Iteration 42/1000 | Loss: 0.00001470
Iteration 43/1000 | Loss: 0.00001470
Iteration 44/1000 | Loss: 0.00001470
Iteration 45/1000 | Loss: 0.00001470
Iteration 46/1000 | Loss: 0.00001470
Iteration 47/1000 | Loss: 0.00001469
Iteration 48/1000 | Loss: 0.00001469
Iteration 49/1000 | Loss: 0.00001469
Iteration 50/1000 | Loss: 0.00001468
Iteration 51/1000 | Loss: 0.00001468
Iteration 52/1000 | Loss: 0.00001468
Iteration 53/1000 | Loss: 0.00001467
Iteration 54/1000 | Loss: 0.00001467
Iteration 55/1000 | Loss: 0.00001466
Iteration 56/1000 | Loss: 0.00001465
Iteration 57/1000 | Loss: 0.00001465
Iteration 58/1000 | Loss: 0.00001465
Iteration 59/1000 | Loss: 0.00001465
Iteration 60/1000 | Loss: 0.00001465
Iteration 61/1000 | Loss: 0.00001464
Iteration 62/1000 | Loss: 0.00001464
Iteration 63/1000 | Loss: 0.00001464
Iteration 64/1000 | Loss: 0.00001464
Iteration 65/1000 | Loss: 0.00001464
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001464
Iteration 68/1000 | Loss: 0.00001464
Iteration 69/1000 | Loss: 0.00001464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.4644469047198072e-05, 1.4644469047198072e-05, 1.4644469047198072e-05, 1.4644469047198072e-05, 1.4644469047198072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4644469047198072e-05

Optimization complete. Final v2v error: 3.2947018146514893 mm

Highest mean error: 3.7175369262695312 mm for frame 51

Lowest mean error: 2.8503198623657227 mm for frame 74

Saving results

Total time: 60.89062166213989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043623
Iteration 2/25 | Loss: 0.01043622
Iteration 3/25 | Loss: 0.01043622
Iteration 4/25 | Loss: 0.00228965
Iteration 5/25 | Loss: 0.00187671
Iteration 6/25 | Loss: 0.00110175
Iteration 7/25 | Loss: 0.00108117
Iteration 8/25 | Loss: 0.00106510
Iteration 9/25 | Loss: 0.00106115
Iteration 10/25 | Loss: 0.00106115
Iteration 11/25 | Loss: 0.00105997
Iteration 12/25 | Loss: 0.00106063
Iteration 13/25 | Loss: 0.00105984
Iteration 14/25 | Loss: 0.00105945
Iteration 15/25 | Loss: 0.00105945
Iteration 16/25 | Loss: 0.00105945
Iteration 17/25 | Loss: 0.00105944
Iteration 18/25 | Loss: 0.00105944
Iteration 19/25 | Loss: 0.00105944
Iteration 20/25 | Loss: 0.00105944
Iteration 21/25 | Loss: 0.00105944
Iteration 22/25 | Loss: 0.00105944
Iteration 23/25 | Loss: 0.00105944
Iteration 24/25 | Loss: 0.00105944
Iteration 25/25 | Loss: 0.00105944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16053987
Iteration 2/25 | Loss: 0.00273060
Iteration 3/25 | Loss: 0.00273060
Iteration 4/25 | Loss: 0.00273060
Iteration 5/25 | Loss: 0.00273060
Iteration 6/25 | Loss: 0.00273060
Iteration 7/25 | Loss: 0.00273060
Iteration 8/25 | Loss: 0.00273060
Iteration 9/25 | Loss: 0.00273060
Iteration 10/25 | Loss: 0.00273060
Iteration 11/25 | Loss: 0.00273060
Iteration 12/25 | Loss: 0.00273060
Iteration 13/25 | Loss: 0.00273060
Iteration 14/25 | Loss: 0.00271301
Iteration 15/25 | Loss: 0.00271301
Iteration 16/25 | Loss: 0.00271301
Iteration 17/25 | Loss: 0.00271301
Iteration 18/25 | Loss: 0.00271301
Iteration 19/25 | Loss: 0.00271301
Iteration 20/25 | Loss: 0.00271301
Iteration 21/25 | Loss: 0.00271301
Iteration 22/25 | Loss: 0.00271301
Iteration 23/25 | Loss: 0.00271301
Iteration 24/25 | Loss: 0.00271301
Iteration 25/25 | Loss: 0.00271301

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00271301
Iteration 2/1000 | Loss: 0.00005592
Iteration 3/1000 | Loss: 0.00003232
Iteration 4/1000 | Loss: 0.00004188
Iteration 5/1000 | Loss: 0.00001768
Iteration 6/1000 | Loss: 0.00017167
Iteration 7/1000 | Loss: 0.00042971
Iteration 8/1000 | Loss: 0.00028010
Iteration 9/1000 | Loss: 0.00042395
Iteration 10/1000 | Loss: 0.00023384
Iteration 11/1000 | Loss: 0.00012741
Iteration 12/1000 | Loss: 0.00012184
Iteration 13/1000 | Loss: 0.00022235
Iteration 14/1000 | Loss: 0.00002386
Iteration 15/1000 | Loss: 0.00016599
Iteration 16/1000 | Loss: 0.00006757
Iteration 17/1000 | Loss: 0.00004087
Iteration 18/1000 | Loss: 0.00002772
Iteration 19/1000 | Loss: 0.00001820
Iteration 20/1000 | Loss: 0.00001535
Iteration 21/1000 | Loss: 0.00003367
Iteration 22/1000 | Loss: 0.00002527
Iteration 23/1000 | Loss: 0.00011129
Iteration 24/1000 | Loss: 0.00004708
Iteration 25/1000 | Loss: 0.00005482
Iteration 26/1000 | Loss: 0.00002319
Iteration 27/1000 | Loss: 0.00011810
Iteration 28/1000 | Loss: 0.00001410
Iteration 29/1000 | Loss: 0.00007542
Iteration 30/1000 | Loss: 0.00002163
Iteration 31/1000 | Loss: 0.00001320
Iteration 32/1000 | Loss: 0.00003554
Iteration 33/1000 | Loss: 0.00001201
Iteration 34/1000 | Loss: 0.00001176
Iteration 35/1000 | Loss: 0.00001175
Iteration 36/1000 | Loss: 0.00001173
Iteration 37/1000 | Loss: 0.00016302
Iteration 38/1000 | Loss: 0.00005130
Iteration 39/1000 | Loss: 0.00001617
Iteration 40/1000 | Loss: 0.00003550
Iteration 41/1000 | Loss: 0.00003588
Iteration 42/1000 | Loss: 0.00001345
Iteration 43/1000 | Loss: 0.00001130
Iteration 44/1000 | Loss: 0.00001130
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001130
Iteration 47/1000 | Loss: 0.00001130
Iteration 48/1000 | Loss: 0.00001130
Iteration 49/1000 | Loss: 0.00001130
Iteration 50/1000 | Loss: 0.00001130
Iteration 51/1000 | Loss: 0.00001123
Iteration 52/1000 | Loss: 0.00001115
Iteration 53/1000 | Loss: 0.00001600
Iteration 54/1000 | Loss: 0.00005599
Iteration 55/1000 | Loss: 0.00002122
Iteration 56/1000 | Loss: 0.00001252
Iteration 57/1000 | Loss: 0.00001099
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001099
Iteration 60/1000 | Loss: 0.00001099
Iteration 61/1000 | Loss: 0.00001099
Iteration 62/1000 | Loss: 0.00001099
Iteration 63/1000 | Loss: 0.00001098
Iteration 64/1000 | Loss: 0.00001098
Iteration 65/1000 | Loss: 0.00001098
Iteration 66/1000 | Loss: 0.00001097
Iteration 67/1000 | Loss: 0.00001097
Iteration 68/1000 | Loss: 0.00001096
Iteration 69/1000 | Loss: 0.00001096
Iteration 70/1000 | Loss: 0.00001095
Iteration 71/1000 | Loss: 0.00001095
Iteration 72/1000 | Loss: 0.00001093
Iteration 73/1000 | Loss: 0.00001093
Iteration 74/1000 | Loss: 0.00001329
Iteration 75/1000 | Loss: 0.00001092
Iteration 76/1000 | Loss: 0.00001091
Iteration 77/1000 | Loss: 0.00001091
Iteration 78/1000 | Loss: 0.00001091
Iteration 79/1000 | Loss: 0.00001091
Iteration 80/1000 | Loss: 0.00001090
Iteration 81/1000 | Loss: 0.00001089
Iteration 82/1000 | Loss: 0.00001089
Iteration 83/1000 | Loss: 0.00001089
Iteration 84/1000 | Loss: 0.00001088
Iteration 85/1000 | Loss: 0.00001088
Iteration 86/1000 | Loss: 0.00001088
Iteration 87/1000 | Loss: 0.00001088
Iteration 88/1000 | Loss: 0.00001088
Iteration 89/1000 | Loss: 0.00001088
Iteration 90/1000 | Loss: 0.00001088
Iteration 91/1000 | Loss: 0.00001088
Iteration 92/1000 | Loss: 0.00001088
Iteration 93/1000 | Loss: 0.00001088
Iteration 94/1000 | Loss: 0.00001088
Iteration 95/1000 | Loss: 0.00001088
Iteration 96/1000 | Loss: 0.00001088
Iteration 97/1000 | Loss: 0.00001088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.087578039005166e-05, 1.087578039005166e-05, 1.087578039005166e-05, 1.087578039005166e-05, 1.087578039005166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.087578039005166e-05

Optimization complete. Final v2v error: 2.8284404277801514 mm

Highest mean error: 4.004528522491455 mm for frame 174

Lowest mean error: 2.38263201713562 mm for frame 178

Saving results

Total time: 92.0594961643219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099511
Iteration 2/25 | Loss: 0.00226631
Iteration 3/25 | Loss: 0.00158100
Iteration 4/25 | Loss: 0.00148156
Iteration 5/25 | Loss: 0.00157894
Iteration 6/25 | Loss: 0.00164479
Iteration 7/25 | Loss: 0.00163461
Iteration 8/25 | Loss: 0.00165447
Iteration 9/25 | Loss: 0.00159288
Iteration 10/25 | Loss: 0.00157059
Iteration 11/25 | Loss: 0.00150787
Iteration 12/25 | Loss: 0.00149132
Iteration 13/25 | Loss: 0.00144005
Iteration 14/25 | Loss: 0.00137119
Iteration 15/25 | Loss: 0.00139680
Iteration 16/25 | Loss: 0.00138595
Iteration 17/25 | Loss: 0.00137423
Iteration 18/25 | Loss: 0.00134753
Iteration 19/25 | Loss: 0.00132276
Iteration 20/25 | Loss: 0.00131483
Iteration 21/25 | Loss: 0.00131026
Iteration 22/25 | Loss: 0.00132861
Iteration 23/25 | Loss: 0.00129161
Iteration 24/25 | Loss: 0.00125987
Iteration 25/25 | Loss: 0.00121544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11010313
Iteration 2/25 | Loss: 0.00299847
Iteration 3/25 | Loss: 0.00263561
Iteration 4/25 | Loss: 0.00263561
Iteration 5/25 | Loss: 0.00263561
Iteration 6/25 | Loss: 0.00263561
Iteration 7/25 | Loss: 0.00263561
Iteration 8/25 | Loss: 0.00263561
Iteration 9/25 | Loss: 0.00263561
Iteration 10/25 | Loss: 0.00263561
Iteration 11/25 | Loss: 0.00263561
Iteration 12/25 | Loss: 0.00263561
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.002635607961565256, 0.002635607961565256, 0.002635607961565256, 0.002635607961565256, 0.002635607961565256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002635607961565256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263561
Iteration 2/1000 | Loss: 0.00098667
Iteration 3/1000 | Loss: 0.00091627
Iteration 4/1000 | Loss: 0.00078072
Iteration 5/1000 | Loss: 0.00077651
Iteration 6/1000 | Loss: 0.00059131
Iteration 7/1000 | Loss: 0.00140032
Iteration 8/1000 | Loss: 0.00073205
Iteration 9/1000 | Loss: 0.00107902
Iteration 10/1000 | Loss: 0.00079798
Iteration 11/1000 | Loss: 0.00089418
Iteration 12/1000 | Loss: 0.00121974
Iteration 13/1000 | Loss: 0.00107318
Iteration 14/1000 | Loss: 0.00115730
Iteration 15/1000 | Loss: 0.00067438
Iteration 16/1000 | Loss: 0.00059199
Iteration 17/1000 | Loss: 0.00044015
Iteration 18/1000 | Loss: 0.00092993
Iteration 19/1000 | Loss: 0.00060139
Iteration 20/1000 | Loss: 0.00062198
Iteration 21/1000 | Loss: 0.00055462
Iteration 22/1000 | Loss: 0.00048443
Iteration 23/1000 | Loss: 0.00049012
Iteration 24/1000 | Loss: 0.00070553
Iteration 25/1000 | Loss: 0.00085448
Iteration 26/1000 | Loss: 0.00072096
Iteration 27/1000 | Loss: 0.00116729
Iteration 28/1000 | Loss: 0.00031024
Iteration 29/1000 | Loss: 0.00022348
Iteration 30/1000 | Loss: 0.00023584
Iteration 31/1000 | Loss: 0.00025815
Iteration 32/1000 | Loss: 0.00019842
Iteration 33/1000 | Loss: 0.00022288
Iteration 34/1000 | Loss: 0.00023470
Iteration 35/1000 | Loss: 0.00027505
Iteration 36/1000 | Loss: 0.00030600
Iteration 37/1000 | Loss: 0.00027845
Iteration 38/1000 | Loss: 0.00018645
Iteration 39/1000 | Loss: 0.00021824
Iteration 40/1000 | Loss: 0.00011985
Iteration 41/1000 | Loss: 0.00012831
Iteration 42/1000 | Loss: 0.00010315
Iteration 43/1000 | Loss: 0.00011372
Iteration 44/1000 | Loss: 0.00057820
Iteration 45/1000 | Loss: 0.00173259
Iteration 46/1000 | Loss: 0.00116577
Iteration 47/1000 | Loss: 0.00127269
Iteration 48/1000 | Loss: 0.00116322
Iteration 49/1000 | Loss: 0.00050687
Iteration 50/1000 | Loss: 0.00052358
Iteration 51/1000 | Loss: 0.00029481
Iteration 52/1000 | Loss: 0.00024412
Iteration 53/1000 | Loss: 0.00036663
Iteration 54/1000 | Loss: 0.00030335
Iteration 55/1000 | Loss: 0.00056972
Iteration 56/1000 | Loss: 0.00014299
Iteration 57/1000 | Loss: 0.00037115
Iteration 58/1000 | Loss: 0.00020476
Iteration 59/1000 | Loss: 0.00036707
Iteration 60/1000 | Loss: 0.00042870
Iteration 61/1000 | Loss: 0.00022737
Iteration 62/1000 | Loss: 0.00037433
Iteration 63/1000 | Loss: 0.00035826
Iteration 64/1000 | Loss: 0.00009843
Iteration 65/1000 | Loss: 0.00042175
Iteration 66/1000 | Loss: 0.00013926
Iteration 67/1000 | Loss: 0.00016981
Iteration 68/1000 | Loss: 0.00022587
Iteration 69/1000 | Loss: 0.00021231
Iteration 70/1000 | Loss: 0.00039479
Iteration 71/1000 | Loss: 0.00025520
Iteration 72/1000 | Loss: 0.00020812
Iteration 73/1000 | Loss: 0.00038157
Iteration 74/1000 | Loss: 0.00022327
Iteration 75/1000 | Loss: 0.00024083
Iteration 76/1000 | Loss: 0.00019563
Iteration 77/1000 | Loss: 0.00015345
Iteration 78/1000 | Loss: 0.00038479
Iteration 79/1000 | Loss: 0.00047398
Iteration 80/1000 | Loss: 0.00017504
Iteration 81/1000 | Loss: 0.00028422
Iteration 82/1000 | Loss: 0.00023521
Iteration 83/1000 | Loss: 0.00027346
Iteration 84/1000 | Loss: 0.00031659
Iteration 85/1000 | Loss: 0.00062689
Iteration 86/1000 | Loss: 0.00027900
Iteration 87/1000 | Loss: 0.00062432
Iteration 88/1000 | Loss: 0.00110679
Iteration 89/1000 | Loss: 0.00077014
Iteration 90/1000 | Loss: 0.00104022
Iteration 91/1000 | Loss: 0.00124189
Iteration 92/1000 | Loss: 0.00063621
Iteration 93/1000 | Loss: 0.00041598
Iteration 94/1000 | Loss: 0.00010963
Iteration 95/1000 | Loss: 0.00022960
Iteration 96/1000 | Loss: 0.00057349
Iteration 97/1000 | Loss: 0.00026402
Iteration 98/1000 | Loss: 0.00023693
Iteration 99/1000 | Loss: 0.00027316
Iteration 100/1000 | Loss: 0.00050960
Iteration 101/1000 | Loss: 0.00025377
Iteration 102/1000 | Loss: 0.00025351
Iteration 103/1000 | Loss: 0.00028270
Iteration 104/1000 | Loss: 0.00027916
Iteration 105/1000 | Loss: 0.00007815
Iteration 106/1000 | Loss: 0.00008141
Iteration 107/1000 | Loss: 0.00005520
Iteration 108/1000 | Loss: 0.00004473
Iteration 109/1000 | Loss: 0.00007104
Iteration 110/1000 | Loss: 0.00033078
Iteration 111/1000 | Loss: 0.00036886
Iteration 112/1000 | Loss: 0.00036942
Iteration 113/1000 | Loss: 0.00053607
Iteration 114/1000 | Loss: 0.00023005
Iteration 115/1000 | Loss: 0.00044843
Iteration 116/1000 | Loss: 0.00027954
Iteration 117/1000 | Loss: 0.00032802
Iteration 118/1000 | Loss: 0.00021785
Iteration 119/1000 | Loss: 0.00006246
Iteration 120/1000 | Loss: 0.00005817
Iteration 121/1000 | Loss: 0.00003562
Iteration 122/1000 | Loss: 0.00027266
Iteration 123/1000 | Loss: 0.00041229
Iteration 124/1000 | Loss: 0.00006996
Iteration 125/1000 | Loss: 0.00047719
Iteration 126/1000 | Loss: 0.00017042
Iteration 127/1000 | Loss: 0.00048000
Iteration 128/1000 | Loss: 0.00042669
Iteration 129/1000 | Loss: 0.00027063
Iteration 130/1000 | Loss: 0.00052636
Iteration 131/1000 | Loss: 0.00054242
Iteration 132/1000 | Loss: 0.00034785
Iteration 133/1000 | Loss: 0.00028216
Iteration 134/1000 | Loss: 0.00008144
Iteration 135/1000 | Loss: 0.00006775
Iteration 136/1000 | Loss: 0.00007214
Iteration 137/1000 | Loss: 0.00007103
Iteration 138/1000 | Loss: 0.00015137
Iteration 139/1000 | Loss: 0.00011549
Iteration 140/1000 | Loss: 0.00018045
Iteration 141/1000 | Loss: 0.00012595
Iteration 142/1000 | Loss: 0.00016740
Iteration 143/1000 | Loss: 0.00011864
Iteration 144/1000 | Loss: 0.00003310
Iteration 145/1000 | Loss: 0.00035651
Iteration 146/1000 | Loss: 0.00033141
Iteration 147/1000 | Loss: 0.00020498
Iteration 148/1000 | Loss: 0.00024980
Iteration 149/1000 | Loss: 0.00009678
Iteration 150/1000 | Loss: 0.00020369
Iteration 151/1000 | Loss: 0.00028740
Iteration 152/1000 | Loss: 0.00025648
Iteration 153/1000 | Loss: 0.00034064
Iteration 154/1000 | Loss: 0.00015691
Iteration 155/1000 | Loss: 0.00013467
Iteration 156/1000 | Loss: 0.00020841
Iteration 157/1000 | Loss: 0.00020135
Iteration 158/1000 | Loss: 0.00010035
Iteration 159/1000 | Loss: 0.00007834
Iteration 160/1000 | Loss: 0.00004902
Iteration 161/1000 | Loss: 0.00004798
Iteration 162/1000 | Loss: 0.00006333
Iteration 163/1000 | Loss: 0.00018602
Iteration 164/1000 | Loss: 0.00012077
Iteration 165/1000 | Loss: 0.00020069
Iteration 166/1000 | Loss: 0.00016146
Iteration 167/1000 | Loss: 0.00022134
Iteration 168/1000 | Loss: 0.00022491
Iteration 169/1000 | Loss: 0.00009381
Iteration 170/1000 | Loss: 0.00007117
Iteration 171/1000 | Loss: 0.00004298
Iteration 172/1000 | Loss: 0.00005795
Iteration 173/1000 | Loss: 0.00004881
Iteration 174/1000 | Loss: 0.00004220
Iteration 175/1000 | Loss: 0.00016881
Iteration 176/1000 | Loss: 0.00004875
Iteration 177/1000 | Loss: 0.00004988
Iteration 178/1000 | Loss: 0.00005181
Iteration 179/1000 | Loss: 0.00006232
Iteration 180/1000 | Loss: 0.00003656
Iteration 181/1000 | Loss: 0.00005711
Iteration 182/1000 | Loss: 0.00005831
Iteration 183/1000 | Loss: 0.00006061
Iteration 184/1000 | Loss: 0.00005452
Iteration 185/1000 | Loss: 0.00006256
Iteration 186/1000 | Loss: 0.00006223
Iteration 187/1000 | Loss: 0.00004629
Iteration 188/1000 | Loss: 0.00006144
Iteration 189/1000 | Loss: 0.00005668
Iteration 190/1000 | Loss: 0.00004216
Iteration 191/1000 | Loss: 0.00007517
Iteration 192/1000 | Loss: 0.00006905
Iteration 193/1000 | Loss: 0.00004113
Iteration 194/1000 | Loss: 0.00002603
Iteration 195/1000 | Loss: 0.00002355
Iteration 196/1000 | Loss: 0.00002238
Iteration 197/1000 | Loss: 0.00002150
Iteration 198/1000 | Loss: 0.00002101
Iteration 199/1000 | Loss: 0.00002062
Iteration 200/1000 | Loss: 0.00002026
Iteration 201/1000 | Loss: 0.00002002
Iteration 202/1000 | Loss: 0.00030559
Iteration 203/1000 | Loss: 0.00030557
Iteration 204/1000 | Loss: 0.00003915
Iteration 205/1000 | Loss: 0.00002735
Iteration 206/1000 | Loss: 0.00021945
Iteration 207/1000 | Loss: 0.00029145
Iteration 208/1000 | Loss: 0.00022004
Iteration 209/1000 | Loss: 0.00028961
Iteration 210/1000 | Loss: 0.00019694
Iteration 211/1000 | Loss: 0.00027898
Iteration 212/1000 | Loss: 0.00022846
Iteration 213/1000 | Loss: 0.00027280
Iteration 214/1000 | Loss: 0.00003978
Iteration 215/1000 | Loss: 0.00030720
Iteration 216/1000 | Loss: 0.00030670
Iteration 217/1000 | Loss: 0.00024570
Iteration 218/1000 | Loss: 0.00023631
Iteration 219/1000 | Loss: 0.00021102
Iteration 220/1000 | Loss: 0.00016613
Iteration 221/1000 | Loss: 0.00020115
Iteration 222/1000 | Loss: 0.00030186
Iteration 223/1000 | Loss: 0.00002847
Iteration 224/1000 | Loss: 0.00002404
Iteration 225/1000 | Loss: 0.00002202
Iteration 226/1000 | Loss: 0.00002052
Iteration 227/1000 | Loss: 0.00001930
Iteration 228/1000 | Loss: 0.00002310
Iteration 229/1000 | Loss: 0.00001844
Iteration 230/1000 | Loss: 0.00001777
Iteration 231/1000 | Loss: 0.00001721
Iteration 232/1000 | Loss: 0.00001692
Iteration 233/1000 | Loss: 0.00001663
Iteration 234/1000 | Loss: 0.00001631
Iteration 235/1000 | Loss: 0.00001623
Iteration 236/1000 | Loss: 0.00001618
Iteration 237/1000 | Loss: 0.00001617
Iteration 238/1000 | Loss: 0.00001616
Iteration 239/1000 | Loss: 0.00001610
Iteration 240/1000 | Loss: 0.00001609
Iteration 241/1000 | Loss: 0.00001607
Iteration 242/1000 | Loss: 0.00001599
Iteration 243/1000 | Loss: 0.00001596
Iteration 244/1000 | Loss: 0.00001593
Iteration 245/1000 | Loss: 0.00001593
Iteration 246/1000 | Loss: 0.00001591
Iteration 247/1000 | Loss: 0.00001591
Iteration 248/1000 | Loss: 0.00001591
Iteration 249/1000 | Loss: 0.00001591
Iteration 250/1000 | Loss: 0.00001591
Iteration 251/1000 | Loss: 0.00001590
Iteration 252/1000 | Loss: 0.00001590
Iteration 253/1000 | Loss: 0.00001590
Iteration 254/1000 | Loss: 0.00001590
Iteration 255/1000 | Loss: 0.00001590
Iteration 256/1000 | Loss: 0.00001590
Iteration 257/1000 | Loss: 0.00001589
Iteration 258/1000 | Loss: 0.00001589
Iteration 259/1000 | Loss: 0.00001585
Iteration 260/1000 | Loss: 0.00001585
Iteration 261/1000 | Loss: 0.00001585
Iteration 262/1000 | Loss: 0.00001582
Iteration 263/1000 | Loss: 0.00001582
Iteration 264/1000 | Loss: 0.00001582
Iteration 265/1000 | Loss: 0.00001582
Iteration 266/1000 | Loss: 0.00001582
Iteration 267/1000 | Loss: 0.00001582
Iteration 268/1000 | Loss: 0.00001582
Iteration 269/1000 | Loss: 0.00001582
Iteration 270/1000 | Loss: 0.00001581
Iteration 271/1000 | Loss: 0.00001581
Iteration 272/1000 | Loss: 0.00001581
Iteration 273/1000 | Loss: 0.00001581
Iteration 274/1000 | Loss: 0.00001581
Iteration 275/1000 | Loss: 0.00001581
Iteration 276/1000 | Loss: 0.00001581
Iteration 277/1000 | Loss: 0.00001580
Iteration 278/1000 | Loss: 0.00001580
Iteration 279/1000 | Loss: 0.00001580
Iteration 280/1000 | Loss: 0.00001580
Iteration 281/1000 | Loss: 0.00001580
Iteration 282/1000 | Loss: 0.00001580
Iteration 283/1000 | Loss: 0.00001580
Iteration 284/1000 | Loss: 0.00001579
Iteration 285/1000 | Loss: 0.00001579
Iteration 286/1000 | Loss: 0.00001579
Iteration 287/1000 | Loss: 0.00001579
Iteration 288/1000 | Loss: 0.00001579
Iteration 289/1000 | Loss: 0.00001579
Iteration 290/1000 | Loss: 0.00001579
Iteration 291/1000 | Loss: 0.00001578
Iteration 292/1000 | Loss: 0.00001578
Iteration 293/1000 | Loss: 0.00001578
Iteration 294/1000 | Loss: 0.00001578
Iteration 295/1000 | Loss: 0.00001578
Iteration 296/1000 | Loss: 0.00001578
Iteration 297/1000 | Loss: 0.00001578
Iteration 298/1000 | Loss: 0.00001577
Iteration 299/1000 | Loss: 0.00001577
Iteration 300/1000 | Loss: 0.00001577
Iteration 301/1000 | Loss: 0.00001577
Iteration 302/1000 | Loss: 0.00001577
Iteration 303/1000 | Loss: 0.00001577
Iteration 304/1000 | Loss: 0.00001576
Iteration 305/1000 | Loss: 0.00001576
Iteration 306/1000 | Loss: 0.00001576
Iteration 307/1000 | Loss: 0.00001576
Iteration 308/1000 | Loss: 0.00001575
Iteration 309/1000 | Loss: 0.00001575
Iteration 310/1000 | Loss: 0.00001575
Iteration 311/1000 | Loss: 0.00001575
Iteration 312/1000 | Loss: 0.00001575
Iteration 313/1000 | Loss: 0.00001574
Iteration 314/1000 | Loss: 0.00001574
Iteration 315/1000 | Loss: 0.00001574
Iteration 316/1000 | Loss: 0.00001574
Iteration 317/1000 | Loss: 0.00001574
Iteration 318/1000 | Loss: 0.00001574
Iteration 319/1000 | Loss: 0.00001574
Iteration 320/1000 | Loss: 0.00001574
Iteration 321/1000 | Loss: 0.00001574
Iteration 322/1000 | Loss: 0.00001574
Iteration 323/1000 | Loss: 0.00001573
Iteration 324/1000 | Loss: 0.00001573
Iteration 325/1000 | Loss: 0.00001573
Iteration 326/1000 | Loss: 0.00001573
Iteration 327/1000 | Loss: 0.00001572
Iteration 328/1000 | Loss: 0.00001572
Iteration 329/1000 | Loss: 0.00001572
Iteration 330/1000 | Loss: 0.00001572
Iteration 331/1000 | Loss: 0.00001572
Iteration 332/1000 | Loss: 0.00001572
Iteration 333/1000 | Loss: 0.00001572
Iteration 334/1000 | Loss: 0.00001571
Iteration 335/1000 | Loss: 0.00001571
Iteration 336/1000 | Loss: 0.00001571
Iteration 337/1000 | Loss: 0.00001571
Iteration 338/1000 | Loss: 0.00001571
Iteration 339/1000 | Loss: 0.00001571
Iteration 340/1000 | Loss: 0.00001571
Iteration 341/1000 | Loss: 0.00001571
Iteration 342/1000 | Loss: 0.00001571
Iteration 343/1000 | Loss: 0.00001571
Iteration 344/1000 | Loss: 0.00001571
Iteration 345/1000 | Loss: 0.00001571
Iteration 346/1000 | Loss: 0.00001570
Iteration 347/1000 | Loss: 0.00001570
Iteration 348/1000 | Loss: 0.00001570
Iteration 349/1000 | Loss: 0.00001570
Iteration 350/1000 | Loss: 0.00001570
Iteration 351/1000 | Loss: 0.00001570
Iteration 352/1000 | Loss: 0.00001570
Iteration 353/1000 | Loss: 0.00001570
Iteration 354/1000 | Loss: 0.00001570
Iteration 355/1000 | Loss: 0.00001570
Iteration 356/1000 | Loss: 0.00001570
Iteration 357/1000 | Loss: 0.00001570
Iteration 358/1000 | Loss: 0.00001570
Iteration 359/1000 | Loss: 0.00001570
Iteration 360/1000 | Loss: 0.00001570
Iteration 361/1000 | Loss: 0.00001570
Iteration 362/1000 | Loss: 0.00001570
Iteration 363/1000 | Loss: 0.00001570
Iteration 364/1000 | Loss: 0.00001570
Iteration 365/1000 | Loss: 0.00001570
Iteration 366/1000 | Loss: 0.00001570
Iteration 367/1000 | Loss: 0.00001570
Iteration 368/1000 | Loss: 0.00001570
Iteration 369/1000 | Loss: 0.00001570
Iteration 370/1000 | Loss: 0.00001570
Iteration 371/1000 | Loss: 0.00001570
Iteration 372/1000 | Loss: 0.00001570
Iteration 373/1000 | Loss: 0.00001570
Iteration 374/1000 | Loss: 0.00001570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 374. Stopping optimization.
Last 5 losses: [1.5700410585850477e-05, 1.5700410585850477e-05, 1.5700410585850477e-05, 1.5700410585850477e-05, 1.5700410585850477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5700410585850477e-05

Optimization complete. Final v2v error: 3.312959909439087 mm

Highest mean error: 6.080009460449219 mm for frame 56

Lowest mean error: 3.0588672161102295 mm for frame 59

Saving results

Total time: 378.15832924842834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00272051
Iteration 2/25 | Loss: 0.00125179
Iteration 3/25 | Loss: 0.00107962
Iteration 4/25 | Loss: 0.00105928
Iteration 5/25 | Loss: 0.00105294
Iteration 6/25 | Loss: 0.00105030
Iteration 7/25 | Loss: 0.00105020
Iteration 8/25 | Loss: 0.00105020
Iteration 9/25 | Loss: 0.00105020
Iteration 10/25 | Loss: 0.00105020
Iteration 11/25 | Loss: 0.00105020
Iteration 12/25 | Loss: 0.00105020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001050201361067593, 0.001050201361067593, 0.001050201361067593, 0.001050201361067593, 0.001050201361067593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001050201361067593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16710544
Iteration 2/25 | Loss: 0.00322454
Iteration 3/25 | Loss: 0.00322454
Iteration 4/25 | Loss: 0.00322454
Iteration 5/25 | Loss: 0.00322453
Iteration 6/25 | Loss: 0.00322453
Iteration 7/25 | Loss: 0.00322453
Iteration 8/25 | Loss: 0.00322453
Iteration 9/25 | Loss: 0.00322453
Iteration 10/25 | Loss: 0.00322453
Iteration 11/25 | Loss: 0.00322453
Iteration 12/25 | Loss: 0.00322453
Iteration 13/25 | Loss: 0.00322453
Iteration 14/25 | Loss: 0.00322453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0032245332840830088, 0.0032245332840830088, 0.0032245332840830088, 0.0032245332840830088, 0.0032245332840830088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032245332840830088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00322453
Iteration 2/1000 | Loss: 0.00003972
Iteration 3/1000 | Loss: 0.00001608
Iteration 4/1000 | Loss: 0.00001220
Iteration 5/1000 | Loss: 0.00001122
Iteration 6/1000 | Loss: 0.00001045
Iteration 7/1000 | Loss: 0.00001001
Iteration 8/1000 | Loss: 0.00000968
Iteration 9/1000 | Loss: 0.00000946
Iteration 10/1000 | Loss: 0.00000921
Iteration 11/1000 | Loss: 0.00000918
Iteration 12/1000 | Loss: 0.00000915
Iteration 13/1000 | Loss: 0.00000914
Iteration 14/1000 | Loss: 0.00000913
Iteration 15/1000 | Loss: 0.00000909
Iteration 16/1000 | Loss: 0.00000908
Iteration 17/1000 | Loss: 0.00000906
Iteration 18/1000 | Loss: 0.00000905
Iteration 19/1000 | Loss: 0.00000904
Iteration 20/1000 | Loss: 0.00000893
Iteration 21/1000 | Loss: 0.00000886
Iteration 22/1000 | Loss: 0.00000882
Iteration 23/1000 | Loss: 0.00000882
Iteration 24/1000 | Loss: 0.00000881
Iteration 25/1000 | Loss: 0.00000881
Iteration 26/1000 | Loss: 0.00000880
Iteration 27/1000 | Loss: 0.00000879
Iteration 28/1000 | Loss: 0.00000879
Iteration 29/1000 | Loss: 0.00000878
Iteration 30/1000 | Loss: 0.00000877
Iteration 31/1000 | Loss: 0.00000876
Iteration 32/1000 | Loss: 0.00000876
Iteration 33/1000 | Loss: 0.00000871
Iteration 34/1000 | Loss: 0.00000870
Iteration 35/1000 | Loss: 0.00000869
Iteration 36/1000 | Loss: 0.00000868
Iteration 37/1000 | Loss: 0.00000867
Iteration 38/1000 | Loss: 0.00000867
Iteration 39/1000 | Loss: 0.00000867
Iteration 40/1000 | Loss: 0.00000867
Iteration 41/1000 | Loss: 0.00000866
Iteration 42/1000 | Loss: 0.00000866
Iteration 43/1000 | Loss: 0.00000866
Iteration 44/1000 | Loss: 0.00000865
Iteration 45/1000 | Loss: 0.00000865
Iteration 46/1000 | Loss: 0.00000864
Iteration 47/1000 | Loss: 0.00000864
Iteration 48/1000 | Loss: 0.00000864
Iteration 49/1000 | Loss: 0.00000863
Iteration 50/1000 | Loss: 0.00000863
Iteration 51/1000 | Loss: 0.00000863
Iteration 52/1000 | Loss: 0.00000862
Iteration 53/1000 | Loss: 0.00000862
Iteration 54/1000 | Loss: 0.00000862
Iteration 55/1000 | Loss: 0.00000861
Iteration 56/1000 | Loss: 0.00000861
Iteration 57/1000 | Loss: 0.00000861
Iteration 58/1000 | Loss: 0.00000860
Iteration 59/1000 | Loss: 0.00000859
Iteration 60/1000 | Loss: 0.00000858
Iteration 61/1000 | Loss: 0.00000858
Iteration 62/1000 | Loss: 0.00000856
Iteration 63/1000 | Loss: 0.00000855
Iteration 64/1000 | Loss: 0.00000855
Iteration 65/1000 | Loss: 0.00000852
Iteration 66/1000 | Loss: 0.00000852
Iteration 67/1000 | Loss: 0.00000851
Iteration 68/1000 | Loss: 0.00000851
Iteration 69/1000 | Loss: 0.00000850
Iteration 70/1000 | Loss: 0.00000850
Iteration 71/1000 | Loss: 0.00000850
Iteration 72/1000 | Loss: 0.00000850
Iteration 73/1000 | Loss: 0.00000849
Iteration 74/1000 | Loss: 0.00000849
Iteration 75/1000 | Loss: 0.00000849
Iteration 76/1000 | Loss: 0.00000849
Iteration 77/1000 | Loss: 0.00000849
Iteration 78/1000 | Loss: 0.00000849
Iteration 79/1000 | Loss: 0.00000849
Iteration 80/1000 | Loss: 0.00000849
Iteration 81/1000 | Loss: 0.00000848
Iteration 82/1000 | Loss: 0.00000848
Iteration 83/1000 | Loss: 0.00000848
Iteration 84/1000 | Loss: 0.00000846
Iteration 85/1000 | Loss: 0.00000846
Iteration 86/1000 | Loss: 0.00000846
Iteration 87/1000 | Loss: 0.00000846
Iteration 88/1000 | Loss: 0.00000846
Iteration 89/1000 | Loss: 0.00000846
Iteration 90/1000 | Loss: 0.00000845
Iteration 91/1000 | Loss: 0.00000845
Iteration 92/1000 | Loss: 0.00000845
Iteration 93/1000 | Loss: 0.00000845
Iteration 94/1000 | Loss: 0.00000845
Iteration 95/1000 | Loss: 0.00000845
Iteration 96/1000 | Loss: 0.00000845
Iteration 97/1000 | Loss: 0.00000845
Iteration 98/1000 | Loss: 0.00000845
Iteration 99/1000 | Loss: 0.00000844
Iteration 100/1000 | Loss: 0.00000843
Iteration 101/1000 | Loss: 0.00000843
Iteration 102/1000 | Loss: 0.00000842
Iteration 103/1000 | Loss: 0.00000842
Iteration 104/1000 | Loss: 0.00000842
Iteration 105/1000 | Loss: 0.00000842
Iteration 106/1000 | Loss: 0.00000842
Iteration 107/1000 | Loss: 0.00000841
Iteration 108/1000 | Loss: 0.00000841
Iteration 109/1000 | Loss: 0.00000841
Iteration 110/1000 | Loss: 0.00000841
Iteration 111/1000 | Loss: 0.00000841
Iteration 112/1000 | Loss: 0.00000841
Iteration 113/1000 | Loss: 0.00000841
Iteration 114/1000 | Loss: 0.00000840
Iteration 115/1000 | Loss: 0.00000840
Iteration 116/1000 | Loss: 0.00000840
Iteration 117/1000 | Loss: 0.00000840
Iteration 118/1000 | Loss: 0.00000839
Iteration 119/1000 | Loss: 0.00000839
Iteration 120/1000 | Loss: 0.00000839
Iteration 121/1000 | Loss: 0.00000839
Iteration 122/1000 | Loss: 0.00000838
Iteration 123/1000 | Loss: 0.00000838
Iteration 124/1000 | Loss: 0.00000838
Iteration 125/1000 | Loss: 0.00000838
Iteration 126/1000 | Loss: 0.00000838
Iteration 127/1000 | Loss: 0.00000838
Iteration 128/1000 | Loss: 0.00000837
Iteration 129/1000 | Loss: 0.00000837
Iteration 130/1000 | Loss: 0.00000837
Iteration 131/1000 | Loss: 0.00000837
Iteration 132/1000 | Loss: 0.00000837
Iteration 133/1000 | Loss: 0.00000836
Iteration 134/1000 | Loss: 0.00000836
Iteration 135/1000 | Loss: 0.00000836
Iteration 136/1000 | Loss: 0.00000836
Iteration 137/1000 | Loss: 0.00000836
Iteration 138/1000 | Loss: 0.00000835
Iteration 139/1000 | Loss: 0.00000835
Iteration 140/1000 | Loss: 0.00000835
Iteration 141/1000 | Loss: 0.00000835
Iteration 142/1000 | Loss: 0.00000835
Iteration 143/1000 | Loss: 0.00000835
Iteration 144/1000 | Loss: 0.00000835
Iteration 145/1000 | Loss: 0.00000835
Iteration 146/1000 | Loss: 0.00000834
Iteration 147/1000 | Loss: 0.00000834
Iteration 148/1000 | Loss: 0.00000834
Iteration 149/1000 | Loss: 0.00000834
Iteration 150/1000 | Loss: 0.00000834
Iteration 151/1000 | Loss: 0.00000834
Iteration 152/1000 | Loss: 0.00000834
Iteration 153/1000 | Loss: 0.00000834
Iteration 154/1000 | Loss: 0.00000834
Iteration 155/1000 | Loss: 0.00000834
Iteration 156/1000 | Loss: 0.00000833
Iteration 157/1000 | Loss: 0.00000833
Iteration 158/1000 | Loss: 0.00000833
Iteration 159/1000 | Loss: 0.00000832
Iteration 160/1000 | Loss: 0.00000832
Iteration 161/1000 | Loss: 0.00000832
Iteration 162/1000 | Loss: 0.00000832
Iteration 163/1000 | Loss: 0.00000832
Iteration 164/1000 | Loss: 0.00000831
Iteration 165/1000 | Loss: 0.00000831
Iteration 166/1000 | Loss: 0.00000831
Iteration 167/1000 | Loss: 0.00000831
Iteration 168/1000 | Loss: 0.00000831
Iteration 169/1000 | Loss: 0.00000831
Iteration 170/1000 | Loss: 0.00000831
Iteration 171/1000 | Loss: 0.00000831
Iteration 172/1000 | Loss: 0.00000831
Iteration 173/1000 | Loss: 0.00000831
Iteration 174/1000 | Loss: 0.00000830
Iteration 175/1000 | Loss: 0.00000830
Iteration 176/1000 | Loss: 0.00000830
Iteration 177/1000 | Loss: 0.00000830
Iteration 178/1000 | Loss: 0.00000829
Iteration 179/1000 | Loss: 0.00000829
Iteration 180/1000 | Loss: 0.00000829
Iteration 181/1000 | Loss: 0.00000829
Iteration 182/1000 | Loss: 0.00000828
Iteration 183/1000 | Loss: 0.00000828
Iteration 184/1000 | Loss: 0.00000828
Iteration 185/1000 | Loss: 0.00000828
Iteration 186/1000 | Loss: 0.00000828
Iteration 187/1000 | Loss: 0.00000828
Iteration 188/1000 | Loss: 0.00000827
Iteration 189/1000 | Loss: 0.00000827
Iteration 190/1000 | Loss: 0.00000827
Iteration 191/1000 | Loss: 0.00000827
Iteration 192/1000 | Loss: 0.00000827
Iteration 193/1000 | Loss: 0.00000827
Iteration 194/1000 | Loss: 0.00000826
Iteration 195/1000 | Loss: 0.00000826
Iteration 196/1000 | Loss: 0.00000826
Iteration 197/1000 | Loss: 0.00000826
Iteration 198/1000 | Loss: 0.00000826
Iteration 199/1000 | Loss: 0.00000826
Iteration 200/1000 | Loss: 0.00000825
Iteration 201/1000 | Loss: 0.00000825
Iteration 202/1000 | Loss: 0.00000825
Iteration 203/1000 | Loss: 0.00000825
Iteration 204/1000 | Loss: 0.00000825
Iteration 205/1000 | Loss: 0.00000825
Iteration 206/1000 | Loss: 0.00000825
Iteration 207/1000 | Loss: 0.00000825
Iteration 208/1000 | Loss: 0.00000825
Iteration 209/1000 | Loss: 0.00000825
Iteration 210/1000 | Loss: 0.00000824
Iteration 211/1000 | Loss: 0.00000824
Iteration 212/1000 | Loss: 0.00000824
Iteration 213/1000 | Loss: 0.00000824
Iteration 214/1000 | Loss: 0.00000824
Iteration 215/1000 | Loss: 0.00000824
Iteration 216/1000 | Loss: 0.00000824
Iteration 217/1000 | Loss: 0.00000824
Iteration 218/1000 | Loss: 0.00000824
Iteration 219/1000 | Loss: 0.00000824
Iteration 220/1000 | Loss: 0.00000824
Iteration 221/1000 | Loss: 0.00000824
Iteration 222/1000 | Loss: 0.00000824
Iteration 223/1000 | Loss: 0.00000824
Iteration 224/1000 | Loss: 0.00000824
Iteration 225/1000 | Loss: 0.00000824
Iteration 226/1000 | Loss: 0.00000824
Iteration 227/1000 | Loss: 0.00000824
Iteration 228/1000 | Loss: 0.00000824
Iteration 229/1000 | Loss: 0.00000824
Iteration 230/1000 | Loss: 0.00000824
Iteration 231/1000 | Loss: 0.00000824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [8.24151993583655e-06, 8.24151993583655e-06, 8.24151993583655e-06, 8.24151993583655e-06, 8.24151993583655e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.24151993583655e-06

Optimization complete. Final v2v error: 2.4803459644317627 mm

Highest mean error: 2.7916481494903564 mm for frame 30

Lowest mean error: 1.970572590827942 mm for frame 0

Saving results

Total time: 49.75166583061218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00340403
Iteration 2/25 | Loss: 0.00123734
Iteration 3/25 | Loss: 0.00108831
Iteration 4/25 | Loss: 0.00107084
Iteration 5/25 | Loss: 0.00106800
Iteration 6/25 | Loss: 0.00106707
Iteration 7/25 | Loss: 0.00106707
Iteration 8/25 | Loss: 0.00106707
Iteration 9/25 | Loss: 0.00106707
Iteration 10/25 | Loss: 0.00106707
Iteration 11/25 | Loss: 0.00106707
Iteration 12/25 | Loss: 0.00106707
Iteration 13/25 | Loss: 0.00106707
Iteration 14/25 | Loss: 0.00106707
Iteration 15/25 | Loss: 0.00106707
Iteration 16/25 | Loss: 0.00106707
Iteration 17/25 | Loss: 0.00106707
Iteration 18/25 | Loss: 0.00106707
Iteration 19/25 | Loss: 0.00106707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010670674964785576, 0.0010670674964785576, 0.0010670674964785576, 0.0010670674964785576, 0.0010670674964785576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010670674964785576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20130169
Iteration 2/25 | Loss: 0.00214820
Iteration 3/25 | Loss: 0.00214819
Iteration 4/25 | Loss: 0.00214819
Iteration 5/25 | Loss: 0.00214819
Iteration 6/25 | Loss: 0.00214819
Iteration 7/25 | Loss: 0.00214819
Iteration 8/25 | Loss: 0.00214819
Iteration 9/25 | Loss: 0.00214819
Iteration 10/25 | Loss: 0.00214819
Iteration 11/25 | Loss: 0.00214819
Iteration 12/25 | Loss: 0.00214819
Iteration 13/25 | Loss: 0.00214819
Iteration 14/25 | Loss: 0.00214819
Iteration 15/25 | Loss: 0.00214819
Iteration 16/25 | Loss: 0.00214819
Iteration 17/25 | Loss: 0.00214819
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002148185856640339, 0.002148185856640339, 0.002148185856640339, 0.002148185856640339, 0.002148185856640339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002148185856640339

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214819
Iteration 2/1000 | Loss: 0.00003616
Iteration 3/1000 | Loss: 0.00002251
Iteration 4/1000 | Loss: 0.00001789
Iteration 5/1000 | Loss: 0.00001607
Iteration 6/1000 | Loss: 0.00001503
Iteration 7/1000 | Loss: 0.00001429
Iteration 8/1000 | Loss: 0.00001367
Iteration 9/1000 | Loss: 0.00001326
Iteration 10/1000 | Loss: 0.00001280
Iteration 11/1000 | Loss: 0.00001239
Iteration 12/1000 | Loss: 0.00001209
Iteration 13/1000 | Loss: 0.00001198
Iteration 14/1000 | Loss: 0.00001185
Iteration 15/1000 | Loss: 0.00001178
Iteration 16/1000 | Loss: 0.00001177
Iteration 17/1000 | Loss: 0.00001174
Iteration 18/1000 | Loss: 0.00001155
Iteration 19/1000 | Loss: 0.00001155
Iteration 20/1000 | Loss: 0.00001152
Iteration 21/1000 | Loss: 0.00001148
Iteration 22/1000 | Loss: 0.00001146
Iteration 23/1000 | Loss: 0.00001142
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001127
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001124
Iteration 28/1000 | Loss: 0.00001123
Iteration 29/1000 | Loss: 0.00001123
Iteration 30/1000 | Loss: 0.00001121
Iteration 31/1000 | Loss: 0.00001117
Iteration 32/1000 | Loss: 0.00001117
Iteration 33/1000 | Loss: 0.00001115
Iteration 34/1000 | Loss: 0.00001115
Iteration 35/1000 | Loss: 0.00001114
Iteration 36/1000 | Loss: 0.00001113
Iteration 37/1000 | Loss: 0.00001113
Iteration 38/1000 | Loss: 0.00001112
Iteration 39/1000 | Loss: 0.00001112
Iteration 40/1000 | Loss: 0.00001112
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001111
Iteration 43/1000 | Loss: 0.00001111
Iteration 44/1000 | Loss: 0.00001110
Iteration 45/1000 | Loss: 0.00001110
Iteration 46/1000 | Loss: 0.00001110
Iteration 47/1000 | Loss: 0.00001110
Iteration 48/1000 | Loss: 0.00001110
Iteration 49/1000 | Loss: 0.00001110
Iteration 50/1000 | Loss: 0.00001110
Iteration 51/1000 | Loss: 0.00001110
Iteration 52/1000 | Loss: 0.00001109
Iteration 53/1000 | Loss: 0.00001109
Iteration 54/1000 | Loss: 0.00001109
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001109
Iteration 57/1000 | Loss: 0.00001109
Iteration 58/1000 | Loss: 0.00001109
Iteration 59/1000 | Loss: 0.00001108
Iteration 60/1000 | Loss: 0.00001108
Iteration 61/1000 | Loss: 0.00001108
Iteration 62/1000 | Loss: 0.00001108
Iteration 63/1000 | Loss: 0.00001107
Iteration 64/1000 | Loss: 0.00001107
Iteration 65/1000 | Loss: 0.00001107
Iteration 66/1000 | Loss: 0.00001106
Iteration 67/1000 | Loss: 0.00001106
Iteration 68/1000 | Loss: 0.00001106
Iteration 69/1000 | Loss: 0.00001106
Iteration 70/1000 | Loss: 0.00001106
Iteration 71/1000 | Loss: 0.00001106
Iteration 72/1000 | Loss: 0.00001106
Iteration 73/1000 | Loss: 0.00001106
Iteration 74/1000 | Loss: 0.00001105
Iteration 75/1000 | Loss: 0.00001105
Iteration 76/1000 | Loss: 0.00001105
Iteration 77/1000 | Loss: 0.00001104
Iteration 78/1000 | Loss: 0.00001104
Iteration 79/1000 | Loss: 0.00001104
Iteration 80/1000 | Loss: 0.00001104
Iteration 81/1000 | Loss: 0.00001103
Iteration 82/1000 | Loss: 0.00001102
Iteration 83/1000 | Loss: 0.00001101
Iteration 84/1000 | Loss: 0.00001101
Iteration 85/1000 | Loss: 0.00001100
Iteration 86/1000 | Loss: 0.00001100
Iteration 87/1000 | Loss: 0.00001099
Iteration 88/1000 | Loss: 0.00001099
Iteration 89/1000 | Loss: 0.00001098
Iteration 90/1000 | Loss: 0.00001098
Iteration 91/1000 | Loss: 0.00001097
Iteration 92/1000 | Loss: 0.00001097
Iteration 93/1000 | Loss: 0.00001097
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001096
Iteration 97/1000 | Loss: 0.00001096
Iteration 98/1000 | Loss: 0.00001095
Iteration 99/1000 | Loss: 0.00001095
Iteration 100/1000 | Loss: 0.00001095
Iteration 101/1000 | Loss: 0.00001094
Iteration 102/1000 | Loss: 0.00001094
Iteration 103/1000 | Loss: 0.00001093
Iteration 104/1000 | Loss: 0.00001093
Iteration 105/1000 | Loss: 0.00001093
Iteration 106/1000 | Loss: 0.00001093
Iteration 107/1000 | Loss: 0.00001093
Iteration 108/1000 | Loss: 0.00001093
Iteration 109/1000 | Loss: 0.00001093
Iteration 110/1000 | Loss: 0.00001093
Iteration 111/1000 | Loss: 0.00001093
Iteration 112/1000 | Loss: 0.00001093
Iteration 113/1000 | Loss: 0.00001093
Iteration 114/1000 | Loss: 0.00001093
Iteration 115/1000 | Loss: 0.00001093
Iteration 116/1000 | Loss: 0.00001093
Iteration 117/1000 | Loss: 0.00001093
Iteration 118/1000 | Loss: 0.00001093
Iteration 119/1000 | Loss: 0.00001093
Iteration 120/1000 | Loss: 0.00001093
Iteration 121/1000 | Loss: 0.00001093
Iteration 122/1000 | Loss: 0.00001093
Iteration 123/1000 | Loss: 0.00001093
Iteration 124/1000 | Loss: 0.00001093
Iteration 125/1000 | Loss: 0.00001093
Iteration 126/1000 | Loss: 0.00001093
Iteration 127/1000 | Loss: 0.00001093
Iteration 128/1000 | Loss: 0.00001093
Iteration 129/1000 | Loss: 0.00001093
Iteration 130/1000 | Loss: 0.00001093
Iteration 131/1000 | Loss: 0.00001093
Iteration 132/1000 | Loss: 0.00001093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.0928201845672447e-05, 1.0928201845672447e-05, 1.0928201845672447e-05, 1.0928201845672447e-05, 1.0928201845672447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0928201845672447e-05

Optimization complete. Final v2v error: 2.87630033493042 mm

Highest mean error: 3.422517776489258 mm for frame 257

Lowest mean error: 2.3560760021209717 mm for frame 37

Saving results

Total time: 48.06876182556152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389859
Iteration 2/25 | Loss: 0.00122275
Iteration 3/25 | Loss: 0.00108847
Iteration 4/25 | Loss: 0.00107075
Iteration 5/25 | Loss: 0.00106755
Iteration 6/25 | Loss: 0.00106644
Iteration 7/25 | Loss: 0.00106644
Iteration 8/25 | Loss: 0.00106644
Iteration 9/25 | Loss: 0.00106644
Iteration 10/25 | Loss: 0.00106644
Iteration 11/25 | Loss: 0.00106644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001066442928276956, 0.001066442928276956, 0.001066442928276956, 0.001066442928276956, 0.001066442928276956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001066442928276956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26411021
Iteration 2/25 | Loss: 0.00250267
Iteration 3/25 | Loss: 0.00250267
Iteration 4/25 | Loss: 0.00250267
Iteration 5/25 | Loss: 0.00250267
Iteration 6/25 | Loss: 0.00250267
Iteration 7/25 | Loss: 0.00250267
Iteration 8/25 | Loss: 0.00250267
Iteration 9/25 | Loss: 0.00250267
Iteration 10/25 | Loss: 0.00250267
Iteration 11/25 | Loss: 0.00250267
Iteration 12/25 | Loss: 0.00250267
Iteration 13/25 | Loss: 0.00250267
Iteration 14/25 | Loss: 0.00250267
Iteration 15/25 | Loss: 0.00250267
Iteration 16/25 | Loss: 0.00250267
Iteration 17/25 | Loss: 0.00250267
Iteration 18/25 | Loss: 0.00250267
Iteration 19/25 | Loss: 0.00250267
Iteration 20/25 | Loss: 0.00250267
Iteration 21/25 | Loss: 0.00250267
Iteration 22/25 | Loss: 0.00250267
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0025026658549904823, 0.0025026658549904823, 0.0025026658549904823, 0.0025026658549904823, 0.0025026658549904823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025026658549904823

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250267
Iteration 2/1000 | Loss: 0.00004501
Iteration 3/1000 | Loss: 0.00002777
Iteration 4/1000 | Loss: 0.00001594
Iteration 5/1000 | Loss: 0.00001373
Iteration 6/1000 | Loss: 0.00001264
Iteration 7/1000 | Loss: 0.00001163
Iteration 8/1000 | Loss: 0.00001130
Iteration 9/1000 | Loss: 0.00001088
Iteration 10/1000 | Loss: 0.00001065
Iteration 11/1000 | Loss: 0.00001046
Iteration 12/1000 | Loss: 0.00001043
Iteration 13/1000 | Loss: 0.00001028
Iteration 14/1000 | Loss: 0.00001016
Iteration 15/1000 | Loss: 0.00001015
Iteration 16/1000 | Loss: 0.00001013
Iteration 17/1000 | Loss: 0.00001012
Iteration 18/1000 | Loss: 0.00001006
Iteration 19/1000 | Loss: 0.00001002
Iteration 20/1000 | Loss: 0.00000998
Iteration 21/1000 | Loss: 0.00000994
Iteration 22/1000 | Loss: 0.00000991
Iteration 23/1000 | Loss: 0.00000986
Iteration 24/1000 | Loss: 0.00000983
Iteration 25/1000 | Loss: 0.00000982
Iteration 26/1000 | Loss: 0.00000982
Iteration 27/1000 | Loss: 0.00000980
Iteration 28/1000 | Loss: 0.00000980
Iteration 29/1000 | Loss: 0.00000980
Iteration 30/1000 | Loss: 0.00000978
Iteration 31/1000 | Loss: 0.00000977
Iteration 32/1000 | Loss: 0.00000973
Iteration 33/1000 | Loss: 0.00000968
Iteration 34/1000 | Loss: 0.00000965
Iteration 35/1000 | Loss: 0.00000963
Iteration 36/1000 | Loss: 0.00000963
Iteration 37/1000 | Loss: 0.00000963
Iteration 38/1000 | Loss: 0.00000962
Iteration 39/1000 | Loss: 0.00000962
Iteration 40/1000 | Loss: 0.00000961
Iteration 41/1000 | Loss: 0.00000961
Iteration 42/1000 | Loss: 0.00000961
Iteration 43/1000 | Loss: 0.00000961
Iteration 44/1000 | Loss: 0.00000961
Iteration 45/1000 | Loss: 0.00000961
Iteration 46/1000 | Loss: 0.00000961
Iteration 47/1000 | Loss: 0.00000961
Iteration 48/1000 | Loss: 0.00000961
Iteration 49/1000 | Loss: 0.00000961
Iteration 50/1000 | Loss: 0.00000960
Iteration 51/1000 | Loss: 0.00000960
Iteration 52/1000 | Loss: 0.00000959
Iteration 53/1000 | Loss: 0.00000959
Iteration 54/1000 | Loss: 0.00000958
Iteration 55/1000 | Loss: 0.00000956
Iteration 56/1000 | Loss: 0.00000956
Iteration 57/1000 | Loss: 0.00000956
Iteration 58/1000 | Loss: 0.00000955
Iteration 59/1000 | Loss: 0.00000955
Iteration 60/1000 | Loss: 0.00000954
Iteration 61/1000 | Loss: 0.00000954
Iteration 62/1000 | Loss: 0.00000954
Iteration 63/1000 | Loss: 0.00000953
Iteration 64/1000 | Loss: 0.00000953
Iteration 65/1000 | Loss: 0.00000953
Iteration 66/1000 | Loss: 0.00000952
Iteration 67/1000 | Loss: 0.00000952
Iteration 68/1000 | Loss: 0.00000952
Iteration 69/1000 | Loss: 0.00000952
Iteration 70/1000 | Loss: 0.00000952
Iteration 71/1000 | Loss: 0.00000952
Iteration 72/1000 | Loss: 0.00000952
Iteration 73/1000 | Loss: 0.00000952
Iteration 74/1000 | Loss: 0.00000952
Iteration 75/1000 | Loss: 0.00000951
Iteration 76/1000 | Loss: 0.00000951
Iteration 77/1000 | Loss: 0.00000951
Iteration 78/1000 | Loss: 0.00000951
Iteration 79/1000 | Loss: 0.00000951
Iteration 80/1000 | Loss: 0.00000950
Iteration 81/1000 | Loss: 0.00000950
Iteration 82/1000 | Loss: 0.00000950
Iteration 83/1000 | Loss: 0.00000950
Iteration 84/1000 | Loss: 0.00000950
Iteration 85/1000 | Loss: 0.00000950
Iteration 86/1000 | Loss: 0.00000950
Iteration 87/1000 | Loss: 0.00000950
Iteration 88/1000 | Loss: 0.00000950
Iteration 89/1000 | Loss: 0.00000950
Iteration 90/1000 | Loss: 0.00000950
Iteration 91/1000 | Loss: 0.00000950
Iteration 92/1000 | Loss: 0.00000950
Iteration 93/1000 | Loss: 0.00000950
Iteration 94/1000 | Loss: 0.00000950
Iteration 95/1000 | Loss: 0.00000950
Iteration 96/1000 | Loss: 0.00000950
Iteration 97/1000 | Loss: 0.00000949
Iteration 98/1000 | Loss: 0.00000949
Iteration 99/1000 | Loss: 0.00000949
Iteration 100/1000 | Loss: 0.00000949
Iteration 101/1000 | Loss: 0.00000949
Iteration 102/1000 | Loss: 0.00000949
Iteration 103/1000 | Loss: 0.00000949
Iteration 104/1000 | Loss: 0.00000949
Iteration 105/1000 | Loss: 0.00000949
Iteration 106/1000 | Loss: 0.00000949
Iteration 107/1000 | Loss: 0.00000949
Iteration 108/1000 | Loss: 0.00000949
Iteration 109/1000 | Loss: 0.00000948
Iteration 110/1000 | Loss: 0.00000948
Iteration 111/1000 | Loss: 0.00000948
Iteration 112/1000 | Loss: 0.00000948
Iteration 113/1000 | Loss: 0.00000948
Iteration 114/1000 | Loss: 0.00000948
Iteration 115/1000 | Loss: 0.00000948
Iteration 116/1000 | Loss: 0.00000948
Iteration 117/1000 | Loss: 0.00000948
Iteration 118/1000 | Loss: 0.00000947
Iteration 119/1000 | Loss: 0.00000947
Iteration 120/1000 | Loss: 0.00000947
Iteration 121/1000 | Loss: 0.00000947
Iteration 122/1000 | Loss: 0.00000947
Iteration 123/1000 | Loss: 0.00000946
Iteration 124/1000 | Loss: 0.00000946
Iteration 125/1000 | Loss: 0.00000946
Iteration 126/1000 | Loss: 0.00000946
Iteration 127/1000 | Loss: 0.00000946
Iteration 128/1000 | Loss: 0.00000946
Iteration 129/1000 | Loss: 0.00000946
Iteration 130/1000 | Loss: 0.00000946
Iteration 131/1000 | Loss: 0.00000946
Iteration 132/1000 | Loss: 0.00000946
Iteration 133/1000 | Loss: 0.00000945
Iteration 134/1000 | Loss: 0.00000945
Iteration 135/1000 | Loss: 0.00000945
Iteration 136/1000 | Loss: 0.00000945
Iteration 137/1000 | Loss: 0.00000945
Iteration 138/1000 | Loss: 0.00000944
Iteration 139/1000 | Loss: 0.00000944
Iteration 140/1000 | Loss: 0.00000944
Iteration 141/1000 | Loss: 0.00000944
Iteration 142/1000 | Loss: 0.00000943
Iteration 143/1000 | Loss: 0.00000943
Iteration 144/1000 | Loss: 0.00000943
Iteration 145/1000 | Loss: 0.00000943
Iteration 146/1000 | Loss: 0.00000943
Iteration 147/1000 | Loss: 0.00000943
Iteration 148/1000 | Loss: 0.00000943
Iteration 149/1000 | Loss: 0.00000943
Iteration 150/1000 | Loss: 0.00000942
Iteration 151/1000 | Loss: 0.00000942
Iteration 152/1000 | Loss: 0.00000942
Iteration 153/1000 | Loss: 0.00000942
Iteration 154/1000 | Loss: 0.00000942
Iteration 155/1000 | Loss: 0.00000942
Iteration 156/1000 | Loss: 0.00000941
Iteration 157/1000 | Loss: 0.00000941
Iteration 158/1000 | Loss: 0.00000941
Iteration 159/1000 | Loss: 0.00000941
Iteration 160/1000 | Loss: 0.00000941
Iteration 161/1000 | Loss: 0.00000940
Iteration 162/1000 | Loss: 0.00000940
Iteration 163/1000 | Loss: 0.00000940
Iteration 164/1000 | Loss: 0.00000940
Iteration 165/1000 | Loss: 0.00000940
Iteration 166/1000 | Loss: 0.00000940
Iteration 167/1000 | Loss: 0.00000939
Iteration 168/1000 | Loss: 0.00000939
Iteration 169/1000 | Loss: 0.00000939
Iteration 170/1000 | Loss: 0.00000939
Iteration 171/1000 | Loss: 0.00000939
Iteration 172/1000 | Loss: 0.00000939
Iteration 173/1000 | Loss: 0.00000939
Iteration 174/1000 | Loss: 0.00000938
Iteration 175/1000 | Loss: 0.00000938
Iteration 176/1000 | Loss: 0.00000938
Iteration 177/1000 | Loss: 0.00000938
Iteration 178/1000 | Loss: 0.00000937
Iteration 179/1000 | Loss: 0.00000937
Iteration 180/1000 | Loss: 0.00000937
Iteration 181/1000 | Loss: 0.00000937
Iteration 182/1000 | Loss: 0.00000937
Iteration 183/1000 | Loss: 0.00000936
Iteration 184/1000 | Loss: 0.00000936
Iteration 185/1000 | Loss: 0.00000936
Iteration 186/1000 | Loss: 0.00000936
Iteration 187/1000 | Loss: 0.00000936
Iteration 188/1000 | Loss: 0.00000936
Iteration 189/1000 | Loss: 0.00000936
Iteration 190/1000 | Loss: 0.00000935
Iteration 191/1000 | Loss: 0.00000935
Iteration 192/1000 | Loss: 0.00000935
Iteration 193/1000 | Loss: 0.00000935
Iteration 194/1000 | Loss: 0.00000934
Iteration 195/1000 | Loss: 0.00000934
Iteration 196/1000 | Loss: 0.00000934
Iteration 197/1000 | Loss: 0.00000934
Iteration 198/1000 | Loss: 0.00000934
Iteration 199/1000 | Loss: 0.00000934
Iteration 200/1000 | Loss: 0.00000934
Iteration 201/1000 | Loss: 0.00000934
Iteration 202/1000 | Loss: 0.00000934
Iteration 203/1000 | Loss: 0.00000934
Iteration 204/1000 | Loss: 0.00000934
Iteration 205/1000 | Loss: 0.00000934
Iteration 206/1000 | Loss: 0.00000933
Iteration 207/1000 | Loss: 0.00000933
Iteration 208/1000 | Loss: 0.00000933
Iteration 209/1000 | Loss: 0.00000933
Iteration 210/1000 | Loss: 0.00000933
Iteration 211/1000 | Loss: 0.00000933
Iteration 212/1000 | Loss: 0.00000933
Iteration 213/1000 | Loss: 0.00000933
Iteration 214/1000 | Loss: 0.00000933
Iteration 215/1000 | Loss: 0.00000933
Iteration 216/1000 | Loss: 0.00000933
Iteration 217/1000 | Loss: 0.00000933
Iteration 218/1000 | Loss: 0.00000933
Iteration 219/1000 | Loss: 0.00000932
Iteration 220/1000 | Loss: 0.00000932
Iteration 221/1000 | Loss: 0.00000932
Iteration 222/1000 | Loss: 0.00000932
Iteration 223/1000 | Loss: 0.00000932
Iteration 224/1000 | Loss: 0.00000932
Iteration 225/1000 | Loss: 0.00000932
Iteration 226/1000 | Loss: 0.00000932
Iteration 227/1000 | Loss: 0.00000932
Iteration 228/1000 | Loss: 0.00000932
Iteration 229/1000 | Loss: 0.00000932
Iteration 230/1000 | Loss: 0.00000932
Iteration 231/1000 | Loss: 0.00000932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [9.320218850916717e-06, 9.320218850916717e-06, 9.320218850916717e-06, 9.320218850916717e-06, 9.320218850916717e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.320218850916717e-06

Optimization complete. Final v2v error: 2.625894069671631 mm

Highest mean error: 3.3758187294006348 mm for frame 106

Lowest mean error: 2.0523993968963623 mm for frame 156

Saving results

Total time: 46.57772088050842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002622
Iteration 2/25 | Loss: 0.00364399
Iteration 3/25 | Loss: 0.00234412
Iteration 4/25 | Loss: 0.00201413
Iteration 5/25 | Loss: 0.00175302
Iteration 6/25 | Loss: 0.00166295
Iteration 7/25 | Loss: 0.00158702
Iteration 8/25 | Loss: 0.00153299
Iteration 9/25 | Loss: 0.00149222
Iteration 10/25 | Loss: 0.00146361
Iteration 11/25 | Loss: 0.00145270
Iteration 12/25 | Loss: 0.00142467
Iteration 13/25 | Loss: 0.00139936
Iteration 14/25 | Loss: 0.00139696
Iteration 15/25 | Loss: 0.00138329
Iteration 16/25 | Loss: 0.00136868
Iteration 17/25 | Loss: 0.00136431
Iteration 18/25 | Loss: 0.00135604
Iteration 19/25 | Loss: 0.00135446
Iteration 20/25 | Loss: 0.00135197
Iteration 21/25 | Loss: 0.00134912
Iteration 22/25 | Loss: 0.00134577
Iteration 23/25 | Loss: 0.00134053
Iteration 24/25 | Loss: 0.00133812
Iteration 25/25 | Loss: 0.00133810

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23131919
Iteration 2/25 | Loss: 0.00463646
Iteration 3/25 | Loss: 0.00385836
Iteration 4/25 | Loss: 0.00385836
Iteration 5/25 | Loss: 0.00385836
Iteration 6/25 | Loss: 0.00385836
Iteration 7/25 | Loss: 0.00385836
Iteration 8/25 | Loss: 0.00385836
Iteration 9/25 | Loss: 0.00385836
Iteration 10/25 | Loss: 0.00385836
Iteration 11/25 | Loss: 0.00385836
Iteration 12/25 | Loss: 0.00385836
Iteration 13/25 | Loss: 0.00385836
Iteration 14/25 | Loss: 0.00385836
Iteration 15/25 | Loss: 0.00385836
Iteration 16/25 | Loss: 0.00385836
Iteration 17/25 | Loss: 0.00385836
Iteration 18/25 | Loss: 0.00385836
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0038583583664149046, 0.0038583583664149046, 0.0038583583664149046, 0.0038583583664149046, 0.0038583583664149046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038583583664149046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00385836
Iteration 2/1000 | Loss: 0.00085437
Iteration 3/1000 | Loss: 0.00054614
Iteration 4/1000 | Loss: 0.00049058
Iteration 5/1000 | Loss: 0.00305576
Iteration 6/1000 | Loss: 0.00163788
Iteration 7/1000 | Loss: 0.00096148
Iteration 8/1000 | Loss: 0.00063837
Iteration 9/1000 | Loss: 0.00052283
Iteration 10/1000 | Loss: 0.00024709
Iteration 11/1000 | Loss: 0.00043313
Iteration 12/1000 | Loss: 0.00075724
Iteration 13/1000 | Loss: 0.00026221
Iteration 14/1000 | Loss: 0.00158435
Iteration 15/1000 | Loss: 0.00117694
Iteration 16/1000 | Loss: 0.00224945
Iteration 17/1000 | Loss: 0.00054930
Iteration 18/1000 | Loss: 0.00061435
Iteration 19/1000 | Loss: 0.00028177
Iteration 20/1000 | Loss: 0.00142815
Iteration 21/1000 | Loss: 0.00094902
Iteration 22/1000 | Loss: 0.00067062
Iteration 23/1000 | Loss: 0.00056125
Iteration 24/1000 | Loss: 0.00035699
Iteration 25/1000 | Loss: 0.00028261
Iteration 26/1000 | Loss: 0.00019863
Iteration 27/1000 | Loss: 0.00129276
Iteration 28/1000 | Loss: 0.00046249
Iteration 29/1000 | Loss: 0.00018444
Iteration 30/1000 | Loss: 0.00016458
Iteration 31/1000 | Loss: 0.00034262
Iteration 32/1000 | Loss: 0.00020540
Iteration 33/1000 | Loss: 0.00023801
Iteration 34/1000 | Loss: 0.00025291
Iteration 35/1000 | Loss: 0.00019607
Iteration 36/1000 | Loss: 0.00026815
Iteration 37/1000 | Loss: 0.00043200
Iteration 38/1000 | Loss: 0.00033698
Iteration 39/1000 | Loss: 0.00029874
Iteration 40/1000 | Loss: 0.00024276
Iteration 41/1000 | Loss: 0.00074794
Iteration 42/1000 | Loss: 0.00034291
Iteration 43/1000 | Loss: 0.00083318
Iteration 44/1000 | Loss: 0.00194527
Iteration 45/1000 | Loss: 0.00432661
Iteration 46/1000 | Loss: 0.00032734
Iteration 47/1000 | Loss: 0.00098323
Iteration 48/1000 | Loss: 0.00013679
Iteration 49/1000 | Loss: 0.00051485
Iteration 50/1000 | Loss: 0.00044091
Iteration 51/1000 | Loss: 0.00023911
Iteration 52/1000 | Loss: 0.00032208
Iteration 53/1000 | Loss: 0.00080363
Iteration 54/1000 | Loss: 0.00079013
Iteration 55/1000 | Loss: 0.00027946
Iteration 56/1000 | Loss: 0.00029101
Iteration 57/1000 | Loss: 0.00010275
Iteration 58/1000 | Loss: 0.00013016
Iteration 59/1000 | Loss: 0.00030991
Iteration 60/1000 | Loss: 0.00078755
Iteration 61/1000 | Loss: 0.00024576
Iteration 62/1000 | Loss: 0.00012665
Iteration 63/1000 | Loss: 0.00013121
Iteration 64/1000 | Loss: 0.00032358
Iteration 65/1000 | Loss: 0.00018244
Iteration 66/1000 | Loss: 0.00011838
Iteration 67/1000 | Loss: 0.00017472
Iteration 68/1000 | Loss: 0.00012272
Iteration 69/1000 | Loss: 0.00030399
Iteration 70/1000 | Loss: 0.00044648
Iteration 71/1000 | Loss: 0.00037644
Iteration 72/1000 | Loss: 0.00014126
Iteration 73/1000 | Loss: 0.00045609
Iteration 74/1000 | Loss: 0.00032135
Iteration 75/1000 | Loss: 0.00033407
Iteration 76/1000 | Loss: 0.00207928
Iteration 77/1000 | Loss: 0.00017384
Iteration 78/1000 | Loss: 0.00144501
Iteration 79/1000 | Loss: 0.00014270
Iteration 80/1000 | Loss: 0.00087065
Iteration 81/1000 | Loss: 0.00029763
Iteration 82/1000 | Loss: 0.00009791
Iteration 83/1000 | Loss: 0.00005854
Iteration 84/1000 | Loss: 0.00020030
Iteration 85/1000 | Loss: 0.00006219
Iteration 86/1000 | Loss: 0.00019346
Iteration 87/1000 | Loss: 0.00005379
Iteration 88/1000 | Loss: 0.00007360
Iteration 89/1000 | Loss: 0.00008188
Iteration 90/1000 | Loss: 0.00008861
Iteration 91/1000 | Loss: 0.00036424
Iteration 92/1000 | Loss: 0.00008909
Iteration 93/1000 | Loss: 0.00007616
Iteration 94/1000 | Loss: 0.00021269
Iteration 95/1000 | Loss: 0.00028698
Iteration 96/1000 | Loss: 0.00159201
Iteration 97/1000 | Loss: 0.00033029
Iteration 98/1000 | Loss: 0.00033102
Iteration 99/1000 | Loss: 0.00019538
Iteration 100/1000 | Loss: 0.00015757
Iteration 101/1000 | Loss: 0.00007584
Iteration 102/1000 | Loss: 0.00019415
Iteration 103/1000 | Loss: 0.00006175
Iteration 104/1000 | Loss: 0.00018403
Iteration 105/1000 | Loss: 0.00006263
Iteration 106/1000 | Loss: 0.00008831
Iteration 107/1000 | Loss: 0.00005682
Iteration 108/1000 | Loss: 0.00007762
Iteration 109/1000 | Loss: 0.00009135
Iteration 110/1000 | Loss: 0.00004073
Iteration 111/1000 | Loss: 0.00008196
Iteration 112/1000 | Loss: 0.00007465
Iteration 113/1000 | Loss: 0.00005158
Iteration 114/1000 | Loss: 0.00004860
Iteration 115/1000 | Loss: 0.00005716
Iteration 116/1000 | Loss: 0.00006814
Iteration 117/1000 | Loss: 0.00006435
Iteration 118/1000 | Loss: 0.00003839
Iteration 119/1000 | Loss: 0.00005972
Iteration 120/1000 | Loss: 0.00008733
Iteration 121/1000 | Loss: 0.00005572
Iteration 122/1000 | Loss: 0.00012831
Iteration 123/1000 | Loss: 0.00007944
Iteration 124/1000 | Loss: 0.00005361
Iteration 125/1000 | Loss: 0.00005563
Iteration 126/1000 | Loss: 0.00004318
Iteration 127/1000 | Loss: 0.00010531
Iteration 128/1000 | Loss: 0.00007161
Iteration 129/1000 | Loss: 0.00011465
Iteration 130/1000 | Loss: 0.00020249
Iteration 131/1000 | Loss: 0.00004371
Iteration 132/1000 | Loss: 0.00009764
Iteration 133/1000 | Loss: 0.00010211
Iteration 134/1000 | Loss: 0.00006021
Iteration 135/1000 | Loss: 0.00006271
Iteration 136/1000 | Loss: 0.00005617
Iteration 137/1000 | Loss: 0.00003301
Iteration 138/1000 | Loss: 0.00002981
Iteration 139/1000 | Loss: 0.00002634
Iteration 140/1000 | Loss: 0.00002567
Iteration 141/1000 | Loss: 0.00002809
Iteration 142/1000 | Loss: 0.00003862
Iteration 143/1000 | Loss: 0.00002332
Iteration 144/1000 | Loss: 0.00004609
Iteration 145/1000 | Loss: 0.00004360
Iteration 146/1000 | Loss: 0.00011828
Iteration 147/1000 | Loss: 0.00005005
Iteration 148/1000 | Loss: 0.00003546
Iteration 149/1000 | Loss: 0.00006927
Iteration 150/1000 | Loss: 0.00007105
Iteration 151/1000 | Loss: 0.00020604
Iteration 152/1000 | Loss: 0.00012428
Iteration 153/1000 | Loss: 0.00010036
Iteration 154/1000 | Loss: 0.00003660
Iteration 155/1000 | Loss: 0.00005095
Iteration 156/1000 | Loss: 0.00005795
Iteration 157/1000 | Loss: 0.00005610
Iteration 158/1000 | Loss: 0.00003287
Iteration 159/1000 | Loss: 0.00003403
Iteration 160/1000 | Loss: 0.00004126
Iteration 161/1000 | Loss: 0.00005133
Iteration 162/1000 | Loss: 0.00003200
Iteration 163/1000 | Loss: 0.00004289
Iteration 164/1000 | Loss: 0.00003916
Iteration 165/1000 | Loss: 0.00005682
Iteration 166/1000 | Loss: 0.00003503
Iteration 167/1000 | Loss: 0.00004995
Iteration 168/1000 | Loss: 0.00005042
Iteration 169/1000 | Loss: 0.00003905
Iteration 170/1000 | Loss: 0.00003591
Iteration 171/1000 | Loss: 0.00006809
Iteration 172/1000 | Loss: 0.00006507
Iteration 173/1000 | Loss: 0.00009868
Iteration 174/1000 | Loss: 0.00011555
Iteration 175/1000 | Loss: 0.00009928
Iteration 176/1000 | Loss: 0.00004718
Iteration 177/1000 | Loss: 0.00004721
Iteration 178/1000 | Loss: 0.00003398
Iteration 179/1000 | Loss: 0.00004295
Iteration 180/1000 | Loss: 0.00002932
Iteration 181/1000 | Loss: 0.00003011
Iteration 182/1000 | Loss: 0.00004320
Iteration 183/1000 | Loss: 0.00004019
Iteration 184/1000 | Loss: 0.00010085
Iteration 185/1000 | Loss: 0.00003516
Iteration 186/1000 | Loss: 0.00003323
Iteration 187/1000 | Loss: 0.00005469
Iteration 188/1000 | Loss: 0.00003448
Iteration 189/1000 | Loss: 0.00004113
Iteration 190/1000 | Loss: 0.00003364
Iteration 191/1000 | Loss: 0.00006866
Iteration 192/1000 | Loss: 0.00003435
Iteration 193/1000 | Loss: 0.00003862
Iteration 194/1000 | Loss: 0.00006072
Iteration 195/1000 | Loss: 0.00005502
Iteration 196/1000 | Loss: 0.00003219
Iteration 197/1000 | Loss: 0.00002838
Iteration 198/1000 | Loss: 0.00002448
Iteration 199/1000 | Loss: 0.00002120
Iteration 200/1000 | Loss: 0.00003088
Iteration 201/1000 | Loss: 0.00002792
Iteration 202/1000 | Loss: 0.00006720
Iteration 203/1000 | Loss: 0.00003997
Iteration 204/1000 | Loss: 0.00007604
Iteration 205/1000 | Loss: 0.00005637
Iteration 206/1000 | Loss: 0.00007483
Iteration 207/1000 | Loss: 0.00009703
Iteration 208/1000 | Loss: 0.00061432
Iteration 209/1000 | Loss: 0.00141082
Iteration 210/1000 | Loss: 0.00050758
Iteration 211/1000 | Loss: 0.00136096
Iteration 212/1000 | Loss: 0.00052997
Iteration 213/1000 | Loss: 0.00006358
Iteration 214/1000 | Loss: 0.00010247
Iteration 215/1000 | Loss: 0.00011382
Iteration 216/1000 | Loss: 0.00002815
Iteration 217/1000 | Loss: 0.00003272
Iteration 218/1000 | Loss: 0.00013862
Iteration 219/1000 | Loss: 0.00002340
Iteration 220/1000 | Loss: 0.00001955
Iteration 221/1000 | Loss: 0.00004478
Iteration 222/1000 | Loss: 0.00001924
Iteration 223/1000 | Loss: 0.00001916
Iteration 224/1000 | Loss: 0.00001913
Iteration 225/1000 | Loss: 0.00003733
Iteration 226/1000 | Loss: 0.00001891
Iteration 227/1000 | Loss: 0.00001889
Iteration 228/1000 | Loss: 0.00001887
Iteration 229/1000 | Loss: 0.00001886
Iteration 230/1000 | Loss: 0.00001884
Iteration 231/1000 | Loss: 0.00001883
Iteration 232/1000 | Loss: 0.00001882
Iteration 233/1000 | Loss: 0.00001880
Iteration 234/1000 | Loss: 0.00001879
Iteration 235/1000 | Loss: 0.00001878
Iteration 236/1000 | Loss: 0.00003538
Iteration 237/1000 | Loss: 0.00004236
Iteration 238/1000 | Loss: 0.00001909
Iteration 239/1000 | Loss: 0.00001835
Iteration 240/1000 | Loss: 0.00001832
Iteration 241/1000 | Loss: 0.00001831
Iteration 242/1000 | Loss: 0.00001875
Iteration 243/1000 | Loss: 0.00001825
Iteration 244/1000 | Loss: 0.00001825
Iteration 245/1000 | Loss: 0.00001825
Iteration 246/1000 | Loss: 0.00001824
Iteration 247/1000 | Loss: 0.00001824
Iteration 248/1000 | Loss: 0.00001824
Iteration 249/1000 | Loss: 0.00001824
Iteration 250/1000 | Loss: 0.00001823
Iteration 251/1000 | Loss: 0.00001823
Iteration 252/1000 | Loss: 0.00001822
Iteration 253/1000 | Loss: 0.00001822
Iteration 254/1000 | Loss: 0.00001822
Iteration 255/1000 | Loss: 0.00001822
Iteration 256/1000 | Loss: 0.00001822
Iteration 257/1000 | Loss: 0.00003498
Iteration 258/1000 | Loss: 0.00001820
Iteration 259/1000 | Loss: 0.00001820
Iteration 260/1000 | Loss: 0.00001815
Iteration 261/1000 | Loss: 0.00001814
Iteration 262/1000 | Loss: 0.00001814
Iteration 263/1000 | Loss: 0.00001814
Iteration 264/1000 | Loss: 0.00001814
Iteration 265/1000 | Loss: 0.00001814
Iteration 266/1000 | Loss: 0.00001814
Iteration 267/1000 | Loss: 0.00001814
Iteration 268/1000 | Loss: 0.00001814
Iteration 269/1000 | Loss: 0.00001814
Iteration 270/1000 | Loss: 0.00001813
Iteration 271/1000 | Loss: 0.00004045
Iteration 272/1000 | Loss: 0.00010944
Iteration 273/1000 | Loss: 0.00007653
Iteration 274/1000 | Loss: 0.00013565
Iteration 275/1000 | Loss: 0.00010765
Iteration 276/1000 | Loss: 0.00003860
Iteration 277/1000 | Loss: 0.00001871
Iteration 278/1000 | Loss: 0.00003255
Iteration 279/1000 | Loss: 0.00001769
Iteration 280/1000 | Loss: 0.00006663
Iteration 281/1000 | Loss: 0.00012431
Iteration 282/1000 | Loss: 0.00004583
Iteration 283/1000 | Loss: 0.00002802
Iteration 284/1000 | Loss: 0.00009910
Iteration 285/1000 | Loss: 0.00003045
Iteration 286/1000 | Loss: 0.00002797
Iteration 287/1000 | Loss: 0.00001670
Iteration 288/1000 | Loss: 0.00001658
Iteration 289/1000 | Loss: 0.00001657
Iteration 290/1000 | Loss: 0.00002890
Iteration 291/1000 | Loss: 0.00009040
Iteration 292/1000 | Loss: 0.00001953
Iteration 293/1000 | Loss: 0.00001808
Iteration 294/1000 | Loss: 0.00001645
Iteration 295/1000 | Loss: 0.00001645
Iteration 296/1000 | Loss: 0.00001645
Iteration 297/1000 | Loss: 0.00001644
Iteration 298/1000 | Loss: 0.00001644
Iteration 299/1000 | Loss: 0.00001643
Iteration 300/1000 | Loss: 0.00001643
Iteration 301/1000 | Loss: 0.00001643
Iteration 302/1000 | Loss: 0.00001642
Iteration 303/1000 | Loss: 0.00001642
Iteration 304/1000 | Loss: 0.00001641
Iteration 305/1000 | Loss: 0.00001640
Iteration 306/1000 | Loss: 0.00001639
Iteration 307/1000 | Loss: 0.00001638
Iteration 308/1000 | Loss: 0.00001638
Iteration 309/1000 | Loss: 0.00001638
Iteration 310/1000 | Loss: 0.00001638
Iteration 311/1000 | Loss: 0.00001638
Iteration 312/1000 | Loss: 0.00001638
Iteration 313/1000 | Loss: 0.00001637
Iteration 314/1000 | Loss: 0.00001637
Iteration 315/1000 | Loss: 0.00002412
Iteration 316/1000 | Loss: 0.00001637
Iteration 317/1000 | Loss: 0.00001631
Iteration 318/1000 | Loss: 0.00001631
Iteration 319/1000 | Loss: 0.00001631
Iteration 320/1000 | Loss: 0.00001630
Iteration 321/1000 | Loss: 0.00001630
Iteration 322/1000 | Loss: 0.00001630
Iteration 323/1000 | Loss: 0.00001630
Iteration 324/1000 | Loss: 0.00001630
Iteration 325/1000 | Loss: 0.00001630
Iteration 326/1000 | Loss: 0.00001630
Iteration 327/1000 | Loss: 0.00001630
Iteration 328/1000 | Loss: 0.00001630
Iteration 329/1000 | Loss: 0.00001630
Iteration 330/1000 | Loss: 0.00001630
Iteration 331/1000 | Loss: 0.00001629
Iteration 332/1000 | Loss: 0.00001629
Iteration 333/1000 | Loss: 0.00001629
Iteration 334/1000 | Loss: 0.00001628
Iteration 335/1000 | Loss: 0.00001628
Iteration 336/1000 | Loss: 0.00001628
Iteration 337/1000 | Loss: 0.00001628
Iteration 338/1000 | Loss: 0.00001628
Iteration 339/1000 | Loss: 0.00001628
Iteration 340/1000 | Loss: 0.00001628
Iteration 341/1000 | Loss: 0.00001627
Iteration 342/1000 | Loss: 0.00001627
Iteration 343/1000 | Loss: 0.00001627
Iteration 344/1000 | Loss: 0.00001627
Iteration 345/1000 | Loss: 0.00001627
Iteration 346/1000 | Loss: 0.00001627
Iteration 347/1000 | Loss: 0.00001627
Iteration 348/1000 | Loss: 0.00001627
Iteration 349/1000 | Loss: 0.00001627
Iteration 350/1000 | Loss: 0.00001626
Iteration 351/1000 | Loss: 0.00001626
Iteration 352/1000 | Loss: 0.00001626
Iteration 353/1000 | Loss: 0.00001626
Iteration 354/1000 | Loss: 0.00001626
Iteration 355/1000 | Loss: 0.00001626
Iteration 356/1000 | Loss: 0.00001626
Iteration 357/1000 | Loss: 0.00001626
Iteration 358/1000 | Loss: 0.00001626
Iteration 359/1000 | Loss: 0.00001625
Iteration 360/1000 | Loss: 0.00001625
Iteration 361/1000 | Loss: 0.00001625
Iteration 362/1000 | Loss: 0.00001625
Iteration 363/1000 | Loss: 0.00001625
Iteration 364/1000 | Loss: 0.00001625
Iteration 365/1000 | Loss: 0.00001625
Iteration 366/1000 | Loss: 0.00001624
Iteration 367/1000 | Loss: 0.00001624
Iteration 368/1000 | Loss: 0.00001624
Iteration 369/1000 | Loss: 0.00001624
Iteration 370/1000 | Loss: 0.00001624
Iteration 371/1000 | Loss: 0.00001624
Iteration 372/1000 | Loss: 0.00001623
Iteration 373/1000 | Loss: 0.00003785
Iteration 374/1000 | Loss: 0.00002395
Iteration 375/1000 | Loss: 0.00002095
Iteration 376/1000 | Loss: 0.00004543
Iteration 377/1000 | Loss: 0.00001626
Iteration 378/1000 | Loss: 0.00001620
Iteration 379/1000 | Loss: 0.00001620
Iteration 380/1000 | Loss: 0.00001620
Iteration 381/1000 | Loss: 0.00001620
Iteration 382/1000 | Loss: 0.00001619
Iteration 383/1000 | Loss: 0.00003619
Iteration 384/1000 | Loss: 0.00004394
Iteration 385/1000 | Loss: 0.00001630
Iteration 386/1000 | Loss: 0.00005357
Iteration 387/1000 | Loss: 0.00002100
Iteration 388/1000 | Loss: 0.00002467
Iteration 389/1000 | Loss: 0.00001727
Iteration 390/1000 | Loss: 0.00003269
Iteration 391/1000 | Loss: 0.00002181
Iteration 392/1000 | Loss: 0.00002597
Iteration 393/1000 | Loss: 0.00002514
Iteration 394/1000 | Loss: 0.00002304
Iteration 395/1000 | Loss: 0.00001756
Iteration 396/1000 | Loss: 0.00002143
Iteration 397/1000 | Loss: 0.00002016
Iteration 398/1000 | Loss: 0.00002582
Iteration 399/1000 | Loss: 0.00003338
Iteration 400/1000 | Loss: 0.00003005
Iteration 401/1000 | Loss: 0.00003521
Iteration 402/1000 | Loss: 0.00002708
Iteration 403/1000 | Loss: 0.00001641
Iteration 404/1000 | Loss: 0.00002004
Iteration 405/1000 | Loss: 0.00001689
Iteration 406/1000 | Loss: 0.00001842
Iteration 407/1000 | Loss: 0.00001658
Iteration 408/1000 | Loss: 0.00001742
Iteration 409/1000 | Loss: 0.00001882
Iteration 410/1000 | Loss: 0.00001767
Iteration 411/1000 | Loss: 0.00001708
Iteration 412/1000 | Loss: 0.00003513
Iteration 413/1000 | Loss: 0.00002575
Iteration 414/1000 | Loss: 0.00001684
Iteration 415/1000 | Loss: 0.00002119
Iteration 416/1000 | Loss: 0.00002158
Iteration 417/1000 | Loss: 0.00001720
Iteration 418/1000 | Loss: 0.00001848
Iteration 419/1000 | Loss: 0.00002179
Iteration 420/1000 | Loss: 0.00002079
Iteration 421/1000 | Loss: 0.00005035
Iteration 422/1000 | Loss: 0.00001821
Iteration 423/1000 | Loss: 0.00002304
Iteration 424/1000 | Loss: 0.00001858
Iteration 425/1000 | Loss: 0.00001618
Iteration 426/1000 | Loss: 0.00001617
Iteration 427/1000 | Loss: 0.00001617
Iteration 428/1000 | Loss: 0.00001616
Iteration 429/1000 | Loss: 0.00001616
Iteration 430/1000 | Loss: 0.00001616
Iteration 431/1000 | Loss: 0.00001615
Iteration 432/1000 | Loss: 0.00001615
Iteration 433/1000 | Loss: 0.00001615
Iteration 434/1000 | Loss: 0.00001614
Iteration 435/1000 | Loss: 0.00001614
Iteration 436/1000 | Loss: 0.00001614
Iteration 437/1000 | Loss: 0.00001613
Iteration 438/1000 | Loss: 0.00001613
Iteration 439/1000 | Loss: 0.00001612
Iteration 440/1000 | Loss: 0.00001612
Iteration 441/1000 | Loss: 0.00001612
Iteration 442/1000 | Loss: 0.00001612
Iteration 443/1000 | Loss: 0.00001612
Iteration 444/1000 | Loss: 0.00001612
Iteration 445/1000 | Loss: 0.00001612
Iteration 446/1000 | Loss: 0.00001612
Iteration 447/1000 | Loss: 0.00001612
Iteration 448/1000 | Loss: 0.00001612
Iteration 449/1000 | Loss: 0.00001611
Iteration 450/1000 | Loss: 0.00001611
Iteration 451/1000 | Loss: 0.00001610
Iteration 452/1000 | Loss: 0.00001610
Iteration 453/1000 | Loss: 0.00001610
Iteration 454/1000 | Loss: 0.00001609
Iteration 455/1000 | Loss: 0.00001609
Iteration 456/1000 | Loss: 0.00001609
Iteration 457/1000 | Loss: 0.00001609
Iteration 458/1000 | Loss: 0.00001609
Iteration 459/1000 | Loss: 0.00001609
Iteration 460/1000 | Loss: 0.00001609
Iteration 461/1000 | Loss: 0.00001609
Iteration 462/1000 | Loss: 0.00001609
Iteration 463/1000 | Loss: 0.00001608
Iteration 464/1000 | Loss: 0.00001608
Iteration 465/1000 | Loss: 0.00001608
Iteration 466/1000 | Loss: 0.00001608
Iteration 467/1000 | Loss: 0.00001608
Iteration 468/1000 | Loss: 0.00001608
Iteration 469/1000 | Loss: 0.00001607
Iteration 470/1000 | Loss: 0.00001607
Iteration 471/1000 | Loss: 0.00001606
Iteration 472/1000 | Loss: 0.00001606
Iteration 473/1000 | Loss: 0.00001606
Iteration 474/1000 | Loss: 0.00001605
Iteration 475/1000 | Loss: 0.00001605
Iteration 476/1000 | Loss: 0.00001605
Iteration 477/1000 | Loss: 0.00001605
Iteration 478/1000 | Loss: 0.00001605
Iteration 479/1000 | Loss: 0.00001605
Iteration 480/1000 | Loss: 0.00002912
Iteration 481/1000 | Loss: 0.00001685
Iteration 482/1000 | Loss: 0.00002078
Iteration 483/1000 | Loss: 0.00001605
Iteration 484/1000 | Loss: 0.00001604
Iteration 485/1000 | Loss: 0.00001604
Iteration 486/1000 | Loss: 0.00001603
Iteration 487/1000 | Loss: 0.00001603
Iteration 488/1000 | Loss: 0.00001603
Iteration 489/1000 | Loss: 0.00001603
Iteration 490/1000 | Loss: 0.00001602
Iteration 491/1000 | Loss: 0.00001601
Iteration 492/1000 | Loss: 0.00001600
Iteration 493/1000 | Loss: 0.00001600
Iteration 494/1000 | Loss: 0.00001600
Iteration 495/1000 | Loss: 0.00001600
Iteration 496/1000 | Loss: 0.00001600
Iteration 497/1000 | Loss: 0.00001600
Iteration 498/1000 | Loss: 0.00001600
Iteration 499/1000 | Loss: 0.00001599
Iteration 500/1000 | Loss: 0.00001599
Iteration 501/1000 | Loss: 0.00001599
Iteration 502/1000 | Loss: 0.00001599
Iteration 503/1000 | Loss: 0.00001599
Iteration 504/1000 | Loss: 0.00003673
Iteration 505/1000 | Loss: 0.00002195
Iteration 506/1000 | Loss: 0.00001751
Iteration 507/1000 | Loss: 0.00001708
Iteration 508/1000 | Loss: 0.00001595
Iteration 509/1000 | Loss: 0.00001595
Iteration 510/1000 | Loss: 0.00001595
Iteration 511/1000 | Loss: 0.00001595
Iteration 512/1000 | Loss: 0.00001595
Iteration 513/1000 | Loss: 0.00001595
Iteration 514/1000 | Loss: 0.00001595
Iteration 515/1000 | Loss: 0.00001595
Iteration 516/1000 | Loss: 0.00001594
Iteration 517/1000 | Loss: 0.00001594
Iteration 518/1000 | Loss: 0.00001594
Iteration 519/1000 | Loss: 0.00001594
Iteration 520/1000 | Loss: 0.00001594
Iteration 521/1000 | Loss: 0.00001594
Iteration 522/1000 | Loss: 0.00001593
Iteration 523/1000 | Loss: 0.00001593
Iteration 524/1000 | Loss: 0.00001593
Iteration 525/1000 | Loss: 0.00001593
Iteration 526/1000 | Loss: 0.00001592
Iteration 527/1000 | Loss: 0.00001592
Iteration 528/1000 | Loss: 0.00001592
Iteration 529/1000 | Loss: 0.00001592
Iteration 530/1000 | Loss: 0.00001592
Iteration 531/1000 | Loss: 0.00001592
Iteration 532/1000 | Loss: 0.00001591
Iteration 533/1000 | Loss: 0.00001591
Iteration 534/1000 | Loss: 0.00001591
Iteration 535/1000 | Loss: 0.00001591
Iteration 536/1000 | Loss: 0.00001591
Iteration 537/1000 | Loss: 0.00001590
Iteration 538/1000 | Loss: 0.00001590
Iteration 539/1000 | Loss: 0.00001589
Iteration 540/1000 | Loss: 0.00001589
Iteration 541/1000 | Loss: 0.00001589
Iteration 542/1000 | Loss: 0.00001589
Iteration 543/1000 | Loss: 0.00001589
Iteration 544/1000 | Loss: 0.00001589
Iteration 545/1000 | Loss: 0.00001589
Iteration 546/1000 | Loss: 0.00002078
Iteration 547/1000 | Loss: 0.00005038
Iteration 548/1000 | Loss: 0.00009113
Iteration 549/1000 | Loss: 0.00001590
Iteration 550/1000 | Loss: 0.00003303
Iteration 551/1000 | Loss: 0.00002359
Iteration 552/1000 | Loss: 0.00002069
Iteration 553/1000 | Loss: 0.00004707
Iteration 554/1000 | Loss: 0.00001711
Iteration 555/1000 | Loss: 0.00001647
Iteration 556/1000 | Loss: 0.00001810
Iteration 557/1000 | Loss: 0.00001622
Iteration 558/1000 | Loss: 0.00001580
Iteration 559/1000 | Loss: 0.00001579
Iteration 560/1000 | Loss: 0.00001579
Iteration 561/1000 | Loss: 0.00001579
Iteration 562/1000 | Loss: 0.00001578
Iteration 563/1000 | Loss: 0.00001578
Iteration 564/1000 | Loss: 0.00001578
Iteration 565/1000 | Loss: 0.00001577
Iteration 566/1000 | Loss: 0.00001577
Iteration 567/1000 | Loss: 0.00001577
Iteration 568/1000 | Loss: 0.00001577
Iteration 569/1000 | Loss: 0.00001577
Iteration 570/1000 | Loss: 0.00001577
Iteration 571/1000 | Loss: 0.00001577
Iteration 572/1000 | Loss: 0.00001577
Iteration 573/1000 | Loss: 0.00001577
Iteration 574/1000 | Loss: 0.00001577
Iteration 575/1000 | Loss: 0.00001577
Iteration 576/1000 | Loss: 0.00001577
Iteration 577/1000 | Loss: 0.00001577
Iteration 578/1000 | Loss: 0.00001577
Iteration 579/1000 | Loss: 0.00001577
Iteration 580/1000 | Loss: 0.00001577
Iteration 581/1000 | Loss: 0.00001577
Iteration 582/1000 | Loss: 0.00001577
Iteration 583/1000 | Loss: 0.00001577
Iteration 584/1000 | Loss: 0.00001577
Iteration 585/1000 | Loss: 0.00001577
Iteration 586/1000 | Loss: 0.00001577
Iteration 587/1000 | Loss: 0.00001577
Iteration 588/1000 | Loss: 0.00001577
Iteration 589/1000 | Loss: 0.00001577
Iteration 590/1000 | Loss: 0.00001577
Iteration 591/1000 | Loss: 0.00001577
Iteration 592/1000 | Loss: 0.00001577
Iteration 593/1000 | Loss: 0.00001577
Iteration 594/1000 | Loss: 0.00001577
Iteration 595/1000 | Loss: 0.00001577
Iteration 596/1000 | Loss: 0.00001577
Iteration 597/1000 | Loss: 0.00001577
Iteration 598/1000 | Loss: 0.00001577
Iteration 599/1000 | Loss: 0.00001577
Iteration 600/1000 | Loss: 0.00001577
Iteration 601/1000 | Loss: 0.00001577
Iteration 602/1000 | Loss: 0.00001577
Iteration 603/1000 | Loss: 0.00001577
Iteration 604/1000 | Loss: 0.00001577
Iteration 605/1000 | Loss: 0.00001577
Iteration 606/1000 | Loss: 0.00001577
Iteration 607/1000 | Loss: 0.00001577
Iteration 608/1000 | Loss: 0.00001577
Iteration 609/1000 | Loss: 0.00001577
Iteration 610/1000 | Loss: 0.00001577
Iteration 611/1000 | Loss: 0.00001577
Iteration 612/1000 | Loss: 0.00001577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 612. Stopping optimization.
Last 5 losses: [1.5765870557515882e-05, 1.5765870557515882e-05, 1.5765870557515882e-05, 1.5765870557515882e-05, 1.5765870557515882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5765870557515882e-05

Optimization complete. Final v2v error: 2.7371723651885986 mm

Highest mean error: 12.223992347717285 mm for frame 169

Lowest mean error: 2.243513345718384 mm for frame 13

Saving results

Total time: 579.4917175769806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930783
Iteration 2/25 | Loss: 0.00165386
Iteration 3/25 | Loss: 0.00124259
Iteration 4/25 | Loss: 0.00121332
Iteration 5/25 | Loss: 0.00120307
Iteration 6/25 | Loss: 0.00120029
Iteration 7/25 | Loss: 0.00120029
Iteration 8/25 | Loss: 0.00120029
Iteration 9/25 | Loss: 0.00120029
Iteration 10/25 | Loss: 0.00120029
Iteration 11/25 | Loss: 0.00120029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00120029307436198, 0.00120029307436198, 0.00120029307436198, 0.00120029307436198, 0.00120029307436198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00120029307436198

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92398596
Iteration 2/25 | Loss: 0.00196582
Iteration 3/25 | Loss: 0.00196581
Iteration 4/25 | Loss: 0.00196581
Iteration 5/25 | Loss: 0.00196581
Iteration 6/25 | Loss: 0.00196581
Iteration 7/25 | Loss: 0.00196581
Iteration 8/25 | Loss: 0.00196581
Iteration 9/25 | Loss: 0.00196581
Iteration 10/25 | Loss: 0.00196581
Iteration 11/25 | Loss: 0.00196581
Iteration 12/25 | Loss: 0.00196581
Iteration 13/25 | Loss: 0.00196581
Iteration 14/25 | Loss: 0.00196581
Iteration 15/25 | Loss: 0.00196581
Iteration 16/25 | Loss: 0.00196581
Iteration 17/25 | Loss: 0.00196581
Iteration 18/25 | Loss: 0.00196581
Iteration 19/25 | Loss: 0.00196581
Iteration 20/25 | Loss: 0.00196581
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019658103119581938, 0.0019658103119581938, 0.0019658103119581938, 0.0019658103119581938, 0.0019658103119581938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019658103119581938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196581
Iteration 2/1000 | Loss: 0.00006589
Iteration 3/1000 | Loss: 0.00004701
Iteration 4/1000 | Loss: 0.00003891
Iteration 5/1000 | Loss: 0.00003658
Iteration 6/1000 | Loss: 0.00003520
Iteration 7/1000 | Loss: 0.00003415
Iteration 8/1000 | Loss: 0.00003337
Iteration 9/1000 | Loss: 0.00003299
Iteration 10/1000 | Loss: 0.00003258
Iteration 11/1000 | Loss: 0.00003233
Iteration 12/1000 | Loss: 0.00003207
Iteration 13/1000 | Loss: 0.00003183
Iteration 14/1000 | Loss: 0.00003162
Iteration 15/1000 | Loss: 0.00003139
Iteration 16/1000 | Loss: 0.00003120
Iteration 17/1000 | Loss: 0.00003101
Iteration 18/1000 | Loss: 0.00003096
Iteration 19/1000 | Loss: 0.00003083
Iteration 20/1000 | Loss: 0.00003074
Iteration 21/1000 | Loss: 0.00003070
Iteration 22/1000 | Loss: 0.00003069
Iteration 23/1000 | Loss: 0.00003069
Iteration 24/1000 | Loss: 0.00003068
Iteration 25/1000 | Loss: 0.00003067
Iteration 26/1000 | Loss: 0.00003066
Iteration 27/1000 | Loss: 0.00003064
Iteration 28/1000 | Loss: 0.00003062
Iteration 29/1000 | Loss: 0.00003062
Iteration 30/1000 | Loss: 0.00003061
Iteration 31/1000 | Loss: 0.00003059
Iteration 32/1000 | Loss: 0.00003059
Iteration 33/1000 | Loss: 0.00003057
Iteration 34/1000 | Loss: 0.00003056
Iteration 35/1000 | Loss: 0.00003056
Iteration 36/1000 | Loss: 0.00003055
Iteration 37/1000 | Loss: 0.00003055
Iteration 38/1000 | Loss: 0.00003054
Iteration 39/1000 | Loss: 0.00003054
Iteration 40/1000 | Loss: 0.00003051
Iteration 41/1000 | Loss: 0.00003051
Iteration 42/1000 | Loss: 0.00003050
Iteration 43/1000 | Loss: 0.00003049
Iteration 44/1000 | Loss: 0.00003048
Iteration 45/1000 | Loss: 0.00003048
Iteration 46/1000 | Loss: 0.00003047
Iteration 47/1000 | Loss: 0.00003047
Iteration 48/1000 | Loss: 0.00003047
Iteration 49/1000 | Loss: 0.00003045
Iteration 50/1000 | Loss: 0.00003044
Iteration 51/1000 | Loss: 0.00003043
Iteration 52/1000 | Loss: 0.00003043
Iteration 53/1000 | Loss: 0.00003043
Iteration 54/1000 | Loss: 0.00003043
Iteration 55/1000 | Loss: 0.00003043
Iteration 56/1000 | Loss: 0.00003043
Iteration 57/1000 | Loss: 0.00003043
Iteration 58/1000 | Loss: 0.00003043
Iteration 59/1000 | Loss: 0.00003043
Iteration 60/1000 | Loss: 0.00003043
Iteration 61/1000 | Loss: 0.00003043
Iteration 62/1000 | Loss: 0.00003042
Iteration 63/1000 | Loss: 0.00003042
Iteration 64/1000 | Loss: 0.00003042
Iteration 65/1000 | Loss: 0.00003042
Iteration 66/1000 | Loss: 0.00003042
Iteration 67/1000 | Loss: 0.00003041
Iteration 68/1000 | Loss: 0.00003041
Iteration 69/1000 | Loss: 0.00003041
Iteration 70/1000 | Loss: 0.00003041
Iteration 71/1000 | Loss: 0.00003040
Iteration 72/1000 | Loss: 0.00003040
Iteration 73/1000 | Loss: 0.00003040
Iteration 74/1000 | Loss: 0.00003040
Iteration 75/1000 | Loss: 0.00003039
Iteration 76/1000 | Loss: 0.00003039
Iteration 77/1000 | Loss: 0.00003039
Iteration 78/1000 | Loss: 0.00003039
Iteration 79/1000 | Loss: 0.00003039
Iteration 80/1000 | Loss: 0.00003039
Iteration 81/1000 | Loss: 0.00003038
Iteration 82/1000 | Loss: 0.00003038
Iteration 83/1000 | Loss: 0.00003038
Iteration 84/1000 | Loss: 0.00003038
Iteration 85/1000 | Loss: 0.00003038
Iteration 86/1000 | Loss: 0.00003037
Iteration 87/1000 | Loss: 0.00003037
Iteration 88/1000 | Loss: 0.00003037
Iteration 89/1000 | Loss: 0.00003037
Iteration 90/1000 | Loss: 0.00003037
Iteration 91/1000 | Loss: 0.00003037
Iteration 92/1000 | Loss: 0.00003037
Iteration 93/1000 | Loss: 0.00003037
Iteration 94/1000 | Loss: 0.00003037
Iteration 95/1000 | Loss: 0.00003036
Iteration 96/1000 | Loss: 0.00003036
Iteration 97/1000 | Loss: 0.00003036
Iteration 98/1000 | Loss: 0.00003036
Iteration 99/1000 | Loss: 0.00003036
Iteration 100/1000 | Loss: 0.00003036
Iteration 101/1000 | Loss: 0.00003036
Iteration 102/1000 | Loss: 0.00003036
Iteration 103/1000 | Loss: 0.00003035
Iteration 104/1000 | Loss: 0.00003035
Iteration 105/1000 | Loss: 0.00003035
Iteration 106/1000 | Loss: 0.00003035
Iteration 107/1000 | Loss: 0.00003035
Iteration 108/1000 | Loss: 0.00003035
Iteration 109/1000 | Loss: 0.00003034
Iteration 110/1000 | Loss: 0.00003034
Iteration 111/1000 | Loss: 0.00003034
Iteration 112/1000 | Loss: 0.00003034
Iteration 113/1000 | Loss: 0.00003034
Iteration 114/1000 | Loss: 0.00003034
Iteration 115/1000 | Loss: 0.00003034
Iteration 116/1000 | Loss: 0.00003034
Iteration 117/1000 | Loss: 0.00003034
Iteration 118/1000 | Loss: 0.00003034
Iteration 119/1000 | Loss: 0.00003033
Iteration 120/1000 | Loss: 0.00003033
Iteration 121/1000 | Loss: 0.00003033
Iteration 122/1000 | Loss: 0.00003033
Iteration 123/1000 | Loss: 0.00003033
Iteration 124/1000 | Loss: 0.00003033
Iteration 125/1000 | Loss: 0.00003033
Iteration 126/1000 | Loss: 0.00003033
Iteration 127/1000 | Loss: 0.00003033
Iteration 128/1000 | Loss: 0.00003033
Iteration 129/1000 | Loss: 0.00003033
Iteration 130/1000 | Loss: 0.00003033
Iteration 131/1000 | Loss: 0.00003033
Iteration 132/1000 | Loss: 0.00003033
Iteration 133/1000 | Loss: 0.00003033
Iteration 134/1000 | Loss: 0.00003033
Iteration 135/1000 | Loss: 0.00003032
Iteration 136/1000 | Loss: 0.00003032
Iteration 137/1000 | Loss: 0.00003032
Iteration 138/1000 | Loss: 0.00003032
Iteration 139/1000 | Loss: 0.00003032
Iteration 140/1000 | Loss: 0.00003032
Iteration 141/1000 | Loss: 0.00003032
Iteration 142/1000 | Loss: 0.00003032
Iteration 143/1000 | Loss: 0.00003032
Iteration 144/1000 | Loss: 0.00003032
Iteration 145/1000 | Loss: 0.00003032
Iteration 146/1000 | Loss: 0.00003032
Iteration 147/1000 | Loss: 0.00003032
Iteration 148/1000 | Loss: 0.00003032
Iteration 149/1000 | Loss: 0.00003031
Iteration 150/1000 | Loss: 0.00003031
Iteration 151/1000 | Loss: 0.00003031
Iteration 152/1000 | Loss: 0.00003031
Iteration 153/1000 | Loss: 0.00003031
Iteration 154/1000 | Loss: 0.00003031
Iteration 155/1000 | Loss: 0.00003031
Iteration 156/1000 | Loss: 0.00003031
Iteration 157/1000 | Loss: 0.00003031
Iteration 158/1000 | Loss: 0.00003031
Iteration 159/1000 | Loss: 0.00003031
Iteration 160/1000 | Loss: 0.00003031
Iteration 161/1000 | Loss: 0.00003031
Iteration 162/1000 | Loss: 0.00003031
Iteration 163/1000 | Loss: 0.00003031
Iteration 164/1000 | Loss: 0.00003030
Iteration 165/1000 | Loss: 0.00003030
Iteration 166/1000 | Loss: 0.00003030
Iteration 167/1000 | Loss: 0.00003030
Iteration 168/1000 | Loss: 0.00003030
Iteration 169/1000 | Loss: 0.00003030
Iteration 170/1000 | Loss: 0.00003030
Iteration 171/1000 | Loss: 0.00003030
Iteration 172/1000 | Loss: 0.00003030
Iteration 173/1000 | Loss: 0.00003030
Iteration 174/1000 | Loss: 0.00003030
Iteration 175/1000 | Loss: 0.00003030
Iteration 176/1000 | Loss: 0.00003030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [3.030230618605856e-05, 3.030230618605856e-05, 3.030230618605856e-05, 3.030230618605856e-05, 3.030230618605856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.030230618605856e-05

Optimization complete. Final v2v error: 4.458609580993652 mm

Highest mean error: 5.596477508544922 mm for frame 128

Lowest mean error: 3.1680450439453125 mm for frame 0

Saving results

Total time: 57.14191198348999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802313
Iteration 2/25 | Loss: 0.00135987
Iteration 3/25 | Loss: 0.00125650
Iteration 4/25 | Loss: 0.00115672
Iteration 5/25 | Loss: 0.00111046
Iteration 6/25 | Loss: 0.00109199
Iteration 7/25 | Loss: 0.00108928
Iteration 8/25 | Loss: 0.00108893
Iteration 9/25 | Loss: 0.00108880
Iteration 10/25 | Loss: 0.00108868
Iteration 11/25 | Loss: 0.00108866
Iteration 12/25 | Loss: 0.00108863
Iteration 13/25 | Loss: 0.00108863
Iteration 14/25 | Loss: 0.00108863
Iteration 15/25 | Loss: 0.00108863
Iteration 16/25 | Loss: 0.00108862
Iteration 17/25 | Loss: 0.00108862
Iteration 18/25 | Loss: 0.00108862
Iteration 19/25 | Loss: 0.00108862
Iteration 20/25 | Loss: 0.00108862
Iteration 21/25 | Loss: 0.00108862
Iteration 22/25 | Loss: 0.00108862
Iteration 23/25 | Loss: 0.00108862
Iteration 24/25 | Loss: 0.00108862
Iteration 25/25 | Loss: 0.00108861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82860470
Iteration 2/25 | Loss: 0.00209789
Iteration 3/25 | Loss: 0.00209788
Iteration 4/25 | Loss: 0.00209788
Iteration 5/25 | Loss: 0.00209788
Iteration 6/25 | Loss: 0.00209788
Iteration 7/25 | Loss: 0.00209788
Iteration 8/25 | Loss: 0.00209788
Iteration 9/25 | Loss: 0.00209788
Iteration 10/25 | Loss: 0.00209788
Iteration 11/25 | Loss: 0.00209788
Iteration 12/25 | Loss: 0.00209788
Iteration 13/25 | Loss: 0.00209788
Iteration 14/25 | Loss: 0.00209788
Iteration 15/25 | Loss: 0.00209788
Iteration 16/25 | Loss: 0.00209788
Iteration 17/25 | Loss: 0.00209788
Iteration 18/25 | Loss: 0.00209788
Iteration 19/25 | Loss: 0.00209788
Iteration 20/25 | Loss: 0.00209788
Iteration 21/25 | Loss: 0.00209788
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0020978807006031275, 0.0020978807006031275, 0.0020978807006031275, 0.0020978807006031275, 0.0020978807006031275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020978807006031275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209788
Iteration 2/1000 | Loss: 0.00003308
Iteration 3/1000 | Loss: 0.00002139
Iteration 4/1000 | Loss: 0.00001739
Iteration 5/1000 | Loss: 0.00001573
Iteration 6/1000 | Loss: 0.00001493
Iteration 7/1000 | Loss: 0.00001449
Iteration 8/1000 | Loss: 0.00001410
Iteration 9/1000 | Loss: 0.00001402
Iteration 10/1000 | Loss: 0.00001378
Iteration 11/1000 | Loss: 0.00001359
Iteration 12/1000 | Loss: 0.00001358
Iteration 13/1000 | Loss: 0.00001352
Iteration 14/1000 | Loss: 0.00001349
Iteration 15/1000 | Loss: 0.00001349
Iteration 16/1000 | Loss: 0.00001339
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001330
Iteration 19/1000 | Loss: 0.00001329
Iteration 20/1000 | Loss: 0.00001329
Iteration 21/1000 | Loss: 0.00001329
Iteration 22/1000 | Loss: 0.00001329
Iteration 23/1000 | Loss: 0.00001328
Iteration 24/1000 | Loss: 0.00001324
Iteration 25/1000 | Loss: 0.00001323
Iteration 26/1000 | Loss: 0.00001323
Iteration 27/1000 | Loss: 0.00001322
Iteration 28/1000 | Loss: 0.00001322
Iteration 29/1000 | Loss: 0.00001322
Iteration 30/1000 | Loss: 0.00001319
Iteration 31/1000 | Loss: 0.00001318
Iteration 32/1000 | Loss: 0.00001316
Iteration 33/1000 | Loss: 0.00001315
Iteration 34/1000 | Loss: 0.00001315
Iteration 35/1000 | Loss: 0.00001315
Iteration 36/1000 | Loss: 0.00001315
Iteration 37/1000 | Loss: 0.00001314
Iteration 38/1000 | Loss: 0.00001314
Iteration 39/1000 | Loss: 0.00001314
Iteration 40/1000 | Loss: 0.00001313
Iteration 41/1000 | Loss: 0.00001312
Iteration 42/1000 | Loss: 0.00001312
Iteration 43/1000 | Loss: 0.00001312
Iteration 44/1000 | Loss: 0.00001312
Iteration 45/1000 | Loss: 0.00001312
Iteration 46/1000 | Loss: 0.00001311
Iteration 47/1000 | Loss: 0.00001311
Iteration 48/1000 | Loss: 0.00001310
Iteration 49/1000 | Loss: 0.00001310
Iteration 50/1000 | Loss: 0.00001310
Iteration 51/1000 | Loss: 0.00001310
Iteration 52/1000 | Loss: 0.00001309
Iteration 53/1000 | Loss: 0.00001309
Iteration 54/1000 | Loss: 0.00001309
Iteration 55/1000 | Loss: 0.00001308
Iteration 56/1000 | Loss: 0.00001308
Iteration 57/1000 | Loss: 0.00001308
Iteration 58/1000 | Loss: 0.00001307
Iteration 59/1000 | Loss: 0.00001307
Iteration 60/1000 | Loss: 0.00001306
Iteration 61/1000 | Loss: 0.00001306
Iteration 62/1000 | Loss: 0.00001306
Iteration 63/1000 | Loss: 0.00001306
Iteration 64/1000 | Loss: 0.00001306
Iteration 65/1000 | Loss: 0.00001306
Iteration 66/1000 | Loss: 0.00001306
Iteration 67/1000 | Loss: 0.00001306
Iteration 68/1000 | Loss: 0.00001306
Iteration 69/1000 | Loss: 0.00001306
Iteration 70/1000 | Loss: 0.00001306
Iteration 71/1000 | Loss: 0.00001306
Iteration 72/1000 | Loss: 0.00001306
Iteration 73/1000 | Loss: 0.00001306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [1.3060542187304236e-05, 1.3060542187304236e-05, 1.3060542187304236e-05, 1.3060542187304236e-05, 1.3060542187304236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3060542187304236e-05

Optimization complete. Final v2v error: 3.056006908416748 mm

Highest mean error: 3.3981993198394775 mm for frame 28

Lowest mean error: 2.58718204498291 mm for frame 176

Saving results

Total time: 43.94737648963928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033052
Iteration 2/25 | Loss: 0.00190751
Iteration 3/25 | Loss: 0.00142878
Iteration 4/25 | Loss: 0.00128392
Iteration 5/25 | Loss: 0.00123130
Iteration 6/25 | Loss: 0.00115655
Iteration 7/25 | Loss: 0.00113196
Iteration 8/25 | Loss: 0.00112355
Iteration 9/25 | Loss: 0.00109252
Iteration 10/25 | Loss: 0.00109547
Iteration 11/25 | Loss: 0.00110024
Iteration 12/25 | Loss: 0.00108455
Iteration 13/25 | Loss: 0.00107386
Iteration 14/25 | Loss: 0.00106862
Iteration 15/25 | Loss: 0.00106727
Iteration 16/25 | Loss: 0.00106938
Iteration 17/25 | Loss: 0.00106606
Iteration 18/25 | Loss: 0.00106334
Iteration 19/25 | Loss: 0.00106222
Iteration 20/25 | Loss: 0.00106182
Iteration 21/25 | Loss: 0.00106160
Iteration 22/25 | Loss: 0.00106146
Iteration 23/25 | Loss: 0.00106130
Iteration 24/25 | Loss: 0.00106110
Iteration 25/25 | Loss: 0.00106099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32707143
Iteration 2/25 | Loss: 0.00201282
Iteration 3/25 | Loss: 0.00201282
Iteration 4/25 | Loss: 0.00201282
Iteration 5/25 | Loss: 0.00201282
Iteration 6/25 | Loss: 0.00201281
Iteration 7/25 | Loss: 0.00201281
Iteration 8/25 | Loss: 0.00201281
Iteration 9/25 | Loss: 0.00201281
Iteration 10/25 | Loss: 0.00201281
Iteration 11/25 | Loss: 0.00201281
Iteration 12/25 | Loss: 0.00201281
Iteration 13/25 | Loss: 0.00201281
Iteration 14/25 | Loss: 0.00201281
Iteration 15/25 | Loss: 0.00201281
Iteration 16/25 | Loss: 0.00201281
Iteration 17/25 | Loss: 0.00201281
Iteration 18/25 | Loss: 0.00201281
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0020128132309764624, 0.0020128132309764624, 0.0020128132309764624, 0.0020128132309764624, 0.0020128132309764624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020128132309764624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201281
Iteration 2/1000 | Loss: 0.00002558
Iteration 3/1000 | Loss: 0.00001706
Iteration 4/1000 | Loss: 0.00001493
Iteration 5/1000 | Loss: 0.00001397
Iteration 6/1000 | Loss: 0.00020088
Iteration 7/1000 | Loss: 0.00012227
Iteration 8/1000 | Loss: 0.00004604
Iteration 9/1000 | Loss: 0.00001349
Iteration 10/1000 | Loss: 0.00009379
Iteration 11/1000 | Loss: 0.00001734
Iteration 12/1000 | Loss: 0.00001286
Iteration 13/1000 | Loss: 0.00001131
Iteration 14/1000 | Loss: 0.00001073
Iteration 15/1000 | Loss: 0.00013798
Iteration 16/1000 | Loss: 0.00001068
Iteration 17/1000 | Loss: 0.00001008
Iteration 18/1000 | Loss: 0.00001003
Iteration 19/1000 | Loss: 0.00000992
Iteration 20/1000 | Loss: 0.00000975
Iteration 21/1000 | Loss: 0.00000971
Iteration 22/1000 | Loss: 0.00000966
Iteration 23/1000 | Loss: 0.00000965
Iteration 24/1000 | Loss: 0.00000956
Iteration 25/1000 | Loss: 0.00000955
Iteration 26/1000 | Loss: 0.00000948
Iteration 27/1000 | Loss: 0.00000948
Iteration 28/1000 | Loss: 0.00000946
Iteration 29/1000 | Loss: 0.00000945
Iteration 30/1000 | Loss: 0.00000945
Iteration 31/1000 | Loss: 0.00000944
Iteration 32/1000 | Loss: 0.00000944
Iteration 33/1000 | Loss: 0.00000943
Iteration 34/1000 | Loss: 0.00000943
Iteration 35/1000 | Loss: 0.00000942
Iteration 36/1000 | Loss: 0.00000941
Iteration 37/1000 | Loss: 0.00000941
Iteration 38/1000 | Loss: 0.00000941
Iteration 39/1000 | Loss: 0.00000941
Iteration 40/1000 | Loss: 0.00000941
Iteration 41/1000 | Loss: 0.00000941
Iteration 42/1000 | Loss: 0.00000941
Iteration 43/1000 | Loss: 0.00000941
Iteration 44/1000 | Loss: 0.00000941
Iteration 45/1000 | Loss: 0.00000941
Iteration 46/1000 | Loss: 0.00000941
Iteration 47/1000 | Loss: 0.00000941
Iteration 48/1000 | Loss: 0.00000940
Iteration 49/1000 | Loss: 0.00000940
Iteration 50/1000 | Loss: 0.00000940
Iteration 51/1000 | Loss: 0.00000940
Iteration 52/1000 | Loss: 0.00000940
Iteration 53/1000 | Loss: 0.00000940
Iteration 54/1000 | Loss: 0.00000939
Iteration 55/1000 | Loss: 0.00000939
Iteration 56/1000 | Loss: 0.00000939
Iteration 57/1000 | Loss: 0.00000939
Iteration 58/1000 | Loss: 0.00000938
Iteration 59/1000 | Loss: 0.00000938
Iteration 60/1000 | Loss: 0.00000937
Iteration 61/1000 | Loss: 0.00000937
Iteration 62/1000 | Loss: 0.00000937
Iteration 63/1000 | Loss: 0.00000937
Iteration 64/1000 | Loss: 0.00000936
Iteration 65/1000 | Loss: 0.00000935
Iteration 66/1000 | Loss: 0.00000935
Iteration 67/1000 | Loss: 0.00000934
Iteration 68/1000 | Loss: 0.00000934
Iteration 69/1000 | Loss: 0.00000934
Iteration 70/1000 | Loss: 0.00000933
Iteration 71/1000 | Loss: 0.00000933
Iteration 72/1000 | Loss: 0.00000933
Iteration 73/1000 | Loss: 0.00000933
Iteration 74/1000 | Loss: 0.00000933
Iteration 75/1000 | Loss: 0.00000933
Iteration 76/1000 | Loss: 0.00000933
Iteration 77/1000 | Loss: 0.00000932
Iteration 78/1000 | Loss: 0.00000932
Iteration 79/1000 | Loss: 0.00000932
Iteration 80/1000 | Loss: 0.00000932
Iteration 81/1000 | Loss: 0.00000932
Iteration 82/1000 | Loss: 0.00000932
Iteration 83/1000 | Loss: 0.00000931
Iteration 84/1000 | Loss: 0.00000931
Iteration 85/1000 | Loss: 0.00000931
Iteration 86/1000 | Loss: 0.00000931
Iteration 87/1000 | Loss: 0.00000931
Iteration 88/1000 | Loss: 0.00000931
Iteration 89/1000 | Loss: 0.00000931
Iteration 90/1000 | Loss: 0.00000931
Iteration 91/1000 | Loss: 0.00000931
Iteration 92/1000 | Loss: 0.00000930
Iteration 93/1000 | Loss: 0.00000930
Iteration 94/1000 | Loss: 0.00000930
Iteration 95/1000 | Loss: 0.00000930
Iteration 96/1000 | Loss: 0.00000930
Iteration 97/1000 | Loss: 0.00000930
Iteration 98/1000 | Loss: 0.00000930
Iteration 99/1000 | Loss: 0.00000930
Iteration 100/1000 | Loss: 0.00000930
Iteration 101/1000 | Loss: 0.00000930
Iteration 102/1000 | Loss: 0.00000930
Iteration 103/1000 | Loss: 0.00000929
Iteration 104/1000 | Loss: 0.00000929
Iteration 105/1000 | Loss: 0.00000929
Iteration 106/1000 | Loss: 0.00000929
Iteration 107/1000 | Loss: 0.00000929
Iteration 108/1000 | Loss: 0.00000929
Iteration 109/1000 | Loss: 0.00000929
Iteration 110/1000 | Loss: 0.00000929
Iteration 111/1000 | Loss: 0.00000929
Iteration 112/1000 | Loss: 0.00000928
Iteration 113/1000 | Loss: 0.00000928
Iteration 114/1000 | Loss: 0.00000928
Iteration 115/1000 | Loss: 0.00000928
Iteration 116/1000 | Loss: 0.00000928
Iteration 117/1000 | Loss: 0.00000928
Iteration 118/1000 | Loss: 0.00000928
Iteration 119/1000 | Loss: 0.00000928
Iteration 120/1000 | Loss: 0.00000928
Iteration 121/1000 | Loss: 0.00000928
Iteration 122/1000 | Loss: 0.00000928
Iteration 123/1000 | Loss: 0.00000928
Iteration 124/1000 | Loss: 0.00000928
Iteration 125/1000 | Loss: 0.00000928
Iteration 126/1000 | Loss: 0.00000928
Iteration 127/1000 | Loss: 0.00000928
Iteration 128/1000 | Loss: 0.00000928
Iteration 129/1000 | Loss: 0.00000928
Iteration 130/1000 | Loss: 0.00000928
Iteration 131/1000 | Loss: 0.00000928
Iteration 132/1000 | Loss: 0.00000928
Iteration 133/1000 | Loss: 0.00000928
Iteration 134/1000 | Loss: 0.00000928
Iteration 135/1000 | Loss: 0.00000928
Iteration 136/1000 | Loss: 0.00000928
Iteration 137/1000 | Loss: 0.00000928
Iteration 138/1000 | Loss: 0.00000928
Iteration 139/1000 | Loss: 0.00000928
Iteration 140/1000 | Loss: 0.00000928
Iteration 141/1000 | Loss: 0.00000928
Iteration 142/1000 | Loss: 0.00000928
Iteration 143/1000 | Loss: 0.00000928
Iteration 144/1000 | Loss: 0.00000928
Iteration 145/1000 | Loss: 0.00000928
Iteration 146/1000 | Loss: 0.00000928
Iteration 147/1000 | Loss: 0.00000928
Iteration 148/1000 | Loss: 0.00000928
Iteration 149/1000 | Loss: 0.00000928
Iteration 150/1000 | Loss: 0.00000928
Iteration 151/1000 | Loss: 0.00000928
Iteration 152/1000 | Loss: 0.00000928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [9.27779638004722e-06, 9.27779638004722e-06, 9.27779638004722e-06, 9.27779638004722e-06, 9.27779638004722e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.27779638004722e-06

Optimization complete. Final v2v error: 2.6156423091888428 mm

Highest mean error: 3.2824020385742188 mm for frame 167

Lowest mean error: 2.1754586696624756 mm for frame 209

Saving results

Total time: 87.7914707660675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805839
Iteration 2/25 | Loss: 0.00145290
Iteration 3/25 | Loss: 0.00122555
Iteration 4/25 | Loss: 0.00119733
Iteration 5/25 | Loss: 0.00119320
Iteration 6/25 | Loss: 0.00117884
Iteration 7/25 | Loss: 0.00117651
Iteration 8/25 | Loss: 0.00117493
Iteration 9/25 | Loss: 0.00117470
Iteration 10/25 | Loss: 0.00117455
Iteration 11/25 | Loss: 0.00117444
Iteration 12/25 | Loss: 0.00117439
Iteration 13/25 | Loss: 0.00117439
Iteration 14/25 | Loss: 0.00117439
Iteration 15/25 | Loss: 0.00117438
Iteration 16/25 | Loss: 0.00117437
Iteration 17/25 | Loss: 0.00117437
Iteration 18/25 | Loss: 0.00117437
Iteration 19/25 | Loss: 0.00117437
Iteration 20/25 | Loss: 0.00117437
Iteration 21/25 | Loss: 0.00117437
Iteration 22/25 | Loss: 0.00117437
Iteration 23/25 | Loss: 0.00117436
Iteration 24/25 | Loss: 0.00117436
Iteration 25/25 | Loss: 0.00117436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34322047
Iteration 2/25 | Loss: 0.00309452
Iteration 3/25 | Loss: 0.00306895
Iteration 4/25 | Loss: 0.00306894
Iteration 5/25 | Loss: 0.00306894
Iteration 6/25 | Loss: 0.00306894
Iteration 7/25 | Loss: 0.00306894
Iteration 8/25 | Loss: 0.00306894
Iteration 9/25 | Loss: 0.00306894
Iteration 10/25 | Loss: 0.00306894
Iteration 11/25 | Loss: 0.00306894
Iteration 12/25 | Loss: 0.00306894
Iteration 13/25 | Loss: 0.00306894
Iteration 14/25 | Loss: 0.00306894
Iteration 15/25 | Loss: 0.00306894
Iteration 16/25 | Loss: 0.00306894
Iteration 17/25 | Loss: 0.00306894
Iteration 18/25 | Loss: 0.00306894
Iteration 19/25 | Loss: 0.00306894
Iteration 20/25 | Loss: 0.00306894
Iteration 21/25 | Loss: 0.00306894
Iteration 22/25 | Loss: 0.00306894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0030689402483403683, 0.0030689402483403683, 0.0030689402483403683, 0.0030689402483403683, 0.0030689402483403683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030689402483403683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306894
Iteration 2/1000 | Loss: 0.00008731
Iteration 3/1000 | Loss: 0.00052540
Iteration 4/1000 | Loss: 0.00003939
Iteration 5/1000 | Loss: 0.00002857
Iteration 6/1000 | Loss: 0.00002309
Iteration 7/1000 | Loss: 0.00002167
Iteration 8/1000 | Loss: 0.00012656
Iteration 9/1000 | Loss: 0.00002610
Iteration 10/1000 | Loss: 0.00023518
Iteration 11/1000 | Loss: 0.00011014
Iteration 12/1000 | Loss: 0.00012948
Iteration 13/1000 | Loss: 0.00002371
Iteration 14/1000 | Loss: 0.00020144
Iteration 15/1000 | Loss: 0.00015603
Iteration 16/1000 | Loss: 0.00037402
Iteration 17/1000 | Loss: 0.00003263
Iteration 18/1000 | Loss: 0.00052023
Iteration 19/1000 | Loss: 0.00032255
Iteration 20/1000 | Loss: 0.00012509
Iteration 21/1000 | Loss: 0.00002147
Iteration 22/1000 | Loss: 0.00002026
Iteration 23/1000 | Loss: 0.00001923
Iteration 24/1000 | Loss: 0.00001870
Iteration 25/1000 | Loss: 0.00001816
Iteration 26/1000 | Loss: 0.00001777
Iteration 27/1000 | Loss: 0.00001777
Iteration 28/1000 | Loss: 0.00001751
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001740
Iteration 31/1000 | Loss: 0.00001733
Iteration 32/1000 | Loss: 0.00001728
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001722
Iteration 36/1000 | Loss: 0.00001722
Iteration 37/1000 | Loss: 0.00001722
Iteration 38/1000 | Loss: 0.00001718
Iteration 39/1000 | Loss: 0.00001717
Iteration 40/1000 | Loss: 0.00001717
Iteration 41/1000 | Loss: 0.00001717
Iteration 42/1000 | Loss: 0.00001716
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001715
Iteration 46/1000 | Loss: 0.00001714
Iteration 47/1000 | Loss: 0.00001714
Iteration 48/1000 | Loss: 0.00001713
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001711
Iteration 51/1000 | Loss: 0.00001711
Iteration 52/1000 | Loss: 0.00001711
Iteration 53/1000 | Loss: 0.00001709
Iteration 54/1000 | Loss: 0.00001709
Iteration 55/1000 | Loss: 0.00001707
Iteration 56/1000 | Loss: 0.00001706
Iteration 57/1000 | Loss: 0.00001706
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001704
Iteration 60/1000 | Loss: 0.00001704
Iteration 61/1000 | Loss: 0.00001702
Iteration 62/1000 | Loss: 0.00001701
Iteration 63/1000 | Loss: 0.00001701
Iteration 64/1000 | Loss: 0.00001701
Iteration 65/1000 | Loss: 0.00001701
Iteration 66/1000 | Loss: 0.00001701
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001697
Iteration 70/1000 | Loss: 0.00001697
Iteration 71/1000 | Loss: 0.00001697
Iteration 72/1000 | Loss: 0.00001696
Iteration 73/1000 | Loss: 0.00001696
Iteration 74/1000 | Loss: 0.00001695
Iteration 75/1000 | Loss: 0.00001695
Iteration 76/1000 | Loss: 0.00001694
Iteration 77/1000 | Loss: 0.00001693
Iteration 78/1000 | Loss: 0.00001693
Iteration 79/1000 | Loss: 0.00001693
Iteration 80/1000 | Loss: 0.00001692
Iteration 81/1000 | Loss: 0.00001692
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001691
Iteration 84/1000 | Loss: 0.00001690
Iteration 85/1000 | Loss: 0.00001689
Iteration 86/1000 | Loss: 0.00001689
Iteration 87/1000 | Loss: 0.00001689
Iteration 88/1000 | Loss: 0.00001688
Iteration 89/1000 | Loss: 0.00001688
Iteration 90/1000 | Loss: 0.00001688
Iteration 91/1000 | Loss: 0.00001687
Iteration 92/1000 | Loss: 0.00001687
Iteration 93/1000 | Loss: 0.00001687
Iteration 94/1000 | Loss: 0.00001686
Iteration 95/1000 | Loss: 0.00001686
Iteration 96/1000 | Loss: 0.00001686
Iteration 97/1000 | Loss: 0.00001686
Iteration 98/1000 | Loss: 0.00001685
Iteration 99/1000 | Loss: 0.00001685
Iteration 100/1000 | Loss: 0.00001685
Iteration 101/1000 | Loss: 0.00001685
Iteration 102/1000 | Loss: 0.00001684
Iteration 103/1000 | Loss: 0.00001684
Iteration 104/1000 | Loss: 0.00001683
Iteration 105/1000 | Loss: 0.00001683
Iteration 106/1000 | Loss: 0.00001683
Iteration 107/1000 | Loss: 0.00001683
Iteration 108/1000 | Loss: 0.00001682
Iteration 109/1000 | Loss: 0.00001682
Iteration 110/1000 | Loss: 0.00001682
Iteration 111/1000 | Loss: 0.00001682
Iteration 112/1000 | Loss: 0.00001681
Iteration 113/1000 | Loss: 0.00001681
Iteration 114/1000 | Loss: 0.00001681
Iteration 115/1000 | Loss: 0.00001680
Iteration 116/1000 | Loss: 0.00001680
Iteration 117/1000 | Loss: 0.00001680
Iteration 118/1000 | Loss: 0.00001679
Iteration 119/1000 | Loss: 0.00001679
Iteration 120/1000 | Loss: 0.00001679
Iteration 121/1000 | Loss: 0.00001679
Iteration 122/1000 | Loss: 0.00001678
Iteration 123/1000 | Loss: 0.00001678
Iteration 124/1000 | Loss: 0.00001678
Iteration 125/1000 | Loss: 0.00001678
Iteration 126/1000 | Loss: 0.00001678
Iteration 127/1000 | Loss: 0.00001677
Iteration 128/1000 | Loss: 0.00001677
Iteration 129/1000 | Loss: 0.00001677
Iteration 130/1000 | Loss: 0.00001677
Iteration 131/1000 | Loss: 0.00001677
Iteration 132/1000 | Loss: 0.00001677
Iteration 133/1000 | Loss: 0.00001677
Iteration 134/1000 | Loss: 0.00001677
Iteration 135/1000 | Loss: 0.00001676
Iteration 136/1000 | Loss: 0.00001676
Iteration 137/1000 | Loss: 0.00001676
Iteration 138/1000 | Loss: 0.00001676
Iteration 139/1000 | Loss: 0.00001676
Iteration 140/1000 | Loss: 0.00001676
Iteration 141/1000 | Loss: 0.00001676
Iteration 142/1000 | Loss: 0.00001675
Iteration 143/1000 | Loss: 0.00001675
Iteration 144/1000 | Loss: 0.00001675
Iteration 145/1000 | Loss: 0.00001675
Iteration 146/1000 | Loss: 0.00001675
Iteration 147/1000 | Loss: 0.00001675
Iteration 148/1000 | Loss: 0.00001675
Iteration 149/1000 | Loss: 0.00001675
Iteration 150/1000 | Loss: 0.00001675
Iteration 151/1000 | Loss: 0.00001675
Iteration 152/1000 | Loss: 0.00001675
Iteration 153/1000 | Loss: 0.00001674
Iteration 154/1000 | Loss: 0.00001674
Iteration 155/1000 | Loss: 0.00001674
Iteration 156/1000 | Loss: 0.00001674
Iteration 157/1000 | Loss: 0.00001674
Iteration 158/1000 | Loss: 0.00001674
Iteration 159/1000 | Loss: 0.00001674
Iteration 160/1000 | Loss: 0.00001674
Iteration 161/1000 | Loss: 0.00001674
Iteration 162/1000 | Loss: 0.00001674
Iteration 163/1000 | Loss: 0.00001674
Iteration 164/1000 | Loss: 0.00001674
Iteration 165/1000 | Loss: 0.00001674
Iteration 166/1000 | Loss: 0.00001674
Iteration 167/1000 | Loss: 0.00001674
Iteration 168/1000 | Loss: 0.00001673
Iteration 169/1000 | Loss: 0.00001673
Iteration 170/1000 | Loss: 0.00001673
Iteration 171/1000 | Loss: 0.00001673
Iteration 172/1000 | Loss: 0.00001673
Iteration 173/1000 | Loss: 0.00001673
Iteration 174/1000 | Loss: 0.00001673
Iteration 175/1000 | Loss: 0.00001673
Iteration 176/1000 | Loss: 0.00001673
Iteration 177/1000 | Loss: 0.00001673
Iteration 178/1000 | Loss: 0.00001673
Iteration 179/1000 | Loss: 0.00001673
Iteration 180/1000 | Loss: 0.00001673
Iteration 181/1000 | Loss: 0.00001673
Iteration 182/1000 | Loss: 0.00001673
Iteration 183/1000 | Loss: 0.00001673
Iteration 184/1000 | Loss: 0.00001673
Iteration 185/1000 | Loss: 0.00001673
Iteration 186/1000 | Loss: 0.00001673
Iteration 187/1000 | Loss: 0.00001672
Iteration 188/1000 | Loss: 0.00001672
Iteration 189/1000 | Loss: 0.00001672
Iteration 190/1000 | Loss: 0.00001672
Iteration 191/1000 | Loss: 0.00001672
Iteration 192/1000 | Loss: 0.00001672
Iteration 193/1000 | Loss: 0.00001672
Iteration 194/1000 | Loss: 0.00001672
Iteration 195/1000 | Loss: 0.00001672
Iteration 196/1000 | Loss: 0.00001672
Iteration 197/1000 | Loss: 0.00001672
Iteration 198/1000 | Loss: 0.00001672
Iteration 199/1000 | Loss: 0.00001672
Iteration 200/1000 | Loss: 0.00001672
Iteration 201/1000 | Loss: 0.00001672
Iteration 202/1000 | Loss: 0.00001672
Iteration 203/1000 | Loss: 0.00001672
Iteration 204/1000 | Loss: 0.00001672
Iteration 205/1000 | Loss: 0.00001672
Iteration 206/1000 | Loss: 0.00001672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.6720166968298145e-05, 1.6720166968298145e-05, 1.6720166968298145e-05, 1.6720166968298145e-05, 1.6720166968298145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6720166968298145e-05

Optimization complete. Final v2v error: 3.505422353744507 mm

Highest mean error: 4.268585205078125 mm for frame 162

Lowest mean error: 2.938206911087036 mm for frame 224

Saving results

Total time: 87.69146227836609
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042834
Iteration 2/25 | Loss: 0.00129915
Iteration 3/25 | Loss: 0.00107540
Iteration 4/25 | Loss: 0.00104828
Iteration 5/25 | Loss: 0.00104684
Iteration 6/25 | Loss: 0.00104203
Iteration 7/25 | Loss: 0.00104161
Iteration 8/25 | Loss: 0.00104156
Iteration 9/25 | Loss: 0.00104156
Iteration 10/25 | Loss: 0.00104155
Iteration 11/25 | Loss: 0.00104155
Iteration 12/25 | Loss: 0.00104154
Iteration 13/25 | Loss: 0.00104154
Iteration 14/25 | Loss: 0.00104154
Iteration 15/25 | Loss: 0.00104154
Iteration 16/25 | Loss: 0.00104154
Iteration 17/25 | Loss: 0.00104154
Iteration 18/25 | Loss: 0.00104154
Iteration 19/25 | Loss: 0.00104154
Iteration 20/25 | Loss: 0.00104154
Iteration 21/25 | Loss: 0.00104153
Iteration 22/25 | Loss: 0.00104153
Iteration 23/25 | Loss: 0.00104153
Iteration 24/25 | Loss: 0.00104153
Iteration 25/25 | Loss: 0.00104153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22085202
Iteration 2/25 | Loss: 0.00220169
Iteration 3/25 | Loss: 0.00220169
Iteration 4/25 | Loss: 0.00220169
Iteration 5/25 | Loss: 0.00220169
Iteration 6/25 | Loss: 0.00220169
Iteration 7/25 | Loss: 0.00220169
Iteration 8/25 | Loss: 0.00220169
Iteration 9/25 | Loss: 0.00220169
Iteration 10/25 | Loss: 0.00220169
Iteration 11/25 | Loss: 0.00220169
Iteration 12/25 | Loss: 0.00220169
Iteration 13/25 | Loss: 0.00220169
Iteration 14/25 | Loss: 0.00220169
Iteration 15/25 | Loss: 0.00220169
Iteration 16/25 | Loss: 0.00220169
Iteration 17/25 | Loss: 0.00220169
Iteration 18/25 | Loss: 0.00220169
Iteration 19/25 | Loss: 0.00220169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002201688475906849, 0.002201688475906849, 0.002201688475906849, 0.002201688475906849, 0.002201688475906849]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002201688475906849

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220169
Iteration 2/1000 | Loss: 0.00004289
Iteration 3/1000 | Loss: 0.00001353
Iteration 4/1000 | Loss: 0.00001205
Iteration 5/1000 | Loss: 0.00001104
Iteration 6/1000 | Loss: 0.00001066
Iteration 7/1000 | Loss: 0.00001008
Iteration 8/1000 | Loss: 0.00000987
Iteration 9/1000 | Loss: 0.00000965
Iteration 10/1000 | Loss: 0.00000953
Iteration 11/1000 | Loss: 0.00000940
Iteration 12/1000 | Loss: 0.00000935
Iteration 13/1000 | Loss: 0.00000924
Iteration 14/1000 | Loss: 0.00000924
Iteration 15/1000 | Loss: 0.00000920
Iteration 16/1000 | Loss: 0.00000919
Iteration 17/1000 | Loss: 0.00000918
Iteration 18/1000 | Loss: 0.00000917
Iteration 19/1000 | Loss: 0.00000916
Iteration 20/1000 | Loss: 0.00000915
Iteration 21/1000 | Loss: 0.00000912
Iteration 22/1000 | Loss: 0.00000910
Iteration 23/1000 | Loss: 0.00000907
Iteration 24/1000 | Loss: 0.00000907
Iteration 25/1000 | Loss: 0.00000905
Iteration 26/1000 | Loss: 0.00000904
Iteration 27/1000 | Loss: 0.00000904
Iteration 28/1000 | Loss: 0.00000904
Iteration 29/1000 | Loss: 0.00000904
Iteration 30/1000 | Loss: 0.00000904
Iteration 31/1000 | Loss: 0.00000904
Iteration 32/1000 | Loss: 0.00000904
Iteration 33/1000 | Loss: 0.00000904
Iteration 34/1000 | Loss: 0.00000903
Iteration 35/1000 | Loss: 0.00000903
Iteration 36/1000 | Loss: 0.00000903
Iteration 37/1000 | Loss: 0.00000903
Iteration 38/1000 | Loss: 0.00000903
Iteration 39/1000 | Loss: 0.00000903
Iteration 40/1000 | Loss: 0.00000901
Iteration 41/1000 | Loss: 0.00000900
Iteration 42/1000 | Loss: 0.00000900
Iteration 43/1000 | Loss: 0.00000900
Iteration 44/1000 | Loss: 0.00000900
Iteration 45/1000 | Loss: 0.00000900
Iteration 46/1000 | Loss: 0.00000900
Iteration 47/1000 | Loss: 0.00000900
Iteration 48/1000 | Loss: 0.00000899
Iteration 49/1000 | Loss: 0.00000899
Iteration 50/1000 | Loss: 0.00000899
Iteration 51/1000 | Loss: 0.00000898
Iteration 52/1000 | Loss: 0.00000898
Iteration 53/1000 | Loss: 0.00000898
Iteration 54/1000 | Loss: 0.00000897
Iteration 55/1000 | Loss: 0.00000897
Iteration 56/1000 | Loss: 0.00000897
Iteration 57/1000 | Loss: 0.00000897
Iteration 58/1000 | Loss: 0.00000896
Iteration 59/1000 | Loss: 0.00000896
Iteration 60/1000 | Loss: 0.00000896
Iteration 61/1000 | Loss: 0.00000896
Iteration 62/1000 | Loss: 0.00000895
Iteration 63/1000 | Loss: 0.00000895
Iteration 64/1000 | Loss: 0.00000895
Iteration 65/1000 | Loss: 0.00000895
Iteration 66/1000 | Loss: 0.00000895
Iteration 67/1000 | Loss: 0.00000895
Iteration 68/1000 | Loss: 0.00000894
Iteration 69/1000 | Loss: 0.00000894
Iteration 70/1000 | Loss: 0.00000894
Iteration 71/1000 | Loss: 0.00000894
Iteration 72/1000 | Loss: 0.00000894
Iteration 73/1000 | Loss: 0.00000894
Iteration 74/1000 | Loss: 0.00000893
Iteration 75/1000 | Loss: 0.00000892
Iteration 76/1000 | Loss: 0.00000892
Iteration 77/1000 | Loss: 0.00000891
Iteration 78/1000 | Loss: 0.00000891
Iteration 79/1000 | Loss: 0.00000891
Iteration 80/1000 | Loss: 0.00000891
Iteration 81/1000 | Loss: 0.00000891
Iteration 82/1000 | Loss: 0.00000891
Iteration 83/1000 | Loss: 0.00000891
Iteration 84/1000 | Loss: 0.00000891
Iteration 85/1000 | Loss: 0.00000891
Iteration 86/1000 | Loss: 0.00000890
Iteration 87/1000 | Loss: 0.00000890
Iteration 88/1000 | Loss: 0.00000890
Iteration 89/1000 | Loss: 0.00000890
Iteration 90/1000 | Loss: 0.00000890
Iteration 91/1000 | Loss: 0.00000890
Iteration 92/1000 | Loss: 0.00000890
Iteration 93/1000 | Loss: 0.00000890
Iteration 94/1000 | Loss: 0.00000890
Iteration 95/1000 | Loss: 0.00000890
Iteration 96/1000 | Loss: 0.00000890
Iteration 97/1000 | Loss: 0.00000890
Iteration 98/1000 | Loss: 0.00000890
Iteration 99/1000 | Loss: 0.00000890
Iteration 100/1000 | Loss: 0.00000890
Iteration 101/1000 | Loss: 0.00000889
Iteration 102/1000 | Loss: 0.00000889
Iteration 103/1000 | Loss: 0.00000889
Iteration 104/1000 | Loss: 0.00000889
Iteration 105/1000 | Loss: 0.00000889
Iteration 106/1000 | Loss: 0.00000889
Iteration 107/1000 | Loss: 0.00000887
Iteration 108/1000 | Loss: 0.00000887
Iteration 109/1000 | Loss: 0.00000887
Iteration 110/1000 | Loss: 0.00000887
Iteration 111/1000 | Loss: 0.00000887
Iteration 112/1000 | Loss: 0.00000886
Iteration 113/1000 | Loss: 0.00000886
Iteration 114/1000 | Loss: 0.00000886
Iteration 115/1000 | Loss: 0.00000886
Iteration 116/1000 | Loss: 0.00000886
Iteration 117/1000 | Loss: 0.00000885
Iteration 118/1000 | Loss: 0.00000885
Iteration 119/1000 | Loss: 0.00000885
Iteration 120/1000 | Loss: 0.00000884
Iteration 121/1000 | Loss: 0.00000884
Iteration 122/1000 | Loss: 0.00000884
Iteration 123/1000 | Loss: 0.00000884
Iteration 124/1000 | Loss: 0.00000883
Iteration 125/1000 | Loss: 0.00000883
Iteration 126/1000 | Loss: 0.00000883
Iteration 127/1000 | Loss: 0.00000883
Iteration 128/1000 | Loss: 0.00000882
Iteration 129/1000 | Loss: 0.00000882
Iteration 130/1000 | Loss: 0.00000882
Iteration 131/1000 | Loss: 0.00000882
Iteration 132/1000 | Loss: 0.00000882
Iteration 133/1000 | Loss: 0.00000881
Iteration 134/1000 | Loss: 0.00000881
Iteration 135/1000 | Loss: 0.00000881
Iteration 136/1000 | Loss: 0.00000881
Iteration 137/1000 | Loss: 0.00000881
Iteration 138/1000 | Loss: 0.00000881
Iteration 139/1000 | Loss: 0.00000881
Iteration 140/1000 | Loss: 0.00000881
Iteration 141/1000 | Loss: 0.00000881
Iteration 142/1000 | Loss: 0.00000881
Iteration 143/1000 | Loss: 0.00000880
Iteration 144/1000 | Loss: 0.00000880
Iteration 145/1000 | Loss: 0.00000880
Iteration 146/1000 | Loss: 0.00000880
Iteration 147/1000 | Loss: 0.00000880
Iteration 148/1000 | Loss: 0.00000879
Iteration 149/1000 | Loss: 0.00000879
Iteration 150/1000 | Loss: 0.00000879
Iteration 151/1000 | Loss: 0.00000879
Iteration 152/1000 | Loss: 0.00000879
Iteration 153/1000 | Loss: 0.00000879
Iteration 154/1000 | Loss: 0.00000879
Iteration 155/1000 | Loss: 0.00000879
Iteration 156/1000 | Loss: 0.00000879
Iteration 157/1000 | Loss: 0.00000879
Iteration 158/1000 | Loss: 0.00000879
Iteration 159/1000 | Loss: 0.00000878
Iteration 160/1000 | Loss: 0.00000878
Iteration 161/1000 | Loss: 0.00000878
Iteration 162/1000 | Loss: 0.00000878
Iteration 163/1000 | Loss: 0.00000878
Iteration 164/1000 | Loss: 0.00000878
Iteration 165/1000 | Loss: 0.00000878
Iteration 166/1000 | Loss: 0.00000878
Iteration 167/1000 | Loss: 0.00000878
Iteration 168/1000 | Loss: 0.00000878
Iteration 169/1000 | Loss: 0.00000878
Iteration 170/1000 | Loss: 0.00000878
Iteration 171/1000 | Loss: 0.00000878
Iteration 172/1000 | Loss: 0.00000878
Iteration 173/1000 | Loss: 0.00000877
Iteration 174/1000 | Loss: 0.00000877
Iteration 175/1000 | Loss: 0.00000877
Iteration 176/1000 | Loss: 0.00000877
Iteration 177/1000 | Loss: 0.00000877
Iteration 178/1000 | Loss: 0.00000877
Iteration 179/1000 | Loss: 0.00000877
Iteration 180/1000 | Loss: 0.00000877
Iteration 181/1000 | Loss: 0.00000877
Iteration 182/1000 | Loss: 0.00000877
Iteration 183/1000 | Loss: 0.00000876
Iteration 184/1000 | Loss: 0.00000876
Iteration 185/1000 | Loss: 0.00000876
Iteration 186/1000 | Loss: 0.00000876
Iteration 187/1000 | Loss: 0.00000876
Iteration 188/1000 | Loss: 0.00000876
Iteration 189/1000 | Loss: 0.00000876
Iteration 190/1000 | Loss: 0.00000876
Iteration 191/1000 | Loss: 0.00000876
Iteration 192/1000 | Loss: 0.00000876
Iteration 193/1000 | Loss: 0.00000876
Iteration 194/1000 | Loss: 0.00000876
Iteration 195/1000 | Loss: 0.00000876
Iteration 196/1000 | Loss: 0.00000876
Iteration 197/1000 | Loss: 0.00000876
Iteration 198/1000 | Loss: 0.00000876
Iteration 199/1000 | Loss: 0.00000876
Iteration 200/1000 | Loss: 0.00000876
Iteration 201/1000 | Loss: 0.00000876
Iteration 202/1000 | Loss: 0.00000876
Iteration 203/1000 | Loss: 0.00000876
Iteration 204/1000 | Loss: 0.00000876
Iteration 205/1000 | Loss: 0.00000876
Iteration 206/1000 | Loss: 0.00000876
Iteration 207/1000 | Loss: 0.00000876
Iteration 208/1000 | Loss: 0.00000876
Iteration 209/1000 | Loss: 0.00000876
Iteration 210/1000 | Loss: 0.00000876
Iteration 211/1000 | Loss: 0.00000876
Iteration 212/1000 | Loss: 0.00000876
Iteration 213/1000 | Loss: 0.00000876
Iteration 214/1000 | Loss: 0.00000876
Iteration 215/1000 | Loss: 0.00000876
Iteration 216/1000 | Loss: 0.00000876
Iteration 217/1000 | Loss: 0.00000876
Iteration 218/1000 | Loss: 0.00000876
Iteration 219/1000 | Loss: 0.00000876
Iteration 220/1000 | Loss: 0.00000876
Iteration 221/1000 | Loss: 0.00000876
Iteration 222/1000 | Loss: 0.00000876
Iteration 223/1000 | Loss: 0.00000876
Iteration 224/1000 | Loss: 0.00000876
Iteration 225/1000 | Loss: 0.00000876
Iteration 226/1000 | Loss: 0.00000876
Iteration 227/1000 | Loss: 0.00000876
Iteration 228/1000 | Loss: 0.00000876
Iteration 229/1000 | Loss: 0.00000876
Iteration 230/1000 | Loss: 0.00000876
Iteration 231/1000 | Loss: 0.00000876
Iteration 232/1000 | Loss: 0.00000876
Iteration 233/1000 | Loss: 0.00000876
Iteration 234/1000 | Loss: 0.00000876
Iteration 235/1000 | Loss: 0.00000876
Iteration 236/1000 | Loss: 0.00000876
Iteration 237/1000 | Loss: 0.00000876
Iteration 238/1000 | Loss: 0.00000876
Iteration 239/1000 | Loss: 0.00000876
Iteration 240/1000 | Loss: 0.00000876
Iteration 241/1000 | Loss: 0.00000876
Iteration 242/1000 | Loss: 0.00000876
Iteration 243/1000 | Loss: 0.00000876
Iteration 244/1000 | Loss: 0.00000876
Iteration 245/1000 | Loss: 0.00000876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [8.756057468417566e-06, 8.756057468417566e-06, 8.756057468417566e-06, 8.756057468417566e-06, 8.756057468417566e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.756057468417566e-06

Optimization complete. Final v2v error: 2.545320749282837 mm

Highest mean error: 2.8338747024536133 mm for frame 96

Lowest mean error: 1.7815186977386475 mm for frame 2

Saving results

Total time: 42.86937880516052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00744011
Iteration 2/25 | Loss: 0.00119432
Iteration 3/25 | Loss: 0.00105478
Iteration 4/25 | Loss: 0.00104121
Iteration 5/25 | Loss: 0.00103840
Iteration 6/25 | Loss: 0.00103840
Iteration 7/25 | Loss: 0.00103840
Iteration 8/25 | Loss: 0.00103840
Iteration 9/25 | Loss: 0.00103840
Iteration 10/25 | Loss: 0.00103840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001038402784615755, 0.001038402784615755, 0.001038402784615755, 0.001038402784615755, 0.001038402784615755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001038402784615755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19873393
Iteration 2/25 | Loss: 0.00176845
Iteration 3/25 | Loss: 0.00176844
Iteration 4/25 | Loss: 0.00176844
Iteration 5/25 | Loss: 0.00176844
Iteration 6/25 | Loss: 0.00176844
Iteration 7/25 | Loss: 0.00176844
Iteration 8/25 | Loss: 0.00176844
Iteration 9/25 | Loss: 0.00176844
Iteration 10/25 | Loss: 0.00176844
Iteration 11/25 | Loss: 0.00176844
Iteration 12/25 | Loss: 0.00176844
Iteration 13/25 | Loss: 0.00176844
Iteration 14/25 | Loss: 0.00176844
Iteration 15/25 | Loss: 0.00176844
Iteration 16/25 | Loss: 0.00176844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001768442103639245, 0.001768442103639245, 0.001768442103639245, 0.001768442103639245, 0.001768442103639245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001768442103639245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176844
Iteration 2/1000 | Loss: 0.00003075
Iteration 3/1000 | Loss: 0.00002304
Iteration 4/1000 | Loss: 0.00002025
Iteration 5/1000 | Loss: 0.00001816
Iteration 6/1000 | Loss: 0.00001676
Iteration 7/1000 | Loss: 0.00001605
Iteration 8/1000 | Loss: 0.00001556
Iteration 9/1000 | Loss: 0.00001514
Iteration 10/1000 | Loss: 0.00001472
Iteration 11/1000 | Loss: 0.00001447
Iteration 12/1000 | Loss: 0.00001426
Iteration 13/1000 | Loss: 0.00001422
Iteration 14/1000 | Loss: 0.00001421
Iteration 15/1000 | Loss: 0.00001420
Iteration 16/1000 | Loss: 0.00001412
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001396
Iteration 19/1000 | Loss: 0.00001396
Iteration 20/1000 | Loss: 0.00001395
Iteration 21/1000 | Loss: 0.00001394
Iteration 22/1000 | Loss: 0.00001393
Iteration 23/1000 | Loss: 0.00001392
Iteration 24/1000 | Loss: 0.00001392
Iteration 25/1000 | Loss: 0.00001390
Iteration 26/1000 | Loss: 0.00001389
Iteration 27/1000 | Loss: 0.00001387
Iteration 28/1000 | Loss: 0.00001386
Iteration 29/1000 | Loss: 0.00001386
Iteration 30/1000 | Loss: 0.00001386
Iteration 31/1000 | Loss: 0.00001386
Iteration 32/1000 | Loss: 0.00001385
Iteration 33/1000 | Loss: 0.00001385
Iteration 34/1000 | Loss: 0.00001385
Iteration 35/1000 | Loss: 0.00001384
Iteration 36/1000 | Loss: 0.00001384
Iteration 37/1000 | Loss: 0.00001382
Iteration 38/1000 | Loss: 0.00001382
Iteration 39/1000 | Loss: 0.00001382
Iteration 40/1000 | Loss: 0.00001381
Iteration 41/1000 | Loss: 0.00001381
Iteration 42/1000 | Loss: 0.00001381
Iteration 43/1000 | Loss: 0.00001380
Iteration 44/1000 | Loss: 0.00001380
Iteration 45/1000 | Loss: 0.00001380
Iteration 46/1000 | Loss: 0.00001380
Iteration 47/1000 | Loss: 0.00001380
Iteration 48/1000 | Loss: 0.00001380
Iteration 49/1000 | Loss: 0.00001380
Iteration 50/1000 | Loss: 0.00001380
Iteration 51/1000 | Loss: 0.00001380
Iteration 52/1000 | Loss: 0.00001380
Iteration 53/1000 | Loss: 0.00001380
Iteration 54/1000 | Loss: 0.00001379
Iteration 55/1000 | Loss: 0.00001378
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001377
Iteration 58/1000 | Loss: 0.00001376
Iteration 59/1000 | Loss: 0.00001376
Iteration 60/1000 | Loss: 0.00001375
Iteration 61/1000 | Loss: 0.00001374
Iteration 62/1000 | Loss: 0.00001374
Iteration 63/1000 | Loss: 0.00001373
Iteration 64/1000 | Loss: 0.00001372
Iteration 65/1000 | Loss: 0.00001372
Iteration 66/1000 | Loss: 0.00001372
Iteration 67/1000 | Loss: 0.00001372
Iteration 68/1000 | Loss: 0.00001372
Iteration 69/1000 | Loss: 0.00001372
Iteration 70/1000 | Loss: 0.00001372
Iteration 71/1000 | Loss: 0.00001372
Iteration 72/1000 | Loss: 0.00001372
Iteration 73/1000 | Loss: 0.00001371
Iteration 74/1000 | Loss: 0.00001371
Iteration 75/1000 | Loss: 0.00001371
Iteration 76/1000 | Loss: 0.00001371
Iteration 77/1000 | Loss: 0.00001371
Iteration 78/1000 | Loss: 0.00001371
Iteration 79/1000 | Loss: 0.00001370
Iteration 80/1000 | Loss: 0.00001370
Iteration 81/1000 | Loss: 0.00001370
Iteration 82/1000 | Loss: 0.00001370
Iteration 83/1000 | Loss: 0.00001370
Iteration 84/1000 | Loss: 0.00001370
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001369
Iteration 87/1000 | Loss: 0.00001369
Iteration 88/1000 | Loss: 0.00001369
Iteration 89/1000 | Loss: 0.00001368
Iteration 90/1000 | Loss: 0.00001368
Iteration 91/1000 | Loss: 0.00001368
Iteration 92/1000 | Loss: 0.00001368
Iteration 93/1000 | Loss: 0.00001367
Iteration 94/1000 | Loss: 0.00001367
Iteration 95/1000 | Loss: 0.00001367
Iteration 96/1000 | Loss: 0.00001366
Iteration 97/1000 | Loss: 0.00001365
Iteration 98/1000 | Loss: 0.00001365
Iteration 99/1000 | Loss: 0.00001365
Iteration 100/1000 | Loss: 0.00001364
Iteration 101/1000 | Loss: 0.00001364
Iteration 102/1000 | Loss: 0.00001364
Iteration 103/1000 | Loss: 0.00001364
Iteration 104/1000 | Loss: 0.00001364
Iteration 105/1000 | Loss: 0.00001363
Iteration 106/1000 | Loss: 0.00001363
Iteration 107/1000 | Loss: 0.00001363
Iteration 108/1000 | Loss: 0.00001362
Iteration 109/1000 | Loss: 0.00001362
Iteration 110/1000 | Loss: 0.00001362
Iteration 111/1000 | Loss: 0.00001361
Iteration 112/1000 | Loss: 0.00001361
Iteration 113/1000 | Loss: 0.00001361
Iteration 114/1000 | Loss: 0.00001360
Iteration 115/1000 | Loss: 0.00001360
Iteration 116/1000 | Loss: 0.00001360
Iteration 117/1000 | Loss: 0.00001360
Iteration 118/1000 | Loss: 0.00001360
Iteration 119/1000 | Loss: 0.00001359
Iteration 120/1000 | Loss: 0.00001359
Iteration 121/1000 | Loss: 0.00001359
Iteration 122/1000 | Loss: 0.00001359
Iteration 123/1000 | Loss: 0.00001358
Iteration 124/1000 | Loss: 0.00001358
Iteration 125/1000 | Loss: 0.00001358
Iteration 126/1000 | Loss: 0.00001358
Iteration 127/1000 | Loss: 0.00001358
Iteration 128/1000 | Loss: 0.00001358
Iteration 129/1000 | Loss: 0.00001358
Iteration 130/1000 | Loss: 0.00001358
Iteration 131/1000 | Loss: 0.00001358
Iteration 132/1000 | Loss: 0.00001358
Iteration 133/1000 | Loss: 0.00001358
Iteration 134/1000 | Loss: 0.00001358
Iteration 135/1000 | Loss: 0.00001357
Iteration 136/1000 | Loss: 0.00001357
Iteration 137/1000 | Loss: 0.00001357
Iteration 138/1000 | Loss: 0.00001357
Iteration 139/1000 | Loss: 0.00001357
Iteration 140/1000 | Loss: 0.00001357
Iteration 141/1000 | Loss: 0.00001357
Iteration 142/1000 | Loss: 0.00001357
Iteration 143/1000 | Loss: 0.00001357
Iteration 144/1000 | Loss: 0.00001357
Iteration 145/1000 | Loss: 0.00001356
Iteration 146/1000 | Loss: 0.00001356
Iteration 147/1000 | Loss: 0.00001356
Iteration 148/1000 | Loss: 0.00001356
Iteration 149/1000 | Loss: 0.00001356
Iteration 150/1000 | Loss: 0.00001356
Iteration 151/1000 | Loss: 0.00001356
Iteration 152/1000 | Loss: 0.00001356
Iteration 153/1000 | Loss: 0.00001356
Iteration 154/1000 | Loss: 0.00001356
Iteration 155/1000 | Loss: 0.00001356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.3563246284320485e-05, 1.3563246284320485e-05, 1.3563246284320485e-05, 1.3563246284320485e-05, 1.3563246284320485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3563246284320485e-05

Optimization complete. Final v2v error: 3.0920586585998535 mm

Highest mean error: 4.403269290924072 mm for frame 96

Lowest mean error: 2.4832088947296143 mm for frame 46

Saving results

Total time: 43.208643674850464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055156
Iteration 2/25 | Loss: 0.01055156
Iteration 3/25 | Loss: 0.01055155
Iteration 4/25 | Loss: 0.01055155
Iteration 5/25 | Loss: 0.01055155
Iteration 6/25 | Loss: 0.01055155
Iteration 7/25 | Loss: 0.01055155
Iteration 8/25 | Loss: 0.01055154
Iteration 9/25 | Loss: 0.01055154
Iteration 10/25 | Loss: 0.01055154
Iteration 11/25 | Loss: 0.01055154
Iteration 12/25 | Loss: 0.01055154
Iteration 13/25 | Loss: 0.01055154
Iteration 14/25 | Loss: 0.01055154
Iteration 15/25 | Loss: 0.01055154
Iteration 16/25 | Loss: 0.01055153
Iteration 17/25 | Loss: 0.01055153
Iteration 18/25 | Loss: 0.01055153
Iteration 19/25 | Loss: 0.01055153
Iteration 20/25 | Loss: 0.01055153
Iteration 21/25 | Loss: 0.01055153
Iteration 22/25 | Loss: 0.01055152
Iteration 23/25 | Loss: 0.01055152
Iteration 24/25 | Loss: 0.01055152
Iteration 25/25 | Loss: 0.01055152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49625373
Iteration 2/25 | Loss: 0.08437680
Iteration 3/25 | Loss: 0.08435322
Iteration 4/25 | Loss: 0.08435319
Iteration 5/25 | Loss: 0.08435319
Iteration 6/25 | Loss: 0.08435319
Iteration 7/25 | Loss: 0.08435319
Iteration 8/25 | Loss: 0.08435318
Iteration 9/25 | Loss: 0.08435318
Iteration 10/25 | Loss: 0.08435318
Iteration 11/25 | Loss: 0.08435318
Iteration 12/25 | Loss: 0.08435318
Iteration 13/25 | Loss: 0.08435318
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.08435317873954773, 0.08435317873954773, 0.08435317873954773, 0.08435317873954773, 0.08435317873954773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08435317873954773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08435318
Iteration 2/1000 | Loss: 0.00076075
Iteration 3/1000 | Loss: 0.00041485
Iteration 4/1000 | Loss: 0.00014422
Iteration 5/1000 | Loss: 0.00006596
Iteration 6/1000 | Loss: 0.00007029
Iteration 7/1000 | Loss: 0.00013518
Iteration 8/1000 | Loss: 0.00008108
Iteration 9/1000 | Loss: 0.00008211
Iteration 10/1000 | Loss: 0.00011860
Iteration 11/1000 | Loss: 0.00026850
Iteration 12/1000 | Loss: 0.00003109
Iteration 13/1000 | Loss: 0.00003939
Iteration 14/1000 | Loss: 0.00001949
Iteration 15/1000 | Loss: 0.00002206
Iteration 16/1000 | Loss: 0.00004800
Iteration 17/1000 | Loss: 0.00001624
Iteration 18/1000 | Loss: 0.00002920
Iteration 19/1000 | Loss: 0.00011849
Iteration 20/1000 | Loss: 0.00001720
Iteration 21/1000 | Loss: 0.00006243
Iteration 22/1000 | Loss: 0.00001523
Iteration 23/1000 | Loss: 0.00001092
Iteration 24/1000 | Loss: 0.00001040
Iteration 25/1000 | Loss: 0.00000997
Iteration 26/1000 | Loss: 0.00002989
Iteration 27/1000 | Loss: 0.00000951
Iteration 28/1000 | Loss: 0.00002761
Iteration 29/1000 | Loss: 0.00002276
Iteration 30/1000 | Loss: 0.00001106
Iteration 31/1000 | Loss: 0.00000957
Iteration 32/1000 | Loss: 0.00000892
Iteration 33/1000 | Loss: 0.00001515
Iteration 34/1000 | Loss: 0.00007637
Iteration 35/1000 | Loss: 0.00007937
Iteration 36/1000 | Loss: 0.00002394
Iteration 37/1000 | Loss: 0.00004765
Iteration 38/1000 | Loss: 0.00000796
Iteration 39/1000 | Loss: 0.00000788
Iteration 40/1000 | Loss: 0.00000780
Iteration 41/1000 | Loss: 0.00000780
Iteration 42/1000 | Loss: 0.00002554
Iteration 43/1000 | Loss: 0.00001442
Iteration 44/1000 | Loss: 0.00002806
Iteration 45/1000 | Loss: 0.00001799
Iteration 46/1000 | Loss: 0.00001719
Iteration 47/1000 | Loss: 0.00000846
Iteration 48/1000 | Loss: 0.00000745
Iteration 49/1000 | Loss: 0.00000745
Iteration 50/1000 | Loss: 0.00000745
Iteration 51/1000 | Loss: 0.00000742
Iteration 52/1000 | Loss: 0.00002818
Iteration 53/1000 | Loss: 0.00000737
Iteration 54/1000 | Loss: 0.00001474
Iteration 55/1000 | Loss: 0.00005234
Iteration 56/1000 | Loss: 0.00006422
Iteration 57/1000 | Loss: 0.00005099
Iteration 58/1000 | Loss: 0.00005158
Iteration 59/1000 | Loss: 0.00001100
Iteration 60/1000 | Loss: 0.00001744
Iteration 61/1000 | Loss: 0.00000708
Iteration 62/1000 | Loss: 0.00000761
Iteration 63/1000 | Loss: 0.00000705
Iteration 64/1000 | Loss: 0.00000704
Iteration 65/1000 | Loss: 0.00000704
Iteration 66/1000 | Loss: 0.00000704
Iteration 67/1000 | Loss: 0.00000704
Iteration 68/1000 | Loss: 0.00000703
Iteration 69/1000 | Loss: 0.00000703
Iteration 70/1000 | Loss: 0.00001351
Iteration 71/1000 | Loss: 0.00000857
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00000712
Iteration 74/1000 | Loss: 0.00000699
Iteration 75/1000 | Loss: 0.00000699
Iteration 76/1000 | Loss: 0.00000699
Iteration 77/1000 | Loss: 0.00000699
Iteration 78/1000 | Loss: 0.00000699
Iteration 79/1000 | Loss: 0.00000699
Iteration 80/1000 | Loss: 0.00000698
Iteration 81/1000 | Loss: 0.00000703
Iteration 82/1000 | Loss: 0.00000703
Iteration 83/1000 | Loss: 0.00000706
Iteration 84/1000 | Loss: 0.00000698
Iteration 85/1000 | Loss: 0.00000698
Iteration 86/1000 | Loss: 0.00000698
Iteration 87/1000 | Loss: 0.00000697
Iteration 88/1000 | Loss: 0.00000698
Iteration 89/1000 | Loss: 0.00000698
Iteration 90/1000 | Loss: 0.00000697
Iteration 91/1000 | Loss: 0.00000697
Iteration 92/1000 | Loss: 0.00000697
Iteration 93/1000 | Loss: 0.00000697
Iteration 94/1000 | Loss: 0.00000697
Iteration 95/1000 | Loss: 0.00000697
Iteration 96/1000 | Loss: 0.00000697
Iteration 97/1000 | Loss: 0.00000697
Iteration 98/1000 | Loss: 0.00000697
Iteration 99/1000 | Loss: 0.00000697
Iteration 100/1000 | Loss: 0.00000697
Iteration 101/1000 | Loss: 0.00000697
Iteration 102/1000 | Loss: 0.00000697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [6.971625680307625e-06, 6.971625680307625e-06, 6.971625680307625e-06, 6.971625680307625e-06, 6.971625680307625e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.971625680307625e-06

Optimization complete. Final v2v error: 2.358250617980957 mm

Highest mean error: 2.6873390674591064 mm for frame 153

Lowest mean error: 2.0842602252960205 mm for frame 183

Saving results

Total time: 92.4618227481842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917598
Iteration 2/25 | Loss: 0.00131981
Iteration 3/25 | Loss: 0.00113665
Iteration 4/25 | Loss: 0.00110498
Iteration 5/25 | Loss: 0.00109569
Iteration 6/25 | Loss: 0.00109213
Iteration 7/25 | Loss: 0.00109140
Iteration 8/25 | Loss: 0.00109140
Iteration 9/25 | Loss: 0.00109140
Iteration 10/25 | Loss: 0.00109140
Iteration 11/25 | Loss: 0.00109140
Iteration 12/25 | Loss: 0.00109140
Iteration 13/25 | Loss: 0.00109140
Iteration 14/25 | Loss: 0.00109140
Iteration 15/25 | Loss: 0.00109140
Iteration 16/25 | Loss: 0.00109140
Iteration 17/25 | Loss: 0.00109140
Iteration 18/25 | Loss: 0.00109140
Iteration 19/25 | Loss: 0.00109140
Iteration 20/25 | Loss: 0.00109140
Iteration 21/25 | Loss: 0.00109140
Iteration 22/25 | Loss: 0.00109140
Iteration 23/25 | Loss: 0.00109140
Iteration 24/25 | Loss: 0.00109140
Iteration 25/25 | Loss: 0.00109140

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24708605
Iteration 2/25 | Loss: 0.00178837
Iteration 3/25 | Loss: 0.00178835
Iteration 4/25 | Loss: 0.00178835
Iteration 5/25 | Loss: 0.00178835
Iteration 6/25 | Loss: 0.00178835
Iteration 7/25 | Loss: 0.00178834
Iteration 8/25 | Loss: 0.00178834
Iteration 9/25 | Loss: 0.00178834
Iteration 10/25 | Loss: 0.00178834
Iteration 11/25 | Loss: 0.00178834
Iteration 12/25 | Loss: 0.00178834
Iteration 13/25 | Loss: 0.00178834
Iteration 14/25 | Loss: 0.00178834
Iteration 15/25 | Loss: 0.00178834
Iteration 16/25 | Loss: 0.00178834
Iteration 17/25 | Loss: 0.00178834
Iteration 18/25 | Loss: 0.00178834
Iteration 19/25 | Loss: 0.00178834
Iteration 20/25 | Loss: 0.00178834
Iteration 21/25 | Loss: 0.00178834
Iteration 22/25 | Loss: 0.00178834
Iteration 23/25 | Loss: 0.00178834
Iteration 24/25 | Loss: 0.00178834
Iteration 25/25 | Loss: 0.00178834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178834
Iteration 2/1000 | Loss: 0.00004306
Iteration 3/1000 | Loss: 0.00002892
Iteration 4/1000 | Loss: 0.00002418
Iteration 5/1000 | Loss: 0.00002275
Iteration 6/1000 | Loss: 0.00002183
Iteration 7/1000 | Loss: 0.00002122
Iteration 8/1000 | Loss: 0.00002058
Iteration 9/1000 | Loss: 0.00002025
Iteration 10/1000 | Loss: 0.00001995
Iteration 11/1000 | Loss: 0.00001976
Iteration 12/1000 | Loss: 0.00001962
Iteration 13/1000 | Loss: 0.00001960
Iteration 14/1000 | Loss: 0.00001950
Iteration 15/1000 | Loss: 0.00001948
Iteration 16/1000 | Loss: 0.00001941
Iteration 17/1000 | Loss: 0.00001934
Iteration 18/1000 | Loss: 0.00001933
Iteration 19/1000 | Loss: 0.00001930
Iteration 20/1000 | Loss: 0.00001928
Iteration 21/1000 | Loss: 0.00001925
Iteration 22/1000 | Loss: 0.00001923
Iteration 23/1000 | Loss: 0.00001922
Iteration 24/1000 | Loss: 0.00001922
Iteration 25/1000 | Loss: 0.00001922
Iteration 26/1000 | Loss: 0.00001922
Iteration 27/1000 | Loss: 0.00001922
Iteration 28/1000 | Loss: 0.00001922
Iteration 29/1000 | Loss: 0.00001922
Iteration 30/1000 | Loss: 0.00001922
Iteration 31/1000 | Loss: 0.00001922
Iteration 32/1000 | Loss: 0.00001922
Iteration 33/1000 | Loss: 0.00001922
Iteration 34/1000 | Loss: 0.00001921
Iteration 35/1000 | Loss: 0.00001921
Iteration 36/1000 | Loss: 0.00001921
Iteration 37/1000 | Loss: 0.00001921
Iteration 38/1000 | Loss: 0.00001921
Iteration 39/1000 | Loss: 0.00001919
Iteration 40/1000 | Loss: 0.00001919
Iteration 41/1000 | Loss: 0.00001918
Iteration 42/1000 | Loss: 0.00001918
Iteration 43/1000 | Loss: 0.00001918
Iteration 44/1000 | Loss: 0.00001918
Iteration 45/1000 | Loss: 0.00001917
Iteration 46/1000 | Loss: 0.00001917
Iteration 47/1000 | Loss: 0.00001917
Iteration 48/1000 | Loss: 0.00001917
Iteration 49/1000 | Loss: 0.00001916
Iteration 50/1000 | Loss: 0.00001916
Iteration 51/1000 | Loss: 0.00001916
Iteration 52/1000 | Loss: 0.00001916
Iteration 53/1000 | Loss: 0.00001916
Iteration 54/1000 | Loss: 0.00001916
Iteration 55/1000 | Loss: 0.00001915
Iteration 56/1000 | Loss: 0.00001915
Iteration 57/1000 | Loss: 0.00001915
Iteration 58/1000 | Loss: 0.00001915
Iteration 59/1000 | Loss: 0.00001915
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001915
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001915
Iteration 66/1000 | Loss: 0.00001914
Iteration 67/1000 | Loss: 0.00001914
Iteration 68/1000 | Loss: 0.00001914
Iteration 69/1000 | Loss: 0.00001914
Iteration 70/1000 | Loss: 0.00001914
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001914
Iteration 73/1000 | Loss: 0.00001913
Iteration 74/1000 | Loss: 0.00001913
Iteration 75/1000 | Loss: 0.00001913
Iteration 76/1000 | Loss: 0.00001913
Iteration 77/1000 | Loss: 0.00001913
Iteration 78/1000 | Loss: 0.00001913
Iteration 79/1000 | Loss: 0.00001912
Iteration 80/1000 | Loss: 0.00001912
Iteration 81/1000 | Loss: 0.00001912
Iteration 82/1000 | Loss: 0.00001912
Iteration 83/1000 | Loss: 0.00001912
Iteration 84/1000 | Loss: 0.00001912
Iteration 85/1000 | Loss: 0.00001912
Iteration 86/1000 | Loss: 0.00001911
Iteration 87/1000 | Loss: 0.00001911
Iteration 88/1000 | Loss: 0.00001911
Iteration 89/1000 | Loss: 0.00001911
Iteration 90/1000 | Loss: 0.00001911
Iteration 91/1000 | Loss: 0.00001910
Iteration 92/1000 | Loss: 0.00001910
Iteration 93/1000 | Loss: 0.00001910
Iteration 94/1000 | Loss: 0.00001910
Iteration 95/1000 | Loss: 0.00001910
Iteration 96/1000 | Loss: 0.00001910
Iteration 97/1000 | Loss: 0.00001910
Iteration 98/1000 | Loss: 0.00001910
Iteration 99/1000 | Loss: 0.00001909
Iteration 100/1000 | Loss: 0.00001909
Iteration 101/1000 | Loss: 0.00001909
Iteration 102/1000 | Loss: 0.00001909
Iteration 103/1000 | Loss: 0.00001909
Iteration 104/1000 | Loss: 0.00001909
Iteration 105/1000 | Loss: 0.00001909
Iteration 106/1000 | Loss: 0.00001909
Iteration 107/1000 | Loss: 0.00001909
Iteration 108/1000 | Loss: 0.00001908
Iteration 109/1000 | Loss: 0.00001908
Iteration 110/1000 | Loss: 0.00001908
Iteration 111/1000 | Loss: 0.00001908
Iteration 112/1000 | Loss: 0.00001908
Iteration 113/1000 | Loss: 0.00001907
Iteration 114/1000 | Loss: 0.00001907
Iteration 115/1000 | Loss: 0.00001907
Iteration 116/1000 | Loss: 0.00001907
Iteration 117/1000 | Loss: 0.00001907
Iteration 118/1000 | Loss: 0.00001907
Iteration 119/1000 | Loss: 0.00001907
Iteration 120/1000 | Loss: 0.00001906
Iteration 121/1000 | Loss: 0.00001906
Iteration 122/1000 | Loss: 0.00001906
Iteration 123/1000 | Loss: 0.00001906
Iteration 124/1000 | Loss: 0.00001906
Iteration 125/1000 | Loss: 0.00001905
Iteration 126/1000 | Loss: 0.00001905
Iteration 127/1000 | Loss: 0.00001905
Iteration 128/1000 | Loss: 0.00001904
Iteration 129/1000 | Loss: 0.00001904
Iteration 130/1000 | Loss: 0.00001904
Iteration 131/1000 | Loss: 0.00001904
Iteration 132/1000 | Loss: 0.00001904
Iteration 133/1000 | Loss: 0.00001903
Iteration 134/1000 | Loss: 0.00001903
Iteration 135/1000 | Loss: 0.00001903
Iteration 136/1000 | Loss: 0.00001903
Iteration 137/1000 | Loss: 0.00001902
Iteration 138/1000 | Loss: 0.00001902
Iteration 139/1000 | Loss: 0.00001902
Iteration 140/1000 | Loss: 0.00001902
Iteration 141/1000 | Loss: 0.00001902
Iteration 142/1000 | Loss: 0.00001901
Iteration 143/1000 | Loss: 0.00001901
Iteration 144/1000 | Loss: 0.00001901
Iteration 145/1000 | Loss: 0.00001901
Iteration 146/1000 | Loss: 0.00001901
Iteration 147/1000 | Loss: 0.00001901
Iteration 148/1000 | Loss: 0.00001901
Iteration 149/1000 | Loss: 0.00001901
Iteration 150/1000 | Loss: 0.00001901
Iteration 151/1000 | Loss: 0.00001901
Iteration 152/1000 | Loss: 0.00001901
Iteration 153/1000 | Loss: 0.00001900
Iteration 154/1000 | Loss: 0.00001900
Iteration 155/1000 | Loss: 0.00001900
Iteration 156/1000 | Loss: 0.00001900
Iteration 157/1000 | Loss: 0.00001900
Iteration 158/1000 | Loss: 0.00001900
Iteration 159/1000 | Loss: 0.00001900
Iteration 160/1000 | Loss: 0.00001900
Iteration 161/1000 | Loss: 0.00001900
Iteration 162/1000 | Loss: 0.00001900
Iteration 163/1000 | Loss: 0.00001900
Iteration 164/1000 | Loss: 0.00001900
Iteration 165/1000 | Loss: 0.00001900
Iteration 166/1000 | Loss: 0.00001900
Iteration 167/1000 | Loss: 0.00001900
Iteration 168/1000 | Loss: 0.00001900
Iteration 169/1000 | Loss: 0.00001899
Iteration 170/1000 | Loss: 0.00001899
Iteration 171/1000 | Loss: 0.00001899
Iteration 172/1000 | Loss: 0.00001899
Iteration 173/1000 | Loss: 0.00001899
Iteration 174/1000 | Loss: 0.00001899
Iteration 175/1000 | Loss: 0.00001899
Iteration 176/1000 | Loss: 0.00001899
Iteration 177/1000 | Loss: 0.00001899
Iteration 178/1000 | Loss: 0.00001898
Iteration 179/1000 | Loss: 0.00001898
Iteration 180/1000 | Loss: 0.00001898
Iteration 181/1000 | Loss: 0.00001898
Iteration 182/1000 | Loss: 0.00001897
Iteration 183/1000 | Loss: 0.00001897
Iteration 184/1000 | Loss: 0.00001897
Iteration 185/1000 | Loss: 0.00001897
Iteration 186/1000 | Loss: 0.00001897
Iteration 187/1000 | Loss: 0.00001897
Iteration 188/1000 | Loss: 0.00001897
Iteration 189/1000 | Loss: 0.00001897
Iteration 190/1000 | Loss: 0.00001897
Iteration 191/1000 | Loss: 0.00001897
Iteration 192/1000 | Loss: 0.00001897
Iteration 193/1000 | Loss: 0.00001897
Iteration 194/1000 | Loss: 0.00001897
Iteration 195/1000 | Loss: 0.00001897
Iteration 196/1000 | Loss: 0.00001897
Iteration 197/1000 | Loss: 0.00001897
Iteration 198/1000 | Loss: 0.00001897
Iteration 199/1000 | Loss: 0.00001897
Iteration 200/1000 | Loss: 0.00001897
Iteration 201/1000 | Loss: 0.00001897
Iteration 202/1000 | Loss: 0.00001897
Iteration 203/1000 | Loss: 0.00001897
Iteration 204/1000 | Loss: 0.00001897
Iteration 205/1000 | Loss: 0.00001897
Iteration 206/1000 | Loss: 0.00001897
Iteration 207/1000 | Loss: 0.00001897
Iteration 208/1000 | Loss: 0.00001897
Iteration 209/1000 | Loss: 0.00001897
Iteration 210/1000 | Loss: 0.00001897
Iteration 211/1000 | Loss: 0.00001897
Iteration 212/1000 | Loss: 0.00001897
Iteration 213/1000 | Loss: 0.00001897
Iteration 214/1000 | Loss: 0.00001897
Iteration 215/1000 | Loss: 0.00001897
Iteration 216/1000 | Loss: 0.00001897
Iteration 217/1000 | Loss: 0.00001897
Iteration 218/1000 | Loss: 0.00001897
Iteration 219/1000 | Loss: 0.00001897
Iteration 220/1000 | Loss: 0.00001897
Iteration 221/1000 | Loss: 0.00001897
Iteration 222/1000 | Loss: 0.00001897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.896966750791762e-05, 1.896966750791762e-05, 1.896966750791762e-05, 1.896966750791762e-05, 1.896966750791762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.896966750791762e-05

Optimization complete. Final v2v error: 3.638638973236084 mm

Highest mean error: 5.238195896148682 mm for frame 66

Lowest mean error: 2.797292947769165 mm for frame 45

Saving results

Total time: 44.29001188278198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00663832
Iteration 2/25 | Loss: 0.00123690
Iteration 3/25 | Loss: 0.00110790
Iteration 4/25 | Loss: 0.00107807
Iteration 5/25 | Loss: 0.00107256
Iteration 6/25 | Loss: 0.00107095
Iteration 7/25 | Loss: 0.00107095
Iteration 8/25 | Loss: 0.00107095
Iteration 9/25 | Loss: 0.00107095
Iteration 10/25 | Loss: 0.00107095
Iteration 11/25 | Loss: 0.00107095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010709520429372787, 0.0010709520429372787, 0.0010709520429372787, 0.0010709520429372787, 0.0010709520429372787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010709520429372787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21072507
Iteration 2/25 | Loss: 0.00230127
Iteration 3/25 | Loss: 0.00230126
Iteration 4/25 | Loss: 0.00230126
Iteration 5/25 | Loss: 0.00230126
Iteration 6/25 | Loss: 0.00230126
Iteration 7/25 | Loss: 0.00230126
Iteration 8/25 | Loss: 0.00230126
Iteration 9/25 | Loss: 0.00230126
Iteration 10/25 | Loss: 0.00230126
Iteration 11/25 | Loss: 0.00230126
Iteration 12/25 | Loss: 0.00230126
Iteration 13/25 | Loss: 0.00230126
Iteration 14/25 | Loss: 0.00230126
Iteration 15/25 | Loss: 0.00230126
Iteration 16/25 | Loss: 0.00230126
Iteration 17/25 | Loss: 0.00230126
Iteration 18/25 | Loss: 0.00230126
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00230125873349607, 0.00230125873349607, 0.00230125873349607, 0.00230125873349607, 0.00230125873349607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00230125873349607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230126
Iteration 2/1000 | Loss: 0.00004436
Iteration 3/1000 | Loss: 0.00002254
Iteration 4/1000 | Loss: 0.00001995
Iteration 5/1000 | Loss: 0.00001904
Iteration 6/1000 | Loss: 0.00001840
Iteration 7/1000 | Loss: 0.00001790
Iteration 8/1000 | Loss: 0.00001756
Iteration 9/1000 | Loss: 0.00001721
Iteration 10/1000 | Loss: 0.00001688
Iteration 11/1000 | Loss: 0.00001669
Iteration 12/1000 | Loss: 0.00001653
Iteration 13/1000 | Loss: 0.00001633
Iteration 14/1000 | Loss: 0.00001624
Iteration 15/1000 | Loss: 0.00001615
Iteration 16/1000 | Loss: 0.00001611
Iteration 17/1000 | Loss: 0.00001610
Iteration 18/1000 | Loss: 0.00001609
Iteration 19/1000 | Loss: 0.00001607
Iteration 20/1000 | Loss: 0.00001607
Iteration 21/1000 | Loss: 0.00001606
Iteration 22/1000 | Loss: 0.00001604
Iteration 23/1000 | Loss: 0.00001604
Iteration 24/1000 | Loss: 0.00001603
Iteration 25/1000 | Loss: 0.00001601
Iteration 26/1000 | Loss: 0.00001601
Iteration 27/1000 | Loss: 0.00001599
Iteration 28/1000 | Loss: 0.00001599
Iteration 29/1000 | Loss: 0.00001597
Iteration 30/1000 | Loss: 0.00001596
Iteration 31/1000 | Loss: 0.00001596
Iteration 32/1000 | Loss: 0.00001595
Iteration 33/1000 | Loss: 0.00001595
Iteration 34/1000 | Loss: 0.00001594
Iteration 35/1000 | Loss: 0.00001594
Iteration 36/1000 | Loss: 0.00001594
Iteration 37/1000 | Loss: 0.00001594
Iteration 38/1000 | Loss: 0.00001594
Iteration 39/1000 | Loss: 0.00001594
Iteration 40/1000 | Loss: 0.00001593
Iteration 41/1000 | Loss: 0.00001593
Iteration 42/1000 | Loss: 0.00001593
Iteration 43/1000 | Loss: 0.00001593
Iteration 44/1000 | Loss: 0.00001593
Iteration 45/1000 | Loss: 0.00001592
Iteration 46/1000 | Loss: 0.00001592
Iteration 47/1000 | Loss: 0.00001592
Iteration 48/1000 | Loss: 0.00001592
Iteration 49/1000 | Loss: 0.00001592
Iteration 50/1000 | Loss: 0.00001591
Iteration 51/1000 | Loss: 0.00001591
Iteration 52/1000 | Loss: 0.00001591
Iteration 53/1000 | Loss: 0.00001591
Iteration 54/1000 | Loss: 0.00001590
Iteration 55/1000 | Loss: 0.00001590
Iteration 56/1000 | Loss: 0.00001590
Iteration 57/1000 | Loss: 0.00001590
Iteration 58/1000 | Loss: 0.00001590
Iteration 59/1000 | Loss: 0.00001589
Iteration 60/1000 | Loss: 0.00001589
Iteration 61/1000 | Loss: 0.00001589
Iteration 62/1000 | Loss: 0.00001589
Iteration 63/1000 | Loss: 0.00001589
Iteration 64/1000 | Loss: 0.00001588
Iteration 65/1000 | Loss: 0.00001588
Iteration 66/1000 | Loss: 0.00001588
Iteration 67/1000 | Loss: 0.00001588
Iteration 68/1000 | Loss: 0.00001588
Iteration 69/1000 | Loss: 0.00001587
Iteration 70/1000 | Loss: 0.00001587
Iteration 71/1000 | Loss: 0.00001587
Iteration 72/1000 | Loss: 0.00001587
Iteration 73/1000 | Loss: 0.00001587
Iteration 74/1000 | Loss: 0.00001587
Iteration 75/1000 | Loss: 0.00001587
Iteration 76/1000 | Loss: 0.00001587
Iteration 77/1000 | Loss: 0.00001587
Iteration 78/1000 | Loss: 0.00001587
Iteration 79/1000 | Loss: 0.00001587
Iteration 80/1000 | Loss: 0.00001587
Iteration 81/1000 | Loss: 0.00001587
Iteration 82/1000 | Loss: 0.00001586
Iteration 83/1000 | Loss: 0.00001586
Iteration 84/1000 | Loss: 0.00001586
Iteration 85/1000 | Loss: 0.00001586
Iteration 86/1000 | Loss: 0.00001586
Iteration 87/1000 | Loss: 0.00001586
Iteration 88/1000 | Loss: 0.00001586
Iteration 89/1000 | Loss: 0.00001586
Iteration 90/1000 | Loss: 0.00001586
Iteration 91/1000 | Loss: 0.00001586
Iteration 92/1000 | Loss: 0.00001586
Iteration 93/1000 | Loss: 0.00001586
Iteration 94/1000 | Loss: 0.00001586
Iteration 95/1000 | Loss: 0.00001586
Iteration 96/1000 | Loss: 0.00001586
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001586
Iteration 99/1000 | Loss: 0.00001586
Iteration 100/1000 | Loss: 0.00001586
Iteration 101/1000 | Loss: 0.00001586
Iteration 102/1000 | Loss: 0.00001586
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001585
Iteration 106/1000 | Loss: 0.00001585
Iteration 107/1000 | Loss: 0.00001585
Iteration 108/1000 | Loss: 0.00001585
Iteration 109/1000 | Loss: 0.00001585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.5854438970563933e-05, 1.5854438970563933e-05, 1.5854438970563933e-05, 1.5854438970563933e-05, 1.5854438970563933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5854438970563933e-05

Optimization complete. Final v2v error: 3.3942983150482178 mm

Highest mean error: 3.889683485031128 mm for frame 26

Lowest mean error: 2.833043336868286 mm for frame 209

Saving results

Total time: 40.86221790313721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_2559/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_2559/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481809
Iteration 2/25 | Loss: 0.00143689
Iteration 3/25 | Loss: 0.00109558
Iteration 4/25 | Loss: 0.00105276
Iteration 5/25 | Loss: 0.00104847
Iteration 6/25 | Loss: 0.00104789
Iteration 7/25 | Loss: 0.00104789
Iteration 8/25 | Loss: 0.00104789
Iteration 9/25 | Loss: 0.00104789
Iteration 10/25 | Loss: 0.00104789
Iteration 11/25 | Loss: 0.00104789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010478852782398462, 0.0010478852782398462, 0.0010478852782398462, 0.0010478852782398462, 0.0010478852782398462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010478852782398462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23667991
Iteration 2/25 | Loss: 0.00171737
Iteration 3/25 | Loss: 0.00171737
Iteration 4/25 | Loss: 0.00171736
Iteration 5/25 | Loss: 0.00171736
Iteration 6/25 | Loss: 0.00171736
Iteration 7/25 | Loss: 0.00171736
Iteration 8/25 | Loss: 0.00171736
Iteration 9/25 | Loss: 0.00171736
Iteration 10/25 | Loss: 0.00171736
Iteration 11/25 | Loss: 0.00171736
Iteration 12/25 | Loss: 0.00171736
Iteration 13/25 | Loss: 0.00171736
Iteration 14/25 | Loss: 0.00171736
Iteration 15/25 | Loss: 0.00171736
Iteration 16/25 | Loss: 0.00171736
Iteration 17/25 | Loss: 0.00171736
Iteration 18/25 | Loss: 0.00171736
Iteration 19/25 | Loss: 0.00171736
Iteration 20/25 | Loss: 0.00171736
Iteration 21/25 | Loss: 0.00171736
Iteration 22/25 | Loss: 0.00171736
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0017173625528812408, 0.0017173625528812408, 0.0017173625528812408, 0.0017173625528812408, 0.0017173625528812408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017173625528812408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171736
Iteration 2/1000 | Loss: 0.00002194
Iteration 3/1000 | Loss: 0.00001493
Iteration 4/1000 | Loss: 0.00001359
Iteration 5/1000 | Loss: 0.00001249
Iteration 6/1000 | Loss: 0.00001185
Iteration 7/1000 | Loss: 0.00001139
Iteration 8/1000 | Loss: 0.00001108
Iteration 9/1000 | Loss: 0.00001074
Iteration 10/1000 | Loss: 0.00001065
Iteration 11/1000 | Loss: 0.00001052
Iteration 12/1000 | Loss: 0.00001051
Iteration 13/1000 | Loss: 0.00001048
Iteration 14/1000 | Loss: 0.00001034
Iteration 15/1000 | Loss: 0.00001034
Iteration 16/1000 | Loss: 0.00001028
Iteration 17/1000 | Loss: 0.00001026
Iteration 18/1000 | Loss: 0.00001019
Iteration 19/1000 | Loss: 0.00001016
Iteration 20/1000 | Loss: 0.00001012
Iteration 21/1000 | Loss: 0.00001010
Iteration 22/1000 | Loss: 0.00001009
Iteration 23/1000 | Loss: 0.00001008
Iteration 24/1000 | Loss: 0.00001007
Iteration 25/1000 | Loss: 0.00001006
Iteration 26/1000 | Loss: 0.00001006
Iteration 27/1000 | Loss: 0.00001005
Iteration 28/1000 | Loss: 0.00001005
Iteration 29/1000 | Loss: 0.00000998
Iteration 30/1000 | Loss: 0.00000997
Iteration 31/1000 | Loss: 0.00000996
Iteration 32/1000 | Loss: 0.00000995
Iteration 33/1000 | Loss: 0.00000995
Iteration 34/1000 | Loss: 0.00000994
Iteration 35/1000 | Loss: 0.00000993
Iteration 36/1000 | Loss: 0.00000993
Iteration 37/1000 | Loss: 0.00000992
Iteration 38/1000 | Loss: 0.00000991
Iteration 39/1000 | Loss: 0.00000990
Iteration 40/1000 | Loss: 0.00000990
Iteration 41/1000 | Loss: 0.00000990
Iteration 42/1000 | Loss: 0.00000990
Iteration 43/1000 | Loss: 0.00000989
Iteration 44/1000 | Loss: 0.00000989
Iteration 45/1000 | Loss: 0.00000989
Iteration 46/1000 | Loss: 0.00000989
Iteration 47/1000 | Loss: 0.00000988
Iteration 48/1000 | Loss: 0.00000988
Iteration 49/1000 | Loss: 0.00000988
Iteration 50/1000 | Loss: 0.00000988
Iteration 51/1000 | Loss: 0.00000987
Iteration 52/1000 | Loss: 0.00000987
Iteration 53/1000 | Loss: 0.00000987
Iteration 54/1000 | Loss: 0.00000987
Iteration 55/1000 | Loss: 0.00000986
Iteration 56/1000 | Loss: 0.00000986
Iteration 57/1000 | Loss: 0.00000986
Iteration 58/1000 | Loss: 0.00000985
Iteration 59/1000 | Loss: 0.00000985
Iteration 60/1000 | Loss: 0.00000985
Iteration 61/1000 | Loss: 0.00000984
Iteration 62/1000 | Loss: 0.00000984
Iteration 63/1000 | Loss: 0.00000984
Iteration 64/1000 | Loss: 0.00000984
Iteration 65/1000 | Loss: 0.00000983
Iteration 66/1000 | Loss: 0.00000983
Iteration 67/1000 | Loss: 0.00000983
Iteration 68/1000 | Loss: 0.00000983
Iteration 69/1000 | Loss: 0.00000983
Iteration 70/1000 | Loss: 0.00000983
Iteration 71/1000 | Loss: 0.00000982
Iteration 72/1000 | Loss: 0.00000982
Iteration 73/1000 | Loss: 0.00000982
Iteration 74/1000 | Loss: 0.00000981
Iteration 75/1000 | Loss: 0.00000981
Iteration 76/1000 | Loss: 0.00000981
Iteration 77/1000 | Loss: 0.00000980
Iteration 78/1000 | Loss: 0.00000980
Iteration 79/1000 | Loss: 0.00000980
Iteration 80/1000 | Loss: 0.00000980
Iteration 81/1000 | Loss: 0.00000979
Iteration 82/1000 | Loss: 0.00000979
Iteration 83/1000 | Loss: 0.00000979
Iteration 84/1000 | Loss: 0.00000978
Iteration 85/1000 | Loss: 0.00000978
Iteration 86/1000 | Loss: 0.00000978
Iteration 87/1000 | Loss: 0.00000977
Iteration 88/1000 | Loss: 0.00000977
Iteration 89/1000 | Loss: 0.00000977
Iteration 90/1000 | Loss: 0.00000977
Iteration 91/1000 | Loss: 0.00000976
Iteration 92/1000 | Loss: 0.00000976
Iteration 93/1000 | Loss: 0.00000976
Iteration 94/1000 | Loss: 0.00000976
Iteration 95/1000 | Loss: 0.00000976
Iteration 96/1000 | Loss: 0.00000976
Iteration 97/1000 | Loss: 0.00000976
Iteration 98/1000 | Loss: 0.00000976
Iteration 99/1000 | Loss: 0.00000976
Iteration 100/1000 | Loss: 0.00000975
Iteration 101/1000 | Loss: 0.00000975
Iteration 102/1000 | Loss: 0.00000975
Iteration 103/1000 | Loss: 0.00000974
Iteration 104/1000 | Loss: 0.00000973
Iteration 105/1000 | Loss: 0.00000973
Iteration 106/1000 | Loss: 0.00000972
Iteration 107/1000 | Loss: 0.00000972
Iteration 108/1000 | Loss: 0.00000972
Iteration 109/1000 | Loss: 0.00000972
Iteration 110/1000 | Loss: 0.00000971
Iteration 111/1000 | Loss: 0.00000971
Iteration 112/1000 | Loss: 0.00000970
Iteration 113/1000 | Loss: 0.00000970
Iteration 114/1000 | Loss: 0.00000970
Iteration 115/1000 | Loss: 0.00000969
Iteration 116/1000 | Loss: 0.00000969
Iteration 117/1000 | Loss: 0.00000969
Iteration 118/1000 | Loss: 0.00000969
Iteration 119/1000 | Loss: 0.00000969
Iteration 120/1000 | Loss: 0.00000969
Iteration 121/1000 | Loss: 0.00000969
Iteration 122/1000 | Loss: 0.00000969
Iteration 123/1000 | Loss: 0.00000969
Iteration 124/1000 | Loss: 0.00000969
Iteration 125/1000 | Loss: 0.00000969
Iteration 126/1000 | Loss: 0.00000969
Iteration 127/1000 | Loss: 0.00000969
Iteration 128/1000 | Loss: 0.00000969
Iteration 129/1000 | Loss: 0.00000969
Iteration 130/1000 | Loss: 0.00000969
Iteration 131/1000 | Loss: 0.00000969
Iteration 132/1000 | Loss: 0.00000969
Iteration 133/1000 | Loss: 0.00000969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [9.686998964753002e-06, 9.686998964753002e-06, 9.686998964753002e-06, 9.686998964753002e-06, 9.686998964753002e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.686998964753002e-06

Optimization complete. Final v2v error: 2.608175277709961 mm

Highest mean error: 3.153228759765625 mm for frame 139

Lowest mean error: 2.209756374359131 mm for frame 10

Saving results

Total time: 42.36311078071594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00751631
Iteration 2/25 | Loss: 0.00158101
Iteration 3/25 | Loss: 0.00153147
Iteration 4/25 | Loss: 0.00145112
Iteration 5/25 | Loss: 0.00142566
Iteration 6/25 | Loss: 0.00135044
Iteration 7/25 | Loss: 0.00136245
Iteration 8/25 | Loss: 0.00136673
Iteration 9/25 | Loss: 0.00134055
Iteration 10/25 | Loss: 0.00133462
Iteration 11/25 | Loss: 0.00133413
Iteration 12/25 | Loss: 0.00133403
Iteration 13/25 | Loss: 0.00133402
Iteration 14/25 | Loss: 0.00133402
Iteration 15/25 | Loss: 0.00133402
Iteration 16/25 | Loss: 0.00133402
Iteration 17/25 | Loss: 0.00133402
Iteration 18/25 | Loss: 0.00133402
Iteration 19/25 | Loss: 0.00133401
Iteration 20/25 | Loss: 0.00133401
Iteration 21/25 | Loss: 0.00133401
Iteration 22/25 | Loss: 0.00133401
Iteration 23/25 | Loss: 0.00133401
Iteration 24/25 | Loss: 0.00133401
Iteration 25/25 | Loss: 0.00133401

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.72117281
Iteration 2/25 | Loss: 0.00164126
Iteration 3/25 | Loss: 0.00164125
Iteration 4/25 | Loss: 0.00164125
Iteration 5/25 | Loss: 0.00164125
Iteration 6/25 | Loss: 0.00164125
Iteration 7/25 | Loss: 0.00164125
Iteration 8/25 | Loss: 0.00164125
Iteration 9/25 | Loss: 0.00164125
Iteration 10/25 | Loss: 0.00164125
Iteration 11/25 | Loss: 0.00164125
Iteration 12/25 | Loss: 0.00164125
Iteration 13/25 | Loss: 0.00164125
Iteration 14/25 | Loss: 0.00164125
Iteration 15/25 | Loss: 0.00164125
Iteration 16/25 | Loss: 0.00164125
Iteration 17/25 | Loss: 0.00164125
Iteration 18/25 | Loss: 0.00164125
Iteration 19/25 | Loss: 0.00164125
Iteration 20/25 | Loss: 0.00164125
Iteration 21/25 | Loss: 0.00164125
Iteration 22/25 | Loss: 0.00164125
Iteration 23/25 | Loss: 0.00164125
Iteration 24/25 | Loss: 0.00164125
Iteration 25/25 | Loss: 0.00164125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164125
Iteration 2/1000 | Loss: 0.00004595
Iteration 3/1000 | Loss: 0.00003101
Iteration 4/1000 | Loss: 0.00162164
Iteration 5/1000 | Loss: 0.00176357
Iteration 6/1000 | Loss: 0.00002613
Iteration 7/1000 | Loss: 0.00002391
Iteration 8/1000 | Loss: 0.00002179
Iteration 9/1000 | Loss: 0.00002072
Iteration 10/1000 | Loss: 0.00160295
Iteration 11/1000 | Loss: 0.00003049
Iteration 12/1000 | Loss: 0.00002056
Iteration 13/1000 | Loss: 0.00001957
Iteration 14/1000 | Loss: 0.00001913
Iteration 15/1000 | Loss: 0.00001881
Iteration 16/1000 | Loss: 0.00001852
Iteration 17/1000 | Loss: 0.00001852
Iteration 18/1000 | Loss: 0.00001834
Iteration 19/1000 | Loss: 0.00001817
Iteration 20/1000 | Loss: 0.00001814
Iteration 21/1000 | Loss: 0.00001803
Iteration 22/1000 | Loss: 0.00001800
Iteration 23/1000 | Loss: 0.00001799
Iteration 24/1000 | Loss: 0.00001799
Iteration 25/1000 | Loss: 0.00001798
Iteration 26/1000 | Loss: 0.00001797
Iteration 27/1000 | Loss: 0.00001797
Iteration 28/1000 | Loss: 0.00001796
Iteration 29/1000 | Loss: 0.00001796
Iteration 30/1000 | Loss: 0.00001796
Iteration 31/1000 | Loss: 0.00001796
Iteration 32/1000 | Loss: 0.00001795
Iteration 33/1000 | Loss: 0.00001795
Iteration 34/1000 | Loss: 0.00001794
Iteration 35/1000 | Loss: 0.00001794
Iteration 36/1000 | Loss: 0.00001793
Iteration 37/1000 | Loss: 0.00001792
Iteration 38/1000 | Loss: 0.00001792
Iteration 39/1000 | Loss: 0.00001791
Iteration 40/1000 | Loss: 0.00001790
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001785
Iteration 44/1000 | Loss: 0.00001784
Iteration 45/1000 | Loss: 0.00001784
Iteration 46/1000 | Loss: 0.00001783
Iteration 47/1000 | Loss: 0.00001783
Iteration 48/1000 | Loss: 0.00001780
Iteration 49/1000 | Loss: 0.00001780
Iteration 50/1000 | Loss: 0.00001778
Iteration 51/1000 | Loss: 0.00001778
Iteration 52/1000 | Loss: 0.00001775
Iteration 53/1000 | Loss: 0.00001775
Iteration 54/1000 | Loss: 0.00001774
Iteration 55/1000 | Loss: 0.00001773
Iteration 56/1000 | Loss: 0.00001773
Iteration 57/1000 | Loss: 0.00001772
Iteration 58/1000 | Loss: 0.00001772
Iteration 59/1000 | Loss: 0.00001772
Iteration 60/1000 | Loss: 0.00001772
Iteration 61/1000 | Loss: 0.00001772
Iteration 62/1000 | Loss: 0.00001772
Iteration 63/1000 | Loss: 0.00001771
Iteration 64/1000 | Loss: 0.00001771
Iteration 65/1000 | Loss: 0.00001771
Iteration 66/1000 | Loss: 0.00001771
Iteration 67/1000 | Loss: 0.00001771
Iteration 68/1000 | Loss: 0.00001771
Iteration 69/1000 | Loss: 0.00001771
Iteration 70/1000 | Loss: 0.00001771
Iteration 71/1000 | Loss: 0.00001771
Iteration 72/1000 | Loss: 0.00001771
Iteration 73/1000 | Loss: 0.00001771
Iteration 74/1000 | Loss: 0.00001771
Iteration 75/1000 | Loss: 0.00001771
Iteration 76/1000 | Loss: 0.00001771
Iteration 77/1000 | Loss: 0.00001770
Iteration 78/1000 | Loss: 0.00001770
Iteration 79/1000 | Loss: 0.00001770
Iteration 80/1000 | Loss: 0.00001770
Iteration 81/1000 | Loss: 0.00001770
Iteration 82/1000 | Loss: 0.00001770
Iteration 83/1000 | Loss: 0.00001770
Iteration 84/1000 | Loss: 0.00001770
Iteration 85/1000 | Loss: 0.00001770
Iteration 86/1000 | Loss: 0.00001770
Iteration 87/1000 | Loss: 0.00001769
Iteration 88/1000 | Loss: 0.00001769
Iteration 89/1000 | Loss: 0.00001769
Iteration 90/1000 | Loss: 0.00001769
Iteration 91/1000 | Loss: 0.00001769
Iteration 92/1000 | Loss: 0.00001769
Iteration 93/1000 | Loss: 0.00001769
Iteration 94/1000 | Loss: 0.00001769
Iteration 95/1000 | Loss: 0.00001769
Iteration 96/1000 | Loss: 0.00001769
Iteration 97/1000 | Loss: 0.00001768
Iteration 98/1000 | Loss: 0.00001768
Iteration 99/1000 | Loss: 0.00001768
Iteration 100/1000 | Loss: 0.00001768
Iteration 101/1000 | Loss: 0.00001768
Iteration 102/1000 | Loss: 0.00001768
Iteration 103/1000 | Loss: 0.00001768
Iteration 104/1000 | Loss: 0.00001768
Iteration 105/1000 | Loss: 0.00001768
Iteration 106/1000 | Loss: 0.00001767
Iteration 107/1000 | Loss: 0.00001767
Iteration 108/1000 | Loss: 0.00001767
Iteration 109/1000 | Loss: 0.00001767
Iteration 110/1000 | Loss: 0.00001766
Iteration 111/1000 | Loss: 0.00001766
Iteration 112/1000 | Loss: 0.00001766
Iteration 113/1000 | Loss: 0.00001766
Iteration 114/1000 | Loss: 0.00001765
Iteration 115/1000 | Loss: 0.00001765
Iteration 116/1000 | Loss: 0.00001765
Iteration 117/1000 | Loss: 0.00001764
Iteration 118/1000 | Loss: 0.00001764
Iteration 119/1000 | Loss: 0.00001764
Iteration 120/1000 | Loss: 0.00001764
Iteration 121/1000 | Loss: 0.00001764
Iteration 122/1000 | Loss: 0.00001764
Iteration 123/1000 | Loss: 0.00001764
Iteration 124/1000 | Loss: 0.00001763
Iteration 125/1000 | Loss: 0.00001763
Iteration 126/1000 | Loss: 0.00001763
Iteration 127/1000 | Loss: 0.00001763
Iteration 128/1000 | Loss: 0.00001763
Iteration 129/1000 | Loss: 0.00001763
Iteration 130/1000 | Loss: 0.00001763
Iteration 131/1000 | Loss: 0.00001763
Iteration 132/1000 | Loss: 0.00001763
Iteration 133/1000 | Loss: 0.00001763
Iteration 134/1000 | Loss: 0.00001762
Iteration 135/1000 | Loss: 0.00001762
Iteration 136/1000 | Loss: 0.00001762
Iteration 137/1000 | Loss: 0.00001762
Iteration 138/1000 | Loss: 0.00001762
Iteration 139/1000 | Loss: 0.00001762
Iteration 140/1000 | Loss: 0.00001762
Iteration 141/1000 | Loss: 0.00001761
Iteration 142/1000 | Loss: 0.00001761
Iteration 143/1000 | Loss: 0.00001761
Iteration 144/1000 | Loss: 0.00001761
Iteration 145/1000 | Loss: 0.00001761
Iteration 146/1000 | Loss: 0.00001761
Iteration 147/1000 | Loss: 0.00001761
Iteration 148/1000 | Loss: 0.00001761
Iteration 149/1000 | Loss: 0.00001761
Iteration 150/1000 | Loss: 0.00001760
Iteration 151/1000 | Loss: 0.00001760
Iteration 152/1000 | Loss: 0.00001760
Iteration 153/1000 | Loss: 0.00001760
Iteration 154/1000 | Loss: 0.00001760
Iteration 155/1000 | Loss: 0.00001760
Iteration 156/1000 | Loss: 0.00001760
Iteration 157/1000 | Loss: 0.00001760
Iteration 158/1000 | Loss: 0.00001760
Iteration 159/1000 | Loss: 0.00001760
Iteration 160/1000 | Loss: 0.00001760
Iteration 161/1000 | Loss: 0.00001760
Iteration 162/1000 | Loss: 0.00001760
Iteration 163/1000 | Loss: 0.00001759
Iteration 164/1000 | Loss: 0.00001759
Iteration 165/1000 | Loss: 0.00001759
Iteration 166/1000 | Loss: 0.00001759
Iteration 167/1000 | Loss: 0.00001759
Iteration 168/1000 | Loss: 0.00001759
Iteration 169/1000 | Loss: 0.00001759
Iteration 170/1000 | Loss: 0.00001759
Iteration 171/1000 | Loss: 0.00001759
Iteration 172/1000 | Loss: 0.00001759
Iteration 173/1000 | Loss: 0.00001759
Iteration 174/1000 | Loss: 0.00001758
Iteration 175/1000 | Loss: 0.00001758
Iteration 176/1000 | Loss: 0.00001758
Iteration 177/1000 | Loss: 0.00001758
Iteration 178/1000 | Loss: 0.00001758
Iteration 179/1000 | Loss: 0.00001758
Iteration 180/1000 | Loss: 0.00001758
Iteration 181/1000 | Loss: 0.00001758
Iteration 182/1000 | Loss: 0.00001758
Iteration 183/1000 | Loss: 0.00001757
Iteration 184/1000 | Loss: 0.00001757
Iteration 185/1000 | Loss: 0.00001757
Iteration 186/1000 | Loss: 0.00001757
Iteration 187/1000 | Loss: 0.00001757
Iteration 188/1000 | Loss: 0.00001757
Iteration 189/1000 | Loss: 0.00001757
Iteration 190/1000 | Loss: 0.00001757
Iteration 191/1000 | Loss: 0.00001757
Iteration 192/1000 | Loss: 0.00001757
Iteration 193/1000 | Loss: 0.00001757
Iteration 194/1000 | Loss: 0.00001756
Iteration 195/1000 | Loss: 0.00001756
Iteration 196/1000 | Loss: 0.00001756
Iteration 197/1000 | Loss: 0.00001756
Iteration 198/1000 | Loss: 0.00001756
Iteration 199/1000 | Loss: 0.00001756
Iteration 200/1000 | Loss: 0.00001756
Iteration 201/1000 | Loss: 0.00001756
Iteration 202/1000 | Loss: 0.00001756
Iteration 203/1000 | Loss: 0.00001755
Iteration 204/1000 | Loss: 0.00001755
Iteration 205/1000 | Loss: 0.00001755
Iteration 206/1000 | Loss: 0.00001755
Iteration 207/1000 | Loss: 0.00001754
Iteration 208/1000 | Loss: 0.00001754
Iteration 209/1000 | Loss: 0.00001754
Iteration 210/1000 | Loss: 0.00001754
Iteration 211/1000 | Loss: 0.00001754
Iteration 212/1000 | Loss: 0.00001754
Iteration 213/1000 | Loss: 0.00001754
Iteration 214/1000 | Loss: 0.00001753
Iteration 215/1000 | Loss: 0.00001753
Iteration 216/1000 | Loss: 0.00001753
Iteration 217/1000 | Loss: 0.00001753
Iteration 218/1000 | Loss: 0.00001752
Iteration 219/1000 | Loss: 0.00001752
Iteration 220/1000 | Loss: 0.00001752
Iteration 221/1000 | Loss: 0.00001752
Iteration 222/1000 | Loss: 0.00001752
Iteration 223/1000 | Loss: 0.00001752
Iteration 224/1000 | Loss: 0.00001751
Iteration 225/1000 | Loss: 0.00001751
Iteration 226/1000 | Loss: 0.00001751
Iteration 227/1000 | Loss: 0.00001751
Iteration 228/1000 | Loss: 0.00001751
Iteration 229/1000 | Loss: 0.00001751
Iteration 230/1000 | Loss: 0.00001751
Iteration 231/1000 | Loss: 0.00001751
Iteration 232/1000 | Loss: 0.00001751
Iteration 233/1000 | Loss: 0.00001751
Iteration 234/1000 | Loss: 0.00001751
Iteration 235/1000 | Loss: 0.00001751
Iteration 236/1000 | Loss: 0.00001750
Iteration 237/1000 | Loss: 0.00001750
Iteration 238/1000 | Loss: 0.00001750
Iteration 239/1000 | Loss: 0.00001750
Iteration 240/1000 | Loss: 0.00001750
Iteration 241/1000 | Loss: 0.00001750
Iteration 242/1000 | Loss: 0.00001750
Iteration 243/1000 | Loss: 0.00001750
Iteration 244/1000 | Loss: 0.00001750
Iteration 245/1000 | Loss: 0.00001750
Iteration 246/1000 | Loss: 0.00001750
Iteration 247/1000 | Loss: 0.00001750
Iteration 248/1000 | Loss: 0.00001750
Iteration 249/1000 | Loss: 0.00001750
Iteration 250/1000 | Loss: 0.00001750
Iteration 251/1000 | Loss: 0.00001750
Iteration 252/1000 | Loss: 0.00001750
Iteration 253/1000 | Loss: 0.00001750
Iteration 254/1000 | Loss: 0.00001750
Iteration 255/1000 | Loss: 0.00001750
Iteration 256/1000 | Loss: 0.00001750
Iteration 257/1000 | Loss: 0.00001750
Iteration 258/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.7497048247605562e-05, 1.7497048247605562e-05, 1.7497048247605562e-05, 1.7497048247605562e-05, 1.7497048247605562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7497048247605562e-05

Optimization complete. Final v2v error: 3.469820022583008 mm

Highest mean error: 4.3519792556762695 mm for frame 25

Lowest mean error: 2.6829993724823 mm for frame 142

Saving results

Total time: 76.61913681030273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989370
Iteration 2/25 | Loss: 0.00189255
Iteration 3/25 | Loss: 0.00182360
Iteration 4/25 | Loss: 0.00148993
Iteration 5/25 | Loss: 0.00144160
Iteration 6/25 | Loss: 0.00145286
Iteration 7/25 | Loss: 0.00144422
Iteration 8/25 | Loss: 0.00142764
Iteration 9/25 | Loss: 0.00141130
Iteration 10/25 | Loss: 0.00139883
Iteration 11/25 | Loss: 0.00137628
Iteration 12/25 | Loss: 0.00140090
Iteration 13/25 | Loss: 0.00136324
Iteration 14/25 | Loss: 0.00135552
Iteration 15/25 | Loss: 0.00135397
Iteration 16/25 | Loss: 0.00135260
Iteration 17/25 | Loss: 0.00135427
Iteration 18/25 | Loss: 0.00135393
Iteration 19/25 | Loss: 0.00135265
Iteration 20/25 | Loss: 0.00134869
Iteration 21/25 | Loss: 0.00135008
Iteration 22/25 | Loss: 0.00135173
Iteration 23/25 | Loss: 0.00135295
Iteration 24/25 | Loss: 0.00134943
Iteration 25/25 | Loss: 0.00135166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31686354
Iteration 2/25 | Loss: 0.00179728
Iteration 3/25 | Loss: 0.00179727
Iteration 4/25 | Loss: 0.00179727
Iteration 5/25 | Loss: 0.00179727
Iteration 6/25 | Loss: 0.00179727
Iteration 7/25 | Loss: 0.00179727
Iteration 8/25 | Loss: 0.00179727
Iteration 9/25 | Loss: 0.00179727
Iteration 10/25 | Loss: 0.00179727
Iteration 11/25 | Loss: 0.00179727
Iteration 12/25 | Loss: 0.00179727
Iteration 13/25 | Loss: 0.00179727
Iteration 14/25 | Loss: 0.00179727
Iteration 15/25 | Loss: 0.00179727
Iteration 16/25 | Loss: 0.00179727
Iteration 17/25 | Loss: 0.00179727
Iteration 18/25 | Loss: 0.00179727
Iteration 19/25 | Loss: 0.00179727
Iteration 20/25 | Loss: 0.00179727
Iteration 21/25 | Loss: 0.00179727
Iteration 22/25 | Loss: 0.00179727
Iteration 23/25 | Loss: 0.00179727
Iteration 24/25 | Loss: 0.00179727
Iteration 25/25 | Loss: 0.00179727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179727
Iteration 2/1000 | Loss: 0.00320599
Iteration 3/1000 | Loss: 0.00286211
Iteration 4/1000 | Loss: 0.00060503
Iteration 5/1000 | Loss: 0.00222840
Iteration 6/1000 | Loss: 0.00045666
Iteration 7/1000 | Loss: 0.00230961
Iteration 8/1000 | Loss: 0.00011339
Iteration 9/1000 | Loss: 0.00204236
Iteration 10/1000 | Loss: 0.00305025
Iteration 11/1000 | Loss: 0.00045105
Iteration 12/1000 | Loss: 0.00017057
Iteration 13/1000 | Loss: 0.00006406
Iteration 14/1000 | Loss: 0.00068899
Iteration 15/1000 | Loss: 0.00013670
Iteration 16/1000 | Loss: 0.00021690
Iteration 17/1000 | Loss: 0.00257879
Iteration 18/1000 | Loss: 0.00063061
Iteration 19/1000 | Loss: 0.00143181
Iteration 20/1000 | Loss: 0.00199661
Iteration 21/1000 | Loss: 0.00122193
Iteration 22/1000 | Loss: 0.00067120
Iteration 23/1000 | Loss: 0.00097754
Iteration 24/1000 | Loss: 0.00089273
Iteration 25/1000 | Loss: 0.00049066
Iteration 26/1000 | Loss: 0.00026874
Iteration 27/1000 | Loss: 0.00052499
Iteration 28/1000 | Loss: 0.00019564
Iteration 29/1000 | Loss: 0.00105187
Iteration 30/1000 | Loss: 0.00062999
Iteration 31/1000 | Loss: 0.00059913
Iteration 32/1000 | Loss: 0.00050800
Iteration 33/1000 | Loss: 0.00028930
Iteration 34/1000 | Loss: 0.00018335
Iteration 35/1000 | Loss: 0.00030725
Iteration 36/1000 | Loss: 0.00035626
Iteration 37/1000 | Loss: 0.00088917
Iteration 38/1000 | Loss: 0.00076015
Iteration 39/1000 | Loss: 0.00031522
Iteration 40/1000 | Loss: 0.00021619
Iteration 41/1000 | Loss: 0.00053447
Iteration 42/1000 | Loss: 0.00024678
Iteration 43/1000 | Loss: 0.00025977
Iteration 44/1000 | Loss: 0.00023628
Iteration 45/1000 | Loss: 0.00028740
Iteration 46/1000 | Loss: 0.00022859
Iteration 47/1000 | Loss: 0.00024236
Iteration 48/1000 | Loss: 0.00025239
Iteration 49/1000 | Loss: 0.00062734
Iteration 50/1000 | Loss: 0.00028833
Iteration 51/1000 | Loss: 0.00010176
Iteration 52/1000 | Loss: 0.00009619
Iteration 53/1000 | Loss: 0.00011847
Iteration 54/1000 | Loss: 0.00015079
Iteration 55/1000 | Loss: 0.00013318
Iteration 56/1000 | Loss: 0.00014393
Iteration 57/1000 | Loss: 0.00012616
Iteration 58/1000 | Loss: 0.00103738
Iteration 59/1000 | Loss: 0.00020784
Iteration 60/1000 | Loss: 0.00013693
Iteration 61/1000 | Loss: 0.00013862
Iteration 62/1000 | Loss: 0.00010731
Iteration 63/1000 | Loss: 0.00005313
Iteration 64/1000 | Loss: 0.00013444
Iteration 65/1000 | Loss: 0.00017427
Iteration 66/1000 | Loss: 0.00005166
Iteration 67/1000 | Loss: 0.00022002
Iteration 68/1000 | Loss: 0.00013470
Iteration 69/1000 | Loss: 0.00018135
Iteration 70/1000 | Loss: 0.00014832
Iteration 71/1000 | Loss: 0.00045128
Iteration 72/1000 | Loss: 0.00013876
Iteration 73/1000 | Loss: 0.00029532
Iteration 74/1000 | Loss: 0.00012812
Iteration 75/1000 | Loss: 0.00013618
Iteration 76/1000 | Loss: 0.00007603
Iteration 77/1000 | Loss: 0.00015894
Iteration 78/1000 | Loss: 0.00018353
Iteration 79/1000 | Loss: 0.00014999
Iteration 80/1000 | Loss: 0.00014362
Iteration 81/1000 | Loss: 0.00011830
Iteration 82/1000 | Loss: 0.00015128
Iteration 83/1000 | Loss: 0.00004745
Iteration 84/1000 | Loss: 0.00017129
Iteration 85/1000 | Loss: 0.00015922
Iteration 86/1000 | Loss: 0.00015051
Iteration 87/1000 | Loss: 0.00017350
Iteration 88/1000 | Loss: 0.00020180
Iteration 89/1000 | Loss: 0.00021538
Iteration 90/1000 | Loss: 0.00014427
Iteration 91/1000 | Loss: 0.00013328
Iteration 92/1000 | Loss: 0.00052402
Iteration 93/1000 | Loss: 0.00012863
Iteration 94/1000 | Loss: 0.00025904
Iteration 95/1000 | Loss: 0.00011611
Iteration 96/1000 | Loss: 0.00022964
Iteration 97/1000 | Loss: 0.00013763
Iteration 98/1000 | Loss: 0.00004721
Iteration 99/1000 | Loss: 0.00003451
Iteration 100/1000 | Loss: 0.00003374
Iteration 101/1000 | Loss: 0.00003023
Iteration 102/1000 | Loss: 0.00014574
Iteration 103/1000 | Loss: 0.00015263
Iteration 104/1000 | Loss: 0.00003279
Iteration 105/1000 | Loss: 0.00002027
Iteration 106/1000 | Loss: 0.00009588
Iteration 107/1000 | Loss: 0.00015013
Iteration 108/1000 | Loss: 0.00008096
Iteration 109/1000 | Loss: 0.00017500
Iteration 110/1000 | Loss: 0.00014061
Iteration 111/1000 | Loss: 0.00011687
Iteration 112/1000 | Loss: 0.00013317
Iteration 113/1000 | Loss: 0.00023177
Iteration 114/1000 | Loss: 0.00017431
Iteration 115/1000 | Loss: 0.00016071
Iteration 116/1000 | Loss: 0.00067854
Iteration 117/1000 | Loss: 0.00032498
Iteration 118/1000 | Loss: 0.00058942
Iteration 119/1000 | Loss: 0.00024008
Iteration 120/1000 | Loss: 0.00014935
Iteration 121/1000 | Loss: 0.00006211
Iteration 122/1000 | Loss: 0.00014708
Iteration 123/1000 | Loss: 0.00049315
Iteration 124/1000 | Loss: 0.00026837
Iteration 125/1000 | Loss: 0.00042367
Iteration 126/1000 | Loss: 0.00021095
Iteration 127/1000 | Loss: 0.00016242
Iteration 128/1000 | Loss: 0.00016045
Iteration 129/1000 | Loss: 0.00067206
Iteration 130/1000 | Loss: 0.00020160
Iteration 131/1000 | Loss: 0.00021313
Iteration 132/1000 | Loss: 0.00003413
Iteration 133/1000 | Loss: 0.00007695
Iteration 134/1000 | Loss: 0.00002361
Iteration 135/1000 | Loss: 0.00002242
Iteration 136/1000 | Loss: 0.00025402
Iteration 137/1000 | Loss: 0.00002805
Iteration 138/1000 | Loss: 0.00007864
Iteration 139/1000 | Loss: 0.00005616
Iteration 140/1000 | Loss: 0.00010491
Iteration 141/1000 | Loss: 0.00005295
Iteration 142/1000 | Loss: 0.00006407
Iteration 143/1000 | Loss: 0.00007738
Iteration 144/1000 | Loss: 0.00016248
Iteration 145/1000 | Loss: 0.00023457
Iteration 146/1000 | Loss: 0.00006779
Iteration 147/1000 | Loss: 0.00007088
Iteration 148/1000 | Loss: 0.00002377
Iteration 149/1000 | Loss: 0.00007853
Iteration 150/1000 | Loss: 0.00019804
Iteration 151/1000 | Loss: 0.00017969
Iteration 152/1000 | Loss: 0.00014833
Iteration 153/1000 | Loss: 0.00007084
Iteration 154/1000 | Loss: 0.00012762
Iteration 155/1000 | Loss: 0.00005215
Iteration 156/1000 | Loss: 0.00017167
Iteration 157/1000 | Loss: 0.00013358
Iteration 158/1000 | Loss: 0.00021707
Iteration 159/1000 | Loss: 0.00015964
Iteration 160/1000 | Loss: 0.00014474
Iteration 161/1000 | Loss: 0.00012962
Iteration 162/1000 | Loss: 0.00014930
Iteration 163/1000 | Loss: 0.00024311
Iteration 164/1000 | Loss: 0.00040595
Iteration 165/1000 | Loss: 0.00024780
Iteration 166/1000 | Loss: 0.00013436
Iteration 167/1000 | Loss: 0.00015259
Iteration 168/1000 | Loss: 0.00012292
Iteration 169/1000 | Loss: 0.00008297
Iteration 170/1000 | Loss: 0.00016504
Iteration 171/1000 | Loss: 0.00012980
Iteration 172/1000 | Loss: 0.00015447
Iteration 173/1000 | Loss: 0.00011104
Iteration 174/1000 | Loss: 0.00015303
Iteration 175/1000 | Loss: 0.00014031
Iteration 176/1000 | Loss: 0.00002667
Iteration 177/1000 | Loss: 0.00002395
Iteration 178/1000 | Loss: 0.00002256
Iteration 179/1000 | Loss: 0.00002189
Iteration 180/1000 | Loss: 0.00002139
Iteration 181/1000 | Loss: 0.00002063
Iteration 182/1000 | Loss: 0.00001933
Iteration 183/1000 | Loss: 0.00001849
Iteration 184/1000 | Loss: 0.00001803
Iteration 185/1000 | Loss: 0.00001769
Iteration 186/1000 | Loss: 0.00001760
Iteration 187/1000 | Loss: 0.00001759
Iteration 188/1000 | Loss: 0.00001755
Iteration 189/1000 | Loss: 0.00001751
Iteration 190/1000 | Loss: 0.00001750
Iteration 191/1000 | Loss: 0.00001749
Iteration 192/1000 | Loss: 0.00001748
Iteration 193/1000 | Loss: 0.00001748
Iteration 194/1000 | Loss: 0.00001747
Iteration 195/1000 | Loss: 0.00001747
Iteration 196/1000 | Loss: 0.00001746
Iteration 197/1000 | Loss: 0.00001746
Iteration 198/1000 | Loss: 0.00001746
Iteration 199/1000 | Loss: 0.00001746
Iteration 200/1000 | Loss: 0.00001746
Iteration 201/1000 | Loss: 0.00001745
Iteration 202/1000 | Loss: 0.00001745
Iteration 203/1000 | Loss: 0.00001745
Iteration 204/1000 | Loss: 0.00001745
Iteration 205/1000 | Loss: 0.00001744
Iteration 206/1000 | Loss: 0.00001744
Iteration 207/1000 | Loss: 0.00001744
Iteration 208/1000 | Loss: 0.00001744
Iteration 209/1000 | Loss: 0.00001744
Iteration 210/1000 | Loss: 0.00001744
Iteration 211/1000 | Loss: 0.00001744
Iteration 212/1000 | Loss: 0.00001744
Iteration 213/1000 | Loss: 0.00001744
Iteration 214/1000 | Loss: 0.00001744
Iteration 215/1000 | Loss: 0.00001744
Iteration 216/1000 | Loss: 0.00001744
Iteration 217/1000 | Loss: 0.00001743
Iteration 218/1000 | Loss: 0.00001743
Iteration 219/1000 | Loss: 0.00001743
Iteration 220/1000 | Loss: 0.00001743
Iteration 221/1000 | Loss: 0.00001743
Iteration 222/1000 | Loss: 0.00001743
Iteration 223/1000 | Loss: 0.00001743
Iteration 224/1000 | Loss: 0.00001743
Iteration 225/1000 | Loss: 0.00001743
Iteration 226/1000 | Loss: 0.00001743
Iteration 227/1000 | Loss: 0.00001742
Iteration 228/1000 | Loss: 0.00001742
Iteration 229/1000 | Loss: 0.00001742
Iteration 230/1000 | Loss: 0.00001742
Iteration 231/1000 | Loss: 0.00001742
Iteration 232/1000 | Loss: 0.00001742
Iteration 233/1000 | Loss: 0.00001741
Iteration 234/1000 | Loss: 0.00001741
Iteration 235/1000 | Loss: 0.00001741
Iteration 236/1000 | Loss: 0.00001741
Iteration 237/1000 | Loss: 0.00001741
Iteration 238/1000 | Loss: 0.00001741
Iteration 239/1000 | Loss: 0.00001741
Iteration 240/1000 | Loss: 0.00001741
Iteration 241/1000 | Loss: 0.00001740
Iteration 242/1000 | Loss: 0.00001740
Iteration 243/1000 | Loss: 0.00001739
Iteration 244/1000 | Loss: 0.00001739
Iteration 245/1000 | Loss: 0.00001739
Iteration 246/1000 | Loss: 0.00001738
Iteration 247/1000 | Loss: 0.00001738
Iteration 248/1000 | Loss: 0.00001738
Iteration 249/1000 | Loss: 0.00001738
Iteration 250/1000 | Loss: 0.00001738
Iteration 251/1000 | Loss: 0.00001738
Iteration 252/1000 | Loss: 0.00001738
Iteration 253/1000 | Loss: 0.00001737
Iteration 254/1000 | Loss: 0.00001737
Iteration 255/1000 | Loss: 0.00001736
Iteration 256/1000 | Loss: 0.00001736
Iteration 257/1000 | Loss: 0.00001736
Iteration 258/1000 | Loss: 0.00001735
Iteration 259/1000 | Loss: 0.00001735
Iteration 260/1000 | Loss: 0.00001733
Iteration 261/1000 | Loss: 0.00001732
Iteration 262/1000 | Loss: 0.00001731
Iteration 263/1000 | Loss: 0.00001731
Iteration 264/1000 | Loss: 0.00001730
Iteration 265/1000 | Loss: 0.00001730
Iteration 266/1000 | Loss: 0.00001730
Iteration 267/1000 | Loss: 0.00001730
Iteration 268/1000 | Loss: 0.00001730
Iteration 269/1000 | Loss: 0.00001730
Iteration 270/1000 | Loss: 0.00001730
Iteration 271/1000 | Loss: 0.00001730
Iteration 272/1000 | Loss: 0.00001730
Iteration 273/1000 | Loss: 0.00001730
Iteration 274/1000 | Loss: 0.00001729
Iteration 275/1000 | Loss: 0.00001729
Iteration 276/1000 | Loss: 0.00001728
Iteration 277/1000 | Loss: 0.00001728
Iteration 278/1000 | Loss: 0.00001728
Iteration 279/1000 | Loss: 0.00001727
Iteration 280/1000 | Loss: 0.00001727
Iteration 281/1000 | Loss: 0.00001727
Iteration 282/1000 | Loss: 0.00001727
Iteration 283/1000 | Loss: 0.00001727
Iteration 284/1000 | Loss: 0.00001727
Iteration 285/1000 | Loss: 0.00001727
Iteration 286/1000 | Loss: 0.00001726
Iteration 287/1000 | Loss: 0.00001724
Iteration 288/1000 | Loss: 0.00001724
Iteration 289/1000 | Loss: 0.00001724
Iteration 290/1000 | Loss: 0.00001723
Iteration 291/1000 | Loss: 0.00001723
Iteration 292/1000 | Loss: 0.00001722
Iteration 293/1000 | Loss: 0.00001722
Iteration 294/1000 | Loss: 0.00001721
Iteration 295/1000 | Loss: 0.00001721
Iteration 296/1000 | Loss: 0.00001720
Iteration 297/1000 | Loss: 0.00001720
Iteration 298/1000 | Loss: 0.00001719
Iteration 299/1000 | Loss: 0.00001718
Iteration 300/1000 | Loss: 0.00001718
Iteration 301/1000 | Loss: 0.00001717
Iteration 302/1000 | Loss: 0.00001716
Iteration 303/1000 | Loss: 0.00001716
Iteration 304/1000 | Loss: 0.00001715
Iteration 305/1000 | Loss: 0.00001715
Iteration 306/1000 | Loss: 0.00001715
Iteration 307/1000 | Loss: 0.00001715
Iteration 308/1000 | Loss: 0.00001715
Iteration 309/1000 | Loss: 0.00001714
Iteration 310/1000 | Loss: 0.00001714
Iteration 311/1000 | Loss: 0.00001714
Iteration 312/1000 | Loss: 0.00001714
Iteration 313/1000 | Loss: 0.00001714
Iteration 314/1000 | Loss: 0.00001714
Iteration 315/1000 | Loss: 0.00001714
Iteration 316/1000 | Loss: 0.00001714
Iteration 317/1000 | Loss: 0.00001714
Iteration 318/1000 | Loss: 0.00001714
Iteration 319/1000 | Loss: 0.00001714
Iteration 320/1000 | Loss: 0.00001713
Iteration 321/1000 | Loss: 0.00001713
Iteration 322/1000 | Loss: 0.00001713
Iteration 323/1000 | Loss: 0.00001713
Iteration 324/1000 | Loss: 0.00001713
Iteration 325/1000 | Loss: 0.00001713
Iteration 326/1000 | Loss: 0.00001713
Iteration 327/1000 | Loss: 0.00001713
Iteration 328/1000 | Loss: 0.00001713
Iteration 329/1000 | Loss: 0.00001713
Iteration 330/1000 | Loss: 0.00001713
Iteration 331/1000 | Loss: 0.00001713
Iteration 332/1000 | Loss: 0.00001713
Iteration 333/1000 | Loss: 0.00001713
Iteration 334/1000 | Loss: 0.00001713
Iteration 335/1000 | Loss: 0.00001713
Iteration 336/1000 | Loss: 0.00001713
Iteration 337/1000 | Loss: 0.00001713
Iteration 338/1000 | Loss: 0.00001713
Iteration 339/1000 | Loss: 0.00001712
Iteration 340/1000 | Loss: 0.00001712
Iteration 341/1000 | Loss: 0.00001712
Iteration 342/1000 | Loss: 0.00001712
Iteration 343/1000 | Loss: 0.00001712
Iteration 344/1000 | Loss: 0.00001712
Iteration 345/1000 | Loss: 0.00001711
Iteration 346/1000 | Loss: 0.00001711
Iteration 347/1000 | Loss: 0.00001711
Iteration 348/1000 | Loss: 0.00001710
Iteration 349/1000 | Loss: 0.00001710
Iteration 350/1000 | Loss: 0.00001710
Iteration 351/1000 | Loss: 0.00001710
Iteration 352/1000 | Loss: 0.00001709
Iteration 353/1000 | Loss: 0.00001709
Iteration 354/1000 | Loss: 0.00001709
Iteration 355/1000 | Loss: 0.00001709
Iteration 356/1000 | Loss: 0.00001709
Iteration 357/1000 | Loss: 0.00001709
Iteration 358/1000 | Loss: 0.00001708
Iteration 359/1000 | Loss: 0.00001708
Iteration 360/1000 | Loss: 0.00001708
Iteration 361/1000 | Loss: 0.00001708
Iteration 362/1000 | Loss: 0.00001708
Iteration 363/1000 | Loss: 0.00001708
Iteration 364/1000 | Loss: 0.00001708
Iteration 365/1000 | Loss: 0.00001707
Iteration 366/1000 | Loss: 0.00001707
Iteration 367/1000 | Loss: 0.00001707
Iteration 368/1000 | Loss: 0.00001707
Iteration 369/1000 | Loss: 0.00001707
Iteration 370/1000 | Loss: 0.00001707
Iteration 371/1000 | Loss: 0.00001707
Iteration 372/1000 | Loss: 0.00001707
Iteration 373/1000 | Loss: 0.00001706
Iteration 374/1000 | Loss: 0.00001706
Iteration 375/1000 | Loss: 0.00001706
Iteration 376/1000 | Loss: 0.00001706
Iteration 377/1000 | Loss: 0.00001706
Iteration 378/1000 | Loss: 0.00001706
Iteration 379/1000 | Loss: 0.00001706
Iteration 380/1000 | Loss: 0.00001706
Iteration 381/1000 | Loss: 0.00001706
Iteration 382/1000 | Loss: 0.00001706
Iteration 383/1000 | Loss: 0.00001706
Iteration 384/1000 | Loss: 0.00001706
Iteration 385/1000 | Loss: 0.00001706
Iteration 386/1000 | Loss: 0.00001705
Iteration 387/1000 | Loss: 0.00001705
Iteration 388/1000 | Loss: 0.00001705
Iteration 389/1000 | Loss: 0.00001705
Iteration 390/1000 | Loss: 0.00001705
Iteration 391/1000 | Loss: 0.00001705
Iteration 392/1000 | Loss: 0.00001705
Iteration 393/1000 | Loss: 0.00001705
Iteration 394/1000 | Loss: 0.00001704
Iteration 395/1000 | Loss: 0.00001704
Iteration 396/1000 | Loss: 0.00001704
Iteration 397/1000 | Loss: 0.00001704
Iteration 398/1000 | Loss: 0.00001704
Iteration 399/1000 | Loss: 0.00001703
Iteration 400/1000 | Loss: 0.00001703
Iteration 401/1000 | Loss: 0.00001703
Iteration 402/1000 | Loss: 0.00001703
Iteration 403/1000 | Loss: 0.00001703
Iteration 404/1000 | Loss: 0.00001703
Iteration 405/1000 | Loss: 0.00001703
Iteration 406/1000 | Loss: 0.00001703
Iteration 407/1000 | Loss: 0.00001703
Iteration 408/1000 | Loss: 0.00001703
Iteration 409/1000 | Loss: 0.00001703
Iteration 410/1000 | Loss: 0.00001702
Iteration 411/1000 | Loss: 0.00001702
Iteration 412/1000 | Loss: 0.00001702
Iteration 413/1000 | Loss: 0.00001702
Iteration 414/1000 | Loss: 0.00001702
Iteration 415/1000 | Loss: 0.00001702
Iteration 416/1000 | Loss: 0.00001702
Iteration 417/1000 | Loss: 0.00001702
Iteration 418/1000 | Loss: 0.00001701
Iteration 419/1000 | Loss: 0.00001701
Iteration 420/1000 | Loss: 0.00001699
Iteration 421/1000 | Loss: 0.00001699
Iteration 422/1000 | Loss: 0.00001698
Iteration 423/1000 | Loss: 0.00001698
Iteration 424/1000 | Loss: 0.00001698
Iteration 425/1000 | Loss: 0.00001698
Iteration 426/1000 | Loss: 0.00001697
Iteration 427/1000 | Loss: 0.00001697
Iteration 428/1000 | Loss: 0.00001697
Iteration 429/1000 | Loss: 0.00001696
Iteration 430/1000 | Loss: 0.00001696
Iteration 431/1000 | Loss: 0.00001696
Iteration 432/1000 | Loss: 0.00001696
Iteration 433/1000 | Loss: 0.00001695
Iteration 434/1000 | Loss: 0.00001695
Iteration 435/1000 | Loss: 0.00001695
Iteration 436/1000 | Loss: 0.00001695
Iteration 437/1000 | Loss: 0.00001695
Iteration 438/1000 | Loss: 0.00001695
Iteration 439/1000 | Loss: 0.00001694
Iteration 440/1000 | Loss: 0.00001694
Iteration 441/1000 | Loss: 0.00001694
Iteration 442/1000 | Loss: 0.00001693
Iteration 443/1000 | Loss: 0.00001693
Iteration 444/1000 | Loss: 0.00001693
Iteration 445/1000 | Loss: 0.00001693
Iteration 446/1000 | Loss: 0.00001693
Iteration 447/1000 | Loss: 0.00001693
Iteration 448/1000 | Loss: 0.00001693
Iteration 449/1000 | Loss: 0.00001692
Iteration 450/1000 | Loss: 0.00001692
Iteration 451/1000 | Loss: 0.00001692
Iteration 452/1000 | Loss: 0.00001692
Iteration 453/1000 | Loss: 0.00001692
Iteration 454/1000 | Loss: 0.00001691
Iteration 455/1000 | Loss: 0.00001691
Iteration 456/1000 | Loss: 0.00001691
Iteration 457/1000 | Loss: 0.00001691
Iteration 458/1000 | Loss: 0.00001690
Iteration 459/1000 | Loss: 0.00001690
Iteration 460/1000 | Loss: 0.00001690
Iteration 461/1000 | Loss: 0.00001689
Iteration 462/1000 | Loss: 0.00001689
Iteration 463/1000 | Loss: 0.00001689
Iteration 464/1000 | Loss: 0.00001688
Iteration 465/1000 | Loss: 0.00001688
Iteration 466/1000 | Loss: 0.00001688
Iteration 467/1000 | Loss: 0.00001687
Iteration 468/1000 | Loss: 0.00001687
Iteration 469/1000 | Loss: 0.00001687
Iteration 470/1000 | Loss: 0.00001687
Iteration 471/1000 | Loss: 0.00001687
Iteration 472/1000 | Loss: 0.00001687
Iteration 473/1000 | Loss: 0.00001687
Iteration 474/1000 | Loss: 0.00001687
Iteration 475/1000 | Loss: 0.00001687
Iteration 476/1000 | Loss: 0.00001687
Iteration 477/1000 | Loss: 0.00001686
Iteration 478/1000 | Loss: 0.00001686
Iteration 479/1000 | Loss: 0.00001686
Iteration 480/1000 | Loss: 0.00001686
Iteration 481/1000 | Loss: 0.00001686
Iteration 482/1000 | Loss: 0.00001686
Iteration 483/1000 | Loss: 0.00001686
Iteration 484/1000 | Loss: 0.00001686
Iteration 485/1000 | Loss: 0.00001686
Iteration 486/1000 | Loss: 0.00001686
Iteration 487/1000 | Loss: 0.00001686
Iteration 488/1000 | Loss: 0.00001686
Iteration 489/1000 | Loss: 0.00001686
Iteration 490/1000 | Loss: 0.00001686
Iteration 491/1000 | Loss: 0.00001685
Iteration 492/1000 | Loss: 0.00001685
Iteration 493/1000 | Loss: 0.00001685
Iteration 494/1000 | Loss: 0.00001685
Iteration 495/1000 | Loss: 0.00001685
Iteration 496/1000 | Loss: 0.00001685
Iteration 497/1000 | Loss: 0.00001685
Iteration 498/1000 | Loss: 0.00001685
Iteration 499/1000 | Loss: 0.00001685
Iteration 500/1000 | Loss: 0.00001684
Iteration 501/1000 | Loss: 0.00001684
Iteration 502/1000 | Loss: 0.00001684
Iteration 503/1000 | Loss: 0.00001684
Iteration 504/1000 | Loss: 0.00001684
Iteration 505/1000 | Loss: 0.00001684
Iteration 506/1000 | Loss: 0.00001684
Iteration 507/1000 | Loss: 0.00001684
Iteration 508/1000 | Loss: 0.00001684
Iteration 509/1000 | Loss: 0.00001684
Iteration 510/1000 | Loss: 0.00001684
Iteration 511/1000 | Loss: 0.00001683
Iteration 512/1000 | Loss: 0.00001683
Iteration 513/1000 | Loss: 0.00001683
Iteration 514/1000 | Loss: 0.00001683
Iteration 515/1000 | Loss: 0.00001683
Iteration 516/1000 | Loss: 0.00001683
Iteration 517/1000 | Loss: 0.00001683
Iteration 518/1000 | Loss: 0.00001683
Iteration 519/1000 | Loss: 0.00001683
Iteration 520/1000 | Loss: 0.00001683
Iteration 521/1000 | Loss: 0.00001683
Iteration 522/1000 | Loss: 0.00001683
Iteration 523/1000 | Loss: 0.00001683
Iteration 524/1000 | Loss: 0.00001682
Iteration 525/1000 | Loss: 0.00001682
Iteration 526/1000 | Loss: 0.00001682
Iteration 527/1000 | Loss: 0.00001682
Iteration 528/1000 | Loss: 0.00001682
Iteration 529/1000 | Loss: 0.00001682
Iteration 530/1000 | Loss: 0.00001682
Iteration 531/1000 | Loss: 0.00001681
Iteration 532/1000 | Loss: 0.00001681
Iteration 533/1000 | Loss: 0.00001681
Iteration 534/1000 | Loss: 0.00001681
Iteration 535/1000 | Loss: 0.00001681
Iteration 536/1000 | Loss: 0.00001681
Iteration 537/1000 | Loss: 0.00001681
Iteration 538/1000 | Loss: 0.00001681
Iteration 539/1000 | Loss: 0.00001681
Iteration 540/1000 | Loss: 0.00001681
Iteration 541/1000 | Loss: 0.00001681
Iteration 542/1000 | Loss: 0.00001680
Iteration 543/1000 | Loss: 0.00001680
Iteration 544/1000 | Loss: 0.00001680
Iteration 545/1000 | Loss: 0.00001680
Iteration 546/1000 | Loss: 0.00001680
Iteration 547/1000 | Loss: 0.00001680
Iteration 548/1000 | Loss: 0.00001680
Iteration 549/1000 | Loss: 0.00001680
Iteration 550/1000 | Loss: 0.00001680
Iteration 551/1000 | Loss: 0.00001680
Iteration 552/1000 | Loss: 0.00001680
Iteration 553/1000 | Loss: 0.00001680
Iteration 554/1000 | Loss: 0.00001680
Iteration 555/1000 | Loss: 0.00001680
Iteration 556/1000 | Loss: 0.00001680
Iteration 557/1000 | Loss: 0.00001680
Iteration 558/1000 | Loss: 0.00001679
Iteration 559/1000 | Loss: 0.00001679
Iteration 560/1000 | Loss: 0.00001679
Iteration 561/1000 | Loss: 0.00001679
Iteration 562/1000 | Loss: 0.00001679
Iteration 563/1000 | Loss: 0.00001679
Iteration 564/1000 | Loss: 0.00001678
Iteration 565/1000 | Loss: 0.00001678
Iteration 566/1000 | Loss: 0.00001678
Iteration 567/1000 | Loss: 0.00001678
Iteration 568/1000 | Loss: 0.00001678
Iteration 569/1000 | Loss: 0.00001678
Iteration 570/1000 | Loss: 0.00001678
Iteration 571/1000 | Loss: 0.00001677
Iteration 572/1000 | Loss: 0.00001677
Iteration 573/1000 | Loss: 0.00001677
Iteration 574/1000 | Loss: 0.00001677
Iteration 575/1000 | Loss: 0.00001676
Iteration 576/1000 | Loss: 0.00001676
Iteration 577/1000 | Loss: 0.00001676
Iteration 578/1000 | Loss: 0.00001675
Iteration 579/1000 | Loss: 0.00001675
Iteration 580/1000 | Loss: 0.00001675
Iteration 581/1000 | Loss: 0.00001675
Iteration 582/1000 | Loss: 0.00001674
Iteration 583/1000 | Loss: 0.00001674
Iteration 584/1000 | Loss: 0.00001674
Iteration 585/1000 | Loss: 0.00001673
Iteration 586/1000 | Loss: 0.00001673
Iteration 587/1000 | Loss: 0.00001673
Iteration 588/1000 | Loss: 0.00001673
Iteration 589/1000 | Loss: 0.00001672
Iteration 590/1000 | Loss: 0.00001672
Iteration 591/1000 | Loss: 0.00001672
Iteration 592/1000 | Loss: 0.00001672
Iteration 593/1000 | Loss: 0.00001671
Iteration 594/1000 | Loss: 0.00001671
Iteration 595/1000 | Loss: 0.00001671
Iteration 596/1000 | Loss: 0.00001670
Iteration 597/1000 | Loss: 0.00001670
Iteration 598/1000 | Loss: 0.00001670
Iteration 599/1000 | Loss: 0.00001669
Iteration 600/1000 | Loss: 0.00001669
Iteration 601/1000 | Loss: 0.00001669
Iteration 602/1000 | Loss: 0.00001669
Iteration 603/1000 | Loss: 0.00001669
Iteration 604/1000 | Loss: 0.00001669
Iteration 605/1000 | Loss: 0.00001669
Iteration 606/1000 | Loss: 0.00001669
Iteration 607/1000 | Loss: 0.00001668
Iteration 608/1000 | Loss: 0.00001668
Iteration 609/1000 | Loss: 0.00001668
Iteration 610/1000 | Loss: 0.00001668
Iteration 611/1000 | Loss: 0.00001667
Iteration 612/1000 | Loss: 0.00001667
Iteration 613/1000 | Loss: 0.00001667
Iteration 614/1000 | Loss: 0.00001667
Iteration 615/1000 | Loss: 0.00001667
Iteration 616/1000 | Loss: 0.00001667
Iteration 617/1000 | Loss: 0.00001667
Iteration 618/1000 | Loss: 0.00001667
Iteration 619/1000 | Loss: 0.00001667
Iteration 620/1000 | Loss: 0.00001666
Iteration 621/1000 | Loss: 0.00001666
Iteration 622/1000 | Loss: 0.00001666
Iteration 623/1000 | Loss: 0.00001666
Iteration 624/1000 | Loss: 0.00001666
Iteration 625/1000 | Loss: 0.00001665
Iteration 626/1000 | Loss: 0.00001665
Iteration 627/1000 | Loss: 0.00001665
Iteration 628/1000 | Loss: 0.00001665
Iteration 629/1000 | Loss: 0.00001665
Iteration 630/1000 | Loss: 0.00001665
Iteration 631/1000 | Loss: 0.00001665
Iteration 632/1000 | Loss: 0.00001664
Iteration 633/1000 | Loss: 0.00001664
Iteration 634/1000 | Loss: 0.00001664
Iteration 635/1000 | Loss: 0.00001663
Iteration 636/1000 | Loss: 0.00001663
Iteration 637/1000 | Loss: 0.00001663
Iteration 638/1000 | Loss: 0.00001662
Iteration 639/1000 | Loss: 0.00001662
Iteration 640/1000 | Loss: 0.00001662
Iteration 641/1000 | Loss: 0.00001661
Iteration 642/1000 | Loss: 0.00001661
Iteration 643/1000 | Loss: 0.00001661
Iteration 644/1000 | Loss: 0.00001660
Iteration 645/1000 | Loss: 0.00001660
Iteration 646/1000 | Loss: 0.00043632
Iteration 647/1000 | Loss: 0.00001800
Iteration 648/1000 | Loss: 0.00001590
Iteration 649/1000 | Loss: 0.00001536
Iteration 650/1000 | Loss: 0.00001485
Iteration 651/1000 | Loss: 0.00001440
Iteration 652/1000 | Loss: 0.00001410
Iteration 653/1000 | Loss: 0.00001390
Iteration 654/1000 | Loss: 0.00001389
Iteration 655/1000 | Loss: 0.00001389
Iteration 656/1000 | Loss: 0.00001388
Iteration 657/1000 | Loss: 0.00001387
Iteration 658/1000 | Loss: 0.00001387
Iteration 659/1000 | Loss: 0.00001386
Iteration 660/1000 | Loss: 0.00001386
Iteration 661/1000 | Loss: 0.00001385
Iteration 662/1000 | Loss: 0.00001384
Iteration 663/1000 | Loss: 0.00001382
Iteration 664/1000 | Loss: 0.00001382
Iteration 665/1000 | Loss: 0.00001381
Iteration 666/1000 | Loss: 0.00001381
Iteration 667/1000 | Loss: 0.00001378
Iteration 668/1000 | Loss: 0.00001378
Iteration 669/1000 | Loss: 0.00001377
Iteration 670/1000 | Loss: 0.00001377
Iteration 671/1000 | Loss: 0.00001376
Iteration 672/1000 | Loss: 0.00001376
Iteration 673/1000 | Loss: 0.00001376
Iteration 674/1000 | Loss: 0.00001375
Iteration 675/1000 | Loss: 0.00001375
Iteration 676/1000 | Loss: 0.00001374
Iteration 677/1000 | Loss: 0.00001374
Iteration 678/1000 | Loss: 0.00001374
Iteration 679/1000 | Loss: 0.00001373
Iteration 680/1000 | Loss: 0.00001373
Iteration 681/1000 | Loss: 0.00001373
Iteration 682/1000 | Loss: 0.00001372
Iteration 683/1000 | Loss: 0.00001372
Iteration 684/1000 | Loss: 0.00001372
Iteration 685/1000 | Loss: 0.00001371
Iteration 686/1000 | Loss: 0.00001371
Iteration 687/1000 | Loss: 0.00001371
Iteration 688/1000 | Loss: 0.00001371
Iteration 689/1000 | Loss: 0.00001370
Iteration 690/1000 | Loss: 0.00001369
Iteration 691/1000 | Loss: 0.00001369
Iteration 692/1000 | Loss: 0.00001369
Iteration 693/1000 | Loss: 0.00001368
Iteration 694/1000 | Loss: 0.00001368
Iteration 695/1000 | Loss: 0.00001368
Iteration 696/1000 | Loss: 0.00001368
Iteration 697/1000 | Loss: 0.00001368
Iteration 698/1000 | Loss: 0.00001368
Iteration 699/1000 | Loss: 0.00001367
Iteration 700/1000 | Loss: 0.00001367
Iteration 701/1000 | Loss: 0.00001367
Iteration 702/1000 | Loss: 0.00001366
Iteration 703/1000 | Loss: 0.00001366
Iteration 704/1000 | Loss: 0.00001366
Iteration 705/1000 | Loss: 0.00001366
Iteration 706/1000 | Loss: 0.00001366
Iteration 707/1000 | Loss: 0.00001366
Iteration 708/1000 | Loss: 0.00001365
Iteration 709/1000 | Loss: 0.00001365
Iteration 710/1000 | Loss: 0.00001365
Iteration 711/1000 | Loss: 0.00001365
Iteration 712/1000 | Loss: 0.00001365
Iteration 713/1000 | Loss: 0.00001364
Iteration 714/1000 | Loss: 0.00001364
Iteration 715/1000 | Loss: 0.00001364
Iteration 716/1000 | Loss: 0.00001364
Iteration 717/1000 | Loss: 0.00001364
Iteration 718/1000 | Loss: 0.00001364
Iteration 719/1000 | Loss: 0.00001363
Iteration 720/1000 | Loss: 0.00001363
Iteration 721/1000 | Loss: 0.00001363
Iteration 722/1000 | Loss: 0.00001363
Iteration 723/1000 | Loss: 0.00001363
Iteration 724/1000 | Loss: 0.00001363
Iteration 725/1000 | Loss: 0.00001363
Iteration 726/1000 | Loss: 0.00001363
Iteration 727/1000 | Loss: 0.00001363
Iteration 728/1000 | Loss: 0.00001363
Iteration 729/1000 | Loss: 0.00001362
Iteration 730/1000 | Loss: 0.00001362
Iteration 731/1000 | Loss: 0.00001362
Iteration 732/1000 | Loss: 0.00001362
Iteration 733/1000 | Loss: 0.00001362
Iteration 734/1000 | Loss: 0.00001361
Iteration 735/1000 | Loss: 0.00001361
Iteration 736/1000 | Loss: 0.00001361
Iteration 737/1000 | Loss: 0.00001361
Iteration 738/1000 | Loss: 0.00001361
Iteration 739/1000 | Loss: 0.00001361
Iteration 740/1000 | Loss: 0.00001361
Iteration 741/1000 | Loss: 0.00001360
Iteration 742/1000 | Loss: 0.00001360
Iteration 743/1000 | Loss: 0.00001360
Iteration 744/1000 | Loss: 0.00001360
Iteration 745/1000 | Loss: 0.00001360
Iteration 746/1000 | Loss: 0.00001360
Iteration 747/1000 | Loss: 0.00001360
Iteration 748/1000 | Loss: 0.00001360
Iteration 749/1000 | Loss: 0.00001360
Iteration 750/1000 | Loss: 0.00001360
Iteration 751/1000 | Loss: 0.00001360
Iteration 752/1000 | Loss: 0.00001360
Iteration 753/1000 | Loss: 0.00001360
Iteration 754/1000 | Loss: 0.00001360
Iteration 755/1000 | Loss: 0.00001360
Iteration 756/1000 | Loss: 0.00001360
Iteration 757/1000 | Loss: 0.00001360
Iteration 758/1000 | Loss: 0.00001360
Iteration 759/1000 | Loss: 0.00001360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 759. Stopping optimization.
Last 5 losses: [1.3598260011349339e-05, 1.3598260011349339e-05, 1.3598260011349339e-05, 1.3598260011349339e-05, 1.3598260011349339e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3598260011349339e-05

Optimization complete. Final v2v error: 3.1285364627838135 mm

Highest mean error: 4.9411797523498535 mm for frame 70

Lowest mean error: 2.8022992610931396 mm for frame 27

Saving results

Total time: 364.0996332168579
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406040
Iteration 2/25 | Loss: 0.00139769
Iteration 3/25 | Loss: 0.00128406
Iteration 4/25 | Loss: 0.00127033
Iteration 5/25 | Loss: 0.00126788
Iteration 6/25 | Loss: 0.00126779
Iteration 7/25 | Loss: 0.00126779
Iteration 8/25 | Loss: 0.00126779
Iteration 9/25 | Loss: 0.00126779
Iteration 10/25 | Loss: 0.00126779
Iteration 11/25 | Loss: 0.00126779
Iteration 12/25 | Loss: 0.00126779
Iteration 13/25 | Loss: 0.00126777
Iteration 14/25 | Loss: 0.00126777
Iteration 15/25 | Loss: 0.00126777
Iteration 16/25 | Loss: 0.00126777
Iteration 17/25 | Loss: 0.00126777
Iteration 18/25 | Loss: 0.00126777
Iteration 19/25 | Loss: 0.00126777
Iteration 20/25 | Loss: 0.00126777
Iteration 21/25 | Loss: 0.00126777
Iteration 22/25 | Loss: 0.00126777
Iteration 23/25 | Loss: 0.00126777
Iteration 24/25 | Loss: 0.00126777
Iteration 25/25 | Loss: 0.00126777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29465318
Iteration 2/25 | Loss: 0.00122139
Iteration 3/25 | Loss: 0.00122139
Iteration 4/25 | Loss: 0.00122139
Iteration 5/25 | Loss: 0.00122139
Iteration 6/25 | Loss: 0.00122139
Iteration 7/25 | Loss: 0.00122139
Iteration 8/25 | Loss: 0.00122139
Iteration 9/25 | Loss: 0.00122139
Iteration 10/25 | Loss: 0.00122139
Iteration 11/25 | Loss: 0.00122139
Iteration 12/25 | Loss: 0.00122139
Iteration 13/25 | Loss: 0.00122139
Iteration 14/25 | Loss: 0.00122139
Iteration 15/25 | Loss: 0.00122139
Iteration 16/25 | Loss: 0.00122139
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012213882291689515, 0.0012213882291689515, 0.0012213882291689515, 0.0012213882291689515, 0.0012213882291689515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012213882291689515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122139
Iteration 2/1000 | Loss: 0.00002578
Iteration 3/1000 | Loss: 0.00001921
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001617
Iteration 6/1000 | Loss: 0.00001534
Iteration 7/1000 | Loss: 0.00001489
Iteration 8/1000 | Loss: 0.00001450
Iteration 9/1000 | Loss: 0.00001411
Iteration 10/1000 | Loss: 0.00001384
Iteration 11/1000 | Loss: 0.00001364
Iteration 12/1000 | Loss: 0.00001346
Iteration 13/1000 | Loss: 0.00001342
Iteration 14/1000 | Loss: 0.00001340
Iteration 15/1000 | Loss: 0.00001336
Iteration 16/1000 | Loss: 0.00001335
Iteration 17/1000 | Loss: 0.00001333
Iteration 18/1000 | Loss: 0.00001332
Iteration 19/1000 | Loss: 0.00001328
Iteration 20/1000 | Loss: 0.00001326
Iteration 21/1000 | Loss: 0.00001325
Iteration 22/1000 | Loss: 0.00001322
Iteration 23/1000 | Loss: 0.00001319
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001318
Iteration 26/1000 | Loss: 0.00001317
Iteration 27/1000 | Loss: 0.00001311
Iteration 28/1000 | Loss: 0.00001308
Iteration 29/1000 | Loss: 0.00001308
Iteration 30/1000 | Loss: 0.00001305
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001294
Iteration 33/1000 | Loss: 0.00001289
Iteration 34/1000 | Loss: 0.00001285
Iteration 35/1000 | Loss: 0.00001284
Iteration 36/1000 | Loss: 0.00001284
Iteration 37/1000 | Loss: 0.00001283
Iteration 38/1000 | Loss: 0.00001283
Iteration 39/1000 | Loss: 0.00001282
Iteration 40/1000 | Loss: 0.00001279
Iteration 41/1000 | Loss: 0.00001279
Iteration 42/1000 | Loss: 0.00001279
Iteration 43/1000 | Loss: 0.00001279
Iteration 44/1000 | Loss: 0.00001279
Iteration 45/1000 | Loss: 0.00001279
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001278
Iteration 48/1000 | Loss: 0.00001278
Iteration 49/1000 | Loss: 0.00001278
Iteration 50/1000 | Loss: 0.00001278
Iteration 51/1000 | Loss: 0.00001278
Iteration 52/1000 | Loss: 0.00001276
Iteration 53/1000 | Loss: 0.00001275
Iteration 54/1000 | Loss: 0.00001275
Iteration 55/1000 | Loss: 0.00001274
Iteration 56/1000 | Loss: 0.00001271
Iteration 57/1000 | Loss: 0.00001271
Iteration 58/1000 | Loss: 0.00001271
Iteration 59/1000 | Loss: 0.00001271
Iteration 60/1000 | Loss: 0.00001271
Iteration 61/1000 | Loss: 0.00001271
Iteration 62/1000 | Loss: 0.00001271
Iteration 63/1000 | Loss: 0.00001271
Iteration 64/1000 | Loss: 0.00001271
Iteration 65/1000 | Loss: 0.00001271
Iteration 66/1000 | Loss: 0.00001271
Iteration 67/1000 | Loss: 0.00001270
Iteration 68/1000 | Loss: 0.00001270
Iteration 69/1000 | Loss: 0.00001270
Iteration 70/1000 | Loss: 0.00001270
Iteration 71/1000 | Loss: 0.00001269
Iteration 72/1000 | Loss: 0.00001268
Iteration 73/1000 | Loss: 0.00001268
Iteration 74/1000 | Loss: 0.00001267
Iteration 75/1000 | Loss: 0.00001267
Iteration 76/1000 | Loss: 0.00001267
Iteration 77/1000 | Loss: 0.00001267
Iteration 78/1000 | Loss: 0.00001266
Iteration 79/1000 | Loss: 0.00001266
Iteration 80/1000 | Loss: 0.00001266
Iteration 81/1000 | Loss: 0.00001266
Iteration 82/1000 | Loss: 0.00001266
Iteration 83/1000 | Loss: 0.00001265
Iteration 84/1000 | Loss: 0.00001265
Iteration 85/1000 | Loss: 0.00001265
Iteration 86/1000 | Loss: 0.00001265
Iteration 87/1000 | Loss: 0.00001265
Iteration 88/1000 | Loss: 0.00001265
Iteration 89/1000 | Loss: 0.00001265
Iteration 90/1000 | Loss: 0.00001265
Iteration 91/1000 | Loss: 0.00001264
Iteration 92/1000 | Loss: 0.00001264
Iteration 93/1000 | Loss: 0.00001264
Iteration 94/1000 | Loss: 0.00001264
Iteration 95/1000 | Loss: 0.00001264
Iteration 96/1000 | Loss: 0.00001264
Iteration 97/1000 | Loss: 0.00001263
Iteration 98/1000 | Loss: 0.00001262
Iteration 99/1000 | Loss: 0.00001262
Iteration 100/1000 | Loss: 0.00001262
Iteration 101/1000 | Loss: 0.00001262
Iteration 102/1000 | Loss: 0.00001262
Iteration 103/1000 | Loss: 0.00001262
Iteration 104/1000 | Loss: 0.00001261
Iteration 105/1000 | Loss: 0.00001261
Iteration 106/1000 | Loss: 0.00001261
Iteration 107/1000 | Loss: 0.00001261
Iteration 108/1000 | Loss: 0.00001260
Iteration 109/1000 | Loss: 0.00001260
Iteration 110/1000 | Loss: 0.00001260
Iteration 111/1000 | Loss: 0.00001259
Iteration 112/1000 | Loss: 0.00001259
Iteration 113/1000 | Loss: 0.00001259
Iteration 114/1000 | Loss: 0.00001259
Iteration 115/1000 | Loss: 0.00001258
Iteration 116/1000 | Loss: 0.00001258
Iteration 117/1000 | Loss: 0.00001258
Iteration 118/1000 | Loss: 0.00001258
Iteration 119/1000 | Loss: 0.00001258
Iteration 120/1000 | Loss: 0.00001258
Iteration 121/1000 | Loss: 0.00001258
Iteration 122/1000 | Loss: 0.00001257
Iteration 123/1000 | Loss: 0.00001257
Iteration 124/1000 | Loss: 0.00001257
Iteration 125/1000 | Loss: 0.00001257
Iteration 126/1000 | Loss: 0.00001256
Iteration 127/1000 | Loss: 0.00001256
Iteration 128/1000 | Loss: 0.00001256
Iteration 129/1000 | Loss: 0.00001256
Iteration 130/1000 | Loss: 0.00001255
Iteration 131/1000 | Loss: 0.00001255
Iteration 132/1000 | Loss: 0.00001255
Iteration 133/1000 | Loss: 0.00001255
Iteration 134/1000 | Loss: 0.00001255
Iteration 135/1000 | Loss: 0.00001254
Iteration 136/1000 | Loss: 0.00001254
Iteration 137/1000 | Loss: 0.00001254
Iteration 138/1000 | Loss: 0.00001254
Iteration 139/1000 | Loss: 0.00001254
Iteration 140/1000 | Loss: 0.00001254
Iteration 141/1000 | Loss: 0.00001254
Iteration 142/1000 | Loss: 0.00001254
Iteration 143/1000 | Loss: 0.00001254
Iteration 144/1000 | Loss: 0.00001254
Iteration 145/1000 | Loss: 0.00001254
Iteration 146/1000 | Loss: 0.00001254
Iteration 147/1000 | Loss: 0.00001254
Iteration 148/1000 | Loss: 0.00001254
Iteration 149/1000 | Loss: 0.00001254
Iteration 150/1000 | Loss: 0.00001254
Iteration 151/1000 | Loss: 0.00001254
Iteration 152/1000 | Loss: 0.00001253
Iteration 153/1000 | Loss: 0.00001253
Iteration 154/1000 | Loss: 0.00001253
Iteration 155/1000 | Loss: 0.00001253
Iteration 156/1000 | Loss: 0.00001253
Iteration 157/1000 | Loss: 0.00001252
Iteration 158/1000 | Loss: 0.00001252
Iteration 159/1000 | Loss: 0.00001252
Iteration 160/1000 | Loss: 0.00001252
Iteration 161/1000 | Loss: 0.00001252
Iteration 162/1000 | Loss: 0.00001252
Iteration 163/1000 | Loss: 0.00001252
Iteration 164/1000 | Loss: 0.00001252
Iteration 165/1000 | Loss: 0.00001252
Iteration 166/1000 | Loss: 0.00001252
Iteration 167/1000 | Loss: 0.00001252
Iteration 168/1000 | Loss: 0.00001252
Iteration 169/1000 | Loss: 0.00001252
Iteration 170/1000 | Loss: 0.00001252
Iteration 171/1000 | Loss: 0.00001252
Iteration 172/1000 | Loss: 0.00001252
Iteration 173/1000 | Loss: 0.00001252
Iteration 174/1000 | Loss: 0.00001252
Iteration 175/1000 | Loss: 0.00001252
Iteration 176/1000 | Loss: 0.00001252
Iteration 177/1000 | Loss: 0.00001252
Iteration 178/1000 | Loss: 0.00001252
Iteration 179/1000 | Loss: 0.00001252
Iteration 180/1000 | Loss: 0.00001252
Iteration 181/1000 | Loss: 0.00001252
Iteration 182/1000 | Loss: 0.00001252
Iteration 183/1000 | Loss: 0.00001252
Iteration 184/1000 | Loss: 0.00001252
Iteration 185/1000 | Loss: 0.00001252
Iteration 186/1000 | Loss: 0.00001252
Iteration 187/1000 | Loss: 0.00001252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.2521080861915834e-05, 1.2521080861915834e-05, 1.2521080861915834e-05, 1.2521080861915834e-05, 1.2521080861915834e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2521080861915834e-05

Optimization complete. Final v2v error: 3.0362555980682373 mm

Highest mean error: 3.281590223312378 mm for frame 120

Lowest mean error: 2.8059325218200684 mm for frame 15

Saving results

Total time: 43.38729453086853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964409
Iteration 2/25 | Loss: 0.00384164
Iteration 3/25 | Loss: 0.00226225
Iteration 4/25 | Loss: 0.00204244
Iteration 5/25 | Loss: 0.00191982
Iteration 6/25 | Loss: 0.00190211
Iteration 7/25 | Loss: 0.00185643
Iteration 8/25 | Loss: 0.00184776
Iteration 9/25 | Loss: 0.00183444
Iteration 10/25 | Loss: 0.00182424
Iteration 11/25 | Loss: 0.00182363
Iteration 12/25 | Loss: 0.00181382
Iteration 13/25 | Loss: 0.00182191
Iteration 14/25 | Loss: 0.00180535
Iteration 15/25 | Loss: 0.00180497
Iteration 16/25 | Loss: 0.00180230
Iteration 17/25 | Loss: 0.00180104
Iteration 18/25 | Loss: 0.00179238
Iteration 19/25 | Loss: 0.00179582
Iteration 20/25 | Loss: 0.00186354
Iteration 21/25 | Loss: 0.00179849
Iteration 22/25 | Loss: 0.00170731
Iteration 23/25 | Loss: 0.00166625
Iteration 24/25 | Loss: 0.00164847
Iteration 25/25 | Loss: 0.00164811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28214622
Iteration 2/25 | Loss: 0.00528673
Iteration 3/25 | Loss: 0.00514839
Iteration 4/25 | Loss: 0.00514839
Iteration 5/25 | Loss: 0.00514839
Iteration 6/25 | Loss: 0.00514838
Iteration 7/25 | Loss: 0.00514838
Iteration 8/25 | Loss: 0.00514838
Iteration 9/25 | Loss: 0.00515191
Iteration 10/25 | Loss: 0.00515190
Iteration 11/25 | Loss: 0.00515190
Iteration 12/25 | Loss: 0.00515190
Iteration 13/25 | Loss: 0.00515190
Iteration 14/25 | Loss: 0.00515190
Iteration 15/25 | Loss: 0.00515190
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005151904653757811, 0.005151904653757811, 0.005151904653757811, 0.005151904653757811, 0.005151904653757811]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005151904653757811

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00515190
Iteration 2/1000 | Loss: 0.00116551
Iteration 3/1000 | Loss: 0.00075119
Iteration 4/1000 | Loss: 0.00049823
Iteration 5/1000 | Loss: 0.00051982
Iteration 6/1000 | Loss: 0.00071072
Iteration 7/1000 | Loss: 0.00099663
Iteration 8/1000 | Loss: 0.00075642
Iteration 9/1000 | Loss: 0.00056264
Iteration 10/1000 | Loss: 0.00071049
Iteration 11/1000 | Loss: 0.00100850
Iteration 12/1000 | Loss: 0.00054390
Iteration 13/1000 | Loss: 0.00031086
Iteration 14/1000 | Loss: 0.00104088
Iteration 15/1000 | Loss: 0.00070493
Iteration 16/1000 | Loss: 0.00075008
Iteration 17/1000 | Loss: 0.00041638
Iteration 18/1000 | Loss: 0.00034078
Iteration 19/1000 | Loss: 0.00052967
Iteration 20/1000 | Loss: 0.00050409
Iteration 21/1000 | Loss: 0.00033593
Iteration 22/1000 | Loss: 0.00107697
Iteration 23/1000 | Loss: 0.00094423
Iteration 24/1000 | Loss: 0.00096668
Iteration 25/1000 | Loss: 0.00055414
Iteration 26/1000 | Loss: 0.00049183
Iteration 27/1000 | Loss: 0.00079945
Iteration 28/1000 | Loss: 0.00073860
Iteration 29/1000 | Loss: 0.00024220
Iteration 30/1000 | Loss: 0.00080781
Iteration 31/1000 | Loss: 0.00043383
Iteration 32/1000 | Loss: 0.00041887
Iteration 33/1000 | Loss: 0.00027623
Iteration 34/1000 | Loss: 0.00070324
Iteration 35/1000 | Loss: 0.00034042
Iteration 36/1000 | Loss: 0.00058803
Iteration 37/1000 | Loss: 0.00038093
Iteration 38/1000 | Loss: 0.00031160
Iteration 39/1000 | Loss: 0.00019211
Iteration 40/1000 | Loss: 0.00042156
Iteration 41/1000 | Loss: 0.00120451
Iteration 42/1000 | Loss: 0.00036650
Iteration 43/1000 | Loss: 0.00059936
Iteration 44/1000 | Loss: 0.00019815
Iteration 45/1000 | Loss: 0.00020340
Iteration 46/1000 | Loss: 0.00083558
Iteration 47/1000 | Loss: 0.00026938
Iteration 48/1000 | Loss: 0.00040838
Iteration 49/1000 | Loss: 0.00018370
Iteration 50/1000 | Loss: 0.00073576
Iteration 51/1000 | Loss: 0.00081757
Iteration 52/1000 | Loss: 0.00016608
Iteration 53/1000 | Loss: 0.00017366
Iteration 54/1000 | Loss: 0.00045233
Iteration 55/1000 | Loss: 0.00035620
Iteration 56/1000 | Loss: 0.00019049
Iteration 57/1000 | Loss: 0.00053036
Iteration 58/1000 | Loss: 0.00024442
Iteration 59/1000 | Loss: 0.00014772
Iteration 60/1000 | Loss: 0.00038396
Iteration 61/1000 | Loss: 0.00014107
Iteration 62/1000 | Loss: 0.00014429
Iteration 63/1000 | Loss: 0.00014120
Iteration 64/1000 | Loss: 0.00014956
Iteration 65/1000 | Loss: 0.00014456
Iteration 66/1000 | Loss: 0.00014321
Iteration 67/1000 | Loss: 0.00046727
Iteration 68/1000 | Loss: 0.00025958
Iteration 69/1000 | Loss: 0.00016177
Iteration 70/1000 | Loss: 0.00014320
Iteration 71/1000 | Loss: 0.00019488
Iteration 72/1000 | Loss: 0.00014477
Iteration 73/1000 | Loss: 0.00013340
Iteration 74/1000 | Loss: 0.00013824
Iteration 75/1000 | Loss: 0.00014036
Iteration 76/1000 | Loss: 0.00013662
Iteration 77/1000 | Loss: 0.00013874
Iteration 78/1000 | Loss: 0.00015589
Iteration 79/1000 | Loss: 0.00013213
Iteration 80/1000 | Loss: 0.00025158
Iteration 81/1000 | Loss: 0.00013671
Iteration 82/1000 | Loss: 0.00013598
Iteration 83/1000 | Loss: 0.00019080
Iteration 84/1000 | Loss: 0.00013698
Iteration 85/1000 | Loss: 0.00013481
Iteration 86/1000 | Loss: 0.00013711
Iteration 87/1000 | Loss: 0.00012627
Iteration 88/1000 | Loss: 0.00012323
Iteration 89/1000 | Loss: 0.00026012
Iteration 90/1000 | Loss: 0.00031720
Iteration 91/1000 | Loss: 0.00014953
Iteration 92/1000 | Loss: 0.00013419
Iteration 93/1000 | Loss: 0.00014061
Iteration 94/1000 | Loss: 0.00012774
Iteration 95/1000 | Loss: 0.00011716
Iteration 96/1000 | Loss: 0.00011415
Iteration 97/1000 | Loss: 0.00011588
Iteration 98/1000 | Loss: 0.00011171
Iteration 99/1000 | Loss: 0.00025238
Iteration 100/1000 | Loss: 0.00011719
Iteration 101/1000 | Loss: 0.00011351
Iteration 102/1000 | Loss: 0.00011119
Iteration 103/1000 | Loss: 0.00011514
Iteration 104/1000 | Loss: 0.00052753
Iteration 105/1000 | Loss: 0.00034832
Iteration 106/1000 | Loss: 0.00036288
Iteration 107/1000 | Loss: 0.00012931
Iteration 108/1000 | Loss: 0.00012667
Iteration 109/1000 | Loss: 0.00011166
Iteration 110/1000 | Loss: 0.00015289
Iteration 111/1000 | Loss: 0.00030595
Iteration 112/1000 | Loss: 0.00029239
Iteration 113/1000 | Loss: 0.00026021
Iteration 114/1000 | Loss: 0.00013825
Iteration 115/1000 | Loss: 0.00010753
Iteration 116/1000 | Loss: 0.00010708
Iteration 117/1000 | Loss: 0.00010669
Iteration 118/1000 | Loss: 0.00010605
Iteration 119/1000 | Loss: 0.00010898
Iteration 120/1000 | Loss: 0.00010609
Iteration 121/1000 | Loss: 0.00010756
Iteration 122/1000 | Loss: 0.00011393
Iteration 123/1000 | Loss: 0.00010412
Iteration 124/1000 | Loss: 0.00013573
Iteration 125/1000 | Loss: 0.00010342
Iteration 126/1000 | Loss: 0.00012661
Iteration 127/1000 | Loss: 0.00010290
Iteration 128/1000 | Loss: 0.00010293
Iteration 129/1000 | Loss: 0.00026206
Iteration 130/1000 | Loss: 0.00015151
Iteration 131/1000 | Loss: 0.00010456
Iteration 132/1000 | Loss: 0.00010291
Iteration 133/1000 | Loss: 0.00010501
Iteration 134/1000 | Loss: 0.00010214
Iteration 135/1000 | Loss: 0.00010595
Iteration 136/1000 | Loss: 0.00013232
Iteration 137/1000 | Loss: 0.00029123
Iteration 138/1000 | Loss: 0.00010649
Iteration 139/1000 | Loss: 0.00010224
Iteration 140/1000 | Loss: 0.00010395
Iteration 141/1000 | Loss: 0.00009979
Iteration 142/1000 | Loss: 0.00010523
Iteration 143/1000 | Loss: 0.00013471
Iteration 144/1000 | Loss: 0.00013258
Iteration 145/1000 | Loss: 0.00010119
Iteration 146/1000 | Loss: 0.00009813
Iteration 147/1000 | Loss: 0.00009809
Iteration 148/1000 | Loss: 0.00009807
Iteration 149/1000 | Loss: 0.00010803
Iteration 150/1000 | Loss: 0.00009775
Iteration 151/1000 | Loss: 0.00009996
Iteration 152/1000 | Loss: 0.00011728
Iteration 153/1000 | Loss: 0.00009740
Iteration 154/1000 | Loss: 0.00053089
Iteration 155/1000 | Loss: 0.00087138
Iteration 156/1000 | Loss: 0.00071748
Iteration 157/1000 | Loss: 0.00156970
Iteration 158/1000 | Loss: 0.00131147
Iteration 159/1000 | Loss: 0.00065877
Iteration 160/1000 | Loss: 0.00038017
Iteration 161/1000 | Loss: 0.00013119
Iteration 162/1000 | Loss: 0.00011661
Iteration 163/1000 | Loss: 0.00013252
Iteration 164/1000 | Loss: 0.00012732
Iteration 165/1000 | Loss: 0.00011189
Iteration 166/1000 | Loss: 0.00010125
Iteration 167/1000 | Loss: 0.00014142
Iteration 168/1000 | Loss: 0.00009279
Iteration 169/1000 | Loss: 0.00012387
Iteration 170/1000 | Loss: 0.00008932
Iteration 171/1000 | Loss: 0.00008927
Iteration 172/1000 | Loss: 0.00010213
Iteration 173/1000 | Loss: 0.00008758
Iteration 174/1000 | Loss: 0.00008888
Iteration 175/1000 | Loss: 0.00009457
Iteration 176/1000 | Loss: 0.00008632
Iteration 177/1000 | Loss: 0.00009003
Iteration 178/1000 | Loss: 0.00008776
Iteration 179/1000 | Loss: 0.00067949
Iteration 180/1000 | Loss: 0.00179983
Iteration 181/1000 | Loss: 0.00053572
Iteration 182/1000 | Loss: 0.00097426
Iteration 183/1000 | Loss: 0.00083818
Iteration 184/1000 | Loss: 0.00074862
Iteration 185/1000 | Loss: 0.00021837
Iteration 186/1000 | Loss: 0.00012020
Iteration 187/1000 | Loss: 0.00009839
Iteration 188/1000 | Loss: 0.00015632
Iteration 189/1000 | Loss: 0.00010144
Iteration 190/1000 | Loss: 0.00013720
Iteration 191/1000 | Loss: 0.00008746
Iteration 192/1000 | Loss: 0.00019157
Iteration 193/1000 | Loss: 0.00013493
Iteration 194/1000 | Loss: 0.00012836
Iteration 195/1000 | Loss: 0.00018587
Iteration 196/1000 | Loss: 0.00008850
Iteration 197/1000 | Loss: 0.00009971
Iteration 198/1000 | Loss: 0.00008735
Iteration 199/1000 | Loss: 0.00008415
Iteration 200/1000 | Loss: 0.00008295
Iteration 201/1000 | Loss: 0.00015875
Iteration 202/1000 | Loss: 0.00008768
Iteration 203/1000 | Loss: 0.00008739
Iteration 204/1000 | Loss: 0.00009089
Iteration 205/1000 | Loss: 0.00035958
Iteration 206/1000 | Loss: 0.00062071
Iteration 207/1000 | Loss: 0.00039768
Iteration 208/1000 | Loss: 0.00039503
Iteration 209/1000 | Loss: 0.00029493
Iteration 210/1000 | Loss: 0.00015929
Iteration 211/1000 | Loss: 0.00009805
Iteration 212/1000 | Loss: 0.00013521
Iteration 213/1000 | Loss: 0.00008183
Iteration 214/1000 | Loss: 0.00013440
Iteration 215/1000 | Loss: 0.00008508
Iteration 216/1000 | Loss: 0.00008679
Iteration 217/1000 | Loss: 0.00007775
Iteration 218/1000 | Loss: 0.00008479
Iteration 219/1000 | Loss: 0.00034565
Iteration 220/1000 | Loss: 0.00015506
Iteration 221/1000 | Loss: 0.00038295
Iteration 222/1000 | Loss: 0.00017368
Iteration 223/1000 | Loss: 0.00048310
Iteration 224/1000 | Loss: 0.00013703
Iteration 225/1000 | Loss: 0.00028578
Iteration 226/1000 | Loss: 0.00010904
Iteration 227/1000 | Loss: 0.00015654
Iteration 228/1000 | Loss: 0.00019344
Iteration 229/1000 | Loss: 0.00008434
Iteration 230/1000 | Loss: 0.00036806
Iteration 231/1000 | Loss: 0.00017284
Iteration 232/1000 | Loss: 0.00024252
Iteration 233/1000 | Loss: 0.00015591
Iteration 234/1000 | Loss: 0.00022456
Iteration 235/1000 | Loss: 0.00007903
Iteration 236/1000 | Loss: 0.00009574
Iteration 237/1000 | Loss: 0.00007708
Iteration 238/1000 | Loss: 0.00033400
Iteration 239/1000 | Loss: 0.00013647
Iteration 240/1000 | Loss: 0.00022723
Iteration 241/1000 | Loss: 0.00007672
Iteration 242/1000 | Loss: 0.00007635
Iteration 243/1000 | Loss: 0.00007344
Iteration 244/1000 | Loss: 0.00007458
Iteration 245/1000 | Loss: 0.00007166
Iteration 246/1000 | Loss: 0.00007187
Iteration 247/1000 | Loss: 0.00007263
Iteration 248/1000 | Loss: 0.00007350
Iteration 249/1000 | Loss: 0.00007113
Iteration 250/1000 | Loss: 0.00007077
Iteration 251/1000 | Loss: 0.00007076
Iteration 252/1000 | Loss: 0.00007076
Iteration 253/1000 | Loss: 0.00007076
Iteration 254/1000 | Loss: 0.00007076
Iteration 255/1000 | Loss: 0.00007076
Iteration 256/1000 | Loss: 0.00007076
Iteration 257/1000 | Loss: 0.00007076
Iteration 258/1000 | Loss: 0.00007076
Iteration 259/1000 | Loss: 0.00007076
Iteration 260/1000 | Loss: 0.00007075
Iteration 261/1000 | Loss: 0.00007133
Iteration 262/1000 | Loss: 0.00007055
Iteration 263/1000 | Loss: 0.00007055
Iteration 264/1000 | Loss: 0.00007055
Iteration 265/1000 | Loss: 0.00007055
Iteration 266/1000 | Loss: 0.00007055
Iteration 267/1000 | Loss: 0.00007055
Iteration 268/1000 | Loss: 0.00007055
Iteration 269/1000 | Loss: 0.00007054
Iteration 270/1000 | Loss: 0.00007054
Iteration 271/1000 | Loss: 0.00007054
Iteration 272/1000 | Loss: 0.00007054
Iteration 273/1000 | Loss: 0.00007054
Iteration 274/1000 | Loss: 0.00007054
Iteration 275/1000 | Loss: 0.00007054
Iteration 276/1000 | Loss: 0.00007054
Iteration 277/1000 | Loss: 0.00007054
Iteration 278/1000 | Loss: 0.00007054
Iteration 279/1000 | Loss: 0.00007072
Iteration 280/1000 | Loss: 0.00007072
Iteration 281/1000 | Loss: 0.00008596
Iteration 282/1000 | Loss: 0.00007042
Iteration 283/1000 | Loss: 0.00007042
Iteration 284/1000 | Loss: 0.00007041
Iteration 285/1000 | Loss: 0.00007041
Iteration 286/1000 | Loss: 0.00007041
Iteration 287/1000 | Loss: 0.00007041
Iteration 288/1000 | Loss: 0.00007041
Iteration 289/1000 | Loss: 0.00007041
Iteration 290/1000 | Loss: 0.00007041
Iteration 291/1000 | Loss: 0.00007041
Iteration 292/1000 | Loss: 0.00007041
Iteration 293/1000 | Loss: 0.00007041
Iteration 294/1000 | Loss: 0.00007038
Iteration 295/1000 | Loss: 0.00007035
Iteration 296/1000 | Loss: 0.00007035
Iteration 297/1000 | Loss: 0.00007034
Iteration 298/1000 | Loss: 0.00007034
Iteration 299/1000 | Loss: 0.00007043
Iteration 300/1000 | Loss: 0.00007033
Iteration 301/1000 | Loss: 0.00007033
Iteration 302/1000 | Loss: 0.00007032
Iteration 303/1000 | Loss: 0.00007032
Iteration 304/1000 | Loss: 0.00007032
Iteration 305/1000 | Loss: 0.00007032
Iteration 306/1000 | Loss: 0.00007029
Iteration 307/1000 | Loss: 0.00007029
Iteration 308/1000 | Loss: 0.00007029
Iteration 309/1000 | Loss: 0.00007029
Iteration 310/1000 | Loss: 0.00007029
Iteration 311/1000 | Loss: 0.00007029
Iteration 312/1000 | Loss: 0.00007029
Iteration 313/1000 | Loss: 0.00007029
Iteration 314/1000 | Loss: 0.00007029
Iteration 315/1000 | Loss: 0.00007029
Iteration 316/1000 | Loss: 0.00007028
Iteration 317/1000 | Loss: 0.00007028
Iteration 318/1000 | Loss: 0.00007028
Iteration 319/1000 | Loss: 0.00007028
Iteration 320/1000 | Loss: 0.00007027
Iteration 321/1000 | Loss: 0.00007027
Iteration 322/1000 | Loss: 0.00007182
Iteration 323/1000 | Loss: 0.00007129
Iteration 324/1000 | Loss: 0.00007022
Iteration 325/1000 | Loss: 0.00007029
Iteration 326/1000 | Loss: 0.00007028
Iteration 327/1000 | Loss: 0.00007027
Iteration 328/1000 | Loss: 0.00007018
Iteration 329/1000 | Loss: 0.00007018
Iteration 330/1000 | Loss: 0.00007017
Iteration 331/1000 | Loss: 0.00007017
Iteration 332/1000 | Loss: 0.00007017
Iteration 333/1000 | Loss: 0.00007017
Iteration 334/1000 | Loss: 0.00007017
Iteration 335/1000 | Loss: 0.00007017
Iteration 336/1000 | Loss: 0.00007016
Iteration 337/1000 | Loss: 0.00007016
Iteration 338/1000 | Loss: 0.00007016
Iteration 339/1000 | Loss: 0.00007016
Iteration 340/1000 | Loss: 0.00007016
Iteration 341/1000 | Loss: 0.00007016
Iteration 342/1000 | Loss: 0.00007016
Iteration 343/1000 | Loss: 0.00007016
Iteration 344/1000 | Loss: 0.00007016
Iteration 345/1000 | Loss: 0.00007016
Iteration 346/1000 | Loss: 0.00007016
Iteration 347/1000 | Loss: 0.00007016
Iteration 348/1000 | Loss: 0.00007016
Iteration 349/1000 | Loss: 0.00007016
Iteration 350/1000 | Loss: 0.00007016
Iteration 351/1000 | Loss: 0.00007016
Iteration 352/1000 | Loss: 0.00007016
Iteration 353/1000 | Loss: 0.00007016
Iteration 354/1000 | Loss: 0.00007016
Iteration 355/1000 | Loss: 0.00007016
Iteration 356/1000 | Loss: 0.00007016
Iteration 357/1000 | Loss: 0.00007016
Iteration 358/1000 | Loss: 0.00007016
Iteration 359/1000 | Loss: 0.00007016
Iteration 360/1000 | Loss: 0.00007016
Iteration 361/1000 | Loss: 0.00007016
Iteration 362/1000 | Loss: 0.00007016
Iteration 363/1000 | Loss: 0.00007016
Iteration 364/1000 | Loss: 0.00007016
Iteration 365/1000 | Loss: 0.00007016
Iteration 366/1000 | Loss: 0.00007016
Iteration 367/1000 | Loss: 0.00007016
Iteration 368/1000 | Loss: 0.00007016
Iteration 369/1000 | Loss: 0.00007016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 369. Stopping optimization.
Last 5 losses: [7.015646406216547e-05, 7.015646406216547e-05, 7.015646406216547e-05, 7.015646406216547e-05, 7.015646406216547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.015646406216547e-05

Optimization complete. Final v2v error: 4.455601215362549 mm

Highest mean error: 11.065325736999512 mm for frame 15

Lowest mean error: 2.988384962081909 mm for frame 100

Saving results

Total time: 472.9758360385895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858812
Iteration 2/25 | Loss: 0.00180808
Iteration 3/25 | Loss: 0.00148767
Iteration 4/25 | Loss: 0.00145820
Iteration 5/25 | Loss: 0.00141416
Iteration 6/25 | Loss: 0.00138035
Iteration 7/25 | Loss: 0.00136874
Iteration 8/25 | Loss: 0.00136180
Iteration 9/25 | Loss: 0.00135999
Iteration 10/25 | Loss: 0.00136254
Iteration 11/25 | Loss: 0.00136121
Iteration 12/25 | Loss: 0.00136047
Iteration 13/25 | Loss: 0.00136323
Iteration 14/25 | Loss: 0.00136110
Iteration 15/25 | Loss: 0.00135779
Iteration 16/25 | Loss: 0.00135767
Iteration 17/25 | Loss: 0.00135767
Iteration 18/25 | Loss: 0.00135767
Iteration 19/25 | Loss: 0.00135767
Iteration 20/25 | Loss: 0.00135767
Iteration 21/25 | Loss: 0.00135766
Iteration 22/25 | Loss: 0.00135766
Iteration 23/25 | Loss: 0.00135766
Iteration 24/25 | Loss: 0.00135766
Iteration 25/25 | Loss: 0.00135766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47508502
Iteration 2/25 | Loss: 0.00150820
Iteration 3/25 | Loss: 0.00150820
Iteration 4/25 | Loss: 0.00150820
Iteration 5/25 | Loss: 0.00150820
Iteration 6/25 | Loss: 0.00150820
Iteration 7/25 | Loss: 0.00150820
Iteration 8/25 | Loss: 0.00150820
Iteration 9/25 | Loss: 0.00150820
Iteration 10/25 | Loss: 0.00150820
Iteration 11/25 | Loss: 0.00150820
Iteration 12/25 | Loss: 0.00150820
Iteration 13/25 | Loss: 0.00150820
Iteration 14/25 | Loss: 0.00150820
Iteration 15/25 | Loss: 0.00150820
Iteration 16/25 | Loss: 0.00150820
Iteration 17/25 | Loss: 0.00150820
Iteration 18/25 | Loss: 0.00150820
Iteration 19/25 | Loss: 0.00150820
Iteration 20/25 | Loss: 0.00150820
Iteration 21/25 | Loss: 0.00150820
Iteration 22/25 | Loss: 0.00150820
Iteration 23/25 | Loss: 0.00150820
Iteration 24/25 | Loss: 0.00150820
Iteration 25/25 | Loss: 0.00150820

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150820
Iteration 2/1000 | Loss: 0.00005606
Iteration 3/1000 | Loss: 0.00003850
Iteration 4/1000 | Loss: 0.00003350
Iteration 5/1000 | Loss: 0.00003108
Iteration 6/1000 | Loss: 0.00002942
Iteration 7/1000 | Loss: 0.00002838
Iteration 8/1000 | Loss: 0.00002776
Iteration 9/1000 | Loss: 0.00002714
Iteration 10/1000 | Loss: 0.00002668
Iteration 11/1000 | Loss: 0.00016542
Iteration 12/1000 | Loss: 0.00003036
Iteration 13/1000 | Loss: 0.00005119
Iteration 14/1000 | Loss: 0.00002657
Iteration 15/1000 | Loss: 0.00003805
Iteration 16/1000 | Loss: 0.00004119
Iteration 17/1000 | Loss: 0.00002511
Iteration 18/1000 | Loss: 0.00003894
Iteration 19/1000 | Loss: 0.00002579
Iteration 20/1000 | Loss: 0.00002457
Iteration 21/1000 | Loss: 0.00002456
Iteration 22/1000 | Loss: 0.00002455
Iteration 23/1000 | Loss: 0.00002522
Iteration 24/1000 | Loss: 0.00002444
Iteration 25/1000 | Loss: 0.00002443
Iteration 26/1000 | Loss: 0.00002443
Iteration 27/1000 | Loss: 0.00002443
Iteration 28/1000 | Loss: 0.00002443
Iteration 29/1000 | Loss: 0.00002440
Iteration 30/1000 | Loss: 0.00002434
Iteration 31/1000 | Loss: 0.00002434
Iteration 32/1000 | Loss: 0.00002427
Iteration 33/1000 | Loss: 0.00002426
Iteration 34/1000 | Loss: 0.00002424
Iteration 35/1000 | Loss: 0.00002423
Iteration 36/1000 | Loss: 0.00002422
Iteration 37/1000 | Loss: 0.00002421
Iteration 38/1000 | Loss: 0.00002420
Iteration 39/1000 | Loss: 0.00002420
Iteration 40/1000 | Loss: 0.00002419
Iteration 41/1000 | Loss: 0.00002418
Iteration 42/1000 | Loss: 0.00002417
Iteration 43/1000 | Loss: 0.00002417
Iteration 44/1000 | Loss: 0.00002417
Iteration 45/1000 | Loss: 0.00002415
Iteration 46/1000 | Loss: 0.00002415
Iteration 47/1000 | Loss: 0.00002415
Iteration 48/1000 | Loss: 0.00002415
Iteration 49/1000 | Loss: 0.00002415
Iteration 50/1000 | Loss: 0.00002415
Iteration 51/1000 | Loss: 0.00002415
Iteration 52/1000 | Loss: 0.00002414
Iteration 53/1000 | Loss: 0.00005328
Iteration 54/1000 | Loss: 0.00002439
Iteration 55/1000 | Loss: 0.00002410
Iteration 56/1000 | Loss: 0.00002409
Iteration 57/1000 | Loss: 0.00002408
Iteration 58/1000 | Loss: 0.00002408
Iteration 59/1000 | Loss: 0.00002407
Iteration 60/1000 | Loss: 0.00002407
Iteration 61/1000 | Loss: 0.00002407
Iteration 62/1000 | Loss: 0.00002407
Iteration 63/1000 | Loss: 0.00002407
Iteration 64/1000 | Loss: 0.00002407
Iteration 65/1000 | Loss: 0.00002407
Iteration 66/1000 | Loss: 0.00002407
Iteration 67/1000 | Loss: 0.00002406
Iteration 68/1000 | Loss: 0.00002406
Iteration 69/1000 | Loss: 0.00002406
Iteration 70/1000 | Loss: 0.00002406
Iteration 71/1000 | Loss: 0.00002406
Iteration 72/1000 | Loss: 0.00002405
Iteration 73/1000 | Loss: 0.00002405
Iteration 74/1000 | Loss: 0.00002405
Iteration 75/1000 | Loss: 0.00002405
Iteration 76/1000 | Loss: 0.00002405
Iteration 77/1000 | Loss: 0.00002405
Iteration 78/1000 | Loss: 0.00002404
Iteration 79/1000 | Loss: 0.00002404
Iteration 80/1000 | Loss: 0.00002404
Iteration 81/1000 | Loss: 0.00002404
Iteration 82/1000 | Loss: 0.00002404
Iteration 83/1000 | Loss: 0.00002404
Iteration 84/1000 | Loss: 0.00002404
Iteration 85/1000 | Loss: 0.00002404
Iteration 86/1000 | Loss: 0.00002404
Iteration 87/1000 | Loss: 0.00002404
Iteration 88/1000 | Loss: 0.00002404
Iteration 89/1000 | Loss: 0.00002404
Iteration 90/1000 | Loss: 0.00002404
Iteration 91/1000 | Loss: 0.00002404
Iteration 92/1000 | Loss: 0.00002404
Iteration 93/1000 | Loss: 0.00002404
Iteration 94/1000 | Loss: 0.00002404
Iteration 95/1000 | Loss: 0.00002404
Iteration 96/1000 | Loss: 0.00002404
Iteration 97/1000 | Loss: 0.00002404
Iteration 98/1000 | Loss: 0.00002404
Iteration 99/1000 | Loss: 0.00002404
Iteration 100/1000 | Loss: 0.00002404
Iteration 101/1000 | Loss: 0.00002404
Iteration 102/1000 | Loss: 0.00002404
Iteration 103/1000 | Loss: 0.00002404
Iteration 104/1000 | Loss: 0.00002404
Iteration 105/1000 | Loss: 0.00002404
Iteration 106/1000 | Loss: 0.00002404
Iteration 107/1000 | Loss: 0.00002404
Iteration 108/1000 | Loss: 0.00002404
Iteration 109/1000 | Loss: 0.00002404
Iteration 110/1000 | Loss: 0.00002404
Iteration 111/1000 | Loss: 0.00002404
Iteration 112/1000 | Loss: 0.00002404
Iteration 113/1000 | Loss: 0.00002404
Iteration 114/1000 | Loss: 0.00002404
Iteration 115/1000 | Loss: 0.00002404
Iteration 116/1000 | Loss: 0.00002404
Iteration 117/1000 | Loss: 0.00002404
Iteration 118/1000 | Loss: 0.00002404
Iteration 119/1000 | Loss: 0.00002404
Iteration 120/1000 | Loss: 0.00002404
Iteration 121/1000 | Loss: 0.00002404
Iteration 122/1000 | Loss: 0.00002404
Iteration 123/1000 | Loss: 0.00002404
Iteration 124/1000 | Loss: 0.00002404
Iteration 125/1000 | Loss: 0.00002404
Iteration 126/1000 | Loss: 0.00002404
Iteration 127/1000 | Loss: 0.00002404
Iteration 128/1000 | Loss: 0.00002404
Iteration 129/1000 | Loss: 0.00002404
Iteration 130/1000 | Loss: 0.00002404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.4038234187173657e-05, 2.4038234187173657e-05, 2.4038234187173657e-05, 2.4038234187173657e-05, 2.4038234187173657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4038234187173657e-05

Optimization complete. Final v2v error: 4.128397464752197 mm

Highest mean error: 5.66476583480835 mm for frame 53

Lowest mean error: 3.4237310886383057 mm for frame 150

Saving results

Total time: 77.5726089477539
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437631
Iteration 2/25 | Loss: 0.00148581
Iteration 3/25 | Loss: 0.00133958
Iteration 4/25 | Loss: 0.00132325
Iteration 5/25 | Loss: 0.00131990
Iteration 6/25 | Loss: 0.00131863
Iteration 7/25 | Loss: 0.00131837
Iteration 8/25 | Loss: 0.00131837
Iteration 9/25 | Loss: 0.00131837
Iteration 10/25 | Loss: 0.00131837
Iteration 11/25 | Loss: 0.00131837
Iteration 12/25 | Loss: 0.00131837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013183669652789831, 0.0013183669652789831, 0.0013183669652789831, 0.0013183669652789831, 0.0013183669652789831]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013183669652789831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34245527
Iteration 2/25 | Loss: 0.00166405
Iteration 3/25 | Loss: 0.00166404
Iteration 4/25 | Loss: 0.00166404
Iteration 5/25 | Loss: 0.00166404
Iteration 6/25 | Loss: 0.00166404
Iteration 7/25 | Loss: 0.00166403
Iteration 8/25 | Loss: 0.00166403
Iteration 9/25 | Loss: 0.00166403
Iteration 10/25 | Loss: 0.00166403
Iteration 11/25 | Loss: 0.00166403
Iteration 12/25 | Loss: 0.00166403
Iteration 13/25 | Loss: 0.00166403
Iteration 14/25 | Loss: 0.00166403
Iteration 15/25 | Loss: 0.00166403
Iteration 16/25 | Loss: 0.00166403
Iteration 17/25 | Loss: 0.00166403
Iteration 18/25 | Loss: 0.00166403
Iteration 19/25 | Loss: 0.00166403
Iteration 20/25 | Loss: 0.00166403
Iteration 21/25 | Loss: 0.00166403
Iteration 22/25 | Loss: 0.00166403
Iteration 23/25 | Loss: 0.00166403
Iteration 24/25 | Loss: 0.00166403
Iteration 25/25 | Loss: 0.00166403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166403
Iteration 2/1000 | Loss: 0.00004368
Iteration 3/1000 | Loss: 0.00002731
Iteration 4/1000 | Loss: 0.00002112
Iteration 5/1000 | Loss: 0.00001913
Iteration 6/1000 | Loss: 0.00001810
Iteration 7/1000 | Loss: 0.00001754
Iteration 8/1000 | Loss: 0.00001707
Iteration 9/1000 | Loss: 0.00001673
Iteration 10/1000 | Loss: 0.00001650
Iteration 11/1000 | Loss: 0.00001631
Iteration 12/1000 | Loss: 0.00001627
Iteration 13/1000 | Loss: 0.00001611
Iteration 14/1000 | Loss: 0.00001607
Iteration 15/1000 | Loss: 0.00001598
Iteration 16/1000 | Loss: 0.00001597
Iteration 17/1000 | Loss: 0.00001587
Iteration 18/1000 | Loss: 0.00001586
Iteration 19/1000 | Loss: 0.00001579
Iteration 20/1000 | Loss: 0.00001579
Iteration 21/1000 | Loss: 0.00001577
Iteration 22/1000 | Loss: 0.00001576
Iteration 23/1000 | Loss: 0.00001571
Iteration 24/1000 | Loss: 0.00001571
Iteration 25/1000 | Loss: 0.00001571
Iteration 26/1000 | Loss: 0.00001570
Iteration 27/1000 | Loss: 0.00001569
Iteration 28/1000 | Loss: 0.00001567
Iteration 29/1000 | Loss: 0.00001567
Iteration 30/1000 | Loss: 0.00001567
Iteration 31/1000 | Loss: 0.00001567
Iteration 32/1000 | Loss: 0.00001567
Iteration 33/1000 | Loss: 0.00001567
Iteration 34/1000 | Loss: 0.00001567
Iteration 35/1000 | Loss: 0.00001567
Iteration 36/1000 | Loss: 0.00001567
Iteration 37/1000 | Loss: 0.00001567
Iteration 38/1000 | Loss: 0.00001566
Iteration 39/1000 | Loss: 0.00001566
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001566
Iteration 42/1000 | Loss: 0.00001566
Iteration 43/1000 | Loss: 0.00001565
Iteration 44/1000 | Loss: 0.00001565
Iteration 45/1000 | Loss: 0.00001565
Iteration 46/1000 | Loss: 0.00001564
Iteration 47/1000 | Loss: 0.00001564
Iteration 48/1000 | Loss: 0.00001564
Iteration 49/1000 | Loss: 0.00001563
Iteration 50/1000 | Loss: 0.00001563
Iteration 51/1000 | Loss: 0.00001563
Iteration 52/1000 | Loss: 0.00001563
Iteration 53/1000 | Loss: 0.00001563
Iteration 54/1000 | Loss: 0.00001563
Iteration 55/1000 | Loss: 0.00001563
Iteration 56/1000 | Loss: 0.00001563
Iteration 57/1000 | Loss: 0.00001563
Iteration 58/1000 | Loss: 0.00001563
Iteration 59/1000 | Loss: 0.00001563
Iteration 60/1000 | Loss: 0.00001562
Iteration 61/1000 | Loss: 0.00001562
Iteration 62/1000 | Loss: 0.00001560
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001557
Iteration 66/1000 | Loss: 0.00001557
Iteration 67/1000 | Loss: 0.00001556
Iteration 68/1000 | Loss: 0.00001554
Iteration 69/1000 | Loss: 0.00001554
Iteration 70/1000 | Loss: 0.00001553
Iteration 71/1000 | Loss: 0.00001553
Iteration 72/1000 | Loss: 0.00001553
Iteration 73/1000 | Loss: 0.00001552
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001552
Iteration 77/1000 | Loss: 0.00001551
Iteration 78/1000 | Loss: 0.00001551
Iteration 79/1000 | Loss: 0.00001551
Iteration 80/1000 | Loss: 0.00001550
Iteration 81/1000 | Loss: 0.00001549
Iteration 82/1000 | Loss: 0.00001549
Iteration 83/1000 | Loss: 0.00001549
Iteration 84/1000 | Loss: 0.00001549
Iteration 85/1000 | Loss: 0.00001549
Iteration 86/1000 | Loss: 0.00001549
Iteration 87/1000 | Loss: 0.00001549
Iteration 88/1000 | Loss: 0.00001549
Iteration 89/1000 | Loss: 0.00001549
Iteration 90/1000 | Loss: 0.00001549
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001548
Iteration 94/1000 | Loss: 0.00001548
Iteration 95/1000 | Loss: 0.00001547
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001547
Iteration 99/1000 | Loss: 0.00001546
Iteration 100/1000 | Loss: 0.00001546
Iteration 101/1000 | Loss: 0.00001546
Iteration 102/1000 | Loss: 0.00001546
Iteration 103/1000 | Loss: 0.00001546
Iteration 104/1000 | Loss: 0.00001546
Iteration 105/1000 | Loss: 0.00001546
Iteration 106/1000 | Loss: 0.00001545
Iteration 107/1000 | Loss: 0.00001545
Iteration 108/1000 | Loss: 0.00001545
Iteration 109/1000 | Loss: 0.00001544
Iteration 110/1000 | Loss: 0.00001544
Iteration 111/1000 | Loss: 0.00001544
Iteration 112/1000 | Loss: 0.00001543
Iteration 113/1000 | Loss: 0.00001543
Iteration 114/1000 | Loss: 0.00001543
Iteration 115/1000 | Loss: 0.00001543
Iteration 116/1000 | Loss: 0.00001543
Iteration 117/1000 | Loss: 0.00001543
Iteration 118/1000 | Loss: 0.00001543
Iteration 119/1000 | Loss: 0.00001542
Iteration 120/1000 | Loss: 0.00001542
Iteration 121/1000 | Loss: 0.00001542
Iteration 122/1000 | Loss: 0.00001542
Iteration 123/1000 | Loss: 0.00001542
Iteration 124/1000 | Loss: 0.00001542
Iteration 125/1000 | Loss: 0.00001542
Iteration 126/1000 | Loss: 0.00001542
Iteration 127/1000 | Loss: 0.00001541
Iteration 128/1000 | Loss: 0.00001541
Iteration 129/1000 | Loss: 0.00001540
Iteration 130/1000 | Loss: 0.00001539
Iteration 131/1000 | Loss: 0.00001539
Iteration 132/1000 | Loss: 0.00001539
Iteration 133/1000 | Loss: 0.00001539
Iteration 134/1000 | Loss: 0.00001539
Iteration 135/1000 | Loss: 0.00001539
Iteration 136/1000 | Loss: 0.00001539
Iteration 137/1000 | Loss: 0.00001539
Iteration 138/1000 | Loss: 0.00001539
Iteration 139/1000 | Loss: 0.00001539
Iteration 140/1000 | Loss: 0.00001539
Iteration 141/1000 | Loss: 0.00001539
Iteration 142/1000 | Loss: 0.00001539
Iteration 143/1000 | Loss: 0.00001537
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001537
Iteration 148/1000 | Loss: 0.00001537
Iteration 149/1000 | Loss: 0.00001537
Iteration 150/1000 | Loss: 0.00001537
Iteration 151/1000 | Loss: 0.00001537
Iteration 152/1000 | Loss: 0.00001537
Iteration 153/1000 | Loss: 0.00001536
Iteration 154/1000 | Loss: 0.00001536
Iteration 155/1000 | Loss: 0.00001536
Iteration 156/1000 | Loss: 0.00001536
Iteration 157/1000 | Loss: 0.00001536
Iteration 158/1000 | Loss: 0.00001535
Iteration 159/1000 | Loss: 0.00001535
Iteration 160/1000 | Loss: 0.00001535
Iteration 161/1000 | Loss: 0.00001535
Iteration 162/1000 | Loss: 0.00001534
Iteration 163/1000 | Loss: 0.00001534
Iteration 164/1000 | Loss: 0.00001534
Iteration 165/1000 | Loss: 0.00001534
Iteration 166/1000 | Loss: 0.00001534
Iteration 167/1000 | Loss: 0.00001534
Iteration 168/1000 | Loss: 0.00001533
Iteration 169/1000 | Loss: 0.00001533
Iteration 170/1000 | Loss: 0.00001533
Iteration 171/1000 | Loss: 0.00001533
Iteration 172/1000 | Loss: 0.00001533
Iteration 173/1000 | Loss: 0.00001533
Iteration 174/1000 | Loss: 0.00001533
Iteration 175/1000 | Loss: 0.00001533
Iteration 176/1000 | Loss: 0.00001533
Iteration 177/1000 | Loss: 0.00001533
Iteration 178/1000 | Loss: 0.00001532
Iteration 179/1000 | Loss: 0.00001532
Iteration 180/1000 | Loss: 0.00001532
Iteration 181/1000 | Loss: 0.00001532
Iteration 182/1000 | Loss: 0.00001532
Iteration 183/1000 | Loss: 0.00001532
Iteration 184/1000 | Loss: 0.00001532
Iteration 185/1000 | Loss: 0.00001531
Iteration 186/1000 | Loss: 0.00001531
Iteration 187/1000 | Loss: 0.00001531
Iteration 188/1000 | Loss: 0.00001531
Iteration 189/1000 | Loss: 0.00001531
Iteration 190/1000 | Loss: 0.00001530
Iteration 191/1000 | Loss: 0.00001530
Iteration 192/1000 | Loss: 0.00001530
Iteration 193/1000 | Loss: 0.00001530
Iteration 194/1000 | Loss: 0.00001530
Iteration 195/1000 | Loss: 0.00001530
Iteration 196/1000 | Loss: 0.00001530
Iteration 197/1000 | Loss: 0.00001530
Iteration 198/1000 | Loss: 0.00001530
Iteration 199/1000 | Loss: 0.00001529
Iteration 200/1000 | Loss: 0.00001529
Iteration 201/1000 | Loss: 0.00001529
Iteration 202/1000 | Loss: 0.00001529
Iteration 203/1000 | Loss: 0.00001529
Iteration 204/1000 | Loss: 0.00001529
Iteration 205/1000 | Loss: 0.00001529
Iteration 206/1000 | Loss: 0.00001528
Iteration 207/1000 | Loss: 0.00001528
Iteration 208/1000 | Loss: 0.00001528
Iteration 209/1000 | Loss: 0.00001528
Iteration 210/1000 | Loss: 0.00001528
Iteration 211/1000 | Loss: 0.00001528
Iteration 212/1000 | Loss: 0.00001528
Iteration 213/1000 | Loss: 0.00001528
Iteration 214/1000 | Loss: 0.00001528
Iteration 215/1000 | Loss: 0.00001528
Iteration 216/1000 | Loss: 0.00001528
Iteration 217/1000 | Loss: 0.00001528
Iteration 218/1000 | Loss: 0.00001528
Iteration 219/1000 | Loss: 0.00001528
Iteration 220/1000 | Loss: 0.00001528
Iteration 221/1000 | Loss: 0.00001527
Iteration 222/1000 | Loss: 0.00001527
Iteration 223/1000 | Loss: 0.00001527
Iteration 224/1000 | Loss: 0.00001527
Iteration 225/1000 | Loss: 0.00001527
Iteration 226/1000 | Loss: 0.00001527
Iteration 227/1000 | Loss: 0.00001527
Iteration 228/1000 | Loss: 0.00001527
Iteration 229/1000 | Loss: 0.00001527
Iteration 230/1000 | Loss: 0.00001527
Iteration 231/1000 | Loss: 0.00001527
Iteration 232/1000 | Loss: 0.00001527
Iteration 233/1000 | Loss: 0.00001526
Iteration 234/1000 | Loss: 0.00001526
Iteration 235/1000 | Loss: 0.00001526
Iteration 236/1000 | Loss: 0.00001526
Iteration 237/1000 | Loss: 0.00001526
Iteration 238/1000 | Loss: 0.00001526
Iteration 239/1000 | Loss: 0.00001526
Iteration 240/1000 | Loss: 0.00001526
Iteration 241/1000 | Loss: 0.00001526
Iteration 242/1000 | Loss: 0.00001526
Iteration 243/1000 | Loss: 0.00001526
Iteration 244/1000 | Loss: 0.00001526
Iteration 245/1000 | Loss: 0.00001526
Iteration 246/1000 | Loss: 0.00001526
Iteration 247/1000 | Loss: 0.00001526
Iteration 248/1000 | Loss: 0.00001526
Iteration 249/1000 | Loss: 0.00001526
Iteration 250/1000 | Loss: 0.00001526
Iteration 251/1000 | Loss: 0.00001526
Iteration 252/1000 | Loss: 0.00001525
Iteration 253/1000 | Loss: 0.00001525
Iteration 254/1000 | Loss: 0.00001525
Iteration 255/1000 | Loss: 0.00001525
Iteration 256/1000 | Loss: 0.00001525
Iteration 257/1000 | Loss: 0.00001525
Iteration 258/1000 | Loss: 0.00001525
Iteration 259/1000 | Loss: 0.00001525
Iteration 260/1000 | Loss: 0.00001525
Iteration 261/1000 | Loss: 0.00001525
Iteration 262/1000 | Loss: 0.00001525
Iteration 263/1000 | Loss: 0.00001525
Iteration 264/1000 | Loss: 0.00001525
Iteration 265/1000 | Loss: 0.00001524
Iteration 266/1000 | Loss: 0.00001524
Iteration 267/1000 | Loss: 0.00001524
Iteration 268/1000 | Loss: 0.00001524
Iteration 269/1000 | Loss: 0.00001524
Iteration 270/1000 | Loss: 0.00001524
Iteration 271/1000 | Loss: 0.00001524
Iteration 272/1000 | Loss: 0.00001524
Iteration 273/1000 | Loss: 0.00001524
Iteration 274/1000 | Loss: 0.00001524
Iteration 275/1000 | Loss: 0.00001523
Iteration 276/1000 | Loss: 0.00001523
Iteration 277/1000 | Loss: 0.00001523
Iteration 278/1000 | Loss: 0.00001523
Iteration 279/1000 | Loss: 0.00001523
Iteration 280/1000 | Loss: 0.00001523
Iteration 281/1000 | Loss: 0.00001523
Iteration 282/1000 | Loss: 0.00001523
Iteration 283/1000 | Loss: 0.00001523
Iteration 284/1000 | Loss: 0.00001523
Iteration 285/1000 | Loss: 0.00001523
Iteration 286/1000 | Loss: 0.00001522
Iteration 287/1000 | Loss: 0.00001522
Iteration 288/1000 | Loss: 0.00001522
Iteration 289/1000 | Loss: 0.00001522
Iteration 290/1000 | Loss: 0.00001522
Iteration 291/1000 | Loss: 0.00001522
Iteration 292/1000 | Loss: 0.00001522
Iteration 293/1000 | Loss: 0.00001522
Iteration 294/1000 | Loss: 0.00001522
Iteration 295/1000 | Loss: 0.00001522
Iteration 296/1000 | Loss: 0.00001521
Iteration 297/1000 | Loss: 0.00001521
Iteration 298/1000 | Loss: 0.00001521
Iteration 299/1000 | Loss: 0.00001521
Iteration 300/1000 | Loss: 0.00001521
Iteration 301/1000 | Loss: 0.00001521
Iteration 302/1000 | Loss: 0.00001521
Iteration 303/1000 | Loss: 0.00001521
Iteration 304/1000 | Loss: 0.00001521
Iteration 305/1000 | Loss: 0.00001521
Iteration 306/1000 | Loss: 0.00001521
Iteration 307/1000 | Loss: 0.00001521
Iteration 308/1000 | Loss: 0.00001521
Iteration 309/1000 | Loss: 0.00001520
Iteration 310/1000 | Loss: 0.00001520
Iteration 311/1000 | Loss: 0.00001520
Iteration 312/1000 | Loss: 0.00001520
Iteration 313/1000 | Loss: 0.00001520
Iteration 314/1000 | Loss: 0.00001520
Iteration 315/1000 | Loss: 0.00001520
Iteration 316/1000 | Loss: 0.00001520
Iteration 317/1000 | Loss: 0.00001520
Iteration 318/1000 | Loss: 0.00001520
Iteration 319/1000 | Loss: 0.00001520
Iteration 320/1000 | Loss: 0.00001520
Iteration 321/1000 | Loss: 0.00001520
Iteration 322/1000 | Loss: 0.00001520
Iteration 323/1000 | Loss: 0.00001520
Iteration 324/1000 | Loss: 0.00001520
Iteration 325/1000 | Loss: 0.00001520
Iteration 326/1000 | Loss: 0.00001520
Iteration 327/1000 | Loss: 0.00001520
Iteration 328/1000 | Loss: 0.00001520
Iteration 329/1000 | Loss: 0.00001520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [1.5197855645965319e-05, 1.5197855645965319e-05, 1.5197855645965319e-05, 1.5197855645965319e-05, 1.5197855645965319e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5197855645965319e-05

Optimization complete. Final v2v error: 3.265320301055908 mm

Highest mean error: 3.962878704071045 mm for frame 56

Lowest mean error: 2.561924457550049 mm for frame 119

Saving results

Total time: 53.51284337043762
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837015
Iteration 2/25 | Loss: 0.00133741
Iteration 3/25 | Loss: 0.00127026
Iteration 4/25 | Loss: 0.00126146
Iteration 5/25 | Loss: 0.00125840
Iteration 6/25 | Loss: 0.00125789
Iteration 7/25 | Loss: 0.00125789
Iteration 8/25 | Loss: 0.00125789
Iteration 9/25 | Loss: 0.00125789
Iteration 10/25 | Loss: 0.00125789
Iteration 11/25 | Loss: 0.00125789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012578886235132813, 0.0012578886235132813, 0.0012578886235132813, 0.0012578886235132813, 0.0012578886235132813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012578886235132813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63021266
Iteration 2/25 | Loss: 0.00147838
Iteration 3/25 | Loss: 0.00147837
Iteration 4/25 | Loss: 0.00147837
Iteration 5/25 | Loss: 0.00147837
Iteration 6/25 | Loss: 0.00147837
Iteration 7/25 | Loss: 0.00147837
Iteration 8/25 | Loss: 0.00147837
Iteration 9/25 | Loss: 0.00147837
Iteration 10/25 | Loss: 0.00147837
Iteration 11/25 | Loss: 0.00147837
Iteration 12/25 | Loss: 0.00147837
Iteration 13/25 | Loss: 0.00147837
Iteration 14/25 | Loss: 0.00147837
Iteration 15/25 | Loss: 0.00147837
Iteration 16/25 | Loss: 0.00147837
Iteration 17/25 | Loss: 0.00147837
Iteration 18/25 | Loss: 0.00147837
Iteration 19/25 | Loss: 0.00147837
Iteration 20/25 | Loss: 0.00147837
Iteration 21/25 | Loss: 0.00147837
Iteration 22/25 | Loss: 0.00147837
Iteration 23/25 | Loss: 0.00147837
Iteration 24/25 | Loss: 0.00147837
Iteration 25/25 | Loss: 0.00147837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147837
Iteration 2/1000 | Loss: 0.00002101
Iteration 3/1000 | Loss: 0.00001536
Iteration 4/1000 | Loss: 0.00001339
Iteration 5/1000 | Loss: 0.00001257
Iteration 6/1000 | Loss: 0.00001206
Iteration 7/1000 | Loss: 0.00001154
Iteration 8/1000 | Loss: 0.00001117
Iteration 9/1000 | Loss: 0.00001098
Iteration 10/1000 | Loss: 0.00001084
Iteration 11/1000 | Loss: 0.00001063
Iteration 12/1000 | Loss: 0.00001059
Iteration 13/1000 | Loss: 0.00001056
Iteration 14/1000 | Loss: 0.00001056
Iteration 15/1000 | Loss: 0.00001056
Iteration 16/1000 | Loss: 0.00001050
Iteration 17/1000 | Loss: 0.00001044
Iteration 18/1000 | Loss: 0.00001041
Iteration 19/1000 | Loss: 0.00001040
Iteration 20/1000 | Loss: 0.00001040
Iteration 21/1000 | Loss: 0.00001036
Iteration 22/1000 | Loss: 0.00001034
Iteration 23/1000 | Loss: 0.00001033
Iteration 24/1000 | Loss: 0.00001033
Iteration 25/1000 | Loss: 0.00001033
Iteration 26/1000 | Loss: 0.00001033
Iteration 27/1000 | Loss: 0.00001031
Iteration 28/1000 | Loss: 0.00001027
Iteration 29/1000 | Loss: 0.00001027
Iteration 30/1000 | Loss: 0.00001026
Iteration 31/1000 | Loss: 0.00001026
Iteration 32/1000 | Loss: 0.00001025
Iteration 33/1000 | Loss: 0.00001024
Iteration 34/1000 | Loss: 0.00001021
Iteration 35/1000 | Loss: 0.00001021
Iteration 36/1000 | Loss: 0.00001017
Iteration 37/1000 | Loss: 0.00001017
Iteration 38/1000 | Loss: 0.00001017
Iteration 39/1000 | Loss: 0.00001016
Iteration 40/1000 | Loss: 0.00001016
Iteration 41/1000 | Loss: 0.00001015
Iteration 42/1000 | Loss: 0.00001014
Iteration 43/1000 | Loss: 0.00001013
Iteration 44/1000 | Loss: 0.00001013
Iteration 45/1000 | Loss: 0.00001013
Iteration 46/1000 | Loss: 0.00001012
Iteration 47/1000 | Loss: 0.00001011
Iteration 48/1000 | Loss: 0.00001011
Iteration 49/1000 | Loss: 0.00001011
Iteration 50/1000 | Loss: 0.00001010
Iteration 51/1000 | Loss: 0.00001010
Iteration 52/1000 | Loss: 0.00001010
Iteration 53/1000 | Loss: 0.00001009
Iteration 54/1000 | Loss: 0.00001009
Iteration 55/1000 | Loss: 0.00001008
Iteration 56/1000 | Loss: 0.00001008
Iteration 57/1000 | Loss: 0.00001008
Iteration 58/1000 | Loss: 0.00001007
Iteration 59/1000 | Loss: 0.00001007
Iteration 60/1000 | Loss: 0.00001006
Iteration 61/1000 | Loss: 0.00001006
Iteration 62/1000 | Loss: 0.00001005
Iteration 63/1000 | Loss: 0.00001005
Iteration 64/1000 | Loss: 0.00001005
Iteration 65/1000 | Loss: 0.00001005
Iteration 66/1000 | Loss: 0.00001005
Iteration 67/1000 | Loss: 0.00001005
Iteration 68/1000 | Loss: 0.00001005
Iteration 69/1000 | Loss: 0.00001005
Iteration 70/1000 | Loss: 0.00001005
Iteration 71/1000 | Loss: 0.00001004
Iteration 72/1000 | Loss: 0.00001004
Iteration 73/1000 | Loss: 0.00001004
Iteration 74/1000 | Loss: 0.00001004
Iteration 75/1000 | Loss: 0.00001003
Iteration 76/1000 | Loss: 0.00001003
Iteration 77/1000 | Loss: 0.00001003
Iteration 78/1000 | Loss: 0.00001003
Iteration 79/1000 | Loss: 0.00001002
Iteration 80/1000 | Loss: 0.00001002
Iteration 81/1000 | Loss: 0.00001002
Iteration 82/1000 | Loss: 0.00001001
Iteration 83/1000 | Loss: 0.00001000
Iteration 84/1000 | Loss: 0.00001000
Iteration 85/1000 | Loss: 0.00000999
Iteration 86/1000 | Loss: 0.00000999
Iteration 87/1000 | Loss: 0.00000999
Iteration 88/1000 | Loss: 0.00000999
Iteration 89/1000 | Loss: 0.00000999
Iteration 90/1000 | Loss: 0.00000999
Iteration 91/1000 | Loss: 0.00000999
Iteration 92/1000 | Loss: 0.00000999
Iteration 93/1000 | Loss: 0.00000999
Iteration 94/1000 | Loss: 0.00000999
Iteration 95/1000 | Loss: 0.00000999
Iteration 96/1000 | Loss: 0.00000999
Iteration 97/1000 | Loss: 0.00000998
Iteration 98/1000 | Loss: 0.00000998
Iteration 99/1000 | Loss: 0.00000998
Iteration 100/1000 | Loss: 0.00000997
Iteration 101/1000 | Loss: 0.00000996
Iteration 102/1000 | Loss: 0.00000996
Iteration 103/1000 | Loss: 0.00000996
Iteration 104/1000 | Loss: 0.00000995
Iteration 105/1000 | Loss: 0.00000995
Iteration 106/1000 | Loss: 0.00000995
Iteration 107/1000 | Loss: 0.00000995
Iteration 108/1000 | Loss: 0.00000995
Iteration 109/1000 | Loss: 0.00000995
Iteration 110/1000 | Loss: 0.00000995
Iteration 111/1000 | Loss: 0.00000995
Iteration 112/1000 | Loss: 0.00000995
Iteration 113/1000 | Loss: 0.00000995
Iteration 114/1000 | Loss: 0.00000995
Iteration 115/1000 | Loss: 0.00000995
Iteration 116/1000 | Loss: 0.00000995
Iteration 117/1000 | Loss: 0.00000995
Iteration 118/1000 | Loss: 0.00000995
Iteration 119/1000 | Loss: 0.00000995
Iteration 120/1000 | Loss: 0.00000995
Iteration 121/1000 | Loss: 0.00000995
Iteration 122/1000 | Loss: 0.00000995
Iteration 123/1000 | Loss: 0.00000995
Iteration 124/1000 | Loss: 0.00000995
Iteration 125/1000 | Loss: 0.00000995
Iteration 126/1000 | Loss: 0.00000995
Iteration 127/1000 | Loss: 0.00000995
Iteration 128/1000 | Loss: 0.00000995
Iteration 129/1000 | Loss: 0.00000995
Iteration 130/1000 | Loss: 0.00000995
Iteration 131/1000 | Loss: 0.00000995
Iteration 132/1000 | Loss: 0.00000995
Iteration 133/1000 | Loss: 0.00000995
Iteration 134/1000 | Loss: 0.00000995
Iteration 135/1000 | Loss: 0.00000995
Iteration 136/1000 | Loss: 0.00000995
Iteration 137/1000 | Loss: 0.00000995
Iteration 138/1000 | Loss: 0.00000995
Iteration 139/1000 | Loss: 0.00000995
Iteration 140/1000 | Loss: 0.00000995
Iteration 141/1000 | Loss: 0.00000995
Iteration 142/1000 | Loss: 0.00000995
Iteration 143/1000 | Loss: 0.00000995
Iteration 144/1000 | Loss: 0.00000995
Iteration 145/1000 | Loss: 0.00000995
Iteration 146/1000 | Loss: 0.00000995
Iteration 147/1000 | Loss: 0.00000995
Iteration 148/1000 | Loss: 0.00000995
Iteration 149/1000 | Loss: 0.00000995
Iteration 150/1000 | Loss: 0.00000995
Iteration 151/1000 | Loss: 0.00000995
Iteration 152/1000 | Loss: 0.00000995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [9.950416824722197e-06, 9.950416824722197e-06, 9.950416824722197e-06, 9.950416824722197e-06, 9.950416824722197e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.950416824722197e-06

Optimization complete. Final v2v error: 2.7290704250335693 mm

Highest mean error: 3.067734718322754 mm for frame 57

Lowest mean error: 2.5458950996398926 mm for frame 128

Saving results

Total time: 36.2281858921051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491775
Iteration 2/25 | Loss: 0.00140107
Iteration 3/25 | Loss: 0.00133457
Iteration 4/25 | Loss: 0.00132688
Iteration 5/25 | Loss: 0.00132472
Iteration 6/25 | Loss: 0.00132472
Iteration 7/25 | Loss: 0.00132472
Iteration 8/25 | Loss: 0.00132472
Iteration 9/25 | Loss: 0.00132472
Iteration 10/25 | Loss: 0.00132472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013247162569314241, 0.0013247162569314241, 0.0013247162569314241, 0.0013247162569314241, 0.0013247162569314241]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013247162569314241

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28909552
Iteration 2/25 | Loss: 0.00161144
Iteration 3/25 | Loss: 0.00161143
Iteration 4/25 | Loss: 0.00161143
Iteration 5/25 | Loss: 0.00161143
Iteration 6/25 | Loss: 0.00161143
Iteration 7/25 | Loss: 0.00161143
Iteration 8/25 | Loss: 0.00161143
Iteration 9/25 | Loss: 0.00161143
Iteration 10/25 | Loss: 0.00161143
Iteration 11/25 | Loss: 0.00161143
Iteration 12/25 | Loss: 0.00161143
Iteration 13/25 | Loss: 0.00161143
Iteration 14/25 | Loss: 0.00161143
Iteration 15/25 | Loss: 0.00161143
Iteration 16/25 | Loss: 0.00161143
Iteration 17/25 | Loss: 0.00161143
Iteration 18/25 | Loss: 0.00161143
Iteration 19/25 | Loss: 0.00161143
Iteration 20/25 | Loss: 0.00161143
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016114297322928905, 0.0016114297322928905, 0.0016114297322928905, 0.0016114297322928905, 0.0016114297322928905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016114297322928905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161143
Iteration 2/1000 | Loss: 0.00004773
Iteration 3/1000 | Loss: 0.00003302
Iteration 4/1000 | Loss: 0.00002794
Iteration 5/1000 | Loss: 0.00002651
Iteration 6/1000 | Loss: 0.00002567
Iteration 7/1000 | Loss: 0.00002496
Iteration 8/1000 | Loss: 0.00002443
Iteration 9/1000 | Loss: 0.00002404
Iteration 10/1000 | Loss: 0.00002374
Iteration 11/1000 | Loss: 0.00002346
Iteration 12/1000 | Loss: 0.00002333
Iteration 13/1000 | Loss: 0.00002331
Iteration 14/1000 | Loss: 0.00002313
Iteration 15/1000 | Loss: 0.00002302
Iteration 16/1000 | Loss: 0.00002295
Iteration 17/1000 | Loss: 0.00002294
Iteration 18/1000 | Loss: 0.00002293
Iteration 19/1000 | Loss: 0.00002293
Iteration 20/1000 | Loss: 0.00002292
Iteration 21/1000 | Loss: 0.00002292
Iteration 22/1000 | Loss: 0.00002292
Iteration 23/1000 | Loss: 0.00002291
Iteration 24/1000 | Loss: 0.00002290
Iteration 25/1000 | Loss: 0.00002286
Iteration 26/1000 | Loss: 0.00002285
Iteration 27/1000 | Loss: 0.00002283
Iteration 28/1000 | Loss: 0.00002283
Iteration 29/1000 | Loss: 0.00002282
Iteration 30/1000 | Loss: 0.00002282
Iteration 31/1000 | Loss: 0.00002278
Iteration 32/1000 | Loss: 0.00002269
Iteration 33/1000 | Loss: 0.00002269
Iteration 34/1000 | Loss: 0.00002262
Iteration 35/1000 | Loss: 0.00002261
Iteration 36/1000 | Loss: 0.00002261
Iteration 37/1000 | Loss: 0.00002258
Iteration 38/1000 | Loss: 0.00002257
Iteration 39/1000 | Loss: 0.00002256
Iteration 40/1000 | Loss: 0.00002256
Iteration 41/1000 | Loss: 0.00002254
Iteration 42/1000 | Loss: 0.00002254
Iteration 43/1000 | Loss: 0.00002253
Iteration 44/1000 | Loss: 0.00002252
Iteration 45/1000 | Loss: 0.00002251
Iteration 46/1000 | Loss: 0.00002251
Iteration 47/1000 | Loss: 0.00002251
Iteration 48/1000 | Loss: 0.00002250
Iteration 49/1000 | Loss: 0.00002250
Iteration 50/1000 | Loss: 0.00002249
Iteration 51/1000 | Loss: 0.00002249
Iteration 52/1000 | Loss: 0.00002249
Iteration 53/1000 | Loss: 0.00002248
Iteration 54/1000 | Loss: 0.00002248
Iteration 55/1000 | Loss: 0.00002248
Iteration 56/1000 | Loss: 0.00002248
Iteration 57/1000 | Loss: 0.00002247
Iteration 58/1000 | Loss: 0.00002247
Iteration 59/1000 | Loss: 0.00002246
Iteration 60/1000 | Loss: 0.00002246
Iteration 61/1000 | Loss: 0.00002246
Iteration 62/1000 | Loss: 0.00002245
Iteration 63/1000 | Loss: 0.00002245
Iteration 64/1000 | Loss: 0.00002245
Iteration 65/1000 | Loss: 0.00002245
Iteration 66/1000 | Loss: 0.00002245
Iteration 67/1000 | Loss: 0.00002244
Iteration 68/1000 | Loss: 0.00002243
Iteration 69/1000 | Loss: 0.00002242
Iteration 70/1000 | Loss: 0.00002242
Iteration 71/1000 | Loss: 0.00002241
Iteration 72/1000 | Loss: 0.00002241
Iteration 73/1000 | Loss: 0.00002240
Iteration 74/1000 | Loss: 0.00002240
Iteration 75/1000 | Loss: 0.00002240
Iteration 76/1000 | Loss: 0.00002239
Iteration 77/1000 | Loss: 0.00002239
Iteration 78/1000 | Loss: 0.00002239
Iteration 79/1000 | Loss: 0.00002238
Iteration 80/1000 | Loss: 0.00002238
Iteration 81/1000 | Loss: 0.00002238
Iteration 82/1000 | Loss: 0.00002237
Iteration 83/1000 | Loss: 0.00002237
Iteration 84/1000 | Loss: 0.00002236
Iteration 85/1000 | Loss: 0.00002236
Iteration 86/1000 | Loss: 0.00002236
Iteration 87/1000 | Loss: 0.00002236
Iteration 88/1000 | Loss: 0.00002236
Iteration 89/1000 | Loss: 0.00002235
Iteration 90/1000 | Loss: 0.00002235
Iteration 91/1000 | Loss: 0.00002235
Iteration 92/1000 | Loss: 0.00002235
Iteration 93/1000 | Loss: 0.00002235
Iteration 94/1000 | Loss: 0.00002235
Iteration 95/1000 | Loss: 0.00002234
Iteration 96/1000 | Loss: 0.00002234
Iteration 97/1000 | Loss: 0.00002234
Iteration 98/1000 | Loss: 0.00002234
Iteration 99/1000 | Loss: 0.00002234
Iteration 100/1000 | Loss: 0.00002233
Iteration 101/1000 | Loss: 0.00002233
Iteration 102/1000 | Loss: 0.00002233
Iteration 103/1000 | Loss: 0.00002233
Iteration 104/1000 | Loss: 0.00002232
Iteration 105/1000 | Loss: 0.00002232
Iteration 106/1000 | Loss: 0.00002232
Iteration 107/1000 | Loss: 0.00002232
Iteration 108/1000 | Loss: 0.00002232
Iteration 109/1000 | Loss: 0.00002232
Iteration 110/1000 | Loss: 0.00002231
Iteration 111/1000 | Loss: 0.00002231
Iteration 112/1000 | Loss: 0.00002231
Iteration 113/1000 | Loss: 0.00002231
Iteration 114/1000 | Loss: 0.00002231
Iteration 115/1000 | Loss: 0.00002231
Iteration 116/1000 | Loss: 0.00002231
Iteration 117/1000 | Loss: 0.00002230
Iteration 118/1000 | Loss: 0.00002230
Iteration 119/1000 | Loss: 0.00002230
Iteration 120/1000 | Loss: 0.00002230
Iteration 121/1000 | Loss: 0.00002230
Iteration 122/1000 | Loss: 0.00002229
Iteration 123/1000 | Loss: 0.00002229
Iteration 124/1000 | Loss: 0.00002229
Iteration 125/1000 | Loss: 0.00002229
Iteration 126/1000 | Loss: 0.00002229
Iteration 127/1000 | Loss: 0.00002229
Iteration 128/1000 | Loss: 0.00002229
Iteration 129/1000 | Loss: 0.00002229
Iteration 130/1000 | Loss: 0.00002229
Iteration 131/1000 | Loss: 0.00002229
Iteration 132/1000 | Loss: 0.00002229
Iteration 133/1000 | Loss: 0.00002229
Iteration 134/1000 | Loss: 0.00002229
Iteration 135/1000 | Loss: 0.00002229
Iteration 136/1000 | Loss: 0.00002229
Iteration 137/1000 | Loss: 0.00002229
Iteration 138/1000 | Loss: 0.00002229
Iteration 139/1000 | Loss: 0.00002229
Iteration 140/1000 | Loss: 0.00002229
Iteration 141/1000 | Loss: 0.00002229
Iteration 142/1000 | Loss: 0.00002229
Iteration 143/1000 | Loss: 0.00002229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.2286212697508745e-05, 2.2286212697508745e-05, 2.2286212697508745e-05, 2.2286212697508745e-05, 2.2286212697508745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2286212697508745e-05

Optimization complete. Final v2v error: 3.637734889984131 mm

Highest mean error: 4.682093143463135 mm for frame 94

Lowest mean error: 3.1564908027648926 mm for frame 135

Saving results

Total time: 43.74799418449402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383810
Iteration 2/25 | Loss: 0.00134273
Iteration 3/25 | Loss: 0.00127232
Iteration 4/25 | Loss: 0.00126569
Iteration 5/25 | Loss: 0.00126326
Iteration 6/25 | Loss: 0.00126326
Iteration 7/25 | Loss: 0.00126326
Iteration 8/25 | Loss: 0.00126326
Iteration 9/25 | Loss: 0.00126326
Iteration 10/25 | Loss: 0.00126326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012632564175873995, 0.0012632564175873995, 0.0012632564175873995, 0.0012632564175873995, 0.0012632564175873995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012632564175873995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45526934
Iteration 2/25 | Loss: 0.00143084
Iteration 3/25 | Loss: 0.00143084
Iteration 4/25 | Loss: 0.00143084
Iteration 5/25 | Loss: 0.00143084
Iteration 6/25 | Loss: 0.00143084
Iteration 7/25 | Loss: 0.00143084
Iteration 8/25 | Loss: 0.00143084
Iteration 9/25 | Loss: 0.00143084
Iteration 10/25 | Loss: 0.00143084
Iteration 11/25 | Loss: 0.00143084
Iteration 12/25 | Loss: 0.00143084
Iteration 13/25 | Loss: 0.00143084
Iteration 14/25 | Loss: 0.00143084
Iteration 15/25 | Loss: 0.00143084
Iteration 16/25 | Loss: 0.00143084
Iteration 17/25 | Loss: 0.00143084
Iteration 18/25 | Loss: 0.00143084
Iteration 19/25 | Loss: 0.00143084
Iteration 20/25 | Loss: 0.00143084
Iteration 21/25 | Loss: 0.00143084
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014308362733572721, 0.0014308362733572721, 0.0014308362733572721, 0.0014308362733572721, 0.0014308362733572721]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014308362733572721

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143084
Iteration 2/1000 | Loss: 0.00002506
Iteration 3/1000 | Loss: 0.00001626
Iteration 4/1000 | Loss: 0.00001457
Iteration 5/1000 | Loss: 0.00001338
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001209
Iteration 8/1000 | Loss: 0.00001179
Iteration 9/1000 | Loss: 0.00001139
Iteration 10/1000 | Loss: 0.00001107
Iteration 11/1000 | Loss: 0.00001092
Iteration 12/1000 | Loss: 0.00001088
Iteration 13/1000 | Loss: 0.00001083
Iteration 14/1000 | Loss: 0.00001078
Iteration 15/1000 | Loss: 0.00001078
Iteration 16/1000 | Loss: 0.00001078
Iteration 17/1000 | Loss: 0.00001077
Iteration 18/1000 | Loss: 0.00001077
Iteration 19/1000 | Loss: 0.00001077
Iteration 20/1000 | Loss: 0.00001077
Iteration 21/1000 | Loss: 0.00001077
Iteration 22/1000 | Loss: 0.00001074
Iteration 23/1000 | Loss: 0.00001074
Iteration 24/1000 | Loss: 0.00001072
Iteration 25/1000 | Loss: 0.00001072
Iteration 26/1000 | Loss: 0.00001071
Iteration 27/1000 | Loss: 0.00001071
Iteration 28/1000 | Loss: 0.00001071
Iteration 29/1000 | Loss: 0.00001071
Iteration 30/1000 | Loss: 0.00001071
Iteration 31/1000 | Loss: 0.00001071
Iteration 32/1000 | Loss: 0.00001071
Iteration 33/1000 | Loss: 0.00001071
Iteration 34/1000 | Loss: 0.00001069
Iteration 35/1000 | Loss: 0.00001067
Iteration 36/1000 | Loss: 0.00001067
Iteration 37/1000 | Loss: 0.00001067
Iteration 38/1000 | Loss: 0.00001066
Iteration 39/1000 | Loss: 0.00001066
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001066
Iteration 42/1000 | Loss: 0.00001066
Iteration 43/1000 | Loss: 0.00001066
Iteration 44/1000 | Loss: 0.00001066
Iteration 45/1000 | Loss: 0.00001066
Iteration 46/1000 | Loss: 0.00001064
Iteration 47/1000 | Loss: 0.00001064
Iteration 48/1000 | Loss: 0.00001063
Iteration 49/1000 | Loss: 0.00001062
Iteration 50/1000 | Loss: 0.00001061
Iteration 51/1000 | Loss: 0.00001061
Iteration 52/1000 | Loss: 0.00001061
Iteration 53/1000 | Loss: 0.00001058
Iteration 54/1000 | Loss: 0.00001056
Iteration 55/1000 | Loss: 0.00001056
Iteration 56/1000 | Loss: 0.00001055
Iteration 57/1000 | Loss: 0.00001054
Iteration 58/1000 | Loss: 0.00001054
Iteration 59/1000 | Loss: 0.00001052
Iteration 60/1000 | Loss: 0.00001051
Iteration 61/1000 | Loss: 0.00001051
Iteration 62/1000 | Loss: 0.00001051
Iteration 63/1000 | Loss: 0.00001051
Iteration 64/1000 | Loss: 0.00001050
Iteration 65/1000 | Loss: 0.00001050
Iteration 66/1000 | Loss: 0.00001050
Iteration 67/1000 | Loss: 0.00001049
Iteration 68/1000 | Loss: 0.00001049
Iteration 69/1000 | Loss: 0.00001048
Iteration 70/1000 | Loss: 0.00001048
Iteration 71/1000 | Loss: 0.00001048
Iteration 72/1000 | Loss: 0.00001047
Iteration 73/1000 | Loss: 0.00001047
Iteration 74/1000 | Loss: 0.00001047
Iteration 75/1000 | Loss: 0.00001046
Iteration 76/1000 | Loss: 0.00001046
Iteration 77/1000 | Loss: 0.00001046
Iteration 78/1000 | Loss: 0.00001046
Iteration 79/1000 | Loss: 0.00001046
Iteration 80/1000 | Loss: 0.00001046
Iteration 81/1000 | Loss: 0.00001046
Iteration 82/1000 | Loss: 0.00001045
Iteration 83/1000 | Loss: 0.00001045
Iteration 84/1000 | Loss: 0.00001045
Iteration 85/1000 | Loss: 0.00001045
Iteration 86/1000 | Loss: 0.00001044
Iteration 87/1000 | Loss: 0.00001043
Iteration 88/1000 | Loss: 0.00001043
Iteration 89/1000 | Loss: 0.00001043
Iteration 90/1000 | Loss: 0.00001042
Iteration 91/1000 | Loss: 0.00001042
Iteration 92/1000 | Loss: 0.00001042
Iteration 93/1000 | Loss: 0.00001042
Iteration 94/1000 | Loss: 0.00001042
Iteration 95/1000 | Loss: 0.00001042
Iteration 96/1000 | Loss: 0.00001041
Iteration 97/1000 | Loss: 0.00001041
Iteration 98/1000 | Loss: 0.00001041
Iteration 99/1000 | Loss: 0.00001041
Iteration 100/1000 | Loss: 0.00001039
Iteration 101/1000 | Loss: 0.00001039
Iteration 102/1000 | Loss: 0.00001038
Iteration 103/1000 | Loss: 0.00001038
Iteration 104/1000 | Loss: 0.00001037
Iteration 105/1000 | Loss: 0.00001037
Iteration 106/1000 | Loss: 0.00001037
Iteration 107/1000 | Loss: 0.00001037
Iteration 108/1000 | Loss: 0.00001037
Iteration 109/1000 | Loss: 0.00001037
Iteration 110/1000 | Loss: 0.00001037
Iteration 111/1000 | Loss: 0.00001037
Iteration 112/1000 | Loss: 0.00001037
Iteration 113/1000 | Loss: 0.00001036
Iteration 114/1000 | Loss: 0.00001036
Iteration 115/1000 | Loss: 0.00001036
Iteration 116/1000 | Loss: 0.00001036
Iteration 117/1000 | Loss: 0.00001036
Iteration 118/1000 | Loss: 0.00001036
Iteration 119/1000 | Loss: 0.00001036
Iteration 120/1000 | Loss: 0.00001036
Iteration 121/1000 | Loss: 0.00001036
Iteration 122/1000 | Loss: 0.00001035
Iteration 123/1000 | Loss: 0.00001035
Iteration 124/1000 | Loss: 0.00001035
Iteration 125/1000 | Loss: 0.00001035
Iteration 126/1000 | Loss: 0.00001034
Iteration 127/1000 | Loss: 0.00001034
Iteration 128/1000 | Loss: 0.00001033
Iteration 129/1000 | Loss: 0.00001033
Iteration 130/1000 | Loss: 0.00001033
Iteration 131/1000 | Loss: 0.00001033
Iteration 132/1000 | Loss: 0.00001033
Iteration 133/1000 | Loss: 0.00001033
Iteration 134/1000 | Loss: 0.00001033
Iteration 135/1000 | Loss: 0.00001033
Iteration 136/1000 | Loss: 0.00001033
Iteration 137/1000 | Loss: 0.00001032
Iteration 138/1000 | Loss: 0.00001032
Iteration 139/1000 | Loss: 0.00001031
Iteration 140/1000 | Loss: 0.00001031
Iteration 141/1000 | Loss: 0.00001031
Iteration 142/1000 | Loss: 0.00001031
Iteration 143/1000 | Loss: 0.00001030
Iteration 144/1000 | Loss: 0.00001030
Iteration 145/1000 | Loss: 0.00001030
Iteration 146/1000 | Loss: 0.00001030
Iteration 147/1000 | Loss: 0.00001030
Iteration 148/1000 | Loss: 0.00001030
Iteration 149/1000 | Loss: 0.00001029
Iteration 150/1000 | Loss: 0.00001029
Iteration 151/1000 | Loss: 0.00001029
Iteration 152/1000 | Loss: 0.00001029
Iteration 153/1000 | Loss: 0.00001029
Iteration 154/1000 | Loss: 0.00001029
Iteration 155/1000 | Loss: 0.00001028
Iteration 156/1000 | Loss: 0.00001028
Iteration 157/1000 | Loss: 0.00001028
Iteration 158/1000 | Loss: 0.00001027
Iteration 159/1000 | Loss: 0.00001027
Iteration 160/1000 | Loss: 0.00001027
Iteration 161/1000 | Loss: 0.00001026
Iteration 162/1000 | Loss: 0.00001026
Iteration 163/1000 | Loss: 0.00001026
Iteration 164/1000 | Loss: 0.00001025
Iteration 165/1000 | Loss: 0.00001025
Iteration 166/1000 | Loss: 0.00001025
Iteration 167/1000 | Loss: 0.00001025
Iteration 168/1000 | Loss: 0.00001024
Iteration 169/1000 | Loss: 0.00001024
Iteration 170/1000 | Loss: 0.00001024
Iteration 171/1000 | Loss: 0.00001023
Iteration 172/1000 | Loss: 0.00001023
Iteration 173/1000 | Loss: 0.00001022
Iteration 174/1000 | Loss: 0.00001022
Iteration 175/1000 | Loss: 0.00001022
Iteration 176/1000 | Loss: 0.00001022
Iteration 177/1000 | Loss: 0.00001021
Iteration 178/1000 | Loss: 0.00001021
Iteration 179/1000 | Loss: 0.00001021
Iteration 180/1000 | Loss: 0.00001020
Iteration 181/1000 | Loss: 0.00001019
Iteration 182/1000 | Loss: 0.00001019
Iteration 183/1000 | Loss: 0.00001019
Iteration 184/1000 | Loss: 0.00001019
Iteration 185/1000 | Loss: 0.00001019
Iteration 186/1000 | Loss: 0.00001019
Iteration 187/1000 | Loss: 0.00001018
Iteration 188/1000 | Loss: 0.00001018
Iteration 189/1000 | Loss: 0.00001018
Iteration 190/1000 | Loss: 0.00001017
Iteration 191/1000 | Loss: 0.00001016
Iteration 192/1000 | Loss: 0.00001016
Iteration 193/1000 | Loss: 0.00001016
Iteration 194/1000 | Loss: 0.00001016
Iteration 195/1000 | Loss: 0.00001015
Iteration 196/1000 | Loss: 0.00001015
Iteration 197/1000 | Loss: 0.00001015
Iteration 198/1000 | Loss: 0.00001015
Iteration 199/1000 | Loss: 0.00001015
Iteration 200/1000 | Loss: 0.00001015
Iteration 201/1000 | Loss: 0.00001015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.0154579285881482e-05, 1.0154579285881482e-05, 1.0154579285881482e-05, 1.0154579285881482e-05, 1.0154579285881482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0154579285881482e-05

Optimization complete. Final v2v error: 2.743834972381592 mm

Highest mean error: 3.126819610595703 mm for frame 130

Lowest mean error: 2.5410068035125732 mm for frame 37

Saving results

Total time: 47.34731721878052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448671
Iteration 2/25 | Loss: 0.00135992
Iteration 3/25 | Loss: 0.00127277
Iteration 4/25 | Loss: 0.00125604
Iteration 5/25 | Loss: 0.00125068
Iteration 6/25 | Loss: 0.00124910
Iteration 7/25 | Loss: 0.00124884
Iteration 8/25 | Loss: 0.00124884
Iteration 9/25 | Loss: 0.00124884
Iteration 10/25 | Loss: 0.00124884
Iteration 11/25 | Loss: 0.00124884
Iteration 12/25 | Loss: 0.00124884
Iteration 13/25 | Loss: 0.00124884
Iteration 14/25 | Loss: 0.00124884
Iteration 15/25 | Loss: 0.00124884
Iteration 16/25 | Loss: 0.00124884
Iteration 17/25 | Loss: 0.00124884
Iteration 18/25 | Loss: 0.00124884
Iteration 19/25 | Loss: 0.00124884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012488355860114098, 0.0012488355860114098, 0.0012488355860114098, 0.0012488355860114098, 0.0012488355860114098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012488355860114098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18272674
Iteration 2/25 | Loss: 0.00148757
Iteration 3/25 | Loss: 0.00148754
Iteration 4/25 | Loss: 0.00148754
Iteration 5/25 | Loss: 0.00148754
Iteration 6/25 | Loss: 0.00148754
Iteration 7/25 | Loss: 0.00148754
Iteration 8/25 | Loss: 0.00148754
Iteration 9/25 | Loss: 0.00148754
Iteration 10/25 | Loss: 0.00148754
Iteration 11/25 | Loss: 0.00148754
Iteration 12/25 | Loss: 0.00148754
Iteration 13/25 | Loss: 0.00148754
Iteration 14/25 | Loss: 0.00148754
Iteration 15/25 | Loss: 0.00148754
Iteration 16/25 | Loss: 0.00148754
Iteration 17/25 | Loss: 0.00148754
Iteration 18/25 | Loss: 0.00148754
Iteration 19/25 | Loss: 0.00148754
Iteration 20/25 | Loss: 0.00148754
Iteration 21/25 | Loss: 0.00148754
Iteration 22/25 | Loss: 0.00148754
Iteration 23/25 | Loss: 0.00148754
Iteration 24/25 | Loss: 0.00148754
Iteration 25/25 | Loss: 0.00148754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148754
Iteration 2/1000 | Loss: 0.00003892
Iteration 3/1000 | Loss: 0.00002476
Iteration 4/1000 | Loss: 0.00002022
Iteration 5/1000 | Loss: 0.00001737
Iteration 6/1000 | Loss: 0.00001616
Iteration 7/1000 | Loss: 0.00001528
Iteration 8/1000 | Loss: 0.00001478
Iteration 9/1000 | Loss: 0.00001416
Iteration 10/1000 | Loss: 0.00001376
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001354
Iteration 13/1000 | Loss: 0.00001336
Iteration 14/1000 | Loss: 0.00001320
Iteration 15/1000 | Loss: 0.00001315
Iteration 16/1000 | Loss: 0.00001315
Iteration 17/1000 | Loss: 0.00001311
Iteration 18/1000 | Loss: 0.00001308
Iteration 19/1000 | Loss: 0.00001307
Iteration 20/1000 | Loss: 0.00001306
Iteration 21/1000 | Loss: 0.00001305
Iteration 22/1000 | Loss: 0.00001300
Iteration 23/1000 | Loss: 0.00001297
Iteration 24/1000 | Loss: 0.00001297
Iteration 25/1000 | Loss: 0.00001296
Iteration 26/1000 | Loss: 0.00001295
Iteration 27/1000 | Loss: 0.00001289
Iteration 28/1000 | Loss: 0.00001285
Iteration 29/1000 | Loss: 0.00001281
Iteration 30/1000 | Loss: 0.00001280
Iteration 31/1000 | Loss: 0.00001279
Iteration 32/1000 | Loss: 0.00001279
Iteration 33/1000 | Loss: 0.00001278
Iteration 34/1000 | Loss: 0.00001278
Iteration 35/1000 | Loss: 0.00001278
Iteration 36/1000 | Loss: 0.00001272
Iteration 37/1000 | Loss: 0.00001272
Iteration 38/1000 | Loss: 0.00001272
Iteration 39/1000 | Loss: 0.00001271
Iteration 40/1000 | Loss: 0.00001271
Iteration 41/1000 | Loss: 0.00001270
Iteration 42/1000 | Loss: 0.00001270
Iteration 43/1000 | Loss: 0.00001270
Iteration 44/1000 | Loss: 0.00001270
Iteration 45/1000 | Loss: 0.00001270
Iteration 46/1000 | Loss: 0.00001269
Iteration 47/1000 | Loss: 0.00001268
Iteration 48/1000 | Loss: 0.00001268
Iteration 49/1000 | Loss: 0.00001267
Iteration 50/1000 | Loss: 0.00001267
Iteration 51/1000 | Loss: 0.00001267
Iteration 52/1000 | Loss: 0.00001267
Iteration 53/1000 | Loss: 0.00001267
Iteration 54/1000 | Loss: 0.00001267
Iteration 55/1000 | Loss: 0.00001267
Iteration 56/1000 | Loss: 0.00001267
Iteration 57/1000 | Loss: 0.00001267
Iteration 58/1000 | Loss: 0.00001266
Iteration 59/1000 | Loss: 0.00001266
Iteration 60/1000 | Loss: 0.00001266
Iteration 61/1000 | Loss: 0.00001265
Iteration 62/1000 | Loss: 0.00001265
Iteration 63/1000 | Loss: 0.00001264
Iteration 64/1000 | Loss: 0.00001264
Iteration 65/1000 | Loss: 0.00001264
Iteration 66/1000 | Loss: 0.00001264
Iteration 67/1000 | Loss: 0.00001264
Iteration 68/1000 | Loss: 0.00001264
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001263
Iteration 72/1000 | Loss: 0.00001263
Iteration 73/1000 | Loss: 0.00001263
Iteration 74/1000 | Loss: 0.00001263
Iteration 75/1000 | Loss: 0.00001263
Iteration 76/1000 | Loss: 0.00001263
Iteration 77/1000 | Loss: 0.00001263
Iteration 78/1000 | Loss: 0.00001262
Iteration 79/1000 | Loss: 0.00001262
Iteration 80/1000 | Loss: 0.00001262
Iteration 81/1000 | Loss: 0.00001262
Iteration 82/1000 | Loss: 0.00001262
Iteration 83/1000 | Loss: 0.00001262
Iteration 84/1000 | Loss: 0.00001262
Iteration 85/1000 | Loss: 0.00001262
Iteration 86/1000 | Loss: 0.00001262
Iteration 87/1000 | Loss: 0.00001262
Iteration 88/1000 | Loss: 0.00001262
Iteration 89/1000 | Loss: 0.00001262
Iteration 90/1000 | Loss: 0.00001262
Iteration 91/1000 | Loss: 0.00001262
Iteration 92/1000 | Loss: 0.00001262
Iteration 93/1000 | Loss: 0.00001261
Iteration 94/1000 | Loss: 0.00001261
Iteration 95/1000 | Loss: 0.00001261
Iteration 96/1000 | Loss: 0.00001260
Iteration 97/1000 | Loss: 0.00001260
Iteration 98/1000 | Loss: 0.00001260
Iteration 99/1000 | Loss: 0.00001259
Iteration 100/1000 | Loss: 0.00001259
Iteration 101/1000 | Loss: 0.00001259
Iteration 102/1000 | Loss: 0.00001259
Iteration 103/1000 | Loss: 0.00001259
Iteration 104/1000 | Loss: 0.00001258
Iteration 105/1000 | Loss: 0.00001258
Iteration 106/1000 | Loss: 0.00001257
Iteration 107/1000 | Loss: 0.00001256
Iteration 108/1000 | Loss: 0.00001256
Iteration 109/1000 | Loss: 0.00001256
Iteration 110/1000 | Loss: 0.00001256
Iteration 111/1000 | Loss: 0.00001256
Iteration 112/1000 | Loss: 0.00001256
Iteration 113/1000 | Loss: 0.00001256
Iteration 114/1000 | Loss: 0.00001256
Iteration 115/1000 | Loss: 0.00001256
Iteration 116/1000 | Loss: 0.00001256
Iteration 117/1000 | Loss: 0.00001255
Iteration 118/1000 | Loss: 0.00001255
Iteration 119/1000 | Loss: 0.00001255
Iteration 120/1000 | Loss: 0.00001255
Iteration 121/1000 | Loss: 0.00001254
Iteration 122/1000 | Loss: 0.00001254
Iteration 123/1000 | Loss: 0.00001254
Iteration 124/1000 | Loss: 0.00001254
Iteration 125/1000 | Loss: 0.00001254
Iteration 126/1000 | Loss: 0.00001254
Iteration 127/1000 | Loss: 0.00001254
Iteration 128/1000 | Loss: 0.00001254
Iteration 129/1000 | Loss: 0.00001254
Iteration 130/1000 | Loss: 0.00001254
Iteration 131/1000 | Loss: 0.00001254
Iteration 132/1000 | Loss: 0.00001254
Iteration 133/1000 | Loss: 0.00001254
Iteration 134/1000 | Loss: 0.00001254
Iteration 135/1000 | Loss: 0.00001254
Iteration 136/1000 | Loss: 0.00001254
Iteration 137/1000 | Loss: 0.00001254
Iteration 138/1000 | Loss: 0.00001254
Iteration 139/1000 | Loss: 0.00001254
Iteration 140/1000 | Loss: 0.00001254
Iteration 141/1000 | Loss: 0.00001254
Iteration 142/1000 | Loss: 0.00001254
Iteration 143/1000 | Loss: 0.00001254
Iteration 144/1000 | Loss: 0.00001254
Iteration 145/1000 | Loss: 0.00001254
Iteration 146/1000 | Loss: 0.00001254
Iteration 147/1000 | Loss: 0.00001254
Iteration 148/1000 | Loss: 0.00001254
Iteration 149/1000 | Loss: 0.00001254
Iteration 150/1000 | Loss: 0.00001254
Iteration 151/1000 | Loss: 0.00001254
Iteration 152/1000 | Loss: 0.00001254
Iteration 153/1000 | Loss: 0.00001254
Iteration 154/1000 | Loss: 0.00001254
Iteration 155/1000 | Loss: 0.00001254
Iteration 156/1000 | Loss: 0.00001254
Iteration 157/1000 | Loss: 0.00001254
Iteration 158/1000 | Loss: 0.00001254
Iteration 159/1000 | Loss: 0.00001254
Iteration 160/1000 | Loss: 0.00001254
Iteration 161/1000 | Loss: 0.00001254
Iteration 162/1000 | Loss: 0.00001254
Iteration 163/1000 | Loss: 0.00001254
Iteration 164/1000 | Loss: 0.00001254
Iteration 165/1000 | Loss: 0.00001254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.2544461242214311e-05, 1.2544461242214311e-05, 1.2544461242214311e-05, 1.2544461242214311e-05, 1.2544461242214311e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2544461242214311e-05

Optimization complete. Final v2v error: 3.074260711669922 mm

Highest mean error: 3.598923683166504 mm for frame 57

Lowest mean error: 2.750413656234741 mm for frame 15

Saving results

Total time: 42.4489209651947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038801
Iteration 2/25 | Loss: 0.01038801
Iteration 3/25 | Loss: 0.01038801
Iteration 4/25 | Loss: 0.01038801
Iteration 5/25 | Loss: 0.01038801
Iteration 6/25 | Loss: 0.01038801
Iteration 7/25 | Loss: 0.01038800
Iteration 8/25 | Loss: 0.01038800
Iteration 9/25 | Loss: 0.01038800
Iteration 10/25 | Loss: 0.01038800
Iteration 11/25 | Loss: 0.01038800
Iteration 12/25 | Loss: 0.01038800
Iteration 13/25 | Loss: 0.01038800
Iteration 14/25 | Loss: 0.01038800
Iteration 15/25 | Loss: 0.01038800
Iteration 16/25 | Loss: 0.01038800
Iteration 17/25 | Loss: 0.01038800
Iteration 18/25 | Loss: 0.01038800
Iteration 19/25 | Loss: 0.01038800
Iteration 20/25 | Loss: 0.01038800
Iteration 21/25 | Loss: 0.01038800
Iteration 22/25 | Loss: 0.01038800
Iteration 23/25 | Loss: 0.01038799
Iteration 24/25 | Loss: 0.01038799
Iteration 25/25 | Loss: 0.01038799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45376766
Iteration 2/25 | Loss: 0.09460425
Iteration 3/25 | Loss: 0.09406639
Iteration 4/25 | Loss: 0.09406637
Iteration 5/25 | Loss: 0.09406637
Iteration 6/25 | Loss: 0.09406637
Iteration 7/25 | Loss: 0.09406637
Iteration 8/25 | Loss: 0.09406637
Iteration 9/25 | Loss: 0.09406637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.09406636655330658, 0.09406636655330658, 0.09406636655330658, 0.09406636655330658, 0.09406636655330658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09406636655330658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09406637
Iteration 2/1000 | Loss: 0.00261119
Iteration 3/1000 | Loss: 0.00273292
Iteration 4/1000 | Loss: 0.00012362
Iteration 5/1000 | Loss: 0.00215977
Iteration 6/1000 | Loss: 0.00006538
Iteration 7/1000 | Loss: 0.00009467
Iteration 8/1000 | Loss: 0.00021198
Iteration 9/1000 | Loss: 0.00020126
Iteration 10/1000 | Loss: 0.00006905
Iteration 11/1000 | Loss: 0.00003248
Iteration 12/1000 | Loss: 0.00011859
Iteration 13/1000 | Loss: 0.00099952
Iteration 14/1000 | Loss: 0.00037391
Iteration 15/1000 | Loss: 0.00004781
Iteration 16/1000 | Loss: 0.00002850
Iteration 17/1000 | Loss: 0.00016749
Iteration 18/1000 | Loss: 0.00003614
Iteration 19/1000 | Loss: 0.00011827
Iteration 20/1000 | Loss: 0.00013936
Iteration 21/1000 | Loss: 0.00015783
Iteration 22/1000 | Loss: 0.00008165
Iteration 23/1000 | Loss: 0.00010651
Iteration 24/1000 | Loss: 0.00004674
Iteration 25/1000 | Loss: 0.00001884
Iteration 26/1000 | Loss: 0.00031012
Iteration 27/1000 | Loss: 0.00008152
Iteration 28/1000 | Loss: 0.00002751
Iteration 29/1000 | Loss: 0.00007977
Iteration 30/1000 | Loss: 0.00002771
Iteration 31/1000 | Loss: 0.00005112
Iteration 32/1000 | Loss: 0.00001703
Iteration 33/1000 | Loss: 0.00022109
Iteration 34/1000 | Loss: 0.00005375
Iteration 35/1000 | Loss: 0.00002664
Iteration 36/1000 | Loss: 0.00003613
Iteration 37/1000 | Loss: 0.00002198
Iteration 38/1000 | Loss: 0.00001784
Iteration 39/1000 | Loss: 0.00001598
Iteration 40/1000 | Loss: 0.00010312
Iteration 41/1000 | Loss: 0.00001615
Iteration 42/1000 | Loss: 0.00001538
Iteration 43/1000 | Loss: 0.00001512
Iteration 44/1000 | Loss: 0.00001480
Iteration 45/1000 | Loss: 0.00013936
Iteration 46/1000 | Loss: 0.00008943
Iteration 47/1000 | Loss: 0.00003273
Iteration 48/1000 | Loss: 0.00001456
Iteration 49/1000 | Loss: 0.00002519
Iteration 50/1000 | Loss: 0.00001435
Iteration 51/1000 | Loss: 0.00001430
Iteration 52/1000 | Loss: 0.00001426
Iteration 53/1000 | Loss: 0.00017766
Iteration 54/1000 | Loss: 0.00001531
Iteration 55/1000 | Loss: 0.00001443
Iteration 56/1000 | Loss: 0.00001403
Iteration 57/1000 | Loss: 0.00001389
Iteration 58/1000 | Loss: 0.00001375
Iteration 59/1000 | Loss: 0.00004778
Iteration 60/1000 | Loss: 0.00003617
Iteration 61/1000 | Loss: 0.00002940
Iteration 62/1000 | Loss: 0.00005090
Iteration 63/1000 | Loss: 0.00003003
Iteration 64/1000 | Loss: 0.00002530
Iteration 65/1000 | Loss: 0.00006680
Iteration 66/1000 | Loss: 0.00003933
Iteration 67/1000 | Loss: 0.00016298
Iteration 68/1000 | Loss: 0.00049124
Iteration 69/1000 | Loss: 0.00010221
Iteration 70/1000 | Loss: 0.00004838
Iteration 71/1000 | Loss: 0.00019990
Iteration 72/1000 | Loss: 0.00028424
Iteration 73/1000 | Loss: 0.00010017
Iteration 74/1000 | Loss: 0.00012968
Iteration 75/1000 | Loss: 0.00001459
Iteration 76/1000 | Loss: 0.00003979
Iteration 77/1000 | Loss: 0.00013572
Iteration 78/1000 | Loss: 0.00001519
Iteration 79/1000 | Loss: 0.00001391
Iteration 80/1000 | Loss: 0.00001369
Iteration 81/1000 | Loss: 0.00001366
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001364
Iteration 84/1000 | Loss: 0.00001364
Iteration 85/1000 | Loss: 0.00001363
Iteration 86/1000 | Loss: 0.00001362
Iteration 87/1000 | Loss: 0.00001362
Iteration 88/1000 | Loss: 0.00001361
Iteration 89/1000 | Loss: 0.00001359
Iteration 90/1000 | Loss: 0.00001358
Iteration 91/1000 | Loss: 0.00001358
Iteration 92/1000 | Loss: 0.00001357
Iteration 93/1000 | Loss: 0.00001357
Iteration 94/1000 | Loss: 0.00001356
Iteration 95/1000 | Loss: 0.00001356
Iteration 96/1000 | Loss: 0.00001356
Iteration 97/1000 | Loss: 0.00001356
Iteration 98/1000 | Loss: 0.00001356
Iteration 99/1000 | Loss: 0.00001355
Iteration 100/1000 | Loss: 0.00001355
Iteration 101/1000 | Loss: 0.00001354
Iteration 102/1000 | Loss: 0.00001353
Iteration 103/1000 | Loss: 0.00001353
Iteration 104/1000 | Loss: 0.00001353
Iteration 105/1000 | Loss: 0.00001352
Iteration 106/1000 | Loss: 0.00001352
Iteration 107/1000 | Loss: 0.00001352
Iteration 108/1000 | Loss: 0.00001352
Iteration 109/1000 | Loss: 0.00001352
Iteration 110/1000 | Loss: 0.00001371
Iteration 111/1000 | Loss: 0.00001371
Iteration 112/1000 | Loss: 0.00001370
Iteration 113/1000 | Loss: 0.00001370
Iteration 114/1000 | Loss: 0.00001356
Iteration 115/1000 | Loss: 0.00001356
Iteration 116/1000 | Loss: 0.00001353
Iteration 117/1000 | Loss: 0.00001350
Iteration 118/1000 | Loss: 0.00001350
Iteration 119/1000 | Loss: 0.00001350
Iteration 120/1000 | Loss: 0.00001350
Iteration 121/1000 | Loss: 0.00001349
Iteration 122/1000 | Loss: 0.00001349
Iteration 123/1000 | Loss: 0.00001349
Iteration 124/1000 | Loss: 0.00001349
Iteration 125/1000 | Loss: 0.00001349
Iteration 126/1000 | Loss: 0.00001349
Iteration 127/1000 | Loss: 0.00001349
Iteration 128/1000 | Loss: 0.00001349
Iteration 129/1000 | Loss: 0.00001349
Iteration 130/1000 | Loss: 0.00001349
Iteration 131/1000 | Loss: 0.00001349
Iteration 132/1000 | Loss: 0.00001349
Iteration 133/1000 | Loss: 0.00001349
Iteration 134/1000 | Loss: 0.00001349
Iteration 135/1000 | Loss: 0.00001349
Iteration 136/1000 | Loss: 0.00001349
Iteration 137/1000 | Loss: 0.00001349
Iteration 138/1000 | Loss: 0.00001349
Iteration 139/1000 | Loss: 0.00001349
Iteration 140/1000 | Loss: 0.00001349
Iteration 141/1000 | Loss: 0.00001349
Iteration 142/1000 | Loss: 0.00001349
Iteration 143/1000 | Loss: 0.00001349
Iteration 144/1000 | Loss: 0.00001349
Iteration 145/1000 | Loss: 0.00001349
Iteration 146/1000 | Loss: 0.00001349
Iteration 147/1000 | Loss: 0.00001349
Iteration 148/1000 | Loss: 0.00001349
Iteration 149/1000 | Loss: 0.00001349
Iteration 150/1000 | Loss: 0.00001349
Iteration 151/1000 | Loss: 0.00001349
Iteration 152/1000 | Loss: 0.00001349
Iteration 153/1000 | Loss: 0.00001349
Iteration 154/1000 | Loss: 0.00001349
Iteration 155/1000 | Loss: 0.00001349
Iteration 156/1000 | Loss: 0.00001349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.3488807780959178e-05, 1.3488807780959178e-05, 1.3488807780959178e-05, 1.3488807780959178e-05, 1.3488807780959178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3488807780959178e-05

Optimization complete. Final v2v error: 3.0672147274017334 mm

Highest mean error: 10.46611213684082 mm for frame 1

Lowest mean error: 2.7584445476531982 mm for frame 240

Saving results

Total time: 140.57769680023193
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403611
Iteration 2/25 | Loss: 0.00138480
Iteration 3/25 | Loss: 0.00128961
Iteration 4/25 | Loss: 0.00127872
Iteration 5/25 | Loss: 0.00127634
Iteration 6/25 | Loss: 0.00127568
Iteration 7/25 | Loss: 0.00127550
Iteration 8/25 | Loss: 0.00127550
Iteration 9/25 | Loss: 0.00127550
Iteration 10/25 | Loss: 0.00127550
Iteration 11/25 | Loss: 0.00127550
Iteration 12/25 | Loss: 0.00127550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012755049392580986, 0.0012755049392580986, 0.0012755049392580986, 0.0012755049392580986, 0.0012755049392580986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012755049392580986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40613794
Iteration 2/25 | Loss: 0.00138546
Iteration 3/25 | Loss: 0.00138545
Iteration 4/25 | Loss: 0.00138545
Iteration 5/25 | Loss: 0.00138545
Iteration 6/25 | Loss: 0.00138545
Iteration 7/25 | Loss: 0.00138545
Iteration 8/25 | Loss: 0.00138545
Iteration 9/25 | Loss: 0.00138545
Iteration 10/25 | Loss: 0.00138545
Iteration 11/25 | Loss: 0.00138545
Iteration 12/25 | Loss: 0.00138545
Iteration 13/25 | Loss: 0.00138545
Iteration 14/25 | Loss: 0.00138545
Iteration 15/25 | Loss: 0.00138545
Iteration 16/25 | Loss: 0.00138545
Iteration 17/25 | Loss: 0.00138545
Iteration 18/25 | Loss: 0.00138545
Iteration 19/25 | Loss: 0.00138545
Iteration 20/25 | Loss: 0.00138545
Iteration 21/25 | Loss: 0.00138545
Iteration 22/25 | Loss: 0.00138545
Iteration 23/25 | Loss: 0.00138545
Iteration 24/25 | Loss: 0.00138545
Iteration 25/25 | Loss: 0.00138545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138545
Iteration 2/1000 | Loss: 0.00003510
Iteration 3/1000 | Loss: 0.00002276
Iteration 4/1000 | Loss: 0.00001752
Iteration 5/1000 | Loss: 0.00001605
Iteration 6/1000 | Loss: 0.00001489
Iteration 7/1000 | Loss: 0.00001421
Iteration 8/1000 | Loss: 0.00001372
Iteration 9/1000 | Loss: 0.00001332
Iteration 10/1000 | Loss: 0.00001310
Iteration 11/1000 | Loss: 0.00001291
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001267
Iteration 15/1000 | Loss: 0.00001264
Iteration 16/1000 | Loss: 0.00001254
Iteration 17/1000 | Loss: 0.00001253
Iteration 18/1000 | Loss: 0.00001248
Iteration 19/1000 | Loss: 0.00001241
Iteration 20/1000 | Loss: 0.00001238
Iteration 21/1000 | Loss: 0.00001233
Iteration 22/1000 | Loss: 0.00001231
Iteration 23/1000 | Loss: 0.00001231
Iteration 24/1000 | Loss: 0.00001230
Iteration 25/1000 | Loss: 0.00001230
Iteration 26/1000 | Loss: 0.00001230
Iteration 27/1000 | Loss: 0.00001229
Iteration 28/1000 | Loss: 0.00001228
Iteration 29/1000 | Loss: 0.00001227
Iteration 30/1000 | Loss: 0.00001227
Iteration 31/1000 | Loss: 0.00001226
Iteration 32/1000 | Loss: 0.00001226
Iteration 33/1000 | Loss: 0.00001226
Iteration 34/1000 | Loss: 0.00001225
Iteration 35/1000 | Loss: 0.00001225
Iteration 36/1000 | Loss: 0.00001224
Iteration 37/1000 | Loss: 0.00001224
Iteration 38/1000 | Loss: 0.00001223
Iteration 39/1000 | Loss: 0.00001223
Iteration 40/1000 | Loss: 0.00001222
Iteration 41/1000 | Loss: 0.00001222
Iteration 42/1000 | Loss: 0.00001221
Iteration 43/1000 | Loss: 0.00001221
Iteration 44/1000 | Loss: 0.00001221
Iteration 45/1000 | Loss: 0.00001220
Iteration 46/1000 | Loss: 0.00001220
Iteration 47/1000 | Loss: 0.00001220
Iteration 48/1000 | Loss: 0.00001220
Iteration 49/1000 | Loss: 0.00001220
Iteration 50/1000 | Loss: 0.00001220
Iteration 51/1000 | Loss: 0.00001220
Iteration 52/1000 | Loss: 0.00001220
Iteration 53/1000 | Loss: 0.00001220
Iteration 54/1000 | Loss: 0.00001220
Iteration 55/1000 | Loss: 0.00001220
Iteration 56/1000 | Loss: 0.00001220
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001220
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001220
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001220
Iteration 67/1000 | Loss: 0.00001220
Iteration 68/1000 | Loss: 0.00001220
Iteration 69/1000 | Loss: 0.00001220
Iteration 70/1000 | Loss: 0.00001220
Iteration 71/1000 | Loss: 0.00001220
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001220
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001220
Iteration 76/1000 | Loss: 0.00001220
Iteration 77/1000 | Loss: 0.00001220
Iteration 78/1000 | Loss: 0.00001220
Iteration 79/1000 | Loss: 0.00001220
Iteration 80/1000 | Loss: 0.00001220
Iteration 81/1000 | Loss: 0.00001220
Iteration 82/1000 | Loss: 0.00001220
Iteration 83/1000 | Loss: 0.00001220
Iteration 84/1000 | Loss: 0.00001220
Iteration 85/1000 | Loss: 0.00001220
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001219
Iteration 88/1000 | Loss: 0.00001219
Iteration 89/1000 | Loss: 0.00001219
Iteration 90/1000 | Loss: 0.00001219
Iteration 91/1000 | Loss: 0.00001219
Iteration 92/1000 | Loss: 0.00001219
Iteration 93/1000 | Loss: 0.00001219
Iteration 94/1000 | Loss: 0.00001219
Iteration 95/1000 | Loss: 0.00001219
Iteration 96/1000 | Loss: 0.00001219
Iteration 97/1000 | Loss: 0.00001219
Iteration 98/1000 | Loss: 0.00001219
Iteration 99/1000 | Loss: 0.00001219
Iteration 100/1000 | Loss: 0.00001219
Iteration 101/1000 | Loss: 0.00001219
Iteration 102/1000 | Loss: 0.00001219
Iteration 103/1000 | Loss: 0.00001219
Iteration 104/1000 | Loss: 0.00001219
Iteration 105/1000 | Loss: 0.00001219
Iteration 106/1000 | Loss: 0.00001219
Iteration 107/1000 | Loss: 0.00001219
Iteration 108/1000 | Loss: 0.00001219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.2194614100735635e-05, 1.2194614100735635e-05, 1.2194614100735635e-05, 1.2194614100735635e-05, 1.2194614100735635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2194614100735635e-05

Optimization complete. Final v2v error: 2.9750702381134033 mm

Highest mean error: 4.4347147941589355 mm for frame 60

Lowest mean error: 2.718458890914917 mm for frame 103

Saving results

Total time: 36.32332372665405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387940
Iteration 2/25 | Loss: 0.00136019
Iteration 3/25 | Loss: 0.00128382
Iteration 4/25 | Loss: 0.00127630
Iteration 5/25 | Loss: 0.00127429
Iteration 6/25 | Loss: 0.00127412
Iteration 7/25 | Loss: 0.00127412
Iteration 8/25 | Loss: 0.00127412
Iteration 9/25 | Loss: 0.00127412
Iteration 10/25 | Loss: 0.00127412
Iteration 11/25 | Loss: 0.00127412
Iteration 12/25 | Loss: 0.00127412
Iteration 13/25 | Loss: 0.00127412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012741246027871966, 0.0012741246027871966, 0.0012741246027871966, 0.0012741246027871966, 0.0012741246027871966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012741246027871966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32978857
Iteration 2/25 | Loss: 0.00141177
Iteration 3/25 | Loss: 0.00141177
Iteration 4/25 | Loss: 0.00141177
Iteration 5/25 | Loss: 0.00141177
Iteration 6/25 | Loss: 0.00141177
Iteration 7/25 | Loss: 0.00141177
Iteration 8/25 | Loss: 0.00141177
Iteration 9/25 | Loss: 0.00141177
Iteration 10/25 | Loss: 0.00141177
Iteration 11/25 | Loss: 0.00141177
Iteration 12/25 | Loss: 0.00141177
Iteration 13/25 | Loss: 0.00141177
Iteration 14/25 | Loss: 0.00141177
Iteration 15/25 | Loss: 0.00141177
Iteration 16/25 | Loss: 0.00141177
Iteration 17/25 | Loss: 0.00141177
Iteration 18/25 | Loss: 0.00141177
Iteration 19/25 | Loss: 0.00141177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014117688406258821, 0.0014117688406258821, 0.0014117688406258821, 0.0014117688406258821, 0.0014117688406258821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014117688406258821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141177
Iteration 2/1000 | Loss: 0.00002305
Iteration 3/1000 | Loss: 0.00001653
Iteration 4/1000 | Loss: 0.00001483
Iteration 5/1000 | Loss: 0.00001402
Iteration 6/1000 | Loss: 0.00001344
Iteration 7/1000 | Loss: 0.00001308
Iteration 8/1000 | Loss: 0.00001288
Iteration 9/1000 | Loss: 0.00001258
Iteration 10/1000 | Loss: 0.00001249
Iteration 11/1000 | Loss: 0.00001245
Iteration 12/1000 | Loss: 0.00001244
Iteration 13/1000 | Loss: 0.00001235
Iteration 14/1000 | Loss: 0.00001226
Iteration 15/1000 | Loss: 0.00001226
Iteration 16/1000 | Loss: 0.00001226
Iteration 17/1000 | Loss: 0.00001225
Iteration 18/1000 | Loss: 0.00001225
Iteration 19/1000 | Loss: 0.00001225
Iteration 20/1000 | Loss: 0.00001225
Iteration 21/1000 | Loss: 0.00001225
Iteration 22/1000 | Loss: 0.00001225
Iteration 23/1000 | Loss: 0.00001225
Iteration 24/1000 | Loss: 0.00001225
Iteration 25/1000 | Loss: 0.00001225
Iteration 26/1000 | Loss: 0.00001225
Iteration 27/1000 | Loss: 0.00001224
Iteration 28/1000 | Loss: 0.00001224
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001216
Iteration 33/1000 | Loss: 0.00001213
Iteration 34/1000 | Loss: 0.00001212
Iteration 35/1000 | Loss: 0.00001210
Iteration 36/1000 | Loss: 0.00001210
Iteration 37/1000 | Loss: 0.00001209
Iteration 38/1000 | Loss: 0.00001208
Iteration 39/1000 | Loss: 0.00001207
Iteration 40/1000 | Loss: 0.00001206
Iteration 41/1000 | Loss: 0.00001206
Iteration 42/1000 | Loss: 0.00001205
Iteration 43/1000 | Loss: 0.00001205
Iteration 44/1000 | Loss: 0.00001204
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001203
Iteration 47/1000 | Loss: 0.00001202
Iteration 48/1000 | Loss: 0.00001200
Iteration 49/1000 | Loss: 0.00001199
Iteration 50/1000 | Loss: 0.00001199
Iteration 51/1000 | Loss: 0.00001198
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001193
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00001193
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001190
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001189
Iteration 72/1000 | Loss: 0.00001188
Iteration 73/1000 | Loss: 0.00001188
Iteration 74/1000 | Loss: 0.00001188
Iteration 75/1000 | Loss: 0.00001187
Iteration 76/1000 | Loss: 0.00001187
Iteration 77/1000 | Loss: 0.00001187
Iteration 78/1000 | Loss: 0.00001187
Iteration 79/1000 | Loss: 0.00001187
Iteration 80/1000 | Loss: 0.00001187
Iteration 81/1000 | Loss: 0.00001187
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001186
Iteration 84/1000 | Loss: 0.00001185
Iteration 85/1000 | Loss: 0.00001185
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001183
Iteration 88/1000 | Loss: 0.00001182
Iteration 89/1000 | Loss: 0.00001182
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001181
Iteration 92/1000 | Loss: 0.00001181
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001179
Iteration 98/1000 | Loss: 0.00001179
Iteration 99/1000 | Loss: 0.00001178
Iteration 100/1000 | Loss: 0.00001178
Iteration 101/1000 | Loss: 0.00001178
Iteration 102/1000 | Loss: 0.00001178
Iteration 103/1000 | Loss: 0.00001177
Iteration 104/1000 | Loss: 0.00001177
Iteration 105/1000 | Loss: 0.00001177
Iteration 106/1000 | Loss: 0.00001177
Iteration 107/1000 | Loss: 0.00001177
Iteration 108/1000 | Loss: 0.00001177
Iteration 109/1000 | Loss: 0.00001177
Iteration 110/1000 | Loss: 0.00001176
Iteration 111/1000 | Loss: 0.00001176
Iteration 112/1000 | Loss: 0.00001176
Iteration 113/1000 | Loss: 0.00001175
Iteration 114/1000 | Loss: 0.00001175
Iteration 115/1000 | Loss: 0.00001175
Iteration 116/1000 | Loss: 0.00001175
Iteration 117/1000 | Loss: 0.00001175
Iteration 118/1000 | Loss: 0.00001175
Iteration 119/1000 | Loss: 0.00001175
Iteration 120/1000 | Loss: 0.00001175
Iteration 121/1000 | Loss: 0.00001174
Iteration 122/1000 | Loss: 0.00001174
Iteration 123/1000 | Loss: 0.00001174
Iteration 124/1000 | Loss: 0.00001173
Iteration 125/1000 | Loss: 0.00001173
Iteration 126/1000 | Loss: 0.00001173
Iteration 127/1000 | Loss: 0.00001173
Iteration 128/1000 | Loss: 0.00001173
Iteration 129/1000 | Loss: 0.00001172
Iteration 130/1000 | Loss: 0.00001172
Iteration 131/1000 | Loss: 0.00001172
Iteration 132/1000 | Loss: 0.00001172
Iteration 133/1000 | Loss: 0.00001172
Iteration 134/1000 | Loss: 0.00001172
Iteration 135/1000 | Loss: 0.00001172
Iteration 136/1000 | Loss: 0.00001172
Iteration 137/1000 | Loss: 0.00001172
Iteration 138/1000 | Loss: 0.00001172
Iteration 139/1000 | Loss: 0.00001172
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001172
Iteration 143/1000 | Loss: 0.00001172
Iteration 144/1000 | Loss: 0.00001172
Iteration 145/1000 | Loss: 0.00001172
Iteration 146/1000 | Loss: 0.00001171
Iteration 147/1000 | Loss: 0.00001171
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001171
Iteration 152/1000 | Loss: 0.00001171
Iteration 153/1000 | Loss: 0.00001171
Iteration 154/1000 | Loss: 0.00001171
Iteration 155/1000 | Loss: 0.00001171
Iteration 156/1000 | Loss: 0.00001171
Iteration 157/1000 | Loss: 0.00001171
Iteration 158/1000 | Loss: 0.00001171
Iteration 159/1000 | Loss: 0.00001171
Iteration 160/1000 | Loss: 0.00001171
Iteration 161/1000 | Loss: 0.00001171
Iteration 162/1000 | Loss: 0.00001171
Iteration 163/1000 | Loss: 0.00001171
Iteration 164/1000 | Loss: 0.00001171
Iteration 165/1000 | Loss: 0.00001171
Iteration 166/1000 | Loss: 0.00001171
Iteration 167/1000 | Loss: 0.00001171
Iteration 168/1000 | Loss: 0.00001171
Iteration 169/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.171254461951321e-05, 1.171254461951321e-05, 1.171254461951321e-05, 1.171254461951321e-05, 1.171254461951321e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.171254461951321e-05

Optimization complete. Final v2v error: 2.9220821857452393 mm

Highest mean error: 3.126711130142212 mm for frame 59

Lowest mean error: 2.766152858734131 mm for frame 38

Saving results

Total time: 36.48464584350586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764817
Iteration 2/25 | Loss: 0.00143475
Iteration 3/25 | Loss: 0.00131952
Iteration 4/25 | Loss: 0.00128755
Iteration 5/25 | Loss: 0.00129699
Iteration 6/25 | Loss: 0.00129082
Iteration 7/25 | Loss: 0.00126670
Iteration 8/25 | Loss: 0.00125615
Iteration 9/25 | Loss: 0.00125359
Iteration 10/25 | Loss: 0.00125155
Iteration 11/25 | Loss: 0.00125338
Iteration 12/25 | Loss: 0.00125368
Iteration 13/25 | Loss: 0.00125389
Iteration 14/25 | Loss: 0.00125277
Iteration 15/25 | Loss: 0.00125156
Iteration 16/25 | Loss: 0.00125362
Iteration 17/25 | Loss: 0.00125420
Iteration 18/25 | Loss: 0.00125330
Iteration 19/25 | Loss: 0.00125221
Iteration 20/25 | Loss: 0.00125420
Iteration 21/25 | Loss: 0.00125365
Iteration 22/25 | Loss: 0.00125121
Iteration 23/25 | Loss: 0.00124960
Iteration 24/25 | Loss: 0.00124917
Iteration 25/25 | Loss: 0.00124896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27581882
Iteration 2/25 | Loss: 0.00150462
Iteration 3/25 | Loss: 0.00150460
Iteration 4/25 | Loss: 0.00150460
Iteration 5/25 | Loss: 0.00150460
Iteration 6/25 | Loss: 0.00150460
Iteration 7/25 | Loss: 0.00150460
Iteration 8/25 | Loss: 0.00150460
Iteration 9/25 | Loss: 0.00150460
Iteration 10/25 | Loss: 0.00150460
Iteration 11/25 | Loss: 0.00150460
Iteration 12/25 | Loss: 0.00150460
Iteration 13/25 | Loss: 0.00150460
Iteration 14/25 | Loss: 0.00150460
Iteration 15/25 | Loss: 0.00150460
Iteration 16/25 | Loss: 0.00150460
Iteration 17/25 | Loss: 0.00150460
Iteration 18/25 | Loss: 0.00150460
Iteration 19/25 | Loss: 0.00150460
Iteration 20/25 | Loss: 0.00150460
Iteration 21/25 | Loss: 0.00150460
Iteration 22/25 | Loss: 0.00150460
Iteration 23/25 | Loss: 0.00150460
Iteration 24/25 | Loss: 0.00150460
Iteration 25/25 | Loss: 0.00150460
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015045951586216688, 0.0015045951586216688, 0.0015045951586216688, 0.0015045951586216688, 0.0015045951586216688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015045951586216688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150460
Iteration 2/1000 | Loss: 0.00004363
Iteration 3/1000 | Loss: 0.00003252
Iteration 4/1000 | Loss: 0.00002721
Iteration 5/1000 | Loss: 0.00002563
Iteration 6/1000 | Loss: 0.00002448
Iteration 7/1000 | Loss: 0.00002373
Iteration 8/1000 | Loss: 0.00002327
Iteration 9/1000 | Loss: 0.00002282
Iteration 10/1000 | Loss: 0.00002251
Iteration 11/1000 | Loss: 0.00002222
Iteration 12/1000 | Loss: 0.00002220
Iteration 13/1000 | Loss: 0.00002203
Iteration 14/1000 | Loss: 0.00002182
Iteration 15/1000 | Loss: 0.00002160
Iteration 16/1000 | Loss: 0.00002154
Iteration 17/1000 | Loss: 0.00002136
Iteration 18/1000 | Loss: 0.00002125
Iteration 19/1000 | Loss: 0.00002123
Iteration 20/1000 | Loss: 0.00002122
Iteration 21/1000 | Loss: 0.00002122
Iteration 22/1000 | Loss: 0.00002122
Iteration 23/1000 | Loss: 0.00002122
Iteration 24/1000 | Loss: 0.00002121
Iteration 25/1000 | Loss: 0.00002120
Iteration 26/1000 | Loss: 0.00002119
Iteration 27/1000 | Loss: 0.00002119
Iteration 28/1000 | Loss: 0.00002119
Iteration 29/1000 | Loss: 0.00002119
Iteration 30/1000 | Loss: 0.00002119
Iteration 31/1000 | Loss: 0.00002119
Iteration 32/1000 | Loss: 0.00002118
Iteration 33/1000 | Loss: 0.00002117
Iteration 34/1000 | Loss: 0.00002116
Iteration 35/1000 | Loss: 0.00002116
Iteration 36/1000 | Loss: 0.00002115
Iteration 37/1000 | Loss: 0.00002115
Iteration 38/1000 | Loss: 0.00002114
Iteration 39/1000 | Loss: 0.00002114
Iteration 40/1000 | Loss: 0.00002114
Iteration 41/1000 | Loss: 0.00002113
Iteration 42/1000 | Loss: 0.00002113
Iteration 43/1000 | Loss: 0.00002113
Iteration 44/1000 | Loss: 0.00002113
Iteration 45/1000 | Loss: 0.00002113
Iteration 46/1000 | Loss: 0.00002112
Iteration 47/1000 | Loss: 0.00002112
Iteration 48/1000 | Loss: 0.00002112
Iteration 49/1000 | Loss: 0.00002112
Iteration 50/1000 | Loss: 0.00002112
Iteration 51/1000 | Loss: 0.00002112
Iteration 52/1000 | Loss: 0.00002111
Iteration 53/1000 | Loss: 0.00002111
Iteration 54/1000 | Loss: 0.00002111
Iteration 55/1000 | Loss: 0.00002111
Iteration 56/1000 | Loss: 0.00002111
Iteration 57/1000 | Loss: 0.00002111
Iteration 58/1000 | Loss: 0.00002111
Iteration 59/1000 | Loss: 0.00002111
Iteration 60/1000 | Loss: 0.00002111
Iteration 61/1000 | Loss: 0.00002111
Iteration 62/1000 | Loss: 0.00002111
Iteration 63/1000 | Loss: 0.00002111
Iteration 64/1000 | Loss: 0.00002110
Iteration 65/1000 | Loss: 0.00002110
Iteration 66/1000 | Loss: 0.00002110
Iteration 67/1000 | Loss: 0.00002110
Iteration 68/1000 | Loss: 0.00002110
Iteration 69/1000 | Loss: 0.00002109
Iteration 70/1000 | Loss: 0.00002109
Iteration 71/1000 | Loss: 0.00002109
Iteration 72/1000 | Loss: 0.00002109
Iteration 73/1000 | Loss: 0.00002109
Iteration 74/1000 | Loss: 0.00002109
Iteration 75/1000 | Loss: 0.00002109
Iteration 76/1000 | Loss: 0.00002109
Iteration 77/1000 | Loss: 0.00002109
Iteration 78/1000 | Loss: 0.00002108
Iteration 79/1000 | Loss: 0.00002108
Iteration 80/1000 | Loss: 0.00002108
Iteration 81/1000 | Loss: 0.00002108
Iteration 82/1000 | Loss: 0.00002108
Iteration 83/1000 | Loss: 0.00002108
Iteration 84/1000 | Loss: 0.00002108
Iteration 85/1000 | Loss: 0.00002108
Iteration 86/1000 | Loss: 0.00002108
Iteration 87/1000 | Loss: 0.00002108
Iteration 88/1000 | Loss: 0.00002108
Iteration 89/1000 | Loss: 0.00002108
Iteration 90/1000 | Loss: 0.00002108
Iteration 91/1000 | Loss: 0.00002107
Iteration 92/1000 | Loss: 0.00002107
Iteration 93/1000 | Loss: 0.00002107
Iteration 94/1000 | Loss: 0.00002106
Iteration 95/1000 | Loss: 0.00002106
Iteration 96/1000 | Loss: 0.00002106
Iteration 97/1000 | Loss: 0.00002106
Iteration 98/1000 | Loss: 0.00002105
Iteration 99/1000 | Loss: 0.00002105
Iteration 100/1000 | Loss: 0.00002105
Iteration 101/1000 | Loss: 0.00002105
Iteration 102/1000 | Loss: 0.00002105
Iteration 103/1000 | Loss: 0.00002105
Iteration 104/1000 | Loss: 0.00002105
Iteration 105/1000 | Loss: 0.00002105
Iteration 106/1000 | Loss: 0.00002105
Iteration 107/1000 | Loss: 0.00002105
Iteration 108/1000 | Loss: 0.00002105
Iteration 109/1000 | Loss: 0.00002105
Iteration 110/1000 | Loss: 0.00002105
Iteration 111/1000 | Loss: 0.00002105
Iteration 112/1000 | Loss: 0.00002105
Iteration 113/1000 | Loss: 0.00002105
Iteration 114/1000 | Loss: 0.00002105
Iteration 115/1000 | Loss: 0.00002105
Iteration 116/1000 | Loss: 0.00002104
Iteration 117/1000 | Loss: 0.00002104
Iteration 118/1000 | Loss: 0.00002104
Iteration 119/1000 | Loss: 0.00002104
Iteration 120/1000 | Loss: 0.00002104
Iteration 121/1000 | Loss: 0.00002104
Iteration 122/1000 | Loss: 0.00002104
Iteration 123/1000 | Loss: 0.00002104
Iteration 124/1000 | Loss: 0.00002104
Iteration 125/1000 | Loss: 0.00002104
Iteration 126/1000 | Loss: 0.00002104
Iteration 127/1000 | Loss: 0.00002104
Iteration 128/1000 | Loss: 0.00002104
Iteration 129/1000 | Loss: 0.00002104
Iteration 130/1000 | Loss: 0.00002104
Iteration 131/1000 | Loss: 0.00002103
Iteration 132/1000 | Loss: 0.00002103
Iteration 133/1000 | Loss: 0.00002103
Iteration 134/1000 | Loss: 0.00002103
Iteration 135/1000 | Loss: 0.00002103
Iteration 136/1000 | Loss: 0.00002103
Iteration 137/1000 | Loss: 0.00002103
Iteration 138/1000 | Loss: 0.00002103
Iteration 139/1000 | Loss: 0.00002103
Iteration 140/1000 | Loss: 0.00002103
Iteration 141/1000 | Loss: 0.00002103
Iteration 142/1000 | Loss: 0.00002103
Iteration 143/1000 | Loss: 0.00002103
Iteration 144/1000 | Loss: 0.00002102
Iteration 145/1000 | Loss: 0.00002102
Iteration 146/1000 | Loss: 0.00002102
Iteration 147/1000 | Loss: 0.00002102
Iteration 148/1000 | Loss: 0.00002102
Iteration 149/1000 | Loss: 0.00002102
Iteration 150/1000 | Loss: 0.00002102
Iteration 151/1000 | Loss: 0.00002102
Iteration 152/1000 | Loss: 0.00002101
Iteration 153/1000 | Loss: 0.00002101
Iteration 154/1000 | Loss: 0.00002101
Iteration 155/1000 | Loss: 0.00002101
Iteration 156/1000 | Loss: 0.00002101
Iteration 157/1000 | Loss: 0.00002101
Iteration 158/1000 | Loss: 0.00002101
Iteration 159/1000 | Loss: 0.00002101
Iteration 160/1000 | Loss: 0.00002101
Iteration 161/1000 | Loss: 0.00002101
Iteration 162/1000 | Loss: 0.00002101
Iteration 163/1000 | Loss: 0.00002101
Iteration 164/1000 | Loss: 0.00002101
Iteration 165/1000 | Loss: 0.00002101
Iteration 166/1000 | Loss: 0.00002101
Iteration 167/1000 | Loss: 0.00002101
Iteration 168/1000 | Loss: 0.00002101
Iteration 169/1000 | Loss: 0.00002101
Iteration 170/1000 | Loss: 0.00002101
Iteration 171/1000 | Loss: 0.00002101
Iteration 172/1000 | Loss: 0.00002101
Iteration 173/1000 | Loss: 0.00002101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.100948586303275e-05, 2.100948586303275e-05, 2.100948586303275e-05, 2.100948586303275e-05, 2.100948586303275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.100948586303275e-05

Optimization complete. Final v2v error: 3.89044451713562 mm

Highest mean error: 4.678333759307861 mm for frame 37

Lowest mean error: 3.1910717487335205 mm for frame 110

Saving results

Total time: 88.79176640510559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014730
Iteration 2/25 | Loss: 0.01014730
Iteration 3/25 | Loss: 0.01014730
Iteration 4/25 | Loss: 0.01014730
Iteration 5/25 | Loss: 0.01014730
Iteration 6/25 | Loss: 0.01014730
Iteration 7/25 | Loss: 0.01014730
Iteration 8/25 | Loss: 0.01014730
Iteration 9/25 | Loss: 0.01014729
Iteration 10/25 | Loss: 0.01014729
Iteration 11/25 | Loss: 0.01014729
Iteration 12/25 | Loss: 0.01014729
Iteration 13/25 | Loss: 0.01014729
Iteration 14/25 | Loss: 0.01014729
Iteration 15/25 | Loss: 0.01014729
Iteration 16/25 | Loss: 0.01014729
Iteration 17/25 | Loss: 0.01014729
Iteration 18/25 | Loss: 0.01014729
Iteration 19/25 | Loss: 0.01014728
Iteration 20/25 | Loss: 0.01014728
Iteration 21/25 | Loss: 0.01014728
Iteration 22/25 | Loss: 0.01014728
Iteration 23/25 | Loss: 0.01014728
Iteration 24/25 | Loss: 0.01014728
Iteration 25/25 | Loss: 0.01014728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27234173
Iteration 2/25 | Loss: 0.17788409
Iteration 3/25 | Loss: 0.17707834
Iteration 4/25 | Loss: 0.17707829
Iteration 5/25 | Loss: 0.17707829
Iteration 6/25 | Loss: 0.17707829
Iteration 7/25 | Loss: 0.17707829
Iteration 8/25 | Loss: 0.17707829
Iteration 9/25 | Loss: 0.17707829
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.17707829177379608, 0.17707829177379608, 0.17707829177379608, 0.17707829177379608, 0.17707829177379608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17707829177379608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17707829
Iteration 2/1000 | Loss: 0.00372173
Iteration 3/1000 | Loss: 0.00081358
Iteration 4/1000 | Loss: 0.00266481
Iteration 5/1000 | Loss: 0.00124648
Iteration 6/1000 | Loss: 0.00090701
Iteration 7/1000 | Loss: 0.00022883
Iteration 8/1000 | Loss: 0.00005825
Iteration 9/1000 | Loss: 0.00003605
Iteration 10/1000 | Loss: 0.00002848
Iteration 11/1000 | Loss: 0.00002496
Iteration 12/1000 | Loss: 0.00079352
Iteration 13/1000 | Loss: 0.00004597
Iteration 14/1000 | Loss: 0.00098255
Iteration 15/1000 | Loss: 0.00008898
Iteration 16/1000 | Loss: 0.00051685
Iteration 17/1000 | Loss: 0.00033789
Iteration 18/1000 | Loss: 0.00002231
Iteration 19/1000 | Loss: 0.00022682
Iteration 20/1000 | Loss: 0.00001746
Iteration 21/1000 | Loss: 0.00018912
Iteration 22/1000 | Loss: 0.00001965
Iteration 23/1000 | Loss: 0.00001633
Iteration 24/1000 | Loss: 0.00017800
Iteration 25/1000 | Loss: 0.00001524
Iteration 26/1000 | Loss: 0.00001484
Iteration 27/1000 | Loss: 0.00001517
Iteration 28/1000 | Loss: 0.00001409
Iteration 29/1000 | Loss: 0.00016892
Iteration 30/1000 | Loss: 0.00002896
Iteration 31/1000 | Loss: 0.00002012
Iteration 32/1000 | Loss: 0.00001337
Iteration 33/1000 | Loss: 0.00001308
Iteration 34/1000 | Loss: 0.00001300
Iteration 35/1000 | Loss: 0.00001273
Iteration 36/1000 | Loss: 0.00001267
Iteration 37/1000 | Loss: 0.00001259
Iteration 38/1000 | Loss: 0.00001258
Iteration 39/1000 | Loss: 0.00001248
Iteration 40/1000 | Loss: 0.00001233
Iteration 41/1000 | Loss: 0.00001231
Iteration 42/1000 | Loss: 0.00001231
Iteration 43/1000 | Loss: 0.00001231
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001226
Iteration 46/1000 | Loss: 0.00001215
Iteration 47/1000 | Loss: 0.00001210
Iteration 48/1000 | Loss: 0.00001206
Iteration 49/1000 | Loss: 0.00001206
Iteration 50/1000 | Loss: 0.00001206
Iteration 51/1000 | Loss: 0.00001206
Iteration 52/1000 | Loss: 0.00001205
Iteration 53/1000 | Loss: 0.00001205
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001203
Iteration 56/1000 | Loss: 0.00001203
Iteration 57/1000 | Loss: 0.00001202
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001201
Iteration 60/1000 | Loss: 0.00001201
Iteration 61/1000 | Loss: 0.00001201
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001200
Iteration 64/1000 | Loss: 0.00001199
Iteration 65/1000 | Loss: 0.00001198
Iteration 66/1000 | Loss: 0.00001198
Iteration 67/1000 | Loss: 0.00001198
Iteration 68/1000 | Loss: 0.00001198
Iteration 69/1000 | Loss: 0.00001197
Iteration 70/1000 | Loss: 0.00001197
Iteration 71/1000 | Loss: 0.00001197
Iteration 72/1000 | Loss: 0.00001197
Iteration 73/1000 | Loss: 0.00001197
Iteration 74/1000 | Loss: 0.00001197
Iteration 75/1000 | Loss: 0.00001197
Iteration 76/1000 | Loss: 0.00001197
Iteration 77/1000 | Loss: 0.00001197
Iteration 78/1000 | Loss: 0.00001197
Iteration 79/1000 | Loss: 0.00001196
Iteration 80/1000 | Loss: 0.00001196
Iteration 81/1000 | Loss: 0.00001196
Iteration 82/1000 | Loss: 0.00001195
Iteration 83/1000 | Loss: 0.00001195
Iteration 84/1000 | Loss: 0.00001195
Iteration 85/1000 | Loss: 0.00001195
Iteration 86/1000 | Loss: 0.00001195
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001194
Iteration 89/1000 | Loss: 0.00001193
Iteration 90/1000 | Loss: 0.00001193
Iteration 91/1000 | Loss: 0.00001193
Iteration 92/1000 | Loss: 0.00001192
Iteration 93/1000 | Loss: 0.00001192
Iteration 94/1000 | Loss: 0.00001192
Iteration 95/1000 | Loss: 0.00001191
Iteration 96/1000 | Loss: 0.00001191
Iteration 97/1000 | Loss: 0.00001191
Iteration 98/1000 | Loss: 0.00001191
Iteration 99/1000 | Loss: 0.00001191
Iteration 100/1000 | Loss: 0.00001191
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001190
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001190
Iteration 108/1000 | Loss: 0.00001190
Iteration 109/1000 | Loss: 0.00001190
Iteration 110/1000 | Loss: 0.00001189
Iteration 111/1000 | Loss: 0.00001189
Iteration 112/1000 | Loss: 0.00001189
Iteration 113/1000 | Loss: 0.00001189
Iteration 114/1000 | Loss: 0.00001189
Iteration 115/1000 | Loss: 0.00001188
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001188
Iteration 118/1000 | Loss: 0.00001188
Iteration 119/1000 | Loss: 0.00001188
Iteration 120/1000 | Loss: 0.00001188
Iteration 121/1000 | Loss: 0.00001188
Iteration 122/1000 | Loss: 0.00001187
Iteration 123/1000 | Loss: 0.00001187
Iteration 124/1000 | Loss: 0.00001187
Iteration 125/1000 | Loss: 0.00001187
Iteration 126/1000 | Loss: 0.00001187
Iteration 127/1000 | Loss: 0.00001187
Iteration 128/1000 | Loss: 0.00001187
Iteration 129/1000 | Loss: 0.00001187
Iteration 130/1000 | Loss: 0.00001187
Iteration 131/1000 | Loss: 0.00001186
Iteration 132/1000 | Loss: 0.00001186
Iteration 133/1000 | Loss: 0.00001186
Iteration 134/1000 | Loss: 0.00001186
Iteration 135/1000 | Loss: 0.00001186
Iteration 136/1000 | Loss: 0.00001186
Iteration 137/1000 | Loss: 0.00001186
Iteration 138/1000 | Loss: 0.00001186
Iteration 139/1000 | Loss: 0.00001186
Iteration 140/1000 | Loss: 0.00001186
Iteration 141/1000 | Loss: 0.00001185
Iteration 142/1000 | Loss: 0.00001184
Iteration 143/1000 | Loss: 0.00001184
Iteration 144/1000 | Loss: 0.00001183
Iteration 145/1000 | Loss: 0.00001183
Iteration 146/1000 | Loss: 0.00001183
Iteration 147/1000 | Loss: 0.00001183
Iteration 148/1000 | Loss: 0.00001183
Iteration 149/1000 | Loss: 0.00001183
Iteration 150/1000 | Loss: 0.00001183
Iteration 151/1000 | Loss: 0.00001183
Iteration 152/1000 | Loss: 0.00001183
Iteration 153/1000 | Loss: 0.00001183
Iteration 154/1000 | Loss: 0.00001183
Iteration 155/1000 | Loss: 0.00001183
Iteration 156/1000 | Loss: 0.00001183
Iteration 157/1000 | Loss: 0.00001182
Iteration 158/1000 | Loss: 0.00001182
Iteration 159/1000 | Loss: 0.00001182
Iteration 160/1000 | Loss: 0.00001182
Iteration 161/1000 | Loss: 0.00001182
Iteration 162/1000 | Loss: 0.00001182
Iteration 163/1000 | Loss: 0.00001182
Iteration 164/1000 | Loss: 0.00001181
Iteration 165/1000 | Loss: 0.00001181
Iteration 166/1000 | Loss: 0.00001181
Iteration 167/1000 | Loss: 0.00001181
Iteration 168/1000 | Loss: 0.00001181
Iteration 169/1000 | Loss: 0.00001181
Iteration 170/1000 | Loss: 0.00001180
Iteration 171/1000 | Loss: 0.00001180
Iteration 172/1000 | Loss: 0.00001180
Iteration 173/1000 | Loss: 0.00001180
Iteration 174/1000 | Loss: 0.00001180
Iteration 175/1000 | Loss: 0.00001180
Iteration 176/1000 | Loss: 0.00001180
Iteration 177/1000 | Loss: 0.00001180
Iteration 178/1000 | Loss: 0.00001180
Iteration 179/1000 | Loss: 0.00001180
Iteration 180/1000 | Loss: 0.00001179
Iteration 181/1000 | Loss: 0.00001179
Iteration 182/1000 | Loss: 0.00001179
Iteration 183/1000 | Loss: 0.00001179
Iteration 184/1000 | Loss: 0.00001179
Iteration 185/1000 | Loss: 0.00001179
Iteration 186/1000 | Loss: 0.00001179
Iteration 187/1000 | Loss: 0.00001179
Iteration 188/1000 | Loss: 0.00001179
Iteration 189/1000 | Loss: 0.00001179
Iteration 190/1000 | Loss: 0.00001179
Iteration 191/1000 | Loss: 0.00001178
Iteration 192/1000 | Loss: 0.00001178
Iteration 193/1000 | Loss: 0.00001178
Iteration 194/1000 | Loss: 0.00001178
Iteration 195/1000 | Loss: 0.00001178
Iteration 196/1000 | Loss: 0.00001178
Iteration 197/1000 | Loss: 0.00001178
Iteration 198/1000 | Loss: 0.00001178
Iteration 199/1000 | Loss: 0.00001177
Iteration 200/1000 | Loss: 0.00001177
Iteration 201/1000 | Loss: 0.00001177
Iteration 202/1000 | Loss: 0.00001177
Iteration 203/1000 | Loss: 0.00001177
Iteration 204/1000 | Loss: 0.00001177
Iteration 205/1000 | Loss: 0.00001177
Iteration 206/1000 | Loss: 0.00001177
Iteration 207/1000 | Loss: 0.00001177
Iteration 208/1000 | Loss: 0.00001177
Iteration 209/1000 | Loss: 0.00001177
Iteration 210/1000 | Loss: 0.00001177
Iteration 211/1000 | Loss: 0.00001177
Iteration 212/1000 | Loss: 0.00001176
Iteration 213/1000 | Loss: 0.00001176
Iteration 214/1000 | Loss: 0.00001176
Iteration 215/1000 | Loss: 0.00001176
Iteration 216/1000 | Loss: 0.00001176
Iteration 217/1000 | Loss: 0.00001176
Iteration 218/1000 | Loss: 0.00001176
Iteration 219/1000 | Loss: 0.00001176
Iteration 220/1000 | Loss: 0.00001175
Iteration 221/1000 | Loss: 0.00001175
Iteration 222/1000 | Loss: 0.00001175
Iteration 223/1000 | Loss: 0.00001175
Iteration 224/1000 | Loss: 0.00001175
Iteration 225/1000 | Loss: 0.00001175
Iteration 226/1000 | Loss: 0.00001175
Iteration 227/1000 | Loss: 0.00001175
Iteration 228/1000 | Loss: 0.00001174
Iteration 229/1000 | Loss: 0.00001174
Iteration 230/1000 | Loss: 0.00001174
Iteration 231/1000 | Loss: 0.00001174
Iteration 232/1000 | Loss: 0.00001174
Iteration 233/1000 | Loss: 0.00001174
Iteration 234/1000 | Loss: 0.00001173
Iteration 235/1000 | Loss: 0.00001173
Iteration 236/1000 | Loss: 0.00001173
Iteration 237/1000 | Loss: 0.00001173
Iteration 238/1000 | Loss: 0.00001173
Iteration 239/1000 | Loss: 0.00001172
Iteration 240/1000 | Loss: 0.00001172
Iteration 241/1000 | Loss: 0.00001172
Iteration 242/1000 | Loss: 0.00001172
Iteration 243/1000 | Loss: 0.00001172
Iteration 244/1000 | Loss: 0.00001172
Iteration 245/1000 | Loss: 0.00001172
Iteration 246/1000 | Loss: 0.00001172
Iteration 247/1000 | Loss: 0.00001172
Iteration 248/1000 | Loss: 0.00001172
Iteration 249/1000 | Loss: 0.00001172
Iteration 250/1000 | Loss: 0.00001172
Iteration 251/1000 | Loss: 0.00001172
Iteration 252/1000 | Loss: 0.00001172
Iteration 253/1000 | Loss: 0.00001172
Iteration 254/1000 | Loss: 0.00001172
Iteration 255/1000 | Loss: 0.00001172
Iteration 256/1000 | Loss: 0.00001172
Iteration 257/1000 | Loss: 0.00001172
Iteration 258/1000 | Loss: 0.00001172
Iteration 259/1000 | Loss: 0.00001172
Iteration 260/1000 | Loss: 0.00001172
Iteration 261/1000 | Loss: 0.00001172
Iteration 262/1000 | Loss: 0.00001172
Iteration 263/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.1718723726517055e-05, 1.1718723726517055e-05, 1.1718723726517055e-05, 1.1718723726517055e-05, 1.1718723726517055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1718723726517055e-05

Optimization complete. Final v2v error: 2.874831438064575 mm

Highest mean error: 8.69665813446045 mm for frame 178

Lowest mean error: 2.709224224090576 mm for frame 224

Saving results

Total time: 85.40507125854492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006230
Iteration 2/25 | Loss: 0.00362719
Iteration 3/25 | Loss: 0.00297064
Iteration 4/25 | Loss: 0.00263169
Iteration 5/25 | Loss: 0.00240582
Iteration 6/25 | Loss: 0.00217170
Iteration 7/25 | Loss: 0.00210140
Iteration 8/25 | Loss: 0.00206290
Iteration 9/25 | Loss: 0.00203135
Iteration 10/25 | Loss: 0.00202387
Iteration 11/25 | Loss: 0.00201187
Iteration 12/25 | Loss: 0.00199372
Iteration 13/25 | Loss: 0.00198896
Iteration 14/25 | Loss: 0.00199022
Iteration 15/25 | Loss: 0.00198225
Iteration 16/25 | Loss: 0.00197446
Iteration 17/25 | Loss: 0.00196963
Iteration 18/25 | Loss: 0.00196898
Iteration 19/25 | Loss: 0.00196504
Iteration 20/25 | Loss: 0.00196533
Iteration 21/25 | Loss: 0.00196778
Iteration 22/25 | Loss: 0.00196299
Iteration 23/25 | Loss: 0.00196551
Iteration 24/25 | Loss: 0.00196101
Iteration 25/25 | Loss: 0.00197302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26241779
Iteration 2/25 | Loss: 0.01157917
Iteration 3/25 | Loss: 0.01160497
Iteration 4/25 | Loss: 0.01160495
Iteration 5/25 | Loss: 0.00994321
Iteration 6/25 | Loss: 0.00991684
Iteration 7/25 | Loss: 0.00987906
Iteration 8/25 | Loss: 0.00987905
Iteration 9/25 | Loss: 0.00987831
Iteration 10/25 | Loss: 0.00987831
Iteration 11/25 | Loss: 0.00987831
Iteration 12/25 | Loss: 0.00987831
Iteration 13/25 | Loss: 0.00987831
Iteration 14/25 | Loss: 0.00987831
Iteration 15/25 | Loss: 0.00987831
Iteration 16/25 | Loss: 0.00987831
Iteration 17/25 | Loss: 0.00987831
Iteration 18/25 | Loss: 0.00987831
Iteration 19/25 | Loss: 0.00987831
Iteration 20/25 | Loss: 0.00987831
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.009878307580947876, 0.009878307580947876, 0.009878307580947876, 0.009878307580947876, 0.009878307580947876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009878307580947876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00987831
Iteration 2/1000 | Loss: 0.00187359
Iteration 3/1000 | Loss: 0.00254066
Iteration 4/1000 | Loss: 0.00092444
Iteration 5/1000 | Loss: 0.00063148
Iteration 6/1000 | Loss: 0.00113234
Iteration 7/1000 | Loss: 0.00058726
Iteration 8/1000 | Loss: 0.00094998
Iteration 9/1000 | Loss: 0.00123458
Iteration 10/1000 | Loss: 0.00376488
Iteration 11/1000 | Loss: 0.00080594
Iteration 12/1000 | Loss: 0.00050322
Iteration 13/1000 | Loss: 0.00063525
Iteration 14/1000 | Loss: 0.00090267
Iteration 15/1000 | Loss: 0.00047908
Iteration 16/1000 | Loss: 0.00044697
Iteration 17/1000 | Loss: 0.00049196
Iteration 18/1000 | Loss: 0.00050158
Iteration 19/1000 | Loss: 0.00051370
Iteration 20/1000 | Loss: 0.00051071
Iteration 21/1000 | Loss: 0.00045585
Iteration 22/1000 | Loss: 0.00039651
Iteration 23/1000 | Loss: 0.00052141
Iteration 24/1000 | Loss: 0.00064832
Iteration 25/1000 | Loss: 0.00048643
Iteration 26/1000 | Loss: 0.00037821
Iteration 27/1000 | Loss: 0.00037633
Iteration 28/1000 | Loss: 0.00046594
Iteration 29/1000 | Loss: 0.00033549
Iteration 30/1000 | Loss: 0.00075705
Iteration 31/1000 | Loss: 0.00039531
Iteration 32/1000 | Loss: 0.00039125
Iteration 33/1000 | Loss: 0.00032937
Iteration 34/1000 | Loss: 0.00041030
Iteration 35/1000 | Loss: 0.00068411
Iteration 36/1000 | Loss: 0.00048754
Iteration 37/1000 | Loss: 0.00032531
Iteration 38/1000 | Loss: 0.00036177
Iteration 39/1000 | Loss: 0.00057098
Iteration 40/1000 | Loss: 0.00038749
Iteration 41/1000 | Loss: 0.00036844
Iteration 42/1000 | Loss: 0.00041762
Iteration 43/1000 | Loss: 0.00050916
Iteration 44/1000 | Loss: 0.00031620
Iteration 45/1000 | Loss: 0.00047483
Iteration 46/1000 | Loss: 0.00057565
Iteration 47/1000 | Loss: 0.00033222
Iteration 48/1000 | Loss: 0.00031923
Iteration 49/1000 | Loss: 0.00032979
Iteration 50/1000 | Loss: 0.00231332
Iteration 51/1000 | Loss: 0.00060736
Iteration 52/1000 | Loss: 0.00031231
Iteration 53/1000 | Loss: 0.00047058
Iteration 54/1000 | Loss: 0.00037496
Iteration 55/1000 | Loss: 0.00030839
Iteration 56/1000 | Loss: 0.00032411
Iteration 57/1000 | Loss: 0.00030262
Iteration 58/1000 | Loss: 0.00034125
Iteration 59/1000 | Loss: 0.00108930
Iteration 60/1000 | Loss: 0.00041570
Iteration 61/1000 | Loss: 0.00034884
Iteration 62/1000 | Loss: 0.00031328
Iteration 63/1000 | Loss: 0.00030123
Iteration 64/1000 | Loss: 0.00044482
Iteration 65/1000 | Loss: 0.00029727
Iteration 66/1000 | Loss: 0.00061801
Iteration 67/1000 | Loss: 0.00123297
Iteration 68/1000 | Loss: 0.00041496
Iteration 69/1000 | Loss: 0.00053816
Iteration 70/1000 | Loss: 0.00030155
Iteration 71/1000 | Loss: 0.00033072
Iteration 72/1000 | Loss: 0.00029840
Iteration 73/1000 | Loss: 0.00042282
Iteration 74/1000 | Loss: 0.00106310
Iteration 75/1000 | Loss: 0.00042476
Iteration 76/1000 | Loss: 0.00030866
Iteration 77/1000 | Loss: 0.00030885
Iteration 78/1000 | Loss: 0.00037877
Iteration 79/1000 | Loss: 0.00063893
Iteration 80/1000 | Loss: 0.00029908
Iteration 81/1000 | Loss: 0.00033017
Iteration 82/1000 | Loss: 0.00029374
Iteration 83/1000 | Loss: 0.00043663
Iteration 84/1000 | Loss: 0.00108942
Iteration 85/1000 | Loss: 0.00051539
Iteration 86/1000 | Loss: 0.00041898
Iteration 87/1000 | Loss: 0.00038782
Iteration 88/1000 | Loss: 0.00029101
Iteration 89/1000 | Loss: 0.00028688
Iteration 90/1000 | Loss: 0.00029192
Iteration 91/1000 | Loss: 0.00031763
Iteration 92/1000 | Loss: 0.00028642
Iteration 93/1000 | Loss: 0.00036762
Iteration 94/1000 | Loss: 0.00049830
Iteration 95/1000 | Loss: 0.00029922
Iteration 96/1000 | Loss: 0.00028474
Iteration 97/1000 | Loss: 0.00046785
Iteration 98/1000 | Loss: 0.00039862
Iteration 99/1000 | Loss: 0.00043022
Iteration 100/1000 | Loss: 0.00030589
Iteration 101/1000 | Loss: 0.00028202
Iteration 102/1000 | Loss: 0.00028172
Iteration 103/1000 | Loss: 0.00028143
Iteration 104/1000 | Loss: 0.00029918
Iteration 105/1000 | Loss: 0.00028097
Iteration 106/1000 | Loss: 0.00037598
Iteration 107/1000 | Loss: 0.00029764
Iteration 108/1000 | Loss: 0.00032318
Iteration 109/1000 | Loss: 0.00038945
Iteration 110/1000 | Loss: 0.00030169
Iteration 111/1000 | Loss: 0.00028497
Iteration 112/1000 | Loss: 0.00028062
Iteration 113/1000 | Loss: 0.00033858
Iteration 114/1000 | Loss: 0.00061475
Iteration 115/1000 | Loss: 0.00029799
Iteration 116/1000 | Loss: 0.00028050
Iteration 117/1000 | Loss: 0.00029520
Iteration 118/1000 | Loss: 0.00028030
Iteration 119/1000 | Loss: 0.00028023
Iteration 120/1000 | Loss: 0.00028015
Iteration 121/1000 | Loss: 0.00035540
Iteration 122/1000 | Loss: 0.00028078
Iteration 123/1000 | Loss: 0.00028016
Iteration 124/1000 | Loss: 0.00028009
Iteration 125/1000 | Loss: 0.00030074
Iteration 126/1000 | Loss: 0.00028069
Iteration 127/1000 | Loss: 0.00028091
Iteration 128/1000 | Loss: 0.00028003
Iteration 129/1000 | Loss: 0.00028003
Iteration 130/1000 | Loss: 0.00028003
Iteration 131/1000 | Loss: 0.00028003
Iteration 132/1000 | Loss: 0.00028003
Iteration 133/1000 | Loss: 0.00028002
Iteration 134/1000 | Loss: 0.00028002
Iteration 135/1000 | Loss: 0.00028002
Iteration 136/1000 | Loss: 0.00028001
Iteration 137/1000 | Loss: 0.00028000
Iteration 138/1000 | Loss: 0.00027989
Iteration 139/1000 | Loss: 0.00027979
Iteration 140/1000 | Loss: 0.00027978
Iteration 141/1000 | Loss: 0.00027978
Iteration 142/1000 | Loss: 0.00027976
Iteration 143/1000 | Loss: 0.00027976
Iteration 144/1000 | Loss: 0.00027975
Iteration 145/1000 | Loss: 0.00027974
Iteration 146/1000 | Loss: 0.00027974
Iteration 147/1000 | Loss: 0.00027974
Iteration 148/1000 | Loss: 0.00030419
Iteration 149/1000 | Loss: 0.00029264
Iteration 150/1000 | Loss: 0.00027964
Iteration 151/1000 | Loss: 0.00027963
Iteration 152/1000 | Loss: 0.00028650
Iteration 153/1000 | Loss: 0.00027955
Iteration 154/1000 | Loss: 0.00027955
Iteration 155/1000 | Loss: 0.00027955
Iteration 156/1000 | Loss: 0.00027955
Iteration 157/1000 | Loss: 0.00027955
Iteration 158/1000 | Loss: 0.00027954
Iteration 159/1000 | Loss: 0.00027954
Iteration 160/1000 | Loss: 0.00027954
Iteration 161/1000 | Loss: 0.00027954
Iteration 162/1000 | Loss: 0.00027954
Iteration 163/1000 | Loss: 0.00027954
Iteration 164/1000 | Loss: 0.00027954
Iteration 165/1000 | Loss: 0.00027954
Iteration 166/1000 | Loss: 0.00028882
Iteration 167/1000 | Loss: 0.00028882
Iteration 168/1000 | Loss: 0.00033694
Iteration 169/1000 | Loss: 0.00033798
Iteration 170/1000 | Loss: 0.00028045
Iteration 171/1000 | Loss: 0.00027964
Iteration 172/1000 | Loss: 0.00027953
Iteration 173/1000 | Loss: 0.00034534
Iteration 174/1000 | Loss: 0.00028685
Iteration 175/1000 | Loss: 0.00029476
Iteration 176/1000 | Loss: 0.00073835
Iteration 177/1000 | Loss: 0.00034358
Iteration 178/1000 | Loss: 0.00029345
Iteration 179/1000 | Loss: 0.00028473
Iteration 180/1000 | Loss: 0.00033877
Iteration 181/1000 | Loss: 0.00028026
Iteration 182/1000 | Loss: 0.00027943
Iteration 183/1000 | Loss: 0.00027943
Iteration 184/1000 | Loss: 0.00027943
Iteration 185/1000 | Loss: 0.00027943
Iteration 186/1000 | Loss: 0.00027943
Iteration 187/1000 | Loss: 0.00027943
Iteration 188/1000 | Loss: 0.00027943
Iteration 189/1000 | Loss: 0.00027943
Iteration 190/1000 | Loss: 0.00027943
Iteration 191/1000 | Loss: 0.00027943
Iteration 192/1000 | Loss: 0.00027942
Iteration 193/1000 | Loss: 0.00027944
Iteration 194/1000 | Loss: 0.00027942
Iteration 195/1000 | Loss: 0.00027942
Iteration 196/1000 | Loss: 0.00027942
Iteration 197/1000 | Loss: 0.00027942
Iteration 198/1000 | Loss: 0.00027942
Iteration 199/1000 | Loss: 0.00027942
Iteration 200/1000 | Loss: 0.00027942
Iteration 201/1000 | Loss: 0.00027941
Iteration 202/1000 | Loss: 0.00027940
Iteration 203/1000 | Loss: 0.00027940
Iteration 204/1000 | Loss: 0.00027940
Iteration 205/1000 | Loss: 0.00027940
Iteration 206/1000 | Loss: 0.00027938
Iteration 207/1000 | Loss: 0.00027937
Iteration 208/1000 | Loss: 0.00027937
Iteration 209/1000 | Loss: 0.00027937
Iteration 210/1000 | Loss: 0.00027937
Iteration 211/1000 | Loss: 0.00027936
Iteration 212/1000 | Loss: 0.00027935
Iteration 213/1000 | Loss: 0.00027935
Iteration 214/1000 | Loss: 0.00027935
Iteration 215/1000 | Loss: 0.00027935
Iteration 216/1000 | Loss: 0.00027935
Iteration 217/1000 | Loss: 0.00027935
Iteration 218/1000 | Loss: 0.00027935
Iteration 219/1000 | Loss: 0.00027935
Iteration 220/1000 | Loss: 0.00027935
Iteration 221/1000 | Loss: 0.00027934
Iteration 222/1000 | Loss: 0.00027934
Iteration 223/1000 | Loss: 0.00027934
Iteration 224/1000 | Loss: 0.00027934
Iteration 225/1000 | Loss: 0.00027934
Iteration 226/1000 | Loss: 0.00027934
Iteration 227/1000 | Loss: 0.00027934
Iteration 228/1000 | Loss: 0.00027934
Iteration 229/1000 | Loss: 0.00027934
Iteration 230/1000 | Loss: 0.00027933
Iteration 231/1000 | Loss: 0.00027933
Iteration 232/1000 | Loss: 0.00027933
Iteration 233/1000 | Loss: 0.00027933
Iteration 234/1000 | Loss: 0.00027933
Iteration 235/1000 | Loss: 0.00027933
Iteration 236/1000 | Loss: 0.00027933
Iteration 237/1000 | Loss: 0.00027933
Iteration 238/1000 | Loss: 0.00027933
Iteration 239/1000 | Loss: 0.00027932
Iteration 240/1000 | Loss: 0.00027932
Iteration 241/1000 | Loss: 0.00027932
Iteration 242/1000 | Loss: 0.00027932
Iteration 243/1000 | Loss: 0.00027932
Iteration 244/1000 | Loss: 0.00027932
Iteration 245/1000 | Loss: 0.00027931
Iteration 246/1000 | Loss: 0.00027931
Iteration 247/1000 | Loss: 0.00027931
Iteration 248/1000 | Loss: 0.00027931
Iteration 249/1000 | Loss: 0.00027931
Iteration 250/1000 | Loss: 0.00027930
Iteration 251/1000 | Loss: 0.00027930
Iteration 252/1000 | Loss: 0.00027930
Iteration 253/1000 | Loss: 0.00027930
Iteration 254/1000 | Loss: 0.00027930
Iteration 255/1000 | Loss: 0.00027930
Iteration 256/1000 | Loss: 0.00027930
Iteration 257/1000 | Loss: 0.00027930
Iteration 258/1000 | Loss: 0.00027930
Iteration 259/1000 | Loss: 0.00027930
Iteration 260/1000 | Loss: 0.00027930
Iteration 261/1000 | Loss: 0.00027930
Iteration 262/1000 | Loss: 0.00027930
Iteration 263/1000 | Loss: 0.00027930
Iteration 264/1000 | Loss: 0.00027929
Iteration 265/1000 | Loss: 0.00027929
Iteration 266/1000 | Loss: 0.00027929
Iteration 267/1000 | Loss: 0.00027929
Iteration 268/1000 | Loss: 0.00027929
Iteration 269/1000 | Loss: 0.00027929
Iteration 270/1000 | Loss: 0.00027929
Iteration 271/1000 | Loss: 0.00027929
Iteration 272/1000 | Loss: 0.00027929
Iteration 273/1000 | Loss: 0.00027929
Iteration 274/1000 | Loss: 0.00027929
Iteration 275/1000 | Loss: 0.00027929
Iteration 276/1000 | Loss: 0.00027929
Iteration 277/1000 | Loss: 0.00027929
Iteration 278/1000 | Loss: 0.00027929
Iteration 279/1000 | Loss: 0.00027929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [0.0002792918821796775, 0.0002792918821796775, 0.0002792918821796775, 0.0002792918821796775, 0.0002792918821796775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002792918821796775

Optimization complete. Final v2v error: 9.422304153442383 mm

Highest mean error: 12.404939651489258 mm for frame 161

Lowest mean error: 4.82581901550293 mm for frame 226

Saving results

Total time: 296.1877410411835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849997
Iteration 2/25 | Loss: 0.00135935
Iteration 3/25 | Loss: 0.00130563
Iteration 4/25 | Loss: 0.00129450
Iteration 5/25 | Loss: 0.00129119
Iteration 6/25 | Loss: 0.00129070
Iteration 7/25 | Loss: 0.00129070
Iteration 8/25 | Loss: 0.00129070
Iteration 9/25 | Loss: 0.00129070
Iteration 10/25 | Loss: 0.00129070
Iteration 11/25 | Loss: 0.00129065
Iteration 12/25 | Loss: 0.00129065
Iteration 13/25 | Loss: 0.00129065
Iteration 14/25 | Loss: 0.00129065
Iteration 15/25 | Loss: 0.00129065
Iteration 16/25 | Loss: 0.00129065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012906469637528062, 0.0012906469637528062, 0.0012906469637528062, 0.0012906469637528062, 0.0012906469637528062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012906469637528062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27116787
Iteration 2/25 | Loss: 0.00192511
Iteration 3/25 | Loss: 0.00192509
Iteration 4/25 | Loss: 0.00192509
Iteration 5/25 | Loss: 0.00192509
Iteration 6/25 | Loss: 0.00192509
Iteration 7/25 | Loss: 0.00192509
Iteration 8/25 | Loss: 0.00192509
Iteration 9/25 | Loss: 0.00192509
Iteration 10/25 | Loss: 0.00192509
Iteration 11/25 | Loss: 0.00192509
Iteration 12/25 | Loss: 0.00192509
Iteration 13/25 | Loss: 0.00192509
Iteration 14/25 | Loss: 0.00192509
Iteration 15/25 | Loss: 0.00192509
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0019250893965363503, 0.0019250893965363503, 0.0019250893965363503, 0.0019250893965363503, 0.0019250893965363503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019250893965363503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192509
Iteration 2/1000 | Loss: 0.00002998
Iteration 3/1000 | Loss: 0.00002063
Iteration 4/1000 | Loss: 0.00001817
Iteration 5/1000 | Loss: 0.00001686
Iteration 6/1000 | Loss: 0.00001588
Iteration 7/1000 | Loss: 0.00001524
Iteration 8/1000 | Loss: 0.00001477
Iteration 9/1000 | Loss: 0.00001455
Iteration 10/1000 | Loss: 0.00001434
Iteration 11/1000 | Loss: 0.00001410
Iteration 12/1000 | Loss: 0.00001406
Iteration 13/1000 | Loss: 0.00001397
Iteration 14/1000 | Loss: 0.00001379
Iteration 15/1000 | Loss: 0.00001376
Iteration 16/1000 | Loss: 0.00001369
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001365
Iteration 19/1000 | Loss: 0.00001360
Iteration 20/1000 | Loss: 0.00001359
Iteration 21/1000 | Loss: 0.00001359
Iteration 22/1000 | Loss: 0.00001359
Iteration 23/1000 | Loss: 0.00001358
Iteration 24/1000 | Loss: 0.00001358
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00001353
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001351
Iteration 30/1000 | Loss: 0.00001351
Iteration 31/1000 | Loss: 0.00001351
Iteration 32/1000 | Loss: 0.00001351
Iteration 33/1000 | Loss: 0.00001350
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001349
Iteration 36/1000 | Loss: 0.00001349
Iteration 37/1000 | Loss: 0.00001349
Iteration 38/1000 | Loss: 0.00001349
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001348
Iteration 42/1000 | Loss: 0.00001348
Iteration 43/1000 | Loss: 0.00001348
Iteration 44/1000 | Loss: 0.00001348
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001346
Iteration 51/1000 | Loss: 0.00001346
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001345
Iteration 55/1000 | Loss: 0.00001345
Iteration 56/1000 | Loss: 0.00001344
Iteration 57/1000 | Loss: 0.00001344
Iteration 58/1000 | Loss: 0.00001344
Iteration 59/1000 | Loss: 0.00001343
Iteration 60/1000 | Loss: 0.00001343
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001342
Iteration 65/1000 | Loss: 0.00001342
Iteration 66/1000 | Loss: 0.00001342
Iteration 67/1000 | Loss: 0.00001342
Iteration 68/1000 | Loss: 0.00001341
Iteration 69/1000 | Loss: 0.00001341
Iteration 70/1000 | Loss: 0.00001341
Iteration 71/1000 | Loss: 0.00001341
Iteration 72/1000 | Loss: 0.00001341
Iteration 73/1000 | Loss: 0.00001341
Iteration 74/1000 | Loss: 0.00001341
Iteration 75/1000 | Loss: 0.00001340
Iteration 76/1000 | Loss: 0.00001340
Iteration 77/1000 | Loss: 0.00001340
Iteration 78/1000 | Loss: 0.00001340
Iteration 79/1000 | Loss: 0.00001340
Iteration 80/1000 | Loss: 0.00001340
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001340
Iteration 83/1000 | Loss: 0.00001340
Iteration 84/1000 | Loss: 0.00001339
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001339
Iteration 87/1000 | Loss: 0.00001339
Iteration 88/1000 | Loss: 0.00001339
Iteration 89/1000 | Loss: 0.00001339
Iteration 90/1000 | Loss: 0.00001338
Iteration 91/1000 | Loss: 0.00001338
Iteration 92/1000 | Loss: 0.00001338
Iteration 93/1000 | Loss: 0.00001338
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001337
Iteration 96/1000 | Loss: 0.00001337
Iteration 97/1000 | Loss: 0.00001337
Iteration 98/1000 | Loss: 0.00001337
Iteration 99/1000 | Loss: 0.00001337
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001337
Iteration 102/1000 | Loss: 0.00001336
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001336
Iteration 105/1000 | Loss: 0.00001336
Iteration 106/1000 | Loss: 0.00001335
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00001335
Iteration 109/1000 | Loss: 0.00001334
Iteration 110/1000 | Loss: 0.00001334
Iteration 111/1000 | Loss: 0.00001334
Iteration 112/1000 | Loss: 0.00001334
Iteration 113/1000 | Loss: 0.00001334
Iteration 114/1000 | Loss: 0.00001334
Iteration 115/1000 | Loss: 0.00001333
Iteration 116/1000 | Loss: 0.00001333
Iteration 117/1000 | Loss: 0.00001333
Iteration 118/1000 | Loss: 0.00001333
Iteration 119/1000 | Loss: 0.00001333
Iteration 120/1000 | Loss: 0.00001333
Iteration 121/1000 | Loss: 0.00001333
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001332
Iteration 124/1000 | Loss: 0.00001332
Iteration 125/1000 | Loss: 0.00001332
Iteration 126/1000 | Loss: 0.00001332
Iteration 127/1000 | Loss: 0.00001332
Iteration 128/1000 | Loss: 0.00001332
Iteration 129/1000 | Loss: 0.00001332
Iteration 130/1000 | Loss: 0.00001332
Iteration 131/1000 | Loss: 0.00001332
Iteration 132/1000 | Loss: 0.00001332
Iteration 133/1000 | Loss: 0.00001332
Iteration 134/1000 | Loss: 0.00001332
Iteration 135/1000 | Loss: 0.00001331
Iteration 136/1000 | Loss: 0.00001331
Iteration 137/1000 | Loss: 0.00001331
Iteration 138/1000 | Loss: 0.00001331
Iteration 139/1000 | Loss: 0.00001331
Iteration 140/1000 | Loss: 0.00001331
Iteration 141/1000 | Loss: 0.00001331
Iteration 142/1000 | Loss: 0.00001331
Iteration 143/1000 | Loss: 0.00001331
Iteration 144/1000 | Loss: 0.00001331
Iteration 145/1000 | Loss: 0.00001331
Iteration 146/1000 | Loss: 0.00001331
Iteration 147/1000 | Loss: 0.00001331
Iteration 148/1000 | Loss: 0.00001331
Iteration 149/1000 | Loss: 0.00001331
Iteration 150/1000 | Loss: 0.00001330
Iteration 151/1000 | Loss: 0.00001330
Iteration 152/1000 | Loss: 0.00001330
Iteration 153/1000 | Loss: 0.00001330
Iteration 154/1000 | Loss: 0.00001330
Iteration 155/1000 | Loss: 0.00001330
Iteration 156/1000 | Loss: 0.00001330
Iteration 157/1000 | Loss: 0.00001330
Iteration 158/1000 | Loss: 0.00001330
Iteration 159/1000 | Loss: 0.00001330
Iteration 160/1000 | Loss: 0.00001330
Iteration 161/1000 | Loss: 0.00001330
Iteration 162/1000 | Loss: 0.00001330
Iteration 163/1000 | Loss: 0.00001330
Iteration 164/1000 | Loss: 0.00001330
Iteration 165/1000 | Loss: 0.00001330
Iteration 166/1000 | Loss: 0.00001329
Iteration 167/1000 | Loss: 0.00001329
Iteration 168/1000 | Loss: 0.00001329
Iteration 169/1000 | Loss: 0.00001329
Iteration 170/1000 | Loss: 0.00001329
Iteration 171/1000 | Loss: 0.00001329
Iteration 172/1000 | Loss: 0.00001329
Iteration 173/1000 | Loss: 0.00001329
Iteration 174/1000 | Loss: 0.00001329
Iteration 175/1000 | Loss: 0.00001329
Iteration 176/1000 | Loss: 0.00001329
Iteration 177/1000 | Loss: 0.00001329
Iteration 178/1000 | Loss: 0.00001329
Iteration 179/1000 | Loss: 0.00001329
Iteration 180/1000 | Loss: 0.00001329
Iteration 181/1000 | Loss: 0.00001329
Iteration 182/1000 | Loss: 0.00001329
Iteration 183/1000 | Loss: 0.00001329
Iteration 184/1000 | Loss: 0.00001329
Iteration 185/1000 | Loss: 0.00001329
Iteration 186/1000 | Loss: 0.00001329
Iteration 187/1000 | Loss: 0.00001329
Iteration 188/1000 | Loss: 0.00001329
Iteration 189/1000 | Loss: 0.00001329
Iteration 190/1000 | Loss: 0.00001329
Iteration 191/1000 | Loss: 0.00001329
Iteration 192/1000 | Loss: 0.00001329
Iteration 193/1000 | Loss: 0.00001329
Iteration 194/1000 | Loss: 0.00001329
Iteration 195/1000 | Loss: 0.00001329
Iteration 196/1000 | Loss: 0.00001329
Iteration 197/1000 | Loss: 0.00001329
Iteration 198/1000 | Loss: 0.00001329
Iteration 199/1000 | Loss: 0.00001329
Iteration 200/1000 | Loss: 0.00001329
Iteration 201/1000 | Loss: 0.00001329
Iteration 202/1000 | Loss: 0.00001329
Iteration 203/1000 | Loss: 0.00001329
Iteration 204/1000 | Loss: 0.00001329
Iteration 205/1000 | Loss: 0.00001329
Iteration 206/1000 | Loss: 0.00001329
Iteration 207/1000 | Loss: 0.00001329
Iteration 208/1000 | Loss: 0.00001329
Iteration 209/1000 | Loss: 0.00001329
Iteration 210/1000 | Loss: 0.00001329
Iteration 211/1000 | Loss: 0.00001329
Iteration 212/1000 | Loss: 0.00001329
Iteration 213/1000 | Loss: 0.00001329
Iteration 214/1000 | Loss: 0.00001329
Iteration 215/1000 | Loss: 0.00001329
Iteration 216/1000 | Loss: 0.00001329
Iteration 217/1000 | Loss: 0.00001329
Iteration 218/1000 | Loss: 0.00001329
Iteration 219/1000 | Loss: 0.00001329
Iteration 220/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.329273345618276e-05, 1.329273345618276e-05, 1.329273345618276e-05, 1.329273345618276e-05, 1.329273345618276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.329273345618276e-05

Optimization complete. Final v2v error: 3.0362491607666016 mm

Highest mean error: 4.001673698425293 mm for frame 39

Lowest mean error: 2.5692548751831055 mm for frame 70

Saving results

Total time: 39.23920512199402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838702
Iteration 2/25 | Loss: 0.00150288
Iteration 3/25 | Loss: 0.00135590
Iteration 4/25 | Loss: 0.00133598
Iteration 5/25 | Loss: 0.00132889
Iteration 6/25 | Loss: 0.00132691
Iteration 7/25 | Loss: 0.00132631
Iteration 8/25 | Loss: 0.00132631
Iteration 9/25 | Loss: 0.00132631
Iteration 10/25 | Loss: 0.00132631
Iteration 11/25 | Loss: 0.00132631
Iteration 12/25 | Loss: 0.00132631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013263141736388206, 0.0013263141736388206, 0.0013263141736388206, 0.0013263141736388206, 0.0013263141736388206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013263141736388206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31763601
Iteration 2/25 | Loss: 0.00159052
Iteration 3/25 | Loss: 0.00159052
Iteration 4/25 | Loss: 0.00159052
Iteration 5/25 | Loss: 0.00159052
Iteration 6/25 | Loss: 0.00159052
Iteration 7/25 | Loss: 0.00159052
Iteration 8/25 | Loss: 0.00159052
Iteration 9/25 | Loss: 0.00159052
Iteration 10/25 | Loss: 0.00159052
Iteration 11/25 | Loss: 0.00159052
Iteration 12/25 | Loss: 0.00159052
Iteration 13/25 | Loss: 0.00159052
Iteration 14/25 | Loss: 0.00159052
Iteration 15/25 | Loss: 0.00159052
Iteration 16/25 | Loss: 0.00159052
Iteration 17/25 | Loss: 0.00159052
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001590518280863762, 0.001590518280863762, 0.001590518280863762, 0.001590518280863762, 0.001590518280863762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001590518280863762

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159052
Iteration 2/1000 | Loss: 0.00005665
Iteration 3/1000 | Loss: 0.00003591
Iteration 4/1000 | Loss: 0.00002607
Iteration 5/1000 | Loss: 0.00002332
Iteration 6/1000 | Loss: 0.00002172
Iteration 7/1000 | Loss: 0.00002052
Iteration 8/1000 | Loss: 0.00001996
Iteration 9/1000 | Loss: 0.00001943
Iteration 10/1000 | Loss: 0.00001900
Iteration 11/1000 | Loss: 0.00001870
Iteration 12/1000 | Loss: 0.00001847
Iteration 13/1000 | Loss: 0.00001826
Iteration 14/1000 | Loss: 0.00001806
Iteration 15/1000 | Loss: 0.00001805
Iteration 16/1000 | Loss: 0.00001793
Iteration 17/1000 | Loss: 0.00001791
Iteration 18/1000 | Loss: 0.00001790
Iteration 19/1000 | Loss: 0.00001788
Iteration 20/1000 | Loss: 0.00001783
Iteration 21/1000 | Loss: 0.00001777
Iteration 22/1000 | Loss: 0.00001773
Iteration 23/1000 | Loss: 0.00001772
Iteration 24/1000 | Loss: 0.00001768
Iteration 25/1000 | Loss: 0.00001762
Iteration 26/1000 | Loss: 0.00001761
Iteration 27/1000 | Loss: 0.00001761
Iteration 28/1000 | Loss: 0.00001760
Iteration 29/1000 | Loss: 0.00001759
Iteration 30/1000 | Loss: 0.00001758
Iteration 31/1000 | Loss: 0.00001757
Iteration 32/1000 | Loss: 0.00001757
Iteration 33/1000 | Loss: 0.00001756
Iteration 34/1000 | Loss: 0.00001756
Iteration 35/1000 | Loss: 0.00001755
Iteration 36/1000 | Loss: 0.00001755
Iteration 37/1000 | Loss: 0.00001755
Iteration 38/1000 | Loss: 0.00001754
Iteration 39/1000 | Loss: 0.00001754
Iteration 40/1000 | Loss: 0.00001754
Iteration 41/1000 | Loss: 0.00001754
Iteration 42/1000 | Loss: 0.00001754
Iteration 43/1000 | Loss: 0.00001754
Iteration 44/1000 | Loss: 0.00001754
Iteration 45/1000 | Loss: 0.00001753
Iteration 46/1000 | Loss: 0.00001753
Iteration 47/1000 | Loss: 0.00001753
Iteration 48/1000 | Loss: 0.00001752
Iteration 49/1000 | Loss: 0.00001752
Iteration 50/1000 | Loss: 0.00001752
Iteration 51/1000 | Loss: 0.00001751
Iteration 52/1000 | Loss: 0.00001751
Iteration 53/1000 | Loss: 0.00001751
Iteration 54/1000 | Loss: 0.00001750
Iteration 55/1000 | Loss: 0.00001750
Iteration 56/1000 | Loss: 0.00001750
Iteration 57/1000 | Loss: 0.00001750
Iteration 58/1000 | Loss: 0.00001749
Iteration 59/1000 | Loss: 0.00001749
Iteration 60/1000 | Loss: 0.00001748
Iteration 61/1000 | Loss: 0.00001748
Iteration 62/1000 | Loss: 0.00001748
Iteration 63/1000 | Loss: 0.00001748
Iteration 64/1000 | Loss: 0.00001748
Iteration 65/1000 | Loss: 0.00001748
Iteration 66/1000 | Loss: 0.00001748
Iteration 67/1000 | Loss: 0.00001748
Iteration 68/1000 | Loss: 0.00001748
Iteration 69/1000 | Loss: 0.00001747
Iteration 70/1000 | Loss: 0.00001747
Iteration 71/1000 | Loss: 0.00001747
Iteration 72/1000 | Loss: 0.00001747
Iteration 73/1000 | Loss: 0.00001747
Iteration 74/1000 | Loss: 0.00001746
Iteration 75/1000 | Loss: 0.00001746
Iteration 76/1000 | Loss: 0.00001746
Iteration 77/1000 | Loss: 0.00001746
Iteration 78/1000 | Loss: 0.00001745
Iteration 79/1000 | Loss: 0.00001745
Iteration 80/1000 | Loss: 0.00001745
Iteration 81/1000 | Loss: 0.00001745
Iteration 82/1000 | Loss: 0.00001745
Iteration 83/1000 | Loss: 0.00001745
Iteration 84/1000 | Loss: 0.00001744
Iteration 85/1000 | Loss: 0.00001744
Iteration 86/1000 | Loss: 0.00001744
Iteration 87/1000 | Loss: 0.00001744
Iteration 88/1000 | Loss: 0.00001744
Iteration 89/1000 | Loss: 0.00001744
Iteration 90/1000 | Loss: 0.00001744
Iteration 91/1000 | Loss: 0.00001743
Iteration 92/1000 | Loss: 0.00001743
Iteration 93/1000 | Loss: 0.00001743
Iteration 94/1000 | Loss: 0.00001743
Iteration 95/1000 | Loss: 0.00001743
Iteration 96/1000 | Loss: 0.00001743
Iteration 97/1000 | Loss: 0.00001743
Iteration 98/1000 | Loss: 0.00001742
Iteration 99/1000 | Loss: 0.00001742
Iteration 100/1000 | Loss: 0.00001742
Iteration 101/1000 | Loss: 0.00001742
Iteration 102/1000 | Loss: 0.00001741
Iteration 103/1000 | Loss: 0.00001741
Iteration 104/1000 | Loss: 0.00001741
Iteration 105/1000 | Loss: 0.00001741
Iteration 106/1000 | Loss: 0.00001741
Iteration 107/1000 | Loss: 0.00001741
Iteration 108/1000 | Loss: 0.00001740
Iteration 109/1000 | Loss: 0.00001740
Iteration 110/1000 | Loss: 0.00001740
Iteration 111/1000 | Loss: 0.00001740
Iteration 112/1000 | Loss: 0.00001739
Iteration 113/1000 | Loss: 0.00001739
Iteration 114/1000 | Loss: 0.00001739
Iteration 115/1000 | Loss: 0.00001739
Iteration 116/1000 | Loss: 0.00001738
Iteration 117/1000 | Loss: 0.00001738
Iteration 118/1000 | Loss: 0.00001738
Iteration 119/1000 | Loss: 0.00001738
Iteration 120/1000 | Loss: 0.00001738
Iteration 121/1000 | Loss: 0.00001738
Iteration 122/1000 | Loss: 0.00001738
Iteration 123/1000 | Loss: 0.00001738
Iteration 124/1000 | Loss: 0.00001737
Iteration 125/1000 | Loss: 0.00001737
Iteration 126/1000 | Loss: 0.00001737
Iteration 127/1000 | Loss: 0.00001737
Iteration 128/1000 | Loss: 0.00001737
Iteration 129/1000 | Loss: 0.00001737
Iteration 130/1000 | Loss: 0.00001737
Iteration 131/1000 | Loss: 0.00001737
Iteration 132/1000 | Loss: 0.00001736
Iteration 133/1000 | Loss: 0.00001736
Iteration 134/1000 | Loss: 0.00001736
Iteration 135/1000 | Loss: 0.00001736
Iteration 136/1000 | Loss: 0.00001736
Iteration 137/1000 | Loss: 0.00001736
Iteration 138/1000 | Loss: 0.00001735
Iteration 139/1000 | Loss: 0.00001735
Iteration 140/1000 | Loss: 0.00001735
Iteration 141/1000 | Loss: 0.00001735
Iteration 142/1000 | Loss: 0.00001735
Iteration 143/1000 | Loss: 0.00001735
Iteration 144/1000 | Loss: 0.00001735
Iteration 145/1000 | Loss: 0.00001735
Iteration 146/1000 | Loss: 0.00001735
Iteration 147/1000 | Loss: 0.00001734
Iteration 148/1000 | Loss: 0.00001734
Iteration 149/1000 | Loss: 0.00001734
Iteration 150/1000 | Loss: 0.00001734
Iteration 151/1000 | Loss: 0.00001734
Iteration 152/1000 | Loss: 0.00001734
Iteration 153/1000 | Loss: 0.00001734
Iteration 154/1000 | Loss: 0.00001734
Iteration 155/1000 | Loss: 0.00001734
Iteration 156/1000 | Loss: 0.00001733
Iteration 157/1000 | Loss: 0.00001733
Iteration 158/1000 | Loss: 0.00001733
Iteration 159/1000 | Loss: 0.00001733
Iteration 160/1000 | Loss: 0.00001733
Iteration 161/1000 | Loss: 0.00001733
Iteration 162/1000 | Loss: 0.00001733
Iteration 163/1000 | Loss: 0.00001733
Iteration 164/1000 | Loss: 0.00001733
Iteration 165/1000 | Loss: 0.00001733
Iteration 166/1000 | Loss: 0.00001733
Iteration 167/1000 | Loss: 0.00001733
Iteration 168/1000 | Loss: 0.00001732
Iteration 169/1000 | Loss: 0.00001732
Iteration 170/1000 | Loss: 0.00001732
Iteration 171/1000 | Loss: 0.00001732
Iteration 172/1000 | Loss: 0.00001732
Iteration 173/1000 | Loss: 0.00001732
Iteration 174/1000 | Loss: 0.00001732
Iteration 175/1000 | Loss: 0.00001732
Iteration 176/1000 | Loss: 0.00001731
Iteration 177/1000 | Loss: 0.00001731
Iteration 178/1000 | Loss: 0.00001731
Iteration 179/1000 | Loss: 0.00001731
Iteration 180/1000 | Loss: 0.00001731
Iteration 181/1000 | Loss: 0.00001731
Iteration 182/1000 | Loss: 0.00001731
Iteration 183/1000 | Loss: 0.00001731
Iteration 184/1000 | Loss: 0.00001731
Iteration 185/1000 | Loss: 0.00001730
Iteration 186/1000 | Loss: 0.00001730
Iteration 187/1000 | Loss: 0.00001730
Iteration 188/1000 | Loss: 0.00001730
Iteration 189/1000 | Loss: 0.00001729
Iteration 190/1000 | Loss: 0.00001729
Iteration 191/1000 | Loss: 0.00001729
Iteration 192/1000 | Loss: 0.00001729
Iteration 193/1000 | Loss: 0.00001729
Iteration 194/1000 | Loss: 0.00001729
Iteration 195/1000 | Loss: 0.00001728
Iteration 196/1000 | Loss: 0.00001728
Iteration 197/1000 | Loss: 0.00001728
Iteration 198/1000 | Loss: 0.00001728
Iteration 199/1000 | Loss: 0.00001728
Iteration 200/1000 | Loss: 0.00001728
Iteration 201/1000 | Loss: 0.00001728
Iteration 202/1000 | Loss: 0.00001728
Iteration 203/1000 | Loss: 0.00001728
Iteration 204/1000 | Loss: 0.00001728
Iteration 205/1000 | Loss: 0.00001728
Iteration 206/1000 | Loss: 0.00001728
Iteration 207/1000 | Loss: 0.00001728
Iteration 208/1000 | Loss: 0.00001728
Iteration 209/1000 | Loss: 0.00001728
Iteration 210/1000 | Loss: 0.00001727
Iteration 211/1000 | Loss: 0.00001727
Iteration 212/1000 | Loss: 0.00001727
Iteration 213/1000 | Loss: 0.00001727
Iteration 214/1000 | Loss: 0.00001727
Iteration 215/1000 | Loss: 0.00001727
Iteration 216/1000 | Loss: 0.00001727
Iteration 217/1000 | Loss: 0.00001727
Iteration 218/1000 | Loss: 0.00001727
Iteration 219/1000 | Loss: 0.00001727
Iteration 220/1000 | Loss: 0.00001727
Iteration 221/1000 | Loss: 0.00001727
Iteration 222/1000 | Loss: 0.00001727
Iteration 223/1000 | Loss: 0.00001727
Iteration 224/1000 | Loss: 0.00001726
Iteration 225/1000 | Loss: 0.00001726
Iteration 226/1000 | Loss: 0.00001726
Iteration 227/1000 | Loss: 0.00001726
Iteration 228/1000 | Loss: 0.00001726
Iteration 229/1000 | Loss: 0.00001726
Iteration 230/1000 | Loss: 0.00001726
Iteration 231/1000 | Loss: 0.00001726
Iteration 232/1000 | Loss: 0.00001726
Iteration 233/1000 | Loss: 0.00001726
Iteration 234/1000 | Loss: 0.00001726
Iteration 235/1000 | Loss: 0.00001726
Iteration 236/1000 | Loss: 0.00001726
Iteration 237/1000 | Loss: 0.00001726
Iteration 238/1000 | Loss: 0.00001726
Iteration 239/1000 | Loss: 0.00001726
Iteration 240/1000 | Loss: 0.00001726
Iteration 241/1000 | Loss: 0.00001726
Iteration 242/1000 | Loss: 0.00001726
Iteration 243/1000 | Loss: 0.00001726
Iteration 244/1000 | Loss: 0.00001725
Iteration 245/1000 | Loss: 0.00001725
Iteration 246/1000 | Loss: 0.00001725
Iteration 247/1000 | Loss: 0.00001725
Iteration 248/1000 | Loss: 0.00001725
Iteration 249/1000 | Loss: 0.00001725
Iteration 250/1000 | Loss: 0.00001725
Iteration 251/1000 | Loss: 0.00001725
Iteration 252/1000 | Loss: 0.00001725
Iteration 253/1000 | Loss: 0.00001725
Iteration 254/1000 | Loss: 0.00001725
Iteration 255/1000 | Loss: 0.00001725
Iteration 256/1000 | Loss: 0.00001725
Iteration 257/1000 | Loss: 0.00001725
Iteration 258/1000 | Loss: 0.00001725
Iteration 259/1000 | Loss: 0.00001725
Iteration 260/1000 | Loss: 0.00001725
Iteration 261/1000 | Loss: 0.00001725
Iteration 262/1000 | Loss: 0.00001725
Iteration 263/1000 | Loss: 0.00001725
Iteration 264/1000 | Loss: 0.00001725
Iteration 265/1000 | Loss: 0.00001725
Iteration 266/1000 | Loss: 0.00001725
Iteration 267/1000 | Loss: 0.00001724
Iteration 268/1000 | Loss: 0.00001724
Iteration 269/1000 | Loss: 0.00001724
Iteration 270/1000 | Loss: 0.00001724
Iteration 271/1000 | Loss: 0.00001724
Iteration 272/1000 | Loss: 0.00001724
Iteration 273/1000 | Loss: 0.00001724
Iteration 274/1000 | Loss: 0.00001724
Iteration 275/1000 | Loss: 0.00001724
Iteration 276/1000 | Loss: 0.00001724
Iteration 277/1000 | Loss: 0.00001724
Iteration 278/1000 | Loss: 0.00001724
Iteration 279/1000 | Loss: 0.00001723
Iteration 280/1000 | Loss: 0.00001723
Iteration 281/1000 | Loss: 0.00001723
Iteration 282/1000 | Loss: 0.00001723
Iteration 283/1000 | Loss: 0.00001723
Iteration 284/1000 | Loss: 0.00001723
Iteration 285/1000 | Loss: 0.00001723
Iteration 286/1000 | Loss: 0.00001723
Iteration 287/1000 | Loss: 0.00001723
Iteration 288/1000 | Loss: 0.00001723
Iteration 289/1000 | Loss: 0.00001723
Iteration 290/1000 | Loss: 0.00001723
Iteration 291/1000 | Loss: 0.00001723
Iteration 292/1000 | Loss: 0.00001723
Iteration 293/1000 | Loss: 0.00001723
Iteration 294/1000 | Loss: 0.00001723
Iteration 295/1000 | Loss: 0.00001723
Iteration 296/1000 | Loss: 0.00001723
Iteration 297/1000 | Loss: 0.00001723
Iteration 298/1000 | Loss: 0.00001723
Iteration 299/1000 | Loss: 0.00001723
Iteration 300/1000 | Loss: 0.00001723
Iteration 301/1000 | Loss: 0.00001723
Iteration 302/1000 | Loss: 0.00001722
Iteration 303/1000 | Loss: 0.00001722
Iteration 304/1000 | Loss: 0.00001722
Iteration 305/1000 | Loss: 0.00001722
Iteration 306/1000 | Loss: 0.00001722
Iteration 307/1000 | Loss: 0.00001722
Iteration 308/1000 | Loss: 0.00001722
Iteration 309/1000 | Loss: 0.00001722
Iteration 310/1000 | Loss: 0.00001722
Iteration 311/1000 | Loss: 0.00001722
Iteration 312/1000 | Loss: 0.00001722
Iteration 313/1000 | Loss: 0.00001722
Iteration 314/1000 | Loss: 0.00001722
Iteration 315/1000 | Loss: 0.00001722
Iteration 316/1000 | Loss: 0.00001722
Iteration 317/1000 | Loss: 0.00001722
Iteration 318/1000 | Loss: 0.00001722
Iteration 319/1000 | Loss: 0.00001722
Iteration 320/1000 | Loss: 0.00001722
Iteration 321/1000 | Loss: 0.00001722
Iteration 322/1000 | Loss: 0.00001722
Iteration 323/1000 | Loss: 0.00001722
Iteration 324/1000 | Loss: 0.00001722
Iteration 325/1000 | Loss: 0.00001722
Iteration 326/1000 | Loss: 0.00001722
Iteration 327/1000 | Loss: 0.00001722
Iteration 328/1000 | Loss: 0.00001722
Iteration 329/1000 | Loss: 0.00001722
Iteration 330/1000 | Loss: 0.00001722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 330. Stopping optimization.
Last 5 losses: [1.7222491806023754e-05, 1.7222491806023754e-05, 1.7222491806023754e-05, 1.7222491806023754e-05, 1.7222491806023754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7222491806023754e-05

Optimization complete. Final v2v error: 3.4517018795013428 mm

Highest mean error: 5.684961795806885 mm for frame 70

Lowest mean error: 2.6926937103271484 mm for frame 127

Saving results

Total time: 52.17943787574768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033162
Iteration 2/25 | Loss: 0.00213001
Iteration 3/25 | Loss: 0.00164376
Iteration 4/25 | Loss: 0.00159718
Iteration 5/25 | Loss: 0.00157447
Iteration 6/25 | Loss: 0.00151699
Iteration 7/25 | Loss: 0.00152065
Iteration 8/25 | Loss: 0.00148856
Iteration 9/25 | Loss: 0.00148228
Iteration 10/25 | Loss: 0.00147548
Iteration 11/25 | Loss: 0.00146740
Iteration 12/25 | Loss: 0.00145864
Iteration 13/25 | Loss: 0.00145637
Iteration 14/25 | Loss: 0.00146019
Iteration 15/25 | Loss: 0.00146068
Iteration 16/25 | Loss: 0.00146598
Iteration 17/25 | Loss: 0.00147053
Iteration 18/25 | Loss: 0.00147073
Iteration 19/25 | Loss: 0.00145990
Iteration 20/25 | Loss: 0.00145633
Iteration 21/25 | Loss: 0.00145765
Iteration 22/25 | Loss: 0.00145034
Iteration 23/25 | Loss: 0.00144938
Iteration 24/25 | Loss: 0.00144913
Iteration 25/25 | Loss: 0.00144905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17278576
Iteration 2/25 | Loss: 0.00153319
Iteration 3/25 | Loss: 0.00153316
Iteration 4/25 | Loss: 0.00153316
Iteration 5/25 | Loss: 0.00153316
Iteration 6/25 | Loss: 0.00153316
Iteration 7/25 | Loss: 0.00153316
Iteration 8/25 | Loss: 0.00153316
Iteration 9/25 | Loss: 0.00153316
Iteration 10/25 | Loss: 0.00153316
Iteration 11/25 | Loss: 0.00153316
Iteration 12/25 | Loss: 0.00153316
Iteration 13/25 | Loss: 0.00153316
Iteration 14/25 | Loss: 0.00153316
Iteration 15/25 | Loss: 0.00153316
Iteration 16/25 | Loss: 0.00153316
Iteration 17/25 | Loss: 0.00153316
Iteration 18/25 | Loss: 0.00153316
Iteration 19/25 | Loss: 0.00153316
Iteration 20/25 | Loss: 0.00153316
Iteration 21/25 | Loss: 0.00153316
Iteration 22/25 | Loss: 0.00153316
Iteration 23/25 | Loss: 0.00153316
Iteration 24/25 | Loss: 0.00153316
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015331574250012636, 0.0015331574250012636, 0.0015331574250012636, 0.0015331574250012636, 0.0015331574250012636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015331574250012636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153316
Iteration 2/1000 | Loss: 0.00007831
Iteration 3/1000 | Loss: 0.00005300
Iteration 4/1000 | Loss: 0.00004692
Iteration 5/1000 | Loss: 0.00004431
Iteration 6/1000 | Loss: 0.00004227
Iteration 7/1000 | Loss: 0.00004118
Iteration 8/1000 | Loss: 0.00004032
Iteration 9/1000 | Loss: 0.00071807
Iteration 10/1000 | Loss: 0.00164928
Iteration 11/1000 | Loss: 0.00005669
Iteration 12/1000 | Loss: 0.00004153
Iteration 13/1000 | Loss: 0.00003648
Iteration 14/1000 | Loss: 0.00003321
Iteration 15/1000 | Loss: 0.00003018
Iteration 16/1000 | Loss: 0.00002871
Iteration 17/1000 | Loss: 0.00002795
Iteration 18/1000 | Loss: 0.00002737
Iteration 19/1000 | Loss: 0.00002660
Iteration 20/1000 | Loss: 0.00002596
Iteration 21/1000 | Loss: 0.00002555
Iteration 22/1000 | Loss: 0.00002536
Iteration 23/1000 | Loss: 0.00002532
Iteration 24/1000 | Loss: 0.00002529
Iteration 25/1000 | Loss: 0.00002527
Iteration 26/1000 | Loss: 0.00002527
Iteration 27/1000 | Loss: 0.00002526
Iteration 28/1000 | Loss: 0.00002525
Iteration 29/1000 | Loss: 0.00002525
Iteration 30/1000 | Loss: 0.00002524
Iteration 31/1000 | Loss: 0.00002524
Iteration 32/1000 | Loss: 0.00002524
Iteration 33/1000 | Loss: 0.00002524
Iteration 34/1000 | Loss: 0.00002523
Iteration 35/1000 | Loss: 0.00002521
Iteration 36/1000 | Loss: 0.00002521
Iteration 37/1000 | Loss: 0.00002520
Iteration 38/1000 | Loss: 0.00002520
Iteration 39/1000 | Loss: 0.00002520
Iteration 40/1000 | Loss: 0.00002519
Iteration 41/1000 | Loss: 0.00002519
Iteration 42/1000 | Loss: 0.00002519
Iteration 43/1000 | Loss: 0.00002518
Iteration 44/1000 | Loss: 0.00002516
Iteration 45/1000 | Loss: 0.00002513
Iteration 46/1000 | Loss: 0.00002510
Iteration 47/1000 | Loss: 0.00002509
Iteration 48/1000 | Loss: 0.00002509
Iteration 49/1000 | Loss: 0.00002506
Iteration 50/1000 | Loss: 0.00002506
Iteration 51/1000 | Loss: 0.00002503
Iteration 52/1000 | Loss: 0.00002502
Iteration 53/1000 | Loss: 0.00002501
Iteration 54/1000 | Loss: 0.00002499
Iteration 55/1000 | Loss: 0.00002499
Iteration 56/1000 | Loss: 0.00002498
Iteration 57/1000 | Loss: 0.00002493
Iteration 58/1000 | Loss: 0.00002490
Iteration 59/1000 | Loss: 0.00002490
Iteration 60/1000 | Loss: 0.00002488
Iteration 61/1000 | Loss: 0.00002488
Iteration 62/1000 | Loss: 0.00002488
Iteration 63/1000 | Loss: 0.00002488
Iteration 64/1000 | Loss: 0.00002488
Iteration 65/1000 | Loss: 0.00002488
Iteration 66/1000 | Loss: 0.00002487
Iteration 67/1000 | Loss: 0.00002487
Iteration 68/1000 | Loss: 0.00002487
Iteration 69/1000 | Loss: 0.00002487
Iteration 70/1000 | Loss: 0.00002487
Iteration 71/1000 | Loss: 0.00002487
Iteration 72/1000 | Loss: 0.00002487
Iteration 73/1000 | Loss: 0.00002487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.4873776055756025e-05, 2.4873776055756025e-05, 2.4873776055756025e-05, 2.4873776055756025e-05, 2.4873776055756025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4873776055756025e-05

Optimization complete. Final v2v error: 4.065175533294678 mm

Highest mean error: 5.4249491691589355 mm for frame 230

Lowest mean error: 3.5458686351776123 mm for frame 54

Saving results

Total time: 95.38497066497803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782673
Iteration 2/25 | Loss: 0.00177372
Iteration 3/25 | Loss: 0.00143068
Iteration 4/25 | Loss: 0.00139556
Iteration 5/25 | Loss: 0.00139070
Iteration 6/25 | Loss: 0.00140388
Iteration 7/25 | Loss: 0.00138073
Iteration 8/25 | Loss: 0.00137514
Iteration 9/25 | Loss: 0.00137460
Iteration 10/25 | Loss: 0.00137456
Iteration 11/25 | Loss: 0.00137455
Iteration 12/25 | Loss: 0.00137455
Iteration 13/25 | Loss: 0.00137455
Iteration 14/25 | Loss: 0.00137455
Iteration 15/25 | Loss: 0.00137455
Iteration 16/25 | Loss: 0.00137455
Iteration 17/25 | Loss: 0.00137455
Iteration 18/25 | Loss: 0.00137455
Iteration 19/25 | Loss: 0.00137455
Iteration 20/25 | Loss: 0.00137455
Iteration 21/25 | Loss: 0.00137454
Iteration 22/25 | Loss: 0.00137454
Iteration 23/25 | Loss: 0.00137454
Iteration 24/25 | Loss: 0.00137454
Iteration 25/25 | Loss: 0.00137454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.63912058
Iteration 2/25 | Loss: 0.00108322
Iteration 3/25 | Loss: 0.00108319
Iteration 4/25 | Loss: 0.00108319
Iteration 5/25 | Loss: 0.00108319
Iteration 6/25 | Loss: 0.00108319
Iteration 7/25 | Loss: 0.00108319
Iteration 8/25 | Loss: 0.00108319
Iteration 9/25 | Loss: 0.00108319
Iteration 10/25 | Loss: 0.00108319
Iteration 11/25 | Loss: 0.00108319
Iteration 12/25 | Loss: 0.00108319
Iteration 13/25 | Loss: 0.00108319
Iteration 14/25 | Loss: 0.00108319
Iteration 15/25 | Loss: 0.00108319
Iteration 16/25 | Loss: 0.00108319
Iteration 17/25 | Loss: 0.00108319
Iteration 18/25 | Loss: 0.00108319
Iteration 19/25 | Loss: 0.00108319
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010831889230757952, 0.0010831889230757952, 0.0010831889230757952, 0.0010831889230757952, 0.0010831889230757952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010831889230757952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108319
Iteration 2/1000 | Loss: 0.00003520
Iteration 3/1000 | Loss: 0.00002421
Iteration 4/1000 | Loss: 0.00002171
Iteration 5/1000 | Loss: 0.00011494
Iteration 6/1000 | Loss: 0.00002029
Iteration 7/1000 | Loss: 0.00016209
Iteration 8/1000 | Loss: 0.00013571
Iteration 9/1000 | Loss: 0.00057318
Iteration 10/1000 | Loss: 0.00002922
Iteration 11/1000 | Loss: 0.00001885
Iteration 12/1000 | Loss: 0.00001851
Iteration 13/1000 | Loss: 0.00012907
Iteration 14/1000 | Loss: 0.00038604
Iteration 15/1000 | Loss: 0.00033846
Iteration 16/1000 | Loss: 0.00038668
Iteration 17/1000 | Loss: 0.00011029
Iteration 18/1000 | Loss: 0.00001850
Iteration 19/1000 | Loss: 0.00001815
Iteration 20/1000 | Loss: 0.00001808
Iteration 21/1000 | Loss: 0.00001805
Iteration 22/1000 | Loss: 0.00001803
Iteration 23/1000 | Loss: 0.00001802
Iteration 24/1000 | Loss: 0.00001802
Iteration 25/1000 | Loss: 0.00001801
Iteration 26/1000 | Loss: 0.00001800
Iteration 27/1000 | Loss: 0.00001799
Iteration 28/1000 | Loss: 0.00001799
Iteration 29/1000 | Loss: 0.00001798
Iteration 30/1000 | Loss: 0.00001796
Iteration 31/1000 | Loss: 0.00001796
Iteration 32/1000 | Loss: 0.00001795
Iteration 33/1000 | Loss: 0.00001792
Iteration 34/1000 | Loss: 0.00001792
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001786
Iteration 37/1000 | Loss: 0.00001786
Iteration 38/1000 | Loss: 0.00001786
Iteration 39/1000 | Loss: 0.00001785
Iteration 40/1000 | Loss: 0.00001784
Iteration 41/1000 | Loss: 0.00001784
Iteration 42/1000 | Loss: 0.00001783
Iteration 43/1000 | Loss: 0.00001783
Iteration 44/1000 | Loss: 0.00001781
Iteration 45/1000 | Loss: 0.00001781
Iteration 46/1000 | Loss: 0.00001780
Iteration 47/1000 | Loss: 0.00001780
Iteration 48/1000 | Loss: 0.00001780
Iteration 49/1000 | Loss: 0.00001779
Iteration 50/1000 | Loss: 0.00001779
Iteration 51/1000 | Loss: 0.00001779
Iteration 52/1000 | Loss: 0.00001779
Iteration 53/1000 | Loss: 0.00001778
Iteration 54/1000 | Loss: 0.00001778
Iteration 55/1000 | Loss: 0.00001778
Iteration 56/1000 | Loss: 0.00001778
Iteration 57/1000 | Loss: 0.00001778
Iteration 58/1000 | Loss: 0.00001778
Iteration 59/1000 | Loss: 0.00001777
Iteration 60/1000 | Loss: 0.00001777
Iteration 61/1000 | Loss: 0.00001777
Iteration 62/1000 | Loss: 0.00001777
Iteration 63/1000 | Loss: 0.00001777
Iteration 64/1000 | Loss: 0.00001777
Iteration 65/1000 | Loss: 0.00001777
Iteration 66/1000 | Loss: 0.00001776
Iteration 67/1000 | Loss: 0.00001775
Iteration 68/1000 | Loss: 0.00001775
Iteration 69/1000 | Loss: 0.00001774
Iteration 70/1000 | Loss: 0.00001774
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001771
Iteration 73/1000 | Loss: 0.00001771
Iteration 74/1000 | Loss: 0.00001770
Iteration 75/1000 | Loss: 0.00001770
Iteration 76/1000 | Loss: 0.00001770
Iteration 77/1000 | Loss: 0.00001769
Iteration 78/1000 | Loss: 0.00001769
Iteration 79/1000 | Loss: 0.00001769
Iteration 80/1000 | Loss: 0.00001769
Iteration 81/1000 | Loss: 0.00001768
Iteration 82/1000 | Loss: 0.00001768
Iteration 83/1000 | Loss: 0.00001768
Iteration 84/1000 | Loss: 0.00001768
Iteration 85/1000 | Loss: 0.00001767
Iteration 86/1000 | Loss: 0.00001767
Iteration 87/1000 | Loss: 0.00001766
Iteration 88/1000 | Loss: 0.00001766
Iteration 89/1000 | Loss: 0.00001766
Iteration 90/1000 | Loss: 0.00001766
Iteration 91/1000 | Loss: 0.00001766
Iteration 92/1000 | Loss: 0.00001766
Iteration 93/1000 | Loss: 0.00001766
Iteration 94/1000 | Loss: 0.00001765
Iteration 95/1000 | Loss: 0.00001765
Iteration 96/1000 | Loss: 0.00001765
Iteration 97/1000 | Loss: 0.00001765
Iteration 98/1000 | Loss: 0.00001765
Iteration 99/1000 | Loss: 0.00001764
Iteration 100/1000 | Loss: 0.00001764
Iteration 101/1000 | Loss: 0.00001764
Iteration 102/1000 | Loss: 0.00001764
Iteration 103/1000 | Loss: 0.00001764
Iteration 104/1000 | Loss: 0.00001764
Iteration 105/1000 | Loss: 0.00001764
Iteration 106/1000 | Loss: 0.00001764
Iteration 107/1000 | Loss: 0.00001763
Iteration 108/1000 | Loss: 0.00001763
Iteration 109/1000 | Loss: 0.00001763
Iteration 110/1000 | Loss: 0.00001763
Iteration 111/1000 | Loss: 0.00001763
Iteration 112/1000 | Loss: 0.00001763
Iteration 113/1000 | Loss: 0.00001763
Iteration 114/1000 | Loss: 0.00001763
Iteration 115/1000 | Loss: 0.00001763
Iteration 116/1000 | Loss: 0.00001763
Iteration 117/1000 | Loss: 0.00001763
Iteration 118/1000 | Loss: 0.00001762
Iteration 119/1000 | Loss: 0.00001762
Iteration 120/1000 | Loss: 0.00001762
Iteration 121/1000 | Loss: 0.00001761
Iteration 122/1000 | Loss: 0.00001761
Iteration 123/1000 | Loss: 0.00001761
Iteration 124/1000 | Loss: 0.00001761
Iteration 125/1000 | Loss: 0.00001761
Iteration 126/1000 | Loss: 0.00001761
Iteration 127/1000 | Loss: 0.00001761
Iteration 128/1000 | Loss: 0.00001761
Iteration 129/1000 | Loss: 0.00001761
Iteration 130/1000 | Loss: 0.00001761
Iteration 131/1000 | Loss: 0.00001761
Iteration 132/1000 | Loss: 0.00001761
Iteration 133/1000 | Loss: 0.00001761
Iteration 134/1000 | Loss: 0.00001760
Iteration 135/1000 | Loss: 0.00001760
Iteration 136/1000 | Loss: 0.00001760
Iteration 137/1000 | Loss: 0.00001760
Iteration 138/1000 | Loss: 0.00001760
Iteration 139/1000 | Loss: 0.00001760
Iteration 140/1000 | Loss: 0.00001760
Iteration 141/1000 | Loss: 0.00001760
Iteration 142/1000 | Loss: 0.00001760
Iteration 143/1000 | Loss: 0.00001760
Iteration 144/1000 | Loss: 0.00001760
Iteration 145/1000 | Loss: 0.00001760
Iteration 146/1000 | Loss: 0.00001760
Iteration 147/1000 | Loss: 0.00001759
Iteration 148/1000 | Loss: 0.00001759
Iteration 149/1000 | Loss: 0.00001759
Iteration 150/1000 | Loss: 0.00001759
Iteration 151/1000 | Loss: 0.00001759
Iteration 152/1000 | Loss: 0.00001759
Iteration 153/1000 | Loss: 0.00001759
Iteration 154/1000 | Loss: 0.00001759
Iteration 155/1000 | Loss: 0.00001759
Iteration 156/1000 | Loss: 0.00001759
Iteration 157/1000 | Loss: 0.00001759
Iteration 158/1000 | Loss: 0.00001759
Iteration 159/1000 | Loss: 0.00001758
Iteration 160/1000 | Loss: 0.00001758
Iteration 161/1000 | Loss: 0.00001758
Iteration 162/1000 | Loss: 0.00001758
Iteration 163/1000 | Loss: 0.00001758
Iteration 164/1000 | Loss: 0.00001758
Iteration 165/1000 | Loss: 0.00001758
Iteration 166/1000 | Loss: 0.00001758
Iteration 167/1000 | Loss: 0.00001758
Iteration 168/1000 | Loss: 0.00001758
Iteration 169/1000 | Loss: 0.00001758
Iteration 170/1000 | Loss: 0.00001758
Iteration 171/1000 | Loss: 0.00001757
Iteration 172/1000 | Loss: 0.00001757
Iteration 173/1000 | Loss: 0.00001757
Iteration 174/1000 | Loss: 0.00001757
Iteration 175/1000 | Loss: 0.00001757
Iteration 176/1000 | Loss: 0.00001757
Iteration 177/1000 | Loss: 0.00001757
Iteration 178/1000 | Loss: 0.00001757
Iteration 179/1000 | Loss: 0.00001757
Iteration 180/1000 | Loss: 0.00001757
Iteration 181/1000 | Loss: 0.00001757
Iteration 182/1000 | Loss: 0.00001757
Iteration 183/1000 | Loss: 0.00001757
Iteration 184/1000 | Loss: 0.00001756
Iteration 185/1000 | Loss: 0.00001756
Iteration 186/1000 | Loss: 0.00001756
Iteration 187/1000 | Loss: 0.00001756
Iteration 188/1000 | Loss: 0.00001756
Iteration 189/1000 | Loss: 0.00001756
Iteration 190/1000 | Loss: 0.00001755
Iteration 191/1000 | Loss: 0.00001755
Iteration 192/1000 | Loss: 0.00001755
Iteration 193/1000 | Loss: 0.00001755
Iteration 194/1000 | Loss: 0.00001755
Iteration 195/1000 | Loss: 0.00001755
Iteration 196/1000 | Loss: 0.00001755
Iteration 197/1000 | Loss: 0.00001755
Iteration 198/1000 | Loss: 0.00001755
Iteration 199/1000 | Loss: 0.00001754
Iteration 200/1000 | Loss: 0.00001754
Iteration 201/1000 | Loss: 0.00001754
Iteration 202/1000 | Loss: 0.00001754
Iteration 203/1000 | Loss: 0.00001754
Iteration 204/1000 | Loss: 0.00001754
Iteration 205/1000 | Loss: 0.00001753
Iteration 206/1000 | Loss: 0.00001753
Iteration 207/1000 | Loss: 0.00001753
Iteration 208/1000 | Loss: 0.00001753
Iteration 209/1000 | Loss: 0.00001753
Iteration 210/1000 | Loss: 0.00001753
Iteration 211/1000 | Loss: 0.00001753
Iteration 212/1000 | Loss: 0.00001753
Iteration 213/1000 | Loss: 0.00001753
Iteration 214/1000 | Loss: 0.00001753
Iteration 215/1000 | Loss: 0.00001753
Iteration 216/1000 | Loss: 0.00001753
Iteration 217/1000 | Loss: 0.00001753
Iteration 218/1000 | Loss: 0.00001753
Iteration 219/1000 | Loss: 0.00001753
Iteration 220/1000 | Loss: 0.00001752
Iteration 221/1000 | Loss: 0.00001752
Iteration 222/1000 | Loss: 0.00001752
Iteration 223/1000 | Loss: 0.00001752
Iteration 224/1000 | Loss: 0.00001752
Iteration 225/1000 | Loss: 0.00001752
Iteration 226/1000 | Loss: 0.00001752
Iteration 227/1000 | Loss: 0.00001752
Iteration 228/1000 | Loss: 0.00001752
Iteration 229/1000 | Loss: 0.00001752
Iteration 230/1000 | Loss: 0.00001752
Iteration 231/1000 | Loss: 0.00001752
Iteration 232/1000 | Loss: 0.00001752
Iteration 233/1000 | Loss: 0.00001752
Iteration 234/1000 | Loss: 0.00001752
Iteration 235/1000 | Loss: 0.00001752
Iteration 236/1000 | Loss: 0.00001752
Iteration 237/1000 | Loss: 0.00001752
Iteration 238/1000 | Loss: 0.00001752
Iteration 239/1000 | Loss: 0.00001752
Iteration 240/1000 | Loss: 0.00001752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.7518215827294625e-05, 1.7518215827294625e-05, 1.7518215827294625e-05, 1.7518215827294625e-05, 1.7518215827294625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7518215827294625e-05

Optimization complete. Final v2v error: 3.5057952404022217 mm

Highest mean error: 3.870025157928467 mm for frame 135

Lowest mean error: 3.0481035709381104 mm for frame 178

Saving results

Total time: 60.74219584465027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500359
Iteration 2/25 | Loss: 0.00139568
Iteration 3/25 | Loss: 0.00132726
Iteration 4/25 | Loss: 0.00131983
Iteration 5/25 | Loss: 0.00131923
Iteration 6/25 | Loss: 0.00131923
Iteration 7/25 | Loss: 0.00131923
Iteration 8/25 | Loss: 0.00131923
Iteration 9/25 | Loss: 0.00131923
Iteration 10/25 | Loss: 0.00131923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013192272745072842, 0.0013192272745072842, 0.0013192272745072842, 0.0013192272745072842, 0.0013192272745072842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013192272745072842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29821742
Iteration 2/25 | Loss: 0.00119024
Iteration 3/25 | Loss: 0.00119022
Iteration 4/25 | Loss: 0.00119022
Iteration 5/25 | Loss: 0.00119022
Iteration 6/25 | Loss: 0.00119022
Iteration 7/25 | Loss: 0.00119022
Iteration 8/25 | Loss: 0.00119022
Iteration 9/25 | Loss: 0.00119022
Iteration 10/25 | Loss: 0.00119022
Iteration 11/25 | Loss: 0.00119022
Iteration 12/25 | Loss: 0.00119022
Iteration 13/25 | Loss: 0.00119022
Iteration 14/25 | Loss: 0.00119022
Iteration 15/25 | Loss: 0.00119022
Iteration 16/25 | Loss: 0.00119022
Iteration 17/25 | Loss: 0.00119022
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011902179103344679, 0.0011902179103344679, 0.0011902179103344679, 0.0011902179103344679, 0.0011902179103344679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011902179103344679

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119022
Iteration 2/1000 | Loss: 0.00002569
Iteration 3/1000 | Loss: 0.00001812
Iteration 4/1000 | Loss: 0.00001648
Iteration 5/1000 | Loss: 0.00001559
Iteration 6/1000 | Loss: 0.00001502
Iteration 7/1000 | Loss: 0.00001478
Iteration 8/1000 | Loss: 0.00001438
Iteration 9/1000 | Loss: 0.00001411
Iteration 10/1000 | Loss: 0.00001386
Iteration 11/1000 | Loss: 0.00001373
Iteration 12/1000 | Loss: 0.00001358
Iteration 13/1000 | Loss: 0.00001354
Iteration 14/1000 | Loss: 0.00001349
Iteration 15/1000 | Loss: 0.00001348
Iteration 16/1000 | Loss: 0.00001347
Iteration 17/1000 | Loss: 0.00001344
Iteration 18/1000 | Loss: 0.00001330
Iteration 19/1000 | Loss: 0.00001328
Iteration 20/1000 | Loss: 0.00001327
Iteration 21/1000 | Loss: 0.00001323
Iteration 22/1000 | Loss: 0.00001314
Iteration 23/1000 | Loss: 0.00001311
Iteration 24/1000 | Loss: 0.00001309
Iteration 25/1000 | Loss: 0.00001309
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001300
Iteration 29/1000 | Loss: 0.00001297
Iteration 30/1000 | Loss: 0.00001297
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001297
Iteration 33/1000 | Loss: 0.00001297
Iteration 34/1000 | Loss: 0.00001296
Iteration 35/1000 | Loss: 0.00001296
Iteration 36/1000 | Loss: 0.00001296
Iteration 37/1000 | Loss: 0.00001296
Iteration 38/1000 | Loss: 0.00001296
Iteration 39/1000 | Loss: 0.00001295
Iteration 40/1000 | Loss: 0.00001295
Iteration 41/1000 | Loss: 0.00001295
Iteration 42/1000 | Loss: 0.00001293
Iteration 43/1000 | Loss: 0.00001291
Iteration 44/1000 | Loss: 0.00001291
Iteration 45/1000 | Loss: 0.00001291
Iteration 46/1000 | Loss: 0.00001290
Iteration 47/1000 | Loss: 0.00001290
Iteration 48/1000 | Loss: 0.00001289
Iteration 49/1000 | Loss: 0.00001289
Iteration 50/1000 | Loss: 0.00001289
Iteration 51/1000 | Loss: 0.00001288
Iteration 52/1000 | Loss: 0.00001287
Iteration 53/1000 | Loss: 0.00001287
Iteration 54/1000 | Loss: 0.00001286
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001285
Iteration 57/1000 | Loss: 0.00001284
Iteration 58/1000 | Loss: 0.00001283
Iteration 59/1000 | Loss: 0.00001283
Iteration 60/1000 | Loss: 0.00001283
Iteration 61/1000 | Loss: 0.00001282
Iteration 62/1000 | Loss: 0.00001282
Iteration 63/1000 | Loss: 0.00001281
Iteration 64/1000 | Loss: 0.00001281
Iteration 65/1000 | Loss: 0.00001281
Iteration 66/1000 | Loss: 0.00001281
Iteration 67/1000 | Loss: 0.00001280
Iteration 68/1000 | Loss: 0.00001280
Iteration 69/1000 | Loss: 0.00001280
Iteration 70/1000 | Loss: 0.00001280
Iteration 71/1000 | Loss: 0.00001280
Iteration 72/1000 | Loss: 0.00001280
Iteration 73/1000 | Loss: 0.00001279
Iteration 74/1000 | Loss: 0.00001279
Iteration 75/1000 | Loss: 0.00001279
Iteration 76/1000 | Loss: 0.00001278
Iteration 77/1000 | Loss: 0.00001278
Iteration 78/1000 | Loss: 0.00001278
Iteration 79/1000 | Loss: 0.00001277
Iteration 80/1000 | Loss: 0.00001277
Iteration 81/1000 | Loss: 0.00001277
Iteration 82/1000 | Loss: 0.00001277
Iteration 83/1000 | Loss: 0.00001277
Iteration 84/1000 | Loss: 0.00001277
Iteration 85/1000 | Loss: 0.00001277
Iteration 86/1000 | Loss: 0.00001277
Iteration 87/1000 | Loss: 0.00001277
Iteration 88/1000 | Loss: 0.00001276
Iteration 89/1000 | Loss: 0.00001276
Iteration 90/1000 | Loss: 0.00001276
Iteration 91/1000 | Loss: 0.00001276
Iteration 92/1000 | Loss: 0.00001276
Iteration 93/1000 | Loss: 0.00001275
Iteration 94/1000 | Loss: 0.00001275
Iteration 95/1000 | Loss: 0.00001275
Iteration 96/1000 | Loss: 0.00001275
Iteration 97/1000 | Loss: 0.00001275
Iteration 98/1000 | Loss: 0.00001275
Iteration 99/1000 | Loss: 0.00001275
Iteration 100/1000 | Loss: 0.00001275
Iteration 101/1000 | Loss: 0.00001274
Iteration 102/1000 | Loss: 0.00001274
Iteration 103/1000 | Loss: 0.00001274
Iteration 104/1000 | Loss: 0.00001274
Iteration 105/1000 | Loss: 0.00001274
Iteration 106/1000 | Loss: 0.00001274
Iteration 107/1000 | Loss: 0.00001273
Iteration 108/1000 | Loss: 0.00001273
Iteration 109/1000 | Loss: 0.00001273
Iteration 110/1000 | Loss: 0.00001273
Iteration 111/1000 | Loss: 0.00001272
Iteration 112/1000 | Loss: 0.00001272
Iteration 113/1000 | Loss: 0.00001272
Iteration 114/1000 | Loss: 0.00001272
Iteration 115/1000 | Loss: 0.00001272
Iteration 116/1000 | Loss: 0.00001271
Iteration 117/1000 | Loss: 0.00001271
Iteration 118/1000 | Loss: 0.00001271
Iteration 119/1000 | Loss: 0.00001271
Iteration 120/1000 | Loss: 0.00001271
Iteration 121/1000 | Loss: 0.00001271
Iteration 122/1000 | Loss: 0.00001271
Iteration 123/1000 | Loss: 0.00001271
Iteration 124/1000 | Loss: 0.00001271
Iteration 125/1000 | Loss: 0.00001270
Iteration 126/1000 | Loss: 0.00001270
Iteration 127/1000 | Loss: 0.00001270
Iteration 128/1000 | Loss: 0.00001270
Iteration 129/1000 | Loss: 0.00001270
Iteration 130/1000 | Loss: 0.00001270
Iteration 131/1000 | Loss: 0.00001270
Iteration 132/1000 | Loss: 0.00001270
Iteration 133/1000 | Loss: 0.00001269
Iteration 134/1000 | Loss: 0.00001269
Iteration 135/1000 | Loss: 0.00001269
Iteration 136/1000 | Loss: 0.00001269
Iteration 137/1000 | Loss: 0.00001269
Iteration 138/1000 | Loss: 0.00001269
Iteration 139/1000 | Loss: 0.00001269
Iteration 140/1000 | Loss: 0.00001269
Iteration 141/1000 | Loss: 0.00001269
Iteration 142/1000 | Loss: 0.00001269
Iteration 143/1000 | Loss: 0.00001269
Iteration 144/1000 | Loss: 0.00001268
Iteration 145/1000 | Loss: 0.00001268
Iteration 146/1000 | Loss: 0.00001268
Iteration 147/1000 | Loss: 0.00001268
Iteration 148/1000 | Loss: 0.00001268
Iteration 149/1000 | Loss: 0.00001268
Iteration 150/1000 | Loss: 0.00001268
Iteration 151/1000 | Loss: 0.00001268
Iteration 152/1000 | Loss: 0.00001268
Iteration 153/1000 | Loss: 0.00001267
Iteration 154/1000 | Loss: 0.00001267
Iteration 155/1000 | Loss: 0.00001267
Iteration 156/1000 | Loss: 0.00001267
Iteration 157/1000 | Loss: 0.00001267
Iteration 158/1000 | Loss: 0.00001267
Iteration 159/1000 | Loss: 0.00001267
Iteration 160/1000 | Loss: 0.00001267
Iteration 161/1000 | Loss: 0.00001267
Iteration 162/1000 | Loss: 0.00001267
Iteration 163/1000 | Loss: 0.00001267
Iteration 164/1000 | Loss: 0.00001267
Iteration 165/1000 | Loss: 0.00001267
Iteration 166/1000 | Loss: 0.00001266
Iteration 167/1000 | Loss: 0.00001266
Iteration 168/1000 | Loss: 0.00001266
Iteration 169/1000 | Loss: 0.00001266
Iteration 170/1000 | Loss: 0.00001266
Iteration 171/1000 | Loss: 0.00001266
Iteration 172/1000 | Loss: 0.00001266
Iteration 173/1000 | Loss: 0.00001266
Iteration 174/1000 | Loss: 0.00001266
Iteration 175/1000 | Loss: 0.00001266
Iteration 176/1000 | Loss: 0.00001266
Iteration 177/1000 | Loss: 0.00001266
Iteration 178/1000 | Loss: 0.00001266
Iteration 179/1000 | Loss: 0.00001265
Iteration 180/1000 | Loss: 0.00001265
Iteration 181/1000 | Loss: 0.00001265
Iteration 182/1000 | Loss: 0.00001265
Iteration 183/1000 | Loss: 0.00001265
Iteration 184/1000 | Loss: 0.00001265
Iteration 185/1000 | Loss: 0.00001265
Iteration 186/1000 | Loss: 0.00001265
Iteration 187/1000 | Loss: 0.00001265
Iteration 188/1000 | Loss: 0.00001265
Iteration 189/1000 | Loss: 0.00001265
Iteration 190/1000 | Loss: 0.00001265
Iteration 191/1000 | Loss: 0.00001265
Iteration 192/1000 | Loss: 0.00001265
Iteration 193/1000 | Loss: 0.00001264
Iteration 194/1000 | Loss: 0.00001264
Iteration 195/1000 | Loss: 0.00001264
Iteration 196/1000 | Loss: 0.00001264
Iteration 197/1000 | Loss: 0.00001264
Iteration 198/1000 | Loss: 0.00001264
Iteration 199/1000 | Loss: 0.00001264
Iteration 200/1000 | Loss: 0.00001264
Iteration 201/1000 | Loss: 0.00001264
Iteration 202/1000 | Loss: 0.00001264
Iteration 203/1000 | Loss: 0.00001264
Iteration 204/1000 | Loss: 0.00001264
Iteration 205/1000 | Loss: 0.00001264
Iteration 206/1000 | Loss: 0.00001264
Iteration 207/1000 | Loss: 0.00001264
Iteration 208/1000 | Loss: 0.00001264
Iteration 209/1000 | Loss: 0.00001264
Iteration 210/1000 | Loss: 0.00001264
Iteration 211/1000 | Loss: 0.00001264
Iteration 212/1000 | Loss: 0.00001263
Iteration 213/1000 | Loss: 0.00001263
Iteration 214/1000 | Loss: 0.00001263
Iteration 215/1000 | Loss: 0.00001263
Iteration 216/1000 | Loss: 0.00001263
Iteration 217/1000 | Loss: 0.00001263
Iteration 218/1000 | Loss: 0.00001263
Iteration 219/1000 | Loss: 0.00001263
Iteration 220/1000 | Loss: 0.00001263
Iteration 221/1000 | Loss: 0.00001263
Iteration 222/1000 | Loss: 0.00001263
Iteration 223/1000 | Loss: 0.00001262
Iteration 224/1000 | Loss: 0.00001262
Iteration 225/1000 | Loss: 0.00001262
Iteration 226/1000 | Loss: 0.00001262
Iteration 227/1000 | Loss: 0.00001262
Iteration 228/1000 | Loss: 0.00001262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.2624914234038442e-05, 1.2624914234038442e-05, 1.2624914234038442e-05, 1.2624914234038442e-05, 1.2624914234038442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2624914234038442e-05

Optimization complete. Final v2v error: 3.0059218406677246 mm

Highest mean error: 3.235344409942627 mm for frame 172

Lowest mean error: 2.7329261302948 mm for frame 224

Saving results

Total time: 49.979841470718384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780421
Iteration 2/25 | Loss: 0.00146737
Iteration 3/25 | Loss: 0.00132133
Iteration 4/25 | Loss: 0.00130324
Iteration 5/25 | Loss: 0.00129955
Iteration 6/25 | Loss: 0.00129902
Iteration 7/25 | Loss: 0.00129902
Iteration 8/25 | Loss: 0.00129902
Iteration 9/25 | Loss: 0.00129902
Iteration 10/25 | Loss: 0.00129902
Iteration 11/25 | Loss: 0.00129902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012990152463316917, 0.0012990152463316917, 0.0012990152463316917, 0.0012990152463316917, 0.0012990152463316917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012990152463316917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29286444
Iteration 2/25 | Loss: 0.00147589
Iteration 3/25 | Loss: 0.00147589
Iteration 4/25 | Loss: 0.00147589
Iteration 5/25 | Loss: 0.00147589
Iteration 6/25 | Loss: 0.00147589
Iteration 7/25 | Loss: 0.00147589
Iteration 8/25 | Loss: 0.00147589
Iteration 9/25 | Loss: 0.00147589
Iteration 10/25 | Loss: 0.00147589
Iteration 11/25 | Loss: 0.00147589
Iteration 12/25 | Loss: 0.00147589
Iteration 13/25 | Loss: 0.00147589
Iteration 14/25 | Loss: 0.00147589
Iteration 15/25 | Loss: 0.00147589
Iteration 16/25 | Loss: 0.00147589
Iteration 17/25 | Loss: 0.00147589
Iteration 18/25 | Loss: 0.00147589
Iteration 19/25 | Loss: 0.00147589
Iteration 20/25 | Loss: 0.00147589
Iteration 21/25 | Loss: 0.00147589
Iteration 22/25 | Loss: 0.00147589
Iteration 23/25 | Loss: 0.00147589
Iteration 24/25 | Loss: 0.00147589
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014758901670575142, 0.0014758901670575142, 0.0014758901670575142, 0.0014758901670575142, 0.0014758901670575142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014758901670575142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147589
Iteration 2/1000 | Loss: 0.00003192
Iteration 3/1000 | Loss: 0.00002320
Iteration 4/1000 | Loss: 0.00002120
Iteration 5/1000 | Loss: 0.00001975
Iteration 6/1000 | Loss: 0.00001871
Iteration 7/1000 | Loss: 0.00001806
Iteration 8/1000 | Loss: 0.00001746
Iteration 9/1000 | Loss: 0.00001706
Iteration 10/1000 | Loss: 0.00001669
Iteration 11/1000 | Loss: 0.00001644
Iteration 12/1000 | Loss: 0.00001623
Iteration 13/1000 | Loss: 0.00001609
Iteration 14/1000 | Loss: 0.00001596
Iteration 15/1000 | Loss: 0.00001591
Iteration 16/1000 | Loss: 0.00001590
Iteration 17/1000 | Loss: 0.00001588
Iteration 18/1000 | Loss: 0.00001588
Iteration 19/1000 | Loss: 0.00001588
Iteration 20/1000 | Loss: 0.00001588
Iteration 21/1000 | Loss: 0.00001588
Iteration 22/1000 | Loss: 0.00001588
Iteration 23/1000 | Loss: 0.00001588
Iteration 24/1000 | Loss: 0.00001588
Iteration 25/1000 | Loss: 0.00001588
Iteration 26/1000 | Loss: 0.00001588
Iteration 27/1000 | Loss: 0.00001587
Iteration 28/1000 | Loss: 0.00001587
Iteration 29/1000 | Loss: 0.00001587
Iteration 30/1000 | Loss: 0.00001587
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001585
Iteration 33/1000 | Loss: 0.00001585
Iteration 34/1000 | Loss: 0.00001585
Iteration 35/1000 | Loss: 0.00001584
Iteration 36/1000 | Loss: 0.00001584
Iteration 37/1000 | Loss: 0.00001583
Iteration 38/1000 | Loss: 0.00001583
Iteration 39/1000 | Loss: 0.00001583
Iteration 40/1000 | Loss: 0.00001582
Iteration 41/1000 | Loss: 0.00001582
Iteration 42/1000 | Loss: 0.00001581
Iteration 43/1000 | Loss: 0.00001581
Iteration 44/1000 | Loss: 0.00001580
Iteration 45/1000 | Loss: 0.00001580
Iteration 46/1000 | Loss: 0.00001580
Iteration 47/1000 | Loss: 0.00001579
Iteration 48/1000 | Loss: 0.00001579
Iteration 49/1000 | Loss: 0.00001579
Iteration 50/1000 | Loss: 0.00001579
Iteration 51/1000 | Loss: 0.00001579
Iteration 52/1000 | Loss: 0.00001578
Iteration 53/1000 | Loss: 0.00001578
Iteration 54/1000 | Loss: 0.00001578
Iteration 55/1000 | Loss: 0.00001577
Iteration 56/1000 | Loss: 0.00001577
Iteration 57/1000 | Loss: 0.00001576
Iteration 58/1000 | Loss: 0.00001576
Iteration 59/1000 | Loss: 0.00001576
Iteration 60/1000 | Loss: 0.00001575
Iteration 61/1000 | Loss: 0.00001575
Iteration 62/1000 | Loss: 0.00001575
Iteration 63/1000 | Loss: 0.00001574
Iteration 64/1000 | Loss: 0.00001574
Iteration 65/1000 | Loss: 0.00001574
Iteration 66/1000 | Loss: 0.00001573
Iteration 67/1000 | Loss: 0.00001573
Iteration 68/1000 | Loss: 0.00001572
Iteration 69/1000 | Loss: 0.00001572
Iteration 70/1000 | Loss: 0.00001572
Iteration 71/1000 | Loss: 0.00001572
Iteration 72/1000 | Loss: 0.00001572
Iteration 73/1000 | Loss: 0.00001571
Iteration 74/1000 | Loss: 0.00001571
Iteration 75/1000 | Loss: 0.00001571
Iteration 76/1000 | Loss: 0.00001570
Iteration 77/1000 | Loss: 0.00001570
Iteration 78/1000 | Loss: 0.00001570
Iteration 79/1000 | Loss: 0.00001570
Iteration 80/1000 | Loss: 0.00001570
Iteration 81/1000 | Loss: 0.00001569
Iteration 82/1000 | Loss: 0.00001569
Iteration 83/1000 | Loss: 0.00001569
Iteration 84/1000 | Loss: 0.00001569
Iteration 85/1000 | Loss: 0.00001568
Iteration 86/1000 | Loss: 0.00001568
Iteration 87/1000 | Loss: 0.00001568
Iteration 88/1000 | Loss: 0.00001568
Iteration 89/1000 | Loss: 0.00001568
Iteration 90/1000 | Loss: 0.00001568
Iteration 91/1000 | Loss: 0.00001568
Iteration 92/1000 | Loss: 0.00001568
Iteration 93/1000 | Loss: 0.00001568
Iteration 94/1000 | Loss: 0.00001567
Iteration 95/1000 | Loss: 0.00001567
Iteration 96/1000 | Loss: 0.00001567
Iteration 97/1000 | Loss: 0.00001567
Iteration 98/1000 | Loss: 0.00001566
Iteration 99/1000 | Loss: 0.00001566
Iteration 100/1000 | Loss: 0.00001565
Iteration 101/1000 | Loss: 0.00001565
Iteration 102/1000 | Loss: 0.00001565
Iteration 103/1000 | Loss: 0.00001564
Iteration 104/1000 | Loss: 0.00001564
Iteration 105/1000 | Loss: 0.00001564
Iteration 106/1000 | Loss: 0.00001564
Iteration 107/1000 | Loss: 0.00001564
Iteration 108/1000 | Loss: 0.00001564
Iteration 109/1000 | Loss: 0.00001563
Iteration 110/1000 | Loss: 0.00001563
Iteration 111/1000 | Loss: 0.00001563
Iteration 112/1000 | Loss: 0.00001563
Iteration 113/1000 | Loss: 0.00001563
Iteration 114/1000 | Loss: 0.00001562
Iteration 115/1000 | Loss: 0.00001562
Iteration 116/1000 | Loss: 0.00001562
Iteration 117/1000 | Loss: 0.00001561
Iteration 118/1000 | Loss: 0.00001561
Iteration 119/1000 | Loss: 0.00001561
Iteration 120/1000 | Loss: 0.00001561
Iteration 121/1000 | Loss: 0.00001561
Iteration 122/1000 | Loss: 0.00001561
Iteration 123/1000 | Loss: 0.00001561
Iteration 124/1000 | Loss: 0.00001561
Iteration 125/1000 | Loss: 0.00001561
Iteration 126/1000 | Loss: 0.00001560
Iteration 127/1000 | Loss: 0.00001560
Iteration 128/1000 | Loss: 0.00001560
Iteration 129/1000 | Loss: 0.00001560
Iteration 130/1000 | Loss: 0.00001560
Iteration 131/1000 | Loss: 0.00001559
Iteration 132/1000 | Loss: 0.00001559
Iteration 133/1000 | Loss: 0.00001559
Iteration 134/1000 | Loss: 0.00001559
Iteration 135/1000 | Loss: 0.00001559
Iteration 136/1000 | Loss: 0.00001559
Iteration 137/1000 | Loss: 0.00001559
Iteration 138/1000 | Loss: 0.00001559
Iteration 139/1000 | Loss: 0.00001559
Iteration 140/1000 | Loss: 0.00001558
Iteration 141/1000 | Loss: 0.00001558
Iteration 142/1000 | Loss: 0.00001558
Iteration 143/1000 | Loss: 0.00001558
Iteration 144/1000 | Loss: 0.00001558
Iteration 145/1000 | Loss: 0.00001558
Iteration 146/1000 | Loss: 0.00001557
Iteration 147/1000 | Loss: 0.00001557
Iteration 148/1000 | Loss: 0.00001556
Iteration 149/1000 | Loss: 0.00001556
Iteration 150/1000 | Loss: 0.00001556
Iteration 151/1000 | Loss: 0.00001556
Iteration 152/1000 | Loss: 0.00001556
Iteration 153/1000 | Loss: 0.00001556
Iteration 154/1000 | Loss: 0.00001555
Iteration 155/1000 | Loss: 0.00001555
Iteration 156/1000 | Loss: 0.00001555
Iteration 157/1000 | Loss: 0.00001555
Iteration 158/1000 | Loss: 0.00001555
Iteration 159/1000 | Loss: 0.00001554
Iteration 160/1000 | Loss: 0.00001554
Iteration 161/1000 | Loss: 0.00001554
Iteration 162/1000 | Loss: 0.00001554
Iteration 163/1000 | Loss: 0.00001554
Iteration 164/1000 | Loss: 0.00001554
Iteration 165/1000 | Loss: 0.00001554
Iteration 166/1000 | Loss: 0.00001553
Iteration 167/1000 | Loss: 0.00001553
Iteration 168/1000 | Loss: 0.00001553
Iteration 169/1000 | Loss: 0.00001553
Iteration 170/1000 | Loss: 0.00001553
Iteration 171/1000 | Loss: 0.00001553
Iteration 172/1000 | Loss: 0.00001553
Iteration 173/1000 | Loss: 0.00001553
Iteration 174/1000 | Loss: 0.00001552
Iteration 175/1000 | Loss: 0.00001552
Iteration 176/1000 | Loss: 0.00001552
Iteration 177/1000 | Loss: 0.00001552
Iteration 178/1000 | Loss: 0.00001552
Iteration 179/1000 | Loss: 0.00001552
Iteration 180/1000 | Loss: 0.00001552
Iteration 181/1000 | Loss: 0.00001552
Iteration 182/1000 | Loss: 0.00001552
Iteration 183/1000 | Loss: 0.00001552
Iteration 184/1000 | Loss: 0.00001552
Iteration 185/1000 | Loss: 0.00001551
Iteration 186/1000 | Loss: 0.00001551
Iteration 187/1000 | Loss: 0.00001551
Iteration 188/1000 | Loss: 0.00001551
Iteration 189/1000 | Loss: 0.00001551
Iteration 190/1000 | Loss: 0.00001551
Iteration 191/1000 | Loss: 0.00001551
Iteration 192/1000 | Loss: 0.00001551
Iteration 193/1000 | Loss: 0.00001551
Iteration 194/1000 | Loss: 0.00001551
Iteration 195/1000 | Loss: 0.00001551
Iteration 196/1000 | Loss: 0.00001551
Iteration 197/1000 | Loss: 0.00001550
Iteration 198/1000 | Loss: 0.00001550
Iteration 199/1000 | Loss: 0.00001550
Iteration 200/1000 | Loss: 0.00001550
Iteration 201/1000 | Loss: 0.00001550
Iteration 202/1000 | Loss: 0.00001550
Iteration 203/1000 | Loss: 0.00001550
Iteration 204/1000 | Loss: 0.00001550
Iteration 205/1000 | Loss: 0.00001550
Iteration 206/1000 | Loss: 0.00001550
Iteration 207/1000 | Loss: 0.00001550
Iteration 208/1000 | Loss: 0.00001550
Iteration 209/1000 | Loss: 0.00001550
Iteration 210/1000 | Loss: 0.00001550
Iteration 211/1000 | Loss: 0.00001550
Iteration 212/1000 | Loss: 0.00001550
Iteration 213/1000 | Loss: 0.00001550
Iteration 214/1000 | Loss: 0.00001550
Iteration 215/1000 | Loss: 0.00001550
Iteration 216/1000 | Loss: 0.00001549
Iteration 217/1000 | Loss: 0.00001549
Iteration 218/1000 | Loss: 0.00001549
Iteration 219/1000 | Loss: 0.00001549
Iteration 220/1000 | Loss: 0.00001549
Iteration 221/1000 | Loss: 0.00001549
Iteration 222/1000 | Loss: 0.00001549
Iteration 223/1000 | Loss: 0.00001549
Iteration 224/1000 | Loss: 0.00001549
Iteration 225/1000 | Loss: 0.00001549
Iteration 226/1000 | Loss: 0.00001549
Iteration 227/1000 | Loss: 0.00001549
Iteration 228/1000 | Loss: 0.00001549
Iteration 229/1000 | Loss: 0.00001549
Iteration 230/1000 | Loss: 0.00001549
Iteration 231/1000 | Loss: 0.00001549
Iteration 232/1000 | Loss: 0.00001549
Iteration 233/1000 | Loss: 0.00001549
Iteration 234/1000 | Loss: 0.00001549
Iteration 235/1000 | Loss: 0.00001549
Iteration 236/1000 | Loss: 0.00001549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.5492290913243778e-05, 1.5492290913243778e-05, 1.5492290913243778e-05, 1.5492290913243778e-05, 1.5492290913243778e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5492290913243778e-05

Optimization complete. Final v2v error: 3.3311426639556885 mm

Highest mean error: 4.065592288970947 mm for frame 90

Lowest mean error: 2.941349744796753 mm for frame 56

Saving results

Total time: 42.31269145011902
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468138
Iteration 2/25 | Loss: 0.00138662
Iteration 3/25 | Loss: 0.00132673
Iteration 4/25 | Loss: 0.00131552
Iteration 5/25 | Loss: 0.00131220
Iteration 6/25 | Loss: 0.00131220
Iteration 7/25 | Loss: 0.00131220
Iteration 8/25 | Loss: 0.00131220
Iteration 9/25 | Loss: 0.00131220
Iteration 10/25 | Loss: 0.00131220
Iteration 11/25 | Loss: 0.00131220
Iteration 12/25 | Loss: 0.00131220
Iteration 13/25 | Loss: 0.00131220
Iteration 14/25 | Loss: 0.00131220
Iteration 15/25 | Loss: 0.00131220
Iteration 16/25 | Loss: 0.00131220
Iteration 17/25 | Loss: 0.00131220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013121991651132703, 0.0013121991651132703, 0.0013121991651132703, 0.0013121991651132703, 0.0013121991651132703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013121991651132703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29716980
Iteration 2/25 | Loss: 0.00132950
Iteration 3/25 | Loss: 0.00132948
Iteration 4/25 | Loss: 0.00132948
Iteration 5/25 | Loss: 0.00132948
Iteration 6/25 | Loss: 0.00132948
Iteration 7/25 | Loss: 0.00132948
Iteration 8/25 | Loss: 0.00132948
Iteration 9/25 | Loss: 0.00132948
Iteration 10/25 | Loss: 0.00132948
Iteration 11/25 | Loss: 0.00132948
Iteration 12/25 | Loss: 0.00132948
Iteration 13/25 | Loss: 0.00132948
Iteration 14/25 | Loss: 0.00132948
Iteration 15/25 | Loss: 0.00132948
Iteration 16/25 | Loss: 0.00132948
Iteration 17/25 | Loss: 0.00132948
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013294785749167204, 0.0013294785749167204, 0.0013294785749167204, 0.0013294785749167204, 0.0013294785749167204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013294785749167204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132948
Iteration 2/1000 | Loss: 0.00002862
Iteration 3/1000 | Loss: 0.00002061
Iteration 4/1000 | Loss: 0.00001832
Iteration 5/1000 | Loss: 0.00001725
Iteration 6/1000 | Loss: 0.00001652
Iteration 7/1000 | Loss: 0.00001619
Iteration 8/1000 | Loss: 0.00001583
Iteration 9/1000 | Loss: 0.00001552
Iteration 10/1000 | Loss: 0.00001530
Iteration 11/1000 | Loss: 0.00001524
Iteration 12/1000 | Loss: 0.00001523
Iteration 13/1000 | Loss: 0.00001522
Iteration 14/1000 | Loss: 0.00001522
Iteration 15/1000 | Loss: 0.00001515
Iteration 16/1000 | Loss: 0.00001513
Iteration 17/1000 | Loss: 0.00001506
Iteration 18/1000 | Loss: 0.00001505
Iteration 19/1000 | Loss: 0.00001504
Iteration 20/1000 | Loss: 0.00001496
Iteration 21/1000 | Loss: 0.00001492
Iteration 22/1000 | Loss: 0.00001478
Iteration 23/1000 | Loss: 0.00001476
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001470
Iteration 26/1000 | Loss: 0.00001470
Iteration 27/1000 | Loss: 0.00001470
Iteration 28/1000 | Loss: 0.00001469
Iteration 29/1000 | Loss: 0.00001468
Iteration 30/1000 | Loss: 0.00001463
Iteration 31/1000 | Loss: 0.00001459
Iteration 32/1000 | Loss: 0.00001459
Iteration 33/1000 | Loss: 0.00001458
Iteration 34/1000 | Loss: 0.00001457
Iteration 35/1000 | Loss: 0.00001457
Iteration 36/1000 | Loss: 0.00001457
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001456
Iteration 39/1000 | Loss: 0.00001455
Iteration 40/1000 | Loss: 0.00001448
Iteration 41/1000 | Loss: 0.00001447
Iteration 42/1000 | Loss: 0.00001447
Iteration 43/1000 | Loss: 0.00001446
Iteration 44/1000 | Loss: 0.00001446
Iteration 45/1000 | Loss: 0.00001446
Iteration 46/1000 | Loss: 0.00001446
Iteration 47/1000 | Loss: 0.00001446
Iteration 48/1000 | Loss: 0.00001445
Iteration 49/1000 | Loss: 0.00001445
Iteration 50/1000 | Loss: 0.00001444
Iteration 51/1000 | Loss: 0.00001444
Iteration 52/1000 | Loss: 0.00001444
Iteration 53/1000 | Loss: 0.00001443
Iteration 54/1000 | Loss: 0.00001443
Iteration 55/1000 | Loss: 0.00001443
Iteration 56/1000 | Loss: 0.00001442
Iteration 57/1000 | Loss: 0.00001441
Iteration 58/1000 | Loss: 0.00001441
Iteration 59/1000 | Loss: 0.00001441
Iteration 60/1000 | Loss: 0.00001441
Iteration 61/1000 | Loss: 0.00001440
Iteration 62/1000 | Loss: 0.00001440
Iteration 63/1000 | Loss: 0.00001439
Iteration 64/1000 | Loss: 0.00001438
Iteration 65/1000 | Loss: 0.00001438
Iteration 66/1000 | Loss: 0.00001437
Iteration 67/1000 | Loss: 0.00001437
Iteration 68/1000 | Loss: 0.00001437
Iteration 69/1000 | Loss: 0.00001437
Iteration 70/1000 | Loss: 0.00001437
Iteration 71/1000 | Loss: 0.00001436
Iteration 72/1000 | Loss: 0.00001436
Iteration 73/1000 | Loss: 0.00001436
Iteration 74/1000 | Loss: 0.00001436
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001435
Iteration 77/1000 | Loss: 0.00001435
Iteration 78/1000 | Loss: 0.00001434
Iteration 79/1000 | Loss: 0.00001433
Iteration 80/1000 | Loss: 0.00001433
Iteration 81/1000 | Loss: 0.00001432
Iteration 82/1000 | Loss: 0.00001432
Iteration 83/1000 | Loss: 0.00001432
Iteration 84/1000 | Loss: 0.00001431
Iteration 85/1000 | Loss: 0.00001431
Iteration 86/1000 | Loss: 0.00001431
Iteration 87/1000 | Loss: 0.00001431
Iteration 88/1000 | Loss: 0.00001430
Iteration 89/1000 | Loss: 0.00001429
Iteration 90/1000 | Loss: 0.00001429
Iteration 91/1000 | Loss: 0.00001429
Iteration 92/1000 | Loss: 0.00001429
Iteration 93/1000 | Loss: 0.00001429
Iteration 94/1000 | Loss: 0.00001429
Iteration 95/1000 | Loss: 0.00001429
Iteration 96/1000 | Loss: 0.00001429
Iteration 97/1000 | Loss: 0.00001429
Iteration 98/1000 | Loss: 0.00001429
Iteration 99/1000 | Loss: 0.00001429
Iteration 100/1000 | Loss: 0.00001429
Iteration 101/1000 | Loss: 0.00001429
Iteration 102/1000 | Loss: 0.00001429
Iteration 103/1000 | Loss: 0.00001429
Iteration 104/1000 | Loss: 0.00001428
Iteration 105/1000 | Loss: 0.00001428
Iteration 106/1000 | Loss: 0.00001427
Iteration 107/1000 | Loss: 0.00001427
Iteration 108/1000 | Loss: 0.00001426
Iteration 109/1000 | Loss: 0.00001426
Iteration 110/1000 | Loss: 0.00001426
Iteration 111/1000 | Loss: 0.00001425
Iteration 112/1000 | Loss: 0.00001425
Iteration 113/1000 | Loss: 0.00001425
Iteration 114/1000 | Loss: 0.00001425
Iteration 115/1000 | Loss: 0.00001425
Iteration 116/1000 | Loss: 0.00001425
Iteration 117/1000 | Loss: 0.00001424
Iteration 118/1000 | Loss: 0.00001424
Iteration 119/1000 | Loss: 0.00001424
Iteration 120/1000 | Loss: 0.00001424
Iteration 121/1000 | Loss: 0.00001423
Iteration 122/1000 | Loss: 0.00001423
Iteration 123/1000 | Loss: 0.00001423
Iteration 124/1000 | Loss: 0.00001423
Iteration 125/1000 | Loss: 0.00001423
Iteration 126/1000 | Loss: 0.00001423
Iteration 127/1000 | Loss: 0.00001422
Iteration 128/1000 | Loss: 0.00001422
Iteration 129/1000 | Loss: 0.00001422
Iteration 130/1000 | Loss: 0.00001422
Iteration 131/1000 | Loss: 0.00001422
Iteration 132/1000 | Loss: 0.00001422
Iteration 133/1000 | Loss: 0.00001422
Iteration 134/1000 | Loss: 0.00001422
Iteration 135/1000 | Loss: 0.00001422
Iteration 136/1000 | Loss: 0.00001422
Iteration 137/1000 | Loss: 0.00001422
Iteration 138/1000 | Loss: 0.00001422
Iteration 139/1000 | Loss: 0.00001422
Iteration 140/1000 | Loss: 0.00001421
Iteration 141/1000 | Loss: 0.00001421
Iteration 142/1000 | Loss: 0.00001421
Iteration 143/1000 | Loss: 0.00001421
Iteration 144/1000 | Loss: 0.00001421
Iteration 145/1000 | Loss: 0.00001421
Iteration 146/1000 | Loss: 0.00001421
Iteration 147/1000 | Loss: 0.00001420
Iteration 148/1000 | Loss: 0.00001420
Iteration 149/1000 | Loss: 0.00001419
Iteration 150/1000 | Loss: 0.00001419
Iteration 151/1000 | Loss: 0.00001419
Iteration 152/1000 | Loss: 0.00001419
Iteration 153/1000 | Loss: 0.00001419
Iteration 154/1000 | Loss: 0.00001419
Iteration 155/1000 | Loss: 0.00001419
Iteration 156/1000 | Loss: 0.00001419
Iteration 157/1000 | Loss: 0.00001419
Iteration 158/1000 | Loss: 0.00001419
Iteration 159/1000 | Loss: 0.00001419
Iteration 160/1000 | Loss: 0.00001419
Iteration 161/1000 | Loss: 0.00001419
Iteration 162/1000 | Loss: 0.00001418
Iteration 163/1000 | Loss: 0.00001418
Iteration 164/1000 | Loss: 0.00001418
Iteration 165/1000 | Loss: 0.00001418
Iteration 166/1000 | Loss: 0.00001418
Iteration 167/1000 | Loss: 0.00001418
Iteration 168/1000 | Loss: 0.00001418
Iteration 169/1000 | Loss: 0.00001418
Iteration 170/1000 | Loss: 0.00001418
Iteration 171/1000 | Loss: 0.00001418
Iteration 172/1000 | Loss: 0.00001418
Iteration 173/1000 | Loss: 0.00001418
Iteration 174/1000 | Loss: 0.00001418
Iteration 175/1000 | Loss: 0.00001418
Iteration 176/1000 | Loss: 0.00001418
Iteration 177/1000 | Loss: 0.00001418
Iteration 178/1000 | Loss: 0.00001418
Iteration 179/1000 | Loss: 0.00001417
Iteration 180/1000 | Loss: 0.00001417
Iteration 181/1000 | Loss: 0.00001417
Iteration 182/1000 | Loss: 0.00001417
Iteration 183/1000 | Loss: 0.00001417
Iteration 184/1000 | Loss: 0.00001417
Iteration 185/1000 | Loss: 0.00001417
Iteration 186/1000 | Loss: 0.00001417
Iteration 187/1000 | Loss: 0.00001417
Iteration 188/1000 | Loss: 0.00001417
Iteration 189/1000 | Loss: 0.00001417
Iteration 190/1000 | Loss: 0.00001416
Iteration 191/1000 | Loss: 0.00001416
Iteration 192/1000 | Loss: 0.00001416
Iteration 193/1000 | Loss: 0.00001416
Iteration 194/1000 | Loss: 0.00001416
Iteration 195/1000 | Loss: 0.00001416
Iteration 196/1000 | Loss: 0.00001416
Iteration 197/1000 | Loss: 0.00001416
Iteration 198/1000 | Loss: 0.00001416
Iteration 199/1000 | Loss: 0.00001416
Iteration 200/1000 | Loss: 0.00001416
Iteration 201/1000 | Loss: 0.00001416
Iteration 202/1000 | Loss: 0.00001415
Iteration 203/1000 | Loss: 0.00001415
Iteration 204/1000 | Loss: 0.00001415
Iteration 205/1000 | Loss: 0.00001415
Iteration 206/1000 | Loss: 0.00001415
Iteration 207/1000 | Loss: 0.00001415
Iteration 208/1000 | Loss: 0.00001415
Iteration 209/1000 | Loss: 0.00001415
Iteration 210/1000 | Loss: 0.00001415
Iteration 211/1000 | Loss: 0.00001415
Iteration 212/1000 | Loss: 0.00001414
Iteration 213/1000 | Loss: 0.00001414
Iteration 214/1000 | Loss: 0.00001414
Iteration 215/1000 | Loss: 0.00001414
Iteration 216/1000 | Loss: 0.00001414
Iteration 217/1000 | Loss: 0.00001414
Iteration 218/1000 | Loss: 0.00001414
Iteration 219/1000 | Loss: 0.00001414
Iteration 220/1000 | Loss: 0.00001414
Iteration 221/1000 | Loss: 0.00001414
Iteration 222/1000 | Loss: 0.00001414
Iteration 223/1000 | Loss: 0.00001414
Iteration 224/1000 | Loss: 0.00001414
Iteration 225/1000 | Loss: 0.00001414
Iteration 226/1000 | Loss: 0.00001414
Iteration 227/1000 | Loss: 0.00001414
Iteration 228/1000 | Loss: 0.00001414
Iteration 229/1000 | Loss: 0.00001414
Iteration 230/1000 | Loss: 0.00001414
Iteration 231/1000 | Loss: 0.00001413
Iteration 232/1000 | Loss: 0.00001413
Iteration 233/1000 | Loss: 0.00001413
Iteration 234/1000 | Loss: 0.00001413
Iteration 235/1000 | Loss: 0.00001413
Iteration 236/1000 | Loss: 0.00001413
Iteration 237/1000 | Loss: 0.00001413
Iteration 238/1000 | Loss: 0.00001413
Iteration 239/1000 | Loss: 0.00001413
Iteration 240/1000 | Loss: 0.00001413
Iteration 241/1000 | Loss: 0.00001413
Iteration 242/1000 | Loss: 0.00001413
Iteration 243/1000 | Loss: 0.00001413
Iteration 244/1000 | Loss: 0.00001413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [1.4128006114333402e-05, 1.4128006114333402e-05, 1.4128006114333402e-05, 1.4128006114333402e-05, 1.4128006114333402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4128006114333402e-05

Optimization complete. Final v2v error: 3.1498923301696777 mm

Highest mean error: 3.4924659729003906 mm for frame 178

Lowest mean error: 2.929654598236084 mm for frame 223

Saving results

Total time: 50.664693117141724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00614461
Iteration 2/25 | Loss: 0.00135255
Iteration 3/25 | Loss: 0.00128332
Iteration 4/25 | Loss: 0.00127151
Iteration 5/25 | Loss: 0.00126745
Iteration 6/25 | Loss: 0.00126685
Iteration 7/25 | Loss: 0.00126685
Iteration 8/25 | Loss: 0.00126685
Iteration 9/25 | Loss: 0.00126685
Iteration 10/25 | Loss: 0.00126685
Iteration 11/25 | Loss: 0.00126685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001266848179511726, 0.001266848179511726, 0.001266848179511726, 0.001266848179511726, 0.001266848179511726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001266848179511726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60879660
Iteration 2/25 | Loss: 0.00144439
Iteration 3/25 | Loss: 0.00144438
Iteration 4/25 | Loss: 0.00144438
Iteration 5/25 | Loss: 0.00144438
Iteration 6/25 | Loss: 0.00144438
Iteration 7/25 | Loss: 0.00144438
Iteration 8/25 | Loss: 0.00144438
Iteration 9/25 | Loss: 0.00144438
Iteration 10/25 | Loss: 0.00144438
Iteration 11/25 | Loss: 0.00144438
Iteration 12/25 | Loss: 0.00144438
Iteration 13/25 | Loss: 0.00144438
Iteration 14/25 | Loss: 0.00144438
Iteration 15/25 | Loss: 0.00144438
Iteration 16/25 | Loss: 0.00144438
Iteration 17/25 | Loss: 0.00144438
Iteration 18/25 | Loss: 0.00144438
Iteration 19/25 | Loss: 0.00144438
Iteration 20/25 | Loss: 0.00144438
Iteration 21/25 | Loss: 0.00144438
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014443808468058705, 0.0014443808468058705, 0.0014443808468058705, 0.0014443808468058705, 0.0014443808468058705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014443808468058705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144438
Iteration 2/1000 | Loss: 0.00002315
Iteration 3/1000 | Loss: 0.00001738
Iteration 4/1000 | Loss: 0.00001522
Iteration 5/1000 | Loss: 0.00001438
Iteration 6/1000 | Loss: 0.00001365
Iteration 7/1000 | Loss: 0.00001338
Iteration 8/1000 | Loss: 0.00001297
Iteration 9/1000 | Loss: 0.00001261
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001208
Iteration 12/1000 | Loss: 0.00001206
Iteration 13/1000 | Loss: 0.00001188
Iteration 14/1000 | Loss: 0.00001171
Iteration 15/1000 | Loss: 0.00001163
Iteration 16/1000 | Loss: 0.00001163
Iteration 17/1000 | Loss: 0.00001160
Iteration 18/1000 | Loss: 0.00001159
Iteration 19/1000 | Loss: 0.00001158
Iteration 20/1000 | Loss: 0.00001156
Iteration 21/1000 | Loss: 0.00001154
Iteration 22/1000 | Loss: 0.00001153
Iteration 23/1000 | Loss: 0.00001152
Iteration 24/1000 | Loss: 0.00001150
Iteration 25/1000 | Loss: 0.00001149
Iteration 26/1000 | Loss: 0.00001143
Iteration 27/1000 | Loss: 0.00001143
Iteration 28/1000 | Loss: 0.00001134
Iteration 29/1000 | Loss: 0.00001131
Iteration 30/1000 | Loss: 0.00001130
Iteration 31/1000 | Loss: 0.00001129
Iteration 32/1000 | Loss: 0.00001128
Iteration 33/1000 | Loss: 0.00001128
Iteration 34/1000 | Loss: 0.00001127
Iteration 35/1000 | Loss: 0.00001126
Iteration 36/1000 | Loss: 0.00001125
Iteration 37/1000 | Loss: 0.00001125
Iteration 38/1000 | Loss: 0.00001124
Iteration 39/1000 | Loss: 0.00001123
Iteration 40/1000 | Loss: 0.00001122
Iteration 41/1000 | Loss: 0.00001119
Iteration 42/1000 | Loss: 0.00001117
Iteration 43/1000 | Loss: 0.00001117
Iteration 44/1000 | Loss: 0.00001117
Iteration 45/1000 | Loss: 0.00001117
Iteration 46/1000 | Loss: 0.00001117
Iteration 47/1000 | Loss: 0.00001117
Iteration 48/1000 | Loss: 0.00001116
Iteration 49/1000 | Loss: 0.00001114
Iteration 50/1000 | Loss: 0.00001114
Iteration 51/1000 | Loss: 0.00001114
Iteration 52/1000 | Loss: 0.00001114
Iteration 53/1000 | Loss: 0.00001113
Iteration 54/1000 | Loss: 0.00001113
Iteration 55/1000 | Loss: 0.00001113
Iteration 56/1000 | Loss: 0.00001112
Iteration 57/1000 | Loss: 0.00001109
Iteration 58/1000 | Loss: 0.00001109
Iteration 59/1000 | Loss: 0.00001109
Iteration 60/1000 | Loss: 0.00001109
Iteration 61/1000 | Loss: 0.00001109
Iteration 62/1000 | Loss: 0.00001109
Iteration 63/1000 | Loss: 0.00001109
Iteration 64/1000 | Loss: 0.00001109
Iteration 65/1000 | Loss: 0.00001109
Iteration 66/1000 | Loss: 0.00001109
Iteration 67/1000 | Loss: 0.00001108
Iteration 68/1000 | Loss: 0.00001108
Iteration 69/1000 | Loss: 0.00001108
Iteration 70/1000 | Loss: 0.00001107
Iteration 71/1000 | Loss: 0.00001107
Iteration 72/1000 | Loss: 0.00001105
Iteration 73/1000 | Loss: 0.00001105
Iteration 74/1000 | Loss: 0.00001105
Iteration 75/1000 | Loss: 0.00001104
Iteration 76/1000 | Loss: 0.00001104
Iteration 77/1000 | Loss: 0.00001104
Iteration 78/1000 | Loss: 0.00001104
Iteration 79/1000 | Loss: 0.00001104
Iteration 80/1000 | Loss: 0.00001103
Iteration 81/1000 | Loss: 0.00001103
Iteration 82/1000 | Loss: 0.00001103
Iteration 83/1000 | Loss: 0.00001103
Iteration 84/1000 | Loss: 0.00001103
Iteration 85/1000 | Loss: 0.00001103
Iteration 86/1000 | Loss: 0.00001103
Iteration 87/1000 | Loss: 0.00001102
Iteration 88/1000 | Loss: 0.00001102
Iteration 89/1000 | Loss: 0.00001102
Iteration 90/1000 | Loss: 0.00001102
Iteration 91/1000 | Loss: 0.00001102
Iteration 92/1000 | Loss: 0.00001102
Iteration 93/1000 | Loss: 0.00001102
Iteration 94/1000 | Loss: 0.00001102
Iteration 95/1000 | Loss: 0.00001102
Iteration 96/1000 | Loss: 0.00001102
Iteration 97/1000 | Loss: 0.00001102
Iteration 98/1000 | Loss: 0.00001102
Iteration 99/1000 | Loss: 0.00001102
Iteration 100/1000 | Loss: 0.00001101
Iteration 101/1000 | Loss: 0.00001101
Iteration 102/1000 | Loss: 0.00001101
Iteration 103/1000 | Loss: 0.00001101
Iteration 104/1000 | Loss: 0.00001101
Iteration 105/1000 | Loss: 0.00001101
Iteration 106/1000 | Loss: 0.00001101
Iteration 107/1000 | Loss: 0.00001101
Iteration 108/1000 | Loss: 0.00001101
Iteration 109/1000 | Loss: 0.00001100
Iteration 110/1000 | Loss: 0.00001100
Iteration 111/1000 | Loss: 0.00001100
Iteration 112/1000 | Loss: 0.00001100
Iteration 113/1000 | Loss: 0.00001100
Iteration 114/1000 | Loss: 0.00001100
Iteration 115/1000 | Loss: 0.00001100
Iteration 116/1000 | Loss: 0.00001099
Iteration 117/1000 | Loss: 0.00001099
Iteration 118/1000 | Loss: 0.00001098
Iteration 119/1000 | Loss: 0.00001098
Iteration 120/1000 | Loss: 0.00001098
Iteration 121/1000 | Loss: 0.00001098
Iteration 122/1000 | Loss: 0.00001098
Iteration 123/1000 | Loss: 0.00001098
Iteration 124/1000 | Loss: 0.00001098
Iteration 125/1000 | Loss: 0.00001098
Iteration 126/1000 | Loss: 0.00001097
Iteration 127/1000 | Loss: 0.00001097
Iteration 128/1000 | Loss: 0.00001097
Iteration 129/1000 | Loss: 0.00001097
Iteration 130/1000 | Loss: 0.00001096
Iteration 131/1000 | Loss: 0.00001096
Iteration 132/1000 | Loss: 0.00001096
Iteration 133/1000 | Loss: 0.00001096
Iteration 134/1000 | Loss: 0.00001095
Iteration 135/1000 | Loss: 0.00001095
Iteration 136/1000 | Loss: 0.00001095
Iteration 137/1000 | Loss: 0.00001095
Iteration 138/1000 | Loss: 0.00001095
Iteration 139/1000 | Loss: 0.00001095
Iteration 140/1000 | Loss: 0.00001095
Iteration 141/1000 | Loss: 0.00001095
Iteration 142/1000 | Loss: 0.00001094
Iteration 143/1000 | Loss: 0.00001094
Iteration 144/1000 | Loss: 0.00001094
Iteration 145/1000 | Loss: 0.00001094
Iteration 146/1000 | Loss: 0.00001094
Iteration 147/1000 | Loss: 0.00001094
Iteration 148/1000 | Loss: 0.00001094
Iteration 149/1000 | Loss: 0.00001094
Iteration 150/1000 | Loss: 0.00001093
Iteration 151/1000 | Loss: 0.00001093
Iteration 152/1000 | Loss: 0.00001093
Iteration 153/1000 | Loss: 0.00001093
Iteration 154/1000 | Loss: 0.00001093
Iteration 155/1000 | Loss: 0.00001093
Iteration 156/1000 | Loss: 0.00001092
Iteration 157/1000 | Loss: 0.00001092
Iteration 158/1000 | Loss: 0.00001092
Iteration 159/1000 | Loss: 0.00001092
Iteration 160/1000 | Loss: 0.00001092
Iteration 161/1000 | Loss: 0.00001092
Iteration 162/1000 | Loss: 0.00001092
Iteration 163/1000 | Loss: 0.00001092
Iteration 164/1000 | Loss: 0.00001092
Iteration 165/1000 | Loss: 0.00001092
Iteration 166/1000 | Loss: 0.00001092
Iteration 167/1000 | Loss: 0.00001092
Iteration 168/1000 | Loss: 0.00001092
Iteration 169/1000 | Loss: 0.00001092
Iteration 170/1000 | Loss: 0.00001092
Iteration 171/1000 | Loss: 0.00001092
Iteration 172/1000 | Loss: 0.00001092
Iteration 173/1000 | Loss: 0.00001092
Iteration 174/1000 | Loss: 0.00001091
Iteration 175/1000 | Loss: 0.00001091
Iteration 176/1000 | Loss: 0.00001091
Iteration 177/1000 | Loss: 0.00001091
Iteration 178/1000 | Loss: 0.00001091
Iteration 179/1000 | Loss: 0.00001091
Iteration 180/1000 | Loss: 0.00001091
Iteration 181/1000 | Loss: 0.00001091
Iteration 182/1000 | Loss: 0.00001091
Iteration 183/1000 | Loss: 0.00001091
Iteration 184/1000 | Loss: 0.00001091
Iteration 185/1000 | Loss: 0.00001091
Iteration 186/1000 | Loss: 0.00001091
Iteration 187/1000 | Loss: 0.00001091
Iteration 188/1000 | Loss: 0.00001091
Iteration 189/1000 | Loss: 0.00001091
Iteration 190/1000 | Loss: 0.00001090
Iteration 191/1000 | Loss: 0.00001090
Iteration 192/1000 | Loss: 0.00001090
Iteration 193/1000 | Loss: 0.00001090
Iteration 194/1000 | Loss: 0.00001090
Iteration 195/1000 | Loss: 0.00001090
Iteration 196/1000 | Loss: 0.00001090
Iteration 197/1000 | Loss: 0.00001090
Iteration 198/1000 | Loss: 0.00001089
Iteration 199/1000 | Loss: 0.00001089
Iteration 200/1000 | Loss: 0.00001089
Iteration 201/1000 | Loss: 0.00001089
Iteration 202/1000 | Loss: 0.00001089
Iteration 203/1000 | Loss: 0.00001089
Iteration 204/1000 | Loss: 0.00001089
Iteration 205/1000 | Loss: 0.00001089
Iteration 206/1000 | Loss: 0.00001089
Iteration 207/1000 | Loss: 0.00001089
Iteration 208/1000 | Loss: 0.00001089
Iteration 209/1000 | Loss: 0.00001089
Iteration 210/1000 | Loss: 0.00001089
Iteration 211/1000 | Loss: 0.00001089
Iteration 212/1000 | Loss: 0.00001089
Iteration 213/1000 | Loss: 0.00001089
Iteration 214/1000 | Loss: 0.00001089
Iteration 215/1000 | Loss: 0.00001088
Iteration 216/1000 | Loss: 0.00001088
Iteration 217/1000 | Loss: 0.00001088
Iteration 218/1000 | Loss: 0.00001088
Iteration 219/1000 | Loss: 0.00001088
Iteration 220/1000 | Loss: 0.00001088
Iteration 221/1000 | Loss: 0.00001088
Iteration 222/1000 | Loss: 0.00001088
Iteration 223/1000 | Loss: 0.00001088
Iteration 224/1000 | Loss: 0.00001088
Iteration 225/1000 | Loss: 0.00001088
Iteration 226/1000 | Loss: 0.00001088
Iteration 227/1000 | Loss: 0.00001088
Iteration 228/1000 | Loss: 0.00001088
Iteration 229/1000 | Loss: 0.00001088
Iteration 230/1000 | Loss: 0.00001088
Iteration 231/1000 | Loss: 0.00001088
Iteration 232/1000 | Loss: 0.00001088
Iteration 233/1000 | Loss: 0.00001088
Iteration 234/1000 | Loss: 0.00001088
Iteration 235/1000 | Loss: 0.00001088
Iteration 236/1000 | Loss: 0.00001088
Iteration 237/1000 | Loss: 0.00001088
Iteration 238/1000 | Loss: 0.00001088
Iteration 239/1000 | Loss: 0.00001088
Iteration 240/1000 | Loss: 0.00001088
Iteration 241/1000 | Loss: 0.00001088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.087912005459657e-05, 1.087912005459657e-05, 1.087912005459657e-05, 1.087912005459657e-05, 1.087912005459657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.087912005459657e-05

Optimization complete. Final v2v error: 2.8503315448760986 mm

Highest mean error: 3.139329195022583 mm for frame 77

Lowest mean error: 2.732168674468994 mm for frame 43

Saving results

Total time: 45.72004222869873
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957696
Iteration 2/25 | Loss: 0.00189912
Iteration 3/25 | Loss: 0.00140344
Iteration 4/25 | Loss: 0.00136802
Iteration 5/25 | Loss: 0.00136197
Iteration 6/25 | Loss: 0.00134357
Iteration 7/25 | Loss: 0.00132968
Iteration 8/25 | Loss: 0.00131338
Iteration 9/25 | Loss: 0.00130445
Iteration 10/25 | Loss: 0.00129976
Iteration 11/25 | Loss: 0.00129062
Iteration 12/25 | Loss: 0.00128485
Iteration 13/25 | Loss: 0.00128343
Iteration 14/25 | Loss: 0.00127993
Iteration 15/25 | Loss: 0.00127967
Iteration 16/25 | Loss: 0.00128077
Iteration 17/25 | Loss: 0.00127960
Iteration 18/25 | Loss: 0.00127959
Iteration 19/25 | Loss: 0.00127959
Iteration 20/25 | Loss: 0.00127959
Iteration 21/25 | Loss: 0.00127959
Iteration 22/25 | Loss: 0.00127959
Iteration 23/25 | Loss: 0.00127959
Iteration 24/25 | Loss: 0.00127959
Iteration 25/25 | Loss: 0.00127959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78839922
Iteration 2/25 | Loss: 0.00139528
Iteration 3/25 | Loss: 0.00135794
Iteration 4/25 | Loss: 0.00135794
Iteration 5/25 | Loss: 0.00135794
Iteration 6/25 | Loss: 0.00135793
Iteration 7/25 | Loss: 0.00135793
Iteration 8/25 | Loss: 0.00135793
Iteration 9/25 | Loss: 0.00135793
Iteration 10/25 | Loss: 0.00135793
Iteration 11/25 | Loss: 0.00135793
Iteration 12/25 | Loss: 0.00135793
Iteration 13/25 | Loss: 0.00135793
Iteration 14/25 | Loss: 0.00135793
Iteration 15/25 | Loss: 0.00135793
Iteration 16/25 | Loss: 0.00135793
Iteration 17/25 | Loss: 0.00135793
Iteration 18/25 | Loss: 0.00135793
Iteration 19/25 | Loss: 0.00135793
Iteration 20/25 | Loss: 0.00135793
Iteration 21/25 | Loss: 0.00135793
Iteration 22/25 | Loss: 0.00135793
Iteration 23/25 | Loss: 0.00135793
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013579323422163725, 0.0013579323422163725, 0.0013579323422163725, 0.0013579323422163725, 0.0013579323422163725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013579323422163725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135793
Iteration 2/1000 | Loss: 0.00006191
Iteration 3/1000 | Loss: 0.00002736
Iteration 4/1000 | Loss: 0.00002274
Iteration 5/1000 | Loss: 0.00003349
Iteration 6/1000 | Loss: 0.00003469
Iteration 7/1000 | Loss: 0.00001696
Iteration 8/1000 | Loss: 0.00005128
Iteration 9/1000 | Loss: 0.00001752
Iteration 10/1000 | Loss: 0.00003782
Iteration 11/1000 | Loss: 0.00001577
Iteration 12/1000 | Loss: 0.00001542
Iteration 13/1000 | Loss: 0.00001518
Iteration 14/1000 | Loss: 0.00001496
Iteration 15/1000 | Loss: 0.00003137
Iteration 16/1000 | Loss: 0.00004400
Iteration 17/1000 | Loss: 0.00003964
Iteration 18/1000 | Loss: 0.00001798
Iteration 19/1000 | Loss: 0.00001468
Iteration 20/1000 | Loss: 0.00001467
Iteration 21/1000 | Loss: 0.00001467
Iteration 22/1000 | Loss: 0.00001466
Iteration 23/1000 | Loss: 0.00001466
Iteration 24/1000 | Loss: 0.00001466
Iteration 25/1000 | Loss: 0.00001465
Iteration 26/1000 | Loss: 0.00001464
Iteration 27/1000 | Loss: 0.00001461
Iteration 28/1000 | Loss: 0.00001461
Iteration 29/1000 | Loss: 0.00001969
Iteration 30/1000 | Loss: 0.00001457
Iteration 31/1000 | Loss: 0.00001456
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00001456
Iteration 34/1000 | Loss: 0.00001456
Iteration 35/1000 | Loss: 0.00001456
Iteration 36/1000 | Loss: 0.00001456
Iteration 37/1000 | Loss: 0.00001456
Iteration 38/1000 | Loss: 0.00001456
Iteration 39/1000 | Loss: 0.00001455
Iteration 40/1000 | Loss: 0.00001455
Iteration 41/1000 | Loss: 0.00001455
Iteration 42/1000 | Loss: 0.00001455
Iteration 43/1000 | Loss: 0.00001455
Iteration 44/1000 | Loss: 0.00001454
Iteration 45/1000 | Loss: 0.00001454
Iteration 46/1000 | Loss: 0.00001775
Iteration 47/1000 | Loss: 0.00001967
Iteration 48/1000 | Loss: 0.00002958
Iteration 49/1000 | Loss: 0.00001447
Iteration 50/1000 | Loss: 0.00001444
Iteration 51/1000 | Loss: 0.00001443
Iteration 52/1000 | Loss: 0.00001443
Iteration 53/1000 | Loss: 0.00001442
Iteration 54/1000 | Loss: 0.00001442
Iteration 55/1000 | Loss: 0.00001442
Iteration 56/1000 | Loss: 0.00001441
Iteration 57/1000 | Loss: 0.00001441
Iteration 58/1000 | Loss: 0.00001441
Iteration 59/1000 | Loss: 0.00001440
Iteration 60/1000 | Loss: 0.00001440
Iteration 61/1000 | Loss: 0.00001440
Iteration 62/1000 | Loss: 0.00001440
Iteration 63/1000 | Loss: 0.00001440
Iteration 64/1000 | Loss: 0.00001440
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001439
Iteration 67/1000 | Loss: 0.00001439
Iteration 68/1000 | Loss: 0.00001439
Iteration 69/1000 | Loss: 0.00001439
Iteration 70/1000 | Loss: 0.00001439
Iteration 71/1000 | Loss: 0.00001438
Iteration 72/1000 | Loss: 0.00001438
Iteration 73/1000 | Loss: 0.00001438
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001438
Iteration 77/1000 | Loss: 0.00001438
Iteration 78/1000 | Loss: 0.00001438
Iteration 79/1000 | Loss: 0.00001438
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001438
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.4378587366081774e-05, 1.4378587366081774e-05, 1.4378587366081774e-05, 1.4378587366081774e-05, 1.4378587366081774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4378587366081774e-05

Optimization complete. Final v2v error: 3.243199348449707 mm

Highest mean error: 3.6399128437042236 mm for frame 190

Lowest mean error: 2.866377830505371 mm for frame 207

Saving results

Total time: 75.31399369239807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410101
Iteration 2/25 | Loss: 0.00137051
Iteration 3/25 | Loss: 0.00128740
Iteration 4/25 | Loss: 0.00127792
Iteration 5/25 | Loss: 0.00127469
Iteration 6/25 | Loss: 0.00127433
Iteration 7/25 | Loss: 0.00127433
Iteration 8/25 | Loss: 0.00127433
Iteration 9/25 | Loss: 0.00127433
Iteration 10/25 | Loss: 0.00127433
Iteration 11/25 | Loss: 0.00127433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012743253028020263, 0.0012743253028020263, 0.0012743253028020263, 0.0012743253028020263, 0.0012743253028020263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012743253028020263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30771255
Iteration 2/25 | Loss: 0.00159470
Iteration 3/25 | Loss: 0.00159468
Iteration 4/25 | Loss: 0.00159468
Iteration 5/25 | Loss: 0.00159468
Iteration 6/25 | Loss: 0.00159468
Iteration 7/25 | Loss: 0.00159468
Iteration 8/25 | Loss: 0.00159468
Iteration 9/25 | Loss: 0.00159468
Iteration 10/25 | Loss: 0.00159468
Iteration 11/25 | Loss: 0.00159468
Iteration 12/25 | Loss: 0.00159468
Iteration 13/25 | Loss: 0.00159468
Iteration 14/25 | Loss: 0.00159468
Iteration 15/25 | Loss: 0.00159468
Iteration 16/25 | Loss: 0.00159468
Iteration 17/25 | Loss: 0.00159468
Iteration 18/25 | Loss: 0.00159468
Iteration 19/25 | Loss: 0.00159468
Iteration 20/25 | Loss: 0.00159468
Iteration 21/25 | Loss: 0.00159468
Iteration 22/25 | Loss: 0.00159468
Iteration 23/25 | Loss: 0.00159468
Iteration 24/25 | Loss: 0.00159468
Iteration 25/25 | Loss: 0.00159468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159468
Iteration 2/1000 | Loss: 0.00004822
Iteration 3/1000 | Loss: 0.00003146
Iteration 4/1000 | Loss: 0.00002522
Iteration 5/1000 | Loss: 0.00002112
Iteration 6/1000 | Loss: 0.00001943
Iteration 7/1000 | Loss: 0.00001808
Iteration 8/1000 | Loss: 0.00001724
Iteration 9/1000 | Loss: 0.00001665
Iteration 10/1000 | Loss: 0.00001619
Iteration 11/1000 | Loss: 0.00001586
Iteration 12/1000 | Loss: 0.00001568
Iteration 13/1000 | Loss: 0.00001541
Iteration 14/1000 | Loss: 0.00001519
Iteration 15/1000 | Loss: 0.00001517
Iteration 16/1000 | Loss: 0.00001515
Iteration 17/1000 | Loss: 0.00001511
Iteration 18/1000 | Loss: 0.00001509
Iteration 19/1000 | Loss: 0.00001509
Iteration 20/1000 | Loss: 0.00001509
Iteration 21/1000 | Loss: 0.00001508
Iteration 22/1000 | Loss: 0.00001508
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001504
Iteration 25/1000 | Loss: 0.00001503
Iteration 26/1000 | Loss: 0.00001503
Iteration 27/1000 | Loss: 0.00001503
Iteration 28/1000 | Loss: 0.00001502
Iteration 29/1000 | Loss: 0.00001502
Iteration 30/1000 | Loss: 0.00001501
Iteration 31/1000 | Loss: 0.00001500
Iteration 32/1000 | Loss: 0.00001500
Iteration 33/1000 | Loss: 0.00001499
Iteration 34/1000 | Loss: 0.00001499
Iteration 35/1000 | Loss: 0.00001498
Iteration 36/1000 | Loss: 0.00001498
Iteration 37/1000 | Loss: 0.00001496
Iteration 38/1000 | Loss: 0.00001496
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001492
Iteration 41/1000 | Loss: 0.00001491
Iteration 42/1000 | Loss: 0.00001487
Iteration 43/1000 | Loss: 0.00001484
Iteration 44/1000 | Loss: 0.00001484
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001477
Iteration 48/1000 | Loss: 0.00001477
Iteration 49/1000 | Loss: 0.00001476
Iteration 50/1000 | Loss: 0.00001476
Iteration 51/1000 | Loss: 0.00001475
Iteration 52/1000 | Loss: 0.00001475
Iteration 53/1000 | Loss: 0.00001474
Iteration 54/1000 | Loss: 0.00001474
Iteration 55/1000 | Loss: 0.00001474
Iteration 56/1000 | Loss: 0.00001473
Iteration 57/1000 | Loss: 0.00001471
Iteration 58/1000 | Loss: 0.00001471
Iteration 59/1000 | Loss: 0.00001468
Iteration 60/1000 | Loss: 0.00001468
Iteration 61/1000 | Loss: 0.00001467
Iteration 62/1000 | Loss: 0.00001467
Iteration 63/1000 | Loss: 0.00001466
Iteration 64/1000 | Loss: 0.00001466
Iteration 65/1000 | Loss: 0.00001465
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001460
Iteration 70/1000 | Loss: 0.00001459
Iteration 71/1000 | Loss: 0.00001459
Iteration 72/1000 | Loss: 0.00001459
Iteration 73/1000 | Loss: 0.00001458
Iteration 74/1000 | Loss: 0.00001458
Iteration 75/1000 | Loss: 0.00001457
Iteration 76/1000 | Loss: 0.00001457
Iteration 77/1000 | Loss: 0.00001455
Iteration 78/1000 | Loss: 0.00001455
Iteration 79/1000 | Loss: 0.00001454
Iteration 80/1000 | Loss: 0.00001454
Iteration 81/1000 | Loss: 0.00001453
Iteration 82/1000 | Loss: 0.00001453
Iteration 83/1000 | Loss: 0.00001453
Iteration 84/1000 | Loss: 0.00001452
Iteration 85/1000 | Loss: 0.00001452
Iteration 86/1000 | Loss: 0.00001452
Iteration 87/1000 | Loss: 0.00001452
Iteration 88/1000 | Loss: 0.00001451
Iteration 89/1000 | Loss: 0.00001451
Iteration 90/1000 | Loss: 0.00001451
Iteration 91/1000 | Loss: 0.00001450
Iteration 92/1000 | Loss: 0.00001450
Iteration 93/1000 | Loss: 0.00001450
Iteration 94/1000 | Loss: 0.00001450
Iteration 95/1000 | Loss: 0.00001449
Iteration 96/1000 | Loss: 0.00001449
Iteration 97/1000 | Loss: 0.00001449
Iteration 98/1000 | Loss: 0.00001449
Iteration 99/1000 | Loss: 0.00001449
Iteration 100/1000 | Loss: 0.00001448
Iteration 101/1000 | Loss: 0.00001448
Iteration 102/1000 | Loss: 0.00001448
Iteration 103/1000 | Loss: 0.00001448
Iteration 104/1000 | Loss: 0.00001448
Iteration 105/1000 | Loss: 0.00001447
Iteration 106/1000 | Loss: 0.00001447
Iteration 107/1000 | Loss: 0.00001447
Iteration 108/1000 | Loss: 0.00001447
Iteration 109/1000 | Loss: 0.00001447
Iteration 110/1000 | Loss: 0.00001447
Iteration 111/1000 | Loss: 0.00001447
Iteration 112/1000 | Loss: 0.00001447
Iteration 113/1000 | Loss: 0.00001447
Iteration 114/1000 | Loss: 0.00001447
Iteration 115/1000 | Loss: 0.00001446
Iteration 116/1000 | Loss: 0.00001446
Iteration 117/1000 | Loss: 0.00001446
Iteration 118/1000 | Loss: 0.00001446
Iteration 119/1000 | Loss: 0.00001445
Iteration 120/1000 | Loss: 0.00001445
Iteration 121/1000 | Loss: 0.00001444
Iteration 122/1000 | Loss: 0.00001444
Iteration 123/1000 | Loss: 0.00001444
Iteration 124/1000 | Loss: 0.00001444
Iteration 125/1000 | Loss: 0.00001444
Iteration 126/1000 | Loss: 0.00001444
Iteration 127/1000 | Loss: 0.00001444
Iteration 128/1000 | Loss: 0.00001444
Iteration 129/1000 | Loss: 0.00001443
Iteration 130/1000 | Loss: 0.00001443
Iteration 131/1000 | Loss: 0.00001443
Iteration 132/1000 | Loss: 0.00001443
Iteration 133/1000 | Loss: 0.00001443
Iteration 134/1000 | Loss: 0.00001443
Iteration 135/1000 | Loss: 0.00001443
Iteration 136/1000 | Loss: 0.00001443
Iteration 137/1000 | Loss: 0.00001443
Iteration 138/1000 | Loss: 0.00001443
Iteration 139/1000 | Loss: 0.00001442
Iteration 140/1000 | Loss: 0.00001442
Iteration 141/1000 | Loss: 0.00001441
Iteration 142/1000 | Loss: 0.00001441
Iteration 143/1000 | Loss: 0.00001441
Iteration 144/1000 | Loss: 0.00001440
Iteration 145/1000 | Loss: 0.00001440
Iteration 146/1000 | Loss: 0.00001440
Iteration 147/1000 | Loss: 0.00001440
Iteration 148/1000 | Loss: 0.00001440
Iteration 149/1000 | Loss: 0.00001440
Iteration 150/1000 | Loss: 0.00001440
Iteration 151/1000 | Loss: 0.00001440
Iteration 152/1000 | Loss: 0.00001440
Iteration 153/1000 | Loss: 0.00001440
Iteration 154/1000 | Loss: 0.00001440
Iteration 155/1000 | Loss: 0.00001440
Iteration 156/1000 | Loss: 0.00001439
Iteration 157/1000 | Loss: 0.00001439
Iteration 158/1000 | Loss: 0.00001439
Iteration 159/1000 | Loss: 0.00001439
Iteration 160/1000 | Loss: 0.00001439
Iteration 161/1000 | Loss: 0.00001438
Iteration 162/1000 | Loss: 0.00001438
Iteration 163/1000 | Loss: 0.00001438
Iteration 164/1000 | Loss: 0.00001438
Iteration 165/1000 | Loss: 0.00001438
Iteration 166/1000 | Loss: 0.00001438
Iteration 167/1000 | Loss: 0.00001438
Iteration 168/1000 | Loss: 0.00001437
Iteration 169/1000 | Loss: 0.00001437
Iteration 170/1000 | Loss: 0.00001436
Iteration 171/1000 | Loss: 0.00001436
Iteration 172/1000 | Loss: 0.00001436
Iteration 173/1000 | Loss: 0.00001435
Iteration 174/1000 | Loss: 0.00001435
Iteration 175/1000 | Loss: 0.00001435
Iteration 176/1000 | Loss: 0.00001435
Iteration 177/1000 | Loss: 0.00001434
Iteration 178/1000 | Loss: 0.00001434
Iteration 179/1000 | Loss: 0.00001434
Iteration 180/1000 | Loss: 0.00001433
Iteration 181/1000 | Loss: 0.00001433
Iteration 182/1000 | Loss: 0.00001433
Iteration 183/1000 | Loss: 0.00001433
Iteration 184/1000 | Loss: 0.00001433
Iteration 185/1000 | Loss: 0.00001433
Iteration 186/1000 | Loss: 0.00001433
Iteration 187/1000 | Loss: 0.00001432
Iteration 188/1000 | Loss: 0.00001432
Iteration 189/1000 | Loss: 0.00001432
Iteration 190/1000 | Loss: 0.00001431
Iteration 191/1000 | Loss: 0.00001431
Iteration 192/1000 | Loss: 0.00001431
Iteration 193/1000 | Loss: 0.00001431
Iteration 194/1000 | Loss: 0.00001431
Iteration 195/1000 | Loss: 0.00001431
Iteration 196/1000 | Loss: 0.00001431
Iteration 197/1000 | Loss: 0.00001431
Iteration 198/1000 | Loss: 0.00001431
Iteration 199/1000 | Loss: 0.00001431
Iteration 200/1000 | Loss: 0.00001431
Iteration 201/1000 | Loss: 0.00001431
Iteration 202/1000 | Loss: 0.00001431
Iteration 203/1000 | Loss: 0.00001430
Iteration 204/1000 | Loss: 0.00001430
Iteration 205/1000 | Loss: 0.00001430
Iteration 206/1000 | Loss: 0.00001430
Iteration 207/1000 | Loss: 0.00001430
Iteration 208/1000 | Loss: 0.00001430
Iteration 209/1000 | Loss: 0.00001430
Iteration 210/1000 | Loss: 0.00001430
Iteration 211/1000 | Loss: 0.00001430
Iteration 212/1000 | Loss: 0.00001430
Iteration 213/1000 | Loss: 0.00001430
Iteration 214/1000 | Loss: 0.00001430
Iteration 215/1000 | Loss: 0.00001429
Iteration 216/1000 | Loss: 0.00001429
Iteration 217/1000 | Loss: 0.00001429
Iteration 218/1000 | Loss: 0.00001429
Iteration 219/1000 | Loss: 0.00001429
Iteration 220/1000 | Loss: 0.00001429
Iteration 221/1000 | Loss: 0.00001429
Iteration 222/1000 | Loss: 0.00001429
Iteration 223/1000 | Loss: 0.00001429
Iteration 224/1000 | Loss: 0.00001429
Iteration 225/1000 | Loss: 0.00001429
Iteration 226/1000 | Loss: 0.00001429
Iteration 227/1000 | Loss: 0.00001428
Iteration 228/1000 | Loss: 0.00001428
Iteration 229/1000 | Loss: 0.00001428
Iteration 230/1000 | Loss: 0.00001428
Iteration 231/1000 | Loss: 0.00001427
Iteration 232/1000 | Loss: 0.00001427
Iteration 233/1000 | Loss: 0.00001427
Iteration 234/1000 | Loss: 0.00001427
Iteration 235/1000 | Loss: 0.00001427
Iteration 236/1000 | Loss: 0.00001427
Iteration 237/1000 | Loss: 0.00001427
Iteration 238/1000 | Loss: 0.00001427
Iteration 239/1000 | Loss: 0.00001427
Iteration 240/1000 | Loss: 0.00001427
Iteration 241/1000 | Loss: 0.00001427
Iteration 242/1000 | Loss: 0.00001427
Iteration 243/1000 | Loss: 0.00001426
Iteration 244/1000 | Loss: 0.00001426
Iteration 245/1000 | Loss: 0.00001426
Iteration 246/1000 | Loss: 0.00001426
Iteration 247/1000 | Loss: 0.00001426
Iteration 248/1000 | Loss: 0.00001426
Iteration 249/1000 | Loss: 0.00001426
Iteration 250/1000 | Loss: 0.00001426
Iteration 251/1000 | Loss: 0.00001426
Iteration 252/1000 | Loss: 0.00001426
Iteration 253/1000 | Loss: 0.00001426
Iteration 254/1000 | Loss: 0.00001426
Iteration 255/1000 | Loss: 0.00001426
Iteration 256/1000 | Loss: 0.00001426
Iteration 257/1000 | Loss: 0.00001426
Iteration 258/1000 | Loss: 0.00001426
Iteration 259/1000 | Loss: 0.00001425
Iteration 260/1000 | Loss: 0.00001425
Iteration 261/1000 | Loss: 0.00001425
Iteration 262/1000 | Loss: 0.00001425
Iteration 263/1000 | Loss: 0.00001425
Iteration 264/1000 | Loss: 0.00001425
Iteration 265/1000 | Loss: 0.00001425
Iteration 266/1000 | Loss: 0.00001425
Iteration 267/1000 | Loss: 0.00001425
Iteration 268/1000 | Loss: 0.00001425
Iteration 269/1000 | Loss: 0.00001425
Iteration 270/1000 | Loss: 0.00001425
Iteration 271/1000 | Loss: 0.00001425
Iteration 272/1000 | Loss: 0.00001424
Iteration 273/1000 | Loss: 0.00001424
Iteration 274/1000 | Loss: 0.00001424
Iteration 275/1000 | Loss: 0.00001424
Iteration 276/1000 | Loss: 0.00001424
Iteration 277/1000 | Loss: 0.00001424
Iteration 278/1000 | Loss: 0.00001424
Iteration 279/1000 | Loss: 0.00001424
Iteration 280/1000 | Loss: 0.00001424
Iteration 281/1000 | Loss: 0.00001424
Iteration 282/1000 | Loss: 0.00001424
Iteration 283/1000 | Loss: 0.00001424
Iteration 284/1000 | Loss: 0.00001424
Iteration 285/1000 | Loss: 0.00001424
Iteration 286/1000 | Loss: 0.00001424
Iteration 287/1000 | Loss: 0.00001424
Iteration 288/1000 | Loss: 0.00001424
Iteration 289/1000 | Loss: 0.00001424
Iteration 290/1000 | Loss: 0.00001424
Iteration 291/1000 | Loss: 0.00001424
Iteration 292/1000 | Loss: 0.00001424
Iteration 293/1000 | Loss: 0.00001424
Iteration 294/1000 | Loss: 0.00001424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [1.4239287338568829e-05, 1.4239287338568829e-05, 1.4239287338568829e-05, 1.4239287338568829e-05, 1.4239287338568829e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4239287338568829e-05

Optimization complete. Final v2v error: 3.1304216384887695 mm

Highest mean error: 5.039323329925537 mm for frame 82

Lowest mean error: 2.646291971206665 mm for frame 33

Saving results

Total time: 51.11736011505127
